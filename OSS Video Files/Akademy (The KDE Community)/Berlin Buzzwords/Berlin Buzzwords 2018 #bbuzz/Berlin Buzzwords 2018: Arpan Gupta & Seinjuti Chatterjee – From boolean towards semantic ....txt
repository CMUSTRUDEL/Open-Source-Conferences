Title: Berlin Buzzwords 2018: Arpan Gupta & Seinjuti Chatterjee – From boolean towards semantic ...
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Arpan Gupta and Seinjuti Chatterjee talking about "From boolean towards semantic retrieval models".

The default boolean retrieval model in generic full-text search engines is usually not the best choice for domains like e-commerce because of the lack of decidability about optimal combination of conjunctions and disjunctions of terms that would yield a significant number of relevant results. To improve this, Apache Lucene/Solr introduced the minimum match criteria, where a specified minimum number of query terms must match. But the problem is to decide and tell Solr which are the most important terms (the most salient query theme) that “must” match.

We present a framework to identify (a) important weighted terms in queries (called Must-have Tokens or MTs) and (b) augment them using synonyms. A dependency parser learns weights of tokens to build the MTs list and a neural net embedding and word sense disambiguation is deployed to learn the synonyms specific to MTs in the query context. The models to determine them are built at scale on Apache Spark by analysing clickstream and catalog data across various domains. A custom query parser for Solr is used to augment the queries with these MTs and synonyms.

Read more:
https://2018.berlinbuzzwords.de/18/session/boolean-towards-semantic-retrieval-models

About Arpan Gupta:
https://2018.berlinbuzzwords.de/users/arpan-gupta

About Seinjuti Chatterjee:
https://2018.berlinbuzzwords.de/users/seinjuti-chatterjee

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,070 --> 00:00:10,170
good afternoon everybody I hope you had

00:00:08,230 --> 00:00:13,510
a wonderful lunch but you're still awake

00:00:10,170 --> 00:00:15,459
the topic for today's discussion is from

00:00:13,510 --> 00:00:18,039
bouillon - its semantic retrieval models

00:00:15,459 --> 00:00:20,500
I am Cindy working as a data scientist

00:00:18,039 --> 00:00:24,099
at unboxed I am urban and I am working

00:00:20,500 --> 00:00:26,769
at as an engineer at unboxed right so

00:00:24,099 --> 00:00:28,570
little about unboxed what do we do so we

00:00:26,769 --> 00:00:30,699
are a leading machine learning platform

00:00:28,570 --> 00:00:34,540
for e-commerce search and we are present

00:00:30,699 --> 00:00:37,450
across verticals like fashion hardware

00:00:34,540 --> 00:00:39,160
electronics groceries home improvements

00:00:37,450 --> 00:00:42,039
and we are integrated with a lot of

00:00:39,160 --> 00:00:44,050
platforms and this enables us to collect

00:00:42,039 --> 00:00:46,480
data at the scale of 1.5 billion

00:00:44,050 --> 00:00:50,379
interactions per month which enables our

00:00:46,480 --> 00:00:51,820
machine learning algorithms some of the

00:00:50,379 --> 00:00:54,760
product offerings that we have an

00:00:51,820 --> 00:00:57,430
unboxed site search which are the search

00:00:54,760 --> 00:00:59,859
services and this powers the client

00:00:57,430 --> 00:01:02,530
websites optimizes their queries track

00:00:59,859 --> 00:01:05,230
CTR and C we are the second offering we

00:01:02,530 --> 00:01:06,930
have is intelligent storefront so this

00:01:05,230 --> 00:01:09,369
is a tool that we give to our

00:01:06,930 --> 00:01:11,650
modernizers to control the merchandising

00:01:09,369 --> 00:01:14,380
what promotions to put what deals to

00:01:11,650 --> 00:01:15,909
control and also analytics on that the

00:01:14,380 --> 00:01:18,130
third offering we have is product

00:01:15,909 --> 00:01:19,390
recommendations and this is where we use

00:01:18,130 --> 00:01:21,280
ml learnt

00:01:19,390 --> 00:01:22,869
personalized modules where we can

00:01:21,280 --> 00:01:27,850
recommend products depending on the

00:01:22,869 --> 00:01:30,729
user's intent mentality and tastes now

00:01:27,850 --> 00:01:32,950
coming to the problem statement boolean

00:01:30,729 --> 00:01:36,460
retrieval what is it and why do we need

00:01:32,950 --> 00:01:39,790
semantic retrieval given a query for

00:01:36,460 --> 00:01:42,159
example black bomber jacket what the

00:01:39,790 --> 00:01:44,320
search engine gives you is a parameter

00:01:42,159 --> 00:01:47,530
or a novice called minimum match

00:01:44,320 --> 00:01:50,290
criteria the minimum match criteria can

00:01:47,530 --> 00:01:52,390
be set at 100% which means that match

00:01:50,290 --> 00:01:54,820
any of the tokens in the query term and

00:01:52,390 --> 00:01:57,820
this corresponds to the union of the

00:01:54,820 --> 00:02:00,399
three tokens and the retrieve sets now

00:01:57,820 --> 00:02:03,399
if we set the minimum match criteria to

00:02:00,399 --> 00:02:05,590
0% it's a very strict setting which

00:02:03,399 --> 00:02:08,170
means retrieve the documents which have

00:02:05,590 --> 00:02:09,940
exactly these terms and this represents

00:02:08,170 --> 00:02:13,240
the set intersection area that you see

00:02:09,940 --> 00:02:15,300
there now if we set the minimum match

00:02:13,240 --> 00:02:18,660
criteria to a value in between

00:02:15,300 --> 00:02:21,810
say 60% then it can match two of the

00:02:18,660 --> 00:02:23,850
three tokens but in that case a black

00:02:21,810 --> 00:02:25,500
bomber it can be a good query but we

00:02:23,850 --> 00:02:28,290
know that bomba jacket is probably more

00:02:25,500 --> 00:02:31,350
important here so there is no way we can

00:02:28,290 --> 00:02:33,420
control the boolean retrieval pertaining

00:02:31,350 --> 00:02:35,790
to the weights of the query tokens and

00:02:33,420 --> 00:02:40,410
that's exactly why semantic retrieval is

00:02:35,790 --> 00:02:41,900
important now there are two important

00:02:40,410 --> 00:02:44,190
relevance measures as we all know

00:02:41,900 --> 00:02:45,900
precision which is the number of

00:02:44,190 --> 00:02:47,700
relevant documents out of the retrieved

00:02:45,900 --> 00:02:50,250
set which shows how accurate your

00:02:47,700 --> 00:02:52,410
results are showing and recall which is

00:02:50,250 --> 00:02:55,080
a coverage measure out of the total

00:02:52,410 --> 00:02:57,750
actual relevant documents how many of

00:02:55,080 --> 00:02:59,370
them have you been able to retrieve with

00:02:57,750 --> 00:03:02,250
semantic retrieval we try to optimize

00:02:59,370 --> 00:03:04,680
these two parameters the first part of

00:03:02,250 --> 00:03:07,440
the retrieval recognizes must have

00:03:04,680 --> 00:03:10,710
tokens which is a way to read the query

00:03:07,440 --> 00:03:13,650
tokens and this makes the results if

00:03:10,710 --> 00:03:15,780
precise but it might so happen trying to

00:03:13,650 --> 00:03:18,000
make it precise we have a low recall the

00:03:15,780 --> 00:03:20,280
number of retrieve documents can we

00:03:18,000 --> 00:03:22,260
really know that's what the second part

00:03:20,280 --> 00:03:22,650
of the algorithm kicks in and that is

00:03:22,260 --> 00:03:25,830
called

00:03:22,650 --> 00:03:28,080
the synonym augmenter so finally what we

00:03:25,830 --> 00:03:30,840
have in semantic retrieval is a query

00:03:28,080 --> 00:03:32,550
which is a diction disjunction on an all

00:03:30,840 --> 00:03:35,010
Clause of the must-have tokens and

00:03:32,550 --> 00:03:37,020
they're synonyms I invite my friend here

00:03:35,010 --> 00:03:39,870
to now let us walk through the details

00:03:37,020 --> 00:03:41,030
of these algorithms all right Thank You

00:03:39,870 --> 00:03:43,380
Cindy

00:03:41,030 --> 00:03:45,030
so what you see on the screen right now

00:03:43,380 --> 00:03:47,340
is there a typical product description

00:03:45,030 --> 00:03:51,120
page it has a product image and a lot of

00:03:47,340 --> 00:03:52,650
attributes that describe this image now

00:03:51,120 --> 00:03:55,110
I have a question for all of you the

00:03:52,650 --> 00:03:59,160
question is which noun according to you

00:03:55,110 --> 00:04:02,070
best describe this product yeah do I

00:03:59,160 --> 00:04:03,420
hear a jacket here right so I am pretty

00:04:02,070 --> 00:04:05,850
sure that most of us would recognize

00:04:03,420 --> 00:04:07,050
this other jacket but the fashionistas

00:04:05,850 --> 00:04:10,890
the longest might say that it is a

00:04:07,050 --> 00:04:13,739
bomber jacket so this is exactly the

00:04:10,890 --> 00:04:15,450
intuition behind the must-have tokens we

00:04:13,739 --> 00:04:19,530
tend to think of must-have tokens as the

00:04:15,450 --> 00:04:21,989
best representation of the product now

00:04:19,530 --> 00:04:23,370
sometimes it so happens that the query

00:04:21,989 --> 00:04:25,440
issued is something like Christmas

00:04:23,370 --> 00:04:28,320
pajamas when the catalog has other

00:04:25,440 --> 00:04:29,279
variations of it like festive PJs or

00:04:28,320 --> 00:04:32,009
Christmas PJs

00:04:29,279 --> 00:04:34,439
when the query does not exactly match

00:04:32,009 --> 00:04:36,389
the contents of the catalog in search

00:04:34,439 --> 00:04:38,789
cases to bridge the gap between the user

00:04:36,389 --> 00:04:41,399
query and the catalog if we make use of

00:04:38,789 --> 00:04:42,959
synonyms so there are broadly three

00:04:41,399 --> 00:04:45,629
categories of synonyms that we make use

00:04:42,959 --> 00:04:48,239
of the first our conventional synonyms

00:04:45,629 --> 00:04:50,669
which are which could be something like

00:04:48,239 --> 00:04:52,529
pullovers and sweaters then there are

00:04:50,669 --> 00:04:55,379
very strongly related words like

00:04:52,529 --> 00:04:58,439
printers laser jet inkjet and so on

00:04:55,379 --> 00:05:00,509
there are spelling or lemma variants so

00:04:58,439 --> 00:05:01,949
this is a language phenomena where there

00:05:00,509 --> 00:05:04,829
are multiple ways of conveying the same

00:05:01,949 --> 00:05:07,110
thing for example wireless enabled phone

00:05:04,829 --> 00:05:09,029
and phone with Wi-Fi they can be the

00:05:07,110 --> 00:05:13,379
same thing and they will lead to similar

00:05:09,029 --> 00:05:15,509
result set then Packers T so Packers T

00:05:13,379 --> 00:05:17,579
and Green Bay Packers t-shirt they would

00:05:15,509 --> 00:05:20,339
they mean the same thing and will also

00:05:17,579 --> 00:05:23,999
lead to similar result sets T and t

00:05:20,339 --> 00:05:25,379
shirts are the llama variants here so

00:05:23,999 --> 00:05:27,719
now let's talk about the query

00:05:25,379 --> 00:05:30,809
understanding clear for a query like

00:05:27,719 --> 00:05:32,579
black bomber jacket when it first hits

00:05:30,809 --> 00:05:34,919
the per-unit is standing layer it is

00:05:32,579 --> 00:05:38,009
first intercepted by the must-have token

00:05:34,919 --> 00:05:39,689
recognizer in such case bomber jacket

00:05:38,009 --> 00:05:41,819
would be recognized as the must-have

00:05:39,689 --> 00:05:44,219
token and then it will be augmented by

00:05:41,819 --> 00:05:47,279
synonyms like motorcycle jacket biker

00:05:44,219 --> 00:05:51,179
jacket hooded jacket and so on by doing

00:05:47,279 --> 00:05:53,279
so we not only show one precise result

00:05:51,179 --> 00:05:55,349
which is the reversible bomber jacket we

00:05:53,279 --> 00:05:59,909
actually expand our result set into six

00:05:55,349 --> 00:06:01,919
highly relevant products now let's delve

00:05:59,909 --> 00:06:04,349
deeper into the nitty gritties of the

00:06:01,919 --> 00:06:07,079
generation of must-have tokens we make

00:06:04,349 --> 00:06:10,139
use of the top queries from our domain

00:06:07,079 --> 00:06:13,169
query logs the first step in this

00:06:10,139 --> 00:06:16,439
generation is to generate a dependency

00:06:13,169 --> 00:06:19,139
parse of the queries so dependency

00:06:16,439 --> 00:06:21,149
parser basically takes a sentence as an

00:06:19,139 --> 00:06:23,809
input and it returns the syntactic

00:06:21,149 --> 00:06:27,479
relation a syntactic structure of the

00:06:23,809 --> 00:06:30,360
sentence so in this case it tells us

00:06:27,479 --> 00:06:32,249
that black is an adjective and it is

00:06:30,360 --> 00:06:33,929
connected with jacket which is a noun

00:06:32,249 --> 00:06:36,989
and this relationship between them is

00:06:33,929 --> 00:06:38,610
the adjective modifier relationship it

00:06:36,989 --> 00:06:41,789
also tells us that bombard which is a

00:06:38,610 --> 00:06:43,080
noun is connected with jacket and it

00:06:41,789 --> 00:06:44,970
also describes jacket

00:06:43,080 --> 00:06:48,180
this relationship between two nouns is

00:06:44,970 --> 00:06:50,189
called a compound relationship so Jacket

00:06:48,180 --> 00:06:52,620
is actually the root of both these

00:06:50,189 --> 00:06:55,409
relationships now we observe that among

00:06:52,620 --> 00:06:57,210
our top queries more than eighty-five

00:06:55,409 --> 00:07:00,569
percent of them had the adjectives

00:06:57,210 --> 00:07:02,520
modify relationship while more than 50%

00:07:00,569 --> 00:07:09,030
of our queries had the compound

00:07:02,520 --> 00:07:11,099
relationships so we make a heavy use of

00:07:09,030 --> 00:07:13,379
these relationships we are particularly

00:07:11,099 --> 00:07:15,750
interested in the root of the adjectives

00:07:13,379 --> 00:07:18,960
modifier relationships so in this case a

00:07:15,750 --> 00:07:21,270
black jacket have the adjectives modify

00:07:18,960 --> 00:07:24,449
relationships and jacket is the root of

00:07:21,270 --> 00:07:28,860
it so we will add jacket to the list of

00:07:24,449 --> 00:07:30,539
candidates then for bomba jacket which

00:07:28,860 --> 00:07:32,789
are connected through the compound

00:07:30,539 --> 00:07:35,789
relationships we will add both of them I

00:07:32,789 --> 00:07:39,449
mean both the nouns as phrases to the

00:07:35,789 --> 00:07:41,279
list of candidates now once we have this

00:07:39,449 --> 00:07:43,680
exhaustive and comprehensive set of

00:07:41,279 --> 00:07:44,789
must-haves tokens the next step is to

00:07:43,680 --> 00:07:47,669
assign importance to each of these

00:07:44,789 --> 00:07:52,440
candidates we do so by assigning a score

00:07:47,669 --> 00:07:55,740
to each of them the first the this code

00:07:52,440 --> 00:07:58,080
is dependent on two major factors so

00:07:55,740 --> 00:08:02,490
it's a composite score the first factor

00:07:58,080 --> 00:08:04,169
here is to look at the number of queries

00:08:02,490 --> 00:08:06,569
that each of these candidates will

00:08:04,169 --> 00:08:09,449
actually cover and among all the queries

00:08:06,569 --> 00:08:12,990
that they cover we look at the queries

00:08:09,449 --> 00:08:15,389
where they actually are the root of a

00:08:12,990 --> 00:08:18,449
mod or are part of compound

00:08:15,389 --> 00:08:20,400
relationships now let's be realistic we

00:08:18,449 --> 00:08:23,250
will always get grammatically incorrect

00:08:20,400 --> 00:08:26,389
queries like jacket bomber black where

00:08:23,250 --> 00:08:29,550
dependency parser would tend to generate

00:08:26,389 --> 00:08:32,729
incorrect dependency power structures in

00:08:29,550 --> 00:08:34,979
such cases black might be added to the

00:08:32,729 --> 00:08:38,310
list of must-haves tokens but this is

00:08:34,979 --> 00:08:41,430
handled here because the count of the

00:08:38,310 --> 00:08:43,199
queries where they actually are the root

00:08:41,430 --> 00:08:46,640
of a mod or a compound relationships

00:08:43,199 --> 00:08:48,990
will be quite low now my friend will

00:08:46,640 --> 00:08:52,380
tell you about the synonym generation

00:08:48,990 --> 00:08:54,180
pipeline right thanks so now the first

00:08:52,380 --> 00:08:55,709
part of the problem was to weigh the

00:08:54,180 --> 00:08:56,730
query tokens figure out what are the

00:08:55,709 --> 00:08:59,370
important must-haves

00:08:56,730 --> 00:09:01,339
instead once we have that we move on to

00:08:59,370 --> 00:09:04,110
the second stage the synonym generation

00:09:01,339 --> 00:09:07,019
the first step here is to build a local

00:09:04,110 --> 00:09:09,870
corpus and a local purpose consists of

00:09:07,019 --> 00:09:11,850
the catalog and a click stream the click

00:09:09,870 --> 00:09:15,029
query logs for a client or a particular

00:09:11,850 --> 00:09:17,190
domain the second step is to train what

00:09:15,029 --> 00:09:18,810
vector emitting of this and we were

00:09:17,190 --> 00:09:21,600
really talking about that in the next

00:09:18,810 --> 00:09:24,180
few slides once we have the entire list

00:09:21,600 --> 00:09:26,880
how we generate synonyms is a two-prong

00:09:24,180 --> 00:09:29,100
process so we feed the list to a global

00:09:26,880 --> 00:09:31,620
corpus open source English concept tools

00:09:29,100 --> 00:09:33,690
like concept net and word net and we get

00:09:31,620 --> 00:09:35,430
the candidates from there the second

00:09:33,690 --> 00:09:37,019
step is to feed it to the local word to

00:09:35,430 --> 00:09:38,610
vector eminent and generate the

00:09:37,019 --> 00:09:42,839
neighborhood the nearest neighbors of

00:09:38,610 --> 00:09:44,880
these words now sometimes it so happens

00:09:42,839 --> 00:09:47,220
that the context of the word is not

00:09:44,880 --> 00:09:49,560
established so as a result we might have

00:09:47,220 --> 00:09:52,680
different senses I have called sin sets

00:09:49,560 --> 00:09:54,990
across the board so to figure out the

00:09:52,680 --> 00:09:57,990
correct sense of the word and to prune

00:09:54,990 --> 00:10:00,600
out the noisy ones we use an agar that

00:09:57,990 --> 00:10:02,100
we called word sense disambiguation and

00:10:00,600 --> 00:10:05,069
we'll be talking about that in the next

00:10:02,100 --> 00:10:07,589
few slides finally once we have the

00:10:05,069 --> 00:10:09,839
candidates the winners we apply spell

00:10:07,589 --> 00:10:14,040
correction and stemming to figure out

00:10:09,839 --> 00:10:16,319
the final since it now language so

00:10:14,040 --> 00:10:18,959
language has various nuances it can be

00:10:16,319 --> 00:10:21,149
confusing as you see here I died under

00:10:18,959 --> 00:10:24,089
standing in front of the ATM machine the

00:10:21,149 --> 00:10:27,269
kid walks up to him asks him dad what

00:10:24,089 --> 00:10:30,300
are you doing he says just checking my

00:10:27,269 --> 00:10:31,980
balance but did he mean that he was

00:10:30,300 --> 00:10:33,990
checking his posture with his hands high

00:10:31,980 --> 00:10:36,839
up in air or was he talking about the

00:10:33,990 --> 00:10:39,420
amount of money in the ATM so it is

00:10:36,839 --> 00:10:41,760
difficult to understand language but

00:10:39,420 --> 00:10:43,529
nowadays there is a lot of tools which

00:10:41,760 --> 00:10:45,899
are generating so many meals and

00:10:43,529 --> 00:10:47,699
documents and texts that we need to

00:10:45,899 --> 00:10:49,529
teach a machine how to be able to

00:10:47,699 --> 00:10:52,290
classify these documents and cluster

00:10:49,529 --> 00:10:56,040
them that's exactly where the word

00:10:52,290 --> 00:10:58,380
vector a meaning helps so it is a way of

00:10:56,040 --> 00:11:00,569
representation in a machine format of a

00:10:58,380 --> 00:11:03,240
vector of high dimensions which can

00:11:00,569 --> 00:11:07,230
embed lexical ambiguity semantic

00:11:03,240 --> 00:11:09,120
relationships and concepts so this

00:11:07,230 --> 00:11:11,460
concept was in

00:11:09,120 --> 00:11:13,260
by Hinton who said that there is should

00:11:11,460 --> 00:11:16,080
be a distributed representation of

00:11:13,260 --> 00:11:19,020
symbols for example cat as a symbol

00:11:16,080 --> 00:11:20,760
means not much but if we can add the

00:11:19,020 --> 00:11:22,410
neighborhood of cat and its different

00:11:20,760 --> 00:11:24,720
concepts that might be more

00:11:22,410 --> 00:11:27,570
representative and that is the intuition

00:11:24,720 --> 00:11:29,430
between what vector and weddings it's a

00:11:27,570 --> 00:11:31,890
function it's it's a function where a

00:11:29,430 --> 00:11:33,330
word is represented by the contextual

00:11:31,890 --> 00:11:36,440
words or words in its nearest

00:11:33,330 --> 00:11:38,640
neighbourhood and the optimal dimensions

00:11:36,440 --> 00:11:41,100
capture most of the nuances of the

00:11:38,640 --> 00:11:44,580
modeling which is context ambiguity

00:11:41,100 --> 00:11:46,440
semantics and different things now there

00:11:44,580 --> 00:11:49,020
are two popular neural network

00:11:46,440 --> 00:11:51,300
architectures to learn this embedding

00:11:49,020 --> 00:11:52,680
one is the SIBO models were given a

00:11:51,300 --> 00:11:54,480
context you're trying to predict the

00:11:52,680 --> 00:11:56,460
missing words and the second

00:11:54,480 --> 00:11:57,839
architectures keep brand models we are

00:11:56,460 --> 00:12:00,150
given a word you're trying to predict

00:11:57,839 --> 00:12:02,220
the context for the product that we have

00:12:00,150 --> 00:12:03,930
released we have used boogers were to

00:12:02,220 --> 00:12:07,800
make algorithm which is a combination of

00:12:03,930 --> 00:12:10,860
the C Bell and skip REM to give you some

00:12:07,800 --> 00:12:13,350
examples of what it can do so if we look

00:12:10,860 --> 00:12:15,990
at a keyword called jacket and we look

00:12:13,350 --> 00:12:18,089
at the word vector omitting space and we

00:12:15,990 --> 00:12:19,500
do PCA just to project it in a lower

00:12:18,089 --> 00:12:21,390
dimensional space to improve our

00:12:19,500 --> 00:12:23,940
understanding what we see are different

00:12:21,390 --> 00:12:25,740
flavors of jacket evolving so on one

00:12:23,940 --> 00:12:28,470
hand you have one ba jacket leather

00:12:25,740 --> 00:12:30,480
jacket windbreakers on the other hand

00:12:28,470 --> 00:12:33,420
you have winter wear like a nut jacket

00:12:30,480 --> 00:12:36,980
puffer jackets and the third type is

00:12:33,420 --> 00:12:40,110
cardigan sweaters and things like that

00:12:36,980 --> 00:12:42,000
to give you another example the word

00:12:40,110 --> 00:12:44,070
shoes has different concepts in the a

00:12:42,000 --> 00:12:47,240
meaning space so it can range from

00:12:44,070 --> 00:12:51,600
running shoes sneaker sportswear to

00:12:47,240 --> 00:12:53,730
flip-flops and sandals and regular wear

00:12:51,600 --> 00:12:57,209
kind of shoes so these are the different

00:12:53,730 --> 00:12:59,700
concepts of the word shoes now we use

00:12:57,209 --> 00:13:02,100
this word vector omitting and we do it

00:12:59,700 --> 00:13:03,810
on the catalog space that gives us the

00:13:02,100 --> 00:13:05,580
set of synonyms that you saw in the

00:13:03,810 --> 00:13:07,529
slides that open explained

00:13:05,580 --> 00:13:09,480
so for bomber jacket we end up with

00:13:07,529 --> 00:13:11,250
biker jacket moto jacket a leather

00:13:09,480 --> 00:13:14,190
jacket as good recommendations for

00:13:11,250 --> 00:13:15,930
synonyms similarly if you see the urines

00:13:14,190 --> 00:13:17,520
and the nearest neighbor you see the

00:13:15,930 --> 00:13:20,100
different stylings of earrings drop

00:13:17,520 --> 00:13:20,790
earrings hoop earrings stud earrings

00:13:20,100 --> 00:13:22,890
evolving

00:13:20,790 --> 00:13:26,420
on the catalogue and also related

00:13:22,890 --> 00:13:28,980
products like necklaces and bracelets

00:13:26,420 --> 00:13:31,140
now if you do the same operation in the

00:13:28,980 --> 00:13:33,900
user query space it so happens that

00:13:31,140 --> 00:13:37,080
users are also looking for synonyms like

00:13:33,900 --> 00:13:38,910
denim truckers and softshell if we do it

00:13:37,080 --> 00:13:41,100
on glass the different kinds of concepts

00:13:38,910 --> 00:13:47,070
of glass of all like plumpers lipsticks

00:13:41,100 --> 00:13:49,590
lip liner lip color bombs etc so having

00:13:47,070 --> 00:13:51,630
learnt the word vector omitting the

00:13:49,590 --> 00:13:54,150
second part of the application is to do

00:13:51,630 --> 00:13:56,160
word sense disambiguation the word

00:13:54,150 --> 00:13:58,410
orange if fashion domain means a color

00:13:56,160 --> 00:14:01,080
but if you feed it to an open-source

00:13:58,410 --> 00:14:02,490
corpus like concept native word net it's

00:14:01,080 --> 00:14:05,310
going to return you the four different

00:14:02,490 --> 00:14:07,230
senses of the word orange first insect

00:14:05,310 --> 00:14:09,360
indicates color the second one is fruit

00:14:07,230 --> 00:14:11,580
third one is a tree fourth one is a

00:14:09,360 --> 00:14:13,560
pigment now how do you teach a machine

00:14:11,580 --> 00:14:16,230
to pick the correct cluster here which

00:14:13,560 --> 00:14:18,990
is the first one that's where we can

00:14:16,230 --> 00:14:20,970
also use word vector embeddings so we

00:14:18,990 --> 00:14:23,130
take the entire word cloud represented

00:14:20,970 --> 00:14:25,230
here projected in the embedding space

00:14:23,130 --> 00:14:27,690
take the vectors and figure out the

00:14:25,230 --> 00:14:29,790
centroid and then we measure the cosine

00:14:27,690 --> 00:14:32,520
similarity with the M in English of the

00:14:29,790 --> 00:14:34,560
original key which is orange and the one

00:14:32,520 --> 00:14:37,020
which minimizes that distance is the

00:14:34,560 --> 00:14:38,970
winner so in this case the winner is the

00:14:37,020 --> 00:14:40,890
color sunset and we get good

00:14:38,970 --> 00:14:44,340
recommendations as synonyms like orange

00:14:40,890 --> 00:14:48,900
red salmon blonde all of all indicating

00:14:44,340 --> 00:14:52,410
colors now to summarize the things that

00:14:48,900 --> 00:14:54,870
we explain here firstly we get a query

00:14:52,410 --> 00:14:57,300
we do a dependency parsing on the query

00:14:54,870 --> 00:14:59,460
a kind of scoring that gives us the

00:14:57,300 --> 00:15:02,040
must-haves tokens we learn the word

00:14:59,460 --> 00:15:04,350
vector embeddings on a local corpus we

00:15:02,040 --> 00:15:07,170
treat the MDS as keys and use the word

00:15:04,350 --> 00:15:08,790
vector a meaning on the catalogs and on

00:15:07,170 --> 00:15:11,100
the click locks to generate the synonym

00:15:08,790 --> 00:15:14,280
candidates we also use it as a key and

00:15:11,100 --> 00:15:17,250
fit it to an open source concept net

00:15:14,280 --> 00:15:19,260
wide net api is and we figure out the

00:15:17,250 --> 00:15:20,730
synonym candidates but we prune the

00:15:19,260 --> 00:15:23,610
noisy ones using word sense

00:15:20,730 --> 00:15:25,770
disambiguation and finally the query now

00:15:23,610 --> 00:15:28,920
becomes an all Clause a disjunction of

00:15:25,770 --> 00:15:31,710
empties and synonyms and this is finally

00:15:28,920 --> 00:15:33,209
effect to the EDA's max solar query so

00:15:31,710 --> 00:15:34,949
we have written the custom solar

00:15:33,209 --> 00:15:39,509
lagoon's which can convert this to a

00:15:34,949 --> 00:15:42,649
format that solar understands all right

00:15:39,509 --> 00:15:45,420
so conclusions and future work since

00:15:42,649 --> 00:15:48,199
dependency path sits at the core of our

00:15:45,420 --> 00:15:50,850
must-have token generation we want to

00:15:48,199 --> 00:15:53,550
train our own dependency parser using

00:15:50,850 --> 00:15:55,920
dependence using deep learning and we

00:15:53,550 --> 00:15:59,100
intend to extend the must-have token and

00:15:55,920 --> 00:16:00,660
the synonym dataset from one client to

00:15:59,100 --> 00:16:03,660
other clients and finally over one

00:16:00,660 --> 00:16:06,240
domain we intend to improve and simplify

00:16:03,660 --> 00:16:07,889
the vector algebra operations so what I

00:16:06,240 --> 00:16:10,290
mean by that is just like mu coleauxv

00:16:07,889 --> 00:16:12,240
established gender as a vector in a

00:16:10,290 --> 00:16:14,519
meeting space we want to do the same for

00:16:12,240 --> 00:16:16,740
a synonym a vector and you want to

00:16:14,519 --> 00:16:19,050
improve the training times for a word 2

00:16:16,740 --> 00:16:22,889
vector aining by making use of MapReduce

00:16:19,050 --> 00:16:25,920
and that finally we want to implement a

00:16:22,889 --> 00:16:28,490
feedback mechanism to auto create the

00:16:25,920 --> 00:16:31,139
good synonym and must-have token pairs

00:16:28,490 --> 00:16:33,269
by automatically pruning out the noisy

00:16:31,139 --> 00:16:36,119
pair so currently our accuracy sits at

00:16:33,269 --> 00:16:38,399
around 80% we want to push it to a

00:16:36,119 --> 00:16:41,790
hundred percent or as as close to

00:16:38,399 --> 00:16:43,740
hundred percent as we can thank you for

00:16:41,790 --> 00:16:45,530
listening we will take your questions

00:16:43,740 --> 00:16:52,990
now thank you so much

00:16:45,530 --> 00:16:52,990
[Applause]

00:16:53,090 --> 00:16:56,600
discussions for speaking

00:17:06,390 --> 00:17:11,579
hey thanks a lot for the presentation

00:17:09,839 --> 00:17:13,319
really impressive that you implemented

00:17:11,579 --> 00:17:16,679
all the stuff and have it running in

00:17:13,319 --> 00:17:19,650
production and did you ever measure like

00:17:16,679 --> 00:17:21,510
how yeah how much slower queries get by

00:17:19,650 --> 00:17:23,970
the pre-processing because dependency

00:17:21,510 --> 00:17:26,339
parsing is very slow as far as I

00:17:23,970 --> 00:17:29,760
remember maybe for short queries not but

00:17:26,339 --> 00:17:32,880
did you measure that or the performance

00:17:29,760 --> 00:17:35,340
impact right so we we are hitting those

00:17:32,880 --> 00:17:37,620
problems introduction so what we do is

00:17:35,340 --> 00:17:39,720
we train it offline so we do the offline

00:17:37,620 --> 00:17:41,910
analysis on the top queries daily or at

00:17:39,720 --> 00:17:43,710
an hourly basis we learn the dependency

00:17:41,910 --> 00:17:46,559
parse and I must have tokens and we put

00:17:43,710 --> 00:17:48,510
it in a cache and that's how we can do

00:17:46,559 --> 00:17:51,000
the online part because we did notice

00:17:48,510 --> 00:17:54,270
that the dependency parsing on a online

00:17:51,000 --> 00:17:56,820
part is quite slow and that's the timing

00:17:54,270 --> 00:18:04,500
solution for any latency issues that we

00:17:56,820 --> 00:18:06,690
might see I didn't quite or didn't

00:18:04,500 --> 00:18:10,590
completely understand what you do on

00:18:06,690 --> 00:18:12,750
disambiguation so what we try to show

00:18:10,590 --> 00:18:15,150
you is to expand the synonym candidate

00:18:12,750 --> 00:18:17,160
sets so other than the local corpus we

00:18:15,150 --> 00:18:19,470
also want to use open source tools like

00:18:17,160 --> 00:18:21,150
ordinate and concept net and that's

00:18:19,470 --> 00:18:23,490
where we are getting a lot of synonym

00:18:21,150 --> 00:18:25,020
candidates but the word orange in

00:18:23,490 --> 00:18:27,660
fashion means color and therefore

00:18:25,020 --> 00:18:30,000
applying a synonymous of fruit wouldn't

00:18:27,660 --> 00:18:31,530
make sense there so that is exactly

00:18:30,000 --> 00:18:34,320
where we do the word sense

00:18:31,530 --> 00:18:36,540
disambiguation so we get the cin sets

00:18:34,320 --> 00:18:39,240
from word net and then we I trade over

00:18:36,540 --> 00:18:41,250
each and for each one we look at the

00:18:39,240 --> 00:18:43,320
word cloud and we take the local and

00:18:41,250 --> 00:18:44,940
bearings and that's when we figure out

00:18:43,320 --> 00:18:47,040
the distance so no the orange fruit

00:18:44,940 --> 00:18:49,380
seems very far from what we're looking

00:18:47,040 --> 00:18:51,570
at and this color seems more intuitive

00:18:49,380 --> 00:18:53,669
because it's distance is minimizing to

00:18:51,570 --> 00:18:55,860
the vector of orange and that's how we

00:18:53,669 --> 00:19:00,120
kind of figure out the right sensit and

00:18:55,860 --> 00:19:02,669
take it forward and second question the

00:19:00,120 --> 00:19:06,540
queues somehow measure the improvement

00:19:02,669 --> 00:19:09,929
of the recall and position you get by

00:19:06,540 --> 00:19:11,910
applying all these methods definitely so

00:19:09,929 --> 00:19:14,250
there are two ways that we are trying to

00:19:11,910 --> 00:19:16,679
do that so a lot of it was manually

00:19:14,250 --> 00:19:18,090
generated must have tokens or synonyms

00:19:16,679 --> 00:19:19,890
put in place so

00:19:18,090 --> 00:19:22,679
after we moved to this automated method

00:19:19,890 --> 00:19:25,770
there has been a huge uptick because lot

00:19:22,679 --> 00:19:27,150
of clients we can do in parallel and we

00:19:25,770 --> 00:19:29,159
are trying to do a/b testing between

00:19:27,150 --> 00:19:31,320
this manually generated set and the

00:19:29,159 --> 00:19:34,200
automatic set and we are seeing an

00:19:31,320 --> 00:19:38,059
improvement of 20% and up depending on

00:19:34,200 --> 00:19:38,059
the clients eyes and things like that

00:19:41,809 --> 00:19:50,039
any more questions um so are you

00:19:46,980 --> 00:19:54,090
harvesting synonyms both from the corpus

00:19:50,039 --> 00:19:58,679
and the query logs as well as wordnet

00:19:54,090 --> 00:20:02,130
and yes so on the sort of word embedding

00:19:58,679 --> 00:20:03,450
query log document based it's very often

00:20:02,130 --> 00:20:05,850
that you'll find nearest neighbors that

00:20:03,450 --> 00:20:07,710
are that are really not the same ideas

00:20:05,850 --> 00:20:09,179
but that are used in similar contexts

00:20:07,710 --> 00:20:11,039
right I think one of the speaker's gave

00:20:09,179 --> 00:20:13,289
an example of like you know confirm and

00:20:11,039 --> 00:20:15,679
cancel tend to be used in a similar

00:20:13,289 --> 00:20:17,940
context and they're very different words

00:20:15,679 --> 00:20:21,690
can you talk a little bit about how you

00:20:17,940 --> 00:20:23,100
end up pruning or dealing with these

00:20:21,690 --> 00:20:24,419
sort of false positives that you might

00:20:23,100 --> 00:20:26,240
find if you're if you're looking at

00:20:24,419 --> 00:20:29,460
nearest neighbors and an embedding space

00:20:26,240 --> 00:20:30,899
right so so the way we are trying to

00:20:29,460 --> 00:20:35,159
solve that and this is work in progress

00:20:30,899 --> 00:20:37,289
is uh we use some feedback mechanism now

00:20:35,159 --> 00:20:39,539
from humans and we are trying to build a

00:20:37,289 --> 00:20:41,039
classifier on top of that synonym set

00:20:39,539 --> 00:20:43,770
generations right now the accuracy that

00:20:41,039 --> 00:20:45,419
it puts 80% but what we're trying to see

00:20:43,770 --> 00:20:47,460
is that this to any person and their

00:20:45,419 --> 00:20:48,750
feature vectors and if we can do

00:20:47,460 --> 00:20:50,520
something like a relevance feedback

00:20:48,750 --> 00:20:53,010
where we automatically reject the

00:20:50,520 --> 00:20:55,140
suggestions when it is in the boundary

00:20:53,010 --> 00:20:56,850
cases so that is something which is

00:20:55,140 --> 00:21:05,100
working progress and that's how we plan

00:20:56,850 --> 00:21:10,260
to solve the problem thank you alpha and

00:21:05,100 --> 00:21:12,450
situ T I think we can take a break for

00:21:10,260 --> 00:21:16,190
coffee so again we'll meet back in

00:21:12,450 --> 00:21:16,190
another 30 minutes thank you

00:21:16,400 --> 00:21:18,460

YouTube URL: https://www.youtube.com/watch?v=n_nR5m8rZRM


