Title: Berlin Buzzwords 2018: Fabian Hueske – Leverage the power and simplicity of SQL on Apache Flink
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Fabian Hueske talking about "Why and how to leverage the power and simplicity of SQL on Apache Flink".

SQL is the lingua franca of data processing and everybody working with data knows SQL. Apache Flink provides SQL support for querying and processing batch and streaming data. Flink’s SQL support powers large-scale production systems at Alibaba, Huawei, and Uber. Based on Flink SQL, these companies have built systems for their internal users as well as publicly offered services for paying customers. In my talk, I will discuss why you should and how you can (not being Alibaba or Uber) leverage the simplicity and power of SQL on Flink.

I will start exploring the use cases that Flink SQL was designed for and present real-world problems that it can solve. In particular, I'll explain why unified batch and stream processing is important and what it means to run SQL queries on streams of data. After discussing why and when you should use Flink SQL, I will show how to leverage its full potential. 

The Flink community is developing a service that integrates a query interface, (external) table catalogs, and result serving functionality for static, appending, and updating result sets. I will discuss the design and features of this query service and how it will enable exploratory batch and streaming queries, ETL pipelines, and live updating query results that serve applications, such as real-time dashboards. The talk concludes with a brief demo of a client running queries against the service.

Read more:
https://2018.berlinbuzzwords.de/18/session/why-and-how-leverage-power-and-simplicity-sql-apache-flink

About Fabian Hueske:
https://2018.berlinbuzzwords.de/users/fabian-hueske

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,240 --> 00:00:12,680
yeah thank you thanks for showing up for

00:00:08,550 --> 00:00:16,800
the last talk of the conference yeah so

00:00:12,680 --> 00:00:20,400
I was already introduced here with the

00:00:16,800 --> 00:00:23,519
title of the talk this talks about why

00:00:20,400 --> 00:00:25,740
you should run sequel on flink and it's

00:00:23,519 --> 00:00:28,349
how you can do that a few words about

00:00:25,740 --> 00:00:32,040
myself I'm a PMC member of a petra fling

00:00:28,349 --> 00:00:35,010
started basically started building up

00:00:32,040 --> 00:00:36,030
this project since day one since -

00:00:35,010 --> 00:00:38,640
enough years working on the

00:00:36,030 --> 00:00:40,410
relationality ISIF link I'm also trying

00:00:38,640 --> 00:00:42,690
to write a book about stream processing

00:00:40,410 --> 00:00:47,040
with the Petra flink it's going so and

00:00:42,690 --> 00:00:49,770
so working progress still and I'm also a

00:00:47,040 --> 00:00:52,400
co-founder of data artisans which is a

00:00:49,770 --> 00:00:55,590
startup founded by the founders of

00:00:52,400 --> 00:00:58,050
original creators of a Petra flink and

00:00:55,590 --> 00:01:01,110
we're providing the the a platform which

00:00:58,050 --> 00:01:04,830
is a bundle of open source Apache flink

00:01:01,110 --> 00:01:07,280
and the application manager and the

00:01:04,830 --> 00:01:09,810
platform basically integrates fling with

00:01:07,280 --> 00:01:11,990
and puts it into into context they can

00:01:09,810 --> 00:01:14,850
use it where it eases the operation

00:01:11,990 --> 00:01:17,670
operations of streaming applications so

00:01:14,850 --> 00:01:22,409
it's integrated with logging metrics CAC

00:01:17,670 --> 00:01:26,219
ICD it uses kubernetes to to deploy

00:01:22,409 --> 00:01:29,429
flink applications and the application

00:01:26,219 --> 00:01:31,280
manager takes care of the managing the

00:01:29,429 --> 00:01:33,689
lifecycle of applications so you can

00:01:31,280 --> 00:01:35,100
stop an application you can resume it

00:01:33,689 --> 00:01:36,869
you can scale it out scale it and you

00:01:35,100 --> 00:01:38,520
can update the code microwave

00:01:36,869 --> 00:01:43,319
gamification to another cluster and all

00:01:38,520 --> 00:01:46,380
these things so for who has heard about

00:01:43,319 --> 00:01:48,749
the flash effective link or nose or so

00:01:46,380 --> 00:01:52,709
most of you so I guess I can cut this

00:01:48,749 --> 00:01:54,659
rather short so Apache flank is yeah a

00:01:52,709 --> 00:01:56,459
stateful stream processor it covers the

00:01:54,659 --> 00:01:58,200
full bandwidth of streaming applications

00:01:56,459 --> 00:02:03,299
starting on the batch side so a

00:01:58,200 --> 00:02:05,189
processing like finite streams so to say

00:02:03,299 --> 00:02:07,829
going to traditional stream processing

00:02:05,189 --> 00:02:11,400
but also to the edge of event-driven

00:02:07,829 --> 00:02:14,610
applications where your applications

00:02:11,400 --> 00:02:16,780
that consume events and perform certain

00:02:14,610 --> 00:02:19,770
computations and react on them

00:02:16,780 --> 00:02:23,920
flink has a couple of nice features it's

00:02:19,770 --> 00:02:27,820
as I said it can process real time and

00:02:23,920 --> 00:02:30,610
historic data streams it is a true

00:02:27,820 --> 00:02:32,440
processor with very low latency it's

00:02:30,610 --> 00:02:36,220
photo and so you can get exactly ones

00:02:32,440 --> 00:02:38,200
semantics for your state it does lots of

00:02:36,220 --> 00:02:41,320
a memory processing it features event

00:02:38,200 --> 00:02:43,740
time processing so you can really have

00:02:41,320 --> 00:02:47,350
the nice semantics in your applications

00:02:43,740 --> 00:02:50,260
and it scales very well so people use it

00:02:47,350 --> 00:02:54,209
at very very large in very large

00:02:50,260 --> 00:02:57,550
deployments so a few of the users here

00:02:54,209 --> 00:03:00,880
are fairly big big enterprises so we

00:02:57,550 --> 00:03:02,980
have for instance Netflix which uses

00:03:00,880 --> 00:03:07,840
fling to process three trillion events

00:03:02,980 --> 00:03:09,959
per day has jobs in production with 20

00:03:07,840 --> 00:03:12,880
terabytes of state there's also Alibaba

00:03:09,959 --> 00:03:15,340
who built a stream processing platform

00:03:12,880 --> 00:03:17,709
based on based on flink ing is using it

00:03:15,340 --> 00:03:19,780
for a fraud detection and they are also

00:03:17,709 --> 00:03:22,720
a couple more users or so this is just a

00:03:19,780 --> 00:03:27,180
selection of where flink runs in

00:03:22,720 --> 00:03:27,180
production flink has a couple of

00:03:27,269 --> 00:03:33,040
programming api's that you can use which

00:03:29,830 --> 00:03:35,650
are kind of layout so you can start at

00:03:33,040 --> 00:03:38,380
the bottom end on the on the process

00:03:35,650 --> 00:03:43,750
function level which is an interface

00:03:38,380 --> 00:03:45,880
that gives you access to to control time

00:03:43,750 --> 00:03:47,940
and state so which are like the basic

00:03:45,880 --> 00:03:50,920
basic building blocks for building

00:03:47,940 --> 00:03:53,799
streaming streaming applications on top

00:03:50,920 --> 00:03:57,400
of that is the data stream API which

00:03:53,799 --> 00:04:00,390
gives you nice nice shortcuts for very

00:03:57,400 --> 00:04:04,299
common stream processing operations like

00:04:00,390 --> 00:04:07,680
windows window windows for instance or

00:04:04,299 --> 00:04:11,950
also doing a synchronous calls against

00:04:07,680 --> 00:04:14,310
external data stores and 5u on top there

00:04:11,950 --> 00:04:18,100
are the high level API is for for

00:04:14,310 --> 00:04:21,160
streaming analytics sequel and on the

00:04:18,100 --> 00:04:23,100
table API and this is also where this

00:04:21,160 --> 00:04:26,950
talk is is about

00:04:23,100 --> 00:04:30,129
so if link has these two types of

00:04:26,950 --> 00:04:32,529
relational api's there is and

00:04:30,129 --> 00:04:35,979
sequel so standard sequel we're not

00:04:32,529 --> 00:04:39,580
using some kind of work streaming fight

00:04:35,979 --> 00:04:43,269
sequel here so this is a very simple

00:04:39,580 --> 00:04:46,569
query which just groups clicks clicks

00:04:43,269 --> 00:04:49,409
tabla per user and computes account and

00:04:46,569 --> 00:04:52,719
then there is the table API which is a

00:04:49,409 --> 00:04:57,610
links that API link stands for language

00:04:52,719 --> 00:05:00,219
integrated query language and here you

00:04:57,610 --> 00:05:02,349
basically embed the query in your

00:05:00,219 --> 00:05:03,789
application so you see you have some

00:05:02,349 --> 00:05:05,499
kind of a table environment here on

00:05:03,789 --> 00:05:07,449
which you can call them at that scan and

00:05:05,499 --> 00:05:10,509
then you say group I select and this way

00:05:07,449 --> 00:05:12,399
you basically build up a query that is

00:05:10,509 --> 00:05:13,979
here is doing exactly the same as the

00:05:12,399 --> 00:05:17,259
secret theory

00:05:13,979 --> 00:05:20,169
so both api's are unified api's for

00:05:17,259 --> 00:05:22,990
veteran stream processing for veteran

00:05:20,169 --> 00:05:25,179
streaming data and this means that a

00:05:22,990 --> 00:05:28,029
query will speci

00:05:25,179 --> 00:05:30,759
or a query specifies exactly the same

00:05:28,029 --> 00:05:33,069
result regardless whether you run the

00:05:30,759 --> 00:05:35,229
query on on batch data on a file for

00:05:33,069 --> 00:05:38,849
instance or whether you run it on an all

00:05:35,229 --> 00:05:44,519
streaming data such as a Kafka topic

00:05:38,849 --> 00:05:48,339
Curie's are translated in this flow so

00:05:44,519 --> 00:05:51,879
we integrated the translation of the

00:05:48,339 --> 00:05:56,129
table API and seeker queries into one

00:05:51,879 --> 00:05:59,740
flow so both query representations are

00:05:56,129 --> 00:06:02,409
translated into a logical logical

00:05:59,740 --> 00:06:04,569
execution or logical plan we're using

00:06:02,409 --> 00:06:05,800
care set for the secret passing and also

00:06:04,569 --> 00:06:11,169
for the logical teammate for the

00:06:05,800 --> 00:06:12,610
optimization so secret furious end table

00:06:11,169 --> 00:06:16,349
API queries are translated into a

00:06:12,610 --> 00:06:18,399
logical plan um then we apply

00:06:16,349 --> 00:06:20,050
optimizations on that again using

00:06:18,399 --> 00:06:21,969
carrots idea Kerr's optimizer and

00:06:20,050 --> 00:06:23,919
depending on whether the query is

00:06:21,969 --> 00:06:27,279
executed in a streaming or in a batch

00:06:23,919 --> 00:06:30,490
context it is translated into a into a

00:06:27,279 --> 00:06:33,550
data set if link data set plan which is

00:06:30,490 --> 00:06:36,300
flings a better API or into a data

00:06:33,550 --> 00:06:41,829
stream plan for for streaming theories

00:06:36,300 --> 00:06:43,600
so the bottom left part in that case the

00:06:41,829 --> 00:06:46,900
data is a spa

00:06:43,600 --> 00:06:49,530
it's pitched and in the other bottom

00:06:46,900 --> 00:06:54,790
right part we would create a plan for

00:06:49,530 --> 00:06:57,010
for streaming data so what happens if we

00:06:54,790 --> 00:07:02,830
now want to run this very simple query

00:06:57,010 --> 00:07:05,260
on on our table clicks that is where the

00:07:02,830 --> 00:07:08,110
table clicks represents a file so in

00:07:05,260 --> 00:07:10,920
that case we basically read the data

00:07:08,110 --> 00:07:15,100
from the file we would give all the data

00:07:10,920 --> 00:07:19,330
somewhat into the query processor and we

00:07:15,100 --> 00:07:23,350
would get a get a result so the data

00:07:19,330 --> 00:07:29,230
input data is read once and the result

00:07:23,350 --> 00:07:31,780
is also produced at once so this is like

00:07:29,230 --> 00:07:35,710
very very simple the the standard way of

00:07:31,780 --> 00:07:37,780
evaluating queries in batch data so but

00:07:35,710 --> 00:07:40,120
what happens if clicks is a stream so

00:07:37,780 --> 00:07:41,590
here we represent it as a as a stream

00:07:40,120 --> 00:07:45,850
could be a Kafka topic could also be

00:07:41,590 --> 00:07:49,300
data in kinases or whatever other stream

00:07:45,850 --> 00:07:51,160
you you you have and in this case this

00:07:49,300 --> 00:07:53,830
table of course also has some kind of

00:07:51,160 --> 00:07:57,220
the schema but the records appear over

00:07:53,830 --> 00:07:59,160
time and as new records arrive we can

00:07:57,220 --> 00:08:02,770
evaluate the query and incrementally

00:07:59,160 --> 00:08:05,020
compute the result of the table so we

00:08:02,770 --> 00:08:07,570
got a record here for Mary and another

00:08:05,020 --> 00:08:09,640
one for Bob so both counts here are one

00:08:07,570 --> 00:08:10,870
if we now get another one for Mary we

00:08:09,640 --> 00:08:13,330
can increment the counter for Mary to

00:08:10,870 --> 00:08:16,480
two and if we then get another one for

00:08:13,330 --> 00:08:18,670
this we add another row for this so in

00:08:16,480 --> 00:08:22,210
this case the data is continuously read

00:08:18,670 --> 00:08:25,240
consume continuously interested and the

00:08:22,210 --> 00:08:27,820
result is continuously updated however

00:08:25,240 --> 00:08:29,590
in the end the result in both cases is

00:08:27,820 --> 00:08:32,380
the same so we have the same input data

00:08:29,590 --> 00:08:35,289
we run the same query and we get the

00:08:32,380 --> 00:08:39,160
same result so why is it important that

00:08:35,289 --> 00:08:41,740
this this is this property of stream bed

00:08:39,160 --> 00:08:46,870
unification so first of all it's of

00:08:41,740 --> 00:08:49,600
course a usability issue if since flink

00:08:46,870 --> 00:08:52,540
implements NC sequel syntax there is no

00:08:49,600 --> 00:08:54,210
no no custom stream sink sequel syntax

00:08:52,540 --> 00:08:57,100
that anybody would need to learn and

00:08:54,210 --> 00:08:58,660
there's also no hidden semantics so we

00:08:57,100 --> 00:09:02,140
and exactly the same semantics as you

00:08:58,660 --> 00:09:05,050
would expect from from a bad fear

00:09:02,140 --> 00:09:06,730
processor this also increases the

00:09:05,050 --> 00:09:08,410
portability of queries because now you

00:09:06,730 --> 00:09:10,990
can run the same query and bounded and

00:09:08,410 --> 00:09:14,950
unbounded data but also on recorded data

00:09:10,990 --> 00:09:19,840
and on real-time data so in case there

00:09:14,950 --> 00:09:22,180
is some kind of outage here you use stop

00:09:19,840 --> 00:09:24,070
receiving data or whatever happens you

00:09:22,180 --> 00:09:27,100
cannot continue processing the query you

00:09:24,070 --> 00:09:29,980
can take exactly the same query and run

00:09:27,100 --> 00:09:31,570
it on on the recorded data set again you

00:09:29,980 --> 00:09:33,880
can also use this feature to bootstrap

00:09:31,570 --> 00:09:38,650
state you can it's also great feature if

00:09:33,880 --> 00:09:40,810
you want to explore a data set or design

00:09:38,650 --> 00:09:44,670
a query on a small sample of batch data

00:09:40,810 --> 00:09:50,100
and later deploy it on a live stream but

00:09:44,670 --> 00:09:52,720
how can we actually achieve this the

00:09:50,100 --> 00:09:55,330
secret semantics when when running

00:09:52,720 --> 00:09:58,000
theories on streams well that's actually

00:09:55,330 --> 00:10:00,070
not something that is very new in fact

00:09:58,000 --> 00:10:04,240
database systems do that for quite some

00:10:00,070 --> 00:10:07,140
time and the corresponding feature in in

00:10:04,240 --> 00:10:10,210
database terms are materialized views

00:10:07,140 --> 00:10:12,430
these this feature on materialized views

00:10:10,210 --> 00:10:15,370
are kind of similar to regular values

00:10:12,430 --> 00:10:18,520
but they are persisted in this in memory

00:10:15,370 --> 00:10:20,740
and whenever the input tables or the

00:10:18,520 --> 00:10:23,020
base tables of the view definition view

00:10:20,740 --> 00:10:26,140
definition change they also reflect

00:10:23,020 --> 00:10:30,610
these updates in the materialized view

00:10:26,140 --> 00:10:35,260
so if you think about it the updates to

00:10:30,610 --> 00:10:37,540
the base tables like the the records

00:10:35,260 --> 00:10:39,820
that you receive by a stream so that

00:10:37,540 --> 00:10:42,310
those are the updates the view

00:10:39,820 --> 00:10:44,290
definition query is this trimming theory

00:10:42,310 --> 00:10:46,600
that you're evaluating and the result is

00:10:44,290 --> 00:10:48,750
the materialized view so if you think it

00:10:46,600 --> 00:10:51,640
think about it that way

00:10:48,750 --> 00:10:54,370
like this this type of incremental query

00:10:51,640 --> 00:10:56,800
evaluation is not that new at all

00:10:54,370 --> 00:10:58,720
however in the context of link we're

00:10:56,800 --> 00:11:00,250
putting it into a distributed stream

00:10:58,720 --> 00:11:05,520
processor this rooted stay for stream

00:11:00,250 --> 00:11:09,040
processor this is yeah something that

00:11:05,520 --> 00:11:12,950
has not been done so off before

00:11:09,040 --> 00:11:14,779
so in the context of a fling we have

00:11:12,950 --> 00:11:17,330
this concept of a dynamic terror which

00:11:14,779 --> 00:11:23,060
is a table that is changing over time

00:11:17,330 --> 00:11:26,290
and with these dynamic tables you can

00:11:23,060 --> 00:11:29,330
these these dynamic tables can produce

00:11:26,290 --> 00:11:32,630
the input as well as well also as well

00:11:29,330 --> 00:11:34,970
as the output of a query so if you have

00:11:32,630 --> 00:11:37,790
an input dynamic table you apply a query

00:11:34,970 --> 00:11:39,529
on it and then the result of this query

00:11:37,790 --> 00:11:41,240
will be another dynamic table so

00:11:39,529 --> 00:11:45,260
whenever something changes in the input

00:11:41,240 --> 00:11:48,890
table these changes will be will be

00:11:45,260 --> 00:11:51,890
reflected in the output table by having

00:11:48,890 --> 00:11:56,450
an incremental theory evaluation

00:11:51,890 --> 00:11:58,779
mechanism so what does it mean for when

00:11:56,450 --> 00:12:03,080
when running these continuous queries on

00:11:58,779 --> 00:12:05,000
dynamic tables where can I get a such a

00:12:03,080 --> 00:12:06,560
dynamic Taylor from well first of all

00:12:05,000 --> 00:12:09,200
usually you have a stream that you

00:12:06,560 --> 00:12:11,089
somehow want to conceptually convert

00:12:09,200 --> 00:12:13,640
into a dynamic table and there is

00:12:11,089 --> 00:12:16,040
different ways how you can translate a

00:12:13,640 --> 00:12:21,230
stream into a dynamic table and also

00:12:16,040 --> 00:12:23,660
back for instance there are append

00:12:21,230 --> 00:12:27,020
conversions where each record that is

00:12:23,660 --> 00:12:29,839
sent by the stream is treated as an

00:12:27,020 --> 00:12:31,550
insert to your dynamic table so whenever

00:12:29,839 --> 00:12:36,640
you get a new record from the stream you

00:12:31,550 --> 00:12:39,950
just appended to to your dynamic tail

00:12:36,640 --> 00:12:44,690
another mode is the absolute conversion

00:12:39,950 --> 00:12:46,279
where you basically each the the schema

00:12:44,690 --> 00:12:48,709
of the stream has a certain key

00:12:46,279 --> 00:12:50,930
attribute and whenever you get a get a

00:12:48,709 --> 00:12:52,820
record you look up in your dynamic table

00:12:50,930 --> 00:12:55,070
whether there's already a record with

00:12:52,820 --> 00:12:57,110
this key and then you update the record

00:12:55,070 --> 00:12:59,930
and if there is no record for such a key

00:12:57,110 --> 00:13:02,990
you insert the record so this absurd

00:12:59,930 --> 00:13:05,720
conversion and finally the most generic

00:13:02,990 --> 00:13:07,820
one is the change low conversion where

00:13:05,720 --> 00:13:10,130
for each record you have something like

00:13:07,820 --> 00:13:11,810
a flag which tells the system hey this

00:13:10,130 --> 00:13:13,580
is a record that you should insert or

00:13:11,810 --> 00:13:15,140
this is a record that you should remove

00:13:13,580 --> 00:13:17,060
from the table so there's no key

00:13:15,140 --> 00:13:20,600
involved there's simply the notion of

00:13:17,060 --> 00:13:22,819
hey at this and remove this and this way

00:13:20,600 --> 00:13:26,169
you can basically it is the most generic

00:13:22,819 --> 00:13:29,029
so kind of like most expensive way of of

00:13:26,169 --> 00:13:31,789
treating updates in a table or treating

00:13:29,029 --> 00:13:34,279
treating up generating tables from a

00:13:31,789 --> 00:13:40,069
firm stream and but this is very very

00:13:34,279 --> 00:13:43,209
very generic so what kind of operations

00:13:40,069 --> 00:13:46,220
does Flickr just fling

00:13:43,209 --> 00:13:47,479
support in flink 1/5 which has been

00:13:46,220 --> 00:13:50,389
released a couple of weeks ago so

00:13:47,479 --> 00:13:51,679
there's of course all these simple

00:13:50,389 --> 00:13:54,829
things like select from where

00:13:51,679 --> 00:13:58,160
so projection and in filters we also

00:13:54,829 --> 00:14:01,939
support group and having classes on non

00:13:58,160 --> 00:14:06,199
windowed on one window to a group by

00:14:01,939 --> 00:14:08,979
classes but also have these shortcuts to

00:14:06,199 --> 00:14:13,809
define tumbling hope and search windows

00:14:08,979 --> 00:14:16,249
in in the group by clause there is a

00:14:13,809 --> 00:14:18,979
certain subset of joints supported

00:14:16,249 --> 00:14:21,739
windowed joints where you have a record

00:14:18,979 --> 00:14:23,779
from one side and you say hey I want to

00:14:21,739 --> 00:14:25,489
join this with everything that is ten

00:14:23,779 --> 00:14:29,239
minutes earlier in 10 minutes later in

00:14:25,489 --> 00:14:30,769
the other stream these are these window

00:14:29,239 --> 00:14:34,159
joints that we support there's also

00:14:30,769 --> 00:14:35,809
non-winner joints these are joints that

00:14:34,159 --> 00:14:37,699
typically have to materialize the full

00:14:35,809 --> 00:14:39,649
table because there's no time time

00:14:37,699 --> 00:14:40,609
constraint on the table on time

00:14:39,649 --> 00:14:43,850
constraint in the in the joint

00:14:40,609 --> 00:14:48,229
attributes at John predicates sorry

00:14:43,850 --> 00:14:49,939
we support quite a quite a few different

00:14:48,229 --> 00:14:52,249
types of user-defined functions so you

00:14:49,939 --> 00:14:54,350
can also plug in your custom custom

00:14:52,249 --> 00:14:55,879
logic into sequel queries we support

00:14:54,350 --> 00:14:59,029
scalar functions aggregation functions

00:14:55,879 --> 00:15:03,649
and also table valued functions and in

00:14:59,029 --> 00:15:06,019
the last release we added CLI client to

00:15:03,649 --> 00:15:10,759
to play around with the API to submit

00:15:06,019 --> 00:15:12,379
theories to to to fling cluster there's

00:15:10,759 --> 00:15:16,459
a few few operations that are only

00:15:12,379 --> 00:15:18,799
supported in either on either streaming

00:15:16,459 --> 00:15:20,149
data or batch data such as the set

00:15:18,799 --> 00:15:25,279
operations which are only supported in

00:15:20,149 --> 00:15:28,819
batch at the moment and over windows for

00:15:25,279 --> 00:15:31,279
for streaming data so what can you build

00:15:28,819 --> 00:15:34,519
with these tools at hand or with this

00:15:31,279 --> 00:15:36,860
feature set of of sequel

00:15:34,519 --> 00:15:38,590
well first of all you

00:15:36,860 --> 00:15:41,150
can build of course a simple ETL

00:15:38,590 --> 00:15:43,220
low-latency TLO data pipelines where you

00:15:41,150 --> 00:15:48,500
ingest data transform it aggregated may

00:15:43,220 --> 00:15:53,180
be filter it and then pipe data from one

00:15:48,500 --> 00:15:54,980
stream into another or you can ingest

00:15:53,180 --> 00:15:56,560
data from a stream right into a

00:15:54,980 --> 00:16:00,100
distributed file system or into a

00:15:56,560 --> 00:16:04,850
database system you can also use it to

00:16:00,100 --> 00:16:07,340
to run stream a batch analytics both on

00:16:04,850 --> 00:16:12,530
historic data but also live data using

00:16:07,340 --> 00:16:14,870
the same query and you can also use

00:16:12,530 --> 00:16:17,180
these queries to power our live - BOTS

00:16:14,870 --> 00:16:22,160
so you basically define a query on a

00:16:17,180 --> 00:16:24,080
stream that generates basically a

00:16:22,160 --> 00:16:26,330
materialized view which is live updated

00:16:24,080 --> 00:16:28,820
as theta of on the stream arrives and

00:16:26,330 --> 00:16:32,870
then have a dashboard viewing this table

00:16:28,820 --> 00:16:35,660
and visualizing the data so I'm going to

00:16:32,870 --> 00:16:38,660
demonstrate here like a few queries

00:16:35,660 --> 00:16:41,450
using this new si Lang client and the

00:16:38,660 --> 00:16:44,810
data set that we're using is the New

00:16:41,450 --> 00:16:48,200
York right take New York City Taxi

00:16:44,810 --> 00:16:50,060
rights dataset we we stripped it down a

00:16:48,200 --> 00:16:53,870
bit so in our case we only have five

00:16:50,060 --> 00:16:56,090
attributes which is an ID for a ride

00:16:53,870 --> 00:16:58,100
it's a start whether this is an event

00:16:56,090 --> 00:16:59,140
that represents the start of a ride or

00:16:58,100 --> 00:17:02,420
an end of a ride

00:16:59,140 --> 00:17:05,350
we also have the longitude and latitude

00:17:02,420 --> 00:17:08,480
values for where this event happened in

00:17:05,350 --> 00:17:10,610
case of a start event this is where the

00:17:08,480 --> 00:17:12,350
passengers entered the taxi and in case

00:17:10,610 --> 00:17:14,330
of the aunt event this is where they

00:17:12,350 --> 00:17:17,420
left the taxi and then there's also of

00:17:14,330 --> 00:17:21,980
course the time attribute for when this

00:17:17,420 --> 00:17:24,110
event happened so if we now would like

00:17:21,980 --> 00:17:26,750
to have this this use case here

00:17:24,110 --> 00:17:28,580
basically we would like to compute for

00:17:26,750 --> 00:17:34,540
for every location every five minutes

00:17:28,580 --> 00:17:37,010
the number of taxi rides that that that

00:17:34,540 --> 00:17:39,590
departed and arrived at a certain

00:17:37,010 --> 00:17:43,210
location within the last 15 minutes then

00:17:39,590 --> 00:17:47,390
this is a classical hopping window or

00:17:43,210 --> 00:17:50,000
slightly also swoops oh it's a sliding

00:17:47,390 --> 00:17:50,630
window this is defined here in the group

00:17:50,000 --> 00:17:52,970
by clause

00:17:50,630 --> 00:17:55,910
where you say hop row time because we

00:17:52,970 --> 00:17:58,040
interested in the timestamp here we say

00:17:55,910 --> 00:17:59,780
interval five minutes so we want to

00:17:58,040 --> 00:18:04,250
compute every something every five

00:17:59,780 --> 00:18:05,360
minutes over the last fifteen minutes we

00:18:04,250 --> 00:18:06,890
also interested in the number of

00:18:05,360 --> 00:18:09,260
departing and arriving taxes so that's

00:18:06,890 --> 00:18:12,050
why we put the is start attribute flag

00:18:09,260 --> 00:18:14,780
odds in the group by clause and we are

00:18:12,050 --> 00:18:16,880
grouping on a cell which we computed

00:18:14,780 --> 00:18:19,060
using a user-defined function - cell ID

00:18:16,880 --> 00:18:21,800
which takes the longitude and latitude

00:18:19,060 --> 00:18:25,850
converts that into a basically in a

00:18:21,800 --> 00:18:27,190
discretized grid and using that as a

00:18:25,850 --> 00:18:29,240
grouping key because if it would

00:18:27,190 --> 00:18:31,730
obviously a group on longitude and

00:18:29,240 --> 00:18:35,960
latitude we would not get very

00:18:31,730 --> 00:18:38,390
meaningful results here so so we're

00:18:35,960 --> 00:18:40,640
grouping on the on the only area here on

00:18:38,390 --> 00:18:43,100
the start and on the time using the

00:18:40,640 --> 00:18:46,580
hopping winner definition and then we

00:18:43,100 --> 00:18:50,720
say simply we also select these fields

00:18:46,580 --> 00:18:54,290
and add a current aggregation that then

00:18:50,720 --> 00:18:56,510
basically counts how many taxes arrived

00:18:54,290 --> 00:18:58,880
or departed at each location within the

00:18:56,510 --> 00:19:03,520
last every five minutes in the last 15

00:18:58,880 --> 00:19:06,380
minutes another use case could be two to

00:19:03,520 --> 00:19:09,140
join the start and end rights on the on

00:19:06,380 --> 00:19:11,510
the right ad and then compute the

00:19:09,140 --> 00:19:13,220
average right duration per pick up

00:19:11,510 --> 00:19:16,130
location so basic for each location we

00:19:13,220 --> 00:19:19,460
want to we want to know how long does it

00:19:16,130 --> 00:19:21,620
take the read last in average when when

00:19:19,460 --> 00:19:28,220
it starts at this location and the query

00:19:21,620 --> 00:19:30,190
here would join these two sub queries

00:19:28,220 --> 00:19:33,820
again we're discretizing the

00:19:30,190 --> 00:19:37,220
longitudinal attitudes to two cells here

00:19:33,820 --> 00:19:39,260
in this case we're filtering on a start

00:19:37,220 --> 00:19:43,070
so we get all the start events and we

00:19:39,260 --> 00:19:45,470
join that with all the events where is

00:19:43,070 --> 00:19:46,940
that is false so these are all the end

00:19:45,470 --> 00:19:48,980
events are joining to start events with

00:19:46,940 --> 00:19:52,490
the end events and the interesting part

00:19:48,980 --> 00:19:54,440
is the join clause the joint predicates

00:19:52,490 --> 00:19:57,860
first we turn on the right ID and then

00:19:54,440 --> 00:20:00,620
we have an have we bound the time on

00:19:57,860 --> 00:20:04,400
which we want to join them here where

00:20:00,620 --> 00:20:06,800
they start time the end time should

00:20:04,400 --> 00:20:09,740
between the start time and the start

00:20:06,800 --> 00:20:10,910
time plus one hour so we only joining we

00:20:09,740 --> 00:20:12,410
are assuming here that it takes the

00:20:10,910 --> 00:20:16,970
right would not take longer than one who

00:20:12,410 --> 00:20:18,350
work and finally we can compute the and

00:20:16,970 --> 00:20:21,110
finally we can compute the average time

00:20:18,350 --> 00:20:26,750
by computing the diff between the two x

00:20:21,110 --> 00:20:29,270
times so with these two it's at hand we

00:20:26,750 --> 00:20:31,340
could of course also ingest data from

00:20:29,270 --> 00:20:33,559
from Kafka right into an elastic search

00:20:31,340 --> 00:20:35,630
and then use for instance Cabana to just

00:20:33,559 --> 00:20:38,870
visualize the data because the query

00:20:35,630 --> 00:20:42,140
takes care that in elastic search we

00:20:38,870 --> 00:20:47,360
always get get the data aggregated in

00:20:42,140 --> 00:20:50,750
the in the right way so how can you use

00:20:47,360 --> 00:20:53,960
it well unfortunately until until

00:20:50,750 --> 00:20:56,660
recently all these secret theories had

00:20:53,960 --> 00:21:00,020
to be embedded in Java or Scala code so

00:20:56,660 --> 00:21:05,480
there was no way to simply send a

00:21:00,020 --> 00:21:07,010
secretive link and let it run the run

00:21:05,480 --> 00:21:08,840
this query you basically had to

00:21:07,010 --> 00:21:13,730
implement a Java class or a Scala class

00:21:08,840 --> 00:21:16,270
and then define your query in this class

00:21:13,730 --> 00:21:20,510
send it to a fling cluster for execution

00:21:16,270 --> 00:21:22,490
however this also means that or it

00:21:20,510 --> 00:21:25,220
doesn't does not occur automatically

00:21:22,490 --> 00:21:29,210
mean but the nice thing about this is

00:21:25,220 --> 00:21:30,920
that the table API and sequel are

00:21:29,210 --> 00:21:32,660
tightly integrated with the data stream

00:21:30,920 --> 00:21:35,630
a data set API so whatever other

00:21:32,660 --> 00:21:38,420
libraries you're using can be can be

00:21:35,630 --> 00:21:42,170
used together together with a sequel or

00:21:38,420 --> 00:21:44,770
or all the table API for instance on the

00:21:42,170 --> 00:21:49,730
on the better side you could use the

00:21:44,770 --> 00:21:50,900
sequel API to four-for-four ETL to get

00:21:49,730 --> 00:21:54,080
the data in the right shape and then

00:21:50,900 --> 00:21:55,880
apply a djeli job on this data

00:21:54,080 --> 00:21:58,090
Delia strings graph processing library

00:21:55,880 --> 00:22:01,160
and on the streaming side you could

00:21:58,090 --> 00:22:03,950
first run a certain pattern using fling

00:22:01,160 --> 00:22:11,270
CP library and then evaluate the result

00:22:03,950 --> 00:22:13,940
with seeker however since since flink

00:22:11,270 --> 00:22:17,450
1:5 the community is no focusing a bit

00:22:13,940 --> 00:22:18,080
more on making the API or easier to

00:22:17,450 --> 00:22:22,760
easier

00:22:18,080 --> 00:22:26,269
suppose exposing the api's to two users

00:22:22,760 --> 00:22:28,429
in a more friendly way so we were no

00:22:26,269 --> 00:22:31,190
working also in catalog services on

00:22:28,429 --> 00:22:34,549
better let us offer support for table

00:22:31,190 --> 00:22:36,500
sauce and table things and this CLI

00:22:34,549 --> 00:22:40,159
client that I'm going to show you is not

00:22:36,500 --> 00:22:42,559
the first first version of having having

00:22:40,159 --> 00:22:45,679
it to where you can simply use fling to

00:22:42,559 --> 00:23:01,130
analyze your streaming a batch data all

00:22:45,679 --> 00:23:02,210
right so this is no demo charm so we can

00:23:01,130 --> 00:23:08,120
you read that or should I increase the

00:23:02,210 --> 00:23:14,630
font size later okay just to be sure

00:23:08,120 --> 00:23:16,850
alright so so what I did here is

00:23:14,630 --> 00:23:20,330
basically I started a few docker

00:23:16,850 --> 00:23:24,679
containers a cough car a fling cluster

00:23:20,330 --> 00:23:26,510
and also this fling seal a client and in

00:23:24,679 --> 00:23:32,200
the background there is a threat that

00:23:26,510 --> 00:23:32,200
pushes data into into a Kafka topic so

00:23:32,649 --> 00:23:36,860
we have here exactly the same table that

00:23:35,750 --> 00:23:39,679
I that I've shown you before

00:23:36,860 --> 00:23:42,200
so this taxi rights table with these

00:23:39,679 --> 00:23:45,620
attributes right ID is that attribute

00:23:42,200 --> 00:23:49,240
longitude latitude in row time and we

00:23:45,620 --> 00:23:54,260
can simply spirit that with the most

00:23:49,240 --> 00:24:00,519
simple theory checking out what's in

00:23:54,260 --> 00:24:00,519
there and

00:24:02,520 --> 00:24:08,860
so this is now the data that's as it

00:24:05,440 --> 00:24:11,920
basically flows into the into the cuff

00:24:08,860 --> 00:24:16,930
cut topic and here in the flink flink

00:24:11,920 --> 00:24:21,400
web UI we also see that this query is no

00:24:16,930 --> 00:24:23,710
no no running this is the operator that

00:24:21,400 --> 00:24:27,160
ingests the data from Kafka and this is

00:24:23,710 --> 00:24:34,600
the thing that sends it back to the to

00:24:27,160 --> 00:24:37,860
the sink to the CLI client so we can

00:24:34,600 --> 00:24:37,860
also do a little bit more fancy stuff

00:24:38,260 --> 00:24:44,050
[Music]

00:24:40,710 --> 00:24:46,030
simply aggregating the data no Priscilla

00:24:44,050 --> 00:24:47,710
D is basically something similar as

00:24:46,030 --> 00:24:48,720
we've done before so we say again to

00:24:47,710 --> 00:24:56,190
silletti

00:24:48,720 --> 00:25:05,260
to discretize the data say count star

00:24:56,190 --> 00:25:12,760
from taxi rides and group by to several

00:25:05,260 --> 00:25:15,940
ID longitude and latitude and now we see

00:25:12,760 --> 00:25:19,720
these are the IDS of the cells and now

00:25:15,940 --> 00:25:27,250
we see how we incrementally compute the

00:25:19,720 --> 00:25:31,030
count and again there was there was a

00:25:27,250 --> 00:25:33,070
query started here something that looked

00:25:31,030 --> 00:25:35,650
a little bit more complex here we have

00:25:33,070 --> 00:25:38,830
the source again reading the data this

00:25:35,650 --> 00:25:41,320
is a Hef's partitioning to send the data

00:25:38,830 --> 00:25:43,360
to the right grouping operator a group I

00:25:41,320 --> 00:25:45,010
that computes the count and then again

00:25:43,360 --> 00:25:47,310
the sync which sends the data back to

00:25:45,010 --> 00:25:47,310
the client

00:25:52,990 --> 00:25:59,420
all right so that's a nice tie but can

00:25:57,050 --> 00:26:01,760
you use it for anything serious well

00:25:59,420 --> 00:26:03,260
obviously not this is just a sea light

00:26:01,760 --> 00:26:06,020
line for for playing around you can use

00:26:03,260 --> 00:26:10,480
it to to look into the data in your

00:26:06,020 --> 00:26:10,480
streams but not much more

00:26:10,780 --> 00:26:16,190
therefore the fling community started

00:26:13,040 --> 00:26:20,600
this what we call flip flink improvement

00:26:16,190 --> 00:26:23,150
proposal for query service and we

00:26:20,600 --> 00:26:25,960
envisioned this to be a rest service

00:26:23,150 --> 00:26:28,880
where you could submit queries to and

00:26:25,960 --> 00:26:32,060
submit and manage secret theories so

00:26:28,880 --> 00:26:34,730
select theories theories that directly

00:26:32,060 --> 00:26:38,330
write into a new engine into a new table

00:26:34,730 --> 00:26:40,160
using insert into select and it should

00:26:38,330 --> 00:26:42,340
also be able to serve the results of a

00:26:40,160 --> 00:26:45,230
secret theory back and this is actually

00:26:42,340 --> 00:26:48,040
basically really difficult on difficult

00:26:45,230 --> 00:26:50,680
parts here start it should also

00:26:48,040 --> 00:26:54,700
integrate with turbo catalogs like

00:26:50,680 --> 00:26:57,050
educator log or schema registries and

00:26:54,700 --> 00:26:59,480
the use cases for for such a service

00:26:57,050 --> 00:27:02,480
that way you can basically send Cebu

00:26:59,480 --> 00:27:05,240
queries to using rest and then either

00:27:02,480 --> 00:27:07,670
directly receive the data or right into

00:27:05,240 --> 00:27:11,090
write it into a enter into another

00:27:07,670 --> 00:27:13,100
storage system would be either data

00:27:11,090 --> 00:27:15,830
exploration using notebooks like Apache

00:27:13,100 --> 00:27:20,900
a Zeppelin you can get real-time access

00:27:15,830 --> 00:27:23,810
to data in your application and also be

00:27:20,900 --> 00:27:27,290
able to easily route data from one one

00:27:23,810 --> 00:27:31,910
topic to another or have an easy way to

00:27:27,290 --> 00:27:34,730
define ETL ETL pipelines the challenge

00:27:31,910 --> 00:27:36,260
here is basically the the serving of

00:27:34,730 --> 00:27:38,620
these dynamic tablets the tablets that

00:27:36,260 --> 00:27:41,840
are dynamically updating right so

00:27:38,620 --> 00:27:45,080
because unbounded input also means

00:27:41,840 --> 00:27:48,770
unbothered results and whereas in the in

00:27:45,080 --> 00:27:50,840
the batch case serving bounded results

00:27:48,770 --> 00:27:52,580
is not that hard right you have the

00:27:50,840 --> 00:27:54,320
results you send it back and then you're

00:27:52,580 --> 00:27:56,840
done of course this can also be very

00:27:54,320 --> 00:27:58,970
large but in principle if you're if

00:27:56,840 --> 00:28:01,550
you're working on a stream they are they

00:27:58,970 --> 00:28:04,630
actually unbounded and depending on the

00:28:01,550 --> 00:28:06,590
query returning results can be either

00:28:04,630 --> 00:28:09,110
very hard or not

00:28:06,590 --> 00:28:11,049
that hard if you look at the career on

00:28:09,110 --> 00:28:13,669
the left-hand side which is a simple

00:28:11,049 --> 00:28:16,730
simple select from where query with a

00:28:13,669 --> 00:28:19,940
simple filter this query if applied on

00:28:16,730 --> 00:28:23,750
an on a stream with an append table what

00:28:19,940 --> 00:28:26,029
result also in an append result table

00:28:23,750 --> 00:28:28,210
right so you would only get new records

00:28:26,029 --> 00:28:30,980
appended to the result and in this case

00:28:28,210 --> 00:28:33,289
the results rows that you've emitted

00:28:30,980 --> 00:28:36,679
will never change and therefore you can

00:28:33,289 --> 00:28:38,929
the the challenges in basically

00:28:36,679 --> 00:28:41,299
buffering the records until they are

00:28:38,929 --> 00:28:43,580
consumed from the read from the consumer

00:28:41,299 --> 00:28:45,110
whereas in the other hand side like this

00:28:43,580 --> 00:28:48,529
is the query that we had in our example

00:28:45,110 --> 00:28:50,149
a group a query with a congregation we

00:28:48,529 --> 00:28:53,929
have a table that is continuously

00:28:50,149 --> 00:28:55,880
updating and in this case we somehow

00:28:53,929 --> 00:28:58,669
need to be able to tell the consumer

00:28:55,880 --> 00:29:02,270
downstream that that the results did

00:28:58,669 --> 00:29:04,610
change right and the result of it also

00:29:02,270 --> 00:29:07,100
needs to be maintained somewhere so

00:29:04,610 --> 00:29:08,600
serving these results in the online

00:29:07,100 --> 00:29:12,950
streaming theories on continuous queries

00:29:08,600 --> 00:29:15,200
is a little bit difficult so the design

00:29:12,950 --> 00:29:17,419
that we envision for this query service

00:29:15,200 --> 00:29:20,840
looks looks like this so we have an

00:29:17,419 --> 00:29:23,210
application which uses rest to

00:29:20,840 --> 00:29:25,309
communicate with the query servers which

00:29:23,210 --> 00:29:28,809
is in the middle the crew service again

00:29:25,309 --> 00:29:32,990
has a rest interface it has a catalog to

00:29:28,809 --> 00:29:36,830
to which it that can be connected to an

00:29:32,990 --> 00:29:39,380
external catalog it also has obviously

00:29:36,830 --> 00:29:41,809
the optimizer to to optimize Nick Fury

00:29:39,380 --> 00:29:44,240
and a component that we call here result

00:29:41,809 --> 00:29:46,640
server when you have a query the

00:29:44,240 --> 00:29:50,630
application would submit the query via

00:29:46,640 --> 00:29:53,270
rest today to the queue service the

00:29:50,630 --> 00:29:55,039
optimizer would then compile the query

00:29:53,270 --> 00:29:56,570
into a streaming job submitted with the

00:29:55,039 --> 00:29:59,360
Flint cluster it would start the query

00:29:56,570 --> 00:30:01,600
on the fling cluster this is typically

00:29:59,360 --> 00:30:04,370
this is a stateful Kaveri stateful

00:30:01,600 --> 00:30:05,929
stifled screaming drop so it would read

00:30:04,370 --> 00:30:09,020
the data from an event log or a database

00:30:05,929 --> 00:30:12,110
or ever and then write the data again

00:30:09,020 --> 00:30:13,610
into an event log or database and from

00:30:12,110 --> 00:30:17,170
there on the curio service which fetch

00:30:13,610 --> 00:30:19,480
it and serve it back to the application

00:30:17,170 --> 00:30:21,640
so in this case all the results are

00:30:19,480 --> 00:30:25,390
served by arrest so this is a very nice

00:30:21,640 --> 00:30:28,330
property if you're in certain cluster

00:30:25,390 --> 00:30:30,640
environments however in this case all

00:30:28,330 --> 00:30:32,680
the data flows through the results

00:30:30,640 --> 00:30:33,970
server so this might in this case the

00:30:32,680 --> 00:30:36,190
query service might become a bottleneck

00:30:33,970 --> 00:30:39,190
right depending on how many queries you

00:30:36,190 --> 00:30:41,020
start or run reading all the data or the

00:30:39,190 --> 00:30:42,400
result data from a Kafka topic or from a

00:30:41,020 --> 00:30:46,770
database and serving it back to an

00:30:42,400 --> 00:30:49,600
application might not work with so well

00:30:46,770 --> 00:30:52,420
because of that we also thought about a

00:30:49,600 --> 00:30:55,630
solution to this to this problem again

00:30:52,420 --> 00:30:58,180
we could submit a query to the query

00:30:55,630 --> 00:30:59,260
service the query service submits the

00:30:58,180 --> 00:31:01,480
job to the fling cluster on the

00:30:59,260 --> 00:31:04,150
left-hand side the data again flows in

00:31:01,480 --> 00:31:05,950
some kind of a storage system but

00:31:04,150 --> 00:31:08,410
instead of returning the result directly

00:31:05,950 --> 00:31:11,980
from the query service we return a

00:31:08,410 --> 00:31:13,810
certain result handle and then there is

00:31:11,980 --> 00:31:16,590
a serving life serving library in the

00:31:13,810 --> 00:31:19,330
application that could then connect to

00:31:16,590 --> 00:31:21,580
to the storage system and fetch the data

00:31:19,330 --> 00:31:25,450
from there and serve it directly within

00:31:21,580 --> 00:31:27,910
the application so all of these design

00:31:25,450 --> 00:31:31,240
decisions are not final yet so if you

00:31:27,910 --> 00:31:33,120
have have a good idea or a certain

00:31:31,240 --> 00:31:37,450
feature that you would like to have for

00:31:33,120 --> 00:31:42,220
security service let us know as I said

00:31:37,450 --> 00:31:44,350
the effort is called flip 24 and we're

00:31:42,220 --> 00:31:45,120
happy to hear your your ideas about this

00:31:44,350 --> 00:31:50,590
feature

00:31:45,120 --> 00:31:55,270
so to summarize unifying pattern stream

00:31:50,590 --> 00:31:57,970
processing is important for our for

00:31:55,270 --> 00:32:01,240
various reasons first of all being able

00:31:57,970 --> 00:32:02,530
to use the same query on streaming and

00:32:01,240 --> 00:32:04,210
batch data makes it makes it very

00:32:02,530 --> 00:32:06,400
portable you can just if something goes

00:32:04,210 --> 00:32:07,900
wrong run it on on historic batch data

00:32:06,400 --> 00:32:10,330
as well

00:32:07,900 --> 00:32:11,500
fling sequel solves many of the

00:32:10,330 --> 00:32:13,390
streaming and bad use cases

00:32:11,500 --> 00:32:17,160
it runs in production at Alibaba uber

00:32:13,390 --> 00:32:19,630
and in others so these companies build

00:32:17,160 --> 00:32:22,870
internal platforms which are powered by

00:32:19,630 --> 00:32:25,300
flink sequel and the community is

00:32:22,870 --> 00:32:28,090
working currently working on improving

00:32:25,300 --> 00:32:30,600
the the user facing side of these of

00:32:28,090 --> 00:32:30,600
these features

00:32:30,720 --> 00:32:34,080
I'd also like to mention there is the

00:32:32,640 --> 00:32:35,520
Fink forward conference happening in

00:32:34,080 --> 00:32:38,880
September begin of September

00:32:35,520 --> 00:32:42,559
exactly here at Cooper aye if you're

00:32:38,880 --> 00:32:45,480
interested in that sign up early early

00:32:42,559 --> 00:32:49,440
bird prices are still available until

00:32:45,480 --> 00:32:50,070
June 22nd and with that I'd like to

00:32:49,440 --> 00:32:52,620
thank you

00:32:50,070 --> 00:32:54,240
small promotion of my book here as well

00:32:52,620 --> 00:33:05,610
you can get it on early release of

00:32:54,240 --> 00:33:08,779
O'Reilly already and yeah thank you all

00:33:05,610 --> 00:33:08,779
the questions let's take them

00:33:15,290 --> 00:33:22,470
hi from what I understand when you do

00:33:19,770 --> 00:33:26,060
like grouping by something right and

00:33:22,470 --> 00:33:29,700
then you need to maintain estates of

00:33:26,060 --> 00:33:32,040
every aggregate so some aggregates like

00:33:29,700 --> 00:33:33,990
averaging to maintain like intermediary

00:33:32,040 --> 00:33:37,290
aggregates like denominator and

00:33:33,990 --> 00:33:42,960
numerator basically can you access those

00:33:37,290 --> 00:33:44,790
as well as a user what kind of a so

00:33:42,960 --> 00:33:48,960
let's say that you're calculating like

00:33:44,790 --> 00:33:50,900
arithmetic mean yeah so you from I

00:33:48,960 --> 00:33:53,010
understand you have to maintain like

00:33:50,900 --> 00:33:56,100
intermediary never for the denominator

00:33:53,010 --> 00:33:59,280
and numerator right can you access those

00:33:56,100 --> 00:34:01,610
as the user as well you can you can

00:33:59,280 --> 00:34:04,440
certainly do that it's not very

00:34:01,610 --> 00:34:08,250
efficient let's put it that way so the

00:34:04,440 --> 00:34:09,629
way this works in in in in flink it

00:34:08,250 --> 00:34:11,700
exactly as I said you have to maintain

00:34:09,629 --> 00:34:15,750
all the state and since this is an

00:34:11,700 --> 00:34:20,730
incremental incremental operation where

00:34:15,750 --> 00:34:22,379
you can also potentially have have need

00:34:20,730 --> 00:34:25,050
to remove something from an aggregation

00:34:22,379 --> 00:34:27,629
again you need to maintain and basically

00:34:25,050 --> 00:34:29,159
all the values that is true like for for

00:34:27,629 --> 00:34:32,310
if you want to want to compute the mean

00:34:29,159 --> 00:34:34,169
you have to put all the values

00:34:32,310 --> 00:34:37,260
interstate this isn't very expensive

00:34:34,169 --> 00:34:38,879
operation in this case yes but it's it's

00:34:37,260 --> 00:34:42,290
possible you can do that but it's

00:34:38,879 --> 00:34:42,290
consumes a lot of state

00:34:55,740 --> 00:35:07,260
hi for the user interfacing have you

00:34:59,910 --> 00:35:07,260
given any thoughts of having a driver

00:35:11,190 --> 00:35:17,940
the yes we thought about that the

00:35:14,560 --> 00:35:20,770
problem with the JDBC or having Mike

00:35:17,940 --> 00:35:22,780
so the problem JDBC is that it works

00:35:20,770 --> 00:35:24,550
better for for bounded datasets right as

00:35:22,780 --> 00:35:27,280
soon as you have to update the result

00:35:24,550 --> 00:35:29,380
this doesn't work so well anymore also I

00:35:27,280 --> 00:35:31,450
mean undaunted returning unlike a table

00:35:29,380 --> 00:35:33,070
that is unbounded upend might work

00:35:31,450 --> 00:35:34,750
because you just have an infinite

00:35:33,070 --> 00:35:36,760
results that we can just fetch data from

00:35:34,750 --> 00:35:39,400
that would certainly work that that we

00:35:36,760 --> 00:35:42,400
could do but it would fail for a certain

00:35:39,400 --> 00:35:46,119
theories that do for instance like this

00:35:42,400 --> 00:35:49,030
group I user count theory at that point

00:35:46,119 --> 00:35:50,500
JDBC is not is not working anymore and

00:35:49,030 --> 00:35:53,670
we would have to come up with something

00:35:50,500 --> 00:35:53,670
on our own yeah

00:35:59,670 --> 00:36:05,200
I have one small problem with it which

00:36:02,680 --> 00:36:08,619
is that I think sequel sucks

00:36:05,200 --> 00:36:11,220
is it just my I'm at fault or the sequel

00:36:08,619 --> 00:36:13,900
have fundamental flaws in this world

00:36:11,220 --> 00:36:16,570
yeah so I mean sequel has its problems

00:36:13,900 --> 00:36:22,510
yeah that's where we also have the table

00:36:16,570 --> 00:36:26,580
API so which is like a more embedded way

00:36:22,510 --> 00:36:31,390
of specifying your your your queries so

00:36:26,580 --> 00:36:33,580
both api's are basically our equivalent

00:36:31,390 --> 00:36:35,140
since since we are there they're going

00:36:33,580 --> 00:36:38,590
through the same translation paths right

00:36:35,140 --> 00:36:42,369
so we have if you know such a fan of

00:36:38,590 --> 00:36:44,320
like specifying the queries as a as a

00:36:42,369 --> 00:36:46,210
string and putting them into an

00:36:44,320 --> 00:36:49,390
application which I also think it's not

00:36:46,210 --> 00:36:53,230
the nice way to it to do to do it you

00:36:49,390 --> 00:36:55,420
can also use as I said a table of the

00:36:53,230 --> 00:36:59,500
the tabular API and then compose it with

00:36:55,420 --> 00:37:00,470
methods in an incremental way and when

00:36:59,500 --> 00:37:02,450
you

00:37:00,470 --> 00:37:08,060
if we look at the example that I had

00:37:02,450 --> 00:37:10,720
here it also solves this like what some

00:37:08,060 --> 00:37:13,700
people made where is it

00:37:10,720 --> 00:37:17,450
consider as an issue with sequel where

00:37:13,700 --> 00:37:20,480
you have to read basically did the query

00:37:17,450 --> 00:37:21,710
from the middle down and then up so

00:37:20,480 --> 00:37:23,630
parsing the cure is not that simple

00:37:21,710 --> 00:37:25,790
where's in this case you can basically

00:37:23,630 --> 00:37:27,920
start from the top you say okay I scan

00:37:25,790 --> 00:37:29,840
my data I group it under then I project

00:37:27,920 --> 00:37:30,920
it instead of hey whereas maybe they

00:37:29,840 --> 00:37:32,660
come from or it comes from the middle

00:37:30,920 --> 00:37:34,510
and then okay I do a group I and then

00:37:32,660 --> 00:37:42,920
I'm jump to the top to see what I get

00:37:34,510 --> 00:37:45,020
yeah kind of following up on the past

00:37:42,920 --> 00:37:57,410
two questions you showed the sequel

00:37:45,020 --> 00:37:58,820
interface and the rest interface are you

00:37:57,410 --> 00:38:03,040
working with any of these tablished

00:37:58,820 --> 00:38:05,560
players for for dashboards for BI tools

00:38:03,040 --> 00:38:09,290
not yet so we're not there yet

00:38:05,560 --> 00:38:20,089
Oh simple might suck but it's a

00:38:09,290 --> 00:38:23,930
significant market space and yeah no

00:38:20,089 --> 00:38:28,869
we're not yet the the idea here would be

00:38:23,930 --> 00:38:31,609
to probably integrate here at that point

00:38:28,869 --> 00:38:34,310
when you basically return a result

00:38:31,609 --> 00:38:36,230
handle and then have basically have a

00:38:34,310 --> 00:38:38,540
have a query that consumes from Kafka

00:38:36,230 --> 00:38:41,030
writes the data into like a relational

00:38:38,540 --> 00:38:44,150
database and then when you return the

00:38:41,030 --> 00:38:45,950
handle you basically say hey this is the

00:38:44,150 --> 00:38:48,710
table this is the JDBC connection on

00:38:45,950 --> 00:38:51,230
which you can fetch the data and then a

00:38:48,710 --> 00:38:53,420
- what solution could use this ready BC

00:38:51,230 --> 00:38:57,050
connection to interactively run queries

00:38:53,420 --> 00:38:58,780
on the data that is updated by takfeer

00:38:57,050 --> 00:39:03,050
why they where the continuous query runs

00:38:58,780 --> 00:39:05,540
make sense what what I see many

00:39:03,050 --> 00:39:07,520
applications are now not producing and

00:39:05,540 --> 00:39:09,890
consuming applications but there are

00:39:07,520 --> 00:39:11,750
applications that put data inside and

00:39:09,890 --> 00:39:12,910
then there are others that read so yeah

00:39:11,750 --> 00:39:14,770
not

00:39:12,910 --> 00:39:17,890
what has been done by a JDBC both ways

00:39:14,770 --> 00:39:23,100
is now done decoupled sense to think

00:39:17,890 --> 00:39:32,350
about how to take any more questions

00:39:23,100 --> 00:39:34,990
there's one left middle hi what I don't

00:39:32,350 --> 00:39:38,410
really understand so if fling is fed by

00:39:34,990 --> 00:39:41,620
a stream of data right and this queries

00:39:38,410 --> 00:39:43,830
can be submitted at hoc how do you know

00:39:41,620 --> 00:39:46,900
how much data you have to preserve in

00:39:43,830 --> 00:39:48,880
flink because it's very different if i'm

00:39:46,900 --> 00:39:51,040
asking you only about the last hour and

00:39:48,880 --> 00:39:53,590
you can drop a lot of data out of the

00:39:51,040 --> 00:39:56,440
state but if the next user asks you like

00:39:53,590 --> 00:39:58,630
for account since the dawn of the data

00:39:56,440 --> 00:40:02,260
then you already have lost all of it

00:39:58,630 --> 00:40:06,130
right yeah so this kind of depends on

00:40:02,260 --> 00:40:11,140
the on the way that a table would be

00:40:06,130 --> 00:40:12,430
defined in fling so usually if you're

00:40:11,140 --> 00:40:15,310
for instance if you take the example of

00:40:12,430 --> 00:40:17,320
a Kafka topic right the data is in Kafka

00:40:15,310 --> 00:40:19,450
and then when you when you define the

00:40:17,320 --> 00:40:22,780
query you can say hey I want this table

00:40:19,450 --> 00:40:25,660
to be represented by reading from the

00:40:22,780 --> 00:40:29,320
beginning or you say hey I want to start

00:40:25,660 --> 00:40:33,940
at the offset from from the current

00:40:29,320 --> 00:40:35,650
offset so to say if you then the the

00:40:33,940 --> 00:40:37,210
proper way of restricting your data then

00:40:35,650 --> 00:40:38,830
if you say okay one with the data from

00:40:37,210 --> 00:40:41,970
the beginning from the early ons or at

00:40:38,830 --> 00:40:43,840
least as long as I have it in Kafka the

00:40:41,970 --> 00:40:48,220
then you don't have to do anything

00:40:43,840 --> 00:40:50,320
because the table is defined as starting

00:40:48,220 --> 00:40:51,910
from the current offset so it goes on

00:40:50,320 --> 00:40:54,520
from there and if you say hey I want

00:40:51,910 --> 00:40:57,160
only want the first day from from the

00:40:54,520 --> 00:40:58,660
first record the first day then you

00:40:57,160 --> 00:41:02,020
basically have to set something like

00:40:58,660 --> 00:41:04,420
like a predicate where my row time is

00:41:02,020 --> 00:41:05,560
smaller at this timestamp and then it

00:41:04,420 --> 00:41:09,550
would automatically it cut it off that

00:41:05,560 --> 00:41:13,240
that point all right let's take one last

00:41:09,550 --> 00:41:19,360
question from here thank you for your

00:41:13,240 --> 00:41:22,600
talk the I honest empathy the nose on

00:41:19,360 --> 00:41:25,180
the bound bounded streams must be

00:41:22,600 --> 00:41:28,700
different but

00:41:25,180 --> 00:41:31,700
the meaning of a window in secret has a

00:41:28,700 --> 00:41:36,620
very specific meaning which is bounded

00:41:31,700 --> 00:41:38,210
bound to bounded set and so but now it

00:41:36,620 --> 00:41:42,410
seems to me that if you use the same

00:41:38,210 --> 00:41:45,850
term on unbounded streams that the

00:41:42,410 --> 00:41:49,100
meaning a little bit changes why did you

00:41:45,850 --> 00:41:57,170
choose to use the same term instead of

00:41:49,100 --> 00:41:59,300
another word yeah yeah so we we're kinda

00:41:57,170 --> 00:42:00,770
like distinguishing so you're coming

00:41:59,300 --> 00:42:03,230
from the from the secret windows that

00:42:00,770 --> 00:42:05,930
you use like a nova cross right so yeah

00:42:03,230 --> 00:42:08,060
we we tend to distinguish them by over

00:42:05,930 --> 00:42:12,650
windows and group by windows basically

00:42:08,060 --> 00:42:15,890
to specify at which at which place there

00:42:12,650 --> 00:42:17,930
are used if you if you use like a group

00:42:15,890 --> 00:42:20,920
by window which is one of these Tumbo

00:42:17,930 --> 00:42:23,630
hopper session you basically this is a

00:42:20,920 --> 00:42:25,430
constraint how you group your data so

00:42:23,630 --> 00:42:28,130
it's like actually a grouping we use

00:42:25,430 --> 00:42:32,840
like a nova window we also support those

00:42:28,130 --> 00:42:35,450
or a subset of those then you define the

00:42:32,840 --> 00:42:37,100
window in the over clause or in the

00:42:35,450 --> 00:42:41,080
window close and use the reference in

00:42:37,100 --> 00:42:45,650
the over so we support both of them but

00:42:41,080 --> 00:42:47,330
yeah on the other side we these this

00:42:45,650 --> 00:42:48,710
group are windows or a week we call them

00:42:47,330 --> 00:42:52,010
group a window size or group I witness a

00:42:48,710 --> 00:42:53,440
library like what people coming from the

00:42:52,010 --> 00:42:56,750
stream processing space usually

00:42:53,440 --> 00:42:58,880
understand as windows so it's kind of

00:42:56,750 --> 00:43:02,600
like a like a trade-off like to get both

00:42:58,880 --> 00:43:04,190
communities somehow in on the same

00:43:02,600 --> 00:43:06,080
semantics and that's why we call them

00:43:04,190 --> 00:43:10,750
the one windows group I witness and the

00:43:06,080 --> 00:43:10,750
others over and us hope that makes sense

00:43:13,270 --> 00:43:17,159
[Applause]

00:43:14,000 --> 00:43:17,159

YouTube URL: https://www.youtube.com/watch?v=qo7tKGuib8o


