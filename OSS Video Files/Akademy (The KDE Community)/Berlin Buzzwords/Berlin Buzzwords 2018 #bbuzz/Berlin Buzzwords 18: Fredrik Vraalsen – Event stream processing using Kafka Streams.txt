Title: Berlin Buzzwords 18: Fredrik Vraalsen – Event stream processing using Kafka Streams
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Further information: https://berlinbuzzwords.de/18/session/event-stream-processing-using-kafka-streams

This workshop will give you hands-on experience using Kafka Streams to solve a variety of event stream processing problems. The examples and exercises are based on real-world usage from our stream processing platform in Schibsted, used to process over 800 million incoming events daily from users across the globe. We will cover topics ranging from the basics like filtering and transforming events, to how you can use Kafka Streams for data routing, aggregating events, and enriching and joining event streams. To solve the exercises we'll be using Java 8 and/or Scala.

If you like this video why not share it ▶ https://youtu.be/OwA_gpWt3xA

If you're new, why not subscribe to our channel!

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,770 --> 00:00:10,139
all right thank you welcome everyone to

00:00:07,319 --> 00:00:12,929
this workshop as I saw I already had

00:00:10,139 --> 00:00:15,120
this slide up on some getting started

00:00:12,929 --> 00:00:16,770
notes if you want to follow along with

00:00:15,120 --> 00:00:18,869
the exercises or do them on your own

00:00:16,770 --> 00:00:21,210
later and you can clone them from this

00:00:18,869 --> 00:00:23,699
repository normally I've done this

00:00:21,210 --> 00:00:25,619
workshop as more of a half day long

00:00:23,699 --> 00:00:29,759
thing so I'm not sure we'll see if we

00:00:25,619 --> 00:00:31,349
get much time for the exercises but if

00:00:29,759 --> 00:00:32,820
someone wants to stay after the talk and

00:00:31,349 --> 00:00:33,960
work on that together with me though

00:00:32,820 --> 00:00:36,180
that's fine as well we can find

00:00:33,960 --> 00:00:38,159
somewhere to to sit down hopefully we'll

00:00:36,180 --> 00:00:41,489
have some time to walk through some of

00:00:38,159 --> 00:00:43,140
them at least the github URL will be at

00:00:41,489 --> 00:00:46,799
the bottom of the slides moving forward

00:00:43,140 --> 00:00:51,030
so quick agenda for today just a quick

00:00:46,799 --> 00:00:53,040
introduction and then go into a little

00:00:51,030 --> 00:00:55,559
bit about why you want to do stream

00:00:53,040 --> 00:00:57,930
processing we'll dive into a bit more on

00:00:55,559 --> 00:00:59,610
Kafka and Kafka streams and then we'll

00:00:57,930 --> 00:01:03,210
go into more detail on the Kafka streams

00:00:59,610 --> 00:01:04,739
API and how to develop using that we'll

00:01:03,210 --> 00:01:07,140
cover some of the basics like filtering

00:01:04,739 --> 00:01:08,310
and transforming your data and routing

00:01:07,140 --> 00:01:10,859
it to different places and then we'll

00:01:08,310 --> 00:01:13,850
look at also some examples of

00:01:10,859 --> 00:01:17,670
aggregations and time handling of time

00:01:13,850 --> 00:01:19,920
all right so first of all Who am I why

00:01:17,670 --> 00:01:20,969
am I here to give this talk so my name

00:01:19,920 --> 00:01:23,600
is Frederick Wilson

00:01:20,969 --> 00:01:27,420
I work as a data engineer in shipstead

00:01:23,600 --> 00:01:28,439
in the Data Platform team there where we

00:01:27,420 --> 00:01:31,320
work with all these kind of cool

00:01:28,439 --> 00:01:33,240
technologies a lot of stream processing

00:01:31,320 --> 00:01:37,619
using Kafka we also use spark for

00:01:33,240 --> 00:01:39,119
example for our batch processing most of

00:01:37,619 --> 00:01:41,700
you are probably not heard of shipstead

00:01:39,119 --> 00:01:43,890
but it's it's a company that owns a lot

00:01:41,700 --> 00:01:45,450
of classified sites and media sites

00:01:43,890 --> 00:01:49,649
across Europe and other parts of the

00:01:45,450 --> 00:01:52,259
world as well so for example leboncoin

00:01:49,649 --> 00:01:54,840
in France block it and Finland Sweden

00:01:52,259 --> 00:01:56,759
Norway are some of the big ones but we

00:01:54,840 --> 00:01:59,429
also others are in other parts of the

00:01:56,759 --> 00:02:01,109
world so these are kind of three main

00:01:59,429 --> 00:02:03,030
business areas okay all right we also

00:02:01,109 --> 00:02:06,240
have tech hubs in multiple places in

00:02:03,030 --> 00:02:12,870
Europe we have about 2,000 developers

00:02:06,240 --> 00:02:14,730
spread across mainly these sites so in

00:02:12,870 --> 00:02:17,010
my team in the data platform team we are

00:02:14,730 --> 00:02:19,349
sort of responsible for gathering

00:02:17,010 --> 00:02:20,730
event from all of these different sites

00:02:19,349 --> 00:02:25,590
and making them available to our

00:02:20,730 --> 00:02:27,329
analysis teams so during the course of a

00:02:25,590 --> 00:02:30,030
week you can see this this kind of

00:02:27,329 --> 00:02:32,099
pattern that we have so we receive

00:02:30,030 --> 00:02:34,110
around 800 million events per day or

00:02:32,099 --> 00:02:37,500
clickstream events mainly but also other

00:02:34,110 --> 00:02:39,390
types of events from around 30 to 40

00:02:37,500 --> 00:02:41,849
sites across the world as you can see

00:02:39,390 --> 00:02:44,040
the load varies a lot across the day so

00:02:41,849 --> 00:02:48,780
we need to be able to scale to vary

00:02:44,040 --> 00:02:50,010
traffic by the way feel free to stop me

00:02:48,780 --> 00:02:52,290
if you have any questions along the way

00:02:50,010 --> 00:02:55,200
and just raise your hand I'll try to to

00:02:52,290 --> 00:02:58,500
catch it so briefly going through our

00:02:55,200 --> 00:03:01,609
data pipeline like I said we receive

00:02:58,500 --> 00:03:06,450
events from various sources from our

00:03:01,609 --> 00:03:08,730
apps on iOS and Android our web sites of

00:03:06,450 --> 00:03:12,329
course through our own tracking solution

00:03:08,730 --> 00:03:13,920
and from a set of back-end systems or

00:03:12,329 --> 00:03:16,769
components as well like for instance

00:03:13,920 --> 00:03:21,959
messaging events payment events and so

00:03:16,769 --> 00:03:24,329
on all of these go into our micro

00:03:21,959 --> 00:03:27,209
service named collector aptly named

00:03:24,329 --> 00:03:29,280
where we then push all those events onto

00:03:27,209 --> 00:03:33,599
amazon Kinesis for that's kind of our

00:03:29,280 --> 00:03:35,160
first stable storage for the events from

00:03:33,599 --> 00:03:37,530
there we pick up the events and pipe

00:03:35,160 --> 00:03:40,470
them into our two main pipelines so one

00:03:37,530 --> 00:03:43,290
going to s3 in and into our batch

00:03:40,470 --> 00:03:44,879
pipelines where people where we sort of

00:03:43,290 --> 00:03:46,949
gather all the events into our early

00:03:44,879 --> 00:03:49,199
data sets and people can do various

00:03:46,949 --> 00:03:50,790
analysis jobs on top of those and then

00:03:49,199 --> 00:03:53,150
we have our streaming pipeline which is

00:03:50,790 --> 00:03:56,819
what we'll be talking about today

00:03:53,150 --> 00:03:58,590
using Kafka and Kafka streams our team

00:03:56,819 --> 00:04:01,019
is mainly as I said responsible for

00:03:58,590 --> 00:04:02,879
making the data available to the

00:04:01,019 --> 00:04:05,519
different consumers so what we use kafka

00:04:02,879 --> 00:04:07,409
streams for in our team is mainly to

00:04:05,519 --> 00:04:10,199
sort of distribute the data split it

00:04:07,409 --> 00:04:12,449
into different data sets in here we get

00:04:10,199 --> 00:04:14,040
everything into one event firehose so we

00:04:12,449 --> 00:04:16,169
need a sort of d-max it again in two

00:04:14,040 --> 00:04:18,239
different types based on the event type

00:04:16,169 --> 00:04:20,820
and what site comes from and so on for

00:04:18,239 --> 00:04:22,810
example then we send it to various

00:04:20,820 --> 00:04:28,210
downstream consumers

00:04:22,810 --> 00:04:30,310
we support the main services in Amazon

00:04:28,210 --> 00:04:33,910
sending it to classes an s3 or ask us

00:04:30,310 --> 00:04:35,830
for messaging based handling or various

00:04:33,910 --> 00:04:38,169
third-party tools like amplitude for

00:04:35,830 --> 00:04:42,750
example which is an analytics tool that

00:04:38,169 --> 00:04:42,750
we use for some of the analysis purposes

00:04:43,530 --> 00:04:50,320
right so in addition to our the team the

00:04:48,520 --> 00:04:52,590
pipeline of our team has built in terms

00:04:50,320 --> 00:04:55,720
of doing this processing for the stream

00:04:52,590 --> 00:04:57,100
stream processing we also have other

00:04:55,720 --> 00:04:59,800
teams that have built on top of our

00:04:57,100 --> 00:05:02,950
platform using kafka streams so one a

00:04:59,800 --> 00:05:06,220
couple of examples are one team has

00:05:02,950 --> 00:05:08,140
built this data quality analysis tool

00:05:06,220 --> 00:05:10,450
that we use for our events to ensure

00:05:08,140 --> 00:05:12,820
that the events contain the right kind

00:05:10,450 --> 00:05:16,240
of data that are useful for the

00:05:12,820 --> 00:05:17,950
downstream analysis teams checking the

00:05:16,240 --> 00:05:21,010
formats of the events the content and so

00:05:17,950 --> 00:05:23,530
on and making this so we can get this

00:05:21,010 --> 00:05:25,540
closed loop in terms of the developers

00:05:23,530 --> 00:05:27,130
on the sides can look at these - person

00:05:25,540 --> 00:05:32,050
see whether the events they sent to us

00:05:27,130 --> 00:05:34,120
are ok as expected as I mentioned we've

00:05:32,050 --> 00:05:36,370
done integration with an analytics tool

00:05:34,120 --> 00:05:39,789
so we have our own tracker solution for

00:05:36,370 --> 00:05:44,260
for the web and our apps name pulse and

00:05:39,789 --> 00:05:46,360
and so instead of having to use many

00:05:44,260 --> 00:05:48,910
different trackers on the sites we try

00:05:46,360 --> 00:05:51,340
to then instead send the events from

00:05:48,910 --> 00:05:53,440
from our tracker into multiple analysis

00:05:51,340 --> 00:05:56,229
analytics tools doing transformations

00:05:53,440 --> 00:05:59,500
along the way into the format that they

00:05:56,229 --> 00:06:01,930
require and so on so this is allowed us

00:05:59,500 --> 00:06:04,479
to easily set up new integrations with

00:06:01,930 --> 00:06:06,280
third-party tools sort of negotiated

00:06:04,479 --> 00:06:10,260
with amplitude for example and how to

00:06:06,280 --> 00:06:10,260
how to send events into their platform

00:06:10,289 --> 00:06:14,740
this is allowed us to onboard new sites

00:06:13,419 --> 00:06:17,919
and components into these analytics

00:06:14,740 --> 00:06:19,060
tools very easily we also created a

00:06:17,919 --> 00:06:21,880
number of other data-driven applications

00:06:19,060 --> 00:06:25,150
on top of this ranging from sort of

00:06:21,880 --> 00:06:27,789
experiments experimentation tools like

00:06:25,150 --> 00:06:29,590
for a/b testing for example maybe you

00:06:27,789 --> 00:06:31,860
want to talk about my colleague

00:06:29,590 --> 00:06:34,870
yesterday on geoip

00:06:31,860 --> 00:06:35,970
enrichment of the events so that's one

00:06:34,870 --> 00:06:39,240
of the things we were built on

00:06:35,970 --> 00:06:43,470
of this as well we have image feature

00:06:39,240 --> 00:06:44,940
ization applications that take images

00:06:43,470 --> 00:06:46,260
are uploaded for example to the

00:06:44,940 --> 00:06:51,180
classified sites and do them image

00:06:46,260 --> 00:06:52,710
analysis on those and post this using

00:06:51,180 --> 00:06:56,030
events coming from from the streaming

00:06:52,710 --> 00:06:58,440
pipeline another another example is

00:06:56,030 --> 00:07:00,450
messaging intent analysis so you can

00:06:58,440 --> 00:07:02,910
tell based on a conversation between

00:07:00,450 --> 00:07:06,150
users what their intent is and then

00:07:02,910 --> 00:07:07,950
offer sort of specialized UI for example

00:07:06,150 --> 00:07:10,610
for handling some of the replies or

00:07:07,950 --> 00:07:15,720
something like that

00:07:10,610 --> 00:07:18,870
alright so why you want to work with

00:07:15,720 --> 00:07:21,080
streaming well I mean typically you want

00:07:18,870 --> 00:07:26,400
to use stream processing systems for

00:07:21,080 --> 00:07:30,450
getting more real-time analysis results

00:07:26,400 --> 00:07:31,410
right or in the lower latency maybe you

00:07:30,450 --> 00:07:35,300
are familiar with the lambda

00:07:31,410 --> 00:07:37,950
architecture where this is sort of

00:07:35,300 --> 00:07:40,650
proposed - a couple of years ago where

00:07:37,950 --> 00:07:42,180
you have the idea at the time was that

00:07:40,650 --> 00:07:44,580
you had you typically had your batch

00:07:42,180 --> 00:07:46,440
jobs processing events every hour or

00:07:44,580 --> 00:07:49,860
every day but you wanted to have more

00:07:46,440 --> 00:07:53,520
online more real-time results as well so

00:07:49,860 --> 00:07:57,600
you added this stream processing layer

00:07:53,520 --> 00:07:59,280
here or the speed layer to get more

00:07:57,600 --> 00:08:01,650
up-to-date results and then you would

00:07:59,280 --> 00:08:05,130
sort of merge the results from the two

00:08:01,650 --> 00:08:07,830
layers in when you create for for data

00:08:05,130 --> 00:08:10,650
the idea here was kind of your speed

00:08:07,830 --> 00:08:12,480
layer was giving you faster results but

00:08:10,650 --> 00:08:14,400
they were also more inaccurate you

00:08:12,480 --> 00:08:16,380
didn't have all the results or you might

00:08:14,400 --> 00:08:19,800
not have the same guarantees for

00:08:16,380 --> 00:08:22,320
processing and so on the process of this

00:08:19,800 --> 00:08:24,090
batch layer was that you had all the raw

00:08:22,320 --> 00:08:25,740
data you could do reprocessing quite

00:08:24,090 --> 00:08:28,320
easily and so on did any of you catch

00:08:25,740 --> 00:08:29,460
the talk by large albertson talking

00:08:28,320 --> 00:08:31,710
about the ten failures of data

00:08:29,460 --> 00:08:33,990
engineering yeah so you talked about

00:08:31,710 --> 00:08:35,610
this sort of it typically have more

00:08:33,990 --> 00:08:37,289
tools support for doing reprocessing and

00:08:35,610 --> 00:08:38,550
stuff on the batch side if you have

00:08:37,289 --> 00:08:40,469
errors in your code you can fix them and

00:08:38,550 --> 00:08:43,650
redeploy and reprocess the data more

00:08:40,469 --> 00:08:45,480
easily but I think it has sort of

00:08:43,650 --> 00:08:47,040
changed a bit in the last couple of

00:08:45,480 --> 00:08:48,870
years so it used to be the case that you

00:08:47,040 --> 00:08:50,970
didn't have the same kind of guarantee

00:08:48,870 --> 00:08:52,110
when it came to the accuracy for example

00:08:50,970 --> 00:08:54,960
of the stream processing but this has

00:08:52,110 --> 00:08:57,000
changed with the new frameworks are are

00:08:54,960 --> 00:08:59,130
available now you're much better at

00:08:57,000 --> 00:09:00,720
gangees and in some ways stream

00:08:59,130 --> 00:09:04,070
processing is actually now sort of a

00:09:00,720 --> 00:09:04,070
superset of what you can do in batch

00:09:04,850 --> 00:09:10,050
talking about stream processing what do

00:09:06,870 --> 00:09:11,490
we kind of mean well first of all when

00:09:10,050 --> 00:09:13,080
you talk about batch processing for

00:09:11,490 --> 00:09:15,300
example you typically talk about bounded

00:09:13,080 --> 00:09:17,340
datasets you work on an hour or a day of

00:09:15,300 --> 00:09:19,200
data or so on whereas in stream

00:09:17,340 --> 00:09:22,470
processing you have unbounded datasets

00:09:19,200 --> 00:09:24,120
there's sort of potentially no end to

00:09:22,470 --> 00:09:26,850
the data you always have new data coming

00:09:24,120 --> 00:09:28,110
in so this is a feature of the stream

00:09:26,850 --> 00:09:31,890
processing frameworks that they support

00:09:28,110 --> 00:09:34,860
handling this kind of data you might

00:09:31,890 --> 00:09:37,250
also have more need to handle unordered

00:09:34,860 --> 00:09:41,820
events events come in out of order or

00:09:37,250 --> 00:09:43,980
delayed for example this can also happen

00:09:41,820 --> 00:09:46,440
in a branch layer though I mean you

00:09:43,980 --> 00:09:48,120
might have a device that is offline and

00:09:46,440 --> 00:09:50,400
then suddenly comes online if you're for

00:09:48,120 --> 00:09:52,200
example traveling by plane suddenly you

00:09:50,400 --> 00:09:54,420
have events that are several hours late

00:09:52,200 --> 00:09:56,220
coming in and and most batch pipelines

00:09:54,420 --> 00:09:59,670
that I've seen don't really handle that

00:09:56,220 --> 00:10:01,950
if you haven't events come in that late

00:09:59,670 --> 00:10:04,340
they are either discarded or included in

00:10:01,950 --> 00:10:08,400
some later data set or something

00:10:04,340 --> 00:10:10,050
so stream processing has sort of

00:10:08,400 --> 00:10:15,540
built-in features to deal with this kind

00:10:10,050 --> 00:10:17,940
of things and in the newer things like

00:10:15,540 --> 00:10:19,290
Kafka streams and flink and so on and a

00:10:17,940 --> 00:10:20,820
newer versions of the other ones as well

00:10:19,290 --> 00:10:23,280
you have much better correctness

00:10:20,820 --> 00:10:25,350
guarantees you have guarantees for at

00:10:23,280 --> 00:10:28,740
least ones processing or even exactly

00:10:25,350 --> 00:10:30,630
once processing of events you have

00:10:28,740 --> 00:10:34,230
consistency in terms of storage and

00:10:30,630 --> 00:10:37,350
handling of failures and so on and you

00:10:34,230 --> 00:10:39,960
have more built in ways to deal with

00:10:37,350 --> 00:10:42,660
time and especially as I mentioned for

00:10:39,960 --> 00:10:44,760
example handling late events typical

00:10:42,660 --> 00:10:47,370
example is that you you you I got group

00:10:44,760 --> 00:10:49,620
dates and now use in in time windows and

00:10:47,370 --> 00:10:51,240
then later on you have events coming in

00:10:49,620 --> 00:10:53,250
to that actually belong to that same

00:10:51,240 --> 00:10:54,750
time window the stream processing

00:10:53,250 --> 00:10:57,780
frameworks and then typically allow you

00:10:54,750 --> 00:11:01,320
to publish and updated results for that

00:10:57,780 --> 00:11:02,259
time window so you can have

00:11:01,320 --> 00:11:05,470
incrementally

00:11:02,259 --> 00:11:09,339
more accurate results as you as you move

00:11:05,470 --> 00:11:13,739
along and it gives you the sort of the

00:11:09,339 --> 00:11:13,739
knobs you need to tune to deal with this

00:11:13,889 --> 00:11:20,799
alright so let's get into cough grab it

00:11:17,109 --> 00:11:23,259
how many here are using Kafka that's

00:11:20,799 --> 00:11:25,299
about half or so cough cough dreams

00:11:23,259 --> 00:11:27,910
anyway a few people more or less the

00:11:25,299 --> 00:11:29,910
same people I think alright so just a

00:11:27,910 --> 00:11:32,619
brief intro to cough Carden

00:11:29,910 --> 00:11:34,720
so there are quite a few people we're

00:11:32,619 --> 00:11:39,189
not using it so Kafka is essentially a

00:11:34,720 --> 00:11:41,529
log based system so you can think of in

00:11:39,189 --> 00:11:43,509
principle Kafka writes events to an

00:11:41,529 --> 00:11:48,939
immutable log you start at the beginning

00:11:43,509 --> 00:11:50,649
you just append events to the end and

00:11:48,939 --> 00:11:54,939
you can essentially keep the events for

00:11:50,649 --> 00:11:58,119
how long you want consumers of the

00:11:54,939 --> 00:12:02,049
events can then read this log at their

00:11:58,119 --> 00:12:03,549
own pace and sort of so if one consumer

00:12:02,049 --> 00:12:05,019
is reading from here the other is

00:12:03,549 --> 00:12:06,249
reading from here they're totally

00:12:05,019 --> 00:12:09,369
independent of each other and they can

00:12:06,249 --> 00:12:11,049
read the same data you can add new

00:12:09,369 --> 00:12:13,749
consumers that can start consuming you

00:12:11,049 --> 00:12:15,579
can also do things like rewind and

00:12:13,749 --> 00:12:17,350
reprocess the data quite easily just

00:12:15,579 --> 00:12:19,029
move the point or essentially like if

00:12:17,350 --> 00:12:20,499
you were here you can just reset to the

00:12:19,029 --> 00:12:22,539
start of the log in reprocess for

00:12:20,499 --> 00:12:24,579
example if you have fixed a bug in your

00:12:22,539 --> 00:12:28,179
processing algorithm then you can just

00:12:24,579 --> 00:12:34,179
reprocess and recreate the more correct

00:12:28,179 --> 00:12:37,499
results in addition in order to achieve

00:12:34,179 --> 00:12:40,480
higher higher performance

00:12:37,499 --> 00:12:42,519
what Kafka does is it splits this log or

00:12:40,480 --> 00:12:45,159
a topic as it's called in in Kafka into

00:12:42,519 --> 00:12:48,309
multiple partitions that are spread

00:12:45,159 --> 00:12:51,279
across the Kafka cluster so typically

00:12:48,309 --> 00:12:53,919
each each server in the Confessor will

00:12:51,279 --> 00:12:57,129
have one or more partitions belonging to

00:12:53,919 --> 00:12:59,829
a topic and this allows you to spread

00:12:57,129 --> 00:13:05,739
the rights to a topic out across the

00:12:59,829 --> 00:13:08,470
cluster the other side of this is that

00:13:05,739 --> 00:13:11,230
you you lose sort of global ordering

00:13:08,470 --> 00:13:13,059
here but you still have ordering within

00:13:11,230 --> 00:13:14,919
each partition so if you are smart in

00:13:13,059 --> 00:13:15,580
how you assign data to the different

00:13:14,919 --> 00:13:18,190
partitions

00:13:15,580 --> 00:13:20,920
keys in the events then you still have

00:13:18,190 --> 00:13:28,390
can achieve the ordering that you need

00:13:20,920 --> 00:13:30,880
typically and of course these topics and

00:13:28,390 --> 00:13:32,710
partitions are replicated across the

00:13:30,880 --> 00:13:34,350
cluster so that if one of the nodes in

00:13:32,710 --> 00:13:38,190
the cluster goes down then you can just

00:13:34,350 --> 00:13:42,220
resume considering from another server

00:13:38,190 --> 00:13:44,380
or writing to another server so on the

00:13:42,220 --> 00:13:45,910
consumer side you have sort of the same

00:13:44,380 --> 00:13:48,610
picture where you have used these

00:13:45,910 --> 00:13:51,000
partitions for to achieve parallelism

00:13:48,610 --> 00:13:53,650
but you also have this concept of

00:13:51,000 --> 00:13:55,950
consumer group which so a single

00:13:53,650 --> 00:13:58,690
consumer I can have multiple threads

00:13:55,950 --> 00:14:01,600
consuming the data from a given topic

00:13:58,690 --> 00:14:04,930
and what Kafka will do then is we'll

00:14:01,600 --> 00:14:07,300
essentially just automatically divide

00:14:04,930 --> 00:14:09,550
the available partitions in this case we

00:14:07,300 --> 00:14:11,710
have twelve partitions to our topic and

00:14:09,550 --> 00:14:13,710
three consuming threads so each of them

00:14:11,710 --> 00:14:16,240
we will get four partitions to consume

00:14:13,710 --> 00:14:18,880
and these threads can be running on the

00:14:16,240 --> 00:14:21,550
same machine or on different nodes in

00:14:18,880 --> 00:14:23,500
the cluster or whatever depending on

00:14:21,550 --> 00:14:25,600
your what kind of hardware you use and

00:14:23,500 --> 00:14:27,520
and so on now if you need more

00:14:25,600 --> 00:14:30,280
processing power you can easily add

00:14:27,520 --> 00:14:32,800
another consuming thread and Kafka will

00:14:30,280 --> 00:14:34,720
automatically distribute the workload

00:14:32,800 --> 00:14:37,210
among them so you don't have to deal

00:14:34,720 --> 00:14:40,930
with this at all and it because of how

00:14:37,210 --> 00:14:43,060
it takes care of offsets and

00:14:40,930 --> 00:14:46,090
checkpointing and that and such you will

00:14:43,060 --> 00:14:49,560
ensure that you will always have still

00:14:46,090 --> 00:14:52,390
always process everything that's needed

00:14:49,560 --> 00:14:54,460
you can tune this in terms of the

00:14:52,390 --> 00:14:56,920
guarantees that you want I think the

00:14:54,460 --> 00:14:59,020
default now is set up for at least once

00:14:56,920 --> 00:15:02,260
delivery of events but with the new

00:14:59,020 --> 00:15:03,730
features in Kafka on the row and Kafka

00:15:02,260 --> 00:15:05,440
streams now you can also have it do

00:15:03,730 --> 00:15:10,620
exactly once processing in events and

00:15:05,440 --> 00:15:10,620
transactions across partitions and so on

00:15:12,090 --> 00:15:17,949
all right

00:15:14,339 --> 00:15:21,730
so so what are the events that we have

00:15:17,949 --> 00:15:25,649
talked about so far the Kafka Kafka

00:15:21,730 --> 00:15:27,940
events are essentially a key value pair

00:15:25,649 --> 00:15:29,620
but Kafka doesn't really care what the

00:15:27,940 --> 00:15:32,230
keys and values are - cough no it's just

00:15:29,620 --> 00:15:34,829
a byte array so it's up to you to bestow

00:15:32,230 --> 00:15:35,980
meaning up on this and use a sort of

00:15:34,829 --> 00:15:39,430
serialization and deserialization

00:15:35,980 --> 00:15:42,130
whether you but Jason or strings or

00:15:39,430 --> 00:15:45,459
Avram messages or whatever on the Kafka

00:15:42,130 --> 00:15:47,170
topic Kafka doesn't really care the only

00:15:45,459 --> 00:15:50,019
thing it sort of cares about is this key

00:15:47,170 --> 00:15:52,690
that determines which partition your

00:15:50,019 --> 00:15:54,940
message will end up on so for instance

00:15:52,690 --> 00:15:57,220
if you are if you have a set of events

00:15:54,940 --> 00:15:58,810
for a user you might want to use the

00:15:57,220 --> 00:16:00,519
user ID as a key so that all of the

00:15:58,810 --> 00:16:02,670
events belonging to the same user end up

00:16:00,519 --> 00:16:05,649
on the same partition and and in order

00:16:02,670 --> 00:16:07,269
right you don't have ordering guarantees

00:16:05,649 --> 00:16:09,220
across partitions but if you use same

00:16:07,269 --> 00:16:14,730
key for events belonging together then

00:16:09,220 --> 00:16:14,730
you can achieve that that ordering

00:16:15,810 --> 00:16:22,510
looking at stream processing frameworks

00:16:19,480 --> 00:16:25,480
to then actually process this data this

00:16:22,510 --> 00:16:27,310
data there are a number of options so

00:16:25,480 --> 00:16:29,769
here are some that we have evaluated

00:16:27,310 --> 00:16:31,300
when we were building our platform we

00:16:29,769 --> 00:16:32,709
were looking at spark streaming for

00:16:31,300 --> 00:16:34,600
example since we were already using

00:16:32,709 --> 00:16:38,529
Street as part for our batch processing

00:16:34,600 --> 00:16:40,630
and things like acha and Samsung and

00:16:38,529 --> 00:16:43,269
flink which you probably have heard

00:16:40,630 --> 00:16:46,959
about or even are using but we ended up

00:16:43,269 --> 00:16:49,779
going for Kafka's own solution which is

00:16:46,959 --> 00:16:53,079
kafka streams and this is something they

00:16:49,779 --> 00:16:55,990
released a year and a half ago or

00:16:53,079 --> 00:17:00,939
something maybe - it was fairly new at

00:16:55,990 --> 00:17:03,399
least when we when we picked it up one

00:17:00,939 --> 00:17:05,679
of the main reasons for for us to use

00:17:03,399 --> 00:17:08,949
this was that it's essentially just a

00:17:05,679 --> 00:17:10,809
library a lightweight Java library that

00:17:08,949 --> 00:17:12,970
you can run and include in any java

00:17:10,809 --> 00:17:15,069
application and you just deployed as a

00:17:12,970 --> 00:17:16,480
regular time application and the kafka

00:17:15,069 --> 00:17:19,510
mechanisms takes care of all this

00:17:16,480 --> 00:17:22,720
distribution of the partitions and so on

00:17:19,510 --> 00:17:24,459
to your consumer so there's no need to

00:17:22,720 --> 00:17:25,959
set up like a cluster that you need to

00:17:24,459 --> 00:17:27,270
deploy your application into and stuff

00:17:25,959 --> 00:17:30,130
like that

00:17:27,270 --> 00:17:32,440
this is nice for us as you could see I'm

00:17:30,130 --> 00:17:34,990
on my first slice we had very sort of

00:17:32,440 --> 00:17:37,510
varying amount of traffic along the day

00:17:34,990 --> 00:17:39,760
so we we wanted to be able to use auto

00:17:37,510 --> 00:17:41,500
scaling mechanisms in Amazon Cloud for

00:17:39,760 --> 00:17:43,990
example and that allowed us to do this

00:17:41,500 --> 00:17:46,480
suggest deploying as a regular job

00:17:43,990 --> 00:17:48,820
application and on 82 auto staying auto

00:17:46,480 --> 00:17:50,800
scaling cluster and based on the load

00:17:48,820 --> 00:17:53,350
you could just remove or add notes as

00:17:50,800 --> 00:17:55,210
needed and the Kafka library took care

00:17:53,350 --> 00:18:00,730
of distributing the work among the

00:17:55,210 --> 00:18:02,170
available notes like most of the stream

00:18:00,730 --> 00:18:04,660
processing frameworks you have this

00:18:02,170 --> 00:18:07,840
notion of both of streams events and

00:18:04,660 --> 00:18:09,940
also tables so what you can do is you

00:18:07,840 --> 00:18:12,670
can use various aggregation mechanisms

00:18:09,940 --> 00:18:16,570
for example to to get aggregated values

00:18:12,670 --> 00:18:17,950
for for your entities and store them in

00:18:16,570 --> 00:18:20,350
tables that you can then look up and

00:18:17,950 --> 00:18:23,830
join with and so on and we'll have some

00:18:20,350 --> 00:18:27,070
we'll have a little bit of look a little

00:18:23,830 --> 00:18:29,830
bit about that later on also you have a

00:18:27,070 --> 00:18:32,910
high level DSL which looks very similar

00:18:29,830 --> 00:18:35,080
to the stream API if you start in Java

00:18:32,910 --> 00:18:36,430
or if you use functional programming

00:18:35,080 --> 00:18:39,970
another language just like Scala for

00:18:36,430 --> 00:18:42,100
example but you also have access to the

00:18:39,970 --> 00:18:44,320
low level sort of processing topology

00:18:42,100 --> 00:18:46,240
and and framework if you didn't really

00:18:44,320 --> 00:18:51,280
need that and we have used that in a

00:18:46,240 --> 00:18:53,560
couple of cases as well so Kafka streams

00:18:51,280 --> 00:18:56,110
trials I saw this graph from there one

00:18:53,560 --> 00:19:00,940
of their documentation to trying to sort

00:18:56,110 --> 00:19:02,940
of hit this simplicity to become a

00:19:00,940 --> 00:19:05,860
simpler to used and some of the other

00:19:02,940 --> 00:19:07,450
frameworks that you have while giving

00:19:05,860 --> 00:19:09,640
away a little bit of power but still you

00:19:07,450 --> 00:19:15,130
can do quite a lot of stuff using just

00:19:09,640 --> 00:19:18,940
Kafka streams so some of the features a

00:19:15,130 --> 00:19:21,160
comfort of streams provides very similar

00:19:18,940 --> 00:19:22,630
to a lot of the other stream processing

00:19:21,160 --> 00:19:25,750
framework so of course is that you can

00:19:22,630 --> 00:19:28,960
you can of course filter your data you

00:19:25,750 --> 00:19:31,150
can use this to of course tailor the

00:19:28,960 --> 00:19:32,920
stream to just what your consumer wants

00:19:31,150 --> 00:19:34,360
this can also be useful in for example a

00:19:32,920 --> 00:19:36,100
privacy setting if you want to filter

00:19:34,360 --> 00:19:38,350
out events for a user status set that

00:19:36,100 --> 00:19:40,600
they want don't want to be processed and

00:19:38,350 --> 00:19:42,890
so on

00:19:40,600 --> 00:19:46,010
you can transform the events bringing

00:19:42,890 --> 00:19:48,200
them into the form that's expected by

00:19:46,010 --> 00:19:50,480
downstream consumers for example or

00:19:48,200 --> 00:19:52,880
whatever you just removed the data that

00:19:50,480 --> 00:19:54,770
you don't need also from a privacy

00:19:52,880 --> 00:19:56,330
standpoint that's quite nice if you want

00:19:54,770 --> 00:19:58,100
to do data minimization so you only keep

00:19:56,330 --> 00:20:01,340
the actual parts of the events that you

00:19:58,100 --> 00:20:05,240
need for your further processing can of

00:20:01,340 --> 00:20:06,860
course compute various aggregates all

00:20:05,240 --> 00:20:09,140
the typical stuff like counting and

00:20:06,860 --> 00:20:14,630
creating a computing sums and so on but

00:20:09,140 --> 00:20:16,700
also a lot more and you can deal with

00:20:14,630 --> 00:20:18,170
this aggregation typically in terms of

00:20:16,700 --> 00:20:21,230
time windows so you can aggregate the

00:20:18,170 --> 00:20:24,050
number of clicks on the site per hour or

00:20:21,230 --> 00:20:26,300
per minute and so on Kafka handles the

00:20:24,050 --> 00:20:31,270
aggregation or the time windowing done

00:20:26,300 --> 00:20:33,800
for you and of course you can also join

00:20:31,270 --> 00:20:35,660
streams together and also streams on

00:20:33,800 --> 00:20:39,080
these I could get tables together to

00:20:35,660 --> 00:20:42,820
enrich your data by joining multiple

00:20:39,080 --> 00:20:42,820
data sets together or streams together

00:20:44,020 --> 00:20:49,790
alright let's go look at some code so

00:20:47,900 --> 00:20:52,100
the basic anatomy of a caustic extremes

00:20:49,790 --> 00:20:55,880
app alright so it consists essentially

00:20:52,100 --> 00:20:57,890
of three parts if you write a kafka

00:20:55,880 --> 00:20:59,930
stream sort of pure Kafka streams up you

00:20:57,890 --> 00:21:01,910
have some configuration that you need to

00:20:59,930 --> 00:21:04,490
sort of the minimal complication is you

00:21:01,910 --> 00:21:10,100
need to tell it where to find the Kafka

00:21:04,490 --> 00:21:13,520
cluster and also then an ID or a name

00:21:10,100 --> 00:21:15,500
for your for your application the second

00:21:13,520 --> 00:21:19,580
part is that you need to build your

00:21:15,500 --> 00:21:20,960
streaming stream processing topology and

00:21:19,580 --> 00:21:23,480
we'll get into that more in the

00:21:20,960 --> 00:21:25,400
following slides and finally you need to

00:21:23,480 --> 00:21:27,440
of course start your Kafka streams

00:21:25,400 --> 00:21:30,020
processing so essentially it is create

00:21:27,440 --> 00:21:31,610
an instance of Kafka streams give it the

00:21:30,020 --> 00:21:33,440
topology and configuration and tell it

00:21:31,610 --> 00:21:35,270
to start and typically also registers

00:21:33,440 --> 00:21:37,400
some kind of shutdown hook so that it

00:21:35,270 --> 00:21:41,000
shuts down properly when when your

00:21:37,400 --> 00:21:44,300
application is terminated so these are

00:21:41,000 --> 00:21:45,800
the three main things that you need of

00:21:44,300 --> 00:21:48,440
course you might need more configuration

00:21:45,800 --> 00:21:50,210
to deal with setting up timeout values

00:21:48,440 --> 00:21:51,820
and stuff like that but I don't want to

00:21:50,210 --> 00:21:54,620
go into details on that right now

00:21:51,820 --> 00:21:57,950
but let's have a look in more detail on

00:21:54,620 --> 00:21:59,660
what building a topology looks like so

00:21:57,950 --> 00:22:06,860
that's where the fun is right that's

00:21:59,660 --> 00:22:08,420
like that's the actual actual work so we

00:22:06,860 --> 00:22:10,310
have this dreams builder let's see what

00:22:08,420 --> 00:22:12,800
we can do or stream builder I think

00:22:10,310 --> 00:22:14,900
that's a typo here actually

00:22:12,800 --> 00:22:17,180
they changed the API and naming of

00:22:14,900 --> 00:22:20,600
things in one dollar or slightly so we

00:22:17,180 --> 00:22:23,030
have a stream builder and as I mentioned

00:22:20,600 --> 00:22:25,040
Kafka doesn't care what the data is two

00:22:23,030 --> 00:22:26,570
guys just bytes so you have to tell it

00:22:25,040 --> 00:22:29,960
how to serialize and deserialize the

00:22:26,570 --> 00:22:32,240
data and it uses this concept called a

00:22:29,960 --> 00:22:35,270
third short for serializer deserialize

00:22:32,240 --> 00:22:38,210
err and it has a bunch of them built in

00:22:35,270 --> 00:22:40,940
so you can this relies string byte

00:22:38,210 --> 00:22:47,930
arrays into strings for example in this

00:22:40,940 --> 00:22:51,290
case so I have here I'm building I'm

00:22:47,930 --> 00:22:54,920
consuming a stream or a topic called

00:22:51,290 --> 00:22:56,960
articles and I'm saying that consuming

00:22:54,920 --> 00:22:59,720
this with a key type of string and value

00:22:56,960 --> 00:23:03,520
type of string and this gives me back a

00:22:59,720 --> 00:23:08,540
case stream which is sort of the basic

00:23:03,520 --> 00:23:10,190
component in Kafka streams and okay

00:23:08,540 --> 00:23:12,140
since this is hello Kafka streams

00:23:10,190 --> 00:23:14,990
example what do we do with it we print

00:23:12,140 --> 00:23:19,640
it out to stand it up and this will show

00:23:14,990 --> 00:23:25,970
something like this so here you have

00:23:19,640 --> 00:23:27,820
sort of the the keys comma the values as

00:23:25,970 --> 00:23:30,230
you can see the keys are not in order

00:23:27,820 --> 00:23:32,000
because they are spread across multiple

00:23:30,230 --> 00:23:33,710
partitions in this case I have only one

00:23:32,000 --> 00:23:36,080
consumer so it will consume all the

00:23:33,710 --> 00:23:39,890
different partitions in one but it will

00:23:36,080 --> 00:23:41,360
receive sort of random it will be random

00:23:39,890 --> 00:23:42,920
ordering between the partitions so

00:23:41,360 --> 00:23:45,140
that's why you don't see one two three

00:23:42,920 --> 00:23:47,000
and some one here but you'll have the

00:23:45,140 --> 00:23:51,110
evidence coming from the different

00:23:47,000 --> 00:23:53,800
partitions interleaved as you can

00:23:51,110 --> 00:23:58,640
probably also see the actual events or

00:23:53,800 --> 00:24:01,190
values here are seem to be Jason so if

00:23:58,640 --> 00:24:03,149
we want to consume the events as Jason

00:24:01,190 --> 00:24:07,229
sort of strings we can also tell

00:24:03,149 --> 00:24:09,210
Kafka seems to do that so what we need

00:24:07,229 --> 00:24:12,210
to do then is create it doesn't provide

00:24:09,210 --> 00:24:14,820
this by default so we need to create

00:24:12,210 --> 00:24:17,219
this JSON absurd but it's essentially

00:24:14,820 --> 00:24:20,249
four lines of Java code and a bunch of

00:24:17,219 --> 00:24:22,379
boilerplate and if you clone the

00:24:20,249 --> 00:24:23,039
repository you'll see an example of how

00:24:22,379 --> 00:24:24,749
you can do that

00:24:23,039 --> 00:24:28,139
so it's essentially just using jackson

00:24:24,749 --> 00:24:32,219
in this case Jackson Jason framework and

00:24:28,139 --> 00:24:34,589
it's three or four lines of code so and

00:24:32,219 --> 00:24:36,629
then since I then specify that the value

00:24:34,589 --> 00:24:39,749
type is now of Jason I get a case stream

00:24:36,629 --> 00:24:42,389
of string type for key and Jason node

00:24:39,749 --> 00:24:45,889
for my value and the printout still

00:24:42,389 --> 00:24:49,109
looks the same since it's still Jason

00:24:45,889 --> 00:24:50,729
oftentimes you will have the same it

00:24:49,109 --> 00:24:52,799
will repeat the same sort of key and

00:24:50,729 --> 00:24:54,710
value types a lot in your application so

00:24:52,799 --> 00:24:57,089
instead of having to specify this

00:24:54,710 --> 00:25:01,589
explicitly you can also configure the

00:24:57,089 --> 00:25:03,269
defaults here so you can in this

00:25:01,589 --> 00:25:05,339
configuration section that we showed

00:25:03,269 --> 00:25:08,279
earlier you can add configuration of

00:25:05,339 --> 00:25:11,070
default serve classes for your keys and

00:25:08,279 --> 00:25:13,349
Alice if you want to and then you can

00:25:11,070 --> 00:25:20,570
just say create a stream from this topic

00:25:13,349 --> 00:25:24,499
and leave out the consumer all right

00:25:20,570 --> 00:25:30,839
time to move on to some more more code

00:25:24,499 --> 00:25:34,799
more examples of what you can do any

00:25:30,839 --> 00:25:39,169
questions so far or no everything our

00:25:34,799 --> 00:25:39,169
own keeping keeping up that's good

00:25:39,679 --> 00:25:47,039
alright so I mentioned you can do

00:25:41,969 --> 00:25:50,489
filtering so we we have our our stream

00:25:47,039 --> 00:25:56,219
of article data from a various news

00:25:50,489 --> 00:26:00,690
sources and I want to fetch just the

00:25:56,219 --> 00:26:04,289
ones that come from BBC now if you if

00:26:00,690 --> 00:26:05,580
you know this the the stream API in Java

00:26:04,289 --> 00:26:10,499
for example this will look very very

00:26:05,580 --> 00:26:13,289
similar you do a call to filter this

00:26:10,499 --> 00:26:14,820
will take in the key value pair that you

00:26:13,289 --> 00:26:16,669
can do the filtering on and you just

00:26:14,820 --> 00:26:20,480
return true or false depending on

00:26:16,669 --> 00:26:21,499
you want Evan to be filter out or not or

00:26:20,480 --> 00:26:23,720
sorry the opposite if you don't keep

00:26:21,499 --> 00:26:26,239
this keep the event or or not so in this

00:26:23,720 --> 00:26:29,179
case I'm just looking for the events

00:26:26,239 --> 00:26:34,759
where BBC equals the site of the Jason

00:26:29,179 --> 00:26:39,200
event and this will give me them back a

00:26:34,759 --> 00:26:43,279
nuke a stream with the same type but

00:26:39,200 --> 00:26:46,009
with only the events that I expect if I

00:26:43,279 --> 00:26:50,840
print this out I'll get only these three

00:26:46,009 --> 00:26:54,820
events we can do more of course we can

00:26:50,840 --> 00:26:58,159
do not just filtering it also transforms

00:26:54,820 --> 00:27:00,489
so if I want to have just not the entire

00:26:58,159 --> 00:27:04,519
event but just the titles for example

00:27:00,489 --> 00:27:08,330
how do we do this so in this case I can

00:27:04,519 --> 00:27:11,269
use the method called math values which

00:27:08,330 --> 00:27:14,059
takes in the value of the event and just

00:27:11,269 --> 00:27:16,090
returns a new value out so in this case

00:27:14,059 --> 00:27:19,489
I extract the title event the title

00:27:16,090 --> 00:27:22,279
value from the jason and return that as

00:27:19,489 --> 00:27:25,460
a string or its text now you can see

00:27:22,279 --> 00:27:26,929
also my case stream type the type of my

00:27:25,460 --> 00:27:30,440
value has changed from jason of the

00:27:26,929 --> 00:27:33,859
string that's expected now if i print

00:27:30,440 --> 00:27:35,659
this out of course i get just the titles

00:27:33,859 --> 00:27:42,169
the same keys though i haven't changed

00:27:35,659 --> 00:27:47,239
the keys is any of you are any of you

00:27:42,169 --> 00:27:49,879
using Scala with a few couple so yeah

00:27:47,239 --> 00:27:51,320
they've made this for some reason even

00:27:49,879 --> 00:27:53,090
though Kafka itself is implement in

00:27:51,320 --> 00:27:56,149
Scala it's a bit hard to actually make

00:27:53,090 --> 00:27:58,340
the API work nicely from Scala but

00:27:56,149 --> 00:28:02,090
they're fixing this in in Kafka 2-0

00:27:58,340 --> 00:28:04,249
they're adding a separate Scala API and

00:28:02,090 --> 00:28:06,080
that's actually supposed to go into code

00:28:04,249 --> 00:28:13,100
freeze today with the release candidate

00:28:06,080 --> 00:28:14,629
so we'll see hopefully alright of course

00:28:13,100 --> 00:28:16,909
you can also join these two together

00:28:14,629 --> 00:28:19,159
like you can with a regular stream API

00:28:16,909 --> 00:28:20,840
in Java so you can just have this as one

00:28:19,159 --> 00:28:24,739
expression where you do the filter and

00:28:20,840 --> 00:28:26,299
map as a chain of operations now we're

00:28:24,739 --> 00:28:27,980
still just printing this stuff out and

00:28:26,299 --> 00:28:29,869
that's probably not what you want to do

00:28:27,980 --> 00:28:31,059
right you want to put this data back

00:28:29,869 --> 00:28:33,220
into

00:28:31,059 --> 00:28:35,890
into your system so that you can do

00:28:33,220 --> 00:28:39,880
quite easily writing it back into Kafka

00:28:35,890 --> 00:28:44,920
by using this two method on your case

00:28:39,880 --> 00:28:47,080
stream just provided with with the topic

00:28:44,920 --> 00:28:50,080
that you want to write to and then also

00:28:47,080 --> 00:28:52,059
specifying how should you deal with what

00:28:50,080 --> 00:28:56,620
the data how do you convert it back to

00:28:52,059 --> 00:28:59,110
byte arrays and then if I from my

00:28:56,620 --> 00:29:01,679
command line I I can use the command

00:28:59,110 --> 00:29:06,630
line interface for consuming data from

00:29:01,679 --> 00:29:10,030
from my topic and it will show me these

00:29:06,630 --> 00:29:21,309
three titles from from the data that I

00:29:10,030 --> 00:29:23,080
had alright so this is the map values is

00:29:21,309 --> 00:29:26,020
sort of this implicit transform you can

00:29:23,080 --> 00:29:29,110
do you also have sometimes you need to

00:29:26,020 --> 00:29:32,170
convert one input event into a series of

00:29:29,110 --> 00:29:36,640
output events for example if you extract

00:29:32,170 --> 00:29:39,280
a list of data from inside of your event

00:29:36,640 --> 00:29:41,950
object so converting from one input

00:29:39,280 --> 00:29:46,300
event to too many or zero in that case

00:29:41,950 --> 00:29:48,160
sometimes if you again if you know

00:29:46,300 --> 00:29:52,570
functional programming or the stream API

00:29:48,160 --> 00:29:54,130
you can use the flat map operation to do

00:29:52,570 --> 00:29:56,140
this and that's essentially that that

00:29:54,130 --> 00:29:58,570
takes in the value and you return an

00:29:56,140 --> 00:30:00,790
iterable of whatever type output type

00:29:58,570 --> 00:30:02,980
you have so this works nicely for

00:30:00,790 --> 00:30:07,090
collections and arrays which are

00:30:02,980 --> 00:30:09,730
iterables in in Java sometimes you work

00:30:07,090 --> 00:30:11,770
with iterators and that is a little bit

00:30:09,730 --> 00:30:15,700
more hassle in particularly in the case

00:30:11,770 --> 00:30:18,940
here of of Jason so I want to extract

00:30:15,700 --> 00:30:22,540
not the title of the article here but

00:30:18,940 --> 00:30:24,850
the list of authors so there could be

00:30:22,540 --> 00:30:27,550
one or more authors of a newspaper

00:30:24,850 --> 00:30:30,309
article for example and this will

00:30:27,550 --> 00:30:36,100
actually fail because this returns an

00:30:30,309 --> 00:30:38,700
iterator are not an iterable and the API

00:30:36,100 --> 00:30:41,860
doesn't support that so but it's quite

00:30:38,700 --> 00:30:43,760
simple to create an iterable because

00:30:41,860 --> 00:30:47,150
what an iterable in java is

00:30:43,760 --> 00:30:49,370
essentially just a function or a class

00:30:47,150 --> 00:30:51,860
that returns an iterator and we already

00:30:49,370 --> 00:30:55,160
have the iterator so we can do that

00:30:51,860 --> 00:30:56,690
quite easily but there's a simpler way

00:30:55,160 --> 00:30:59,360
to do this since this is a single

00:30:56,690 --> 00:31:03,010
abstract method we can replace this

00:30:59,360 --> 00:31:05,630
whole thing with just an operation or

00:31:03,010 --> 00:31:07,580
anonymous function that a that has no

00:31:05,630 --> 00:31:09,860
input parameters but returns an iterator

00:31:07,580 --> 00:31:11,510
so you get this kind of funny-looking

00:31:09,860 --> 00:31:15,020
structure here so you have your input

00:31:11,510 --> 00:31:18,200
value and it actually returns a function

00:31:15,020 --> 00:31:28,520
here so we have a function that returns

00:31:18,200 --> 00:31:30,980
the iterator and that works so that's

00:31:28,520 --> 00:31:34,340
how you can extract multiple events but

00:31:30,980 --> 00:31:35,360
if you if you already just if you have

00:31:34,340 --> 00:31:37,250
something the returns a collection

00:31:35,360 --> 00:31:41,770
directly you don't need to have this

00:31:37,250 --> 00:31:41,770
sort of funny-looking operation here yes

00:31:44,620 --> 00:31:52,460
yes since I'm doing flat map values I

00:31:48,470 --> 00:31:55,730
don't change the key you have similar

00:31:52,460 --> 00:31:58,400
operations called flat map and and just

00:31:55,730 --> 00:32:01,310
map that allow you to also change the

00:31:58,400 --> 00:32:02,750
key of the events I'll get back to that

00:32:01,310 --> 00:32:07,220
a little bit later

00:32:02,750 --> 00:32:10,880
but I sort of as I mentioned the key

00:32:07,220 --> 00:32:12,170
determines where your events end up

00:32:10,880 --> 00:32:12,940
right so if you change the key you need

00:32:12,170 --> 00:32:15,140
to move them around

00:32:12,940 --> 00:32:18,650
essentially it we'll talk a bit more

00:32:15,140 --> 00:32:20,960
about that later the final sort of basic

00:32:18,650 --> 00:32:23,330
thing that I wanted to mention is that

00:32:20,960 --> 00:32:24,680
you have support for branching of events

00:32:23,330 --> 00:32:27,470
if you want to split your events into

00:32:24,680 --> 00:32:30,080
multiple streams you can do that so

00:32:27,470 --> 00:32:32,450
essentially you just call a method

00:32:30,080 --> 00:32:36,290
branch and you provide it with a set of

00:32:32,450 --> 00:32:39,530
predicates and it will return an array

00:32:36,290 --> 00:32:41,840
of streams so in this case I have I want

00:32:39,530 --> 00:32:44,350
to split pi events into streams for

00:32:41,840 --> 00:32:48,500
articles from BBC CNN Fox News and then

00:32:44,350 --> 00:32:53,380
whatever is not covered by those so I

00:32:48,500 --> 00:32:55,580
will get back four streams and I can

00:32:53,380 --> 00:32:57,170
these are just regular k streams so I

00:32:55,580 --> 00:33:02,150
can write those out to different

00:32:57,170 --> 00:33:03,950
the topics written and the things I've

00:33:02,150 --> 00:33:05,930
gone through now are essentially what we

00:33:03,950 --> 00:33:08,120
have used to build up our mainstreaming

00:33:05,930 --> 00:33:10,870
pipeline the filtering transforms and

00:33:08,120 --> 00:33:12,590
this branching is what we do to

00:33:10,870 --> 00:33:15,140
essentially the building blocks that we

00:33:12,590 --> 00:33:16,700
use to to move all of our incoming

00:33:15,140 --> 00:33:18,440
events to the different downstream

00:33:16,700 --> 00:33:21,980
consumers in the format that they expect

00:33:18,440 --> 00:33:28,250
and so on using these simple building

00:33:21,980 --> 00:33:36,640
blocks so I think we're pretty good on

00:33:28,250 --> 00:33:40,880
time so have have you all been able to

00:33:36,640 --> 00:33:43,190
download and get the code from the

00:33:40,880 --> 00:33:49,760
github repo or at least some of you I

00:33:43,190 --> 00:33:50,840
figured if you don't have it then if you

00:33:49,760 --> 00:33:56,390
want to work something together with

00:33:50,840 --> 00:33:57,770
someone that's also good I won't spend

00:33:56,390 --> 00:34:02,090
that much time on this right now because

00:33:57,770 --> 00:34:03,620
I think we I can go through some of the

00:34:02,090 --> 00:34:05,360
examples later on and then you can also

00:34:03,620 --> 00:34:07,820
work on this on your own or we can hang

00:34:05,360 --> 00:34:10,730
out after after the talk and go through

00:34:07,820 --> 00:34:13,790
more details sound good so I'm thinking

00:34:10,730 --> 00:34:15,350
we spend about 15 minutes on this now

00:34:13,790 --> 00:34:17,450
then do a short break and I'll go

00:34:15,350 --> 00:34:19,850
through some of the exercises after I'll

00:34:17,450 --> 00:34:23,380
go if people have questions please raise

00:34:19,850 --> 00:34:23,380
your hand I can go around and help out

00:34:24,310 --> 00:34:30,110
there are also some example code showing

00:34:27,800 --> 00:34:37,460
the different things so let me actually

00:34:30,110 --> 00:34:42,590
open up my IDE to show you let's see if

00:34:37,460 --> 00:34:52,300
I can get this window here where has it

00:34:42,590 --> 00:34:52,300
gone I display no mirroring all right

00:34:55,850 --> 00:35:12,770
oh can you it's a bit small maybe let me

00:35:06,620 --> 00:35:17,600
change the font size here so is that

00:35:12,770 --> 00:35:19,270
readable I hope so if you open this

00:35:17,600 --> 00:35:23,780
repository you will see that you have

00:35:19,270 --> 00:35:26,530
examples and exercises here and the goal

00:35:23,780 --> 00:35:30,290
of the exercises is to essentially

00:35:26,530 --> 00:35:32,060
they're written as tests that initially

00:35:30,290 --> 00:35:33,710
will fail so if you run the Gradle build

00:35:32,060 --> 00:35:35,510
you'll see that you have have like

00:35:33,710 --> 00:35:36,980
sixteen failing tests or something and

00:35:35,510 --> 00:35:40,160
the goal is to actually have the tests

00:35:36,980 --> 00:35:41,930
pass by filling out what I pass in is

00:35:40,160 --> 00:35:45,350
this stream builder that we saw earlier

00:35:41,930 --> 00:35:48,190
and you're expected to create enough to

00:35:45,350 --> 00:35:50,840
topic containing some particular data as

00:35:48,190 --> 00:35:52,370
described in the in the in the sort of

00:35:50,840 --> 00:35:54,890
documentation above each of these

00:35:52,370 --> 00:35:56,720
methods so there's one sort of just

00:35:54,890 --> 00:35:59,420
basic example here which just passes the

00:35:56,720 --> 00:36:01,040
events through unchanged just to give an

00:35:59,420 --> 00:36:05,000
idea

00:36:01,040 --> 00:36:06,650
there are also some example code in the

00:36:05,000 --> 00:36:10,580
neighboring package here that you can

00:36:06,650 --> 00:36:12,650
have a look at as well if you are

00:36:10,580 --> 00:36:15,170
adventurous and want to do this in Scala

00:36:12,650 --> 00:36:19,100
there are similar examples and exercises

00:36:15,170 --> 00:36:22,640
in Scala but you need to change a value

00:36:19,100 --> 00:36:26,210
here in this exercise base to uncomment

00:36:22,640 --> 00:36:28,040
the Java exercises and sorry I've I've

00:36:26,210 --> 00:36:29,950
already done that but so you need to

00:36:28,040 --> 00:36:35,780
switch these two if you want to use the

00:36:29,950 --> 00:36:39,950
scholar once so that's not good alright

00:36:35,780 --> 00:36:41,510
let me know if you I'll walk around we

00:36:39,950 --> 00:36:43,700
spend about fifteen or twenty minutes on

00:36:41,510 --> 00:36:46,460
this I think and I'll go through some of

00:36:43,700 --> 00:36:47,780
the examples and exercises after and

00:36:46,460 --> 00:36:52,310
then continue to continue with in the

00:36:47,780 --> 00:36:53,720
second part of the talk so raise your

00:36:52,310 --> 00:36:57,620
hand if you want to if you have

00:36:53,720 --> 00:36:59,830
questions are any any issues on head

00:36:57,620 --> 00:36:59,830
around

00:38:34,089 --> 00:38:40,940
all right just one thing which I forgot

00:38:38,480 --> 00:38:44,030
to put up here is that if you don't have

00:38:40,940 --> 00:38:45,500
the Scala plugin for IntelliJ it might

00:38:44,030 --> 00:38:47,359
not be able to compile so in that case

00:38:45,500 --> 00:38:50,089
you need to compile from the command

00:38:47,359 --> 00:38:53,180
line using Gradle you can still use

00:38:50,089 --> 00:38:56,240
their ID or any text editor to edit it

00:38:53,180 --> 00:38:59,530
but I think it requires the Scala since

00:38:56,240 --> 00:38:59,530
the tests are written in Scala

00:46:34,819 --> 00:46:39,510
so I I got a question about the slides I

00:46:38,190 --> 00:46:41,460
just uploaded them earlier to

00:46:39,510 --> 00:46:44,599
slideshare.net so you go there and

00:46:41,460 --> 00:46:47,750
search for Kafka streams workshop and

00:46:44,599 --> 00:46:50,130
Fredrik you should probably find them

00:46:47,750 --> 00:46:52,549
hopefully otherwise I'll put up a link

00:46:50,130 --> 00:46:52,549
after

00:47:39,170 --> 00:47:46,740
all right I think I'm gonna start just

00:47:43,830 --> 00:47:48,690
going through some of the exercises show

00:47:46,740 --> 00:47:50,190
you some of the solutions and we want to

00:47:48,690 --> 00:47:51,660
unfortunately have time to go through

00:47:50,190 --> 00:47:52,950
all of this now but I see at least a lot

00:47:51,660 --> 00:47:55,560
of you have gotten started and that's

00:47:52,950 --> 00:47:57,570
good as I mentioned earlier if you wanna

00:47:55,560 --> 00:48:01,080
hang out later and work on more of the

00:47:57,570 --> 00:48:03,710
exercises that's I'm happy to do that or

00:48:01,080 --> 00:48:07,230
you can also do it as homework of course

00:48:03,710 --> 00:48:09,030
all right so let's see I'll just go

00:48:07,230 --> 00:48:12,480
through a couple of the first ones to

00:48:09,030 --> 00:48:15,270
show you what it was the what's expected

00:48:12,480 --> 00:48:16,980
here in the git repository there's also

00:48:15,270 --> 00:48:18,810
a solutions branch that you can check

00:48:16,980 --> 00:48:21,690
out and have a look at if you want so

00:48:18,810 --> 00:48:25,830
you can see some suggested solutions

00:48:21,690 --> 00:48:28,050
here most of these examples require only

00:48:25,830 --> 00:48:30,500
like three or four lines of code so what

00:48:28,050 --> 00:48:34,050
we want here if we look at the exercise

00:48:30,500 --> 00:48:36,810
we get in the set of strings on lines

00:48:34,050 --> 00:48:43,920
and we want to return just a length of

00:48:36,810 --> 00:48:48,360
that string so that means sorry we need

00:48:43,920 --> 00:48:55,130
to create our input stream first from

00:48:48,360 --> 00:48:58,830
the topic text and that is consumed with

00:48:55,130 --> 00:49:02,760
strings and and strings right as before

00:48:58,830 --> 00:49:08,430
so we have our our input stream we can

00:49:02,760 --> 00:49:12,300
call that lines so now we have we input

00:49:08,430 --> 00:49:15,270
our input data can people read okay in

00:49:12,300 --> 00:49:18,590
the back there I'll I can even collapse

00:49:15,270 --> 00:49:20,360
this one and make it a slightly larger

00:49:18,590 --> 00:49:24,720
[Music]

00:49:20,360 --> 00:49:27,060
like so so on lines we want to do map

00:49:24,720 --> 00:49:30,270
values then since we are transforming

00:49:27,060 --> 00:49:32,550
our value from the text into the actual

00:49:30,270 --> 00:49:36,030
length of the text so we have a line in

00:49:32,550 --> 00:49:37,440
this case as input and we are just we

00:49:36,030 --> 00:49:40,950
just want to return the length of that

00:49:37,440 --> 00:49:44,490
so we can do that or as IntelliJ is

00:49:40,950 --> 00:49:47,040
happily reminding me instead of doing

00:49:44,490 --> 00:49:49,430
line lengths here I can replace this

00:49:47,040 --> 00:49:51,230
with a method reference if I want

00:49:49,430 --> 00:49:53,839
just referring the the length method on

00:49:51,230 --> 00:49:56,720
the string I am kind of partial to

00:49:53,839 --> 00:50:03,140
actually using this form so but up to

00:49:56,720 --> 00:50:06,319
you lengths and finally we want to write

00:50:03,140 --> 00:50:11,390
this out to our output topic with the

00:50:06,319 --> 00:50:19,010
two method the topic name from here line

00:50:11,390 --> 00:50:23,569
lengths and then produce not concerned

00:50:19,010 --> 00:50:27,050
with strings as the keys still but now

00:50:23,569 --> 00:50:30,170
our values are our integers so we need

00:50:27,050 --> 00:50:33,109
to use the different insert which so the

00:50:30,170 --> 00:50:34,760
cells are provided up here and here you

00:50:33,109 --> 00:50:36,170
can also see this Jason node sir that I

00:50:34,760 --> 00:50:37,430
mentioned earlier if you want to look at

00:50:36,170 --> 00:50:39,290
how that is implemented you can navigate

00:50:37,430 --> 00:50:40,730
to that and you can see that there's a

00:50:39,290 --> 00:50:41,930
bunch of empty boilerplate stuff the

00:50:40,730 --> 00:50:43,880
next section handling but it's

00:50:41,930 --> 00:50:50,109
essentially essentially just like four

00:50:43,880 --> 00:50:53,960
lines of actual code anyway back to this

00:50:50,109 --> 00:50:56,000
and we can of course we don't have to

00:50:53,960 --> 00:50:58,309
assign everything to intermediate

00:50:56,000 --> 00:51:01,549
variables here so you can inline a bunch

00:50:58,309 --> 00:51:05,000
of stuff and typically you will see

00:51:01,549 --> 00:51:08,690
something that looks kind of like kind

00:51:05,000 --> 00:51:17,000
of like this that's fairly common to see

00:51:08,690 --> 00:51:19,280
a format for it now words per line it's

00:51:17,000 --> 00:51:21,500
very similar but instead of counting I'm

00:51:19,280 --> 00:51:23,569
gonna start with the same code

00:51:21,500 --> 00:51:25,790
essentially just to get a bit faster

00:51:23,569 --> 00:51:27,020
here instead of counting the number of

00:51:25,790 --> 00:51:31,809
characters we only count the number of

00:51:27,020 --> 00:51:33,440
words easiest way to do that is to split

00:51:31,809 --> 00:51:36,859
the line

00:51:33,440 --> 00:51:43,069
spaces and then just do the length on

00:51:36,859 --> 00:51:44,690
that and that will be words per line now

00:51:43,069 --> 00:51:47,380
let's see if we can actually run some of

00:51:44,690 --> 00:51:47,380
the tests here

00:51:52,920 --> 00:51:56,740
no okay

00:51:54,849 --> 00:52:06,960
exercises then I'll just find my

00:51:56,740 --> 00:52:06,960
exercises here why is it no no here

00:52:07,650 --> 00:52:18,849
here's the actual tests don't need to

00:52:10,599 --> 00:52:22,390
run run so if you run this from the

00:52:18,849 --> 00:52:24,220
Gradle command line actually that works

00:52:22,390 --> 00:52:26,440
too but it will output quite a lot of

00:52:24,220 --> 00:52:29,619
information so sometimes it is if you

00:52:26,440 --> 00:52:32,589
see in the in the IDE what their stuff

00:52:29,619 --> 00:52:34,150
is working or not but as I mentioned

00:52:32,589 --> 00:52:35,800
requires that you have the Scala plug-in

00:52:34,150 --> 00:52:37,540
installed so if you don't have that and

00:52:35,800 --> 00:52:39,700
use the grade one now

00:52:37,540 --> 00:52:41,470
it's bit small here but maybe you can

00:52:39,700 --> 00:52:44,200
see here now that the three first tests

00:52:41,470 --> 00:52:47,890
are green and the rest are still red and

00:52:44,200 --> 00:52:50,230
that's the remainder of the exercise

00:52:47,890 --> 00:52:55,170
also I think I'll show one or two more

00:52:50,230 --> 00:52:58,000
just to show you some examples so here

00:52:55,170 --> 00:53:01,420
all the words is an example where we

00:52:58,000 --> 00:53:04,960
want to split one line of text into

00:53:01,420 --> 00:53:06,810
multiple events so we can start out with

00:53:04,960 --> 00:53:12,490
the one that we had up here again

00:53:06,810 --> 00:53:14,410
perhaps but now instead of splitting and

00:53:12,490 --> 00:53:17,230
returning the length we actually just

00:53:14,410 --> 00:53:19,510
want to return the words themselves but

00:53:17,230 --> 00:53:21,520
this doesn't work with map values of

00:53:19,510 --> 00:53:24,940
course because then we will instead

00:53:21,520 --> 00:53:27,250
return our data type with the array of

00:53:24,940 --> 00:53:29,260
strings but we wanted still the output

00:53:27,250 --> 00:53:31,720
to be strings just individual words and

00:53:29,260 --> 00:53:40,109
not the whole lines so this is where we

00:53:31,720 --> 00:53:45,520
want flat map values and it doesn't

00:53:40,109 --> 00:53:50,530
let's see split this is work again sorry

00:53:45,520 --> 00:53:52,990
good oh yeah so the trick here since it

00:53:50,530 --> 00:53:55,150
doesn't handle arrays directly but it

00:53:52,990 --> 00:53:58,290
does handle collections you can use the

00:53:55,150 --> 00:53:58,290
built in arrays

00:53:58,810 --> 00:54:05,980
health care class and do as list and

00:54:03,460 --> 00:54:07,960
that will give you and of course our

00:54:05,980 --> 00:54:10,270
output type is not in spot its strings

00:54:07,960 --> 00:54:12,220
and the compiler is happy the only thing

00:54:10,270 --> 00:54:17,800
we need a fix is the name of the output

00:54:12,220 --> 00:54:19,360
topic so now if I rerun my tests

00:54:17,800 --> 00:54:23,470
hopefully we should have one more green

00:54:19,360 --> 00:54:35,619
test and yeah we do get all the words of

00:54:23,470 --> 00:54:37,660
screen ok yeah if you guys as I said

00:54:35,619 --> 00:54:39,760
when I go through more of this after the

00:54:37,660 --> 00:54:42,910
talked and I'm very happy to do that but

00:54:39,760 --> 00:54:45,460
I think we should move on with some more

00:54:42,910 --> 00:54:47,650
of the content and I also had some

00:54:45,460 --> 00:54:50,200
questions and in the break here that I

00:54:47,650 --> 00:54:58,170
wanted to cover at the end if we have

00:54:50,200 --> 00:55:06,369
time so see if I can switch back here I

00:54:58,170 --> 00:55:11,230
wanted to turn off mirroring back into

00:55:06,369 --> 00:55:13,330
presentation mode alright so we covered

00:55:11,230 --> 00:55:14,530
and you saw some examples and worked a

00:55:13,330 --> 00:55:17,140
little bit with some of the basic

00:55:14,530 --> 00:55:19,660
features that allow you to do

00:55:17,140 --> 00:55:22,180
essentially kind of ETL like the

00:55:19,660 --> 00:55:24,010
behavior using kafka streams just taking

00:55:22,180 --> 00:55:26,020
your events moving them from somewhere

00:55:24,010 --> 00:55:31,090
to somewhere else and possibly doing

00:55:26,020 --> 00:55:32,020
some transformations along the way but

00:55:31,090 --> 00:55:33,640
sometimes you want to do some

00:55:32,020 --> 00:55:35,380
computations on your data as well right

00:55:33,640 --> 00:55:39,300
and that's where the aggregations come

00:55:35,380 --> 00:55:44,500
in so if we stick with our example of

00:55:39,300 --> 00:55:45,760
news articles I want to compute I don't

00:55:44,500 --> 00:55:47,470
want to see just the raw stream of

00:55:45,760 --> 00:55:51,240
articles I want to compute the number of

00:55:47,470 --> 00:55:53,500
articles that are published first site

00:55:51,240 --> 00:55:55,300
and this is very similar to what we do

00:55:53,500 --> 00:55:56,920
when we get our click stream events and

00:55:55,300 --> 00:56:02,530
you want to count the number of visitors

00:55:56,920 --> 00:56:06,310
for example parasite and so on what you

00:56:02,530 --> 00:56:09,490
can do then is instead of doing your map

00:56:06,310 --> 00:56:11,530
and and so on the case stream itself you

00:56:09,490 --> 00:56:14,860
want to create something called a cake

00:56:11,530 --> 00:56:16,330
stream so what you do is you essentially

00:56:14,860 --> 00:56:21,130
you take your input events and you tell

00:56:16,330 --> 00:56:23,080
it to group your event by some by some

00:56:21,130 --> 00:56:25,540
value so you based on your key and your

00:56:23,080 --> 00:56:31,510
value you can group your events in this

00:56:25,540 --> 00:56:33,970
case by the site of the article and what

00:56:31,510 --> 00:56:38,350
this does is essentially create a new

00:56:33,970 --> 00:56:41,260
key for your events so you're you are

00:56:38,350 --> 00:56:42,880
rekeying your events so that all of the

00:56:41,260 --> 00:56:47,530
events that belong together end up in

00:56:42,880 --> 00:56:49,870
the same partition I had to actually

00:56:47,530 --> 00:56:52,270
sort of borrow this this picture is

00:56:49,870 --> 00:56:54,850
borrowed from a spark documentation or

00:56:52,270 --> 00:56:59,980
talk but what you want to do is in order

00:56:54,850 --> 00:57:01,690
to be able to to do aggregations on a

00:56:59,980 --> 00:57:03,160
set of data you need to ensure that all

00:57:01,690 --> 00:57:05,440
of the events that belong together are

00:57:03,160 --> 00:57:07,780
ending up on the same node so that you

00:57:05,440 --> 00:57:09,730
can process them together so that makes

00:57:07,780 --> 00:57:11,500
sense so that's kind of similar to what

00:57:09,730 --> 00:57:14,320
you do when you do is read partitioning

00:57:11,500 --> 00:57:18,460
and/or inspark where you move data

00:57:14,320 --> 00:57:19,960
around and you can spark there's a lot

00:57:18,460 --> 00:57:21,940
of this sort of under the hood for you

00:57:19,960 --> 00:57:24,370
in a more sense but in conference rooms

00:57:21,940 --> 00:57:26,320
you need to be more explicit in terms of

00:57:24,370 --> 00:57:30,790
what you want to group by or how you

00:57:26,320 --> 00:57:33,220
want to group your events so this what

00:57:30,790 --> 00:57:34,900
happens then in in Kafka streams is that

00:57:33,220 --> 00:57:37,810
it will actually create an intermediate

00:57:34,900 --> 00:57:41,260
topic for you that you write to with

00:57:37,810 --> 00:57:43,540
this new set of keys so that the events

00:57:41,260 --> 00:57:46,840
end up together that you want to do

00:57:43,540 --> 00:57:49,390
aggregate this repartition will happen

00:57:46,840 --> 00:57:51,070
not only by this group by operation but

00:57:49,390 --> 00:57:53,170
by a bunch of different operations as

00:57:51,070 --> 00:57:55,900
well as we talked already looked at map

00:57:53,170 --> 00:57:57,880
valleys and flat map values they those

00:57:55,900 --> 00:57:59,860
don't touch the keys so that they did

00:57:57,880 --> 00:58:01,690
not trigger this tree partitioning but

00:57:59,860 --> 00:58:03,610
if you use map which allows you to

00:58:01,690 --> 00:58:05,920
change both the key and the value and

00:58:03,610 --> 00:58:07,810
similarly for flat map or any of these

00:58:05,920 --> 00:58:10,480
other operations by select key group by

00:58:07,810 --> 00:58:14,740
key and so on they will trigger this

00:58:10,480 --> 00:58:18,370
repartition and that enough sort of move

00:58:14,740 --> 00:58:20,310
data across your gopher cluster so that

00:58:18,370 --> 00:58:23,020
you you get everything together all

00:58:20,310 --> 00:58:25,130
right so back to our code so we've done

00:58:23,020 --> 00:58:28,549
this group by operation here you

00:58:25,130 --> 00:58:31,460
back a group dream and this has a new

00:58:28,549 --> 00:58:33,980
set of message that we can apply and the

00:58:31,460 --> 00:58:38,420
simplest one in this case is we can do a

00:58:33,980 --> 00:58:40,640
call to account an account will return

00:58:38,420 --> 00:58:42,470
not the stream but a K table you

00:58:40,640 --> 00:58:44,960
remember we mentioned briefly that you

00:58:42,470 --> 00:58:47,799
have this duality of kind of streams of

00:58:44,960 --> 00:58:50,660
events and tables with aggregated state

00:58:47,799 --> 00:58:53,170
so in this case we will create a table

00:58:50,660 --> 00:58:55,940
which aggregates the state for each key

00:58:53,170 --> 00:59:00,289
the string is the keys string here and

00:58:55,940 --> 00:59:03,079
it's this site ID we will have an

00:59:00,289 --> 00:59:07,190
increasing count of articles as we get

00:59:03,079 --> 00:59:09,200
new events coming in so this here we

00:59:07,190 --> 00:59:10,970
have sort of all of their events here

00:59:09,200 --> 00:59:12,769
you will have a stateful store inside

00:59:10,970 --> 00:59:13,940
Kafka stream so that you can query and

00:59:12,769 --> 00:59:17,240
do things to it

00:59:13,940 --> 00:59:19,819
and also join with other data streams so

00:59:17,240 --> 00:59:22,369
we can again we can sort of how can we

00:59:19,819 --> 00:59:25,430
look at this well first of all we can of

00:59:22,369 --> 00:59:29,269
course also put this thing together as a

00:59:25,430 --> 00:59:31,309
single stream of transformations if we

00:59:29,269 --> 00:59:32,990
try to print this out it will typically

00:59:31,309 --> 00:59:35,180
show or something like this so it will

00:59:32,990 --> 00:59:38,859
not necessarily show us all the

00:59:35,180 --> 00:59:41,930
intermediate states as it also

00:59:38,859 --> 00:59:46,940
periodically output new aggregated

00:59:41,930 --> 00:59:48,140
values I mentioned this is a duality

00:59:46,940 --> 00:59:49,849
because you can actually look at the

00:59:48,140 --> 00:59:52,369
tables and streams as two different

00:59:49,849 --> 00:59:55,460
viewpoints on the data so if you have a

00:59:52,369 --> 00:59:58,400
database table you can easily create a

00:59:55,460 --> 01:00:01,609
stream of the changelog events for your

00:59:58,400 --> 01:00:03,769
data right so here we changed a row or

01:00:01,609 --> 01:00:06,589
add a row so we can output this as a

01:00:03,769 --> 01:00:07,970
changelog and then we add a new row we

01:00:06,589 --> 01:00:10,369
can output this as a change drug event

01:00:07,970 --> 01:00:11,960
now we change an existing row to new

01:00:10,369 --> 01:00:14,509
values so we just output a new event

01:00:11,960 --> 01:00:17,930
with the new value for that key and so

01:00:14,509 --> 01:00:19,789
on and this is very very much what your

01:00:17,930 --> 01:00:22,279
actual databases do right there are

01:00:19,789 --> 01:00:23,450
transaction logs and so on so if you if

01:00:22,279 --> 01:00:25,730
you think that your database actually

01:00:23,450 --> 01:00:27,230
has everything stored in tables like

01:00:25,730 --> 01:00:30,319
this it's typically that at least the

01:00:27,230 --> 01:00:34,970
latest changes are in a log kind of like

01:00:30,319 --> 01:00:38,920
this but you can do the opposite as well

01:00:34,970 --> 01:00:41,260
given this stream of change events

01:00:38,920 --> 01:00:42,549
you can recreate your table right that's

01:00:41,260 --> 01:00:44,109
the whole point in a database

01:00:42,549 --> 01:00:46,569
transaction log is that you can recreate

01:00:44,109 --> 01:00:49,660
your table States so you can sort of

01:00:46,569 --> 01:00:53,589
view these as two different views on the

01:00:49,660 --> 01:00:54,819
same data the difference is that in in

01:00:53,589 --> 01:00:57,460
the K table and typically in the

01:00:54,819 --> 01:01:00,940
database you only see the last version

01:00:57,460 --> 01:01:02,680
of your state but in your stream of

01:01:00,940 --> 01:01:07,450
events or you change log you have every

01:01:02,680 --> 01:01:09,819
change to that happen to your data so it

01:01:07,450 --> 01:01:11,650
can be both can be very useful and but

01:01:09,819 --> 01:01:14,200
this ability to go sort of back and

01:01:11,650 --> 01:01:27,670
forth is kind of core to have a kafka

01:01:14,200 --> 01:01:30,270
streams works all right yes I wanted to

01:01:27,670 --> 01:01:33,369
also talk a bit about windowed

01:01:30,270 --> 01:01:35,890
aggregations because those are very

01:01:33,369 --> 01:01:37,930
powerful so you can do aggregations just

01:01:35,890 --> 01:01:39,579
like across all time but typically you

01:01:37,930 --> 01:01:43,299
want to group them into some time

01:01:39,579 --> 01:01:44,589
windows so we've got streams lets you do

01:01:43,299 --> 01:01:46,930
that very easily you can specify

01:01:44,589 --> 01:01:49,390
different types of time windows you can

01:01:46,930 --> 01:01:51,670
have tumbling windows you could have

01:01:49,390 --> 01:01:53,470
time windows that overlap kind of like

01:01:51,670 --> 01:01:55,569
you just you have a 10-minute window and

01:01:53,470 --> 01:01:59,470
you move one minute ahead for example

01:01:55,569 --> 01:02:03,160
all the time and and it will

01:01:59,470 --> 01:02:06,010
automatically down together the records

01:02:03,160 --> 01:02:08,859
that belong together and you can do the

01:02:06,010 --> 01:02:11,530
aggregations within that time window so

01:02:08,859 --> 01:02:14,859
you can count the number of events per

01:02:11,530 --> 01:02:16,270
five minutes for example in this case or

01:02:14,859 --> 01:02:19,420
do some other kind of aggregation maybe

01:02:16,270 --> 01:02:20,530
you want to the count or some are sort

01:02:19,420 --> 01:02:22,299
of the simple ones but you can

01:02:20,530 --> 01:02:23,829
essentially do any sort of reduction

01:02:22,299 --> 01:02:28,030
operation that you want or aggregation

01:02:23,829 --> 01:02:30,220
operation you want on your your data as

01:02:28,030 --> 01:02:31,660
you do get event access to the raw

01:02:30,220 --> 01:02:35,950
events and you can combine them however

01:02:31,660 --> 01:02:39,339
you want so if you want to concatenate

01:02:35,950 --> 01:02:41,980
strings or do whatever build up a map of

01:02:39,339 --> 01:02:45,730
values seen in a given hour or something

01:02:41,980 --> 01:02:47,880
that's totally doable and then have that

01:02:45,730 --> 01:02:50,609
sort of output

01:02:47,880 --> 01:02:55,579
as a new changelog but with this time

01:02:50,609 --> 01:02:55,579
window information on a new Kafka topic

01:03:00,170 --> 01:03:04,920
right so let's have a look at what this

01:03:02,609 --> 01:03:06,569
window will look like so instead of

01:03:04,920 --> 01:03:09,569
having the simple count that we had

01:03:06,569 --> 01:03:14,150
earlier we want to have this count a

01:03:09,569 --> 01:03:16,680
number of articles seen per hour and

01:03:14,150 --> 01:03:18,470
instead of so we still do this group I

01:03:16,680 --> 01:03:22,470
hope raishin when you serve doing a

01:03:18,470 --> 01:03:25,319
count we will on our grouped stream we

01:03:22,470 --> 01:03:29,190
do a windowed buy and we give it a time

01:03:25,319 --> 01:03:31,980
in it in this case one hour and then we

01:03:29,190 --> 01:03:36,089
tell it to count and then we need to

01:03:31,980 --> 01:03:38,210
give it a state store so what it will do

01:03:36,089 --> 01:03:44,490
it will keep an internal state for this

01:03:38,210 --> 01:03:47,910
aggregation in an internal key value

01:03:44,490 --> 01:03:50,460
store essentially so by default Kafka

01:03:47,910 --> 01:03:52,710
streams uses rocks DB as its internal

01:03:50,460 --> 01:03:55,410
state so you can configure it to just

01:03:52,710 --> 01:03:57,960
store stuff in memory or you can have it

01:03:55,410 --> 01:04:00,690
actually spill to disk if you are having

01:03:57,960 --> 01:04:03,619
large amount of state for example or if

01:04:00,690 --> 01:04:06,450
you wanna have more durable durability

01:04:03,619 --> 01:04:09,269
but even even if your the cool thing

01:04:06,450 --> 01:04:11,700
about this is that been sort of behind

01:04:09,269 --> 01:04:14,130
the curve behind the scene here Kafka

01:04:11,700 --> 01:04:15,539
streams will create not only this

01:04:14,130 --> 01:04:16,890
internal state store in the key value

01:04:15,539 --> 01:04:20,069
store but it will create a change log

01:04:16,890 --> 01:04:23,099
for that key value store as a stream of

01:04:20,069 --> 01:04:24,750
events so you can always sorry if even

01:04:23,099 --> 01:04:25,769
if your note goes down and you need to

01:04:24,750 --> 01:04:29,549
take it back up again

01:04:25,769 --> 01:04:31,859
you can just consume again this change

01:04:29,549 --> 01:04:35,549
log and build up your state store and

01:04:31,859 --> 01:04:38,269
continue consume consuming firm from

01:04:35,549 --> 01:04:38,269
where you left off

01:04:43,610 --> 01:04:49,650
yes yeah so it's both represented the

01:04:48,240 --> 01:04:51,810
question was is it is it both

01:04:49,650 --> 01:04:53,970
represented as a table and as a topic

01:04:51,810 --> 01:04:56,220
yes that's exactly what it is so

01:04:53,970 --> 01:04:58,350
internally it's represented as a table

01:04:56,220 --> 01:05:03,900
in a key value store and as a changelog

01:04:58,350 --> 01:05:06,620
topic with the changes to that table and

01:05:03,900 --> 01:05:09,360
we saw when I showed briefly example of

01:05:06,620 --> 01:05:11,400
printing it out and that that was

01:05:09,360 --> 01:05:18,200
essentially that the changelog it

01:05:11,400 --> 01:05:20,460
printed out from this table okay so

01:05:18,200 --> 01:05:22,820
alright so we have this state store but

01:05:20,460 --> 01:05:26,310
what can we do with it

01:05:22,820 --> 01:05:30,540
well we can we can query it we can

01:05:26,310 --> 01:05:32,880
connect to it this is just a again it

01:05:30,540 --> 01:05:36,030
you can include kafka streams in any

01:05:32,880 --> 01:05:38,160
regular java application so typically

01:05:36,030 --> 01:05:39,990
what you will do is you will have an

01:05:38,160 --> 01:05:42,900
application with kafka streams embedded

01:05:39,990 --> 01:05:44,940
in it and then you can just query that

01:05:42,900 --> 01:05:48,720
using for example a regular REST API

01:05:44,940 --> 01:05:50,880
that you provide and that REST API can

01:05:48,720 --> 01:05:56,730
query the internal state stores inside

01:05:50,880 --> 01:05:58,980
of your Kafka streams app so remember we

01:05:56,730 --> 01:06:02,730
gave we materialize this thing as a kit

01:05:58,980 --> 01:06:07,200
as a as a state store and we can then

01:06:02,730 --> 01:06:09,840
also query that so I can ask my Kafka

01:06:07,200 --> 01:06:12,600
streams runtime for the state store

01:06:09,840 --> 01:06:14,760
named articles per hour of type window

01:06:12,600 --> 01:06:18,630
store and I will get back a query Ball

01:06:14,760 --> 01:06:22,710
State store so from this I can say okay

01:06:18,630 --> 01:06:25,500
I want to look at all the accounts of

01:06:22,710 --> 01:06:27,630
articles for BBC from the start of time

01:06:25,500 --> 01:06:31,800
which is in this case is January 1st

01:06:27,630 --> 01:06:33,860
1970 or epoch until now and then this

01:06:31,800 --> 01:06:36,660
will give me a list of all of the

01:06:33,860 --> 01:06:41,250
aggregated states it has in the in the

01:06:36,660 --> 01:06:44,520
state store so this iterator

01:06:41,250 --> 01:06:47,250
I can iterate over using the regular

01:06:44,520 --> 01:06:49,560
mechanisms in Java so for each key value

01:06:47,250 --> 01:06:50,940
I can extract the key and the value and

01:06:49,560 --> 01:06:53,640
just print out for example in this case

01:06:50,940 --> 01:06:55,650
or I could respond to the REST API call

01:06:53,640 --> 01:06:58,530
with a set of JSON objects so

01:06:55,650 --> 01:07:02,430
whatever in this case I just print out

01:06:58,530 --> 01:07:04,050
two standard out so I only have one time

01:07:02,430 --> 01:07:05,520
window populated here so that's all it's

01:07:04,050 --> 01:07:07,770
gonna give me but if I had not full ones

01:07:05,520 --> 01:07:11,700
it would show me a list of up it so you

01:07:07,770 --> 01:07:15,480
can you can have your rest the API then

01:07:11,700 --> 01:07:22,470
fetch the state stores query for data

01:07:15,480 --> 01:07:23,880
and respond with with that now this is

01:07:22,470 --> 01:07:26,880
simple if you have all of your data on

01:07:23,880 --> 01:07:30,150
one node right but typically you will

01:07:26,880 --> 01:07:31,350
have a cuff custom zap or very often at

01:07:30,150 --> 01:07:36,480
least if you'll have it distributed

01:07:31,350 --> 01:07:38,820
across multiple nodes so the challenge

01:07:36,480 --> 01:07:40,770
then is that your state will actually be

01:07:38,820 --> 01:07:42,720
distributed as well since each node

01:07:40,770 --> 01:07:46,050
along you know about the data given from

01:07:42,720 --> 01:07:48,860
the partitions that it is consuming so

01:07:46,050 --> 01:07:53,510
you might need to query across your

01:07:48,860 --> 01:07:55,980
consumers and this is kind of where

01:07:53,510 --> 01:07:57,990
Kafka streams doesn't really give you

01:07:55,980 --> 01:08:02,250
everything for free but you still have

01:07:57,990 --> 01:08:05,010
API so you can query to see where is the

01:08:02,250 --> 01:08:06,360
data located for this given key and so

01:08:05,010 --> 01:08:08,460
on but you might need to implement

01:08:06,360 --> 01:08:09,990
things like retries and stuff if a node

01:08:08,460 --> 01:08:11,850
suddenly goes down as you're trying to

01:08:09,990 --> 01:08:13,560
query it and move data moves around so

01:08:11,850 --> 01:08:16,410
you need to deal with those kind of

01:08:13,560 --> 01:08:18,090
things on yourself yourself but if you

01:08:16,410 --> 01:08:20,940
have something that lives in just a

01:08:18,090 --> 01:08:23,490
single node and it's quite easy to to do

01:08:20,940 --> 01:08:25,350
this and it depends on the amount of

01:08:23,490 --> 01:08:35,300
data that you need to process an

01:08:25,350 --> 01:08:35,300
aggregate of course it's yes

01:08:37,839 --> 01:08:43,420
yes exactly so the state for a given key

01:08:40,690 --> 01:08:45,589
if you if you add multiple consumers or

01:08:43,420 --> 01:08:48,109
if you add additional consumers or

01:08:45,589 --> 01:08:55,759
remove some your data or state will kind

01:08:48,109 --> 01:08:59,210
of move around right that your yeah

01:08:55,759 --> 01:09:01,520
excuse me so when you go to build your

01:08:59,210 --> 01:09:05,659
state store again your state stores

01:09:01,520 --> 01:09:07,099
gonna come from Kafka and rebuild itself

01:09:05,659 --> 01:09:10,339
with the partitions that are associated

01:09:07,099 --> 01:09:12,799
with that particular client yeah so what

01:09:10,339 --> 01:09:15,909
the the new node that it's added for

01:09:12,799 --> 01:09:18,139
example it will read this changelog

01:09:15,909 --> 01:09:19,489
building up its internal states or and

01:09:18,139 --> 01:09:22,219
when that is ready it will sort of take

01:09:19,489 --> 01:09:26,179
over processing that that partition I

01:09:22,219 --> 01:09:28,489
believe and of course then the client I

01:09:26,179 --> 01:09:30,770
was the query needs to ask Kafka where

01:09:28,489 --> 01:09:33,230
or they use the Kafka API to figure out

01:09:30,770 --> 01:09:37,639
which of these nodes has that

01:09:33,230 --> 01:09:38,079
information I'm interested in does that

01:09:37,639 --> 01:09:45,559
make sense

01:09:38,079 --> 01:09:47,869
cool now I have a certain set of

01:09:45,559 --> 01:09:49,400
exercises but I don't know if we should

01:09:47,869 --> 01:09:53,779
go into those right now I think that we

01:09:49,400 --> 01:09:55,039
can let me see I have actually have a

01:09:53,779 --> 01:10:02,449
couple of slides I wanted to go through

01:09:55,039 --> 01:10:04,699
in addition to this I would rather spend

01:10:02,449 --> 01:10:10,309
time I think if you guys have questions

01:10:04,699 --> 01:10:14,030
and stuff but so another thing is in

01:10:10,309 --> 01:10:18,409
addition to just occurring the state

01:10:14,030 --> 01:10:20,929
stores for example you might want to get

01:10:18,409 --> 01:10:22,750
data into other systems or out of other

01:10:20,929 --> 01:10:25,520
systems

01:10:22,750 --> 01:10:28,969
cough-cough streams itself only supports

01:10:25,520 --> 01:10:31,940
Kafka right so you can only consume from

01:10:28,969 --> 01:10:34,159
Kafka and produce to Kafka this is

01:10:31,940 --> 01:10:35,300
different from a lot of the other stream

01:10:34,159 --> 01:10:39,260
processing frameworks that support

01:10:35,300 --> 01:10:41,449
multiple data sources and data sinks but

01:10:39,260 --> 01:10:43,190
but they've chosen to keep Kafka streams

01:10:41,449 --> 01:10:45,079
as simple as possible only the image

01:10:43,190 --> 01:10:47,349
Kafka and they have other mechanisms for

01:10:45,079 --> 01:10:51,940
getting data in and out of Kafka itself

01:10:47,349 --> 01:10:54,710
so in particular confluent and i Kanaka

01:10:51,940 --> 01:10:57,199
open source project as well

01:10:54,710 --> 01:10:59,570
provides this component called Kafka

01:10:57,199 --> 01:11:01,429
Connect which allows you to connect the

01:10:59,570 --> 01:11:04,099
different data sources and things so it

01:11:01,429 --> 01:11:07,420
could be a database it could be s3 for

01:11:04,099 --> 01:11:10,699
example and so on and it will you can

01:11:07,420 --> 01:11:12,260
deploy various connectors into this

01:11:10,699 --> 01:11:15,159
Kafka connect cluster and it will take

01:11:12,260 --> 01:11:21,500
care of pushing data in and out of Kafka

01:11:15,159 --> 01:11:23,570
for you and it will for for certain

01:11:21,500 --> 01:11:25,039
setups it will also sort of give you

01:11:23,570 --> 01:11:28,730
these kind of guarantees that you have

01:11:25,039 --> 01:11:31,599
in for exactly once processing for but

01:11:28,730 --> 01:11:35,239
that I think only if you're using Kafka

01:11:31,599 --> 01:11:38,420
mostly Kafka intend with some additional

01:11:35,239 --> 01:11:40,159
support but but this works very nice I

01:11:38,420 --> 01:11:43,190
think for for connecting to various

01:11:40,159 --> 01:11:44,719
sources and sending data to downstream

01:11:43,190 --> 01:11:48,469
consumers that are not consuming from

01:11:44,719 --> 01:11:49,880
Kafka the other alternative instead of

01:11:48,469 --> 01:11:53,210
using Africa Connect is of course to use

01:11:49,880 --> 01:11:55,610
the other parts of the calf ecosystem so

01:11:53,210 --> 01:11:59,719
Kafka consists of many things you have

01:11:55,610 --> 01:12:02,179
the cluster the server part of it then

01:11:59,719 --> 01:12:05,269
you have sort of the regular Kafka

01:12:02,179 --> 01:12:06,860
producers and consumers which are Java

01:12:05,269 --> 01:12:08,750
client so no they I believe they also

01:12:06,860 --> 01:12:11,090
have support for a number of other

01:12:08,750 --> 01:12:13,099
languages you have the stream processors

01:12:11,090 --> 01:12:15,590
which is what we have looked at now

01:12:13,099 --> 01:12:17,659
which are kind of combined producers and

01:12:15,590 --> 01:12:19,429
consumers they just they both consume

01:12:17,659 --> 01:12:21,110
data from craft companies to it so

01:12:19,429 --> 01:12:23,809
that's the Kafka stream API and then you

01:12:21,110 --> 01:12:27,949
have this this connector API that I just

01:12:23,809 --> 01:12:31,159
mentioned so you can write things using

01:12:27,949 --> 01:12:33,139
regular producers and consumers and this

01:12:31,159 --> 01:12:36,699
is actually what we are doing in our set

01:12:33,139 --> 01:12:40,599
up is a we had a legacy stream based

01:12:36,699 --> 01:12:43,969
solution already supported a number of

01:12:40,599 --> 01:12:46,699
the things in such that we wanted so we

01:12:43,969 --> 01:12:49,429
just sort of migrated that code to use

01:12:46,699 --> 01:12:51,889
Kafka clients to consume from from Kafka

01:12:49,429 --> 01:12:55,340
itself and that's quite simple it

01:12:51,889 --> 01:12:57,469
most of our our connectors are just one

01:12:55,340 --> 01:12:59,840
or two pages of code essentially just

01:12:57,469 --> 01:13:02,749
going in a loop fetching data from craft

01:12:59,840 --> 01:13:04,749
girl riding it somewhere else so that's

01:13:02,749 --> 01:13:07,249
totally doable

01:13:04,749 --> 01:13:08,599
of course you still have the option of

01:13:07,249 --> 01:13:12,760
using Kafka Connect if you want to

01:13:08,599 --> 01:13:15,530
Avista so the full full ecosystem I

01:13:12,760 --> 01:13:17,449
really you had a question about s3 right

01:13:15,530 --> 01:13:19,789
if you wanted to move data to s3 for

01:13:17,449 --> 01:13:23,019
example so caf-co connect supports that

01:13:19,789 --> 01:13:23,019
I believe there are also some other

01:13:24,070 --> 01:13:28,539
other third-party solutions to do that

01:13:26,539 --> 01:13:35,269
but I believe those are using the old

01:13:28,539 --> 01:13:36,670
Kafka library or old Kafka versions but

01:13:35,269 --> 01:13:39,559
what we are doing is essentially just

01:13:36,670 --> 01:13:41,719
fetching a batch of events from Kafka

01:13:39,559 --> 01:13:54,530
and writing them as a micro batch into

01:13:41,719 --> 01:13:55,670
s3 in our system all right yeah I don't

01:13:54,530 --> 01:13:57,409
think we have time to do anything useful

01:13:55,670 --> 01:14:00,739
here but if you are interested in more

01:13:57,409 --> 01:14:02,900
details about our platform you can watch

01:14:00,739 --> 01:14:05,420
a couple of talks that are available

01:14:02,900 --> 01:14:09,590
also this is my colleague large Mario's

01:14:05,420 --> 01:14:11,300
he did talk at Java Sohn so actually two

01:14:09,590 --> 01:14:14,269
years ago but it still covers most of

01:14:11,300 --> 01:14:16,130
our platform how we ingest in process

01:14:14,269 --> 01:14:20,239
seven or eight hundred million events

01:14:16,130 --> 01:14:22,849
per day from our clickstream data and

01:14:20,239 --> 01:14:25,099
then some of you already saw my

01:14:22,849 --> 01:14:27,800
colleague who cool yesterday he was

01:14:25,099 --> 01:14:31,999
talking about doing this enrichment of

01:14:27,800 --> 01:14:35,599
the events using IP to geo coordinates

01:14:31,999 --> 01:14:37,460
look up he also I'm assuming that that

01:14:35,599 --> 01:14:39,380
the version that he gave yesterday will

01:14:37,460 --> 01:14:44,239
be available at the video from that as

01:14:39,380 --> 01:14:47,840
well soon so see I'm actually quite

01:14:44,239 --> 01:14:49,849
ahead of time but there's a bunch of

01:14:47,840 --> 01:14:52,429
further reading that I would recommend

01:14:49,849 --> 01:14:53,960
if you are looking to go in to start

01:14:52,429 --> 01:14:57,349
using stream processing I would highly

01:14:53,960 --> 01:15:00,710
recommend these two articles the bat the

01:14:57,349 --> 01:15:03,469
world beyond batch they have a go into a

01:15:00,710 --> 01:15:05,329
lot of detail on some of the aspects

01:15:03,469 --> 01:15:07,820
that I've talked about today in terms of

01:15:05,329 --> 01:15:10,849
handling of time how you handle

01:15:07,820 --> 01:15:12,800
events that are coming in late and so on

01:15:10,849 --> 01:15:15,949
and and going into very good

01:15:12,800 --> 01:15:18,020
explanations I think on how that works

01:15:15,949 --> 01:15:21,889
confluent the commercial package behind

01:15:18,020 --> 01:15:23,750
Kafka also have a very good blog I think

01:15:21,889 --> 01:15:26,949
with a lot of good articles on how to

01:15:23,750 --> 01:15:30,139
use cough cough cough cough screams I

01:15:26,949 --> 01:15:32,090
didn't talk about joints now but you can

01:15:30,139 --> 01:15:33,739
if you guys want we can we can go into

01:15:32,090 --> 01:15:36,460
some examples of that as well I have

01:15:33,739 --> 01:15:39,590
some slides I can I can show if you want

01:15:36,460 --> 01:15:41,179
because the talk by my colleague

01:15:39,590 --> 01:15:43,820
yesterday showed some examples of how

01:15:41,179 --> 01:15:45,679
that didn't work so well for us but I

01:15:43,820 --> 01:15:51,310
can I can show some examples of how to

01:15:45,679 --> 01:16:00,520
do that's in the streams API I just

01:15:51,310 --> 01:16:00,520
unhide some of my slides here yeah so

01:16:00,880 --> 01:16:06,380
joining streams and enriching your data

01:16:04,639 --> 01:16:08,239
since we have about ten minutes left I

01:16:06,380 --> 01:16:12,860
think that's a good topic to dive into a

01:16:08,239 --> 01:16:16,219
little bit so don't use dreams do not

01:16:12,860 --> 01:16:18,440
cross your streams right all right but

01:16:16,219 --> 01:16:20,540
sometimes you do want that and and it it

01:16:18,440 --> 01:16:23,840
has very powerful mechanisms for kind of

01:16:20,540 --> 01:16:26,480
joining data together you can do

01:16:23,840 --> 01:16:31,400
multiple types of joints in side of

01:16:26,480 --> 01:16:33,139
Kafka's room so here I'm not going to go

01:16:31,400 --> 01:16:36,110
into detail on all of this because

01:16:33,139 --> 01:16:39,380
that's a long blog post on that content

01:16:36,110 --> 01:16:41,900
plug-in itself but you can join 2k

01:16:39,380 --> 01:16:44,889
streams together or you can join various

01:16:41,900 --> 01:16:46,760
combinations of K streams and K tables

01:16:44,889 --> 01:16:48,590
and you have also this thing called

01:16:46,760 --> 01:16:50,719
global K table which I haven't touched

01:16:48,590 --> 01:16:53,090
on today really but it's essentially a K

01:16:50,719 --> 01:16:56,500
table that has the same state across all

01:16:53,090 --> 01:16:58,429
nodes so if you have some sort of

01:16:56,500 --> 01:17:00,500
metadata that you want to enrich

01:16:58,429 --> 01:17:02,389
everything all the air events with you

01:17:00,500 --> 01:17:04,400
will put that isn't that big you will

01:17:02,389 --> 01:17:05,659
put that in the global 2k table and all

01:17:04,400 --> 01:17:09,320
of your notes will have access to the

01:17:05,659 --> 01:17:12,230
same same data and you can do different

01:17:09,320 --> 01:17:14,989
types of joins here if you're familiar

01:17:12,230 --> 01:17:17,869
with joins from this from sequel and you

01:17:14,989 --> 01:17:19,489
know like you do in your Joyce left

01:17:17,869 --> 01:17:20,489
joins and so on depending and so an

01:17:19,489 --> 01:17:22,369
inner join will

01:17:20,489 --> 01:17:26,119
join the events if they are if you have

01:17:22,369 --> 01:17:30,510
events that match that coming from both

01:17:26,119 --> 01:17:32,309
both sides a left join will join

01:17:30,510 --> 01:17:34,769
anything coming in on the left side

01:17:32,309 --> 01:17:36,599
whatever it has on the right side and an

01:17:34,769 --> 01:17:38,579
outer join will join all combinations

01:17:36,599 --> 01:17:41,579
together so giving you potential null

01:17:38,579 --> 01:17:43,170
values here and there but depending on

01:17:41,579 --> 01:17:43,769
what you want that's that's what's

01:17:43,170 --> 01:17:47,659
available

01:17:43,769 --> 01:17:52,889
so that's dime into short example here

01:17:47,659 --> 01:17:58,349
so staying on the same kind of topic

01:17:52,889 --> 01:18:01,789
with our news articles I want to see so

01:17:58,349 --> 01:18:04,590
I have a set of click events essentially

01:18:01,789 --> 01:18:08,639
articles being read and I have some

01:18:04,590 --> 01:18:12,840
information about my users my logged in

01:18:08,639 --> 01:18:15,019
users in the table so I want to join

01:18:12,840 --> 01:18:17,849
these two together to enrich these

01:18:15,019 --> 01:18:21,539
article event or click events with some

01:18:17,849 --> 01:18:26,159
more information so in this case I'm

01:18:21,539 --> 01:18:28,980
trying to get the reads the number of

01:18:26,159 --> 01:18:33,510
eats by country right so I know which

01:18:28,980 --> 01:18:34,409
country my users are are from so I can

01:18:33,510 --> 01:18:38,010
do this

01:18:34,409 --> 01:18:41,269
I can do a join of this table with this

01:18:38,010 --> 01:18:45,929
stream so what I will have done is that

01:18:41,269 --> 01:18:49,260
I will have article and user events

01:18:45,929 --> 01:18:54,449
coming in to your ni I need to tell it

01:18:49,260 --> 01:18:55,889
where yeah what is the output of that

01:18:54,449 --> 01:18:59,130
join and in this case I'm just

01:18:55,889 --> 01:19:04,469
interested in the users the country of

01:18:59,130 --> 01:19:07,949
the user and then I can take this reads

01:19:04,469 --> 01:19:12,469
by country and do what we saw earlier I

01:19:07,949 --> 01:19:17,120
can do a group by that

01:19:12,469 --> 01:19:21,350
the country name and the account right

01:19:17,120 --> 01:19:25,219
the the trick here to be able to do this

01:19:21,350 --> 01:19:30,469
join these two have to have the same

01:19:25,219 --> 01:19:31,820
keys to be able to fork of code to be

01:19:30,469 --> 01:19:35,300
able to join them you have to use the

01:19:31,820 --> 01:19:37,910
same keys in both your left and right

01:19:35,300 --> 01:19:39,530
hand side here so so what I need to do

01:19:37,910 --> 01:19:42,110
potentially here if it isn't already

01:19:39,530 --> 01:19:45,500
using the user ID as a key I will need

01:19:42,110 --> 01:19:48,080
to do this kind of reelect key operation

01:19:45,500 --> 01:19:50,420
I mentioned briefly before so that I can

01:19:48,080 --> 01:19:53,710
get a new stream that has the same keys

01:19:50,420 --> 01:19:58,550
as my user table because I'm making

01:19:53,710 --> 01:20:00,170
first of all it will ensure that all of

01:19:58,550 --> 01:20:02,180
the events that are supposed to be join

01:20:00,170 --> 01:20:03,980
again end up on the same processing node

01:20:02,180 --> 01:20:10,760
similar to when we did the aggregation

01:20:03,980 --> 01:20:12,199
operation but once that is satisfied it

01:20:10,760 --> 01:20:16,940
will do this join out automatically

01:20:12,199 --> 01:20:21,620
output in this case all of the the

01:20:16,940 --> 01:20:23,660
countries from this from for each event

01:20:21,620 --> 01:20:26,840
coming in here it will look up in the

01:20:23,660 --> 01:20:29,510
table find the appropriate user object

01:20:26,840 --> 01:20:31,310
and I can extract the country from there

01:20:29,510 --> 01:20:35,199
and produce essentially a stream that

01:20:31,310 --> 01:20:39,949
just contains a list of countries right

01:20:35,199 --> 01:20:44,540
so for this I see normally Norway Spain

01:20:39,949 --> 01:20:45,949
Germany France and so on and then I can

01:20:44,540 --> 01:20:50,050
group by that and do the count an

01:20:45,949 --> 01:20:53,050
aggregation does that make sense

01:20:50,050 --> 01:20:53,050
hopefully

01:20:53,080 --> 01:20:59,360
joints are quite powerful and and it's

01:20:57,560 --> 01:21:02,270
taken me quite a while to sort of

01:20:59,360 --> 01:21:05,240
understand how they how they work how

01:21:02,270 --> 01:21:07,220
they actually do this based on since

01:21:05,240 --> 01:21:11,660
essentially when you're dealing with

01:21:07,220 --> 01:21:13,610
streams of events of course it's not the

01:21:11,660 --> 01:21:15,650
same as joining a database table where

01:21:13,610 --> 01:21:18,230
all of your data is present the events

01:21:15,650 --> 01:21:20,720
come in at different times right so when

01:21:18,230 --> 01:21:22,040
you're joining this K table with K

01:21:20,720 --> 01:21:23,990
stream that's kind of simple because it

01:21:22,040 --> 01:21:25,100
will only for each event coming in or

01:21:23,990 --> 01:21:28,040
the stream it will just look up the

01:21:25,100 --> 01:21:29,870
corresponding value in their aggregate

01:21:28,040 --> 01:21:31,820
state store but if you're joining two

01:21:29,870 --> 01:21:34,100
streams together you need to deal with

01:21:31,820 --> 01:21:36,380
what is the time window I'm looking at

01:21:34,100 --> 01:21:38,150
for a join what is my join time window

01:21:36,380 --> 01:21:40,040
and so on so you can configure that in

01:21:38,150 --> 01:21:42,350
your joints saying I want to join every

01:21:40,040 --> 01:21:55,120
matching event that comes in within five

01:21:42,350 --> 01:22:03,830
minutes for example style minibike great

01:21:55,120 --> 01:22:06,850
yeah I was asking how do you make sure

01:22:03,830 --> 01:22:09,800
that an instance of your application

01:22:06,850 --> 01:22:13,250
gets the same keys from the partitions

01:22:09,800 --> 01:22:15,290
from the different topics yeah that's a

01:22:13,250 --> 01:22:20,300
good question so how do you ensure that

01:22:15,290 --> 01:22:22,640
the the same so the articles for a user

01:22:20,300 --> 01:22:25,580
match up on the same note as the user

01:22:22,640 --> 01:22:28,160
data for that user essentially yes so

01:22:25,580 --> 01:22:29,300
the trick is of course there's two

01:22:28,160 --> 01:22:30,860
things you need to do first of all you

01:22:29,300 --> 01:22:32,900
need to ensure that you have the same

01:22:30,860 --> 01:22:36,170
keys on the events as I mentioned so you

01:22:32,900 --> 01:22:39,440
need to rekey your article read events

01:22:36,170 --> 01:22:42,140
with the user ID for that event in

01:22:39,440 --> 01:22:43,910
addition you need to ensure that your if

01:22:42,140 --> 01:22:45,440
you're joining two streams for example

01:22:43,910 --> 01:22:47,650
you need to ensure that both streams

01:22:45,440 --> 01:22:50,540
have the same number of partitions

01:22:47,650 --> 01:22:52,370
otherwise if one stream has five

01:22:50,540 --> 01:22:53,720
partitions and the other has seven and

01:22:52,370 --> 01:22:56,810
it will not be able to sort of match

01:22:53,720 --> 01:22:58,520
them up so so that's a those are the two

01:22:56,810 --> 01:23:00,680
kind of considerations you need to to

01:22:58,520 --> 01:23:03,850
make there and and you can do this kind

01:23:00,680 --> 01:23:07,240
of inside of Kafka streams

01:23:03,850 --> 01:23:13,390
in the rear partitioning right there was

01:23:07,240 --> 01:23:17,140
the question up here as well yeah so do

01:23:13,390 --> 01:23:21,940
each one of these operations emit a

01:23:17,140 --> 01:23:23,440
subsequent stream to Kafka so in the

01:23:21,940 --> 01:23:25,570
joint operation for the joint operation

01:23:23,440 --> 01:23:28,150
here will emit essentially one event per

01:23:25,570 --> 01:23:29,620
event coming in on the stream here but

01:23:28,150 --> 01:23:31,780
if you are joining two streams together

01:23:29,620 --> 01:23:34,150
you can have multiple events as there

01:23:31,780 --> 01:23:37,240
can be multiple matches inside of the

01:23:34,150 --> 01:23:40,180
time the join window so and then the

01:23:37,240 --> 01:23:41,920
output of the join is that another

01:23:40,180 --> 01:23:44,380
stream yeah that's another stream

01:23:41,920 --> 01:23:46,630
exactly here it's just a case stream it

01:23:44,380 --> 01:23:49,210
may be materialized inside Africa

01:23:46,630 --> 01:23:50,560
streams or inside your calculus or not

01:23:49,210 --> 01:23:52,780
it might just be internally in your

01:23:50,560 --> 01:23:54,930
Kafka streams application although I

01:23:52,780 --> 01:23:56,880
think it will actually have it be

01:23:54,930 --> 01:23:59,020
materialized

01:23:56,880 --> 01:24:00,640
so there's I'm not going to go into

01:23:59,020 --> 01:24:03,610
details on the joints themselves but

01:24:00,640 --> 01:24:06,940
there's again these articles I mentioned

01:24:03,610 --> 01:24:09,160
there's another one here that I had in

01:24:06,940 --> 01:24:10,810
my read list goes into a lot of detail

01:24:09,160 --> 01:24:13,510
if you want to figure out how joints

01:24:10,810 --> 01:24:15,670
work this has a lot of examples on what

01:24:13,510 --> 01:24:17,200
is the incoming events what is the

01:24:15,670 --> 01:24:18,490
output depending on whether you doing an

01:24:17,200 --> 01:24:23,140
inner stream drawing or an outer join

01:24:18,490 --> 01:24:26,080
our table join and so on so that's yeah

01:24:23,140 --> 01:24:30,550
that's the topic for a whole talk on our

01:24:26,080 --> 01:24:32,740
workshop on its own enrichments so you

01:24:30,550 --> 01:24:34,090
can also of course in rich events with

01:24:32,740 --> 01:24:35,770
the information from external sources

01:24:34,090 --> 01:24:43,270
and this is what my colleague talks

01:24:35,770 --> 01:24:45,850
about yesterday because essentially your

01:24:43,270 --> 01:24:48,130
spark code or sorry with your costumes

01:24:45,850 --> 01:24:49,420
code it's just regular Java code you can

01:24:48,130 --> 01:24:51,310
do whatever you want inside your Mac

01:24:49,420 --> 01:24:54,100
values for example you can call out to a

01:24:51,310 --> 01:24:56,620
third-party service fetch some data and

01:24:54,100 --> 01:24:59,620
add that to your event and that's your

01:24:56,620 --> 01:25:01,510
enrichment you also have this more

01:24:59,620 --> 01:25:03,700
low-level API that I mentioned that it's

01:25:01,510 --> 01:25:05,140
called the processor API or there are

01:25:03,700 --> 01:25:07,960
two of them there's one called processor

01:25:05,140 --> 01:25:10,180
and one called transformer they're very

01:25:07,960 --> 01:25:13,210
similar but sort of different levels of

01:25:10,180 --> 01:25:16,250
the API and you can use that for more

01:25:13,210 --> 01:25:18,080
backstory nted processing where you have

01:25:16,250 --> 01:25:19,880
you can collect a set of events and you

01:25:18,080 --> 01:25:21,380
have some punctuation calls and stuff

01:25:19,880 --> 01:25:22,610
like that so you can you can do more

01:25:21,380 --> 01:25:25,130
better ain't it

01:25:22,610 --> 01:25:26,510
if you want a for example batch events

01:25:25,130 --> 01:25:28,240
up before you call the service instead

01:25:26,510 --> 01:25:30,560
of calling it for each individual event

01:25:28,240 --> 01:25:32,960
so I believe that's what the one we are

01:25:30,560 --> 01:25:34,400
actually using now for our GOI B

01:25:32,960 --> 01:25:36,050
enrichment instead of doing the joints

01:25:34,400 --> 01:25:42,620
we're doing this this kind of enrichment

01:25:36,050 --> 01:25:44,750
instead alright so yeah we went through

01:25:42,620 --> 01:25:46,150
this thing already there's a lot of good

01:25:44,750 --> 01:25:48,800
documentation on the confluent site

01:25:46,150 --> 01:25:52,460
confluent a tile site on Kafka streams

01:25:48,800 --> 01:25:54,680
and Kafka in general you also have

01:25:52,460 --> 01:26:01,040
similar documentation on the Kafka

01:25:54,680 --> 01:26:04,210
apache site as well so with that I say

01:26:01,040 --> 01:26:04,210
thank you for listening

01:26:08,679 --> 01:26:11,830
I think we have three more minutes if

01:26:10,630 --> 01:26:18,310
there any more questions at the end

01:26:11,830 --> 01:26:21,670
thank you this one here yes I'm involved

01:26:18,310 --> 01:26:23,560
in the Apache agent project and I was

01:26:21,670 --> 01:26:25,270
sort of while you were talking I was

01:26:23,560 --> 01:26:29,199
sort of comparing it with what we're

01:26:25,270 --> 01:26:31,300
doing there so it seems to be a perfect

01:26:29,199 --> 01:26:34,150
addition to Kafka streams because

01:26:31,300 --> 01:26:36,310
especially with IOT and edge devices and

01:26:34,150 --> 01:26:39,190
usually don't have a cluster to work

01:26:36,310 --> 01:26:42,790
with yeah yeah so having almost the same

01:26:39,190 --> 01:26:44,820
syntax just without the key concept it

01:26:42,790 --> 01:26:48,280
seems like a perfect addition to sort of

01:26:44,820 --> 01:26:50,679
build your IOT applications yeah so the

01:26:48,280 --> 01:26:52,300
first project I worked with the Kafka

01:26:50,679 --> 01:26:53,969
was in an IOT setting where we had

01:26:52,300 --> 01:26:56,409
sensors sending in events from

01:26:53,969 --> 01:26:58,599
households about power usage and stuff

01:26:56,409 --> 01:27:01,780
like that so it was a quite perfect fit

01:26:58,599 --> 01:27:09,190
for that actually yeah any more

01:27:01,780 --> 01:27:10,440
questions yeah I don't quite understand

01:27:09,190 --> 01:27:12,760
Oh

01:27:10,440 --> 01:27:14,469
rapid stream would work without being

01:27:12,760 --> 01:27:17,139
window in time

01:27:14,469 --> 01:27:22,719
don't you need a concept of start and

01:27:17,139 --> 01:27:24,489
end to be able to aggregate data you can

01:27:22,719 --> 01:27:25,869
do aggregations in time windows but you

01:27:24,489 --> 01:27:28,780
can also sort of just your continuous

01:27:25,869 --> 01:27:32,619
aggregation so from from the first event

01:27:28,780 --> 01:27:36,159
that you consume until and so what will

01:27:32,619 --> 01:27:37,599
happen in both global aggregations and

01:27:36,159 --> 01:27:39,580
in the time window aggregations is that

01:27:37,599 --> 01:27:43,599
well it Kafka will periodically produce

01:27:39,580 --> 01:27:45,639
new incremental results right so even in

01:27:43,599 --> 01:27:47,770
the time window once as I mentioned how

01:27:45,639 --> 01:27:49,900
do you handle out of time other sort of

01:27:47,770 --> 01:27:51,730
late events coming in that actually

01:27:49,900 --> 01:27:54,250
belong to an earlier time window so you

01:27:51,730 --> 01:27:55,810
can configure how long you are able to

01:27:54,250 --> 01:27:57,639
deal with those events how long should

01:27:55,810 --> 01:27:59,139
you keep that time window around and if

01:27:57,639 --> 01:28:01,030
it sees new events coming in it will

01:27:59,139 --> 01:28:03,340
actually publish an updated version of

01:28:01,030 --> 01:28:05,020
that time window to its changelog so

01:28:03,340 --> 01:28:06,580
when you query the state store for an

01:28:05,020 --> 01:28:08,889
older time window you will get a new

01:28:06,580 --> 01:28:12,940
results and it's kind of similar for

01:28:08,889 --> 01:28:15,639
global aggregations without time windows

01:28:12,940 --> 01:28:17,130
it will just publish new results

01:28:15,639 --> 01:28:20,850
periodically for the

01:28:17,130 --> 01:28:25,710
does that make sense yeah cool we still

01:28:20,850 --> 01:28:28,380
have time for some questions if not I'll

01:28:25,710 --> 01:28:30,120
happily stay around outside here or

01:28:28,380 --> 01:28:34,159
somewhere and have a chat with you if

01:28:30,120 --> 01:28:36,000
you want to ask me more stuff so oh

01:28:34,159 --> 01:28:42,500
thank you again

01:28:36,000 --> 01:28:42,500

YouTube URL: https://www.youtube.com/watch?v=OwA_gpWt3xA


