Title: Berlin Buzzwords 2018: Michael Noll – distributed stream processing for everyone with KSQL #bbuzz
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Michael Noll talking about "Big Data, Fast Data, Easy Data: distributed stream processing for everyone with KSQL, the streaming SQL engine for Apache Kafka".

Modern businesses have data at their core, and this data is changing continuously. Stream processing is what allows you harness this torrent of information in real-time, and thousands of companies use Apache Kafka as the streaming platform to transform and reshape their industries. However, the world of stream processing still has a very high barrier to entry. 

Today’s most popular stream processing technologies require the user to write code in programming languages such as Java or Scala. This hard requirement on coding skills is preventing many companies to unlock the benefits of stream processing to their full effect.

However, imagine that instead of having to write a lot of code, all you’d need to get started with stream processing is a simple SQL statement, such as: SELECT* FROM payments-kafka-stream WHERE fraudProbability greater than 0.8, so that you can detect anomalies and fraudulent activities in data feeds, monitor application behavior and infrastructure, conduct session-based analysis of user activities, and perform real-time ETL.

In this talk, I introduce the audience to KSQL, the open source streaming SQL engine for Apache Kafka. KSQL provides an easy and completely interactive SQL interface for data processing on Kafka - no need to write any programming code. KSQL brings together the worlds of streams and databases by allowing you to work with your data in a stream and in a table format. Built on top of Kafka's Streams API, KSQL supports many powerful operations including filtering, transformations, aggregations, joins, windowing, sessionization, and much more. 

It is open source, distributed, scalable, fault-tolerant, and real-time. You will learn how KSQL makes it easy to get started with a wide range of stream processing use cases such as those described at the beginning. I cover how to get up and running with KSQL and explore the under-the-hood details of how it all works.

Read more: 
https://2018.berlinbuzzwords.de/18/session/big-data-fast-data-easy-data-distributed-stream-processing-everyone-ksql-streaming-sql

About Michael Noll:
https://2018.berlinbuzzwords.de/users/michael-noll

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,110 --> 00:00:10,670
so thanks again for turning up it's my

00:00:08,809 --> 00:00:12,170
third time here at Berlin bus routes

00:00:10,670 --> 00:00:15,200
last year I've talked about Kafka

00:00:12,170 --> 00:00:16,880
streams with I think 30 plus degrees

00:00:15,200 --> 00:00:21,710
Celsius it's so cool to be here without

00:00:16,880 --> 00:00:26,300
sweating all the time oh it's the same

00:00:21,710 --> 00:00:29,570
for you so I'll talk for about the next

00:00:26,300 --> 00:00:31,310
half hour about K sequel which is a

00:00:29,570 --> 00:00:36,850
distributed streaming sequel engine for

00:00:31,310 --> 00:00:40,910
Kafka and first a few words about myself

00:00:36,850 --> 00:00:42,320
so I work at confluent and if I start

00:00:40,910 --> 00:00:47,420
who knows conflict who knows what

00:00:42,320 --> 00:00:49,190
content is ok that's almost everyone for

00:00:47,420 --> 00:00:50,570
the few of you who don't a confluent is

00:00:49,190 --> 00:00:53,000
the company founded by the credits of

00:00:50,570 --> 00:00:55,070
Apache Kafka we're based in the United

00:00:53,000 --> 00:00:58,070
States I am actually based in

00:00:55,070 --> 00:01:01,539
Switzerland working from remote so we

00:00:58,070 --> 00:01:04,159
are pretty distributed team and I joined

00:01:01,539 --> 00:01:05,960
confluent about three years ago on the

00:01:04,159 --> 00:01:08,120
engineering team but since then have

00:01:05,960 --> 00:01:10,940
moved to product management and now

00:01:08,120 --> 00:01:13,250
responsible for stream processing with

00:01:10,940 --> 00:01:14,750
Kafka and confluent and specifically

00:01:13,250 --> 00:01:17,030
this is Kafka streams which I talked

00:01:14,750 --> 00:01:22,610
about last year and case ago which is

00:01:17,030 --> 00:01:24,380
what I cover today so that was a few

00:01:22,610 --> 00:01:26,720
bits about myself let me know you a

00:01:24,380 --> 00:01:32,170
little bit more who of you is using

00:01:26,720 --> 00:01:38,210
Kafka yeah yeah almost everyone again

00:01:32,170 --> 00:01:41,060
who of you is using Kafka streams okay

00:01:38,210 --> 00:01:43,820
who of you is using or has learned that

00:01:41,060 --> 00:01:47,990
there is something called case sequel ok

00:01:43,820 --> 00:01:49,610
a lot who of you knows sequel like the

00:01:47,990 --> 00:01:54,470
traditional sequel like my sequel post

00:01:49,610 --> 00:01:56,030
grad and so on ok everyone cool so this

00:01:54,470 --> 00:01:58,090
is cool because one of the things that I

00:01:56,030 --> 00:02:00,740
want to talk about in this session is

00:01:58,090 --> 00:02:03,980
the world of streaming with Apache Kafka

00:02:00,740 --> 00:02:07,640
as shown here and the world of databases

00:02:03,980 --> 00:02:08,989
and one thing I will talk about in

00:02:07,640 --> 00:02:10,879
particular is that there was a very

00:02:08,989 --> 00:02:14,930
close relationship between the two they

00:02:10,879 --> 00:02:17,420
like each other very much so and to set

00:02:14,930 --> 00:02:19,470
the stage for this

00:02:17,420 --> 00:02:22,800
we're pretty international audience

00:02:19,470 --> 00:02:24,360
you're Grillin buzzwords so in order to

00:02:22,800 --> 00:02:27,270
get here you typically would have to

00:02:24,360 --> 00:02:30,090
poke a fly it reserved a hotel order a

00:02:27,270 --> 00:02:31,320
cab or taxi probably listen to the music

00:02:30,090 --> 00:02:33,540
on your way here

00:02:31,320 --> 00:02:34,770
maybe you select to your colleagues

00:02:33,540 --> 00:02:38,040
right here because you still have to do

00:02:34,770 --> 00:02:39,390
some work and as it turns out a lot of

00:02:38,040 --> 00:02:41,040
these daily activities are nowadays

00:02:39,390 --> 00:02:45,090
poverty by kafka behind the scenes

00:02:41,040 --> 00:02:48,209
whether you know it or not so it's there

00:02:45,090 --> 00:02:49,820
anyone here that wants to say like 1 2

00:02:48,209 --> 00:02:52,290
cents about how they're using Kafka

00:02:49,820 --> 00:02:55,860
maybe one of the speakers around here

00:02:52,290 --> 00:02:57,720
that don't feel shy to speak up okay

00:02:55,860 --> 00:02:59,000
some people are real hesitant but they

00:02:57,720 --> 00:03:04,170
don't want to talk about it

00:02:59,000 --> 00:03:06,840
okay bye Devin escape hatch so since we

00:03:04,170 --> 00:03:08,700
tech people tend to be introverts here

00:03:06,840 --> 00:03:10,680
is a an example that I think should

00:03:08,700 --> 00:03:12,480
still be relevant to most people here at

00:03:10,680 --> 00:03:15,300
least if you're working for or like a

00:03:12,480 --> 00:03:17,730
company in the private industry so a lot

00:03:15,300 --> 00:03:19,200
of times what you do at a company is

00:03:17,730 --> 00:03:21,030
that you're getting a lot of signals

00:03:19,200 --> 00:03:23,370
about your customer through a variety of

00:03:21,030 --> 00:03:25,380
internal channels and through a variety

00:03:23,370 --> 00:03:27,420
of external channels so in what

00:03:25,380 --> 00:03:29,310
typically you would like to do any

00:03:27,420 --> 00:03:30,570
companies to aggregate all this

00:03:29,310 --> 00:03:32,670
information that you know about your

00:03:30,570 --> 00:03:35,519
customer and then create consolidated

00:03:32,670 --> 00:03:37,830
customer profiles that could be used for

00:03:35,519 --> 00:03:39,360
a variety of things such as if you're

00:03:37,830 --> 00:03:43,739
doing fraud detection you want to know

00:03:39,360 --> 00:03:45,989
that a person is currently in Berlin so

00:03:43,739 --> 00:03:48,209
it seems to be fraudulent activity if

00:03:45,989 --> 00:03:51,329
the credit card of the person is

00:03:48,209 --> 00:03:53,489
suddenly new somewhere in Argentina and

00:03:51,329 --> 00:03:55,230
there a variety of scenarios where these

00:03:53,489 --> 00:03:56,850
things can help and as you can see here

00:03:55,230 --> 00:03:59,640
what we're looking at at a very high

00:03:56,850 --> 00:04:01,410
conceptual level is you will have a lot

00:03:59,640 --> 00:04:03,510
of input data in this example in the

00:04:01,410 --> 00:04:06,720
form of streams and you want to process

00:04:03,510 --> 00:04:10,410
that in real time into a table something

00:04:06,720 --> 00:04:12,810
like a normal database so we talk about

00:04:10,410 --> 00:04:17,130
that later on in a variety of different

00:04:12,810 --> 00:04:19,620
facets so here is where our case seeker

00:04:17,130 --> 00:04:21,329
enters the picture so as I mentioned

00:04:19,620 --> 00:04:26,190
before case lists the streaming sequel

00:04:21,329 --> 00:04:27,990
engine for Apache Kafka and at a very

00:04:26,190 --> 00:04:30,159
high level here is how you would use

00:04:27,990 --> 00:04:32,020
that so you have your data in Kafka

00:04:30,159 --> 00:04:33,520
and we're like in the motivating example

00:04:32,020 --> 00:04:35,589
informations and signals about the

00:04:33,520 --> 00:04:37,539
customers and then you want to process

00:04:35,589 --> 00:04:40,269
their data you want to analyze it peek

00:04:37,539 --> 00:04:41,409
into it and so on and with Kay sequel

00:04:40,269 --> 00:04:43,569
you would run

00:04:41,409 --> 00:04:45,729
k sequel and I show you what it means to

00:04:43,569 --> 00:04:47,679
run K sequel at the perimeter of your

00:04:45,729 --> 00:04:49,929
cuff tear cluster and then we'll talk

00:04:47,679 --> 00:04:51,909
across the network to a Kafka cluster in

00:04:49,929 --> 00:04:54,639
order to read data write data and so on

00:04:51,909 --> 00:04:56,379
so very much like running Kafka streams

00:04:54,639 --> 00:04:58,300
application for those of you raise the

00:04:56,379 --> 00:04:59,979
hand earlier that there isn't this

00:04:58,300 --> 00:05:03,429
either for writing Java applications or

00:04:59,979 --> 00:05:04,959
scale applications and that's also all

00:05:03,429 --> 00:05:07,389
you need right you don't need anything

00:05:04,959 --> 00:05:08,860
else you don't need to install and I do

00:05:07,389 --> 00:05:10,269
cluster because you want to have fault

00:05:08,860 --> 00:05:12,819
tolerant stream processing that's why I

00:05:10,269 --> 00:05:17,289
need HDFS and so on that's all you need

00:05:12,819 --> 00:05:19,539
and I won't go through this thing here

00:05:17,289 --> 00:05:22,779
in detail but in case you want to take

00:05:19,539 --> 00:05:24,759
like a reference picture here are some

00:05:22,779 --> 00:05:27,369
of the cool properties of k sequel in a

00:05:24,759 --> 00:05:31,689
nutshell i will talk about primarily the

00:05:27,369 --> 00:05:33,489
top half the top part and a little bit

00:05:31,689 --> 00:05:35,469
of the bottom half but I will skip over

00:05:33,489 --> 00:05:37,569
exactly once processing I skipped over

00:05:35,469 --> 00:05:39,249
Kafka Security's of anyone here's in

00:05:37,569 --> 00:05:40,629
finance or some other heavily regulated

00:05:39,249 --> 00:05:42,550
industry there is not much information

00:05:40,629 --> 00:05:47,529
in this talk but feel free to talk and

00:05:42,550 --> 00:05:49,629
to talk to me afterwards so just some

00:05:47,529 --> 00:05:54,009
things that I will cover in the next few

00:05:49,629 --> 00:05:56,139
minutes so before there was case equal

00:05:54,009 --> 00:05:58,149
the way that you would process your data

00:05:56,139 --> 00:06:00,219
in Kafka through Kafka streams would

00:05:58,149 --> 00:06:03,189
look some somewhat like this so here is

00:06:00,219 --> 00:06:05,339
a skull application like an entrance

00:06:03,189 --> 00:06:08,740
calaf lubrication that reads from Kafka

00:06:05,339 --> 00:06:11,860
and you know apply some simple fraud

00:06:08,740 --> 00:06:13,929
detection logic on input data in real

00:06:11,860 --> 00:06:16,059
time and then writes the results back to

00:06:13,929 --> 00:06:17,499
Kafka so apart from the import

00:06:16,059 --> 00:06:18,669
statements this is literally the

00:06:17,499 --> 00:06:21,069
application that you would have to write

00:06:18,669 --> 00:06:23,229
in this scenario and it would run on a

00:06:21,069 --> 00:06:25,149
single machine signal container it could

00:06:23,229 --> 00:06:28,959
run on dozens machines dozens containers

00:06:25,149 --> 00:06:30,729
and even more even so a lot of people

00:06:28,959 --> 00:06:32,949
that we work with and a lot of you that

00:06:30,729 --> 00:06:34,959
have used Kafka said that this is still

00:06:32,949 --> 00:06:36,999
quite a high barrier for people to use

00:06:34,959 --> 00:06:39,189
either because they're not Java experts

00:06:36,999 --> 00:06:41,529
or Scala experts for that matter or

00:06:39,189 --> 00:06:44,110
because they are so busy doing other

00:06:41,529 --> 00:06:46,569
things that it just takes them too

00:06:44,110 --> 00:06:49,120
to implement Java Scala applications and

00:06:46,569 --> 00:06:50,949
then you know deploy them and so on so

00:06:49,120 --> 00:06:53,259
with case ago all of these lines of

00:06:50,949 --> 00:06:55,300
codes and you know I'm I'm a developer

00:06:53,259 --> 00:06:57,430
by trade so for me I would be okay with

00:06:55,300 --> 00:06:59,319
the above we've only just filled before

00:06:57,430 --> 00:07:02,860
but all of that collapses down to this

00:06:59,319 --> 00:07:05,349
single single statement so you don't

00:07:02,860 --> 00:07:06,879
need to write any JavaScript you don't

00:07:05,349 --> 00:07:08,680
need to embed those sequel statements

00:07:06,879 --> 00:07:10,000
inside some other application or some

00:07:08,680 --> 00:07:13,689
kind of person job that's all you need

00:07:10,000 --> 00:07:15,370
and one of the nice things of that is

00:07:13,689 --> 00:07:18,099
that you have a much faster and more

00:07:15,370 --> 00:07:19,900
interactive workflow so with Kafka

00:07:18,099 --> 00:07:21,969
streams or no similar tools you would

00:07:19,900 --> 00:07:24,219
write your code in Java Scala and you

00:07:21,969 --> 00:07:26,229
know compile it package it run and

00:07:24,219 --> 00:07:27,490
deploy it and so on versus with case you

00:07:26,229 --> 00:07:30,069
just write your statement and that's it

00:07:27,490 --> 00:07:31,300
this is super cool if you're you know

00:07:30,069 --> 00:07:33,729
sitting down at lunchtime you have this

00:07:31,300 --> 00:07:36,639
cool idea and just let's just let me see

00:07:33,729 --> 00:07:39,009
what happens if I do this and then you

00:07:36,639 --> 00:07:41,860
know do very quick iterations on your

00:07:39,009 --> 00:07:43,659
idea this is super cool for that we've

00:07:41,860 --> 00:07:45,580
seen that also being used for some of

00:07:43,659 --> 00:07:47,050
operations people so as our East I'd

00:07:45,580 --> 00:07:49,180
want to figure out why was this message

00:07:47,050 --> 00:07:50,139
and a process so take me look at let's

00:07:49,180 --> 00:07:52,270
take a look at all the data that

00:07:50,139 --> 00:07:53,710
happened in less today and see whether

00:07:52,270 --> 00:07:56,080
there was any thing you know

00:07:53,710 --> 00:07:56,889
particularly problematic in a message

00:07:56,080 --> 00:07:59,050
that we got from this particular

00:07:56,889 --> 00:08:04,089
customer during this time period and so

00:07:59,050 --> 00:08:05,740
on so when I say Kasich how would you

00:08:04,089 --> 00:08:07,690
use that so I showed the Kasich or

00:08:05,740 --> 00:08:10,000
command-line interface here there are

00:08:07,690 --> 00:08:12,279
three ways you could use case equal at

00:08:10,000 --> 00:08:13,779
the moment one is through the CLI you

00:08:12,279 --> 00:08:15,419
know a bit like an icicle or phosphorous

00:08:13,779 --> 00:08:19,180
pronged route and type in your queries

00:08:15,419 --> 00:08:20,740
you can use a modern WI for that shown

00:08:19,180 --> 00:08:22,569
in the middle and you can also use the

00:08:20,740 --> 00:08:24,490
REST API and that is for people that

00:08:22,569 --> 00:08:25,659
like the sequel part but they still want

00:08:24,490 --> 00:08:27,550
to drive it through their favorite

00:08:25,659 --> 00:08:31,060
programming language like you know go

00:08:27,550 --> 00:08:34,899
micro service or no JavaScript for

00:08:31,060 --> 00:08:37,360
example here's a simple REST API example

00:08:34,899 --> 00:08:39,159
how that would look like and what you

00:08:37,360 --> 00:08:41,380
can do with the REST API is either what

00:08:39,159 --> 00:08:43,089
I'm showing here is use you know sending

00:08:41,380 --> 00:08:45,550
a query and the results are streamed

00:08:43,089 --> 00:08:47,320
back in real time to the client but you

00:08:45,550 --> 00:08:49,120
can also submit

00:08:47,320 --> 00:08:52,000
statements such as creating a table

00:08:49,120 --> 00:08:53,920
creating a stream and 10 like submit the

00:08:52,000 --> 00:08:57,300
query to KC Qin and then it keeps

00:08:53,920 --> 00:08:57,300
running behind the scenes for you

00:08:57,980 --> 00:09:03,720
so that was a quick introduction and

00:09:00,630 --> 00:09:07,950
high-level overview of K sequel so what

00:09:03,720 --> 00:09:09,360
can you do with that take a sip it's not

00:09:07,950 --> 00:09:17,910
as hot as it was last year but it's

00:09:09,360 --> 00:09:19,440
still hot so one of the first things

00:09:17,910 --> 00:09:21,570
that people typically with kasey kahne

00:09:19,440 --> 00:09:24,090
our experience is that they just enjoy

00:09:21,570 --> 00:09:25,530
looking at their data so it's very cool

00:09:24,090 --> 00:09:27,510
if you'd want to explore your data in

00:09:25,530 --> 00:09:29,700
Kafka so you know they're raised for you

00:09:27,510 --> 00:09:31,980
to see the topics that value in your

00:09:29,700 --> 00:09:34,260
Kafka cluster you can peek into what is

00:09:31,980 --> 00:09:35,280
actually inside a topic so you know this

00:09:34,260 --> 00:09:36,870
is actually the one that I want to look

00:09:35,280 --> 00:09:40,230
at because I have this idea about you

00:09:36,870 --> 00:09:41,700
know combining customer activities with

00:09:40,230 --> 00:09:44,460
you know progress that we get from some

00:09:41,700 --> 00:09:46,470
external partner and of course you can

00:09:44,460 --> 00:09:51,060
do the normal sequel style of your

00:09:46,470 --> 00:09:52,350
selecting your data and so on what you

00:09:51,060 --> 00:09:55,080
can of course also do is you can use it

00:09:52,350 --> 00:09:57,720
to enrich your data so you could say

00:09:55,080 --> 00:09:59,340
that I have some a payment stream of

00:09:57,720 --> 00:10:00,960
incoming financial transactions and I

00:09:59,340 --> 00:10:02,640
want to enrich that with customer

00:10:00,960 --> 00:10:03,960
profile information like you know the

00:10:02,640 --> 00:10:06,120
very motivating example at the beginning

00:10:03,960 --> 00:10:07,170
and then I can make more informed

00:10:06,120 --> 00:10:09,360
decisions

00:10:07,170 --> 00:10:10,740
downstream whether or not I would flag

00:10:09,360 --> 00:10:12,960
for some of this transactions for the

00:10:10,740 --> 00:10:15,360
land or not goes back to this idea if

00:10:12,960 --> 00:10:17,520
the customer is currently in Berlin and

00:10:15,360 --> 00:10:19,110
the trench action happens in Argentina

00:10:17,520 --> 00:10:21,630
probably this is an indication that

00:10:19,110 --> 00:10:23,940
something fishy is going on so what

00:10:21,630 --> 00:10:26,250
Kasich will support here is no joints

00:10:23,940 --> 00:10:28,290
like stream table joints and so on where

00:10:26,250 --> 00:10:31,860
you can't combine data sources in real

00:10:28,290 --> 00:10:34,770
time you can also use it for things such

00:10:31,860 --> 00:10:37,380
as streaming e-tail or real-time ETL so

00:10:34,770 --> 00:10:39,450
use it to filter data expense data so if

00:10:37,380 --> 00:10:41,850
those of you in the room for like me

00:10:39,450 --> 00:10:43,350
received dozens of emails about hey this

00:10:41,850 --> 00:10:46,710
is our updated privacy notice there's

00:10:43,350 --> 00:10:48,600
this thing called gdpr please read out

00:10:46,710 --> 00:10:50,880
your privacy policy so a lot of these

00:10:48,600 --> 00:10:52,980
companies are also using a sequel in

00:10:50,880 --> 00:10:55,800
order to make sure that no data is being

00:10:52,980 --> 00:10:58,080
anonymized of studentized appropriately

00:10:55,800 --> 00:10:59,040
in order to comply with shitty power and

00:10:58,080 --> 00:11:00,390
of course there's a bunch of other

00:10:59,040 --> 00:11:02,370
things that if you're working in that

00:11:00,390 --> 00:11:05,390
space you have probably had a whole lot

00:11:02,370 --> 00:11:05,390
of fun in the past few months

00:11:05,850 --> 00:11:11,890
similarly you can use it for anomaly

00:11:08,350 --> 00:11:12,940
detection so a very simple example was

00:11:11,890 --> 00:11:14,770
from the beginning here's another one

00:11:12,940 --> 00:11:17,140
straight forward so I can fit it on one

00:11:14,770 --> 00:11:19,300
slide so what we're doing here is we're

00:11:17,140 --> 00:11:21,340
aggregating the raw input data then

00:11:19,300 --> 00:11:23,560
we're making some you know heuristics

00:11:21,340 --> 00:11:25,240
are some thresholding on the aggregates

00:11:23,560 --> 00:11:28,690
that we have just computed in real time

00:11:25,240 --> 00:11:30,400
and then saying okay if more than X or

00:11:28,690 --> 00:11:31,750
front authorisation attempts failed in a

00:11:30,400 --> 00:11:33,490
certain time period then this looks like

00:11:31,750 --> 00:11:39,610
something bad is going on so then you

00:11:33,490 --> 00:11:41,500
want to learn we want to follow up for

00:11:39,610 --> 00:11:44,010
people that work in an IOT space whether

00:11:41,500 --> 00:11:47,830
it's like connected cars you know

00:11:44,010 --> 00:11:49,870
earthquake sensors whatever that is you

00:11:47,830 --> 00:11:52,240
can do the same thing here so we can

00:11:49,870 --> 00:11:55,210
look at the data in real time aggregated

00:11:52,240 --> 00:11:57,640
and then alert follow-up you know create

00:11:55,210 --> 00:12:03,670
actions and trigger things off of that

00:11:57,640 --> 00:12:06,130
as well and of course it can be used for

00:12:03,670 --> 00:12:08,350
more mundane tasks so oftentimes in

00:12:06,130 --> 00:12:10,960
Kafka what you like to do is you want to

00:12:08,350 --> 00:12:12,340
convert data for example let's say your

00:12:10,960 --> 00:12:14,650
input data is in JSON but you want to

00:12:12,340 --> 00:12:16,450
edit have it in a fro or you want to

00:12:14,650 --> 00:12:18,400
reap rotation your data because you want

00:12:16,450 --> 00:12:19,570
to scale out that it's very easy to do

00:12:18,400 --> 00:12:21,340
with case you could we know with

00:12:19,570 --> 00:12:22,930
one-liner we're saying it I want to I

00:12:21,340 --> 00:12:25,600
have for some of this many partitions or

00:12:22,930 --> 00:12:27,310
I want to have this output data format

00:12:25,600 --> 00:12:30,790
in this case we are converting whatever

00:12:27,310 --> 00:12:35,830
form of the input stream is into JSON

00:12:30,790 --> 00:12:37,930
format now where's Kasich will not not

00:12:35,830 --> 00:12:40,480
such a great fit a case equal is a

00:12:37,930 --> 00:12:43,630
streaming sequel engine so it is not

00:12:40,480 --> 00:12:46,750
optimized for you know random lookup of

00:12:43,630 --> 00:12:47,860
arbitrary field in your data so for

00:12:46,750 --> 00:12:50,740
example if you're looking for one

00:12:47,860 --> 00:12:52,750
specific message in a stream of Kafka

00:12:50,740 --> 00:12:55,210
messages it will not return this in

00:12:52,750 --> 00:12:58,060
constant time like probably database

00:12:55,210 --> 00:13:00,100
with indexes would and for the same

00:12:58,060 --> 00:13:02,620
reason it's also not such a great fit

00:13:00,100 --> 00:13:04,120
for another traditional VI tooling well

00:13:02,620 --> 00:13:06,640
because as I mentioned there are no

00:13:04,120 --> 00:13:08,530
indexes yet in case people also because

00:13:06,640 --> 00:13:10,480
there is no JDBC driver yet well there

00:13:08,530 --> 00:13:12,880
is a community one but not one part of

00:13:10,480 --> 00:13:14,950
the case of the project but also because

00:13:12,880 --> 00:13:16,890
a lot of these tools and in this space

00:13:14,950 --> 00:13:19,380
are not yet working

00:13:16,890 --> 00:13:23,190
with continuously updated streaming

00:13:19,380 --> 00:13:24,840
results so they are not good at working

00:13:23,190 --> 00:13:26,940
with your data in real time that is

00:13:24,840 --> 00:13:30,780
another reason which goes beyond what

00:13:26,940 --> 00:13:36,660
case you could does or does not so how

00:13:30,780 --> 00:13:38,520
does case equal work if you were here

00:13:36,660 --> 00:13:41,600
last year when I talked about Kafka

00:13:38,520 --> 00:13:44,490
streams I talked about how Kafka streams

00:13:41,600 --> 00:13:47,580
was usable in production right from the

00:13:44,490 --> 00:13:49,260
very beginning because 90% of the hopple

00:13:47,580 --> 00:13:52,590
it had to solve were already solved by

00:13:49,260 --> 00:13:54,180
kafka the foundation of Kafka streams so

00:13:52,590 --> 00:13:57,030
Kafka she was standing on the shoulders

00:13:54,180 --> 00:13:59,100
of the Kafka giants or the streaming

00:13:57,030 --> 00:14:01,410
Giants case you could ask the same thing

00:13:59,100 --> 00:14:04,080
so case equal itself is built on top of

00:14:01,410 --> 00:14:06,420
the Kafka streams API which in turn is

00:14:04,080 --> 00:14:09,540
built on top of the Kafka producer and

00:14:06,420 --> 00:14:11,250
consumer API so which then begs the

00:14:09,540 --> 00:14:12,840
question well this looks actually pretty

00:14:11,250 --> 00:14:15,270
cool across the board but when should I

00:14:12,840 --> 00:14:17,430
use either of those can I combine them

00:14:15,270 --> 00:14:18,570
but first yes you can combine them so

00:14:17,430 --> 00:14:21,060
it's pretty common that for example

00:14:18,570 --> 00:14:22,740
someone is using Kafka streams to know

00:14:21,060 --> 00:14:24,480
massaged the input data and I give an

00:14:22,740 --> 00:14:27,360
example for when you would want to use

00:14:24,480 --> 00:14:29,340
that and then hand it over to a case

00:14:27,360 --> 00:14:32,220
sequel based workflow and then this is

00:14:29,340 --> 00:14:33,930
then being taken into another Kafka

00:14:32,220 --> 00:14:36,990
streams workflow and so on so people are

00:14:33,930 --> 00:14:39,900
doing that a lot and the way I would

00:14:36,990 --> 00:14:42,000
just oppose them is the higher up you go

00:14:39,900 --> 00:14:44,580
in this pyramid the more you get

00:14:42,000 --> 00:14:47,040
ease-of-use at the expense of

00:14:44,580 --> 00:14:49,800
flexibility in what you can express in

00:14:47,040 --> 00:14:50,970
your application so for example if

00:14:49,800 --> 00:14:54,480
you're using the core consumer or the

00:14:50,970 --> 00:14:56,970
producer you can really work on like the

00:14:54,480 --> 00:14:59,040
nuts and bolts of Kafka so you can

00:14:56,970 --> 00:15:02,160
subscribe to Kafka topic you can peek

00:14:59,040 --> 00:15:04,350
inside the Kafka topic etc so it's like

00:15:02,160 --> 00:15:06,600
you know your soldering iron working

00:15:04,350 --> 00:15:08,720
with Kafka if you're going for the up

00:15:06,600 --> 00:15:11,430
the stack you get Kafka streams which

00:15:08,720 --> 00:15:14,130
gives you actually two AP is one is a

00:15:11,430 --> 00:15:16,110
functional programming style API called

00:15:14,130 --> 00:15:18,030
the Kafka streams DSL and one is an

00:15:16,110 --> 00:15:21,930
imperative style more like you know

00:15:18,030 --> 00:15:24,990
event at a time processing way called

00:15:21,930 --> 00:15:27,300
the processor API so what you can do

00:15:24,990 --> 00:15:29,550
here is know this example is that ezel

00:15:27,300 --> 00:15:30,259
it's a very Scala collections like you

00:15:29,550 --> 00:15:34,050
know

00:15:30,259 --> 00:15:36,540
a flat map and so on and then on top of

00:15:34,050 --> 00:15:38,929
that is K sequel which is Eng gives you

00:15:36,540 --> 00:15:41,459
a secret express your processing logic

00:15:38,929 --> 00:15:43,889
and that is also how I would Chuck

00:15:41,459 --> 00:15:46,649
suppose those in terms of when you would

00:15:43,889 --> 00:15:47,639
you want to use them so imagine this

00:15:46,649 --> 00:15:49,230
example here

00:15:47,639 --> 00:15:51,179
so here's case you let the table and the

00:15:49,230 --> 00:15:54,899
same kind of logic at the bottom for

00:15:51,179 --> 00:15:56,399
Kafka streams now where would you not

00:15:54,899 --> 00:15:59,009
use case equal because it looks pretty

00:15:56,399 --> 00:16:00,749
simple in the rich icky sense like no

00:15:59,009 --> 00:16:03,420
not a lot of moving parts right there's

00:16:00,749 --> 00:16:06,720
this one thing that it does a good

00:16:03,420 --> 00:16:08,730
example that I like to use is for

00:16:06,720 --> 00:16:11,069
example if you have to implement a

00:16:08,730 --> 00:16:13,439
finite state machine it's something that

00:16:11,069 --> 00:16:16,709
doesn't really flow naturally in my

00:16:13,439 --> 00:16:19,350
opinion in any kind of sequel tool but a

00:16:16,709 --> 00:16:20,879
repro comanche is pretty good at that so

00:16:19,350 --> 00:16:22,860
we've seen some customers that work on

00:16:20,879 --> 00:16:25,709
network telemetry data where they are

00:16:22,860 --> 00:16:27,959
getting raw network traffic packets and

00:16:25,709 --> 00:16:29,790
they're stitching together TCP sessions

00:16:27,959 --> 00:16:30,929
and in TCP there is a finite state

00:16:29,790 --> 00:16:32,790
machine that tells you here's an a

00:16:30,929 --> 00:16:34,559
connection that is being established and

00:16:32,790 --> 00:16:37,319
then it's continuing and at the end it's

00:16:34,559 --> 00:16:38,730
being closed and that is something that

00:16:37,319 --> 00:16:39,959
I think it's more naturally into a

00:16:38,730 --> 00:16:41,129
program language where you can build

00:16:39,959 --> 00:16:42,600
your finite state machines and so on

00:16:41,129 --> 00:16:45,619
this is not something that I think

00:16:42,600 --> 00:16:49,259
that's very well into the secret world

00:16:45,619 --> 00:16:52,709
in terms of architecture and how does

00:16:49,259 --> 00:16:54,509
that all work behind the scenes so with

00:16:52,709 --> 00:16:56,759
Kasich oh just just just one thing

00:16:54,509 --> 00:16:58,410
really that the dust work and that is

00:16:56,759 --> 00:17:00,529
the case achill server which is a JVM

00:16:58,410 --> 00:17:04,140
process and the server has two parts

00:17:00,529 --> 00:17:06,209
there is the case equal engine and the

00:17:04,140 --> 00:17:08,689
case covers the API the rest API allows

00:17:06,209 --> 00:17:10,919
you to interact with the server and you

00:17:08,689 --> 00:17:13,589
start the server with a simple command

00:17:10,919 --> 00:17:15,600
and that could be on a physical machine

00:17:13,589 --> 00:17:16,649
like your laptop when you just download

00:17:15,600 --> 00:17:19,589
it and want to play with for the first

00:17:16,649 --> 00:17:22,350
time can be a docker container it can be

00:17:19,589 --> 00:17:23,669
on pram it can be in the cloud can be

00:17:22,350 --> 00:17:26,010
public or private cloud it doesn't

00:17:23,669 --> 00:17:27,689
matter works as well as open chef on

00:17:26,010 --> 00:17:31,140
open ship as it does somewhere in event

00:17:27,689 --> 00:17:34,130
the emperor based setup on GC PE or with

00:17:31,140 --> 00:17:36,419
Google Google Cloud or confluent cloud

00:17:34,130 --> 00:17:38,190
the actual processing happens in the

00:17:36,419 --> 00:17:40,409
engine and as I mentioned earlier this

00:17:38,190 --> 00:17:43,020
is based on Kafka streams so the engine

00:17:40,409 --> 00:17:47,700
in case equal use utilizes the Kafka

00:17:43,020 --> 00:17:48,780
dreams API to do the processing and as I

00:17:47,700 --> 00:17:52,110
mentioned earlier there are a couple of

00:17:48,780 --> 00:17:55,710
ways they can use these Kasich observers

00:17:52,110 --> 00:17:58,140
interactively either for UI for a CLI or

00:17:55,710 --> 00:18:03,210
by driving the REST API directly in your

00:17:58,140 --> 00:18:05,190
favorite programming language so and

00:18:03,210 --> 00:18:06,300
just to stress the fact again that I

00:18:05,190 --> 00:18:09,150
mentioned earlier because I think this

00:18:06,300 --> 00:18:11,310
is super important case you could just

00:18:09,150 --> 00:18:13,710
like have costumes it runs everywhere so

00:18:11,310 --> 00:18:16,320
wherever you can deploy a JVM process

00:18:13,710 --> 00:18:19,200
you can deploy this and that means it is

00:18:16,320 --> 00:18:21,330
equally viable for a super small scale

00:18:19,200 --> 00:18:23,940
use case like a proof-of-concept or

00:18:21,330 --> 00:18:26,910
prototype or for very large-scale

00:18:23,940 --> 00:18:29,070
production setup and that I think is

00:18:26,910 --> 00:18:31,140
pretty cool because typically we have

00:18:29,070 --> 00:18:33,810
other you know particular like the big

00:18:31,140 --> 00:18:35,550
data tools that originate in the big

00:18:33,810 --> 00:18:38,100
data world is you have to reach this

00:18:35,550 --> 00:18:39,570
minimum threshold of pain that you want

00:18:38,100 --> 00:18:41,100
to go through before you take the job

00:18:39,570 --> 00:18:43,530
and start you know for something using

00:18:41,100 --> 00:18:45,390
Hadoop or spark here you can use the

00:18:43,530 --> 00:18:46,980
same tool from your initial testing with

00:18:45,390 --> 00:18:48,330
test data locally all the way to

00:18:46,980 --> 00:18:54,600
large-scale production on you know

00:18:48,330 --> 00:18:56,970
dozens of machines and more and to

00:18:54,600 --> 00:18:58,590
showcase how that work is then how to go

00:18:56,970 --> 00:19:02,220
from like a single container for some to

00:18:58,590 --> 00:19:05,640
distribute it set up as a reminder case

00:19:02,220 --> 00:19:07,290
equal service read and write to Kafka

00:19:05,640 --> 00:19:09,090
across the network they are not running

00:19:07,290 --> 00:19:12,630
inside the Kafka cluster they're not

00:19:09,090 --> 00:19:17,580
running inside the Kafka procas you can

00:19:12,630 --> 00:19:19,470
run one server or many of them and if

00:19:17,580 --> 00:19:23,340
you run many of them they automatically

00:19:19,470 --> 00:19:24,990
form a case equal cluster behind the

00:19:23,340 --> 00:19:27,240
scenes what they're doing is they're

00:19:24,990 --> 00:19:28,560
forming a Kafka consumer group if you

00:19:27,240 --> 00:19:30,510
know a little bit about how Kafka

00:19:28,560 --> 00:19:32,820
consumers work behind the scenes that

00:19:30,510 --> 00:19:34,770
means if they are in the same cluster

00:19:32,820 --> 00:19:36,750
they're in the same consumer group and

00:19:34,770 --> 00:19:39,240
first they collaborate Lee and in

00:19:36,750 --> 00:19:41,070
parallel start processing your data and

00:19:39,240 --> 00:19:42,480
there is nothing you need to do that

00:19:41,070 --> 00:19:43,950
there's no coordinator they need to run

00:19:42,480 --> 00:19:46,350
etc there's no master node or anything

00:19:43,950 --> 00:19:48,300
all of this is being handled by actually

00:19:46,350 --> 00:19:49,830
the Kafka bag and behind the scenes so

00:19:48,300 --> 00:19:52,350
if you need to have like five servers

00:19:49,830 --> 00:19:54,300
running you run five containers if you

00:19:52,350 --> 00:19:56,480
want to have two running just stop three

00:19:54,300 --> 00:20:00,860
and talk about that in a second

00:19:56,480 --> 00:20:02,750
and similarly because it's so

00:20:00,860 --> 00:20:04,490
lightweight udeploy and what you

00:20:02,750 --> 00:20:06,110
typically see and also what we recommend

00:20:04,490 --> 00:20:08,390
is that actually you deploy not just

00:20:06,110 --> 00:20:10,880
humongous Kasich a cluster inside your

00:20:08,390 --> 00:20:13,940
company so actually quite the opposite

00:20:10,880 --> 00:20:15,680
because you want to deploy a case equal

00:20:13,940 --> 00:20:17,300
cluster and cluster size very heavy

00:20:15,680 --> 00:20:18,920
weight you could say like maybe a case

00:20:17,300 --> 00:20:22,250
equal deployment or case equal

00:20:18,920 --> 00:20:25,730
application per project or per team or

00:20:22,250 --> 00:20:27,770
per use case that also allows you to use

00:20:25,730 --> 00:20:29,510
different version of case equal against

00:20:27,770 --> 00:20:31,760
the same Kafka cluster so some teams

00:20:29,510 --> 00:20:33,110
prefer to use only like Troodon tried

00:20:31,760 --> 00:20:34,610
versions of case Eagle that they have

00:20:33,110 --> 00:20:36,440
been using for six months only then they

00:20:34,610 --> 00:20:38,780
go to production with it they can stick

00:20:36,440 --> 00:20:39,920
to these you know older versions or and

00:20:38,780 --> 00:20:41,360
then you can have you know the very

00:20:39,920 --> 00:20:43,250
innovative teams in a company that can

00:20:41,360 --> 00:20:44,720
wreak havoc make a lot of mistakes or

00:20:43,250 --> 00:20:46,730
quickly they can use the very latest

00:20:44,720 --> 00:20:50,210
version or maybe even running of case

00:20:46,730 --> 00:20:51,770
equal master the master branch directly

00:20:50,210 --> 00:20:53,930
in production so you could D cup of

00:20:51,770 --> 00:20:56,480
teams and ten lines very easily which is

00:20:53,930 --> 00:20:58,610
super cool if you're in a company that

00:20:56,480 --> 00:21:00,290
has maybe more than ten people working

00:20:58,610 --> 00:21:02,030
on that problem so if you're like a

00:21:00,290 --> 00:21:04,040
small start-up it's super cool because

00:21:02,030 --> 00:21:06,080
you can get up and running like in a

00:21:04,040 --> 00:21:07,820
minute with this but in a bigger company

00:21:06,080 --> 00:21:09,470
often the problem is the organizational

00:21:07,820 --> 00:21:12,110
tooling and the organization processes

00:21:09,470 --> 00:21:13,400
around that so that also allows you to

00:21:12,110 --> 00:21:18,320
be couple of teams and timelines very

00:21:13,400 --> 00:21:19,810
easily and if you want to use case equal

00:21:18,320 --> 00:21:22,520
there are two ways you can do that

00:21:19,810 --> 00:21:24,710
here's the interactive usage and I'm

00:21:22,520 --> 00:21:26,510
Jack the poster to in a second with

00:21:24,710 --> 00:21:28,700
interactive usage you start in one or

00:21:26,510 --> 00:21:30,380
more cases called servers and then you

00:21:28,700 --> 00:21:32,150
can interact with those servers along

00:21:30,380 --> 00:21:34,370
with your classical deployment through

00:21:32,150 --> 00:21:37,970
the CLI the UI or the rest of API

00:21:34,370 --> 00:21:41,150
directly the second way to deploy case

00:21:37,970 --> 00:21:43,430
sequel is in headless configuration when

00:21:41,150 --> 00:21:45,470
you're doing this the case you'll

00:21:43,430 --> 00:21:47,390
service disable any interactive access

00:21:45,470 --> 00:21:50,480
like the REST API is completely disabled

00:21:47,390 --> 00:21:52,430
and the way they know what to process is

00:21:50,480 --> 00:21:55,670
through a sequel file that you give them

00:21:52,430 --> 00:21:57,200
and in this set up like these three case

00:21:55,670 --> 00:21:59,000
code servers they would fall in our case

00:21:57,200 --> 00:22:00,410
ago deployment ok second cluster they

00:21:59,000 --> 00:22:01,970
would only run those queries that are

00:22:00,410 --> 00:22:04,820
predefined in a sequel file that you

00:22:01,970 --> 00:22:06,890
give them and that is pretty cool for

00:22:04,820 --> 00:22:08,510
companies that want to have in a clear

00:22:06,890 --> 00:22:10,070
audit trail what is being pushed to

00:22:08,510 --> 00:22:12,950
production what is not that

00:22:10,070 --> 00:22:15,560
can roll back if need be and that in

00:22:12,950 --> 00:22:19,400
general should fit into their existing

00:22:15,560 --> 00:22:21,080
CIC D pipeline where you want to prevent

00:22:19,400 --> 00:22:23,240
human mistakes happening in production

00:22:21,080 --> 00:22:25,730
so this allows you to lock down k sequel

00:22:23,240 --> 00:22:29,420
and minimize any mistakes you know for

00:22:25,730 --> 00:22:32,360
human operators so an example journey

00:22:29,420 --> 00:22:33,560
often looks like this so people have an

00:22:32,360 --> 00:22:35,150
idea or they want to try this out so

00:22:33,560 --> 00:22:37,400
what they do is they have an interactive

00:22:35,150 --> 00:22:38,750
case achill a set up where they're just

00:22:37,400 --> 00:22:40,040
you know typing their queries seeing

00:22:38,750 --> 00:22:43,550
what works with dozen and then they're

00:22:40,040 --> 00:22:45,350
making good iterations on that and come

00:22:43,550 --> 00:22:47,210
production you know then you know what

00:22:45,350 --> 00:22:48,620
you've got what you want to do you also

00:22:47,210 --> 00:22:50,480
have an idea about like the capacity

00:22:48,620 --> 00:22:52,760
that you would need to only like one of

00:22:50,480 --> 00:22:56,420
them don't need like ten of them etc

00:22:52,760 --> 00:22:58,970
then you would run a headless case' call

00:22:56,420 --> 00:23:01,700
deployment within your predefined sets

00:22:58,970 --> 00:23:03,980
of queries it's a very simple and you

00:23:01,700 --> 00:23:05,480
can combine them as well that said there

00:23:03,980 --> 00:23:07,040
of course also other people that use

00:23:05,480 --> 00:23:09,620
interactive case acrylic production as

00:23:07,040 --> 00:23:10,100
well so also an option if you're fine

00:23:09,620 --> 00:23:11,930
with that

00:23:10,100 --> 00:23:13,700
we've seen that not just for people that

00:23:11,930 --> 00:23:15,530
work in the line of business that are

00:23:13,700 --> 00:23:17,510
actually like building the product or

00:23:15,530 --> 00:23:19,370
I'd say the fraud team the

00:23:17,510 --> 00:23:20,780
personalization team or whatever but

00:23:19,370 --> 00:23:23,480
also people that work in operations

00:23:20,780 --> 00:23:28,360
where they have a way to use case we'll

00:23:23,480 --> 00:23:28,360
to look into the flows of data in Kafka

00:23:28,990 --> 00:23:33,500
so now let's talk about something that

00:23:31,610 --> 00:23:35,360
is hopefully even a bit cooler if you're

00:23:33,500 --> 00:23:39,770
interested in the the techie and and

00:23:35,360 --> 00:23:41,570
conceptual side of things so something

00:23:39,770 --> 00:23:43,730
that we already stressed with Kafka

00:23:41,570 --> 00:23:45,650
streams in Kafka and sometimes we're

00:23:43,730 --> 00:23:47,930
also stressing case equal is what we

00:23:45,650 --> 00:23:50,960
call the stream table duality now what

00:23:47,930 --> 00:23:52,010
is that and why should you even care so

00:23:50,960 --> 00:23:55,160
here's an example from the previous

00:23:52,010 --> 00:23:57,290
slides maybe it was hard to notice but

00:23:55,160 --> 00:23:59,450
in some cases we actually showed

00:23:57,290 --> 00:24:02,510
sequences that created streams and in

00:23:59,450 --> 00:24:04,460
other cases we created tables okay oh

00:24:02,510 --> 00:24:06,770
and is that right what what is the

00:24:04,460 --> 00:24:09,020
difference there so I'll talk about that

00:24:06,770 --> 00:24:11,840
in a second the most important point is

00:24:09,020 --> 00:24:14,120
in practice most use cases that you

00:24:11,840 --> 00:24:17,030
implement and even you know the infamous

00:24:14,120 --> 00:24:20,090
word count is an example of such a use

00:24:17,030 --> 00:24:22,740
case you need both streams and tables so

00:24:20,090 --> 00:24:26,429
if you have only streaming tools

00:24:22,740 --> 00:24:28,800
you built the table part yourself if you

00:24:26,429 --> 00:24:30,360
only have the database tables you build

00:24:28,800 --> 00:24:31,590
all of this reading part yourself and

00:24:30,360 --> 00:24:33,929
that kind of sucks

00:24:31,590 --> 00:24:36,090
I mean I've before I joined confluent I

00:24:33,929 --> 00:24:37,590
was you know using those technologies

00:24:36,090 --> 00:24:39,660
not building them and it was really

00:24:37,590 --> 00:24:41,760
really hard like all of this you had to

00:24:39,660 --> 00:24:43,440
implement yourself and that's why for

00:24:41,760 --> 00:24:45,270
Kafka we decided and for Kasich oh we

00:24:43,440 --> 00:24:46,350
decided now this is some that should be

00:24:45,270 --> 00:24:48,840
working out of the box like the

00:24:46,350 --> 00:24:53,220
batteries should be included and let me

00:24:48,840 --> 00:24:58,290
tell you why this is so important first

00:24:53,220 --> 00:25:00,690
thing is what is actually a table so if

00:24:58,290 --> 00:25:02,520
you look at this example here you have a

00:25:00,690 --> 00:25:04,410
table on the left side that is

00:25:02,520 --> 00:25:08,580
undergoing mutations from top to bottom

00:25:04,410 --> 00:25:10,380
and what we can do is we can do change

00:25:08,580 --> 00:25:13,170
data data capture on the table which

00:25:10,380 --> 00:25:15,300
means we're recording the mutations

00:25:13,170 --> 00:25:17,670
against that table that gives us a

00:25:15,300 --> 00:25:19,620
stream a changelog stream and the center

00:25:17,670 --> 00:25:22,260
column and then we can use that

00:25:19,620 --> 00:25:25,500
information this stream to reconstruct

00:25:22,260 --> 00:25:28,050
the table for any particular point in

00:25:25,500 --> 00:25:29,280
time cool part is that you can also do

00:25:28,050 --> 00:25:32,370
that on a different machine in a

00:25:29,280 --> 00:25:34,500
distributed environment so I plugged

00:25:32,370 --> 00:25:36,750
about that like the the bottom link is I

00:25:34,500 --> 00:25:39,330
talked about that in length recently

00:25:36,750 --> 00:25:41,250
we're use Scala code to juxtapose that

00:25:39,330 --> 00:25:43,710
including kafka streams code in case you

00:25:41,250 --> 00:25:47,610
code to explain the relationship between

00:25:43,710 --> 00:25:50,880
those concepts here's a sneak peek so

00:25:47,610 --> 00:25:54,000
here's a stream and you're turning that

00:25:50,880 --> 00:25:57,150
into a table of the latest user

00:25:54,000 --> 00:25:58,920
locations for a user so the stream is an

00:25:57,150 --> 00:26:00,990
input of user locations you know Ellis's

00:25:58,920 --> 00:26:03,870
and paris now editors in Rome and you

00:26:00,990 --> 00:26:09,150
see how the table is changing over time

00:26:03,870 --> 00:26:10,830
as two input data is being processed now

00:26:09,150 --> 00:26:12,240
we're looking at the same example but at

00:26:10,830 --> 00:26:14,429
the top there is no the change lock

00:26:12,240 --> 00:26:17,850
screen for the table and as you can see

00:26:14,429 --> 00:26:19,830
here is in this specific example the

00:26:17,850 --> 00:26:24,210
stream at the top is a copy of the

00:26:19,830 --> 00:26:26,880
stream at the bottom and by the way with

00:26:24,210 --> 00:26:28,920
Kafka's in case of course no they

00:26:26,880 --> 00:26:30,960
realize that and they don't duplicate

00:26:28,920 --> 00:26:32,850
your data on necessarily but here's

00:26:30,960 --> 00:26:34,710
about the concepts

00:26:32,850 --> 00:26:36,389
so this looks a bit trivial right

00:26:34,710 --> 00:26:38,399
because the data looks almost the same

00:26:36,389 --> 00:26:40,440
top to bottom so let's look at something

00:26:38,399 --> 00:26:42,269
different what we're doing here is the

00:26:40,440 --> 00:26:44,309
same input data the same input stream of

00:26:42,269 --> 00:26:46,440
users two locations and now we are

00:26:44,309 --> 00:26:50,399
counting how many locations the

00:26:46,440 --> 00:26:51,929
particular user has visited no more the

00:26:50,399 --> 00:26:54,570
trivial example could be a frequent

00:26:51,929 --> 00:27:00,120
fliers computation if you're an airline

00:26:54,570 --> 00:27:01,860
for example and what we can see here the

00:27:00,120 --> 00:27:04,100
changelog stream for the table at the

00:27:01,860 --> 00:27:07,919
top is not a bit different now we're

00:27:04,100 --> 00:27:12,629
seeing in numbers for users being

00:27:07,919 --> 00:27:14,549
capturing that changelog stream so there

00:27:12,629 --> 00:27:17,539
is a clear relationship between a stream

00:27:14,549 --> 00:27:20,610
as a table as the stream as a table and

00:27:17,539 --> 00:27:23,190
with case equal you can explore that in

00:27:20,610 --> 00:27:25,559
your applications and the core

00:27:23,190 --> 00:27:27,990
realization of that is if you have a

00:27:25,559 --> 00:27:30,870
stream you can get to a table by

00:27:27,990 --> 00:27:33,059
aggregating that stream that could be

00:27:30,870 --> 00:27:36,450
for example accounting it could be

00:27:33,059 --> 00:27:39,990
summing it could be top K analysis and

00:27:36,450 --> 00:27:42,840
so on so to get to get to a table you

00:27:39,990 --> 00:27:44,460
take a stream and aggregate it or in

00:27:42,840 --> 00:27:46,590
something like you interpret that stream

00:27:44,460 --> 00:27:49,200
in a particular way and if you have a

00:27:46,590 --> 00:27:50,690
table you get back the stream or just

00:27:49,200 --> 00:27:54,750
looking at the changing of that table

00:27:50,690 --> 00:27:57,059
and that's also why these two have such

00:27:54,750 --> 00:28:00,929
a close relationship there are two sides

00:27:57,059 --> 00:28:03,419
of the same coin that's also why Apache

00:28:00,929 --> 00:28:08,190
kafka and databases are so much related

00:28:03,419 --> 00:28:10,049
and if you've read up on on that that

00:28:08,190 --> 00:28:13,080
concept you know the idea of turning the

00:28:10,049 --> 00:28:16,019
database inside out this is turning the

00:28:13,080 --> 00:28:17,639
database inside out but now what does

00:28:16,019 --> 00:28:18,809
that mean for you as a user so let's

00:28:17,639 --> 00:28:23,789
take a look at what you can do with that

00:28:18,809 --> 00:28:27,899
in case equal for example imagine you're

00:28:23,789 --> 00:28:29,879
doing change data capture from your my

00:28:27,899 --> 00:28:34,320
sequel or you know whatever already be

00:28:29,879 --> 00:28:36,149
nice you're using database into Kafka so

00:28:34,320 --> 00:28:39,090
you get for some customer information in

00:28:36,149 --> 00:28:40,799
a real-time stream data Kafka through

00:28:39,090 --> 00:28:42,690
Kafka Connect so there are connectors

00:28:40,799 --> 00:28:45,269
that you can use for Oracle MySQL

00:28:42,690 --> 00:28:46,440
post-crash you know whatever to get data

00:28:45,269 --> 00:28:49,409
flown in real time

00:28:46,440 --> 00:28:51,539
Kafka then you can use case equal to

00:28:49,409 --> 00:28:54,059
process those changes to your customer

00:28:51,539 --> 00:28:56,070
data in real time and then send it to

00:28:54,059 --> 00:28:59,039
somewhere else with Kafka connect again

00:28:56,070 --> 00:29:01,669
like elastic so can build an elastic

00:28:59,039 --> 00:29:06,509
search base dashboard or some other

00:29:01,669 --> 00:29:09,539
downstream roughly off of that similar

00:29:06,509 --> 00:29:12,090
example let's stick to the customer

00:29:09,539 --> 00:29:13,470
table at the bottom which is a table but

00:29:12,090 --> 00:29:15,840
you can also have things like you know

00:29:13,470 --> 00:29:18,840
connected cards in an IT scenario which

00:29:15,840 --> 00:29:21,230
are writing updates continuously as they

00:29:18,840 --> 00:29:23,730
happen in real time to Kafka as well

00:29:21,230 --> 00:29:27,149
what you can then do with K sequel is

00:29:23,730 --> 00:29:30,149
you can then join the stream and the

00:29:27,149 --> 00:29:33,120
tables in real time and then you know

00:29:30,149 --> 00:29:36,950
sent them wherever they are needed none

00:29:33,120 --> 00:29:39,269
of that with minimal effort so that is

00:29:36,950 --> 00:29:40,289
example so I've showed on previous has

00:29:39,269 --> 00:29:42,629
already some code so I'm going to do

00:29:40,289 --> 00:29:44,279
that here again how you do that with you

00:29:42,629 --> 00:29:46,950
know case code statements but this is

00:29:44,279 --> 00:29:49,830
how you benefit from this as a user but

00:29:46,950 --> 00:29:51,840
in order to also explain how case you

00:29:49,830 --> 00:29:53,970
could dog fruits itself with the stream

00:29:51,840 --> 00:29:55,289
table duality let's take a closer look

00:29:53,970 --> 00:29:57,110
at what happens behind the scenes with

00:29:55,289 --> 00:30:01,860
case equal and we're going back to

00:29:57,110 --> 00:30:04,200
deployment or operational parts so a key

00:30:01,860 --> 00:30:05,610
challenge in any distributed system or

00:30:04,200 --> 00:30:07,889
specifically for stream processing is

00:30:05,610 --> 00:30:09,480
for tolerance and for stream processing

00:30:07,889 --> 00:30:14,399
it's primarily about fault tolerant

00:30:09,480 --> 00:30:15,990
state so what is that imagine you have a

00:30:14,399 --> 00:30:19,559
computation that does any kind of

00:30:15,990 --> 00:30:22,289
stateful processing typically examples

00:30:19,559 --> 00:30:24,539
are you know Giants and aggregations but

00:30:22,289 --> 00:30:26,669
also windowed applications particular

00:30:24,539 --> 00:30:28,289
very saying that I'm doing five minute

00:30:26,669 --> 00:30:29,850
averages of something or I'm doing a

00:30:28,289 --> 00:30:32,039
stream to stream join over an

00:30:29,850 --> 00:30:34,879
overlapping period of let's say 60

00:30:32,039 --> 00:30:37,200
minutes all of this requires state and

00:30:34,879 --> 00:30:40,769
managing that state behind the scenes

00:30:37,200 --> 00:30:43,679
for you so in this case you know I used

00:30:40,769 --> 00:30:46,559
a blue database icon to represent the

00:30:43,679 --> 00:30:48,299
state of the server that is because you

00:30:46,559 --> 00:30:51,419
asked it to do some kind of stateful

00:30:48,299 --> 00:30:55,649
processing you know like it table join

00:30:51,419 --> 00:30:57,629
or aggregation now going back what we

00:30:55,649 --> 00:30:58,770
showed very early that you know a table

00:30:57,629 --> 00:31:01,559
as a change

00:30:58,770 --> 00:31:03,120
the table and so on what is happening

00:31:01,559 --> 00:31:05,370
behind the scenes that case circle is

00:31:03,120 --> 00:31:07,740
kind of streaming Becky doing a

00:31:05,370 --> 00:31:10,110
streaming backup of any state changes

00:31:07,740 --> 00:31:11,970
from the case achill server that runs at

00:31:10,110 --> 00:31:15,480
the perimeter of the kafka cluster to

00:31:11,970 --> 00:31:17,970
the kafka cluster itself now whenever

00:31:15,480 --> 00:31:19,860
that server happens to go down like the

00:31:17,970 --> 00:31:21,980
machine crashes or communities decided

00:31:19,860 --> 00:31:26,700
to move the container somewhere else

00:31:21,980 --> 00:31:29,820
then the replacement container of the

00:31:26,700 --> 00:31:31,740
end server whatever will say I will not

00:31:29,820 --> 00:31:34,170
restore the state of the failed machine

00:31:31,740 --> 00:31:36,179
to exactly the point where it was when

00:31:34,170 --> 00:31:38,429
it died and only then I would resume the

00:31:36,179 --> 00:31:39,570
processing so that is super cool like

00:31:38,429 --> 00:31:40,890
there is nothing else and you do all

00:31:39,570 --> 00:31:44,700
that happens behind the scenes and

00:31:40,890 --> 00:31:46,950
automatically for you and to show that

00:31:44,700 --> 00:31:48,750
in an example where there are a bit more

00:31:46,950 --> 00:31:52,559
servers around imagine your three

00:31:48,750 --> 00:31:53,760
servers one of them dies and why because

00:31:52,559 --> 00:31:56,700
they're doing stream processing and

00:31:53,760 --> 00:31:59,490
stateful stream processing work the

00:31:56,700 --> 00:32:02,309
failed state from the failed machine is

00:31:59,490 --> 00:32:05,640
moved over and split to the remaining

00:32:02,309 --> 00:32:07,260
ones and I'm oversimplifying here a

00:32:05,640 --> 00:32:10,080
little bit we're gonna say X split and

00:32:07,260 --> 00:32:11,100
merge later on so ignore that I don't

00:32:10,080 --> 00:32:13,640
want to make it too complicated your

00:32:11,100 --> 00:32:15,720
ending a little bit of time that we have

00:32:13,640 --> 00:32:17,640
so what happens then behind the scenes

00:32:15,720 --> 00:32:20,220
is if this happens like server three

00:32:17,640 --> 00:32:22,080
Dyas behind the scenes because kcq let's

00:32:20,220 --> 00:32:24,690
build on Kafka streams which is built on

00:32:22,080 --> 00:32:26,160
Kafka producers and consumers a Kafka

00:32:24,690 --> 00:32:27,510
consumer probe rebalance is triggered

00:32:26,160 --> 00:32:29,070
which is essentially the event saying

00:32:27,510 --> 00:32:30,360
that oh something happened with one of

00:32:29,070 --> 00:32:33,270
the things that we're doing the

00:32:30,360 --> 00:32:37,350
processing and as a result of that

00:32:33,270 --> 00:32:39,720
processing like the logic plus the state

00:32:37,350 --> 00:32:41,940
of the processing is being migrated to a

00:32:39,720 --> 00:32:43,559
new place in this case is being migrated

00:32:41,940 --> 00:32:47,640
to the live service that are still

00:32:43,559 --> 00:32:51,179
running and similarly if you frame back

00:32:47,640 --> 00:32:53,820
up server number three the work is split

00:32:51,179 --> 00:32:56,670
again across the three instances the

00:32:53,820 --> 00:32:58,880
three service and they start

00:32:56,670 --> 00:33:00,870
collaborating and share the work again

00:32:58,880 --> 00:33:03,270
what happens behind the scenes again is

00:33:00,870 --> 00:33:05,880
another rebalance is triggered and then

00:33:03,270 --> 00:33:08,970
the logic is and including the state is

00:33:05,880 --> 00:33:12,630
being migrated so that was the fault

00:33:08,970 --> 00:33:14,280
tolerance view which is things happen

00:33:12,630 --> 00:33:18,770
technically and you rather have them not

00:33:14,280 --> 00:33:21,750
to happen but it's the same it's not a

00:33:18,770 --> 00:33:24,110
side of the same coin because it's also

00:33:21,750 --> 00:33:25,920
relevant for elasticity and scalability

00:33:24,110 --> 00:33:28,920
essentially no difference between them

00:33:25,920 --> 00:33:31,470
really if you look at it so you can also

00:33:28,920 --> 00:33:33,780
add remove or restart a circuit service

00:33:31,470 --> 00:33:35,850
term live operations right so if you

00:33:33,780 --> 00:33:37,950
think that your retailer and there was a

00:33:35,850 --> 00:33:39,930
Black Friday coming tomorrow you can add

00:33:37,950 --> 00:33:43,320
more machines to your live application

00:33:39,930 --> 00:33:45,090
as it is running to prepare for the

00:33:43,320 --> 00:33:46,560
onslaught of you know customers buying

00:33:45,090 --> 00:33:49,140
things from your website on Black Friday

00:33:46,560 --> 00:33:50,220
and once it's done you just stop some of

00:33:49,140 --> 00:33:51,480
the instance that you no longer need

00:33:50,220 --> 00:33:55,200
because you know you don't want to pay

00:33:51,480 --> 00:33:56,730
for them on AWS for example and it just

00:33:55,200 --> 00:33:58,560
continues working there's no data loss

00:33:56,730 --> 00:34:02,000
there's no duplicate processing and so

00:33:58,560 --> 00:34:04,620
on so how would that work very similarly

00:34:02,000 --> 00:34:06,840
you decide well we need more processing

00:34:04,620 --> 00:34:09,870
power so just start additional case

00:34:06,840 --> 00:34:11,640
achill servers what happens is that the

00:34:09,870 --> 00:34:12,960
existing ones will start sharing some of

00:34:11,640 --> 00:34:17,040
their work including sharing some of

00:34:12,960 --> 00:34:19,020
their state to the new ones which do the

00:34:17,040 --> 00:34:20,910
processing and again what happens behind

00:34:19,020 --> 00:34:24,030
the scenes is rebalance event status

00:34:20,910 --> 00:34:25,530
beam are created and if you are done and

00:34:24,030 --> 00:34:26,760
you can scale down again you can do the

00:34:25,530 --> 00:34:31,020
very same thing you just stop some of

00:34:26,760 --> 00:34:32,610
them and they scale down gracefully so

00:34:31,020 --> 00:34:35,250
which is super cool and this is also why

00:34:32,610 --> 00:34:36,870
you can easily run not just case ago but

00:34:35,250 --> 00:34:38,460
also kafka streams application that's a

00:34:36,870 --> 00:34:40,170
containerized on kubernetes

00:34:38,460 --> 00:34:41,820
because there's not much you need to do

00:34:40,170 --> 00:34:48,420
it just start as many or as little

00:34:41,820 --> 00:34:49,950
containers as you need so as I mentioned

00:34:48,420 --> 00:34:52,320
I have to apologize a bit because it's a

00:34:49,950 --> 00:34:54,210
bit not simplified so I didn't talk

00:34:52,320 --> 00:34:57,230
about like threading model or what

00:34:54,210 --> 00:35:00,510
stream tasks are because that is where

00:34:57,230 --> 00:35:03,180
Kafka makes assignment between input

00:35:00,510 --> 00:35:05,190
data stayed and the processing so I

00:35:03,180 --> 00:35:06,960
ignore all of that here because it's

00:35:05,190 --> 00:35:10,620
otherwise a bit complicated to show on

00:35:06,960 --> 00:35:12,510
on one slide but you can read up on that

00:35:10,620 --> 00:35:17,040
in the Apache Kafka documentation how

00:35:12,510 --> 00:35:19,430
that all works behind the scenes so to

00:35:17,040 --> 00:35:19,430
wrap up

00:35:23,850 --> 00:35:28,170
so if there is only one thing that you

00:35:26,430 --> 00:35:29,730
should probably remember for today is

00:35:28,170 --> 00:35:31,980
that Kasich Ulis is trimming sequel

00:35:29,730 --> 00:35:34,170
engine for Apache Kafka so if you like

00:35:31,980 --> 00:35:36,900
to use sequel in order to express what

00:35:34,170 --> 00:35:41,040
you want from your data Kay sequel is a

00:35:36,900 --> 00:35:43,500
great tool if your data is in Kafka it

00:35:41,040 --> 00:35:46,590
has a bunch of nice properties some of

00:35:43,500 --> 00:35:48,990
them I just alluded to ignored that it

00:35:46,590 --> 00:35:52,290
can do exactly once processing I ignored

00:35:48,990 --> 00:35:53,850
that it integrates natively with Kafka

00:35:52,290 --> 00:35:56,210
security so it can encrypt data in

00:35:53,850 --> 00:35:59,010
transit between you know the case achill

00:35:56,210 --> 00:36:00,320
servers and your Kafka cluster you have

00:35:59,010 --> 00:36:02,730
authentication authorization

00:36:00,320 --> 00:36:04,920
particularly now here in Europe when you

00:36:02,730 --> 00:36:06,870
want to prevent people from you know

00:36:04,920 --> 00:36:08,700
accessing sensitive data in your Kafka

00:36:06,870 --> 00:36:12,540
installation you can do that also with K

00:36:08,700 --> 00:36:14,550
sequel it fits right in I didn't talk

00:36:12,540 --> 00:36:16,530
about event time processing but this

00:36:14,550 --> 00:36:20,580
allows you for example to reprocess data

00:36:16,530 --> 00:36:23,130
with K sequel without case we were

00:36:20,580 --> 00:36:24,270
thinking that oh you're telling you're

00:36:23,130 --> 00:36:26,610
reading back like one month of

00:36:24,270 --> 00:36:28,860
production data and if you're not having

00:36:26,610 --> 00:36:30,600
event time processing support case agree

00:36:28,860 --> 00:36:32,280
with otherwise think that oh this just

00:36:30,600 --> 00:36:33,510
happened right now like one month of

00:36:32,280 --> 00:36:36,720
traffic happened right now

00:36:33,510 --> 00:36:38,580
oh god alarm bells DDoS attack fraud

00:36:36,720 --> 00:36:40,470
whatever right so and with the meant

00:36:38,580 --> 00:36:42,390
time processing what you can ensure is

00:36:40,470 --> 00:36:46,160
that data is actually being processed

00:36:42,390 --> 00:36:49,470
tomorrow as it was being processed today

00:36:46,160 --> 00:36:51,630
so hope that was a good summary if that

00:36:49,470 --> 00:36:53,970
was interesting to you here are some

00:36:51,630 --> 00:36:56,340
pointers so the case with your project

00:36:53,970 --> 00:36:58,770
is on github feel free to you know go

00:36:56,340 --> 00:37:01,470
there take a look even more encouraged

00:36:58,770 --> 00:37:04,140
to contribute some code or ask questions

00:37:01,470 --> 00:37:05,640
tell us what works what doesn't and if

00:37:04,140 --> 00:37:07,230
you're really interested we are always

00:37:05,640 --> 00:37:08,760
hiring a confluence so if you're

00:37:07,230 --> 00:37:10,140
interested in that and not just working

00:37:08,760 --> 00:37:12,390
on distributed systems but also working

00:37:10,140 --> 00:37:14,490
in distributed team you'll just come

00:37:12,390 --> 00:37:18,260
talk to me or come talk to us more than

00:37:14,490 --> 00:37:18,260
happy to chat thank you

00:37:22,970 --> 00:37:33,770
and I think we have time for like wastin

00:37:25,910 --> 00:37:37,700
few questions so whenever you do an

00:37:33,770 --> 00:37:39,770
operation that requires some kind of

00:37:37,700 --> 00:37:44,900
state some aggregation as you were

00:37:39,770 --> 00:37:48,260
saying you're generating a table now you

00:37:44,900 --> 00:37:52,579
have local rocks DB that's holding your

00:37:48,260 --> 00:37:53,839
state store essentially is there every

00:37:52,579 --> 00:37:57,049
time you do an operation

00:37:53,839 --> 00:37:58,970
it sends data to Kafka and then Kafka

00:37:57,049 --> 00:38:03,170
reads it back as the changelog to build

00:37:58,970 --> 00:38:05,630
the store state this is that's what's

00:38:03,170 --> 00:38:07,609
happening I tried to reformulate your

00:38:05,630 --> 00:38:10,849
question because I think you're asking a

00:38:07,609 --> 00:38:16,430
question underneath that and let me go

00:38:10,849 --> 00:38:18,049
back to that slide this slide I think

00:38:16,430 --> 00:38:19,880
you're kind of referring to correct

00:38:18,049 --> 00:38:21,770
which is when we're doing stateful

00:38:19,880 --> 00:38:24,410
computation and K sequel and like

00:38:21,770 --> 00:38:25,910
sectionals and Kafka streams what is

00:38:24,410 --> 00:38:27,440
happening behind the scenes in order to

00:38:25,910 --> 00:38:29,690
ensure fault tolerant processing so

00:38:27,440 --> 00:38:31,460
right you don't want to miss to process

00:38:29,690 --> 00:38:33,920
a transaction like a payment if the

00:38:31,460 --> 00:38:36,369
machine fails and also you don't want to

00:38:33,920 --> 00:38:38,690
processes more than once

00:38:36,369 --> 00:38:39,950
what is going on there and then I think

00:38:38,690 --> 00:38:43,789
this is also part of your question is

00:38:39,950 --> 00:38:45,920
this efficient so of course this is like

00:38:43,789 --> 00:38:48,619
the high-level overview what is

00:38:45,920 --> 00:38:50,839
happening is that all the computation

00:38:48,619 --> 00:38:52,400
that happens on a particular Kasich

00:38:50,839 --> 00:38:54,140
server happens locally so you refer to

00:38:52,400 --> 00:38:57,559
that as no we're using behind the scenes

00:38:54,140 --> 00:38:59,180
works to be as a local storage engine

00:38:57,559 --> 00:39:01,250
for doing Network yes so that is what is

00:38:59,180 --> 00:39:02,240
happening well then it's also happening

00:39:01,250 --> 00:39:03,740
which is the second part of your

00:39:02,240 --> 00:39:07,220
question is that for fault tolerance

00:39:03,740 --> 00:39:09,680
reasons these changes to work stee-rike

00:39:07,220 --> 00:39:11,329
the state and the storage part is being

00:39:09,680 --> 00:39:13,460
backed up to Kafka and is this efficient

00:39:11,329 --> 00:39:14,569
said yes this is pretty efficient so

00:39:13,460 --> 00:39:16,069
there are a lot of optimizations that

00:39:14,569 --> 00:39:17,630
are being done behind the scenes so that

00:39:16,069 --> 00:39:19,250
nobody or not if you're getting 1

00:39:17,630 --> 00:39:21,920
million input mate records that are

00:39:19,250 --> 00:39:24,020
mutating a table you're not sending 1

00:39:21,920 --> 00:39:27,740
million records to the changelog topic

00:39:24,020 --> 00:39:30,289
there so there is some let's say it's a

00:39:27,740 --> 00:39:31,339
compaction being done for these updates

00:39:30,289 --> 00:39:33,770
so that this is actually rather

00:39:31,339 --> 00:39:34,550
efficient the feature that you would

00:39:33,770 --> 00:39:37,220
look up

00:39:34,550 --> 00:39:40,700
or in the documentation would be record

00:39:37,220 --> 00:39:42,920
caching which allows you to tune you can

00:39:40,700 --> 00:39:45,490
also choose the encasing about for most

00:39:42,920 --> 00:39:48,470
use case you don't need to look at that

00:39:45,490 --> 00:39:50,330
this allows you to say how many of these

00:39:48,470 --> 00:39:52,430
let's say intermediate updates

00:39:50,330 --> 00:39:55,820
intermediate results would need be

00:39:52,430 --> 00:39:57,530
compacted like squashed away before it

00:39:55,820 --> 00:39:58,670
is being sent into such a topic or

00:39:57,530 --> 00:40:01,610
before they sent to a downstream

00:39:58,670 --> 00:40:03,200
operation as well so that that is

00:40:01,610 --> 00:40:05,270
optimized a lot behind the scenes

00:40:03,200 --> 00:40:06,530
because you're right this requires a

00:40:05,270 --> 00:40:08,000
round trip across the network or at

00:40:06,530 --> 00:40:13,610
least like one run trip across the

00:40:08,000 --> 00:40:15,680
network hi first of all thanks for the

00:40:13,610 --> 00:40:18,260
talk I would like to know if it's

00:40:15,680 --> 00:40:20,510
possible to use case equal when your

00:40:18,260 --> 00:40:23,690
messages are encoded with formats like

00:40:20,510 --> 00:40:28,550
protobuf and especially when you publish

00:40:23,690 --> 00:40:30,230
a schema update so not sure whether the

00:40:28,550 --> 00:40:31,340
microphone is record or not so the

00:40:30,230 --> 00:40:32,960
question was about supported data

00:40:31,340 --> 00:40:35,510
formats particularly if they now have

00:40:32,960 --> 00:40:38,330
schemas are involved at the moment case

00:40:35,510 --> 00:40:41,540
acute supports three broad categories of

00:40:38,330 --> 00:40:45,500
data one is no delimited thing our CSV

00:40:41,540 --> 00:40:48,760
data one is JSON and one is afro ever is

00:40:45,500 --> 00:40:51,880
an example of no read from the schema

00:40:48,760 --> 00:40:56,150
wrote above is currently not supported

00:40:51,880 --> 00:40:57,260
so no there's no protobuf support the

00:40:56,150 --> 00:40:58,760
reason why that is not is because

00:40:57,260 --> 00:41:02,690
currently we are wrapping up work on

00:40:58,760 --> 00:41:04,880
integrating struct support so a very

00:41:02,690 --> 00:41:06,800
good support for nested data once we

00:41:04,880 --> 00:41:08,420
have that then we're looking at no

00:41:06,800 --> 00:41:09,590
opening the kind of forms to have like

00:41:08,420 --> 00:41:14,660
pluggable data formats including

00:41:09,590 --> 00:41:16,910
protobuf so thank you for your

00:41:14,660 --> 00:41:18,170
presentation cool so we run out of time

00:41:16,910 --> 00:41:22,700
if you have more questions you know just

00:41:18,170 --> 00:41:25,730
let me above - yeah where if you want

00:41:22,700 --> 00:41:26,510
you can meet him later thank you very

00:41:25,730 --> 00:41:30,329
much thank you

00:41:26,510 --> 00:41:30,329

YouTube URL: https://www.youtube.com/watch?v=nf4enboASio


