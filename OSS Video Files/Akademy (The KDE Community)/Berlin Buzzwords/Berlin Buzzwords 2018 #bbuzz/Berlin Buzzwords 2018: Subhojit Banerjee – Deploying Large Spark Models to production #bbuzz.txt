Title: Berlin Buzzwords 2018: Subhojit Banerjee – Deploying Large Spark Models to production #bbuzz
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Subhojit Banerjee talking about "Deploying Large Spark Models to production and model scoring in near real time".

1. How does one build a pyspark model and deploy it in a scala pipeline with no code rewrite - Solving the greatest fights between data scientist who want to code in python and data engineers who like the tried and tested type safety of the JVM.
2. How does one beat the spark context latency to serve spark models in milliseconds to handle near realtime business needs
3. How does one build a ML model, zip it up and deploy it across platforms in a completely vendor neutral way i.e. build your model on AWS and deploy it on GCP or vice-versa.
4. How does one leverage the years of efforts spent in software engineering and use it directly in building data science pipelines without reinventing the wheel and pain.
5. How does on build a completely GDPR compliant machine learning model with 0.88 on the ROC curve.

Read more:
https://2018.berlinbuzzwords.de/18/session/deploying-large-spark-models-production-and-model-scoring-near-real-time

About Subhojit Banerjee:
https://2018.berlinbuzzwords.de/users/subhojit-banerjee

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,630 --> 00:00:11,020
good morning as the boys clear

00:00:08,430 --> 00:00:13,990
modulation do I have to speak up speak

00:00:11,020 --> 00:00:14,710
down is this can everyone hear cool

00:00:13,990 --> 00:00:16,390
thanks

00:00:14,710 --> 00:00:19,240
firstly a note of thanks to the

00:00:16,390 --> 00:00:21,970
organizers and thank you guys for coming

00:00:19,240 --> 00:00:25,570
to this talk the intent of this talk is

00:00:21,970 --> 00:00:26,950
to share my war stories and hopefully

00:00:25,570 --> 00:00:29,140
you guys don't have to go through the

00:00:26,950 --> 00:00:36,160
same pain of deploying large spark

00:00:29,140 --> 00:00:39,250
models in production so so big leader is

00:00:36,160 --> 00:00:42,040
not easy and I apologize for the term

00:00:39,250 --> 00:00:44,830
big data for let's for our definition

00:00:42,040 --> 00:00:48,820
say data that doesn't fit into a single

00:00:44,830 --> 00:00:50,620
node and the reason big data is not easy

00:00:48,820 --> 00:00:53,490
is because it sets at a confluence of

00:00:50,620 --> 00:00:56,200
three very diverse and very deep fears

00:00:53,490 --> 00:00:58,360
machine learning distributed systems and

00:00:56,200 --> 00:01:00,040
domain of business understanding now you

00:00:58,360 --> 00:01:02,320
need to merge and mix these three

00:01:00,040 --> 00:01:04,089
different fields to make profits or

00:01:02,320 --> 00:01:08,770
revenue which is a very difficult task

00:01:04,089 --> 00:01:11,380
in itself so Gartner found that only 15%

00:01:08,770 --> 00:01:17,080
of companies have machine learning

00:01:11,380 --> 00:01:21,430
models in production and the top five

00:01:17,080 --> 00:01:26,650
vendors are in losses and ninety percent

00:01:21,430 --> 00:01:29,350
of all data leaks fail so does Big Data

00:01:26,650 --> 00:01:31,630
promise that was given to us you know

00:01:29,350 --> 00:01:35,950
it's failing in some ways where is it

00:01:31,630 --> 00:01:38,590
failing Gutner tried again to find out

00:01:35,950 --> 00:01:42,780
why this is failing the top the top two

00:01:38,590 --> 00:01:46,780
are very organizational related reasons

00:01:42,780 --> 00:01:48,670
so last yesterday mention about the data

00:01:46,780 --> 00:01:50,080
being in silos and it's very difficult

00:01:48,670 --> 00:01:52,330
to get the data out of those silos

00:01:50,080 --> 00:01:55,930
also companies do not yet know how to

00:01:52,330 --> 00:01:58,030
take advantage of big data but this talk

00:01:55,930 --> 00:02:00,549
is not about the first two slifer's two

00:01:58,030 --> 00:02:03,400
points this talk is about the last point

00:02:00,549 --> 00:02:07,349
that is how do we deploy machine

00:02:03,400 --> 00:02:10,810
learning models quickly to production

00:02:07,349 --> 00:02:14,079
the slide that you see here this is the

00:02:10,810 --> 00:02:15,939
ML code and this is from the paper

00:02:14,079 --> 00:02:18,130
hidden technical debt in machine

00:02:15,939 --> 00:02:22,870
learning this was

00:02:18,130 --> 00:02:24,970
nips 2015 and the this this part of the

00:02:22,870 --> 00:02:27,880
code is the machine learning code right

00:02:24,970 --> 00:02:29,500
and the the color gradation is based on

00:02:27,880 --> 00:02:31,660
the complexity of the code so this is

00:02:29,500 --> 00:02:33,400
possibly the most complex part of the

00:02:31,660 --> 00:02:36,610
code and this is possibly the easiest

00:02:33,400 --> 00:02:40,510
part of the code but the size of these

00:02:36,610 --> 00:02:42,900
boxes represent the amount of code that

00:02:40,510 --> 00:02:45,480
you have to write most of the media

00:02:42,900 --> 00:02:48,640
concentrates on this part of the code

00:02:45,480 --> 00:02:50,410
but to ensure that the entire data

00:02:48,640 --> 00:02:52,870
pipeline the production pipeline goes

00:02:50,410 --> 00:02:55,000
into production you know you need to

00:02:52,870 --> 00:02:56,980
talk about configuration data collection

00:02:55,000 --> 00:02:59,800
feature extraction process management

00:02:56,980 --> 00:03:04,990
analysis tools serving infrastructure AV

00:02:59,800 --> 00:03:08,440
testing and the entire kitchen sink as

00:03:04,990 --> 00:03:10,810
per the paper 5% of your code base is

00:03:08,440 --> 00:03:18,010
machine learning code the remaining 95%

00:03:10,810 --> 00:03:19,810
is the glue code the way I structured

00:03:18,010 --> 00:03:22,630
this presentation is I'll be going

00:03:19,810 --> 00:03:25,060
through a business use case I'll present

00:03:22,630 --> 00:03:26,860
the first solution what problems have

00:03:25,060 --> 00:03:28,840
faced I'll mention about a better

00:03:26,860 --> 00:03:30,460
solution and the most interesting part

00:03:28,840 --> 00:03:32,560
the demo hopefully they'd ever go

00:03:30,460 --> 00:03:33,940
observe with us today I'll present the

00:03:32,560 --> 00:03:38,620
conclusions and then we'll be having

00:03:33,940 --> 00:03:40,390
questions so key takeaways at the end of

00:03:38,620 --> 00:03:42,670
this presentation hopefully you can take

00:03:40,390 --> 00:03:45,400
this along with you

00:03:42,670 --> 00:03:47,050
a better than random model is has

00:03:45,400 --> 00:03:49,480
revenue generating propensity from day

00:03:47,050 --> 00:03:51,370
one and I'm not talking about medical

00:03:49,480 --> 00:03:55,060
grade machine learning models you know

00:03:51,370 --> 00:03:57,330
for that you possibly want 90% say

00:03:55,060 --> 00:04:00,460
possibly an ROC of 0.9 on the ROC curve

00:03:57,330 --> 00:04:02,410
but you know for standard business

00:04:00,460 --> 00:04:04,120
related machine learning models a better

00:04:02,410 --> 00:04:05,680
than random model has a better

00:04:04,120 --> 00:04:07,990
propensity of generating revenue right

00:04:05,680 --> 00:04:09,880
from day one pi spark models can be

00:04:07,990 --> 00:04:12,400
deployed in scalar pipelines so now

00:04:09,880 --> 00:04:14,170
there is a war going on between data

00:04:12,400 --> 00:04:16,090
scientist and data engineers you know

00:04:14,170 --> 00:04:18,850
data scientist want to write their code

00:04:16,090 --> 00:04:20,980
in Python and R and push it over the

00:04:18,850 --> 00:04:23,590
wall to the data engineers who have to

00:04:20,980 --> 00:04:25,000
again learn what the data scientists

00:04:23,590 --> 00:04:27,310
were thinking and write it into a Java

00:04:25,000 --> 00:04:30,370
code or a scalar code you know and in

00:04:27,310 --> 00:04:32,050
this process of throwing the models

00:04:30,370 --> 00:04:33,220
across the wall

00:04:32,050 --> 00:04:35,680
certain assumptions that the data

00:04:33,220 --> 00:04:37,690
scientists made just falls apart

00:04:35,680 --> 00:04:39,940
and so it becomes a huge pain for the

00:04:37,690 --> 00:04:42,490
data engineers who don't know about the

00:04:39,940 --> 00:04:45,699
data science process to write code in

00:04:42,490 --> 00:04:48,250
Java that mimics the entire complex data

00:04:45,699 --> 00:04:53,199
science pipeline data science or the

00:04:48,250 --> 00:04:55,210
machine learning models right so the

00:04:53,199 --> 00:04:57,750
intent here would be to show you how

00:04:55,210 --> 00:05:00,190
data scientists can directly without

00:04:57,750 --> 00:05:02,199
data engineers directly without seeing

00:05:00,190 --> 00:05:04,990
any part of the data science code can

00:05:02,199 --> 00:05:07,240
push the models into production or have

00:05:04,990 --> 00:05:08,409
a pipeline for that spark models can be

00:05:07,240 --> 00:05:10,930
scored in near-real-time

00:05:08,409 --> 00:05:13,449
so spark you know it good it's very good

00:05:10,930 --> 00:05:16,330
for large processing but when you have

00:05:13,449 --> 00:05:18,759
to process a single row of data spark is

00:05:16,330 --> 00:05:20,949
not that good because of the distributor

00:05:18,759 --> 00:05:23,620
tax and I'll go into it how we can solve

00:05:20,949 --> 00:05:26,889
that problem spot models can be dock

00:05:23,620 --> 00:05:29,889
rised and once you docker is a model so

00:05:26,889 --> 00:05:31,659
docker today is the currency of scale if

00:05:29,889 --> 00:05:34,360
you can juggle as a model you

00:05:31,659 --> 00:05:37,360
immediately get the advantages of CI c/d

00:05:34,360 --> 00:05:41,199
a/b testing scale without you needing to

00:05:37,360 --> 00:05:43,930
put any effort on that when I say you

00:05:41,199 --> 00:05:46,659
it's the data scientist and all this

00:05:43,930 --> 00:05:51,340
above can be done in a matter of minutes

00:05:46,659 --> 00:05:53,740
and I'll show you how in the demo and if

00:05:51,340 --> 00:05:55,900
you are in Europe gdpr has already come

00:05:53,740 --> 00:05:58,150
in and so if you have a data scientist

00:05:55,900 --> 00:05:58,719
you need to take care of personal

00:05:58,150 --> 00:06:00,759
information

00:05:58,719 --> 00:06:02,500
suta anonymize data using pseudo

00:06:00,759 --> 00:06:05,229
anonymous data it is possible to get

00:06:02,500 --> 00:06:08,889
good machine learning models and I'll

00:06:05,229 --> 00:06:14,169
show you how I got a result of 0.88 on

00:06:08,889 --> 00:06:19,779
the ROC curve in one of my gates so

00:06:14,169 --> 00:06:22,599
roughly in March last year business came

00:06:19,779 --> 00:06:24,969
to me and said that boogy we have this

00:06:22,599 --> 00:06:27,849
website people come to this website and

00:06:24,969 --> 00:06:29,710
buy stuff in real time can you even

00:06:27,849 --> 00:06:31,240
before the session completes can you

00:06:29,710 --> 00:06:33,819
identify whether the user is going to

00:06:31,240 --> 00:06:35,680
buy or not buy a product you know so

00:06:33,819 --> 00:06:38,860
that if he's if you know the user intent

00:06:35,680 --> 00:06:40,180
we can show him certain discounts so

00:06:38,860 --> 00:06:42,339
that we can change the intent of the

00:06:40,180 --> 00:06:44,800
user right so it's a very standard

00:06:42,339 --> 00:06:45,879
problem nothing complex trivial it's a

00:06:44,800 --> 00:06:48,219
real-time segmentation

00:06:45,879 --> 00:06:51,669
of user into a real-time classification

00:06:48,219 --> 00:06:54,550
of the user into by versus differ but

00:06:51,669 --> 00:06:57,789
what makes this problem tough is it has

00:06:54,550 --> 00:06:59,589
to be real-time and it what it makes

00:06:57,789 --> 00:07:00,669
even more tougher is that it has to be

00:06:59,589 --> 00:07:04,330
gdpr compliant

00:07:00,669 --> 00:07:06,219
right so use a segmentation on data on

00:07:04,330 --> 00:07:07,330
using personalized data that is it's a

00:07:06,219 --> 00:07:09,729
solved problem you have it everywhere

00:07:07,330 --> 00:07:11,080
right but on an anima is data that is

00:07:09,729 --> 00:07:14,409
something that makes this problem

00:07:11,080 --> 00:07:16,330
interesting but first we need to collect

00:07:14,409 --> 00:07:19,149
data right this is the real time even we

00:07:16,330 --> 00:07:20,860
need to collect the events and I need to

00:07:19,149 --> 00:07:22,240
also mention about the constraints so

00:07:20,860 --> 00:07:23,349
that I was the only guy who was doing

00:07:22,240 --> 00:07:25,809
both the real science at the data

00:07:23,349 --> 00:07:27,849
engineering and I wanted to limit the

00:07:25,809 --> 00:07:29,860
DevOps because then if I don't limit the

00:07:27,849 --> 00:07:32,259
devops then I have to do the divorce as

00:07:29,860 --> 00:07:34,779
well you know so from day one I had to

00:07:32,259 --> 00:07:36,490
use all managed infrastructure as much

00:07:34,779 --> 00:07:39,369
as possible so that I can concentrate on

00:07:36,490 --> 00:07:41,619
the good parts so the solution that I

00:07:39,369 --> 00:07:45,999
chose was snowplow who have you who has

00:07:41,619 --> 00:07:47,889
used snowplow before snowplow is a

00:07:45,999 --> 00:07:50,649
skinner based even collection and

00:07:47,889 --> 00:07:53,919
processing tool the good thing is it's

00:07:50,649 --> 00:07:56,619
just plug plug and play so you have the

00:07:53,919 --> 00:07:59,289
website you can collect events from the

00:07:56,619 --> 00:08:01,300
website in real time you can then

00:07:59,289 --> 00:08:03,759
transform and enrich it and then dump

00:08:01,300 --> 00:08:05,469
the data into s3 or you can even plug in

00:08:03,759 --> 00:08:07,839
your machine learning api's on the

00:08:05,469 --> 00:08:09,039
events as they are coming in and each of

00:08:07,839 --> 00:08:11,199
those components that you see they

00:08:09,039 --> 00:08:14,949
collect the transform these are

00:08:11,199 --> 00:08:17,829
individually scalable so you can have

00:08:14,949 --> 00:08:19,869
multiple copies of this of these same

00:08:17,829 --> 00:08:23,079
and then you can scale up and down

00:08:19,869 --> 00:08:24,459
depending upon your needs so this is

00:08:23,079 --> 00:08:26,579
your standard lambda architecture

00:08:24,459 --> 00:08:29,379
nothing fancy you have all seen this

00:08:26,579 --> 00:08:32,800
this is the data comes in the real-time

00:08:29,379 --> 00:08:35,680
evens come in it this is based on AWS

00:08:32,800 --> 00:08:37,899
it's hitting the kindnesses roster from

00:08:35,680 --> 00:08:40,029
the kindnesses roster it comes into the

00:08:37,899 --> 00:08:42,310
kindnesses enrich and also it goes to

00:08:40,029 --> 00:08:46,300
the batch processing pipeline here you

00:08:42,310 --> 00:08:50,800
have a schema schema registry or a clue

00:08:46,300 --> 00:08:53,649
server that basically disambiguates the

00:08:50,800 --> 00:08:55,180
good events from the bad events and the

00:08:53,649 --> 00:08:56,949
good events from the good even so you

00:08:55,180 --> 00:08:58,779
can actually hook it up to elasticsearch

00:08:56,949 --> 00:09:00,580
or a machine learning api to actually

00:08:58,779 --> 00:09:02,290
get

00:09:00,580 --> 00:09:04,870
or the bad events where you can actually

00:09:02,290 --> 00:09:07,150
process it later on the on this side you

00:09:04,870 --> 00:09:09,640
actually have the events getting

00:09:07,150 --> 00:09:12,940
collected and once it lands on s3

00:09:09,640 --> 00:09:16,780
you can use EMR or any big data

00:09:12,940 --> 00:09:25,540
processing tool to create models out of

00:09:16,780 --> 00:09:29,530
it I don't and this is a conference of

00:09:25,540 --> 00:09:31,600
scale people have shown huge events but

00:09:29,530 --> 00:09:33,640
this was at the start of when I was

00:09:31,600 --> 00:09:35,410
starting of the project we just had 23

00:09:33,640 --> 00:09:37,630
million events per day it's not very big

00:09:35,410 --> 00:09:39,280
but that doesn't mean that we didn't

00:09:37,630 --> 00:09:41,110
have the we shouldn't have the

00:09:39,280 --> 00:09:42,850
discipline of ensuring that it can be

00:09:41,110 --> 00:09:45,760
auto scaled because the traffic was

00:09:42,850 --> 00:09:47,890
pretty volatile so we collected about

00:09:45,760 --> 00:09:49,720
150 140 gigs of data this was for the

00:09:47,890 --> 00:09:52,920
initial machine learning model building

00:09:49,720 --> 00:09:55,810
and now we have collected a lot of data

00:09:52,920 --> 00:09:57,550
once this pipeline has been set up and

00:09:55,810 --> 00:09:59,940
the cost was pretty cheap you know six

00:09:57,550 --> 00:10:03,580
to 12 euros per day and this was on AWS

00:09:59,940 --> 00:10:05,800
so the first solution I tried to do a

00:10:03,580 --> 00:10:09,360
performance test and just I doubled the

00:10:05,800 --> 00:10:12,340
load on that just to check whether my

00:10:09,360 --> 00:10:15,280
where my pipeline is able to take the

00:10:12,340 --> 00:10:17,410
real time even so not and most of the 99

00:10:15,280 --> 00:10:19,360
percentile I was able to get with less

00:10:17,410 --> 00:10:20,530
than half a second with no errors which

00:10:19,360 --> 00:10:24,610
was good enough for me

00:10:20,530 --> 00:10:27,000
for to get this process started ok so

00:10:24,610 --> 00:10:30,940
now that I have the business use case

00:10:27,000 --> 00:10:32,140
how can we have a real-time pipeline

00:10:30,940 --> 00:10:35,920
that we can train and screw around with

00:10:32,140 --> 00:10:38,410
ml models on the first model that I used

00:10:35,920 --> 00:10:40,750
was a Markov chain model and remember

00:10:38,410 --> 00:10:44,320
the GDP our compliance thing I cannot

00:10:40,750 --> 00:10:46,720
use any of the person's ADA so what I

00:10:44,320 --> 00:10:48,640
took was the sequence of the web pages

00:10:46,720 --> 00:10:53,650
visited and then I created a Markov

00:10:48,640 --> 00:10:56,980
chain model out of on that it was I made

00:10:53,650 --> 00:11:00,930
the model in our I uploaded it to s3 and

00:10:56,980 --> 00:11:05,160
using lambda I exposed it as an API

00:11:00,930 --> 00:11:08,770
right but there were some problems faced

00:11:05,160 --> 00:11:12,700
AWS lambda gives you a restriction of 50

00:11:08,770 --> 00:11:16,540
MB compressed size so on my first try

00:11:12,700 --> 00:11:18,970
to hit 212 MB of the zipped size of the

00:11:16,540 --> 00:11:21,010
model so I had to hack on it I brought

00:11:18,970 --> 00:11:22,960
it down to 26 MB but I wouldn't

00:11:21,010 --> 00:11:26,560
recommend anyone hacking on the core our

00:11:22,960 --> 00:11:28,780
libraries are is not supported on AWS

00:11:26,560 --> 00:11:31,060
lambda so I had to create a Python

00:11:28,780 --> 00:11:32,580
wrapper on top of it so that I can use

00:11:31,060 --> 00:11:35,560
my AWS lambda on it

00:11:32,580 --> 00:11:37,000
but here comes the last problem and this

00:11:35,560 --> 00:11:39,910
is the final coffin in the nail for the

00:11:37,000 --> 00:11:41,590
Markov chain model every time front-end

00:11:39,910 --> 00:11:44,380
would change the sequence of webpages

00:11:41,590 --> 00:11:46,390
visited would change and hence my Markov

00:11:44,380 --> 00:11:48,940
chain model that I built on the old data

00:11:46,390 --> 00:11:51,490
is not valid anymore right

00:11:48,940 --> 00:11:53,380
so it's stall again we hit the cold

00:11:51,490 --> 00:11:56,860
start problem for every time front-end

00:11:53,380 --> 00:11:58,930
would change so we needed a better

00:11:56,860 --> 00:12:00,340
solution but before a better solutions

00:11:58,930 --> 00:12:04,440
have a few learnings you know the effort

00:12:00,340 --> 00:12:07,990
was well worth it because I could see

00:12:04,440 --> 00:12:11,050
almost my practice the economies of

00:12:07,990 --> 00:12:13,180
Solace and this is from John McAfee

00:12:11,050 --> 00:12:17,380
sorry for the name dropping this

00:12:13,180 --> 00:12:19,450
gentleman had way back in 1961 had said

00:12:17,380 --> 00:12:21,640
that computing is one day going to be a

00:12:19,450 --> 00:12:23,350
utility so you just like you open your

00:12:21,640 --> 00:12:25,060
tap and you you just pay for the amount

00:12:23,350 --> 00:12:27,220
of water that you use you could just

00:12:25,060 --> 00:12:29,170
start using computing and you could just

00:12:27,220 --> 00:12:33,160
pay only for the compute that you can

00:12:29,170 --> 00:12:36,490
use so it was really nice to see in real

00:12:33,160 --> 00:12:41,110
life on the what the dream was and how

00:12:36,490 --> 00:12:42,670
it actually materialized one fact though

00:12:41,110 --> 00:12:44,080
was that John McCarthy was a Turing

00:12:42,670 --> 00:12:46,330
Award winner and I had married three

00:12:44,080 --> 00:12:52,120
times no correlation between those two

00:12:46,330 --> 00:12:54,100
though requirements so we had to come up

00:12:52,120 --> 00:12:56,020
with a better solution the better

00:12:54,100 --> 00:12:58,270
solution was I had to decrease the time

00:12:56,020 --> 00:12:59,830
of the models to move from notebook to

00:12:58,270 --> 00:13:03,760
production right that's very important

00:12:59,830 --> 00:13:05,380
for me that'll be super scalable and the

00:13:03,760 --> 00:13:07,630
PI spark models I should be able to

00:13:05,380 --> 00:13:10,360
deploy it with minimal or almost zero

00:13:07,630 --> 00:13:11,950
code change and serving our inference

00:13:10,360 --> 00:13:13,120
had to be super fast because this is a

00:13:11,950 --> 00:13:16,480
real-time use case that we are talking

00:13:13,120 --> 00:13:19,030
about technical analysis of this user

00:13:16,480 --> 00:13:21,160
requirement for model super fast for

00:13:19,030 --> 00:13:23,140
model serving to be super fast we had to

00:13:21,160 --> 00:13:24,760
take it out of the super of the spark

00:13:23,140 --> 00:13:25,870
context because of the distributor tax

00:13:24,760 --> 00:13:28,150
of spark

00:13:25,870 --> 00:13:31,270
it has to be completely vendor-neutral I

00:13:28,150 --> 00:13:32,620
I mean if I'd build my model in the my

00:13:31,270 --> 00:13:35,320
own datacenter I should be able to

00:13:32,620 --> 00:13:36,790
deploy it to AWS or I build it on a

00:13:35,320 --> 00:13:41,500
tablet I should be able to deploy it on

00:13:36,790 --> 00:13:44,680
GCP and it has to be truly portable that

00:13:41,500 --> 00:13:47,830
is can I see realize the model and send

00:13:44,680 --> 00:13:52,600
it to my coworker and he scores the

00:13:47,830 --> 00:13:56,050
model in his own ecosystem you know so

00:13:52,600 --> 00:14:03,190
this was the stringent requirements I

00:13:56,050 --> 00:14:06,790
had but wise pass slow in scoring spark

00:14:03,190 --> 00:14:08,800
it's very good for large data processing

00:14:06,790 --> 00:14:10,779
you know but while processing a large

00:14:08,800 --> 00:14:12,850
amount of data it creates a stack it

00:14:10,779 --> 00:14:15,700
creates this execution plan and it

00:14:12,850 --> 00:14:18,100
creates this lineage for large data

00:14:15,700 --> 00:14:20,350
that's fine because this distributed tax

00:14:18,100 --> 00:14:22,630
is being spread across to this entire

00:14:20,350 --> 00:14:24,910
large the entire data set right which

00:14:22,630 --> 00:14:26,890
runs into gigs that's fine but when you

00:14:24,910 --> 00:14:29,230
are scoring a single row of data this

00:14:26,890 --> 00:14:31,510
distributed tax becomes very cumbersome

00:14:29,230 --> 00:14:35,589
for one row of data to actually use for

00:14:31,510 --> 00:14:38,740
real-time processing so what were my

00:14:35,589 --> 00:14:40,779
available options that I had to bring

00:14:38,740 --> 00:14:45,130
the spark model out of the spark

00:14:40,779 --> 00:14:47,410
ecosystem I had a few metrics and I had

00:14:45,130 --> 00:14:50,080
a few options these slides will be

00:14:47,410 --> 00:14:51,580
available to you later on so you can

00:14:50,080 --> 00:14:53,740
actually go what are the strengths and

00:14:51,580 --> 00:14:56,260
weaknesses of it but finally I chose

00:14:53,740 --> 00:14:58,209
Emily because it had support for the

00:14:56,260 --> 00:15:00,790
languages that I was using and the most

00:14:58,209 --> 00:15:04,080
important part was my entire pipeline I

00:15:00,790 --> 00:15:07,120
could basically deploy it outside the

00:15:04,080 --> 00:15:12,520
spark ecosystem so that was not

00:15:07,120 --> 00:15:15,130
available in the other products and also

00:15:12,520 --> 00:15:18,490
I need to mention that data breaks has

00:15:15,130 --> 00:15:20,680
its own DB ml which also does this but

00:15:18,490 --> 00:15:24,190
at that time it was only available to

00:15:20,680 --> 00:15:27,730
the data breaks customers and I wanted

00:15:24,190 --> 00:15:28,209
something that was open-source first

00:15:27,730 --> 00:15:31,240
things first

00:15:28,209 --> 00:15:32,950
Emily was is possible because of the

00:15:31,240 --> 00:15:36,430
good work of Holland Wilkinson mikowski

00:15:32,950 --> 00:15:38,650
minute what is a B it's a common

00:15:36,430 --> 00:15:39,730
serialization format for executing

00:15:38,650 --> 00:15:42,100
machine learning

00:15:39,730 --> 00:15:45,970
blinds it supports sparks chi can learn

00:15:42,100 --> 00:15:49,060
intensive law and once you see there is

00:15:45,970 --> 00:15:50,649
a model you can run it on AWS GCP so you

00:15:49,060 --> 00:15:52,269
get the model out as a bundle as a

00:15:50,649 --> 00:15:55,420
serialized model and you can just run it

00:15:52,269 --> 00:15:56,829
anywhere using a docker container for

00:15:55,420 --> 00:15:58,959
most parts you don't have to change any

00:15:56,829 --> 00:16:01,149
of your internal code and it's open so

00:15:58,959 --> 00:16:03,220
you can go under the hood and change the

00:16:01,149 --> 00:16:05,290
code yourselves in fact I had to change

00:16:03,220 --> 00:16:07,690
a part of the code and I'll show you

00:16:05,290 --> 00:16:10,630
that part so that I could deploy it on

00:16:07,690 --> 00:16:13,000
areas so this is just a visual

00:16:10,630 --> 00:16:14,350
representation of how this looks like so

00:16:13,000 --> 00:16:18,760
you have the data layer you have spark

00:16:14,350 --> 00:16:21,160
and on top of it you have Emily which

00:16:18,760 --> 00:16:24,010
creates MD bundle which is the

00:16:21,160 --> 00:16:27,100
serialized model and then using the

00:16:24,010 --> 00:16:30,100
Emily brown time you can actually create

00:16:27,100 --> 00:16:34,480
an API using the MVP runtime on top of

00:16:30,100 --> 00:16:37,480
it this is a quick components that it

00:16:34,480 --> 00:16:40,000
has the the main thing here is the

00:16:37,480 --> 00:16:42,670
linear algebra library that is very

00:16:40,000 --> 00:16:46,870
important because the linear algebra

00:16:42,670 --> 00:16:49,360
library it that is the main thing that

00:16:46,870 --> 00:16:51,100
routes you back to the spark ecosystem

00:16:49,360 --> 00:16:54,550
so if you have one-to-one mapping

00:16:51,100 --> 00:16:56,740
between M leap and spark you can move

00:16:54,550 --> 00:16:59,079
all these models away from this particle

00:16:56,740 --> 00:17:01,029
system to the new ecosystem I mean if

00:16:59,079 --> 00:17:02,199
you look at the complexity of what we

00:17:01,029 --> 00:17:03,760
are trying to achieve

00:17:02,199 --> 00:17:06,040
you're trying to move their entire spark

00:17:03,760 --> 00:17:08,079
models out of spark so all the data

00:17:06,040 --> 00:17:09,910
frames transformers estimators that you

00:17:08,079 --> 00:17:16,720
have in the spark ago system should also

00:17:09,910 --> 00:17:19,959
be available in the new ecosystem the so

00:17:16,720 --> 00:17:22,449
this bit the bundle ml this is one of

00:17:19,959 --> 00:17:24,400
them cool components and what it does it

00:17:22,449 --> 00:17:26,319
provides you a common serialization for

00:17:24,400 --> 00:17:29,770
your spark and Emily it is 100 percent

00:17:26,319 --> 00:17:32,650
wrote above and JSON based so it's so

00:17:29,770 --> 00:17:34,900
you can deploy it anywhere and because

00:17:32,650 --> 00:17:37,740
most of the supporting systems today

00:17:34,900 --> 00:17:39,880
have a support for protobuf and json and

00:17:37,740 --> 00:17:42,610
can be written to zip files and it's

00:17:39,880 --> 00:17:46,540
completely portable so yeah and this is

00:17:42,610 --> 00:17:48,700
the m leap pipeline so I would one of

00:17:46,540 --> 00:17:51,730
the so the in the demo what I'm going to

00:17:48,700 --> 00:17:54,160
do is create a model in spark use the M

00:17:51,730 --> 00:17:56,890
leap processing

00:17:54,160 --> 00:17:59,260
to convert at the spark model and

00:17:56,890 --> 00:18:02,640
serialize it out and then once it's been

00:17:59,260 --> 00:18:05,530
serialized I can expose it as an API and

00:18:02,640 --> 00:18:09,100
so this is a similar sort of diagram so

00:18:05,530 --> 00:18:12,520
I'll be building the my model in a

00:18:09,100 --> 00:18:15,870
notebook using spark and Hadoop and then

00:18:12,520 --> 00:18:18,610
I'll be using my Emily transformers

00:18:15,870 --> 00:18:21,010
basically I'll create a bundle out of

00:18:18,610 --> 00:18:25,090
that and then expose it so about that

00:18:21,010 --> 00:18:27,220
and expose it as a API right and all

00:18:25,090 --> 00:18:30,970
these containers can basically use the

00:18:27,220 --> 00:18:32,440
REST API to read the data these are the

00:18:30,970 --> 00:18:36,520
available power transformers and

00:18:32,440 --> 00:18:38,170
estimators that we have in Emily

00:18:36,520 --> 00:18:42,520
it supports almost everything that you

00:18:38,170 --> 00:18:46,270
have on spark the last time I checked it

00:18:42,520 --> 00:18:48,190
didn't have support for Els so but the

00:18:46,270 --> 00:18:50,020
thing is you you could do your custom

00:18:48,190 --> 00:19:02,590
transformation and you'll just get pull

00:18:50,020 --> 00:19:03,850
away from it okay so now the demo so we

00:19:02,590 --> 00:19:05,920
are going to build a PI Spock model

00:19:03,850 --> 00:19:08,080
using cable data in a chip eater

00:19:05,920 --> 00:19:10,510
notebook we are going to seal as a model

00:19:08,080 --> 00:19:12,220
in a JSON and protobuf format we are

00:19:10,510 --> 00:19:14,550
going to load the serialized pipeline in

00:19:12,220 --> 00:19:17,140
a docker container for near real-time

00:19:14,550 --> 00:19:19,420
model scoring and we are going to solve

00:19:17,140 --> 00:19:22,090
the docker container from a scalable AWS

00:19:19,420 --> 00:19:24,670
REST API in minutes now I'm using AWS

00:19:22,090 --> 00:19:27,400
but you could use any container based

00:19:24,670 --> 00:19:28,990
platform you know manage kubernetes

00:19:27,400 --> 00:19:32,160
works fine as well as long as it can

00:19:28,990 --> 00:19:40,350
just run docker you know okay let's

00:19:32,160 --> 00:19:46,920
shift now to some code is this visible

00:19:40,350 --> 00:19:46,920
is this visible yeah okay

00:19:47,430 --> 00:19:58,300
um you do you want me to enlarge it a

00:19:50,590 --> 00:20:00,610
bit it's it good yeah so this is the

00:19:58,300 --> 00:20:01,990
cattle there's one cattle computation

00:20:00,610 --> 00:20:04,750
where you have to predict house prices

00:20:01,990 --> 00:20:07,180
this is a sample I can't use production

00:20:04,750 --> 00:20:09,070
data so I'm just using this data to just

00:20:07,180 --> 00:20:11,470
show you this is the house sales data

00:20:09,070 --> 00:20:13,030
and in real time so basically I'm just

00:20:11,470 --> 00:20:15,490
going to run over and quickly create a

00:20:13,030 --> 00:20:17,560
model the intent of this notebook is not

00:20:15,490 --> 00:20:19,240
the actual data processing or creating

00:20:17,560 --> 00:20:20,770
the model but how once you create the

00:20:19,240 --> 00:20:21,970
model how do you expose it out you know

00:20:20,770 --> 00:20:25,300
that's the part that you're most

00:20:21,970 --> 00:20:27,400
interested in so so this is your

00:20:25,300 --> 00:20:30,730
standard thing that you would do you

00:20:27,400 --> 00:20:33,450
know find out the features find out the

00:20:30,730 --> 00:20:35,530
correlation between them and then

00:20:33,450 --> 00:20:38,740
basically identify which of these

00:20:35,530 --> 00:20:41,050
features gives better bang for your for

00:20:38,740 --> 00:20:43,300
the time spent and finally you come up

00:20:41,050 --> 00:20:46,600
with the features that are the most

00:20:43,300 --> 00:20:50,290
important you know so this is where it's

00:20:46,600 --> 00:20:52,270
time for spark to actually use this

00:20:50,290 --> 00:20:55,690
features that we have generated into

00:20:52,270 --> 00:20:57,790
this pipeline and ensure that we can use

00:20:55,690 --> 00:21:00,190
them leap from this right so this is

00:20:57,790 --> 00:21:03,660
where the actual I'll start my notebook

00:21:00,190 --> 00:21:08,140
chromium so I'm just using the standard

00:21:03,660 --> 00:21:12,190
libraries you know the spark libraries

00:21:08,140 --> 00:21:14,560
that you love and creating custom schema

00:21:12,190 --> 00:21:16,720
out of this and this is based on the

00:21:14,560 --> 00:21:21,070
fields or the features that I'll be

00:21:16,720 --> 00:21:23,170
using I'm reading the CSV file and then

00:21:21,070 --> 00:21:27,490
I have the continuous features and the

00:21:23,170 --> 00:21:30,160
categorical features here and I've run

00:21:27,490 --> 00:21:32,680
this notebook earlier so that because we

00:21:30,160 --> 00:21:34,660
have less time here but the the main

00:21:32,680 --> 00:21:36,880
part I'm going to run it and I'm going

00:21:34,660 --> 00:21:39,330
to show you this is just to show you the

00:21:36,880 --> 00:21:42,610
flow of thought as we are going forward

00:21:39,330 --> 00:21:44,710
and then I scale the features the

00:21:42,610 --> 00:21:47,980
convenience features and the categorical

00:21:44,710 --> 00:21:50,470
features so I run my transfer of the

00:21:47,980 --> 00:21:53,920
standard transformers on that and then I

00:21:50,470 --> 00:21:57,580
create a pipeline so let me just show

00:21:53,920 --> 00:21:59,530
you this so once I create the pipeline

00:21:57,580 --> 00:22:00,549
with both the pre transformers and the

00:21:59,530 --> 00:22:05,950
post transpose

00:22:00,549 --> 00:22:08,109
transformer Sonic I run a linear

00:22:05,950 --> 00:22:12,190
regression on this and what I do is

00:22:08,109 --> 00:22:14,200
basically I am going to transform the

00:22:12,190 --> 00:22:15,849
data set it's a very small data set so

00:22:14,200 --> 00:22:24,459
that I could just show you very quickly

00:22:15,849 --> 00:22:26,259
as part of this demo and yeah so yep so

00:22:24,459 --> 00:22:30,969
you have that and it's pretty quick

00:22:26,259 --> 00:22:33,039
let me just chime this in because it's a

00:22:30,969 --> 00:22:37,329
local data set let me just time this and

00:22:33,039 --> 00:22:42,399
just show you how much it how much this

00:22:37,329 --> 00:22:44,679
takes and it's close to two hundred and

00:22:42,399 --> 00:22:47,079
fifteen milliseconds this is not a

00:22:44,679 --> 00:22:51,190
distributed data set you know this is

00:22:47,079 --> 00:22:53,079
just a in house on my laptop or an Spock

00:22:51,190 --> 00:22:55,349
takes for a very small data set you know

00:22:53,079 --> 00:22:57,789
it's two hundred fifteen milliseconds

00:22:55,349 --> 00:23:01,359
now comes the important part okay so

00:22:57,789 --> 00:23:03,429
till here your data scientist can do the

00:23:01,359 --> 00:23:06,639
job okay till this part your data

00:23:03,429 --> 00:23:08,559
scientists do it and then this is where

00:23:06,639 --> 00:23:11,409
they would throw it to you to actually

00:23:08,559 --> 00:23:13,779
run your to basically change the entire

00:23:11,409 --> 00:23:15,459
code to into Java but they don't have to

00:23:13,779 --> 00:23:18,729
do it all that they need to do is

00:23:15,459 --> 00:23:20,859
basically run this bit of code just two

00:23:18,729 --> 00:23:24,899
lines import em leap and the serializer

00:23:20,859 --> 00:23:27,369
and what you need to do they do is just

00:23:24,899 --> 00:23:28,749
see they are seedless this bundle out

00:23:27,369 --> 00:23:30,669
that's all

00:23:28,749 --> 00:23:32,919
so your entire machine learning pipeline

00:23:30,669 --> 00:23:34,629
including the Transformers including the

00:23:32,919 --> 00:23:38,379
pre and post transformers everything is

00:23:34,629 --> 00:23:41,489
now out in the PI spark LR except let me

00:23:38,379 --> 00:23:41,489
just quickly show you that

00:23:48,030 --> 00:24:02,670
this is visible yeah this way okay let

00:23:56,980 --> 00:24:05,140
me just show you so we created this

00:24:02,670 --> 00:24:08,800
right now the PI's Park Allah does it

00:24:05,140 --> 00:24:10,809
okay now what I did was I moved the

00:24:08,800 --> 00:24:20,290
spice Park a lot except to separate

00:24:10,809 --> 00:24:23,050
folder here under models and I had

00:24:20,290 --> 00:24:24,760
created it yesterday night and so this

00:24:23,050 --> 00:24:36,700
is the model that we'll be using

00:24:24,760 --> 00:24:44,800
okay now what i'm doing here is no no no

00:24:36,700 --> 00:24:53,280
I can't shut down the okay um am i back

00:24:44,800 --> 00:24:53,280
to this okay

00:24:54,480 --> 00:25:02,799
so this guy over here is the M leap

00:24:58,960 --> 00:25:06,249
serving server it's running in the

00:25:02,799 --> 00:25:08,409
background and it's waiting from pushing

00:25:06,249 --> 00:25:10,989
the models and also it acting it's

00:25:08,409 --> 00:25:14,710
acting as an API server so that you can

00:25:10,989 --> 00:25:16,690
get you can push your models that is you

00:25:14,710 --> 00:25:21,909
can push your not only the models but

00:25:16,690 --> 00:25:27,850
also your API to score in real-time what

00:25:21,909 --> 00:25:32,259
I'm going to do is now I'm going to post

00:25:27,850 --> 00:25:36,330
the models we just now created so this

00:25:32,259 --> 00:25:36,330
was the model we had okay let me just

00:25:42,020 --> 00:25:46,740
so this was the model that we had so the

00:25:44,970 --> 00:25:49,080
only server that is running I need to

00:25:46,740 --> 00:25:53,660
push this model the newly created model

00:25:49,080 --> 00:25:57,900
to the M leap server so that's done and

00:25:53,660 --> 00:26:00,150
now I need to see ok very quickly though

00:25:57,900 --> 00:26:02,900
let me also show you the JSON file which

00:26:00,150 --> 00:26:02,900
I will be sending across

00:26:16,910 --> 00:26:22,290
so this is the JSON files that I'll be

00:26:19,920 --> 00:26:24,150
sending this has you know the standard

00:26:22,290 --> 00:26:25,710
schema where basically the number of

00:26:24,150 --> 00:26:27,450
bedrooms bathrooms

00:26:25,710 --> 00:26:29,340
waterfront everything is going to these

00:26:27,450 --> 00:26:30,780
are the features and this is the actual

00:26:29,340 --> 00:26:32,250
data that I need to score this is just

00:26:30,780 --> 00:26:42,090
one row of data that I'll be sending

00:26:32,250 --> 00:26:44,670
across okay so the moment of truth 44

00:26:42,090 --> 00:26:46,200
seconds that was two hundred two hundred

00:26:44,670 --> 00:26:48,870
and twelve seconds of course that was

00:26:46,200 --> 00:26:50,580
scoring a little bit more data but I'll

00:26:48,870 --> 00:26:52,530
show you in the later part also that

00:26:50,580 --> 00:26:54,600
with one row of data also it's actually

00:26:52,530 --> 00:26:56,670
sparkles taking quite a long but here in

00:26:54,600 --> 00:27:01,620
forty forty four milliseconds you have

00:26:56,670 --> 00:27:05,360
the entire scoring done okay but this is

00:27:01,620 --> 00:27:08,790
entirely sitting on my laptop right I

00:27:05,360 --> 00:27:12,480
need to move this into a more scalable

00:27:08,790 --> 00:27:15,660
platform remember the five thousand

00:27:12,480 --> 00:27:18,120
ninety five percent but the presentation

00:27:15,660 --> 00:27:20,340
slide that I showed you earlier in order

00:27:18,120 --> 00:27:22,800
to really push the model into production

00:27:20,340 --> 00:27:26,580
I need to have that 95 percent of the

00:27:22,800 --> 00:27:28,920
code also and the more managed services

00:27:26,580 --> 00:27:30,840
are used the better it is for me because

00:27:28,920 --> 00:27:33,870
then I can concentrate on the real

00:27:30,840 --> 00:27:39,240
business rather than solving issues

00:27:33,870 --> 00:27:44,220
associated with their DevOps so some

00:27:39,240 --> 00:27:50,100
time back Amazon came up with something

00:27:44,220 --> 00:27:51,990
called as Amazon sage maker and this

00:27:50,100 --> 00:27:54,240
basically provides you a platform where

00:27:51,990 --> 00:27:56,790
you can create your models and also

00:27:54,240 --> 00:28:01,170
deploy your models so that you can

00:27:56,790 --> 00:28:03,780
create an API in real time so before

00:28:01,170 --> 00:28:05,880
that what I did was the docker container

00:28:03,780 --> 00:28:09,360
that I created I think it's better I

00:28:05,880 --> 00:28:13,170
also show you the docker file that it's

00:28:09,360 --> 00:28:15,540
a pretty simple docker file so here the

00:28:13,170 --> 00:28:17,910
main thing though is the EM leap serving

00:28:15,540 --> 00:28:19,920
this is the bit of code that I had to

00:28:17,910 --> 00:28:21,480
change a little bit to change it

00:28:19,920 --> 00:28:24,740
according to the needs of the amateurs

00:28:21,480 --> 00:28:27,450
on Amazon Sage maker docker requirements

00:28:24,740 --> 00:28:29,279
but other than that there's no change at

00:28:27,450 --> 00:28:32,379
all

00:28:29,279 --> 00:28:37,620
so I created the docker container and

00:28:32,379 --> 00:28:41,460
what I did was I uploaded it to the

00:28:37,620 --> 00:28:43,960
elasticsearch contain the elastic

00:28:41,460 --> 00:28:46,330
container service of Amazon it's a fully

00:28:43,960 --> 00:28:48,429
managed service and this is basically

00:28:46,330 --> 00:28:53,139
the container that I uploaded it to ok

00:28:48,429 --> 00:28:56,649
and using Amazon sage maker what I did

00:28:53,139 --> 00:28:58,659
was I created so this is the model I

00:28:56,649 --> 00:29:05,320
created the model from here I created an

00:28:58,659 --> 00:29:07,779
endpoint configuration and using that I

00:29:05,320 --> 00:29:19,679
also created an endpoints to save time

00:29:07,779 --> 00:29:24,100
okay so one good thing though here is is

00:29:19,679 --> 00:29:27,759
this bit you know the auto scaling you

00:29:24,100 --> 00:29:29,700
can set it on so your API is you you

00:29:27,759 --> 00:29:32,110
don't know when the volatility hits you

00:29:29,700 --> 00:29:34,029
so you can just buy a click of a button

00:29:32,110 --> 00:29:37,509
you can actually put on or off the auto

00:29:34,029 --> 00:29:39,519
scaling you can also turn on you can

00:29:37,509 --> 00:29:42,100
increase so you have used a very small

00:29:39,519 --> 00:29:44,799
instance but you can use a larger

00:29:42,100 --> 00:29:47,230
instance as well and of course you can

00:29:44,799 --> 00:29:51,519
increase the instance count but the good

00:29:47,230 --> 00:29:54,669
part about this is the logs the metrics

00:29:51,519 --> 00:29:57,279
and you could also create an a/b test

00:29:54,669 --> 00:29:59,649
all out of the box without you having to

00:29:57,279 --> 00:30:01,509
change a build any part of that code

00:29:59,649 --> 00:30:03,580
anyone who has built that part of the

00:30:01,509 --> 00:30:09,039
infrastructure knows how difficult it is

00:30:03,580 --> 00:30:12,070
to build and then maintain it ok so now

00:30:09,039 --> 00:30:16,240
that that endpoint has been created does

00:30:12,070 --> 00:30:18,749
it work so this is the JSON file that I

00:30:16,240 --> 00:30:18,749
showed you earlier

00:30:24,580 --> 00:30:27,240
okay

00:30:29,630 --> 00:30:34,700
this is the JSON file we also send it to

00:30:33,350 --> 00:30:36,470
the docker container if you remember in

00:30:34,700 --> 00:30:38,390
the command line this is the same JSON

00:30:36,470 --> 00:30:46,930
file and it's the same one single row

00:30:38,390 --> 00:30:46,930
what I'm doing is I am sending this to

00:30:46,990 --> 00:30:51,010
hopefully if I can get there

00:30:57,960 --> 00:31:05,790
so there you are I send that request and

00:31:01,310 --> 00:31:09,780
I'm not sure if you can see here you

00:31:05,790 --> 00:31:12,990
know very quickly it I could just send a

00:31:09,780 --> 00:31:15,960
request off to AWS get back the results

00:31:12,990 --> 00:31:17,880
in um I can't see the but I think it's

00:31:15,960 --> 00:31:27,560
can someone tell me what the number was

00:31:17,880 --> 00:31:27,560
there 212 okay so

00:31:43,200 --> 00:31:48,750
so that was the request sending part to

00:31:46,380 --> 00:31:51,320
it to Amazon and getting back the

00:31:48,750 --> 00:31:57,960
results it has the same results that we

00:31:51,320 --> 00:32:01,170
we also saw earlier and yeah that's

00:31:57,960 --> 00:32:06,540
that's that I have to change the keys

00:32:01,170 --> 00:32:15,750
though so let's go back to the

00:32:06,540 --> 00:32:22,020
presentation so we build a PI Spock

00:32:15,750 --> 00:32:24,270
model we see realize the model out we

00:32:22,020 --> 00:32:27,690
created docker container out of it and

00:32:24,270 --> 00:32:31,680
then we pushed that model to AWS for

00:32:27,690 --> 00:32:34,320
real-time model scoring coming back to

00:32:31,680 --> 00:32:38,010
the GTO compliance and the model how

00:32:34,320 --> 00:32:41,040
quickly what sort of results can we got

00:32:38,010 --> 00:32:44,190
by using anonymized data so this was

00:32:41,040 --> 00:32:47,070
something that I had to run for one of

00:32:44,190 --> 00:32:48,870
my clients and using gdpr comes and

00:32:47,070 --> 00:32:50,760
features for the same use case that I

00:32:48,870 --> 00:32:54,180
was started off you know with the

00:32:50,760 --> 00:32:58,050
real-time user segmentation I was able

00:32:54,180 --> 00:33:01,440
to achieve 0.88 on the ROC curve and the

00:32:58,050 --> 00:33:03,450
entire model using M leap I could score

00:33:01,440 --> 00:33:07,290
the model in 50 milliseconds on 100

00:33:03,450 --> 00:33:10,680
gates of TM and this is based on their

00:33:07,290 --> 00:33:12,840
exes 15 paper because you know

00:33:10,680 --> 00:33:15,300
clickstream is basically as it follows

00:33:12,840 --> 00:33:18,810
the seasonal patterns you need a bunch

00:33:15,300 --> 00:33:22,620
of features to identify the seasonal

00:33:18,810 --> 00:33:27,150
patterns and this is the features that

00:33:22,620 --> 00:33:30,600
gave the best results and I just wanted

00:33:27,150 --> 00:33:33,540
to check if this is just a one-time hit

00:33:30,600 --> 00:33:38,850
or is it applicable to all use cases so

00:33:33,540 --> 00:33:40,740
I tried the same the same model and the

00:33:38,850 --> 00:33:43,140
same sort of technique and the same sort

00:33:40,740 --> 00:33:45,120
of features on this talking data at

00:33:43,140 --> 00:33:47,810
tracking fraud detection challenge it

00:33:45,120 --> 00:33:51,960
was just recently concluded on cable and

00:33:47,810 --> 00:33:54,930
although I did not win the competition

00:33:51,960 --> 00:33:56,500
but this is I wanted to check how far in

00:33:54,930 --> 00:33:59,140
a single model

00:33:56,500 --> 00:34:01,390
q because this model was the random

00:33:59,140 --> 00:34:03,430
forest model and if I could get good

00:34:01,390 --> 00:34:05,770
results here I can immediately deploy it

00:34:03,430 --> 00:34:09,460
using the model framework that I showed

00:34:05,770 --> 00:34:12,429
you earlier there I was able to get 0.88

00:34:09,460 --> 00:34:14,020
on the ROC curve so the 0.88 that I got

00:34:12,429 --> 00:34:16,389
in production was not a fluke because

00:34:14,020 --> 00:34:18,370
it's giving you the same sort of result

00:34:16,389 --> 00:34:34,270
on publicly available computation data

00:34:18,370 --> 00:34:39,909
as well okay conclusion this is a

00:34:34,270 --> 00:34:43,870
timeline of man's possibly man's effort

00:34:39,909 --> 00:34:46,120
to control electronic chaos on the top

00:34:43,870 --> 00:34:47,500
part you have this line that is that

00:34:46,120 --> 00:34:53,909
shows the history of computer science

00:34:47,500 --> 00:34:56,710
and on this side is your data science

00:34:53,909 --> 00:35:01,150
1837 Babbage created the analytical

00:34:56,710 --> 00:35:02,860
machine 1972 you know version control

00:35:01,150 --> 00:35:06,430
came out but I'm pretty sure that they

00:35:02,860 --> 00:35:08,860
lost the code Midway before that 1990

00:35:06,430 --> 00:35:10,180
there was a World Wide Web 2004 was

00:35:08,860 --> 00:35:12,220
pretty interesting

00:35:10,180 --> 00:35:15,060
you had AWS you have the Solaris zones

00:35:12,220 --> 00:35:19,540
and also the MapReduce paper came out

00:35:15,060 --> 00:35:23,650
2005 big data term was coined and then

00:35:19,540 --> 00:35:31,720
2013 docker was possibly started to get

00:35:23,650 --> 00:35:33,880
more and more attention and 1959 ml was

00:35:31,720 --> 00:35:36,640
coined the term ml was quite machine

00:35:33,880 --> 00:35:38,350
learning I'm using the electronic

00:35:36,640 --> 00:35:41,800
trading part here because possibly

00:35:38,350 --> 00:35:43,960
electronic trading is where the first

00:35:41,800 --> 00:35:45,550
instance where data science was used to

00:35:43,960 --> 00:35:47,590
make money so this was the first

00:35:45,550 --> 00:35:50,110
instance where electronic trading

00:35:47,590 --> 00:35:52,990
started 1997 was when the black-scholes

00:35:50,110 --> 00:35:56,920
model got the economics Nobel Prize and

00:35:52,990 --> 00:36:01,330
people realize that yes now we can use

00:35:56,920 --> 00:36:04,120
mathematics to make money but these two

00:36:01,330 --> 00:36:06,520
has two separate lines that were going

00:36:04,120 --> 00:36:08,980
across you know and now is the time

00:36:06,520 --> 00:36:10,450
where machine learning has matured

00:36:08,980 --> 00:36:13,150
itself to go into product

00:36:10,450 --> 00:36:16,420
now we could build this we could

00:36:13,150 --> 00:36:20,980
reinvent this entire pipeline ourselves

00:36:16,420 --> 00:36:22,510
or use the effort that was done using

00:36:20,980 --> 00:36:23,980
computer science that computer science

00:36:22,510 --> 00:36:27,849
has already invented and fine

00:36:23,980 --> 00:36:30,280
found out and fine-tuned by using a

00:36:27,849 --> 00:36:32,410
darker base infrastructure or by

00:36:30,280 --> 00:36:36,220
changing a model into a docker container

00:36:32,410 --> 00:36:38,559
we immediately can use the same currency

00:36:36,220 --> 00:36:42,369
which the computer science pipeline can

00:36:38,559 --> 00:36:44,859
use by using docker based machine

00:36:42,369 --> 00:36:47,650
learning model the entire shebang that

00:36:44,859 --> 00:36:49,329
is associated with your machine learning

00:36:47,650 --> 00:36:52,420
pipeline you know the 95 percent that I

00:36:49,329 --> 00:36:54,400
was showing you earlier that without any

00:36:52,420 --> 00:36:58,599
effort a very minimal effort you can

00:36:54,400 --> 00:37:01,329
achieve it anecdote I would like to end

00:36:58,599 --> 00:37:06,460
this with a small anecdote this is from

00:37:01,329 --> 00:37:08,559
the book art and fear and the the

00:37:06,460 --> 00:37:10,990
ceramics teacher came into the class and

00:37:08,559 --> 00:37:14,470
divided the class into two bits into two

00:37:10,990 --> 00:37:17,140
halves one half say was going to be

00:37:14,470 --> 00:37:19,270
evaluated on the number of actual

00:37:17,140 --> 00:37:21,130
articles or ceramic pieces that was made

00:37:19,270 --> 00:37:24,309
you know so slowly based on the quantity

00:37:21,130 --> 00:37:27,900
the other part of the class was going to

00:37:24,309 --> 00:37:31,089
be evaluated on the quality of the task

00:37:27,900 --> 00:37:34,450
at the end of the day what was found out

00:37:31,089 --> 00:37:36,700
was the highest quality was produced by

00:37:34,450 --> 00:37:39,609
the group that was being graded for

00:37:36,700 --> 00:37:42,250
quantity so this was like pretty

00:37:39,609 --> 00:37:46,240
interesting like why why how is that

00:37:42,250 --> 00:37:48,400
possible it was found out that while the

00:37:46,240 --> 00:37:51,670
the group that was geht was going to be

00:37:48,400 --> 00:37:53,980
evaluated on quality was where I da ting

00:37:51,670 --> 00:37:55,900
on the best piece that had to come out

00:37:53,980 --> 00:37:57,700
the guys who were working on the

00:37:55,900 --> 00:38:00,220
quantity peace with continuously

00:37:57,700 --> 00:38:02,079
building the ceramics pieces learning

00:38:00,220 --> 00:38:06,549
from the mistakes and then trying to

00:38:02,079 --> 00:38:08,500
build a better piece from that now if we

00:38:06,549 --> 00:38:11,650
bring that same similes to the machine

00:38:08,500 --> 00:38:13,990
learning world you know data science is

00:38:11,650 --> 00:38:17,440
an iterative process data the shape of

00:38:13,990 --> 00:38:19,960
your data changes and new models come in

00:38:17,440 --> 00:38:24,160
all that is needed is an infrastructure

00:38:19,960 --> 00:38:26,950
that BR allows you to create

00:38:24,160 --> 00:38:28,480
end-to-end models quickly so you would

00:38:26,950 --> 00:38:30,369
build a model quickly push into the

00:38:28,480 --> 00:38:32,230
production check whether it's working

00:38:30,369 --> 00:38:33,970
fine or not and then it would have an

00:38:32,230 --> 00:38:36,119
expiry date by which you push out the

00:38:33,970 --> 00:38:38,619
models and pull in on your model so

00:38:36,119 --> 00:38:42,160
hopefully in this talk I could give you

00:38:38,619 --> 00:38:55,960
a rough sketch of how to do it in

00:38:42,160 --> 00:38:58,710
production so that's that questions okay

00:38:55,960 --> 00:38:58,710
anyone have any questions

00:39:02,609 --> 00:39:07,390
you talked about cost how much does it

00:39:05,049 --> 00:39:10,869
cost to deploy your new model creating

00:39:07,390 --> 00:39:14,920
the model 100 gigs of data it ran for 8

00:39:10,869 --> 00:39:17,500
hours I used about 4 notes on EMR and it

00:39:14,920 --> 00:39:19,809
cost me about 11 euros that's 100 gigs

00:39:17,500 --> 00:39:21,160
of data but this is the air map if you

00:39:19,809 --> 00:39:27,789
run it on GCP it were to be much more

00:39:21,160 --> 00:39:30,720
cheaper we still have time for one more

00:39:27,789 --> 00:39:30,720
question anyone

00:39:38,170 --> 00:39:43,550
so did he use any historical features in

00:39:41,750 --> 00:39:45,710
the classifier so something like the

00:39:43,550 --> 00:39:48,050
user has injected with those pages the

00:39:45,710 --> 00:39:50,600
last five pages and how would you model

00:39:48,050 --> 00:39:52,850
such thing in a leap library

00:39:50,600 --> 00:39:54,890
so having historic not only the events

00:39:52,850 --> 00:39:56,720
current data like user agent browser and

00:39:54,890 --> 00:39:58,310
some stuff like this but also historical

00:39:56,720 --> 00:40:01,280
orders or something

00:39:58,310 --> 00:40:04,760
is it possible to package this in the ml

00:40:01,280 --> 00:40:07,250
leap transformer so right now it has

00:40:04,760 --> 00:40:08,690
linear if you're talking about online

00:40:07,250 --> 00:40:10,810
machine learning models are you talking

00:40:08,690 --> 00:40:14,800
about online no just about historic data

00:40:10,810 --> 00:40:17,870
data coming into into the classifier so

00:40:14,800 --> 00:40:20,570
where did you use it so like in the

00:40:17,870 --> 00:40:22,160
historical database like I know dynamo

00:40:20,570 --> 00:40:25,070
was something where you look up

00:40:22,160 --> 00:40:27,530
historical interactions of the profile

00:40:25,070 --> 00:40:29,480
yeah so this entire the hundred gigs of

00:40:27,530 --> 00:40:31,610
data that I was talking about that was

00:40:29,480 --> 00:40:33,560
all historical data the model was built

00:40:31,610 --> 00:40:35,870
on using that hundred gigs of historical

00:40:33,560 --> 00:40:37,910
data that had come till that point I

00:40:35,870 --> 00:40:43,040
build a model on top of it and then

00:40:37,910 --> 00:40:44,300
expose it okay so um what I meant is the

00:40:43,040 --> 00:40:47,270
data you're using at the point of

00:40:44,300 --> 00:40:48,080
friction time if you also have access to

00:40:47,270 --> 00:40:50,030
historically

00:40:48,080 --> 00:40:52,490
within the same session data of the

00:40:50,030 --> 00:40:55,340
profile yeah I just took the current

00:40:52,490 --> 00:40:57,140
event data frame yes so only the current

00:40:55,340 --> 00:41:00,740
event only the current even that is

00:40:57,140 --> 00:41:02,150
coming in using only that bit because

00:41:00,740 --> 00:41:03,830
that was generating the field that was

00:41:02,150 --> 00:41:06,170
good enough for the features and those

00:41:03,830 --> 00:41:07,970
features were sent in to the model and

00:41:06,170 --> 00:41:09,710
that was called in real time so here the

00:41:07,970 --> 00:41:11,870
model is not an online machine learning

00:41:09,710 --> 00:41:14,720
model I created the model offline and

00:41:11,870 --> 00:41:16,670
then expose the model as an API and the

00:41:14,720 --> 00:41:18,410
real-time data or the real-time even

00:41:16,670 --> 00:41:20,540
server coming in that was getting scored

00:41:18,410 --> 00:41:26,570
in real time and being exposed out okay

00:41:20,540 --> 00:41:29,180
thank you I just have a question about

00:41:26,570 --> 00:41:32,920
the response time I you mentioned it's

00:41:29,180 --> 00:41:37,360
about 50 milliseconds and can you

00:41:32,920 --> 00:41:42,740
produce this time using larger instances

00:41:37,360 --> 00:41:47,090
in the AWS or is it some fixed

00:41:42,740 --> 00:41:49,760
number yeah so they did a benchmark on

00:41:47,090 --> 00:41:51,320
Emily and there are a lot of

00:41:49,760 --> 00:41:54,200
presentations around on the web you can

00:41:51,320 --> 00:41:58,070
look at it they were able to get in less

00:41:54,200 --> 00:41:59,630
than eleven milliseconds as well it also

00:41:58,070 --> 00:42:01,040
depends upon the sort of model that you

00:41:59,630 --> 00:42:02,960
are using random for us you know

00:42:01,040 --> 00:42:04,460
depending upon what sort of model if you

00:42:02,960 --> 00:42:06,580
have a larger model with more number of

00:42:04,460 --> 00:42:10,430
branches it takes a little bit more time

00:42:06,580 --> 00:42:14,510
but again if speed is your criteria you

00:42:10,430 --> 00:42:17,660
might want to choose a easier model too

00:42:14,510 --> 00:42:19,640
so it's all based on fine-tuning your

00:42:17,660 --> 00:42:22,780
needs based on the use case and what

00:42:19,640 --> 00:42:25,310
sort of accuracy you want okay thanks

00:42:22,780 --> 00:42:26,990
thank you and I think we ran out of time

00:42:25,310 --> 00:42:28,110
thank you so is it for the talking the

00:42:26,990 --> 00:42:33,849
demonstration

00:42:28,110 --> 00:42:33,849

YouTube URL: https://www.youtube.com/watch?v=6xVvfTU_aag


