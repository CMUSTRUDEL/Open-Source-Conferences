Title: Berlin Buzzwords 2018: Marton Elek – Docker to kubernetes: running Hadoop in a cloud native way
Publication date: 2018-06-14
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Creating containers for an application is easy (even if it’s a good old distributed application like Apache Hadoop), just a few steps of packaging. The hard part isn't packaging: it's deploying

How can we run the containers together? How to configure them? How do the services in the containers find and talk to each other? How do you deploy and manage clusters with hundred of nodes?

Modern cloud native tools like Kubernetes or Consul/Nomad could help a lot but they could be used in different way. It this presentation I will demonstrate multiple solutions to manage containerized clusters with different cloud-native tools including kubernetes, and docker-swarm/compose. No matter which tools you use, the same questions of service discovery and configuration management arise. This talk will show the key elements needed to make that containerized cluster work.

Read more:
https://2018.berlinbuzzwords.de/18/session/docker-kubernetes-running-apache-hadoop-cloud-native-way

About Marton Elek:
https://2018.berlinbuzzwords.de/users/marton-elek
Captions: 
	00:00:05,109 --> 00:00:11,120
hi I would like to speak about

00:00:08,290 --> 00:00:12,760
kubernetes or and hello but more

00:00:11,120 --> 00:00:15,080
precisely I would like to speak about

00:00:12,760 --> 00:00:18,980
distributed systems and continuous

00:00:15,080 --> 00:00:21,260
environment I'm an Apache cometary

00:00:18,980 --> 00:00:23,990
narrative project which is incubated

00:00:21,260 --> 00:00:27,020
project and this is a ruffed library for

00:00:23,990 --> 00:00:29,390
Java which could be embedded and in fact

00:00:27,020 --> 00:00:32,539
it's embedded in had a new sub project

00:00:29,390 --> 00:00:36,170
in Hadoop this Apache old Apache Hadoop

00:00:32,539 --> 00:00:40,489
O's on an Apache Hadoop HDDs I also have

00:00:36,170 --> 00:00:43,040
a experimental project when I'm trying

00:00:40,489 --> 00:00:45,620
to contain eyes the whole world or at

00:00:43,040 --> 00:00:48,320
least the big data part and I'm running

00:00:45,620 --> 00:00:50,930
the Apache Big Data projects in

00:00:48,320 --> 00:00:57,370
containers in Dockers for kubernetes and

00:00:50,930 --> 00:01:00,020
in docker compose in other ways ok so

00:00:57,370 --> 00:01:03,050
I'm working for the Hortonworks and a

00:01:00,020 --> 00:01:05,659
significant time of my daily job is just

00:01:03,050 --> 00:01:08,360
starting and stopping head up containers

00:01:05,659 --> 00:01:11,179
and I really like it I think it's a it's

00:01:08,360 --> 00:01:14,119
a good job it just depends what kind of

00:01:11,179 --> 00:01:16,179
tools are you used and I'm pretty sure

00:01:14,119 --> 00:01:20,509
that using containers and

00:01:16,179 --> 00:01:23,329
containerization it's a managing Hadoop

00:01:20,509 --> 00:01:26,079
it's more like something like this so it

00:01:23,329 --> 00:01:30,020
could be more robust and more manageable

00:01:26,079 --> 00:01:31,700
but this talk is about head but to be

00:01:30,020 --> 00:01:34,549
honest it could be any other kind of

00:01:31,700 --> 00:01:39,109
animal because it's not strictly

00:01:34,549 --> 00:01:40,849
connected to head you know just I don't

00:01:39,109 --> 00:01:42,469
know if you know the feeling that you

00:01:40,849 --> 00:01:44,029
read some getting started guide and

00:01:42,469 --> 00:01:48,079
everything is fine but with a real world

00:01:44,029 --> 00:01:49,819
application it's not not that the case

00:01:48,079 --> 00:01:51,319
though I use head too because it's a

00:01:49,819 --> 00:01:53,569
very good at this example it's a good

00:01:51,319 --> 00:01:55,549
all the major application so if

00:01:53,569 --> 00:01:58,069
something could work with head and I

00:01:55,549 --> 00:01:59,479
think we know the receipt to start any

00:01:58,069 --> 00:02:02,419
kind of distributed application in

00:01:59,479 --> 00:02:05,299
containers ok next question do you know

00:02:02,419 --> 00:02:07,789
what's this I'm pretty sure you know

00:02:05,299 --> 00:02:09,410
because yeah this is from the European

00:02:07,789 --> 00:02:11,660
Union this is a regulation that every

00:02:09,410 --> 00:02:13,610
household appliance should be tagged

00:02:11,660 --> 00:02:15,860
with some kind of label just to make it

00:02:13,610 --> 00:02:18,170
easier to compare not just based on the

00:02:15,860 --> 00:02:20,330
price but for example based on the

00:02:18,170 --> 00:02:21,680
energy consumption or the noise level or

00:02:20,330 --> 00:02:24,050
something like this yeah it's it's

00:02:21,680 --> 00:02:26,930
pretty smart and what I'm wondering that

00:02:24,050 --> 00:02:29,420
how can I do similar labors to compare

00:02:26,930 --> 00:02:30,950
the containerized environments for

00:02:29,420 --> 00:02:33,740
example cookware that is how can I

00:02:30,950 --> 00:02:35,270
understand the key questions and this is

00:02:33,740 --> 00:02:38,330
what I would like to do in this talk

00:02:35,270 --> 00:02:41,300
just label different contour eyes the

00:02:38,330 --> 00:02:43,640
environment and learn what are the key

00:02:41,300 --> 00:02:45,770
questions which should be sold anyway

00:02:43,640 --> 00:02:49,760
with some kind of tools for example

00:02:45,770 --> 00:02:52,190
kubernetes yeah so for that one we need

00:02:49,760 --> 00:02:56,150
some real world applications or

00:02:52,190 --> 00:02:58,790
environments so I will show multiple one

00:02:56,150 --> 00:03:01,250
but unfortunately we don't have enough

00:02:58,790 --> 00:03:04,340
time to check all of the technical

00:03:01,250 --> 00:03:06,440
details so we'll use a simplified

00:03:04,340 --> 00:03:09,670
methodology just check one good thing

00:03:06,440 --> 00:03:11,570
and one bad thing for for every example

00:03:09,670 --> 00:03:13,040
actually usually I'm more interested

00:03:11,570 --> 00:03:15,800
about the best thing right that's the

00:03:13,040 --> 00:03:20,180
limitation that's wholly how it will be

00:03:15,800 --> 00:03:22,400
failed so that's what we will check it

00:03:20,180 --> 00:03:26,329
ok the next question do you know what is

00:03:22,400 --> 00:03:29,600
Apache Hadoop and so on if you know ok

00:03:26,329 --> 00:03:32,989
most of them okay but to be on the same

00:03:29,600 --> 00:03:35,570
side I have a very short no I have a

00:03:32,989 --> 00:03:38,060
complete head of training actually in 60

00:03:35,570 --> 00:03:40,340
seconds this is the way how I explain

00:03:38,060 --> 00:03:42,890
what is Hadoop for what I'm doing for my

00:03:40,340 --> 00:03:44,480
grandparents so it's just very high

00:03:42,890 --> 00:03:45,769
levels so sorry if you don't know the

00:03:44,480 --> 00:03:48,260
head but it's enough for this

00:03:45,769 --> 00:03:51,260
presentation yeah so head hoop is a very

00:03:48,260 --> 00:03:53,540
popular big data to set or big data

00:03:51,260 --> 00:03:55,670
application ok next question what is Big

00:03:53,540 --> 00:03:57,890
Data well this is the same as the small

00:03:55,670 --> 00:04:00,860
later just just in big right so what is

00:03:57,890 --> 00:04:02,180
small data yeah that's that's the

00:04:00,860 --> 00:04:03,820
easiest question I'm pretty sure that

00:04:02,180 --> 00:04:07,760
the small data is an excel sheet right

00:04:03,820 --> 00:04:09,320
so Big Data is an excel sheet which does

00:04:07,760 --> 00:04:11,420
not fit on my computer so it need

00:04:09,320 --> 00:04:13,700
multiple computer and Hadoop could

00:04:11,420 --> 00:04:16,450
handle this yeah the first problem is

00:04:13,700 --> 00:04:21,410
that we need to split it somehow and

00:04:16,450 --> 00:04:24,800
just store one part in one cool node

00:04:21,410 --> 00:04:25,910
that's what HDFS could do and the next

00:04:24,800 --> 00:04:28,039
problem is that we need the same

00:04:25,910 --> 00:04:31,879
calculation for for example we need the

00:04:28,039 --> 00:04:33,649
maximum value from the excel sheet so

00:04:31,879 --> 00:04:35,479
it's pretty easy right we need to just

00:04:33,649 --> 00:04:36,679
calculate the maximum at every node and

00:04:35,479 --> 00:04:38,479
after that we need the maximum or the

00:04:36,679 --> 00:04:40,069
maximum so this kind of calculation is

00:04:38,479 --> 00:04:43,129
handled by the yarn and other sub

00:04:40,069 --> 00:04:44,899
project in Hadoop and we can write in

00:04:43,129 --> 00:04:46,909
the calculation with the MapReduce

00:04:44,899 --> 00:04:49,550
framework but it's not necessary you can

00:04:46,909 --> 00:04:53,929
use any ad or other framework or product

00:04:49,550 --> 00:04:58,009
like sparkling and in fact we have a new

00:04:53,929 --> 00:05:01,789
or two new Hadoop sub-project ozone and

00:04:58,009 --> 00:05:04,909
HDDs but there will be a presentation

00:05:01,789 --> 00:05:07,879
about them tomorrow okay and you can

00:05:04,909 --> 00:05:09,740
imagine that Hadoop as a lot of master

00:05:07,879 --> 00:05:11,269
components worker components we don't

00:05:09,740 --> 00:05:13,069
need to know all of the details about

00:05:11,269 --> 00:05:14,449
but we have master components worker

00:05:13,069 --> 00:05:15,919
conference and we would like to contrast

00:05:14,449 --> 00:05:18,499
an okay let's start with the

00:05:15,919 --> 00:05:19,939
containerization okay that's the docker

00:05:18,499 --> 00:05:22,869
file right that the containerization

00:05:19,939 --> 00:05:26,599
have you ever wrote something like this

00:05:22,869 --> 00:05:28,789
okay but I think all of you can imagine

00:05:26,599 --> 00:05:31,669
that we I had a base image I'm adding in

00:05:28,789 --> 00:05:33,889
a Hadoop and yeah that's all originally

00:05:31,669 --> 00:05:36,349
the title of my presentation was how was

00:05:33,889 --> 00:05:38,629
how to quantize Hadoop and I just

00:05:36,349 --> 00:05:42,919
realized that okay it's very easy that's

00:05:38,629 --> 00:05:44,719
just one slide right dad yeah the

00:05:42,919 --> 00:05:46,579
question is that how the containers

00:05:44,719 --> 00:05:48,559
could work together right we have some

00:05:46,579 --> 00:05:50,329
question questions which should be

00:05:48,559 --> 00:05:51,559
answer the configuration management the

00:05:50,329 --> 00:05:53,689
provisioning schedule looking at all of

00:05:51,559 --> 00:05:55,999
them so that's not the the tricky part

00:05:53,689 --> 00:05:58,369
is not the containerization itself but

00:05:55,999 --> 00:06:01,009
running the containers together ok so

00:05:58,369 --> 00:06:02,869
let's start with configuration just just

00:06:01,009 --> 00:06:05,599
as an example so this is the good old

00:06:02,869 --> 00:06:07,490
head of style configuration I have no

00:06:05,599 --> 00:06:09,740
problem with that that's eczema but yeah

00:06:07,490 --> 00:06:13,519
I can do the same type of with XML and

00:06:09,740 --> 00:06:16,159
llamó files so it's no problem another

00:06:13,519 --> 00:06:18,309
problem is that in docker environment

00:06:16,159 --> 00:06:20,990
typically I have a little bit different

00:06:18,309 --> 00:06:23,089
configuration so the most dope core

00:06:20,990 --> 00:06:25,339
native configuration is the environment

00:06:23,089 --> 00:06:27,229
variables right so this is a docker

00:06:25,339 --> 00:06:29,319
compose definition and you can see that

00:06:27,229 --> 00:06:31,729
I have a base image hosting port and

00:06:29,319 --> 00:06:35,539
environment variables in fact I can

00:06:31,729 --> 00:06:39,079
mount the XML files but it's more harder

00:06:35,539 --> 00:06:42,169
to manage so what I would like to use is

00:06:39,079 --> 00:06:44,959
just a set of environment variables so

00:06:42,169 --> 00:06:46,280
what can I do yeah I can create a

00:06:44,959 --> 00:06:48,200
launcher script so it's

00:06:46,280 --> 00:06:50,810
not a big deal right it's very easy to

00:06:48,200 --> 00:06:52,940
map something and the launcher script

00:06:50,810 --> 00:06:55,010
could create the configuration files and

00:06:52,940 --> 00:06:56,660
after that the good ole distributed

00:06:55,010 --> 00:06:57,590
application could be started for example

00:06:56,660 --> 00:07:00,860
they had ooh

00:06:57,590 --> 00:07:03,169
so it's just a few lines of code and

00:07:00,860 --> 00:07:05,510
after that I could have this kind of

00:07:03,169 --> 00:07:08,480
docker compose file you can see that now

00:07:05,510 --> 00:07:10,850
we I have some kind of naming convention

00:07:08,480 --> 00:07:13,210
and based on the naming convention the

00:07:10,850 --> 00:07:17,030
stores are skipped will generate the

00:07:13,210 --> 00:07:19,550
final configuration and based on the

00:07:17,030 --> 00:07:22,160
extension I can choose the format so if

00:07:19,550 --> 00:07:27,640
it's an XML it should be converted to a

00:07:22,160 --> 00:07:27,640
Hadoop XML format okay done

00:07:28,030 --> 00:07:32,600
let's not learn here is that we would

00:07:30,470 --> 00:07:34,070
like to manage the configuration value

00:07:32,600 --> 00:07:35,720
it's not the format format it's it's

00:07:34,070 --> 00:07:38,360
very easy right we can just convert it

00:07:35,720 --> 00:07:40,010
from one format or other format but with

00:07:38,360 --> 00:07:42,290
docker it's easier to manage in

00:07:40,010 --> 00:07:45,710
environment variables so we can anij it

00:07:42,290 --> 00:07:48,200
in environment variables okay so the

00:07:45,710 --> 00:07:51,020
first part is done we we know that if

00:07:48,200 --> 00:07:53,270
it's noisy or not this freezer okay so

00:07:51,020 --> 00:07:55,940
we have this the source of the

00:07:53,270 --> 00:07:58,520
configuration has been sold currently we

00:07:55,940 --> 00:08:01,160
have no pre-processing and there is no

00:07:58,520 --> 00:08:02,300
support for change so if I am I would

00:08:01,160 --> 00:08:04,190
like to change something in the

00:08:02,300 --> 00:08:06,470
environment variables I need to restart

00:08:04,190 --> 00:08:08,990
everything manually okay but there is

00:08:06,470 --> 00:08:12,340
there is an other big question there are

00:08:08,990 --> 00:08:15,289
two main approaches to to containerize

00:08:12,340 --> 00:08:16,760
big data application one is just put

00:08:15,289 --> 00:08:19,220
everything in one container it is the

00:08:16,760 --> 00:08:21,800
easiest all right just one container per

00:08:19,220 --> 00:08:25,039
node and the other one which usually

00:08:21,800 --> 00:08:27,650
suggested by the docker literature to

00:08:25,039 --> 00:08:30,650
put every application in a separated

00:08:27,650 --> 00:08:32,270
small container this is usually this is

00:08:30,650 --> 00:08:34,940
the such a situate but the left side is

00:08:32,270 --> 00:08:37,159
more easy because we don't need to

00:08:34,940 --> 00:08:39,800
modify anything any kind of existing

00:08:37,159 --> 00:08:43,580
application could be started just as it

00:08:39,800 --> 00:08:46,160
would be another big container so why

00:08:43,580 --> 00:08:48,050
what is the best and and why that's the

00:08:46,160 --> 00:08:50,080
question yeah we we need to understand

00:08:48,050 --> 00:08:54,170
the whys so if we need a freezer or not

00:08:50,080 --> 00:08:56,180
yeah I prefer the right side and not

00:08:54,170 --> 00:08:58,550
this is because it is suggested by the

00:08:56,180 --> 00:09:00,120
literature but I think it's more it

00:08:58,550 --> 00:09:03,150
seems to be harder

00:09:00,120 --> 00:09:05,640
but after right it's just easier imagine

00:09:03,150 --> 00:09:08,040
if you would like to restart for example

00:09:05,640 --> 00:09:10,290
or or update the hive with one big

00:09:08,040 --> 00:09:12,540
monolid container you need to upgrade

00:09:10,290 --> 00:09:15,060
the container and restart everywhere

00:09:12,540 --> 00:09:17,160
so it's more easier if the container is

00:09:15,060 --> 00:09:19,290
the unit of the packaging and you can

00:09:17,160 --> 00:09:21,839
handle all of the application in a very

00:09:19,290 --> 00:09:24,510
same way if you for example if you add

00:09:21,839 --> 00:09:26,880
something to the launcher script then it

00:09:24,510 --> 00:09:30,350
will be available easily for every

00:09:26,880 --> 00:09:32,730
application so the power of the

00:09:30,350 --> 00:09:35,070
containerization at least in a local

00:09:32,730 --> 00:09:37,080
environment or in in dr. Campos

00:09:35,070 --> 00:09:38,930
environment is inside the launcher

00:09:37,080 --> 00:09:42,420
script so with launcher script we can do

00:09:38,930 --> 00:09:45,510
very powerful things just we need to add

00:09:42,420 --> 00:09:48,510
this simple script and if environment in

00:09:45,510 --> 00:09:51,029
any environment variable said that we

00:09:48,510 --> 00:09:53,339
can do some kind of magic okay for

00:09:51,029 --> 00:09:55,940
example this configuration

00:09:53,339 --> 00:09:58,260
transformation but we can do any other

00:09:55,940 --> 00:10:01,830
magic for example waiting for another

00:09:58,260 --> 00:10:04,470
service or we can download an additional

00:10:01,830 --> 00:10:07,200
component drawer file we can prepare the

00:10:04,470 --> 00:10:10,470
ad HDFS Hadoop requires some kind of

00:10:07,200 --> 00:10:12,270
formatting before the first tarped on

00:10:10,470 --> 00:10:15,029
the HDFS side but it's very easy to

00:10:12,270 --> 00:10:17,880
adjust if we need format and if at if

00:10:15,029 --> 00:10:20,820
the director is missing just do a name

00:10:17,880 --> 00:10:23,730
not forma yeah we can also setup the

00:10:20,820 --> 00:10:25,920
credentials enabled promted smart ring

00:10:23,730 --> 00:10:27,480
withdraw or agent so it's more and we

00:10:25,920 --> 00:10:29,820
can do more and more complex in just

00:10:27,480 --> 00:10:31,500
with the launcher my personal favorite

00:10:29,820 --> 00:10:34,380
is the last one so I don't know if you

00:10:31,500 --> 00:10:36,180
know what are what is in in the wire

00:10:34,380 --> 00:10:38,040
between the Hadoop components so you

00:10:36,180 --> 00:10:40,170
know there is proto buffer and the

00:10:38,040 --> 00:10:43,650
Hadoop components are just speaking with

00:10:40,170 --> 00:10:46,980
each other and in our launcher script we

00:10:43,650 --> 00:10:49,920
have just a simple Java agent it could

00:10:46,980 --> 00:10:51,959
be turned on because we handle all of

00:10:49,920 --> 00:10:56,100
the all the applications in the same way

00:10:51,959 --> 00:11:00,060
and yeah this is a standard output where

00:10:56,100 --> 00:11:04,410
the log-log standard log just replaced

00:11:00,060 --> 00:11:06,180
with the messages which are replaced

00:11:04,410 --> 00:11:08,130
between the name node and data node the

00:11:06,180 --> 00:11:10,770
master and worker so I can see all of

00:11:08,130 --> 00:11:13,410
the all of the network traffic with just

00:11:10,770 --> 00:11:14,130
one environment variable turning on yeah

00:11:13,410 --> 00:11:16,440
I will

00:11:14,130 --> 00:11:17,850
I really like it so uh if this is not

00:11:16,440 --> 00:11:19,320
the good thing then I don't know what

00:11:17,850 --> 00:11:21,000
but could be the good thing in the

00:11:19,320 --> 00:11:23,850
Indies dr. Campos based approach but

00:11:21,000 --> 00:11:25,980
what is the best thing yeah it's it's

00:11:23,850 --> 00:11:27,800
very powerful then we use it a lot of

00:11:25,980 --> 00:11:29,850
times during the development or just

00:11:27,800 --> 00:11:31,650
provide the documentation for the new

00:11:29,850 --> 00:11:33,510
features but the problem is that it

00:11:31,650 --> 00:11:34,680
couldn't work with multiple nodes right

00:11:33,510 --> 00:11:36,390
that's the problem that's the biggest

00:11:34,680 --> 00:11:39,720
problem we need a real production

00:11:36,390 --> 00:11:42,230
cluster so what can we do yeah we can

00:11:39,720 --> 00:11:44,310
start with kubernetes everybody know and

00:11:42,230 --> 00:11:46,050
everybody knows that kubernetes is the

00:11:44,310 --> 00:11:48,630
new unicore so we can just try it out

00:11:46,050 --> 00:11:50,640
but currently we would like to

00:11:48,630 --> 00:11:54,540
understand the question and compare them

00:11:50,640 --> 00:11:57,570
based on the main categories so it's

00:11:54,540 --> 00:12:00,810
very easy to win a competition if there

00:11:57,570 --> 00:12:03,690
is there is nobody else in your category

00:12:00,810 --> 00:12:05,820
so I just created somebody else to to

00:12:03,690 --> 00:12:12,300
compare them with kubernetes and this is

00:12:05,820 --> 00:12:13,980
a Heshy curve stack based approach well

00:12:12,300 --> 00:12:18,240
it's more like a do-it-yourself approach

00:12:13,980 --> 00:12:21,510
because Hoshi Corp is a company who who

00:12:18,240 --> 00:12:23,730
provides multiple smaller application

00:12:21,510 --> 00:12:26,400
which are very powerful but you need to

00:12:23,730 --> 00:12:29,760
assembly or you need to create your

00:12:26,400 --> 00:12:34,680
solution manually and and just adjust

00:12:29,760 --> 00:12:39,570
all the components how many of you use

00:12:34,680 --> 00:12:42,780
console or I don't know mold anybody

00:12:39,570 --> 00:12:45,450
here ok so one of the one of the famous

00:12:42,780 --> 00:12:47,280
product is console it's a service

00:12:45,450 --> 00:12:50,640
discovery server but if you have never

00:12:47,280 --> 00:12:53,520
seen it you can imagine it ISA as a key

00:12:50,640 --> 00:12:55,410
value store actually it's a little bit

00:12:53,520 --> 00:12:58,460
more than for example if the key is a

00:12:55,410 --> 00:13:02,010
service name and value is an IP address

00:12:58,460 --> 00:13:05,220
then you can retrieve the information

00:13:02,010 --> 00:13:10,580
even over a DNS interface so it could

00:13:05,220 --> 00:13:12,600
work as a DNS server what is an other

00:13:10,580 --> 00:13:15,330
application again you can imagine it is

00:13:12,600 --> 00:13:17,370
a key value store where the value is a

00:13:15,330 --> 00:13:20,220
secret so it's some kind of secret

00:13:17,370 --> 00:13:22,140
management but it has also some advanced

00:13:20,220 --> 00:13:24,570
capabilities authentication

00:13:22,140 --> 00:13:27,870
authorization and dynamic secret

00:13:24,570 --> 00:13:31,920
generation and Nomad is the scheduler

00:13:27,870 --> 00:13:34,230
it could run application on a note so

00:13:31,920 --> 00:13:36,150
that's that's our situation currently

00:13:34,230 --> 00:13:38,880
right so it's more than one know that

00:13:36,150 --> 00:13:40,920
that was our problem so and we have a

00:13:38,880 --> 00:13:43,460
nomad agent did say go long the

00:13:40,920 --> 00:13:45,779
applications it's very easy to start and

00:13:43,460 --> 00:13:47,880
my problem is that I would like to start

00:13:45,779 --> 00:13:49,500
a container somewhere I'm not interested

00:13:47,880 --> 00:13:51,960
where it will be started it will be

00:13:49,500 --> 00:13:54,839
started somewhere by the scheduler by no

00:13:51,960 --> 00:13:57,150
man ok and maybe I need four for

00:13:54,839 --> 00:14:00,870
container and it will be sorted when

00:13:57,150 --> 00:14:04,020
there are available capacities ok so

00:14:00,870 --> 00:14:07,410
this is the Hadoop cluster I would like

00:14:04,020 --> 00:14:10,560
to start five data node and one name

00:14:07,410 --> 00:14:12,630
node the master component and I asked

00:14:10,560 --> 00:14:14,760
know much to start it somewhere I don't

00:14:12,630 --> 00:14:17,010
know where it will be started so it

00:14:14,760 --> 00:14:19,890
depends from the available capacity so I

00:14:17,010 --> 00:14:21,839
don't know how I can set that I need an

00:14:19,890 --> 00:14:24,180
a data node at every node but the name

00:14:21,839 --> 00:14:25,100
not really started somewhere okay what's

00:14:24,180 --> 00:14:28,020
the problem with that

00:14:25,100 --> 00:14:30,360
yeah the first problem is the networking

00:14:28,020 --> 00:14:32,430
by default the docker provides an

00:14:30,360 --> 00:14:35,550
internal networking and no specific

00:14:32,430 --> 00:14:38,430
networking so between no nodes there is

00:14:35,550 --> 00:14:41,190
no connection so one worker not here

00:14:38,430 --> 00:14:43,560
can't see the other worker node unless I

00:14:41,190 --> 00:14:49,410
do some very magic port mapping which is

00:14:43,560 --> 00:14:51,810
not very manageable so what can I do one

00:14:49,410 --> 00:14:53,730
simple solution is just use the docker

00:14:51,810 --> 00:14:55,830
host network docker hosts per network

00:14:53,730 --> 00:14:57,900
means that oh doctor is very good I need

00:14:55,830 --> 00:14:59,910
the docker accept the networking part

00:14:57,900 --> 00:15:03,450
for the networking part I would like to

00:14:59,910 --> 00:15:06,690
use the host the host interface of the

00:15:03,450 --> 00:15:09,360
network you can see that my containers

00:15:06,690 --> 00:15:12,089
are here and but the IP address and the

00:15:09,360 --> 00:15:14,220
interface is exactly the same as in the

00:15:12,089 --> 00:15:17,040
node there are some limitations that I

00:15:14,220 --> 00:15:19,290
can start the same service twice on the

00:15:17,040 --> 00:15:21,150
same node because I can't use the same

00:15:19,290 --> 00:15:24,000
port twice but it's not a problem

00:15:21,150 --> 00:15:26,130
because it's Hadoop I need just one work

00:15:24,000 --> 00:15:28,500
I know that every node so it could work

00:15:26,130 --> 00:15:31,320
and it could be very powerful

00:15:28,500 --> 00:15:33,480
so container is just the unit of the

00:15:31,320 --> 00:15:36,810
packaging I don't need to use all of the

00:15:33,480 --> 00:15:40,110
features of the Dockers

00:15:36,810 --> 00:15:42,570
ok so these are some other

00:15:40,110 --> 00:15:45,780
problems which should be checked with

00:15:42,570 --> 00:15:48,630
every continent so that how can I use

00:15:45,780 --> 00:15:50,310
the what kind of network could be used

00:15:48,630 --> 00:15:53,310
between the containers if it's a multi

00:15:50,310 --> 00:15:55,920
node consider okay but still there is

00:15:53,310 --> 00:15:58,890
another problem because that was all

00:15:55,920 --> 00:16:02,550
good on configuration and here I have a

00:15:58,890 --> 00:16:05,070
host name so what should be returned to

00:16:02,550 --> 00:16:07,680
here what is the host of the name node

00:16:05,070 --> 00:16:10,170
well I don't know because the name node

00:16:07,680 --> 00:16:12,450
this red container will be scheduled

00:16:10,170 --> 00:16:13,500
somewhere so it could be node 4 or no -

00:16:12,450 --> 00:16:15,570
who knows

00:16:13,500 --> 00:16:17,580
I don't know so we need some kind of

00:16:15,570 --> 00:16:19,980
service discovery because we need to

00:16:17,580 --> 00:16:23,010
configure the data nodes to access the

00:16:19,980 --> 00:16:24,570
name nodes okay what can we do yeah and

00:16:23,010 --> 00:16:26,700
this is the place where we can use

00:16:24,570 --> 00:16:29,580
console we need just a console at every

00:16:26,700 --> 00:16:31,350
node and Nomad could save the

00:16:29,580 --> 00:16:33,450
information the scheduling information

00:16:31,350 --> 00:16:36,000
to console Steven it's some magic

00:16:33,450 --> 00:16:38,130
because before the start we need to

00:16:36,000 --> 00:16:42,480
modify the configuration of the data

00:16:38,130 --> 00:16:44,910
nodes but we have already already need

00:16:42,480 --> 00:16:47,100
it right we do it's very similar we need

00:16:44,910 --> 00:16:51,510
some modification on in the launcher

00:16:47,100 --> 00:16:54,330
script and we can do it okay so let's

00:16:51,510 --> 00:16:57,510
check it from closer I have we have the

00:16:54,330 --> 00:16:59,910
Nomad we have a container we have a Java

00:16:57,510 --> 00:17:01,590
process ahead of Java process in in this

00:16:59,910 --> 00:17:03,480
case but it could be any other kind of

00:17:01,590 --> 00:17:06,270
distributed application we have a

00:17:03,480 --> 00:17:08,580
console and during the scheduling Nomad

00:17:06,270 --> 00:17:10,370
we start the docker instance and save

00:17:08,580 --> 00:17:15,060
the data to the service information

00:17:10,370 --> 00:17:16,740
service name IP address so when Java

00:17:15,060 --> 00:17:19,199
process is starting we need some

00:17:16,740 --> 00:17:20,730
launcher a good example is a is the

00:17:19,199 --> 00:17:23,310
console template which is an open source

00:17:20,730 --> 00:17:25,500
project which could render the final

00:17:23,310 --> 00:17:27,270
configuration because we have good old

00:17:25,500 --> 00:17:29,640
application which can't speak with

00:17:27,270 --> 00:17:31,440
console natively and we can render the

00:17:29,640 --> 00:17:34,980
configuration and just start the Java

00:17:31,440 --> 00:17:38,400
process after that and with this this

00:17:34,980 --> 00:17:40,560
setup we have some other space for doing

00:17:38,400 --> 00:17:43,800
other powerful things for example we can

00:17:40,560 --> 00:17:46,740
restart the Java process in case of of

00:17:43,800 --> 00:17:48,330
server change all right because we

00:17:46,740 --> 00:17:50,490
started the head of Java process we can

00:17:48,330 --> 00:17:52,230
listen on the service info in the

00:17:50,490 --> 00:17:53,420
console and just restart the Java in

00:17:52,230 --> 00:17:55,550
case of configuration

00:17:53,420 --> 00:17:57,140
and we can also upload all of the

00:17:55,550 --> 00:17:59,360
configuration to the console because

00:17:57,140 --> 00:18:00,830
it's a key value store and the console

00:17:59,360 --> 00:18:02,900
template can just download the

00:18:00,830 --> 00:18:07,220
configuration render them based on the

00:18:02,900 --> 00:18:11,150
dynamic dynamic servicing food service

00:18:07,220 --> 00:18:12,770
name and an IP address and we can render

00:18:11,150 --> 00:18:14,600
the configuration and save it to the

00:18:12,770 --> 00:18:17,050
disk and start the Hadoop yeah it seems

00:18:14,600 --> 00:18:20,120
to be a little bit complex right so

00:18:17,050 --> 00:18:22,910
don't panic if you if you can't follow

00:18:20,120 --> 00:18:25,100
it but that's the feeling of this

00:18:22,910 --> 00:18:28,660
approach actually something like this so

00:18:25,100 --> 00:18:29,990
it's a very do it yourself application

00:18:28,660 --> 00:18:33,470
yeah

00:18:29,990 --> 00:18:40,160
I can also show can it maybe it's better

00:18:33,470 --> 00:18:44,900
to show from here oh I need a sudo for

00:18:40,160 --> 00:18:51,400
this oh yeah

00:18:44,900 --> 00:18:54,860
I have it oh I have it here with more

00:18:51,400 --> 00:18:57,470
better resolution so that is just the

00:18:54,860 --> 00:18:59,330
feeling of this approach this is the UI

00:18:57,470 --> 00:19:03,830
of the console you can see that I have

00:18:59,330 --> 00:19:05,360
multiple node there at the bottom I have

00:19:03,830 --> 00:19:08,570
to work or node

00:19:05,360 --> 00:19:12,050
those are cons or locks of the of the to

00:19:08,570 --> 00:19:14,000
worker node so this is the console I can

00:19:12,050 --> 00:19:16,280
see the services I have the name node

00:19:14,000 --> 00:19:19,280
for example the name of the schedule the

00:19:16,280 --> 00:19:21,140
Nomad - in that case I can check the

00:19:19,280 --> 00:19:23,750
configuration all of the configurations

00:19:21,140 --> 00:19:28,370
are uploaded to here okay this is the

00:19:23,750 --> 00:19:30,470
HDFS configuration I can check it you

00:19:28,370 --> 00:19:34,880
can see here is the magic right this

00:19:30,470 --> 00:19:36,770
will be replaced during the startup ok

00:19:34,880 --> 00:19:39,100
let's go forward

00:19:36,770 --> 00:19:44,950
I have other configuration as well yeah

00:19:39,100 --> 00:19:50,030
for example the log4j properties and i

00:19:44,950 --> 00:19:52,430
can just start oh i can modify this for

00:19:50,030 --> 00:19:55,820
example the info level to debug level

00:19:52,430 --> 00:19:59,840
and i can just save it that's the same

00:19:55,820 --> 00:20:01,940
battle and but you can see here that I

00:19:59,840 --> 00:20:04,760
just clicked to the save in the console

00:20:01,940 --> 00:20:06,920
and it's automatically reloaded all of

00:20:04,760 --> 00:20:07,400
the configuration so this is debug logs

00:20:06,920 --> 00:20:09,860
are

00:20:07,400 --> 00:20:12,260
so everything works without any

00:20:09,860 --> 00:20:14,210
modification so I think it's a it's a

00:20:12,260 --> 00:20:22,610
very very powerful tool

00:20:14,210 --> 00:20:23,930
oh that's and the to do all of this yeah

00:20:22,610 --> 00:20:25,309
this this is what I mentioned this is

00:20:23,930 --> 00:20:26,990
the own change we can restore the

00:20:25,309 --> 00:20:29,030
restore the application without any

00:20:26,990 --> 00:20:32,390
modification in the application itself

00:20:29,030 --> 00:20:34,490
so it's it's very powerful to use yeah

00:20:32,390 --> 00:20:37,370
it's a little bit do-it-yourself but it

00:20:34,490 --> 00:20:39,440
it could be very useful and other big

00:20:37,370 --> 00:20:41,990
advantages of this approach that this is

00:20:39,440 --> 00:20:44,300
not just cloud native but it's Hadoop

00:20:41,990 --> 00:20:46,640
native because this is exactly the same

00:20:44,300 --> 00:20:49,400
way how the Hadoop was tested in the

00:20:46,640 --> 00:20:52,670
last I don't know 10 years right because

00:20:49,400 --> 00:20:54,980
we are using docker host network this is

00:20:52,670 --> 00:20:56,870
exactly the same network as usually the

00:20:54,980 --> 00:21:01,100
Hadoop is used so no problem with that

00:20:56,870 --> 00:21:03,170
okay but let's compare it with we do

00:21:01,100 --> 00:21:06,380
with the Hadoop with the kubernetes

00:21:03,170 --> 00:21:08,390
approach if the Heshy Corp version was

00:21:06,380 --> 00:21:12,350
the do-it-yourself the kubernetes is the

00:21:08,390 --> 00:21:15,500
out-of-the-box version okay do you know

00:21:12,350 --> 00:21:17,900
kubernetes or do you use kubernetes okay

00:21:15,500 --> 00:21:20,090
half of them very good I have a other

00:21:17,900 --> 00:21:24,320
full kubernetes training in 60 seconds

00:21:20,090 --> 00:21:26,900
or maybe two minutes okay so what is

00:21:24,320 --> 00:21:28,880
kubernetes the situation is the same we

00:21:26,900 --> 00:21:31,910
have nodes right and I would like to I

00:21:28,880 --> 00:21:34,220
would like to start the contrast and I

00:21:31,910 --> 00:21:36,620
don't care where they will be started

00:21:34,220 --> 00:21:38,450
they should be started where I have

00:21:36,620 --> 00:21:40,220
enough capacity so it could be hundred

00:21:38,450 --> 00:21:42,140
of nodes okay

00:21:40,220 --> 00:21:43,700
another application I need two

00:21:42,140 --> 00:21:46,190
containers they will be started or I

00:21:43,700 --> 00:21:48,350
need three containers they will be

00:21:46,190 --> 00:21:52,850
started the big difference is between no

00:21:48,350 --> 00:21:55,550
mod and an kubernetes that normal is

00:21:52,850 --> 00:21:57,170
just a scheduler it could start and it

00:21:55,550 --> 00:21:59,330
couldn't do anything as for example the

00:21:57,170 --> 00:22:01,270
network problem was not solved right we

00:21:59,330 --> 00:22:04,070
solved it with the current network and

00:22:01,270 --> 00:22:07,280
but with kubernetes the kubernetes can

00:22:04,070 --> 00:22:10,309
see these containers as application so

00:22:07,280 --> 00:22:11,780
they are tied together and it could

00:22:10,309 --> 00:22:12,620
solve all of the other problems the

00:22:11,780 --> 00:22:14,390
network problems

00:22:12,620 --> 00:22:17,390
yeah it's Bugaboo but we have multiple

00:22:14,390 --> 00:22:19,970
options to provide some kind of networks

00:22:17,390 --> 00:22:21,200
between the containers we have solution

00:22:19,970 --> 00:22:22,669
for the storage

00:22:21,200 --> 00:22:25,700
that's the out-of-the-box out-of-the-box

00:22:22,669 --> 00:22:27,769
we have all of these features for

00:22:25,700 --> 00:22:30,980
example any kind of configuration or

00:22:27,769 --> 00:22:33,649
secret could be mounted as a external

00:22:30,980 --> 00:22:35,630
mount point or a file and will be

00:22:33,649 --> 00:22:38,480
available from all of the container in

00:22:35,630 --> 00:22:41,210
the in the same application and all of

00:22:38,480 --> 00:22:43,549
the complexity of the kubernetes is just

00:22:41,210 --> 00:22:45,889
all of these resources or most of these

00:22:43,549 --> 00:22:49,639
resources are just definition or some

00:22:45,889 --> 00:22:51,860
kind of rules to say that oh where this

00:22:49,639 --> 00:22:53,840
kind of containers should be started

00:22:51,860 --> 00:22:57,470
for example the daemons that is a rule

00:22:53,840 --> 00:22:59,750
to start something on every node or I

00:22:57,470 --> 00:23:02,179
could have a replica set when I say that

00:22:59,750 --> 00:23:03,860
okay it should be started on just it

00:23:02,179 --> 00:23:07,250
doesn't matter where but I need five

00:23:03,860 --> 00:23:10,880
instances so all of this complexity is

00:23:07,250 --> 00:23:14,299
just in fact just the rule to start the

00:23:10,880 --> 00:23:18,139
containers somewhere okay seems to be

00:23:14,299 --> 00:23:20,480
pretty cool right we finished that's the

00:23:18,139 --> 00:23:22,820
sounds good everybody knows that it's

00:23:20,480 --> 00:23:24,440
cool what's the problem with that well

00:23:22,820 --> 00:23:28,279
there is no problem with that

00:23:24,440 --> 00:23:29,840
exactly but the thinking of kubernetes

00:23:28,279 --> 00:23:32,120
is different from the thinking of the

00:23:29,840 --> 00:23:34,309
head tube for example this is a back-end

00:23:32,120 --> 00:23:37,130
application I defined that I need three

00:23:34,309 --> 00:23:39,500
instances and you just scaled up good I

00:23:37,130 --> 00:23:41,179
have a front-end application and I need

00:23:39,500 --> 00:23:43,190
some kind of connection between the

00:23:41,179 --> 00:23:46,580
front end and the back end it's a good

00:23:43,190 --> 00:23:49,429
old front-end application so it it

00:23:46,580 --> 00:23:53,230
doesn't know if the backend is killed up

00:23:49,429 --> 00:23:57,049
or not so there is an other kubernetes

00:23:53,230 --> 00:23:59,840
resource side service which could which

00:23:57,049 --> 00:24:02,240
works as the load balancer and the

00:23:59,840 --> 00:24:05,750
problem here is that the service has a

00:24:02,240 --> 00:24:08,960
network identity a DNS but the ports or

00:24:05,750 --> 00:24:11,389
the containers itself has no network

00:24:08,960 --> 00:24:14,480
identity what do we need for the Hadoop

00:24:11,389 --> 00:24:17,090
yeah we need DNS or it could be turned

00:24:14,480 --> 00:24:20,389
off but typically we need a network

00:24:17,090 --> 00:24:22,880
identity because we solved all of this

00:24:20,389 --> 00:24:26,380
replication problem manually so we could

00:24:22,880 --> 00:24:29,600
be do it more effectively but we need

00:24:26,380 --> 00:24:31,850
network network names between the

00:24:29,600 --> 00:24:34,030
components because we are just managing

00:24:31,850 --> 00:24:36,460
the replicas and

00:24:34,030 --> 00:24:38,560
blowing all the stuff so it could be

00:24:36,460 --> 00:24:40,150
started without DNS but it's not an easy

00:24:38,560 --> 00:24:41,710
there are a lot of options just turn off

00:24:40,150 --> 00:24:43,710
turn off turn off turn off then and

00:24:41,710 --> 00:24:46,750
that's the part where the Nomad based

00:24:43,710 --> 00:24:49,270
approach was Hadoop native because it

00:24:46,750 --> 00:24:52,960
could work as before with DNS but here

00:24:49,270 --> 00:24:55,380
it's more tricky yeah but we have we

00:24:52,960 --> 00:24:58,300
have the stateful set where we have a

00:24:55,380 --> 00:25:00,790
kubernetes container or port together

00:24:58,300 --> 00:25:02,830
with a network identity and it could be

00:25:00,790 --> 00:25:05,590
used and in fact this is the mostly used

00:25:02,830 --> 00:25:09,340
resource tie for forehead opened and

00:25:05,590 --> 00:25:11,110
stateful application a small problem

00:25:09,340 --> 00:25:13,150
that we can't use the other resources

00:25:11,110 --> 00:25:16,450
just the stateful set for example the

00:25:13,150 --> 00:25:18,940
daemon said couldn't be used easily even

00:25:16,450 --> 00:25:22,960
if it's so cool to store something at

00:25:18,940 --> 00:25:26,200
every node okay so that was it was not

00:25:22,960 --> 00:25:28,090
bad sake actually it just a difference

00:25:26,200 --> 00:25:30,460
between the thinking of Hadoop and

00:25:28,090 --> 00:25:33,130
kubernetes kubernetes tries to manage

00:25:30,460 --> 00:25:35,130
all of the good old and dummy

00:25:33,130 --> 00:25:38,950
applications and we have a very smart

00:25:35,130 --> 00:25:41,260
distributed application but we can use a

00:25:38,950 --> 00:25:43,890
restricted set of the kubernetes

00:25:41,260 --> 00:25:47,200
resources and it could work very well

00:25:43,890 --> 00:25:50,620
okay so what is the good thing I think

00:25:47,200 --> 00:25:54,580
it could be defined in in multiple ways

00:25:50,620 --> 00:25:59,710
but my favorite two things is the

00:25:54,580 --> 00:26:03,370
ecosystem and flexibility so what what

00:25:59,710 --> 00:26:05,530
does it mean exactly so let's do some

00:26:03,370 --> 00:26:08,860
example exercise so do you know

00:26:05,530 --> 00:26:12,820
promoters do you are promoters is a

00:26:08,860 --> 00:26:15,040
cloud native monitoring tool and this is

00:26:12,820 --> 00:26:16,900
something like this monitoring to call

00:26:15,040 --> 00:26:20,020
Excel above metrics and we can just

00:26:16,900 --> 00:26:22,210
check what's happening okay so I would

00:26:20,020 --> 00:26:24,640
like to use Pro Mattel's together with

00:26:22,210 --> 00:26:28,510
head of my default Hadoop doesn't

00:26:24,640 --> 00:26:31,090
support Pro metal so what can we do well

00:26:28,510 --> 00:26:33,520
there are tube providers is it's very

00:26:31,090 --> 00:26:37,420
simple so it's a server application and

00:26:33,520 --> 00:26:39,640
by default it poles the components and

00:26:37,420 --> 00:26:41,860
each component should provide an HTTP

00:26:39,640 --> 00:26:46,780
endpoint where the metrics are published

00:26:41,860 --> 00:26:47,860
so we need to think one is a HTTP

00:26:46,780 --> 00:26:49,270
endpoint here and

00:26:47,860 --> 00:26:51,160
here and in all the data knowledge

00:26:49,270 --> 00:26:52,960
Hadoop doesn't have it so we need some

00:26:51,160 --> 00:26:55,210
magic and the other one what we need

00:26:52,960 --> 00:26:57,700
that primate house should know where are

00:26:55,210 --> 00:26:59,200
the HTTP endpoints you know that we have

00:26:57,700 --> 00:27:03,549
a lot of containers I don't know where

00:26:59,200 --> 00:27:06,610
they are started so flexibility and

00:27:03,549 --> 00:27:09,610
ecosystem a start with the ecosystem but

00:27:06,610 --> 00:27:12,520
I mean on ecosystem that it's something

00:27:09,610 --> 00:27:14,710
like the get you can start you can't

00:27:12,520 --> 00:27:17,020
create a new developer to we don't get

00:27:14,710 --> 00:27:19,059
support I guess right because it is it's

00:27:17,020 --> 00:27:22,360
it's supported everywhere it's it's very

00:27:19,059 --> 00:27:24,760
widely used and I think sooner or later

00:27:22,360 --> 00:27:26,620
it will be the same for kubernetes so

00:27:24,760 --> 00:27:29,830
the ecosystem is that we have a lot of

00:27:26,620 --> 00:27:32,100
tools and more and more tools support

00:27:29,830 --> 00:27:35,169
the kubernetes environment for example

00:27:32,100 --> 00:27:39,010
comatose because it's it's under the

00:27:35,169 --> 00:27:42,820
same cloud native foundation it's it

00:27:39,010 --> 00:27:45,340
supports natively the kubernetes so it

00:27:42,820 --> 00:27:48,429
can just retrieve the available services

00:27:45,340 --> 00:27:52,720
from the kubernetes api and check where

00:27:48,429 --> 00:27:54,280
are the HTTP endpoints are started this

00:27:52,720 --> 00:27:56,590
is something like this so this is a

00:27:54,280 --> 00:27:58,000
kubernetes service or definition you

00:27:56,590 --> 00:28:01,360
don't need to understand all of the

00:27:58,000 --> 00:28:06,640
things you need to just find difference

00:28:01,360 --> 00:28:09,580
between the two slide so that's the good

00:28:06,640 --> 00:28:11,500
thing we need just add two annotations

00:28:09,580 --> 00:28:13,960
and because pro men-tel's is listening

00:28:11,500 --> 00:28:16,210
in the kubernetes api if something is

00:28:13,960 --> 00:28:16,929
started with these two annotations it

00:28:16,210 --> 00:28:18,880
will be polled

00:28:16,929 --> 00:28:21,040
automatically so this is the echo system

00:28:18,880 --> 00:28:25,210
and it's not just the promoters so a lot

00:28:21,040 --> 00:28:29,530
of other applications and just newer and

00:28:25,210 --> 00:28:33,970
newer application we will support this

00:28:29,530 --> 00:28:37,720
kind of we support kubernetes in this

00:28:33,970 --> 00:28:38,400
way ok so let's go forward still we have

00:28:37,720 --> 00:28:41,230
a problem

00:28:38,400 --> 00:28:42,760
so promoters knows that where the name

00:28:41,230 --> 00:28:45,730
node and data node will be started and

00:28:42,760 --> 00:28:47,590
where the HTTP endpoints will be started

00:28:45,730 --> 00:28:50,380
but we have no HTTP endpoint right

00:28:47,590 --> 00:28:53,080
because it's Hadoop Hadoop has DMX no

00:28:50,380 --> 00:28:55,750
HTTP endpoint what can we do yeah we can

00:28:53,080 --> 00:28:58,360
modify Hadoop but thats part is the

00:28:55,750 --> 00:29:01,270
flexibility you know ecosystem and

00:28:58,360 --> 00:29:01,539
flexibility so until no I said in the

00:29:01,270 --> 00:29:02,979
coop

00:29:01,539 --> 00:29:05,229
natives we have containers that the

00:29:02,979 --> 00:29:07,239
containers are started yeah that was a

00:29:05,229 --> 00:29:09,519
little bit high level so in fact we have

00:29:07,239 --> 00:29:13,149
no containers but boats boats

00:29:09,519 --> 00:29:15,789
are the basic unity in in the kubernetes

00:29:13,149 --> 00:29:17,979
world and yeah it contains one container

00:29:15,789 --> 00:29:20,499
and a few other things for example the

00:29:17,979 --> 00:29:22,570
volume - volume definition that for

00:29:20,499 --> 00:29:25,330
example which secret should be mounted

00:29:22,570 --> 00:29:28,029
to the pod and usually usually one pod

00:29:25,330 --> 00:29:31,629
is one container but in some special

00:29:28,029 --> 00:29:33,519
cases we can put two containers in the

00:29:31,629 --> 00:29:36,759
same pod so two containers will be

00:29:33,519 --> 00:29:38,109
started in the same host and and it's

00:29:36,759 --> 00:29:40,389
not just started in the same house but

00:29:38,109 --> 00:29:42,340
they could see a they can share a lot of

00:29:40,389 --> 00:29:44,710
things for example the same network

00:29:42,340 --> 00:29:48,340
interface will be used the same volume

00:29:44,710 --> 00:29:50,169
or even the same processes so it's a

00:29:48,340 --> 00:29:53,009
it's an alpha feature but we can turn it

00:29:50,169 --> 00:29:56,559
on and both of the containers with seed

00:29:53,009 --> 00:29:59,289
the same processes so this is the

00:29:56,559 --> 00:30:01,869
sidecar pattern and imagine that this is

00:29:59,289 --> 00:30:03,879
my good old hadoop application this is a

00:30:01,869 --> 00:30:05,979
sidecar application which will be

00:30:03,879 --> 00:30:08,710
started it's an other container and it

00:30:05,979 --> 00:30:12,070
just check the processes and it will

00:30:08,710 --> 00:30:14,979
start an HTTP server and publish all of

00:30:12,070 --> 00:30:17,769
the gmx interfaces and publish all of

00:30:14,979 --> 00:30:19,659
the gmx monitoring information from the

00:30:17,769 --> 00:30:21,309
java process so it can check the java

00:30:19,659 --> 00:30:26,499
process get all of the information and

00:30:21,309 --> 00:30:31,239
publish here OPA oh something has been

00:30:26,499 --> 00:30:33,519
happened so again this is this is just

00:30:31,239 --> 00:30:36,639
two lines right so that's the

00:30:33,519 --> 00:30:37,929
flexibility so it's it's very easy this

00:30:36,639 --> 00:30:42,249
is just one example there are other

00:30:37,929 --> 00:30:44,919
examples to check the kubernetes api and

00:30:42,249 --> 00:30:47,289
use operator but this is an example that

00:30:44,919 --> 00:30:51,249
just with two lines without modifying

00:30:47,289 --> 00:30:54,340
the good old hadoop everything is

00:30:51,249 --> 00:30:56,169
published to primate house without any

00:30:54,340 --> 00:30:58,629
any other change and it works not just

00:30:56,169 --> 00:31:01,029
for the Hadoop this works for every java

00:30:58,629 --> 00:31:04,799
application so i think it's a it's a

00:31:01,029 --> 00:31:07,950
pretty powerful thing and back to the

00:31:04,799 --> 00:31:11,529
ecosystem that ecosystem also means that

00:31:07,950 --> 00:31:15,280
yeah it includes the fact that it's also

00:31:11,529 --> 00:31:17,350
a common language so even now most

00:31:15,280 --> 00:31:21,160
the cloud provider supports kubernetes

00:31:17,350 --> 00:31:24,210
so it's very easy to run the application

00:31:21,160 --> 00:31:30,190
the same way on pram or in the cloud

00:31:24,210 --> 00:31:32,590
okay so we did it I think that's the

00:31:30,190 --> 00:31:36,820
that's the final label for for

00:31:32,590 --> 00:31:38,920
kubernetes and I didn't mention all of

00:31:36,820 --> 00:31:42,010
these things but most of them are out of

00:31:38,920 --> 00:31:44,170
the box included or we have a external

00:31:42,010 --> 00:31:46,330
tool in the ecosystem such like the ham

00:31:44,170 --> 00:31:48,730
which is some kind of package management

00:31:46,330 --> 00:31:51,160
the config map is an other kubernetes

00:31:48,730 --> 00:31:55,180
resource that we have this small booty

00:31:51,160 --> 00:31:57,160
host support with with pluggable this is

00:31:55,180 --> 00:32:01,000
a pluggable interface and there are

00:31:57,160 --> 00:32:04,090
multiple options to use network between

00:32:01,000 --> 00:32:07,360
the containers this is just one set so

00:32:04,090 --> 00:32:09,850
you can you can do it multiple way for

00:32:07,360 --> 00:32:12,520
example you can use console for the for

00:32:09,850 --> 00:32:14,800
the configuration or you can use docker

00:32:12,520 --> 00:32:16,510
host Network to achieve a more help

00:32:14,800 --> 00:32:18,700
Native approach so there are just

00:32:16,510 --> 00:32:21,370
elements and you can use it in multiple

00:32:18,700 --> 00:32:25,570
ways but what I would like to say that

00:32:21,370 --> 00:32:28,150
if we are comparing the freezers or the

00:32:25,570 --> 00:32:31,300
or the washing machines before we buy it

00:32:28,150 --> 00:32:34,660
then we can compare the containers

00:32:31,300 --> 00:32:36,790
environment as well and I think these

00:32:34,660 --> 00:32:42,580
are the most important questions which

00:32:36,790 --> 00:32:45,480
should be answered somehow anyway ok

00:32:42,580 --> 00:32:50,580
that was the that was the summary so

00:32:45,480 --> 00:32:53,440
don't bother checking the label the

00:32:50,580 --> 00:32:56,890
other one is that the containerization

00:32:53,440 --> 00:33:00,790
can help a lot so we saw that with this

00:32:56,890 --> 00:33:03,760
just with three lines or four lines we

00:33:00,790 --> 00:33:06,040
got some monitoring and this is the this

00:33:03,760 --> 00:33:08,170
is true for local local action for

00:33:06,040 --> 00:33:10,120
example it's just a few lines because a

00:33:08,170 --> 00:33:12,280
lot of tools are available for the

00:33:10,120 --> 00:33:13,900
containers environment and especially

00:33:12,280 --> 00:33:18,130
for for kubernetes

00:33:13,900 --> 00:33:21,450
then then it's it's I believe that it's

00:33:18,130 --> 00:33:26,020
more effective to use this kind of

00:33:21,450 --> 00:33:28,840
applications in kubernetes and third one

00:33:26,020 --> 00:33:31,419
is just it's the question is that how do

00:33:28,840 --> 00:33:36,360
Hadoop is Hadoop cloud native or or not

00:33:31,419 --> 00:33:39,610
so it depends but I think it's almost so

00:33:36,360 --> 00:33:41,770
Hadoop is designed to be and distributed

00:33:39,610 --> 00:33:44,500
application so it's very easy to start

00:33:41,770 --> 00:33:47,799
containers there are some limitation for

00:33:44,500 --> 00:33:50,710
example this DNS in thingy which could

00:33:47,799 --> 00:33:53,650
be improved but they they are not big

00:33:50,710 --> 00:34:00,520
architectural change it's just small

00:33:53,650 --> 00:34:07,899
changes but I think we need to add to

00:34:00,520 --> 00:34:09,639
Hadoop okay that's pretty much all this

00:34:07,899 --> 00:34:12,609
is my availability you can check out

00:34:09,639 --> 00:34:15,040
almost all of the codes not in a very

00:34:12,609 --> 00:34:18,429
well-documented format but you can see

00:34:15,040 --> 00:34:21,099
my work all of these docker images and

00:34:18,429 --> 00:34:24,849
kubernetes definition and I will have

00:34:21,099 --> 00:34:27,250
another talk tomorrow about I think they

00:34:24,849 --> 00:34:30,639
I think the title is the same but it

00:34:27,250 --> 00:34:33,280
will be more about Hadoop and Hadoop

00:34:30,639 --> 00:34:36,669
ozone and Hadoop HDDs and how Hadoop

00:34:33,280 --> 00:34:42,159
could provide some kind of persistent

00:34:36,669 --> 00:34:45,810
storage to kubernetes clusters so that's

00:34:42,159 --> 00:34:45,810
all is there any question

00:34:46,510 --> 00:34:54,629
[Applause]

00:34:56,460 --> 00:35:01,480
first of all very good talk very visual

00:34:59,050 --> 00:35:03,880
so I like it very much um about the

00:35:01,480 --> 00:35:07,960
sidecar approach about having Prometheus

00:35:03,880 --> 00:35:10,450
matrix is a function of having the

00:35:07,960 --> 00:35:12,760
containers in the pod share processes

00:35:10,450 --> 00:35:15,490
the Alpha feature as I mentioned have

00:35:12,760 --> 00:35:17,860
you looked at push gateway or primitives

00:35:15,490 --> 00:35:19,840
it's a server that you can push metrics

00:35:17,860 --> 00:35:21,760
and have it exposed since parameters of

00:35:19,840 --> 00:35:24,220
okay so although also problem also

00:35:21,760 --> 00:35:26,290
possible to use push but the native mode

00:35:24,220 --> 00:35:28,120
is just the pool model so usually pro

00:35:26,290 --> 00:35:30,880
may tell you know easier to use the pool

00:35:28,120 --> 00:35:32,320
model yeah so that's why the second

00:35:30,880 --> 00:35:36,580
container the second image that you have

00:35:32,320 --> 00:35:41,410
like gmx sidecar sorry I'm making you go

00:35:36,580 --> 00:35:43,990
back to white yeah yeah yeah in where is

00:35:41,410 --> 00:35:46,360
it it's in exercises I think it's a Nemo

00:35:43,990 --> 00:35:49,090
configuration kind of so I in fact it's

00:35:46,360 --> 00:35:52,210
more I simplified a little bit story

00:35:49,090 --> 00:35:54,790
yeah but if if you are interested

00:35:52,210 --> 00:35:55,300
oh that's somewhere yeah things forward

00:35:54,790 --> 00:35:57,310
forward

00:35:55,300 --> 00:36:01,120
okay in fact you know this is there is a

00:35:57,310 --> 00:36:03,280
Java attachment API and what's happening

00:36:01,120 --> 00:36:05,910
here in my container that is checking

00:36:03,280 --> 00:36:09,250
all of the processes in if there is a

00:36:05,910 --> 00:36:12,330
Java process a new Java agent will be

00:36:09,250 --> 00:36:14,890
attached to the Java which contains

00:36:12,330 --> 00:36:16,870
internally there is this code this is

00:36:14,890 --> 00:36:19,750
exactly the almost the same as the Java

00:36:16,870 --> 00:36:22,210
parameters exporter so it just reads the

00:36:19,750 --> 00:36:24,520
Duramax and will be exported but in fact

00:36:22,210 --> 00:36:27,040
the HTTP here but I had simplified a

00:36:24,520 --> 00:36:29,170
little bit story it also could be done

00:36:27,040 --> 00:36:31,240
with a different way to just read the

00:36:29,170 --> 00:36:34,480
jam X and publish from here but that's

00:36:31,240 --> 00:36:35,410
the that's the how it works currently

00:36:34,480 --> 00:36:37,180
awesome

00:36:35,410 --> 00:36:42,240
but if ten lines of bash code actually

00:36:37,180 --> 00:36:42,240
so it's great thanks yeah

00:36:56,510 --> 00:37:03,070
yeah thank you for the talk questions so

00:36:59,450 --> 00:37:08,690
you at as a statement or maybe also as a

00:37:03,070 --> 00:37:10,730
question at the end so a tube as being a

00:37:08,690 --> 00:37:12,710
first-class citizen and a containerized

00:37:10,730 --> 00:37:15,950
MIT so my question would be so if I have

00:37:12,710 --> 00:37:18,590
now I took running have gone there and

00:37:15,950 --> 00:37:20,660
deploy my naps in that environment so

00:37:18,590 --> 00:37:23,900
isn't that then a container in a

00:37:20,660 --> 00:37:26,990
container so why wouldn't I am thinking

00:37:23,900 --> 00:37:31,640
of pouring my young EPS so that they are

00:37:26,990 --> 00:37:35,660
then native to kubernetes it's okay I

00:37:31,640 --> 00:37:37,070
think it's more a it's a complex

00:37:35,660 --> 00:37:39,950
question but it's it's mostly about here

00:37:37,070 --> 00:37:44,450
on right the scheduling part then what

00:37:39,950 --> 00:37:46,100
about the scheduling and yeah that there

00:37:44,450 --> 00:37:48,590
are two kind of scheduling in in yarn

00:37:46,100 --> 00:37:50,540
one is just scheduled native MapReduce

00:37:48,590 --> 00:37:54,380
application another one is a scheduling

00:37:50,540 --> 00:37:57,740
docker I think it's it's it's a

00:37:54,380 --> 00:38:00,680
reasonable way to run MapReduce jobs

00:37:57,740 --> 00:38:02,450
inside at the container in your but for

00:38:00,680 --> 00:38:04,880
example the SPARC follows another

00:38:02,450 --> 00:38:08,150
approach SPARC natively could be

00:38:04,880 --> 00:38:10,070
scheduled jobs on on kubernetes so it's

00:38:08,150 --> 00:38:14,750
an under approach I think currently the

00:38:10,070 --> 00:38:16,700
yarn yarn has better scheduling than the

00:38:14,750 --> 00:38:19,490
kubernetes but I think the kubernetes

00:38:16,700 --> 00:38:22,160
ecosystem it's it's more advanced so

00:38:19,490 --> 00:38:24,800
there are multiple options and I can see

00:38:22,160 --> 00:38:28,250
in the future but one option could be

00:38:24,800 --> 00:38:30,140
just yeah I think the revalue in in yarn

00:38:28,250 --> 00:38:32,030
is the scheduling part and not running

00:38:30,140 --> 00:38:33,859
the containers I think running counters

00:38:32,030 --> 00:38:35,900
that could be done in a more effective

00:38:33,859 --> 00:38:45,290
way would kubernetes but this is my

00:38:35,900 --> 00:38:47,030
personal opinion so thank you yesterday

00:38:45,290 --> 00:38:50,600
there was a talk by a Dunning who

00:38:47,030 --> 00:38:52,780
mentioned also kubernetes and one of the

00:38:50,600 --> 00:38:57,440
key problems he touched there is storage

00:38:52,780 --> 00:39:03,619
so where do you put the data in in this

00:38:57,440 --> 00:39:07,490
kind of setup in yeah there are so in

00:39:03,619 --> 00:39:09,860
this where is I need a Hadoop cluster is

00:39:07,490 --> 00:39:12,320
this a Hadoop cluster oh let's see

00:39:09,860 --> 00:39:17,330
this is a new cluster oh not this one

00:39:12,320 --> 00:39:19,670
this one so by default the Hadoop could

00:39:17,330 --> 00:39:21,320
so there are different answers for

00:39:19,670 --> 00:39:25,040
different applications but speaking

00:39:21,320 --> 00:39:29,360
about Hadoop we have the data nodes who

00:39:25,040 --> 00:39:32,480
can the data node data nodes can handle

00:39:29,360 --> 00:39:35,600
all of these storage questions so by

00:39:32,480 --> 00:39:37,610
default the easiest way just use local

00:39:35,600 --> 00:39:40,640
storage here because anyway it doesn't

00:39:37,610 --> 00:39:42,890
matter if it will be lost or not because

00:39:40,640 --> 00:39:45,350
we have a replication mechanism by

00:39:42,890 --> 00:39:48,080
Hadoop between the now nodes so we don't

00:39:45,350 --> 00:39:51,080
need any network storage we need fast

00:39:48,080 --> 00:39:53,570
local storage even SSD or something like

00:39:51,080 --> 00:39:56,120
this and one interesting thing about the

00:39:53,570 --> 00:39:57,230
new head of sub-project for example the

00:39:56,120 --> 00:39:59,930
Apache Hadoop

00:39:57,230 --> 00:40:03,950
hdds and Apache Hadoop ozone that the

00:39:59,930 --> 00:40:06,020
plan is that this storage cluster could

00:40:03,950 --> 00:40:10,190
provide storage for the MapReduce or

00:40:06,020 --> 00:40:13,100
small job could provide a s3 like object

00:40:10,190 --> 00:40:15,080
store and could provide network storage

00:40:13,100 --> 00:40:17,630
for the other container similar to the

00:40:15,080 --> 00:40:21,050
NFS so the currently it's use ice cozy

00:40:17,630 --> 00:40:23,480
but later it could be improved so it in

00:40:21,050 --> 00:40:25,730
that case you will have one storage

00:40:23,480 --> 00:40:28,820
cluster inside kubernetes and you can

00:40:25,730 --> 00:40:30,880
use it as a HDFS store you can use it as

00:40:28,820 --> 00:40:33,920
an object store and you can use it as a

00:40:30,880 --> 00:40:36,620
kubernetes native block store by add

00:40:33,920 --> 00:40:38,950
from other containers at least that's

00:40:36,620 --> 00:40:38,950
the plan

00:40:42,710 --> 00:40:46,640
okay thanks Martin we anyway run out of

00:40:46,160 --> 00:40:48,220
time

00:40:46,640 --> 00:40:51,860
yeah yeah thank you very much

00:40:48,220 --> 00:40:51,860

YouTube URL: https://www.youtube.com/watch?v=Fs-zcR-sOJY


