Title: Berlin Buzzwords 2018: Patrick Baier, Lorand Dali â€“ Continuous Monitoring of Machine Learning Models
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Patrick Baier and Lorand Dali "Continuous Live Monitoring of Machine Learning Models with Delayed Label Feedback".

The usual steps of developing a machine learning model are: training on a training set, tuning on a validation set and evaluating the performance on the test set. Often this is the end of the story. However, if the model is particularly good, it will be deployed to serve predictions in a production system. In this talk we present what happens to a machine learning model after it is deployed in production at Zalando Payments. We focus on the precautions we need to take to ensure that a modelâ€™s predictions always stay at the high quality we expect. 

The stakes are high, particularly for models that directly touch the revenue stream. Since we cannot afford to let a drop in prediction quality pass unnoticed, we need to continuously monitor our deployed machine learning models. As we operate in the fraud detection domain, one additional challenge we face is that we only know several weeks later if a customer paid his order at Zalando and if our predictions were accurate in that case. 

This makes the simple solution of monitoring the prediction accuracy impractical, because by the time we notice the problem, it is already too late. In this talk, we present our solution, which consists of monitoring the similarity between the distributions of features in the live traffic and the distributions of features in the test set on which the model was evaluated. This allows us to immediately detect if the conditions under which the model was evaluated have substantially changed, which would invalidate the conclusions we drew in the initial testing. We describe how the mentioned changes in feature distributions are automatically detected using the TDigest algorithm, and how alerts are raised. 

Further, we delve into the technical implementation decisions: First, we describe how we collect the live traffic of a mission-critical service in a non-intrusive way, in order to avoid interfering with the normal operation of the service. Secondly, we present how the collected data is processed in a scalable way using Apache Spark. Finally, we show how we automate everything with AWS Data Pipelines.

Read more:
https://2018.berlinbuzzwords.de/18/session/continuous-live-monitoring-machine-learning-models-delayed-label-feedback

About Patrick Baier:
https://2018.berlinbuzzwords.de/users/patrick-baier

About Lorand Dali:
https://2018.berlinbuzzwords.de/users/lorand-dali

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,560 --> 00:00:09,990
so yeah welcome everyone to our talk

00:00:07,260 --> 00:00:12,450
today which is about monitoring of

00:00:09,990 --> 00:00:15,660
machine learning so we are both from

00:00:12,450 --> 00:00:17,910
solando payments which is kind of sub

00:00:15,660 --> 00:00:19,500
company of solando so I think people

00:00:17,910 --> 00:00:22,290
that live in Europe have heard of

00:00:19,500 --> 00:00:26,930
solando before it's the biggest online

00:00:22,290 --> 00:00:29,640
fashion shop in Europe and this talk we

00:00:26,930 --> 00:00:33,390
want to give you a brief idea what we do

00:00:29,640 --> 00:00:36,030
in terms of data science there and then

00:00:33,390 --> 00:00:38,309
we go over to this main topic which is

00:00:36,030 --> 00:00:41,370
why we should monitor machine learning

00:00:38,309 --> 00:00:43,350
models then we I will talk about a

00:00:41,370 --> 00:00:46,070
little bit about prediction monitoring

00:00:43,350 --> 00:00:48,359
in general and law and in the end we'll

00:00:46,070 --> 00:00:52,230
give you some insights about our

00:00:48,359 --> 00:00:54,960
implementation of these topics so who we

00:00:52,230 --> 00:00:57,679
are and what we do so

00:00:54,960 --> 00:00:59,399
I'm Patrick and here's Laurent he will

00:00:57,679 --> 00:01:02,010
catch up later

00:00:59,399 --> 00:01:05,190
so we both work as data scientist at

00:01:02,010 --> 00:01:07,680
solando I'm there for three and a half

00:01:05,190 --> 00:01:09,960
years low understeer for one and a half

00:01:07,680 --> 00:01:13,020
years yeah and we have different

00:01:09,960 --> 00:01:14,579
backgrounds maybe basically in data

00:01:13,020 --> 00:01:19,110
science machine learning computer

00:01:14,579 --> 00:01:21,229
science so maybe more interesting what

00:01:19,110 --> 00:01:24,750
we do at solando so as I said before

00:01:21,229 --> 00:01:28,259
solando is the biggest fashion online

00:01:24,750 --> 00:01:30,840
shop in Europe so maybe for people who

00:01:28,259 --> 00:01:32,729
didn't heard it before it's like Amazon

00:01:30,840 --> 00:01:35,640
but only in Europe and only for fashion

00:01:32,729 --> 00:01:38,789
right so what happens typically in such

00:01:35,640 --> 00:01:40,979
online shop there are people they chop

00:01:38,789 --> 00:01:44,539
they put items in the cart they want to

00:01:40,979 --> 00:01:48,060
have this shirt and they spend and then

00:01:44,539 --> 00:01:51,659
we ship them the stuff and then they pay

00:01:48,060 --> 00:01:54,119
so we have a very special thing here in

00:01:51,659 --> 00:01:57,659
Germany it's called invoice payment so

00:01:54,119 --> 00:01:59,579
it's like when you buy something we

00:01:57,659 --> 00:02:02,719
would allow you not to pay immediately

00:01:59,579 --> 00:02:05,159
but only after two weeks right so that's

00:02:02,719 --> 00:02:06,780
some people not from Germany they they

00:02:05,159 --> 00:02:08,580
also they always think that's kind of

00:02:06,780 --> 00:02:11,009
weird but it's actually what our Germans

00:02:08,580 --> 00:02:12,510
love they don't trust internet very much

00:02:11,009 --> 00:02:16,120
they want to have something before they

00:02:12,510 --> 00:02:18,550
pay so but the problem with this

00:02:16,120 --> 00:02:22,480
approach is more or less that this could

00:02:18,550 --> 00:02:24,910
happen right so we would ship stuff to

00:02:22,480 --> 00:02:28,300
the customers but they will not pay so

00:02:24,910 --> 00:02:30,880
this is obviously not so attractive for

00:02:28,300 --> 00:02:34,330
solando to to ship them something that

00:02:30,880 --> 00:02:36,880
they don't pay so our job is kind of to

00:02:34,330 --> 00:02:42,160
prevent this okay so we are in the

00:02:36,880 --> 00:02:44,080
payment fraud department I can give a

00:02:42,160 --> 00:02:46,780
little bit more detail throughout the

00:02:44,080 --> 00:02:49,989
course of this talk but I will not talk

00:02:46,780 --> 00:02:52,690
like in detail how we actually exactly

00:02:49,989 --> 00:02:55,209
do this and how we prevent just payment

00:02:52,690 --> 00:02:58,870
fraud but at least you can get a idea I

00:02:55,209 --> 00:03:00,880
think so what we do for this we have a

00:02:58,870 --> 00:03:05,470
machine learning model so we will like

00:03:00,880 --> 00:03:07,569
charge every order and we'll kind of try

00:03:05,470 --> 00:03:09,580
to estimate a probability of someone

00:03:07,569 --> 00:03:12,130
pace or not right depending on this

00:03:09,580 --> 00:03:14,290
probability if it's very high probably

00:03:12,130 --> 00:03:16,870
we don't ship the order or I have other

00:03:14,290 --> 00:03:19,000
measures in place but the interesting

00:03:16,870 --> 00:03:21,579
thing for you is like we have orders in

00:03:19,000 --> 00:03:23,500
the shop and for every for every order

00:03:21,579 --> 00:03:25,630
we have to attach just for probability

00:03:23,500 --> 00:03:29,590
that's that's the end of our story

00:03:25,630 --> 00:03:34,380
that's what we do and how we do is if we

00:03:29,590 --> 00:03:38,320
like have data are lying on AWS three

00:03:34,380 --> 00:03:41,380
AWS we have and it's kind of a

00:03:38,320 --> 00:03:44,680
collection of all order data in solando

00:03:41,380 --> 00:03:46,570
and it's quite a huge big data set

00:03:44,680 --> 00:03:48,880
because solando is out there now for

00:03:46,570 --> 00:03:50,620
around ten years so we basically have

00:03:48,880 --> 00:03:53,980
access to all these orders and we can

00:03:50,620 --> 00:03:56,200
just check who paid in the past who not

00:03:53,980 --> 00:03:58,780
we try to build a machine learning model

00:03:56,200 --> 00:04:01,180
with apache spark on top which then use

00:03:58,780 --> 00:04:03,459
the stuff like locust regression random

00:04:01,180 --> 00:04:05,470
forest GBG neural networks whatever we

00:04:03,459 --> 00:04:06,880
have always tried to find new features

00:04:05,470 --> 00:04:09,730
try to find new model to make this

00:04:06,880 --> 00:04:12,910
product more accurate okay so it's a

00:04:09,730 --> 00:04:16,120
very classical binary classification

00:04:12,910 --> 00:04:22,539
problem you want to predict for one not

00:04:16,120 --> 00:04:24,550
for zero the thing we have to do which

00:04:22,539 --> 00:04:27,520
goes a little bit beyond machine

00:04:24,550 --> 00:04:30,280
learning is that we also have to

00:04:27,520 --> 00:04:34,660
kind of supply our predictions in the

00:04:30,280 --> 00:04:36,699
production system so we actually want to

00:04:34,660 --> 00:04:39,190
evaluate the fraud risk for every order

00:04:36,699 --> 00:04:40,780
very timely so other teams depend on it

00:04:39,190 --> 00:04:42,970
and we have to provide it within one

00:04:40,780 --> 00:04:46,300
second so someone orders we have to say

00:04:42,970 --> 00:04:50,590
fraud sir not fraudster and for this we

00:04:46,300 --> 00:04:52,870
also built module a rest service which

00:04:50,590 --> 00:04:55,509
contains this module that I showed you

00:04:52,870 --> 00:04:58,539
just one slide before so it basically is

00:04:55,509 --> 00:05:00,849
fed by some features X which of course

00:04:58,539 --> 00:05:04,210
top secret and it puts out a four

00:05:00,849 --> 00:05:09,460
probability so this rest service is run

00:05:04,210 --> 00:05:11,530
in Scala Play and since SPARC is also in

00:05:09,460 --> 00:05:15,550
Scala you know we can just load there

00:05:11,530 --> 00:05:16,960
the models and do the prediction so no I

00:05:15,550 --> 00:05:21,759
think that's that's what you need to

00:05:16,960 --> 00:05:25,120
know about this slide in general what we

00:05:21,759 --> 00:05:27,970
use in our team is quite diverse so if

00:05:25,120 --> 00:05:30,910
we interview people it's kind of always

00:05:27,970 --> 00:05:32,650
difficult to to like ensure that they

00:05:30,910 --> 00:05:37,599
are full of this stuff but we actually

00:05:32,650 --> 00:05:39,280
cannot like as we cannot have fun people

00:05:37,599 --> 00:05:42,070
who know all of this but probably will

00:05:39,280 --> 00:05:44,530
ask for people no Scala because we write

00:05:42,070 --> 00:05:45,580
machine learning code in Scala and of

00:05:44,530 --> 00:05:47,590
course they should know machine learning

00:05:45,580 --> 00:05:50,080
but on top of this you know we use all

00:05:47,590 --> 00:05:52,840
this other stuff like Amazon AWS for

00:05:50,080 --> 00:05:55,120
deploying stuff Jenkins for production

00:05:52,840 --> 00:05:57,820
stuff spark is very important our set up

00:05:55,120 --> 00:05:59,860
and if we do experimentation we rely on

00:05:57,820 --> 00:06:06,220
our pies and so whatever the data

00:05:59,860 --> 00:06:08,979
science people prefer so this was this

00:06:06,220 --> 00:06:11,860
introduction role so now we come to the

00:06:08,979 --> 00:06:14,229
topic why we should monitor and I will

00:06:11,860 --> 00:06:17,050
first give you some rather abstract

00:06:14,229 --> 00:06:19,979
example which could happen like this in

00:06:17,050 --> 00:06:24,940
an online shop but it's not very

00:06:19,979 --> 00:06:26,860
attached to what we do but it could be

00:06:24,940 --> 00:06:29,159
that you are working in such a data

00:06:26,860 --> 00:06:31,780
science team in an online shop and

00:06:29,159 --> 00:06:34,330
there's someone coming to you and say I

00:06:31,780 --> 00:06:36,520
am deploy model for fraud detection in

00:06:34,330 --> 00:06:38,680
online shop so you should basically

00:06:36,520 --> 00:06:40,490
build this binary classification model

00:06:38,680 --> 00:06:43,639
that I just described and deployed

00:06:40,490 --> 00:06:45,949
production so there's different steps

00:06:43,639 --> 00:06:48,110
you have to do obviously so the first

00:06:45,949 --> 00:06:50,090
one is you have to somehow collect the

00:06:48,110 --> 00:06:51,949
training data you have done to train a

00:06:50,090 --> 00:06:54,080
model and finally you deploy to

00:06:51,949 --> 00:06:55,419
production so let's look at these steps

00:06:54,080 --> 00:06:59,690
in the bitchery

00:06:55,419 --> 00:07:03,770
so at least what we do we kind of

00:06:59,690 --> 00:07:06,979
collect our data training data in one

00:07:03,770 --> 00:07:08,810
central store which is MSO nostri so not

00:07:06,979 --> 00:07:13,220
sure I guess most of you know what it is

00:07:08,810 --> 00:07:15,139
it's like on a big key value store which

00:07:13,220 --> 00:07:17,330
is very cheap and you could just put a

00:07:15,139 --> 00:07:19,130
lot of data there so if you have done

00:07:17,330 --> 00:07:20,930
this online shop like solando you would

00:07:19,130 --> 00:07:23,090
run around through all the systems

00:07:20,930 --> 00:07:25,430
because you need features from

00:07:23,090 --> 00:07:28,550
everywhere and some of this data may be

00:07:25,430 --> 00:07:31,160
only accessible it locks because some

00:07:28,550 --> 00:07:32,840
some other team had a live system which

00:07:31,160 --> 00:07:34,909
produces these locks but doesn't write

00:07:32,840 --> 00:07:37,370
it to data base so you have to fetch

00:07:34,909 --> 00:07:39,650
this locks if you are a little bit more

00:07:37,370 --> 00:07:42,289
fortunate it may be in a database where

00:07:39,650 --> 00:07:45,320
you can just call this database or even

00:07:42,289 --> 00:07:46,699
a data warehouse so but actually what I

00:07:45,320 --> 00:07:49,070
really recommend is that you collect

00:07:46,699 --> 00:07:51,530
this kind of proactively because what we

00:07:49,070 --> 00:07:54,110
had in the initial stage is every time

00:07:51,530 --> 00:07:55,849
we learned a model we went just before

00:07:54,110 --> 00:07:57,800
learning to all the systems and try to

00:07:55,849 --> 00:08:00,710
extract the data but this did not really

00:07:57,800 --> 00:08:02,810
work well because the database is a live

00:08:00,710 --> 00:08:05,930
database right so you should not collect

00:08:02,810 --> 00:08:09,770
you in business high peak business time

00:08:05,930 --> 00:08:12,080
their data and the log data is maybe

00:08:09,770 --> 00:08:13,699
also not available so what we do we

00:08:12,080 --> 00:08:17,960
collected offline during the night for

00:08:13,699 --> 00:08:20,090
every day and put it to s3 good then you

00:08:17,960 --> 00:08:23,419
have some kind of dating training data

00:08:20,090 --> 00:08:25,580
set so you have to be if you have smart

00:08:23,419 --> 00:08:28,849
data scientists they will come up with

00:08:25,580 --> 00:08:31,550
features which would kind of indicate if

00:08:28,849 --> 00:08:33,800
this is fraud or not so one thing

00:08:31,550 --> 00:08:35,899
probably could be that someone who is a

00:08:33,800 --> 00:08:38,560
fraudster has some kind of a bot running

00:08:35,899 --> 00:08:41,270
who does all the orders for you right so

00:08:38,560 --> 00:08:44,990
maybe one good feature is the time to

00:08:41,270 --> 00:08:48,380
order so what is put here is just the

00:08:44,990 --> 00:08:50,000
time in seconds when someone comes to

00:08:48,380 --> 00:08:51,650
your homepage and then the Tremonti

00:08:50,000 --> 00:08:54,589
orders and

00:08:51,650 --> 00:08:56,720
if what in this example here I have

00:08:54,589 --> 00:08:59,300
several cases where they are like all

00:08:56,720 --> 00:09:01,190
these secret features and we use but one

00:08:59,300 --> 00:09:03,920
of them is time to order and you see

00:09:01,190 --> 00:09:06,470
like there's a fraud case here and it

00:09:03,920 --> 00:09:08,900
was only five seconds so maybe it was a

00:09:06,470 --> 00:09:10,790
bot because no one could probably order

00:09:08,900 --> 00:09:13,490
in five seconds

00:09:10,790 --> 00:09:17,870
and most not most of the not fraud cases

00:09:13,490 --> 00:09:19,850
are around between 120 and 300 but

00:09:17,870 --> 00:09:23,270
there's other one fraud case which for

00:09:19,850 --> 00:09:26,600
250 so maybe this is a good feature or

00:09:23,270 --> 00:09:29,120
not but just remember there is this kind

00:09:26,600 --> 00:09:33,410
of time to order feature may be helpful

00:09:29,120 --> 00:09:36,830
for our model so if we look now at this

00:09:33,410 --> 00:09:41,630
feature and we would just plot some kind

00:09:36,830 --> 00:09:44,480
of a histogram or kind of a empirical

00:09:41,630 --> 00:09:47,810
distribution of it yeah we would see if

00:09:44,480 --> 00:09:49,850
we just put plot this here with past and

00:09:47,810 --> 00:09:54,110
it would be somehow normal distributed

00:09:49,850 --> 00:09:56,420
around 200 seconds okay I mean sounds

00:09:54,110 --> 00:10:02,060
reasonable right so maybe most of the

00:09:56,420 --> 00:10:06,890
people use actually this time when we

00:10:02,060 --> 00:10:09,650
now go live there at leasts probably in

00:10:06,890 --> 00:10:11,450
most cases there will be another service

00:10:09,650 --> 00:10:14,779
which feeds you this features because

00:10:11,450 --> 00:10:17,480
you a kind of machine learning data

00:10:14,779 --> 00:10:19,400
science team and all this stuff that we

00:10:17,480 --> 00:10:21,860
collected before offline for training

00:10:19,400 --> 00:10:23,959
needs to be available life we would

00:10:21,860 --> 00:10:25,970
really fast so probably there's other

00:10:23,959 --> 00:10:27,860
team who sends you just exactly this

00:10:25,970 --> 00:10:31,550
data and then you just have to predict

00:10:27,860 --> 00:10:36,050
and put out this P fraud so that's how

00:10:31,550 --> 00:10:38,360
it works in our case and once we alive

00:10:36,050 --> 00:10:42,589
we get features X and over by different

00:10:38,360 --> 00:10:45,010
microservice in real time so well and

00:10:42,589 --> 00:10:47,450
mostly this team probably is just

00:10:45,010 --> 00:10:50,450
services built by a smart software

00:10:47,450 --> 00:10:54,410
engineers smart Big Data engineers but

00:10:50,450 --> 00:10:56,300
not part of your team so now we come to

00:10:54,410 --> 00:10:59,810
this monitoring part so you deploy this

00:10:56,300 --> 00:11:02,750
new model and X come in P fort goes out

00:10:59,810 --> 00:11:04,320
and now we all know microservices you

00:11:02,750 --> 00:11:08,460
have to monitor all right you have to

00:11:04,320 --> 00:11:11,970
look at stuff like CPU usage memory

00:11:08,460 --> 00:11:13,200
usage latency all this kind of classical

00:11:11,970 --> 00:11:16,200
stuff from the software engineering

00:11:13,200 --> 00:11:19,040
world and if we say this is all cooled

00:11:16,200 --> 00:11:22,470
and probably this thing works okay but

00:11:19,040 --> 00:11:23,550
that's only one part of the story at

00:11:22,470 --> 00:11:27,590
least if you have a machine learning

00:11:23,550 --> 00:11:31,070
model because then this code happens so

00:11:27,590 --> 00:11:34,200
some weeks later people are angry and

00:11:31,070 --> 00:11:35,730
they say tell you that you did not

00:11:34,200 --> 00:11:39,120
detect the fraud and the business is

00:11:35,730 --> 00:11:40,710
ruined okay so maybe one point about

00:11:39,120 --> 00:11:42,600
this what we have especially in our

00:11:40,710 --> 00:11:44,670
domain if you want to take payment fraud

00:11:42,600 --> 00:11:46,860
and there is as I told you earlier

00:11:44,670 --> 00:11:49,260
there's this like this delay so people

00:11:46,860 --> 00:11:52,290
have two weeks time to pay or not they

00:11:49,260 --> 00:11:54,750
kind of you're only will detect if they

00:11:52,290 --> 00:11:57,120
paid after two weeks right so if you're

00:11:54,750 --> 00:12:00,060
if something goes wrong in your model

00:11:57,120 --> 00:12:01,860
and it's producing now crappy

00:12:00,060 --> 00:12:03,870
predictions you will find out two weeks

00:12:01,860 --> 00:12:05,100
later if you have two labels right and

00:12:03,870 --> 00:12:09,270
in this time your business could be

00:12:05,100 --> 00:12:11,640
ruined so this is the problem we kind of

00:12:09,270 --> 00:12:14,130
want to tackle and now you start

00:12:11,640 --> 00:12:15,840
investigation because this business guy

00:12:14,130 --> 00:12:17,670
just comes to you and say something is

00:12:15,840 --> 00:12:19,140
really wrong look at your system and

00:12:17,670 --> 00:12:21,600
then you maybe do the same thing again

00:12:19,140 --> 00:12:26,220
that we just did and you say huh looks

00:12:21,600 --> 00:12:28,740
fine I don't know what happened but now

00:12:26,220 --> 00:12:30,930
comes this part where you not only look

00:12:28,740 --> 00:12:36,240
at this this classical stuff but you

00:12:30,930 --> 00:12:40,500
also could look at X and now we look at

00:12:36,240 --> 00:12:42,810
X and we now see these values right and

00:12:40,500 --> 00:12:44,970
they're kind of different to what we saw

00:12:42,810 --> 00:12:46,320
in the training data and all if we plot

00:12:44,970 --> 00:12:48,360
them now on this empirical distribution

00:12:46,320 --> 00:12:51,210
you also see like there's a different

00:12:48,360 --> 00:12:53,640
number which is now 200,000 so what

00:12:51,210 --> 00:12:56,400
happened the mean shifted from two

00:12:53,640 --> 00:12:58,440
hundred to two hundred thousand and then

00:12:56,400 --> 00:13:00,120
you go poby to this team who built this

00:12:58,440 --> 00:13:02,940
micro service feeding your prediction

00:13:00,120 --> 00:13:05,850
system and then you find out that the

00:13:02,940 --> 00:13:09,710
feature is not sent to us in militia in

00:13:05,850 --> 00:13:12,480
seconds but in milliseconds right so I

00:13:09,710 --> 00:13:14,190
think this is not really I did not

00:13:12,480 --> 00:13:15,600
really happen so far our T but I think

00:13:14,190 --> 00:13:17,220
it could happen because you know data

00:13:15,600 --> 00:13:18,000
scientists they may be thinking seconds

00:13:17,220 --> 00:13:20,800
and

00:13:18,000 --> 00:13:22,800
everything is milliseconds right but the

00:13:20,800 --> 00:13:25,950
obvious thing is now if this feature

00:13:22,800 --> 00:13:28,030
tells you that then all these

00:13:25,950 --> 00:13:30,790
predictions that you produced our

00:13:28,030 --> 00:13:32,650
garbage or could be garbage if this

00:13:30,790 --> 00:13:36,030
feature super important because now our

00:13:32,650 --> 00:13:39,700
model is like very sensitive to detect

00:13:36,030 --> 00:13:41,440
BOTS by saying if the small the time to

00:13:39,700 --> 00:13:43,030
order is very small then it's probably a

00:13:41,440 --> 00:13:45,220
thought so but it now gets feed all

00:13:43,030 --> 00:13:47,290
these values it virtual say ah no BOTS

00:13:45,220 --> 00:13:49,150
will like take like three hundred

00:13:47,290 --> 00:13:51,850
thousand seconds to order anything right

00:13:49,150 --> 00:13:53,980
maybe it's very undecided customer who

00:13:51,850 --> 00:13:58,120
spends a lot of time on our homepage so

00:13:53,980 --> 00:14:00,310
the the key point is here this is not

00:13:58,120 --> 00:14:03,250
good so there are several problems we

00:14:00,310 --> 00:14:05,440
lost we lost a lot of money and we did

00:14:03,250 --> 00:14:07,300
not detect it in time and we could have

00:14:05,440 --> 00:14:08,560
detected it in time and provided a fix

00:14:07,300 --> 00:14:11,290
if we looked at this feature

00:14:08,560 --> 00:14:14,920
distribution much earlier not only after

00:14:11,290 --> 00:14:16,600
two weeks so the conclusion of this is

00:14:14,920 --> 00:14:18,340
that we need to make sure that the

00:14:16,600 --> 00:14:20,790
distributions of input features are

00:14:18,340 --> 00:14:24,610
always the same as in the training data

00:14:20,790 --> 00:14:27,190
because we train our model on data with

00:14:24,610 --> 00:14:28,690
certain distributions and we only can

00:14:27,190 --> 00:14:30,310
rely on these predictions that they are

00:14:28,690 --> 00:14:33,340
really doing what we want if they would

00:14:30,310 --> 00:14:37,240
be same the same kind of data although

00:14:33,340 --> 00:14:40,510
in production and this brings me now to

00:14:37,240 --> 00:14:43,540
prediction monitoring so I will like now

00:14:40,510 --> 00:14:45,610
give rough overview and then in the

00:14:43,540 --> 00:14:48,840
second part log and will tell you some

00:14:45,610 --> 00:14:52,740
implementation details how we did it so

00:14:48,840 --> 00:14:58,060
the first thing we want to monitor is

00:14:52,740 --> 00:15:01,360
failing features so consider that once

00:14:58,060 --> 00:15:04,870
you have this feature time to time to

00:15:01,360 --> 00:15:07,630
order this could also be null a lot of

00:15:04,870 --> 00:15:10,510
time so we are living like in an

00:15:07,630 --> 00:15:12,160
uncertain world and this team that could

00:15:10,510 --> 00:15:13,900
collect these features and send it over

00:15:12,160 --> 00:15:16,000
to you they could also just put a null

00:15:13,900 --> 00:15:18,430
in this field and what would you do in

00:15:16,000 --> 00:15:20,650
your model you can not predict on a data

00:15:18,430 --> 00:15:22,570
point where there is a null so what is

00:15:20,650 --> 00:15:24,730
typically done you do some kind of

00:15:22,570 --> 00:15:27,070
imputation right so you take some kind

00:15:24,730 --> 00:15:29,890
of the median or average that you saw on

00:15:27,070 --> 00:15:33,430
training data but this will still

00:15:29,890 --> 00:15:35,529
kind of corrupt your prediction so one

00:15:33,430 --> 00:15:37,390
very important thing in our world is to

00:15:35,529 --> 00:15:40,329
monitor how often a feature fails and

00:15:37,390 --> 00:15:42,579
there's some kind of natural failing if

00:15:40,329 --> 00:15:45,339
it's only a few percent of predictions

00:15:42,579 --> 00:15:48,190
but if you see this list so if this time

00:15:45,339 --> 00:15:51,250
to prediction would be 90% not in all

00:15:48,190 --> 00:15:52,990
cases then you should go to this team

00:15:51,250 --> 00:15:57,839
that sends you this over and I should

00:15:52,990 --> 00:16:00,459
ask them why this is the case something

00:15:57,839 --> 00:16:03,279
now we come to the more elaborate thing

00:16:00,459 --> 00:16:05,050
is what you really should do and what

00:16:03,279 --> 00:16:06,930
what would have actually taken up this

00:16:05,050 --> 00:16:10,300
case I introduced earlier is if you

00:16:06,930 --> 00:16:13,540
compare the distributions of every

00:16:10,300 --> 00:16:15,459
feature between the test data and the

00:16:13,540 --> 00:16:19,390
live data so this picture just shows

00:16:15,459 --> 00:16:22,300
show each small picture is a feature and

00:16:19,390 --> 00:16:24,220
this is how it was distributed in tests

00:16:22,300 --> 00:16:26,730
or training and how was it distributed

00:16:24,220 --> 00:16:28,930
in life so I put their test data because

00:16:26,730 --> 00:16:30,220
you could also do it on training but

00:16:28,930 --> 00:16:32,230
typically the test data is the data

00:16:30,220 --> 00:16:34,930
where you're very confident because it's

00:16:32,230 --> 00:16:36,940
where you generated this performance

00:16:34,930 --> 00:16:39,519
measurements like area under a curve and

00:16:36,940 --> 00:16:41,290
thing and you want to behave your model

00:16:39,519 --> 00:16:43,300
in life like you saw it when you

00:16:41,290 --> 00:16:46,930
measured it right because this is where

00:16:43,300 --> 00:16:50,140
you said it's good so you would compare

00:16:46,930 --> 00:16:52,990
it on the test data and I have someone

00:16:50,140 --> 00:16:55,899
plot here which we actually generate and

00:16:52,990 --> 00:16:57,640
you have the three things so the red one

00:16:55,899 --> 00:16:59,410
is the live data the blue one is the

00:16:57,640 --> 00:17:03,670
test data and the green one is the Train

00:16:59,410 --> 00:17:06,459
data so now you see like actually in the

00:17:03,670 --> 00:17:11,169
live data there are much more values of

00:17:06,459 --> 00:17:12,970
point nine say or not so but what does

00:17:11,169 --> 00:17:16,030
this picture tell us is this is this

00:17:12,970 --> 00:17:17,679
still good or not and the answer is that

00:17:16,030 --> 00:17:22,650
there is no easy answer for this right

00:17:17,679 --> 00:17:26,260
so what we kind of did is we monitor

00:17:22,650 --> 00:17:28,150
continuously and we compare this

00:17:26,260 --> 00:17:32,710
distribution to each other's and they

00:17:28,150 --> 00:17:34,900
are like tons of come out of

00:17:32,710 --> 00:17:40,110
distribution comparisons in statistics

00:17:34,900 --> 00:17:43,660
one of them is a KS test which kind of

00:17:40,110 --> 00:17:45,700
compares the cdf slow and we'll say more

00:17:43,660 --> 00:17:47,410
about it but with this with how we

00:17:45,700 --> 00:17:49,660
implemented us we can really say okay

00:17:47,410 --> 00:17:51,580
this feature was looking and test like

00:17:49,660 --> 00:17:53,860
this in life it's looking like this and

00:17:51,580 --> 00:17:55,900
that's the difference this number is the

00:17:53,860 --> 00:17:58,210
difference between this distribution and

00:17:55,900 --> 00:18:02,380
if it was really high then it's really

00:17:58,210 --> 00:18:05,260
bad and depending on your business you

00:18:02,380 --> 00:18:08,500
have to define what is bad and what is

00:18:05,260 --> 00:18:13,480
good because there's no Universal answer

00:18:08,500 --> 00:18:15,010
to this so if somebody says okay no

00:18:13,480 --> 00:18:17,110
features really it's a little bit

00:18:15,010 --> 00:18:19,180
different but it's still okay there's no

00:18:17,110 --> 00:18:21,220
like business effect we saw in the past

00:18:19,180 --> 00:18:23,170
did not really suffer from this

00:18:21,220 --> 00:18:26,590
distribution maybe it's just kind of

00:18:23,170 --> 00:18:28,000
seasonality that's okay but this is the

00:18:26,590 --> 00:18:29,410
thing you have to stretch all these

00:18:28,000 --> 00:18:31,060
values and then you have to find out

00:18:29,410 --> 00:18:36,520
your own what is acceptable and what not

00:18:31,060 --> 00:18:38,800
and there are some other things so now

00:18:36,520 --> 00:18:41,590
imagine we have notice system which kind

00:18:38,800 --> 00:18:44,050
of compares even in certain time frames

00:18:41,590 --> 00:18:46,360
the current feature distribution to the

00:18:44,050 --> 00:18:48,250
test so there have kind of camera

00:18:46,360 --> 00:18:51,640
parameters you can tune and the first

00:18:48,250 --> 00:18:53,500
one is our big should be the size of

00:18:51,640 --> 00:18:54,970
your aggregation window right because if

00:18:53,500 --> 00:18:57,490
you want to do build this distribution

00:18:54,970 --> 00:19:01,090
block you have to window it somehow and

00:18:57,490 --> 00:19:03,520
count the number of occurrences and one

00:19:01,090 --> 00:19:07,120
could be you take it where is very small

00:19:03,520 --> 00:19:09,070
one then it means your I only look at

00:19:07,120 --> 00:19:11,380
one out last hour and compare this to

00:19:09,070 --> 00:19:13,210
the test distribution the good thing is

00:19:11,380 --> 00:19:16,450
this is like you can detect anomalies

00:19:13,210 --> 00:19:18,340
very fast but if you have seasonality in

00:19:16,450 --> 00:19:20,440
the data you know that people in an

00:19:18,340 --> 00:19:22,570
online shop behave differently in the

00:19:20,440 --> 00:19:24,490
morning than in the evening then you

00:19:22,570 --> 00:19:28,570
would get a lot of false positive alarms

00:19:24,490 --> 00:19:30,850
and maybe the other extreme so as you

00:19:28,570 --> 00:19:34,030
look at your affiliate gate data from

00:19:30,850 --> 00:19:36,850
the live system over 12 hours then you

00:19:34,030 --> 00:19:38,800
kind of aggregate out all this short

00:19:36,850 --> 00:19:41,800
term seasonality s at least during the

00:19:38,800 --> 00:19:43,900
day but you're slower of detecting

00:19:41,800 --> 00:19:46,240
anomalies because you notice the sliding

00:19:43,900 --> 00:19:49,960
window if there's an element in now the

00:19:46,240 --> 00:19:52,150
sliding window will slide over it and

00:19:49,960 --> 00:19:53,830
only if this anomaly makes most of the

00:19:52,150 --> 00:19:54,859
window you will see like a difference or

00:19:53,830 --> 00:19:59,279
something

00:19:54,859 --> 00:20:03,840
the same as yeah you often should you do

00:19:59,279 --> 00:20:05,940
this and this is kind of if we go live

00:20:03,840 --> 00:20:08,039
here and then you have this 12-hour

00:20:05,940 --> 00:20:10,350
window you could maybe do this you know

00:20:08,039 --> 00:20:13,200
every hour you could look back on the

00:20:10,350 --> 00:20:16,019
last 12 hours and it's also trade-off

00:20:13,200 --> 00:20:19,909
for this so though more often you do the

00:20:16,019 --> 00:20:23,009
more quicker you can define it come

00:20:19,909 --> 00:20:25,320
detect anomalies but it's very complex

00:20:23,009 --> 00:20:29,399
you know you have to it it costs money

00:20:25,320 --> 00:20:32,489
if you do it at least on AWS and then

00:20:29,399 --> 00:20:35,909
you have to do it more often so it's

00:20:32,489 --> 00:20:37,739
more complex and it's also kind of

00:20:35,909 --> 00:20:39,269
complex operation or you do it less

00:20:37,739 --> 00:20:40,859
often then you don't have to spend

00:20:39,269 --> 00:20:43,739
enough money but maybe you also have to

00:20:40,859 --> 00:20:45,659
steal a again so actually I think there

00:20:43,739 --> 00:20:47,070
are some approaches I think there was a

00:20:45,659 --> 00:20:49,019
twitter paper where you actually could

00:20:47,070 --> 00:20:50,759
really do this in kind of real-time so

00:20:49,019 --> 00:20:54,629
with every few data points they will

00:20:50,759 --> 00:20:58,350
kind of detect this anomaly but at least

00:20:54,629 --> 00:21:00,869
for our system this kind of approach

00:20:58,350 --> 00:21:03,239
works very well so we don't have to

00:21:00,869 --> 00:21:06,869
build a very complex system to master

00:21:03,239 --> 00:21:08,999
this okay and this live monitoring last

00:21:06,869 --> 00:21:12,299
slide for me is why why we do it and

00:21:08,999 --> 00:21:14,700
what we had in the past so the number

00:21:12,299 --> 00:21:16,379
one thing is this technical problem so

00:21:14,700 --> 00:21:18,029
the data that you get from other teams

00:21:16,379 --> 00:21:20,609
is just not the data that you thought

00:21:18,029 --> 00:21:22,350
your in test and this could be that they

00:21:20,609 --> 00:21:24,269
have a problem on their side collecting

00:21:22,350 --> 00:21:26,369
data that they have different units that

00:21:24,269 --> 00:21:28,950
they implemented maybe this data

00:21:26,369 --> 00:21:35,850
gathering feature different than you and

00:21:28,950 --> 00:21:38,009
for this it's very useful since I'm kind

00:21:35,850 --> 00:21:42,989
of out of time I will now give over to

00:21:38,009 --> 00:21:46,529
Lauren for the implementation part I

00:21:42,989 --> 00:21:48,690
will continue with going a bit more into

00:21:46,529 --> 00:21:53,399
detail of how we implemented the things

00:21:48,690 --> 00:21:55,679
that are at risk describe you so we

00:21:53,399 --> 00:21:58,139
start with a central problem of

00:21:55,679 --> 00:22:00,379
disappearance nation is how to measure

00:21:58,139 --> 00:22:03,840
the difference between two distributions

00:22:00,379 --> 00:22:06,359
automatically so in this picture we have

00:22:03,840 --> 00:22:08,310
two distribution represented as their

00:22:06,359 --> 00:22:12,090
probability density functions

00:22:08,310 --> 00:22:14,910
or histograms so we have the one which

00:22:12,090 --> 00:22:17,220
which is a blue and the yellow one so

00:22:14,910 --> 00:22:19,440
the blue one has a peak around the

00:22:17,220 --> 00:22:21,120
middle and the yellow one has two peaks

00:22:19,440 --> 00:22:23,880
on one to the left and one to the right

00:22:21,120 --> 00:22:27,360
so looking at this visually you can see

00:22:23,880 --> 00:22:29,160
that is quite different but doing

00:22:27,360 --> 00:22:32,160
something very simplistic like just

00:22:29,160 --> 00:22:34,860
taking the average the average would

00:22:32,160 --> 00:22:38,910
probably quite close to each other so we

00:22:34,860 --> 00:22:41,760
define this distance measure between the

00:22:38,910 --> 00:22:45,210
distributions which is just two

00:22:41,760 --> 00:22:47,760
integrals so the denominator the

00:22:45,210 --> 00:22:51,380
normalizing factor is just the area of

00:22:47,760 --> 00:22:56,880
everything like this overlap

00:22:51,380 --> 00:23:01,760
distributions and the numerator is the

00:22:56,880 --> 00:23:04,200
area of the parts which which differ so

00:23:01,760 --> 00:23:06,540
in practice we don't compute these

00:23:04,200 --> 00:23:09,210
integrals we just take sample points and

00:23:06,540 --> 00:23:12,510
we measure how big they are like for

00:23:09,210 --> 00:23:15,540
instance there there is an example of

00:23:12,510 --> 00:23:17,550
one one one difference and then we sum

00:23:15,540 --> 00:23:20,310
them up and normalized and to be between

00:23:17,550 --> 00:23:22,740
zero and one so if two distributions are

00:23:20,310 --> 00:23:24,750
completely identical then the distance

00:23:22,740 --> 00:23:32,910
will be zero if they are completely

00:23:24,750 --> 00:23:36,510
different a distance will be one so we

00:23:32,910 --> 00:23:39,210
do it like this except that we in we

00:23:36,510 --> 00:23:42,330
actually don't keep track of the of the

00:23:39,210 --> 00:23:45,660
histograms but we use the cumulative

00:23:42,330 --> 00:23:47,670
distribution functions so this is not a

00:23:45,660 --> 00:23:50,310
problem because cumulative distribution

00:23:47,670 --> 00:23:52,530
functions and probability density

00:23:50,310 --> 00:23:54,530
functions are kind of equivalent you can

00:23:52,530 --> 00:23:57,750
go from one to the other by

00:23:54,530 --> 00:24:00,540
differentiation or integration and the

00:23:57,750 --> 00:24:04,800
reason why we prefer cumulative

00:24:00,540 --> 00:24:09,120
distribution functions is that if you

00:24:04,800 --> 00:24:11,640
have the histogram and the most common

00:24:09,120 --> 00:24:12,900
implementation of histograms is that you

00:24:11,640 --> 00:24:16,560
would take the minimum and the maximum

00:24:12,900 --> 00:24:19,170
value that you see and then chop chop up

00:24:16,560 --> 00:24:21,700
the space into equal size buckets and

00:24:19,170 --> 00:24:24,300
then count how many things are there

00:24:21,700 --> 00:24:32,350
so if the maximum is pretty far out on

00:24:24,300 --> 00:24:34,870
your on your left then then most of the

00:24:32,350 --> 00:24:36,850
data will be just in in one or two

00:24:34,870 --> 00:24:40,840
buckets and it will not be very good so

00:24:36,850 --> 00:24:43,360
so the cumulative distribution function

00:24:40,840 --> 00:24:47,380
gives us kind of percentiles so what it

00:24:43,360 --> 00:24:51,130
what it tells us is that for certain

00:24:47,380 --> 00:24:53,920
value let's say zero how many how many

00:24:51,130 --> 00:24:57,100
of the values are smaller than it so is

00:24:53,920 --> 00:24:59,320
here we see about 40 percent of values

00:24:57,100 --> 00:25:01,810
are smaller than 0 and we we get this

00:24:59,320 --> 00:25:06,000
for every value and conversely we can

00:25:01,810 --> 00:25:09,820
see okay what is our sixty percentile or

00:25:06,000 --> 00:25:13,840
99 percent and so on so we can cut down

00:25:09,820 --> 00:25:17,890
like this some outliers so the

00:25:13,840 --> 00:25:20,800
disadvantage of using percentage is that

00:25:17,890 --> 00:25:25,480
percent is not so easy to compute like

00:25:20,800 --> 00:25:30,070
histogram but we computed use it just

00:25:25,480 --> 00:25:32,740
approximate approximately compute this

00:25:30,070 --> 00:25:34,420
percentile and for this we use very

00:25:32,740 --> 00:25:38,170
useful technique that is called tea

00:25:34,420 --> 00:25:40,810
digest so I have put here some scholars

00:25:38,170 --> 00:25:44,050
code just to show how easy it is to do

00:25:40,810 --> 00:25:46,780
this so first we just import that that

00:25:44,050 --> 00:25:50,260
library and then here we have two

00:25:46,780 --> 00:25:52,060
functions both create and the first one

00:25:50,260 --> 00:25:54,340
can create a tea digest which is a

00:25:52,060 --> 00:25:57,430
summary of a cumulative distribution

00:25:54,340 --> 00:25:59,800
function and it creates it from a in

00:25:57,430 --> 00:26:01,870
memory collection so we just get these

00:25:59,800 --> 00:26:05,080
numbers which is sequence of doubles and

00:26:01,870 --> 00:26:07,840
then we just create the T digestant and

00:26:05,080 --> 00:26:11,350
we just add each of the numbers there

00:26:07,840 --> 00:26:13,630
and we we have the we have the digit

00:26:11,350 --> 00:26:17,620
digest basically and from here we can

00:26:13,630 --> 00:26:20,770
get percentiles and and such and if the

00:26:17,620 --> 00:26:22,870
data is really big and we can store it

00:26:20,770 --> 00:26:25,570
as a distributed collection for instance

00:26:22,870 --> 00:26:28,030
inspark so we have this RTD of double

00:26:25,570 --> 00:26:30,970
and it's not much more complicated than

00:26:28,030 --> 00:26:32,650
than this the RTD will be will have

00:26:30,970 --> 00:26:35,080
several partitions and for each

00:26:32,650 --> 00:26:36,490
partitions we basically do this and

00:26:35,080 --> 00:26:38,409
and we have a lot of different tea

00:26:36,490 --> 00:26:43,169
digests so this happens in this

00:26:38,409 --> 00:26:46,120
segmental operator and then we we have a

00:26:43,169 --> 00:26:48,940
tea digest for every partition and to

00:26:46,120 --> 00:26:51,820
digest a nice property that it can be

00:26:48,940 --> 00:26:53,830
combined so all the digests of all

00:26:51,820 --> 00:27:00,720
partitions are combined to the final

00:26:53,830 --> 00:27:05,559
result in in the combining operation so

00:27:00,720 --> 00:27:08,139
now let's look into how the data is

00:27:05,559 --> 00:27:10,570
actually collected so before when we see

00:27:08,139 --> 00:27:12,399
when we saw how we we measured the

00:27:10,570 --> 00:27:16,840
distances we assumed we just have these

00:27:12,399 --> 00:27:18,639
distributions but this values of these

00:27:16,840 --> 00:27:20,200
features are collected from from

00:27:18,639 --> 00:27:23,830
production so this is a schematic

00:27:20,200 --> 00:27:26,080
overview of how our prediction service

00:27:23,830 --> 00:27:29,320
looks like so we have a rest service

00:27:26,080 --> 00:27:31,299
there is a prediction engine there there

00:27:29,320 --> 00:27:32,950
are some machine learning models which

00:27:31,299 --> 00:27:35,789
were trained previously and they are

00:27:32,950 --> 00:27:39,700
loaded from s3 so a request comes in

00:27:35,789 --> 00:27:42,730
then prediction is made and the answer

00:27:39,700 --> 00:27:45,309
is is sent out so this is working very

00:27:42,730 --> 00:27:47,620
well and if we want to build this data

00:27:45,309 --> 00:27:49,929
collection on top of it one thing we

00:27:47,620 --> 00:27:53,019
want to take care is that we don't mess

00:27:49,929 --> 00:27:58,120
up this system very much so we just add

00:27:53,019 --> 00:28:02,409
them some components to the to the right

00:27:58,120 --> 00:28:05,380
so we just add an S qsq and another

00:28:02,409 --> 00:28:08,110
process which collects the data so this

00:28:05,380 --> 00:28:10,809
if I'm not mistaken is also called

00:28:08,110 --> 00:28:12,909
something like lumberjack pattern so you

00:28:10,809 --> 00:28:15,639
log everything that you get into the

00:28:12,909 --> 00:28:18,429
system and that you respond so you get

00:28:15,639 --> 00:28:21,340
log all of this your request and the

00:28:18,429 --> 00:28:23,830
feature that you have computed and and

00:28:21,340 --> 00:28:27,250
then you send just send this off to an

00:28:23,830 --> 00:28:29,769
sqs queue the the reason is that this is

00:28:27,250 --> 00:28:32,710
not a critical process we are allowed to

00:28:29,769 --> 00:28:34,750
fail in collecting some of the data

00:28:32,710 --> 00:28:37,659
points here but we never want to be

00:28:34,750 --> 00:28:40,179
delayed or failed because this this go

00:28:37,659 --> 00:28:42,700
wrong goes wrong so we move this a

00:28:40,179 --> 00:28:44,669
collection and saving of the data logic

00:28:42,700 --> 00:28:47,350
to a different service to this

00:28:44,669 --> 00:28:53,590
lumberjack process

00:28:47,350 --> 00:28:58,300
and this process of collecting

00:28:53,590 --> 00:29:00,910
collecting the logs here I have we have

00:28:58,300 --> 00:29:03,010
simplified the code a bit that it can

00:29:00,910 --> 00:29:05,260
fit to a slide so by throwing away all

00:29:03,010 --> 00:29:08,290
the error handling and cleanup and other

00:29:05,260 --> 00:29:12,730
stuff but basically is it still captures

00:29:08,290 --> 00:29:15,490
the the main ideas so what we have here

00:29:12,730 --> 00:29:18,730
is a function this go code so we have

00:29:15,490 --> 00:29:21,430
this sqs interface then we have a dump

00:29:18,730 --> 00:29:24,070
size like how much how much locks we

00:29:21,430 --> 00:29:26,530
want to handle together then we have a

00:29:24,070 --> 00:29:28,870
channel on which the process can be

00:29:26,530 --> 00:29:31,000
interrupted and then we have like a

00:29:28,870 --> 00:29:34,120
callback function once we gathered

00:29:31,000 --> 00:29:36,250
enough locks this upload function will

00:29:34,120 --> 00:29:39,400
do something with it put it into a

00:29:36,250 --> 00:29:41,590
database or in s3 in our case so we

00:29:39,400 --> 00:29:43,510
start to initialize these buffers and to

00:29:41,590 --> 00:29:47,530
the timeout and then we have this

00:29:43,510 --> 00:29:49,540
infinite loop which rich on the

00:29:47,530 --> 00:29:52,600
interrupt channel if in case you get a

00:29:49,540 --> 00:29:57,130
message there it has to stop in case the

00:29:52,600 --> 00:30:00,400
time a timeout happens then also in that

00:29:57,130 --> 00:30:03,370
case we upload the data or otherwise we

00:30:00,400 --> 00:30:05,860
just read receive new messages from from

00:30:03,370 --> 00:30:08,260
the queue append it to the buffer and

00:30:05,860 --> 00:30:11,740
when the buffer reaches a certain size

00:30:08,260 --> 00:30:14,170
we just upload the data so what what do

00:30:11,740 --> 00:30:19,210
this upload function does in our case

00:30:14,170 --> 00:30:26,470
it's just it creates a file and then it

00:30:19,210 --> 00:30:29,080
it uploads it to s3 so since this week

00:30:26,470 --> 00:30:32,320
the World Cup on football starts I will

00:30:29,080 --> 00:30:34,720
also mention that our systems are called

00:30:32,320 --> 00:30:37,440
after football players and the name of

00:30:34,720 --> 00:30:43,990
this system is platinum

00:30:37,440 --> 00:30:47,020
so now putting it together in a SS data

00:30:43,990 --> 00:30:51,220
pipeline so then the pipeline looks like

00:30:47,020 --> 00:30:53,740
this so before this lumberjack process

00:30:51,220 --> 00:30:57,130
collected all the logs and put them on

00:30:53,740 --> 00:30:59,380
s3 and then periodically we we run this

00:30:57,130 --> 00:31:01,240
jobs which do these aggregations over

00:30:59,380 --> 00:31:02,890
let's say this 12

00:31:01,240 --> 00:31:05,050
our time window that Patrick showed

00:31:02,890 --> 00:31:07,120
before so we take the locks and there's

00:31:05,050 --> 00:31:09,940
some spark job which process this locks

00:31:07,120 --> 00:31:12,340
it groups it by the model so there are

00:31:09,940 --> 00:31:14,890
always several models in production

00:31:12,340 --> 00:31:17,320
running and then failed features are

00:31:14,890 --> 00:31:19,300
computed like how what percentage of

00:31:17,320 --> 00:31:21,460
feature is failed then we create this

00:31:19,300 --> 00:31:23,650
tea digest and we create the histograms

00:31:21,460 --> 00:31:26,290
and then we measure the distances

00:31:23,650 --> 00:31:31,540
between the different features and then

00:31:26,290 --> 00:31:34,960
we save this to like a common data

00:31:31,540 --> 00:31:38,350
format which can be used later to create

00:31:34,960 --> 00:31:41,380
some reports where you can look visually

00:31:38,350 --> 00:31:43,420
and compare or some some distributions

00:31:41,380 --> 00:31:47,590
and look at some tables of which

00:31:43,420 --> 00:31:50,650
features are missing or more useful we

00:31:47,590 --> 00:31:52,930
can directly do some alerts thresholds

00:31:50,650 --> 00:31:55,720
on something on the on the distance and

00:31:52,930 --> 00:31:57,970
make alerts because usually there is no

00:31:55,720 --> 00:32:01,690
time to look at a lot of this a lot of

00:31:57,970 --> 00:32:05,860
these reports so one lesson that we also

00:32:01,690 --> 00:32:08,410
learned is that is it was not good to to

00:32:05,860 --> 00:32:10,540
make this job create the report or the

00:32:08,410 --> 00:32:13,210
alerts directly because the thing about

00:32:10,540 --> 00:32:15,360
reports is is that when you look at them

00:32:13,210 --> 00:32:19,180
always you want to change something and

00:32:15,360 --> 00:32:23,200
measure something else and so on so so

00:32:19,180 --> 00:32:26,320
if we have an intermediate step then we

00:32:23,200 --> 00:32:27,730
can easily just change the reports how

00:32:26,320 --> 00:32:32,710
they look like what they show and

00:32:27,730 --> 00:32:35,290
everything so the final notes if you

00:32:32,710 --> 00:32:37,420
have a machine learning system in

00:32:35,290 --> 00:32:39,580
production you obviously have to monitor

00:32:37,420 --> 00:32:43,060
it somehow and this monitoring is

00:32:39,580 --> 00:32:45,310
especially important if the performance

00:32:43,060 --> 00:32:48,130
feedback comes with a large delay like

00:32:45,310 --> 00:32:50,770
in our case because you cannot just add

00:32:48,130 --> 00:32:52,630
at prediction performance and you have

00:32:50,770 --> 00:32:56,490
to find some ways of seeing if your

00:32:52,630 --> 00:33:00,310
system doesn't work beforehand so

00:32:56,490 --> 00:33:03,040
usually there is a lot of research and a

00:33:00,310 --> 00:33:06,100
lot of related work with which have a

00:33:03,040 --> 00:33:09,910
very very complex way of monitoring but

00:33:06,100 --> 00:33:12,940
it's better to just start simple and try

00:33:09,910 --> 00:33:14,629
to not interfere with production systems

00:33:12,940 --> 00:33:17,509
too much

00:33:14,629 --> 00:33:18,379
because it's no fun running this by hand

00:33:17,509 --> 00:33:20,600
all the time

00:33:18,379 --> 00:33:26,389
it's better to automate as much as

00:33:20,600 --> 00:33:28,820
possible and also if they are there

00:33:26,389 --> 00:33:30,710
start to come out some best practices

00:33:28,820 --> 00:33:32,899
around monitoring and if you want to

00:33:30,710 --> 00:33:35,840
measure how far you are then I recommend

00:33:32,899 --> 00:33:38,840
to answer the questions in this Google

00:33:35,840 --> 00:33:42,320
paper what's your ml test score so

00:33:38,840 --> 00:33:44,299
although we worked on it quite for quite

00:33:42,320 --> 00:33:46,490
a while I think we're only about halfway

00:33:44,299 --> 00:33:49,279
through so there's a lot more things to

00:33:46,490 --> 00:33:51,619
do so

00:33:49,279 --> 00:33:53,570
thank you very much I'm happy that you

00:33:51,619 --> 00:33:57,139
came in such a large number and I hope

00:33:53,570 --> 00:33:59,240
it is also useful for you so we are open

00:33:57,139 --> 00:34:02,499
to questions now or later in the coffee

00:33:59,240 --> 00:34:02,499
breaks Thanks

00:34:07,309 --> 00:34:19,109
any questions hi thank you very much so

00:34:17,579 --> 00:34:21,319
I'm wondering so I think it's great

00:34:19,109 --> 00:34:24,869
what are you doing and I are also taking

00:34:21,319 --> 00:34:27,629
initiatives to reduce this online

00:34:24,869 --> 00:34:29,429
offline serving skew in the organization

00:34:27,629 --> 00:34:31,470
away so because when I look at the

00:34:29,429 --> 00:34:32,940
slides you had like the data team could

00:34:31,470 --> 00:34:34,530
access everything like locks and the

00:34:32,940 --> 00:34:36,540
data warehouse and just grab feature

00:34:34,530 --> 00:34:38,639
somewhere and all the other code to

00:34:36,540 --> 00:34:40,770
deploy there to develop the real-time

00:34:38,639 --> 00:34:43,919
features scattered around the company so

00:34:40,770 --> 00:34:45,569
other efforts to solve the problem from

00:34:43,919 --> 00:34:47,639
the start and not from the end and

00:34:45,569 --> 00:34:50,159
looking what went wrong you know what I

00:34:47,639 --> 00:34:52,919
mean - like don't realize something or

00:34:50,159 --> 00:34:55,800
have a data with feature repository or

00:34:52,919 --> 00:34:59,309
something yeah I think it's a difficult

00:34:55,800 --> 00:35:02,670
question so it's easy to let's say

00:34:59,309 --> 00:35:07,140
easier to find out the problem where it

00:35:02,670 --> 00:35:13,770
happens but some team that creates some

00:35:07,140 --> 00:35:16,740
some data it's hard to foresee every way

00:35:13,770 --> 00:35:19,319
it will be used so maybe this code was

00:35:16,740 --> 00:35:21,950
written years ago and and our team maybe

00:35:19,319 --> 00:35:24,869
didn't even exist then so it's hard to

00:35:21,950 --> 00:35:27,540
like in theory this should be done but

00:35:24,869 --> 00:35:30,720
in practice is quite hard to pull this

00:35:27,540 --> 00:35:32,940
off so it's better to have each team

00:35:30,720 --> 00:35:35,760
especially because if when they work

00:35:32,940 --> 00:35:38,490
autonomously that each team monitor very

00:35:35,760 --> 00:35:42,059
well their system and their their use

00:35:38,490 --> 00:35:47,130
case is monitored so then hopefully

00:35:42,059 --> 00:35:49,230
we'll each the whole is also working

00:35:47,130 --> 00:35:52,460
well if the parts are working well yeah

00:35:49,230 --> 00:35:52,460
ok thank you

00:35:54,650 --> 00:35:57,859
more questions

00:35:58,130 --> 00:36:03,309
if not thank you very much for your

00:36:00,680 --> 00:36:03,309

YouTube URL: https://www.youtube.com/watch?v=h1ewJRLefhk


