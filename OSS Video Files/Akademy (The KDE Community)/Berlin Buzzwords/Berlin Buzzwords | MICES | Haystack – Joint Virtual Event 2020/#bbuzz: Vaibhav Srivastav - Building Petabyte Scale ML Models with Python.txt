Title: #bbuzz: Vaibhav Srivastav - Building Petabyte Scale ML Models with Python
Publication date: 2020-06-29
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/building-petabyte-scale-ml-models-python

Although building ML models on small/ toy data-set is easy, most production-grade problems involve massive datasets which current ML practices don’t scale to. In this talk, we cover how you can drastically increase the amount of data that your models can learn from using distributed data/ml pipes.

Detailed Description:

It can be difficult to figure out how to work with large data-sets (which do not fit in your RAM), even if you’re already comfortable with ML libraries/ APIs within python. Many questions immediately come up: Which library should I use, and why? What’s the difference between a “map-reduce” and a “task-graph”? What’s a partial fit function, and what format does it expect the data in? Is it okay for my training data to have more features than observations? What’s the appropriate machine learning model to use? And so on…

In this talk, we’ll answer all those questions, and more!

We’ll start by walking through the current distributed analytics (out-of-core learning) landscape in order to understand the pain-points and some solutions to this problem.

Here is a sketch of a system designed to achieve this goal (of building scalable ML models):

    1. a way to stream instances
    2. a way to extract features from instances
    3. an incremental algorithm

Then we’ll read a large dataset into Dask, Tensorflow (tf.data) & sklearn streaming, and immediately apply what we’ve learned about in last section. We’ll move on to the model building process, including a discussion of which model is most appropriate for the task. We’ll evaluate our model a few different ways, and then examine the model for greater insight into how the data is influencing its predictions. Finally, we’ll practice this entire workflow on a new dataset, and end with a discussion of which parts of the process are worth tuning for improved performance.

Detailed Outline:

    1. Intro to out-of-core learning
    2. Representing large datasets as instances
    3. Transforming data (in batches) – live code [3-5]
    4. Feature Engineering & Scaling
    5. Building and evaluating a model (on entire datasets)
    6. Practicing this workflow on another dataset
    7. Benchmark other libraries/ for OOC learning
    8. Questions and Answers

Key takeaway

By the end of the talk participants would know how to build petabyte scale ML models, beyond the shackles of conventional python libraries.

Participants would have a benchmarks and best case practices for building such ML models at scale.
Captions: 
	00:00:09,620 --> 00:00:14,510
everyone good evening the

00:00:12,509 --> 00:00:16,289
today to welcome by Bakshi

00:00:14,510 --> 00:00:19,019
so what

00:00:16,289 --> 00:00:20,579
data scientists with Deloitte and he

00:00:19,019 --> 00:00:22,800
will be talking to us about building

00:00:20,579 --> 00:00:29,599
petabyte scale machine learning models

00:00:22,800 --> 00:00:33,780
so on to you alright thanks a lot now

00:00:29,599 --> 00:00:36,629
good evening everyone today I'm gonna be

00:00:33,780 --> 00:00:39,239
talking to you about machine learning

00:00:36,629 --> 00:00:41,909
models with Python all right let me just

00:00:39,239 --> 00:00:44,640
quickly switch to where my screen is

00:00:41,909 --> 00:00:47,040
yeah before I get started first of all

00:00:44,640 --> 00:00:49,229
thank you so much to the boolean

00:00:47,040 --> 00:00:52,159
buzzwords organizing committee I know

00:00:49,229 --> 00:00:55,920
that the times are not that great and

00:00:52,159 --> 00:00:57,869
for you know pulling through this online

00:00:55,920 --> 00:00:59,879
conference it's amazing thank you so

00:00:57,869 --> 00:01:03,269
much hope I am able to justify my

00:00:59,879 --> 00:01:06,030
presence here now let me just give a

00:01:03,269 --> 00:01:07,590
quick walkthrough of Who I am I'm a data

00:01:06,030 --> 00:01:10,110
scientist I work with Deloitte

00:01:07,590 --> 00:01:12,780
Consulting here in India which means

00:01:10,110 --> 00:01:16,079
that right now it's it's it's 11 40 41

00:01:12,780 --> 00:01:18,060
p.m. my time and I architect machine

00:01:16,079 --> 00:01:19,649
learning workflows and products for

00:01:18,060 --> 00:01:22,800
Fortune Technology to end clients on

00:01:19,649 --> 00:01:24,510
Google cloud I've been I've been working

00:01:22,800 --> 00:01:26,820
in the realm of data or data science

00:01:24,510 --> 00:01:29,820
machine learning for about five to six

00:01:26,820 --> 00:01:31,320
years now and have been building large

00:01:29,820 --> 00:01:33,990
scale machine learning workflows for all

00:01:31,320 --> 00:01:36,240
these friends right now enough about me

00:01:33,990 --> 00:01:37,830
let's talk about the agenda right so

00:01:36,240 --> 00:01:40,229
what I'm going to be talking about today

00:01:37,830 --> 00:01:41,850
with you all is first of all why do we

00:01:40,229 --> 00:01:43,530
need distribute distributed machine

00:01:41,850 --> 00:01:45,960
learning right I mean all the code that

00:01:43,530 --> 00:01:48,090
you put out on your laptops works works

00:01:45,960 --> 00:01:50,130
fine right so what's the specific need

00:01:48,090 --> 00:01:52,530
of distributed machine learning within

00:01:50,130 --> 00:01:55,710
that realm what is out of coal machine

00:01:52,530 --> 00:01:59,159
learning right then we're gonna do a bit

00:01:55,710 --> 00:02:01,310
of deep dive in terms of how do we build

00:01:59,159 --> 00:02:04,740
a scalable machine learning worth you

00:02:01,310 --> 00:02:06,570
and then as as they always say that no

00:02:04,740 --> 00:02:08,549
talk is good without code so we're gonna

00:02:06,570 --> 00:02:10,470
do a bit of code walk through about how

00:02:08,549 --> 00:02:13,650
you can build scalable machine learning

00:02:10,470 --> 00:02:16,439
walktroughs workflows with with dusk and

00:02:13,650 --> 00:02:18,269
with tensorflow and give you ready to

00:02:16,439 --> 00:02:21,019
use code snippets which which which you

00:02:18,269 --> 00:02:23,760
can leverage in your you know

00:02:21,019 --> 00:02:25,499
experiments at your work or for your

00:02:23,760 --> 00:02:27,689
projects and so on right

00:02:25,499 --> 00:02:29,880
then we're gonna try and answer the the

00:02:27,689 --> 00:02:31,320
question of which one should you use

00:02:29,880 --> 00:02:32,970
since we're gonna be talking about two

00:02:31,320 --> 00:02:35,970
approaches and then we're gonna head for

00:02:32,970 --> 00:02:39,300
questions and answers all right without

00:02:35,970 --> 00:02:41,040
further ado now before we we we get into

00:02:39,300 --> 00:02:41,760
why do we need distributed machine

00:02:41,040 --> 00:02:43,470
learning

00:02:41,760 --> 00:02:46,680
let's then do a quick walkthrough of

00:02:43,470 --> 00:02:48,210
what machine learning is right so in a

00:02:46,680 --> 00:02:49,830
typical machine learning example you

00:02:48,210 --> 00:02:51,540
would have some data right for this

00:02:49,830 --> 00:02:54,690
particular use case let's let's take the

00:02:51,540 --> 00:02:58,380
example of predicting whether a person

00:02:54,690 --> 00:03:00,870
has their mask on or not right so my so

00:02:58,380 --> 00:03:02,880
my data would would basically be images

00:03:00,870 --> 00:03:04,500
of people who would have their mask on

00:03:02,880 --> 00:03:06,900
or who would not have their mask on

00:03:04,500 --> 00:03:08,910
right and so I basically have two

00:03:06,900 --> 00:03:11,610
glasses and given a person going through

00:03:08,910 --> 00:03:13,350
my lab through my camera I want to bring

00:03:11,610 --> 00:03:15,120
whether they have their mask on so mine

00:03:13,350 --> 00:03:16,290
so my task is a classification task I

00:03:15,120 --> 00:03:18,270
want to pacify whether they have their

00:03:16,290 --> 00:03:21,050
masks fall or not right and I have some

00:03:18,270 --> 00:03:23,310
images for that now what I'll do is I'll

00:03:21,050 --> 00:03:25,200
like in a typical machine learning task

00:03:23,310 --> 00:03:26,820
I built a machine learning model for it

00:03:25,200 --> 00:03:28,890
and I'll build a model for it it can be

00:03:26,820 --> 00:03:32,220
I can be a linear model it can be a

00:03:28,890 --> 00:03:34,260
Bayesian model can be a you know can be

00:03:32,220 --> 00:03:36,180
a neural network can be a nonlinear

00:03:34,260 --> 00:03:38,330
model and can pretty much be anything

00:03:36,180 --> 00:03:41,340
pick your favorite model for this right

00:03:38,330 --> 00:03:42,390
then the next part off of that machine

00:03:41,340 --> 00:03:45,720
learning workflow would would be

00:03:42,390 --> 00:03:47,820
defining a loss function right what this

00:03:45,720 --> 00:03:50,490
loss function typically would do is it

00:03:47,820 --> 00:03:52,620
would define the quality or like how

00:03:50,490 --> 00:03:55,800
accurate your your your model is at a

00:03:52,620 --> 00:03:58,590
given point of time right and and that

00:03:55,800 --> 00:04:00,990
can be through when it's searching into

00:03:58,590 --> 00:04:03,270
the into into the space of all of your

00:04:00,990 --> 00:04:05,970
data to find the exact equation which

00:04:03,270 --> 00:04:09,209
which defines your your data to its

00:04:05,970 --> 00:04:13,459
labels right and last but not least

00:04:09,209 --> 00:04:15,600
comes the optimization comes the

00:04:13,459 --> 00:04:16,980
optimization procedure right so in this

00:04:15,600 --> 00:04:20,160
particular case what we'll do is we

00:04:16,980 --> 00:04:23,010
optimize on minimizing the loss so the

00:04:20,160 --> 00:04:25,620
minimum the loss the the better by

00:04:23,010 --> 00:04:27,180
modular P now we haven't touched

00:04:25,620 --> 00:04:29,100
distributed machine learning up until

00:04:27,180 --> 00:04:30,900
now but this is how your basic machine

00:04:29,100 --> 00:04:33,000
learning workflow looks like right so

00:04:30,900 --> 00:04:34,770
you have some data you have a task it

00:04:33,000 --> 00:04:36,960
can be classification regression you'll

00:04:34,770 --> 00:04:39,360
have a modeling you know trying to do

00:04:36,960 --> 00:04:42,419
that task you have some sort of a loss

00:04:39,360 --> 00:04:43,560
function which would help your model

00:04:42,419 --> 00:04:45,900
converge to

00:04:43,560 --> 00:04:49,110
the best possible or the most accurate

00:04:45,900 --> 00:04:51,030
model that it can be given the hyper

00:04:49,110 --> 00:04:53,190
parameters that you define for it and

00:04:51,030 --> 00:04:56,900
they know then an optimization procedure

00:04:53,190 --> 00:05:00,690
to get to that to get to that global

00:04:56,900 --> 00:05:06,990
minima in the in the fastest possibility

00:05:00,690 --> 00:05:10,410
right now this was all this was all fine

00:05:06,990 --> 00:05:12,450
right we had we have my machine learning

00:05:10,410 --> 00:05:14,100
workflow and this works fine on my

00:05:12,450 --> 00:05:16,889
laptop this works fine on my washing

00:05:14,100 --> 00:05:19,950
machine but why do I need distributed

00:05:16,889 --> 00:05:21,870
machine learning right that's where

00:05:19,950 --> 00:05:23,669
these three components come in right

00:05:21,870 --> 00:05:25,710
typically you would need distribute

00:05:23,669 --> 00:05:28,710
machine learning when you have massive

00:05:25,710 --> 00:05:30,870
data scale right so building a machine

00:05:28,710 --> 00:05:34,650
learning model on say an iris dataset

00:05:30,870 --> 00:05:37,229
which has about 200 odd rows is quite

00:05:34,650 --> 00:05:39,419
easy on your laptop or your workstation

00:05:37,229 --> 00:05:41,280
right because it can easily fit in your

00:05:39,419 --> 00:05:43,620
realm right and you can easily build

00:05:41,280 --> 00:05:45,300
your model on your ram itself right but

00:05:43,620 --> 00:05:47,160
what if you're dealing with a different

00:05:45,300 --> 00:05:49,740
problem so for example if we if we take

00:05:47,160 --> 00:05:53,700
our example further what if you have

00:05:49,740 --> 00:05:56,789
images of people from from airports from

00:05:53,700 --> 00:05:58,050
metro stations from there on the streets

00:05:56,789 --> 00:05:59,850
and you're trying to predict whether

00:05:58,050 --> 00:06:02,039
they have their mask on or not right

00:05:59,850 --> 00:06:04,310
that can be terabytes of data but that

00:06:02,039 --> 00:06:06,960
data would not fit on your particular

00:06:04,310 --> 00:06:09,450
laptop or on your particular machine

00:06:06,960 --> 00:06:11,880
reading cluster right because you've got

00:06:09,450 --> 00:06:14,190
huge feeder scale so typically when you

00:06:11,880 --> 00:06:15,539
have large data scale that's when that's

00:06:14,190 --> 00:06:18,600
when you need distributed machine

00:06:15,539 --> 00:06:22,169
learning right a second s massive model

00:06:18,600 --> 00:06:25,919
scales right so ever since the vgg 16

00:06:22,169 --> 00:06:29,639
model in in image or the GPT models in

00:06:25,919 --> 00:06:31,260
NLP there's like almost every other week

00:06:29,639 --> 00:06:33,600
or almost every other month you would

00:06:31,260 --> 00:06:35,340
see that that the scale of them of the

00:06:33,600 --> 00:06:37,139
or rather the size of the machine

00:06:35,340 --> 00:06:39,180
learning model has been increasing

00:06:37,139 --> 00:06:41,760
drastically right in fact couple of

00:06:39,180 --> 00:06:46,889
weeks back open air released their GPT

00:06:41,760 --> 00:06:47,310
three language model which has 3.7

00:06:46,889 --> 00:06:49,710
billion

00:06:47,310 --> 00:06:52,140
parameters right I mean that model would

00:06:49,710 --> 00:06:54,390
not fit in in lekha

00:06:52,140 --> 00:06:56,520
in your memory itself just for inference

00:06:54,390 --> 00:06:56,650
let alone be able to train or like you

00:06:56,520 --> 00:06:59,259
know

00:06:56,650 --> 00:07:01,030
fine given it further so typically when

00:06:59,259 --> 00:07:03,550
you're dealing with large model

00:07:01,030 --> 00:07:06,310
architectures or large neural networks

00:07:03,550 --> 00:07:07,750
for you to be able to train those you

00:07:06,310 --> 00:07:09,310
need a distributed machine learning

00:07:07,750 --> 00:07:10,990
workflow right

00:07:09,310 --> 00:07:13,240
third is is there something which is

00:07:10,990 --> 00:07:15,070
applicable across the board right so

00:07:13,240 --> 00:07:17,949
this is something that you would need

00:07:15,070 --> 00:07:19,780
for for efficient computation of your

00:07:17,949 --> 00:07:21,970
algorithms right so say for example if

00:07:19,780 --> 00:07:25,419
you're building a neural network right

00:07:21,970 --> 00:07:27,910
so we all know that gradient descent for

00:07:25,419 --> 00:07:31,240
you know we can use gradient descent for

00:07:27,910 --> 00:07:34,060
finding the best possible parameters for

00:07:31,240 --> 00:07:35,289
your model right but that but that's a

00:07:34,060 --> 00:07:37,720
brute-force approach right so that's

00:07:35,289 --> 00:07:39,280
gonna take a lot of time but similarly

00:07:37,720 --> 00:07:41,850
you can also use two plastic region in

00:07:39,280 --> 00:07:45,430
the center right which is which is

00:07:41,850 --> 00:07:47,650
insanely first and manifold times better

00:07:45,430 --> 00:07:49,419
than gradient descent in terms of

00:07:47,650 --> 00:07:51,639
getting the output fine gradient descent

00:07:49,419 --> 00:07:54,090
would be accurate but it's faster right

00:07:51,639 --> 00:07:56,710
so how can we make these algorithms

00:07:54,090 --> 00:08:00,220
efficient for their computation rate or

00:07:56,710 --> 00:08:02,199
rather how how can we how do we be able

00:08:00,220 --> 00:08:05,050
to distribute the task itself for these

00:08:02,199 --> 00:08:07,360
algorithms but them to run as fast as

00:08:05,050 --> 00:08:09,720
possible right so this is why we need

00:08:07,360 --> 00:08:12,460
distributed machine learning right now

00:08:09,720 --> 00:08:14,710
now that we know why we need distributed

00:08:12,460 --> 00:08:17,770
machine learning let's talk about out of

00:08:14,710 --> 00:08:22,060
Coromant right so out of core ml as the

00:08:17,770 --> 00:08:25,389
as the as the name suggests it's it's

00:08:22,060 --> 00:08:26,800
basically machine learning which machine

00:08:25,389 --> 00:08:31,750
running which runs at a scale where in

00:08:26,800 --> 00:08:34,270
your data is is is of higher order as

00:08:31,750 --> 00:08:36,940
compared to your core itself or meaning

00:08:34,270 --> 00:08:38,770
your now so it's basically an algorithm

00:08:36,940 --> 00:08:41,320
which exploits your external storage

00:08:38,770 --> 00:08:44,350
that can be a hard disk or your buffer

00:08:41,320 --> 00:08:46,480
buffer memory in order to support large

00:08:44,350 --> 00:08:49,360
data volumes that cannot be supported by

00:08:46,480 --> 00:08:51,910
primary memory right so now let's let's

00:08:49,360 --> 00:08:56,380
spend a minute on this wait 1 out of 4

00:08:51,910 --> 00:08:57,640
ml in like a typical machine learning in

00:08:56,380 --> 00:08:59,529
like a typical machine learning world

00:08:57,640 --> 00:09:01,300
would would Venus that you take your

00:08:59,529 --> 00:09:03,700
data set in our case let's take the

00:09:01,300 --> 00:09:06,220
images of people whether they have their

00:09:03,700 --> 00:09:08,920
masks on or off and you batch those

00:09:06,220 --> 00:09:09,290
images rate you batch those images to to

00:09:08,920 --> 00:09:11,600
say

00:09:09,290 --> 00:09:13,720
which is just enough to fit in your ramp

00:09:11,600 --> 00:09:17,000
it in your in your primary memory itself

00:09:13,720 --> 00:09:20,120
and then you then you take that data

00:09:17,000 --> 00:09:22,460
right and then you basically do a

00:09:20,120 --> 00:09:24,500
partial fit on your model right and then

00:09:22,460 --> 00:09:27,730
you you give your model more data and

00:09:24,500 --> 00:09:29,540
let it update its its its weights

00:09:27,730 --> 00:09:31,460
continuity is the time you've exhausted

00:09:29,540 --> 00:09:34,490
on the data right that's what out of

00:09:31,460 --> 00:09:37,430
poor ml is and why it is novel whatever

00:09:34,490 --> 00:09:40,670
why it's useful is say for example if

00:09:37,430 --> 00:09:41,510
you have a laptop which which has eight

00:09:40,670 --> 00:09:42,890
gigs of RAM

00:09:41,510 --> 00:09:44,750
or if you have a virtual machine which

00:09:42,890 --> 00:09:47,690
has eight gigs of RAM and your data size

00:09:44,750 --> 00:09:50,030
is 16 gigabytes in that case as well you

00:09:47,690 --> 00:09:51,230
can chunk your data in such a way and

00:09:50,030 --> 00:09:53,870
you can create your feature engineering

00:09:51,230 --> 00:09:56,840
pipeline in such a way that you would

00:09:53,870 --> 00:09:57,320
not require to fit all the data in one

00:09:56,840 --> 00:09:59,540
go

00:09:57,320 --> 00:10:00,770
that's why out of 4 ml is is good and

00:09:59,540 --> 00:10:05,210
we're going to be doing a deep type of

00:10:00,770 --> 00:10:07,310
this we do not right now going back to

00:10:05,210 --> 00:10:09,080
our going back to our example right how

00:10:07,310 --> 00:10:10,820
would this out of core ml fit into the

00:10:09,080 --> 00:10:14,540
example that we that we spoke about

00:10:10,820 --> 00:10:17,600
right in the in in the use case that we

00:10:14,540 --> 00:10:20,210
were talking about out of poor ml would

00:10:17,600 --> 00:10:23,450
would would look like something wherein

00:10:20,210 --> 00:10:25,520
I I split my data into two partitions

00:10:23,450 --> 00:10:27,290
right so there is one data set partition

00:10:25,520 --> 00:10:29,630
one then there is a data set partition

00:10:27,290 --> 00:10:31,880
to write my tasks still remains the same

00:10:29,630 --> 00:10:33,740
which is to classify except now my data

00:10:31,880 --> 00:10:35,750
set has reduced and this is being

00:10:33,740 --> 00:10:38,000
operated on two different nodes right

00:10:35,750 --> 00:10:40,070
which can communicate with each other

00:10:38,000 --> 00:10:41,630
again my model still remains the same

00:10:40,070 --> 00:10:44,180
this can be say for example for our

00:10:41,630 --> 00:10:46,220
image classification tasks we can assume

00:10:44,180 --> 00:10:49,490
it sir it's a CNN type architecture

00:10:46,220 --> 00:10:51,680
photo module right now there's a slight

00:10:49,490 --> 00:10:53,600
update in my architecture here what's

00:10:51,680 --> 00:10:55,340
changed is the loss function so if you

00:10:53,600 --> 00:11:00,020
see on the left side the loss function

00:10:55,340 --> 00:11:01,970
now runs from I equal to 1 to M by 2 M

00:11:00,020 --> 00:11:04,430
by 2 is basically half of my data right

00:11:01,970 --> 00:11:07,220
similarly on the other side it runs from

00:11:04,430 --> 00:11:10,070
I equal to M by 2 plus 1 to M which is

00:11:07,220 --> 00:11:11,690
like half of the data set is this side

00:11:10,070 --> 00:11:14,390
and half the data set on the other side

00:11:11,690 --> 00:11:17,030
right and also what has happened is my

00:11:14,390 --> 00:11:18,560
optimization role has also updated right

00:11:17,030 --> 00:11:20,510
so what what's happened to my

00:11:18,560 --> 00:11:22,390
optimization doing now is that it

00:11:20,510 --> 00:11:25,540
basically optimizes on

00:11:22,390 --> 00:11:28,350
loss of tasks one which has the aricept

00:11:25,540 --> 00:11:32,080
partition one and also for the loss of

00:11:28,350 --> 00:11:34,450
data set partition two right and then it

00:11:32,080 --> 00:11:36,220
sounds it up and and and in that way

00:11:34,450 --> 00:11:37,960
because all of these are communicating

00:11:36,220 --> 00:11:40,270
with each other you can distribute the

00:11:37,960 --> 00:11:42,220
tasks across you know two nodes or three

00:11:40,270 --> 00:11:47,140
nodes or four nodes and still be able to

00:11:42,220 --> 00:11:49,780
get to the best possible solution cool

00:11:47,140 --> 00:11:51,880
now that was that was a lot of curium

00:11:49,780 --> 00:11:53,740
then in in in past ten minutes right

00:11:51,880 --> 00:11:55,690
let's talk about how this would look

00:11:53,740 --> 00:11:57,670
like in real life right so there are

00:11:55,690 --> 00:11:59,800
there are two ways you can you can build

00:11:57,670 --> 00:12:01,420
out of poor ml or basically scale a

00:11:59,800 --> 00:12:04,480
scalable machine learning more closely

00:12:01,420 --> 00:12:06,280
the first is by using incremental models

00:12:04,480 --> 00:12:09,250
right incremental models or something

00:12:06,280 --> 00:12:11,560
which would support chunking of data

00:12:09,250 --> 00:12:14,620
right in a typical scikit-learn lingo

00:12:11,560 --> 00:12:16,570
you you would basically do a partial fit

00:12:14,620 --> 00:12:18,220
on your model so this is basically your

00:12:16,570 --> 00:12:22,600
stochastic gradient descent progresses

00:12:18,220 --> 00:12:26,440
your random forests your the the neural

00:12:22,600 --> 00:12:27,790
net within the second library and and

00:12:26,440 --> 00:12:29,380
couple moves there's an exhaustive list

00:12:27,790 --> 00:12:31,510
if you search incremental models on

00:12:29,380 --> 00:12:34,030
cyclone you would find that in fact I'll

00:12:31,510 --> 00:12:35,620
link it in the presentation as well so

00:12:34,030 --> 00:12:37,300
as soon as you find it you can get a

00:12:35,620 --> 00:12:39,940
list of all the second compliant

00:12:37,300 --> 00:12:42,220
incremental models so what you'll have

00:12:39,940 --> 00:12:43,990
to do with with model or partial fit is

00:12:42,220 --> 00:12:46,060
you have to create your custom logic

00:12:43,990 --> 00:12:48,760
which chunks your data into small small

00:12:46,060 --> 00:12:50,980
sizes and then feeds it into the into

00:12:48,760 --> 00:12:52,540
the model right and then with each first

00:12:50,980 --> 00:12:54,310
the model will start learning about your

00:12:52,540 --> 00:12:57,640
behaviors of the data right so that's

00:12:54,310 --> 00:12:59,290
that's the end of the most recommended

00:12:57,640 --> 00:13:01,210
way if you already have a machine

00:12:59,290 --> 00:13:02,830
learning workflow built in like a legacy

00:13:01,210 --> 00:13:06,460
fashion and you want to update it to

00:13:02,830 --> 00:13:09,790
work on more data you know like on

00:13:06,460 --> 00:13:10,930
larger data set size the second thing is

00:13:09,790 --> 00:13:12,640
is something we'll generally recommend

00:13:10,930 --> 00:13:14,590
to people who are building new workflows

00:13:12,640 --> 00:13:17,080
now right or starting a project from

00:13:14,590 --> 00:13:19,600
scratch is to build and define a draft

00:13:17,080 --> 00:13:21,670
base training flow right so if you

00:13:19,600 --> 00:13:25,030
typically use template ends of logo or

00:13:21,670 --> 00:13:27,910
PI torch or MX net or you know tasks ml

00:13:25,030 --> 00:13:29,380
or or like any new deep learning library

00:13:27,910 --> 00:13:31,870
that that keeps from popping up every

00:13:29,380 --> 00:13:34,360
other week what they would generally do

00:13:31,870 --> 00:13:35,860
is they would build a directed acyclic

00:13:34,360 --> 00:13:38,380
graph

00:13:35,860 --> 00:13:41,530
of your entire data your pre-processing

00:13:38,380 --> 00:13:43,510
jobs your you know your your business

00:13:41,530 --> 00:13:44,920
rules and pretty much any functions any

00:13:43,510 --> 00:13:46,060
classes that you give it'll create a

00:13:44,920 --> 00:13:48,340
graph out of it

00:13:46,060 --> 00:13:50,440
now what this graph does is it does not

00:13:48,340 --> 00:13:52,270
load everything into memory first right

00:13:50,440 --> 00:13:54,220
what it would do is it would save the

00:13:52,270 --> 00:13:56,160
references of your functions your

00:13:54,220 --> 00:13:59,200
classes your data itself your patches

00:13:56,160 --> 00:14:01,120
your dictionaries whatever you gift into

00:13:59,200 --> 00:14:03,610
that graphic it would save the reference

00:14:01,120 --> 00:14:05,380
of it right and what it would do is it

00:14:03,610 --> 00:14:07,270
would take that graph and when it is

00:14:05,380 --> 00:14:09,430
executing and when it's at a particular

00:14:07,270 --> 00:14:12,250
node of execution then it would load

00:14:09,430 --> 00:14:14,800
those references into memory right so in

00:14:12,250 --> 00:14:20,740
in that sense it is very easy to use

00:14:14,800 --> 00:14:22,690
these these graph based training jobs to

00:14:20,740 --> 00:14:25,990
use these to use these graph based

00:14:22,690 --> 00:14:28,300
training jobs and you know build your

00:14:25,990 --> 00:14:30,100
machine learning workflow and it's very

00:14:28,300 --> 00:14:31,930
easy to replicate the same flow on on

00:14:30,100 --> 00:14:33,640
multiple nodes so what we were talking

00:14:31,930 --> 00:14:36,250
about before you can have two nodes

00:14:33,640 --> 00:14:38,820
learning the same graph object and then

00:14:36,250 --> 00:14:41,110
converge their results together right so

00:14:38,820 --> 00:14:42,820
in just about a site or two we're going

00:14:41,110 --> 00:14:47,650
to be doing a bit of code walkthrough of

00:14:42,820 --> 00:14:50,200
both of these approaches now again I

00:14:47,650 --> 00:14:51,730
spoke about these two scenarios again

00:14:50,200 --> 00:14:53,110
I'm going to rush pass through them

00:14:51,730 --> 00:14:54,490
scenario one is something where you

00:14:53,110 --> 00:14:55,990
already have a machine learning workflow

00:14:54,490 --> 00:15:00,460
probably written in scikit-learn or a

00:14:55,990 --> 00:15:01,570
starts modules you know job and scenario

00:15:00,460 --> 00:15:03,550
two is where you were building a new

00:15:01,570 --> 00:15:05,530
experiment flow from scratch like how do

00:15:03,550 --> 00:15:06,730
you tackle these two scenarios and you

00:15:05,530 --> 00:15:10,630
build a scalable machine learning

00:15:06,730 --> 00:15:11,740
workflow which can take you and build a

00:15:10,630 --> 00:15:13,600
flow which can which is very much

00:15:11,740 --> 00:15:17,470
scalable and be able to take as much

00:15:13,600 --> 00:15:20,860
weight as it throughout it so for the

00:15:17,470 --> 00:15:23,140
for the for the older for the older or

00:15:20,860 --> 00:15:25,360
old legacy machine learning workflows

00:15:23,140 --> 00:15:28,420
my recommendation would be to use tasks

00:15:25,360 --> 00:15:32,640
tasks is is again like a distributed

00:15:28,420 --> 00:15:34,870
analytics pipeline natively in Python

00:15:32,640 --> 00:15:37,060
which is written natively in Python and

00:15:34,870 --> 00:15:40,150
and helps you create batches of data you

00:15:37,060 --> 00:15:43,090
can think of it as analogous to pandas

00:15:40,150 --> 00:15:45,580
except this is scalable pandas and it

00:15:43,090 --> 00:15:47,260
can automatically help you in defining

00:15:45,580 --> 00:15:49,990
your optimization strategies your data

00:15:47,260 --> 00:15:53,480
distribution strategies and

00:15:49,990 --> 00:15:56,000
make it worthwhile and actually make it

00:15:53,480 --> 00:15:58,190
make it faster for you to build models

00:15:56,000 --> 00:16:01,870
which are out of code right and ask has

00:15:58,190 --> 00:16:03,980
ties with pandas task has very very nice

00:16:01,870 --> 00:16:06,140
compatibility with scikit-learn and

00:16:03,980 --> 00:16:08,540
starts models and all of these packages

00:16:06,140 --> 00:16:11,420
so you can mix the two you can use tusk

00:16:08,540 --> 00:16:14,330
pre-processing your your you know your

00:16:11,420 --> 00:16:17,690
data and then you scikit-learn do well

00:16:14,330 --> 00:16:19,210
your models itself right now comes now

00:16:17,690 --> 00:16:22,550
towards the deep learning module side

00:16:19,210 --> 00:16:23,870
you can use like my personal favorite is

00:16:22,550 --> 00:16:25,220
the sense of law and we're going to be

00:16:23,870 --> 00:16:30,020
doing like a quick walkthrough of that

00:16:25,220 --> 00:16:31,580
to spin in a bit and but I wouldn't

00:16:30,020 --> 00:16:33,140
restrict you to just tensorflow you can

00:16:31,580 --> 00:16:34,700
use tensor flow you can use MX net you

00:16:33,140 --> 00:16:37,430
can use PI torch all three of those have

00:16:34,700 --> 00:16:40,910
a very robust ecosystem so feel free to

00:16:37,430 --> 00:16:42,770
use that right all right now enough talk

00:16:40,910 --> 00:16:48,200
let's let's let's talk about some code

00:16:42,770 --> 00:16:50,180
right let me just so just just a quick

00:16:48,200 --> 00:16:51,890
heads up all the code that I'm going to

00:16:50,180 --> 00:16:53,420
be working through would are being

00:16:51,890 --> 00:16:55,460
uploaded as a collab notebook and you

00:16:53,420 --> 00:16:57,890
can access those notebooks through the

00:16:55,460 --> 00:17:00,170
links given in the presentation I try

00:16:57,890 --> 00:17:02,450
and share those on the slack chart as

00:17:00,170 --> 00:17:04,850
well so that you can get access to it as

00:17:02,450 --> 00:17:06,829
early as possible but yeah so you don't

00:17:04,850 --> 00:17:08,209
have to worry about taking screenshots

00:17:06,829 --> 00:17:11,300
and stuff like that you can easily get

00:17:08,209 --> 00:17:15,170
access to those so I actually have this

00:17:11,300 --> 00:17:17,300
open let's first talk about building

00:17:15,170 --> 00:17:17,780
distributed training workflows with

00:17:17,300 --> 00:17:20,420
tensorflow

00:17:17,780 --> 00:17:22,520
so this is the this is actually a collab

00:17:20,420 --> 00:17:24,530
notebook and they uploaded it into into

00:17:22,520 --> 00:17:26,449
cable goodness so that you can actually

00:17:24,530 --> 00:17:27,920
get like the best sense of it I would

00:17:26,449 --> 00:17:32,240
recommend you doing the same thing as

00:17:27,920 --> 00:17:35,360
well and you know try and run this so

00:17:32,240 --> 00:17:36,650
cool so when we talk about distributed

00:17:35,360 --> 00:17:37,130
training with tens of you right so

00:17:36,650 --> 00:17:38,750
tensorflow

00:17:37,130 --> 00:17:42,440
natively provides you couple of

00:17:38,750 --> 00:17:44,480
distribution strategies right so these

00:17:42,440 --> 00:17:46,910
are these are neatly packed in tf2 or

00:17:44,480 --> 00:17:48,560
distributed strategy api with intensive

00:17:46,910 --> 00:17:50,570
flow rate in this particular example

00:17:48,560 --> 00:17:52,430
what we're going to be using is we're

00:17:50,570 --> 00:17:55,310
going to be using TF for distributed

00:17:52,430 --> 00:17:56,780
mirrored strategy right now let's let's

00:17:55,310 --> 00:17:58,880
talk about this mirrored strategy for

00:17:56,780 --> 00:18:01,010
for half a minute here right so what

00:17:58,880 --> 00:18:03,230
mirrored strategy does is whatever tens

00:18:01,010 --> 00:18:06,049
of your code that you write write it

00:18:03,230 --> 00:18:08,840
replicated across all the GPUs or all

00:18:06,049 --> 00:18:11,270
the nodes that you have connected with

00:18:08,840 --> 00:18:13,520
your with a node on which you're running

00:18:11,270 --> 00:18:15,020
right so it would replicate the same

00:18:13,520 --> 00:18:17,360
code that you're running on multiple

00:18:15,020 --> 00:18:20,030
machines right and then it would run

00:18:17,360 --> 00:18:22,460
those right on different chunks of data

00:18:20,030 --> 00:18:24,530
and then eventually the the context

00:18:22,460 --> 00:18:26,690
manager would take all the outputs from

00:18:24,530 --> 00:18:28,220
these nodes and then give you the final

00:18:26,690 --> 00:18:30,500
output that's how the distribution

00:18:28,220 --> 00:18:33,350
strategy works if you are able to see my

00:18:30,500 --> 00:18:37,370
screen you will I have the distributed

00:18:33,350 --> 00:18:39,950
training tensorflow guide open right

00:18:37,370 --> 00:18:41,390
there are about six to ten different

00:18:39,950 --> 00:18:42,980
training strategies you can see here

00:18:41,390 --> 00:18:45,200
there is multi worker mirrored strategy

00:18:42,980 --> 00:18:47,210
there's just me leechers PPO

00:18:45,200 --> 00:18:48,530
clustered resolver strategy and so on

00:18:47,210 --> 00:18:50,450
there are there are a lot of it and you

00:18:48,530 --> 00:18:53,720
can also define your own strategy as

00:18:50,450 --> 00:18:56,059
well so again the link to this is is in

00:18:53,720 --> 00:18:57,460
the notebook so you can easily check

00:18:56,059 --> 00:19:00,410
that out as well

00:18:57,460 --> 00:19:02,780
all right I see that I have very less

00:19:00,410 --> 00:19:04,429
time left so I'm just gonna zoom pass

00:19:02,780 --> 00:19:06,919
this again so what we're going to be

00:19:04,429 --> 00:19:08,720
doing here is using the middle TF dot

00:19:06,919 --> 00:19:11,630
distribute mirrored strategy we're going

00:19:08,720 --> 00:19:13,640
to be training a very simple Kiera's or

00:19:11,630 --> 00:19:16,630
more dense flow 2.0 tea party dress

00:19:13,640 --> 00:19:18,980
model right and we're going to be using

00:19:16,630 --> 00:19:20,840
fashion MLS theta self weight for all

00:19:18,980 --> 00:19:22,490
those of you who do not know fashion MLS

00:19:20,840 --> 00:19:25,130
dataset this is something which soul

00:19:22,490 --> 00:19:28,010
and/or research came up with and this is

00:19:25,130 --> 00:19:32,960
basically set basically a data set of

00:19:28,010 --> 00:19:35,090
70,000 images and there are 10 broad

00:19:32,960 --> 00:19:37,610
categories right these categories can be

00:19:35,090 --> 00:19:39,650
shirts can be socks can be bells can be

00:19:37,610 --> 00:19:42,410
you know skirts or trousers and so on

00:19:39,650 --> 00:19:44,860
and so forth it you can look look it up

00:19:42,410 --> 00:19:47,210
like what all categories are there and

00:19:44,860 --> 00:19:48,590
this is basically one step ahead the

00:19:47,210 --> 00:19:50,540
empliciti knows and everyone uses MS

00:19:48,590 --> 00:19:52,130
data sets but the clothing apparel one

00:19:50,540 --> 00:19:55,100
has has quite a lot of challenges

00:19:52,130 --> 00:19:56,870
associated with it right so that's the

00:19:55,100 --> 00:19:57,919
data set that we're trying to do where

00:19:56,870 --> 00:20:00,380
we're going to build like a

00:19:57,919 --> 00:20:01,970
classification image reciprocation model

00:20:00,380 --> 00:20:04,610
hopefully a very simple model like that

00:20:01,970 --> 00:20:08,020
and see how that how that would look

00:20:04,610 --> 00:20:08,020
like you know typical

00:20:09,630 --> 00:20:16,410
sorry in a in a typical distributed

00:20:11,970 --> 00:20:17,700
fashion so here are my you know normal

00:20:16,410 --> 00:20:21,240
imports so I have my dense probate

00:20:17,700 --> 00:20:24,470
assets I have my tens of 200 STF you can

00:20:21,240 --> 00:20:27,990
see that I'm using tf2 point-to-point oh

00:20:24,470 --> 00:20:31,559
then this this MLS data set is something

00:20:27,990 --> 00:20:34,950
which is already in the which is already

00:20:31,559 --> 00:20:37,830
in the TF data sets itself so we load

00:20:34,950 --> 00:20:39,539
that and I define my distribution

00:20:37,830 --> 00:20:41,580
strategy here which is TF run

00:20:39,539 --> 00:20:43,650
distributed minute strategy way so again

00:20:41,580 --> 00:20:45,240
like note that whatever is being added

00:20:43,650 --> 00:20:47,309
into like your typical intensive low

00:20:45,240 --> 00:20:48,840
flow rate so up until now from whatever

00:20:47,309 --> 00:20:50,070
way you would write a simple tense of

00:20:48,840 --> 00:20:53,039
the pipeline the only thing that is

00:20:50,070 --> 00:20:57,690
added is ta protein distributed minute

00:20:53,039 --> 00:20:59,970
strategy so I then you set up your your

00:20:57,690 --> 00:21:02,130
input pipeline this is again simple you

00:20:59,970 --> 00:21:05,940
you have your trained examples you have

00:21:02,130 --> 00:21:07,799
your test examples right and then you

00:21:05,940 --> 00:21:11,100
have a couple of like like a very simple

00:21:07,799 --> 00:21:12,330
pre-processing pre-processing step where

00:21:11,100 --> 00:21:15,559
in your you essentially scaling your

00:21:12,330 --> 00:21:19,260
images back to the scale of 0 to 1 and

00:21:15,559 --> 00:21:21,990
you can this is being done by the scale

00:21:19,260 --> 00:21:23,400
function then over here you're

00:21:21,990 --> 00:21:26,100
essentially just applying that scale

00:21:23,400 --> 00:21:28,620
function using a map function and we

00:21:26,100 --> 00:21:30,510
create a very simple model another thing

00:21:28,620 --> 00:21:32,390
to note there is this new line adder

00:21:30,510 --> 00:21:36,150
which is which strategy dot scope so

00:21:32,390 --> 00:21:39,679
this tells my my my tensor flow directed

00:21:36,150 --> 00:21:41,669
acyclic graph that all the model

00:21:39,679 --> 00:21:46,830
everything that's written within the

00:21:41,669 --> 00:21:48,240
model everything that's defined as model

00:21:46,830 --> 00:21:50,610
is something which comes in the scope of

00:21:48,240 --> 00:21:51,840
my distributed strategy right so that's

00:21:50,610 --> 00:21:53,429
that that's something that's changed

00:21:51,840 --> 00:21:54,780
right and my model is quite simple I

00:21:53,429 --> 00:21:56,669
have a convolutional there I have a

00:21:54,780 --> 00:21:58,169
maximum layer I have a fattening layer I

00:21:56,669 --> 00:21:59,520
have a dense layer and then I have a

00:21:58,169 --> 00:22:02,190
dense then layer because of any losses

00:21:59,520 --> 00:22:04,380
right you can see that analogous to our

00:22:02,190 --> 00:22:06,200
example my loss function is a sparse

00:22:04,380 --> 00:22:09,120
categorical cross entropy

00:22:06,200 --> 00:22:10,679
Louis and I'm using an atom optimizer so

00:22:09,120 --> 00:22:12,419
pretty much boilerplate code

00:22:10,679 --> 00:22:13,710
nothing nothing nothing fancy here

00:22:12,419 --> 00:22:15,419
everything's saying the only thing

00:22:13,710 --> 00:22:18,059
that's been added is the strategy voted

00:22:15,419 --> 00:22:21,809
right and again I'm just defining these

00:22:18,059 --> 00:22:22,880
callbacks this is the tensor board is

00:22:21,809 --> 00:22:24,770
something that I

00:22:22,880 --> 00:22:26,510
works with tidal but when you run it on

00:22:24,770 --> 00:22:28,340
your own laptops this would run quite

00:22:26,510 --> 00:22:31,730
fine so you would be able to see all the

00:22:28,340 --> 00:22:33,710
losses the accuracy and the and and all

00:22:31,730 --> 00:22:35,870
the other metrics at unifying onto the

00:22:33,710 --> 00:22:37,970
drain support itself then we define

00:22:35,870 --> 00:22:40,460
model check point this is to save the

00:22:37,970 --> 00:22:42,260
model after every epoch and we define a

00:22:40,460 --> 00:22:45,590
learning rate scaleable this is

00:22:42,260 --> 00:22:48,590
basically to be able to expedite the way

00:22:45,590 --> 00:22:51,410
I train my model so I will have you know

00:22:48,590 --> 00:22:53,030
a higher learning rate at the start and

00:22:51,410 --> 00:22:56,900
at like the first epoch so that I can

00:22:53,030 --> 00:22:58,940
jump to to do my prove the minimum as

00:22:56,900 --> 00:23:02,060
fast as possible and then I can reduce

00:22:58,940 --> 00:23:03,860
the learning rate right so again pretty

00:23:02,060 --> 00:23:06,620
much simple boilerplate code you can you

00:23:03,860 --> 00:23:08,600
can see it here and then we do train and

00:23:06,620 --> 00:23:10,010
evaluate so this I've done this

00:23:08,600 --> 00:23:12,260
benchmark when you run it on your laptop

00:23:10,010 --> 00:23:14,480
right on one like a very simple laptop

00:23:12,260 --> 00:23:15,980
for EPOC takes about a minute or two

00:23:14,480 --> 00:23:18,140
right but when I'm running on a

00:23:15,980 --> 00:23:21,200
distributed platform using the mirrored

00:23:18,140 --> 00:23:25,160
strategy you can you can see that it's

00:23:21,200 --> 00:23:27,470
taking 18 seconds per you know right and

00:23:25,160 --> 00:23:29,930
let's not go into the accuracy and so on

00:23:27,470 --> 00:23:31,340
it's giving like 98 0.62 percent

00:23:29,930 --> 00:23:33,380
accuracy but that's fine that's that's

00:23:31,340 --> 00:23:38,450
more of like how the architecture is

00:23:33,380 --> 00:23:40,640
defined and and so on right let's step

00:23:38,450 --> 00:23:42,860
ahead and then you can see that I have

00:23:40,640 --> 00:23:45,860
all my models kind of put into this

00:23:42,860 --> 00:23:48,140
checkpoint folder so I have my

00:23:45,860 --> 00:23:52,460
checkpoint 1 2 3 4 5 6 on the meter line

00:23:48,140 --> 00:23:54,590
right and again since my model is

00:23:52,460 --> 00:23:56,360
already there I can quickly pick it up

00:23:54,590 --> 00:23:59,750
and load the model and also evaluate it

00:23:56,360 --> 00:24:02,480
right so up until now I've already saved

00:23:59,750 --> 00:24:04,750
my model am able to load it so you don't

00:24:02,480 --> 00:24:07,460
have to come up with some sort of fancy

00:24:04,750 --> 00:24:10,220
you know code because you you had

00:24:07,460 --> 00:24:11,270
distributed training environment you

00:24:10,220 --> 00:24:12,590
don't have to change anything in your

00:24:11,270 --> 00:24:15,530
instance but your influence still

00:24:12,590 --> 00:24:20,690
remains the same right and it works out

00:24:15,530 --> 00:24:23,860
fine however if I go here if you if you

00:24:20,690 --> 00:24:26,690
want to have like a like a distributed

00:24:23,860 --> 00:24:28,640
evaluation as well right you should you

00:24:26,690 --> 00:24:30,440
you you can do the same thing with the

00:24:28,640 --> 00:24:32,450
same strategy right you define the same

00:24:30,440 --> 00:24:36,450
strategy school and you would be able

00:24:32,450 --> 00:24:39,690
together you would be able to get a

00:24:36,450 --> 00:24:42,120
a distributed influenced environment as

00:24:39,690 --> 00:24:43,710
well right so I know that I rush through

00:24:42,120 --> 00:24:48,780
a lot of code and again I'm going to

00:24:43,710 --> 00:24:51,420
give the I'm going to send the the link

00:24:48,780 --> 00:24:53,490
to this notebook out to you but yeah

00:24:51,420 --> 00:24:55,230
this was just to showcase like how a

00:24:53,490 --> 00:24:57,540
very simple distributed strategy can be

00:24:55,230 --> 00:24:59,220
applied to your you know normal tensor

00:24:57,540 --> 00:25:01,860
flow code which is towards three lines

00:24:59,220 --> 00:25:05,190
of code you should be able to build a

00:25:01,860 --> 00:25:09,360
highly scalable workflow without a lot

00:25:05,190 --> 00:25:10,920
of effort right now I know that I'm kind

00:25:09,360 --> 00:25:15,780
of pressed by time but I'm going to do

00:25:10,920 --> 00:25:20,190
like a quick walkthrough of how the how

00:25:15,780 --> 00:25:23,280
the task process works like right again

00:25:20,190 --> 00:25:25,320
just a just a quick check quick

00:25:23,280 --> 00:25:28,320
checkpoint here what we're going to be

00:25:25,320 --> 00:25:30,510
doing in the second let me take a step

00:25:28,320 --> 00:25:34,020
back so in the tensor for example what

00:25:30,510 --> 00:25:36,360
my main aim of talking over there was

00:25:34,020 --> 00:25:38,580
that how do you build machine learning

00:25:36,360 --> 00:25:41,520
workflows if you want to do if you were

00:25:38,580 --> 00:25:43,560
to solve say a deep learning problem

00:25:41,520 --> 00:25:45,780
right but what if you you have like a

00:25:43,560 --> 00:25:48,510
normal statistical problem right how

00:25:45,780 --> 00:25:50,970
would you build a workflow for that and

00:25:48,510 --> 00:25:52,650
earlier on we decided that you know we

00:25:50,970 --> 00:25:54,330
can open through how tasks and cycle

00:25:52,650 --> 00:25:55,470
don't kind of go with each other so in

00:25:54,330 --> 00:25:58,080
this example we're going to be talking

00:25:55,470 --> 00:26:00,810
about how tasks and XJ boost and kind of

00:25:58,080 --> 00:26:02,190
work together right so again dusk just a

00:26:00,810 --> 00:26:05,220
quick refresher on tasks is nothing but

00:26:02,190 --> 00:26:08,610
a large panel dieter frame you know

00:26:05,220 --> 00:26:13,560
library and it sort of helps you you

00:26:08,610 --> 00:26:17,280
know build larger than memory you know

00:26:13,560 --> 00:26:20,810
larger than memory data frames and kind

00:26:17,280 --> 00:26:23,400
of help you in in process it goes right

00:26:20,810 --> 00:26:25,170
these are all on all the same inputs

00:26:23,400 --> 00:26:26,940
just a quick thing again so dusk

00:26:25,170 --> 00:26:28,620
gifts like when you run into dust coat

00:26:26,940 --> 00:26:30,900
it will give you like a nice dashboard

00:26:28,620 --> 00:26:32,420
so you can see how your workers and

00:26:30,900 --> 00:26:34,620
cores and your memory is being utilized

00:26:32,420 --> 00:26:34,860
when you're running your dashboard as

00:26:34,620 --> 00:26:38,220
well

00:26:34,860 --> 00:26:40,350
okay again for this exercise but using

00:26:38,220 --> 00:26:41,370
the New York City taxi fare prediction

00:26:40,350 --> 00:26:42,750
so essentially it's Excel regression

00:26:41,370 --> 00:26:45,720
problem and we're trying to predict the

00:26:42,750 --> 00:26:48,300
fare of a New York City Taxi based on

00:26:45,720 --> 00:26:50,509
their baby so multiple features like

00:26:48,300 --> 00:26:54,379
what is their source what is their

00:26:50,509 --> 00:26:57,619
target and so on now just a quick check

00:26:54,379 --> 00:27:01,129
if I was to talk about if I was to talk

00:26:57,619 --> 00:27:03,320
about using tasks how would you be able

00:27:01,129 --> 00:27:05,659
to take a mean of first 1000 numbers and

00:27:03,320 --> 00:27:08,539
ask that would be something like this

00:27:05,659 --> 00:27:13,309
you do you create a numpy array of n P

00:27:08,539 --> 00:27:16,219
dot range 1000 or 10,000 and you you

00:27:13,309 --> 00:27:19,369
basically put it into tasks using d dot

00:27:16,219 --> 00:27:21,049
from array and you do why not mean not

00:27:19,369 --> 00:27:23,089
compute right and this isn't technically

00:27:21,049 --> 00:27:25,849
fast you were able to get the get the

00:27:23,089 --> 00:27:29,239
results and you know less than a second

00:27:25,849 --> 00:27:36,949
right similarly you can see that that

00:27:29,239 --> 00:27:39,619
there were about there what about 5.54 I

00:27:36,949 --> 00:27:43,069
think about 55 million rows and it was

00:27:39,619 --> 00:27:44,779
able to you know get those into into

00:27:43,069 --> 00:27:46,999
into a directory in about twenty five

00:27:44,779 --> 00:27:49,759
point seven seven seconds let's try and

00:27:46,999 --> 00:27:52,669
load this into into my trailer frame for

00:27:49,759 --> 00:27:54,229
for about fifty five million rows pandas

00:27:52,669 --> 00:27:56,929
would just not work you can see that I

00:27:54,229 --> 00:27:59,659
have the pandas line here this just did

00:27:56,929 --> 00:28:01,849
not work right my turn will crashed my

00:27:59,659 --> 00:28:03,139
computer crashed and so on so this would

00:28:01,849 --> 00:28:05,179
just not work pandas would just not be

00:28:03,139 --> 00:28:07,909
able to take this much memory in but

00:28:05,179 --> 00:28:10,759
task was able to devote it right in just

00:28:07,909 --> 00:28:12,529
90.7 milliseconds it was able to get all

00:28:10,759 --> 00:28:16,819
the references of data and it was able

00:28:12,529 --> 00:28:18,709
to get this data right now again over

00:28:16,819 --> 00:28:20,719
here we're basically defining through

00:28:18,709 --> 00:28:22,699
the data frame itself for defining what

00:28:20,719 --> 00:28:24,769
should be my train types my desk types

00:28:22,699 --> 00:28:28,489
defining all of those I'm defining my

00:28:24,769 --> 00:28:31,009
training columns my you know I'm

00:28:28,489 --> 00:28:35,269
defining a bounding box in terms of you

00:28:31,009 --> 00:28:37,369
know where my where my city is and where

00:28:35,269 --> 00:28:42,469
my sources where my destination is and

00:28:37,369 --> 00:28:45,109
so on right and then I have couple of

00:28:42,469 --> 00:28:46,489
functions in terms of how do I load my

00:28:45,109 --> 00:28:49,819
data so what I'll call them so I want to

00:28:46,489 --> 00:28:52,309
take what what fraction of those rows do

00:28:49,819 --> 00:28:54,169
I do I would do I know just to millions

00:28:52,309 --> 00:28:58,069
and so on so we you can configure all of

00:28:54,169 --> 00:29:02,209
this through and then we note that eat

00:28:58,069 --> 00:29:03,499
away and then you can see that the -

00:29:02,209 --> 00:29:04,190
data frame was able to get everything

00:29:03,499 --> 00:29:07,399
you

00:29:04,190 --> 00:29:09,710
doneita frame and although fair amounts

00:29:07,399 --> 00:29:12,529
what we have to predict the date time

00:29:09,710 --> 00:29:14,539
the longitude the latitude drop-off pick

00:29:12,529 --> 00:29:16,580
up one of these these things are a kind

00:29:14,539 --> 00:29:20,240
of ended a skater frame itself right and

00:29:16,580 --> 00:29:23,059
again I can't stress on this enough like

00:29:20,240 --> 00:29:25,850
being able to do this on a 55 Mill Road

00:29:23,059 --> 00:29:27,559
data set in like just couple of seconds

00:29:25,850 --> 00:29:29,000
is magical because if you were to do it

00:29:27,559 --> 00:29:32,299
in pandas you would just not be able to

00:29:29,000 --> 00:29:34,789
do right and then you basically create

00:29:32,299 --> 00:29:37,340
your extra boost model and over here

00:29:34,789 --> 00:29:40,039
we're using extra boost and and ask

00:29:37,340 --> 00:29:41,779
actually post and this is the same way

00:29:40,039 --> 00:29:43,970
that you build your model and I see that

00:29:41,779 --> 00:29:45,260
there's an error

00:29:43,970 --> 00:29:47,890
all right I'm not going to get into the

00:29:45,260 --> 00:29:51,400
error right now but I fix it before I

00:29:47,890 --> 00:29:54,080
shared this long with you but

00:29:51,400 --> 00:29:56,030
essentially using the same code and

00:29:54,080 --> 00:29:58,750
using just the extreme post model you

00:29:56,030 --> 00:30:01,610
would be able to create a scalable

00:29:58,750 --> 00:30:03,800
feature processing pipeline and be able

00:30:01,610 --> 00:30:04,880
to you know push it into XT post and and

00:30:03,800 --> 00:30:07,880
get results out of it

00:30:04,880 --> 00:30:09,410
now I have just one slide to to cover so

00:30:07,880 --> 00:30:13,430
I'm just going to quickly head over back

00:30:09,410 --> 00:30:18,530
to my slides and again put both the pod

00:30:13,430 --> 00:30:20,360
snippets are are on the are on the links

00:30:18,530 --> 00:30:22,130
in the slides and we can go through it

00:30:20,360 --> 00:30:24,380
now you may ask the question that which

00:30:22,130 --> 00:30:26,150
one should I use should I use the task

00:30:24,380 --> 00:30:29,060
scikit-learn combo should I use the

00:30:26,150 --> 00:30:31,070
tensorflow and and like the PI torch one

00:30:29,060 --> 00:30:32,750
the answer to that that is that there

00:30:31,070 --> 00:30:35,690
have actually no solutions they are just

00:30:32,750 --> 00:30:37,550
play toss like so it depends on it

00:30:35,690 --> 00:30:40,100
depends on what your use cases what your

00:30:37,550 --> 00:30:41,810
problem is I I personally use a lot of

00:30:40,100 --> 00:30:44,360
tasks like a flow and combo I also use

00:30:41,810 --> 00:30:46,130
ten flow for my tea planning book as

00:30:44,360 --> 00:30:49,040
well but the way I defined my

00:30:46,130 --> 00:30:51,140
distinction is that I use whenever I

00:30:49,040 --> 00:30:52,490
have to build a simple circuit or model

00:30:51,140 --> 00:30:54,350
I would use tasks along with

00:30:52,490 --> 00:30:56,300
scikit-learn to build O's but if I have

00:30:54,350 --> 00:30:58,940
to build something which is probably

00:30:56,300 --> 00:31:00,440
with uses image processing or nasha and

00:30:58,940 --> 00:31:04,460
processing I would build it again

00:31:00,440 --> 00:31:07,160
tensorflow using K after theta with that

00:31:04,460 --> 00:31:09,290
thank you so much I know I rushed

00:31:07,160 --> 00:31:11,930
through a lot of content but I hope this

00:31:09,290 --> 00:31:13,280
was helpful to you you can you can tweet

00:31:11,930 --> 00:31:15,470
out your questions to me at or eat

00:31:13,280 --> 00:31:17,660
underscore VB on Twitter and you can

00:31:15,470 --> 00:31:18,830
find more about my talks on their blog

00:31:17,660 --> 00:31:20,390
as well

00:31:18,830 --> 00:31:23,650
and if you have any questions we can

00:31:20,390 --> 00:31:27,010
take those now great thanks for both

00:31:23,650 --> 00:31:29,510
very interesting pointers and insights

00:31:27,010 --> 00:31:32,270
guys if you have any questions please

00:31:29,510 --> 00:31:35,300
ask in the slack channel maybe we still

00:31:32,270 --> 00:31:37,670
have a couple of minutes actually I have

00:31:35,300 --> 00:31:40,850
a question myself so when you talk about

00:31:37,670 --> 00:31:42,320
trade-offs so when we talk about you

00:31:40,850 --> 00:31:44,000
know distributing that's a machine

00:31:42,320 --> 00:31:46,430
learning workload so something we have

00:31:44,000 --> 00:31:49,100
been using is to you spark

00:31:46,430 --> 00:31:51,770
do you have some thoughts on how sparkle

00:31:49,100 --> 00:31:55,730
pairs with the desk or is it also it

00:31:51,770 --> 00:31:57,530
depends now so actually actually spark

00:31:55,730 --> 00:31:59,680
is a very good option

00:31:57,530 --> 00:32:03,440
right spark has very nice CENTAC

00:31:59,680 --> 00:32:06,320
integrations with with you know spark ml

00:32:03,440 --> 00:32:09,980
and and and also with tensorflow

00:32:06,320 --> 00:32:12,320
in in certain silos right but but the

00:32:09,980 --> 00:32:13,940
reason where I digress from spark is

00:32:12,320 --> 00:32:14,330
generally when you write the spark code

00:32:13,940 --> 00:32:16,670
right

00:32:14,330 --> 00:32:19,460
maybe me in my spark or in Scala for

00:32:16,670 --> 00:32:21,890
your pre-processing and so on it becomes

00:32:19,460 --> 00:32:23,690
quite clunky because whenever you you're

00:32:21,890 --> 00:32:25,310
building your pipelines you can find you

00:32:23,690 --> 00:32:28,520
can put it into production quite easily

00:32:25,310 --> 00:32:31,910
right but it's native interactions with

00:32:28,520 --> 00:32:34,070
quite a lot of you know well built up

00:32:31,910 --> 00:32:37,550
libraries within Python like you know pi

00:32:34,070 --> 00:32:40,070
torch or with MX net and so on becomes

00:32:37,550 --> 00:32:41,030
quite quite difficult right so you would

00:32:40,070 --> 00:32:42,890
you would have to introduce some

00:32:41,030 --> 00:32:44,390
middleware in between your spark P

00:32:42,890 --> 00:32:48,410
processing pipeline and your module

00:32:44,390 --> 00:32:50,600
itself so again I a lot of phones you

00:32:48,410 --> 00:32:52,460
spark for their machine learning models

00:32:50,600 --> 00:32:54,530
and they put it as well they also will

00:32:52,460 --> 00:32:56,750
spark streaming jobs as well to put

00:32:54,530 --> 00:33:00,350
their models in streaming pipelines as

00:32:56,750 --> 00:33:03,080
well but my my my only problem with that

00:33:00,350 --> 00:33:04,520
is that it becomes too clunky for you

00:33:03,080 --> 00:33:05,900
know one person to build something or

00:33:04,520 --> 00:33:08,420
just like a small team to build

00:33:05,900 --> 00:33:10,520
something out of and plus obviously it

00:33:08,420 --> 00:33:12,470
adds like a lot of you know language

00:33:10,520 --> 00:33:14,600
issues as well that you write in Scala

00:33:12,470 --> 00:33:16,880
or you write in Java or you write in you

00:33:14,600 --> 00:33:20,090
know Python or using my spot whatever

00:33:16,880 --> 00:33:22,220
yeah so you would say dusk is more of a

00:33:20,090 --> 00:33:25,760
lightweight native kind of yes

00:33:22,220 --> 00:33:28,180
alternator yeah so actually dusk is

00:33:25,760 --> 00:33:31,700
being used by by by by a lot of

00:33:28,180 --> 00:33:36,700
organizations so DARPA uses it which is

00:33:31,700 --> 00:33:40,850
the defense in in in in u.s. a lot of

00:33:36,700 --> 00:33:43,040
you know aeronautical companies like

00:33:40,850 --> 00:33:45,080
NASA also use it and also a lot of HPC

00:33:43,040 --> 00:33:46,100
is use it as well thankful for

00:33:45,080 --> 00:33:47,990
high-performance computing so it's

00:33:46,100 --> 00:33:50,750
picked up quite fast and it's built from

00:33:47,990 --> 00:33:54,830
ground up in Python so it's quite fast

00:33:50,750 --> 00:33:56,870
as well great thanks for well I don't I

00:33:54,830 --> 00:33:59,240
guess we had a bit out of time and I

00:33:56,870 --> 00:34:01,130
don't see any more question so guys

00:33:59,240 --> 00:34:02,900
please feel free to reach out to web

00:34:01,130 --> 00:34:05,660
hobo was lack or Briella

00:34:02,900 --> 00:34:08,570
and thanks again web of yeah I guess

00:34:05,660 --> 00:34:11,090
it's a bit late in your part but have a

00:34:08,570 --> 00:34:14,440
great evening and keep in touch

00:34:11,090 --> 00:34:14,440
thanks thanks thanks Cheers

00:34:20,399 --> 00:34:22,460

YouTube URL: https://www.youtube.com/watch?v=mWTlDV2Kb10


