Title: #bbuzz: Alexey Grigorev - Deep learning in production: Serving image models at scale
Publication date: 2020-06-29
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/deep-learning-production-serving-image-models-scale

Deep learning achieves great performance in many areas, and it’s especially useful for computer vision tasks. However, using deep learning in production is challenging: it requires a lot of effort for developing and running the infrastructure to serve deep learning models at scale.

In this talk, we present a system for classifying images on one of the largest online classified advertising platforms. The main requirement for this system is to classify tens of millions of images daily and be able to operate reliably even during peak hours.

It took a year and lots of trial and error to arrive at the system we currently use. We present the details of this journey and tell our story: how we approached it initially, what worked and what didn’t, how it evolved and how it’s working right now.

Of course, we also walk you through the technical details and show how to implement a similar system using Python, AWS, Kubernetes, MXNet, and TensorFlow.
Captions: 
	00:00:08,760 --> 00:00:14,920
hello

00:00:11,150 --> 00:00:18,730
sexy and today I'm going to talk

00:00:14,920 --> 00:00:23,260
introduction first a few words about me

00:00:18,730 --> 00:00:25,660
I have I've been working as a software

00:00:23,260 --> 00:00:27,880
engineer professionally for more than

00:00:25,660 --> 00:00:31,349
ten years and six of them I spent

00:00:27,880 --> 00:00:33,730
working with machine learning systems

00:00:31,349 --> 00:00:39,550
right now I work as a lead at a

00:00:33,730 --> 00:00:42,870
scientist at electro and this is you can

00:00:39,550 --> 00:00:46,180
see the logo forex group in this picture

00:00:42,870 --> 00:00:48,730
is a company it is a group of online

00:00:46,180 --> 00:00:52,629
classifieds companies so maybe you've

00:00:48,730 --> 00:00:59,199
heard of some of them it looks that go

00:00:52,629 --> 00:01:01,750
and a couple of more brands and the main

00:00:59,199 --> 00:01:05,379
idea of online classifieds is a place

00:01:01,750 --> 00:01:07,030
where you can share where you can solve

00:01:05,379 --> 00:01:11,140
something where you can buy something so

00:01:07,030 --> 00:01:14,410
this is Alex India it is the place where

00:01:11,140 --> 00:01:15,670
people can come and so things they don't

00:01:14,410 --> 00:01:18,760
need anymore

00:01:15,670 --> 00:01:22,290
or people where people can come and buy

00:01:18,760 --> 00:01:25,360
things we use things for cheaper prices

00:01:22,290 --> 00:01:30,360
um Alex India is one of the biggest

00:01:25,360 --> 00:01:34,180
websites so a couple of hours

00:01:30,360 --> 00:01:41,110
gee brain Alex India then we also have

00:01:34,180 --> 00:01:45,329
presence Africa Asia South America so

00:01:41,110 --> 00:01:49,469
this on this slide you see it's Ukraine

00:01:45,329 --> 00:01:52,420
and typically for online classifieds

00:01:49,469 --> 00:01:54,909
pictures are very important so when you

00:01:52,420 --> 00:01:57,189
want to buy something when you browse

00:01:54,909 --> 00:02:01,539
through the catalogue you see many many

00:01:57,189 --> 00:02:05,289
pictures of things and having good

00:02:01,539 --> 00:02:06,969
pictures is very important for for

00:02:05,289 --> 00:02:08,890
deciding whether you want to learn more

00:02:06,969 --> 00:02:13,450
about something what do you want to

00:02:08,890 --> 00:02:18,310
contact the seller and ask to to arrange

00:02:13,450 --> 00:02:20,740
anything to buy something olives we have

00:02:18,310 --> 00:02:24,010
a lot of images 10 million images are

00:02:20,740 --> 00:02:26,489
uploaded by our customers daily and the

00:02:24,010 --> 00:02:29,189
challenge we are going to talk about in

00:02:26,489 --> 00:02:31,470
this presentation is how to apply deep

00:02:29,189 --> 00:02:36,709
learning models to ten million images

00:02:31,470 --> 00:02:39,510
per day so in this talk will first

00:02:36,709 --> 00:02:44,129
discuss motivation why are we doing this

00:02:39,510 --> 00:02:49,620
why do we need it then we'll spend some

00:02:44,129 --> 00:02:52,319
time talking how we train models and

00:02:49,620 --> 00:02:54,540
then after training is done there comes

00:02:52,319 --> 00:02:58,140
next step how to actually serve the

00:02:54,540 --> 00:03:00,720
model and we'll talk about the evolution

00:02:58,140 --> 00:03:03,000
of model serving that we had two weeks

00:03:00,720 --> 00:03:05,370
so how we started initially how it

00:03:03,000 --> 00:03:11,010
evolved what was the original

00:03:05,370 --> 00:03:13,859
architecture what was what were some

00:03:11,010 --> 00:03:21,000
drawbacks of this lecture how we changed

00:03:13,859 --> 00:03:25,139
it and provide so imagine you're you

00:03:21,000 --> 00:03:29,489
have a car and you want to solve so what

00:03:25,139 --> 00:03:33,209
you do is you go to Alex you create

00:03:29,489 --> 00:03:36,329
listing view in some details and then

00:03:33,209 --> 00:03:39,419
you take a picture and what happens next

00:03:36,329 --> 00:03:42,389
this feature of this picture is uploaded

00:03:39,419 --> 00:03:45,780
to our image hosting our image hosting

00:03:42,389 --> 00:03:49,680
and is based on s/3 s/3 is a service

00:03:45,780 --> 00:03:51,599
that in AWS for storing files so this is

00:03:49,680 --> 00:03:54,169
basically I think where you can put

00:03:51,599 --> 00:03:57,419
files and then you can get buy out back

00:03:54,169 --> 00:04:00,060
and every day

00:03:57,419 --> 00:04:03,799
ten million images are uploaded to these

00:04:00,060 --> 00:04:06,150
image postings is this him image hosting

00:04:03,799 --> 00:04:10,159
that means that we have billions and

00:04:06,150 --> 00:04:12,540
billions of picture in all three buckets

00:04:10,159 --> 00:04:16,919
what we want to do what we want to know

00:04:12,540 --> 00:04:20,549
about these images is wanted to know

00:04:16,919 --> 00:04:22,800
some information about them how good are

00:04:20,549 --> 00:04:25,800
these images because as mentioned

00:04:22,800 --> 00:04:27,599
earlier for people who want to buy

00:04:25,800 --> 00:04:31,699
things it's very important to have a

00:04:27,599 --> 00:04:36,570
good picture to get a good impression or

00:04:31,699 --> 00:04:40,520
how the item might look like and decide

00:04:36,570 --> 00:04:44,039
whether they want to contact the seller

00:04:40,520 --> 00:04:46,740
and if a picture is good then it

00:04:44,039 --> 00:04:49,770
maximizes the chances that they will

00:04:46,740 --> 00:04:55,020
decide to actually contact the seller

00:04:49,770 --> 00:04:58,439
and buy the item of Julia so here we

00:04:55,020 --> 00:05:01,409
have two pictures one picture is better

00:04:58,439 --> 00:05:03,479
quality the other picture is worse

00:05:01,409 --> 00:05:05,699
so we want to know which pictures are

00:05:03,479 --> 00:05:09,150
good which pictures about and in case a

00:05:05,699 --> 00:05:12,569
picture is not the best quality we want

00:05:09,150 --> 00:05:15,180
to contact the seller and suggest some

00:05:12,569 --> 00:05:18,509
ways to to improve the image and the

00:05:15,180 --> 00:05:23,340
overall listing then we are also

00:05:18,509 --> 00:05:25,919
interested in what what is it on these

00:05:23,340 --> 00:05:29,340
images what are the objects on these

00:05:25,919 --> 00:05:32,569
images as you see there are many many

00:05:29,340 --> 00:05:39,749
things that people can post an upload

00:05:32,569 --> 00:05:43,080
bikes cars sometimes people can upload

00:05:39,749 --> 00:05:45,419
something they want to they can try to

00:05:43,080 --> 00:05:48,539
sell something that they aren't supposed

00:05:45,419 --> 00:05:53,129
to sell like happen we also want to know

00:05:48,539 --> 00:05:55,289
that in this case we have somebody's

00:05:53,129 --> 00:06:00,319
trying to sell my chatter of course this

00:05:55,289 --> 00:06:00,319
is we should prevent this from happening

00:06:00,710 --> 00:06:06,149
so we from images who want to know what

00:06:03,839 --> 00:06:07,649
is on the image so for each image we

00:06:06,149 --> 00:06:09,659
want to know whether it's an image of a

00:06:07,649 --> 00:06:15,060
truck whether it's an image of a fish or

00:06:09,659 --> 00:06:17,219
it's an image of some web so the idea is

00:06:15,060 --> 00:06:19,529
simple so how can we extract this

00:06:17,219 --> 00:06:22,919
information of course using machine

00:06:19,529 --> 00:06:28,979
learning and so we get all this images

00:06:22,919 --> 00:06:31,620
that we have send them to the machine

00:06:28,979 --> 00:06:35,159
learning model and then store the output

00:06:31,620 --> 00:06:38,069
somewhere in a in a database so this can

00:06:35,159 --> 00:06:40,849
be emitted at the database well and all

00:06:38,069 --> 00:06:43,379
these labels all these categories are

00:06:40,849 --> 00:06:47,819
the fields in this database basically

00:06:43,379 --> 00:06:50,009
the information about each image to the

00:06:47,819 --> 00:06:52,889
pan is clear we want to use machine

00:06:50,009 --> 00:06:53,550
learning how do we actually train models

00:06:52,889 --> 00:06:57,210
so

00:06:53,550 --> 00:07:00,510
Ramos is quite simple these days

00:06:57,210 --> 00:07:03,510
there are any services cloud services

00:07:00,510 --> 00:07:07,220
that make the job a lot easier than the

00:07:03,510 --> 00:07:11,070
few years ago we use Amazon sage maker

00:07:07,220 --> 00:07:14,850
with sage maker it's quite simple so all

00:07:11,070 --> 00:07:17,250
we need to do is upload the data to s3

00:07:14,850 --> 00:07:20,310
and trance h make a job the sage maker

00:07:17,250 --> 00:07:22,020
job and gets the data from us we train

00:07:20,310 --> 00:07:23,250
the model with the parameters we specify

00:07:22,020 --> 00:07:26,640
and saves ourselves

00:07:23,250 --> 00:07:30,810
so it looks like that so we have a way

00:07:26,640 --> 00:07:34,190
to collect the data so it can be our own

00:07:30,810 --> 00:07:38,430
service for labeling or you can also use

00:07:34,190 --> 00:07:41,610
Amazon Mechanical Turk and then for each

00:07:38,430 --> 00:07:45,660
image we know for example image quality

00:07:41,610 --> 00:07:49,380
how good the images we get all this data

00:07:45,660 --> 00:07:51,510
and save to s3 and in this you would

00:07:49,380 --> 00:07:54,660
have take so you may just pass the

00:07:51,510 --> 00:07:57,030
labels then we can run a sage maker job

00:07:54,660 --> 00:08:00,420
so you make a job pay just a you know

00:07:57,030 --> 00:08:03,810
just wrong s3 train some model and then

00:08:00,420 --> 00:08:09,570
saves the results today game to s3 as a

00:08:03,810 --> 00:08:14,300
modifier quite simple and we can have a

00:08:09,570 --> 00:08:18,150
model in more time so let's say we spent

00:08:14,300 --> 00:08:20,280
some time a week to train in the model

00:08:18,150 --> 00:08:26,130
we have it the model is quite good

00:08:20,280 --> 00:08:27,810
what's next what we can do next is for

00:08:26,130 --> 00:08:30,060
example since we use sage maker for

00:08:27,810 --> 00:08:32,780
training we can also use sage maker for

00:08:30,060 --> 00:08:35,970
model serving so what we want to do is

00:08:32,780 --> 00:08:39,030
just take this model and put it inside a

00:08:35,970 --> 00:08:42,750
sage maker endpoint but then actually

00:08:39,030 --> 00:08:45,930
not we don't just want to put this model

00:08:42,750 --> 00:08:50,730
in to an endpoint we want to get all the

00:08:45,930 --> 00:08:53,070
images that have in SV run them through

00:08:50,730 --> 00:08:56,630
the model and save the results in

00:08:53,070 --> 00:09:00,990
metadata database such that the users

00:08:56,630 --> 00:09:04,740
can benefit from this so we need a way

00:09:00,990 --> 00:09:08,540
to to get images put them to model

00:09:04,740 --> 00:09:08,540
safety results make

00:09:09,030 --> 00:09:17,460
so now we'll talk about how to actually

00:09:12,060 --> 00:09:20,900
do this now to do it we create a special

00:09:17,460 --> 00:09:23,520
service called metadata service that

00:09:20,900 --> 00:09:25,320
clients of this service that is

00:09:23,520 --> 00:09:28,530
typically others the other teams that

00:09:25,320 --> 00:09:30,870
meet the predictions of our models they

00:09:28,530 --> 00:09:31,530
communicate to this this may be that the

00:09:30,870 --> 00:09:34,320
service

00:09:31,530 --> 00:09:38,490
talks to a model the rapper this is a

00:09:34,320 --> 00:09:42,000
simple very simple service that simply

00:09:38,490 --> 00:09:44,400
fetches images from s3 and then and the

00:09:42,000 --> 00:09:47,310
images and then sends them to search me

00:09:44,400 --> 00:09:51,150
say you make her processes the images

00:09:47,310 --> 00:09:53,430
returns the predictions and this model

00:09:51,150 --> 00:09:56,070
our original predictions again to

00:09:53,430 --> 00:09:58,770
metadata service metadata services the

00:09:56,070 --> 00:10:02,430
results to database and response to the

00:09:58,770 --> 00:10:04,740
user now the user can use can use these

00:10:02,430 --> 00:10:08,670
predictions to do what whatever they

00:10:04,740 --> 00:10:13,200
want and the first initial version was

00:10:08,670 --> 00:10:15,620
quite good so we could already analyze

00:10:13,200 --> 00:10:19,650
the quality of images that we had and

00:10:15,620 --> 00:10:25,170
somehow educate our users that in cases

00:10:19,650 --> 00:10:27,720
that image is not good we can say what

00:10:25,170 --> 00:10:31,380
is wrong with the image and suggest

00:10:27,720 --> 00:10:34,230
waste enjoyed until about some problems

00:10:31,380 --> 00:10:35,910
with our initial rejection

00:10:34,230 --> 00:10:41,880
first of all stage maker turned out to

00:10:35,910 --> 00:10:43,860
be quite expensive if we when we simply

00:10:41,880 --> 00:10:46,410
deploy it to our own kubernetes cluster

00:10:43,860 --> 00:10:51,030
instead of using sage bacon on point we

00:10:46,410 --> 00:10:54,150
could reduce costs like four times then

00:10:51,030 --> 00:10:58,680
the next step we noticed that it's quite

00:10:54,150 --> 00:11:01,200
difficult to to deal with spites of

00:10:58,680 --> 00:11:06,630
traffic when we have sudden spice spikes

00:11:01,200 --> 00:11:09,150
of traffic and when during during some

00:11:06,630 --> 00:11:13,470
days suddenly a lot of users try to

00:11:09,150 --> 00:11:18,330
upload images then to gracefully scale

00:11:13,470 --> 00:11:21,590
up and scale down we added a bit of we

00:11:18,330 --> 00:11:24,180
made our services synchronous so good

00:11:21,590 --> 00:11:27,510
iq between the metadata service and

00:11:24,180 --> 00:11:30,630
modular rapper and with this Q we can it

00:11:27,510 --> 00:11:32,880
was a lot easier to actually scale our

00:11:30,630 --> 00:11:35,490
models because we didn't need to process

00:11:32,880 --> 00:11:41,730
everything immediately we could just

00:11:35,490 --> 00:11:44,820
wait a bit and simply scale our models

00:11:41,730 --> 00:11:49,260
up and then process through pics of

00:11:44,820 --> 00:11:53,610
traffic we have two models so basically

00:11:49,260 --> 00:11:56,640
for each of the model we had a special

00:11:53,610 --> 00:12:01,800
thing that we previously called model

00:11:56,640 --> 00:12:04,140
wrapper for each separate one which

00:12:01,800 --> 00:12:09,570
could fetch images from s3 and then talk

00:12:04,140 --> 00:12:14,730
to tender for serving or MX MX net

00:12:09,570 --> 00:12:17,910
server so serving and process models so

00:12:14,730 --> 00:12:21,690
let's walk through the process so first

00:12:17,910 --> 00:12:25,470
a client's and submits a request so it

00:12:21,690 --> 00:12:30,260
can be for this files I want to know the

00:12:25,470 --> 00:12:32,310
category of the objects on these images

00:12:30,260 --> 00:12:34,560
so it means we want to run a

00:12:32,310 --> 00:12:37,560
classification model then the metadata

00:12:34,560 --> 00:12:40,680
service responds immediately saying that

00:12:37,560 --> 00:12:44,010
your request is include wait will tell

00:12:40,680 --> 00:12:46,590
you when it's finished now metadata

00:12:44,010 --> 00:12:49,410
service checks the database to see if we

00:12:46,590 --> 00:12:53,940
already have some results for some of

00:12:49,410 --> 00:12:57,060
the files if we don't we submit this

00:12:53,940 --> 00:12:59,940
request to a queue then the image

00:12:57,060 --> 00:13:03,120
category model wrapper listens to this

00:12:59,940 --> 00:13:06,420
queue both messages from there typically

00:13:03,120 --> 00:13:12,810
does this in batches of 10 image Indian

00:13:06,420 --> 00:13:14,850
images get the images from s3 and then

00:13:12,810 --> 00:13:17,760
does some pre-processing because we need

00:13:14,850 --> 00:13:19,980
to get the images besides them convert

00:13:17,760 --> 00:13:25,200
to numpy array do some pre-processing

00:13:19,980 --> 00:13:27,570
like normalize the arrays then

00:13:25,200 --> 00:13:29,910
eventually takes the race and converts

00:13:27,570 --> 00:13:32,160
them to protocol once we have heard

00:13:29,910 --> 00:13:33,950
above we use your PC to talk don't

00:13:32,160 --> 00:13:37,040
answer for serving

00:13:33,950 --> 00:13:40,900
we sent requests on the fossil in the

00:13:37,040 --> 00:13:43,340
place with results a game over jealousy

00:13:40,900 --> 00:13:47,060
and then the model rapper puts the

00:13:43,340 --> 00:13:51,320
results to the response kill metadata

00:13:47,060 --> 00:13:53,300
service distance to this queue yes the

00:13:51,320 --> 00:13:57,890
results save them to the database and

00:13:53,300 --> 00:14:01,040
then finally responds to to the client

00:13:57,890 --> 00:14:03,980
and this is using a call back saying hey

00:14:01,040 --> 00:14:09,850
for these IDs these are the categories

00:14:03,980 --> 00:14:15,050
and this worked quite well so we could

00:14:09,850 --> 00:14:18,640
use this in this models for for many use

00:14:15,050 --> 00:14:22,250
cases like one I already mentioned

00:14:18,640 --> 00:14:24,620
detecting the images with not great

00:14:22,250 --> 00:14:26,450
quality and then suggesting this else

00:14:24,620 --> 00:14:30,200
ways to improve it and the other case

00:14:26,450 --> 00:14:31,640
was moderation when somebody is trying

00:14:30,200 --> 00:14:35,510
to solve something they are not supposed

00:14:31,640 --> 00:14:40,240
to so we could catch these images and

00:14:35,510 --> 00:14:42,830
not that can go live to platform

00:14:40,240 --> 00:14:47,120
terrible unfortunately some drawbacks in

00:14:42,830 --> 00:14:50,530
this architecture so this was pretty

00:14:47,120 --> 00:14:53,480
inconvenient for the clients because

00:14:50,530 --> 00:14:59,750
when a system is an asynchronous it

00:14:53,480 --> 00:15:01,340
makes it quite difficult to to to work

00:14:59,750 --> 00:15:03,440
with this instead of just simply saying

00:15:01,340 --> 00:15:06,520
as sending the request engage me

00:15:03,440 --> 00:15:09,920
response they need to also keep track of

00:15:06,520 --> 00:15:12,200
photos so what request and then also

00:15:09,920 --> 00:15:15,440
provide a callback so it means the thing

00:15:12,200 --> 00:15:19,420
to make the service publicly available

00:15:15,440 --> 00:15:23,240
to to make this endpoint it's a bit

00:15:19,420 --> 00:15:26,270
difficult for the clients then it's also

00:15:23,240 --> 00:15:29,630
not really the real time because in some

00:15:26,270 --> 00:15:34,570
cases when we have pics of traffic it's

00:15:29,630 --> 00:15:38,780
difficult to let's say at the same time

00:15:34,570 --> 00:15:40,660
10,000 users uploaded the images so of

00:15:38,780 --> 00:15:44,450
course there will be some delay in

00:15:40,660 --> 00:15:47,030
working through this backlog of images

00:15:44,450 --> 00:15:47,779
and it means that somebody is trying to

00:15:47,030 --> 00:15:51,499
sell again it

00:15:47,779 --> 00:15:53,420
this point we might need to wait fire

00:15:51,499 --> 00:15:56,240
for sometimes ten minute we come 10

00:15:53,420 --> 00:15:58,790
minutes before we can actually catch

00:15:56,240 --> 00:16:02,029
this case and remove the ad in these

00:15:58,790 --> 00:16:05,439
cases we of course want to react real in

00:16:02,029 --> 00:16:09,439
real time and although you know about

00:16:05,439 --> 00:16:13,600
guns at the moment they are uploaded to

00:16:09,439 --> 00:16:17,600
the platform and it's also too much

00:16:13,600 --> 00:16:21,129
synchronous like we need to do have a

00:16:17,600 --> 00:16:24,019
lot of queues and just following through

00:16:21,129 --> 00:16:25,990
the system to see how requests are

00:16:24,019 --> 00:16:30,620
propagating sometimes quite difficult

00:16:25,990 --> 00:16:35,360
it's also expensive we use SKS public

00:16:30,620 --> 00:16:38,990
use and it works through polling so the

00:16:35,360 --> 00:16:43,670
the clients could services who listen on

00:16:38,990 --> 00:16:45,769
the queue and they simply ask the queue

00:16:43,670 --> 00:16:49,069
hey are there new messages other new

00:16:45,769 --> 00:16:52,790
messages and they doing this of course

00:16:49,069 --> 00:16:54,980
there is some delay but eventually for

00:16:52,790 --> 00:16:58,610
each request we have to pay a certain

00:16:54,980 --> 00:17:01,399
amount of money and then at some point

00:16:58,610 --> 00:17:06,199
half of the costs for the infrastructure

00:17:01,399 --> 00:17:08,959
was just simply SKS polling that was too

00:17:06,199 --> 00:17:12,350
expensive then we also had some

00:17:08,959 --> 00:17:16,159
duplicated project because this model

00:17:12,350 --> 00:17:19,069
wrappers they both need to talk to s3 do

00:17:16,159 --> 00:17:21,409
some pre-processing so when creating

00:17:19,069 --> 00:17:24,470
these services we need to somehow put

00:17:21,409 --> 00:17:28,189
this logic in the library it was and be

00:17:24,470 --> 00:17:32,929
difficult mundane then database that we

00:17:28,189 --> 00:17:34,970
have was not good for analytics we can

00:17:32,929 --> 00:17:39,679
of course store the results there and

00:17:34,970 --> 00:17:41,600
use them for responses but when people

00:17:39,679 --> 00:17:44,809
wanted to analyze the results of the

00:17:41,600 --> 00:17:47,440
models we couldn't simply use that for

00:17:44,809 --> 00:17:50,870
that and we also also used my sequel and

00:17:47,440 --> 00:17:55,309
with our traffic we found out pretty

00:17:50,870 --> 00:17:58,460
fast that just grows too much and it's a

00:17:55,309 --> 00:18:01,220
bit of a burden to maintain to make sure

00:17:58,460 --> 00:18:03,740
that the database is not overloaded

00:18:01,220 --> 00:18:06,980
it has sufficiently large in sauce and

00:18:03,740 --> 00:18:09,530
things like that and finally when we

00:18:06,980 --> 00:18:11,690
need to add a new model turns out that

00:18:09,530 --> 00:18:16,850
there are too many places where we need

00:18:11,690 --> 00:18:19,100
to let me need to modify so when we want

00:18:16,850 --> 00:18:20,780
to add a third model we need to change

00:18:19,100 --> 00:18:23,809
something in the metadata service we

00:18:20,780 --> 00:18:27,289
need to add two more queues we need to

00:18:23,809 --> 00:18:30,530
create a new service for the rapper we

00:18:27,289 --> 00:18:34,190
need to put some logic tail for getting

00:18:30,530 --> 00:18:37,220
files from us three then put these two

00:18:34,190 --> 00:18:42,380
tons of for serving or MMS so it's a lot

00:18:37,220 --> 00:18:47,179
of work to just add a new model that's

00:18:42,380 --> 00:18:49,909
why we try to make it a bit simpler this

00:18:47,179 --> 00:18:53,150
is something I will talk about now so

00:18:49,909 --> 00:18:55,820
this is what we have and we try to see

00:18:53,150 --> 00:18:59,570
how we can improve it so the first step

00:18:55,820 --> 00:19:02,500
that we did was to to get all this model

00:18:59,570 --> 00:19:05,030
wrappers and put them into one service

00:19:02,500 --> 00:19:08,330
so now we don't have multiple services

00:19:05,030 --> 00:19:10,520
is just one thing and we call it in

00:19:08,330 --> 00:19:14,000
image model service because this theme

00:19:10,520 --> 00:19:18,289
contained the logic for getting the data

00:19:14,000 --> 00:19:20,630
from s3 processing the images and then

00:19:18,289 --> 00:19:26,690
talking to tons of observing to the

00:19:20,630 --> 00:19:28,520
actual models with this setup we no

00:19:26,690 --> 00:19:33,289
longer needed the metadata service and

00:19:28,520 --> 00:19:37,789
all these queues and the client simply

00:19:33,289 --> 00:19:42,380
could send HTTP requests you do IMS

00:19:37,789 --> 00:19:46,880
itself now and then we weren't happy

00:19:42,380 --> 00:19:51,640
about MMS MMS is a max net model service

00:19:46,880 --> 00:19:54,710
so that's why we also they moved it and

00:19:51,640 --> 00:19:57,409
with this our architecture becomes quite

00:19:54,710 --> 00:20:01,340
simple so in retrospect like right now

00:19:57,409 --> 00:20:03,020
it seems quite easy yeah so this is in

00:20:01,340 --> 00:20:07,039
retrospect what we should have started

00:20:03,020 --> 00:20:09,830
with just a simple thing that accepts

00:20:07,039 --> 00:20:13,789
requests and then forward sound to

00:20:09,830 --> 00:20:14,809
actual models of course we need to have

00:20:13,789 --> 00:20:16,820
a caching layer

00:20:14,809 --> 00:20:21,980
on top of that to make sure that we

00:20:16,820 --> 00:20:24,409
don't we don't score our images multiple

00:20:21,980 --> 00:20:27,679
times and the way we do it we use

00:20:24,409 --> 00:20:29,960
dynamic viq by the store and for the key

00:20:27,679 --> 00:20:34,730
instead of using the file name we use

00:20:29,960 --> 00:20:36,289
md5 hash up there wow we have quite a

00:20:34,730 --> 00:20:39,080
few duplicates on our platform

00:20:36,289 --> 00:20:41,360
people often upload the same image

00:20:39,080 --> 00:20:44,120
multiple times so in this case we don't

00:20:41,360 --> 00:20:47,690
want to send it to score it again

00:20:44,120 --> 00:20:49,669
and send it to the model we can see that

00:20:47,690 --> 00:20:51,980
for this md5 hash we already have

00:20:49,669 --> 00:20:55,580
results and we can simply return to the

00:20:51,980 --> 00:20:57,950
user in the same we don't need to

00:20:55,580 --> 00:21:01,789
calculate md5 hash we can simply use

00:20:57,950 --> 00:21:06,799
attack from s3 because their attack in

00:21:01,789 --> 00:21:08,869
most cases is the same as md5 - then we

00:21:06,799 --> 00:21:11,649
needed to add a few more models so the

00:21:08,869 --> 00:21:13,999
first model was detecting if there is a

00:21:11,649 --> 00:21:18,679
artificially embedded text on the

00:21:13,999 --> 00:21:21,259
pictures in some cases it's prohibited

00:21:18,679 --> 00:21:24,379
on our platforms - to add this text we

00:21:21,259 --> 00:21:29,360
want to have a model that detects that

00:21:24,379 --> 00:21:31,490
it's the case and help us remove this

00:21:29,360 --> 00:21:33,320
from the platform so there was another

00:21:31,490 --> 00:21:37,580
model that simply checks if there is

00:21:33,320 --> 00:21:40,009
text so not adding this model was quite

00:21:37,580 --> 00:21:43,549
simple so we do you need to touch many

00:21:40,009 --> 00:21:45,080
places so we just needed you to adjust

00:21:43,549 --> 00:21:49,490
the code of Emma's

00:21:45,080 --> 00:21:53,029
to add the new pre-processing class for

00:21:49,490 --> 00:21:57,019
this and add another instance of tons of

00:21:53,029 --> 00:21:59,779
observing with this model then we had

00:21:57,019 --> 00:22:02,840
another problem that we also wanted to

00:21:59,779 --> 00:22:06,950
solve with deep learning it was we

00:22:02,840 --> 00:22:09,919
wanted to detect nudity so we trained

00:22:06,950 --> 00:22:12,289
another model for classifying if an

00:22:09,919 --> 00:22:14,269
image is safe for work or not that was

00:22:12,289 --> 00:22:21,409
another model we used max net for that

00:22:14,269 --> 00:22:25,340
again trained using sage maker and for

00:22:21,409 --> 00:22:28,120
that we wrote our own MX net serving

00:22:25,340 --> 00:22:31,420
because we didn't like MMS

00:22:28,120 --> 00:22:35,350
we wrote a simple thing simple wrapper

00:22:31,420 --> 00:22:42,340
around him on Mike's Amex net that can

00:22:35,350 --> 00:22:44,530
get a compressed numpy array in Pocket

00:22:42,340 --> 00:22:46,240
apply the model and return back the

00:22:44,530 --> 00:22:48,400
response something pretty similar to

00:22:46,240 --> 00:22:54,460
tender for serving but instead of using

00:22:48,400 --> 00:22:57,220
browser buff it used HTTP that was the

00:22:54,460 --> 00:23:01,330
our final architecture so it was quite

00:22:57,220 --> 00:23:04,240
simple one caveat here was that tuning

00:23:01,330 --> 00:23:07,270
it was more difficult than in

00:23:04,240 --> 00:23:09,760
asynchronous case because when we have a

00:23:07,270 --> 00:23:13,980
synchronous we can gracefully react to

00:23:09,760 --> 00:23:18,930
Vixen traffic when there is a sudden

00:23:13,980 --> 00:23:23,130
sudden peak 10,000 images are uploaded

00:23:18,930 --> 00:23:27,850
we can simply put them to the queue and

00:23:23,130 --> 00:23:29,860
scale it out and then process through

00:23:27,850 --> 00:23:31,780
the backlog and then scale it down and

00:23:29,860 --> 00:23:34,570
we don't need to worry that something

00:23:31,780 --> 00:23:40,780
gets lost here we need to be more

00:23:34,570 --> 00:23:42,700
careful with this because if we are

00:23:40,780 --> 00:23:45,610
synchronous and need to respond

00:23:42,700 --> 00:23:47,520
immediately it means that to react to

00:23:45,610 --> 00:23:50,140
these peaks of traffic we need to

00:23:47,520 --> 00:23:52,270
sometimes over-provision instances so

00:23:50,140 --> 00:23:57,130
some to have some instances that are

00:23:52,270 --> 00:24:00,790
idle and one once they started once they

00:23:57,130 --> 00:24:06,070
start they ship traffic we add more

00:24:00,790 --> 00:24:10,030
instances in this way we can react to

00:24:06,070 --> 00:24:12,790
peaks of traffic with with less problems

00:24:10,030 --> 00:24:16,090
but it's again it took her quite a while

00:24:12,790 --> 00:24:19,090
to actually tune it to be able to do

00:24:16,090 --> 00:24:21,670
process through a lot of requests at the

00:24:19,090 --> 00:24:28,000
same time with asynchronous case it was

00:24:21,670 --> 00:24:30,820
easier not just one last thing is we

00:24:28,000 --> 00:24:33,870
have analysts and analysts are quite

00:24:30,820 --> 00:24:38,260
interested in analyzing the results of

00:24:33,870 --> 00:24:41,080
these predictions and of course they

00:24:38,260 --> 00:24:41,920
often want to do to see how many images

00:24:41,080 --> 00:24:44,380
for example

00:24:41,920 --> 00:24:47,230
contain text or images for pornographic

00:24:44,380 --> 00:24:49,960
or how images contained cancer things

00:24:47,230 --> 00:24:52,510
like this how many cars were there among

00:24:49,960 --> 00:24:56,350
images with the previous setup it was

00:24:52,510 --> 00:25:01,090
difficult and let's briefly talk about

00:24:56,350 --> 00:25:04,530
how we made it easier for analysts so

00:25:01,090 --> 00:25:10,540
the moment a user uploads an image to s3

00:25:04,530 --> 00:25:12,970
as we can generate s3 event notification

00:25:10,540 --> 00:25:16,030
saying hey there was a file in the

00:25:12,970 --> 00:25:18,190
bucket do something with this and it's

00:25:16,030 --> 00:25:21,670
possible to put these notifications to

00:25:18,190 --> 00:25:24,580
take you and then what we can do is we

00:25:21,670 --> 00:25:28,420
can simply listen to this q2 all the

00:25:24,580 --> 00:25:33,640
events - all the new newly uploaded

00:25:28,420 --> 00:25:37,000
images and then simply send them to IMS

00:25:33,640 --> 00:25:40,240
to our image model service then model

00:25:37,000 --> 00:25:42,130
service responds with results we can put

00:25:40,240 --> 00:25:44,680
it to these artists we can a system in

00:25:42,130 --> 00:25:49,230
our case and then eventually save it to

00:25:44,680 --> 00:25:53,980
s3 when we have the data in s3 we can

00:25:49,230 --> 00:26:00,940
simply put them to look at all can have

00:25:53,980 --> 00:26:04,060
the data in Latino tables athena is is

00:26:00,940 --> 00:26:07,720
basically a managed Chico engine and

00:26:04,060 --> 00:26:10,630
presto and you can use it to query all

00:26:07,720 --> 00:26:14,200
the results so analysts could use sequel

00:26:10,630 --> 00:26:17,080
to way they know and love to query the

00:26:14,200 --> 00:26:22,420
results of our models and they were very

00:26:17,080 --> 00:26:25,720
happy about this that was it

00:26:22,420 --> 00:26:32,110
so we talked about motivation why we

00:26:25,720 --> 00:26:34,450
wanted to to build our model server why

00:26:32,110 --> 00:26:37,600
we needed to serve different models when

00:26:34,450 --> 00:26:41,440
we briefly talked about how we trained

00:26:37,600 --> 00:26:47,200
models and then discussed our retexture

00:26:41,440 --> 00:26:51,070
for this just want to summarize all the

00:26:47,200 --> 00:26:52,450
talk into the main takeaway points so we

00:26:51,070 --> 00:26:55,030
use deep learning to extract metadata

00:26:52,450 --> 00:26:56,860
from images

00:26:55,030 --> 00:27:00,640
there's a couple of models and then we

00:26:56,860 --> 00:27:03,790
run them on safe results in metadata

00:27:00,640 --> 00:27:08,170
database we you say the message maker

00:27:03,790 --> 00:27:10,900
and it makes it very easy to to train

00:27:08,170 --> 00:27:13,660
deep learning models we simply need to

00:27:10,900 --> 00:27:16,150
specify the location with the data the

00:27:13,660 --> 00:27:19,630
parameters of the model press a button

00:27:16,150 --> 00:27:25,570
and trains a model and saves the results

00:27:19,630 --> 00:27:28,900
to still serving with each maker is not

00:27:25,570 --> 00:27:31,840
as nice as training so it's you need it

00:27:28,900 --> 00:27:35,380
requires a bit of code to actually get

00:27:31,840 --> 00:27:38,140
the images from s3 process them in a way

00:27:35,380 --> 00:27:40,600
we need and then it gets a bit expensive

00:27:38,140 --> 00:27:45,100
at scale when we need to process a lot

00:27:40,600 --> 00:27:48,430
of images at the same time working

00:27:45,100 --> 00:27:51,070
through a lot of images through peaks of

00:27:48,430 --> 00:27:53,890
traffic is easier when the system is in

00:27:51,070 --> 00:27:56,530
Cygnus but there is a downside of that

00:27:53,890 --> 00:27:58,930
the ter more complex they're more

00:27:56,530 --> 00:28:01,120
convenient for the users and then they

00:27:58,930 --> 00:28:02,920
are not always real time because when

00:28:01,120 --> 00:28:05,380
there is a backlog of items in the

00:28:02,920 --> 00:28:08,410
queues that we need to process it may

00:28:05,380 --> 00:28:12,730
take some time to to process all the

00:28:08,410 --> 00:28:16,450
items we use the three event

00:28:12,730 --> 00:28:21,070
notifications mechanism a non-intrusive

00:28:16,450 --> 00:28:23,620
way to connect our systems to connect

00:28:21,070 --> 00:28:25,600
all the images our image hosting from

00:28:23,620 --> 00:28:29,350
the english posting and then process

00:28:25,600 --> 00:28:33,130
them and put the results to to the Avena

00:28:29,350 --> 00:28:37,060
table and that made it a lot easier for

00:28:33,130 --> 00:28:41,980
analysts to analyze all the results and

00:28:37,060 --> 00:28:44,410
then finally athena is quite an easy to

00:28:41,980 --> 00:28:49,150
maintain solution for analytics it's

00:28:44,410 --> 00:28:53,350
scalable you pay only for the data use

00:28:49,150 --> 00:29:00,400
can you simply can put all the data in

00:28:53,350 --> 00:29:04,760
s3 and let analysis play with the data

00:29:00,400 --> 00:29:08,990
that is almost all from my side so this

00:29:04,760 --> 00:29:12,710
talk is based on two blog posts in our

00:29:08,990 --> 00:29:15,620
block you can go there and read for for

00:29:12,710 --> 00:29:19,310
more details for more information there

00:29:15,620 --> 00:29:21,170
is also in the part two a bit there are

00:29:19,310 --> 00:29:22,940
some details about the way we serve

00:29:21,170 --> 00:29:27,260
comics net models so if you're

00:29:22,940 --> 00:29:30,230
interested go check it and then finally

00:29:27,260 --> 00:29:33,860
I am working on a book called machine

00:29:30,230 --> 00:29:36,560
during boot camp boot boot camp the idea

00:29:33,860 --> 00:29:41,150
is to teach machine learning through

00:29:36,560 --> 00:29:45,500
projects if you're interested go check

00:29:41,150 --> 00:29:51,080
the link and you can get 40% percent

00:29:45,500 --> 00:29:54,020
discount with the code here I will

00:29:51,080 --> 00:29:58,550
appreciate if you give me any feedback

00:29:54,020 --> 00:30:02,150
on the talk you find it interesting or

00:29:58,550 --> 00:30:05,840
maybe it was too slow or too fast so if

00:30:02,150 --> 00:30:08,450
you want to do it you can check the

00:30:05,840 --> 00:30:13,250
squirrel code or this thing and give me

00:30:08,450 --> 00:30:17,000
some feedback also there you'll find the

00:30:13,250 --> 00:30:18,890
link to the slides and you can also if

00:30:17,000 --> 00:30:20,720
you're interested in a free copy of

00:30:18,890 --> 00:30:23,930
machine learning bootcamp boot camp

00:30:20,720 --> 00:30:27,830
leave your others email address and you

00:30:23,930 --> 00:30:30,100
get a chance to to win a free copy this

00:30:27,830 --> 00:30:32,420
is all from me thank you for attention

00:30:30,100 --> 00:30:33,980
please let me know if you have any

00:30:32,420 --> 00:30:37,120
questions I'll be very happy to answer

00:30:33,980 --> 00:30:37,120
them thank you

00:30:42,820 --> 00:30:44,880

YouTube URL: https://www.youtube.com/watch?v=JJnmGqypE2k


