Title: #bbuzz: Anshum Gupta - Cross DC replication in Apache Solr - Beyond just forwarding data
Publication date: 2020-07-05
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/cross-dc-replication-apache-solr-beyond-just-forwarding-data

The increase is data and traffic accompanied with the feature set offered by Solr has led to an increased usage of Solr in recent times. As a lot of these applications are critical, a reliable disaster recovery mechanism has become essential.

While Solr already offers a way to accomplish disaster recovery using cross-dc replication out of the box, there are other ways to accomplish the same. Copying data is the obvious part of this system but a lot of supporting features or systems like monitoring, self-defense mechanism etc. are needed for a complete cross-dc solution.

A simple solution, like the one that Solr offers out of the box works 90% of the times, but the lack of supporting features, monitoring tools, and also self-defense mechanism make it a difficult choice in a lot of places. Using such a system might catch you off-guard and leave you with a false sense of DR availability.

During this talk, I would like to talk about a few approaches of achieving cross-dc availability in Solr, something that I have tried and used over the years and highlight the pros and cons of each of them. I would also talk about all the satellite features and applications that are needed to ensure consistency of these clusters in addition to the self-healing, and self-defense mechanisms that are needed to reliably run and use the DR clusters.

At the end of this talk, attendees would have a much better understanding of achieving cross-dc for Solr and their reliability levels. They would also have learnt about the supporting systems that are needed to have a solid cross-dc story in Solr, one that scales and works reliably.
Captions: 
	00:00:11,070 --> 00:00:18,260
you know so hi everyone welcome to my

00:00:16,190 --> 00:00:21,410
education in a passive solar beyond just

00:00:18,260 --> 00:00:22,600
forwarding data and thank you for the

00:00:21,410 --> 00:00:26,030
introduction

00:00:22,600 --> 00:00:28,220
so these enough for the day and thank

00:00:26,030 --> 00:00:30,380
you for bearing with us the agenda for

00:00:28,220 --> 00:00:32,090
the day is I'm going to start off with

00:00:30,380 --> 00:00:34,340
talking about the basic architecture of

00:00:32,090 --> 00:00:36,980
solar cloud then move on to talking

00:00:34,340 --> 00:00:40,640
about the need for disaster recovery and

00:00:36,980 --> 00:00:42,320
then I'm gonna talk about different

00:00:40,640 --> 00:00:44,540
approaches to achieving cross DC and

00:00:42,320 --> 00:00:47,210
solar and going to spend most of the

00:00:44,540 --> 00:00:49,670
time for this talk on that one specific

00:00:47,210 --> 00:00:52,040
thing and then wrap it up by talking

00:00:49,670 --> 00:00:55,460
about what I believe could be the future

00:00:52,040 --> 00:00:58,430
of of how achieved rustici in solar in a

00:00:55,460 --> 00:01:04,879
manner that he serve and works for most

00:00:58,430 --> 00:01:07,909
people so a very simplified version of a

00:01:04,879 --> 00:01:08,990
solar architecture diagram is something

00:01:07,909 --> 00:01:10,970
that looks like this

00:01:08,990 --> 00:01:13,630
you've got shards that have leaders

00:01:10,970 --> 00:01:16,370
followers and they talk to each other

00:01:13,630 --> 00:01:18,950
and they can they pretty much play the

00:01:16,370 --> 00:01:20,360
same role other than the fact that every

00:01:18,950 --> 00:01:21,830
incoming update is drawn through the

00:01:20,360 --> 00:01:24,700
leader who's also responsible for

00:01:21,830 --> 00:01:28,880
versioning and doing a bunch of other

00:01:24,700 --> 00:01:32,830
you know managerial things as far as

00:01:28,880 --> 00:01:36,260
handling documents and data is concerned

00:01:32,830 --> 00:01:40,040
so like any distributed system it seems

00:01:36,260 --> 00:01:42,020
fairly easy to run and manage and the

00:01:40,040 --> 00:01:44,030
shard replicas there they provide

00:01:42,020 --> 00:01:45,710
scalability and availability so if one

00:01:44,030 --> 00:01:49,580
of them goes away the other one comes in

00:01:45,710 --> 00:01:51,500
assumes leadership also if years if you

00:01:49,580 --> 00:01:55,480
need to support more traffic you can

00:01:51,500 --> 00:01:58,070
always add more more replicas there and

00:01:55,480 --> 00:02:01,610
so that kind of solves most of the

00:01:58,070 --> 00:02:04,940
problem for standard use cases but when

00:02:01,610 --> 00:02:09,560
it comes to critical systems it's kind

00:02:04,940 --> 00:02:12,500
of a no-go to just rely on those basic

00:02:09,560 --> 00:02:15,409
basic things that a basic slow cloud

00:02:12,500 --> 00:02:18,080
setup provides because any critical

00:02:15,409 --> 00:02:19,849
system for need to account for things

00:02:18,080 --> 00:02:22,610
like hardware failures natural

00:02:19,849 --> 00:02:24,710
calamities and more most importantly and

00:02:22,610 --> 00:02:28,310
the most common of the reasons as to why

00:02:24,710 --> 00:02:30,560
things go down human errors inevitable

00:02:28,310 --> 00:02:32,870
solutions to solve these

00:02:30,560 --> 00:02:35,300
the most common one being backup and

00:02:32,870 --> 00:02:37,640
restore well it doesn't really do

00:02:35,300 --> 00:02:39,080
completely I mean off the job

00:02:37,640 --> 00:02:41,180
also when you're backing up and

00:02:39,080 --> 00:02:43,340
restoring unique you should be backing

00:02:41,180 --> 00:02:45,860
it up to a data center oh that's that's

00:02:43,340 --> 00:02:47,750
outside of your primary data center only

00:02:45,860 --> 00:02:50,120
because if something were to happen to

00:02:47,750 --> 00:02:52,910
your data center you need somewhere to

00:02:50,120 --> 00:02:56,120
restore your data from so that comes

00:02:52,910 --> 00:02:59,000
with a lot of caveats in addition to the

00:02:56,120 --> 00:03:01,370
fact that it it doesn't provide

00:02:59,000 --> 00:03:03,560
everything that you might need from from

00:03:01,370 --> 00:03:06,670
our solution for that supports a

00:03:03,560 --> 00:03:10,330
critical system so there comes across TC

00:03:06,670 --> 00:03:12,890
replication which not only provides

00:03:10,330 --> 00:03:15,800
guarantees around availability but also

00:03:12,890 --> 00:03:18,950
provides things like scalability and

00:03:15,800 --> 00:03:21,140
reduced latency so for example you know

00:03:18,950 --> 00:03:23,240
if you were running a service that was

00:03:21,140 --> 00:03:25,250
supposed to only clear to North America

00:03:23,240 --> 00:03:27,350
and you had your data sitting in data

00:03:25,250 --> 00:03:28,940
centers across the u.s. all of a sudden

00:03:27,350 --> 00:03:32,390
you had to provide some sort of support

00:03:28,940 --> 00:03:34,490
to order some you were launching the

00:03:32,390 --> 00:03:37,180
same service making this data accessible

00:03:34,490 --> 00:03:42,770
to people and in Europe you could set up

00:03:37,180 --> 00:03:44,840
across DC replicated solar cluster in a

00:03:42,770 --> 00:03:46,790
data center in Europe and that would

00:03:44,840 --> 00:03:54,800
allow for you to have lower latencies

00:03:46,790 --> 00:03:58,310
for users who are in Europe so what what

00:03:54,800 --> 00:04:02,450
is cross this your application in the

00:03:58,310 --> 00:04:05,480
most simplest simplest of terms it's

00:04:02,450 --> 00:04:07,280
nothing but a means to achieve the Sun

00:04:05,480 --> 00:04:09,770
of mirroring effect across data centers

00:04:07,280 --> 00:04:13,250
for solar clusters so anything that you

00:04:09,770 --> 00:04:15,260
video ingest in data center one should

00:04:13,250 --> 00:04:18,620
show up in data center two for it to be

00:04:15,260 --> 00:04:20,540
searched on retrieved or whatever so

00:04:18,620 --> 00:04:24,380
that's the most that's a basic

00:04:20,540 --> 00:04:28,100
understanding basic need of why you

00:04:24,380 --> 00:04:33,920
might want the CDC across data center

00:04:28,100 --> 00:04:36,170
application so now we're going to talk

00:04:33,920 --> 00:04:39,650
about the different approaches to

00:04:36,170 --> 00:04:43,850
achieving cross data center replication

00:04:39,650 --> 00:04:47,140
the first one the client base replicate

00:04:43,850 --> 00:04:50,690
is something that we started off with

00:04:47,140 --> 00:04:53,390
with an apple and it can't predates all

00:04:50,690 --> 00:04:55,370
other solutions because it doesn't

00:04:53,390 --> 00:04:58,610
really require too much to be built on

00:04:55,370 --> 00:05:01,190
it it does rely on your users being wise

00:04:58,610 --> 00:05:04,580
it does rely on your users taking

00:05:01,190 --> 00:05:06,650
responsibility so this this solution

00:05:04,580 --> 00:05:09,100
kind of predates everything else that

00:05:06,650 --> 00:05:13,310
was built to work out of the box or

00:05:09,100 --> 00:05:19,850
things that for support something like

00:05:13,310 --> 00:05:21,500
this if you look at what happens in a in

00:05:19,850 --> 00:05:25,040
a client based data central application

00:05:21,500 --> 00:05:27,740
as obvious it's the responsibility for

00:05:25,040 --> 00:05:32,900
everything relies on the client in such

00:05:27,740 --> 00:05:34,760
case and the client is basically

00:05:32,900 --> 00:05:37,130
responsible for managing external

00:05:34,760 --> 00:05:38,930
versions so it does provide some form of

00:05:37,130 --> 00:05:43,700
not some form of but a pretty good

00:05:38,930 --> 00:05:45,380
optimistic concurrency and so as long as

00:05:43,700 --> 00:05:47,750
you you're worsening your documents

00:05:45,380 --> 00:05:51,290
correctly so there's gonna take care of

00:05:47,750 --> 00:05:52,910
things for you but but the versioning

00:05:51,290 --> 00:05:54,730
has to come from the client in a setup

00:05:52,910 --> 00:05:59,150
that relies on the client to provide

00:05:54,730 --> 00:06:01,280
cross data central replication it and

00:05:59,150 --> 00:06:04,100
not only that in this case client would

00:06:01,280 --> 00:06:10,460
also have to plan also ends of managing

00:06:04,100 --> 00:06:12,530
request failures in three tries so the

00:06:10,460 --> 00:06:15,290
challenges in such a system though are

00:06:12,530 --> 00:06:19,250
if you had if you had a client sending

00:06:15,290 --> 00:06:29,420
data and say it succeeds in the local

00:06:19,250 --> 00:06:33,320
data center that's activator so if you

00:06:29,420 --> 00:06:35,930
had a client sending data to your data

00:06:33,320 --> 00:06:37,940
center and it succeeds and one of the

00:06:35,930 --> 00:06:41,300
data centers but does not succeed on the

00:06:37,940 --> 00:06:43,670
other data center it's the clients

00:06:41,300 --> 00:06:46,670
responsibility to make sure that that

00:06:43,670 --> 00:06:49,580
that failures taken care of and the data

00:06:46,670 --> 00:06:53,870
is synchronized and consistency is

00:06:49,580 --> 00:06:55,780
checked and users are alerted when when

00:06:53,870 --> 00:06:58,360
the data doesn't make it through to

00:06:55,780 --> 00:07:00,550
you know across all the data centers not

00:06:58,360 --> 00:07:03,910
only that it's all the process has to be

00:07:00,550 --> 00:07:06,610
synchronous some extent the reason being

00:07:03,910 --> 00:07:10,240
if the client sends data to data center

00:07:06,610 --> 00:07:12,130
one hears back it's positive acts since

00:07:10,240 --> 00:07:15,130
data to data center two doesn't hear

00:07:12,130 --> 00:07:16,840
back it's its responsibility to close

00:07:15,130 --> 00:07:19,180
that loop and either delete that data

00:07:16,840 --> 00:07:22,419
from data center one or wait until data

00:07:19,180 --> 00:07:26,200
center 2 also has that data to

00:07:22,419 --> 00:07:28,360
positively acknowledge that request so

00:07:26,200 --> 00:07:32,820
it kind of makes it rather difficult for

00:07:28,360 --> 00:07:35,200
the client to operate in such a mode

00:07:32,820 --> 00:07:37,210
also when it when a data center goes

00:07:35,200 --> 00:07:40,030
down so for example if data center to go

00:07:37,210 --> 00:07:43,950
to go away due to network outage weather

00:07:40,030 --> 00:07:46,450
issues whatever it might be the client

00:07:43,950 --> 00:07:49,900
is going great stuck and that might

00:07:46,450 --> 00:07:53,020
require changes in the configuration for

00:07:49,900 --> 00:07:57,790
the client to make sure that you can go

00:07:53,020 --> 00:07:59,560
ahead and you can go ahead and forget

00:07:57,790 --> 00:08:02,620
those problems and ignore those problems

00:07:59,560 --> 00:08:05,440
for the client to understand that that

00:08:02,620 --> 00:08:08,440
it's okay to ignore those failures might

00:08:05,440 --> 00:08:10,960
require a config change in in such a

00:08:08,440 --> 00:08:13,870
case so pilot while the system looks

00:08:10,960 --> 00:08:16,690
very simple to begin with in my opinion

00:08:13,870 --> 00:08:19,360
the cost to operate this is pretty high

00:08:16,690 --> 00:08:21,220
especially if you are going to end up

00:08:19,360 --> 00:08:23,770
using it at any given point in time and

00:08:21,220 --> 00:08:26,320
that that comes from our experience

00:08:23,770 --> 00:08:28,570
having asked our users to start off with

00:08:26,320 --> 00:08:33,820
this vicar's of lack of any other

00:08:28,570 --> 00:08:36,150
solution a long long time ago and it's

00:08:33,820 --> 00:08:39,610
kind of still okay if you're doing this

00:08:36,150 --> 00:08:41,680
by yourself for a sort of cluster that

00:08:39,610 --> 00:08:43,360
you're the user of but kind of gets

00:08:41,680 --> 00:08:45,790
really difficult when you're just a

00:08:43,360 --> 00:08:49,740
provider of the platform than others use

00:08:45,790 --> 00:08:57,580
because as I said in one of my talks I

00:08:49,740 --> 00:09:00,870
think last year sorry I've said in one

00:08:57,580 --> 00:09:00,870
of my talks last year

00:09:04,780 --> 00:09:10,930
is that when you're finding a platform

00:09:08,110 --> 00:09:14,050
your hue servers are not your best

00:09:10,930 --> 00:09:16,810
friends so there they want the crates

00:09:14,050 --> 00:09:18,790
table of your running system they want

00:09:16,810 --> 00:09:20,530
their best the best performance to come

00:09:18,790 --> 00:09:22,540
out of it but they're not they're

00:09:20,530 --> 00:09:25,420
willing to take you down if that means

00:09:22,540 --> 00:09:27,610
you know a marginal improvement for

00:09:25,420 --> 00:09:29,830
theory use case so they're not they're

00:09:27,610 --> 00:09:33,940
not bothered but they will bother you

00:09:29,830 --> 00:09:37,270
when it comes to resolving a problem and

00:09:33,940 --> 00:09:39,010
so you know in such a system where

00:09:37,270 --> 00:09:41,290
you're relying on your client to send in

00:09:39,010 --> 00:09:43,270
these this data and make sure that it's

00:09:41,290 --> 00:09:45,160
consistently sent across multiple data

00:09:43,270 --> 00:09:48,550
centers and cleaned up when there's need

00:09:45,160 --> 00:09:52,090
to do so that problem gets amplified

00:09:48,550 --> 00:09:55,360
when you're running a platform so I

00:09:52,090 --> 00:09:58,060
really feel that in in platforms this

00:09:55,360 --> 00:10:00,880
value kind of goes not it not to zero

00:09:58,060 --> 00:10:02,560
but a time stood to negative because

00:10:00,880 --> 00:10:05,140
you're firefighting and trying to figure

00:10:02,560 --> 00:10:07,120
out how to now heal and fix these

00:10:05,140 --> 00:10:09,280
problems that were created by users who

00:10:07,120 --> 00:10:15,550
did not understand the implication of

00:10:09,280 --> 00:10:19,180
what they were doing so that brings me

00:10:15,550 --> 00:10:21,370
to the to the next approach so while

00:10:19,180 --> 00:10:24,670
that was something that I know

00:10:21,370 --> 00:10:26,170
well we haven't were doing and a few

00:10:24,670 --> 00:10:28,480
other companies were also doing things

00:10:26,170 --> 00:10:31,030
that way a bunch of people in the

00:10:28,480 --> 00:10:33,640
community realized there was a need for

00:10:31,030 --> 00:10:41,590
a genuine cross data center replication

00:10:33,640 --> 00:10:44,170
solution within solar and and so a few

00:10:41,590 --> 00:10:46,450
folks in the community Erik Erikson I

00:10:44,170 --> 00:10:49,270
guess primarily Oh fanned out and

00:10:46,450 --> 00:10:51,610
started working on across DC replication

00:10:49,270 --> 00:10:55,510
mechanism that would be supported by

00:10:51,610 --> 00:10:59,080
solar out out of the box and the way it

00:10:55,510 --> 00:11:01,660
works is any request that comes in and

00:10:59,080 --> 00:11:03,760
remember most of these requests that I'm

00:11:01,660 --> 00:11:05,980
looking at or either admin requests that

00:11:03,760 --> 00:11:09,910
make changes to the state of the system

00:11:05,980 --> 00:11:12,760
or their requests that are there update

00:11:09,910 --> 00:11:14,260
requests anything outside of that does

00:11:12,760 --> 00:11:15,560
not really need

00:11:14,260 --> 00:11:18,420
[Music]

00:11:15,560 --> 00:11:21,540
replication for example select ways have

00:11:18,420 --> 00:11:23,100
no need to be replicated to go anywhere

00:11:21,540 --> 00:11:27,959
other than the data center of their

00:11:23,100 --> 00:11:30,540
they're sent to and so to collection

00:11:27,959 --> 00:11:33,899
admin requests that cost no state

00:11:30,540 --> 00:11:36,899
changes to the cluster itself so when a

00:11:33,899 --> 00:11:39,600
request comes in the data centers are

00:11:36,899 --> 00:11:42,899
working in isolation part they to have

00:11:39,600 --> 00:11:46,790
some knowledge about each other so the

00:11:42,899 --> 00:11:50,790
shard in this example the shard one

00:11:46,790 --> 00:11:53,339
leader is going to not only index things

00:11:50,790 --> 00:11:56,399
locally and send them to the follower

00:11:53,339 --> 00:11:59,730
but is also going to somehow manage to

00:11:56,399 --> 00:12:02,550
send this data across the pipe so that

00:11:59,730 --> 00:12:05,820
across DC replication happens to the

00:12:02,550 --> 00:12:08,160
leader of the corresponding shard in

00:12:05,820 --> 00:12:11,100
another tanker now if there are more

00:12:08,160 --> 00:12:14,430
than one of these kinds this leader is

00:12:11,100 --> 00:12:17,160
going to ensure that it reaches every

00:12:14,430 --> 00:12:24,270
other leader that should have this data

00:12:17,160 --> 00:12:28,920
this document so in terms of the code

00:12:24,270 --> 00:12:31,649
path the way it really works is the

00:12:28,920 --> 00:12:33,390
request comes in and for our few who are

00:12:31,649 --> 00:12:34,410
of the era of how sort of works there's

00:12:33,390 --> 00:12:36,260
something called a update request

00:12:34,410 --> 00:12:39,930
processor chain which is nothing but a

00:12:36,260 --> 00:12:42,779
but a pipeline of sort that works to

00:12:39,930 --> 00:12:46,170
process if every incoming update and so

00:12:42,779 --> 00:12:47,670
on the leader it comes in gets processed

00:12:46,170 --> 00:12:50,760
through the update request processor

00:12:47,670 --> 00:12:52,740
chain and then gets dumped into the

00:12:50,760 --> 00:12:59,310
transaction log transaction log and

00:12:52,740 --> 00:13:01,110
solar is is basically it's a log it's a

00:12:59,310 --> 00:13:03,720
log of all the updates that are coming

00:13:01,110 --> 00:13:06,029
in and there is a thread that's running

00:13:03,720 --> 00:13:10,470
on side of the leader that called the

00:13:06,029 --> 00:13:13,470
CDCR replicator thread that tracks what

00:13:10,470 --> 00:13:15,839
are the replicated data centers data

00:13:13,470 --> 00:13:19,079
center clusters or CDCR clusters and

00:13:15,839 --> 00:13:22,490
reads data of this transaction log and

00:13:19,079 --> 00:13:25,890
based on some basic configuration around

00:13:22,490 --> 00:13:28,050
bandwidth utilization and other other

00:13:25,890 --> 00:13:28,980
things reach data from the transaction

00:13:28,050 --> 00:13:31,949
log and

00:13:28,980 --> 00:13:35,490
is that out to the leader of the target

00:13:31,949 --> 00:13:37,920
DC leader on the target DC the

00:13:35,490 --> 00:13:39,630
difference on the target DC being it's

00:13:37,920 --> 00:13:42,540
smart enough to make sure that it

00:13:39,630 --> 00:13:46,800
doesn't this data does not does not come

00:13:42,540 --> 00:13:52,829
back to the primary DC that originally

00:13:46,800 --> 00:13:55,440
sent this data so in that in our diagram

00:13:52,829 --> 00:13:57,720
what we sees that the transaction log

00:13:55,440 --> 00:13:59,550
basically is acting like a cue because

00:13:57,720 --> 00:14:01,290
everything that's indexed locally is

00:13:59,550 --> 00:14:03,930
going into the transaction log kind of

00:14:01,290 --> 00:14:06,570
getting buffered there and then being

00:14:03,930 --> 00:14:07,920
read by by the thread that's responsible

00:14:06,570 --> 00:14:09,600
to send out all of these updates to

00:14:07,920 --> 00:14:13,380
everyone who spoke was supposed to be

00:14:09,600 --> 00:14:16,170
replicating and we do get and eventually

00:14:13,380 --> 00:14:18,180
consistently our cluster that supports

00:14:16,170 --> 00:14:20,550
both active passive and active active

00:14:18,180 --> 00:14:23,160
set ups but with a bunch of caveat

00:14:20,550 --> 00:14:25,889
especially because the versioning in in

00:14:23,160 --> 00:14:28,170
solar CDC our solution relies on a

00:14:25,889 --> 00:14:30,420
clocks and so if you have data centers

00:14:28,170 --> 00:14:32,820
who have clocks let us think that might

00:14:30,420 --> 00:14:38,579
be that might be kind of complicated or

00:14:32,820 --> 00:14:40,470
or scary like the best part in my

00:14:38,579 --> 00:14:42,329
opinion here is that its standalone

00:14:40,470 --> 00:14:44,130
which basically means it translates to

00:14:42,329 --> 00:14:45,690
no external dependency you don't really

00:14:44,130 --> 00:14:47,699
need anything else running on the side

00:14:45,690 --> 00:14:50,699
you don't need to rely on a third party

00:14:47,699 --> 00:14:54,300
system also because it's part of solar

00:14:50,699 --> 00:14:56,180
you do get community support from others

00:14:54,300 --> 00:14:59,430
we're using the exact same solution

00:14:56,180 --> 00:15:02,699
instead of having their own custom

00:14:59,430 --> 00:15:05,420
solutions trying to replicate data from

00:15:02,699 --> 00:15:05,420
one place to the other

00:15:07,190 --> 00:15:15,569
so the rip but obviously this comes with

00:15:12,480 --> 00:15:17,519
limitations one of them being the

00:15:15,569 --> 00:15:19,769
transaction log considering its use of

00:15:17,519 --> 00:15:22,850
the queue if your data if your target

00:15:19,769 --> 00:15:25,230
data center for some reason goes away

00:15:22,850 --> 00:15:27,810
your primary data center is going to

00:15:25,230 --> 00:15:29,670
accumulate all all the incoming updates

00:15:27,810 --> 00:15:31,529
in its transaction log without purging

00:15:29,670 --> 00:15:33,660
them only because it can't

00:15:31,529 --> 00:15:36,690
that's the only place from fair to

00:15:33,660 --> 00:15:39,899
through free play and replicate stuff to

00:15:36,690 --> 00:15:41,880
target data centers and that that

00:15:39,899 --> 00:15:42,750
certainly Mike Ranney are at risk on

00:15:41,880 --> 00:15:45,240
your privateer

00:15:42,750 --> 00:15:47,280
Center the configuration on this one is

00:15:45,240 --> 00:15:49,260
not easy for some reason there's

00:15:47,280 --> 00:15:51,210
collection level configs and cluster

00:15:49,260 --> 00:15:53,370
level configs and a bunch of other

00:15:51,210 --> 00:15:57,000
configs that kind of make it harder to

00:15:53,370 --> 00:15:58,800
use in my opinion and then the

00:15:57,000 --> 00:16:00,270
transaction log approach doesn't really

00:15:58,800 --> 00:16:02,190
catch up because you're looking at a

00:16:00,270 --> 00:16:03,990
file based transaction log that's

00:16:02,190 --> 00:16:08,490
sitting on the system

00:16:03,990 --> 00:16:10,350
OAH which is okay for small use cases

00:16:08,490 --> 00:16:13,380
that don't have a ton of data coming in

00:16:10,350 --> 00:16:14,790
every second but if the target cluster

00:16:13,380 --> 00:16:16,710
is gone and you have a ton of data

00:16:14,790 --> 00:16:20,070
coming in the transaction log will grow

00:16:16,710 --> 00:16:23,180
to a size that those CDCR thread is not

00:16:20,070 --> 00:16:26,010
going to be able to catch up with so

00:16:23,180 --> 00:16:31,770
it's kind of a no-go for any

00:16:26,010 --> 00:16:33,810
high-throughput situation and then the

00:16:31,770 --> 00:16:35,370
the support for implicit wrapping is

00:16:33,810 --> 00:16:38,400
missing which what that basically means

00:16:35,370 --> 00:16:40,320
in solar is if you are not using solar

00:16:38,400 --> 00:16:44,520
scoffs that idea routers or hash-based

00:16:40,320 --> 00:16:47,960
traveler robbers if you're trying to

00:16:44,520 --> 00:16:51,210
send data explicitly to a specific shard

00:16:47,960 --> 00:16:53,339
that will not get replicated by the CDCR

00:16:51,210 --> 00:16:56,460
that is that comes out of the box with

00:16:53,339 --> 00:17:00,150
solar because that's not supported the

00:16:56,460 --> 00:17:03,420
metrics that CDCR supports also get

00:17:00,150 --> 00:17:06,660
reset on restart a lot of those metrics

00:17:03,420 --> 00:17:08,520
are okay to be reset but there are times

00:17:06,660 --> 00:17:10,650
when you might need historical data

00:17:08,520 --> 00:17:14,490
which will go away as soon as your solar

00:17:10,650 --> 00:17:16,199
instances bounced and obviously there's

00:17:14,490 --> 00:17:18,270
an extra burden on chart leaders because

00:17:16,199 --> 00:17:20,100
they're responsible for replicating all

00:17:18,270 --> 00:17:25,670
of this data across to the other data

00:17:20,100 --> 00:17:29,190
center so why spoke about all of that

00:17:25,670 --> 00:17:32,340
one interesting thing to know is that we

00:17:29,190 --> 00:17:34,830
never really used at Apple CCR that was

00:17:32,340 --> 00:17:36,600
offered by solar not because we had

00:17:34,830 --> 00:17:39,360
concerns I mean there were certainly

00:17:36,600 --> 00:17:42,960
things that we were concerned about but

00:17:39,360 --> 00:17:47,520
because while the fever runs on the

00:17:42,960 --> 00:17:50,280
first approach to solving CDC are by

00:17:47,520 --> 00:17:51,900
using client-side replication we had

00:17:50,280 --> 00:17:53,730
already started working on our own

00:17:51,900 --> 00:17:55,150
internal solution that was closely

00:17:53,730 --> 00:17:59,410
coupled with

00:17:55,150 --> 00:18:01,960
with what what we needed and while that

00:17:59,410 --> 00:18:04,420
happened the community-based CDC are

00:18:01,960 --> 00:18:07,720
also was introduced but we already had

00:18:04,420 --> 00:18:09,130
our own version of CDC are that we

00:18:07,720 --> 00:18:10,990
thought was doing the job pretty well

00:18:09,130 --> 00:18:13,990
and was also interesting a lot of these

00:18:10,990 --> 00:18:16,059
problems that I just spoke about so this

00:18:13,990 --> 00:18:19,300
third and the last part is pretty much

00:18:16,059 --> 00:18:22,690
that covering and touching upon those

00:18:19,300 --> 00:18:28,690
things so we've been using this for

00:18:22,690 --> 00:18:31,059
think about for five years now and what

00:18:28,690 --> 00:18:34,059
it provides is it writes an easy way to

00:18:31,059 --> 00:18:36,850
replicate you know n ways across n data

00:18:34,059 --> 00:18:42,130
centers in an active-active manner it

00:18:36,850 --> 00:18:46,080
also allows for provides retries and

00:18:42,130 --> 00:18:51,090
error handling error handling for failed

00:18:46,080 --> 00:18:55,559
requests across data centers and

00:18:51,090 --> 00:18:58,510
consistency isn't it wasn't a sorry

00:18:55,559 --> 00:19:01,780
that's wrong I mean consistency in this

00:18:58,510 --> 00:19:03,880
system is checked it's not guaranteed

00:19:01,780 --> 00:19:08,050
obviously because if something happens

00:19:03,880 --> 00:19:09,520
and you get out of sync the least that

00:19:08,050 --> 00:19:11,710
would happen is that he would get no

00:19:09,520 --> 00:19:13,540
award n't a consistency we have systems

00:19:11,710 --> 00:19:17,470
in place therefore that would allow us

00:19:13,540 --> 00:19:19,780
to fix it but it and the very least

00:19:17,470 --> 00:19:24,670
informs us of consistencies when they

00:19:19,780 --> 00:19:28,540
happen and insights were missing from

00:19:24,670 --> 00:19:31,120
the previous rep trustee see replication

00:19:28,540 --> 00:19:34,420
things like latency consistency errors

00:19:31,120 --> 00:19:35,860
and retry counts but in this case we

00:19:34,420 --> 00:19:41,410
build a system where we get all of those

00:19:35,860 --> 00:19:45,330
numbers as well so how is this this set

00:19:41,410 --> 00:19:50,620
up look so the clock this is the basic

00:19:45,330 --> 00:19:53,710
architecture diagram basic architecture

00:19:50,620 --> 00:19:56,440
diagram for what we use and the Hughes

00:19:53,710 --> 00:20:00,480
Kafka people trustees your application

00:19:56,440 --> 00:20:03,340
using proprietary cues but also Kafka

00:20:00,480 --> 00:20:06,460
and when a request from the client comes

00:20:03,340 --> 00:20:07,070
in it comes into the primary or whatever

00:20:06,460 --> 00:20:10,390
is this

00:20:07,070 --> 00:20:13,760
targeted solar DC that it comes into

00:20:10,390 --> 00:20:17,960
that is its own local zookeeper and it

00:20:13,760 --> 00:20:20,390
has its own local cue so if you look at

00:20:17,960 --> 00:20:23,620
the closely coupled orange yellow and

00:20:20,390 --> 00:20:26,600
the green boxes they're together in one

00:20:23,620 --> 00:20:28,580
one data center and then there's a

00:20:26,600 --> 00:20:32,150
mirror in the middle that mirrors this

00:20:28,580 --> 00:20:33,680
onto another cue and sends this data to

00:20:32,150 --> 00:20:35,360
be consumed by something called the

00:20:33,680 --> 00:20:36,890
cross DC consumer which is a standalone

00:20:35,360 --> 00:20:42,260
app and we're going to get to that in a

00:20:36,890 --> 00:20:45,680
bit and that runs in isolation again and

00:20:42,260 --> 00:20:47,990
has no idea about the existence of off

00:20:45,680 --> 00:20:51,410
the data center that this request

00:20:47,990 --> 00:20:53,990
originally came in came into so what

00:20:51,410 --> 00:20:58,700
this gives us is the ability to isolate

00:20:53,990 --> 00:21:05,930
the Q and a mirror from from all of the

00:20:58,700 --> 00:21:08,630
stuff that is best solar so see we're

00:21:05,930 --> 00:21:11,930
running short on time now so data flow

00:21:08,630 --> 00:21:13,730
in in this trusty C plugin kind of is

00:21:11,930 --> 00:21:17,930
implemented as an update request process

00:21:13,730 --> 00:21:20,480
or when a client sends in a request it

00:21:17,930 --> 00:21:23,330
comes in where the doc version n an ID

00:21:20,480 --> 00:21:27,800
and the receiving data center accepts it

00:21:23,330 --> 00:21:30,140
only if the received doc version is more

00:21:27,800 --> 00:21:32,810
than the already existing version for

00:21:30,140 --> 00:21:35,600
that document if that document exists in

00:21:32,810 --> 00:21:37,610
the index already and then as and then

00:21:35,600 --> 00:21:39,800
solar obviously assigns its own version

00:21:37,610 --> 00:21:42,980
value which is used internally where's

00:21:39,800 --> 00:21:45,200
my solar but what this this update

00:21:42,980 --> 00:21:48,620
request processor does is it strips off

00:21:45,200 --> 00:21:54,440
this version field and and inserts this

00:21:48,620 --> 00:21:56,450
into the Q into the source Q on on the

00:21:54,440 --> 00:21:59,390
source or the originating data center

00:21:56,450 --> 00:22:01,730
which is then replicated and copied over

00:21:59,390 --> 00:22:08,420
to the other other data center by the

00:22:01,730 --> 00:22:11,740
mirror in the middle so the its then

00:22:08,420 --> 00:22:14,540
received by the cross DC consumer and

00:22:11,740 --> 00:22:15,650
the job the cross DC consumer is pretty

00:22:14,540 --> 00:22:17,780
straightforward it's a very

00:22:15,650 --> 00:22:20,220
straightforward simple app what it does

00:22:17,780 --> 00:22:21,929
is it reads from

00:22:20,220 --> 00:22:23,309
the destination queue which is isolated

00:22:21,929 --> 00:22:27,210
from the source here because the mirror

00:22:23,309 --> 00:22:30,539
just mirrors stuff based on config and

00:22:27,210 --> 00:22:33,809
then it reads that data and it tries to

00:22:30,539 --> 00:22:36,590
send these updates to solar now this

00:22:33,809 --> 00:22:44,130
does request could in isolation

00:22:36,590 --> 00:22:47,580
succeed or fail fee over time learnt by

00:22:44,130 --> 00:22:49,770
large error by making a lot of errors as

00:22:47,580 --> 00:22:52,020
to what makes sense to be retried and

00:22:49,770 --> 00:22:55,530
what makes sense to be resubmitted or

00:22:52,020 --> 00:22:57,720
discarded so one of the one of the these

00:22:55,530 --> 00:22:59,789
things happen when when a request is

00:22:57,720 --> 00:23:01,799
processed by the trustee see consumer

00:22:59,789 --> 00:23:04,650
they either are successful requests or

00:23:01,799 --> 00:23:06,659
their failed request that should not be

00:23:04,650 --> 00:23:08,789
retried anymore or their temporary

00:23:06,659 --> 00:23:10,980
failures that could be fixed and so

00:23:08,789 --> 00:23:14,760
these are then resubmitted onto a

00:23:10,980 --> 00:23:17,100
separate topic and we submit these with

00:23:14,760 --> 00:23:18,570
with a time stamp on when this was lost

00:23:17,100 --> 00:23:22,230
retried how many times it has been

00:23:18,570 --> 00:23:25,350
retried so far so that or when it comes

00:23:22,230 --> 00:23:26,880
in again to be free retried win no how

00:23:25,350 --> 00:23:28,880
many times has it been three tried to

00:23:26,880 --> 00:23:31,610
limit

00:23:28,880 --> 00:23:34,630
at or alert if if request has been

00:23:31,610 --> 00:23:34,630
retried too many times

00:23:34,900 --> 00:23:41,630
without any success and we put some form

00:23:39,140 --> 00:23:43,310
of an exponential back-off algorithm in

00:23:41,630 --> 00:23:46,310
there to make sure that the same request

00:23:43,310 --> 00:23:47,750
doesn't get get retried too often I know

00:23:46,310 --> 00:23:48,890
and the two kinds of requests that

00:23:47,750 --> 00:23:50,960
primarily if we finally ended up

00:23:48,890 --> 00:23:54,860
rejecting outrightly are the four nines

00:23:50,960 --> 00:23:56,180
which is basically something is a case

00:23:54,860 --> 00:23:58,700
where sort of would go out and say that

00:23:56,180 --> 00:24:00,260
based on optimistic concurrency the you

00:23:58,700 --> 00:24:02,330
know the version that you're sending for

00:24:00,260 --> 00:24:04,670
this document is older than what I

00:24:02,330 --> 00:24:06,770
already have which means there's no need

00:24:04,670 --> 00:24:09,710
to send me this update I already have

00:24:06,770 --> 00:24:11,150
newer version of this document or in

00:24:09,710 --> 00:24:13,250
case of a collection creation come on

00:24:11,150 --> 00:24:17,270
that that fails because the collection

00:24:13,250 --> 00:24:19,160
already exists because if the way it's

00:24:17,270 --> 00:24:23,660
set up right now is the primary DC's

00:24:19,160 --> 00:24:25,820
submits these requests into the queue if

00:24:23,660 --> 00:24:28,040
for some reason it failed or passed and

00:24:25,820 --> 00:24:30,170
two of these requests were submitted to

00:24:28,040 --> 00:24:32,810
the primary DC and made it into the

00:24:30,170 --> 00:24:35,630
queue they would be received by the

00:24:32,810 --> 00:24:37,520
receiving DC's they should just be

00:24:35,630 --> 00:24:39,590
rejected because they they're kind of

00:24:37,520 --> 00:24:48,070
important they wouldn't cause any damage

00:24:39,590 --> 00:24:53,330
but there's no point retrying them and

00:24:48,070 --> 00:24:56,150
the other good thing here is the envy of

00:24:53,330 --> 00:24:59,000
replication which kind of makes it easy

00:24:56,150 --> 00:25:00,710
and isolates everything so if you look

00:24:59,000 --> 00:25:03,020
at the solar cluster at the top the

00:25:00,710 --> 00:25:04,760
yellow and the red block the yellow red

00:25:03,020 --> 00:25:07,040
and green boxes that are around each

00:25:04,760 --> 00:25:09,530
other or kind of they work in isolation

00:25:07,040 --> 00:25:14,060
so the solar cluster writes to a load to

00:25:09,530 --> 00:25:17,570
the source topic solar also works a long

00:25:14,060 --> 00:25:19,640
way the cross DC consumer which reads

00:25:17,570 --> 00:25:21,140
from a destination topic and then

00:25:19,640 --> 00:25:23,480
there's a mirror sitting in the middle

00:25:21,140 --> 00:25:25,670
that's configured to just trade off of

00:25:23,480 --> 00:25:28,430
all sort of a source topic and replica

00:25:25,670 --> 00:25:30,050
and copy this data or into multiple

00:25:28,430 --> 00:25:33,920
destination topics so if you wanted to

00:25:30,050 --> 00:25:37,190
add another another data center yes

00:25:33,920 --> 00:25:40,040
there's some some effort that would be

00:25:37,190 --> 00:25:41,840
required to bootstrap these but once you

00:25:40,040 --> 00:25:42,800
have your indexes book strapped all you

00:25:41,840 --> 00:25:45,470
need to do is

00:25:42,800 --> 00:25:47,570
set up or add a convict with a mirror

00:25:45,470 --> 00:25:49,760
set up a destination topic start a

00:25:47,570 --> 00:25:50,930
crossed easy consumer and you'd have a

00:25:49,760 --> 00:25:54,140
sort of cluster that would now be

00:25:50,930 --> 00:25:59,420
receiving updates from from other sort

00:25:54,140 --> 00:26:01,310
of clusters and also mirror is something

00:25:59,420 --> 00:26:06,070
that's lightweight and is kind of

00:26:01,310 --> 00:26:06,070
provided by most most queue systems

00:26:06,280 --> 00:26:11,690
handling of elite requests is a little

00:26:08,960 --> 00:26:13,790
different because they're not versioned

00:26:11,690 --> 00:26:17,000
relate by queries are not handled

00:26:13,790 --> 00:26:20,660
they're not supported deeper IDs are

00:26:17,000 --> 00:26:25,670
however supported but it works as a

00:26:20,660 --> 00:26:28,130
tombstone and this to zoning is very

00:26:25,670 --> 00:26:32,540
different from listening to stones in a

00:26:28,130 --> 00:26:34,910
sense that the document is left there as

00:26:32,540 --> 00:26:36,530
per if you were to compare to a leucine

00:26:34,910 --> 00:26:40,910
document the document is left there

00:26:36,530 --> 00:26:43,280
active and alive further in terms of the

00:26:40,910 --> 00:26:45,800
scene but in terms of the the business

00:26:43,280 --> 00:26:47,390
use case its marked as deleted so that

00:26:45,800 --> 00:26:49,670
none of the requests come back with

00:26:47,390 --> 00:26:51,850
these documents the reason why this was

00:26:49,670 --> 00:26:54,470
done this way was to ensure that any

00:26:51,850 --> 00:26:58,070
accidental delete request that was sent

00:26:54,470 --> 00:27:01,310
to a primary DC does not blindly delete

00:26:58,070 --> 00:27:02,570
stuff and replicate that stuff because

00:27:01,310 --> 00:27:05,570
that would defeat the purpose of having

00:27:02,570 --> 00:27:08,630
a TR strategy because it wouldn't

00:27:05,570 --> 00:27:11,080
address that that human error so you

00:27:08,630 --> 00:27:14,600
could go back and undo leave these but

00:27:11,080 --> 00:27:16,970
if we generally never need this which

00:27:14,600 --> 00:27:19,400
allows us to run path jobs to clean up

00:27:16,970 --> 00:27:28,300
all of these tombstone documents on a

00:27:19,400 --> 00:27:30,680
regular basis inconsistency detection

00:27:28,300 --> 00:27:33,200
which is kind of super critical because

00:27:30,680 --> 00:27:36,800
if you have a crosstie C set up up and

00:27:33,200 --> 00:27:38,450
running but you don't know if the data

00:27:36,800 --> 00:27:40,880
across your data centers is actually

00:27:38,450 --> 00:27:43,880
replicated whether or not you you don't

00:27:40,880 --> 00:27:47,120
really know whether you can use it and

00:27:43,880 --> 00:27:50,240
in case of complicated systems that

00:27:47,120 --> 00:27:52,970
might lead to to a situation where

00:27:50,240 --> 00:27:55,490
you're investing a ton of money into

00:27:52,970 --> 00:27:56,120
large D are clusters that are sitting

00:27:55,490 --> 00:27:58,580
but data

00:27:56,120 --> 00:28:00,680
is inconsistent and unusable so you

00:27:58,580 --> 00:28:02,540
didn't need an inconsistency detection

00:28:00,680 --> 00:28:04,430
mechanism at the very least to make sure

00:28:02,540 --> 00:28:07,430
that all the money that you have

00:28:04,430 --> 00:28:12,880
invested in setting up a dr cluster is

00:28:07,430 --> 00:28:19,490
is actually ready to be used or ready to

00:28:12,880 --> 00:28:21,320
or it's kind of making sense so as I

00:28:19,490 --> 00:28:23,570
said detection is the more essential

00:28:21,320 --> 00:28:25,070
part of it healing not so much because

00:28:23,570 --> 00:28:27,440
there are multiple ways to heal you can

00:28:25,070 --> 00:28:29,240
replicate again also if you're having

00:28:27,440 --> 00:28:32,150
multiple having this problem over and

00:28:29,240 --> 00:28:34,100
over again that's a bigger problem to

00:28:32,150 --> 00:28:36,620
look at as to why you're cropping data

00:28:34,100 --> 00:28:40,490
or why do you even have this is this

00:28:36,620 --> 00:28:42,110
inconsistency but otherwise all you need

00:28:40,490 --> 00:28:44,720
is a detection mechanism at the very

00:28:42,110 --> 00:28:47,480
least and this detection has to happen

00:28:44,720 --> 00:28:49,130
not on just merely the basis of number

00:28:47,480 --> 00:28:51,400
of documents which a lot of people - as

00:28:49,130 --> 00:28:54,890
a very basic rudimentary approach to

00:28:51,400 --> 00:28:57,380
figuring out if the data across two

00:28:54,890 --> 00:29:00,170
different disease is the same or not but

00:28:57,380 --> 00:29:04,160
what we do is that we we check on every

00:29:00,170 --> 00:29:06,800
document a la document ID and version

00:29:04,160 --> 00:29:10,430
trouble to make sure that that that pair

00:29:06,800 --> 00:29:12,680
exists across all disease and the

00:29:10,430 --> 00:29:15,320
interesting thing in designing a system

00:29:12,680 --> 00:29:19,220
like this is to a is to account for all

00:29:15,320 --> 00:29:22,130
the data that is still in flow so when

00:29:19,220 --> 00:29:25,100
you look at the state of of a primary

00:29:22,130 --> 00:29:28,400
data center or a tailor or potato Center

00:29:25,100 --> 00:29:29,690
X when you look at another data center

00:29:28,400 --> 00:29:31,880
that's supposed to be a mirror of this

00:29:29,690 --> 00:29:34,760
one there would be data that would have

00:29:31,880 --> 00:29:38,210
gone from into between or between these

00:29:34,760 --> 00:29:41,600
two data centers cause a for a different

00:29:38,210 --> 00:29:44,510
version of your data's not sure that

00:29:41,600 --> 00:29:46,040
you'll be looking at so when you're

00:29:44,510 --> 00:29:51,320
designing such a system you need to be

00:29:46,040 --> 00:29:53,300
aware of of this and I think we

00:29:51,320 --> 00:29:55,760
certainly don't have enough time to look

00:29:53,300 --> 00:29:57,980
at this but the way we solve this

00:29:55,760 --> 00:30:01,990
problem is by using Merkle tree based

00:29:57,980 --> 00:30:04,400
implementation and the way it works is

00:30:01,990 --> 00:30:06,020
every document so we have a bunch of

00:30:04,400 --> 00:30:07,930
seeds we define how many seats do we

00:30:06,020 --> 00:30:09,510
have and we define the number of buckets

00:30:07,930 --> 00:30:13,530
or seeds

00:30:09,510 --> 00:30:14,670
just randomized and when we define seeds

00:30:13,530 --> 00:30:18,120
and pockets is basically a

00:30:14,670 --> 00:30:21,210
two-dimensional array the top every

00:30:18,120 --> 00:30:23,780
document in a data center is then hashed

00:30:21,210 --> 00:30:26,760
using each of those seeds and then

00:30:23,780 --> 00:30:29,600
marked in each of those buckets so you

00:30:26,760 --> 00:30:33,270
see one blue cross in each of those rows

00:30:29,600 --> 00:30:35,640
there that represents they talk one or

00:30:33,270 --> 00:30:37,950
the bucket that document belong to when

00:30:35,640 --> 00:30:44,310
they were hashed using seed 1 C 2 and C

00:30:37,950 --> 00:30:46,170
3 so you get this this array based on

00:30:44,310 --> 00:30:49,710
your document your seed and your number

00:30:46,170 --> 00:30:51,690
of pockets and you do the same exercise

00:30:49,710 --> 00:30:54,000
in your secondary data center and you

00:30:51,690 --> 00:30:55,740
compare these two to figure out the

00:30:54,000 --> 00:30:59,130
difference or the missing data between

00:30:55,740 --> 00:31:01,110
both of these now the way we look go

00:30:59,130 --> 00:31:03,090
back to circa to look at whether they're

00:31:01,110 --> 00:31:07,350
consistent or not is not to just read

00:31:03,090 --> 00:31:09,750
run this after say say two minutes but

00:31:07,350 --> 00:31:12,750
what we do is we we have a predefined

00:31:09,750 --> 00:31:15,210
time limit and say okay we know the

00:31:12,750 --> 00:31:17,760
documents that were not visible in the

00:31:15,210 --> 00:31:20,370
last in the last run so what we wanted

00:31:17,760 --> 00:31:22,290
concentrate on now is did those

00:31:20,370 --> 00:31:25,860
documents ever show up if they did show

00:31:22,290 --> 00:31:28,290
up or they are there with the version

00:31:25,860 --> 00:31:30,750
that we saw them originally in the

00:31:28,290 --> 00:31:32,640
primary data center with or with a newer

00:31:30,750 --> 00:31:34,950
version both of these cases are good

00:31:32,640 --> 00:31:36,930
cases but if they either did not show up

00:31:34,950 --> 00:31:40,410
or showed up but had a version that was

00:31:36,930 --> 00:31:42,390
lower then then what we saw the primary

00:31:40,410 --> 00:31:45,060
data center that's a red flag that's

00:31:42,390 --> 00:31:46,860
when we kind of alert for like hey there

00:31:45,060 --> 00:31:48,360
seems to be an inconsistency it's

00:31:46,860 --> 00:31:53,340
something something needs to be done

00:31:48,360 --> 00:31:56,280
about it and the cool part about that

00:31:53,340 --> 00:31:59,270
brooch also is of using such a system is

00:31:56,280 --> 00:32:02,520
you have a queue based system right so

00:31:59,270 --> 00:32:04,500
you can always replay things if you

00:32:02,520 --> 00:32:06,990
think you missed out and stop on

00:32:04,500 --> 00:32:10,080
something or you're dropped data it's

00:32:06,990 --> 00:32:13,110
always possible to go back and replay if

00:32:10,080 --> 00:32:15,480
all of this data is sitting on a system

00:32:13,110 --> 00:32:17,550
that you put to work with your solar

00:32:15,480 --> 00:32:19,800
cluster rather than completely relying

00:32:17,550 --> 00:32:22,490
on just your solar cluster

00:32:19,800 --> 00:32:24,440
so to summarize their approach or

00:32:22,490 --> 00:32:25,940
approach their segregation of

00:32:24,440 --> 00:32:28,070
responsibility is something that we

00:32:25,940 --> 00:32:30,290
really wanted to do so that Solar is not

00:32:28,070 --> 00:32:34,220
left with taking care of stuff outside

00:32:30,290 --> 00:32:36,230
of search and queuing and mirroring or

00:32:34,220 --> 00:32:39,440
managed by third party cues so what that

00:32:36,230 --> 00:32:41,570
means is you don't have to really even

00:32:39,440 --> 00:32:43,429
run it yourself you could be using and

00:32:41,570 --> 00:32:47,179
consuming a service that is either

00:32:43,429 --> 00:32:51,050
provided by someone else it could be it

00:32:47,179 --> 00:32:53,270
could be publicly provided queuing the

00:32:51,050 --> 00:32:54,830
chasm or it could be another team that

00:32:53,270 --> 00:32:57,890
provides the service to you in your

00:32:54,830 --> 00:33:00,380
organization and there's retrying of

00:32:57,890 --> 00:33:02,330
failed requests in addition to the

00:33:00,380 --> 00:33:05,920
possibility of just retrying everything

00:33:02,330 --> 00:33:09,140
but rewinding the head on your cue and

00:33:05,920 --> 00:33:11,420
add another TC is easy because all of

00:33:09,140 --> 00:33:13,309
these all of these data centers are kind

00:33:11,420 --> 00:33:15,350
of agnostic of each other the only

00:33:13,309 --> 00:33:19,059
person who really knows about the

00:33:15,350 --> 00:33:21,230
existence of each of those pcs is is the

00:33:19,059 --> 00:33:26,570
consistency checker which runs in

00:33:21,230 --> 00:33:29,030
isolation or the mirror config and the

00:33:26,570 --> 00:33:31,670
crusty C consumer as I said doesn't use

00:33:29,030 --> 00:33:34,220
solar resources you're not going to run

00:33:31,670 --> 00:33:37,550
out of the trunk disk space because of

00:33:34,220 --> 00:33:39,650
transaction log exploding or the leader

00:33:37,550 --> 00:33:41,600
being bombarded with a ton of updates

00:33:39,650 --> 00:33:44,000
will not translate into okay now I'm

00:33:41,600 --> 00:33:46,160
kind of contending for resources where I

00:33:44,000 --> 00:33:48,470
want to send this data across a city

00:33:46,160 --> 00:33:50,390
using CDCR but at the same time also

00:33:48,470 --> 00:33:53,030
processed them locally and make sure

00:33:50,390 --> 00:33:56,540
that this data is up and running stable

00:33:53,030 --> 00:33:59,030
on the primary PC to begin with and then

00:33:56,540 --> 00:34:01,880
you could have poor visibility into

00:33:59,030 --> 00:34:05,590
telemetry metrics if if you use a cue

00:34:01,880 --> 00:34:05,590
that is managed by a third-party system

00:34:05,950 --> 00:34:14,119
so to just wrap it up the feature the we

00:34:12,230 --> 00:34:17,570
did have a discussion in the community

00:34:14,119 --> 00:34:21,970
and talk about converging the solutions

00:34:17,570 --> 00:34:24,320
so that we have fun end-to-end solution

00:34:21,970 --> 00:34:26,149
we didn't get the bat we didn't have the

00:34:24,320 --> 00:34:28,190
bandwidth to do that so far but the

00:34:26,149 --> 00:34:30,350
plans to do that in the near future if

00:34:28,190 --> 00:34:33,350
that's something that other folks in the

00:34:30,350 --> 00:34:35,750
community are interested in so watching

00:34:33,350 --> 00:34:39,909
for the zeros and if you're interested

00:34:35,750 --> 00:34:42,560
in Crusty's DC replication and solar

00:34:39,909 --> 00:34:46,460
please reach out and participate in the

00:34:42,560 --> 00:34:48,639
community and yeah any self any form of

00:34:46,460 --> 00:34:53,409
participation it doesn't have to be just

00:34:48,639 --> 00:34:56,659
a code level participation but design

00:34:53,409 --> 00:35:00,290
use is sharing your use cases challenges

00:34:56,659 --> 00:35:03,500
that you face is as valuable as anything

00:35:00,290 --> 00:35:08,540
else as the code or providing tests so

00:35:03,500 --> 00:35:12,579
yeah participate that's about it thank

00:35:08,540 --> 00:35:15,440
you so much yeah um thanks for the talk

00:35:12,579 --> 00:35:19,099
there like a few questions on Channel

00:35:15,440 --> 00:35:21,920
but then she take one question out of

00:35:19,099 --> 00:35:24,050
those and then have the rest have you

00:35:21,920 --> 00:35:27,530
answered the rest in the Gigi breakout

00:35:24,050 --> 00:35:29,660
room up to the top so I'm gonna you like

00:35:27,530 --> 00:35:31,550
answer it in the short version because

00:35:29,660 --> 00:35:35,569
it seems like the question evolved

00:35:31,550 --> 00:35:39,530
around the gia pointing to remove CDCR

00:35:35,569 --> 00:35:41,930
from solar so if you have like answer to

00:35:39,530 --> 00:35:43,369
why there's a Giro and like that and

00:35:41,930 --> 00:35:45,950
like people have questions around that

00:35:43,369 --> 00:35:50,210
we can answer that now and then you can

00:35:45,950 --> 00:35:52,910
probably I can I control the I cannot

00:35:50,210 --> 00:35:54,470
try and answer that in as few words as

00:35:52,910 --> 00:35:59,750
possible for now and then we can and

00:35:54,470 --> 00:36:01,460
elaborate in an offline discussion yeah

00:35:59,750 --> 00:36:04,310
because of the challenges that I

00:36:01,460 --> 00:36:05,150
mentioned that we realized that the

00:36:04,310 --> 00:36:09,140
current CDCR

00:36:05,150 --> 00:36:11,960
has it kind of seems like it needs to be

00:36:09,140 --> 00:36:14,180
either removed or redone in either case

00:36:11,960 --> 00:36:17,960
removed from its from the way it

00:36:14,180 --> 00:36:20,540
currently stands I to to have something

00:36:17,960 --> 00:36:23,329
that is usable in a practical and set up

00:36:20,540 --> 00:36:25,730
in a practical environment so yeah

00:36:23,329 --> 00:36:27,710
that's that's the reason why I said the

00:36:25,730 --> 00:36:29,359
feature of this is for the community to

00:36:27,710 --> 00:36:31,579
discuss and come up with a solution that

00:36:29,359 --> 00:36:33,950
converges and actually solves the

00:36:31,579 --> 00:36:36,200
problem rather than have a feature that

00:36:33,950 --> 00:36:38,810
exists but comes with so many drawbacks

00:36:36,200 --> 00:36:41,589
that it makes it impossible to use in in

00:36:38,810 --> 00:36:41,589
the real world

00:36:47,690 --> 00:36:49,750

YouTube URL: https://www.youtube.com/watch?v=tJLVN0pT0uc


