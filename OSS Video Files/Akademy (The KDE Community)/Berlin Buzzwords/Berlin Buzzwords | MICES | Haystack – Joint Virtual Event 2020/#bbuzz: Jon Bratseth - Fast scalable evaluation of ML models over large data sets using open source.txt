Title: #bbuzz: Jon Bratseth - Fast scalable evaluation of ML models over large data sets using open source
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/fast-and-scalable-evaluation-machine-learned-models-over-large-data-sets-using-open-source

Modern solutions to search and recommendation require evaluating machine-learned models over large data sets with low latency. Producing the best results typically require combining fast (approximate) nearest neighbour search in vector spaces to limit candidates, filtering to surface only the appropriate subset of results in each case, and evaluation of more complex ML models such as deep neural nets computing over both vectors and semantic features. Combining these needs into a working and scalable solution is a large challenge as separate components solving for each requirement cannot be composed into a scalable whole for fundamental reasons.

This talk will explain the architectural challenges of this problem, show the advantages of solving it on concrete cases and introduce an open source engine - Vespa.ai - that provides a scalable solution by implementing all the elements in a single distributed execution.
Captions: 
	00:00:10,349 --> 00:00:16,450
hey I'm unit sets the architect of

00:00:13,650 --> 00:00:18,849
respite a I and I'll be talking about

00:00:16,450 --> 00:00:21,490
fast and scalable regulation on machine

00:00:18,849 --> 00:00:26,730
learning models especially over large

00:00:21,490 --> 00:00:29,470
datasets many of you probably know but

00:00:26,730 --> 00:00:32,680
if not there's an ongoing revolution

00:00:29,470 --> 00:00:36,640
happening in search a kay search to

00:00:32,680 --> 00:00:40,239
total where people are moving from text

00:00:36,640 --> 00:00:45,159
tokens token based retrieval plus

00:00:40,239 --> 00:00:48,549
relevance using typically fairly small

00:00:45,159 --> 00:00:50,260
set of scaler features and gradient

00:00:48,549 --> 00:00:53,979
boosted trees for machine machine

00:00:50,260 --> 00:00:56,470
learning for relevance to embedding both

00:00:53,979 --> 00:00:59,909
the queries and the documents in a

00:00:56,470 --> 00:01:02,170
vector space and doing which we will buy

00:00:59,909 --> 00:01:05,970
nearest neighbor in this vector space

00:01:02,170 --> 00:01:09,340
and then doing relevance typically using

00:01:05,970 --> 00:01:13,090
some variant of deep neural Nets which

00:01:09,340 --> 00:01:19,479
means using large tensors with maybe

00:01:13,090 --> 00:01:21,579
thousands or millions so features in

00:01:19,479 --> 00:01:25,179
addition to this change happening in

00:01:21,579 --> 00:01:27,310
search these technology on the right the

00:01:25,179 --> 00:01:31,119
vector embeddings and so on is also used

00:01:27,310 --> 00:01:33,659
in another set of areas that are using

00:01:31,119 --> 00:01:35,950
typically the same technologies such as

00:01:33,659 --> 00:01:41,469
recommendation personalization ad

00:01:35,950 --> 00:01:44,399
targeting and so on so while there are

00:01:41,469 --> 00:01:47,140
good technologies for each of these

00:01:44,399 --> 00:01:50,799
pieces that you want to be together to

00:01:47,140 --> 00:01:52,600
make a solution on the right it's hard

00:01:50,799 --> 00:01:54,999
to productionize it as I'll talk about

00:01:52,600 --> 00:01:59,619
in a minute so the pieces are typically

00:01:54,999 --> 00:02:01,270
some kind of search engine and library

00:01:59,619 --> 00:02:05,280
for doing approximate nearest neighbor

00:02:01,270 --> 00:02:09,729
search and some kind of mobile server or

00:02:05,280 --> 00:02:12,790
library for evaluating these deep neural

00:02:09,729 --> 00:02:18,370
nets right so each of these pieces are

00:02:12,790 --> 00:02:19,660
good and if you're doing submitting to a

00:02:18,370 --> 00:02:22,010
chi level competition or something like

00:02:19,660 --> 00:02:24,500
that it's quite easy to

00:02:22,010 --> 00:02:29,659
put these together and make it work for

00:02:24,500 --> 00:02:31,580
your submission and that's it right but

00:02:29,659 --> 00:02:35,450
when you want to productionize it you

00:02:31,580 --> 00:02:38,510
run into bernal challenges combining

00:02:35,450 --> 00:02:42,080
these things with good performance is

00:02:38,510 --> 00:02:45,200
challenging in practice if you're doing

00:02:42,080 --> 00:02:47,360
a production solution you typically want

00:02:45,200 --> 00:02:50,269
to combine nearest neighbor search with

00:02:47,360 --> 00:02:51,860
the query features because we solutions

00:02:50,269 --> 00:02:57,170
typically have filters for example if

00:02:51,860 --> 00:02:59,569
you are searching for new news articles

00:02:57,170 --> 00:03:03,500
you want to filter out some publications

00:02:59,569 --> 00:03:07,250
for some customers or some languages or

00:03:03,500 --> 00:03:09,230
comte resource and like that and all

00:03:07,250 --> 00:03:13,340
kinds of solutions have similar business

00:03:09,230 --> 00:03:17,299
needs right and in addition while this

00:03:13,340 --> 00:03:20,989
revolution is ongoing and text embedding

00:03:17,299 --> 00:03:24,799
since ah are becoming better typically

00:03:20,989 --> 00:03:27,549
you get good results only by combining

00:03:24,799 --> 00:03:30,950
these embedding techniques with

00:03:27,549 --> 00:03:33,680
traditional text search beyond 25 and so

00:03:30,950 --> 00:03:37,450
on so you need to run a hybrid where you

00:03:33,680 --> 00:03:43,579
do typically do text matching based on

00:03:37,450 --> 00:03:46,190
the both neural nets and the traditional

00:03:43,579 --> 00:03:48,950
text search and Khobar and features of

00:03:46,190 --> 00:03:54,260
both and return that and that gives you

00:03:48,950 --> 00:03:56,680
the best results right so how do you

00:03:54,260 --> 00:04:00,650
combine these things right you need to

00:03:56,680 --> 00:04:03,440
do the text search or the search or

00:04:00,650 --> 00:04:07,370
filters in some way in a search engine

00:04:03,440 --> 00:04:09,139
and then use nearest-neighbor library to

00:04:07,370 --> 00:04:11,060
search for nearest neighbors and then

00:04:09,139 --> 00:04:13,760
you need to combine the results somehow

00:04:11,060 --> 00:04:15,379
if you just do that Maile it will be

00:04:13,760 --> 00:04:18,519
very expensive because you're doing two

00:04:15,379 --> 00:04:19,699
different search risks that may give you

00:04:18,519 --> 00:04:21,829
completely

00:04:19,699 --> 00:04:24,199
this young results and then you need to

00:04:21,829 --> 00:04:27,710
go buy them somehow and also how do you

00:04:24,199 --> 00:04:31,250
know that you're asking for enough to

00:04:27,710 --> 00:04:34,130
actually be able to combine these into

00:04:31,250 --> 00:04:35,840
single results that's another hard

00:04:34,130 --> 00:04:40,370
problem right if you

00:04:35,840 --> 00:04:43,310
the filters filters out 99% of all the

00:04:40,370 --> 00:04:47,240
documents then you probably won't find

00:04:43,310 --> 00:04:51,639
enough matches in your nearest neighbor

00:04:47,240 --> 00:04:55,520
search to even return a result right so

00:04:51,639 --> 00:04:57,949
this is a hard problem and production

00:04:55,520 --> 00:04:59,990
icing it is also challenging because in

00:04:57,949 --> 00:05:02,270
a production service you need things

00:04:59,990 --> 00:05:04,600
like sustain will to climb out dates

00:05:02,270 --> 00:05:07,460
including removal of documents and so on

00:05:04,600 --> 00:05:09,440
which is libraries typically don't do

00:05:07,460 --> 00:05:11,240
very well but that doesn't matter for

00:05:09,440 --> 00:05:14,300
competitions as all but it matters a lot

00:05:11,240 --> 00:05:17,990
for wheeled production systems and you

00:05:14,300 --> 00:05:22,040
also need reasonably fast restart times

00:05:17,990 --> 00:05:24,520
so the libraries that are only doing

00:05:22,040 --> 00:05:27,940
this stuff in memory and not persisting

00:05:24,520 --> 00:05:32,750
won't really work well in practice right

00:05:27,940 --> 00:05:35,600
some of them do and some will not if you

00:05:32,750 --> 00:05:40,070
are have dis change systems for this and

00:05:35,600 --> 00:05:42,110
you update them separately then you also

00:05:40,070 --> 00:05:43,820
need to deal with the case where the

00:05:42,110 --> 00:05:47,389
update succeeds in one and not the other

00:05:43,820 --> 00:05:49,630
and they diverge requirements or lastly

00:05:47,389 --> 00:05:59,690
scaling these solutions is also pretty

00:05:49,630 --> 00:06:02,690
difficult this means is when you scale

00:05:59,690 --> 00:06:04,729
to more data or more CPU per query you

00:06:02,690 --> 00:06:09,050
need to partition your content and

00:06:04,729 --> 00:06:12,520
spread it or many nodes right now you

00:06:09,050 --> 00:06:16,250
have the problem that you can't really

00:06:12,520 --> 00:06:19,820
the model inference that you want to do

00:06:16,250 --> 00:06:21,979
on a subset of your results or a

00:06:19,820 --> 00:06:24,740
different mode because that will quickly

00:06:21,979 --> 00:06:30,620
saturate your network for example if you

00:06:24,740 --> 00:06:33,680
have a 10 GB network then you can only

00:06:30,620 --> 00:06:37,729
do thousand dogs per query if you have

00:06:33,680 --> 00:06:42,200
actors with 500 floats if you do that

00:06:37,729 --> 00:06:44,659
your total capacity will be 300 queries

00:06:42,200 --> 00:06:48,640
per second and adding more nodes won't

00:06:44,659 --> 00:06:48,640
help because you saturate network

00:06:50,120 --> 00:06:55,050
so model servers won't really help you

00:06:53,339 --> 00:06:57,749
and more so you need some kind of local

00:06:55,050 --> 00:06:59,729
level inference library that you need to

00:06:57,749 --> 00:07:02,879
integrate on all of the content

00:06:59,729 --> 00:07:05,939
partitions which is a lot more work and

00:07:02,879 --> 00:07:08,249
more challenging you are the same kind

00:07:05,939 --> 00:07:13,169
of problem with approximate nearest

00:07:08,249 --> 00:07:16,589
neighbor integration so obvious all this

00:07:13,169 --> 00:07:19,650
well one way to do it is to just use

00:07:16,589 --> 00:07:21,899
vespa a I which is a open source

00:07:19,650 --> 00:07:25,169
platform that's supports all of these

00:07:21,899 --> 00:07:29,339
things out of the box it started as a

00:07:25,169 --> 00:07:31,259
web search engine a long time ago

00:07:29,339 --> 00:07:33,809
so it has all the traditional text

00:07:31,259 --> 00:07:37,110
search features text-based relevance

00:07:33,809 --> 00:07:37,740
weight positions linguistic stemming and

00:07:37,110 --> 00:07:42,569
so on

00:07:37,740 --> 00:07:47,189
BM 25-week and operator which is

00:07:42,569 --> 00:07:51,479
important for scaling text search or

00:07:47,189 --> 00:07:56,189
tokens optimized support for gradients

00:07:51,479 --> 00:07:57,509
decision trees text snippet iing that

00:07:56,189 --> 00:08:00,839
you want to do in text search and soul

00:07:57,509 --> 00:08:02,490
but it also has support for nearest

00:08:00,839 --> 00:08:06,029
neighbor search and approximate nearest

00:08:02,490 --> 00:08:07,979
neighbor search in vector spaces support

00:08:06,029 --> 00:08:10,649
for adding tensor they taught your

00:08:07,979 --> 00:08:14,009
documents and queries and doing tensor

00:08:10,649 --> 00:08:17,610
mathematics integration with our next

00:08:14,009 --> 00:08:19,559
intensive flow to import complex machine

00:08:17,610 --> 00:08:22,499
learning models directly and running

00:08:19,559 --> 00:08:24,180
them on the content nodes so you get

00:08:22,499 --> 00:08:27,479
this scaling I just talked about

00:08:24,180 --> 00:08:30,749
for free and you can combine all these

00:08:27,479 --> 00:08:32,519
features in a single query and in a

00:08:30,749 --> 00:08:34,469
single reference model so you can get

00:08:32,519 --> 00:08:36,029
the best of both worlds and experiments

00:08:34,469 --> 00:08:40,039
and so on with these different features

00:08:36,029 --> 00:08:43,860
and lastly it's for high availability

00:08:40,039 --> 00:08:45,329
production systems so you can change the

00:08:43,860 --> 00:08:48,300
hardware change the machine there models

00:08:45,329 --> 00:08:51,410
change the data on low change and so on

00:08:48,300 --> 00:08:57,779
while you're serving and writing without

00:08:51,410 --> 00:09:00,120
interruption and these systems are I

00:08:57,779 --> 00:09:01,380
mean West buys built to scale to

00:09:00,120 --> 00:09:03,120
hundreds obedience look

00:09:01,380 --> 00:09:06,210
hundreds of thousands of queries per

00:09:03,120 --> 00:09:08,520
second and can typically do a couple of

00:09:06,210 --> 00:09:10,290
tenths of thousands so rice per mole per

00:09:08,520 --> 00:09:13,350
second sustained and that includes

00:09:10,290 --> 00:09:19,830
rights that remove documents change

00:09:13,350 --> 00:09:22,230
fields all of these things so I won't be

00:09:19,830 --> 00:09:25,200
talking too much about west by itself

00:09:22,230 --> 00:09:28,560
but mention some of its usages so that

00:09:25,200 --> 00:09:32,190
you can be assured it's a real

00:09:28,560 --> 00:09:34,410
production system use it extends layout

00:09:32,190 --> 00:09:39,120
the company that employs me with this

00:09:34,410 --> 00:09:44,180
which is Verizon Media you're serving

00:09:39,120 --> 00:09:48,780
over a billion users red Vespa about

00:09:44,180 --> 00:09:53,240
350,000 queries per second and so many

00:09:48,780 --> 00:09:56,100
use cases are delivering personalized

00:09:53,240 --> 00:10:01,580
content to all the users that visit

00:09:56,100 --> 00:10:06,330
Yahoo pages and so on which means

00:10:01,580 --> 00:10:08,460
evaluating a burn show while doing all

00:10:06,330 --> 00:10:11,960
the things I just talked about really

00:10:08,460 --> 00:10:15,330
very much the user to a vector space and

00:10:11,960 --> 00:10:17,490
do a vector search to come up with best

00:10:15,330 --> 00:10:20,220
articles and real machine learning

00:10:17,490 --> 00:10:21,750
models to fine-tune what you are

00:10:20,220 --> 00:10:24,360
returning and so on and we do that for

00:10:21,750 --> 00:10:27,570
every user that is visiting one of these

00:10:24,360 --> 00:10:31,590
sites in real time when they are loading

00:10:27,570 --> 00:10:34,470
the page and we are doing the same kind

00:10:31,590 --> 00:10:36,390
of thing the AB network owned by the

00:10:34,470 --> 00:10:40,170
company which is the third largest in

00:10:36,390 --> 00:10:42,180
the work we're doing similar things but

00:10:40,170 --> 00:10:45,540
even more complex because you take

00:10:42,180 --> 00:10:48,180
bidding into counts and so and all that

00:10:45,540 --> 00:10:53,790
friends on RESPA and is serving in real

00:10:48,180 --> 00:10:55,260
time so just a quick overview oh that's

00:10:53,790 --> 00:10:57,270
why it's a two-tier system you have a

00:10:55,260 --> 00:11:00,120
stateless Java container on top that

00:10:57,270 --> 00:11:01,500
handles the incoming inquiries rights

00:11:00,120 --> 00:11:06,440
and so on or you can have multiple

00:11:01,500 --> 00:11:08,790
different container clusters if you like

00:11:06,440 --> 00:11:10,350
below that you have content clusters

00:11:08,790 --> 00:11:14,550
that stores

00:11:10,350 --> 00:11:19,890
actual contents maintain Reverse indices

00:11:14,550 --> 00:11:22,320
for texts indices for vector

00:11:19,890 --> 00:11:25,020
nearest-neighbor searching and so on and

00:11:22,320 --> 00:11:27,800
which is doing all the distributed query

00:11:25,020 --> 00:11:30,480
execution including finding the matress

00:11:27,800 --> 00:11:34,080
evaluating machine their models and so

00:11:30,480 --> 00:11:36,840
on because these systems can contain no

00:11:34,080 --> 00:11:39,200
many nodes many processing zone we also

00:11:36,840 --> 00:11:43,230
have an administration and config

00:11:39,200 --> 00:11:45,210
cluster that sets up and manages these

00:11:43,230 --> 00:11:47,340
nodes for you and what the user is

00:11:45,210 --> 00:11:48,810
seeing is a more high-level abstraction

00:11:47,340 --> 00:11:51,930
which we call an application package

00:11:48,810 --> 00:11:54,960
which I'll show you an example later the

00:11:51,930 --> 00:11:56,490
application package basically describes

00:11:54,960 --> 00:11:59,580
the system that you want to run and

00:11:56,490 --> 00:12:02,360
complain stand a Java components that

00:11:59,580 --> 00:12:06,450
you want to run the machine learn models

00:12:02,360 --> 00:12:07,590
and so when you work with the

00:12:06,450 --> 00:12:09,390
application you just change the

00:12:07,590 --> 00:12:12,240
application package and deploy it and

00:12:09,390 --> 00:12:14,940
the system will safely carry out the

00:12:12,240 --> 00:12:18,300
change from the currently running system

00:12:14,940 --> 00:12:20,340
to the system described by the new

00:12:18,300 --> 00:12:22,770
version of the application package so we

00:12:20,340 --> 00:12:26,850
typically do this in a CD fashion where

00:12:22,770 --> 00:12:29,190
you just we have a process that turns

00:12:26,850 --> 00:12:30,900
from github or whatever you using

00:12:29,190 --> 00:12:34,400
building your application package and

00:12:30,900 --> 00:12:38,760
just submitting it to this and it will

00:12:34,400 --> 00:12:40,260
roll it out safely in production for

00:12:38,760 --> 00:12:44,820
others approximate nearest neighbor

00:12:40,260 --> 00:12:47,130
searches work on vespa for user it's

00:12:44,820 --> 00:12:48,630
just another query item that you can

00:12:47,130 --> 00:12:52,670
combine with any others in the query

00:12:48,630 --> 00:12:54,990
tree so you can combine text search and

00:12:52,670 --> 00:12:57,000
nearest neighbor in the same query and

00:12:54,990 --> 00:12:59,370
even have multiple nearest neighbor

00:12:57,000 --> 00:13:03,540
operators or different fields or

00:12:59,370 --> 00:13:04,430
whatever in the same query the

00:13:03,540 --> 00:13:07,220
approximate nearest neighbor

00:13:04,430 --> 00:13:10,100
implementation we use is based on the h

00:13:07,220 --> 00:13:15,260
sv algorithm which is a network

00:13:10,100 --> 00:13:20,760
algorithm which is fastest algorithms

00:13:15,260 --> 00:13:22,850
generally we have our own implementation

00:13:20,760 --> 00:13:26,049
that the live worst

00:13:22,850 --> 00:13:29,239
on the needs I talked about earlier like

00:13:26,049 --> 00:13:33,289
supporting removal of modes from the

00:13:29,239 --> 00:13:36,919
graph and so on and it also works

00:13:33,289 --> 00:13:38,899
efficiently with other query turn so you

00:13:36,919 --> 00:13:41,449
can combine into the filters and so on

00:13:38,899 --> 00:13:46,789
and still do an efficient approximate

00:13:41,449 --> 00:13:48,439
nearest neighbor search now this model

00:13:46,789 --> 00:13:53,209
inference work in Vespa

00:13:48,439 --> 00:14:00,169
so Vespa has tensor data model where you

00:13:53,209 --> 00:14:04,220
can add tensors to both documents and

00:14:00,169 --> 00:14:06,919
queries and the application package so a

00:14:04,220 --> 00:14:10,369
tensor is just multi-dimensional

00:14:06,919 --> 00:14:14,359
collection of numbers each of the

00:14:10,369 --> 00:14:17,720
dimensions can be sparse or dance and

00:14:14,359 --> 00:14:21,049
you can combine for sometimes dimensions

00:14:17,720 --> 00:14:23,179
in the same tensor as I showed an

00:14:21,049 --> 00:14:27,679
example over here where you have a two

00:14:23,179 --> 00:14:33,259
dimensional tensor with a sparse key and

00:14:27,679 --> 00:14:37,549
a dense vector so it's really a map or

00:14:33,259 --> 00:14:40,999
vectors right then you can do tensor

00:14:37,549 --> 00:14:43,899
math to express machined or models or

00:14:40,999 --> 00:14:46,999
business logic over these tensors

00:14:43,899 --> 00:14:49,779
there's a small set of core operations

00:14:46,999 --> 00:14:51,949
which we use in our cancer engine for

00:14:49,779 --> 00:14:55,850
optimization and that may have a larger

00:14:51,949 --> 00:14:57,919
set or higher level functions which are

00:14:55,850 --> 00:15:02,209
the ones you will typically use in your

00:14:57,919 --> 00:15:04,100
models but which maps to those primitive

00:15:02,209 --> 00:15:07,669
functions that we have joined and map

00:15:04,100 --> 00:15:09,109
and so on which is quite neat but not of

00:15:07,669 --> 00:15:12,739
interesting for users so yet you just

00:15:09,109 --> 00:15:15,109
use the high level methods or if you

00:15:12,739 --> 00:15:18,169
don't want to write your expressions by

00:15:15,109 --> 00:15:22,489
hand you can just deploy tensorflow

00:15:18,169 --> 00:15:26,179
or omx or extra boost or like GBM models

00:15:22,489 --> 00:15:28,129
directly in Vespa and let's Pavel do the

00:15:26,179 --> 00:15:32,889
translation automatically when you

00:15:28,129 --> 00:15:36,079
deploy the model so we have our own

00:15:32,889 --> 00:15:36,410
tensor execution engine inside Vespa

00:15:36,079 --> 00:15:38,660
that

00:15:36,410 --> 00:15:41,180
optimized for repeated execution or the

00:15:38,660 --> 00:15:42,529
models or many data which is what we

00:15:41,180 --> 00:15:43,339
typically want to do in this kind of

00:15:42,529 --> 00:15:45,440
systems right

00:15:43,339 --> 00:15:46,850
you're not just evaluating over a single

00:15:45,440 --> 00:15:49,310
data point per query but you're

00:15:46,850 --> 00:15:56,660
evaluating our many data points articles

00:15:49,310 --> 00:15:59,449
or movies or authorities and just to

00:15:56,660 --> 00:16:05,439
show a quick example or the hybrid model

00:15:59,449 --> 00:16:08,769
thing I talked about earlier what we see

00:16:05,439 --> 00:16:11,600
really almost every time when we look at

00:16:08,769 --> 00:16:16,639
the performance we get out of these

00:16:11,600 --> 00:16:19,459
various models is that you don't get the

00:16:16,639 --> 00:16:24,050
best models by using either some

00:16:19,459 --> 00:16:27,920
traditional texts features or by using a

00:16:24,050 --> 00:16:30,500
neural net model but you get the very

00:16:27,920 --> 00:16:32,990
best performance by combining both and

00:16:30,500 --> 00:16:36,649
here's a very simple example here where

00:16:32,990 --> 00:16:39,740
we have some traditional text features

00:16:36,649 --> 00:16:44,870
here in Boehm Google rank profile and

00:16:39,740 --> 00:16:47,540
another right profile which is just the

00:16:44,870 --> 00:16:49,250
distance in a vector space or this

00:16:47,540 --> 00:16:52,339
embedding and then we have a hybrid

00:16:49,250 --> 00:16:54,410
model which is just some or ball things

00:16:52,339 --> 00:16:56,560
and that all performs children so it's a

00:16:54,410 --> 00:16:59,060
very simple example because it's from

00:16:56,560 --> 00:17:04,579
one of our sample applications for the

00:16:59,060 --> 00:17:07,280
daily stress this point so I'm going to

00:17:04,579 --> 00:17:11,289
go to another example application a bit

00:17:07,280 --> 00:17:14,959
more in depth and I've chosen to

00:17:11,289 --> 00:17:20,990
application that we call core 19 the

00:17:14,959 --> 00:17:24,850
vespa both AI when the pondan pandemic

00:17:20,990 --> 00:17:28,010
broke out the Allen Institute released

00:17:24,850 --> 00:17:30,309
data set or initially for Kittleson and

00:17:28,010 --> 00:17:34,070
now about on here in turkey Towson

00:17:30,309 --> 00:17:37,870
papers about the corona virus at least

00:17:34,070 --> 00:17:41,990
related to the corona virus somehow and

00:17:37,870 --> 00:17:45,230
my team turned around to take a week or

00:17:41,990 --> 00:17:47,030
two out to build the tool to help

00:17:45,230 --> 00:17:49,059
exploring these data sets so that

00:17:47,030 --> 00:17:52,309
researchers secured

00:17:49,059 --> 00:17:55,299
more quickly do science to learn both

00:17:52,309 --> 00:17:58,429
the new disease which seemed like a

00:17:55,299 --> 00:18:00,679
important thing to do at the time

00:17:58,429 --> 00:18:03,169
so these combined traditional text

00:18:00,679 --> 00:18:06,850
search features with article similarity

00:18:03,169 --> 00:18:09,440
search and also grouping and filtering

00:18:06,850 --> 00:18:12,590
which is something it typical to do when

00:18:09,440 --> 00:18:17,260
you do exploration and here everything

00:18:12,590 --> 00:18:21,290
is open both the datasets and also the

00:18:17,260 --> 00:18:25,309
best by itself but also the respiration

00:18:21,290 --> 00:18:29,179
that incorporates that implements the

00:18:25,309 --> 00:18:32,090
core 19 application as well as fronting

00:18:29,179 --> 00:18:34,419
that we built on top so that's the

00:18:32,090 --> 00:18:36,770
advantage of a stitch open data set

00:18:34,419 --> 00:18:40,070
everything is open sourced this is what

00:18:36,770 --> 00:18:42,470
JJ said the data set is very small just

00:18:40,070 --> 00:18:46,970
two hundred thousand hundred and thirty

00:18:42,470 --> 00:18:51,340
thousand articles but vespa scales to

00:18:46,970 --> 00:18:54,110
about a million times as much content

00:18:51,340 --> 00:18:56,600
without really changing anything other

00:18:54,110 --> 00:19:01,190
than adding more notes videos you need

00:18:56,600 --> 00:19:07,549
more resources for that obviously so let

00:19:01,190 --> 00:19:09,919
me see the presentation and show you the

00:19:07,549 --> 00:19:13,130
core 19 application how it works so this

00:19:09,919 --> 00:19:16,580
is the front page here you can write a

00:19:13,130 --> 00:19:21,620
query as you would expect but I'll just

00:19:16,580 --> 00:19:24,820
click on one of these now this one for

00:19:21,620 --> 00:19:28,640
example so this is a rather complex

00:19:24,820 --> 00:19:32,059
query and you get results as you would

00:19:28,640 --> 00:19:35,799
expect and here you see all the matches

00:19:32,059 --> 00:19:41,000
that you get in various sources journals

00:19:35,799 --> 00:19:43,880
and so on so this is grouping feature in

00:19:41,000 --> 00:19:45,770
vespa and then you can also do a search

00:19:43,880 --> 00:19:52,280
for similar articles here and what we're

00:19:45,770 --> 00:19:55,190
doing them is adding this related to

00:19:52,280 --> 00:19:58,070
term to the query which is just picked

00:19:55,190 --> 00:20:01,260
up by a custom java component in this

00:19:58,070 --> 00:20:04,890
application that fetches that

00:20:01,260 --> 00:20:07,140
to go from Vespa fetches the embedding

00:20:04,890 --> 00:20:11,130
vector of data article and then adds

00:20:07,140 --> 00:20:15,330
that embedding vector to the query that

00:20:11,130 --> 00:20:16,920
is then sent down to get combination of

00:20:15,330 --> 00:20:20,720
the text features that you added any

00:20:16,920 --> 00:20:23,190
query here to a nearest neighbor search

00:20:20,720 --> 00:20:25,170
over that article so you get the

00:20:23,190 --> 00:20:27,900
combination of both and that's very

00:20:25,170 --> 00:20:30,690
useful when you are exploring right

00:20:27,900 --> 00:20:33,030
because you have an article that

00:20:30,690 --> 00:20:34,470
represents somehow the topic you are

00:20:33,030 --> 00:20:38,880
interested in and then you combine that

00:20:34,470 --> 00:20:44,190
with text search features that more

00:20:38,880 --> 00:20:47,900
precisely expresses conditions on what

00:20:44,190 --> 00:20:51,360
you're interested in right you can also

00:20:47,900 --> 00:20:54,330
enter the article itself which is just

00:20:51,360 --> 00:20:58,980
served from Vespa as well and here you

00:20:54,330 --> 00:21:01,770
can also do a similar article search or

00:20:58,980 --> 00:21:06,300
different embedding vectors that are

00:21:01,770 --> 00:21:09,480
provided things like that okay so how is

00:21:06,300 --> 00:21:18,660
this implemented let's go into it a bit

00:21:09,480 --> 00:21:21,540
more in detail so this is github repo

00:21:18,660 --> 00:21:24,840
for the front end and we have a separate

00:21:21,540 --> 00:21:29,700
repo for the back end which is no sorry

00:21:24,840 --> 00:21:35,040
one link for the for the vespa

00:21:29,700 --> 00:21:36,750
application which is the example of an

00:21:35,040 --> 00:21:41,430
application package which I mentioned

00:21:36,750 --> 00:21:44,100
before so this is the repo for the vespa

00:21:41,430 --> 00:21:53,490
application I'll go to what it contains

00:21:44,100 --> 00:21:56,880
but first I have it checked out here so

00:21:53,490 --> 00:21:58,560
I checked out this ribbon go to source

00:21:56,880 --> 00:22:01,860
and here you can see the slice of the

00:21:58,560 --> 00:22:03,660
whole thing so it's it contains like GBM

00:22:01,860 --> 00:22:06,540
model that we have been experimenting

00:22:03,660 --> 00:22:09,660
with so there's that's a lot of lines of

00:22:06,540 --> 00:22:12,150
codes but of course also generated by

00:22:09,660 --> 00:22:14,790
that machine learning but the work on

00:22:12,150 --> 00:22:17,870
that is just about 600

00:22:14,790 --> 00:22:21,990
lines are cold implementing this entire

00:22:17,870 --> 00:22:26,330
core 19 application which will scale to

00:22:21,990 --> 00:22:26,330
any size you want and you can combine

00:22:27,950 --> 00:22:36,030
vector similarity and text search snip a

00:22:31,860 --> 00:22:39,900
team grouping and aggregation and all

00:22:36,030 --> 00:22:43,860
these things so let's look at what it

00:22:39,900 --> 00:22:47,070
actually contains so you have the

00:22:43,860 --> 00:22:51,540
application itself which basically

00:22:47,070 --> 00:22:54,809
contains these two files a services file

00:22:51,540 --> 00:22:56,610
which case describes the clusters that

00:22:54,809 --> 00:22:58,380
you want to run in this case we run wall

00:22:56,610 --> 00:23:01,080
mode a stateless Java or container

00:22:58,380 --> 00:23:03,960
clusters and one content cluster that

00:23:01,080 --> 00:23:07,980
holds the contents and the container

00:23:03,960 --> 00:23:09,480
cluster we have some custom Java

00:23:07,980 --> 00:23:14,520
components which we'll take a quick look

00:23:09,480 --> 00:23:16,830
at later then we just specifies the

00:23:14,520 --> 00:23:19,830
resources that is cluster should run on

00:23:16,830 --> 00:23:22,500
these trends on the public respite lab

00:23:19,830 --> 00:23:24,480
and then we can just specify the

00:23:22,500 --> 00:23:28,980
resources we will have and deploy it and

00:23:24,480 --> 00:23:32,040
the system will get those resources on a

00:23:28,980 --> 00:23:34,679
BS and run in this case we just

00:23:32,040 --> 00:23:36,780
specified the real the sources of each

00:23:34,679 --> 00:23:39,419
node and then we say we want from four

00:23:36,780 --> 00:23:43,200
to two four nodes depending on the load

00:23:39,419 --> 00:23:46,460
we are seen for the content cluster we

00:23:43,200 --> 00:23:50,820
there so a little bit attuning year and

00:23:46,460 --> 00:23:53,730
also tuning all the snippets and apart

00:23:50,820 --> 00:23:55,799
from that we just referenced a single

00:23:53,730 --> 00:23:57,870
schema that we use for the documents and

00:23:55,799 --> 00:24:00,480
again I'm specifying the resources and

00:23:57,870 --> 00:24:02,940
that's it and then we have a deployment

00:24:00,480 --> 00:24:04,770
text map which specifies where this

00:24:02,940 --> 00:24:08,700
route drum and this just runs in a

00:24:04,770 --> 00:24:12,000
single ABS region if you're self hosting

00:24:08,700 --> 00:24:15,360
Vespa is really the same thing but

00:24:12,000 --> 00:24:18,450
instead of saying this you just list the

00:24:15,360 --> 00:24:21,750
actual hosts that you want to run for

00:24:18,450 --> 00:24:23,210
this cluster here that's the only

00:24:21,750 --> 00:24:25,429
difference

00:24:23,210 --> 00:24:28,869
and then you don't need the Department

00:24:25,429 --> 00:24:34,059
XML flow so the else is here there's the

00:24:28,869 --> 00:24:41,379
machine learned light GBM mobile and

00:24:34,059 --> 00:24:41,379
some certificates and a specification of

00:24:43,029 --> 00:24:51,349
the stuff you can send in a query which

00:24:45,830 --> 00:24:53,809
is these embedding vectors and then

00:24:51,349 --> 00:24:56,029
there's the single schema were to use

00:24:53,809 --> 00:25:00,589
that describes the data we have air

00:24:56,029 --> 00:25:02,899
which is a single type representing the

00:25:00,589 --> 00:25:05,059
scholarly article itself so it has a

00:25:02,899 --> 00:25:10,609
bunch of fields as you would expect with

00:25:05,059 --> 00:25:14,869
the title the content itself citations

00:25:10,609 --> 00:25:16,849
and whatnot and in addition some

00:25:14,869 --> 00:25:21,649
embedding vectors so we have an

00:25:16,849 --> 00:25:24,830
embedding vector for the abstract for a

00:25:21,649 --> 00:25:27,379
title and then we have another embedding

00:25:24,830 --> 00:25:29,599
vector which is supplied by the alum

00:25:27,379 --> 00:25:33,190
Institute team which is called a

00:25:29,599 --> 00:25:39,109
spectrum building and those are all

00:25:33,190 --> 00:25:43,389
single dimension dance tensors the

00:25:39,109 --> 00:25:46,099
scheme also describes how we can rank or

00:25:43,389 --> 00:25:48,619
alternatively evaluate machine Mobile's

00:25:46,099 --> 00:25:49,399
or this kind of data which is what we

00:25:48,619 --> 00:25:52,369
call the run

00:25:49,399 --> 00:25:54,259
profile so there's a bunch of those here

00:25:52,369 --> 00:25:58,789
I won't go into them in detail but

00:25:54,259 --> 00:26:02,239
there's one that just to normal text

00:25:58,789 --> 00:26:06,139
features on that is BM 25 which is also

00:26:02,239 --> 00:26:12,169
the normal text features and then we

00:26:06,139 --> 00:26:17,359
have one that references that uses light

00:26:12,169 --> 00:26:20,239
GBM model and this could also be

00:26:17,359 --> 00:26:21,859
combined with other features and

00:26:20,239 --> 00:26:24,619
expressions and tensors and whatnot

00:26:21,859 --> 00:26:28,909
because all of this is just math as you

00:26:24,619 --> 00:26:29,590
can see here you can say plus like GBM

00:26:28,909 --> 00:26:33,820
model here

00:26:29,590 --> 00:26:36,309
or whatever we also have some models

00:26:33,820 --> 00:26:41,440
that are used for the related searchers

00:26:36,309 --> 00:26:46,390
where we just access what we call a goal

00:26:41,440 --> 00:26:48,730
score all this embedded vector nearest

00:26:46,390 --> 00:26:54,159
neighbor search which will return a

00:26:48,730 --> 00:26:56,529
distance and that's really all you need

00:26:54,159 --> 00:27:01,360
to create an application in addition we

00:26:56,529 --> 00:27:05,610
have some custom Java code here to

00:27:01,360 --> 00:27:05,610
implement the stuff I mentioned around

00:27:06,179 --> 00:27:14,710
searching for related articles so we

00:27:12,580 --> 00:27:16,840
call these components that can intercept

00:27:14,710 --> 00:27:18,570
the query and all the results a searcher

00:27:16,840 --> 00:27:24,279
they just implement a single method

00:27:18,570 --> 00:27:27,250
which is the search method which gets

00:27:24,279 --> 00:27:29,350
the query and returns two results in

00:27:27,250 --> 00:27:31,899
this case it's just looking to see if

00:27:29,350 --> 00:27:36,700
there is one of these related to items

00:27:31,899 --> 00:27:38,500
in a query if not it just returns which

00:27:36,700 --> 00:27:41,409
means it just nothing and you have a

00:27:38,500 --> 00:27:43,870
normal search otherwise it's translating

00:27:41,409 --> 00:27:48,340
that related to item to the approximate

00:27:43,870 --> 00:27:50,770
nearest neighbor operator which is in a

00:27:48,340 --> 00:27:56,740
sub list let's take a quick look at that

00:27:50,770 --> 00:27:59,919
as well here we can see that as you can

00:27:56,740 --> 00:28:04,240
see it's just two new a nearest neighbor

00:27:59,919 --> 00:28:07,390
item here we don't approach allow

00:28:04,240 --> 00:28:08,980
approximate nearest neighbor because the

00:28:07,390 --> 00:28:10,840
data set is so small so that's the only

00:28:08,980 --> 00:28:13,059
thing you would change other than adding

00:28:10,840 --> 00:28:14,890
resources if you want to do scale to a

00:28:13,059 --> 00:28:18,580
billion documents you will definitely

00:28:14,890 --> 00:28:23,399
set allow approximate true but other

00:28:18,580 --> 00:28:23,399
than that everything will be the same

00:28:23,669 --> 00:28:34,390
the adds the great and I'm item method

00:28:28,539 --> 00:28:36,309
this used up here where we combine it

00:28:34,390 --> 00:28:38,269
with the other items in a query and here

00:28:36,309 --> 00:28:44,659
you can also see an example where

00:28:38,269 --> 00:28:49,309
we create 2mm nearest-neighbor items and

00:28:44,659 --> 00:28:51,259
combine them with or to search near four

00:28:49,309 --> 00:28:53,269
nearest neighbors both in the abstract

00:28:51,259 --> 00:29:01,359
and the title so things like that you

00:28:53,269 --> 00:29:04,219
can do freely and it just works okay

00:29:01,359 --> 00:29:05,690
that's all I really wanted to cover and

00:29:04,219 --> 00:29:07,729
that's really all the waste in this

00:29:05,690 --> 00:29:08,450
application you can easily check this

00:29:07,729 --> 00:29:11,839
out

00:29:08,450 --> 00:29:14,089
yourself if you go to github.com that's

00:29:11,839 --> 00:29:18,379
behind chain sample applications you can

00:29:14,089 --> 00:29:24,739
find it here or just go to core mine

00:29:18,379 --> 00:29:32,779
team and click the open source leak on

00:29:24,739 --> 00:29:37,809
top ok so to wrap up vectorbase

00:29:32,779 --> 00:29:42,169
retrieval and tensor based relevance

00:29:37,809 --> 00:29:44,989
which is one way to look at these deep

00:29:42,169 --> 00:29:49,299
neural networks at least one you want to

00:29:44,989 --> 00:29:51,799
just do inference is emerging as an

00:29:49,299 --> 00:29:54,019
alternative to Dyson traditional search

00:29:51,799 --> 00:29:57,529
and it's also already the state of art

00:29:54,019 --> 00:30:01,700
for recommendation personalization and

00:29:57,529 --> 00:30:04,309
targeting and so on but production using

00:30:01,700 --> 00:30:05,869
these methods on your own even though

00:30:04,309 --> 00:30:08,149
there are good tools for each of the

00:30:05,869 --> 00:30:14,239
pieces it's hard to combine them to a

00:30:08,149 --> 00:30:16,700
production quality system that this good

00:30:14,239 --> 00:30:22,839
performance in all cases and sustain

00:30:16,700 --> 00:30:25,489
good performance as you make changes and

00:30:22,839 --> 00:30:27,559
that you can combine with filtering and

00:30:25,489 --> 00:30:30,679
traditional search and so on which also

00:30:27,559 --> 00:30:34,039
is operable and scalable if you don't

00:30:30,679 --> 00:30:35,179
want to do all that work you can just

00:30:34,039 --> 00:30:38,059
try out

00:30:35,179 --> 00:30:40,969
let's photodiode which provides all the

00:30:38,059 --> 00:30:43,639
weight in a single integrated solution

00:30:40,969 --> 00:30:44,170
with better performance than you would

00:30:43,639 --> 00:30:47,570
get

00:30:44,170 --> 00:30:50,860
for sure by combining these pieces on

00:30:47,570 --> 00:30:55,280
your own and you can find Vespa at

00:30:50,860 --> 00:31:00,430
baseball DoDEA so that's all then we can

00:30:55,280 --> 00:31:00,430
switch to live and take questions

00:31:01,180 --> 00:31:07,430
so thanks John for the great

00:31:03,590 --> 00:31:10,840
presentation the chord 19 means so it's

00:31:07,430 --> 00:31:13,400
super useful so all the best with that

00:31:10,840 --> 00:31:15,650
guys we still have a couple of minutes

00:31:13,400 --> 00:31:18,620
if you have any questions please ask

00:31:15,650 --> 00:31:20,870
them on the slack Channel I guess shown

00:31:18,620 --> 00:31:25,940
you already provided a link to the

00:31:20,870 --> 00:31:27,980
github site to you maybe well be

00:31:25,940 --> 00:31:30,590
awaiting do you have any further

00:31:27,980 --> 00:31:36,710
feedback on how you plant we have all

00:31:30,590 --> 00:31:38,150
this power what's the roadmap so where

00:31:36,710 --> 00:31:41,240
we're spending most of our efforts right

00:31:38,150 --> 00:31:47,330
now really is on the cloud service for

00:31:41,240 --> 00:31:50,630
all the applications started using it in

00:31:47,330 --> 00:31:52,610
my company we provide a cloud service

00:31:50,630 --> 00:31:54,850
and we just were recently started

00:31:52,610 --> 00:31:57,950
providing that cloud service to external

00:31:54,850 --> 00:32:02,270
customers as well so they are mostly

00:31:57,950 --> 00:32:05,200
focusing on making that more broadly

00:32:02,270 --> 00:32:07,790
available and adding more features for

00:32:05,200 --> 00:32:20,519
you know making it cheaper to run on

00:32:07,790 --> 00:32:23,299
things like that so we do see

00:32:20,519 --> 00:32:26,909
question the question is from Edward

00:32:23,299 --> 00:32:29,759
that's basically oh sorry okay so

00:32:26,909 --> 00:32:31,649
there's one more before from Maya she

00:32:29,759 --> 00:32:34,789
would like to understand how can we

00:32:31,649 --> 00:32:37,049
build vector embeddings for articles I

00:32:34,789 --> 00:32:40,320
guess it's more like how can you add

00:32:37,049 --> 00:32:42,419
them yeah yeah I think maybe the

00:32:40,320 --> 00:32:44,459
question is how to come up with the

00:32:42,419 --> 00:32:48,959
vectors so that's the machine learning

00:32:44,459 --> 00:32:50,759
part really and that's somebody else's

00:32:48,959 --> 00:32:55,229
problem as far as we concerned we just

00:32:50,759 --> 00:32:57,749
make it fast to between them and compute

00:32:55,229 --> 00:32:59,789
with them once you have created the

00:32:57,749 --> 00:33:01,559
vectors but how do you create the

00:32:59,789 --> 00:33:04,109
embeddings that's the machine learning

00:33:01,559 --> 00:33:11,700
part which typically happens outside

00:33:04,109 --> 00:33:15,539
list we have another question so it's

00:33:11,700 --> 00:33:19,229
basically from Edward and he's asking if

00:33:15,539 --> 00:33:22,019
he so how so does the wisp architecture

00:33:19,229 --> 00:33:24,539
allow plugging new artificial neural

00:33:22,019 --> 00:33:35,759
network algorithms so basically how

00:33:24,539 --> 00:33:37,679
extensible is the architecture so the so

00:33:35,759 --> 00:33:41,149
the tensor language we have allows you

00:33:37,679 --> 00:33:44,489
to express pretty much all the models

00:33:41,149 --> 00:33:47,879
I've seen recently as we had to when

00:33:44,489 --> 00:33:50,639
people came up with Berk type models

00:33:47,879 --> 00:33:53,669
transformer models with lots of matrix

00:33:50,639 --> 00:33:57,089
and so on we had to extend the tensor

00:33:53,669 --> 00:33:59,759
math language a bit but apart from that

00:33:57,089 --> 00:34:04,169
it should handle all kinds of models you

00:33:59,759 --> 00:34:06,929
would come up with with what we have

00:34:04,169 --> 00:34:08,460
there already because the core

00:34:06,929 --> 00:34:11,669
operations that I mentioned like

00:34:08,460 --> 00:34:14,149
MapReduce and so on are very general in

00:34:11,669 --> 00:34:17,069
general so you can pretty much increment

00:34:14,149 --> 00:34:22,169
all kinds of computations over tensors

00:34:17,069 --> 00:34:25,169
on table and I think there was one more

00:34:22,169 --> 00:34:28,070
question as well yes so actually I think

00:34:25,169 --> 00:34:36,260
Edward has a follow-up question

00:34:28,070 --> 00:34:40,540
I'm not sure I understand oh but you see

00:34:36,260 --> 00:34:43,550
it as well yeah if we can plug in other

00:34:40,540 --> 00:34:46,370
mail approximate nearest neighbor search

00:34:43,550 --> 00:34:48,410
algorithms into Vespa no you cannot not

00:34:46,370 --> 00:34:50,300
without lots and lots of work

00:34:48,410 --> 00:34:54,230
it's basically what you have been doing

00:34:50,300 --> 00:34:56,750
for about six months now it's to plug in

00:34:54,230 --> 00:35:00,020
one algorithm for this in the West bar

00:34:56,750 --> 00:35:04,250
which means implementing it in C++ so

00:35:00,020 --> 00:35:06,260
that it works for with the rest of the

00:35:04,250 --> 00:35:08,810
engine and supports all the operations

00:35:06,260 --> 00:35:13,790
that we need to support with high TripIt

00:35:08,810 --> 00:35:15,770
including removal of documents and so on

00:35:13,790 --> 00:35:18,080
most of these algorithms don't handle

00:35:15,770 --> 00:35:20,390
this very well so I don't think it is

00:35:18,080 --> 00:35:22,610
worthwhile for production to plug

00:35:20,390 --> 00:35:23,840
something in unity implemented from

00:35:22,610 --> 00:35:26,590
scratch with all these requirements

00:35:23,840 --> 00:35:29,240
taken into account if it's more than for

00:35:26,590 --> 00:35:32,090
experimenting but I think we haven't

00:35:29,240 --> 00:35:34,520
chosen the right waiting for this now so

00:35:32,090 --> 00:35:37,480
I don't think there's a great need to

00:35:34,520 --> 00:35:38,960
plug in something else to be honest

00:35:37,480 --> 00:35:41,380
right

00:35:38,960 --> 00:35:43,970
and there's another question but Maya

00:35:41,380 --> 00:35:46,460
also related to embedding some text

00:35:43,970 --> 00:35:48,860
retrieval features I think part of

00:35:46,460 --> 00:35:53,780
answered that maybe if you just want to

00:35:48,860 --> 00:35:55,370
comment a bit more yeah yeah do you

00:35:53,780 --> 00:35:59,210
include them in the single model sheet

00:35:55,370 --> 00:36:01,970
mask yeah so combining embeddings with

00:35:59,210 --> 00:36:03,320
text retrieval features there's two

00:36:01,970 --> 00:36:06,110
parts to it one right one is the

00:36:03,320 --> 00:36:08,270
retrieval you want to retrieve both the

00:36:06,110 --> 00:36:10,040
nearest neighbor to some vector but you

00:36:08,270 --> 00:36:13,400
also want to retrieve the documents that

00:36:10,040 --> 00:36:16,700
are not their neighbors but are matching

00:36:13,400 --> 00:36:19,790
the same tokens so we want to retrieve a

00:36:16,700 --> 00:36:23,030
mix of both and that's sort of logically

00:36:19,790 --> 00:36:27,860
easy but difficult to do efficiently

00:36:23,030 --> 00:36:29,540
because when you want to do it

00:36:27,860 --> 00:36:31,940
efficiently you want to evaluate both

00:36:29,540 --> 00:36:33,860
things in parallel region taking filters

00:36:31,940 --> 00:36:36,380
into account and so that's not a reason

00:36:33,860 --> 00:36:38,810
why you need to integrate this deep into

00:36:36,380 --> 00:36:40,160
engine to make it really efficient but

00:36:38,810 --> 00:36:41,720
we have done that so when you're using

00:36:40,160 --> 00:36:43,940
it is just

00:36:41,720 --> 00:36:45,620
you create a nearest neighbor item or

00:36:43,940 --> 00:36:47,720
several items in the query tree and you

00:36:45,620 --> 00:36:51,980
can just combine it with handle or and

00:36:47,720 --> 00:36:55,520
so on with texts items and the other

00:36:51,980 --> 00:36:57,710
part is irrelevance and as you saw there

00:36:55,520 --> 00:37:00,740
in one model that we actually get got

00:36:57,710 --> 00:37:06,260
pretty good result from you just added

00:37:00,740 --> 00:37:10,550
together the closeness in vector space

00:37:06,260 --> 00:37:12,710
with some simple text features so

00:37:10,550 --> 00:37:14,900
features like being 25 or whatever

00:37:12,710 --> 00:37:18,280
and just add them together probably a

00:37:14,900 --> 00:37:21,380
weighted sum it's fine

00:37:18,280 --> 00:37:27,170
so do you have this benchmarking results

00:37:21,380 --> 00:37:30,100
on where does it yabba-doo yeah it's

00:37:27,170 --> 00:37:32,420
part of a sample application that we

00:37:30,100 --> 00:37:35,180
provide so you can run the whole thing

00:37:32,420 --> 00:37:37,250
yourself actually if you look in the

00:37:35,180 --> 00:37:39,080
parent directory of the thing I shared

00:37:37,250 --> 00:37:43,570
earlier you'll find all the sample

00:37:39,080 --> 00:37:46,760
applications with benchmark sensible so

00:37:43,570 --> 00:37:49,070
great I don't think it I don't see any

00:37:46,760 --> 00:37:51,200
other question so thanks again John and

00:37:49,070 --> 00:37:53,270
so everyone we can of course continue

00:37:51,200 --> 00:37:57,440
the discussion in the breakout Channel

00:37:53,270 --> 00:37:59,300
so basically the B bus - and you know

00:37:57,440 --> 00:38:01,250
thanks again for the presentation and

00:37:59,300 --> 00:38:04,309
have a nice evening

00:38:01,250 --> 00:38:04,309
[Music]

00:38:09,210 --> 00:38:11,270

YouTube URL: https://www.youtube.com/watch?v=JqZSNJIRfW4


