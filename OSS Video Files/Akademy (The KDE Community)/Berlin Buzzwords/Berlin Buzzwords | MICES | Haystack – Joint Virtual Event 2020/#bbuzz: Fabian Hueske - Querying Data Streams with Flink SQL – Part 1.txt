Title: #bbuzz: Fabian Hueske - Querying Data Streams with Flink SQL – Part 1
Publication date: 2020-09-10
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/querying-data-streams-flink-sql-part-i-eu

Apache Flink supports SQL as a unified API for stream and batch processing. SQL is easier to use than Flink’s lower-level APIs and covers a wide variety of use cases. In this hands-on tutorial you will learn how to run SQL queries on data streams with Apache Flink. We will look at the concepts behind continuous queries and dynamic tables and will show you how to solve different use cases with streaming SQL, including enriching and joining streaming data, computing windowed aggregations, and maintaining materialized views in external storage systems.

Prerequisites:
 - No prior knowledge of Apache Flink is required. We assume basic knowledge of SQL
 - You will need a computer with at least 8 GB RAM and Docker installed. To save time during the event, we would also like to ask you to set up the tutorial environment beforehand by following the instructions at https://github.com/ververica/sql-training/wiki/Setting-up-the-Training-Environment
Captions: 
	00:00:11,440 --> 00:00:16,960
this tutorial is

00:00:13,759 --> 00:00:18,960
about flink sql it's actually

00:00:16,960 --> 00:00:20,320
designed to be a full day training where

00:00:18,960 --> 00:00:22,560
i'm gonna

00:00:20,320 --> 00:00:24,080
try to crunch it down to two times uh

00:00:22,560 --> 00:00:26,800
two hours now

00:00:24,080 --> 00:00:27,760
we're probably not we can't cover

00:00:26,800 --> 00:00:29,199
everything but

00:00:27,760 --> 00:00:30,960
all the whole material and everything

00:00:29,199 --> 00:00:34,079
and slides and exercise

00:00:30,960 --> 00:00:34,719
everything is available online so even

00:00:34,079 --> 00:00:38,320
if

00:00:34,719 --> 00:00:41,360
we don't we're not able to

00:00:38,320 --> 00:00:43,280
cover the whole thing you can still

00:00:41,360 --> 00:00:45,840
have a look at the slides yourself and

00:00:43,280 --> 00:00:49,039
try to solve the exercises

00:00:45,840 --> 00:00:52,480
all right so i hope you all

00:00:49,039 --> 00:00:55,680
also were able to follow the

00:00:52,480 --> 00:00:59,840
setup instructions

00:00:55,680 --> 00:00:59,840
let me see

00:01:01,760 --> 00:01:08,080
so the setup instructions are

00:01:05,280 --> 00:01:09,119
in this repository i have a verica

00:01:08,080 --> 00:01:11,680
github

00:01:09,119 --> 00:01:12,479
i'll get up a very secret training and

00:01:11,680 --> 00:01:14,799
if you click

00:01:12,479 --> 00:01:14,799
on the

00:01:15,759 --> 00:01:19,840
wiki link then

00:01:18,270 --> 00:01:21,680
[Music]

00:01:19,840 --> 00:01:22,880
there's here on the menu on the right

00:01:21,680 --> 00:01:24,080
hand side

00:01:22,880 --> 00:01:27,360
is the setting of the training

00:01:24,080 --> 00:01:28,960
environment instructions

00:01:27,360 --> 00:01:30,560
it's actually quite simple it's uh it's

00:01:28,960 --> 00:01:33,119
a docker compose setup so if you have

00:01:30,560 --> 00:01:35,280
docker installed and

00:01:33,119 --> 00:01:36,479
have assigned enough resources so you

00:01:35,280 --> 00:01:38,840
should

00:01:36,479 --> 00:01:40,400
assign something around three to four

00:01:38,840 --> 00:01:42,240
gigs

00:01:40,400 --> 00:01:43,840
then everything should pretty much work

00:01:42,240 --> 00:01:47,520
out of the box if you

00:01:43,840 --> 00:01:47,520
bring up the docker compose environment

00:01:50,880 --> 00:01:56,880
all right but now let's get get

00:01:54,079 --> 00:01:57,280
get started so i'm not sure how much

00:01:56,880 --> 00:01:59,840
you've

00:01:57,280 --> 00:02:01,119
uh how much you've heard about apache

00:01:59,840 --> 00:02:02,640
flink so i'll

00:02:01,119 --> 00:02:05,280
give a very brief introduction into

00:02:02,640 --> 00:02:09,550
apache flink and then

00:02:05,280 --> 00:02:11,360
move into the secret parts of it

00:02:09,550 --> 00:02:16,800
[Music]

00:02:11,360 --> 00:02:18,480
i'm not let me see if i can

00:02:16,800 --> 00:02:20,800
see the questions here yeah so from time

00:02:18,480 --> 00:02:22,319
to time i'm simply

00:02:20,800 --> 00:02:24,239
we'll come over to this uh to this

00:02:22,319 --> 00:02:27,440
window and see if there's if there

00:02:24,239 --> 00:02:30,080
are any questions and um

00:02:27,440 --> 00:02:31,920
yeah i think it's also possible that you

00:02:30,080 --> 00:02:35,519
uh enabling microphone and

00:02:31,920 --> 00:02:37,280
maybe uh talk to me directly let's let's

00:02:35,519 --> 00:02:40,480
see if that works later

00:02:37,280 --> 00:02:41,280
okay so i'm getting confused with all

00:02:40,480 --> 00:02:44,000
these windows

00:02:41,280 --> 00:02:45,920
so all right so apache flink is a

00:02:44,000 --> 00:02:48,959
distributed data processing system

00:02:45,920 --> 00:02:51,360
it's um a system for

00:02:48,959 --> 00:02:53,040
yeah processing large amounts of data in

00:02:51,360 --> 00:02:55,200
a parallel fashion

00:02:53,040 --> 00:02:56,319
you can process all kinds of data with

00:02:55,200 --> 00:02:58,480
apache flink

00:02:56,319 --> 00:03:00,800
it's good for processing real-time

00:02:58,480 --> 00:03:04,959
events so streams of data

00:03:00,800 --> 00:03:08,000
for instance coming from

00:03:04,959 --> 00:03:11,599
log systems like apache kafka

00:03:08,000 --> 00:03:13,120
or sensor data streams but you can also

00:03:11,599 --> 00:03:15,840
process uh

00:03:13,120 --> 00:03:16,879
static data like data like like files

00:03:15,840 --> 00:03:19,360
reading from a

00:03:16,879 --> 00:03:21,440
distributed file system or reading data

00:03:19,360 --> 00:03:24,000
from a database

00:03:21,440 --> 00:03:25,840
flink is used for mainly three different

00:03:24,000 --> 00:03:27,519
types of applications

00:03:25,840 --> 00:03:28,879
one of them being streaming so-called

00:03:27,519 --> 00:03:30,080
streaming or what we call streaming

00:03:28,879 --> 00:03:33,920
pipelines

00:03:30,080 --> 00:03:35,680
it's basically um yeah a type of

00:03:33,920 --> 00:03:37,120
application that moves data from one

00:03:35,680 --> 00:03:39,599
system to the other

00:03:37,120 --> 00:03:42,080
uh maybe doing some some uh

00:03:39,599 --> 00:03:43,840
transformations aggregations

00:03:42,080 --> 00:03:46,159
enriching the data on and so on so

00:03:43,840 --> 00:03:47,280
basically something that you can imagine

00:03:46,159 --> 00:03:51,360
as being a

00:03:47,280 --> 00:03:54,720
low latency etl style of job

00:03:51,360 --> 00:03:58,799
then there is also analytics both for

00:03:54,720 --> 00:04:00,879
streaming and batch data so just the

00:03:58,799 --> 00:04:02,879
traditional uh yeah but what you would

00:04:00,879 --> 00:04:04,239
usually do with the sql queries or

00:04:02,879 --> 00:04:07,120
machine learning

00:04:04,239 --> 00:04:08,799
applications analyzing the data and

00:04:07,120 --> 00:04:11,120
crunching it down

00:04:08,799 --> 00:04:11,840
and then finally there's another type of

00:04:11,120 --> 00:04:14,080
application

00:04:11,840 --> 00:04:15,360
what we call event driven applications

00:04:14,080 --> 00:04:18,880
uh which are

00:04:15,360 --> 00:04:21,519
applications that are basically um

00:04:18,880 --> 00:04:22,479
yeah get their their their data or the

00:04:21,519 --> 00:04:25,040
interaction

00:04:22,479 --> 00:04:26,080
through events and then process all

00:04:25,040 --> 00:04:28,080
these events you can

00:04:26,080 --> 00:04:30,320
you can think of like a event driven

00:04:28,080 --> 00:04:31,360
application uh processing order events

00:04:30,320 --> 00:04:35,120
for instance

00:04:31,360 --> 00:04:37,120
um flink recently um added a new api

00:04:35,120 --> 00:04:38,320
for for these uh event-driven

00:04:37,120 --> 00:04:41,040
applications

00:04:38,320 --> 00:04:41,600
uh these so-called state-funded state

00:04:41,040 --> 00:04:44,800
fun

00:04:41,600 --> 00:04:45,680
api my colleague stefan even just gave a

00:04:44,800 --> 00:04:47,840
talk about this

00:04:45,680 --> 00:04:50,479
so if you're interested in that i can

00:04:47,840 --> 00:04:53,520
really recommend going back and

00:04:50,479 --> 00:04:56,880
checking out his talk as well

00:04:53,520 --> 00:04:58,720
all right so that's this is like apache

00:04:56,880 --> 00:05:00,400
flink on a very high level from the use

00:04:58,720 --> 00:05:02,320
case point of view you can run flink in

00:05:00,400 --> 00:05:05,840
very different kinds of environments

00:05:02,320 --> 00:05:09,440
like uh kubernetes uh yarn messes

00:05:05,840 --> 00:05:13,600
uh also uh on bare metal clusters if you

00:05:09,440 --> 00:05:15,600
really want to do that you can

00:05:13,600 --> 00:05:16,880
integrate it integrates with many many

00:05:15,600 --> 00:05:19,520
other systems in the

00:05:16,880 --> 00:05:20,320
in the ecosystem you can store your data

00:05:19,520 --> 00:05:24,800
on hdfs

00:05:20,320 --> 00:05:26,960
s3 azure and so on so it's uh

00:05:24,800 --> 00:05:29,039
like a full-fledged uh distributed data

00:05:26,960 --> 00:05:31,840
processor

00:05:29,039 --> 00:05:33,120
uh flink comes with uh very flexible and

00:05:31,840 --> 00:05:35,840
expressive apis

00:05:33,120 --> 00:05:36,240
it guarantees the correctness so there

00:05:35,840 --> 00:05:39,840
is

00:05:36,240 --> 00:05:42,880
exactly one state consistency so even if

00:05:39,840 --> 00:05:43,520
one of your distributed processes goes

00:05:42,880 --> 00:05:46,880
down

00:05:43,520 --> 00:05:49,600
flink guarantees that it will

00:05:46,880 --> 00:05:50,560
continue processing once or it will

00:05:49,600 --> 00:05:53,919
recover this

00:05:50,560 --> 00:05:55,840
task and continue processing as is that

00:05:53,919 --> 00:05:56,960
basically as if this error never

00:05:55,840 --> 00:05:59,039
happened um

00:05:56,960 --> 00:06:01,120
it has event time semantics we'll

00:05:59,039 --> 00:06:04,000
actually have have a look at that

00:06:01,120 --> 00:06:05,120
uh tomorrow a little bit and it runs at

00:06:04,000 --> 00:06:08,560
a really large scale

00:06:05,120 --> 00:06:10,560
so we have users

00:06:08,560 --> 00:06:11,600
that run flink jobs on ten thousand of

00:06:10,560 --> 00:06:14,960
course

00:06:11,600 --> 00:06:17,120
managing several terabytes of data

00:06:14,960 --> 00:06:18,960
this is a list of some of the companies

00:06:17,120 --> 00:06:20,560
using apache flink either using flink

00:06:18,960 --> 00:06:23,680
for the internal use cases or

00:06:20,560 --> 00:06:26,880
providing services or

00:06:23,680 --> 00:06:27,440
yeah computing services based on apache

00:06:26,880 --> 00:06:32,319
link

00:06:27,440 --> 00:06:35,280
for instance amazon or alibaba offering

00:06:32,319 --> 00:06:37,120
flink in the cloud cloud environments um

00:06:35,280 --> 00:06:39,840
other users like

00:06:37,120 --> 00:06:39,840
lyft or

00:06:40,560 --> 00:06:45,199
or researchgate or uber use

00:06:43,600 --> 00:06:47,680
flink for their internal data processing

00:06:45,199 --> 00:06:50,639
needs this is just a small

00:06:47,680 --> 00:06:51,280
well not that small but uh but a sample

00:06:50,639 --> 00:06:54,560
of

00:06:51,280 --> 00:06:55,360
all the flink users um we are uh running

00:06:54,560 --> 00:06:58,720
a conference

00:06:55,360 --> 00:07:02,080
uh called apache uh sorry fling forward

00:06:58,720 --> 00:07:04,720
um have run it for uh like several times

00:07:02,080 --> 00:07:06,080
now and there is a bunch of talks that

00:07:04,720 --> 00:07:09,360
we recorded and put

00:07:06,080 --> 00:07:12,560
on youtube so

00:07:09,360 --> 00:07:13,680
why would you like to use sql for stream

00:07:12,560 --> 00:07:16,800
processing

00:07:13,680 --> 00:07:20,560
well first of all stream processing

00:07:16,800 --> 00:07:23,120
is implementing stream processing

00:07:20,560 --> 00:07:26,000
requires a certain skill set it's not

00:07:23,120 --> 00:07:26,960
that it's not that easy to find so first

00:07:26,000 --> 00:07:29,759
of all you need

00:07:26,960 --> 00:07:30,240
at least for apache flink you need a

00:07:29,759 --> 00:07:32,319
good

00:07:30,240 --> 00:07:34,160
java or scala developers so flink is

00:07:32,319 --> 00:07:36,960
implemented in java so and

00:07:34,160 --> 00:07:37,680
integrate into the jbm ecosystem so you

00:07:36,960 --> 00:07:40,639
need

00:07:37,680 --> 00:07:41,199
somebody who can code in java or scada

00:07:40,639 --> 00:07:42,960
you need

00:07:41,199 --> 00:07:44,800
a good knowledge of stream processing

00:07:42,960 --> 00:07:47,759
concepts like how to handle

00:07:44,800 --> 00:07:49,840
uh how to work with uh time and state

00:07:47,759 --> 00:07:50,560
and also since it's a distributed system

00:07:49,840 --> 00:07:53,280
have some

00:07:50,560 --> 00:07:54,000
some some knowledge on uh experience in

00:07:53,280 --> 00:07:55,680
working with

00:07:54,000 --> 00:07:59,199
distributed data processing systems so

00:07:55,680 --> 00:08:02,879
that's actually a skill set that is

00:07:59,199 --> 00:08:07,120
well not that

00:08:02,879 --> 00:08:08,879
that available yet at the same time

00:08:07,120 --> 00:08:11,360
see everybody knows sql everybody uses

00:08:08,879 --> 00:08:12,240
sql or has used sql at some point in

00:08:11,360 --> 00:08:15,199
their

00:08:12,240 --> 00:08:16,240
career everybody who's working with data

00:08:15,199 --> 00:08:19,840
sql queries

00:08:16,240 --> 00:08:22,160
can be optimized and are usually

00:08:19,840 --> 00:08:22,960
efficiently executed by the by the

00:08:22,160 --> 00:08:26,400
system that

00:08:22,960 --> 00:08:29,520
runs the sql queries and if you

00:08:26,400 --> 00:08:31,440
do it right you can have a unified

00:08:29,520 --> 00:08:33,200
syntax and semantics for both batch and

00:08:31,440 --> 00:08:33,839
streaming data and this is basically

00:08:33,200 --> 00:08:37,279
what

00:08:33,839 --> 00:08:41,839
flink is trying to build so

00:08:37,279 --> 00:08:45,760
flink flink aims to provide a

00:08:41,839 --> 00:08:49,040
standard compliant sql service to query

00:08:45,760 --> 00:08:51,279
both static and streaming data alike

00:08:49,040 --> 00:08:52,560
and of course running that on apache

00:08:51,279 --> 00:08:55,279
flink using

00:08:52,560 --> 00:08:56,320
the correctness guarantees scalability

00:08:55,279 --> 00:09:00,480
and performance of

00:08:56,320 --> 00:09:02,480
flink so um

00:09:00,480 --> 00:09:04,800
let me quickly check if there's any

00:09:02,480 --> 00:09:07,839
questions not yet

00:09:04,800 --> 00:09:09,920
so what is what makes streaming sql

00:09:07,839 --> 00:09:13,040
different from traditional sql

00:09:09,920 --> 00:09:15,600
well um if you think about it

00:09:13,040 --> 00:09:16,959
basically or or most tables that you

00:09:15,600 --> 00:09:19,760
query with

00:09:16,959 --> 00:09:21,519
uh a secret query are uh are are

00:09:19,760 --> 00:09:24,000
changing right you have a have it

00:09:21,519 --> 00:09:26,000
have a table that is modified by some

00:09:24,000 --> 00:09:27,600
application with transactions

00:09:26,000 --> 00:09:30,080
uh some kind of order table where

00:09:27,600 --> 00:09:33,200
constant continuously new orders are in

00:09:30,080 --> 00:09:37,120
inserted or tables about

00:09:33,200 --> 00:09:39,120
your customers the customers

00:09:37,120 --> 00:09:41,519
change their addresses or whatever so

00:09:39,120 --> 00:09:41,519
basically

00:09:41,920 --> 00:09:46,000
usually database tables are are changing

00:09:44,880 --> 00:09:47,760
right and all these changes are

00:09:46,000 --> 00:09:50,880
basically maintained by the database

00:09:47,760 --> 00:09:51,440
and whenever you run a query on such a

00:09:50,880 --> 00:09:53,519
table

00:09:51,440 --> 00:09:55,440
the database system will conceptually

00:09:53,519 --> 00:09:57,120
take a snapshot of the table

00:09:55,440 --> 00:09:58,720
and run the query on this static

00:09:57,120 --> 00:10:01,279
snapshot and then

00:09:58,720 --> 00:10:02,240
once the query is done it gives you the

00:10:01,279 --> 00:10:05,360
result of the

00:10:02,240 --> 00:10:06,399
of the data of the of the query and this

00:10:05,360 --> 00:10:09,760
basically means

00:10:06,399 --> 00:10:12,240
that the input of the query is is finite

00:10:09,760 --> 00:10:15,360
it's like some some finite amount of

00:10:12,240 --> 00:10:16,880
of rows and um

00:10:15,360 --> 00:10:18,959
since the input is fine it also the

00:10:16,880 --> 00:10:20,640
query result is finite

00:10:18,959 --> 00:10:22,079
and it also never needs to be updated

00:10:20,640 --> 00:10:23,760
because

00:10:22,079 --> 00:10:25,120
you run a query of a snapshot of the

00:10:23,760 --> 00:10:27,040
data and

00:10:25,120 --> 00:10:28,160
you get a get a result for that if you

00:10:27,040 --> 00:10:31,040
run the query later

00:10:28,160 --> 00:10:32,000
on the next day you might get a

00:10:31,040 --> 00:10:35,200
different result because

00:10:32,000 --> 00:10:37,040
the data was changed in the meantime um

00:10:35,200 --> 00:10:38,320
stream sql processors work a little bit

00:10:37,040 --> 00:10:41,040
different

00:10:38,320 --> 00:10:42,160
um instead of taking a snapshot snapshot

00:10:41,040 --> 00:10:46,000
of the table

00:10:42,160 --> 00:10:49,680
these uh clear processors uh basically

00:10:46,000 --> 00:10:51,760
connect to the table process the data

00:10:49,680 --> 00:10:53,600
and they also process all the upcoming

00:10:51,760 --> 00:10:54,800
updates which means the queries

00:10:53,600 --> 00:10:57,680
continuously running

00:10:54,800 --> 00:10:58,320
and it receives all the updates that are

00:10:57,680 --> 00:11:00,160
applied

00:10:58,320 --> 00:11:02,000
on the on the original on the source

00:11:00,160 --> 00:11:05,920
table and then

00:11:02,000 --> 00:11:09,360
uh apply the changes on the input

00:11:05,920 --> 00:11:11,440
um compute basically the delta

00:11:09,360 --> 00:11:13,040
uh on the on the previous result and

00:11:11,440 --> 00:11:14,399
then update the result so you have a

00:11:13,040 --> 00:11:16,079
continuously running query

00:11:14,399 --> 00:11:17,519
that automatically mirrors all the

00:11:16,079 --> 00:11:20,640
changes on the input

00:11:17,519 --> 00:11:24,000
to the uh to the output

00:11:20,640 --> 00:11:26,079
this basically means that the input of

00:11:24,000 --> 00:11:27,920
of a streaming query is unbounded

00:11:26,079 --> 00:11:30,000
because you're continuously waiting for

00:11:27,920 --> 00:11:31,600
new update updates on these on the

00:11:30,000 --> 00:11:33,839
source table

00:11:31,600 --> 00:11:35,680
but it also means that the result of

00:11:33,839 --> 00:11:37,839
such a query is never final

00:11:35,680 --> 00:11:39,440
because it might change at any point in

00:11:37,839 --> 00:11:41,839
time they might at

00:11:39,440 --> 00:11:43,839
any point in time there might come some

00:11:41,839 --> 00:11:48,000
some row in this in the source table

00:11:43,839 --> 00:11:50,639
that changes just some fields or some

00:11:48,000 --> 00:11:53,120
some rows of the output so the output

00:11:50,639 --> 00:11:57,440
needs to be continuously updated

00:11:53,120 --> 00:11:59,200
however you can you can basically

00:11:57,440 --> 00:12:01,200
also show that the semantics of such a

00:11:59,200 --> 00:12:02,079
query of a query that is run on the

00:12:01,200 --> 00:12:06,000
static

00:12:02,079 --> 00:12:06,880
um snapshot of a table or contingency on

00:12:06,000 --> 00:12:09,760
the table

00:12:06,880 --> 00:12:10,000
um are actually the same and we'll have

00:12:09,760 --> 00:12:14,000
a

00:12:10,000 --> 00:12:16,000
small small example for that so

00:12:14,000 --> 00:12:17,279
let's say we're running a one-time query

00:12:16,000 --> 00:12:19,120
on a changing table

00:12:17,279 --> 00:12:20,880
we have this simple table here three

00:12:19,120 --> 00:12:23,839
fields user

00:12:20,880 --> 00:12:25,760
some change time and a url click time

00:12:23,839 --> 00:12:26,959
and some url so you can imagine this is

00:12:25,760 --> 00:12:29,519
some kind of

00:12:26,959 --> 00:12:30,880
click stream table where every click

00:12:29,519 --> 00:12:33,040
that a user does

00:12:30,880 --> 00:12:34,639
ends up being a row in this table so we

00:12:33,040 --> 00:12:38,000
have this

00:12:34,639 --> 00:12:39,360
table user mary clicks on at 12 o'clock

00:12:38,000 --> 00:12:42,240
at noon

00:12:39,360 --> 00:12:43,360
on some link then comes user bob user

00:12:42,240 --> 00:12:46,959
mary again

00:12:43,360 --> 00:12:50,959
and at that point in time somebody

00:12:46,959 --> 00:12:52,959
wants to run a query and runs this query

00:12:50,959 --> 00:12:55,040
i'm saying okay i'd like to know how

00:12:52,959 --> 00:12:58,399
often every user has clicked

00:12:55,040 --> 00:13:00,079
on on a link so what the

00:12:58,399 --> 00:13:01,440
what a traditional data system would do

00:13:00,079 --> 00:13:02,240
it would basically take a snapshot of

00:13:01,440 --> 00:13:06,320
this

00:13:02,240 --> 00:13:08,399
query would run the query on it

00:13:06,320 --> 00:13:10,079
and if while the queue is running or

00:13:08,399 --> 00:13:12,399
after the query is running a new

00:13:10,079 --> 00:13:13,760
row would be added well this row would

00:13:12,399 --> 00:13:16,560
not be considered right

00:13:13,760 --> 00:13:16,560
so basically

00:13:17,200 --> 00:13:20,959
the query would only run over the first

00:13:19,279 --> 00:13:24,160
three rows

00:13:20,959 --> 00:13:25,040
and when the query finishes result is

00:13:24,160 --> 00:13:27,680
produced

00:13:25,040 --> 00:13:28,399
and that's it the query terminates at

00:13:27,680 --> 00:13:32,800
some point

00:13:28,399 --> 00:13:32,800
the result is final and the input is

00:13:32,839 --> 00:13:37,440
is uh continued to change however the

00:13:35,440 --> 00:13:42,079
query was only run on a snapshot of

00:13:37,440 --> 00:13:45,120
it if we run this query now on uh

00:13:42,079 --> 00:13:46,639
on a in a continuous way well then let's

00:13:45,120 --> 00:13:48,399
say we start the query at this point in

00:13:46,639 --> 00:13:51,120
time when the table is empty

00:13:48,399 --> 00:13:51,839
now the first row arrives the query

00:13:51,120 --> 00:13:56,720
ingests

00:13:51,839 --> 00:13:58,480
the first row and updates the results so

00:13:56,720 --> 00:13:59,760
at this point in time there's only one

00:13:58,480 --> 00:14:01,519
user who clicked

00:13:59,760 --> 00:14:03,279
some link which is user mary and they

00:14:01,519 --> 00:14:05,279
clicked exactly once

00:14:03,279 --> 00:14:07,120
all right now the next row comes and the

00:14:05,279 --> 00:14:08,639
query updates the result the next row

00:14:07,120 --> 00:14:11,839
comes and now here you see

00:14:08,639 --> 00:14:15,279
that we're not only uh inserting a new

00:14:11,839 --> 00:14:17,040
um we're not only inserting a new row

00:14:15,279 --> 00:14:19,360
but we're also actually updating this

00:14:17,040 --> 00:14:19,839
row here uh basically changing the count

00:14:19,360 --> 00:14:23,279
from

00:14:19,839 --> 00:14:23,680
two from one to two and at this point in

00:14:23,279 --> 00:14:25,360
time

00:14:23,680 --> 00:14:26,800
the result of the query is exactly the

00:14:25,360 --> 00:14:29,760
same as the

00:14:26,800 --> 00:14:31,440
uh as the query that we run on the on

00:14:29,760 --> 00:14:31,760
the same snapshot that on the snapshot

00:14:31,440 --> 00:14:33,920
that

00:14:31,760 --> 00:14:35,680
concert had this exactly the same data

00:14:33,920 --> 00:14:38,320
as

00:14:35,680 --> 00:14:39,199
the table right now and if now we will

00:14:38,320 --> 00:14:42,160
we're adding

00:14:39,199 --> 00:14:43,440
more one more row then this update will

00:14:42,160 --> 00:14:47,839
always also be

00:14:43,440 --> 00:14:50,880
being reflected in the in the table

00:14:47,839 --> 00:14:52,639
so by this uh

00:14:50,880 --> 00:14:54,079
small small demo i wanted to basically

00:14:52,639 --> 00:14:57,360
to show that

00:14:54,079 --> 00:14:59,120
you can have the same the same semantics

00:14:57,360 --> 00:15:00,399
for sql queries regardless whether you

00:14:59,120 --> 00:15:02,399
evaluate them

00:15:00,399 --> 00:15:04,240
uh once on a snapshot of data or

00:15:02,399 --> 00:15:08,320
continue continuously

00:15:04,240 --> 00:15:08,320
on new arriving data

00:15:10,079 --> 00:15:13,839
all right any questions so far

00:15:22,000 --> 00:15:24,880
all right then

00:15:25,199 --> 00:15:28,800
let's continue um all right so these

00:15:27,680 --> 00:15:31,920
queries were actually

00:15:28,800 --> 00:15:34,000
pretty simple uh because i just used

00:15:31,920 --> 00:15:37,040
them for uh for a simple example

00:15:34,000 --> 00:15:39,040
um but um flink

00:15:37,040 --> 00:15:40,959
actually supports a fairly uh fairly

00:15:39,040 --> 00:15:44,320
large set of sql

00:15:40,959 --> 00:15:47,120
both on the streaming and the batch side

00:15:44,320 --> 00:15:49,279
on the batch side actually uh flink

00:15:47,120 --> 00:15:51,519
supports a full tpc ds support

00:15:49,279 --> 00:15:52,880
which is a tpcds it's a fairly

00:15:51,519 --> 00:15:55,120
well-known

00:15:52,880 --> 00:15:56,880
benchmark for analytical queries ds

00:15:55,120 --> 00:16:00,079
stands for decision support

00:15:56,880 --> 00:16:03,680
and the whole benchmark consists of

00:16:00,079 --> 00:16:06,839
99 queries and

00:16:03,680 --> 00:16:08,079
flink can run all of them for the

00:16:06,839 --> 00:16:09,680
streaming uh

00:16:08,079 --> 00:16:11,600
look at looking at streaming we'll

00:16:09,680 --> 00:16:12,880
support the simple

00:16:11,600 --> 00:16:15,279
simple things like selection and

00:16:12,880 --> 00:16:18,480
projection from where clauses

00:16:15,279 --> 00:16:21,519
select from where uh also supporting

00:16:18,480 --> 00:16:22,000
different types of grouper aggregations

00:16:21,519 --> 00:16:24,560
uh

00:16:22,000 --> 00:16:25,519
several types of joins uh user defined

00:16:24,560 --> 00:16:28,399
functions

00:16:25,519 --> 00:16:29,839
uh we also support um so-called over

00:16:28,399 --> 00:16:32,240
windows we'll have a look at

00:16:29,839 --> 00:16:33,279
what that is um if you're not familiar

00:16:32,240 --> 00:16:35,199
with these uh

00:16:33,279 --> 00:16:37,600
this is standard sql syntax so all of

00:16:35,199 --> 00:16:38,880
this is on standard sql syntax

00:16:37,600 --> 00:16:42,959
if you're not familiar with these over

00:16:38,880 --> 00:16:45,920
windows we'll discuss them tomorrow

00:16:42,959 --> 00:16:47,440
um yeah and there's also something that

00:16:45,920 --> 00:16:49,199
is pretty exciting

00:16:47,440 --> 00:16:51,600
which is the so-called match recognize

00:16:49,199 --> 00:16:52,720
clause it's a fairly recent addition to

00:16:51,600 --> 00:16:58,000
the sql standard

00:16:52,720 --> 00:17:01,279
it came with a sql 2016 and it's

00:16:58,000 --> 00:17:04,720
it's an it's a clause that allows you to

00:17:01,279 --> 00:17:06,319
evaluate patterns on on tables

00:17:04,720 --> 00:17:07,760
and you can imagine that pattern

00:17:06,319 --> 00:17:10,720
matching and stream processing or

00:17:07,760 --> 00:17:11,839
and continuously evaluating newly

00:17:10,720 --> 00:17:15,280
arriving data

00:17:11,839 --> 00:17:16,400
is can lead to very interesting or can

00:17:15,280 --> 00:17:19,439
means that you can solve very

00:17:16,400 --> 00:17:22,240
interesting use cases with it

00:17:19,439 --> 00:17:23,679
um so what are the common use cases for

00:17:22,240 --> 00:17:25,360
flink seeker well

00:17:23,679 --> 00:17:26,799
i told you a little bit about data

00:17:25,360 --> 00:17:28,160
pipelines before so this is something

00:17:26,799 --> 00:17:31,200
that is uh

00:17:28,160 --> 00:17:32,160
obviously a common use case for apache

00:17:31,200 --> 00:17:34,640
link

00:17:32,160 --> 00:17:37,039
because you can use it to easily move

00:17:34,640 --> 00:17:38,960
data around so you can

00:17:37,039 --> 00:17:40,880
read data from for instance from a kafka

00:17:38,960 --> 00:17:43,120
topic and then

00:17:40,880 --> 00:17:46,080
perform some some aggregations and write

00:17:43,120 --> 00:17:48,480
it to a file or to a database

00:17:46,080 --> 00:17:49,280
basically moving data from one system to

00:17:48,480 --> 00:17:52,720
the other

00:17:49,280 --> 00:17:55,919
and applying arbitrary transformations

00:17:52,720 --> 00:17:58,000
or joining data streams together

00:17:55,919 --> 00:17:59,440
with sql is much easier than

00:17:58,000 --> 00:18:01,600
implementing a

00:17:59,440 --> 00:18:04,880
full-fledged stream processing job in

00:18:01,600 --> 00:18:04,880
java or scala for that

00:18:05,520 --> 00:18:13,039
the other use case is analytics

00:18:10,320 --> 00:18:14,640
here meaning that you're not not really

00:18:13,039 --> 00:18:18,480
moving data around

00:18:14,640 --> 00:18:21,600
but but mostly using flink to

00:18:18,480 --> 00:18:23,840
compute aggregates or

00:18:21,600 --> 00:18:25,200
or some kind of some other metrics that

00:18:23,840 --> 00:18:28,400
you're interested in

00:18:25,200 --> 00:18:30,000
and then you can store the result of

00:18:28,400 --> 00:18:31,760
this query for instance

00:18:30,000 --> 00:18:33,760
uh as something that is kind of like

00:18:31,760 --> 00:18:37,840
similar to a materialist view

00:18:33,760 --> 00:18:40,320
in a database and then use um

00:18:37,840 --> 00:18:42,080
a security database to always get the

00:18:40,320 --> 00:18:44,880
latest result of the

00:18:42,080 --> 00:18:44,880
of the query

00:18:46,480 --> 00:18:50,799
all right so much for a rough

00:18:49,280 --> 00:18:54,480
introduction into what

00:18:50,799 --> 00:18:57,200
uh flink sql is

00:18:54,480 --> 00:18:59,200
let's now have a look at the uh at the

00:18:57,200 --> 00:19:01,679
at the training environment

00:18:59,200 --> 00:19:03,280
um all right what we're doing in the

00:19:01,679 --> 00:19:06,720
training

00:19:03,280 --> 00:19:07,520
let me quickly check all right uh so

00:19:06,720 --> 00:19:10,400
what will we do

00:19:07,520 --> 00:19:11,520
in this uh in this tutorial well first

00:19:10,400 --> 00:19:13,440
of all we'll uh

00:19:11,520 --> 00:19:15,760
run a bunch of queries on streaming data

00:19:13,440 --> 00:19:17,280
um so you get a get a feeling

00:19:15,760 --> 00:19:18,960
for how this works and what you can

00:19:17,280 --> 00:19:22,480
actually do with it uh

00:19:18,960 --> 00:19:24,960
we'll uh also uh express like common

00:19:22,480 --> 00:19:28,640
stream processing operations like

00:19:24,960 --> 00:19:30,799
aggregating data streams on time

00:19:28,640 --> 00:19:31,840
hopefully also getting to the point that

00:19:30,799 --> 00:19:35,039
we can

00:19:31,840 --> 00:19:36,320
join streams together or write some of

00:19:35,039 --> 00:19:39,440
these

00:19:36,320 --> 00:19:42,160
next recognized clauses if we don't

00:19:39,440 --> 00:19:43,760
come to that as i said all the material

00:19:42,160 --> 00:19:45,440
is online available in the github

00:19:43,760 --> 00:19:48,480
repository and you can

00:19:45,440 --> 00:19:52,640
check out the slides and the exercises

00:19:48,480 --> 00:19:52,640
uh also after the after the tutorial

00:19:52,799 --> 00:19:55,360
we'll also

00:19:56,080 --> 00:20:01,600
write some of the use of link sql to

00:19:59,679 --> 00:20:04,480
write data to external systems so just

00:20:01,600 --> 00:20:05,600
running sql queries on streams is fun

00:20:04,480 --> 00:20:07,679
but when you actually want to do

00:20:05,600 --> 00:20:08,320
something with it you kind of like have

00:20:07,679 --> 00:20:11,679
to

00:20:08,320 --> 00:20:14,400
materialize your results somewhere and

00:20:11,679 --> 00:20:16,320
we'll do that with apache kafka so

00:20:14,400 --> 00:20:17,919
writing

00:20:16,320 --> 00:20:20,320
the result of a streaming query back

00:20:17,919 --> 00:20:23,760
into kafka or into my sql

00:20:20,320 --> 00:20:26,880
and then we'll use flink's cli client

00:20:23,760 --> 00:20:30,559
sql cli client for all of the

00:20:26,880 --> 00:20:30,559
exercises in the tutorial

00:20:32,880 --> 00:20:36,159
we're using um a scenario here which is

00:20:35,440 --> 00:20:39,919
based on

00:20:36,159 --> 00:20:43,600
uh taxi ride data so this this is a

00:20:39,919 --> 00:20:46,400
public data set that basically

00:20:43,600 --> 00:20:48,400
uh provides data about taxi rights in

00:20:46,400 --> 00:20:51,520
new york city

00:20:48,400 --> 00:20:52,880
we're using this as a yeah basically as

00:20:51,520 --> 00:20:54,480
a scenario to run off some of the

00:20:52,880 --> 00:20:57,200
queries to make it a little bit

00:20:54,480 --> 00:20:57,919
more meaningful interesting we splitted

00:20:57,200 --> 00:21:00,799
the data

00:20:57,919 --> 00:21:02,720
up into three different tables there's a

00:21:00,799 --> 00:21:05,520
rights table that

00:21:02,720 --> 00:21:06,480
contains one start and one end event for

00:21:05,520 --> 00:21:08,720
every

00:21:06,480 --> 00:21:10,320
taxi ride that happened so when the taxi

00:21:08,720 --> 00:21:13,360
rate starts there's an event

00:21:10,320 --> 00:21:16,640
going into the rights table that says

00:21:13,360 --> 00:21:19,600
this this is the time this is

00:21:16,640 --> 00:21:21,600
the coordinates longitude and latitude

00:21:19,600 --> 00:21:25,520
this is how many passengers we

00:21:21,600 --> 00:21:28,240
we are we are

00:21:25,520 --> 00:21:30,080
moving with this right and then there's

00:21:28,240 --> 00:21:31,039
also an ant event that says okay the

00:21:30,080 --> 00:21:34,320
right ended

00:21:31,039 --> 00:21:34,799
at this location at this time so every

00:21:34,320 --> 00:21:38,000
right

00:21:34,799 --> 00:21:40,159
has two is represented as two

00:21:38,000 --> 00:21:42,240
uh different events then there is a

00:21:40,159 --> 00:21:44,480
ferrous table

00:21:42,240 --> 00:21:45,919
which has one payment event for each

00:21:44,480 --> 00:21:49,360
ride which happens

00:21:45,919 --> 00:21:51,440
um shortly before the end of the

00:21:49,360 --> 00:21:52,559
the ride and then there's another table

00:21:51,440 --> 00:21:56,080
driver changes

00:21:52,559 --> 00:21:57,520
which has one event uh for every time

00:21:56,080 --> 00:22:00,159
when a taxi is

00:21:57,520 --> 00:22:02,080
used by another by a different driver so

00:22:00,159 --> 00:22:06,559
basically we're capturing here

00:22:02,080 --> 00:22:06,559
whenever yeah taxi is

00:22:06,880 --> 00:22:11,520
used by a different driver so all of

00:22:09,840 --> 00:22:15,200
these tables are already

00:22:11,520 --> 00:22:18,080
registered and available for

00:22:15,200 --> 00:22:19,440
for immediate use in our training

00:22:18,080 --> 00:22:21,840
environment so there is no need for you

00:22:19,440 --> 00:22:24,159
to define these tables

00:22:21,840 --> 00:22:25,039
and each of these tables is backed by a

00:22:24,159 --> 00:22:27,840
kafka topic

00:22:25,039 --> 00:22:30,240
so you can imagine that there is some

00:22:27,840 --> 00:22:34,000
process pushing data into these tables

00:22:30,240 --> 00:22:36,559
and what you see in the

00:22:34,000 --> 00:22:37,440
cli client is basically all the data

00:22:36,559 --> 00:22:41,280
that was pushed

00:22:37,440 --> 00:22:44,159
into these kafka topics here's

00:22:41,280 --> 00:22:44,640
some some sample data so if you run a

00:22:44,159 --> 00:22:47,600
simple

00:22:44,640 --> 00:22:49,440
select stack query from rights you'll

00:22:47,600 --> 00:22:50,240
get a result that looks like this so you

00:22:49,440 --> 00:22:52,799
see there's an

00:22:50,240 --> 00:22:53,440
id for a right there's a taxi id

00:22:52,799 --> 00:22:57,200
basically

00:22:53,440 --> 00:22:59,440
the car that did the did the write

00:22:57,200 --> 00:23:02,000
is start here is a boolean variable that

00:22:59,440 --> 00:23:04,080
says true if it's at a start event

00:23:02,000 --> 00:23:05,440
longitude latitude time and passenger

00:23:04,080 --> 00:23:07,039
account

00:23:05,440 --> 00:23:08,559
for the fares table it looks quite

00:23:07,039 --> 00:23:09,919
similar we have the right id

00:23:08,559 --> 00:23:11,760
basically you want to know for which

00:23:09,919 --> 00:23:13,520
right somebody paid

00:23:11,760 --> 00:23:15,120
there's the time when the payment

00:23:13,520 --> 00:23:18,080
happened there's the payment

00:23:15,120 --> 00:23:20,000
method here yeah this csh means cash i

00:23:18,080 --> 00:23:23,520
guess

00:23:20,000 --> 00:23:25,440
amount of tip talls and fare

00:23:23,520 --> 00:23:27,520
and then there is these uh this driver

00:23:25,440 --> 00:23:30,640
change table where we have a

00:23:27,520 --> 00:23:32,240
taxi id the drive id and when

00:23:30,640 --> 00:23:33,919
the timestamp when the user when the

00:23:32,240 --> 00:23:37,840
driver started using the

00:23:33,919 --> 00:23:37,840
the taxi

00:23:38,000 --> 00:23:43,760
the training environment that

00:23:41,039 --> 00:23:45,360
we will use based on docker compose so

00:23:43,760 --> 00:23:48,240
each of the

00:23:45,360 --> 00:23:49,679
colored boxes here basically corresponds

00:23:48,240 --> 00:23:52,880
to a docker container

00:23:49,679 --> 00:23:54,240
we have a docker container for the cli

00:23:52,880 --> 00:23:55,919
client

00:23:54,240 --> 00:23:57,679
this is the container that we basically

00:23:55,919 --> 00:23:59,840
use to run our queries

00:23:57,679 --> 00:24:01,120
when you submit a query it basically

00:23:59,840 --> 00:24:04,320
gets submitted

00:24:01,120 --> 00:24:06,400
to job manager job manager

00:24:04,320 --> 00:24:08,480
is a component of apache flink which is

00:24:06,400 --> 00:24:11,600
the uh

00:24:08,480 --> 00:24:15,279
yeah the master node of

00:24:11,600 --> 00:24:17,200
uh of apache flink

00:24:15,279 --> 00:24:18,720
there is a web front end that you can

00:24:17,200 --> 00:24:21,360
access when everything's running here

00:24:18,720 --> 00:24:25,120
it's on localhost 8081

00:24:21,360 --> 00:24:27,760
um and then there's the task manager

00:24:25,120 --> 00:24:30,480
component flink cluster can have one or

00:24:27,760 --> 00:24:32,159
more task managers task managers are

00:24:30,480 --> 00:24:33,679
other workers that are executing the

00:24:32,159 --> 00:24:36,559
tasks and

00:24:33,679 --> 00:24:37,600
um basically when you submit a cover to

00:24:36,559 --> 00:24:40,880
the job manager

00:24:37,600 --> 00:24:42,559
it uh breaks the job done into sub

00:24:40,880 --> 00:24:45,279
into multiple tasks and distributes them

00:24:42,559 --> 00:24:47,760
to the task ventures for execution

00:24:45,279 --> 00:24:48,480
um there's a container for apache kafka

00:24:47,760 --> 00:24:51,200
because we use

00:24:48,480 --> 00:24:52,480
apache kafka as a as a data source here

00:24:51,200 --> 00:24:54,799
the sequel click line

00:24:52,480 --> 00:24:56,480
has actually two rows it's not only

00:24:54,799 --> 00:24:59,600
there for running in the cli

00:24:56,480 --> 00:25:03,039
but it also continuously pushes data

00:24:59,600 --> 00:25:06,480
into apache kafka at 10x speed here

00:25:03,039 --> 00:25:09,679
basically means that it pushes data

00:25:06,480 --> 00:25:09,679
with 10 times the

00:25:09,919 --> 00:25:14,240
timestamp timestamp speed basically if

00:25:13,279 --> 00:25:16,559
you see here

00:25:14,240 --> 00:25:18,080
two timestamps okay they have all the

00:25:16,559 --> 00:25:18,640
time all the same time stamps but if you

00:25:18,080 --> 00:25:20,720
have

00:25:18,640 --> 00:25:22,640
see here two timestamps here then you

00:25:20,720 --> 00:25:25,600
basically um

00:25:22,640 --> 00:25:26,880
um this is at 36 seconds and this is at

00:25:25,600 --> 00:25:28,960
one minute and one

00:25:26,880 --> 00:25:29,919
there's like roughly 30 seconds in

00:25:28,960 --> 00:25:33,039
between

00:25:29,919 --> 00:25:33,679
uh these two events but uh to make the

00:25:33,039 --> 00:25:34,559
whole

00:25:33,679 --> 00:25:37,120
environment a little bit more

00:25:34,559 --> 00:25:39,279
interesting we actually push them only

00:25:37,120 --> 00:25:40,720
two seconds uh three seconds apart from

00:25:39,279 --> 00:25:41,600
each other so to just get a little bit

00:25:40,720 --> 00:25:43,520
more data

00:25:41,600 --> 00:25:45,120
because this is a real data set and just

00:25:43,520 --> 00:25:47,440
looking at it

00:25:45,120 --> 00:25:49,840
at regular speed would be a little bit

00:25:47,440 --> 00:25:49,840
boring

00:25:50,159 --> 00:25:53,600
yeah then there's a small zookeeper node

00:25:52,159 --> 00:25:55,600
which is

00:25:53,600 --> 00:25:57,120
at this point still needed by kafka they

00:25:55,600 --> 00:25:59,520
are currently working on getting

00:25:57,120 --> 00:26:01,440
getting rid of the zookeeper dependency

00:25:59,520 --> 00:26:04,080
and then there is also a small

00:26:01,440 --> 00:26:11,840
mysql container that will use to push

00:26:04,080 --> 00:26:11,840
data into

00:26:12,640 --> 00:26:17,039
okay so um i said we're gonna we're

00:26:15,600 --> 00:26:21,039
going to run the queries

00:26:17,039 --> 00:26:24,559
using the flink seal icline this is a

00:26:21,039 --> 00:26:27,039
component that comes with apache flink

00:26:24,559 --> 00:26:27,600
and here two screenshots so you can get

00:26:27,039 --> 00:26:29,520
a

00:26:27,600 --> 00:26:33,440
rough rough idea how it looks it's just

00:26:29,520 --> 00:26:36,480
a regular sql cli

00:26:33,440 --> 00:26:39,679
and the flinxieli client has

00:26:36,480 --> 00:26:42,000
two modes of executing queries

00:26:39,679 --> 00:26:44,480
the first one is the so-called

00:26:42,000 --> 00:26:46,799
interactive query submission mode

00:26:44,480 --> 00:26:48,080
here you basically the user enters the

00:26:46,799 --> 00:26:51,360
query

00:26:48,080 --> 00:26:55,120
submits the query to the cli

00:26:51,360 --> 00:26:58,240
the secret client has

00:26:55,120 --> 00:27:01,440
has a catalog embedded has an optimizer

00:26:58,240 --> 00:27:03,600
also embedded and the optimizer

00:27:01,440 --> 00:27:04,720
optimizes the query and submits the

00:27:03,600 --> 00:27:07,039
resulting

00:27:04,720 --> 00:27:10,640
job to the flink cluster and then the

00:27:07,039 --> 00:27:12,480
flink cluster will start accessing the

00:27:10,640 --> 00:27:14,080
the data or reading the data from the

00:27:12,480 --> 00:27:15,520
from the tables that were referenced in

00:27:14,080 --> 00:27:18,799
the query

00:27:15,520 --> 00:27:20,320
the data goes into the query

00:27:18,799 --> 00:27:21,840
and the data goes into the queue that's

00:27:20,320 --> 00:27:24,640
running in the flink cluster

00:27:21,840 --> 00:27:25,039
and then flink feeds the results back

00:27:24,640 --> 00:27:28,080
into

00:27:25,039 --> 00:27:31,279
the sql click line and the cli displays

00:27:28,080 --> 00:27:32,559
the uh the results so this is the

00:27:31,279 --> 00:27:35,279
interactive mode so you basically run

00:27:32,559 --> 00:27:38,559
the queries and you directly see the

00:27:35,279 --> 00:27:41,840
results interactive in the cli client

00:27:38,559 --> 00:27:44,240
and there's another mode when you're

00:27:41,840 --> 00:27:45,760
submitting a query as a using the insert

00:27:44,240 --> 00:27:47,840
into

00:27:45,760 --> 00:27:49,520
syntax this basically means that you

00:27:47,840 --> 00:27:50,559
want to write the result into some other

00:27:49,520 --> 00:27:53,760
table

00:27:50,559 --> 00:27:56,880
at that uh if you do that the cure is

00:27:53,760 --> 00:27:59,120
again optimized by the sql iclient

00:27:56,880 --> 00:28:00,559
but and submit to the flink cluster but

00:27:59,120 --> 00:28:02,240
here the result of the query is

00:28:00,559 --> 00:28:05,840
basically fed back into the

00:28:02,240 --> 00:28:07,679
system that holds the sync table and

00:28:05,840 --> 00:28:10,320
the sqlc like line doesn't get any of

00:28:07,679 --> 00:28:12,720
the results back it only gets

00:28:10,320 --> 00:28:14,399
back a message that the query was

00:28:12,720 --> 00:28:17,120
submitted and this is the

00:28:14,399 --> 00:28:18,880
id of the query then you can look up the

00:28:17,120 --> 00:28:22,720
query in the

00:28:18,880 --> 00:28:22,720
end link web ui

00:28:24,240 --> 00:28:30,640
all right so much for the

00:28:27,279 --> 00:28:32,559
introduction let's now uh try to get our

00:28:30,640 --> 00:28:36,159
hands a little bit dirty

00:28:32,559 --> 00:28:38,799
and work with the actual thing

00:28:36,159 --> 00:28:40,159
um i have to admit i've never done a

00:28:38,799 --> 00:28:42,240
virtual tutorial

00:28:40,159 --> 00:28:43,679
before so usually this would be the time

00:28:42,240 --> 00:28:46,799
when

00:28:43,679 --> 00:28:48,640
you guys start coding and

00:28:46,799 --> 00:28:50,399
working and whenever you have a question

00:28:48,640 --> 00:28:53,840
you raise your hand and i uh

00:28:50,399 --> 00:28:56,799
come here come to your uh

00:28:53,840 --> 00:28:57,679
to your desk and then you ask me a

00:28:56,799 --> 00:28:59,840
question

00:28:57,679 --> 00:29:01,679
so i'm not quite sure how we should do

00:28:59,840 --> 00:29:05,039
it i would say

00:29:01,679 --> 00:29:07,919
maybe i'll walk you through the

00:29:05,039 --> 00:29:08,640
exercises and whenever something is

00:29:07,919 --> 00:29:12,320
unclear

00:29:08,640 --> 00:29:14,240
you'll ask a question and

00:29:12,320 --> 00:29:16,559
i'm going slow so you have a chance to

00:29:14,240 --> 00:29:16,559
also

00:29:16,880 --> 00:29:20,159
do some things on your own let's just

00:29:19,600 --> 00:29:23,120
start the

00:29:20,159 --> 00:29:25,039
the environment so i'm let me increase

00:29:23,120 --> 00:29:27,840
the font size a little bit

00:29:25,039 --> 00:29:27,840
um

00:29:36,159 --> 00:29:41,840
so i'm bringing up the docker compose

00:29:38,840 --> 00:29:41,840
environment

00:29:45,840 --> 00:29:50,320
now all the containers are being started

00:29:53,679 --> 00:30:01,039
and now i basically

00:29:56,799 --> 00:30:01,039
enter the sql cli

00:30:01,279 --> 00:30:03,679
client

00:30:05,279 --> 00:30:08,559
by running this command it basically

00:30:07,440 --> 00:30:12,240
runs this

00:30:08,559 --> 00:30:14,799
shell script in the sql c like container

00:30:12,240 --> 00:30:16,640
and here you see that we have this nice

00:30:14,799 --> 00:30:19,279
squirrel here

00:30:16,640 --> 00:30:20,159
on the c li secrecy light line all right

00:30:19,279 --> 00:30:22,640
let's first

00:30:20,159 --> 00:30:23,279
check uh which which which tables are

00:30:22,640 --> 00:30:26,880
available

00:30:23,279 --> 00:30:28,480
so we'll just say show tables

00:30:26,880 --> 00:30:30,399
and you see those are the three tables

00:30:28,480 --> 00:30:34,240
that we actually um

00:30:30,399 --> 00:30:34,240
already talked about you can

00:30:34,880 --> 00:30:39,840
uh describe a table let's say the rights

00:30:37,520 --> 00:30:39,840
table

00:30:40,480 --> 00:30:44,000
and uh get some some information about

00:30:42,799 --> 00:30:47,120
the schema here

00:30:44,000 --> 00:30:49,600
you see right at the tax cd and so on

00:30:47,120 --> 00:30:52,000
uh right time here is a kind of like a

00:30:49,600 --> 00:30:53,039
special column it's a so-called row time

00:30:52,000 --> 00:30:56,320
column

00:30:53,039 --> 00:30:57,519
which we will use tomorrow when we talk

00:30:56,320 --> 00:31:02,000
about

00:30:57,519 --> 00:31:05,039
queries and time this is basically

00:31:02,000 --> 00:31:07,360
somewhat corresponding to a

00:31:05,039 --> 00:31:08,480
to the event time attribute that you get

00:31:07,360 --> 00:31:11,039
when you use uh

00:31:08,480 --> 00:31:12,480
the flink data stream api so if you have

00:31:11,039 --> 00:31:14,480
have done that before if not

00:31:12,480 --> 00:31:15,760
don't worry about it yet uh we'll come

00:31:14,480 --> 00:31:18,960
to that later

00:31:15,760 --> 00:31:24,720
so we can now let's say

00:31:18,960 --> 00:31:24,720
run a simple query on this table

00:31:25,360 --> 00:31:30,159
and yeah when i start basically started

00:31:27,840 --> 00:31:33,200
the command line client

00:31:30,159 --> 00:31:36,080
or the docker compost environment um

00:31:33,200 --> 00:31:37,120
the uh sql cli container started pushing

00:31:36,080 --> 00:31:39,200
data into

00:31:37,120 --> 00:31:40,159
on the apache kafka topic and this is

00:31:39,200 --> 00:31:43,440
still happening

00:31:40,159 --> 00:31:44,240
uh now so we're basically still reading

00:31:43,440 --> 00:31:48,240
the data

00:31:44,240 --> 00:31:48,240
that is being pushed to apache kafka

00:31:51,679 --> 00:31:56,320
so you see it's continuously adding more

00:31:53,519 --> 00:31:59,919
data it's not like reading a lot of data

00:31:56,320 --> 00:32:00,720
from this kafka topic and still being uh

00:31:59,919 --> 00:32:02,720
displaying it

00:32:00,720 --> 00:32:04,000
but actually the data is fed into the

00:32:02,720 --> 00:32:07,120
kafka topic

00:32:04,000 --> 00:32:07,120
right at this point in time

00:32:07,279 --> 00:32:10,880
so usually this interactive session has

00:32:10,320 --> 00:32:13,760
meant

00:32:10,880 --> 00:32:16,559
that you like become familiar with the

00:32:13,760 --> 00:32:16,559
environment here

00:32:18,640 --> 00:32:23,200
so we've done that select star from

00:32:21,120 --> 00:32:25,279
rights

00:32:23,200 --> 00:32:27,679
there's also a bunch of sql functions

00:32:25,279 --> 00:32:32,399
already available

00:32:27,679 --> 00:32:32,399
we say show functions

00:32:32,559 --> 00:32:36,960
this is basically the list of all

00:32:34,720 --> 00:32:40,840
functions that are

00:32:36,960 --> 00:32:43,600
built into apache flink

00:32:40,840 --> 00:32:46,559
um

00:32:43,600 --> 00:32:48,000
yeah this is the list of uh let's build

00:32:46,559 --> 00:32:51,600
in functions

00:32:48,000 --> 00:32:52,880
and um in addition to that there's also

00:32:51,600 --> 00:32:55,840
a few

00:32:52,880 --> 00:32:57,440
um user defined functions that we

00:32:55,840 --> 00:32:59,919
implemented here

00:32:57,440 --> 00:33:01,120
that make it easier to uh to work with

00:32:59,919 --> 00:33:03,200
some of the data

00:33:01,120 --> 00:33:05,760
like for instance a time diff function

00:33:03,200 --> 00:33:08,799
that takes to timestamps and then

00:33:05,760 --> 00:33:11,200
basically returns the difference uh

00:33:08,799 --> 00:33:12,080
of the timestamps in milliseconds there

00:33:11,200 --> 00:33:15,360
is this

00:33:12,080 --> 00:33:18,240
um is in uh nyc

00:33:15,360 --> 00:33:19,919
function where you pass the longitudinal

00:33:18,240 --> 00:33:21,679
the latitude

00:33:19,919 --> 00:33:23,679
and the function will basically tell you

00:33:21,679 --> 00:33:26,159
whether this

00:33:23,679 --> 00:33:28,799
location is roughly in the near new york

00:33:26,159 --> 00:33:28,799
city area

00:33:29,600 --> 00:33:36,240
to functions to area id

00:33:32,640 --> 00:33:39,840
where longitude and latitude coordinates

00:33:36,240 --> 00:33:42,480
are mapped into a grid of cells

00:33:39,840 --> 00:33:43,600
each cell being roughly 100 by 100

00:33:42,480 --> 00:33:45,440
meters

00:33:43,600 --> 00:33:47,120
we're going to use that to later

00:33:45,440 --> 00:33:49,840
aggregate on this

00:33:47,120 --> 00:33:52,399
on this area id and also the inverse

00:33:49,840 --> 00:33:53,360
function the two coordinates function

00:33:52,399 --> 00:33:55,360
where you

00:33:53,360 --> 00:33:56,480
basically pass the array id and then it

00:33:55,360 --> 00:34:00,880
will uh

00:33:56,480 --> 00:34:02,960
return the center of the of of the cell

00:34:00,880 --> 00:34:05,200
um yeah and then there's also this

00:34:02,960 --> 00:34:07,120
driver function so i'm not going to

00:34:05,200 --> 00:34:08,960
talk about this now it's uh mostly

00:34:07,120 --> 00:34:12,480
important for some of the

00:34:08,960 --> 00:34:13,440
joining exercises later flink also

00:34:12,480 --> 00:34:15,359
support or the

00:34:13,440 --> 00:34:16,720
cli client also supports uh creating

00:34:15,359 --> 00:34:21,839
views so you can

00:34:16,720 --> 00:34:21,839
pretty easily um

00:34:22,079 --> 00:34:26,320
implement views like this this now

00:34:25,520 --> 00:34:28,639
creates a view

00:34:26,320 --> 00:34:30,720
right starts where we only have the

00:34:28,639 --> 00:34:33,040
rights that

00:34:30,720 --> 00:34:35,040
the right events that are right start

00:34:33,040 --> 00:34:39,280
events so we can now

00:34:35,040 --> 00:34:39,280
select from

00:34:40,879 --> 00:34:45,760
right starts

00:34:43,919 --> 00:34:47,200
and just as you would expect now we get

00:34:45,760 --> 00:34:51,119
all the starting events you see

00:34:47,200 --> 00:34:51,119
the start flag is always true

00:34:51,520 --> 00:34:55,280
not that exciting to be honest

00:34:55,440 --> 00:35:03,119
um all right i think

00:34:58,880 --> 00:35:05,920
we can

00:35:03,119 --> 00:35:08,079
skip the exercises here that's mostly

00:35:05,920 --> 00:35:08,079
for

00:35:08,960 --> 00:35:12,880
for defining views so here's

00:35:15,200 --> 00:35:20,320
an exercise this task is to ex the task

00:35:18,640 --> 00:35:21,359
of this exercise to cleanse the tables

00:35:20,320 --> 00:35:23,760
of right events

00:35:21,359 --> 00:35:25,520
by removing events that do not start or

00:35:23,760 --> 00:35:30,000
end in new york city

00:35:25,520 --> 00:35:33,920
for this we would basically use the um

00:35:30,000 --> 00:35:39,280
the is an nyc user defined function

00:35:33,920 --> 00:35:42,880
and then call the function with the

00:35:39,280 --> 00:35:46,320
coordinates um with the start or

00:35:42,880 --> 00:35:50,160
end coordinates of

00:35:46,320 --> 00:35:52,720
the of the event and then filter on

00:35:50,160 --> 00:35:54,880
filter on the result and only letting

00:35:52,720 --> 00:35:57,839
those through that are actually in

00:35:54,880 --> 00:36:00,880
in new york city so it's very simple

00:35:57,839 --> 00:36:00,880
select statement here

00:36:03,119 --> 00:36:06,720
all right so um yeah what we've done so

00:36:05,599 --> 00:36:09,520
far was yeah

00:36:06,720 --> 00:36:12,000
fairly fairly simple simple very basic

00:36:09,520 --> 00:36:14,079
sql stuff nothing exciting yet

00:36:12,000 --> 00:36:16,000
but yeah you just got in basically an

00:36:14,079 --> 00:36:19,119
idea of

00:36:16,000 --> 00:36:21,200
how these how this training environment

00:36:19,119 --> 00:36:24,079
looks like and what it means to or

00:36:21,200 --> 00:36:26,400
how it feels to run simple queries on

00:36:24,079 --> 00:36:30,000
data streams

00:36:26,400 --> 00:36:31,680
so in flink or the

00:36:30,000 --> 00:36:33,920
like the underlying concept and flink

00:36:31,680 --> 00:36:35,760
that that flink uses for for running

00:36:33,920 --> 00:36:40,320
continuous secret queries

00:36:35,760 --> 00:36:42,960
is a so-called dynamic table and um

00:36:40,320 --> 00:36:45,839
a dynamic table is basically what the

00:36:42,960 --> 00:36:48,880
name suggests it's a table that is uh

00:36:45,839 --> 00:36:50,800
changing over time that is evolving um

00:36:48,880 --> 00:36:52,880
as i said before basically every table

00:36:50,800 --> 00:36:54,800
is is changing but

00:36:52,880 --> 00:36:56,400
usually this is not exposed to the to

00:36:54,800 --> 00:36:57,119
the query right because when you run a

00:36:56,400 --> 00:36:59,200
query

00:36:57,119 --> 00:37:00,400
uh the databases uh database system

00:36:59,200 --> 00:37:03,520
takes a snapshot

00:37:00,400 --> 00:37:05,920
um using techniques like uh yeah

00:37:03,520 --> 00:37:07,280
this this isolation and transactions to

00:37:05,920 --> 00:37:08,640
actually make sure that

00:37:07,280 --> 00:37:10,320
you only get a consistent view of the

00:37:08,640 --> 00:37:12,560
data

00:37:10,320 --> 00:37:12,560
but

00:37:13,520 --> 00:37:17,359
the security itself only sees a static

00:37:15,599 --> 00:37:19,040
data set

00:37:17,359 --> 00:37:21,040
in contrast when you're running a

00:37:19,040 --> 00:37:22,880
continuous query in flink

00:37:21,040 --> 00:37:24,640
you're running the query on a dynamic

00:37:22,880 --> 00:37:28,560
table and

00:37:24,640 --> 00:37:31,119
the query itself internally needs to use

00:37:28,560 --> 00:37:33,839
needs needs to have some some state to

00:37:31,119 --> 00:37:37,040
hold intermediate results

00:37:33,839 --> 00:37:41,839
and then the output of such a

00:37:37,040 --> 00:37:45,200
query is again a dynamic table

00:37:41,839 --> 00:37:47,920
a dynamic table can get its data

00:37:45,200 --> 00:37:48,720
through different kinds of connectors

00:37:47,920 --> 00:37:52,400
flink is a

00:37:48,720 --> 00:37:52,400
has a bunch of connectors it has a

00:37:52,800 --> 00:37:57,440
kafka connector for apache kafka you can

00:37:56,000 --> 00:38:01,119
also read data

00:37:57,440 --> 00:38:02,800
via a jdbc uh driver so there's actually

00:38:01,119 --> 00:38:05,200
many more databases that you can access

00:38:02,800 --> 00:38:08,320
than just post or send my sql every

00:38:05,200 --> 00:38:08,880
database that has uh has a jdbc

00:38:08,320 --> 00:38:10,560
connector

00:38:08,880 --> 00:38:13,119
it also works with different file

00:38:10,560 --> 00:38:16,079
systems like hdfs or s3

00:38:13,119 --> 00:38:18,800
and then again can materialize the

00:38:16,079 --> 00:38:21,359
result of a query which is represented

00:38:18,800 --> 00:38:22,720
internally again as a dynamic table

00:38:21,359 --> 00:38:24,560
also to all these different kinds of

00:38:22,720 --> 00:38:27,599
systems

00:38:24,560 --> 00:38:28,480
something that is important to uh to

00:38:27,599 --> 00:38:30,000
understand is

00:38:28,480 --> 00:38:31,680
that the dynamic table is not

00:38:30,000 --> 00:38:35,520
necessarily something that is

00:38:31,680 --> 00:38:37,920
materialized within apache flink

00:38:35,520 --> 00:38:38,640
so it's uh first of all it's a more or

00:38:37,920 --> 00:38:42,560
less a

00:38:38,640 --> 00:38:44,079
conceptual conceptual thing here

00:38:42,560 --> 00:38:46,079
it does not mean that flink fully

00:38:44,079 --> 00:38:51,760
materializes or a

00:38:46,079 --> 00:38:51,760
copy of the of the input data

00:38:53,040 --> 00:38:56,400
there is sometimes depending on the on

00:38:55,680 --> 00:38:59,430
the

00:38:56,400 --> 00:39:00,720
data and depending on the on the query

00:38:59,430 --> 00:39:02,800
[Music]

00:39:00,720 --> 00:39:04,000
some or all of the table data needs to

00:39:02,800 --> 00:39:06,079
be materialized

00:39:04,000 --> 00:39:07,119
but this is not true for all queries

00:39:06,079 --> 00:39:10,160
there is

00:39:07,119 --> 00:39:11,040
also many queries that materialize only

00:39:10,160 --> 00:39:14,320
a small

00:39:11,040 --> 00:39:16,960
small portion of the dynamic table

00:39:14,320 --> 00:39:18,079
in state and uh can be therefore being

00:39:16,960 --> 00:39:21,280
executed quite

00:39:18,079 --> 00:39:23,119
uh quite efficiently um

00:39:21,280 --> 00:39:24,720
it's also important to know that the

00:39:23,119 --> 00:39:27,359
flink community continuously

00:39:24,720 --> 00:39:28,960
uh works on improving the connectors and

00:39:27,359 --> 00:39:31,040
also adding new connectors to different

00:39:28,960 --> 00:39:33,200
systems so

00:39:31,040 --> 00:39:35,760
this is something that is uh has become

00:39:33,200 --> 00:39:38,240
uh yeah a little bit of a focus now

00:39:35,760 --> 00:39:39,520
also to make it easier to connect uh

00:39:38,240 --> 00:39:44,720
different systems

00:39:39,520 --> 00:39:44,720
uh to this to this infrastructure

00:39:45,680 --> 00:39:49,839
so if you look at this basically there's

00:39:48,160 --> 00:39:53,040
you can you can think of

00:39:49,839 --> 00:39:54,079
the query processing like in three

00:39:53,040 --> 00:39:58,000
different steps

00:39:54,079 --> 00:40:00,800
the first step is basically getting data

00:39:58,000 --> 00:40:02,720
into uh into apache flink you can think

00:40:00,800 --> 00:40:04,960
of like it like basically converting the

00:40:02,720 --> 00:40:07,280
data that comes from kafka

00:40:04,960 --> 00:40:09,200
to a concept conceptual to a dynamic

00:40:07,280 --> 00:40:10,800
table i said before it's not necessarily

00:40:09,200 --> 00:40:13,920
materialized but conceptually

00:40:10,800 --> 00:40:15,839
there's a conversion step

00:40:13,920 --> 00:40:18,000
from from these source systems to

00:40:15,839 --> 00:40:18,720
dynamic table then there is the query

00:40:18,000 --> 00:40:21,760
execution

00:40:18,720 --> 00:40:24,000
that gets the dynamic the

00:40:21,760 --> 00:40:25,200
table from the dynamic table and

00:40:24,000 --> 00:40:27,599
computes it

00:40:25,200 --> 00:40:28,880
to a new dynamic table then there's the

00:40:27,599 --> 00:40:32,240
output step

00:40:28,880 --> 00:40:37,520
when the dynamic table now needs to be

00:40:32,240 --> 00:40:37,520
written to a sync system so there's the

00:40:37,599 --> 00:40:40,880
ingestion step there's the processing

00:40:39,440 --> 00:40:43,920
step and then there's the

00:40:40,880 --> 00:40:47,280
uh output step

00:40:43,920 --> 00:40:48,000
um right now for this uh stream or for

00:40:47,280 --> 00:40:51,440
this uh

00:40:48,000 --> 00:40:53,280
input to dynamic table conversion flink

00:40:51,440 --> 00:40:56,960
in the current version flink

00:40:53,280 --> 00:41:00,079
110 only supports the insert mode

00:40:56,960 --> 00:41:00,800
or or repent mode which basically means

00:41:00,079 --> 00:41:04,720
that

00:41:00,800 --> 00:41:08,880
all records that are pushed to flink

00:41:04,720 --> 00:41:08,880
are interpreted as insert

00:41:09,280 --> 00:41:13,280
as records that are inserted into a

00:41:11,280 --> 00:41:16,319
table so for instance if

00:41:13,280 --> 00:41:18,720
this stream of events comes from apache

00:41:16,319 --> 00:41:18,720
kafka

00:41:19,040 --> 00:41:24,640
then each of these events would

00:41:22,240 --> 00:41:27,599
correspond to a new row

00:41:24,640 --> 00:41:29,440
in this table this means of course that

00:41:27,599 --> 00:41:31,280
this table is ever growing and the more

00:41:29,440 --> 00:41:34,960
data that is added to this

00:41:31,280 --> 00:41:38,560
table on the conceptual dynamic tablets

00:41:34,960 --> 00:41:40,880
uh is is growing uh

00:41:38,560 --> 00:41:42,720
more and more and uh if you think about

00:41:40,880 --> 00:41:44,960
that and have a high volume stream

00:41:42,720 --> 00:41:46,000
um then this is also a clear indication

00:41:44,960 --> 00:41:48,400
that flink

00:41:46,000 --> 00:41:49,040
is would not even be able to materialize

00:41:48,400 --> 00:41:51,599
such a

00:41:49,040 --> 00:41:52,800
such a table internally instead because

00:41:51,599 --> 00:41:55,920
it would over time

00:41:52,800 --> 00:41:55,920
just grow too large

00:41:57,200 --> 00:42:01,200
however as i said so this is currently

00:41:59,040 --> 00:42:05,359
the only only way to

00:42:01,200 --> 00:42:07,920
get data into or convert

00:42:05,359 --> 00:42:09,680
external data into a dynamic table

00:42:07,920 --> 00:42:10,640
interpreting all of the rows as insert

00:42:09,680 --> 00:42:14,839
statements

00:42:10,640 --> 00:42:18,890
for the upcoming release flink one

00:42:14,839 --> 00:42:20,319
whoops flink 111

00:42:18,890 --> 00:42:23,680
[Music]

00:42:20,319 --> 00:42:23,680
the community worked on

00:42:25,599 --> 00:42:32,079
work on a mode that can also ingest

00:42:28,880 --> 00:42:35,040
full change locks and for instance

00:42:32,079 --> 00:42:36,480
i can convert the basium change locks

00:42:35,040 --> 00:42:40,960
into dynamic tablets so

00:42:36,480 --> 00:42:40,960
you can basically connect

00:42:41,119 --> 00:42:45,200
for instance if you're extracting

00:42:43,359 --> 00:42:47,599
postgres locks or the

00:42:45,200 --> 00:42:48,400
postgres converting a post table into a

00:42:47,599 --> 00:42:51,920
change log

00:42:48,400 --> 00:42:54,079
using using dybasium you could

00:42:51,920 --> 00:42:55,520
basically connect the dybasium change

00:42:54,079 --> 00:42:57,920
log to apache flink

00:42:55,520 --> 00:42:59,280
and then have a dynamic table that

00:42:57,920 --> 00:43:02,240
exactly mirrors

00:42:59,280 --> 00:43:03,440
um the uh table in in postgres and then

00:43:02,240 --> 00:43:05,599
run queries on

00:43:03,440 --> 00:43:06,640
uh on this table and when something is

00:43:05,599 --> 00:43:09,280
changed in

00:43:06,640 --> 00:43:10,640
uh in the postgres table the uh

00:43:09,280 --> 00:43:12,800
changelook would forward this

00:43:10,640 --> 00:43:15,520
change into apache flink apache flink

00:43:12,800 --> 00:43:18,400
would pick up this change apply it

00:43:15,520 --> 00:43:20,800
on the on the on the query and emit the

00:43:18,400 --> 00:43:20,800
result

00:43:21,280 --> 00:43:26,160
um so now you basi we basically have

00:43:25,040 --> 00:43:28,079
this

00:43:26,160 --> 00:43:29,520
conceptual this dynamic table in apache

00:43:28,079 --> 00:43:33,280
flank and

00:43:29,520 --> 00:43:34,880
we can run some queries on on it and

00:43:33,280 --> 00:43:37,839
these tablets can be cured just with

00:43:34,880 --> 00:43:40,319
regular sql so there's no

00:43:37,839 --> 00:43:41,200
not really special syntax required there

00:43:40,319 --> 00:43:45,200
is

00:43:41,200 --> 00:43:49,119
uh there are a few like special

00:43:45,200 --> 00:43:51,680
uh how to say that uh

00:43:49,119 --> 00:43:52,960
patterns i would say it's still it is

00:43:51,680 --> 00:43:55,680
standard sql but it's a

00:43:52,960 --> 00:43:57,280
it's a certain certain patterns that

00:43:55,680 --> 00:44:00,319
patterns that you use

00:43:57,280 --> 00:44:01,920
later if we're handling with uh temporal

00:44:00,319 --> 00:44:03,040
conditions so we're gonna talk about

00:44:01,920 --> 00:44:06,160
that tomorrow

00:44:03,040 --> 00:44:09,040
uh but uh the the syntax

00:44:06,160 --> 00:44:11,040
is uh all a standard compliance sql

00:44:09,040 --> 00:44:13,839
syntax

00:44:11,040 --> 00:44:14,640
and when such a query is continuously

00:44:13,839 --> 00:44:17,680
started

00:44:14,640 --> 00:44:20,240
then the results are as i said

00:44:17,680 --> 00:44:22,000
incrementally computed and updated

00:44:20,240 --> 00:44:24,640
so let's have a look at what that means

00:44:22,000 --> 00:44:27,839
for uh with a simple example

00:44:24,640 --> 00:44:30,000
so again we have uh have a have this

00:44:27,839 --> 00:44:33,040
clicks table here on the left hand side

00:44:30,000 --> 00:44:34,960
um with a user click tab url and we are

00:44:33,040 --> 00:44:38,079
running a simple filter query on

00:44:34,960 --> 00:44:41,160
uh on this on this table then

00:44:38,079 --> 00:44:42,560
we're just checking for urls that

00:44:41,160 --> 00:44:46,800
[Music]

00:44:42,560 --> 00:44:46,800
access this home home url

00:44:46,880 --> 00:44:51,359
then the first record access exactly

00:44:49,520 --> 00:44:54,960
that ul so that's why it passes

00:44:51,359 --> 00:44:54,960
the next one doesn't and so on

00:44:55,440 --> 00:44:59,280
and you see that this is a very simple

00:44:58,000 --> 00:45:02,720
query that

00:44:59,280 --> 00:45:04,880
basically also means that this query

00:45:02,720 --> 00:45:08,880
that the result of this

00:45:04,880 --> 00:45:12,000
query will result in a table that is

00:45:08,880 --> 00:45:13,680
also append only so at any point in time

00:45:12,000 --> 00:45:15,280
we can make for every record we can make

00:45:13,680 --> 00:45:17,920
a decision

00:45:15,280 --> 00:45:18,960
whether this record is part of the

00:45:17,920 --> 00:45:21,040
result or not

00:45:18,960 --> 00:45:22,720
and once we made the decision there's no

00:45:21,040 --> 00:45:24,720
reason to really

00:45:22,720 --> 00:45:26,240
change the result of this because the

00:45:24,720 --> 00:45:26,960
query doesn't change and the row doesn't

00:45:26,240 --> 00:45:30,079
change

00:45:26,960 --> 00:45:32,319
so all

00:45:30,079 --> 00:45:33,839
all updates here in the result table are

00:45:32,319 --> 00:45:36,960
always

00:45:33,839 --> 00:45:39,520
append only this is also a good example

00:45:36,960 --> 00:45:41,839
for instance for a query

00:45:39,520 --> 00:45:43,440
that would not need to materialize any

00:45:41,839 --> 00:45:45,599
of the input data right you can

00:45:43,440 --> 00:45:47,440
imagine that this clicks tablets

00:45:45,599 --> 00:45:49,520
conceptually super large

00:45:47,440 --> 00:45:51,760
but flink does not doesn't need to

00:45:49,520 --> 00:45:54,000
materialize any of the rows

00:45:51,760 --> 00:45:55,680
so the dynamic table would basically not

00:45:54,000 --> 00:45:58,720
be materialized for this query

00:45:55,680 --> 00:46:02,000
we can just apply a simple

00:45:58,720 --> 00:46:04,079
the the simple filter on the fly and uh

00:46:02,000 --> 00:46:06,640
not need to materialize any of the any

00:46:04,079 --> 00:46:06,640
of the data

00:46:07,920 --> 00:46:11,760
if we run a different query so this is

00:46:09,839 --> 00:46:13,599
the query that we had in our other

00:46:11,760 --> 00:46:16,960
example before

00:46:13,599 --> 00:46:16,960
or a similar query

00:46:18,160 --> 00:46:22,160
we basically have an aggregation query

00:46:20,000 --> 00:46:24,400
now and if we run the query

00:46:22,160 --> 00:46:25,520
we basically the result table gets

00:46:24,400 --> 00:46:28,720
updated

00:46:25,520 --> 00:46:31,040
and here you basically see

00:46:28,720 --> 00:46:33,280
that the rows of the result table need

00:46:31,040 --> 00:46:35,520
to be retracted so we need to

00:46:33,280 --> 00:46:35,520
uh

00:46:36,800 --> 00:46:39,839
to update some of the rows that we

00:46:38,640 --> 00:46:43,119
already emitted

00:46:39,839 --> 00:46:45,200
and change them if whenever we

00:46:43,119 --> 00:46:47,839
when we see new data so this the

00:46:45,200 --> 00:46:51,119
behavior of this query

00:46:47,839 --> 00:46:52,079
is is different in this in this regard

00:46:51,119 --> 00:46:55,440
to the other one

00:46:52,079 --> 00:46:56,880
because we need to change

00:46:55,440 --> 00:46:59,280
some of the results that we already

00:46:56,880 --> 00:46:59,280
emitted

00:47:02,880 --> 00:47:09,440
so now coming back to the uh to the last

00:47:07,280 --> 00:47:12,160
let me first check if there's any

00:47:09,440 --> 00:47:12,160
questions no

00:47:12,960 --> 00:47:19,440
so if we know uh now the last

00:47:17,040 --> 00:47:21,200
part of the qr execution is then

00:47:19,440 --> 00:47:26,240
basically converting the

00:47:21,200 --> 00:47:30,480
dynamic table back to the output system

00:47:26,240 --> 00:47:32,559
and there's also different modes how

00:47:30,480 --> 00:47:34,079
how this can be done and it pretty much

00:47:32,559 --> 00:47:38,160
depends on whether the

00:47:34,079 --> 00:47:38,160
result table needs to be updated or not

00:47:38,319 --> 00:47:42,319
because if they if the result table has

00:47:40,720 --> 00:47:43,920
updates they kind like

00:47:42,319 --> 00:47:45,760
need to be somehow encoded into the

00:47:43,920 --> 00:47:48,190
outgoing stream and there's different

00:47:45,760 --> 00:47:49,359
ways how fling can do that

00:47:48,190 --> 00:47:51,920
[Music]

00:47:49,359 --> 00:47:52,640
the first case is the uh is the simple

00:47:51,920 --> 00:47:56,400
case

00:47:52,640 --> 00:48:00,079
where the uh result of it of a query

00:47:56,400 --> 00:48:02,800
uh like this simple filter query is an

00:48:00,079 --> 00:48:03,839
insert only stream in this case it's the

00:48:02,800 --> 00:48:06,960
conversion is very

00:48:03,839 --> 00:48:09,520
is just exactly the same as uh

00:48:06,960 --> 00:48:10,800
as before as for for the append only or

00:48:09,520 --> 00:48:13,680
insert only

00:48:10,800 --> 00:48:14,800
ingestion in this case basically each

00:48:13,680 --> 00:48:18,800
row that passes

00:48:14,800 --> 00:48:22,400
the that is produced for by the query

00:48:18,800 --> 00:48:24,480
is just passed on as an insertion

00:48:22,400 --> 00:48:25,520
record and the downstream system can

00:48:24,480 --> 00:48:28,400
just write out

00:48:25,520 --> 00:48:29,200
all of these rows that it receives

00:48:28,400 --> 00:48:31,280
because

00:48:29,200 --> 00:48:32,800
it knows none of these rows ever needs

00:48:31,280 --> 00:48:35,839
to be updated again

00:48:32,800 --> 00:48:39,040
so this is something that we can use to

00:48:35,839 --> 00:48:42,400
write data to an apache kafka topic

00:48:39,040 --> 00:48:44,640
into some kind of file but also to other

00:48:42,400 --> 00:48:44,640
more

00:48:44,720 --> 00:48:49,839
other data stores that allow more data

00:48:46,960 --> 00:48:49,839
more

00:48:50,800 --> 00:48:54,480
update modes like elasticsearch key

00:48:53,440 --> 00:48:57,760
value store or

00:48:54,480 --> 00:49:00,400
a regular database

00:48:57,760 --> 00:49:02,640
so this is the kind of like the easiest

00:49:00,400 --> 00:49:04,319
output mode because we can just

00:49:02,640 --> 00:49:06,640
continuously write data out and never

00:49:04,319 --> 00:49:09,920
need to update anything

00:49:06,640 --> 00:49:10,400
if however the query looks like this it

00:49:09,920 --> 00:49:11,839
has

00:49:10,400 --> 00:49:15,680
some kind of aggregation theory that

00:49:11,839 --> 00:49:18,240
also needs to update some of the results

00:49:15,680 --> 00:49:19,599
then all these changes can be converted

00:49:18,240 --> 00:49:22,640
into

00:49:19,599 --> 00:49:22,640
insertion and deletion

00:49:22,720 --> 00:49:26,400
statements so for instance if you look

00:49:24,640 --> 00:49:29,599
at the at the third record here

00:49:26,400 --> 00:49:31,920
where we get the second value for mary

00:49:29,599 --> 00:49:33,440
and this time when we when the queuer is

00:49:31,920 --> 00:49:36,480
processing this record

00:49:33,440 --> 00:49:37,599
we need to basically uh remove the

00:49:36,480 --> 00:49:40,079
previous or

00:49:37,599 --> 00:49:41,119
the previous result for mary which means

00:49:40,079 --> 00:49:43,119
we

00:49:41,119 --> 00:49:44,720
here the minus indicates that we delete

00:49:43,119 --> 00:49:46,720
this record

00:49:44,720 --> 00:49:47,839
so before we for the first record we

00:49:46,720 --> 00:49:49,680
inserted mary

00:49:47,839 --> 00:49:50,960
with the count of one then we got the

00:49:49,680 --> 00:49:53,920
next record for bob

00:49:50,960 --> 00:49:56,240
added this one with the count of one and

00:49:53,920 --> 00:49:58,319
then we get the next record for mary

00:49:56,240 --> 00:50:00,720
so we need to update the count and we do

00:49:58,319 --> 00:50:03,440
this by first deleting this

00:50:00,720 --> 00:50:05,040
on our previous result and then

00:50:03,440 --> 00:50:09,359
inserting a new one

00:50:05,040 --> 00:50:14,160
so this basically means that

00:50:09,359 --> 00:50:18,400
this kind of conversion mode needs

00:50:14,160 --> 00:50:22,480
some external system that is able to

00:50:18,400 --> 00:50:24,640
delete arbitrary records

00:50:22,480 --> 00:50:26,319
this is not very easy if you if you want

00:50:24,640 --> 00:50:27,119
to do that on a file system it's not

00:50:26,319 --> 00:50:28,640
usually not

00:50:27,119 --> 00:50:31,359
not efficient because if you're writing

00:50:28,640 --> 00:50:35,440
data out for instance in some kind of

00:50:31,359 --> 00:50:37,839
format like or orc

00:50:35,440 --> 00:50:39,200
these formats are good for bulk

00:50:37,839 --> 00:50:41,280
ingestion but

00:50:39,200 --> 00:50:42,720
not very efficient for deleting

00:50:41,280 --> 00:50:46,319
individual records

00:50:42,720 --> 00:50:46,319
so you usually wouldn't do that

00:50:46,640 --> 00:50:51,200
or wouldn't write such a data that you

00:50:49,920 --> 00:50:54,640
kind of need to

00:50:51,200 --> 00:50:57,040
update on a per record level

00:50:54,640 --> 00:50:58,319
to a two file system however if you want

00:50:57,040 --> 00:51:01,119
to materialize such a

00:50:58,319 --> 00:51:02,960
such result in a relational database or

00:51:01,119 --> 00:51:06,480
in a key value store like elasticsearch

00:51:02,960 --> 00:51:06,480
then this is much easier because

00:51:06,640 --> 00:51:11,680
you can um delete individual records and

00:51:11,760 --> 00:51:17,359
also insert them

00:51:15,040 --> 00:51:18,240
and finally there's another upset

00:51:17,359 --> 00:51:19,839
another mode

00:51:18,240 --> 00:51:23,359
the so-called absurd mode upset and

00:51:19,839 --> 00:51:25,520
delete in this case

00:51:23,359 --> 00:51:27,119
this is basically all the update

00:51:25,520 --> 00:51:30,559
operations are happening

00:51:27,119 --> 00:51:32,480
on a on a key on a unique key so each

00:51:30,559 --> 00:51:33,920
row of the output needs to have a unique

00:51:32,480 --> 00:51:37,359
key

00:51:33,920 --> 00:51:39,839
that can be used to address this record

00:51:37,359 --> 00:51:41,119
in case of our query here we and this is

00:51:39,839 --> 00:51:43,280
the user field

00:51:41,119 --> 00:51:44,960
because we group on users so for every

00:51:43,280 --> 00:51:48,160
user we get a

00:51:44,960 --> 00:51:49,599
unique every user has exactly one one

00:51:48,160 --> 00:51:52,720
result record

00:51:49,599 --> 00:51:55,200
and now we can basically

00:51:52,720 --> 00:51:55,200
update

00:51:56,400 --> 00:52:01,040
we can update the result using a key

00:51:59,119 --> 00:52:03,680
which is

00:52:01,040 --> 00:52:05,440
more efficient because we don't

00:52:03,680 --> 00:52:07,760
depending on the system we don't need to

00:52:05,440 --> 00:52:09,839
first delete the record and

00:52:07,760 --> 00:52:12,559
add it again but instead we can say

00:52:09,839 --> 00:52:16,160
please update this record

00:52:12,559 --> 00:52:18,480
and this is the new value and for this

00:52:16,160 --> 00:52:19,280
for this kind of output mode you can

00:52:18,480 --> 00:52:20,720
think of

00:52:19,280 --> 00:52:23,839
writing this to something like a

00:52:20,720 --> 00:52:23,839
compacted kafka topic

00:52:24,319 --> 00:52:27,760
kafka is these are compacted topics

00:52:26,880 --> 00:52:29,680
where you can

00:52:27,760 --> 00:52:31,520
define a key message and then the topic

00:52:29,680 --> 00:52:34,720
will always only store the

00:52:31,520 --> 00:52:36,240
or provide the latest value for a key

00:52:34,720 --> 00:52:39,200
or again a key value store or a

00:52:36,240 --> 00:52:39,200
relational database

00:52:39,760 --> 00:52:44,640
let's continue with operators in state

00:52:44,839 --> 00:52:49,280
um

00:52:46,720 --> 00:52:50,160
so there is this goes nowhere a little

00:52:49,280 --> 00:52:53,599
bit into the

00:52:50,160 --> 00:52:56,720
internals of of the system

00:52:53,599 --> 00:52:58,880
so there's basically three different

00:52:56,720 --> 00:53:02,559
types of operators that flink

00:52:58,880 --> 00:53:05,200
flink users to process such queries

00:53:02,559 --> 00:53:07,040
there is stateless operators like filter

00:53:05,200 --> 00:53:10,800
and projection

00:53:07,040 --> 00:53:12,880
these are operators that don't need to

00:53:10,800 --> 00:53:14,240
persist any intermediate results when a

00:53:12,880 --> 00:53:16,720
filter gets a wreck it gets

00:53:14,240 --> 00:53:18,559
gets a record it can directly make the

00:53:16,720 --> 00:53:20,640
decision

00:53:18,559 --> 00:53:22,079
whether this record should pass or

00:53:20,640 --> 00:53:25,040
should not pass when it

00:53:22,079 --> 00:53:26,880
when a projection operator gets a row it

00:53:25,040 --> 00:53:28,559
can immediately apply the

00:53:26,880 --> 00:53:30,640
transformation that is defined in the

00:53:28,559 --> 00:53:34,319
projection and pass the row on

00:53:30,640 --> 00:53:36,160
and once the row is forwarded

00:53:34,319 --> 00:53:37,520
it doesn't need to remember this row

00:53:36,160 --> 00:53:39,760
anymore because it's a

00:53:37,520 --> 00:53:42,400
it's a simple operation and the operator

00:53:39,760 --> 00:53:45,119
doesn't need to have any state for that

00:53:42,400 --> 00:53:47,920
there's other operators like operators

00:53:45,119 --> 00:53:51,440
like aggregations or joins

00:53:47,920 --> 00:53:55,119
that need to memorize

00:53:51,440 --> 00:53:58,079
records or intermediate results because

00:53:55,119 --> 00:54:00,000
these these operators um combine

00:53:58,079 --> 00:54:02,800
multiple records with each other right

00:54:00,000 --> 00:54:04,240
so an aggregation operator um if you

00:54:02,800 --> 00:54:07,440
have for instance a

00:54:04,240 --> 00:54:11,040
group by sum operator it sums the values

00:54:07,440 --> 00:54:14,800
of multiple rows so it kind of needs to

00:54:11,040 --> 00:54:17,839
have some intermediate state to

00:54:14,800 --> 00:54:18,640
remember what what the current

00:54:17,839 --> 00:54:21,760
intermediate

00:54:18,640 --> 00:54:24,640
value for the for for the sum is

00:54:21,760 --> 00:54:25,920
a join operator joins rows of different

00:54:24,640 --> 00:54:27,680
tables together

00:54:25,920 --> 00:54:29,599
so it first needs it kind of like needs

00:54:27,680 --> 00:54:31,359
to remember the rows of the one side

00:54:29,599 --> 00:54:32,960
and when a row comes from the other side

00:54:31,359 --> 00:54:35,839
it kind of needs to look up

00:54:32,960 --> 00:54:36,400
if there are any matching partners for

00:54:35,839 --> 00:54:41,040
for this

00:54:36,400 --> 00:54:43,119
new row on the other side and vice versa

00:54:41,040 --> 00:54:44,480
this is roughly how uh how streaming

00:54:43,119 --> 00:54:48,000
joins work

00:54:44,480 --> 00:54:49,359
so a join join and also aggregation

00:54:48,000 --> 00:54:52,720
operators need to

00:54:49,359 --> 00:54:53,920
need to materialize some state

00:54:52,720 --> 00:54:57,359
internally right

00:54:53,920 --> 00:54:57,920
and then there is a and another type of

00:54:57,359 --> 00:54:59,920
uh

00:54:57,920 --> 00:55:01,280
of operators the so-called temple

00:54:59,920 --> 00:55:04,720
operators which we'll discuss

00:55:01,280 --> 00:55:06,400
uh tomorrow and these operators have

00:55:04,720 --> 00:55:09,440
some kind of temporal condition

00:55:06,400 --> 00:55:13,760
that bound the

00:55:09,440 --> 00:55:16,319
uh how to say that

00:55:13,760 --> 00:55:18,000
the bond the range of the computation so

00:55:16,319 --> 00:55:21,680
for instance you can have a

00:55:18,000 --> 00:55:25,280
have a group i operation that groups

00:55:21,680 --> 00:55:26,960
records that arrived within 10 minutes

00:55:25,280 --> 00:55:28,880
if you do that you know that after 10

00:55:26,960 --> 00:55:31,040
minutes you don't

00:55:28,880 --> 00:55:33,119
need these results anymore because you

00:55:31,040 --> 00:55:36,319
can perform the computation

00:55:33,119 --> 00:55:38,240
completed provide a immediate result

00:55:36,319 --> 00:55:39,680
and then the next 10 minutes start and

00:55:38,240 --> 00:55:40,880
whatever happens you never have to look

00:55:39,680 --> 00:55:43,760
back

00:55:40,880 --> 00:55:45,119
at the at the previous 10 minutes so

00:55:43,760 --> 00:55:46,400
these operators

00:55:45,119 --> 00:55:48,559
also hold some state but some

00:55:46,400 --> 00:55:51,040
intermediate state but they're able to

00:55:48,559 --> 00:55:52,880
clean up this state automatically

00:55:51,040 --> 00:55:54,880
because

00:55:52,880 --> 00:55:57,040
there's some temporal condition that

00:55:54,880 --> 00:55:59,280
just bounce

00:55:57,040 --> 00:56:01,359
how long an operation needs to needs to

00:55:59,280 --> 00:56:03,200
hold the state

00:56:01,359 --> 00:56:06,640
but as i said we'll talk about that a

00:56:03,200 --> 00:56:09,440
little bit more in detail tomorrow

00:56:06,640 --> 00:56:10,240
um so if you look at this query again

00:56:09,440 --> 00:56:13,280
this is the

00:56:10,240 --> 00:56:14,079
this this this group back theory um then

00:56:13,280 --> 00:56:16,960
we see that

00:56:14,079 --> 00:56:18,240
um the query here internally basically

00:56:16,960 --> 00:56:20,799
needs to hold

00:56:18,240 --> 00:56:22,400
uh one count for every user that is

00:56:20,799 --> 00:56:24,480
being processed right

00:56:22,400 --> 00:56:26,240
when we uh when when we think of how

00:56:24,480 --> 00:56:30,319
they revolve the result

00:56:26,240 --> 00:56:31,920
um before this record he arrived uh the

00:56:30,319 --> 00:56:34,640
count for mary was two

00:56:31,920 --> 00:56:35,440
so internally the query had to remember

00:56:34,640 --> 00:56:37,599
for mary

00:56:35,440 --> 00:56:39,040
the for the user mary the current card

00:56:37,599 --> 00:56:42,079
is two and when

00:56:39,040 --> 00:56:44,079
this the last record here arrived uh

00:56:42,079 --> 00:56:45,440
it said okay i need to count this so

00:56:44,079 --> 00:56:47,760
counting means i have to inc

00:56:45,440 --> 00:56:49,359
i've seen another record for um for mary

00:56:47,760 --> 00:56:52,880
i have to increment

00:56:49,359 --> 00:56:56,079
now the count for this record

00:56:52,880 --> 00:56:56,880
um and then it said okay the new count

00:56:56,079 --> 00:56:59,920
here

00:56:56,880 --> 00:57:03,280
uh is three

00:56:59,920 --> 00:57:06,960
and um internally also start this

00:57:03,280 --> 00:57:10,240
this intermediate result uh as a state

00:57:06,960 --> 00:57:12,079
but also pass it on to the to the output

00:57:10,240 --> 00:57:15,680
of the query

00:57:12,079 --> 00:57:18,960
so that the query could could update the

00:57:15,680 --> 00:57:20,559
result in the external system

00:57:18,960 --> 00:57:22,000
so this query here this simple query

00:57:20,559 --> 00:57:24,079
here basically has

00:57:22,000 --> 00:57:25,040
three different kinds of key value pairs

00:57:24,079 --> 00:57:27,119
internally

00:57:25,040 --> 00:57:29,359
one for a merry one for bob and one for

00:57:27,119 --> 00:57:29,359
this

00:57:32,799 --> 00:57:39,760
yeah so that's basically yeah and

00:57:37,280 --> 00:57:40,319
something that that i didn't mention yet

00:57:39,760 --> 00:57:42,079
is

00:57:40,319 --> 00:57:43,520
that the query here basically needs to

00:57:42,079 --> 00:57:46,880
hold this state

00:57:43,520 --> 00:57:50,160
forever so there's uh no point in time

00:57:46,880 --> 00:57:52,160
when this con query runs continuously uh

00:57:50,160 --> 00:57:53,200
if if you run this query for one year

00:57:52,160 --> 00:57:55,920
for instance

00:57:53,200 --> 00:57:56,880
then the query would accumulate all

00:57:55,920 --> 00:58:00,000
unique users

00:57:56,880 --> 00:58:00,000
that that were

00:58:00,240 --> 00:58:05,599
that were seen by the query

00:58:04,079 --> 00:58:07,119
because at any point in time there could

00:58:05,599 --> 00:58:10,079
be a new record coming

00:58:07,119 --> 00:58:11,200
that updated our account for for any

00:58:10,079 --> 00:58:14,240
user

00:58:11,200 --> 00:58:15,599
so um this also shows that

00:58:14,240 --> 00:58:17,680
for for some of the queries you have to

00:58:15,599 --> 00:58:20,839
be not necessarily

00:58:17,680 --> 00:58:23,520
uh careful but you can't like have

00:58:20,839 --> 00:58:25,839
should uh

00:58:23,520 --> 00:58:27,760
think a little bit about what the query

00:58:25,839 --> 00:58:31,599
would internally do

00:58:27,760 --> 00:58:32,960
and then do some uh back on the napkin

00:58:31,599 --> 00:58:36,079
calculation whether this is something

00:58:32,960 --> 00:58:38,240
that the system can handle or not

00:58:36,079 --> 00:58:40,160
if you're just computing counts for

00:58:38,240 --> 00:58:44,559
users then this is something that

00:58:40,160 --> 00:58:47,599
is probably not too bad um

00:58:44,559 --> 00:58:48,480
yeah um it kind of like depends it not

00:58:47,599 --> 00:58:50,240
not

00:58:48,480 --> 00:58:52,559
for some of the aggregations it actually

00:58:50,240 --> 00:58:55,839
does not only depend on the

00:58:52,559 --> 00:58:58,319
on the number of user keys

00:58:55,839 --> 00:59:00,160
but it can also depend on the type of

00:58:58,319 --> 00:59:01,440
aggregation function that you're using

00:59:00,160 --> 00:59:03,599
for instance if you're using a min

00:59:01,440 --> 00:59:06,880
function or max function

00:59:03,599 --> 00:59:06,880
then it can happen that

00:59:07,280 --> 00:59:11,680
the state for this function is not only

00:59:09,599 --> 00:59:15,200
the

00:59:11,680 --> 00:59:16,880
current minimum or maximum but also

00:59:15,200 --> 00:59:19,200
all of the intermediate values because

00:59:16,880 --> 00:59:22,720
if you have if you have a

00:59:19,200 --> 00:59:24,240
source table that might also remove some

00:59:22,720 --> 00:59:25,599
of the values it might happen that the

00:59:24,240 --> 00:59:27,839
smallest value of the

00:59:25,599 --> 00:59:28,880
or that if you have a min function that

00:59:27,839 --> 00:59:30,960
exactly the minimum

00:59:28,880 --> 00:59:32,640
value is removed and then you would

00:59:30,960 --> 00:59:34,960
basically need to update it

00:59:32,640 --> 00:59:36,240
to the second smallest value and if that

00:59:34,960 --> 00:59:37,599
is again removed you would need to

00:59:36,240 --> 00:59:39,760
update it to the third

00:59:37,599 --> 00:59:42,559
to the previously third smallest value

00:59:39,760 --> 00:59:46,000
so for instance for a min or a max

00:59:42,559 --> 00:59:47,839
function it can happen

00:59:46,000 --> 00:59:49,200
that the state of the aggregation

00:59:47,839 --> 00:59:52,799
function is basically

00:59:49,200 --> 00:59:56,880
one small data point for every

00:59:52,799 --> 00:59:58,400
record that was for every record of the

00:59:56,880 --> 01:00:01,839
input

00:59:58,400 --> 01:00:01,839
dynamic table

01:00:03,200 --> 01:00:08,000
so um yeah so this is

01:00:06,319 --> 01:00:09,680
just a just a summary of what i said

01:00:08,000 --> 01:00:11,200
before

01:00:09,680 --> 01:00:14,480
some of these materializing operators

01:00:11,200 --> 01:00:16,799
like aggregations and joints

01:00:14,480 --> 01:00:18,319
need to hold state forever because there

01:00:16,799 --> 01:00:21,280
is no

01:00:18,319 --> 01:00:22,480
temporal boundary for this for this

01:00:21,280 --> 01:00:24,720
computation

01:00:22,480 --> 01:00:24,720
and

01:00:25,680 --> 01:00:31,920
data can change at any point in time so

01:00:29,200 --> 01:00:32,880
and in order to account for that you

01:00:31,920 --> 01:00:36,000
just need to

01:00:32,880 --> 01:00:39,920
have a pro possibly

01:00:36,000 --> 01:00:39,920
a large amount of intermediate state

01:00:40,640 --> 01:00:43,440
i also said that these tempera operators

01:00:42,240 --> 01:00:44,319
can be a little bit more clever about it

01:00:43,440 --> 01:00:45,839
because

01:00:44,319 --> 01:00:47,599
um if you if you have a temporary

01:00:45,839 --> 01:00:50,160
boundary around your

01:00:47,599 --> 01:00:51,920
uh your computation then you also know

01:00:50,160 --> 01:00:55,040
how long you need to hold the state

01:00:51,920 --> 01:00:58,240
and once you know that you don't need

01:00:55,040 --> 01:01:00,799
the state anymore once you know that

01:00:58,240 --> 01:01:01,920
you you passed the temporal boundary you

01:01:00,799 --> 01:01:04,000
know that you've seen

01:01:01,920 --> 01:01:05,280
all of the data you can produce a result

01:01:04,000 --> 01:01:10,880
and then also discard

01:01:05,280 --> 01:01:14,079
all of the intermediate data

01:01:10,880 --> 01:01:17,119
so um but what can you do to

01:01:14,079 --> 01:01:18,720
or is is there a way that you can how

01:01:17,119 --> 01:01:21,920
can you handle this

01:01:18,720 --> 01:01:23,520
uh possibly growing state so there's

01:01:21,920 --> 01:01:25,119
different types of

01:01:23,520 --> 01:01:27,760
or different solutions how you how you

01:01:25,119 --> 01:01:30,319
can do that if you have

01:01:27,760 --> 01:01:30,880
some data that is growing rather slowly

01:01:30,319 --> 01:01:33,920
like

01:01:30,880 --> 01:01:35,119
the number of users that you have then

01:01:33,920 --> 01:01:37,680
you can

01:01:35,119 --> 01:01:39,920
solve this problem possibly by just

01:01:37,680 --> 01:01:41,760
scaling up the query so if

01:01:39,920 --> 01:01:43,920
you run the query on two nodes you can

01:01:41,760 --> 01:01:46,240
just scale the query possibly to

01:01:43,920 --> 01:01:47,680
three or four nodes um if you have a

01:01:46,240 --> 01:01:49,760
growing number of users then

01:01:47,680 --> 01:01:50,720
this is probably a first of all a good

01:01:49,760 --> 01:01:54,160
sign

01:01:50,720 --> 01:01:56,319
and you

01:01:54,160 --> 01:01:59,119
maybe you can afford to run the curie

01:01:56,319 --> 01:02:02,240
just on a larger setup

01:01:59,119 --> 01:02:05,200
if this is not the case

01:02:02,240 --> 01:02:07,599
then you can also use something that

01:02:05,200 --> 01:02:12,160
flink calls

01:02:07,599 --> 01:02:14,079
idle state cleanup

01:02:12,160 --> 01:02:16,079
and this is useful for situations like

01:02:14,079 --> 01:02:18,640
this if your data first has some kind of

01:02:16,079 --> 01:02:19,839
session id

01:02:18,640 --> 01:02:22,079
and you would like to group by the

01:02:19,839 --> 01:02:24,319
session id then uh

01:02:22,079 --> 01:02:25,520
it is pretty clear from the context or

01:02:24,319 --> 01:02:27,200
from from what we know

01:02:25,520 --> 01:02:28,799
that this session is only valid for a

01:02:27,200 --> 01:02:33,359
certain amount of time right

01:02:28,799 --> 01:02:36,480
um if the session hasn't is

01:02:33,359 --> 01:02:39,520
has become basically become in inactive

01:02:36,480 --> 01:02:42,000
uh we know that it will not be updated

01:02:39,520 --> 01:02:43,119
at any point in time uh in the future

01:02:42,000 --> 01:02:46,640
again

01:02:43,119 --> 01:02:48,720
however this is something that the

01:02:46,640 --> 01:02:50,400
system that flink doesn't really know

01:02:48,720 --> 01:02:51,280
and therefore can really account for so

01:02:50,400 --> 01:02:53,520
it doesn't know

01:02:51,280 --> 01:02:55,359
that this session will not be used

01:02:53,520 --> 01:02:57,440
session id will not be used again

01:02:55,359 --> 01:02:58,880
and hence it will not automatically

01:02:57,440 --> 01:03:03,119
clean up the state

01:02:58,880 --> 01:03:05,440
uh in a situation like this you can

01:03:03,119 --> 01:03:06,880
configure something like this idle state

01:03:05,440 --> 01:03:10,160
cleanup

01:03:06,880 --> 01:03:13,200
and if you do that flink

01:03:10,160 --> 01:03:14,640
will automatically remove state that has

01:03:13,200 --> 01:03:16,079
not been accessed for

01:03:14,640 --> 01:03:17,119
a certain amount of time and this is

01:03:16,079 --> 01:03:18,000
basically something that you can

01:03:17,119 --> 01:03:20,079
configure

01:03:18,000 --> 01:03:22,000
if you know that a session uh will not

01:03:20,079 --> 01:03:22,319
be used again if it hasn't been updated

01:03:22,000 --> 01:03:25,359
for

01:03:22,319 --> 01:03:28,079
for an hour for instance you could

01:03:25,359 --> 01:03:29,839
set this idle state cleanup time for one

01:03:28,079 --> 01:03:31,280
or 30 minutes or two hours if you want

01:03:29,839 --> 01:03:34,000
to be on the safe side

01:03:31,280 --> 01:03:34,960
and then if it has been updated for for

01:03:34,000 --> 01:03:37,680
two hours

01:03:34,960 --> 01:03:38,799
flink would go ahead and remove this

01:03:37,680 --> 01:03:41,920
date

01:03:38,799 --> 01:03:44,960
for this session id

01:03:41,920 --> 01:03:46,319
this works well if the data that was

01:03:44,960 --> 01:03:50,000
removed

01:03:46,319 --> 01:03:52,640
won't be used will not be used again

01:03:50,000 --> 01:03:54,640
um so in that case uh all the

01:03:52,640 --> 01:03:57,920
computations are remain valid

01:03:54,640 --> 01:04:00,160
and everything is fine however if you

01:03:57,920 --> 01:04:03,760
remove some state too early

01:04:00,160 --> 01:04:06,880
then flink will just see the new

01:04:03,760 --> 01:04:10,000
record as something completely new and

01:04:06,880 --> 01:04:12,240
uh will not remember that it has had

01:04:10,000 --> 01:04:14,960
done any computations for this

01:04:12,240 --> 01:04:17,440
before so at that point in time you then

01:04:14,960 --> 01:04:17,440
would get

01:04:17,760 --> 01:04:21,520
inconsistent results and basically what

01:04:20,319 --> 01:04:24,640
you're doing here is

01:04:21,520 --> 01:04:25,200
you would possibly trading the accuracy

01:04:24,640 --> 01:04:28,880
of the

01:04:25,200 --> 01:04:28,880
of the result for the size of this data

01:04:30,640 --> 01:04:36,960
um yeah so to summarize um

01:04:34,319 --> 01:04:38,319
streams are or the input is basically

01:04:36,960 --> 01:04:40,480
interpreted as some kind of changelog

01:04:38,319 --> 01:04:43,599
for table

01:04:40,480 --> 01:04:44,400
in the current version flink110 we only

01:04:43,599 --> 01:04:46,640
support

01:04:44,400 --> 01:04:48,640
insert into changelogs basically change

01:04:46,640 --> 01:04:51,839
logs that represent

01:04:48,640 --> 01:04:54,240
only insertions in the upcoming version

01:04:51,839 --> 01:04:56,079
where that is currently being finalized

01:04:54,240 --> 01:04:57,920
there will be support for full change

01:04:56,079 --> 01:04:59,760
locks

01:04:57,920 --> 01:05:00,960
secret curious that we run on a dynamic

01:04:59,760 --> 01:05:03,920
table

01:05:00,960 --> 01:05:04,799
produce a new dynamic table and the

01:05:03,920 --> 01:05:08,079
query

01:05:04,799 --> 01:05:10,480
kind of determines whether the result

01:05:08,079 --> 01:05:12,960
dynamic table is an append only table or

01:05:10,480 --> 01:05:14,799
a table that is that is being updated

01:05:12,960 --> 01:05:16,319
and when you convert it back

01:05:14,799 --> 01:05:18,960
there's also different modes how you can

01:05:16,319 --> 01:05:22,559
convert a dynamic table back

01:05:18,960 --> 01:05:25,039
if the dynamic table is also insert only

01:05:22,559 --> 01:05:26,799
it's a fairly straightforward con

01:05:25,039 --> 01:05:28,160
conversion because every

01:05:26,799 --> 01:05:30,000
record that is being inserted

01:05:28,160 --> 01:05:30,960
conceptually inserted into the dynamic

01:05:30,000 --> 01:05:34,319
table

01:05:30,960 --> 01:05:34,319
is being emitted as a neuro

01:05:34,559 --> 01:05:37,680
if there is uh update also update and

01:05:36,720 --> 01:05:40,079
delete changes

01:05:37,680 --> 01:05:41,760
then there is uh different update modes

01:05:40,079 --> 01:05:42,559
like insert and delete or upset and

01:05:41,760 --> 01:05:45,520
delete

01:05:42,559 --> 01:05:46,240
um that can be used to write the data

01:05:45,520 --> 01:05:49,440
out

01:05:46,240 --> 01:05:51,119
to uh to an external system usually you

01:05:49,440 --> 01:05:53,039
don't really

01:05:51,119 --> 01:05:54,880
um you don't have to do that manually

01:05:53,039 --> 01:05:56,400
when you can select a connector for a

01:05:54,880 --> 01:05:58,960
sync table

01:05:56,400 --> 01:06:01,119
then the connector will basically know

01:05:58,960 --> 01:06:03,200
which kind of uh conversions it

01:06:01,119 --> 01:06:05,200
it supports and then flink will

01:06:03,200 --> 01:06:08,799
automatically use the

01:06:05,200 --> 01:06:11,839
right conversion for this connector

01:06:08,799 --> 01:06:13,359
to write the data out and if you tried

01:06:11,839 --> 01:06:16,960
for instance to write

01:06:13,359 --> 01:06:18,000
some some updating data to a kafka topic

01:06:16,960 --> 01:06:21,760
then flink will simply say

01:06:18,000 --> 01:06:23,680
no sorry i can't do that there's

01:06:21,760 --> 01:06:26,400
no way the the connector does not

01:06:23,680 --> 01:06:32,799
support updating

01:06:26,400 --> 01:06:36,079
results or updating rows in in kafka

01:06:32,799 --> 01:06:38,000
um yeah you can run basically

01:06:36,079 --> 01:06:39,599
uh just regular secret furious on these

01:06:38,000 --> 01:06:42,240
dynamic tables

01:06:39,599 --> 01:06:42,720
um right actually writing and executing

01:06:42,240 --> 01:06:44,720
these

01:06:42,720 --> 01:06:47,200
curiouses is rather easy because it's

01:06:44,720 --> 01:06:48,960
just standard sql however

01:06:47,200 --> 01:06:51,200
you should pay some attention to the

01:06:48,960 --> 01:06:52,640
state requirements of your query and

01:06:51,200 --> 01:06:55,839
depending on the query and the input

01:06:52,640 --> 01:07:01,839
data the state might just grow

01:06:55,839 --> 01:07:01,839
very large

01:07:16,240 --> 01:07:20,000
okay then let's maybe have a look at the

01:07:20,319 --> 01:07:26,880
sorry the results so the

01:07:23,520 --> 01:07:28,240
first exercise was fairly simple query

01:07:26,880 --> 01:07:31,680
we just want to basically get a

01:07:28,240 --> 01:07:34,640
histogram of how many rights

01:07:31,680 --> 01:07:36,880
happened with how many passengers so if

01:07:34,640 --> 01:07:36,880
we

01:07:38,480 --> 01:07:42,079
remember how the table writes table

01:07:40,720 --> 01:07:43,599
looked like there's this passenger

01:07:42,079 --> 01:07:47,839
account field

01:07:43,599 --> 01:07:50,319
and so we can do a simple query

01:07:47,839 --> 01:07:50,319
select

01:07:51,200 --> 01:07:55,520
oops passenger account

01:07:55,680 --> 01:08:02,640
can't star from

01:08:00,160 --> 01:08:02,640
rights

01:08:03,440 --> 01:08:07,520
oh i think we only wanted to have rights

01:08:05,119 --> 01:08:11,839
that started right so

01:08:07,520 --> 01:08:11,839
yes start

01:08:15,039 --> 01:08:18,080
where is start

01:08:18,239 --> 01:08:27,120
group by passenger account and if you

01:08:24,319 --> 01:08:27,120
run this query

01:08:27,759 --> 01:08:31,040
you'll basically see oh there's even a

01:08:29,440 --> 01:08:33,359
right with no passenger

01:08:31,040 --> 01:08:34,080
or a couple of rides with no passengers

01:08:33,359 --> 01:08:37,600
um

01:08:34,080 --> 01:08:40,560
so you basically see how the result is

01:08:37,600 --> 01:08:42,719
continuously refined

01:08:40,560 --> 01:08:43,759
based on the data that is uh that is

01:08:42,719 --> 01:08:45,920
that is arriving

01:08:43,759 --> 01:08:47,920
you can actually if this is uh going too

01:08:45,920 --> 01:08:50,960
fast you can actually also

01:08:47,920 --> 01:08:54,799
slow down the update rate

01:08:50,960 --> 01:08:56,560
so a bit well no this way wrong way

01:08:54,799 --> 01:08:58,000
let's say now we have an update every

01:08:56,560 --> 01:09:02,239
one minute

01:08:58,000 --> 01:09:05,120
um so the uh see like line doesn't up

01:09:02,239 --> 01:09:06,640
out update at the at the rate at which

01:09:05,120 --> 01:09:10,319
the query updates the result but

01:09:06,640 --> 01:09:12,239
only update up refreshes the page every

01:09:10,319 --> 01:09:13,679
every one minute so here you can see

01:09:12,239 --> 01:09:17,440
that clearly

01:09:13,679 --> 01:09:18,000
uh rights with a single single passenger

01:09:17,440 --> 01:09:21,359
are

01:09:18,000 --> 01:09:23,120
clearly uh the most

01:09:21,359 --> 01:09:25,600
followed by rights with two passengers

01:09:23,120 --> 01:09:25,600
and so on

01:09:25,839 --> 01:09:28,960
all right so that's a fairly simple

01:09:27,440 --> 01:09:31,199
query and i think the only

01:09:28,960 --> 01:09:33,120
interesting part here is that it's like

01:09:31,199 --> 01:09:37,359
continuously being

01:09:33,120 --> 01:09:40,960
being updated and and refined

01:09:37,359 --> 01:09:40,960
the other query was a little bit more

01:09:41,120 --> 01:09:47,199
uh like the from the

01:09:44,239 --> 01:09:49,199
structure point of view exactly the same

01:09:47,199 --> 01:09:50,159
in this case we want to group the data

01:09:49,199 --> 01:09:53,279
based on the

01:09:50,159 --> 01:09:54,320
hour of time and on the area id so

01:09:53,279 --> 01:09:59,440
basically

01:09:54,320 --> 01:10:02,480
um it's kind of like the

01:09:59,440 --> 01:10:05,120
kind like very similar here

01:10:02,480 --> 01:10:07,120
so here we group on a couple of more

01:10:05,120 --> 01:10:10,239
fields we group on this to area id

01:10:07,120 --> 01:10:14,000
function start hour

01:10:10,239 --> 01:10:16,960
basically converts the time stem into

01:10:14,000 --> 01:10:19,520
only extracts the hour of day from the

01:10:16,960 --> 01:10:22,560
timestamp

01:10:19,520 --> 01:10:25,679
we again filter on is in new york city

01:10:22,560 --> 01:10:28,800
and so on and adding a having clause

01:10:25,679 --> 01:10:28,800
and then this query

01:10:29,520 --> 01:10:32,159
sorry

01:10:39,760 --> 01:10:46,400
if we run this

01:10:44,000 --> 01:10:46,400
very

01:10:47,199 --> 01:10:54,000
it does exactly what we're

01:10:50,560 --> 01:10:55,440
expected to do so you see here we have

01:10:54,000 --> 01:10:58,480
now we're now here at

01:10:55,440 --> 01:10:58,480
hour of day eight

01:11:00,719 --> 01:11:07,360
simply because due to this 10x speed

01:11:03,679 --> 01:11:07,360
we're already eight hours into the

01:11:08,640 --> 01:11:15,199
taxi ride events

01:11:12,080 --> 01:11:27,360
all right so much for for this um

01:11:15,199 --> 01:11:29,520
then let's

01:11:27,360 --> 01:11:31,040
yes so the crap yeah so so andre asked

01:11:29,520 --> 01:11:33,440
the question um

01:11:31,040 --> 01:11:34,880
uh except for table and field names uh

01:11:33,440 --> 01:11:36,719
fling sql is case

01:11:34,880 --> 01:11:40,560
insensitive that is uh that is true yeah

01:11:36,719 --> 01:11:43,920
so uh table field names need to be

01:11:40,560 --> 01:11:46,320
are case sensitive and for everything

01:11:43,920 --> 01:11:55,840
else for the keywords

01:11:46,320 --> 01:11:55,840
yeah you can do it however you want

01:11:56,320 --> 01:12:01,920
okay then let's maybe continue how much

01:11:58,960 --> 01:12:06,400
time is lefty i've got

01:12:01,920 --> 01:12:10,080
something like i think 25 minutes

01:12:06,400 --> 01:12:13,360
left if i'm right he started a bit

01:12:10,080 --> 01:12:15,920
late or 30 minutes let's maybe look at

01:12:13,360 --> 01:12:15,920
this one

01:12:16,800 --> 01:12:19,920
um

01:12:17,140 --> 01:12:22,719
[Music]

01:12:19,920 --> 01:12:22,719
this is about

01:12:24,640 --> 01:12:31,520
how you can create tables create tables

01:12:28,320 --> 01:12:33,040
in the sense of a ddl statement

01:12:31,520 --> 01:12:35,120
and connect the table to an external

01:12:33,040 --> 01:12:38,000
system and then

01:12:35,120 --> 01:12:38,800
use this table basically also to write

01:12:38,000 --> 01:12:43,840
out the

01:12:38,800 --> 01:12:43,840
query result to an external system

01:12:44,080 --> 01:12:48,840
so um i did not explicitly mention this

01:12:48,000 --> 01:12:52,480
before

01:12:48,840 --> 01:12:54,000
um but apache flink is not a data store

01:12:52,480 --> 01:12:57,600
so flink does not

01:12:54,000 --> 01:13:02,400
store any data except for

01:12:57,600 --> 01:13:04,880
in-flight data that is needed to for for

01:13:02,400 --> 01:13:06,480
query processing or processing of

01:13:04,880 --> 01:13:09,679
streaming applications

01:13:06,480 --> 01:13:12,800
so you cannot use flink as a as

01:13:09,679 --> 01:13:15,840
a database to just store

01:13:12,800 --> 01:13:19,040
all your data it's

01:13:15,840 --> 01:13:20,640
focusing on processing data

01:13:19,040 --> 01:13:22,400
it stores as i said intermediate data

01:13:20,640 --> 01:13:24,960
that is needed for for

01:13:22,400 --> 01:13:25,679
processing but it's not a database so

01:13:24,960 --> 01:13:27,920
whenever

01:13:25,679 --> 01:13:27,920
you're

01:13:28,960 --> 01:13:33,120
interacting with data you probably read

01:13:31,040 --> 01:13:35,280
it from some extent system and

01:13:33,120 --> 01:13:36,719
system and when you're done usually you

01:13:35,280 --> 01:13:38,000
also write it out to some external

01:13:36,719 --> 01:13:40,239
system

01:13:38,000 --> 01:13:41,920
and flink provides connectors for many

01:13:40,239 --> 01:13:46,400
different storage systems

01:13:41,920 --> 01:13:48,960
and formats for

01:13:46,400 --> 01:13:51,280
sql this is apache kafka because it's

01:13:48,960 --> 01:13:52,880
the most

01:13:51,280 --> 01:13:54,719
uh most widely used um

01:13:52,880 --> 01:13:56,560
[Music]

01:13:54,719 --> 01:13:58,480
stream store or system to uh to

01:13:56,560 --> 01:14:01,440
distribute data streams

01:13:58,480 --> 01:14:02,480
it has connectors for uh jdbc i

01:14:01,440 --> 01:14:05,679
mentioned that before

01:14:02,480 --> 01:14:07,360
elasticsearch apache hbase

01:14:05,679 --> 01:14:09,360
it also has a good integration with

01:14:07,360 --> 01:14:12,719
apache hive which is uh

01:14:09,360 --> 01:14:15,440
mostly valuable for uh for a

01:14:12,719 --> 01:14:16,480
batch running batch queries so you can

01:14:15,440 --> 01:14:19,679
read

01:14:16,480 --> 01:14:22,560
five tables with flink sql and then

01:14:19,679 --> 01:14:23,120
use flink's flink sequence batch engine

01:14:22,560 --> 01:14:27,440
to

01:14:23,120 --> 01:14:31,040
process these these hive tables

01:14:27,440 --> 01:14:31,040
you can write to different file systems

01:14:31,280 --> 01:14:34,880
and you can also write in different

01:14:32,880 --> 01:14:39,120
formats uh such as

01:14:34,880 --> 01:14:42,480
arbor json csv um parquet and or c

01:14:39,120 --> 01:14:45,520
there's it's a bit of

01:14:42,480 --> 01:14:47,120
not that easy to say which combination

01:14:45,520 --> 01:14:51,040
works how

01:14:47,120 --> 01:14:53,120
for instance packet or a ceo of

01:14:51,040 --> 01:14:55,280
obviously file systems

01:14:53,120 --> 01:14:56,719
formats that are only relevant in the

01:14:55,280 --> 01:14:59,520
context of file systems

01:14:56,719 --> 01:15:00,320
whereas avro for instance averages and

01:14:59,520 --> 01:15:03,600
csv

01:15:00,320 --> 01:15:06,719
are also relevant for

01:15:03,600 --> 01:15:10,239
instance for apache kafka which stores

01:15:06,719 --> 01:15:13,040
binary data that you can encode

01:15:10,239 --> 01:15:14,480
in whatever you where you want so for

01:15:13,040 --> 01:15:18,320
instance for kafka

01:15:14,480 --> 01:15:22,320
flink sql supports uh records that are

01:15:18,320 --> 01:15:24,480
serialized in in avro json or cc

01:15:22,320 --> 01:15:26,400
and then other systems like jdbc

01:15:24,480 --> 01:15:27,840
obviously have their own storage system

01:15:26,400 --> 01:15:31,040
so they don't really

01:15:27,840 --> 01:15:33,760
need to need a um

01:15:31,040 --> 01:15:33,760
apache ever

01:15:36,000 --> 01:15:40,880
there's also um yeah here there's this

01:15:39,440 --> 01:15:43,040
uh link in the documentation that

01:15:40,880 --> 01:15:46,800
basically tells you like what

01:15:43,040 --> 01:15:48,560
uh combination and which formats and um

01:15:46,800 --> 01:15:50,000
connectors are currently available for

01:15:48,560 --> 01:15:52,800
flink sql

01:15:50,000 --> 01:15:54,080
um in the next version in flink uh 111

01:15:52,800 --> 01:15:57,600
there'll be

01:15:54,080 --> 01:15:58,560
many more connectors and uh yeah better

01:15:57,600 --> 01:16:02,159
support for

01:15:58,560 --> 01:16:02,159
for for some of the external systems

01:16:02,320 --> 01:16:06,800
so if you want to create a table now you

01:16:05,120 --> 01:16:09,840
basically have to

01:16:06,800 --> 01:16:11,760
provide different types of information

01:16:09,840 --> 01:16:16,080
here

01:16:11,760 --> 01:16:18,800
first of all um the create

01:16:16,080 --> 01:16:19,600
table step and the dll syntax looks in

01:16:18,800 --> 01:16:22,400
the first part

01:16:19,600 --> 01:16:22,800
should look familiar to you so you say

01:16:22,400 --> 01:16:24,960
create

01:16:22,800 --> 01:16:26,480
table you give the table a name for

01:16:24,960 --> 01:16:29,679
instance order here

01:16:26,480 --> 01:16:31,520
and then you specify the schema of the

01:16:29,679 --> 01:16:33,199
of of the table and this is just a

01:16:31,520 --> 01:16:35,520
regular schema definition so

01:16:33,199 --> 01:16:36,560
you can have an order id and give it a

01:16:35,520 --> 01:16:38,400
type a

01:16:36,560 --> 01:16:39,920
big end you have an older amount you can

01:16:38,400 --> 01:16:41,520
give it a type decimal

01:16:39,920 --> 01:16:44,000
flink supports all of the common

01:16:41,520 --> 01:16:46,560
commonly used

01:16:44,000 --> 01:16:47,600
sql types and then there is something

01:16:46,560 --> 01:16:51,679
like order time

01:16:47,600 --> 01:16:51,679
that is of type timestamp

01:16:51,840 --> 01:16:55,840
and then comes something that you

01:16:54,000 --> 01:16:57,199
probably haven't seen before which is

01:16:55,840 --> 01:16:59,280
this watermark field

01:16:57,199 --> 01:17:01,360
watermark for order time you see that

01:16:59,280 --> 01:17:03,120
here again for the timestamp

01:17:01,360 --> 01:17:06,239
and then some kind of expression all the

01:17:03,120 --> 01:17:09,679
time minus interval of 10 seconds

01:17:06,239 --> 01:17:12,560
this is a watermark definition

01:17:09,679 --> 01:17:14,800
watermarks are flink's mechanism for

01:17:12,560 --> 01:17:17,920
tracking progress and

01:17:14,800 --> 01:17:20,480
time progress as i said we'll

01:17:17,920 --> 01:17:22,320
talk about queries and time tomorrow so

01:17:20,480 --> 01:17:24,960
i won't go into the details here

01:17:22,320 --> 01:17:26,560
but the important thing here is that you

01:17:24,960 --> 01:17:29,120
can

01:17:26,560 --> 01:17:30,000
specify the watermark expression

01:17:29,120 --> 01:17:33,440
directly

01:17:30,000 --> 01:17:33,440
within the ddl syntax

01:17:33,520 --> 01:17:38,080
it's optional so if you don't need

01:17:36,080 --> 01:17:40,800
watermarks

01:17:38,080 --> 01:17:41,440
or time handling then you don't don't

01:17:40,800 --> 01:17:45,840
have to

01:17:41,440 --> 01:17:45,840
provide this watermark

01:17:46,239 --> 01:17:50,800
this watermark clause and then the next

01:17:48,800 --> 01:17:54,480
part

01:17:50,800 --> 01:17:56,320
is also something that not sure

01:17:54,480 --> 01:17:58,719
you might have seen before in other

01:17:56,320 --> 01:18:00,320
systems that don't that are not data

01:17:58,719 --> 01:18:03,679
stores

01:18:00,320 --> 01:18:05,760
but this is basically a definition that

01:18:03,679 --> 01:18:07,199
gives all of the connected format

01:18:05,760 --> 01:18:09,840
properties that

01:18:07,199 --> 01:18:10,800
flink needs to create an appropriate

01:18:09,840 --> 01:18:13,760
connector

01:18:10,800 --> 01:18:15,520
to read and write the data so here for

01:18:13,760 --> 01:18:18,080
instance we're connecting to kafka

01:18:15,520 --> 01:18:19,920
so connected type is kafka it's a simple

01:18:18,080 --> 01:18:22,400
key value format

01:18:19,920 --> 01:18:23,040
the connected version is universal that

01:18:22,400 --> 01:18:26,960
is

01:18:23,040 --> 01:18:30,080
a flink internal thing that uses the

01:18:26,960 --> 01:18:35,040
yeah kafka's universal

01:18:30,080 --> 01:18:36,960
api for for accessing kafka

01:18:35,040 --> 01:18:38,960
we obviously also need to tell the

01:18:36,960 --> 01:18:41,440
connector from which kafka topic we want

01:18:38,960 --> 01:18:42,800
to read this is called auras here

01:18:41,440 --> 01:18:44,480
there's some other properties like

01:18:42,800 --> 01:18:46,960
bootstrap servers here

01:18:44,480 --> 01:18:49,360
this is like running on my local machine

01:18:46,960 --> 01:18:53,040
and we also want to specify format

01:18:49,360 --> 01:18:56,159
we want to read json data here and

01:18:53,040 --> 01:18:58,960
flink internally basically from the

01:18:56,159 --> 01:19:00,480
schema definition of the table i can

01:18:58,960 --> 01:19:03,440
infer that

01:19:00,480 --> 01:19:05,040
there's json schema that it needs to

01:19:03,440 --> 01:19:07,280
read

01:19:05,040 --> 01:19:08,640
so um this is now the basically the

01:19:07,280 --> 01:19:13,520
properties that you

01:19:08,640 --> 01:19:15,840
would need to provide to read from a

01:19:13,520 --> 01:19:16,800
json encoded kafka topic obviously if

01:19:15,840 --> 01:19:20,480
you're using

01:19:16,800 --> 01:19:23,520
avro or using some other

01:19:20,480 --> 01:19:26,239
storage system all these properties uh

01:19:23,520 --> 01:19:28,080
look uh look a bit different there's

01:19:26,239 --> 01:19:29,760
also more properties here for

01:19:28,080 --> 01:19:31,040
kafka you could also specify whether you

01:19:29,760 --> 01:19:33,120
want to read from the beginning of the

01:19:31,040 --> 01:19:35,360
topic or from the end of the topic

01:19:33,120 --> 01:19:36,800
uh you can provide a group id and so on

01:19:35,360 --> 01:19:39,520
so there's lots more lots of more

01:19:36,800 --> 01:19:39,520
configuration

01:19:39,760 --> 01:19:45,120
or configuration options that i didn't

01:19:42,400 --> 01:19:45,120
didn't show here

01:19:46,320 --> 01:19:51,600
great table is not the only syntax that

01:19:47,920 --> 01:19:54,159
flink supports it also supports

01:19:51,600 --> 01:19:55,120
altering dropping of tables or functions

01:19:54,159 --> 01:19:57,760
so you can

01:19:55,120 --> 01:20:00,159
you use that to register your own user

01:19:57,760 --> 01:20:04,239
defined functions

01:20:00,159 --> 01:20:06,639
you can create databases and so on and

01:20:04,239 --> 01:20:07,679
the ddl support is also extended in

01:20:06,639 --> 01:20:10,880
future versions

01:20:07,679 --> 01:20:12,719
again flink111 will make a

01:20:10,880 --> 01:20:15,040
we'll add a couple of nice features here

01:20:12,719 --> 01:20:15,040
as well

01:20:16,400 --> 01:20:19,440
internally flink's default catalog is

01:20:18,560 --> 01:20:23,520
not durable

01:20:19,440 --> 01:20:23,520
so for instance the catalog that is used

01:20:23,600 --> 01:20:28,560
in the demo environment is based on the

01:20:26,400 --> 01:20:30,239
is a simple in-memory catalog

01:20:28,560 --> 01:20:32,560
that stores all the catalog data in

01:20:30,239 --> 01:20:35,760
memory but that does not persist

01:20:32,560 --> 01:20:36,800
the the information uh anywhere on the

01:20:35,760 --> 01:20:40,080
secrecy like line

01:20:36,800 --> 01:20:43,360
has a has a yaml file

01:20:40,080 --> 01:20:44,480
where you can uh specify all the all the

01:20:43,360 --> 01:20:47,520
connectors

01:20:44,480 --> 01:20:49,440
properties and when the

01:20:47,520 --> 01:20:51,199
cli client is started this year profile

01:20:49,440 --> 01:20:53,679
is parsed and uh

01:20:51,199 --> 01:20:55,120
tables are created for all the entries

01:20:53,679 --> 01:20:56,480
in the yaml file and this is basically

01:20:55,120 --> 01:20:58,880
how

01:20:56,480 --> 01:21:01,600
these three tables the rights the fares

01:20:58,880 --> 01:21:04,080
and the driver changes

01:21:01,600 --> 01:21:05,199
tables were initialized in the cli

01:21:04,080 --> 01:21:06,560
client that we use for the demo

01:21:05,199 --> 01:21:10,080
environment

01:21:06,560 --> 01:21:12,560
but if you would now create a new table

01:21:10,080 --> 01:21:13,120
in the docker container then would

01:21:12,560 --> 01:21:15,120
simply

01:21:13,120 --> 01:21:18,239
stop the client and restart the client

01:21:15,120 --> 01:21:20,880
this table would be done so

01:21:18,239 --> 01:21:20,880
by default

01:21:21,840 --> 01:21:26,080
none of the catalog data is really

01:21:23,280 --> 01:21:28,480
persisted in in flink fling is not is

01:21:26,080 --> 01:21:29,440
again not a data store it doesn't store

01:21:28,480 --> 01:21:33,280
data like in

01:21:29,440 --> 01:21:36,400
at some external locations however

01:21:33,280 --> 01:21:39,440
flink supports hives metastore

01:21:36,400 --> 01:21:41,920
so you can simply connect

01:21:39,440 --> 01:21:43,280
flink with a hive metastore and then use

01:21:41,920 --> 01:21:46,000
that as a

01:21:43,280 --> 01:21:46,960
persistent external catalog service um

01:21:46,000 --> 01:21:48,719
when you then

01:21:46,960 --> 01:21:50,639
basically connect flink to uh to

01:21:48,719 --> 01:21:53,360
metastore you

01:21:50,639 --> 01:21:55,360
create a new catalog i could call it

01:21:53,360 --> 01:21:59,040
hcat or whatever

01:21:55,360 --> 01:22:01,679
and then in the flink ui in the flink

01:21:59,040 --> 01:22:03,280
cli you can say use catalog hcat and

01:22:01,679 --> 01:22:06,480
when you create a new table there

01:22:03,280 --> 01:22:08,320
then this table information will be

01:22:06,480 --> 01:22:11,120
persisted in hive better store

01:22:08,320 --> 01:22:12,159
and even when you then stop the client

01:22:11,120 --> 01:22:16,080
restart it again

01:22:12,159 --> 01:22:16,080
if you again connect to hcat

01:22:16,639 --> 01:22:20,800
sorry um then the table that you created

01:22:19,679 --> 01:22:25,840
in the previous session

01:22:20,800 --> 01:22:25,840
will still be there

01:22:32,800 --> 01:22:38,159
okay so um writing table to

01:22:36,080 --> 01:22:39,120
writing query results to external tables

01:22:38,159 --> 01:22:42,000
as a

01:22:39,120 --> 01:22:42,880
something that is very very easy um just

01:22:42,000 --> 01:22:45,760
regular

01:22:42,880 --> 01:22:47,120
sql syntax uh you just use the insert

01:22:45,760 --> 01:22:49,920
into statement for that

01:22:47,120 --> 01:22:50,719
so here's a very super simple example

01:22:49,920 --> 01:22:53,490
let's say we have

01:22:50,719 --> 01:22:55,440
some table people

01:22:53,490 --> 01:22:58,800
[Music]

01:22:55,440 --> 01:23:01,760
that has just information about persons

01:22:58,800 --> 01:23:02,960
and then we could just filter or filter

01:23:01,760 --> 01:23:06,320
on the x range between

01:23:02,960 --> 01:23:10,480
10 and 20 and then write all of the data

01:23:06,320 --> 01:23:10,480
into a table called teenagers

01:23:10,800 --> 01:23:17,280
if you do that then the schema of the

01:23:14,400 --> 01:23:18,320
the result schema of the query must

01:23:17,280 --> 01:23:20,719
match with the

01:23:18,320 --> 01:23:21,920
table schema so here for instance

01:23:20,719 --> 01:23:27,679
teenager

01:23:21,920 --> 01:23:27,679
must have those two fields name and age

01:23:28,239 --> 01:23:31,679
and if one of these fields is not in is

01:23:30,960 --> 01:23:33,360
not

01:23:31,679 --> 01:23:35,920
available in teenager the query will

01:23:33,360 --> 01:23:39,120
fail and flink will say i don't know how

01:23:35,920 --> 01:23:42,639
to write this data to teenager

01:23:39,120 --> 01:23:46,159
and also the tables

01:23:42,639 --> 01:23:46,960
the connector that was configured for

01:23:46,159 --> 01:23:49,520
teenage for the

01:23:46,960 --> 01:23:51,920
teenagers table needs to be able to

01:23:49,520 --> 01:23:51,920
update

01:23:52,320 --> 01:24:00,159
the table but

01:23:55,840 --> 01:24:03,199
basically needs to be able to write the

01:24:00,159 --> 01:24:05,199
result table of this query um to the

01:24:03,199 --> 01:24:07,840
external system so for instance if this

01:24:05,199 --> 01:24:09,920
is this is a simple filter query so

01:24:07,840 --> 01:24:13,679
there should not be any issues with it

01:24:09,920 --> 01:24:15,600
but if you if you would um

01:24:13,679 --> 01:24:17,920
have a query here that produces an

01:24:15,600 --> 01:24:19,920
updating result

01:24:17,920 --> 01:24:21,920
then we cannot simply write this

01:24:19,920 --> 01:24:23,040
updating result to a file system or to a

01:24:21,920 --> 01:24:25,360
kafka topic

01:24:23,040 --> 01:24:26,159
because the connector would not know how

01:24:25,360 --> 01:24:31,040
to

01:24:26,159 --> 01:24:34,080
forward a delete or abstract change

01:24:31,040 --> 01:24:34,960
into the into the sync system that holds

01:24:34,080 --> 01:24:37,760
the

01:24:34,960 --> 01:24:37,760
teenager data

01:24:39,920 --> 01:24:44,800
let's do one more hands-on exercise this

01:24:42,480 --> 01:24:49,199
is now

01:24:44,800 --> 01:24:49,199
uh not the next one it's actually the

01:24:51,520 --> 01:24:54,639
last one

01:24:56,480 --> 01:24:59,679
so if we go to the wiki page again it's

01:24:59,199 --> 01:25:02,159
this

01:24:59,679 --> 01:25:04,719
number six creating tables and writing

01:25:02,159 --> 01:25:13,040
query results to external tables

01:25:04,719 --> 01:25:15,760
let's only do the

01:25:13,040 --> 01:25:17,520
let's only do the this one here

01:25:15,760 --> 01:25:19,679
maintaining a continuous

01:25:17,520 --> 01:25:20,719
the updated materialist view in my sql

01:25:19,679 --> 01:25:24,560
the other one needs

01:25:20,719 --> 01:25:26,800
uh the other exercise above that needs

01:25:24,560 --> 01:25:27,679
um a temporal operator that we're just

01:25:26,800 --> 01:25:30,800
gonna discuss

01:25:27,679 --> 01:25:33,840
tomorrow um so

01:25:30,800 --> 01:25:36,960
i would um

01:25:33,840 --> 01:25:40,080
say let maybe um let's give it

01:25:36,960 --> 01:25:44,159
five minutes or so um and you

01:25:40,080 --> 01:25:45,040
try to run this query i prepared the ddl

01:25:44,159 --> 01:25:47,840
statement here

01:25:45,040 --> 01:25:50,080
for you already so you don't have to

01:25:47,840 --> 01:25:52,159
look up all of these

01:25:50,080 --> 01:25:54,400
connector properties you can simply copy

01:25:52,159 --> 01:25:56,800
paste this for

01:25:54,400 --> 01:25:59,120
for this exercise and then there is a

01:25:56,800 --> 01:25:59,120
small

01:25:59,840 --> 01:26:03,760
exercise here for for query that you

01:26:02,239 --> 01:26:06,560
could specify

01:26:03,760 --> 01:26:07,280
and while the query is running you can

01:26:06,560 --> 01:26:09,840
then

01:26:07,280 --> 01:26:13,280
also look into my sequel and see how the

01:26:09,840 --> 01:26:16,080
table in my sql is changing

01:26:13,280 --> 01:26:17,679
yeah i would say let's you can you can

01:26:16,080 --> 01:26:21,679
try that for

01:26:17,679 --> 01:26:24,880
a few minutes and then we can

01:26:21,679 --> 01:26:26,719
go through that together uh one more

01:26:24,880 --> 01:26:29,440
time and

01:26:26,719 --> 01:26:30,159
i yeah point out some hopefully

01:26:29,440 --> 01:26:34,840
interesting

01:26:30,159 --> 01:26:37,840
uh interesting things about this

01:26:34,840 --> 01:26:37,840
exercise

01:26:55,440 --> 01:27:01,120
yeah yeah so the the job keeps running

01:26:58,320 --> 01:27:05,120
when you exit the cli this is the

01:27:01,120 --> 01:27:07,920
idea of this detached mode exactly

01:27:05,120 --> 01:27:08,560
so if you look at the uh and in the

01:27:07,920 --> 01:27:11,920
flink

01:27:08,560 --> 01:27:13,360
ui and the flag flink

01:27:11,920 --> 01:27:16,560
web front end you see that the job is

01:27:13,360 --> 01:27:18,000
still running actually you cannot even

01:27:16,560 --> 01:27:19,280
maybe that's amazing i guess that's a

01:27:18,000 --> 01:27:20,800
missing feature but you cannot even

01:27:19,280 --> 01:27:24,000
cancel the job from the

01:27:20,800 --> 01:27:27,280
fleeing c like fling cli anymore you

01:27:24,000 --> 01:27:28,960
get the uh drop id back but um there's

01:27:27,280 --> 01:27:31,120
currently no feature to cancel the job

01:27:28,960 --> 01:27:36,000
you have to go into the

01:27:31,120 --> 01:27:38,800
flink ui and then stop it there

01:27:36,000 --> 01:27:38,800
all right i think we're

01:27:39,040 --> 01:27:43,520
somewhat close towards the end i would

01:27:44,639 --> 01:27:50,880
no go through this exercise um

01:27:48,400 --> 01:27:51,520
and um show a little bit how it works

01:27:50,880 --> 01:27:54,000
and then

01:27:51,520 --> 01:27:55,920
we can have maybe a few more questions

01:27:54,000 --> 01:27:58,239
towards the end if there's if there are

01:27:55,920 --> 01:28:02,800
any questions

01:27:58,239 --> 01:28:08,080
and um yeah all right so the um

01:28:02,800 --> 01:28:08,080
the exercise here is basically to um

01:28:09,360 --> 01:28:14,639
sorry to write the result of a of a

01:28:11,679 --> 01:28:19,760
query to my secret table right

01:28:14,639 --> 01:28:19,760
my secret table is defined by this

01:28:19,920 --> 01:28:25,280
with this ddl statements it's a simple

01:28:22,320 --> 01:28:28,880
table called area counts

01:28:25,280 --> 01:28:31,679
has two fields area id and account

01:28:28,880 --> 01:28:32,480
it's backed by the jdbc connector and

01:28:31,679 --> 01:28:34,400
this is the

01:28:32,480 --> 01:28:38,480
the database they connect to its mysql

01:28:34,400 --> 01:28:42,080
database table area accounts

01:28:38,480 --> 01:28:44,159
user and password and so on

01:28:42,080 --> 01:28:46,000
this is all runs in the dock container

01:28:44,159 --> 01:28:49,040
in my secret docker container so

01:28:46,000 --> 01:28:51,679
now if you say

01:28:49,040 --> 01:28:51,679
describe

01:28:52,159 --> 01:28:58,480
area counts

01:28:55,520 --> 01:29:01,920
we get just the schema information back

01:28:58,480 --> 01:29:05,760
if you now want to write into this table

01:29:01,920 --> 01:29:05,760
let me maybe create

01:29:07,760 --> 01:29:11,840
we can just go to the

01:29:13,840 --> 01:29:21,679
to another terminal and if we now

01:29:18,080 --> 01:29:21,679
connect to my sql

01:29:22,480 --> 01:29:26,800
the table has been or the the my secret

01:29:24,560 --> 01:29:29,679
table has been created before so

01:29:26,800 --> 01:29:31,360
uh usually you would have also go to my

01:29:29,679 --> 01:29:34,239
sql and then

01:29:31,360 --> 01:29:35,360
um create a tablet there but uh we

01:29:34,239 --> 01:29:38,000
already did that

01:29:35,360 --> 01:29:40,000
so there's no need to create an extra

01:29:38,000 --> 01:29:42,239
table in my sequence if you know

01:29:40,000 --> 01:29:42,239
say

01:29:43,120 --> 01:29:49,120
oops show tables

01:29:47,600 --> 01:29:50,639
and run this you see there's exactly

01:29:49,120 --> 01:29:54,480
this error accounts table

01:29:50,639 --> 01:29:54,480
and if we say describe

01:29:56,080 --> 01:30:03,360
area counts whoops

01:30:00,159 --> 01:30:06,480
uh it has exactly it's um first we just

01:30:03,360 --> 01:30:08,320
hit end and the other one is a big end

01:30:06,480 --> 01:30:10,000
and this is how the table looks in my

01:30:08,320 --> 01:30:13,360
sql and what we did

01:30:10,000 --> 01:30:15,120
with this connector here is we basically

01:30:13,360 --> 01:30:17,120
create a table in the flink catalog that

01:30:15,120 --> 01:30:20,880
is backed by the

01:30:17,120 --> 01:30:23,199
table in mysql if you know right to this

01:30:20,880 --> 01:30:25,679
let's maybe check what's in the table

01:30:23,199 --> 01:30:25,679
select

01:30:29,679 --> 01:30:38,560
from every accounts

01:30:35,280 --> 01:30:40,719
then this table is empty

01:30:38,560 --> 01:30:42,880
and if you now start a static variant

01:30:40,719 --> 01:30:46,880
flink that writes to this table

01:30:42,880 --> 01:30:46,880
insert into

01:30:47,040 --> 01:30:54,960
area counts

01:30:50,239 --> 01:30:58,239
um select

01:30:54,960 --> 01:30:58,239
what is it air

01:30:58,560 --> 01:31:07,280
to area id i hope

01:31:02,400 --> 01:31:07,280
long lut s

01:31:08,400 --> 01:31:13,540
area id and

01:31:11,520 --> 01:31:14,880
card star from

01:31:13,540 --> 01:31:19,040
[Music]

01:31:14,880 --> 01:31:23,040
rights group by

01:31:19,040 --> 01:31:26,560
to area id long

01:31:23,040 --> 01:31:26,560
blood oops

01:31:29,040 --> 01:31:33,360
right and if you know start this query i

01:31:32,000 --> 01:31:37,199
hope it's working oh yeah

01:31:33,360 --> 01:31:39,280
um so now we see job id this

01:31:37,199 --> 01:31:40,239
and um there seems to be something

01:31:39,280 --> 01:31:43,120
running

01:31:40,239 --> 01:31:45,679
if we select now here we see that there

01:31:43,120 --> 01:31:46,719
are a couple of rows inserted into this

01:31:45,679 --> 01:31:52,080
table

01:31:46,719 --> 01:31:54,880
and it also seems that this data is

01:31:52,080 --> 01:31:56,560
changing and it's changing because flink

01:31:54,880 --> 01:31:59,280
automatically

01:31:56,560 --> 01:32:00,000
writes the result of the of this group

01:31:59,280 --> 01:32:04,560
by query

01:32:00,000 --> 01:32:06,480
that we started into uh into my sql

01:32:04,560 --> 01:32:08,239
and we can now also look how the query

01:32:06,480 --> 01:32:13,679
looks like if we connect

01:32:08,239 --> 01:32:15,920
to um

01:32:13,679 --> 01:32:17,679
to the flink web ui and we see that this

01:32:15,920 --> 01:32:18,480
security is running and it's exactly the

01:32:17,679 --> 01:32:21,120
query that

01:32:18,480 --> 01:32:21,679
i just wrote insert into area accounts

01:32:21,120 --> 01:32:24,840
select

01:32:21,679 --> 01:32:28,080
to area and so on we can look at the

01:32:24,840 --> 01:32:31,280
query get the query plan here

01:32:28,080 --> 01:32:32,080
it's a simple query runs with a paradism

01:32:31,280 --> 01:32:35,520
of one

01:32:32,080 --> 01:32:36,960
we have here this is flink tries to get

01:32:35,520 --> 01:32:38,960
as much

01:32:36,960 --> 01:32:40,719
computation as possible into a into a

01:32:38,960 --> 01:32:41,760
processing node so this is actually the

01:32:40,719 --> 01:32:44,080
source node

01:32:41,760 --> 01:32:47,040
it's a projection down to only the

01:32:44,080 --> 01:32:47,040
fields that we read

01:32:47,120 --> 01:32:52,239
and if we would have added a filter also

01:32:50,000 --> 01:32:54,719
the filter would have been in here

01:32:52,239 --> 01:32:56,159
and then it does a hash partitioning

01:32:54,719 --> 01:32:56,800
here okay hash partitioning with the

01:32:56,159 --> 01:32:58,320
parallelism of

01:32:56,800 --> 01:33:00,560
one doesn't really make sense but

01:32:58,320 --> 01:33:02,480
imagine it would be like 10 or so

01:33:00,560 --> 01:33:03,600
um and then there's a group aggregate

01:33:02,480 --> 01:33:06,719
here that

01:33:03,600 --> 01:33:07,440
does the group by on the area id the

01:33:06,719 --> 01:33:12,080
count

01:33:07,440 --> 01:33:14,880
and uh the writing to the flink

01:33:12,080 --> 01:33:17,199
to writing the data to the to the to the

01:33:14,880 --> 01:33:17,199
sync

01:33:17,280 --> 01:33:22,800
you can see here how many records have

01:33:19,040 --> 01:33:26,480
been received and produced and

01:33:22,800 --> 01:33:29,280
let me see if the queue is running with

01:33:26,480 --> 01:33:30,159
checkpoints it's not running with the

01:33:29,280 --> 01:33:32,800
checkpoint so

01:33:30,159 --> 01:33:34,080
um if we would have configured the see

01:33:32,800 --> 01:33:35,040
like line maybe we should do that in the

01:33:34,080 --> 01:33:37,199
future

01:33:35,040 --> 01:33:39,040
to have check pointing enabled then the

01:33:37,199 --> 01:33:39,679
query would also be checkpointed as a

01:33:39,040 --> 01:33:42,480
reg

01:33:39,679 --> 01:33:43,600
just like a regular flink drop so um if

01:33:42,480 --> 01:33:45,440
you have some experience with flink

01:33:43,600 --> 01:33:46,239
before checkpoints of link's mechanism

01:33:45,440 --> 01:33:49,440
to

01:33:46,239 --> 01:33:50,000
um for our fault tolerance so you can

01:33:49,440 --> 01:33:52,800
think of it

01:33:50,000 --> 01:33:53,760
as basically that flink periodically

01:33:52,800 --> 01:33:56,800
takes a

01:33:53,760 --> 01:33:57,760
snapshot of all state in the in the of

01:33:56,800 --> 01:34:00,400
the job

01:33:57,760 --> 01:34:02,560
writes the state into a distributed

01:34:00,400 --> 01:34:05,679
persistent durable storage like

01:34:02,560 --> 01:34:08,719
s3 or hdfs and then

01:34:05,679 --> 01:34:11,600
if some failure happens the whole

01:34:08,719 --> 01:34:11,600
job gets

01:34:12,639 --> 01:34:19,520
the the the checkpoint is uh taken

01:34:15,760 --> 01:34:22,080
to re-initialize a new job and

01:34:19,520 --> 01:34:22,560
since we did a copy of everything the

01:34:22,080 --> 01:34:24,800
job

01:34:22,560 --> 01:34:27,360
then looks exactly the same when it

01:34:24,800 --> 01:34:29,760
continues it looks exactly the same

01:34:27,360 --> 01:34:31,280
as as at the point in time when the

01:34:29,760 --> 01:34:34,400
checkpoint was taken

01:34:31,280 --> 01:34:36,639
and if you use transactional syncs then

01:34:34,400 --> 01:34:39,600
actually

01:34:36,639 --> 01:34:40,239
the whole behavior is as if nothing ever

01:34:39,600 --> 01:34:44,480
happened

01:34:40,239 --> 01:34:46,000
so even even if you had a failure

01:34:44,480 --> 01:34:47,600
you will not see anything in your output

01:34:46,000 --> 01:34:50,719
or in your state everything will be

01:34:47,600 --> 01:34:52,320
um absolutely consistent all right so

01:34:50,719 --> 01:34:52,800
this is basically now the query that we

01:34:52,320 --> 01:34:55,040
are

01:34:52,800 --> 01:34:56,480
running um i said before we cannot even

01:34:55,040 --> 01:34:58,960
change it and

01:34:56,480 --> 01:35:01,760
cancel the query here from the flink cli

01:34:58,960 --> 01:35:06,239
so we have to do it from the dashboard

01:35:01,760 --> 01:35:09,440
we just go here can click on cancel

01:35:06,239 --> 01:35:09,440
cancel the job yes

01:35:10,560 --> 01:35:18,400
come on

01:35:15,280 --> 01:35:22,880
now the job is being cancelled

01:35:18,400 --> 01:35:25,760
and also

01:35:22,880 --> 01:35:27,520
yeah okay it's a bit hard now to see but

01:35:25,760 --> 01:35:30,560
there are no more updates here on the

01:35:27,520 --> 01:35:32,960
on the table in my sequel and obviously

01:35:30,560 --> 01:35:34,880
you could now also

01:35:32,960 --> 01:35:37,119
cure this my secret table from some some

01:35:34,880 --> 01:35:39,440
other um

01:35:37,119 --> 01:35:41,520
from from some other application or with

01:35:39,440 --> 01:35:43,600
the dashboarding

01:35:41,520 --> 01:35:44,560
application and since the table is

01:35:43,600 --> 01:35:47,520
updated

01:35:44,560 --> 01:35:50,639
pretty much in real well not exactly

01:35:47,520 --> 01:35:53,840
real-time but with very low latency

01:35:50,639 --> 01:35:55,280
you can you can get then

01:35:53,840 --> 01:35:57,679
the application that reads from the my

01:35:55,280 --> 01:36:11,840
secret table always gets the

01:35:57,679 --> 01:36:11,840
freshest results

01:36:20,960 --> 01:36:23,040

YouTube URL: https://www.youtube.com/watch?v=9QcuJ82m6Q4


