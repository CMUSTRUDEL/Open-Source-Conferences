Title: #bbuzz: Flavio Junqueira – Delivering stream data reliably with Pravega
Publication date: 2020-07-08
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/delivering-stream-data-reliably-pravega

Pravega is storage for streams. Pravega exposes stream as a core storage primitive, which enables applications continuously generating data (e.g., from sensors, end users, servers, or cameras) to ingest and store such data permanently. Applications that consume stream data from Pravega are able to access the data through the same API, independent of whether it is tailing the stream or reprocessing historical data. Pravega has some unique features such as the ability of storing an unbounded amount data per stream, while guaranteeing strong delivery semantics with transactions and scaling according to changes to the workload.

Pravega streams build on segments: append-only sequences of bytes that can be open or sealed. Segments are indivisible units that form streams and that enable important features. Segments enable Pravega to spread the load of streams across servers, to scale streams, to implement transactions efficiently, and to implement state replication.

In this presentation, we overview the architecture of Pravega and its core features. To enable reliable ingestion of events, we discuss how to use transactions along with state synchronization to guarantee that events from memoryful sources are ingested exactly once. Such a property is important to guarantee correctness when processing the data.

Pravega is an open-source project hosted on GitHub, and aims to be a community-driven project. It is under active development and encourages interested contributors to check it out and interact with the developers.
Captions: 
	00:00:08,000 --> 00:00:11,280
um hello everybody thanks for joining

00:00:09,679 --> 00:00:13,280
the station today i hope everyone is

00:00:11,280 --> 00:00:16,080
safe and sound

00:00:13,280 --> 00:00:17,279
um so my name is flavio i am a senior

00:00:16,080 --> 00:00:18,720
director and senior distinguished

00:00:17,279 --> 00:00:20,880
engineer now

00:00:18,720 --> 00:00:22,640
i have been working on this project

00:00:20,880 --> 00:00:24,320
called provega for the past few years

00:00:22,640 --> 00:00:27,279
with my colleagues that i'm

00:00:24,320 --> 00:00:28,560
in in the team and today i will be

00:00:27,279 --> 00:00:30,720
giving an introduction

00:00:28,560 --> 00:00:31,760
of provega i'll tell you a bit about

00:00:30,720 --> 00:00:34,079
what it is and

00:00:31,760 --> 00:00:35,120
and and its architecture its core

00:00:34,079 --> 00:00:36,960
features

00:00:35,120 --> 00:00:39,520
and then i'll focus on one specific

00:00:36,960 --> 00:00:40,399
problem which is the one of ingesting

00:00:39,520 --> 00:00:43,760
data

00:00:40,399 --> 00:00:45,680
in with exactly one semantics using

00:00:43,760 --> 00:00:47,120
the some of the features and mechanisms

00:00:45,680 --> 00:00:48,399
that we expose in provega

00:00:47,120 --> 00:00:51,360
so that's the the core of the

00:00:48,399 --> 00:00:51,360
presentation today

00:00:53,039 --> 00:00:57,680
first let me give you a few key points

00:00:55,840 --> 00:00:59,440
about provega

00:00:57,680 --> 00:01:00,800
i want i wanted to remember some of

00:00:59,440 --> 00:01:01,359
these terms because i'll be talking

00:01:00,800 --> 00:01:03,520
about them

00:01:01,359 --> 00:01:04,799
in more detail across the top and these

00:01:03,520 --> 00:01:05,760
are some of the things i'd like you to

00:01:04,799 --> 00:01:08,720
remember

00:01:05,760 --> 00:01:09,600
after this presentation so pervega is a

00:01:08,720 --> 00:01:13,280
state a stream

00:01:09,600 --> 00:01:17,360
store is a storage system that exposes

00:01:13,280 --> 00:01:19,520
a stream as as its main primitive

00:01:17,360 --> 00:01:21,360
the architectural project is based on

00:01:19,520 --> 00:01:22,560
the concept of or the abstraction of

00:01:21,360 --> 00:01:24,479
segments

00:01:22,560 --> 00:01:26,000
and the reason why segments are

00:01:24,479 --> 00:01:29,680
interesting is because they enable

00:01:26,000 --> 00:01:32,960
us to flexibly compose streams

00:01:29,680 --> 00:01:34,320
and to implement some very important

00:01:32,960 --> 00:01:37,439
features

00:01:34,320 --> 00:01:40,240
when it comes to uh storing streams

00:01:37,439 --> 00:01:41,280
and embedding such storage in in data

00:01:40,240 --> 00:01:44,560
pipelines

00:01:41,280 --> 00:01:47,600
like scaling streams transactions

00:01:44,560 --> 00:01:48,720
and even state replication proviga is an

00:01:47,600 --> 00:01:52,720
open source project

00:01:48,720 --> 00:01:54,960
um i haven't slide two

00:01:52,720 --> 00:01:56,079
important urls one is our website

00:01:54,960 --> 00:01:59,439
provega.io

00:01:56,079 --> 00:02:00,240
and the other one is our vega repository

00:01:59,439 --> 00:02:02,880
on github

00:02:00,240 --> 00:02:05,600
so if you're interested uh go visit and

00:02:02,880 --> 00:02:05,600
and check it out

00:02:06,159 --> 00:02:09,599
in this talk there will be um in broad

00:02:09,039 --> 00:02:12,720
terms four

00:02:09,599 --> 00:02:14,560
sections one the first one i will

00:02:12,720 --> 00:02:16,879
cover data streams i will you know i

00:02:14,560 --> 00:02:18,000
know perhaps a good chunk of the people

00:02:16,879 --> 00:02:20,560
if not everyone

00:02:18,000 --> 00:02:22,000
in the audience knows what or has some

00:02:20,560 --> 00:02:23,520
notion of what a data stream is

00:02:22,000 --> 00:02:25,680
but i want us to be on the same page so

00:02:23,520 --> 00:02:28,160
i'll talk a bit about data streams from

00:02:25,680 --> 00:02:29,200
from my perspective then i'll do an

00:02:28,160 --> 00:02:31,280
introduction of

00:02:29,200 --> 00:02:33,040
chopra vega including talking a bit

00:02:31,280 --> 00:02:35,519
about its architecture

00:02:33,040 --> 00:02:37,200
then i'll focus on the specific problem

00:02:35,519 --> 00:02:39,519
that i mentioned which is the one

00:02:37,200 --> 00:02:41,680
of ingesting data in an exactly once

00:02:39,519 --> 00:02:45,040
managed to upper vega stream

00:02:41,680 --> 00:02:46,800
and then uh hopefully if time allows

00:02:45,040 --> 00:02:48,080
i'll give you some i give you a quick

00:02:46,800 --> 00:02:51,680
demo and show

00:02:48,080 --> 00:02:54,080
you some some code all right so let's

00:02:51,680 --> 00:02:56,160
talk a bit about data streams

00:02:54,080 --> 00:02:57,599
so when i think about data streams what

00:02:56,160 --> 00:03:00,879
i'm referring to

00:02:57,599 --> 00:03:02,959
is um data that is coming from sources

00:03:00,879 --> 00:03:06,239
that are continuously generating it

00:03:02,959 --> 00:03:07,280
and it could be end users in traditional

00:03:06,239 --> 00:03:09,040
applications

00:03:07,280 --> 00:03:10,879
uh that you can think of like social

00:03:09,040 --> 00:03:12,879
networks or users that

00:03:10,879 --> 00:03:15,040
are doing you know shopping online and

00:03:12,879 --> 00:03:16,480
they are uh purchasing and their online

00:03:15,040 --> 00:03:20,000
transactions all those

00:03:16,480 --> 00:03:23,519
can generate a continuous flow of

00:03:20,000 --> 00:03:26,480
of events which constitutes a

00:03:23,519 --> 00:03:27,440
an event stream um or one or more

00:03:26,480 --> 00:03:29,200
advanced streams

00:03:27,440 --> 00:03:30,720
but it's not only about end users it can

00:03:29,200 --> 00:03:32,799
have machine generated data

00:03:30,720 --> 00:03:34,799
i can have servers that are continuously

00:03:32,799 --> 00:03:37,200
emitting telemetry

00:03:34,799 --> 00:03:38,959
i can have tools and services that we

00:03:37,200 --> 00:03:42,640
use on a daily basis

00:03:38,959 --> 00:03:45,760
jira gets jenkins all those

00:03:42,640 --> 00:03:47,599
all those outputting messages

00:03:45,760 --> 00:03:50,000
continuously that we want to ingest and

00:03:47,599 --> 00:03:53,200
process and perhaps visualize

00:03:50,000 --> 00:03:55,040
and then we have another set which is uh

00:03:53,200 --> 00:03:56,959
a hot topic it has been a hot topic for

00:03:55,040 --> 00:03:59,040
a few years it remains a hot topic and

00:03:56,959 --> 00:04:02,000
will continue to be given

00:03:59,040 --> 00:04:03,680
the the expansion of the of the the

00:04:02,000 --> 00:04:07,360
space of applications

00:04:03,680 --> 00:04:10,720
that care about it which is iot uh and

00:04:07,360 --> 00:04:11,599
which is related to edge and in those

00:04:10,720 --> 00:04:13,120
you have

00:04:11,599 --> 00:04:14,720
maybe sensors devices that are

00:04:13,120 --> 00:04:18,079
continuously emitting

00:04:14,720 --> 00:04:21,359
um uh samples uh

00:04:18,079 --> 00:04:24,000
events it could be yeah so it sounds

00:04:21,359 --> 00:04:24,400
simple events continuously meeting data

00:04:24,000 --> 00:04:25,680
that

00:04:24,400 --> 00:04:27,919
you might want to broad you might want

00:04:25,680 --> 00:04:28,400
to ingest and not in processing various

00:04:27,919 --> 00:04:30,720
ways

00:04:28,400 --> 00:04:32,400
you might wanna you might wanna have a

00:04:30,720 --> 00:04:33,040
digital twins application in which you

00:04:32,400 --> 00:04:36,720
have

00:04:33,040 --> 00:04:37,520
a um a digital mirror of devices and

00:04:36,720 --> 00:04:40,160
group them

00:04:37,520 --> 00:04:41,120
and visualizing uh in different ways so

00:04:40,160 --> 00:04:44,000
all those are

00:04:41,120 --> 00:04:45,040
examples of sources of continuously

00:04:44,000 --> 00:04:47,199
generated data

00:04:45,040 --> 00:04:50,880
and how those mapped to uh to the notion

00:04:47,199 --> 00:04:50,880
of streams that i've been talking about

00:04:51,040 --> 00:04:57,280
so the landscape that uh that we put

00:04:54,080 --> 00:05:01,039
prevegan to is this one in which we have

00:04:57,280 --> 00:05:03,759
uh on the left hand side a

00:05:01,039 --> 00:05:05,280
distinct sources of of continuously

00:05:03,759 --> 00:05:07,120
generated data

00:05:05,280 --> 00:05:08,479
as i mentioned could be end users but be

00:05:07,120 --> 00:05:11,759
machine generated

00:05:08,479 --> 00:05:15,680
uh like sensors

00:05:11,759 --> 00:05:19,280
connected cars servers

00:05:15,680 --> 00:05:23,039
any of these things we ingest that data

00:05:19,280 --> 00:05:26,400
and we process it now processing it

00:05:23,039 --> 00:05:28,320
might require my requirements for steps

00:05:26,400 --> 00:05:29,759
it's not necessarily ingesting the data

00:05:28,320 --> 00:05:31,759
processing can be done

00:05:29,759 --> 00:05:33,759
you can have multiple stages of of

00:05:31,759 --> 00:05:35,680
storing the data and processing the data

00:05:33,759 --> 00:05:36,880
and outputting and storing some more and

00:05:35,680 --> 00:05:39,039
processing it again

00:05:36,880 --> 00:05:40,320
so you could have multiple stages of

00:05:39,039 --> 00:05:42,639
that

00:05:40,320 --> 00:05:44,000
but the ultimate goal could be a number

00:05:42,639 --> 00:05:46,960
of things again it could be

00:05:44,000 --> 00:05:48,720
visualizing the the data that is being

00:05:46,960 --> 00:05:52,000
that has been generated uh it could be

00:05:48,720 --> 00:05:54,400
its raw form it could be um different

00:05:52,000 --> 00:05:55,280
ways in which makes it more intuitive to

00:05:54,400 --> 00:05:58,639
understand

00:05:55,280 --> 00:06:00,800
what um what what's happening in

00:05:58,639 --> 00:06:01,680
in in a fleet of servers and set of

00:06:00,800 --> 00:06:04,080
devices

00:06:01,680 --> 00:06:05,360
um it could be alerts about bad things

00:06:04,080 --> 00:06:06,479
that are happening in the system it

00:06:05,360 --> 00:06:09,919
could be

00:06:06,479 --> 00:06:12,240
um it could be insights

00:06:09,919 --> 00:06:13,360
about about users again about devices

00:06:12,240 --> 00:06:15,440
about the environment

00:06:13,360 --> 00:06:17,600
uh recommendations uh actionable

00:06:15,440 --> 00:06:18,000
insights all those are valid outputs for

00:06:17,600 --> 00:06:21,840
uh

00:06:18,000 --> 00:06:21,840
for such data pipelines

00:06:22,160 --> 00:06:25,840
now when it comes to use cases that we

00:06:25,360 --> 00:06:29,280
have

00:06:25,840 --> 00:06:31,360
uh that we have for prevega one

00:06:29,280 --> 00:06:33,680
one type of use case that we have been

00:06:31,360 --> 00:06:34,479
seeing quite frequently is the one of

00:06:33,680 --> 00:06:37,039
drones

00:06:34,479 --> 00:06:38,160
which are emitting telemetry and uh and

00:06:37,039 --> 00:06:40,639
video streams

00:06:38,160 --> 00:06:41,600
so there are from drones you're seeing

00:06:40,639 --> 00:06:43,440
at

00:06:41,600 --> 00:06:45,520
at least two streams again one of which

00:06:43,440 --> 00:06:48,240
is video the other day is telemetry

00:06:45,520 --> 00:06:49,440
you want to ingest that data you want to

00:06:48,240 --> 00:06:52,400
process it

00:06:49,440 --> 00:06:52,960
and and often you want to process the

00:06:52,400 --> 00:06:54,720
data

00:06:52,960 --> 00:06:56,479
as it's been generated so you're

00:06:54,720 --> 00:06:59,759
essentially taking the stream

00:06:56,479 --> 00:07:02,080
but also you want to process

00:06:59,759 --> 00:07:02,880
in retrospect so perform historical

00:07:02,080 --> 00:07:05,199
processing

00:07:02,880 --> 00:07:06,720
over the data data that has been gested

00:07:05,199 --> 00:07:08,000
uh some time back

00:07:06,720 --> 00:07:09,919
and you want to go back and either

00:07:08,000 --> 00:07:11,840
reprocess it or perform some kind of

00:07:09,919 --> 00:07:14,880
historical processing from a

00:07:11,840 --> 00:07:15,520
from scratch and the reason why various

00:07:14,880 --> 00:07:18,800
companies

00:07:15,520 --> 00:07:21,919
want to do this are they go from up

00:07:18,800 --> 00:07:22,560
inspecting the health of of your cattle

00:07:21,919 --> 00:07:25,919
to

00:07:22,560 --> 00:07:27,759
uh airplanes between in between flights

00:07:25,919 --> 00:07:30,160
looking for structural damage on a

00:07:27,759 --> 00:07:32,960
on such airplanes so if there is

00:07:30,160 --> 00:07:32,960
substantially

00:07:33,440 --> 00:07:36,560
another type of use case is the one of

00:07:35,440 --> 00:07:39,280
industrial iot

00:07:36,560 --> 00:07:40,880
where you have um where you have cameras

00:07:39,280 --> 00:07:42,880
you have sensors

00:07:40,880 --> 00:07:44,400
they owe emitting data continuously and

00:07:42,880 --> 00:07:46,800
again the the principle

00:07:44,400 --> 00:07:47,759
of ingesting the data processing the the

00:07:46,800 --> 00:07:50,000
fresh data

00:07:47,759 --> 00:07:51,759
and perhaps going back to the to the

00:07:50,000 --> 00:07:52,639
other data and performing historical

00:07:51,759 --> 00:07:54,720
processing

00:07:52,639 --> 00:07:57,039
is very appealing to a lot of these use

00:07:54,720 --> 00:08:00,800
cases so inducer iot we have seen

00:07:57,039 --> 00:08:01,840
a number of um a number of use cases

00:08:00,800 --> 00:08:04,800
that also have

00:08:01,840 --> 00:08:04,800
such requirements

00:08:07,039 --> 00:08:10,160
now stepping back and thinking a bit

00:08:08,879 --> 00:08:13,120
more abstractly about

00:08:10,160 --> 00:08:14,720
about data streams let's um let's talk

00:08:13,120 --> 00:08:16,639
about what are the what are some

00:08:14,720 --> 00:08:19,199
of the core properties that we would

00:08:16,639 --> 00:08:20,400
like or we observe when talking about

00:08:19,199 --> 00:08:22,080
streams

00:08:20,400 --> 00:08:23,520
so the most intuitive way of thinking

00:08:22,080 --> 00:08:26,560
about a stream is uh

00:08:23,520 --> 00:08:28,560
one sequence of uh of events as

00:08:26,560 --> 00:08:29,599
new events come in you append those

00:08:28,560 --> 00:08:32,399
events to the

00:08:29,599 --> 00:08:33,680
to to that stream so it has it has a

00:08:32,399 --> 00:08:34,640
head at the beginning of the stream it

00:08:33,680 --> 00:08:36,320
has a tail

00:08:34,640 --> 00:08:38,800
which is where you are appending the the

00:08:36,320 --> 00:08:41,120
new events

00:08:38,800 --> 00:08:42,320
but it's not as simple as that because

00:08:41,120 --> 00:08:44,720
we can have

00:08:42,320 --> 00:08:45,360
a fleet of servers a fleet of a a group

00:08:44,720 --> 00:08:47,920
of

00:08:45,360 --> 00:08:49,600
of of sensors and all emitting samples

00:08:47,920 --> 00:08:51,440
at the same time so a stream looks

00:08:49,600 --> 00:08:55,120
more like this where we have a certain

00:08:51,440 --> 00:08:57,600
degree of of parallelism

00:08:55,120 --> 00:08:58,160
but it doesn't stop there either because

00:08:57,600 --> 00:09:00,880
the

00:08:58,160 --> 00:09:02,480
traffic can fluctuate traffic can

00:09:00,880 --> 00:09:05,360
fluctuate because of uh

00:09:02,480 --> 00:09:07,040
of uh daily cycles weekly cycles of

00:09:05,360 --> 00:09:10,399
regular cycles

00:09:07,040 --> 00:09:13,600
the in the in the generation of data

00:09:10,399 --> 00:09:16,560
uh it could be also uh

00:09:13,600 --> 00:09:18,880
occasional spikes because introduced

00:09:16,560 --> 00:09:20,959
more devices introduced more servers

00:09:18,880 --> 00:09:22,480
uh or maybe a chunk of them have been

00:09:20,959 --> 00:09:24,320
decommissioned or crashed

00:09:22,480 --> 00:09:26,320
so all those could induce fluctuations

00:09:24,320 --> 00:09:29,519
in that uh in that

00:09:26,320 --> 00:09:33,279
in that traffic and so it's not

00:09:29,519 --> 00:09:34,800
just a constant uh a constant rate

00:09:33,279 --> 00:09:36,720
for the stream all the time it could

00:09:34,800 --> 00:09:39,440
observe the fluctuation of dropping

00:09:36,720 --> 00:09:39,440
or increasing

00:09:40,240 --> 00:09:44,000
these streams are also unbounded right

00:09:42,399 --> 00:09:45,040
so you can you can have stream running

00:09:44,000 --> 00:09:47,040
for

00:09:45,040 --> 00:09:48,160
for as long as you like it's uh we were

00:09:47,040 --> 00:09:50,320
talking about data that is being

00:09:48,160 --> 00:09:52,560
generated continuously so

00:09:50,320 --> 00:09:54,560
it's going to be generated for as long

00:09:52,560 --> 00:09:58,080
as the sources are there

00:09:54,560 --> 00:10:00,320
and ideally we do not make an

00:09:58,080 --> 00:10:01,760
artificial split between the old data

00:10:00,320 --> 00:10:02,399
and and the fresh data which i'm going

00:10:01,760 --> 00:10:04,800
to call

00:10:02,399 --> 00:10:06,399
the lambda way in in reference to the to

00:10:04,800 --> 00:10:09,440
the architecture

00:10:06,399 --> 00:10:11,920
so ideally we make such a distinguished

00:10:09,440 --> 00:10:12,800
distinction between fresh and historical

00:10:11,920 --> 00:10:15,360
data

00:10:12,800 --> 00:10:16,399
and when we expose the abstraction of

00:10:15,360 --> 00:10:19,279
the stream

00:10:16,399 --> 00:10:20,720
uh the stream encompasses all the data

00:10:19,279 --> 00:10:23,120
you need from it

00:10:20,720 --> 00:10:25,760
the historical part of it and uh and the

00:10:23,120 --> 00:10:29,279
tail part of the

00:10:25,760 --> 00:10:29,279
of the of the stream

00:10:29,839 --> 00:10:34,480
but all of that is about the right side

00:10:32,640 --> 00:10:37,120
of the stream so appending to the stream

00:10:34,480 --> 00:10:38,640
creating the stream but also we need

00:10:37,120 --> 00:10:39,279
when we think about a system that

00:10:38,640 --> 00:10:42,320
provides

00:10:39,279 --> 00:10:44,560
the ability to users to

00:10:42,320 --> 00:10:46,160
um to store such a stream we also need

00:10:44,560 --> 00:10:48,560
to give the degree

00:10:46,160 --> 00:10:50,000
of reading such a stream in a way that

00:10:48,560 --> 00:10:52,480
is uh

00:10:50,000 --> 00:10:54,399
that uh that is uh not only convenient

00:10:52,480 --> 00:10:57,440
but it meets the requirements of uh

00:10:54,399 --> 00:11:00,720
of applications and so if

00:10:57,440 --> 00:11:03,839
if we are to fluctuate the traffic and

00:11:00,720 --> 00:11:05,519
scale a stream then we need on the right

00:11:03,839 --> 00:11:07,440
side we also need to provide

00:11:05,519 --> 00:11:09,680
that same capability on the on the read

00:11:07,440 --> 00:11:09,680
side

00:11:11,040 --> 00:11:15,200
so in the end if if i group all those

00:11:13,519 --> 00:11:17,600
characteristics that i just

00:11:15,200 --> 00:11:18,640
mentioned uh go back to the notion of a

00:11:17,600 --> 00:11:21,040
stream and and

00:11:18,640 --> 00:11:23,600
and prevega the key thing that we're

00:11:21,040 --> 00:11:26,800
trying to do with provega is to provide

00:11:23,600 --> 00:11:29,279
a storage system that gives up

00:11:26,800 --> 00:11:31,680
properties that are not only are

00:11:29,279 --> 00:11:32,720
desirable from the perspective of an

00:11:31,680 --> 00:11:34,560
application

00:11:32,720 --> 00:11:36,160
performing stream analytics or

00:11:34,560 --> 00:11:38,880
processing streams in a

00:11:36,160 --> 00:11:40,480
in general but also that uh that those

00:11:38,880 --> 00:11:42,959
properties they meet

00:11:40,480 --> 00:11:44,880
um the characteristics of a cloud native

00:11:42,959 --> 00:11:47,279
application so they provide

00:11:44,880 --> 00:11:49,040
the they provide the the notion of

00:11:47,279 --> 00:11:49,360
unbounded data for a stream so stream

00:11:49,040 --> 00:11:52,800
can

00:11:49,360 --> 00:11:56,240
accommodate uh as much data as its

00:11:52,800 --> 00:11:59,040
storage systems allow it uh

00:11:56,240 --> 00:12:01,440
it's elastic so it can fluctuate it's

00:11:59,040 --> 00:12:04,079
its capacity according to them

00:12:01,440 --> 00:12:04,880
according to the incoming traffic it's

00:12:04,079 --> 00:12:07,279
consistent

00:12:04,880 --> 00:12:09,120
it provides semantics that allow allow

00:12:07,279 --> 00:12:10,160
it to be correct when you process the

00:12:09,120 --> 00:12:13,839
data and

00:12:10,160 --> 00:12:16,720
it enables the the applications

00:12:13,839 --> 00:12:17,519
both to tail the stream or process the

00:12:16,720 --> 00:12:20,160
fresh data

00:12:17,519 --> 00:12:21,360
and perform historical processing the

00:12:20,160 --> 00:12:25,360
all the data

00:12:21,360 --> 00:12:27,040
of of data analytics of um of the stream

00:12:25,360 --> 00:12:29,120
that's what i'm referring to when i say

00:12:27,040 --> 00:12:32,480
cloud native in the context of

00:12:29,120 --> 00:12:32,480
of storage for data streams

00:12:34,720 --> 00:12:40,079
let's now talk a bit about about vega

00:12:38,720 --> 00:12:42,160
remember i mentioned right in the

00:12:40,079 --> 00:12:44,160
beginning that one key concept in

00:12:42,160 --> 00:12:47,200
provega is the one of a stream segment

00:12:44,160 --> 00:12:49,360
a stream segment is a stream of bytes

00:12:47,200 --> 00:12:51,200
it's an appendage data structure weapon

00:12:49,360 --> 00:12:53,760
bites to it

00:12:51,200 --> 00:12:55,040
notes that i mentioned events many times

00:12:53,760 --> 00:12:58,399
and now i'm talking about bytes

00:12:55,040 --> 00:12:58,880
so in vega the segments they do not

00:12:58,399 --> 00:13:01,680
store

00:12:58,880 --> 00:13:03,040
segment events directly they store uh

00:13:01,680 --> 00:13:04,959
the bytes of the events

00:13:03,040 --> 00:13:07,040
and to get to the bytes we expect the

00:13:04,959 --> 00:13:08,320
serializer to be given so that

00:13:07,040 --> 00:13:10,320
we can we can perform that

00:13:08,320 --> 00:13:14,000
transformation

00:13:10,320 --> 00:13:16,240
now we with segments we

00:13:14,000 --> 00:13:17,200
we can provide parallelism so we can

00:13:16,240 --> 00:13:21,200
have a number

00:13:17,200 --> 00:13:23,839
of segments in parallel and

00:13:21,200 --> 00:13:25,360
from a writer perspective we the writer

00:13:23,839 --> 00:13:29,680
uses routing keys

00:13:25,360 --> 00:13:31,440
to map an append to a given segment

00:13:29,680 --> 00:13:33,519
and that's important because we provide

00:13:31,440 --> 00:13:36,560
per key order

00:13:33,519 --> 00:13:37,600
now one thing that is interesting in

00:13:36,560 --> 00:13:39,760
provega

00:13:37,600 --> 00:13:41,920
is that that parallelism that degree of

00:13:39,760 --> 00:13:44,079
parallelism is not static

00:13:41,920 --> 00:13:46,000
a streaming pro vega can scale up and

00:13:44,079 --> 00:13:48,079
down and so this

00:13:46,000 --> 00:13:49,040
i'm showing a representation of a stream

00:13:48,079 --> 00:13:51,279
that starts

00:13:49,040 --> 00:13:53,040
with uh with two segments remember that

00:13:51,279 --> 00:13:55,440
i'm starting my stream from uh

00:13:53,040 --> 00:13:57,040
from from the right side from the head

00:13:55,440 --> 00:14:00,079
it starts with two segments

00:13:57,040 --> 00:14:00,959
then scales up and and goes it becomes

00:14:00,079 --> 00:14:02,639
five

00:14:00,959 --> 00:14:05,600
five segments and then it scales down it

00:14:02,639 --> 00:14:08,639
becomes three so this kind of dynamics

00:14:05,600 --> 00:14:10,079
um is a feature that uh that we provide

00:14:08,639 --> 00:14:12,639
in provega and

00:14:10,079 --> 00:14:15,199
uh streams can scale that way according

00:14:12,639 --> 00:14:16,800
to a policy that um

00:14:15,199 --> 00:14:19,120
that is described when you create a

00:14:16,800 --> 00:14:19,120
stream

00:14:22,079 --> 00:14:25,839
another feature that we can provide with

00:14:23,600 --> 00:14:28,000
segments is transactions

00:14:25,839 --> 00:14:29,279
so when an application comes and creates

00:14:28,000 --> 00:14:30,959
a transaction

00:14:29,279 --> 00:14:33,600
what provega does internally is to

00:14:30,959 --> 00:14:34,560
create temporary transaction segments so

00:14:33,600 --> 00:14:36,800
the green

00:14:34,560 --> 00:14:39,040
segments on a on the slide correspond to

00:14:36,800 --> 00:14:40,959
the temporary transaction segments

00:14:39,040 --> 00:14:42,320
it's temporary in the sense that uh it

00:14:40,959 --> 00:14:45,040
don't exist until the

00:14:42,320 --> 00:14:46,000
the transaction commits or aborts so all

00:14:45,040 --> 00:14:48,880
appends

00:14:46,000 --> 00:14:50,240
done to the transaction uh while the

00:14:48,880 --> 00:14:52,079
transaction is open

00:14:50,240 --> 00:14:53,600
are going to go to those temporary

00:14:52,079 --> 00:14:55,040
transaction segments

00:14:53,600 --> 00:14:56,959
and when the application decides to

00:14:55,040 --> 00:14:58,639
commit then those transactions are

00:14:56,959 --> 00:15:00,480
merged to the main

00:14:58,639 --> 00:15:02,240
segments of the stream in which case the

00:15:00,480 --> 00:15:03,680
data becomes

00:15:02,240 --> 00:15:06,399
becomes available from that point

00:15:03,680 --> 00:15:08,880
onwards or the application can

00:15:06,399 --> 00:15:09,920
decide to abort and the transaction

00:15:08,880 --> 00:15:13,600
segments are simply

00:15:09,920 --> 00:15:15,120
discarded and in that case the data does

00:15:13,600 --> 00:15:17,279
not become visible at all

00:15:15,120 --> 00:15:18,240
and while this transaction is ongoing it

00:15:17,279 --> 00:15:23,040
does not block

00:15:18,240 --> 00:15:23,040
the data in the in the the mainstream

00:15:24,839 --> 00:15:29,120
segments

00:15:26,079 --> 00:15:30,320
finally another interesting um

00:15:29,120 --> 00:15:32,240
another interesting feature that i'm

00:15:30,320 --> 00:15:33,839
going to use for the exactly one

00:15:32,240 --> 00:15:37,120
suggestion case

00:15:33,839 --> 00:15:40,639
is the one of uh of state synchronizers

00:15:37,120 --> 00:15:42,639
persisting estate in in prevega and in a

00:15:40,639 --> 00:15:43,360
way that i can be shared and synchronize

00:15:42,639 --> 00:15:46,800
across

00:15:43,360 --> 00:15:50,079
a set of processes uh we do that by

00:15:46,800 --> 00:15:51,759
having a special feature of um

00:15:50,079 --> 00:15:53,920
of streams which is performing

00:15:51,759 --> 00:15:56,959
conditional pens so we have this

00:15:53,920 --> 00:15:59,120
abstraction of a revision stream

00:15:56,959 --> 00:16:00,399
and the revision stream is what the the

00:15:59,120 --> 00:16:02,399
state synchronizer

00:16:00,399 --> 00:16:03,839
which is again this abstraction that

00:16:02,399 --> 00:16:04,480
gives you the ability of processing

00:16:03,839 --> 00:16:07,279
states

00:16:04,480 --> 00:16:08,480
across prostities uses so state

00:16:07,279 --> 00:16:11,199
synchronizer builds on

00:16:08,480 --> 00:16:12,560
on these revision streams and enables us

00:16:11,199 --> 00:16:15,839
to um

00:16:12,560 --> 00:16:16,320
to uh to update state in a consistent

00:16:15,839 --> 00:16:23,120
manner

00:16:16,320 --> 00:16:24,399
and share across processes if if needed

00:16:23,120 --> 00:16:28,320
let's now talk a bit about the the

00:16:24,399 --> 00:16:28,320
private architecture um

00:16:28,959 --> 00:16:34,399
the improv vega if you are

00:16:32,160 --> 00:16:36,800
ingesting data you can in and you're

00:16:34,399 --> 00:16:38,480
ingesting using the event api

00:16:36,800 --> 00:16:40,320
you use an event writer so the

00:16:38,480 --> 00:16:40,720
application is an event writer one or

00:16:40,320 --> 00:16:45,279
more

00:16:40,720 --> 00:16:48,240
to append events to um to the stream

00:16:45,279 --> 00:16:49,839
the proviga checks the writer position

00:16:48,240 --> 00:16:51,680
in the case that uh that we have

00:16:49,839 --> 00:16:53,920
disconnections and we need to reconnect

00:16:51,680 --> 00:16:56,000
so that uh the writer knows where to

00:16:53,920 --> 00:16:59,279
resume from

00:16:56,000 --> 00:17:01,120
then the application reads from uh from

00:16:59,279 --> 00:17:03,440
that stream using event readers

00:17:01,120 --> 00:17:04,400
we group the event readers into reader

00:17:03,440 --> 00:17:06,959
groups and

00:17:04,400 --> 00:17:08,240
those readers split the load of of

00:17:06,959 --> 00:17:09,919
segments

00:17:08,240 --> 00:17:12,240
among them and they use the state

00:17:09,919 --> 00:17:13,520
synchronizer to perform that uh that

00:17:12,240 --> 00:17:17,199
kind of coordination

00:17:13,520 --> 00:17:19,199
so we can add in and remove readers

00:17:17,199 --> 00:17:20,959
as as needed so that gives us that bit

00:17:19,199 --> 00:17:23,600
of growing and shrinking

00:17:20,959 --> 00:17:25,839
uh the capacity of uh of of a reader

00:17:23,600 --> 00:17:25,839
group

00:17:27,280 --> 00:17:31,120
and the readers were given the presence

00:17:29,600 --> 00:17:32,480
of stream scaling remember that i

00:17:31,120 --> 00:17:33,039
mentioned the stream scaling feature

00:17:32,480 --> 00:17:36,080
where

00:17:33,039 --> 00:17:36,720
the degree of of parallelism in the

00:17:36,080 --> 00:17:39,840
stream

00:17:36,720 --> 00:17:42,160
can change over time so prevega takes

00:17:39,840 --> 00:17:44,000
care of of preserving the order

00:17:42,160 --> 00:17:45,280
of of segments that are that are

00:17:44,000 --> 00:17:48,799
generated

00:17:45,280 --> 00:17:50,799
and and that's um that assignment of

00:17:48,799 --> 00:17:52,640
of segments as they are created and they

00:17:50,799 --> 00:17:54,880
are sealed that's performed

00:17:52,640 --> 00:17:56,559
internally by uh by the logic of uh well

00:17:54,880 --> 00:17:59,360
by providing by the logic of

00:17:56,559 --> 00:18:00,960
of the reader group so the reader groups

00:17:59,360 --> 00:18:03,360
work even in the presence of uh

00:18:00,960 --> 00:18:06,640
in preserve order even in the presence

00:18:03,360 --> 00:18:06,640
of uh of autoscape

00:18:07,679 --> 00:18:12,720
we have um so there is the we prevent

00:18:11,039 --> 00:18:15,360
the control plane in the data plane

00:18:12,720 --> 00:18:16,160
um with the controller and the segment

00:18:15,360 --> 00:18:17,840
store

00:18:16,160 --> 00:18:19,520
so the controller is the component that

00:18:17,840 --> 00:18:20,640
is responsible for the lifecycle of

00:18:19,520 --> 00:18:23,360
streams

00:18:20,640 --> 00:18:24,880
it takes requests to create a stream to

00:18:23,360 --> 00:18:28,000
delete a stream

00:18:24,880 --> 00:18:29,600
and also takes care of transactions so

00:18:28,000 --> 00:18:31,600
if if an application creates a

00:18:29,600 --> 00:18:32,799
transaction appends to it and then

00:18:31,600 --> 00:18:34,559
commits the control

00:18:32,799 --> 00:18:36,000
will be responsible for performing the

00:18:34,559 --> 00:18:38,960
merges

00:18:36,000 --> 00:18:40,960
and the segment store is unaware of

00:18:38,960 --> 00:18:42,799
streams the segment store all the deals

00:18:40,960 --> 00:18:45,679
with the life cycle of

00:18:42,799 --> 00:18:46,720
segments stores the the segment data it

00:18:45,679 --> 00:18:49,120
performs

00:18:46,720 --> 00:18:51,600
merges create segments delete segments

00:18:49,120 --> 00:18:51,600
as needed

00:18:54,000 --> 00:18:57,600
now both the controller and the segment

00:18:55,919 --> 00:19:00,640
store are stateless

00:18:57,600 --> 00:19:03,679
they pres they keep states in uh

00:19:00,640 --> 00:19:05,120
in a storage tier which has uh three

00:19:03,679 --> 00:19:07,200
main components

00:19:05,120 --> 00:19:08,160
uh we use zookeeper for some uh

00:19:07,200 --> 00:19:10,080
coordination

00:19:08,160 --> 00:19:11,679
tasks we do not use the keeper for

00:19:10,080 --> 00:19:14,880
metadata

00:19:11,679 --> 00:19:17,520
so the the we do not rely on zookeeper

00:19:14,880 --> 00:19:18,240
for metadata data that can grow without

00:19:17,520 --> 00:19:20,640
bound so

00:19:18,240 --> 00:19:22,400
as we add new streams uh the state that

00:19:20,640 --> 00:19:24,640
is kept in zookeeper does not

00:19:22,400 --> 00:19:25,679
does not grow accordingly now in the

00:19:24,640 --> 00:19:28,880
segment store

00:19:25,679 --> 00:19:32,080
we rely on on both

00:19:28,880 --> 00:19:32,720
bookkeeper uh for what we for for our

00:19:32,080 --> 00:19:36,000
journals

00:19:32,720 --> 00:19:38,799
so as data come in we store data in uh

00:19:36,000 --> 00:19:40,320
in journal for uh for for the sorry

00:19:38,799 --> 00:19:43,600
bookkeeper ledgers

00:19:40,320 --> 00:19:46,080
uh for durability and we

00:19:43,600 --> 00:19:48,000
asynchronously flush the data to

00:19:46,080 --> 00:19:49,919
long-term storage we can be

00:19:48,000 --> 00:19:51,760
implemented with fire or object we give

00:19:49,919 --> 00:19:54,080
different options for their long-term

00:19:51,760 --> 00:19:55,679
storage

00:19:54,080 --> 00:19:58,320
inside the segment store we have the

00:19:55,679 --> 00:19:59,360
concept of of segment containers the

00:19:58,320 --> 00:20:01,600
segment container

00:19:59,360 --> 00:20:03,039
is it is not to be confused with the

00:20:01,600 --> 00:20:06,559
linux containers

00:20:03,039 --> 00:20:09,919
is a grouping of segments is the units

00:20:06,559 --> 00:20:12,400
we use uh of work assignment across

00:20:09,919 --> 00:20:13,039
segment store instances so given the set

00:20:12,400 --> 00:20:15,919
of uh

00:20:13,039 --> 00:20:17,760
of uh segment containers in the system

00:20:15,919 --> 00:20:18,880
uh the controller will be responsible

00:20:17,760 --> 00:20:20,559
for

00:20:18,880 --> 00:20:22,880
assigning all those segment control

00:20:20,559 --> 00:20:25,440
containers across the the segment store

00:20:22,880 --> 00:20:25,440
instances

00:20:25,919 --> 00:20:28,960
and this is the next point that i want

00:20:27,280 --> 00:20:31,360
to quickly touch upon so

00:20:28,960 --> 00:20:32,480
the controller uh knows the segment the

00:20:31,360 --> 00:20:35,600
second containers that

00:20:32,480 --> 00:20:36,400
that are in the system and and it

00:20:35,600 --> 00:20:38,720
assigns

00:20:36,400 --> 00:20:41,600
the existing segment containers to the

00:20:38,720 --> 00:20:44,240
to the existing segment store instances

00:20:41,600 --> 00:20:45,760
and in the case say that we add a new

00:20:44,240 --> 00:20:47,679
segment store instance the controller

00:20:45,760 --> 00:20:49,200
will be responsible for performing the

00:20:47,679 --> 00:20:51,919
the reassignment

00:20:49,200 --> 00:20:53,039
so that it balances the load across the

00:20:51,919 --> 00:20:55,840
the existing

00:20:53,039 --> 00:20:55,840
segment stores

00:20:58,480 --> 00:21:01,600
now looking at the right and the read

00:21:00,000 --> 00:21:02,559
path let's look more closely and see

00:21:01,600 --> 00:21:05,280
what's going on so

00:21:02,559 --> 00:21:07,200
on the right path an event stream writer

00:21:05,280 --> 00:21:10,320
if he wants to append

00:21:07,200 --> 00:21:11,039
events then he first will talk to the

00:21:10,320 --> 00:21:13,600
controller

00:21:11,039 --> 00:21:15,039
and see which segment store is supposed

00:21:13,600 --> 00:21:17,200
to connect to

00:21:15,039 --> 00:21:21,280
once it does it it will connect to the

00:21:17,200 --> 00:21:21,280
segment store and start appending bytes

00:21:21,520 --> 00:21:25,600
the segment store won't acknowledge to

00:21:24,080 --> 00:21:28,159
the event writer

00:21:25,600 --> 00:21:29,440
until it has persisted in apache boot

00:21:28,159 --> 00:21:31,840
keeper

00:21:29,440 --> 00:21:32,640
and once it does it can acknowledge um

00:21:31,840 --> 00:21:35,280
but the data is

00:21:32,640 --> 00:21:36,159
immediately written to long-term storage

00:21:35,280 --> 00:21:39,360
that data is

00:21:36,159 --> 00:21:42,799
synchronously flushed to uh to that

00:21:39,360 --> 00:21:43,280
tiered storage so apache bookkeeper is

00:21:42,799 --> 00:21:46,080
there

00:21:43,280 --> 00:21:47,679
to guarantee durability we we use it

00:21:46,080 --> 00:21:49,600
because we want to guarantee

00:21:47,679 --> 00:21:50,799
low latency in the presence of a of

00:21:49,600 --> 00:21:53,360
small rights

00:21:50,799 --> 00:21:54,400
uh but then we rely on long-term storage

00:21:53,360 --> 00:21:56,159
to keep data

00:21:54,400 --> 00:21:57,919
for uh for for a longer period of

00:21:56,159 --> 00:21:58,640
periods of time remember that one of our

00:21:57,919 --> 00:22:01,760
goals

00:21:58,640 --> 00:22:02,320
is to store an unbounded amount of data

00:22:01,760 --> 00:22:04,799
per

00:22:02,320 --> 00:22:05,600
per string and for that we have again

00:22:04,799 --> 00:22:08,000
different options

00:22:05,600 --> 00:22:09,280
it can be based on file or objects and

00:22:08,000 --> 00:22:13,840
this is configurable

00:22:09,280 --> 00:22:13,840
at the deployment time

00:22:14,400 --> 00:22:19,200
for the read path uh we it's similar the

00:22:17,679 --> 00:22:20,640
event stream reader needs to contact the

00:22:19,200 --> 00:22:24,720
controller to know which segment

00:22:20,640 --> 00:22:28,159
store to talk to um the segment store

00:22:24,720 --> 00:22:30,400
has a cache where it keeps the data

00:22:28,159 --> 00:22:31,919
that uh that is supposed to flush to

00:22:30,400 --> 00:22:34,640
long-term storage

00:22:31,919 --> 00:22:35,600
and that's considered to be the detail

00:22:34,640 --> 00:22:38,320
of uh

00:22:35,600 --> 00:22:39,679
of the streams or of the segments that

00:22:38,320 --> 00:22:41,600
that is managing for the segment

00:22:39,679 --> 00:22:44,559
containers it has assigned to it

00:22:41,600 --> 00:22:47,679
so when an event stream reader submits a

00:22:44,559 --> 00:22:47,679
request to read data

00:22:52,840 --> 00:22:55,840
from

00:23:03,360 --> 00:23:06,320
okay something have

00:23:06,640 --> 00:23:12,080
something happened to my to my um

00:23:09,760 --> 00:23:12,880
to my headset i don't know if uh if i'm

00:23:12,080 --> 00:23:18,000
audible

00:23:12,880 --> 00:23:19,200
um okay so sorry let me go back

00:23:18,000 --> 00:23:22,400
all right so let's talk about the read

00:23:19,200 --> 00:23:24,480
path so on the read path again you

00:23:22,400 --> 00:23:26,480
the event stream reader contacts the

00:23:24,480 --> 00:23:27,760
controller then it talks to the to the

00:23:26,480 --> 00:23:30,240
segment store

00:23:27,760 --> 00:23:31,200
request data from the segment store if

00:23:30,240 --> 00:23:32,559
it has in it

00:23:31,200 --> 00:23:34,559
in the cache it's going to return from

00:23:32,559 --> 00:23:36,159
the cache and this is suppose

00:23:34,559 --> 00:23:38,240
this is expected to happen in the case

00:23:36,159 --> 00:23:40,880
that uh the reader is stealing

00:23:38,240 --> 00:23:41,600
the the stream if not if it's a cache

00:23:40,880 --> 00:23:43,840
missed then we'll

00:23:41,600 --> 00:23:44,960
you read data from from long-term

00:23:43,840 --> 00:23:47,120
storage and uh

00:23:44,960 --> 00:23:48,000
and populate the cache and we'll serve

00:23:47,120 --> 00:23:51,200
from there

00:23:48,000 --> 00:23:52,960
note that um we never serve reads from

00:23:51,200 --> 00:23:55,600
apache bookkeeper today

00:23:52,960 --> 00:23:56,320
so that data in bookkeeper is used only

00:23:55,600 --> 00:23:59,279
upon

00:23:56,320 --> 00:24:01,039
recovery so for example when we move a

00:23:59,279 --> 00:24:01,840
segment container to a different segment

00:24:01,039 --> 00:24:05,120
store

00:24:01,840 --> 00:24:06,480
uh or when the segment store uh give us

00:24:05,120 --> 00:24:09,919
given segments for instance

00:24:06,480 --> 00:24:13,840
um restarts uh a segment container

00:24:09,919 --> 00:24:16,880
it can repopulate the cache or with uh

00:24:13,840 --> 00:24:20,400
with uh or in its metadata from the

00:24:16,880 --> 00:24:20,400
apache boot keeper lighters

00:24:23,840 --> 00:24:29,120
okay so that was an overview of uh

00:24:26,880 --> 00:24:30,880
of prevegan so remember that i talked a

00:24:29,120 --> 00:24:32,799
bit about data streams and uh and some

00:24:30,880 --> 00:24:34,640
core properties uh then i covered

00:24:32,799 --> 00:24:35,440
provega i talked about some of the core

00:24:34,640 --> 00:24:37,039
features

00:24:35,440 --> 00:24:38,799
and then i i talked a bit about the

00:24:37,039 --> 00:24:40,159
architecture of prevega

00:24:38,799 --> 00:24:42,080
now i'm going to switch gears and talk

00:24:40,159 --> 00:24:42,960
about one concrete problem which is the

00:24:42,080 --> 00:24:45,679
one of uh

00:24:42,960 --> 00:24:48,880
ingesting data in a in an exactly once

00:24:45,679 --> 00:24:51,760
manner to a pro vegas stream

00:24:48,880 --> 00:24:52,559
so the setup looks like this i have a

00:24:51,760 --> 00:24:54,320
data source

00:24:52,559 --> 00:24:56,240
which can be a number of things it can

00:24:54,320 --> 00:24:59,120
be an end user it can be again

00:24:56,240 --> 00:24:59,840
a server it can be a sensor it can be uh

00:24:59,120 --> 00:25:03,039
even perfect

00:24:59,840 --> 00:25:06,320
itself uh or files uh

00:25:03,039 --> 00:25:08,640
in a file system i have an application

00:25:06,320 --> 00:25:09,360
that is receiving this data is reading

00:25:08,640 --> 00:25:11,919
this data

00:25:09,360 --> 00:25:12,720
and it's using an eventwriter to append

00:25:11,919 --> 00:25:15,840
data to

00:25:12,720 --> 00:25:18,960
a vegas stream so that's that's the

00:25:15,840 --> 00:25:22,320
the setup now

00:25:18,960 --> 00:25:25,279
there are a couple of uh in broad terms

00:25:22,320 --> 00:25:27,600
two source two two data source types

00:25:25,279 --> 00:25:29,279
that we care about

00:25:27,600 --> 00:25:30,799
and i'm going to call them memory less

00:25:29,279 --> 00:25:33,120
and memory full

00:25:30,799 --> 00:25:35,279
so memoryless are the sources that are

00:25:33,120 --> 00:25:38,240
not capable of reference meeting events

00:25:35,279 --> 00:25:39,360
so they produce events they emit it and

00:25:38,240 --> 00:25:40,400
they forget about it there's no

00:25:39,360 --> 00:25:43,360
buffering there is no

00:25:40,400 --> 00:25:44,559
no no caching there is uh no persistent

00:25:43,360 --> 00:25:46,799
store

00:25:44,559 --> 00:25:48,320
right so we cannot guarantee exactly one

00:25:46,799 --> 00:25:50,159
semantics for such sources

00:25:48,320 --> 00:25:51,919
in the process of process crashes so

00:25:50,159 --> 00:25:53,520
we'll see what we can do

00:25:51,919 --> 00:25:55,600
and one example would be stateless

00:25:53,520 --> 00:25:56,080
sensors that again emits samples and do

00:25:55,600 --> 00:26:00,400
not

00:25:56,080 --> 00:26:02,000
um uh and do not keep them

00:26:00,400 --> 00:26:03,360
and then there's the memory foam version

00:26:02,000 --> 00:26:05,440
of it which are data sources that are

00:26:03,360 --> 00:26:06,000
capable of retransmitting from any given

00:26:05,440 --> 00:26:10,159
offset

00:26:06,000 --> 00:26:12,000
so from a position that you can refer to

00:26:10,159 --> 00:26:13,919
uh of course you might not be able to

00:26:12,000 --> 00:26:14,640
arbitrarily go back in time so that's

00:26:13,919 --> 00:26:17,440
probably

00:26:14,640 --> 00:26:18,880
that's possibly bounded but from uh for

00:26:17,440 --> 00:26:20,880
all practical purposes

00:26:18,880 --> 00:26:22,880
it's uh you should be able to resume

00:26:20,880 --> 00:26:24,640
from any point that uh that you need to

00:26:22,880 --> 00:26:26,080
so that's the assumption we're making

00:26:24,640 --> 00:26:28,640
for such data sources

00:26:26,080 --> 00:26:29,520
many examples would be uh would be files

00:26:28,640 --> 00:26:33,360
or uh or

00:26:29,520 --> 00:26:34,400
vegas streams all right so memoryless

00:26:33,360 --> 00:26:37,200
sources

00:26:34,400 --> 00:26:38,000
so from memoryless sources um let's say

00:26:37,200 --> 00:26:40,720
a sample

00:26:38,000 --> 00:26:41,440
a sensor so it emits three samples the

00:26:40,720 --> 00:26:44,240
application

00:26:41,440 --> 00:26:46,080
starts receiving them and appending them

00:26:44,240 --> 00:26:49,679
to a chopra vegas stream

00:26:46,080 --> 00:26:52,880
so the application uh takes

00:26:49,679 --> 00:26:55,520
blue and reds and uh and makes requests

00:26:52,880 --> 00:26:57,600
you append them

00:26:55,520 --> 00:26:59,919
now let's say that uh the connection

00:26:57,600 --> 00:27:01,919
breaks at that point

00:26:59,919 --> 00:27:03,520
right so what happened to the events

00:27:01,919 --> 00:27:05,039
what has gone through and what has not

00:27:03,520 --> 00:27:08,320
gone through

00:27:05,039 --> 00:27:10,559
so to deal with that uh we

00:27:08,320 --> 00:27:12,960
we do the following and here are four

00:27:10,559 --> 00:27:15,600
steps that uh that i i want to show

00:27:12,960 --> 00:27:16,480
in this slide so the first step is uh

00:27:15,600 --> 00:27:19,279
the writer

00:27:16,480 --> 00:27:20,880
appending the blue events there is

00:27:19,279 --> 00:27:21,440
internally there is a notion of a writer

00:27:20,880 --> 00:27:23,520
id

00:27:21,440 --> 00:27:25,520
and the segment store keeps a mapping of

00:27:23,520 --> 00:27:27,039
the writer id to the event that has been

00:27:25,520 --> 00:27:29,600
successfully appended

00:27:27,039 --> 00:27:31,360
so when blue is successfully appended uh

00:27:29,600 --> 00:27:32,080
it sets the event number to one and

00:27:31,360 --> 00:27:35,440
acknowledges

00:27:32,080 --> 00:27:38,480
back to the reader now in uh

00:27:35,440 --> 00:27:41,440
in in step two the writer

00:27:38,480 --> 00:27:42,399
submits the red events but the

00:27:41,440 --> 00:27:44,480
connection breaks

00:27:42,399 --> 00:27:46,159
at that point and he doesn't know if red

00:27:44,480 --> 00:27:47,600
event has gone through or not

00:27:46,159 --> 00:27:50,320
uh it happens that he has gone through

00:27:47,600 --> 00:27:53,760
the segment store uh has a record of it

00:27:50,320 --> 00:27:56,640
but the writer doesn't know so so

00:27:53,760 --> 00:27:58,240
in that case uh what the writer is going

00:27:56,640 --> 00:28:00,559
to do is it's going to

00:27:58,240 --> 00:28:02,240
start a new connection as part of that

00:28:00,559 --> 00:28:03,600
you have you do a handshake with the

00:28:02,240 --> 00:28:06,240
segment store

00:28:03,600 --> 00:28:07,840
and you tell its writer id the segment

00:28:06,240 --> 00:28:11,120
store will send back

00:28:07,840 --> 00:28:12,880
the last event that has been written now

00:28:11,120 --> 00:28:14,320
the writer knows that uh the red event

00:28:12,880 --> 00:28:15,520
has been written so he moves to the

00:28:14,320 --> 00:28:17,840
black events

00:28:15,520 --> 00:28:19,039
and uh and uh wrist and submits the

00:28:17,840 --> 00:28:21,600
black event to uh

00:28:19,039 --> 00:28:22,240
to be appended and now it's acknowledged

00:28:21,600 --> 00:28:24,480
the

00:28:22,240 --> 00:28:25,600
the the application is good to go right

00:28:24,480 --> 00:28:28,880
so the application

00:28:25,600 --> 00:28:30,960
has appended everything that uh that he

00:28:28,880 --> 00:28:33,279
needed

00:28:30,960 --> 00:28:34,720
now what happens if instead of the

00:28:33,279 --> 00:28:37,360
connection breaking the application

00:28:34,720 --> 00:28:40,880
crashes so in that case

00:28:37,360 --> 00:28:44,159
um the application can

00:28:40,880 --> 00:28:47,279
can restart but it doesn't have

00:28:44,159 --> 00:28:48,960
a way of of getting the the it doesn't

00:28:47,279 --> 00:28:50,640
have a way of determining what has been

00:28:48,960 --> 00:28:52,000
written it doesn't have a way of

00:28:50,640 --> 00:28:54,880
of getting from the sense of what's

00:28:52,000 --> 00:28:58,240
missing so the sensor can be transmitted

00:28:54,880 --> 00:28:58,880
by by design so let's look at what we

00:28:58,240 --> 00:29:01,520
can do

00:28:58,880 --> 00:29:02,399
with memory memory food sources now uh

00:29:01,520 --> 00:29:04,640
because we can do

00:29:02,399 --> 00:29:06,640
some interesting things there so with

00:29:04,640 --> 00:29:08,799
memory food sources let's assume that

00:29:06,640 --> 00:29:11,200
such a source is capable of rewinding

00:29:08,799 --> 00:29:13,039
so it's based on some notion of of

00:29:11,200 --> 00:29:14,799
offset

00:29:13,039 --> 00:29:18,000
a simple example of such a source would

00:29:14,799 --> 00:29:21,039
be a set of files that we want to ingest

00:29:18,000 --> 00:29:23,600
in in order and the offset can be

00:29:21,039 --> 00:29:24,320
a file name and a file name and enough

00:29:23,600 --> 00:29:26,720
an offset

00:29:24,320 --> 00:29:27,679
for that file there could be more

00:29:26,720 --> 00:29:30,000
complex

00:29:27,679 --> 00:29:31,760
examples of such a source it could be

00:29:30,000 --> 00:29:37,120
say a flink job where

00:29:31,760 --> 00:29:38,960
an offset could be a a job snapshot

00:29:37,120 --> 00:29:41,520
for the sake of example in this

00:29:38,960 --> 00:29:45,039
presentation i'll focus on

00:29:41,520 --> 00:29:45,039
files as a data source okay

00:29:45,200 --> 00:29:49,039
files as a data source the application

00:29:47,919 --> 00:29:52,080
starts reading

00:29:49,039 --> 00:29:55,200
it it reads the first two events blue

00:29:52,080 --> 00:29:57,600
and reds it depends then

00:29:55,200 --> 00:29:59,360
and then the application crashes right

00:29:57,600 --> 00:30:00,559
so the question is which events have

00:29:59,360 --> 00:30:02,960
been successfully

00:30:00,559 --> 00:30:05,360
appended to the to the stream so the

00:30:02,960 --> 00:30:07,279
application doesn't know

00:30:05,360 --> 00:30:08,720
so what we can do here is to introduce

00:30:07,279 --> 00:30:11,200
transactions

00:30:08,720 --> 00:30:12,799
so in the way we introduce transactions

00:30:11,200 --> 00:30:15,440
the application creates a transaction

00:30:12,799 --> 00:30:16,880
before it appends anything

00:30:15,440 --> 00:30:19,200
inside provider when it creates a

00:30:16,880 --> 00:30:20,720
transaction it will proviga will create

00:30:19,200 --> 00:30:23,600
the transaction segments as i have

00:30:20,720 --> 00:30:26,080
described before

00:30:23,600 --> 00:30:27,120
then it will append events to the

00:30:26,080 --> 00:30:31,200
transaction

00:30:27,120 --> 00:30:33,440
um the and once it has done it

00:30:31,200 --> 00:30:35,120
it commits the transaction and now all

00:30:33,440 --> 00:30:37,840
events become available as part of

00:30:35,120 --> 00:30:37,840
of the mainstream

00:30:39,360 --> 00:30:43,279
then for the remaining events it creates

00:30:41,279 --> 00:30:46,880
a new transaction

00:30:43,279 --> 00:30:46,880
it appends the remaining events

00:30:47,120 --> 00:30:50,640
it commits the transaction and now we're

00:30:50,320 --> 00:30:52,880
done

00:30:50,640 --> 00:30:54,640
so we have covered the the the whole

00:30:52,880 --> 00:30:57,919
file right and append all the

00:30:54,640 --> 00:31:00,640
the the corresponding events but now

00:30:57,919 --> 00:31:01,360
what happens if the application crashes

00:31:00,640 --> 00:31:02,880
in the middle

00:31:01,360 --> 00:31:05,039
of a transaction so before an

00:31:02,880 --> 00:31:09,600
application has the chance

00:31:05,039 --> 00:31:09,600
of uh of uh of committing it

00:31:09,679 --> 00:31:12,880
so let's say that we're in the situation

00:31:11,039 --> 00:31:15,840
that we committed the first transaction

00:31:12,880 --> 00:31:17,279
but we have we haven't committed the

00:31:15,840 --> 00:31:20,559
second transaction

00:31:17,279 --> 00:31:22,880
and then the application crashes so how

00:31:20,559 --> 00:31:23,919
does it know uh up to what offset has

00:31:22,880 --> 00:31:27,440
been successfully

00:31:23,919 --> 00:31:27,919
committed um it doesn't right so it

00:31:27,440 --> 00:31:31,120
needs

00:31:27,919 --> 00:31:32,240
some state to determine uh where where

00:31:31,120 --> 00:31:34,159
the application was

00:31:32,240 --> 00:31:35,679
and even if there is an outstanding

00:31:34,159 --> 00:31:38,159
transaction so that's where we

00:31:35,679 --> 00:31:40,960
introduced the state synchronizer

00:31:38,159 --> 00:31:42,399
so the state synchronizer is an

00:31:40,960 --> 00:31:43,519
abstraction available as part of the

00:31:42,399 --> 00:31:45,200
client api

00:31:43,519 --> 00:31:47,919
it enables the coordination of state

00:31:45,200 --> 00:31:50,399
across process uh when i say process um

00:31:47,919 --> 00:31:52,240
you know it can be application instances

00:31:50,399 --> 00:31:53,919
we do use it internally as part of our

00:31:52,240 --> 00:31:54,480
reader groups to coordinate the actions

00:31:53,919 --> 00:31:57,360
of

00:31:54,480 --> 00:31:59,039
of the readers in a in a reader group

00:31:57,360 --> 00:32:01,440
but again it's also exposed through the

00:31:59,039 --> 00:32:04,240
api and applications can use it

00:32:01,440 --> 00:32:06,240
the processes that that use the state

00:32:04,240 --> 00:32:08,080
synchronizer they can update states

00:32:06,240 --> 00:32:09,440
conditionally and they can read state

00:32:08,080 --> 00:32:13,200
updates to update the

00:32:09,440 --> 00:32:15,679
their um their local state in our case

00:32:13,200 --> 00:32:16,799
in in our application in this example i

00:32:15,679 --> 00:32:18,720
will persist states

00:32:16,799 --> 00:32:21,120
using the state synchronizer interface

00:32:18,720 --> 00:32:22,960
and the state is a simple pair

00:32:21,120 --> 00:32:25,840
of starting file set offset and

00:32:22,960 --> 00:32:25,840
transaction id

00:32:26,320 --> 00:32:31,200
so now the application will create a

00:32:28,960 --> 00:32:33,120
transaction before it does anything

00:32:31,200 --> 00:32:34,880
it will update the state in the

00:32:33,120 --> 00:32:35,600
synchronizer saying you know i met

00:32:34,880 --> 00:32:38,320
offset

00:32:35,600 --> 00:32:39,519
0 of the file and i have start

00:32:38,320 --> 00:32:43,360
transaction id with

00:32:39,519 --> 00:32:43,840
with this id and then you will perform

00:32:43,360 --> 00:32:47,679
the

00:32:43,840 --> 00:32:48,799
appends and then we'll commit the

00:32:47,679 --> 00:32:51,440
transaction

00:32:48,799 --> 00:32:51,840
and for the remaining events it will

00:32:51,440 --> 00:32:54,080
create

00:32:51,840 --> 00:32:56,399
a new transaction updates the state of

00:32:54,080 --> 00:32:58,640
the synchronizer

00:32:56,399 --> 00:33:00,960
update events and then commit the

00:32:58,640 --> 00:33:03,200
transaction

00:33:00,960 --> 00:33:04,320
and now what happens if the application

00:33:03,200 --> 00:33:07,519
crashes before

00:33:04,320 --> 00:33:08,559
completing the transaction so if the

00:33:07,519 --> 00:33:10,240
application crashes

00:33:08,559 --> 00:33:11,840
before completing the transaction so now

00:33:10,240 --> 00:33:15,039
we have a transaction open

00:33:11,840 --> 00:33:17,440
the events have been appended but the

00:33:15,039 --> 00:33:18,320
transaction is open uh the application

00:33:17,440 --> 00:33:20,240
will reads

00:33:18,320 --> 00:33:22,240
they will recover and as part of their

00:33:20,240 --> 00:33:24,640
recovery will read the state of the

00:33:22,240 --> 00:33:26,320
state synchronizer

00:33:24,640 --> 00:33:27,760
it will determine that uh it would check

00:33:26,320 --> 00:33:30,080
the status of transaction with the

00:33:27,760 --> 00:33:32,640
because it has the i the transaction id

00:33:30,080 --> 00:33:35,440
it will check the the status of the

00:33:32,640 --> 00:33:37,279
transaction against provega

00:33:35,440 --> 00:33:39,360
it would tell provigo to abort the

00:33:37,279 --> 00:33:42,559
transaction

00:33:39,360 --> 00:33:43,279
and it will start over so create a

00:33:42,559 --> 00:33:46,240
transaction

00:33:43,279 --> 00:33:47,039
append the the state of the synchronizer

00:33:46,240 --> 00:33:50,159
append

00:33:47,039 --> 00:33:51,519
the the the remaining events and then

00:33:50,159 --> 00:33:54,720
commit the transaction

00:33:51,519 --> 00:33:57,200
and now our job is uh is done for for

00:33:54,720 --> 00:33:57,200
that file

00:33:58,080 --> 00:34:02,880
okay so now i wanted to give a quick

00:34:01,279 --> 00:34:04,840
talk a bit about codename and give a

00:34:02,880 --> 00:34:08,159
quick demo

00:34:04,840 --> 00:34:10,000
um so

00:34:08,159 --> 00:34:11,520
the simple code that i have to show so

00:34:10,000 --> 00:34:14,000
you have the url

00:34:11,520 --> 00:34:15,599
in the at the bottom of uh at the bottom

00:34:14,000 --> 00:34:17,599
right of the slide

00:34:15,599 --> 00:34:19,839
and what this code does is the following

00:34:17,599 --> 00:34:20,320
uh in in broad terms it iterates over a

00:34:19,839 --> 00:34:22,879
set of

00:34:20,320 --> 00:34:25,040
of sample files uh and we perform one

00:34:22,879 --> 00:34:26,960
transaction per file to simplify it

00:34:25,040 --> 00:34:28,480
uh there's no bound in the amount of

00:34:26,960 --> 00:34:30,560
data in a transaction so

00:34:28,480 --> 00:34:32,159
it doesn't matter the size of the file

00:34:30,560 --> 00:34:34,000
even though this can be calibrated

00:34:32,159 --> 00:34:35,919
uh if it's if it's uh it's small enough

00:34:34,000 --> 00:34:37,520
then uh though doing one five

00:34:35,919 --> 00:34:39,919
transaction is uh

00:34:37,520 --> 00:34:40,879
is is perfectly reasonable and we commit

00:34:39,919 --> 00:34:44,079
zones once

00:34:40,879 --> 00:34:46,720
all the events have been written then

00:34:44,079 --> 00:34:47,520
it's simple code also implements a state

00:34:46,720 --> 00:34:49,440
synchronizer

00:34:47,520 --> 00:34:51,359
upon creating a new transaction it

00:34:49,440 --> 00:34:53,440
updates the synchronized states

00:34:51,359 --> 00:34:55,200
the state is a single value an instance

00:34:53,440 --> 00:34:59,040
of status where status is

00:34:55,200 --> 00:35:01,040
uh is a file id transaction id

00:34:59,040 --> 00:35:03,920
upon recovery it fetches the latest

00:35:01,040 --> 00:35:08,640
status abort any outstanding transaction

00:35:03,920 --> 00:35:10,560
and and sets the the start file

00:35:08,640 --> 00:35:12,800
this is how the streaming initialization

00:35:10,560 --> 00:35:16,000
look like there are two streams

00:35:12,800 --> 00:35:19,520
one stream for the state synchronizer uh

00:35:16,000 --> 00:35:19,520
and one for the data stream

00:35:19,599 --> 00:35:23,680
now note that we have we are setting the

00:35:21,920 --> 00:35:25,440
scaling policy for both streams

00:35:23,680 --> 00:35:26,720
uh in this case we don't care about the

00:35:25,440 --> 00:35:28,560
auto scaling

00:35:26,720 --> 00:35:30,079
and so we are just setting a fixed

00:35:28,560 --> 00:35:31,839
number of segments so

00:35:30,079 --> 00:35:33,359
one for the safe synchronization because

00:35:31,839 --> 00:35:36,160
we don't need more and

00:35:33,359 --> 00:35:37,920
uh actually you can't use more and uh

00:35:36,160 --> 00:35:42,320
and for the data stream we're setting

00:35:37,920 --> 00:35:42,320
a fixed number of ten segments

00:35:42,800 --> 00:35:46,960
then we iterate over the files so given

00:35:45,839 --> 00:35:51,680
the list of files

00:35:46,960 --> 00:35:54,400
uh for each file uh for each file we

00:35:51,680 --> 00:35:56,800
begin a transaction we update the state

00:35:54,400 --> 00:35:57,280
of the state synchronizer with the file

00:35:56,800 --> 00:36:00,160
id

00:35:57,280 --> 00:36:00,400
and um and the transaction id note that

00:36:00,160 --> 00:36:02,720
uh

00:36:00,400 --> 00:36:04,560
if we're recovering you know if this is

00:36:02,720 --> 00:36:08,240
a restart and we're recovering then uh

00:36:04,560 --> 00:36:11,440
then this is going to skip some files

00:36:08,240 --> 00:36:12,640
and and now once we start a file we

00:36:11,440 --> 00:36:16,160
write the events of a file to the

00:36:12,640 --> 00:36:16,160
transaction and then we commit it

00:36:18,079 --> 00:36:22,240
implementing the synchronized the

00:36:19,599 --> 00:36:24,079
synchronizer is is also simple we need

00:36:22,240 --> 00:36:26,400
to define what the status

00:36:24,079 --> 00:36:28,000
uh the state i'm calling it up data both

00:36:26,400 --> 00:36:29,520
status here

00:36:28,000 --> 00:36:31,839
then we need to define what is the

00:36:29,520 --> 00:36:34,480
initial states and and how

00:36:31,839 --> 00:36:34,960
it performs state updates and all those

00:36:34,480 --> 00:36:37,680
are

00:36:34,960 --> 00:36:38,640
uh defined in those in those three

00:36:37,680 --> 00:36:40,880
classes

00:36:38,640 --> 00:36:42,400
and then we have access methods that uh

00:36:40,880 --> 00:36:44,079
that that's what is

00:36:42,400 --> 00:36:45,520
exposed to the applications how the

00:36:44,079 --> 00:36:49,359
application interacts

00:36:45,520 --> 00:36:49,359
with with the synchronizer class

00:36:49,599 --> 00:36:54,000
for recovery once we once the

00:36:52,720 --> 00:36:56,320
application starts

00:36:54,000 --> 00:36:58,000
it gets the state of the synchronizer it

00:36:56,320 --> 00:37:00,079
would check the status of

00:36:58,000 --> 00:37:02,400
of the outstanding transaction if it's

00:37:00,079 --> 00:37:04,720
too open then the application aborts it

00:37:02,400 --> 00:37:06,000
if it's supporting then uh then uh it

00:37:04,720 --> 00:37:08,800
let it be it's uh

00:37:06,000 --> 00:37:09,920
it um it starts from uh from the given

00:37:08,800 --> 00:37:11,839
file id

00:37:09,920 --> 00:37:14,160
because it assumes that it hasn't been

00:37:11,839 --> 00:37:15,040
done yet now if it's committed then it

00:37:14,160 --> 00:37:19,520
moves

00:37:15,040 --> 00:37:21,920
to the to the to the next file

00:37:19,520 --> 00:37:24,000
aborting the transaction as in the open

00:37:21,920 --> 00:37:26,000
case is not strictly necessary because

00:37:24,000 --> 00:37:27,040
provega would eventually time out and

00:37:26,000 --> 00:37:29,520
and uh and

00:37:27,040 --> 00:37:34,000
remove it but it's good practice to

00:37:29,520 --> 00:37:34,000
remove any data data that is unnecessary

00:37:34,240 --> 00:37:39,760
so now i want to give a demo of the

00:37:37,680 --> 00:37:40,960
approach that i have described and i

00:37:39,760 --> 00:37:43,520
want to show the flow

00:37:40,960 --> 00:37:45,119
of the of the sample code that i i've

00:37:43,520 --> 00:37:48,000
just talked about

00:37:45,119 --> 00:37:49,599
so let me talk a bit about the demo

00:37:48,000 --> 00:37:53,440
setup first

00:37:49,599 --> 00:37:54,560
i will i'll have a bunch of pre-created

00:37:53,440 --> 00:37:57,520
input files

00:37:54,560 --> 00:37:58,240
uh specifically i have 50 files each

00:37:57,520 --> 00:38:01,520
files

00:37:58,240 --> 00:38:03,040
each file has a thousand samples

00:38:01,520 --> 00:38:04,800
each sample is another object

00:38:03,040 --> 00:38:08,720
representing a simple event

00:38:04,800 --> 00:38:10,640
that has three fields then i will ingest

00:38:08,720 --> 00:38:13,680
the content of those files into

00:38:10,640 --> 00:38:15,920
a vega stream using the technique

00:38:13,680 --> 00:38:18,720
i have just i have described using

00:38:15,920 --> 00:38:21,520
transactions in a state synchronizer

00:38:18,720 --> 00:38:22,480
and then i use a flink job just to check

00:38:21,520 --> 00:38:24,240
the state of uh

00:38:22,480 --> 00:38:26,079
of the stream and and visualize it

00:38:24,240 --> 00:38:30,240
through uh through the dashboard

00:38:26,079 --> 00:38:32,720
so let's go let's go watch the demo

00:38:30,240 --> 00:38:34,320
let's first check that out all the files

00:38:32,720 --> 00:38:36,480
are there

00:38:34,320 --> 00:38:37,760
that we can ingest them into a vegas

00:38:36,480 --> 00:38:39,359
stream

00:38:37,760 --> 00:38:41,920
so all the files are there as i

00:38:39,359 --> 00:38:44,839
mentioned they have been pre-generated

00:38:41,920 --> 00:38:46,400
now the next step is to start pro vegas

00:38:44,839 --> 00:38:48,240
standalone

00:38:46,400 --> 00:38:50,160
i will start by vegas send them on and

00:38:48,240 --> 00:38:54,480
leave it running in the

00:38:50,160 --> 00:38:57,440
in the background and then

00:38:54,480 --> 00:38:59,359
the next step is to run the process that

00:38:57,440 --> 00:39:02,160
is going to read from the files

00:38:59,359 --> 00:39:03,839
and ingest it because it's going to be

00:39:02,160 --> 00:39:06,320
using transactions we'll see

00:39:03,839 --> 00:39:08,079
messages in the output making references

00:39:06,320 --> 00:39:10,960
to the transactions it's creating

00:39:08,079 --> 00:39:11,839
uh and committing now this is going to

00:39:10,960 --> 00:39:13,920
be

00:39:11,839 --> 00:39:15,920
using one transaction file so we'll read

00:39:13,920 --> 00:39:18,800
the content of a file

00:39:15,920 --> 00:39:19,760
right in as part of a transaction and

00:39:18,800 --> 00:39:23,920
then commit it

00:39:19,760 --> 00:39:23,920
so again we'll see one transaction per

00:39:26,839 --> 00:39:31,119
file

00:39:28,320 --> 00:39:33,520
so let's get it to run uh we should

00:39:31,119 --> 00:39:35,839
shortly start saying it uh

00:39:33,520 --> 00:39:38,240
it's starting the transactions all right

00:39:35,839 --> 00:39:41,760
so it's starting the first transaction

00:39:38,240 --> 00:39:43,520
um i have slowed down the the the the

00:39:41,760 --> 00:39:45,599
transactions so that we can see the

00:39:43,520 --> 00:39:48,079
output otherwise it would go too fast

00:39:45,599 --> 00:39:48,880
uh so it's going as i mentioned one file

00:39:48,079 --> 00:39:51,359
at a time

00:39:48,880 --> 00:39:52,640
now let's let's break this let's break

00:39:51,359 --> 00:39:54,640
the flow

00:39:52,640 --> 00:39:57,119
uh so that we see what happens so if you

00:39:54,640 --> 00:39:59,520
break the flow what should happen is

00:39:57,119 --> 00:40:00,720
uh because i i'm trying to ingest it in

00:39:59,520 --> 00:40:02,160
exactly once manner

00:40:00,720 --> 00:40:04,800
i don't want to repeat data i don't want

00:40:02,160 --> 00:40:06,160
to have duplicates of of data that i i

00:40:04,800 --> 00:40:10,560
have already ingested

00:40:06,160 --> 00:40:12,640
so it should from the states

00:40:10,560 --> 00:40:13,839
um of the application determine what is

00:40:12,640 --> 00:40:16,160
the next transaction

00:40:13,839 --> 00:40:17,680
or the next file to start from so

00:40:16,160 --> 00:40:21,040
because it committed

00:40:17,680 --> 00:40:24,079
um it committed up to

00:40:21,040 --> 00:40:26,480
nine we would expect it to start at uh

00:40:24,079 --> 00:40:28,640
at 10 if nine really committed should

00:40:26,480 --> 00:40:31,440
start at 10 so it's either 9 or 10

00:40:28,640 --> 00:40:32,160
depending on the on the current state so

00:40:31,440 --> 00:40:34,640
let's see how

00:40:32,160 --> 00:40:38,160
yes it starts from 10 uh and you can

00:40:34,640 --> 00:40:41,200
keep going it keeps going

00:40:38,160 --> 00:40:43,280
so let's do a few more and uh and repeat

00:40:41,200 --> 00:40:43,280
it

00:40:44,319 --> 00:40:49,280
so let's control c again let's resume

00:40:46,960 --> 00:40:49,280
again

00:40:49,359 --> 00:40:55,119
see where it starts from

00:40:53,119 --> 00:40:56,880
they should again skip the files that it

00:40:55,119 --> 00:40:59,599
has already ingested

00:40:56,880 --> 00:41:00,400
and uh and resume from the one that he

00:40:59,599 --> 00:41:03,040
has

00:41:00,400 --> 00:41:04,720
hasn't really committed so it correctly

00:41:03,040 --> 00:41:07,040
it starts from 17 so

00:41:04,720 --> 00:41:09,839
now let's see the state of the stream uh

00:41:07,040 --> 00:41:14,560
as i mentioned we'll do that using

00:41:09,839 --> 00:41:14,560
a think job so let's start a local flink

00:41:18,839 --> 00:41:24,960
cluster

00:41:21,760 --> 00:41:28,960
and start a reader

00:41:24,960 --> 00:41:35,839
to read the content of the stream

00:41:28,960 --> 00:41:35,839
let's see that's in the

00:41:36,240 --> 00:41:49,839
in the dashboard

00:41:50,240 --> 00:41:54,000
okay so the job has started

00:41:54,480 --> 00:41:58,160
uh we should eventually see

00:41:58,480 --> 00:42:02,319
we should eventually see the job um

00:42:01,440 --> 00:42:05,680
reading the

00:42:02,319 --> 00:42:07,760
the whole 50 000 events

00:42:05,680 --> 00:42:09,200
that are supposed to be in the stream

00:42:07,760 --> 00:42:12,319
yes so

00:42:09,200 --> 00:42:14,400
it updated up to 50 000 events that's

00:42:12,319 --> 00:42:14,880
the the exact number that we expect to

00:42:14,400 --> 00:42:17,359
see

00:42:14,880 --> 00:42:18,160
because again we have 55 1000 events per

00:42:17,359 --> 00:42:21,280
file

00:42:18,160 --> 00:42:24,240
uh that adds up to a total of of

00:42:21,280 --> 00:42:25,440
50 000 so it has no duplicates it didn't

00:42:24,240 --> 00:42:28,960
miss anything so

00:42:25,440 --> 00:42:30,640
this is what we we wanted to show

00:42:28,960 --> 00:42:32,960
now let's go back to the to the

00:42:30,640 --> 00:42:35,359
presentation

00:42:32,960 --> 00:42:36,160
one aspect i haven't i haven't covered

00:42:35,359 --> 00:42:38,000
in the demo

00:42:36,160 --> 00:42:39,760
but it's very very important if we're

00:42:38,000 --> 00:42:41,680
talking about running this in production

00:42:39,760 --> 00:42:44,240
is concurrency so let's say

00:42:41,680 --> 00:42:45,119
that the process data is supposed to

00:42:44,240 --> 00:42:49,119
ingest

00:42:45,119 --> 00:42:52,800
crashes and we want to finish the job so

00:42:49,119 --> 00:42:55,520
we would start a new a new process

00:42:52,800 --> 00:42:56,079
but in distributed systems you never

00:42:55,520 --> 00:42:59,119
really know

00:42:56,079 --> 00:42:59,680
things are just slow or they crashed so

00:42:59,119 --> 00:43:02,400
it could

00:42:59,680 --> 00:43:03,839
it could happen that uh we have we start

00:43:02,400 --> 00:43:04,240
a new writer but there's the old one is

00:43:03,839 --> 00:43:06,880
still

00:43:04,240 --> 00:43:08,079
still is still there and in such cases

00:43:06,880 --> 00:43:09,920
we could end up having

00:43:08,079 --> 00:43:11,119
duplicates to have two processes reading

00:43:09,920 --> 00:43:12,079
from the same file starting a

00:43:11,119 --> 00:43:14,160
transaction and

00:43:12,079 --> 00:43:15,280
and committing it so we would need to

00:43:14,160 --> 00:43:17,920
change

00:43:15,280 --> 00:43:19,520
the logic in the in the in the in our

00:43:17,920 --> 00:43:21,520
program to do that

00:43:19,520 --> 00:43:22,880
and that's possible because again of the

00:43:21,520 --> 00:43:24,640
condition updates that are

00:43:22,880 --> 00:43:26,640
able to do through the the state

00:43:24,640 --> 00:43:29,680
synchronizer that would avoid

00:43:26,640 --> 00:43:31,680
duplicates uh

00:43:29,680 --> 00:43:33,280
with respect to making sure that we read

00:43:31,680 --> 00:43:35,440
all the content we just saw the content

00:43:33,280 --> 00:43:38,079
there are two parts to it one is

00:43:35,440 --> 00:43:38,800
the logic is such that uh no writer

00:43:38,079 --> 00:43:41,280
skips

00:43:38,800 --> 00:43:42,079
files so it has if i was to sort it or

00:43:41,280 --> 00:43:44,000
there are files

00:43:42,079 --> 00:43:45,119
so guarantees that it goes through

00:43:44,000 --> 00:43:48,400
everything if

00:43:45,119 --> 00:43:49,839
if it finishes um but but also we need

00:43:48,400 --> 00:43:51,920
to guarantee that it finishes

00:43:49,839 --> 00:43:52,880
in which case we need to use some

00:43:51,920 --> 00:43:54,960
mechanism for

00:43:52,880 --> 00:43:56,400
electing a leader let's say so some form

00:43:54,960 --> 00:43:59,040
of leader selection

00:43:56,400 --> 00:43:59,599
they will guarantee that uh we ingest

00:43:59,040 --> 00:44:03,359
all the

00:43:59,599 --> 00:44:03,359
other the content we're interested in

00:44:04,640 --> 00:44:08,400
one last topic to to to close the

00:44:07,440 --> 00:44:11,760
presentation

00:44:08,400 --> 00:44:14,079
is uh exactly one entrance so i have

00:44:11,760 --> 00:44:16,319
discussed in this presentation uh

00:44:14,079 --> 00:44:17,280
exactly one semantics for ingestion but

00:44:16,319 --> 00:44:19,760
clearly

00:44:17,280 --> 00:44:21,440
an application is interested in exactly

00:44:19,760 --> 00:44:23,520
in exactly applications are interesting

00:44:21,440 --> 00:44:26,640
exactly one semantics end to end

00:44:23,520 --> 00:44:28,560
not only the ingestion i have

00:44:26,640 --> 00:44:30,720
covered informal presentations the

00:44:28,560 --> 00:44:31,839
mechanisms to do it but let's recap here

00:44:30,720 --> 00:44:34,880
just so that we have

00:44:31,839 --> 00:44:38,160
we have the full picture so if we if we

00:44:34,880 --> 00:44:40,000
use the scheme here to

00:44:38,160 --> 00:44:41,920
ingest in an exactly one smarter using

00:44:40,000 --> 00:44:45,680
transactions and synchronizers

00:44:41,920 --> 00:44:46,319
um now we want to have a pipeline where

00:44:45,680 --> 00:44:48,800
we read

00:44:46,319 --> 00:44:50,480
data from proviga and uh and we end up

00:44:48,800 --> 00:44:53,040
outputting to prevega again

00:44:50,480 --> 00:44:54,880
we can use checkpoints at the source and

00:44:53,040 --> 00:44:57,359
the transactions at the sink

00:44:54,880 --> 00:44:58,640
to implement a mechanism that guarantees

00:44:57,359 --> 00:45:01,680
exactly one sanctuary

00:44:58,640 --> 00:45:02,319
so applications can do it or they can

00:45:01,680 --> 00:45:05,280
use

00:45:02,319 --> 00:45:06,720
say extreme processors which implement

00:45:05,280 --> 00:45:07,040
that for you so if you write a job and

00:45:06,720 --> 00:45:10,319
say

00:45:07,040 --> 00:45:12,480
apache flink that uh and it's using

00:45:10,319 --> 00:45:13,599
vega as a source and as a sync you would

00:45:12,480 --> 00:45:16,319
be able to

00:45:13,599 --> 00:45:18,800
use such such features from pravega with

00:45:16,319 --> 00:45:21,200
such mechanisms

00:45:18,800 --> 00:45:22,160
so in flink you have this implementation

00:45:21,200 --> 00:45:24,720
of a two-phase

00:45:22,160 --> 00:45:27,119
commit protocol and the way this works

00:45:24,720 --> 00:45:28,800
with uh with provega is by

00:45:27,119 --> 00:45:30,400
is by having the master initiating a

00:45:28,800 --> 00:45:31,280
checkpoint and requesting a checkpoint

00:45:30,400 --> 00:45:33,200
from provega

00:45:31,280 --> 00:45:35,119
then running the the distributed

00:45:33,200 --> 00:45:36,240
snapshotting algorithm def link

00:45:35,119 --> 00:45:40,240
implements

00:45:36,240 --> 00:45:42,480
and when the the snapshot completes

00:45:40,240 --> 00:45:43,359
the transactions that at the sync are

00:45:42,480 --> 00:45:46,640
able to commit

00:45:43,359 --> 00:45:47,520
and make the output uh available so

00:45:46,640 --> 00:45:49,440
that's

00:45:47,520 --> 00:45:51,760
that's roughly how this um how this

00:45:49,440 --> 00:45:51,760
works

00:45:53,440 --> 00:45:56,480
i'm not ready to wrap up so in

00:45:55,520 --> 00:45:59,119
conclusion

00:45:56,480 --> 00:46:00,000
what i have covered today is i have

00:45:59,119 --> 00:46:03,359
started describing

00:46:00,000 --> 00:46:06,400
provega prevega is a storage system

00:46:03,359 --> 00:46:07,359
for streams it's one of its main

00:46:06,400 --> 00:46:10,800
abilities is to

00:46:07,359 --> 00:46:11,280
provide um applications with uh with an

00:46:10,800 --> 00:46:15,440
a with

00:46:11,280 --> 00:46:17,920
apis to read fresh data with low latency

00:46:15,440 --> 00:46:18,480
and also go back in time and uh and

00:46:17,920 --> 00:46:20,800
process

00:46:18,480 --> 00:46:22,319
historically so process historical data

00:46:20,800 --> 00:46:25,760
of a stream

00:46:22,319 --> 00:46:28,000
it builds on the concept of segments

00:46:25,760 --> 00:46:28,880
segments enable a number of interesting

00:46:28,000 --> 00:46:31,839
features

00:46:28,880 --> 00:46:33,520
uh we have discussed transactions uh

00:46:31,839 --> 00:46:36,800
there is auto scaling

00:46:33,520 --> 00:46:38,640
um um even safe synchronizers

00:46:36,800 --> 00:46:39,839
so all those come from the fact that we

00:46:38,640 --> 00:46:43,040
build on this

00:46:39,839 --> 00:46:44,880
this foundation of segments and

00:46:43,040 --> 00:46:46,240
provega is an open source project we are

00:46:44,880 --> 00:46:49,680
currently hosted on uh

00:46:46,240 --> 00:46:51,440
hosting the source code on github

00:46:49,680 --> 00:46:53,440
one specific problem that i have covered

00:46:51,440 --> 00:46:54,640
which is very important for applications

00:46:53,440 --> 00:46:57,200
that require strong

00:46:54,640 --> 00:46:58,400
strong consistency guarantees is the one

00:46:57,200 --> 00:47:01,119
of ingest

00:46:58,400 --> 00:47:02,640
ingesting data in an exactly once manner

00:47:01,119 --> 00:47:04,319
so i have described how to use

00:47:02,640 --> 00:47:05,280
transactions and the state synchronizer

00:47:04,319 --> 00:47:08,560
in provega

00:47:05,280 --> 00:47:10,640
to achieve that goal but of course

00:47:08,560 --> 00:47:13,200
just ingesting is not sufficient so i

00:47:10,640 --> 00:47:15,599
have also uh briefly touched upon

00:47:13,200 --> 00:47:17,119
how to uh to obtain exactly one's

00:47:15,599 --> 00:47:20,000
entrance describing

00:47:17,119 --> 00:47:21,280
some some earlier mechanisms that i i

00:47:20,000 --> 00:47:24,160
have talked about

00:47:21,280 --> 00:47:26,160
in other um in other presentations using

00:47:24,160 --> 00:47:28,640
checkpoint transactions to uh

00:47:26,160 --> 00:47:30,319
for exactly one's end to end this is

00:47:28,640 --> 00:47:32,240
actually implementing apache flink so if

00:47:30,319 --> 00:47:34,720
you're apache link user you can

00:47:32,240 --> 00:47:35,440
uh have exactly one sanctuary using

00:47:34,720 --> 00:47:38,640
provega

00:47:35,440 --> 00:47:41,520
and flink

00:47:38,640 --> 00:47:42,800
this uh this is a list of uh i want to

00:47:41,520 --> 00:47:45,440
leave with uh with a

00:47:42,800 --> 00:47:46,640
list of resources links to various

00:47:45,440 --> 00:47:48,640
things that i have covered throughout

00:47:46,640 --> 00:47:52,000
the talk the provega website

00:47:48,640 --> 00:47:54,400
the github uh flink connector and

00:47:52,000 --> 00:47:57,119
the sample code for the demo i have

00:47:54,400 --> 00:47:57,119
shown today

00:47:57,280 --> 00:48:07,839
so thank you

00:48:19,520 --> 00:48:21,599

YouTube URL: https://www.youtube.com/watch?v=2KK-KNArcPo


