Title: #bbuzz: Jim Dowling - The Feature Store: where Data Engineering meets Data Science
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/feature-store-where-data-engineering-meets-data-science

Engineering features for machine learning is hard. Before you start, you need to know: are you developing the features for training the model (Python?) or for serving the model (Java/javascript/etc), and if both - how do you ensure consistency of your features between training and inferencing? Could anybody else in your organization find the feature useful in their model(s)? If you are using a traditional data warehouse, how do you retrieve the value of a feature from last year (that has now been overwritten with more recent data) to test my model on data from last year?  How do you efficiently join features originating from different backend systems.

In this talk, we will answer these questions in the context of the Feature Store. We will show how a Feature Store can provide a natural interface between Data Engineers, who create reusable features from diverse data sources, and Data Scientists, who experiment with predictive models, built from the same features. We will dive into the only fully open-source Feature Store for machine learning, Hopsworks, to better understand the potential of Feature Stores.
Captions: 
	00:00:08,180 --> 00:00:14,800
hello everyone it's my pleasure to

00:00:11,280 --> 00:00:17,470
introduce Jim darling with the CEO

00:00:14,800 --> 00:00:20,890
clocks and also an associate professor

00:00:17,470 --> 00:00:23,650
at kth and he will be talking to us

00:00:20,890 --> 00:00:26,170
about the feature store where data

00:00:23,650 --> 00:00:31,090
engineering meets data science so over

00:00:26,170 --> 00:00:36,370
to you Jim ty can you hear me can you

00:00:31,090 --> 00:00:40,060
see me yes what am I wearing this is my

00:00:36,370 --> 00:00:42,430
third future so I'm not there in person

00:00:40,060 --> 00:00:43,840
but I'm there in spirits I'm in

00:00:42,430 --> 00:00:46,420
Stockholm actually so I'm calling it

00:00:43,840 --> 00:00:50,230
from Socko so yeah my name is Jim

00:00:46,420 --> 00:00:52,570
darling I'm I have the jewel position

00:00:50,230 --> 00:00:55,030
I'm a CEO of a start-up collage of

00:00:52,570 --> 00:00:56,920
clocks and then I'm also associate

00:00:55,030 --> 00:00:58,239
professor and this project I'm going to

00:00:56,920 --> 00:01:00,579
talk about that feature stores in

00:00:58,239 --> 00:01:02,949
general and what is the feature store

00:01:00,579 --> 00:01:04,540
but I'm also going to be specific we

00:01:02,949 --> 00:01:06,100
have an open source feature store that

00:01:04,540 --> 00:01:07,750
we've developed it's the first up own

00:01:06,100 --> 00:01:09,250
it's actually the only fully open source

00:01:07,750 --> 00:01:12,790
features store in the world right now

00:01:09,250 --> 00:01:14,710
it's called helps works and we've been

00:01:12,790 --> 00:01:16,390
developing it for a number of years or

00:01:14,710 --> 00:01:18,370
the platform called hops works for a

00:01:16,390 --> 00:01:20,710
number of years we worked on it at the

00:01:18,370 --> 00:01:23,410
University kth a research institute in

00:01:20,710 --> 00:01:25,630
stockholm rice and now the company are

00:01:23,410 --> 00:01:28,630
primarily driving this platform

00:01:25,630 --> 00:01:31,090
logical thoughts so it's been the result

00:01:28,630 --> 00:01:32,860
of a number of years of work but I get

00:01:31,090 --> 00:01:35,470
to concentrate on the feature store

00:01:32,860 --> 00:01:37,570
aspect of the platform because that is a

00:01:35,470 --> 00:01:40,210
modular platform and we're finding a lot

00:01:37,570 --> 00:01:41,620
of interest in the platform so the

00:01:40,210 --> 00:01:44,140
company comes from a research background

00:01:41,620 --> 00:01:46,870
and our chief scientist is a professor

00:01:44,140 --> 00:01:48,250
kth but we have a diverse bunch of

00:01:46,870 --> 00:01:53,770
people from different companies like

00:01:48,250 --> 00:01:55,240
Spotify team Taylor and others so let's

00:01:53,770 --> 00:01:58,930
get started and talk about what is a

00:01:55,240 --> 00:02:00,070
feature store this is a data engineering

00:01:58,930 --> 00:02:02,050
conference I'm going to give it data

00:02:00,070 --> 00:02:04,450
engineering perspective on the future

00:02:02,050 --> 00:02:06,520
store because like anything there's

00:02:04,450 --> 00:02:08,319
multiple perspectives on the same thing

00:02:06,520 --> 00:02:10,300
a data scientist might think a feature

00:02:08,319 --> 00:02:12,010
story is an easy way for them to get

00:02:10,300 --> 00:02:14,500
features for training models which is

00:02:12,010 --> 00:02:16,180
kind of true but the data scientist

00:02:14,500 --> 00:02:18,940
doesn't have to do all the hard work

00:02:16,180 --> 00:02:20,800
that's the data engineers job you as a

00:02:18,940 --> 00:02:22,900
data engineer need to get the data

00:02:20,800 --> 00:02:26,020
that's over here I'm going to use a pen

00:02:22,900 --> 00:02:28,569
if that works and we want to get it into

00:02:26,020 --> 00:02:30,430
a numerical representation

00:02:28,569 --> 00:02:32,859
data scientists work with so shock

00:02:30,430 --> 00:02:35,500
horror data scientists tend to work with

00:02:32,859 --> 00:02:37,299
arrays of numbers they don't they work

00:02:35,500 --> 00:02:39,819
with numerical data primarily so they

00:02:37,299 --> 00:02:43,030
don't work with varchars they don't

00:02:39,819 --> 00:02:45,579
really care too much about you know

00:02:43,030 --> 00:02:46,510
database backends de lakes and they

00:02:45,579 --> 00:02:49,180
don't know too much and they don't

00:02:46,510 --> 00:02:52,090
necessarily want to know too much so

00:02:49,180 --> 00:02:54,069
part of the challenge in in building

00:02:52,090 --> 00:02:56,200
end-to-end machine learning pipelines is

00:02:54,069 --> 00:02:57,670
that and you probably have heard this if

00:02:56,200 --> 00:02:59,200
you're interested in this talk you'll

00:02:57,670 --> 00:03:01,480
probably assume that a machine learning

00:02:59,200 --> 00:03:04,329
pipeline is an end-to-end pipeline where

00:03:01,480 --> 00:03:05,829
we take our raw data that's or and then

00:03:04,329 --> 00:03:06,909
we're going to transform us we're going

00:03:05,829 --> 00:03:08,439
to transform it and we're gonna have

00:03:06,909 --> 00:03:10,090
some features and the features will be

00:03:08,439 --> 00:03:12,819
used to train a model with this big

00:03:10,090 --> 00:03:15,669
enmity on that model and it's an

00:03:12,819 --> 00:03:17,590
end-to-end pipeline and there are some

00:03:15,669 --> 00:03:20,519
frameworks that still believe this but

00:03:17,590 --> 00:03:24,400
we don't so what we see is that we have

00:03:20,519 --> 00:03:27,250
this API in the middle this API here is

00:03:24,400 --> 00:03:30,040
mind the gap this is the feature store

00:03:27,250 --> 00:03:32,200
so I'm going to use FS to indicators and

00:03:30,040 --> 00:03:35,560
this is the API between data engineering

00:03:32,200 --> 00:03:37,900
and data science so the data engineers

00:03:35,560 --> 00:03:40,180
over here will work with data lakes

00:03:37,900 --> 00:03:45,190
databases online databases data

00:03:40,180 --> 00:03:46,239
warehouses event streams Kafka and what

00:03:45,190 --> 00:03:48,669
they're going to do is they're going to

00:03:46,239 --> 00:03:50,199
take the raw data and they're going to

00:03:48,669 --> 00:03:52,739
transform it into features and we're

00:03:50,199 --> 00:03:55,389
going to call those feature pipelines

00:03:52,739 --> 00:03:57,400
so a feature pipeline is a day it's

00:03:55,389 --> 00:03:59,199
effectively a data data processing

00:03:57,400 --> 00:04:01,540
pipeline that you may be familiar from

00:03:59,199 --> 00:04:03,909
platforms like flink and and SPARC and

00:04:01,540 --> 00:04:06,040
the feature pipelines will take the raw

00:04:03,909 --> 00:04:07,449
data at the backend and transform it

00:04:06,040 --> 00:04:09,280
into features and I'll explain what a

00:04:07,449 --> 00:04:10,870
feature isn't in a minute because not

00:04:09,280 --> 00:04:13,689
all data engineers know exactly what a

00:04:10,870 --> 00:04:17,260
feature is so that's going to be one

00:04:13,689 --> 00:04:19,209
pipeline and the monolithic end-to-end

00:04:17,260 --> 00:04:20,859
machine learning pipeline going from the

00:04:19,209 --> 00:04:22,270
raw data to the model we're breaking up

00:04:20,859 --> 00:04:24,759
into a feature pipeline and then we'll

00:04:22,270 --> 00:04:27,330
have a training pipeline over here I'm

00:04:24,759 --> 00:04:29,530
going to talk about both of them but

00:04:27,330 --> 00:04:31,750
will briefly introduce both but

00:04:29,530 --> 00:04:33,610
basically you'll have two pipelines one

00:04:31,750 --> 00:04:35,500
running at one cadence so the data will

00:04:33,610 --> 00:04:37,800
come in at a certain cadence and then

00:04:35,500 --> 00:04:39,789
the training pipelines tend to be either

00:04:37,800 --> 00:04:42,310
operationalized and they'll run at a

00:04:39,789 --> 00:04:43,930
picker cadence as well or

00:04:42,310 --> 00:04:46,030
we'll be done on demand so data

00:04:43,930 --> 00:04:49,720
scientists will run pipelines to create

00:04:46,030 --> 00:04:53,500
training data train models and so on so

00:04:49,720 --> 00:04:55,270
let's get cracking what is a feature so

00:04:53,500 --> 00:04:57,370
here's an example of the feature and it

00:04:55,270 --> 00:04:59,200
comes from a data engineering

00:04:57,370 --> 00:05:01,390
perspective we have a table on the

00:04:59,200 --> 00:05:03,910
left-hand side we're gonna call it hotel

00:05:01,390 --> 00:05:05,680
bookings and every time a booking is

00:05:03,910 --> 00:05:07,660
made we enter a row in this table and

00:05:05,680 --> 00:05:10,150
the table will have for example the room

00:05:07,660 --> 00:05:12,670
number maybe customer ID for who who

00:05:10,150 --> 00:05:14,620
booked it at the room booking dates and

00:05:12,670 --> 00:05:17,110
so on so I'm only showing two the

00:05:14,620 --> 00:05:19,240
columns for simplicity and it might you

00:05:17,110 --> 00:05:20,800
know you might have a row per days even

00:05:19,240 --> 00:05:22,750
depending on how you've implemented this

00:05:20,800 --> 00:05:25,780
so you might have a start date and an

00:05:22,750 --> 00:05:29,380
end date for your booking so the

00:05:25,780 --> 00:05:31,360
features what a feature is is think of

00:05:29,380 --> 00:05:33,400
it being a column in a table in the

00:05:31,360 --> 00:05:35,860
database that will help you predict

00:05:33,400 --> 00:05:38,170
something so it's it's a piece of

00:05:35,860 --> 00:05:40,540
information that's useful for for making

00:05:38,170 --> 00:05:42,730
a prediction because ultimately that's

00:05:40,540 --> 00:05:44,950
what our models are that we get in in

00:05:42,730 --> 00:05:47,020
data science we have a model and it's

00:05:44,950 --> 00:05:50,170
used to basically put in some data and

00:05:47,020 --> 00:05:52,390
make predictions so if we can take the

00:05:50,170 --> 00:05:54,700
raw data on the left and convert it into

00:05:52,390 --> 00:05:56,830
a representation transform it into a

00:05:54,700 --> 00:05:59,710
representation that makes it easier for

00:05:56,830 --> 00:06:02,710
machine learning algorithms to work with

00:05:59,710 --> 00:06:05,200
that data to make predictions from that

00:06:02,710 --> 00:06:07,030
data then we we do feature

00:06:05,200 --> 00:06:09,130
transformations so an example here would

00:06:07,030 --> 00:06:12,510
be to compute this thing called the load

00:06:09,130 --> 00:06:16,090
factor and in this case the load factor

00:06:12,510 --> 00:06:18,190
is a load factor for a given room and

00:06:16,090 --> 00:06:20,200
this is a feature and that feature isn't

00:06:18,190 --> 00:06:22,030
present in the original hotel bookings

00:06:20,200 --> 00:06:25,240
database we actually have to write some

00:06:22,030 --> 00:06:27,490
code to transform the raw data into the

00:06:25,240 --> 00:06:29,170
load factor and load factor is quite a

00:06:27,490 --> 00:06:32,050
useful feature because it can be used

00:06:29,170 --> 00:06:33,970
for anything to predict whether room

00:06:32,050 --> 00:06:36,250
will be free it could be so we might

00:06:33,970 --> 00:06:37,750
have one pipeline that's trying to train

00:06:36,250 --> 00:06:39,820
a model to compute whether room will be

00:06:37,750 --> 00:06:41,440
available at a given base we could have

00:06:39,820 --> 00:06:44,410
another pipeline there another problem

00:06:41,440 --> 00:06:47,500
or we trying to predict the amount of

00:06:44,410 --> 00:06:49,690
revenue that a hotel room will generate

00:06:47,500 --> 00:06:51,310
over a period of time so there's many

00:06:49,690 --> 00:06:53,380
different uses for this feature and

00:06:51,310 --> 00:06:55,090
that's another motivation for the future

00:06:53,380 --> 00:06:56,199
store so yeah do you find a feature

00:06:55,090 --> 00:06:58,900
store is that

00:06:56,199 --> 00:07:00,279
if we can reuse a feature many times in

00:06:58,900 --> 00:07:02,800
many different models

00:07:00,279 --> 00:07:04,389
well maybe we should reuse it maybe you

00:07:02,800 --> 00:07:06,490
shouldn't rewrite many different

00:07:04,389 --> 00:07:08,319
pipelines to compute recompute that

00:07:06,490 --> 00:07:11,139
feature in potentially many different

00:07:08,319 --> 00:07:12,490
ways which are not consistent so you're

00:07:11,139 --> 00:07:13,990
now at the point of saying well maybe I

00:07:12,490 --> 00:07:15,789
can have a library for that we're gonna

00:07:13,990 --> 00:07:17,289
see that yes a library will get any part

00:07:15,789 --> 00:07:19,389
of the way but typically we actually

00:07:17,289 --> 00:07:22,439
cache the feature data rather than just

00:07:19,389 --> 00:07:25,569
using a library so let's let's move on

00:07:22,439 --> 00:07:27,460
and give a more concrete example this is

00:07:25,569 --> 00:07:29,620
more data science like here we have a

00:07:27,460 --> 00:07:32,259
table with an ID and we have an array of

00:07:29,620 --> 00:07:33,689
features and you may look at this data

00:07:32,259 --> 00:07:35,800
and say I've done a machine learning

00:07:33,689 --> 00:07:38,139
course and I can see that this is

00:07:35,800 --> 00:07:40,150
numerical data and we know that when we

00:07:38,139 --> 00:07:42,400
train models we have numerical data and

00:07:40,150 --> 00:07:44,020
everything's great and maybe the Wizkid

00:07:42,400 --> 00:07:47,050
davis scientist looks at this and says

00:07:44,020 --> 00:07:49,060
well you know I see quite you know quite

00:07:47,050 --> 00:07:50,889
a spread between these numbers so we

00:07:49,060 --> 00:07:54,069
might have some miracle instability here

00:07:50,889 --> 00:07:55,900
I'm going to normalize these numbers so

00:07:54,069 --> 00:07:58,900
these numbers are floating point numbers

00:07:55,900 --> 00:08:00,610
normalizing it means it means squashing

00:07:58,900 --> 00:08:03,460
those numbers into a range typically

00:08:00,610 --> 00:08:07,810
between either 0 and 1 or 0.5 and minus

00:08:03,460 --> 00:08:09,639
0.5 and and this normalization where

00:08:07,810 --> 00:08:11,349
we're taking the original column the

00:08:09,639 --> 00:08:13,930
features column and we're converting it

00:08:11,349 --> 00:08:15,729
into another column called l1 norm that

00:08:13,930 --> 00:08:18,009
norm which the l1 norm were computing

00:08:15,729 --> 00:08:22,389
over those values and you write some

00:08:18,009 --> 00:08:24,339
code to do that so in in a typical you

00:08:22,389 --> 00:08:25,689
know data engineering company you'll

00:08:24,339 --> 00:08:27,069
have a little bit of code it might be

00:08:25,689 --> 00:08:28,779
written in pandas it might be written in

00:08:27,069 --> 00:08:31,479
PI SPARC this example is showing you the

00:08:28,779 --> 00:08:33,909
code anti-spark and it's going to use an

00:08:31,479 --> 00:08:38,800
l1 norm to compute that so there's that

00:08:33,909 --> 00:08:40,719
life is already available in ice bar so

00:08:38,800 --> 00:08:43,149
our particular features or the hop

00:08:40,719 --> 00:08:45,610
search feature store it builds on three

00:08:43,149 --> 00:08:47,019
main concepts and I'm jumping straight

00:08:45,610 --> 00:08:48,699
into that rather than talking

00:08:47,019 --> 00:08:51,430
historically about other features stores

00:08:48,699 --> 00:08:52,870
so companies like uber and airbnb if

00:08:51,430 --> 00:08:54,760
doctor at their features there's a

00:08:52,870 --> 00:08:57,190
website called feature store org that

00:08:54,760 --> 00:08:58,630
has a very good summary of available

00:08:57,190 --> 00:09:00,519
production feature stores and pretty

00:08:58,630 --> 00:09:02,769
much every hyper scale company as a

00:09:00,519 --> 00:09:04,480
feature store but our feature store is a

00:09:02,769 --> 00:09:06,610
general-purpose feature store we didn't

00:09:04,480 --> 00:09:09,370
build it for a specific domain we didn't

00:09:06,610 --> 00:09:10,030
have a domain-specific language to do

00:09:09,370 --> 00:09:11,500
feature end

00:09:10,030 --> 00:09:14,380
earing so the example I showed you was

00:09:11,500 --> 00:09:16,180
code in PI spark so the abstraction we

00:09:14,380 --> 00:09:18,940
build our future store on is a data

00:09:16,180 --> 00:09:21,160
frame so what we the abstractions we

00:09:18,940 --> 00:09:23,080
provide to users we're gonna ingest the

00:09:21,160 --> 00:09:26,170
data into our feature store as data

00:09:23,080 --> 00:09:29,170
frames so as a result of that we what we

00:09:26,170 --> 00:09:31,810
have our features as a kind of a flat

00:09:29,170 --> 00:09:33,580
namespace of what features are available

00:09:31,810 --> 00:09:35,260
in the feature store will see in our

00:09:33,580 --> 00:09:37,660
latest version we actually scoped them

00:09:35,260 --> 00:09:38,650
by the Future group name and but today

00:09:37,660 --> 00:09:42,610
I'm just gonna show you the flat

00:09:38,650 --> 00:09:44,410
namespace version but the features that

00:09:42,610 --> 00:09:47,770
come into the Future store they come in

00:09:44,410 --> 00:09:50,110
as data frames or groups of features so

00:09:47,770 --> 00:09:51,550
maybe we have a weakness this is an

00:09:50,110 --> 00:09:53,530
example from data science it's quite a

00:09:51,550 --> 00:09:55,480
well-known example you're trying to

00:09:53,530 --> 00:09:57,460
predict if a passenger and the Titanic

00:09:55,480 --> 00:09:59,230
survived or not and this data set exists

00:09:57,460 --> 00:10:02,320
and maybe it comes from a database okay

00:09:59,230 --> 00:10:03,910
and you know in a different type of use

00:10:02,320 --> 00:10:07,090
case this data will be updated

00:10:03,910 --> 00:10:10,390
frequently in our platform we support

00:10:07,090 --> 00:10:11,680
hoodie as a packing table for the for

00:10:10,390 --> 00:10:14,620
these feature groups so they're going to

00:10:11,680 --> 00:10:16,270
be stored on one hoodie in hive so as an

00:10:14,620 --> 00:10:17,980
external table in hive but we're going

00:10:16,270 --> 00:10:19,750
to use the hoodie file format that means

00:10:17,980 --> 00:10:21,160
we can do up certs we can update these

00:10:19,750 --> 00:10:23,830
feature groups as they come in we don't

00:10:21,160 --> 00:10:26,290
need to drop them and recreate them but

00:10:23,830 --> 00:10:28,300
the the data comes in and we store it

00:10:26,290 --> 00:10:29,920
the set of features as a feature groups

00:10:28,300 --> 00:10:33,010
here here we have four features inside

00:10:29,920 --> 00:10:34,270
the titanic past peer list data but data

00:10:33,010 --> 00:10:37,300
will come in from many different sources

00:10:34,270 --> 00:10:38,830
not just from a one database it may come

00:10:37,300 --> 00:10:42,130
in from a data Lake and may come in from

00:10:38,830 --> 00:10:44,080
a Kafka it may come in from wherever so

00:10:42,130 --> 00:10:46,300
let's assume that another source of data

00:10:44,080 --> 00:10:49,660
came in and in this case the data was a

00:10:46,300 --> 00:10:51,490
historian who looked up the passengers

00:10:49,660 --> 00:10:53,590
of the titanic and looked up how much

00:10:51,490 --> 00:10:58,440
money they had in their bank account and

00:10:53,590 --> 00:11:02,200
this came in from this museum over here

00:10:58,440 --> 00:11:03,940
so we now have a set of features in our

00:11:02,200 --> 00:11:06,070
feature store they're grouped by future

00:11:03,940 --> 00:11:08,620
groups and data scientists can come in

00:11:06,070 --> 00:11:10,960
and they can look at the features and

00:11:08,620 --> 00:11:12,700
they can basically say well I I would

00:11:10,960 --> 00:11:14,170
like to join together a bunch of these

00:11:12,700 --> 00:11:16,540
creatures that are in the feature store

00:11:14,170 --> 00:11:19,900
to create some training test data that

00:11:16,540 --> 00:11:22,060
I'll use to train my model with so what

00:11:19,900 --> 00:11:23,900
they would do typically in this case if

00:11:22,060 --> 00:11:26,240
you saw this data is you might have I

00:11:23,900 --> 00:11:28,610
offices that if I joined the bike

00:11:26,240 --> 00:11:30,440
balanced and here with the existing

00:11:28,610 --> 00:11:32,480
features in our Titanic passenger list

00:11:30,440 --> 00:11:34,600
well then I might be better able to

00:11:32,480 --> 00:11:37,880
predict if the passenger survives or not

00:11:34,600 --> 00:11:39,410
okay so the data scientist is gonna join

00:11:37,880 --> 00:11:41,090
these together I'll show you the code

00:11:39,410 --> 00:11:43,460
right doing do this it's much simpler

00:11:41,090 --> 00:11:44,570
than this diagram but they'll join those

00:11:43,460 --> 00:11:48,050
features together

00:11:44,570 --> 00:11:49,640
get a training test data sets actually

00:11:48,050 --> 00:11:52,100
data frame they're gonna get back and

00:11:49,640 --> 00:11:54,230
then they can decide on file format to

00:11:52,100 --> 00:11:55,430
materialize that as if they're gonna you

00:11:54,230 --> 00:11:56,690
can work with the data frame directly

00:11:55,430 --> 00:11:58,730
but often if you're doing deep learning

00:11:56,690 --> 00:12:00,800
you might say well gonna store this TF

00:11:58,730 --> 00:12:03,770
record file format and I'm gonna store

00:12:00,800 --> 00:12:05,900
it where GCSE s3 or on our file system

00:12:03,770 --> 00:12:09,500
house if S which is a next-generation

00:12:05,900 --> 00:12:10,820
HDFS so these are the concepts that we

00:12:09,500 --> 00:12:13,280
have you have features they're grouped

00:12:10,820 --> 00:12:14,630
together you can combine them or join

00:12:13,280 --> 00:12:16,040
them together to create training test

00:12:14,630 --> 00:12:17,900
datasets and you store those wherever

00:12:16,040 --> 00:12:20,270
you want to use them now everything here

00:12:17,900 --> 00:12:22,700
is going to be version because we want

00:12:20,270 --> 00:12:24,620
to be able to reproduce this process of

00:12:22,700 --> 00:12:26,800
creating training data sets and with the

00:12:24,620 --> 00:12:29,120
help of hoodie we can actually even even

00:12:26,800 --> 00:12:30,740
version based on the current state of

00:12:29,120 --> 00:12:34,310
the future group the current state of it

00:12:30,740 --> 00:12:36,530
as of this moment in time okay so let's

00:12:34,310 --> 00:12:37,730
let's take a little step back that's the

00:12:36,530 --> 00:12:39,740
abstractions that we have in our

00:12:37,730 --> 00:12:41,540
platform and talked a bit about where

00:12:39,740 --> 00:12:44,810
these features come from so the features

00:12:41,540 --> 00:12:46,340
will come in and be ingested into the

00:12:44,810 --> 00:12:49,880
feature store I said already it's going

00:12:46,340 --> 00:12:52,130
to be a store and some of them will have

00:12:49,880 --> 00:12:54,080
very low latency requirements and some

00:12:52,130 --> 00:12:55,880
of them will maybe be ingested

00:12:54,080 --> 00:12:57,890
periodically every day or every week or

00:12:55,880 --> 00:12:59,720
every month from a dead lake they may

00:12:57,890 --> 00:13:04,300
come in from operational databases data

00:12:59,720 --> 00:13:06,800
warehouses Kafka other event sources and

00:13:04,300 --> 00:13:08,450
here a couple of examples of them so we

00:13:06,800 --> 00:13:10,430
have one company we're working with who

00:13:08,450 --> 00:13:12,200
need the features to be transformed in

00:13:10,430 --> 00:13:14,570
less than two seconds so someone's

00:13:12,200 --> 00:13:16,550
entering something on a form then we

00:13:14,570 --> 00:13:18,440
need that data being entered by the user

00:13:16,550 --> 00:13:21,050
to be transformed into features so they

00:13:18,440 --> 00:13:22,610
can be used by the application but

00:13:21,050 --> 00:13:24,890
that's a very low latency use case but

00:13:22,610 --> 00:13:27,320
for all of the other use cases where we

00:13:24,890 --> 00:13:30,170
have at you know 10 20 30 seconds or

00:13:27,320 --> 00:13:32,720
minutes or hours or days well then we

00:13:30,170 --> 00:13:34,580
can we can run different types of

00:13:32,720 --> 00:13:37,640
applications so we can maybe use a batch

00:13:34,580 --> 00:13:37,850
spark application to take the data from

00:13:37,640 --> 00:13:39,380
the

00:13:37,850 --> 00:13:40,759
back in platform and push it into the

00:13:39,380 --> 00:13:43,370
feature store and in this case we also

00:13:40,759 --> 00:13:47,000
do streaming applications for the lower

00:13:43,370 --> 00:13:48,860
latency data sources so all of these

00:13:47,000 --> 00:13:49,940
will come in these features will come in

00:13:48,860 --> 00:13:51,350
active and the different Cadence's

00:13:49,940 --> 00:13:53,779
they're going to update the feature

00:13:51,350 --> 00:13:55,579
store so who's going to use the future

00:13:53,779 --> 00:13:57,649
store well we have it here an online

00:13:55,579 --> 00:13:59,810
application and the online application

00:13:57,649 --> 00:14:02,240
for example if it's trying to predict

00:13:59,810 --> 00:14:04,699
something like fraud it might say well I

00:14:02,240 --> 00:14:06,769
have a bunch of features that I got

00:14:04,699 --> 00:14:07,490
which were for example what is being

00:14:06,769 --> 00:14:09,019
purchased

00:14:07,490 --> 00:14:11,870
what's the ID of the item being

00:14:09,019 --> 00:14:13,940
purchased what is my session look like

00:14:11,870 --> 00:14:15,589
right now my shopping cart do you have

00:14:13,940 --> 00:14:18,019
it I have an identity already in the

00:14:15,589 --> 00:14:20,120
system that we can use so these IDs we

00:14:18,019 --> 00:14:22,850
can use to look up features historical

00:14:20,120 --> 00:14:24,769
features about the user have they bought

00:14:22,850 --> 00:14:27,589
off a lot from here have their shopping

00:14:24,769 --> 00:14:30,199
cart has it been filled up and emptied

00:14:27,589 --> 00:14:32,870
recently laughs you know anything any

00:14:30,199 --> 00:14:35,180
events or features that can help predict

00:14:32,870 --> 00:14:36,980
if with this transaction is going to be

00:14:35,180 --> 00:14:39,680
fraudulent or not that we can go to the

00:14:36,980 --> 00:14:41,480
feature store other users of the feature

00:14:39,680 --> 00:14:43,399
store our data scientists will create

00:14:41,480 --> 00:14:45,589
training data and we saw that already in

00:14:43,399 --> 00:14:47,269
the previous example but also you're

00:14:45,589 --> 00:14:49,310
gonna have batch applications can just

00:14:47,269 --> 00:14:51,110
pull features directly and use them to

00:14:49,310 --> 00:14:52,699
make predictions so it's not going to

00:14:51,110 --> 00:14:54,110
pull out the actual label and it's just

00:14:52,699 --> 00:14:56,600
going to say I need I can use these

00:14:54,110 --> 00:14:58,399
features I have a model and now it can

00:14:56,600 --> 00:15:00,350
actually just run those features against

00:14:58,399 --> 00:15:03,139
the model to make predictions or score

00:15:00,350 --> 00:15:05,240
that model now the thing that's

00:15:03,139 --> 00:15:08,209
interesting here is that if the feature

00:15:05,240 --> 00:15:10,130
store were a database you would expect

00:15:08,209 --> 00:15:11,509
that it would be able to provide the

00:15:10,130 --> 00:15:13,370
features of the features store unless

00:15:11,509 --> 00:15:16,310
it's the online application in less than

00:15:13,370 --> 00:15:17,839
10 milliseconds and you expect also that

00:15:16,310 --> 00:15:20,029
it will scale to many terabytes in size

00:15:17,839 --> 00:15:23,720
we have a customer swear bank you have

00:15:20,029 --> 00:15:26,389
over 40 terabytes of feature data in

00:15:23,720 --> 00:15:28,040
just one particular use case those who

00:15:26,389 --> 00:15:30,259
talk at the sparks last part somewhat

00:15:28,040 --> 00:15:32,240
about that so the problem here is that

00:15:30,259 --> 00:15:34,069
we want to store many terabytes of data

00:15:32,240 --> 00:15:36,110
we want to scale our database and it

00:15:34,069 --> 00:15:38,569
also needs to have extremely low latency

00:15:36,110 --> 00:15:40,490
so there aren't any existing databases

00:15:38,569 --> 00:15:42,050
that we're aware of anybody else who's

00:15:40,490 --> 00:15:45,110
developing a future store is aware of

00:15:42,050 --> 00:15:46,670
and that has these properties so what we

00:15:45,110 --> 00:15:49,639
do is we actually split up the feature

00:15:46,670 --> 00:15:51,710
store into an online layer that's going

00:15:49,639 --> 00:15:53,510
to serve the feature as a serving store

00:15:51,710 --> 00:15:56,180
an offline feature store so this is a

00:15:53,510 --> 00:15:58,190
scalable sequel database typically so

00:15:56,180 --> 00:16:00,950
what what this does is it now

00:15:58,190 --> 00:16:02,900
complicates the process quite

00:16:00,950 --> 00:16:06,920
significantly as you can see here

00:16:02,900 --> 00:16:09,320
whenever we want to store these features

00:16:06,920 --> 00:16:10,820
we need to actually now know how am i

00:16:09,320 --> 00:16:13,310
storing to online or I started an

00:16:10,820 --> 00:16:14,840
offline I'm restoring to both and you

00:16:13,310 --> 00:16:16,910
can imagine the complexity of writing

00:16:14,840 --> 00:16:19,010
your spark streaming app or your spark

00:16:16,910 --> 00:16:21,380
batch app and it's got to like to your

00:16:19,010 --> 00:16:22,970
online database and it's also going to

00:16:21,380 --> 00:16:25,190
write your offline database so our

00:16:22,970 --> 00:16:27,590
online database is my sequel cluster or

00:16:25,190 --> 00:16:29,900
NDB and are offline database is Apache

00:16:27,590 --> 00:16:32,960
hive and they share the same metadata

00:16:29,900 --> 00:16:35,720
layer in our platform now what we did to

00:16:32,960 --> 00:16:38,660
make this very complex architecture

00:16:35,720 --> 00:16:40,340
simpler it's kind of lambda architecture

00:16:38,660 --> 00:16:42,770
like but we have introduced a data frame

00:16:40,340 --> 00:16:45,800
API so what the data frame API gives us

00:16:42,770 --> 00:16:47,180
is the ability for our applications so

00:16:45,800 --> 00:16:49,160
we have spark streaming apps here for

00:16:47,180 --> 00:16:50,750
example and they're gonna just save

00:16:49,160 --> 00:16:52,700
their data frames to the feature store

00:16:50,750 --> 00:16:54,590
and our API gives them that simplicity

00:16:52,700 --> 00:16:56,360
they don't need to worry about whether

00:16:54,590 --> 00:16:57,830
it's going online offline they just set

00:16:56,360 --> 00:16:59,810
a flag when they create a feature group

00:16:57,830 --> 00:17:01,640
and say hey this is going to both online

00:16:59,810 --> 00:17:05,660
and offline or this is just going to

00:17:01,640 --> 00:17:07,280
offline and the data frame API doesn't

00:17:05,660 --> 00:17:10,940
just work for spark it also works for

00:17:07,280 --> 00:17:13,880
pandas Python data friends it will

00:17:10,940 --> 00:17:16,430
convert them to power to park' files and

00:17:13,880 --> 00:17:18,680
ultimately offline data will be stored

00:17:16,430 --> 00:17:22,160
as park' and external tables and hive

00:17:18,680 --> 00:17:24,920
with OD if you decide to use woody and

00:17:22,160 --> 00:17:28,040
then the the online feature store will

00:17:24,920 --> 00:17:30,770
have tables in my sequel cluster for use

00:17:28,040 --> 00:17:32,360
cases where we can't we don't have

00:17:30,770 --> 00:17:33,830
enough time to push the data at the

00:17:32,360 --> 00:17:36,140
feature store and pull it out at the

00:17:33,830 --> 00:17:38,540
online application and we count on our

00:17:36,140 --> 00:17:41,330
platform support link and I think can

00:17:38,540 --> 00:17:43,130
read data from an account of the topic

00:17:41,330 --> 00:17:44,630
that's input and then write the output

00:17:43,130 --> 00:17:46,430
to another Kafka topic and the other

00:17:44,630 --> 00:17:48,950
line lock can pull it and we can do

00:17:46,430 --> 00:17:54,470
we're doing that for a two second use

00:17:48,950 --> 00:17:56,930
case so so what we introduced here was

00:17:54,470 --> 00:17:58,610
this this notion of the data frame API

00:17:56,930 --> 00:18:00,350
to simplify the process of ingesting

00:17:58,610 --> 00:18:02,570
data so let's have a look at the code

00:18:00,350 --> 00:18:04,730
and there's a very simple example we

00:18:02,570 --> 00:18:05,040
were going to have a data frame that's

00:18:04,730 --> 00:18:07,590
going to

00:18:05,040 --> 00:18:08,670
data from a back-end system and then

00:18:07,590 --> 00:18:11,700
we're going to do some feature end

00:18:08,670 --> 00:18:13,050
engineering on that data frame we saw an

00:18:11,700 --> 00:18:14,580
example earlier and then we create a

00:18:13,050 --> 00:18:16,680
feature group if we want to make it

00:18:14,580 --> 00:18:19,280
online we just set online equals true

00:18:16,680 --> 00:18:21,900
here and it will synchronize that

00:18:19,280 --> 00:18:25,740
updates to that data frame to both the

00:18:21,900 --> 00:18:27,780
online and offline feature stores so

00:18:25,740 --> 00:18:29,580
let's go ahead so yeah we've covered

00:18:27,780 --> 00:18:31,890
this part here this is the ingestion of

00:18:29,580 --> 00:18:34,440
data into the PQ store that's a quick

00:18:31,890 --> 00:18:37,970
look at online applications and then how

00:18:34,440 --> 00:18:40,140
we create training path data sets oh

00:18:37,970 --> 00:18:43,320
creating training cuts data sets we'll

00:18:40,140 --> 00:18:46,350
start with that one so our platform uses

00:18:43,320 --> 00:18:48,180
at spark to to join the feature groups

00:18:46,350 --> 00:18:50,040
together so if you have features that

00:18:48,180 --> 00:18:51,990
that come from several different feature

00:18:50,040 --> 00:18:54,420
groups and your code might look

00:18:51,990 --> 00:18:56,040
something like this FS get features and

00:18:54,420 --> 00:18:57,990
in the flat namespace case we just put

00:18:56,040 --> 00:19:00,150
in the names of the features and it will

00:18:57,990 --> 00:19:01,560
return a data frame so what it's

00:19:00,150 --> 00:19:04,590
actually doing is it's running a spark

00:19:01,560 --> 00:19:07,440
application to read up these data frames

00:19:04,590 --> 00:19:09,300
join them together and using the common

00:19:07,440 --> 00:19:11,430
join key so if our creep on it will look

00:19:09,300 --> 00:19:13,770
for the largest overlapping set of

00:19:11,430 --> 00:19:15,570
primary keys between the two data frames

00:19:13,770 --> 00:19:17,130
involved and if you've multiple data

00:19:15,570 --> 00:19:19,890
frames then it's going to look between

00:19:17,130 --> 00:19:22,200
the joints it will again examine the

00:19:19,890 --> 00:19:24,510
largest overlapping set of join keys or

00:19:22,200 --> 00:19:26,760
primary keys now this spark application

00:19:24,510 --> 00:19:29,640
can run on our platform mouse works or

00:19:26,760 --> 00:19:31,320
it can run on data breaks for example

00:19:29,640 --> 00:19:35,730
next any external spark cluster will

00:19:31,320 --> 00:19:38,520
work and it will that spark application

00:19:35,730 --> 00:19:40,170
will then materialize the training test

00:19:38,520 --> 00:19:42,060
data to wherever you want it to be

00:19:40,170 --> 00:19:44,070
stored and this is what it looks like in

00:19:42,060 --> 00:19:47,460
code this is a slightly more complete

00:19:44,070 --> 00:19:49,290
example and in this example we get back

00:19:47,460 --> 00:19:51,990
this data frame as the join of all of

00:19:49,290 --> 00:19:53,610
the features and we just say hey I'd

00:19:51,990 --> 00:19:56,160
like this data to be stored as TF

00:19:53,610 --> 00:19:58,080
records and we have another parameter

00:19:56,160 --> 00:20:01,050
here called connector and the connector

00:19:58,080 --> 00:20:04,080
can point to an s3 bucket or to our file

00:20:01,050 --> 00:20:05,850
system and hop so fast and everything is

00:20:04,080 --> 00:20:07,830
version so you can specify versions if

00:20:05,850 --> 00:20:09,390
you want you can get latest versions if

00:20:07,830 --> 00:20:12,450
you want to increment the version of a

00:20:09,390 --> 00:20:15,090
training test test dataset so it's

00:20:12,450 --> 00:20:15,840
basically yes now we have updated this

00:20:15,090 --> 00:20:18,240
API

00:20:15,840 --> 00:20:18,780
somewhat and to hardscope so I'm going

00:20:18,240 --> 00:20:22,470
to show you how

00:20:18,780 --> 00:20:24,120
we do this so in the newer version that

00:20:22,470 --> 00:20:26,190
will be released this month it's not not

00:20:24,120 --> 00:20:28,320
that yes and we get the feature groups

00:20:26,190 --> 00:20:29,940
so we need to know them where where the

00:20:28,320 --> 00:20:31,680
features come from what feature groups

00:20:29,940 --> 00:20:34,560
so they come from so we get the Future

00:20:31,680 --> 00:20:36,540
groups and it's good practice to specify

00:20:34,560 --> 00:20:39,060
the version to make product like proper

00:20:36,540 --> 00:20:41,580
production code now what you can do it

00:20:39,060 --> 00:20:43,680
to try and use our query planner again

00:20:41,580 --> 00:20:45,750
as you can just say in a very pythonic

00:20:43,680 --> 00:20:47,370
way select all the features from the

00:20:45,750 --> 00:20:48,890
first feature group and join them with

00:20:47,370 --> 00:20:52,050
all the features from the second feature

00:20:48,890 --> 00:20:55,830
and this data frame that you get return

00:20:52,050 --> 00:20:58,500
join features we we can then save it so

00:20:55,830 --> 00:21:00,810
in this case when we now have an extra

00:20:58,500 --> 00:21:03,180
line because the training data object is

00:21:00,810 --> 00:21:04,800
a metadata object it's a lazy object

00:21:03,180 --> 00:21:06,720
that we use to create the training data

00:21:04,800 --> 00:21:08,520
set so at this point there's no training

00:21:06,720 --> 00:21:10,560
data has been materialized it's only

00:21:08,520 --> 00:21:13,530
after we make the last line called TD

00:21:10,560 --> 00:21:15,030
that save on the data frame that we got

00:21:13,530 --> 00:21:16,980
back from the feature groups that we

00:21:15,030 --> 00:21:18,300
joined together it's only at this point

00:21:16,980 --> 00:21:21,120
then here where we're actually going to

00:21:18,300 --> 00:21:22,680
save the date train data to a file

00:21:21,120 --> 00:21:25,680
system and in this case we're saving it

00:21:22,680 --> 00:21:27,840
to s3 to a bucket and that's specified

00:21:25,680 --> 00:21:30,210
here so some other improvements we have

00:21:27,840 --> 00:21:32,490
apart from the things we have before us

00:21:30,210 --> 00:21:35,160
which is that the file format things

00:21:32,490 --> 00:21:38,520
like numpy you know peda storm are

00:21:35,160 --> 00:21:40,980
popular CSV or popular file formats but

00:21:38,520 --> 00:21:42,960
it's nice and easy now to just create

00:21:40,980 --> 00:21:45,180
the split so your training day will be

00:21:42,960 --> 00:21:47,460
70% to the data 20 and the data will be

00:21:45,180 --> 00:21:50,250
tested and 10% will be validation data

00:21:47,460 --> 00:21:53,700
it just makes it a little bit easier I'm

00:21:50,250 --> 00:21:56,910
using this new API now we support

00:21:53,700 --> 00:21:58,560
time-travel queries for creating these

00:21:56,910 --> 00:22:00,000
training data sets you can say hey I'd

00:21:58,560 --> 00:22:01,800
like to have the training data for these

00:22:00,000 --> 00:22:04,530
feature groups as the Train data looked

00:22:01,800 --> 00:22:06,360
like at this moment in time or you can

00:22:04,530 --> 00:22:09,180
say for example give me the training

00:22:06,360 --> 00:22:11,220
data that or that the data the features

00:22:09,180 --> 00:22:13,140
that arrived in a feature group between

00:22:11,220 --> 00:22:15,810
a time interval between a start point

00:22:13,140 --> 00:22:17,250
and an end point in time so the way it

00:22:15,810 --> 00:22:21,300
works is it's using Apache hoodie

00:22:17,250 --> 00:22:24,360
underneath the covers to basically get

00:22:21,300 --> 00:22:25,860
the changes in a feature group in a

00:22:24,360 --> 00:22:27,150
particular interval and this is the

00:22:25,860 --> 00:22:29,670
interval case where we're going to get

00:22:27,150 --> 00:22:31,410
the bank data as it look at the bank

00:22:29,670 --> 00:22:32,440
data that arrived between the 1st of

00:22:31,410 --> 00:22:35,500
January 1912

00:22:32,440 --> 00:22:36,850
than the fourth 14th of April and in the

00:22:35,500 --> 00:22:39,130
first case we're going to get this

00:22:36,850 --> 00:22:41,020
feature group as it looked like at this

00:22:39,130 --> 00:22:43,300
particular moment in time to this point

00:22:41,020 --> 00:22:46,000
of times we can secretly have this

00:22:43,300 --> 00:22:48,930
keyword house up and that's what we have

00:22:46,000 --> 00:22:48,930
effectively here

00:22:49,260 --> 00:22:54,730
is very similar to Delta link by

00:22:52,660 --> 00:22:56,680
barracks and it provides atomic and

00:22:54,730 --> 00:23:00,040
incremental updates of feature groups

00:22:56,680 --> 00:23:01,690
which is really nice and there's a

00:23:00,040 --> 00:23:04,030
hoodie talk I think out Thursday which

00:23:01,690 --> 00:23:06,070
you might want to look into so how does

00:23:04,030 --> 00:23:07,630
an online feature story use this or

00:23:06,070 --> 00:23:09,940
online applications use this online

00:23:07,630 --> 00:23:13,120
feature store well if you have an online

00:23:09,940 --> 00:23:14,860
application that we have here what it

00:23:13,120 --> 00:23:16,390
wants to do firstly that says well I'm

00:23:14,860 --> 00:23:17,770
missing a bunch of features so I need to

00:23:16,390 --> 00:23:19,000
go to the feature store to guess the

00:23:17,770 --> 00:23:20,680
features that I'm missing things like

00:23:19,000 --> 00:23:22,650
historical information about what the

00:23:20,680 --> 00:23:25,600
user has done maybe their credit history

00:23:22,650 --> 00:23:27,970
anything that's been computed offline

00:23:25,600 --> 00:23:30,670
and materialized to the online feature

00:23:27,970 --> 00:23:32,290
store we can do so what the online

00:23:30,670 --> 00:23:33,850
application can do and this can be done

00:23:32,290 --> 00:23:35,890
within the module as well but basically

00:23:33,850 --> 00:23:39,790
they make a query to the feature store

00:23:35,890 --> 00:23:41,350
and say hey I have the I have the keys

00:23:39,790 --> 00:23:43,120
the primary keys for this particular

00:23:41,350 --> 00:23:45,130
training data set so I have the primary

00:23:43,120 --> 00:23:46,930
keys for the feature groups that were

00:23:45,130 --> 00:23:49,690
joined together to make up this training

00:23:46,930 --> 00:23:53,280
dataset can you please give me back a

00:23:49,690 --> 00:23:56,290
feature vector so that basically does a

00:23:53,280 --> 00:23:58,030
a bunch of primary key lookups and joins

00:23:56,290 --> 00:24:00,880
them together in the online feature

00:23:58,030 --> 00:24:03,160
storm icicle cluster when you start up

00:24:00,880 --> 00:24:06,160
the app you typically will will get that

00:24:03,160 --> 00:24:07,480
query back as a cached prepared

00:24:06,160 --> 00:24:10,660
statement and then you just make the

00:24:07,480 --> 00:24:11,980
it'll make the query for you directly on

00:24:10,660 --> 00:24:13,720
the database you know there's not a two

00:24:11,980 --> 00:24:17,020
two round-trips just a single round trip

00:24:13,720 --> 00:24:18,610
to get that feature vector so what yeah

00:24:17,020 --> 00:24:20,980
app once you have the feature vector you

00:24:18,610 --> 00:24:23,530
go to your model and typically we don't

00:24:20,980 --> 00:24:25,900
serve models embedded inside the

00:24:23,530 --> 00:24:28,570
application if it's tensorflow serving

00:24:25,900 --> 00:24:29,980
server we would deploy the model on that

00:24:28,570 --> 00:24:30,550
test was our server typically in

00:24:29,980 --> 00:24:33,070
kubernetes

00:24:30,550 --> 00:24:35,140
and then it would to be replicated

00:24:33,070 --> 00:24:36,700
across different availability zones for

00:24:35,140 --> 00:24:39,430
example in the cloud and our database

00:24:36,700 --> 00:24:41,560
our online database is a che it's

00:24:39,430 --> 00:24:43,660
transactional and it supports high

00:24:41,560 --> 00:24:44,550
availability cross availability zones in

00:24:43,660 --> 00:24:45,639
the Clio

00:24:44,550 --> 00:24:47,589
so

00:24:45,639 --> 00:24:48,909
a little bit more detail on how that

00:24:47,589 --> 00:24:52,149
works internally our online application

00:24:48,909 --> 00:24:55,119
has it's received a number of ids entity

00:24:52,149 --> 00:24:57,219
IDs if it's a after all example you

00:24:55,119 --> 00:24:58,779
might have the idea of the order the

00:24:57,219 --> 00:25:00,279
customer has involved the amount of

00:24:58,779 --> 00:25:04,329
money the product ID and things like

00:25:00,279 --> 00:25:06,399
that with those ID IDs and our keys as

00:25:04,329 --> 00:25:08,950
we're calling them here and what we can

00:25:06,399 --> 00:25:11,139
do is we can go to the feature store and

00:25:08,950 --> 00:25:13,059
say hey I want to prepared statements to

00:25:11,139 --> 00:25:15,070
look up the feature vector with these

00:25:13,059 --> 00:25:17,169
keys so once it's got that prepared

00:25:15,070 --> 00:25:19,119
statement it can cache it locally the

00:25:17,169 --> 00:25:21,070
other application and then it can make

00:25:19,119 --> 00:25:23,979
this query on the other line PQ store

00:25:21,070 --> 00:25:25,509
with that set of keys and then once it's

00:25:23,979 --> 00:25:28,450
got back the feature vector it can then

00:25:25,509 --> 00:25:31,149
make the predictions on the model so

00:25:28,450 --> 00:25:33,279
this is part of hops works like I said

00:25:31,149 --> 00:25:37,450
it's a modular platform and the feature

00:25:33,279 --> 00:25:39,070
store itself it's a it's here and it

00:25:37,450 --> 00:25:43,209
needs the filesystem

00:25:39,070 --> 00:25:44,829
it has a hive and my seagull Buster you

00:25:43,209 --> 00:25:46,149
can run the whole platform so you can do

00:25:44,829 --> 00:25:48,309
your feature engineering here and your

00:25:46,149 --> 00:25:49,509
model training here on a platform you

00:25:48,309 --> 00:25:51,940
can even do a model serving and

00:25:49,509 --> 00:25:54,039
monitoring if you want and optionally

00:25:51,940 --> 00:25:56,349
katka is included in the platform as

00:25:54,039 --> 00:25:59,259
well so it's a it's a pretty complete

00:25:56,349 --> 00:26:01,869
platform for doing not just model

00:25:59,259 --> 00:26:03,070
training but also of data engineering we

00:26:01,869 --> 00:26:04,869
can see here the weave in of air flow

00:26:03,070 --> 00:26:08,379
and I'll show a quick demo on that as

00:26:04,869 --> 00:26:09,669
well in a minute so here's what Manto

00:26:08,379 --> 00:26:11,139
and pipeline looks like and it's

00:26:09,669 --> 00:26:12,849
remember it's not an end-to-end pipeline

00:26:11,139 --> 00:26:15,039
because the pipeline will will start

00:26:12,849 --> 00:26:16,539
here at the back end and end the first

00:26:15,039 --> 00:26:17,859
pipeline will start there and the second

00:26:16,539 --> 00:26:22,479
pipeline will be training a little

00:26:17,859 --> 00:26:23,859
appear after so that's a quick look we

00:26:22,479 --> 00:26:25,719
have feature engineering that happens

00:26:23,859 --> 00:26:27,969
first and once the features are in the

00:26:25,719 --> 00:26:31,629
features store then the data scientist

00:26:27,969 --> 00:26:34,059
can can basically do all of that we can

00:26:31,629 --> 00:26:35,859
see here these stats that they can train

00:26:34,059 --> 00:26:37,629
models and and it's an experiment it's

00:26:35,859 --> 00:26:39,879
an haider the process you get features

00:26:37,629 --> 00:26:42,700
you get training data you train models

00:26:39,879 --> 00:26:44,709
you analyze your models validate them

00:26:42,700 --> 00:26:47,019
deploy them to a model repository and

00:26:44,709 --> 00:26:49,570
then applications patch application and

00:26:47,019 --> 00:26:51,549
use them directly if you have online or

00:26:49,570 --> 00:26:53,259
operational models you then deploy that

00:26:51,549 --> 00:26:55,089
model to a model sparing server and

00:26:53,259 --> 00:26:57,190
online applications can use them you

00:26:55,089 --> 00:26:59,410
don't mind applications or the monster

00:26:57,190 --> 00:27:02,020
itself can get the feature actors

00:26:59,410 --> 00:27:03,640
as it needs them to build the whole

00:27:02,020 --> 00:27:05,770
feature vector that he used to serve the

00:27:03,640 --> 00:27:07,690
models we do model monitoring by

00:27:05,770 --> 00:27:09,970
actually taking the predictions that are

00:27:07,690 --> 00:27:13,870
made and logging into Kafka and then we

00:27:09,970 --> 00:27:15,700
can use spark streaming typically to to

00:27:13,870 --> 00:27:18,700
monitor those predictions to make sure

00:27:15,700 --> 00:27:21,070
that the incoming features don't

00:27:18,700 --> 00:27:23,350
diverged significantly from the features

00:27:21,070 --> 00:27:24,730
that we trained the model on and the way

00:27:23,350 --> 00:27:26,200
we do that is we actually go to the

00:27:24,730 --> 00:27:27,820
online feature store and say hey give me

00:27:26,200 --> 00:27:29,650
the statistics and descriptive

00:27:27,820 --> 00:27:31,809
statistics for this training data set

00:27:29,650 --> 00:27:33,789
and then we can compare those with the

00:27:31,809 --> 00:27:35,890
ones that are being computed on live

00:27:33,789 --> 00:27:37,900
data so we use Windows to compute that

00:27:35,890 --> 00:27:40,690
you know the mean the standard deviation

00:27:37,900 --> 00:27:43,539
max min forgiven features and then we

00:27:40,690 --> 00:27:47,530
can automatically pair those with the

00:27:43,539 --> 00:27:50,679
training statistics and then notify if

00:27:47,530 --> 00:27:52,659
there's a problem so a platform is a bit

00:27:50,679 --> 00:27:55,030
more than this we do multi worker

00:27:52,659 --> 00:27:57,700
training I'm going to maybe show Maggie

00:27:55,030 --> 00:27:59,679
which is awaiting her parameter

00:27:57,700 --> 00:28:01,330
optimization but it's just part of the

00:27:59,679 --> 00:28:04,090
example and then we also do

00:28:01,330 --> 00:28:06,280
project-based multi-tenancy so you can

00:28:04,090 --> 00:28:09,190
have sensitive data in a shared cluster

00:28:06,280 --> 00:28:11,320
and there is also support for provenance

00:28:09,190 --> 00:28:13,900
and similar to T effects and M up flow

00:28:11,320 --> 00:28:15,190
and what we do it implicitly and I won't

00:28:13,900 --> 00:28:17,679
have time to go through that in this

00:28:15,190 --> 00:28:19,179
talk I'm going to do a demo in a second

00:28:17,679 --> 00:28:20,770
but I just tell you how you can try out

00:28:19,179 --> 00:28:23,530
the platform it is open source there's a

00:28:20,770 --> 00:28:25,150
community version there's an installer

00:28:23,530 --> 00:28:27,309
for it if you go to our documentation

00:28:25,150 --> 00:28:30,070
just google help search documentation

00:28:27,309 --> 00:28:32,500
you'll find us if you're interested in

00:28:30,070 --> 00:28:34,210
trying it the enterprise version you can

00:28:32,500 --> 00:28:36,640
go to the manage version we have a SAS

00:28:34,210 --> 00:28:38,620
version on AWS cooperate study I and

00:28:36,640 --> 00:28:42,450
then we also have an Enterprise version

00:28:38,620 --> 00:28:46,570
that can be installed on premise or in

00:28:42,450 --> 00:28:48,730
the cloud so an azure and GCP as well so

00:28:46,570 --> 00:28:50,049
I'm gonna do a demo and I give this go

00:28:48,730 --> 00:28:52,059
through a demo where we have a churn

00:28:50,049 --> 00:28:54,789
model we're computing a turn model for

00:28:52,059 --> 00:28:57,070
telecom users this data set came from a

00:28:54,789 --> 00:28:59,049
Kaggle competition but we're gonna

00:28:57,070 --> 00:29:01,929
create a feature group from a this

00:28:59,049 --> 00:29:03,549
original raw data we're gonna create

00:29:01,929 --> 00:29:05,230
some training data from the features and

00:29:03,549 --> 00:29:07,419
that we've added the feature store

00:29:05,230 --> 00:29:09,040
trainer model using it and that's kind

00:29:07,419 --> 00:29:12,270
of as far as we'll get in this example

00:29:09,040 --> 00:29:14,010
you can extend this then to do online

00:29:12,270 --> 00:29:18,570
future servants and it's going to skip

00:29:14,010 --> 00:29:19,890
to the my browser I'm not going to show

00:29:18,570 --> 00:29:22,290
you how to get started and hubscher it's

00:29:19,890 --> 00:29:23,940
AI but this is what I'm using and you

00:29:22,290 --> 00:29:28,950
can create an account er you need to

00:29:23,940 --> 00:29:30,720
link your AWS account to to enable hop

00:29:28,950 --> 00:29:32,550
source AI to launch a cluster in your

00:29:30,720 --> 00:29:34,080
account so you just create click once

00:29:32,550 --> 00:29:35,640
you've done I can click create cluster

00:29:34,080 --> 00:29:38,820
I've run this one already

00:29:35,640 --> 00:29:40,350
now that's this one here I'll just show

00:29:38,820 --> 00:29:43,620
you so when when it comes up you'll see

00:29:40,350 --> 00:29:45,000
basically this and you can log in now

00:29:43,620 --> 00:29:46,020
I've created a project already but I

00:29:45,000 --> 00:29:47,130
have with a couple of minutes so I'll

00:29:46,020 --> 00:29:51,390
just show you how to create a new

00:29:47,130 --> 00:29:54,270
project I'll call this one Telecom

00:29:51,390 --> 00:29:56,460
a project is I mentioned already it's a

00:29:54,270 --> 00:29:58,650
it's a kind of a sandbox we have this

00:29:56,460 --> 00:30:02,820
project based multi-tenancy model so all

00:29:58,650 --> 00:30:04,200
data and users and programs inside this

00:30:02,820 --> 00:30:05,970
project kind of like the gift of

00:30:04,200 --> 00:30:08,400
repository and they're going to be

00:30:05,970 --> 00:30:10,679
private so if I give someone access to

00:30:08,400 --> 00:30:13,020
the project as a in a role called data

00:30:10,679 --> 00:30:15,090
scientist then they can't copy data out

00:30:13,020 --> 00:30:16,860
they can't even read data from other

00:30:15,090 --> 00:30:18,570
projects that are a member of so it's

00:30:16,860 --> 00:30:20,490
it's really you're really restricting

00:30:18,570 --> 00:30:22,650
them inside here so I created this

00:30:20,490 --> 00:30:25,350
project and the example I'm going to go

00:30:22,650 --> 00:30:30,750
through is here at Jim darling slash

00:30:25,350 --> 00:30:32,220
turn on github and well I can just do to

00:30:30,750 --> 00:30:35,610
get started and get installed a bunch of

00:30:32,220 --> 00:30:37,850
Python libraries so in this platform the

00:30:35,610 --> 00:30:40,050
way install python libraries is you just

00:30:37,850 --> 00:30:42,270
use calendar or pip

00:30:40,050 --> 00:30:44,520
so off you go and it'll take a couple of

00:30:42,270 --> 00:30:48,179
seconds at the example actually it's

00:30:44,520 --> 00:30:51,480
going to be it's some raw telecom data

00:30:48,179 --> 00:30:53,640
but I'm going to use XG boost to train

00:30:51,480 --> 00:30:55,440
the model we'd support next you boostin

00:30:53,640 --> 00:30:58,370
scikit-learn cantaloupe i torch there

00:30:55,440 --> 00:31:01,890
the pretty common ones the common

00:30:58,370 --> 00:31:03,090
frameworks that people use take a couple

00:31:01,890 --> 00:31:04,710
of seconds you can actually also

00:31:03,090 --> 00:31:06,390
important to calm the file here instead

00:31:04,710 --> 00:31:08,640
we're going slightly faster but i

00:31:06,390 --> 00:31:10,890
thought i'll do it this way so we have

00:31:08,640 --> 00:31:13,380
hip here as well and i can just search

00:31:10,890 --> 00:31:14,730
you down so what makes it slightly

00:31:13,380 --> 00:31:16,590
different than that will actually build

00:31:14,730 --> 00:31:19,890
a docker file in the background from

00:31:16,590 --> 00:31:21,510
your kondeh environment file here now

00:31:19,890 --> 00:31:22,950
that's kind of nice because data

00:31:21,510 --> 00:31:24,710
scientist then don't need to actually

00:31:22,950 --> 00:31:27,839
write docker files

00:31:24,710 --> 00:31:27,839
[Music]

00:31:27,909 --> 00:31:32,740
the last layer here at physical

00:31:29,409 --> 00:31:35,139
imbalance learn there we go

00:31:32,740 --> 00:31:36,789
okay so they're gonna start installing

00:31:35,139 --> 00:31:39,760
them this is what we get in our base

00:31:36,789 --> 00:31:42,039
environment what I'm gonna do is there's

00:31:39,760 --> 00:31:43,840
no feature store here currently we have

00:31:42,039 --> 00:31:46,450
feature group strain data sets feature

00:31:43,840 --> 00:31:49,179
search on food store details I'm just

00:31:46,450 --> 00:31:51,370
gonna open a notebook and in the

00:31:49,179 --> 00:31:54,639
notebook I think this should be okay I'm

00:31:51,370 --> 00:32:00,010
going to put in my kit with repo which

00:31:54,639 --> 00:32:02,409
was here copied us and I added an API

00:32:00,010 --> 00:32:04,690
key already so we enabled it that's

00:32:02,409 --> 00:32:07,389
pretty good idea and this is a kid have

00:32:04,690 --> 00:32:08,080
repo I have an API key upload it to hops

00:32:07,389 --> 00:32:13,630
right so you can see in our

00:32:08,080 --> 00:32:20,500
documentation I had to do that today

00:32:13,630 --> 00:32:25,320
upload turn and be ok it doesn't look

00:32:20,500 --> 00:32:41,649
like one of those okay so a quick look

00:32:25,320 --> 00:32:50,139
secrets okay let's go back to Nate look

00:32:41,649 --> 00:32:53,889
it again there we go and I'm gonna just

00:32:50,139 --> 00:32:56,049
deploy this from master so it takes a

00:32:53,889 --> 00:32:57,639
second to go get up and find us okay so

00:32:56,049 --> 00:33:00,279
in this case it's actually going to run

00:32:57,639 --> 00:33:02,580
a PI spark application I'm gonna give it

00:33:00,279 --> 00:33:05,049
two gigs to the driver two gigs to the

00:33:02,580 --> 00:33:08,230
executor and it's just a single CPU it's

00:33:05,049 --> 00:33:13,840
not a huge amount of resources and this

00:33:08,230 --> 00:33:14,830
is something called and hit lab and we

00:33:13,840 --> 00:33:20,019
can see here there's a number of

00:33:14,830 --> 00:33:22,389
notebooks we have this this is our data

00:33:20,019 --> 00:33:24,100
set here and this is what you'll find a

00:33:22,389 --> 00:33:25,750
geared up so what I'm going to do is a

00:33:24,100 --> 00:33:31,059
couple of things I'm going to copy this

00:33:25,750 --> 00:33:33,100
data set into hops works we could run

00:33:31,059 --> 00:33:34,539
with it locally one I'm gonna make a job

00:33:33,100 --> 00:33:37,720
out of this later on so I'm just going

00:33:34,539 --> 00:33:39,340
to go through this so let's not blow

00:33:37,720 --> 00:33:45,010
let's create a folder first

00:33:39,340 --> 00:33:47,170
I gotta call the full return and I'm

00:33:45,010 --> 00:33:50,230
just gonna we're gonna yeah I'm gonna

00:33:47,170 --> 00:33:53,200
upload my I some of the code from from

00:33:50,230 --> 00:33:57,070
github - I got about everything in here

00:33:53,200 --> 00:33:58,810
actually - two hops work so this is

00:33:57,070 --> 00:34:05,710
uploading data to the file system so I

00:33:58,810 --> 00:34:08,169
got this copy from local star - the path

00:34:05,710 --> 00:34:10,720
in HDFS it just copies this data and it

00:34:08,169 --> 00:34:12,130
maintains the same username so if we go

00:34:10,720 --> 00:34:14,590
back here we'll see that the date has

00:34:12,130 --> 00:34:17,110
been copied oh now you cannot probably

00:34:14,590 --> 00:34:19,480
you know attach the community ID if you

00:34:17,110 --> 00:34:21,909
want to get to notice which version of

00:34:19,480 --> 00:34:23,590
the repo it is so I'm going to go back

00:34:21,909 --> 00:34:25,000
and I'm going to actually then run the

00:34:23,590 --> 00:34:28,810
first notebook which is to create a

00:34:25,000 --> 00:34:30,370
feature group let's actually before we

00:34:28,810 --> 00:34:35,340
do that let's check to make sure that

00:34:30,370 --> 00:34:37,780
everything has been installed yes it has

00:34:35,340 --> 00:34:39,730
and then the actually before we do that

00:34:37,780 --> 00:34:43,570
I'm going to close this notebook and I'm

00:34:39,730 --> 00:34:45,429
going to go back for one second and show

00:34:43,570 --> 00:34:48,070
you the feature store because I notice

00:34:45,429 --> 00:34:49,990
I'm running out of time so we can this

00:34:48,070 --> 00:34:53,290
is a feature store that I've already run

00:34:49,990 --> 00:34:55,750
and this one is what you'll get when you

00:34:53,290 --> 00:34:59,590
run the tour and you can see we have a

00:34:55,750 --> 00:35:02,020
bunch of feature groups here and we have

00:34:59,590 --> 00:35:04,330
the ability to look up statistics on the

00:35:02,020 --> 00:35:06,670
future groups so when you ingest a

00:35:04,330 --> 00:35:08,440
feature group it will automatically by

00:35:06,670 --> 00:35:10,960
default it will compute these statistics

00:35:08,440 --> 00:35:12,790
so things like clustering analysis we

00:35:10,960 --> 00:35:16,030
have correlations and distributions of

00:35:12,790 --> 00:35:18,700
individual features so this is nice for

00:35:16,030 --> 00:35:20,740
exploratory data analysis you there's a

00:35:18,700 --> 00:35:22,810
UI here for creating training data sets

00:35:20,740 --> 00:35:28,180
by selecting features I'm gonna skip

00:35:22,810 --> 00:35:30,340
that and we have individual features

00:35:28,180 --> 00:35:32,200
here so the overview the feature story

00:35:30,340 --> 00:35:34,930
here you can see there's 24 features six

00:35:32,200 --> 00:35:36,690
feet two groups one training dataset and

00:35:34,930 --> 00:35:38,950
[Music]

00:35:36,690 --> 00:35:41,190
yeah so that's a basic overview the

00:35:38,950 --> 00:35:43,720
feature you can search for features here

00:35:41,190 --> 00:35:45,280
and we can see here that we can search

00:35:43,720 --> 00:35:46,930
of features across the different feature

00:35:45,280 --> 00:35:49,680
stores in fact I think I might have a

00:35:46,930 --> 00:35:52,569
minute to just run this one example I

00:35:49,680 --> 00:35:53,979
just skipped through it and

00:35:52,569 --> 00:35:58,019
the there's it there's a bunch of

00:35:53,979 --> 00:36:01,869
different it's building back in Louis

00:35:58,019 --> 00:36:04,150
okay so I'm just gonna run this here run

00:36:01,869 --> 00:36:09,579
ourselves so this is a spark application

00:36:04,150 --> 00:36:12,489
and what it's doing is it's reading up

00:36:09,579 --> 00:36:14,709
some data from from our HDFS so I copied

00:36:12,489 --> 00:36:17,339
this data in I can copy it here I copied

00:36:14,709 --> 00:36:20,199
it into churn so we need to change that

00:36:17,339 --> 00:36:23,709
resources churn okay that's the correct

00:36:20,199 --> 00:36:25,390
path and it's going to do some feature

00:36:23,709 --> 00:36:26,949
engineering so this is the feature

00:36:25,390 --> 00:36:28,179
engineering you can look at it and then

00:36:26,949 --> 00:36:30,699
it's going to create the feature group

00:36:28,179 --> 00:36:32,799
so once it's created a feature group

00:36:30,699 --> 00:36:35,679
what it's running the spark application

00:36:32,799 --> 00:36:37,239
here now we can look at that when it

00:36:35,679 --> 00:36:41,739
starts we can see it just as a single

00:36:37,239 --> 00:36:43,299
executor and if we go back we can see

00:36:41,739 --> 00:36:46,689
what it look like when it finished so I

00:36:43,299 --> 00:36:48,249
run this already and when it finishes

00:36:46,689 --> 00:36:51,699
you'll get this feature group appearing

00:36:48,249 --> 00:36:53,559
in here now it computes statistics on it

00:36:51,699 --> 00:36:54,849
we saw that already we can get a preview

00:36:53,559 --> 00:36:55,859
of the data as well which is kind of

00:36:54,849 --> 00:36:59,109
nice

00:36:55,859 --> 00:37:00,729
that's this one so like as a data

00:36:59,109 --> 00:37:05,109
scientist you often want to preview data

00:37:00,729 --> 00:37:09,459
very quickly what I what I also did in

00:37:05,109 --> 00:37:11,229
this case is I you can also take those

00:37:09,459 --> 00:37:12,609
notebooks and turn them into jobs so

00:37:11,229 --> 00:37:16,329
that's originally where I created them

00:37:12,609 --> 00:37:17,859
as I copied them to HDFS because we can

00:37:16,329 --> 00:37:20,939
create a job from those notebooks and

00:37:17,859 --> 00:37:23,409
then from those notebooks we have a

00:37:20,939 --> 00:37:25,390
every notebook can be turned individual

00:37:23,409 --> 00:37:26,890
job and we can actually chain them

00:37:25,390 --> 00:37:28,779
together in air flow and I'm just going

00:37:26,890 --> 00:37:30,819
to show that it's the last thing I do

00:37:28,779 --> 00:37:33,849
before I finish so you can have for

00:37:30,819 --> 00:37:38,529
example your training pipeline look like

00:37:33,849 --> 00:37:40,349
this we can add a two notebooks for

00:37:38,529 --> 00:37:42,279
example we could say the first one is

00:37:40,349 --> 00:37:45,489
we're going to compute the hyper

00:37:42,279 --> 00:37:48,069
parameters and save that and then in the

00:37:45,489 --> 00:37:50,499
second notebook we are going to wait for

00:37:48,069 --> 00:37:52,929
the hyper parameters to be computed and

00:37:50,499 --> 00:37:55,269
they're going to train so we computed

00:37:52,929 --> 00:37:56,409
good height parameters once we get good

00:37:55,269 --> 00:37:58,499
how clear parameters you can pass them

00:37:56,409 --> 00:38:02,439
on to training so this this particular

00:37:58,499 --> 00:38:06,280
notebook that I just created which was

00:38:02,439 --> 00:38:07,840
one of these here this is a a a

00:38:06,280 --> 00:38:11,920
it's not a notebook it's a it's a dag

00:38:07,840 --> 00:38:13,240
it's a die in an air flow we can

00:38:11,920 --> 00:38:16,240
actually run that them and you can show

00:38:13,240 --> 00:38:19,390
you how to run as often as you want and

00:38:16,240 --> 00:38:21,340
it was this one I created I think you

00:38:19,390 --> 00:38:24,280
can change it to run like air early or

00:38:21,340 --> 00:38:26,200
on an event and we can see that what

00:38:24,280 --> 00:38:29,620
will happen is that it'll actually start

00:38:26,200 --> 00:38:34,900
the job running when it kicks off we can

00:38:29,620 --> 00:38:36,430
see it started kicked off here let's see

00:38:34,900 --> 00:38:38,430
yeah so it's going to call those two

00:38:36,430 --> 00:38:41,410
phases those two different notebooks and

00:38:38,430 --> 00:38:43,990
if we go back to our jobs UI we can see

00:38:41,410 --> 00:38:45,610
that it's kicked off the first one it's

00:38:43,990 --> 00:38:47,500
just accepted and then this will go into

00:38:45,610 --> 00:38:49,030
the running State so now it's in the

00:38:47,500 --> 00:38:51,070
running State it's just like any sparked

00:38:49,030 --> 00:38:53,590
out you can kind of monitor it look at

00:38:51,070 --> 00:38:59,770
the metrics and so on I'm at a time so

00:38:53,590 --> 00:39:02,470
I'm going to go back and state thank you

00:38:59,770 --> 00:39:04,150
that's kind of the brief demo of the

00:39:02,470 --> 00:39:05,980
platform we have a lot more videos on

00:39:04,150 --> 00:39:08,320
the website you can look up its logic

00:39:05,980 --> 00:39:08,710
voxcom we're on twitter and we're on

00:39:08,320 --> 00:39:10,540
kiddo

00:39:08,710 --> 00:39:13,090
and i have a lot of people to thank

00:39:10,540 --> 00:39:16,750
other company and you can see their

00:39:13,090 --> 00:39:21,010
names there okay so that's it from me

00:39:16,750 --> 00:39:23,950
are we back to stream era that's kind of

00:39:21,010 --> 00:39:25,210
cool infinite view so thanks about

00:39:23,950 --> 00:39:27,580
German that was a very interesting

00:39:25,210 --> 00:39:30,400
presentation I hope you still have time

00:39:27,580 --> 00:39:32,140
for a couple of questions maybe we yeah

00:39:30,400 --> 00:39:34,540
I'll drop over to their to their to the

00:39:32,140 --> 00:39:35,860
room I you know I know I have another

00:39:34,540 --> 00:39:38,740
meeting coming up but I'll be there for

00:39:35,860 --> 00:39:40,960
compliments okay maybe we can just take

00:39:38,740 --> 00:39:43,900
a couple of questions here and then I

00:39:40,960 --> 00:39:45,340
will maybe because the day just one just

00:39:43,900 --> 00:39:47,230
one and then okay

00:39:45,340 --> 00:39:49,300
all right actually I think the most

00:39:47,230 --> 00:39:51,760
common one that we have is related to

00:39:49,300 --> 00:39:53,560
the time travel API so a couple of

00:39:51,760 --> 00:39:55,690
questions so is it possible with hops

00:39:53,560 --> 00:39:58,150
words to go back in time with feature

00:39:55,690 --> 00:39:59,920
values to consistently back test your ml

00:39:58,150 --> 00:40:03,190
model and there's the related question

00:39:59,920 --> 00:40:05,470
so about the time travel API yeah one of

00:40:03,190 --> 00:40:07,720
the main use cases is to kind of avoid

00:40:05,470 --> 00:40:10,030
future poisoning of label data so I

00:40:07,720 --> 00:40:13,480
don't want get me all the records at

00:40:10,030 --> 00:40:15,700
time t1 yeah I would want to say get me

00:40:13,480 --> 00:40:17,500
all the records as they were at the time

00:40:15,700 --> 00:40:19,880
of an event let's say click off the

00:40:17,500 --> 00:40:22,040
record click off records could

00:40:19,880 --> 00:40:26,450
makes comment on that yeah so I mean

00:40:22,040 --> 00:40:28,910
like we you can add a you know a column

00:40:26,450 --> 00:40:30,770
to your future Gertz called the time of

00:40:28,910 --> 00:40:32,210
which the event was created as well and

00:40:30,770 --> 00:40:34,520
typically you might roll them up to a

00:40:32,210 --> 00:40:37,400
day and if you want to or to an hour or

00:40:34,520 --> 00:40:39,020
so in some interval of time and so so

00:40:37,400 --> 00:40:41,150
when you join your feature groups if you

00:40:39,020 --> 00:40:43,910
join on that column then it will match

00:40:41,150 --> 00:40:45,320
them up according to the time so that

00:40:43,910 --> 00:40:47,450
the features will be correct at the same

00:40:45,320 --> 00:40:50,090
time so you can explicitly explicitly

00:40:47,450 --> 00:40:53,560
join on the on the time column your

00:40:50,090 --> 00:40:53,560
feature groups then that will work fine

00:41:00,030 --> 00:41:02,090

YouTube URL: https://www.youtube.com/watch?v=TPuqyNJX7T4


