Title: #bbuzz: Niels Basjes - When your stream doesn’t stream very well
Publication date: 2020-06-29
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/when-your-stream-doesnt-stream-very-well

So you got the job of creating this great new streaming analysis on top of an existing data stream. To avoid overloading your laptop you start the project by listening to the data from the test environment which gives you a manageable volume. You get the data into Apache Flink or Beam and you see raw data coming in. Yet your first attempts at doing a very simple analysis on this data results in … nothing coming out. Then you simply take the raw data and use the advised way to write it to something like HBase ... and it takes 30 minutes for the records to appear in the database.

What is going wrong?

The reality of streaming analytics is that the analytics works great on a continuous and big enough stream. Testing streams are very often “too dry” causing all kinds of basic systems in stream processing frameworks to behave differently from what you want. But not only the streams in the test environment are a problem, also streams that are used to process incoming files (i.e. batches on a stream) can be quite a problem to handle correctly.

In this talk I will go into some of the practical problems we ran into while building streaming applications with Apache Kafka, Apache Flink and similar tools over the last years. And I will show the solutions we use that allow people to successfully build analytics/processing solutions on these "not so streaming" streams.
Captions: 
	00:00:07,690 --> 00:00:14,200
everyone let's welcome our next speaker

00:00:09,850 --> 00:00:18,009
Niels he works at Bolcom and his focus

00:00:14,200 --> 00:00:20,830
areas are around scale and reliability

00:00:18,009 --> 00:00:24,490
he's Niels is an active open source

00:00:20,830 --> 00:00:27,220
developer a PMC member of the Apache

00:00:24,490 --> 00:00:31,359
Avro project he contributes to other

00:00:27,220 --> 00:00:35,739
projects like Hadoop HBase flink beam

00:00:31,359 --> 00:00:38,110
and storm this year at Berlin buzzwords

00:00:35,739 --> 00:00:41,469
needs is gonna speak about when your

00:00:38,110 --> 00:00:42,280
stream doesn't stream very well so let's

00:00:41,469 --> 00:00:47,550
have it for me

00:00:42,280 --> 00:00:50,420
hi so welcome

00:00:47,550 --> 00:00:53,460
it's time I am going to talk about

00:00:50,420 --> 00:00:56,070
experiences from the wild when building

00:00:53,460 --> 00:00:57,870
actual streaming applications and things

00:00:56,070 --> 00:01:03,059
move differently than you expect

00:00:57,870 --> 00:01:06,240
so benefi gender I'm gonna briefly talk

00:01:03,059 --> 00:01:09,600
about our context in which I am creating

00:01:06,240 --> 00:01:12,030
these systems we're talking about

00:01:09,600 --> 00:01:15,720
streaming solutions and specifically

00:01:12,030 --> 00:01:17,610
about broken and problematic streams I'm

00:01:15,720 --> 00:01:20,399
also going to explain how these systems

00:01:17,610 --> 00:01:23,190
works at least the basics and then also

00:01:20,399 --> 00:01:26,750
include solutions directions that we are

00:01:23,190 --> 00:01:30,380
working on and some final conclusions

00:01:26,750 --> 00:01:33,300
first a bit of background from about me

00:01:30,380 --> 00:01:35,280
for those who don't know me yet I have a

00:01:33,300 --> 00:01:37,679
background in both computer science and

00:01:35,280 --> 00:01:40,200
business I have been doing software

00:01:37,679 --> 00:01:44,070
development algorithm research and

00:01:40,200 --> 00:01:45,600
architecture for a long time nowadays I

00:01:44,070 --> 00:01:47,940
put corporate inventor on my business

00:01:45,600 --> 00:01:50,909
card because that best explains what I

00:01:47,940 --> 00:01:53,250
really do and I contribute to a lot of

00:01:50,909 --> 00:01:56,550
open source projects and I speak at

00:01:53,250 --> 00:01:59,670
quite a few conferences so the context

00:01:56,550 --> 00:02:03,360
where work is Bolcom it's an online

00:01:59,670 --> 00:02:07,800
retailer we sell stuff we sell lots of

00:02:03,360 --> 00:02:11,430
stuff in fact we have over 22 23 million

00:02:07,800 --> 00:02:14,220
items actually available right now

00:02:11,430 --> 00:02:17,430
it varies per day so this is a from a

00:02:14,220 --> 00:02:19,500
from a while ago and one of the things

00:02:17,430 --> 00:02:21,390
you need to do in order to make sure

00:02:19,500 --> 00:02:23,370
that customers are able to find their

00:02:21,390 --> 00:02:27,000
products is personalization

00:02:23,370 --> 00:02:29,550
so because I'm recognized we do all

00:02:27,000 --> 00:02:31,379
kinds of changes to the pages to make

00:02:29,550 --> 00:02:34,230
sure that the person is able to find

00:02:31,379 --> 00:02:36,180
what they're looking for so we have a

00:02:34,230 --> 00:02:38,270
new service in my region I get to see

00:02:36,180 --> 00:02:41,129
that we have some general advertising

00:02:38,270 --> 00:02:44,040
and these are based on the fact that I

00:02:41,129 --> 00:02:46,890
have kids that like movies like Harry

00:02:44,040 --> 00:02:51,470
Potter and transformers but also I like

00:02:46,890 --> 00:02:53,510
to play games and in addition

00:02:51,470 --> 00:02:55,540
I looked at something hey maybe other

00:02:53,510 --> 00:02:59,620
things are interesting as well and

00:02:55,540 --> 00:03:03,110
products items I looked at before you

00:02:59,620 --> 00:03:05,780
can understand that in order to make all

00:03:03,110 --> 00:03:07,070
of this as real-time as possible we do

00:03:05,780 --> 00:03:08,900
quite a bit of stream processing

00:03:07,070 --> 00:03:10,910
nowadays we use both of our chief

00:03:08,900 --> 00:03:14,720
Lincoln Apache beam in a lot of our

00:03:10,910 --> 00:03:17,540
production scenarios and for this talk I

00:03:14,720 --> 00:03:20,240
have constructed a very simple example

00:03:17,540 --> 00:03:23,360
use case that is actually something I

00:03:20,240 --> 00:03:26,209
tried to build and failed and ran into a

00:03:23,360 --> 00:03:29,270
lot of problems and I'm going to explain

00:03:26,209 --> 00:03:32,690
those problems see in this case this

00:03:29,270 --> 00:03:35,000
it's a person behind a laptop who visits

00:03:32,690 --> 00:03:37,250
our website and the interaction events

00:03:35,000 --> 00:03:39,140
go into Kafka I spoke out on this

00:03:37,250 --> 00:03:41,450
project last year at this conference so

00:03:39,140 --> 00:03:44,510
look back at that if you're more

00:03:41,450 --> 00:03:47,540
interested about that in this example

00:03:44,510 --> 00:03:50,570
I'm attaching a very simple flink SQL

00:03:47,540 --> 00:03:54,050
statement because I want to get a graph

00:03:50,570 --> 00:03:57,769
on the number of pageviews per hour so

00:03:54,050 --> 00:03:59,989
this link SQL is then stored in HBase in

00:03:57,769 --> 00:04:04,190
this example so you can attach something

00:03:59,989 --> 00:04:06,650
that shows you graph now let's pull this

00:04:04,190 --> 00:04:10,220
very simple application apart into the

00:04:06,650 --> 00:04:13,700
elements that that I have problems with

00:04:10,220 --> 00:04:18,019
and the first thing to realize is that

00:04:13,700 --> 00:04:21,350
time is essential in our use case I am

00:04:18,019 --> 00:04:24,169
saying interval one hour so the main

00:04:21,350 --> 00:04:27,200
question is then what kind of time are

00:04:24,169 --> 00:04:30,590
we talking about are we talking about

00:04:27,200 --> 00:04:32,990
even time are we interested in reporting

00:04:30,590 --> 00:04:35,900
on the time that the event originally

00:04:32,990 --> 00:04:37,970
occurred if we build a processing around

00:04:35,900 --> 00:04:40,419
this concept that is nice because in

00:04:37,970 --> 00:04:43,040
case of a processing disruption

00:04:40,419 --> 00:04:44,960
eventually consistent the end result

00:04:43,040 --> 00:04:48,610
will remain the same because the end

00:04:44,960 --> 00:04:51,740
result solely depends on the data

00:04:48,610 --> 00:04:54,470
systems like flink also support in just

00:04:51,740 --> 00:04:57,560
in time and processing time which is

00:04:54,470 --> 00:04:59,840
essentially the time the data arrived in

00:04:57,560 --> 00:05:03,370
the processing system or in the in the

00:04:59,840 --> 00:05:05,770
source and where it was processed

00:05:03,370 --> 00:05:08,350
it's important to realize that if you

00:05:05,770 --> 00:05:10,270
build something on top of this that in

00:05:08,350 --> 00:05:12,040
case of a processing disruption the

00:05:10,270 --> 00:05:13,690
result will be different because the

00:05:12,040 --> 00:05:16,780
processing time will be later than

00:05:13,690 --> 00:05:19,750
expected so the end result really

00:05:16,780 --> 00:05:23,500
depends on both the data and the

00:05:19,750 --> 00:05:25,750
operations of the processing system now

00:05:23,500 --> 00:05:29,470
the most common example to explain all

00:05:25,750 --> 00:05:32,290
this as the Star Wars where we have the

00:05:29,470 --> 00:05:36,639
story time event time that goes from

00:05:32,290 --> 00:05:38,680
episode 1 2 3 4 5 6 etc yet the arrival

00:05:36,639 --> 00:05:41,620
of the movies was that we first got the

00:05:38,680 --> 00:05:43,990
mill 3 then the first three and then the

00:05:41,620 --> 00:05:47,229
last three and the side stories were

00:05:43,990 --> 00:05:49,510
dabbled in between in a wrong order also

00:05:47,229 --> 00:05:51,610
and then later on they created the nice

00:05:49,510 --> 00:05:55,419
series called the Mandalorian which is

00:05:51,610 --> 00:05:56,979
also somewhere halfway the processing

00:05:55,419 --> 00:06:00,430
time is a fundamentally different thing

00:05:56,979 --> 00:06:02,680
than event time and looking at the

00:06:00,430 --> 00:06:05,620
things around me I see that for example

00:06:02,680 --> 00:06:07,360
in operations a lot of times you need

00:06:05,620 --> 00:06:09,479
processing time because you need to know

00:06:07,360 --> 00:06:12,270
when something actually happened how the

00:06:09,479 --> 00:06:14,740
operations of a system actually behaved

00:06:12,270 --> 00:06:16,950
what you see here is a disturbance we

00:06:14,740 --> 00:06:19,870
had in one of her flink processing tasks

00:06:16,950 --> 00:06:21,849
in our in our production environment it

00:06:19,870 --> 00:06:26,200
went down for an hour and then had an

00:06:21,849 --> 00:06:30,370
enormous bubble of higher processing to

00:06:26,200 --> 00:06:33,460
catch up what it still had to do in this

00:06:30,370 --> 00:06:35,979
example we are doing web stats we want

00:06:33,460 --> 00:06:38,320
to know the reporting on the event time

00:06:35,979 --> 00:06:43,000
we want reporting on when something

00:06:38,320 --> 00:06:45,099
really happened the second part of the

00:06:43,000 --> 00:06:48,190
application that needs addressing is the

00:06:45,099 --> 00:06:52,710
group by event time because that's what

00:06:48,190 --> 00:06:52,710
we're doing we're doing a group by hour

00:06:52,919 --> 00:06:57,669
now if you're doing a group by on a

00:06:55,599 --> 00:06:59,770
fixed data set that is nice because you

00:06:57,669 --> 00:07:03,010
have an overview of all events in there

00:06:59,770 --> 00:07:05,650
and you can simply sort them group them

00:07:03,010 --> 00:07:09,699
and then have the grew by of whatever

00:07:05,650 --> 00:07:12,280
fields your feel like but this is a

00:07:09,699 --> 00:07:14,939
forever changing data set it's a

00:07:12,280 --> 00:07:18,009
never-ending stream it is

00:07:14,939 --> 00:07:22,000
impossible to have an overview of all

00:07:18,009 --> 00:07:24,189
events so if you want to do a group by

00:07:22,000 --> 00:07:26,740
type of operation you have to do it

00:07:24,189 --> 00:07:28,990
slightly different and streaming systems

00:07:26,740 --> 00:07:31,719
like link use the concept of a window

00:07:28,990 --> 00:07:34,599
which is essentially a group by buffer

00:07:31,719 --> 00:07:36,819
you incrementally fill it with events as

00:07:34,599 --> 00:07:39,159
they arrive and then have some kind of

00:07:36,819 --> 00:07:41,469
assessment of completeness and when that

00:07:39,159 --> 00:07:43,030
assessment arrives you take all of these

00:07:41,469 --> 00:07:47,259
events and ship them out for further

00:07:43,030 --> 00:07:49,779
processing so when is a window complete

00:07:47,259 --> 00:07:53,169
the construct they use is called

00:07:49,779 --> 00:07:55,090
watermarks these are special records in

00:07:53,169 --> 00:07:57,009
the data stream that are hidden from the

00:07:55,090 --> 00:07:59,439
application but are there for the

00:07:57,009 --> 00:08:01,900
framework to react on so a window sees

00:07:59,439 --> 00:08:04,840
them but the application code doesn't

00:08:01,900 --> 00:08:07,689
and they simply indicates that there

00:08:04,840 --> 00:08:10,330
will no be no more older events so if

00:08:07,689 --> 00:08:12,849
you see them you can respond by shipping

00:08:10,330 --> 00:08:16,240
out the window and it's common to base

00:08:12,849 --> 00:08:17,770
them on the data which gives you an

00:08:16,240 --> 00:08:19,839
estimate or a guess of completeness

00:08:17,770 --> 00:08:22,330
there are situations where you guess

00:08:19,839 --> 00:08:28,539
wrong I'm not going to in going into

00:08:22,330 --> 00:08:32,260
those situations in this talk so a very

00:08:28,539 --> 00:08:34,089
simple application we create records

00:08:32,260 --> 00:08:37,659
throw them in Kafka and then into a

00:08:34,089 --> 00:08:39,969
flinger thing so the first thing to

00:08:37,659 --> 00:08:43,120
realize is that you when you started the

00:08:39,969 --> 00:08:46,990
window has a sense of last seen

00:08:43,120 --> 00:08:51,010
watermark which is very long time ago a

00:08:46,990 --> 00:08:54,550
to the dinosaurs well actually it's min

00:08:51,010 --> 00:08:56,890
long which is 292 million years ago and

00:08:54,550 --> 00:08:59,290
the first dinosaurs only appeared 50

00:08:56,890 --> 00:09:02,430
million years later so it's even older

00:08:59,290 --> 00:09:05,800
than the dinosaurs but you get the idea

00:09:02,430 --> 00:09:07,630
so my application creates an event that

00:09:05,800 --> 00:09:10,029
goes through Kafka into the application

00:09:07,630 --> 00:09:13,329
and goes into a bucket that is grouped

00:09:10,029 --> 00:09:17,290
for the 11 o'clock window and then

00:09:13,329 --> 00:09:20,500
another event arrives and then some part

00:09:17,290 --> 00:09:23,140
of the code says hey I saw something

00:09:20,500 --> 00:09:26,560
occur and I choose now to insert a

00:09:23,140 --> 00:09:28,940
watermark so it inserts a watermark for

00:09:26,560 --> 00:09:32,090
12:05 which replaces our darkness

00:09:28,940 --> 00:09:34,640
and then the application says the 11

00:09:32,090 --> 00:09:38,690
o'clock window is complete I can ship it

00:09:34,640 --> 00:09:40,970
out and use it for the reporting the

00:09:38,690 --> 00:09:45,130
first thing to notice is that Kafka is

00:09:40,970 --> 00:09:47,870
not a simple pipe a Kafka consists of

00:09:45,130 --> 00:09:51,050
Kafka topic persists consists of

00:09:47,870 --> 00:09:54,410
partitions and partitions are

00:09:51,050 --> 00:09:56,960
essentially smaller pipes that each give

00:09:54,410 --> 00:10:00,040
ordering guarantees and you can have a

00:09:56,960 --> 00:10:03,140
lot of them to have a large scalability

00:10:00,040 --> 00:10:05,990
but on a per PI basis there are ordering

00:10:03,140 --> 00:10:09,980
guarantees so our application really

00:10:05,990 --> 00:10:12,710
looks a bit like this per incoming pipe

00:10:09,980 --> 00:10:14,000
you can have a source component you can

00:10:12,710 --> 00:10:15,440
also have source components doing

00:10:14,000 --> 00:10:17,710
multiple but let's keep it at a

00:10:15,440 --> 00:10:20,870
one-on-one example for this talk so

00:10:17,710 --> 00:10:24,740
again our application starts firing

00:10:20,870 --> 00:10:29,090
these events same as before but now you

00:10:24,740 --> 00:10:32,780
see that it has for each incoming source

00:10:29,090 --> 00:10:37,190
it has a separate rememory of what is

00:10:32,780 --> 00:10:39,980
the latest watermark so then at the

00:10:37,190 --> 00:10:42,740
other one a message arrives and only

00:10:39,980 --> 00:10:47,330
then the assessment is wait is made that

00:10:42,740 --> 00:10:49,190
the 11 o'clock is is finished the third

00:10:47,330 --> 00:10:51,110
part in the application that we need to

00:10:49,190 --> 00:10:56,510
think about is the writing to the

00:10:51,110 --> 00:11:00,290
database that part if you use HBase and

00:10:56,510 --> 00:11:02,540
I'm using this as an example I truly see

00:11:00,290 --> 00:11:05,240
that in a lot of other database systems

00:11:02,540 --> 00:11:07,610
you see similar effects writing through

00:11:05,240 --> 00:11:10,460
a database you have a method that puts

00:11:07,610 --> 00:11:12,380
in a single record now in case of HBase

00:11:10,460 --> 00:11:13,040
there is a remote call to the database

00:11:12,380 --> 00:11:15,260
server

00:11:13,040 --> 00:11:17,570
that puts that single record in and you

00:11:15,260 --> 00:11:21,470
have to wait for the network latency for

00:11:17,570 --> 00:11:25,220
it to return which can be quite slow if

00:11:21,470 --> 00:11:26,690
you have a lot of them so that is why in

00:11:25,220 --> 00:11:29,300
HBase they have something called a

00:11:26,690 --> 00:11:31,670
buffered mutator which actually buffers

00:11:29,300 --> 00:11:33,710
two megabytes of mutations and when the

00:11:31,670 --> 00:11:37,700
buffer is full it does a single call to

00:11:33,710 --> 00:11:40,310
the back-end writing in a way that is if

00:11:37,700 --> 00:11:42,680
you have a lot of measurements a lot

00:11:40,310 --> 00:11:46,040
faster

00:11:42,680 --> 00:11:49,100
so I have this application I run it on

00:11:46,040 --> 00:11:53,810
my laptop and I get no output at all

00:11:49,100 --> 00:11:56,690
and I kinda panic well it's important to

00:11:53,810 --> 00:11:59,959
realize that some streams don't stream

00:11:56,690 --> 00:12:03,500
very well and over the years I have come

00:11:59,959 --> 00:12:06,459
to recognize for situations for classes

00:12:03,500 --> 00:12:10,790
of streams that downstream very well and

00:12:06,459 --> 00:12:14,600
the first is slow streams streams with a

00:12:10,790 --> 00:12:18,529
very low event rate the second is slim

00:12:14,600 --> 00:12:20,959
streams if own streams that only have

00:12:18,529 --> 00:12:24,560
events in in one or a few Kafka

00:12:20,959 --> 00:12:27,440
partitions there are streams that are

00:12:24,560 --> 00:12:30,320
called bursty or batchi where you have

00:12:27,440 --> 00:12:35,390
bursts of events and then long gaps in

00:12:30,320 --> 00:12:38,149
between those this is by design usually

00:12:35,390 --> 00:12:39,860
and there are broken streams also with

00:12:38,149 --> 00:12:44,810
large gaps but now caused by the

00:12:39,860 --> 00:12:46,850
disruption or by maintenance so let's

00:12:44,810 --> 00:12:50,180
look at some of those some examples of

00:12:46,850 --> 00:12:52,459
this I have encountered a combination of

00:12:50,180 --> 00:12:56,120
slow and slim streams on our test

00:12:52,459 --> 00:12:58,370
environment so let's get back to our

00:12:56,120 --> 00:13:00,620
example application you know I'm on the

00:12:58,370 --> 00:13:03,830
test environment and I'm clicking on our

00:13:00,620 --> 00:13:06,290
website so I'm producing some events

00:13:03,830 --> 00:13:09,529
that are stored in Windows and the

00:13:06,290 --> 00:13:11,510
watermarks drive and then if you look

00:13:09,529 --> 00:13:14,390
back also to my other talk what you see

00:13:11,510 --> 00:13:20,240
is that we have said we are doing a

00:13:14,390 --> 00:13:22,610
partition by session ID that way every

00:13:20,240 --> 00:13:25,160
event for a single session goes into a

00:13:22,610 --> 00:13:28,850
single Kafka partition which means that

00:13:25,160 --> 00:13:30,520
the ordering is maintained and then also

00:13:28,850 --> 00:13:32,600
at the end of flink

00:13:30,520 --> 00:13:34,610
depending on the application of course

00:13:32,600 --> 00:13:37,339
we have the option of maintaining that

00:13:34,610 --> 00:13:40,579
ordering by doing a key by session ID

00:13:37,339 --> 00:13:45,470
and that is very nice but in this

00:13:40,579 --> 00:13:47,660
situation I'm alone I'm alone on this

00:13:45,470 --> 00:13:49,670
website and that means that I have to

00:13:47,660 --> 00:13:52,520
have wait a really really long time

00:13:49,670 --> 00:13:56,320
before somebody else creates an event

00:13:52,520 --> 00:13:58,960
that takes that dinosaur away because

00:13:56,320 --> 00:14:03,400
when that dinosaur is away can my window

00:13:58,960 --> 00:14:05,380
be shipped out and you see that only the

00:14:03,400 --> 00:14:07,270
11 o'clock window is shipped out because

00:14:05,380 --> 00:14:09,250
the 12 o'clock window does not have a

00:14:07,270 --> 00:14:13,450
guarantee from all inputs that it is

00:14:09,250 --> 00:14:15,340
finished now this drawing only shows you

00:14:13,450 --> 00:14:17,770
two Kafka partitions but in our

00:14:15,340 --> 00:14:20,080
production environment we have over 240

00:14:17,770 --> 00:14:25,120
that has to do with a number of hard

00:14:20,080 --> 00:14:28,450
drives in our cluster but this is not

00:14:25,120 --> 00:14:30,910
all why I don't get any data the other

00:14:28,450 --> 00:14:35,310
reason is that we're writing to HBase

00:14:30,910 --> 00:14:38,710
and over there is the buffered mutator

00:14:35,310 --> 00:14:43,990
so the windows are going or by far not

00:14:38,710 --> 00:14:46,510
enough to flush the buffer another very

00:14:43,990 --> 00:14:50,170
common example I've seen is slim streams

00:14:46,510 --> 00:14:52,090
and something we ran into a couple of

00:14:50,170 --> 00:14:55,000
years ago when we first started with our

00:14:52,090 --> 00:14:58,120
project is that on acceptance we run

00:14:55,000 --> 00:14:59,650
loads tests so we have a load to

00:14:58,120 --> 00:15:02,640
narrator some kind of robot that

00:14:59,650 --> 00:15:05,650
produces traffic to test the website and

00:15:02,640 --> 00:15:08,470
in this processing it is important to

00:15:05,650 --> 00:15:12,040
realize that the windows while there are

00:15:08,470 --> 00:15:13,980
incomplete they occupy memory memory in

00:15:12,040 --> 00:15:16,870
the state system of something like flink

00:15:13,980 --> 00:15:20,440
while this can be persisted to some kind

00:15:16,870 --> 00:15:23,860
of a stories layer it is memory that you

00:15:20,440 --> 00:15:25,750
are using so if you are done having a

00:15:23,860 --> 00:15:28,510
low test generator that is not that

00:15:25,750 --> 00:15:31,060
clever and produces events with

00:15:28,510 --> 00:15:34,030
everything the same session ID over or

00:15:31,060 --> 00:15:37,780
only a few session IDs you will blow it

00:15:34,030 --> 00:15:40,180
up because none of the windows will ever

00:15:37,780 --> 00:15:45,400
get flushed all of the events will stay

00:15:40,180 --> 00:15:49,090
in the application because that dinosaur

00:15:45,400 --> 00:15:50,650
is not taken out and if you have this

00:15:49,090 --> 00:15:52,090
situation you thought islets the

00:15:50,650 --> 00:15:55,630
terabyte of memory should be plenty

00:15:52,090 --> 00:16:02,120
that's not if you run the load as long

00:15:55,630 --> 00:16:03,830
enough anything breaks another exam

00:16:02,120 --> 00:16:07,550
that I have seen in production as

00:16:03,830 --> 00:16:10,040
something we call bursty streams data

00:16:07,550 --> 00:16:12,500
that arrives in bursts and a good

00:16:10,040 --> 00:16:15,380
example is something we were designing a

00:16:12,500 --> 00:16:17,740
while ago is when we have partners that

00:16:15,380 --> 00:16:20,270
want to offer upload data to a landscape

00:16:17,740 --> 00:16:24,350
for example a seller that wants to

00:16:20,270 --> 00:16:26,270
update their prices so we want to have

00:16:24,350 --> 00:16:28,520
two kinds of interfaces for these people

00:16:26,270 --> 00:16:33,140
arrest interface where they can have a

00:16:28,520 --> 00:16:36,980
REST API with a single change and a file

00:16:33,140 --> 00:16:40,400
upload so the REST API is you know API

00:16:36,980 --> 00:16:42,350
some transport like pops up or Kafka and

00:16:40,400 --> 00:16:45,500
then some processing and then a storage

00:16:42,350 --> 00:16:48,010
in some database system and in this

00:16:45,500 --> 00:16:51,500
example I'm using HBase or BigTable and

00:16:48,010 --> 00:16:53,720
as a side thing the file upload and then

00:16:51,500 --> 00:16:56,150
every time the file arrives you pull

00:16:53,720 --> 00:16:57,410
that apart into single lines and throw

00:16:56,150 --> 00:17:02,000
it into the same processing line

00:16:57,410 --> 00:17:04,699
pipeline now if somebody uploads for

00:17:02,000 --> 00:17:07,069
example a five megabyte file that is cut

00:17:04,699 --> 00:17:08,660
into all the individual records and the

00:17:07,069 --> 00:17:10,670
first two megabytes of those records

00:17:08,660 --> 00:17:13,220
just simply pass on into the database

00:17:10,670 --> 00:17:15,760
the second two megabytes pass on into

00:17:13,220 --> 00:17:19,220
the database and the last 1 megabyte

00:17:15,760 --> 00:17:21,829
will remain stuck in the buffer then

00:17:19,220 --> 00:17:24,559
somebody inserts the records Friday as

00:17:21,829 --> 00:17:28,660
REST API and another one and they will

00:17:24,559 --> 00:17:32,720
simply be remain stuck in the buffer and

00:17:28,660 --> 00:17:34,880
then there are broken streams an in

00:17:32,720 --> 00:17:37,429
production we all go down for

00:17:34,880 --> 00:17:39,920
maintenance sometimes even the largest

00:17:37,429 --> 00:17:42,950
systems can go down for maintenance and

00:17:39,920 --> 00:17:44,059
one of those very large systems I see is

00:17:42,950 --> 00:17:47,990
the Niagara Falls

00:17:44,059 --> 00:17:52,220
a very very large streaming system that

00:17:47,990 --> 00:17:54,170
went down for maintenance in 1969 they

00:17:52,220 --> 00:17:57,470
put up a dam and shut it down for

00:17:54,170 --> 00:17:59,780
maintenance and in our very simple

00:17:57,470 --> 00:18:02,660
application having a disruption in the

00:17:59,780 --> 00:18:05,090
data stream would mean that you know

00:18:02,660 --> 00:18:09,500
data stays in the buffer and it's not

00:18:05,090 --> 00:18:11,060
written to HBase so what

00:18:09,500 --> 00:18:13,850
are the kinds of solutions that we've

00:18:11,060 --> 00:18:17,060
come up with how do we try to handle

00:18:13,850 --> 00:18:19,610
these things well first if I the first

00:18:17,060 --> 00:18:22,670
of all the group by event time it's

00:18:19,610 --> 00:18:25,400
slightly more complex than our simple

00:18:22,670 --> 00:18:28,640
example because we do multistage

00:18:25,400 --> 00:18:30,830
processing it's not just the website the

00:18:28,640 --> 00:18:32,510
Kafka and the flink we have an ad rich

00:18:30,830 --> 00:18:35,980
clean filter and step in between

00:18:32,510 --> 00:18:40,490
with another Kafka topic this step

00:18:35,980 --> 00:18:42,920
determines visit if a session is idle

00:18:40,490 --> 00:18:46,300
for more than 30 minutes then a new

00:18:42,920 --> 00:18:48,770
visit starts it slaps on GOP idea

00:18:46,300 --> 00:18:51,260
information okay based on the IP address

00:18:48,770 --> 00:18:53,300
what country it disassembles the user

00:18:51,260 --> 00:18:56,680
agent string to figure out is it a phone

00:18:53,300 --> 00:18:59,390
or a tablet and based on those

00:18:56,680 --> 00:19:03,520
assessments we do our first assessment

00:18:59,390 --> 00:19:07,040
or if this is this a robot or not and

00:19:03,520 --> 00:19:10,100
last but not least we apply some GDP our

00:19:07,040 --> 00:19:15,350
processing for example the IP address is

00:19:10,100 --> 00:19:18,410
cleaned is nullified so how do we do

00:19:15,350 --> 00:19:20,030
then change how do we then fix this well

00:19:18,410 --> 00:19:22,910
there are two primary solution

00:19:20,030 --> 00:19:25,610
directions that I've seen either you

00:19:22,910 --> 00:19:28,160
change the data for example you inject

00:19:25,610 --> 00:19:30,830
non data records that are effectively

00:19:28,160 --> 00:19:33,710
just a timestamp I'll go into that a bit

00:19:30,830 --> 00:19:37,460
more later or II change the processing

00:19:33,710 --> 00:19:40,100
and in flink they recently committed a

00:19:37,460 --> 00:19:44,240
feature that allows detecting an idle

00:19:40,100 --> 00:19:47,270
timeout so if a Kafka topic or partition

00:19:44,240 --> 00:19:50,540
does not have data for a too long time

00:19:47,270 --> 00:19:52,820
it automatically marks it as idle and

00:19:50,540 --> 00:19:55,870
effectively says your we're not looking

00:19:52,820 --> 00:19:59,570
at you anymore in terms of watermarks I

00:19:55,870 --> 00:20:01,810
find it very important to stress that if

00:19:59,570 --> 00:20:05,210
you use this feature you are mixing

00:20:01,810 --> 00:20:07,910
processing time and event time you have

00:20:05,210 --> 00:20:12,020
an event time system and suddenly the

00:20:07,910 --> 00:20:16,140
processing time matters it may be useful

00:20:12,020 --> 00:20:18,240
in some cases I consider it a risk so

00:20:16,140 --> 00:20:20,400
way we tried to handle this is that we

00:20:18,240 --> 00:20:23,640
now have a very simple single threaded

00:20:20,400 --> 00:20:26,730
dummy component that simply creates

00:20:23,640 --> 00:20:29,250
every 500 milliseconds a timestamp only

00:20:26,730 --> 00:20:33,950
event that is thrown into all cough

00:20:29,250 --> 00:20:35,880
competitions any overhead is negligible

00:20:33,950 --> 00:20:39,120
so how does that look

00:20:35,880 --> 00:20:41,640
well the code is really simple you have

00:20:39,120 --> 00:20:44,340
a Kafka producer you just ask it for all

00:20:41,640 --> 00:20:48,450
the partitions and then keep running

00:20:44,340 --> 00:20:51,300
forever do the sleep or something and

00:20:48,450 --> 00:20:53,820
then get the time create a single

00:20:51,300 --> 00:20:57,420
measurement based on that timestamp and

00:20:53,820 --> 00:21:01,590
then for every partition send out that

00:20:57,420 --> 00:21:03,240
message it is important to note that the

00:21:01,590 --> 00:21:06,390
message that goes into all of the

00:21:03,240 --> 00:21:08,640
traffic or partitions is identical they

00:21:06,390 --> 00:21:13,140
all have the same key the same value at

00:21:08,640 --> 00:21:16,740
the same times then so how does that

00:21:13,140 --> 00:21:18,570
look in our streams well a normal stream

00:21:16,740 --> 00:21:21,270
now looks something like this where the

00:21:18,570 --> 00:21:24,870
green balls represent the data and the

00:21:21,270 --> 00:21:28,560
red clocks the the fake records in a

00:21:24,870 --> 00:21:31,410
slow stream you now have this in a slim

00:21:28,560 --> 00:21:33,810
stream you know I have this and if you

00:21:31,410 --> 00:21:37,260
have a bursty or broken stream you get

00:21:33,810 --> 00:21:42,090
something like this so you always have

00:21:37,260 --> 00:21:44,610
the timestamp events in all partitions

00:21:42,090 --> 00:21:48,810
that trigger watermarks that make sure

00:21:44,610 --> 00:21:51,870
that processing continues so if I look

00:21:48,810 --> 00:21:55,800
back at this drawing what we have now is

00:21:51,870 --> 00:21:59,070
next to our website in ready to run in

00:21:55,800 --> 00:22:01,050
to feed the data into Kafka at timed

00:21:59,070 --> 00:22:04,470
event generator which are essentially

00:22:01,050 --> 00:22:05,820
guaranteed Fantine events it's a very

00:22:04,470 --> 00:22:08,670
simple thing that runs from Cuban

00:22:05,820 --> 00:22:10,590
Eddie's and if it's not there it's not

00:22:08,670 --> 00:22:13,730
too much of a problem because it's

00:22:10,590 --> 00:22:16,650
automatically restarted within seconds

00:22:13,730 --> 00:22:19,710
so now because of those events

00:22:16,650 --> 00:22:22,140
if this intermediate component goes down

00:22:19,710 --> 00:22:24,930
for maintenance the component at the end

00:22:22,140 --> 00:22:27,330
can detect the difference between the

00:22:24,930 --> 00:22:29,810
website is down and the stream is down

00:22:27,330 --> 00:22:31,880
and the

00:22:29,810 --> 00:22:34,970
detection alternative that is new ink in

00:22:31,880 --> 00:22:37,430
Flint 111 will change the final data

00:22:34,970 --> 00:22:41,420
because it will start changing the way

00:22:37,430 --> 00:22:43,010
the data is handled so how do you see

00:22:41,420 --> 00:22:44,930
the difference between side down and

00:22:43,010 --> 00:22:46,520
stream down it's actually quite simple

00:22:44,930 --> 00:22:49,070
if the site is down you have the

00:22:46,520 --> 00:22:51,980
watermarks but no data if the stream is

00:22:49,070 --> 00:22:54,200
down there is nothing at all and I think

00:22:51,980 --> 00:22:59,540
you should simply wait for the data to

00:22:54,200 --> 00:23:02,560
arrive there is however a catch the

00:22:59,540 --> 00:23:08,540
enriched step must handle it all

00:23:02,560 --> 00:23:14,270
processing steps must be able to to work

00:23:08,540 --> 00:23:16,820
with this so what you see is that in our

00:23:14,270 --> 00:23:18,190
application because we are maintaining

00:23:16,820 --> 00:23:21,200
ordering per session

00:23:18,190 --> 00:23:23,690
we first do the flink operation key by

00:23:21,200 --> 00:23:25,460
session ID which routes everything with

00:23:23,690 --> 00:23:28,490
the same session ID to a single

00:23:25,460 --> 00:23:31,580
component and then in the one component

00:23:28,490 --> 00:23:34,310
that we treat receives all the timestamp

00:23:31,580 --> 00:23:36,980
events some kind of deduplication has to

00:23:34,310 --> 00:23:38,890
occur and then at some point it has to

00:23:36,980 --> 00:23:42,760
forward all of these to the outgoing

00:23:38,890 --> 00:23:46,340
partitions of the rest of the stream and

00:23:42,760 --> 00:23:49,700
it's important to realize that you must

00:23:46,340 --> 00:23:51,740
D duplicate and forward because the

00:23:49,700 --> 00:23:53,600
number of partitions in the incoming

00:23:51,740 --> 00:23:56,840
stream and in the outcoming stream are

00:23:53,600 --> 00:24:00,470
in general different so if you have ten

00:23:56,840 --> 00:24:03,410
partitions coming in in 15 going out all

00:24:00,470 --> 00:24:07,370
fifteen outgoing must have a timestamp

00:24:03,410 --> 00:24:09,860
event I do realize that the ordering and

00:24:07,370 --> 00:24:12,050
the synchronization between the various

00:24:09,860 --> 00:24:13,610
streams is hard so that's still

00:24:12,050 --> 00:24:19,460
something that we need to figure out how

00:24:13,610 --> 00:24:21,920
to handle properly any final implication

00:24:19,460 --> 00:24:24,260
must also handle it because now it

00:24:21,920 --> 00:24:26,720
receives the data and the watermarks or

00:24:24,260 --> 00:24:28,730
the the timestamp events create

00:24:26,720 --> 00:24:31,910
watermarks from those and then drop the

00:24:28,730 --> 00:24:34,190
timestamp events after that normal

00:24:31,910 --> 00:24:40,190
processing can't continue because the

00:24:34,190 --> 00:24:41,700
data will still be only the data so this

00:24:40,190 --> 00:24:45,390
seems to work for us

00:24:41,700 --> 00:24:48,360
in our situation but I am thinking can

00:24:45,390 --> 00:24:50,970
we make this something more generic can

00:24:48,360 --> 00:24:54,390
we make this a feature that can go into

00:24:50,970 --> 00:24:58,020
flink or some system like that so I was

00:24:54,390 --> 00:25:02,309
thinking what if we just redefine what

00:24:58,020 --> 00:25:05,730
we call our topology so in the talk in

00:25:02,309 --> 00:25:09,990
in the example so far we had two because

00:25:05,730 --> 00:25:12,179
we had to fling jobs and at the start of

00:25:09,990 --> 00:25:15,360
the pipeline it is important to realize

00:25:12,179 --> 00:25:18,059
that wall clock time processing time

00:25:15,360 --> 00:25:21,000
event time are all the same so the

00:25:18,059 --> 00:25:23,549
events are created in the producing

00:25:21,000 --> 00:25:26,730
system whether it's a sensor or website

00:25:23,549 --> 00:25:32,010
or whatever with the time and of the

00:25:26,730 --> 00:25:34,590
machine at that moment and then when it

00:25:32,010 --> 00:25:38,400
goes through the pipeline we will create

00:25:34,590 --> 00:25:40,380
each time a watermark that goes with it

00:25:38,400 --> 00:25:43,169
through the flow within the flink

00:25:40,380 --> 00:25:45,780
topology and then it's thrown away then

00:25:43,169 --> 00:25:51,090
the data goes into Kafka again and we

00:25:45,780 --> 00:25:53,040
repeat that in the in the final step so

00:25:51,090 --> 00:25:55,140
what if we look at it slightly

00:25:53,040 --> 00:25:58,650
differently what what if we have the

00:25:55,140 --> 00:26:02,700
same set of applications but now we see

00:25:58,650 --> 00:26:06,770
this as the topology and we call our

00:26:02,700 --> 00:26:10,049
event generator a watermark producer

00:26:06,770 --> 00:26:12,510
then we can say that the watermarks are

00:26:10,049 --> 00:26:14,370
created at the same time stamp as the

00:26:12,510 --> 00:26:16,890
wall clock time processing time event

00:26:14,370 --> 00:26:22,230
time as the data and they are thrown

00:26:16,890 --> 00:26:24,510
into Kafka as well and if then they go

00:26:22,230 --> 00:26:29,610
through the day data stream it's more

00:26:24,510 --> 00:26:31,490
like this they are simply demultiplexed

00:26:29,610 --> 00:26:34,470
and multiplex deserialized and

00:26:31,490 --> 00:26:37,919
serialized every time they go in and out

00:26:34,470 --> 00:26:40,049
of a fling component the hard part like

00:26:37,919 --> 00:26:43,820
I mentioned before will be things like

00:26:40,049 --> 00:26:43,820
the filtering in the deduplication

00:26:43,850 --> 00:26:49,919
things I also realized is that this

00:26:47,610 --> 00:26:53,100
generation of the watermarks has to be

00:26:49,919 --> 00:26:55,140
done at a place that is consistent with

00:26:53,100 --> 00:26:56,970
all event producers

00:26:55,140 --> 00:27:00,210
if you have an event producer that does

00:26:56,970 --> 00:27:04,170
a lot of buffering for example an IOT

00:27:00,210 --> 00:27:06,620
device or an a mobile app that caches

00:27:04,170 --> 00:27:10,309
the data for a quote/unquote long time

00:27:06,620 --> 00:27:12,840
then this system will also not work I

00:27:10,309 --> 00:27:14,490
spoken last year at fling forward with

00:27:12,840 --> 00:27:16,830
somebody from Netflix and they explained

00:27:14,490 --> 00:27:19,290
to me that for example their mobile app

00:27:16,830 --> 00:27:21,270
is a problem because all the events of

00:27:19,290 --> 00:27:24,270
somebody watching a movie in the in the

00:27:21,270 --> 00:27:26,780
airplane with delay the data a very very

00:27:24,270 --> 00:27:26,780
long time

00:27:28,790 --> 00:27:33,720
also the deduplication is very important

00:27:31,500 --> 00:27:37,370
because if you have a few operations

00:27:33,720 --> 00:27:39,840
within your topology are the number of

00:27:37,370 --> 00:27:42,929
duplicates of all the watermarks will be

00:27:39,840 --> 00:27:45,390
very very great so that is something

00:27:42,929 --> 00:27:52,470
that I still have to look into and on

00:27:45,390 --> 00:27:55,380
how that can be solved and also ordering

00:27:52,470 --> 00:27:57,210
I suspect that this will only work if

00:27:55,380 --> 00:28:00,419
you have a transport type in-between

00:27:57,210 --> 00:28:02,790
that guarantees ordering so the final

00:28:00,419 --> 00:28:06,500
step to think about that we will talk

00:28:02,790 --> 00:28:09,630
about is the writing to the database

00:28:06,500 --> 00:28:11,730
HBase the perfect mutator with a two

00:28:09,630 --> 00:28:14,940
mega byte buffer the default setting but

00:28:11,730 --> 00:28:16,880
you can change that that was essentially

00:28:14,940 --> 00:28:19,559
written in the time of MapReduce

00:28:16,880 --> 00:28:22,140
everything was a batch there were no

00:28:19,559 --> 00:28:24,870
stream processing systems so the

00:28:22,140 --> 00:28:27,299
implementation simply flushes the buffer

00:28:24,870 --> 00:28:29,100
when it's full or when you close it down

00:28:27,299 --> 00:28:32,549
when you do a shutdown it does a flush

00:28:29,100 --> 00:28:34,230
and with if you have problematic streams

00:28:32,549 --> 00:28:37,559
the data stuck and the buffer as I've

00:28:34,230 --> 00:28:39,780
shown now the solution in these kinds of

00:28:37,559 --> 00:28:40,799
situations in this in this situation was

00:28:39,780 --> 00:28:45,809
actually quite simple

00:28:40,799 --> 00:28:47,520
so in 2018 I submitted a patch for HBase

00:28:45,809 --> 00:28:50,400
which is now in the clonic libraries

00:28:47,520 --> 00:28:54,660
that allows you to periodically check is

00:28:50,400 --> 00:28:56,730
there data in the buffer and if it's

00:28:54,660 --> 00:29:00,710
there for too long or you can set that

00:28:56,730 --> 00:29:03,150
in a number of milliseconds flush it and

00:29:00,710 --> 00:29:04,860
this has solved for our streaming

00:29:03,150 --> 00:29:07,879
solutions the problems that we were

00:29:04,860 --> 00:29:10,309
running into so

00:29:07,879 --> 00:29:11,959
in general if you look at applications

00:29:10,309 --> 00:29:15,499
and you're trying to develop something

00:29:11,959 --> 00:29:18,109
and no data comes out don't panic

00:29:15,499 --> 00:29:20,839
have a look at all the things I've shown

00:29:18,109 --> 00:29:23,419
you and it may be possible that in your

00:29:20,839 --> 00:29:26,599
case it is a normal effect of the way

00:29:23,419 --> 00:29:28,729
these systems work and I hope the the

00:29:26,599 --> 00:29:34,519
scenarios I've given you will help you

00:29:28,729 --> 00:29:38,089
in in solving that concluding this what

00:29:34,519 --> 00:29:40,519
I have seen over the years is that the

00:29:38,089 --> 00:29:44,209
primary calls for all of these problems

00:29:40,519 --> 00:29:46,190
is buffering buffering in combination

00:29:44,209 --> 00:29:49,239
with problematic streams and then the

00:29:46,190 --> 00:29:52,339
question when is my buffer flushed and

00:29:49,239 --> 00:29:55,359
if the buffering is done because of an

00:29:52,339 --> 00:29:57,979
algorithm like a grew by or a window I

00:29:55,359 --> 00:30:01,279
recommend you solve it from the data or

00:29:57,979 --> 00:30:03,619
from the algorithm and you can do that

00:30:01,279 --> 00:30:07,459
if you truly understand both your data

00:30:03,619 --> 00:30:10,969
and your algorithm and in any case I

00:30:07,459 --> 00:30:13,129
recommend sticking to even time avoid

00:30:10,969 --> 00:30:16,519
missed mixing in the processing time if

00:30:13,129 --> 00:30:18,349
you can but if your use case allows for

00:30:16,519 --> 00:30:22,339
it and it's not a problem you can use

00:30:18,349 --> 00:30:26,749
this and if your buffering for

00:30:22,339 --> 00:30:29,149
performance like a writing the buffer

00:30:26,749 --> 00:30:34,579
for a database or something like that

00:30:29,149 --> 00:30:37,399
just use a timeout Kafka has the Kafka

00:30:34,579 --> 00:30:39,559
producer has something like that there

00:30:37,399 --> 00:30:42,529
is a max linker time over there nowadays

00:30:39,559 --> 00:30:45,079
the HBase has it and it's quite simple

00:30:42,529 --> 00:30:49,519
to implement in most systems that are

00:30:45,079 --> 00:30:52,959
out there thank you for listening

00:30:49,519 --> 00:30:56,019
this was my talk if you're curious about

00:30:52,959 --> 00:30:58,669
the other talk I mentioned last year

00:30:56,019 --> 00:31:01,309
there is a link at the bottom of the

00:30:58,669 --> 00:31:03,139
screen towards a playlist that I've

00:31:01,309 --> 00:31:06,379
created with the talks I've given over

00:31:03,139 --> 00:31:09,769
the last few years so you can look it up

00:31:06,379 --> 00:31:14,679
there and I assume there are questions

00:31:09,769 --> 00:31:14,679
now so let me see how that works

00:31:15,459 --> 00:31:21,570
it means thanks to the dog I don't like

00:31:19,009 --> 00:31:23,999
I saw a few people starting to die

00:31:21,570 --> 00:31:26,220
but then I don't see your question there

00:31:23,999 --> 00:31:28,919
yet so let's give it another minute

00:31:26,220 --> 00:31:31,350
otherwise yes people might have more

00:31:28,919 --> 00:31:34,379
informal questions and we can use the

00:31:31,350 --> 00:31:36,629
breakout rooms for that I'm really

00:31:34,379 --> 00:31:39,989
curious to hear what people think of my

00:31:36,629 --> 00:31:44,519
opinion around the way to handle water

00:31:39,989 --> 00:31:47,789
marks in idle situations and what they

00:31:44,519 --> 00:31:52,999
are they think of my opinion regarding

00:31:47,789 --> 00:31:58,049
the the idle time at system all right

00:31:52,999 --> 00:32:00,359
there's a question yes max asks

00:31:58,049 --> 00:32:02,669
concerning the slim and slow streams

00:32:00,359 --> 00:32:04,349
wouldn't it suffice to use one

00:32:02,669 --> 00:32:06,359
processing time triggers to trigger a

00:32:04,349 --> 00:32:09,200
window computation from time to time or

00:32:06,359 --> 00:32:12,149
and to add a time-based flushing to the

00:32:09,200 --> 00:32:14,369
HBase output well this is essentially

00:32:12,149 --> 00:32:17,159
what the new feature in flink does the

00:32:14,369 --> 00:32:19,369
first point processing time triggers the

00:32:17,159 --> 00:32:24,059
trigger window computation occasionally

00:32:19,369 --> 00:32:26,669
and my statement is that if you use this

00:32:24,059 --> 00:32:29,940
in a stream that is primarily focused

00:32:26,669 --> 00:32:32,070
around processing then this will work

00:32:29,940 --> 00:32:34,049
fine if you have a stream where the

00:32:32,070 --> 00:32:40,349
computation is based on the event time

00:32:34,049 --> 00:32:44,779
you are mixing the two and and and and

00:32:40,349 --> 00:32:44,779
that is the reason for me to think about

00:32:44,960 --> 00:32:52,619
that it will cause problems because now

00:32:50,159 --> 00:32:55,139
the data is no longer purely based on

00:32:52,619 --> 00:32:57,509
the outcome of the processing is no

00:32:55,139 --> 00:33:00,840
longer purely based on the data but also

00:32:57,509 --> 00:33:04,559
based on the processing of the pipeline

00:33:00,840 --> 00:33:07,080
and depending on the use case that is a

00:33:04,559 --> 00:33:09,330
very important sight point to make

00:33:07,080 --> 00:33:13,909
depending on the use case this may be

00:33:09,330 --> 00:33:13,909
fine and in some cases it won't

00:33:20,310 --> 00:33:22,370

YouTube URL: https://www.youtube.com/watch?v=bQmz7JOmE_4


