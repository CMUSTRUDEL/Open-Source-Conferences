Title: Berlin Buzzwords 2015: Stefan Savev - Using Random Projections to Make Sense of High-Dimensional ...
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	It is hard to understand what is hidden in big high dimensional data. However, a moderate number of simple one dimensional projections is enough to answer hard questions about the data via techniques such as visualization, classification and clustering. Random projections have emerged as an extremely effective component of many algorithms for high dimensional data. For example, they are used in the context of nearest neighbor search (via locality sensitive hashing), dimensionality reduction and clustering. 

The goal of the talk is to give a pleasant journey into the rich area of random projections via many graphical illustrations and intuitive examples. We present how and why random projections work and where they break. We discuss several interesting properties of high dimensional data. For example, why data in high dimensions is likely to look Gaussian when projected in low dimensions; how to spot interesting patterns in high dimensional data by projecting into a lower dimension; and how to choose meaningful low dimensional projections. 

The method of random projections has a number of good properties: 1) scalability; 2) it reduces the machine learning problem to search and can take advantage of existing infrastructure; 3) it is relatively simple to implement and 4) it is robust to noisy data.

Read more:
https://2015.berlinbuzzwords.de/session/using-random-projections-make-sense-high-dimensional-big-data

About Stefan Savev:
https://2015.berlinbuzzwords.de/users/stefan-savev

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
Oh

00:00:06,750 --> 00:00:12,349
hi Jess

00:00:10,280 --> 00:00:14,490
about my

00:00:12,349 --> 00:00:17,260
working for research

00:00:14,490 --> 00:00:21,520
but this doesn't change anything so

00:00:17,260 --> 00:00:25,480
we're still gonna have a great time so

00:00:21,520 --> 00:00:28,660
I'm very excited to stay in front of you

00:00:25,480 --> 00:00:32,469
because I think you are a great audience

00:00:28,660 --> 00:00:34,480
to tell a great story and in this story

00:00:32,469 --> 00:00:36,700
I have incorporated systems from

00:00:34,480 --> 00:00:39,910
algorithm I having incorporated elements

00:00:36,700 --> 00:00:43,329
from systems algorithms a little bit of

00:00:39,910 --> 00:00:45,250
philosophy and even a joke and this is

00:00:43,329 --> 00:00:47,010
because these are multiple projections

00:00:45,250 --> 00:00:51,850
of the world and the talk is about

00:00:47,010 --> 00:00:55,300
random projections the talk is also

00:00:51,850 --> 00:00:58,690
connected to two big data and I think

00:00:55,300 --> 00:01:01,719
big data requires big questions and and

00:00:58,690 --> 00:01:05,440
big thinking so I have a big question

00:01:01,719 --> 00:01:07,720
for you so listen listen well so which

00:01:05,440 --> 00:01:17,110
do you think is harder is it search or

00:01:07,720 --> 00:01:20,830
is it machine learning so we'll revisit

00:01:17,110 --> 00:01:23,350
this question at the end and maybe you

00:01:20,830 --> 00:01:28,390
see another perspective in which you can

00:01:23,350 --> 00:01:31,480
answer this question we'll start with

00:01:28,390 --> 00:01:34,090
search and we start with text search

00:01:31,480 --> 00:01:40,270
because text search is is the most

00:01:34,090 --> 00:01:42,370
popular kind of search and why does text

00:01:40,270 --> 00:01:44,140
search work well it's not only because

00:01:42,370 --> 00:01:46,810
of the systems that have implemented

00:01:44,140 --> 00:01:49,840
text search engine but because of the

00:01:46,810 --> 00:01:53,140
properties of the data and if you think

00:01:49,840 --> 00:01:55,690
of textual data so this data has two

00:01:53,140 --> 00:01:58,000
important properties so text text search

00:01:55,690 --> 00:02:01,270
can work first the data is very very

00:01:58,000 --> 00:02:03,100
sparse 99.9% so this means if you pick

00:02:01,270 --> 00:02:07,150
some document this document is going to

00:02:03,100 --> 00:02:10,569
have 0.1% of the words that are in the

00:02:07,150 --> 00:02:12,790
in the language and then the second

00:02:10,569 --> 00:02:14,739
reason that it works is that when we

00:02:12,790 --> 00:02:17,349
query we query with words which are very

00:02:14,739 --> 00:02:20,290
strong query features okay so this is a

00:02:17,349 --> 00:02:24,010
special data that's why that's why it

00:02:20,290 --> 00:02:28,210
works but if you switch the domain say

00:02:24,010 --> 00:02:28,959
images okay images have have different

00:02:28,210 --> 00:02:32,650
proper

00:02:28,959 --> 00:02:34,480
so first data is not so sparse so for

00:02:32,650 --> 00:02:38,019
example here's the image of the digit 3

00:02:34,480 --> 00:02:40,269
so it has 20% above the possible pixels

00:02:38,019 --> 00:02:42,670
that carry information but you try to

00:02:40,269 --> 00:02:45,040
search similar images to this image now

00:02:42,670 --> 00:02:47,200
you cannot just take 3 pixels now in

00:02:45,040 --> 00:02:49,329
text search you can take 3 words but in

00:02:47,200 --> 00:02:51,730
images you can't take 3 pixels so these

00:02:49,329 --> 00:02:54,609
are properties that are fundamentally

00:02:51,730 --> 00:02:57,939
different and there is basically

00:02:54,609 --> 00:03:01,450
different kind of data okay so to

00:02:57,939 --> 00:03:03,549
position the talk I choose to think of

00:03:01,450 --> 00:03:05,590
two kinds of search one search is for

00:03:03,549 --> 00:03:08,109
sparse data where the query tends to be

00:03:05,590 --> 00:03:11,019
short and this is keyword search solved

00:03:08,109 --> 00:03:13,299
with the inverted index now if your data

00:03:11,019 --> 00:03:15,790
is dense and your query is long for

00:03:13,299 --> 00:03:18,939
example the image it's going to be a

00:03:15,790 --> 00:03:22,870
long query with lots of pixels now this

00:03:18,939 --> 00:03:25,419
requires a different different search

00:03:22,870 --> 00:03:28,419
method and the random projection method

00:03:25,419 --> 00:03:32,500
it turns out that it can work for dense

00:03:28,419 --> 00:03:34,209
data not so good for sparse but there

00:03:32,500 --> 00:03:36,370
are some use cases for the dense data

00:03:34,209 --> 00:03:42,760
including image search semantic search

00:03:36,370 --> 00:03:46,359
and recommendation systems so my my my

00:03:42,760 --> 00:03:49,500
plan is to walk you first through some

00:03:46,359 --> 00:03:51,790
for some basic concepts about

00:03:49,500 --> 00:03:54,579
representation of data and this is

00:03:51,790 --> 00:03:57,190
material we're going to just go through

00:03:54,579 --> 00:03:59,799
to put everybody on the same page and

00:03:57,190 --> 00:04:01,989
because it should be familiar but then

00:03:59,799 --> 00:04:03,790
we revisit this algorithm of random

00:04:01,989 --> 00:04:06,220
projections and third we actually

00:04:03,790 --> 00:04:08,409
demonstrate how it works on on real data

00:04:06,220 --> 00:04:12,189
so let's get started

00:04:08,409 --> 00:04:15,609
so we have an image and image to the

00:04:12,189 --> 00:04:18,159
computer is just pixels and if you are a

00:04:15,609 --> 00:04:21,250
data guy you take the pixels and you and

00:04:18,159 --> 00:04:26,020
you create a vector okay so if these

00:04:21,250 --> 00:04:28,419
Peaks if this image has 800 pixels now

00:04:26,020 --> 00:04:31,210
here would be 800 dimensions so it will

00:04:28,419 --> 00:04:32,949
be a long vector if you work with a data

00:04:31,210 --> 00:04:35,830
set with lots of images you take each

00:04:32,949 --> 00:04:37,300
image make it a vector and then stack

00:04:35,830 --> 00:04:40,659
the vectors on top of each other make

00:04:37,300 --> 00:04:42,230
matrix so this should be this is pretty

00:04:40,659 --> 00:04:44,030
much common currency in the

00:04:42,230 --> 00:04:46,970
the world so people were working with

00:04:44,030 --> 00:04:49,910
matrices so same story for documents you

00:04:46,970 --> 00:04:52,700
can represent them as vectors the

00:04:49,910 --> 00:04:54,440
dimensions are the words but there's a

00:04:52,700 --> 00:04:57,410
fundamental difference to these data is

00:04:54,440 --> 00:05:00,970
sparse okay and the image data was was

00:04:57,410 --> 00:05:04,250
dense if you get another kind of data

00:05:00,970 --> 00:05:06,320
click data from your website it's a gain

00:05:04,250 --> 00:05:08,570
can be represented vector so this vector

00:05:06,320 --> 00:05:11,090
is going to represent the user who

00:05:08,570 --> 00:05:13,160
clicked and the user clicked on the

00:05:11,090 --> 00:05:16,550
first book so we're going to write one

00:05:13,160 --> 00:05:19,400
in this vector okay so two kinds of data

00:05:16,550 --> 00:05:21,530
going to the beginning and dense data

00:05:19,400 --> 00:05:24,140
represent with dense matrix and sparse

00:05:21,530 --> 00:05:26,150
data represented with sparse matrix so

00:05:24,140 --> 00:05:29,540
one you can search with loose in the

00:05:26,150 --> 00:05:31,400
other you cannot search now people in

00:05:29,540 --> 00:05:33,890
especially in recommendation systems

00:05:31,400 --> 00:05:36,770
like to take these sparse matrix and

00:05:33,890 --> 00:05:37,100
make it dance okay so once you make it

00:05:36,770 --> 00:05:41,870
dance

00:05:37,100 --> 00:05:45,590
you cannot searched anymore okay so why

00:05:41,870 --> 00:05:47,780
do people choose to make the sparse

00:05:45,590 --> 00:05:51,470
matrix dance by the way this has a name

00:05:47,780 --> 00:05:53,720
called dimensionality reduction and here

00:05:51,470 --> 00:05:55,970
is a very simple method to accomplish

00:05:53,720 --> 00:05:58,820
dimensionality reduction simply find

00:05:55,970 --> 00:06:00,620
words like beer and wine that are

00:05:58,820 --> 00:06:02,630
correlated or somehow semantically

00:06:00,620 --> 00:06:04,760
similar and they are represented by

00:06:02,630 --> 00:06:06,770
these two columns and then merge that

00:06:04,760 --> 00:06:08,390
comes together now if you merge a lot of

00:06:06,770 --> 00:06:10,490
columns together you're going to end up

00:06:08,390 --> 00:06:14,510
with matrix that is with fewer columns

00:06:10,490 --> 00:06:17,420
but the information density is higher so

00:06:14,510 --> 00:06:20,300
we have condensed your matrix okay so

00:06:17,420 --> 00:06:22,820
why do we want to do this well if this

00:06:20,300 --> 00:06:25,730
example is not convincing let's look

00:06:22,820 --> 00:06:27,860
what happens to two images so we have

00:06:25,730 --> 00:06:30,130
image if you take images and you run

00:06:27,860 --> 00:06:34,160
some dimensionality reduction algorithm

00:06:30,130 --> 00:06:37,340
then this dimensional reduction

00:06:34,160 --> 00:06:40,190
discovers patterns for example four four

00:06:37,340 --> 00:06:42,650
three it discovers that three can be

00:06:40,190 --> 00:06:44,750
created by by this by superimposing

00:06:42,650 --> 00:06:47,240
these two patterns and by the way these

00:06:44,750 --> 00:06:49,220
are not the only patterns that that are

00:06:47,240 --> 00:06:51,500
needed to represent the three but these

00:06:49,220 --> 00:06:53,930
are one of the most dominant so when

00:06:51,500 --> 00:06:55,699
people say dimensionality reduction they

00:06:53,930 --> 00:06:58,249
actually want to do pattern this car

00:06:55,699 --> 00:07:02,990
and when they do this dimensionality

00:06:58,249 --> 00:07:05,810
reduction the data gets dense now we're

00:07:02,990 --> 00:07:08,840
reaching the method that I will talk

00:07:05,810 --> 00:07:12,379
about so it has a fancy name random

00:07:08,840 --> 00:07:14,659
projections and projection this is a

00:07:12,379 --> 00:07:16,729
geometrical notion but if you look at

00:07:14,659 --> 00:07:18,379
the the field of machine learning so

00:07:16,729 --> 00:07:20,960
projection this is basically a dot

00:07:18,379 --> 00:07:24,650
product if you take linear regression so

00:07:20,960 --> 00:07:28,849
linear regression is a projection okay

00:07:24,650 --> 00:07:32,629
if you take a tree it also uses multiple

00:07:28,849 --> 00:07:34,250
projections so a lot of stuff in machine

00:07:32,629 --> 00:07:36,590
learning is about dot products and about

00:07:34,250 --> 00:07:39,020
projections and here it's doing this

00:07:36,590 --> 00:07:40,729
stuff randomly okay so it's really

00:07:39,020 --> 00:07:42,949
surprising that it works if you come

00:07:40,729 --> 00:07:46,250
think this way but also what is

00:07:42,949 --> 00:07:49,490
interesting here is that this is a very

00:07:46,250 --> 00:07:51,500
foundational topic that connects a bunch

00:07:49,490 --> 00:07:53,479
of learning algorithms like random

00:07:51,500 --> 00:07:59,439
forest so are people familiar with

00:07:53,479 --> 00:08:02,659
random forest ok support vector machine

00:07:59,439 --> 00:08:04,129
ok can you connect the random forest to

00:08:02,659 --> 00:08:06,409
the support vector machine these are

00:08:04,129 --> 00:08:09,199
quite different ok

00:08:06,409 --> 00:08:11,569
but there are some recent advances that

00:08:09,199 --> 00:08:14,029
when then they represent the support

00:08:11,569 --> 00:08:15,229
vector machines through random

00:08:14,029 --> 00:08:18,430
projections and you can see some

00:08:15,229 --> 00:08:21,620
connections this is very interesting and

00:08:18,430 --> 00:08:23,060
on the other hand the SVD one of the

00:08:21,620 --> 00:08:25,789
most capable algorithms for

00:08:23,060 --> 00:08:30,159
dimensionality reduction they rely on

00:08:25,789 --> 00:08:32,719
these techniques to make it scalable and

00:08:30,159 --> 00:08:35,120
finally which is the main topic for

00:08:32,719 --> 00:08:37,039
today is the nearest neighbor ok nearest

00:08:35,120 --> 00:08:39,769
neighbor machine learning algorithm but

00:08:37,039 --> 00:08:41,630
it's also a search so this is a machine

00:08:39,769 --> 00:08:43,600
learning which you do through search so

00:08:41,630 --> 00:08:48,250
there are connection between these two

00:08:43,600 --> 00:08:51,380
different fields now in practice

00:08:48,250 --> 00:08:53,149
previous slide I talked about the

00:08:51,380 --> 00:08:55,250
theoretical foundations and the

00:08:53,149 --> 00:09:00,290
theoretical interests about this method

00:08:55,250 --> 00:09:02,360
but in practice some companies have

00:09:00,290 --> 00:09:04,660
incorporated this method into their

00:09:02,360 --> 00:09:08,890
systems for example

00:09:04,660 --> 00:09:10,750
Spotify they developed a tool which is

00:09:08,890 --> 00:09:13,330
available online and which I will

00:09:10,750 --> 00:09:15,700
demonstrate they use it they use this

00:09:13,330 --> 00:09:19,330
method for music recommendations okay

00:09:15,700 --> 00:09:21,700
another company Etsy also uses this as

00:09:19,330 --> 00:09:23,980
part of their pipeline you should also

00:09:21,700 --> 00:09:26,290
know this random projection method

00:09:23,980 --> 00:09:28,570
sometimes is referred as locality

00:09:26,290 --> 00:09:31,089
sensitive hashing okay so this is

00:09:28,570 --> 00:09:33,209
another view there are multiple views of

00:09:31,089 --> 00:09:38,589
this projection one is geometric Oh

00:09:33,209 --> 00:09:40,630
another is hashing and this makes it

00:09:38,589 --> 00:09:43,360
very exciting

00:09:40,630 --> 00:09:46,870
now we've reached the second stop for

00:09:43,360 --> 00:09:49,810
our four for today so how does this

00:09:46,870 --> 00:09:53,050
thing work we just know it's random and

00:09:49,810 --> 00:09:56,019
it's a projection and let's see how it

00:09:53,050 --> 00:09:58,149
works at the high level I said you want

00:09:56,019 --> 00:10:01,750
to search through this matrix so if you

00:09:58,149 --> 00:10:04,269
if the first row could be some user okay

00:10:01,750 --> 00:10:06,430
it's the vector it's a vector which is

00:10:04,269 --> 00:10:08,980
representing a user of your website and

00:10:06,430 --> 00:10:10,600
every row it's a different user so if

00:10:08,980 --> 00:10:13,660
you want to find more similar users you

00:10:10,600 --> 00:10:15,220
can basically do brute force yes just go

00:10:13,660 --> 00:10:19,270
scan through the matrix compare every

00:10:15,220 --> 00:10:21,399
role to the query but what you can think

00:10:19,270 --> 00:10:22,930
here you could and what I'm going to

00:10:21,399 --> 00:10:24,760
show here you could do it a bit smarter

00:10:22,930 --> 00:10:26,490
more efficiently if you take the

00:10:24,760 --> 00:10:29,980
information from this matrix and

00:10:26,490 --> 00:10:35,230
represent it as trees and trees are

00:10:29,980 --> 00:10:39,010
searchable okay let's see how it works

00:10:35,230 --> 00:10:40,899
if you have a data set that has only two

00:10:39,010 --> 00:10:43,930
attributes now we are living in two

00:10:40,899 --> 00:10:48,850
dimensions and two dimensions you can

00:10:43,930 --> 00:10:50,860
visualize and the first row is red it's

00:10:48,850 --> 00:10:54,640
a data point so I'm going to represent

00:10:50,860 --> 00:10:58,089
it here okay it's this red dot now the

00:10:54,640 --> 00:11:02,589
Green Row is a data point here okay so

00:10:58,089 --> 00:11:05,800
every row is a data point now the search

00:11:02,589 --> 00:11:08,860
in this framework is also known as the

00:11:05,800 --> 00:11:11,740
nearest neighbor is the nearest neighbor

00:11:08,860 --> 00:11:13,930
problem and you start with a query you

00:11:11,740 --> 00:11:15,579
want to find the closest point to your

00:11:13,930 --> 00:11:18,010
query so in two dimensions you can

00:11:15,579 --> 00:11:21,000
visualize it but in high dimensions it's

00:11:18,010 --> 00:11:23,639
very difficult and

00:11:21,000 --> 00:11:26,069
so here's the first tip okay instead of

00:11:23,639 --> 00:11:29,699
doing brute force on the whole data set

00:11:26,069 --> 00:11:33,029
we can restrict the region around the

00:11:29,699 --> 00:11:36,629
query and do brute force only in the

00:11:33,029 --> 00:11:40,139
region so there will be two stages one

00:11:36,629 --> 00:11:42,269
stage is candidate selection just select

00:11:40,139 --> 00:11:44,430
somehow find this point in the circle

00:11:42,269 --> 00:11:49,740
and then with brute force evaluate every

00:11:44,430 --> 00:11:53,459
point okay but finding this neighborhood

00:11:49,740 --> 00:11:56,220
is very hard so now we do another

00:11:53,459 --> 00:11:59,160
approximation and the core idea is to

00:11:56,220 --> 00:12:01,980
partition our space into into random

00:11:59,160 --> 00:12:05,339
grits so these grits will be dynamic

00:12:01,980 --> 00:12:08,009
okay so we put more lines more

00:12:05,339 --> 00:12:11,250
partitioning where there are more points

00:12:08,009 --> 00:12:13,199
and we do it randomly so there are a

00:12:11,250 --> 00:12:16,079
bunch of reasons why I want to do it

00:12:13,199 --> 00:12:18,029
like this we can take we can talk later

00:12:16,079 --> 00:12:20,699
but one reason is that random is

00:12:18,029 --> 00:12:22,709
actually cheap okay and later we'll see

00:12:20,699 --> 00:12:27,389
there's also another reason why you want

00:12:22,709 --> 00:12:30,240
to do it randomly okay so let's start I

00:12:27,389 --> 00:12:34,589
put a random hyperplane and I split the

00:12:30,240 --> 00:12:36,449
data set into two okay now going back to

00:12:34,589 --> 00:12:38,970
this foundational concept of the of the

00:12:36,449 --> 00:12:41,430
projection so what is the projection

00:12:38,970 --> 00:12:45,779
well the projection is basically shining

00:12:41,430 --> 00:12:48,569
a light perpendicular to this shining a

00:12:45,779 --> 00:12:52,769
light parallel to the splitting line and

00:12:48,569 --> 00:12:55,769
here it's like you have a random

00:12:52,769 --> 00:12:57,930
direction and you put your data set on

00:12:55,769 --> 00:13:01,019
this random line and this means a

00:12:57,930 --> 00:13:04,529
histogram now in one dimension you can

00:13:01,019 --> 00:13:07,500
look at the date okay so basically this

00:13:04,529 --> 00:13:09,420
is the method it is nothing nothing

00:13:07,500 --> 00:13:12,540
fancy linear regression does like this

00:13:09,420 --> 00:13:14,160
works in exactly the same way it you can

00:13:12,540 --> 00:13:15,839
think okay linear regression with lots

00:13:14,160 --> 00:13:19,019
of attributes but in the end it's going

00:13:15,839 --> 00:13:20,959
to project the data and it's going to

00:13:19,019 --> 00:13:26,790
put the whole data into one dimension

00:13:20,959 --> 00:13:30,809
okay and this reminds me of Plato Eric

00:13:26,790 --> 00:13:32,429
allegory of the cave and here's the

00:13:30,809 --> 00:13:34,220
philosophy component that I put in the

00:13:32,429 --> 00:13:37,280
top so

00:13:34,220 --> 00:13:39,560
Plato wrote this story about the

00:13:37,280 --> 00:13:41,630
prisoners who live in a cave so their

00:13:39,560 --> 00:13:45,320
change in the cave but there's a whole

00:13:41,630 --> 00:13:48,380
world outside the cave which they can

00:13:45,320 --> 00:13:51,410
only only see the shadow of the world

00:13:48,380 --> 00:13:54,200
they can only see the projection of the

00:13:51,410 --> 00:13:56,210
outside world on the cave wall so the

00:13:54,200 --> 00:13:58,430
outside world could be three dimensions

00:13:56,210 --> 00:14:04,160
but they can see only two dimensions so

00:13:58,430 --> 00:14:06,260
the philosopher's goal is is to escape

00:14:04,160 --> 00:14:08,510
from the cave and to understand the

00:14:06,260 --> 00:14:11,150
outside world but they can only see in

00:14:08,510 --> 00:14:13,460
two dimensions and now it's similar for

00:14:11,150 --> 00:14:15,970
the data scientist okay so the data

00:14:13,460 --> 00:14:19,580
scientists can see only this thing

00:14:15,970 --> 00:14:22,160
okay but the good data scientists they

00:14:19,580 --> 00:14:23,870
want to escape from from the cave which

00:14:22,160 --> 00:14:25,850
by the way is known as the curse of

00:14:23,870 --> 00:14:27,340
dimensionality in machine learning so

00:14:25,850 --> 00:14:32,300
they want to escape from the cave and

00:14:27,340 --> 00:14:37,280
and bring the light let's continue with

00:14:32,300 --> 00:14:39,440
the algorithm partitioning into two this

00:14:37,280 --> 00:14:42,440
can be represented as a tree so the red

00:14:39,440 --> 00:14:43,880
points are in the red branch of the tree

00:14:42,440 --> 00:14:46,190
the green points in the green branch of

00:14:43,880 --> 00:14:48,050
the tree now you basically take this

00:14:46,190 --> 00:14:51,440
idea and do it recursively

00:14:48,050 --> 00:14:53,960
okay now you split again we end up with

00:14:51,440 --> 00:14:56,840
four partitions let's run the whole this

00:14:53,960 --> 00:14:59,660
method as simple as it is for a few

00:14:56,840 --> 00:15:01,700
iterations to get a few what happens so

00:14:59,660 --> 00:15:06,740
this is the second iteration we have

00:15:01,700 --> 00:15:09,320
four partitions then third iteration

00:15:06,740 --> 00:15:11,900
eight partitions now sixteen partitions

00:15:09,320 --> 00:15:15,080
it's already very fine-grained so we are

00:15:11,900 --> 00:15:16,760
going to stop and what's the purpose of

00:15:15,080 --> 00:15:18,860
this well the purpose was we started

00:15:16,760 --> 00:15:21,830
with this because we wanted to find the

00:15:18,860 --> 00:15:23,840
nearest neighbors okay so a nearest

00:15:21,830 --> 00:15:26,270
neighbors means close points when two

00:15:23,840 --> 00:15:29,120
points are close they're going to end up

00:15:26,270 --> 00:15:34,100
in the same partition even if you do it

00:15:29,120 --> 00:15:37,100
randomly and I'm going to emphasize this

00:15:34,100 --> 00:15:40,160
concept okay they're likely okay they're

00:15:37,100 --> 00:15:42,340
not certain okay so when something is

00:15:40,160 --> 00:15:45,080
likely to make it certain what you do it

00:15:42,340 --> 00:15:47,330
what you do is you repeat it multiple

00:15:45,080 --> 00:15:48,110
times and now from likely if you repeat

00:15:47,330 --> 00:15:50,540
it enough time

00:15:48,110 --> 00:15:52,640
it becomes certain and you could think

00:15:50,540 --> 00:15:54,980
of each partitioning as exploring the

00:15:52,640 --> 00:15:57,620
space of the possible neighbors so

00:15:54,980 --> 00:16:00,950
here's the red point and we built one

00:15:57,620 --> 00:16:03,380
tree and this tree explored the lower

00:16:00,950 --> 00:16:05,390
left corner of the space okay but you

00:16:03,380 --> 00:16:08,450
see their points that are close to the

00:16:05,390 --> 00:16:09,980
query in the yellow part okay so what

00:16:08,450 --> 00:16:13,160
about this point we're missing on them

00:16:09,980 --> 00:16:15,769
okay then what you do is you run the

00:16:13,160 --> 00:16:19,730
second tree and it explores a bit of

00:16:15,769 --> 00:16:22,370
point some some points in the upper part

00:16:19,730 --> 00:16:25,100
of the space a third tree explores a

00:16:22,370 --> 00:16:26,899
different part of the space so basically

00:16:25,100 --> 00:16:30,740
build more trees that's the conclusion

00:16:26,899 --> 00:16:33,589
and if you run it and visualize it what

00:16:30,740 --> 00:16:37,130
happens how this exploration goals okay

00:16:33,589 --> 00:16:40,640
the more trees you add the more you

00:16:37,130 --> 00:16:43,490
explore around but if you add a lot of

00:16:40,640 --> 00:16:44,899
trees you now you are now certain to

00:16:43,490 --> 00:16:46,610
have explored the nearest neighbor you

00:16:44,899 --> 00:16:50,649
have seen them okay but if you have seen

00:16:46,610 --> 00:16:53,920
too much stuff okay so now you are

00:16:50,649 --> 00:16:59,690
spending too much computation on this

00:16:53,920 --> 00:17:03,140
okay so next we we do a constraint so

00:16:59,690 --> 00:17:05,770
you will we will constraint the we will

00:17:03,140 --> 00:17:09,199
constrain the region in such a way that

00:17:05,770 --> 00:17:12,230
it gets more so we can basically ask

00:17:09,199 --> 00:17:13,910
okay we see neighbors so you run your

00:17:12,230 --> 00:17:17,540
query through the trees and every tree

00:17:13,910 --> 00:17:19,069
comes with with candidates okay so if

00:17:17,540 --> 00:17:21,140
some candidates appear in more trees

00:17:19,069 --> 00:17:24,049
they're basically more likely so this is

00:17:21,140 --> 00:17:27,049
this threshold in how many trees does a

00:17:24,049 --> 00:17:31,250
nearest neighbor candidate appear and if

00:17:27,049 --> 00:17:34,700
you put a high threshold now from this

00:17:31,250 --> 00:17:37,880
this region which is without any

00:17:34,700 --> 00:17:40,370
threshold is going to shrink so now your

00:17:37,880 --> 00:17:42,200
your region of nearest neighbors of

00:17:40,370 --> 00:17:45,410
candidate neighbors shrinks and you

00:17:42,200 --> 00:17:47,720
you're going to to because and you are

00:17:45,410 --> 00:17:49,850
going to to actually because you have to

00:17:47,720 --> 00:17:53,290
evaluate them fully with brute force so

00:17:49,850 --> 00:17:57,080
one this region to shrink and this idea

00:17:53,290 --> 00:17:59,780
it kind of reminds me of this joke about

00:17:57,080 --> 00:18:01,700
about three statisticians so I'm going

00:17:59,780 --> 00:18:03,919
to tell you the joke because I want to

00:18:01,700 --> 00:18:06,320
remember this this method used in this

00:18:03,919 --> 00:18:09,019
algorithm but also this method is in

00:18:06,320 --> 00:18:12,409
many other algorithms so there are these

00:18:09,019 --> 00:18:14,659
three statisticians they go hunting for

00:18:12,409 --> 00:18:17,389
moose the first statistician shoots and

00:18:14,659 --> 00:18:20,149
he misses one meter to the left the

00:18:17,389 --> 00:18:22,820
second statistician shoots he misses one

00:18:20,149 --> 00:18:26,330
meter to the right and the third one

00:18:22,820 --> 00:18:29,149
well you think you'll shoot but he just

00:18:26,330 --> 00:18:36,710
says good job guys we've got him because

00:18:29,149 --> 00:18:39,620
he's averaging so he key point is when

00:18:36,710 --> 00:18:43,240
you go hunting for data you know bring

00:18:39,620 --> 00:18:47,990
some statisticians and you've got him

00:18:43,240 --> 00:18:51,769
okay now we have reached our our third

00:18:47,990 --> 00:18:54,529
stop which is to see how the the method

00:18:51,769 --> 00:18:58,429
that that I described how this method

00:18:54,529 --> 00:19:01,340
actually performs okay I put I choose a

00:18:58,429 --> 00:19:02,870
data set that some people can be

00:19:01,340 --> 00:19:06,289
familiar with this is from the website

00:19:02,870 --> 00:19:08,630
Iago and these data set people used to

00:19:06,289 --> 00:19:11,720
teach themselves machine learning in the

00:19:08,630 --> 00:19:13,100
past people use this data set to develop

00:19:11,720 --> 00:19:16,669
neural networks and other learning

00:19:13,100 --> 00:19:20,210
methods and these data sets so what I'm

00:19:16,669 --> 00:19:24,620
going to do is I'm going to use an image

00:19:20,210 --> 00:19:28,880
search as a prediction system so put an

00:19:24,620 --> 00:19:30,230
image okay this looks like a three could

00:19:28,880 --> 00:19:31,789
be a three could be a five I don't know

00:19:30,230 --> 00:19:38,809
so I want to find the label of this

00:19:31,789 --> 00:19:40,789
imaging and one way to do it is

00:19:38,809 --> 00:19:43,700
basically put it in a search system and

00:19:40,789 --> 00:19:46,669
see what comes what comes he has labels

00:19:43,700 --> 00:19:48,649
from our training set and then we say

00:19:46,669 --> 00:19:51,860
okay the first game that is a five so

00:19:48,649 --> 00:19:54,409
I'm gonna say this is a five so machine

00:19:51,860 --> 00:19:57,409
learning actually works with two data

00:19:54,409 --> 00:19:59,570
sets the test data set and the training

00:19:57,409 --> 00:20:01,460
date the the training dataset and the

00:19:59,570 --> 00:20:03,049
test dataset now because we're doing

00:20:01,460 --> 00:20:04,070
machine learning with with a search the

00:20:03,049 --> 00:20:06,919
so called nearest-neighbor

00:20:04,070 --> 00:20:09,679
we have to do some indexing so indexing

00:20:06,919 --> 00:20:12,379
means take the data and put it into an

00:20:09,679 --> 00:20:14,779
index so roughly it looks something like

00:20:12,379 --> 00:20:15,440
this python code so the key idea is that

00:20:14,779 --> 00:20:20,000
every

00:20:15,440 --> 00:20:24,529
which it has a vector okay which we use

00:20:20,000 --> 00:20:25,940
for indexing and it has a label which we

00:20:24,529 --> 00:20:30,320
remember because we're gonna need the

00:20:25,940 --> 00:20:32,870
label later now for stage two the I

00:20:30,320 --> 00:20:34,820
already described but you put in an

00:20:32,870 --> 00:20:38,690
image to search and you get the label of

00:20:34,820 --> 00:20:41,809
the of the top result okay and this is

00:20:38,690 --> 00:20:43,929
Python code that looks like this by the

00:20:41,809 --> 00:20:48,740
way this is very similar to the Spotify

00:20:43,929 --> 00:20:54,769
too which uses Python so this code is

00:20:48,740 --> 00:20:57,919
very close to what you can run now I use

00:20:54,769 --> 00:21:00,529
the Spotify to mainly but I also

00:20:57,919 --> 00:21:02,509
developed a tool on my own because I

00:21:00,529 --> 00:21:04,879
want to get deep you know and make sure

00:21:02,509 --> 00:21:06,230
that I understand everything because

00:21:04,879 --> 00:21:07,909
when I come here and present you're

00:21:06,230 --> 00:21:09,379
going to ask me tough questions I have

00:21:07,909 --> 00:21:11,840
to understand everything so I developed

00:21:09,379 --> 00:21:16,789
it on my own okay so I'm going to see

00:21:11,840 --> 00:21:23,000
these two tools and this is also a open

00:21:16,789 --> 00:21:25,759
source so Randy Spotify tool okay so the

00:21:23,000 --> 00:21:30,580
Spotify tool in the Spotify tool I built

00:21:25,759 --> 00:21:33,169
ten trees okay and each tree explores a

00:21:30,580 --> 00:21:34,610
thousand candidates so this is a

00:21:33,169 --> 00:21:37,600
thousand candidates that need to be

00:21:34,610 --> 00:21:42,519
related fully it has a decent accuracy

00:21:37,600 --> 00:21:45,980
ninety seven percent but it's very slow

00:21:42,519 --> 00:21:48,980
okay five milliseconds per query okay

00:21:45,980 --> 00:21:51,259
you can do better so how do you do

00:21:48,980 --> 00:21:53,570
better where we take this dimension and

00:21:51,259 --> 00:21:55,429
you bring it down using this

00:21:53,570 --> 00:21:57,230
dimensionality reduction technique and

00:21:55,429 --> 00:21:59,509
this is another bonus for this

00:21:57,230 --> 00:22:02,389
dimensionality reduction technique that

00:21:59,509 --> 00:22:03,860
it makes cheaper the data smaller and it

00:22:02,389 --> 00:22:06,860
so it's cheaper to compute with this

00:22:03,860 --> 00:22:10,250
data and once you did this okay

00:22:06,860 --> 00:22:13,789
the search time went down 0.7

00:22:10,250 --> 00:22:16,190
milliseconds per query but also the

00:22:13,789 --> 00:22:18,169
accuracy went up and why is that because

00:22:16,190 --> 00:22:20,809
dimensionality reduction as I said in

00:22:18,169 --> 00:22:23,330
the beginning it discovers patrons so

00:22:20,809 --> 00:22:24,919
the first line it worked at the pixel

00:22:23,330 --> 00:22:27,529
level while the second line you could

00:22:24,919 --> 00:22:29,350
think it it worked at the concept level

00:22:27,529 --> 00:22:32,650
so this concept layer is

00:22:29,350 --> 00:22:35,530
here is some patches from the image okay

00:22:32,650 --> 00:22:37,419
now there's another bottleneck we

00:22:35,530 --> 00:22:41,080
basically valuate fully to many

00:22:37,419 --> 00:22:48,190
candidates okay and here I put my tool

00:22:41,080 --> 00:22:52,270
and I got the time roughly half of what

00:22:48,190 --> 00:22:58,059
I had before but I had to build more

00:22:52,270 --> 00:23:01,600
trees now to be fair so it could be that

00:22:58,059 --> 00:23:05,080
I didn't really do a lot of a lot of

00:23:01,600 --> 00:23:07,120
tweaking on the on the Spotify tool it

00:23:05,080 --> 00:23:10,690
could be that you could tweak it and

00:23:07,120 --> 00:23:13,659
reduce these candidates and you and

00:23:10,690 --> 00:23:17,250
these two could really even get lower so

00:23:13,659 --> 00:23:20,169
this is this is quite quite possible but

00:23:17,250 --> 00:23:22,270
at the main point actually here is not

00:23:20,169 --> 00:23:24,760
about whether one two is better or not

00:23:22,270 --> 00:23:29,679
so it's more about some algorithmic

00:23:24,760 --> 00:23:31,360
heuristics and the there is some some

00:23:29,679 --> 00:23:37,480
difference in the algorithms of these

00:23:31,360 --> 00:23:39,100
two but kind of small difference and the

00:23:37,480 --> 00:23:42,130
main point I want to carry across is

00:23:39,100 --> 00:23:44,710
that okay if you tweak a system like

00:23:42,130 --> 00:23:48,700
this you are likely to get good

00:23:44,710 --> 00:23:52,390
performance and in many cases you're

00:23:48,700 --> 00:23:54,280
likely to get good takers okay now we've

00:23:52,390 --> 00:23:56,350
reached the question I asked from the

00:23:54,280 --> 00:23:58,990
beginning so which is harder is it

00:23:56,350 --> 00:24:02,679
search or or machine learning and

00:23:58,990 --> 00:24:06,070
hearing this talk we present I presented

00:24:02,679 --> 00:24:10,980
a system that searches dense data dense

00:24:06,070 --> 00:24:14,530
vectors using these trees and this

00:24:10,980 --> 00:24:15,940
indexing structure it looks the

00:24:14,530 --> 00:24:19,419
following we have the tree and at the

00:24:15,940 --> 00:24:21,039
bottom you have the point IDs okay so

00:24:19,419 --> 00:24:23,440
when you search you're walking these

00:24:21,039 --> 00:24:25,510
trees and you find some some candidate

00:24:23,440 --> 00:24:27,789
points in you are evaluating them okay

00:24:25,510 --> 00:24:32,940
and if you do machine learning you could

00:24:27,789 --> 00:24:35,669
use the same thing however you could be

00:24:32,940 --> 00:24:39,190
you don't have to store the food data

00:24:35,669 --> 00:24:40,780
okay you could just what is important

00:24:39,190 --> 00:24:42,940
for machine learning is predicting the

00:24:40,780 --> 00:24:45,850
label so this label can

00:24:42,940 --> 00:24:48,040
from some many neighbors and we can

00:24:45,850 --> 00:24:49,960
aggregate so this is a key point here

00:24:48,040 --> 00:24:54,400
that in the leaves we could store a

00:24:49,960 --> 00:24:56,560
histogram so in this way of thinking you

00:24:54,400 --> 00:24:59,320
could think okay this is search ok

00:24:56,560 --> 00:25:02,470
search has to store more data and when

00:24:59,320 --> 00:25:06,130
it searches it has to be precise and it

00:25:02,470 --> 00:25:07,930
really cannot borrow what's called

00:25:06,130 --> 00:25:09,880
statistical strength from from many

00:25:07,930 --> 00:25:12,100
other points but if you do machine

00:25:09,880 --> 00:25:14,380
learning okay you can aggregate your

00:25:12,100 --> 00:25:17,020
data so this is cheaper and because you

00:25:14,380 --> 00:25:19,210
you borrow knowledge from many data

00:25:17,020 --> 00:25:21,400
points aggregating this histogram it's

00:25:19,210 --> 00:25:24,880
actually it looks like machine learning

00:25:21,400 --> 00:25:30,580
is easier than search so this is my take

00:25:24,880 --> 00:25:33,730
away point and in conclusion I would say

00:25:30,580 --> 00:25:36,880
that this method is useful when you

00:25:33,730 --> 00:25:38,410
search for for in dense data and the

00:25:36,880 --> 00:25:44,010
other application is that other

00:25:38,410 --> 00:25:50,160
algorithms are using it to become faster

00:25:44,010 --> 00:25:50,160
and this is the end thank you very much

00:25:54,150 --> 00:25:56,210

YouTube URL: https://www.youtube.com/watch?v=V9zl09w1SGM


