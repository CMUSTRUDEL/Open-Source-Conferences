Title: Berlin Buzzwords 2015: Ameya Kanitkar - Real Time Big Data Analytics with Kafka, Storm, HBase #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Relevance and Personalization is crucial to building personalized local commerce experience at Groupon. We have built infrastructure that processes real time user interaction stream and produces personalized real time analytics that are further enhanced to present relevant personalized experience to hundreds of millions of users of Groupon across the world. 

This talk covers the use case and use of our Kafka-Storm-HBase-Redis pipeline to ingest over 3 million data points per second in real time which in turn brings in millions of dollars in additional revenue. Specially we will discuss how we scaled this system for hundreds of millions of users including solution choices, different techniques and strategies, traditional and innovative approaches. 

Solution includes some interesting algorithmic choices to reduce data size such as bloom filters and HyperLogLog, as well as use of big data technologies such as HBase, Kafka & Storm. Attendees can take away learnings from our real-life experience that can help them understand various tuning methods, their tradeoffs and apply them in their solutions.

Read more:
https://2015.berlinbuzzwords.de/session/real-time-big-data-analytics-kafka-storm-hbase

About Ameya Kanitkar:
https://2015.berlinbuzzwords.de/users/ameya-kanitkar

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,359 --> 00:00:12,380
hi Thank thanks for your introduction I

00:00:10,049 --> 00:00:15,740
just want to get sense or

00:00:12,380 --> 00:00:19,160
who all are in the audience how many of

00:00:15,740 --> 00:00:22,430
you are from Berlin all right

00:00:19,160 --> 00:00:26,270
interesting okay and how many of you are

00:00:22,430 --> 00:00:28,070
aware of the big data stuff like our at

00:00:26,270 --> 00:00:31,939
least have worked on like Hadoop or

00:00:28,070 --> 00:00:35,180
hbase or storm or Kafka like cool lot of

00:00:31,939 --> 00:00:41,630
you cool awesome that makes my life

00:00:35,180 --> 00:00:44,870
easier let's see so let me introduce

00:00:41,630 --> 00:00:47,270
myself a little bit more or I'll just

00:00:44,870 --> 00:00:49,900
wait for a few more seconds till people

00:00:47,270 --> 00:00:49,900
settle in

00:00:55,970 --> 00:01:03,990
one more question how many of you know

00:00:58,380 --> 00:01:06,000
Groupon no I'm not advertising but that

00:01:03,990 --> 00:01:09,570
way I can skip some of the parts and go

00:01:06,000 --> 00:01:11,850
to the main stuff but now I'm

00:01:09,570 --> 00:01:15,510
advertising advertise we have office

00:01:11,850 --> 00:01:19,530
here in Berlin groupon office but I i

00:01:15,510 --> 00:01:24,240
work in San Francisco office and all

00:01:19,530 --> 00:01:25,920
right I'll start now so I'm a lead

00:01:24,240 --> 00:01:30,270
engineer on the real-time infrastructure

00:01:25,920 --> 00:01:32,340
at Groupon so let me go back a little

00:01:30,270 --> 00:01:35,280
bit we we are about six year old now

00:01:32,340 --> 00:01:37,170
Groupon and we have been through you

00:01:35,280 --> 00:01:40,500
know sort of lot of changes you know you

00:01:37,170 --> 00:01:42,660
call them p votes or whatever but when

00:01:40,500 --> 00:01:44,700
we started groupon started with a very

00:01:42,660 --> 00:01:48,119
simple model like something like this

00:01:44,700 --> 00:01:51,450
you know you have one deal a day this is

00:01:48,119 --> 00:01:54,300
some some kiteboarding deal or something

00:01:51,450 --> 00:01:56,250
like that and what will do is that will

00:01:54,300 --> 00:01:58,860
have one dealer day it will be for like

00:01:56,250 --> 00:02:02,009
fifty dollars 104 $50 or something like

00:01:58,860 --> 00:02:05,340
that and we'll send out this deal to all

00:02:02,009 --> 00:02:07,560
our users that day and a lot of people

00:02:05,340 --> 00:02:09,990
will buy them and lot of people will not

00:02:07,560 --> 00:02:13,830
buy them right and as groupon grew very

00:02:09,990 --> 00:02:15,900
quickly we we are mass lot of users in a

00:02:13,830 --> 00:02:18,330
lot of cities so to give you an example

00:02:15,900 --> 00:02:20,340
in San Francisco we had over a million

00:02:18,330 --> 00:02:22,890
users in I don't know like under eight

00:02:20,340 --> 00:02:26,430
months or something like that so what

00:02:22,890 --> 00:02:28,200
what that means is that you know it kind

00:02:26,430 --> 00:02:30,450
of became much harder to scale at that

00:02:28,200 --> 00:02:31,890
point and it created short of a lose

00:02:30,450 --> 00:02:34,560
lose lose situation from a business

00:02:31,890 --> 00:02:36,630
standpoint and let me explain you how so

00:02:34,560 --> 00:02:39,180
let's say we ran a massage deal and

00:02:36,630 --> 00:02:42,270
we'll sell say whatever fifty-dollar

00:02:39,180 --> 00:02:44,490
massage for 50 euro massage for 100 euro

00:02:42,270 --> 00:02:47,400
something like that to a million people

00:02:44,490 --> 00:02:48,569
in say Berlin and what would happen is

00:02:47,400 --> 00:02:50,370
that you have a lot of them it's

00:02:48,569 --> 00:02:52,170
irrelevant deal right so they they will

00:02:50,370 --> 00:02:55,260
stops opening their emails or something

00:02:52,170 --> 00:02:56,459
like that but out of a million users you

00:02:55,260 --> 00:02:58,470
know lot of people will actually buy

00:02:56,459 --> 00:03:01,290
them right so let's say we sold seven

00:02:58,470 --> 00:03:03,450
thousand massage or spa appointments now

00:03:01,290 --> 00:03:04,590
the the small business that is going to

00:03:03,450 --> 00:03:05,970
serve those appointments they're not

00:03:04,590 --> 00:03:07,530
going to sell it they're not going to

00:03:05,970 --> 00:03:08,040
serve that many appointments easily

00:03:07,530 --> 00:03:10,319
right

00:03:08,040 --> 00:03:12,930
so it's bad for the the business that we

00:03:10,319 --> 00:03:14,099
are doing group 14 it's bad for

00:03:12,930 --> 00:03:16,709
customers because they're getting a lot

00:03:14,099 --> 00:03:18,750
of irrelevant emails and it's bad for

00:03:16,709 --> 00:03:21,120
Groupon because we could only run one

00:03:18,750 --> 00:03:23,430
deal a day in a large city like Berlin

00:03:21,120 --> 00:03:24,900
or bad city like San Francisco so this

00:03:23,430 --> 00:03:28,829
became kind of a lose lose lose

00:03:24,900 --> 00:03:31,109
situation and we had to come up with

00:03:28,829 --> 00:03:35,489
something so that you know it becomes

00:03:31,109 --> 00:03:37,530
much easier for everybody so one idea of

00:03:35,489 --> 00:03:38,939
doing that was okay let's instead of

00:03:37,530 --> 00:03:42,329
running one dealer day let's run

00:03:38,939 --> 00:03:44,250
multiple density right so that idea

00:03:42,329 --> 00:03:45,629
sounded easy the problem is that you

00:03:44,250 --> 00:03:49,200
know in order to do that you know you

00:03:45,629 --> 00:03:51,689
have to match the deals to the right

00:03:49,200 --> 00:03:53,220
users right if you have there if you can

00:03:51,689 --> 00:03:55,829
match the right deal to the right user

00:03:53,220 --> 00:03:58,200
then we come from that lose lose lose

00:03:55,829 --> 00:04:00,629
situation I just explained to sort of a

00:03:58,200 --> 00:04:02,669
win-win-win situation so the users will

00:04:00,629 --> 00:04:04,680
get more relevant deals the business can

00:04:02,669 --> 00:04:08,370
handle only as many customers as they

00:04:04,680 --> 00:04:10,319
want to handle or as few as a more or

00:04:08,370 --> 00:04:13,799
less right and for Groupon we could

00:04:10,319 --> 00:04:15,959
scale with more deals without

00:04:13,799 --> 00:04:19,440
necessarily going into more cities right

00:04:15,959 --> 00:04:21,810
so great idea so the reason I'm

00:04:19,440 --> 00:04:24,030
emphasizing on this is because you know

00:04:21,810 --> 00:04:25,800
for a lot of other businesses the

00:04:24,030 --> 00:04:28,380
relevance or recommendation systems are

00:04:25,800 --> 00:04:30,539
kind of an optimization layer on top of

00:04:28,380 --> 00:04:31,740
their business model maybe for Amazon

00:04:30,539 --> 00:04:34,020
like because you also bought or

00:04:31,740 --> 00:04:35,550
something like that but for Groupon it's

00:04:34,020 --> 00:04:38,669
a fundamental part of our business model

00:04:35,550 --> 00:04:42,599
itself without these systems groupon

00:04:38,669 --> 00:04:44,190
cannot exist right all right cool so in

00:04:42,599 --> 00:04:45,840
order to do that you know what are the

00:04:44,190 --> 00:04:49,289
things we need to build so that we could

00:04:45,840 --> 00:04:52,889
build this relevant system there are

00:04:49,289 --> 00:04:57,720
sort of two parts to it one is that you

00:04:52,889 --> 00:04:59,729
know when you if you build a good graph

00:04:57,720 --> 00:05:02,000
about users like what what those users

00:04:59,729 --> 00:05:05,370
are what each user likes what their

00:05:02,000 --> 00:05:07,289
preferences are and you also go and

00:05:05,370 --> 00:05:09,770
build a good understanding about the

00:05:07,289 --> 00:05:12,599
deals like what kind of a deal it is

00:05:09,770 --> 00:05:15,900
where it's located what price it is what

00:05:12,599 --> 00:05:18,630
category it is you know etcetera

00:05:15,900 --> 00:05:21,760
etcetera then if then then if you match

00:05:18,630 --> 00:05:22,990
these users to these deals then we can

00:05:21,760 --> 00:05:24,760
get started with some sort of irrelevant

00:05:22,990 --> 00:05:27,400
system right so those are the

00:05:24,760 --> 00:05:29,620
fundamental parts of our 11th scenario

00:05:27,400 --> 00:05:31,420
where you build some understanding about

00:05:29,620 --> 00:05:35,770
users and you build some understanding

00:05:31,420 --> 00:05:37,720
about deals the other part of this is

00:05:35,770 --> 00:05:39,670
that once you build that there are two

00:05:37,720 --> 00:05:42,480
sort of systems from which we have to

00:05:39,670 --> 00:05:45,340
deliver these recommendations or

00:05:42,480 --> 00:05:48,610
personalization aspects right one is the

00:05:45,340 --> 00:05:52,300
email which groupon relied on for a

00:05:48,610 --> 00:05:55,330
first half of its life all we still rely

00:05:52,300 --> 00:05:57,070
on it but it's much less now so that was

00:05:55,330 --> 00:06:00,340
the email system or we call it offline

00:05:57,070 --> 00:06:01,540
system so every day will will go and you

00:06:00,340 --> 00:06:03,820
know generate new emails new

00:06:01,540 --> 00:06:05,260
personalized emails for everyone and

00:06:03,820 --> 00:06:07,450
will compute you know what would be the

00:06:05,260 --> 00:06:10,180
best deal for you amongst the deals we

00:06:07,450 --> 00:06:11,860
have and the other scenario is you open

00:06:10,180 --> 00:06:14,020
your groupon app or you could go to a

00:06:11,860 --> 00:06:16,390
groupon website and then that particular

00:06:14,020 --> 00:06:18,060
experience is personalized for you so

00:06:16,390 --> 00:06:20,290
that's more of a real-time

00:06:18,060 --> 00:06:24,880
personalization system right so we had

00:06:20,290 --> 00:06:26,320
sort of two systems so I'll give you a

00:06:24,880 --> 00:06:28,180
little bit of a history on like how

00:06:26,320 --> 00:06:30,490
these systems started and this mostly

00:06:28,180 --> 00:06:32,350
this talk is how we moved from the older

00:06:30,490 --> 00:06:36,970
system we had to the new system we have

00:06:32,350 --> 00:06:39,010
now which is more real-time so about two

00:06:36,970 --> 00:06:40,930
years ago we we had something system

00:06:39,010 --> 00:06:44,740
something like this most of our business

00:06:40,930 --> 00:06:46,570
was relied on emails so we had this

00:06:44,740 --> 00:06:48,280
pipeline of you know all this data

00:06:46,570 --> 00:06:51,100
pipeline which will bring all the data

00:06:48,280 --> 00:06:53,590
about users email records what emails we

00:06:51,100 --> 00:06:55,900
have sent you which what emails you have

00:06:53,590 --> 00:06:58,150
opened which you have not opened what

00:06:55,900 --> 00:07:01,810
things you have clicked on so on and so

00:06:58,150 --> 00:07:03,850
forth and data about deals and that data

00:07:01,810 --> 00:07:06,280
pipeline would bring all this data would

00:07:03,850 --> 00:07:08,290
put this into a Hadoop system and in

00:07:06,280 --> 00:07:11,640
Hadoop we'll run some MapReduce jobs to

00:07:08,290 --> 00:07:14,290
compute what goes into your email and

00:07:11,640 --> 00:07:17,050
those emails will be sent out next day

00:07:14,290 --> 00:07:20,050
so this was kind of a delayed pipeline

00:07:17,050 --> 00:07:21,820
about a day worth of delay this use this

00:07:20,050 --> 00:07:24,520
used to take quite a long time I think

00:07:21,820 --> 00:07:26,800
like 12 or 15 our jobs are not quite a

00:07:24,520 --> 00:07:30,130
large cluster and then emails will be

00:07:26,800 --> 00:07:33,850
sent out a real time scenario was a far

00:07:30,130 --> 00:07:35,569
less advanced so what we used to do is

00:07:33,850 --> 00:07:37,939
that we will compute some of this

00:07:35,569 --> 00:07:41,240
part of this was dumped into my sickle

00:07:37,939 --> 00:07:46,369
store and then we had some API on top

00:07:41,240 --> 00:07:50,869
that would serve out the the app the the

00:07:46,369 --> 00:07:52,789
website or the app the two more things

00:07:50,869 --> 00:07:54,709
happen during this time one is that you

00:07:52,789 --> 00:07:57,919
know when we built this system you know

00:07:54,709 --> 00:08:00,949
this was more like 2012 scenario when we

00:07:57,919 --> 00:08:02,809
built that this so this is how 2012 look

00:08:00,949 --> 00:08:05,719
for us you know like very few deals like

00:08:02,809 --> 00:08:07,999
we moved away from daily deal to a few

00:08:05,719 --> 00:08:10,849
more deals right but we still had you

00:08:07,999 --> 00:08:13,189
know like like eight or ten thousand is

00:08:10,849 --> 00:08:14,959
total on our platform right in a given

00:08:13,189 --> 00:08:17,569
city we would only have you know like 20

00:08:14,959 --> 00:08:20,689
days or something like that but that

00:08:17,569 --> 00:08:23,330
moved too much longer so today we have I

00:08:20,689 --> 00:08:25,459
think over 200,000 deals and there may

00:08:23,330 --> 00:08:28,639
be more now and some of the cities have

00:08:25,459 --> 00:08:30,589
even thousands of dates so we had to

00:08:28,639 --> 00:08:32,990
scale on the on the business side where

00:08:30,589 --> 00:08:35,180
the structure of the problem is change

00:08:32,990 --> 00:08:36,709
earlier problem was given this user and

00:08:35,180 --> 00:08:38,959
out of 10 deals which is the best deal

00:08:36,709 --> 00:08:40,819
to send right now the problem is given

00:08:38,959 --> 00:08:42,589
this user and is hundreds of thousands

00:08:40,819 --> 00:08:44,389
of deals which is the best deal to

00:08:42,589 --> 00:08:46,699
choose for this user right which is a

00:08:44,389 --> 00:08:47,750
which is not scaling from 0 to a million

00:08:46,699 --> 00:08:50,870
or something but it's still

00:08:47,750 --> 00:08:53,360
significantly different and what we

00:08:50,870 --> 00:08:54,500
initially started out with because we

00:08:53,360 --> 00:08:56,839
initially started out with some

00:08:54,500 --> 00:08:59,000
something simple like okay just for loop

00:08:56,839 --> 00:09:00,769
over all the deals score each deal and

00:08:59,000 --> 00:09:01,819
the best deal is at the top right but

00:09:00,769 --> 00:09:03,350
when you have hundreds of thousands of

00:09:01,819 --> 00:09:07,250
deals you can't just for loop over it

00:09:03,350 --> 00:09:09,230
right the other other fundamental

00:09:07,250 --> 00:09:12,079
business change happened was that people

00:09:09,230 --> 00:09:15,380
started moving from email to web app or

00:09:12,079 --> 00:09:17,449
mobile right the mobile change so today

00:09:15,380 --> 00:09:18,800
we have over 110 million subscribers but

00:09:17,449 --> 00:09:20,750
what that means is that people are no

00:09:18,800 --> 00:09:22,939
longer you know like opening their

00:09:20,750 --> 00:09:25,459
emails or not opening as much of emails

00:09:22,939 --> 00:09:28,040
but rather checking things on the app so

00:09:25,459 --> 00:09:29,839
what that means this crappy architecture

00:09:28,040 --> 00:09:32,300
for real-time scenario wouldn't work

00:09:29,839 --> 00:09:34,759
right where I'll get into some of the

00:09:32,300 --> 00:09:36,230
issues with this but the clear problem

00:09:34,759 --> 00:09:40,459
here is this my sequel guy right which

00:09:36,230 --> 00:09:42,829
doesn't scale much so in order to keep

00:09:40,459 --> 00:09:44,899
users engage we have to have much better

00:09:42,829 --> 00:09:49,329
real-time personalization scenario

00:09:44,899 --> 00:09:49,329
otherwise we'll be in bit of a trouble

00:09:49,410 --> 00:09:54,160
it's the same thing grows with mobile

00:09:52,089 --> 00:09:56,380
business reduce dependence on email

00:09:54,160 --> 00:09:59,200
marketing change in strategy from daily

00:09:56,380 --> 00:10:00,940
deal market Louis to the reason I'm

00:09:59,200 --> 00:10:02,290
explaining this is lot of times what

00:10:00,940 --> 00:10:03,820
happens you have systems and and

00:10:02,290 --> 00:10:06,730
business changes and then you have to

00:10:03,820 --> 00:10:08,620
have you have to respond how you change

00:10:06,730 --> 00:10:12,070
your technology to respond to your

00:10:08,620 --> 00:10:13,600
business right so this is kind of a okay

00:10:12,070 --> 00:10:16,120
so what were the issues with the old

00:10:13,600 --> 00:10:18,910
system the first issue is this data

00:10:16,120 --> 00:10:21,250
pipeline which is if you open an email

00:10:18,910 --> 00:10:22,959
or if you click on some web page or

00:10:21,250 --> 00:10:24,880
something like that we wouldn't know

00:10:22,959 --> 00:10:27,190
about it for almost a day or even

00:10:24,880 --> 00:10:30,640
sometimes longer right so the data comes

00:10:27,190 --> 00:10:32,920
a day later and then we compute emails

00:10:30,640 --> 00:10:35,320
for you which takes another 14 15 hours

00:10:32,920 --> 00:10:37,540
and then we send out an email so if you

00:10:35,320 --> 00:10:39,910
do some actions on Monday you won't see

00:10:37,540 --> 00:10:41,680
a result of for which on until wednesday

00:10:39,910 --> 00:10:43,480
right so many times the deals are

00:10:41,680 --> 00:10:46,779
expired by that time so it's kind of

00:10:43,480 --> 00:10:49,660
useless the other thing is this my

00:10:46,779 --> 00:10:51,640
sequel store right so it had some basic

00:10:49,660 --> 00:10:53,470
data like some gender and location and

00:10:51,640 --> 00:10:57,579
something like that but it was very hard

00:10:53,470 --> 00:10:59,860
to go scale this I mean you could but at

00:10:57,579 --> 00:11:02,550
least for us out of the box to scale my

00:10:59,860 --> 00:11:05,410
sequel as is to Euro 200 million users

00:11:02,550 --> 00:11:08,740
with all the real-time aspects that we

00:11:05,410 --> 00:11:10,750
wanted to bring in so this this system

00:11:08,740 --> 00:11:14,079
is not scaling business is changing we

00:11:10,750 --> 00:11:16,300
have to do something all right so we

00:11:14,079 --> 00:11:18,730
started out with thinking okay what what

00:11:16,300 --> 00:11:20,980
would be the ideal system you know if we

00:11:18,730 --> 00:11:22,779
want to build this again and if you want

00:11:20,980 --> 00:11:25,420
to build this right what how would it

00:11:22,779 --> 00:11:28,870
look like that would scale for us so

00:11:25,420 --> 00:11:31,510
here are sort of our wish list so a

00:11:28,870 --> 00:11:33,820
common data store that serves both

00:11:31,510 --> 00:11:35,350
online and offline systems so remember

00:11:33,820 --> 00:11:37,329
here the problem is that we have this

00:11:35,350 --> 00:11:41,350
Hadoop system which is completely

00:11:37,329 --> 00:11:43,360
disconnected to the real-time system so

00:11:41,350 --> 00:11:45,130
our wish list is ok we don't want like

00:11:43,360 --> 00:11:48,430
two separate pipelines one going for

00:11:45,130 --> 00:11:52,120
emails one going for app let's let's

00:11:48,430 --> 00:11:53,529
have a common one well data store that

00:11:52,120 --> 00:11:56,350
scales to hundreds of millions of

00:11:53,529 --> 00:11:59,829
Records which was this my sequel issue i

00:11:56,350 --> 00:12:01,570
have mentioned earlier data store that

00:11:59,829 --> 00:12:02,440
plays well with our existing hadoop

00:12:01,570 --> 00:12:04,780
based system so

00:12:02,440 --> 00:12:06,310
so we had a lot of data scientists and

00:12:04,780 --> 00:12:07,780
engineers who had written a lot of

00:12:06,310 --> 00:12:09,760
algorithms we didn't want to just throw

00:12:07,780 --> 00:12:12,040
it away right and that that was written

00:12:09,760 --> 00:12:13,480
in Java MapReduce kind of a system so we

00:12:12,040 --> 00:12:15,220
don't want to you know just completely

00:12:13,480 --> 00:12:17,620
just throw that away and start from

00:12:15,220 --> 00:12:20,020
scratch so let's build a system that

00:12:17,620 --> 00:12:22,600
plays well with that infrastructure and

00:12:20,020 --> 00:12:24,370
we want this to be real time right we

00:12:22,600 --> 00:12:26,380
don't want this add a delay you know if

00:12:24,370 --> 00:12:28,660
you make certain actions on the previous

00:12:26,380 --> 00:12:30,460
page the next time you go to the website

00:12:28,660 --> 00:12:33,010
or the next page you go to we want that

00:12:30,460 --> 00:12:34,840
to be incorporated in our algorithms

00:12:33,010 --> 00:12:37,450
ride or in at least are in our

00:12:34,840 --> 00:12:39,130
experience so what that also means is

00:12:37,450 --> 00:12:41,590
that we should be able to handle at

00:12:39,130 --> 00:12:44,140
about one hundred thousand messages per

00:12:41,590 --> 00:12:45,490
second right all right cool those are

00:12:44,140 --> 00:12:49,450
that's a wish list it's good to start

00:12:45,490 --> 00:12:51,640
with a wish list so this is what we came

00:12:49,450 --> 00:12:54,580
up with and I won't spend a lot of time

00:12:51,640 --> 00:12:56,560
on explaining why certain components we

00:12:54,580 --> 00:12:59,500
used and not the other computing

00:12:56,560 --> 00:13:01,600
components but i'll tell you like now we

00:12:59,500 --> 00:13:05,200
have we are using these components what

00:13:01,600 --> 00:13:07,180
were the issues in scaling this and what

00:13:05,200 --> 00:13:10,090
are some of the learnings we had right

00:13:07,180 --> 00:13:13,150
so let me explain how how what this

00:13:10,090 --> 00:13:15,040
system is about so we have these website

00:13:13,150 --> 00:13:19,150
logs and mobile logs and also the email

00:13:15,040 --> 00:13:22,300
logs and they go into Kafka and from

00:13:19,150 --> 00:13:24,520
Kafka we read into storm so storm is a

00:13:22,300 --> 00:13:27,010
real-time sort of a processing system

00:13:24,520 --> 00:13:32,250
Kafka's like a messaging system for

00:13:27,010 --> 00:13:34,810
locks and we dump them into HBase and we

00:13:32,250 --> 00:13:36,760
we would we would write some memories

00:13:34,810 --> 00:13:38,650
are algorithms that we were written

00:13:36,760 --> 00:13:42,820
MapReduce they could run on this data

00:13:38,650 --> 00:13:46,240
insert HBase and generate emails and for

00:13:42,820 --> 00:13:48,130
online online personalization that data

00:13:46,240 --> 00:13:50,650
will be in HBase so you can just read

00:13:48,130 --> 00:13:53,200
from HBase sense of your web app our

00:13:50,650 --> 00:13:54,550
mobile app right okay it sounded good a

00:13:53,200 --> 00:13:56,830
lot of these components are kind of

00:13:54,550 --> 00:13:59,020
default so I'll just given idea about

00:13:56,830 --> 00:14:01,060
that so Kafka is a great system for

00:13:59,020 --> 00:14:03,280
moving logs from one place to other I

00:14:01,060 --> 00:14:05,950
think it's a clear winner now so there's

00:14:03,280 --> 00:14:08,110
not much of a choice storm you can

00:14:05,950 --> 00:14:10,839
perhaps consider other systems but we

00:14:08,110 --> 00:14:13,870
chose storm I'll get into what was our

00:14:10,839 --> 00:14:15,730
experience with storm HBase the reason

00:14:13,870 --> 00:14:16,130
we chose is where each base was because

00:14:15,730 --> 00:14:18,230
it

00:14:16,130 --> 00:14:19,970
it's in the same huh Luke family and it

00:14:18,230 --> 00:14:21,620
had native MapReduce integration it

00:14:19,970 --> 00:14:23,570
built on Hadoop we had already had a

00:14:21,620 --> 00:14:27,860
harder cluster so it was easier to go

00:14:23,570 --> 00:14:29,240
with that cool so sounds good if we do

00:14:27,860 --> 00:14:31,730
put this together it should probably

00:14:29,240 --> 00:14:34,790
work right okay but let's see what were

00:14:31,730 --> 00:14:38,120
the issues so first issue like let's say

00:14:34,790 --> 00:14:39,860
if we have h base which what what we

00:14:38,120 --> 00:14:41,840
need to do with that every base first it

00:14:39,860 --> 00:14:44,840
needs to handle 100,000 writes per

00:14:41,840 --> 00:14:46,610
second so if we if we have all the data

00:14:44,840 --> 00:14:49,430
coming from all web apps and mobile apps

00:14:46,610 --> 00:14:51,740
and email coming in the real time at the

00:14:49,430 --> 00:14:54,170
peak we we have to write 100,000 writes

00:14:51,740 --> 00:14:57,830
per second into HBase otherwise we would

00:14:54,170 --> 00:15:00,410
be behind real time right and if we if

00:14:57,830 --> 00:15:01,820
let's say we achieve that then what we

00:15:00,410 --> 00:15:04,030
need to do is that because it's also

00:15:01,820 --> 00:15:07,250
serving layer for the real time web site

00:15:04,030 --> 00:15:09,860
it needs to have it needs to keep a very

00:15:07,250 --> 00:15:11,690
tight SL on late Reed Layton sees while

00:15:09,860 --> 00:15:13,370
our MapReduce jobs are running on the

00:15:11,690 --> 00:15:14,840
cluster so if you have a MapReduce job

00:15:13,370 --> 00:15:18,650
that's ring on the HBase cluster

00:15:14,840 --> 00:15:22,100
computing your recommendations and and

00:15:18,650 --> 00:15:24,650
also users who also being served on the

00:15:22,100 --> 00:15:26,780
app or website the latency needs to be

00:15:24,650 --> 00:15:30,920
consistent and they cannot just exterior

00:15:26,780 --> 00:15:32,660
SLE and sometimes you know like we will

00:15:30,920 --> 00:15:34,760
also have bunch of data that will

00:15:32,660 --> 00:15:36,830
compute offline in the MapReduce cluster

00:15:34,760 --> 00:15:40,370
and will upload back into HBase to be

00:15:36,830 --> 00:15:41,960
served in the real time now now again

00:15:40,370 --> 00:15:46,400
that those Layton sees needs to be

00:15:41,960 --> 00:15:49,010
within your SLE ok so it didn't work

00:15:46,400 --> 00:15:50,210
just how we imagined initially right we

00:15:49,010 --> 00:15:52,760
couldn't write hundred thousand messages

00:15:50,210 --> 00:15:54,530
per second as is and when the MapReduce

00:15:52,760 --> 00:15:57,770
jobs started running on HBase their

00:15:54,530 --> 00:16:01,430
latencies we went to the roof all right

00:15:57,770 --> 00:16:03,350
so what do we do so this is what we did

00:16:01,430 --> 00:16:06,650
to write hundred thousand writes per

00:16:03,350 --> 00:16:09,650
second so the data data is very simple

00:16:06,650 --> 00:16:12,230
so the key is a user key and the the

00:16:09,650 --> 00:16:14,630
data we want to write in that value is

00:16:12,230 --> 00:16:17,620
just user events so user clicked on this

00:16:14,630 --> 00:16:20,150
page click on this deal open this thing

00:16:17,620 --> 00:16:22,820
open this email received this email blah

00:16:20,150 --> 00:16:24,110
blah blah so or purchase this deal or

00:16:22,820 --> 00:16:25,700
something like that right so that the

00:16:24,110 --> 00:16:27,920
data is pretty straightforward it's just

00:16:25,700 --> 00:16:29,639
if i read that users data then we get

00:16:27,920 --> 00:16:32,499
all the data but at you

00:16:29,639 --> 00:16:34,480
so one way to do that is perhaps have

00:16:32,499 --> 00:16:37,959
some sort of a read end and the right

00:16:34,480 --> 00:16:40,839
thing so you you read the data from from

00:16:37,959 --> 00:16:44,649
in it for that key you update the new

00:16:40,839 --> 00:16:45,999
event and you write it back we've found

00:16:44,649 --> 00:16:48,639
out that it's it's very hard to do that

00:16:45,999 --> 00:16:51,730
at 100,000 writes per second so what we

00:16:48,639 --> 00:16:54,339
do now is HBase comes with this sort of

00:16:51,730 --> 00:16:55,839
a concept of dynamic columns are you

00:16:54,339 --> 00:16:57,459
know qualifiers so you have a column

00:16:55,839 --> 00:16:58,839
family but within that column family you

00:16:57,459 --> 00:17:01,149
can write whatever column then move on

00:16:58,839 --> 00:17:04,689
so each base is kind of a hash map of a

00:17:01,149 --> 00:17:06,429
hashmap so all right so we said good if

00:17:04,689 --> 00:17:09,370
we can come up with a sort of a unique

00:17:06,429 --> 00:17:10,990
column name or qualifier name then we

00:17:09,370 --> 00:17:13,720
can just keep on appending data for that

00:17:10,990 --> 00:17:17,529
user so here's an example let's say this

00:17:13,720 --> 00:17:19,779
user looked at some sort of a deal and

00:17:17,529 --> 00:17:23,559
clicked on a deal then for this user

00:17:19,779 --> 00:17:27,159
will create this column name so it's a

00:17:23,559 --> 00:17:30,370
timestamp in the in milliseconds deal ID

00:17:27,159 --> 00:17:37,210
and in the event type right so whatever

00:17:30,370 --> 00:17:39,669
that event type is excuse me whatever

00:17:37,210 --> 00:17:41,350
that event type is so that way we could

00:17:39,669 --> 00:17:44,409
keep on writing as many events as we

00:17:41,350 --> 00:17:46,450
want with some some amount of buffering

00:17:44,409 --> 00:17:47,590
like to second buffering or one second

00:17:46,450 --> 00:17:50,260
buffering we could write hundred

00:17:47,590 --> 00:17:52,059
thousand messages per second good so

00:17:50,260 --> 00:17:55,059
that solve the first problem what about

00:17:52,059 --> 00:17:57,240
the next problem that was relatively

00:17:55,059 --> 00:18:00,130
easy to solve so all we did is that we

00:17:57,240 --> 00:18:01,600
just put two clusters one is the real

00:18:00,130 --> 00:18:04,240
time cluster and the other is a batch

00:18:01,600 --> 00:18:08,529
cluster and we use the ready-made

00:18:04,240 --> 00:18:10,990
application that each base comes with so

00:18:08,529 --> 00:18:13,299
all the data will go all the all the

00:18:10,990 --> 00:18:15,039
data will be written into the search

00:18:13,299 --> 00:18:17,169
base and get replicated two batches base

00:18:15,039 --> 00:18:20,230
all the math addresses will run on this

00:18:17,169 --> 00:18:22,240
HBase so because of that we don't have

00:18:20,230 --> 00:18:24,100
to worry about how the latencies are

00:18:22,240 --> 00:18:27,220
affected on a GPS right so because it's

00:18:24,100 --> 00:18:28,960
on a separate cluster and then we'll use

00:18:27,220 --> 00:18:30,640
the the functionality that edge base

00:18:28,960 --> 00:18:32,350
comes with called bulk upload so what it

00:18:30,640 --> 00:18:35,919
does is that creates whatever you want

00:18:32,350 --> 00:18:38,950
to write e in in a direct H file so the

00:18:35,919 --> 00:18:40,690
underlying format of HBase in a separate

00:18:38,950 --> 00:18:41,730
MapReduce job and then it just copies

00:18:40,690 --> 00:18:44,190
those files

00:18:41,730 --> 00:18:47,310
literally just like a disk copy like a

00:18:44,190 --> 00:18:49,830
Hadoop Hadoop copy and it brings all the

00:18:47,310 --> 00:18:51,480
data in so like we could you could I

00:18:49,830 --> 00:18:53,640
mean in our cluster we could copy you

00:18:51,480 --> 00:18:56,130
know like 100 gigs of data into HBase

00:18:53,640 --> 00:18:58,470
cluster and they're like 10-15 seconds

00:18:56,130 --> 00:19:00,060
right because it's literally just a copy

00:18:58,470 --> 00:19:04,050
there's nothing you can just rename the

00:19:00,060 --> 00:19:05,970
file that's it so cool so we could write

00:19:04,050 --> 00:19:07,590
hundred thousand writes per second we

00:19:05,970 --> 00:19:09,930
could load bunch of data into HBase in

00:19:07,590 --> 00:19:14,430
real time and now the HBase park works

00:19:09,930 --> 00:19:16,230
perfect so let's that so each place is

00:19:14,430 --> 00:19:17,760
kind of a user side story right so user

00:19:16,230 --> 00:19:19,290
and all the data about users and we

00:19:17,760 --> 00:19:21,000
could run MapReduce to find user

00:19:19,290 --> 00:19:22,770
personalization all that stuff so the

00:19:21,000 --> 00:19:26,030
other interesting part is the deal side

00:19:22,770 --> 00:19:28,380
right so let's see what that part is so

00:19:26,030 --> 00:19:30,300
I'll give you idea about what relevance

00:19:28,380 --> 00:19:34,140
is or what is the sort of intuition

00:19:30,300 --> 00:19:36,960
behind is relevance algorithms so we

00:19:34,140 --> 00:19:38,850
said okay if we if we if we know you

00:19:36,960 --> 00:19:41,220
know like for example questions to these

00:19:38,850 --> 00:19:43,530
answers to these questions like say how

00:19:41,220 --> 00:19:47,040
do women in bar Berlin convert for peas

00:19:43,530 --> 00:19:48,930
are deals right or how are women in

00:19:47,040 --> 00:19:51,240
Berlin are converting for a particular

00:19:48,930 --> 00:19:52,710
piece our deal you know conversion rate

00:19:51,240 --> 00:19:54,330
being you know say if we show this deal

00:19:52,710 --> 00:19:55,740
two hundred users how many of them are

00:19:54,330 --> 00:19:58,350
buying you know that's your conversion

00:19:55,740 --> 00:19:59,850
rate then we could potentially start

00:19:58,350 --> 00:20:02,190
thinking about how we can use the

00:19:59,850 --> 00:20:04,140
relevance of girls right we could make

00:20:02,190 --> 00:20:06,270
it more interesting and add a few more

00:20:04,140 --> 00:20:10,110
dimensions to it so let's see how that

00:20:06,270 --> 00:20:13,380
works so how are women in Berlin from me

00:20:10,110 --> 00:20:15,750
ta area age 45 to 50 convert from New

00:20:13,380 --> 00:20:17,760
York style pizza wendy is located within

00:20:15,750 --> 00:20:20,190
two miles and when deal is priced

00:20:17,760 --> 00:20:22,860
between 10 or 20 euros right so you

00:20:20,190 --> 00:20:24,090
added more dimensions and imagine you

00:20:22,860 --> 00:20:27,000
know if you have answers to these

00:20:24,090 --> 00:20:28,950
questions like you know for all these

00:20:27,000 --> 00:20:31,440
attributes what is the conversion rate

00:20:28,950 --> 00:20:33,480
and you take a user and you find which

00:20:31,440 --> 00:20:35,220
bucket that user belongs to in all those

00:20:33,480 --> 00:20:36,660
attributes and find which deal is

00:20:35,220 --> 00:20:40,680
converting best for that user you

00:20:36,660 --> 00:20:42,630
potentially have a ranking right what we

00:20:40,680 --> 00:20:44,070
found out is that you can't just keep it

00:20:42,630 --> 00:20:45,600
at a category level so this is just a

00:20:44,070 --> 00:20:47,280
category level at a piece are right or

00:20:45,600 --> 00:20:49,890
new upsell piece or something like that

00:20:47,280 --> 00:20:52,680
each deal performs very differently so

00:20:49,890 --> 00:20:54,660
to give you an example a neighborhood

00:20:52,680 --> 00:20:55,380
coffee shop deal performs much

00:20:54,660 --> 00:20:57,540
differently

00:20:55,380 --> 00:20:58,860
say a starbucks deal right so deals are

00:20:57,540 --> 00:21:01,050
very different but although both our

00:20:58,860 --> 00:21:03,660
coffee deals right so we can't just keep

00:21:01,050 --> 00:21:05,310
it at the complete at a category level

00:21:03,660 --> 00:21:09,450
but we want to we need to compute these

00:21:05,310 --> 00:21:11,670
things at ed level not just a complete

00:21:09,450 --> 00:21:13,890
category level so the same thing but

00:21:11,670 --> 00:21:15,840
since everything on the left side on the

00:21:13,890 --> 00:21:20,730
right hand side only that it's for a

00:21:15,840 --> 00:21:22,350
particular deal more more conv but but

00:21:20,730 --> 00:21:24,120
this is not enough right so we wanted to

00:21:22,350 --> 00:21:29,190
do more complex stuff so here is an

00:21:24,120 --> 00:21:31,860
example so so how are women in Berlin

00:21:29,190 --> 00:21:33,660
from a particular area age 45 to 50

00:21:31,860 --> 00:21:35,250
convert for New York style pizza when

00:21:33,660 --> 00:21:37,260
deal is located within two miles and

00:21:35,250 --> 00:21:39,900
when Dale is priced between 10 and 20

00:21:37,260 --> 00:21:41,910
euros and that user likes activities

00:21:39,900 --> 00:21:44,640
such as biking and who have been very

00:21:41,910 --> 00:21:46,290
active user of Groupon and they serve on

00:21:44,640 --> 00:21:48,120
mobile platform right something like

00:21:46,290 --> 00:21:49,890
that the problem is that you know you

00:21:48,120 --> 00:21:51,690
start with very something simple right

00:21:49,890 --> 00:21:53,730
you started with you know peas our deal

00:21:51,690 --> 00:21:55,650
for women right and now you have added

00:21:53,730 --> 00:21:57,810
these extra dimensions now the problem

00:21:55,650 --> 00:21:59,550
is that this is actually an exponential

00:21:57,810 --> 00:22:02,790
problem because every time you add a

00:21:59,550 --> 00:22:06,030
dimension it you know to two genders and

00:22:02,790 --> 00:22:08,390
then you add you know like an area then

00:22:06,030 --> 00:22:11,280
to into that number of areas you have

00:22:08,390 --> 00:22:13,800
women just all the way back in two

00:22:11,280 --> 00:22:16,650
locations into price categories into age

00:22:13,800 --> 00:22:18,270
categories and what we found out was

00:22:16,650 --> 00:22:19,650
that you know if we have to answer all

00:22:18,270 --> 00:22:22,080
these questions we have to compute

00:22:19,650 --> 00:22:26,310
probably you know like 15 or 20 billion

00:22:22,080 --> 00:22:28,500
different events or separate buckets in

00:22:26,310 --> 00:22:30,480
order to answer those questions so I

00:22:28,500 --> 00:22:33,510
said okay that's interesting problem ok

00:22:30,480 --> 00:22:34,980
let's try to solve that just before that

00:22:33,510 --> 00:22:36,900
this is what i mean by a conversion rate

00:22:34,980 --> 00:22:39,180
number of purchases in that bucket over

00:22:36,900 --> 00:22:41,520
number of impressions impression can be

00:22:39,180 --> 00:22:43,890
anything every time you see something or

00:22:41,520 --> 00:22:45,720
on email web or mobile we counted as

00:22:43,890 --> 00:22:47,700
impression right and then every time you

00:22:45,720 --> 00:22:49,080
purchase that's a conversion so how many

00:22:47,700 --> 00:22:54,750
times you saw it how many times you

00:22:49,080 --> 00:22:56,610
bought it or in that bucket so i'll come

00:22:54,750 --> 00:22:59,070
come to you know how we solve that 15

00:22:56,610 --> 00:23:00,950
billion different buckets question but

00:22:59,070 --> 00:23:03,750
let's first talk about how how to

00:23:00,950 --> 00:23:05,760
compute these different buckets in the

00:23:03,750 --> 00:23:07,110
real time right we want to do this in

00:23:05,760 --> 00:23:08,559
the real time so the moment deal is

00:23:07,110 --> 00:23:09,940
launched within 15

00:23:08,559 --> 00:23:11,980
20 minutes we have a good idea about

00:23:09,940 --> 00:23:16,929
that deal and we can target it better

00:23:11,980 --> 00:23:18,580
right so this is what we use remember I

00:23:16,929 --> 00:23:20,320
mentioned earlier we had we have cough

00:23:18,580 --> 00:23:23,139
cough which which has a stream of events

00:23:20,320 --> 00:23:26,950
so we can just plug into that stream and

00:23:23,139 --> 00:23:30,159
get all the data so that's the kafka

00:23:26,950 --> 00:23:31,779
part then we have a strong one of the

00:23:30,159 --> 00:23:32,980
topology rights in to hbase but that's

00:23:31,779 --> 00:23:34,720
the user side we are talking about the

00:23:32,980 --> 00:23:36,340
inside now so what we do in that

00:23:34,720 --> 00:23:38,499
topology is that you know can we say

00:23:36,340 --> 00:23:40,629
fine will will will read the event and

00:23:38,499 --> 00:23:43,299
then we'll if we can increase the

00:23:40,629 --> 00:23:44,950
appropriate counter then we have a

00:23:43,299 --> 00:23:47,409
conversion rate remember conversion rate

00:23:44,950 --> 00:23:49,869
is just a counter right so if we could

00:23:47,409 --> 00:23:51,669
just count how many impressions we have

00:23:49,869 --> 00:23:53,679
in each bucket and how will you purchase

00:23:51,669 --> 00:23:55,210
in each bucket the problem is solved

00:23:53,679 --> 00:23:57,309
this is really a counting problem this

00:23:55,210 --> 00:23:58,480
is not any harder than that just that

00:23:57,309 --> 00:24:00,460
the problem is hard because we are

00:23:58,480 --> 00:24:05,019
dealing with lot of data across lot of

00:24:00,460 --> 00:24:07,419
buckets so so we used a Redis for

00:24:05,019 --> 00:24:09,159
counting so each place is good for

00:24:07,419 --> 00:24:12,159
100,000 writes per second and all that

00:24:09,159 --> 00:24:14,200
stuff but for this we we needed about 3

00:24:12,159 --> 00:24:15,909
million updates per second right because

00:24:14,200 --> 00:24:17,919
the problem is that you know when you've

00:24:15,909 --> 00:24:20,289
in it when a went come say this

00:24:17,919 --> 00:24:23,019
particular user bought this deal it's

00:24:20,289 --> 00:24:25,480
not just one event it gets into multiple

00:24:23,019 --> 00:24:27,820
buckets so that user is also a male that

00:24:25,480 --> 00:24:29,980
user is from this area this deal is this

00:24:27,820 --> 00:24:32,230
price and we have to count against all

00:24:29,980 --> 00:24:35,559
those buckets right not just one bucket

00:24:32,230 --> 00:24:38,679
so we had to write or update about three

00:24:35,559 --> 00:24:40,749
million times per second so so we use

00:24:38,679 --> 00:24:43,480
Redis for that so this is the high level

00:24:40,749 --> 00:24:45,940
high level idea so data we read from

00:24:43,480 --> 00:24:49,840
Kafka stream in analytics topology we

00:24:45,940 --> 00:24:52,299
decide which exact buckets to increment

00:24:49,840 --> 00:24:57,700
increment our counters and then we write

00:24:52,299 --> 00:25:00,159
into Redis this is kind of the same just

00:24:57,700 --> 00:25:03,070
I just described Kafka read data events

00:25:00,159 --> 00:25:04,659
in Kafka find which buckets even falls

00:25:03,070 --> 00:25:07,299
into increase event counter for

00:25:04,659 --> 00:25:09,519
appropriate buckets in the Redis and red

00:25:07,299 --> 00:25:11,919
is a single Redis was not enough so we

00:25:09,519 --> 00:25:16,169
did our own sharding on the client side

00:25:11,919 --> 00:25:19,299
and we updated that data in the Redis

00:25:16,169 --> 00:25:20,950
cool so I'll just go into you know some

00:25:19,299 --> 00:25:22,199
of the scaling challenges so that that's

00:25:20,950 --> 00:25:24,849
really a tree you know like how

00:25:22,199 --> 00:25:26,499
that it's really simple the data comes

00:25:24,849 --> 00:25:28,509
in we decide which bucket show update we

00:25:26,499 --> 00:25:30,699
update the counter done right if we do

00:25:28,509 --> 00:25:32,319
that for all users and against all

00:25:30,699 --> 00:25:34,199
buckets the problem is solved you have

00:25:32,319 --> 00:25:36,549
the relevant system which is real time

00:25:34,199 --> 00:25:38,439
well as it turned out you know every

00:25:36,549 --> 00:25:40,329
component is not that straightforward to

00:25:38,439 --> 00:25:44,709
scale so that's the scaling part I am

00:25:40,329 --> 00:25:48,609
going to talk about so the one of the

00:25:44,709 --> 00:25:50,229
first issues is the cough kind storm

00:25:48,609 --> 00:25:52,809
part the scaling of cough kind strong

00:25:50,229 --> 00:25:54,639
but in Group one we have a sort of a

00:25:52,809 --> 00:25:57,489
centralized kafka cluster where you know

00:25:54,639 --> 00:25:58,749
if all the data comes there and I don't

00:25:57,489 --> 00:26:00,219
work on that team but they kind of

00:25:58,749 --> 00:26:04,299
manage our scaling in which is a very

00:26:00,219 --> 00:26:07,209
nice setup for us to have but the the

00:26:04,299 --> 00:26:09,669
challenging part was storm especially

00:26:07,209 --> 00:26:11,589
when the messages count became you know

00:26:09,669 --> 00:26:14,349
like 100,000 mrs. per second and then

00:26:11,589 --> 00:26:16,389
two million updates per second and all

00:26:14,349 --> 00:26:19,779
that stuff the storm became harder to

00:26:16,389 --> 00:26:22,029
scale the problem is that you know in

00:26:19,779 --> 00:26:25,239
real time systems you have to manage the

00:26:22,029 --> 00:26:27,579
flow so you have say step one two and

00:26:25,239 --> 00:26:29,319
three and step three is slightly slower

00:26:27,579 --> 00:26:31,509
then it creates back pressure on step

00:26:29,319 --> 00:26:32,859
two and if step two is now slower it

00:26:31,509 --> 00:26:34,269
creates a back pressure on step one

00:26:32,859 --> 00:26:37,779
because there are only so many events in

00:26:34,269 --> 00:26:40,659
the funnel you can have right and you

00:26:37,779 --> 00:26:42,789
have to sort of I just you know like you

00:26:40,659 --> 00:26:44,829
based on your computation you know each

00:26:42,789 --> 00:26:46,959
each step may take more or less time

00:26:44,829 --> 00:26:49,799
right and you have to sort of at just

00:26:46,959 --> 00:26:52,029
how many bolts you need in each step and

00:26:49,799 --> 00:26:55,139
that becomes kind of a challenge you

00:26:52,029 --> 00:26:57,189
know we spend a lot of time tuning that

00:26:55,139 --> 00:27:00,759
there are other things that you have to

00:26:57,189 --> 00:27:03,249
make sure for example the try to do a

00:27:00,759 --> 00:27:05,139
localized so let's say this is all

00:27:03,249 --> 00:27:07,479
distributed systems right so from step

00:27:05,139 --> 00:27:09,999
one you move on to the step two if

00:27:07,479 --> 00:27:11,499
possible make sure that you send that

00:27:09,999 --> 00:27:12,939
data from step one to step two on the

00:27:11,499 --> 00:27:14,859
same machine instead of through the

00:27:12,939 --> 00:27:18,279
network because then the network becomes

00:27:14,859 --> 00:27:20,769
a bottling so for example if you just

00:27:18,279 --> 00:27:23,589
implement storm as is out of default it

00:27:20,769 --> 00:27:25,839
has kind of a random passing the

00:27:23,589 --> 00:27:27,729
messages and what we saw that a network

00:27:25,839 --> 00:27:29,739
was out of bandwidth right very quickly

00:27:27,729 --> 00:27:32,259
and then we moved to this kind of

00:27:29,739 --> 00:27:34,089
localized thing and you know we up it

00:27:32,259 --> 00:27:34,870
came down to like 20 percent of what it

00:27:34,089 --> 00:27:36,610
was earlier

00:27:34,870 --> 00:27:38,800
so there are likes a lot of little

00:27:36,610 --> 00:27:43,000
things that you have to do for a storm

00:27:38,800 --> 00:27:45,880
to work at that this scale there were

00:27:43,000 --> 00:27:48,700
other things like because the speed of

00:27:45,880 --> 00:27:51,070
each processing part is different you

00:27:48,700 --> 00:27:53,530
have to sort of stop how many messages

00:27:51,070 --> 00:27:55,870
you are getting into the storm system

00:27:53,530 --> 00:27:57,520
otherwise the spout that you have it

00:27:55,870 --> 00:27:59,410
keeps on getting messages and if the

00:27:57,520 --> 00:28:01,059
messages are not getting processed you

00:27:59,410 --> 00:28:03,070
know it just creates a lot of ram

00:28:01,059 --> 00:28:05,920
pressure or memory pressure and you know

00:28:03,070 --> 00:28:08,950
bad things happen so there are settings

00:28:05,920 --> 00:28:10,809
like use macs pout pending etcetera

00:28:08,950 --> 00:28:12,309
basically that stops how many

00:28:10,809 --> 00:28:16,750
unprocessed messages you have in your

00:28:12,309 --> 00:28:19,210
topology okay so here is another

00:28:16,750 --> 00:28:21,360
interesting part what happens is that

00:28:19,210 --> 00:28:23,470
you know all these systems are

00:28:21,360 --> 00:28:26,800
distributed and they do not give you any

00:28:23,470 --> 00:28:29,260
guarantees of the most of them give you

00:28:26,800 --> 00:28:32,040
at least once guarantee right but they

00:28:29,260 --> 00:28:34,630
don't give you at exactly once guarantee

00:28:32,040 --> 00:28:37,840
so what I'm what I mean by that is that

00:28:34,630 --> 00:28:40,030
you can get duplicate messages so which

00:28:37,840 --> 00:28:42,460
is now an interesting problem because if

00:28:40,030 --> 00:28:43,929
you count purchases twice your

00:28:42,460 --> 00:28:47,110
conversion rates are completely going to

00:28:43,929 --> 00:28:50,920
be wrong right so what do we do with

00:28:47,110 --> 00:28:53,140
that with HBase that was easy because if

00:28:50,920 --> 00:28:56,559
the message comes again will create the

00:28:53,140 --> 00:28:58,420
same same column name for it so we'll

00:28:56,559 --> 00:29:00,370
just overwrite that message so it's not

00:28:58,420 --> 00:29:02,140
a problem on each base side but the

00:29:00,370 --> 00:29:04,420
problem is definitely there in the

00:29:02,140 --> 00:29:07,090
another this radish analytics topology

00:29:04,420 --> 00:29:10,390
that I just explained because now your

00:29:07,090 --> 00:29:13,690
counters are going to be wrong right so

00:29:10,390 --> 00:29:15,550
what we do is we use bloom filters so it

00:29:13,690 --> 00:29:18,730
kind of covers you know ninety nine

00:29:15,550 --> 00:29:20,830
point sort of nine percent of duplicate

00:29:18,730 --> 00:29:21,820
error cases but this is something to

00:29:20,830 --> 00:29:24,400
remember if you are building these

00:29:21,820 --> 00:29:26,470
systems you have to build it so that you

00:29:24,400 --> 00:29:27,910
know that there will be duplicate

00:29:26,470 --> 00:29:31,950
messages there will be delayed messages

00:29:27,910 --> 00:29:36,730
and your system needs to adjust to that

00:29:31,950 --> 00:29:38,679
Redis so this is a interesting right

00:29:36,730 --> 00:29:41,230
because the the fundamental part of

00:29:38,679 --> 00:29:43,300
scaling Redis was managing our memory

00:29:41,230 --> 00:29:44,830
footprint because as I mention you know

00:29:43,300 --> 00:29:47,620
we had this 14 billion different buckets

00:29:44,830 --> 00:29:48,730
of something like that if we just use

00:29:47,620 --> 00:29:51,640
another

00:29:48,730 --> 00:29:55,450
normal ready ski for 14 billion buckets

00:29:51,640 --> 00:29:57,669
the memory footprint was huge right and

00:29:55,450 --> 00:29:59,470
what we found out in the Redis is that

00:29:57,669 --> 00:30:02,350
they have this nice hash hash

00:29:59,470 --> 00:30:04,419
functionality where you have sort of a

00:30:02,350 --> 00:30:07,990
top-level key and like inside you have

00:30:04,419 --> 00:30:09,790
another hash if you use that you could

00:30:07,990 --> 00:30:14,380
potentially reduce your memory footprint

00:30:09,790 --> 00:30:17,080
by a factor of 100 or at least you know

00:30:14,380 --> 00:30:18,580
50 or something right but by a huge

00:30:17,080 --> 00:30:20,320
margin so we are talking about instead

00:30:18,580 --> 00:30:22,630
of using two hundred machines using 20

00:30:20,320 --> 00:30:24,520
machines right so it's much much much

00:30:22,630 --> 00:30:27,790
big it's a huge difference and the

00:30:24,520 --> 00:30:29,770
reason it works that way is because like

00:30:27,790 --> 00:30:31,809
things like ex-pirate expiry and all

00:30:29,770 --> 00:30:33,429
that stuff is maintained at at the top

00:30:31,809 --> 00:30:38,470
level hash key instead of at each key

00:30:33,429 --> 00:30:41,440
level so you can use Redis with hashes

00:30:38,470 --> 00:30:42,970
to have multiple different keys but your

00:30:41,440 --> 00:30:47,650
overall member memory footprint would be

00:30:42,970 --> 00:30:49,510
much much less you know the other issue

00:30:47,650 --> 00:30:52,090
we have with the system is that you know

00:30:49,510 --> 00:30:54,760
like in order to even keep reddy's up

00:30:52,090 --> 00:30:58,179
and keep on going redis comes up with

00:30:54,760 --> 00:31:00,790
sort of two persistent formats one is

00:30:58,179 --> 00:31:03,760
called aof which is every time you write

00:31:00,790 --> 00:31:06,190
Redis updates the disk and you know

00:31:03,760 --> 00:31:08,020
right starting and the other is our DB

00:31:06,190 --> 00:31:09,730
which is kind of a snapshotting system

00:31:08,020 --> 00:31:11,620
so you can say every five minutes or 20

00:31:09,730 --> 00:31:14,590
minutes take a dump from memory into

00:31:11,620 --> 00:31:17,470
disk and that's what you have so what we

00:31:14,590 --> 00:31:20,260
did is that we we turned off aof so not

00:31:17,470 --> 00:31:22,419
every not every operation gets to the

00:31:20,260 --> 00:31:25,059
disk right away we we have we kind of

00:31:22,419 --> 00:31:27,040
lazy implementation here so if say

00:31:25,059 --> 00:31:31,030
something breaks for 15 minutes then we

00:31:27,040 --> 00:31:33,330
kind of lose data for 15 minutes but but

00:31:31,030 --> 00:31:35,380
that way we could we could scale Redis

00:31:33,330 --> 00:31:36,910
the other thing is that I want to

00:31:35,380 --> 00:31:39,520
mention is that Redis was probably the

00:31:36,910 --> 00:31:41,290
easiest for us to scale I mean it's

00:31:39,520 --> 00:31:45,220
really has no overhead you just start

00:31:41,290 --> 00:31:49,390
and it just works I think I just briefly

00:31:45,220 --> 00:31:51,880
mentioned about bloom filters but they

00:31:49,390 --> 00:31:53,080
are very handy you can especially for

00:31:51,880 --> 00:31:55,390
the systems like these you know your

00:31:53,080 --> 00:31:58,150
overall memory deduplication is a huge

00:31:55,390 --> 00:32:01,190
problem and we could solve that pretty

00:31:58,150 --> 00:32:03,390
quickly with this

00:32:01,190 --> 00:32:06,720
another interesting part about this

00:32:03,390 --> 00:32:08,310
system is that the architecture of the

00:32:06,720 --> 00:32:10,560
system is relatively simple because it

00:32:08,310 --> 00:32:13,320
is kafka strong the Redis and the other

00:32:10,560 --> 00:32:15,090
part is just Kafka storm HBase the

00:32:13,320 --> 00:32:17,340
problem is that the error scenarios

00:32:15,090 --> 00:32:18,960
right because this is a real-time system

00:32:17,340 --> 00:32:20,580
if something happens something wrong

00:32:18,960 --> 00:32:22,920
happens you have to make sure you can

00:32:20,580 --> 00:32:25,320
come back to it at a reasonable level

00:32:22,920 --> 00:32:26,670
and because lot of business and lot of

00:32:25,320 --> 00:32:29,700
revenue is actually riding on these

00:32:26,670 --> 00:32:33,390
systems so bringing systems back to the

00:32:29,700 --> 00:32:37,080
correct status is very critical so how

00:32:33,390 --> 00:32:38,790
do we do that so Reddy's I mentioned you

00:32:37,080 --> 00:32:40,440
know we have these are DB backup so what

00:32:38,790 --> 00:32:43,920
we do is we have a slaver it is also

00:32:40,440 --> 00:32:46,110
which gets replicated and we take these

00:32:43,920 --> 00:32:48,810
already backups and copy them to Hadoop

00:32:46,110 --> 00:32:49,800
cluster HDFS every I think 20 minutes or

00:32:48,810 --> 00:32:52,080
something like that so if something

00:32:49,800 --> 00:32:54,330
happens we can go back to the state

00:32:52,080 --> 00:32:56,040
where whatever Redis was 20 minutes ago

00:32:54,330 --> 00:32:57,240
all right at each 20 minute interval for

00:32:56,040 --> 00:33:00,510
I think last seven days or something

00:32:57,240 --> 00:33:03,150
like that hbase HBase comes up with a

00:33:00,510 --> 00:33:05,040
snapshot functionality so you can tell

00:33:03,150 --> 00:33:06,960
edge base to take a snapshot at whatever

00:33:05,040 --> 00:33:08,700
time you want and you can just tell

00:33:06,960 --> 00:33:11,430
HBase to go back to that snapshot so

00:33:08,700 --> 00:33:13,590
let's say something happened some bad

00:33:11,430 --> 00:33:15,090
data is coming remember the data that we

00:33:13,590 --> 00:33:17,370
have we don't control the data you know

00:33:15,090 --> 00:33:19,590
whoever is writing the mobile app where

00:33:17,370 --> 00:33:21,120
is writing the web app they are doing

00:33:19,590 --> 00:33:22,470
the log and this is all log base data

00:33:21,120 --> 00:33:24,180
right so it's very possible that

00:33:22,470 --> 00:33:26,340
somebody writes some bad code and we get

00:33:24,180 --> 00:33:28,530
bad data right so let's say some date

00:33:26,340 --> 00:33:29,940
bad data is coming as long as we are

00:33:28,530 --> 00:33:32,100
taking these snapshots we can say oh

00:33:29,940 --> 00:33:34,110
that bad data is coming from you know

00:33:32,100 --> 00:33:35,940
yesterday two o'clock or something so we

00:33:34,110 --> 00:33:37,980
just take all our systems rewind back to

00:33:35,940 --> 00:33:41,400
two o'clock based on these snapshots and

00:33:37,980 --> 00:33:42,690
we replay the data when that problem is

00:33:41,400 --> 00:33:48,000
fixed and we get all the right data

00:33:42,690 --> 00:33:51,210
right Kafka and storm so what we also

00:33:48,000 --> 00:33:53,370
have is on the kafka we have a topic

00:33:51,210 --> 00:33:56,040
that reads from CAF chondrites to HDFS

00:33:53,370 --> 00:33:57,750
so all the data that's coming to Kafka

00:33:56,040 --> 00:33:59,250
because the kafka has some I think three

00:33:57,750 --> 00:34:01,230
day expiry or something like that so we

00:33:59,250 --> 00:34:03,570
don't have all the data inside Kafka but

00:34:01,230 --> 00:34:06,330
all the data inside car gets to HDFS so

00:34:03,570 --> 00:34:08,129
let's say you want to replay data from

00:34:06,330 --> 00:34:10,200
two weeks ago for the two hours period

00:34:08,129 --> 00:34:11,790
which which were we had a bad data we

00:34:10,200 --> 00:34:13,470
could potentially do that right we could

00:34:11,790 --> 00:34:14,190
just replay that data from HDFS back

00:34:13,470 --> 00:34:18,510
into cough cough

00:34:14,190 --> 00:34:20,069
to this system one more interesting

00:34:18,510 --> 00:34:23,220
thing we did is that you know we wrote

00:34:20,069 --> 00:34:25,109
lot of monitors and cpu monitors this

00:34:23,220 --> 00:34:26,940
model to that monitor but there was

00:34:25,109 --> 00:34:30,089
another interesting monitor we wrote and

00:34:26,940 --> 00:34:31,889
in practicality that was the most

00:34:30,089 --> 00:34:34,109
important monitor we had written and

00:34:31,889 --> 00:34:36,720
what we did that what we what was that

00:34:34,109 --> 00:34:38,790
monitor was we actually wrote a crawler

00:34:36,720 --> 00:34:42,119
that goes and crawls our website or

00:34:38,790 --> 00:34:43,790
mobile app and then we check back in

00:34:42,119 --> 00:34:46,290
each base if those events have arrived

00:34:43,790 --> 00:34:48,060
right and it takes continuously every

00:34:46,290 --> 00:34:50,579
two minutes or something like that so we

00:34:48,060 --> 00:34:54,089
crawl our own website we fake a by our

00:34:50,579 --> 00:34:56,099
fig we fake the birch purchases of fake

00:34:54,089 --> 00:34:58,440
deals and then we check back in each

00:34:56,099 --> 00:35:00,510
base and with that what happens it's

00:34:58,440 --> 00:35:02,640
kind of an integration sort of a monitor

00:35:00,510 --> 00:35:04,140
test whatever you call it but it takes

00:35:02,640 --> 00:35:06,300
the whole thing you know it takes the

00:35:04,140 --> 00:35:08,310
app it takes the website it takes Kafka

00:35:06,300 --> 00:35:09,839
check storm it takes HBase it checks

00:35:08,310 --> 00:35:11,970
traders you know it takes the whole

00:35:09,839 --> 00:35:13,140
thing and if something is wrong

00:35:11,970 --> 00:35:15,180
somewhere there are so many moving

00:35:13,140 --> 00:35:16,530
components if something is wrong

00:35:15,180 --> 00:35:18,089
somewhere you find out within a couple

00:35:16,530 --> 00:35:20,369
of minutes that something is wrong right

00:35:18,089 --> 00:35:21,660
and then we just go back and say okay we

00:35:20,369 --> 00:35:23,760
have to stop this now we'll see what

00:35:21,660 --> 00:35:28,560
happens and all that stuff so that that

00:35:23,760 --> 00:35:31,770
that's a very useful monitor we have we

00:35:28,560 --> 00:35:33,540
are hiring my manager asked me to put

00:35:31,770 --> 00:35:38,390
this slide he won't otherwise let me

00:35:33,540 --> 00:35:38,390
speak so and questions

00:35:43,670 --> 00:35:45,730

YouTube URL: https://www.youtube.com/watch?v=-pOGiVA9ll4


