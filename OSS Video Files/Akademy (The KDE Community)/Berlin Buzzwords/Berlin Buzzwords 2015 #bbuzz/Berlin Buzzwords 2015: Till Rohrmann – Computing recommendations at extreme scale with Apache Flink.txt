Title: Berlin Buzzwords 2015: Till Rohrmann – Computing recommendations at extreme scale with Apache Flink
Publication date: 2015-06-02
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Recommender Systems are a very successful application of large scale data processing. They are used to recommend new items of interest to users of a service, such as new movies on Netflix, or shopping articles on Amazon. Recommender systems have become an essential part of most web-based services to enhance the user experience. 

A powerful approach for implementing recommenders are the so called "latent factor models", a special case of the collaborative filtering techniques, which exploit the similarity between user tastes and item characteristics, which are automatically extracted.:

This talk details our experience with implementing three variants of the ALS (Alternating Least Squares) algorithm to train a latent factor model using the Apache Flink system and scaling them to large clusters and data sets. In order to scale these algorithms to extremely large data sets, Flink’s functionality was significantly enhanced to be able to distribute and process very large records efficiently.

Read more:
https://2015.berlinbuzzwords.de/session/computing-recommendations-extreme-scale-apache-flink

About Till Rohrmann:
https://2015.berlinbuzzwords.de/users/till-rohrmann

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,490 --> 00:00:11,059
yeah hi everyone

00:00:11,610 --> 00:00:18,150
I'm till as you already know and I'm a

00:00:15,360 --> 00:00:20,669
think emitter these days working mainly

00:00:18,150 --> 00:00:22,829
on the the machine learning level of

00:00:20,669 --> 00:00:25,619
Apache flink and today I'm going to tell

00:00:22,829 --> 00:00:27,570
you a little bit about how to compute

00:00:25,619 --> 00:00:31,230
recommendations at an extremely large

00:00:27,570 --> 00:00:35,100
scale with a pet chief link as the title

00:00:31,230 --> 00:00:37,230
of my presentation indicates i have

00:00:35,100 --> 00:00:38,460
divided my target to three part in the

00:00:37,230 --> 00:00:41,070
first part I'm going to talk a little

00:00:38,460 --> 00:00:42,800
bit about recommendations in general how

00:00:41,070 --> 00:00:45,510
you do it with a collaborative filtering

00:00:42,800 --> 00:00:47,630
then I will introduce a petty thing for

00:00:45,510 --> 00:00:50,460
those of you haven't heard about it and

00:00:47,630 --> 00:00:53,300
last but at least I will fuse these two

00:00:50,460 --> 00:00:55,770
things and show you how to implement a

00:00:53,300 --> 00:00:58,489
recommender system which can deal with

00:00:55,770 --> 00:01:03,989
we really really large waiting matrices

00:00:58,489 --> 00:01:07,200
so let's get started first I guess that

00:01:03,989 --> 00:01:08,970
every one of you got somehow in touch

00:01:07,200 --> 00:01:11,219
with some kind of recommender systems

00:01:08,970 --> 00:01:13,590
because these days they are basically

00:01:11,219 --> 00:01:17,220
everywhere on the internet so no matter

00:01:13,590 --> 00:01:21,360
whether you buy some products in on

00:01:17,220 --> 00:01:24,479
Amazon or you you watch some serious on

00:01:21,360 --> 00:01:26,970
netflix or listen to some songs on

00:01:24,479 --> 00:01:28,590
Spotify there's some system and the

00:01:26,970 --> 00:01:30,240
background trying to analyze here are

00:01:28,590 --> 00:01:32,820
your preferences and based on these

00:01:30,240 --> 00:01:36,150
preferences gives you new suggestions of

00:01:32,820 --> 00:01:37,710
what you should consume next and well of

00:01:36,150 --> 00:01:41,010
course this is not done because of

00:01:37,710 --> 00:01:43,650
charity by the companies they they want

00:01:41,010 --> 00:01:47,070
you to improve the user experience and

00:01:43,650 --> 00:01:49,979
thus born the user strong a two to their

00:01:47,070 --> 00:01:52,950
product so that eventually the the sales

00:01:49,979 --> 00:01:55,890
and revenue is increased and due to that

00:01:52,950 --> 00:01:58,170
the whole recommendation systems

00:01:55,890 --> 00:02:02,150
recommender system thing is really big

00:01:58,170 --> 00:02:05,610
these days but how does it actually work

00:02:02,150 --> 00:02:07,950
what to give you a short example let's

00:02:05,610 --> 00:02:11,730
let's assume we want to recommend some

00:02:07,950 --> 00:02:14,909
websites to our customers and our

00:02:11,730 --> 00:02:17,640
customers is what are the king the Queen

00:02:14,909 --> 00:02:19,709
and the daughter the princess and they

00:02:17,640 --> 00:02:23,670
visit the three websites Amazon Facebook

00:02:19,709 --> 00:02:24,900
and Twitter so what we know is from our

00:02:23,670 --> 00:02:27,569
luck data

00:02:24,900 --> 00:02:29,970
that the King really likes to spend some

00:02:27,569 --> 00:02:33,780
time on Amazon because they can spend

00:02:29,970 --> 00:02:36,180
his wealth furthermore he he spends a

00:02:33,780 --> 00:02:37,769
good amount of his time on on facebook

00:02:36,180 --> 00:02:40,439
where he keeps in touch with the fellow

00:02:37,769 --> 00:02:42,930
kings and queens and what I can can brag

00:02:40,439 --> 00:02:47,250
about his latest conquest for example so

00:02:42,930 --> 00:02:49,079
and and additionally we know that the

00:02:47,250 --> 00:02:52,920
the time his daughter the princess

00:02:49,079 --> 00:02:54,750
spends on Amazon is also not too too

00:02:52,920 --> 00:02:57,019
little because apparently that the

00:02:54,750 --> 00:02:59,609
pocket money is not too bad either so

00:02:57,019 --> 00:03:03,989
from the the shared preference of the

00:02:59,609 --> 00:03:06,000
King for Amazon and and the the

00:03:03,989 --> 00:03:08,579
preference of the princess for Amazon we

00:03:06,000 --> 00:03:11,129
can deduce that may be that the princess

00:03:08,579 --> 00:03:13,230
will also look would also like to to

00:03:11,129 --> 00:03:15,090
spend some time on Facebook because the

00:03:13,230 --> 00:03:18,090
print the King heads of the same

00:03:15,090 --> 00:03:20,040
preference and so we could simply

00:03:18,090 --> 00:03:23,430
recommend Facebook to hear that's

00:03:20,040 --> 00:03:28,349
basically how collaborative filtering

00:03:23,430 --> 00:03:31,349
works the idea so to do um say a little

00:03:28,349 --> 00:03:34,260
bit more abstractly is if you want you

00:03:31,349 --> 00:03:36,389
to recommend items for and user a and

00:03:34,260 --> 00:03:39,049
what you do is you look for users who

00:03:36,389 --> 00:03:42,690
have shown in the past similar

00:03:39,049 --> 00:03:45,780
consumption behavior and based on what

00:03:42,690 --> 00:03:48,930
these group of people have consumed you

00:03:45,780 --> 00:03:53,099
can then make recommendations to to this

00:03:48,930 --> 00:03:56,129
user a and well how do you find out

00:03:53,099 --> 00:03:58,530
whether some users share a similar

00:03:56,129 --> 00:04:01,799
preference for for some kind of products

00:03:58,530 --> 00:04:04,470
well here's where the the latent factor

00:04:01,799 --> 00:04:07,229
models come into play which basically

00:04:04,470 --> 00:04:10,620
assigned for for each assigned to each

00:04:07,229 --> 00:04:13,199
user a set of preferences and to each to

00:04:10,620 --> 00:04:17,060
each item a set of characteristics which

00:04:13,199 --> 00:04:20,609
somehow describes on the the product and

00:04:17,060 --> 00:04:23,639
usually what is done is that that these

00:04:20,609 --> 00:04:26,430
kind of features are like represented as

00:04:23,639 --> 00:04:28,349
numeric numerical vectors such such that

00:04:26,430 --> 00:04:32,070
the scalar product of these vectors

00:04:28,349 --> 00:04:34,469
gives you a preference value for for

00:04:32,070 --> 00:04:36,539
user and an item so the higher this

00:04:34,469 --> 00:04:37,540
value is the more likely it is that the

00:04:36,539 --> 00:04:42,190
user

00:04:37,540 --> 00:04:44,200
well likes this item before I get to the

00:04:42,190 --> 00:04:47,740
point where tell you how to calculate

00:04:44,200 --> 00:04:50,560
these latent factors let's get one step

00:04:47,740 --> 00:04:53,580
back and consider what we have initially

00:04:50,560 --> 00:04:56,920
as input and what we have is we have

00:04:53,580 --> 00:04:59,680
user item ratings this can either be

00:04:56,920 --> 00:05:02,680
explicit or implicit explicitly ratings

00:04:59,680 --> 00:05:05,110
means that for example the user our

00:05:02,680 --> 00:05:08,920
faith watched a series of Netflix gives

00:05:05,110 --> 00:05:11,710
one to five stars to it that's explicit

00:05:08,920 --> 00:05:15,010
waiting an example for implicit waiting

00:05:11,710 --> 00:05:19,540
would be there that you analyze the

00:05:15,010 --> 00:05:20,770
behavior of the user for example you you

00:05:19,540 --> 00:05:24,220
measure how long he stays on a website

00:05:20,770 --> 00:05:28,500
or how often he watches a certain film

00:05:24,220 --> 00:05:31,300
to get a kind of waiting value foot and

00:05:28,500 --> 00:05:35,080
these waiting values can be organized in

00:05:31,300 --> 00:05:37,750
a matrix where the roles are the users

00:05:35,080 --> 00:05:41,260
and the columns are the items and of

00:05:37,750 --> 00:05:44,080
course this mate matrix is by no means

00:05:41,260 --> 00:05:46,750
dance so it's it contains a lot of

00:05:44,080 --> 00:05:50,650
blanks because not every user has seen

00:05:46,750 --> 00:05:52,390
all movies or listen to all songs so the

00:05:50,650 --> 00:05:56,950
the goal is now to fill in these planks

00:05:52,390 --> 00:05:59,290
and that's what we going to try to do

00:05:56,950 --> 00:06:01,660
next please bear with me on this slide

00:05:59,290 --> 00:06:04,420
that's going to be one of two slides

00:06:01,660 --> 00:06:07,300
with formulas but actually really easy

00:06:04,420 --> 00:06:12,520
so what we have is we have a rating

00:06:07,300 --> 00:06:16,150
matrix and to find the latent factors we

00:06:12,520 --> 00:06:19,360
have to find and two matrices the user

00:06:16,150 --> 00:06:21,430
matrix X and the item matrix Y such that

00:06:19,360 --> 00:06:24,640
the product of these made of this these

00:06:21,430 --> 00:06:26,880
two matrices gives you approximately the

00:06:24,640 --> 00:06:32,050
rating matrix once we found these two

00:06:26,880 --> 00:06:34,420
matrices which are usually a flowing we

00:06:32,050 --> 00:06:37,120
also have found our latent factors

00:06:34,420 --> 00:06:39,600
because the wealth of the user matrix X

00:06:37,120 --> 00:06:43,150
are the latent factors of the users and

00:06:39,600 --> 00:06:47,950
the columns of the item matrix are the

00:06:43,150 --> 00:06:50,289
the latent factors of the items so but

00:06:47,950 --> 00:06:53,499
how can we find these two matrices

00:06:50,289 --> 00:06:55,869
X of Y well here well that's that's a

00:06:53,499 --> 00:06:59,710
case for a machine learning where we can

00:06:55,869 --> 00:07:01,749
apply some methods but in order to apply

00:06:59,710 --> 00:07:04,659
some machine learning methods we usually

00:07:01,749 --> 00:07:06,999
need some kind of loss function which

00:07:04,659 --> 00:07:10,960
tells us whether our solution we found

00:07:06,999 --> 00:07:14,229
is a good solution and in our case we

00:07:10,960 --> 00:07:18,610
use the root mean squared error which

00:07:14,229 --> 00:07:22,740
sounds might sound like complicated but

00:07:18,610 --> 00:07:25,509
actually it's it's only the sum of the

00:07:22,740 --> 00:07:29,740
the sum of the squared differences of

00:07:25,509 --> 00:07:32,919
the prediction values and the writing

00:07:29,740 --> 00:07:35,139
values so the smaller the sum get the

00:07:32,919 --> 00:07:37,270
more closely the better serve I

00:07:35,139 --> 00:07:41,080
approximation of the right images so

00:07:37,270 --> 00:07:43,889
yeah that's it basically now since we

00:07:41,080 --> 00:07:46,419
have like a loss function we can apply

00:07:43,889 --> 00:07:50,259
optimization algorithm which will find

00:07:46,419 --> 00:07:53,099
us our X of Y values well the problem

00:07:50,259 --> 00:07:56,469
here is that the loss function is

00:07:53,099 --> 00:08:00,879
depends on two variables that's the item

00:07:56,469 --> 00:08:03,189
matrix X Y and the user matrix X what we

00:08:00,879 --> 00:08:06,099
could use for example is su casa

00:08:03,189 --> 00:08:07,240
gradient descent this always works but

00:08:06,099 --> 00:08:09,099
today I want to show you a different

00:08:07,240 --> 00:08:12,370
approach which is called alternating

00:08:09,099 --> 00:08:15,939
least squares the idea of alternative

00:08:12,370 --> 00:08:18,069
least squares is that that by fixing one

00:08:15,939 --> 00:08:20,289
of the variables let's say the item

00:08:18,069 --> 00:08:23,499
matrix why we obtain a quadratic

00:08:20,289 --> 00:08:25,209
quadratic optimization problem and we

00:08:23,499 --> 00:08:27,759
know that's easy to solve we only have

00:08:25,209 --> 00:08:31,899
to take the first derivative set to zero

00:08:27,759 --> 00:08:34,689
and solve for the unknown I bear with

00:08:31,899 --> 00:08:36,610
the math here espero spare the math you

00:08:34,689 --> 00:08:38,979
and you have to trust me that this is

00:08:36,610 --> 00:08:41,949
right if you do that on to some with

00:08:38,979 --> 00:08:45,870
reformulation of the equation you end up

00:08:41,949 --> 00:08:51,360
with this update formula for user vector

00:08:45,870 --> 00:08:53,800
X you what is interesting to see here is

00:08:51,360 --> 00:08:55,720
what we actually need to compute an

00:08:53,800 --> 00:08:58,470
updated I use a vector and that is

00:08:55,720 --> 00:09:01,449
actually all the waiting's the use of

00:08:58,470 --> 00:09:03,660
the user has done so the writing values

00:09:01,449 --> 00:09:05,310
and only

00:09:03,660 --> 00:09:07,500
those item vectors for which the user

00:09:05,310 --> 00:09:12,570
has given a rating that's going to be

00:09:07,500 --> 00:09:15,120
important later on okay now we've seen

00:09:12,570 --> 00:09:18,480
how we can update the user vector

00:09:15,120 --> 00:09:21,270
vectors with respect to the item vectors

00:09:18,480 --> 00:09:23,820
so if it's fixed so how can we finally

00:09:21,270 --> 00:09:27,530
end up with an overall solution of our

00:09:23,820 --> 00:09:33,240
optimization problem so that's quite

00:09:27,530 --> 00:09:38,460
simple we first fix the item matrix why

00:09:33,240 --> 00:09:42,510
update the user matrix X and use this

00:09:38,460 --> 00:09:45,750
value and the next half step to update

00:09:42,510 --> 00:09:50,010
the item matrix Y and these two half

00:09:45,750 --> 00:09:53,090
steps gives you on the what what we call

00:09:50,010 --> 00:09:55,170
like an a last iteration steps so we

00:09:53,090 --> 00:09:59,250
executing this these two steps

00:09:55,170 --> 00:10:03,330
repeatedly improves incrementally your

00:09:59,250 --> 00:10:04,980
your item and user matrix x and y and if

00:10:03,330 --> 00:10:06,690
you do that until you can watch you have

00:10:04,980 --> 00:10:09,450
solved your optimization problem and

00:10:06,690 --> 00:10:16,950
that's how you factorize a can factorize

00:10:09,450 --> 00:10:18,870
matrices okay at the beginning I said

00:10:16,950 --> 00:10:20,970
that we want to do recommendations at an

00:10:18,870 --> 00:10:23,430
extreme large scale so this means we are

00:10:20,970 --> 00:10:26,010
talking about data sizes which no longer

00:10:23,430 --> 00:10:28,530
fit in the memory of a single machine so

00:10:26,010 --> 00:10:32,210
what we have to do to to make it still

00:10:28,530 --> 00:10:34,260
feasible is to implement a distributed

00:10:32,210 --> 00:10:37,290
alternating least square a square

00:10:34,260 --> 00:10:41,160
algorithm and that's where our chief

00:10:37,290 --> 00:10:42,960
link comes into play for those of you

00:10:41,160 --> 00:10:46,940
who have never had a about our chief

00:10:42,960 --> 00:10:50,700
link will give you a short introduction

00:10:46,940 --> 00:10:53,340
Apache chief link is a power stream

00:10:50,700 --> 00:10:56,040
processing engine and as such a system

00:10:53,340 --> 00:10:58,200
the the key component of it is is a

00:10:56,040 --> 00:11:01,110
streaming data for one time the

00:10:58,200 --> 00:11:04,560
streaming data flow one time allows you

00:11:01,110 --> 00:11:06,600
to or enables you to execute dataflow

00:11:04,560 --> 00:11:10,380
programs in parallel and the data flow

00:11:06,600 --> 00:11:13,410
can be either a bad job or a streaming

00:11:10,380 --> 00:11:16,060
job actually and it's executed on the

00:11:13,410 --> 00:11:21,880
the same one time

00:11:16,060 --> 00:11:26,590
in order to to define these data flows a

00:11:21,880 --> 00:11:29,410
petty fling offers you one API for for

00:11:26,590 --> 00:11:32,500
batch programs so the data set API which

00:11:29,410 --> 00:11:36,400
is available for java scale and pison as

00:11:32,500 --> 00:11:38,580
well as a data stream ipi which is

00:11:36,400 --> 00:11:42,690
available available for Java and Scala

00:11:38,580 --> 00:11:46,120
but he is actually not worthy that that

00:11:42,690 --> 00:11:49,540
batch processing is actually a special

00:11:46,120 --> 00:11:51,880
case of stream processing when you do

00:11:49,540 --> 00:11:55,570
batch processing you can think of having

00:11:51,880 --> 00:11:57,400
a finite stream for example and they are

00:11:55,570 --> 00:12:00,400
well there are some special code paths

00:11:57,400 --> 00:12:04,210
in Apache flink which makes the batch

00:12:00,400 --> 00:12:13,330
execution more efficient but to speed it

00:12:04,210 --> 00:12:19,840
up on top of the bad batch processing

00:12:13,330 --> 00:12:23,110
API by now well we could say which

00:12:19,840 --> 00:12:25,300
ecosystem has been developed so there

00:12:23,110 --> 00:12:26,980
are some extension libraries some of

00:12:25,300 --> 00:12:29,140
them already part of the the current

00:12:26,980 --> 00:12:31,720
release some of them will be part of the

00:12:29,140 --> 00:12:33,940
upcoming 0.9 release which will

00:12:31,720 --> 00:12:37,450
hopefully be released in one to two

00:12:33,940 --> 00:12:41,530
weeks and some of them are the most

00:12:37,450 --> 00:12:43,180
important additions are the the a dupe

00:12:41,530 --> 00:12:46,000
compatibility layer which allows you to

00:12:43,180 --> 00:12:49,090
run a dupe jobs on on a budget thing

00:12:46,000 --> 00:12:52,690
then we have a library for a graph

00:12:49,090 --> 00:12:54,970
processing which is called jelly then

00:12:52,690 --> 00:12:56,800
recently we started working on the

00:12:54,970 --> 00:12:59,710
machine learning library where I

00:12:56,800 --> 00:13:02,680
currently spend most of my time and we

00:12:59,710 --> 00:13:06,360
added what we call the table API which

00:13:02,680 --> 00:13:09,790
is a really nice sql-like declarative

00:13:06,360 --> 00:13:13,240
programming interface so if you if you

00:13:09,790 --> 00:13:15,220
know SQL then you quickly will be

00:13:13,240 --> 00:13:16,990
familiar with the table API as well and

00:13:15,220 --> 00:13:22,030
it's really nice to work with that

00:13:16,990 --> 00:13:24,790
actually and the table API is also

00:13:22,030 --> 00:13:28,660
available for on the the screaming part

00:13:24,790 --> 00:13:29,900
of a patchy thing but since the swimming

00:13:28,660 --> 00:13:32,120
part is is younger

00:13:29,900 --> 00:13:36,850
the best part the ecosystem isn't as

00:13:32,120 --> 00:13:39,440
which yet as for the batch part and

00:13:36,850 --> 00:13:42,230
besides these these libraries and

00:13:39,440 --> 00:13:44,720
extension think is already is also used

00:13:42,230 --> 00:13:47,230
by many other projects as a back-end

00:13:44,720 --> 00:13:51,230
execution back-end so for example the

00:13:47,230 --> 00:13:55,190
well it is it can be used by google data

00:13:51,230 --> 00:13:57,500
flow API as a back-end meq/l is executed

00:13:55,190 --> 00:14:00,800
on flink we are currently working on

00:13:57,500 --> 00:14:02,900
bringing support like a skating support

00:14:00,800 --> 00:14:08,480
so that cascading jobs can be run on a

00:14:02,900 --> 00:14:10,790
patch if link and the link was added as

00:14:08,480 --> 00:14:15,350
a stream processing engine to the Apache

00:14:10,790 --> 00:14:17,870
Samoa project the summer project is well

00:14:15,350 --> 00:14:19,820
it's it deals with as viewing machine

00:14:17,870 --> 00:14:21,980
learning so in that sense some more

00:14:19,820 --> 00:14:23,960
brings assuming machine learning to

00:14:21,980 --> 00:14:28,700
Apache think as well that is really cool

00:14:23,960 --> 00:14:32,150
actually and all that can either be one

00:14:28,700 --> 00:14:34,820
locally on your machine you can start a

00:14:32,150 --> 00:14:36,640
center long cluster somewhere on some

00:14:34,820 --> 00:14:39,800
computers and one your jobs you can

00:14:36,640 --> 00:14:42,520
deploy fling on yarn that's really easy

00:14:39,800 --> 00:14:47,060
to run some jobs your job can be

00:14:42,520 --> 00:14:50,840
executed on tests or embedded lee on the

00:14:47,060 --> 00:14:52,640
next slides I will mainly concentrate on

00:14:50,840 --> 00:14:54,740
highlighting the the reasons why I think

00:14:52,640 --> 00:14:58,010
is a good fit for implementing these

00:14:54,740 --> 00:15:00,380
kind of recommender systems but if you

00:14:58,010 --> 00:15:02,690
want to learn more about the the in

00:15:00,380 --> 00:15:04,640
terms of thing are about the individual

00:15:02,690 --> 00:15:08,240
components then i can highly recommend

00:15:04,640 --> 00:15:10,280
you the apache fling deep dive talk

00:15:08,240 --> 00:15:13,760
which will be given by stephan even

00:15:10,280 --> 00:15:16,790
tomorrow i think from 1222 one o'clock

00:15:13,760 --> 00:15:18,650
on stage three so if you have time you

00:15:16,790 --> 00:15:23,660
should definitely check it out is it's

00:15:18,650 --> 00:15:27,650
going to be a good talk okay why why

00:15:23,660 --> 00:15:30,710
shall we use thing for ALS well to make

00:15:27,650 --> 00:15:32,870
a long story short there are four four

00:15:30,710 --> 00:15:36,410
main points one of them is that it has a

00:15:32,870 --> 00:15:38,960
really expressive API it has a stream

00:15:36,410 --> 00:15:41,420
pipeline screen possessor it offers

00:15:38,960 --> 00:15:43,690
closed loop iterations and its

00:15:41,420 --> 00:15:46,870
operations are executed on manage memory

00:15:43,690 --> 00:15:50,470
so I will explain all of these points

00:15:46,870 --> 00:15:55,630
and second in more detail so let's start

00:15:50,470 --> 00:15:58,300
with the expressive API and well the the

00:15:55,630 --> 00:16:02,110
recommender is implemented as a bad job

00:15:58,300 --> 00:16:05,200
so we have to talk about the batch API

00:16:02,110 --> 00:16:07,690
right the key abstraction of the the

00:16:05,200 --> 00:16:11,080
batch JP I is a what we call a data set

00:16:07,690 --> 00:16:13,660
the data set is and well we present some

00:16:11,080 --> 00:16:16,660
data which is possibly distributed in

00:16:13,660 --> 00:16:19,180
our custom and you define your

00:16:16,660 --> 00:16:21,130
computations are by applying

00:16:19,180 --> 00:16:24,700
transformations to this data set which

00:16:21,130 --> 00:16:27,520
which generates a new data set on which

00:16:24,700 --> 00:16:29,530
you can apply new sets of computations

00:16:27,520 --> 00:16:31,690
and that way you specify a more or less

00:16:29,530 --> 00:16:35,500
the data flow which encapsulate your

00:16:31,690 --> 00:16:38,020
your program logic well since--since all

00:16:35,500 --> 00:16:40,060
theory is gray let's let's take a look

00:16:38,020 --> 00:16:44,290
at an example which is like the hell

00:16:40,060 --> 00:16:48,010
world of big data the word count well

00:16:44,290 --> 00:16:49,720
they be a goal is we have a text and we

00:16:48,010 --> 00:16:52,780
want to count how off each word appears

00:16:49,720 --> 00:16:54,610
on this text how can we do that well

00:16:52,780 --> 00:16:57,820
first of all we have to define some case

00:16:54,610 --> 00:16:59,680
class which basically only stores for

00:16:57,820 --> 00:17:03,010
each word how often we've seen it so

00:16:59,680 --> 00:17:06,130
there's a string and interviewed then we

00:17:03,010 --> 00:17:08,829
have to weaken our data which is done in

00:17:06,130 --> 00:17:10,449
the second line the the text is

00:17:08,829 --> 00:17:13,030
represented as a data set of strings

00:17:10,449 --> 00:17:16,810
whereas each element will be the data

00:17:13,030 --> 00:17:18,280
set is one line of the text so what we

00:17:16,810 --> 00:17:21,280
have to do in order to calculate the

00:17:18,280 --> 00:17:23,439
number of words how often what appears

00:17:21,280 --> 00:17:26,920
on this text we first have to split each

00:17:23,439 --> 00:17:28,840
line into it worse and this is done by

00:17:26,920 --> 00:17:31,690
the first line with this flat map

00:17:28,840 --> 00:17:34,600
operation we take a line then call split

00:17:31,690 --> 00:17:38,380
on it and pack it or weapon in a word

00:17:34,600 --> 00:17:41,310
instant of this case class next we have

00:17:38,380 --> 00:17:44,170
to to collect all these were done senses

00:17:41,310 --> 00:17:46,300
so all identical words so that we can

00:17:44,170 --> 00:17:48,850
count a half they appear that's done by

00:17:46,300 --> 00:17:51,550
goodbye and the following some function

00:17:48,850 --> 00:17:54,700
which simply accounts how often the word

00:17:51,550 --> 00:17:57,179
appears and that's all to obtain the the

00:17:54,700 --> 00:18:01,059
word counts

00:17:57,179 --> 00:18:03,070
so this this exemption show how easy it

00:18:01,059 --> 00:18:06,460
is to to program with a Apache link and

00:18:03,070 --> 00:18:08,290
in fact it's a lot of fun to do and made

00:18:06,460 --> 00:18:12,010
the development of this recommender

00:18:08,290 --> 00:18:13,780
system really easy next I said we have

00:18:12,010 --> 00:18:15,670
the pipeline stream processor well

00:18:13,780 --> 00:18:19,420
unlike many other data flow systems

00:18:15,670 --> 00:18:23,080
which which executes the data flow stage

00:18:19,420 --> 00:18:25,450
wise so one Operator after another think

00:18:23,080 --> 00:18:29,410
twice to bring as many operators online

00:18:25,450 --> 00:18:31,900
as possible by doing it between this we

00:18:29,410 --> 00:18:35,410
can avoid to materialize intermediate

00:18:31,900 --> 00:18:38,280
results so that so we can simply take

00:18:35,410 --> 00:18:41,380
one element of your fear of the input

00:18:38,280 --> 00:18:43,120
give it to one operator take the result

00:18:41,380 --> 00:18:45,299
give it to the next so to pipe it

00:18:43,120 --> 00:18:47,770
through the data for more or less and

00:18:45,299 --> 00:18:49,780
for jobs where some of the enemy

00:18:47,770 --> 00:18:52,720
intermediate results are so blown up

00:18:49,780 --> 00:18:54,669
that they have to to write your data of

00:18:52,720 --> 00:18:58,030
the Demeter side to disk or that even

00:18:54,669 --> 00:19:00,429
your your system crashes the ad helps

00:18:58,030 --> 00:19:02,500
that we have this pipelining execution

00:19:00,429 --> 00:19:06,490
mode because flink doesn't have to

00:19:02,500 --> 00:19:08,770
materialize the intermediate data next

00:19:06,490 --> 00:19:12,730
I've talked about closed loop iterations

00:19:08,770 --> 00:19:15,549
well many many systems offer you an ad

00:19:12,730 --> 00:19:19,059
hoc duration mechanism in that sense

00:19:15,549 --> 00:19:23,830
that the client somehow we submit part

00:19:19,059 --> 00:19:25,950
of your job job repeatedly unlike these

00:19:23,830 --> 00:19:28,210
systems are flink offers a real

00:19:25,950 --> 00:19:31,480
iteration operator which is part of the

00:19:28,210 --> 00:19:36,040
data flow and which allows the the

00:19:31,480 --> 00:19:38,500
optimizer to to optimize programs which

00:19:36,040 --> 00:19:40,929
which contain iterations this is

00:19:38,500 --> 00:19:43,900
important for cases where where you

00:19:40,929 --> 00:19:46,720
include some some computations in your

00:19:43,900 --> 00:19:49,419
iterations which are actually static so

00:19:46,720 --> 00:19:51,640
which only has to be computed once so

00:19:49,419 --> 00:19:53,950
flink and the optimizer think can detect

00:19:51,640 --> 00:19:57,850
these kinds of computations pulls it out

00:19:53,950 --> 00:20:02,230
of the iteration computes at once cancel

00:19:57,850 --> 00:20:04,240
it caches it on on the note and so makes

00:20:02,230 --> 00:20:06,870
it available for for the extra iteration

00:20:04,240 --> 00:20:09,760
so you basically save some computations

00:20:06,870 --> 00:20:10,809
how the iterations are specified within

00:20:09,760 --> 00:20:13,659
think is

00:20:10,809 --> 00:20:15,909
that you give a step function to it

00:20:13,659 --> 00:20:18,070
which is a part of a data flow which

00:20:15,909 --> 00:20:20,259
basically calculates a form an input

00:20:18,070 --> 00:20:23,740
data set the recital the next iteration

00:20:20,259 --> 00:20:25,720
and what happens at the one time is that

00:20:23,740 --> 00:20:28,090
this step function is first chord or

00:20:25,720 --> 00:20:30,039
others dataflow part is first called

00:20:28,090 --> 00:20:32,769
with the initial input which generates

00:20:30,039 --> 00:20:36,159
the the result of the first situation I

00:20:32,769 --> 00:20:39,429
just beside is given back to the step

00:20:36,159 --> 00:20:41,769
function which then computes the with

00:20:39,429 --> 00:20:44,470
the result of the second iteration and

00:20:41,769 --> 00:20:46,480
this goes on until either the maximum

00:20:44,470 --> 00:20:48,490
number of iterations is reached or some

00:20:46,480 --> 00:20:52,809
kind of convergence criterion has been

00:20:48,490 --> 00:20:57,129
reached so last but not least I've said

00:20:52,809 --> 00:21:00,340
that flink thinks operations work on

00:20:57,129 --> 00:21:01,990
managed memory what does this mean well

00:21:00,340 --> 00:21:04,629
you understand that I want to give you

00:21:01,990 --> 00:21:07,259
some some feedback or some background

00:21:04,629 --> 00:21:10,450
information on how the JVM works

00:21:07,259 --> 00:21:15,039
whenever you you create some some

00:21:10,450 --> 00:21:17,200
objects in like a Java program they

00:21:15,039 --> 00:21:20,399
usually stored on the heap and for these

00:21:17,200 --> 00:21:24,909
objects is really hard to estimate the

00:21:20,399 --> 00:21:26,499
size of these objects so this means it

00:21:24,909 --> 00:21:28,539
is hard to detect when your heap

00:21:26,499 --> 00:21:30,549
actually gets full and when you would

00:21:28,539 --> 00:21:32,860
have to to take some of the elements

00:21:30,549 --> 00:21:36,070
from the heap put them somewhere on disk

00:21:32,860 --> 00:21:37,960
and make make new space available for

00:21:36,070 --> 00:21:40,360
four new elements this is often the case

00:21:37,960 --> 00:21:42,159
for these kinds of systems because you

00:21:40,360 --> 00:21:45,210
usually work on data which is larger

00:21:42,159 --> 00:21:48,940
than the total amount of memory you have

00:21:45,210 --> 00:21:51,369
blink circumvent this problem by working

00:21:48,940 --> 00:21:54,309
internally mostly on on serialized data

00:21:51,369 --> 00:21:57,940
and additionally it comes with its own

00:21:54,309 --> 00:22:00,039
see like memory manager so what happens

00:21:57,940 --> 00:22:03,460
whenever an element is given to think

00:22:00,039 --> 00:22:05,529
that it is serialized so it's given to

00:22:03,460 --> 00:22:09,279
think see well as a framework which see

00:22:05,529 --> 00:22:12,480
you lies and the objects into some

00:22:09,279 --> 00:22:14,740
purpose of the managed memory and the

00:22:12,480 --> 00:22:18,249
this memory is managed by the memory

00:22:14,740 --> 00:22:20,649
manager and the manner mensia notice

00:22:18,249 --> 00:22:22,779
when there are no more buffers free

00:22:20,649 --> 00:22:24,220
buffers left this tells him that some of

00:22:22,779 --> 00:22:26,020
the buffers

00:22:24,220 --> 00:22:28,690
which are full have to be split two

00:22:26,020 --> 00:22:30,460
discs from where they can at a later

00:22:28,690 --> 00:22:35,159
point of time of the execution can be

00:22:30,460 --> 00:22:38,530
retrieved and that way think avoids to

00:22:35,159 --> 00:22:41,429
the thing think of voids too are you in

00:22:38,530 --> 00:22:43,900
most well you will we can say never see

00:22:41,429 --> 00:22:46,480
out of memory exception caused by one of

00:22:43,900 --> 00:22:48,190
the internal components of link because

00:22:46,480 --> 00:22:54,210
it will always gracefully speed to disk

00:22:48,190 --> 00:22:56,770
if it's needed okay so much for think

00:22:54,210 --> 00:23:01,659
now let's get back to the actual

00:22:56,770 --> 00:23:04,990
implementation okay to understand or to

00:23:01,659 --> 00:23:09,400
see what we have to do we have to take a

00:23:04,990 --> 00:23:14,020
look at the update formula again so we

00:23:09,400 --> 00:23:19,330
see we or we can update the the user

00:23:14,020 --> 00:23:21,820
vector X you by well we need the

00:23:19,330 --> 00:23:25,870
writings of the user you and the rated

00:23:21,820 --> 00:23:27,610
item vectors of the well the items which

00:23:25,870 --> 00:23:30,700
the user you waited to compute the

00:23:27,610 --> 00:23:34,120
updated value of x you to illustrate

00:23:30,700 --> 00:23:36,190
that let's let's try to do it for the

00:23:34,120 --> 00:23:40,960
first user so we see in the weighting

00:23:36,190 --> 00:23:44,530
matrix that the first user has waited

00:23:40,960 --> 00:23:47,169
the second and fourth item right so what

00:23:44,530 --> 00:23:50,620
we need to calculate the the new X 1

00:23:47,169 --> 00:23:53,200
vector is these two writings as well as

00:23:50,620 --> 00:23:56,590
the item vectors on the second and the

00:23:53,200 --> 00:23:58,600
fourth item vector we have these

00:23:56,590 --> 00:24:02,320
informations and get them on one note we

00:23:58,600 --> 00:24:05,080
can calculate the updated vector so we

00:24:02,320 --> 00:24:08,289
can can generate or get this information

00:24:05,080 --> 00:24:12,640
to one place by first joining the rating

00:24:08,289 --> 00:24:16,390
matrix with the the item matrix where we

00:24:12,640 --> 00:24:20,710
use the column ID as John Key and then

00:24:16,390 --> 00:24:24,130
group these pairs on the on the user ID

00:24:20,710 --> 00:24:25,870
or row index of the rating matrix that's

00:24:24,130 --> 00:24:29,380
all we have to do to get all information

00:24:25,870 --> 00:24:32,890
in one place to compute the updated

00:24:29,380 --> 00:24:34,990
values for X 1 however here in this

00:24:32,890 --> 00:24:38,100
example we already see one of the

00:24:34,990 --> 00:24:41,490
drawbacks of this approach that is the

00:24:38,100 --> 00:24:43,740
user 1 and user to actually computed on

00:24:41,490 --> 00:24:45,960
the same amount right additionally we

00:24:43,740 --> 00:24:48,840
see that user 1 and user to have both

00:24:45,960 --> 00:24:51,330
right of the second item so even though

00:24:48,840 --> 00:24:54,809
they are kept in the same machine the

00:24:51,330 --> 00:24:58,380
second item vector it has to be sent

00:24:54,809 --> 00:24:59,700
twice to the same note so we are

00:24:58,380 --> 00:25:02,549
basically sending redundant information

00:24:59,700 --> 00:25:06,990
over the network which is which is bad

00:25:02,549 --> 00:25:10,799
actually but let's first take a look at

00:25:06,990 --> 00:25:13,289
the at the pros of this approach well

00:25:10,799 --> 00:25:15,210
the paws of this naive implementation is

00:25:13,289 --> 00:25:17,220
that it is straightforward and thus

00:25:15,210 --> 00:25:20,940
really easy to implement and maintain so

00:25:17,220 --> 00:25:26,549
the code is rather well it's it's yeah

00:25:20,940 --> 00:25:28,650
easy however we have a lot of network IO

00:25:26,549 --> 00:25:30,240
with this approach because we first of

00:25:28,650 --> 00:25:34,049
all we send redundant information over

00:25:30,240 --> 00:25:36,240
the network and second sickly since we

00:25:34,049 --> 00:25:39,240
did not pre a partition the ratings

00:25:36,240 --> 00:25:41,429
matrix we have to shuffle steps one

00:25:39,240 --> 00:25:45,120
after the first joint operation and the

00:25:41,429 --> 00:25:47,630
second shuffle step after the well for

00:25:45,120 --> 00:25:50,159
the group would use operation and

00:25:47,630 --> 00:25:53,940
shuffles means always like sending the

00:25:50,159 --> 00:25:59,059
data from from well across the network

00:25:53,940 --> 00:26:03,270
which is bad so can we do any better

00:25:59,059 --> 00:26:06,539
well in fact we we can we can do better

00:26:03,270 --> 00:26:09,020
by addressing these two last problems so

00:26:06,539 --> 00:26:12,480
namely sending redundant information and

00:26:09,020 --> 00:26:14,880
having to shuffle steps so first of all

00:26:12,480 --> 00:26:17,549
what we can do is we simply group all

00:26:14,880 --> 00:26:21,659
the users which are computed on one node

00:26:17,549 --> 00:26:23,580
together so for example for node 1 we

00:26:21,659 --> 00:26:28,590
would create a user block which contains

00:26:23,580 --> 00:26:30,690
the first and the second user once we

00:26:28,590 --> 00:26:32,580
have done that we can also group all the

00:26:30,690 --> 00:26:34,590
item vectors which we need to calculate

00:26:32,580 --> 00:26:38,700
the updated values for these to our

00:26:34,590 --> 00:26:41,250
users so basically the second and fourth

00:26:38,700 --> 00:26:44,760
item vector but here it's important to

00:26:41,250 --> 00:26:47,400
note that that that all these item

00:26:44,760 --> 00:26:50,580
vectors are only sent at most once so

00:26:47,400 --> 00:26:51,660
this this drastically reduces the amount

00:26:50,580 --> 00:26:54,840
of

00:26:51,660 --> 00:26:57,750
traffic you have and last but not least

00:26:54,840 --> 00:27:00,750
we can can pre-partition the weighting

00:26:57,750 --> 00:27:04,170
matrix once according to the user blocks

00:27:00,750 --> 00:27:06,780
and once for the the item blocks and

00:27:04,170 --> 00:27:10,410
keep them on the machines that way on

00:27:06,780 --> 00:27:13,220
the the first joint operation is happens

00:27:10,410 --> 00:27:15,510
completely locally and doesn't doesn't

00:27:13,220 --> 00:27:21,860
where you don't have to do a shuffle

00:27:15,510 --> 00:27:26,010
step which is nice so if we summed it up

00:27:21,860 --> 00:27:28,350
well the report of this this approach is

00:27:26,010 --> 00:27:31,530
that we significantly significantly

00:27:28,350 --> 00:27:35,100
could reduce the network I off first of

00:27:31,530 --> 00:27:38,220
all by avoiding this application of data

00:27:35,100 --> 00:27:41,430
we said and by catching the right X

00:27:38,220 --> 00:27:45,180
right ratings which well it gets gets

00:27:41,430 --> 00:27:48,060
rid of one shuffle step however the the

00:27:45,180 --> 00:27:49,950
con is that first of all that we have to

00:27:48,060 --> 00:27:52,380
store the the rating matrix twice

00:27:49,950 --> 00:27:54,870
because it's applicated because the

00:27:52,380 --> 00:27:56,730
different partitioning and the code well

00:27:54,870 --> 00:27:59,490
I haven't put it here the code gods or

00:27:56,730 --> 00:28:03,420
more complex or significantly more

00:27:59,490 --> 00:28:05,660
complex however when we take a look at

00:28:03,420 --> 00:28:08,730
the performance research we see that

00:28:05,660 --> 00:28:11,070
it's the advantages clearly outweigh the

00:28:08,730 --> 00:28:13,920
disadvantages so what we've done here is

00:28:11,070 --> 00:28:16,530
we've learned some lend some experiments

00:28:13,920 --> 00:28:18,960
on a matrix accusation on a 40 note

00:28:16,530 --> 00:28:22,740
Google compute engine cluster where each

00:28:18,960 --> 00:28:27,660
of the instances instances was a eight

00:28:22,740 --> 00:28:31,200
core with fifty two gigabytes machine we

00:28:27,660 --> 00:28:33,300
calculated ten ALS iterations with 50

00:28:31,200 --> 00:28:35,550
latent factors for rating matrices of

00:28:33,300 --> 00:28:39,360
varying sizes we started with really

00:28:35,550 --> 00:28:41,730
small matrices and slowly scaled up what

00:28:39,360 --> 00:28:44,340
we can see use that the naive ailis

00:28:41,730 --> 00:28:47,190
implementation performed worse than the

00:28:44,340 --> 00:28:50,520
blocked LS implementation that's not so

00:28:47,190 --> 00:28:52,380
surprising and at some point there was I

00:28:50,520 --> 00:28:55,530
think eight billion where the rating

00:28:52,380 --> 00:28:59,490
waitress had eight billion nonzero

00:28:55,530 --> 00:29:02,220
entries it for matrices bigger than that

00:28:59,490 --> 00:29:05,130
it took so long that well the

00:29:02,220 --> 00:29:05,730
calculation didn't finish so encounters

00:29:05,130 --> 00:29:07,290
that

00:29:05,730 --> 00:29:09,870
see that the block areas implementation

00:29:07,290 --> 00:29:12,990
scales much much better and we could

00:29:09,870 --> 00:29:15,750
could factorize matrices with up to I'm

00:29:12,990 --> 00:29:19,380
28 billion nonzero entries which is on

00:29:15,750 --> 00:29:20,370
the scale off of netflix and spotify so

00:29:19,380 --> 00:29:25,260
that there is a real world problem

00:29:20,370 --> 00:29:29,280
actually it took us 14 hours to to

00:29:25,260 --> 00:29:32,600
finish the 10 iterations and by by

00:29:29,280 --> 00:29:36,480
doubling the number of course and the

00:29:32,600 --> 00:29:38,610
memory we have we could even cut the

00:29:36,480 --> 00:29:41,310
time down to five and half hours which

00:29:38,610 --> 00:29:43,050
now allows two to calculate the matrix

00:29:41,310 --> 00:29:45,540
accurate factorization multiple times a

00:29:43,050 --> 00:29:51,780
day which allows you to do well more

00:29:45,540 --> 00:29:54,480
like up-to-date recommendations okay now

00:29:51,780 --> 00:29:57,420
you might say well this this is quite

00:29:54,480 --> 00:29:59,570
nice but if I have to implement myself

00:29:57,420 --> 00:30:03,300
then maybe it's not worth the effort

00:29:59,570 --> 00:30:06,570
however you don't have to worry the the

00:30:03,300 --> 00:30:08,940
ALS implementation is part of the

00:30:06,570 --> 00:30:10,950
machine learning library so it's really

00:30:08,940 --> 00:30:15,390
easy to use and that's we can see on the

00:30:10,950 --> 00:30:17,750
right side how you would use it first of

00:30:15,390 --> 00:30:21,860
all you would have to instantiate the

00:30:17,750 --> 00:30:24,390
AAS facto miser object we end the

00:30:21,860 --> 00:30:27,600
weighting matrix which is represented as

00:30:24,390 --> 00:30:30,600
a data set of tuples well index callum

00:30:27,600 --> 00:30:34,020
index writing value then you specify

00:30:30,600 --> 00:30:35,970
some parameters for ALS namely the

00:30:34,020 --> 00:30:38,250
number of iterations so many latent

00:30:35,970 --> 00:30:39,330
factors you want to have the lambda

00:30:38,250 --> 00:30:44,030
value which is important for the

00:30:39,330 --> 00:30:47,400
regulars like regularization the actual

00:30:44,030 --> 00:30:49,230
factorization happens in the fig method

00:30:47,400 --> 00:30:51,540
where you provide the rating matrix and

00:30:49,230 --> 00:30:55,740
the parameters once you've done that you

00:30:51,540 --> 00:30:59,300
can basically query your calculate for

00:30:55,740 --> 00:31:02,190
well or user ID item ID pairs the

00:30:59,300 --> 00:31:06,360
predictions by simply calling ALS

00:31:02,190 --> 00:31:07,980
predict or where you provide the repairs

00:31:06,360 --> 00:31:10,800
for which you want to ok with the

00:31:07,980 --> 00:31:13,200
predictions yeah but that's not all um

00:31:10,800 --> 00:31:15,390
you can do with the current machine

00:31:13,200 --> 00:31:17,700
learning lab we they also I wouldn't

00:31:15,390 --> 00:31:19,059
include it for clustering regression and

00:31:17,700 --> 00:31:22,029
classification tasks

00:31:19,059 --> 00:31:24,549
and all comes with a psychic learn like

00:31:22,029 --> 00:31:26,769
pipeline mechanisms so it's really easy

00:31:24,549 --> 00:31:33,639
to set up even complex analytical

00:31:26,769 --> 00:31:36,940
pipelines yeah this brings me to the

00:31:33,639 --> 00:31:39,789
conclusion of my talk so what if what

00:31:36,940 --> 00:31:41,799
have you seen today well first of all

00:31:39,789 --> 00:31:43,389
you've seen how one can make

00:31:41,799 --> 00:31:47,740
recommendations using collaborative

00:31:43,389 --> 00:31:51,490
filtering even seen some some math then

00:31:47,740 --> 00:31:53,980
you have seen Apache flink the that

00:31:51,490 --> 00:31:56,019
Apache flink is a powerful powerful

00:31:53,980 --> 00:31:58,450
stream processing engine with which you

00:31:56,019 --> 00:32:02,379
can implement an efficient matrix

00:31:58,450 --> 00:32:07,720
accusation system which scales up to

00:32:02,379 --> 00:32:09,730
really we large major sizes yeah the

00:32:07,720 --> 00:32:12,730
last thing a which is left for me to do

00:32:09,730 --> 00:32:16,029
now is to draw your attention to the

00:32:12,730 --> 00:32:18,369
upcoming fling forward conference this

00:32:16,029 --> 00:32:21,039
will take place on the twelfth and

00:32:18,369 --> 00:32:22,990
thirteenth of October so if you got

00:32:21,039 --> 00:32:26,830
interested or maybe even want to give a

00:32:22,990 --> 00:32:29,200
talk there then save the date if you

00:32:26,830 --> 00:32:31,869
can't wait that long you should you

00:32:29,200 --> 00:32:35,080
should go to a flink dot apache lock and

00:32:31,869 --> 00:32:38,769
check check out link or follow playing

00:32:35,080 --> 00:32:40,480
on twitter at edgy fink and yes with

00:32:38,769 --> 00:32:42,090
that I'd like to thank you for attention

00:32:40,480 --> 00:32:45,149
and I hope you have enjoyed the talk

00:32:42,090 --> 00:32:45,149
very much

00:32:47,660 --> 00:32:49,720

YouTube URL: https://www.youtube.com/watch?v=CmHPxASIj6Y


