Title: Berlin Buzzwords 2015: Heloise Nonne - Online learning, Vowpal Wabbit and Hadoop #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Online learning has recently caught a lot of attention, following some competitions, and especially after Criteo released 11GB for the training set of a Kaggle contest. Online learning allows to process massive data as the learner processes data in a sequential way using up a low amount of memory and limited CPU ressources. It is also particularly suited for handling time-evolving date.

Vowpal Wabbit has become quite popular: it is a handy, light and efficient command line tool allowing to do online learning on GB of data, even on a standard laptop with standard memory. After a brief reminder of the online learning principles, we present how to run Vowpal Wabbit on Hadoop in a distributed fashion.

Read more:
https://2015.berlinbuzzwords.de/session/online-learning-vowpal-wabbit-and-hadoop

About Heloise Nonne:
https://2015.berlinbuzzwords.de/users/heloise-nonne

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
Oh

00:00:06,380 --> 00:00:13,460
so hi everyone I'm many ways I am data

00:00:09,650 --> 00:00:15,590
scientist at Konami tree this is our

00:00:13,460 --> 00:00:19,910
team you've already seen it this morning

00:00:15,590 --> 00:00:24,289
by at mature and Isabela presentation

00:00:19,910 --> 00:00:27,920
and I am to tell you about an

00:00:24,289 --> 00:00:32,360
experimental project that we we did kind

00:00:27,920 --> 00:00:35,480
of a fan in addition to our consulting

00:00:32,360 --> 00:00:38,780
activities but I am to tell you about

00:00:35,480 --> 00:00:42,140
some some work involving online earning

00:00:38,780 --> 00:00:45,379
baba rabbits and Hadoop I think is that

00:00:42,140 --> 00:00:48,980
everyone is tired this is the end of the

00:00:45,379 --> 00:00:54,440
conference config back is coming so I'm

00:00:48,980 --> 00:00:57,280
gonna start with the story so a long

00:00:54,440 --> 00:01:00,050
time ago in the country far far away

00:00:57,280 --> 00:01:03,050
after one of the most terrible conflict

00:01:00,050 --> 00:01:05,920
the world has ever known a few inspired

00:01:03,050 --> 00:01:09,799
men inventing one of the greatest

00:01:05,920 --> 00:01:15,409
inventions of all time the Turing

00:01:09,799 --> 00:01:18,470
machine actually wasn't really built but

00:01:15,409 --> 00:01:22,520
anyway a few years later some guys came

00:01:18,470 --> 00:01:25,159
up for the first computer and by by 56

00:01:22,520 --> 00:01:28,570
they were able to store three megabytes

00:01:25,159 --> 00:01:33,200
in big machines like that I kind of cool

00:01:28,570 --> 00:01:36,290
so people thought we've got artificial

00:01:33,200 --> 00:01:39,880
intelligence solved we're almost done we

00:01:36,290 --> 00:01:42,649
people invented neural networks and

00:01:39,880 --> 00:01:45,320
people thought ok we're going to we're

00:01:42,649 --> 00:01:48,710
going to build intelligent system clever

00:01:45,320 --> 00:01:54,140
more clever than men and things are

00:01:48,710 --> 00:01:59,840
going to be quite easy in the end didn't

00:01:54,140 --> 00:02:02,509
work that well so next episodes reality

00:01:59,840 --> 00:02:04,399
strikes back and you don't have enough

00:02:02,509 --> 00:02:06,710
computing power you don't have enough

00:02:04,399 --> 00:02:10,150
storage capacity we don't have enough

00:02:06,710 --> 00:02:16,490
data and your algorithm are not

00:02:10,150 --> 00:02:19,599
efficient enough so in the end we are

00:02:16,490 --> 00:02:22,099
still stuck with the regressions

00:02:19,599 --> 00:02:25,730
people are doing regressions today okay

00:02:22,099 --> 00:02:28,819
we still we do some more some more fancy

00:02:25,730 --> 00:02:33,680
stuff right now right now but still many

00:02:28,819 --> 00:02:35,750
people do regression and point is now we

00:02:33,680 --> 00:02:39,140
can do it fast we can do it really fast

00:02:35,750 --> 00:02:42,980
and in a very efficient way so that's

00:02:39,140 --> 00:02:45,680
what I want to talk today about so you

00:02:42,980 --> 00:02:47,840
have a regression what you do it's

00:02:45,680 --> 00:02:49,849
simple you have n samples you have many

00:02:47,840 --> 00:02:53,690
features and you want to predict the

00:02:49,849 --> 00:02:56,450
number so you fit a linear model so you

00:02:53,690 --> 00:02:59,989
have a model with some parameters cold

00:02:56,450 --> 00:03:02,540
weights and this model gives you a

00:02:59,989 --> 00:03:05,030
prediction and you know reality so

00:03:02,540 --> 00:03:07,790
you're going to to see how much error we

00:03:05,030 --> 00:03:09,620
do so you can do this on a simple

00:03:07,790 --> 00:03:12,769
regression or you can you can classify

00:03:09,620 --> 00:03:15,470
this you can classify you have 0 1 you

00:03:12,769 --> 00:03:19,180
have red blue you have if you're at

00:03:15,470 --> 00:03:23,599
retail you're doing click or not click

00:03:19,180 --> 00:03:27,139
Shakespeare to be or not to be anyway

00:03:23,599 --> 00:03:29,359
when you have a regression you need to

00:03:27,139 --> 00:03:32,239
choose a loss function that's going to

00:03:29,359 --> 00:03:35,629
account for how much error your model

00:03:32,239 --> 00:03:38,000
does so according to use case you choose

00:03:35,629 --> 00:03:42,069
different type of loss function that

00:03:38,000 --> 00:03:44,690
accounts for this error you can choose a

00:03:42,069 --> 00:03:46,340
least square for if you're doing simple

00:03:44,690 --> 00:03:48,319
regression of you if you're doing

00:03:46,340 --> 00:03:50,510
classification it choose logistic

00:03:48,319 --> 00:03:53,419
regression or hinge regression

00:03:50,510 --> 00:03:54,799
there are many type of loss function you

00:03:53,419 --> 00:03:57,139
can choose but in the end this is a

00:03:54,799 --> 00:03:59,900
function that accounts for the error and

00:03:57,139 --> 00:04:02,180
you want to minimize this so you're

00:03:59,900 --> 00:04:06,829
going to change your your weights are

00:04:02,180 --> 00:04:14,720
going to according to this traditionally

00:04:06,829 --> 00:04:17,229
you have your doing batch learning so

00:04:14,720 --> 00:04:20,030
with batch learning you have n samples

00:04:17,229 --> 00:04:25,280
you have a model with some parameters to

00:04:20,030 --> 00:04:27,530
weights you initialize this and when you

00:04:25,280 --> 00:04:30,080
initiate this you loads all your

00:04:27,530 --> 00:04:33,979
datasets into memory

00:04:30,080 --> 00:04:36,379
and you you take a sample you predict

00:04:33,979 --> 00:04:39,020
what your model says about this sample

00:04:36,379 --> 00:04:40,939
and you compare it with reality and that

00:04:39,020 --> 00:04:44,330
gives you the individual last function

00:04:40,939 --> 00:04:46,189
of your sample then you do this follow

00:04:44,330 --> 00:04:48,650
your sample and you accumulate

00:04:46,189 --> 00:04:51,080
everything into a global loss function

00:04:48,650 --> 00:04:55,430
that gives you the global error all of

00:04:51,080 --> 00:04:58,969
your model on the entire data set having

00:04:55,430 --> 00:05:03,500
computed this global loss on the entire

00:04:58,969 --> 00:05:06,169
data set you have some simple update

00:05:03,500 --> 00:05:09,909
rule that tells you how to change your

00:05:06,169 --> 00:05:12,590
weights in order to get your model to be

00:05:09,909 --> 00:05:17,300
closer to reality to give better

00:05:12,590 --> 00:05:20,569
prediction and you do this you do one

00:05:17,300 --> 00:05:22,969
iteration at a time but computing and

00:05:20,569 --> 00:05:26,750
computing each time on the entire data

00:05:22,969 --> 00:05:30,529
set and then you iterate many times each

00:05:26,750 --> 00:05:32,719
iteration involves the entire data set

00:05:30,529 --> 00:05:35,360
you have many of these and the

00:05:32,719 --> 00:05:38,750
complexity of each each iteration is o

00:05:35,360 --> 00:05:43,009
of n so it's very very expensive in

00:05:38,750 --> 00:05:45,710
terms of computation so to give you a

00:05:43,009 --> 00:05:48,199
picture of what happens this is the

00:05:45,710 --> 00:05:50,750
parameter space so you have you have two

00:05:48,199 --> 00:05:54,310
weights here but you may have a higher

00:05:50,750 --> 00:05:56,930
dimensional space and for each iteration

00:05:54,310 --> 00:05:59,960
you're trying to find which is the

00:05:56,930 --> 00:06:01,930
direction that is going to bring you to

00:05:59,960 --> 00:06:05,000
the minimum of this last function

00:06:01,930 --> 00:06:08,750
because the minimum of the last function

00:06:05,000 --> 00:06:11,000
here you have the contour of your global

00:06:08,750 --> 00:06:13,909
loss function and you're trying to reach

00:06:11,000 --> 00:06:17,240
this place where this is where your

00:06:13,909 --> 00:06:20,839
error is minimal but you can't do it

00:06:17,240 --> 00:06:22,729
straight because otherwise you start at

00:06:20,839 --> 00:06:26,779
some point with your weights that are

00:06:22,729 --> 00:06:28,669
initiated and and then you cannot choose

00:06:26,779 --> 00:06:30,650
a direction because you're not going to

00:06:28,669 --> 00:06:33,680
go straight to the minimum so you need

00:06:30,650 --> 00:06:36,110
to do many it rash iterations one a one

00:06:33,680 --> 00:06:39,050
after the other each time with a

00:06:36,110 --> 00:06:42,560
complexity of oh of n in order to find

00:06:39,050 --> 00:06:45,889
bit by bit to to go down the slope of

00:06:42,560 --> 00:06:51,740
your loss function and this is really

00:06:45,889 --> 00:06:54,080
expensive and the problem is that you're

00:06:51,740 --> 00:06:57,830
working with your entire data set into

00:06:54,080 --> 00:07:01,339
in-memory so what if your data doesn't

00:06:57,830 --> 00:07:03,349
fit in memory then you you're stuck you

00:07:01,339 --> 00:07:05,360
don't want to subsample because you have

00:07:03,349 --> 00:07:07,789
may have many parameters so you need all

00:07:05,360 --> 00:07:09,500
the data you can get to have a good

00:07:07,789 --> 00:07:12,169
predictive model so I don't want to

00:07:09,500 --> 00:07:15,979
subsample my dear

00:07:12,169 --> 00:07:17,659
what if also you have sample data said

00:07:15,979 --> 00:07:19,430
that fits in memory but you have many

00:07:17,659 --> 00:07:21,529
features and you want to combine them

00:07:19,430 --> 00:07:23,539
together maybe if you combine two

00:07:21,529 --> 00:07:26,300
features it's going to give you a lot

00:07:23,539 --> 00:07:29,060
more information that just just one each

00:07:26,300 --> 00:07:31,550
each of them or if you're doing natural

00:07:29,060 --> 00:07:33,649
language processing you're going to want

00:07:31,550 --> 00:07:35,690
to combine words together so you're

00:07:33,649 --> 00:07:37,310
doing in grams and this inflates your

00:07:35,690 --> 00:07:41,120
data set and that may not fit in memory

00:07:37,310 --> 00:07:43,699
anymore what can happen is also maybe

00:07:41,120 --> 00:07:45,949
some time you you trying on a death set

00:07:43,699 --> 00:07:48,680
and at some point someone gives you a

00:07:45,949 --> 00:07:50,839
new column and you'd like to learn this

00:07:48,680 --> 00:07:53,509
new column but you don't want to lose

00:07:50,839 --> 00:07:57,740
all of you what you have learned on the

00:07:53,509 --> 00:08:00,050
on the on the data set before so you'd

00:07:57,740 --> 00:08:02,569
like to be able to end to handle this

00:08:00,050 --> 00:08:05,599
kind of new features you have a so

00:08:02,569 --> 00:08:07,490
phenomenon such as when your model

00:08:05,599 --> 00:08:10,550
drifts with time you're trying to model

00:08:07,490 --> 00:08:12,409
something for instance the stock market

00:08:10,550 --> 00:08:14,779
is going to change with with time and

00:08:12,409 --> 00:08:18,139
you cannot learn the model long before

00:08:14,779 --> 00:08:21,139
and then predict later because things

00:08:18,139 --> 00:08:23,569
have changed in the in in the mean time

00:08:21,139 --> 00:08:26,509
so you'd like to do some kind of

00:08:23,569 --> 00:08:32,050
learning that can evolve evolve with

00:08:26,509 --> 00:08:35,120
your phenomena and so on and learning is

00:08:32,050 --> 00:08:36,640
one of the solution to to disable to

00:08:35,120 --> 00:08:40,459
deal with that in a very efficient way

00:08:36,640 --> 00:08:43,370
so it works the same way you have n

00:08:40,459 --> 00:08:45,560
samples and and you have a predictive

00:08:43,370 --> 00:08:50,900
model that you initiate with some

00:08:45,560 --> 00:08:53,240
weights and then you actually load just

00:08:50,900 --> 00:08:54,360
one sample and you work on one sample at

00:08:53,240 --> 00:08:57,089
the time in memory

00:08:54,360 --> 00:09:00,749
so you take this sample you computes

00:08:57,089 --> 00:09:04,019
with prediction you can compute in the

00:09:00,749 --> 00:09:06,869
individual loss function on this sample

00:09:04,019 --> 00:09:09,410
but just this one and have an update

00:09:06,869 --> 00:09:14,220
rule that just take the individual loss

00:09:09,410 --> 00:09:17,670
so and a new iterates a lot of time so

00:09:14,220 --> 00:09:20,249
complexity is just all of one because

00:09:17,670 --> 00:09:26,220
kind of all one because you just have

00:09:20,249 --> 00:09:27,899
one sample at a time and to picture

00:09:26,220 --> 00:09:30,239
what's what's going on there

00:09:27,899 --> 00:09:33,360
on the global loss because this is the

00:09:30,239 --> 00:09:36,059
global last you want to minimize if you

00:09:33,360 --> 00:09:38,160
work one sample at the time the sample

00:09:36,059 --> 00:09:40,799
you're minimizing the loss function of

00:09:38,160 --> 00:09:44,069
this sample so you may not go in the

00:09:40,799 --> 00:09:47,420
direction to the minimum of your global

00:09:44,069 --> 00:09:49,709
loss function but you on average

00:09:47,420 --> 00:09:51,689
actually you're going you're going in

00:09:49,709 --> 00:09:54,119
the right direction maybe your sample is

00:09:51,689 --> 00:09:57,319
not representative enough of your entire

00:09:54,119 --> 00:10:01,350
data set so you you may wander around

00:09:57,319 --> 00:10:04,169
your global loss function but in the end

00:10:01,350 --> 00:10:07,439
you get there and the thing is that you

00:10:04,169 --> 00:10:10,139
do it very often you iterate many times

00:10:07,439 --> 00:10:12,959
on your data set and you're going fast

00:10:10,139 --> 00:10:15,509
to the minimum so you you you stabilize

00:10:12,959 --> 00:10:18,899
this kind of stochastic behavior you

00:10:15,509 --> 00:10:20,429
stabilize this by doing many updates one

00:10:18,899 --> 00:10:23,660
on one sample at a time

00:10:20,429 --> 00:10:27,709
so you approach the minimum very quickly

00:10:23,660 --> 00:10:30,209
problem with online learning is that you

00:10:27,709 --> 00:10:32,339
approach this minimum very quickly but

00:10:30,209 --> 00:10:35,220
one when you're there you're still

00:10:32,339 --> 00:10:37,669
processing samples so you get one sample

00:10:35,220 --> 00:10:40,589
and this sample is going to tell you ok

00:10:37,669 --> 00:10:43,470
you're there but my minimum is is not

00:10:40,589 --> 00:10:46,769
there is there and then you move there

00:10:43,470 --> 00:10:49,139
by with your add a true and then you

00:10:46,769 --> 00:10:50,669
process another sample these samples is

00:10:49,139 --> 00:10:52,619
you know my minimum is not there is

00:10:50,669 --> 00:10:54,989
there so that's why you wander around

00:10:52,619 --> 00:10:57,269
your minimum because what you're

00:10:54,989 --> 00:11:00,540
minimizing is the loss function of an

00:10:57,269 --> 00:11:03,779
individual sample at a time and not the

00:11:00,540 --> 00:11:07,980
global loss function so that can be a

00:11:03,779 --> 00:11:12,430
problem in principle but in practice

00:11:07,980 --> 00:11:14,500
it's not it's not so bad so to give you

00:11:12,430 --> 00:11:17,350
an idea of a comparison between online

00:11:14,500 --> 00:11:19,960
earning and batch learning here in blue

00:11:17,350 --> 00:11:22,900
you have online earning and in red you

00:11:19,960 --> 00:11:26,260
have batch running here it's the

00:11:22,900 --> 00:11:30,700
accuracy of your model so when you go

00:11:26,260 --> 00:11:33,940
write your error is going to to diminish

00:11:30,700 --> 00:11:36,820
and on y-axis you have the training time

00:11:33,940 --> 00:11:40,420
so online learning to reach a given

00:11:36,820 --> 00:11:42,250
accuracy online learning is a lot faster

00:11:40,420 --> 00:11:44,050
than budget availing at least at the

00:11:42,250 --> 00:11:47,590
beginning because it's going very fast

00:11:44,050 --> 00:11:49,750
to the minimum and at some point things

00:11:47,590 --> 00:11:52,150
cross batch learning becomes more

00:11:49,750 --> 00:11:54,790
becomes much more efficient to find the

00:11:52,150 --> 00:11:56,560
minimum because it's noiseless online

00:11:54,790 --> 00:11:58,900
learning is going to continue wander

00:11:56,560 --> 00:12:01,510
around a minimum while batch learning is

00:11:58,900 --> 00:12:05,890
slow to get there but once it's there it

00:12:01,510 --> 00:12:08,440
knows where it is and nice idea is to

00:12:05,890 --> 00:12:10,840
combine those two approach start with

00:12:08,440 --> 00:12:14,530
online earning and then do a batch

00:12:10,840 --> 00:12:20,890
learning for the last few bits of the

00:12:14,530 --> 00:12:24,220
last few steps to reach the minimum so

00:12:20,890 --> 00:12:26,890
what I want to tell you about is a

00:12:24,220 --> 00:12:30,790
library that implements online earning

00:12:26,890 --> 00:12:34,420
and that we we started to use a few

00:12:30,790 --> 00:12:36,790
months ago which is called and so it's

00:12:34,420 --> 00:12:40,720
called wobba wobba wobba wobba - it's a

00:12:36,790 --> 00:12:42,730
straight name but it's it's an open

00:12:40,720 --> 00:12:46,450
source project that's been developed at

00:12:42,730 --> 00:12:50,440
Yahoo by John Lankford it's written in

00:12:46,450 --> 00:12:57,610
C++ and it's implements online learning

00:12:50,440 --> 00:13:01,750
very very efficient way and as you can

00:12:57,610 --> 00:13:04,360
see on you can see maybe you can't see

00:13:01,750 --> 00:13:06,580
you so well for some people but last

00:13:04,360 --> 00:13:08,820
year there was a really increase of

00:13:06,580 --> 00:13:10,810
interest in vibhava bits that's

00:13:08,820 --> 00:13:13,240
translating to the number of comments

00:13:10,810 --> 00:13:15,370
you can see on the project so people

00:13:13,240 --> 00:13:18,670
really got excited about that I'm gonna

00:13:15,370 --> 00:13:20,790
tell you a bit why why is it so

00:13:18,670 --> 00:13:25,200
interesting to use

00:13:20,790 --> 00:13:28,530
so Papa rabbit has many algorithm

00:13:25,200 --> 00:13:30,960
implemented in its system so you have

00:13:28,530 --> 00:13:32,910
regression you have fancy regression you

00:13:30,960 --> 00:13:36,330
have k-means we even have nonlinear

00:13:32,910 --> 00:13:38,820
models such as neural networks or SVM so

00:13:36,330 --> 00:13:41,400
you've got basically you've got

00:13:38,820 --> 00:13:44,060
everything you've got more than the

00:13:41,400 --> 00:13:47,070
basics of machine learning in there and

00:13:44,060 --> 00:13:50,130
it's implemented in a very efficient way

00:13:47,070 --> 00:13:51,750
with clever algorithms such at vs GS

00:13:50,130 --> 00:13:55,140
conjugate gradients anyway

00:13:51,750 --> 00:13:57,600
the clever algorithm plus some tricks to

00:13:55,140 --> 00:14:00,630
to do the learning in a very efficient

00:13:57,600 --> 00:14:04,470
way it can also it has the ability to

00:14:00,630 --> 00:14:07,410
combine features do engrams process text

00:14:04,470 --> 00:14:09,180
and everything and the other thing is

00:14:07,410 --> 00:14:11,940
that you

00:14:09,180 --> 00:14:13,830
it has implemented by default some

00:14:11,940 --> 00:14:16,950
automatic tuning of your hyper

00:14:13,830 --> 00:14:21,030
parameters the learning rates for

00:14:16,950 --> 00:14:23,940
instance is how fast you go down to your

00:14:21,030 --> 00:14:27,330
minimum and this is very critical and in

00:14:23,940 --> 00:14:29,820
practice to to train another Gordon but

00:14:27,330 --> 00:14:32,210
it's very difficult to tune it depends

00:14:29,820 --> 00:14:35,430
on your use case and everything so

00:14:32,210 --> 00:14:38,220
actually the default settings of Papa

00:14:35,430 --> 00:14:40,710
rabbits allow you to reach already an

00:14:38,220 --> 00:14:43,200
efficiency without much work and that's

00:14:40,710 --> 00:14:46,050
very valuable when you start to discover

00:14:43,200 --> 00:14:49,160
datasets you want to be efficient very

00:14:46,050 --> 00:14:51,860
fast so you can tune the last bit and

00:14:49,160 --> 00:14:55,170
low-power a bit is good for that because

00:14:51,860 --> 00:14:57,900
it's it's kind of efficient by default

00:14:55,170 --> 00:15:01,500
in most cases and it has some more

00:14:57,900 --> 00:15:04,050
features that are pretty nice another

00:15:01,500 --> 00:15:08,190
thing is the input formats it looks a

00:15:04,050 --> 00:15:10,050
bit weird at the beginning but so so

00:15:08,190 --> 00:15:13,170
that's that's the kind of input you have

00:15:10,050 --> 00:15:17,640
to to to give to the power bit so here

00:15:13,170 --> 00:15:19,410
you have a label it's 1 or minus 1 you

00:15:17,640 --> 00:15:21,180
may have some weight doesn't really

00:15:19,410 --> 00:15:23,280
matter and then your features that are

00:15:21,180 --> 00:15:25,470
separated separated by pipes so you can

00:15:23,280 --> 00:15:27,510
bubble your features together give it

00:15:25,470 --> 00:15:29,610
numbers you can process binary in

00:15:27,510 --> 00:15:34,380
numerical features categorical features

00:15:29,610 --> 00:15:38,460
and it can process text very easily

00:15:34,380 --> 00:15:41,490
it can also handle missing values like

00:15:38,460 --> 00:15:43,440
here you see you have two two features

00:15:41,490 --> 00:15:45,060
height and length and here just have

00:15:43,440 --> 00:15:47,520
length doesn't matter for what pal

00:15:45,060 --> 00:15:50,130
because it doesn't expect to have all

00:15:47,520 --> 00:15:53,610
the features at all time so it handles

00:15:50,130 --> 00:15:56,460
this sparse features a set and missing

00:15:53,610 --> 00:15:58,110
values very well and that's quite

00:15:56,460 --> 00:16:00,540
valuable when you have the dates that

00:15:58,110 --> 00:16:03,390
you want to discover because it allows

00:16:00,540 --> 00:16:05,610
you to just avoid some data preparation

00:16:03,390 --> 00:16:07,680
step you want to avoid when you start to

00:16:05,610 --> 00:16:10,170
try to understand what's what's going on

00:16:07,680 --> 00:16:15,660
in your when you dead set and what

00:16:10,170 --> 00:16:18,450
you're trying to model so summarize

00:16:15,660 --> 00:16:21,300
low-power a bit it's an efficient

00:16:18,450 --> 00:16:23,130
leverage you can do fast turning in

00:16:21,300 --> 00:16:24,960
itself for scoring that's what it was

00:16:23,130 --> 00:16:29,910
designed for but also the other feature

00:16:24,960 --> 00:16:31,800
can allow you to not spend too much time

00:16:29,910 --> 00:16:34,290
to prepare your data to start to

00:16:31,800 --> 00:16:36,570
discover phenomenon so it's also a very

00:16:34,290 --> 00:16:38,700
good tool for experimentation maybe even

00:16:36,570 --> 00:16:42,870
if you plan to use a more complex model

00:16:38,700 --> 00:16:44,490
later it's it's quite nice so you're

00:16:42,870 --> 00:16:49,860
fast you're doing online learning with a

00:16:44,490 --> 00:16:55,620
fast fast library but you would like to

00:16:49,860 --> 00:16:58,380
you'd like to paralyze this and so you'd

00:16:55,620 --> 00:17:00,540
like to paralyze the power a bit you can

00:16:58,380 --> 00:17:05,640
do it actually it's been implemented in

00:17:00,540 --> 00:17:08,040
football but why would you do this so

00:17:05,640 --> 00:17:10,230
first you you're fast but you want to

00:17:08,040 --> 00:17:12,959
speed up you want to spread your jobs on

00:17:10,230 --> 00:17:16,560
different nodes so you'd like to be

00:17:12,959 --> 00:17:18,990
faster or maybe your data isn't fits in

00:17:16,560 --> 00:17:20,640
your in a single machine storage maybe

00:17:18,990 --> 00:17:23,490
you will have one terabyte of data that

00:17:20,640 --> 00:17:27,000
doesn't fit in your laptop and you don't

00:17:23,490 --> 00:17:28,590
want to subsample or you have data you

00:17:27,000 --> 00:17:30,930
want to take advantage of this rich

00:17:28,590 --> 00:17:37,710
storage so you don't move that that data

00:17:30,930 --> 00:17:39,570
around your clusters and the other the

00:17:37,710 --> 00:17:42,060
other reason you would want to paralyze

00:17:39,570 --> 00:17:44,700
is to take advantage of this distributed

00:17:42,060 --> 00:17:46,950
memory because on signal mushing you're

00:17:44,700 --> 00:17:47,820
having a memory to maybe inflate your

00:17:46,950 --> 00:17:51,780
data set

00:17:47,820 --> 00:17:56,540
by combining features together so how to

00:17:51,780 --> 00:17:59,010
do this so the idea is as I told you

00:17:56,540 --> 00:18:02,760
nice ideas to combine online learning

00:17:59,010 --> 00:18:05,880
and batch learning so you would send a

00:18:02,760 --> 00:18:08,430
job too many notes ask them to do an

00:18:05,880 --> 00:18:12,120
online pass to start learning on each

00:18:08,430 --> 00:18:14,790
one on its on its own data and then you

00:18:12,120 --> 00:18:18,270
would like to send back the weights that

00:18:14,790 --> 00:18:19,470
it has learned somewhere to average the

00:18:18,270 --> 00:18:22,170
knowledge that's each node has

00:18:19,470 --> 00:18:26,190
accumulated so you combine this together

00:18:22,170 --> 00:18:28,140
you average this and you Brooke you

00:18:26,190 --> 00:18:30,810
would broadcast down this average back

00:18:28,140 --> 00:18:32,700
to the notes and maybe start other

00:18:30,810 --> 00:18:36,390
running so doing batch learning and

00:18:32,700 --> 00:18:38,340
maybe do some iteration so maybe so you

00:18:36,390 --> 00:18:41,340
really reach your minimum of your loss

00:18:38,340 --> 00:18:43,920
function and it's been implemented in

00:18:41,340 --> 00:18:51,270
verbal a bit projects by John Langford

00:18:43,920 --> 00:18:54,390
and some collaborators and I will

00:18:51,270 --> 00:18:57,830
present this approach that they use they

00:18:54,390 --> 00:19:02,040
implemented and gives you a little

00:18:57,830 --> 00:19:04,770
account for our experience using it so

00:19:02,040 --> 00:19:09,510
what I wanted to do is to capitalize to

00:19:04,770 --> 00:19:12,120
use both all reduce which I'm going to

00:19:09,510 --> 00:19:12,510
tell you about later or reduce and Map

00:19:12,120 --> 00:19:15,540
Reduce

00:19:12,510 --> 00:19:19,020
so what you need to do when you paralyze

00:19:15,540 --> 00:19:20,580
this this thing is you you want an

00:19:19,020 --> 00:19:21,990
effective communication infrastructure

00:19:20,580 --> 00:19:24,290
because you need to communicate do

00:19:21,990 --> 00:19:27,270
weights between nodes and and everything

00:19:24,290 --> 00:19:30,120
you need that that's what that eccentric

00:19:27,270 --> 00:19:32,010
platform so MapReduce is no good for

00:19:30,120 --> 00:19:34,020
that because you have full knowledge of

00:19:32,010 --> 00:19:36,480
the data allocation and handles handles

00:19:34,020 --> 00:19:41,520
things well you want a fault-tolerant

00:19:36,480 --> 00:19:44,850
system and you you want some way to not

00:19:41,520 --> 00:19:47,070
to recode everything because you've got

00:19:44,850 --> 00:19:48,690
a nice implementation in c plus with

00:19:47,070 --> 00:19:51,780
with wapa rabbit and you'd want to

00:19:48,690 --> 00:19:55,350
recode all your implementation or your

00:19:51,780 --> 00:19:58,830
optimization to java or some other

00:19:55,350 --> 00:20:01,470
language so you'd like to be able to

00:19:58,830 --> 00:20:05,639
avoid rewriting everything

00:20:01,470 --> 00:20:09,649
and I'll reduce so the combination of

00:20:05,639 --> 00:20:13,169
all reduce and Map Reduce they use

00:20:09,649 --> 00:20:16,649
allows to to do this paralyzation all

00:20:13,169 --> 00:20:20,429
reduce is based on a communication

00:20:16,649 --> 00:20:24,570
structure entry so you have a tree you

00:20:20,429 --> 00:20:27,210
build a tree on over your notes and each

00:20:24,570 --> 00:20:30,289
node has a number it's like the result

00:20:27,210 --> 00:20:33,120
of the online pass it's the weights so

00:20:30,289 --> 00:20:35,940
here you have one weight but you have

00:20:33,120 --> 00:20:38,610
actually a vector of weights and first

00:20:35,940 --> 00:20:41,460
type each one starts with a number and

00:20:38,610 --> 00:20:44,669
the first step is to this reduce step so

00:20:41,460 --> 00:20:47,669
you want to sum up your weights all the

00:20:44,669 --> 00:20:51,509
way up your tree in order to do the

00:20:47,669 --> 00:20:54,299
average so you're going to sum up these

00:20:51,509 --> 00:20:56,490
two children are going to send that

00:20:54,299 --> 00:20:59,970
number to the parents and the parent is

00:20:56,490 --> 00:21:04,049
going to make the addition so 8 + 9 10 1

00:20:59,970 --> 00:21:07,769
is 17 and the other one thumbs up to 16

00:21:04,049 --> 00:21:10,769
then you do you do the sum up to the

00:21:07,769 --> 00:21:13,590
parents and you end up with 34 at the

00:21:10,769 --> 00:21:16,019
top and then the second step of our

00:21:13,590 --> 00:21:18,570
reduce is to broadcast down the result

00:21:16,019 --> 00:21:22,740
of your addition down to all your tree

00:21:18,570 --> 00:21:25,409
and every node ends up with the sum

00:21:22,740 --> 00:21:27,299
everybody and it just need to divide by

00:21:25,409 --> 00:21:30,210
the number of nodes to have the average

00:21:27,299 --> 00:21:33,629
of the number so that's quite a simple

00:21:30,210 --> 00:21:36,450
idea and that's why they when they in

00:21:33,629 --> 00:21:37,950
turn did this they thought it was good

00:21:36,450 --> 00:21:41,750
an efficient communication

00:21:37,950 --> 00:21:45,769
infrastructure to build them and

00:21:41,750 --> 00:21:49,309
combining with MapReduce that gives you

00:21:45,769 --> 00:21:52,159
the the implementation of

00:21:49,309 --> 00:21:55,529
parallelization of Apple a bit on Hadoop

00:21:52,159 --> 00:21:59,039
so you start your daemon that's your

00:21:55,529 --> 00:22:01,980
communication system and then you send

00:21:59,039 --> 00:22:03,600
your your job with MapReduce actually

00:22:01,980 --> 00:22:06,450
send just a mapper you don't have a

00:22:03,600 --> 00:22:10,139
reducer you just send a mapper to each

00:22:06,450 --> 00:22:13,169
node and each node is going to make an

00:22:10,139 --> 00:22:15,220
online passwords data then you

00:22:13,169 --> 00:22:18,010
initialize your tree you bind

00:22:15,220 --> 00:22:20,620
three that allows you to to send back

00:22:18,010 --> 00:22:24,909
your your weights to the parents to the

00:22:20,620 --> 00:22:27,309
masternode you use all reduce algorithm

00:22:24,909 --> 00:22:30,340
to average the weights over all the

00:22:27,309 --> 00:22:33,370
notes that's the result of the ulna pass

00:22:30,340 --> 00:22:36,039
and then you send it back to all the

00:22:33,370 --> 00:22:39,150
notes so that they can do their batch

00:22:36,039 --> 00:22:44,830
steps so there are a few bad steps to do

00:22:39,150 --> 00:22:47,590
in a row and so they do their bad step

00:22:44,830 --> 00:22:49,690
they sent back the weight its average

00:22:47,590 --> 00:22:52,090
and then it's sent down again to the

00:22:49,690 --> 00:22:58,929
notes so they can do another bad step

00:22:52,090 --> 00:23:00,640
and you iterate so the advantage of this

00:22:58,929 --> 00:23:03,130
implementation is that you have a

00:23:00,640 --> 00:23:05,440
minimal additional programming effort in

00:23:03,130 --> 00:23:07,929
the sense that you're using MapReduce

00:23:05,440 --> 00:23:10,679
you hacking somehow MapReduce just to

00:23:07,929 --> 00:23:13,240
send a mapper would pop all wabbits

00:23:10,679 --> 00:23:15,280
comments asking for power that's

00:23:13,240 --> 00:23:20,740
installed on your all your all your all

00:23:15,280 --> 00:23:23,409
your nodes to do the job so my produce

00:23:20,740 --> 00:23:27,640
is just kind of a way to send job and

00:23:23,409 --> 00:23:30,000
and that's it and you capitalize on the

00:23:27,640 --> 00:23:35,230
data allocation knowledge of MapReduce

00:23:30,000 --> 00:23:36,700
with this implementation all reduce

00:23:35,230 --> 00:23:39,159
allows you to have a small

00:23:36,700 --> 00:23:42,370
synchronization of our heads because you

00:23:39,159 --> 00:23:46,860
have a tree so you're in log of number

00:23:42,370 --> 00:23:49,360
of tree so the time spent in all this

00:23:46,860 --> 00:23:51,940
operation is much lower than the

00:23:49,360 --> 00:23:56,230
computation time on each node so you'll

00:23:51,940 --> 00:23:58,630
find there are also tricks to use to

00:23:56,230 --> 00:24:00,970
capitalize on Hadoop speculative

00:23:58,630 --> 00:24:04,600
execution to handle the problem with

00:24:00,970 --> 00:24:08,799
snow nodes and dead nodes that's quite

00:24:04,600 --> 00:24:10,900
clever but I could go I could discuss

00:24:08,799 --> 00:24:13,780
that with you later I don't want to go

00:24:10,900 --> 00:24:15,760
into details and combining online

00:24:13,780 --> 00:24:18,010
learning and batch learning gives you

00:24:15,760 --> 00:24:19,870
rapid convergence because with online

00:24:18,010 --> 00:24:22,419
learning paths you're reaching the

00:24:19,870 --> 00:24:24,250
surrounding of your minimum and then

00:24:22,419 --> 00:24:27,690
batch learning helps you to do the last

00:24:24,250 --> 00:24:27,690
steps of the running

00:24:27,780 --> 00:24:34,050
so that's the theory and we thought okay

00:24:32,070 --> 00:24:37,710
it's cool it's implemented it's gonna

00:24:34,050 --> 00:24:41,970
work so you download the code on github

00:24:37,710 --> 00:24:43,350
and in principle log work so you know

00:24:41,970 --> 00:24:46,110
the theory you've read paper you

00:24:43,350 --> 00:24:47,790
understand how it works and in principle

00:24:46,110 --> 00:24:50,160
you start reading and there is a code in

00:24:47,790 --> 00:24:52,080
the github called the spanning tree it's

00:24:50,160 --> 00:24:53,790
set up to your communication system you

00:24:52,080 --> 00:24:55,830
launch your MapReduce job

00:24:53,790 --> 00:24:59,820
you kill your spanning tree and it works

00:24:55,830 --> 00:25:02,490
and your MapReduce job is using Hadoop

00:24:59,820 --> 00:25:04,590
streaming it's very simple you want

00:25:02,490 --> 00:25:06,900
speculative execution you don't have any

00:25:04,590 --> 00:25:10,470
register input output that's simple you

00:25:06,900 --> 00:25:14,250
load a few libraries that verbal web it

00:25:10,470 --> 00:25:17,280
needs to work and you give it a mapper

00:25:14,250 --> 00:25:22,230
and that's it but it doesn't work that

00:25:17,280 --> 00:25:25,830
well so first we decided to use Amazon

00:25:22,230 --> 00:25:27,780
Web Services to do this but honestly it

00:25:25,830 --> 00:25:29,610
was the first time we with we used it

00:25:27,780 --> 00:25:34,080
it's a bit different that the usual

00:25:29,610 --> 00:25:37,650
Hadoop cluster we were used to with with

00:25:34,080 --> 00:25:40,590
Amazon what you do is you use transient

00:25:37,650 --> 00:25:43,670
Kuster so instead of having you cluster

00:25:40,590 --> 00:25:46,710
up and running and signing job and

00:25:43,670 --> 00:25:50,360
monitoring what's what's going on what

00:25:46,710 --> 00:25:54,150
you have is your data on s3 buckets and

00:25:50,360 --> 00:25:57,180
you start an EMF coaster you start your

00:25:54,150 --> 00:25:59,700
cluster you maybe strap some actions

00:25:57,180 --> 00:26:01,590
like installing libraries such as low

00:25:59,700 --> 00:26:04,050
power E's and other things you need some

00:26:01,590 --> 00:26:07,410
configuration so you can just do it with

00:26:04,050 --> 00:26:07,830
the simple scripts and then you run your

00:26:07,410 --> 00:26:10,260
job

00:26:07,830 --> 00:26:12,990
that's called step in in Amazon Web

00:26:10,260 --> 00:26:15,990
Services and when the job is done you're

00:26:12,990 --> 00:26:19,230
shut down you cluster so actually your

00:26:15,990 --> 00:26:21,000
cluster is only live during it is it's

00:26:19,230 --> 00:26:23,250
live only during the time of your

00:26:21,000 --> 00:26:25,800
MapReduce job and then it's killed

00:26:23,250 --> 00:26:27,960
afterwards so it's a bit weird when

00:26:25,800 --> 00:26:30,830
you're not used to it it has some

00:26:27,960 --> 00:26:33,330
advantages and it has some drawbacks

00:26:30,830 --> 00:26:35,190
advantages is that it's easy to say that

00:26:33,330 --> 00:26:38,580
it works you don't have to do

00:26:35,190 --> 00:26:41,700
maintenance it's low cost problem is

00:26:38,580 --> 00:26:43,410
that we really struggle to find the logs

00:26:41,700 --> 00:26:44,940
the logs are easy to find on how to

00:26:43,410 --> 00:26:47,670
cluster because you know where they are

00:26:44,940 --> 00:26:50,280
what they are and it took us some time

00:26:47,670 --> 00:26:52,470
to figure out what to find them and

00:26:50,280 --> 00:26:56,690
there is some configuration that's not

00:26:52,470 --> 00:27:00,690
by defaults you need to set up debugging

00:26:56,690 --> 00:27:03,030
debugging option in Hamas on so really

00:27:00,690 --> 00:27:09,120
read the dark of Amazon because

00:27:03,030 --> 00:27:13,860
otherwise it's difficult to find so once

00:27:09,120 --> 00:27:16,650
we we got this Amazon cluster up and

00:27:13,860 --> 00:27:20,820
running you you need to make it work

00:27:16,650 --> 00:27:23,190
so Papa rabbits needs to work it needs

00:27:20,820 --> 00:27:25,169
MapReduce environment variables it needs

00:27:23,190 --> 00:27:30,570
to know the number and the total number

00:27:25,169 --> 00:27:33,990
of of tasks because it just needs to

00:27:30,570 --> 00:27:36,059
divide the sum over all nodes by the

00:27:33,990 --> 00:27:38,400
number of nodes so it's not nice to know

00:27:36,059 --> 00:27:41,340
that it needs to know in some IDs it

00:27:38,400 --> 00:27:44,130
needs to know the private DNS of the

00:27:41,340 --> 00:27:48,870
parents and the children in order to

00:27:44,130 --> 00:27:52,549
communicate with all reduce and so the

00:27:48,870 --> 00:27:54,870
problem is that this implementation of

00:27:52,549 --> 00:27:58,860
paralyzation of a puppet was done for

00:27:54,870 --> 00:28:01,320
Hadoop one and the name of name of the

00:27:58,860 --> 00:28:03,290
environment variable changed so you have

00:28:01,320 --> 00:28:06,330
to change then you have to find out

00:28:03,290 --> 00:28:10,950
change a bit the code that you can find

00:28:06,330 --> 00:28:13,530
on github and the MapReduce the mapper

00:28:10,950 --> 00:28:17,190
that you have is written in shell in

00:28:13,530 --> 00:28:19,200
bash and getting the environment

00:28:17,190 --> 00:28:22,380
variables were not possible so we had to

00:28:19,200 --> 00:28:24,299
hacker a bit with Python to to reget

00:28:22,380 --> 00:28:26,540
this thing and took us some time to

00:28:24,299 --> 00:28:30,690
figure out what what was the problem

00:28:26,540 --> 00:28:33,179
another problem that you have when you

00:28:30,690 --> 00:28:36,480
try to run this is the number of splits

00:28:33,179 --> 00:28:40,799
you need to brute-force the MapReduce

00:28:36,480 --> 00:28:43,890
job to to do the number of plaits you

00:28:40,799 --> 00:28:45,809
you want that means the number you know

00:28:43,890 --> 00:28:48,990
you need the number of splits to be

00:28:45,809 --> 00:28:51,380
equal to the number of nodes you want to

00:28:48,990 --> 00:28:54,260
do your job on

00:28:51,380 --> 00:28:56,720
and we got some hints from jong-hong

00:28:54,260 --> 00:29:01,400
ford codes because he was kind of

00:28:56,720 --> 00:29:03,140
computing this by by inside the code and

00:29:01,400 --> 00:29:07,190
using this option but it doesn't work

00:29:03,140 --> 00:29:09,170
anymore so we kind of did we were we

00:29:07,190 --> 00:29:11,000
were shorting time so we do dirty work

00:29:09,170 --> 00:29:13,880
around to split the data and put

00:29:11,000 --> 00:29:17,930
everything into a gzip file because GC

00:29:13,880 --> 00:29:20,000
files are not are not spit able so my

00:29:17,930 --> 00:29:26,420
producer won't try to split it more than

00:29:20,000 --> 00:29:28,430
you want and there was a last thing if

00:29:26,420 --> 00:29:32,840
you try to run it there is an option

00:29:28,430 --> 00:29:36,650
called called that involves the number

00:29:32,840 --> 00:29:39,200
how much RAM you want to allocate to

00:29:36,650 --> 00:29:43,160
vibhava bits and this I haven't figured

00:29:39,200 --> 00:29:45,620
out what's the problem with it yet but

00:29:43,160 --> 00:29:46,610
you have to be aware of this Ram

00:29:45,620 --> 00:29:48,770
allocation

00:29:46,610 --> 00:29:51,140
also that doesn't work very well on a

00:29:48,770 --> 00:29:53,000
single machine even if you paralyze on a

00:29:51,140 --> 00:29:57,140
single machine but doesn't work anymore

00:29:53,000 --> 00:30:01,400
on Hadoop being aware of that we solved

00:29:57,140 --> 00:30:05,840
all this and as you can see it's it

00:30:01,400 --> 00:30:08,330
works so here this is the log from the

00:30:05,840 --> 00:30:12,140
node so this is this comes from the

00:30:08,330 --> 00:30:14,480
bagua bit and here you can see it's the

00:30:12,140 --> 00:30:16,580
IP number of the nodes it starts raining

00:30:14,480 --> 00:30:20,360
and does some stuff so it gives you some

00:30:16,580 --> 00:30:24,950
parameters it's going to use and there

00:30:20,360 --> 00:30:28,280
it starts online learning and when it's

00:30:24,950 --> 00:30:30,260
done when it's done when it's completed

00:30:28,280 --> 00:30:33,020
so the first thing is the last thing of

00:30:30,260 --> 00:30:35,530
all the all the Passover dataset it

00:30:33,020 --> 00:30:38,090
connects to its parents send its waits

00:30:35,530 --> 00:30:40,070
waits for the answer from parents

00:30:38,090 --> 00:30:44,240
because apparently going to send back

00:30:40,070 --> 00:30:48,140
the the average and then it tells you

00:30:44,240 --> 00:30:50,240
some help on how much examples it it has

00:30:48,140 --> 00:30:53,480
been working and then you do the batch

00:30:50,240 --> 00:30:56,120
the batch part so BFGS is a kind of an

00:30:53,480 --> 00:30:58,400
implementation of batch learning it runs

00:30:56,120 --> 00:31:02,030
it connects to its parents it waits for

00:30:58,400 --> 00:31:04,210
the answer with the sum over all nodes

00:31:02,030 --> 00:31:08,100
and when you reach the

00:31:04,210 --> 00:31:11,140
of steps you've set you it stops and

00:31:08,100 --> 00:31:19,740
sends the results back to back to

00:31:11,140 --> 00:31:23,140
masternode so I tried to do a benchmark

00:31:19,740 --> 00:31:28,780
actually this was done yesterday at 2:00

00:31:23,140 --> 00:31:31,990
a.m. so a bit short but so I've tried I

00:31:28,780 --> 00:31:34,930
haven't savvy an eyeball with a hotel to

00:31:31,990 --> 00:31:39,100
upload a lot of data on Hadoop but just

00:31:34,930 --> 00:31:44,640
with six gigabytes that means 15 50

00:31:39,100 --> 00:31:47,770
million example running on 52 billions

00:31:44,640 --> 00:31:50,890
features means that I've done many

00:31:47,770 --> 00:31:53,740
combination of all this feature on a

00:31:50,890 --> 00:31:55,660
single machine means on my laptop was

00:31:53,740 --> 00:31:58,780
able to run this with just online

00:31:55,660 --> 00:32:02,470
earning in 26 minutes and on a Hadoop it

00:31:58,780 --> 00:32:05,200
worked in 6 minutes so it don't speed up

00:32:02,470 --> 00:32:07,930
a lot and you have to take into account

00:32:05,200 --> 00:32:10,060
that this was quite a small data set and

00:32:07,930 --> 00:32:13,740
on that that I said it should really

00:32:10,060 --> 00:32:22,560
speed up your online earning pass for

00:32:13,740 --> 00:32:26,440
similar similar accuracy so to conclude

00:32:22,560 --> 00:32:30,040
about online learning online learning is

00:32:26,440 --> 00:32:32,140
is fine when whenever you your

00:32:30,040 --> 00:32:34,240
computation time is the bottleneck of

00:32:32,140 --> 00:32:37,090
your job then you should use the online

00:32:34,240 --> 00:32:39,970
learning even if your laptop even if you

00:32:37,090 --> 00:32:42,940
have enough RAM it's always good to to

00:32:39,970 --> 00:32:44,950
use online learning because you can then

00:32:42,940 --> 00:32:47,290
you can process more data and you can

00:32:44,950 --> 00:32:49,060
include more feature more combination in

00:32:47,290 --> 00:32:51,040
your analysis and that's very useful for

00:32:49,060 --> 00:32:53,260
scoring but also very useful for

00:32:51,040 --> 00:32:58,630
research and experimentation and data

00:32:53,260 --> 00:33:00,550
set and working this projects we got to

00:32:58,630 --> 00:33:02,710
really understand what was going on so

00:33:00,550 --> 00:33:06,520
understand the basic of optimization

00:33:02,710 --> 00:33:12,010
algorithm and understand a lot on how to

00:33:06,520 --> 00:33:15,600
put this on a Hadoop so we really were

00:33:12,010 --> 00:33:18,130
very interesting and those guys long for

00:33:15,600 --> 00:33:21,070
number two I got one they

00:33:18,130 --> 00:33:23,680
did a lot of work and in implementing

00:33:21,070 --> 00:33:29,320
this and speaking speeding up online

00:33:23,680 --> 00:33:33,160
learning so the last episode is coming

00:33:29,320 --> 00:33:36,250
soon there are still things to debug on

00:33:33,160 --> 00:33:37,900
the codes but we were willing to do some

00:33:36,250 --> 00:33:40,930
pushes on get up so if you are

00:33:37,900 --> 00:33:43,420
interested just it's going to come soon

00:33:40,930 --> 00:33:48,460
when we solve the last bit of problems

00:33:43,420 --> 00:33:51,160
and what we would like to do is of

00:33:48,460 --> 00:33:55,930
course benchmark on very large data set

00:33:51,160 --> 00:33:58,150
more more just more large data set and

00:33:55,930 --> 00:34:01,750
they are available there are solutions

00:33:58,150 --> 00:34:03,700
such as graphlab and ml leap that

00:34:01,750 --> 00:34:06,850
implements online learning and we would

00:34:03,700 --> 00:34:12,480
like to know how how about what it

00:34:06,850 --> 00:34:16,720
compares with with this dissolution and

00:34:12,480 --> 00:34:20,280
so if you have any question um I'm done

00:34:16,720 --> 00:34:20,280
and thank you for listening

00:34:24,140 --> 00:34:27,310

YouTube URL: https://www.youtube.com/watch?v=anZa5gxJPjQ


