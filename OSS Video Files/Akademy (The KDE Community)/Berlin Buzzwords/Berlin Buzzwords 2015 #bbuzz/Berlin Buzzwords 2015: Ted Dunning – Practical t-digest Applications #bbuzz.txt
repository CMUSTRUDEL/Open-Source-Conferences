Title: Berlin Buzzwords 2015: Ted Dunning â€“ Practical t-digest Applications #bbuzz
Publication date: 2015-06-02
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	The t-digest is a state-of-the-art algorithm for computing approximate quantiles with adjustable accuracy limits and very few limitations.

Implementations of t-digest algorithm are easy to use and have been integrated in all kinds of software from ElasticSearch to Apache Mahout. Certain kinds of queries such as finding the top 99.999th %-ile can be accelerated by several orders of magnitude by using t-digest.

I will describe the basic algorithm and demonstrate the effect of some variations of the algorithm. I will also show how to use the algorithm in your code or your queries.

Read more:
https://2015.berlinbuzzwords.de/session/practical-t-digest-applications

About Ted Dunning:
https://2015.berlinbuzzwords.de/users/ted-dunning

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,660 --> 00:00:11,530
I'm gonna talk about T ty jest here

00:00:08,090 --> 00:00:14,480
which is a pretty geeky sort of talk

00:00:11,530 --> 00:00:17,930
I'm Ted Dunning I'm chief application

00:00:14,480 --> 00:00:20,419
architected map R but I'm also an Apache

00:00:17,930 --> 00:00:23,090
member and committer on many projects in

00:00:20,419 --> 00:00:26,449
this particular project that I'm talking

00:00:23,090 --> 00:00:29,359
today is completely open source so no

00:00:26,449 --> 00:00:33,170
need for the map R hat this is with

00:00:29,359 --> 00:00:35,960
Apache hat on so I'd also like to

00:00:33,170 --> 00:00:39,890
mention before I go this is appropriate

00:00:35,960 --> 00:00:42,920
for anomaly detection we have a book a

00:00:39,890 --> 00:00:44,899
small book that describes anomaly

00:00:42,920 --> 00:00:47,539
detection techniques that Ellen and I

00:00:44,899 --> 00:00:50,719
wrote we also have a book on time series

00:00:47,539 --> 00:00:54,140
for very high performance ingest for

00:00:50,719 --> 00:00:57,170
Internet of Things and most recently we

00:00:54,140 --> 00:01:00,020
have a book about real world Hadoop this

00:00:57,170 --> 00:01:02,629
whole series is about practical things

00:01:00,020 --> 00:01:04,670
that can be put into a small book that

00:01:02,629 --> 00:01:08,420
you might be able to use in a day or a

00:01:04,670 --> 00:01:10,569
week well let's talk about tea digest

00:01:08,420 --> 00:01:13,670
here so I want to talk first about

00:01:10,569 --> 00:01:16,130
quantiles and why we should care why we

00:01:13,670 --> 00:01:19,630
should do this online and then I'm going

00:01:16,130 --> 00:01:23,750
to talk a little bit about how to digest

00:01:19,630 --> 00:01:27,020
actually works and then how you can get

00:01:23,750 --> 00:01:29,450
it how you can use it so the first

00:01:27,020 --> 00:01:31,640
question is why do we need online

00:01:29,450 --> 00:01:33,770
algorithms and an online algorithm is

00:01:31,640 --> 00:01:37,250
one that always gives you the answer

00:01:33,770 --> 00:01:39,110
right now it takes data it has a finite

00:01:37,250 --> 00:01:41,899
amount of memory and you can get the

00:01:39,110 --> 00:01:44,959
answer from it at any given time well

00:01:41,899 --> 00:01:47,500
here's an example of why you need online

00:01:44,959 --> 00:01:50,149
algorithms this is from a very popular

00:01:47,500 --> 00:01:51,500
URL shortening site I won't say their

00:01:50,149 --> 00:01:53,720
name because it's probably too

00:01:51,500 --> 00:01:56,569
embarrassing but if you notice there are

00:01:53,720 --> 00:02:01,280
seven clicks today on ten different

00:01:56,569 --> 00:02:07,119
links seven clicks ten different links

00:02:01,280 --> 00:02:12,620
sixteen of the seven clicks came from

00:02:07,119 --> 00:02:15,380
buzzwords okay seven clicks ten links

00:02:12,620 --> 00:02:17,120
all nonzero apparently I think we have a

00:02:15,380 --> 00:02:19,520
problem from the beginning and then

00:02:17,120 --> 00:02:23,000
sixteen nine five and four

00:02:19,520 --> 00:02:25,550
so what's happening here is they're not

00:02:23,000 --> 00:02:28,070
using online algorithms so the counts

00:02:25,550 --> 00:02:32,000
are delayed by different amounts for

00:02:28,070 --> 00:02:35,470
different things and I can ask you do

00:02:32,000 --> 00:02:39,550
you believe them no because it's a silly

00:02:35,470 --> 00:02:42,830
will you ever believe what they say

00:02:39,550 --> 00:02:45,350
maybe in a year or so after they do

00:02:42,830 --> 00:02:47,720
perfect jobs for that full year you

00:02:45,350 --> 00:02:51,920
might believe them again when will their

00:02:47,720 --> 00:02:53,390
investors believe them never so this is

00:02:51,920 --> 00:02:56,210
a great example where they should have

00:02:53,390 --> 00:02:57,860
been using good online algorithms or at

00:02:56,210 --> 00:03:01,190
least delaying everything the same

00:02:57,860 --> 00:03:03,470
amount and getting decent out answers so

00:03:01,190 --> 00:03:06,980
that's one answer online gives you

00:03:03,470 --> 00:03:09,380
answers now hopefully good answers so

00:03:06,980 --> 00:03:13,910
the next question is why quantiles why

00:03:09,380 --> 00:03:16,490
percentiles well suppose we have a whole

00:03:13,910 --> 00:03:18,740
bunch of users and they talk to a whole

00:03:16,490 --> 00:03:22,310
bunch of websites and we would like to

00:03:18,740 --> 00:03:24,950
characterize how well the universe of

00:03:22,310 --> 00:03:27,830
websites is doing we do that by a high

00:03:24,950 --> 00:03:30,970
percentile response time the the average

00:03:27,830 --> 00:03:34,130
response time is very very uninformative

00:03:30,970 --> 00:03:36,500
because it's dominated by the average by

00:03:34,130 --> 00:03:38,960
most things what we want to find out

00:03:36,500 --> 00:03:43,430
about is what's bad things have happened

00:03:38,960 --> 00:03:46,280
with very low frequency say the 99.99%

00:03:43,430 --> 00:03:48,950
IEL latency but we have a hundred

00:03:46,280 --> 00:03:51,440
million people and say each one of those

00:03:48,950 --> 00:03:54,110
goes to a thousand websites per day so

00:03:51,440 --> 00:03:57,260
we've got billions of measurements per

00:03:54,110 --> 00:04:00,290
day we want to have online results for

00:03:57,260 --> 00:04:02,540
any kind of subsets like how did people

00:04:00,290 --> 00:04:04,400
in Kansas get results how about the

00:04:02,540 --> 00:04:07,130
people who complained yesterday but not

00:04:04,400 --> 00:04:10,010
the day before how can we compare what

00:04:07,130 --> 00:04:12,530
they saw what were the worst cases that

00:04:10,010 --> 00:04:15,140
they saw keeping that sort of thing in

00:04:12,530 --> 00:04:17,450
log files is a good idea but it's not

00:04:15,140 --> 00:04:20,359
going to give us instantaneous responses

00:04:17,450 --> 00:04:23,870
to this sort of problem suppose we have

00:04:20,359 --> 00:04:26,180
a cluster a single cluster a thousand

00:04:23,870 --> 00:04:28,340
machines every machine talks to every

00:04:26,180 --> 00:04:31,040
other machine with with remote procedure

00:04:28,340 --> 00:04:32,510
calls every machine has its own storage

00:04:31,040 --> 00:04:35,720
devices

00:04:32,510 --> 00:04:38,480
and we want to know all of the response

00:04:35,720 --> 00:04:41,480
time characteristics the entire spectrum

00:04:38,480 --> 00:04:44,510
of how fast things happens what

00:04:41,480 --> 00:04:47,630
percentage of the time well we also need

00:04:44,510 --> 00:04:50,390
of course to minimize the overhead be

00:04:47,630 --> 00:04:52,580
able to tandel every sample in no more

00:04:50,390 --> 00:04:55,580
than a few nanoseconds maybe a few

00:04:52,580 --> 00:04:58,010
hundred nanoseconds and we want to only

00:04:55,580 --> 00:05:00,410
use a few megabytes of memory on each

00:04:58,010 --> 00:05:02,030
machine to do this we're not going to do

00:05:00,410 --> 00:05:04,730
this with a log file we couldn't write

00:05:02,030 --> 00:05:06,530
to the log file in 100 nanoseconds we

00:05:04,730 --> 00:05:09,380
couldn't even probably format the

00:05:06,530 --> 00:05:13,010
message for a log file in that much time

00:05:09,380 --> 00:05:15,890
so how are we going to get these answers

00:05:13,010 --> 00:05:20,810
about what's working what's not well the

00:05:15,890 --> 00:05:23,480
thing we need is high-end qui tiles the

00:05:20,810 --> 00:05:26,870
the high end or the low end near zero

00:05:23,480 --> 00:05:29,270
near one sort of quantiles those are the

00:05:26,870 --> 00:05:31,220
things that we need to understand and be

00:05:29,270 --> 00:05:34,940
able to measure in an online and

00:05:31,220 --> 00:05:37,760
efficient way now of course the question

00:05:34,940 --> 00:05:39,560
in any sort of thing like this when we

00:05:37,760 --> 00:05:42,410
start going to these online algorithms

00:05:39,560 --> 00:05:46,100
of certain quantities we cannot do them

00:05:42,410 --> 00:05:48,350
exactly quantiles the top hitters the

00:05:46,100 --> 00:05:50,450
the number of uniques all of these

00:05:48,350 --> 00:05:53,090
things can only be approximated in

00:05:50,450 --> 00:05:55,820
online algorithms and so the question is

00:05:53,090 --> 00:05:58,190
how accurate do we want to be here's

00:05:55,820 --> 00:06:02,660
some examples if we want to compete the

00:05:58,190 --> 00:06:05,960
median which is the 50th percentile plus

00:06:02,660 --> 00:06:07,910
or minus half a percent so the answer we

00:06:05,960 --> 00:06:10,250
get is somewhere between forty nine

00:06:07,910 --> 00:06:12,680
point five percent aisle and the fifty

00:06:10,250 --> 00:06:18,230
point fifth percentile that's probably

00:06:12,680 --> 00:06:21,070
okay but the 99.99% tile plus or minus a

00:06:18,230 --> 00:06:24,500
half percent makes no sense whatsoever

00:06:21,070 --> 00:06:27,110
because you know we were talking about

00:06:24,500 --> 00:06:31,580
something that's point O one away from

00:06:27,110 --> 00:06:33,290
the end and yet we talk about errors

00:06:31,580 --> 00:06:35,120
that are much larger than that so that

00:06:33,290 --> 00:06:39,320
makes no sense so it makes a lot of

00:06:35,120 --> 00:06:41,450
sense to have a 99.99% aisle to very

00:06:39,320 --> 00:06:43,570
high accuracy but then if we apply that

00:06:41,450 --> 00:06:46,430
same accuracy to the median

00:06:43,570 --> 00:06:50,300
we're probably over killing it

00:06:46,430 --> 00:06:53,330
so we need variable accuracy loose

00:06:50,300 --> 00:06:55,509
accuracy in the middle tight accuracy at

00:06:53,330 --> 00:06:58,820
the ends in fact what we would like is

00:06:55,509 --> 00:07:01,460
constant relative accuracy relative to

00:06:58,820 --> 00:07:04,430
how far we are to either of the ends

00:07:01,460 --> 00:07:07,280
very accurate at the ends less accurate

00:07:04,430 --> 00:07:09,590
in the middle the tea digest does

00:07:07,280 --> 00:07:13,729
exactly that and that is the one

00:07:09,590 --> 00:07:15,650
characteristic that changes when we go

00:07:13,729 --> 00:07:16,310
from other algorithms to approximate

00:07:15,650 --> 00:07:19,669
quantiles

00:07:16,310 --> 00:07:22,610
to the tea digest it has exactly this

00:07:19,669 --> 00:07:25,910
property of variable accuracy and

00:07:22,610 --> 00:07:28,580
constant relative accuracy now the way

00:07:25,910 --> 00:07:31,460
it does this is it keeps something like

00:07:28,580 --> 00:07:33,770
clusters in fact the original versions

00:07:31,460 --> 00:07:37,460
of the algorithm were almost exactly a

00:07:33,770 --> 00:07:39,860
k-means algorithm in one dimension with

00:07:37,460 --> 00:07:43,610
the big difference that the size of the

00:07:39,860 --> 00:07:47,509
cluster was allowed to be large in the

00:07:43,610 --> 00:07:51,139
middle where Q equals about 0.5 and had

00:07:47,509 --> 00:07:55,760
to be small at the 0 end and at the one

00:07:51,139 --> 00:08:00,470
end now choosing exactly how that size

00:07:55,760 --> 00:08:04,430
is is done we could choose it to say be

00:08:00,470 --> 00:08:07,280
equal to Q times 1 minus Q that means

00:08:04,430 --> 00:08:09,530
that near the ends the size of the

00:08:07,280 --> 00:08:12,020
cluster gets smaller in direct

00:08:09,530 --> 00:08:14,030
proportion to the distance to that end

00:08:12,020 --> 00:08:16,909
of the scale that we're talking about

00:08:14,030 --> 00:08:19,880
and that means our accuracy has exactly

00:08:16,909 --> 00:08:23,289
that same relative accuracy property

00:08:19,880 --> 00:08:25,639
that we want that that one idea of

00:08:23,289 --> 00:08:28,639
restricting the size of the clusters

00:08:25,639 --> 00:08:31,729
gives us all of the accuracy properties

00:08:28,639 --> 00:08:34,339
that we want now originally we'd use

00:08:31,729 --> 00:08:37,089
this form but it was pointed out to me

00:08:34,339 --> 00:08:40,400
by a guy he's in the credits at the end

00:08:37,089 --> 00:08:42,349
that we could do better by having

00:08:40,400 --> 00:08:45,500
something that grows a bit faster

00:08:42,349 --> 00:08:48,740
something that has the square root of Q

00:08:45,500 --> 00:08:51,709
times Q 1 minus Q and that's because we

00:08:48,740 --> 00:08:55,730
can do linear interpolation here is the

00:08:51,709 --> 00:08:57,800
cumulative distribution of just a normal

00:08:55,730 --> 00:09:00,019
distribution doesn't really matter but

00:08:57,800 --> 00:09:01,759
you can see that if we have

00:09:00,019 --> 00:09:06,439
these lines are close together at the

00:09:01,759 --> 00:09:09,319
ends and further apart in the middle we

00:09:06,439 --> 00:09:12,499
can do linear interpolation of this

00:09:09,319 --> 00:09:15,679
cumulative distribution and we get

00:09:12,499 --> 00:09:18,709
errors that are quadratic in the size of

00:09:15,679 --> 00:09:21,949
the cluster that we have and since the

00:09:18,709 --> 00:09:24,319
errors only are the curvature not the

00:09:21,949 --> 00:09:27,079
linear part because they're quadratic in

00:09:24,319 --> 00:09:31,160
that size we can therefore decrease the

00:09:27,079 --> 00:09:32,959
the the power on the limit that gives us

00:09:31,160 --> 00:09:37,189
a very important property that the size

00:09:32,959 --> 00:09:39,470
of the tea digest is bounded for the

00:09:37,189 --> 00:09:41,839
same accuracy no matter how much data

00:09:39,470 --> 00:09:46,279
you give it it has a finite amount of

00:09:41,839 --> 00:09:47,779
size and we do that by using I'm going

00:09:46,279 --> 00:09:50,209
to go quickly through these parts and

00:09:47,779 --> 00:09:52,429
then give some more examples these are

00:09:50,209 --> 00:09:54,529
going to be for reference more there's

00:09:52,429 --> 00:09:58,100
the the fundamental concept is that we

00:09:54,529 --> 00:10:02,989
have a mapping from the Q space the

00:09:58,100 --> 00:10:05,540
quantile to the centroid index this

00:10:02,989 --> 00:10:08,540
mapping is nonlinear it's steep at the

00:10:05,540 --> 00:10:11,059
ends so that we have small clusters at

00:10:08,540 --> 00:10:14,929
the ends flat in the middle relatively

00:10:11,059 --> 00:10:20,589
flat and we allow every cluster to have

00:10:14,929 --> 00:10:22,790
a bound in case Kayle of at most one

00:10:20,589 --> 00:10:25,490
that means that we can build this

00:10:22,790 --> 00:10:27,799
algorithm with a very simple algorithm

00:10:25,490 --> 00:10:31,040
what we do is we just order the points

00:10:27,799 --> 00:10:36,369
and we collect points together as long

00:10:31,040 --> 00:10:41,660
as that scaled version of the quantile

00:10:36,369 --> 00:10:46,220
come on there we go as long as this

00:10:41,660 --> 00:10:49,480
scaled size of the Quan tire of the

00:10:46,220 --> 00:10:53,660
centroid of the cluster is less than 1

00:10:49,480 --> 00:10:55,639
then we continue to merge things in once

00:10:53,660 --> 00:10:58,069
it's too big then we commit that one

00:10:55,639 --> 00:11:01,040
centroid and we start on the next one

00:10:58,069 --> 00:11:03,319
and we keep accumulating points until

00:11:01,040 --> 00:11:05,269
its size is too big we set it aside so

00:11:03,319 --> 00:11:07,819
that's the algorithm we walk through

00:11:05,269 --> 00:11:12,290
sorted data collecting them together in

00:11:07,819 --> 00:11:13,970
that way in that scaled size so that

00:11:12,290 --> 00:11:16,639
things that the ends are small

00:11:13,970 --> 00:11:19,399
things in the middle are big that's sort

00:11:16,639 --> 00:11:21,649
of scaling that's sort of merging gives

00:11:19,399 --> 00:11:24,319
us this scaling we want it's a very

00:11:21,649 --> 00:11:27,550
simple thing and what we can do is we

00:11:24,319 --> 00:11:30,560
can have a short buffer for new points

00:11:27,550 --> 00:11:33,589
when that fills up we sort it and merge

00:11:30,560 --> 00:11:36,019
it with the old centroids we then fill

00:11:33,589 --> 00:11:38,720
up the buffer sort it and merge it with

00:11:36,019 --> 00:11:41,149
the old centroids because of the size

00:11:38,720 --> 00:11:43,370
bound these can all be statically

00:11:41,149 --> 00:11:45,410
allocated at the beginning of the

00:11:43,370 --> 00:11:48,589
algorithm and there's no allocation in

00:11:45,410 --> 00:11:50,629
the process we can improve it a little

00:11:48,589 --> 00:11:53,899
bit by using an in-place merge that gets

00:11:50,629 --> 00:11:56,689
rid of half of the space almost we can

00:11:53,899 --> 00:11:59,060
use an approximate for that curve the

00:11:56,689 --> 00:12:00,050
compression curve that will improve the

00:11:59,060 --> 00:12:02,720
speed because we don't have a

00:12:00,050 --> 00:12:05,779
trigonometric function and we can get

00:12:02,720 --> 00:12:08,300
the cost per point down around or

00:12:05,779 --> 00:12:10,730
possibly below a hundred nanoseconds per

00:12:08,300 --> 00:12:13,160
point because we have no allocations all

00:12:10,730 --> 00:12:17,500
of the code is straight line very very

00:12:13,160 --> 00:12:20,689
simple sort of code this is really fast

00:12:17,500 --> 00:12:22,459
this is really truly online we can get

00:12:20,689 --> 00:12:25,610
any of the quantiles out that we'd like

00:12:22,459 --> 00:12:28,910
we can do alerting on large values we

00:12:25,610 --> 00:12:33,860
can do all of the things that we marked

00:12:28,910 --> 00:12:37,309
as necessary early on and it's easy to

00:12:33,860 --> 00:12:39,740
integrate you can use tea digest as an

00:12:37,309 --> 00:12:42,230
aggregator directly in elastic search a

00:12:39,740 --> 00:12:46,490
guy who was on this stage earlier adrian

00:12:42,230 --> 00:12:50,089
ground integrated this into elastic

00:12:46,490 --> 00:12:52,939
search you can use it from stream live

00:12:50,089 --> 00:12:55,639
which is a collection of algorithms for

00:12:52,939 --> 00:12:58,579
doing approximate counting and things

00:12:55,639 --> 00:13:01,160
like that you can use it as a UDF for

00:12:58,579 --> 00:13:04,100
drill that's not quite released yet but

00:13:01,160 --> 00:13:07,610
it will be soon it's already in Apache

00:13:04,100 --> 00:13:10,220
mahute and it's in maven central so the

00:13:07,610 --> 00:13:12,470
API is also trivial you ask it for a tea

00:13:10,220 --> 00:13:14,990
digest it's a data structure you feed it

00:13:12,470 --> 00:13:20,360
points and you can ask for quantiles at

00:13:14,990 --> 00:13:22,759
any time so the upshot is that we can

00:13:20,360 --> 00:13:27,350
now build streaming applications to do

00:13:22,759 --> 00:13:28,010
percentiles accurately and conveniently

00:13:27,350 --> 00:13:31,640
and quickly

00:13:28,010 --> 00:13:35,960
we can use it to do anomaly detection

00:13:31,640 --> 00:13:39,500
what is higher than the 99.9% I'll we

00:13:35,960 --> 00:13:42,560
had a speaker earlier who was building a

00:13:39,500 --> 00:13:44,900
maximum detector for instance she was

00:13:42,560 --> 00:13:47,210
asked after the talk how do you decide

00:13:44,900 --> 00:13:49,850
what maximum really is what if some

00:13:47,210 --> 00:13:52,430
there's one sample that's very large we

00:13:49,850 --> 00:13:54,080
possibly do to noise there's no good

00:13:52,430 --> 00:13:56,540
answer other than ranked based

00:13:54,080 --> 00:13:58,790
statistics you could say a maximum is

00:13:56,540 --> 00:14:02,120
something that's larger then the 99th

00:13:58,790 --> 00:14:03,740
percentile of samples within a minute so

00:14:02,120 --> 00:14:06,950
you could get that sort of thing there

00:14:03,740 --> 00:14:08,540
and you can use this almost anywhere I'm

00:14:06,950 --> 00:14:11,380
going to move to the questions quickly

00:14:08,540 --> 00:14:15,890
but before I do here's a few credits

00:14:11,380 --> 00:14:19,280
unmoral gave the idea of that Kate to Q

00:14:15,890 --> 00:14:21,860
curve on during grand did the fastest

00:14:19,280 --> 00:14:27,050
current implementation in a tree sort of

00:14:21,860 --> 00:14:29,600
thing Hoss from the the the solar

00:14:27,050 --> 00:14:32,420
community he has provided some API

00:14:29,600 --> 00:14:34,940
improvements cam Davidson pretty nice

00:14:32,420 --> 00:14:36,860
some very very interesting public

00:14:34,940 --> 00:14:40,490
discussions of this and of course

00:14:36,860 --> 00:14:43,160
there's one name missing here that's

00:14:40,490 --> 00:14:45,320
your name you can contribute to this to

00:14:43,160 --> 00:14:47,510
if you have any needs have any

00:14:45,320 --> 00:14:50,200
suggestions or anything you'd like to

00:14:47,510 --> 00:14:50,200

YouTube URL: https://www.youtube.com/watch?v=CR4-aVvjE6A


