Title: Berlin Buzzwords 2015: Ryan Ernst - Compression in Lucene #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Modern search engines can store billions of records containing both text and structured data, but as the amount of data being searched grows, so do the requirements for disk space and memory. Various compression techniques are used to decrease the necessary storage, but still allow fast access for search.  

While Lucene has always used compression for its inverted index, compression techniques have improved and been generalized to other parts of the index, like the built-in document and column-oriented data stores. In this presentation, Ryan Ernst will give an introduction to how compression is used in Lucene, including recent improvements for Lucene 5.0.

Read more:
https://2015.berlinbuzzwords.de/session/compression-lucene

About Ryan Ernst:
https://2015.berlinbuzzwords.de/users/ryan-ernst

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
I

00:00:05,560 --> 00:00:11,049
great hi thanks my name is Ryan I work

00:00:08,950 --> 00:00:13,869
for last search as he said I'm also an

00:00:11,049 --> 00:00:16,119
apache Lucene committer and so I'm going

00:00:13,869 --> 00:00:17,260
to be talking about a lot of the

00:00:16,119 --> 00:00:19,900
different ways that compression

00:00:17,260 --> 00:00:23,439
techniques are used within leucine to

00:00:19,900 --> 00:00:25,359
make indexes smaller so first of all to

00:00:23,439 --> 00:00:28,089
talk about compression we need to kind

00:00:25,359 --> 00:00:29,859
of understand what's in an index so we

00:00:28,089 --> 00:00:31,300
start out with documents right so you're

00:00:29,859 --> 00:00:33,970
going to take a bunch of documents here

00:00:31,300 --> 00:00:37,030
we have a document that is Berlin

00:00:33,970 --> 00:00:38,950
buzzwords and this is the year 2015 and

00:00:37,030 --> 00:00:41,230
we have like a unique identifier for be

00:00:38,950 --> 00:00:43,660
buzzed 2015 and now we want to index

00:00:41,230 --> 00:00:44,830
that so first thing we're going to do is

00:00:43,660 --> 00:00:46,930
we're going to generate this posting

00:00:44,830 --> 00:00:49,930
lists probably familiar with these

00:00:46,930 --> 00:00:52,870
essentially it's inverting the terms so

00:00:49,930 --> 00:00:54,130
we're going to for each term that exists

00:00:52,870 --> 00:00:56,860
in all the documents we're going to keep

00:00:54,130 --> 00:00:59,530
a list of which documents contain that

00:00:56,860 --> 00:01:01,240
term right and then we're also going to

00:00:59,530 --> 00:01:03,130
have something doc values this is

00:01:01,240 --> 00:01:07,450
something you've a talked about earlier

00:01:03,130 --> 00:01:11,020
and this is just a the inverted look up

00:01:07,450 --> 00:01:12,939
there so it's from document I want to

00:01:11,020 --> 00:01:14,999
look up for field what value did this

00:01:12,939 --> 00:01:17,619
originally have when I indexed it and

00:01:14,999 --> 00:01:21,789
then we also have this stored field

00:01:17,619 --> 00:01:24,579
sword fields which is just like a string

00:01:21,789 --> 00:01:26,530
that I want to keep with my document

00:01:24,579 --> 00:01:28,509
like the original value of the field for

00:01:26,530 --> 00:01:30,399
instance so I can get it back out later

00:01:28,509 --> 00:01:32,679
this isn't everything that's in like

00:01:30,399 --> 00:01:38,490
Lucien index but it's kind of general

00:01:32,679 --> 00:01:40,840
gist so understand compression we first

00:01:38,490 --> 00:01:45,069
what we're going to do is go through an

00:01:40,840 --> 00:01:46,119
example query so when we're running a

00:01:45,069 --> 00:01:49,329
query we're going to have to decompress

00:01:46,119 --> 00:01:52,240
some things and so let's go through this

00:01:49,329 --> 00:01:53,350
query part right so we pass a query

00:01:52,240 --> 00:01:56,829
through we're going to get documents

00:01:53,350 --> 00:01:58,270
back what does that look like so first

00:01:56,829 --> 00:02:00,189
of all we need to find the terms right

00:01:58,270 --> 00:02:03,399
so our query is going to be brilliant

00:02:00,189 --> 00:02:05,049
buzzwords and so we need to find what

00:02:03,399 --> 00:02:08,039
documents contain the word Berlin and

00:02:05,049 --> 00:02:11,260
what documents contain learn buzzwords

00:02:08,039 --> 00:02:15,849
so first we gotta remember back to an

00:02:11,260 --> 00:02:19,060
FST I'm not going to explain how an FST

00:02:15,849 --> 00:02:22,300
works here but in general it is

00:02:19,060 --> 00:02:25,000
a structured that is going to allow very

00:02:22,300 --> 00:02:27,790
quickly iterating through and getting a

00:02:25,000 --> 00:02:29,800
match and the T part transforms it's

00:02:27,790 --> 00:02:32,470
going to spit something back out and so

00:02:29,800 --> 00:02:35,849
that's something we're going to use to

00:02:32,470 --> 00:02:39,010
find some information in the index so

00:02:35,849 --> 00:02:42,160
the way that looks is we have this thing

00:02:39,010 --> 00:02:45,849
called a prefix FST it's called a block

00:02:42,160 --> 00:02:48,370
terms reader in lucene the idea is that

00:02:45,849 --> 00:02:51,850
we have an FST with prefixes of our

00:02:48,370 --> 00:02:55,660
terms and the output when we get to the

00:02:51,850 --> 00:02:57,819
end of when we find a prefix that

00:02:55,660 --> 00:03:02,080
matched in there is going to be the

00:02:57,819 --> 00:03:04,480
block within this terms blocks where

00:03:02,080 --> 00:03:09,130
that term actually exists or where it

00:03:04,480 --> 00:03:10,989
should exist and so we can traverse the

00:03:09,130 --> 00:03:13,830
FST and the output is going to be this

00:03:10,989 --> 00:03:16,360
offset into the file and now we get to

00:03:13,830 --> 00:03:18,580
to these suffixes right that are going

00:03:16,360 --> 00:03:23,620
to be tacked on to the prefix that we

00:03:18,580 --> 00:03:27,299
matched so now if we zoom in on this one

00:03:23,620 --> 00:03:29,799
block so we looked for 2015 for instance

00:03:27,299 --> 00:03:33,220
we went through we got to the block and

00:03:29,799 --> 00:03:37,180
now we have five there right so we

00:03:33,220 --> 00:03:41,079
matched a 201 we have five and we see

00:03:37,180 --> 00:03:44,140
this postings so this is yet again

00:03:41,079 --> 00:03:46,000
another offset that we're going to store

00:03:44,140 --> 00:03:47,650
in there and this is the offset into the

00:03:46,000 --> 00:03:52,480
postings file so for where we're going

00:03:47,650 --> 00:03:54,970
to find that list of document IDs so how

00:03:52,480 --> 00:03:59,739
are we going to encode this posting this

00:03:54,970 --> 00:04:02,109
number here so for many years since the

00:03:59,739 --> 00:04:05,650
70s or 80s there's been something called

00:04:02,109 --> 00:04:07,900
vient the bite warrant it has a lot of

00:04:05,650 --> 00:04:10,690
different names if you look in I our

00:04:07,900 --> 00:04:14,130
textbooks but the general idea is that

00:04:10,690 --> 00:04:17,049
we are going to take an integer and

00:04:14,130 --> 00:04:19,870
we're going to find the significant bits

00:04:17,049 --> 00:04:21,639
right so extra zeros that are up at the

00:04:19,870 --> 00:04:23,020
top we don't care about those so we want

00:04:21,639 --> 00:04:25,389
to find out the minimum number of bits

00:04:23,020 --> 00:04:26,620
that an integer is going to take and

00:04:25,389 --> 00:04:29,440
then we're going to split it up into

00:04:26,620 --> 00:04:31,240
groups of seven bits and the way we're

00:04:29,440 --> 00:04:33,280
going to encode this is

00:04:31,240 --> 00:04:35,289
taking the first seven bits and putting

00:04:33,280 --> 00:04:37,569
it into a bite and then adding a one

00:04:35,289 --> 00:04:40,300
there and that means there are more

00:04:37,569 --> 00:04:42,729
bites to read and so then we can keep

00:04:40,300 --> 00:04:44,830
doing this until we've run out of bits

00:04:42,729 --> 00:04:48,130
that we need to encode and then when

00:04:44,830 --> 00:04:51,699
we're decoding we can read through until

00:04:48,130 --> 00:04:54,220
there is no longer a one in that most

00:04:51,699 --> 00:04:57,160
significant bit and then we can use bit

00:04:54,220 --> 00:04:59,319
masks and shifting to rejoin all the

00:04:57,160 --> 00:05:00,669
bits together and we have our original

00:04:59,319 --> 00:05:04,419
integer so let's look at some examples

00:05:00,669 --> 00:05:06,699
of that so we start out with a very

00:05:04,419 --> 00:05:09,460
simple number right one notice that

00:05:06,699 --> 00:05:11,229
there's a lot of zeros these gray zeros

00:05:09,460 --> 00:05:14,199
or things that don't matter right so the

00:05:11,229 --> 00:05:16,960
only meaningful bits when we have this

00:05:14,199 --> 00:05:20,319
one bite is the zero at the top because

00:05:16,960 --> 00:05:22,900
that means this is the last bite so it's

00:05:20,319 --> 00:05:27,009
just a one-bite integer and then we have

00:05:22,900 --> 00:05:30,789
the lower one bit there then we go to a

00:05:27,009 --> 00:05:32,650
value 253 so normally this value would

00:05:30,789 --> 00:05:34,930
take up 8 bits and we could store it in

00:05:32,650 --> 00:05:36,550
a single bite if we wanted right but in

00:05:34,930 --> 00:05:40,659
this case because we only have seven

00:05:36,550 --> 00:05:43,419
bits per byte we have to spill it over

00:05:40,659 --> 00:05:46,180
to another bite so again we have a bunch

00:05:43,419 --> 00:05:48,880
of zeros but if you a bunch of wasted

00:05:46,180 --> 00:05:54,130
zeros but if you look at the total

00:05:48,880 --> 00:05:56,740
number of of relevant bits across our 16

00:05:54,130 --> 00:06:00,719
that we have there it's it's still a

00:05:56,740 --> 00:06:04,000
pretty good compression ratio and then

00:06:00,719 --> 00:06:06,580
then we get to yet an art another larger

00:06:04,000 --> 00:06:10,090
number notice here that in the third

00:06:06,580 --> 00:06:13,000
bite it looks exactly the same as that

00:06:10,090 --> 00:06:14,949
first bite that we had before so context

00:06:13,000 --> 00:06:16,270
really matters right we need to start at

00:06:14,949 --> 00:06:18,669
the beginning of where the integer is

00:06:16,270 --> 00:06:20,800
because there's no way for us to know

00:06:18,669 --> 00:06:23,020
like this isn't the beginning of an

00:06:20,800 --> 00:06:26,050
integer right so things like utf-8 they

00:06:23,020 --> 00:06:28,509
have special special values that they

00:06:26,050 --> 00:06:32,530
have in these significant bits that tell

00:06:28,509 --> 00:06:34,719
them to tell a decoder where it is so

00:06:32,530 --> 00:06:37,270
you can actually jump back to the

00:06:34,719 --> 00:06:40,300
beginning of a utf-8 encoded sink once

00:06:37,270 --> 00:06:44,229
but with v bite you camped and then

00:06:40,300 --> 00:06:44,680
finally this number all one's a central

00:06:44,229 --> 00:06:47,710
right we

00:06:44,680 --> 00:06:49,030
we have 28 bits of ones this is the

00:06:47,710 --> 00:06:51,039
biggest number that we can fit in 4

00:06:49,030 --> 00:06:52,630
bytes so if we had a larger value in

00:06:51,039 --> 00:06:57,060
this stash going to take more than if we

00:06:52,630 --> 00:06:59,530
hadn't covered in int that's okay

00:06:57,060 --> 00:07:03,220
because we don't have very very large

00:06:59,530 --> 00:07:05,860
numbers like that very often so now that

00:07:03,220 --> 00:07:08,259
we've found this offset into the

00:07:05,860 --> 00:07:09,729
postings list we're going to go try to

00:07:08,259 --> 00:07:15,460
decode our postings so how are we going

00:07:09,729 --> 00:07:18,580
to do that so if we have our original

00:07:15,460 --> 00:07:23,590
document IDs that we had encoded in the

00:07:18,580 --> 00:07:24,669
postings list usually these are going to

00:07:23,590 --> 00:07:26,380
first of all there going to be

00:07:24,669 --> 00:07:27,580
sequential Rex we're going to do them in

00:07:26,380 --> 00:07:30,400
order so that we can do like fast

00:07:27,580 --> 00:07:31,960
intersections so we can keep track as

00:07:30,400 --> 00:07:33,160
we're intersecting and know that we

00:07:31,960 --> 00:07:36,430
don't have to go back and look for a

00:07:33,160 --> 00:07:37,810
different number but most of the time or

00:07:36,430 --> 00:07:40,449
all the time we're going to want to

00:07:37,810 --> 00:07:42,820
store deltas because usually if you have

00:07:40,449 --> 00:07:44,500
a term that is very common then the

00:07:42,820 --> 00:07:46,240
difference between the two numbers is

00:07:44,500 --> 00:07:49,120
going to be much smaller than the actual

00:07:46,240 --> 00:07:50,800
number right so if I have document ten

00:07:49,120 --> 00:07:53,139
thousand five and document ten thousand

00:07:50,800 --> 00:07:54,460
and six I don't really need to store you

00:07:53,139 --> 00:07:56,020
know three bytes for each of those I

00:07:54,460 --> 00:07:59,349
just need to store one bite the

00:07:56,020 --> 00:08:03,820
difference between them and so we can

00:07:59,349 --> 00:08:06,010
use vient the bite right for that as we

00:08:03,820 --> 00:08:08,169
saw earlier notice again there's a bunch

00:08:06,010 --> 00:08:09,789
of grey zeros that means they're they're

00:08:08,169 --> 00:08:13,000
wasted space right even though we were

00:08:09,789 --> 00:08:15,520
much better than four by tints we're

00:08:13,000 --> 00:08:19,570
still not as good as we could be and so

00:08:15,520 --> 00:08:21,909
that leads us to how we're going to take

00:08:19,570 --> 00:08:25,090
care of very large postings list so this

00:08:21,909 --> 00:08:26,650
vient that i described this was that

00:08:25,090 --> 00:08:29,860
like i said has been around since 70s or

00:08:26,650 --> 00:08:31,659
80s and it's still used in the scene but

00:08:29,860 --> 00:08:37,079
it's used much less and so when we have

00:08:31,659 --> 00:08:37,079
a very large postings list we actually

00:08:37,979 --> 00:08:45,399
we want to use less than one byte per

00:08:42,300 --> 00:08:47,649
value to encode it because for instance

00:08:45,399 --> 00:08:49,750
if we have a lot of just one two one two

00:08:47,649 --> 00:08:51,100
one two right these are very small

00:08:49,750 --> 00:08:52,959
numbers that we can code in a couple

00:08:51,100 --> 00:08:57,040
bits we don't need a whole bite to

00:08:52,959 --> 00:08:58,630
encode them so in order to do this pact

00:08:57,040 --> 00:09:01,780
ince we're going to go

00:08:58,630 --> 00:09:04,270
all the values within a block that we

00:09:01,780 --> 00:09:06,730
want to encode and we're going to find

00:09:04,270 --> 00:09:08,860
what's the minimum number of bits that

00:09:06,730 --> 00:09:11,530
is required to encode all of the values

00:09:08,860 --> 00:09:14,500
that we saw on this block once we do

00:09:11,530 --> 00:09:18,130
that then we can put each of the

00:09:14,500 --> 00:09:20,470
integers into a bit field so and we know

00:09:18,130 --> 00:09:21,790
that it'll fit inside those bits so we

00:09:20,470 --> 00:09:25,060
are going to have any leftover bits that

00:09:21,790 --> 00:09:28,180
don't fit and then when we're trying to

00:09:25,060 --> 00:09:30,540
decode because we know ahead of time how

00:09:28,180 --> 00:09:33,460
many bits are the size of that bit field

00:09:30,540 --> 00:09:38,190
we can use simple offset math right so

00:09:33,460 --> 00:09:42,910
you see here if we would just have to

00:09:38,190 --> 00:09:46,600
multiply and divide right and then and

00:09:42,910 --> 00:09:50,560
again shifts and masks similar to V bite

00:09:46,600 --> 00:09:52,570
it's a little bit a little different

00:09:50,560 --> 00:09:55,360
shift right so V bite we're shifting by

00:09:52,570 --> 00:10:03,850
seven bits here we're shifting by some

00:09:55,360 --> 00:10:06,190
number and so an example with the with

00:10:03,850 --> 00:10:08,380
patents postings so our original numbers

00:10:06,190 --> 00:10:11,380
we have 15 and to remember these are

00:10:08,380 --> 00:10:16,420
deltas these were Delta's that we had in

00:10:11,380 --> 00:10:19,210
our last example for vient so the first

00:10:16,420 --> 00:10:21,340
thing we're going to store this number

00:10:19,210 --> 00:10:23,650
that's all in red this is like a

00:10:21,340 --> 00:10:27,010
description this all this says is this

00:10:23,650 --> 00:10:29,080
is the number of bits per value that

00:10:27,010 --> 00:10:31,900
we're going to store and so we have a 4

00:10:29,080 --> 00:10:33,610
there right in binary and then each

00:10:31,900 --> 00:10:36,160
integer is only going to take up four

00:10:33,610 --> 00:10:37,510
bytes so notice that we are taking the

00:10:36,160 --> 00:10:39,910
same number bytes in this little toy

00:10:37,510 --> 00:10:41,650
example that we did with vient but if we

00:10:39,910 --> 00:10:43,660
kept going if we had a lot more values

00:10:41,650 --> 00:10:46,990
obviously still that fit within those

00:10:43,660 --> 00:10:48,610
four bits then we'd be able to store

00:10:46,990 --> 00:10:52,030
them more compressed than if we had

00:10:48,610 --> 00:10:55,330
vient at a slightly potentially extra

00:10:52,030 --> 00:11:00,340
klaustreichs we have to shift and mask a

00:10:55,330 --> 00:11:03,940
little bit more so that's what we use to

00:11:00,340 --> 00:11:05,650
decode our postings lists and so we're

00:11:03,940 --> 00:11:08,260
going through our query we found the

00:11:05,650 --> 00:11:10,210
terms we found the postings lists that

00:11:08,260 --> 00:11:11,810
were associated with those terms we did

00:11:10,210 --> 00:11:13,160
let's say an intersection

00:11:11,810 --> 00:11:17,210
so we wanted to find the documents it

00:11:13,160 --> 00:11:18,770
had Berlin and buzzwords and now we want

00:11:17,210 --> 00:11:20,960
to figure out what are the most relevant

00:11:18,770 --> 00:11:24,620
documents what are what do we want to

00:11:20,960 --> 00:11:25,910
actually return to our user and so for

00:11:24,620 --> 00:11:29,660
that we're going to score the documents

00:11:25,910 --> 00:11:31,070
so loosing has this a complicated

00:11:29,660 --> 00:11:33,830
scoring function you don't have to

00:11:31,070 --> 00:11:35,630
understand anything about it just know

00:11:33,830 --> 00:11:38,120
that we're going to talk about this part

00:11:35,630 --> 00:11:41,870
here so this is these are the norms

00:11:38,120 --> 00:11:43,940
these are an index time scoring factor

00:11:41,870 --> 00:11:46,490
that we're going to store in the index

00:11:43,940 --> 00:11:48,620
and we're going to load or have loaded

00:11:46,490 --> 00:11:54,230
that query time available for us so that

00:11:48,620 --> 00:11:58,940
we can tack that into the score so it's

00:11:54,230 --> 00:12:01,130
all the indexed time scoring factors the

00:11:58,940 --> 00:12:04,130
main portion of it is what's called a

00:12:01,130 --> 00:12:06,140
length normalization what that means is

00:12:04,130 --> 00:12:08,630
if I have a match for instance in a

00:12:06,140 --> 00:12:12,860
title field that is very short it's

00:12:08,630 --> 00:12:15,710
probably a more relevant than if I have

00:12:12,860 --> 00:12:19,130
a match in a very long field like a body

00:12:15,710 --> 00:12:20,690
field or something like that and so this

00:12:19,130 --> 00:12:22,370
normalization factor is basically a

00:12:20,690 --> 00:12:25,220
weight that's going to be applied into

00:12:22,370 --> 00:12:29,390
that score that's going to boost up

00:12:25,220 --> 00:12:33,890
documents that that matched in a shorter

00:12:29,390 --> 00:12:36,680
field this value is actually eight an

00:12:33,890 --> 00:12:38,990
eight bit floating point integer or

00:12:36,680 --> 00:12:43,610
floating point value and coded as an

00:12:38,990 --> 00:12:48,440
integer the reason for that is only big

00:12:43,610 --> 00:12:51,920
differences matter so if I have a

00:12:48,440 --> 00:12:53,720
document that matches that that matched

00:12:51,920 --> 00:12:55,790
in field that was a million in length

00:12:53,720 --> 00:12:57,500
and won the match in a field that was

00:12:55,790 --> 00:13:00,620
two million in length they're basically

00:12:57,500 --> 00:13:02,930
the same right what matters is if I have

00:13:00,620 --> 00:13:05,720
a field that matched with the head to

00:13:02,930 --> 00:13:07,760
our length of to write or three or four

00:13:05,720 --> 00:13:09,230
but all those three two three and four

00:13:07,760 --> 00:13:12,560
those are also essentially the same so

00:13:09,230 --> 00:13:14,839
we have small and large and so we're

00:13:12,560 --> 00:13:17,900
actually going to truncate this value

00:13:14,839 --> 00:13:20,089
essentially we're going to its lossy but

00:13:17,900 --> 00:13:22,700
it doesn't matter it it's only these big

00:13:20,089 --> 00:13:25,100
difference is the matter however the API

00:13:22,700 --> 00:13:25,610
does allow for Long's so the encoding

00:13:25,100 --> 00:13:29,390
has to

00:13:25,610 --> 00:13:31,370
who has to support that this is more for

00:13:29,390 --> 00:13:34,360
ir researchers and stuff like that but

00:13:31,370 --> 00:13:36,890
by default we use an 8-bit float and

00:13:34,360 --> 00:13:41,180
norms right now actually have seven

00:13:36,890 --> 00:13:45,350
different techniques and talk about most

00:13:41,180 --> 00:13:47,779
of them so the first one is simple this

00:13:45,350 --> 00:13:49,190
is just an array so remember we have an

00:13:47,779 --> 00:13:54,260
8-bit float so we know that all the

00:13:49,190 --> 00:13:55,940
values are 255 0 to 255 and so we

00:13:54,260 --> 00:14:02,630
distort an array of the values

00:13:55,940 --> 00:14:05,000
essentially next is going to be constant

00:14:02,630 --> 00:14:06,890
this is for cases where let's say I

00:14:05,000 --> 00:14:09,920
accidentally enabled norms for a field

00:14:06,890 --> 00:14:13,250
and I didn't mean to and all the values

00:14:09,920 --> 00:14:16,040
are there just length 1 right so every

00:14:13,250 --> 00:14:18,410
single of these documents is going to

00:14:16,040 --> 00:14:20,870
have the exact same norms value and so

00:14:18,410 --> 00:14:23,120
for this edge case we have a special

00:14:20,870 --> 00:14:25,339
encoding which just the first detects

00:14:23,120 --> 00:14:27,380
but all the values are the same and then

00:14:25,339 --> 00:14:34,579
just says hey this is the number values

00:14:27,380 --> 00:14:43,820
we had and here's that one value then we

00:14:34,579 --> 00:14:46,459
have C Delta so Delta is like the pack

00:14:43,820 --> 00:14:48,350
tints that we saw earlier with one

00:14:46,459 --> 00:14:53,120
additional thing we're going to store

00:14:48,350 --> 00:14:54,980
this this minimum value and then once we

00:14:53,120 --> 00:14:57,019
store that minimum value every time we

00:14:54,980 --> 00:15:01,040
decode one of these packed integers we

00:14:57,019 --> 00:15:04,339
just add it to that minimum value so in

00:15:01,040 --> 00:15:06,620
this case you can see that 10 and 5 or

00:15:04,339 --> 00:15:08,930
just like we expect but 20 normally

00:15:06,620 --> 00:15:11,449
would have taken five bits but because

00:15:08,930 --> 00:15:14,209
we offset it by that minimum value we

00:15:11,449 --> 00:15:18,050
were able to store it in just four bits

00:15:14,209 --> 00:15:21,949
and so if that minimum value is large

00:15:18,050 --> 00:15:28,699
then it can help us compare compression

00:15:21,949 --> 00:15:33,730
rate the next one is going to be table

00:15:28,699 --> 00:15:38,329
compression this is again going to use

00:15:33,730 --> 00:15:39,080
packed in stepped that the values that

00:15:38,329 --> 00:15:44,270
we're going to

00:15:39,080 --> 00:15:46,070
right out here so like 0 1 0 0 and 0 1

00:15:44,270 --> 00:15:49,430
right so these values are actually

00:15:46,070 --> 00:15:52,550
officer indexes into a table of the real

00:15:49,430 --> 00:15:55,250
values and the reason we do this is the

00:15:52,550 --> 00:15:56,480
real values were larger right 10 and 5

00:15:55,250 --> 00:15:59,120
we wouldn't have been able to fit those

00:15:56,480 --> 00:16:01,760
into the tube it's that way encoded here

00:15:59,120 --> 00:16:04,100
and so if we had lots of other values

00:16:01,760 --> 00:16:05,240
but they were all the same essential if

00:16:04,100 --> 00:16:07,670
we just had a bunch of tens and fives

00:16:05,240 --> 00:16:10,250
and tens and fives then we're able to

00:16:07,670 --> 00:16:12,350
encode this much more compactly by

00:16:10,250 --> 00:16:21,530
storing the values separately and we saw

00:16:12,350 --> 00:16:24,890
them sorted to indirect is a little bit

00:16:21,530 --> 00:16:27,740
more complicated so the idea here is

00:16:24,890 --> 00:16:30,020
that if we have a value that's very

00:16:27,740 --> 00:16:32,900
common but it's not the only value we

00:16:30,020 --> 00:16:35,450
see so here we have two tens and a five

00:16:32,900 --> 00:16:39,230
right imagine we had tons more tense in

00:16:35,450 --> 00:16:42,290
fact it's the trigger that we use is

00:16:39,230 --> 00:16:46,460
ninety-seven percent it was kind of

00:16:42,290 --> 00:16:48,020
randomly chosen but but tested so if we

00:16:46,460 --> 00:16:50,780
see ninety-seven percent of the values

00:16:48,020 --> 00:16:53,330
have one or ninety-seven percent of the

00:16:50,780 --> 00:16:56,720
documents have one value then we'll just

00:16:53,330 --> 00:16:59,900
encode that common value and then this

00:16:56,720 --> 00:17:02,480
indirect is going to have two arrays so

00:16:59,900 --> 00:17:06,260
we start out first of all the array of

00:17:02,480 --> 00:17:09,880
the doc IDs that had a value that wasn't

00:17:06,260 --> 00:17:12,020
the common value and so here our

00:17:09,880 --> 00:17:16,370
document one right this is zero based

00:17:12,020 --> 00:17:19,730
indexing so our second document had did

00:17:16,370 --> 00:17:23,810
not have ten so this is the uncommon

00:17:19,730 --> 00:17:26,449
value and so that was our first document

00:17:23,810 --> 00:17:29,090
that we encoded in our array of rarities

00:17:26,449 --> 00:17:31,970
and then we have a corresponding array

00:17:29,090 --> 00:17:34,700
that matches it element for element with

00:17:31,970 --> 00:17:37,610
the actual value that that document had

00:17:34,700 --> 00:17:40,070
and because this first array is sorted

00:17:37,610 --> 00:17:44,030
by document ID we're able to do binary

00:17:40,070 --> 00:17:46,640
search over it so we first do a binary

00:17:44,030 --> 00:17:48,770
search and see does the document that

00:17:46,640 --> 00:17:50,720
we're trying to find a value for does it

00:17:48,770 --> 00:17:51,299
exist if it doesn't exist we know it's

00:17:50,720 --> 00:17:55,320
the comment about

00:17:51,299 --> 00:17:57,539
you and if it does exist then we take

00:17:55,320 --> 00:18:00,929
that index where we found it in our

00:17:57,539 --> 00:18:04,070
array of rare doc IDs and we go over to

00:18:00,929 --> 00:18:08,279
the values array and we look it up there

00:18:04,070 --> 00:18:11,159
and the final one that we're going to

00:18:08,279 --> 00:18:13,230
talk about for norms is patched so

00:18:11,159 --> 00:18:15,929
they're actually two patched but they're

00:18:13,230 --> 00:18:19,379
basically the same the idea here is to

00:18:15,929 --> 00:18:23,999
combine a couple techniques that we've

00:18:19,379 --> 00:18:27,359
already seen so we again have common

00:18:23,999 --> 00:18:28,739
values but this this time it's multiple

00:18:27,359 --> 00:18:32,100
common value so if we have a small

00:18:28,739 --> 00:18:36,869
number of common values then we can

00:18:32,100 --> 00:18:39,090
again use a an array of sorted values or

00:18:36,869 --> 00:18:42,960
and use the index into there as an

00:18:39,090 --> 00:18:47,519
ordinal meaning an identifier for that

00:18:42,960 --> 00:18:50,759
value and then we reserve one special

00:18:47,519 --> 00:18:52,799
value in our bit field here and we say

00:18:50,759 --> 00:18:55,710
that special value means it's not

00:18:52,799 --> 00:18:57,539
actually in the array and for that

00:18:55,710 --> 00:19:00,629
special value we're actually going to go

00:18:57,539 --> 00:19:02,179
back to a table just like we saw so

00:19:00,629 --> 00:19:04,320
those special values remember they're

00:19:02,179 --> 00:19:05,429
three percent or less of the documents

00:19:04,320 --> 00:19:06,779
are going to happen so it's very rare

00:19:05,429 --> 00:19:09,239
that we're actually going to go to this

00:19:06,779 --> 00:19:10,799
exception table so it's a little bit

00:19:09,239 --> 00:19:13,679
more expensive when we have to do this

00:19:10,799 --> 00:19:19,499
extra in direction but the very very

00:19:13,679 --> 00:19:24,210
common values are going to be faster now

00:19:19,499 --> 00:19:26,820
with that scoring that we saw sometimes

00:19:24,210 --> 00:19:29,639
we want to add some extra stuff to that

00:19:26,820 --> 00:19:31,080
score maybe 1 and x a popularity and so

00:19:29,639 --> 00:19:34,259
every document and we're going to store

00:19:31,080 --> 00:19:36,570
this popularity and this is where we

00:19:34,259 --> 00:19:39,179
need this way to look up for every

00:19:36,570 --> 00:19:41,730
document every doc ID what's the value

00:19:39,179 --> 00:19:43,649
that was associated with that when we

00:19:41,730 --> 00:19:48,989
index the document and so that's where

00:19:43,649 --> 00:19:51,389
we get doc values so doc value is there

00:19:48,989 --> 00:19:55,049
are a couple different types of doc

00:19:51,389 --> 00:19:57,359
values we start with numerics we have

00:19:55,049 --> 00:20:00,179
support for both single and multi valued

00:19:57,359 --> 00:20:02,369
binary and in binary enumeration so

00:20:00,179 --> 00:20:04,060
binary murmuration is more like the

00:20:02,369 --> 00:20:06,250
ordinals so we have like

00:20:04,060 --> 00:20:09,310
a fixed number of values that we're

00:20:06,250 --> 00:20:11,320
going to see and we're going to refer to

00:20:09,310 --> 00:20:14,410
them by this ordinal this unique ID for

00:20:11,320 --> 00:20:17,680
that value notice that binary doesn't

00:20:14,410 --> 00:20:19,780
have multi valued that's because if you

00:20:17,680 --> 00:20:21,280
want to have multi valued support for

00:20:19,780 --> 00:20:23,680
binary you could encode whatever you

00:20:21,280 --> 00:20:28,000
want into bio right it's just a bunch of

00:20:23,680 --> 00:20:30,580
binary so some of the ways that we're

00:20:28,000 --> 00:20:32,650
going to encode doc values we've already

00:20:30,580 --> 00:20:35,320
seen a bunch of these so a delta

00:20:32,650 --> 00:20:39,030
compressed table compressed const these

00:20:35,320 --> 00:20:43,240
are things that we saw before with norms

00:20:39,030 --> 00:20:46,180
we also have prefix compression which is

00:20:43,240 --> 00:20:50,190
very similar to what we're doing in in

00:20:46,180 --> 00:20:53,830
the prefix tree right that prefix FST

00:20:50,190 --> 00:20:56,110
but there are two different two extra

00:20:53,830 --> 00:20:57,610
techniques that are kind of a base for a

00:20:56,110 --> 00:20:58,720
bunch of the different doc values

00:20:57,610 --> 00:21:03,280
formats that we're going to talk about

00:20:58,720 --> 00:21:07,240
now so the first one is GCD so imagine

00:21:03,280 --> 00:21:09,880
that I have a bunch of values and let's

00:21:07,240 --> 00:21:12,160
take a time stamp for instance so let's

00:21:09,880 --> 00:21:16,660
say I'm storing milliseconds since epic

00:21:12,160 --> 00:21:19,000
and I'm actually passing in seconds I

00:21:16,660 --> 00:21:20,320
for whatever reason i'm passing seconds

00:21:19,000 --> 00:21:22,330
but i'm storing milliseconds so that

00:21:20,320 --> 00:21:24,130
means every one of my values it's going

00:21:22,330 --> 00:21:28,600
to be a thousand or a multiple of

00:21:24,130 --> 00:21:30,490
thousand so what i can do is divide all

00:21:28,600 --> 00:21:32,520
of those values by this common multiple

00:21:30,490 --> 00:21:35,050
right the greatest common denominator

00:21:32,520 --> 00:21:46,360
and then i can just store the greatest

00:21:35,050 --> 00:21:50,500
common denominator i'm going to store

00:21:46,360 --> 00:21:55,270
that 10 and then every value is going to

00:21:50,500 --> 00:21:57,790
be just the smaller value the divided by

00:21:55,270 --> 00:22:01,000
the GCD and so we're able to again use

00:21:57,790 --> 00:22:06,810
this pact ince right we're able to pack

00:22:01,000 --> 00:22:12,730
the bits in to a small values we can fit

00:22:06,810 --> 00:22:15,280
the next one is monotonic so the idea of

00:22:12,730 --> 00:22:16,810
monotonic is we have values that each

00:22:15,280 --> 00:22:18,090
document is going to have a bigger and

00:22:16,810 --> 00:22:21,600
bigger value

00:22:18,090 --> 00:22:23,370
and we often see that with like offsets

00:22:21,600 --> 00:22:27,150
for instance right so like those offsets

00:22:23,370 --> 00:22:29,190
that we saw in postings or two to refer

00:22:27,150 --> 00:22:30,450
to the postings lists these are going to

00:22:29,190 --> 00:22:31,440
be bigger and bigger right because the

00:22:30,450 --> 00:22:32,610
first term is going to be written for

00:22:31,440 --> 00:22:34,289
the second term is going to be looking

00:22:32,610 --> 00:22:39,110
for the third term these values are

00:22:34,289 --> 00:22:42,150
going to grow and what we can do is fit

00:22:39,110 --> 00:22:45,150
in this case a linear model it's what we

00:22:42,150 --> 00:22:48,809
use to the growth and then we can just

00:22:45,150 --> 00:22:52,140
encode the difference from the model and

00:22:48,809 --> 00:22:53,490
so here we can see that our first two

00:22:52,140 --> 00:22:55,770
values right those are the ones we fit

00:22:53,490 --> 00:22:57,299
the model to and they fall right on it

00:22:55,770 --> 00:22:59,159
so all we need to encode is a zero for

00:22:57,299 --> 00:23:01,169
those and then we have this one value

00:22:59,159 --> 00:23:03,659
five there it's a little bit off but we

00:23:01,169 --> 00:23:05,929
just need to encode a one right so this

00:23:03,659 --> 00:23:08,789
is what it's going to look like and

00:23:05,929 --> 00:23:10,440
obviously we have to encode what the

00:23:08,789 --> 00:23:13,980
model is right so we're going to store

00:23:10,440 --> 00:23:16,799
the M and the B and then for each value

00:23:13,980 --> 00:23:19,850
we're able to take that apply which

00:23:16,799 --> 00:23:27,419
document number we're on and add the

00:23:19,850 --> 00:23:30,090
deviation from that so finally we have

00:23:27,419 --> 00:23:31,679
got all of the documents now that are

00:23:30,090 --> 00:23:34,760
most relevant right we've known a sort

00:23:31,679 --> 00:23:36,899
on this score that we we took our

00:23:34,760 --> 00:23:38,460
relevancy score from the scene we

00:23:36,899 --> 00:23:41,490
multiplied it by popularity that we

00:23:38,460 --> 00:23:44,730
pulled from doc values and now we want

00:23:41,490 --> 00:23:48,770
to return something to the user so what

00:23:44,730 --> 00:23:52,770
are we going to return so we want to

00:23:48,770 --> 00:23:54,750
look up in the stored fields API in the

00:23:52,770 --> 00:23:56,940
scene stuff that we stored along with

00:23:54,750 --> 00:23:59,700
documents so this could be like unique

00:23:56,940 --> 00:24:02,250
identifiers that we originally had maybe

00:23:59,700 --> 00:24:04,409
it's the the full text of the document

00:24:02,250 --> 00:24:08,070
like underscore source in elastic search

00:24:04,409 --> 00:24:11,100
and so we want to look that up so how

00:24:08,070 --> 00:24:14,429
are we compressing that so there's two

00:24:11,100 --> 00:24:21,750
different techniques that are used we

00:24:14,429 --> 00:24:25,679
first have lz4 and so lz4 is based on

00:24:21,750 --> 00:24:30,330
lz77 and in general the way it works

00:24:25,679 --> 00:24:33,730
this is not going to be very exact but

00:24:30,330 --> 00:24:39,040
so we're going to start by writing out

00:24:33,730 --> 00:24:41,440
we have bee buzz 2015 bebas so as we're

00:24:39,040 --> 00:24:44,500
riding out we write out be buzzed 2015

00:24:41,440 --> 00:24:46,390
and then we get to the second be buzzed

00:24:44,500 --> 00:24:48,970
and we notice hey this is the same thing

00:24:46,390 --> 00:24:51,250
that I saw earlier so instead of writing

00:24:48,970 --> 00:24:53,950
out all of those be buzzed characters

00:24:51,250 --> 00:24:57,430
again we're going to write out a couple

00:24:53,950 --> 00:24:59,320
bites that actually refer back to where

00:24:57,430 --> 00:25:02,980
that be buds originally occurred right

00:24:59,320 --> 00:25:05,470
so it's a reverse offset and a length

00:25:02,980 --> 00:25:08,290
that we're going to store and so

00:25:05,470 --> 00:25:11,110
essentially we have two different types

00:25:08,290 --> 00:25:15,310
of entities within the encoded stream of

00:25:11,110 --> 00:25:16,840
LZ for lz77 we have literals which are

00:25:15,310 --> 00:25:19,870
like these are the things that I

00:25:16,840 --> 00:25:21,280
actually wrote out and then we have back

00:25:19,870 --> 00:25:24,220
references write these things that are

00:25:21,280 --> 00:25:36,000
referring back to previous text that we

00:25:24,220 --> 00:25:39,010
found now in in recent versions of

00:25:36,000 --> 00:25:42,760
leucine we added the ability to have a

00:25:39,010 --> 00:25:47,650
higher compression ratio and what we do

00:25:42,760 --> 00:25:52,270
this with is deflate so deflate is very

00:25:47,650 --> 00:25:57,790
similar to lz77 s or lz4 lz77 s all the

00:25:52,270 --> 00:26:00,460
same family of of encodings the idea is

00:25:57,790 --> 00:26:02,830
to take lz77 and then do a second pass

00:26:00,460 --> 00:26:05,020
over it and in that second pass we're

00:26:02,830 --> 00:26:06,490
going to find these common things that

00:26:05,020 --> 00:26:08,350
are still common and we're going to

00:26:06,490 --> 00:26:10,480
actually compress them down even more

00:26:08,350 --> 00:26:14,020
using Huffman codes so we're going to

00:26:10,480 --> 00:26:15,550
build a dictionary you know keeping

00:26:14,020 --> 00:26:18,040
track of how many times we saw

00:26:15,550 --> 00:26:23,230
particular tokens and then we're going

00:26:18,040 --> 00:26:29,080
to rewrite it with these smaller binary

00:26:23,230 --> 00:26:31,000
values okay so that is my talk and for

00:26:29,080 --> 00:26:35,970
questions if anyone has any questions I

00:26:31,000 --> 00:26:39,090
have beers in honor of Robert mirror and

00:26:35,970 --> 00:26:41,150
yeah thank you

00:26:39,090 --> 00:26:41,150

YouTube URL: https://www.youtube.com/watch?v=kCQbFxqusN4


