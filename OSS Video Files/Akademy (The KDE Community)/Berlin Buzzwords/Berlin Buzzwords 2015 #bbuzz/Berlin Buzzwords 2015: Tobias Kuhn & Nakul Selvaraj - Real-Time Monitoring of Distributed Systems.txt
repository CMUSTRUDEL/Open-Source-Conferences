Title: Berlin Buzzwords 2015: Tobias Kuhn & Nakul Selvaraj - Real-Time Monitoring of Distributed Systems
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Instrumentation has seen explosive adoption on the cloud in recent years. With the rise of micro-services we are now in an era where we measure the most trivial events in our systems. At Trademob, a mobile DSP with upwards of 125k requests per second across +700 instances, we generate and collect millions of time-series data points. Gaining key insights from this data has proven to be a huge challenge.

Outlier and Anomaly detection are two techniques that help us comprehend the behavior of our systems and allow us to take actionable decisions with little or no human intervention. Outlier Detection is the identification of misbehavior across multiple subsystems and/or aggregation layers on a machine level, whereas Anomaly Detection lets us identify issues by detecting deviations against normal behavior on a temporal level.

Read more:
https://2015.berlinbuzzwords.de/session/real-time-monitoring-distributed-systems

About Tobias Kuhn:
https://2015.berlinbuzzwords.de/users/tobias-kuhn

About Nakul Selvaraj:
https://2015.berlinbuzzwords.de/users/nakul-selvaraj

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
Oh

00:00:06,000 --> 00:00:12,170
so I'm Nicole and together with

00:00:10,550 --> 00:00:15,050
today we would like to talk to you about

00:00:12,170 --> 00:00:18,140
how we do monitoring of distributed

00:00:15,050 --> 00:00:20,390
systems at trade mob and just to give

00:00:18,140 --> 00:00:22,850
you a bit of context and trade mob is a

00:00:20,390 --> 00:00:25,730
Berlin based startup and we are a mobile

00:00:22,850 --> 00:00:28,220
app marketing platform we do things like

00:00:25,730 --> 00:00:30,919
user acquisition retargeting and we're

00:00:28,220 --> 00:00:33,160
also mobile only demand-side platform

00:00:30,919 --> 00:00:35,900
for in the real-time bidding ecosystem

00:00:33,160 --> 00:00:39,260
so I think a lot of people already heard

00:00:35,900 --> 00:00:42,710
about RTB systems a lot a bit earlier in

00:00:39,260 --> 00:00:44,449
one of the other talks this talk is

00:00:42,710 --> 00:00:47,000
going to be a two part talk and the

00:00:44,449 --> 00:00:48,679
first but I will talk about how the

00:00:47,000 --> 00:00:51,980
architecture and the engineering part of

00:00:48,679 --> 00:00:54,320
how we monitor a trade mob while so we

00:00:51,980 --> 00:00:56,449
will then talk about the science and the

00:00:54,320 --> 00:01:02,210
techniques behind them how we do these

00:00:56,449 --> 00:01:04,699
things as a DSP and we are used to

00:01:02,210 --> 00:01:06,710
handling about nearly from 10,000

00:01:04,699 --> 00:01:09,650
requests a second to a hundred or

00:01:06,710 --> 00:01:12,880
hundred 50,000 requests per second on a

00:01:09,650 --> 00:01:15,410
daily basis so this naturally means that

00:01:12,880 --> 00:01:18,230
the infrastructure costs that we have

00:01:15,410 --> 00:01:19,850
trade mob are fairly high so our

00:01:18,230 --> 00:01:22,600
infrastructure has to actually scale

00:01:19,850 --> 00:01:25,460
organically based on on our traffic as

00:01:22,600 --> 00:01:27,320
Etsy already popularized and measure

00:01:25,460 --> 00:01:29,710
anything and measure everything no

00:01:27,320 --> 00:01:31,790
matter what scale you are I think

00:01:29,710 --> 00:01:33,770
collecting metrics gives you visibility

00:01:31,790 --> 00:01:36,920
into what you're actually doing helps

00:01:33,770 --> 00:01:38,840
you increase your mean time to detect or

00:01:36,920 --> 00:01:41,600
respond to the issues that you are

00:01:38,840 --> 00:01:43,090
facing in production but at the scale

00:01:41,600 --> 00:01:45,980
that we are operating in it quickly

00:01:43,090 --> 00:01:48,800
became an issue for us we were

00:01:45,980 --> 00:01:51,560
collecting about 2000 metrics per second

00:01:48,800 --> 00:01:53,980
or on average and are on 5,000

00:01:51,560 --> 00:01:56,840
individual metrics per second at peak

00:01:53,980 --> 00:01:59,630
the number of dashboards to developers

00:01:56,840 --> 00:02:02,360
that we had was too much for us to deal

00:01:59,630 --> 00:02:03,590
with so when you can't really gain

00:02:02,360 --> 00:02:04,910
insights from your metrics you're

00:02:03,590 --> 00:02:07,030
collecting you no longer have control

00:02:04,910 --> 00:02:09,729
over the services that you're building

00:02:07,030 --> 00:02:12,379
we needed something a lot more smarter

00:02:09,729 --> 00:02:14,290
at this point of time a trending topic

00:02:12,379 --> 00:02:17,120
that was in the community of monitoring

00:02:14,290 --> 00:02:21,530
again as popularized by Etsy and Twitter

00:02:17,120 --> 00:02:23,690
was anomaly detection so with that okay

00:02:21,530 --> 00:02:24,710
now we have all this data and we can

00:02:23,690 --> 00:02:26,480
apply

00:02:24,710 --> 00:02:28,130
normally detection or outlier analysis

00:02:26,480 --> 00:02:32,270
on top of this data to actually gain

00:02:28,130 --> 00:02:33,560
insights to when things go wrong but we

00:02:32,270 --> 00:02:35,360
quickly realized that the number of

00:02:33,560 --> 00:02:37,580
false positives that we actually got

00:02:35,360 --> 00:02:39,860
with anomaly detection was way too much

00:02:37,580 --> 00:02:42,500
the number of alerts we were getting

00:02:39,860 --> 00:02:44,510
were insane our email was being

00:02:42,500 --> 00:02:48,080
bombarded by false positives all the

00:02:44,510 --> 00:02:50,270
time clearly this wasn't helping us so

00:02:48,080 --> 00:02:52,160
of course what this meant was anomaly

00:02:50,270 --> 00:02:54,080
detection was an extension to your

00:02:52,160 --> 00:02:56,530
toolkit and it was not just the solution

00:02:54,080 --> 00:02:58,760
itself we need to apply something

00:02:56,530 --> 00:03:01,310
additional to normal detection

00:02:58,760 --> 00:03:03,380
techniques such as convergent cross

00:03:01,310 --> 00:03:06,170
mapping or seasonality or trend

00:03:03,380 --> 00:03:08,000
decomposition to actually combine the

00:03:06,170 --> 00:03:10,280
signals from multiple algorithms and

00:03:08,000 --> 00:03:14,090
then collectively make decisions based

00:03:10,280 --> 00:03:17,090
on this but unfortunately there wasn't a

00:03:14,090 --> 00:03:20,180
system or a project that let us do this

00:03:17,090 --> 00:03:22,520
fairly easily so we decided we wanted to

00:03:20,180 --> 00:03:26,330
build something on our own and we had a

00:03:22,520 --> 00:03:28,700
set of requirements for that it had to

00:03:26,330 --> 00:03:31,100
be real-time or at least knurled

00:03:28,700 --> 00:03:33,260
real-time I have the Opera just because

00:03:31,100 --> 00:03:35,690
the doc said real-time monitoring of

00:03:33,260 --> 00:03:38,000
distributed systems but this is not

00:03:35,690 --> 00:03:40,190
possible because of various aggregation

00:03:38,000 --> 00:03:41,300
or policies that you have in terms of

00:03:40,190 --> 00:03:44,960
how you're collecting the metrics

00:03:41,300 --> 00:03:46,820
themselves now also we wanted an

00:03:44,960 --> 00:03:48,590
isolated point of view from different

00:03:46,820 --> 00:03:50,390
algorithms or techniques based on the on

00:03:48,590 --> 00:03:52,580
the metrics that we were collecting we

00:03:50,390 --> 00:03:54,230
wanted anomaly detection or seasonality

00:03:52,580 --> 00:03:56,390
to look at the same data points at

00:03:54,230 --> 00:03:58,640
different perspectives in an isolated

00:03:56,390 --> 00:04:01,790
manner so that we could gain signals

00:03:58,640 --> 00:04:03,560
from these and then use them naturally

00:04:01,790 --> 00:04:05,810
the reason for us to actually build this

00:04:03,560 --> 00:04:09,260
so that was for us to optimize our

00:04:05,810 --> 00:04:11,510
infrastructure costs so the system had

00:04:09,260 --> 00:04:13,840
have a really low footprint in terms of

00:04:11,510 --> 00:04:15,920
maintenance and operation costs and

00:04:13,840 --> 00:04:19,130
finally it had to be extensible

00:04:15,920 --> 00:04:21,260
extensible in terms of adding new

00:04:19,130 --> 00:04:23,380
algorithms or new techniques and also

00:04:21,260 --> 00:04:25,820
the data sources that we were actually

00:04:23,380 --> 00:04:28,450
integrating with so we wanted to be able

00:04:25,820 --> 00:04:32,060
to even measure and collect metrics from

00:04:28,450 --> 00:04:35,830
the dashboards or our business KPIs or

00:04:32,060 --> 00:04:38,760
others and this actually led us to build

00:04:35,830 --> 00:04:41,650
animali as we call it

00:04:38,760 --> 00:04:43,780
animala is a Python project and that

00:04:41,650 --> 00:04:46,690
lets you collect metrics and then

00:04:43,780 --> 00:04:49,390
analyze these bits multiple techniques

00:04:46,690 --> 00:04:51,630
with dynamic which allows you to have

00:04:49,390 --> 00:04:54,400
dynamic monitoring at the end of the day

00:04:51,630 --> 00:04:58,390
the main focus here for us was to

00:04:54,400 --> 00:04:59,950
abstract the complexity for developers

00:04:58,390 --> 00:05:02,140
so that you could just plug in your

00:04:59,950 --> 00:05:05,890
metrics and then have multiple

00:05:02,140 --> 00:05:08,350
algorithms looking at this now before we

00:05:05,890 --> 00:05:11,320
actually did this we had to deal with

00:05:08,350 --> 00:05:14,050
another issue we had to scope the

00:05:11,320 --> 00:05:15,850
metrics we are talking about distributed

00:05:14,050 --> 00:05:18,370
systems here which means that we have a

00:05:15,850 --> 00:05:19,480
large number of instances running across

00:05:18,370 --> 00:05:23,110
in different groups or different

00:05:19,480 --> 00:05:25,450
clusters and all of these instances or

00:05:23,110 --> 00:05:28,210
services are actually sending two kinds

00:05:25,450 --> 00:05:30,490
of metrics one is application metrics

00:05:28,210 --> 00:05:32,560
which come from your services and the

00:05:30,490 --> 00:05:36,310
second being the system metrics such as

00:05:32,560 --> 00:05:39,970
CPU or disk or whatever now what do I

00:05:36,310 --> 00:05:42,340
mean by scoping we want you to look at

00:05:39,970 --> 00:05:45,220
these metrics not just at an individual

00:05:42,340 --> 00:05:47,320
or an instance level but at different

00:05:45,220 --> 00:05:51,010
levels of granularity we wanted to have

00:05:47,320 --> 00:05:52,120
a micro and a macro view on the services

00:05:51,010 --> 00:05:54,280
and the metrics that we were collecting

00:05:52,120 --> 00:05:56,440
in terms of an instance level or a

00:05:54,280 --> 00:06:00,010
cluster or any sort of logical grouping

00:05:56,440 --> 00:06:02,320
at different levels so we need to build

00:06:00,010 --> 00:06:06,580
a solution for this which means meant

00:06:02,320 --> 00:06:09,790
that we had to hack certain paths to

00:06:06,580 --> 00:06:12,220
give you a small example this is one of

00:06:09,790 --> 00:06:15,160
a metric and that comes from one of our

00:06:12,220 --> 00:06:17,260
services as you can see it has a number

00:06:15,160 --> 00:06:19,840
of different tags including the host

00:06:17,260 --> 00:06:22,600
name the region or the auto scaling

00:06:19,840 --> 00:06:26,200
group or the availability zone and so on

00:06:22,600 --> 00:06:29,290
now when such a metric is pushed by the

00:06:26,200 --> 00:06:31,930
service we then multiplex it by

00:06:29,290 --> 00:06:33,940
multiplexing it what we get is a count

00:06:31,930 --> 00:06:36,820
number of possible combinations of that

00:06:33,940 --> 00:06:38,800
individual metric based on the tag so

00:06:36,820 --> 00:06:42,730
one metric when we actually receive at

00:06:38,800 --> 00:06:44,730
animali it becomes multiple log that is

00:06:42,730 --> 00:06:47,800
split at different levels of granularity

00:06:44,730 --> 00:06:49,900
these are then aggregated based on

00:06:47,800 --> 00:06:51,310
different policies with different

00:06:49,900 --> 00:06:52,480
interval periods

00:06:51,310 --> 00:06:54,670
so that when the data is actually

00:06:52,480 --> 00:06:57,520
received at animali it's ready for the

00:06:54,670 --> 00:07:00,670
algorithms to begin and start doing

00:06:57,520 --> 00:07:03,520
their magic to give a short overview of

00:07:00,670 --> 00:07:06,520
the architecture itself again this is

00:07:03,520 --> 00:07:08,860
what animali looks like we have a number

00:07:06,520 --> 00:07:11,200
of services sending metrics to our

00:07:08,860 --> 00:07:13,840
multiplexer or aggregator layer where

00:07:11,200 --> 00:07:16,870
each metric is multiplexed aggregated

00:07:13,840 --> 00:07:20,080
and computed and then it is pushed to

00:07:16,870 --> 00:07:22,420
the collector layer the collector layer

00:07:20,080 --> 00:07:22,960
is a piping implementation on top of

00:07:22,420 --> 00:07:25,600
libuv

00:07:22,960 --> 00:07:27,960
that performs really well and it pushes

00:07:25,600 --> 00:07:30,790
data to what we call a metric store

00:07:27,960 --> 00:07:33,430
which in our case happens to be Redis

00:07:30,790 --> 00:07:36,490
for sake of simplicity but you could

00:07:33,430 --> 00:07:38,910
easily plug in any other key value store

00:07:36,490 --> 00:07:41,410
based on your scenario

00:07:38,910 --> 00:07:44,200
now once the metrics are in the metrics

00:07:41,410 --> 00:07:47,560
store we have the task runner which is

00:07:44,200 --> 00:07:49,210
basically a celery instance that is

00:07:47,560 --> 00:07:51,850
running behind the scenes and

00:07:49,210 --> 00:07:54,100
selectively runs different algorithms on

00:07:51,850 --> 00:07:57,880
the data in the metric store and then

00:07:54,100 --> 00:08:00,220
this triggers further data or events

00:07:57,880 --> 00:08:02,260
which are then handled by event handlers

00:08:00,220 --> 00:08:03,580
now in our case this event handlers

00:08:02,260 --> 00:08:05,470
could be something as simple as sending

00:08:03,580 --> 00:08:07,570
Orton alert or it could also be

00:08:05,470 --> 00:08:10,110
something like killing an instance

00:08:07,570 --> 00:08:14,200
because it is performing sub-optimally

00:08:10,110 --> 00:08:16,690
so anomaly to summarize the architecture

00:08:14,200 --> 00:08:19,150
it's just lets you selectively collect

00:08:16,690 --> 00:08:21,190
metrics and then analyze this and

00:08:19,150 --> 00:08:25,060
trigger events upon which you can then

00:08:21,190 --> 00:08:27,700
react to leading developers to have a

00:08:25,060 --> 00:08:29,560
hands-free approach on top of this now

00:08:27,700 --> 00:08:31,750
Toby will talk about the algorithms

00:08:29,560 --> 00:08:36,250
which we actually use to help in

00:08:31,750 --> 00:08:39,280
production yeah thanks I cool um

00:08:36,250 --> 00:08:42,340
I said I will going to present the

00:08:39,280 --> 00:08:45,910
algorithmic part of anomaly and first of

00:08:42,340 --> 00:08:47,950
all I would like to start with a short

00:08:45,910 --> 00:08:49,960
introduction how anomalies could look

00:08:47,950 --> 00:08:53,230
like in our case and what possibly ways

00:08:49,960 --> 00:08:55,600
they are to solve those issues or to

00:08:53,230 --> 00:08:59,050
detect those issues so the first example

00:08:55,600 --> 00:09:01,660
is rather simple it's just showing the

00:08:59,050 --> 00:09:03,590
CPU utilization of a single box which

00:09:01,660 --> 00:09:05,510
suddenly peaks at 100%

00:09:03,590 --> 00:09:08,060
and because it's also quite easy to

00:09:05,510 --> 00:09:11,630
detect if you just apply a static

00:09:08,060 --> 00:09:14,960
threshold we can catch this and react on

00:09:11,630 --> 00:09:19,070
that the second example also shows CPU

00:09:14,960 --> 00:09:21,529
utilization in this case it looks rather

00:09:19,070 --> 00:09:23,900
okay so it's quite stable over time but

00:09:21,529 --> 00:09:24,800
it is not okay if we put this into

00:09:23,900 --> 00:09:27,529
context

00:09:24,800 --> 00:09:29,420
this means the green band what we see

00:09:27,529 --> 00:09:30,740
here will come to this in a bit more

00:09:29,420 --> 00:09:34,310
detail the next slides

00:09:30,740 --> 00:09:37,060
define something like expected state of

00:09:34,310 --> 00:09:39,589
this box in this particular service and

00:09:37,060 --> 00:09:41,810
yeah the shown box obviously does not

00:09:39,589 --> 00:09:44,600
fit into that so depending on the time

00:09:41,810 --> 00:09:49,460
you can give define this also as an

00:09:44,600 --> 00:09:51,740
outlier and the third example is a time

00:09:49,460 --> 00:09:54,260
series plot of incoming system requests

00:09:51,740 --> 00:09:56,900
a trade map we see three consecutive

00:09:54,260 --> 00:10:00,230
days and I guess you all see also the

00:09:56,900 --> 00:10:02,960
outages what we have here and well the

00:10:00,230 --> 00:10:04,990
problem here is more of having an

00:10:02,960 --> 00:10:08,360
algorithm which is capable of detecting

00:10:04,990 --> 00:10:11,780
outages like this and if we all put this

00:10:08,360 --> 00:10:14,210
together it leads to the point that

00:10:11,780 --> 00:10:17,089
before we can do proper anomaly

00:10:14,210 --> 00:10:19,339
detection we need idea of what we expect

00:10:17,089 --> 00:10:21,800
from our system so we need a definition

00:10:19,339 --> 00:10:25,339
of normality before we can charge

00:10:21,800 --> 00:10:27,980
certain situations as okay or as false

00:10:25,339 --> 00:10:29,800
in the end and in the following I would

00:10:27,980 --> 00:10:32,570
like to go into two different directions

00:10:29,800 --> 00:10:35,660
the first one is the behavior of the

00:10:32,570 --> 00:10:37,430
distributed system so as mentioned we

00:10:35,660 --> 00:10:39,470
are running up to a thousand boxes at

00:10:37,430 --> 00:10:43,310
time and yeah the question is how two

00:10:39,470 --> 00:10:47,330
single boxes within specific services do

00:10:43,310 --> 00:10:50,240
behave and the approach we are using is

00:10:47,330 --> 00:10:53,920
rather simple it's the so called tackies

00:10:50,240 --> 00:10:55,910
outlier filter and again it's using

00:10:53,920 --> 00:10:59,870
statistics so what we see here is a

00:10:55,910 --> 00:11:01,970
distribution of different instances of

00:10:59,870 --> 00:11:03,920
again CPU utilization I don't know if

00:11:01,970 --> 00:11:07,130
you see it it's from 0 to 100 percent

00:11:03,920 --> 00:11:10,459
and we see the most most of the boxes at

00:11:07,130 --> 00:11:12,650
around 50 to 60 65 percent but there are

00:11:10,459 --> 00:11:16,570
also some which are far below or above

00:11:12,650 --> 00:11:18,730
those values and the question is

00:11:16,570 --> 00:11:22,030
how can we chat so how can we get them

00:11:18,730 --> 00:11:25,210
as outliers and one way of doing this is

00:11:22,030 --> 00:11:29,680
applying quantize to that so what we see

00:11:25,210 --> 00:11:32,230
here quantile 25 and the quantize 75 so

00:11:29,680 --> 00:11:35,770
quick reminder quantity 25 is just a cut

00:11:32,230 --> 00:11:39,160
off from the lower 25% to the upper 75%

00:11:35,770 --> 00:11:41,860
of the distribution and both values

00:11:39,160 --> 00:11:44,020
together define the so-called inter

00:11:41,860 --> 00:11:47,290
quartile range which is a quite robust

00:11:44,020 --> 00:11:50,440
statistics containing the middle 50% of

00:11:47,290 --> 00:11:53,710
the values and if we now take a constant

00:11:50,440 --> 00:11:56,410
of that and add it to the quantized 75

00:11:53,710 --> 00:11:59,080
and subtract it from the quantity 25 we

00:11:56,410 --> 00:12:01,330
get a screen band which in the end

00:11:59,080 --> 00:12:04,810
defines something like an healthy range

00:12:01,330 --> 00:12:08,080
or as a stage of the system which we

00:12:04,810 --> 00:12:11,020
accept and everything above or below we

00:12:08,080 --> 00:12:13,420
would then define as an outlier this is

00:12:11,020 --> 00:12:13,720
only a snapshot if you look at it over

00:12:13,420 --> 00:12:16,390
time

00:12:13,720 --> 00:12:19,030
it looks of course more like this so we

00:12:16,390 --> 00:12:22,210
have again the evolving green band and

00:12:19,030 --> 00:12:23,950
most of the instances are okay but

00:12:22,210 --> 00:12:28,030
there's one box which is only using a

00:12:23,950 --> 00:12:29,800
fraction of possible CPU and yet you can

00:12:28,030 --> 00:12:31,720
say we are essentially losing money

00:12:29,800 --> 00:12:35,590
there because we are not using what we

00:12:31,720 --> 00:12:38,350
could use so if this dates persists for

00:12:35,590 --> 00:12:40,690
a fixed period of time like one hour two

00:12:38,350 --> 00:12:42,490
hours we can react on that by just

00:12:40,690 --> 00:12:47,320
restarting the service or kill the box

00:12:42,490 --> 00:12:48,690
in this case to sum this up the idea

00:12:47,320 --> 00:12:52,150
here is that we want to identify

00:12:48,690 --> 00:12:54,490
auto-scaling issues not only about CP

00:12:52,150 --> 00:12:56,710
you can also think about memory leaks or

00:12:54,490 --> 00:13:00,760
business values also so incoming

00:12:56,710 --> 00:13:06,130
requests per box for instance of course

00:13:00,760 --> 00:13:07,870
the approach has restrictions so in the

00:13:06,130 --> 00:13:10,180
end what we always need is kind of a

00:13:07,870 --> 00:13:12,700
help arrange so if you think about a

00:13:10,180 --> 00:13:15,070
service group and all boxes have issues

00:13:12,700 --> 00:13:16,930
you would still define something like an

00:13:15,070 --> 00:13:19,330
interquartile range and you will find a

00:13:16,930 --> 00:13:22,150
lot of boxes as healthy even though the

00:13:19,330 --> 00:13:24,580
whole service has problems and to tackle

00:13:22,150 --> 00:13:27,070
this it's not sufficient to just use

00:13:24,580 --> 00:13:28,750
this of course so we also need to look

00:13:27,070 --> 00:13:32,620
at the overall system performance

00:13:28,750 --> 00:13:35,500
and yeah one last word to CPU usage you

00:13:32,620 --> 00:13:38,470
can just think about taking the average

00:13:35,500 --> 00:13:41,170
CPU usage of all the boxes within that

00:13:38,470 --> 00:13:43,960
service and if this metric has issues

00:13:41,170 --> 00:13:47,650
then well the whole service has issues

00:13:43,960 --> 00:13:51,940
and not only a couple of boxes and yeah

00:13:47,650 --> 00:13:53,950
also here there are several ways to go

00:13:51,940 --> 00:13:57,010
on this problem and you approach what we

00:13:53,950 --> 00:14:01,120
are using it's mostly the so-called

00:13:57,010 --> 00:14:03,550
season a 20 composition algorithm it's

00:14:01,120 --> 00:14:06,040
using the fact that most of our metrics

00:14:03,550 --> 00:14:08,350
have this inherit seasonality which is

00:14:06,040 --> 00:14:11,740
one day usually so what we see here are

00:14:08,350 --> 00:14:13,870
seven consecutive days and yeah I guess

00:14:11,740 --> 00:14:17,800
the pattern is quite obvious that those

00:14:13,870 --> 00:14:19,800
are daily metrics and by the use of this

00:14:17,800 --> 00:14:22,870
you can then apply this algorithm and

00:14:19,800 --> 00:14:26,500
decompose this time series into two

00:14:22,870 --> 00:14:27,970
parts so we have first the seasonal part

00:14:26,500 --> 00:14:30,910
which is then more or less identical

00:14:27,970 --> 00:14:35,620
over the days and we also have the trend

00:14:30,910 --> 00:14:37,650
part it's also looking quite constant in

00:14:35,620 --> 00:14:40,089
the end we see a slight increase and

00:14:37,650 --> 00:14:42,850
that is for a good reason so if you look

00:14:40,089 --> 00:14:44,440
into the days you see that this was

00:14:42,850 --> 00:14:46,900
Saturday Sunday and we have a slight

00:14:44,440 --> 00:14:50,440
increase of traffic and yet this is

00:14:46,900 --> 00:14:54,420
reflected by this trend component how do

00:14:50,440 --> 00:14:58,020
we use that another example to that so

00:14:54,420 --> 00:15:01,060
incoming requests for two days again and

00:14:58,020 --> 00:15:04,470
we now apply the algorithm and decompose

00:15:01,060 --> 00:15:07,360
this time series and generate those two

00:15:04,470 --> 00:15:11,200
time series seasonal and trend part and

00:15:07,360 --> 00:15:15,820
you can just add them up now so you can

00:15:11,200 --> 00:15:18,220
create a model of your incoming service

00:15:15,820 --> 00:15:21,100
metric so that defines something like

00:15:18,220 --> 00:15:24,190
the expected state and if you put this

00:15:21,100 --> 00:15:27,339
together to the raw input and just take

00:15:24,190 --> 00:15:30,040
the difference it's yeah quite obvious

00:15:27,339 --> 00:15:31,900
that you get the distribution of error

00:15:30,040 --> 00:15:35,080
or deviation and in this case with

00:15:31,900 --> 00:15:38,680
simple thresholds you could detect those

00:15:35,080 --> 00:15:41,470
outages unfortunately it doesn't look

00:15:38,680 --> 00:15:43,190
that nice all the time so this final

00:15:41,470 --> 00:15:47,630
error distribution

00:15:43,190 --> 00:15:49,580
in reality looks more like this so we

00:15:47,630 --> 00:15:52,880
have Peaks drops longer one stronger

00:15:49,580 --> 00:15:55,490
ones and the question is of course when

00:15:52,880 --> 00:15:59,030
do we want to react so the overall goal

00:15:55,490 --> 00:16:00,740
is for sure that we have a very small

00:15:59,030 --> 00:16:04,610
number of false positive but we want

00:16:00,740 --> 00:16:08,270
also to detect as much as issues we have

00:16:04,610 --> 00:16:11,030
so there are several ways and this is

00:16:08,270 --> 00:16:13,730
actually a big part of the whole problem

00:16:11,030 --> 00:16:15,560
how to judge those distributions and

00:16:13,730 --> 00:16:17,960
just to name a few you can use

00:16:15,560 --> 00:16:21,320
normalization for instance if you have a

00:16:17,960 --> 00:16:24,380
broad range of your incoming metric you

00:16:21,320 --> 00:16:27,380
can set minimum deviations you can use

00:16:24,380 --> 00:16:30,130
static thresholds and the way we are

00:16:27,380 --> 00:16:32,900
going for usually our dynamic thresholds

00:16:30,130 --> 00:16:34,790
by the use of t digests which was i

00:16:32,900 --> 00:16:37,490
guess some of you heard it yesterday

00:16:34,790 --> 00:16:41,600
presented by deadening also so a quite

00:16:37,490 --> 00:16:44,120
nice online clustering to estimate

00:16:41,600 --> 00:16:46,040
quantize of your distribution and if

00:16:44,120 --> 00:16:49,370
you've applied this or combination of

00:16:46,040 --> 00:16:51,170
that then essentially we want to build

00:16:49,370 --> 00:16:53,240
something like that so we have a flag

00:16:51,170 --> 00:16:55,480
now which is just saying everything is

00:16:53,240 --> 00:16:58,640
fine or we are in an arrow state and

00:16:55,480 --> 00:17:00,700
yeah pending on the length of this arrow

00:16:58,640 --> 00:17:03,440
state we then can take actions like

00:17:00,700 --> 00:17:07,939
restarting services or boxes or

00:17:03,440 --> 00:17:11,750
alert some responsible person social and

00:17:07,939 --> 00:17:14,270
yeah that's it about animali last but

00:17:11,750 --> 00:17:17,569
not least we decided that we want to

00:17:14,270 --> 00:17:21,470
optimize this project so actually it's

00:17:17,569 --> 00:17:23,569
already open sourced since today so you

00:17:21,470 --> 00:17:26,000
will find that on github the whole

00:17:23,569 --> 00:17:29,060
animali project and additionally also on

00:17:26,000 --> 00:17:31,160
python implementation of t digest which

00:17:29,060 --> 00:17:33,770
is heavily used within animali

00:17:31,160 --> 00:17:36,190
and well that's it from my side thank

00:17:33,770 --> 00:17:36,190

YouTube URL: https://www.youtube.com/watch?v=iSxlLW-CUNw


