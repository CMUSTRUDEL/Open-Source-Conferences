Title: Berlin Buzzwords 2015: Isabelle Robin & Matthieu Vautrot â€“ How to automate the deployment of ...
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Data Science has enabled companies to establish predictive models about their sales, forecast their needs in human resources, enhance their customer knowledge and so much more. But what is the afterlife of these models? Are they doomed to perform one-shot predictions and then fade away? After the training and testing steps, the final part of an end-to-end Data Science project should be deploying the constructed model in a production environment in order to reuse easily its results.

Today this step can be quite time-consuming when it involves rewriting completely the machine learning model in another language or combining specific skills in machine learning and production coding. In this talk, we will present several techniques to automate the deployment of any R model in two complementary production environments: a big data cluster and a web service.

Read more:
https://2015.berlinbuzzwords.de/session/model-code-how-automate-deployment-r-models-production-environment

About Isabelle Robin:
https://2015.berlinbuzzwords.de/users/isabelle-robin

About Matthieu Vautrot:
https://2015.berlinbuzzwords.de/users/matthieu-vautrot

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,980 --> 00:00:10,730
all right thank you for the introduction

00:00:08,870 --> 00:00:13,070
hello everyone

00:00:10,730 --> 00:00:15,469
we are really happy to be here today and

00:00:13,070 --> 00:00:17,150
thank you for attending our our little

00:00:15,469 --> 00:00:20,960
talk this is the first time we're going

00:00:17,150 --> 00:00:24,009
to do this talk so please be be a

00:00:20,960 --> 00:00:26,060
patient if it's a bit rusty sometime

00:00:24,009 --> 00:00:29,560
today we're going to talk about a

00:00:26,060 --> 00:00:32,390
project we've done for one of our client

00:00:29,560 --> 00:00:36,050
which we called Model S code

00:00:32,390 --> 00:00:41,480
the aim of this project was to to

00:00:36,050 --> 00:00:43,460
develop an approach on how to automate

00:00:41,480 --> 00:00:46,460
the deployment of our models in two

00:00:43,460 --> 00:00:49,430
different production environments so

00:00:46,460 --> 00:00:52,430
first a little bit a little word about

00:00:49,430 --> 00:00:58,100
canary canary is a young startup we are

00:00:52,430 --> 00:01:00,590
three years old now and we consulting in

00:00:58,100 --> 00:01:05,180
data science in based in Paris so we

00:01:00,590 --> 00:01:07,760
help our client with the analytical data

00:01:05,180 --> 00:01:12,380
project and so on this is going to be a

00:01:07,760 --> 00:01:16,130
two-person speech so first it was going

00:01:12,380 --> 00:01:19,310
to be Isabel another scientist at khatma

00:01:16,130 --> 00:01:25,850
tree and and me was also working for

00:01:19,310 --> 00:01:28,520
Coventry other as a consultant so just

00:01:25,850 --> 00:01:31,159
for an introduction and for for having a

00:01:28,520 --> 00:01:34,549
little bit of context usually what we

00:01:31,159 --> 00:01:36,259
see at our clients and how they deal

00:01:34,549 --> 00:01:38,750
their data science project and they're

00:01:36,259 --> 00:01:41,360
all the analytical project is that they

00:01:38,750 --> 00:01:43,610
have to separately the environment for

00:01:41,360 --> 00:01:46,340
form for doing there are the different

00:01:43,610 --> 00:01:48,920
and each core projects usually they have

00:01:46,340 --> 00:01:52,460
their projection environment where they

00:01:48,920 --> 00:01:55,040
have all their businesses use cases this

00:01:52,460 --> 00:01:57,829
is where they have their typical a DBMS

00:01:55,040 --> 00:02:00,170
sometimes they use Hadoop in production

00:01:57,829 --> 00:02:02,600
and different different system for

00:02:00,170 --> 00:02:04,700
further and further production and then

00:02:02,600 --> 00:02:06,740
on the other end as a sandbox there's a

00:02:04,700 --> 00:02:09,440
development platform sometimes we call

00:02:06,740 --> 00:02:11,840
it data lab this is where we are going

00:02:09,440 --> 00:02:13,730
to gather and centralize all the data

00:02:11,840 --> 00:02:16,040
and this is where all the scientists

00:02:13,730 --> 00:02:18,410
will will develop their model develop

00:02:16,040 --> 00:02:19,460
the insight from from there from the

00:02:18,410 --> 00:02:22,040
data they have in there

00:02:19,460 --> 00:02:24,020
penny so I'm just going to present a

00:02:22,040 --> 00:02:26,390
little bit faster the different steps of

00:02:24,020 --> 00:02:29,000
this kind of project though are actually

00:02:26,390 --> 00:02:31,190
quite typical steps the first step is

00:02:29,000 --> 00:02:34,160
usually to first collect the different

00:02:31,190 --> 00:02:38,060
data sources sources can be external

00:02:34,160 --> 00:02:41,030
sources or internal sources then you

00:02:38,060 --> 00:02:44,150
have to store the data source into a

00:02:41,030 --> 00:02:46,360
DBMS into almost akin no sequel engine

00:02:44,150 --> 00:02:49,190
or even sometime the file system of your

00:02:46,360 --> 00:02:51,860
development environment and sometimes

00:02:49,190 --> 00:02:54,410
even Hadoop then you do you the

00:02:51,860 --> 00:02:56,450
pre-processing the different first big

00:02:54,410 --> 00:02:57,890
analysis and this is where we're going

00:02:56,450 --> 00:03:01,460
to make your data talk with each other

00:02:57,890 --> 00:03:04,730
and get rid of any duplicates if needed

00:03:01,460 --> 00:03:06,980
and so on and then there's a predict

00:03:04,730 --> 00:03:10,820
step this is where usually the data

00:03:06,980 --> 00:03:13,790
scientist use this use R or Python to

00:03:10,820 --> 00:03:16,520
produce 30 chief model on the data that

00:03:13,790 --> 00:03:19,550
they just gathered and organized the

00:03:16,520 --> 00:03:21,710
little bit and finally usually in those

00:03:19,550 --> 00:03:24,410
kind of project the real-life test on

00:03:21,710 --> 00:03:27,020
the model they just developed to say

00:03:24,410 --> 00:03:30,290
okay if is the model good enough for

00:03:27,020 --> 00:03:33,500
business does it meet the requirement we

00:03:30,290 --> 00:03:35,630
we want heed so if not we'll go back to

00:03:33,500 --> 00:03:37,670
a full cycle and if yes what happens

00:03:35,630 --> 00:03:40,400
what what do we do now that we are happy

00:03:37,670 --> 00:03:42,170
with our model this is going to be the

00:03:40,400 --> 00:03:46,040
question at this talk today

00:03:42,170 --> 00:03:47,510
what about deployment usually we still a

00:03:46,040 --> 00:03:49,760
lot of time that struggle with this

00:03:47,510 --> 00:03:51,830
question and say okay my data scientist

00:03:49,760 --> 00:03:54,560
are ready they know are they know Hadoop

00:03:51,830 --> 00:03:59,480
and so on but what next they are afraid

00:03:54,560 --> 00:04:04,280
of of the model being stay expect in in

00:03:59,480 --> 00:04:06,020
those kind of sandbox so usually the

00:04:04,280 --> 00:04:09,170
classical approach is to tackle this

00:04:06,020 --> 00:04:12,740
problem is to first recode the model

00:04:09,170 --> 00:04:14,900
once it's trained on our Python then you

00:04:12,740 --> 00:04:16,820
can get your results back and say okay I

00:04:14,900 --> 00:04:18,890
will implement the different coefficient

00:04:16,820 --> 00:04:22,490
of my my linear regression and so on

00:04:18,890 --> 00:04:24,380
into sequel C++ and Java is this fine

00:04:22,490 --> 00:04:27,050
works well of course it takes a bit of

00:04:24,380 --> 00:04:27,620
time and you can also use the PMML

00:04:27,050 --> 00:04:29,419
stand-down

00:04:27,620 --> 00:04:32,360
which is a

00:04:29,419 --> 00:04:36,079
kind of norm which allows you to write

00:04:32,360 --> 00:04:39,379
any a lot of models into a XML file and

00:04:36,079 --> 00:04:42,679
then you can use a API such open scoring

00:04:39,379 --> 00:04:44,779
to read back your XML file and do the

00:04:42,679 --> 00:04:48,289
scoring scoring on your different

00:04:44,779 --> 00:04:50,809
platforms but PMML is fine but what

00:04:48,289 --> 00:04:53,119
about non-sporting models but all models

00:04:50,809 --> 00:04:56,509
are as per be are able to be written

00:04:53,119 --> 00:04:58,939
into into xml and some prediction

00:04:56,509 --> 00:05:03,139
environment doesn't speak PMML so it's

00:04:58,939 --> 00:05:05,089
not really a general general approach so

00:05:03,139 --> 00:05:08,269
was for this problem who who didn't

00:05:05,089 --> 00:05:13,309
bother our client was a player in the

00:05:08,269 --> 00:05:16,579
French energy industry and he did he

00:05:13,309 --> 00:05:19,699
built model for predicting consumption

00:05:16,579 --> 00:05:21,949
consumption into aggregates part of

00:05:19,699 --> 00:05:24,559
France and they were really happy with

00:05:21,949 --> 00:05:27,499
the model and they didn't want us to

00:05:24,559 --> 00:05:30,409
improve it which which which is fine for

00:05:27,499 --> 00:05:32,749
some time and as you can see you the

00:05:30,409 --> 00:05:35,629
model they play they use in orange line

00:05:32,749 --> 00:05:40,360
that the predicted consumption and in

00:05:35,629 --> 00:05:44,839
blue the realized realized the observed

00:05:40,360 --> 00:05:48,309
consumption so this works well but and

00:05:44,839 --> 00:05:50,659
this is a well the problem is they

00:05:48,309 --> 00:05:53,329
really happy with the model and they

00:05:50,659 --> 00:05:55,189
don't want to change anything and what

00:05:53,329 --> 00:05:58,399
they do is that they use for this kind

00:05:55,189 --> 00:06:01,489
of use case what we call gum model gum

00:05:58,399 --> 00:06:04,729
is a generalized additive model

00:06:01,489 --> 00:06:08,089
it's another model that's trying to fit

00:06:04,729 --> 00:06:10,219
on any variables of your data data set

00:06:08,089 --> 00:06:14,659
one's going to try to fit a function

00:06:10,219 --> 00:06:18,009
that it has into its its library so they

00:06:14,659 --> 00:06:21,199
use a very specific R package called MDC

00:06:18,009 --> 00:06:24,649
which is not supported by PMML and that

00:06:21,199 --> 00:06:27,949
the results are not easily read really

00:06:24,649 --> 00:06:30,879
reco recordable you cannot record that

00:06:27,949 --> 00:06:37,399
is those kind of function that our model

00:06:30,879 --> 00:06:39,610
uses so so they has test is there any

00:06:37,399 --> 00:06:43,599
way for for

00:06:39,610 --> 00:06:47,110
less to get less struggle with this step

00:06:43,599 --> 00:06:49,449
and we had an approach in in our mind

00:06:47,110 --> 00:06:52,419
for for quite a while and so and it was

00:06:49,449 --> 00:06:54,490
the time to actually say okay we we have

00:06:52,419 --> 00:06:56,889
an idea which would should work on paper

00:06:54,490 --> 00:06:57,580
and we will try we'll try with you and

00:06:56,889 --> 00:07:00,039
see how it goes

00:06:57,580 --> 00:07:02,530
so the model of code the name of the

00:07:00,039 --> 00:07:05,080
project the the goal is to directly

00:07:02,530 --> 00:07:07,300
deploy and use the AR object for

00:07:05,080 --> 00:07:08,949
projection so the goal is to say okay

00:07:07,300 --> 00:07:11,020
I'm going to use my our object which

00:07:08,949 --> 00:07:13,029
does the prediction fire the prediction

00:07:11,020 --> 00:07:14,889
step quite seriously and I'm going to

00:07:13,029 --> 00:07:17,110
deploy it directly into my production

00:07:14,889 --> 00:07:22,479
environment and use it for my for my

00:07:17,110 --> 00:07:25,569
day-to-day analysis to do so we set

00:07:22,479 --> 00:07:27,370
three goalie in our mind first of course

00:07:25,569 --> 00:07:29,800
we want you to drastically reduce the

00:07:27,370 --> 00:07:32,830
deployment time so that it was really

00:07:29,800 --> 00:07:35,020
take less time than recording the the

00:07:32,830 --> 00:07:36,789
model we wanted the approach to be as

00:07:35,020 --> 00:07:39,639
general as possible so for one

00:07:36,789 --> 00:07:42,580
environment we wanted to put just one

00:07:39,639 --> 00:07:44,919
code for any model we will submit after

00:07:42,580 --> 00:07:47,729
that and we wanted of course to be

00:07:44,919 --> 00:07:50,069
stable in performance we didn't know if

00:07:47,729 --> 00:07:52,690
stacking up the production size

00:07:50,069 --> 00:07:55,180
prediction service plus R plus the

00:07:52,690 --> 00:07:57,099
different layer of this approach would

00:07:55,180 --> 00:08:00,460
be really stable in performance so we

00:07:57,099 --> 00:08:02,349
want you to be careful with that so to

00:08:00,460 --> 00:08:05,229
go into a little bit more details I'm

00:08:02,349 --> 00:08:08,589
going to let Isabel talk to for the next

00:08:05,229 --> 00:08:11,919
part of this page okay so I explained

00:08:08,589 --> 00:08:15,819
you first the few details about our

00:08:11,919 --> 00:08:18,009
approach how we designed it so first

00:08:15,819 --> 00:08:20,860
thing first you've got your development

00:08:18,009 --> 00:08:23,259
platform where R is installed on it and

00:08:20,860 --> 00:08:25,330
you've got your production platform

00:08:23,259 --> 00:08:28,629
where a service is running let's say

00:08:25,330 --> 00:08:31,949
it's your data or Hadoop so the first

00:08:28,629 --> 00:08:34,360
step is to serialize your your model

00:08:31,949 --> 00:08:37,620
this step is quite straightforward

00:08:34,360 --> 00:08:42,159
because in our for example you've got

00:08:37,620 --> 00:08:45,010
some functions to save RDS save in RDS

00:08:42,159 --> 00:08:47,949
or save in RDA format which are

00:08:45,010 --> 00:08:49,600
serialized format so once you've got

00:08:47,949 --> 00:08:52,000
your binary objects

00:08:49,600 --> 00:08:53,170
you should deploy it to the production

00:08:52,000 --> 00:08:55,120
platform

00:08:53,170 --> 00:08:56,800
the step is also quite easy because you

00:08:55,120 --> 00:08:59,560
just have to copy it to the production

00:08:56,800 --> 00:09:01,540
platform but the most tricky step is

00:08:59,560 --> 00:09:03,250
when you want to do the prediction in

00:09:01,540 --> 00:09:05,079
fact you want to enable the

00:09:03,250 --> 00:09:07,690
communication between our and your

00:09:05,079 --> 00:09:10,420
service so first step is to install our

00:09:07,690 --> 00:09:12,240
in your production platform and then to

00:09:10,420 --> 00:09:18,190
enable the communication between the two

00:09:12,240 --> 00:09:21,130
so you for this you will need some

00:09:18,190 --> 00:09:24,070
specific tools fortunately in Java there

00:09:21,130 --> 00:09:27,220
is a library that exists it's called our

00:09:24,070 --> 00:09:29,529
Java and in merges two projects gyri

00:09:27,220 --> 00:09:29,920
which enables to open our session in

00:09:29,529 --> 00:09:33,430
Java

00:09:29,920 --> 00:09:36,399
that's the library we will use in this

00:09:33,430 --> 00:09:39,850
project and our Java which enables to

00:09:36,399 --> 00:09:42,519
use Java in your art session so the name

00:09:39,850 --> 00:09:46,540
our Java bundles both are Java and

00:09:42,519 --> 00:09:50,470
j'irai then to use j'irai you just need

00:09:46,540 --> 00:09:53,050
to use to start to start your are in

00:09:50,470 --> 00:09:55,420
join to to open your our session in Java

00:09:53,050 --> 00:09:57,940
so what you will just use the function

00:09:55,420 --> 00:10:00,519
get main in join and then after that you

00:09:57,940 --> 00:10:03,579
just have to use you can use any our

00:10:00,519 --> 00:10:06,070
command in fact with with the function

00:10:03,579 --> 00:10:09,130
eval and you just have to pass your our

00:10:06,070 --> 00:10:12,220
code into a parameter of your function

00:10:09,130 --> 00:10:14,680
evil so that's quite simple in fact but

00:10:12,220 --> 00:10:16,930
on our way to prediction we realize that

00:10:14,680 --> 00:10:18,610
our objects our modal's

00:10:16,930 --> 00:10:21,730
the circularized modules were really

00:10:18,610 --> 00:10:25,480
quite heavy in fact it's because they

00:10:21,730 --> 00:10:29,350
carry everything for for training so

00:10:25,480 --> 00:10:32,079
they really have to in fact we just

00:10:29,350 --> 00:10:34,600
wanted to do the prediction with this

00:10:32,079 --> 00:10:37,949
modal's so we didn't need at all all the

00:10:34,600 --> 00:10:40,899
data for all training so we decided to

00:10:37,949 --> 00:10:45,149
remove all this metadata all this hidden

00:10:40,899 --> 00:10:48,550
metadata by iteratively iteratively

00:10:45,149 --> 00:10:52,029
removing the the data that was not used

00:10:48,550 --> 00:10:54,329
for that was not necessary for a

00:10:52,029 --> 00:10:54,329
prediction

00:10:54,449 --> 00:11:03,010
so you

00:10:59,820 --> 00:11:04,930
so you you can you can develop this

00:11:03,010 --> 00:11:08,920
approach in several prediction

00:11:04,930 --> 00:11:12,580
environments it could be a web service

00:11:08,920 --> 00:11:14,020
Hadoop relational database complex even

00:11:12,580 --> 00:11:16,840
processing in fact the approach is quite

00:11:14,020 --> 00:11:20,440
general so you could imagine a lot of

00:11:16,840 --> 00:11:23,050
prediction environments but we decided

00:11:20,440 --> 00:11:25,600
in the time we had for this short

00:11:23,050 --> 00:11:28,120
project to develop two of these

00:11:25,600 --> 00:11:32,010
approaches the web service and the

00:11:28,120 --> 00:11:35,560
Hadoop approach so now I will talk about

00:11:32,010 --> 00:11:40,330
the web service approach which was known

00:11:35,560 --> 00:11:42,850
as the REST API so how how will that

00:11:40,330 --> 00:11:45,970
work first you build your predictive

00:11:42,850 --> 00:11:47,800
model on R then you serialize and deploy

00:11:45,970 --> 00:11:51,190
it to your API things to put request

00:11:47,800 --> 00:11:52,780
then you have to launch your prediction

00:11:51,190 --> 00:11:56,020
on new data thanks to a post request

00:11:52,780 --> 00:11:58,980
okay so the architecture is quite simple

00:11:56,020 --> 00:12:01,300
so you just it's just classic

00:11:58,980 --> 00:12:03,100
architecture the only new thing is in

00:12:01,300 --> 00:12:09,040
the communication between R and Java as

00:12:03,100 --> 00:12:11,650
we said earlier so in your request you

00:12:09,040 --> 00:12:13,240
have to enter three parameters the type

00:12:11,650 --> 00:12:15,880
of the modal let's say it's a linear

00:12:13,240 --> 00:12:19,540
regression logistic regression again

00:12:15,880 --> 00:12:21,910
modal you have to enter the ID of the

00:12:19,540 --> 00:12:24,760
modal in fact it's the name of the RDS

00:12:21,910 --> 00:12:28,780
site of the RDS file and you have to

00:12:24,760 --> 00:12:31,510
enter the daysand data so this requests

00:12:28,780 --> 00:12:34,420
goes to the modal exposure class which

00:12:31,510 --> 00:12:37,000
translates every HTTP request into a

00:12:34,420 --> 00:12:43,710
call to a function which will deploy the

00:12:37,000 --> 00:12:47,200
our code then this deploy your class

00:12:43,710 --> 00:12:50,320
gathers old functions that can call the

00:12:47,200 --> 00:12:52,870
our code thanks to joy and your data

00:12:50,320 --> 00:12:57,100
goes to the input/output class that

00:12:52,870 --> 00:13:00,010
modal's your JSON data you will have

00:12:57,100 --> 00:13:03,820
free type off requests the first the get

00:13:00,010 --> 00:13:05,020
request which enables you to return old

00:13:03,820 --> 00:13:07,330
available modal's

00:13:05,020 --> 00:13:10,540
in your API the put request which

00:13:07,330 --> 00:13:12,499
enables you to deploy a modal and load

00:13:10,540 --> 00:13:15,499
in a serial libraries of

00:13:12,499 --> 00:13:18,049
your API and then the post request which

00:13:15,499 --> 00:13:21,289
enables you to have the prediction of

00:13:18,049 --> 00:13:23,959
your new data this the design of this

00:13:21,289 --> 00:13:26,959
freeway request was I kind of inspired

00:13:23,959 --> 00:13:30,829
of another projects open scoring which

00:13:26,959 --> 00:13:33,469
is an API for deploying PMML modal you

00:13:30,829 --> 00:13:35,239
can check the rig a job and there are

00:13:33,469 --> 00:13:38,599
quite a bunch of interesting information

00:13:35,239 --> 00:13:40,639
that so how about the post requests how

00:13:38,599 --> 00:13:44,449
do you do the prediction first you've

00:13:40,639 --> 00:13:47,059
got to read the data so we chose to do

00:13:44,449 --> 00:13:49,849
this kind of structure of JSON so you

00:13:47,059 --> 00:13:53,689
can have several features possible and

00:13:49,849 --> 00:13:56,689
you can your multiple rows are are in an

00:13:53,689 --> 00:14:01,359
array in fact and in this API we were

00:13:56,689 --> 00:14:06,019
able to score on double values features

00:14:01,359 --> 00:14:09,379
then you have to convert this JSON data

00:14:06,019 --> 00:14:12,049
to data that are will understand so

00:14:09,379 --> 00:14:15,979
thanks to RJ's entire library you will

00:14:12,049 --> 00:14:18,439
convert that to our data frame and then

00:14:15,979 --> 00:14:20,839
you have to load the modal so loads of

00:14:18,439 --> 00:14:23,389
specific libraries according to type of

00:14:20,839 --> 00:14:27,529
your modal in our case it was mg CV

00:14:23,389 --> 00:14:30,169
library and then you have to use our

00:14:27,529 --> 00:14:33,349
function to read the modal to read the

00:14:30,169 --> 00:14:37,579
RDS file ok so now you're ready for

00:14:33,349 --> 00:14:43,459
prediction you only have to to use this

00:14:37,579 --> 00:14:46,220
our code thanks to the gyri library so

00:14:43,459 --> 00:14:49,970
you only have to deploy this code and in

00:14:46,220 --> 00:14:51,799
our there are there is only a function

00:14:49,970 --> 00:14:55,220
that is called predict and that enables

00:14:51,799 --> 00:14:58,729
you to to predict on several several

00:14:55,220 --> 00:15:00,919
modal's any any any type of modal in

00:14:58,729 --> 00:15:03,319
fact so you only have to pass the

00:15:00,919 --> 00:15:06,889
parameter the idea of the modal and the

00:15:03,319 --> 00:15:08,989
new data ok so once you've done that you

00:15:06,889 --> 00:15:13,759
do the same work and convert this data

00:15:08,989 --> 00:15:15,589
frame to JSON output ok so I'll hand

00:15:13,759 --> 00:15:17,739
over to metaphor it's a big data

00:15:15,589 --> 00:15:17,739
approach

00:15:17,750 --> 00:15:23,180
yeah so the rest API was really working

00:15:22,220 --> 00:15:25,310
really great

00:15:23,180 --> 00:15:28,490
we had good results because it was quite

00:15:25,310 --> 00:15:31,940
dynamic we will see we'll talk a little

00:15:28,490 --> 00:15:34,400
bit about the result later to fold as a

00:15:31,940 --> 00:15:36,080
conclusion and so the the second

00:15:34,400 --> 00:15:38,840
approach was more for a batch approach

00:15:36,080 --> 00:15:41,300
where if you have a lot a lot of data to

00:15:38,840 --> 00:15:42,860
to score what if you don't just want to

00:15:41,300 --> 00:15:45,740
score aggregate part of friends but

00:15:42,860 --> 00:15:47,690
maybe every single household maybe every

00:15:45,740 --> 00:15:49,400
single who knows with the internet of in

00:15:47,690 --> 00:15:52,640
every single fridge who knows what they

00:15:49,400 --> 00:15:55,370
want to do next so maybe they have an

00:15:52,640 --> 00:15:57,920
Hadoop cluster so we we thought it was a

00:15:55,370 --> 00:16:01,160
good idea to try to develop the the

00:15:57,920 --> 00:16:04,580
approach and on a Hadoop cluster so

00:16:01,160 --> 00:16:07,040
first as to there's a lot of ways to

00:16:04,580 --> 00:16:11,390
implement to make a communication

00:16:07,040 --> 00:16:12,980
between R and Hadoop in if you are if

00:16:11,390 --> 00:16:14,780
you want to split into two big families

00:16:12,980 --> 00:16:17,900
there's a streaming streaming approach

00:16:14,780 --> 00:16:20,420
which works really well it doesn't take

00:16:17,900 --> 00:16:23,240
a lot to set up it's easy to implement

00:16:20,420 --> 00:16:25,550
but it uses standard input and output

00:16:23,240 --> 00:16:30,080
communicate between your your Hadoop

00:16:25,550 --> 00:16:31,850
jobs and the r the r functions and

00:16:30,080 --> 00:16:33,770
actually we made it work in just a

00:16:31,850 --> 00:16:36,860
couple hours just to find the right the

00:16:33,770 --> 00:16:39,110
right line right here and find the right

00:16:36,860 --> 00:16:41,870
jar in the right type of streaming job

00:16:39,110 --> 00:16:43,490
so what we want you to do to do

00:16:41,870 --> 00:16:44,990
something a little bit more complex

00:16:43,490 --> 00:16:46,760
because we were quite concerned about

00:16:44,990 --> 00:16:48,110
the performance and the stability of the

00:16:46,760 --> 00:16:51,080
approach even though it's probably

00:16:48,110 --> 00:16:54,050
working really fine so another big

00:16:51,080 --> 00:16:54,560
family of approaches that you can do on

00:16:54,050 --> 00:16:57,140
Hadoop's

00:16:54,560 --> 00:16:59,930
were thinking about encapsulation you

00:16:57,140 --> 00:17:04,220
can rewrite a specific MapReduce jobs or

00:16:59,930 --> 00:17:06,410
a specific UDF UDF just just as a

00:17:04,220 --> 00:17:09,829
reminder it's user-defined function it's

00:17:06,410 --> 00:17:12,410
usually something you find in our DBMS

00:17:09,829 --> 00:17:15,560
databases where you can implement your

00:17:12,410 --> 00:17:18,760
your specific function in in your code

00:17:15,560 --> 00:17:23,000
or C++ code and hive and pig actually

00:17:18,760 --> 00:17:25,819
gives you the ability to write UDF so it

00:17:23,000 --> 00:17:28,940
is without better because we have a

00:17:25,819 --> 00:17:30,200
better control on input data

00:17:28,940 --> 00:17:33,350
we'll be sent between our different

00:17:30,200 --> 00:17:35,630
different HDFS file it's going to be

00:17:33,350 --> 00:17:39,560
Java code that runs on Java codes we

00:17:35,630 --> 00:17:42,200
like that and it uses GRI as Isabelle

00:17:39,560 --> 00:17:44,450
told you to communicate between R and

00:17:42,200 --> 00:17:48,650
Java and we started to turn a little bit

00:17:44,450 --> 00:17:52,580
about this library so we went for the

00:17:48,650 --> 00:17:54,110
UDF ugf approach and just really quick

00:17:52,580 --> 00:17:58,160
we won't go too much into too much

00:17:54,110 --> 00:18:01,010
detail about this one the general cycle

00:17:58,160 --> 00:18:03,680
we wanted to achieve was to having hive

00:18:01,010 --> 00:18:06,860
to launch the query with the UDF that

00:18:03,680 --> 00:18:08,720
will be will develop and then the UDF

00:18:06,860 --> 00:18:12,200
first will have to deal with the input

00:18:08,720 --> 00:18:14,090
data see if there's null value see how

00:18:12,200 --> 00:18:17,720
many parameters will be sent into the

00:18:14,090 --> 00:18:19,390
ugf and organize the work for GRI for

00:18:17,720 --> 00:18:23,240
the next step

00:18:19,390 --> 00:18:25,910
ugh GRI part will deal the oscillation

00:18:23,240 --> 00:18:28,250
and send instruction to our for the

00:18:25,910 --> 00:18:30,680
photo scoring and scoring phase then a

00:18:28,250 --> 00:18:33,290
score score the the line that hit

00:18:30,680 --> 00:18:35,990
receives it as an input and then

00:18:33,290 --> 00:18:38,270
although the result goes back undone to

00:18:35,990 --> 00:18:44,830
hive that aggregates hold the result for

00:18:38,270 --> 00:18:48,680
a lot different file on HDFS HDFS to be

00:18:44,830 --> 00:18:51,050
going to more details about ugf on hive

00:18:48,680 --> 00:18:55,970
you can develop two types of UTF a

00:18:51,050 --> 00:18:58,370
simple one which is good well it's also

00:18:55,970 --> 00:19:01,190
a bit more complex one a bit more

00:18:58,370 --> 00:19:03,830
complex to write but which offers you a

00:19:01,190 --> 00:19:06,560
bit more control of your of your data

00:19:03,830 --> 00:19:08,060
they're called generic UDF read some

00:19:06,560 --> 00:19:10,430
somewhere that it's supposed to be even

00:19:08,060 --> 00:19:13,100
better in performance than regular UDF

00:19:10,430 --> 00:19:15,020
so might be even better so it deals

00:19:13,100 --> 00:19:16,580
dynamic number of parameters because

00:19:15,020 --> 00:19:18,260
once again we wanted the approach to be

00:19:16,580 --> 00:19:20,150
general so what if we change the model

00:19:18,260 --> 00:19:22,940
and the number of parameter inside the

00:19:20,150 --> 00:19:24,590
model changes then we we have we want

00:19:22,940 --> 00:19:27,650
the UDF to adapt in this kind of

00:19:24,590 --> 00:19:29,960
situation a better with nil value and

00:19:27,650 --> 00:19:32,120
also it deals constant parameter better

00:19:29,960 --> 00:19:35,210
as well as we will see we we are sending

00:19:32,120 --> 00:19:36,980
constant a constant string in in our ugf

00:19:35,210 --> 00:19:40,790
to organize a little bit of the of the

00:19:36,980 --> 00:19:42,480
work so yeah we recommend to use a

00:19:40,790 --> 00:19:46,950
generic ugf as

00:19:42,480 --> 00:19:50,450
praxis first things first

00:19:46,950 --> 00:19:56,220
setup stays so far if you have a cluster

00:19:50,450 --> 00:19:57,780
with multiple nodes then then then you

00:19:56,220 --> 00:20:00,900
have to set up your environment for each

00:19:57,780 --> 00:20:03,330
each Hadoop node and so first you have

00:20:00,900 --> 00:20:04,830
to install our and our Java and every

00:20:03,330 --> 00:20:07,020
single node you have to install the

00:20:04,830 --> 00:20:09,360
model required libraries which in our

00:20:07,020 --> 00:20:11,960
case would be mg CV so of course if you

00:20:09,360 --> 00:20:15,030
change your model then you'll have to

00:20:11,960 --> 00:20:17,309
reinstall a specific library I guess and

00:20:15,030 --> 00:20:20,640
then yeah I won't go into too much

00:20:17,309 --> 00:20:23,850
detail but there were quite some

00:20:20,640 --> 00:20:25,260
vironment enviable to set up so I down

00:20:23,850 --> 00:20:28,559
the slide if you want to try the

00:20:25,260 --> 00:20:31,020
approach some file that you have to move

00:20:28,559 --> 00:20:35,490
into the Hadoop some Hadoop short file

00:20:31,020 --> 00:20:37,590
some shell library yeah you slugs

00:20:35,490 --> 00:20:39,900
it helped us to save you a lot of time

00:20:37,590 --> 00:20:42,570
because in the setup phase we did a lot

00:20:39,900 --> 00:20:45,720
of try and an error and actually Hadoop

00:20:42,570 --> 00:20:47,700
slugs are great and we there everything

00:20:45,720 --> 00:20:48,720
there and the message are quite explicit

00:20:47,700 --> 00:20:52,549
when there's something that goes wrong

00:20:48,720 --> 00:20:57,000
or any variable that are not set quirky

00:20:52,549 --> 00:20:58,700
if we take the workflows walk through

00:20:57,000 --> 00:21:01,500
classical workflow of the approach back

00:20:58,700 --> 00:21:05,549
then what about this serialize and

00:21:01,500 --> 00:21:06,030
deploy the model of phase as a new

00:21:05,549 --> 00:21:09,090
station

00:21:06,030 --> 00:21:13,080
if we save your model or our model and

00:21:09,090 --> 00:21:15,780
we call it a new model then there's a

00:21:13,080 --> 00:21:19,230
various phase which will put you give

00:21:15,780 --> 00:21:21,590
you the RTS file and we will use the

00:21:19,230 --> 00:21:24,419
Hadoop distributed cache function to

00:21:21,590 --> 00:21:28,140
deploy the model and every single node

00:21:24,419 --> 00:21:30,480
of our cluster and on hive it's it's

00:21:28,140 --> 00:21:33,929
written like that is just a ID file

00:21:30,480 --> 00:21:36,059
model dot add yes and then your file is

00:21:33,929 --> 00:21:41,190
is been distributed into the memory of

00:21:36,059 --> 00:21:45,419
every single node on your cluster for

00:21:41,190 --> 00:21:48,480
the prediction step so just for an

00:21:45,419 --> 00:21:50,580
illustration if you could into much more

00:21:48,480 --> 00:21:53,070
detail about the the model when you

00:21:50,580 --> 00:21:55,200
train the model in our case it was a gap

00:21:53,070 --> 00:21:56,880
model you say okay my model

00:21:55,200 --> 00:22:00,870
formula and I want to predict the

00:21:56,880 --> 00:22:04,230
y-variable as an expression of my first

00:22:00,870 --> 00:22:06,750
variable r1 plus my second variable r2

00:22:04,230 --> 00:22:09,480
and when you want to call prediction you

00:22:06,750 --> 00:22:11,970
want to build a data data frame from

00:22:09,480 --> 00:22:13,850
your from your new data and then you

00:22:11,970 --> 00:22:17,250
call the predict function this is a

00:22:13,850 --> 00:22:19,049
specific on any model prediction model

00:22:17,250 --> 00:22:21,389
any predictive model implements a

00:22:19,049 --> 00:22:23,700
predictor function so it's quite generic

00:22:21,389 --> 00:22:27,779
so you called your project function on

00:22:23,700 --> 00:22:31,200
new model and with your new new data so

00:22:27,779 --> 00:22:34,019
what does EDF do is actually it's going

00:22:31,200 --> 00:22:36,419
just to do the mapping between the

00:22:34,019 --> 00:22:39,360
expected value the expected parameters

00:22:36,419 --> 00:22:42,419
in the our model and the different

00:22:39,360 --> 00:22:43,409
fields from from our hive table we want

00:22:42,419 --> 00:22:46,830
to score

00:22:43,409 --> 00:22:49,950
so basically pretty much it we just take

00:22:46,830 --> 00:22:53,580
the the concentric constant string we

00:22:49,950 --> 00:22:56,519
sent to the to the UDF and we do the

00:22:53,580 --> 00:23:00,659
mapping between f1 and f2 so we say okay

00:22:56,519 --> 00:23:05,309
we built a string r1 equals f1 which is

00:23:00,659 --> 00:23:08,580
a hive hive : and our - we go equals f2

00:23:05,309 --> 00:23:13,740
which is another hive : and then with gr

00:23:08,580 --> 00:23:16,260
I will send the prediction to R so the

00:23:13,740 --> 00:23:19,380
whole hive probably looks like that you

00:23:16,260 --> 00:23:21,990
had with your different libraries are

00:23:19,380 --> 00:23:24,929
you using your in your you gf+ the UDF

00:23:21,990 --> 00:23:26,610
the add file and then the Select query

00:23:24,929 --> 00:23:31,850
looks like looks like that it's very

00:23:26,610 --> 00:23:35,580
simple so now we are going to try to

00:23:31,850 --> 00:23:36,929
have a little demo which will it's not

00:23:35,580 --> 00:23:39,750
going to be as impressive as the other

00:23:36,929 --> 00:23:47,659
one we won't tap in our hand but we will

00:23:39,750 --> 00:23:50,460
still try to do something ok so yeah I

00:23:47,659 --> 00:23:54,630
reduced on my microphone so that I don't

00:23:50,460 --> 00:23:58,710
mess up if we this is a query I'm going

00:23:54,630 --> 00:24:00,779
to stand to hive to to actually query

00:23:58,710 --> 00:24:02,730
the data we have in our our table we

00:24:00,779 --> 00:24:05,879
want to score let's say we want to

00:24:02,730 --> 00:24:09,080
scroll this this table so it's hive so

00:24:05,879 --> 00:24:14,039
hopefully we have a little bit of time

00:24:09,080 --> 00:24:16,879
and if it it's not that fast then I'll

00:24:14,039 --> 00:24:16,879
skip the demo part

00:24:17,539 --> 00:24:22,799
okay so this is how it does take quite a

00:24:20,279 --> 00:24:25,139
long time so it's real high and we did

00:24:22,799 --> 00:24:28,169
developed it on a on a VM and we had

00:24:25,139 --> 00:24:30,149
quite good results or origin so if let's

00:24:28,169 --> 00:24:33,990
say we have we want to call this table

00:24:30,149 --> 00:24:36,330
and our data set being used to train the

00:24:33,990 --> 00:24:40,590
model looked like that we had a time

00:24:36,330 --> 00:24:42,749
which are it's like a time theory the

00:24:40,590 --> 00:24:46,440
idea is the time value and then a second

00:24:42,749 --> 00:24:51,779
other variables that helped us to build

00:24:46,440 --> 00:24:54,690
our model then if we want to watch the

00:24:51,779 --> 00:24:57,330
the query just to just to show you I

00:24:54,690 --> 00:25:01,139
didn't lie to you it looked like that

00:24:57,330 --> 00:25:03,779
then you add your jaw you and you use

00:25:01,139 --> 00:25:07,350
the query for this specific specific

00:25:03,779 --> 00:25:09,990
case so once again the constant string

00:25:07,350 --> 00:25:12,090
indicates which which model you want to

00:25:09,990 --> 00:25:14,399
use which are your data scientist we

00:25:12,090 --> 00:25:18,779
produced a new model which is called a

00:25:14,399 --> 00:25:21,720
zone 1 1 then with with these parameters

00:25:18,779 --> 00:25:23,369
then you just have to say ok my ugf i

00:25:21,720 --> 00:25:26,100
call it with this string function and

00:25:23,369 --> 00:25:32,690
the corresponding variable from my hash

00:25:26,100 --> 00:25:32,690
table so if we send it to hive

00:25:35,759 --> 00:25:43,289
also take a couple seconds but

00:25:40,009 --> 00:25:45,869
apparently it doesn't lunch is MapReduce

00:25:43,289 --> 00:25:48,659
jobs from the father knew new version of

00:25:45,869 --> 00:25:51,240
height when we developed it it wasn't a

00:25:48,659 --> 00:25:53,610
bit holder version of hive so every

00:25:51,240 --> 00:25:55,259
single time we did some some tests we we

00:25:53,610 --> 00:25:58,169
had the typical MapReduce failed with a

00:25:55,259 --> 00:25:58,619
different percentage going on but there

00:25:58,169 --> 00:26:01,830
you go

00:25:58,619 --> 00:26:04,919
I called the the projection on my menu

00:26:01,830 --> 00:26:08,429
data set and the results here shown in a

00:26:04,919 --> 00:26:12,869
projection concern consumption is the

00:26:08,429 --> 00:26:18,379
results of our using the RDS RDS file as

00:26:12,869 --> 00:26:18,379
a other model so that's it for the demo

00:26:22,130 --> 00:26:26,430
so I'm going to make the he that will do

00:26:24,810 --> 00:26:28,310
the conclusion and perspective and

00:26:26,430 --> 00:26:33,240
finish the job

00:26:28,310 --> 00:26:36,360
okay so in fact we realized that these

00:26:33,240 --> 00:26:39,810
two approaches were two complementary

00:26:36,360 --> 00:26:43,650
approaches in fact the rest API is quite

00:26:39,810 --> 00:26:47,790
agile and you score quite fast on a

00:26:43,650 --> 00:26:51,870
small amount of rows but once you you've

00:26:47,790 --> 00:26:56,910
reached the limit of the API the UDF

00:26:51,870 --> 00:27:00,000
i've approach is is really more relevant

00:26:56,910 --> 00:27:04,320
so because it scores really fast on a

00:27:00,000 --> 00:27:07,760
big amount of rows so both both

00:27:04,320 --> 00:27:13,440
approaches are complementary so that is

00:27:07,760 --> 00:27:17,670
what satisfied the client in this this

00:27:13,440 --> 00:27:21,360
project okay so for perspectives we sell

00:27:17,670 --> 00:27:24,300
a lot of quite cool tools in the Berlin

00:27:21,360 --> 00:27:27,330
buzzwords conference so that could be

00:27:24,300 --> 00:27:31,430
really realized to test the general

00:27:27,330 --> 00:27:35,100
models code approach in these tools and

00:27:31,430 --> 00:27:38,490
what about Python so earlier for the

00:27:35,100 --> 00:27:42,000
data science data science process step

00:27:38,490 --> 00:27:43,920
we talked about Python a lot of data

00:27:42,000 --> 00:27:47,220
scientists love Titan there's not only

00:27:43,920 --> 00:27:50,640
are that that enables you to to do

00:27:47,220 --> 00:27:54,840
analytical models so what about it could

00:27:50,640 --> 00:28:00,840
our approach be generalizable to to buy

00:27:54,840 --> 00:28:03,840
them we think that yes because RDS files

00:28:00,840 --> 00:28:07,200
the serialization in our could be done

00:28:03,840 --> 00:28:10,170
with pickle file in Python and for jri

00:28:07,200 --> 00:28:11,580
library we could use our two pi and for

00:28:10,170 --> 00:28:14,730
example for the web service we could

00:28:11,580 --> 00:28:18,870
have used Jango so we we were to think

00:28:14,730 --> 00:28:23,520
that this this approach would would fit

00:28:18,870 --> 00:28:27,300
also in in Python so thank you for your

00:28:23,520 --> 00:28:30,800
attention and now it's time for lunch or

00:28:27,300 --> 00:28:30,800

YouTube URL: https://www.youtube.com/watch?v=GLpvHDVdHSE


