Title: Berlin Buzzwords 2015: Ema Iancuta & Radu Chilom â€“ In-memory data pipeline and warehouse at scale
Publication date: 2015-06-05
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Live demo of building an in-memory data pipeline and data warehouse from a web console with architectural guidelines and lessons learned. The tools and APIs behind are built on top of Spark, Tachyon, Mesos/YARN, SparkSQL and are using our own open-sourced Spark Job Server and Http Spark SQL Rest Service.

Last year Spark emerged as a strong technology candidate for distributed computing at scale, as a Hadoop MapReduce and Hive successor. Tachyon is a very promising young project that provides an in-memory distributed file system that serves a caching layer for sharing datasets across multiple Spark/Hadoop applications. Mesos and YARN are resource managers that ensure a more efficient utilization of the resources in a distributed cluster. Parquet is a columnar storage format that enables storing data and schema in the same file, without the need for an external metastore.

In this talk we will showcase the strengths of all these open source technologies and will share the lessons learned while using them for building an in-memory data pipeline (data ingestion and transformation) and a data warehouse for interactive querying of the in-memory datasets (Spark JVM and Tachyon) using a familiar SQL interface. 

Read more:
https://2015.berlinbuzzwords.de/session/memory-data-pipeline-and-warehouse-scale-using-spark-spark-sql-tachyon-and-parquet

About Ema Iancuta:
https://2015.berlinbuzzwords.de/users/ema-iancuta

About Radu Chilom:
https://2015.berlinbuzzwords.de/users/radu-chilom

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,140 --> 00:00:12,719
um hi my name is a Mayan kusa and

00:00:10,680 --> 00:00:15,179
together with my colleague radical on we

00:00:12,719 --> 00:00:18,270
will present in memory data pipeline and

00:00:15,179 --> 00:00:21,630
warehouse at scale using spark spark

00:00:18,270 --> 00:00:23,699
sequel tachyon and parkette both of us

00:00:21,630 --> 00:00:26,010
we are software engineers at a teacher a

00:00:23,699 --> 00:00:27,990
teacher is a company that provides big

00:00:26,010 --> 00:00:31,949
data analytics and machine learning

00:00:27,990 --> 00:00:34,500
solutions a teacher worked with Hadoop

00:00:31,949 --> 00:00:37,199
for more than six years and then switch

00:00:34,500 --> 00:00:40,170
to spark and so we have been working

00:00:37,199 --> 00:00:42,420
with it for the past two years in our

00:00:40,170 --> 00:00:46,559
free time we build up a big data

00:00:42,420 --> 00:00:49,409
research community that focuses sorry I

00:00:46,559 --> 00:00:51,600
don't know how to that focuses on the

00:00:49,409 --> 00:00:55,400
technical problems that this field has

00:00:51,600 --> 00:00:58,199
and offers of open source solutions

00:00:55,400 --> 00:01:00,869
after this quick intro will describe our

00:00:58,199 --> 00:01:04,530
use case we will speak about how to

00:01:00,869 --> 00:01:07,010
build a data pipeline with spark we will

00:01:04,530 --> 00:01:09,630
speak about our to open source projects

00:01:07,010 --> 00:01:13,080
specialized service and the sparks

00:01:09,630 --> 00:01:15,260
Aquarius service code named Jose there

00:01:13,080 --> 00:01:17,610
will be a few words about Poquette

00:01:15,260 --> 00:01:20,790
briefly go through the benefits of using

00:01:17,610 --> 00:01:25,680
tachyon there will be a too-short ademas

00:01:20,790 --> 00:01:27,690
also so our use case is to build a

00:01:25,680 --> 00:01:31,200
memory data pipeline that will process

00:01:27,690 --> 00:01:34,020
millions of financial transactions that

00:01:31,200 --> 00:01:36,290
would be yours downstream by the data

00:01:34,020 --> 00:01:38,700
scientists in order to detect fraud

00:01:36,290 --> 00:01:42,690
first we will need to ingest the data

00:01:38,700 --> 00:01:46,320
from my stream into tachyon cluster

00:01:42,690 --> 00:01:48,240
backed up by a cluster of HDFS HFS will

00:01:46,320 --> 00:01:51,030
be the underlayer file system for

00:01:48,240 --> 00:01:52,830
tachyon then we will have to transform

00:01:51,030 --> 00:01:55,080
the data in order to fit the

00:01:52,830 --> 00:01:57,659
requirements of our data scientists and

00:01:55,080 --> 00:01:59,880
in the end we will want to build a

00:01:57,659 --> 00:02:03,450
warehouse that will allow to run

00:01:59,880 --> 00:02:05,070
interactive queries with sparks equal in

00:02:03,450 --> 00:02:07,530
the next part of our presentation I will

00:02:05,070 --> 00:02:13,910
letter I do deep dive into the first

00:02:07,530 --> 00:02:16,500
challenges this use case brings up high

00:02:13,910 --> 00:02:18,390
next time going to present to the first

00:02:16,500 --> 00:02:19,230
part of the in-memory data pipeline

00:02:18,390 --> 00:02:21,480
which

00:02:19,230 --> 00:02:23,970
assists in two steps downloading and

00:02:21,480 --> 00:02:26,549
transforming a large amount of data as

00:02:23,970 --> 00:02:29,720
in memory distributed engine we've

00:02:26,549 --> 00:02:32,519
chosen apache spark a few words about it

00:02:29,720 --> 00:02:35,489
spark is a fast in general engine for

00:02:32,519 --> 00:02:37,860
large-scale data processing it was build

00:02:35,489 --> 00:02:39,959
around the concept of our DD which is an

00:02:37,860 --> 00:02:43,250
a beautiful collection of elements

00:02:39,959 --> 00:02:46,890
partitioned across the clusters nodes

00:02:43,250 --> 00:02:50,010
spark has a rich epi consisting in

00:02:46,890 --> 00:02:51,660
eighty high level operators and on top

00:02:50,010 --> 00:02:57,079
of it there is a stack of high-level

00:02:51,660 --> 00:02:57,079
tools including ml lib and spark sequel

00:02:59,030 --> 00:03:05,609
for our use case we want to download the

00:03:02,160 --> 00:03:07,349
data from an external source public s3

00:03:05,609 --> 00:03:10,890
bucket named public financial

00:03:07,349 --> 00:03:13,829
transactions even if we are downloading

00:03:10,890 --> 00:03:16,260
for s3 in this use case are the things

00:03:13,829 --> 00:03:20,129
that I'm going to present apply to other

00:03:16,260 --> 00:03:23,030
external sources so as you can see the

00:03:20,129 --> 00:03:25,919
structure for this bucket is very simple

00:03:23,030 --> 00:03:29,519
the scheme is in the scheme folder and

00:03:25,919 --> 00:03:34,440
the data files are in CSV format and are

00:03:29,519 --> 00:03:36,329
divided into multiple data folders the

00:03:34,440 --> 00:03:38,549
easiest way to get the data into our

00:03:36,329 --> 00:03:42,510
cluster would be to use the method whole

00:03:38,549 --> 00:03:44,940
text file let's park on text offers as

00:03:42,510 --> 00:03:48,030
input for this method we have a regular

00:03:44,940 --> 00:03:50,370
expression containing two wild cards the

00:03:48,030 --> 00:03:52,470
first wild card will match all the folds

00:03:50,370 --> 00:03:55,349
inside all the folders inside the bucket

00:03:52,470 --> 00:03:59,400
and the second wild card will match all

00:03:55,349 --> 00:04:02,340
the files inside the folders in order to

00:03:59,400 --> 00:04:05,669
transform from wild cards into file

00:04:02,340 --> 00:04:08,609
names behind scenes sparklies the files

00:04:05,669 --> 00:04:11,669
metadata this is a problem because

00:04:08,609 --> 00:04:13,950
listing the metadata for a large number

00:04:11,669 --> 00:04:17,789
of files in an external source can take

00:04:13,950 --> 00:04:21,419
a long time so in order to improve this

00:04:17,789 --> 00:04:23,849
we can list the metadata distributed so

00:04:21,419 --> 00:04:27,390
instead of listing all the files on the

00:04:23,849 --> 00:04:31,090
driver as we did before we can now get

00:04:27,390 --> 00:04:33,970
the folders list there paralyzed it

00:04:31,090 --> 00:04:36,250
and get the metadata for the files

00:04:33,970 --> 00:04:42,400
inside each folder in a distributed way

00:04:36,250 --> 00:04:44,860
on multiple cores let's see the code for

00:04:42,400 --> 00:04:47,050
this so as you can see in the first part

00:04:44,860 --> 00:04:50,020
or obtaining the folder list with the

00:04:47,050 --> 00:04:54,580
use of an s3 client and then we are

00:04:50,020 --> 00:04:57,310
paralyzing the list by the way it's

00:04:54,580 --> 00:04:59,919
worth paralyzing only if the folder list

00:04:57,310 --> 00:05:02,139
is greater than 1 because if you have

00:04:59,919 --> 00:05:04,900
only one folder then it will only move

00:05:02,139 --> 00:05:09,820
the computation from the driver to one

00:05:04,900 --> 00:05:12,790
of the workers ok and then with a flat

00:05:09,820 --> 00:05:16,840
map we're going to get to go to list all

00:05:12,790 --> 00:05:20,050
the files from the folders by default

00:05:16,840 --> 00:05:23,740
the Paralyzed method creates a number of

00:05:20,050 --> 00:05:27,729
partitions in our cluster that depends

00:05:23,740 --> 00:05:32,020
on your cluster manager however in order

00:05:27,729 --> 00:05:34,479
to fine-tune your jobs you can specify

00:05:32,020 --> 00:05:37,570
the number of partitions by adding an

00:05:34,479 --> 00:05:41,590
extra integral parameter to the method

00:05:37,570 --> 00:05:44,440
call as a tip you will probably want to

00:05:41,590 --> 00:05:47,500
have the number of partitions as least

00:05:44,440 --> 00:05:49,150
as the number of available course in

00:05:47,500 --> 00:05:53,770
order to fully benefit from our

00:05:49,150 --> 00:05:55,800
computation power ok the dell the

00:05:53,770 --> 00:05:58,419
download part is pretty straightforward

00:05:55,800 --> 00:06:01,090
we're doing a map in which were

00:05:58,419 --> 00:06:03,849
initializing an s3 client and we're

00:06:01,090 --> 00:06:06,030
downloading the file the problem that

00:06:03,849 --> 00:06:10,479
might arise here is if you have

00:06:06,030 --> 00:06:13,389
unbalanced partitions so we can end up

00:06:10,479 --> 00:06:15,700
in a situation similar to this where the

00:06:13,389 --> 00:06:18,729
data is not evenly distributed among

00:06:15,700 --> 00:06:20,860
partitions this happens because the

00:06:18,729 --> 00:06:23,860
default partitioner is a hash petitioner

00:06:20,860 --> 00:06:27,180
and for the string data type it doesn't

00:06:23,860 --> 00:06:31,900
always provide an even distribution as

00:06:27,180 --> 00:06:34,000
you may know each part the elements in

00:06:31,900 --> 00:06:36,490
each partition in spark are processed by

00:06:34,000 --> 00:06:39,940
a separate task so the real problem is

00:06:36,490 --> 00:06:41,830
unbalanced partitions are that while

00:06:39,940 --> 00:06:43,639
some of the tasks finish very very

00:06:41,830 --> 00:06:47,180
quickly the rest of the

00:06:43,639 --> 00:06:49,969
of the tasks struggle to finish this

00:06:47,180 --> 00:06:54,529
leads to an increased overall time for

00:06:49,969 --> 00:06:58,939
your stage okay a solution for this

00:06:54,529 --> 00:07:03,199
would be to rebalance our partitions by

00:06:58,939 --> 00:07:04,819
hashing on an integral index we can

00:07:03,199 --> 00:07:08,599
achieve this by transforming the

00:07:04,819 --> 00:07:13,900
filename rdd into a parody be that would

00:07:08,599 --> 00:07:17,360
have as a key a unique index attached

00:07:13,900 --> 00:07:19,669
okay so now let's see the code for this

00:07:17,360 --> 00:07:23,210
as you can see we can achieve this with

00:07:19,669 --> 00:07:28,159
the Zipit index operator and what this

00:07:23,210 --> 00:07:31,939
does is that it adds to each element of

00:07:28,159 --> 00:07:34,039
our rdd a unique index then we have to

00:07:31,939 --> 00:07:37,759
use a map operator in order to

00:07:34,039 --> 00:07:41,680
interchange the key and the value and so

00:07:37,759 --> 00:07:44,870
the index ends up being the pair ski

00:07:41,680 --> 00:07:47,689
after that we want to repartition our

00:07:44,870 --> 00:07:51,139
rdd in order to shuffle the data around

00:07:47,689 --> 00:07:54,889
and balance the partitions please keep

00:07:51,139 --> 00:07:58,039
in mind that repartitioning your data is

00:07:54,889 --> 00:08:00,979
a fairly expensive operation because it

00:07:58,039 --> 00:08:03,169
shuffles data across the cluster however

00:08:00,979 --> 00:08:05,810
in some use cases the benefit of

00:08:03,169 --> 00:08:12,289
repartitioning is much more greater than

00:08:05,810 --> 00:08:15,860
the penalty it inflicts now let's move

00:08:12,289 --> 00:08:18,020
to the data transformation step data

00:08:15,860 --> 00:08:21,050
cleaning is the first step in any data

00:08:18,020 --> 00:08:23,960
science project in our use case we have

00:08:21,050 --> 00:08:26,509
to remove the lines that don't match the

00:08:23,960 --> 00:08:31,099
structure and some of the columns that

00:08:26,509 --> 00:08:33,289
are useless for our analysis another

00:08:31,099 --> 00:08:36,349
thing we want to do is transform the

00:08:33,289 --> 00:08:40,039
data in order to met to have a

00:08:36,349 --> 00:08:43,279
consistent format so we have two columns

00:08:40,039 --> 00:08:45,290
both representing country codes the

00:08:43,279 --> 00:08:48,110
problem is that one is in numeric format

00:08:45,290 --> 00:08:52,310
and the other one is in alpha numeric

00:08:48,110 --> 00:08:55,660
format so in order to be consistent we

00:08:52,310 --> 00:09:00,240
can join the transactions RDD with

00:08:55,660 --> 00:09:00,240
country's my part d which is this and

00:09:00,300 --> 00:09:06,069
change the value from the transaction

00:09:02,949 --> 00:09:09,100
from numeric to alphanumeric the

00:09:06,069 --> 00:09:13,689
problems with joints in spark and also i

00:09:09,100 --> 00:09:15,750
think when doing hive arise when you

00:09:13,689 --> 00:09:19,360
have asked you in your key distribution

00:09:15,750 --> 00:09:21,819
so as you might know all the values for

00:09:19,360 --> 00:09:25,509
a for a key he'll have to be processed

00:09:21,819 --> 00:09:27,790
in the same task so if a key is very

00:09:25,509 --> 00:09:30,459
frequent much more frequent than the

00:09:27,790 --> 00:09:33,689
others the task that will process that

00:09:30,459 --> 00:09:37,360
key will take much more time to finish

00:09:33,689 --> 00:09:40,209
you can spot the SKU key distributions

00:09:37,360 --> 00:09:42,790
by looking at the summary metrics when

00:09:40,209 --> 00:09:45,220
running an app an action on top of your

00:09:42,790 --> 00:09:47,350
joint transformation so as you can see

00:09:45,220 --> 00:09:49,959
here while seventy percent of the task

00:09:47,350 --> 00:09:54,639
finish in under two seconds the maximum

00:09:49,959 --> 00:09:57,040
task took 17 seconds also you may want

00:09:54,639 --> 00:09:58,899
to look into the shuffle redfield and

00:09:57,040 --> 00:10:02,350
you can see that there are very large

00:09:58,899 --> 00:10:04,300
differences so when you can see this

00:10:02,350 --> 00:10:06,699
kind of differences in a joint or in a

00:10:04,300 --> 00:10:11,769
group by it's very probable that you

00:10:06,699 --> 00:10:14,949
have a skewed key distribution we can

00:10:11,769 --> 00:10:18,790
achieve this transformation by also by

00:10:14,949 --> 00:10:22,689
shipping our country code map to each

00:10:18,790 --> 00:10:25,240
worker in our cluster we can do this

00:10:22,689 --> 00:10:29,290
with a broadcast operator which takes

00:10:25,240 --> 00:10:34,660
your value and send it to each node of

00:10:29,290 --> 00:10:37,089
the cluster this method is suitable only

00:10:34,660 --> 00:10:39,399
if the data you are shipping to the

00:10:37,089 --> 00:10:43,180
cluster it's not that big because it

00:10:39,399 --> 00:10:45,459
inflicts serious IL penalties as you can

00:10:43,180 --> 00:10:47,370
see in the code after broadcasting the

00:10:45,459 --> 00:10:50,019
country's map we can achieve our

00:10:47,370 --> 00:10:56,259
transformation with a simple map or flat

00:10:50,019 --> 00:10:58,779
map in which change the value now let's

00:10:56,259 --> 00:11:00,790
take a look at the matrix from running

00:10:58,779 --> 00:11:03,399
an action on top of this transformation

00:11:00,790 --> 00:11:07,509
as you can see the running times are now

00:11:03,399 --> 00:11:08,830
very close in range and we can also say

00:11:07,509 --> 00:11:12,640
that about the input

00:11:08,830 --> 00:11:15,220
so yeah when each task process is the

00:11:12,640 --> 00:11:17,580
same amount the duration is only seven

00:11:15,220 --> 00:11:20,380
seconds per task instead of having

00:11:17,580 --> 00:11:25,080
multiple tasks that finish in two

00:11:20,380 --> 00:11:25,080
seconds and one that runs in 17 seconds

00:11:25,290 --> 00:11:32,470
now I want to show your chart with the

00:11:28,630 --> 00:11:34,890
running times from run from running with

00:11:32,470 --> 00:11:37,570
the running times of the both presented

00:11:34,890 --> 00:11:40,450
transformation versions as you can see

00:11:37,570 --> 00:11:43,990
the broadcasted Mac version is much more

00:11:40,450 --> 00:11:48,490
faster and it looks linear while the

00:11:43,990 --> 00:11:50,770
joint version looks exponential however

00:11:48,490 --> 00:11:55,270
this only applies when you have skilled

00:11:50,770 --> 00:11:57,690
key distribution now that we have

00:11:55,270 --> 00:12:00,190
implemented a downloading job and

00:11:57,690 --> 00:12:02,650
transformation one we have to run them

00:12:00,190 --> 00:12:05,650
we can run them on top of our job rest

00:12:02,650 --> 00:12:07,840
which is a restful server that provides

00:12:05,650 --> 00:12:12,070
context management and job submission

00:12:07,840 --> 00:12:15,580
services spur job rest is an open source

00:12:12,070 --> 00:12:17,850
server which we created in order to fix

00:12:15,580 --> 00:12:21,730
the inability to run multiple spark

00:12:17,850 --> 00:12:24,670
context in the same jvm this is a spark

00:12:21,730 --> 00:12:27,190
or bug that still hasn't been fixed and

00:12:24,670 --> 00:12:29,290
that also affects we all as per job

00:12:27,190 --> 00:12:32,980
server from each by the way we've

00:12:29,290 --> 00:12:35,860
started but facing the urge to run on

00:12:32,980 --> 00:12:40,090
multiple contexts we came up with this

00:12:35,860 --> 00:12:44,620
server that as a solution spins up a new

00:12:40,090 --> 00:12:49,110
process for each context it creates i'm

00:12:44,620 --> 00:12:49,110
going to show you the quick demo now

00:12:56,530 --> 00:13:03,160
okay so this is the UI for spar job rest

00:12:59,680 --> 00:13:06,980
it context in it has three sections

00:13:03,160 --> 00:13:09,080
context jobs and jars in the context tab

00:13:06,980 --> 00:13:12,620
you can see all the running context on

00:13:09,080 --> 00:13:15,470
your cluster you can create one or you

00:13:12,620 --> 00:13:20,090
can delete one a very nice feature is

00:13:15,470 --> 00:13:23,420
that we have managed to spark you I port

00:13:20,090 --> 00:13:25,430
in order to show it here and also in

00:13:23,420 --> 00:13:27,710
order to give you a quick link to the

00:13:25,430 --> 00:13:33,760
spark you I page where you can see all

00:13:27,710 --> 00:13:36,590
the tasks that have run on your context

00:13:33,760 --> 00:13:39,500
in order to create a context you have to

00:13:36,590 --> 00:13:41,690
provide a jar with your classes so you

00:13:39,500 --> 00:13:45,110
can go to the jar section and upload

00:13:41,690 --> 00:13:47,420
your jar as you can see here I have to

00:13:45,110 --> 00:13:49,790
upload it jars one of them is s3

00:13:47,420 --> 00:13:55,760
download job I'm going to create a

00:13:49,790 --> 00:13:57,350
context and run that job okay so in

00:13:55,760 --> 00:14:00,410
order to create the context you should

00:13:57,350 --> 00:14:07,910
specify a context name let's say data

00:14:00,410 --> 00:14:09,530
download context a jar a nice cool

00:14:07,910 --> 00:14:11,780
feature is that here you can add

00:14:09,530 --> 00:14:14,600
multiple jars and also you can provide

00:14:11,780 --> 00:14:16,700
HDFS path so if you have your jar in

00:14:14,600 --> 00:14:20,720
HDFS you don't need to download it and

00:14:16,700 --> 00:14:23,840
upload it to the server in the parameter

00:14:20,720 --> 00:14:25,670
section you can specify any spark

00:14:23,840 --> 00:14:28,250
parameters that you want to be set on

00:14:25,670 --> 00:14:34,990
your context so for example we can set

00:14:28,250 --> 00:14:44,110
the spark executor memory to be four

00:14:34,990 --> 00:14:44,110
gigabytes so now okay an error

00:14:53,480 --> 00:15:00,530
yeah I misspelled park executor here

00:15:00,560 --> 00:15:11,340
four gigabytes okay so when the server

00:15:09,240 --> 00:15:13,830
received is request it will spin on you

00:15:11,340 --> 00:15:16,170
processed and on that process it will

00:15:13,830 --> 00:15:20,190
initialize a spark context as you can

00:15:16,170 --> 00:15:23,160
see the context was created now we can

00:15:20,190 --> 00:15:27,060
go to the job section here you can see

00:15:23,160 --> 00:15:29,940
all the jobs that were finished or that

00:15:27,060 --> 00:15:32,400
are still running on your context a nice

00:15:29,940 --> 00:15:34,830
cool feature is that you can see the

00:15:32,400 --> 00:15:39,210
error directly here if it failed so you

00:15:34,830 --> 00:15:41,520
don't have to go to the log files also

00:15:39,210 --> 00:15:43,860
you can see the output of the jobs this

00:15:41,520 --> 00:15:48,750
is the output for the normalization job

00:15:43,860 --> 00:15:50,610
and now let's run the download job so

00:15:48,750 --> 00:15:56,010
you have to specify the full name of

00:15:50,610 --> 00:15:59,970
your class you have to choose your

00:15:56,010 --> 00:16:03,270
context and here you can enter any

00:15:59,970 --> 00:16:06,060
parameter that you want in order to

00:16:03,270 --> 00:16:07,980
customize your job so the s3 download

00:16:06,060 --> 00:16:12,480
job was created with three parameters

00:16:07,980 --> 00:16:16,500
the first parameter is the pocket we

00:16:12,480 --> 00:16:25,650
want to clone so yes three bucket it's

00:16:16,500 --> 00:16:27,690
public the second parameter would be the

00:16:25,650 --> 00:16:34,800
number of partitions in which we want

00:16:27,690 --> 00:16:36,600
our data to be paralyzed and the last

00:16:34,800 --> 00:16:39,090
parameter would be the output for this

00:16:36,600 --> 00:16:43,350
job but I didn't mention anything about

00:16:39,090 --> 00:16:46,170
this until now because we want to stay

00:16:43,350 --> 00:16:48,510
in memory we want to put our files in

00:16:46,170 --> 00:16:50,940
memory so i will write to tachyon

00:16:48,510 --> 00:16:53,610
tachyon is the in-memory file system

00:16:50,940 --> 00:16:55,970
about which my colleague Emma we'll talk

00:16:53,610 --> 00:16:55,970
later

00:16:59,490 --> 00:17:05,730
so I have to provide the valiant valid

00:17:02,459 --> 00:17:12,559
tachyon path this is similar to HDFS you

00:17:05,730 --> 00:17:22,740
have to specify the master the port and

00:17:12,559 --> 00:17:24,750
the path where to write ok now let's run

00:17:22,740 --> 00:17:27,540
the job as you can see the job have

00:17:24,750 --> 00:17:31,760
started its running we can go in the

00:17:27,540 --> 00:17:34,470
context tab and into the spark you I and

00:17:31,760 --> 00:17:39,809
here we can see the stages that I run on

00:17:34,470 --> 00:17:42,660
top of our our context as I've said you

00:17:39,809 --> 00:17:46,370
can go here on a stage and see the

00:17:42,660 --> 00:17:51,620
summary metrics for your for your stages

00:17:46,370 --> 00:17:54,840
ok now if you don't like to use the UI

00:17:51,620 --> 00:17:57,809
spur job rest also offers an HTTP client

00:17:54,840 --> 00:18:00,929
which provides an abstraction on top of

00:17:57,809 --> 00:18:03,059
the of the rest for later so you can go

00:18:00,929 --> 00:18:07,980
and write code in order to execute our

00:18:03,059 --> 00:18:10,320
pipelines oh now that the first part of

00:18:07,980 --> 00:18:12,750
the pipeline is over I will let Emma

00:18:10,320 --> 00:18:15,350
continue it creating an in-memory data

00:18:12,750 --> 00:18:15,350
warehouse

00:18:31,510 --> 00:18:39,400
okay so there are a lot of sparks or a

00:18:36,430 --> 00:18:41,920
lot of sequel engines that can run on

00:18:39,400 --> 00:18:44,140
top of data stored on ETFs I could have

00:18:41,920 --> 00:18:46,840
could have used them all but I choose

00:18:44,140 --> 00:18:50,350
Park sequel i chose particle because

00:18:46,840 --> 00:18:52,470
we're great fans of spark and because we

00:18:50,350 --> 00:18:56,260
invested in an open source project that

00:18:52,470 --> 00:18:58,810
submits queries on top of it for those

00:18:56,260 --> 00:19:02,710
of you who don't know spark sequel is

00:18:58,810 --> 00:19:05,560
one of sparks modules that is used for

00:19:02,710 --> 00:19:07,660
processing structured data spark

00:19:05,560 --> 00:19:11,050
sequence supports several input formats

00:19:07,660 --> 00:19:15,010
like files stored in Burkett or JSON or

00:19:11,050 --> 00:19:18,250
the data stored in hive tables where the

00:19:15,010 --> 00:19:22,240
actual data laid resides on a GF s and

00:19:18,250 --> 00:19:24,340
the schema is in the hive meta store it

00:19:22,240 --> 00:19:27,490
also supports as an input already

00:19:24,340 --> 00:19:30,190
existing rd DS or data stored in

00:19:27,490 --> 00:19:32,800
external tables like Cassandra for

00:19:30,190 --> 00:19:37,360
example data that it's retrieved through

00:19:32,800 --> 00:19:39,930
JT be seen sparks equal organizes its

00:19:37,360 --> 00:19:43,690
the data into distributed collections

00:19:39,930 --> 00:19:45,820
called data frames or the former schema

00:19:43,690 --> 00:19:47,620
LEDs for those of you who are more

00:19:45,820 --> 00:19:51,400
familiar with previous versions of

00:19:47,620 --> 00:19:54,010
scoffs part we can work with it either

00:19:51,400 --> 00:19:57,040
writing code in Java in scale our Python

00:19:54,010 --> 00:20:00,870
or we can execute sequel statements on

00:19:57,040 --> 00:20:03,700
top of the sparks equal context okay so

00:20:00,870 --> 00:20:07,050
the data of your of our pipeline until

00:20:03,700 --> 00:20:09,820
now was exported in CSV format in

00:20:07,050 --> 00:20:12,370
tachyon in in memory so in order to

00:20:09,820 --> 00:20:15,040
query it we will have to create that

00:20:12,370 --> 00:20:18,070
obstruct ization that data frame that i

00:20:15,040 --> 00:20:23,950
mentioned before and in order to do that

00:20:18,070 --> 00:20:27,310
we first need to have we first need to

00:20:23,950 --> 00:20:29,170
have a spark sequel context and we will

00:20:27,310 --> 00:20:32,590
use this context in order to read the

00:20:29,170 --> 00:20:34,930
schema schema I said that it's comma

00:20:32,590 --> 00:20:37,960
separated so we will read the file from

00:20:34,930 --> 00:20:40,570
memory split it after coma and then map

00:20:37,960 --> 00:20:44,230
each field into a struct field that will

00:20:40,570 --> 00:20:45,200
create the struct type containing our

00:20:44,230 --> 00:20:47,840
schema

00:20:45,200 --> 00:20:50,240
the data will be writing the same in the

00:20:47,840 --> 00:20:53,389
same way we read the data file split it

00:20:50,240 --> 00:20:56,450
after coma and then map Ichiro inside

00:20:53,389 --> 00:20:59,179
your data file into a spark sequel at

00:20:56,450 --> 00:21:02,269
all that will be stored in the in an RDD

00:20:59,179 --> 00:21:04,669
containing rose so now that we have the

00:21:02,269 --> 00:21:07,159
schema created and we have the robot DD

00:21:04,669 --> 00:21:10,010
created we can create the data frame

00:21:07,159 --> 00:21:13,549
that will use to query to have a look

00:21:10,010 --> 00:21:15,710
inside our data so in order to explore

00:21:13,549 --> 00:21:21,110
it we can perform simple queries either

00:21:15,710 --> 00:21:23,510
directly using the methods built-in upon

00:21:21,110 --> 00:21:26,690
a data frame here is an example of a

00:21:23,510 --> 00:21:29,740
group by or you can register your data

00:21:26,690 --> 00:21:34,429
frame into a temporary table and then

00:21:29,740 --> 00:21:40,130
execute sequels sequel commands on top

00:21:34,429 --> 00:21:43,460
of it however this it's not is not that

00:21:40,130 --> 00:21:46,220
easy to do so you won't want to map your

00:21:43,460 --> 00:21:49,070
data into a data frame and then register

00:21:46,220 --> 00:21:51,740
it as a table each time you query each

00:21:49,070 --> 00:21:53,210
time your spark sequel context dies all

00:21:51,740 --> 00:21:56,570
you have when you you create a new

00:21:53,210 --> 00:21:59,269
session so what would you like to do is

00:21:56,570 --> 00:22:01,850
to persist your tables somehow in order

00:21:59,269 --> 00:22:05,600
to share them across sessions you have

00:22:01,850 --> 00:22:08,450
several ways to do that either creating

00:22:05,600 --> 00:22:10,880
a hive table executing a sequel

00:22:08,450 --> 00:22:13,309
statement on top of hive context as you

00:22:10,880 --> 00:22:15,350
can see here I have a hive context this

00:22:13,309 --> 00:22:17,389
actually extends the sparks equal

00:22:15,350 --> 00:22:19,880
context and adds the functionality of

00:22:17,389 --> 00:22:23,600
operating with tables with the data

00:22:19,880 --> 00:22:26,870
stored in hive tables another way to to

00:22:23,600 --> 00:22:30,320
share the tables between sessions is to

00:22:26,870 --> 00:22:32,960
save your data frame as a table like

00:22:30,320 --> 00:22:35,149
here what does this mean you're that

00:22:32,960 --> 00:22:36,740
you're a table will be created the

00:22:35,149 --> 00:22:39,440
sparks equal table will be created

00:22:36,740 --> 00:22:43,010
having the schema stored in the hive

00:22:39,440 --> 00:22:45,889
meta store okay but we're building

00:22:43,010 --> 00:22:48,080
in-memory data pipeline so the ingestion

00:22:45,889 --> 00:22:50,179
was in memory the transformations were

00:22:48,080 --> 00:22:53,269
in memory so what I would like to do is

00:22:50,179 --> 00:22:56,600
that my warehouse works with files

00:22:53,269 --> 00:22:58,580
stored in memory what I like to do is

00:22:56,600 --> 00:23:01,100
save the data frame into

00:22:58,580 --> 00:23:03,409
a pocket format inside tachyon if

00:23:01,100 --> 00:23:06,500
possible so I will be tempted to do this

00:23:03,409 --> 00:23:10,220
in the following way however this won't

00:23:06,500 --> 00:23:12,289
work it won't work if the driver the

00:23:10,220 --> 00:23:15,559
spark driver it's on a machine that

00:23:12,289 --> 00:23:18,529
doesn't have a tachyon worker it will

00:23:15,559 --> 00:23:22,640
fail during the schema during writing

00:23:18,529 --> 00:23:25,640
the schema I into into the pocket of the

00:23:22,640 --> 00:23:29,029
packet file a colleague of ours open

00:23:25,640 --> 00:23:33,460
sourced an API that fixes this issue by

00:23:29,029 --> 00:23:35,929
sending the code that executes the

00:23:33,460 --> 00:23:37,519
railroad that writes the ski mind to

00:23:35,929 --> 00:23:40,460
park at file in tachyon to the workers

00:23:37,519 --> 00:23:42,350
so in the job I have created for

00:23:40,460 --> 00:23:44,630
exporting the files to talk you and I

00:23:42,350 --> 00:23:47,059
have used his library you can take it

00:23:44,630 --> 00:23:49,730
from github or you can download it from

00:23:47,059 --> 00:23:54,350
from the maple may want public reckons

00:23:49,730 --> 00:23:57,320
it's published okay there are several

00:23:54,350 --> 00:23:59,990
file formats that we have used for

00:23:57,320 --> 00:24:03,130
different pipelines however we favor

00:23:59,990 --> 00:24:05,929
parkett because being a columnar data

00:24:03,130 --> 00:24:07,820
representation format speeds up the

00:24:05,929 --> 00:24:10,159
aggregation queries so for the

00:24:07,820 --> 00:24:14,389
long-running query also if so for query

00:24:10,159 --> 00:24:18,559
queries that required only some columns

00:24:14,389 --> 00:24:21,950
but there will be read only the required

00:24:18,559 --> 00:24:24,320
ones and not the entire row what's cool

00:24:21,950 --> 00:24:28,669
about it is that it supports nested

00:24:24,320 --> 00:24:30,440
sorry nested data types and the schema

00:24:28,669 --> 00:24:33,289
resides with the data so you won't need

00:24:30,440 --> 00:24:36,289
an external meta store to store the

00:24:33,289 --> 00:24:39,080
schema for the files also working with

00:24:36,289 --> 00:24:41,990
parkett and will spark sequel it's nice

00:24:39,080 --> 00:24:44,630
because it supports schema evolution so

00:24:41,990 --> 00:24:46,730
if you have some files in park at format

00:24:44,630 --> 00:24:49,880
having a schema and then you add another

00:24:46,730 --> 00:24:52,730
file that adds a new field then spark

00:24:49,880 --> 00:24:56,600
sequel will know to merge those schemas

00:24:52,730 --> 00:24:59,090
together the most important feature

00:24:56,600 --> 00:25:01,700
though that I like about parkette is

00:24:59,090 --> 00:25:04,039
that supports efficient compression and

00:25:01,700 --> 00:25:08,270
encoding schemes as they can be

00:25:04,039 --> 00:25:09,770
specified per column and if there is

00:25:08,270 --> 00:25:11,650
something that I have learned while

00:25:09,770 --> 00:25:14,770
working with files in

00:25:11,650 --> 00:25:17,710
is that the footprint of your data it's

00:25:14,770 --> 00:25:21,130
more important than the compression and

00:25:17,710 --> 00:25:23,770
the compression speed because spilling

00:25:21,130 --> 00:25:26,050
files to disc it will always be more

00:25:23,770 --> 00:25:29,290
expensive than a slower the compression

00:25:26,050 --> 00:25:31,570
algorithm of course it depends on how

00:25:29,290 --> 00:25:33,670
large your data set is and how many

00:25:31,570 --> 00:25:36,100
memory you have on your machines it's

00:25:33,670 --> 00:25:41,560
something you will have to balance this

00:25:36,100 --> 00:25:44,740
battle between the disk i/o and CPUs ha

00:25:41,560 --> 00:25:48,610
ok I have mentioned tachyon tachyon it's

00:25:44,740 --> 00:25:51,340
a memory file system and what it's

00:25:48,610 --> 00:25:53,950
really cool about it it's that is not

00:25:51,340 --> 00:25:56,110
going it's not going to disk each time

00:25:53,950 --> 00:25:59,050
you want to read some data that you

00:25:56,110 --> 00:26:02,500
frequently use it avoids this by keeping

00:25:59,050 --> 00:26:04,660
it keeping the data in memory and the

00:26:02,500 --> 00:26:07,930
fact that the data is in memory makes it

00:26:04,660 --> 00:26:10,660
possible for tachyon to allow different

00:26:07,930 --> 00:26:13,120
frameworks running on the cluster to

00:26:10,660 --> 00:26:18,010
access the same data set with the

00:26:13,120 --> 00:26:21,370
in-memory speed tae-hyun spills the data

00:26:18,010 --> 00:26:24,190
also to disk into a pluggable underlayer

00:26:21,370 --> 00:26:25,990
file system that could be either HDFS or

00:26:24,190 --> 00:26:30,610
s3 I don't know blaster office whatever

00:26:25,990 --> 00:26:32,740
fits you best there is another way to

00:26:30,610 --> 00:26:35,950
keep your data in memory you could use

00:26:32,740 --> 00:26:39,280
Park cash for that you could cash or

00:26:35,950 --> 00:26:41,550
tables either calling the cash method on

00:26:39,280 --> 00:26:44,020
top of your sequel context either

00:26:41,550 --> 00:26:46,960
calling the cash method on your data

00:26:44,020 --> 00:26:49,780
frame or executing on standard sequel

00:26:46,960 --> 00:26:51,280
cash table comment on your context it

00:26:49,780 --> 00:26:54,700
doesn't matter which of these methods

00:26:51,280 --> 00:26:57,280
you choose because Park will catch your

00:26:54,700 --> 00:27:00,790
data in a columnar format in memory and

00:26:57,280 --> 00:27:03,070
will fine-tune the compression in order

00:27:00,790 --> 00:27:07,360
to to minimize the pressure on your

00:27:03,070 --> 00:27:11,140
memory so if we have spark cash why

00:27:07,360 --> 00:27:13,450
using tachyon well there are a set of

00:27:11,140 --> 00:27:16,660
reason why we like it it's because

00:27:13,450 --> 00:27:19,930
sometimes power context my crash and it

00:27:16,660 --> 00:27:22,260
does if it does and if it does you will

00:27:19,930 --> 00:27:24,120
lose your data from memory and

00:27:22,260 --> 00:27:26,700
this means that you will have to upload

00:27:24,120 --> 00:27:28,410
to cash once again your your table or

00:27:26,700 --> 00:27:31,890
you will have to trigger once again your

00:27:28,410 --> 00:27:33,750
transformations and for large data sets

00:27:31,890 --> 00:27:36,870
it's something that you won't want to do

00:27:33,750 --> 00:27:39,950
so using tae-hyun you avoid this because

00:27:36,870 --> 00:27:44,910
the data will be outside or jvm and

00:27:39,950 --> 00:27:49,860
sorry outsider jvn so you won't lose it

00:27:44,910 --> 00:27:52,740
when the spark JVM will die generally

00:27:49,860 --> 00:27:55,440
speaking spark cash will outperform

00:27:52,740 --> 00:27:57,480
tachyon and this is because while

00:27:55,440 --> 00:28:00,480
working with with tyonne there is a time

00:27:57,480 --> 00:28:02,430
penalty inflicted by i don't know the

00:28:00,480 --> 00:28:05,460
serialization or the serialization or

00:28:02,430 --> 00:28:09,090
the calls between the two je viens but

00:28:05,460 --> 00:28:12,180
for long running queries spark cash will

00:28:09,090 --> 00:28:14,580
encounter GC and if it does your queries

00:28:12,180 --> 00:28:17,880
will be slower this is something that

00:28:14,580 --> 00:28:20,730
you won't you won't face if you're using

00:28:17,880 --> 00:28:24,180
tachyon because you're our DD we will be

00:28:20,730 --> 00:28:28,140
stored of hip it will be in there in RAM

00:28:24,180 --> 00:28:31,170
disk so in these situations we have seen

00:28:28,140 --> 00:28:33,960
tae-hyun outperform spark cash but only

00:28:31,170 --> 00:28:36,300
I have to to say it once again only when

00:28:33,960 --> 00:28:39,060
garbage collection kicks in and this

00:28:36,300 --> 00:28:40,770
usually happens when your data is too

00:28:39,060 --> 00:28:46,620
large and there is a pressure on your

00:28:40,770 --> 00:28:48,720
memory the most important thing why why

00:28:46,620 --> 00:28:51,030
we use tae-hyun is what I said before

00:28:48,720 --> 00:28:54,300
that the data can be shared between

00:28:51,030 --> 00:28:56,460
applications with the in-memory speed so

00:28:54,300 --> 00:28:58,350
you won't have to build a huge

00:28:56,460 --> 00:29:01,020
application to do them all you can have

00:28:58,350 --> 00:29:03,570
micro services or micro apps to do

00:29:01,020 --> 00:29:08,280
several things and share the same data

00:29:03,570 --> 00:29:10,230
with the memory speed okay next I

00:29:08,280 --> 00:29:13,140
represent one contribution we have made

00:29:10,230 --> 00:29:16,110
to the spark echo system it sparked your

00:29:13,140 --> 00:29:18,660
breasts it's a data warehouse that

00:29:16,110 --> 00:29:21,540
offers the possibility to concurrently

00:29:18,660 --> 00:29:24,180
and I synchronously submit spark sequel

00:29:21,540 --> 00:29:27,540
queries actually it's an it's an

00:29:24,180 --> 00:29:30,210
alternative to spark sequel jdbc but it

00:29:27,540 --> 00:29:32,730
has an interactive UI and it has a set

00:29:30,210 --> 00:29:35,020
of features that i will i will demo

00:29:32,730 --> 00:29:37,870
later on

00:29:35,020 --> 00:29:42,310
Jose has quite a history I have been

00:29:37,870 --> 00:29:46,090
working on it since Park 091 back then

00:29:42,310 --> 00:29:48,550
it worked on on top of shark now its

00:29:46,090 --> 00:29:52,930
support sparks equal and you also can

00:29:48,550 --> 00:29:55,240
run hive queries on hadoop mapreduce in

00:29:52,930 --> 00:29:58,630
future we plan to support some other

00:29:55,240 --> 00:30:01,870
sequel engines that run on on top of

00:29:58,630 --> 00:30:07,750
data stored on HDFS this is the github

00:30:01,870 --> 00:30:11,140
link ok for resilience reasons you might

00:30:07,750 --> 00:30:13,720
want to deploy Jose on several instances

00:30:11,140 --> 00:30:16,870
and put it under the same load balancer

00:30:13,720 --> 00:30:19,180
and thanks to the ARCA actor system

00:30:16,870 --> 00:30:21,640
these instances can communicate through

00:30:19,180 --> 00:30:23,410
each other which makes possible a nice

00:30:21,640 --> 00:30:27,070
feature that we have like query

00:30:23,410 --> 00:30:29,110
cancellation meaning that it doesn't

00:30:27,070 --> 00:30:32,410
matter on which of these instances the

00:30:29,110 --> 00:30:34,180
cancel called ends up the I correct I

00:30:32,410 --> 00:30:37,540
characters will send this message

00:30:34,180 --> 00:30:42,130
through to all the instances so that the

00:30:37,540 --> 00:30:44,440
context that triggered the query at the

00:30:42,130 --> 00:30:49,810
beginning will be the one that cancels

00:30:44,440 --> 00:30:52,060
it ok next I will I will show the rest

00:30:49,810 --> 00:30:57,640
of the features directly in the UI to

00:30:52,060 --> 00:30:59,530
have a look to see how it how it is the

00:30:57,640 --> 00:31:01,450
code that we used for this use case

00:30:59,530 --> 00:31:03,460
meaning the download job the

00:31:01,450 --> 00:31:06,010
transformation job and the job that run

00:31:03,460 --> 00:31:08,260
that writes the files to park it is

00:31:06,010 --> 00:31:13,320
available on github and the following

00:31:08,260 --> 00:31:13,320
address I will put this here too shortly

00:31:13,620 --> 00:31:19,170
ok do you hear me ok ok

00:31:26,520 --> 00:31:33,870
okay this is how Joe's looks like this

00:31:30,820 --> 00:31:40,780
is how it looks like we have here

00:31:33,870 --> 00:31:44,289
Aquarion history we can see the results

00:31:40,780 --> 00:31:47,080
this is a quite cool feature that sparks

00:31:44,289 --> 00:31:50,169
equal jdbc doesn't support is the

00:31:47,080 --> 00:31:53,289
retrieval of large results what do we do

00:31:50,169 --> 00:31:55,270
we take the results led with zip it with

00:31:53,289 --> 00:31:59,280
index and after that store it into

00:31:55,270 --> 00:32:02,140
tachyon or now or HDFS if you want and

00:31:59,280 --> 00:32:05,620
while retrieving the results paginate it

00:32:02,140 --> 00:32:07,960
we filter for we filter with for that

00:32:05,620 --> 00:32:11,440
index in order to provide samples from

00:32:07,960 --> 00:32:14,559
the for the results for a certain

00:32:11,440 --> 00:32:16,780
queries you can see the logs here you

00:32:14,559 --> 00:32:18,850
can take a look into the hive warehouse

00:32:16,780 --> 00:32:22,120
you can see all the databases stored in

00:32:18,850 --> 00:32:24,070
hive for the dead database you can see

00:32:22,120 --> 00:32:28,510
all the tables and of course or the

00:32:24,070 --> 00:32:30,520
columns for a table and another nice

00:32:28,510 --> 00:32:35,230
feature that we have is the pocket

00:32:30,520 --> 00:32:38,890
tables management what does this mean we

00:32:35,230 --> 00:32:41,830
map pocket files from tachyon or from a

00:32:38,890 --> 00:32:45,520
gfs into temporary tables in hive and

00:32:41,830 --> 00:32:48,280
persist them across sessions I know that

00:32:45,520 --> 00:32:50,890
this is the feature that the latest

00:32:48,280 --> 00:32:52,750
version of sparks equal support but we

00:32:50,890 --> 00:32:56,710
needed it before so we had to implement

00:32:52,750 --> 00:32:59,470
it this means that if you are using

00:32:56,710 --> 00:33:03,669
spark within another version and you

00:32:59,470 --> 00:33:06,610
need it you can use jaws okay what do we

00:33:03,669 --> 00:33:09,429
have next there is a file browser that

00:33:06,610 --> 00:33:12,520
browse the files on tachyon this folder

00:33:09,429 --> 00:33:15,010
contains the output of the job that

00:33:12,520 --> 00:33:19,150
wrote the trans financial transactions

00:33:15,010 --> 00:33:29,370
into pocket files okay we can map this

00:33:19,150 --> 00:33:32,370
folder to a table let's say it's buzz

00:33:29,370 --> 00:33:32,370
oops

00:33:32,970 --> 00:33:41,350
ok now the table was registered so we

00:33:38,559 --> 00:33:44,230
can we should see her see it under the

00:33:41,350 --> 00:33:46,480
park at tables tab it is here and now we

00:33:44,230 --> 00:33:48,660
can query it so let's say we want to

00:33:46,480 --> 00:33:55,540
select I will write it with one hand

00:33:48,660 --> 00:33:58,360
select I let's say count all the

00:33:55,540 --> 00:34:08,620
transactions let's say that group by

00:33:58,360 --> 00:34:11,770
side from buzz group by side awesome how

00:34:08,620 --> 00:34:16,450
hope it works ok you can see the logs

00:34:11,770 --> 00:34:18,879
and then the results so this is jose

00:34:16,450 --> 00:34:21,040
let's see how tachyon looks like for the

00:34:18,879 --> 00:34:23,350
ones who does who don't know it looks

00:34:21,040 --> 00:34:27,820
just like a normal file system but the

00:34:23,350 --> 00:34:31,450
files are in memory here is the packet

00:34:27,820 --> 00:34:33,580
format that i showed you before if you

00:34:31,450 --> 00:34:35,610
go inside it you can see that all the

00:34:33,580 --> 00:34:38,230
files are one hundred percent in memory

00:34:35,610 --> 00:34:42,310
here you can see the underlayer file

00:34:38,230 --> 00:34:44,889
system it's HDFS and for each file each

00:34:42,310 --> 00:34:50,230
market file you see the location the

00:34:44,889 --> 00:34:52,780
watched ip where it's stored ok also not

00:34:50,230 --> 00:34:54,340
another nice thing if you go in my sauce

00:34:52,780 --> 00:34:56,830
we are using muscles as a resource

00:34:54,340 --> 00:34:59,290
manager if you go in vessels you could

00:34:56,830 --> 00:35:02,050
see that the sparks equal context the

00:34:59,290 --> 00:35:05,410
Joe's particle context it's alive can be

00:35:02,050 --> 00:35:06,940
queried and if we look in the terminated

00:35:05,410 --> 00:35:09,550
frameworks you will see that there is

00:35:06,940 --> 00:35:14,020
the context that wrote the data in park

00:35:09,550 --> 00:35:17,020
it in tachyon and this shows a show us

00:35:14,020 --> 00:35:19,540
that even if the context sparks equal

00:35:17,020 --> 00:35:22,270
context is dead and buried we can steal

00:35:19,540 --> 00:35:24,630
query its output with the in-memory

00:35:22,270 --> 00:35:29,410
speed and this is only because tachyon

00:35:24,630 --> 00:35:31,750
ok so here is the github having the

00:35:29,410 --> 00:35:36,070
projects that we used for the use case

00:35:31,750 --> 00:35:39,030
and here is the github for Joe's it

00:35:36,070 --> 00:35:41,650
split between between not into

00:35:39,030 --> 00:35:44,500
microservices one that triggers hive

00:35:41,650 --> 00:35:44,890
queries and one that recurs spark sequel

00:35:44,500 --> 00:35:47,380
query

00:35:44,890 --> 00:35:49,990
you have a readme file explaining all

00:35:47,380 --> 00:35:52,450
the api's and also you have a

00:35:49,990 --> 00:35:54,789
configuration file to configure all the

00:35:52,450 --> 00:35:58,539
spark settings you want in order to

00:35:54,789 --> 00:36:00,970
improve your queries okay so thanks if

00:35:58,539 --> 00:36:04,260
you have any questions for both of us

00:36:00,970 --> 00:36:04,260

YouTube URL: https://www.youtube.com/watch?v=acjMiWoFHcA


