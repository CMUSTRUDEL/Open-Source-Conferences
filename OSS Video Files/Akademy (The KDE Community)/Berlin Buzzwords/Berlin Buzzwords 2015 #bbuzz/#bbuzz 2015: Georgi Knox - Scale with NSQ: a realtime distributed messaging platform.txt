Title: #bbuzz 2015: Georgi Knox - Scale with NSQ: a realtime distributed messaging platform
Publication date: 2015-06-05
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Find more information here: http://berlinbuzzwords.de/session/scale-nsq-realtime-distributed-messaging-platformtform

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:10,230 --> 00:00:13,600
you it's always hard being the last

00:00:12,000 --> 00:00:17,260
speaker on the last day

00:00:13,600 --> 00:00:19,540
to scale with nsq yes so my name is

00:00:17,260 --> 00:00:21,940
Georgie I'm a back-end software engineer

00:00:19,540 --> 00:00:23,890
originally from Sydney Australia and I

00:00:21,940 --> 00:00:27,820
moved to New York in about the summer of

00:00:23,890 --> 00:00:29,710
last year to work at bitly for those who

00:00:27,820 --> 00:00:32,590
haven't heard of bitly we're a really

00:00:29,710 --> 00:00:34,540
popular URL shortener most of our stack

00:00:32,590 --> 00:00:36,940
is written in Python and go and on

00:00:34,540 --> 00:00:39,100
average we serve around eight to ten

00:00:36,940 --> 00:00:41,530
billion clicks per month and that works

00:00:39,100 --> 00:00:44,199
out to be about 8,000 requests per

00:00:41,530 --> 00:00:46,239
second we then compute a lot of

00:00:44,199 --> 00:00:49,180
interesting analytics around how and

00:00:46,239 --> 00:00:51,250
where those links are shared so we get

00:00:49,180 --> 00:00:54,040
to tackle lots of interesting problems

00:00:51,250 --> 00:00:58,510
of scale at bitly and one of the core

00:00:54,040 --> 00:01:01,600
pieces of our architecture is nsq so

00:00:58,510 --> 00:01:03,790
what is nsq well it's an open source

00:01:01,600 --> 00:01:06,430
real-time distributed messaging platform

00:01:03,790 --> 00:01:08,860
that was built in-house and bitly and

00:01:06,430 --> 00:01:10,600
it's written entirely and go it's now

00:01:08,860 --> 00:01:13,420
been in production for over three years

00:01:10,600 --> 00:01:15,790
and is used by a range of companies that

00:01:13,420 --> 00:01:18,100
you can see here and there are three

00:01:15,790 --> 00:01:20,969
core components that make up and ask you

00:01:18,100 --> 00:01:24,789
that I'll going to go over in this talk

00:01:20,969 --> 00:01:26,530
so to explain how nsq works we'll start

00:01:24,789 --> 00:01:29,409
with a really simple example and build

00:01:26,530 --> 00:01:31,630
on top of that so let's say for example

00:01:29,409 --> 00:01:34,359
this is really early bitly we have a

00:01:31,630 --> 00:01:37,090
single host and an API that shortens

00:01:34,359 --> 00:01:40,780
lengths and returns synchronously to our

00:01:37,090 --> 00:01:42,789
API clients and then a new requirement

00:01:40,780 --> 00:01:45,640
comes in to track metrics against these

00:01:42,789 --> 00:01:48,159
requests don't the naive approach here

00:01:45,640 --> 00:01:50,469
would be to add in a step before we send

00:01:48,159 --> 00:01:52,329
the response back which where we

00:01:50,469 --> 00:01:54,100
synchronously right in to our metric

00:01:52,329 --> 00:01:56,140
system before returning back to our

00:01:54,100 --> 00:01:59,140
client but what would be the problem

00:01:56,140 --> 00:02:01,689
with this approach what happens when our

00:01:59,140 --> 00:02:04,630
metric system goes down will our API

00:02:01,689 --> 00:02:06,969
request then hang or fail what happens

00:02:04,630 --> 00:02:09,520
when our API becomes really popular how

00:02:06,969 --> 00:02:11,590
will we Hale scandal in scaling

00:02:09,520 --> 00:02:15,250
increasing API requests volume or

00:02:11,590 --> 00:02:17,050
breadth of metrics collection so we're

00:02:15,250 --> 00:02:19,510
starting to see that this current setup

00:02:17,050 --> 00:02:21,220
has a tight coupling problem and if we

00:02:19,510 --> 00:02:23,530
continue to build additions to this

00:02:21,220 --> 00:02:26,270
system in the same way we'll end up with

00:02:23,530 --> 00:02:28,790
a real mess of interconnected components

00:02:26,270 --> 00:02:30,740
one way to resolve all of these issues

00:02:28,790 --> 00:02:33,680
is to perform the work of writing into

00:02:30,740 --> 00:02:36,110
our metric system asynchronously that is

00:02:33,680 --> 00:02:38,690
place our data in some sort of queue and

00:02:36,110 --> 00:02:40,990
write in to our metric system via some

00:02:38,690 --> 00:02:43,880
other process that consumes that queue

00:02:40,990 --> 00:02:46,220
this decoupling allows the system to be

00:02:43,880 --> 00:02:49,520
more robust and fault tolerant a bitly

00:02:46,220 --> 00:02:50,990
we use nsq to achieve this let's talk

00:02:49,520 --> 00:02:52,610
briefly about a couple of basic

00:02:50,990 --> 00:02:56,300
messaging patterns which are core

00:02:52,610 --> 00:03:00,020
concepts of nsq this is a broadcast

00:02:56,300 --> 00:03:02,210
delivery mechanism you can see that the

00:03:00,020 --> 00:03:04,340
message is copied across and delivered

00:03:02,210 --> 00:03:07,400
to two distinct groups of consumers a

00:03:04,340 --> 00:03:09,230
and B for example a here could be

00:03:07,400 --> 00:03:11,660
consuming it could be something that

00:03:09,230 --> 00:03:14,240
updates a metric system and consumer B

00:03:11,660 --> 00:03:16,460
might be one that writes metric writes

00:03:14,240 --> 00:03:19,400
the messages to disk for audit logging

00:03:16,460 --> 00:03:22,220
so what does this pattern achieve well

00:03:19,400 --> 00:03:24,710
it allows us to decouple producers and

00:03:22,220 --> 00:03:26,570
consumers the producers don't need to

00:03:24,710 --> 00:03:29,390
know about who is consuming the message

00:03:26,570 --> 00:03:33,680
and similarly the consumers don't know

00:03:29,390 --> 00:03:37,880
or care whether data comes from next we

00:03:33,680 --> 00:03:39,830
have the distribution pattern in this

00:03:37,880 --> 00:03:41,870
messaging pattern data is pushed down a

00:03:39,830 --> 00:03:44,150
pipe where we then load balanced

00:03:41,870 --> 00:03:46,730
messages across Alcon connected

00:03:44,150 --> 00:03:47,960
consumers you'll notice both consumers

00:03:46,730 --> 00:03:49,730
are the same color and they're both

00:03:47,960 --> 00:03:51,140
called consumer a so they're going to be

00:03:49,730 --> 00:03:53,450
doing the same work and that could be

00:03:51,140 --> 00:03:55,850
for example writing messages to disk for

00:03:53,450 --> 00:03:58,700
audit logging what this pattern allows

00:03:55,850 --> 00:04:01,130
for is horizontal scalability as the

00:03:58,700 --> 00:04:03,320
volume of our stream changes we can

00:04:01,130 --> 00:04:05,540
introduce and retire consumers as

00:04:03,320 --> 00:04:08,120
required to handle variable throughput

00:04:05,540 --> 00:04:11,150
the other advantage of this pattern is

00:04:08,120 --> 00:04:12,560
that it's great for failure cases so

00:04:11,150 --> 00:04:15,580
let's see what happens when one of our

00:04:12,560 --> 00:04:15,580
consumers fail

00:04:20,859 --> 00:04:25,599
love that animation when the next

00:04:23,680 --> 00:04:27,400
message comes through we see that the

00:04:25,599 --> 00:04:29,259
Kim is the consumers that are still

00:04:27,400 --> 00:04:32,469
available we'll continue to be able to

00:04:29,259 --> 00:04:36,159
handle the throughput and lastly when

00:04:32,469 --> 00:04:37,930
all the consumers die in a fire our

00:04:36,159 --> 00:04:40,629
queue will hang on to those messages

00:04:37,930 --> 00:04:48,849
until such a time when new consumers can

00:04:40,629 --> 00:04:50,469
come back online to handle them these

00:04:48,849 --> 00:04:53,139
message patterns are the building block

00:04:50,469 --> 00:04:55,090
for something called nsq D which is the

00:04:53,139 --> 00:04:59,349
first component in the larger and SQ

00:04:55,090 --> 00:05:01,000
architecture nsq has a concept of topics

00:04:59,349 --> 00:05:03,759
and channels which are implemented as

00:05:01,000 --> 00:05:06,279
primitives a topic is a unique stream of

00:05:03,759 --> 00:05:08,229
messages and a real world of example of

00:05:06,279 --> 00:05:10,960
this that bitly might be our klicks

00:05:08,229 --> 00:05:12,729
topic which is a stream of messages for

00:05:10,960 --> 00:05:15,490
every click that is made on a bit length

00:05:12,729 --> 00:05:18,099
on the Internet and a channel is a given

00:05:15,490 --> 00:05:22,150
copy of that stream of messages for a

00:05:18,099 --> 00:05:23,889
given set of consumers at bitly for

00:05:22,150 --> 00:05:26,379
example we might add a few channels of

00:05:23,889 --> 00:05:29,349
the klicks topic such as metrics channel

00:05:26,379 --> 00:05:31,360
to count all the things a spam analysis

00:05:29,349 --> 00:05:33,849
channel to understand if these were in

00:05:31,360 --> 00:05:36,789
fact in fact legitimate clicks or if

00:05:33,849 --> 00:05:38,589
they were spam an archive channel which

00:05:36,789 --> 00:05:40,629
writes each of these messages to disk

00:05:38,589 --> 00:05:43,750
that so that our data science team can

00:05:40,629 --> 00:05:45,669
do some analysis at a later point under

00:05:43,750 --> 00:05:48,310
these under the hood topics and channels

00:05:45,669 --> 00:05:50,439
are both independent cues - and these

00:05:48,310 --> 00:05:53,949
properties enable nsq to support both

00:05:50,439 --> 00:05:56,069
multicast which is a topic Kabat copying

00:05:53,949 --> 00:05:58,509
each message to end channels and

00:05:56,069 --> 00:06:00,729
distributed message delivery where a

00:05:58,509 --> 00:06:03,879
channel equally divides its messages

00:06:00,729 --> 00:06:05,860
among end consumers and it's important

00:06:03,879 --> 00:06:07,839
to point out here that both topics and

00:06:05,860 --> 00:06:09,849
channels are created at runtime so

00:06:07,839 --> 00:06:13,449
there's no need to describe this

00:06:09,849 --> 00:06:15,370
hierarchy upfront as producers come and

00:06:13,449 --> 00:06:17,770
write messages to a given topic the

00:06:15,370 --> 00:06:20,289
topic will be created conversely if

00:06:17,770 --> 00:06:23,289
consumers come the channels that they

00:06:20,289 --> 00:06:25,150
are subscribing to will be created so

00:06:23,289 --> 00:06:28,990
let's see this in action firstly I'll

00:06:25,150 --> 00:06:31,790
just add a few consumers so a message is

00:06:28,990 --> 00:06:34,130
pushed through NS QD

00:06:31,790 --> 00:06:37,760
and copied across in a topic to all the

00:06:34,130 --> 00:06:39,170
channels it's then load-balanced to the

00:06:37,760 --> 00:06:43,430
connected consumers for a specific

00:06:39,170 --> 00:06:45,320
channel and I'll just show that again so

00:06:43,430 --> 00:06:49,150
the message is copied across and then

00:06:45,320 --> 00:06:49,150
delivered to just one of the consumers

00:06:52,630 --> 00:06:57,590
so what we've just seen here is a

00:06:55,070 --> 00:07:07,970
combination of broadcast messaging with

00:06:57,590 --> 00:07:12,770
load balancing and fault tolerance to go

00:07:07,970 --> 00:07:14,750
through yeah so what's happening under

00:07:12,770 --> 00:07:16,720
the hood most of this stuff is

00:07:14,750 --> 00:07:19,460
implemented in with go channels

00:07:16,720 --> 00:07:21,410
specifically nsq leverages buffered

00:07:19,460 --> 00:07:24,320
channels to manage in-memory message

00:07:21,410 --> 00:07:26,000
queues and writes overflow to DES so nsq

00:07:24,320 --> 00:07:28,760
has this concept of a high watermark

00:07:26,000 --> 00:07:31,190
where during extended downtime of a

00:07:28,760 --> 00:07:32,510
downstream system messages will be kept

00:07:31,190 --> 00:07:34,430
in memory until they reach that

00:07:32,510 --> 00:07:36,470
watermark and after which time they'll

00:07:34,430 --> 00:07:38,510
be written to disk then when the

00:07:36,470 --> 00:07:40,670
downstream systems come back online the

00:07:38,510 --> 00:07:43,490
messages both on disk and in memory will

00:07:40,670 --> 00:07:45,650
be sent topics and channels are

00:07:43,490 --> 00:07:47,870
independent and this means that if one

00:07:45,650 --> 00:07:49,910
of your downstream systems is having

00:07:47,870 --> 00:07:51,950
issues then only a single channel will

00:07:49,910 --> 00:07:53,510
backup and all of the other channels for

00:07:51,950 --> 00:07:57,710
that topic will still be able to

00:07:53,510 --> 00:08:00,290
successfully process messages let's go

00:07:57,710 --> 00:08:02,030
back now to that example of our simple

00:08:00,290 --> 00:08:04,250
API that we had at the beginning of the

00:08:02,030 --> 00:08:09,440
talk and we'll walk through some steps

00:08:04,250 --> 00:08:11,570
to add in NS QD so firstly we'll spin up

00:08:09,440 --> 00:08:14,420
an instance of NS QD on the same host

00:08:11,570 --> 00:08:16,940
that runs our API next we'll update our

00:08:14,420 --> 00:08:20,150
API application to publish to the local

00:08:16,940 --> 00:08:21,770
NS q instance to queue events now

00:08:20,150 --> 00:08:23,600
instead of directly writing

00:08:21,770 --> 00:08:26,600
synchronously into the metric system

00:08:23,600 --> 00:08:28,490
we'll instead write to NS QD and this

00:08:26,600 --> 00:08:29,540
can be as simple as performing an HTTP

00:08:28,490 --> 00:08:32,300
POST request

00:08:29,540 --> 00:08:35,270
lastly we'll build a consumer in a

00:08:32,300 --> 00:08:37,640
language of our choosing and using an NS

00:08:35,270 --> 00:08:39,260
q client library subscribe to that

00:08:37,640 --> 00:08:42,080
stream of data coming through the

00:08:39,260 --> 00:08:44,210
channel our consumers can then process

00:08:42,080 --> 00:08:45,300
the events and write asynchronously into

00:08:44,210 --> 00:08:48,090
our metrics

00:08:45,300 --> 00:08:50,160
our consumers can also just be run

00:08:48,090 --> 00:08:53,700
locally on the same host as our API and

00:08:50,160 --> 00:08:55,770
nsq the instance by adding an NS QD

00:08:53,700 --> 00:08:57,960
we've now decoupled the production and

00:08:55,770 --> 00:09:00,660
consumption of data which means that if

00:08:57,960 --> 00:09:02,250
the metrics system experiences issues it

00:09:00,660 --> 00:09:04,350
will be isolated from our system's

00:09:02,250 --> 00:09:07,500
ability to shorten links and return to

00:09:04,350 --> 00:09:09,870
our kernel clients the other nice

00:09:07,500 --> 00:09:12,300
approach of co-locating everything is

00:09:09,870 --> 00:09:14,370
that we can scale horizontally we can

00:09:12,300 --> 00:09:17,220
easily put a load balancer in front of

00:09:14,370 --> 00:09:21,330
two or more hosts and continue to handle

00:09:17,220 --> 00:09:23,610
increases in incoming volume and sq

00:09:21,330 --> 00:09:27,570
lookup D is the second component of the

00:09:23,610 --> 00:09:30,480
nsq architecture so what does a typical

00:09:27,570 --> 00:09:33,900
nsq cluster look like here you can see

00:09:30,480 --> 00:09:37,830
three co-located NS QD instances with

00:09:33,900 --> 00:09:40,530
their API the API is publishing locally

00:09:37,830 --> 00:09:43,140
to NS QD and topics are created as run

00:09:40,530 --> 00:09:46,380
at runtime when the first message is

00:09:43,140 --> 00:09:48,750
received by n s QD on a topic it'll push

00:09:46,380 --> 00:09:52,770
a registration message over to all

00:09:48,750 --> 00:09:54,780
lookup D hosts lookup D is the daemon

00:09:52,770 --> 00:09:56,720
that manages topology information and

00:09:54,780 --> 00:10:00,990
it's basically just a directory service

00:09:56,720 --> 00:10:03,180
which matches topics to producers look

00:10:00,990 --> 00:10:04,650
up the instances don't coordinate so

00:10:03,180 --> 00:10:06,840
that means that they each have their own

00:10:04,650 --> 00:10:09,390
independent copy of the mapping of

00:10:06,840 --> 00:10:11,550
topics to producers and this gives some

00:10:09,390 --> 00:10:13,710
nice fold fault tolerant characteristics

00:10:11,550 --> 00:10:16,830
so you could for example lose one of

00:10:13,710 --> 00:10:18,480
these lookup D instances and the others

00:10:16,830 --> 00:10:21,120
would continue to service requests just

00:10:18,480 --> 00:10:25,950
fine in a typical data center you might

00:10:21,120 --> 00:10:28,589
see three of these so why have we why

00:10:25,950 --> 00:10:30,270
the need to add lookup there here well

00:10:28,589 --> 00:10:33,120
if we took look up D out of the picture

00:10:30,270 --> 00:10:35,850
each consumer would need to hard-code

00:10:33,120 --> 00:10:37,860
the address of where each of the NS QD

00:10:35,850 --> 00:10:40,230
instances live and this is kind of a

00:10:37,860 --> 00:10:42,450
pain what you really want is for the

00:10:40,230 --> 00:10:44,940
configuration to evolve and be accessed

00:10:42,450 --> 00:10:48,839
at runtime based on the state of the NS

00:10:44,940 --> 00:10:51,510
q cluster the NS QD instances maintain a

00:10:48,839 --> 00:10:53,940
persistent TCP connection to the lookup

00:10:51,510 --> 00:10:56,580
D instances and register themselves as a

00:10:53,940 --> 00:10:58,620
as a producer for a given topic and all

00:10:56,580 --> 00:11:01,360
the channels that they know about

00:10:58,620 --> 00:11:04,570
this means that consumers can query

00:11:01,360 --> 00:11:06,430
lookup D for topic locations rather than

00:11:04,570 --> 00:11:08,350
the hard coding them when they get the

00:11:06,430 --> 00:11:09,970
IP addresses of these producers they

00:11:08,350 --> 00:11:21,100
Union them all together and then

00:11:09,970 --> 00:11:30,090
subscribe to all of them directly of

00:11:21,100 --> 00:11:32,350
having animation fail here so over time

00:11:30,090 --> 00:11:34,030
these consumers will learn about the

00:11:32,350 --> 00:11:36,850
existence of new producers and be able

00:11:34,030 --> 00:11:39,400
to route around failures because in this

00:11:36,850 --> 00:11:42,220
example we have co-located and as QD

00:11:39,400 --> 00:11:44,590
instances with our API what we've done

00:11:42,220 --> 00:11:47,080
is seamlessly create three shards that

00:11:44,590 --> 00:11:48,190
our external stream feeds into and that

00:11:47,080 --> 00:11:50,470
isn't something that we needed to

00:11:48,190 --> 00:11:54,850
configure up front and again comes as a

00:11:50,470 --> 00:11:56,440
side-effect of our deployment so back to

00:11:54,850 --> 00:11:58,870
our original service that we're scaling

00:11:56,440 --> 00:12:01,480
again this time we'll see what happens

00:11:58,870 --> 00:12:04,870
when we add in lookup D and some new

00:12:01,480 --> 00:12:08,050
consumers so nsq will log in and push a

00:12:04,870 --> 00:12:10,660
registration event across now let's say

00:12:08,050 --> 00:12:13,600
that we want to archive this topic we

00:12:10,660 --> 00:12:15,550
can use a NS q2 file which is a tool

00:12:13,600 --> 00:12:18,730
bundled with the NS q are binary

00:12:15,550 --> 00:12:21,910
download and all it does is write

00:12:18,730 --> 00:12:23,560
streams to disk this tool works the same

00:12:21,910 --> 00:12:25,630
way as other consumers where it

00:12:23,560 --> 00:12:27,430
continually checks in with lookup D to

00:12:25,630 --> 00:12:30,370
find out where the topics are being

00:12:27,430 --> 00:12:32,290
produced the consumers could then

00:12:30,370 --> 00:12:35,980
connect to all discovered producers and

00:12:32,290 --> 00:12:37,570
subscribe to their topics so now we have

00:12:35,980 --> 00:12:39,430
a second group of consumers that is

00:12:37,570 --> 00:12:42,310
consuming the same data and performing

00:12:39,430 --> 00:12:44,650
some action and this archived data can

00:12:42,310 --> 00:12:47,290
be really useful because it can be

00:12:44,650 --> 00:12:49,960
pushed into things like s3 or Hadoop and

00:12:47,290 --> 00:12:53,110
be used for like an audit log for any

00:12:49,960 --> 00:12:55,120
production problems so we've seen in

00:12:53,110 --> 00:12:58,180
this example how discoverability works

00:12:55,120 --> 00:13:00,490
within nsq by using the lookup directory

00:12:58,180 --> 00:13:02,530
service lookup D directory service we've

00:13:00,490 --> 00:13:05,710
been able to decouple producers from

00:13:02,530 --> 00:13:07,600
consumers and adding in our new set of

00:13:05,710 --> 00:13:09,400
consumers was really trivial we just

00:13:07,600 --> 00:13:12,089
specified what topic we were interested

00:13:09,400 --> 00:13:15,490
in and queried lookup d

00:13:12,089 --> 00:13:17,290
so what are some nsq guarantees or

00:13:15,490 --> 00:13:19,390
messages are delivered at least once

00:13:17,290 --> 00:13:21,399
which means you can and will receive

00:13:19,390 --> 00:13:24,010
duplicate duplicates and this could be

00:13:21,399 --> 00:13:27,579
for a variety of reasons such as client

00:13:24,010 --> 00:13:30,040
timeouts disconnections or riku udders

00:13:27,579 --> 00:13:32,649
it's the clients responsibility to

00:13:30,040 --> 00:13:36,399
perform item item potent operations or

00:13:32,649 --> 00:13:39,399
deegeu messages are also not durable by

00:13:36,399 --> 00:13:41,350
default we talked previously about the

00:13:39,399 --> 00:13:44,050
configurable high watermark but by

00:13:41,350 --> 00:13:47,860
default at bitly we run with a hybrid

00:13:44,050 --> 00:13:49,930
in-memory on disk setup messages

00:13:47,860 --> 00:13:51,940
received are unordered so you can't rely

00:13:49,930 --> 00:13:54,160
on the order of messages being delivered

00:13:51,940 --> 00:13:56,920
to consumers and this is a result of

00:13:54,160 --> 00:13:59,410
recuse and the combination of in-memory

00:13:56,920 --> 00:14:02,910
and on disk storage and the fact that

00:13:59,410 --> 00:14:05,769
each n SQ d node shares nothing

00:14:02,910 --> 00:14:09,310
consumers will eventually find all topic

00:14:05,769 --> 00:14:10,930
producers the discovery service and SQL

00:14:09,310 --> 00:14:13,769
cup D is designed to be eventually

00:14:10,930 --> 00:14:16,240
consistent and as mentioned previously

00:14:13,769 --> 00:14:20,160
lookup D nodes don't coordinate to

00:14:16,240 --> 00:14:22,600
maintain or state or answer queries

00:14:20,160 --> 00:14:25,089
topic and channel pausing are a couple

00:14:22,600 --> 00:14:26,620
of Handy features of nsq tooling here

00:14:25,089 --> 00:14:28,779
we've had some channels similar to

00:14:26,620 --> 00:14:30,339
previous slides we've got the metrics

00:14:28,779 --> 00:14:32,709
channel a reporting channel and an

00:14:30,339 --> 00:14:35,200
archives channel we really know that

00:14:32,709 --> 00:14:38,829
normally messages get copied they get

00:14:35,200 --> 00:14:40,959
delivered and processed successfully but

00:14:38,829 --> 00:14:44,980
now let's see what happens when we pause

00:14:40,959 --> 00:14:47,020
the clicks topic messages start pooling

00:14:44,980 --> 00:14:49,750
at the topic level and this is really

00:14:47,020 --> 00:14:52,360
useful for a variety of operation things

00:14:49,750 --> 00:14:55,149
such as renaming channels or introducing

00:14:52,360 --> 00:14:57,040
new ones in an atomic window you could

00:14:55,149 --> 00:15:00,100
also introduce new systems that you want

00:14:57,040 --> 00:15:01,600
your data to right into when we unpause

00:15:00,100 --> 00:15:03,779
the messages will be processed

00:15:01,600 --> 00:15:03,779
successfully

00:15:09,310 --> 00:15:13,610
we also have channel pausing and if we

00:15:12,110 --> 00:15:16,820
pause the reporting channel here then

00:15:13,610 --> 00:15:19,820
messages will queue at that channel and

00:15:16,820 --> 00:15:24,650
again beyond be processed successfully

00:15:19,820 --> 00:15:26,960
when we unpause a couple more tooling

00:15:24,650 --> 00:15:28,910
features so this image here is of NS q

00:15:26,960 --> 00:15:32,960
admin and that's the final component of

00:15:28,910 --> 00:15:34,820
the whole NS q architecture and what NS

00:15:32,960 --> 00:15:36,980
q admin does is allow you to view

00:15:34,820 --> 00:15:39,170
aggregated clusters statistics in real

00:15:36,980 --> 00:15:41,510
time and perform various administrative

00:15:39,170 --> 00:15:43,340
tasks so it will show things like queue

00:15:41,510 --> 00:15:45,710
depths how often messages are being

00:15:43,340 --> 00:15:48,170
recued and how fast clients are

00:15:45,710 --> 00:15:50,180
processing things it will also allow you

00:15:48,170 --> 00:15:53,780
to do things like pause channels and

00:15:50,180 --> 00:15:55,790
topics and lastly NS Q has something

00:15:53,780 --> 00:15:57,350
called ephemeral channels and these are

00:15:55,790 --> 00:15:59,630
channels that disappear when your last

00:15:57,350 --> 00:16:02,450
client disconnects and are useful for

00:15:59,630 --> 00:16:07,070
one-off scripts or inspecting a stream

00:16:02,450 --> 00:16:09,560
for debugging purposes so that's all I

00:16:07,070 --> 00:16:11,570
have you can find a copy of my slides at

00:16:09,560 --> 00:16:13,280
this link and I thought given that this

00:16:11,570 --> 00:16:14,930
is like the final talk on the final day

00:16:13,280 --> 00:16:16,520
rather than keeping everyone here to

00:16:14,930 --> 00:16:18,080
answer questions maybe what I'll do is

00:16:16,520 --> 00:16:21,790
leave it here and if you do have any

00:16:18,080 --> 00:16:21,790

YouTube URL: https://www.youtube.com/watch?v=OwD-W7uU2zU


