Title: Berlin Buzzwords 2015: Michal Rutkowski - Cassandra at Yammer #bbuzz
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance.

That may very well be true, but it took us a year to get there and that road was dotted with site’s lowered availability and compromised performance.

In this talk I am going to share the insights we gained when we migrated parts of the Yammer’s messaging pipeline from a custom storage solution backed by Java BDB to Cassandra. Covering topics like:

- Why we decided to move to Cassandra from our proprietary Berkeley DB backed database 
- Modeling data and capacity supported by metrics
- Zero downtime production rollout
- How things started falling apart after three months of seamless operation 
- How to diagnose and fix things later on

Read more:
https://2015.berlinbuzzwords.de/session/cassandra-yammer

About Michal Rutkowski:
https://2015.berlinbuzzwords.de/users/michal-rutkowski

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
I

00:00:05,660 --> 00:00:09,940
hello I'm Mohammad kofsky i work at

00:00:08,809 --> 00:00:12,980
Yammer

00:00:09,940 --> 00:00:16,609
engineer sorry I just need

00:00:12,980 --> 00:00:18,020
the tools okay so this is the rough

00:00:16,609 --> 00:00:19,820
outline what we're going to talk about I

00:00:18,020 --> 00:00:23,119
will briefly tell you about Yammer for

00:00:19,820 --> 00:00:26,810
those who don't know then I will explain

00:00:23,119 --> 00:00:29,420
what we wanted to change and why then

00:00:26,810 --> 00:00:31,460
how we roll out Cassandra and what

00:00:29,420 --> 00:00:33,500
problems we encountered and finally I

00:00:31,460 --> 00:00:35,269
will conclude with by talking of what

00:00:33,500 --> 00:00:39,500
we've learned and what worked well for

00:00:35,269 --> 00:00:41,449
us on that project so as you can see

00:00:39,500 --> 00:00:44,540
Yammer the enterprise social network

00:00:41,449 --> 00:00:45,680
whose goal is to facilitate better and

00:00:44,540 --> 00:00:48,079
faster communication within an

00:00:45,680 --> 00:00:50,870
organization and here are two

00:00:48,079 --> 00:00:52,579
screenshots one shows a view of Yammer

00:00:50,870 --> 00:00:55,460
which probably looks familiar quite like

00:00:52,579 --> 00:00:57,230
Facebook I guess it shows a group view

00:00:55,460 --> 00:01:00,640
we are looking at one of the groups in

00:00:57,230 --> 00:01:03,680
the amur where teams collaborate and and

00:01:00,640 --> 00:01:07,369
the right hand side screenshot is the

00:01:03,680 --> 00:01:10,160
inbox which covers the conversations

00:01:07,369 --> 00:01:13,880
which the user follows which are address

00:01:10,160 --> 00:01:15,619
to them or the user was mentioned in but

00:01:13,880 --> 00:01:17,360
both these are considered to be

00:01:15,619 --> 00:01:18,830
something we call the message feed there

00:01:17,360 --> 00:01:21,200
are quite a few other message feeds and

00:01:18,830 --> 00:01:23,060
i would say that Yammer is all about

00:01:21,200 --> 00:01:26,000
messaging and a concept of a message

00:01:23,060 --> 00:01:28,179
feed is quite central to the product but

00:01:26,000 --> 00:01:30,500
the talk will be about extracting this

00:01:28,179 --> 00:01:32,810
inbox message feed to a separate service

00:01:30,500 --> 00:01:38,300
and why we did it and how we approached

00:01:32,810 --> 00:01:40,459
it so this is a quite outdated diagram

00:01:38,300 --> 00:01:43,220
of service dependencies in the amur but

00:01:40,459 --> 00:01:44,720
that's the best I could find as you can

00:01:43,220 --> 00:01:47,179
see in the center there's this purple

00:01:44,720 --> 00:01:49,520
box called work feat that's our Ruby on

00:01:47,179 --> 00:01:53,929
Rails mana up it's a bit of a legacy and

00:01:49,520 --> 00:01:57,319
the green things around it our drop

00:01:53,929 --> 00:01:58,340
wizard java based services for those I

00:01:57,319 --> 00:02:01,670
don't know how many people know what

00:01:58,340 --> 00:02:03,170
drop wizard is okay quite a few well

00:02:01,670 --> 00:02:05,270
drop wizard is not so much a framework

00:02:03,170 --> 00:02:06,560
is more like a glue which brings

00:02:05,270 --> 00:02:09,080
together several convenient to use

00:02:06,560 --> 00:02:11,209
libraries and which allows that it's

00:02:09,080 --> 00:02:13,370
open source and it allows us to quickly

00:02:11,209 --> 00:02:16,459
develop services it kind of removes the

00:02:13,370 --> 00:02:17,870
boilerplate anyway so the problem we are

00:02:16,459 --> 00:02:20,840
facing we are trying to extract features

00:02:17,870 --> 00:02:23,120
from work feed into separate services so

00:02:20,840 --> 00:02:24,769
we can iterate on them independently but

00:02:23,120 --> 00:02:25,920
it is a bit of a legacy to work with

00:02:24,769 --> 00:02:27,330
that and it

00:02:25,920 --> 00:02:30,060
see as well that this is pretty complex

00:02:27,330 --> 00:02:32,190
so this requires us to have quite a bit

00:02:30,060 --> 00:02:34,470
of operational a decent operational to

00:02:32,190 --> 00:02:37,560
link so we obviously have some sort of

00:02:34,470 --> 00:02:40,410
we have continuous integrations to ship

00:02:37,560 --> 00:02:42,150
with confidence we have a tool for the

00:02:40,410 --> 00:02:44,010
deployment it's also a drop wizard

00:02:42,150 --> 00:02:45,750
service would we build we have quite a

00:02:44,010 --> 00:02:48,780
bit of analytics we even built a system

00:02:45,750 --> 00:02:50,310
for cui requiring multiple databases and

00:02:48,780 --> 00:02:52,110
we have a whole team devoted to

00:02:50,310 --> 00:02:56,550
analyzing data data scientists I guess

00:02:52,110 --> 00:02:57,690
and drop was it comes with a library

00:02:56,550 --> 00:03:00,239
called metrics which makes it very easy

00:02:57,690 --> 00:03:02,970
to capture and measure behavior of the

00:03:00,239 --> 00:03:06,180
service or characteristics of the data

00:03:02,970 --> 00:03:08,640
which is processes and it's also also an

00:03:06,180 --> 00:03:11,580
open source project we piped it to Kafka

00:03:08,640 --> 00:03:14,459
and there is being transformed a bit and

00:03:11,580 --> 00:03:16,470
it's pushed to add a tool like a third

00:03:14,459 --> 00:03:18,060
party tool called white front allows us

00:03:16,470 --> 00:03:21,330
to visualize and create quite complex

00:03:18,060 --> 00:03:23,150
queries actually both for past behavior

00:03:21,330 --> 00:03:25,530
analysis as well as kind of on call

00:03:23,150 --> 00:03:27,060
real-time monitoring of the system so

00:03:25,530 --> 00:03:28,880
this is more or less real time it's I

00:03:27,060 --> 00:03:31,950
think it updates in 10 second intervals

00:03:28,880 --> 00:03:36,450
and and we of course we use log

00:03:31,950 --> 00:03:38,250
aggregation work session cabana and so

00:03:36,450 --> 00:03:40,829
just to give you an example this is how

00:03:38,250 --> 00:03:42,329
the screen for our deployment tool right

00:03:40,829 --> 00:03:44,280
on my list name of the service it was

00:03:42,329 --> 00:03:47,940
developed in London it dealt with inbox

00:03:44,280 --> 00:03:49,530
and sound like a sensible 9 you can see

00:03:47,940 --> 00:03:50,820
you can create a package you can deploy

00:03:49,530 --> 00:03:53,430
it to a different environments you can

00:03:50,820 --> 00:03:55,709
see the recent history you can also see

00:03:53,430 --> 00:03:58,590
a more wider view of what's happening in

00:03:55,709 --> 00:04:00,109
the Oregon terms of deployments and here

00:03:58,590 --> 00:04:03,540
to give you an example of the our

00:04:00,109 --> 00:04:06,359
monitoring this is reading the inbox

00:04:03,540 --> 00:04:08,780
graph you can see latency and you can

00:04:06,359 --> 00:04:11,519
see frequency there's this pearl this

00:04:08,780 --> 00:04:12,780
one graph which is a bit out of line it

00:04:11,519 --> 00:04:15,120
turned out this was a faulty node

00:04:12,780 --> 00:04:16,769
basically we had RAM errors so it's

00:04:15,120 --> 00:04:19,169
getting a bit out of sync and it also

00:04:16,769 --> 00:04:22,340
affected our p99 latency of serving the

00:04:19,169 --> 00:04:26,160
inbox but is the kind of things we use

00:04:22,340 --> 00:04:27,930
so more to the point so what did we want

00:04:26,160 --> 00:04:31,050
to change in Y in this project so we

00:04:27,930 --> 00:04:32,340
undertake extract the inbox feature from

00:04:31,050 --> 00:04:35,070
an existing service it was already

00:04:32,340 --> 00:04:36,810
extracted after the menore app because

00:04:35,070 --> 00:04:39,390
it powered all the messaging feeds and

00:04:36,810 --> 00:04:40,800
we wanted to be able to iterate faster

00:04:39,390 --> 00:04:42,270
on the in box itself we didn't want to

00:04:40,800 --> 00:04:45,000
be constrained by the kind of common

00:04:42,270 --> 00:04:47,340
denominator of feeds and we also were

00:04:45,000 --> 00:04:49,110
facing a different problem we were

00:04:47,340 --> 00:04:52,110
working across DC we needed this for

00:04:49,110 --> 00:04:55,650
data recovery and our existing database

00:04:52,110 --> 00:04:58,230
based on Berkeley DB was in process it

00:04:55,650 --> 00:05:00,300
was expensive to scale I had a bad

00:04:58,230 --> 00:05:04,530
support story and it didn't particularly

00:05:00,300 --> 00:05:06,750
well work with cross DC setup so there's

00:05:04,530 --> 00:05:07,800
a like a product require motivation but

00:05:06,750 --> 00:05:11,400
we also had an infrastructural

00:05:07,800 --> 00:05:12,750
motivation for this project so to give

00:05:11,400 --> 00:05:14,550
you an overview of like from that more

00:05:12,750 --> 00:05:16,740
complex diagram that's roughly how our

00:05:14,550 --> 00:05:19,110
message pipeline looks likes we have the

00:05:16,740 --> 00:05:22,590
rails man up we have which is powered

00:05:19,110 --> 00:05:26,100
mostly by them as postgres but we also

00:05:22,590 --> 00:05:27,750
have an icing job to update to talk to

00:05:26,100 --> 00:05:30,150
the drop user service which powers are

00:05:27,750 --> 00:05:32,790
messaging feeds it serves as a bit as I

00:05:30,150 --> 00:05:34,980
like an index service if you like for

00:05:32,790 --> 00:05:36,900
the database there because the core the

00:05:34,980 --> 00:05:39,900
range course on this SQL database are

00:05:36,900 --> 00:05:43,560
too expensive for us so just to give you

00:05:39,900 --> 00:05:45,120
an overview either post a message dab

00:05:43,560 --> 00:05:48,030
does some logic to decide if they are

00:05:45,120 --> 00:05:50,550
allowed etc etc it's stored in the

00:05:48,030 --> 00:05:53,700
database and then a task is put on

00:05:50,550 --> 00:05:55,680
rabbit a worker picks it up besides who

00:05:53,700 --> 00:05:57,420
is to be delivered to and then sent off

00:05:55,680 --> 00:06:00,120
to the drop wizard service which updates

00:05:57,420 --> 00:06:02,460
is there's database and when the user

00:06:00,120 --> 00:06:04,830
wants to read an inbox it adds a rails

00:06:02,460 --> 00:06:06,090
app that rails up delegates to the drop

00:06:04,830 --> 00:06:07,710
wizard service to tell us was the

00:06:06,090 --> 00:06:10,290
content of that user's inbox what

00:06:07,710 --> 00:06:11,970
threads are in it and then it hydrates

00:06:10,290 --> 00:06:14,460
it with the contents of the messages

00:06:11,970 --> 00:06:19,020
sorted the postgres TB which is actually

00:06:14,460 --> 00:06:23,760
fronted with memcache yeah and its

00:06:19,020 --> 00:06:25,080
result is returned to the user so what

00:06:23,760 --> 00:06:30,030
part of that system we wanted to change

00:06:25,080 --> 00:06:35,670
and how well again the view so we wanted

00:06:30,030 --> 00:06:37,830
to train that back-end and yeah so sorry

00:06:35,670 --> 00:06:39,330
it was a quick term so we wanted to

00:06:37,830 --> 00:06:40,260
break down that drop wizard service and

00:06:39,330 --> 00:06:42,090
this is what I'm gonna be actually

00:06:40,260 --> 00:06:44,100
talking about we didn't want to break it

00:06:42,090 --> 00:06:48,030
into two pieces and we wanted to remove

00:06:44,100 --> 00:06:50,010
the bdb back end so now we came up with

00:06:48,030 --> 00:06:52,040
a rather complex diagram I would say

00:06:50,010 --> 00:06:54,230
it's probably too complex but

00:06:52,040 --> 00:06:56,600
kingstown in our overall overhauling of

00:06:54,230 --> 00:06:58,850
posting architecture but basically what

00:06:56,600 --> 00:07:01,370
happens here is the old stuff happens

00:06:58,850 --> 00:07:04,160
and one this once it happens we spin off

00:07:01,370 --> 00:07:06,800
a second task to update our new service

00:07:04,160 --> 00:07:08,270
basically it would probably be like long

00:07:06,800 --> 00:07:12,200
term we want it to be more a pops up

00:07:08,270 --> 00:07:13,580
model and actually I'm going to talk

00:07:12,200 --> 00:07:16,790
about this component of the picture

00:07:13,580 --> 00:07:21,410
today so I drop with your service backed

00:07:16,790 --> 00:07:23,720
by a Cassandra so okay we had that we

00:07:21,410 --> 00:07:25,610
know why we wanted to do it and how

00:07:23,720 --> 00:07:29,420
roughly but a few words about our

00:07:25,610 --> 00:07:31,850
methodology so we decided so we wanted

00:07:29,420 --> 00:07:33,950
to capture our API and the semantics of

00:07:31,850 --> 00:07:36,020
that API as early on integration tests

00:07:33,950 --> 00:07:37,490
so that we kind of knew that what we are

00:07:36,020 --> 00:07:40,580
delivering actually satisfies the

00:07:37,490 --> 00:07:43,700
requirements we also wanted to use

00:07:40,580 --> 00:07:46,130
production traffic for both capacity

00:07:43,700 --> 00:07:48,710
planning and load testing to that end we

00:07:46,130 --> 00:07:50,120
pretty early on we did a shadow deploy

00:07:48,710 --> 00:07:52,760
of the surface and we use double

00:07:50,120 --> 00:07:56,180
dispatch for both verizon reads just to

00:07:52,760 --> 00:07:57,590
see how it performed etc we migrated the

00:07:56,180 --> 00:07:59,600
data early serve either to work on the

00:07:57,590 --> 00:08:01,730
realistic data set and we're running

00:07:59,600 --> 00:08:03,530
verification tasks to pick up any data

00:08:01,730 --> 00:08:06,470
and consistencies between the two

00:08:03,530 --> 00:08:08,810
databases because as I said this is a it

00:08:06,470 --> 00:08:10,400
was a also a legacy integration so we're

00:08:08,810 --> 00:08:14,330
bound to miss fangs and we wanted to

00:08:10,400 --> 00:08:17,630
detect that and we kind of also assume

00:08:14,330 --> 00:08:20,030
we're going to make mistakes both in

00:08:17,630 --> 00:08:22,850
data modeling so we'll come up with bad

00:08:20,030 --> 00:08:24,470
design but will also miss use cases

00:08:22,850 --> 00:08:26,720
something I mentioned as before so you

00:08:24,470 --> 00:08:28,850
wanted to make the backfill or migration

00:08:26,720 --> 00:08:30,700
task from the old to the new database as

00:08:28,850 --> 00:08:33,290
cheap as possible and easy to do

00:08:30,700 --> 00:08:38,150
effectively so that if needed we could

00:08:33,290 --> 00:08:40,910
do it few times a day effectively so to

00:08:38,150 --> 00:08:43,310
give you some context new is that inbox

00:08:40,910 --> 00:08:47,600
is very heavy it receives about half a

00:08:43,310 --> 00:08:49,070
billion read queries a day it's not so

00:08:47,600 --> 00:08:52,580
right heavy is one tenth of that is

00:08:49,070 --> 00:08:54,560
actually right for two individual in

00:08:52,580 --> 00:08:56,510
boxes but what we have is we have

00:08:54,560 --> 00:08:59,930
massive spikes so a single message

00:08:56,510 --> 00:09:04,430
sometimes can fan out to 300,000 in

00:08:59,930 --> 00:09:06,089
boxes and we do fan out on right so we

00:09:04,430 --> 00:09:07,680
needed a technology

00:09:06,089 --> 00:09:09,930
that will be good for reads but could

00:09:07,680 --> 00:09:12,329
also provide like real time delivery in

00:09:09,930 --> 00:09:13,620
the face of these massive fan outs we

00:09:12,329 --> 00:09:17,999
wanted that the rights to be actually

00:09:13,620 --> 00:09:19,439
also pretty fast so the first step was

00:09:17,999 --> 00:09:21,990
to choose the database to back our new

00:09:19,439 --> 00:09:24,360
service and we considered react and

00:09:21,990 --> 00:09:25,860
Cassandra we actually have run react in

00:09:24,360 --> 00:09:28,199
the Yammer we were running really am

00:09:25,860 --> 00:09:33,029
already but we weren't sure if it will

00:09:28,199 --> 00:09:34,800
be a good fit for data model so but both

00:09:33,029 --> 00:09:36,300
of them are chardon and replicated so

00:09:34,800 --> 00:09:37,319
that provides us horizontal scaling so

00:09:36,300 --> 00:09:40,649
the address is they're expensive to

00:09:37,319 --> 00:09:42,540
scale aspect and it replicators should

00:09:40,649 --> 00:09:44,579
be highly available at both work well

00:09:42,540 --> 00:09:46,379
across across disease and have a support

00:09:44,579 --> 00:09:48,209
story time which is important for us we

00:09:46,379 --> 00:09:49,970
have paying customers and we don't

00:09:48,209 --> 00:09:52,379
necessarily have the in-house expertise

00:09:49,970 --> 00:09:55,379
so we chose in the end which was

00:09:52,379 --> 00:09:56,819
Cassandra because we did not want to be

00:09:55,379 --> 00:09:58,379
forced to the read-modify-write on a

00:09:56,819 --> 00:10:01,110
message delivery basically by that I

00:09:58,379 --> 00:10:03,600
mean we've react unless we went to super

00:10:01,110 --> 00:10:06,240
complex difficult to understand data in

00:10:03,600 --> 00:10:08,550
terms of performance data model we would

00:10:06,240 --> 00:10:11,670
have to read the whole inbox updated and

00:10:08,550 --> 00:10:13,829
write it back now that would might could

00:10:11,670 --> 00:10:15,629
provide natural saturation problems but

00:10:13,829 --> 00:10:17,399
it was also bad from performance

00:10:15,629 --> 00:10:18,629
perspective and if specially if the

00:10:17,399 --> 00:10:19,949
inbox was contended because there were

00:10:18,629 --> 00:10:21,839
multiple right stood at the same time

00:10:19,949 --> 00:10:24,329
this kind of optimistic transaction

00:10:21,839 --> 00:10:25,350
system the setup wasn't great for us so

00:10:24,329 --> 00:10:27,029
that's why we went with Cassandra

00:10:25,350 --> 00:10:28,499
because it allowed us to update exactly

00:10:27,029 --> 00:10:31,730
what we wanted without reading the inbox

00:10:28,499 --> 00:10:34,620
which would give us like very fast rides

00:10:31,730 --> 00:10:37,589
so the second phase was to get something

00:10:34,620 --> 00:10:39,779
working out so we've proven the hardware

00:10:37,589 --> 00:10:42,839
with this week I'm okay now came up with

00:10:39,779 --> 00:10:44,279
a decent restful api by decent I mean

00:10:42,839 --> 00:10:46,259
some people captured probably eighteen

00:10:44,279 --> 00:10:48,929
ninety percent of the use cases the rest

00:10:46,259 --> 00:10:51,839
we discovered in the due process and we

00:10:48,929 --> 00:10:56,189
build tests around that API and we set

00:10:51,839 --> 00:10:58,199
up a build process that runs those tests

00:10:56,189 --> 00:11:00,389
and actually starts the service and hits

00:10:58,199 --> 00:11:02,579
Cassandra this is a Travis orvis

00:11:00,389 --> 00:11:05,100
Cassandra's in Java so we used my event

00:11:02,579 --> 00:11:07,740
basically but it takes two minutes to

00:11:05,100 --> 00:11:10,470
run the whole suite of I think 200 tests

00:11:07,740 --> 00:11:13,430
and we started implementing against that

00:11:10,470 --> 00:11:17,180
basically so

00:11:13,430 --> 00:11:20,029
to go deeper into the design of like the

00:11:17,180 --> 00:11:22,220
principles by which the inbox works so

00:11:20,029 --> 00:11:24,320
it stores as I said before it starts to

00:11:22,220 --> 00:11:28,670
Fred's address and watered by the user

00:11:24,320 --> 00:11:30,950
and those threads when we view them are

00:11:28,670 --> 00:11:32,089
ordered by most recently replied to so

00:11:30,950 --> 00:11:34,270
we want the most recently updated

00:11:32,089 --> 00:11:36,560
threads to be at the top of the inbox

00:11:34,270 --> 00:11:37,880
but the Fred contents as I've said

00:11:36,560 --> 00:11:39,920
before isn't actually stored in the

00:11:37,880 --> 00:11:43,029
service that's the hydration happens

00:11:39,920 --> 00:11:46,100
outside this is like a index basically

00:11:43,029 --> 00:11:48,560
so on a message post we would deliver

00:11:46,100 --> 00:11:50,120
the message to every inbox and this

00:11:48,560 --> 00:11:52,040
effectively amounts to updating the last

00:11:50,120 --> 00:11:54,230
message ID so that we can maintain the

00:11:52,040 --> 00:11:57,500
or drink and some little pieces of

00:11:54,230 --> 00:12:00,800
metadata and on read will paginate will

00:11:57,500 --> 00:12:02,839
just read the most recent Fred's let's

00:12:00,800 --> 00:12:05,510
say top true the most recent 20 freds

00:12:02,839 --> 00:12:06,860
and quite a fan would filter mostly by

00:12:05,510 --> 00:12:11,000
red on red there are some other cases

00:12:06,860 --> 00:12:14,260
but they don't matter that much so we

00:12:11,000 --> 00:12:17,000
can remove the first design mmm it was

00:12:14,260 --> 00:12:18,950
pretty simple we had a table for in

00:12:17,000 --> 00:12:20,959
boxes it was partitioned by inbox ID it

00:12:18,950 --> 00:12:24,830
was primary keyed my inbox ID last

00:12:20,959 --> 00:12:26,839
message ID for those not familiar of

00:12:24,830 --> 00:12:28,430
Cassandra if you have stuff in the

00:12:26,839 --> 00:12:30,800
primary key basically data as ordered by

00:12:28,430 --> 00:12:32,410
a primary key so that allowed so that

00:12:30,800 --> 00:12:35,779
affect Lee meant but data is ordered

00:12:32,410 --> 00:12:38,089
from the most recent to less recent

00:12:35,779 --> 00:12:43,000
stuff and we use secondary indices for

00:12:38,089 --> 00:12:45,410
filtering the filters were of small like

00:12:43,000 --> 00:12:47,690
there were like three values for a

00:12:45,410 --> 00:12:49,940
filtered mouse so there were a good fit

00:12:47,690 --> 00:12:52,010
and we had a threat table because we

00:12:49,940 --> 00:12:57,200
needed to maintain some metadata for

00:12:52,010 --> 00:13:00,020
updating the 10 books table like who got

00:12:57,200 --> 00:13:02,390
updates in the past it was the last

00:13:00,020 --> 00:13:03,950
message because in order to update that

00:13:02,390 --> 00:13:06,470
table we need to know what was in it

00:13:03,950 --> 00:13:09,800
before so we had to check it by thread

00:13:06,470 --> 00:13:15,190
and yeah we this was partitioned by Fred

00:13:09,800 --> 00:13:17,089
ID and it used the set crdt so and of

00:13:15,190 --> 00:13:18,649
eventual consistent data structure

00:13:17,089 --> 00:13:21,920
provided by Cassandra which was very

00:13:18,649 --> 00:13:24,500
convenient so it was great that's we're

00:13:21,920 --> 00:13:25,779
passing it fitted perfectly with our

00:13:24,500 --> 00:13:27,699
usage patterns

00:13:25,779 --> 00:13:29,589
and was self-healing the presence of out

00:13:27,699 --> 00:13:32,620
of order deliveries or system partitions

00:13:29,589 --> 00:13:35,110
it was also an important more or less so

00:13:32,620 --> 00:13:38,800
except that it brought our migration

00:13:35,110 --> 00:13:43,809
task to a halt it just wasn't working

00:13:38,800 --> 00:13:45,790
with our migration traffic so what went

00:13:43,809 --> 00:13:48,490
wrong well we discover that the

00:13:45,790 --> 00:13:51,759
secondary indices are slow as hell

00:13:48,490 --> 00:13:53,920
actually and crd teas are ok for small

00:13:51,759 --> 00:13:55,899
and if in frequently updated data sets

00:13:53,920 --> 00:13:57,040
but not something like our subscription

00:13:55,899 --> 00:14:00,670
lists which we try to keep very

00:13:57,040 --> 00:14:02,740
consistent and and secondly the cost of

00:14:00,670 --> 00:14:04,509
our conveniently stored data was the

00:14:02,740 --> 00:14:06,579
heavy the reliance of Dilys because the

00:14:04,509 --> 00:14:08,230
message ID which was in the primary key

00:14:06,579 --> 00:14:10,990
could not be modified it had to be

00:14:08,230 --> 00:14:15,370
deleted and reinserted so that that's a

00:14:10,990 --> 00:14:17,769
no-no in Cassandra world but so what do

00:14:15,370 --> 00:14:19,600
we do now well we actually expected this

00:14:17,769 --> 00:14:21,370
kind of thing because we're only just

00:14:19,600 --> 00:14:23,309
learning Cassandra nobody of us from

00:14:21,370 --> 00:14:25,480
from our team use Cassandra before and

00:14:23,309 --> 00:14:27,189
we actually wanted to push something out

00:14:25,480 --> 00:14:29,529
as quickly as possible so we could get

00:14:27,189 --> 00:14:32,819
like realistic data to benchmark our

00:14:29,529 --> 00:14:36,939
solutions but what was important is that

00:14:32,819 --> 00:14:39,040
this did whatever our API was already

00:14:36,939 --> 00:14:41,889
established and we had the semantics of

00:14:39,040 --> 00:14:44,889
that API well captured in tests and we

00:14:41,889 --> 00:14:48,309
had this migration task available so we

00:14:44,889 --> 00:14:50,110
could use our metrics our tests and the

00:14:48,309 --> 00:14:52,180
migration to quickly iterate on

00:14:50,110 --> 00:14:55,029
implementation so this was a bit of a

00:14:52,180 --> 00:14:57,009
setback but it wasn't and the world so

00:14:55,029 --> 00:14:59,620
we came up with a second design which

00:14:57,009 --> 00:15:01,420
was a bit dramatically different we

00:14:59,620 --> 00:15:05,410
forgot all the cassandra extras and

00:15:01,420 --> 00:15:07,509
designed around what is good at and and

00:15:05,410 --> 00:15:09,459
in order to come up with this design we

00:15:07,509 --> 00:15:11,050
had we kind of went deeper into our

00:15:09,459 --> 00:15:13,269
product requirements or future

00:15:11,050 --> 00:15:15,779
requirements and try to leverage that to

00:15:13,269 --> 00:15:19,179
make sure that our design actually works

00:15:15,779 --> 00:15:21,610
and we use analytics metrics etc so we

00:15:19,179 --> 00:15:25,809
discover what we do not need to keep all

00:15:21,610 --> 00:15:28,089
the data we just need to keep recent

00:15:25,809 --> 00:15:31,689
stuff because content is searchable

00:15:28,089 --> 00:15:33,939
otherwise and for instance 5000 entries

00:15:31,689 --> 00:15:36,129
only take 75 kilobytes so it's

00:15:33,939 --> 00:15:38,559
relatively little and it covers four

00:15:36,129 --> 00:15:39,580
years for an active user so if we kept

00:15:38,559 --> 00:15:41,470
on that would be

00:15:39,580 --> 00:15:45,910
you pretty much doing everything we need

00:15:41,470 --> 00:15:48,190
two or more and will be probably we will

00:15:45,910 --> 00:15:49,800
be able to do what we I will explain in

00:15:48,190 --> 00:15:52,420
the next slide and then importantly

00:15:49,800 --> 00:15:53,890
since this is an index which basically

00:15:52,420 --> 00:15:55,990
tries to show you what content isn't

00:15:53,890 --> 00:15:57,790
available and in what order we actually

00:15:55,990 --> 00:16:01,300
don't have to be that exact about what

00:15:57,790 --> 00:16:02,470
data we store so let me explain so just

00:16:01,300 --> 00:16:04,780
to give you an example what we came up

00:16:02,470 --> 00:16:06,640
with we decided well partition still bi

00:16:04,780 --> 00:16:10,090
inbox ID this was important because it

00:16:06,640 --> 00:16:11,950
meant that hot and less hot networks of

00:16:10,090 --> 00:16:14,890
in the amur would be evenly distributed

00:16:11,950 --> 00:16:16,810
so there would be no hot spots but this

00:16:14,890 --> 00:16:20,200
time we decided to partition by inbox ID

00:16:16,810 --> 00:16:23,920
and thread ID and made last message ID

00:16:20,200 --> 00:16:26,830
that our mutable metadata stored in the

00:16:23,920 --> 00:16:28,780
inbox and the secondary table basically

00:16:26,830 --> 00:16:31,060
dropped that that Fred table basically

00:16:28,780 --> 00:16:32,890
dropped the use of CR DTS we just use

00:16:31,060 --> 00:16:37,390
this as a controlled manual maintain

00:16:32,890 --> 00:16:41,230
secondary index so this was clearly

00:16:37,390 --> 00:16:43,900
going to be mutation heavy so we decided

00:16:41,230 --> 00:16:45,160
to use level compaction and I don't know

00:16:43,900 --> 00:16:49,840
how many people are familiar of

00:16:45,160 --> 00:16:52,480
compaction Cassandra okay so Cassandra

00:16:49,840 --> 00:16:55,480
uses like Asif has a two tiers to it it

00:16:52,480 --> 00:16:57,250
has a append only log that's why it's

00:16:55,480 --> 00:17:01,060
very facet writing but then that log is

00:16:57,250 --> 00:17:03,610
consumed and materialized in Samuel is

00:17:01,060 --> 00:17:06,990
called an SS typist assorted table I

00:17:03,610 --> 00:17:10,180
guess and that's only then it's

00:17:06,990 --> 00:17:11,920
conceivable for reading and these tables

00:17:10,180 --> 00:17:13,330
are immutable so when there's let's say

00:17:11,920 --> 00:17:15,250
you have an entry in the table and you

00:17:13,330 --> 00:17:17,470
mutate it what effect will happen is

00:17:15,250 --> 00:17:19,959
better a new table will create it with

00:17:17,470 --> 00:17:21,699
an override so a query for a value will

00:17:19,959 --> 00:17:22,780
have to go through all the SS tables

00:17:21,699 --> 00:17:26,740
which contain it and take the most

00:17:22,780 --> 00:17:28,480
recent one and and typically typically

00:17:26,740 --> 00:17:30,460
cassandra is called sighs tiered

00:17:28,480 --> 00:17:32,200
compaction so where to SS tables become

00:17:30,460 --> 00:17:34,090
of similar size or four I don't remember

00:17:32,200 --> 00:17:37,330
the exact details they're compacting to

00:17:34,090 --> 00:17:38,860
one so this way next time you query that

00:17:37,330 --> 00:17:41,980
as a stable you only have to read one

00:17:38,860 --> 00:17:45,700
not four but level compaction is a bit

00:17:41,980 --> 00:17:47,290
different it uses 12 levels and ensures

00:17:45,700 --> 00:17:49,360
that at mouse you have to consult 12 as

00:17:47,290 --> 00:17:52,490
the stables but expected number of SS

00:17:49,360 --> 00:17:55,700
tables around one so that makes it

00:17:52,490 --> 00:18:05,630
of constant low I oh it makes reads much

00:17:55,700 --> 00:18:08,090
faster without clear okay sorry and so

00:18:05,630 --> 00:18:11,210
we knew that there will be racist race

00:18:08,090 --> 00:18:13,760
conditions on updates but that's what I

00:18:11,210 --> 00:18:15,980
meant by not being exact on active

00:18:13,760 --> 00:18:18,800
threat it doesn't matter we just order

00:18:15,980 --> 00:18:21,130
and filter so if two Fred's are

00:18:18,800 --> 00:18:23,780
competing for the high top position or

00:18:21,130 --> 00:18:25,429
they have like the last message right is

00:18:23,780 --> 00:18:26,900
not they actually last message ID but

00:18:25,429 --> 00:18:29,570
the one before it doesn't matter because

00:18:26,900 --> 00:18:32,150
we still can serve relevant content and

00:18:29,570 --> 00:18:34,640
unless active ones races will be

00:18:32,150 --> 00:18:38,330
negligible and users will correct it

00:18:34,640 --> 00:18:40,130
effectively and this the second

00:18:38,330 --> 00:18:42,590
especially the second one was a bit of a

00:18:40,130 --> 00:18:45,500
hypothesis which we had to verify but it

00:18:42,590 --> 00:18:50,660
proved correct there are no complaints

00:18:45,500 --> 00:18:53,030
about ordering in inbox and on read we

00:18:50,660 --> 00:18:55,179
read the whole inbox now not just the

00:18:53,030 --> 00:18:58,190
top twenty first but we have to read

00:18:55,179 --> 00:19:00,679
everything we sorted and filter but it

00:18:58,190 --> 00:19:05,360
is small this is actually ok even better

00:19:00,679 --> 00:19:07,100
because of how we represent Fred's where

00:19:05,360 --> 00:19:09,590
the Fred ID is there actually the idea

00:19:07,100 --> 00:19:11,690
of the first message in that thread this

00:19:09,590 --> 00:19:13,970
data is semi sorted so sorting of 5000

00:19:11,690 --> 00:19:16,070
entries isn't particularly expensive and

00:19:13,970 --> 00:19:18,410
we trim access data to make sure that

00:19:16,070 --> 00:19:20,690
inbox stays in check so we read I think

00:19:18,410 --> 00:19:22,640
fifteen thousand records trim it to five

00:19:20,690 --> 00:19:24,950
thousand after it exceeded five and a

00:19:22,640 --> 00:19:28,630
half that so we don't trim every time

00:19:24,950 --> 00:19:31,700
but give ourselves a bit of a buffer and

00:19:28,630 --> 00:19:34,370
so this time we go to production and

00:19:31,700 --> 00:19:36,350
pass the migration step it was even

00:19:34,370 --> 00:19:39,620
working in the shadow mode module some

00:19:36,350 --> 00:19:42,020
data and consistencies from Miss ed use

00:19:39,620 --> 00:19:43,970
cases which I mentioned but the rich

00:19:42,020 --> 00:19:48,050
performance was very varied and below

00:19:43,970 --> 00:19:51,620
our expectations so what now so again

00:19:48,050 --> 00:19:54,710
usage metrics zoom in even further get

00:19:51,620 --> 00:19:57,340
more detail out of it so out of the half

00:19:54,710 --> 00:20:00,010
a million queries we see every day

00:19:57,340 --> 00:20:02,870
ninety percent are for an unread count

00:20:00,010 --> 00:20:03,770
actually it's important to visualization

00:20:02,870 --> 00:20:06,140
in the amur

00:20:03,770 --> 00:20:10,580
because the reason that is the case is

00:20:06,140 --> 00:20:14,810
because we use we heavily rely on real

00:20:10,580 --> 00:20:16,340
time delivery fruit like free comedy so

00:20:14,810 --> 00:20:18,170
actually the actual load of an inbox

00:20:16,340 --> 00:20:21,500
happens relatively rarely but we

00:20:18,170 --> 00:20:25,580
referred unread can't quite often and

00:20:21,500 --> 00:20:28,070
forever more than read the number of

00:20:25,580 --> 00:20:30,110
unread messages is usually very small so

00:20:28,070 --> 00:20:32,890
users have ninety five percent of the

00:20:30,110 --> 00:20:37,730
users have less than 100 unread messages

00:20:32,890 --> 00:20:40,520
1% has 1000 and more so reading 5,000

00:20:37,730 --> 00:20:43,160
entry was totally excessive and exposes

00:20:40,520 --> 00:20:44,810
too much too much I oh so we decided to

00:20:43,160 --> 00:20:46,790
materialise that worried and basically

00:20:44,810 --> 00:20:49,910
maintain a second table for those two

00:20:46,790 --> 00:20:51,380
cups it was a separate table just on red

00:20:49,910 --> 00:20:54,110
stuff so it was trivial to query will

00:20:51,380 --> 00:20:56,870
just query the thousand get select

00:20:54,110 --> 00:21:00,670
thousand entries from the table and we

00:20:56,870 --> 00:21:03,320
return the count so where did it get us

00:21:00,670 --> 00:21:04,730
so our write latency is less than

00:21:03,320 --> 00:21:06,320
hundred milliseconds for regular

00:21:04,730 --> 00:21:09,260
messages I mean this is not like this

00:21:06,320 --> 00:21:12,230
this is a whole service latency if you

00:21:09,260 --> 00:21:15,410
like and it's less than 10 seconds for

00:21:12,230 --> 00:21:16,880
massive spiky announcements which is

00:21:15,410 --> 00:21:18,440
okay announcements don't have to be that

00:21:16,880 --> 00:21:23,030
real-time but overall system is pretty

00:21:18,440 --> 00:21:25,790
robust in terms of rights and and read

00:21:23,030 --> 00:21:29,750
latency is pretty good RP 99 is 250

00:21:25,790 --> 00:21:31,550
milliseconds triple line is less than

00:21:29,750 --> 00:21:33,440
500 milliseconds that's for the whole

00:21:31,550 --> 00:21:35,570
inbox again that happens relatively

00:21:33,440 --> 00:21:39,170
rarely so having a bit of a spinner for

00:21:35,570 --> 00:21:41,240
that that is okay and the critical query

00:21:39,170 --> 00:21:44,300
is less than 50 30 milliseconds so it

00:21:41,240 --> 00:21:46,340
gives us what we want so yeah we shipped

00:21:44,300 --> 00:21:50,750
and yeah I should ask you for questions

00:21:46,340 --> 00:21:53,480
now but actually not so fast because

00:21:50,750 --> 00:21:58,130
three months later on my first day of my

00:21:53,480 --> 00:22:00,410
summer holiday Yammer is down and the

00:21:58,130 --> 00:22:02,600
site is down our services on fire and

00:22:00,410 --> 00:22:05,060
free Cassandra knows are on fire I've

00:22:02,600 --> 00:22:08,840
spent few hours on a call with San Jose

00:22:05,060 --> 00:22:10,760
and San Francisco from war so when my

00:22:08,840 --> 00:22:16,230
family expected me to be with them so

00:22:10,760 --> 00:22:18,870
that was in fun so what went wrong

00:22:16,230 --> 00:22:21,179
so first of all it turns out but having

00:22:18,870 --> 00:22:23,880
an hae service we had free nose with a

00:22:21,179 --> 00:22:27,390
load balancer NetScaler all that cool

00:22:23,880 --> 00:22:31,140
stuff and highly available database

00:22:27,390 --> 00:22:33,059
doesn't make your site AJ the route is

00:22:31,140 --> 00:22:34,740
up it's never one thing but that the

00:22:33,059 --> 00:22:37,049
root of it was a massive inbox which was

00:22:34,740 --> 00:22:40,470
receiving tons of updates but was never

00:22:37,049 --> 00:22:42,530
read this method was never trimmed so it

00:22:40,470 --> 00:22:45,720
grew to like 100,000 of entries

00:22:42,530 --> 00:22:47,580
effectively now level compaction didn't

00:22:45,720 --> 00:22:49,350
like it because level compaction has to

00:22:47,580 --> 00:22:50,910
rewrite an inbox on every compaction so

00:22:49,350 --> 00:22:54,150
each ride would effectively trigger a

00:22:50,910 --> 00:22:59,370
compaction that compaction had to deal

00:22:54,150 --> 00:23:01,320
with loads and loads of data so our

00:22:59,370 --> 00:23:02,960
training strategy so what were the

00:23:01,320 --> 00:23:05,520
deeper problem so our trimming strategy

00:23:02,960 --> 00:23:08,280
didn't account for over growing in boxes

00:23:05,520 --> 00:23:09,840
impact on compaction we just again new

00:23:08,280 --> 00:23:14,700
technology we didn't fully understand it

00:23:09,840 --> 00:23:16,350
so yeah we overlooked it but there are

00:23:14,700 --> 00:23:18,480
more fundamental problem our brain app

00:23:16,350 --> 00:23:21,660
didn't have circuit breakers so when the

00:23:18,480 --> 00:23:23,820
service was slow and respond it was

00:23:21,660 --> 00:23:25,470
waiting for quite a long time and the

00:23:23,820 --> 00:23:28,440
first poll was exhausted so we couldn't

00:23:25,470 --> 00:23:31,590
serve traffic the service itself didn't

00:23:28,440 --> 00:23:33,150
control its resources as in when a

00:23:31,590 --> 00:23:37,590
request was running long it would run

00:23:33,150 --> 00:23:39,150
for however long it had to maybe ten

00:23:37,590 --> 00:23:41,400
seconds I think that's how much by

00:23:39,150 --> 00:23:42,750
default Cassandra it's probably about 10

00:23:41,400 --> 00:23:46,440
20 seconds because I think it gives a

00:23:42,750 --> 00:23:48,210
second try the data sex driver so yeah

00:23:46,440 --> 00:23:50,460
so it was easy with 20 seconds per

00:23:48,210 --> 00:23:52,290
request it was easy to exhaust its Fred

00:23:50,460 --> 00:23:55,890
polls and backup requests in their

00:23:52,290 --> 00:23:59,309
request queues so what did we do to fix

00:23:55,890 --> 00:24:01,710
it so first of all we added

00:23:59,309 --> 00:24:03,720
probabilistic and asynchronous trimming

00:24:01,710 --> 00:24:05,960
on delivery this was to deal with those

00:24:03,720 --> 00:24:09,049
in boxes which brought us down initially

00:24:05,960 --> 00:24:11,610
but we also went a bit further and we

00:24:09,049 --> 00:24:14,040
decided to keep stoom stands for shorter

00:24:11,610 --> 00:24:16,830
so but actually even if we because after

00:24:14,040 --> 00:24:19,500
they delete the tombstone stays until it

00:24:16,830 --> 00:24:20,669
expires so even though we delete data it

00:24:19,500 --> 00:24:22,799
would still have an impact on compaction

00:24:20,669 --> 00:24:26,040
until it's collected you need to keep a

00:24:22,799 --> 00:24:27,540
tombstone because if you delete and the

00:24:26,040 --> 00:24:29,230
data hasn't been replicated to all nodes

00:24:27,540 --> 00:24:30,490
it will resurrect

00:24:29,230 --> 00:24:31,990
tombstones like a marker for this has

00:24:30,490 --> 00:24:33,040
been deleted and you keep it until you

00:24:31,990 --> 00:24:36,760
make sure that your delete has been

00:24:33,040 --> 00:24:38,650
propagated everywhere and the second bit

00:24:36,760 --> 00:24:40,750
so the repair job this is something you

00:24:38,650 --> 00:24:42,580
run a independently across the whole

00:24:40,750 --> 00:24:44,410
cluster this makes sure that all the

00:24:42,580 --> 00:24:46,030
rice were propagated this allows you

00:24:44,410 --> 00:24:48,940
time on actual right because you can

00:24:46,030 --> 00:24:50,200
just pry to a quorum and then you run a

00:24:48,940 --> 00:24:51,790
repair job would make sure that this

00:24:50,200 --> 00:24:55,360
right has been propagated to everybody

00:24:51,790 --> 00:24:57,790
and this took so we decreased the grace

00:24:55,360 --> 00:25:00,850
period so terms of state for shorter so

00:24:57,790 --> 00:25:02,110
we had lower impact and we had frequent

00:25:00,850 --> 00:25:03,220
more frequent repairs for that reason

00:25:02,110 --> 00:25:06,040
because you need to repair more

00:25:03,220 --> 00:25:07,390
frequently even your tombstone lives so

00:25:06,040 --> 00:25:09,100
if you tube the lives for 10 days you

00:25:07,390 --> 00:25:10,780
should repair every 8 the whole class

00:25:09,100 --> 00:25:13,299
forever eight days so what your

00:25:10,780 --> 00:25:15,669
tombstone doesn't go away before you've

00:25:13,299 --> 00:25:17,980
propagated the delete so we shorten that

00:25:15,669 --> 00:25:22,360
period to favor smaller but more

00:25:17,980 --> 00:25:25,600
frequent repairs this kind of made that

00:25:22,360 --> 00:25:27,400
our trimming also much more effective we

00:25:25,600 --> 00:25:29,290
also looked at the service and we use

00:25:27,400 --> 00:25:33,750
something which i guess is referred to

00:25:29,290 --> 00:25:36,460
as a bulkhead pattern so it was not it

00:25:33,750 --> 00:25:38,679
it's effectively time outs but its

00:25:36,460 --> 00:25:40,929
surface side so basically the service

00:25:38,679 --> 00:25:44,320
controls how much it longs runs the

00:25:40,929 --> 00:25:46,299
operation so it this way we for each

00:25:44,320 --> 00:25:49,480
logical operations I deliver a message

00:25:46,299 --> 00:25:52,030
read a read and read count read the

00:25:49,480 --> 00:25:54,580
inbox we allocated a separate thread

00:25:52,030 --> 00:25:56,500
pool for that operation and we guarded

00:25:54,580 --> 00:25:58,330
it with a timeout so that if that

00:25:56,500 --> 00:26:00,340
operation is running for too long we

00:25:58,330 --> 00:26:04,030
basically cancel it regardless of the

00:26:00,340 --> 00:26:05,260
time out on the client and on the

00:26:04,030 --> 00:26:08,620
application we're all out circuit

00:26:05,260 --> 00:26:13,870
breakers so i will go explain this in

00:26:08,620 --> 00:26:17,850
more detail so is this fixed now is this

00:26:13,870 --> 00:26:20,559
the end of all of that well more or less

00:26:17,850 --> 00:26:24,010
we still have some problems with running

00:26:20,559 --> 00:26:25,960
repairs successfully all the time and we

00:26:24,010 --> 00:26:28,030
also seen things like a single button

00:26:25,960 --> 00:26:31,660
owed by bad i don't mean dead because

00:26:28,030 --> 00:26:33,750
that's relatively easy I mean slow so if

00:26:31,660 --> 00:26:36,100
one which things is ok but it isn't

00:26:33,750 --> 00:26:39,190
taking down the whole cluster we think

00:26:36,100 --> 00:26:41,169
it's basically because the it is

00:26:39,190 --> 00:26:42,160
basically replay of what we saw with our

00:26:41,169 --> 00:26:44,350
system

00:26:42,160 --> 00:26:46,330
basically the nodes the node response so

00:26:44,350 --> 00:26:48,610
the other nodes wait for it but it

00:26:46,330 --> 00:26:51,460
responds so slowly but they basically

00:26:48,610 --> 00:26:53,980
exhaust their resources but it's not so

00:26:51,460 --> 00:26:55,390
trivial to tweak and it's actually not

00:26:53,980 --> 00:26:57,220
very easy to reproduce because it

00:26:55,390 --> 00:26:59,710
theoretically shouldn't happen but we

00:26:57,220 --> 00:27:03,220
saw it happen so we have an ongoing task

00:26:59,710 --> 00:27:06,910
to inject latency and reproduce it to

00:27:03,220 --> 00:27:08,980
actually figure out how to solve it but

00:27:06,910 --> 00:27:10,210
what's important is that even if we

00:27:08,980 --> 00:27:13,570
haven't solved all the problems with the

00:27:10,210 --> 00:27:15,100
backend a problem of Cassandra order

00:27:13,570 --> 00:27:18,310
service doesn't take the side down

00:27:15,100 --> 00:27:22,300
because it's me has decent kind of

00:27:18,310 --> 00:27:25,420
failure isolation and the bulk heading

00:27:22,300 --> 00:27:27,820
pattern means but only the users whose

00:27:25,420 --> 00:27:30,280
data lies on the problematic nodes are

00:27:27,820 --> 00:27:32,770
actually affected so what do I mean by

00:27:30,280 --> 00:27:34,450
this and how this is achieved well let's

00:27:32,770 --> 00:27:36,760
say we have a 10 note cluster and three

00:27:34,450 --> 00:27:39,430
nodes for some reason are in a bad state

00:27:36,760 --> 00:27:41,880
so if you have so that's roughly

00:27:39,430 --> 00:27:45,850
accounts for thirty percent of the data

00:27:41,880 --> 00:27:48,430
so in the past that would effectively

00:27:45,850 --> 00:27:49,780
mean because of faults in the service

00:27:48,430 --> 00:27:52,060
design that took me that's still all

00:27:49,780 --> 00:27:53,470
users would be affected now we actually

00:27:52,060 --> 00:27:54,850
observe that if that happens if that can

00:27:53,470 --> 00:27:57,250
happen but three notes for some reason

00:27:54,850 --> 00:28:01,780
bad memory etc we run our own DC so we

00:27:57,250 --> 00:28:03,670
deal with that and we have only thirty

00:28:01,780 --> 00:28:05,380
percent of the users affected so the

00:28:03,670 --> 00:28:06,970
ones whose data isn't good nose don't

00:28:05,380 --> 00:28:09,760
see it because they're thanks to the

00:28:06,970 --> 00:28:12,220
bulkhead they're isolated and the bucket

00:28:09,760 --> 00:28:14,170
works the following way so we know

00:28:12,220 --> 00:28:16,090
what's our peak traffic we know what's

00:28:14,170 --> 00:28:19,720
the worst-case latency we're willing to

00:28:16,090 --> 00:28:21,760
accept so we sighs the fred pohl in a

00:28:19,720 --> 00:28:24,850
such a way but we have enough threads in

00:28:21,760 --> 00:28:26,950
during the worst latency so even if some

00:28:24,850 --> 00:28:28,540
requests are timing out it doesn't

00:28:26,950 --> 00:28:30,250
exhaust the Fred bulb and there are

00:28:28,540 --> 00:28:34,630
threads available for the other requests

00:28:30,250 --> 00:28:38,110
roughly speaking so what worked well for

00:28:34,630 --> 00:28:40,690
us with this rollout our methodology

00:28:38,110 --> 00:28:42,730
proved to be quite successful I guess so

00:28:40,690 --> 00:28:44,230
we have the integration test which

00:28:42,730 --> 00:28:47,050
allowed us to ship to prod with

00:28:44,230 --> 00:28:49,630
confidence we had data modification

00:28:47,050 --> 00:28:52,390
model modifications which we took about

00:28:49,630 --> 00:28:55,120
an hour two hours to implement and then

00:28:52,390 --> 00:28:55,510
we two and a half hours later it was in

00:28:55,120 --> 00:28:57,940
product

00:28:55,510 --> 00:29:00,760
so that's pretty good I guess the

00:28:57,940 --> 00:29:02,620
shuttle deploy and double dispatch gave

00:29:00,760 --> 00:29:05,740
has a great feedback on design from the

00:29:02,620 --> 00:29:08,650
actual traffic's migration gave us peak

00:29:05,740 --> 00:29:11,920
performance feedback and existing

00:29:08,650 --> 00:29:13,780
metrics and analytics were very informal

00:29:11,920 --> 00:29:17,650
like give us loads of information to

00:29:13,780 --> 00:29:19,480
inform our design choices and I don't

00:29:17,650 --> 00:29:21,040
want that's actually I think that was

00:29:19,480 --> 00:29:23,470
really important the easy to run

00:29:21,040 --> 00:29:25,090
migration really allowed us to move from

00:29:23,470 --> 00:29:27,340
one model to another very quickly and

00:29:25,090 --> 00:29:30,340
this was not a problem effectively so

00:29:27,340 --> 00:29:33,280
that's it so instead of trying to avoid

00:29:30,340 --> 00:29:35,350
problems we kind of allowed ourselves to

00:29:33,280 --> 00:29:37,600
quickly deal with them I guess this was

00:29:35,350 --> 00:29:40,600
the the trade-off we did and that pretty

00:29:37,600 --> 00:29:42,340
successful what we've learned however is

00:29:40,600 --> 00:29:45,730
that even for big my organization

00:29:42,340 --> 00:29:48,180
Yammer's I guess now 350 400 people

00:29:45,730 --> 00:29:50,170
probably about a hundred engineers

00:29:48,180 --> 00:29:53,590
introducing a new technology has a high

00:29:50,170 --> 00:29:55,870
cost getting a working model in

00:29:53,590 --> 00:29:58,540
production to Conley three months even

00:29:55,870 --> 00:30:00,220
though the actual product last a bit

00:29:58,540 --> 00:30:02,710
longer but this was due to dealing with

00:30:00,220 --> 00:30:05,890
legacy integration but getting like the

00:30:02,710 --> 00:30:07,540
technology out was relatively quick but

00:30:05,890 --> 00:30:09,480
what takes a lot of time and you have to

00:30:07,540 --> 00:30:11,710
commit your resources to it if you're a

00:30:09,480 --> 00:30:14,050
serious organization is ironing out the

00:30:11,710 --> 00:30:15,970
operations and that will take at least a

00:30:14,050 --> 00:30:18,670
air because you need to understand your

00:30:15,970 --> 00:30:21,460
system much in much more deeper detail

00:30:18,670 --> 00:30:23,800
than just rather feature and if you like

00:30:21,460 --> 00:30:25,660
if like yammering heaven s light would

00:30:23,800 --> 00:30:27,130
you commit you really to have a much

00:30:25,660 --> 00:30:28,990
deeper understanding than just throw up

00:30:27,130 --> 00:30:31,390
your feature you'll have a lot of

00:30:28,990 --> 00:30:35,140
firefighting and fixing and you also

00:30:31,390 --> 00:30:37,510
have to train people because even if you

00:30:35,140 --> 00:30:39,580
do it as we did in a cross-functional

00:30:37,510 --> 00:30:40,870
team there was a person from every bit

00:30:39,580 --> 00:30:45,550
of the stack including operations

00:30:40,870 --> 00:30:47,170
engineer involved well we don't have too

00:30:45,550 --> 00:30:48,700
many we have all the necessary resources

00:30:47,170 --> 00:30:50,620
then you have to propagate the knowledge

00:30:48,700 --> 00:30:53,530
across small operations engineers of

00:30:50,620 --> 00:30:57,480
small services people etc etc and that

00:30:53,530 --> 00:31:00,310
takes time as well and support helps and

00:30:57,480 --> 00:31:03,250
they did help data facts we support

00:31:00,310 --> 00:31:05,050
helped us a lot but above still holds so

00:31:03,250 --> 00:31:08,020
if you don't have support you will be in

00:31:05,050 --> 00:31:08,540
a worse situation I guess it kind of

00:31:08,020 --> 00:31:10,910
echo

00:31:08,540 --> 00:31:13,220
the stuff mentioned by a guy who gave a

00:31:10,910 --> 00:31:15,320
talk today from etsy but let's try to

00:31:13,220 --> 00:31:18,140
solve school problems using boring

00:31:15,320 --> 00:31:19,670
technology rather than introduce

00:31:18,140 --> 00:31:22,970
unnecessarily in this case we had a

00:31:19,670 --> 00:31:25,370
reason so but any technology introduced

00:31:22,970 --> 00:31:28,940
to your org will effectively mean this I

00:31:25,370 --> 00:31:33,460
would argue yeah so thank you this time

00:31:28,940 --> 00:31:33,460

YouTube URL: https://www.youtube.com/watch?v=yS3BiBsS6K4


