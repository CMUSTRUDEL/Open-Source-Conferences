Title: Berlin Buzzwords 2014: Michael Hausenblas - Lambda Architecture or How I Learned to Stop Worrying...
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	When Nathan Marz coined the term Lambda Architecture back in 2012 he might have only been in search for a somewhat sensical title for his upcoming book. No doubt, the Lambda Architecture has since gained traction, functioning as a blueprint to build large-scale, distributed data processing systems in a flexible and extensible manner. But it also turns out that there is a sometimes overlooked aspect of the Lambda Architecture: human fault tolerance. Humans make mistakes. Machines don't. Machines scale. Humans don't.

Read more:
https://2014.berlinbuzzwords.de/session/lambda-architecture-or-how-i-learned-stop-worrying-and-love-human-fault-tolerance

About Michael Hausenblas:
https://2014.berlinbuzzwords.de/user/303/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,520 --> 00:00:11,100
all right good afternoon my name is

00:00:10,059 --> 00:00:13,640
Michael Howsam glass

00:00:11,100 --> 00:00:15,830
chief data engineer at web Arctic

00:00:13,640 --> 00:00:18,630
and today I'm going to talk about

00:00:15,830 --> 00:00:23,460
or how i learned to stop worrying and

00:00:18,630 --> 00:00:26,849
love human fault tolerance so let's have

00:00:23,460 --> 00:00:29,520
a look at what fault tolerance might

00:00:26,849 --> 00:00:32,820
mean in a distributed system like Hadoop

00:00:29,520 --> 00:00:37,710
for example we generally consider the

00:00:32,820 --> 00:00:40,050
hardware being commodity hardware not

00:00:37,710 --> 00:00:43,290
that reliable right we compensate for

00:00:40,050 --> 00:00:46,290
that using software right whoever you

00:00:43,290 --> 00:00:48,750
look HDFS HBase whatever you have

00:00:46,290 --> 00:00:53,040
mechanisms in there that compensate for

00:00:48,750 --> 00:00:58,620
Hardware going down right but what about

00:00:53,040 --> 00:01:01,140
a developer let's talk about developers

00:00:58,620 --> 00:01:05,789
I'm not talking about that kind we know

00:01:01,140 --> 00:01:08,759
that I would argue it all started when

00:01:05,789 --> 00:01:12,210
we invited the elephant somehow we

00:01:08,759 --> 00:01:15,450
presenting Hadoop to the tea party so

00:01:12,210 --> 00:01:17,039
things got complicated you know you

00:01:15,450 --> 00:01:19,110
develop something in the previous talk

00:01:17,039 --> 00:01:21,360
if you've been around we saw some very

00:01:19,110 --> 00:01:23,400
good examples they're having a staging

00:01:21,360 --> 00:01:25,320
area having can erase around and so on

00:01:23,400 --> 00:01:28,640
and so forth but then at the end of the

00:01:25,320 --> 00:01:31,380
day you still experience problems and

00:01:28,640 --> 00:01:34,590
part of that talk is to raise awareness

00:01:31,380 --> 00:01:38,759
around that problem and the other part

00:01:34,590 --> 00:01:40,380
is a suggestion how to combat that we

00:01:38,759 --> 00:01:44,280
also know that and I'm not going to talk

00:01:40,380 --> 00:01:48,840
about this necktie interface problem I'm

00:01:44,280 --> 00:01:53,729
talking about this right worked fine in

00:01:48,840 --> 00:01:55,530
death it's an ops problem now so let's

00:01:53,729 --> 00:01:57,329
maybe talk about human fault currents

00:01:55,530 --> 00:02:01,430
rather than developers in general

00:01:57,329 --> 00:02:04,880
developers are smart motivated DevOps

00:02:01,430 --> 00:02:04,880
they know what they're doing

00:02:08,560 --> 00:02:15,050
most of the time so when things go wrong

00:02:12,140 --> 00:02:19,250
and they can go wrong at any scale we

00:02:15,050 --> 00:02:24,140
see that quite interesting things with

00:02:19,250 --> 00:02:29,050
her bangs even Google has these down

00:02:24,140 --> 00:02:33,140
types quite often something that when

00:02:29,050 --> 00:02:36,020
nicely and smoothly in a development

00:02:33,140 --> 00:02:38,210
environment maybe even in the staging

00:02:36,020 --> 00:02:40,280
environment somehow blows up into your

00:02:38,210 --> 00:02:45,170
face when you do it when you put it into

00:02:40,280 --> 00:02:47,660
production so I would argue from an

00:02:45,170 --> 00:02:50,000
architectural level if we can provide

00:02:47,660 --> 00:02:52,850
some guidance there if we can equip

00:02:50,000 --> 00:02:56,810
people with a way of thinking about

00:02:52,850 --> 00:02:59,000
systems at least half of that challenge

00:02:56,810 --> 00:03:02,990
is already addressed so let's step back

00:02:59,000 --> 00:03:06,230
a bit the lambda architecture at least

00:03:02,990 --> 00:03:08,810
the name the idea was probably around

00:03:06,230 --> 00:03:11,240
for for quite some time ready was

00:03:08,810 --> 00:03:13,910
established by a smart used to work at

00:03:11,240 --> 00:03:16,190
Twitter early on back type and the

00:03:13,910 --> 00:03:18,890
creator of many goodies out there storm

00:03:16,190 --> 00:03:20,300
and Kass clapping two examples he

00:03:18,890 --> 00:03:22,040
started to ride on a book and if you

00:03:20,300 --> 00:03:24,980
haven't checked it out yet you might

00:03:22,040 --> 00:03:28,250
want to it's available under this URL

00:03:24,980 --> 00:03:30,890
and you had to come up with some name

00:03:28,250 --> 00:03:33,110
for that architecture so it chose to

00:03:30,890 --> 00:03:37,280
name it lambda architecture and why that

00:03:33,110 --> 00:03:38,720
is so we will see in a minute so looking

00:03:37,280 --> 00:03:41,660
at the experience he had both the back

00:03:38,720 --> 00:03:43,940
type and Twitter he essentially or his

00:03:41,660 --> 00:03:46,430
team andent himself put together a

00:03:43,940 --> 00:03:48,200
number of requirements they thought

00:03:46,430 --> 00:03:50,540
would be desirable to have for such a

00:03:48,200 --> 00:03:52,430
system it should be fault tolerance

00:03:50,540 --> 00:03:54,920
against both the hardware failures or

00:03:52,430 --> 00:03:58,519
well and human errors that's probably

00:03:54,920 --> 00:04:01,580
something you obviously if you look at

00:03:58,519 --> 00:04:05,030
the workload twitter has not only this

00:04:01,580 --> 00:04:08,060
batch mode should be supported but also

00:04:05,030 --> 00:04:11,030
a low latency query it should have the

00:04:08,060 --> 00:04:13,420
capability of scaling outliner ii so

00:04:11,030 --> 00:04:16,430
throwing more commodity hardware i did

00:04:13,420 --> 00:04:18,739
and it should also be extensible right

00:04:16,430 --> 00:04:21,200
the business requirements might change

00:04:18,739 --> 00:04:22,850
your environment might change your user

00:04:21,200 --> 00:04:25,880
bass might change so you want to have an

00:04:22,850 --> 00:04:27,490
accessible system being able to not only

00:04:25,880 --> 00:04:30,920
manage it and operate it but also

00:04:27,490 --> 00:04:33,980
accommodate new features and that's what

00:04:30,920 --> 00:04:36,290
he ended up with suggesting and if you

00:04:33,980 --> 00:04:38,540
look at that part that pretty much if

00:04:36,290 --> 00:04:40,520
you turn your head 90 degrees looks like

00:04:38,540 --> 00:04:42,800
a lambda right that's why it's called

00:04:40,520 --> 00:04:48,100
lambda architecture so you essentially

00:04:42,800 --> 00:04:50,390
have new data streaming in here which is

00:04:48,100 --> 00:04:52,550
presented to or offered to both the

00:04:50,390 --> 00:04:56,480
batch layer on the one hand and the

00:04:52,550 --> 00:05:00,710
speed layer in the batch layer you have

00:04:56,480 --> 00:05:03,740
an immutable master data set and a batch

00:05:00,710 --> 00:05:05,870
process that computes the views I come

00:05:03,740 --> 00:05:08,060
to an example and concrete

00:05:05,870 --> 00:05:12,200
implementations in a minute so bear with

00:05:08,060 --> 00:05:14,240
me for now just trying to appreciate

00:05:12,200 --> 00:05:17,600
some of the key terms like this

00:05:14,240 --> 00:05:21,260
immutable master data set on the other

00:05:17,600 --> 00:05:24,100
hand you have the speed layer that

00:05:21,260 --> 00:05:26,720
essentially processes the stream and

00:05:24,100 --> 00:05:29,570
updates the real-time views and then you

00:05:26,720 --> 00:05:32,300
want to combine these two to satisfy any

00:05:29,570 --> 00:05:35,540
given query right very very

00:05:32,300 --> 00:05:39,320
straightforward and simple at least on

00:05:35,540 --> 00:05:44,480
an architectural level so as I said the

00:05:39,320 --> 00:05:46,480
batch layer the main task there is to

00:05:44,480 --> 00:05:49,400
manage that master date and said

00:05:46,480 --> 00:05:51,950
hopefully because they're all your your

00:05:49,400 --> 00:05:54,920
data should be available in a reliable

00:05:51,950 --> 00:05:56,990
way and given the scale of the operation

00:05:54,920 --> 00:06:01,550
it should be you know typically

00:05:56,990 --> 00:06:03,020
distributed master data set and it

00:06:01,550 --> 00:06:06,380
should be able to pre-compute any

00:06:03,020 --> 00:06:08,630
arbitrary query yeah that one could

00:06:06,380 --> 00:06:10,820
imagine and the result of that are the

00:06:08,630 --> 00:06:14,180
batch views and the certain layer

00:06:10,820 --> 00:06:16,130
essentially indexes these views and can

00:06:14,180 --> 00:06:18,340
be cured in an ad hoc fashion so there

00:06:16,130 --> 00:06:21,260
you have the low latency realized and

00:06:18,340 --> 00:06:25,280
the speed layer essentially and I come

00:06:21,260 --> 00:06:29,810
back to it this slide compensates for

00:06:25,280 --> 00:06:33,170
the latency of the batch layer so the

00:06:29,810 --> 00:06:34,610
bench layer let's say runs once a day or

00:06:33,170 --> 00:06:36,740
every two hours and all the

00:06:34,610 --> 00:06:40,280
data that has arrived between the last

00:06:36,740 --> 00:06:43,789
run and now which has not been absorbed

00:06:40,280 --> 00:06:47,629
into the master data set is catered

00:06:43,789 --> 00:06:50,449
forest served by speed layer okay so

00:06:47,629 --> 00:06:55,280
that's the kind of the things that how

00:06:50,449 --> 00:06:58,189
the architecture deals with ya the

00:06:55,280 --> 00:07:01,280
incoming data and if you think about

00:06:58,189 --> 00:07:04,370
that if you deploy if you're a developer

00:07:01,280 --> 00:07:09,620
and you have something some some bug in

00:07:04,370 --> 00:07:11,750
there it even if that back shows it only

00:07:09,620 --> 00:07:15,529
shows for that time the next time the

00:07:11,750 --> 00:07:17,479
bed run goes over that you're may be

00:07:15,529 --> 00:07:20,539
inconsistent data or whatever goes away

00:07:17,479 --> 00:07:22,789
right the batch layer essentially always

00:07:20,539 --> 00:07:24,800
takes the entire master data set into

00:07:22,789 --> 00:07:28,219
account so let's have a look at a

00:07:24,800 --> 00:07:29,479
concrete example took that from open

00:07:28,219 --> 00:07:31,699
flight store org because they have to

00:07:29,479 --> 00:07:33,860
date available you might want to do

00:07:31,699 --> 00:07:36,800
something else there but for something

00:07:33,860 --> 00:07:38,779
concrete and mutual master data set and

00:07:36,800 --> 00:07:42,589
you have always these time stems there

00:07:38,779 --> 00:07:45,259
and that essentially says this slide eii

00:07:42,589 --> 00:07:48,289
123 took off in Dublin at a certain

00:07:45,259 --> 00:07:51,919
point in time and you got that for all

00:07:48,289 --> 00:07:55,520
the others and you always ever append

00:07:51,919 --> 00:07:58,969
you never update anything in place so

00:07:55,520 --> 00:08:03,050
rather than updating for the next day

00:07:58,969 --> 00:08:05,000
this record here you would upend another

00:08:03,050 --> 00:08:07,400
record there you always keep the data

00:08:05,000 --> 00:08:10,039
around in its rawest form you never

00:08:07,400 --> 00:08:12,110
update anything in place the implication

00:08:10,039 --> 00:08:17,210
being you better have a good data

00:08:12,110 --> 00:08:20,180
platform that can handle that data okay

00:08:17,210 --> 00:08:23,449
so let's have a look at the fuse so one

00:08:20,180 --> 00:08:25,759
very very simple view well I had that

00:08:23,449 --> 00:08:28,430
one sentence in mind that counting is

00:08:25,759 --> 00:08:31,159
actually not a very simple operation but

00:08:28,430 --> 00:08:33,949
a very simple way of looking at things

00:08:31,159 --> 00:08:36,140
is I'm asking how many airport how many

00:08:33,949 --> 00:08:38,719
planes are airborne right so I go

00:08:36,140 --> 00:08:41,169
through that list and all of them who

00:08:38,719 --> 00:08:44,810
have taken off but not landed yet are

00:08:41,169 --> 00:08:49,880
hopefully airborne

00:08:44,810 --> 00:08:53,399
yeah there might be exceptions or

00:08:49,880 --> 00:08:57,329
another view might be provide me an

00:08:53,399 --> 00:09:00,870
overview about airborne airplanes her

00:08:57,329 --> 00:09:04,019
airline so I would go over that and take

00:09:00,870 --> 00:09:05,910
into account the airline identifier

00:09:04,019 --> 00:09:08,730
there or another one would be the

00:09:05,910 --> 00:09:10,800
airport load so how this is an airport

00:09:08,730 --> 00:09:13,730
right but you get the idea you always

00:09:10,800 --> 00:09:17,550
look over the entire master data set and

00:09:13,730 --> 00:09:20,759
by applying a certain query you get the

00:09:17,550 --> 00:09:23,790
views that are then served by well

00:09:20,759 --> 00:09:26,819
serving there in case you want to learn

00:09:23,790 --> 00:09:29,339
more about lambda architecture we put

00:09:26,819 --> 00:09:33,839
together a website called lambda dead

00:09:29,339 --> 00:09:37,110
dosch adesh architecture net where we're

00:09:33,839 --> 00:09:39,540
trying to document implementations use

00:09:37,110 --> 00:09:40,589
cases and so on and if you're in a

00:09:39,540 --> 00:09:43,050
position that you have already

00:09:40,589 --> 00:09:45,089
implemented this lambda architecture

00:09:43,050 --> 00:09:48,600
somewhere please let me know or my

00:09:45,089 --> 00:09:50,100
colleague Nathan Mason violence and

00:09:48,600 --> 00:09:52,290
we're happy to put that there as well

00:09:50,100 --> 00:09:56,910
it's really meant to serve the community

00:09:52,290 --> 00:09:59,100
to ya document good practices so coming

00:09:56,910 --> 00:10:00,750
to implementation of the land or

00:09:59,100 --> 00:10:03,269
architecture so far you know an

00:10:00,750 --> 00:10:04,920
architecture that's nice that's not very

00:10:03,269 --> 00:10:07,230
useful you want to implement that right

00:10:04,920 --> 00:10:10,439
you've got a deadline you've got people

00:10:07,230 --> 00:10:12,000
and you want to implement that so one of

00:10:10,439 --> 00:10:13,829
the things we're doing there on that

00:10:12,000 --> 00:10:16,560
website and the advocacy website is

00:10:13,829 --> 00:10:18,779
essentially listing the components that

00:10:16,560 --> 00:10:21,839
you could use for realizing certain

00:10:18,779 --> 00:10:24,120
layers that alone is probably not too

00:10:21,839 --> 00:10:27,449
useful you still need some experience

00:10:24,120 --> 00:10:29,069
around that and a gun essentially

00:10:27,449 --> 00:10:31,259
through a number of architectures are

00:10:29,069 --> 00:10:36,420
found in our user base and and outside

00:10:31,259 --> 00:10:38,790
and one of these things that somehow is

00:10:36,420 --> 00:10:41,759
a recurrent pattern is not very

00:10:38,790 --> 00:10:45,959
surprisingly that for you to master data

00:10:41,759 --> 00:10:50,180
set hdfs is used for the bachelor quite

00:10:45,959 --> 00:10:50,180
often life is used or pig and wife

00:10:50,839 --> 00:10:58,250
sorry Fred in the speed layer you would

00:10:55,920 --> 00:11:03,120
typically see Kafka in front of storm

00:10:58,250 --> 00:11:06,209
right so these are things that we quite

00:11:03,120 --> 00:11:09,060
often find in implementations and then

00:11:06,209 --> 00:11:11,639
the serving layer would for example be

00:11:09,060 --> 00:11:14,639
realized with page base where you merge

00:11:11,639 --> 00:11:19,439
both the badge view and the real-time

00:11:14,639 --> 00:11:23,970
view right so who besides Ted can spot

00:11:19,439 --> 00:11:27,540
the problem with this approach using

00:11:23,970 --> 00:11:29,040
hive deploying a storm topology and so

00:11:27,540 --> 00:11:31,199
on and so forth I'm not saying it's

00:11:29,040 --> 00:11:34,009
impossible obviously customers have that

00:11:31,199 --> 00:11:37,170
in production but what could you imagine

00:11:34,009 --> 00:11:40,379
also in terms of minimizing the

00:11:37,170 --> 00:11:42,930
potential errors someone could you know

00:11:40,379 --> 00:11:45,410
introduce there what's the problem with

00:11:42,930 --> 00:11:49,790
this approach having many many different

00:11:45,410 --> 00:11:52,649
systems environments languages and so on

00:11:49,790 --> 00:11:54,990
well the problem is that you're

00:11:52,649 --> 00:11:57,839
essentially repeating the business logic

00:11:54,990 --> 00:12:00,120
both in the badge layer and in the speed

00:11:57,839 --> 00:12:03,300
layer once you write your hive query

00:12:00,120 --> 00:12:05,759
once you write your storm topology in

00:12:03,300 --> 00:12:09,540
Java it's the same business logic

00:12:05,759 --> 00:12:10,980
implemented including testing and so on

00:12:09,540 --> 00:12:14,550
and so forth you're essentially

00:12:10,980 --> 00:12:17,130
maintaining parallel and illogical level

00:12:14,550 --> 00:12:18,689
parallel things which leads us to the

00:12:17,130 --> 00:12:20,550
question how about an integrated

00:12:18,689 --> 00:12:23,310
approach is there anything out there

00:12:20,550 --> 00:12:26,459
that allows me to do to implement the

00:12:23,310 --> 00:12:30,029
lambda architecture using one framework

00:12:26,459 --> 00:12:33,240
on language one platform so there's

00:12:30,029 --> 00:12:37,160
Twitter summing bird real least end of

00:12:33,240 --> 00:12:39,600
last year which is a very nice approach

00:12:37,160 --> 00:12:42,089
not going to comment on that further

00:12:39,600 --> 00:12:44,639
just pointing it out it is there there

00:12:42,089 --> 00:12:46,740
is lamb dupe which I believe will soon

00:12:44,639 --> 00:12:50,309
be open sourced you can request a demo

00:12:46,740 --> 00:12:52,199
there on their side and their spark who

00:12:50,309 --> 00:12:55,139
a few have heard about spark already

00:12:52,199 --> 00:12:58,709
apache spark cool

00:12:55,139 --> 00:13:02,429
so I'm going to argue that apache spark

00:12:58,709 --> 00:13:04,739
is probably the best way based on our

00:13:02,429 --> 00:13:08,759
experience to implement the lambda

00:13:04,739 --> 00:13:11,040
architecture recent being that it allows

00:13:08,759 --> 00:13:15,089
you to pick your language scallop eyes

00:13:11,040 --> 00:13:17,699
know whatever develop both sides the

00:13:15,089 --> 00:13:20,549
batchview and the speed a data stream

00:13:17,699 --> 00:13:23,819
screaming sidereal time views using one

00:13:20,549 --> 00:13:26,220
framework one paradigm so bit of a

00:13:23,819 --> 00:13:28,169
background to spark initially developed

00:13:26,220 --> 00:13:30,569
by the emblem folks we heard it in the

00:13:28,169 --> 00:13:33,889
earlier talk also message is quite close

00:13:30,569 --> 00:13:36,209
to them there early this year it got

00:13:33,889 --> 00:13:39,600
promoted into top level Apache project

00:13:36,209 --> 00:13:43,019
and the kind of commercial shepherds our

00:13:39,600 --> 00:13:44,790
data breaks and you can get enterprise

00:13:43,019 --> 00:13:49,529
support from Hadoop distributions such

00:13:44,790 --> 00:13:53,269
as this the spark community has actually

00:13:49,529 --> 00:13:56,879
pretty rapidly grown and there you see

00:13:53,269 --> 00:14:00,329
contributors committers users people who

00:13:56,879 --> 00:14:03,660
in generally do something with spark by

00:14:00,329 --> 00:14:07,139
to contribute and or use and from a kind

00:14:03,660 --> 00:14:09,720
of hundred thousand feet view these deck

00:14:07,139 --> 00:14:11,939
looks like that so you got a data

00:14:09,720 --> 00:14:14,879
platform and spark in a sense is rather

00:14:11,939 --> 00:14:16,230
agnostic to that it says I don't care

00:14:14,879 --> 00:14:20,009
where the data comes from could be as

00:14:16,230 --> 00:14:22,439
free could be H differs whatever but at

00:14:20,009 --> 00:14:25,379
the end of the day you need that data

00:14:22,439 --> 00:14:29,220
platform you need something that you

00:14:25,379 --> 00:14:32,419
know for your master dataset is you have

00:14:29,220 --> 00:14:35,369
an execution environment again very

00:14:32,419 --> 00:14:37,559
flexible you can have mesos you can have

00:14:35,369 --> 00:14:40,199
yarn you standalone mode where you don't

00:14:37,559 --> 00:14:43,559
need any other framework for that that

00:14:40,199 --> 00:14:48,449
essentially yeah execute the spark core

00:14:43,559 --> 00:14:51,179
engine and there's a ecosystem and as

00:14:48,449 --> 00:14:55,980
you can see there are things like sparks

00:14:51,179 --> 00:14:58,019
equal or shark in its previous form a

00:14:55,980 --> 00:14:59,910
streaming part which is micro batch

00:14:58,019 --> 00:15:01,709
you've got a machine learning part here

00:14:59,910 --> 00:15:03,929
you've graphics and number of other

00:15:01,709 --> 00:15:06,029
upcoming things there so this allows you

00:15:03,929 --> 00:15:08,189
if you look back at the challenge we had

00:15:06,029 --> 00:15:08,940
they're implementing the business logic

00:15:08,189 --> 00:15:12,180
both on

00:15:08,940 --> 00:15:14,910
betch layer and on the speed layer to do

00:15:12,180 --> 00:15:17,820
that with one framework again click your

00:15:14,910 --> 00:15:21,810
language currently there are three of

00:15:17,820 --> 00:15:24,590
them supported it's pison Scala Sparky's

00:15:21,810 --> 00:15:28,200
itself is written in Scala and Java and

00:15:24,590 --> 00:15:31,650
sorry as I know there are more to come

00:15:28,200 --> 00:15:35,700
up so on the one hand at least to me

00:15:31,650 --> 00:15:38,340
spark has a bit this MongoDB feeling you

00:15:35,700 --> 00:15:40,080
download it and you immediately get

00:15:38,340 --> 00:15:44,000
something that you immediately can do

00:15:40,080 --> 00:15:46,620
some cool stuff it is in this range of

00:15:44,000 --> 00:15:49,730
small data something that probably fits

00:15:46,620 --> 00:15:52,920
on that laptop or on my mobile phone to

00:15:49,730 --> 00:15:56,400
mid-sized data to a large data something

00:15:52,920 --> 00:15:59,580
that very nicely also addresses this low

00:15:56,400 --> 00:16:02,460
end part this mid-sized data part where

00:15:59,580 --> 00:16:04,020
with a MapReduce implementation you

00:16:02,460 --> 00:16:06,900
might sometimes run into a problem

00:16:04,020 --> 00:16:09,300
justifying that it also has a very

00:16:06,900 --> 00:16:11,910
expressive API we know that app API from

00:16:09,300 --> 00:16:19,920
good old map reduce its map and reduce

00:16:11,910 --> 00:16:24,150
right however that's what spark offers

00:16:19,920 --> 00:16:27,270
so a very expressive API many of the

00:16:24,150 --> 00:16:29,580
things that in MapReduce you would

00:16:27,270 --> 00:16:32,970
essentially either defer to high level

00:16:29,580 --> 00:16:36,960
languages like high for pig or cascading

00:16:32,970 --> 00:16:44,850
or whatever you get directly from apache

00:16:36,960 --> 00:16:46,820
spark there right and we've again put

00:16:44,850 --> 00:16:50,250
together and advocacy side for that

00:16:46,820 --> 00:16:54,089
sparks teknorg we're trying to keep up

00:16:50,250 --> 00:16:59,280
with the news that come around that

00:16:54,089 --> 00:17:06,240
spark sec and now i have still some time

00:16:59,280 --> 00:17:09,110
for questions answers which i hope we

00:17:06,240 --> 00:17:09,110
can enjoy

00:17:21,280 --> 00:17:25,790
and have you ever tried using

00:17:24,050 --> 00:17:29,240
stratosphere and how would you compare

00:17:25,790 --> 00:17:31,040
it to spark like couple of years ago we

00:17:29,240 --> 00:17:33,560
had a talk about stratosphere and that

00:17:31,040 --> 00:17:36,050
game try to make a point of comparing

00:17:33,560 --> 00:17:39,320
with spark what you can say about right

00:17:36,050 --> 00:17:41,390
so the question was if I or we have used

00:17:39,320 --> 00:17:45,710
stratosphere I personally haven't i'm

00:17:41,390 --> 00:17:48,730
looking at tap ok would be very

00:17:45,710 --> 00:17:51,830
interesting to see that comparison there

00:17:48,730 --> 00:17:57,410
have you got some previous experience

00:17:51,830 --> 00:17:59,900
with stratosphere yourself or sorry just

00:17:57,410 --> 00:18:02,360
a bit ok and if you contrast that with

00:17:59,900 --> 00:18:13,600
what i showed so far here would you say

00:18:02,360 --> 00:18:16,670
there is a one-to-one fit sorry for

00:18:13,600 --> 00:18:20,720
sending you run try to keep it a bit

00:18:16,670 --> 00:18:23,270
interactive ok yeah it has a strong

00:18:20,720 --> 00:18:26,210
support for iterative algorithms so

00:18:23,270 --> 00:18:29,810
basically it's a strong feature i would

00:18:26,210 --> 00:18:33,850
say and then also they emphasize the

00:18:29,810 --> 00:18:37,160
optimizer which i'm not sure how spark

00:18:33,850 --> 00:18:39,170
is comparison this respect right but

00:18:37,160 --> 00:18:41,510
that's is that I would be very

00:18:39,170 --> 00:18:44,480
interested in having your comparison and

00:18:41,510 --> 00:18:47,300
I think Ted also has something that you

00:18:44,480 --> 00:18:50,230
get back right we're running a fairly

00:18:47,300 --> 00:18:52,340
large Hadoop cluster and it's

00:18:50,230 --> 00:18:56,960
operationally fairly heavy we spend a

00:18:52,340 --> 00:18:58,970
lot of time keeping it running spark

00:18:56,960 --> 00:19:00,650
similar in that aspect is my first

00:18:58,970 --> 00:19:04,760
question and the follow-up question is

00:19:00,650 --> 00:19:08,450
if spark is easier to operate you still

00:19:04,760 --> 00:19:11,480
need some storage layer like HDFS if you

00:19:08,450 --> 00:19:13,520
want to avoid the operational elephant

00:19:11,480 --> 00:19:15,890
of Hadoop or where you have suggestions

00:19:13,520 --> 00:19:18,830
for other options right right I guess

00:19:15,890 --> 00:19:23,430
that was even though i only made this

00:19:18,830 --> 00:19:26,700
point shortly so this is

00:19:23,430 --> 00:19:29,130
you for example download apache spark

00:19:26,700 --> 00:19:30,690
from the apache web site or it comes

00:19:29,130 --> 00:19:35,460
with memoir wherever you want to get it

00:19:30,690 --> 00:19:37,350
this is what you get in its core but in

00:19:35,460 --> 00:19:39,450
order to operate it as you rightly

00:19:37,350 --> 00:19:41,610
pointed out you want that so the

00:19:39,450 --> 00:19:45,060
question is for example if things like

00:19:41,610 --> 00:19:47,460
high availability disaster recovery and

00:19:45,060 --> 00:19:49,230
so on are important to you then I would

00:19:47,460 --> 00:19:51,750
say well you probably want to have a

00:19:49,230 --> 00:19:54,900
data platform that really can offer that

00:19:51,750 --> 00:19:56,850
we can deliver on that so here in the

00:19:54,900 --> 00:19:59,430
execution environment you have got a lot

00:19:56,850 --> 00:20:01,410
of choices right so you might want to go

00:19:59,430 --> 00:20:03,360
with yarn you might have been around in

00:20:01,410 --> 00:20:05,610
the earlier talk want to try out methods

00:20:03,360 --> 00:20:07,530
there or for certain applications the

00:20:05,610 --> 00:20:09,000
standalone mode might be sufficient

00:20:07,530 --> 00:20:12,240
depending on if you have different

00:20:09,000 --> 00:20:17,120
workloads in your cluster or not but I

00:20:12,240 --> 00:20:20,670
would argue that many of the enterprise

00:20:17,120 --> 00:20:23,790
demands and features such as hey J and D

00:20:20,670 --> 00:20:28,760
are are probably met in these two layers

00:20:23,790 --> 00:20:33,570
I think Ted 12 sorry does it somehow

00:20:28,760 --> 00:20:41,850
yeah zooming in on the storage are there

00:20:33,570 --> 00:20:45,710
any alternatives to HDFS basically so

00:20:41,850 --> 00:20:48,930
yeah there are alternatives so there are

00:20:45,710 --> 00:20:51,510
commercial hardware alternatives EMC

00:20:48,930 --> 00:20:56,040
netapp both provide reasonable storage

00:20:51,510 --> 00:21:00,690
appliances and map are the guys with the

00:20:56,040 --> 00:21:03,930
buttons falling off we have high

00:21:00,690 --> 00:21:05,370
performance HDFS clone that provides a

00:21:03,930 --> 00:21:09,210
che that provides much easier

00:21:05,370 --> 00:21:12,870
operational setting than stock a dupe

00:21:09,210 --> 00:21:15,000
and gives you read write access but this

00:21:12,870 --> 00:21:16,410
is an open source conference so the

00:21:15,000 --> 00:21:18,660
speaker is trying to be very good and

00:21:16,410 --> 00:21:22,260
not say these things but we'll talk

00:21:18,660 --> 00:21:25,290
about it later if you want offline and

00:21:22,260 --> 00:21:27,840
spark itself is very easy to run yeah

00:21:25,290 --> 00:21:29,460
that yeah yeah I guess that's what I'm

00:21:27,840 --> 00:21:31,650
trying to point out with that's the

00:21:29,460 --> 00:21:33,210
MongoDB experience to me that says

00:21:31,650 --> 00:21:34,620
everything I don't mean it doesn't say

00:21:33,210 --> 00:21:36,060
to everyone but you know download it and

00:21:34,620 --> 00:21:36,820
you get at me I'm going to be

00:21:36,060 --> 00:21:38,909
experienced

00:21:36,820 --> 00:21:41,380
easy to install and it was data later

00:21:38,909 --> 00:21:44,440
which progress you get you get you get

00:21:41,380 --> 00:21:50,559
started easy right but Festus re-spark

00:21:44,440 --> 00:21:52,899
doesn't do that ok oh yeah from my

00:21:50,559 --> 00:21:55,630
experience with Spock we have been using

00:21:52,899 --> 00:21:59,860
not yet in full production but for a few

00:21:55,630 --> 00:22:02,049
months when it's a exactly it's very

00:21:59,860 --> 00:22:04,509
easy to use the share is great to learn

00:22:02,049 --> 00:22:07,179
it extra a main problem we had well

00:22:04,509 --> 00:22:10,409
that's difficult to tune the memory

00:22:07,179 --> 00:22:13,750
usage and it does not degrade gracefully

00:22:10,409 --> 00:22:16,960
when that that's a data sets are big

00:22:13,750 --> 00:22:19,720
sometimes the cash does end up a bit on

00:22:16,960 --> 00:22:22,149
the disk right and sometimes you just

00:22:19,720 --> 00:22:25,360
have out of memory errors vector on your

00:22:22,149 --> 00:22:29,019
horse relations yeah so the thing is I

00:22:25,360 --> 00:22:32,320
intentionally I I put in the second part

00:22:29,019 --> 00:22:34,000
around spark to kind of such as the best

00:22:32,320 --> 00:22:36,070
practice how the lambda architecture

00:22:34,000 --> 00:22:39,070
which itself might you know look rather

00:22:36,070 --> 00:22:40,809
abstract how you can realize that it's

00:22:39,070 --> 00:22:42,549
intentionally not talk about spark and

00:22:40,809 --> 00:22:44,919
unhappy too you know authentic that

00:22:42,549 --> 00:22:48,009
further about the the main problem or

00:22:44,919 --> 00:22:49,830
one of the architectural decisions there

00:22:48,009 --> 00:22:53,980
is that essentially the driver program

00:22:49,830 --> 00:22:56,019
in spark you know has has the whole

00:22:53,980 --> 00:22:58,419
control there and it's also you know

00:22:56,019 --> 00:23:02,710
consuming as much memory as as it can

00:22:58,419 --> 00:23:04,769
find let's put it that way so as far as

00:23:02,710 --> 00:23:09,309
I know there are a number of things

00:23:04,769 --> 00:23:11,159
coming up that might address that in may

00:23:09,309 --> 00:23:15,120
be already in one point oh I don't know

00:23:11,159 --> 00:23:20,470
but as you said this is something where

00:23:15,120 --> 00:23:24,129
there's still a lot of yeah community

00:23:20,470 --> 00:23:26,230
resources that need to be documented and

00:23:24,129 --> 00:23:28,120
shared necessary it's it's nothing that

00:23:26,230 --> 00:23:30,909
you know there's a simple and clear

00:23:28,120 --> 00:23:35,289
answer do XYZ it is as you said it's

00:23:30,909 --> 00:23:39,549
it's a known issue there hi I had a

00:23:35,289 --> 00:23:41,110
question up I yeah I I haven't really

00:23:39,549 --> 00:23:43,720
used spark much but I was playing around

00:23:41,110 --> 00:23:45,909
with it a couple of weeks ago and key

00:23:43,720 --> 00:23:49,059
phrase that kept coming up in the

00:23:45,909 --> 00:23:50,550
literature was in memory MapReduce right

00:23:49,059 --> 00:23:52,260
so that

00:23:50,550 --> 00:23:54,360
sort of made sense when I related little

00:23:52,260 --> 00:23:55,650
more but could you you know elaborate a

00:23:54,360 --> 00:23:57,390
little more on how it works when the

00:23:55,650 --> 00:24:00,330
data is really large and doesn't really

00:23:57,390 --> 00:24:02,280
fit in memory right right so yes the

00:24:00,330 --> 00:24:03,960
core idea are these are dd's to

00:24:02,280 --> 00:24:06,840
Switzerland destroy the data sets and

00:24:03,960 --> 00:24:09,240
again it's not a spark talk so that's

00:24:06,840 --> 00:24:11,460
why I spared out all these details so

00:24:09,240 --> 00:24:14,010
you essentially have a data set that

00:24:11,460 --> 00:24:16,500
apply some transformation and you apply

00:24:14,010 --> 00:24:18,990
the transformation resulting into a new

00:24:16,500 --> 00:24:21,030
data set you can pin these data sets

00:24:18,990 --> 00:24:23,610
into memory and start up to you know

00:24:21,030 --> 00:24:26,160
have it already cached which which is a

00:24:23,610 --> 00:24:29,430
lot of speed up the question still what

00:24:26,160 --> 00:24:31,590
if the data set does not fit in memory

00:24:29,430 --> 00:24:35,010
the simple answers it will spill to disk

00:24:31,590 --> 00:24:38,480
so it will become slow right that's what

00:24:35,010 --> 00:24:41,730
I said in terms of you might not have a

00:24:38,480 --> 00:24:45,030
big data problem but something that is

00:24:41,730 --> 00:24:47,010
where your standard relational database

00:24:45,030 --> 00:24:48,900
or whatever you're using essentially

00:24:47,010 --> 00:24:52,800
chokes on so you want to have something

00:24:48,900 --> 00:24:55,620
that as your data grows is able to deal

00:24:52,800 --> 00:25:01,020
with this mid-sized data but also you

00:24:55,620 --> 00:25:05,190
know you can crunch a bigger sizes of

00:25:01,020 --> 00:25:07,110
data sets as well and I've seen some of

00:25:05,190 --> 00:25:10,770
these migrations where you might have I

00:25:07,110 --> 00:25:13,770
don't know pig and hive script together

00:25:10,770 --> 00:25:15,720
and you might keep for you know going

00:25:13,770 --> 00:25:18,720
through 5 petabytes you might keep your

00:25:15,720 --> 00:25:21,780
pic script around there right because

00:25:18,720 --> 00:25:26,130
it's a bad job it runs once a day so you

00:25:21,780 --> 00:25:28,170
know the SLA is that whatever you need

00:25:26,130 --> 00:25:29,940
to guarantee that the job is done at a

00:25:28,170 --> 00:25:32,610
certain time or whatever is not that

00:25:29,940 --> 00:25:37,410
critical and in terms of throughput the

00:25:32,610 --> 00:25:40,200
MapReduce take by extension is still a

00:25:37,410 --> 00:25:43,410
very good way to go right for not saying

00:25:40,200 --> 00:25:45,930
that throw out head open MapReduce and

00:25:43,410 --> 00:25:48,810
whatever but have a look at if you have

00:25:45,930 --> 00:25:51,270
spark on the one hand and tests and

00:25:48,810 --> 00:25:53,490
other things on the other hand maybe

00:25:51,270 --> 00:25:54,840
spark can offer you more integrated

00:25:53,490 --> 00:25:56,640
approaches i call it there might be

00:25:54,840 --> 00:25:59,570
other people calling it different ways

00:25:56,640 --> 00:26:02,600
but more easier way

00:25:59,570 --> 00:26:05,370
also to address the last two comments

00:26:02,600 --> 00:26:08,000
one of the purposes of land architecture

00:26:05,370 --> 00:26:11,130
is that you don't run very large batches

00:26:08,000 --> 00:26:14,520
you run in the real time layer a very

00:26:11,130 --> 00:26:18,390
moderate load in the batch layer you run

00:26:14,520 --> 00:26:19,830
the last partition to in a batch mode so

00:26:18,390 --> 00:26:22,380
that you can always rerun that in a

00:26:19,830 --> 00:26:24,720
partitioned way and so you don't wind up

00:26:22,380 --> 00:26:28,110
having to handle a very very large

00:26:24,720 --> 00:26:31,370
amount of data in one in one single run

00:26:28,110 --> 00:26:34,730
and that makes spark a very natural

00:26:31,370 --> 00:26:37,050
component in lambda because of that and

00:26:34,730 --> 00:26:40,110
sparks equal i think is compatible with

00:26:37,050 --> 00:26:42,390
hive syntactically so you can mix and

00:26:40,110 --> 00:26:43,920
match the execution mode that's

00:26:42,390 --> 00:26:49,790
appropriate for whatever you're doing

00:26:43,920 --> 00:26:53,210
yep with one more question here yeah

00:26:49,790 --> 00:26:53,210
just throw it

00:27:00,190 --> 00:27:04,179
hey I one of the things you mentioned

00:27:02,350 --> 00:27:06,100
about the lambda architecture is that

00:27:04,179 --> 00:27:09,370
the there's an immutable storage it just

00:27:06,100 --> 00:27:11,259
goes up and only right right isn't that

00:27:09,370 --> 00:27:14,409
like sort of like an intrinsic scale

00:27:11,259 --> 00:27:16,509
issue there as over time you like even

00:27:14,409 --> 00:27:18,909
if your data set is like fairly fixed

00:27:16,509 --> 00:27:20,470
rate aren't you saying that you're two

00:27:18,909 --> 00:27:21,850
years from now three years from now

00:27:20,470 --> 00:27:23,049
you're going to have like an increasing

00:27:21,850 --> 00:27:25,320
and at some point you're going to hit a

00:27:23,049 --> 00:27:28,360
scale limitation so I guess are two

00:27:25,320 --> 00:27:32,470
answers to that sort of the question in

00:27:28,360 --> 00:27:34,179
case it's not clear at least as far as I

00:27:32,470 --> 00:27:36,909
understand isn't there a scalability

00:27:34,179 --> 00:27:40,269
issue somehow implying okay you need to

00:27:36,909 --> 00:27:43,269
keep all the data around right so on the

00:27:40,269 --> 00:27:44,889
one hand obviously or not so obvious we

00:27:43,269 --> 00:27:46,419
are dealing with schema and read in this

00:27:44,889 --> 00:27:48,250
setup right so the application

00:27:46,419 --> 00:27:50,500
essentially decides how to interpret

00:27:48,250 --> 00:27:52,570
these things which offers a lot of

00:27:50,500 --> 00:27:54,370
flexibility because what you saw today

00:27:52,570 --> 00:27:56,500
in this application is the right schema

00:27:54,370 --> 00:27:58,000
might change with a different use case

00:27:56,500 --> 00:28:00,070
or different business requirement

00:27:58,000 --> 00:28:04,120
whatever so that's definitely an upside

00:28:00,070 --> 00:28:05,769
the down downside is yes you need a very

00:28:04,120 --> 00:28:09,610
good data platform where you can keep

00:28:05,769 --> 00:28:12,669
all of that around right you might if

00:28:09,610 --> 00:28:14,379
you're really dealing with large you

00:28:12,669 --> 00:28:18,509
know the hundreds of pepper potts

00:28:14,379 --> 00:28:21,549
whatever might consider for example

00:28:18,509 --> 00:28:23,049
formats like parquet or org that in

00:28:21,549 --> 00:28:25,029
itself I have a more compact

00:28:23,049 --> 00:28:27,129
representation binary compact

00:28:25,029 --> 00:28:31,570
representation rather than a CSV file

00:28:27,129 --> 00:28:34,870
for example right you might want to if

00:28:31,570 --> 00:28:36,639
you say this to 10 year old data if you

00:28:34,870 --> 00:28:39,159
have it in operations for 10 years this

00:28:36,639 --> 00:28:42,669
10 year old data you can aggregate that

00:28:39,159 --> 00:28:45,490
a bit for example I would argue that

00:28:42,669 --> 00:28:47,500
there is not enough knowledge or

00:28:45,490 --> 00:28:49,210
experience in the community yet to

00:28:47,500 --> 00:28:50,889
really say okay here the three options

00:28:49,210 --> 00:28:53,169
how to deal with that but yes we're

00:28:50,889 --> 00:28:55,919
talking about a data platform that can

00:28:53,169 --> 00:29:01,090
easily you know store and manage

00:28:55,919 --> 00:29:02,610
petabytes of data which yeah hopefully

00:29:01,090 --> 00:29:07,269
is available

00:29:02,610 --> 00:29:13,440
how we're doing timewise there one more

00:29:07,269 --> 00:29:13,440
question or yeah so one last question

00:29:13,470 --> 00:29:26,049
okay oh there you go yeah so from what i

00:29:24,399 --> 00:29:29,070
read about lambda architecture it was

00:29:26,049 --> 00:29:31,059
more about that you would always

00:29:29,070 --> 00:29:34,299
recalculate the batch over your entire

00:29:31,059 --> 00:29:36,850
data set but now Ted mentioned that you

00:29:34,299 --> 00:29:40,360
would rather always just recalculate the

00:29:36,850 --> 00:29:42,100
last partition so are there just to

00:29:40,360 --> 00:29:45,220
different protests that you can both go

00:29:42,100 --> 00:29:47,289
or in the batch layer right so you look

00:29:45,220 --> 00:29:49,659
at the at the entire master data set and

00:29:47,289 --> 00:29:54,129
create the views over the enter master

00:29:49,659 --> 00:29:57,070
data said yes right are you worried

00:29:54,129 --> 00:29:59,230
about errors so every partition that

00:29:57,070 --> 00:30:02,080
might have errors right then you should

00:29:59,230 --> 00:30:04,480
recompute if you can partition it if you

00:30:02,080 --> 00:30:06,700
can't partition it what fits an not an

00:30:04,480 --> 00:30:08,980
aggregating sort of thing then maybe you

00:30:06,700 --> 00:30:17,639
have to run every all every bit of data

00:30:08,980 --> 00:30:17,639
all the time your choice okay thank

00:30:21,870 --> 00:30:23,930

YouTube URL: https://www.youtube.com/watch?v=ZAImpQzTJiE


