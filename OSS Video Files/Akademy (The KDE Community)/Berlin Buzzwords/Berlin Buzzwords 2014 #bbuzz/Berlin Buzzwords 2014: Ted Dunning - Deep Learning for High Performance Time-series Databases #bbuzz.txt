Title: Berlin Buzzwords 2014: Ted Dunning - Deep Learning for High Performance Time-series Databases #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Recent developments in deep learning make it possible to improve time series databases. I will show how these methods work and how to implement them using Apache Mahout.

Systems such as the Open Time Series Database (Open TSDB) make good use of the ability of HBase and related databases to store columns sparsely. This allows a single row to store many time samples and allows raw scans to retrieve a large number of samples very quickly for visualization or analysis.  Typically, older data points are batched together and compressed to save space. At high insertion rates, this approach falters largely because of the limited insert/update rate of HBase.  In such situations, it is often better to short segments of data and insert batches that span short time ranges rather than inserting individual data points.

Read more:
https://2014.berlinbuzzwords.de/session/deep-learning-high-performance-time-series-databases

About Ted Dunning:
https://2014.berlinbuzzwords.de/user/323/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,560 --> 00:00:14,559
okay so here we go this is going to be

00:00:10,660 --> 00:00:17,170
kind of a dense talk there's a lot in it

00:00:14,559 --> 00:00:19,210
there's practical aspects but there's

00:00:17,170 --> 00:00:22,000
also philosophy there's even a little

00:00:19,210 --> 00:00:24,339
bit of mathematics this time so what I

00:00:22,000 --> 00:00:26,679
want to talk about is how the insights

00:00:24,339 --> 00:00:29,529
of anomaly detection anomaly detection

00:00:26,679 --> 00:00:32,140
done right can lead to some very very

00:00:29,529 --> 00:00:36,220
interesting capabilities for time series

00:00:32,140 --> 00:00:38,890
database and the ideas lead to time

00:00:36,220 --> 00:00:41,800
series databases that are first of all

00:00:38,890 --> 00:00:46,120
ten or more times more performant but

00:00:41,800 --> 00:00:49,210
also which ultimately can do semantic

00:00:46,120 --> 00:00:50,770
search on sequences and that's that's an

00:00:49,210 --> 00:00:53,530
extraordinary capability for a time

00:00:50,770 --> 00:00:57,880
series database most of them just draw a

00:00:53,530 --> 00:01:00,880
graph and not much else so I'm me I

00:00:57,880 --> 00:01:03,580
worked for map our case most people

00:01:00,880 --> 00:01:06,400
can't read hats because they've blinded

00:01:03,580 --> 00:01:08,860
to advertising you can reach me a lot of

00:01:06,400 --> 00:01:11,409
different ways I'm involved with Apache

00:01:08,860 --> 00:01:14,740
as well and been involved with open

00:01:11,409 --> 00:01:16,899
source for a very long time now part

00:01:14,740 --> 00:01:18,909
makes Hadoop distribution which has a

00:01:16,899 --> 00:01:21,850
lot of cool technology we can talk about

00:01:18,909 --> 00:01:23,469
it later so anyway let's talk real quick

00:01:21,850 --> 00:01:25,030
and I'm going to go very quickly through

00:01:23,469 --> 00:01:27,840
the first part about what anomaly

00:01:25,030 --> 00:01:31,539
detection is and how it should be done

00:01:27,840 --> 00:01:33,579
so the problem here is not like many

00:01:31,539 --> 00:01:36,090
kinds of machine learning where we have

00:01:33,579 --> 00:01:38,499
examples of what we want to find and

00:01:36,090 --> 00:01:41,679
then we ask the machine to find more of

00:01:38,499 --> 00:01:46,479
that here we have to find the things we

00:01:41,679 --> 00:01:48,039
don't know to ask for it's difficult the

00:01:46,479 --> 00:01:50,799
machine has to figure out somehow

00:01:48,039 --> 00:01:54,310
something that stands outside the normal

00:01:50,799 --> 00:01:58,840
and there's a story that my own journey

00:01:54,310 --> 00:02:01,659
with anomaly detection started when we

00:01:58,840 --> 00:02:03,579
had a system at music Matt and the

00:02:01,659 --> 00:02:05,590
problem was that the CEO would wake up

00:02:03,579 --> 00:02:07,689
at two in the morning and go and check

00:02:05,590 --> 00:02:09,100
the system and call me and said Ted we

00:02:07,689 --> 00:02:13,300
haven't had a sale in five minutes do

00:02:09,100 --> 00:02:14,860
you think that's okay I'd go hey I mean

00:02:13,300 --> 00:02:15,939
else can you say two in the morning when

00:02:14,860 --> 00:02:19,239
somebody calls you and asks you a

00:02:15,939 --> 00:02:19,510
question like that and so this happened

00:02:19,239 --> 00:02:21,760
a lot

00:02:19,510 --> 00:02:24,129
and then the real problem was he was

00:02:21,760 --> 00:02:26,439
often right and this is very bad for

00:02:24,129 --> 00:02:30,370
CEOs because it leads them to believe

00:02:26,439 --> 00:02:33,670
that they're always right you should

00:02:30,370 --> 00:02:35,799
never trained a CEO that way so what we

00:02:33,670 --> 00:02:37,450
did is we built a system finally and it

00:02:35,799 --> 00:02:39,190
was difficult because we had people from

00:02:37,450 --> 00:02:43,420
Japan people from Europe people from the

00:02:39,190 --> 00:02:45,579
US buying and and so on and so what we

00:02:43,420 --> 00:02:48,010
did is we built a system that understood

00:02:45,579 --> 00:02:51,159
the patterns that we had and would call

00:02:48,010 --> 00:02:53,920
me first that there was if there was a

00:02:51,159 --> 00:02:56,590
real problem and then it would let him

00:02:53,920 --> 00:03:00,040
as dashboard turn red about 10 minutes

00:02:56,590 --> 00:03:02,590
later and so he would call me then he go

00:03:00,040 --> 00:03:04,359
oh my god the dashboards red and and it

00:03:02,590 --> 00:03:06,549
hasn't had a sale in 12 minutes I go

00:03:04,359 --> 00:03:09,190
Dennis where you been I've been on this

00:03:06,549 --> 00:03:11,019
already we got it fixed we know what's

00:03:09,190 --> 00:03:13,359
happening I mean just go to sleep you're

00:03:11,019 --> 00:03:15,819
you got better things to do and it took

00:03:13,359 --> 00:03:16,989
about four times and it trained him to

00:03:15,819 --> 00:03:19,599
sleep through the night it's very

00:03:16,989 --> 00:03:21,120
important to train these people to sleep

00:03:19,599 --> 00:03:23,290
through the night it's difficult

00:03:21,120 --> 00:03:25,569
Isabelle can talk about this sort of

00:03:23,290 --> 00:03:27,699
difficulty but with CEOs you have the

00:03:25,569 --> 00:03:29,949
same problem and the idea is once you

00:03:27,699 --> 00:03:32,049
get a sense of confidence once they know

00:03:29,949 --> 00:03:36,010
that you will find problems before they

00:03:32,049 --> 00:03:37,569
can it's calm and actually everything

00:03:36,010 --> 00:03:38,709
works better because they do what

00:03:37,569 --> 00:03:41,260
they're supposed to do which is not

00:03:38,709 --> 00:03:45,819
minute to minute observation so here's a

00:03:41,260 --> 00:03:47,919
signal this is random numbers you get an

00:03:45,819 --> 00:03:50,019
idea what is supposed to do and then you

00:03:47,919 --> 00:03:52,090
see something like this and you say

00:03:50,019 --> 00:03:55,449
anomaly what's happening there is you've

00:03:52,090 --> 00:03:59,230
built a model in your head e is normal

00:03:55,449 --> 00:04:01,540
woo is not now you might see the spikes

00:03:59,230 --> 00:04:04,030
like this periodically that might become

00:04:01,540 --> 00:04:06,579
part of your normal and then the step

00:04:04,030 --> 00:04:09,609
would be the anomaly so the idea here

00:04:06,579 --> 00:04:12,280
again is the the normal may not be a

00:04:09,609 --> 00:04:14,199
simple sort of thing it may not just be

00:04:12,280 --> 00:04:16,239
here plus or minus a little bit of noise

00:04:14,199 --> 00:04:18,669
here plus or minus a little bit of noise

00:04:16,239 --> 00:04:22,330
and occasionally short spikes but not

00:04:18,669 --> 00:04:24,880
long steps you elaborate very quickly

00:04:22,330 --> 00:04:26,320
this model in your head and so you want

00:04:24,880 --> 00:04:28,539
to make take action when something

00:04:26,320 --> 00:04:30,820
breaks when something abnormal happens

00:04:28,539 --> 00:04:33,240
definition of breaks is not clear

00:04:30,820 --> 00:04:35,220
definition of working is not clear

00:04:33,240 --> 00:04:38,099
so we want to trade off action and

00:04:35,220 --> 00:04:40,139
inaction in the right times and we want

00:04:38,099 --> 00:04:42,060
to build a model that does this for us

00:04:40,139 --> 00:04:43,979
here's another look we could

00:04:42,060 --> 00:04:46,080
automatically using things like the T

00:04:43,979 --> 00:04:49,199
digest that I talked about at the bar

00:04:46,080 --> 00:04:52,020
camp set thresholds and then the system

00:04:49,199 --> 00:04:54,000
can tell us that we can set a budget we

00:04:52,020 --> 00:04:56,940
want to be woken up this many times per

00:04:54,000 --> 00:04:59,250
month and we want the most of those to

00:04:56,940 --> 00:05:01,020
be when there really is a problem and we

00:04:59,250 --> 00:05:03,840
could build a system like that where the

00:05:01,020 --> 00:05:06,960
online summarizer pics whatever

00:05:03,840 --> 00:05:10,550
percentile we want to set and that's as

00:05:06,960 --> 00:05:13,580
a threshold and mahout has this and

00:05:10,550 --> 00:05:16,259
we're done right nine slides we're good

00:05:13,580 --> 00:05:18,180
but not quite so much easy I mean people

00:05:16,259 --> 00:05:20,610
have done this sort of thing at sea has

00:05:18,180 --> 00:05:24,539
released skyline but what about signals

00:05:20,610 --> 00:05:29,460
like this a is easy to find but B is

00:05:24,539 --> 00:05:31,080
hard B is hiding in the valley and so a

00:05:29,460 --> 00:05:33,960
simple threshold is definitely not going

00:05:31,080 --> 00:05:36,060
to work you also have signals like this

00:05:33,960 --> 00:05:38,190
where you get this big anomaly over

00:05:36,060 --> 00:05:39,930
there I'd like to think of these signals

00:05:38,190 --> 00:05:43,620
though is not so much anomalous because

00:05:39,930 --> 00:05:49,259
that's a heartbeat those are important

00:05:43,620 --> 00:05:51,330
to me and here's an anomaly this is an

00:05:49,259 --> 00:05:53,159
anomaly and a real heartbeat there

00:05:51,330 --> 00:05:56,719
you've got an irregular heartbeat the

00:05:53,159 --> 00:05:59,729
the the shape and the timing changes so

00:05:56,719 --> 00:06:03,150
again we have to have a bit more of an

00:05:59,729 --> 00:06:05,400
elaborated model but model is still the

00:06:03,150 --> 00:06:08,460
key word there we want a model of what

00:06:05,400 --> 00:06:10,050
is normal and then we want to say

00:06:08,460 --> 00:06:13,380
whatever doesn't fit that model is

00:06:10,050 --> 00:06:16,460
anomaly and if we built a good and a

00:06:13,380 --> 00:06:19,319
tight model it really does express

00:06:16,460 --> 00:06:21,810
normality then the things that deviate

00:06:19,319 --> 00:06:25,110
from that will be honest to god and

00:06:21,810 --> 00:06:26,490
important deviations now for simple

00:06:25,110 --> 00:06:30,560
things it could just be normal

00:06:26,490 --> 00:06:34,500
distribution that's boring but for the

00:06:30,560 --> 00:06:37,680
EKG signal the electrocardiogram we can

00:06:34,500 --> 00:06:40,590
build a model of this signal by taking

00:06:37,680 --> 00:06:43,169
short windows and mathematically the

00:06:40,590 --> 00:06:46,440
windows are formed by a signal that's a

00:06:43,169 --> 00:06:51,870
little bump like that it's really just

00:06:46,440 --> 00:06:53,700
sine squared or cosine squared and we

00:06:51,870 --> 00:06:56,280
multiplied element by element sample by

00:06:53,700 --> 00:06:58,830
sample and so this it's zero everywhere

00:06:56,280 --> 00:07:01,410
except in the bump and so the signal

00:06:58,830 --> 00:07:05,700
goes away except inside the bump and so

00:07:01,410 --> 00:07:07,380
as we move the the window along we get

00:07:05,700 --> 00:07:08,790
different little views of that and

00:07:07,380 --> 00:07:11,970
they're all the same length because the

00:07:08,790 --> 00:07:13,890
window is always the same length now we

00:07:11,970 --> 00:07:17,640
get see that signal looks a lot like the

00:07:13,890 --> 00:07:19,290
one at the beginning and that one looks

00:07:17,640 --> 00:07:21,900
a lot like the one at the beginning and

00:07:19,290 --> 00:07:25,530
that one looks a lot like the beginning

00:07:21,900 --> 00:07:28,560
the idea there is then we could cluster

00:07:25,530 --> 00:07:30,360
these these little windowed signals now

00:07:28,560 --> 00:07:32,190
that the the window is chosen very

00:07:30,360 --> 00:07:34,230
cleverly not by me by a guy called

00:07:32,190 --> 00:07:37,530
hemming who came up with lots of clever

00:07:34,230 --> 00:07:39,420
things so that when you shift them just

00:07:37,530 --> 00:07:42,780
by half thing and you add them up they

00:07:39,420 --> 00:07:44,730
add up to exactly one and that means

00:07:42,780 --> 00:07:47,090
that if you just add up all those little

00:07:44,730 --> 00:07:50,520
windowed signals you get the original or

00:07:47,090 --> 00:07:52,620
if you add up some nice approximation of

00:07:50,520 --> 00:07:54,180
each of the windowed signals you wind up

00:07:52,620 --> 00:07:56,760
with a nice approximation of the

00:07:54,180 --> 00:07:59,340
original and here for instance are the

00:07:56,760 --> 00:08:02,160
most common shapes for the EKG signal

00:07:59,340 --> 00:08:05,550
several hours that I looked at and you

00:08:02,160 --> 00:08:07,380
can see there's that mean that's the QRS

00:08:05,550 --> 00:08:09,600
complex there's another version of it

00:08:07,380 --> 00:08:11,970
slightly shifted slightly shifted

00:08:09,600 --> 00:08:13,919
another one another one and then you

00:08:11,970 --> 00:08:16,950
have the bumps afterwards and so on so

00:08:13,919 --> 00:08:21,390
this thing has learned the shapes that

00:08:16,950 --> 00:08:23,280
make up a normal EKG and we can now

00:08:21,390 --> 00:08:25,140
build the original and we can build the

00:08:23,280 --> 00:08:27,060
reconstruction it's probably hard to see

00:08:25,140 --> 00:08:29,640
but you can see that the reconstruction

00:08:27,060 --> 00:08:31,980
is just like the original except a

00:08:29,640 --> 00:08:34,469
little bit smoother little noisy things

00:08:31,980 --> 00:08:38,370
the hard things that the things that

00:08:34,469 --> 00:08:40,289
don't repeat our fall away and the

00:08:38,370 --> 00:08:42,390
reconstruction error here which is just

00:08:40,289 --> 00:08:44,880
the subtraction of those it's pretty

00:08:42,390 --> 00:08:46,440
small most of the time it's quite rare

00:08:44,880 --> 00:08:49,470
in fact for it to be as large as a

00:08:46,440 --> 00:08:51,540
hundred plus or minus so this

00:08:49,470 --> 00:08:54,870
reconstruction thing is a measure of

00:08:51,540 --> 00:08:58,200
deviation from our sig normal and the

00:08:54,870 --> 00:08:59,880
reconstruction is now a nice signal kind

00:08:58,200 --> 00:09:02,940
of like the first one we saw just

00:08:59,880 --> 00:09:05,130
livity noise sitting there no Big D VA

00:09:02,940 --> 00:09:06,870
shin so big deviations could be

00:09:05,130 --> 00:09:10,530
considered anomalies because we can

00:09:06,870 --> 00:09:13,320
reduce to that earlier problem here's an

00:09:10,530 --> 00:09:14,940
anomaly for instance it's hard to see

00:09:13,320 --> 00:09:16,680
what that anomaly is especially if

00:09:14,940 --> 00:09:20,250
you're sitting over here behind the the

00:09:16,680 --> 00:09:22,590
lectern but here it is magnified this is

00:09:20,250 --> 00:09:24,930
this is indeed a really bad thing that's

00:09:22,590 --> 00:09:27,480
the beginning of atrial fibrillation the

00:09:24,930 --> 00:09:30,480
herd be went did it it stopped I mean

00:09:27,480 --> 00:09:31,830
that's why the guys survived otherwise

00:09:30,480 --> 00:09:33,600
probably would be dead because there's a

00:09:31,830 --> 00:09:37,800
portable heart monitor this data is

00:09:33,600 --> 00:09:41,250
coming from and the anomaly is that the

00:09:37,800 --> 00:09:43,290
thing that understands and reconstructs

00:09:41,250 --> 00:09:48,030
normal heart beats can't reconstruct

00:09:43,290 --> 00:09:50,670
this thing that is so anomalous it's a

00:09:48,030 --> 00:09:52,080
subtle difference between that and a

00:09:50,670 --> 00:09:54,690
normal heartbeat but it's a difference

00:09:52,080 --> 00:09:56,880
that the model cannot understand cannot

00:09:54,690 --> 00:09:58,530
see cannot reconstruct and so the

00:09:56,880 --> 00:10:01,800
reconstruction error has a large error

00:09:58,530 --> 00:10:05,490
there there's another anomaly this is an

00:10:01,800 --> 00:10:08,160
instrumentation bug and the system just

00:10:05,490 --> 00:10:10,380
says I can't see this as a heartbeat and

00:10:08,160 --> 00:10:13,290
so it's just this huge anomaly so here's

00:10:10,380 --> 00:10:14,430
a revised system as a model we subtract

00:10:13,290 --> 00:10:16,830
that off and now we do the online

00:10:14,430 --> 00:10:19,890
summarizer yeah we got it well I got it

00:10:16,830 --> 00:10:21,840
going and this is model Delta sort of

00:10:19,890 --> 00:10:24,480
anomaly detector and it's fine it's good

00:10:21,840 --> 00:10:25,470
it does what it says and the idea here

00:10:24,480 --> 00:10:27,690
is thinking about probability

00:10:25,470 --> 00:10:30,150
distributions is a really good way to do

00:10:27,690 --> 00:10:33,090
this you can even do this with events

00:10:30,150 --> 00:10:36,230
dreams where it isn't so much that they

00:10:33,090 --> 00:10:38,550
follow a same shape but they happen at

00:10:36,230 --> 00:10:41,970
irregular intervals but the irregular

00:10:38,550 --> 00:10:44,130
intervals themselves have shape to them

00:10:41,970 --> 00:10:46,650
they have a model that you can build a

00:10:44,130 --> 00:10:50,700
Poisson thing with variable rate is a

00:10:46,650 --> 00:10:54,030
commonly used model for that and so we

00:10:50,700 --> 00:10:55,530
can build again a log a model like that

00:10:54,030 --> 00:10:59,940
we can use log probability as the

00:10:55,530 --> 00:11:02,310
anomaly for detecting it so if we just

00:10:59,940 --> 00:11:03,900
slip back a little bit we're just a

00:11:02,310 --> 00:11:05,640
little bit in here but we have this idea

00:11:03,900 --> 00:11:08,370
of what anomaly detection is build a

00:11:05,640 --> 00:11:10,500
model look for anomaly look for things

00:11:08,370 --> 00:11:12,630
that don't Mitch and we can deal with a

00:11:10,500 --> 00:11:14,640
lot of different kinds of signals

00:11:12,630 --> 00:11:16,770
but here's here's where philosophy

00:11:14,640 --> 00:11:19,500
starts up to now is totally practical

00:11:16,770 --> 00:11:22,410
things you could build you know in the

00:11:19,500 --> 00:11:24,690
kitchen at home sort of thing but the

00:11:22,410 --> 00:11:28,670
idea here that's very very interesting

00:11:24,690 --> 00:11:32,520
is when we build a model that fits our

00:11:28,670 --> 00:11:36,800
observations we are also building a

00:11:32,520 --> 00:11:40,050
compressor and it's an interesting

00:11:36,800 --> 00:11:42,900
mathematical truth that the model that

00:11:40,050 --> 00:11:46,410
builds the best compressor is also the

00:11:42,900 --> 00:11:48,810
closest one to truth truth being some

00:11:46,410 --> 00:11:54,120
unobservable what the probabilities

00:11:48,810 --> 00:11:57,570
really are it's stunning thing but it's

00:11:54,120 --> 00:11:59,490
it's based on the idea that the log is a

00:11:57,570 --> 00:12:01,350
privileged function it's a special kind

00:11:59,490 --> 00:12:04,200
of thing it is the best choice for

00:12:01,350 --> 00:12:06,810
measuring information but it also

00:12:04,200 --> 00:12:09,980
happens to be the thing that determines

00:12:06,810 --> 00:12:12,420
how much we can compress things and so

00:12:09,980 --> 00:12:14,070
taking it to minimal error making that

00:12:12,420 --> 00:12:15,900
reconstruction error as small as

00:12:14,070 --> 00:12:19,020
possible with whatever model structure

00:12:15,900 --> 00:12:22,230
we have is maximum likelihood maximum

00:12:19,020 --> 00:12:26,370
likelihood is maximum compression and by

00:12:22,230 --> 00:12:28,350
accident also truth to get the best

00:12:26,370 --> 00:12:30,480
understanding out of a signal we make it

00:12:28,350 --> 00:12:32,870
as small as possible the residue of

00:12:30,480 --> 00:12:35,550
things we don't understand is smaller

00:12:32,870 --> 00:12:38,130
therefore the bulk of things we do

00:12:35,550 --> 00:12:40,440
understand is larger now there's nice

00:12:38,130 --> 00:12:43,050
wonderful mathematics here but the shape

00:12:40,440 --> 00:12:48,060
of things is what's important this log

00:12:43,050 --> 00:12:54,120
function here has a maximum relative to

00:12:48,060 --> 00:12:56,550
this X minus 1 line exactly at zero when

00:12:54,120 --> 00:13:02,370
when x equals 1 and that's what makes

00:12:56,550 --> 00:13:05,190
that optimum unique and exact so if we

00:13:02,370 --> 00:13:08,400
think about this clustering we windowed

00:13:05,190 --> 00:13:10,020
we clustered we scaled the cluster just

00:13:08,400 --> 00:13:13,140
the right size with the right number of

00:13:10,020 --> 00:13:14,550
things there make it as big as so on we

00:13:13,140 --> 00:13:18,300
were subtracted from the original thing

00:13:14,550 --> 00:13:21,420
and we combine that with this idea of

00:13:18,300 --> 00:13:22,860
information compression and some kind of

00:13:21,420 --> 00:13:25,050
what we're doing there is we're taking

00:13:22,860 --> 00:13:26,520
the original signal we're encoding it

00:13:25,050 --> 00:13:30,360
with a probability model

00:13:26,520 --> 00:13:32,100
and then reconstructing the signal and

00:13:30,360 --> 00:13:34,650
we're trying to make that encoding as

00:13:32,100 --> 00:13:37,470
tight as possible and in in the the

00:13:34,650 --> 00:13:39,530
simple tiny naive Saturday afternoon

00:13:37,470 --> 00:13:42,270
kind of thing that I built a year it's

00:13:39,530 --> 00:13:45,840
10 or 30 to 1 somewhere in that range

00:13:42,270 --> 00:13:47,340
compression it's quite quite tight but

00:13:45,840 --> 00:13:48,870
this is also called an information

00:13:47,340 --> 00:13:53,090
bottleneck and it's been studied for

00:13:48,870 --> 00:13:57,690
many years in AI so we've moved from

00:13:53,090 --> 00:14:00,240
anomaly detection now to an idea that by

00:13:57,690 --> 00:14:02,400
reconstructing here we're building a

00:14:00,240 --> 00:14:05,430
compact representation a compact

00:14:02,400 --> 00:14:09,780
encoding and in fact the clustering that

00:14:05,430 --> 00:14:13,650
we used is the first step in a deep

00:14:09,780 --> 00:14:16,020
learning system if we look at a neural

00:14:13,650 --> 00:14:18,570
network the way a neural network works

00:14:16,020 --> 00:14:21,120
is we have this is a an auto encoding

00:14:18,570 --> 00:14:23,880
neural network the input is the same as

00:14:21,120 --> 00:14:26,160
the output except for the little primes

00:14:23,880 --> 00:14:28,830
on it meaning it's not quite exact the

00:14:26,160 --> 00:14:31,410
input our values those circles are

00:14:28,830 --> 00:14:35,010
values every time we follow an edge we

00:14:31,410 --> 00:14:37,260
multiply by the weight of that edge and

00:14:35,010 --> 00:14:40,190
when we get to the things in the middle

00:14:37,260 --> 00:14:42,990
we add up all the incoming lines and

00:14:40,190 --> 00:14:45,900
then we do that again those intermediate

00:14:42,990 --> 00:14:50,790
circles have values and we go to the

00:14:45,900 --> 00:14:52,890
output things and we add up now in the

00:14:50,790 --> 00:14:55,740
middle layer there's always some sort of

00:14:52,890 --> 00:14:56,910
non-linearity in interesting neural nets

00:14:55,740 --> 00:14:59,700
because if you didn't have the

00:14:56,910 --> 00:15:01,920
non-linearity there you could just kind

00:14:59,700 --> 00:15:05,640
of reduce it all to one light one step

00:15:01,920 --> 00:15:08,190
and if you were to look at this one kind

00:15:05,640 --> 00:15:14,070
of non-linearity that we can add is we

00:15:08,190 --> 00:15:18,980
can say only one of these int interior

00:15:14,070 --> 00:15:23,310
nodes can be nonzero exactly one of K

00:15:18,980 --> 00:15:25,350
that's clustering we'd pick the biggest

00:15:23,310 --> 00:15:28,680
one the reason it's clustering is

00:15:25,350 --> 00:15:32,340
because this incoming step the sum of

00:15:28,680 --> 00:15:35,700
weighted inputs it's a dot product to

00:15:32,340 --> 00:15:37,800
dot product with the weights dot product

00:15:35,700 --> 00:15:39,730
is the same as Euclidean distance I mean

00:15:37,800 --> 00:15:42,579
X minus y square

00:15:39,730 --> 00:15:45,130
x squared plus y squared minus two x dot

00:15:42,579 --> 00:15:47,800
y and if the magnitudes of x and y are

00:15:45,130 --> 00:15:50,170
the same then the dot product there

00:15:47,800 --> 00:15:52,899
tells us when the distance is smallest

00:15:50,170 --> 00:15:55,510
dot product its biggest or distance is

00:15:52,899 --> 00:15:58,870
smallest as we're slipping through all

00:15:55,510 --> 00:16:00,880
the wise and so what we're doing when we

00:15:58,870 --> 00:16:03,370
say only one of these can be non-zero

00:16:00,880 --> 00:16:06,579
we're saying pick the nearest cluster

00:16:03,370 --> 00:16:11,110
the clusters are those weights on the

00:16:06,579 --> 00:16:12,310
inputs and those are the same weights on

00:16:11,110 --> 00:16:15,070
the outputs because this is a

00:16:12,310 --> 00:16:16,329
symmetrical auto encoder so that

00:16:15,070 --> 00:16:18,790
clustering that we did there which

00:16:16,329 --> 00:16:21,190
seemed very simple and easily motivated

00:16:18,790 --> 00:16:23,680
just from kind of look at it find

00:16:21,190 --> 00:16:26,680
patterns sort of mode is actually inner

00:16:23,680 --> 00:16:30,639
lip and it happens to be a special case

00:16:26,680 --> 00:16:33,040
of a case barse encoder we could say

00:16:30,639 --> 00:16:35,470
just pick the three of the things that

00:16:33,040 --> 00:16:37,600
are the biggest and we can train them

00:16:35,470 --> 00:16:40,500
the same way as we would with k-means

00:16:37,600 --> 00:16:42,519
learning or with neural net learning and

00:16:40,500 --> 00:16:46,720
then we have one of the more interesting

00:16:42,519 --> 00:16:49,990
and advanced neural nets that we can

00:16:46,720 --> 00:16:52,839
apply to this auto encoding signal and

00:16:49,990 --> 00:16:54,850
by enforcing that sparsity we can have

00:16:52,839 --> 00:16:58,660
many normally you would say you could

00:16:54,850 --> 00:17:01,360
only have 32 basis vectors 32 patterns

00:16:58,660 --> 00:17:03,910
if you allow dense patterns the dense

00:17:01,360 --> 00:17:06,370
reconstructions that's just vector space

00:17:03,910 --> 00:17:09,400
sort of things but if I say only one of

00:17:06,370 --> 00:17:12,250
k then I can do like I did here where I

00:17:09,400 --> 00:17:14,500
have 400 patterns instead of 32 the

00:17:12,250 --> 00:17:17,140
window size is 32 that's why I say that

00:17:14,500 --> 00:17:19,030
so it's a 32 dimensional vector I don't

00:17:17,140 --> 00:17:22,059
have enough fingers to point in all the

00:17:19,030 --> 00:17:24,309
directions and and so by having that

00:17:22,059 --> 00:17:26,949
clustering having that one of K I can

00:17:24,309 --> 00:17:28,480
use a very wide interior layer and by

00:17:26,949 --> 00:17:31,059
using M of cake and cut it down a little

00:17:28,480 --> 00:17:33,240
bit and I can encode very interesting

00:17:31,059 --> 00:17:36,370
signal so i really have kind of this

00:17:33,240 --> 00:17:37,990
neural net for the first window second

00:17:36,370 --> 00:17:40,929
neural net for the second one to third

00:17:37,990 --> 00:17:45,660
and so on and their reconstructions give

00:17:40,929 --> 00:17:45,660
us this this is kind of cool

00:17:45,890 --> 00:17:53,300
we can make these neural Nets deeper on

00:17:48,880 --> 00:17:57,950
that interior one of K signal we can do

00:17:53,300 --> 00:18:00,320
the same operation we can put a further

00:17:57,950 --> 00:18:02,810
abstracted value which will give us an

00:18:00,320 --> 00:18:04,880
iota better compression and it will give

00:18:02,810 --> 00:18:07,880
us significantly better understanding

00:18:04,880 --> 00:18:10,010
and we don't even need to keep the

00:18:07,880 --> 00:18:12,320
reconstruction if we don't want to so

00:18:10,010 --> 00:18:15,170
what we can do is we can just keep the

00:18:12,320 --> 00:18:16,700
first level nodes values but of course

00:18:15,170 --> 00:18:19,070
those are encoded by the second level

00:18:16,700 --> 00:18:22,070
nodes so we can just keep those with or

00:18:19,070 --> 00:18:24,470
without an error signal and so we can

00:18:22,070 --> 00:18:27,530
just keep that interior node which is a

00:18:24,470 --> 00:18:29,300
semantic representation now it's only

00:18:27,530 --> 00:18:31,700
been in the last five to ten years that

00:18:29,300 --> 00:18:34,850
how to learn these models has been

00:18:31,700 --> 00:18:39,520
practical this is a new thing to be able

00:18:34,850 --> 00:18:41,840
to do that but we have the potential now

00:18:39,520 --> 00:18:44,990
going from anomaly detection this all

00:18:41,840 --> 00:18:47,000
modeling idea to build time series

00:18:44,990 --> 00:18:51,440
databases that do not store the signal

00:18:47,000 --> 00:18:53,480
they store the meaning they use these

00:18:51,440 --> 00:18:56,390
ideas of anomaly detection these ideas

00:18:53,480 --> 00:18:58,790
of what normal is the ideas of maximum

00:18:56,390 --> 00:19:01,760
compression being maximally much like

00:18:58,790 --> 00:19:05,510
the true model to compress things and

00:19:01,760 --> 00:19:07,880
allow now not just signal search like I

00:19:05,510 --> 00:19:10,760
want this hour of data and plot it now

00:19:07,880 --> 00:19:13,010
from that signal from that signal but

00:19:10,760 --> 00:19:16,880
actually to say I want to find anomalies

00:19:13,010 --> 00:19:20,240
that look like this one I want to say I

00:19:16,880 --> 00:19:22,640
want to find where the pump failed with

00:19:20,240 --> 00:19:24,740
this predecessor sort of signal want to

00:19:22,640 --> 00:19:28,130
find more examples of that I can now say

00:19:24,740 --> 00:19:31,670
give me semantic search and at the same

00:19:28,130 --> 00:19:32,930
time the database becomes 10x hundred X

00:19:31,670 --> 00:19:36,170
somewhere in there depends on the

00:19:32,930 --> 00:19:38,810
signals smaller which is the same thing

00:19:36,170 --> 00:19:42,140
as the data rate coming in can be 10x

00:19:38,810 --> 00:19:44,810
larger this is really kind of cool and

00:19:42,140 --> 00:19:46,250
the trick here is you have to be able to

00:19:44,810 --> 00:19:49,250
build and they've been a lot of talks

00:19:46,250 --> 00:19:51,830
about this you have to use real-time and

00:19:49,250 --> 00:19:53,510
long time together to be able to build

00:19:51,830 --> 00:19:55,190
these systems Michael talked yesterday

00:19:53,510 --> 00:19:58,640
about lambda architecture which is an

00:19:55,190 --> 00:19:59,730
approach to that I like extending it by

00:19:58,640 --> 00:20:01,650
using a real time

00:19:59,730 --> 00:20:04,320
log so that you can recover more quickly

00:20:01,650 --> 00:20:07,170
than just waiting for the batch layer to

00:20:04,320 --> 00:20:09,900
come back around and the trick of course

00:20:07,170 --> 00:20:12,960
is that Hadoop in systems like it or not

00:20:09,900 --> 00:20:15,360
very real time the the ordinary file

00:20:12,960 --> 00:20:19,110
systems that you use with them are right

00:20:15,360 --> 00:20:21,150
once and sooner or later people will see

00:20:19,110 --> 00:20:23,700
the results but only after flushing or

00:20:21,150 --> 00:20:25,950
closing and so what we want to do is win

00:20:23,700 --> 00:20:28,470
or take the stuff that the batch system

00:20:25,950 --> 00:20:32,580
has processed and the stuff that the

00:20:28,470 --> 00:20:34,800
real times has processed storm sparks

00:20:32,580 --> 00:20:36,750
equal whatever you like and we need to

00:20:34,800 --> 00:20:38,910
provide this blended view and the

00:20:36,750 --> 00:20:40,860
blended view now consists of taking

00:20:38,910 --> 00:20:44,060
models built in the long time and

00:20:40,860 --> 00:20:48,510
applying them in real time to the signal

00:20:44,060 --> 00:20:50,640
so I've strove and mightily to stop as

00:20:48,510 --> 00:20:51,840
quickly as possible so we could have

00:20:50,640 --> 00:20:53,970
time for questions because it's always

00:20:51,840 --> 00:20:55,260
the best part of any talk just hearing

00:20:53,970 --> 00:20:57,450
what people do and what they're

00:20:55,260 --> 00:20:59,510
interested in and what they'd like where

00:20:57,450 --> 00:21:02,160
they'd like to drive some of this stuff

00:20:59,510 --> 00:21:04,740
so there we are we're 20 minutes we've

00:21:02,160 --> 00:21:06,210
now got 20 minutes to talk what do you

00:21:04,740 --> 00:21:08,640
guys think what do you guys want to do

00:21:06,210 --> 00:21:12,570
with this any thoughts it's a little bit

00:21:08,640 --> 00:21:15,570
fast wasn't it it's kind of a lot to fit

00:21:12,570 --> 00:21:18,750
into 20 30 minutes here's a microphone

00:21:15,570 --> 00:21:24,360
and a man with a microphone it makes it

00:21:18,750 --> 00:21:25,920
portable so I kind claimed it understand

00:21:24,360 --> 00:21:28,050
all that of the mathematics but it seems

00:21:25,920 --> 00:21:29,340
to be the into intuitively you know if

00:21:28,050 --> 00:21:31,230
we were speaking about you know like the

00:21:29,340 --> 00:21:33,840
model being some form of compression of

00:21:31,230 --> 00:21:36,690
the underlying data that and there being

00:21:33,840 --> 00:21:41,010
some approximation to it that the more

00:21:36,690 --> 00:21:42,300
we compress it you know the less clear

00:21:41,010 --> 00:21:43,980
the signal is going to be in the more

00:21:42,300 --> 00:21:47,040
approximately it's going to be and they

00:21:43,980 --> 00:21:48,750
know that the representation of a data

00:21:47,040 --> 00:21:50,940
series that has the least amount of

00:21:48,750 --> 00:21:54,140
error is the data itself so in order to

00:21:50,940 --> 00:21:57,030
gain compression we lose you know

00:21:54,140 --> 00:22:01,400
precision is that the right way of

00:21:57,030 --> 00:22:04,440
thinking about it no but otherwise close

00:22:01,400 --> 00:22:08,220
no because what I'm talking about is for

00:22:04,440 --> 00:22:10,850
a particular constant accuracy what can

00:22:08,220 --> 00:22:13,650
you do to compress the data the most and

00:22:10,850 --> 00:22:16,320
if you hold accuracy constant

00:22:13,650 --> 00:22:19,200
you can still compress signals that are

00:22:16,320 --> 00:22:23,430
not totally random if they were totally

00:22:19,200 --> 00:22:26,190
random then there is no the the entropy

00:22:23,430 --> 00:22:28,890
of the signal is the bit rate in any

00:22:26,190 --> 00:22:30,450
real signal certainly any real signal

00:22:28,890 --> 00:22:32,490
that we want to interpret the way humans

00:22:30,450 --> 00:22:34,680
interpret it and humans are doing a

00:22:32,490 --> 00:22:37,710
massive job of compression as they see

00:22:34,680 --> 00:22:39,030
things as they hear things what you can

00:22:37,710 --> 00:22:41,250
do is you can hold the error you can

00:22:39,030 --> 00:22:43,890
define what errors insignificant means

00:22:41,250 --> 00:22:46,560
and I'll get back to that in a moment

00:22:43,890 --> 00:22:50,040
and you can hold that constant and you

00:22:46,560 --> 00:22:52,920
can still compress so compression does

00:22:50,040 --> 00:22:56,280
not mean loss more compression does not

00:22:52,920 --> 00:22:58,590
need mean more laws more compression

00:22:56,280 --> 00:23:02,820
with constant loss means you have a

00:22:58,590 --> 00:23:04,680
better model more compression with a

00:23:02,820 --> 00:23:07,410
constant model that's totally general

00:23:04,680 --> 00:23:09,000
purpose yeah eventually you get down to

00:23:07,410 --> 00:23:11,640
 and you just have one bit and you

00:23:09,000 --> 00:23:15,030
go bit and and you've lost all the

00:23:11,640 --> 00:23:16,530
information that is not what I'm talking

00:23:15,030 --> 00:23:19,350
about I'm talking about holding the loss

00:23:16,530 --> 00:23:22,470
constant and also if you think about it

00:23:19,350 --> 00:23:24,510
this was originally 12 bit signals the

00:23:22,470 --> 00:23:28,560
error in reconstruction is typically

00:23:24,510 --> 00:23:30,360
less than 1 bit on those things and so

00:23:28,560 --> 00:23:33,090
all of those less than one bit things

00:23:30,360 --> 00:23:35,460
can be Justin coded as zero with a run

00:23:33,090 --> 00:23:38,400
length and is round when you reconstruct

00:23:35,460 --> 00:23:41,550
and then you could store the error when

00:23:38,400 --> 00:23:44,130
there is a high error when the air gets

00:23:41,550 --> 00:23:46,290
large you can actually store that now

00:23:44,130 --> 00:23:49,770
anomalies by definition don't occur

00:23:46,290 --> 00:23:51,360
often therefore you store at full

00:23:49,770 --> 00:23:54,600
bandwidth you have the semantic

00:23:51,360 --> 00:23:57,420
representation and you say but and so

00:23:54,600 --> 00:23:59,610
you just search for but and find these

00:23:57,420 --> 00:24:02,610
sorts of things and so you have perfect

00:23:59,610 --> 00:24:04,860
fidelity there and you have almost

00:24:02,610 --> 00:24:06,720
always very high compression every time

00:24:04,860 --> 00:24:11,940
things are normal the system is going on

00:24:06,720 --> 00:24:13,860
its normal type 30 to 37 41 and and you

00:24:11,940 --> 00:24:15,780
get it it's like the prisoners telling

00:24:13,860 --> 00:24:18,590
jokes they've compressed it they have a

00:24:15,780 --> 00:24:21,270
fine dictionary that encodes that and

00:24:18,590 --> 00:24:24,150
then when there's an anomaly you go but

00:24:21,270 --> 00:24:25,920
here's the error so you can still get

00:24:24,150 --> 00:24:27,250
perfect fidelity or nearly perfect

00:24:25,920 --> 00:24:29,350
fidelity or if

00:24:27,250 --> 00:24:31,240
fidelity as good as your signal really

00:24:29,350 --> 00:24:34,450
is there's always the question of

00:24:31,240 --> 00:24:37,270
significance so yeah you don't have to

00:24:34,450 --> 00:24:40,600
lose the signal so so if the variance in

00:24:37,270 --> 00:24:42,010
the data is very high the mechanism is

00:24:40,600 --> 00:24:45,100
not going to perform as well as if it

00:24:42,010 --> 00:24:48,760
would be a very uniform signal with very

00:24:45,100 --> 00:24:50,500
rare abnormalities well if the

00:24:48,760 --> 00:24:53,560
abnormalities are not rare than they are

00:24:50,500 --> 00:24:56,080
not abnormalities so you know you

00:24:53,560 --> 00:24:59,950
there's a very slippery sort of semantic

00:24:56,080 --> 00:25:03,130
arguments there and here in this

00:24:59,950 --> 00:25:05,530
original signal variance is not the

00:25:03,130 --> 00:25:09,670
issue variance is the the size of that

00:25:05,530 --> 00:25:12,310
thing but that isn't the big deal I mean

00:25:09,670 --> 00:25:15,340
a little bit if I have quantization here

00:25:12,310 --> 00:25:17,590
see here's 0 to 5 so if one is the

00:25:15,340 --> 00:25:19,780
smallest thing I could do then clearly

00:25:17,590 --> 00:25:22,480
having a variance this small means I

00:25:19,780 --> 00:25:24,670
really have this value of 0 most of the

00:25:22,480 --> 00:25:26,740
time if I really had a quantized signal

00:25:24,670 --> 00:25:28,450
but that would look very different and I

00:25:26,740 --> 00:25:31,090
would model it with a dictionary i would

00:25:28,450 --> 00:25:33,750
say 0 must of the time run length whoa

00:25:31,090 --> 00:25:36,490
there's a signal and so on and so

00:25:33,750 --> 00:25:39,390
variance is really not the right concept

00:25:36,490 --> 00:25:42,190
their entropy and relative entropy

00:25:39,390 --> 00:25:46,780
relative to the model is the correct

00:25:42,190 --> 00:25:48,970
concept now we may just be words you do

00:25:46,780 --> 00:25:52,300
different things than this so words can

00:25:48,970 --> 00:25:54,400
be an exact but a little bit of care is

00:25:52,300 --> 00:25:59,290
careful is important they're right next

00:25:54,400 --> 00:26:02,110
to you somebody had a finger up so my

00:25:59,290 --> 00:26:04,270
question is about what the main sort of

00:26:02,110 --> 00:26:06,160
to look at the data add because your

00:26:04,270 --> 00:26:08,140
talk seems to be focusing on looking at

00:26:06,160 --> 00:26:10,300
the time domain so all of your signals I

00:26:08,140 --> 00:26:12,790
essential is sort of overtime if we

00:26:10,300 --> 00:26:14,770
actually flip into some other domain

00:26:12,790 --> 00:26:16,690
let's say you know we do wavelets you

00:26:14,770 --> 00:26:18,790
know sort of decomposition or you know

00:26:16,690 --> 00:26:20,620
Fourier transform you know we would

00:26:18,790 --> 00:26:24,040
basically start looking at a different

00:26:20,620 --> 00:26:26,410
sort of data sets is there any sort of

00:26:24,040 --> 00:26:27,850
theory behind when one domain is better

00:26:26,410 --> 00:26:31,300
than the other for the type of thing

00:26:27,850 --> 00:26:33,010
that you were talking about yeah so so I

00:26:31,300 --> 00:26:35,290
think there's kind of like three or four

00:26:33,010 --> 00:26:37,900
questions in that oh and I'm going to

00:26:35,290 --> 00:26:39,890
start with a simple most 19th century

00:26:37,900 --> 00:26:44,600
sort of just question

00:26:39,890 --> 00:26:47,810
you mentioned for you transform now ouya

00:26:44,600 --> 00:26:50,690
transforms take a signal of some size or

00:26:47,810 --> 00:26:52,220
or some period and if it's a discrete

00:26:50,690 --> 00:26:55,790
signal you can use the discrete Fourier

00:26:52,220 --> 00:26:58,520
transform and so on and it restates that

00:26:55,790 --> 00:27:02,060
in terms of what are called basis

00:26:58,520 --> 00:27:08,270
vectors these are sinusoidal Co Co

00:27:02,060 --> 00:27:13,010
sinusoidal properties in that it loses

00:27:08,270 --> 00:27:18,320
no information and it is optimal in many

00:27:13,010 --> 00:27:20,570
ways but it misses the point you know

00:27:18,320 --> 00:27:24,050
back in the 19th century guys like filet

00:27:20,570 --> 00:27:25,820
and and in Hamilton and many others were

00:27:24,050 --> 00:27:28,880
coming up with his idea from physics and

00:27:25,820 --> 00:27:30,950
they hide the idea that sinusoids were

00:27:28,880 --> 00:27:33,530
somehow special and that complete

00:27:30,950 --> 00:27:36,230
orthogonal orthonormal basis were a very

00:27:33,530 --> 00:27:37,940
very important thing but in fact that's

00:27:36,230 --> 00:27:39,230
really not the right way to go they

00:27:37,940 --> 00:27:41,180
started with that assumption because

00:27:39,230 --> 00:27:42,980
they're building and inventing linear

00:27:41,180 --> 00:27:45,260
algebra at the same time and they had

00:27:42,980 --> 00:27:50,090
all these optimality prism but that

00:27:45,260 --> 00:27:52,640
really wasn't the point and 15 years ago

00:27:50,090 --> 00:27:55,880
or so Candace and others came up with

00:27:52,640 --> 00:27:58,670
this idea that in fact signals of

00:27:55,880 --> 00:28:03,410
interest very very often are better

00:27:58,670 --> 00:28:06,020
expressed as a sparse value in a much

00:28:03,410 --> 00:28:09,950
higher dimensional space so the idea of

00:28:06,020 --> 00:28:11,600
over complete basis came up and that's

00:28:09,950 --> 00:28:14,150
what we have here we have 400 basis

00:28:11,600 --> 00:28:17,450
vectors in a 32 dimensional space it's

00:28:14,150 --> 00:28:19,820
way over done and then we encode as

00:28:17,450 --> 00:28:22,010
exactly one of them now maybe we could

00:28:19,820 --> 00:28:23,480
encode is exactly thrilled but if Lee a

00:28:22,010 --> 00:28:30,260
sort of thing would have encoded it as

00:28:23,480 --> 00:28:32,740
32 of 32 and the difference here is that

00:28:30,260 --> 00:28:38,810
there is apparently a strong correlation

00:28:32,740 --> 00:28:41,990
to visual perception very very much

00:28:38,810 --> 00:28:44,960
seems like humans do this and this is

00:28:41,990 --> 00:28:47,870
how they see things and understand them

00:28:44,960 --> 00:28:49,550
quickly now there's no reason the world

00:28:47,870 --> 00:28:52,190
has to work that way we could imagine

00:28:49,550 --> 00:28:52,999
worlds that don't work that way but our

00:28:52,190 --> 00:28:54,889
world does

00:28:52,999 --> 00:28:59,779
to work that way at least as we perceive

00:28:54,889 --> 00:29:01,849
it and systems that have strong

00:28:59,779 --> 00:29:03,559
regularities can be shown to have these

00:29:01,849 --> 00:29:05,959
kinds of properties as well it isn't

00:29:03,559 --> 00:29:07,969
just that the human visual cortex or

00:29:05,959 --> 00:29:11,179
auditory cortex has to be able to see

00:29:07,969 --> 00:29:12,709
these changes and so it's importantly

00:29:11,179 --> 00:29:14,929
true that we don't worry about these

00:29:12,709 --> 00:29:17,269
orthonormal basis sets so then we moved

00:29:14,929 --> 00:29:19,669
to wavelets in the 1980s and double

00:29:17,269 --> 00:29:23,539
she's and things like that those are an

00:29:19,669 --> 00:29:25,579
attempt to do the flu a thing with a new

00:29:23,539 --> 00:29:27,909
set that has what's called compact

00:29:25,579 --> 00:29:30,799
support meaning they're bounded in time

00:29:27,909 --> 00:29:34,389
but it's still assuming orthonormal

00:29:30,799 --> 00:29:37,939
complete basis and the point was

00:29:34,389 --> 00:29:40,909
sparsity sparsity alone has been shown

00:29:37,939 --> 00:29:43,909
to be a vastly important organizing

00:29:40,909 --> 00:29:47,329
principle for these learning systems it

00:29:43,909 --> 00:29:51,589
alone sparked the entire deep learning

00:29:47,329 --> 00:29:54,049
revolution practically it alone has been

00:29:51,589 --> 00:29:56,479
the driving principle that allows now

00:29:54,049 --> 00:30:00,649
computers to do almost anything we can

00:29:56,479 --> 00:30:02,329
do at a glance the the Android speech

00:30:00,649 --> 00:30:07,279
recognition system is based on these and

00:30:02,329 --> 00:30:11,779
so I really try to draw a line from the

00:30:07,279 --> 00:30:15,079
the 19th century view of these physics

00:30:11,779 --> 00:30:17,569
based minimal basis it's a very

00:30:15,079 --> 00:30:21,709
different kind of thing so that's one of

00:30:17,569 --> 00:30:24,709
22 of three questions then you asked

00:30:21,709 --> 00:30:29,059
where does this work I'd spaced it out

00:30:24,709 --> 00:30:31,309
entirely I don't know there's there's

00:30:29,059 --> 00:30:34,279
lots of systems it does work in it works

00:30:31,309 --> 00:30:36,349
in text it works based on my prior work

00:30:34,279 --> 00:30:39,589
on genomes it works in time series

00:30:36,349 --> 00:30:42,799
databases works in a video databases it

00:30:39,589 --> 00:30:44,539
works in auditory databases but it has

00:30:42,799 --> 00:30:46,609
to be things that have regularities they

00:30:44,539 --> 00:30:49,189
can be discovered by these methods these

00:30:46,609 --> 00:30:51,829
methods are not very well understood yet

00:30:49,189 --> 00:30:54,799
and so we don't know what the bounds and

00:30:51,829 --> 00:30:57,379
lemons are my guess is anything that

00:30:54,799 --> 00:31:00,409
compresses super well with a nice

00:30:57,379 --> 00:31:02,779
understandable compression scheme it's

00:31:00,409 --> 00:31:04,399
basically the same as what I showed and

00:31:02,779 --> 00:31:06,410
of course the stuff I showed here is

00:31:04,399 --> 00:31:11,660
incredibly not even simple

00:31:06,410 --> 00:31:19,250
it's what fits in 20 minutes my god yeah

00:31:11,660 --> 00:31:21,800
there's somebody over here any

00:31:19,250 --> 00:31:24,800
applications you could recommend to have

00:31:21,800 --> 00:31:27,800
a closer look and weds implemented your

00:31:24,800 --> 00:31:30,860
model like you said to not get the CEO a

00:31:27,800 --> 00:31:33,440
CIO waked up during the night yo like

00:31:30,860 --> 00:31:37,100
how skyline works from metzia is it

00:31:33,440 --> 00:31:38,600
actually applying the workout sergeant

00:31:37,100 --> 00:31:41,540
the connection between CEOs and scholar

00:31:38,600 --> 00:31:44,600
but no skyline what you're scaling that

00:31:41,540 --> 00:31:48,170
from FC skyline skyline I know how it

00:31:44,600 --> 00:31:50,870
work out of it yes so skyline is a very

00:31:48,170 --> 00:31:53,300
simple tool you set or I think you can

00:31:50,870 --> 00:31:55,640
automatically set possibly thresholds

00:31:53,300 --> 00:31:57,920
and things cross them so it doesn't do

00:31:55,640 --> 00:32:01,130
any kind of learning along these lines

00:31:57,920 --> 00:32:02,690
so that's an example of applications

00:32:01,130 --> 00:32:05,450
which yeah yeah there's many

00:32:02,690 --> 00:32:09,740
applications for this and it's always

00:32:05,450 --> 00:32:14,510
places where systems are not possible to

00:32:09,740 --> 00:32:17,000
be watched continuously implementations

00:32:14,510 --> 00:32:20,750
examples wits actually power plants

00:32:17,000 --> 00:32:22,340
medical instrumentation oil fields where

00:32:20,750 --> 00:32:24,800
they're drilling and I'm in software

00:32:22,340 --> 00:32:26,660
wise like what software wise with us no

00:32:24,800 --> 00:32:29,180
I don't have an example I mean this all

00:32:26,660 --> 00:32:33,520
is on github but it's a little bit small

00:32:29,180 --> 00:32:36,260
to be used in any honest-to-god

00:32:33,520 --> 00:32:38,120
applications I don't know of any open

00:32:36,260 --> 00:32:41,300
source software that does this in a

00:32:38,120 --> 00:32:45,770
general way now this is very very close

00:32:41,300 --> 00:32:49,880
to things like the open TS DB which is

00:32:45,770 --> 00:32:52,430
an HBase based system that uses the two

00:32:49,880 --> 00:32:54,980
advantage the fact that HBase ranges

00:32:52,430 --> 00:32:57,560
things with sequential Keys together and

00:32:54,980 --> 00:32:59,950
so that you can read stuff and it also

00:32:57,560 --> 00:33:03,920
uses this idea that you would compress

00:32:59,950 --> 00:33:05,450
historical data into little blobs now it

00:33:03,920 --> 00:33:08,810
has the problem that it stores the exact

00:33:05,450 --> 00:33:11,300
data in the database and so it's limited

00:33:08,810 --> 00:33:13,820
speed wise dramatically I mean without

00:33:11,300 --> 00:33:15,890
that one limitation and with a real-time

00:33:13,820 --> 00:33:17,990
data store of other forms you can get

00:33:15,890 --> 00:33:20,220
and when we've had systems that exceed

00:33:17,990 --> 00:33:23,070
100 million data points per second

00:33:20,220 --> 00:33:25,799
but anywhere any industrial thing where

00:33:23,070 --> 00:33:27,330
things break and cost people money they

00:33:25,799 --> 00:33:30,450
want to find these and so those are the

00:33:27,330 --> 00:33:32,429
applications i bet i don't have openly

00:33:30,450 --> 00:33:35,190
available software we have several

00:33:32,429 --> 00:33:43,409
customers doing this but they don't make

00:33:35,190 --> 00:33:46,010
their software open source yeah we're

00:33:43,409 --> 00:33:48,990
going to have to do traveling salesman I

00:33:46,010 --> 00:33:51,450
thank you for the talk I have a question

00:33:48,990 --> 00:33:53,070
concerning one of the the figures in the

00:33:51,450 --> 00:33:56,580
beginning when you showed the neural

00:33:53,070 --> 00:34:03,150
network so the input and the output

00:33:56,580 --> 00:34:04,799
layer yeah he packin the individual

00:34:03,150 --> 00:34:07,980
neurons in the input and the output

00:34:04,799 --> 00:34:09,960
layer they are the signals at a

00:34:07,980 --> 00:34:11,940
particular instance in time is this

00:34:09,960 --> 00:34:16,530
right it signals at a particular

00:34:11,940 --> 00:34:19,649
instance in time is after windowing so

00:34:16,530 --> 00:34:23,550
of the different input nodes different

00:34:19,649 --> 00:34:27,060
instances in time or yeah and so then we

00:34:23,550 --> 00:34:29,730
have a separate neural net for each half

00:34:27,060 --> 00:34:32,520
window step so here's the first window

00:34:29,730 --> 00:34:36,629
there's this first half window step

00:34:32,520 --> 00:34:40,109
here's the next window over yeah okay

00:34:36,629 --> 00:34:43,290
thank you so I got the right and then

00:34:40,109 --> 00:34:45,000
the the reduced representations the

00:34:43,290 --> 00:34:47,070
compressions that you get in the middle

00:34:45,000 --> 00:34:48,540
layer you talked about the sparseness

00:34:47,070 --> 00:34:54,480
you're using their vote for the

00:34:48,540 --> 00:34:56,399
clustering so I'm what is the intuition

00:34:54,480 --> 00:34:58,530
of what is the reason for young is

00:34:56,399 --> 00:35:00,810
sparse representation in the hidden

00:34:58,530 --> 00:35:02,670
layer I don't you use a smaller hidden

00:35:00,810 --> 00:35:05,130
layer smaller number of nodes nor a

00:35:02,670 --> 00:35:07,470
dimension and the full space like why

00:35:05,130 --> 00:35:10,230
don't you generate disputed it's kind of

00:35:07,470 --> 00:35:12,260
disability representations yeah so that

00:35:10,230 --> 00:35:16,320
that's a very interesting question and

00:35:12,260 --> 00:35:18,660
that's taken mug I can't say that it's

00:35:16,320 --> 00:35:21,020
been answered but since the early 80s

00:35:18,660 --> 00:35:24,630
and since even before with the early

00:35:21,020 --> 00:35:28,710
distributed representation work in the

00:35:24,630 --> 00:35:32,369
mathematical sphere people thought that

00:35:28,710 --> 00:35:33,630
the best way to do a bottleneck

00:35:32,369 --> 00:35:36,539
architecture

00:35:33,630 --> 00:35:39,000
like this or you have the small center

00:35:36,539 --> 00:35:40,920
thing that the best way to do that was

00:35:39,000 --> 00:35:42,869
to have a small number of nodes that all

00:35:40,920 --> 00:35:45,390
could be activated and you would put

00:35:42,869 --> 00:35:47,190
some regularization on that by like try

00:35:45,390 --> 00:35:49,980
to minimize the sum of the squares of

00:35:47,190 --> 00:35:53,579
the weights yeah very very common idea

00:35:49,980 --> 00:35:55,710
and there are results that show that the

00:35:53,579 --> 00:35:58,920
representations you get something like

00:35:55,710 --> 00:36:01,769
the house it totally whatever they do in

00:35:58,920 --> 00:36:04,740
Latin semantic indexing the they're very

00:36:01,769 --> 00:36:07,740
much like an SVD yeah yeah yeah and they

00:36:04,740 --> 00:36:10,200
don't work sometimes sometimes they do

00:36:07,740 --> 00:36:12,690
but not very well you know about current

00:36:10,200 --> 00:36:14,970
work of Google like producing such

00:36:12,690 --> 00:36:17,279
reduced representations for birds in

00:36:14,970 --> 00:36:18,990
language or the old work yeah but the

00:36:17,279 --> 00:36:20,849
best ones are all sparse and the

00:36:18,990 --> 00:36:23,279
sparseness yeah I think you're talking

00:36:20,849 --> 00:36:25,200
about nikka loves work this guy has some

00:36:23,279 --> 00:36:29,130
very interesting ideas that that he

00:36:25,200 --> 00:36:31,259
encodes words and the directions encode

00:36:29,130 --> 00:36:33,509
relationships between words so you can

00:36:31,259 --> 00:36:35,369
have words here and the plurals will all

00:36:33,509 --> 00:36:37,470
be the same direction and distance from

00:36:35,369 --> 00:36:40,019
the originals you can have man and woman

00:36:37,470 --> 00:36:41,819
and you get king and queen same

00:36:40,019 --> 00:36:44,400
direction same magnitude from these

00:36:41,819 --> 00:36:47,789
others and he does use dense

00:36:44,400 --> 00:36:49,170
representations yeah but very few people

00:36:47,789 --> 00:36:51,809
have been able to replicate that work

00:36:49,170 --> 00:36:54,150
and all of the operational systems that

00:36:51,809 --> 00:36:56,130
that are doing seriously good work the

00:36:54,150 --> 00:36:59,750
image recognition systems the speech

00:36:56,130 --> 00:37:03,259
recognition systems use interior sparse

00:36:59,750 --> 00:37:07,259
representations and so it isn't clear

00:37:03,259 --> 00:37:10,289
really it isn't what's going on there in

00:37:07,259 --> 00:37:12,660
a deep sense but it does seem that

00:37:10,289 --> 00:37:17,220
sparsity is a very very important

00:37:12,660 --> 00:37:19,109
property and that's been true it's been

00:37:17,220 --> 00:37:21,230
very clear since the late 90s with the

00:37:19,109 --> 00:37:24,029
compressive sensing literature that

00:37:21,230 --> 00:37:26,549
sparse representations were very very

00:37:24,029 --> 00:37:29,759
powerful in encoding physical phenomena

00:37:26,549 --> 00:37:31,619
you know if you want so in a dense

00:37:29,759 --> 00:37:33,720
representation it if we're looking at a

00:37:31,619 --> 00:37:36,200
time series we have some signal that's

00:37:33,720 --> 00:37:39,450
band-limited we have to have samples

00:37:36,200 --> 00:37:42,480
often enough that we have no aliasing of

00:37:39,450 --> 00:37:44,670
signals that's the Nyquist sort of thing

00:37:42,480 --> 00:37:46,510
we have to sample often to characterize

00:37:44,670 --> 00:37:51,520
a real signal that was the

00:37:46,510 --> 00:37:53,830
from 1800's until recently and if you

00:37:51,520 --> 00:37:58,030
know though that the signal is say the

00:37:53,830 --> 00:38:00,370
mixture of three sinusoids it has to be

00:37:58,030 --> 00:38:03,580
no more than three sinusoids mixture not

00:38:00,370 --> 00:38:07,660
a continuous mixture then you need very

00:38:03,580 --> 00:38:09,730
few data points you need about five to

00:38:07,660 --> 00:38:12,610
be able to characterize it regardless of

00:38:09,730 --> 00:38:15,940
how long a period that is you can find

00:38:12,610 --> 00:38:18,550
out which sinusoids it is well not

00:38:15,940 --> 00:38:21,700
regardless but yeah a little bit of hand

00:38:18,550 --> 00:38:23,590
waving there and so you can use far far

00:38:21,700 --> 00:38:26,410
less information you have a massively

00:38:23,590 --> 00:38:29,950
underdetermined system and with this

00:38:26,410 --> 00:38:32,050
sparsity constraint you can represent

00:38:29,950 --> 00:38:34,950
that now there's also some talk about

00:38:32,050 --> 00:38:36,280
how rotationally symmetric

00:38:34,950 --> 00:38:38,980
representations the dense

00:38:36,280 --> 00:38:41,860
representations you talk about have

00:38:38,980 --> 00:38:43,750
lower learning complexity and large

00:38:41,860 --> 00:38:46,450
dimensions have higher dimensionals and

00:38:43,750 --> 00:38:49,030
non rotational ones higher complexity

00:38:46,450 --> 00:38:50,980
but the fact is that these high

00:38:49,030 --> 00:38:54,370
dimensional non rotationally symmetric

00:38:50,980 --> 00:38:58,390
systems that are sparse have very very

00:38:54,370 --> 00:39:00,160
low learning complexity and the

00:38:58,390 --> 00:39:02,260
representations are very easy to learn

00:39:00,160 --> 00:39:04,060
and they're easy to interpret they're

00:39:02,260 --> 00:39:05,830
easy to search for I mean like an

00:39:04,060 --> 00:39:08,590
elastic search can take these first

00:39:05,830 --> 00:39:10,330
things and drop them into an index and

00:39:08,590 --> 00:39:11,740
so there's some very important

00:39:10,330 --> 00:39:14,140
properties as i say not well understood

00:39:11,740 --> 00:39:15,910
but it does appear to be very very

00:39:14,140 --> 00:39:18,730
important and it does appear to be

00:39:15,910 --> 00:39:22,480
biologically very plausible built into

00:39:18,730 --> 00:39:24,730
us and we are better at this than any

00:39:22,480 --> 00:39:28,060
computer so far so it's not a bad thing

00:39:24,730 --> 00:39:31,090
to take a hint thank you not that

00:39:28,060 --> 00:39:33,510
airplanes or birds but the hints are

00:39:31,090 --> 00:39:33,510
still good

00:39:36,940 --> 00:39:42,609
I was wondering um in one of the grass

00:39:39,190 --> 00:39:47,530
he showed a zigzag line with a spike and

00:39:42,609 --> 00:39:49,210
then a systemic shift that may occur if

00:39:47,530 --> 00:39:51,369
for example you're measuring performance

00:39:49,210 --> 00:39:54,160
of a website and there's a new release

00:39:51,369 --> 00:39:58,210
and it turns out to be a lot slower or

00:39:54,160 --> 00:40:00,190
faster how would you a update to model

00:39:58,210 --> 00:40:02,740
would that be a continuous process or

00:40:00,190 --> 00:40:05,170
would that be some other way what do you

00:40:02,740 --> 00:40:07,660
think that should be done you know

00:40:05,170 --> 00:40:08,829
somebody talks about the heart there's

00:40:07,660 --> 00:40:11,140
very few hard problems in computer

00:40:08,829 --> 00:40:12,910
science it's like naming schemes and

00:40:11,140 --> 00:40:15,250
cache coherence and so on well there's

00:40:12,910 --> 00:40:18,970
very few Universal answers but the the

00:40:15,250 --> 00:40:23,339
universal answer is it depends and you

00:40:18,970 --> 00:40:26,530
just earned that that answer but the the

00:40:23,339 --> 00:40:28,530
it starts with the thing that when you

00:40:26,530 --> 00:40:30,760
detect an anomaly it may be good or bad

00:40:28,530 --> 00:40:33,310
you may have just released a new much

00:40:30,760 --> 00:40:35,020
better system like you say and that

00:40:33,310 --> 00:40:37,300
would be an anomaly this is not working

00:40:35,020 --> 00:40:39,369
the way it used to system panics and it

00:40:37,300 --> 00:40:41,530
goes uh huh and you go it's all right

00:40:39,369 --> 00:40:43,480
it's all right I know I know about that

00:40:41,530 --> 00:40:44,890
that's good you know you want the alarms

00:40:43,480 --> 00:40:46,390
go off when you do something like that

00:40:44,890 --> 00:40:49,089
it's a little bit of confidence

00:40:46,390 --> 00:40:51,280
inspiring sort of thing how do you train

00:40:49,089 --> 00:40:54,970
after that well if the world has changed

00:40:51,280 --> 00:40:57,819
like the shift then training in an

00:40:54,970 --> 00:41:01,930
online fashion that keeps a big memory

00:40:57,819 --> 00:41:04,599
of the past is disastrously bad starting

00:41:01,930 --> 00:41:07,180
over at zero and having a very low

00:41:04,599 --> 00:41:08,680
resolution model for a short time that

00:41:07,180 --> 00:41:12,130
gets better and better as you go along

00:41:08,680 --> 00:41:14,730
now the past could give you prejudices

00:41:12,130 --> 00:41:19,359
about what kinds of models you might see

00:41:14,730 --> 00:41:21,430
some priors but the details of the

00:41:19,359 --> 00:41:25,270
previous model are the things that need

00:41:21,430 --> 00:41:27,460
to be rejected quite quite decisively at

00:41:25,270 --> 00:41:30,160
that moment and then the classes of

00:41:27,460 --> 00:41:34,150
models the priors can still inform you

00:41:30,160 --> 00:41:36,369
so you can learn very quickly but yeah

00:41:34,150 --> 00:41:38,530
if things change you have to change the

00:41:36,369 --> 00:41:43,030
models otherwise the alarm will just go

00:41:38,530 --> 00:41:45,619
off forever and that's not useful there

00:41:43,030 --> 00:41:47,660
was somebody in the back yeah

00:41:45,619 --> 00:41:49,910
there were several people in the back

00:41:47,660 --> 00:41:57,259
actually yeah we'll have to make a

00:41:49,910 --> 00:41:59,119
backside tour of the microphone you ever

00:41:57,259 --> 00:42:01,039
noticed that when people smash into the

00:41:59,119 --> 00:42:04,609
room they always stand in the back so

00:42:01,039 --> 00:42:07,039
obviously shy people come late yeah um

00:42:04,609 --> 00:42:09,140
so all the techniques that you that you

00:42:07,039 --> 00:42:11,390
showed assume that some windowing

00:42:09,140 --> 00:42:13,880
happens before so all of the time series

00:42:11,390 --> 00:42:17,569
the edge techniques yeah so does that

00:42:13,880 --> 00:42:19,579
mean that they apply to two scenarios

00:42:17,569 --> 00:42:21,980
where the length of a period is constant

00:42:19,579 --> 00:42:24,140
that is and is known upfront or does it

00:42:21,980 --> 00:42:26,390
also work with the variable over the

00:42:24,140 --> 00:42:27,740
varian varying period length or is there

00:42:26,390 --> 00:42:31,609
simple trick to make it work with a

00:42:27,740 --> 00:42:32,990
varying period length oh there's a

00:42:31,609 --> 00:42:35,630
simple trick in the data that I had

00:42:32,990 --> 00:42:37,490
which was pick a window length it's

00:42:35,630 --> 00:42:40,640
small enough that the clustering will

00:42:37,490 --> 00:42:43,190
finish before sunday comes so that was

00:42:40,640 --> 00:42:45,470
that was my strategy there and then oh

00:42:43,190 --> 00:42:48,710
my god it sunday morning it works that

00:42:45,470 --> 00:42:51,200
was really good now the real world is

00:42:48,710 --> 00:42:52,730
not usually so forgiving and and

00:42:51,200 --> 00:42:55,400
requires a little bit more thought than

00:42:52,730 --> 00:42:58,099
that then this is just what I can get to

00:42:55,400 --> 00:43:00,230
work and there's there's two

00:42:58,099 --> 00:43:02,299
considerations one is how large do you

00:43:00,230 --> 00:43:05,869
make I'm trying to find you the prize

00:43:02,299 --> 00:43:07,190
that I award for a good question yeah

00:43:05,869 --> 00:43:11,599
because that really is a deep question

00:43:07,190 --> 00:43:13,579
what size do you make that window and in

00:43:11,599 --> 00:43:18,230
the signal that i showed you make it as

00:43:13,579 --> 00:43:21,319
short as plausible to still capture the

00:43:18,230 --> 00:43:23,089
short time scale features and then you

00:43:21,319 --> 00:43:25,730
ask do you use a variable time window

00:43:23,089 --> 00:43:27,319
and the answer is if you really have the

00:43:25,730 --> 00:43:30,499
deep learning stuff so that the window i

00:43:27,319 --> 00:43:34,039
had was too short for instance to detect

00:43:30,499 --> 00:43:36,619
a change in the heart rate which is

00:43:34,039 --> 00:43:41,200
pretty important you know as heart

00:43:36,619 --> 00:43:46,099
signals go but the if we start building

00:43:41,200 --> 00:43:48,799
the deep learning system where it has

00:43:46,099 --> 00:43:50,509
higher and higher levels i'm totally

00:43:48,799 --> 00:43:54,140
distracted here but if you start

00:43:50,509 --> 00:43:58,680
building the the higher levels then you

00:43:54,140 --> 00:44:02,700
there it is can you catch there you go

00:43:58,680 --> 00:44:05,520
whoa no can I throw is the question I

00:44:02,700 --> 00:44:10,750
was off by an entire meter out of 10

00:44:05,520 --> 00:44:13,710
yeah so anyway the the question is that

00:44:10,750 --> 00:44:17,260
the deeper layers will start in coding

00:44:13,710 --> 00:44:19,300
regions of Windows and so they start

00:44:17,260 --> 00:44:21,940
gathering data from a longer time string

00:44:19,300 --> 00:44:25,830
and so the low level window can be

00:44:21,940 --> 00:44:28,690
usually fixed and relatively short and

00:44:25,830 --> 00:44:33,970
then the deeper layers start encoding

00:44:28,690 --> 00:44:36,880
the larger signals basically symbol

00:44:33,970 --> 00:44:39,430
co-occurrence heartbeat there nothing

00:44:36,880 --> 00:44:43,900
heartbeat with a shift over here that

00:44:39,430 --> 00:44:49,000
means heart rate buxom so that's the

00:44:43,900 --> 00:44:52,420
idea now details will vary when this

00:44:49,000 --> 00:44:55,330
really hits a real problem it won't be

00:44:52,420 --> 00:44:57,580
looking quite like this so that was a

00:44:55,330 --> 00:44:59,470
good question and then this lady over

00:44:57,580 --> 00:45:03,300
there sorry running out of the talk

00:44:59,470 --> 00:45:03,300
we're out of time catch me in the hall

00:45:04,410 --> 00:45:11,140
such a sadness oh good she says one of

00:45:09,280 --> 00:45:13,120
the questions was answered by pure luck

00:45:11,140 --> 00:45:16,660
and and clever see you did deserve the

00:45:13,120 --> 00:45:18,770
little now that is open-source candy so

00:45:16,660 --> 00:45:20,830
you have to share

00:45:18,770 --> 00:45:20,830

YouTube URL: https://www.youtube.com/watch?v=Q3DkkXElBtQ


