Title: Berlin Buzzwords 2014: Eric Evans - Time Series Data With Apache Cassandra #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Whether it's statistics, weather forecasting, astronomy, finance, or network management, time series data plays a critical role in analytics and forecasting. Unfortunately, while many tools exist for time series storage and analysis, few are able to scale past memory limits, or provide rich query and analytics capabilities outside what is necessary to produce simple plots; For those challenged by large volumes of data, there is much room for improvement.

Apache Cassandra is a fully distributed second-generation database. Cassandra stores data in key-sorted order making it ideal for time series, and its high throughput and linear scalability make it well suited to very large data sets.

This talk will cover some of the requirements and challenges of large scale time series storage and analysis. Cassandra data and query modeling for this use-case will be discussed, and Newts, an open source Cassandra-based time series store under development at The OpenNMS Group will be introduced.

Read more:
https://2014.berlinbuzzwords.de/session/time-series-data-apache-cassandra

About Eric Evans:
https://2014.berlinbuzzwords.de/user/200/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,689 --> 00:00:09,769
all right then I guess we'll get started

00:00:10,639 --> 00:00:18,509
so my name is eric evans i'm a member of

00:00:14,460 --> 00:00:19,939
the Cassandra pmc been with the project

00:00:18,509 --> 00:00:23,430
since about the time it entered the

00:00:19,939 --> 00:00:26,490
incubator working for rackspace at the

00:00:23,430 --> 00:00:29,609
time and i work for a company called the

00:00:26,490 --> 00:00:33,390
open end ms group now before i get

00:00:29,609 --> 00:00:36,629
started i'm going to try to correct an

00:00:33,390 --> 00:00:37,979
ongoing problem I have it's happened

00:00:36,629 --> 00:00:40,949
it's happened even here at buzz words

00:00:37,979 --> 00:00:42,960
where I'll be talking about opennms for

00:00:40,949 --> 00:00:46,110
a while I'll be well into a conversation

00:00:42,960 --> 00:00:49,409
and someone will say what did you say

00:00:46,110 --> 00:00:51,659
what did you say the name was is it so

00:00:49,409 --> 00:00:53,369
if you heard if if you weren't able to

00:00:51,659 --> 00:00:55,589
hear the acronym I know this is all my

00:00:53,369 --> 00:00:58,559
fault I I don't enunciate it well enough

00:00:55,589 --> 00:01:02,549
I say too fast so if you're thinking did

00:00:58,559 --> 00:01:07,470
he say sorry mark things that work in

00:01:02,549 --> 00:01:13,259
here if you're thinking did he say open

00:01:07,470 --> 00:01:15,630
eminem's no but I like this idea so if

00:01:13,259 --> 00:01:19,289
anybody wants to work on this let me

00:01:15,630 --> 00:01:24,990
know if you're thinking did he say open

00:01:19,289 --> 00:01:27,359
enemas no I did not and if you want to

00:01:24,990 --> 00:01:30,170
work on this leave me out of it I want

00:01:27,359 --> 00:01:34,439
no part of it no part of this no it's

00:01:30,170 --> 00:01:36,450
nms for network management system which

00:01:34,439 --> 00:01:39,780
is convenient because it is in fact a

00:01:36,450 --> 00:01:42,450
system for managing networks so what

00:01:39,780 --> 00:01:45,780
this means is discovery of your network

00:01:42,450 --> 00:01:47,850
what devices are running on it what kind

00:01:45,780 --> 00:01:51,719
of devices they are is it a cisco switch

00:01:47,850 --> 00:01:54,960
or a linux host running 312 what are the

00:01:51,719 --> 00:01:57,929
capabilities what services what agents

00:01:54,960 --> 00:01:59,249
are running on it what's the topology of

00:01:57,929 --> 00:02:01,679
the network how are things

00:01:59,249 --> 00:02:04,770
interconnected what's the critical path

00:02:01,679 --> 00:02:06,030
between devices and then it you'll use

00:02:04,770 --> 00:02:07,950
this information the sort of self

00:02:06,030 --> 00:02:09,840
configure to provision the nodes and

00:02:07,950 --> 00:02:13,080
then you get kind of an asset database

00:02:09,840 --> 00:02:16,080
in the process service monitoring is

00:02:13,080 --> 00:02:19,200
everything from from something as simple

00:02:16,080 --> 00:02:20,959
as ICMP of an IP interface

00:02:19,200 --> 00:02:23,849
up to you know rather sophisticated

00:02:20,959 --> 00:02:28,140
synthetic transactions clicking through

00:02:23,849 --> 00:02:30,870
web page and validating the result we do

00:02:28,140 --> 00:02:32,400
data collection so this is our time

00:02:30,870 --> 00:02:34,769
series use case this is basically what

00:02:32,400 --> 00:02:38,310
the rest of the talk will be about this

00:02:34,769 --> 00:02:40,940
is the collection the storage of time

00:02:38,310 --> 00:02:49,110
series data trending values over time

00:02:40,940 --> 00:02:51,330
threshold engraving the reception of

00:02:49,110 --> 00:02:54,230
external events generating events for

00:02:51,330 --> 00:02:57,209
service failures threshold failures

00:02:54,230 --> 00:03:00,810
deduplication correlation of events and

00:02:57,209 --> 00:03:04,440
turning those into notifications opennms

00:03:00,810 --> 00:03:06,209
is a rather old project amateur project

00:03:04,440 --> 00:03:09,750
it's been around for about 15 years in

00:03:06,209 --> 00:03:11,420
the dog years of the internet and

00:03:09,750 --> 00:03:14,700
software that's that's quite a long time

00:03:11,420 --> 00:03:19,500
it's also it is written in Java and it's

00:03:14,700 --> 00:03:22,019
a free software open source software for

00:03:19,500 --> 00:03:24,389
time series data we currently use our D

00:03:22,019 --> 00:03:29,069
tool our d stands for round robin

00:03:24,389 --> 00:03:31,500
database our d to liz is also a very

00:03:29,069 --> 00:03:33,660
very mature project also have been

00:03:31,500 --> 00:03:37,680
around for about 15 years are ubiquitous

00:03:33,660 --> 00:03:39,569
used in a lot of well-known tools it's

00:03:37,680 --> 00:03:42,569
austin stably a serie a solution for

00:03:39,569 --> 00:03:44,420
time series data one that's file based

00:03:42,569 --> 00:03:47,639
and that you you get to sort of

00:03:44,420 --> 00:03:50,750
automatic incremental aggregation so the

00:03:47,639 --> 00:03:53,370
way this works is you create an RD and

00:03:50,750 --> 00:03:54,510
in doing so define all of the metrics

00:03:53,370 --> 00:03:57,000
that will that should be stored within

00:03:54,510 --> 00:04:01,950
it and the aggregations that correspond

00:03:57,000 --> 00:04:04,859
to those metrics so five minute averages

00:04:01,950 --> 00:04:08,250
for you know you want to store 30 days

00:04:04,859 --> 00:04:10,560
of those or one hour averages over the

00:04:08,250 --> 00:04:12,359
course of a year and in doing so all of

00:04:10,560 --> 00:04:15,450
the space that will ever need is

00:04:12,359 --> 00:04:17,250
allocated and each time you update an RD

00:04:15,450 --> 00:04:18,959
with a metric you're incorporating that

00:04:17,250 --> 00:04:22,049
that new value into each of the

00:04:18,959 --> 00:04:24,450
aggregations and when they get so old

00:04:22,049 --> 00:04:26,280
that that they exceed the number you've

00:04:24,450 --> 00:04:27,540
you've allocated they just fall at the

00:04:26,280 --> 00:04:31,470
end

00:04:27,540 --> 00:04:32,940
and it does graphing in fact this is

00:04:31,470 --> 00:04:36,060
actually what it does that's why i said

00:04:32,940 --> 00:04:38,070
ostensibly time series it's really all

00:04:36,060 --> 00:04:40,350
about graphing and everything else is a

00:04:38,070 --> 00:04:44,610
means to this end this is why it does

00:04:40,350 --> 00:04:46,410
aggregations actually it's it's why you

00:04:44,610 --> 00:04:48,870
only get aggregations the raw date is

00:04:46,410 --> 00:04:51,450
not available if you think of the

00:04:48,870 --> 00:04:54,780
problem of drawing a graph you've got a

00:04:51,450 --> 00:04:57,680
canvas of some finite size say 400

00:04:54,780 --> 00:05:00,300
pixels in width and you want to plot

00:04:57,680 --> 00:05:01,620
data that's a sample the five minute

00:05:00,300 --> 00:05:03,330
intervals and you want to draw the last

00:05:01,620 --> 00:05:05,340
six months you've clearly got clearly

00:05:03,330 --> 00:05:07,320
have more data points than you have

00:05:05,340 --> 00:05:09,960
pixels to draw them in so you need to

00:05:07,320 --> 00:05:12,300
aggregate that down you also want the

00:05:09,960 --> 00:05:14,940
points along the x-axis to be evenly

00:05:12,300 --> 00:05:17,370
distributed and aligned on a common time

00:05:14,940 --> 00:05:18,960
boundary and and all of this

00:05:17,370 --> 00:05:20,760
normalization is what these aggregations

00:05:18,960 --> 00:05:23,250
are for they're meant to be plugged

00:05:20,760 --> 00:05:25,050
directly into the graph and it's kind of

00:05:23,250 --> 00:05:26,760
cool having an all-in-one solution for

00:05:25,050 --> 00:05:29,310
you know storing the data and getting a

00:05:26,760 --> 00:05:31,770
graph but there is an ugly downside to

00:05:29,310 --> 00:05:35,340
it and that is the the IO involved this

00:05:31,770 --> 00:05:37,550
is a read-modify-write so each update of

00:05:35,340 --> 00:05:41,160
an RD involves reading and writing

00:05:37,550 --> 00:05:43,500
file-based metadata and you know per

00:05:41,160 --> 00:05:45,750
metric metadata and of course the the

00:05:43,500 --> 00:05:47,760
actual values themselves and so kind of

00:05:45,750 --> 00:05:50,690
at a minimum you get 5 I ops just to

00:05:47,760 --> 00:05:53,130
update a single metric so if you have

00:05:50,690 --> 00:05:55,080
hundreds of thousands or millions of

00:05:53,130 --> 00:05:56,880
metrics then that's going to be

00:05:55,080 --> 00:06:00,150
thousands or tens of thousands of I ops

00:05:56,880 --> 00:06:03,360
even it you know rather long of sample

00:06:00,150 --> 00:06:05,490
interval of say five minutes to put that

00:06:03,360 --> 00:06:07,320
into perspective even a really nice

00:06:05,490 --> 00:06:10,920
high-end rotational hard drive is only

00:06:07,320 --> 00:06:12,630
good for about 200 I I ops it's tempting

00:06:10,920 --> 00:06:15,540
to say well that's that's certainly

00:06:12,630 --> 00:06:19,080
within range of an SSD but since you get

00:06:15,540 --> 00:06:21,360
a finite number of deletes or overwrites

00:06:19,080 --> 00:06:24,930
in an SSD and since each one of these

00:06:21,360 --> 00:06:26,970
updates constitutes a new right you'll

00:06:24,930 --> 00:06:29,160
actually you know the longevity of an

00:06:26,970 --> 00:06:31,650
SSD is really poor under this kind of

00:06:29,160 --> 00:06:33,180
workload it's a very high volume and you

00:06:31,650 --> 00:06:36,810
sort of run through the lifespan of the

00:06:33,180 --> 00:06:38,340
drive rather quickly dear am based SSD

00:06:36,810 --> 00:06:40,110
is a really good fit but they're also

00:06:38,340 --> 00:06:40,950
very expensive and that's kind of what

00:06:40,110 --> 00:06:43,890
this slide is

00:06:40,950 --> 00:06:45,330
to show that we're kind of the end of

00:06:43,890 --> 00:06:49,380
the runway in terms of vertical

00:06:45,330 --> 00:06:51,120
scalability as engineers we you know we

00:06:49,380 --> 00:06:53,250
try to make everything as optimal as

00:06:51,120 --> 00:06:54,810
possible and so the idea of simply

00:06:53,250 --> 00:06:58,350
throwing more hardware resources at it

00:06:54,810 --> 00:06:59,850
is kind of unpalatable but vertical

00:06:58,350 --> 00:07:01,680
scaling is actually a great way to go if

00:06:59,850 --> 00:07:04,140
you just usually the cheapest thing you

00:07:01,680 --> 00:07:05,940
can do but only so long as you're

00:07:04,140 --> 00:07:08,700
available availability of inexpensive

00:07:05,940 --> 00:07:12,000
commodity hardware outstrips your ax

00:07:08,700 --> 00:07:15,690
your actual needs and we're clearly I

00:07:12,000 --> 00:07:16,980
passed that point or getting there so in

00:07:15,690 --> 00:07:20,040
other words we're sort of drinking from

00:07:16,980 --> 00:07:24,210
the proverbial fire hose we have more

00:07:20,040 --> 00:07:25,620
data than we can easily write out this

00:07:24,210 --> 00:07:27,120
is kind of a bad problem for us this is

00:07:25,620 --> 00:07:28,290
supposed to be in our wheelhouse this is

00:07:27,120 --> 00:07:31,260
something that you know it's kind of a

00:07:28,290 --> 00:07:34,560
you know core functionality networks are

00:07:31,260 --> 00:07:36,810
getting bigger more complex more devices

00:07:34,560 --> 00:07:39,450
so more metrics to collect the Internet

00:07:36,810 --> 00:07:41,010
of Things is upon us and threatens to

00:07:39,450 --> 00:07:43,710
bring you know just an explosion in the

00:07:41,010 --> 00:07:45,120
number of devices and and and we don't

00:07:43,710 --> 00:07:47,970
really have a good enough right

00:07:45,120 --> 00:07:51,270
throughput as it is it's also

00:07:47,970 --> 00:07:54,000
interesting maybe maybe ironic that the

00:07:51,270 --> 00:07:56,670
source of the heiio is is aggregations

00:07:54,000 --> 00:07:59,340
the aggregations exist explicitly for

00:07:56,670 --> 00:08:01,710
the purpose of graphing and yet we graph

00:07:59,340 --> 00:08:04,680
a very very small amount of the data we

00:08:01,710 --> 00:08:07,620
collect it probably doesn't seem like

00:08:04,680 --> 00:08:09,690
that to our users but you know the

00:08:07,620 --> 00:08:11,460
average knock if it has you know 10 15

00:08:09,690 --> 00:08:13,140
20 people that's probably a pretty good

00:08:11,460 --> 00:08:15,330
sized team working on any given shift

00:08:13,140 --> 00:08:17,430
maybe they have a wall board projected

00:08:15,330 --> 00:08:19,350
up with you know graphs of all their

00:08:17,430 --> 00:08:21,630
land links you know half a dozen let's

00:08:19,350 --> 00:08:23,310
say you know they may be periodically

00:08:21,630 --> 00:08:25,620
looking at graphs of an hour's worth of

00:08:23,310 --> 00:08:27,300
data or last hours the last day's worth

00:08:25,620 --> 00:08:28,650
of data you know throughout the day to

00:08:27,300 --> 00:08:30,570
troubleshoot issues but I mean how many

00:08:28,650 --> 00:08:32,160
grafts could because somebody actually

00:08:30,570 --> 00:08:34,620
look at within a given day or

00:08:32,160 --> 00:08:36,540
meaningfully you know for millions of

00:08:34,620 --> 00:08:38,640
metrics what actually ends up getting

00:08:36,540 --> 00:08:40,830
red is just a very small fraction of the

00:08:38,640 --> 00:08:43,440
data so this is a really poor trade-off

00:08:40,830 --> 00:08:45,390
to be to be aggregating everything it's

00:08:43,440 --> 00:08:47,130
like we're optimizing to read every

00:08:45,390 --> 00:08:49,520
single solitary second of every minute

00:08:47,130 --> 00:08:53,180
of every metric we have at least once

00:08:49,520 --> 00:08:55,649
and almost the exact opposite is true

00:08:53,180 --> 00:08:58,800
there are other problems with our ID for

00:08:55,649 --> 00:09:01,290
us as well not everything is a graph and

00:08:58,800 --> 00:09:03,450
so you're trying to perform other forms

00:09:01,290 --> 00:09:05,700
of analytics on data that's spread out

00:09:03,450 --> 00:09:08,100
through the file system using api's

00:09:05,700 --> 00:09:09,450
they're really designed for graphing and

00:09:08,100 --> 00:09:11,730
the data is already aggregated so

00:09:09,450 --> 00:09:14,880
there's a loss of resolution that's not

00:09:11,730 --> 00:09:17,490
ideal rd is a little bit inflexible in

00:09:14,880 --> 00:09:18,570
order for us to to first store data we

00:09:17,490 --> 00:09:22,290
would have had to have known about it

00:09:18,570 --> 00:09:24,000
ahead of time created an already to put

00:09:22,290 --> 00:09:26,120
it in and then once you do that you

00:09:24,000 --> 00:09:28,560
can't really change them after the fact

00:09:26,120 --> 00:09:31,829
we have a variety of problems that have

00:09:28,560 --> 00:09:33,930
proven intractable to solve incremental

00:09:31,829 --> 00:09:36,060
backups is kind of a classic example it

00:09:33,930 --> 00:09:38,339
is file based so you can simply back up

00:09:36,060 --> 00:09:40,620
the file system but due to the way the

00:09:38,339 --> 00:09:43,190
bites are laid out on disk there's no

00:09:40,620 --> 00:09:48,510
easy way to get at just what's changed

00:09:43,190 --> 00:09:49,980
and then perhaps most importantly you

00:09:48,510 --> 00:09:51,630
know we're a network management system

00:09:49,980 --> 00:09:54,360
so if there's a problem with the network

00:09:51,630 --> 00:09:57,959
we're expected to be available in order

00:09:54,360 --> 00:09:59,490
to to notify operations there's a

00:09:57,959 --> 00:10:01,380
problem we're expected to be available

00:09:59,490 --> 00:10:03,839
so that we can be used to troubleshoot

00:10:01,380 --> 00:10:06,240
and mitigate the problem but we aren't

00:10:03,839 --> 00:10:09,089
in fact dependent upon the network and

00:10:06,240 --> 00:10:11,699
so saying that we you know we rely on

00:10:09,089 --> 00:10:12,630
the file system is a fancy way of saying

00:10:11,699 --> 00:10:18,180
that we have a single point of failure

00:10:12,630 --> 00:10:19,829
and so that's not at all ideal one

00:10:18,180 --> 00:10:22,199
takeaway i get from rd or at least I

00:10:19,829 --> 00:10:25,680
think it's already that kind of pointed

00:10:22,199 --> 00:10:29,010
me at this is that we access metrics in

00:10:25,680 --> 00:10:31,440
groups already encourages this that you

00:10:29,010 --> 00:10:33,600
put similar or related metrics that you

00:10:31,440 --> 00:10:36,540
intend to access together within a

00:10:33,600 --> 00:10:39,510
single rd and that is that is what we do

00:10:36,540 --> 00:10:43,050
that is the natural way of modeling this

00:10:39,510 --> 00:10:45,810
type of data if you think of graphing

00:10:43,050 --> 00:10:47,399
how likely are you to graph incoming

00:10:45,810 --> 00:10:49,260
bites from a network interface without

00:10:47,399 --> 00:10:55,110
also plotting out going on the same

00:10:49,260 --> 00:10:56,190
graph or coors on a cpu or 15 and 15

00:10:55,110 --> 00:10:57,570
minute load averages you're usually

00:10:56,190 --> 00:10:59,130
going to do these things together in

00:10:57,570 --> 00:11:00,779
fact most of these visualizations are

00:10:59,130 --> 00:11:02,459
interesting when they let you correlate

00:11:00,779 --> 00:11:05,480
multiple data points on a single a

00:11:02,459 --> 00:11:07,610
single graph so we naturally

00:11:05,480 --> 00:11:11,329
collect things in groups and I think

00:11:07,610 --> 00:11:13,399
that's that's an important feature so

00:11:11,329 --> 00:11:15,470
given all of this what does what are

00:11:13,399 --> 00:11:17,449
open in two meses requirements well what

00:11:15,470 --> 00:11:19,279
do we need in the way of a time-series

00:11:17,449 --> 00:11:21,320
storage well we don't have enough

00:11:19,279 --> 00:11:22,940
throughput so we need something that's

00:11:21,320 --> 00:11:25,790
higher throughput we need better

00:11:22,940 --> 00:11:28,040
availability I put late aggregation up

00:11:25,790 --> 00:11:29,870
here we need to be smarter about

00:11:28,040 --> 00:11:32,810
aggregation smarter about expending

00:11:29,870 --> 00:11:34,250
those resources the cost value benefit

00:11:32,810 --> 00:11:36,620
needs to be better and most importantly

00:11:34,250 --> 00:11:38,060
it needs to be decoupled from from

00:11:36,620 --> 00:11:39,230
storage we need to be able to store

00:11:38,060 --> 00:11:43,310
regardless of whether we have the

00:11:39,230 --> 00:11:45,199
capacity to to aggregate those and then

00:11:43,310 --> 00:11:47,180
we need to take advantage of this

00:11:45,199 --> 00:11:48,740
grouped storage and retrieval we know we

00:11:47,180 --> 00:11:50,410
access metrics we didn't collect them

00:11:48,740 --> 00:11:53,389
and store them at the same time so

00:11:50,410 --> 00:11:56,750
whatever we use should should make that

00:11:53,389 --> 00:11:58,040
easy and most storage mechanisms are

00:11:56,750 --> 00:12:00,170
probably going to have a way of

00:11:58,040 --> 00:12:05,149
optimizing for this and making it more

00:12:00,170 --> 00:12:07,220
efficient so from the title of talk it's

00:12:05,149 --> 00:12:09,920
probably no mystery that our solution to

00:12:07,220 --> 00:12:12,139
this involves Cassandra for those of you

00:12:09,920 --> 00:12:14,329
not familiar cassandra is an Apache

00:12:12,139 --> 00:12:16,490
top-level project for a distributed

00:12:14,329 --> 00:12:18,290
database one that is well known to be

00:12:16,490 --> 00:12:21,079
highly available and have high

00:12:18,290 --> 00:12:22,730
throughput and it's also known as a

00:12:21,079 --> 00:12:25,370
solution that utilizes is tunable

00:12:22,730 --> 00:12:27,740
consistency you may have heard eventual

00:12:25,370 --> 00:12:29,480
consistency talking about the same thing

00:12:27,740 --> 00:12:32,569
eventual consistency has kind of a

00:12:29,480 --> 00:12:34,370
negative connotation to it and this is

00:12:32,569 --> 00:12:36,500
this is not a negative or downside I

00:12:34,370 --> 00:12:38,300
would argue this is this is an important

00:12:36,500 --> 00:12:42,529
feature and hopefully I I can

00:12:38,300 --> 00:12:45,019
demonstrate that so let me regale you

00:12:42,529 --> 00:12:46,819
with the reasons why I think Cassandra

00:12:45,019 --> 00:12:50,540
is a really good fit for time series

00:12:46,819 --> 00:12:52,100
data and and for opennms so first

00:12:50,540 --> 00:12:54,560
looking within a given node in the right

00:12:52,100 --> 00:12:57,769
path for a single node you won't

00:12:54,560 --> 00:12:59,300
remember I said that even a nice

00:12:57,769 --> 00:13:02,089
high-end rotational hard drive is only

00:12:59,300 --> 00:13:03,949
good for a couple hundred I ops this is

00:13:02,089 --> 00:13:06,620
this is caused by the fact that they're

00:13:03,949 --> 00:13:08,360
mechanical and you know to start a new

00:13:06,620 --> 00:13:10,160
operation to read or write you first

00:13:08,360 --> 00:13:11,779
must position the head and there's

00:13:10,160 --> 00:13:14,089
rotational latency how long does it take

00:13:11,779 --> 00:13:16,490
to move the platter at least 180 degrees

00:13:14,089 --> 00:13:18,319
as all this is mechanical it all works

00:13:16,490 --> 00:13:19,040
at fixed speeds and so you can only do

00:13:18,319 --> 00:13:21,199
so many of the

00:13:19,040 --> 00:13:23,089
was in the given period of time but once

00:13:21,199 --> 00:13:24,560
you start an operation once you position

00:13:23,089 --> 00:13:27,019
the head and you're ready to go they can

00:13:24,560 --> 00:13:28,730
move a tremendous amount of data so what

00:13:27,019 --> 00:13:31,910
Cassandra tries to do is optimize for

00:13:28,730 --> 00:13:33,440
that sequential disk access and the way

00:13:31,910 --> 00:13:36,259
it does that is with a log structured

00:13:33,440 --> 00:13:38,149
right path so the client starts by

00:13:36,259 --> 00:13:40,880
writing the data into an in-memory

00:13:38,149 --> 00:13:43,279
structure the client right lands in an

00:13:40,880 --> 00:13:45,829
in-memory structure that's maintained in

00:13:43,279 --> 00:13:48,440
sorted order and it's also written to a

00:13:45,829 --> 00:13:50,060
commit log the commit log is simply

00:13:48,440 --> 00:13:51,319
crash recovery is just to replay back

00:13:50,060 --> 00:13:54,949
into the mem table that there's an

00:13:51,319 --> 00:13:57,139
unexpected outage so we don't read from

00:13:54,949 --> 00:13:58,310
that and thus its append only that's

00:13:57,139 --> 00:14:00,350
what that keeps the disk access

00:13:58,310 --> 00:14:02,089
sequential when a threshold is reached

00:14:00,350 --> 00:14:04,040
the mem table is flushed to disk to

00:14:02,089 --> 00:14:06,860
create an SS table and they're never

00:14:04,040 --> 00:14:09,680
updated in place a new one is created

00:14:06,860 --> 00:14:12,529
each time so those are sequential as

00:14:09,680 --> 00:14:14,389
well and what this means is that we're

00:14:12,529 --> 00:14:17,060
optimized for write throughput which is

00:14:14,389 --> 00:14:18,769
exactly what we need and the fact that

00:14:17,060 --> 00:14:21,380
sorted on disk makes it a really great

00:14:18,769 --> 00:14:23,990
fit for time series because you know we

00:14:21,380 --> 00:14:27,500
we sample things and store them by time

00:14:23,990 --> 00:14:29,240
time is obviously sorted when you

00:14:27,500 --> 00:14:31,100
retrieve this data we expect to start

00:14:29,240 --> 00:14:34,579
with you know some time in the past and

00:14:31,100 --> 00:14:36,680
and you know take a range of data up to

00:14:34,579 --> 00:14:38,959
some time either less in the past to the

00:14:36,680 --> 00:14:41,420
present and we expect to get it back in

00:14:38,959 --> 00:14:46,490
an in ascending order and that's exactly

00:14:41,420 --> 00:14:48,199
the way it's persisted pulling back out

00:14:46,490 --> 00:14:50,839
of a single node and looking at the

00:14:48,199 --> 00:14:52,190
cluster as a whole obviously if we want

00:14:50,839 --> 00:14:54,610
to distribute this data we need a way of

00:14:52,190 --> 00:14:57,470
partitioning the cluster so that we can

00:14:54,610 --> 00:15:00,050
assign the data to the nodes within it

00:14:57,470 --> 00:15:02,240
so the way we usually visualize this if

00:15:00,050 --> 00:15:04,189
you imagine a namespace and covering and

00:15:02,240 --> 00:15:07,339
encompassing all possible primary keys

00:15:04,189 --> 00:15:09,889
and you mapped it on to sort of a ring

00:15:07,339 --> 00:15:11,839
or clockface in ascending order sort of

00:15:09,889 --> 00:15:14,389
sorted in ascending order working

00:15:11,839 --> 00:15:16,550
clockwise around the ring lowest value

00:15:14,389 --> 00:15:18,769
at 12 and then wrapping around to the

00:15:16,550 --> 00:15:20,870
highest value of 12 then you can

00:15:18,769 --> 00:15:24,050
position the nodes within within this

00:15:20,870 --> 00:15:25,670
this namespace and a partition simply

00:15:24,050 --> 00:15:27,500
becomes the interval between where a

00:15:25,670 --> 00:15:29,930
node resides on the ring and and the

00:15:27,500 --> 00:15:31,490
preceding node so when you want to know

00:15:29,930 --> 00:15:32,250
where something goes you just find its

00:15:31,490 --> 00:15:35,580
sort order

00:15:32,250 --> 00:15:38,250
and put it on that node additional

00:15:35,580 --> 00:15:41,610
copies you just use something that's

00:15:38,250 --> 00:15:43,290
that's that's deterministic based on the

00:15:41,610 --> 00:15:45,240
first location once they're all

00:15:43,290 --> 00:15:46,470
positioned they're all identical none of

00:15:45,240 --> 00:15:50,040
them are special this is just the

00:15:46,470 --> 00:15:52,170
algorithm for placement once we have

00:15:50,040 --> 00:15:53,790
multiple copies of the data then we have

00:15:52,170 --> 00:15:56,430
to deal with this reality which you may

00:15:53,790 --> 00:15:58,140
have heard you know either here at buzz

00:15:56,430 --> 00:16:01,530
words or perhaps at another conference

00:15:58,140 --> 00:16:03,810
the cap theorem these are all desirable

00:16:01,530 --> 00:16:05,220
properties we want consistency we want

00:16:03,810 --> 00:16:07,140
availability and we want partition

00:16:05,220 --> 00:16:09,360
tolerance but the cap Theory cap theorem

00:16:07,140 --> 00:16:10,980
tells us we can have at most two of

00:16:09,360 --> 00:16:14,490
these at any given at any given time

00:16:10,980 --> 00:16:16,770
it's really pretty intuitive if if you

00:16:14,490 --> 00:16:18,120
were to write a value to two hosts if

00:16:16,770 --> 00:16:20,430
you were going to synchronously

00:16:18,120 --> 00:16:23,430
replicate a value of any value to any

00:16:20,430 --> 00:16:24,990
two hosts and by synchronous I mean

00:16:23,430 --> 00:16:26,400
you're going to write it to both of them

00:16:24,990 --> 00:16:27,750
and it's successful once it's been

00:16:26,400 --> 00:16:30,060
written to both of them then it follows

00:16:27,750 --> 00:16:32,250
that that value is consistent you know

00:16:30,060 --> 00:16:34,200
you've you explicitly made sure that it

00:16:32,250 --> 00:16:36,330
was but if one of those nodes is down

00:16:34,200 --> 00:16:37,440
you can't do that you can't write it to

00:16:36,330 --> 00:16:39,780
both of them because one of them is

00:16:37,440 --> 00:16:42,540
unavailable you've traded availability

00:16:39,780 --> 00:16:45,000
in favor of consistency and likewise for

00:16:42,540 --> 00:16:46,410
an asynchronous replication you may get

00:16:45,000 --> 00:16:48,120
that availability but you lose the

00:16:46,410 --> 00:16:50,040
consistency because you can't reason

00:16:48,120 --> 00:16:52,650
about the data after disconnecting from

00:16:50,040 --> 00:16:54,750
one host that's all this really means

00:16:52,650 --> 00:16:56,610
it's just a way of describing the

00:16:54,750 --> 00:17:00,420
contentious properties of distributed

00:16:56,610 --> 00:17:02,420
storage so how does Cassandra deal with

00:17:00,420 --> 00:17:04,560
this well it's actually quite simple

00:17:02,420 --> 00:17:06,510
rather than making it an all-or-nothing

00:17:04,560 --> 00:17:08,459
proposition we either synchronously

00:17:06,510 --> 00:17:12,209
replicate and consider all of the copies

00:17:08,459 --> 00:17:14,670
on a reed or we asynchronously it's

00:17:12,209 --> 00:17:16,829
tunable how many of the replicas are

00:17:14,670 --> 00:17:20,280
synchronous versus asynchronous and it's

00:17:16,829 --> 00:17:22,670
on a per operation basis so imagine

00:17:20,280 --> 00:17:26,250
replication factor of three and

00:17:22,670 --> 00:17:28,319
Cassandra's most utility of consistency

00:17:26,250 --> 00:17:31,200
levels is quorum which is simply

00:17:28,319 --> 00:17:32,880
majority so if we have three copies and

00:17:31,200 --> 00:17:34,560
we write a quorum we'll synchronously

00:17:32,880 --> 00:17:38,010
replicate the two that's what will

00:17:34,560 --> 00:17:40,020
constitute successful right if we also

00:17:38,010 --> 00:17:41,700
read a quorum then we're going to

00:17:40,020 --> 00:17:43,950
consider two copies we're going to need

00:17:41,700 --> 00:17:45,270
to retrieve two copies in order to

00:17:43,950 --> 00:17:47,280
consider that a valid

00:17:45,270 --> 00:17:49,680
and if we do that there's no way we

00:17:47,280 --> 00:17:50,940
won't overlap and get at least one of

00:17:49,680 --> 00:17:52,290
the one of the most recent rights

00:17:50,940 --> 00:17:55,170
assuming that there was any

00:17:52,290 --> 00:17:56,820
inconsistency to begin with and so long

00:17:55,170 --> 00:17:59,340
as the number of copies we synchronously

00:17:56,820 --> 00:18:01,380
replicate to and that we consider on a

00:17:59,340 --> 00:18:03,510
reed is more than the replica count will

00:18:01,380 --> 00:18:05,400
always have read or write consistency

00:18:03,510 --> 00:18:07,650
you'll always read the last most

00:18:05,400 --> 00:18:09,720
up-to-date value you wrote what's great

00:18:07,650 --> 00:18:11,670
about that is that both the read and the

00:18:09,720 --> 00:18:13,380
right in this scenario could survive a

00:18:11,670 --> 00:18:15,300
single node failure in the replica group

00:18:13,380 --> 00:18:17,970
and everything continues on as normal

00:18:15,300 --> 00:18:19,920
there's the possibility of inconsistency

00:18:17,970 --> 00:18:22,050
within the cluster but who cares you

00:18:19,920 --> 00:18:26,310
still reading what you wrote and those

00:18:22,050 --> 00:18:28,620
values will get fixed eventually so the

00:18:26,310 --> 00:18:30,120
properties of distribution are that it's

00:18:28,620 --> 00:18:32,370
symmetrical given the way that algorithm

00:18:30,120 --> 00:18:34,500
works there's no need for coordination

00:18:32,370 --> 00:18:37,850
all you need to know is all of the nodes

00:18:34,500 --> 00:18:39,750
in the cluster which is easy and

00:18:37,850 --> 00:18:42,480
placement can be done by anybody

00:18:39,750 --> 00:18:45,750
everyone follows the same rules that

00:18:42,480 --> 00:18:48,360
makes it very operationally simple to

00:18:45,750 --> 00:18:50,790
have all of the nodes to be identical it

00:18:48,360 --> 00:18:52,110
also means it's linearly scalable so you

00:18:50,790 --> 00:18:53,610
know you can literally double the size

00:18:52,110 --> 00:18:56,250
of the cluster you know go from five

00:18:53,610 --> 00:18:58,460
notes to ten and you'll get you know

00:18:56,250 --> 00:19:00,900
twice the throughput twice the capacity

00:18:58,460 --> 00:19:03,360
it's redundant because their stores we

00:19:00,900 --> 00:19:05,790
store multiple copies so if one fails

00:19:03,360 --> 00:19:07,800
one machine fails in a replica group we

00:19:05,790 --> 00:19:10,010
don't lose any data and it's highly

00:19:07,800 --> 00:19:13,200
available because we can game those

00:19:10,010 --> 00:19:16,380
multiple copies those replicas to trade

00:19:13,200 --> 00:19:18,360
away a little consistency in favor of

00:19:16,380 --> 00:19:22,410
availability and still get consistency

00:19:18,360 --> 00:19:26,130
at the end so on i think is a really

00:19:22,410 --> 00:19:28,050
really good fit for time series so what

00:19:26,130 --> 00:19:30,540
does a data model look like for time

00:19:28,050 --> 00:19:33,930
series time series data again given the

00:19:30,540 --> 00:19:36,570
requirement for group storage so let's

00:19:33,930 --> 00:19:37,710
start with the resource our resources i

00:19:36,570 --> 00:19:40,760
guess in this case is kind of an

00:19:37,710 --> 00:19:45,000
abstract concept the resource will be

00:19:40,760 --> 00:19:46,950
that instance that we associate our

00:19:45,000 --> 00:19:50,010
metrics with or our group of metrics so

00:19:46,950 --> 00:19:52,230
a resource here could be a host or an

00:19:50,010 --> 00:19:54,240
application or more importantly since

00:19:52,230 --> 00:19:56,880
it's a group it might be an ethernet

00:19:54,240 --> 00:19:57,890
interface on a host if the group is

00:19:56,880 --> 00:19:59,690
meant to be

00:19:57,890 --> 00:20:03,260
statistics for the ethernet interface or

00:19:59,690 --> 00:20:05,000
it could be a processor on that host if

00:20:03,260 --> 00:20:08,180
it's meant to be the group is meant to

00:20:05,000 --> 00:20:10,580
represent processor statistics so we're

00:20:08,180 --> 00:20:12,350
going to want well first let's look in

00:20:10,580 --> 00:20:14,450
this an abstract terms we're going to

00:20:12,350 --> 00:20:17,510
want a 12 minute relationship between

00:20:14,450 --> 00:20:18,620
this resource and the sample times we're

00:20:17,510 --> 00:20:20,750
also going to want a one-to-many

00:20:18,620 --> 00:20:22,400
relationship between the sample times

00:20:20,750 --> 00:20:24,860
and the metrics that we want to store at

00:20:22,400 --> 00:20:26,120
that time so if we were modeling this in

00:20:24,860 --> 00:20:27,380
a relational database this is probably

00:20:26,120 --> 00:20:30,980
something we would do with a couple of

00:20:27,380 --> 00:20:33,170
join tables joins are not possible in

00:20:30,980 --> 00:20:39,980
Cassandra but we do have this nifty

00:20:33,170 --> 00:20:42,290
support for wide rows in in cql so this

00:20:39,980 --> 00:20:43,670
would be the DD l for a perhaps an

00:20:42,290 --> 00:20:47,390
oversimplified version of what I'm

00:20:43,670 --> 00:20:49,670
talking about let's let TM and vb the

00:20:47,390 --> 00:20:51,650
the timestamp the metric name and the

00:20:49,670 --> 00:20:54,230
value respectively and of course the

00:20:51,650 --> 00:20:56,840
resource is that resource string all of

00:20:54,230 --> 00:20:59,510
the the magic in this happens in the

00:20:56,840 --> 00:21:01,820
primary key definition for what's with

00:20:59,510 --> 00:21:04,130
what's in the parentheses so resource

00:21:01,820 --> 00:21:06,170
first that means it's the partition key

00:21:04,130 --> 00:21:07,790
that's the one that determines placement

00:21:06,170 --> 00:21:11,270
within the cluster if you remember the

00:21:07,790 --> 00:21:12,680
diagram of the ring and then the the the

00:21:11,270 --> 00:21:14,600
next columns in order will be the

00:21:12,680 --> 00:21:16,250
timestamp column and the metric column

00:21:14,600 --> 00:21:17,570
and what this does is it causes a

00:21:16,250 --> 00:21:20,270
grouping of the columns in the

00:21:17,570 --> 00:21:21,770
underlying storage such that we can we

00:21:20,270 --> 00:21:23,780
can create this one too many mapping

00:21:21,770 --> 00:21:25,970
first between the resource and time

00:21:23,780 --> 00:21:28,490
stamps and then time stamps and metrics

00:21:25,970 --> 00:21:30,620
since the value doesn't doesn't appear

00:21:28,490 --> 00:21:33,260
in that will have exactly one value for

00:21:30,620 --> 00:21:38,780
every resource time stamp and metric

00:21:33,260 --> 00:21:41,450
combination so this is my attempt at

00:21:38,780 --> 00:21:42,590
visualizing this I don't know how well

00:21:41,450 --> 00:21:46,130
this is going to work first time I try

00:21:42,590 --> 00:21:49,670
to like this this is I will stress what

00:21:46,130 --> 00:21:51,350
I'm trying to convey is the what happens

00:21:49,670 --> 00:21:53,300
in the underlying storage right so this

00:21:51,350 --> 00:21:57,770
is not what Cassandra presents you but

00:21:53,300 --> 00:21:59,720
in the underlying storage everything

00:21:57,770 --> 00:22:02,150
that is identified by a primary key our

00:21:59,720 --> 00:22:04,310
resource here is going to is going to be

00:22:02,150 --> 00:22:06,590
essentially a collection of sorted

00:22:04,310 --> 00:22:08,600
columns and so that primary key

00:22:06,590 --> 00:22:10,820
definition what it does is it sets up

00:22:08,600 --> 00:22:13,370
some compounding of the column names

00:22:10,820 --> 00:22:15,919
in order to group these and since you

00:22:13,370 --> 00:22:18,019
know timestamp appears first all of

00:22:15,919 --> 00:22:19,700
these groupings will be prefixed by a

00:22:18,019 --> 00:22:23,539
timestamp and it means that all of them

00:22:19,700 --> 00:22:25,130
will be sorted first by timestamp so

00:22:23,539 --> 00:22:27,350
what that means is if we select from

00:22:25,130 --> 00:22:28,580
samples or resources some resource right

00:22:27,350 --> 00:22:29,960
at that point that automatically

00:22:28,580 --> 00:22:31,940
indicates that everything that follows

00:22:29,960 --> 00:22:36,529
all the predicates for the follow will

00:22:31,940 --> 00:22:38,419
be with from within this single row we

00:22:36,529 --> 00:22:39,860
can simply find the records in the

00:22:38,419 --> 00:22:42,380
inning as much as possible they'll be

00:22:39,860 --> 00:22:44,720
continuous on disk we can simply find

00:22:42,380 --> 00:22:46,940
those and construct a tabular results

00:22:44,720 --> 00:22:48,500
set just exactly what we would get if we

00:22:46,940 --> 00:22:50,990
were using join tables in relational

00:22:48,500 --> 00:22:52,490
database this is the results you get

00:22:50,990 --> 00:22:55,909
from Cassandra you would get a tabular

00:22:52,490 --> 00:22:58,389
result that contains the group of

00:22:55,909 --> 00:23:01,279
metrics and values for given time stamp

00:22:58,389 --> 00:23:03,320
it works the same way for a range of

00:23:01,279 --> 00:23:05,840
timestamps to some you know more

00:23:03,320 --> 00:23:08,090
interesting you know where timestamp is

00:23:05,840 --> 00:23:11,179
greater than or equal to t1 and less

00:23:08,090 --> 00:23:13,250
than or equal to t3 the important part

00:23:11,179 --> 00:23:15,980
here is to know that that this would

00:23:13,250 --> 00:23:18,200
result in you know in a range of columns

00:23:15,980 --> 00:23:20,240
that again in as much as possible are

00:23:18,200 --> 00:23:22,159
stored contiguously on disk and so as

00:23:20,240 --> 00:23:27,460
much as possible or result in a

00:23:22,159 --> 00:23:29,480
sequential read of the data okay so

00:23:27,460 --> 00:23:31,429
Cassandra makes a really good time

00:23:29,480 --> 00:23:32,750
series database you could pretty much

00:23:31,429 --> 00:23:34,850
pick it up and use it just like it is

00:23:32,750 --> 00:23:36,019
but as these things usually go there's

00:23:34,850 --> 00:23:37,909
there's also plenty of room for

00:23:36,019 --> 00:23:41,389
abstractions for this particular use

00:23:37,909 --> 00:23:43,340
case so what we've done is we've we have

00:23:41,389 --> 00:23:45,769
started a project called newts to

00:23:43,340 --> 00:23:47,929
implement the features that I talked

00:23:45,769 --> 00:23:51,409
about the late or disconnected

00:23:47,929 --> 00:23:54,139
aggregation and the grouping and since

00:23:51,409 --> 00:23:56,090
we think that that our use case for time

00:23:54,139 --> 00:23:57,649
series storage is there's nothing unique

00:23:56,090 --> 00:23:59,690
to us that it would be generally useful

00:23:57,649 --> 00:24:01,659
to others we've made this a separate

00:23:59,690 --> 00:24:03,919
project and it's it's a standalone

00:24:01,659 --> 00:24:08,149
datastore that you could use in your own

00:24:03,919 --> 00:24:10,580
projects we do raw sample storage and

00:24:08,149 --> 00:24:15,019
retrieval no processing is done on your

00:24:10,580 --> 00:24:17,210
on your on your samples nope no snow

00:24:15,019 --> 00:24:18,950
processing is automatically done and you

00:24:17,210 --> 00:24:22,730
can retrieve the samples exactly as you

00:24:18,950 --> 00:24:24,120
stored the man you can however perform a

00:24:22,730 --> 00:24:25,920
query that will result in

00:24:24,120 --> 00:24:28,950
gations and give you sort of graph ready

00:24:25,920 --> 00:24:31,350
results and those aggregations will

00:24:28,950 --> 00:24:33,240
include because again we're storing Raw

00:24:31,350 --> 00:24:35,940
results that includes like counter

00:24:33,240 --> 00:24:38,250
values are you know the actual value of

00:24:35,940 --> 00:24:39,900
the register or store draw so these

00:24:38,250 --> 00:24:43,260
aggregations can calculate rate from

00:24:39,900 --> 00:24:44,640
counters you can apply aggregate

00:24:43,260 --> 00:24:46,590
functions including ones that you write

00:24:44,640 --> 00:24:48,540
yourself and you can perform arbitrary

00:24:46,590 --> 00:24:50,780
calculations and even calculations on

00:24:48,540 --> 00:24:52,800
calculations to scale them or or

00:24:50,780 --> 00:24:55,260
aggregate them you know multiple

00:24:52,800 --> 00:24:57,120
aggregates into a single single value so

00:24:55,260 --> 00:24:59,550
it's pretty flexible and all of this

00:24:57,120 --> 00:25:02,940
runs a cassandra speed which is pretty

00:24:59,550 --> 00:25:05,480
rip and fast i was doing some tests

00:25:02,940 --> 00:25:08,610
right before I came to the conference on

00:25:05,480 --> 00:25:11,220
rackspace instances performance 2 15's

00:25:08,610 --> 00:25:14,130
which is 15 gigs of memory and for

00:25:11,220 --> 00:25:19,230
virtual CPUs and I pretty reliably got

00:25:14,130 --> 00:25:23,430
about just about 15,000 samples per

00:25:19,230 --> 00:25:25,860
second in jest rate per core so you know

00:25:23,430 --> 00:25:28,050
15,000 times 4 or 15,000 times 8

00:25:25,860 --> 00:25:30,300
depending however many however many

00:25:28,050 --> 00:25:35,250
cores you have and that's that's that's

00:25:30,300 --> 00:25:37,260
pretty pretty fast I think newts has a

00:25:35,250 --> 00:25:38,970
Java API so you could embed it directly

00:25:37,260 --> 00:25:42,420
in your project if it's Java there's

00:25:38,970 --> 00:25:46,740
also a rest endpoint it's a open source

00:25:42,420 --> 00:25:51,330
Apache License caffeine-free sustainably

00:25:46,740 --> 00:25:54,390
grown all those buzzwords this is

00:25:51,330 --> 00:25:55,860
buzzwords right and yeah it's upon

00:25:54,390 --> 00:25:59,700
github we would love to see

00:25:55,860 --> 00:26:03,120
contributions I would say it's in sort

00:25:59,700 --> 00:26:05,070
of a you know maybe late alpha early

00:26:03,120 --> 00:26:06,870
beta stage the software's actually works

00:26:05,070 --> 00:26:09,870
pretty good but in the grand tradition

00:26:06,870 --> 00:26:11,790
of open-source software there's

00:26:09,870 --> 00:26:18,120
definitely a few usability knits and

00:26:11,790 --> 00:26:19,230
absolutely no documentation so we

00:26:18,120 --> 00:26:21,180
haven't really gotten to the point where

00:26:19,230 --> 00:26:23,090
we consider first release yet so I can I

00:26:21,180 --> 00:26:25,230
can hand wave that away and say you know

00:26:23,090 --> 00:26:28,410
it'll all be a good at Italian you know

00:26:25,230 --> 00:26:29,760
when the time comes but certainly

00:26:28,410 --> 00:26:31,620
anybody with you know who's slightly

00:26:29,760 --> 00:26:33,840
initiated I'm sure could make could make

00:26:31,620 --> 00:26:36,720
good use of it and I will promise

00:26:33,840 --> 00:26:37,650
everybody here that i will make i will

00:26:36,720 --> 00:26:39,330
make a

00:26:37,650 --> 00:26:41,010
i will clear my plate if you want to use

00:26:39,330 --> 00:26:42,540
it and you have any issues or you like

00:26:41,010 --> 00:26:44,840
to check it out need explanations or

00:26:42,540 --> 00:26:46,710
something like that so check it out

00:26:44,840 --> 00:26:47,970
that's all I have and I guess we have

00:26:46,710 --> 00:26:49,260
five minutes left so if there's any

00:26:47,970 --> 00:26:53,340
questions I think we got plenty of time

00:26:49,260 --> 00:26:55,170
for it grab one down here yeah first of

00:26:53,340 --> 00:26:57,780
all what's the relation to tie us to be

00:26:55,170 --> 00:27:05,160
so how much would be awesome to summer

00:26:57,780 --> 00:27:09,000
base time series door yeah just the

00:27:05,160 --> 00:27:12,840
comparison to kyles to be okay from your

00:27:09,000 --> 00:27:14,790
point uh so grouping would be the big

00:27:12,840 --> 00:27:16,200
one Carlos doesn't do any grouping

00:27:14,790 --> 00:27:19,290
something in the case as I mentioned

00:27:16,200 --> 00:27:20,730
where we where we collect 212 metrics at

00:27:19,290 --> 00:27:23,130
a time which is very common that would

00:27:20,730 --> 00:27:25,650
be 2 to 12 separate query you can group

00:27:23,130 --> 00:27:27,570
on text I'm sorry Kyle's to be has text

00:27:25,650 --> 00:27:30,570
and you can group on them as far as I

00:27:27,570 --> 00:27:32,100
know you yeah it has tags so that you

00:27:30,570 --> 00:27:33,600
could switch you could identify our

00:27:32,100 --> 00:27:37,530
market group of groups but if they're

00:27:33,600 --> 00:27:39,600
still stored in separate rows right so

00:27:37,530 --> 00:27:41,700
so it's still still require to

00:27:39,600 --> 00:27:43,050
retrieving them and still require you

00:27:41,700 --> 00:27:48,840
know that many queries that many

00:27:43,050 --> 00:27:52,650
individual queries when I here at the

00:27:48,840 --> 00:27:54,930
back oh sorry well got a really similar

00:27:52,650 --> 00:27:59,820
question okay how does it compare to

00:27:54,930 --> 00:28:01,620
graphite carbon storage to graph I what

00:27:59,820 --> 00:28:06,990
storage the carbon storage yeah it's

00:28:01,620 --> 00:28:08,460
integrated with graphite I don't know

00:28:06,990 --> 00:28:09,900
that's that's that's that's not a

00:28:08,460 --> 00:28:14,760
distributed storage is it that was

00:28:09,900 --> 00:28:16,380
probably the big difference yeah I

00:28:14,760 --> 00:28:18,540
thought you were talking about whisper

00:28:16,380 --> 00:28:20,760
TV yeah I think that's I think that's

00:28:18,540 --> 00:28:22,470
like rd without the ordering constraints

00:28:20,760 --> 00:28:23,760
they can store out of our order data I

00:28:22,470 --> 00:28:28,730
think otherwise it's pretty similar to

00:28:23,760 --> 00:28:28,730
already thank you

00:28:32,659 --> 00:28:39,950
hi could you go back to the data molding

00:28:35,299 --> 00:28:42,499
slide with your cql place which which DQ

00:28:39,950 --> 00:28:45,830
all the ddl this one yes did you say

00:28:42,499 --> 00:28:48,259
that you're the resource here is there

00:28:45,830 --> 00:28:50,539
ok yeah the resource here is where we

00:28:48,259 --> 00:28:53,929
can trick instantly call the roki the

00:28:50,539 --> 00:28:57,590
partition key so does that mean that all

00:28:53,929 --> 00:29:01,159
of your events for a particular resource

00:28:57,590 --> 00:29:03,739
end up on the same row yes what happens

00:29:01,159 --> 00:29:05,749
I mean the row has has limits what

00:29:03,739 --> 00:29:08,479
happens when you spill over that's a

00:29:05,749 --> 00:29:11,479
good question so we assume most people

00:29:08,479 --> 00:29:12,649
will probably use TTL columns so that's

00:29:11,479 --> 00:29:15,229
that's one thing that they would expire

00:29:12,649 --> 00:29:16,849
this is not the case right now but we're

00:29:15,229 --> 00:29:18,019
planning to partition is probably one of

00:29:16,849 --> 00:29:20,809
the first things I'll do after leaving

00:29:18,019 --> 00:29:23,299
here is partition that that roki by some

00:29:20,809 --> 00:29:26,749
time element probably in a probably week

00:29:23,299 --> 00:29:29,389
or something so that in that case each

00:29:26,749 --> 00:29:31,039
row will not grow beyond whatever

00:29:29,389 --> 00:29:32,929
whatever you collect and sentenced or in

00:29:31,039 --> 00:29:35,479
a weeks period of time ok which is

00:29:32,929 --> 00:29:37,429
pretty reasonable I work at spotify

00:29:35,479 --> 00:29:39,529
we're fairly heavy cassandra users and

00:29:37,429 --> 00:29:42,440
we've had a couple of cases where you

00:29:39,529 --> 00:29:44,779
sort of store endlessly store things to

00:29:42,440 --> 00:29:47,179
one single row and even if you delete

00:29:44,779 --> 00:29:49,759
things they don't disappear in Cassandra

00:29:47,179 --> 00:29:51,919
because you end up with tombstones so so

00:29:49,759 --> 00:29:53,359
the effectively your performance for

00:29:51,919 --> 00:29:56,779
that particular row goes down further

00:29:53,359 --> 00:29:59,409
and further so you need you will need to

00:29:56,779 --> 00:30:01,970
partition like you yes like you stay a

00:29:59,409 --> 00:30:03,950
follow-up question does that mean that

00:30:01,970 --> 00:30:06,619
if you have very heavy traffic on one

00:30:03,950 --> 00:30:10,129
particular resource then you create do

00:30:06,619 --> 00:30:11,929
you create hotspots in there so maybe

00:30:10,129 --> 00:30:13,729
that's not a I don't I don't anticipate

00:30:11,929 --> 00:30:15,859
that that would actually be a problem I

00:30:13,729 --> 00:30:17,779
mean that you would have such high

00:30:15,859 --> 00:30:19,580
sample frequently or frequency or so

00:30:17,779 --> 00:30:22,039
many metrics I would probably indicate a

00:30:19,580 --> 00:30:24,559
modeling problem this this does kind of

00:30:22,039 --> 00:30:26,450
put the onus on the user to kind of you

00:30:24,559 --> 00:30:28,580
know to establish what those groups are

00:30:26,450 --> 00:30:29,869
you can always just store one value per

00:30:28,580 --> 00:30:32,479
which is the way a lot of time sort of

00:30:29,869 --> 00:30:33,889
time series databases do it where you

00:30:32,479 --> 00:30:35,989
know it's essentially a key and a value

00:30:33,889 --> 00:30:38,330
and the resource would be the key and

00:30:35,989 --> 00:30:40,039
you know you'd have one value so the

00:30:38,330 --> 00:30:42,019
grouping is kind of a feature and it's

00:30:40,039 --> 00:30:44,029
it's incumbent upon the user to decide

00:30:42,019 --> 00:30:45,560
what makes sense for a group so I think

00:30:44,029 --> 00:30:48,080
for that to happen you'd have to have a

00:30:45,560 --> 00:30:53,810
you know a group that didn't make sense

00:30:48,080 --> 00:31:04,340
a group that was too large it was one

00:30:53,810 --> 00:31:05,780
back here hi why is the consistency so

00:31:04,340 --> 00:31:08,210
important for you because basically

00:31:05,780 --> 00:31:10,340
you're aggregating data which means in

00:31:08,210 --> 00:31:12,620
my book that if you loose like a few

00:31:10,340 --> 00:31:17,720
metrics it's not going to usually affect

00:31:12,620 --> 00:31:19,940
your your aggregates fight yeah we

00:31:17,720 --> 00:31:25,280
probably don't have really really strong

00:31:19,940 --> 00:31:27,110
consistency guarantees that doesn't mean

00:31:25,280 --> 00:31:28,490
that we don't want to replicate and that

00:31:27,110 --> 00:31:30,380
we don't want you know we don't want to

00:31:28,490 --> 00:31:33,620
store you know to have a replica count

00:31:30,380 --> 00:31:36,890
greater than one particularly because

00:31:33,620 --> 00:31:38,750
again you know so i guess to answer your

00:31:36,890 --> 00:31:42,050
question we want availability more than

00:31:38,750 --> 00:31:44,510
anything and so having multiple replicas

00:31:42,050 --> 00:31:46,160
gives us redundancy and you know yeah we

00:31:44,510 --> 00:31:47,690
may use a consistency level of one that

00:31:46,160 --> 00:31:50,090
will actually be a choice that's

00:31:47,690 --> 00:31:51,740
actually choice through notes that you

00:31:50,090 --> 00:31:54,440
can choose your consistency for us i

00:31:51,740 --> 00:31:56,090
think that most use cases for this it

00:31:54,440 --> 00:31:58,160
would just be one which just means that

00:31:56,090 --> 00:32:00,980
it's even even more available the

00:31:58,160 --> 00:32:04,430
availability is even higher okay thanks

00:32:00,980 --> 00:32:07,190
a follow-up question so how does this

00:32:04,430 --> 00:32:09,140
compare to say a lock test elasticsearch

00:32:07,190 --> 00:32:11,300
set up so we're programming data in

00:32:09,140 --> 00:32:14,420
production I'm sorry how does it compare

00:32:11,300 --> 00:32:25,460
to what logstash inelastic search mom

00:32:14,420 --> 00:32:27,640
not sure okay anyone else it went down

00:32:25,460 --> 00:32:27,640
here

00:32:36,230 --> 00:32:42,410
hi so is the grouping of the metrics

00:32:38,510 --> 00:32:44,480
that you do only four different metrics

00:32:42,410 --> 00:32:47,330
on the same resource you don't you don't

00:32:44,480 --> 00:32:50,900
want any grouping for metrics on

00:32:47,330 --> 00:32:52,480
different resources no I think I would

00:32:50,900 --> 00:32:55,520
probably consider that like an indexing

00:32:52,480 --> 00:32:58,190
problem probably yeah this this is

00:32:55,520 --> 00:32:59,840
assuming that you want to you want to to

00:32:58,190 --> 00:33:01,130
query this data model assumes and newts

00:32:59,840 --> 00:33:03,650
assumes that you want to query a group

00:33:01,130 --> 00:33:06,860
of metrics by a resource not necessarily

00:33:03,650 --> 00:33:09,049
you know a group a group of metrics

00:33:06,860 --> 00:33:10,820
across the set of resources it would be

00:33:09,049 --> 00:33:12,770
a different problem i guess and one that

00:33:10,820 --> 00:33:15,230
this doesn't address a follow-up

00:33:12,770 --> 00:33:17,660
question so if you store a single wide

00:33:15,230 --> 00:33:19,340
row for a resource and if you have a

00:33:17,660 --> 00:33:22,820
large number of metrics for that one

00:33:19,340 --> 00:33:25,910
resource could that not cause some

00:33:22,820 --> 00:33:26,990
performance issues I think that's I

00:33:25,910 --> 00:33:29,240
think that's similar to the question of

00:33:26,990 --> 00:33:30,710
the back and the answer would be I think

00:33:29,240 --> 00:33:32,600
if you've grouped correctly you know

00:33:30,710 --> 00:33:34,340
you're probably you know that a good

00:33:32,600 --> 00:33:36,320
size group is probably anywhere from two

00:33:34,340 --> 00:33:38,929
metrics to you know maybe a dozen or

00:33:36,320 --> 00:33:41,090
maybe even two dozen I think that before

00:33:38,929 --> 00:33:43,460
you would actually push the the row

00:33:41,090 --> 00:33:44,540
constraint you know push to the width of

00:33:43,460 --> 00:33:47,270
the row to the point where you caused

00:33:44,540 --> 00:33:48,830
performance I think you're sampling

00:33:47,270 --> 00:33:51,559
sample frequency we would have to be

00:33:48,830 --> 00:33:53,540
very right not to do the math to be a to

00:33:51,559 --> 00:33:54,890
give you examples but i think the sample

00:33:53,540 --> 00:33:56,960
frequency would have to be very very

00:33:54,890 --> 00:33:58,520
high and the metric group very very

00:33:56,960 --> 00:34:00,470
large before that became an issue again

00:33:58,520 --> 00:34:05,120
we're going to partition the row keys so

00:34:00,470 --> 00:34:06,679
that at most you know I think week is

00:34:05,120 --> 00:34:08,359
probably what would it be partitioned on

00:34:06,679 --> 00:34:10,490
and so at most a week's worth of data

00:34:08,359 --> 00:34:13,399
would be in a given row so would you

00:34:10,490 --> 00:34:19,060
break up a resource with lots of metrics

00:34:13,399 --> 00:34:24,350
down into smaller resources yeah okay

00:34:19,060 --> 00:34:25,879
thanks so to achieve high throughput is

00:34:24,350 --> 00:34:27,230
there anything on the network layer that

00:34:25,879 --> 00:34:31,970
you're doing special like what does the

00:34:27,230 --> 00:34:35,510
transport look like up until well the

00:34:31,970 --> 00:34:37,220
four core tests that the four core test

00:34:35,510 --> 00:34:40,010
I was doing where where I was getting

00:34:37,220 --> 00:34:43,090
about 15,000 samples per second per core

00:34:40,010 --> 00:34:45,560
you know sixty thousand requests or so

00:34:43,090 --> 00:34:47,899
that was only generating about 60 Meg's

00:34:45,560 --> 00:34:49,070
60 mega bit of traffic so you know 100

00:34:47,899 --> 00:34:53,870
Meg link would do for

00:34:49,070 --> 00:34:55,640
that special yeah same same sort of

00:34:53,870 --> 00:35:09,430
capacity planning to apply to anything

00:34:55,640 --> 00:35:09,430
else I think anybody else okay thank you

00:35:13,619 --> 00:35:15,680

YouTube URL: https://www.youtube.com/watch?v=xVwo9lsrxfg


