Title: Berlin Buzzwords 2014: Adrien Grand - ElasticSearch - aggregations #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	No need anymore to present faceted search, which is used on most search engines in order to give insights about the hits that matched the query.

ElasticSearch 1.0 introduced aggregations, which have been designed to be the successor of facets. What do aggregations bring that wasn't possible with facets? Composability. Although this may not look like a huge improvement, this actually opens up many possibilities, that this presentation will discuss.

Read more:
https://2014.berlinbuzzwords.de/session/elasticsearch-aggregations

About Adrien Grand:
https://2014.berlinbuzzwords.de/user/298/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,460 --> 00:00:14,660
so against it stop oops

00:00:08,840 --> 00:00:17,509
time to stop okay so we are going to

00:00:14,660 --> 00:00:19,099
start so my name is adilyn gong

00:00:17,509 --> 00:00:20,960
I'm a software engineer at elasticsearch

00:00:19,099 --> 00:00:23,090
as well as a committee on the upper

00:00:20,960 --> 00:00:25,520
tributes in project and today we are

00:00:23,090 --> 00:00:28,090
going to talk together for about 20

00:00:25,520 --> 00:00:30,860
minutes about aggregation elasticsearch

00:00:28,090 --> 00:00:32,450
so aggregations are a new feature of

00:00:30,860 --> 00:00:36,440
elasticsearch which have been introduced

00:00:32,450 --> 00:00:39,350
in the release 1.0 and so that's well

00:00:36,440 --> 00:00:41,000
four months ago in January and today I'm

00:00:39,350 --> 00:00:44,510
going to talk about what aggregations

00:00:41,000 --> 00:00:46,100
are why we built them and also about how

00:00:44,510 --> 00:00:48,260
they work I think it's important to get

00:00:46,100 --> 00:00:49,760
some basic understanding of how they

00:00:48,260 --> 00:00:51,590
work because it will help you understand

00:00:49,760 --> 00:00:54,050
the trade-offs that we made that when

00:00:51,590 --> 00:00:56,090
designing allegations and gematria is

00:00:54,050 --> 00:00:57,590
going to be helpful for you in order to

00:00:56,090 --> 00:01:00,079
understand whether or not it's also

00:00:57,590 --> 00:01:02,570
right to the right tool to tackle the

00:01:00,079 --> 00:01:04,009
problem that you are having so I won't

00:01:02,570 --> 00:01:05,659
give any example about syntax and

00:01:04,009 --> 00:01:08,600
response format it's only going to be

00:01:05,659 --> 00:01:12,170
about what you can do in how they work

00:01:08,600 --> 00:01:13,760
so aggregations are all about analytics

00:01:12,170 --> 00:01:15,710
as they allow you to compute histograms

00:01:13,760 --> 00:01:18,530
distributions of value and you can know

00:01:15,710 --> 00:01:20,870
statistics and have some key features

00:01:18,530 --> 00:01:23,090
one key feature is that they can be run

00:01:20,870 --> 00:01:24,799
on any portion of your data which means

00:01:23,090 --> 00:01:30,470
that whatever you can match with a gray

00:01:24,799 --> 00:01:33,830
such as what that work can you hear me

00:01:30,470 --> 00:01:36,770
okay any query that Clinton just

00:01:33,830 --> 00:01:38,810
described can be used and you can use

00:01:36,770 --> 00:01:40,250
this core in order to select the set of

00:01:38,810 --> 00:01:42,979
document that you'd like to aggregate on

00:01:40,250 --> 00:01:45,020
and everything is going to be computed

00:01:42,979 --> 00:01:48,970
in near-real-time so what does

00:01:45,020 --> 00:01:48,970
near-real-time mean it means that

00:02:00,830 --> 00:02:07,220
one two it works out okay okay

00:02:56,390 --> 00:03:04,970
that work okay so so what's this refresh

00:03:03,230 --> 00:03:08,300
interval means that when you index data

00:03:04,970 --> 00:03:10,430
you might need to wait until this amount

00:03:08,300 --> 00:03:12,140
of time before data becomes visible for

00:03:10,430 --> 00:03:14,930
search and by default it would be one

00:03:12,140 --> 00:03:17,360
second for elasticsearch so that might

00:03:14,930 --> 00:03:19,340
sound like a very hard row back for last

00:03:17,360 --> 00:03:21,410
week's search especially if you compare

00:03:19,340 --> 00:03:22,820
it to more traditional databases that

00:03:21,410 --> 00:03:24,830
would ensure that everything that you

00:03:22,820 --> 00:03:27,050
put into the system becomes visible

00:03:24,830 --> 00:03:29,060
instantly but actually this allows for a

00:03:27,050 --> 00:03:31,430
lot of mutation that we are going to

00:03:29,060 --> 00:03:33,680
discuss briefly and finally aggregations

00:03:31,430 --> 00:03:36,200
can be composed this is something that

00:03:33,680 --> 00:03:37,850
means in something that can be described

00:03:36,200 --> 00:03:40,250
as a previous stage of aggregations that

00:03:37,850 --> 00:03:42,530
we call facet elasticsearch and this is

00:03:40,250 --> 00:03:45,260
the reason why we build applications

00:03:42,530 --> 00:03:47,840
which was to be able to compose

00:03:45,260 --> 00:03:49,489
aggregations together so what does this

00:03:47,840 --> 00:03:51,680
compressibility mean in the context of

00:03:49,489 --> 00:03:54,140
recreations there are two main kinds of

00:03:51,680 --> 00:03:55,850
aggregations with an elastic search the

00:03:54,140 --> 00:03:57,470
first kind can be described as bucket

00:03:55,850 --> 00:03:59,000
operations which are basically

00:03:57,470 --> 00:04:00,829
responsible for taking a set of

00:03:59,000 --> 00:04:04,700
documents and partitioning into

00:04:00,829 --> 00:04:05,989
different sets of document and on the

00:04:04,700 --> 00:04:08,209
other hand we also have matrix

00:04:05,989 --> 00:04:10,910
aggregation whose responsibly is to take

00:04:08,209 --> 00:04:12,920
a set of document to compute something

00:04:10,910 --> 00:04:14,959
on this type of document so this can be

00:04:12,920 --> 00:04:18,140
anything from basic statistics like

00:04:14,959 --> 00:04:20,150
minimum maximum average and some to

00:04:18,140 --> 00:04:22,010
something which are a bit more

00:04:20,150 --> 00:04:25,910
interesting like approximate percentiles

00:04:22,010 --> 00:04:28,220
and unique counts so when I said

00:04:25,910 --> 00:04:32,360
compressibility what does it mean every

00:04:28,220 --> 00:04:34,729
bucket aggregation can have one zero one

00:04:32,360 --> 00:04:36,620
or many subrogation that can be either

00:04:34,729 --> 00:04:39,050
bucket a creation or matrix aggregation

00:04:36,620 --> 00:04:43,190
and matrix aggregation can only be used

00:04:39,050 --> 00:04:45,440
as a leaf of an aggregation tree so numb

00:04:43,190 --> 00:04:47,030
this might still sound a bit abstract at

00:04:45,440 --> 00:04:49,010
that point at that point and I'm going

00:04:47,030 --> 00:04:50,419
to give a few example of things that you

00:04:49,010 --> 00:04:51,979
can do with a relation that you couldn't

00:04:50,419 --> 00:04:55,070
do we said before because they lacked

00:04:51,979 --> 00:04:58,160
this composability so first let's try to

00:04:55,070 --> 00:04:59,990
think about traffic analysis let's

00:04:58,160 --> 00:05:01,940
imagine that you are willing to build

00:04:59,990 --> 00:05:03,880
something that would look like Google

00:05:01,940 --> 00:05:05,840
Analytics and really would have

00:05:03,880 --> 00:05:08,150
documents about your traffic that look

00:05:05,840 --> 00:05:09,800
like this you which would have to field

00:05:08,150 --> 00:05:12,020
one about the

00:05:09,800 --> 00:05:13,940
IP of the requests that have been

00:05:12,020 --> 00:05:16,280
performed on your website and another

00:05:13,940 --> 00:05:18,650
one about the timestamp at which the

00:05:16,280 --> 00:05:20,509
request has been performed and if your

00:05:18,650 --> 00:05:23,449
documents look like this you can run the

00:05:20,509 --> 00:05:25,849
the following aggregation which puts a

00:05:23,449 --> 00:05:28,099
cutting in aggregation on the source IP

00:05:25,849 --> 00:05:29,870
field and the an Instagram aggregation

00:05:28,099 --> 00:05:31,490
and those attempts time field in order

00:05:29,870 --> 00:05:33,470
to build this kind of chart that

00:05:31,490 --> 00:05:37,550
displays a unique amount of visitors

00:05:33,470 --> 00:05:39,710
that you got on a per day basis okay so

00:05:37,550 --> 00:05:41,300
let's try another example because you

00:05:39,710 --> 00:05:43,069
don't you do not need to put a cardinal

00:05:41,300 --> 00:05:45,740
segregation and our histogram you get

00:05:43,069 --> 00:05:47,330
something else in that case we what we

00:05:45,740 --> 00:05:49,069
would like to do is some kind of

00:05:47,330 --> 00:05:50,960
performance analysis let's imagine that

00:05:49,069 --> 00:05:52,750
you are looking performance analysis of

00:05:50,960 --> 00:05:55,159
your application it will last Excel

00:05:52,750 --> 00:05:57,650
documents would typically look like this

00:05:55,159 --> 00:05:59,300
with a response time and a timestamp in

00:05:57,650 --> 00:06:01,039
that case if you put a person test

00:05:59,300 --> 00:06:03,530
application under Instagram aggregation

00:06:01,039 --> 00:06:05,659
and if so for example interval of the

00:06:03,530 --> 00:06:07,669
histogram aggregation is 3 hours you

00:06:05,659 --> 00:06:10,270
would be able to have the percentiles

00:06:07,669 --> 00:06:12,500
for example the median 19th and 99th

00:06:10,270 --> 00:06:16,300
percentile of the response half of your

00:06:12,500 --> 00:06:21,020
application for every bucket of 3 hours

00:06:16,300 --> 00:06:23,029
so we just saw two basic aggregations so

00:06:21,020 --> 00:06:24,770
let's try to see one that would be a bit

00:06:23,029 --> 00:06:26,960
more complex and you know that you

00:06:24,770 --> 00:06:29,090
understand do you do use case so this

00:06:26,960 --> 00:06:30,800
time we are dealing about e-commerce and

00:06:29,090 --> 00:06:33,169
for example you are either a price

00:06:30,800 --> 00:06:34,909
comparison website or marketplace and

00:06:33,169 --> 00:06:36,620
you are indexing all files from

00:06:34,909 --> 00:06:38,389
different sites and you would like to

00:06:36,620 --> 00:06:40,009
build aggregations on this document so

00:06:38,389 --> 00:06:42,139
your documents typically have one

00:06:40,009 --> 00:06:44,500
category one site where they come from

00:06:42,139 --> 00:06:47,090
one brand a designation and a price and

00:06:44,500 --> 00:06:49,520
so let's try to run this aggregation to

00:06:47,090 --> 00:06:52,520
see what it does so first we are going

00:06:49,520 --> 00:06:55,879
to partition on a data according to the

00:06:52,520 --> 00:06:57,710
category the offer belongs to in that

00:06:55,879 --> 00:07:01,400
case elasticsearch figured out that

00:06:57,710 --> 00:07:04,759
there are 23 dry season 19 shoes and 8

00:07:01,400 --> 00:07:06,740
skirts and then for each category we

00:07:04,759 --> 00:07:08,569
want to compute two things first we want

00:07:06,740 --> 00:07:11,180
to compute the number of unique sites

00:07:08,569 --> 00:07:13,490
that sell items in this category and for

00:07:11,180 --> 00:07:15,469
example we got nine for the dresses

00:07:13,490 --> 00:07:18,800
category three for the shoes category

00:07:15,469 --> 00:07:23,000
and five category but we also want to

00:07:18,800 --> 00:07:26,630
compute the unique set of brands

00:07:23,000 --> 00:07:28,430
who have items in this category and I

00:07:26,630 --> 00:07:30,530
only put it for the dresses category but

00:07:28,430 --> 00:07:32,630
you would have a very small output for

00:07:30,530 --> 00:07:34,820
Susan skirt and for example we found out

00:07:32,630 --> 00:07:38,210
that there are eight dresses of the

00:07:34,820 --> 00:07:39,740
brand is equal under this category in

00:07:38,210 --> 00:07:41,890
the end for each brand in each category

00:07:39,740 --> 00:07:45,920
will sound to compute the minimum price

00:07:41,890 --> 00:07:50,000
so you would also have it in the output

00:07:45,920 --> 00:07:51,830
of elasticsearch okay so now that you

00:07:50,000 --> 00:07:53,750
should understand a bit more what

00:07:51,830 --> 00:07:56,480
aggregations are about and what you can

00:07:53,750 --> 00:07:58,100
do I'm going to try to explain why it

00:07:56,480 --> 00:08:00,020
makes sense to implement them

00:07:58,100 --> 00:08:02,000
elasticsearch this might be a bit

00:08:00,020 --> 00:08:03,710
surprising if you come from more

00:08:02,000 --> 00:08:05,840
traditional databases where you would

00:08:03,710 --> 00:08:07,190
have expected this kind of feature to be

00:08:05,840 --> 00:08:08,900
implemented but actually these are very

00:08:07,190 --> 00:08:10,970
good reasons to implement it

00:08:08,900 --> 00:08:12,710
elasticsearch one of them is that it's

00:08:10,970 --> 00:08:15,290
very powerful if you combine it with

00:08:12,710 --> 00:08:18,020
search because everything is computed

00:08:15,290 --> 00:08:19,850
dynamically and on the fly which means

00:08:18,020 --> 00:08:20,930
that if you are user initially entered

00:08:19,850 --> 00:08:22,610
one query

00:08:20,930 --> 00:08:24,490
if you refine this curry you can

00:08:22,610 --> 00:08:26,920
recompute or discounts dynamically

00:08:24,490 --> 00:08:29,660
another reason is more on the

00:08:26,920 --> 00:08:32,360
implementation side and the reason is

00:08:29,660 --> 00:08:34,310
that search engine have had faceted

00:08:32,360 --> 00:08:36,110
search for a long time and one

00:08:34,310 --> 00:08:37,550
consequence of that is that search

00:08:36,110 --> 00:08:40,370
engines such as a loose in elastic

00:08:37,550 --> 00:08:42,800
search have a storage that is highly

00:08:40,370 --> 00:08:44,090
optimized for such cases I'm going to

00:08:42,800 --> 00:08:47,120
talk about it a bit more in the next

00:08:44,090 --> 00:08:48,620
slider and aggregation basically are not

00:08:47,120 --> 00:08:50,960
a revolution in the sense that we don't

00:08:48,620 --> 00:08:53,270
we didn't need to rebuild loosing in

00:08:50,960 --> 00:08:56,810
order to make it possible but instead we

00:08:53,270 --> 00:08:58,070
are building an existing work which has

00:08:56,810 --> 00:09:02,380
been done in loose in and in particular

00:08:58,070 --> 00:09:07,130
in order to support efficient compressed

00:09:02,380 --> 00:09:09,320
columnar storage so I agree if that are

00:09:07,130 --> 00:09:12,800
fast and I'm going to try to explain a

00:09:09,320 --> 00:09:14,240
bit why so one reason why original fast

00:09:12,800 --> 00:09:16,250
is that they are built on lucina

00:09:14,240 --> 00:09:18,290
and the trade-off of leucine is to make

00:09:16,250 --> 00:09:20,060
search as fast as possible so it's a

00:09:18,290 --> 00:09:22,280
very conscious trade-off and if you've

00:09:20,060 --> 00:09:24,380
seen that someday has the choice to make

00:09:22,280 --> 00:09:26,180
either indexing faster or search faster

00:09:24,380 --> 00:09:27,920
it's going to make search faster and

00:09:26,180 --> 00:09:29,300
then work on in the inkling side in

00:09:27,920 --> 00:09:31,790
order to improve algorithm and maybe

00:09:29,300 --> 00:09:34,460
that data structure to make it as fast

00:09:31,790 --> 00:09:36,889
as possible but the right the focus is

00:09:34,460 --> 00:09:38,899
really on search so this is

00:09:36,889 --> 00:09:40,160
true for the inverted index which is

00:09:38,899 --> 00:09:45,319
used in order to find a matching

00:09:40,160 --> 00:09:47,089
document but this is also true for some

00:09:45,319 --> 00:09:50,269
columnist right that losing has which is

00:09:47,089 --> 00:09:52,999
called dock values on which are

00:09:50,269 --> 00:09:54,379
efficiently compressed so if you come

00:09:52,999 --> 00:09:55,850
from loosing you would call it that

00:09:54,379 --> 00:09:57,709
values if you come from elasticsearch

00:09:55,850 --> 00:10:00,589
would collect field data but basically

00:09:57,709 --> 00:10:04,249
it is exactly the same and one important

00:10:00,589 --> 00:10:06,439
optimization a political master h is

00:10:04,249 --> 00:10:09,259
that we've seen an elastic search never

00:10:06,439 --> 00:10:11,689
never manipulate string base directly

00:10:09,259 --> 00:10:14,559
instead a Lucene index is actually made

00:10:11,689 --> 00:10:17,359
of a few typically around fifty

00:10:14,559 --> 00:10:19,730
immutable indices which are smaller and

00:10:17,359 --> 00:10:22,009
in each of these immutable indices

00:10:19,730 --> 00:10:24,379
strings are restored as enums

00:10:22,009 --> 00:10:26,809
so why is it important because if you

00:10:24,379 --> 00:10:28,970
saw the strings installer stole them

00:10:26,809 --> 00:10:30,889
somewhere in the file system you can

00:10:28,970 --> 00:10:32,509
refer to these strings according to the

00:10:30,889 --> 00:10:34,790
ordinal and for example if you want to

00:10:32,509 --> 00:10:36,679
compare string a again string b you

00:10:34,790 --> 00:10:38,269
don't need to compare the bite but you

00:10:36,679 --> 00:10:40,220
can directly compare the ordinals in

00:10:38,269 --> 00:10:41,629
order to understand if they are equal or

00:10:40,220 --> 00:10:44,059
which one is greater than the other one

00:10:41,629 --> 00:10:47,299
so this is a an optimization which is

00:10:44,059 --> 00:10:50,209
used everywhere in missing in one less

00:10:47,299 --> 00:10:53,209
return why is that fast is that no

00:10:50,209 --> 00:10:54,649
matter how many levels of aggregations

00:10:53,209 --> 00:10:58,339
you have everything is going to be

00:10:54,649 --> 00:11:00,619
computed in one single pass so how does

00:10:58,339 --> 00:11:03,139
it work so when you were on the crack

00:11:00,619 --> 00:11:04,610
with elastic search basically you have

00:11:03,139 --> 00:11:06,799
something which is called inverted index

00:11:04,610 --> 00:11:08,269
which comes from Mussina which when

00:11:06,799 --> 00:11:10,279
combined with the clay is going to be

00:11:08,269 --> 00:11:12,739
able to tell you what are the matching

00:11:10,279 --> 00:11:14,360
documents and typically these matching

00:11:12,739 --> 00:11:16,459
documents are going to be written to you

00:11:14,360 --> 00:11:20,869
as an information and you are going to

00:11:16,459 --> 00:11:23,049
be able to to have listeners on this

00:11:20,869 --> 00:11:25,249
iteration and they are called collectors

00:11:23,049 --> 00:11:27,529
typically by default you would only

00:11:25,249 --> 00:11:29,209
always have the top his collector which

00:11:27,529 --> 00:11:31,699
is used in order to collect the top hit

00:11:29,209 --> 00:11:34,309
but you can also plug in a collector for

00:11:31,699 --> 00:11:36,139
aggregations and the important part

00:11:34,309 --> 00:11:38,299
about this slide is that I wanted to

00:11:36,139 --> 00:11:40,609
explain that computing aggregations and

00:11:38,299 --> 00:11:42,160
computing hit top hit is actually going

00:11:40,609 --> 00:11:46,939
to happen at the same time in the same

00:11:42,160 --> 00:11:49,220
collection of your document matches okay

00:11:46,939 --> 00:11:50,810
now let's focus a bit more on the

00:11:49,220 --> 00:11:54,500
regions bottom

00:11:50,810 --> 00:11:58,760
and basically as I said no I didn't say

00:11:54,500 --> 00:12:00,200
it's right something which is useful to

00:11:58,760 --> 00:12:02,030
know is that everything is tossed

00:12:00,200 --> 00:12:03,740
sequentially in Massena so this for girl

00:12:02,030 --> 00:12:05,530
parties could be one segment and even

00:12:03,740 --> 00:12:08,690
about it index is going to give you

00:12:05,530 --> 00:12:10,600
ideas of document that matched and for

00:12:08,690 --> 00:12:13,100
this index which is stored sequentially

00:12:10,600 --> 00:12:15,230
we could have welcomed for example two

00:12:13,100 --> 00:12:16,730
columns for to filter so in that case we

00:12:15,230 --> 00:12:18,770
have two field which are a category in

00:12:16,730 --> 00:12:21,350
the price which store the category in

00:12:18,770 --> 00:12:23,270
the price of the document and in order

00:12:21,350 --> 00:12:24,800
to compute aggregation elasticsearch is

00:12:23,270 --> 00:12:26,840
going to take the bucket of the parent

00:12:24,800 --> 00:12:28,790
aggregation so it could be the root but

00:12:26,840 --> 00:12:30,500
it could also be the result of any other

00:12:28,790 --> 00:12:33,080
bucket or creation it would work exactly

00:12:30,500 --> 00:12:36,020
the same way and then it will iterate

00:12:33,080 --> 00:12:37,490
over all the matches so first we

00:12:36,020 --> 00:12:39,260
discover a new category for which the

00:12:37,490 --> 00:12:41,150
one there was no bucket so we need to

00:12:39,260 --> 00:12:43,220
create a new bucket with the document

00:12:41,150 --> 00:12:44,690
count of one and the price is going to

00:12:43,220 --> 00:12:47,210
be the price of the single offer that

00:12:44,690 --> 00:12:48,980
this bucket contains then the same

00:12:47,210 --> 00:12:50,840
happens with clothing which is a

00:12:48,980 --> 00:12:51,320
category that we have never never seen

00:12:50,840 --> 00:12:53,330
before

00:12:51,320 --> 00:12:54,860
and then we see shoes again so instead

00:12:53,330 --> 00:12:56,480
of creating a new bucket what we are

00:12:54,860 --> 00:12:59,270
going to do is that we are going to go

00:12:56,480 --> 00:13:01,700
to the existing Birkett increment the

00:12:59,270 --> 00:13:03,590
document count by one and update if

00:13:01,700 --> 00:13:05,780
necessary is the minimum price the

00:13:03,590 --> 00:13:07,720
minimum price is lower than the previous

00:13:05,780 --> 00:13:10,250
minimum price so we need to update it

00:13:07,720 --> 00:13:13,070
again a new category so we create a new

00:13:10,250 --> 00:13:15,410
bucket and for the last document in this

00:13:13,070 --> 00:13:17,840
set of matching document the price was

00:13:15,410 --> 00:13:20,150
higher than the previous buy price so we

00:13:17,840 --> 00:13:22,880
don't need to update the minimum price

00:13:20,150 --> 00:13:24,560
so this is really how it works

00:13:22,880 --> 00:13:26,840
at the shower level however

00:13:24,560 --> 00:13:28,700
elasticsearch is not a single shard

00:13:26,840 --> 00:13:30,680
single block so changing everything can

00:13:28,700 --> 00:13:32,780
run distributed which means that you

00:13:30,680 --> 00:13:34,910
need to be able to mount results of

00:13:32,780 --> 00:13:36,410
several aggregations together and the

00:13:34,910 --> 00:13:38,330
way it works that elastic search is

00:13:36,410 --> 00:13:40,550
going to take the tree of aggregations

00:13:38,330 --> 00:13:41,570
for every shot and to melt them together

00:13:40,550 --> 00:13:43,610
recursively

00:13:41,570 --> 00:13:46,820
by merging together buckets that have

00:13:43,610 --> 00:13:48,740
the same level so let's take back this

00:13:46,820 --> 00:13:50,420
example and try to match it with the

00:13:48,740 --> 00:13:52,810
results that we got on another shadow

00:13:50,420 --> 00:13:57,230
and basically we are going to mount

00:13:52,810 --> 00:13:59,120
recursively so first we're going to

00:13:57,230 --> 00:14:01,550
choose a category and we can see that

00:13:59,120 --> 00:14:02,540
it's present in two variations so we

00:14:01,550 --> 00:14:04,579
need to set up cots

00:14:02,540 --> 00:14:06,019
so the total count would be 5

00:14:04,579 --> 00:14:07,429
and then recursively we'll go to the

00:14:06,019 --> 00:14:10,009
child aberration which is a mean price

00:14:07,429 --> 00:14:12,319
and so we need to take the minimum of 50

00:14:10,009 --> 00:14:15,350
and 60 which would be 50 we do the same

00:14:12,319 --> 00:14:16,970
with Slovenia and it can also happen

00:14:15,350 --> 00:14:18,769
that some buckets are only present in

00:14:16,970 --> 00:14:20,329
one aggression in what in which case

00:14:18,769 --> 00:14:20,779
it's going to be very easy we just need

00:14:20,329 --> 00:14:23,269
to recopy

00:14:20,779 --> 00:14:25,189
it verbatim and we'll have our

00:14:23,269 --> 00:14:28,189
separation and the same is true for

00:14:25,189 --> 00:14:31,759
accessories and that's it

00:14:28,189 --> 00:14:37,579
so we just get merged result for the

00:14:31,759 --> 00:14:39,860
aggregations so on the sub slide I give

00:14:37,579 --> 00:14:41,899
you examples the aggregation that exists

00:14:39,860 --> 00:14:44,420
in elasticsearch in particular a bucket

00:14:41,899 --> 00:14:46,639
and metrics a creation myth actually you

00:14:44,420 --> 00:14:49,369
have a few aggregation that I did not

00:14:46,639 --> 00:14:52,009
mention so something you could be

00:14:49,369 --> 00:14:57,290
interested in is that elasticsearch has

00:14:52,009 --> 00:14:58,730
some basic support for document

00:14:57,290 --> 00:15:00,920
relations when it comes to aggregation

00:14:58,730 --> 00:15:03,230
in particular if you know about nested

00:15:00,920 --> 00:15:04,699
documents in a stick search you need to

00:15:03,230 --> 00:15:06,920
know that we also have nested and

00:15:04,699 --> 00:15:09,379
reversed nested agree aggregations in

00:15:06,920 --> 00:15:11,809
order to make it very easy to leverage

00:15:09,379 --> 00:15:12,889
these relations in aggregations we also

00:15:11,809 --> 00:15:15,649
have something which is called

00:15:12,889 --> 00:15:18,110
significant terms that maca would who

00:15:15,649 --> 00:15:21,259
created them usually said that it's

00:15:18,110 --> 00:15:21,649
helpful to find what is uncommonly come

00:15:21,259 --> 00:15:23,959
on

00:15:21,649 --> 00:15:27,049
in late data set this can be typically

00:15:23,959 --> 00:15:29,739
useful in order to detect fraud for

00:15:27,049 --> 00:15:34,220
example or to if you take database of

00:15:29,739 --> 00:15:36,889
comments about products sorry a database

00:15:34,220 --> 00:15:38,959
about of comments about products to get

00:15:36,889 --> 00:15:41,660
what are the most important terms about

00:15:38,959 --> 00:15:43,369
a particular product a for example for a

00:15:41,660 --> 00:15:45,949
particular cow it could be that it's

00:15:43,369 --> 00:15:47,600
done rules or it very nice draw so this

00:15:45,949 --> 00:15:50,299
is a kind of information that you could

00:15:47,600 --> 00:15:52,819
extract there is also a new aggregation

00:15:50,299 --> 00:15:53,929
which is going to come investigate 1.3

00:15:52,819 --> 00:15:55,939
thanks to Martina

00:15:53,929 --> 00:15:57,999
with RIA which is the top piece

00:15:55,939 --> 00:16:01,069
aggregation which is going to help you

00:15:57,999 --> 00:16:03,350
compute the top each or every bucket so

00:16:01,069 --> 00:16:05,209
if we go back to our example about

00:16:03,350 --> 00:16:07,429
e-commerce this means that for example

00:16:05,209 --> 00:16:10,279
for every category you could return the

00:16:07,429 --> 00:16:12,559
top it to the user so let's imagine that

00:16:10,279 --> 00:16:14,720
your user enter the query you would be

00:16:12,559 --> 00:16:17,689
able to return the top hits for the

00:16:14,720 --> 00:16:18,290
shoes category for the scales categories

00:16:17,689 --> 00:16:20,749
etcetera

00:16:18,290 --> 00:16:23,149
and one last thing that I wanted to

00:16:20,749 --> 00:16:25,549
mention is that performance and memory

00:16:23,149 --> 00:16:26,959
usage of aggregations improve a lot in

00:16:25,549 --> 00:16:30,350
general and in particular in the last

00:16:26,959 --> 00:16:32,509
release which was done last week which

00:16:30,350 --> 00:16:34,009
was elasticsearch 1.2 so if you are

00:16:32,509 --> 00:16:36,049
already using a relation I highly

00:16:34,009 --> 00:16:38,029
recommend to to upgrade and in

00:16:36,049 --> 00:16:40,850
particular if you are using discs

00:16:38,029 --> 00:16:44,629
biggest field data performance will be

00:16:40,850 --> 00:16:47,449
much better now so that's everything I

00:16:44,629 --> 00:16:49,339
wanted to say so thank you for your

00:16:47,449 --> 00:16:51,739
attention and if you have questions I

00:16:49,339 --> 00:16:54,339
think we still have a few minutes to try

00:16:51,739 --> 00:16:54,339
to answer them

00:17:02,879 --> 00:17:10,449
yeah hello yeah the aggregation starts

00:17:07,720 --> 00:17:13,470
really very cool very nice you mentioned

00:17:10,449 --> 00:17:16,449
about aggregating across segments

00:17:13,470 --> 00:17:20,549
individual segments or across different

00:17:16,449 --> 00:17:22,959
I guess in indices how about things like

00:17:20,549 --> 00:17:24,579
cardinality you know where you're going

00:17:22,959 --> 00:17:26,679
to get some sort of error right because

00:17:24,579 --> 00:17:27,880
you don't know exactly what what what

00:17:26,679 --> 00:17:29,830
the cardinality things when you merge

00:17:27,880 --> 00:17:32,289
and how do you cope with that

00:17:29,830 --> 00:17:34,120
so actually we're not using a crate

00:17:32,289 --> 00:17:36,340
algorithm to do that so for both the

00:17:34,120 --> 00:17:38,559
cardinality and the percentiles

00:17:36,340 --> 00:17:40,990
aggregations were using approximate

00:17:38,559 --> 00:17:42,940
algorithm and so if you know about hyper

00:17:40,990 --> 00:17:45,039
loglog papers this is the name of the

00:17:42,940 --> 00:17:47,049
algorithm that we are using in order to

00:17:45,039 --> 00:17:49,480
compute carnality and if you know about

00:17:47,049 --> 00:17:51,549
tea digest deadening is here is the

00:17:49,480 --> 00:17:53,529
father of this algorithm this is an

00:17:51,549 --> 00:17:57,220
algorithm that we are using for the

00:17:53,529 --> 00:18:00,700
percentile segregation so this algorithm

00:17:57,220 --> 00:18:03,100
have been designed to be to not be fully

00:18:00,700 --> 00:18:05,380
accurate but to be able to work in a

00:18:03,100 --> 00:18:07,330
distributed environment when you where

00:18:05,380 --> 00:18:09,809
you cannot know about all the data at

00:18:07,330 --> 00:18:09,809
the same time

00:18:20,410 --> 00:18:26,120
you could keep all of the items unique

00:18:24,380 --> 00:18:28,760
items if you really wanted to get an

00:18:26,120 --> 00:18:30,890
exact unique count it just would be

00:18:28,760 --> 00:18:32,810
tremendously expensive right yes and

00:18:30,890 --> 00:18:48,380
that's something we do rule to to allow

00:18:32,810 --> 00:18:51,770
users to do it that make sense hello all

00:18:48,380 --> 00:18:55,580
the plans to do efficient pagination on

00:18:51,770 --> 00:18:58,070
aggregations so sorry congratu nation

00:18:55,580 --> 00:19:02,980
okay Suzuka she is about pagination and

00:18:58,070 --> 00:19:05,720
aggregation on aggregations okay so I

00:19:02,980 --> 00:19:08,210
think this is something that we are not

00:19:05,720 --> 00:19:10,190
going to do okay because it's

00:19:08,210 --> 00:19:12,770
complicated if you want to do pagination

00:19:10,190 --> 00:19:14,150
it typically typically means that for

00:19:12,770 --> 00:19:16,340
example let's take the example of a term

00:19:14,150 --> 00:19:18,020
segregation it typically means that you

00:19:16,340 --> 00:19:19,340
have a lots and lots of terms that you

00:19:18,020 --> 00:19:23,090
would like to be able to paginate honor

00:19:19,340 --> 00:19:26,030
and the thing is that it's not an easy

00:19:23,090 --> 00:19:28,250
issue not an easy problem to solve to

00:19:26,030 --> 00:19:29,330
have a crate counts of a large Canyon

00:19:28,250 --> 00:19:31,670
cut energy field

00:19:29,330 --> 00:19:33,980
if data current comes from several

00:19:31,670 --> 00:19:35,930
shards and you don't want to forward the

00:19:33,980 --> 00:19:39,020
whole set of values to the note that

00:19:35,930 --> 00:19:41,750
coordinates the search so if we find a

00:19:39,020 --> 00:19:45,980
way to make it cheap shall we will do it

00:19:41,750 --> 00:19:50,030
but so far we I mean we we not know I

00:19:45,980 --> 00:19:52,550
don't think we can solve it but if

00:19:50,030 --> 00:20:02,550
someone has an idea how to solve it I

00:19:52,550 --> 00:20:09,900
deserve welcome three question here

00:20:02,550 --> 00:20:12,570
I what would be the influence on the

00:20:09,900 --> 00:20:15,960
memory consumption if we are going to

00:20:12,570 --> 00:20:17,550
use aggregation sorry what would be

00:20:15,960 --> 00:20:20,970
zillions the influence on the memory

00:20:17,550 --> 00:20:24,900
consumption so in in addition to the

00:20:20,970 --> 00:20:26,070
won't we have anyways so sorry I didn't

00:20:24,900 --> 00:20:28,350
understand everything so it's about

00:20:26,070 --> 00:20:32,100
memory consumption yeah and with regard

00:20:28,350 --> 00:20:33,570
to what right cost of memory

00:20:32,100 --> 00:20:35,430
okay what's the cost of memory so it

00:20:33,570 --> 00:20:38,280
depends on the aggregation that you

00:20:35,430 --> 00:20:40,260
you'd like to right now and so typically

00:20:38,280 --> 00:20:41,700
if you are running a term separation the

00:20:40,260 --> 00:20:44,040
memory usage is going to depend on the

00:20:41,700 --> 00:20:48,440
unique number of terms that your fill

00:20:44,040 --> 00:20:52,020
has so under each other so on every shot

00:20:48,440 --> 00:20:53,430
so I give this example typically and I

00:20:52,020 --> 00:20:56,310
never shall the elasticsearch is going

00:20:53,430 --> 00:20:58,230
to build one bucket for every existing

00:20:56,310 --> 00:20:59,820
term in your shop so the more and

00:20:58,230 --> 00:21:00,210
everything is going to be loaded into

00:20:59,820 --> 00:21:01,940
memory

00:21:00,210 --> 00:21:05,520
hopefully it's highly compressed and

00:21:01,940 --> 00:21:07,380
something so as I said earlier loosie

00:21:05,520 --> 00:21:08,760
loosie an elastic search almost never

00:21:07,380 --> 00:21:11,340
used biased in order to actually

00:21:08,760 --> 00:21:13,050
represent terms the user ordinarily

00:21:11,340 --> 00:21:17,130
instead and what would happen in that

00:21:13,050 --> 00:21:18,720
case is that we would first not choose

00:21:17,130 --> 00:21:21,240
the string representation of the

00:21:18,720 --> 00:21:23,610
category here but it's odd nor okay and

00:21:21,240 --> 00:21:25,740
we would then we would prove in order to

00:21:23,610 --> 00:21:28,140
find the top categories and only after

00:21:25,740 --> 00:21:30,090
that and after that would replace the

00:21:28,140 --> 00:21:32,160
ordinal with the actual bytes of the

00:21:30,090 --> 00:21:34,560
category so it's optimized but obviously

00:21:32,160 --> 00:21:35,790
you have a memory usage which is linear

00:21:34,560 --> 00:21:39,060
with the number of buckets that you have

00:21:35,790 --> 00:21:41,340
so this is an example for the term

00:21:39,060 --> 00:21:45,210
segregation and we also have variations

00:21:41,340 --> 00:21:47,540
where you can trade some memory for

00:21:45,210 --> 00:21:50,910
crazy so this is typically the case for

00:21:47,540 --> 00:21:55,730
percentiles and cardinality well you you

00:21:50,910 --> 00:21:57,930
have so you have a way to say please use

00:21:55,730 --> 00:22:00,420
more memory in order to improve accuracy

00:21:57,930 --> 00:22:03,570
or less memory because I want it to be

00:22:00,420 --> 00:22:06,830
as light as possible so it depends on

00:22:03,570 --> 00:22:06,830
the aggregation we are talking about

00:22:21,070 --> 00:22:27,199
yeah so basically it's just a running

00:22:23,419 --> 00:22:39,109
function now okay so the question is

00:22:27,199 --> 00:22:41,179
about how do Instagram work okay so can

00:22:39,109 --> 00:22:44,479
I use my own back mapping function in

00:22:41,179 --> 00:22:46,669
lot to map time stamp to make it so we

00:22:44,479 --> 00:22:48,259
are to in the context of the histogram

00:22:46,669 --> 00:22:48,529
aggregation which is going to build back

00:22:48,259 --> 00:22:52,039
it

00:22:48,529 --> 00:22:54,379
so by default there are there is a high

00:22:52,039 --> 00:22:56,119
number of intervals that are predefined

00:22:54,379 --> 00:22:57,289
that which are typically minute or you

00:22:56,119 --> 00:23:01,069
can say 3 huh

00:22:57,289 --> 00:23:02,659
I've - 5-month cetera but if you want to

00:23:01,069 --> 00:23:04,459
do something which is more complicated

00:23:02,659 --> 00:23:06,889
and in particular which would not be a

00:23:04,459 --> 00:23:08,329
fixed side to advocate what you would

00:23:06,889 --> 00:23:11,089
need to do would be to use a term

00:23:08,329 --> 00:23:14,149
segregation and to use a script in order

00:23:11,089 --> 00:23:15,859
to define your bracket so typically you

00:23:14,149 --> 00:23:17,479
could say for that here I want because

00:23:15,859 --> 00:23:19,519
of one months and for that job on

00:23:17,479 --> 00:23:22,069
buckets of one week this is something

00:23:19,519 --> 00:23:25,959
you could do with the term saturation so

00:23:22,069 --> 00:23:25,959
the reason why I'm saying that is that

00:23:26,649 --> 00:23:30,679
instagrams histogram migration is

00:23:28,940 --> 00:23:32,839
actually a specialized term segregation

00:23:30,679 --> 00:23:34,819
which is going to merge the robecca's

00:23:32,839 --> 00:23:36,169
together okay and this is something that

00:23:34,819 --> 00:23:38,389
you can do with a script in a stick

00:23:36,169 --> 00:23:40,219
search the reason why we have histogram

00:23:38,389 --> 00:23:42,979
aggression is that is typically used for

00:23:40,219 --> 00:23:44,989
days and we need the special handling

00:23:42,979 --> 00:23:47,869
for that in particular we want to allow

00:23:44,989 --> 00:23:50,239
you to use that math okay but you could

00:23:47,869 --> 00:23:52,219
definitely use term saturation with a

00:23:50,239 --> 00:23:54,529
script in order to generate the levels

00:23:52,219 --> 00:23:58,909
of your bucket and you could do pretty

00:23:54,529 --> 00:24:00,859
much everything that you want Adrian

00:23:58,909 --> 00:24:04,190
thank you very much for your talk as

00:24:00,859 --> 00:24:06,049
always we are running out of time now

00:24:04,190 --> 00:24:10,659
there are large break so have a good

00:24:06,049 --> 00:24:10,659
meal and thanks

00:24:13,529 --> 00:24:16,679

YouTube URL: https://www.youtube.com/watch?v=kKqIXvoQpbY


