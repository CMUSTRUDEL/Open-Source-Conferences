Title: Berlin Buzzwords 2014: Mikio Braun - Real-time personalization and recommendation with stream mining
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Recommendation and personalization system usually use elaborate store and batch algorithms to periodically crunch user event data like views, ratings, or purchases to compute predictions. A downside of this approach is that recommendations do not reflect the current user behavior, leading to missed opportunities in making good recommendations, or out-dated recommendations, for example when the purchase has already been made. 

We discuss novel systems based on stream mining algorithms which accumulate statistics on user behavior in real-time in a streaming fashion, this way always reflecting the most recent user behavior. Comparing profiles accross different time-scales, one is also able to classify recent behavior which deviates from the long-term trend and might be particularly interesting. Such algorithms have applications in ad targeting, recommendation, retail, monitoring, some of which will be discussed in more detail.

Read more:
https://2014.berlinbuzzwords.de/session/real-time-personalization-and-recommendation-stream-mining

About Mikio Braun:
https://2014.berlinbuzzwords.de/user/320/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,919 --> 00:00:12,990
hello everyone thanks for showing up to

00:00:10,010 --> 00:00:14,930
my talk my name is Michael Brown

00:00:12,990 --> 00:00:17,540
the I'm a so-called

00:00:14,930 --> 00:00:19,760
scientist and today I want to talk a bit

00:00:17,540 --> 00:00:22,400
about new approaches to achieving real

00:00:19,760 --> 00:00:28,460
time which don't just rely on scaling

00:00:22,400 --> 00:00:31,580
okay so reacting to user behavior so

00:00:28,460 --> 00:00:33,950
there's a lot of data which is generated

00:00:31,580 --> 00:00:36,680
these days and which which you also use

00:00:33,950 --> 00:00:38,059
a big data for to analyze and that's

00:00:36,680 --> 00:00:40,400
actually use a data right so there's a

00:00:38,059 --> 00:00:43,730
data user here he goes to website and

00:00:40,400 --> 00:00:45,290
then everything he does there is written

00:00:43,730 --> 00:00:47,290
into some logs so this might be

00:00:45,290 --> 00:00:50,899
pageviews when he clicks on something

00:00:47,290 --> 00:00:53,720
when you put something into is into a

00:00:50,899 --> 00:00:55,190
card or something and based on this

00:00:53,720 --> 00:00:57,260
right so you want to analyze it to

00:00:55,190 --> 00:00:59,030
crunch the data and for example you get

00:00:57,260 --> 00:01:02,390
out okay right now this user's

00:00:59,030 --> 00:01:04,729
interested in cars okay so I could you

00:01:02,390 --> 00:01:07,820
know show him an ad for some car maker

00:01:04,729 --> 00:01:10,759
or I could do no recommend him a nice

00:01:07,820 --> 00:01:12,440
new car or I can show articles based on

00:01:10,759 --> 00:01:14,840
these cars and then I can feed all these

00:01:12,440 --> 00:01:17,000
things back to the user so that the

00:01:14,840 --> 00:01:18,590
website is adapting to him and showing

00:01:17,000 --> 00:01:21,580
him the stuff which is interesting for

00:01:18,590 --> 00:01:24,880
him right so it's it's a form of a

00:01:21,580 --> 00:01:27,080
conversation with the user and

00:01:24,880 --> 00:01:29,600
apparently this has to be in real time

00:01:27,080 --> 00:01:31,010
right so if I if I look for cars now and

00:01:29,600 --> 00:01:32,689
then the system like after an hour

00:01:31,010 --> 00:01:35,030
decides over there somebody who's just

00:01:32,689 --> 00:01:36,439
interested in cars then maybe I already

00:01:35,030 --> 00:01:42,530
stopped using the website so this is

00:01:36,439 --> 00:01:44,119
somehow yeah okay and then the year out

00:01:42,530 --> 00:01:46,220
up there there's also like these are

00:01:44,119 --> 00:01:47,810
like items on your website so this is

00:01:46,220 --> 00:01:50,020
this also goes into the database so

00:01:47,810 --> 00:01:53,960
there's a whole lot of data in there

00:01:50,020 --> 00:01:55,880
okay so far so good now if you if you

00:01:53,960 --> 00:01:58,400
would do this nowadays right then

00:01:55,880 --> 00:02:00,229
usually would do it may be like 10 years

00:01:58,400 --> 00:02:03,079
ago or five years ago you would do it

00:02:00,229 --> 00:02:05,360
like this you collect all your data into

00:02:03,079 --> 00:02:07,100
a big sequel database and then you

00:02:05,360 --> 00:02:09,950
analyze it and analyzing usually means

00:02:07,100 --> 00:02:11,989
you do some you know some counting over

00:02:09,950 --> 00:02:14,120
certain time frames like you take the

00:02:11,989 --> 00:02:16,790
last month of data and then for each

00:02:14,120 --> 00:02:18,260
user you count how often he has looked

00:02:16,790 --> 00:02:20,360
at certain categories and stuff like

00:02:18,260 --> 00:02:22,310
that and then the result again ends up

00:02:20,360 --> 00:02:24,410
in a data database and then if the users

00:02:22,310 --> 00:02:26,570
there you can actually show them the

00:02:24,410 --> 00:02:28,520
stuff you think he's interested in but

00:02:26,570 --> 00:02:30,320
the problem here of causes that this

00:02:28,520 --> 00:02:35,150
it's a lot long time right so if you

00:02:30,320 --> 00:02:37,580
have several I know 100,000 users per

00:02:35,150 --> 00:02:41,390
day then alone calculating this thing

00:02:37,580 --> 00:02:43,700
over whole week will take more time also

00:02:41,390 --> 00:02:45,710
it won't work that well so as new data

00:02:43,700 --> 00:02:47,930
comes in the database gets even slower

00:02:45,710 --> 00:02:50,780
so this is yeah something we apparently

00:02:47,930 --> 00:02:52,490
no doesn't work very well okay so maybe

00:02:50,780 --> 00:02:54,590
like three years ago you actually do it

00:02:52,490 --> 00:02:56,240
like this you don't put it into a sequel

00:02:54,590 --> 00:02:58,550
database but instead you just write the

00:02:56,240 --> 00:03:01,730
locks on your Hadoop cluster and then

00:02:58,550 --> 00:03:03,380
you can run a MapReduce job to analyze

00:03:01,730 --> 00:03:04,880
the data and then stall the results

00:03:03,380 --> 00:03:08,060
again in some database and you can get

00:03:04,880 --> 00:03:09,940
it out this is a like it's an

00:03:08,060 --> 00:03:13,760
improvement so at least it's scalable

00:03:09,940 --> 00:03:16,460
but again these jobs take a long time to

00:03:13,760 --> 00:03:18,740
run so probably longer than the user is

00:03:16,460 --> 00:03:20,270
actually interacting with the website so

00:03:18,740 --> 00:03:22,790
then the next thing so if you do it like

00:03:20,270 --> 00:03:24,890
this year maybe you have something you

00:03:22,790 --> 00:03:27,320
switch from this batch oriented

00:03:24,890 --> 00:03:28,820
processing to a stream oriented

00:03:27,320 --> 00:03:30,590
processing where you process all the

00:03:28,820 --> 00:03:32,630
events is they come in so you probably

00:03:30,590 --> 00:03:34,730
use something like a patchy Kafka which

00:03:32,630 --> 00:03:39,140
is basically a piece of infrastructure

00:03:34,730 --> 00:03:40,850
which lets you collect lots of data in a

00:03:39,140 --> 00:03:43,100
reliable fashion and then you would use

00:03:40,850 --> 00:03:46,010
something like storm for example so this

00:03:43,100 --> 00:03:48,970
is a string processing framework which

00:03:46,010 --> 00:03:51,770
you probably all know where you can

00:03:48,970 --> 00:03:53,870
process the events as they come in and

00:03:51,770 --> 00:03:55,820
you can also scale that out and then you

00:03:53,870 --> 00:03:57,440
would probably not use HBase but some

00:03:55,820 --> 00:03:59,600
memory based database like red is

00:03:57,440 --> 00:04:01,730
because it has to be fast right so here

00:03:59,600 --> 00:04:04,010
at this point now you're in a position

00:04:01,730 --> 00:04:05,630
like if you have a lot of money you can

00:04:04,010 --> 00:04:08,840
actually get to a point where you can

00:04:05,630 --> 00:04:11,840
like see what the user is doing and and

00:04:08,840 --> 00:04:14,420
react to it in real time okay so far so

00:04:11,840 --> 00:04:18,580
good but the problem is so there are

00:04:14,420 --> 00:04:21,109
some problems with this so one is that

00:04:18,580 --> 00:04:24,880
actually if there's a lot of data then

00:04:21,109 --> 00:04:27,200
this needs to be quite big because

00:04:24,880 --> 00:04:29,090
because like it's processing unit and

00:04:27,200 --> 00:04:30,650
because of all the distribution overhead

00:04:29,090 --> 00:04:33,080
is actually not that fast so each of

00:04:30,650 --> 00:04:34,760
your nodes can process maybe know a few

00:04:33,080 --> 00:04:38,720
hundred a few thousand events per second

00:04:34,760 --> 00:04:40,640
something like this this so basically it

00:04:38,720 --> 00:04:42,330
all comes down to this right so one does

00:04:40,640 --> 00:04:44,729
not simply scale into real time I

00:04:42,330 --> 00:04:46,080
you can if you have a lot of money but

00:04:44,729 --> 00:04:51,870
still the question is whether you should

00:04:46,080 --> 00:04:54,060
or not and so in a way like so far all

00:04:51,870 --> 00:04:56,310
of this big data has been about scaling

00:04:54,060 --> 00:04:58,439
so it has been like thinking about how

00:04:56,310 --> 00:05:01,129
we can break down the stuff we do with

00:04:58,439 --> 00:05:04,229
our data into pieces of infrastructure

00:05:01,129 --> 00:05:05,610
which each individually can scale out

00:05:04,229 --> 00:05:08,189
and then you can put them together like

00:05:05,610 --> 00:05:10,259
this right so each of this is a piece of

00:05:08,189 --> 00:05:12,060
infrastructure this is for storage this

00:05:10,259 --> 00:05:15,690
is for computing another storage layer

00:05:12,060 --> 00:05:18,270
and we've sort of like build all this

00:05:15,690 --> 00:05:20,580
tool box of stuff and if you put it

00:05:18,270 --> 00:05:23,610
together we can build something which

00:05:20,580 --> 00:05:26,219
skates but maybe isn't so fast in the in

00:05:23,610 --> 00:05:29,729
the long run right because yeah okay I

00:05:26,219 --> 00:05:32,879
come to the because later on okay so in

00:05:29,729 --> 00:05:34,680
a way like usually when it is usually

00:05:32,879 --> 00:05:36,180
works and technology is like you're in

00:05:34,680 --> 00:05:38,219
some field there is a certain approach

00:05:36,180 --> 00:05:40,169
and you optimize this approach and

00:05:38,219 --> 00:05:43,189
optimize and optimize it until you come

00:05:40,169 --> 00:05:46,139
to a point where you sort us have have

00:05:43,189 --> 00:05:48,029
reached like the optimum which you can

00:05:46,139 --> 00:05:51,360
achieve with a certain kind of approach

00:05:48,029 --> 00:05:52,680
and you basically at a at a dead end so

00:05:51,360 --> 00:05:54,629
I'm not saying big data is at the data

00:05:52,680 --> 00:05:56,819
android but this specific approach can

00:05:54,629 --> 00:05:58,740
only get use it that far and then what's

00:05:56,819 --> 00:06:00,659
needed you know in order to go somewhere

00:05:58,740 --> 00:06:02,460
else actually you have to jump outside

00:06:00,659 --> 00:06:06,120
of this box or it's not really a box

00:06:02,460 --> 00:06:08,550
with a blob you have to jump outside you

00:06:06,120 --> 00:06:10,229
know by looking back at your problem

00:06:08,550 --> 00:06:11,669
problem and realizing that there are

00:06:10,229 --> 00:06:13,740
some things which you've always assumed

00:06:11,669 --> 00:06:15,599
which which are not always true and that

00:06:13,740 --> 00:06:18,810
then helps you to move over then for

00:06:15,599 --> 00:06:21,210
example so before we we had like this

00:06:18,810 --> 00:06:22,650
asset compliant databases right so the

00:06:21,210 --> 00:06:25,250
like the traditional database with

00:06:22,650 --> 00:06:27,629
transactions which were basically

00:06:25,250 --> 00:06:30,300
designed in the time where you would use

00:06:27,629 --> 00:06:32,819
them to really like keep track of money

00:06:30,300 --> 00:06:35,610
and other things and that was really

00:06:32,819 --> 00:06:37,710
important that like if like two people

00:06:35,610 --> 00:06:40,169
are making deposits then they don't

00:06:37,710 --> 00:06:42,210
interact and all kind of stuff but then

00:06:40,169 --> 00:06:44,460
as they as we started to build all these

00:06:42,210 --> 00:06:47,460
websites based on this database

00:06:44,460 --> 00:06:51,139
technology it became clear that like not

00:06:47,460 --> 00:06:53,899
everything here is really necessary for

00:06:51,139 --> 00:06:55,620
particular people realize that the

00:06:53,899 --> 00:06:56,160
consistency you don't need strong

00:06:55,620 --> 00:06:58,710
consistent

00:06:56,160 --> 00:07:00,120
see so if you use your database just to

00:06:58,710 --> 00:07:02,370
store like user interactions or

00:07:00,120 --> 00:07:05,130
something it's okay if some of the users

00:07:02,370 --> 00:07:07,650
see the results only after 10 minute or

00:07:05,130 --> 00:07:09,900
so so we dropped consistency and we made

00:07:07,650 --> 00:07:13,170
this jump and came out at the with no

00:07:09,900 --> 00:07:14,850
sequel in a similar way so we all had

00:07:13,170 --> 00:07:16,920
these laptops and then like Steve Jobs

00:07:14,850 --> 00:07:18,990
realized okay you don't for many

00:07:16,920 --> 00:07:20,580
applications you don't really need a

00:07:18,990 --> 00:07:22,230
keyboard you don't need all this

00:07:20,580 --> 00:07:23,940
processing power or you even don't need

00:07:22,230 --> 00:07:25,500
a file system right and then he said

00:07:23,940 --> 00:07:27,180
okay so maybe instead we want something

00:07:25,500 --> 00:07:28,860
like which is basically screen which

00:07:27,180 --> 00:07:30,960
which we can carry it out anywhere we go

00:07:28,860 --> 00:07:32,910
and then we came up with a new solution

00:07:30,960 --> 00:07:35,400
right so this doesn't solve all the

00:07:32,910 --> 00:07:38,130
problems which this also in a way it's a

00:07:35,400 --> 00:07:40,260
it's a it's a change so you lose

00:07:38,130 --> 00:07:41,700
something but actually for some

00:07:40,260 --> 00:07:42,990
application it makes a lot of sense to

00:07:41,700 --> 00:07:45,930
do that and then you end up with

00:07:42,990 --> 00:07:48,590
something which is much better fitted

00:07:45,930 --> 00:07:52,560
than the solution you had before okay

00:07:48,590 --> 00:07:55,950
and in a way so this is maybe the first

00:07:52,560 --> 00:07:57,750
main point of this talk so the thing for

00:07:55,950 --> 00:08:01,650
there are applications out there and I

00:07:57,750 --> 00:08:04,230
think reducer profiling is one of them a

00:08:01,650 --> 00:08:06,990
reactance user we're actually actually

00:08:04,230 --> 00:08:09,780
exactness is not necessary so in

00:08:06,990 --> 00:08:12,270
classical big data so in particular

00:08:09,780 --> 00:08:14,220
these exact aggregates like count an

00:08:12,270 --> 00:08:15,930
average which we like which originally

00:08:14,220 --> 00:08:18,120
came from the database you know all

00:08:15,930 --> 00:08:19,500
these so you select from the table and

00:08:18,120 --> 00:08:21,210
you say you want to count and then you

00:08:19,500 --> 00:08:22,500
group by on the user and then you get in

00:08:21,210 --> 00:08:23,460
the end out how often the user is

00:08:22,500 --> 00:08:25,620
interacting with your website or

00:08:23,460 --> 00:08:27,780
something so these are these basic

00:08:25,620 --> 00:08:32,030
aggregate operations and that's that's

00:08:27,780 --> 00:08:35,250
something we have since the 70s okay

00:08:32,030 --> 00:08:37,169
yeah there they are exact right in the

00:08:35,250 --> 00:08:39,719
sense that they really return the number

00:08:37,169 --> 00:08:42,780
of examples they are but in a way they

00:08:39,719 --> 00:08:45,150
also restrict our abilities to deal with

00:08:42,780 --> 00:08:47,820
lots of data because for like even for

00:08:45,150 --> 00:08:49,980
events which are very not not very often

00:08:47,820 --> 00:08:52,050
you still have to keep that count so if

00:08:49,980 --> 00:08:54,030
you get that if you if you lose that and

00:08:52,050 --> 00:08:56,430
you say okay from my application it's

00:08:54,030 --> 00:08:59,430
not necessary just as you wouldn't use a

00:08:56,430 --> 00:09:01,560
laptop to surf from your couch okay then

00:08:59,430 --> 00:09:04,110
you end up with so-called stream mining

00:09:01,560 --> 00:09:09,180
algorithms which is a class of

00:09:04,110 --> 00:09:09,900
approximate algorithms so yeah okay yeah

00:09:09,180 --> 00:09:12,840
sorry

00:09:09,900 --> 00:09:17,460
to talk about this later on okay but

00:09:12,840 --> 00:09:20,070
just to come back so right so this is

00:09:17,460 --> 00:09:22,140
how it's done right now actually what

00:09:20,070 --> 00:09:23,820
I'm saying is if you say I don't need

00:09:22,140 --> 00:09:26,040
exact results you can actually replace

00:09:23,820 --> 00:09:27,540
all of this with a solution which is

00:09:26,040 --> 00:09:29,700
much better integrated and fitted and

00:09:27,540 --> 00:09:31,530
has much higher performance so this has

00:09:29,700 --> 00:09:35,520
also work and there are cases where this

00:09:31,530 --> 00:09:36,930
is exactly what you need but if you go

00:09:35,520 --> 00:09:38,130
the other way there are solutions which

00:09:36,930 --> 00:09:43,320
I must simply and I'm going to talk

00:09:38,130 --> 00:09:46,110
about how to do that now okay so just to

00:09:43,320 --> 00:09:48,720
say why why is it that if you if you

00:09:46,110 --> 00:09:51,150
deal with user data while is it that in

00:09:48,720 --> 00:09:54,750
many cases you can actually live with

00:09:51,150 --> 00:09:57,540
having a proximate result and the main

00:09:54,750 --> 00:09:59,790
reason is that the dislike the activity

00:09:57,540 --> 00:10:01,920
of users is usually look something like

00:09:59,790 --> 00:10:04,230
this so there are a few users who are

00:10:01,920 --> 00:10:06,270
interacting a lot and then there are

00:10:04,230 --> 00:10:08,400
many who just you know show up once a

00:10:06,270 --> 00:10:11,250
week or so and of course like this is

00:10:08,400 --> 00:10:13,140
the part where you really want to spend

00:10:11,250 --> 00:10:15,300
all your computing time and your money

00:10:13,140 --> 00:10:16,770
to be able to react in real time because

00:10:15,300 --> 00:10:19,110
these are the users who are most active

00:10:16,770 --> 00:10:22,080
we know who who actually buying things

00:10:19,110 --> 00:10:24,360
whereas these are users so right it

00:10:22,080 --> 00:10:26,160
doesn't really make sense to to have

00:10:24,360 --> 00:10:30,210
like a whole class that just all these

00:10:26,160 --> 00:10:31,890
are results for these users or you can

00:10:30,210 --> 00:10:33,930
just do it in the old way but then you

00:10:31,890 --> 00:10:36,360
don't have to speed it up then that that

00:10:33,930 --> 00:10:41,610
that you don't have to spend so much

00:10:36,360 --> 00:10:44,340
money to speed it up okay yeah okay so

00:10:41,610 --> 00:10:45,780
what is three mining so stream mining is

00:10:44,340 --> 00:10:48,150
a class of algorithms which has been

00:10:45,780 --> 00:10:49,860
developed in the mid-2000s to deal with

00:10:48,150 --> 00:10:51,750
the questions of answering so-called

00:10:49,860 --> 00:10:53,310
stream queries with finite resources so

00:10:51,750 --> 00:10:55,620
you have an event stream which comes by

00:10:53,310 --> 00:10:58,410
and you say you don't have enough memory

00:10:55,620 --> 00:11:00,780
so neither in them dealer like ram or

00:10:58,410 --> 00:11:02,250
disk to store all of the information is

00:11:00,780 --> 00:11:06,600
in there but you're still interested for

00:11:02,250 --> 00:11:08,040
example to count how often the items the

00:11:06,600 --> 00:11:10,140
different items appear in the stream

00:11:08,040 --> 00:11:12,540
right so if the event stream is a user

00:11:10,140 --> 00:11:15,660
so you want to know which user has

00:11:12,540 --> 00:11:17,460
viewed how many web pages or the events

00:11:15,660 --> 00:11:18,840
are web pages you want to know like

00:11:17,460 --> 00:11:20,580
pages on your website you want to know

00:11:18,840 --> 00:11:23,220
which page has been you too often and

00:11:20,580 --> 00:11:23,880
you say but i but i only have so i know

00:11:23,220 --> 00:11:25,650
there

00:11:23,880 --> 00:11:29,370
like I have 1 billion users but I only

00:11:25,650 --> 00:11:31,770
have one megabyte of RAM to count it ok

00:11:29,370 --> 00:11:36,090
and this these are these are algorithms

00:11:31,770 --> 00:11:38,820
which right so extreme pastor data you

00:11:36,090 --> 00:11:40,440
you have this analyzer which has a only

00:11:38,820 --> 00:11:43,410
uses bounded resources and then you get

00:11:40,440 --> 00:11:44,880
another result which is approximate but

00:11:43,410 --> 00:11:46,710
usually comes with a theoretical

00:11:44,880 --> 00:11:48,090
guarantee which says if I have this

00:11:46,710 --> 00:11:50,250
amount of memory then the error i'm

00:11:48,090 --> 00:11:52,200
going to make it smaller than that so

00:11:50,250 --> 00:11:54,660
how do these algorithms look like so

00:11:52,200 --> 00:11:57,980
luckily they actually quite easy to

00:11:54,660 --> 00:12:00,480
understand so here's an algorithm which

00:11:57,980 --> 00:12:02,340
counts these activities of a large item

00:12:00,480 --> 00:12:04,500
sets of millions of users IP addresses

00:12:02,340 --> 00:12:07,140
Twitter users whatever and the algorithm

00:12:04,500 --> 00:12:09,720
works as follows so you have a fixed

00:12:07,140 --> 00:12:11,850
table of counts so in this case I say I

00:12:09,720 --> 00:12:14,970
have I only have room for six numbers

00:12:11,850 --> 00:12:17,640
okay and these are the names of the

00:12:14,970 --> 00:12:19,290
users who came to my website so and if i

00:12:17,640 --> 00:12:20,940
have a new user coming by actually i

00:12:19,290 --> 00:12:23,520
have two cases so either he's already in

00:12:20,940 --> 00:12:25,590
the table then I just increased his

00:12:23,520 --> 00:12:29,160
count so Paul goes from all those

00:12:25,590 --> 00:12:31,380
actually Paul from 12 to 13 or if it's a

00:12:29,160 --> 00:12:33,240
user that which was not in there then I

00:12:31,380 --> 00:12:35,850
take the the one which has the least

00:12:33,240 --> 00:12:38,340
activity in here and I dropped him but I

00:12:35,850 --> 00:12:40,050
take his count as a starting point for

00:12:38,340 --> 00:12:44,460
the new one and then you can prove that

00:12:40,050 --> 00:12:47,190
this is the like the the worst case of

00:12:44,460 --> 00:12:49,380
times that Nico was already in the table

00:12:47,190 --> 00:12:50,730
but then I removed him because somebody

00:12:49,380 --> 00:12:53,940
else showed up right so it might be that

00:12:50,730 --> 00:12:56,220
Nico had been in here yeah at most three

00:12:53,940 --> 00:12:58,080
times in a way okay and then there's a

00:12:56,220 --> 00:12:59,340
paper you know you can as you want to

00:12:58,080 --> 00:13:01,380
really interested in the proves you can

00:12:59,340 --> 00:13:03,000
all read all that but so the interesting

00:13:01,380 --> 00:13:09,210
thing is it's actually it's quite a

00:13:03,000 --> 00:13:10,740
simple algorithm which yeah which sort

00:13:09,210 --> 00:13:13,080
of also does its own memory management

00:13:10,740 --> 00:13:14,880
so I'm not saying you can you cannot

00:13:13,080 --> 00:13:17,670
implement this using existing

00:13:14,880 --> 00:13:20,070
architecture okay it's just you could if

00:13:17,670 --> 00:13:21,450
you wanted to another algorithm which

00:13:20,070 --> 00:13:24,390
you probably Oh towns are these count

00:13:21,450 --> 00:13:27,570
min sketches and so the algorithm before

00:13:24,390 --> 00:13:29,250
actually has like a count so here you

00:13:27,570 --> 00:13:32,850
really have a list of all the people who

00:13:29,250 --> 00:13:34,710
are on a website and you can later on go

00:13:32,850 --> 00:13:37,370
in and say okay so tell me who is the

00:13:34,710 --> 00:13:39,779
user who was your most often

00:13:37,370 --> 00:13:42,509
so count min sketches work differently

00:13:39,779 --> 00:13:45,060
so you don't actually store the IDS of

00:13:42,509 --> 00:13:46,740
the things you count it's only a data

00:13:45,060 --> 00:13:48,629
structure which you can query so if I

00:13:46,740 --> 00:13:50,040
have that user I can ask so how often is

00:13:48,629 --> 00:13:51,899
that user been here and then I get an

00:13:50,040 --> 00:13:54,449
approximation for how often he's been

00:13:51,899 --> 00:13:56,759
there but I cannot go in and say okay

00:13:54,449 --> 00:14:00,930
tell me who's the like the most active

00:13:56,759 --> 00:14:04,199
one it works like this so you have a

00:14:00,930 --> 00:14:06,209
certain number of bins and then we have

00:14:04,199 --> 00:14:10,110
a certain number so you have these end

00:14:06,209 --> 00:14:11,610
times M bins and times and each row here

00:14:10,110 --> 00:14:13,529
in this matrix corresponds to a

00:14:11,610 --> 00:14:16,199
different hash function and if you have

00:14:13,529 --> 00:14:18,899
a new entry you compute for each row the

00:14:16,199 --> 00:14:21,060
hash functions and then you count the

00:14:18,899 --> 00:14:22,410
the entries to which these different

00:14:21,060 --> 00:14:23,910
hash functions point right so here I

00:14:22,410 --> 00:14:26,579
have some new entry i don't know maybe

00:14:23,910 --> 00:14:28,560
niko is coming by again so here it's the

00:14:26,579 --> 00:14:30,899
the third pin the second the falls and

00:14:28,560 --> 00:14:34,709
the third and then I count these things

00:14:30,899 --> 00:14:37,699
up and when I query it actually I go I

00:14:34,709 --> 00:14:40,649
do the same thing but then I take the

00:14:37,699 --> 00:14:42,509
the count which the smallest count I get

00:14:40,649 --> 00:14:44,459
in all of those and that way I'm

00:14:42,509 --> 00:14:46,709
minimizing the collisions right so

00:14:44,459 --> 00:14:49,829
another user might also end up counting

00:14:46,709 --> 00:14:51,930
up this pin here but I do it like n

00:14:49,829 --> 00:14:53,699
different times and therefore I get

00:14:51,930 --> 00:14:57,209
better and better the more the more

00:14:53,699 --> 00:15:02,250
memory I have okay it's very again like

00:14:57,209 --> 00:15:05,040
a very simple it's not yeah it's i said

00:15:02,250 --> 00:15:06,329
i'd say it's quite easy to understand

00:15:05,040 --> 00:15:07,350
how it works so if you want implement

00:15:06,329 --> 00:15:09,120
actually it's a bit different a

00:15:07,350 --> 00:15:10,529
difficult how you get n different hash

00:15:09,120 --> 00:15:13,680
functions for their techniques for that

00:15:10,529 --> 00:15:16,500
but that's the basic idea here right and

00:15:13,680 --> 00:15:18,060
this is very good if you write if you

00:15:16,500 --> 00:15:19,380
really only want to query if you don't

00:15:18,060 --> 00:15:20,880
want the trend but you just want to

00:15:19,380 --> 00:15:23,220
query and you want to have like a better

00:15:20,880 --> 00:15:25,110
approximation of all of those where it's

00:15:23,220 --> 00:15:29,040
like in this case for the smaller ones

00:15:25,110 --> 00:15:30,839
there i will usually quite not okay so

00:15:29,040 --> 00:15:32,490
counting we already have like so instead

00:15:30,839 --> 00:15:34,439
of doing these average things and

00:15:32,490 --> 00:15:36,420
counting like in is in sequel you could

00:15:34,439 --> 00:15:37,860
use a data structure like this and get

00:15:36,420 --> 00:15:40,709
approximate count and not use a lot of

00:15:37,860 --> 00:15:43,050
memory then the other thing but with the

00:15:40,709 --> 00:15:45,149
sequel count is always that you usually

00:15:43,050 --> 00:15:47,189
want to have activity over time frame

00:15:45,149 --> 00:15:49,019
you want to say okay who the most which

00:15:47,189 --> 00:15:50,040
page has been visited most often today

00:15:49,019 --> 00:15:51,570
and then

00:15:50,040 --> 00:15:53,940
had like this where clause where you

00:15:51,570 --> 00:15:57,360
where you bracket the x terms of your

00:15:53,940 --> 00:15:59,790
events and that's also something where

00:15:57,360 --> 00:16:01,709
you can do a approximation right so if

00:15:59,790 --> 00:16:03,690
you do this exactly you really have to

00:16:01,709 --> 00:16:06,000
keep all the events in there so that you

00:16:03,690 --> 00:16:07,860
can you know go through it or if you try

00:16:06,000 --> 00:16:09,449
to optimize it a bit then you probably

00:16:07,860 --> 00:16:11,310
you have a data structure where you put

00:16:09,449 --> 00:16:14,009
in the event when it occurs and then you

00:16:11,310 --> 00:16:15,810
take it out like a like a day later or a

00:16:14,009 --> 00:16:17,940
week later but then you still have to

00:16:15,810 --> 00:16:19,470
keep all the events in here or what you

00:16:17,940 --> 00:16:22,410
can do is you can like dump the whole

00:16:19,470 --> 00:16:24,180
table at certain intervals and then you

00:16:22,410 --> 00:16:25,740
can look at like what is the count today

00:16:24,180 --> 00:16:27,630
what is the counter day ago and then the

00:16:25,740 --> 00:16:29,310
difference between those will be the

00:16:27,630 --> 00:16:31,889
actual number of times this a conduct

00:16:29,310 --> 00:16:34,170
occurred so by doing instead instead of

00:16:31,889 --> 00:16:35,970
doing this you could also do do an

00:16:34,170 --> 00:16:38,339
exponential decay that is so if the

00:16:35,970 --> 00:16:41,220
counter-curse it starts with one but

00:16:38,339 --> 00:16:44,519
then over time it will just decay right

00:16:41,220 --> 00:16:46,560
in that way so each time you so event a

00:16:44,519 --> 00:16:48,959
curse and then if no event occurs the

00:16:46,560 --> 00:16:51,180
count will just decay and then when a

00:16:48,959 --> 00:16:52,440
new one comes you add them up and then

00:16:51,180 --> 00:16:54,990
you have a new thing which again the

00:16:52,440 --> 00:16:57,510
case okay and this is not the same thing

00:16:54,990 --> 00:16:59,970
as having these exact numbers but it's

00:16:57,510 --> 00:17:02,490
again a good approximation over if you

00:16:59,970 --> 00:17:06,419
use different time scales here you get

00:17:02,490 --> 00:17:10,500
different approximations over these

00:17:06,419 --> 00:17:13,350
counters with time scales okay and there

00:17:10,500 --> 00:17:15,120
that sort of gives you like a way to

00:17:13,350 --> 00:17:16,740
very quickly okay and the good thing

00:17:15,120 --> 00:17:19,290
here is you really just need to store

00:17:16,740 --> 00:17:22,189
the score and the last time stamp you've

00:17:19,290 --> 00:17:24,720
seen here so just two numbers pantry

00:17:22,189 --> 00:17:27,870
okay and you can you can combine this

00:17:24,720 --> 00:17:30,690
with this data structure to get

00:17:27,870 --> 00:17:32,520
something where you have you

00:17:30,690 --> 00:17:34,049
automatically have a trend over a

00:17:32,520 --> 00:17:37,049
certain time scale so you get the most

00:17:34,049 --> 00:17:38,790
active users over the time scale you

00:17:37,049 --> 00:17:41,970
have defined and that way you you

00:17:38,790 --> 00:17:43,590
already to get an approximation to the

00:17:41,970 --> 00:17:48,870
Select statement I talked about it in

00:17:43,590 --> 00:17:51,450
the first slide okay there's one more

00:17:48,870 --> 00:17:55,500
thing so usually so here right it's just

00:17:51,450 --> 00:17:57,210
it's just entries I just seem simple in

00:17:55,500 --> 00:17:59,010
its single names but it could also be

00:17:57,210 --> 00:18:00,360
tuple so it could also be like not not

00:17:59,010 --> 00:18:03,059
the vendors not just that Frank has

00:18:00,360 --> 00:18:05,340
occurred but Frank Frank Cain

00:18:03,059 --> 00:18:06,899
from that refer or something and he

00:18:05,340 --> 00:18:10,049
looked at web that website so you could

00:18:06,899 --> 00:18:12,570
actually have something where you the

00:18:10,049 --> 00:18:14,879
event is not just a single object but

00:18:12,570 --> 00:18:17,600
it's actually like a combination of a

00:18:14,879 --> 00:18:21,289
page a referral and an IP address and

00:18:17,600 --> 00:18:24,299
then you count how often this specific

00:18:21,289 --> 00:18:25,919
combination has occurred and then what

00:18:24,299 --> 00:18:28,679
you can do is you can have secondary

00:18:25,919 --> 00:18:29,759
indices so this is already about getting

00:18:28,679 --> 00:18:31,769
it's starting to get a bit more

00:18:29,759 --> 00:18:33,450
complicated so we still like the sings a

00:18:31,769 --> 00:18:36,720
single table with accounts the bounded

00:18:33,450 --> 00:18:39,330
table but you also have like a binary

00:18:36,720 --> 00:18:42,059
search tree like on the side of it which

00:18:39,330 --> 00:18:44,249
keeps this thing sorry for each of these

00:18:42,059 --> 00:18:45,960
columns you have like another entry

00:18:44,249 --> 00:18:48,059
which says okay now and then what you

00:18:45,960 --> 00:18:49,740
can do is you can say now give me only

00:18:48,059 --> 00:18:52,110
the entries which have been referred

00:18:49,740 --> 00:18:55,139
from google or give me only the entries

00:18:52,110 --> 00:18:56,730
for the index page okay and that way you

00:18:55,139 --> 00:18:59,009
are already getting to a point where you

00:18:56,730 --> 00:19:00,539
cannot just have one count but actually

00:18:59,009 --> 00:19:02,220
you have like a data structure which you

00:19:00,539 --> 00:19:07,759
can query for all kinds of relationships

00:19:02,220 --> 00:19:11,730
between your data ok so now using that

00:19:07,759 --> 00:19:13,830
right you can't start to to store data

00:19:11,730 --> 00:19:16,440
in a very compact manner to do all kinds

00:19:13,830 --> 00:19:18,480
of analysis so for example if you just

00:19:16,440 --> 00:19:21,960
want to store like an array in here so

00:19:18,480 --> 00:19:24,269
you have counts for XD and a so it the

00:19:21,960 --> 00:19:26,999
council XS or window course I've times D

00:19:24,269 --> 00:19:28,649
or a two and A or III you put them into

00:19:26,999 --> 00:19:31,919
the strand and then it's already it's

00:19:28,649 --> 00:19:35,909
automatically sorted but that way it's

00:19:31,919 --> 00:19:37,230
also like you can put a whole vector in

00:19:35,909 --> 00:19:39,749
there like a whole matrix and whole

00:19:37,230 --> 00:19:41,970
array and then the data structure

00:19:39,749 --> 00:19:44,669
automatically only keeps the the largest

00:19:41,970 --> 00:19:45,899
items okay if that's and if that's what

00:19:44,669 --> 00:19:49,080
you're interested in then everything is

00:19:45,899 --> 00:19:51,990
good so you can you know just count

00:19:49,080 --> 00:19:53,070
stuff over very large scales and you

00:19:51,990 --> 00:19:54,570
have something which automatically

00:19:53,070 --> 00:19:56,610
focuses on the things which are most

00:19:54,570 --> 00:19:58,409
active you can extend this and for

00:19:56,610 --> 00:20:00,840
example if you have whole profile so for

00:19:58,409 --> 00:20:03,179
X for example you're not only have one

00:20:00,840 --> 00:20:05,429
count but you say so some so X might be

00:20:03,179 --> 00:20:08,070
user and a might be some category he's

00:20:05,429 --> 00:20:11,009
been looking at so this he has seen a

00:20:08,070 --> 00:20:13,289
five times b1 times c2 times and user

00:20:11,009 --> 00:20:16,049
why has seen a eight times and d 2 times

00:20:13,289 --> 00:20:17,669
right and then you just sort these you

00:20:16,049 --> 00:20:20,730
put these in and then you get this sort

00:20:17,669 --> 00:20:24,690
of thing out here sorted by the you the

00:20:20,730 --> 00:20:29,220
actual score and if you then do the

00:20:24,690 --> 00:20:31,830
query on these column indices you will

00:20:29,220 --> 00:20:34,710
like you will reconstruct these these

00:20:31,830 --> 00:20:37,740
profiles yeah okay and then again if you

00:20:34,710 --> 00:20:41,669
have more proof more data then you have

00:20:37,740 --> 00:20:44,250
like a memory then it will start to

00:20:41,669 --> 00:20:46,019
automatically discard the ones which are

00:20:44,250 --> 00:20:47,759
very small and you sort of get an

00:20:46,019 --> 00:20:52,919
approximation which focuses on the most

00:20:47,759 --> 00:20:54,809
active ones okay so one more thing so

00:20:52,919 --> 00:20:57,720
you can even like store matrix in there

00:20:54,809 --> 00:21:00,809
so like the eyes are the the rows and

00:20:57,720 --> 00:21:04,139
jays are the columns and if you store it

00:21:00,809 --> 00:21:08,279
in here right you get the so 8 is the

00:21:04,139 --> 00:21:10,740
largest so I i equal 3 j equal 38 is the

00:21:08,279 --> 00:21:13,679
entry so 338 this is just what's not

00:21:10,740 --> 00:21:15,629
there at 11 is the 5 and so on and then

00:21:13,679 --> 00:21:18,539
you can store the sparse matrix in here

00:21:15,629 --> 00:21:20,879
and if you sort of like if you then use

00:21:18,539 --> 00:21:22,860
the the industry's you have on the

00:21:20,879 --> 00:21:26,869
columns of this matrix here you can

00:21:22,860 --> 00:21:29,309
either get the row so too would be like

00:21:26,869 --> 00:21:31,980
like in the second row you have a two

00:21:29,309 --> 00:21:39,029
and one and the first row you have the 5

00:21:31,980 --> 00:21:41,190
into 3 2 what oh I'm up okay now this is

00:21:39,029 --> 00:21:42,779
this column here okay so I'm just saying

00:21:41,190 --> 00:21:44,639
so you can you can actually take a

00:21:42,779 --> 00:21:47,340
sparse matrix which is something you

00:21:44,639 --> 00:21:49,169
know you would use to sana and I'm going

00:21:47,340 --> 00:21:50,850
to discuss in the middle towel would you

00:21:49,169 --> 00:21:52,980
how that occurs in recommendation but

00:21:50,850 --> 00:21:54,119
you can also take like a mathematical

00:21:52,980 --> 00:21:57,359
data structure like that and actually

00:21:54,119 --> 00:21:59,340
store it into one of these tables in

00:21:57,359 --> 00:22:00,960
this data structures industry money data

00:21:59,340 --> 00:22:02,789
structures and get an approximation at

00:22:00,960 --> 00:22:07,740
all time which never grows in memory

00:22:02,789 --> 00:22:09,749
which is always very stable okay so

00:22:07,740 --> 00:22:11,730
actually we've built this thing called

00:22:09,749 --> 00:22:14,029
spin drill can look at it there is a

00:22:11,730 --> 00:22:17,190
demo version you can download which is a

00:22:14,029 --> 00:22:19,440
exactly a real-time analysis engine

00:22:17,190 --> 00:22:22,200
which has been built using these data

00:22:19,440 --> 00:22:23,879
structures so it's based on a heavy

00:22:22,200 --> 00:22:26,369
hitters counting exponential decay as I

00:22:23,879 --> 00:22:28,619
explained and you get instant counts and

00:22:26,369 --> 00:22:30,010
top K results over time windows just by

00:22:28,619 --> 00:22:32,650
carrying the data structure it

00:22:30,010 --> 00:22:36,280
all in memory in written and scada and

00:22:32,650 --> 00:22:38,290
we also started to construct modules

00:22:36,280 --> 00:22:40,570
based on this which are also going to

00:22:38,290 --> 00:22:42,280
talk about a bit now so the interesting

00:22:40,570 --> 00:22:43,690
thing is so so people always when I say

00:22:42,280 --> 00:22:45,550
let's talk about three people always ask

00:22:43,690 --> 00:22:47,350
me whether you can't can't you just do

00:22:45,550 --> 00:22:49,120
that in storm right but I think that's

00:22:47,350 --> 00:22:50,680
the wrong question so of course we could

00:22:49,120 --> 00:22:52,840
also have implemented this a storm but

00:22:50,680 --> 00:22:55,360
actually we could just implement these

00:22:52,840 --> 00:22:57,040
things not using any of the big data

00:22:55,360 --> 00:22:58,330
infrastructure's but we still ended up

00:22:57,040 --> 00:23:00,550
with something that we can deal with

00:22:58,330 --> 00:23:01,630
lots of data and millions of events just

00:23:00,550 --> 00:23:04,150
on a single machine you know without

00:23:01,630 --> 00:23:06,610
eating without eating scaling out and I

00:23:04,150 --> 00:23:10,000
think that's the that sort of shows what

00:23:06,610 --> 00:23:13,240
the power of this approaches okay so

00:23:10,000 --> 00:23:15,100
real time user profiles so how would you

00:23:13,240 --> 00:23:18,580
do this now actually it is it's quite

00:23:15,100 --> 00:23:20,440
simple actually so like if the vendors

00:23:18,580 --> 00:23:22,720
the event is that the user has looked at

00:23:20,440 --> 00:23:24,400
a certain category you have a trend

00:23:22,720 --> 00:23:27,580
which tracks users in categories and

00:23:24,400 --> 00:23:30,340
then if you if you look for a certain

00:23:27,580 --> 00:23:33,490
user using these indices you get the

00:23:30,340 --> 00:23:36,010
profile out okay and then if you so you

00:23:33,490 --> 00:23:38,860
update the corresponding combinations

00:23:36,010 --> 00:23:40,390
here and just as I said period i said

00:23:38,860 --> 00:23:42,090
before how you would store profile

00:23:40,390 --> 00:23:44,380
information you can just get it out by

00:23:42,090 --> 00:23:46,030
consulting these entities and this

00:23:44,380 --> 00:23:48,010
doesn't take any additional computation

00:23:46,030 --> 00:23:50,200
time you just get it out and so these

00:23:48,010 --> 00:23:53,830
would be like for neuronal cars you have

00:23:50,200 --> 00:23:56,860
that much inactivity for video games you

00:23:53,830 --> 00:24:00,160
get this and so on so we actually have

00:23:56,860 --> 00:24:01,480
that in production with a grave a pilot

00:24:00,160 --> 00:24:03,400
project with the company who does

00:24:01,480 --> 00:24:05,290
behavioral targeting so they they are

00:24:03,400 --> 00:24:07,420
the ones who follow you over website and

00:24:05,290 --> 00:24:09,340
set cookies and then they analyse the

00:24:07,420 --> 00:24:12,640
website you look at and then they build

00:24:09,340 --> 00:24:14,110
a profile of you too and then like if

00:24:12,640 --> 00:24:16,300
you when you show weird you can actually

00:24:14,110 --> 00:24:17,530
say okay show me an ad so this ad only

00:24:16,300 --> 00:24:20,680
two people were interested in cars

00:24:17,530 --> 00:24:22,300
something like that ok but so the thing

00:24:20,680 --> 00:24:24,490
that the system they had was

00:24:22,300 --> 00:24:26,050
specifically designed to integrate all

00:24:24,490 --> 00:24:27,880
this information over time so that they

00:24:26,050 --> 00:24:29,830
get a profile like a basic profile of

00:24:27,880 --> 00:24:32,920
you and it could not deal with these

00:24:29,830 --> 00:24:34,660
real-time things but this way so this

00:24:32,920 --> 00:24:41,080
the way this works right now is they

00:24:34,660 --> 00:24:42,850
have a big about 30 notes which on each

00:24:41,080 --> 00:24:43,559
of these nodes run a number of ruby

00:24:42,850 --> 00:24:45,659
rayes on

00:24:43,559 --> 00:24:47,789
Ruby in style in census which actually

00:24:45,659 --> 00:24:50,549
do the analysis and then all these

00:24:47,789 --> 00:24:54,320
packages these notes send by UDP the

00:24:50,549 --> 00:24:57,269
events to stream draw and then for that

00:24:54,320 --> 00:24:59,549
for each user you can get this profile

00:24:57,269 --> 00:25:01,590
out for the last over the last day the

00:24:59,549 --> 00:25:03,690
last week and then you can directly you

00:25:01,590 --> 00:25:07,080
know make the comparison and see okay so

00:25:03,690 --> 00:25:09,029
there is a category here which over the

00:25:07,080 --> 00:25:10,559
last week of the day wasn't very active

00:25:09,029 --> 00:25:11,850
but now it's very active so this is

00:25:10,559 --> 00:25:14,999
something which is interesting which you

00:25:11,850 --> 00:25:17,610
could use to show in something okay so

00:25:14,999 --> 00:25:19,889
this looks like this it's very small so

00:25:17,610 --> 00:25:21,629
there's a dashboard but the dashboard is

00:25:19,889 --> 00:25:23,340
not really like it's just for us to see

00:25:21,629 --> 00:25:25,679
whether it's working or not so this the

00:25:23,340 --> 00:25:27,210
real value comes from having a rest

00:25:25,679 --> 00:25:29,309
interface on the back where you can

00:25:27,210 --> 00:25:30,749
within milliseconds for each user get

00:25:29,309 --> 00:25:31,919
these profiles and make the decision

00:25:30,749 --> 00:25:34,049
whether you there's something you want

00:25:31,919 --> 00:25:36,029
to show this one or not so these are the

00:25:34,049 --> 00:25:39,330
users down here and the activities and

00:25:36,029 --> 00:25:40,830
these are these these fingerprints and

00:25:39,330 --> 00:25:42,720
these heroes are show the histograms

00:25:40,830 --> 00:25:45,269
over the different categories and for a

00:25:42,720 --> 00:25:46,889
single user can look like this so these

00:25:45,269 --> 00:25:49,710
are the categories chat information

00:25:46,889 --> 00:25:52,049
entertainment media of a weekday an hour

00:25:49,710 --> 00:25:54,509
and here are the the differences in

00:25:52,049 --> 00:25:56,580
percentage points between those and then

00:25:54,509 --> 00:25:58,200
you can say okay here like entertainment

00:25:56,580 --> 00:26:06,240
media he's currently interested in that

00:25:58,200 --> 00:26:08,129
so that show him that okay and so we

00:26:06,240 --> 00:26:11,190
were a we were able to process about

00:26:08,129 --> 00:26:13,169
10,000 events per second on just one

00:26:11,190 --> 00:26:18,379
machine which has like 16 gigabytes of

00:26:13,169 --> 00:26:20,669
RAM okay so it's really not much sorry

00:26:18,379 --> 00:26:22,499
we haven't even thought of so you could

00:26:20,669 --> 00:26:27,179
even chart this very easily just by

00:26:22,499 --> 00:26:30,539
users but it hasn't been necessary so

00:26:27,179 --> 00:26:33,590
far so with one gigabyte of so with 16

00:26:30,539 --> 00:26:37,470
gigabytes you can track about 12 million

00:26:33,590 --> 00:26:39,059
like single counts which already gets

00:26:37,470 --> 00:26:41,519
you a lot of information about the most

00:26:39,059 --> 00:26:45,990
active users those which you want to

00:26:41,519 --> 00:26:50,639
engage with I think this is really so

00:26:45,990 --> 00:26:52,049
this is for us it's always very like I

00:26:50,639 --> 00:26:54,450
think it's very impressive right so you

00:26:52,049 --> 00:26:56,310
so you're not sticking together together

00:26:54,450 --> 00:26:58,170
stuff like from this different in fact

00:26:56,310 --> 00:26:59,730
parts which you could also do but even

00:26:58,170 --> 00:27:01,350
if you just say okay I just have a

00:26:59,730 --> 00:27:02,910
single machine which which also has

00:27:01,350 --> 00:27:04,740
advantages like you don't have to deal

00:27:02,910 --> 00:27:07,080
with just this is distribution with a

00:27:04,740 --> 00:27:08,640
networking overhead you can build

00:27:07,080 --> 00:27:16,820
something which I can actually process

00:27:08,640 --> 00:27:19,620
assyrians amount of data okay right okay

00:27:16,820 --> 00:27:21,510
so recommendation recommendations are a

00:27:19,620 --> 00:27:23,250
bit more difficult of course so the

00:27:21,510 --> 00:27:25,020
basic idea is so we're using this on a

00:27:23,250 --> 00:27:27,690
website called zero in junkies which is

00:27:25,020 --> 00:27:31,170
a german website of a TV shows and the

00:27:27,690 --> 00:27:32,820
goal was that you for one TV show you

00:27:31,170 --> 00:27:37,260
want to see here in real time what are

00:27:32,820 --> 00:27:38,820
we like related TV shows down here so i

00:27:37,260 --> 00:27:40,770
mean a recommendation is very hard i

00:27:38,820 --> 00:27:42,690
know that ok but so the the good thing

00:27:40,770 --> 00:27:44,190
about this specific approach was that

00:27:42,690 --> 00:27:46,860
there weren't so many items you could

00:27:44,190 --> 00:27:48,780
rank recommend fun from there were only

00:27:46,860 --> 00:27:50,370
like four thousand TV shows in the

00:27:48,780 --> 00:27:53,580
database and most of them of course are

00:27:50,370 --> 00:27:54,990
not not active anymore so it's a i mean

00:27:53,580 --> 00:27:56,430
recommendation gets very hard when you

00:27:54,990 --> 00:27:58,770
really have to recommend for millions of

00:27:56,430 --> 00:28:00,840
objects because the machine is no way to

00:27:58,770 --> 00:28:02,910
tell what is related to what because the

00:28:00,840 --> 00:28:04,950
data source parts but in this case if

00:28:02,910 --> 00:28:06,630
you have like a few thousand items then

00:28:04,950 --> 00:28:09,420
like a normal collaborative filtering

00:28:06,630 --> 00:28:13,920
like approach works very well okay and

00:28:09,420 --> 00:28:16,080
usually so recommendation like the basic

00:28:13,920 --> 00:28:18,360
entity in all of recommendation is this

00:28:16,080 --> 00:28:20,730
matrix here so on this axis you have

00:28:18,360 --> 00:28:24,060
users so each row is a user and these

00:28:20,730 --> 00:28:26,070
are items and every time a user looks at

00:28:24,060 --> 00:28:28,530
a certain item you put a one in year or

00:28:26,070 --> 00:28:30,720
your counted up or whatever okay and

00:28:28,530 --> 00:28:33,090
then from this you can actually compute

00:28:30,720 --> 00:28:36,090
the relationships between like the witch

00:28:33,090 --> 00:28:39,720
if I have this item which item to

00:28:36,090 --> 00:28:41,970
recommend based on the scores here by by

00:28:39,720 --> 00:28:43,830
summing up the information here so you

00:28:41,970 --> 00:28:45,930
take this item and then you go up you

00:28:43,830 --> 00:28:47,970
look at the users who have also looked

00:28:45,930 --> 00:28:49,950
at that item and then you aggregate

00:28:47,970 --> 00:28:52,410
information like which other items has

00:28:49,950 --> 00:28:54,210
looked at so it's like it's like what M

00:28:52,410 --> 00:28:55,560
isn't always says right so people who

00:28:54,210 --> 00:28:57,690
have bought this oh I've also bought

00:28:55,560 --> 00:28:59,190
this they do something much more

00:28:57,690 --> 00:29:01,020
complicated in the back but if you want

00:28:59,190 --> 00:29:03,360
to implement it then this is its is this

00:29:01,020 --> 00:29:06,000
like that's the basic the simplest way

00:29:03,360 --> 00:29:07,980
of doing a recommendation okay normally

00:29:06,000 --> 00:29:09,580
you would do this so first you have like

00:29:07,980 --> 00:29:11,350
all your log data

00:29:09,580 --> 00:29:13,390
you construct this matrix which is very

00:29:11,350 --> 00:29:15,610
big like millions of users thousands of

00:29:13,390 --> 00:29:16,870
items and then basically that you have

00:29:15,610 --> 00:29:19,330
to do one big matrix-matrix

00:29:16,870 --> 00:29:21,130
multiplication and there are ways to do

00:29:19,330 --> 00:29:22,570
this and I do very easily but now the

00:29:21,130 --> 00:29:26,980
interesting thing is you can do the same

00:29:22,570 --> 00:29:29,470
thing again in a streaming fashion using

00:29:26,980 --> 00:29:31,990
exactly these data structures if you

00:29:29,470 --> 00:29:34,360
don't do this if you do it like online

00:29:31,990 --> 00:29:37,780
so you store this is Hollywood you store

00:29:34,360 --> 00:29:40,120
this matrix as a sparse matrix as a user

00:29:37,780 --> 00:29:41,950
item friend and you store this matrix

00:29:40,120 --> 00:29:44,410
again as an item item trend i said as i

00:29:41,950 --> 00:29:46,660
described you know by having the the

00:29:44,410 --> 00:29:49,810
coordinates and the count only in there

00:29:46,660 --> 00:29:52,870
and no no 0 items and then every time a

00:29:49,810 --> 00:29:55,870
conventicle you actually so this user

00:29:52,870 --> 00:29:59,140
looks at this item then you just do the

00:29:55,870 --> 00:30:01,570
update you know along this line using

00:29:59,140 --> 00:30:04,030
the other items this user head looked at

00:30:01,570 --> 00:30:06,910
so it's like instead of you know taking

00:30:04,030 --> 00:30:08,860
a we a month of data and looking how

00:30:06,910 --> 00:30:10,780
often like all the items have been

00:30:08,860 --> 00:30:13,210
really used in relationship to everyone

00:30:10,780 --> 00:30:16,150
you just do this for each user just as

00:30:13,210 --> 00:30:18,430
he comes along right in that way your

00:30:16,150 --> 00:30:20,890
approximate like the real thing again

00:30:18,430 --> 00:30:22,360
the idea of approximation in a way where

00:30:20,890 --> 00:30:26,260
you just have to do like a finite amount

00:30:22,360 --> 00:30:27,850
of computation for each event and the

00:30:26,260 --> 00:30:31,990
good thing is also that like if you

00:30:27,850 --> 00:30:34,870
bring these this time for time scales

00:30:31,990 --> 00:30:37,240
into account and actually this this

00:30:34,870 --> 00:30:40,750
matrix is changing all the time so it's

00:30:37,240 --> 00:30:43,120
really a reflection of which user looked

00:30:40,750 --> 00:30:46,210
at which wich items in the last week and

00:30:43,120 --> 00:30:48,610
if user behavior changes then the whole

00:30:46,210 --> 00:30:50,650
matrix also changes right and the same

00:30:48,610 --> 00:30:54,340
also for this side here so you have a

00:30:50,650 --> 00:30:56,470
system which is not not only you know

00:30:54,340 --> 00:30:58,450
doesn't require you to do these batches

00:30:56,470 --> 00:31:02,290
but which still adapts over time and

00:30:58,450 --> 00:31:03,880
reacts in a very quick point fashion so

00:31:02,290 --> 00:31:05,890
and the nice thing is also that it's

00:31:03,880 --> 00:31:07,570
actually if you want to build this it's

00:31:05,890 --> 00:31:09,850
actually quite easy because it's really

00:31:07,570 --> 00:31:11,440
like when you put a bit of JavaScript on

00:31:09,850 --> 00:31:15,400
the web page then when the web page

00:31:11,440 --> 00:31:18,490
loads it sends the click event via a

00:31:15,400 --> 00:31:20,290
rest call to stream Doyle and then as a

00:31:18,490 --> 00:31:21,620
result already gets the recommendation

00:31:20,290 --> 00:31:23,330
and then you just rent

00:31:21,620 --> 00:31:26,330
recommendation you can also of course

00:31:23,330 --> 00:31:28,160
seed the recommender by sampling past

00:31:26,330 --> 00:31:29,600
clicks so you just you know have if you

00:31:28,160 --> 00:31:30,950
have lots of clicks you just sample

00:31:29,600 --> 00:31:33,920
randomly from that and let it run off

00:31:30,950 --> 00:31:36,800
for time as I said a dramatic adepts

00:31:33,920 --> 00:31:38,930
over time yeah and there are so many

00:31:36,800 --> 00:31:40,640
other things so you because trends are

00:31:38,930 --> 00:31:44,090
very easy to compute yes thank you you

00:31:40,640 --> 00:31:45,590
can also use all kinds of other

00:31:44,090 --> 00:31:47,450
information like what are trending

00:31:45,590 --> 00:31:50,660
trending if you would just usually want

00:31:47,450 --> 00:31:51,860
to make a mixed of like user dependent

00:31:50,660 --> 00:31:53,480
recommendations item-based

00:31:51,860 --> 00:31:54,800
recommendations and trending items you

00:31:53,480 --> 00:31:56,900
can also do that very easily because

00:31:54,800 --> 00:32:04,280
it's all it all are in the end breaks

00:31:56,900 --> 00:32:08,990
down to these data searches okay so yeah

00:32:04,280 --> 00:32:10,760
I hope the one thing you saw was dad it

00:32:08,990 --> 00:32:12,559
doesn't always have to be scaling right

00:32:10,760 --> 00:32:15,080
so there are some applications are not

00:32:12,559 --> 00:32:17,420
saying this works for everything but

00:32:15,080 --> 00:32:19,429
they are important applications i think

00:32:17,420 --> 00:32:21,800
or it's not really necessary that you

00:32:19,429 --> 00:32:23,780
have the exact numbers especially if you

00:32:21,800 --> 00:32:26,320
do like any kind of data analysis which

00:32:23,780 --> 00:32:28,760
which has a large margin of error anyway

00:32:26,320 --> 00:32:30,530
and if you go that direction there are

00:32:28,760 --> 00:32:33,320
algorithms these three mining based

00:32:30,530 --> 00:32:34,580
algorithms which at first look very

00:32:33,320 --> 00:32:38,390
simple because you say they're just

00:32:34,580 --> 00:32:40,280
counting just counting counts but

00:32:38,390 --> 00:32:42,670
actually like in the way I described you

00:32:40,280 --> 00:32:45,800
can use these to build data structures

00:32:42,670 --> 00:32:47,390
to store all kinds of informations and

00:32:45,800 --> 00:32:50,150
correlations between data and you can

00:32:47,390 --> 00:32:53,179
create them quite effectively and solve

00:32:50,150 --> 00:32:56,030
these problems like and especially if

00:32:53,179 --> 00:32:57,740
you're so if you're like as large as in

00:32:56,030 --> 00:33:00,920
the last talk maybe then you have no way

00:32:57,740 --> 00:33:03,050
but really do it like the old way and a

00:33:00,920 --> 00:33:04,490
big cluster but for I think there are

00:33:03,050 --> 00:33:06,800
many many applications where you have

00:33:04,490 --> 00:33:08,120
like a medium-sized website and you want

00:33:06,800 --> 00:33:09,800
to have recommendations recommendations

00:33:08,120 --> 00:33:12,320
would add a lot but you cannot really

00:33:09,800 --> 00:33:14,330
afford you know 2 x 2 to x or render do

00:33:12,320 --> 00:33:16,820
cluster with 10 nodes on there and then

00:33:14,330 --> 00:33:22,780
this kind of thing can we get you a long

00:33:16,820 --> 00:33:22,780
way yeah that's it thank you very much

00:33:29,880 --> 00:33:41,769
so think of time for questions yep and

00:33:38,350 --> 00:33:44,799
does this type of free commander so

00:33:41,769 --> 00:33:48,510
first from self-fulfilling prophecy I

00:33:44,799 --> 00:33:51,610
mean if it recommends something and then

00:33:48,510 --> 00:33:54,309
the recommended the objects will be

00:33:51,610 --> 00:33:57,940
clicked more yeah it's out this kind of

00:33:54,309 --> 00:33:59,740
feedback oh yeah yes but I think they

00:33:57,940 --> 00:34:03,299
like all systems more less suffer from

00:33:59,740 --> 00:34:06,070
that so then it's more the question like

00:34:03,299 --> 00:34:08,139
like if you know that people clicked on

00:34:06,070 --> 00:34:11,020
a recommendation you know you should do

00:34:08,139 --> 00:34:12,490
it in a way that you can distinguish

00:34:11,020 --> 00:34:14,560
between those clicks and other cliques

00:34:12,490 --> 00:34:16,149
and they're not feed those back or

00:34:14,560 --> 00:34:17,379
something right so I have also talked

00:34:16,149 --> 00:34:19,179
about it i know of course you need to

00:34:17,379 --> 00:34:21,780
you know do the a B testing and

00:34:19,179 --> 00:34:33,419
everything to see whether it works yeah

00:34:21,780 --> 00:34:33,419
you can think okay that was one

00:34:41,339 --> 00:34:48,940
so if I understand correctly the way you

00:34:44,760 --> 00:34:51,460
parameterize your system is by defining

00:34:48,940 --> 00:34:57,160
how many hash functions you have right

00:34:51,460 --> 00:35:01,480
to have more precision or not maybe

00:34:57,160 --> 00:35:03,369
think so depends okay now sovaldi for

00:35:01,480 --> 00:35:05,680
this heavy hitters this top k our group

00:35:03,369 --> 00:35:07,510
no I hash function that's just the size

00:35:05,680 --> 00:35:09,339
of the table for these count min

00:35:07,510 --> 00:35:12,210
sketches there to there the number of

00:35:09,339 --> 00:35:15,700
bins and the number of hash functions

00:35:12,210 --> 00:35:17,770
okay so what yeah usually it would

00:35:15,700 --> 00:35:19,510
probably I don't know whether you have

00:35:17,770 --> 00:35:21,010
to look at there's a formula for the

00:35:19,510 --> 00:35:24,190
error and then you have to look what it

00:35:21,010 --> 00:35:25,779
is but there there's a way how with like

00:35:24,190 --> 00:35:28,390
with two hash functions you can actually

00:35:25,779 --> 00:35:30,430
construct an arbitrary number of hash

00:35:28,390 --> 00:35:32,680
functions or of hash hash values you can

00:35:30,430 --> 00:35:36,730
use for that kind of algorithm grats on

00:35:32,680 --> 00:35:39,760
really restriction okay is there a type

00:35:36,730 --> 00:35:42,010
of algorithms that you parameterize your

00:35:39,760 --> 00:35:46,599
system by the time of computation

00:35:42,010 --> 00:35:49,779
instead of the use of memory okay so for

00:35:46,599 --> 00:35:56,039
example well I i can accept two minutes

00:35:49,779 --> 00:35:59,020
well it must be more precise ah yeah

00:35:56,039 --> 00:36:01,089
sorry I mean in a way right as these

00:35:59,020 --> 00:36:04,180
data structures get larger they also get

00:36:01,089 --> 00:36:05,799
a bit slower but I think maybe you're

00:36:04,180 --> 00:36:08,980
thinking more like in a kind of like

00:36:05,799 --> 00:36:10,660
sampling or literature approximation and

00:36:08,980 --> 00:36:18,130
then you stopped earlier just say the

00:36:10,660 --> 00:36:19,450
time is all there may be no yeah I think

00:36:18,130 --> 00:36:21,160
they're I don't already know but I think

00:36:19,450 --> 00:36:22,930
so like that would be the kind of

00:36:21,160 --> 00:36:25,210
algorithms where you start a computation

00:36:22,930 --> 00:36:27,760
and as the computation goes on you're

00:36:25,210 --> 00:36:29,529
already constructing your result right

00:36:27,760 --> 00:36:32,430
and you could stop if you stop earlier

00:36:29,529 --> 00:36:34,869
then actually there will be larger error

00:36:32,430 --> 00:36:37,980
so you said like like yeah as I said

00:36:34,869 --> 00:36:41,440
sampling kind of algorithms so you don't

00:36:37,980 --> 00:36:45,779
you're not using this kind of algorithms

00:36:41,440 --> 00:36:49,390
in in stream drill no oh you know okay

00:36:45,779 --> 00:36:53,140
um here

00:36:49,390 --> 00:36:55,960
um right here I already have a mic hi

00:36:53,140 --> 00:36:57,730
how do you backup your data when you

00:36:55,960 --> 00:36:59,230
have your memory data structures you

00:36:57,730 --> 00:37:01,990
know you make sure your system crashes

00:36:59,230 --> 00:37:05,080
do you think it periodically to disk yes

00:37:01,990 --> 00:37:08,260
yeah so you want it okay sorry in what

00:37:05,080 --> 00:37:10,600
system that could be anything I mean you

00:37:08,260 --> 00:37:13,720
could just put it on disk so the the

00:37:10,600 --> 00:37:15,820
good thing is solid it's like like it's

00:37:13,720 --> 00:37:17,290
it's less than the amount of RAM you

00:37:15,820 --> 00:37:20,440
have so the files are not really that

00:37:17,290 --> 00:37:21,970
big so if you have like 16 or 10

00:37:20,440 --> 00:37:23,500
gigabytes of RAM because of all the

00:37:21,970 --> 00:37:25,720
indices and you wouldn't store this you

00:37:23,500 --> 00:37:27,280
just or the raw data then maybe have

00:37:25,720 --> 00:37:29,740
like three gigabytes of data you can

00:37:27,280 --> 00:37:32,050
store and then you would do this like I

00:37:29,740 --> 00:37:35,080
don't know once an hour and then go back

00:37:32,050 --> 00:37:36,970
from there all you do like like a master

00:37:35,080 --> 00:37:38,650
like you have to running in parallel and

00:37:36,970 --> 00:37:41,310
one phase then the other takes over and

00:37:38,650 --> 00:37:47,830
then you can you know that kind of thing

00:37:41,310 --> 00:37:51,130
thank you recall yeah so could you tell

00:37:47,830 --> 00:37:53,050
us more on how you right here hi hi

00:37:51,130 --> 00:37:55,930
could you tell us more on how you

00:37:53,050 --> 00:37:58,630
propagate these changes in the user item

00:37:55,930 --> 00:38:01,870
matrix to the item item matrix in a

00:37:58,630 --> 00:38:03,400
localized manner like only partial

00:38:01,870 --> 00:38:08,650
changes to the idea meet with the

00:38:03,400 --> 00:38:11,950
recommendation yes I think actually it's

00:38:08,650 --> 00:38:13,600
likely as I described so what I mean

00:38:11,950 --> 00:38:16,780
what like in the weird thing you also be

00:38:13,600 --> 00:38:19,240
do a bit of normalization so that your

00:38:16,780 --> 00:38:21,970
but it's basically like this so when

00:38:19,240 --> 00:38:24,070
when there is an event you sort of look

00:38:21,970 --> 00:38:25,870
at other users who have also looked at

00:38:24,070 --> 00:38:28,240
that and then you extract like the

00:38:25,870 --> 00:38:31,930
things they also looked at and then you

00:38:28,240 --> 00:38:34,570
just add them up is it either you can

00:38:31,930 --> 00:38:38,970
talk probably tour after the talk yeah

00:38:34,570 --> 00:38:38,970
we're gonna measure easier yeah thanks

00:38:40,390 --> 00:38:44,870
thanks for the hike actually yeah I

00:38:43,160 --> 00:38:47,960
remember that you referred to the good

00:38:44,870 --> 00:38:49,850
recommendations as up to date and the

00:38:47,960 --> 00:38:52,400
ones that are already made based on the

00:38:49,850 --> 00:38:54,020
recent activities Android users but I

00:38:52,400 --> 00:38:56,780
you know it's a bit complicated because

00:38:54,020 --> 00:38:59,600
sometimes I am just looking for coffee

00:38:56,780 --> 00:39:02,330
makers but just for fun see something on

00:38:59,600 --> 00:39:04,370
top of the page and about juicers or

00:39:02,330 --> 00:39:06,080
whatever and just go there and bro

00:39:04,370 --> 00:39:09,170
something but i am not really interested

00:39:06,080 --> 00:39:11,200
in juicers and i don't need them so you

00:39:09,170 --> 00:39:13,640
know it's a bit you know like a

00:39:11,200 --> 00:39:16,540
complicated things to see what what we

00:39:13,640 --> 00:39:19,100
have to recommend to users and also if

00:39:16,540 --> 00:39:21,860
even you you make that based on the

00:39:19,100 --> 00:39:24,170
recent activities but they really want

00:39:21,860 --> 00:39:26,330
something else like some new things on

00:39:24,170 --> 00:39:28,760
their past behaviors you know is i mean

00:39:26,330 --> 00:39:31,400
how you can figure it out you know your

00:39:28,760 --> 00:39:33,020
yeah that's always always difficult I

00:39:31,400 --> 00:39:34,430
mean one thing you could do here you

00:39:33,020 --> 00:39:36,860
could have the recommendation running

00:39:34,430 --> 00:39:40,100
for different time scales right and then

00:39:36,860 --> 00:39:42,470
you say like based on your behavior over

00:39:40,100 --> 00:39:44,810
the last week this is the thing which is

00:39:42,470 --> 00:39:47,180
probably interesting and already like

00:39:44,810 --> 00:39:48,770
the user profiling so you would like if

00:39:47,180 --> 00:39:50,180
you're looking at the juicer you would

00:39:48,770 --> 00:39:52,100
see okay this is something you never

00:39:50,180 --> 00:39:55,130
looked at but right now there's a lot of

00:39:52,100 --> 00:39:56,840
activity there but if you stop then it's

00:39:55,130 --> 00:39:59,510
also something which decays very quickly

00:39:56,840 --> 00:40:01,910
so it disappears very quickly okay yes

00:39:59,510 --> 00:40:03,710
it was it like often it's like so once

00:40:01,910 --> 00:40:05,660
it's in the database then you're stuck

00:40:03,710 --> 00:40:06,800
with it for a week or so yeah but I mean

00:40:05,660 --> 00:40:08,450
in general so if you really wanted a

00:40:06,800 --> 00:40:09,830
recommendation then I think you also

00:40:08,450 --> 00:40:11,660
have to look at the items themselves

00:40:09,830 --> 00:40:15,140
because there are some items you just

00:40:11,660 --> 00:40:17,510
like by 12 a year or or less and some of

00:40:15,140 --> 00:40:20,120
you by very often and then I think that

00:40:17,510 --> 00:40:22,190
way you can you can try to make it more

00:40:20,120 --> 00:40:24,170
high quality recommendations in yes of

00:40:22,190 --> 00:40:26,060
course here yet so it's about more like

00:40:24,170 --> 00:40:27,740
dimensions that we are you have to take

00:40:26,060 --> 00:40:30,410
you to an interac on because bull calm

00:40:27,740 --> 00:40:32,510
also refers to good recommendations like

00:40:30,410 --> 00:40:34,790
a most accurate owns or something like

00:40:32,510 --> 00:40:37,970
that so I think we really need to fix

00:40:34,790 --> 00:40:41,540
you know what we need from that in such

00:40:37,970 --> 00:40:45,770
context or website resistance oh yeah

00:40:41,540 --> 00:40:50,470
that visit you Thanks okay all right I

00:40:45,770 --> 00:40:50,470
think one more okay

00:40:52,519 --> 00:40:59,099
hi in the last presentation he talked

00:40:56,789 --> 00:41:00,930
about multi-factor recommendations you

00:40:59,099 --> 00:41:03,119
know that different kind of events would

00:41:00,930 --> 00:41:05,999
affect the user profiles differently

00:41:03,119 --> 00:41:09,119
here since it's your share and item on

00:41:05,999 --> 00:41:11,700
facebook or if you oh yeah look into the

00:41:09,119 --> 00:41:13,890
product for a long time if you buy it if

00:41:11,700 --> 00:41:15,559
you just you know so you're different

00:41:13,890 --> 00:41:17,579
kind of events that effects the

00:41:15,559 --> 00:41:20,190
underlying recommendation model

00:41:17,579 --> 00:41:22,410
differently would with your approach

00:41:20,190 --> 00:41:26,599
Harriet streams will be able to handle

00:41:22,410 --> 00:41:30,509
multi-factor rooms yeah I mean you can

00:41:26,599 --> 00:41:34,229
no I mean you can for example you could

00:41:30,509 --> 00:41:36,089
have a different record so I'm you can

00:41:34,229 --> 00:41:38,130
have different scores like how important

00:41:36,089 --> 00:41:40,109
it is that you did something but you

00:41:38,130 --> 00:41:42,089
could also have like to recommend us

00:41:40,109 --> 00:41:44,069
running one looking at social activities

00:41:42,089 --> 00:41:45,509
and the other looking at that and then

00:41:44,069 --> 00:41:49,529
in the end you do a combination of those

00:41:45,509 --> 00:41:52,170
things so yeah so you could write say

00:41:49,529 --> 00:41:53,549
okay if there is something like based on

00:41:52,170 --> 00:41:55,799
your social behavior something I would

00:41:53,549 --> 00:41:57,779
really recommend and I do that and I

00:41:55,799 --> 00:41:59,999
also do a mix of like the thing I think

00:41:57,779 --> 00:42:02,579
you would want to have based on what you

00:41:59,999 --> 00:42:04,799
clicked on on our website so they are

00:42:02,579 --> 00:42:06,449
infinite ways of I mean this is not

00:42:04,799 --> 00:42:08,249
something that's in there so the signals

00:42:06,449 --> 00:42:09,630
in there is this basic thing I had here

00:42:08,249 --> 00:42:12,539
so if you would put it in production

00:42:09,630 --> 00:42:14,130
than you yeah I would have to think

00:42:12,539 --> 00:42:15,869
about different ways of using it and

00:42:14,130 --> 00:42:20,279
then integrating it in the in the end

00:42:15,869 --> 00:42:22,489
thank you thanks okay yeah thank you

00:42:20,279 --> 00:42:22,489
very much

00:42:25,000 --> 00:42:27,060

YouTube URL: https://www.youtube.com/watch?v=pYRdK8O2GMs


