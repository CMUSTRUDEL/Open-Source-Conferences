Title: Berlin Buzzwords 2014: Stephan Ewen - Big Data looks tiny from Stratosphere #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Stratosphere (http://stratosphere.eu) is a next-generation Apache licensed platform for Big Data Analysis. Stratosphere combines the flexibility and scalability of MapReduce-like systems with a high-performance runtime and automatic optimization technology inspired by MPP databases. 

Stratosphere offers fluent APIs in Java and Scala that extend the MapReduce model with arbitrarily long programs and more operators such as join, cogroup, cross, and iterate. Stratosphere’s runtime uses main memory efficiently, and gradually degrades to disk with good performance under memory pressure. Stratosphere’ cost-based optimizer automatically picks the best execution strategy for programs taking into account data and hardware characteristics. 

Finally, Stratosphere features end-to-end first class support for iterative programs, achieving similar performance to Giraph while still being a general (not graph-specific) system. Stratosphere is compatible with the Hadoop ecosystem, runs on top of YARN, and can use HDFS for data storage. Stratosphere is developed by a growing developer community, and is currently witnessing its first commercial installations and use cases.

Read more:
https://2014.berlinbuzzwords.de/session/big-data-looks-tiny-stratosphere

About Stephan Ewen:
https://2014.berlinbuzzwords.de/user/209/event/1 

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,870 --> 00:00:12,420
let's get started the first talk in the

00:00:09,030 --> 00:00:16,080
afternoon session here I'm going to talk

00:00:12,420 --> 00:00:17,630
about stratosphere or if link I'm going

00:00:16,080 --> 00:00:20,730
to explain the name duality in a second

00:00:17,630 --> 00:00:22,050
I'm Stefan and I'm i hope i can tell you

00:00:20,730 --> 00:00:26,910
for him to sting things in the next

00:00:22,050 --> 00:00:31,169
minutes okay so yeah as I said I am I'm

00:00:26,910 --> 00:00:33,000
I'm Stefan I am currently a yeah PhD

00:00:31,169 --> 00:00:35,190
student at the hue from Berlin actually

00:00:33,000 --> 00:00:38,579
University of Technology last days so

00:00:35,190 --> 00:00:42,359
one of the guys was basically waiting

00:00:38,579 --> 00:00:44,480
for the professor's to find a a day

00:00:42,359 --> 00:00:47,640
where they can actually hold the defense

00:00:44,480 --> 00:00:50,039
can take longer than you might think I'm

00:00:47,640 --> 00:00:52,170
one of the people that has been involved

00:00:50,039 --> 00:00:53,609
with a stratosphere project I'm about

00:00:52,170 --> 00:00:55,350
which I'm going to talk from the from

00:00:53,609 --> 00:01:00,269
the very beginning I'm someone oughtta

00:00:55,350 --> 00:01:02,429
of the core developers okay and as you

00:01:00,269 --> 00:01:05,580
may have seen on the on the title slide

00:01:02,429 --> 00:01:07,470
actually it the the talk is announced as

00:01:05,580 --> 00:01:10,350
stratosphere on the title slide I wrote

00:01:07,470 --> 00:01:13,320
stratosphere / flank so by the time

00:01:10,350 --> 00:01:15,090
actually we we handed in the talk this

00:01:13,320 --> 00:01:17,070
was still the project the project

00:01:15,090 --> 00:01:19,410
stratosphere which started as a project

00:01:17,070 --> 00:01:22,800
here in dublin area in the mean time

00:01:19,410 --> 00:01:25,440
between handing in the torque and this

00:01:22,800 --> 00:01:28,290
event happening stratosphere is actually

00:01:25,440 --> 00:01:29,520
been accepted at the Apache Incubator so

00:01:28,290 --> 00:01:31,170
it's moving into the Apache Incubator

00:01:29,520 --> 00:01:33,060
it's actually taken on a new name

00:01:31,170 --> 00:01:36,750
because there's already a project called

00:01:33,060 --> 00:01:38,280
Apache Stratos so we um we voted on a

00:01:36,750 --> 00:01:42,060
new name and the new name is going to be

00:01:38,280 --> 00:01:43,530
flink so I'm going to say stratosphere

00:01:42,060 --> 00:01:45,540
for the remainder of the talk because

00:01:43,530 --> 00:01:47,160
this really describes the outcome of

00:01:45,540 --> 00:01:49,910
stratosphere which is going to be the

00:01:47,160 --> 00:01:52,260
starting point of the flink project but

00:01:49,910 --> 00:01:54,720
yeah you can think of stratosphere and

00:01:52,260 --> 00:01:57,870
flink being synonyms for the sake of

00:01:54,720 --> 00:01:59,970
this talk and um courtesy of Alan

00:01:57,870 --> 00:02:03,300
Freedman is the first draft of the of

00:01:59,970 --> 00:02:05,240
the mascot of of the flink project so

00:02:03,300 --> 00:02:08,099
for those of you are not German speakers

00:02:05,240 --> 00:02:12,120
flink is actually I it's a German word

00:02:08,099 --> 00:02:17,240
and means agile nimble so that's why we

00:02:12,120 --> 00:02:19,380
have the are the squirrel sa as a mascot

00:02:17,240 --> 00:02:20,790
ok so much for the introduction

00:02:19,380 --> 00:02:23,900
so whenever I say stratosphere you can

00:02:20,790 --> 00:02:27,450
just in your head think of link as well

00:02:23,900 --> 00:02:31,260
okay so what is what is status fear all

00:02:27,450 --> 00:02:34,740
about it's um it's a fairly new open

00:02:31,260 --> 00:02:36,480
source project as such so a few of you

00:02:34,740 --> 00:02:39,870
may have heard about it a few of you may

00:02:36,480 --> 00:02:42,690
not to sum it up in a few words it's a

00:02:39,870 --> 00:02:44,850
it's an analytics platform so it's in

00:02:42,690 --> 00:02:50,220
the in the space of systems like Hadoop

00:02:44,850 --> 00:02:52,230
and apache spark it is it is not built

00:02:50,220 --> 00:02:53,910
on top of a depend on top of spark so

00:02:52,230 --> 00:02:58,830
it's it's it's own system it's it's own

00:02:53,910 --> 00:03:01,440
stack it it runs very nicely on top of

00:02:58,830 --> 00:03:05,490
HDFS and yarn so it integrates very well

00:03:01,440 --> 00:03:09,540
with the with the Hadoop stack and it

00:03:05,490 --> 00:03:12,270
has its it has its focus on on a variety

00:03:09,540 --> 00:03:14,040
of use cases and especially on on ease

00:03:12,270 --> 00:03:19,530
of programming providing high level

00:03:14,040 --> 00:03:20,880
abstractions for um for programmers I'm

00:03:19,530 --> 00:03:23,040
to give you a little idea where this

00:03:20,880 --> 00:03:24,840
project is standing it's it started

00:03:23,040 --> 00:03:26,790
quite a while ago I say as a research

00:03:24,840 --> 00:03:30,600
project here in the berlin area shared

00:03:26,790 --> 00:03:33,020
between different universities um when

00:03:30,600 --> 00:03:35,190
the e when the research project was over

00:03:33,020 --> 00:03:37,710
we thought that the result was actually

00:03:35,190 --> 00:03:39,870
kind of nice and we got very good

00:03:37,710 --> 00:03:42,360
feedback on it so we we kind of pushed

00:03:39,870 --> 00:03:44,310
it to become an open source project so

00:03:42,360 --> 00:03:47,550
that's where it is right now it's um it

00:03:44,310 --> 00:03:49,260
is at this moment the the codebase that

00:03:47,550 --> 00:03:51,959
you can get is still hosted on github it

00:03:49,260 --> 00:03:53,550
Sam it's an open source project there it

00:03:51,959 --> 00:03:54,870
is as I said moving to the apache

00:03:53,550 --> 00:03:58,170
software foundation under the names

00:03:54,870 --> 00:04:00,060
linked we are in the in the fifth

00:03:58,170 --> 00:04:01,470
release so three threes is actually

00:04:00,060 --> 00:04:03,840
coming coming out right about now

00:04:01,470 --> 00:04:09,720
there's the second release candidate is

00:04:03,840 --> 00:04:11,670
I think online since the weekend and we

00:04:09,720 --> 00:04:13,860
begin pushing it into the open source

00:04:11,670 --> 00:04:15,330
about half a year ago and it has

00:04:13,860 --> 00:04:17,100
actually gotten quite a bit of adoption

00:04:15,330 --> 00:04:19,080
or adoption right now so we have a total

00:04:17,100 --> 00:04:22,200
of thirty-eight contributors already not

00:04:19,080 --> 00:04:24,600
all of them from Berlin actually so we

00:04:22,200 --> 00:04:27,060
also have people working on on projects

00:04:24,600 --> 00:04:32,460
related to a stratosphere in various

00:04:27,060 --> 00:04:33,210
places and yeah we also we're seeing

00:04:32,460 --> 00:04:36,449
first companies

00:04:33,210 --> 00:04:38,250
trying it out for four applications okay

00:04:36,449 --> 00:04:40,919
so much as the background so what is

00:04:38,250 --> 00:04:45,479
what is stratosphere really to you as

00:04:40,919 --> 00:04:46,919
somebody who wants to try it out so what

00:04:45,479 --> 00:04:48,720
is greatest view and why would actually

00:04:46,919 --> 00:04:50,550
the ecosystem need something else then I

00:04:48,720 --> 00:04:55,110
do Ben spark and there's so many other

00:04:50,550 --> 00:04:57,780
systems drill and so when we started

00:04:55,110 --> 00:05:00,960
stratosphere we thought they're kind of

00:04:57,780 --> 00:05:04,500
these two spaces there's the MapReduce

00:05:00,960 --> 00:05:08,070
style space with projects of course like

00:05:04,500 --> 00:05:10,229
Hadoop MapReduce and apps Park getting

00:05:08,070 --> 00:05:12,509
more more traction also fitting in my

00:05:10,229 --> 00:05:16,580
opinion more in dead space and there's

00:05:12,509 --> 00:05:18,870
the this the area of of databases

00:05:16,580 --> 00:05:20,460
initially mostly relational databases

00:05:18,870 --> 00:05:24,409
but also databases that go beyond that

00:05:20,460 --> 00:05:27,690
like Apache drill and there is this

00:05:24,409 --> 00:05:31,560
spectrum between those um between those

00:05:27,690 --> 00:05:33,630
different types of technologies they

00:05:31,560 --> 00:05:36,659
they each have their very individual

00:05:33,630 --> 00:05:38,430
strengths so the the MapReduce space has

00:05:36,659 --> 00:05:40,229
focused heavily on scalability on

00:05:38,430 --> 00:05:44,669
user-defined functions as first-class

00:05:40,229 --> 00:05:49,130
citizens are very complex data types all

00:05:44,669 --> 00:05:52,530
of that the database area has focused I

00:05:49,130 --> 00:05:54,509
since the 60s or 70s actually on are

00:05:52,530 --> 00:05:56,940
very much on declare activity so you

00:05:54,509 --> 00:05:59,190
you've right a very concise query you

00:05:56,940 --> 00:06:01,349
delegate a lot of of the decisions to

00:05:59,190 --> 00:06:03,780
the system how how should it be executed

00:06:01,349 --> 00:06:06,030
it comes with this optimizer component

00:06:03,780 --> 00:06:08,820
it has for what it does a fairly

00:06:06,030 --> 00:06:10,349
efficient runtime and we kind of thought

00:06:08,820 --> 00:06:11,699
there's there's a nice sweet spot

00:06:10,349 --> 00:06:13,560
between those two systems where you can

00:06:11,699 --> 00:06:14,759
combine some of the characteristics and

00:06:13,560 --> 00:06:16,620
this is where stratosphere actually sits

00:06:14,759 --> 00:06:17,880
it sits between the met reduced our

00:06:16,620 --> 00:06:20,039
technologies and the database

00:06:17,880 --> 00:06:21,930
technologies and it adds also it's very

00:06:20,039 --> 00:06:23,940
own twist to it so we've added

00:06:21,930 --> 00:06:26,219
functionality especially for iterative

00:06:23,940 --> 00:06:27,900
algorithms which are very important if

00:06:26,219 --> 00:06:30,810
you are looking into applications like

00:06:27,900 --> 00:06:34,849
machine learning graph analysis I'm it

00:06:30,810 --> 00:06:39,330
is fairly complex data flow programs and

00:06:34,849 --> 00:06:41,400
it's adding it's adding um the a bit of

00:06:39,330 --> 00:06:44,089
that decorative Attilan magic sauce that

00:06:41,400 --> 00:06:48,320
is known from databases for

00:06:44,089 --> 00:06:48,320
non-relational kind of programs

00:06:49,040 --> 00:06:54,980
so this is what it what it looks like if

00:06:51,990 --> 00:06:58,710
you look at it from 20,000 feet or so

00:06:54,980 --> 00:07:01,650
it's a yeah it's a stack it sits on top

00:06:58,710 --> 00:07:05,460
of a of various storage and unclassified

00:07:01,650 --> 00:07:07,110
management solutions the storage being

00:07:05,460 --> 00:07:09,420
serviced feed self does not store data

00:07:07,110 --> 00:07:11,160
so it it reads files structured files

00:07:09,420 --> 00:07:14,010
unstructured files it draws data from

00:07:11,160 --> 00:07:18,480
databases in various formats or local

00:07:14,010 --> 00:07:20,250
files HDFS cloud file systems and it can

00:07:18,480 --> 00:07:21,810
be deployed on two clusters using our

00:07:20,250 --> 00:07:24,390
frameworks like yarn or you can directly

00:07:21,810 --> 00:07:26,670
install it on the bare metal and on top

00:07:24,390 --> 00:07:30,000
of that sit sit status with runtime

00:07:26,670 --> 00:07:32,160
optimizer and a series of api's and I

00:07:30,000 --> 00:07:35,070
think this picture kind of tells quite a

00:07:32,160 --> 00:07:37,680
bit about the system especially the is

00:07:35,070 --> 00:07:39,480
very prominent component here which is

00:07:37,680 --> 00:07:41,460
which is very much inspired by the

00:07:39,480 --> 00:07:44,190
architecture of relational databases so

00:07:41,460 --> 00:07:46,320
so this view has a has a common runtime

00:07:44,190 --> 00:07:48,240
and a common optimizer for multiple

00:07:46,320 --> 00:07:50,000
programming IP is their record oriented

00:07:48,240 --> 00:07:54,270
programming api's in Java and Scala

00:07:50,000 --> 00:07:57,720
graph oriented ap is a scripting

00:07:54,270 --> 00:07:59,310
language for for Jason language all of

00:07:57,720 --> 00:08:02,040
these go through the same optimized at

00:07:59,310 --> 00:08:03,660
the same runtime and there you can some

00:08:02,040 --> 00:08:09,060
of them you can actually even mix and

00:08:03,660 --> 00:08:10,830
match in a very nice way so what what

00:08:09,060 --> 00:08:12,570
what makes stratosphere kind of unique

00:08:10,830 --> 00:08:17,280
in that in that space of Big Data

00:08:12,570 --> 00:08:19,710
technology there I'll talk about the

00:08:17,280 --> 00:08:21,450
individual appoints later in more detail

00:08:19,710 --> 00:08:24,420
but I think you can sum it up in and

00:08:21,450 --> 00:08:25,980
basically for four broad categories so

00:08:24,420 --> 00:08:28,980
very strong focuses on easy to use

00:08:25,980 --> 00:08:31,200
developer API sin different languages as

00:08:28,980 --> 00:08:32,550
I said javis Carla graphs already there

00:08:31,200 --> 00:08:34,080
there's Python and sequel under

00:08:32,550 --> 00:08:37,470
development I think actually de python

00:08:34,080 --> 00:08:39,810
api is in beta status right now with

00:08:37,470 --> 00:08:43,470
these easy-to-use develop api's kind of

00:08:39,810 --> 00:08:45,870
handed hand comes automatic optimization

00:08:43,470 --> 00:08:48,200
so it is something that comes more from

00:08:45,870 --> 00:08:50,820
the space of the relational databases um

00:08:48,200 --> 00:08:52,520
what what strategy does it it it

00:08:50,820 --> 00:08:55,080
implements an optimized that it is an

00:08:52,520 --> 00:08:57,420
inspired although not the same as an

00:08:55,080 --> 00:08:58,720
optimizer in relational databases so you

00:08:57,420 --> 00:09:02,910
can write programs and

00:08:58,720 --> 00:09:05,439
not worry about many of the individual

00:09:02,910 --> 00:09:07,600
low-level decisions that you need to

00:09:05,439 --> 00:09:10,180
need to make when you write something

00:09:07,600 --> 00:09:11,589
for example in Hadoop burn spark and

00:09:10,180 --> 00:09:16,000
just delegate it to the system and say

00:09:11,589 --> 00:09:17,649
okay figure that out for me um it comes

00:09:16,000 --> 00:09:20,819
with its own runtimes it's not sitting

00:09:17,649 --> 00:09:23,649
on my produce a run time that is running

00:09:20,819 --> 00:09:25,899
computation in memory as far as possible

00:09:23,649 --> 00:09:27,670
it's going out of core when necessary it

00:09:25,899 --> 00:09:30,339
that is fairly unique as well I think

00:09:27,670 --> 00:09:32,230
streams data between operations even

00:09:30,339 --> 00:09:33,850
though it has a batch API on top of it

00:09:32,230 --> 00:09:36,370
so that might sound a little weird

00:09:33,850 --> 00:09:38,829
initially but it does actually make

00:09:36,370 --> 00:09:41,500
sense for a lot of operations because it

00:09:38,829 --> 00:09:43,139
implies that if you have multiple steps

00:09:41,500 --> 00:09:45,339
after another you can actually produce

00:09:43,139 --> 00:09:46,689
pretty big intermediate results which

00:09:45,339 --> 00:09:50,639
you'll never have to materialize in

00:09:46,689 --> 00:09:54,129
store and yeah I mentioned that earlier

00:09:50,639 --> 00:09:56,680
there's there's a very deep support for

00:09:54,129 --> 00:09:59,620
iterative algorithms um they're very

00:09:56,680 --> 00:10:01,990
deeply embedded both in the api's and in

00:09:59,620 --> 00:10:04,089
the runtime and then I'm going to show

00:10:01,990 --> 00:10:05,910
you a few examples later of or that

00:10:04,089 --> 00:10:11,589
actually means to the program around our

00:10:05,910 --> 00:10:14,709
track securing iterative algorithms okay

00:10:11,589 --> 00:10:17,319
so from the high level view of the

00:10:14,709 --> 00:10:18,850
system what is it you look like if you

00:10:17,319 --> 00:10:22,059
actually write your first example

00:10:18,850 --> 00:10:23,800
program um so here's the use the in

00:10:22,059 --> 00:10:25,930
famous word count which is kinda there

00:10:23,800 --> 00:10:28,329
you know it's kind of the hello world I

00:10:25,930 --> 00:10:30,490
think of the of the parallel analytic

00:10:28,329 --> 00:10:34,870
space so this is what it looks like and

00:10:30,490 --> 00:10:37,120
so this use Java API it's the EAP is

00:10:34,870 --> 00:10:40,509
both in Java and Scala are centered

00:10:37,120 --> 00:10:43,029
around data sets that you create and on

00:10:40,509 --> 00:10:46,509
which you apply arm yeah transformations

00:10:43,029 --> 00:10:50,050
functions like mapping over them

00:10:46,509 --> 00:10:52,329
grouping aggregating and so on you right

00:10:50,050 --> 00:10:55,089
you right your these transformations

00:10:52,329 --> 00:10:57,660
just as Java functions and unlike unlike

00:10:55,089 --> 00:11:00,910
a dupe you can really use you can use

00:10:57,660 --> 00:11:02,889
just plain java objects the basic types

00:11:00,910 --> 00:11:05,500
you can also use your own your own

00:11:02,889 --> 00:11:07,929
classes and so on there's a this kind of

00:11:05,500 --> 00:11:10,149
a mixture of of analysis of the types

00:11:07,929 --> 00:11:11,590
reflection analysis and general purpose

00:11:10,149 --> 00:11:16,000
utilization going on to make

00:11:11,590 --> 00:11:17,950
at work okay so this is where the this

00:11:16,000 --> 00:11:19,810
is for the Java API I think this gives

00:11:17,950 --> 00:11:24,040
you a rough impression of what it looks

00:11:19,810 --> 00:11:25,540
like yeah data sets your types tuples

00:11:24,040 --> 00:11:27,250
are kind of built in as a special

00:11:25,540 --> 00:11:28,810
concept in the hadoop world everything

00:11:27,250 --> 00:11:31,450
is around key value pairs but we thought

00:11:28,810 --> 00:11:33,760
for more um more complicated program

00:11:31,450 --> 00:11:36,160
this actually gets a little this gets a

00:11:33,760 --> 00:11:37,690
little ugly after a while and for those

00:11:36,160 --> 00:11:39,270
of you who are scholar program is

00:11:37,690 --> 00:11:44,050
they'll actually noticed our concept

00:11:39,270 --> 00:11:45,370
it's um is fairly powerful so in Scala

00:11:44,050 --> 00:11:47,560
of course everything looks even a little

00:11:45,370 --> 00:11:50,710
more beautiful if if you're a fan of

00:11:47,560 --> 00:11:52,180
Scala this is the same program um it's

00:11:50,710 --> 00:11:53,860
not one hundred percent the same but

00:11:52,180 --> 00:11:55,390
roughly the same program written in

00:11:53,860 --> 00:11:57,100
Scala so it's the same thing you start

00:11:55,390 --> 00:11:59,230
with uh with the data set created from a

00:11:57,100 --> 00:12:02,650
text file should form it here with an

00:11:59,230 --> 00:12:06,280
operation that splits the lines in this

00:12:02,650 --> 00:12:08,530
case you just group by the entire the

00:12:06,280 --> 00:12:10,450
entire record so this is what this

00:12:08,530 --> 00:12:12,310
lambda he expresses given that you get a

00:12:10,450 --> 00:12:18,580
word which is group on the entire world

00:12:12,310 --> 00:12:20,800
and then then count where it occurs so

00:12:18,580 --> 00:12:22,810
far so good that should look for those

00:12:20,800 --> 00:12:26,250
of you who have looked into frameworks

00:12:22,810 --> 00:12:33,250
like let's say spark or crunch or so

00:12:26,250 --> 00:12:36,010
somewhat familiar um the the whole api's

00:12:33,250 --> 00:12:38,710
are designed to round a rich set of

00:12:36,010 --> 00:12:41,350
operators so the runtime if you wish

00:12:38,710 --> 00:12:43,510
knows knows a few operators MapReduce

00:12:41,350 --> 00:12:47,490
join core group Union cross iterating

00:12:43,510 --> 00:12:52,570
iterate delta i think the the first six

00:12:47,490 --> 00:12:54,760
should be you should recognize probably

00:12:52,570 --> 00:12:57,280
from if you work with cascading or so

00:12:54,760 --> 00:12:58,870
before or actually spark iterate an

00:12:57,280 --> 00:13:00,700
internet delta i might seem a little

00:12:58,870 --> 00:13:03,430
special and i'm going to talk about them

00:13:00,700 --> 00:13:05,110
in a bit this is what kind of the course

00:13:03,430 --> 00:13:06,490
the system knows in the api's there's a

00:13:05,110 --> 00:13:08,140
lot of derived operators as well

00:13:06,490 --> 00:13:10,810
filtering flat mapping project

00:13:08,140 --> 00:13:13,120
aggregating duplicate elimination server

00:13:10,810 --> 00:13:14,920
forms of joints and also derived

00:13:13,120 --> 00:13:17,070
operators which are kind of compositions

00:13:14,920 --> 00:13:19,480
of other operators for example arm

00:13:17,070 --> 00:13:21,310
vertex centric graph computations which

00:13:19,480 --> 00:13:22,140
are a combination of iterations and co

00:13:21,310 --> 00:13:28,410
group and

00:13:22,140 --> 00:13:29,790
joints so what what does the system do

00:13:28,410 --> 00:13:31,530
internally once you write such a program

00:13:29,790 --> 00:13:33,330
so it's not breaking it down into a

00:13:31,530 --> 00:13:35,280
series of MapReduce jobs it's not just

00:13:33,330 --> 00:13:37,050
saying okay let me execute one step

00:13:35,280 --> 00:13:39,720
materialize the result may execute the

00:13:37,050 --> 00:13:42,000
other step the next step what it's

00:13:39,720 --> 00:13:45,810
really doing is it's um it's taking the

00:13:42,000 --> 00:13:48,270
entire program constructing a data flow

00:13:45,810 --> 00:13:50,580
graph out of it so this operation

00:13:48,270 --> 00:13:52,050
streaming data from here and streaming

00:13:50,580 --> 00:13:53,940
it into the reduced operation to reduce

00:13:52,050 --> 00:13:55,620
operation gathering it for hashing

00:13:53,940 --> 00:14:01,620
sorting and streaming it into the join

00:13:55,620 --> 00:14:04,170
and so this is this is really a pipeline

00:14:01,620 --> 00:14:07,140
a dataflow with multiple operators

00:14:04,170 --> 00:14:08,970
possibly executing at the same time that

00:14:07,140 --> 00:14:11,910
is online if you execute the program and

00:14:08,970 --> 00:14:14,760
it has the ability to also take take

00:14:11,910 --> 00:14:16,350
data from from a later step and in a

00:14:14,760 --> 00:14:18,840
controlled fashion feed it back to an

00:14:16,350 --> 00:14:21,000
earlier stage there by closing a loop

00:14:18,840 --> 00:14:28,560
and enabling to do a charity for

00:14:21,000 --> 00:14:31,050
recursive computations okay um so much

00:14:28,560 --> 00:14:33,720
as a as it as a teaser from from that

00:14:31,050 --> 00:14:37,950
level let's actually walk through some

00:14:33,720 --> 00:14:39,770
cases where um where the where what we

00:14:37,950 --> 00:14:42,300
added to the system in terms of

00:14:39,770 --> 00:14:44,490
optimization and um and ease of use

00:14:42,300 --> 00:14:47,880
actually becomes a little more a little

00:14:44,490 --> 00:14:52,010
more graspable so um here's a variant

00:14:47,880 --> 00:14:56,480
that that that runs there a program

00:14:52,010 --> 00:14:59,640
roughly sketched by this but this yeah

00:14:56,480 --> 00:15:01,770
but this abstract algebra tree here it's

00:14:59,640 --> 00:15:03,690
a it's a two joints between a large and

00:15:01,770 --> 00:15:06,330
a medium sized table with a small table

00:15:03,690 --> 00:15:09,180
and an aggregation afterwards something

00:15:06,330 --> 00:15:10,950
that that does occur as I don't know

00:15:09,180 --> 00:15:14,730
possibly intermediate steps and

00:15:10,950 --> 00:15:16,410
algorithm or pre-processing steps so in

00:15:14,730 --> 00:15:18,720
what you would do in stratosphere is you

00:15:16,410 --> 00:15:21,300
would you'd write the program starting

00:15:18,720 --> 00:15:23,730
to read the data in this case from csv

00:15:21,300 --> 00:15:26,220
files which which would parse the data

00:15:23,730 --> 00:15:29,190
into into some topple data structure and

00:15:26,220 --> 00:15:32,100
then they would do two joints between

00:15:29,190 --> 00:15:34,050
the large in the medium table the syntax

00:15:32,100 --> 00:15:35,939
here if you have toppled data you can

00:15:34,050 --> 00:15:37,499
just express okay which fields of the

00:15:35,939 --> 00:15:39,149
able to use if you have don't have

00:15:37,499 --> 00:15:40,829
toppled data you can throw in the lambda

00:15:39,149 --> 00:15:44,519
that says okay here's how you get

00:15:40,829 --> 00:15:47,279
actually the key from the data type so

00:15:44,519 --> 00:15:49,379
you join these two tables a runner and

00:15:47,279 --> 00:15:52,199
one day I grew by an aggregation over it

00:15:49,379 --> 00:15:55,409
in the end and then you submit it to the

00:15:52,199 --> 00:16:01,199
system and what does the system do with

00:15:55,409 --> 00:16:05,179
it there for those of you have actually

00:16:01,199 --> 00:16:08,099
spent some time with with Hadoop in hive

00:16:05,179 --> 00:16:10,619
they might know that they're especially

00:16:08,099 --> 00:16:14,339
for joints are there are several ways to

00:16:10,619 --> 00:16:16,559
execute them so in i think in hive they

00:16:14,339 --> 00:16:19,139
call them reduce that join or maps i

00:16:16,559 --> 00:16:22,199
join they call them sort join and hash

00:16:19,139 --> 00:16:24,269
join and so on and you can build a lot

00:16:22,199 --> 00:16:26,429
of combinations to execute these to

00:16:24,269 --> 00:16:28,049
execute these different joints with

00:16:26,429 --> 00:16:29,549
different algorithms some of which are

00:16:28,049 --> 00:16:32,459
good in one situation and some of which

00:16:29,549 --> 00:16:34,829
are good in another situation so all in

00:16:32,459 --> 00:16:37,709
all if you take this program and you

00:16:34,829 --> 00:16:39,720
tell the programmer okay you need to

00:16:37,709 --> 00:16:41,039
figure out now what exactly is the way

00:16:39,720 --> 00:16:43,739
to execute the first joint and the

00:16:41,039 --> 00:16:45,509
second join they're going to they're

00:16:43,739 --> 00:16:47,129
going to be quite a few combinations and

00:16:45,509 --> 00:16:48,689
actually figuring that out is not it's

00:16:47,129 --> 00:16:50,369
not unusual because it really makes

00:16:48,689 --> 00:16:54,899
difference in the execution times in

00:16:50,369 --> 00:16:56,429
orders of magnitudes so below here is a

00:16:54,899 --> 00:16:58,739
list of the strategies that status your

00:16:56,429 --> 00:17:02,369
nose internally so partition join which

00:16:58,739 --> 00:17:04,139
is kind of close to the reducer join

00:17:02,369 --> 00:17:06,120
versus replicated joins which is close

00:17:04,139 --> 00:17:09,750
to the maps are join sorting and hashing

00:17:06,120 --> 00:17:11,399
algorithms underneath and trying to I'm

00:17:09,750 --> 00:17:14,009
trying to come up with a good way there

00:17:11,399 --> 00:17:16,529
is something that um that the system can

00:17:14,009 --> 00:17:18,059
can hear do for you so your submit is

00:17:16,529 --> 00:17:20,339
dropped to the optimizer and what the

00:17:18,059 --> 00:17:22,439
optimizer does is it actually looks at

00:17:20,339 --> 00:17:24,419
the it looks at the sizes of the of the

00:17:22,439 --> 00:17:26,339
data that go into the operations it

00:17:24,419 --> 00:17:28,139
makes an estimate how this operation and

00:17:26,339 --> 00:17:29,759
behaves how it changes the data what the

00:17:28,139 --> 00:17:31,799
data is Isis's that go into the success

00:17:29,759 --> 00:17:33,629
of operations it's going to look

00:17:31,799 --> 00:17:35,340
holistically on the plan i'm going to

00:17:33,629 --> 00:17:37,110
see okay if I do this here can I

00:17:35,340 --> 00:17:38,759
probably possibly reuse something here

00:17:37,110 --> 00:17:40,590
so can I collapse these operations into

00:17:38,759 --> 00:17:42,659
one operation can i maybe do something

00:17:40,590 --> 00:17:45,000
here that can reuse later at that point

00:17:42,659 --> 00:17:46,200
so it's going to take this yeah holistic

00:17:45,000 --> 00:17:49,650
look at the plan and trying to optimize

00:17:46,200 --> 00:17:52,590
it so here in example one way

00:17:49,650 --> 00:17:55,680
that you could do it is say okay take

00:17:52,590 --> 00:17:57,450
the first join here and make this a make

00:17:55,680 --> 00:17:59,190
this a partition Tash join which is I

00:17:57,450 --> 00:18:01,320
don't think it's something that you can

00:17:59,190 --> 00:18:02,880
actually do in my produce it's closer to

00:18:01,320 --> 00:18:06,060
the reducer joy and then do the maps are

00:18:02,880 --> 00:18:08,340
join but it's not really it the second

00:18:06,060 --> 00:18:11,310
join could be a what does it do

00:18:08,340 --> 00:18:13,170
broadcast has shown here and that gives

00:18:11,310 --> 00:18:17,100
you a very interesting very interesting

00:18:13,170 --> 00:18:19,110
setup because it allows you if you if

00:18:17,100 --> 00:18:20,790
you look here the grouping field that

00:18:19,110 --> 00:18:22,530
use use it this position as kind of this

00:18:20,790 --> 00:18:24,840
is the same field that you use you for

00:18:22,530 --> 00:18:27,890
joining so given that you did something

00:18:24,840 --> 00:18:31,410
like partitioning on this field here

00:18:27,890 --> 00:18:34,110
broadcasting on the other side and for

00:18:31,410 --> 00:18:36,960
this join you can simply reuse it so it

00:18:34,110 --> 00:18:39,120
means that if that service we can come

00:18:36,960 --> 00:18:41,490
up with a way of executing this program

00:18:39,120 --> 00:18:43,290
that that this aggregation which is

00:18:41,490 --> 00:18:45,570
normally a reducer comes basically for

00:18:43,290 --> 00:18:47,580
free so you can you pay you pay 11 join

00:18:45,570 --> 00:18:48,990
you pay a map search on here and you get

00:18:47,580 --> 00:18:50,820
the successive aggregation kind of for

00:18:48,990 --> 00:18:54,870
free this is how a damn stitches

00:18:50,820 --> 00:18:57,180
together the execution plans so this is

00:18:54,870 --> 00:18:59,460
a this is kind of the value you get out

00:18:57,180 --> 00:19:05,250
of an optimizer I'm being added between

00:18:59,460 --> 00:19:07,860
the api's at the runtime okay now that

00:19:05,250 --> 00:19:09,900
you with that we've written a program

00:19:07,860 --> 00:19:11,370
that we've looked at okay what can

00:19:09,900 --> 00:19:15,080
actually happen in terms of executor of

00:19:11,370 --> 00:19:17,430
optimization how do we execute it and

00:19:15,080 --> 00:19:19,920
they're they're different ways of doing

00:19:17,430 --> 00:19:21,990
that so I guess from the from the Hadoop

00:19:19,920 --> 00:19:24,390
space the the natural thing would be

00:19:21,990 --> 00:19:27,980
yeah package you can package it into a

00:19:24,390 --> 00:19:30,150
jar file copy it onto your cluster and

00:19:27,980 --> 00:19:33,630
invoke the invoke the script that

00:19:30,150 --> 00:19:35,630
executes the execute the program so that

00:19:33,630 --> 00:19:37,380
actually you can do the same thing here

00:19:35,630 --> 00:19:39,840
something that's actually even more

00:19:37,380 --> 00:19:42,360
comfortable in em and my opinion is

00:19:39,840 --> 00:19:44,220
saying okay I'm just defining the

00:19:42,360 --> 00:19:45,930
program to work on a remote environment

00:19:44,220 --> 00:19:47,190
so I'm creating an environment that

00:19:45,930 --> 00:19:49,440
points at the cluster where it's

00:19:47,190 --> 00:19:51,720
supposed to be executed I'm going to

00:19:49,440 --> 00:19:53,220
invoke it and it will be the program

00:19:51,720 --> 00:19:56,520
will be serialized and thrown onto the

00:19:53,220 --> 00:19:58,050
cluster where an RPC call and if you

00:19:56,520 --> 00:19:59,790
just you know if you're writing the

00:19:58,050 --> 00:20:02,350
program and you're not absolutely sure

00:19:59,790 --> 00:20:03,789
you've gotten it right the first time or

00:20:02,350 --> 00:20:05,650
want to play around a little bit you can

00:20:03,789 --> 00:20:08,020
also just define a local environment and

00:20:05,650 --> 00:20:10,090
execute it from from anywhere in your

00:20:08,020 --> 00:20:11,890
IDE you can actually do this embed it

00:20:10,090 --> 00:20:13,600
with another program so you have a

00:20:11,890 --> 00:20:15,280
regular Java program you write something

00:20:13,600 --> 00:20:17,650
then you're better part of a

00:20:15,280 --> 00:20:19,299
stratosphere program say ok execute this

00:20:17,650 --> 00:20:20,890
locally embedded in this JVM and then

00:20:19,299 --> 00:20:23,200
you continue with the with the other

00:20:20,890 --> 00:20:24,730
parts of the program so depending on how

00:20:23,200 --> 00:20:31,230
you configure it will run single

00:20:24,730 --> 00:20:33,429
threaded or multi-threaded all right um

00:20:31,230 --> 00:20:35,409
let me show you two slides about the

00:20:33,429 --> 00:20:37,630
runtime I personally can talk about the

00:20:35,409 --> 00:20:40,809
runtime probably for two hours along

00:20:37,630 --> 00:20:44,140
because this is my favorite part but um

00:20:40,809 --> 00:20:47,260
let me try and give you just a just a

00:20:44,140 --> 00:20:50,110
quick overview of what it looks like so

00:20:47,260 --> 00:20:52,600
from from a high level the runtime I

00:20:50,110 --> 00:20:54,370
said is fear and Hadoop don't look don't

00:20:52,600 --> 00:20:56,500
look too different you have you have a

00:20:54,370 --> 00:20:58,830
master to witches up my job's the master

00:20:56,500 --> 00:21:01,390
does Resource Management scheduling and

00:20:58,830 --> 00:21:03,730
you have the yep the task managers which

00:21:01,390 --> 00:21:05,230
do the actual work ideally you co-locate

00:21:03,730 --> 00:21:07,539
them with data nodes of the distributed

00:21:05,230 --> 00:21:12,010
file system to get local reads but you

00:21:07,539 --> 00:21:13,929
don't have to and um yeah the the master

00:21:12,010 --> 00:21:16,570
pushes out work to the workers the

00:21:13,929 --> 00:21:21,010
workers communicate with each other to

00:21:16,570 --> 00:21:22,570
exchange intermediate results in in the

00:21:21,010 --> 00:21:24,640
in the details this works a little

00:21:22,570 --> 00:21:27,370
different than then it does for example

00:21:24,640 --> 00:21:30,640
in in Hadoop worlds and spark as I said

00:21:27,370 --> 00:21:32,919
earlier the the work is to actually do

00:21:30,640 --> 00:21:36,490
that actually do a streaming exchange of

00:21:32,919 --> 00:21:40,299
data and also all operations are written

00:21:36,490 --> 00:21:41,590
such that they they allocate a lot of

00:21:40,299 --> 00:21:44,380
memory when they start up there trying

00:21:41,590 --> 00:21:46,480
to fill it up as much as possible once

00:21:44,380 --> 00:21:48,280
they hit once they hit the level that

00:21:46,480 --> 00:21:49,539
they've fused it up the operations are

00:21:48,280 --> 00:21:52,510
written such that they gradually move

00:21:49,539 --> 00:21:54,370
parts of the computation to disk so this

00:21:52,510 --> 00:21:56,049
is also something that is if you wish

00:21:54,370 --> 00:21:57,130
more inherited from the database systems

00:21:56,049 --> 00:21:59,200
and the way these algorithms are

00:21:57,130 --> 00:22:01,059
implemented most of them have a

00:21:59,200 --> 00:22:04,780
characteristic that they have a very

00:22:01,059 --> 00:22:06,760
gradual way of going to disk so if you

00:22:04,780 --> 00:22:08,500
get this one record that actually makes

00:22:06,760 --> 00:22:09,820
you go beyond the capacity of your

00:22:08,500 --> 00:22:11,650
memory it's not all of a sudden that

00:22:09,820 --> 00:22:14,850
everything goes to disk but the first

00:22:11,650 --> 00:22:14,850
parts go to disk only

00:22:16,160 --> 00:22:23,190
okay and there's um there's another

00:22:20,150 --> 00:22:25,710
another mass spectra that makes it a bit

00:22:23,190 --> 00:22:30,690
different than most of the systems that

00:22:25,710 --> 00:22:34,800
are around there so we've placed kind of

00:22:30,690 --> 00:22:38,100
an emphasis on on robustness here which

00:22:34,800 --> 00:22:40,260
means that we don't we don't gather

00:22:38,100 --> 00:22:42,600
let's say if you have an intermediate

00:22:40,260 --> 00:22:45,300
result you gather data for sorting or

00:22:42,600 --> 00:22:48,030
for a hash table or you just you want to

00:22:45,300 --> 00:22:50,100
cache the data yeah in memory we don't

00:22:48,030 --> 00:22:51,030
gather the the object that you work on

00:22:50,100 --> 00:22:52,380
even though you write that you're

00:22:51,030 --> 00:22:55,080
working on objects but what you actually

00:22:52,380 --> 00:22:58,920
do is we gather binary representations

00:22:55,080 --> 00:23:01,470
of the of the data that is somewhat

00:22:58,920 --> 00:23:04,020
similar and the way that the Hadoop does

00:23:01,470 --> 00:23:05,370
it which gathers this utilize data

00:23:04,020 --> 00:23:08,040
although we do it a little differently

00:23:05,370 --> 00:23:10,110
so status view has a chunk of memory

00:23:08,040 --> 00:23:13,620
pages which are if you wish just by the

00:23:10,110 --> 00:23:15,750
race of 64 K size in in which it gathers

00:23:13,620 --> 00:23:18,690
the data but it um it does so in a

00:23:15,750 --> 00:23:23,190
fairly transparent way so if you if you

00:23:18,690 --> 00:23:24,930
use a class for example a temple we know

00:23:23,190 --> 00:23:26,790
very specifically actually how the data

00:23:24,930 --> 00:23:30,270
looks like how the data is laid out once

00:23:26,790 --> 00:23:32,610
you create a binary representation what

00:23:30,270 --> 00:23:34,080
this allows you to do is it allows you

00:23:32,610 --> 00:23:36,570
to write certain algorithms of the

00:23:34,080 --> 00:23:40,320
runtime to to work directly on this

00:23:36,570 --> 00:23:43,350
binary data so you don't have to UM to

00:23:40,320 --> 00:23:45,450
go to the arm to the binary data turn it

00:23:43,350 --> 00:23:47,190
back into an object to whatever

00:23:45,450 --> 00:23:48,570
operation you want on the object for

00:23:47,190 --> 00:23:51,180
example comparing them if you want to

00:23:48,570 --> 00:23:52,410
sort them but a lot of the arm of these

00:23:51,180 --> 00:23:55,050
operations can actually happen directly

00:23:52,410 --> 00:23:57,420
on the on the binary data and that goes

00:23:55,050 --> 00:23:59,610
beyond let's say key value pairs or so

00:23:57,420 --> 00:24:00,900
also for for more complicated structures

00:23:59,610 --> 00:24:03,590
that you get from longer tuples or

00:24:00,900 --> 00:24:03,590
nested objects

00:24:06,470 --> 00:24:12,360
alright um let me tell you a little bit

00:24:10,140 --> 00:24:15,020
okay I know I'm actually moving kind of

00:24:12,360 --> 00:24:18,360
fast through it so if you have questions

00:24:15,020 --> 00:24:20,310
and I'd be happy to spend some time in

00:24:18,360 --> 00:24:21,420
the end actually going also back to the

00:24:20,310 --> 00:24:25,050
individual sections and answering

00:24:21,420 --> 00:24:26,640
questions there okay um let me spend a

00:24:25,050 --> 00:24:29,700
few minutes on iterative algorithms

00:24:26,640 --> 00:24:32,220
because that is that is something we're

00:24:29,700 --> 00:24:33,720
where we added were added quite a bit of

00:24:32,220 --> 00:24:35,340
functionality also in a way that I think

00:24:33,720 --> 00:24:38,040
is quite different from what most

00:24:35,340 --> 00:24:39,660
systems aren't there do so a turret of

00:24:38,040 --> 00:24:41,160
algorithms are kind of an interesting

00:24:39,660 --> 00:24:44,010
class of algorithms they were very

00:24:41,160 --> 00:24:45,480
important for um for a lot of lot of

00:24:44,010 --> 00:24:47,030
applications for a lot of algorithms

00:24:45,480 --> 00:24:50,550
from the fields of clustering

00:24:47,030 --> 00:24:53,490
optimization graph graph analysis graph

00:24:50,550 --> 00:24:56,310
processing what they what they really do

00:24:53,490 --> 00:24:59,550
is they are they do multiple passes over

00:24:56,310 --> 00:25:01,860
the data most of the time these

00:24:59,550 --> 00:25:05,280
algorithms start out with with a set of

00:25:01,860 --> 00:25:09,750
parameters that is somewhat randomly

00:25:05,280 --> 00:25:12,260
initialized or words the it's the state

00:25:09,750 --> 00:25:14,490
of let's say the previous day or so and

00:25:12,260 --> 00:25:16,710
what you then do is you go multiple

00:25:14,490 --> 00:25:20,010
times over your over your data to

00:25:16,710 --> 00:25:22,380
analyze and each time you refine the

00:25:20,010 --> 00:25:23,670
parameters a little bit you go over them

00:25:22,380 --> 00:25:24,900
the next time we find them a little bit

00:25:23,670 --> 00:25:26,670
more until you've kind of reach your

00:25:24,900 --> 00:25:28,170
convergence date and then you've you've

00:25:26,670 --> 00:25:34,290
trained your classifier you have found

00:25:28,170 --> 00:25:37,770
your clustering so the the way that is

00:25:34,290 --> 00:25:40,650
that this is most often done this by

00:25:37,770 --> 00:25:42,240
implementing the implementing that

00:25:40,650 --> 00:25:45,810
functionality that makes one pass over

00:25:42,240 --> 00:25:47,190
their data and really just invoking it a

00:25:45,810 --> 00:25:50,070
lot of times which seems like the

00:25:47,190 --> 00:25:51,570
natural thing to do so how do in Hadoop

00:25:50,070 --> 00:25:53,130
you would say okay I have my data here

00:25:51,570 --> 00:25:55,680
in the distributed file system I call

00:25:53,130 --> 00:25:57,660
this the step function which would does

00:25:55,680 --> 00:26:00,030
Switched is one computation of this

00:25:57,660 --> 00:26:01,530
algorithm the result is again going to

00:26:00,030 --> 00:26:03,600
the distributed file system so I'm going

00:26:01,530 --> 00:26:05,790
to do that again from distributed file

00:26:03,600 --> 00:26:09,720
system to distributed file system on and

00:26:05,790 --> 00:26:12,270
on and on it works but you pay kind of a

00:26:09,720 --> 00:26:15,810
kind of an high overhead for always

00:26:12,270 --> 00:26:18,600
doing the full amount of work so the

00:26:15,810 --> 00:26:19,650
Apache spark project is this in a in a

00:26:18,600 --> 00:26:21,450
way that's a bit cleverer

00:26:19,650 --> 00:26:23,070
by saying okay let's just do the it's

00:26:21,450 --> 00:26:25,140
just do the computation from round to

00:26:23,070 --> 00:26:27,780
round so we've read from this the first

00:26:25,140 --> 00:26:31,470
time and ever always afterwards we just

00:26:27,780 --> 00:26:33,330
read from Ram um this gives you also the

00:26:31,470 --> 00:26:34,920
ability that if you have something that

00:26:33,330 --> 00:26:36,960
doesn't change between iterations you

00:26:34,920 --> 00:26:42,980
can specifically say okay pin me that

00:26:36,960 --> 00:26:45,770
into the main memory and yeah reuse it

00:26:42,980 --> 00:26:48,390
stratosphere does it a bit different um

00:26:45,770 --> 00:26:50,280
what what stratosphere does is once you

00:26:48,390 --> 00:26:53,340
bring up an iterative algorithm you

00:26:50,280 --> 00:26:57,570
really bring up one instance of this

00:26:53,340 --> 00:26:59,820
data flow that is the computation so if

00:26:57,570 --> 00:27:02,310
the if the if the computation consists

00:26:59,820 --> 00:27:04,070
your of a map reduced Joe and join then

00:27:02,310 --> 00:27:06,720
instead of having it once and then

00:27:04,070 --> 00:27:07,980
another time afterwards in another time

00:27:06,720 --> 00:27:11,280
afterwards you really bring it up once

00:27:07,980 --> 00:27:12,900
and just um close a a data flow edge

00:27:11,280 --> 00:27:16,020
back from the last operation to the

00:27:12,900 --> 00:27:17,910
first operation so in that sense it's

00:27:16,020 --> 00:27:20,580
not it's not a loop that you kind of

00:27:17,910 --> 00:27:26,670
roll out it's really a closed loop if

00:27:20,580 --> 00:27:29,520
you wish and that is it has it has the

00:27:26,670 --> 00:27:31,680
advantage that that you can actually

00:27:29,520 --> 00:27:36,510
share part of the computation across

00:27:31,680 --> 00:27:39,270
multiple steps so assume that that there

00:27:36,510 --> 00:27:41,580
is some let's say some auxiliary data

00:27:39,270 --> 00:27:46,110
set that you want to initialize inside

00:27:41,580 --> 00:27:48,510
this operator once are once you once you

00:27:46,110 --> 00:27:50,580
start the computation this can be lets

00:27:48,510 --> 00:27:52,440
say a library with another classifier

00:27:50,580 --> 00:27:54,000
that you you know you load and to bring

00:27:52,440 --> 00:27:55,830
it up and then you invoke this function

00:27:54,000 --> 00:27:57,210
instead of doing this every time while

00:27:55,830 --> 00:27:58,680
you unroll that you can you really do

00:27:57,210 --> 00:28:00,300
this once and the data just pipes

00:27:58,680 --> 00:28:01,530
through the operator multiple times it

00:28:00,300 --> 00:28:02,760
doesn't do they also in an uncontrolled

00:28:01,530 --> 00:28:05,190
fashion so there's a clear

00:28:02,760 --> 00:28:07,020
synchronization of super steps between

00:28:05,190 --> 00:28:08,610
that but this is really you know just

00:28:07,020 --> 00:28:10,290
lightweight coordination between the

00:28:08,610 --> 00:28:11,520
operators there's no it's really

00:28:10,290 --> 00:28:16,910
deployed into the cluster and

00:28:11,520 --> 00:28:19,620
initialized only once um another thing

00:28:16,910 --> 00:28:22,400
again coming back to this to this

00:28:19,620 --> 00:28:26,700
optimizer that that stratosphere does is

00:28:22,400 --> 00:28:29,340
it dumb it also can it also looks at

00:28:26,700 --> 00:28:32,190
this at this loop the way you specify it

00:28:29,340 --> 00:28:32,940
and figures out okay where's waste data

00:28:32,190 --> 00:28:34,560
loop and very

00:28:32,940 --> 00:28:37,770
so what happens let's say once across

00:28:34,560 --> 00:28:39,570
all loops what can I cash in order to

00:28:37,770 --> 00:28:42,150
say okay let me get back at this result

00:28:39,570 --> 00:28:44,700
in every in every time I feedback the

00:28:42,150 --> 00:28:46,380
data again instead of recomputing it so

00:28:44,700 --> 00:28:48,540
it's it's placing such caches

00:28:46,380 --> 00:28:50,550
automatically it's it's looking at the

00:28:48,540 --> 00:28:51,990
data flow and trying to push some part

00:28:50,550 --> 00:28:53,940
of the computation out of the loop if

00:28:51,990 --> 00:28:58,560
possible so to do it before the entire

00:28:53,940 --> 00:29:00,090
loop starts and um yeah if it has

00:28:58,560 --> 00:29:01,920
actually first-class support for

00:29:00,090 --> 00:29:03,780
maintaining state across iterations

00:29:01,920 --> 00:29:08,430
which it can them where it can actually

00:29:03,780 --> 00:29:10,590
use a an index to access it so this is a

00:29:08,430 --> 00:29:12,180
screenshot actually from the from the

00:29:10,590 --> 00:29:14,580
optimizers visualization tool where you

00:29:12,180 --> 00:29:20,760
can have a look at at the way status for

00:29:14,580 --> 00:29:22,230
execute programs okay and what that

00:29:20,760 --> 00:29:24,300
allows us to do if we have a loop that

00:29:22,230 --> 00:29:25,770
is so tightly integrated into the

00:29:24,300 --> 00:29:28,980
runtime you can actually mix and match

00:29:25,770 --> 00:29:32,280
certain paradigms so here is an example

00:29:28,980 --> 00:29:34,080
program that that starts it starts with

00:29:32,280 --> 00:29:36,330
an execution environment it creates two

00:29:34,080 --> 00:29:39,270
data sets here for vertex studies for

00:29:36,330 --> 00:29:41,040
edges it runs a regular maps tab

00:29:39,270 --> 00:29:43,080
function is a pre-processing step and

00:29:41,040 --> 00:29:44,340
then it invokes a vertex and Rick

00:29:43,080 --> 00:29:46,440
iteration so you can actually embed

00:29:44,340 --> 00:29:48,060
different paradigms even in in the

00:29:46,440 --> 00:29:50,100
programs you start record oriented to

00:29:48,060 --> 00:29:51,630
switch to vertex oriented you can if you

00:29:50,100 --> 00:29:53,310
wish switch back to record oriented

00:29:51,630 --> 00:29:56,910
later switch back to vertex oriented for

00:29:53,310 --> 00:29:59,010
another time if you want so um this

00:29:56,910 --> 00:30:00,360
year's something that is for those of

00:29:59,010 --> 00:30:02,280
you who know appetitive Ravitz and

00:30:00,360 --> 00:30:05,520
interface it's very similar to it it's

00:30:02,280 --> 00:30:07,470
um you describe a graph computation by

00:30:05,520 --> 00:30:09,240
by looking at the graph from the

00:30:07,470 --> 00:30:10,590
perspective of a vertex and saying okay

00:30:09,240 --> 00:30:12,150
I'm receiving some messages from my

00:30:10,590 --> 00:30:14,310
neighbors and I'm sending messages to my

00:30:12,150 --> 00:30:16,770
neighbors so this is this is really that

00:30:14,310 --> 00:30:18,510
thing just embedded into into an or

00:30:16,770 --> 00:30:24,810
encapsulated in an operator that you can

00:30:18,510 --> 00:30:26,970
embed in this in this API and the

00:30:24,810 --> 00:30:28,740
ability true to maintain state across

00:30:26,970 --> 00:30:30,210
multiple steps of iteration is actually

00:30:28,740 --> 00:30:33,510
something that is really powerful if you

00:30:30,210 --> 00:30:35,130
use it so for many of these machine

00:30:33,510 --> 00:30:37,170
learning algorithms you have a you have

00:30:35,130 --> 00:30:38,670
this characteristic that not all of

00:30:37,170 --> 00:30:41,130
these parameters that you that you

00:30:38,670 --> 00:30:43,350
refine in terms of the computation

00:30:41,130 --> 00:30:44,289
actually needs as much computation in

00:30:43,350 --> 00:30:46,429
some parameters

00:30:44,289 --> 00:30:49,879
converge very fast and some of them

00:30:46,429 --> 00:30:52,099
converge very slow so it it is a good

00:30:49,879 --> 00:30:53,690
thing to say okay once the parameters

00:30:52,099 --> 00:30:55,849
converged I'm actually keeping it out of

00:30:53,690 --> 00:30:57,589
the loop so I'm doing the loop only on

00:30:55,849 --> 00:30:59,989
on whatever parameters are still

00:30:57,589 --> 00:31:01,940
changing and because I can really keep

00:30:59,989 --> 00:31:04,279
them let's say in the operator put it to

00:31:01,940 --> 00:31:05,690
the side you can you can keep them

00:31:04,279 --> 00:31:07,909
around without computing on them and

00:31:05,690 --> 00:31:09,679
this if you implement this correctly

00:31:07,909 --> 00:31:15,169
than the this can get your runtime down

00:31:09,679 --> 00:31:17,379
arm by very much okay let me let me skip

00:31:15,169 --> 00:31:19,549
over those slides and just give you a

00:31:17,379 --> 00:31:21,109
rough road map of where the system is

00:31:19,549 --> 00:31:23,950
going and if i have time actually a two

00:31:21,109 --> 00:31:25,969
minute demo in the end so what we're

00:31:23,950 --> 00:31:27,799
what i've shown you now is going to the

00:31:25,969 --> 00:31:29,539
end of the stratosphere project so where

00:31:27,799 --> 00:31:31,879
the flink project is going right now is

00:31:29,539 --> 00:31:34,579
yeah of course it's moving to apache so

00:31:31,879 --> 00:31:37,999
we're releasing the latest pre Apache

00:31:34,579 --> 00:31:39,799
version of 15 now you can try out the

00:31:37,999 --> 00:31:41,119
release candidates if you want so one of

00:31:39,799 --> 00:31:42,739
the next things we're going to add this

00:31:41,119 --> 00:31:43,940
mid query fault tolerance that is

00:31:42,739 --> 00:31:45,409
something we didn't take from the

00:31:43,940 --> 00:31:49,399
research project into the open source

00:31:45,409 --> 00:31:52,039
yet support for interactive queries and

00:31:49,399 --> 00:31:54,909
cross query cashing in our design those

00:31:52,039 --> 00:31:58,099
two actually very closely related so um

00:31:54,909 --> 00:32:00,349
this is a common effort and we've

00:31:58,099 --> 00:32:05,629
started looking into tears or Tess is a

00:32:00,349 --> 00:32:08,299
is a runtime that is developed by mainly

00:32:05,629 --> 00:32:10,339
by the hive community I would say it's

00:32:08,299 --> 00:32:11,839
the it's the new runtime under the under

00:32:10,339 --> 00:32:14,929
the latest version of hive but it is

00:32:11,839 --> 00:32:16,940
actually a general data for runtime so

00:32:14,929 --> 00:32:18,769
we actually seeing how stratosphere is

00:32:16,940 --> 00:32:23,200
let's say the api's and the optimizer

00:32:18,769 --> 00:32:25,459
can be integrated with with tests and

00:32:23,200 --> 00:32:27,559
adding it as an alternative execution

00:32:25,459 --> 00:32:30,679
engine to our on to our own execution

00:32:27,559 --> 00:32:32,509
engine because the good thing is heart

00:32:30,679 --> 00:32:33,799
works is putting a lot into tears and

00:32:32,509 --> 00:32:35,419
making sure it runs on ten thousand

00:32:33,799 --> 00:32:38,659
notes that is honestly beyond what we

00:32:35,419 --> 00:32:41,719
have done yet the 10,000 notes so it

00:32:38,659 --> 00:32:44,589
makes sense to to say if we we have we

00:32:41,719 --> 00:32:46,849
have strong higher-level layers and and

00:32:44,589 --> 00:32:51,469
find kind of an orthogonal match on the

00:32:46,849 --> 00:32:54,320
lower on the lower layers um the this

00:32:51,469 --> 00:32:56,299
probably add

00:32:54,320 --> 00:32:58,039
I'm sure if there's a talk about that

00:32:56,299 --> 00:32:59,480
for those of you from Berlin the last

00:32:58,039 --> 00:33:01,850
recommender get together on thursday

00:32:59,480 --> 00:33:04,009
actually introduced the new Mahad

00:33:01,850 --> 00:33:06,679
scholar dsl for linear algebra

00:33:04,009 --> 00:33:08,600
operations they're running it currently

00:33:06,679 --> 00:33:10,309
on spark what we're doing is we're also

00:33:08,600 --> 00:33:12,429
adding operations underneath and trying

00:33:10,309 --> 00:33:14,870
to integrate my out with stratosphere so

00:33:12,429 --> 00:33:16,759
that you can that you can actually run

00:33:14,870 --> 00:33:19,850
the linear algebra computations from

00:33:16,759 --> 00:33:22,070
Iran stratosphere and then there's

00:33:19,850 --> 00:33:24,200
there's a group in in Budapest that's

00:33:22,070 --> 00:33:25,809
working on streaming so I mentioned that

00:33:24,200 --> 00:33:29,629
the runtime the lower levels actually

00:33:25,809 --> 00:33:31,190
have streaming like computation so what

00:33:29,629 --> 00:33:33,559
we're trying to do is actually surface

00:33:31,190 --> 00:33:35,480
them in them in a high level in a higher

00:33:33,559 --> 00:33:38,269
level API some some stole my

00:33:35,480 --> 00:33:39,620
capabilities the nice thing about that

00:33:38,269 --> 00:33:41,330
is once you actually have this in the

00:33:39,620 --> 00:33:43,389
same flow system you can kind of build a

00:33:41,330 --> 00:33:45,889
lambda architecture kind of thing

00:33:43,389 --> 00:33:47,480
without wiring together let's say how to

00:33:45,889 --> 00:33:50,659
open store we can really build it in one

00:33:47,480 --> 00:33:52,669
end to end integrated system where you

00:33:50,659 --> 00:33:55,779
get you get time safe and efficient

00:33:52,669 --> 00:33:58,700
interchange of data types and everything

00:33:55,779 --> 00:34:00,080
yeah and at last thing they want to

00:33:58,700 --> 00:34:02,509
mention on the roadmap is that we're

00:34:00,080 --> 00:34:06,740
we're trying to actually improve the the

00:34:02,509 --> 00:34:09,859
upper layer if api's to go more to to a

00:34:06,740 --> 00:34:13,639
logical way of specifying queries so

00:34:09,859 --> 00:34:15,379
what what that can give you is is shown

00:34:13,639 --> 00:34:16,669
here there's only a prototype for that

00:34:15,379 --> 00:34:20,329
it's not part of the release right now

00:34:16,669 --> 00:34:22,760
um this is it's the same same word count

00:34:20,329 --> 00:34:25,099
example instead of working on tablets it

00:34:22,760 --> 00:34:28,669
works on a very simple class defined

00:34:25,099 --> 00:34:31,369
here with two fields a word in a count

00:34:28,669 --> 00:34:34,129
field and instead of writing functions

00:34:31,369 --> 00:34:36,349
that that that really access these

00:34:34,129 --> 00:34:38,179
fields or modify these fields you can

00:34:36,349 --> 00:34:39,579
you can simply drop in the names of the

00:34:38,179 --> 00:34:43,429
fields you say group by the word

00:34:39,579 --> 00:34:45,859
aggregate or sum up the count field so

00:34:43,429 --> 00:34:48,109
in order to ease the interchange at the

00:34:45,859 --> 00:34:53,419
interplay between arm between objects

00:34:48,109 --> 00:34:55,639
and empty API ok and with that I'd like

00:34:53,419 --> 00:34:57,109
to conclude the talk our motorways big

00:34:55,639 --> 00:34:59,000
data looks tiny from stratosphere we're

00:34:57,109 --> 00:35:00,980
still looking for a good a good slogan

00:34:59,000 --> 00:35:02,180
for Flinx of anybody's any suggestions

00:35:00,980 --> 00:35:04,670
please drop us a note on the mailing

00:35:02,180 --> 00:35:06,650
list you can find a lot of the resources

00:35:04,670 --> 00:35:08,720
that

00:35:06,650 --> 00:35:10,910
that show how to get started that give

00:35:08,720 --> 00:35:12,500
you an idea of what the what the api's

00:35:10,910 --> 00:35:14,690
looked like and so on at the status

00:35:12,500 --> 00:35:18,380
field at you or on github so is your

00:35:14,690 --> 00:35:20,299
stratosphere and yeah if you're

00:35:18,380 --> 00:35:22,630
interested in release announcement and

00:35:20,299 --> 00:35:32,150
use and so on follow us on twitter

00:35:22,630 --> 00:35:34,190
thanks so we have a few minutes for

00:35:32,150 --> 00:35:35,900
questions or a demo i'll actually leave

00:35:34,190 --> 00:35:42,319
that up to you whatever you prefer so it

00:35:35,900 --> 00:35:44,049
seems questions huh okay um hello I end

00:35:42,319 --> 00:35:46,609
adventure of questions mostly about

00:35:44,049 --> 00:35:50,390
comparison with spark but you mostly

00:35:46,609 --> 00:35:54,049
answered the then unfortunately I only

00:35:50,390 --> 00:35:57,740
learned about the stratosphere like few

00:35:54,049 --> 00:36:01,760
weeks ago where I spark was a we are

00:35:57,740 --> 00:36:07,579
working with it one year ago so we have

00:36:01,760 --> 00:36:10,039
no expense do you consider stratosphere

00:36:07,579 --> 00:36:13,640
was freeing to be a prediction you label

00:36:10,039 --> 00:36:17,420
or two to consider it what is it

00:36:13,640 --> 00:36:20,000
prediction ready is that comparing the

00:36:17,420 --> 00:36:22,279
two spark because i vant leader both

00:36:20,000 --> 00:36:26,960
projects are very similar there with

00:36:22,279 --> 00:36:30,829
video streaming with stratosphere to the

00:36:26,960 --> 00:36:34,520
most striking difference advantage I

00:36:30,829 --> 00:36:37,069
would see for us atmosphere in the the

00:36:34,520 --> 00:36:40,000
the way to degrade to disc we spark

00:36:37,069 --> 00:36:43,130
sometimes you are out of memory it's

00:36:40,000 --> 00:36:47,000
much harder to tune sometimes worse but

00:36:43,130 --> 00:36:48,980
sometimes not yeah okay so there was a

00:36:47,000 --> 00:36:50,660
bunch of questions I think the main the

00:36:48,980 --> 00:36:52,339
main point was the comparison with with

00:36:50,660 --> 00:36:55,880
spark what are the main advantages is a

00:36:52,339 --> 00:36:57,589
test production ready and so on so the

00:36:55,880 --> 00:36:59,000
projects have been going on similarly

00:36:57,589 --> 00:37:00,410
long i think our open source effort

00:36:59,000 --> 00:37:06,680
started quite a bit later only half a

00:37:00,410 --> 00:37:08,630
year ago is it production ready I think

00:37:06,680 --> 00:37:10,700
you'd have to you have to try it out at

00:37:08,630 --> 00:37:13,279
the the part that we've shown here we've

00:37:10,700 --> 00:37:15,710
actually we've tested them there are

00:37:13,279 --> 00:37:17,869
some people that are using it I think

00:37:15,710 --> 00:37:20,000
none of them has deployed it into

00:37:17,869 --> 00:37:23,120
production pipeline most of that is the

00:37:20,000 --> 00:37:27,290
say let's say clusters for you know data

00:37:23,120 --> 00:37:33,650
exploration and so on that it does work

00:37:27,290 --> 00:37:35,780
that much I can say the the streaming

00:37:33,650 --> 00:37:37,820
parts are there currently under

00:37:35,780 --> 00:37:39,020
development so everything what I'm

00:37:37,820 --> 00:37:40,790
saying right now is about the batch

00:37:39,020 --> 00:37:42,950
parts the stuff that I talked here not

00:37:40,790 --> 00:37:44,090
about the roadmap part so I would say

00:37:42,950 --> 00:37:47,570
it's in the shape that you can try it

00:37:44,090 --> 00:37:49,910
out definitely also try it on you should

00:37:47,570 --> 00:37:51,260
be able to to really do something with

00:37:49,910 --> 00:37:54,140
it not spend debugging your entire

00:37:51,260 --> 00:37:55,490
afternoon the the most striking

00:37:54,140 --> 00:37:58,250
difference is if you would compare to

00:37:55,490 --> 00:38:00,230
spark are yes so there's this the and

00:37:58,250 --> 00:38:02,180
optimizes and we're putting a lot of

00:38:00,230 --> 00:38:05,270
effort on making the high level API

00:38:02,180 --> 00:38:07,580
simpler the optimizer also the in the

00:38:05,270 --> 00:38:09,830
way the road map shows that we try to do

00:38:07,580 --> 00:38:12,620
the the high level yeah I maybe I sauna

00:38:09,830 --> 00:38:14,570
in a more logical and physical way the

00:38:12,620 --> 00:38:16,700
way that you do iterations in a stateful

00:38:14,570 --> 00:38:19,160
manners for you you can actually run the

00:38:16,700 --> 00:38:22,250
same operators for multiple for multiple

00:38:19,160 --> 00:38:25,100
loops you can stratosphere bundles a lot

00:38:22,250 --> 00:38:26,720
of of operators into the same jvm so you

00:38:25,100 --> 00:38:28,580
can share data structures across there

00:38:26,720 --> 00:38:30,250
so if you have let's say libraries or

00:38:28,580 --> 00:38:32,330
cache data or so that you share across

00:38:30,250 --> 00:38:34,520
operators because you run multiple of

00:38:32,330 --> 00:38:39,920
them together that can give you

00:38:34,520 --> 00:38:42,470
definitely an advantage here and yeah

00:38:39,920 --> 00:38:43,790
the the system has been designed from

00:38:42,470 --> 00:38:46,330
the start would be actually a system

00:38:43,790 --> 00:38:48,860
that can go nicely out of course so

00:38:46,330 --> 00:38:52,250
except for one operator which is the

00:38:48,860 --> 00:38:54,500
which is the iteration state index which

00:38:52,250 --> 00:38:56,600
is an operator that they were just I

00:38:54,500 --> 00:38:57,980
would say one of the really of the most

00:38:56,600 --> 00:38:59,840
advanced features and they except for

00:38:57,980 --> 00:39:03,500
that one all of them are actually

00:38:59,840 --> 00:39:05,330
architected in a in a fairly clean way

00:39:03,500 --> 00:39:07,310
of going to disk so except for that one

00:39:05,330 --> 00:39:08,990
I don't think any of them precious if

00:39:07,310 --> 00:39:12,320
you run out of memory and also without

00:39:08,990 --> 00:39:13,520
joining so that should work if you find

00:39:12,320 --> 00:39:15,710
it otherwise please let us know and

00:39:13,520 --> 00:39:18,620
we'll fix it yeah thank thank you for

00:39:15,710 --> 00:39:20,960
the dancers a a bit more first park it's

00:39:18,620 --> 00:39:24,470
very easy to test with the shell the

00:39:20,960 --> 00:39:27,050
command line interface and I guess you

00:39:24,470 --> 00:39:30,670
talked about the stratosphere we'll have

00:39:27,050 --> 00:39:33,710
that in the near future right yes so the

00:39:30,670 --> 00:39:39,380
something like the like the calm

00:39:33,710 --> 00:39:44,750
like an interactive way of doing the way

00:39:39,380 --> 00:39:46,940
go back to the to the roadmap and an

00:39:44,750 --> 00:39:49,339
interactive way of calm of writing

00:39:46,940 --> 00:39:51,410
programs is on the way I mean still you

00:39:49,339 --> 00:39:53,660
can already fairly easily use it armed

00:39:51,410 --> 00:39:55,550
with this with the local environments

00:39:53,660 --> 00:39:57,650
and just executing that executing them

00:39:55,550 --> 00:39:59,810
locally so let me see if we can actually

00:39:57,650 --> 00:40:02,720
see that here so if you see a program

00:39:59,810 --> 00:40:05,510
here in and start this video can you can

00:40:02,720 --> 00:40:10,310
just generate data sets by saying okay

00:40:05,510 --> 00:40:11,900
um just say okay environment create a

00:40:10,310 --> 00:40:13,400
data set from some elements that I throw

00:40:11,900 --> 00:40:14,570
in I just give a local collection it's

00:40:13,400 --> 00:40:16,099
going to move that collection into the

00:40:14,570 --> 00:40:18,619
cluster and uses the data set you can

00:40:16,099 --> 00:40:21,530
you can say just okay whatever print the

00:40:18,619 --> 00:40:23,599
result here or gather it back in in a

00:40:21,530 --> 00:40:25,070
collection output format and so on and

00:40:23,599 --> 00:40:27,500
directly test against it and then

00:40:25,070 --> 00:40:30,950
execute it locally so all of that you

00:40:27,500 --> 00:40:32,450
can just you can can hit run and then

00:40:30,950 --> 00:40:33,700
locally test it and then you can say

00:40:32,450 --> 00:40:35,780
okay okay let's change the environment

00:40:33,700 --> 00:40:37,190
point it to the cluster and move it into

00:40:35,780 --> 00:40:38,869
the cluster instead so run it locally

00:40:37,190 --> 00:40:41,990
and then pointed to some other place and

00:40:38,869 --> 00:40:43,010
run it in the cluster so it's it is true

00:40:41,990 --> 00:40:45,890
there are some cases where the

00:40:43,010 --> 00:40:47,630
interactive shell makes it easier it's

00:40:45,890 --> 00:40:50,450
going to come but i think is already

00:40:47,630 --> 00:40:57,280
fairly easy to use to be honest for for

00:40:50,450 --> 00:40:57,280
quite a few algorithms that question on

00:40:58,750 --> 00:41:06,320
about how you handle the the the fourth

00:41:03,859 --> 00:41:08,180
tolerance how do you work with photo in

00:41:06,320 --> 00:41:10,730
in spark you have at least called

00:41:08,180 --> 00:41:14,720
concept of our DD you have something

00:41:10,730 --> 00:41:19,450
similar or is it something else ok let's

00:41:14,720 --> 00:41:24,440
go to this slide here or maybe even two

00:41:19,450 --> 00:41:26,869
to that one ok so as I said for

00:41:24,440 --> 00:41:28,460
tolerance is actually coming up the

00:41:26,869 --> 00:41:30,020
university project had for tolerance

00:41:28,460 --> 00:41:31,550
which we decided not to move into the

00:41:30,020 --> 00:41:33,770
open source because we weren't confident

00:41:31,550 --> 00:41:35,420
enough that it was robust so we can

00:41:33,770 --> 00:41:39,020
reworking this and adding it to the open

00:41:35,420 --> 00:41:40,820
source and the way that the way that it

00:41:39,020 --> 00:41:43,280
works is if you wish it's not so

00:41:40,820 --> 00:41:44,810
different from our duties because the

00:41:43,280 --> 00:41:47,000
data flows that that stratosphere

00:41:44,810 --> 00:41:48,800
constructs kind of

00:41:47,000 --> 00:41:50,480
between between the data flow in a

00:41:48,800 --> 00:41:51,980
lineage it's kind of the same thing they

00:41:50,480 --> 00:41:54,290
are the only difference in the between

00:41:51,980 --> 00:41:55,880
the systems is that spark would execute

00:41:54,290 --> 00:41:57,800
let's say these two together then that

00:41:55,880 --> 00:41:59,630
one then that one and then that one

00:41:57,800 --> 00:42:02,720
where a stratosphere may actually

00:41:59,630 --> 00:42:04,640
execute them at the same time so if one

00:42:02,720 --> 00:42:06,260
of those fails you can you can do the

00:42:04,640 --> 00:42:07,730
same trick you can backtrack through the

00:42:06,260 --> 00:42:09,200
graph and say okay where's the latest

00:42:07,730 --> 00:42:12,350
point where I actually materialized my

00:42:09,200 --> 00:42:14,030
results the difference here is that once

00:42:12,350 --> 00:42:15,950
a later operator that you let's say

00:42:14,030 --> 00:42:18,680
stream the data to has already been

00:42:15,950 --> 00:42:21,710
started you might actually have to tell

00:42:18,680 --> 00:42:24,260
that operator key reset and start from

00:42:21,710 --> 00:42:25,880
the start from the beginning the later

00:42:24,260 --> 00:42:29,120
operators because they may have consumed

00:42:25,880 --> 00:42:30,890
some data that is that is invalid then

00:42:29,120 --> 00:42:32,720
once you restart predecessors or so

00:42:30,890 --> 00:42:34,340
that's really the I'd say the only big

00:42:32,720 --> 00:42:37,880
difference other than that the two

00:42:34,340 --> 00:42:41,330
concepts kind of kind of the same okay

00:42:37,880 --> 00:42:44,030
we have one last question here mmm I you

00:42:41,330 --> 00:42:45,920
mentioned that the optimizer try decides

00:42:44,030 --> 00:42:47,510
what kind of joints to use in what order

00:42:45,920 --> 00:42:51,020
to run sometimes based on the size of

00:42:47,510 --> 00:42:52,730
the data yeah is there some trick that

00:42:51,020 --> 00:42:56,210
you use this to find out what the size

00:42:52,730 --> 00:42:58,580
of the data is beforehand yeah good

00:42:56,210 --> 00:43:00,590
question there actually is so if if you

00:42:58,580 --> 00:43:02,780
would look at this program the the input

00:43:00,590 --> 00:43:04,790
formats that that that read the data

00:43:02,780 --> 00:43:06,530
here in this case it's the CSV input

00:43:04,790 --> 00:43:08,810
formats input formats being very similar

00:43:06,530 --> 00:43:12,260
to the Hadoop input formats they have

00:43:08,810 --> 00:43:13,850
actually the ability to say okay give me

00:43:12,260 --> 00:43:16,150
some statistics over the data and the

00:43:13,850 --> 00:43:18,620
default input formats implement

00:43:16,150 --> 00:43:20,090
typically something something fairly

00:43:18,620 --> 00:43:22,640
lightweight you know connecting to the

00:43:20,090 --> 00:43:25,580
to the data node and 0 to the to the

00:43:22,640 --> 00:43:28,070
name node summing up sizes of the files

00:43:25,580 --> 00:43:30,560
gathering a lightweight sample of let's

00:43:28,070 --> 00:43:33,500
say 50 lines or so and figuring out the

00:43:30,560 --> 00:43:35,480
average length of the line so so um to

00:43:33,500 --> 00:43:36,920
figure out how many how many rows are

00:43:35,480 --> 00:43:39,080
actually going to be in that file

00:43:36,920 --> 00:43:41,660
roughly so that's what it that's what it

00:43:39,080 --> 00:43:43,850
does to to get an idea of what they the

00:43:41,660 --> 00:43:47,660
characteristics of these initial data

00:43:43,850 --> 00:43:52,370
sets are once you actually go into into

00:43:47,660 --> 00:43:55,220
a later operator there are some there's

00:43:52,370 --> 00:43:57,860
some like and databases from some some

00:43:55,220 --> 00:43:59,960
assumptions how these operators behave

00:43:57,860 --> 00:44:01,220
that give you a

00:43:59,960 --> 00:44:04,580
your rough estimate of what the data

00:44:01,220 --> 00:44:07,010
says afterwards is so these can actually

00:44:04,580 --> 00:44:09,740
be off they they're often off by quite a

00:44:07,010 --> 00:44:11,630
bit but in many cases that's not totally

00:44:09,740 --> 00:44:13,339
bad because they have to give you like a

00:44:11,630 --> 00:44:14,630
rough book or ballpark it's going to be

00:44:13,339 --> 00:44:16,760
a really huge one is it going to be very

00:44:14,630 --> 00:44:22,970
small one and so on so as long as that

00:44:16,760 --> 00:44:24,589
works that is fine what what we actually

00:44:22,970 --> 00:44:27,109
doing and the are going to do in the

00:44:24,589 --> 00:44:28,910
future in order to improve this is once

00:44:27,109 --> 00:44:30,830
you have this let's say the interactive

00:44:28,910 --> 00:44:32,210
mode of running queries the optimizer

00:44:30,830 --> 00:44:33,500
can actually submit the quiz also

00:44:32,210 --> 00:44:35,300
interactively so what you can do is you

00:44:33,500 --> 00:44:36,770
can submit the first join and have a

00:44:35,300 --> 00:44:39,050
look at the first data it produces

00:44:36,770 --> 00:44:40,910
before you actually say okay I really am

00:44:39,050 --> 00:44:42,980
executing the second row and also that

00:44:40,910 --> 00:44:45,980
gives you a moment to reconsider before

00:44:42,980 --> 00:44:49,970
before executing later parts of the join

00:44:45,980 --> 00:44:51,619
and also you can say join and say system

00:44:49,970 --> 00:44:53,119
please decide for me which way to go but

00:44:51,619 --> 00:44:54,589
if you say okay I'm actually very I'm

00:44:53,119 --> 00:44:55,880
very confident and know how to do this

00:44:54,589 --> 00:44:58,369
correctly and I'd rather not take

00:44:55,880 --> 00:45:00,589
chances with the system then just say

00:44:58,369 --> 00:45:03,260
join with large or join with tiny or

00:45:00,589 --> 00:45:05,420
join symmetric or so and you will tell

00:45:03,260 --> 00:45:07,070
it exactly how to do it and give how

00:45:05,420 --> 00:45:08,839
much I think we have to give the

00:45:07,070 --> 00:45:12,910
audience a chance to go do our talks

00:45:08,839 --> 00:45:12,910

YouTube URL: https://www.youtube.com/watch?v=HZRJ7l8hh3U


