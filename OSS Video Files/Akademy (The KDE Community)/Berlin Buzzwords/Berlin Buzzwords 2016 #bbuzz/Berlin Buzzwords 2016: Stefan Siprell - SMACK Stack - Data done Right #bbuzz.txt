Title: Berlin Buzzwords 2016: Stefan Siprell - SMACK Stack - Data done Right #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	A talk covering the best-of-breed platform consisting of Spark, Mesos, Akka, Cassandra and Kafka. SMACK is more of a toolbox of technologies to allow the building of resilient ingestion pipelines, offering a high degree of freedom in the selection of analysis and query possibilities and baked-in support for flow-control. More and more customers are using this stack, which is rapidly becoming the new industry standard for Big Data solutions. 

Read more:
https://2016.berlinbuzzwords.de/session/smack-stack-data-done-right

About Stefan Siprell:
https://2016.berlinbuzzwords.de/users/stefan-siprell

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:03,380 --> 00:00:09,269
okay guys just prepared will be skipping

00:00:06,540 --> 00:00:11,309
some slides probably this is comfortably

00:00:09,269 --> 00:00:14,910
fills an hour you get 40 minutes but

00:00:11,309 --> 00:00:17,820
we'll manage somehow I won't reduce

00:00:14,910 --> 00:00:20,520
myself too much my name is Stefan Stefan

00:00:17,820 --> 00:00:22,859
super al you'll find the slides and

00:00:20,520 --> 00:00:24,930
everything later a record code centric

00:00:22,859 --> 00:00:28,410
and if the doors are sealed on the sales

00:00:24,930 --> 00:00:30,630
guy but don't worry this is going to be

00:00:28,410 --> 00:00:32,700
a pure technical talk but this is what

00:00:30,630 --> 00:00:35,070
we do everyday never pre-sales I've

00:00:32,700 --> 00:00:36,960
Thunder starius technical conferences

00:00:35,070 --> 00:00:46,230
and they've never been thrown off stage

00:00:36,960 --> 00:00:48,629
so please endure who sort of smack ok

00:00:46,230 --> 00:00:51,120
I'm surprised as our phones is the

00:00:48,629 --> 00:00:56,760
buzzword do you know fast data as

00:00:51,120 --> 00:00:59,850
opposed to big data ok fast data to talk

00:00:56,760 --> 00:01:02,160
it's a quarter it's been coined by by

00:00:59,850 --> 00:01:05,159
typesafe the company or not types of

00:01:02,160 --> 00:01:07,200
light Bend and this basically it's a

00:01:05,159 --> 00:01:09,630
stack consisting of five different

00:01:07,200 --> 00:01:12,299
technology spark mezzos acha Cassandra

00:01:09,630 --> 00:01:15,030
and Kafka so it's basically kind of like

00:01:12,299 --> 00:01:18,500
lamp you know yet linux apache mysql PHP

00:01:15,030 --> 00:01:21,570
or Python this goes to same direction

00:01:18,500 --> 00:01:24,479
it's so sits there there six different

00:01:21,570 --> 00:01:25,799
tools they're not too powerful alone but

00:01:24,479 --> 00:01:29,520
you can do pretty cool stuff of them

00:01:25,799 --> 00:01:35,970
together so let's look at the tool's

00:01:29,520 --> 00:01:39,030
first soo no spark ok good actually it's

00:01:35,970 --> 00:01:42,780
not a Swiss Army knife for data spark is

00:01:39,030 --> 00:01:45,420
a distributed computing platform but the

00:01:42,780 --> 00:01:47,549
ecosystem of spark is pretty rich and

00:01:45,420 --> 00:01:51,570
its most of the ecosystem resides and

00:01:47,549 --> 00:01:53,610
data so if you're doing etl jobs extract

00:01:51,570 --> 00:01:55,280
transform load you can do this very

00:01:53,610 --> 00:01:57,570
nicely in spark with the batch

00:01:55,280 --> 00:02:00,479
topologies you have the spark cluster

00:01:57,570 --> 00:02:02,909
you can maintain it 24-7 you just deploy

00:02:00,479 --> 00:02:07,200
your jobs on it and you have a real

00:02:02,909 --> 00:02:09,750
enterprise level etl sweet you want to

00:02:07,200 --> 00:02:11,700
do micro batching on streams no problem

00:02:09,750 --> 00:02:13,330
I mean we've heard a lot about Flint the

00:02:11,700 --> 00:02:15,700
last couple days

00:02:13,330 --> 00:02:19,360
about the big fight about latency and

00:02:15,700 --> 00:02:21,850
throughput if through producer game if

00:02:19,360 --> 00:02:27,790
you can wait seconds for results spark

00:02:21,850 --> 00:02:30,280
can make you happy running SQL and join

00:02:27,790 --> 00:02:33,460
so non-relational database systems no

00:02:30,280 --> 00:02:35,470
problem so if you have a bi department

00:02:33,460 --> 00:02:38,140
who's used working SQL is using

00:02:35,470 --> 00:02:40,510
something like tableau or whatever you

00:02:38,140 --> 00:02:43,150
can you can talk to elasticsearch

00:02:40,510 --> 00:02:47,110
Cassandra whatever you want to using the

00:02:43,150 --> 00:02:49,330
SQL dialect running graph operations on

00:02:47,110 --> 00:02:54,730
line graph databases in the problem

00:02:49,330 --> 00:02:57,880
either last but not least MapReduce and

00:02:54,730 --> 00:03:00,900
super fast thank you but I'll skip the

00:02:57,880 --> 00:03:06,640
spark stuff as your people involved that

00:03:00,900 --> 00:03:09,490
measures or measles this is always spark

00:03:06,640 --> 00:03:11,350
everybody knows measles is not as known

00:03:09,490 --> 00:03:15,370
even I think it's a pity because it's

00:03:11,350 --> 00:03:16,900
really cool technology if you look at

00:03:15,370 --> 00:03:19,030
the operating system with what the

00:03:16,900 --> 00:03:21,940
operating system really really does it

00:03:19,030 --> 00:03:24,610
manage resources so you have 200 300

00:03:21,940 --> 00:03:27,610
processes running and the processes are

00:03:24,610 --> 00:03:30,370
sharing the same CPU to say memory the

00:03:27,610 --> 00:03:32,260
same network hard to say mass data and

00:03:30,370 --> 00:03:34,510
the data and the operating system

00:03:32,260 --> 00:03:36,610
schedules it makes sure that each

00:03:34,510 --> 00:03:38,230
process think he's running alone on the

00:03:36,610 --> 00:03:41,530
system and he gets the resources he

00:03:38,230 --> 00:03:44,769
needs this is what one operating system

00:03:41,530 --> 00:03:47,709
what Colonel does mrs. does this for

00:03:44,769 --> 00:03:51,790
data center so every resource you have

00:03:47,709 --> 00:03:54,760
if it's if it's a hard disk solid-state

00:03:51,790 --> 00:03:57,340
disk network cards memory CPUs is

00:03:54,760 --> 00:03:59,230
managed by measles and it makes sure

00:03:57,340 --> 00:04:01,930
that your process is running in the

00:03:59,230 --> 00:04:07,390
cluster in your data center gets the

00:04:01,930 --> 00:04:09,820
resources it needs so the nice thing is

00:04:07,390 --> 00:04:12,700
if you have a measles deployment what

00:04:09,820 --> 00:04:15,160
you do you install the measles lays on

00:04:12,700 --> 00:04:18,609
all of your machines it's one image to

00:04:15,160 --> 00:04:21,940
deploy all of your machines and during

00:04:18,609 --> 00:04:24,340
the workload measles will decide how

00:04:21,940 --> 00:04:26,350
many instances of rich process to run on

00:04:24,340 --> 00:04:27,060
which machines so if you're thinking

00:04:26,350 --> 00:04:31,919
about they

00:04:27,060 --> 00:04:34,560
data auto scaling a resilience of failed

00:04:31,919 --> 00:04:36,540
nodes missus is a friend he can he can

00:04:34,560 --> 00:04:42,630
make sure that the right process of

00:04:36,540 --> 00:04:44,520
being are gonna be run as opposed to to

00:04:42,630 --> 00:04:47,520
VN layer solutions or two darker

00:04:44,520 --> 00:04:50,010
solutions you're very open here you can

00:04:47,520 --> 00:04:52,770
use whatever you want to Hadoop Kafka

00:04:50,010 --> 00:04:54,930
spark and akka they run as native Java

00:04:52,770 --> 00:04:56,250
jobs on the under guest on the host

00:04:54,930 --> 00:04:58,919
operating system there's no

00:04:56,250 --> 00:05:00,510
virtualization no isolation no

00:04:58,919 --> 00:05:03,290
containerization is just starting up the

00:05:00,510 --> 00:05:09,150
jobs so you have very efficient

00:05:03,290 --> 00:05:10,770
processing and again cooper nate is i

00:05:09,150 --> 00:05:12,479
think they just released the newest

00:05:10,770 --> 00:05:14,820
version where they say you can have up

00:05:12,479 --> 00:05:16,770
to thousands of parts you never had this

00:05:14,820 --> 00:05:25,950
limitation misses you could always run

00:05:16,770 --> 00:05:29,970
data centers acha okay this is my this

00:05:25,950 --> 00:05:33,150
is my weak spot the beauty if acha is

00:05:29,970 --> 00:05:34,979
you can freight these tally performing

00:05:33,150 --> 00:05:38,039
reactive applications so you just think

00:05:34,979 --> 00:05:40,260
of your application as there's the big

00:05:38,039 --> 00:05:42,000
tree of actors working together in the

00:05:40,260 --> 00:05:44,220
assam chris fashion it's highly

00:05:42,000 --> 00:05:47,940
performant because any machine can take

00:05:44,220 --> 00:05:49,800
over any requests concurrency is built

00:05:47,940 --> 00:05:52,350
in you always work with messages in your

00:05:49,800 --> 00:05:57,180
mailboxes it's elastic no single point

00:05:52,350 --> 00:06:00,240
of failure yada yada yada anybody looked

00:05:57,180 --> 00:06:04,950
at legume yet the new the follow-up of

00:06:00,240 --> 00:06:09,150
acha okay nor if I then I have nobody to

00:06:04,950 --> 00:06:10,530
talk to you later and we're talking

00:06:09,150 --> 00:06:13,169
about Big Data let's talk about big

00:06:10,530 --> 00:06:15,000
performance the beauty of it is you

00:06:13,169 --> 00:06:17,130
working a circle issue and actors

00:06:15,000 --> 00:06:20,970
communicating to its other actor verse

00:06:17,130 --> 00:06:22,979
Maya asynchronous messages if you do

00:06:20,970 --> 00:06:24,450
this the typical JMS environment you

00:06:22,979 --> 00:06:25,919
would you would limit the number of

00:06:24,450 --> 00:06:27,450
messages you're putting over the bus

00:06:25,919 --> 00:06:30,410
because the message is a very expensive

00:06:27,450 --> 00:06:32,669
cause the transactional behavior in etc

00:06:30,410 --> 00:06:34,470
you don't have to worry about this if

00:06:32,669 --> 00:06:36,960
you work in soccer because you can

00:06:34,470 --> 00:06:39,910
excellent actually send out 50 million

00:06:36,960 --> 00:06:43,750
requests per machine per second

00:06:39,910 --> 00:06:49,660
so this whole delay send a message never

00:06:43,750 --> 00:06:52,390
hurts in the aqha environment so we

00:06:49,660 --> 00:07:01,090
covered the STM in the a let's go to

00:06:52,390 --> 00:07:03,520
Cassandra known ok cassandra is the

00:07:01,090 --> 00:07:06,760
performance always up no SQL database

00:07:03,520 --> 00:07:08,830
it's it throws out love garbage from the

00:07:06,760 --> 00:07:10,660
rtms world so it doesn't try to do

00:07:08,830 --> 00:07:12,760
transactions it doesn't try to do

00:07:10,660 --> 00:07:14,800
foreign key constraints that doesn't do

00:07:12,760 --> 00:07:17,770
a lot of kind of stuff you use some

00:07:14,800 --> 00:07:21,400
databases and therefore what's left of

00:07:17,770 --> 00:07:23,560
cassandra is super fast so the rule of

00:07:21,400 --> 00:07:25,680
thumb is 10,000 requests per machine per

00:07:23,560 --> 00:07:27,600
second this is insert or read up

00:07:25,680 --> 00:07:32,250
operations who don't really matter

00:07:27,600 --> 00:07:34,120
there's no downtime because Cassandra

00:07:32,250 --> 00:07:35,830
doesn't have a single point of failure

00:07:34,120 --> 00:07:38,310
it doesn't even have a single data

00:07:35,830 --> 00:07:40,320
center a failure if you have multiple

00:07:38,310 --> 00:07:42,880
availability zones in your cluster

00:07:40,320 --> 00:07:47,580
Cassandra can spread across these and

00:07:42,880 --> 00:07:49,870
and survive a Down of availability zone

00:07:47,580 --> 00:07:52,150
it's a column index so you have

00:07:49,870 --> 00:07:55,900
something like sequel which it smells

00:07:52,150 --> 00:07:58,419
like SQL but you have to append only

00:07:55,900 --> 00:08:00,430
performance so any operations are doing

00:07:58,419 --> 00:08:03,400
is actually just append only mechanism

00:08:00,430 --> 00:08:05,500
so that's why cassandra is such a nice

00:08:03,400 --> 00:08:11,350
sweet spot mixing only read and write

00:08:05,500 --> 00:08:15,880
load multiple data centers we have that

00:08:11,350 --> 00:08:18,370
covered and anybody who worked with

00:08:15,880 --> 00:08:19,960
Cassandra it's it's really mind-boggling

00:08:18,370 --> 00:08:22,270
cause the first couple of weeks you

00:08:19,960 --> 00:08:24,070
always want to normalize you always want

00:08:22,270 --> 00:08:26,050
to do a joint statement you always want

00:08:24,070 --> 00:08:29,020
to say but I already know the customer

00:08:26,050 --> 00:08:30,880
I'll just join it from this table it's

00:08:29,020 --> 00:08:33,250
wrong what you want to do in Cassandra

00:08:30,880 --> 00:08:35,500
is you want to have denormalized modes

00:08:33,250 --> 00:08:37,870
so every time you're thinking of new

00:08:35,500 --> 00:08:40,419
Creary you probably end up writing a new

00:08:37,870 --> 00:08:42,160
table in Cassandra this is where the

00:08:40,419 --> 00:08:45,730
performance comes from because it's not

00:08:42,160 --> 00:08:51,670
doing any of this of disjoining and

00:08:45,730 --> 00:08:53,150
resolution of normalizing ok we'll have

00:08:51,670 --> 00:08:56,360
to last part cover

00:08:53,150 --> 00:09:02,690
the second Kafka can't do anything with

00:08:56,360 --> 00:09:04,130
caffeine nowadays it's it's messaging

00:09:02,690 --> 00:09:06,680
for big data I just mentioned this

00:09:04,130 --> 00:09:09,820
before active and Qi work with this a

00:09:06,680 --> 00:09:12,410
lot and in the past ipad apache activemq

00:09:09,820 --> 00:09:14,870
the problem is it wants to have

00:09:12,410 --> 00:09:16,610
guarantees once guarantee the client the

00:09:14,870 --> 00:09:19,540
consumers message is being processed

00:09:16,610 --> 00:09:24,050
exactly once this is expensive because

00:09:19,540 --> 00:09:25,760
but what could but act activemq will do

00:09:24,050 --> 00:09:28,400
in a cluster mode it's going to use the

00:09:25,760 --> 00:09:30,410
database for lock handling okay so every

00:09:28,400 --> 00:09:32,420
time you're pushing or reading something

00:09:30,410 --> 00:09:33,860
from the data from the from the queue

00:09:32,420 --> 00:09:36,560
you're going to have hits in the

00:09:33,860 --> 00:09:37,850
database and this just does not scale

00:09:36,560 --> 00:09:39,710
because you're trying to avoid

00:09:37,850 --> 00:09:43,460
relational databases you're getting them

00:09:39,710 --> 00:09:46,490
back in here so let's Kafka does it

00:09:43,460 --> 00:09:48,770
Kafka can consume hundreds of megabytes

00:09:46,490 --> 00:09:51,500
per second and then push them to next

00:09:48,770 --> 00:09:56,900
clients and it also breaks down the data

00:09:51,500 --> 00:09:59,300
to manageable volumes also since you can

00:09:56,900 --> 00:10:02,060
append only mode you can safely buffer

00:09:59,300 --> 00:10:04,130
terabytes of data without a performance

00:10:02,060 --> 00:10:06,140
without a performance hit consuming the

00:10:04,130 --> 00:10:11,750
data because it's just pushed behind the

00:10:06,140 --> 00:10:17,840
old data and obviously is the superior

00:10:11,750 --> 00:10:19,880
from the ground up so now we just saw

00:10:17,840 --> 00:10:21,730
these loose technologies most of you

00:10:19,880 --> 00:10:23,750
heard of them I think except mesosphere

00:10:21,730 --> 00:10:26,600
there's nothing new we can give you the

00:10:23,750 --> 00:10:29,120
talk up to now but the question is what

00:10:26,600 --> 00:10:32,330
do we do with this and in the beginning

00:10:29,120 --> 00:10:34,100
of big data from my point of view six

00:10:32,330 --> 00:10:37,220
seven eight years ago everything was

00:10:34,100 --> 00:10:41,060
Hadoop okay Hadoop was the first system

00:10:37,220 --> 00:10:42,800
which actually allowed to do big data

00:10:41,060 --> 00:10:44,960
big data means that the that the

00:10:42,800 --> 00:10:48,440
instructions go to the data and get

00:10:44,960 --> 00:10:50,180
processed locally as compared to the old

00:10:48,440 --> 00:10:51,860
database model where you had to load the

00:10:50,180 --> 00:10:53,810
data into memory first two up two

00:10:51,860 --> 00:10:55,550
processes you're breaking this paradigm

00:10:53,810 --> 00:10:57,500
you're turning it up and around and what

00:10:55,550 --> 00:10:59,900
you done which is didn't beginning was

00:10:57,500 --> 00:11:02,150
always MapReduce you were in there

00:10:59,900 --> 00:11:04,400
purely big batch mode I think this is

00:11:02,150 --> 00:11:06,470
the first examples of MapReduce were the

00:11:04,400 --> 00:11:07,260
page rank calculations of Google this

00:11:06,470 --> 00:11:09,690
was the perfect

00:11:07,260 --> 00:11:17,670
sample that you MapReduce cruncher that

00:11:09,690 --> 00:11:19,710
night business kind of learned okay

00:11:17,670 --> 00:11:22,350
what's kind of neat I can classify my

00:11:19,710 --> 00:11:27,390
customers i know that Stefan is probably

00:11:22,350 --> 00:11:29,760
a a father because he's constantly blind

00:11:27,390 --> 00:11:31,860
bluray videos of snow white but he's

00:11:29,760 --> 00:11:34,140
also a nerd because he's still buying

00:11:31,860 --> 00:11:36,570
his magic the gathering cards so Amazon

00:11:34,140 --> 00:11:39,000
kind of learned new stuff about me but

00:11:36,570 --> 00:11:41,370
the business always wants to find out

00:11:39,000 --> 00:11:43,710
this stuff faster it wants to know

00:11:41,370 --> 00:11:46,650
Stefan is still in the checkout queue

00:11:43,710 --> 00:11:48,900
what can we show him we just noticed in

00:11:46,650 --> 00:11:50,760
Stefan's bank around a bank account

00:11:48,900 --> 00:11:52,560
statement that is moving money around

00:11:50,760 --> 00:11:54,000
maybe you want to interrupt it and see

00:11:52,560 --> 00:11:56,250
if he can take the money from him and

00:11:54,000 --> 00:12:00,590
this is what we've seen so far as

00:11:56,250 --> 00:12:03,060
consultants is that a latency between

00:12:00,590 --> 00:12:08,160
sensor reading the inside has to become

00:12:03,060 --> 00:12:10,440
faster and faster obviously the dupe

00:12:08,160 --> 00:12:11,730
ecosystems reacted to it MapReduce is

00:12:10,440 --> 00:12:14,490
not the only mechanism they have

00:12:11,730 --> 00:12:17,040
nowadays storm sparkle all these

00:12:14,490 --> 00:12:21,390
technologies also work on the HDFS but

00:12:17,040 --> 00:12:25,260
we're looking at smack today on the

00:12:21,390 --> 00:12:27,900
other hand so we had to slow overnight

00:12:25,260 --> 00:12:30,120
bad fronts with myth a myth MapReduce

00:12:27,900 --> 00:12:32,340
and their other end we have this high

00:12:30,120 --> 00:12:34,350
for high frequency processing so this is

00:12:32,340 --> 00:12:36,480
like high frequency trading the stock

00:12:34,350 --> 00:12:38,460
market you know when you swap stocks

00:12:36,480 --> 00:12:40,590
thousand times a second if you're

00:12:38,460 --> 00:12:42,540
monitoring a continental power grid this

00:12:40,590 --> 00:12:45,150
is very demanding to find out exactly

00:12:42,540 --> 00:12:49,170
how much what are unpaired when eating

00:12:45,150 --> 00:12:50,940
my system monitoring data centers all of

00:12:49,170 --> 00:12:52,440
these are processes we actually have

00:12:50,940 --> 00:12:54,930
guaranteed execution time of

00:12:52,440 --> 00:12:56,790
milliseconds you have to promise to your

00:12:54,930 --> 00:13:00,470
customer you have to promise to client

00:12:56,790 --> 00:13:02,900
you will execute in milliseconds

00:13:00,470 --> 00:13:06,030
obviously this can only be achieved by

00:13:02,900 --> 00:13:09,060
being very sloppy in working in a very

00:13:06,030 --> 00:13:11,370
small context if you have to respond to

00:13:09,060 --> 00:13:14,160
Millie or microseconds you can only use

00:13:11,370 --> 00:13:16,530
the l1 or l2 cache so a very limited

00:13:14,160 --> 00:13:20,700
data set very limited context you can

00:13:16,530 --> 00:13:22,440
work on and it's sloppy before you lose

00:13:20,700 --> 00:13:24,060
the cadence before you trip and fall

00:13:22,440 --> 00:13:26,910
you're just going to ignore the request

00:13:24,060 --> 00:13:28,860
and take the next request and this is

00:13:26,910 --> 00:13:36,570
something most of our customers are not

00:13:28,860 --> 00:13:38,460
willing to do so this the sweet spot for

00:13:36,570 --> 00:13:42,030
me for smack is everything in between

00:13:38,460 --> 00:13:43,590
you want to react in seconds yeah but

00:13:42,030 --> 00:13:45,630
you want to react in seconds of the big

00:13:43,590 --> 00:13:47,760
context you want to make you want to

00:13:45,630 --> 00:13:49,320
know what did Stefan do in the past what

00:13:47,760 --> 00:13:50,910
is Stefan doing right now you want to

00:13:49,320 --> 00:13:53,550
have a big context you want to work in

00:13:50,910 --> 00:13:55,590
seconds this is for me the sweet spot

00:13:53,550 --> 00:13:58,590
where we would see smack so examples

00:13:55,590 --> 00:14:01,170
would be updating news pages classifying

00:13:58,590 --> 00:14:04,200
users is this valuable users is a fraud

00:14:01,170 --> 00:14:06,750
user I've seen this a couple of times

00:14:04,200 --> 00:14:09,780
today real yesterday real-time bidding

00:14:06,750 --> 00:14:12,390
for advertising our motive in IOT we're

00:14:09,780 --> 00:14:15,210
doing projects or industry 4.0 these are

00:14:12,390 --> 00:14:17,850
the typical use cases the machine is

00:14:15,210 --> 00:14:20,430
behaving oddly is this having any effect

00:14:17,850 --> 00:14:30,120
of my output this is these are second

00:14:20,430 --> 00:14:31,350
decisions does that make sense so far if

00:14:30,120 --> 00:14:36,540
you don't say yes I'll just repeat

00:14:31,350 --> 00:14:39,780
myself okay so let's get to the juicy

00:14:36,540 --> 00:14:42,240
parts um what do we want we want to have

00:14:39,780 --> 00:14:45,990
a reliable ingestion libel ingestion

00:14:42,240 --> 00:14:47,610
means that when your data producers keep

00:14:45,990 --> 00:14:49,290
producing data and you have a little

00:14:47,610 --> 00:14:51,150
hiccup in your system because you cannot

00:14:49,290 --> 00:14:54,240
consume the data fast enough because

00:14:51,150 --> 00:14:56,310
your system crashed because the data

00:14:54,240 --> 00:14:58,590
scientists made a new algorithm which

00:14:56,310 --> 00:15:03,840
doesn't scale as well all of this kind

00:14:58,590 --> 00:15:06,330
of can't create trouble okay and if you

00:15:03,840 --> 00:15:07,680
then you want to be able to to handle

00:15:06,330 --> 00:15:09,480
this trouble you'll get to this in a

00:15:07,680 --> 00:15:12,120
second you want to have flexible storage

00:15:09,480 --> 00:15:13,500
and Creole alternatives the talk which

00:15:12,120 --> 00:15:16,200
is hurt before you know how you can

00:15:13,500 --> 00:15:17,970
build grass on the fly some problems are

00:15:16,200 --> 00:15:20,370
really easy to solve if you consider the

00:15:17,970 --> 00:15:22,440
problem to be a graph some problems are

00:15:20,370 --> 00:15:23,910
really simple to solve if you see them

00:15:22,440 --> 00:15:25,920
as a number of tuples you can do

00:15:23,910 --> 00:15:28,200
MapReduce on but you don't want to be

00:15:25,920 --> 00:15:29,790
limited by your database choice you want

00:15:28,200 --> 00:15:32,630
to pick the database which is the right

00:15:29,790 --> 00:15:32,630
one for your job

00:15:33,269 --> 00:15:39,879
and you want some what a management out

00:15:37,149 --> 00:15:41,500
of the box okay if you've ever set up

00:15:39,879 --> 00:15:44,410
cough cough if you've ever set up a

00:15:41,500 --> 00:15:46,480
spark cluster did these can be royal

00:15:44,410 --> 00:15:48,250
pains in the neck and you don't want to

00:15:46,480 --> 00:15:49,720
do this all the time manually don't want

00:15:48,250 --> 00:15:55,709
to monitor this manually you want to

00:15:49,720 --> 00:15:59,170
have some kind of management features so

00:15:55,709 --> 00:16:00,639
what is Mac it's an architecture toolbox

00:15:59,170 --> 00:16:03,399
we looking at the architecture in a

00:16:00,639 --> 00:16:05,500
second it's the best of reach platform

00:16:03,399 --> 00:16:08,199
we've seen customers who don't use spark

00:16:05,500 --> 00:16:10,420
who use flink you've seen customers you

00:16:08,199 --> 00:16:12,490
use a droid or elastic search of

00:16:10,420 --> 00:16:14,350
Cassandra it's open do whatever feels

00:16:12,490 --> 00:16:18,160
best for you guys it's just an example

00:16:14,350 --> 00:16:20,680
of how you can set up a mesosphere

00:16:18,160 --> 00:16:22,569
introduced infinity this is a new tool

00:16:20,680 --> 00:16:25,449
from mesosphere that's the company

00:16:22,569 --> 00:16:27,370
backing mezzos and they have all these

00:16:25,449 --> 00:16:34,300
tools or in print printer graded as

00:16:27,370 --> 00:16:39,430
binary so who's done micro batching in

00:16:34,300 --> 00:16:41,920
his life okay I already feel a little

00:16:39,430 --> 00:16:44,339
old school everybody's doing fling where

00:16:41,920 --> 00:16:47,379
you actually do where you process each

00:16:44,339 --> 00:16:48,939
event if individually micro batching is

00:16:47,379 --> 00:16:50,829
little different I think was a good

00:16:48,939 --> 00:16:52,600
example if you think of a stream

00:16:50,829 --> 00:16:54,699
processing you can have like a pipeline

00:16:52,600 --> 00:16:57,459
if you think micro batching you think

00:16:54,699 --> 00:16:59,470
little buckets so you fill your water in

00:16:57,459 --> 00:17:02,410
your bucket and then you can process the

00:16:59,470 --> 00:17:04,630
bucket as a total bucket so what you're

00:17:02,410 --> 00:17:07,120
doing is you're taking the stream and

00:17:04,630 --> 00:17:09,220
you're creating windows or static

00:17:07,120 --> 00:17:11,169
versions of this new one once the bucket

00:17:09,220 --> 00:17:15,459
is full you can see the bucket and you

00:17:11,169 --> 00:17:18,459
can process it we typically use micro

00:17:15,459 --> 00:17:19,750
batching for to establish a context this

00:17:18,459 --> 00:17:21,339
means I want to find out what's

00:17:19,750 --> 00:17:22,780
happening in my environment I don't want

00:17:21,339 --> 00:17:25,780
to react to the environment what other

00:17:22,780 --> 00:17:28,750
what's happening how many do I have any

00:17:25,780 --> 00:17:30,940
bots on the system to trying to do to do

00:17:28,750 --> 00:17:32,710
give me wrong recommendations do I have

00:17:30,940 --> 00:17:36,850
users who are trying to brute force

00:17:32,710 --> 00:17:38,289
login are there any specific categories

00:17:36,850 --> 00:17:40,360
or products which are selling really

00:17:38,289 --> 00:17:42,309
well my website this is something we'll

00:17:40,360 --> 00:17:44,580
just establish context which is purely

00:17:42,309 --> 00:17:44,580
passive

00:17:47,540 --> 00:17:53,160
so what spark will do for you will take

00:17:50,340 --> 00:17:55,740
events and I'll generate these r dds out

00:17:53,160 --> 00:18:00,030
of this or denise is a concept known to

00:17:55,740 --> 00:18:02,430
everybody okay I'll just do a quick

00:18:00,030 --> 00:18:06,590
version the rdd is resilient distributed

00:18:02,430 --> 00:18:09,240
data set and it's a data container

00:18:06,590 --> 00:18:11,190
distributed across the cluster and it's

00:18:09,240 --> 00:18:13,710
in it's shorted across the cluster and

00:18:11,190 --> 00:18:16,350
the rd DS they have like relations to

00:18:13,710 --> 00:18:19,230
each other you can say this is my basic

00:18:16,350 --> 00:18:21,780
creamy I do a filter operation I get a

00:18:19,230 --> 00:18:24,090
new RTD i do a map operation i get a new

00:18:21,780 --> 00:18:27,480
RTD I join it with other data I get a

00:18:24,090 --> 00:18:29,880
new rtd and so you have a whole cascade

00:18:27,480 --> 00:18:34,020
of already DS blondie together and this

00:18:29,880 --> 00:18:35,940
is the basic work unit in spark you can

00:18:34,020 --> 00:18:38,130
fill the RT DS whatever you want to if

00:18:35,940 --> 00:18:40,110
you have Kafka go ahead if you have a

00:18:38,130 --> 00:18:41,580
database you can pull it the new

00:18:40,110 --> 00:18:42,960
versions of Cassandra they're going to

00:18:41,580 --> 00:18:44,429
even support triggers so you can

00:18:42,960 --> 00:18:47,940
actually have like a stored procedure

00:18:44,429 --> 00:18:50,130
running you can have a car or any other

00:18:47,940 --> 00:18:56,550
stream filling your data instead of

00:18:50,130 --> 00:18:58,800
stream these windows they can be flushed

00:18:56,550 --> 00:19:00,450
to persistent storage so what you can do

00:18:58,800 --> 00:19:02,790
is you can run your aggregations so you

00:19:00,450 --> 00:19:08,790
can do your classifications and store

00:19:02,790 --> 00:19:11,429
the high that the data of the high

00:19:08,790 --> 00:19:16,770
information load you can extract it from

00:19:11,429 --> 00:19:18,929
the windows you can also query them you

00:19:16,770 --> 00:19:21,210
can you can Korean modify them with SQL

00:19:18,929 --> 00:19:23,040
or a memory for instance or you can save

00:19:21,210 --> 00:19:25,850
the results again so what I'm trying to

00:19:23,040 --> 00:19:28,620
say is any kind of events you just

00:19:25,850 --> 00:19:30,210
window it and you do a gregation zana

00:19:28,620 --> 00:19:32,850
stand this is the context you can you

00:19:30,210 --> 00:19:35,010
can work with if you know this user is

00:19:32,850 --> 00:19:36,630
looking at different products he looked

00:19:35,010 --> 00:19:38,520
at in the past you can you can probably

00:19:36,630 --> 00:19:42,720
assume something changed in this life if

00:19:38,520 --> 00:19:44,610
you see that users that a user is trying

00:19:42,720 --> 00:19:48,500
to hack the system because he's giving

00:19:44,610 --> 00:19:48,500
wrong IP addresses you can react to this

00:19:54,850 --> 00:19:59,659
not a nice thing is we've you seen this

00:19:57,769 --> 00:20:01,850
quite often in the projects that always

00:19:59,659 --> 00:20:02,990
clear when do i do wanna calculation in

00:20:01,850 --> 00:20:06,080
the beginning you want to do everything

00:20:02,990 --> 00:20:08,390
in memory but then after all you realize

00:20:06,080 --> 00:20:10,220
you're building such large windows you

00:20:08,390 --> 00:20:12,200
having so much memory in data that

00:20:10,220 --> 00:20:14,419
application is getting sloggi so there's

00:20:12,200 --> 00:20:16,640
the developers will always be shifting

00:20:14,419 --> 00:20:19,190
lodging around between stream processing

00:20:16,640 --> 00:20:21,289
and batch processing the nice thing is

00:20:19,190 --> 00:20:23,929
if you're working with spark I assume

00:20:21,289 --> 00:20:26,600
this is the same fling nowadays is if

00:20:23,929 --> 00:20:28,549
you're having aggregation in aggregation

00:20:26,600 --> 00:20:30,889
runs on the fast lane or in the batch

00:20:28,549 --> 00:20:34,730
Lane doesn't really matter because it's

00:20:30,889 --> 00:20:36,919
the same RP so what we what we learned

00:20:34,730 --> 00:20:38,690
is that this flexibility between moving

00:20:36,919 --> 00:20:41,870
data from fast or slow lane or

00:20:38,690 --> 00:20:52,070
operations it happens more often than

00:20:41,870 --> 00:20:53,750
you expect okay now we have a feeling

00:20:52,070 --> 00:20:55,880
that I know what's happening the system

00:20:53,750 --> 00:20:58,220
I know if I have any for our general

00:20:55,880 --> 00:21:00,710
users I know if have any BOTS but it's

00:20:58,220 --> 00:21:02,240
not no it's not enough enough to know it

00:21:00,710 --> 00:21:04,820
something's happening you have to act

00:21:02,240 --> 00:21:06,799
upon this okay so when you found the

00:21:04,820 --> 00:21:09,019
body we want the app the back to stop

00:21:06,799 --> 00:21:10,940
when they've classified my user I want

00:21:09,019 --> 00:21:12,889
to decide which ad do I want to show

00:21:10,940 --> 00:21:15,080
which off upsell offering do I want to

00:21:12,889 --> 00:21:19,700
show and this is pretty much where we

00:21:15,080 --> 00:21:22,309
typically use acha so what we can do is

00:21:19,700 --> 00:21:25,460
that when the when we get the request

00:21:22,309 --> 00:21:27,860
the show me an ad or some an upsell you

00:21:25,460 --> 00:21:29,990
give acha the request let acha handled

00:21:27,860 --> 00:21:31,820
it okay a knocker I has some pretty

00:21:29,990 --> 00:21:34,309
strong friends the background you can go

00:21:31,820 --> 00:21:38,630
to spark you can go to cassandra and ask

00:21:34,309 --> 00:21:40,519
him give me a context ok naka also has a

00:21:38,630 --> 00:21:43,010
nice way that since everything's a

00:21:40,519 --> 00:21:44,990
sickness lean passed down you can you

00:21:43,010 --> 00:21:49,309
can actually deal with your own timeouts

00:21:44,990 --> 00:21:50,799
you can set a locker okay acha ask spark

00:21:49,309 --> 00:21:53,960
what Stephanus looking at right now

00:21:50,799 --> 00:21:56,059
Stefan earth ask Cassandra buddy would

00:21:53,960 --> 00:21:57,889
it last year and if I can't make up my

00:21:56,059 --> 00:22:00,169
mind or if Cassandra is too slow or

00:21:57,889 --> 00:22:02,690
whatever I respond i'm gonna make a

00:22:00,169 --> 00:22:04,039
best-effort estimate and just give him

00:22:02,690 --> 00:22:05,120
the product which was shown the most

00:22:04,039 --> 00:22:08,600
time

00:22:05,120 --> 00:22:11,210
was all the most time so acha you can

00:22:08,600 --> 00:22:13,430
build his hierarchies of strategies and

00:22:11,210 --> 00:22:15,590
how to deal with a request and also

00:22:13,430 --> 00:22:20,780
having worked in different service

00:22:15,590 --> 00:22:25,130
levels does that make sense if you don't

00:22:20,780 --> 00:22:33,920
say yes I'm going to repeat it okay at

00:22:25,130 --> 00:22:35,390
least some knots so we mentioned the old

00:22:33,920 --> 00:22:40,160
problem that we want to have a reliable

00:22:35,390 --> 00:22:41,990
ingestion it's the old school I mean

00:22:40,160 --> 00:22:43,490
when I did enterprise Java I was

00:22:41,990 --> 00:22:46,280
typically you had a couple thousand

00:22:43,490 --> 00:22:48,590
requests per hour and if the request

00:22:46,280 --> 00:22:51,559
would collect over the wild was okay

00:22:48,590 --> 00:22:54,320
okay because if you're storing millions

00:22:51,559 --> 00:22:56,990
or thousands or billions of rows didn't

00:22:54,320 --> 00:22:59,450
really hurt you that much the problem is

00:22:56,990 --> 00:23:01,640
nowadays there there's so much load so

00:22:59,450 --> 00:23:03,230
much data being passed around you have

00:23:01,640 --> 00:23:05,809
to be a little more careful yeah if

00:23:03,230 --> 00:23:07,910
friday night a system crashes and it's

00:23:05,809 --> 00:23:10,040
filling your ftp server with 30

00:23:07,910 --> 00:23:13,340
gigabytes an hour you probably have to

00:23:10,040 --> 00:23:15,830
react over the weekend okay because the

00:23:13,340 --> 00:23:17,420
the the boats are keeping or getting

00:23:15,830 --> 00:23:19,100
bigger you're dealing with and the

00:23:17,420 --> 00:23:27,800
bigger the boat the battery is when it

00:23:19,100 --> 00:23:29,780
sinks so um core concept of this whole

00:23:27,800 --> 00:23:34,760
system are the reactive streams anybody

00:23:29,780 --> 00:23:38,510
heard of them before okay it's going to

00:23:34,760 --> 00:23:39,800
be boring for you guys I'm sorry the but

00:23:38,510 --> 00:23:41,090
the basic idea is you have three

00:23:39,800 --> 00:23:42,740
different kinds of streams there's a

00:23:41,090 --> 00:23:45,080
direct stream I'm not going to dive into

00:23:42,740 --> 00:23:46,700
this between Kafka and spark but there's

00:23:45,080 --> 00:23:48,650
something like the raw streams this is

00:23:46,700 --> 00:23:51,320
what we typically avah developers know

00:23:48,650 --> 00:23:53,210
this is when I consume something from a

00:23:51,320 --> 00:23:57,320
TCP connection or when I want to push

00:23:53,210 --> 00:24:00,679
something into a TCP connection and

00:23:57,320 --> 00:24:02,450
they're reactive streams the they behave

00:24:00,679 --> 00:24:04,429
like streams but they have a much better

00:24:02,450 --> 00:24:09,260
back pressure support back pressure

00:24:04,429 --> 00:24:11,020
known to anybody ok this is this is flow

00:24:09,260 --> 00:24:13,520
controls is when you still have the old

00:24:11,020 --> 00:24:14,810
modems you can adjust the flow control

00:24:13,520 --> 00:24:17,750
if you want to have software hardware

00:24:14,810 --> 00:24:18,630
flow control the flow controlled hell's

00:24:17,750 --> 00:24:21,840
basically

00:24:18,630 --> 00:24:24,860
is it tells the system stop talking to

00:24:21,840 --> 00:24:28,140
me I have enough information right now

00:24:24,860 --> 00:24:30,810
okay this just means if I buffer is full

00:24:28,140 --> 00:24:32,340
it's saturated and if you give me more

00:24:30,810 --> 00:24:34,830
information I'm just one I just won't

00:24:32,340 --> 00:24:37,950
listen to you anymore so this is this is

00:24:34,830 --> 00:24:40,140
something which creates back pressure if

00:24:37,950 --> 00:24:48,750
you if you're pushing data in too fast

00:24:40,140 --> 00:24:51,060
for a system to consume it basically

00:24:48,750 --> 00:24:52,680
this is physics and then the back

00:24:51,060 --> 00:24:54,780
pressure is if you take a strong you

00:24:52,680 --> 00:24:56,610
blow through it and then tighter the

00:24:54,780 --> 00:24:58,560
holes destroys the more back pressure

00:24:56,610 --> 00:25:02,340
you have this is a back pressure in

00:24:58,560 --> 00:25:08,880
American how so called wastewater system

00:25:02,340 --> 00:25:10,440
with the manhole covers blown off but we

00:25:08,880 --> 00:25:14,790
look at this in a second again just keep

00:25:10,440 --> 00:25:19,440
this in mind now so how does the data

00:25:14,790 --> 00:25:24,990
flow in and out of the system preferably

00:25:19,440 --> 00:25:26,880
over Casca why it's it's an append only

00:25:24,990 --> 00:25:29,130
system again this means that consumers

00:25:26,880 --> 00:25:30,570
may be offline for days so if you write

00:25:29,130 --> 00:25:33,150
some kind of logic with machine learning

00:25:30,570 --> 00:25:35,160
which processes your data you want to

00:25:33,150 --> 00:25:36,840
have the data coming in over kashka so

00:25:35,160 --> 00:25:38,940
in case your machine learning algorithm

00:25:36,840 --> 00:25:41,490
crashes or whatever you still have the

00:25:38,940 --> 00:25:44,940
data you can really put a repeat that

00:25:41,490 --> 00:25:46,350
analysis you won't have a broker broker

00:25:44,940 --> 00:25:49,830
reduces the number of point-to-point

00:25:46,350 --> 00:25:51,690
connections all you're a cop doesn't

00:25:49,830 --> 00:25:54,480
have to know all the other acha jobs it

00:25:51,690 --> 00:25:59,820
just has to know its broker and what the

00:25:54,480 --> 00:26:02,160
topics to use you can use routing on

00:25:59,820 --> 00:26:05,070
streams you can multiplex demultiplex

00:26:02,160 --> 00:26:07,280
events example was from the thought

00:26:05,070 --> 00:26:09,990
before sometimes it makes sense to

00:26:07,280 --> 00:26:12,210
monitor trains individually so each

00:26:09,990 --> 00:26:14,130
strain gets its own topic sometimes

00:26:12,210 --> 00:26:16,500
makes more sense to analyze all trains

00:26:14,130 --> 00:26:19,350
of a certain mark of a certain

00:26:16,500 --> 00:26:20,880
manufacturer maybe you want to Wilda

00:26:19,350 --> 00:26:22,590
Plex your events different kind of way

00:26:20,880 --> 00:26:27,840
this is something which you can do quite

00:26:22,590 --> 00:26:31,820
elegantly in Kafka we'll have a look at

00:26:27,840 --> 00:26:34,010
this in a second you will have overloads

00:26:31,820 --> 00:26:36,020
mechanisms you can react to this you can

00:26:34,010 --> 00:26:38,390
either scale up on the hardware level

00:26:36,020 --> 00:26:42,260
you can scale out in the hardware level

00:26:38,390 --> 00:26:43,880
I'm sorry you can also tell the client

00:26:42,260 --> 00:26:47,720
to communicate list or you can just

00:26:43,880 --> 00:26:51,620
buffer it and the buffer is the first

00:26:47,720 --> 00:26:55,010
line of defense and the best part about

00:26:51,620 --> 00:26:57,860
acha about casca is my point of view the

00:26:55,010 --> 00:27:00,080
replay mechanism you usually have a

00:26:57,860 --> 00:27:02,510
couple days worth of data in your calf

00:27:00,080 --> 00:27:06,260
 you and your Kafka system this is

00:27:02,510 --> 00:27:08,360
especially nice because most big data

00:27:06,260 --> 00:27:10,790
environments I've seen so far they have

00:27:08,360 --> 00:27:12,560
very little possibilities to do load

00:27:10,790 --> 00:27:15,110
tests & test environments yeah because

00:27:12,560 --> 00:27:16,940
you can't have a test environment which

00:27:15,110 --> 00:27:18,850
cost you millions of dollars a year most

00:27:16,940 --> 00:27:21,530
customers won't do this so you usually

00:27:18,850 --> 00:27:23,090
do a lot of a lot of deployments

00:27:21,530 --> 00:27:24,440
directly into production which is fine

00:27:23,090 --> 00:27:27,860
if you know if you know what you're

00:27:24,440 --> 00:27:29,930
doing if you deploy broken algorithm

00:27:27,860 --> 00:27:31,850
which marks all of your customers was

00:27:29,930 --> 00:27:33,500
fraud customers then you have a big

00:27:31,850 --> 00:27:36,440
boo-boo because they can't order there

00:27:33,500 --> 00:27:40,390
anymore fix your algorithm replay the

00:27:36,440 --> 00:27:42,440
data from Kafka and you can fix the data

00:27:40,390 --> 00:27:44,540
okay I think this I think this is the

00:27:42,440 --> 00:27:46,640
best part cuz I'm always relaxed doing

00:27:44,540 --> 00:27:48,680
the release in production when I know I

00:27:46,640 --> 00:27:57,890
can get out of it again this is your

00:27:48,680 --> 00:27:59,840
ticket out okay this is um I'm a German

00:27:57,890 --> 00:28:02,900
guy we drum engineers who sometimes

00:27:59,840 --> 00:28:04,640
really tough giving talks and add this

00:28:02,900 --> 00:28:07,160
one guy and he talked to me for an hour

00:28:04,640 --> 00:28:12,620
after talk saying Kafka was because

00:28:07,160 --> 00:28:14,570
it didn't support exactly once nothing

00:28:12,620 --> 00:28:16,520
works exactly once in distributed system

00:28:14,570 --> 00:28:18,560
so don't even think about doing it just

00:28:16,520 --> 00:28:20,690
right your systems I didn't pretend so

00:28:18,560 --> 00:28:22,730
you can deal with double requests okay

00:28:20,690 --> 00:28:31,040
but this is probably for another

00:28:22,730 --> 00:28:37,300
audience okay cloud bare metal who's

00:28:31,040 --> 00:28:37,300
running in the cloud public or your own

00:28:38,169 --> 00:28:46,570
okay you can there are multiple reasons

00:28:44,499 --> 00:28:48,580
to do things different ways some

00:28:46,570 --> 00:28:53,350
customers they have data privacy issue

00:28:48,580 --> 00:28:55,659
some customers are worried about the own

00:28:53,350 --> 00:28:57,309
IT department but in the whole smack

00:28:55,659 --> 00:29:00,039
stack since you're running with Messrs

00:28:57,309 --> 00:29:02,619
you can do both message does not require

00:29:00,039 --> 00:29:04,539
any virtualization that's the require

00:29:02,619 --> 00:29:07,929
information infrastructure as a service

00:29:04,539 --> 00:29:10,330
you can run mezzos directly on the bare

00:29:07,929 --> 00:29:13,179
metal if you want to but if you have a

00:29:10,330 --> 00:29:18,549
cloud you have redundancy and elasticity

00:29:13,179 --> 00:29:20,470
in it go ahead and use it I mentioned

00:29:18,549 --> 00:29:22,989
this before you do not eat any

00:29:20,470 --> 00:29:26,139
ritualization or continues ation so if

00:29:22,989 --> 00:29:28,450
you can I would try to run tools like

00:29:26,139 --> 00:29:31,450
Kafka tools like spark I'd like to run

00:29:28,450 --> 00:29:33,249
them natively on the host because

00:29:31,450 --> 00:29:35,230
there's no reason you want to separate

00:29:33,249 --> 00:29:37,169
room there's no reason you need a own

00:29:35,230 --> 00:29:40,389
kernel there's no reason you need a

00:29:37,169 --> 00:29:42,129
share your i/o ops with other processes

00:29:40,389 --> 00:29:45,779
just try to run in bare metal if

00:29:42,129 --> 00:29:45,779
possible but you don't have to

00:29:51,600 --> 00:29:57,600
so i think i'm doing right way too fast

00:29:54,600 --> 00:29:59,780
let's see basically you can do whatever

00:29:57,600 --> 00:30:02,880
you want to there's this there's no

00:29:59,780 --> 00:30:05,520
governing agency telling you how to use

00:30:02,880 --> 00:30:08,309
smack as I mentioned before I try to use

00:30:05,520 --> 00:30:11,250
Kafka as much as possible if you have

00:30:08,309 --> 00:30:13,289
systems which have to which require

00:30:11,250 --> 00:30:15,720
response milliseconds because they're

00:30:13,289 --> 00:30:17,160
coming in over web socket or the coming

00:30:15,720 --> 00:30:19,620
of the rest service you probably want to

00:30:17,160 --> 00:30:23,429
process directly naka and have a car to

00:30:19,620 --> 00:30:26,039
do the service level agreements but if

00:30:23,429 --> 00:30:28,500
this can be done later always push it

00:30:26,039 --> 00:30:32,190
push it through Kafka your Aaka can ask

00:30:28,500 --> 00:30:34,860
spark your sparking right read from

00:30:32,190 --> 00:30:37,919
Cassandra to read and write from HDFS so

00:30:34,860 --> 00:30:40,289
this is a lot of customers have

00:30:37,919 --> 00:30:43,110
different flows like one customer they

00:30:40,289 --> 00:30:45,179
had a lot they used I UT that a lot of

00:30:43,110 --> 00:30:47,159
data coming in so we push them in the

00:30:45,179 --> 00:30:48,630
kafka we would have a spark chop

00:30:47,159 --> 00:30:51,780
listening which would create a right

00:30:48,630 --> 00:30:54,929
repeat log in HDFS so they could keep it

00:30:51,780 --> 00:30:57,150
over years could make sense doesn't have

00:30:54,929 --> 00:30:59,669
to make sense Nessa said before if you

00:30:57,150 --> 00:31:01,590
have a droid here you can you can sit

00:30:59,669 --> 00:31:03,840
right here if you have elasticsearch

00:31:01,590 --> 00:31:07,260
running there or HBase go ahead nobody's

00:31:03,840 --> 00:31:10,350
going to stop you but it's important for

00:31:07,260 --> 00:31:13,830
me at least that's that's my two cents I

00:31:10,350 --> 00:31:17,010
learned from from big data is the tools

00:31:13,830 --> 00:31:19,200
or so they have very narrow sweet spots

00:31:17,010 --> 00:31:22,049
they do very few things but they do it

00:31:19,200 --> 00:31:24,360
very very well I grew up with Miss

00:31:22,049 --> 00:31:26,220
Oracle and database and then db2 systems

00:31:24,360 --> 00:31:27,900
and everybody told you use Oracle use

00:31:26,220 --> 00:31:30,150
Oracle for any problem we had but this

00:31:27,900 --> 00:31:32,809
is changing now and this should reflect

00:31:30,150 --> 00:31:32,809
in your stacks

00:31:36,710 --> 00:31:47,000
does it make sense okay look at the

00:31:43,460 --> 00:31:48,830
streams one more time thanks stream

00:31:47,000 --> 00:31:50,480
processing probably her saw this slide

00:31:48,830 --> 00:31:52,490
and thousands of different variations

00:31:50,480 --> 00:31:55,760
last couple two days there an unbound

00:31:52,490 --> 00:31:57,980
and continuous sequence of events this

00:31:55,760 --> 00:32:00,260
means we do not know when that will end

00:31:57,980 --> 00:32:02,690
we do not know how many events will be

00:32:00,260 --> 00:32:05,480
coming in and all of this is open and I

00:32:02,690 --> 00:32:07,159
think these Layton's these shifts load

00:32:05,480 --> 00:32:09,860
are very important as well especially if

00:32:07,159 --> 00:32:13,390
you diverted iut environment you do not

00:32:09,860 --> 00:32:13,390
know when your sensors are going to fire

00:32:15,130 --> 00:32:23,809
so what do I do typically when I work

00:32:19,520 --> 00:32:26,059
the streams um I usually do not have

00:32:23,809 --> 00:32:27,860
this problems long periods of time and

00:32:26,059 --> 00:32:30,590
threatening fluctuation load usually

00:32:27,860 --> 00:32:35,029
even I deal with streams I do custom

00:32:30,590 --> 00:32:36,710
buffer copying I would create a buffered

00:32:35,029 --> 00:32:40,100
streams whatever it is what I usually

00:32:36,710 --> 00:32:44,090
did the Java were all right if I would

00:32:40,100 --> 00:32:46,429
use a buffer to which is bound I can I

00:32:44,090 --> 00:32:48,740
can survive more okay when the client is

00:32:46,429 --> 00:32:50,960
pushing more down Anakin process I can

00:32:48,740 --> 00:32:53,870
always use the bound buffer the if it's

00:32:50,960 --> 00:32:56,059
ram or 42 hard disk or database to say

00:32:53,870 --> 00:32:57,320
okay if it doesn't fit in my register

00:32:56,059 --> 00:33:00,740
anymore just push it off to another

00:32:57,320 --> 00:33:03,860
system but these buffers can exhaust

00:33:00,740 --> 00:33:06,529
quite quickly so the questions always

00:33:03,860 --> 00:33:08,510
what do I do when the custom in a client

00:33:06,529 --> 00:33:10,789
is sending more than I can process do i

00:33:08,510 --> 00:33:13,130
drop the old data do i drop the new data

00:33:10,789 --> 00:33:16,010
or do I just reduce the sampling rate

00:33:13,130 --> 00:33:22,610
and ignore every tenth every first every

00:33:16,010 --> 00:33:24,470
second request now this is where

00:33:22,610 --> 00:33:26,809
reactive streams come in they they have

00:33:24,470 --> 00:33:29,840
a nice mechanism and interactive streams

00:33:26,809 --> 00:33:34,070
you have like a pearl necklace or graph

00:33:29,840 --> 00:33:38,360
of consumers listening to previous

00:33:34,070 --> 00:33:41,720
consumers okay so for instance I know

00:33:38,360 --> 00:33:43,789
that I curse working on the stream it's

00:33:41,720 --> 00:33:45,470
doing some selects then it comes to me I

00:33:43,789 --> 00:33:47,389
should actor I should write some

00:33:45,470 --> 00:33:49,220
aggregations to database and then it

00:33:47,389 --> 00:33:50,090
would pass to recurse on the next system

00:33:49,220 --> 00:33:53,720
to do something

00:33:50,090 --> 00:33:56,600
with it and usually this works in the

00:33:53,720 --> 00:33:58,490
push mechanism so the so the practices

00:33:56,600 --> 00:34:01,909
are pushes events to me and I just

00:33:58,490 --> 00:34:03,890
consume them now what happens is when

00:34:01,909 --> 00:34:07,100
I'm too slow and I can that process the

00:34:03,890 --> 00:34:09,409
data anymore the predecessor is informed

00:34:07,100 --> 00:34:12,200
nice clear switch to a pool mechanism so

00:34:09,409 --> 00:34:13,940
it's very human so when I come home and

00:34:12,200 --> 00:34:16,490
my wife talks to me and I'm just

00:34:13,940 --> 00:34:18,730
overloaded I tell her I'll pick up I'll

00:34:16,490 --> 00:34:21,109
come to you and ask you in 10 minutes

00:34:18,730 --> 00:34:23,899
okay and this is how the reactive

00:34:21,109 --> 00:34:26,359
streams work and this and this mechanism

00:34:23,899 --> 00:34:28,099
does this this fail-over can go through

00:34:26,359 --> 00:34:29,839
the entire graph so it can start the

00:34:28,099 --> 00:34:32,780
very end where somebody's not fast

00:34:29,839 --> 00:34:34,730
enough and it's failover of push to pull

00:34:32,780 --> 00:34:37,849
is going to go through the entire graph

00:34:34,730 --> 00:34:41,359
until until the source and the source

00:34:37,849 --> 00:34:45,560
has to decide what to do with this okay

00:34:41,359 --> 00:34:48,740
if the source is some kind of a car HTTP

00:34:45,560 --> 00:34:51,230
claw server it has to know what to do do

00:34:48,740 --> 00:34:53,960
I stop listening to clients do I cue the

00:34:51,230 --> 00:34:56,210
results but if your source is Kafka

00:34:53,960 --> 00:34:59,150
everything's easy because Kafka is

00:34:56,210 --> 00:35:03,470
capable of buffering this data for days

00:34:59,150 --> 00:35:05,750
and weeks and as soon as your consumer

00:35:03,470 --> 00:35:08,390
picks up again because this congestion

00:35:05,750 --> 00:35:09,830
is removed the whole failback is going

00:35:08,390 --> 00:35:13,369
to revert itself is going to fall back

00:35:09,830 --> 00:35:15,230
to a push mechanism and these are the

00:35:13,369 --> 00:35:18,530
kind of nifty things which save your

00:35:15,230 --> 00:35:25,430
neck later in production okay so it's

00:35:18,530 --> 00:35:27,500
not like you know the problems when you

00:35:25,430 --> 00:35:29,270
come to work and you in and you see the

00:35:27,500 --> 00:35:31,550
locks are full of exceptions you can see

00:35:29,270 --> 00:35:34,010
a lot of poisonous pills and your back

00:35:31,550 --> 00:35:42,050
locks what happens you don't really know

00:35:34,010 --> 00:35:44,390
the system here can help itself blah

00:35:42,050 --> 00:35:46,490
blah blah blah basically the slide says

00:35:44,390 --> 00:35:50,089
that all these components we talked so

00:35:46,490 --> 00:35:54,440
far use the same protocol of reactive

00:35:50,089 --> 00:35:56,150
streams so you can what what comes out

00:35:54,440 --> 00:35:59,060
of spark streaming you can you reuse

00:35:56,150 --> 00:36:00,890
again which is reactive icona supports

00:35:59,060 --> 00:36:03,619
it so you could this whole flow to hop

00:36:00,890 --> 00:36:04,510
it is completely open and as I mentioned

00:36:03,619 --> 00:36:06,920
before

00:36:04,510 --> 00:36:10,490
kafka is a perfect candidate for a bound

00:36:06,920 --> 00:36:13,190
buffer use it as much as possible and

00:36:10,490 --> 00:36:18,320
measles can scale consumers on the fly

00:36:13,190 --> 00:36:22,070
during the fall back so this is my mo ok

00:36:18,320 --> 00:36:24,410
I'm done a second what Maysles can do it

00:36:22,070 --> 00:36:26,750
can scale up so for instance if you have

00:36:24,410 --> 00:36:28,400
a stream consumed where we're trying to

00:36:26,750 --> 00:36:30,860
write to cassandra and cassandra is

00:36:28,400 --> 00:36:33,020
getting too slow measles can go ahead

00:36:30,860 --> 00:36:35,540
and do a scale up on your cassandra

00:36:33,020 --> 00:36:36,830
installation but this takes time if you

00:36:35,540 --> 00:36:39,530
have a big cassandra distribute

00:36:36,830 --> 00:36:41,210
installation and you should activate

00:36:39,530 --> 00:36:44,450
more nodes it's going to rebalance the

00:36:41,210 --> 00:36:47,180
data and it can take 30-45 minutes so in

00:36:44,450 --> 00:36:49,280
this time your measles is scaling up you

00:36:47,180 --> 00:36:51,230
can have Kafka covering your back then

00:36:49,280 --> 00:36:53,600
missus is going to be up then cuff

00:36:51,230 --> 00:36:57,580
Kraken can switch over to a regular mode

00:36:53,600 --> 00:37:05,540
again and you can work off the back load

00:36:57,580 --> 00:37:07,100
does that make sense we've been a

00:37:05,540 --> 00:37:09,980
type-safe partner for quite some time

00:37:07,100 --> 00:37:13,130
but we've never seen so much interest in

00:37:09,980 --> 00:37:15,560
spark and scala then since we didn't

00:37:13,130 --> 00:37:18,530
spend working spark projects and i don't

00:37:15,560 --> 00:37:21,440
think it's a coincidence so screams love

00:37:18,530 --> 00:37:23,660
to be processed in parallel ok and

00:37:21,440 --> 00:37:25,760
streams love scala so every time you do

00:37:23,660 --> 00:37:27,230
something sparked you in a scholar i saw

00:37:25,760 --> 00:37:29,750
all the flink examples were written in

00:37:27,230 --> 00:37:32,710
scala because it has some features

00:37:29,750 --> 00:37:36,140
buildings are quite nice you have this

00:37:32,710 --> 00:37:37,790
built-in preference for immutability so

00:37:36,140 --> 00:37:40,040
when you working when you work on

00:37:37,790 --> 00:37:41,780
streams never manipulate the events

00:37:40,040 --> 00:37:43,630
create new events and push them down to

00:37:41,780 --> 00:37:47,050
another stream this way it's always

00:37:43,630 --> 00:37:49,850
reproducible what happened to the events

00:37:47,050 --> 00:37:51,980
Scala has this notion of having a better

00:37:49,850 --> 00:37:53,840
support for functions functions meaning

00:37:51,980 --> 00:37:56,210
that you don't have any side effects of

00:37:53,840 --> 00:37:58,310
a function if you look at a typical Java

00:37:56,210 --> 00:38:00,740
construction like a servlet you can have

00:37:58,310 --> 00:38:02,750
instance variables bound to disturb the

00:38:00,740 --> 00:38:05,270
instance meaning that the service will

00:38:02,750 --> 00:38:07,040
respond in different ways depending on

00:38:05,270 --> 00:38:08,600
when you ask the servlet but you don't

00:38:07,040 --> 00:38:12,370
want this in functions functions should

00:38:08,600 --> 00:38:14,600
be neutral to any side effects and

00:38:12,370 --> 00:38:16,730
another nice thing this is where I

00:38:14,600 --> 00:38:17,900
started love JavaScript when you can use

00:38:16,730 --> 00:38:23,180
functions first class

00:38:17,900 --> 00:38:30,260
citizens and pass them around I'll skip

00:38:23,180 --> 00:38:40,670
the inverted index example but like to

00:38:30,260 --> 00:38:44,360
show my kitten slide any questions any

00:38:40,670 --> 00:38:54,650
feedback including oh my god this was

00:38:44,360 --> 00:38:58,820
boring or one moment thanks for the talk

00:38:54,650 --> 00:39:01,700
is there any really huge case a really

00:38:58,820 --> 00:39:07,430
huge case you've applied will this case

00:39:01,700 --> 00:39:08,600
for the smack stack um what I mean is

00:39:07,430 --> 00:39:10,580
real use case are you talking about

00:39:08,600 --> 00:39:14,320
projects we're doing are you talking yes

00:39:10,580 --> 00:39:14,320
there are projects for this it's

00:39:15,880 --> 00:39:20,750
emphasizes really strongly about context

00:39:18,440 --> 00:39:22,310
detection and context reaction and as

00:39:20,750 --> 00:39:25,430
soon as you have a problem where the

00:39:22,310 --> 00:39:27,230
context does not fit into a heap stack

00:39:25,430 --> 00:39:32,630
at the browser anymore then you have a

00:39:27,230 --> 00:39:34,580
use case so if you have a website and

00:39:32,630 --> 00:39:36,530
you're dealing with million customers

00:39:34,580 --> 00:39:38,060
and you want to classify the customers

00:39:36,530 --> 00:39:40,970
everything you're probably better

00:39:38,060 --> 00:39:43,580
writing everything in regular java.util

00:39:40,970 --> 00:39:46,400
classes but as soon as your context gets

00:39:43,580 --> 00:39:48,590
too big to decide on this on the fly or

00:39:46,400 --> 00:39:50,330
dis items in one machine you want to

00:39:48,590 --> 00:39:58,610
you're moving towards this max stack

00:39:50,330 --> 00:40:01,760
then that's my feeling please discuss so

00:39:58,610 --> 00:40:03,440
our examples was we have two customers

00:40:01,760 --> 00:40:06,950
from the automotive industry where we're

00:40:03,440 --> 00:40:08,720
doing these evaluation if all the

00:40:06,950 --> 00:40:10,990
sensors to put the cars are delivering

00:40:08,720 --> 00:40:13,850
we have one customer doing real-time

00:40:10,990 --> 00:40:17,080
advertising and those are the most

00:40:13,850 --> 00:40:17,080
interesting ones at the moment

00:40:20,940 --> 00:40:30,060
any more thoughts any more questions to

00:40:24,550 --> 00:40:30,060

YouTube URL: https://www.youtube.com/watch?v=ZNrDzjxkGrE


