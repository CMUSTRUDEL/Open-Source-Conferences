Title: Berlin Buzzwords 2016: Andrew Psaltis - Help I need a stream processor - learning how to chose ...
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Today if a byte of data were a gallon of water, in only 10 seconds there would be enough data to fill an average home, in 2020 it will only take 2 seconds. With this explosive growth comes the demand from consumers and businesses to leverage and act on what is happening right now. Without stream processing these demands will never be met, and there will be no big data and no Internet of Things. It is only a matter of time before you will be faced with building a real-time streaming pipeline. 

As soon as you embark on this journey, you will be faced with a myriad of questions. A major key decision you will need to quickly answer is which stream-processing framework should you use? When you survey the landscape you will find many contenders. In this session we will focus on the most popular open source frameworks, in particular: Apache Spark Streaming, Apache Storm, Apache Flink, and Apache Samza. 

We will dive into each of these tools and tease out all of the essential pieces you need to consider, compare and contrast them and end up with an understanding of how to evaluate each as well as future products.

Read more:
https://2016.berlinbuzzwords.de/session/help-i-need-stream-processor-learning-how-chose-between-spark-flink-samza-and-storm

About Andrew Psaltis:
https://2016.berlinbuzzwords.de/users/andrew-psaltis

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,210 --> 00:00:06,660
stand here and go over benchmarks of

00:00:04,470 --> 00:00:09,389
which one's faster yeah but really to

00:00:06,660 --> 00:00:10,469
kind of go over the high-level concepts

00:00:09,389 --> 00:00:12,090
that you need to keep in mind that

00:00:10,469 --> 00:00:15,000
you're going to find throughout all

00:00:12,090 --> 00:00:17,490
these tools those fundamental concepts

00:00:15,000 --> 00:00:19,590
really don't change as the projects

00:00:17,490 --> 00:00:23,070
mature there's somewhat baked in and

00:00:19,590 --> 00:00:24,869
changing them could be hard so let's

00:00:23,070 --> 00:00:27,300
start with kind of thinking through a

00:00:24,869 --> 00:00:29,550
scenario here to get things going so

00:00:27,300 --> 00:00:32,070
let's imagine that we're working for an

00:00:29,550 --> 00:00:35,480
energy company that traditionally has

00:00:32,070 --> 00:00:39,059
gotten one record of data every 30 days

00:00:35,480 --> 00:00:40,559
small format consistent data structure

00:00:39,059 --> 00:00:43,170
put it in your data warehouse

00:00:40,559 --> 00:00:45,930
everything's good but now you kind of

00:00:43,170 --> 00:00:48,480
want to take advantage of this IOT stuff

00:00:45,930 --> 00:00:49,739
going on jump on that bandwagon and you

00:00:48,480 --> 00:00:52,920
decide you're going to deploy power

00:00:49,739 --> 00:00:54,420
meters that are smart meters and as you

00:00:52,920 --> 00:00:56,520
do that now you're going to have

00:00:54,420 --> 00:00:58,050
commercial businesses that instead of

00:00:56,520 --> 00:01:00,480
sending you a record once a month are

00:00:58,050 --> 00:01:03,149
now going to send you a reading fifth

00:01:00,480 --> 00:01:05,250
every five minutes throughout the day so

00:01:03,149 --> 00:01:06,810
from one time to somewhere around like

00:01:05,250 --> 00:01:09,510
eighty five hundred times a month and

00:01:06,810 --> 00:01:11,549
then residential customers can't leave

00:01:09,510 --> 00:01:13,860
them out instead of that one time a

00:01:11,549 --> 00:01:16,140
month in this fixed structure are now

00:01:13,860 --> 00:01:16,979
going to be about every 15 minutes send

00:01:16,140 --> 00:01:21,479
ends up being somewhere around like

00:01:16,979 --> 00:01:23,369
2,500 3,000 a month so what do you do in

00:01:21,479 --> 00:01:25,590
the course working for progressive

00:01:23,369 --> 00:01:27,060
energy company maybe want to provide

00:01:25,590 --> 00:01:29,009
different types of services to customers

00:01:27,060 --> 00:01:31,320
all right you want to allow yours

00:01:29,009 --> 00:01:33,960
consumers to log in and see what their

00:01:31,320 --> 00:01:36,900
power consumption is maybe you have an

00:01:33,960 --> 00:01:39,170
idea that you want to be able to incent

00:01:36,900 --> 00:01:42,420
people to use appliances at off hours

00:01:39,170 --> 00:01:43,829
well discount your energy bill if you

00:01:42,420 --> 00:01:47,070
run your washing machine at night or

00:01:43,829 --> 00:01:49,079
your dishwasher at night and I'd imagine

00:01:47,070 --> 00:01:51,030
that perhaps we want to look at our

00:01:49,079 --> 00:01:53,610
overall system usage to make

00:01:51,030 --> 00:01:55,530
infrastructure decisions all right so if

00:01:53,610 --> 00:01:58,530
you think about that it's a streaming

00:01:55,530 --> 00:02:00,299
problem that we would now have right so

00:01:58,530 --> 00:02:02,549
now we've got to set out to figure out

00:02:00,299 --> 00:02:06,180
what are we going to do and what do we

00:02:02,549 --> 00:02:09,629
use this is what we end up looking like

00:02:06,180 --> 00:02:11,250
all right a kid in the candy store he's

00:02:09,629 --> 00:02:13,050
sitting there going okay there's all

00:02:11,250 --> 00:02:13,890
these different projects what do I

00:02:13,050 --> 00:02:17,640
possibly

00:02:13,890 --> 00:02:19,890
as its spark is it flank based on you

00:02:17,640 --> 00:02:22,800
know the conference this week seems like

00:02:19,890 --> 00:02:24,870
flings a good choice right yeah Psalms a

00:02:22,800 --> 00:02:26,790
Kafka streaming they said there's all

00:02:24,870 --> 00:02:28,709
sorts from right as Apache apex there's

00:02:26,790 --> 00:02:32,190
different ones that come out all the

00:02:28,709 --> 00:02:33,720
time concorde know if you've heard of

00:02:32,190 --> 00:02:36,900
that recently came out it's now going to

00:02:33,720 --> 00:02:40,050
be the spark you know d throne spark so

00:02:36,900 --> 00:02:41,520
brand new project so as we start to

00:02:40,050 --> 00:02:42,420
think about this right there's some

00:02:41,520 --> 00:02:44,150
things that we're going to think to

00:02:42,420 --> 00:02:48,150
think about and how do we get out of

00:02:44,150 --> 00:02:50,400
this position the first thing we want to

00:02:48,150 --> 00:02:51,450
do is start thinking about time all

00:02:50,400 --> 00:02:53,489
right so as you're working with the

00:02:51,450 --> 00:02:55,560
stream of data time becomes important

00:02:53,489 --> 00:02:57,209
all rights no longer just at rest but we

00:02:55,560 --> 00:02:59,780
went from having one record every 30

00:02:57,209 --> 00:03:03,269
days going into a warehouse to possibly

00:02:59,780 --> 00:03:05,040
thousands coming in every day every

00:03:03,269 --> 00:03:06,810
month that now you got to figure out

00:03:05,040 --> 00:03:10,800
what to do right not the same structure

00:03:06,810 --> 00:03:13,019
may have all the measurements from every

00:03:10,800 --> 00:03:17,519
device in your house because everyone

00:03:13,019 --> 00:03:20,760
now has a smart house the first thing

00:03:17,519 --> 00:03:22,110
we're going to look at is event time but

00:03:20,760 --> 00:03:25,850
so whether it's coming off of your

00:03:22,110 --> 00:03:28,799
washing machine cell phone smart car

00:03:25,850 --> 00:03:30,239
every time a message is generated that

00:03:28,799 --> 00:03:33,299
would be considered the event time of

00:03:30,239 --> 00:03:34,769
when it actually originated let's just

00:03:33,299 --> 00:03:37,980
assume for now it's going to go into a

00:03:34,769 --> 00:03:39,480
queue kafka is a likely choice could be

00:03:37,980 --> 00:03:40,950
something else but we're going to take

00:03:39,480 --> 00:03:43,620
this data and stream it into a queue

00:03:40,950 --> 00:03:49,590
somehow and coming over the wire from

00:03:43,620 --> 00:03:51,360
our house from our car we then have the

00:03:49,590 --> 00:03:53,130
streaming platform where we're going to

00:03:51,360 --> 00:03:56,160
pull that data in all right we're going

00:03:53,130 --> 00:03:59,549
to receive that data this you could call

00:03:56,160 --> 00:04:01,769
stream time fling folks did a good job

00:03:59,549 --> 00:04:03,870
of calling it ingestion called ingestion

00:04:01,769 --> 00:04:05,370
time stream time really is kind of the

00:04:03,870 --> 00:04:06,870
same thing right it's the point that the

00:04:05,370 --> 00:04:11,070
data is coming into that streaming

00:04:06,870 --> 00:04:13,290
platform whatever one you use after you

00:04:11,070 --> 00:04:15,750
receive it you're they're going to do

00:04:13,290 --> 00:04:17,489
something with it it doesn't matter

00:04:15,750 --> 00:04:19,109
whether it's an operator and flank

00:04:17,489 --> 00:04:20,789
whether it's a bolt and storm where

00:04:19,109 --> 00:04:22,200
there's a transformation spark you're

00:04:20,789 --> 00:04:23,760
going to do something with this data and

00:04:22,200 --> 00:04:26,260
there's going to be a processing time

00:04:23,760 --> 00:04:29,020
where this happens

00:04:26,260 --> 00:04:33,060
so as we look across these you got to

00:04:29,020 --> 00:04:37,960
think about which does which right so

00:04:33,060 --> 00:04:42,820
Psalms a spark storm all handled stream

00:04:37,960 --> 00:04:44,530
time flink handles all these cases okay

00:04:42,820 --> 00:04:52,030
can handle a vent time stream time

00:04:44,530 --> 00:04:54,190
processing time these become important

00:04:52,030 --> 00:04:56,380
as your processing data you're looking

00:04:54,190 --> 00:04:58,330
at things right there's this notion like

00:04:56,380 --> 00:05:00,970
I said of event time and then you have

00:04:58,330 --> 00:05:02,470
stream timer and just in time and then

00:05:00,970 --> 00:05:05,410
if we plot it without processing time

00:05:02,470 --> 00:05:07,600
but what end up happening is you have

00:05:05,410 --> 00:05:08,920
events that are generated going across

00:05:07,600 --> 00:05:12,100
the bottom maybe for your washing

00:05:08,920 --> 00:05:13,480
machine a car whatever it is that are

00:05:12,100 --> 00:05:15,280
coming in at a certain time that a

00:05:13,480 --> 00:05:17,020
time-stamped and then you have the time

00:05:15,280 --> 00:05:18,760
that you're actually receiving them in

00:05:17,020 --> 00:05:22,210
the platform and then possibly a time

00:05:18,760 --> 00:05:25,000
that you're processing them over time

00:05:22,210 --> 00:05:26,890
not to abuse the word but there's going

00:05:25,000 --> 00:05:28,840
to be drift between those two and you're

00:05:26,890 --> 00:05:30,310
going to get skew and you're going to

00:05:28,840 --> 00:05:33,040
want to understand how does the

00:05:30,310 --> 00:05:36,220
framework deal with that if we overlaid

00:05:33,040 --> 00:05:38,230
on here the processing time we'd hope

00:05:36,220 --> 00:05:41,140
that it follows the line of stream time

00:05:38,230 --> 00:05:42,610
but it may not all right maybe doing

00:05:41,140 --> 00:05:44,680
something that takes longer to compute

00:05:42,610 --> 00:05:46,990
and there may be some skew there as well

00:05:44,680 --> 00:05:49,120
so this is something to keep in mind as

00:05:46,990 --> 00:05:50,530
you're working with these platforms and

00:05:49,120 --> 00:05:52,900
as you're thinking about which one to

00:05:50,530 --> 00:05:55,750
use and thinking about your data in some

00:05:52,900 --> 00:05:57,850
cases it really doesn't matter in other

00:05:55,750 --> 00:05:59,860
cases it matters a lot you may be

00:05:57,850 --> 00:06:01,420
working with mobile devices or working

00:05:59,860 --> 00:06:04,540
with things that are sending your data

00:06:01,420 --> 00:06:06,190
add a batch interval and it's real time

00:06:04,540 --> 00:06:08,500
for them and it's still a stream coming

00:06:06,190 --> 00:06:10,240
in in the aggregate and you want to be

00:06:08,500 --> 00:06:12,820
able to look at when did that actually

00:06:10,240 --> 00:06:14,920
happen not do analysis and when you

00:06:12,820 --> 00:06:20,590
received it or when you're processing it

00:06:14,920 --> 00:06:22,630
but when the event actually occurred so

00:06:20,590 --> 00:06:26,610
if you keeping score let's see how we're

00:06:22,630 --> 00:06:30,070
doing with the concepts of time so storm

00:06:26,610 --> 00:06:32,460
handle event time it also has the

00:06:30,070 --> 00:06:35,860
ability to handle the processing time

00:06:32,460 --> 00:06:38,980
spark string time only or ingesting time

00:06:35,860 --> 00:06:40,009
it has no ability today it is obviously

00:06:38,980 --> 00:06:40,879
changes that are

00:06:40,009 --> 00:06:43,129
happening and there's things happening

00:06:40,879 --> 00:06:45,050
with spark 20 it's not currently

00:06:43,129 --> 00:06:48,020
shipping so all this is based on what's

00:06:45,050 --> 00:06:50,389
out there today only handle stream time

00:06:48,020 --> 00:06:54,789
has no ability for you to look and run

00:06:50,389 --> 00:06:59,509
windowing over when an event occurred

00:06:54,789 --> 00:07:01,729
flink handles all three cases and psalms

00:06:59,509 --> 00:07:03,800
as handling stream time alright its

00:07:01,729 --> 00:07:05,839
preferred input is coming from Kafka and

00:07:03,800 --> 00:07:14,120
it's just at that point of consumption

00:07:05,839 --> 00:07:17,689
that you're seeing the data it's one

00:07:14,120 --> 00:07:22,249
thing that we should mention about storm

00:07:17,689 --> 00:07:24,710
as far as the processing time the way

00:07:22,249 --> 00:07:26,770
that it works with trident and we'll see

00:07:24,710 --> 00:07:30,169
this we look at like out of order is

00:07:26,770 --> 00:07:33,349
when you're using it excuse me when

00:07:30,169 --> 00:07:36,589
you're using that if there is a lag in

00:07:33,349 --> 00:07:39,800
the data it will drop the data it will

00:07:36,589 --> 00:07:41,599
log in but drop it okay and talk about

00:07:39,800 --> 00:07:47,180
that as we go over some other stuff

00:07:41,599 --> 00:07:49,099
someone doing so the next thing after

00:07:47,180 --> 00:07:50,719
time is you start to think about windows

00:07:49,099 --> 00:07:54,289
and how you're going to operate on the

00:07:50,719 --> 00:07:57,469
data the first type of window we talk

00:07:54,289 --> 00:07:59,180
about stumbling windows we're not going

00:07:57,469 --> 00:08:00,499
to go over all the different types of

00:07:59,180 --> 00:08:02,029
windows that are out there but really

00:08:00,499 --> 00:08:04,099
talk about a couple of the major ones

00:08:02,029 --> 00:08:06,289
that you'll see and depending upon the

00:08:04,099 --> 00:08:09,709
framework you may see a lot more support

00:08:06,289 --> 00:08:11,360
and more advanced support or less than

00:08:09,709 --> 00:08:15,519
what we're even going to go over so

00:08:11,360 --> 00:08:19,039
tumbling windows we look at two types

00:08:15,519 --> 00:08:21,289
tumbling time it's the first one and the

00:08:19,039 --> 00:08:23,599
way that this works is there's a window

00:08:21,289 --> 00:08:26,269
length and there's a sliding interval so

00:08:23,599 --> 00:08:28,459
there's a trigger that's the length for

00:08:26,269 --> 00:08:31,039
when the data is processed and then an

00:08:28,459 --> 00:08:33,010
eviction policy for what happens in a

00:08:31,039 --> 00:08:36,289
doubling window and in tumbling time

00:08:33,010 --> 00:08:39,169
these are both based on time as the

00:08:36,289 --> 00:08:41,870
interval so time becomes the policy for

00:08:39,169 --> 00:08:43,699
when things get evicted time becomes the

00:08:41,870 --> 00:08:46,760
trigger for when to determine that a

00:08:43,699 --> 00:08:49,180
window is full and after its process its

00:08:46,760 --> 00:08:49,180
emptied

00:08:49,250 --> 00:08:58,130
okay spark in Psalms oh don't support

00:08:52,340 --> 00:09:02,870
this flink and storm do with the latest

00:08:58,130 --> 00:09:05,440
version of storm the next version of

00:09:02,870 --> 00:09:08,630
tumbling ends up dealing with counting

00:09:05,440 --> 00:09:11,060
similar type of principles apply where

00:09:08,630 --> 00:09:13,040
there's an eviction policy and a trigger

00:09:11,060 --> 00:09:16,100
policy but these are based off of a

00:09:13,040 --> 00:09:20,030
count not off of adoration okay so in

00:09:16,100 --> 00:09:24,080
this case we have a interval of two and

00:09:20,030 --> 00:09:26,840
a window length of two so Italy evicted

00:09:24,080 --> 00:09:30,550
when you exceed more than two and the

00:09:26,840 --> 00:09:30,550
window be processed when you have two

00:09:31,840 --> 00:09:44,360
against park in psalms a-- don't support

00:09:35,090 --> 00:09:46,330
this but flank and storm do the next one

00:09:44,360 --> 00:09:49,250
we talked about is sliding window

00:09:46,330 --> 00:09:52,130
similar type of thing where there is a

00:09:49,250 --> 00:09:53,990
trigger and there's an eviction policy

00:09:52,130 --> 00:09:55,250
except we're going to see with sliding

00:09:53,990 --> 00:09:57,080
windows it works a little bit

00:09:55,250 --> 00:10:01,610
differently as you can imagine based on

00:09:57,080 --> 00:10:04,100
the name it slides the first type is

00:10:01,610 --> 00:10:06,140
sliding time window if you look at the

00:10:04,100 --> 00:10:08,210
spark documentation is just a sliding

00:10:06,140 --> 00:10:10,160
window there's an interval and a length

00:10:08,210 --> 00:10:12,170
this is the sliding when doing that

00:10:10,160 --> 00:10:14,240
they're talking about so the way that it

00:10:12,170 --> 00:10:15,920
works is you have a length based on time

00:10:14,240 --> 00:10:18,470
that you're going to look at a window of

00:10:15,920 --> 00:10:20,180
data and then you'd have an interval

00:10:18,470 --> 00:10:23,750
that you're executing and seeing that

00:10:20,180 --> 00:10:27,320
piece of data right so as the data moves

00:10:23,750 --> 00:10:28,820
across you see the whole window and then

00:10:27,320 --> 00:10:31,070
the eviction is going to happen on that

00:10:28,820 --> 00:10:34,339
sliding interval as new data comes in

00:10:31,070 --> 00:10:37,390
it's going to evict the oldest data okay

00:10:34,339 --> 00:10:37,390
so it's going to you to slide forward

00:10:37,750 --> 00:10:44,570
flink storm spark all support this sums

00:10:41,990 --> 00:10:47,780
up if you squint hard enough and look at

00:10:44,570 --> 00:10:50,900
it close enough kind of maybe it has the

00:10:47,780 --> 00:10:53,060
notion of a counter you know in a

00:10:50,900 --> 00:10:55,990
callback that you could get notified at

00:10:53,060 --> 00:10:58,250
an interval of when something happened

00:10:55,990 --> 00:11:00,500
but you don't get the window of data

00:10:58,250 --> 00:11:01,880
alright so and all the other processing

00:11:00,500 --> 00:11:03,110
frameworks you'd actually get the whole

00:11:01,880 --> 00:11:05,120
window a data operate on

00:11:03,110 --> 00:11:07,399
on in Psalms oh you're going to be able

00:11:05,120 --> 00:11:09,709
to keep track of account so kind of

00:11:07,399 --> 00:11:11,930
maybe depending upon how much you squint

00:11:09,709 --> 00:11:19,339
may kind of feel like it's a sliding

00:11:11,930 --> 00:11:21,200
window the opposite side that similar to

00:11:19,339 --> 00:11:24,800
what we saw with tumbling is a sliding

00:11:21,200 --> 00:11:26,209
count so in this case you can have a

00:11:24,800 --> 00:11:27,980
window length that's going to be the

00:11:26,209 --> 00:11:30,110
total size the number of tuples in the

00:11:27,980 --> 00:11:32,200
stream and then the interval that's

00:11:30,110 --> 00:11:35,149
going to be the number of tuples as well

00:11:32,200 --> 00:11:37,720
so here we have a window length of eight

00:11:35,149 --> 00:11:39,950
elements and a sliding interval of four

00:11:37,720 --> 00:11:42,620
okay so you would see those eight

00:11:39,950 --> 00:11:44,690
elements in your window of data and as

00:11:42,620 --> 00:11:49,640
it slides and four come in you're going

00:11:44,690 --> 00:11:53,000
to get others pushed out spark in Psalms

00:11:49,640 --> 00:11:58,160
of no support for this flink and storm

00:11:53,000 --> 00:12:00,890
do so the next thing that comes up is

00:11:58,160 --> 00:12:03,170
and it often comes up when you start to

00:12:00,890 --> 00:12:06,140
look at real data is it's messy right

00:12:03,170 --> 00:12:08,000
data doesn't obey rules that you want

00:12:06,140 --> 00:12:09,470
right it comes in when it wants people

00:12:08,000 --> 00:12:12,380
shut off their device they turn on the

00:12:09,470 --> 00:12:15,230
device things happen in batch mode when

00:12:12,380 --> 00:12:17,510
it's on sailor or it son satellite and

00:12:15,230 --> 00:12:19,100
things happen out of order I need to

00:12:17,510 --> 00:12:21,019
kind of reason about how do you handle

00:12:19,100 --> 00:12:23,060
it and what do you do and what kind of

00:12:21,019 --> 00:12:28,250
support do you get from a framework you

00:12:23,060 --> 00:12:32,060
may choose so if we imagine it kind of

00:12:28,250 --> 00:12:35,329
looking like this that data is coming

00:12:32,060 --> 00:12:38,980
along and time is marching on and we get

00:12:35,329 --> 00:12:42,680
data at these intervals that are really

00:12:38,980 --> 00:12:44,269
40 plus 1 in these different cases right

00:12:42,680 --> 00:12:49,490
what do you do and how do you handle

00:12:44,269 --> 00:12:52,519
that spark in Psalms up you're on your

00:12:49,490 --> 00:12:54,529
own it has no support for it right spark

00:12:52,519 --> 00:12:56,180
ends up being micro batch it knows

00:12:54,529 --> 00:12:58,130
nothing about this data being out of

00:12:56,180 --> 00:13:00,410
order all right this is a based on

00:12:58,130 --> 00:13:01,880
knowing event time and it doesn't know

00:13:00,410 --> 00:13:03,949
the event time it just knows the

00:13:01,880 --> 00:13:07,279
ingestion time so you're kind of on your

00:13:03,949 --> 00:13:11,060
own at that point to try and do it flink

00:13:07,279 --> 00:13:12,480
not a problem carry on all right works

00:13:11,060 --> 00:13:17,399
without a problem

00:13:12,480 --> 00:13:20,339
storm it'll notice it and if that data

00:13:17,399 --> 00:13:22,709
lags it will log it but it will discard

00:13:20,339 --> 00:13:26,040
the data so you won't see it in the

00:13:22,709 --> 00:13:28,380
processing okay so out of order data is

00:13:26,040 --> 00:13:30,089
basically just dropped on the floor but

00:13:28,380 --> 00:13:32,579
there's logging to let you know that

00:13:30,089 --> 00:13:34,260
it's dropping data on the floor so at

00:13:32,579 --> 00:13:37,260
least you could try and do something

00:13:34,260 --> 00:13:41,399
with it so let's see where we are on the

00:13:37,260 --> 00:13:46,680
scorecard for this so windowing

00:13:41,399 --> 00:13:50,420
windowing scorecard temporal tumbling

00:13:46,680 --> 00:13:53,850
windows could do in storm to do in flank

00:13:50,420 --> 00:13:56,370
counting temporal windows or counting

00:13:53,850 --> 00:14:01,740
tumbling windows not a problem in storm

00:13:56,370 --> 00:14:03,630
or fling sliding time windows supported

00:14:01,740 --> 00:14:05,430
by all and again Psalms or the caveat

00:14:03,630 --> 00:14:06,959
that you really got to kind of think

00:14:05,430 --> 00:14:09,870
about it differently and it's much more

00:14:06,959 --> 00:14:12,060
of just getting notified at a certain

00:14:09,870 --> 00:14:14,459
interval something happen every 60

00:14:12,060 --> 00:14:16,529
seconds you want to have a call back

00:14:14,459 --> 00:14:18,240
into a handler you get notified that

00:14:16,529 --> 00:14:22,889
happen you could count the events that

00:14:18,240 --> 00:14:27,480
are going on sliding windows as support

00:14:22,889 --> 00:14:29,339
count again storm flink and then flink

00:14:27,480 --> 00:14:32,430
probably not surprisingly looking at

00:14:29,339 --> 00:14:33,329
this has a lot of custom when doing that

00:14:32,430 --> 00:14:35,399
you could do and you can create your own

00:14:33,329 --> 00:14:37,949
windowing and different advanced windows

00:14:35,399 --> 00:14:39,779
that you could do as well so if you

00:14:37,949 --> 00:14:41,130
keeping score here I'd say from the

00:14:39,779 --> 00:14:44,430
windowing standpoint of window becomes

00:14:41,130 --> 00:14:49,370
pretty important to your use case it's a

00:14:44,430 --> 00:14:49,370
leader right there out of order data

00:14:49,519 --> 00:14:53,639
sparks not going to give you direct

00:14:52,230 --> 00:14:55,290
support for it you got to figure out

00:14:53,639 --> 00:14:56,790
what to do you could probably do

00:14:55,290 --> 00:14:58,649
something with it again it doesn't know

00:14:56,790 --> 00:15:01,350
anything about event time flink can

00:14:58,649 --> 00:15:04,050
handle it just fine and storms going to

00:15:01,350 --> 00:15:08,180
discard the data for you but then log it

00:15:04,050 --> 00:15:08,180
that it let go of your data for you

00:15:16,880 --> 00:15:21,390
the next thing to think about after

00:15:19,710 --> 00:15:23,790
we've covered time and then covering

00:15:21,390 --> 00:15:26,160
windowing is the processing semantics

00:15:23,790 --> 00:15:28,170
and what type of semantics is a

00:15:26,160 --> 00:15:30,360
framework you want to use support and

00:15:28,170 --> 00:15:34,500
really need to also bounce us against

00:15:30,360 --> 00:15:37,050
what makes sense in your use case so the

00:15:34,500 --> 00:15:40,770
first one which is the weakest guarantee

00:15:37,050 --> 00:15:43,160
is really just at most once so in here

00:15:40,770 --> 00:15:45,360
we have this incoming stream of data

00:15:43,160 --> 00:15:47,940
message is coming in in this case we

00:15:45,360 --> 00:15:49,740
have to we'll just call this a stream

00:15:47,940 --> 00:15:51,900
processor of somewhere inside of the

00:15:49,740 --> 00:15:54,690
streaming platform our business logic is

00:15:51,900 --> 00:15:59,400
running and doing work and then one

00:15:54,690 --> 00:16:02,400
message and goes out that's it right so

00:15:59,400 --> 00:16:03,930
at most one time but it may not go

00:16:02,400 --> 00:16:06,540
through all right so you're kind of just

00:16:03,930 --> 00:16:15,270
left going okay at least maybe get the

00:16:06,540 --> 00:16:19,710
data similar type of thing in this case

00:16:15,270 --> 00:16:21,990
if it crashes you may not get any data

00:16:19,710 --> 00:16:23,220
that comes out right at most once

00:16:21,990 --> 00:16:27,720
doesn't guarantee that it'll actually

00:16:23,220 --> 00:16:33,530
deliver anything so Samsa does not

00:16:27,720 --> 00:16:33,530
support this spark flank storm and do

00:16:34,940 --> 00:16:39,390
moving up from a guarantee standpoint to

00:16:37,830 --> 00:16:42,120
stronger guarantee we'd have at least

00:16:39,390 --> 00:16:43,980
once right and in this case you're going

00:16:42,120 --> 00:16:46,560
to be guaranteed that a message is going

00:16:43,980 --> 00:16:50,760
to be processed by the framework at

00:16:46,560 --> 00:16:54,300
least one time okay so you're guaranteed

00:16:50,760 --> 00:16:57,990
to see it at least once you may see it

00:16:54,300 --> 00:17:01,800
more so in this case if we send two

00:16:57,990 --> 00:17:03,360
messages in and something crashes we

00:17:01,800 --> 00:17:06,600
have an undocumented feature that takes

00:17:03,360 --> 00:17:08,280
down our processor comes back up perhaps

00:17:06,600 --> 00:17:10,080
there was some state that was saved and

00:17:08,280 --> 00:17:12,540
now we may see three messages that come

00:17:10,080 --> 00:17:13,950
through right so as you're thinking

00:17:12,540 --> 00:17:16,320
about this and as you're building things

00:17:13,950 --> 00:17:18,480
that downstream from there you need to

00:17:16,320 --> 00:17:21,180
be able to take into consideration that

00:17:18,480 --> 00:17:24,480
you may see the same message multiple

00:17:21,180 --> 00:17:29,050
times okay it's not a guarantee that

00:17:24,480 --> 00:17:33,350
it's just once it's at least once

00:17:29,050 --> 00:17:35,720
so storm you can make this work you have

00:17:33,350 --> 00:17:38,210
some work to do to do it it gives you

00:17:35,720 --> 00:17:41,870
the facility to do it but you have to do

00:17:38,210 --> 00:17:48,080
the work for it in spark flanken Samsa

00:17:41,870 --> 00:17:50,120
you have this the next one that everyone

00:17:48,080 --> 00:17:52,070
clamors for and everyone wants in a lot

00:17:50,120 --> 00:17:55,250
of cases though it may or may not make

00:17:52,070 --> 00:17:57,620
sense you have exactly once and this is

00:17:55,250 --> 00:18:00,520
really what it says you get two meses

00:17:57,620 --> 00:18:03,080
going in your processor could crash

00:18:00,520 --> 00:18:04,280
something could you know pull the cable

00:18:03,080 --> 00:18:07,460
out of the wall whatever may happen

00:18:04,280 --> 00:18:11,150
going to guarantee that the message was

00:18:07,460 --> 00:18:13,490
processed exactly one time okay to pull

00:18:11,150 --> 00:18:14,870
this off a lot of the frameworks going

00:18:13,490 --> 00:18:17,300
to have to interact with some other

00:18:14,870 --> 00:18:20,390
system right if you're using Kafka as

00:18:17,300 --> 00:18:23,090
the incoming stream of data it provides

00:18:20,390 --> 00:18:25,030
you the mechanism to implement exactly

00:18:23,090 --> 00:18:27,080
once but it doesn't implement it for you

00:18:25,030 --> 00:18:28,700
right and then it's same with another

00:18:27,080 --> 00:18:30,580
system so coffee gives you the offsets

00:18:28,700 --> 00:18:32,990
but you have to storm somewhere else

00:18:30,580 --> 00:18:36,050
writes you storm in some other data

00:18:32,990 --> 00:18:38,630
store some distributed system so that

00:18:36,050 --> 00:18:41,770
you have control over those and you go

00:18:38,630 --> 00:18:43,760
back and ask for if the data goes away

00:18:41,770 --> 00:18:46,130
before you get a chance to go back and

00:18:43,760 --> 00:18:47,750
get it you're kind of you got to look at

00:18:46,130 --> 00:18:50,390
that point but at least from the

00:18:47,750 --> 00:18:52,640
streaming side of things assuming the

00:18:50,390 --> 00:18:55,460
data is still in whatever q you're using

00:18:52,640 --> 00:18:57,320
that's feeding it then you'd be able to

00:18:55,460 --> 00:19:03,770
reach over and grab that data and pull

00:18:57,320 --> 00:19:07,640
it in okay so storm spark and flink all

00:19:03,770 --> 00:19:12,380
could do this and sums a can't at this

00:19:07,640 --> 00:19:18,170
time so let's check our scorecard for

00:19:12,380 --> 00:19:22,310
those keeping track so storm spark flink

00:19:18,170 --> 00:19:25,730
all ok without most once at least once

00:19:22,310 --> 00:19:28,160
in storm you have to do if you use a

00:19:25,730 --> 00:19:31,190
non-transactional you get at least once

00:19:28,160 --> 00:19:34,910
you get that support across the rest of

00:19:31,190 --> 00:19:38,050
the frameworks exactly once you just

00:19:34,910 --> 00:19:41,510
tried it and you can get that with storm

00:19:38,050 --> 00:19:41,870
not going to go into the pros and cons I

00:19:41,510 --> 00:19:43,700
tried

00:19:41,870 --> 00:19:47,390
some people fans some people don't like

00:19:43,700 --> 00:19:49,960
it that becomes a religious battle as to

00:19:47,390 --> 00:19:52,730
which way you fall in the fence there

00:19:49,960 --> 00:19:56,150
and with sparking flink you get the

00:19:52,730 --> 00:19:58,490
support storm also has another guarantee

00:19:56,150 --> 00:20:01,730
another semantics that's called best

00:19:58,490 --> 00:20:04,420
effort not quite sure what's really

00:20:01,730 --> 00:20:08,630
meant there but it supports best effort

00:20:04,420 --> 00:20:10,430
so I guess if it falls outside of one of

00:20:08,630 --> 00:20:13,220
these and something really goes haywire

00:20:10,430 --> 00:20:17,929
is a kind of you know a disclaimer of we

00:20:13,220 --> 00:20:19,730
gave it a best effort and tried okay to

00:20:17,929 --> 00:20:22,100
do the exactly once with Trident though

00:20:19,730 --> 00:20:25,850
and do exactly once a storm your left

00:20:22,100 --> 00:20:28,130
doing work okay the recommended pattern

00:20:25,850 --> 00:20:31,429
if you look at it is to get that to work

00:20:28,130 --> 00:20:33,500
when you recover you need to keep the

00:20:31,429 --> 00:20:36,230
data in another store and you need to

00:20:33,500 --> 00:20:38,750
keep track of current value previous

00:20:36,230 --> 00:20:40,790
value transaction ID the exactly once

00:20:38,750 --> 00:20:43,250
guarantee they're able to pull off

00:20:40,790 --> 00:20:45,500
because they guarantee that the batch of

00:20:43,250 --> 00:20:47,300
data you're looking at always has the

00:20:45,500 --> 00:20:50,600
same transaction ID and that would be

00:20:47,300 --> 00:20:52,190
the same if you restart okay so you'd

00:20:50,600 --> 00:20:55,040
see those again but you may get the data

00:20:52,190 --> 00:21:01,220
downstream and have to do the work to

00:20:55,040 --> 00:21:03,920
figure out did I see this before so that

00:21:01,220 --> 00:21:05,720
brings us to the next thing we need to

00:21:03,920 --> 00:21:08,179
think about and then ends up being state

00:21:05,720 --> 00:21:11,600
and there's really two types of state

00:21:08,179 --> 00:21:14,179
that we're going to be interested in the

00:21:11,600 --> 00:21:16,370
first one is application state or user

00:21:14,179 --> 00:21:18,920
defined state alright and this really

00:21:16,370 --> 00:21:21,530
has to do with your applications running

00:21:18,920 --> 00:21:23,240
what type of state do you want to keep

00:21:21,530 --> 00:21:26,960
what type of state does a framework

00:21:23,240 --> 00:21:29,090
provide for you or state facilities for

00:21:26,960 --> 00:21:31,880
you so you could keep it right if you

00:21:29,090 --> 00:21:33,050
spark you have update state by key right

00:21:31,880 --> 00:21:35,630
so you have these different state

00:21:33,050 --> 00:21:40,280
notions so you could hold on to state

00:21:35,630 --> 00:21:41,870
for your computations all right it

00:21:40,280 --> 00:21:43,280
really kind of looks like this right if

00:21:41,870 --> 00:21:45,350
we just had this simple state that we're

00:21:43,280 --> 00:21:49,100
keeping we have this incoming stream

00:21:45,350 --> 00:21:51,170
let's assume it has some IDs and you

00:21:49,100 --> 00:21:53,540
know letters associated with them

00:21:51,170 --> 00:21:55,580
perhaps it's which device I was using

00:21:53,540 --> 00:21:57,740
you want to group those

00:21:55,580 --> 00:21:59,720
they want to account and we're going to

00:21:57,740 --> 00:22:01,610
want to keep say that state for the

00:21:59,720 --> 00:22:03,380
current hour right so we have the state

00:22:01,610 --> 00:22:05,240
in that count by user ID that we're

00:22:03,380 --> 00:22:06,830
holding on to and we want to have a

00:22:05,240 --> 00:22:08,630
running aggregate of what's happening

00:22:06,830 --> 00:22:11,630
the current hour and then we send it to

00:22:08,630 --> 00:22:13,790
some output right if you attended

00:22:11,630 --> 00:22:14,930
Stefan's talked yesterday about flanking

00:22:13,790 --> 00:22:16,430
about some of the benchmarking and

00:22:14,930 --> 00:22:18,050
things that they're working on or you

00:22:16,430 --> 00:22:19,970
know as you start to really push that

00:22:18,050 --> 00:22:22,280
and really try and make that work in

00:22:19,970 --> 00:22:24,800
production at high volume the bottleneck

00:22:22,280 --> 00:22:26,840
becomes that output alright that output

00:22:24,800 --> 00:22:28,490
is another data store you're going to

00:22:26,840 --> 00:22:29,960
have limitations on the network or

00:22:28,490 --> 00:22:31,220
limitations on this other store that

00:22:29,960 --> 00:22:32,960
you're trying to push these aggregates

00:22:31,220 --> 00:22:35,450
to somewhere you need to get the data

00:22:32,960 --> 00:22:38,720
out to present it and to do take some

00:22:35,450 --> 00:22:41,210
action with it right at this count by

00:22:38,720 --> 00:22:43,870
user ID area right here we're keeping it

00:22:41,210 --> 00:22:46,400
you're still inside of this platform

00:22:43,870 --> 00:22:48,170
right and the output would be some

00:22:46,400 --> 00:22:51,290
destination you sending it to whether

00:22:48,170 --> 00:22:54,620
it's to Cassandra or to hbase or the

00:22:51,290 --> 00:22:59,560
Redis or TV or whatever it may be it's

00:22:54,620 --> 00:23:01,760
going somewhere so to get away from that

00:22:59,560 --> 00:23:04,840
you could kind of do something like this

00:23:01,760 --> 00:23:06,590
so this queryable simple state

00:23:04,840 --> 00:23:09,590
ironically this was talked about

00:23:06,590 --> 00:23:12,950
yesterday and there is a flink jira for

00:23:09,590 --> 00:23:15,830
this so it's going to be put into flink

00:23:12,950 --> 00:23:18,710
as an option or has a capability that

00:23:15,830 --> 00:23:20,870
now as you're keeping state for the hour

00:23:18,710 --> 00:23:24,890
you'd be able to go back and query that

00:23:20,870 --> 00:23:26,870
system it's the only framework once

00:23:24,890 --> 00:23:30,380
that's complete that will actually have

00:23:26,870 --> 00:23:32,720
a story to tell with this I've done this

00:23:30,380 --> 00:23:34,010
before in storm a couple years back so

00:23:32,720 --> 00:23:36,790
if you're interested in talking about it

00:23:34,010 --> 00:23:38,870
be more than happy to offline from here

00:23:36,790 --> 00:23:40,880
some interesting things that you have to

00:23:38,870 --> 00:23:43,490
do to make it work but there was not a

00:23:40,880 --> 00:23:46,070
whole lot of framework infrastructure

00:23:43,490 --> 00:23:48,560
provided to make it happen so pretty

00:23:46,070 --> 00:23:50,390
compelling use case if that is actually

00:23:48,560 --> 00:23:53,060
going to be a capability within sight of

00:23:50,390 --> 00:23:54,980
link because now you could have clients

00:23:53,060 --> 00:23:57,680
that are live querying a stream without

00:23:54,980 --> 00:23:58,760
having to go elsewhere all right so it

00:23:57,680 --> 00:24:01,460
becomes pretty powerful from a

00:23:58,760 --> 00:24:04,550
dashboarding standpoint or from a real

00:24:01,460 --> 00:24:06,380
real time if you will notification of

00:24:04,550 --> 00:24:07,940
what's happening in stream all right in

00:24:06,380 --> 00:24:09,620
this case we're just showing you know

00:24:07,940 --> 00:24:13,480
counting by user ID but you

00:24:09,620 --> 00:24:13,480
imagine being a variety of other things

00:24:14,230 --> 00:24:20,300
when you think about why it's not there

00:24:16,990 --> 00:24:22,430
you know you really look about this

00:24:20,300 --> 00:24:24,440
chart here of like the complexity and

00:24:22,430 --> 00:24:27,050
the features you're doing it in memory

00:24:24,440 --> 00:24:28,760
of you just holding it and even if it's

00:24:27,050 --> 00:24:30,710
for an hour and you're provided you know

00:24:28,760 --> 00:24:32,120
the ability from a framework to say you

00:24:30,710 --> 00:24:34,640
could hold on to the state and we'll

00:24:32,120 --> 00:24:35,930
make sure that if you crash we have it

00:24:34,640 --> 00:24:39,110
it's persisted and when you restart

00:24:35,930 --> 00:24:42,080
it'll be rehydrated it's pretty low in

00:24:39,110 --> 00:24:43,429
the complexity graph as you go across

00:24:42,080 --> 00:24:45,590
though and now you want to have this

00:24:43,429 --> 00:24:49,070
replicated queryable persistence store

00:24:45,590 --> 00:24:51,170
that's live in the stream the complexity

00:24:49,070 --> 00:24:54,460
goes up and the features required go up

00:24:51,170 --> 00:24:57,950
so hard for some systems to bake this in

00:24:54,460 --> 00:25:00,500
after the fact you know flink based upon

00:24:57,950 --> 00:25:03,170
design as you may have heard yesterday

00:25:00,500 --> 00:25:04,790
during Stefan's talk a lot of the

00:25:03,170 --> 00:25:07,100
underlying plumbing was already there

00:25:04,790 --> 00:25:09,050
it's a matter of surfacing it in spark

00:25:07,100 --> 00:25:11,360
this becomes hard because it's micro

00:25:09,050 --> 00:25:14,240
batch and storm becomes hard because

00:25:11,360 --> 00:25:18,470
it's just not designed to handle some of

00:25:14,240 --> 00:25:21,410
this and in sums of its it's not going

00:25:18,470 --> 00:25:23,929
to happen soon it's just not not its

00:25:21,410 --> 00:25:25,970
model right and its really nothing about

00:25:23,929 --> 00:25:30,020
Songza but this is not designed for

00:25:25,970 --> 00:25:34,340
doing this it has the state but it's not

00:25:30,020 --> 00:25:35,780
part of the model the next state which

00:25:34,340 --> 00:25:37,580
is really kind of what you see when you

00:25:35,780 --> 00:25:39,170
start to look across is a system state

00:25:37,580 --> 00:25:41,720
all right this is everything else that

00:25:39,170 --> 00:25:44,360
these frameworks provides you to make

00:25:41,720 --> 00:25:48,100
sure that when things crash because they

00:25:44,360 --> 00:25:48,100
will that it's recoverable

00:25:53,320 --> 00:25:59,149
state most of the time involves

00:25:56,990 --> 00:26:00,529
checkpointing or snapshotting we're

00:25:59,149 --> 00:26:01,820
going to kind of lump them together and

00:26:00,529 --> 00:26:04,429
just call them checkpointing for right

00:26:01,820 --> 00:26:07,730
now we're just going to look at how each

00:26:04,429 --> 00:26:12,740
of them handle checkpointing you're so

00:26:07,730 --> 00:26:14,870
spark is running along and at different

00:26:12,740 --> 00:26:18,500
time intervals you could choose to check

00:26:14,870 --> 00:26:20,809
point data okay the checkpointing is

00:26:18,500 --> 00:26:24,590
only going to happen to have it actually

00:26:20,809 --> 00:26:26,059
be reliable in a CFS or s3 or another

00:26:24,590 --> 00:26:28,279
distributed store today those are the

00:26:26,059 --> 00:26:31,760
two supported and that may change over

00:26:28,279 --> 00:26:35,029
time but it's based upon those batches

00:26:31,760 --> 00:26:38,210
of time I can see as it goes through

00:26:35,029 --> 00:26:40,190
each time you're not losing data but

00:26:38,210 --> 00:26:46,070
it's doing checkpointing and keeping

00:26:40,190 --> 00:26:49,429
track of what's going on something that

00:26:46,070 --> 00:26:52,370
works differently it has local state by

00:26:49,429 --> 00:26:54,950
default rocks DB and it will constantly

00:26:52,370 --> 00:26:57,580
be writing data to it and at the same

00:26:54,950 --> 00:27:00,320
time it's streaming that data to Kafka

00:26:57,580 --> 00:27:04,220
okay so as a changelog that's constantly

00:27:00,320 --> 00:27:06,490
feeding into Kafka so in this bit from

00:27:04,220 --> 00:27:08,929
this perspective you're pretty safe

00:27:06,490 --> 00:27:11,149
rights you have data going to a local

00:27:08,929 --> 00:27:13,190
rocks TV and you have data being

00:27:11,149 --> 00:27:22,039
committed to Kafka as well so it's

00:27:13,190 --> 00:27:23,809
constantly checkpointing flink it's a

00:27:22,039 --> 00:27:26,690
little bit different right it stays a

00:27:23,809 --> 00:27:29,059
hundred percent as a stream so in this

00:27:26,690 --> 00:27:31,549
case it actually have these esas

00:27:29,059 --> 00:27:35,080
indicating like a snapshot not a time

00:27:31,549 --> 00:27:37,730
window but a snapshot that's occurring

00:27:35,080 --> 00:27:39,830
you could do it in memory at a certain

00:27:37,730 --> 00:27:42,440
point though you're going to want to use

00:27:39,830 --> 00:27:45,140
some sort of distributed resilience

00:27:42,440 --> 00:27:48,590
store perhaps HDFS to capture this

00:27:45,140 --> 00:27:50,360
information but again it's not turning

00:27:48,590 --> 00:27:54,070
it into a micro badge this is still

00:27:50,360 --> 00:27:57,070
happening in the stream as part of

00:27:54,070 --> 00:27:57,070
snapshots

00:27:59,670 --> 00:28:07,450
storm gets turned into a microbe at

00:28:04,240 --> 00:28:09,550
system when you want to do this so went

00:28:07,450 --> 00:28:13,540
from being an event-based to now be a

00:28:09,550 --> 00:28:16,570
micro batch okay so as windows of time

00:28:13,540 --> 00:28:19,560
go on as batches happen now you need to

00:28:16,570 --> 00:28:23,350
commit these checkpoints to their HDFS

00:28:19,560 --> 00:28:25,810
into memory pretty risky into Cassandra

00:28:23,350 --> 00:28:29,560
into some data store that you could

00:28:25,810 --> 00:28:38,950
trust so we just turn storm from a vent

00:28:29,560 --> 00:28:41,740
base to micro batch so let's see how

00:28:38,950 --> 00:28:44,380
we're doing in relation to that and

00:28:41,740 --> 00:28:49,600
again this checkpointing is for recovery

00:28:44,380 --> 00:28:52,420
purposes so at least once Samsa still

00:28:49,600 --> 00:28:55,840
maintains that guarantee and exactly

00:28:52,420 --> 00:28:57,400
once storm spark and flank maintain that

00:28:55,840 --> 00:28:59,590
guarantee as well so you don't lose the

00:28:57,400 --> 00:29:02,860
guarantee even when you have this

00:28:59,590 --> 00:29:12,070
involved and again with storm there's

00:29:02,860 --> 00:29:14,380
work to do so hopefully now we kind of

00:29:12,070 --> 00:29:17,620
got closer to this right we kind of

00:29:14,380 --> 00:29:19,900
almost have an idea as to what Jar candy

00:29:17,620 --> 00:29:21,310
we want to reach for and now it's a

00:29:19,900 --> 00:29:23,440
matter of applying the rules and

00:29:21,310 --> 00:29:28,150
figuring out what makes sense in your

00:29:23,440 --> 00:29:32,520
business thank you for your time and

00:29:28,150 --> 00:29:32,520
take any questions they may have

00:29:44,390 --> 00:29:48,929
hi thanks for talk yeah we heard a

00:29:47,460 --> 00:29:52,049
little bit in the keynote this morning

00:29:48,929 --> 00:29:54,299
about Kafka streams have you had a

00:29:52,049 --> 00:29:56,420
chance to look at that and how that fits

00:29:54,299 --> 00:29:59,429
into this landscape resuscitation you

00:29:56,420 --> 00:30:02,670
looked at it a little bit I haven't put

00:29:59,429 --> 00:30:05,549
it through paces but sire I can't speak

00:30:02,670 --> 00:30:07,290
that much to it but it's going to try to

00:30:05,549 --> 00:30:09,540
accomplish some of these things and it's

00:30:07,290 --> 00:30:12,360
going after the similar thing right and

00:30:09,540 --> 00:30:14,940
trying to make streaming not depend upon

00:30:12,360 --> 00:30:16,020
a platform right for all the bells and

00:30:14,940 --> 00:30:18,900
whistles you get from all these

00:30:16,020 --> 00:30:21,570
platforms there is a pretty unique

00:30:18,900 --> 00:30:23,669
advantage from Kafka streaming that you

00:30:21,570 --> 00:30:26,460
install Kafka you have a library you're

00:30:23,669 --> 00:30:29,070
ready to go there's definitely times

00:30:26,460 --> 00:30:33,299
where having to have one more cluster

00:30:29,070 --> 00:30:36,030
deployed becomes a lot right so I'd

00:30:33,299 --> 00:30:37,470
imagine there'll be some opportunity

00:30:36,030 --> 00:30:39,780
there and it definitely will serve some

00:30:37,470 --> 00:30:47,360
use cases how would a matchup I think is

00:30:39,780 --> 00:30:47,360
kind of early to tell any more questions

00:30:50,120 --> 00:30:54,350

YouTube URL: https://www.youtube.com/watch?v=sZ2w0e8taDs


