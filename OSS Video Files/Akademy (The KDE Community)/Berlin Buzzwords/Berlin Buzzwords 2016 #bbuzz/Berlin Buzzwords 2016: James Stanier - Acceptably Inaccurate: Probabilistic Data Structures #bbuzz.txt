Title: Berlin Buzzwords 2016: James Stanier - Acceptably Inaccurate: Probabilistic Data Structures #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Writing software for the Internet puts engineers face to face with traffic that makes doing even simple things difficult. Seemingly straightforward operations such as counting, set membership and set cardinality become either extremely slow or prohibitively expensive to do using traditional methods.

Help is at hand, however: probabilistic techniques are both fascinating and extremely useful. By sacrificing a predictable amount of accuracy, we can perform operations at scales we never thought possible, and fast! We'll introduce approaches such as Bloom filters for set membership, count-min sketch for frequency in streams, and HyperLogLog for cardinality. No maths PhD is required.

We'll look at the before and after effects of using these techniques in real-world scenarios, and present the libraries that you can go away and play with right now.

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,740 --> 00:00:06,299
and over the last couple of years we've

00:00:04,920 --> 00:00:08,460
built some new products that use these

00:00:06,299 --> 00:00:09,750
in the back end and I thought I'd share

00:00:08,460 --> 00:00:12,120
it with you so what are we doing today

00:00:09,750 --> 00:00:13,830
for things first thing is some

00:00:12,120 --> 00:00:16,199
motivation so if you don't know what

00:00:13,830 --> 00:00:17,850
these data structures are or have any

00:00:16,199 --> 00:00:19,410
idea at all then I'll give you some

00:00:17,850 --> 00:00:20,850
motivation as to why you should care

00:00:19,410 --> 00:00:22,560
about them and might why you might find

00:00:20,850 --> 00:00:24,390
it interesting and then we're going to

00:00:22,560 --> 00:00:26,189
be looking at three probabilistic data

00:00:24,390 --> 00:00:28,500
structures and I think that these are in

00:00:26,189 --> 00:00:29,820
order of complexity we're going to start

00:00:28,500 --> 00:00:31,740
off with bloom filters which I think

00:00:29,820 --> 00:00:33,750
quite simple to understand then we're

00:00:31,740 --> 00:00:34,770
going to go into count min sketch which

00:00:33,750 --> 00:00:36,600
is a little bit more tricky to

00:00:34,770 --> 00:00:38,309
understand and then we're going to try

00:00:36,600 --> 00:00:41,550
our best to explain hyper log look which

00:00:38,309 --> 00:00:43,079
is quite complicated but first off why

00:00:41,550 --> 00:00:45,660
should you care about probabilistic data

00:00:43,079 --> 00:00:47,550
structures so I've tried to prepare some

00:00:45,660 --> 00:00:49,530
motivation for you here to see why you

00:00:47,550 --> 00:00:52,289
might be interested here are some

00:00:49,530 --> 00:00:53,879
storage technologies and we have a line

00:00:52,289 --> 00:00:55,710
underneath these storage technologies

00:00:53,879 --> 00:00:57,149
now i'm going to put an arrow head on

00:00:55,710 --> 00:00:58,710
this line and try and show you some

00:00:57,149 --> 00:01:01,050
things so if we put an arrow head on

00:00:58,710 --> 00:01:02,699
here I'd say that the speed of these

00:01:01,050 --> 00:01:05,129
things in terms of the speed of reading

00:01:02,699 --> 00:01:06,780
the speed of writing goes up as we go in

00:01:05,129 --> 00:01:08,159
that direction on the slide so I think

00:01:06,780 --> 00:01:10,460
we will agree that memory is like the

00:01:08,159 --> 00:01:13,610
fastest thing to read and write to yeah

00:01:10,460 --> 00:01:24,299
some nodding good some people are awake

00:01:13,610 --> 00:01:28,320
SSD excuse me SSD is excuse me well too

00:01:24,299 --> 00:01:30,450
much smoking SSD is quite fast compared

00:01:28,320 --> 00:01:32,400
to hard drive but hard drive is spinning

00:01:30,450 --> 00:01:39,930
disk so it's slower and tape is very

00:01:32,400 --> 00:01:43,200
very very slow in terms of cost is in

00:01:39,930 --> 00:01:45,600
actual dollars pounds euros and / 1

00:01:43,200 --> 00:01:47,250
gigabyte of any of these things you'll

00:01:45,600 --> 00:01:49,500
find that a gigabyte of memory costs you

00:01:47,250 --> 00:01:50,790
a lot more than a gigabyte of SSD which

00:01:49,500 --> 00:01:52,979
costs you a lot more than a gigabyte of

00:01:50,790 --> 00:01:56,040
hard drive and so on so cost is an

00:01:52,979 --> 00:01:59,250
important factor I'd also say that the

00:01:56,040 --> 00:02:00,630
ease of use to you as a developer is

00:01:59,250 --> 00:02:02,579
definitely in this direction as well

00:02:00,630 --> 00:02:04,590
because if you want to use memory in

00:02:02,579 --> 00:02:07,170
order to store data it's actually really

00:02:04,590 --> 00:02:08,640
easy you just initialize an array or you

00:02:07,170 --> 00:02:11,009
initialize a set or something in your

00:02:08,640 --> 00:02:13,349
code use one line however if you're

00:02:11,009 --> 00:02:13,850
going to be using an SSD or a hard drive

00:02:13,349 --> 00:02:15,860
in order to

00:02:13,850 --> 00:02:17,660
be doing stuff for your data you might

00:02:15,860 --> 00:02:19,790
need to use an open source framework in

00:02:17,660 --> 00:02:21,620
order to do it so here's a very naive

00:02:19,790 --> 00:02:24,470
example of perhaps I could declare a set

00:02:21,620 --> 00:02:25,760
in memory of strings but if that's too

00:02:24,470 --> 00:02:27,170
big for memory then maybe I need to

00:02:25,760 --> 00:02:29,150
store some documents in elastic search

00:02:27,170 --> 00:02:30,980
or maybe solar and if we're going

00:02:29,150 --> 00:02:33,320
further down the chain and we're using

00:02:30,980 --> 00:02:35,810
hard drives we may need to use Hadoop or

00:02:33,320 --> 00:02:37,940
HDFS or something like that the idea

00:02:35,810 --> 00:02:39,740
with this is that if we keep things in

00:02:37,940 --> 00:02:42,670
memory it's so much easier for you when

00:02:39,740 --> 00:02:44,680
you're writing your programs to do stuff

00:02:42,670 --> 00:02:46,940
that error goes in the other direction

00:02:44,680 --> 00:02:49,160
when we're talking about storage per

00:02:46,940 --> 00:02:51,500
node so this is either on a physical

00:02:49,160 --> 00:02:54,020
server in your data center or AWS

00:02:51,500 --> 00:02:56,420
instance or something you'll find that

00:02:54,020 --> 00:02:58,850
usually you have more memory and sorry

00:02:56,420 --> 00:03:01,250
less memory available that you do space

00:02:58,850 --> 00:03:03,320
on an SSD and if you have some massive

00:03:01,250 --> 00:03:04,550
raid configuration of hard drives you

00:03:03,320 --> 00:03:06,350
can end up having a lot of storage

00:03:04,550 --> 00:03:08,990
available and hard drives so the

00:03:06,350 --> 00:03:11,300
question is how do we do more stuff here

00:03:08,990 --> 00:03:13,910
because not only is it faster it's also

00:03:11,300 --> 00:03:15,740
easier to use in your code and you have

00:03:13,910 --> 00:03:17,180
to spend less money on storing stuff and

00:03:15,740 --> 00:03:19,490
you don't need to maintain any external

00:03:17,180 --> 00:03:22,810
storage systems so that's the motivation

00:03:19,490 --> 00:03:24,950
for these data structures so

00:03:22,810 --> 00:03:27,440
probabilistic data structures allow you

00:03:24,950 --> 00:03:28,850
to do things in memory at a scale that

00:03:27,440 --> 00:03:31,310
you previously didn't think was possible

00:03:28,850 --> 00:03:33,230
they often use a really tiny memory

00:03:31,310 --> 00:03:34,790
footprint they also have really cool

00:03:33,230 --> 00:03:35,960
names so when you talk to your

00:03:34,790 --> 00:03:37,160
colleagues about them and you show them

00:03:35,960 --> 00:03:39,250
your code you submit your pull request

00:03:37,160 --> 00:03:41,330
you look really clever which is awesome

00:03:39,250 --> 00:03:43,190
and I think they're really fascinating

00:03:41,330 --> 00:03:44,690
things and as I said at the beginning of

00:03:43,190 --> 00:03:47,330
the talk sometimes the barrier to entry

00:03:44,690 --> 00:03:48,650
is a little bit high so what we're going

00:03:47,330 --> 00:03:49,850
to do today is we're not going to blind

00:03:48,650 --> 00:03:51,350
you with loads of mathematics we're

00:03:49,850 --> 00:03:53,420
going to look at some Java code that

00:03:51,350 --> 00:03:54,890
does things in a naive way we're going

00:03:53,420 --> 00:03:56,360
to look at the effect that has on the

00:03:54,890 --> 00:03:58,100
heat and how much memory we're using

00:03:56,360 --> 00:03:59,480
then we're going to walk through the

00:03:58,100 --> 00:04:00,680
algorithm of each of these data

00:03:59,480 --> 00:04:02,840
structures and show you how they work

00:04:00,680 --> 00:04:04,400
visually and then we're going to rewrite

00:04:02,840 --> 00:04:06,140
that code using the proper ballistic

00:04:04,400 --> 00:04:07,880
data structure in the library that you

00:04:06,140 --> 00:04:09,260
can go away in use today and then we'll

00:04:07,880 --> 00:04:10,820
look at the heap again so hopefully that

00:04:09,260 --> 00:04:12,890
gives you some sort of concrete examples

00:04:10,820 --> 00:04:14,090
of how you'd use them but one thing that

00:04:12,890 --> 00:04:16,040
you have to remember when you're using

00:04:14,090 --> 00:04:19,070
these data structures is a very

00:04:16,040 --> 00:04:22,070
important thing that as a developer you

00:04:19,070 --> 00:04:25,669
are able to accept a predictable level

00:04:22,070 --> 00:04:26,750
of in accuracy so inaccurate in the so

00:04:25,669 --> 00:04:27,660
much that when you use this data

00:04:26,750 --> 00:04:31,170
structure it

00:04:27,660 --> 00:04:33,360
be wrong and also it might be wrong by

00:04:31,170 --> 00:04:35,190
some amount but when you're using them

00:04:33,360 --> 00:04:37,830
you accept the amount that you accept it

00:04:35,190 --> 00:04:39,930
to be wrong and if you're ok with that

00:04:37,830 --> 00:04:41,220
then you can use them but another

00:04:39,930 --> 00:04:42,840
important thing to think about when you

00:04:41,220 --> 00:04:44,700
use these things not just in a back-end

00:04:42,840 --> 00:04:46,710
system but when you use them in

00:04:44,700 --> 00:04:47,940
something that's user facing is that the

00:04:46,710 --> 00:04:50,220
numbers that come out of these things

00:04:47,940 --> 00:04:51,270
might not be completely correct so that

00:04:50,220 --> 00:04:52,590
what does that mean to the business

00:04:51,270 --> 00:04:54,420
analysts who are using your software

00:04:52,590 --> 00:04:56,490
what does it mean to the users if things

00:04:54,420 --> 00:04:58,710
are slightly wrong just something to

00:04:56,490 --> 00:05:00,150
think about without further ado we're

00:04:58,710 --> 00:05:03,180
going to do three data structures today

00:05:00,150 --> 00:05:04,920
the first one bloom filters so let's get

00:05:03,180 --> 00:05:07,020
started on this they don't have anything

00:05:04,920 --> 00:05:08,430
to do with flowers at all they're named

00:05:07,020 --> 00:05:10,800
after a mathematician called Burton

00:05:08,430 --> 00:05:13,530
Howard bloom and what they're used for

00:05:10,800 --> 00:05:16,560
is set membership tests so you can test

00:05:13,530 --> 00:05:17,910
whether an element is in a set and to

00:05:16,560 --> 00:05:19,380
begin with the easiest way to explain

00:05:17,910 --> 00:05:22,440
why you would use and is to show a

00:05:19,380 --> 00:05:24,480
really naive example using Java so we're

00:05:22,440 --> 00:05:27,210
going to use a java.util.set here and

00:05:24,480 --> 00:05:28,950
this isn't how you would code visitors

00:05:27,210 --> 00:05:30,150
to a site because we're using strings

00:05:28,950 --> 00:05:32,310
for eye peas and all that kind of thing

00:05:30,150 --> 00:05:33,840
but it's just an example what we're

00:05:32,310 --> 00:05:35,730
doing at the top is we're making a new

00:05:33,840 --> 00:05:38,760
set of strings and hopefully you've all

00:05:35,730 --> 00:05:39,960
seen a set before in Java util then

00:05:38,760 --> 00:05:41,490
we're going to add three elements to

00:05:39,960 --> 00:05:43,740
this set which are three different IP

00:05:41,490 --> 00:05:46,230
addresses and then at the bottom there

00:05:43,740 --> 00:05:47,850
we can call the contains method to see

00:05:46,230 --> 00:05:50,400
whether one of these elements is in that

00:05:47,850 --> 00:05:52,590
set and the first element is because we

00:05:50,400 --> 00:05:54,950
put it in there and the second element

00:05:52,590 --> 00:05:57,540
isn't because we didn't put it in there

00:05:54,950 --> 00:06:00,240
pretty straightforward but the problem

00:05:57,540 --> 00:06:01,500
is if you're using a set in order to do

00:06:00,240 --> 00:06:03,390
this kind of comparison of whether

00:06:01,500 --> 00:06:04,620
something's there or not the set starts

00:06:03,390 --> 00:06:06,240
to get big because you're having to

00:06:04,620 --> 00:06:08,550
actually store the strings in the set

00:06:06,240 --> 00:06:11,850
when you're doing it so I wrote a little

00:06:08,550 --> 00:06:14,220
program I created random you you IDs the

00:06:11,850 --> 00:06:15,990
64 character unique identifiers in Java

00:06:14,220 --> 00:06:18,210
and then I use visual vm to have a look

00:06:15,990 --> 00:06:20,370
at heap and as we are moving up towards

00:06:18,210 --> 00:06:22,680
a million uuid is in our set we're

00:06:20,370 --> 00:06:25,860
looking at a fair amount of he pits 264

00:06:22,680 --> 00:06:27,419
meg at that point and if we were for

00:06:25,860 --> 00:06:28,860
example very popular website and we're

00:06:27,419 --> 00:06:30,780
receiving millions and millions maybe

00:06:28,860 --> 00:06:32,130
tens of millions of hits a day if we

00:06:30,780 --> 00:06:34,650
were trying to keep this all in memory

00:06:32,130 --> 00:06:38,910
it would suddenly get very very large so

00:06:34,650 --> 00:06:40,289
this is where bloom filters coming so

00:06:38,910 --> 00:06:41,139
yes there's a paper at the bottom which

00:06:40,289 --> 00:06:43,389
is called space-time

00:06:41,139 --> 00:06:44,979
trade-offs in hash coding with allowable

00:06:43,389 --> 00:06:46,689
errors and you'll see that it was

00:06:44,979 --> 00:06:48,460
published a very very long time ago when

00:06:46,689 --> 00:06:50,800
computers had significantly less memory

00:06:48,460 --> 00:06:54,819
than they do today so it comes from 1970

00:06:50,800 --> 00:06:56,229
I wasn't even alive that paper is quite

00:06:54,819 --> 00:06:57,939
old I wouldn't recommend reading it

00:06:56,229 --> 00:06:59,770
there's better summaries and hopefully

00:06:57,939 --> 00:07:01,479
this is one of them and the best way to

00:06:59,770 --> 00:07:03,969
show you how it works is just by example

00:07:01,479 --> 00:07:07,719
so we're going to initialize a new bloom

00:07:03,969 --> 00:07:10,449
filter there we go isn't that cool it's

00:07:07,719 --> 00:07:11,860
just an array of bits and it's of some

00:07:10,449 --> 00:07:14,319
size and we'll talk about how you

00:07:11,860 --> 00:07:17,319
determine the size of it later on but

00:07:14,319 --> 00:07:20,349
it's an array of bit they're all set to

00:07:17,319 --> 00:07:22,300
zero that's the first bit the second

00:07:20,349 --> 00:07:24,340
part is that we have some number of hash

00:07:22,300 --> 00:07:26,860
functions and all of these hash

00:07:24,340 --> 00:07:28,330
functions have to be different but

00:07:26,860 --> 00:07:30,490
there's I'm going to talk about how many

00:07:28,330 --> 00:07:31,599
you need later on and then what we're

00:07:30,490 --> 00:07:33,909
going to do is we're going to add an

00:07:31,599 --> 00:07:36,189
element to our bloom filter so like in

00:07:33,909 --> 00:07:37,990
that example the first element comes

00:07:36,189 --> 00:07:40,180
along it goes through the first hash

00:07:37,990 --> 00:07:42,310
function and it gets mapped to one of

00:07:40,180 --> 00:07:45,219
the bits in its array and then that bit

00:07:42,310 --> 00:07:47,259
get set to one and it also goes through

00:07:45,219 --> 00:07:49,120
the second hash function which maps to a

00:07:47,259 --> 00:07:52,349
different bit and that gets set to one

00:07:49,120 --> 00:07:54,339
does that make sense so far nodding good

00:07:52,349 --> 00:07:56,139
what we're going to do next is we're

00:07:54,339 --> 00:07:59,080
going to see if something that we didn't

00:07:56,139 --> 00:08:00,210
put in there is in the bloom filter so

00:07:59,080 --> 00:08:02,589
this element from the earlier example

00:08:00,210 --> 00:08:04,539
goes through the first hash function and

00:08:02,589 --> 00:08:07,180
it goes to one of the bits which is set

00:08:04,539 --> 00:08:09,310
to one and then it goes through the

00:08:07,180 --> 00:08:10,689
second hash function which doesn't map

00:08:09,310 --> 00:08:12,879
to any of the bits that have been set

00:08:10,689 --> 00:08:14,770
and no matter how many hash functions

00:08:12,879 --> 00:08:18,310
you have if any of the bits are not set

00:08:14,770 --> 00:08:19,839
it's definitely not in there when we

00:08:18,310 --> 00:08:22,000
check to see whether an element that we

00:08:19,839 --> 00:08:23,199
have put in there is there it goes

00:08:22,000 --> 00:08:25,240
through those two hash functions like

00:08:23,199 --> 00:08:28,210
before and it maps to the bits that have

00:08:25,240 --> 00:08:31,479
been set and in this situation the bloom

00:08:28,210 --> 00:08:33,490
filter tells you maybe so this is the

00:08:31,479 --> 00:08:35,140
probabilistic part in that there's a

00:08:33,490 --> 00:08:38,620
percentage of error that may be

00:08:35,140 --> 00:08:40,089
occurring here before we look into what

00:08:38,620 --> 00:08:42,070
the percentage of error is and how it

00:08:40,089 --> 00:08:43,269
affects the bloom filter let's just show

00:08:42,070 --> 00:08:44,949
you how you can whack one of these in

00:08:43,269 --> 00:08:47,709
your code straight away I would

00:08:44,949 --> 00:08:49,870
recommend using the guava library which

00:08:47,709 --> 00:08:51,250
is Google's sort of bag of Java tools

00:08:49,870 --> 00:08:53,079
that has loads of things in it and that

00:08:51,250 --> 00:08:54,030
was the latest version as of a couple of

00:08:53,079 --> 00:08:55,560
weeks ago

00:08:54,030 --> 00:08:57,270
and then I've rewritten that naive

00:08:55,560 --> 00:08:59,520
example with a set but using one of

00:08:57,270 --> 00:09:02,220
their bling filters so you see at the

00:08:59,520 --> 00:09:05,160
top there we've replaced our set with a

00:09:02,220 --> 00:09:06,870
bloom filter which takes strings and it

00:09:05,160 --> 00:09:09,330
has a static create method which takes

00:09:06,870 --> 00:09:11,310
two parameters the first one is this

00:09:09,330 --> 00:09:13,170
curious thing called a funnel all the

00:09:11,310 --> 00:09:14,910
funnel is is you're telling the balloon

00:09:13,170 --> 00:09:16,770
filter what object you are putting in

00:09:14,910 --> 00:09:19,890
there and then gravel will go and do

00:09:16,770 --> 00:09:21,660
some hashing for you there are built-in

00:09:19,890 --> 00:09:24,210
funnels for all of the primitive types

00:09:21,660 --> 00:09:25,530
like integers and strings and so on if

00:09:24,210 --> 00:09:27,360
you're going to be putting some of your

00:09:25,530 --> 00:09:29,730
own objects in there then you can define

00:09:27,360 --> 00:09:31,010
your own funnel and you say hash on this

00:09:29,730 --> 00:09:34,320
field in this field in this field and

00:09:31,010 --> 00:09:36,030
the second parameter there is the number

00:09:34,320 --> 00:09:39,150
of elements that I'm expecting to put

00:09:36,030 --> 00:09:41,280
into the bloom filter so it's 10,000 in

00:09:39,150 --> 00:09:42,480
this particular situation I mean as you

00:09:41,280 --> 00:09:43,920
can see that's actually wrong because

00:09:42,480 --> 00:09:45,870
I'm only putting free in so the bloom

00:09:43,920 --> 00:09:48,330
filter would be too big but just as an

00:09:45,870 --> 00:09:49,890
example and then liking the other set

00:09:48,330 --> 00:09:52,290
example we just put those elements in

00:09:49,890 --> 00:09:53,880
the bloom filter and then instead of a

00:09:52,290 --> 00:09:56,340
contains method like you get on a set

00:09:53,880 --> 00:09:58,020
you get a may contain and in this case

00:09:56,340 --> 00:09:59,520
the first one returns true because we

00:09:58,020 --> 00:10:01,700
put it in there and the second one

00:09:59,520 --> 00:10:05,310
returns false because we didn't

00:10:01,700 --> 00:10:09,900
understandable excellent so what does

00:10:05,310 --> 00:10:11,550
this do to our measurements well as the

00:10:09,900 --> 00:10:12,990
number of view IDs went up with a set we

00:10:11,550 --> 00:10:14,880
obviously had to store every single

00:10:12,990 --> 00:10:17,730
string in the set so we were getting on

00:10:14,880 --> 00:10:19,680
for 264 made when I was using the guava

00:10:17,730 --> 00:10:21,540
bloom filter you can see that the amount

00:10:19,680 --> 00:10:23,370
of megabyte of heaps being used for this

00:10:21,540 --> 00:10:25,230
is very very small so even when I was

00:10:23,370 --> 00:10:26,250
expecting a million elements we're only

00:10:25,230 --> 00:10:28,410
looking at Norton point nine megabytes

00:10:26,250 --> 00:10:29,940
in order to do this equality test so

00:10:28,410 --> 00:10:35,220
that saves a significant amount of space

00:10:29,940 --> 00:10:36,810
if you just want to do set membership so

00:10:35,220 --> 00:10:39,090
you're able to suggest the size that the

00:10:36,810 --> 00:10:41,730
bling filter should be in the parameters

00:10:39,090 --> 00:10:43,680
and we had 10,000 in the example what

00:10:41,730 --> 00:10:44,910
this changes in the bloom filter is the

00:10:43,680 --> 00:10:47,460
number of bits that you have in that

00:10:44,910 --> 00:10:48,840
array so the number of bits goes up as

00:10:47,460 --> 00:10:52,140
the number of things that you want to

00:10:48,840 --> 00:10:55,350
store goes up so if i wanted to store 10

00:10:52,140 --> 00:10:57,450
things it initialized with 40 bits when

00:10:55,350 --> 00:11:00,840
it was given a million as a suggested

00:10:57,450 --> 00:11:02,820
input it was 3.6 million bits in the

00:11:00,840 --> 00:11:04,470
bloom filter which is quite a lot but

00:11:02,820 --> 00:11:07,710
bits aren't that expensive compared to

00:11:04,470 --> 00:11:09,720
store in the strings by default in guava

00:11:07,710 --> 00:11:11,970
you get a three percent false positive

00:11:09,720 --> 00:11:14,310
probability rate and I abbreviate this

00:11:11,970 --> 00:11:16,440
to f PP in some of the tables if that

00:11:14,310 --> 00:11:18,510
and it's a little easier to put on the

00:11:16,440 --> 00:11:19,740
slides and what that means is that when

00:11:18,510 --> 00:11:21,600
it tells you that something is there

00:11:19,740 --> 00:11:24,270
there's a three percent chance that it

00:11:21,600 --> 00:11:27,150
actually isn't and you have to think

00:11:24,270 --> 00:11:28,680
about whether that's okay but with the

00:11:27,150 --> 00:11:30,510
guava implementation what's quite nice

00:11:28,680 --> 00:11:33,180
is it can take an additional third

00:11:30,510 --> 00:11:34,650
parameter and you can specify the error

00:11:33,180 --> 00:11:36,300
rate that you're happy with so if you

00:11:34,650 --> 00:11:37,830
think that three percent is too

00:11:36,300 --> 00:11:39,690
inaccurate you can give it one so this

00:11:37,830 --> 00:11:42,060
case it's a naught point five percent

00:11:39,690 --> 00:11:45,150
instead of three percent and what this

00:11:42,060 --> 00:11:48,120
does when you change that is that as you

00:11:45,150 --> 00:11:49,680
change the false probability rate the

00:11:48,120 --> 00:11:52,620
number of hash functions that the bloom

00:11:49,680 --> 00:11:54,240
filter is using increases so with my

00:11:52,620 --> 00:11:58,310
very accurate bloom filter at the bottom

00:11:54,240 --> 00:12:00,840
which is 0.0001 percent in accuracy

00:11:58,310 --> 00:12:02,730
there are 20 hash functions so every

00:12:00,840 --> 00:12:05,190
time I insert and read something is

00:12:02,730 --> 00:12:06,690
being hashed 20 times either way so

00:12:05,190 --> 00:12:08,010
those are the two design parameters the

00:12:06,690 --> 00:12:09,060
number of bits for the size and the

00:12:08,010 --> 00:12:11,340
number of hash functions for the

00:12:09,060 --> 00:12:13,440
probability and there you go you

00:12:11,340 --> 00:12:16,050
understand bloom filters wonderful so

00:12:13,440 --> 00:12:18,060
what are they used in generally caching

00:12:16,050 --> 00:12:19,170
stuff is a really useful use case

00:12:18,060 --> 00:12:22,290
there's a paper that I've linked to the

00:12:19,170 --> 00:12:23,790
bottom there which is from a coma which

00:12:22,290 --> 00:12:26,280
talked about using bloom filters in

00:12:23,790 --> 00:12:27,690
order to on high traffic websites the

00:12:26,280 --> 00:12:31,110
first time that you visit if you're like

00:12:27,690 --> 00:12:33,210
a what they call a one-hit-wonder serve

00:12:31,110 --> 00:12:35,070
up a cached version of the page if you

00:12:33,210 --> 00:12:36,600
are continuing to browse or maybe you're

00:12:35,070 --> 00:12:38,880
an actual human interacting with the

00:12:36,600 --> 00:12:39,870
website then using that bloom filter you

00:12:38,880 --> 00:12:42,150
can check whether they've been there

00:12:39,870 --> 00:12:45,720
before and you can serve up some more

00:12:42,150 --> 00:12:47,190
compute heavy content the second one

00:12:45,720 --> 00:12:48,840
there is quite interesting in HBase and

00:12:47,190 --> 00:12:50,790
Cassandra bloom filters used to encode

00:12:48,840 --> 00:12:52,380
the locations of data so that when you

00:12:50,790 --> 00:12:53,640
scan across the cluster it can skip out

00:12:52,380 --> 00:12:55,980
places where it knows that there is no

00:12:53,640 --> 00:12:58,230
data and one thing that we used it for

00:12:55,980 --> 00:12:59,820
and was in real time matching so people

00:12:58,230 --> 00:13:01,350
were able to search to specify a whole

00:12:59,820 --> 00:13:03,560
other people on Twitter who they were

00:13:01,350 --> 00:13:05,670
interested in getting alerts for and

00:13:03,560 --> 00:13:07,980
often these lists could be huge like

00:13:05,670 --> 00:13:09,150
millions of Twitter authors and then we

00:13:07,980 --> 00:13:11,580
see realize that into a bloom filter

00:13:09,150 --> 00:13:13,290
into the database and then we could poke

00:13:11,580 --> 00:13:15,060
it into our storm topology as a bloom

00:13:13,290 --> 00:13:16,350
filter that we read out so it's quite a

00:13:15,060 --> 00:13:18,810
nice technique as well if you don't need

00:13:16,350 --> 00:13:21,059
to know who those people are it's a

00:13:18,810 --> 00:13:22,409
great one down two to go

00:13:21,059 --> 00:13:24,929
next we're going to look at countenance

00:13:22,409 --> 00:13:27,449
sketch and this is a slightly more

00:13:24,929 --> 00:13:30,329
recent data structure it says not as old

00:13:27,449 --> 00:13:32,219
as 1970 and what it's used for as the

00:13:30,329 --> 00:13:34,789
name suggests is for tracking the counts

00:13:32,219 --> 00:13:37,199
so if you're for example interested in

00:13:34,789 --> 00:13:39,089
reading a stream of data from Twitter

00:13:37,199 --> 00:13:41,759
and then storing the number of times

00:13:39,089 --> 00:13:43,379
that you've seen each hashtag that kind

00:13:41,759 --> 00:13:46,109
of use case then count men sketch is

00:13:43,379 --> 00:13:49,259
what you want to use let's look at the

00:13:46,109 --> 00:13:50,609
really naive example first using Java so

00:13:49,259 --> 00:13:53,579
what I've used here just to keep it

00:13:50,609 --> 00:13:56,699
concisely as a multiset a multiset

00:13:53,579 --> 00:13:59,399
really is just a hashmap in the

00:13:56,699 --> 00:14:01,439
background it's a guava thing so the key

00:13:59,399 --> 00:14:03,179
of the hash map is the thing that we're

00:14:01,439 --> 00:14:06,299
tracking so for example the hashtag and

00:14:03,179 --> 00:14:08,189
then the value is the counter of how

00:14:06,299 --> 00:14:10,139
many times we've seen it so every time

00:14:08,189 --> 00:14:11,579
the hashtag would come along it would

00:14:10,139 --> 00:14:14,429
get added and then incremented the

00:14:11,579 --> 00:14:16,859
counter certainly sense cool so in this

00:14:14,429 --> 00:14:19,769
example we use the IP addresses again

00:14:16,859 --> 00:14:21,959
just just to show we create a multiset

00:14:19,769 --> 00:14:23,909
at the top which is of string generic

00:14:21,959 --> 00:14:25,889
type and then we add some things into it

00:14:23,909 --> 00:14:29,099
the first two elements get added once

00:14:25,889 --> 00:14:31,259
and then the last element NER gets added

00:14:29,099 --> 00:14:33,539
twice and then there's a count method

00:14:31,259 --> 00:14:35,309
that you can call to show you how many

00:14:33,539 --> 00:14:36,749
times you've seen it that elements at

00:14:35,309 --> 00:14:38,339
the top there we've seen twice because

00:14:36,749 --> 00:14:41,119
we added it twice and in the last

00:14:38,339 --> 00:14:44,579
element 0 times because we didn't add it

00:14:41,119 --> 00:14:47,399
really simple stuff but like the set

00:14:44,579 --> 00:14:48,899
example before you'll also see that it

00:14:47,399 --> 00:14:50,639
starts to use a lot of heap when you're

00:14:48,899 --> 00:14:52,649
traveling lots of things and the thing

00:14:50,639 --> 00:14:53,729
that's also annoying about the counting

00:14:52,649 --> 00:14:55,259
problem is that often if you were

00:14:53,729 --> 00:14:57,929
looking at streams of data like tweets

00:14:55,259 --> 00:14:58,919
the tale of these things is huge so the

00:14:57,929 --> 00:15:02,279
amount of things that have only been

00:14:58,919 --> 00:15:03,509
tweeted once is often very very big so I

00:15:02,279 --> 00:15:06,689
did the same thing as before so I

00:15:03,509 --> 00:15:08,489
generated random new UID and the heap

00:15:06,689 --> 00:15:10,889
similar kind of profile so as we got

00:15:08,489 --> 00:15:14,039
towards a million elements in our count

00:15:10,889 --> 00:15:16,559
0 in our multi set we're looking at 234

00:15:14,039 --> 00:15:18,419
Meg being stored which is quite a lot so

00:15:16,559 --> 00:15:19,919
if you're looking at for example the

00:15:18,419 --> 00:15:21,689
Twitter Decker hose which is like 50

00:15:19,919 --> 00:15:22,739
million tweets a day and trying to keep

00:15:21,689 --> 00:15:25,019
this in memory it starts to get really

00:15:22,739 --> 00:15:26,429
big prohibitively big and then you might

00:15:25,019 --> 00:15:27,599
want to spill it to a data store but

00:15:26,429 --> 00:15:29,699
then you've got the time that it takes

00:15:27,599 --> 00:15:32,519
to do things with that and so on can you

00:15:29,699 --> 00:15:34,840
keep it in memory you can with kampmann

00:15:32,519 --> 00:15:37,400
sketch so here's a paper which is quite

00:15:34,840 --> 00:15:40,070
it's not the original paper the original

00:15:37,400 --> 00:15:41,810
papers from 2003 but this was a sort of

00:15:40,070 --> 00:15:43,700
summary paper that the original authors

00:15:41,810 --> 00:15:46,420
wrote which is only a few pages long and

00:15:43,700 --> 00:15:48,560
it describes it really concisely and

00:15:46,420 --> 00:15:51,830
it's called can't mean sketch because it

00:15:48,560 --> 00:15:53,360
does counting excellent it uses the word

00:15:51,830 --> 00:15:55,070
min because you will see that it uses a

00:15:53,360 --> 00:15:57,110
minimum function in order to find out

00:15:55,070 --> 00:15:59,330
what the count is and it's a sketch

00:15:57,110 --> 00:16:01,100
because it's kind of an approximation so

00:15:59,330 --> 00:16:03,290
what does it look like and I'll leave

00:16:01,100 --> 00:16:06,440
this here for just a few seconds it

00:16:03,290 --> 00:16:08,240
looks like this so remember that the

00:16:06,440 --> 00:16:10,970
bloom filter wasn't an array of bits

00:16:08,240 --> 00:16:12,860
which we were using the count mean

00:16:10,970 --> 00:16:14,480
sketch is an array of counters this time

00:16:12,860 --> 00:16:17,210
so we want to be able to count things so

00:16:14,480 --> 00:16:18,710
imagine that they're integers but we

00:16:17,210 --> 00:16:20,810
don't only just have one row of counters

00:16:18,710 --> 00:16:22,670
we have multiple rows and when we talk

00:16:20,810 --> 00:16:24,800
about account mean sketch we say that

00:16:22,670 --> 00:16:26,480
the width of the sketch is the number of

00:16:24,800 --> 00:16:28,730
counters that we have on every row and

00:16:26,480 --> 00:16:31,910
then the depth of the sketch is the

00:16:28,730 --> 00:16:33,830
number of rows that we have each of

00:16:31,910 --> 00:16:35,840
these rows if you look on the right hand

00:16:33,830 --> 00:16:37,820
side has a hash function that's

00:16:35,840 --> 00:16:39,590
associated with it now it's important

00:16:37,820 --> 00:16:41,210
that every row has a different hash

00:16:39,590 --> 00:16:43,820
function that's really important and

00:16:41,210 --> 00:16:45,290
you'll see why in a minute and it's best

00:16:43,820 --> 00:16:47,000
to show how it works by example I think

00:16:45,290 --> 00:16:48,290
so here's what we're going to do we're

00:16:47,000 --> 00:16:49,580
going to add an element in and I'm also

00:16:48,290 --> 00:16:52,400
going to shrink the sketch down so it

00:16:49,580 --> 00:16:55,250
fits on the slide let's add our first

00:16:52,400 --> 00:16:56,750
element the element comes in and then we

00:16:55,250 --> 00:16:58,190
go to the first row of counters which

00:16:56,750 --> 00:17:00,800
has the first hash function associated

00:16:58,190 --> 00:17:02,300
with it and we hash it and it maps to

00:17:00,800 --> 00:17:05,300
one of those counters and it is

00:17:02,300 --> 00:17:07,820
incremented from 0 to 1 and then on the

00:17:05,300 --> 00:17:09,470
second row we also run it through that

00:17:07,820 --> 00:17:10,880
different hash function and it maps to

00:17:09,470 --> 00:17:13,339
one of the other counters and it

00:17:10,880 --> 00:17:15,500
increments from 0 to 1 and then the same

00:17:13,339 --> 00:17:18,380
again on the third row it comes in it

00:17:15,500 --> 00:17:20,270
maps to the counter over there from the

00:17:18,380 --> 00:17:25,070
third hash function can it increments

00:17:20,270 --> 00:17:27,470
from 0 to 1 that make sense cool so then

00:17:25,070 --> 00:17:29,870
let's add the same element again for the

00:17:27,470 --> 00:17:31,820
second time each of these hash functions

00:17:29,870 --> 00:17:33,980
will map to exactly the same places and

00:17:31,820 --> 00:17:38,270
it finds a 1 there and then all of these

00:17:33,980 --> 00:17:40,310
ones get incremented to two cool let's

00:17:38,270 --> 00:17:43,700
add a different element in this time so

00:17:40,310 --> 00:17:45,740
here's a new element on the first row it

00:17:43,700 --> 00:17:47,270
maps to that first counter which was

00:17:45,740 --> 00:17:48,650
zero and it increments it to one and

00:17:47,270 --> 00:17:50,210
then on the second row

00:17:48,650 --> 00:17:52,460
I've done here as I've simulated a hash

00:17:50,210 --> 00:17:54,650
collision taking place so this element

00:17:52,460 --> 00:17:57,140
on the second row maps to exactly the

00:17:54,650 --> 00:17:59,510
same counter this time it can't Lee

00:17:57,140 --> 00:18:01,070
increment from 0 to 1 it increments 2 to

00:17:59,510 --> 00:18:03,290
3 so this is where it starts to get

00:18:01,070 --> 00:18:05,420
wrong but that's ok as you will see in a

00:18:03,290 --> 00:18:07,540
minute on the third row it maps to a new

00:18:05,420 --> 00:18:10,190
counter and increments from 0 to 1 so

00:18:07,540 --> 00:18:12,080
now we want a query count mean sketch to

00:18:10,190 --> 00:18:14,720
say how many times have I seen a certain

00:18:12,080 --> 00:18:16,190
thing for the first thing that we see

00:18:14,720 --> 00:18:18,470
which is the first element that we added

00:18:16,190 --> 00:18:20,330
we run through the hash functions again

00:18:18,470 --> 00:18:22,820
and then we get the values that are at

00:18:20,330 --> 00:18:25,790
those places first has shrunk from fines

00:18:22,820 --> 00:18:27,260
to the second hash function is where the

00:18:25,790 --> 00:18:30,110
collision took place at it finds three

00:18:27,260 --> 00:18:31,760
the third hash function Maps 22 and then

00:18:30,110 --> 00:18:34,010
here's the min part of counting sketches

00:18:31,760 --> 00:18:35,840
it takes the minimum of those values in

00:18:34,010 --> 00:18:37,700
order to give you a result so the

00:18:35,840 --> 00:18:40,640
minimum of two three and two is two

00:18:37,700 --> 00:18:42,620
which is correct in this case and then

00:18:40,640 --> 00:18:44,270
if we try and get the value out of the

00:18:42,620 --> 00:18:46,610
second element that we added on the

00:18:44,270 --> 00:18:48,080
first row it maps to a one the second

00:18:46,610 --> 00:18:50,300
row is where it collided and it maps to

00:18:48,080 --> 00:18:52,130
a three and then on the third row it

00:18:50,300 --> 00:18:53,780
maps to a one and then we take the

00:18:52,130 --> 00:18:56,090
minimum of those three values and we get

00:18:53,780 --> 00:18:58,040
a one so you can see that by the hash

00:18:56,090 --> 00:18:59,570
functions being different it means you

00:18:58,040 --> 00:19:02,480
will get collisions but because you have

00:18:59,570 --> 00:19:03,860
lots of rows hopefully when you go over

00:19:02,480 --> 00:19:06,650
all of them you'll get one of them that

00:19:03,860 --> 00:19:07,820
is accurate enough so the thing that's

00:19:06,650 --> 00:19:10,340
different about the bling filter with

00:19:07,820 --> 00:19:11,660
counting sketch is that for a human

00:19:10,340 --> 00:19:13,100
being with the bloomfield so i think

00:19:11,660 --> 00:19:14,420
about how many things i'm going to put

00:19:13,100 --> 00:19:16,640
in there and then i'm going to specify

00:19:14,420 --> 00:19:18,440
that amount and then that's how it works

00:19:16,640 --> 00:19:20,060
the count being sketch is kind of

00:19:18,440 --> 00:19:21,650
different because it initializes to a

00:19:20,060 --> 00:19:24,110
fixed size at the beginning and it never

00:19:21,650 --> 00:19:26,480
changes and the parameters that you use

00:19:24,110 --> 00:19:27,770
in order to size it are a bit more

00:19:26,480 --> 00:19:31,160
awkward to get your head around and

00:19:27,770 --> 00:19:33,380
here's what they are two parameters the

00:19:31,160 --> 00:19:36,710
first one is called epsilon which is

00:19:33,380 --> 00:19:38,690
util and and this is the accepted amount

00:19:36,710 --> 00:19:41,540
of error that you are ok with being

00:19:38,690 --> 00:19:45,170
added to each item so how wrong are you

00:19:41,540 --> 00:19:47,150
ok with accounts being the second one is

00:19:45,170 --> 00:19:49,670
called delta which is also really

00:19:47,150 --> 00:19:52,550
helpful and this is the probability that

00:19:49,670 --> 00:19:55,480
when you receive a result that it's

00:19:52,550 --> 00:19:59,240
outside of that epsilon value so it's

00:19:55,480 --> 00:20:00,770
wronger than you are okay with and those

00:19:59,240 --> 00:20:03,110
are your two parameters

00:20:00,770 --> 00:20:04,970
and this code is taken from the library

00:20:03,110 --> 00:20:07,550
that I'm just about to show you if you

00:20:04,970 --> 00:20:09,500
plug epsilon and Delta into these two

00:20:07,550 --> 00:20:11,090
equations then that calculates the whip

00:20:09,500 --> 00:20:12,740
from the depth of the sketch and we'll

00:20:11,090 --> 00:20:15,980
have a look at how that changes with the

00:20:12,740 --> 00:20:17,750
different values in a second so you kind

00:20:15,980 --> 00:20:19,610
of understand it which is good let's use

00:20:17,750 --> 00:20:21,470
it in some code so i would recommend

00:20:19,610 --> 00:20:24,230
using the clear spring analytic string

00:20:21,470 --> 00:20:26,750
library this was written by the folks

00:20:24,230 --> 00:20:28,370
that add this which I think do buttons

00:20:26,750 --> 00:20:29,510
and websites where you share stuff and

00:20:28,370 --> 00:20:31,220
then it counts how many times it's been

00:20:29,510 --> 00:20:33,680
shared so I assume that they use this

00:20:31,220 --> 00:20:35,900
stuff quite a lot and I've rewritten the

00:20:33,680 --> 00:20:37,310
example that we had earlier so replace

00:20:35,900 --> 00:20:40,310
the multiset with a count mean sketch

00:20:37,310 --> 00:20:42,320
and it's very easy you're declaring a

00:20:40,310 --> 00:20:43,730
count min sketch at the top and it's

00:20:42,320 --> 00:20:46,250
taking three parameters in the

00:20:43,730 --> 00:20:49,790
constructor here the first one is that

00:20:46,250 --> 00:20:51,680
epsilon value which is how accurate how

00:20:49,790 --> 00:20:53,840
acceptable accurate should my counts be

00:20:51,680 --> 00:20:57,500
so in this particular case I want them

00:20:53,840 --> 00:20:59,600
to be 0.1 percent accurate and then the

00:20:57,500 --> 00:21:02,810
second parameter which is naught point

00:20:59,600 --> 00:21:03,830
99 there is how much of the time should

00:21:02,810 --> 00:21:05,570
they be within the bounds that I

00:21:03,830 --> 00:21:06,830
specified I know that's difficult to get

00:21:05,570 --> 00:21:09,230
your head around but it makes sense

00:21:06,830 --> 00:21:11,030
after a while and the third parameter

00:21:09,230 --> 00:21:13,430
there which is I wish there just

00:21:11,030 --> 00:21:16,040
overridden it is a random seed I've just

00:21:13,430 --> 00:21:18,080
put one and then you can add things into

00:21:16,040 --> 00:21:19,790
the sketch added those first two items

00:21:18,080 --> 00:21:22,130
once and then I've added the third item

00:21:19,790 --> 00:21:24,290
twice and then i can call estimate count

00:21:22,130 --> 00:21:26,090
instead of count and it tells me to for

00:21:24,290 --> 00:21:29,150
the first one and zero for the second

00:21:26,090 --> 00:21:30,650
one so with direct comparison of the

00:21:29,150 --> 00:21:32,630
bloom filter in the heat it's kind of

00:21:30,650 --> 00:21:34,070
difficult to do because it's it doesn't

00:21:32,630 --> 00:21:36,590
scale with the number of things that go

00:21:34,070 --> 00:21:38,330
in there but just to remind ourselves

00:21:36,590 --> 00:21:43,070
when the multiset had a million elements

00:21:38,330 --> 00:21:44,120
in it it was 234 megabytes of heap if we

00:21:43,070 --> 00:21:46,880
have a look at what tuning the

00:21:44,120 --> 00:21:48,560
parameters does at the top we've got a

00:21:46,880 --> 00:21:50,870
counting sketch that's fairly inaccurate

00:21:48,560 --> 00:21:52,940
I'm accepting ten percent difference in

00:21:50,870 --> 00:21:54,140
the counts that come out but still 99

00:21:52,940 --> 00:21:56,630
percent of the time it will be within

00:21:54,140 --> 00:21:58,490
that and you'll see that as you tune it

00:21:56,630 --> 00:22:00,530
to be more and more accurate with the

00:21:58,490 --> 00:22:02,600
epsilon the Delta increasing the number

00:22:00,530 --> 00:22:04,670
of counters the width on every row goes

00:22:02,600 --> 00:22:06,860
up as you'd expect but actually not up

00:22:04,670 --> 00:22:09,800
by that much it goes from seven to 17

00:22:06,860 --> 00:22:12,350
counters per row but where it gets quite

00:22:09,800 --> 00:22:13,610
big is the depth so in the very accurate

00:22:12,350 --> 00:22:14,150
count mean sketch of the bottom there

00:22:13,610 --> 00:22:16,010
are 20

00:22:14,150 --> 00:22:19,540
thousand rows of counters which

00:22:16,010 --> 00:22:21,920
obviously means 20,000 hashes being done

00:22:19,540 --> 00:22:24,050
but the nice thing is is that the heat

00:22:21,920 --> 00:22:26,030
usage is very small so you can keep the

00:22:24,050 --> 00:22:28,760
counts of millions of million

00:22:26,030 --> 00:22:29,810
developments in about 2.7 megs of heat

00:22:28,760 --> 00:22:31,630
there if you want something fairly

00:22:29,810 --> 00:22:34,160
accurate or if you want something that's

00:22:31,630 --> 00:22:36,440
fairly inaccurate you can get away with

00:22:34,160 --> 00:22:39,770
barely any heap at all which is much

00:22:36,440 --> 00:22:41,870
nicer than using the multiset and use

00:22:39,770 --> 00:22:43,370
cases this any kind of frequency

00:22:41,870 --> 00:22:46,640
tracking counting stuff is the obvious

00:22:43,370 --> 00:22:48,380
thing one really nice use case is NLP so

00:22:46,640 --> 00:22:50,540
I read an example where they took a 90

00:22:48,380 --> 00:22:52,570
gigabyte corpus of text and then they

00:22:50,540 --> 00:22:55,460
funneled it all into account min sketch

00:22:52,570 --> 00:22:57,500
to do the word frequencies and that fit

00:22:55,460 --> 00:22:58,940
into about 8 gigabytes of data structure

00:22:57,500 --> 00:23:00,080
which then means you can keep it in

00:22:58,940 --> 00:23:02,450
memory and do stuff with it which is

00:23:00,080 --> 00:23:03,710
really cool there's two extensions I'm

00:23:02,450 --> 00:23:05,570
not going to talk about today because I

00:23:03,710 --> 00:23:07,850
will run out of time but there's a an

00:23:05,570 --> 00:23:09,920
extension called heavy hitters which

00:23:07,850 --> 00:23:12,140
uses a heap next to the counting sketch

00:23:09,920 --> 00:23:13,730
in order to keep the top x of things

00:23:12,140 --> 00:23:16,010
that you've seen so the obvious use case

00:23:13,730 --> 00:23:18,350
there is what are the top hashtags right

00:23:16,010 --> 00:23:19,790
now in my area and there's also an

00:23:18,350 --> 00:23:21,560
extension called range query which will

00:23:19,790 --> 00:23:22,730
probably have heard of in solar only

00:23:21,560 --> 00:23:25,250
seen and I'll leave that as an exercise

00:23:22,730 --> 00:23:26,900
for you to read about it so good we're

00:23:25,250 --> 00:23:31,400
two things down is everyone still awake

00:23:26,900 --> 00:23:33,350
and alive good the last one the more

00:23:31,400 --> 00:23:35,600
complicated one and I'll apologize

00:23:33,350 --> 00:23:37,190
upfront for having skimmed over some of

00:23:35,600 --> 00:23:39,350
the maths in this one but I hope that

00:23:37,190 --> 00:23:41,240
you'll thank me for it in Susa the one

00:23:39,350 --> 00:23:44,330
with the silliest name hyper log log

00:23:41,240 --> 00:23:46,640
who's heard of hyper log log most of the

00:23:44,330 --> 00:23:49,100
room so hopefully I'll get this right

00:23:46,640 --> 00:23:50,390
the easiest way to understand it what

00:23:49,100 --> 00:23:52,340
you tend to do is you go and download

00:23:50,390 --> 00:23:54,140
the paper and then you open up the paper

00:23:52,340 --> 00:23:55,730
and then you look at all that crazy

00:23:54,140 --> 00:23:57,080
maths and then you get very scared and

00:23:55,730 --> 00:23:58,880
then you put it away and you're going to

00:23:57,080 --> 00:24:00,110
do something else and so what I'm going

00:23:58,880 --> 00:24:02,540
to show you this sort of the journey

00:24:00,110 --> 00:24:05,180
that the authors take towards getting

00:24:02,540 --> 00:24:07,220
towards high / low blog so what is it

00:24:05,180 --> 00:24:09,770
forward to begin with it's for

00:24:07,220 --> 00:24:11,630
cardinality estimation so given some

00:24:09,770 --> 00:24:13,460
huge list of items what's the

00:24:11,630 --> 00:24:15,530
cardinality so how many unique items are

00:24:13,460 --> 00:24:17,270
their we're going to start off as per

00:24:15,530 --> 00:24:19,340
the other ones with our naive java

00:24:17,270 --> 00:24:21,800
example and for this I'm just going to

00:24:19,340 --> 00:24:23,840
reuse the set because the mathematical

00:24:21,800 --> 00:24:25,970
property of a set is that each item is

00:24:23,840 --> 00:24:27,890
unique so what going to do is create a

00:24:25,970 --> 00:24:28,280
new hash set I'm going to stick

00:24:27,890 --> 00:24:30,710
something

00:24:28,280 --> 00:24:32,030
in there there's three unique elements

00:24:30,710 --> 00:24:34,730
there because we add one thing three

00:24:32,030 --> 00:24:37,070
times we call size which is effectively

00:24:34,730 --> 00:24:38,900
the cardinality and it says three you

00:24:37,070 --> 00:24:41,570
wonder how sets work hopefully that

00:24:38,900 --> 00:24:44,240
makes sense but just to remind you as

00:24:41,570 --> 00:24:45,530
before when we're using large amounts of

00:24:44,240 --> 00:24:47,330
things in that set when we get to a

00:24:45,530 --> 00:24:48,890
million elements we're looking at

00:24:47,330 --> 00:24:51,140
hundreds of megabytes of heat being used

00:24:48,890 --> 00:24:53,090
and if you all for example computing the

00:24:51,140 --> 00:24:55,040
daily visitors of a top website like

00:24:53,090 --> 00:24:56,660
LinkedIn or Google or something you can

00:24:55,040 --> 00:24:57,800
see that doing this cardinalis and

00:24:56,660 --> 00:24:59,390
stuffing memory is actually quite a

00:24:57,800 --> 00:25:02,150
challenge because you have to hang on to

00:24:59,390 --> 00:25:03,650
all of it while you're doing it so we're

00:25:02,150 --> 00:25:06,800
going to do now is a very gentle walk

00:25:03,650 --> 00:25:08,510
into hyper log log so it's an iteration

00:25:06,800 --> 00:25:11,240
of an iteration of an iteration of

00:25:08,510 --> 00:25:13,250
different techniques and if you see the

00:25:11,240 --> 00:25:14,720
progression it's I think easier to

00:25:13,250 --> 00:25:17,120
getting into intuition as to how it

00:25:14,720 --> 00:25:19,190
works the actual implementation is still

00:25:17,120 --> 00:25:21,320
quite complicated but I'm hoping that it

00:25:19,190 --> 00:25:23,360
will have some kind of impact by going

00:25:21,320 --> 00:25:25,610
for it today we're going to start off

00:25:23,360 --> 00:25:27,800
with something called linear counting so

00:25:25,610 --> 00:25:29,210
this is from a paper in 1990 and it's

00:25:27,800 --> 00:25:30,530
called a linear time probabilistic

00:25:29,210 --> 00:25:32,420
counting algorithm for database

00:25:30,530 --> 00:25:34,070
applications the application they're

00:25:32,420 --> 00:25:36,320
talking about here is if I have a

00:25:34,070 --> 00:25:38,000
massive database table how do I know how

00:25:36,320 --> 00:25:41,570
big it is without having to actually

00:25:38,000 --> 00:25:42,770
scan everything they use this technique

00:25:41,570 --> 00:25:45,170
that looks a little bit like a bloom

00:25:42,770 --> 00:25:46,700
filter so here we have an array of bits

00:25:45,170 --> 00:25:48,980
we had an array of bits in our bloom

00:25:46,700 --> 00:25:52,430
filter it's of some particular size

00:25:48,980 --> 00:25:53,570
we're going to call it size n then what

00:25:52,430 --> 00:25:56,150
we're going to do is have one hash

00:25:53,570 --> 00:25:58,130
function just one and we're going to

00:25:56,150 --> 00:26:00,020
insert an element we run it through our

00:25:58,130 --> 00:26:03,500
hash function and it maps to one of the

00:26:00,020 --> 00:26:04,670
bits and it sets it to one and then

00:26:03,500 --> 00:26:06,770
we're going to add another element and

00:26:04,670 --> 00:26:09,500
it's going to nap to one of the bits and

00:26:06,770 --> 00:26:10,760
we're going to sell it to one and then

00:26:09,500 --> 00:26:12,940
what we're going to do is an equation

00:26:10,760 --> 00:26:16,820
and I'll explain what this equation is

00:26:12,940 --> 00:26:19,250
so the variables in this equation M is

00:26:16,820 --> 00:26:21,620
the width of our linear counter that

00:26:19,250 --> 00:26:24,740
means how many bits do we have and then

00:26:21,620 --> 00:26:26,420
W is the weight of our mask which is

00:26:24,740 --> 00:26:28,730
just the number of ones that we have and

00:26:26,420 --> 00:26:30,800
then L n is the natural logarithm so if

00:26:28,730 --> 00:26:32,720
you plug in this you get minus ten times

00:26:30,800 --> 00:26:35,060
the natural logarithm of ten minus two

00:26:32,720 --> 00:26:37,760
divided by 10 which is 2.2 as an

00:26:35,060 --> 00:26:39,940
estimation of the cardinality the

00:26:37,760 --> 00:26:41,700
cardinality is too so it's not far off

00:26:39,940 --> 00:26:43,980
however the very

00:26:41,700 --> 00:26:46,559
it's on this is really really big and it

00:26:43,980 --> 00:26:47,820
gets quite unwieldy at large amounts but

00:26:46,559 --> 00:26:49,260
there's an intuition there that you can

00:26:47,820 --> 00:26:52,169
record something and then do an equation

00:26:49,260 --> 00:26:53,669
in order to estimate so next we're going

00:26:52,169 --> 00:26:55,200
to move on to log log and this is much

00:26:53,669 --> 00:26:56,460
more powerful but it's much more

00:26:55,200 --> 00:26:57,840
complicated and we're going to look at

00:26:56,460 --> 00:27:00,450
the multiple steps that we used to get

00:26:57,840 --> 00:27:02,519
there first off we're going to have an

00:27:00,450 --> 00:27:05,070
intuition of flipping coins so

00:27:02,519 --> 00:27:07,320
everyone's flips a coin before fifty

00:27:05,070 --> 00:27:09,750
percent chance that you get ahead fifty

00:27:07,320 --> 00:27:12,269
percent chance that you get a tail now

00:27:09,750 --> 00:27:13,350
if I got five heads in a row how many

00:27:12,269 --> 00:27:17,070
times do you think I flipped the coin

00:27:13,350 --> 00:27:19,470
maybe twenty something like that if I

00:27:17,070 --> 00:27:21,139
got a hundred heads in a row how many

00:27:19,470 --> 00:27:24,000
times would I've been flipping coins for

00:27:21,139 --> 00:27:25,740
thousands if I got ten thousand heads in

00:27:24,000 --> 00:27:27,899
a row I've probably been flipping the

00:27:25,740 --> 00:27:29,340
coin for a very very very long time the

00:27:27,899 --> 00:27:32,100
idea is the more things you get in a row

00:27:29,340 --> 00:27:33,539
the less likely is that it would have

00:27:32,100 --> 00:27:36,240
happened and the longer I would have

00:27:33,539 --> 00:27:39,019
been doing that particular thing so with

00:27:36,240 --> 00:27:42,419
that intuition in mind of rows of heads

00:27:39,019 --> 00:27:45,090
let's use hashing so let's take an

00:27:42,419 --> 00:27:47,190
element let's hash it and then get the

00:27:45,090 --> 00:27:49,470
binary representation and then let's

00:27:47,190 --> 00:27:52,230
count the number of zeros that it begins

00:27:49,470 --> 00:27:54,630
with this particular binary

00:27:52,230 --> 00:27:56,519
representation starts with the one so we

00:27:54,630 --> 00:28:00,029
have no leading zeros whatsoever so we

00:27:56,519 --> 00:28:03,000
record a zero second one let's hash it

00:28:00,029 --> 00:28:05,309
same hash function and then record the

00:28:03,000 --> 00:28:07,669
number of leading zeros which is in this

00:28:05,309 --> 00:28:09,929
case one because it starts with 10 and

00:28:07,669 --> 00:28:13,200
then another element and so on and

00:28:09,929 --> 00:28:14,519
record the number of leading zeros now

00:28:13,200 --> 00:28:17,399
what if we could use that intuition that

00:28:14,519 --> 00:28:20,010
it's more rare to get lots of leading

00:28:17,399 --> 00:28:21,690
zeros in a row and plug that into an

00:28:20,010 --> 00:28:24,779
equation we could do something really

00:28:21,690 --> 00:28:26,399
really naive like 2 to the power of the

00:28:24,779 --> 00:28:28,620
maximum number of leading zeros that we

00:28:26,399 --> 00:28:30,570
had to estimate so 2 to the power of

00:28:28,620 --> 00:28:33,059
maximum number of leading zeros which is

00:28:30,570 --> 00:28:35,250
1 so 2 to the 1 is 2 and I can't now as

00:28:33,059 --> 00:28:37,710
the estimate is too but that's wrong

00:28:35,250 --> 00:28:39,779
because it's three but he's just another

00:28:37,710 --> 00:28:42,990
step towards getting to where they're

00:28:39,779 --> 00:28:44,429
going so the author's then show how you

00:28:42,990 --> 00:28:45,929
can take that intuition and you can

00:28:44,429 --> 00:28:47,970
improve it and one way that you can

00:28:45,929 --> 00:28:50,700
improve it is to do lots and lots and

00:28:47,970 --> 00:28:52,350
lots of different hashes i'm talking

00:28:50,700 --> 00:28:54,780
like many many many different hashes and

00:28:52,350 --> 00:28:56,580
then you can take the average of all the

00:28:54,780 --> 00:28:57,660
according that you've seen in order to

00:28:56,580 --> 00:29:00,330
give you something that's slightly more

00:28:57,660 --> 00:29:02,460
accurate but the problem is as you throw

00:29:00,330 --> 00:29:04,620
all these hashes at it it starts to get

00:29:02,460 --> 00:29:05,850
really expensive in a bit clunky so is

00:29:04,620 --> 00:29:07,410
there something more clever that you can

00:29:05,850 --> 00:29:08,850
do and the authors show that there is

00:29:07,410 --> 00:29:10,920
and there's this thing called stochastic

00:29:08,850 --> 00:29:12,240
averaging which I'm not going to go into

00:29:10,920 --> 00:29:14,490
the maps but I'll give you an intuition

00:29:12,240 --> 00:29:16,950
what you can do is you can get away with

00:29:14,490 --> 00:29:19,680
just using one hash function on the

00:29:16,950 --> 00:29:21,630
right hand side you declare a number of

00:29:19,680 --> 00:29:23,370
buckets and this is very similar to

00:29:21,630 --> 00:29:26,340
count mean sketch they're just an array

00:29:23,370 --> 00:29:28,020
of counters and those counters are where

00:29:26,340 --> 00:29:30,690
you're going to store the number of

00:29:28,020 --> 00:29:34,050
leading zeros that you've seen and what

00:29:30,690 --> 00:29:36,000
you do is when you hash it once you slip

00:29:34,050 --> 00:29:37,980
off the prefix of the hash and I've just

00:29:36,000 --> 00:29:41,030
used for as an example it's different in

00:29:37,980 --> 00:29:44,280
the real thing though and that prefix

00:29:41,030 --> 00:29:46,170
maps to one of the buckets and then the

00:29:44,280 --> 00:29:48,980
remaining part of the hash you count the

00:29:46,170 --> 00:29:50,940
leading zeros and then in that bucket if

00:29:48,980 --> 00:29:53,100
the number of leavings areas that you

00:29:50,940 --> 00:29:55,970
have is greater than or equal to the one

00:29:53,100 --> 00:29:58,800
that's already in there you record it I

00:29:55,970 --> 00:30:00,330
hope that makes sense but you take the

00:29:58,800 --> 00:30:02,040
first bit nuts to a bucket record the

00:30:00,330 --> 00:30:03,510
leading zeros of the last bit and then

00:30:02,040 --> 00:30:05,910
you have a fixed number of counters that

00:30:03,510 --> 00:30:07,830
you are using and then the authors do

00:30:05,910 --> 00:30:12,180
something even more magic which is they

00:30:07,830 --> 00:30:15,030
show that given that you two and you

00:30:12,180 --> 00:30:16,440
raise it to the power of the sum of the

00:30:15,030 --> 00:30:17,640
maximum zeros that you seem divided by

00:30:16,440 --> 00:30:19,650
the number of buckets that you have

00:30:17,640 --> 00:30:23,160
which is fixed multiplied by the number

00:30:19,650 --> 00:30:24,510
of buckets x magic number and if you

00:30:23,160 --> 00:30:26,040
ever see an implementation of this you

00:30:24,510 --> 00:30:28,950
will see a massive array of magic

00:30:26,040 --> 00:30:30,570
numbers and what these are a statistical

00:30:28,950 --> 00:30:32,610
analysis that they do to show that a

00:30:30,570 --> 00:30:34,320
different sizes of estimates if you

00:30:32,610 --> 00:30:37,010
multiply it by a magic number it nudges

00:30:34,320 --> 00:30:39,210
the estimate in the right direction

00:30:37,010 --> 00:30:41,340
wonderful but i'm not going to show you

00:30:39,210 --> 00:30:42,930
the magic numbers but from all of this

00:30:41,340 --> 00:30:44,370
stuff they show that they can get an

00:30:42,930 --> 00:30:46,680
average error in the cardinality

00:30:44,370 --> 00:30:48,510
estimate of 1.3 divided by the square

00:30:46,680 --> 00:30:51,600
root of the number of buckets and they

00:30:48,510 --> 00:30:54,000
also say that with 1024 buckets you have

00:30:51,600 --> 00:30:56,160
enough counters to do estimates of

00:30:54,000 --> 00:30:59,130
billions of elements which means you get

00:30:56,160 --> 00:31:01,680
a four percent cardinality in accuracy

00:30:59,130 --> 00:31:04,590
but the nice thing is with a 1024

00:31:01,680 --> 00:31:06,450
buckets you'll need five bits per bucket

00:31:04,590 --> 00:31:08,519
in order to do this which means that you

00:31:06,450 --> 00:31:11,999
can do the hypha log log in about 5 km

00:31:08,519 --> 00:31:13,200
which is absolutely nothing but actually

00:31:11,999 --> 00:31:14,789
that's not the whole thing so there's a

00:31:13,200 --> 00:31:16,830
hyper Lord paper which improves it even

00:31:14,789 --> 00:31:19,049
further and what the authors do now and

00:31:16,830 --> 00:31:21,330
it's called near-optimal in this case is

00:31:19,049 --> 00:31:23,129
that when they get the counts of the

00:31:21,330 --> 00:31:24,749
leading zeros they throw away the

00:31:23,129 --> 00:31:26,429
largest thirty percent of recordings

00:31:24,749 --> 00:31:28,619
that they have and they leave the

00:31:26,429 --> 00:31:30,149
remaining seventy percent and just by

00:31:28,619 --> 00:31:32,580
doing that because the sketcher always

00:31:30,149 --> 00:31:34,279
tends to 0 on the large side they can

00:31:32,580 --> 00:31:36,509
get there are down to this now which is

00:31:34,279 --> 00:31:38,820
1.05 divided by the square root of

00:31:36,509 --> 00:31:40,469
buckets so if we plug in a thousand and

00:31:38,820 --> 00:31:42,349
twenty four buckets we get

00:31:40,469 --> 00:31:44,459
three-point-two percent in accuracy and

00:31:42,349 --> 00:31:46,379
then they can't even stop there they go

00:31:44,459 --> 00:31:48,059
one step further and they say instead of

00:31:46,379 --> 00:31:50,039
doing a regular average in the power of

00:31:48,059 --> 00:31:51,029
two they do a harmonic mean instead

00:31:50,039 --> 00:31:52,829
which is a slightly different way of

00:31:51,029 --> 00:31:55,320
calculating an average and it goes down

00:31:52,829 --> 00:31:58,079
to three percent so that's an intuition

00:31:55,320 --> 00:31:59,729
of how it works it's quite magic but

00:31:58,079 --> 00:32:00,899
it's also very cool and if you're really

00:31:59,729 --> 00:32:02,729
good at math you'll really enjoy the

00:32:00,899 --> 00:32:04,109
paper if you want to use it in your own

00:32:02,729 --> 00:32:06,119
code you don't have to do any of that

00:32:04,109 --> 00:32:07,979
stuff there is a same library that we

00:32:06,119 --> 00:32:10,019
used before for Kampmann sketch has

00:32:07,979 --> 00:32:12,359
hyper log log in it clearspring analyst

00:32:10,019 --> 00:32:14,429
extreme I've rewritten the example from

00:32:12,359 --> 00:32:16,139
earlier we create a new hyper log log at

00:32:14,429 --> 00:32:18,149
the top we give it the percentage of

00:32:16,139 --> 00:32:20,190
accuracy that we're interested in we add

00:32:18,149 --> 00:32:24,029
some elements in and then we call

00:32:20,190 --> 00:32:25,709
cardinality and there you go heap size

00:32:24,029 --> 00:32:27,659
brilliant so if we're doing that

00:32:25,709 --> 00:32:29,309
cardinality estimate with a set hundreds

00:32:27,659 --> 00:32:31,739
of Meg's of heap if we're using hyper

00:32:29,309 --> 00:32:33,089
log log nothing so if you can imagine if

00:32:31,739 --> 00:32:34,859
you have lots of these in a storm's

00:32:33,089 --> 00:32:37,979
apology you're not going to be putting

00:32:34,859 --> 00:32:41,339
any pressure on your workers at all use

00:32:37,979 --> 00:32:43,259
cases for that constant time cardinality

00:32:41,339 --> 00:32:46,739
estimate for example unique site

00:32:43,259 --> 00:32:47,669
visitors estimates of massive tables

00:32:46,739 --> 00:32:50,279
because you could either do two things

00:32:47,669 --> 00:32:51,719
you can either have a running hyper log

00:32:50,279 --> 00:32:53,879
log counter that you keep all the time

00:32:51,719 --> 00:32:55,859
and you keep adding to it or you could

00:32:53,879 --> 00:32:57,599
just use it when you say do a big batch

00:32:55,859 --> 00:33:01,200
job and you stream out loads of data and

00:32:57,599 --> 00:33:03,389
you just count on the fly and there you

00:33:01,200 --> 00:33:04,950
go that is it so we've gone through

00:33:03,389 --> 00:33:06,959
three probabilistic data structures I

00:33:04,950 --> 00:33:08,729
hope that made sense and we also had a

00:33:06,959 --> 00:33:10,079
look at some code before and after to

00:33:08,729 --> 00:33:11,129
show you that there's libraries out

00:33:10,079 --> 00:33:12,869
there that make it really easy to use

00:33:11,129 --> 00:33:15,089
this stuff I almost choked at the

00:33:12,869 --> 00:33:16,229
beginning which is excellent and thank

00:33:15,089 --> 00:33:18,299
you very much for listening just to

00:33:16,229 --> 00:33:21,389
recap bloom filters are for set

00:33:18,299 --> 00:33:22,190
membership good for caching Kampmann

00:33:21,389 --> 00:33:23,659
sketch

00:33:22,190 --> 00:33:27,019
good for counting how many times you've

00:33:23,659 --> 00:33:28,669
seen something and hyper Lord Lord is an

00:33:27,019 --> 00:33:30,950
interesting story and its really good

00:33:28,669 --> 00:33:36,519
for cardinality estimates and that is it

00:33:30,950 --> 00:33:36,519
thank you very much for listening okay

00:33:38,529 --> 00:33:57,940
thank you thank you James can we unmute

00:33:44,960 --> 00:34:00,110
this mic tell me what 10 okay wake up

00:33:57,940 --> 00:34:02,210
thank you James you started to meet

00:34:00,110 --> 00:34:03,679
ahead of time and finish two minutes

00:34:02,210 --> 00:34:08,359
ahead of time to give two more minutes

00:34:03,679 --> 00:34:12,139
for Q&A any questions in the room this

00:34:08,359 --> 00:34:14,379
is where I really scared regarding the

00:34:12,139 --> 00:34:16,730
bloom filters have you compared to them

00:34:14,379 --> 00:34:20,060
performance and memory consumption these

00:34:16,730 --> 00:34:23,179
are cuckoo filters for example because

00:34:20,060 --> 00:34:25,669
they have they own advantages sorry can

00:34:23,179 --> 00:34:28,040
you say once more coo coo filters yes

00:34:25,669 --> 00:34:29,569
because the main problem of the bloom

00:34:28,040 --> 00:34:33,409
filters you could not remove the data

00:34:29,569 --> 00:34:35,929
from the filters are oh wait that's the

00:34:33,409 --> 00:34:37,760
really tricky thing said the particular

00:34:35,929 --> 00:34:40,550
use case that we used it for it was

00:34:37,760 --> 00:34:42,169
absolutely fine not to have that data in

00:34:40,550 --> 00:34:45,050
memory because we didn't need to iterate

00:34:42,169 --> 00:34:46,609
for it later but obviously if you do

00:34:45,050 --> 00:34:47,899
need to iterate iterate for it later

00:34:46,609 --> 00:34:50,210
then that's the trade-off that you have

00:34:47,899 --> 00:34:52,010
so i don't know maybe for example you

00:34:50,210 --> 00:34:54,560
could have a bloom filter but then store

00:34:52,010 --> 00:34:57,200
the data in a solar elastic search index

00:34:54,560 --> 00:34:58,520
for a look up when you need to but yeah

00:34:57,200 --> 00:35:02,260
it's use case specific i hope that

00:34:58,520 --> 00:35:05,839
answers your question maybe it didn't

00:35:02,260 --> 00:35:08,089
yeah okay basically a zero-sum tasks

00:35:05,839 --> 00:35:11,420
when you need to remove the data from

00:35:08,089 --> 00:35:13,400
there are fish for example yes clear now

00:35:11,420 --> 00:35:15,560
that's a really really important point

00:35:13,400 --> 00:35:17,780
so not only can you not get the data out

00:35:15,560 --> 00:35:20,180
as in if you had a set you could remove

00:35:17,780 --> 00:35:22,130
the data again when you put stuff into a

00:35:20,180 --> 00:35:25,099
bloom filter it's very very difficult to

00:35:22,130 --> 00:35:27,109
remove it if not impossible so there are

00:35:25,099 --> 00:35:28,550
extensions to bloom filters that can do

00:35:27,109 --> 00:35:29,930
these things but having looked at some

00:35:28,550 --> 00:35:33,140
of the papers it all just gets really

00:35:29,930 --> 00:35:34,490
complicated so you do have to think if

00:35:33,140 --> 00:35:35,220
you are keeping a load of bloom filters

00:35:34,490 --> 00:35:37,650
in memory

00:35:35,220 --> 00:35:39,330
particular data sets what happens when

00:35:37,650 --> 00:35:40,710
that data set changes to say if you have

00:35:39,330 --> 00:35:42,359
a cache that invalidates then you have

00:35:40,710 --> 00:35:44,070
to reload the ho bling filter again and

00:35:42,359 --> 00:35:46,290
that's definitely some it's bear in mind

00:35:44,070 --> 00:35:52,980
thank you for bringing up but I haven't

00:35:46,290 --> 00:35:56,730
got any good advice there ok ok next

00:35:52,980 --> 00:35:59,220
sticky question it's not so much a

00:35:56,730 --> 00:36:00,660
question as that I just want it in the

00:35:59,220 --> 00:36:03,080
intervention that might be clear to

00:36:00,660 --> 00:36:05,849
everyone but has always helped me

00:36:03,080 --> 00:36:08,520
bowhunters I think are different from a

00:36:05,849 --> 00:36:10,890
lot of probabilistic algorithms in that

00:36:08,520 --> 00:36:14,310
one of the trick questions the no

00:36:10,890 --> 00:36:16,050
question is always right so you can add

00:36:14,310 --> 00:36:20,730
or use bone filters in a lot of

00:36:16,050 --> 00:36:23,280
situations where where if you get an

00:36:20,730 --> 00:36:25,680
answer yes you will look up anyway but

00:36:23,280 --> 00:36:27,930
if you get an answer no then you save a

00:36:25,680 --> 00:36:31,200
lot of work and you can do it without

00:36:27,930 --> 00:36:32,400
any probabilistic trade-offs yeah that's

00:36:31,200 --> 00:36:35,070
a really really good point and I should

00:36:32,400 --> 00:36:36,630
have said that in the talk so and if you

00:36:35,070 --> 00:36:38,580
were using a balloon filter as part of a

00:36:36,630 --> 00:36:39,630
loading cash or something and then

00:36:38,580 --> 00:36:43,440
that's really good because you have a

00:36:39,630 --> 00:36:46,530
hundred percent guaranteed correct No so

00:36:43,440 --> 00:36:48,180
yes and if that filters out most of your

00:36:46,530 --> 00:36:49,680
requests and then when you do get a yes

00:36:48,180 --> 00:36:51,300
you do something else then that's that's

00:36:49,680 --> 00:36:54,270
a really good use of it thanks for

00:36:51,300 --> 00:36:57,150
bringing up I think I have a question

00:36:54,270 --> 00:36:59,849
related to a hyper lock lock in order in

00:36:57,150 --> 00:37:03,839
the MapReduce environment is there any

00:36:59,849 --> 00:37:05,820
chance to do calculations in each member

00:37:03,839 --> 00:37:08,520
or each reducer and combine them

00:37:05,820 --> 00:37:12,170
afterwards so especially for unique use

00:37:08,520 --> 00:37:16,830
our calculations where don't want to

00:37:12,170 --> 00:37:18,480
salt first by user that's a really good

00:37:16,830 --> 00:37:19,980
question and I don't know the answer I

00:37:18,480 --> 00:37:21,599
know that for example with bloom filters

00:37:19,980 --> 00:37:23,550
you can do operations to kind of combine

00:37:21,599 --> 00:37:24,990
them and but I'm not sure if the same is

00:37:23,550 --> 00:37:27,660
true with hyper log look maybe somebody

00:37:24,990 --> 00:37:29,310
else may know the answer I guess if you

00:37:27,660 --> 00:37:30,420
do lots of separate hyper load logs and

00:37:29,310 --> 00:37:33,450
then combine them it just gets more and

00:37:30,420 --> 00:37:38,849
more and more inaccurate but I don't

00:37:33,450 --> 00:37:43,250
like sorry any more questions or

00:37:38,849 --> 00:37:43,250
comments just please raise your hand

00:37:43,810 --> 00:37:54,730
one I wanted to answer the previous

00:37:52,210 --> 00:37:56,290
question the Clear Springs have an

00:37:54,730 --> 00:37:59,620
article on how to combine hyper law

00:37:56,290 --> 00:38:03,040
clerks with different even if they have

00:37:59,620 --> 00:38:11,560
different accuracy okay great co-op look

00:38:03,040 --> 00:38:18,490
at I thank you anyone else going once at

00:38:11,560 --> 00:38:20,920
the front twice so you can increase

00:38:18,490 --> 00:38:23,650
accuracy of the idle resins by adding

00:38:20,920 --> 00:38:26,530
more hash functions of the does this

00:38:23,650 --> 00:38:29,500
have a notable mmm an impact on the run

00:38:26,530 --> 00:38:30,850
time or does it not matter so the hash

00:38:29,500 --> 00:38:32,320
functions don't need to be really

00:38:30,850 --> 00:38:34,240
complicated they don't need to be like

00:38:32,320 --> 00:38:36,250
cryptographic level hashes or anything

00:38:34,240 --> 00:38:37,480
they use a technique which i think is

00:38:36,250 --> 00:38:38,950
called pairwise hashing where you

00:38:37,480 --> 00:38:40,930
actually just have one hash function but

00:38:38,950 --> 00:38:42,460
is parametrized so for example in the

00:38:40,930 --> 00:38:44,080
count win sketch some of the inputs to

00:38:42,460 --> 00:38:46,090
the hash function would be the row that

00:38:44,080 --> 00:38:48,250
you're on and then it can do some like

00:38:46,090 --> 00:38:49,390
modulo arithmetic so as far as I know as

00:38:48,250 --> 00:38:51,670
long as they're all constant time it

00:38:49,390 --> 00:39:00,550
should be okay and but I can't give you

00:38:51,670 --> 00:39:04,810
any good proof ok thanks ok any other

00:39:00,550 --> 00:39:08,550
hand I don't see I think that's it ok

00:39:04,810 --> 00:39:08,550

YouTube URL: https://www.youtube.com/watch?v=NLXjsMS7uBM


