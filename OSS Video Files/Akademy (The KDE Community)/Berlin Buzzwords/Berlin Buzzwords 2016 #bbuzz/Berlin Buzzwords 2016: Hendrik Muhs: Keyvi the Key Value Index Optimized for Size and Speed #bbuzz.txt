Title: Berlin Buzzwords 2016: Hendrik Muhs: Keyvi the Key Value Index Optimized for Size and Speed #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Yet another key-value store? No, it's an index not a store and it is based on 'Finite State technology', but what does that mean?

Keyvi - the short form for "Key value index" - defines a special subtype of the popular key value store (KVS) technologies. As you can imagine from the name, keyvi is an immutable key value store, therefore an index not a store. Keyvi's strengths: high compression ratio and extreme scalability.

Keyvi powers Cliqz Websearch engine, replacing former engines based on Redis and Elastic Search. Serving terrabytes of data at scale and low-latency, keyvi is already proven technology while still being a young OSS project.

But keyvi is also different to well-established NoSQL engines, it is not an efficient implementation of well-known, common used data structures like hash tables and B-Trees. It brings finite state not only to the same level, but efficiently allows approximate, completion and graph matching to boldy go where NoSQL hasn't gone before.

Read more:
https://2016.berlinbuzzwords.de/session/keyvi-key-value-index-optimized-size-and-speed

About Hendrik Muhs:
https://2016.berlinbuzzwords.de/users/hendrik-muhs

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:03,410 --> 00:00:18,210
so hi I'm the new one my name is TV I'm

00:00:10,710 --> 00:00:20,609
a key value index I'm based on finite

00:00:18,210 --> 00:00:22,769
state if you don't know what finite

00:00:20,609 --> 00:00:25,080
state means doesn't matter i think i

00:00:22,769 --> 00:00:29,099
will show it during the doing the talk

00:00:25,080 --> 00:00:36,230
i'm an open source project not even a

00:00:29,099 --> 00:00:39,690
year old so first who's using TV and i'm

00:00:36,230 --> 00:00:42,300
working for clicks and probably also you

00:00:39,690 --> 00:00:47,010
probably don't really know us we are

00:00:42,300 --> 00:00:48,930
building building a browser with anti

00:00:47,010 --> 00:00:54,719
tracking technology as well as

00:00:48,930 --> 00:00:58,079
integrated search so auntie V is part of

00:00:54,719 --> 00:01:02,250
the part of the search engine and yeah I

00:00:58,079 --> 00:01:05,339
mean please install us okay search

00:01:02,250 --> 00:01:08,369
engine wise let's look into let's talk

00:01:05,339 --> 00:01:12,450
look into into the numbers we have like

00:01:08,369 --> 00:01:14,790
1.8 building new else indexed I roughly

00:01:12,450 --> 00:01:17,700
count it took a long time 13 billion

00:01:14,790 --> 00:01:21,119
data points key value pairs our index

00:01:17,700 --> 00:01:23,729
sizes around two and half terabytes that

00:01:21,119 --> 00:01:26,729
number has to be a little bit put into

00:01:23,729 --> 00:01:29,670
context and you will later see why we

00:01:26,729 --> 00:01:32,790
have like 60 milliseconds average

00:01:29,670 --> 00:01:36,150
latency and as you might have seen in

00:01:32,790 --> 00:01:39,810
the browser what we do is we do search

00:01:36,150 --> 00:01:41,250
as you type so you are not putting in

00:01:39,810 --> 00:01:43,049
the query press and I get to search

00:01:41,250 --> 00:01:47,670
roulette but we searched while you are

00:01:43,049 --> 00:01:50,729
typing right now we get something like

00:01:47,670 --> 00:01:54,390
six thousand requests per minute we are

00:01:50,729 --> 00:01:55,770
able to do to do more so yeah and

00:01:54,390 --> 00:02:01,049
everything works actually quite fine

00:01:55,770 --> 00:02:02,969
with 99 percent availability so why did

00:02:01,049 --> 00:02:05,909
we build it and why didn't we show

00:02:02,969 --> 00:02:07,740
something else so that means that we

00:02:05,909 --> 00:02:10,530
have two little bit look into into the

00:02:07,740 --> 00:02:13,110
history and we started actually quite

00:02:10,530 --> 00:02:13,950
the normal approach if you're building a

00:02:13,110 --> 00:02:18,269
search engine okay

00:02:13,950 --> 00:02:20,700
at stake elasticsearch actually this

00:02:18,269 --> 00:02:25,230
didn't really work quite well far for

00:02:20,700 --> 00:02:28,920
our use case this is more to the fact

00:02:25,230 --> 00:02:32,970
that we are not doing a classical

00:02:28,920 --> 00:02:36,150
full-text search but rather that we do

00:02:32,970 --> 00:02:39,200
do it a little different and so the

00:02:36,150 --> 00:02:43,860
reason for like abandon elasticsearch

00:02:39,200 --> 00:02:46,739
was because it didn't fit our model so

00:02:43,860 --> 00:02:49,319
the next actually our model fit more to

00:02:46,739 --> 00:02:51,599
something like a key value store and we

00:02:49,319 --> 00:02:54,750
switch to read us and we we're super

00:02:51,599 --> 00:02:57,510
maze about it it's great it's simple its

00:02:54,750 --> 00:02:59,790
space efficient and what we also love

00:02:57,510 --> 00:03:02,579
this is if you know ready see

00:02:59,790 --> 00:03:07,200
server-side scripting stuff so you you

00:03:02,579 --> 00:03:10,860
can run lure scripts on server side so

00:03:07,200 --> 00:03:12,989
you're already reducing your search

00:03:10,860 --> 00:03:15,180
result in the on your node in the

00:03:12,989 --> 00:03:18,090
cluster before sending it back saves a

00:03:15,180 --> 00:03:22,760
lot of time we know that networking is

00:03:18,090 --> 00:03:27,959
is one of the biggest bottlenecks still

00:03:22,760 --> 00:03:29,940
Redis was also not not best with more

00:03:27,959 --> 00:03:32,130
users we run actually into capacity

00:03:29,940 --> 00:03:35,760
problems the first capacity problem we

00:03:32,130 --> 00:03:38,310
had was space so the size and when I

00:03:35,760 --> 00:03:40,319
showed you the 2.5 tablets in the in one

00:03:38,310 --> 00:03:43,560
of the first slides actually when we

00:03:40,319 --> 00:03:46,019
when we decommissioned Redis our index

00:03:43,560 --> 00:03:50,880
size was five terabytes and with Kiwi

00:03:46,019 --> 00:03:52,950
reduced to two terabytes if you a little

00:03:50,880 --> 00:03:56,010
bit now like we run on AWS if you were a

00:03:52,950 --> 00:03:57,569
little bit now the prices there you can

00:03:56,010 --> 00:04:02,609
calculate yourself how much money that

00:03:57,569 --> 00:04:04,380
that means still other so one capacity

00:04:02,609 --> 00:04:08,220
problem was actually to get enough

00:04:04,380 --> 00:04:10,139
machine machines the other thing is if

00:04:08,220 --> 00:04:13,380
you are getting a large machine because

00:04:10,139 --> 00:04:15,419
everything I mean this is my might not

00:04:13,380 --> 00:04:17,489
have been clear yet if we do this sixty

00:04:15,419 --> 00:04:19,669
MLS second search we actually have to

00:04:17,489 --> 00:04:21,989
serve everything from memory all right

00:04:19,669 --> 00:04:23,940
so if you're getting a large machine and

00:04:21,989 --> 00:04:27,180
Amazon you're also getting a lot a lot

00:04:23,940 --> 00:04:27,810
of CPUs doesn't really fit with Redis

00:04:27,180 --> 00:04:30,180
because

00:04:27,810 --> 00:04:32,220
it's a single-threaded and I think like

00:04:30,180 --> 00:04:37,650
ninety percent of of reddit users don't

00:04:32,220 --> 00:04:39,540
hit this problem but we hit it hard the

00:04:37,650 --> 00:04:42,350
reasons why it signified is also because

00:04:39,540 --> 00:04:45,900
everything is on the heap of the process

00:04:42,350 --> 00:04:49,430
which makes it also quite risky because

00:04:45,900 --> 00:04:51,930
if this process dies you lost it and

00:04:49,430 --> 00:04:55,830
actually takes quite some time to reload

00:04:51,930 --> 00:04:58,470
this stuff so that that's why Kiwi came

00:04:55,830 --> 00:05:03,540
in and first of all it was a massive

00:04:58,470 --> 00:05:08,669
size reduction second it's it's

00:05:03,540 --> 00:05:10,680
multi-threaded so we can do it with many

00:05:08,669 --> 00:05:12,479
processes and the reason for this multi

00:05:10,680 --> 00:05:16,500
threading is because using shared memory

00:05:12,479 --> 00:05:18,060
so memory mapping basically and last but

00:05:16,500 --> 00:05:20,040
not least due to this memory mapping

00:05:18,060 --> 00:05:22,590
it's also more reliable because well

00:05:20,040 --> 00:05:24,810
when it process dies it dies you have

00:05:22,590 --> 00:05:26,940
other processes no problem if you reload

00:05:24,810 --> 00:05:31,020
things it's cached in the u.s. it's

00:05:26,940 --> 00:05:34,919
quickly there so and to give you a

00:05:31,020 --> 00:05:37,110
little bit more reasoning here is like a

00:05:34,919 --> 00:05:42,090
very simple size comparison between

00:05:37,110 --> 00:05:44,520
Radisson TV yeah I mean the first thing

00:05:42,090 --> 00:05:49,080
you see on the slide okay give you a

00:05:44,520 --> 00:05:51,510
smaller great if you look a little bit

00:05:49,080 --> 00:05:54,360
deeper into the numbers you actually see

00:05:51,510 --> 00:05:58,500
it's not linear all right it's

00:05:54,360 --> 00:06:00,810
interesting it's like 1 million pairs 10

00:05:58,500 --> 00:06:03,780
million pairs is 10 times more than 1

00:06:00,810 --> 00:06:05,370
million 100 million is 10 times more

00:06:03,780 --> 00:06:07,830
than 10 million you see that for readies

00:06:05,370 --> 00:06:12,419
for TV it's kind of interesting this is

00:06:07,830 --> 00:06:14,430
not linear it's sup linear and when I

00:06:12,419 --> 00:06:17,400
explain this finite state thing you

00:06:14,430 --> 00:06:20,900
probably will get why still this this

00:06:17,400 --> 00:06:24,030
wasn't this is not the the sixty percent

00:06:20,900 --> 00:06:28,110
space savings that I claimed in the

00:06:24,030 --> 00:06:31,919
beginning so actually space savings is

00:06:28,110 --> 00:06:34,229
based on your data and I print here a

00:06:31,919 --> 00:06:37,530
little bit the compression ratios that

00:06:34,229 --> 00:06:39,330
you can can get with TV I mean if you

00:06:37,530 --> 00:06:40,980
know Redis registers a hash table it

00:06:39,330 --> 00:06:45,960
doesn't matter what you put in

00:06:40,980 --> 00:06:47,910
from like a space perspective for Q it

00:06:45,960 --> 00:06:51,350
actually makes a big difference so

00:06:47,910 --> 00:06:53,640
something like a term dictionary okay

00:06:51,350 --> 00:06:55,530
then we have like content which was the

00:06:53,640 --> 00:06:57,780
previous kind of benchmark content is

00:06:55,530 --> 00:07:02,700
something like okay you have the the

00:06:57,780 --> 00:07:05,580
text of of the page and so on if you

00:07:02,700 --> 00:07:07,890
just have filtering that is like like a

00:07:05,580 --> 00:07:09,690
list of domains which you don't want to

00:07:07,890 --> 00:07:13,650
but you want to blackness because they

00:07:09,690 --> 00:07:15,240
are adult content or there you know that

00:07:13,650 --> 00:07:19,170
they lead to four force or something

00:07:15,240 --> 00:07:20,910
like that and the most compression you

00:07:19,170 --> 00:07:23,040
get with scores the scores I mean

00:07:20,910 --> 00:07:25,940
something like statistical models so

00:07:23,040 --> 00:07:28,350
basically numbers something that you

00:07:25,940 --> 00:07:34,430
have in your in your model something

00:07:28,350 --> 00:07:36,900
some ranking features basically and okay

00:07:34,430 --> 00:07:44,460
we saw the space now let's look at the

00:07:36,900 --> 00:07:46,860
at a look at benchmarks I try to make it

00:07:44,460 --> 00:07:48,600
visible I mean it's always hot actually

00:07:46,860 --> 00:07:51,450
it's always hard to do benchmarks and

00:07:48,600 --> 00:07:52,950
you'd always do it wrong and the other

00:07:51,450 --> 00:07:56,730
thing is with doing this kind of

00:07:52,950 --> 00:07:58,740
benchmarks is do you measure the store

00:07:56,730 --> 00:08:00,510
or do you measure your network actually

00:07:58,740 --> 00:08:03,780
most of the time you virtual network and

00:08:00,510 --> 00:08:09,260
not the not the store still you can see

00:08:03,780 --> 00:08:11,970
some stuff here we see that if i try i

00:08:09,260 --> 00:08:15,330
say try try to make it kind of a fair

00:08:11,970 --> 00:08:17,580
comparison i put a kiwi into a single

00:08:15,330 --> 00:08:19,140
threaded mode as Redis and then just

00:08:17,580 --> 00:08:21,840
compare the one that that's just the

00:08:19,140 --> 00:08:25,770
blue and the and the yellow one we see

00:08:21,840 --> 00:08:27,720
it's kind of on pair it is a little bit

00:08:25,770 --> 00:08:29,850
worse maybe you have to tell a little

00:08:27,720 --> 00:08:32,370
bit on what a chunk size means if you

00:08:29,850 --> 00:08:34,830
know Redis you have something like a

00:08:32,370 --> 00:08:37,530
call the reddest pipeline so actually

00:08:34,830 --> 00:08:40,470
you are not doing one look up at the

00:08:37,530 --> 00:08:42,240
time that would be bad because you you

00:08:40,470 --> 00:08:46,170
would just spend time for your for your

00:08:42,240 --> 00:08:50,300
networking you wouldn't you would like

00:08:46,170 --> 00:08:54,060
lose all your audio performance in RPC

00:08:50,300 --> 00:08:55,020
so what you do is you collect you

00:08:54,060 --> 00:08:57,180
collect what you want to

00:08:55,020 --> 00:09:04,200
no send it over the network and then get

00:08:57,180 --> 00:09:06,570
10 20 or 10 50 100 200 entries in one

00:09:04,200 --> 00:09:10,440
step and that's what these chunk size

00:09:06,570 --> 00:09:14,580
means so there's even a little problem

00:09:10,440 --> 00:09:17,460
with Kiwi it seems on this 10 chunk size

00:09:14,580 --> 00:09:21,510
10 but if we are now putting it into a

00:09:17,460 --> 00:09:24,180
multi threaded mode it clearly

00:09:21,510 --> 00:09:25,830
outperforms lettuce and also the the

00:09:24,180 --> 00:09:28,560
other thing is what if we are not doing

00:09:25,830 --> 00:09:30,270
IPC which we don't have to because of

00:09:28,560 --> 00:09:32,180
Chad memory I will explain a little bit

00:09:30,270 --> 00:09:35,730
more into a gone bit into detail later

00:09:32,180 --> 00:09:38,090
for this no I pc case but this is just

00:09:35,730 --> 00:09:39,960
to give you a little bit on an idea what

00:09:38,090 --> 00:09:42,060
because when you do this kind of

00:09:39,960 --> 00:09:43,860
benchmarks you have to see what is the

00:09:42,060 --> 00:09:49,680
cost of the of the extra look up and

00:09:43,860 --> 00:09:52,530
what is aside from that I pc RPC stuff

00:09:49,680 --> 00:09:55,350
like that this benchmark was also only

00:09:52,530 --> 00:09:58,380
on a local machine because what I'm not

00:09:55,350 --> 00:10:00,510
interesting here is is on the network of

00:09:58,380 --> 00:10:03,660
course on a network it looks even even

00:10:00,510 --> 00:10:05,820
completely different so let's a little

00:10:03,660 --> 00:10:10,380
bit look into how how this thing works

00:10:05,820 --> 00:10:13,650
and this is a quite super complicated

00:10:10,380 --> 00:10:15,510
picture but this is the way it is right

00:10:13,650 --> 00:10:17,280
you have a you have a couple of workers

00:10:15,510 --> 00:10:19,020
on one side and you have a couple a

00:10:17,280 --> 00:10:23,130
cluster full of machines on the other

00:10:19,020 --> 00:10:25,040
side what I haven't told you actually we

00:10:23,130 --> 00:10:27,570
would not put one Redis on one machine

00:10:25,040 --> 00:10:29,130
but we would use several registers I

00:10:27,570 --> 00:10:31,890
mean that's how you scale out Redis you

00:10:29,130 --> 00:10:33,390
you just start several ones and then you

00:10:31,890 --> 00:10:35,790
have instead of single threaded you have

00:10:33,390 --> 00:10:38,820
then I don't off five frets because

00:10:35,790 --> 00:10:40,830
you've started five ready service so

00:10:38,820 --> 00:10:43,860
this is a bit complicated let's let's

00:10:40,830 --> 00:10:48,480
assume a little bit into this just look

00:10:43,860 --> 00:10:52,290
at one machine so you have a couple of

00:10:48,480 --> 00:10:55,460
ready service okay still the data is

00:10:52,290 --> 00:10:58,740
everyone on the heap of this process

00:10:55,460 --> 00:11:00,540
data access is local so it means I

00:10:58,740 --> 00:11:04,530
cannot from one ready server I cannot

00:11:00,540 --> 00:11:06,330
access to the other this is for some

00:11:04,530 --> 00:11:07,590
applications actually in our search case

00:11:06,330 --> 00:11:10,830
this

00:11:07,590 --> 00:11:14,190
is a problem because sometimes you want

00:11:10,830 --> 00:11:20,370
to have access or some some ranking data

00:11:14,190 --> 00:11:21,960
you always need right and you can't do

00:11:20,370 --> 00:11:23,670
that so what you have to do you have to

00:11:21,960 --> 00:11:26,340
do deep copies you have to put it on

00:11:23,670 --> 00:11:29,790
Eddie everywhere the server then your

00:11:26,340 --> 00:11:32,340
size will go up again if one of these

00:11:29,790 --> 00:11:34,920
reddit servers dies it has to be

00:11:32,340 --> 00:11:37,320
reloaded and our in our use case we had

00:11:34,920 --> 00:11:39,000
times like 40 minutes and that's pretty

00:11:37,320 --> 00:11:43,980
bad I mean you don't want to have 40

00:11:39,000 --> 00:11:46,410
minutes down time so how would it look

00:11:43,980 --> 00:11:49,110
in key be well as I said before it's

00:11:46,410 --> 00:11:51,000
shared memory so instead of dividing it

00:11:49,110 --> 00:11:52,440
into five pieces if I would have five

00:11:51,000 --> 00:11:54,740
ready service before I would now have

00:11:52,440 --> 00:11:57,450
just one piece on one machine with a

00:11:54,740 --> 00:12:02,490
with shared memory I can start as many

00:11:57,450 --> 00:12:05,700
processes as I want if it crashes no

00:12:02,490 --> 00:12:08,700
problem you have enough workers you just

00:12:05,700 --> 00:12:11,310
are new one I mean we use we actually

00:12:08,700 --> 00:12:13,920
use multiple processes not multiple

00:12:11,310 --> 00:12:18,540
threads so that's a little bit more

00:12:13,920 --> 00:12:19,830
robust and the other benefit of this

00:12:18,540 --> 00:12:22,050
whole thing which I haven't touched upon

00:12:19,830 --> 00:12:24,420
yet is it readies load something you

00:12:22,050 --> 00:12:28,380
have to deserialize it so it takes these

00:12:24,420 --> 00:12:31,020
40 minutes I told you on TV this is just

00:12:28,380 --> 00:12:33,180
put into memory and that's it so there's

00:12:31,020 --> 00:12:35,220
no diff civilization time so instead of

00:12:33,180 --> 00:12:39,150
these 40 minutes down time you have 0

00:12:35,220 --> 00:12:41,790
minutes down time because of this shared

00:12:39,150 --> 00:12:44,520
memory actually could argue like why do

00:12:41,790 --> 00:12:46,920
I need a server she texture at all

00:12:44,520 --> 00:12:49,590
actually on the local machine i mean i

00:12:46,920 --> 00:12:52,980
can just load it right you can do that

00:12:49,590 --> 00:12:56,880
and because we traditionally started

00:12:52,980 --> 00:12:58,620
from replacing radio Swift TV we

00:12:56,880 --> 00:13:00,360
actually haven't really fought about

00:12:58,620 --> 00:13:03,480
this use case but we later realize that

00:13:00,360 --> 00:13:06,870
actually read is completely an in-memory

00:13:03,480 --> 00:13:08,790
store but keavy actually isn't because

00:13:06,870 --> 00:13:10,710
it's shared memory it's you put it into

00:13:08,790 --> 00:13:14,220
virtual memory of the operating system

00:13:10,710 --> 00:13:17,570
so you can have even a bigger index than

00:13:14,220 --> 00:13:20,220
the size of your RAM on the machine so

00:13:17,570 --> 00:13:21,329
this is an example we have like an index

00:13:20,220 --> 00:13:23,220
size of two tablets

00:13:21,329 --> 00:13:27,540
putting on one machine which is like 250

00:13:23,220 --> 00:13:30,689
gigs of ram so it's almost like 10 you

00:13:27,540 --> 00:13:34,709
have to like 10 times more index then on

00:13:30,689 --> 00:13:37,889
the then you can store on on RAM and

00:13:34,709 --> 00:13:41,699
what the OS is doing is like paging the

00:13:37,889 --> 00:13:44,100
whole thing right this is compared to to

00:13:41,699 --> 00:13:49,040
a setup we have right now it's like okay

00:13:44,100 --> 00:13:54,660
10 the alternative is having 10 machines

00:13:49,040 --> 00:13:56,910
the Charlotte data and so and the

00:13:54,660 --> 00:14:01,829
alternative is the putting everything on

00:13:56,910 --> 00:14:05,040
on SST so basically you have on and the

00:14:01,829 --> 00:14:07,799
two upper scenarios you have the network

00:14:05,040 --> 00:14:11,910
I owe on the two lower scenarios you

00:14:07,799 --> 00:14:13,649
have the disk i/o and we are comparing

00:14:11,910 --> 00:14:16,249
here is a little bit like this guy over

00:14:13,649 --> 00:14:19,439
a network I oh and this whole SSD case

00:14:16,249 --> 00:14:21,209
really on rotation a list forget it but

00:14:19,439 --> 00:14:23,399
with SSDs is actually quite an

00:14:21,209 --> 00:14:29,759
interesting case because it completely

00:14:23,399 --> 00:14:31,769
changes the picture so on a you see like

00:14:29,759 --> 00:14:34,259
okay on a code when it quite takes

00:14:31,769 --> 00:14:36,509
sometimes the cold run means you have

00:14:34,259 --> 00:14:40,139
nothing in your realm you just freshly

00:14:36,509 --> 00:14:43,040
load it but once something is loaded and

00:14:40,139 --> 00:14:45,689
actually the in the test we are loading

00:14:43,040 --> 00:14:50,970
it's also doing some I oh it's not

00:14:45,689 --> 00:14:55,230
completely without paging we see that it

00:14:50,970 --> 00:14:56,910
actually works quite well on SST this is

00:14:55,230 --> 00:15:02,040
something we are working on it's not

00:14:56,910 --> 00:15:06,029
quite clear ok so I think now you should

00:15:02,040 --> 00:15:11,759
know the why and now I want to tell you

00:15:06,029 --> 00:15:14,579
a little bit about the how and now i

00:15:11,759 --> 00:15:16,709
also like look and look into what what

00:15:14,579 --> 00:15:20,819
again was is this what finite-state

00:15:16,709 --> 00:15:23,399
something so this is a actually it's not

00:15:20,819 --> 00:15:25,949
a fine settings users if you correctly

00:15:23,399 --> 00:15:28,619
it's finite state automaton nila i just

00:15:25,949 --> 00:15:32,130
put like four words into into it for you

00:15:28,619 --> 00:15:34,620
berlin buzzwords past keywords and

00:15:32,130 --> 00:15:36,840
some of you know that for some of you it

00:15:34,620 --> 00:15:38,250
just looks a bit similar to something

00:15:36,840 --> 00:15:41,360
that you know and probably you know this

00:15:38,250 --> 00:15:43,800
one which is a try a try data structure

00:15:41,360 --> 00:15:46,020
so what's the difference is it's just

00:15:43,800 --> 00:15:48,690
that okay this one has a little bit more

00:15:46,020 --> 00:15:50,520
a little less circles right let's just

00:15:48,690 --> 00:15:54,660
like kind of obvious as little less

00:15:50,520 --> 00:15:58,520
circles what we do in a try we we

00:15:54,660 --> 00:16:00,630
compress prefixes in the in the

00:15:58,520 --> 00:16:02,600
statements user we also compress

00:16:00,630 --> 00:16:07,640
suffixes you see that keywords and

00:16:02,600 --> 00:16:10,620
buzzwords actually share the same suffix

00:16:07,640 --> 00:16:14,850
it's a little bit hard to always explain

00:16:10,620 --> 00:16:16,380
this on such a small example but as I

00:16:14,850 --> 00:16:21,390
showed you before with the compression

00:16:16,380 --> 00:16:23,970
ratios it quite met us and it's also it

00:16:21,390 --> 00:16:27,870
it's different on what kind of data you

00:16:23,970 --> 00:16:30,030
put pudding okay how do we built this

00:16:27,870 --> 00:16:33,540
thing we have big data right we have

00:16:30,030 --> 00:16:34,860
like terabytes of data so obviously this

00:16:33,540 --> 00:16:38,480
whole thing doesn't fit into memory I

00:16:34,860 --> 00:16:41,610
mean that should be immediately clear

00:16:38,480 --> 00:16:44,220
what does it mean for us well if we want

00:16:41,610 --> 00:16:46,230
to construct such a state we have to

00:16:44,220 --> 00:16:50,160
actually know all the outgoing

00:16:46,230 --> 00:16:52,980
transitions simple solution we just sort

00:16:50,160 --> 00:16:55,860
upfront sorting we all learned it in

00:16:52,980 --> 00:17:01,050
your University it's some external sort

00:16:55,860 --> 00:17:02,580
it's a solved problem what we in

00:17:01,050 --> 00:17:05,280
addition have to do is we have to find

00:17:02,580 --> 00:17:11,670
this this ways to optimize this fing the

00:17:05,280 --> 00:17:14,339
so-called minimization and what you can

00:17:11,670 --> 00:17:18,120
do here is now just user use a hash

00:17:14,339 --> 00:17:19,880
table this has a you're getting back to

00:17:18,120 --> 00:17:22,880
this problem of everything in memory so

00:17:19,880 --> 00:17:26,520
your ash table should be quite efficient

00:17:22,880 --> 00:17:28,710
the other thing is and I'm not making

00:17:26,520 --> 00:17:30,390
academic career with that statement is

00:17:28,710 --> 00:17:32,460
we make the hash table bounded that

00:17:30,390 --> 00:17:35,270
means that the finite state machine will

00:17:32,460 --> 00:17:39,570
not be minimal this is horrible for

00:17:35,270 --> 00:17:44,370
academics but in practice this is like

00:17:39,570 --> 00:17:45,660
saves you and what we what QB also

00:17:44,370 --> 00:17:47,880
applies is

00:17:45,660 --> 00:17:50,400
Liu algorithm so at least recently used

00:17:47,880 --> 00:17:53,010
we just pushed the states in the hash

00:17:50,400 --> 00:17:54,540
table up which have been minimized and

00:17:53,010 --> 00:17:57,150
we throw away the ones which don't

00:17:54,540 --> 00:18:01,260
minimize and in the end view somehow F

00:17:57,150 --> 00:18:02,790
to stream input and output so i'm not

00:18:01,260 --> 00:18:05,730
sure if there are some leucine folks

00:18:02,790 --> 00:18:08,190
here but you know this kind of thing

00:18:05,730 --> 00:18:11,580
right this leucine has this right they

00:18:08,190 --> 00:18:15,480
have a finite state since uses deep in

00:18:11,580 --> 00:18:19,170
lucene somewhere i makin a little bit

00:18:15,480 --> 00:18:24,150
compare TV and andalusia implementation

00:18:19,170 --> 00:18:25,650
and okay we have to Samos solve the deep

00:18:24,150 --> 00:18:27,830
resistance problem so what we need is

00:18:25,650 --> 00:18:33,470
some kind of clever bit logic and the

00:18:27,830 --> 00:18:33,470
leucine is also doing it quite clever

00:18:33,800 --> 00:18:38,100
basically what we've seen does it puts

00:18:36,180 --> 00:18:41,060
it persists the node as small as

00:18:38,100 --> 00:18:44,790
possible as some kind of of bit vector

00:18:41,060 --> 00:18:47,130
the difference to key B is it uses the

00:18:44,790 --> 00:18:48,960
so-called sparse array packing and I

00:18:47,130 --> 00:18:52,560
will explain you what what this parcel a

00:18:48,960 --> 00:18:55,080
pecking means so you have like a say

00:18:52,560 --> 00:18:59,850
this is like your state and we put that

00:18:55,080 --> 00:19:01,920
into some some representation into a

00:18:59,850 --> 00:19:04,350
sparse array of presentations part of a

00:19:01,920 --> 00:19:11,030
just means okay you have a lot of holes

00:19:04,350 --> 00:19:15,360
in your in your array so a lot of zeros

00:19:11,030 --> 00:19:17,790
to make it efficient we use for the

00:19:15,360 --> 00:19:21,120
labels we use one bite now you can say

00:19:17,790 --> 00:19:24,750
okay single white encoding we are in the

00:19:21,120 --> 00:19:29,220
year suit 2016 it's quite easy just use

00:19:24,750 --> 00:19:32,760
utf-8 like a show here that just means

00:19:29,220 --> 00:19:34,800
okay i have three states say if i have a

00:19:32,760 --> 00:19:37,170
utf-8 character with which is three

00:19:34,800 --> 00:19:41,160
bites I just have free states here which

00:19:37,170 --> 00:19:43,880
I somehow wrapped into one state so it

00:19:41,160 --> 00:19:47,570
looks like the same from the outside

00:19:43,880 --> 00:19:51,180
then we need something like okay every

00:19:47,570 --> 00:19:54,180
structure needs some kind of pointers

00:19:51,180 --> 00:19:58,590
right so we have also a vector of

00:19:54,180 --> 00:20:00,300
pointers here to connect the different

00:19:58,590 --> 00:20:04,120
states

00:20:00,300 --> 00:20:05,710
so still be like okay we can put this

00:20:04,120 --> 00:20:08,310
one after another but that wouldn't work

00:20:05,710 --> 00:20:11,530
right that would not save the space so

00:20:08,310 --> 00:20:14,380
how do we combine these and why do we

00:20:11,530 --> 00:20:17,320
need the labels at all so maybe I hope

00:20:14,380 --> 00:20:20,230
this gets cleared out so given of these

00:20:17,320 --> 00:20:24,490
two of these paths race we try to

00:20:20,230 --> 00:20:26,080
combine them and if we would like in

00:20:24,490 --> 00:20:30,430
this case we want to try to interleave

00:20:26,080 --> 00:20:32,920
them that would actually mean this

00:20:30,430 --> 00:20:35,560
doesn't work it's somehow clashes all

00:20:32,920 --> 00:20:38,910
day okay next try ah clashes on the

00:20:35,560 --> 00:20:46,270
beginning next try okay this looks now

00:20:38,910 --> 00:20:49,840
almost again yeah I think we made it so

00:20:46,270 --> 00:20:52,960
we finally found a way to put these two

00:20:49,840 --> 00:20:56,830
things together and it looks in the end

00:20:52,960 --> 00:21:02,670
it looks like that so we combined both

00:20:56,830 --> 00:21:02,670
of these sparse representations into one

00:21:02,790 --> 00:21:09,220
I'm not telling you the whole story

00:21:05,310 --> 00:21:12,340
that's obviously a little bit more a lot

00:21:09,220 --> 00:21:15,340
of lot of tiny tricks into to make this

00:21:12,340 --> 00:21:19,240
super fast and quick we use something

00:21:15,340 --> 00:21:21,130
like intrinsic there's I told you this

00:21:19,240 --> 00:21:23,820
this to bite thing there's obviously

00:21:21,130 --> 00:21:27,520
some kind of variable length encoding

00:21:23,820 --> 00:21:29,830
behind the scenes we are not using

00:21:27,520 --> 00:21:31,870
absolute offsets but relatives offsets

00:21:29,830 --> 00:21:37,360
are saying something like okay gay go

00:21:31,870 --> 00:21:43,240
five steps left or five steps right we

00:21:37,360 --> 00:21:45,370
have to get the when we built the

00:21:43,240 --> 00:21:47,230
structure we have to keep the memory

00:21:45,370 --> 00:21:51,760
requirement low so we use something like

00:21:47,230 --> 00:21:54,670
sliding windows and etc etc etc okay

00:21:51,760 --> 00:21:57,910
let's now let's look at two into the

00:21:54,670 --> 00:21:59,950
lookup itself and you might already

00:21:57,910 --> 00:22:03,820
guess it it's quite easy you just start

00:21:59,950 --> 00:22:05,850
from an offset you go a little bit I you

00:22:03,820 --> 00:22:08,740
want to look up something like the e

00:22:05,850 --> 00:22:11,410
there you find a pointer to the next one

00:22:08,740 --> 00:22:13,900
and then you just repeat the whole thing

00:22:11,410 --> 00:22:15,370
and out something comes from this pretty

00:22:13,900 --> 00:22:16,900
interesting in this data structure

00:22:15,370 --> 00:22:18,820
actually it doesn't matter at all how

00:22:16,900 --> 00:22:23,230
many data is in there because it only

00:22:18,820 --> 00:22:25,480
made us the length of your key right as

00:22:23,230 --> 00:22:28,390
long as you can build this thing the

00:22:25,480 --> 00:22:35,350
lookup complexity of this thing is n

00:22:28,390 --> 00:22:40,240
which is the length of your key ok so if

00:22:35,350 --> 00:22:42,970
I just showed you how to look up and I

00:22:40,240 --> 00:22:46,480
showed you a little bit a state

00:22:42,970 --> 00:22:48,340
automaton what we also key value means

00:22:46,480 --> 00:22:50,230
that we also want to have values and I

00:22:48,340 --> 00:22:53,320
can just briefly mention that in how it

00:22:50,230 --> 00:22:55,030
works in key vs is because that's not so

00:22:53,320 --> 00:22:56,320
much the interesting part is of course

00:22:55,030 --> 00:22:58,120
we need some kind of buffer where we

00:22:56,320 --> 00:23:00,490
have the values in and we have some some

00:22:58,120 --> 00:23:03,270
kind of pointer which points points of

00:23:00,490 --> 00:23:07,210
that and this is pretty much the same

00:23:03,270 --> 00:23:09,370
independent of whether us TV or radio so

00:23:07,210 --> 00:23:13,930
I mean in the end this part is always

00:23:09,370 --> 00:23:15,430
the same we have types like something

00:23:13,930 --> 00:23:17,350
like he only which would mean this

00:23:15,430 --> 00:23:20,800
filter type of things we have something

00:23:17,350 --> 00:23:24,190
like in so if you just have neat very

00:23:20,800 --> 00:23:26,200
simple things strings which is actually

00:23:24,190 --> 00:23:29,110
superseded by Jason Jason means you can

00:23:26,200 --> 00:23:31,240
like store anything in there there's

00:23:29,110 --> 00:23:33,250
something like a special subtype which

00:23:31,240 --> 00:23:36,510
is called in with innervates that's

00:23:33,250 --> 00:23:40,090
that's for a special use case where you

00:23:36,510 --> 00:23:43,600
want to while you are looking up go a

00:23:40,090 --> 00:23:45,850
certain path sounds complicated that's

00:23:43,600 --> 00:23:50,470
exactly the completion case you're

00:23:45,850 --> 00:23:52,540
starting with fa and the most you want

00:23:50,470 --> 00:23:54,220
to want to have the most relevant

00:23:52,540 --> 00:23:56,320
completion which we probably facebook

00:23:54,220 --> 00:23:57,880
and you don't want to like traverse the

00:23:56,320 --> 00:23:59,830
whole thing but you want to have it

00:23:57,880 --> 00:24:02,350
pretty inefficient then for that you

00:23:59,830 --> 00:24:04,810
need something like in a weights of

00:24:02,350 --> 00:24:09,010
course we are not storing Jason as is

00:24:04,810 --> 00:24:10,350
but using some clever compression with

00:24:09,010 --> 00:24:13,480
message peg which is a binary

00:24:10,350 --> 00:24:19,450
representation we can put snobby set

00:24:13,480 --> 00:24:23,280
live forever on top so I said key value

00:24:19,450 --> 00:24:27,000
index and not Q value store so

00:24:23,280 --> 00:24:31,020
what about updates and what about real

00:24:27,000 --> 00:24:33,240
time that's how I see two questions but

00:24:31,020 --> 00:24:36,000
I have to challenge that a bit what is

00:24:33,240 --> 00:24:38,310
really your use case do you really need

00:24:36,000 --> 00:24:40,500
real time or do you just need to have it

00:24:38,310 --> 00:24:43,680
fast enough I think most of people just

00:24:40,500 --> 00:24:45,330
need it fast enough right and the same

00:24:43,680 --> 00:24:48,000
for the updates okay if you have

00:24:45,330 --> 00:24:50,370
constant rights constantly right to it

00:24:48,000 --> 00:24:53,490
TV is not for you use Redis use

00:24:50,370 --> 00:24:56,480
something else if you are have mostly

00:24:53,490 --> 00:25:00,420
weeds and a little bit of rights

00:24:56,480 --> 00:25:04,380
consider it in the end it's the same

00:25:00,420 --> 00:25:08,010
kind of limitation or thing that that

00:25:04,380 --> 00:25:09,930
you have in lucene and you see leucine

00:25:08,010 --> 00:25:12,300
you have these segments and they they

00:25:09,930 --> 00:25:16,170
are immutable and then the segment's get

00:25:12,300 --> 00:25:18,540
merged we don't have that yet what feet

00:25:16,170 --> 00:25:20,610
who is the pragmatic solution of segment

00:25:18,540 --> 00:25:25,110
matching it's just having one huge

00:25:20,610 --> 00:25:26,670
master segment and on that we apply just

00:25:25,110 --> 00:25:29,490
Delta updates and they are not

00:25:26,670 --> 00:25:32,460
cumulative we are just always replacing

00:25:29,490 --> 00:25:35,460
them but on the size size wise these

00:25:32,460 --> 00:25:40,380
data updates are much much smaller and

00:25:35,460 --> 00:25:42,270
we can like deploy them in in a short

00:25:40,380 --> 00:25:44,400
amount of time and depending on the use

00:25:42,270 --> 00:25:49,100
case some we want to update our lives on

00:25:44,400 --> 00:25:49,100
daily some some stuff just on the amount

00:25:50,540 --> 00:25:57,110
so what I haven't told you about this is

00:25:54,030 --> 00:26:00,300
what else can you do with TV because

00:25:57,110 --> 00:26:04,050
it's it's finite state it's not a hash

00:26:00,300 --> 00:26:09,840
table you have this little nice graph

00:26:04,050 --> 00:26:11,640
structure and this finite state some of

00:26:09,840 --> 00:26:14,280
you might know to somehow sounds like

00:26:11,640 --> 00:26:17,340
NLP and actually this is where it used

00:26:14,280 --> 00:26:18,840
to be used so what you can do is if you

00:26:17,340 --> 00:26:23,250
want you can still build an entity

00:26:18,840 --> 00:26:29,400
recognizer we are using it also in in

00:26:23,250 --> 00:26:31,830
spark the other part of things is like

00:26:29,400 --> 00:26:35,490
all these approximate or completion type

00:26:31,830 --> 00:26:37,230
of matching things there's also things

00:26:35,490 --> 00:26:39,809
like you can use it for Gio

00:26:37,230 --> 00:26:42,419
applications because in the end the geo

00:26:39,809 --> 00:26:44,970
look up is just an approximate match you

00:26:42,419 --> 00:26:47,340
are here you want to know where the next

00:26:44,970 --> 00:26:55,590
drug store is so it's an approximate

00:26:47,340 --> 00:26:59,640
match in the end all right this it's

00:26:55,590 --> 00:27:02,460
just a small project right now but still

00:26:59,640 --> 00:27:06,000
for more infos and material just go to

00:27:02,460 --> 00:27:08,460
the to the UL there which is right now

00:27:06,000 --> 00:27:10,290
just to redirect or to get a page but

00:27:08,460 --> 00:27:14,370
what you find there is like all these

00:27:10,290 --> 00:27:15,840
more information about that also the

00:27:14,370 --> 00:27:18,150
papers and so on which are basically

00:27:15,840 --> 00:27:22,410
like similar it's based on the same

00:27:18,150 --> 00:27:26,070
stuff that leucine is based on yeah and

00:27:22,410 --> 00:27:34,590
look around and I hope maybe you have a

00:27:26,070 --> 00:27:38,520
use for it thank you so I hope that they

00:27:34,590 --> 00:27:45,090
are some questions thank you Oh many

00:27:38,520 --> 00:27:49,260
questions I I have a question about one

00:27:45,090 --> 00:27:53,490
of the first slides you show and you

00:27:49,260 --> 00:27:59,010
have this FST with suffix compaction yet

00:27:53,490 --> 00:28:02,370
okay as the Edit teapot FST finite-state

00:27:59,010 --> 00:28:05,340
with this one this one yes so you have

00:28:02,370 --> 00:28:09,059
here like buzzwords and keyboards yeah

00:28:05,340 --> 00:28:11,580
and they end up in the same state how so

00:28:09,059 --> 00:28:15,750
you can say that you have those two

00:28:11,580 --> 00:28:17,700
different words in your dictionary okay

00:28:15,750 --> 00:28:20,040
but how do you associate different

00:28:17,700 --> 00:28:22,770
values which that wouldn't work in this

00:28:20,040 --> 00:28:24,929
case okay so this is exactly the point

00:28:22,770 --> 00:28:27,120
that if well you basically means that

00:28:24,929 --> 00:28:29,520
your end state is different and then you

00:28:27,120 --> 00:28:32,340
would not be able to compress it I mean

00:28:29,520 --> 00:28:35,070
in this example is simplified it with

00:28:32,340 --> 00:28:37,590
just showing a key only kind of

00:28:35,070 --> 00:28:39,210
structure in real applications we of

00:28:37,590 --> 00:28:43,049
course have values and then the the

00:28:39,210 --> 00:28:45,990
picture changes but still if you have

00:28:43,049 --> 00:28:48,960
something like ranking data some model

00:28:45,990 --> 00:28:50,260
data it's the slide that I showed before

00:28:48,960 --> 00:28:53,370
it

00:28:50,260 --> 00:28:56,890
still compressors and it's still like

00:28:53,370 --> 00:28:59,620
what helps TV is giving it a lot lot lot

00:28:56,890 --> 00:29:02,890
of data because the more data you

00:28:59,620 --> 00:29:05,530
increase the probability of some kind of

00:29:02,890 --> 00:29:09,390
duplication in the end you can say this

00:29:05,530 --> 00:29:09,390
is somehow and deduplication engine

00:29:10,650 --> 00:29:17,830
Nevermore yes you mentioned the scene

00:29:14,050 --> 00:29:19,630
has an FST in it that's yeah and but how

00:29:17,830 --> 00:29:21,610
does it compare to this one and you

00:29:19,630 --> 00:29:23,620
mentioned you do la interview intervene

00:29:21,610 --> 00:29:25,420
for compression they have different

00:29:23,620 --> 00:29:28,080
cooperation machine but performance wise

00:29:25,420 --> 00:29:31,810
memory wise these kind of things ok I

00:29:28,080 --> 00:29:35,320
prepared for this question haha that's

00:29:31,810 --> 00:29:37,960
why I asked yeah I mean the thing the

00:29:35,320 --> 00:29:41,370
the reason why I didn't show it it's

00:29:37,960 --> 00:29:43,870
it's again my kind of questioning this

00:29:41,370 --> 00:29:47,680
benchmarking because it's it's always

00:29:43,870 --> 00:29:49,990
hard to do benchmarks right and I I

00:29:47,680 --> 00:29:52,660
follow the the implementation in low

00:29:49,990 --> 00:29:55,480
scene for quite some time I know it but

00:29:52,660 --> 00:29:58,210
I'm not an expert on it so what i did

00:29:55,480 --> 00:30:02,170
was i I just quickly last week

00:29:58,210 --> 00:30:05,730
implemented some tools to make these

00:30:02,170 --> 00:30:12,190
kind of benchmarks and the size wise

00:30:05,730 --> 00:30:14,800
it's almost equal to leucine but I was

00:30:12,190 --> 00:30:17,350
not able to get a large put in large

00:30:14,800 --> 00:30:20,100
data I just it for me just crashes the

00:30:17,350 --> 00:30:25,360
the memory consumption doing the

00:30:20,100 --> 00:30:27,310
construction is is too high it doesn't

00:30:25,360 --> 00:30:29,710
work because it basically uses when I

00:30:27,310 --> 00:30:32,650
said when I showed this hash table thing

00:30:29,710 --> 00:30:36,340
it uses something unbounded it doesn't

00:30:32,650 --> 00:30:39,970
have this bounded a hash table for

00:30:36,340 --> 00:30:44,110
example and that just blows up for look

00:30:39,970 --> 00:30:45,580
up I did some comparison here because

00:30:44,110 --> 00:30:49,180
what you have in lucene is you have

00:30:45,580 --> 00:30:51,100
every stage is is it's not using this

00:30:49,180 --> 00:30:52,900
positive reefing but it puts all the

00:30:51,100 --> 00:30:54,340
outgoing transitions in this state so

00:30:52,900 --> 00:30:56,020
what you have to do is doing a lookup

00:30:54,340 --> 00:30:58,150
you always do a linear scan in every

00:30:56,020 --> 00:31:01,390
state and have to find the right spot to

00:30:58,150 --> 00:31:03,879
continue while you and key we can do the

00:31:01,390 --> 00:31:08,049
direct you can go

00:31:03,879 --> 00:31:10,389
directly jump directly so I have a

00:31:08,049 --> 00:31:13,479
benchmark here but honestly I'm not sure

00:31:10,389 --> 00:31:16,690
is that due to Java is it you too Lucy

00:31:13,479 --> 00:31:20,709
due to c plus plus i don't know i don't

00:31:16,690 --> 00:31:23,349
want to start a language whoa so it is

00:31:20,709 --> 00:31:28,179
something like eight times faster in TV

00:31:23,349 --> 00:31:29,440
but i mean if we make a proper

00:31:28,179 --> 00:31:32,709
comparison we should compare algorithms

00:31:29,440 --> 00:31:34,779
and not implementations and maybe it's

00:31:32,709 --> 00:31:38,619
it's some something that leucine folks

00:31:34,779 --> 00:31:40,569
got code log into and considering taking

00:31:38,619 --> 00:31:42,219
some of the features out of TV and

00:31:40,569 --> 00:31:54,509
putting it into the leucine FST

00:31:42,219 --> 00:31:54,509

YouTube URL: https://www.youtube.com/watch?v=GBjisdmHe4g


