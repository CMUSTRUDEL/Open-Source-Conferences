Title: Berlin Buzzwords 2016: Jan Graßegger - Real-time analytics with Flink and Druid #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Real time insights into multi-dimensional data is a key asset for data-driven businesses. We present the architecture of our fast and reliable streaming-only data processing pipeline which harnesses the qualities of Kafka, Flink and Druid. This trio turns out to be a very good choice for building real-time online analytics systems.

In recent years Apache Kafka has become the de-facto standard for highly available and highly scalable messaging.

Apache Flink allows us to consume, process and produce data with minimum delay. When using a streaming-only approach the challenge is to guarantee the correctness of your data. Flink’s capability of using different sources (in our case Kafka for real-time and HDFS for historical data) easily lets us reprocess data without any need of maintaining multiple code bases often needed in Lambda Architectures.

Druid is a datastore designed for real-time multidimensional analytics and overcomes weaknesses of alternative approaches like RDBs and Key-Value stores. It’s streaming ingestion plays extremely well with Flink which is able to process every event as it arrives. We have already contributed our Flink sink to the Druid project so that you can use it out-of-the-box.

Read more:
https://2016.berlinbuzzwords.de/session/real-time-analytics-flink-and-druid

About Jan Graßegger:
https://2016.berlinbuzzwords.de/users/jan-grassegger

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,959 --> 00:00:09,420
hello okay yeah welcome everybody to my

00:00:07,410 --> 00:00:12,540
talk about real-time analytics both

00:00:09,420 --> 00:00:16,230
flink intrude yeah my name is Ian kosaka

00:00:12,540 --> 00:00:21,119
and I'm working as a data engineer with

00:00:16,230 --> 00:00:23,789
mb are targeting MBR steering real-time

00:00:21,119 --> 00:00:26,430
advertising means that we are providing

00:00:23,789 --> 00:00:29,640
a demand-side platform this P to help

00:00:26,430 --> 00:00:35,000
advertisers which the customers more

00:00:29,640 --> 00:00:39,239
efficiently and the last time we were at

00:00:35,000 --> 00:00:41,969
last time we working you were working on

00:00:39,239 --> 00:00:45,480
real-time analysis and reporting to and

00:00:41,969 --> 00:00:47,430
this is very important for us and there

00:00:45,480 --> 00:00:51,270
are customers key to get live insights

00:00:47,430 --> 00:00:56,340
into their campaigns that are running on

00:00:51,270 --> 00:01:01,280
our platform but this task is not that

00:00:56,340 --> 00:01:04,650
review since we have some data like

00:01:01,280 --> 00:01:08,400
70,000 events per second each of this

00:01:04,650 --> 00:01:12,380
event as up to 50 different dimensions

00:01:08,400 --> 00:01:15,240
and we are calculating 20 metrics based

00:01:12,380 --> 00:01:20,460
on these events even types and

00:01:15,240 --> 00:01:26,670
dimensions so this is kind of a lot of

00:01:20,460 --> 00:01:31,079
data and we had to look for for storage

00:01:26,670 --> 00:01:33,930
that is able at first to consume that

00:01:31,079 --> 00:01:40,229
amount of data and show it to us

00:01:33,930 --> 00:01:45,630
immediately and it is also able to to

00:01:40,229 --> 00:01:49,049
handle queries where we can combine any

00:01:45,630 --> 00:01:51,570
dimension with each other and get a

00:01:49,049 --> 00:01:56,219
response in an acceptable amount of time

00:01:51,570 --> 00:01:58,469
like in under 1 second or so and we were

00:01:56,219 --> 00:02:03,509
also looking for an open source source

00:01:58,469 --> 00:02:05,369
solution and since the problem is is

00:02:03,509 --> 00:02:08,190
quite hard and it's not easy to solve

00:02:05,369 --> 00:02:12,569
them with relational databases or QA

00:02:08,190 --> 00:02:13,890
stores since you ever have dimension

00:02:12,569 --> 00:02:16,260
with a quite high card

00:02:13,890 --> 00:02:19,020
that he liked top-level domain or

00:02:16,260 --> 00:02:21,240
whatever their if million of millions of

00:02:19,020 --> 00:02:25,230
entries of different entries and these

00:02:21,240 --> 00:02:27,900
columns and four for example for a key

00:02:25,230 --> 00:02:32,430
where you would have to pre-compute all

00:02:27,900 --> 00:02:35,040
the possible combinations of these

00:02:32,430 --> 00:02:38,400
different dimensions so we were looking

00:02:35,040 --> 00:02:48,720
for for storage that does all this for

00:02:38,400 --> 00:02:51,690
us and we found route through this is an

00:02:48,720 --> 00:02:57,000
open source online analytical processing

00:02:51,690 --> 00:03:00,180
system that is made for this purpose for

00:02:57,000 --> 00:03:02,489
having high dimensional data by creating

00:03:00,180 --> 00:03:05,790
it very fast and also it has

00:03:02,489 --> 00:03:11,910
capabilities to handle this data in real

00:03:05,790 --> 00:03:14,550
time yet well it's a calamari entered

00:03:11,910 --> 00:03:17,840
storage it's distributed distributed

00:03:14,550 --> 00:03:21,750
means that you have multiple different

00:03:17,840 --> 00:03:24,060
roads like like a real-time node that

00:03:21,750 --> 00:03:26,880
handles the real-time data estoy que

00:03:24,060 --> 00:03:29,269
node that is able to read historic data

00:03:26,880 --> 00:03:33,390
also from distributed storage just like

00:03:29,269 --> 00:03:36,090
HDFS or as free and you have have a

00:03:33,390 --> 00:03:39,180
broker well that knows where to carry

00:03:36,090 --> 00:03:43,410
forward data and we you're crazy it has

00:03:39,180 --> 00:03:46,650
to go to it has built-in data charting

00:03:43,410 --> 00:03:49,200
capabilities means all your data is

00:03:46,650 --> 00:03:53,220
organized in in so-called segments and

00:03:49,200 --> 00:03:58,410
these segments are shot up by time when

00:03:53,220 --> 00:04:00,690
you create data include you you're

00:03:58,410 --> 00:04:03,420
saying yeah I want segments that contain

00:04:00,690 --> 00:04:07,320
data of one hour or one day or one

00:04:03,420 --> 00:04:10,109
minute and whatever and you carry food

00:04:07,320 --> 00:04:15,320
with adjacent crappy language yr rest

00:04:10,109 --> 00:04:20,549
like HTTP API so the question is now

00:04:15,320 --> 00:04:22,830
what makes food fast and why is it how

00:04:20,549 --> 00:04:25,830
is it able to to handle this

00:04:22,830 --> 00:04:31,849
multi-dimensional data

00:04:25,830 --> 00:04:36,270
and it's one thing is that it is the way

00:04:31,849 --> 00:04:40,740
how it creates how it creates the data

00:04:36,270 --> 00:04:46,169
structure for example we have a column

00:04:40,740 --> 00:04:49,530
of top private domains just two here we

00:04:46,169 --> 00:04:53,789
have two different domains with two

00:04:49,530 --> 00:04:57,919
entries each and what does it maps all

00:04:53,789 --> 00:05:02,669
string values of columns to integers and

00:04:57,919 --> 00:05:06,629
so it is able to to encode the column

00:05:02,669 --> 00:05:10,550
data very efficiently that's one thing

00:05:06,629 --> 00:05:15,719
so it this trains the amount of data a

00:05:10,550 --> 00:05:19,500
lot by a lot and a second data structure

00:05:15,719 --> 00:05:24,719
our bitmap indices that means for every

00:05:19,500 --> 00:05:28,620
column it stores is is that where you in

00:05:24,719 --> 00:05:30,090
that column or not so if if we have for

00:05:28,620 --> 00:05:33,029
example we have bet on that on the first

00:05:30,090 --> 00:05:35,400
two columns so it was the one for these

00:05:33,029 --> 00:05:37,469
two columns and the zero for the other

00:05:35,400 --> 00:05:42,839
columns and this makes it quite

00:05:37,469 --> 00:05:45,449
efficiently to to further on on entries

00:05:42,839 --> 00:05:49,469
like for example if we want all entries

00:05:45,449 --> 00:05:52,440
that that contain battle net and knocks

00:05:49,469 --> 00:05:54,900
it come when we could do your operation

00:05:52,440 --> 00:05:59,759
on these two areas and get all columns

00:05:54,900 --> 00:06:01,139
that contain these two entries yeah

00:05:59,759 --> 00:06:05,639
that's on data structures now's the

00:06:01,139 --> 00:06:10,339
question how do we get our data into

00:06:05,639 --> 00:06:14,580
truth and they are so-called fire hoses

00:06:10,339 --> 00:06:16,500
which are the data ingestion

00:06:14,580 --> 00:06:21,060
capabilities of two didn't usually a

00:06:16,500 --> 00:06:25,080
data stream poll based means that you

00:06:21,060 --> 00:06:29,190
create these fire hoses wire why at the

00:06:25,080 --> 00:06:32,699
JSON API and then you give a local file

00:06:29,190 --> 00:06:36,919
paths or HDFS file paths or you give a

00:06:32,699 --> 00:06:36,919
Kafka topic and

00:06:38,080 --> 00:06:43,180
possible but what we finally wanted for

00:06:41,349 --> 00:06:46,780
our system was something different we

00:06:43,180 --> 00:06:50,819
wanted to to push data into food to read

00:06:46,780 --> 00:06:54,400
it from stream that comes from Kafka and

00:06:50,819 --> 00:06:57,789
put it on transform it a bit and then

00:06:54,400 --> 00:07:00,639
put it into you interviewed and so now

00:06:57,789 --> 00:07:04,090
we decided to to use truth as our

00:07:00,639 --> 00:07:07,539
storage which was quite because it was

00:07:04,090 --> 00:07:11,069
quite impressive to us but we needed

00:07:07,539 --> 00:07:15,460
some some other tool to to put the data

00:07:11,069 --> 00:07:20,229
entered food and so we took a look look

00:07:15,460 --> 00:07:22,360
at fling fling is a platform for

00:07:20,229 --> 00:07:25,030
distributed stream and batch processing

00:07:22,360 --> 00:07:27,490
but the interesting thing on fling is

00:07:25,030 --> 00:07:30,879
that everything is stream based so you

00:07:27,490 --> 00:07:33,909
can work on every single event it's

00:07:30,879 --> 00:07:36,580
quite nice yeah i think the talk

00:07:33,909 --> 00:07:40,860
afterwards we'll get a more deeply into

00:07:36,580 --> 00:07:43,629
thing so i will keep it short but flink

00:07:40,860 --> 00:07:47,909
it's really nice to work with and so we

00:07:43,629 --> 00:07:51,069
came up with these really kind of

00:07:47,909 --> 00:07:53,259
architecture are designing how we want

00:07:51,069 --> 00:07:56,319
to process our data we want to consume

00:07:53,259 --> 00:07:58,120
it we have it can conquer once you

00:07:56,319 --> 00:08:02,020
consume it will fling and put it into it

00:07:58,120 --> 00:08:09,120
now the question is how to how to

00:08:02,020 --> 00:08:13,360
combine fling and intrude and there's a

00:08:09,120 --> 00:08:18,669
tool sub-project of of truth that's

00:08:13,360 --> 00:08:21,909
called funk ility and fertility is made

00:08:18,669 --> 00:08:25,270
for this purpose of putting real time

00:08:21,909 --> 00:08:28,300
data touch-based into truth and it

00:08:25,270 --> 00:08:31,839
provides several adapters like for them

00:08:28,300 --> 00:08:35,050
the spark storm and now it also contains

00:08:31,839 --> 00:08:36,849
an flink adapter because we committed it

00:08:35,050 --> 00:08:40,329
to the project but it was quite fast

00:08:36,849 --> 00:08:44,169
forward to do that and now it also has a

00:08:40,329 --> 00:08:48,819
fling sink worm that enables you to

00:08:44,169 --> 00:08:51,589
easily put your data into food and also

00:08:48,819 --> 00:08:54,450
provides some standalone HTTP

00:08:51,589 --> 00:08:57,050
server or Kafka application where each

00:08:54,450 --> 00:09:02,100
data from calf computed into to it but

00:08:57,050 --> 00:09:06,149
yeah doesn't matter for us so now we we

00:09:02,100 --> 00:09:13,170
know how to put the data from fling to

00:09:06,149 --> 00:09:16,140
truth but there was one open problem

00:09:13,170 --> 00:09:21,209
that we discovered or two things that

00:09:16,140 --> 00:09:25,380
we're still missing and the main thing

00:09:21,209 --> 00:09:30,690
is how how are we doing replaced with

00:09:25,380 --> 00:09:33,360
this architecture because tranquility is

00:09:30,690 --> 00:09:38,269
really made for the purpose of of

00:09:33,360 --> 00:09:43,890
putting putting real-time data in and

00:09:38,269 --> 00:09:48,540
it's made with the there was an image

00:09:43,890 --> 00:09:51,050
matter made from the idea of a lambda

00:09:48,540 --> 00:09:54,089
architecture where you have different

00:09:51,050 --> 00:09:56,910
processing paths for stream for

00:09:54,089 --> 00:10:04,680
streaming and for batch processing means

00:09:56,910 --> 00:10:09,029
that he means that you put data here's

00:10:04,680 --> 00:10:11,579
the image means that he put data while

00:10:09,029 --> 00:10:14,520
streams and real-time into truth but

00:10:11,579 --> 00:10:17,160
there's but he repossess everything

00:10:14,520 --> 00:10:22,700
again with period and reading it from a

00:10:17,160 --> 00:10:27,300
HDFS and use is dead sese ground truth

00:10:22,700 --> 00:10:29,600
so if we would have gone with this kind

00:10:27,300 --> 00:10:33,440
of architecture we would have to

00:10:29,600 --> 00:10:36,209
maintain two different code paths two

00:10:33,440 --> 00:10:42,209
different tools that that put the data

00:10:36,209 --> 00:10:47,640
into our into our storage and also to

00:10:42,209 --> 00:10:50,310
put to prepare the data for true to 22 h

00:10:47,640 --> 00:10:53,370
and write it prepare to HDFS and this is

00:10:50,310 --> 00:10:57,269
something that we didn't want what we

00:10:53,370 --> 00:11:01,560
would like to do is to put this together

00:10:57,269 --> 00:11:04,079
to have something of have like one code

00:11:01,560 --> 00:11:05,130
paths that processes everything and so

00:11:04,079 --> 00:11:10,320
we came up with

00:11:05,130 --> 00:11:15,530
kappa of the idea we with of a Kappa

00:11:10,320 --> 00:11:18,990
architecture and the Kappa architecture

00:11:15,530 --> 00:11:22,020
relies on relies on the idea of that

00:11:18,990 --> 00:11:25,110
streaming is more reliable nowadays that

00:11:22,020 --> 00:11:28,590
you can also have a crown truth bate

00:11:25,110 --> 00:11:32,790
bate only based on streaming but there

00:11:28,590 --> 00:11:34,710
was one thing that the cafe the idea of

00:11:32,790 --> 00:11:39,540
copper is like even just working with

00:11:34,710 --> 00:11:42,300
with Kafka you have like thirty 30 days

00:11:39,540 --> 00:11:43,680
of your data stored as historical data

00:11:42,300 --> 00:11:45,720
and Kafka and if you want to reach

00:11:43,680 --> 00:11:50,370
possess something you just read the

00:11:45,720 --> 00:11:52,020
kafka stream again but in our case we we

00:11:50,370 --> 00:11:54,660
have a lot of data already in our

00:11:52,020 --> 00:11:57,300
warehouse we have like almost all data

00:11:54,660 --> 00:11:59,820
that ever came to to us in our warehouse

00:11:57,300 --> 00:12:04,200
so we would like to be able to process

00:11:59,820 --> 00:12:10,050
that and not to stir it in also in Kafka

00:12:04,200 --> 00:12:13,890
and so we came up with this kind of an

00:12:10,050 --> 00:12:20,930
idea of of an architecture where we have

00:12:13,890 --> 00:12:24,240
Kafka and HD f SS sources for fling and

00:12:20,930 --> 00:12:27,870
the dicen are the only to you that the

00:12:24,240 --> 00:12:30,690
sources are the only difference in the

00:12:27,870 --> 00:12:36,480
code path when we want to process our

00:12:30,690 --> 00:12:39,150
data that's still thinks it's really

00:12:36,480 --> 00:12:43,470
nice idea and weather was really really

00:12:39,150 --> 00:12:46,560
hard to achieve that because food was

00:12:43,470 --> 00:12:49,890
really designed and also tranquility was

00:12:46,560 --> 00:12:54,560
designed to you to work with lambda

00:12:49,890 --> 00:12:57,840
architectures and for example

00:12:54,560 --> 00:13:00,590
tranquility drops every event that is if

00:12:57,840 --> 00:13:03,960
you if you have segments that are

00:13:00,590 --> 00:13:05,700
partitioned by by our for example then

00:13:03,960 --> 00:13:09,420
it drops every message that is older

00:13:05,700 --> 00:13:12,900
than one hour and there was not built

00:13:09,420 --> 00:13:16,500
into to be able to to add all the data

00:13:12,900 --> 00:13:17,980
and also it awesome food you cannot

00:13:16,500 --> 00:13:20,590
reopen a segment

00:13:17,980 --> 00:13:23,740
you have to delete it and then create a

00:13:20,590 --> 00:13:27,510
new one and this is all of this we had

00:13:23,740 --> 00:13:30,790
to you I first find out and then

00:13:27,510 --> 00:13:34,000
implement it on ourselves but this is

00:13:30,790 --> 00:13:36,520
something that we finally achieved so

00:13:34,000 --> 00:13:40,330
now we have this couple like

00:13:36,520 --> 00:13:44,020
architecture that is able to do we place

00:13:40,330 --> 00:13:46,270
from HDFS and Kafka and do does

00:13:44,020 --> 00:13:48,640
everything with linked with the same

00:13:46,270 --> 00:13:53,290
code pass with a streaming API off link

00:13:48,640 --> 00:13:59,230
and yeah that's what was quite nice to

00:13:53,290 --> 00:14:01,780
achieve and to to summit now up a bit we

00:13:59,230 --> 00:14:06,070
as I already said we already committed

00:14:01,780 --> 00:14:10,870
the fling sink too to tranquility we

00:14:06,070 --> 00:14:13,390
have these kind of hacked replays that

00:14:10,870 --> 00:14:18,190
we edited faculty but this is still on

00:14:13,390 --> 00:14:21,700
our in our repository we are going to

00:14:18,190 --> 00:14:23,380
try to commit this to oncology but i'm

00:14:21,700 --> 00:14:26,140
not sure if they will accept it but

00:14:23,380 --> 00:14:28,390
you're still free to have a look at our

00:14:26,140 --> 00:14:32,620
tranquility repository from mb are

00:14:28,390 --> 00:14:36,730
targeting so also on github so yeah and

00:14:32,620 --> 00:14:39,010
finally to to the final goal that we

00:14:36,730 --> 00:14:41,500
want you each is the to have a real-time

00:14:39,010 --> 00:14:44,290
reporting and analysis tool that's

00:14:41,500 --> 00:14:49,560
something that we achieved and yeah that

00:14:44,290 --> 00:14:53,830
was quite cool in it an interesting way

00:14:49,560 --> 00:15:01,320
we learned a lot on this way yeah thank

00:14:53,830 --> 00:15:01,320
you very much questions please ok

00:15:05,190 --> 00:15:18,820
I'm running I'm curious about how you

00:15:14,370 --> 00:15:21,970
got it all together like the iterators

00:15:18,820 --> 00:15:24,579
in Kafka the checkpoints from fling the

00:15:21,970 --> 00:15:27,579
druid segments and I guess the Hadoop

00:15:24,579 --> 00:15:29,050
files or partitions I mean they all have

00:15:27,579 --> 00:15:35,470
to be in sync can you tell us a little

00:15:29,050 --> 00:15:40,990
bit about that yeah we have let's start

00:15:35,470 --> 00:15:45,100
with the data in HDFS so that something

00:15:40,990 --> 00:15:47,019
we already had and it's petitioned by by

00:15:45,100 --> 00:15:49,810
it's already petitioned by our it's

00:15:47,019 --> 00:15:55,300
these are five cables that are potential

00:15:49,810 --> 00:15:58,079
buyer then we have these fling job that

00:15:55,300 --> 00:16:03,730
runs all the time and fetches the data

00:15:58,079 --> 00:16:07,209
from from Kafka and thus checkpointing

00:16:03,730 --> 00:16:15,670
so if you if it fails it starts again on

00:16:07,209 --> 00:16:20,050
the same offset and so just and it's

00:16:15,670 --> 00:16:22,000
quite simple because we also have to do

00:16:20,050 --> 00:16:24,820
some joining in our pipeline that but

00:16:22,000 --> 00:16:28,290
that was all happening before so we are

00:16:24,820 --> 00:16:31,570
just reading already joint events

00:16:28,290 --> 00:16:33,399
transforming them to and filtering

00:16:31,570 --> 00:16:36,459
filtering out the dimensions we don't

00:16:33,399 --> 00:16:39,730
need for food and then it runs all the

00:16:36,459 --> 00:16:44,140
time if it fails we just restarted and

00:16:39,730 --> 00:16:49,060
it keeps up running and the HDFS replay

00:16:44,140 --> 00:16:52,029
is something that is more that we

00:16:49,060 --> 00:16:55,839
developed you to handle failures either

00:16:52,029 --> 00:17:01,029
in the workflow with a note crashes and

00:16:55,839 --> 00:17:03,220
we have there for some reason I events

00:17:01,029 --> 00:17:05,140
missing or if something was

00:17:03,220 --> 00:17:08,980
misconfigured in our campaigns or so and

00:17:05,140 --> 00:17:11,050
then we are able to restart this replace

00:17:08,980 --> 00:17:15,150
manually so we are not doing this lambda

00:17:11,050 --> 00:17:17,790
architecture weights usually

00:17:15,150 --> 00:17:20,820
way works the way that he did screaming

00:17:17,790 --> 00:17:23,220
all the time but you always override the

00:17:20,820 --> 00:17:28,730
data by the batch process that's

00:17:23,220 --> 00:17:35,130
something we do again nothing yeah okay

00:17:28,730 --> 00:17:39,920
one more minute left for questions just

00:17:35,130 --> 00:17:43,710
give me a second okay have you tried to

00:17:39,920 --> 00:17:50,250
to send the message from a gfs back into

00:17:43,710 --> 00:17:53,700
calc and we reuse your your flow in a

00:17:50,250 --> 00:17:57,630
production system I did something like

00:17:53,700 --> 00:18:00,330
these but the amount of data that I have

00:17:57,630 --> 00:18:02,730
to send back to to to calc is very

00:18:00,330 --> 00:18:04,290
limited in my use case but in yours

00:18:02,730 --> 00:18:07,710
because you have to do the full

00:18:04,290 --> 00:18:10,080
processing yeah if you have to recompute

00:18:07,710 --> 00:18:13,290
everything I expect you to have to send

00:18:10,080 --> 00:18:15,900
a lot of data back into kafir kafir

00:18:13,290 --> 00:18:20,250
probably more than the 30 days let's say

00:18:15,900 --> 00:18:22,680
you want to recompute a fuel history but

00:18:20,250 --> 00:18:25,740
yeah in that case you would really need

00:18:22,680 --> 00:18:29,370
to to to be able to read from HDFS but

00:18:25,740 --> 00:18:32,040
it's just an idea and you guys can send

00:18:29,370 --> 00:18:34,890
the data from ATF has back into caffeine

00:18:32,040 --> 00:18:37,530
to a different topic and if the data to

00:18:34,890 --> 00:18:42,260
repossess is not that much you can reuse

00:18:37,530 --> 00:18:45,000
the the code that reads from Kafka yeah

00:18:42,260 --> 00:18:47,640
yeah we thought about it but the point

00:18:45,000 --> 00:18:52,620
is that its kind kind of trivially to

00:18:47,640 --> 00:18:57,270
eat from HDFS with link so to write a

00:18:52,620 --> 00:18:59,580
process that to develop the software

00:18:57,270 --> 00:19:01,830
that puts it to you to Kafka again and

00:18:59,580 --> 00:19:03,690
then you read it from calf gets more

00:19:01,830 --> 00:19:06,360
complex than just reading it from a TFS

00:19:03,690 --> 00:19:09,660
and it's both data is in the same format

00:19:06,360 --> 00:19:11,280
so it's really for us it's in our case

00:19:09,660 --> 00:19:14,850
it's just a difference of the source and

00:19:11,280 --> 00:19:17,640
the data is equal and that so wasn't

00:19:14,850 --> 00:19:20,360
really a challenge to each you have to

00:19:17,640 --> 00:19:23,820
read from HDFS and it works quite well

00:19:20,360 --> 00:19:27,270
okay thank you with this question we

00:19:23,820 --> 00:19:28,710
should finish this session and in 10

00:19:27,270 --> 00:19:30,360
minutes we will continue

00:19:28,710 --> 00:19:33,260
with another session on the flink topic

00:19:30,360 --> 00:19:33,260

YouTube URL: https://www.youtube.com/watch?v=mYGF4BUwtaw


