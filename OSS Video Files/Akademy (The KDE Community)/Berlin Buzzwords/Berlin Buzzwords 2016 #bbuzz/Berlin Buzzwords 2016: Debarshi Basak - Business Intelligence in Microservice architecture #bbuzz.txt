Title: Berlin Buzzwords 2016: Debarshi Basak - Business Intelligence in Microservice architecture #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Bol.com is the largest online retailer in the Netherlands and Belgium and is still growing at a staggering rate. Bol.com is a fact based decision making company hence business intelligence plays a key role in the organization. For gathering business intelligence (BI) insights we have a team of twelve dedicated BI engineers. 

However, it is not just the scale of data, but also the complexity of these services that can makes it difficult to gather all the business insights. The growth of data, users and complexity have forced our team of to rethink our traditional relational data warehouse structure. Currently we are moving towards a more hybrid BI solution based on Microservices at big data scale, using Hadoop, Hbase, relational databases, etc.  

In this presentation, we will discuss key concepts that govern our datawarehousing, the unique challenges we have faced and are still facing with petabytes of data along with design decisions, tooling landscape and architectural choices. We have created and explored various toolings and concepts for solving our problems.  

We discuss concepts pertaining to Data quality at big data scale, large scale batch scheduling, job monitoring, ETL processing, reporting, continuous deployment, shift in mindset of traditional datawarehouse developers and team autonomy for BI at scale. Essentially, all the hacks to get our systems towards awesomeness. 

The presentation will guide audience through transformation of legacy (vintage) BI towards more scalable BI setup as well as mental and techincal block that came along the way. We believe it is important to educate impact of microservices on datawarehouse and we have learned how we could do that at petabyte scale.

Read more:
https://2016.berlinbuzzwords.de/session/business-intelligence-scale-microservice-architecture

About Debarshi Basak:
https://2016.berlinbuzzwords.de/users/debarshi-basak

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,300 --> 00:00:12,809
awesome so I hope you guys can hear me

00:00:08,269 --> 00:00:15,150
yeah so my name is Dabashi know and I'm

00:00:12,809 --> 00:00:18,029
gonna talk about business intelligence

00:00:15,150 --> 00:00:21,810
and Microsoft's architecture I mean so

00:00:18,029 --> 00:00:23,670
uh boon comm is the company I work for

00:00:21,810 --> 00:00:25,019
and it's it's actually going to a

00:00:23,670 --> 00:00:27,150
transition right now and we are moving

00:00:25,019 --> 00:00:30,449
towards micro services so I thought we

00:00:27,150 --> 00:00:32,430
we are kind of doing some cool stuff we

00:00:30,449 --> 00:00:34,739
think it's cool but i thought we I would

00:00:32,430 --> 00:00:38,460
share with you guys and let you guys

00:00:34,739 --> 00:00:41,040
decide so what can you expect from this

00:00:38,460 --> 00:00:42,600
presentation well I'll give you a brief

00:00:41,040 --> 00:00:47,190
introduction about me and my company

00:00:42,600 --> 00:00:50,370
well and it run you through the history

00:00:47,190 --> 00:00:52,710
of business intelligence that was that

00:00:50,370 --> 00:00:55,920
was happening in Volcom since the

00:00:52,710 --> 00:00:58,649
inception of the company and and we have

00:00:55,920 --> 00:01:02,460
in my opinion they're like two eras have

00:00:58,649 --> 00:01:03,989
a good that's going to in in Bulacan one

00:01:02,460 --> 00:01:06,810
is the map we'd use face and now it's

00:01:03,989 --> 00:01:09,020
more streaming and and how we do bi with

00:01:06,810 --> 00:01:11,610
that kind of stuff it's fairly new

00:01:09,020 --> 00:01:14,369
fairly untested but I thought I would

00:01:11,610 --> 00:01:16,500
share some insights and some operational

00:01:14,369 --> 00:01:18,659
stuff about how we manage our

00:01:16,500 --> 00:01:22,350
infrastructure so but me

00:01:18,659 --> 00:01:26,580
so I'm Dabashi I'm a software engineer

00:01:22,350 --> 00:01:28,590
Volcom so I have twin responsibility at

00:01:26,580 --> 00:01:31,110
goal outcomes I'm part of a big data

00:01:28,590 --> 00:01:34,259
platform team that's I'm responsible

00:01:31,110 --> 00:01:36,060
it's a DevOps team that builds tools and

00:01:34,259 --> 00:01:38,820
and other kind of infrastructure stuff

00:01:36,060 --> 00:01:43,979
for Hadoop and and second iam also part

00:01:38,820 --> 00:01:46,649
of us CA CR Co team which makes Google

00:01:43,979 --> 00:01:48,950
Google related optimizations and stuff

00:01:46,649 --> 00:01:48,950
like that

00:01:49,690 --> 00:02:02,650
yeah so uh and about Bolcom to give you

00:01:58,150 --> 00:02:04,060
a context so it's it's an so so it's

00:02:02,650 --> 00:02:07,270
it's an e-commerce platform in

00:02:04,060 --> 00:02:09,940
Netherlands fairly popular well so the

00:02:07,270 --> 00:02:13,150
the companies or a company is one of the

00:02:09,940 --> 00:02:15,550
few first implementations of scrum in

00:02:13,150 --> 00:02:18,250
Netherlands and we have over a thousand

00:02:15,550 --> 00:02:21,250
employees actually over 200 to 300

00:02:18,250 --> 00:02:23,620
employees in IT and over 40 scrum teams

00:02:21,250 --> 00:02:27,280
and it's fairly young the average age is

00:02:23,620 --> 00:02:29,410
around 28 32 so it's it's fairly yeah

00:02:27,280 --> 00:02:32,500
and we have a motor that we built you

00:02:29,410 --> 00:02:35,740
build you run it and you love it what

00:02:32,500 --> 00:02:39,820
you build so so and to give you a idea

00:02:35,740 --> 00:02:41,470
how big the size of the data is so these

00:02:39,820 --> 00:02:44,100
are some metrics about the actual

00:02:41,470 --> 00:02:46,330
functional data but I mean in terms of

00:02:44,100 --> 00:02:50,080
the plaster we have a fairly small

00:02:46,330 --> 00:02:53,560
cluster of 26 nodes and we have a weekly

00:02:50,080 --> 00:02:55,420
we have around 90 unique jobs running

00:02:53,560 --> 00:02:59,370
you know monthly Gurgaon 300 jobs

00:02:55,420 --> 00:03:04,540
I got unique jobs running in the cluster

00:02:59,370 --> 00:03:07,720
so it's about nodes RVD now has really

00:03:04,540 --> 00:03:09,850
high capacity so - totally it's around

00:03:07,720 --> 00:03:12,850
1.5 petabytes of data that's available

00:03:09,850 --> 00:03:15,040
in our cluster so that's pretty much

00:03:12,850 --> 00:03:17,500
about what what's happening so but

00:03:15,040 --> 00:03:21,720
today's focus is about micro services

00:03:17,500 --> 00:03:24,430
and how you can do bi in it it's it's so

00:03:21,720 --> 00:03:27,970
before we good at the topic so what is

00:03:24,430 --> 00:03:31,269
micro service I mean if you see this

00:03:27,970 --> 00:03:35,320
thing this this this is a called the

00:03:31,269 --> 00:03:38,140
dead star architecture it's like yeah I

00:03:35,320 --> 00:03:41,500
hope this I'm assuming that's all war

00:03:38,140 --> 00:03:43,120
fans yeah so I yeah so I mean in the in

00:03:41,500 --> 00:03:49,720
the cool of this dead star is actually

00:03:43,120 --> 00:03:52,060
something called dependency hell so it's

00:03:49,720 --> 00:03:53,800
actually a thing so this is the IP is

00:03:52,060 --> 00:03:55,660
really badly for the shop but I mean I

00:03:53,800 --> 00:03:57,730
picked it up from from a presentation

00:03:55,660 --> 00:04:00,160
it's an actual thing so what happens is

00:03:57,730 --> 00:04:02,319
in micro service often you if you pick

00:04:00,160 --> 00:04:03,360
up any presentation view of often have

00:04:02,319 --> 00:04:06,189
Amman

00:04:03,360 --> 00:04:08,140
where you which is probably Java or will

00:04:06,189 --> 00:04:10,750
be up and then you have a database and

00:04:08,140 --> 00:04:13,590
there's some magic happening and you get

00:04:10,750 --> 00:04:15,879
some services out of it which I like so

00:04:13,590 --> 00:04:19,180
there's actually a science behind it so

00:04:15,879 --> 00:04:20,769
you I'm not really sure but you find you

00:04:19,180 --> 00:04:22,600
have dependencies and you try to figure

00:04:20,769 --> 00:04:24,669
out cohesion between the dependencies

00:04:22,600 --> 00:04:28,330
and then you decouple them together but

00:04:24,669 --> 00:04:30,340
you basically build services out of your

00:04:28,330 --> 00:04:32,620
existing will take on probably new

00:04:30,340 --> 00:04:36,190
services or something so that's that's

00:04:32,620 --> 00:04:40,539
pretty much what micro service is I well

00:04:36,190 --> 00:04:43,090
just for the context and the second part

00:04:40,539 --> 00:04:46,630
of the presentation is about business

00:04:43,090 --> 00:04:49,840
intelligence um for those who are fairly

00:04:46,630 --> 00:04:52,539
new in this field so business

00:04:49,840 --> 00:04:54,940
intelligence is about analyzing data

00:04:52,539 --> 00:05:00,460
sets and collecting actionable items for

00:04:54,940 --> 00:05:03,669
your stakeholders yeah basically you you

00:05:00,460 --> 00:05:05,590
want some KPIs or for your stakeholders

00:05:03,669 --> 00:05:08,229
to understand how your business is doing

00:05:05,590 --> 00:05:10,479
as essentially so some of the

00:05:08,229 --> 00:05:11,889
requirements that often an organization

00:05:10,479 --> 00:05:13,960
has is that all this data should be

00:05:11,889 --> 00:05:16,180
collected continuously and in an

00:05:13,960 --> 00:05:18,099
automatic fashion and often have

00:05:16,180 --> 00:05:21,430
different kind of data sources like flat

00:05:18,099 --> 00:05:24,520
files and xml and Excel she's internal

00:05:21,430 --> 00:05:27,669
databases FTP HTTP the biggest kind of

00:05:24,520 --> 00:05:29,530
sources so and once you collected them

00:05:27,669 --> 00:05:31,570
and you have transformed them the

00:05:29,530 --> 00:05:33,729
analytics that you can do on this data

00:05:31,570 --> 00:05:34,750
set should be flexible so these are

00:05:33,729 --> 00:05:38,050
often the requirements of an

00:05:34,750 --> 00:05:40,570
organization - so it's the business

00:05:38,050 --> 00:05:43,120
intelligence it's not really a new field

00:05:40,570 --> 00:05:47,139
to be honest we already have techniques

00:05:43,120 --> 00:05:48,669
like ETL which extracts data from know

00:05:47,139 --> 00:05:50,260
which basically means extracting data

00:05:48,669 --> 00:05:52,990
from different sources we have

00:05:50,260 --> 00:05:55,720
transformation lag and you have these

00:05:52,990 --> 00:05:58,419
architectures exist and then you'd load

00:05:55,720 --> 00:06:01,060
it to a target data model and target

00:05:58,419 --> 00:06:03,639
data model have something like kimbos

00:06:01,060 --> 00:06:06,669
our data modeling techniques like a star

00:06:03,639 --> 00:06:08,560
schema or snowflake schema and more

00:06:06,669 --> 00:06:11,260
advanced will be OLAP cubes which are

00:06:08,560 --> 00:06:13,229
like derivative of of star schema to

00:06:11,260 --> 00:06:16,090
give you an idea about that

00:06:13,229 --> 00:06:20,560
so on my so

00:06:16,090 --> 00:06:22,690
on my left left are you this is more of

00:06:20,560 --> 00:06:24,850
a star schema so the granularity of your

00:06:22,690 --> 00:06:28,090
data which is the thing in the middle is

00:06:24,850 --> 00:06:30,580
called fact which is the lowest lower

00:06:28,090 --> 00:06:32,169
most granular data which has the

00:06:30,580 --> 00:06:36,490
references to all this dimensional data

00:06:32,169 --> 00:06:39,040
of facts are basically mutations and you

00:06:36,490 --> 00:06:40,720
have dimensional data which is the

00:06:39,040 --> 00:06:43,360
actual when you combine them together

00:06:40,720 --> 00:06:46,450
you get the actual context of your data

00:06:43,360 --> 00:06:49,530
and more advanced is basically the cubes

00:06:46,450 --> 00:06:53,320
which is basically you aggregate your

00:06:49,530 --> 00:06:55,090
data for different dimensions so you can

00:06:53,320 --> 00:06:58,030
actually drill down and drill in easily

00:06:55,090 --> 00:07:00,610
and in if you have standard to length

00:06:58,030 --> 00:07:02,979
not sunk tooling but yeah if you but if

00:07:00,610 --> 00:07:05,470
you have if you buy to so it has a

00:07:02,979 --> 00:07:08,710
different way of querying so you use MDX

00:07:05,470 --> 00:07:10,600
to query will up cubes I'm not the best

00:07:08,710 --> 00:07:12,610
person to give you idea about this but I

00:07:10,600 --> 00:07:14,470
mean it's it's a fairly open data you

00:07:12,610 --> 00:07:18,669
can find it's been there for a long time

00:07:14,470 --> 00:07:22,900
so it's kind of the idea behind it so in

00:07:18,669 --> 00:07:26,080
in a monolithic days back in I think

00:07:22,900 --> 00:07:28,510
early around 2000 or something we used

00:07:26,080 --> 00:07:30,430
to have this so we had online we Cibola

00:07:28,510 --> 00:07:33,729
come as a webshop so we had online

00:07:30,430 --> 00:07:35,740
systems and if a marketing wants to make

00:07:33,729 --> 00:07:38,470
a campaign the marketing person would

00:07:35,740 --> 00:07:42,430
actually query the online systems make a

00:07:38,470 --> 00:07:45,520
report or out of it doesn't matter you

00:07:42,430 --> 00:07:46,690
know and then do stuff but at one point

00:07:45,520 --> 00:07:49,960
you know it doesn't really work it

00:07:46,690 --> 00:07:51,490
affects the actual webshop and at that

00:07:49,960 --> 00:07:57,250
point they started thinking of actually

00:07:51,490 --> 00:07:58,360
decoupling your bi system and well at

00:07:57,250 --> 00:08:00,910
that point they hired a lot of

00:07:58,360 --> 00:08:04,510
consultants and architects and they came

00:08:00,910 --> 00:08:07,660
up with the architecture of called the

00:08:04,510 --> 00:08:09,760
data hub architecture hub hub and spoke

00:08:07,660 --> 00:08:12,430
architecture and bi so you actually have

00:08:09,760 --> 00:08:16,780
a data as actually replicated to a data

00:08:12,430 --> 00:08:18,850
hub so you have a materialized view with

00:08:16,780 --> 00:08:21,550
a remote link to your online systems and

00:08:18,850 --> 00:08:23,380
you would request a refresh your

00:08:21,550 --> 00:08:25,120
materialized view every 15 minutes or

00:08:23,380 --> 00:08:27,159
maybe daily

00:08:25,120 --> 00:08:29,680
no it depends upon how often you want

00:08:27,159 --> 00:08:30,910
the data and from the data

00:08:29,680 --> 00:08:33,640
that's that step is called application

00:08:30,910 --> 00:08:35,140
and from our deed hub to the data

00:08:33,640 --> 00:08:37,169
warehouse you would have under the

00:08:35,140 --> 00:08:41,440
materialized view that would sink your

00:08:37,169 --> 00:08:43,450
with the data hub every 15 minutes so it

00:08:41,440 --> 00:08:48,600
depends or how often you want the data

00:08:43,450 --> 00:08:51,310
that step is called replication do but

00:08:48,600 --> 00:08:53,680
and then you would so once the data is

00:08:51,310 --> 00:08:56,410
available in the data warehouse you

00:08:53,680 --> 00:08:58,990
apply your ETL kind of logic you know

00:08:56,410 --> 00:09:03,490
and you would get your target data model

00:08:58,990 --> 00:09:05,589
but your end users would be put on so it

00:09:03,490 --> 00:09:08,529
implementation was fairly easy you would

00:09:05,589 --> 00:09:12,910
buy in some existing prepared

00:09:08,529 --> 00:09:16,480
proprietary stuff from some user support

00:09:12,910 --> 00:09:17,830
database provider complexities are

00:09:16,480 --> 00:09:22,240
abstracted you don't have to care about

00:09:17,830 --> 00:09:23,860
refreshing data really I mean and but

00:09:22,240 --> 00:09:26,170
then there are data overheads and of

00:09:23,860 --> 00:09:29,200
course there's latency you know because

00:09:26,170 --> 00:09:32,320
of the refreshing happening across the

00:09:29,200 --> 00:09:37,600
systems so so some of the business guys

00:09:32,320 --> 00:09:41,230
said I want data I want it now so so we

00:09:37,600 --> 00:09:45,010
had we still had the online systems so

00:09:41,230 --> 00:09:47,050
but then we had the message queues in

00:09:45,010 --> 00:09:49,540
place so the online systems would

00:09:47,050 --> 00:09:51,250
publish messages in the in the broker

00:09:49,540 --> 00:09:53,260
and we would have in the data west side

00:09:51,250 --> 00:09:56,170
we would have listeners which were

00:09:53,260 --> 00:09:56,790
written in stored procedures back in

00:09:56,170 --> 00:09:59,560
those days

00:09:56,790 --> 00:10:02,230
mm yeah so they would schedule this

00:09:59,560 --> 00:10:08,680
listeners and this would pull in data

00:10:02,230 --> 00:10:11,380
from from message brokers you know so

00:10:08,680 --> 00:10:14,200
the problem is you know if well in our

00:10:11,380 --> 00:10:17,910
case loss of messages because of the

00:10:14,200 --> 00:10:20,709
tooling that we used was not was

00:10:17,910 --> 00:10:21,970
messages by guaranteed but it it the

00:10:20,709 --> 00:10:23,230
thing is you can have in this

00:10:21,970 --> 00:10:26,080
architecture you can have lots of

00:10:23,230 --> 00:10:28,589
messages if databases are not really

00:10:26,080 --> 00:10:30,910
kind of made for this directly and

00:10:28,589 --> 00:10:33,279
implementation is complicated in fact in

00:10:30,910 --> 00:10:36,040
certain places they actually had absurd

00:10:33,279 --> 00:10:37,839
for every message coming in on the

00:10:36,040 --> 00:10:39,700
target table so that's an implementing

00:10:37,839 --> 00:10:41,560
it if you don't understand what you are

00:10:39,700 --> 00:10:43,120
doing it you can end up making really

00:10:41,560 --> 00:10:46,300
bad software

00:10:43,120 --> 00:10:49,510
in and then you have a nightmare for

00:10:46,300 --> 00:10:52,390
operations no because if message broker

00:10:49,510 --> 00:10:57,700
goes down you have a lot of different

00:10:52,390 --> 00:11:01,090
things happening so so that's it so that

00:10:57,700 --> 00:11:04,330
was okay no but when we move towards

00:11:01,090 --> 00:11:06,070
micro-services architecture the things

00:11:04,330 --> 00:11:09,580
just magnified all the problems

00:11:06,070 --> 00:11:11,230
magnified and what we saw was there were

00:11:09,580 --> 00:11:12,610
too many just too many sources to

00:11:11,230 --> 00:11:14,250
collect data form within the

00:11:12,610 --> 00:11:17,290
organization all the internal sources

00:11:14,250 --> 00:11:18,940
exploded it affects the stability of

00:11:17,290 --> 00:11:21,100
reports because you have too many

00:11:18,940 --> 00:11:25,960
dependencies to cadena collect the data

00:11:21,100 --> 00:11:28,660
form and now what we had was we had a

00:11:25,960 --> 00:11:31,450
team of seven or eight people now who

00:11:28,660 --> 00:11:34,000
they had to do collect data from three

00:11:31,450 --> 00:11:36,550
hundred services and just the BI team

00:11:34,000 --> 00:11:37,960
doesn't scale as much and then you have

00:11:36,550 --> 00:11:41,400
to on top with that you have to apply

00:11:37,960 --> 00:11:44,500
all these ETL log logic for most of the

00:11:41,400 --> 00:11:46,450
data and and then you have service

00:11:44,500 --> 00:11:51,490
concatenation that you have to do and it

00:11:46,450 --> 00:11:54,640
just is too much for a team of few

00:11:51,490 --> 00:11:57,610
people so at that point we decided to

00:11:54,640 --> 00:12:00,880
move to Hadoop because as the services

00:11:57,610 --> 00:12:02,170
were scaling the data was growing and we

00:12:00,880 --> 00:12:04,690
had to do something about it

00:12:02,170 --> 00:12:07,570
so you at Budokan we have we finally

00:12:04,690 --> 00:12:09,250
have a good experience with Hadoop we

00:12:07,570 --> 00:12:13,630
have been running it in production for

00:12:09,250 --> 00:12:15,730
2010 we have being we know how take what

00:12:13,630 --> 00:12:18,040
things can go wrong and we know how to

00:12:15,730 --> 00:12:20,110
fix it so some of the jobs like a

00:12:18,040 --> 00:12:23,230
commander was working it's really bad in

00:12:20,110 --> 00:12:24,760
terms of the amount of data processes so

00:12:23,230 --> 00:12:28,480
we know what kind of failures we can

00:12:24,760 --> 00:12:30,670
expect and also in that point we also

00:12:28,480 --> 00:12:32,920
started defining how our service looks

00:12:30,670 --> 00:12:37,930
like so at this point we made a decision

00:12:32,920 --> 00:12:42,130
of a conscious decision of how we can

00:12:37,930 --> 00:12:44,320
make bi even easier even though if we

00:12:42,130 --> 00:12:46,000
scale out from services things so we

00:12:44,320 --> 00:12:50,140
defined how a service should look like

00:12:46,000 --> 00:12:51,610
so a common thing in a service or micro

00:12:50,140 --> 00:12:54,310
service-oriented architecture is is

00:12:51,610 --> 00:12:56,499
having RPC over HTTP like a guess

00:12:54,310 --> 00:12:59,589
service or something and

00:12:56,499 --> 00:13:04,629
message queues but what we introduced

00:12:59,589 --> 00:13:06,969
was the idea of bulk interfaces so it is

00:13:04,629 --> 00:13:10,389
it is the philosophy behind it is that

00:13:06,969 --> 00:13:16,059
you should be able to transfer huge

00:13:10,389 --> 00:13:17,409
loads of data across services so so

00:13:16,059 --> 00:13:20,139
think of a scenario where you have to do

00:13:17,409 --> 00:13:22,239
an initial load on some service no you

00:13:20,139 --> 00:13:24,069
you don't really want to do that on

00:13:22,239 --> 00:13:26,889
we're using HTTP no you what should be

00:13:24,069 --> 00:13:31,029
able to do that directly to another way

00:13:26,889 --> 00:13:33,639
so that's what we decided to build so

00:13:31,029 --> 00:13:35,289
bulk interfaces are in our

00:13:33,639 --> 00:13:37,329
infrastructure it's basic it's a

00:13:35,289 --> 00:13:39,369
philosophy but in our infrastructure is

00:13:37,329 --> 00:13:41,979
basically an HBase table it's a

00:13:39,369 --> 00:13:48,309
key/value think so think about a think

00:13:41,979 --> 00:13:51,729
of it as a key it's a key value entity

00:13:48,309 --> 00:13:55,509
but with the key composition consists of

00:13:51,729 --> 00:13:58,059
a time component in it Center tower and

00:13:55,509 --> 00:14:01,539
the and functional key component in it

00:13:58,059 --> 00:14:05,619
so the time is prepended yeah time is

00:14:01,539 --> 00:14:07,809
prepended to the function key and when

00:14:05,619 --> 00:14:10,149
you replay all the events because it has

00:14:07,809 --> 00:14:12,489
time you know and you can scan certain

00:14:10,149 --> 00:14:14,079
you can have a sliding window in your

00:14:12,489 --> 00:14:17,249
data set and if you replay all the

00:14:14,079 --> 00:14:20,559
events you get the latest state of your

00:14:17,249 --> 00:14:23,199
of your data essentially so in this case

00:14:20,559 --> 00:14:25,269
if you replay the time at times and you

00:14:23,199 --> 00:14:27,429
can actually get the state of different

00:14:25,269 --> 00:14:30,099
way dodged a latest state of different

00:14:27,429 --> 00:14:33,399
wear assets it's also it's is fairly

00:14:30,099 --> 00:14:36,189
inspired by event sourcing pattern it's

00:14:33,399 --> 00:14:38,409
a design pattern essentially which does

00:14:36,189 --> 00:14:41,439
something like this and the key design

00:14:38,409 --> 00:14:43,599
is inspired by open TS DB open BSD B has

00:14:41,439 --> 00:14:46,449
a similar key design but it has a metric

00:14:43,599 --> 00:14:51,669
name in front of it something like that

00:14:46,449 --> 00:14:53,589
so we we used this kind of principle the

00:14:51,669 --> 00:14:55,989
idea and the second part is that the

00:14:53,589 --> 00:14:56,739
data here is kind of immutable so you

00:14:55,989 --> 00:15:01,049
just prepend

00:14:56,739 --> 00:15:04,809
data so in this way you yeah you can

00:15:01,049 --> 00:15:07,119
collect if you have seen a presentation

00:15:04,809 --> 00:15:08,919
about turning databases inside out it

00:15:07,119 --> 00:15:09,880
talks about it basically looks like a we

00:15:08,919 --> 00:15:13,870
do login

00:15:09,880 --> 00:15:17,259
databases are my sequel bin in the my

00:15:13,870 --> 00:15:22,209
sequel databases if so so that's the

00:15:17,259 --> 00:15:24,160
principle behind bulk interfaces so so

00:15:22,209 --> 00:15:26,050
once we have this the source systems

00:15:24,160 --> 00:15:29,230
this is the way services I cannot share

00:15:26,050 --> 00:15:34,509
data with us so we started thinking of

00:15:29,230 --> 00:15:38,079
how we can now we think about bi in in

00:15:34,509 --> 00:15:41,259
in Hadoop so so these are the essential

00:15:38,079 --> 00:15:44,290
steps so let's let's assume we have

00:15:41,259 --> 00:15:46,690
three services what because we have the

00:15:44,290 --> 00:15:49,149
data in this fashion we can do Delta

00:15:46,690 --> 00:15:53,740
processing easily so it's a it's a key

00:15:49,149 --> 00:15:56,199
concept and an ETL so you don't want to

00:15:53,740 --> 00:15:58,690
take the complete data set every time

00:15:56,199 --> 00:16:00,910
and and and processor so you want to

00:15:58,690 --> 00:16:04,990
have a sliding window where you go from

00:16:00,910 --> 00:16:07,029
time but do you want to t T X and you

00:16:04,990 --> 00:16:11,380
just want to only that slice of the data

00:16:07,029 --> 00:16:15,790
so we so the way we get this data is why

00:16:11,380 --> 00:16:18,009
accuse into the HBase tables and then so

00:16:15,790 --> 00:16:20,319
the services and Hassim has there is a

00:16:18,009 --> 00:16:21,759
mapping layer called tooling custom

00:16:20,319 --> 00:16:22,060
tooling within our infrastructure called

00:16:21,759 --> 00:16:24,670
Eddy

00:16:22,060 --> 00:16:27,339
so it takes so it so all the services

00:16:24,670 --> 00:16:30,370
actually send an XML data or adjacent

00:16:27,339 --> 00:16:32,470
data it can be any data format and Eddie

00:16:30,370 --> 00:16:35,019
translates it into an insert statement

00:16:32,470 --> 00:16:38,439
in HBase and puts it so teams do not

00:16:35,019 --> 00:16:41,500
have to deal with or with HPS directly

00:16:38,439 --> 00:16:43,420
so you can if you are actually doing it

00:16:41,500 --> 00:16:45,519
you can also use the yes service from

00:16:43,420 --> 00:16:47,410
from MySpace if you don't know do not

00:16:45,519 --> 00:16:50,589
want deal with the the intervals of

00:16:47,410 --> 00:16:52,750
HBase as a team so we insert the data

00:16:50,589 --> 00:16:54,370
into all the service and on all the

00:16:52,750 --> 00:16:59,170
service we have Delta processing

00:16:54,370 --> 00:17:01,720
happening and so what we use as we use

00:16:59,170 --> 00:17:03,790
we have big scripts written which which

00:17:01,720 --> 00:17:08,049
do Delta processing on on these Deek on

00:17:03,790 --> 00:17:10,419
these HBase tables and we have big this

00:17:08,049 --> 00:17:12,189
is fair and then we have a scoop job

00:17:10,419 --> 00:17:14,439
which inserts data into our data

00:17:12,189 --> 00:17:16,289
warehouse when into our models all the

00:17:14,439 --> 00:17:19,240
transformations are happening there and

00:17:16,289 --> 00:17:20,260
actually to be honest we do not do it

00:17:19,240 --> 00:17:23,230
any transformations

00:17:20,260 --> 00:17:23,620
so the whole idea of transformations of

00:17:23,230 --> 00:17:25,540
question

00:17:23,620 --> 00:17:29,230
back to the services because we are only

00:17:25,540 --> 00:17:30,910
stalking the states so the only so we

00:17:29,230 --> 00:17:32,680
don't we all we do is we do Delta

00:17:30,910 --> 00:17:34,690
processing on the data collect them

00:17:32,680 --> 00:17:37,360
aggregate them and then push it back to

00:17:34,690 --> 00:17:41,110
data warehouse so this is fairly the

00:17:37,360 --> 00:17:42,220
Commons architecture if you ask your

00:17:41,110 --> 00:17:43,690
people who have been doing data

00:17:42,220 --> 00:17:46,270
warehouse in Hadoop but there is a

00:17:43,690 --> 00:17:49,920
problem here right I mean no the problem

00:17:46,270 --> 00:17:52,600
is at the scoop and that's a bottleneck

00:17:49,920 --> 00:17:56,230
if you want to move towards more

00:17:52,600 --> 00:17:58,360
self-service bi it's it's it's fairly

00:17:56,230 --> 00:18:01,960
difficult to do that and kind of this

00:17:58,360 --> 00:18:03,220
kind of scenario so yeah so we have a

00:18:01,960 --> 00:18:05,980
tooling so we have one DAC for

00:18:03,220 --> 00:18:09,610
scheduling these jobs we have Chronicle

00:18:05,980 --> 00:18:13,270
for the database side yeah so uh so

00:18:09,610 --> 00:18:14,679
before I so first of all there are two

00:18:13,270 --> 00:18:16,420
problems here there's one that's

00:18:14,679 --> 00:18:19,179
basically the bottleneck that can happen

00:18:16,420 --> 00:18:22,630
at the scoop and because of its going to

00:18:19,179 --> 00:18:25,120
a single Amanat data data warehouse and

00:18:22,630 --> 00:18:28,929
the second part is that you have to

00:18:25,120 --> 00:18:31,330
guide big scripts for all these all

00:18:28,929 --> 00:18:34,650
these services that you have to collect

00:18:31,330 --> 00:18:37,660
data from so first we try to solve that

00:18:34,650 --> 00:18:42,340
basically what we did was we try to we

00:18:37,660 --> 00:18:43,809
invent a bi integration unit so the

00:18:42,340 --> 00:18:46,059
thing is once you push the

00:18:43,809 --> 00:18:48,850
transformation to the services all you

00:18:46,059 --> 00:18:51,040
have to do is do a very dumb thing where

00:18:48,850 --> 00:18:53,410
you have to take the events and we play

00:18:51,040 --> 00:18:55,120
all the events and just push the data so

00:18:53,410 --> 00:18:59,350
I mean this is a very common pattern

00:18:55,120 --> 00:19:03,429
across all the all the services so which

00:18:59,350 --> 00:19:04,690
we thought we can actually automate this

00:19:03,429 --> 00:19:06,760
thing now we shouldn't be writing

00:19:04,690 --> 00:19:09,309
pictures for all the all the sources so

00:19:06,760 --> 00:19:10,870
what we did was we thought what are the

00:19:09,309 --> 00:19:14,020
use case what are the things that we

00:19:10,870 --> 00:19:16,390
need one is we need an equation job for

00:19:14,020 --> 00:19:20,200
every service and we need a way to

00:19:16,390 --> 00:19:25,840
concatenate all the services so on one

00:19:20,200 --> 00:19:30,160
of many keys so we got we because I

00:19:25,840 --> 00:19:32,860
purely did not have the dev ops

00:19:30,160 --> 00:19:34,660
background but we took an inspiration

00:19:32,860 --> 00:19:36,730
from puppet so this is basically a

00:19:34,660 --> 00:19:38,770
puppet ice-t-- way of add

00:19:36,730 --> 00:19:41,080
service into the whole infrastructure so

00:19:38,770 --> 00:19:43,450
we we have an H base table we map it to

00:19:41,080 --> 00:19:46,090
an Oracle table and what this script

00:19:43,450 --> 00:19:48,809
essentially does we have a tooling which

00:19:46,090 --> 00:19:51,549
actually generates a big script all the

00:19:48,809 --> 00:19:54,340
monitoring I'll on that and and and

00:19:51,549 --> 00:19:56,740
scoop script and it it pushes data to

00:19:54,340 --> 00:20:00,160
the the warehouse so you basically also

00:19:56,740 --> 00:20:03,850
have types from from that's required so

00:20:00,160 --> 00:20:05,410
if something fails it gets notified and

00:20:03,850 --> 00:20:08,140
things like that so in this way it's

00:20:05,410 --> 00:20:09,760
pretty tightly chained with the whole

00:20:08,140 --> 00:20:11,919
infrastructure it's also get scheduled

00:20:09,760 --> 00:20:15,340
also you can also specify the schedule

00:20:11,919 --> 00:20:18,190
that you want and and so that's that's

00:20:15,340 --> 00:20:22,360
one thing no we didn't do not want to

00:20:18,190 --> 00:20:24,250
collect data from every service but big

00:20:22,360 --> 00:20:26,260
fiction so we started generating them so

00:20:24,250 --> 00:20:30,190
you would just plug your server sent to

00:20:26,260 --> 00:20:33,490
the data warehouse like this but then we

00:20:30,190 --> 00:20:36,160
still have now we have two problems one

00:20:33,490 --> 00:20:39,309
is the bottleneck yes the second one is

00:20:36,160 --> 00:20:42,040
that everything is batch so there is

00:20:39,309 --> 00:20:43,840
still latency you know there is still a

00:20:42,040 --> 00:20:46,090
certain amount of latency but because

00:20:43,840 --> 00:20:48,100
MapReduce for example is optimized for

00:20:46,090 --> 00:20:48,970
high throughput so every time you got

00:20:48,100 --> 00:20:51,850
the job

00:20:48,970 --> 00:20:54,480
it's it takes a lot of it takes certain

00:20:51,850 --> 00:20:57,130
amount of containers in in your cluster

00:20:54,480 --> 00:21:01,030
and and it depends upon the how much

00:20:57,130 --> 00:21:02,559
data you have but still you it's a it's

00:21:01,030 --> 00:21:06,340
optimized for high throughput you don't

00:21:02,559 --> 00:21:09,910
want to do that every every 15 minutes

00:21:06,340 --> 00:21:12,850
or every 2 minutes or you were you can

00:21:09,910 --> 00:21:17,020
how small can you make your batch no so

00:21:12,850 --> 00:21:19,630
that's that's the problem it's bad thing

00:21:17,020 --> 00:21:21,970
and then you have all these pipelines it

00:21:19,630 --> 00:21:24,580
takes time to get the data into the we

00:21:21,970 --> 00:21:27,669
put and since we are in e-commerce

00:21:24,580 --> 00:21:31,270
platform we we really want to be on top

00:21:27,669 --> 00:21:35,679
of the of the curve we really want to be

00:21:31,270 --> 00:21:40,210
half the data and analysis right away so

00:21:35,679 --> 00:21:42,960
we said we realized that actually data

00:21:40,210 --> 00:21:46,090
is a synchronous every data is as as

00:21:42,960 --> 00:21:49,059
almost every sources that we had in our

00:21:46,090 --> 00:21:50,559
system is a synchronous plates are

00:21:49,059 --> 00:21:52,509
synchronous orders

00:21:50,559 --> 00:21:55,990
are synchronous offers are synchronous

00:21:52,509 --> 00:21:59,950
so in fact a batch is is actually a

00:21:55,990 --> 00:22:01,659
stream know so its stream missus

00:21:59,950 --> 00:22:03,580
essentially you have a point star point

00:22:01,659 --> 00:22:07,740
and you have your endpoint is infinity

00:22:03,580 --> 00:22:12,490
and batch is more of a bounded stream so

00:22:07,740 --> 00:22:16,659
so we decided to move to the streaming a

00:22:12,490 --> 00:22:21,490
Ghana with flank and with flank what we

00:22:16,659 --> 00:22:24,820
had was the entry of bagger is low your

00:22:21,490 --> 00:22:26,950
code is smaller and and you have really

00:22:24,820 --> 00:22:29,259
nice java functional api's to do that

00:22:26,950 --> 00:22:30,970
and we kind of have operational

00:22:29,259 --> 00:22:33,429
experience now because we have been

00:22:30,970 --> 00:22:36,159
trying it out a lot so we decided to

00:22:33,429 --> 00:22:38,200
actually experiment with our things but

00:22:36,159 --> 00:22:41,139
one thing that we realized is that you

00:22:38,200 --> 00:22:43,690
don't really need cues for swimming or

00:22:41,139 --> 00:22:47,259
you don't always need accuse cues for

00:22:43,690 --> 00:22:48,759
streaming you can use actually you can

00:22:47,259 --> 00:22:50,919
use edge base tables for streaming to

00:22:48,759 --> 00:22:56,919
know its stream you can make streams out

00:22:50,919 --> 00:22:59,590
of files too so what we started thinking

00:22:56,919 --> 00:23:01,659
was so let's say we have a given because

00:22:59,590 --> 00:23:05,559
we have this immense source pattern so

00:23:01,659 --> 00:23:07,360
let's say we have a start time and then

00:23:05,559 --> 00:23:09,820
you can with HBase you can actually ask

00:23:07,360 --> 00:23:13,840
for give me next X records no it can be

00:23:09,820 --> 00:23:17,889
100 that can be 2,000 so you can ask for

00:23:13,840 --> 00:23:21,159
and with what would by to give me X

00:23:17,889 --> 00:23:24,399
records you keep getting a small batches

00:23:21,159 --> 00:23:27,610
of it costs back and your in this way

00:23:24,399 --> 00:23:29,019
you can build a stream out of it the

00:23:27,610 --> 00:23:31,929
other way to do that is basically you

00:23:29,019 --> 00:23:33,999
say you give start to an end or to your

00:23:31,929 --> 00:23:37,149
so you know in your sourcing component

00:23:33,999 --> 00:23:38,529
and you can ask for give me a start time

00:23:37,149 --> 00:23:39,970
and time give me all the great cost from

00:23:38,529 --> 00:23:42,399
this thing so in this way you can

00:23:39,970 --> 00:23:45,970
actually stream your edge base tables

00:23:42,399 --> 00:23:49,269
all of a sudden so so let's take an

00:23:45,970 --> 00:23:51,700
example for example we have offers and

00:23:49,269 --> 00:23:55,119
we have product catalog so you want to

00:23:51,700 --> 00:23:56,830
see let's say for product category X how

00:23:55,119 --> 00:23:59,919
many offers are there or what is the

00:23:56,830 --> 00:24:02,590
best offer for this product with some

00:23:59,919 --> 00:24:04,469
description so what you would do is for

00:24:02,590 --> 00:24:06,999
given product ID

00:24:04,469 --> 00:24:08,979
you would start streaming them so

00:24:06,999 --> 00:24:14,589
because the Buddhist teams are

00:24:08,979 --> 00:24:16,599
independent of each other also offers is

00:24:14,589 --> 00:24:19,839
calculating best offer for this Product

00:24:16,599 --> 00:24:23,289
ID while credit cat years is kind of a

00:24:19,839 --> 00:24:26,229
slow because it's a lot of data so it it

00:24:23,289 --> 00:24:30,549
eventually enters and on the on the we

00:24:26,229 --> 00:24:34,179
have a sink where we have the table has

00:24:30,549 --> 00:24:37,929
Kia's Product ID and everything else so

00:24:34,179 --> 00:24:40,989
it looks like a star schema so you can

00:24:37,929 --> 00:24:43,869
add more things with product IDs key so

00:24:40,989 --> 00:24:47,649
it kind of becomes like a star schema

00:24:43,869 --> 00:24:50,200
and and it's an H base table and you

00:24:47,649 --> 00:24:52,089
apply platform which is a new tooling

00:24:50,200 --> 00:24:55,899
that we are using for self-service bi on

00:24:52,089 --> 00:24:59,799
on HBase so it kind of you can actually

00:24:55,899 --> 00:25:02,889
start using this stock on platform right

00:24:59,799 --> 00:25:05,229
away so we have been thinking how we can

00:25:02,889 --> 00:25:07,589
automate this and what we ended up doing

00:25:05,229 --> 00:25:10,389
is a wrapper around

00:25:07,589 --> 00:25:14,440
flink so you can actually say from this

00:25:10,389 --> 00:25:17,019
table on certain key you can and you can

00:25:14,440 --> 00:25:20,739
look up while you are streaming and you

00:25:17,019 --> 00:25:24,219
sink to certain multiple tables so

00:25:20,739 --> 00:25:27,879
that's that's one of the things that we

00:25:24,219 --> 00:25:31,899
did and finally we have been because

00:25:27,879 --> 00:25:35,259
flink is a streaming application we do

00:25:31,899 --> 00:25:37,749
not deploy it like Hadoop so what we do

00:25:35,259 --> 00:25:40,690
is we you know a maven

00:25:37,749 --> 00:25:43,089
we have built a maven project where

00:25:40,690 --> 00:25:46,029
which it builds into a docker image and

00:25:43,089 --> 00:25:48,820
we push it to a docker registry and

00:25:46,029 --> 00:25:51,129
while deploying it pulls in the docker

00:25:48,820 --> 00:25:53,259
the latest or the version of docker

00:25:51,129 --> 00:25:56,489
image we tag it with the the version

00:25:53,259 --> 00:26:00,519
number and it pulls it and pushes to

00:25:56,489 --> 00:26:02,859
mesos and starts the flink client on on

00:26:00,519 --> 00:26:06,669
Messer's and submits the job and in

00:26:02,859 --> 00:26:11,619
hadoop yarn and yeah so one thing that

00:26:06,669 --> 00:26:13,629
we actually learned I was basically you

00:26:11,619 --> 00:26:17,200
do need a dedicated team for doing

00:26:13,629 --> 00:26:18,130
innovations of these sorts because it it

00:26:17,200 --> 00:26:20,650
takes a four

00:26:18,130 --> 00:26:22,720
come team is too much to do these things

00:26:20,650 --> 00:26:24,790
and the one thing you have to think

00:26:22,720 --> 00:26:27,250
about this is not tools but how to solve

00:26:24,790 --> 00:26:31,390
problems it's it's kind of a hipster way

00:26:27,250 --> 00:26:35,260
of solving bi problem but and the other

00:26:31,390 --> 00:26:38,110
thing that we saw was Flint can be

00:26:35,260 --> 00:26:40,780
flinky because we you have to figure out

00:26:38,110 --> 00:26:42,520
a lot of things yourself and and there

00:26:40,780 --> 00:26:44,800
are a lot of issues well if you have

00:26:42,520 --> 00:26:48,130
attended nice presentation there's an

00:26:44,800 --> 00:26:49,420
issue with Kerberos ticket and stuff and

00:26:48,130 --> 00:26:52,360
there's a lot of frameworks out there

00:26:49,420 --> 00:26:55,660
and when we started doing this bi thing

00:26:52,360 --> 00:26:58,780
on Hadoop well we there was there was no

00:26:55,660 --> 00:27:03,930
khylin back in those days around 2013 to

00:26:58,780 --> 00:27:07,120
2014 so khylin is actually a new project

00:27:03,930 --> 00:27:09,280
which which i think is if you can start

00:27:07,120 --> 00:27:11,830
with that you should definitely look at

00:27:09,280 --> 00:27:14,380
it and and yeah and also like a lot of

00:27:11,830 --> 00:27:16,000
VI and hadoop developers don't think

00:27:14,380 --> 00:27:18,190
about infrastructure they take it for

00:27:16,000 --> 00:27:21,460
guarantee I mean I would say you expect

00:27:18,190 --> 00:27:24,250
the infrastructure for ado and yeah and

00:27:21,460 --> 00:27:33,940
that's pretty much it yeah thank you

00:27:24,250 --> 00:27:34,750
thank you very much okay we do have time

00:27:33,940 --> 00:27:40,290
for questions

00:27:34,750 --> 00:27:40,290
does anybody have any burning question

00:27:49,450 --> 00:27:54,190
could you give me a sense of like the so

00:27:52,990 --> 00:27:56,710
you sat back in the beginning the

00:27:54,190 --> 00:27:58,960
problem is that you have batches and the

00:27:56,710 --> 00:28:02,559
batches can be very large right so there

00:27:58,960 --> 00:28:05,590
is a big delay proportionally so just

00:28:02,559 --> 00:28:08,200
just like orders of magnitude you

00:28:05,590 --> 00:28:10,059
managed to like improve this delay like

00:28:08,200 --> 00:28:13,120
how much quick because the pipeline

00:28:10,059 --> 00:28:15,850
seems a lot more complicated yeah but is

00:28:13,120 --> 00:28:17,500
it is it like at which point is it worth

00:28:15,850 --> 00:28:19,480
it that's amazing which dealer you're

00:28:17,500 --> 00:28:21,580
talking about I mean well you said in

00:28:19,480 --> 00:28:24,669
the beginning that like from your from

00:28:21,580 --> 00:28:26,139
your offer or from your application to

00:28:24,669 --> 00:28:28,179
the data warehouse they were like two

00:28:26,139 --> 00:28:31,269
batches yes yes yeah which take a long

00:28:28,179 --> 00:28:33,070
time but now you kind of removed them so

00:28:31,269 --> 00:28:37,240
like how much quicker is the data flow

00:28:33,070 --> 00:28:40,240
so for example like in previously

00:28:37,240 --> 00:28:42,250
previously before I joined teamed there

00:28:40,240 --> 00:28:44,679
with certain jobs that did self join

00:28:42,250 --> 00:28:47,110
with themselves and to calculate the

00:28:44,679 --> 00:28:49,870
best offers and they were really bad no

00:28:47,110 --> 00:28:51,429
some of the times if the offers would do

00:28:49,870 --> 00:28:54,850
an initial load they would take a day to

00:28:51,429 --> 00:28:56,710
process and and with these it's it's it

00:28:54,850 --> 00:29:05,980
would definitely finish within the Nog

00:28:56,710 --> 00:29:08,139
or something no I would like to ask how

00:29:05,980 --> 00:29:10,830
did you manage the dependency while

00:29:08,139 --> 00:29:15,419
moving through microservices what

00:29:10,830 --> 00:29:18,490
dependency with other so are these yes I

00:29:15,419 --> 00:29:20,679
mean we haven't moved completely and and

00:29:18,490 --> 00:29:24,490
that's always a discussion it's actually

00:29:20,679 --> 00:29:27,700
an organizational problem then the thing

00:29:24,490 --> 00:29:29,919
so I mean the way they they do it as you

00:29:27,700 --> 00:29:31,419
look at the common dependencies together

00:29:29,919 --> 00:29:33,250
and you try to make a service out of it

00:29:31,419 --> 00:29:36,100
so that it becomes an independent

00:29:33,250 --> 00:29:38,409
deployable unit on on your

00:29:36,100 --> 00:29:41,730
infrastructure but I mean it's it's

00:29:38,409 --> 00:29:44,889
always a discussion I mean what is the

00:29:41,730 --> 00:29:46,240
how do you take that out of that so that

00:29:44,889 --> 00:29:48,789
I don't know if that answered your

00:29:46,240 --> 00:29:50,470
question so so that's that's how we do

00:29:48,789 --> 00:29:52,510
it now we try to figure out how we can

00:29:50,470 --> 00:29:53,919
independently deploy certain part of

00:29:52,510 --> 00:29:56,289
application for example you have to

00:29:53,919 --> 00:29:58,600
check out a fulfillment or something so

00:29:56,289 --> 00:30:01,360
what are the services required for

00:29:58,600 --> 00:30:03,640
for certain things and you deploy it

00:30:01,360 --> 00:30:12,640
that way so that you can continuously

00:30:03,640 --> 00:30:14,980
deploy the application so are you using

00:30:12,640 --> 00:30:18,190
it in production for historical data or

00:30:14,980 --> 00:30:21,549
just for for kind of very recent data

00:30:18,190 --> 00:30:24,460
like one month yeah we have historical

00:30:21,549 --> 00:30:26,700
data okay so if you are using it for

00:30:24,460 --> 00:30:29,890
historical data how do you want though

00:30:26,700 --> 00:30:31,720
later ieaving facts and later arrives or

00:30:29,890 --> 00:30:34,360
emissions so it eventually becomes

00:30:31,720 --> 00:30:37,600
consistent and if you look at the HBS

00:30:34,360 --> 00:30:41,200
kind of with the plink architecture it

00:30:37,600 --> 00:30:44,020
eventually becomes consistent so it's so

00:30:41,200 --> 00:30:45,870
that's that's how we solve it I don't

00:30:44,020 --> 00:30:48,429
know if that answered your question

00:30:45,870 --> 00:30:50,260
yes it does so it means that at the

00:30:48,429 --> 00:31:01,900
point you have to fully represents the

00:30:50,260 --> 00:31:03,850
entire data yeah anybody else you're

00:31:01,900 --> 00:31:06,700
using missiles in one of the last slides

00:31:03,850 --> 00:31:10,690
on which scale for some services or

00:31:06,700 --> 00:31:13,270
generally so I'm not sure if I can give

00:31:10,690 --> 00:31:14,650
you a complete idea but I mean I think

00:31:13,270 --> 00:31:17,530
it's around is we are going to

00:31:14,650 --> 00:31:19,299
production with meso soon so and the

00:31:17,530 --> 00:31:22,720
scale is around it's a small scale at

00:31:19,299 --> 00:31:25,630
Lake around five to ten machines but I

00:31:22,720 --> 00:31:34,090
mean we have it's going to be part of

00:31:25,630 --> 00:31:37,059
our of our core infrastructure yeah okay

00:31:34,090 --> 00:31:39,690
thank you very much let's thank the

00:31:37,059 --> 00:31:39,690

YouTube URL: https://www.youtube.com/watch?v=0FT8EB9gQoA


