Title: Berlin Buzzwords 2016: Michael Noll - Introducing Kafka Streams, new stream processing library ...
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	In the past few years Apache Kafka has established itself as the world's most popular real-time, large-scale messaging system. It is used across a wide range of industries by thousands of companies such as Netflix, Cisco, PayPal, Twitter, and many others.

In this session I am introducing the audience to Kafka Streams, which is the latest addition to the Apache Kafka project. Kafka Streams is a stream processing library natively integrated with Kafka. It has a very low barrier to entry, easy operationalization, and a high-level DSL for writing stream processing applications. As such it is the most convenient yet scalable option to process and analyze data that is backed by Kafka.  

We will provide the audience with an overview of Kafka Streams including its design and API, typical use cases, code examples, and an outlook of its upcoming roadmap. We will also compare Kafka Streams' light-weight library approach with heavier, framework-based tools such as Apache Storm and Spark Streaming, which require you to understand and operate a whole different infrastructure for processing real-time data in Kafka.

Read more:
https://2016.berlinbuzzwords.de/session/introducing-kafka-streams-new-stream-processing-library-apache-kafka

About Michael Noll:
https://2016.berlinbuzzwords.de/users/michael-noll

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,990 --> 00:00:07,140
hey thank you

00:00:05,220 --> 00:00:08,820
thanks for joining I know it's been a

00:00:07,140 --> 00:00:11,130
very hot day at a very cool conference

00:00:08,820 --> 00:00:13,410
and you're close to the end of the first

00:00:11,130 --> 00:00:15,870
day so thanks for attending this session

00:00:13,410 --> 00:00:19,260
is about Apache Kafka and Kafka streams

00:00:15,870 --> 00:00:21,150
which is part of Apache Kafka my name is

00:00:19,260 --> 00:00:23,420
Michael naal I am an engineer turned

00:00:21,150 --> 00:00:27,300
product manager working for confluent

00:00:23,420 --> 00:00:29,400
confluent is a US based company funded

00:00:27,300 --> 00:00:30,869
by the creators of Apache Kafka we are

00:00:29,400 --> 00:00:33,450
headquartered in Palo Alto California

00:00:30,869 --> 00:00:35,700
and we're building a stream data

00:00:33,450 --> 00:00:38,399
platform at the heart of which is Apache

00:00:35,700 --> 00:00:41,070
kafka since we started about one and a

00:00:38,399 --> 00:00:42,690
half years ago we attracted the great

00:00:41,070 --> 00:00:45,539
majority of kafka committers to join our

00:00:42,690 --> 00:00:48,420
team and we are working on improving

00:00:45,539 --> 00:00:51,570
Apache kafka and introducing kafka

00:00:48,420 --> 00:00:54,510
streams of course and also to build the

00:00:51,570 --> 00:00:57,989
larger confluence platform so personally

00:00:54,510 --> 00:01:00,059
I am native German based in Switzerland

00:00:57,989 --> 00:01:01,500
not in California so I'm actually quite

00:01:00,059 --> 00:01:03,719
happy that I have this opportunity to

00:01:01,500 --> 00:01:05,280
come to my home country to Berlin

00:01:03,719 --> 00:01:07,530
Bosphorus and speak with all of you

00:01:05,280 --> 00:01:10,290
about interesting technology so thanks

00:01:07,530 --> 00:01:12,750
for having me now before we start let me

00:01:10,290 --> 00:01:15,450
also get to know you a little bit so let

00:01:12,750 --> 00:01:17,549
me ask the question who of you has heard

00:01:15,450 --> 00:01:20,490
of Apache Kafka before if you have just

00:01:17,549 --> 00:01:23,430
raised your hand well that's I think

00:01:20,490 --> 00:01:26,490
basically everyone who if you are using

00:01:23,430 --> 00:01:27,630
Apache kafka in production whatever the

00:01:26,490 --> 00:01:31,979
different different of production is

00:01:27,630 --> 00:01:36,479
it's 60 70 percent that's pretty cool ok

00:01:31,979 --> 00:01:39,210
so still there are some of you that some

00:01:36,479 --> 00:01:40,590
of you that don't really know yet what

00:01:39,210 --> 00:01:41,970
the Apache kafka is and what Kefka

00:01:40,590 --> 00:01:43,079
streams is so let me start with an

00:01:41,970 --> 00:01:44,970
analogies that we are all on the same

00:01:43,079 --> 00:01:48,840
page and since we're in Berlin where we

00:01:44,970 --> 00:01:51,119
have so many critical phase I want to

00:01:48,840 --> 00:01:52,409
use proving delicious espresso as an

00:01:51,119 --> 00:01:54,540
analogy so imagine you have a

00:01:52,409 --> 00:01:57,149
data-driven product and that product

00:01:54,540 --> 00:02:01,740
would be a very create a sprayer so what

00:01:57,149 --> 00:02:03,509
would you need to prove that coffee so

00:02:01,740 --> 00:02:06,509
if you're at home what you would need is

00:02:03,509 --> 00:02:08,429
two things you need water which is a key

00:02:06,509 --> 00:02:10,470
ingredient for a good coffee and you

00:02:08,429 --> 00:02:12,480
need to get the water to your home the

00:02:10,470 --> 00:02:13,560
water pipes so Kafka would be these

00:02:12,480 --> 00:02:15,150
water pipes

00:02:13,560 --> 00:02:17,280
what's important is that this

00:02:15,150 --> 00:02:19,290
infrastructure is reliable there is a

00:02:17,280 --> 00:02:22,290
continuous stream of the water available

00:02:19,290 --> 00:02:24,870
to you because if it isn't you can't for

00:02:22,290 --> 00:02:26,580
coffee and there are further challenges

00:02:24,870 --> 00:02:31,319
like you know you have some problems to

00:02:26,580 --> 00:02:34,080
go to the restroom but all of that is

00:02:31,319 --> 00:02:35,790
not sufficient for prune your coffee so

00:02:34,080 --> 00:02:37,800
you need something that turns to water

00:02:35,790 --> 00:02:40,080
and some chronic coffee into your

00:02:37,800 --> 00:02:42,330
espresso so we need a coffee machine for

00:02:40,080 --> 00:02:45,330
that and kafka streams is the equivalent

00:02:42,330 --> 00:02:46,530
of a coffee machine in that analogy now

00:02:45,330 --> 00:02:48,690
once you have all these things in place

00:02:46,530 --> 00:02:50,280
you have the water you have convenient

00:02:48,690 --> 00:02:52,890
access to the water in a reliable way

00:02:50,280 --> 00:02:55,140
there oh no you know water leaks and so

00:02:52,890 --> 00:02:56,910
on and you have a very good coffee

00:02:55,140 --> 00:03:02,250
machine then you can create a great

00:02:56,910 --> 00:03:05,310
product and have happy users now before

00:03:02,250 --> 00:03:07,560
we talk about kafka streams let me take

00:03:05,310 --> 00:03:10,290
a step back and revisit what the current

00:03:07,560 --> 00:03:13,019
experience is if you venture into the

00:03:10,290 --> 00:03:15,150
realm of stream processing so that is

00:03:13,019 --> 00:03:17,160
before kafka streams and of course I'm

00:03:15,150 --> 00:03:21,239
somewhat exaggerating to try for my

00:03:17,160 --> 00:03:23,250
point but not all that much so if you

00:03:21,239 --> 00:03:25,769
decide that my problem is processing

00:03:23,250 --> 00:03:28,590
data let me pick a stream processing

00:03:25,769 --> 00:03:30,989
tool and then I'm not good the reality

00:03:28,590 --> 00:03:32,640
is a bit different any one of you has

00:03:30,989 --> 00:03:33,870
really worked with that in production

00:03:32,640 --> 00:03:38,130
you know this is not a frictionless

00:03:33,870 --> 00:03:40,829
experience and figuratively speaking the

00:03:38,130 --> 00:03:43,049
question that I wanna ask is how did

00:03:40,829 --> 00:03:46,260
something that is as simple as this if

00:03:43,049 --> 00:03:48,389
you're running on a single machine turn

00:03:46,260 --> 00:03:51,239
into something like that even if more

00:03:48,389 --> 00:03:54,180
than one machine so I'll not be talking

00:03:51,239 --> 00:03:55,950
about lambda architecture or going like

00:03:54,180 --> 00:04:00,389
through all these things that are good

00:03:55,950 --> 00:04:02,700
and bad my point is there is this

00:04:00,389 --> 00:04:04,590
tremendous discrepancy between how we

00:04:02,700 --> 00:04:09,900
would like to work and how we actually

00:04:04,590 --> 00:04:12,450
do work in practice so when you're

00:04:09,900 --> 00:04:14,010
setting out to rethink how you can do

00:04:12,450 --> 00:04:15,720
this rehearsal or when you set out to

00:04:14,010 --> 00:04:17,700
evaluate existing stream processing

00:04:15,720 --> 00:04:20,519
tools you should ask yourself the

00:04:17,700 --> 00:04:24,510
question is the tool part of the

00:04:20,519 --> 00:04:26,490
solution or part of the problem or is it

00:04:24,510 --> 00:04:27,419
actually adding another problem to the

00:04:26,490 --> 00:04:31,050
ones that you're already

00:04:27,419 --> 00:04:32,610
and I think if you want to build a tool

00:04:31,050 --> 00:04:34,560
that is part of the solution a good

00:04:32,610 --> 00:04:37,620
strategy is to make complex things

00:04:34,560 --> 00:04:42,449
simple and eventually thereby easy and

00:04:37,620 --> 00:04:44,669
fun now what do we gain if we follow

00:04:42,449 --> 00:04:46,469
that strategy one of the games is

00:04:44,669 --> 00:04:49,229
developer efficiency and that sounds a

00:04:46,469 --> 00:04:50,099
bit like a management password so let me

00:04:49,229 --> 00:04:53,039
rephrase that

00:04:50,099 --> 00:04:56,550
what we gain is that we scale the UN

00:04:53,039 --> 00:05:00,469
site like us our teams because we don't

00:04:56,550 --> 00:05:04,379
get better every 18 months and I think

00:05:00,469 --> 00:05:06,990
Apache Kafka has done a great job of

00:05:04,379 --> 00:05:08,849
providing a solution for data messaging

00:05:06,990 --> 00:05:10,710
transporting the data getting it from A

00:05:08,849 --> 00:05:12,509
to B and so on you've heard a couple of

00:05:10,710 --> 00:05:14,969
tours already today that we're making

00:05:12,509 --> 00:05:18,840
reference to Kafka as a key part of

00:05:14,969 --> 00:05:21,060
infrastructure now we wanted to do the

00:05:18,840 --> 00:05:23,060
same thing for stream processing with

00:05:21,060 --> 00:05:25,110
Kafka streams so following the same

00:05:23,060 --> 00:05:28,379
principle of making complex things

00:05:25,110 --> 00:05:31,229
simple easy and fun now what is Kafka

00:05:28,379 --> 00:05:34,110
streams Kafka streams is a powerful yet

00:05:31,229 --> 00:05:37,110
easy to use Java library it's part of

00:05:34,110 --> 00:05:38,930
open source Kafka and the source code is

00:05:37,110 --> 00:05:41,190
available under the Apache cover project

00:05:38,930 --> 00:05:43,080
using Kafka streams you can build your

00:05:41,190 --> 00:05:45,900
own applications that are thanks to

00:05:43,080 --> 00:05:48,779
Kafka and Kafka streams highly scalable

00:05:45,900 --> 00:05:52,909
follow tolerant stateful and so on we

00:05:48,779 --> 00:05:55,379
talked about that in more detail now

00:05:52,909 --> 00:05:58,949
before we take a closer look I want to

00:05:55,379 --> 00:06:00,569
give you the bigger picture if you have

00:05:58,949 --> 00:06:01,680
Kafka in the center of the

00:06:00,569 --> 00:06:03,060
infrastructure like your central nervous

00:06:01,680 --> 00:06:05,729
system for all your data

00:06:03,060 --> 00:06:08,759
Kafka streams is the part that reads

00:06:05,729 --> 00:06:11,490
data from Kafka transforms it and adds

00:06:08,759 --> 00:06:13,229
value to that data and writes the result

00:06:11,490 --> 00:06:14,909
back to Kafka and then there are other

00:06:13,229 --> 00:06:16,680
parts of your infrastructure that get

00:06:14,909 --> 00:06:19,229
the data into Kafka and out of Kafka

00:06:16,680 --> 00:06:24,900
such as Kafka Connect which is also part

00:06:19,229 --> 00:06:29,189
of the Apache Kafka project so again the

00:06:24,900 --> 00:06:30,569
coffee machine that is Kafka streams now

00:06:29,189 --> 00:06:31,680
what that picture might be good for

00:06:30,569 --> 00:06:34,979
people that are already familiar with

00:06:31,680 --> 00:06:37,560
Kafka some of you might not be that

00:06:34,979 --> 00:06:41,169
familiar yet so let me use a UNIX

00:06:37,560 --> 00:06:45,219
analogy if you have a command pipeline

00:06:41,169 --> 00:06:49,090
Kafka would be the pipes Kafka's dreams

00:06:45,219 --> 00:06:50,830
would be the commands and cough connect

00:06:49,090 --> 00:06:52,719
or the producer and client applications

00:06:50,830 --> 00:06:54,159
that you can build would be the parts

00:06:52,719 --> 00:06:58,090
that get data into and out of the

00:06:54,159 --> 00:06:59,349
pipeline the last an energy I want to

00:06:58,090 --> 00:07:00,629
make is for those people that are more

00:06:59,349 --> 00:07:04,120
familiar with Java

00:07:00,629 --> 00:07:07,270
so when Java was released in its initial

00:07:04,120 --> 00:07:09,430
version in 1996 machines typically had

00:07:07,270 --> 00:07:12,189
just a single core so that's when you

00:07:09,430 --> 00:07:14,439
have things like Java dot Lang some

00:07:12,189 --> 00:07:16,509
years later mighty core machines were a

00:07:14,439 --> 00:07:18,509
much more common at which point things

00:07:16,509 --> 00:07:21,219
like Java you to concurrently introduced

00:07:18,509 --> 00:07:22,659
now today if you have multi-core

00:07:21,219 --> 00:07:25,300
machines and you run applications that

00:07:22,659 --> 00:07:27,719
spend Multi machines but unfortunately

00:07:25,300 --> 00:07:30,339
there is no Java distributed but thanks

00:07:27,719 --> 00:07:37,419
we have Kafka's dreams that will fill

00:07:30,339 --> 00:07:40,089
that gap for you so before we take the

00:07:37,419 --> 00:07:41,650
extra closer look on Kafka's dreams let

00:07:40,089 --> 00:07:45,460
me talk a little bit about when you

00:07:41,650 --> 00:07:47,020
would like to use it so things that you

00:07:45,460 --> 00:07:49,770
would like to use it for is really

00:07:47,020 --> 00:07:51,699
building stream processing applications

00:07:49,770 --> 00:07:54,310
today there are already some reference

00:07:51,699 --> 00:07:57,250
to fast data applications so things that

00:07:54,310 --> 00:07:59,289
create continuous stream of data and do

00:07:57,250 --> 00:07:59,770
Tonkinese continuous transformation to

00:07:59,289 --> 00:08:01,240
it

00:07:59,770 --> 00:08:04,029
things like reactive and stateful

00:08:01,240 --> 00:08:05,649
applications and so on things where you

00:08:04,029 --> 00:08:07,629
might look elsewhere at least at this

00:08:05,649 --> 00:08:08,139
point is everything that is like heavy

00:08:07,629 --> 00:08:12,029
lifting

00:08:08,139 --> 00:08:12,029
data mining machine learning and so on

00:08:12,599 --> 00:08:17,919
now here yeah car shut up just show me

00:08:15,580 --> 00:08:21,129
some code right okay so in Kafka streams

00:08:17,919 --> 00:08:23,469
you have two options the first API is a

00:08:21,129 --> 00:08:24,849
DSL that allows you to express your

00:08:23,469 --> 00:08:27,039
programming logic in a declarative way

00:08:24,849 --> 00:08:29,110
that looks pretty much like Scala and

00:08:27,039 --> 00:08:31,029
that would actually be the equivalent of

00:08:29,110 --> 00:08:35,589
the scale exam that I showed at the very

00:08:31,029 --> 00:08:37,479
beginning the second option is a lower

00:08:35,589 --> 00:08:41,709
level processor API which looks a bit

00:08:37,479 --> 00:08:43,630
more like imperative programming so you

00:08:41,709 --> 00:08:46,269
have options to you know start up your

00:08:43,630 --> 00:08:48,519
processors you can tell it how to

00:08:46,269 --> 00:08:51,790
process every incoming record you can

00:08:48,519 --> 00:08:55,030
schedule periodic activities and you can

00:08:51,790 --> 00:08:58,070
specify what happens on shutdown

00:08:55,030 --> 00:09:00,230
so that's a quick look at the API but

00:08:58,070 --> 00:09:02,240
actually you will not hear me talking a

00:09:00,230 --> 00:09:05,630
lot about the API and the reason for

00:09:02,240 --> 00:09:08,000
that is I think the API is just a tiny

00:09:05,630 --> 00:09:09,320
part of what is important when you're

00:09:08,000 --> 00:09:11,330
picking a tool such as a stream

00:09:09,320 --> 00:09:13,250
processing tool there is a whole lot

00:09:11,330 --> 00:09:15,200
that happens underneath underneath the

00:09:13,250 --> 00:09:16,940
surface that is very very important and

00:09:15,200 --> 00:09:20,090
actually much more important than what

00:09:16,940 --> 00:09:22,520
you see as a developer for example

00:09:20,090 --> 00:09:23,930
operations so when you look at a tool or

00:09:22,520 --> 00:09:26,030
when you set out to design such a tool

00:09:23,930 --> 00:09:28,010
you want to do a full stack evaluation

00:09:26,030 --> 00:09:30,290
of everything that will happen what is

00:09:28,010 --> 00:09:33,890
the benefit that you're getting and at

00:09:30,290 --> 00:09:36,590
which cause does this benefit come so in

00:09:33,890 --> 00:09:39,740
kafka streams just like in Kafka we

00:09:36,590 --> 00:09:42,050
wanted to reduce everything that lurks

00:09:39,740 --> 00:09:45,290
underneath the surface as much as we can

00:09:42,050 --> 00:09:49,430
and we're doing this by making complex

00:09:45,290 --> 00:09:51,680
things simple easy and fun but how do we

00:09:49,430 --> 00:09:53,750
actually do this at a high level and the

00:09:51,680 --> 00:09:56,000
answer is we are outsourcing hard

00:09:53,750 --> 00:09:59,780
problems to Kafka they're already solved

00:09:56,000 --> 00:10:02,000
there why should we reinvent that now at

00:09:59,780 --> 00:10:04,040
this point you know typically see like

00:10:02,000 --> 00:10:05,690
people like raising their eyebrows so

00:10:04,040 --> 00:10:08,120
now okay you know how tell me how do I

00:10:05,690 --> 00:10:10,160
install this thing said well you don't

00:10:08,120 --> 00:10:11,420
install Kafka streams and there should

00:10:10,160 --> 00:10:13,850
be no installation it's a Java library

00:10:11,420 --> 00:10:17,090
edit reallocation just like any other

00:10:13,850 --> 00:10:20,000
library so then people typically have a

00:10:17,090 --> 00:10:22,550
pause and they stirred me given this

00:10:20,000 --> 00:10:29,870
suspicious look an Aspie all right where

00:10:22,550 --> 00:10:31,070
is the cluster and the answer is I think

00:10:29,870 --> 00:10:33,290
some of you might already expect the

00:10:31,070 --> 00:10:35,960
answer there is no cluster if you don't

00:10:33,290 --> 00:10:39,410
mean the cluster why should you since

00:10:35,960 --> 00:10:41,860
when did we have to use the cluster when

00:10:39,410 --> 00:10:44,240
we set out to do cool things with data

00:10:41,860 --> 00:10:45,830
so Kafka streams allows you to stay lean

00:10:44,240 --> 00:10:47,650
and lightweight if you want to and go

00:10:45,830 --> 00:10:50,930
all out with clustering if you want to

00:10:47,650 --> 00:10:52,940
now the next question then is okay how

00:10:50,930 --> 00:10:55,910
do I parent deploy my application so how

00:10:52,940 --> 00:10:58,310
do I XYZ and I think the reason why

00:10:55,910 --> 00:11:01,400
people ask that question is if you're

00:10:58,310 --> 00:11:03,020
working on data in your company your

00:11:01,400 --> 00:11:04,820
typical like the guy in the yellow shirt

00:11:03,020 --> 00:11:06,500
who's the social order or the

00:11:04,820 --> 00:11:07,640
party-pooper who does things differently

00:11:06,500 --> 00:11:09,440
than all the

00:11:07,640 --> 00:11:11,420
of the organization you have a different

00:11:09,440 --> 00:11:13,580
rate to run your applications to

00:11:11,420 --> 00:11:15,530
interface with applications and you

00:11:13,580 --> 00:11:17,450
actually have a hard time making a case

00:11:15,530 --> 00:11:19,600
for why people should go for whatever

00:11:17,450 --> 00:11:22,160
tool you're suggesting

00:11:19,600 --> 00:11:23,870
so with Kafka streams what we wanted to

00:11:22,160 --> 00:11:26,510
do is give you the option to do whatever

00:11:23,870 --> 00:11:27,710
you want to and we feel it's very

00:11:26,510 --> 00:11:29,300
important that the stream processing

00:11:27,710 --> 00:11:31,460
tool should not be opinionated and

00:11:29,300 --> 00:11:34,580
dictate rules how you package it how

00:11:31,460 --> 00:11:36,710
deploy it and so on so that means you

00:11:34,580 --> 00:11:38,930
can look at this in a coolness fraction

00:11:36,710 --> 00:11:40,970
of let's say deployment technologies you

00:11:38,930 --> 00:11:43,430
can do either very uncool boring or

00:11:40,970 --> 00:11:44,840
technology that actually work or you can

00:11:43,430 --> 00:11:46,430
move all the way to the other extreme of

00:11:44,840 --> 00:11:48,140
the spectrum and pick the latest

00:11:46,430 --> 00:11:53,840
craziness on Twitter whatever is

00:11:48,140 --> 00:11:55,970
mentioned there that probably works so

00:11:53,840 --> 00:11:59,960
this was more like the look and feel of

00:11:55,970 --> 00:12:02,060
Kafka streams let's talk a little bit of

00:11:59,960 --> 00:12:03,530
it up about the underlying concept of

00:12:02,060 --> 00:12:06,500
Kafka streams and I'll start first with

00:12:03,530 --> 00:12:08,420
the concepts of Kafka so since most of

00:12:06,500 --> 00:12:11,600
you are familiar with it I'll just be

00:12:08,420 --> 00:12:13,700
very proven concise so in Kafka you have

00:12:11,600 --> 00:12:15,920
producers that write day dr. Kafka and

00:12:13,700 --> 00:12:18,710
consumers that read that data from Kafka

00:12:15,920 --> 00:12:21,770
and the important part is that reading

00:12:18,710 --> 00:12:23,630
and writing is decoupled the service in

00:12:21,770 --> 00:12:26,570
your Kafka cluster other procures who

00:12:23,630 --> 00:12:29,540
are responsible for serving and storing

00:12:26,570 --> 00:12:32,780
the data now if we zoom into the Kafka

00:12:29,540 --> 00:12:35,750
cluster your data is being organized by

00:12:32,780 --> 00:12:39,170
you into topics for example a topic that

00:12:35,750 --> 00:12:41,960
captures user activity events within a

00:12:39,170 --> 00:12:43,580
topic that has been partitioned within a

00:12:41,960 --> 00:12:45,080
partition that is strongly ordered and

00:12:43,580 --> 00:12:47,600
these partitions are distributed across

00:12:45,080 --> 00:12:49,610
the Kafka cluster for enough scalability

00:12:47,600 --> 00:12:56,320
allowed balancing and if you add

00:12:49,610 --> 00:13:00,080
replication also for fault tolerance now

00:12:56,320 --> 00:13:01,850
within a partition the messages are

00:13:00,080 --> 00:13:03,380
actually key value pairs and I'm only

00:13:01,850 --> 00:13:05,120
mentioning this because you will see a

00:13:03,380 --> 00:13:08,350
very strong similarity now that we're

00:13:05,120 --> 00:13:11,900
switching to the Kafka streams concepts

00:13:08,350 --> 00:13:14,150
in Kafka streams you have a stream of

00:13:11,900 --> 00:13:16,750
data which is an ordered sequence of key

00:13:14,150 --> 00:13:16,750
value records

00:13:17,629 --> 00:13:22,619
these data streams are being processed

00:13:19,589 --> 00:13:24,779
by processor topologies where the notes

00:13:22,619 --> 00:13:30,529
are stream processors and the edges are

00:13:24,779 --> 00:13:33,509
the streams if you zoom into that

00:13:30,529 --> 00:13:36,179
similar like Kafka has topic rotations

00:13:33,509 --> 00:13:40,109
Kafka has a streams has stream rotations

00:13:36,179 --> 00:13:42,929
and stream tasks are basically copies of

00:13:40,109 --> 00:13:44,819
the same processing logic that are

00:13:42,929 --> 00:13:47,639
getting a subset of the data that needs

00:13:44,819 --> 00:13:49,199
to be processed so I will not go into

00:13:47,639 --> 00:13:51,209
detail here just give it like a higher

00:13:49,199 --> 00:13:54,660
level view of what that is the key

00:13:51,209 --> 00:13:56,939
takeaway is Kafka partitions data for

00:13:54,660 --> 00:13:59,189
transport and Kafka streams partitions

00:13:56,939 --> 00:14:02,609
data for processing so there is a close

00:13:59,189 --> 00:14:04,410
mapping between the two concepts the

00:14:02,609 --> 00:14:05,669
reason why I am very brief here is

00:14:04,410 --> 00:14:07,649
because I want to talk about something

00:14:05,669 --> 00:14:09,269
that is more interesting and that is

00:14:07,649 --> 00:14:13,230
there is a close relationship between

00:14:09,269 --> 00:14:15,239
streams and tables now if you look at

00:14:13,230 --> 00:14:17,759
the first column that's like your

00:14:15,239 --> 00:14:19,589
typical database table and what you see

00:14:17,759 --> 00:14:22,230
is you have like insert and update

00:14:19,589 --> 00:14:25,949
statements that mutate the table if you

00:14:22,230 --> 00:14:29,449
capture all these changes then you get a

00:14:25,949 --> 00:14:32,279
stream of changes a changelog stream

00:14:29,449 --> 00:14:34,589
once you have that you can reconstruct

00:14:32,279 --> 00:14:36,059
the table on the left side at another

00:14:34,589 --> 00:14:39,689
location which is what you see on the

00:14:36,059 --> 00:14:41,939
right side so you can take a table turn

00:14:39,689 --> 00:14:44,610
into a stream turn it into a table turn

00:14:41,939 --> 00:14:47,910
it into stream and so on and we'll see

00:14:44,610 --> 00:14:49,949
why this is so important and let me

00:14:47,910 --> 00:14:52,369
start with an example so here we have a

00:14:49,949 --> 00:14:54,959
very simple stream of data and we look

00:14:52,369 --> 00:14:56,610
how this stream that is interpreted

00:14:54,959 --> 00:15:00,629
differently depending on whether you are

00:14:56,610 --> 00:15:02,549
a stream or a table if you are a stream

00:15:00,629 --> 00:15:03,959
you look at that data and think okay I

00:15:02,549 --> 00:15:07,769
improved this as a stream of data

00:15:03,959 --> 00:15:10,079
records if your are at the table you

00:15:07,769 --> 00:15:12,119
interpret the same data as a change book

00:15:10,079 --> 00:15:14,339
stream and the table becomes a

00:15:12,119 --> 00:15:18,239
continuously updated materialized view

00:15:14,339 --> 00:15:20,129
of that stream so at this point in time

00:15:18,239 --> 00:15:24,059
both the stream and the table would say

00:15:20,129 --> 00:15:26,160
Alice clicked two times if there is a

00:15:24,059 --> 00:15:28,139
further update for Ellis the stream

00:15:26,160 --> 00:15:30,510
would say actually Alex clicked five

00:15:28,139 --> 00:15:33,080
times first the tables have no no

00:15:30,510 --> 00:15:36,870
she'd neatly - she clicked three times

00:15:33,080 --> 00:15:38,130
so that example is a bit maybe not so

00:15:36,870 --> 00:15:40,380
easy to understand but I wanted to show

00:15:38,130 --> 00:15:41,460
the same data let me use a more

00:15:40,380 --> 00:15:44,040
practical example where we have

00:15:41,460 --> 00:15:46,230
different data the first example is user

00:15:44,040 --> 00:15:47,700
purchase records that are interpreted as

00:15:46,230 --> 00:15:50,700
a string and the other example is a

00:15:47,700 --> 00:15:52,440
table with user profile information at

00:15:50,700 --> 00:15:55,350
this point in time the stream would say

00:15:52,440 --> 00:15:58,460
Alice bought eggs in the supermarket the

00:15:55,350 --> 00:16:01,320
table would say LS is currently in Paris

00:15:58,460 --> 00:16:03,780
moving forward in time the stream would

00:16:01,320 --> 00:16:06,150
say Alice Bock bought both eggs and milk

00:16:03,780 --> 00:16:08,310
and the table would say Alice is not

00:16:06,150 --> 00:16:11,670
anymore in Paris she's now in Berlin at

00:16:08,310 --> 00:16:13,650
buzzwords so the difference is the

00:16:11,670 --> 00:16:15,750
stream shows you all the values for key

00:16:13,650 --> 00:16:20,010
and the table gives you just the very

00:16:15,750 --> 00:16:22,880
latest value of the key so in terms of

00:16:20,010 --> 00:16:26,610
the Kafka streams DSL there are

00:16:22,880 --> 00:16:29,040
operations that transform from a table

00:16:26,610 --> 00:16:30,690
to a stream and from a stream to a table

00:16:29,040 --> 00:16:32,430
you can always convert the table to a

00:16:30,690 --> 00:16:35,280
stream by just iterating through the

00:16:32,430 --> 00:16:36,480
table but if you are converting from a

00:16:35,280 --> 00:16:38,610
stream to a table need to provide

00:16:36,480 --> 00:16:40,950
semantics if you have a list of input

00:16:38,610 --> 00:16:42,360
values you must define when this is

00:16:40,950 --> 00:16:44,280
something only use user know how to

00:16:42,360 --> 00:16:47,520
squash all these values into one single

00:16:44,280 --> 00:16:50,700
value as an end result so a simple

00:16:47,520 --> 00:16:53,010
example is a joint so let's say you

00:16:50,700 --> 00:16:56,160
compute user clicks by region and your

00:16:53,010 --> 00:16:58,020
input would be a list click 13 times and

00:16:56,160 --> 00:17:00,180
the other input would be alice is

00:16:58,020 --> 00:17:02,370
currently in Europe this is how it would

00:17:00,180 --> 00:17:04,170
look like in the Kafka streams DSL and

00:17:02,370 --> 00:17:06,209
again I'm not walking through the code

00:17:04,170 --> 00:17:08,070
I'm only saying that there's actually

00:17:06,209 --> 00:17:10,800
this part that we will look at now in an

00:17:08,070 --> 00:17:12,300
example and also mentioning there is

00:17:10,800 --> 00:17:15,180
this region with clicks thing that you

00:17:12,300 --> 00:17:16,589
may or not be able to read and the

00:17:15,180 --> 00:17:18,990
reason we have that is only because Java

00:17:16,589 --> 00:17:21,930
doesn't support tuples natively in Scala

00:17:18,990 --> 00:17:25,680
it's much more natural so in this

00:17:21,930 --> 00:17:28,890
example we have an input stream which is

00:17:25,680 --> 00:17:31,650
a stream we're joining it with a table

00:17:28,890 --> 00:17:35,400
to know when a let's click 13 times she

00:17:31,650 --> 00:17:36,720
was in Europe and then we are changing

00:17:35,400 --> 00:17:39,630
the key because we want to aggravate by

00:17:36,720 --> 00:17:41,870
user region and not by user name and

00:17:39,630 --> 00:17:43,980
it's to the string and now we are

00:17:41,870 --> 00:17:44,250
creating this information to come up

00:17:43,980 --> 00:17:47,309
with

00:17:44,250 --> 00:17:49,080
total count in this case Europe is 13

00:17:47,309 --> 00:17:52,740
clicks after the first event and then

00:17:49,080 --> 00:17:54,210
it's 18 clicks after the second event so

00:17:52,740 --> 00:17:56,130
going back to that picture there are

00:17:54,210 --> 00:17:59,039
certain transformations that change the

00:17:56,130 --> 00:18:01,320
shape of your data structure and certain

00:17:59,039 --> 00:18:03,720
transformations that don't and it's

00:18:01,320 --> 00:18:05,400
important in our opinion that your

00:18:03,720 --> 00:18:06,960
stream processing tool understands that

00:18:05,400 --> 00:18:08,250
difference and gives you the tools it

00:18:06,960 --> 00:18:13,020
needs to make proper decisions what is

00:18:08,250 --> 00:18:14,250
correct for your use case so lastly one

00:18:13,020 --> 00:18:16,980
thing that might not be totally obvious

00:18:14,250 --> 00:18:18,390
from that example this table when you're

00:18:16,980 --> 00:18:19,919
doing a join for example it's

00:18:18,390 --> 00:18:21,510
continuously being updated with the

00:18:19,919 --> 00:18:24,059
latest data behind the scenes because

00:18:21,510 --> 00:18:26,250
it's being backed by a cafe topic so as

00:18:24,059 --> 00:18:29,280
soon as your your stream that your that

00:18:26,250 --> 00:18:31,650
is joined against that table gets

00:18:29,280 --> 00:18:32,970
another input the latest value is

00:18:31,650 --> 00:18:34,950
automatically retrieved from that table

00:18:32,970 --> 00:18:39,510
so that is very cool for building

00:18:34,950 --> 00:18:41,220
stateful applications all right so we

00:18:39,510 --> 00:18:42,690
talked a little bit about you know look

00:18:41,220 --> 00:18:44,340
and feel of Kafka screams what it is

00:18:42,690 --> 00:18:46,500
when you should use it we talked a bit

00:18:44,340 --> 00:18:48,210
about key concepts and so on now let me

00:18:46,500 --> 00:18:52,320
talk a little bit about the key features

00:18:48,210 --> 00:19:00,270
of the current implementation first of

00:18:52,320 --> 00:19:02,190
all let me take a drink so of course by

00:19:00,270 --> 00:19:05,010
definition Kafka streams is 100%

00:19:02,190 --> 00:19:06,960
compatible with Kafka so if you have

00:19:05,010 --> 00:19:09,299
ever had various that your latest 2,000

00:19:06,960 --> 00:19:11,820
interface well with Kafka that problem

00:19:09,299 --> 00:19:14,190
is served in Kafka streams also Kafka

00:19:11,820 --> 00:19:16,020
streams inherits Kafka security model so

00:19:14,190 --> 00:19:17,669
you can encrypt data in transit for

00:19:16,020 --> 00:19:19,559
example restrict access to certain

00:19:17,669 --> 00:19:21,090
topics which is great if you reckon on

00:19:19,559 --> 00:19:25,169
sensitive data like personally

00:19:21,090 --> 00:19:28,830
identifiable information in terms of the

00:19:25,169 --> 00:19:31,590
API of course functions that turn a

00:19:28,830 --> 00:19:34,470
Kafka topic inter stream a Kafka topic

00:19:31,590 --> 00:19:36,270
into a table or a table back to Kafka

00:19:34,470 --> 00:19:39,720
topic or stewing bag into a casket topic

00:19:36,270 --> 00:19:41,429
and when you're writing applications

00:19:39,720 --> 00:19:43,500
that use the Kafka streams library you

00:19:41,429 --> 00:19:45,210
can figure both Kafka stream specific

00:19:43,500 --> 00:19:47,690
settings as well as settings of the

00:19:45,210 --> 00:19:50,280
underlying Kafka producer and consumer

00:19:47,690 --> 00:19:51,990
clients if you're already familiar with

00:19:50,280 --> 00:19:53,370
Kafka I think that will be kraid if

00:19:51,990 --> 00:19:54,780
you're not yet familiar just skip that

00:19:53,370 --> 00:19:55,950
part it just means that yeah you have a

00:19:54,780 --> 00:19:59,030
lot of flexibility to fine-tune your

00:19:55,950 --> 00:19:59,030
application if you need to

00:19:59,870 --> 00:20:03,210
when you're using kafka streams you

00:20:02,130 --> 00:20:04,950
automatically get the following

00:20:03,210 --> 00:20:06,750
properties for your application high

00:20:04,950 --> 00:20:10,920
scalability fault tolerance and

00:20:06,750 --> 00:20:12,660
elasticity and how does that work so

00:20:10,920 --> 00:20:14,610
imagine you have data in a cupcake

00:20:12,660 --> 00:20:16,200
cluster and your task is it to create an

00:20:14,610 --> 00:20:18,890
application that process the data to

00:20:16,200 --> 00:20:20,910
create something like an espresso

00:20:18,890 --> 00:20:22,860
initially one machine might be

00:20:20,910 --> 00:20:24,780
sufficient for presence all the data but

00:20:22,860 --> 00:20:26,040
it's someone need to scale out how do

00:20:24,780 --> 00:20:28,410
you do that in Kafka streams

00:20:26,040 --> 00:20:30,630
well you just start literally the same

00:20:28,410 --> 00:20:34,710
application the same code on another

00:20:30,630 --> 00:20:36,179
machine these two instances will become

00:20:34,710 --> 00:20:38,820
aware of each other and start dividing

00:20:36,179 --> 00:20:41,850
the work if you need more machines just

00:20:38,820 --> 00:20:44,600
add more three four ten 20 whatever if

00:20:41,850 --> 00:20:46,530
one of the machines goes offline failure

00:20:44,600 --> 00:20:48,630
maintenance you don't need the capacity

00:20:46,530 --> 00:20:50,370
and longer the other machines will

00:20:48,630 --> 00:20:54,450
become aware of that and start taking

00:20:50,370 --> 00:20:56,190
over the work now when I started out

00:20:54,450 --> 00:20:57,840
memory design data that is just I don't

00:20:56,190 --> 00:20:59,429
believe that that just can't be true

00:20:57,840 --> 00:21:00,780
right and the reason why that works is

00:20:59,429 --> 00:21:03,000
and I'm not going into details here is

00:21:00,780 --> 00:21:05,460
because we are outsourcing hard problems

00:21:03,000 --> 00:21:06,990
to Kafka Kafka has already concepts like

00:21:05,460 --> 00:21:09,240
consumer groups and so on that does this

00:21:06,990 --> 00:21:11,730
rebalancing work between new machines

00:21:09,240 --> 00:21:13,950
machines that go offline so we're just

00:21:11,730 --> 00:21:17,970
using that don't need to implement

00:21:13,950 --> 00:21:19,500
anything ourselves in cupca streams you

00:21:17,970 --> 00:21:21,870
can also do stateful and stateless

00:21:19,500 --> 00:21:23,730
computations and an example of that

00:21:21,870 --> 00:21:27,480
would be aggregations or joints like the

00:21:23,730 --> 00:21:29,100
ones that we have in an example so state

00:21:27,480 --> 00:21:31,590
stores in Kafka streams allows your

00:21:29,100 --> 00:21:34,590
application to manage state these state

00:21:31,590 --> 00:21:37,350
stores are isolated that is per stream

00:21:34,590 --> 00:21:39,570
task they are local for best performance

00:21:37,350 --> 00:21:41,820
you don't have to do like lookups over

00:21:39,570 --> 00:21:44,669
the network and they are replicated to

00:21:41,820 --> 00:21:47,610
Kafka for allowing two things elasticity

00:21:44,669 --> 00:21:50,910
scale out shrink down and fault

00:21:47,610 --> 00:21:56,130
tolerance let's go back to that example

00:21:50,910 --> 00:21:59,010
and caveat this is a very simplified

00:21:56,130 --> 00:22:00,720
view because of lack of time so be

00:21:59,010 --> 00:22:02,490
careful when you jump to conclusions hey

00:22:00,720 --> 00:22:05,880
this doesn't scale well or so on because

00:22:02,490 --> 00:22:07,230
it's really a simplified view so imagine

00:22:05,880 --> 00:22:09,090
you're if you're three instances of your

00:22:07,230 --> 00:22:11,190
application and they do some stateful

00:22:09,090 --> 00:22:13,100
stuff so these applications will

00:22:11,190 --> 00:22:16,860
this instance would have state stores

00:22:13,100 --> 00:22:19,590
one or more now remember that we talked

00:22:16,860 --> 00:22:23,250
about how a table and the key value set

00:22:19,590 --> 00:22:24,930
store is a table is also a stream and

00:22:23,250 --> 00:22:27,900
that again can be converted back into

00:22:24,930 --> 00:22:30,780
the table so whenever your state stores

00:22:27,900 --> 00:22:32,400
change when they're being mutated we're

00:22:30,780 --> 00:22:34,770
sending this information to Kafka and

00:22:32,400 --> 00:22:37,860
persist it there so as soon as one of

00:22:34,770 --> 00:22:41,970
the instances goes offline the other

00:22:37,860 --> 00:22:43,860
instances can take care of the network

00:22:41,970 --> 00:22:46,500
by reconstructing the local state at the

00:22:43,860 --> 00:22:48,270
point of failure taking down for

00:22:46,500 --> 00:22:50,430
maintenance and so on and resumed the

00:22:48,270 --> 00:22:51,870
processing it doesn't need to God always

00:22:50,430 --> 00:22:55,350
to one machine like in this example but

00:22:51,870 --> 00:22:58,170
as I said it's a simplified view how

00:22:55,350 --> 00:23:00,450
does it work again we're punting the

00:22:58,170 --> 00:23:02,250
hard problems to Kafka there's nothing

00:23:00,450 --> 00:23:03,840
that we need to do in Kafka streams just

00:23:02,250 --> 00:23:05,220
like back pressure I will not be talking

00:23:03,840 --> 00:23:06,750
about back pressure here because there

00:23:05,220 --> 00:23:12,090
is no need for something like that in

00:23:06,750 --> 00:23:13,950
the way Kafka and Kafka streams work now

00:23:12,090 --> 00:23:15,720
state stores like how complicated is

00:23:13,950 --> 00:23:17,730
that actually if you use the Kafka

00:23:15,720 --> 00:23:19,770
streams DSL you won't even be aware of

00:23:17,730 --> 00:23:22,320
these state stores they are abstracted

00:23:19,770 --> 00:23:24,360
away from you so if you doing counts for

00:23:22,320 --> 00:23:26,490
do you see aggregations you will not

00:23:24,360 --> 00:23:29,010
even see them if you use the low level

00:23:26,490 --> 00:23:31,920
processor API you have direct access to

00:23:29,010 --> 00:23:33,690
these state stores so you're very

00:23:31,920 --> 00:23:36,180
flexible to have more money at work but

00:23:33,690 --> 00:23:38,460
still it's relatively simple when your

00:23:36,180 --> 00:23:39,750
processors start up you can get a

00:23:38,460 --> 00:23:41,850
reference to one or more state stores

00:23:39,750 --> 00:23:45,500
and then when you're processing records

00:23:41,850 --> 00:23:45,500
you can do like gets and puts and so on

00:23:46,130 --> 00:23:51,180
so the other thing that is important for

00:23:49,710 --> 00:23:53,340
a stream processing tool is that it

00:23:51,180 --> 00:23:54,960
properly models time I think the

00:23:53,340 --> 00:23:57,510
canonical example is the Star Wars

00:23:54,960 --> 00:23:59,280
franchise so in Star Wars we have seven

00:23:57,510 --> 00:24:01,080
episodes and if we look at the event

00:23:59,280 --> 00:24:04,110
time when things happen in that universe

00:24:01,080 --> 00:24:07,550
the first episode was the Phantom Menace

00:24:04,110 --> 00:24:09,270
with Jar Jar Binks unfortunately

00:24:07,550 --> 00:24:11,640
processing time was when they were

00:24:09,270 --> 00:24:16,010
released and filmed it's quite different

00:24:11,640 --> 00:24:18,390
the first one was in 1977 and you hope

00:24:16,010 --> 00:24:21,180
so if you convert that to Kafka streams

00:24:18,390 --> 00:24:23,820
and Kafka event on would be the time

00:24:21,180 --> 00:24:25,059
when the producers would generate the

00:24:23,820 --> 00:24:27,340
original event and write its end

00:24:25,059 --> 00:24:28,749
- Kafka processing time would be when

00:24:27,340 --> 00:24:30,249
your cough costumes application actually

00:24:28,749 --> 00:24:32,169
looks at the data and does something to

00:24:30,249 --> 00:24:33,669
it and there's also ingestion time which

00:24:32,169 --> 00:24:36,039
is the time when the prokhor like the

00:24:33,669 --> 00:24:38,529
Kafka cluster received the magic message

00:24:36,039 --> 00:24:40,330
for the first time and in Kafka strains

00:24:38,529 --> 00:24:42,190
you configure the desired time semantics

00:24:40,330 --> 00:24:43,899
through times than extractors and by

00:24:42,190 --> 00:24:46,929
default you get event time semantics and

00:24:43,899 --> 00:24:50,080
here you see an example of event time

00:24:46,929 --> 00:24:51,309
and processing time it's pretty simple

00:24:50,080 --> 00:24:53,710
it's a one-liner how to do that

00:24:51,309 --> 00:24:55,419
you can use the same to extract existing

00:24:53,710 --> 00:24:56,529
time stand out of your payload if you

00:24:55,419 --> 00:25:02,409
already have a stream that has that

00:24:56,529 --> 00:25:03,909
information available so the other part

00:25:02,409 --> 00:25:05,350
that is already relevant in the first

00:25:03,909 --> 00:25:08,470
version of Kafka streams is windowing

00:25:05,350 --> 00:25:12,580
it's very very related to time so I'm

00:25:08,470 --> 00:25:14,049
trying to make a link between the two if

00:25:12,580 --> 00:25:15,879
you have a continuous stream of data

00:25:14,049 --> 00:25:18,570
sometimes you've used cases where you

00:25:15,879 --> 00:25:20,830
want to break this into discrete chunks

00:25:18,570 --> 00:25:22,960
so for example you want to say something

00:25:20,830 --> 00:25:24,730
like I want to get aggregates over a

00:25:22,960 --> 00:25:26,110
period of three seconds and every few

00:25:24,730 --> 00:25:28,960
seconds I want to get an update order

00:25:26,110 --> 00:25:30,190
latest aggregation result is like and

00:25:28,960 --> 00:25:32,950
that that's what you would call a

00:25:30,190 --> 00:25:36,639
tumbling time window so you can easily

00:25:32,950 --> 00:25:38,409
express that in Kafka streams similarly

00:25:36,639 --> 00:25:40,179
there is a generalization of that called

00:25:38,409 --> 00:25:42,879
hopping windows where you say I want to

00:25:40,179 --> 00:25:44,740
get aggregations for three second

00:25:42,879 --> 00:25:47,619
periods of time but I want to get

00:25:44,740 --> 00:25:49,149
updates sooner like every second not

00:25:47,619 --> 00:25:50,590
just every three seconds and then you

00:25:49,149 --> 00:25:53,019
have multiple of these windows in fly

00:25:50,590 --> 00:25:54,190
that you won't even notice what Kafka

00:25:53,019 --> 00:25:57,490
streams manage that behind-the-scenes

00:25:54,190 --> 00:25:59,110
for you and why would you need that

00:25:57,490 --> 00:26:00,999
there are many use cases I just show

00:25:59,110 --> 00:26:02,200
maybe the most obvious one is if you

00:26:00,999 --> 00:26:05,249
have a monitoring application like

00:26:02,200 --> 00:26:08,110
confident control center you want to

00:26:05,249 --> 00:26:09,789
compute and visualize times years of

00:26:08,110 --> 00:26:12,009
data and you want to do this across

00:26:09,789 --> 00:26:14,200
different levels of granularity for

00:26:12,009 --> 00:26:22,480
example one minute aggregates 5 min

00:26:14,200 --> 00:26:23,860
attacker gets and so on so lastly there

00:26:22,480 --> 00:26:25,379
are three things I've seen like we have

00:26:23,860 --> 00:26:27,490
five minutes left

00:26:25,379 --> 00:26:31,450
there are few things that I just talked

00:26:27,490 --> 00:26:34,450
about very briefly one Kafka stream

00:26:31,450 --> 00:26:36,610
supports late arriving and out of order

00:26:34,450 --> 00:26:38,620
data so for example if you have a

00:26:36,610 --> 00:26:41,169
five-minute window

00:26:38,620 --> 00:26:43,150
sometimes messages don't arrive within

00:26:41,169 --> 00:26:45,490
these five minutes imagine you have an

00:26:43,150 --> 00:26:47,100
Internet of Things platform and you're

00:26:45,490 --> 00:26:49,539
collecting data from devices that are

00:26:47,100 --> 00:26:51,520
distributed all over the planet and

00:26:49,539 --> 00:26:53,380
sometimes these devices have unreliable

00:26:51,520 --> 00:26:56,080
internet connectivity so you might get

00:26:53,380 --> 00:26:57,490
an update from an advice but within five

00:26:56,080 --> 00:27:00,279
minutes but maybe after an hour or maybe

00:26:57,490 --> 00:27:02,730
after a day so with Kafka streams you

00:27:00,279 --> 00:27:06,340
can define how these lady writing data

00:27:02,730 --> 00:27:08,260
should be interpreted by application on

00:27:06,340 --> 00:27:12,820
describe them do want to send an update

00:27:08,260 --> 00:27:14,740
downstream and so on in terms of how

00:27:12,820 --> 00:27:16,990
fast or what's low latency of Kafka

00:27:14,740 --> 00:27:19,059
streams applications it has millisecond

00:27:16,990 --> 00:27:20,559
processing latency because it's it's

00:27:19,059 --> 00:27:23,679
doing actually one message at a time

00:27:20,559 --> 00:27:24,970
processing and has no micro batching so

00:27:23,679 --> 00:27:27,039
if you have a use case that requires low

00:27:24,970 --> 00:27:31,630
latency Kafka stream student should be a

00:27:27,039 --> 00:27:33,580
good fit and talking about processing

00:27:31,630 --> 00:27:36,309
the guarantees that we give for

00:27:33,580 --> 00:27:39,159
processing in Kafka streams at the

00:27:36,309 --> 00:27:40,779
moment is at least once because again

00:27:39,159 --> 00:27:44,730
we're building on the shoulders of Kafka

00:27:40,779 --> 00:27:47,890
and Kafka das at least once messaging

00:27:44,730 --> 00:27:49,390
that said the work on adding exactly one

00:27:47,890 --> 00:27:51,100
support is already in the works that

00:27:49,390 --> 00:27:52,690
work has already started and actually

00:27:51,100 --> 00:27:54,100
one of the engineers who is responsible

00:27:52,690 --> 00:28:02,799
as it is in this room and I'm not

00:27:54,100 --> 00:28:05,049
pointing out now so wrapping up so we

00:28:02,799 --> 00:28:08,590
did talk briefly about what is Kafka was

00:28:05,049 --> 00:28:12,070
Kafka streams when would you use it and

00:28:08,590 --> 00:28:13,390
why we think it hopefully advances the

00:28:12,070 --> 00:28:15,100
study of the art a little bit so that

00:28:13,390 --> 00:28:20,169
you can build cooler applications with

00:28:15,100 --> 00:28:22,720
less sweat and tears now assume that I

00:28:20,169 --> 00:28:24,549
raced your interest a little bit you can

00:28:22,720 --> 00:28:26,500
get it right now it was released a few

00:28:24,549 --> 00:28:28,270
days ago alongside Kafka Oh point 10 and

00:28:26,500 --> 00:28:30,580
it's also available as part of the

00:28:28,270 --> 00:28:32,649
confluent platformer II as I said build

00:28:30,580 --> 00:28:37,000
more than just the core of Apache Kafka

00:28:32,649 --> 00:28:39,549
in in and one big platform we also

00:28:37,000 --> 00:28:40,419
provide a lot of demos and examples to

00:28:39,549 --> 00:28:42,610
get you started

00:28:40,419 --> 00:28:45,669
like showcasing how to do the canonical

00:28:42,610 --> 00:28:49,090
word count how to do joins how to read

00:28:45,669 --> 00:28:51,970
invite every data and so on and if this

00:28:49,090 --> 00:28:53,950
talk I mean I couldn't really cover

00:28:51,970 --> 00:28:55,960
a lot of that in detail but we have

00:28:53,950 --> 00:28:58,810
extensive documentation that I think is

00:28:55,960 --> 00:29:00,580
too concise so we have quick starts deep

00:28:58,810 --> 00:29:01,930
dive on concept architecture developer

00:29:00,580 --> 00:29:07,360
guide and so on if you're interested in

00:29:01,930 --> 00:29:08,980
that and lastly we also do bi-weekly ask

00:29:07,360 --> 00:29:11,380
me anything sessions if you're

00:29:08,980 --> 00:29:17,910
interested in joining these just drop me

00:29:11,380 --> 00:29:22,390
in out some of the things to come so

00:29:17,910 --> 00:29:23,980
what will happen next I already

00:29:22,390 --> 00:29:27,250
mentioned we are working on adding

00:29:23,980 --> 00:29:30,670
exactly one semantics of Kafka and two

00:29:27,250 --> 00:29:33,610
Kafka streams the other thing that we

00:29:30,670 --> 00:29:37,480
are working on is Kerrville State so if

00:29:33,610 --> 00:29:38,980
you think of you know these tables that

00:29:37,480 --> 00:29:41,770
are being consciously updated and kept

00:29:38,980 --> 00:29:43,120
up-to-date oftentimes this is already

00:29:41,770 --> 00:29:45,670
everything that you need for whatever

00:29:43,120 --> 00:29:47,320
you're doing like your espresso so we

00:29:45,670 --> 00:29:50,620
want to allow you to directly tap into

00:29:47,320 --> 00:29:52,600
that state and expose it to you that

00:29:50,620 --> 00:29:55,210
will open a whole lot of different use

00:29:52,600 --> 00:29:56,410
cases that would be great fit for Kafka

00:29:55,210 --> 00:29:59,650
stream so they were making your life

00:29:56,410 --> 00:30:02,200
easier the other thing that we're

00:29:59,650 --> 00:30:06,250
working on is a sequel interface so that

00:30:02,200 --> 00:30:08,320
you would not need a developer API to

00:30:06,250 --> 00:30:14,140
express how you want to compute your

00:30:08,320 --> 00:30:15,610
data and lastly but it's not least it's

00:30:14,140 --> 00:30:20,680
the first version that we just released

00:30:15,610 --> 00:30:22,510
in Kafka o10 so we were welcoming all of

00:30:20,680 --> 00:30:24,550
you to tell us what you like and what

00:30:22,510 --> 00:30:26,350
you don't like so can we make it better

00:30:24,550 --> 00:30:28,030
so if you have any feedback I encourage

00:30:26,350 --> 00:30:32,400
you to share it by letting us know in

00:30:28,030 --> 00:30:34,570
the car can use a manual list so in

00:30:32,400 --> 00:30:36,490
regards to things to come tomorrow

00:30:34,570 --> 00:30:38,830
morning we have a keynote by Neon Ikeda

00:30:36,490 --> 00:30:40,600
she is our CTO at confluent and one of

00:30:38,830 --> 00:30:43,840
our co-founders and she'll be talking

00:30:40,600 --> 00:30:45,790
about the bigger picture of stream

00:30:43,840 --> 00:30:47,740
processing so if this talk was a

00:30:45,790 --> 00:30:49,890
technique of you focusing on kafka

00:30:47,740 --> 00:30:52,630
streams now I will talk tomorrow about

00:30:49,890 --> 00:30:55,180
the bigger picture the strategic view of

00:30:52,630 --> 00:30:59,350
what stream processing is where it fits

00:30:55,180 --> 00:31:02,050
in and so on so I think we still have

00:30:59,350 --> 00:31:04,060
time for some questions now but if

00:31:02,050 --> 00:31:05,590
you're not feeling confident in front of

00:31:04,060 --> 00:31:27,549
everyone you can just find

00:31:05,590 --> 00:31:30,970
for the conference thank you so I have a

00:31:27,549 --> 00:31:32,679
question about the state store and I

00:31:30,970 --> 00:31:34,900
think it's pretty clever pretty smart to

00:31:32,679 --> 00:31:38,110
push to push the data back into Kafka

00:31:34,900 --> 00:31:41,140
but wouldn't that mean like if one nodes

00:31:38,110 --> 00:31:46,860
goes down that it can take quite a while

00:31:41,140 --> 00:31:49,270
to recreate the state in another node so

00:31:46,860 --> 00:31:51,970
shall I repeat the question for the

00:31:49,270 --> 00:31:56,470
recording or is it fine it's fine

00:31:51,970 --> 00:31:58,720
okay so one thing that we're exploiting

00:31:56,470 --> 00:32:02,500
here is a feature called lock compaction

00:31:58,720 --> 00:32:07,029
in Apache Kafka and I hope I find it a

00:32:02,500 --> 00:32:09,399
very quick way to summarize that if

00:32:07,029 --> 00:32:11,919
you're having a stream of data let's say

00:32:09,399 --> 00:32:14,919
with a lot of updates for Ellis as in

00:32:11,919 --> 00:32:17,200
one of the examples that we showed what

00:32:14,919 --> 00:32:18,580
lock compassionately do is if we say I

00:32:17,200 --> 00:32:21,549
have 10 updates for Ellis

00:32:18,580 --> 00:32:24,250
but since state's doors are interpreted

00:32:21,549 --> 00:32:26,350
as a change lock stream I can throw away

00:32:24,250 --> 00:32:27,970
all the updates but the very latest one

00:32:26,350 --> 00:32:30,100
for Ellis so if you have 1 million

00:32:27,970 --> 00:32:31,690
updates for Ellis what will end up in

00:32:30,100 --> 00:32:34,270
Kafka after log compaction is just a

00:32:31,690 --> 00:32:37,870
single entry so that is why it will be

00:32:34,270 --> 00:32:42,210
very efficient this doesn't answer your

00:32:37,870 --> 00:32:42,210
question ok ok

00:32:44,289 --> 00:32:51,289
what if after compaction I still have a

00:32:47,029 --> 00:32:54,289
million entries they would have to be

00:32:51,289 --> 00:32:56,630
recreated on another note right yeah it

00:32:54,289 --> 00:32:59,210
will just take time so even though it

00:32:56,630 --> 00:33:02,149
might be like normally like millisecond

00:32:59,210 --> 00:33:04,730
latency there might be one minute pass

00:33:02,149 --> 00:33:06,169
or something in processing right there

00:33:04,730 --> 00:33:08,149
will not be a pause but it will take a

00:33:06,169 --> 00:33:10,220
while until the data is being a restored

00:33:08,149 --> 00:33:12,380
to exactly state when the failure

00:33:10,220 --> 00:33:17,690
happened yeah so we pay Suncoast in the

00:33:12,380 --> 00:33:20,059
presence of failures there's quite a few

00:33:17,690 --> 00:33:22,309
things that I think originally might

00:33:20,059 --> 00:33:25,399
have been in Sansa or Sam's it was kind

00:33:22,309 --> 00:33:27,500
of on the way to getting towards doing

00:33:25,399 --> 00:33:29,870
is there any reason that is Kafka

00:33:27,500 --> 00:33:31,909
streams like a new rewrite or does it

00:33:29,870 --> 00:33:33,440
include Sam's or his sons are gonna sort

00:33:31,909 --> 00:33:36,890
of fade away now I'm not really sure

00:33:33,440 --> 00:33:40,159
what do you think so I I will not be

00:33:36,890 --> 00:33:41,270
able to speak for the santur project but

00:33:40,159 --> 00:33:44,240
of course there are some strong

00:33:41,270 --> 00:33:45,740
similarities between Samsa and Kafka

00:33:44,240 --> 00:33:47,600
streams to the point that some people

00:33:45,740 --> 00:33:50,390
not us are saying like it's like Samsa

00:33:47,600 --> 00:33:54,289
plus plus or so I think that that is not

00:33:50,390 --> 00:33:56,299
a fair comparison but some of the people

00:33:54,289 --> 00:33:58,370
that work on Samsa now in our team so

00:33:56,299 --> 00:34:02,120
what we actually did with Kafka streams

00:33:58,370 --> 00:34:03,230
was we looked at you know all the prior

00:34:02,120 --> 00:34:05,720
art in that space

00:34:03,230 --> 00:34:07,460
academia open source tools like Samsa

00:34:05,720 --> 00:34:08,960
and we said okay what are the things

00:34:07,460 --> 00:34:10,760
that injera peak here and what are the

00:34:08,960 --> 00:34:14,450
things that we consciously decided not

00:34:10,760 --> 00:34:16,429
to do to keep Kafka stream simple which

00:34:14,450 --> 00:34:17,389
is like a pretty abstract answer to your

00:34:16,429 --> 00:34:18,859
question but I think that's the

00:34:17,389 --> 00:34:22,520
higher-level answer a concrete example

00:34:18,859 --> 00:34:26,780
would be that in Samsa there are two

00:34:22,520 --> 00:34:29,599
things one is it typically wants you to

00:34:26,780 --> 00:34:31,339
run in in yarn and brew felt like oh

00:34:29,599 --> 00:34:33,020
that is a pretty huge investment I mean

00:34:31,339 --> 00:34:36,409
if you have ever worked in larger

00:34:33,020 --> 00:34:38,929
enterprise telling the governance board

00:34:36,409 --> 00:34:40,550
your manager whatever just because you

00:34:38,929 --> 00:34:44,270
have a stream processing problem to

00:34:40,550 --> 00:34:45,560
install Yaron that will not fly so we

00:34:44,270 --> 00:34:47,750
didn't want to make the same mistake or

00:34:45,560 --> 00:34:50,030
basically we didn't want to make it you

00:34:47,750 --> 00:34:53,869
run into the same problems the second

00:34:50,030 --> 00:34:55,619
part is the second example is that in

00:34:53,869 --> 00:34:57,509
Samsa

00:34:55,619 --> 00:35:00,480
is used as the default messaging layer

00:34:57,509 --> 00:35:01,890
but it is pluggable what that means is

00:35:00,480 --> 00:35:04,740
your is like like this the smallest

00:35:01,890 --> 00:35:05,910
common denominator for messaging and in

00:35:04,740 --> 00:35:08,670
Kafka stream you said no we're not going

00:35:05,910 --> 00:35:10,769
to do that no one ever swapped out Kafka

00:35:08,670 --> 00:35:13,589
for Samsa so we're actually embracing

00:35:10,769 --> 00:35:15,660
Kafka and by doing so yes we're telling

00:35:13,589 --> 00:35:18,390
you that normally you will always read

00:35:15,660 --> 00:35:20,279
from Kafka and right back to Kafka but

00:35:18,390 --> 00:35:22,890
we can punt a lot of hard problems to

00:35:20,279 --> 00:35:24,660
Kafka and by really embracing it so if

00:35:22,890 --> 00:35:26,160
you ever are used and hope there's no

00:35:24,660 --> 00:35:28,200
long line in answer to your question if

00:35:26,160 --> 00:35:29,670
you ever use one of the other stream

00:35:28,200 --> 00:35:32,819
processing tools they often implement

00:35:29,670 --> 00:35:35,849
their own messaging layer so what does

00:35:32,819 --> 00:35:39,059
that mean so let let's think about it in

00:35:35,849 --> 00:35:40,109
terms of flag economies of scale what

00:35:39,059 --> 00:35:42,690
you want to do is you want to build a

00:35:40,109 --> 00:35:45,269
stream processing tool that is very good

00:35:42,690 --> 00:35:46,769
at stream processing and then you feel

00:35:45,269 --> 00:35:48,450
like oh wait a minute I also need to

00:35:46,769 --> 00:35:49,920
pass messages around so we can do the

00:35:48,450 --> 00:35:52,769
processing in a distributed system

00:35:49,920 --> 00:35:54,390
let's also solve that problem but

00:35:52,769 --> 00:35:56,130
actually you know we learned in Kafka

00:35:54,390 --> 00:35:58,859
serving the messaging problem it's

00:35:56,130 --> 00:36:00,539
really tough all by itself so why would

00:35:58,859 --> 00:36:02,489
you want to re-implement this as like a

00:36:00,539 --> 00:36:04,249
second or third or fourth priority if

00:36:02,489 --> 00:36:06,779
what we want to do is stream processing

00:36:04,249 --> 00:36:08,549
so and with Kafka streams what we can do

00:36:06,779 --> 00:36:11,489
is whenever there is improvement to

00:36:08,549 --> 00:36:13,380
Kafka hey we added security guess what

00:36:11,489 --> 00:36:15,059
you get it for free in Kafka streams you

00:36:13,380 --> 00:36:17,130
have better performance in Kafka you get

00:36:15,059 --> 00:36:18,599
it for free in Kafka streams so for us

00:36:17,130 --> 00:36:20,549
it's also economies of scale when you're

00:36:18,599 --> 00:36:22,410
maintaining the project by making it

00:36:20,549 --> 00:36:23,460
simpler and benefiting from all these

00:36:22,410 --> 00:36:25,980
other projects when they're doing

00:36:23,460 --> 00:36:27,509
innovative stuff and I think a sample

00:36:25,980 --> 00:36:30,420
that I was alluding to his deployment

00:36:27,509 --> 00:36:32,309
frameworks resource managers whatever

00:36:30,420 --> 00:36:34,349
you're doing in your stream processing

00:36:32,309 --> 00:36:37,079
project he will never be as good as

00:36:34,349 --> 00:36:39,119
resource managers like me sauce and why

00:36:37,079 --> 00:36:41,519
is that the case because their purpose

00:36:39,119 --> 00:36:43,650
is to solve this exact problem he will

00:36:41,519 --> 00:36:46,170
never get better with like some that you

00:36:43,650 --> 00:36:47,489
do at this on the side so I think some

00:36:46,170 --> 00:36:49,109
of the lessons learned and the

00:36:47,489 --> 00:36:51,509
similarities with between Samsa and

00:36:49,109 --> 00:36:53,160
Kafka streams is that we looked at what

00:36:51,509 --> 00:36:56,450
worked and what didn't and try to

00:36:53,160 --> 00:36:59,130
improve upon that but in terms of like

00:36:56,450 --> 00:37:04,800
actual taking over code that didn't

00:36:59,130 --> 00:37:07,800
happen very good thank you

00:37:04,800 --> 00:37:07,800
anyone

00:37:08,290 --> 00:37:13,600
I have a hard time seeing as most of you

00:37:10,720 --> 00:37:16,200
it's thanks a lot okay

00:37:13,600 --> 00:37:16,200

YouTube URL: https://www.youtube.com/watch?v=o7zSLNiTZbA


