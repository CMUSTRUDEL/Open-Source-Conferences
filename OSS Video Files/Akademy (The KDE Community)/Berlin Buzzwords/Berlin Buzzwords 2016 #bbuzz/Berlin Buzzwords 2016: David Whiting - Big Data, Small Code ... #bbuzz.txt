Title: Berlin Buzzwords 2016: David Whiting - Big Data, Small Code ... #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	David Whiting talking about "Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark".

New execution platforms may be popping up all the time with the intention of being the "hot new thing" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. 

Apache Crunch is a simple framework on top of MapReduce - with support for running on Spark as well - which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. 

This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.

Read more:
https://2016.berlinbuzzwords.de/session/big-data-small-code-using-java-8-and-apache-crunch-quickly-develop-concise-efficient

About David Whiting:
https://2016.berlinbuzzwords.de/users/david-whiting-0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,810 --> 00:00:07,440
hello welcome everybody this is a

00:00:05,790 --> 00:00:09,420
session called big data in small code

00:00:07,440 --> 00:00:12,450
and it's all about using Java rate and

00:00:09,420 --> 00:00:14,969
apache crunch to quickly develop concise

00:00:12,450 --> 00:00:16,619
efficient and testable data pipelines I

00:00:14,969 --> 00:00:17,910
got to tell you a little bit about

00:00:16,619 --> 00:00:19,500
apache crunch and going to tell you a

00:00:17,910 --> 00:00:21,090
bit about the new crunch lambda module

00:00:19,500 --> 00:00:23,130
i'm going to give you some real-world

00:00:21,090 --> 00:00:24,810
code examples from how we're using it at

00:00:23,130 --> 00:00:26,910
soundcloud and the impact that's had on

00:00:24,810 --> 00:00:28,980
the teams and then talk a little bit

00:00:26,910 --> 00:00:30,420
about the future at the end but before

00:00:28,980 --> 00:00:32,790
we get on to all of that let me just

00:00:30,420 --> 00:00:34,980
quickly introduce myself my name is

00:00:32,790 --> 00:00:36,300
david whiting that's me there i've been

00:00:34,980 --> 00:00:38,309
working with Hadoop and big data for

00:00:36,300 --> 00:00:40,950
about five years now first at last.fm

00:00:38,309 --> 00:00:42,600
then it's Spotify now at soundcloud in

00:00:40,950 --> 00:00:43,890
berlin so you might be able to see a

00:00:42,600 --> 00:00:45,930
theme in the kind of companies i've been

00:00:43,890 --> 00:00:47,940
working for as well as my normal

00:00:45,930 --> 00:00:49,469
day-to-day jobs I've got especially

00:00:47,940 --> 00:00:52,620
interested in improving the developer

00:00:49,469 --> 00:00:54,750
experience for big data like to make it

00:00:52,620 --> 00:00:59,129
as easy as possible to pragmatically

00:00:54,750 --> 00:01:00,539
deliver the data that people need I'm a

00:00:59,129 --> 00:01:03,379
committer on the Apache crunch project

00:01:00,539 --> 00:01:06,030
so that's my full disclosure there and

00:01:03,379 --> 00:01:07,590
I'm also a music producer in artist so

00:01:06,030 --> 00:01:09,780
you can go to there if you want to hear

00:01:07,590 --> 00:01:12,479
stuff if you want to get in contact with

00:01:09,780 --> 00:01:16,290
me Deb w at Apache dog or at da whiting

00:01:12,479 --> 00:01:19,170
in tweet abuse or comments and criticism

00:01:16,290 --> 00:01:21,509
afterwards so let's look at the state of

00:01:19,170 --> 00:01:22,979
the big data world in 2016 if you go to

00:01:21,509 --> 00:01:24,750
a lot of these conferences you hear a

00:01:22,979 --> 00:01:27,810
lot of hype about new technologies new

00:01:24,750 --> 00:01:30,240
execution platforms new ways of doing

00:01:27,810 --> 00:01:32,189
things new api's you hear a lot about

00:01:30,240 --> 00:01:33,990
Sparky hear about machine learning you

00:01:32,189 --> 00:01:36,420
hear about real-time stream processing

00:01:33,990 --> 00:01:38,549
hear about these weird advanced

00:01:36,420 --> 00:01:40,380
visualization techniques and especially

00:01:38,549 --> 00:01:41,460
now I think the trend at this conference

00:01:40,380 --> 00:01:43,320
is probably gonna be about event

00:01:41,460 --> 00:01:45,390
sourcing and doing everything from an

00:01:43,320 --> 00:01:47,070
event pace base point of view if you

00:01:45,390 --> 00:01:48,630
actually talk to the people who are

00:01:47,070 --> 00:01:51,960
working in organizations which are doing

00:01:48,630 --> 00:01:53,549
this kind of thing doing big data you'll

00:01:51,960 --> 00:01:56,369
find a lot of them are actually just

00:01:53,549 --> 00:01:57,869
using MapReduce to run etl aggregation

00:01:56,369 --> 00:02:01,350
and analytics jobs as they always have

00:01:57,869 --> 00:02:04,320
been there's a there's a huge wealth of

00:02:01,350 --> 00:02:06,110
code and legacy and infrastructure that

00:02:04,320 --> 00:02:07,890
was all built for Hadoop MapReduce and

00:02:06,110 --> 00:02:09,450
people aren't going to throw that away

00:02:07,890 --> 00:02:11,069
in a second when the new technology

00:02:09,450 --> 00:02:12,599
comes along or at least it's not very

00:02:11,069 --> 00:02:12,960
sensible to throw that away when the new

00:02:12,599 --> 00:02:16,470
techno

00:02:12,960 --> 00:02:19,500
she comes along so in short and

00:02:16,470 --> 00:02:21,000
apologies if in the audience is a person

00:02:19,500 --> 00:02:22,260
whose talk is called this but I wrote

00:02:21,000 --> 00:02:23,580
the slide before I knew this there's

00:02:22,260 --> 00:02:26,400
another tool called MapReduce is not

00:02:23,580 --> 00:02:27,960
dead tomorrow but yeah this is my my

00:02:26,400 --> 00:02:29,790
point here is that MapReduce is not dead

00:02:27,960 --> 00:02:33,780
it's still being used in all kinds of

00:02:29,790 --> 00:02:39,090
organizations it still works so let's

00:02:33,780 --> 00:02:41,370
try and make it better so in in my in my

00:02:39,090 --> 00:02:43,920
role in data infrastructure at

00:02:41,370 --> 00:02:45,870
soundcloud i'm working with what you

00:02:43,920 --> 00:02:47,730
might call the boring end of the data

00:02:45,870 --> 00:02:49,290
pipeline so we're not doing fancy

00:02:47,730 --> 00:02:51,990
analytics we're not doing a fancy

00:02:49,290 --> 00:02:53,520
machine learning we're preparing the

00:02:51,990 --> 00:02:55,470
data for all those teams to be able to

00:02:53,520 --> 00:02:57,480
do what they need to do so we're

00:02:55,470 --> 00:02:59,490
filtering out any debt bad data that

00:02:57,480 --> 00:03:01,470
might have come from the clients we're

00:02:59,490 --> 00:03:03,030
transforming data that might be in

00:03:01,470 --> 00:03:04,710
different formats so we have some

00:03:03,030 --> 00:03:07,350
clients that might emit some jason in

00:03:04,710 --> 00:03:08,790
some old schema because that client we

00:03:07,350 --> 00:03:12,180
can't update it anymore it's out in the

00:03:08,790 --> 00:03:13,590
world we also might have some new client

00:03:12,180 --> 00:03:15,870
or some server emitting the new proto

00:03:13,590 --> 00:03:18,150
buff data and we need to consolidate all

00:03:15,870 --> 00:03:20,880
that into the same format so we can

00:03:18,150 --> 00:03:24,090
process it downstream we also need to

00:03:20,880 --> 00:03:25,860
split the data take all the data of all

00:03:24,090 --> 00:03:27,960
the different event types and the time

00:03:25,860 --> 00:03:30,660
and it happens and split it into buckets

00:03:27,960 --> 00:03:32,430
but idea the type of event it is and

00:03:30,660 --> 00:03:35,310
time it happen so that means when people

00:03:32,430 --> 00:03:37,590
are processing the data later they can

00:03:35,310 --> 00:03:40,560
just read a directory on HDFS and not

00:03:37,590 --> 00:03:42,810
have to pre filter everything read too

00:03:40,560 --> 00:03:44,010
much and throw most of it away we have

00:03:42,810 --> 00:03:46,500
some correlation jobs so we can

00:03:44,010 --> 00:03:48,330
correlate related events so for example

00:03:46,500 --> 00:03:50,160
if someone on soundcloud starts

00:03:48,330 --> 00:03:51,510
listening to a track and then they seek

00:03:50,160 --> 00:03:54,060
to another part of the track and then

00:03:51,510 --> 00:03:56,760
rewind and then carry on playing we want

00:03:54,060 --> 00:03:58,800
to see all those events as one view so

00:03:56,760 --> 00:04:01,320
we have some some jobs to stack up

00:03:58,800 --> 00:04:02,970
events on top of each other because we

00:04:01,320 --> 00:04:05,550
want to post them as one pro system is

00:04:02,970 --> 00:04:09,180
one unit we also have some monitoring

00:04:05,550 --> 00:04:10,500
workloads monitoring the Council of

00:04:09,180 --> 00:04:11,880
events and the types of events we're

00:04:10,500 --> 00:04:12,900
getting because that's a really good

00:04:11,880 --> 00:04:16,230
sign that something might have gone

00:04:12,900 --> 00:04:18,630
wrong if we see big spikes or dips in

00:04:16,230 --> 00:04:21,030
the counts and then some simple pre

00:04:18,630 --> 00:04:24,390
aggregation jobs which feed into

00:04:21,030 --> 00:04:25,190
analytics or other databases like

00:04:24,390 --> 00:04:29,060
Cassandra

00:04:25,190 --> 00:04:30,710
that kind of thing but we have a problem

00:04:29,060 --> 00:04:33,140
all of this stuff is written in

00:04:30,710 --> 00:04:36,290
MapReduce or at least it was six months

00:04:33,140 --> 00:04:38,510
ago but the MapReduce API hasn't dated

00:04:36,290 --> 00:04:40,430
very well it hasn't aged very well since

00:04:38,510 --> 00:04:44,600
about ten years or something it's been

00:04:40,430 --> 00:04:46,340
around maybe more the rest of the way we

00:04:44,600 --> 00:04:48,920
interact with data has moved on a lot

00:04:46,340 --> 00:04:50,630
since then and if you're still writing

00:04:48,920 --> 00:04:52,310
code against the original MapReduce API

00:04:50,630 --> 00:04:55,250
you're probably getting a bit frustrated

00:04:52,310 --> 00:04:57,860
with how much you have to do which is

00:04:55,250 --> 00:05:00,860
where Apache crunch comes in Apache

00:04:57,860 --> 00:05:02,600
crunch is an attempt to make that

00:05:00,860 --> 00:05:05,420
developer experience for MapReduce a

00:05:02,600 --> 00:05:06,800
whole lot better and more modern so I'm

00:05:05,420 --> 00:05:08,960
going to take a quote from the Apache

00:05:06,800 --> 00:05:11,060
crunch web page because it explains it

00:05:08,960 --> 00:05:12,680
better than I can and that's that the

00:05:11,060 --> 00:05:14,390
Apache crunch Java library provides a

00:05:12,680 --> 00:05:16,340
framework for writing testing and

00:05:14,390 --> 00:05:17,570
running MapReduce pipelines its goal is

00:05:16,340 --> 00:05:19,430
to make pipelines which are composed of

00:05:17,570 --> 00:05:22,150
many user-defined functions simple to

00:05:19,430 --> 00:05:24,440
write easy to test and efficient to run

00:05:22,150 --> 00:05:26,360
so in practice that means it's

00:05:24,440 --> 00:05:27,770
functional you express everything as a

00:05:26,360 --> 00:05:30,800
series of functional transformations

00:05:27,770 --> 00:05:33,830
it's typesafe rather than fields based

00:05:30,800 --> 00:05:35,630
as many other products in this space are

00:05:33,830 --> 00:05:37,430
so that means is particularly

00:05:35,630 --> 00:05:38,960
well-suited for the boring end of the

00:05:37,430 --> 00:05:40,970
pipeline when you're dealing with big

00:05:38,960 --> 00:05:43,940
structured records that you have to

00:05:40,970 --> 00:05:45,860
propagate intact and you can't deal with

00:05:43,940 --> 00:05:48,320
any mismatch and that data and you have

00:05:45,860 --> 00:05:50,480
a strict schema to adhere to everything

00:05:48,320 --> 00:05:51,590
runs as MapReduce there's no extra

00:05:50,480 --> 00:05:53,990
software you have to install on your

00:05:51,590 --> 00:05:55,580
Hadoop cluster you just build it against

00:05:53,990 --> 00:05:58,400
crunch and that gives you a jar you can

00:05:55,580 --> 00:06:01,370
run with Hadoop jar or yarn jar on your

00:05:58,400 --> 00:06:02,870
cluster there is there is a zero

00:06:01,370 --> 00:06:06,200
boilerplate requirement unless you're

00:06:02,870 --> 00:06:09,080
doing anything particularly fancy none

00:06:06,200 --> 00:06:11,450
of that setup and configuration and that

00:06:09,080 --> 00:06:14,900
whole ceremony about writing that

00:06:11,450 --> 00:06:17,180
produced job supplies it also has a

00:06:14,900 --> 00:06:19,160
switchable implementation which ones in

00:06:17,180 --> 00:06:21,919
memory so that makes it really easy to

00:06:19,160 --> 00:06:23,480
run unit tests of your pipelines and

00:06:21,919 --> 00:06:26,180
also the way you construct your

00:06:23,480 --> 00:06:28,250
pipelines is comprised of standard Java

00:06:26,180 --> 00:06:31,160
functions so if you want to do low-level

00:06:28,250 --> 00:06:34,580
unit testing you're just testing normal

00:06:31,160 --> 00:06:35,900
functions and it's used at a despite not

00:06:34,580 --> 00:06:37,580
being the coolest or the most trendy

00:06:35,900 --> 00:06:38,350
thing it's actually used at a surprising

00:06:37,580 --> 00:06:40,540
number of major

00:06:38,350 --> 00:06:42,550
Jose shins we're using it soundcloud

00:06:40,540 --> 00:06:44,680
before i used it at spotify it's used

00:06:42,550 --> 00:06:46,120
its owner it's used a whole host of

00:06:44,680 --> 00:06:47,470
companies that don't really speak

00:06:46,120 --> 00:06:49,390
publicly about the stuff they're doing

00:06:47,470 --> 00:06:50,800
so i can't tell you what they are but

00:06:49,390 --> 00:06:54,430
i've spoken to people who work there who

00:06:50,800 --> 00:06:57,310
use crunch so it's pretty stable it's

00:06:54,430 --> 00:06:59,440
pretty usable so the basic approach

00:06:57,310 --> 00:07:00,760
behind crunch should be familiar to

00:06:59,440 --> 00:07:03,280
anyone who's seen any of the more modern

00:07:00,760 --> 00:07:06,130
api's for developing with data and

00:07:03,280 --> 00:07:09,520
that's that each data set as you read it

00:07:06,130 --> 00:07:12,880
as you in every stage of processing is

00:07:09,520 --> 00:07:16,780
an immutable lazily evaluated collection

00:07:12,880 --> 00:07:18,250
of of records so you take a collection

00:07:16,780 --> 00:07:19,390
and then you apply a transformation you

00:07:18,250 --> 00:07:20,410
get another collection you apply a

00:07:19,390 --> 00:07:22,780
transformation you get another

00:07:20,410 --> 00:07:24,730
collection and at some point when you're

00:07:22,780 --> 00:07:26,320
done you write it to HDFS and that's

00:07:24,730 --> 00:07:28,690
when the whole thing gets evaluated a

00:07:26,320 --> 00:07:31,690
query planner comes along and turns it

00:07:28,690 --> 00:07:33,190
into MapReduce jobs and then writes out

00:07:31,690 --> 00:07:36,550
runs the jobs writes everything out to

00:07:33,190 --> 00:07:39,370
HDFS so if you're familiar with spark

00:07:36,550 --> 00:07:44,380
and hardy DS a spark idd is sort of like

00:07:39,370 --> 00:07:47,110
a crunch p collection and the API I'm

00:07:44,380 --> 00:07:48,280
not sure why Britain gr8 stream API it's

00:07:47,110 --> 00:07:50,040
not quite relevant to this bit it's

00:07:48,280 --> 00:07:53,230
relevant to the next day nevermind so

00:07:50,040 --> 00:07:54,970
still 10 years on Hadoop everyone likes

00:07:53,230 --> 00:07:57,970
to see this example it's mostly a tool

00:07:54,970 --> 00:08:00,880
for counting words so in crunch land it

00:07:57,970 --> 00:08:02,560
looks a little bit like this so from the

00:08:00,880 --> 00:08:04,060
top this is the whole code by the way if

00:08:02,560 --> 00:08:07,000
you put this in a public static void

00:08:04,060 --> 00:08:09,880
main then put it in a jar it'll run as a

00:08:07,000 --> 00:08:12,280
MapReduce job on the cluster so we're

00:08:09,880 --> 00:08:13,990
reading from a text file in HDFS then we

00:08:12,280 --> 00:08:16,540
have this parallel do function which

00:08:13,990 --> 00:08:18,430
takes each line of text we split it by

00:08:16,540 --> 00:08:20,680
space iterate through the words in the

00:08:18,430 --> 00:08:22,960
line omit the word as the record to

00:08:20,680 --> 00:08:24,910
output then this built-in count function

00:08:22,960 --> 00:08:27,490
counts the number of instances of each

00:08:24,910 --> 00:08:29,320
word then we have another parallel do

00:08:27,490 --> 00:08:31,180
thing which takes the pair of strings

00:08:29,320 --> 00:08:34,390
along so that's the word and the count

00:08:31,180 --> 00:08:37,840
and formats it for output by just here

00:08:34,390 --> 00:08:39,610
taking the word colon count then we

00:08:37,840 --> 00:08:42,310
write to the text file and then when

00:08:39,610 --> 00:08:44,260
right crunch done that means it's going

00:08:42,310 --> 00:08:46,440
to do all the query planning and the

00:08:44,260 --> 00:08:49,950
execution and

00:08:46,440 --> 00:08:52,900
run your jobs right everything to HDFS

00:08:49,950 --> 00:08:56,020
now this is already a big improvement on

00:08:52,900 --> 00:08:57,700
working with the normal MapReduce api we

00:08:56,020 --> 00:09:00,010
haven't had to do any mappers and

00:08:57,700 --> 00:09:03,070
reducers and implementing classes and

00:09:00,010 --> 00:09:04,180
passing class names to everything but

00:09:03,070 --> 00:09:05,560
there's still a lot of noise here

00:09:04,180 --> 00:09:07,990
there's still a lot of redundant

00:09:05,560 --> 00:09:09,670
information if you can see when we're

00:09:07,990 --> 00:09:12,100
doing these do FM's and these map FM's

00:09:09,670 --> 00:09:14,230
we're expressing the type information of

00:09:12,100 --> 00:09:17,380
the inputs and the outputs at least

00:09:14,230 --> 00:09:21,490
three times in each of those and that's

00:09:17,380 --> 00:09:23,890
too many and also it hinders readability

00:09:21,490 --> 00:09:25,930
if you're trying to pass all of this and

00:09:23,890 --> 00:09:27,310
you've got all this extra noise around

00:09:25,930 --> 00:09:28,990
the anonymous inner classes in the way

00:09:27,310 --> 00:09:33,340
because actually what we're really

00:09:28,990 --> 00:09:34,720
interested in is this which is a whole

00:09:33,340 --> 00:09:39,940
lot less information and a whole lot

00:09:34,720 --> 00:09:42,550
easier to read now luckily for us java 8

00:09:39,940 --> 00:09:45,100
is out and now means that we don't have

00:09:42,550 --> 00:09:46,620
to do this whole ceremony of anonymous

00:09:45,100 --> 00:09:50,320
inner classes if we want to pass around

00:09:46,620 --> 00:09:55,720
functions that we've defined at the at

00:09:50,320 --> 00:09:58,900
the cool side so in about late last year

00:09:55,720 --> 00:10:01,030
about October November time we at

00:09:58,900 --> 00:10:03,070
soundcloud upgraded our Hadoop cluster

00:10:01,030 --> 00:10:05,710
to the next version along and in the

00:10:03,070 --> 00:10:07,390
process we also upgraded to Java 8 and

00:10:05,710 --> 00:10:09,820
that gives us the ability to run Java 8

00:10:07,390 --> 00:10:11,230
stuff on our Hadoop cluster I don't

00:10:09,820 --> 00:10:12,910
think it's officially supported by

00:10:11,230 --> 00:10:17,280
Hadoop yet but we've been running it on

00:10:12,910 --> 00:10:21,070
cloud errors cdh 5 with no problems so

00:10:17,280 --> 00:10:22,930
it seems to be pretty stable for us but

00:10:21,070 --> 00:10:25,090
the trouble was there was no compatible

00:10:22,930 --> 00:10:27,580
Java 8 API for using with crunch and

00:10:25,090 --> 00:10:30,040
because crunch is still very much based

00:10:27,580 --> 00:10:31,810
on abstract classes rather than

00:10:30,040 --> 00:10:34,390
interfaces it doesn't it there is no

00:10:31,810 --> 00:10:38,470
drop-in replacement for him so in

00:10:34,390 --> 00:10:41,320
January of this year I wrote a new a new

00:10:38,470 --> 00:10:43,660
API for crunch based on Java 8 called

00:10:41,320 --> 00:10:45,970
crunch lambda and the idea behind it was

00:10:43,660 --> 00:10:50,170
to take advantage of all Java eights

00:10:45,970 --> 00:10:54,040
features in method references and lambda

00:10:50,170 --> 00:10:56,350
expressions and the streams API and wrap

00:10:54,040 --> 00:10:59,050
crunch in that so you can interact with

00:10:56,350 --> 00:11:00,540
it in a way that feels as native to Java

00:10:59,050 --> 00:11:04,660
8 as cost

00:11:00,540 --> 00:11:06,670
so that means we work with streams

00:11:04,660 --> 00:11:08,860
instead of iterators and iterate balls

00:11:06,670 --> 00:11:10,930
and all those logic that we saw

00:11:08,860 --> 00:11:14,260
expressed as these do FM's and map

00:11:10,930 --> 00:11:18,700
offense are now just lambda expressions

00:11:14,260 --> 00:11:20,920
or method references so as of last month

00:11:18,700 --> 00:11:25,240
that is now in the main crunch release

00:11:20,920 --> 00:11:26,410
oh Fortino and ready for use and it

00:11:25,240 --> 00:11:30,700
works pretty well we're using it a

00:11:26,410 --> 00:11:32,320
soundcloud so if you take the example I

00:11:30,700 --> 00:11:35,140
just gave you and put that into crunch

00:11:32,320 --> 00:11:37,930
lambda work world it looks a little bit

00:11:35,140 --> 00:11:39,250
like this now the first thing you can

00:11:37,930 --> 00:11:42,220
notice about this is there's a lot less

00:11:39,250 --> 00:11:44,410
code but the second thing like I'm not

00:11:42,220 --> 00:11:46,750
one to cut down code volume arbitrarily

00:11:44,410 --> 00:11:48,130
if it's not saving anything but the most

00:11:46,750 --> 00:11:50,230
important thing about this as I think

00:11:48,130 --> 00:11:52,720
it's more a lot more intuitive and easy

00:11:50,230 --> 00:11:55,410
to read you can follow the structure of

00:11:52,720 --> 00:11:57,970
the program a lot more easily see

00:11:55,410 --> 00:11:59,890
reading from the text line we're taking

00:11:57,970 --> 00:12:03,420
each line we're splitting it by space

00:11:59,890 --> 00:12:06,790
but instead of doing this in a pair of

00:12:03,420 --> 00:12:09,520
input and omit we're doing a flat map

00:12:06,790 --> 00:12:11,320
which is usually a bit more intuitive

00:12:09,520 --> 00:12:13,570
for people to think about as the

00:12:11,320 --> 00:12:16,360
transformation from one record 20 or

00:12:13,570 --> 00:12:18,340
more records we do a count operation and

00:12:16,360 --> 00:12:20,470
then we're mapping them to the same

00:12:18,340 --> 00:12:22,090
formats before writing out of the text

00:12:20,470 --> 00:12:24,640
file and the only thing that's different

00:12:22,090 --> 00:12:26,470
about this in terms of using the API as

00:12:24,640 --> 00:12:30,480
we've wrapped the original collection we

00:12:26,470 --> 00:12:34,690
read from the data in this lambda trap

00:12:30,480 --> 00:12:36,430
so that's crunch lambda it's fairly

00:12:34,690 --> 00:12:38,320
simple it's a really lightweight wrapper

00:12:36,430 --> 00:12:41,530
so it uses all the crunch execution

00:12:38,320 --> 00:12:44,860
underneath in order to understand a bit

00:12:41,530 --> 00:12:46,360
more about crunch it helps to look a

00:12:44,860 --> 00:12:50,680
little bit at the data model behind it

00:12:46,360 --> 00:12:52,720
so in in the world of crunch these three

00:12:50,680 --> 00:12:55,440
types of collection there's AP

00:12:52,720 --> 00:12:58,180
collection a p table and a P group table

00:12:55,440 --> 00:13:01,690
in a P collection you're just looking at

00:12:58,180 --> 00:13:03,730
individual records of data so that could

00:13:01,690 --> 00:13:05,530
be a string if it was lines of text in a

00:13:03,730 --> 00:13:08,410
fire or it could be numbers if you've

00:13:05,530 --> 00:13:10,120
got some numeric data set to process in

00:13:08,410 --> 00:13:12,280
our case it's most offered in structured

00:13:10,120 --> 00:13:13,100
records so we have all of our event data

00:13:12,280 --> 00:13:17,000
stored as

00:13:13,100 --> 00:13:18,080
buff's in sequence files on HDFS so when

00:13:17,000 --> 00:13:20,410
we have a collection of things it's

00:13:18,080 --> 00:13:22,670
usually a collection of some event type

00:13:20,410 --> 00:13:24,620
but the important distinction between

00:13:22,670 --> 00:13:26,810
that and other types is that according

00:13:24,620 --> 00:13:28,010
to crunch it doesn't know about the

00:13:26,810 --> 00:13:31,430
structure in the data readjust knows

00:13:28,010 --> 00:13:35,060
that you have one event so in their

00:13:31,430 --> 00:13:36,830
table representation you can go from in

00:13:35,060 --> 00:13:38,720
the table representation crunch knows

00:13:36,830 --> 00:13:41,390
about a separate key part and a value

00:13:38,720 --> 00:13:43,130
part of each record so here you might

00:13:41,390 --> 00:13:45,740
take your structured record and extract

00:13:43,130 --> 00:13:47,420
a key from it to create a key part of

00:13:45,740 --> 00:13:50,450
the record as well or you might be

00:13:47,420 --> 00:13:52,460
reading data from HDFS in for example a

00:13:50,450 --> 00:13:54,170
sequence file which has a key in a value

00:13:52,460 --> 00:13:57,140
part already so that's naturally

00:13:54,170 --> 00:13:58,700
represented as a table so in this

00:13:57,140 --> 00:14:01,040
example you can see that some of the

00:13:58,700 --> 00:14:02,960
value rose share the same key and some

00:14:01,040 --> 00:14:05,090
of them have different keys so when we

00:14:02,960 --> 00:14:07,520
when we take the table when we group by

00:14:05,090 --> 00:14:08,900
key we go to a group table and that's

00:14:07,520 --> 00:14:10,910
where we collect together all the values

00:14:08,900 --> 00:14:13,160
for each unique key so if you do a

00:14:10,910 --> 00:14:14,840
transformation from a table to a group

00:14:13,160 --> 00:14:18,950
table and then back to a collection or a

00:14:14,840 --> 00:14:21,530
table that pretty much in all cases gets

00:14:18,950 --> 00:14:22,940
run as a MapReduce cycle so when you go

00:14:21,530 --> 00:14:24,140
from the table to the group's table

00:14:22,940 --> 00:14:27,110
that's when you're going from the map

00:14:24,140 --> 00:14:28,880
side to the reduced side if you if it

00:14:27,110 --> 00:14:32,330
helps you to think about in terms of

00:14:28,880 --> 00:14:33,800
MapReduce originally so with those three

00:14:32,330 --> 00:14:34,970
types i'm going to show you another

00:14:33,800 --> 00:14:36,350
slide that everyone tells me i should

00:14:34,970 --> 00:14:37,490
remove from this presentation because

00:14:36,350 --> 00:14:41,870
it's confusing of these arrows

00:14:37,490 --> 00:14:44,030
everywhere but we're actually missing

00:14:41,870 --> 00:14:45,020
the left part of it as well but this is

00:14:44,030 --> 00:14:46,520
just showing you the different

00:14:45,020 --> 00:14:48,250
transformations between the collections

00:14:46,520 --> 00:14:50,720
and how you get from one to the other so

00:14:48,250 --> 00:14:53,270
we can read in data and get a collection

00:14:50,720 --> 00:14:54,950
or a table and when we have a table with

00:14:53,270 --> 00:14:57,020
a key in a value we can group by key to

00:14:54,950 --> 00:14:58,850
go to a group table and then in group

00:14:57,020 --> 00:15:02,330
table you see all these operations for

00:14:58,850 --> 00:15:04,340
aggregating values by keys so we have

00:15:02,330 --> 00:15:05,990
ways to combine them or providing a

00:15:04,340 --> 00:15:08,630
reduced function or collect them into a

00:15:05,990 --> 00:15:11,440
collection or do some other kind of

00:15:08,630 --> 00:15:11,440
custom processing

00:15:12,090 --> 00:15:18,370
so that's all well and good and seem

00:15:15,850 --> 00:15:21,460
sensible but what if we're not running a

00:15:18,370 --> 00:15:24,070
word count company it's quite often in

00:15:21,460 --> 00:15:26,020
these kind of talks to give a couple of

00:15:24,070 --> 00:15:28,120
twigs amples and then say here you go

00:15:26,020 --> 00:15:30,550
off you go run with it without any

00:15:28,120 --> 00:15:31,750
examples from the real world so I'm

00:15:30,550 --> 00:15:33,760
going to take you through a couple of

00:15:31,750 --> 00:15:37,779
examples on how we're using crunch right

00:15:33,760 --> 00:15:40,680
now in soundcloud to do some one simple

00:15:37,779 --> 00:15:42,670
workload and one more complex workload

00:15:40,680 --> 00:15:44,770
so the first example I'm going to talk

00:15:42,670 --> 00:15:46,720
to you about is all about monitoring

00:15:44,770 --> 00:15:48,670
event counts so as I mentioned before

00:15:46,720 --> 00:15:50,560
it's really useful to monitor the

00:15:48,670 --> 00:15:52,570
Council of events as they come through

00:15:50,560 --> 00:15:54,910
to look for anomalies that might be

00:15:52,570 --> 00:15:57,089
happening in the clients or anomalies

00:15:54,910 --> 00:16:00,339
that might be happening in our transport

00:15:57,089 --> 00:16:02,080
our event transport system a quite

00:16:00,339 --> 00:16:03,880
common case is that someone pushes out a

00:16:02,080 --> 00:16:05,740
new client and they don't tell us about

00:16:03,880 --> 00:16:07,360
it and they've slightly changed the

00:16:05,740 --> 00:16:08,740
schema so they're emitting a string when

00:16:07,360 --> 00:16:10,930
they're in they used to be emitting an

00:16:08,740 --> 00:16:13,000
int or something and then the count for

00:16:10,930 --> 00:16:15,820
that massively drops because we can't

00:16:13,000 --> 00:16:19,470
pass it in our system so it's really

00:16:15,820 --> 00:16:19,470
useful to monitor counts over time and

00:16:19,530 --> 00:16:25,120
one thing we do is just count events by

00:16:23,170 --> 00:16:27,310
the type of events that might be sound

00:16:25,120 --> 00:16:29,890
played or user click to like button

00:16:27,310 --> 00:16:32,290
something like that and applications

00:16:29,890 --> 00:16:36,970
that would be like iOS Android web

00:16:32,290 --> 00:16:40,450
something like that so we do that by

00:16:36,970 --> 00:16:42,220
using a metric system called prometheus

00:16:40,450 --> 00:16:43,690
if you don't know about it it's a really

00:16:42,220 --> 00:16:45,730
good way of tracking metrics and doing

00:16:43,690 --> 00:16:47,200
alerts for your services and things

00:16:45,730 --> 00:16:49,750
there's another open source project that

00:16:47,200 --> 00:16:51,070
came out of soundcloud so if you wanna

00:16:49,750 --> 00:16:53,410
know more about prometheus just as an

00:16:51,070 --> 00:16:55,150
aside go to Prometheus taio it's growing

00:16:53,410 --> 00:16:58,600
way beyond soundcloud now and it's there

00:16:55,150 --> 00:17:00,610
it's a big thing but here we're going to

00:16:58,600 --> 00:17:02,170
use the push gateway which is a way of

00:17:00,610 --> 00:17:04,360
pushing metrics that Prometheus where

00:17:02,170 --> 00:17:06,100
prometheus is normally a pull system and

00:17:04,360 --> 00:17:07,660
so you push it to the push gateway it

00:17:06,100 --> 00:17:08,949
hangs around on there for the Prometheus

00:17:07,660 --> 00:17:12,120
scraper to come around and scrape the

00:17:08,949 --> 00:17:14,380
metrics so we create the crunch pipeline

00:17:12,120 --> 00:17:16,990
using the MapReduce implementation

00:17:14,380 --> 00:17:19,240
create a push gateway client with which

00:17:16,990 --> 00:17:21,310
is just an HTTP m point that we push the

00:17:19,240 --> 00:17:22,929
metrics at this gauge thing is a thing

00:17:21,310 --> 00:17:24,970
from prometheus so we're creating a

00:17:22,929 --> 00:17:26,770
gauge with event count

00:17:24,970 --> 00:17:28,419
as the metric and two labels which is

00:17:26,770 --> 00:17:32,679
the event type in the application so

00:17:28,419 --> 00:17:34,600
sound played and iOS for example we

00:17:32,679 --> 00:17:37,210
create that gage and then we read the

00:17:34,600 --> 00:17:39,039
events into a table this read events

00:17:37,210 --> 00:17:41,080
function is a little bit more complex

00:17:39,039 --> 00:17:42,820
than just read from text file or

00:17:41,080 --> 00:17:45,900
something so i factored that into a

00:17:42,820 --> 00:17:49,000
function i'm not showing that here but

00:17:45,900 --> 00:17:51,309
when we've done that we have a

00:17:49,000 --> 00:17:53,500
collection of key value pairs in the

00:17:51,309 --> 00:17:56,530
table which is a long time stamp and a

00:17:53,500 --> 00:17:58,150
structured protobuf event type but we're

00:17:56,530 --> 00:17:59,679
not interested in the timestamp because

00:17:58,150 --> 00:18:01,570
in the read process we are already

00:17:59,679 --> 00:18:03,250
bucketing it by time so we throw away

00:18:01,570 --> 00:18:06,309
the key part which is what this values

00:18:03,250 --> 00:18:08,049
thing does and then we map it through

00:18:06,309 --> 00:18:10,330
just a plain old java function which is

00:18:08,049 --> 00:18:13,480
creating the stats key object from the

00:18:10,330 --> 00:18:15,010
event and that stats key is just

00:18:13,480 --> 00:18:17,650
comprised of the event type and

00:18:15,010 --> 00:18:18,820
application in the real world is

00:18:17,650 --> 00:18:20,409
actually a few more fields in there

00:18:18,820 --> 00:18:21,970
which is why I justify having a separate

00:18:20,409 --> 00:18:25,120
type for it but I've slightly simplified

00:18:21,970 --> 00:18:26,830
it for this then again just like the

00:18:25,120 --> 00:18:30,370
word count example we're counting the

00:18:26,830 --> 00:18:32,230
number of events for each stats key then

00:18:30,370 --> 00:18:33,340
we use this materialized function and

00:18:32,230 --> 00:18:34,840
the materialized function is really

00:18:33,340 --> 00:18:37,770
powerful feature of the crunch

00:18:34,840 --> 00:18:41,350
abstraction while materialized does is

00:18:37,770 --> 00:18:43,870
write out the results at that point to a

00:18:41,350 --> 00:18:46,659
file so it will run the MapReduce job

00:18:43,870 --> 00:18:48,070
right to a file on HDFS and then open

00:18:46,659 --> 00:18:50,440
the file for reading and give you a

00:18:48,070 --> 00:18:53,020
pointer into the data locally from where

00:18:50,440 --> 00:18:55,360
you're running the job so all of this

00:18:53,020 --> 00:18:58,179
the map and the count has been run on

00:18:55,360 --> 00:18:59,679
the to cluster and then we when we use

00:18:58,179 --> 00:19:02,200
materialized we're bringing that data

00:18:59,679 --> 00:19:04,539
locally and running some more code with

00:19:02,200 --> 00:19:09,429
it so it gives us a Java 8 stream of

00:19:04,539 --> 00:19:11,140
those stats key and count 2 pairs so

00:19:09,429 --> 00:19:13,659
then when we have all that data locally

00:19:11,140 --> 00:19:15,820
we can just iterate over it in this in

00:19:13,659 --> 00:19:18,640
this application itself so that's the

00:19:15,820 --> 00:19:20,950
stream style for each here we take the

00:19:18,640 --> 00:19:23,260
statue of stats record apply the labels

00:19:20,950 --> 00:19:24,520
to the gauge and set the count then

00:19:23,260 --> 00:19:26,770
finally at the end of it we can push all

00:19:24,520 --> 00:19:28,780
the metrics to the Prometheus push

00:19:26,770 --> 00:19:31,539
gateway and then on the Prometheus push

00:19:28,780 --> 00:19:33,789
gateway we set up graphs and we can set

00:19:31,539 --> 00:19:35,710
up a loading rules so if any of these

00:19:33,789 --> 00:19:37,709
keys is much higher or much lower than

00:19:35,710 --> 00:19:40,629
it was yesterday or last week

00:19:37,709 --> 00:19:41,739
then we can fire off alerts wake someone

00:19:40,629 --> 00:19:44,379
up in the middle of the night or

00:19:41,739 --> 00:19:46,509
something like that so that's a really

00:19:44,379 --> 00:19:49,719
simple use case for how we're using it

00:19:46,509 --> 00:19:53,429
in our event transport and delivery

00:19:49,719 --> 00:19:56,950
pipeline as a just kind of last check

00:19:53,429 --> 00:19:58,959
very simple very simple check but it

00:19:56,950 --> 00:20:04,509
also helps us notice a lot of failures

00:19:58,959 --> 00:20:05,589
earlier than we ordinarily would so the

00:20:04,509 --> 00:20:12,609
second example I'm going to take you

00:20:05,589 --> 00:20:15,159
through is about a user facing product

00:20:12,609 --> 00:20:17,409
and that's the the SoundCloud stats

00:20:15,159 --> 00:20:19,029
product if you're a creator on

00:20:17,409 --> 00:20:23,259
soundcloud enough is anyone here a

00:20:19,029 --> 00:20:25,119
creator on soundcloud okay so got one at

00:20:23,259 --> 00:20:29,529
the back so if you're an artist on

00:20:25,119 --> 00:20:31,119
soundcloud sound creator then you have

00:20:29,529 --> 00:20:33,849
access to a stats product which gives

00:20:31,119 --> 00:20:36,669
you feedback about how people are

00:20:33,849 --> 00:20:41,019
listening to your sounds so this is mine

00:20:36,669 --> 00:20:43,209
for one of my artists pages so you can

00:20:41,019 --> 00:20:45,909
see all kinds of data about when people

00:20:43,209 --> 00:20:47,739
have listened to me how many plays how

00:20:45,909 --> 00:20:50,739
many likes how many comments but in the

00:20:47,739 --> 00:20:53,829
bottom half of this you see some top

00:20:50,739 --> 00:20:56,200
lists explaining different dimensions so

00:20:53,829 --> 00:20:58,629
we have the most played tracks by this

00:20:56,200 --> 00:21:01,599
artist we have the countries in which

00:20:58,629 --> 00:21:03,399
the tracks by this artist are played the

00:21:01,599 --> 00:21:05,619
most we have which users are listening

00:21:03,399 --> 00:21:08,320
to the artists most and we have which

00:21:05,619 --> 00:21:10,719
websites they get to your page from so

00:21:08,320 --> 00:21:13,149
it's all about giving sound creators a

00:21:10,719 --> 00:21:15,639
bit of insight into how people are

00:21:13,149 --> 00:21:18,190
listening and maybe like if you see one

00:21:15,639 --> 00:21:20,499
country spiking than you might think ok

00:21:18,190 --> 00:21:22,570
now I should try and engage listeners in

00:21:20,499 --> 00:21:23,889
that country or if you see one track

00:21:22,570 --> 00:21:25,389
that you're not expecting to be popular

00:21:23,889 --> 00:21:26,769
is more popular than all the rest of

00:21:25,389 --> 00:21:28,149
your tracks maybe it's time to

00:21:26,769 --> 00:21:30,669
investigate why and see if you can do

00:21:28,149 --> 00:21:33,509
something with them so it's really

00:21:30,669 --> 00:21:37,479
powerful data for the creators to use

00:21:33,509 --> 00:21:39,059
and the the backend for this is

00:21:37,479 --> 00:21:42,849
implemented again as a crunch pie

00:21:39,059 --> 00:21:45,819
pumpkin chai pipeline so this is the the

00:21:42,849 --> 00:21:46,989
main part of the code for this before

00:21:45,819 --> 00:21:48,849
this there's a little bit that reads the

00:21:46,989 --> 00:21:50,250
events and at the end of this there's a

00:21:48,849 --> 00:21:52,800
part of it which

00:21:50,250 --> 00:21:54,630
takes all the output data and formats

00:21:52,800 --> 00:21:56,760
into records which get loaded into

00:21:54,630 --> 00:21:59,520
Cassandra we actually create the SS

00:21:56,760 --> 00:22:01,320
tables and push them from the MapReduce

00:21:59,520 --> 00:22:02,250
job itself but I've left that bit out

00:22:01,320 --> 00:22:05,070
because I want to talk about the

00:22:02,250 --> 00:22:07,260
processing steps so this is a little bit

00:22:05,070 --> 00:22:09,960
more complex example it's going to

00:22:07,260 --> 00:22:11,580
manifest as two separate MapReduce type

00:22:09,960 --> 00:22:13,890
cycles but let's go through it

00:22:11,580 --> 00:22:17,460
step-by-step so we have an event stream

00:22:13,890 --> 00:22:19,400
of events that happen somewhere in

00:22:17,460 --> 00:22:22,380
soundcloud so here we have an example

00:22:19,400 --> 00:22:24,960
sound sound played so the sound has been

00:22:22,380 --> 00:22:28,620
played by David that's me in Germany on

00:22:24,960 --> 00:22:30,390
the web client now there's a function

00:22:28,620 --> 00:22:34,440
called create facts from event that

00:22:30,390 --> 00:22:38,100
generates some one or more facts from

00:22:34,440 --> 00:22:39,510
that event so in this case there's three

00:22:38,100 --> 00:22:41,640
facts that come from that event one is

00:22:39,510 --> 00:22:44,130
that that the sound has been played in

00:22:41,640 --> 00:22:45,840
Germany and next fact is that the sound

00:22:44,130 --> 00:22:47,040
has been played on a web client and the

00:22:45,840 --> 00:22:49,890
next fact is that the sound has been

00:22:47,040 --> 00:22:52,050
played by the user David and we emit a

00:22:49,890 --> 00:22:54,240
what's called a bucket is the key part

00:22:52,050 --> 00:22:57,690
of this and a value of one for each one

00:22:54,240 --> 00:22:58,890
of those so we're driving like more than

00:22:57,690 --> 00:23:00,900
one piece of information from each

00:22:58,890 --> 00:23:02,310
factor from each event that comes in and

00:23:00,900 --> 00:23:05,130
then we have other event types like

00:23:02,310 --> 00:23:09,390
sound favorited or reposted or something

00:23:05,130 --> 00:23:10,920
like that then we group those by key and

00:23:09,390 --> 00:23:12,210
we sum up all the counts so we can see

00:23:10,920 --> 00:23:14,370
that this track was played in Germany

00:23:12,210 --> 00:23:17,610
twice for example with that input data

00:23:14,370 --> 00:23:21,210
of two records for the Germany bucket

00:23:17,610 --> 00:23:23,430
and one record for the UK bucket and

00:23:21,210 --> 00:23:25,860
then after we've summed up those counts

00:23:23,430 --> 00:23:28,950
so when we do this reduced values that's

00:23:25,860 --> 00:23:31,590
when it will happen on the reduced side

00:23:28,950 --> 00:23:33,180
of the MapReduce we also need to filter

00:23:31,590 --> 00:23:35,640
the values because some events can

00:23:33,180 --> 00:23:37,800
generate negative facts so if if someone

00:23:35,640 --> 00:23:39,830
unfollow someone or unfavorites

00:23:37,800 --> 00:23:42,150
something it can create a negative count

00:23:39,830 --> 00:23:44,490
and it turns out if you present the data

00:23:42,150 --> 00:23:46,890
to users with negative favorites or

00:23:44,490 --> 00:23:48,480
negative reposts then they're not so

00:23:46,890 --> 00:23:49,680
happy about it so we filter out the

00:23:48,480 --> 00:23:53,610
negative counts because the user don't

00:23:49,680 --> 00:23:55,910
like seeing those and then we move on to

00:23:53,610 --> 00:23:58,560
the next step and the next step is about

00:23:55,910 --> 00:24:00,660
we have to shuffle shuffle the key in

00:23:58,560 --> 00:24:02,640
the value parts of the record so what

00:24:00,660 --> 00:24:03,690
when it used to be a bucket play country

00:24:02,640 --> 00:24:07,009
Germany

00:24:03,690 --> 00:24:11,129
to what we're now interested in is some

00:24:07,009 --> 00:24:13,200
aggregating things by the by the column

00:24:11,129 --> 00:24:15,539
in that topless view that I showed you

00:24:13,200 --> 00:24:19,320
earlier that will be the the heading for

00:24:15,539 --> 00:24:21,210
the top list so here this is the top

00:24:19,320 --> 00:24:23,460
country this top countries that this

00:24:21,210 --> 00:24:25,429
track is played in and then the value is

00:24:23,460 --> 00:24:27,389
a topless pair of Germany and two

00:24:25,429 --> 00:24:29,250
because the idea is you want to collect

00:24:27,389 --> 00:24:33,000
all those values together and keep the

00:24:29,250 --> 00:24:34,649
top k of those so once we've shuffled

00:24:33,000 --> 00:24:36,600
the key of value part which is just this

00:24:34,649 --> 00:24:38,580
remap dimension keys just takes one

00:24:36,600 --> 00:24:40,710
thing out of one record and builds this

00:24:38,580 --> 00:24:43,559
new pair of these two parts of the

00:24:40,710 --> 00:24:45,809
record then again we do a group by key

00:24:43,559 --> 00:24:48,269
and it gives us a group table and then

00:24:45,809 --> 00:24:50,850
we can operate on the value part of this

00:24:48,269 --> 00:24:54,450
and this map values function will just

00:24:50,850 --> 00:25:00,629
give you a pair of the key and a stream

00:24:54,450 --> 00:25:02,250
of the values and excuse me give you a

00:25:00,629 --> 00:25:05,250
pair of the key and a stream of the

00:25:02,250 --> 00:25:07,440
values and then you can return some kind

00:25:05,250 --> 00:25:10,139
of record so here we're using a plain

00:25:07,440 --> 00:25:13,289
old java streaming top-k algorithm which

00:25:10,139 --> 00:25:15,179
is this fine top k so we just create a

00:25:13,289 --> 00:25:18,269
sorted set with a comparator the count

00:25:15,179 --> 00:25:21,360
reversed so we want the the biggest ones

00:25:18,269 --> 00:25:22,740
of the foot at the top and then we

00:25:21,360 --> 00:25:24,299
compare also by the dimension value

00:25:22,740 --> 00:25:25,830
which is like the country name because

00:25:24,299 --> 00:25:26,879
we want to keep this deterministic if we

00:25:25,830 --> 00:25:29,429
run it again we want to have the same

00:25:26,879 --> 00:25:31,320
output and then for every input we add

00:25:29,429 --> 00:25:33,240
it to the set evict stuff if it gets

00:25:31,320 --> 00:25:36,840
bigger than k because k in this case is

00:25:33,240 --> 00:25:40,679
something like 50 so this is perfectly

00:25:36,840 --> 00:25:43,169
sensible way of doing it so since we've

00:25:40,679 --> 00:25:44,970
been changing things around to use

00:25:43,169 --> 00:25:46,649
crunch at South out we had a when I

00:25:44,970 --> 00:25:48,899
turned up at soundcloud about eight or

00:25:46,649 --> 00:25:51,509
nine months ago we had a lot of old

00:25:48,899 --> 00:25:54,120
MapReduce code using with the original

00:25:51,509 --> 00:25:56,519
MapReduce API and some custom in-house

00:25:54,120 --> 00:26:01,200
abstraction over it which didn't really

00:25:56,519 --> 00:26:04,409
help very much then we had quite an

00:26:01,200 --> 00:26:06,840
impact at soundcloud so we see a lot of

00:26:04,409 --> 00:26:09,090
these complex MapReduce job crafts job

00:26:06,840 --> 00:26:10,679
graphs replaced with simple pipe lines

00:26:09,090 --> 00:26:12,570
which are easy to read they're easy to

00:26:10,679 --> 00:26:14,490
understand another developer can come

00:26:12,570 --> 00:26:15,559
and pick it up read it understand what

00:26:14,490 --> 00:26:19,759
it does make my

00:26:15,559 --> 00:26:21,409
locations to it so just in my team as

00:26:19,759 --> 00:26:25,039
well as those two we have the entire

00:26:21,409 --> 00:26:26,840
event cleanup and splitting and that

00:26:25,039 --> 00:26:29,379
part of the pipeline is all in crunch

00:26:26,840 --> 00:26:31,519
now and we have the job that takes

00:26:29,379 --> 00:26:35,600
related events and stacks them up

00:26:31,519 --> 00:26:37,129
related events related to sounds being

00:26:35,600 --> 00:26:39,320
played and stacks them up so we can get

00:26:37,129 --> 00:26:40,490
an accurate idea of the duration that

00:26:39,320 --> 00:26:43,129
someone has been listening to a sound

00:26:40,490 --> 00:26:44,570
for that also happens in crunch now we

00:26:43,129 --> 00:26:46,309
have four different teams using it we

00:26:44,570 --> 00:26:49,639
have our team in the data infrastructure

00:26:46,309 --> 00:26:51,230
we have the insights team the royalties

00:26:49,639 --> 00:26:53,409
and reporting team delivering their

00:26:51,230 --> 00:26:57,259
their reports to the record labels and

00:26:53,409 --> 00:27:00,350
also the trust security and safety team

00:26:57,259 --> 00:27:03,499
are doing some of their analysis using

00:27:00,350 --> 00:27:05,330
it and all of them are saying that

00:27:03,499 --> 00:27:07,879
they're they're much happier using this

00:27:05,330 --> 00:27:10,059
than MapReduce we've even had some

00:27:07,879 --> 00:27:12,409
people that were writing stuff in spark

00:27:10,059 --> 00:27:13,940
changed to using crunch because the

00:27:12,409 --> 00:27:15,529
execution is a little bit more reliable

00:27:13,940 --> 00:27:19,129
and it doesn't use so much memory on the

00:27:15,529 --> 00:27:22,730
cluster so for especially if you have a

00:27:19,129 --> 00:27:23,869
really big data set spark can sometimes

00:27:22,730 --> 00:27:26,690
be a bit if you with that when you're

00:27:23,869 --> 00:27:28,129
spinning to disk so people who were

00:27:26,690 --> 00:27:30,769
using sparker now he's in crunch and

00:27:28,129 --> 00:27:33,590
finally get more reliable and finally

00:27:30,769 --> 00:27:35,600
because the way it's much easier to test

00:27:33,590 --> 00:27:37,700
some one like this then using the

00:27:35,600 --> 00:27:39,919
MapReduce API we find the jobs they

00:27:37,700 --> 00:27:42,350
didn't have any tests at all now have

00:27:39,919 --> 00:27:44,749
tests which is a great improvement we

00:27:42,350 --> 00:27:47,629
actually have some confidence both in

00:27:44,749 --> 00:27:54,049
our existing old logic and when people

00:27:47,629 --> 00:27:57,649
are a new stuff that it might work so

00:27:54,049 --> 00:28:00,289
for the future I mean crunch is a bit of

00:27:57,649 --> 00:28:02,389
a strange case here because it's a

00:28:00,289 --> 00:28:04,700
relatively new API in the last couple of

00:28:02,389 --> 00:28:06,909
years or so to work on a relatively old

00:28:04,700 --> 00:28:09,679
technology that most people consider

00:28:06,909 --> 00:28:12,169
towards the end of life from our

00:28:09,679 --> 00:28:14,659
experience despite its zero dot version

00:28:12,169 --> 00:28:17,090
number the version numbers of cruncher

00:28:14,659 --> 00:28:20,029
extremely conservative i will consider

00:28:17,090 --> 00:28:23,629
it's been really stable for four or five

00:28:20,029 --> 00:28:25,630
versions at least now and it's feature

00:28:23,629 --> 00:28:28,120
complete it's stable it works

00:28:25,630 --> 00:28:31,540
we're using it really happily as a as a

00:28:28,120 --> 00:28:33,280
pragmatic way to keep writing MapReduce

00:28:31,540 --> 00:28:36,130
code that does the things that we need

00:28:33,280 --> 00:28:38,800
to do every day we're really excited

00:28:36,130 --> 00:28:40,240
about the Apache beem beem project I'm

00:28:38,800 --> 00:28:42,400
hoping to hear more about that I think

00:28:40,240 --> 00:28:44,950
there's a couple of talks about it at

00:28:42,400 --> 00:28:46,600
the rest of this conference which is

00:28:44,950 --> 00:28:48,220
actually based on the same paper that

00:28:46,600 --> 00:28:49,900
the crunch API was originally based on

00:28:48,220 --> 00:28:52,200
so the crunch API was based on this

00:28:49,900 --> 00:28:55,990
flume Java paper that came out of Google

00:28:52,200 --> 00:28:57,850
and apache beam is the open-source

00:28:55,990 --> 00:29:01,600
version of the google data flow api

00:28:57,850 --> 00:29:04,240
which is also an evolution of the flume

00:29:01,600 --> 00:29:06,190
java paper so it seems that all of these

00:29:04,240 --> 00:29:10,090
technologies are kind of converging on

00:29:06,190 --> 00:29:15,220
the same same API and certainly the same

00:29:10,090 --> 00:29:17,290
approach to programming so we're with

00:29:15,220 --> 00:29:18,880
beam in the pipeline where we're not

00:29:17,290 --> 00:29:20,050
worried at all about writing crunch code

00:29:18,880 --> 00:29:23,740
we don't think this is a waste of time

00:29:20,050 --> 00:29:27,270
to do because the the path from crunch

00:29:23,740 --> 00:29:29,530
to beam or spark or any of the other

00:29:27,270 --> 00:29:31,510
technologies should be relatively simple

00:29:29,530 --> 00:29:33,400
the programming model is the same the

00:29:31,510 --> 00:29:35,620
way you have to decompose your programs

00:29:33,400 --> 00:29:37,060
is the same beam is getting a lot of

00:29:35,620 --> 00:29:38,980
traction we're excited about that of

00:29:37,060 --> 00:29:42,720
course but we won't be able to use it

00:29:38,980 --> 00:29:45,310
probably until this time next year and

00:29:42,720 --> 00:29:47,980
I've even considered I'm not sure I'll

00:29:45,310 --> 00:29:49,420
do this or not but you I was discussing

00:29:47,980 --> 00:29:52,030
last night at the speaker's dinner maybe

00:29:49,420 --> 00:29:54,100
we could write a beam runner for crunch

00:29:52,030 --> 00:29:57,400
for MapReduce so you can continue to run

00:29:54,100 --> 00:29:59,170
your your your beam applications on

00:29:57,400 --> 00:30:01,510
MapReduce by crunch but it sounds a

00:29:59,170 --> 00:30:02,470
little bit like pipeline inception so

00:30:01,510 --> 00:30:04,600
I'm not sure if that's a good idea or

00:30:02,470 --> 00:30:08,130
not it might happen oh I made a slide

00:30:04,600 --> 00:30:11,470
about that okay yeah so in summary

00:30:08,130 --> 00:30:12,730
MapReduce isn't going away soon much as

00:30:11,470 --> 00:30:14,920
we'd like it to and much as the hype

00:30:12,730 --> 00:30:16,480
would tell us it is there's a lot of

00:30:14,920 --> 00:30:18,460
investment and everything that's gone

00:30:16,480 --> 00:30:20,620
into that and it's still quite a good

00:30:18,460 --> 00:30:23,680
fit for a lot of problems so crunch is a

00:30:20,620 --> 00:30:27,100
really pragmatic way to keep working

00:30:23,680 --> 00:30:29,410
with it and not be so frustrated java 8

00:30:27,100 --> 00:30:31,930
has been a really great improvement for

00:30:29,410 --> 00:30:34,000
functional programming it stops everyone

00:30:31,930 --> 00:30:35,170
from having to jump at scala immediately

00:30:34,000 --> 00:30:37,630
if they have some kind of vaguely

00:30:35,170 --> 00:30:40,340
functional need

00:30:37,630 --> 00:30:43,220
we're finding it actually scholar is

00:30:40,340 --> 00:30:45,050
such a big and complex language that if

00:30:43,220 --> 00:30:47,390
we can keep people on Java H rather than

00:30:45,050 --> 00:30:49,580
reaching for scholar then we'll save

00:30:47,390 --> 00:30:51,650
ourselves a lot of complexity in the

00:30:49,580 --> 00:30:54,080
long term so we're really happy to be on

00:30:51,650 --> 00:30:56,510
Java rate and using crunch chart crunch

00:30:54,080 --> 00:30:59,030
lambda and Java rate it ties all the

00:30:56,510 --> 00:31:02,000
concepts together and makes us more

00:30:59,030 --> 00:31:04,430
productive data engineers if you want to

00:31:02,000 --> 00:31:06,770
learn more about crunch a good place to

00:31:04,430 --> 00:31:08,480
start is crunched at Apache dorg the

00:31:06,770 --> 00:31:10,670
user guide for crunches is really

00:31:08,480 --> 00:31:12,140
complete I haven't yet got around to

00:31:10,670 --> 00:31:13,760
writing the user guide for crunch lambda

00:31:12,140 --> 00:31:15,280
but the javadocs are really complete so

00:31:13,760 --> 00:31:17,780
if you go to the javadocs for crunch

00:31:15,280 --> 00:31:20,380
look up the lambda class and it's got

00:31:17,780 --> 00:31:22,490
lots of examples and information there

00:31:20,380 --> 00:31:24,290
if you're if you're interested in the

00:31:22,490 --> 00:31:26,180
use cases I presented from SoundCloud

00:31:24,290 --> 00:31:27,950
I've written a long floor long formed

00:31:26,180 --> 00:31:30,380
blog post on the SoundCloud developer

00:31:27,950 --> 00:31:32,330
blog about those two examples so if

00:31:30,380 --> 00:31:35,120
you're a developer soundcloud.com / blog

00:31:32,330 --> 00:31:37,610
you can read about them if you want to

00:31:35,120 --> 00:31:39,110
read more about crunch lambda and also

00:31:37,610 --> 00:31:41,990
general topics that I'm writing about

00:31:39,110 --> 00:31:44,810
Java and modern Java styles that's my

00:31:41,990 --> 00:31:46,130
blog at radical Java calm and if you

00:31:44,810 --> 00:31:48,110
want to read these slides again because

00:31:46,130 --> 00:31:51,280
you missed something there up on

00:31:48,110 --> 00:31:53,690
tinyurl.com / be buzzed crunch and

00:31:51,280 --> 00:31:55,370
that's it I've got a new album out last

00:31:53,690 --> 00:31:59,360
month so check it out if you're

00:31:55,370 --> 00:32:01,940
interested I got time for a little bit

00:31:59,360 --> 00:32:04,840
of QA great thanks tanks are done David

00:32:01,940 --> 00:32:04,840
for the intriguing

00:32:07,670 --> 00:32:13,260
if any one of you have any questions

00:32:10,740 --> 00:32:18,530
please feel free to raise your hand I'll

00:32:13,260 --> 00:32:18,530
just quickly run across any questions

00:32:19,100 --> 00:32:28,350
any thoughts to add we are good all

00:32:26,970 --> 00:32:31,940
right can't that be off the hook like

00:32:28,350 --> 00:32:35,309
that we got one of the back oh cool

00:32:31,940 --> 00:32:38,610
please introduce yourself hi mattias

00:32:35,309 --> 00:32:41,160
andersson I was just wondering are you

00:32:38,610 --> 00:32:43,740
happy working with it with your Java 8

00:32:41,160 --> 00:32:46,920
lambdas to your compiler no not your

00:32:43,740 --> 00:32:49,740
compiler but use eclipse with which has

00:32:46,920 --> 00:32:51,679
type notations that are used in or do

00:32:49,740 --> 00:32:54,450
you have any problems with piping for it

00:32:51,679 --> 00:32:56,160
nice dreams I'm using IntelliJ most of

00:32:54,450 --> 00:32:58,890
the time and I don't really have any

00:32:56,160 --> 00:33:02,040
problems it's a it's a bit confusing

00:32:58,890 --> 00:33:04,860
because sometimes like for the crunch

00:33:02,040 --> 00:33:06,690
things you put the type information like

00:33:04,860 --> 00:33:08,730
you have your lambda expression then you

00:33:06,690 --> 00:33:10,650
have an indicator of the type and

00:33:08,730 --> 00:33:13,380
sometimes I find myself I have to fill

00:33:10,650 --> 00:33:15,179
in the type thing first because then

00:33:13,380 --> 00:33:16,860
I'll get the autocomplete on the lambda

00:33:15,179 --> 00:33:18,510
expression so there's a couple of little

00:33:16,860 --> 00:33:20,280
in the idiosyncrasies like that because

00:33:18,510 --> 00:33:21,540
it can't figure out that no matter what

00:33:20,280 --> 00:33:23,760
I put in the second part it's still

00:33:21,540 --> 00:33:26,429
going to be the same input but apart

00:33:23,760 --> 00:33:32,670
from that it's I have already had any

00:33:26,429 --> 00:33:38,550
problems with IntelliJ any more

00:33:32,670 --> 00:33:39,929
questions alright so thank you David

00:33:38,550 --> 00:33:40,890
again feel free to come and ask me

00:33:39,929 --> 00:33:43,040
afterwards if you have any more

00:33:40,890 --> 00:33:43,040
questions

00:33:43,750 --> 00:33:45,810

YouTube URL: https://www.youtube.com/watch?v=r6GUlo-QK5Q


