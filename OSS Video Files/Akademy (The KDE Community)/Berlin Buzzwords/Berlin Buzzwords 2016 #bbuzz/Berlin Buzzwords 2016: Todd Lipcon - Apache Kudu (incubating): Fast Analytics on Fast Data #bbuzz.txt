Title: Berlin Buzzwords 2016: Todd Lipcon - Apache Kudu (incubating): Fast Analytics on Fast Data #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Over the past several years, the Hadoop ecosystem has made great strides in its real-time access capabilities, narrowing the gap compared to traditional database technologies. With systems such as Impala and Spark, analysts can now run complex queries or jobs over large datasets within a matter of seconds. With systems such as Apache HBase and Apache Phoenix, applications can achieve millisecond-scale random access to arbitrarily-sized datasets.

Despite these advances, some important gaps remain that prevent many applications from transitioning to Hadoop-based architectures. Users are often caught between a rock and a hard place: columnar formats such as Apache Parquet offer extremely fast scan rates for analytics, but little to no ability for real-time modification or row-by-row indexed access. Online systems such as HBase offer very fast random access, but scan rates that are too slow for large scale data warehousing workloads.

This talk will investigate the trade-offs between real-time transactional access and fast analytic performance from the perspective of storage engine internals. It will also describe Apache Kudu (incubating), a new addition to the open source Hadoop ecosystem that fills the gap described above, complementing HDFS and HBase to provide a new option to achieve fast scans and fast random access from a single API.

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,570 --> 00:00:06,899
thank you yeah please I hope you had

00:00:05,370 --> 00:00:09,030
coffee i'm talking about a database

00:00:06,899 --> 00:00:11,820
today it's a little bit boring please

00:00:09,030 --> 00:00:13,410
don't fall asleep so a reminder this is

00:00:11,820 --> 00:00:15,030
an open source project that i'm talking

00:00:13,410 --> 00:00:18,359
about today please tweet about it at

00:00:15,030 --> 00:00:19,710
apache kudu is our our user you can feel

00:00:18,359 --> 00:00:21,990
free to ask any questions you don't get

00:00:19,710 --> 00:00:25,730
a chance to ask today via twitter or

00:00:21,990 --> 00:00:28,289
emailing me I'm taught at Cloudera calm

00:00:25,730 --> 00:00:30,539
so to quickly set the stage how many

00:00:28,289 --> 00:00:33,480
people here are Hadoop users any part of

00:00:30,539 --> 00:00:36,390
the Hadoop ecosystem spark HDFS etc ok a

00:00:33,480 --> 00:00:37,829
third a half maybe so for those of you

00:00:36,390 --> 00:00:39,989
who aren't Hadoop users this is kind of

00:00:37,829 --> 00:00:42,809
the most marketing oriented slide i have

00:00:39,989 --> 00:00:45,000
here with the Hadoop ecosystem so Hadoop

00:00:42,809 --> 00:00:46,440
is not just one open source project the

00:00:45,000 --> 00:00:48,210
way people usually talk about it it's

00:00:46,440 --> 00:00:50,670
actually an ecosystem there is of course

00:00:48,210 --> 00:00:53,039
one official Apache Hadoop people

00:00:50,670 --> 00:00:56,460
usually mean had do plus things like

00:00:53,039 --> 00:00:58,260
spark and hive and flume and scoop and

00:00:56,460 --> 00:00:59,609
uzi and all these different Apache

00:00:58,260 --> 00:01:02,609
projects you might have heard bandied

00:00:59,609 --> 00:01:05,010
about and creative is a new part of that

00:01:02,609 --> 00:01:06,960
ecosystem and specifically kudu is on

00:01:05,010 --> 00:01:09,659
the bottom layer of the ecosystem it's a

00:01:06,960 --> 00:01:11,330
storage engine that's meant to work with

00:01:09,659 --> 00:01:13,409
the other components that already exist

00:01:11,330 --> 00:01:15,360
so this is just to clear up some

00:01:13,409 --> 00:01:17,340
confusion kudu itself doesn't do any

00:01:15,360 --> 00:01:19,560
sequel it doesn't do any stream

00:01:17,340 --> 00:01:21,990
processing it doesn't do any job

00:01:19,560 --> 00:01:24,479
management all that is the existing

00:01:21,990 --> 00:01:26,549
Hadoop components and cooed is just the

00:01:24,479 --> 00:01:30,900
storage engine and we'll talk about that

00:01:26,549 --> 00:01:32,700
in upcoming slides so before I talk too

00:01:30,900 --> 00:01:35,100
much about what we do is I wanted to set

00:01:32,700 --> 00:01:37,409
the stage for why we decided to build

00:01:35,100 --> 00:01:42,630
kudu starting about three and a half

00:01:37,409 --> 00:01:43,890
almost four years ago so to introduce

00:01:42,630 --> 00:01:45,630
myself a little bit more i've been at

00:01:43,890 --> 00:01:47,310
Cloudera for about seven and a half

00:01:45,630 --> 00:01:49,049
years so i worked with a lot of

00:01:47,310 --> 00:01:51,720
customers and users in the community

00:01:49,049 --> 00:01:53,579
using the Hadoop ecosystem projects for

00:01:51,720 --> 00:01:55,920
different applications and I saw that

00:01:53,579 --> 00:01:57,420
basically there was two options that

00:01:55,920 --> 00:01:59,369
these customers and users could choose

00:01:57,420 --> 00:02:01,290
for storage I was specifically working

00:01:59,369 --> 00:02:04,890
on HDFS and HBase for most of those

00:02:01,290 --> 00:02:06,540
years and I took those two systems HDFS

00:02:04,890 --> 00:02:09,569
and HBase and plotted them on this graph

00:02:06,540 --> 00:02:11,400
I always joke this is an MBA graph not

00:02:09,569 --> 00:02:12,840
an engineering graph because there's no

00:02:11,400 --> 00:02:15,840
numbers it's just kind of these

00:02:12,840 --> 00:02:18,410
relaxes that we're placing icons on and

00:02:15,840 --> 00:02:21,300
on the top left here we have Hadoop HDFS

00:02:18,410 --> 00:02:23,220
HDFS is kind of the traditional original

00:02:21,300 --> 00:02:26,400
high deep storage manager based on

00:02:23,220 --> 00:02:28,319
Google's gfs paper and it may be a

00:02:26,400 --> 00:02:30,720
little small to read but the y-axis here

00:02:28,319 --> 00:02:32,819
is performance for analytics so

00:02:30,720 --> 00:02:35,280
specifically I mean they're the ability

00:02:32,819 --> 00:02:37,379
to run a job or a query over a large

00:02:35,280 --> 00:02:40,080
amount of data like looking at a day's

00:02:37,379 --> 00:02:42,269
worth or a month or a year's worth of

00:02:40,080 --> 00:02:44,730
data to extract some kind of insights do

00:02:42,269 --> 00:02:46,590
aggregations build a search index build

00:02:44,730 --> 00:02:48,090
a machine learning model something where

00:02:46,590 --> 00:02:50,640
you're looking at aggregate data sets

00:02:48,090 --> 00:02:52,790
and Hadoop is very very fast at that

00:02:50,640 --> 00:02:55,470
it's very high up on this y-axis and

00:02:52,790 --> 00:02:56,579
that's all great until you realize that

00:02:55,470 --> 00:02:58,379
you occasionally need to do something

00:02:56,579 --> 00:03:01,079
that's more like random access you want

00:02:58,379 --> 00:03:04,110
to update an individual record or insert

00:03:01,079 --> 00:03:05,790
one record at a time or look look up one

00:03:04,110 --> 00:03:08,220
record very quickly low latency and

00:03:05,790 --> 00:03:10,860
that's where HBase came along so about

00:03:08,220 --> 00:03:14,129
two or three years after HDFS started to

00:03:10,860 --> 00:03:15,299
gain some steam HBase came out and HBase

00:03:14,129 --> 00:03:17,489
is very good at the opposite

00:03:15,299 --> 00:03:19,859
characteristics it's super fast for a

00:03:17,489 --> 00:03:22,950
low latency random access you can look

00:03:19,859 --> 00:03:25,680
up right read an individual row in a

00:03:22,950 --> 00:03:27,450
couple of milliseconds but people who

00:03:25,680 --> 00:03:28,799
chose HBase found that they didn't

00:03:27,450 --> 00:03:32,069
really get the analytics they were used

00:03:28,799 --> 00:03:34,349
to on HDFS so it's pretty far right for

00:03:32,069 --> 00:03:36,930
this online random access characteristic

00:03:34,349 --> 00:03:39,690
but not very high up for the analytic

00:03:36,930 --> 00:03:41,519
use case and talking to a lot of our

00:03:39,690 --> 00:03:43,230
early users in a Hadoop ecosystem I

00:03:41,519 --> 00:03:45,329
found there is this big gap in the

00:03:43,230 --> 00:03:47,400
middle or people had use cases that

00:03:45,329 --> 00:03:49,709
weren't solidly analytics or solidly

00:03:47,400 --> 00:03:51,419
random access but actually had some of

00:03:49,709 --> 00:03:53,099
both characteristics maybe they're

00:03:51,419 --> 00:03:55,109
mostly doing analytics but need to do

00:03:53,099 --> 00:03:56,700
updates or they're mostly doing random

00:03:55,109 --> 00:03:58,680
reads and writes but they want to run

00:03:56,700 --> 00:04:00,630
some end of day analytics or build a

00:03:58,680 --> 00:04:02,669
search index on that online data set

00:04:00,630 --> 00:04:04,980
there's this big empty spot in the

00:04:02,669 --> 00:04:06,780
middle so essentially identified this

00:04:04,980 --> 00:04:08,849
big gap and I went and pitched to our

00:04:06,780 --> 00:04:10,980
co-founders and said hey we should build

00:04:08,849 --> 00:04:13,290
a new thing called kudu that fits in

00:04:10,980 --> 00:04:15,359
this gap and kuta has two main design

00:04:13,290 --> 00:04:17,789
goals one is to be high throughput

00:04:15,359 --> 00:04:19,650
meaning high up on the y-axis for

00:04:17,789 --> 00:04:22,770
analytics and the other is to maintain

00:04:19,650 --> 00:04:24,570
that low latency I like HBase you'll

00:04:22,770 --> 00:04:26,460
notice though we're not you know up into

00:04:24,570 --> 00:04:26,700
the right of both we're not trying to be

00:04:26,460 --> 00:04:28,410
a

00:04:26,700 --> 00:04:31,170
hdfs we're not trying to be a better

00:04:28,410 --> 00:04:33,330
HBase in fact our goal is a little bit

00:04:31,170 --> 00:04:36,240
less lofty we don't want to be more than

00:04:33,330 --> 00:04:38,790
twice as slow as HDFS and we just want

00:04:36,240 --> 00:04:40,320
to be in that same ballpark as HBase so

00:04:38,790 --> 00:04:43,380
if you have an application that's really

00:04:40,320 --> 00:04:44,700
great on HDFS great please use it I've

00:04:43,380 --> 00:04:46,140
got a lot of friends teammates who

00:04:44,700 --> 00:04:48,390
continue to work on it we're continuing

00:04:46,140 --> 00:04:50,130
to invest in it and same with HBase I

00:04:48,390 --> 00:04:52,710
said right next the HBase team and we're

00:04:50,130 --> 00:04:53,850
continuing to work on that software but

00:04:52,710 --> 00:04:56,130
if you have an application that needs

00:04:53,850 --> 00:04:59,670
some aspects of both this might be a

00:04:56,130 --> 00:05:03,330
kind of happy medium Goldilocks kind of

00:04:59,670 --> 00:05:04,590
system to make your life easier so the

00:05:03,330 --> 00:05:06,570
other main goal and this was actually

00:05:04,590 --> 00:05:08,610
touched on by eric evans who just

00:05:06,570 --> 00:05:10,890
presented in this room prior the

00:05:08,610 --> 00:05:12,600
changing Hardware landscape machines are

00:05:10,890 --> 00:05:15,630
changing quite a bit compared to the

00:05:12,600 --> 00:05:17,820
early 2000s when the original GSS and

00:05:15,630 --> 00:05:21,330
big table papers were written so Google

00:05:17,820 --> 00:05:24,420
published the GFS paper in 2003 and big

00:05:21,330 --> 00:05:25,890
table in 2006 and the the machines they

00:05:24,420 --> 00:05:28,410
were quoting in these papers in the

00:05:25,890 --> 00:05:31,110
benchmarks are kind of quaint now you're

00:05:28,410 --> 00:05:35,400
talking maybe two or four cores early

00:05:31,110 --> 00:05:36,840
like pentium 4 things like that so the

00:05:35,400 --> 00:05:39,390
the biggest trend though that affects

00:05:36,840 --> 00:05:41,370
storage is the actual storage media so

00:05:39,390 --> 00:05:43,770
from the early 2000s until the last

00:05:41,370 --> 00:05:46,050
couple of years we saw a big shift from

00:05:43,770 --> 00:05:48,690
spinning disks onto solid state disks

00:05:46,050 --> 00:05:50,250
specifically for random access certainly

00:05:48,690 --> 00:05:51,930
for analytics the spinning disks are

00:05:50,250 --> 00:05:53,970
still pretty good and most cost

00:05:51,930 --> 00:05:57,540
effective but if you're measuring on

00:05:53,970 --> 00:06:00,150
price / io performance the SSDs are a

00:05:57,540 --> 00:06:01,980
lot faster for the money so the price is

00:06:00,150 --> 00:06:04,500
continuing to drop and the throughput

00:06:01,980 --> 00:06:06,390
both in terms of random ayos and

00:06:04,500 --> 00:06:08,970
sequential is just continuing to go up

00:06:06,390 --> 00:06:10,830
and up so with NAND flash kind of the

00:06:08,970 --> 00:06:12,840
current generation of flash that most

00:06:10,830 --> 00:06:14,610
people use today we're up to several

00:06:12,840 --> 00:06:16,560
hundred thousand iOS per second on a

00:06:14,610 --> 00:06:18,750
device you can get for only a couple

00:06:16,560 --> 00:06:20,550
thousand dollars for a terabyte so this

00:06:18,750 --> 00:06:22,260
is maybe a third of a cost of a server

00:06:20,550 --> 00:06:24,210
machine and it gives you this really

00:06:22,260 --> 00:06:25,710
terrific i/o performance completely

00:06:24,210 --> 00:06:28,350
different than the spinning disks we had

00:06:25,710 --> 00:06:30,000
before and the next generation was

00:06:28,350 --> 00:06:32,850
recently announced about eight months

00:06:30,000 --> 00:06:34,740
ago by intel called 3d cross point

00:06:32,850 --> 00:06:37,680
spelled with an X so you know it's

00:06:34,740 --> 00:06:39,500
really cool and this has another couple

00:06:37,680 --> 00:06:41,810
orders of magnitude improvement

00:06:39,500 --> 00:06:43,610
and flash so it's about a thousand times

00:06:41,810 --> 00:06:46,280
faster for random access compared to

00:06:43,610 --> 00:06:47,810
nand flash but it's cheaper than ram so

00:06:46,280 --> 00:06:50,180
this is a cool technology you take this

00:06:47,810 --> 00:06:52,070
this storage media they put it on a dim

00:06:50,180 --> 00:06:54,290
so it's actually it looks like a memory

00:06:52,070 --> 00:06:56,420
stick and you stick it in your Xeon

00:06:54,290 --> 00:06:58,280
machine and it basically looks like

00:06:56,420 --> 00:06:59,960
memory to everything except that those

00:06:58,280 --> 00:07:02,330
rights to memory are actually persistent

00:06:59,960 --> 00:07:03,860
across the power off and reboot so it's

00:07:02,330 --> 00:07:05,420
completely changes the ways we want to

00:07:03,860 --> 00:07:08,110
think about storage and think about

00:07:05,420 --> 00:07:10,340
writing software on top of that storage

00:07:08,110 --> 00:07:12,169
so the other trend is a little bit less

00:07:10,340 --> 00:07:14,030
glamorous but it's just the steady march

00:07:12,169 --> 00:07:15,950
of Moore's Law Ram is getting bigger and

00:07:14,030 --> 00:07:18,380
bigger I think the big table paper

00:07:15,950 --> 00:07:20,570
quotes machines with 16 gigs of RAM when

00:07:18,380 --> 00:07:24,169
i started at Cloudera 64 was kind of top

00:07:20,570 --> 00:07:25,940
end now 256 is quite common if you use

00:07:24,169 --> 00:07:27,560
amazon you can now get a machine i think

00:07:25,940 --> 00:07:30,050
a month or two ago they announced a two

00:07:27,560 --> 00:07:32,150
terabyte machine on amazon so these are

00:07:30,050 --> 00:07:34,010
totally commodity machines nat with tera

00:07:32,150 --> 00:07:37,430
bytes of ram and that trend is just

00:07:34,010 --> 00:07:39,140
going to keep on marching forward so the

00:07:37,430 --> 00:07:41,000
takeaway here is that the actual

00:07:39,140 --> 00:07:43,820
bottleneck for these database systems is

00:07:41,000 --> 00:07:46,610
the CPU it's not actually the discs

00:07:43,820 --> 00:07:49,700
anymore so in the big table gfs papers

00:07:46,610 --> 00:07:51,650
which sort of underlie HDFS and HBase

00:07:49,700 --> 00:07:53,870
they talk a lot about all these tricks

00:07:51,650 --> 00:07:56,900
to sequential eyes all of the rights and

00:07:53,870 --> 00:07:58,940
avoid seeks especially on rights in

00:07:56,900 --> 00:08:00,710
order to get low latency that stuff not

00:07:58,940 --> 00:08:03,560
as important anymore compared to

00:08:00,710 --> 00:08:05,780
actually optimizing for CPU we can do 10

00:08:03,560 --> 00:08:08,060
disc seeks in a couple of microseconds

00:08:05,780 --> 00:08:10,790
if your disk is actually this persistent

00:08:08,060 --> 00:08:12,380
memory technology so we can redesign the

00:08:10,790 --> 00:08:14,030
way we do storage and that's why we

00:08:12,380 --> 00:08:15,590
decided with kudu to start from the

00:08:14,030 --> 00:08:17,720
ground up with something that was

00:08:15,590 --> 00:08:19,910
designed for this modern hardware some

00:08:17,720 --> 00:08:22,070
of the specific things we do is written

00:08:19,910 --> 00:08:24,020
entirely in C++ we've been collaborating

00:08:22,070 --> 00:08:26,030
with intel on support for this

00:08:24,020 --> 00:08:27,290
persistent memory technology since even

00:08:26,030 --> 00:08:29,270
before it was announced we have a

00:08:27,290 --> 00:08:30,620
special relationship with them and we're

00:08:29,270 --> 00:08:33,050
already ready to be running on these

00:08:30,620 --> 00:08:37,430
machines with tons of ram and superfast

00:08:33,050 --> 00:08:38,690
storage so that's sort of the why why

00:08:37,430 --> 00:08:40,669
did we go and build this new project

00:08:38,690 --> 00:08:44,660
when there are already so many projects

00:08:40,669 --> 00:08:47,180
existing for storage so now the what in

00:08:44,660 --> 00:08:49,810
a sentence guddu is scalable and fast

00:08:47,180 --> 00:08:51,860
tabular storage so we'll break that down

00:08:49,810 --> 00:08:53,000
scalable it means it should scale like

00:08:51,860 --> 00:08:55,640
the rest of the hoodie because

00:08:53,000 --> 00:08:57,980
system so far we've only tested 275

00:08:55,640 --> 00:08:59,510
nodes in my experience most people are

00:08:57,980 --> 00:09:01,670
probably under that scale some are

00:08:59,510 --> 00:09:03,950
higher and we're planning to continue to

00:09:01,670 --> 00:09:06,350
test up to larger scale it was fine when

00:09:03,950 --> 00:09:08,420
we ran on 275 it's not like it broke and

00:09:06,350 --> 00:09:09,770
that's why we stopped we stopped because

00:09:08,420 --> 00:09:12,050
that's the size cluster that they gave

00:09:09,770 --> 00:09:13,640
me to test on we're planning on

00:09:12,050 --> 00:09:17,750
deploying on a much larger cluster in

00:09:13,640 --> 00:09:19,940
the coming months fast we just talked

00:09:17,750 --> 00:09:22,130
about how fast the storages crew should

00:09:19,940 --> 00:09:23,660
be able to drive that storage to the

00:09:22,130 --> 00:09:26,030
same speed the storage is physically

00:09:23,660 --> 00:09:27,800
capable of so that means on a you know

00:09:26,030 --> 00:09:30,050
10 node cluster or so we should get

00:09:27,800 --> 00:09:32,210
millions of random reads or writes per

00:09:30,050 --> 00:09:35,420
second in fact we benchmarked a single

00:09:32,210 --> 00:09:37,640
node kudiye cluster doing 750,000 random

00:09:35,420 --> 00:09:40,280
reads a second that's an in RAM data set

00:09:37,640 --> 00:09:41,930
of course but that's not too too strange

00:09:40,280 --> 00:09:44,110
to think about in RAM data sets when you

00:09:41,930 --> 00:09:47,300
have a terabyte of ram per machine and

00:09:44,110 --> 00:09:49,100
for sequential true but for analytics we

00:09:47,300 --> 00:09:51,110
expect multiple gigabytes a second per

00:09:49,100 --> 00:09:53,030
node so a single flash device can give

00:09:51,110 --> 00:09:54,680
you three gigs a second we don't want to

00:09:53,030 --> 00:09:57,290
be cpu-bound we need to be able to take

00:09:54,680 --> 00:09:59,300
all of that bandwidth from the disk

00:09:57,290 --> 00:10:03,170
shove it through the CPU as efficiently

00:09:59,300 --> 00:10:05,630
as possible and the actual data model of

00:10:03,170 --> 00:10:07,880
kudu is tabular this is really important

00:10:05,630 --> 00:10:09,440
kudu is not a file system we're not

00:10:07,880 --> 00:10:12,500
trying to have a file system like API

00:10:09,440 --> 00:10:14,570
and it's also not a key value store like

00:10:12,500 --> 00:10:16,400
HBase or Cassandra it has tables and

00:10:14,570 --> 00:10:18,380
tables are the only construct that are

00:10:16,400 --> 00:10:20,990
stored in kudu and those tables have

00:10:18,380 --> 00:10:23,360
finite numbers of columns very much like

00:10:20,990 --> 00:10:25,700
post grass my sequel Oracle you create a

00:10:23,360 --> 00:10:27,890
table you say here my thirty fifty

00:10:25,700 --> 00:10:30,589
hundred columns here are the types here

00:10:27,890 --> 00:10:32,870
are the names and that's that there's no

00:10:30,589 --> 00:10:35,510
kind of freeform creating columns on the

00:10:32,870 --> 00:10:37,430
fly and this makes it much easier for

00:10:35,510 --> 00:10:39,440
users to use you can hook this into a

00:10:37,430 --> 00:10:41,000
sequel engine and start querying these

00:10:39,440 --> 00:10:43,220
tables immediately there's no data

00:10:41,000 --> 00:10:46,120
mapping step or any kind of complicated

00:10:43,220 --> 00:10:48,740
bite wise serialization and encoding of

00:10:46,120 --> 00:10:50,750
course big data is changing frequently

00:10:48,740 --> 00:10:53,210
and you need to be able to add new

00:10:50,750 --> 00:10:55,520
columns as your data evolves so we

00:10:53,210 --> 00:10:57,530
designed alter table to be very fast in

00:10:55,520 --> 00:10:59,810
fact it's constant time to do alter

00:10:57,530 --> 00:11:03,320
table regardless how large your data set

00:10:59,810 --> 00:11:05,209
is and lastly we realize that sequel

00:11:03,320 --> 00:11:06,860
data models are useful but a lot of

00:11:05,209 --> 00:11:09,380
people prefer programming

00:11:06,860 --> 00:11:11,270
with a no sequel style API especially

00:11:09,380 --> 00:11:13,970
ingesting data from a streaming system

00:11:11,270 --> 00:11:17,210
or doing random reads and writes so we

00:11:13,970 --> 00:11:19,490
have Java C++ and Python API s you can

00:11:17,210 --> 00:11:21,380
use those to directly contact kudu and

00:11:19,490 --> 00:11:24,170
get millisecond scale random read and

00:11:21,380 --> 00:11:25,430
write of course your analysts probably

00:11:24,170 --> 00:11:27,890
want to use something like sequel or

00:11:25,430 --> 00:11:30,080
spark so we have integrations with

00:11:27,890 --> 00:11:32,840
things like sparks equal Impala and

00:11:30,080 --> 00:11:35,630
drill and there's a presto integration

00:11:32,840 --> 00:11:40,340
also kind of in the works in order to

00:11:35,630 --> 00:11:41,630
query query data from those systems so

00:11:40,340 --> 00:11:44,620
let's talk about the use cases and

00:11:41,630 --> 00:11:47,030
architectures where kudu can make sense

00:11:44,620 --> 00:11:49,730
cooter is designed for use cases that

00:11:47,030 --> 00:11:52,610
combine as we saw earlier the random

00:11:49,730 --> 00:11:54,680
access and the analytic performance so

00:11:52,610 --> 00:11:56,270
you have some accesses that are these

00:11:54,680 --> 00:11:58,760
quick look ups and some that are these

00:11:56,270 --> 00:12:00,620
large scans so to take one specific

00:11:58,760 --> 00:12:02,270
example we'll look at financial time

00:12:00,620 --> 00:12:04,370
series so I've talked to a lot of

00:12:02,270 --> 00:12:06,620
customers of ours in New York and London

00:12:04,370 --> 00:12:08,150
and other financial centres and they

00:12:06,620 --> 00:12:10,760
have these data sets that are streaming

00:12:08,150 --> 00:12:12,590
in from market data providers so they

00:12:10,760 --> 00:12:14,360
get you know thousands of UDP messages

00:12:12,590 --> 00:12:15,950
per second with this little binary

00:12:14,360 --> 00:12:17,720
format called fix I don't know if

00:12:15,950 --> 00:12:20,030
anybody here is works in finance and

00:12:17,720 --> 00:12:21,740
they take this data and need to ingest

00:12:20,030 --> 00:12:24,110
it and they need to ingest it at real

00:12:21,740 --> 00:12:26,270
time they don't want to wait until some

00:12:24,110 --> 00:12:28,070
batch load the next day to start doing

00:12:26,270 --> 00:12:29,480
their analytics because they're trying

00:12:28,070 --> 00:12:32,030
to build applications like fraud

00:12:29,480 --> 00:12:34,010
detection and fraud prevention and if

00:12:32,030 --> 00:12:36,620
you wait until the next day's bulk load

00:12:34,010 --> 00:12:38,600
to see that you had fraud yesterday you

00:12:36,620 --> 00:12:40,490
kind of missed the opportunity to do

00:12:38,600 --> 00:12:42,580
anything about it and even more

00:12:40,490 --> 00:12:45,140
important if you're trying to do actual

00:12:42,580 --> 00:12:46,970
trading based on this data you need to

00:12:45,140 --> 00:12:50,360
ingest the data run analytics

00:12:46,970 --> 00:12:52,310
immediately have results so the workload

00:12:50,360 --> 00:12:54,590
has these inserts coming from these

00:12:52,310 --> 00:12:56,690
platforms that are delivering data they

00:12:54,590 --> 00:12:59,810
have scans in order to build models and

00:12:56,690 --> 00:13:01,340
look up individual information and if

00:12:59,810 --> 00:13:03,950
you go want to see the history of one

00:13:01,340 --> 00:13:05,660
particular stock or equity then you need

00:13:03,950 --> 00:13:08,060
a low-latency lookup of that particular

00:13:05,660 --> 00:13:09,620
time range for that particular equity

00:13:08,060 --> 00:13:11,870
include you can do all of these things

00:13:09,620 --> 00:13:14,090
in one system I would very good

00:13:11,870 --> 00:13:15,980
performance and the really surprising

00:13:14,090 --> 00:13:18,230
thing that I found talking to these

00:13:15,980 --> 00:13:20,630
users is that they also have updates and

00:13:18,230 --> 00:13:22,250
deletes you wouldn't really guess this

00:13:20,630 --> 00:13:24,230
but with these market data providers

00:13:22,250 --> 00:13:26,330
sometimes they call you up literally on

00:13:24,230 --> 00:13:28,490
the phone and say we sent you this bad

00:13:26,330 --> 00:13:30,290
data yesterday can you actually go

00:13:28,490 --> 00:13:31,550
delete all the records we sent in this

00:13:30,290 --> 00:13:34,580
time range because they were just

00:13:31,550 --> 00:13:36,560
incorrect or we said it was in euros but

00:13:34,580 --> 00:13:38,870
actually that was US dollars so go

00:13:36,560 --> 00:13:41,240
update all those values and if you have

00:13:38,870 --> 00:13:42,980
a system that only supports append only

00:13:41,240 --> 00:13:45,200
you can't really handle these things

00:13:42,980 --> 00:13:50,030
very well and you end up with a lot of

00:13:45,200 --> 00:13:51,680
work to go fix these issues so people do

00:13:50,030 --> 00:13:53,270
build these kind of systems today on

00:13:51,680 --> 00:13:55,910
Hadoop and this is an architecture

00:13:53,270 --> 00:13:57,800
diagram kind of boils down but a lot of

00:13:55,910 --> 00:14:00,440
our users are doing in the Hadoop

00:13:57,800 --> 00:14:02,510
ecosystem so you see the top left we

00:14:00,440 --> 00:14:04,970
have incoming data from some kind of a

00:14:02,510 --> 00:14:08,180
messaging system maybe that's via flew

00:14:04,970 --> 00:14:10,130
more scribe or Kafka or flink or UDP

00:14:08,180 --> 00:14:13,460
packets anything that's kind of

00:14:10,130 --> 00:14:15,740
streaming data in ingest and typically

00:14:13,460 --> 00:14:18,020
when this data arrives they write it

00:14:15,740 --> 00:14:19,700
into a system something like HBase it

00:14:18,020 --> 00:14:21,080
could also be cassandra or you know

00:14:19,700 --> 00:14:23,450
another system that supports streaming

00:14:21,080 --> 00:14:26,150
ingest and that's because you can't do

00:14:23,450 --> 00:14:29,240
individual row by row writes into

00:14:26,150 --> 00:14:31,100
something like HDFS but if you tried to

00:14:29,240 --> 00:14:32,930
run the analytics directly on HBase

00:14:31,100 --> 00:14:35,750
you'd find the performance isn't very

00:14:32,930 --> 00:14:38,030
good it works but it's maybe 10 to 50

00:14:35,750 --> 00:14:40,760
times slower than running that same job

00:14:38,030 --> 00:14:43,010
against HDFS so they have these cron

00:14:40,760 --> 00:14:45,650
jobs which look for a threshold like

00:14:43,010 --> 00:14:47,510
once an hour or after a certain number

00:14:45,650 --> 00:14:50,770
of gigabytes have been accumulated and

00:14:47,510 --> 00:14:52,820
they dumped HBase into Parque files

00:14:50,770 --> 00:14:55,250
again you could choose different

00:14:52,820 --> 00:14:57,560
technologies it might be 0 RC file or

00:14:55,250 --> 00:14:59,840
something like that but basically some

00:14:57,560 --> 00:15:03,260
sort of efficient columnar format on

00:14:59,840 --> 00:15:05,960
HDFS and you've got these files now once

00:15:03,260 --> 00:15:07,550
every hour and you get another cron job

00:15:05,960 --> 00:15:09,710
which looks for these new files to show

00:15:07,550 --> 00:15:12,260
up and run some kind of a metadata

00:15:09,710 --> 00:15:14,540
operation to add them into your hive

00:15:12,260 --> 00:15:17,180
meta store or move them into the right

00:15:14,540 --> 00:15:18,970
directory so they can be queried maybe

00:15:17,180 --> 00:15:21,170
creating a new partition for that hour

00:15:18,970 --> 00:15:24,170
so you've all these cron jobs working

00:15:21,170 --> 00:15:26,420
and eventually you get your data on HDFS

00:15:24,170 --> 00:15:28,730
in the fastest format and you can run

00:15:26,420 --> 00:15:31,100
reporting with Impala or sparks equal or

00:15:28,730 --> 00:15:34,440
presto whatever your reporting tool of

00:15:31,100 --> 00:15:37,230
choice is so the problems here are many

00:15:34,440 --> 00:15:39,270
for one it took about an hour between

00:15:37,230 --> 00:15:41,190
data arriving and actually being

00:15:39,270 --> 00:15:43,680
available for analytics it's not very

00:15:41,190 --> 00:15:47,160
real time you can miss a lot of business

00:15:43,680 --> 00:15:49,770
value in an hour in second it doesn't

00:15:47,160 --> 00:15:52,080
really handle updates at all or deletes

00:15:49,770 --> 00:15:54,060
if you need to update or delete some

00:15:52,080 --> 00:15:56,190
data that's already in the historic data

00:15:54,060 --> 00:15:59,310
store you have to take out an entire

00:15:56,190 --> 00:16:01,170
partition apply those updates and then

00:15:59,310 --> 00:16:02,670
do this swapping mechanism where you try

00:16:01,170 --> 00:16:04,530
to move the new partition with the

00:16:02,670 --> 00:16:06,570
updates and remove the old partition and

00:16:04,530 --> 00:16:08,310
you hope that a query that's running at

00:16:06,570 --> 00:16:10,380
the same time doesn't fail because you

00:16:08,310 --> 00:16:12,990
deleted a file or move the file under it

00:16:10,380 --> 00:16:14,850
it's a lot of complexity and talking to

00:16:12,990 --> 00:16:16,860
a lot of users who built this kind of

00:16:14,850 --> 00:16:19,110
system they spend a lot of engineering

00:16:16,860 --> 00:16:20,580
time just managing keeping all these

00:16:19,110 --> 00:16:23,580
files moving around and worrying about

00:16:20,580 --> 00:16:25,860
partitioning and the other problem is

00:16:23,580 --> 00:16:27,930
the thing called compaction where will

00:16:25,860 --> 00:16:30,210
accumulate maybe thousands or hundreds

00:16:27,930 --> 00:16:31,560
of thousands of these little files in a

00:16:30,210 --> 00:16:33,990
new file every five minutes or ten

00:16:31,560 --> 00:16:36,120
minutes and when we run a query on a

00:16:33,990 --> 00:16:38,100
hundred thousand files the performance

00:16:36,120 --> 00:16:39,810
actually isn't very good these query

00:16:38,100 --> 00:16:42,720
engines prefer to have a smaller number

00:16:39,810 --> 00:16:45,390
of large files versus a large number of

00:16:42,720 --> 00:16:47,520
small files so we have these other jobs

00:16:45,390 --> 00:16:49,440
called compaction switch take a bunch of

00:16:47,520 --> 00:16:52,680
small files merge them together create a

00:16:49,440 --> 00:16:55,080
big file basically I've talked too long

00:16:52,680 --> 00:16:56,490
about this slide already and that's how

00:16:55,080 --> 00:17:00,690
people feel when they implement these

00:16:56,490 --> 00:17:02,550
systems it's a lot of complexity so the

00:17:00,690 --> 00:17:04,980
idea with kudu is it just simplifies

00:17:02,550 --> 00:17:07,199
everything you have one system you can

00:17:04,980 --> 00:17:09,240
insert row by row you can run your

00:17:07,199 --> 00:17:10,890
reporting on that system you don't need

00:17:09,240 --> 00:17:13,290
to worry about all these file management

00:17:10,890 --> 00:17:14,220
as metadata these two different storage

00:17:13,290 --> 00:17:16,829
systems that have different

00:17:14,220 --> 00:17:19,410
characteristics it's all built in into

00:17:16,829 --> 00:17:21,170
one system updates just apply an update

00:17:19,410 --> 00:17:23,280
you don't need to do something different

00:17:21,170 --> 00:17:26,130
so this really simplifies everything

00:17:23,280 --> 00:17:28,050
we're not trying to say that your query

00:17:26,130 --> 00:17:29,850
at the end is going to run faster it

00:17:28,050 --> 00:17:32,460
probably won't probably run around the

00:17:29,850 --> 00:17:34,470
same speed but it makes your life much

00:17:32,460 --> 00:17:38,790
much easier and that's the main selling

00:17:34,470 --> 00:17:40,590
point of kudu so I want to show a real

00:17:38,790 --> 00:17:42,030
use case I don't know if people here are

00:17:40,590 --> 00:17:43,830
familiar with Xiaomi they make this

00:17:42,030 --> 00:17:46,500
wonderful mobile phone here that I'm

00:17:43,830 --> 00:17:47,660
using it's a smartphone very very

00:17:46,500 --> 00:17:49,850
popular in China and

00:17:47,660 --> 00:17:51,560
spanning to the rest of the world now so

00:17:49,850 --> 00:17:53,420
the world's fourth largest smartphone

00:17:51,560 --> 00:17:55,850
maker they've got I think about a

00:17:53,420 --> 00:17:58,250
billion customers now and they provide a

00:17:55,850 --> 00:17:59,870
lot of online apps if you use apples

00:17:58,250 --> 00:18:02,810
like iCloud they have a message

00:17:59,870 --> 00:18:05,840
messaging photo sharing social things

00:18:02,810 --> 00:18:07,400
all these different features and all of

00:18:05,840 --> 00:18:09,650
these online services are gathering

00:18:07,400 --> 00:18:11,450
tracing information about how people are

00:18:09,650 --> 00:18:13,070
using the service the performance

00:18:11,450 --> 00:18:16,640
they're seeing any errors they're

00:18:13,070 --> 00:18:18,650
hitting the standard kind of web service

00:18:16,640 --> 00:18:20,390
monitoring you might do they're also

00:18:18,650 --> 00:18:22,730
gathering a lot of telemetry from the

00:18:20,390 --> 00:18:25,250
phones themselves so if an app crashes

00:18:22,730 --> 00:18:27,410
it will report back this crash trace

00:18:25,250 --> 00:18:30,530
including recent log messages the

00:18:27,410 --> 00:18:32,000
phone's device information etc so this

00:18:30,530 --> 00:18:33,980
is very high throughput they've got a

00:18:32,000 --> 00:18:35,990
billion phones out there and they're all

00:18:33,980 --> 00:18:37,520
collecting data all the time so it's

00:18:35,990 --> 00:18:39,980
about 20 billion records per day and

00:18:37,520 --> 00:18:42,590
growing fast and they need to take this

00:18:39,980 --> 00:18:44,900
data and do two things with it one they

00:18:42,590 --> 00:18:46,610
need to be able to query the data to see

00:18:44,900 --> 00:18:49,100
trends and understand how people are

00:18:46,610 --> 00:18:51,080
using the software and two they need to

00:18:49,100 --> 00:18:53,870
be able to respond quickly if they have

00:18:51,080 --> 00:18:56,390
new bugs so when they're releasing a new

00:18:53,870 --> 00:18:58,280
version of their Android fork and they

00:18:56,390 --> 00:19:01,040
put it out to a bunch of phones maybe

00:18:58,280 --> 00:19:03,530
200 million phones and they start to see

00:19:01,040 --> 00:19:05,810
the crash rate is elevating they can do

00:19:03,530 --> 00:19:08,120
analytics really quickly to see oh this

00:19:05,810 --> 00:19:09,590
particular model of phone using this

00:19:08,120 --> 00:19:11,600
particular version of the mail

00:19:09,590 --> 00:19:13,430
application is crashing here is a

00:19:11,600 --> 00:19:15,680
particular crash log and then the

00:19:13,430 --> 00:19:17,390
developers want to respond quickly look

00:19:15,680 --> 00:19:19,420
up that phone information look up the

00:19:17,390 --> 00:19:21,620
crash log get the symbols for the the

00:19:19,420 --> 00:19:23,840
application that crashed and do this

00:19:21,620 --> 00:19:26,630
interactive debugging so they need very

00:19:23,840 --> 00:19:28,820
low latency lookups of this data so they

00:19:26,630 --> 00:19:33,380
can troubleshoot and fix the problem or

00:19:28,820 --> 00:19:35,030
roll back the OS upgrade so they built

00:19:33,380 --> 00:19:37,100
this application before it's not a new

00:19:35,030 --> 00:19:38,930
application for them and they built it

00:19:37,100 --> 00:19:41,540
using a couple of the older technologies

00:19:38,930 --> 00:19:43,580
so they're using scribe writing to HDFS

00:19:41,540 --> 00:19:45,950
in the sequence file format which is

00:19:43,580 --> 00:19:48,320
this append only format and they're also

00:19:45,950 --> 00:19:50,870
writing to hbase so they can get some

00:19:48,320 --> 00:19:53,390
data more quickly they have cron jobs

00:19:50,870 --> 00:19:56,600
running etl using hive MapReduce and

00:19:53,390 --> 00:19:57,980
spark once an hour and that's resulting

00:19:56,600 --> 00:20:00,980
in parque files which they can then

00:19:57,980 --> 00:20:01,430
query via impala so the problem here is

00:20:00,980 --> 00:20:04,370
the

00:20:01,430 --> 00:20:06,200
see new data arrives and it takes at

00:20:04,370 --> 00:20:08,540
least an hour before they can even see

00:20:06,200 --> 00:20:10,100
it for analysis and that means they

00:20:08,540 --> 00:20:12,710
can't respond quickly to problems when

00:20:10,100 --> 00:20:14,060
problems are happening online and when

00:20:12,710 --> 00:20:15,770
something starts to go wrong in this

00:20:14,060 --> 00:20:18,560
complex pipeline they've got two

00:20:15,770 --> 00:20:21,110
different storage systems three or four

00:20:18,560 --> 00:20:22,790
different file formats different ETL

00:20:21,110 --> 00:20:25,250
engines it's really complicated so

00:20:22,790 --> 00:20:27,290
things go wrong pretty often it can take

00:20:25,250 --> 00:20:31,820
up to a day with this pipeline is shut

00:20:27,290 --> 00:20:33,920
down before they can get that data so

00:20:31,820 --> 00:20:36,020
with ku the pipeline is a little bit

00:20:33,920 --> 00:20:37,990
different the only storage system here

00:20:36,020 --> 00:20:40,460
now is kudu they're taking the data

00:20:37,990 --> 00:20:43,040
using scribing Kostka as a sort of

00:20:40,460 --> 00:20:45,770
buffer and then using storm which reads

00:20:43,040 --> 00:20:48,620
from Kafka does a little bit of online

00:20:45,770 --> 00:20:51,020
etl data processing lunging into a

00:20:48,620 --> 00:20:53,270
tabular format and then writes to kudu

00:20:51,020 --> 00:20:55,310
and some other applications like the

00:20:53,270 --> 00:20:57,710
back end web services can actually write

00:20:55,310 --> 00:21:00,620
directly from the data source into kudu

00:20:57,710 --> 00:21:03,020
using the Java API so the latency here

00:21:00,620 --> 00:21:05,480
is between about 10 milliseconds best

00:21:03,020 --> 00:21:08,030
case and worst case maybe 10 seconds if

00:21:05,480 --> 00:21:11,180
something is going wrong on Kafka it has

00:21:08,030 --> 00:21:13,460
to buffer for a couple seconds so this

00:21:11,180 --> 00:21:16,010
et al is all real time and as soon as

00:21:13,460 --> 00:21:18,290
the data lands in kudu their users can

00:21:16,010 --> 00:21:21,490
query it with Impala or use the API

00:21:18,290 --> 00:21:25,040
directly to look up individual records

00:21:21,490 --> 00:21:26,990
so essentially their their pipelines a

00:21:25,040 --> 00:21:31,780
lot simpler and the latency is a lot

00:21:26,990 --> 00:21:34,700
lower alright getting it to how it works

00:21:31,780 --> 00:21:37,280
we'll start from the high level how guru

00:21:34,700 --> 00:21:38,840
looks as a distributed system so like

00:21:37,280 --> 00:21:40,760
other systems we have this concept of

00:21:38,840 --> 00:21:42,680
horizontal partitioning so every table

00:21:40,760 --> 00:21:44,750
which might be a trillion rose is

00:21:42,680 --> 00:21:47,390
chunked into smaller chunks called

00:21:44,750 --> 00:21:49,520
tablets and we can use either range or

00:21:47,390 --> 00:21:51,890
hash partitioning or a combination

00:21:49,520 --> 00:21:54,470
thereof so this is best described with

00:21:51,890 --> 00:21:56,540
an example if you have a time series you

00:21:54,470 --> 00:21:58,880
might have host metric and time stamp

00:21:56,540 --> 00:22:01,100
and you want to distribute these evenly

00:21:58,880 --> 00:22:03,110
across the cluster so you can just say

00:22:01,100 --> 00:22:05,240
distribute by hash of time stamp into

00:22:03,110 --> 00:22:07,280
100 buckets and that automatically

00:22:05,240 --> 00:22:09,980
computes the hash code does the correct

00:22:07,280 --> 00:22:11,300
encoding to spread the data if you're

00:22:09,980 --> 00:22:12,770
familiar with something like HBase

00:22:11,300 --> 00:22:15,230
you're probably doing this by hand

00:22:12,770 --> 00:22:16,850
there's techniques called key salting

00:22:15,230 --> 00:22:19,160
we'll spend a lot of effort building

00:22:16,850 --> 00:22:22,160
their row keys to achieve this so with

00:22:19,160 --> 00:22:23,990
cootie we just built it in so once we

00:22:22,160 --> 00:22:26,299
run our partitioning we split the table

00:22:23,990 --> 00:22:28,250
up into tablets each tablet then has to

00:22:26,299 --> 00:22:29,900
be replicated because one of the servers

00:22:28,250 --> 00:22:32,570
the tablet server is hosting those

00:22:29,900 --> 00:22:34,880
tablets could crash so we make three

00:22:32,570 --> 00:22:36,350
replicas by default could be at five or

00:22:34,880 --> 00:22:38,690
seven if you need more at fault

00:22:36,350 --> 00:22:41,000
tolerance and we use raft consensus

00:22:38,690 --> 00:22:42,549
which is an algorithm for building

00:22:41,000 --> 00:22:45,830
agreement on the state of the database

00:22:42,549 --> 00:22:47,780
between three or five servers and this

00:22:45,830 --> 00:22:49,820
gives us very very fast fault tolerance

00:22:47,780 --> 00:22:52,490
if a server crashes we have two more

00:22:49,820 --> 00:22:54,049
replicas ready to take over typically

00:22:52,490 --> 00:22:55,940
four and a half seconds it's three

00:22:54,049 --> 00:22:59,240
heartbeat periods or heartby is 1.5

00:22:55,940 --> 00:23:01,040
seconds and these tablets servers then

00:22:59,240 --> 00:23:02,480
host the data directly on their local

00:23:01,040 --> 00:23:04,730
hard drives because we're doing

00:23:02,480 --> 00:23:06,919
replication with raft we don't need a

00:23:04,730 --> 00:23:09,080
distributed file system underneath we

00:23:06,919 --> 00:23:10,880
can just install directly kudu on your

00:23:09,080 --> 00:23:13,190
servers there's no other dependencies

00:23:10,880 --> 00:23:17,360
there's no zookeeper there's no HDFS etc

00:23:13,190 --> 00:23:18,740
so it's very easy to install and run the

00:23:17,360 --> 00:23:21,530
other component beyond the tablet

00:23:18,740 --> 00:23:23,570
servers is the master so the master acts

00:23:21,530 --> 00:23:25,850
as a tablet directory this is basically

00:23:23,570 --> 00:23:27,950
the phone book for the cluster when you

00:23:25,850 --> 00:23:29,419
need to read or write data the client

00:23:27,950 --> 00:23:31,309
needs to be able to find where those

00:23:29,419 --> 00:23:34,669
tablets are located and it goes to the

00:23:31,309 --> 00:23:36,530
master to find out that information it

00:23:34,669 --> 00:23:38,570
also keeps the metadata about the actual

00:23:36,530 --> 00:23:40,370
tables that exist and their schemas and

00:23:38,570 --> 00:23:43,250
that other you know small pieces of

00:23:40,370 --> 00:23:45,559
information about the cluster and lastly

00:23:43,250 --> 00:23:48,200
it enforces policies about the cluster

00:23:45,559 --> 00:23:50,330
having this centralized point for policy

00:23:48,200 --> 00:23:52,160
enforcement makes it a lot easier to

00:23:50,330 --> 00:23:53,750
reason about what's happening the

00:23:52,160 --> 00:23:56,990
previous presenter mentioned some issues

00:23:53,750 --> 00:23:59,090
around throttling rear application after

00:23:56,990 --> 00:24:01,400
a crash things like that from a

00:23:59,090 --> 00:24:03,260
centralized component are much easier to

00:24:01,400 --> 00:24:05,299
understand versus a fully peer-to-peer

00:24:03,260 --> 00:24:06,799
system where there can be emergent

00:24:05,299 --> 00:24:09,740
behavior which is more difficult to

00:24:06,799 --> 00:24:12,440
understand so because this only does

00:24:09,740 --> 00:24:14,840
these simple metadata operations it's

00:24:12,440 --> 00:24:17,150
very very fast all the data is so small

00:24:14,840 --> 00:24:20,000
we can keep it cashed in memory and

00:24:17,150 --> 00:24:23,059
every lookup is essentially receive an

00:24:20,000 --> 00:24:26,240
RPC look up in a C++ map respond the

00:24:23,059 --> 00:24:28,970
result so the 99.99 percentile is

00:24:26,240 --> 00:24:31,970
sub-millisecond in a 300 node cluster

00:24:28,970 --> 00:24:33,650
test 275 node cluster test so almost

00:24:31,970 --> 00:24:37,090
everything is on the order of a few

00:24:33,650 --> 00:24:39,740
microseconds to service these requests

00:24:37,090 --> 00:24:42,140
so this is easier to understand in a

00:24:39,740 --> 00:24:44,540
form of a diagram so on the top left we

00:24:42,140 --> 00:24:48,260
have the client the client is the java

00:24:44,540 --> 00:24:51,169
jar or the c++ library shared object or

00:24:48,260 --> 00:24:53,570
the python module running on some

00:24:51,169 --> 00:24:55,580
machine and the client needs to read or

00:24:53,570 --> 00:24:59,360
write some data so the first thing it

00:24:55,580 --> 00:25:01,190
does is it sends an RPC to the master so

00:24:59,360 --> 00:25:02,990
the master is logically one unit but

00:25:01,190 --> 00:25:05,929
actually is replicated as well also

00:25:02,990 --> 00:25:08,090
using raft so it contacts the master and

00:25:05,929 --> 00:25:10,429
says hey I'd like to write the row

00:25:08,090 --> 00:25:12,610
called T live con maybe it's a user's

00:25:10,429 --> 00:25:15,230
table and I'm the user being written and

00:25:12,610 --> 00:25:19,130
the master looks that information up in

00:25:15,230 --> 00:25:21,440
memory cached and respond saying it's

00:25:19,130 --> 00:25:24,080
part of tablet too and tablet 2 is on

00:25:21,440 --> 00:25:26,710
server Z Y and X here's their host names

00:25:24,080 --> 00:25:30,140
and the information to connect to them

00:25:26,710 --> 00:25:32,090
the client then caches that information

00:25:30,140 --> 00:25:34,280
in this little cloud on the top left

00:25:32,090 --> 00:25:35,450
called the meta cash this is so that

00:25:34,280 --> 00:25:38,360
doesn't have to keep going back and

00:25:35,450 --> 00:25:40,340
forth every time to the master for every

00:25:38,360 --> 00:25:42,290
row it reads or writes it can keep this

00:25:40,340 --> 00:25:46,070
information automatically and validate

00:25:42,290 --> 00:25:47,720
if something becomes incorrect the

00:25:46,070 --> 00:25:50,960
client then sends the update request

00:25:47,720 --> 00:25:53,630
directly to the tablet server hosting

00:25:50,960 --> 00:25:55,730
that data so the master you'll notice is

00:25:53,630 --> 00:25:58,040
not on the data path at all and doesn't

00:25:55,730 --> 00:26:00,140
form any kind of bottleneck and because

00:25:58,040 --> 00:26:05,210
the master is replicated it also doesn't

00:26:00,140 --> 00:26:06,770
form any kind of availability issue so

00:26:05,210 --> 00:26:08,929
now I want to transition and talk more

00:26:06,770 --> 00:26:11,809
about what's happening inside the tablet

00:26:08,929 --> 00:26:14,030
server on each node specifically about

00:26:11,809 --> 00:26:17,870
how we store data on disk for fast

00:26:14,030 --> 00:26:19,880
analytics so here we have a table this

00:26:17,870 --> 00:26:22,250
is an example table for the Twitter fire

00:26:19,880 --> 00:26:25,040
hose and we're just showing four columns

00:26:22,250 --> 00:26:27,799
here at in reality that's probably 40 or

00:26:25,040 --> 00:26:30,169
50 maybe more columns for every tweet

00:26:27,799 --> 00:26:32,540
that's ever been written and we have the

00:26:30,169 --> 00:26:35,500
tweet ID the username the creation

00:26:32,540 --> 00:26:39,050
timestamp and the actual text a

00:26:35,500 --> 00:26:41,360
traditional database likes a post grads

00:26:39,050 --> 00:26:44,059
from my sequel is called a roast or

00:26:41,360 --> 00:26:46,130
and that means that on disk the data is

00:26:44,059 --> 00:26:48,110
actually laid out in the same order that

00:26:46,130 --> 00:26:50,299
you would read it so starting with the

00:26:48,110 --> 00:26:53,090
top row you'll see all the columns of

00:26:50,299 --> 00:26:55,429
that top row and then following that in

00:26:53,090 --> 00:26:58,910
the file you see the next row and so on

00:26:55,429 --> 00:27:01,010
so imagine a TSV file on disk stored in

00:26:58,910 --> 00:27:04,820
the same order might be binary but

00:27:01,010 --> 00:27:07,250
essentially the same order sts-v so a

00:27:04,820 --> 00:27:09,260
column store is different a column store

00:27:07,250 --> 00:27:11,750
actually creates a separate area on disk

00:27:09,260 --> 00:27:13,970
you can think of a separate file for

00:27:11,750 --> 00:27:16,100
each of the columns so this tweet ID is

00:27:13,970 --> 00:27:19,940
stored with all the tweet IDs together

00:27:16,100 --> 00:27:22,190
and similarly the other columns so if

00:27:19,940 --> 00:27:24,740
you look at this naively you think this

00:27:22,190 --> 00:27:26,750
is probably a dumb idea when we go read

00:27:24,740 --> 00:27:28,220
one of those tweets we don't just read

00:27:26,750 --> 00:27:30,679
one file we have to go read four

00:27:28,220 --> 00:27:32,870
different files do for disk seeks to go

00:27:30,679 --> 00:27:34,790
find all the information stitch them

00:27:32,870 --> 00:27:37,370
back together spend some work to do that

00:27:34,790 --> 00:27:39,230
and in return the row and that's true if

00:27:37,370 --> 00:27:42,320
you look back in the academic literature

00:27:39,230 --> 00:27:44,270
from 15 years ago 10 years ago people

00:27:42,320 --> 00:27:46,460
thought it was crazy to do random access

00:27:44,270 --> 00:27:48,530
on a column store and in fact if you try

00:27:46,460 --> 00:27:50,480
it on a system like vertica it's not

00:27:48,530 --> 00:27:53,870
very good these older systems weren't

00:27:50,480 --> 00:27:56,000
built for this but as I talked about

00:27:53,870 --> 00:27:58,700
earlier the storage is getting so fast

00:27:56,000 --> 00:28:01,370
and the RAM is getting so large that

00:27:58,700 --> 00:28:03,080
these four discs seeks to go read those

00:28:01,370 --> 00:28:06,110
individual pieces and put them back

00:28:03,080 --> 00:28:08,240
together no longer cost very much that's

00:28:06,110 --> 00:28:11,120
actually pretty much trivial to do if

00:28:08,240 --> 00:28:14,780
it's in RAM for cache misses not a big

00:28:11,120 --> 00:28:16,669
deal so that's why it's not so bad to do

00:28:14,780 --> 00:28:20,750
random access but why is it actually

00:28:16,669 --> 00:28:23,090
good to organize your data like this the

00:28:20,750 --> 00:28:26,690
first thing to notice is the sizes of

00:28:23,090 --> 00:28:29,330
the columns so the tweet IDs and the

00:28:26,690 --> 00:28:32,000
creation timestamps are integers maybe

00:28:29,330 --> 00:28:34,610
there are 1 gigabyte each the username

00:28:32,000 --> 00:28:37,510
is text may be a little bit larger 2

00:28:34,610 --> 00:28:40,220
gigabytes and then the actual tweet is

00:28:37,510 --> 00:28:42,679
likely to be orders of magnitude larger

00:28:40,220 --> 00:28:45,830
than the other data if you 200 gigabytes

00:28:42,679 --> 00:28:47,410
and imagine you've got an analyst who

00:28:45,830 --> 00:28:50,450
comes and runs a very simple query

00:28:47,410 --> 00:28:54,530
select count star where username equals

00:28:50,450 --> 00:28:55,320
some username the hacker news bot think

00:28:54,530 --> 00:28:56,880
about how you need to

00:28:55,320 --> 00:28:58,470
implement this query if you're you know

00:28:56,880 --> 00:29:00,780
Java program we're giving these data

00:28:58,470 --> 00:29:02,820
sets you wouldn't open and read all the

00:29:00,780 --> 00:29:05,550
files you just read the username file

00:29:02,820 --> 00:29:08,040
scan through find the matching usernames

00:29:05,550 --> 00:29:10,200
and count them and that's what cooter

00:29:08,040 --> 00:29:12,900
does as well so you're reading two

00:29:10,200 --> 00:29:14,430
gigabytes of data in a row store where

00:29:12,900 --> 00:29:16,290
we've got all these columns mixed

00:29:14,430 --> 00:29:18,180
together this would be really slow

00:29:16,290 --> 00:29:20,280
because we have to skip over all those

00:29:18,180 --> 00:29:23,160
useless bites from the tweets we'd be

00:29:20,280 --> 00:29:24,570
reading 204 gigabytes of data so we're

00:29:23,160 --> 00:29:27,120
serving this query a hundred times

00:29:24,570 --> 00:29:29,670
faster by reading a hundred times less

00:29:27,120 --> 00:29:32,310
data and this is very very common for

00:29:29,670 --> 00:29:36,420
analytic workloads most analytic queries

00:29:32,310 --> 00:29:38,940
access a subset of the columns so you'll

00:29:36,420 --> 00:29:41,550
notice here I said it's a hundred times

00:29:38,940 --> 00:29:43,620
faster I can make this arbitrarily more

00:29:41,550 --> 00:29:45,810
faster so if you ever see a benchmark

00:29:43,620 --> 00:29:48,000
between a column store like kudu and a

00:29:45,810 --> 00:29:49,890
roast or like my sequel you should just

00:29:48,000 --> 00:29:51,750
realize you're comparing apples and

00:29:49,890 --> 00:29:53,940
oranges I can be a million times faster

00:29:51,750 --> 00:29:55,980
than my sequel by just adding a bunch of

00:29:53,940 --> 00:29:57,840
columns to my sequel and querying one of

00:29:55,980 --> 00:29:59,310
them so you have to be very careful to

00:29:57,840 --> 00:30:02,930
understand what workload is being

00:29:59,310 --> 00:30:06,180
benchmarked and what work that you have

00:30:02,930 --> 00:30:08,190
so the other benefit of the columns is

00:30:06,180 --> 00:30:11,070
compression so take this example

00:30:08,190 --> 00:30:12,540
creation timestamp column and maybe a

00:30:11,070 --> 00:30:15,210
little small to read for those of you in

00:30:12,540 --> 00:30:17,430
the back but the first five or six

00:30:15,210 --> 00:30:21,360
digits of these timestamps are all the

00:30:17,430 --> 00:30:23,790
same 14 for 282 and this is very very

00:30:21,360 --> 00:30:27,420
common in real datasets you're inserting

00:30:23,790 --> 00:30:29,550
data essentially as the data arrives the

00:30:27,420 --> 00:30:32,880
timestamps are clustered together with

00:30:29,550 --> 00:30:35,910
nearby rose so CUDA you can notice this

00:30:32,880 --> 00:30:37,650
information and instead of storing the

00:30:35,910 --> 00:30:40,470
column with the the full 64-bit

00:30:37,650 --> 00:30:41,970
timestamps well actually compute the

00:30:40,470 --> 00:30:44,310
difference between each successive

00:30:41,970 --> 00:30:45,960
timestamp and the differences are much

00:30:44,310 --> 00:30:47,640
much smaller than the original

00:30:45,960 --> 00:30:49,890
timestamps and this difference

00:30:47,640 --> 00:30:52,650
calculation we can do in a single CPU

00:30:49,890 --> 00:30:54,900
operation 48 timestamp at the same time

00:30:52,650 --> 00:30:57,660
there's an instruction set called AVX in

00:30:54,900 --> 00:30:59,640
modern CPUs that allows you to load a

00:30:57,660 --> 00:31:02,070
vector register with a bunch of integers

00:30:59,640 --> 00:31:04,380
and then you shift that by one and do a

00:31:02,070 --> 00:31:06,630
subtraction so in two cycles we've done

00:31:04,380 --> 00:31:09,030
eight subtractions these are essentially

00:31:06,630 --> 00:31:12,840
free to implement on modern CPUs

00:31:09,030 --> 00:31:14,520
and we get 6x compression for free it's

00:31:12,840 --> 00:31:17,370
much much faster than using something

00:31:14,520 --> 00:31:20,670
like lzo or gzip and you get great

00:31:17,370 --> 00:31:22,710
compression so a lot of compression a

00:31:20,670 --> 00:31:25,590
lot of columns can compress down to only

00:31:22,710 --> 00:31:27,930
a few bits per row things like time

00:31:25,590 --> 00:31:30,210
series values time stamps things that

00:31:27,930 --> 00:31:32,130
don't change much and also low

00:31:30,210 --> 00:31:34,620
cardinality strings like the hostname

00:31:32,130 --> 00:31:36,990
example for a time series store you may

00:31:34,620 --> 00:31:39,810
be only have a hundred hosts but the

00:31:36,990 --> 00:31:41,580
strings are 30 or 40 bytes long well 100

00:31:39,810 --> 00:31:44,430
hosts we could code that in a dictionary

00:31:41,580 --> 00:31:46,590
and use seven bits and kruti we'll

00:31:44,430 --> 00:31:48,750
figure that out and do that and give you

00:31:46,590 --> 00:31:50,610
very very good compression for free it's

00:31:48,750 --> 00:31:55,500
much easier to do in this columnar form

00:31:50,610 --> 00:31:57,030
compared to a row form so it saves you a

00:31:55,500 --> 00:31:59,580
lot of space and as we saw in that

00:31:57,030 --> 00:32:01,770
previous example saving space means less

00:31:59,580 --> 00:32:03,570
data is coming off of your devices less

00:32:01,770 --> 00:32:08,460
data is flowing through your CPU caches

00:32:03,570 --> 00:32:10,800
and things just run much faster so

00:32:08,460 --> 00:32:13,890
usually people will use guddu API

00:32:10,800 --> 00:32:15,270
directly for so the online random access

00:32:13,890 --> 00:32:17,940
but it's important to integrate with

00:32:15,270 --> 00:32:19,470
other systems for analytics so I won't

00:32:17,940 --> 00:32:21,300
read all the syntax out to you you can

00:32:19,470 --> 00:32:23,370
go find these slides later but we have

00:32:21,300 --> 00:32:26,100
spark data source integration so you can

00:32:23,370 --> 00:32:28,830
use data frames are dd's sparks equal

00:32:26,100 --> 00:32:31,170
really easily we've integrated with

00:32:28,830 --> 00:32:33,750
Impala so we have new sequel syntax for

00:32:31,170 --> 00:32:35,670
insert update delete creating kudu

00:32:33,750 --> 00:32:37,950
tables drop included tables etc and

00:32:35,670 --> 00:32:40,580
there's a few other sequel engines with

00:32:37,950 --> 00:32:43,080
work in progress integration as well and

00:32:40,580 --> 00:32:44,610
if anybody is still writing new jobs in

00:32:43,080 --> 00:32:46,170
MapReduce it's a little bit legacy at

00:32:44,610 --> 00:32:48,240
this point but we do have MapReduce

00:32:46,170 --> 00:32:49,680
integration as well we test it heavily

00:32:48,240 --> 00:32:53,790
we do a lot of stress testing if

00:32:49,680 --> 00:32:55,410
directness testing using MapReduce all

00:32:53,790 --> 00:32:56,820
right so it's a sort of wrap up here I

00:32:55,410 --> 00:32:58,770
wanted to show a couple of performance

00:32:56,820 --> 00:33:02,370
benchmarks to back up my claim that we

00:32:58,770 --> 00:33:06,420
do is actually fast so the first

00:33:02,370 --> 00:33:08,070
benchmark is t PCH G BCH is an analytics

00:33:06,420 --> 00:33:10,370
benchmark put together by this

00:33:08,070 --> 00:33:12,270
transaction processing consortium

00:33:10,370 --> 00:33:13,980
basically a bunch of people get together

00:33:12,270 --> 00:33:15,090
in a room and come up with a bunch of

00:33:13,980 --> 00:33:16,560
queries that they think their

00:33:15,090 --> 00:33:18,780
competitors will do poorly on and

00:33:16,560 --> 00:33:20,470
they'll do well on and then they publish

00:33:18,780 --> 00:33:22,659
these as a big PDF

00:33:20,470 --> 00:33:25,390
go pay someone to audit so this is a

00:33:22,659 --> 00:33:27,340
essentially 20 22 different queries and

00:33:25,390 --> 00:33:29,799
a standardized data set modeled around

00:33:27,340 --> 00:33:31,809
data warehousing and this is an example

00:33:29,799 --> 00:33:34,140
query the queries are not trivial they

00:33:31,809 --> 00:33:37,539
have things like group by order by

00:33:34,140 --> 00:33:39,130
aggregations etc and we ran this

00:33:37,539 --> 00:33:41,710
benchmark with a hundred gigabytes of

00:33:39,130 --> 00:33:44,440
generated data on a relatively large

00:33:41,710 --> 00:33:46,780
cluster 75 nodes so you'll notice if

00:33:44,440 --> 00:33:49,120
you're if you're quick this fits and RAM

00:33:46,780 --> 00:33:51,010
on this data set this data set fits in

00:33:49,120 --> 00:33:53,530
RAM on this cluster so you might think

00:33:51,010 --> 00:33:55,210
I'm cheating but as we said earlier the

00:33:53,530 --> 00:33:57,820
storage is actually getting faster and

00:33:55,210 --> 00:33:59,020
faster to approach the speed of RAM so

00:33:57,820 --> 00:34:00,789
this is the benchmark that will matter

00:33:59,020 --> 00:34:02,500
in the future for your interactive

00:34:00,789 --> 00:34:05,799
analytics it's going to be on fast

00:34:02,500 --> 00:34:09,220
storage so we ran this benchmark

00:34:05,799 --> 00:34:12,310
comparing Impala on kudu versus Impala

00:34:09,220 --> 00:34:13,659
on parque by using Impala 23 which is a

00:34:12,310 --> 00:34:16,869
little bit old at this point but will

00:34:13,659 --> 00:34:19,060
rerun it soon and these are the results

00:34:16,869 --> 00:34:21,010
we found that kudu actually was

00:34:19,060 --> 00:34:22,929
thirty-one percent faster than parque

00:34:21,010 --> 00:34:24,700
for this workload we didn't expect that

00:34:22,929 --> 00:34:25,869
we actually double check the results to

00:34:24,700 --> 00:34:28,119
make sure we didn't switch the two

00:34:25,869 --> 00:34:31,149
columns but it actually is the case for

00:34:28,119 --> 00:34:32,800
this benchmark kudu was faster for a

00:34:31,149 --> 00:34:34,839
much larger data set where it doesn't

00:34:32,800 --> 00:34:37,270
fit in RAM guddu was about forty or

00:34:34,839 --> 00:34:39,429
fifty percent slower but the the main

00:34:37,270 --> 00:34:41,800
idea here is a kudu is fast for

00:34:39,429 --> 00:34:44,560
analytics it's not a big trade-off to

00:34:41,800 --> 00:34:47,609
choose kudu vs barca vs our see file for

00:34:44,560 --> 00:34:49,869
analytics will give you a good speed

00:34:47,609 --> 00:34:51,700
comparing vs phoenix i don't know if

00:34:49,869 --> 00:34:54,760
anybody here uses phoenix it's a sequel

00:34:51,700 --> 00:34:57,220
engine on each base and basically kudu

00:34:54,760 --> 00:34:59,859
impala blows away phoenix on HBase

00:34:57,220 --> 00:35:02,740
Phoenix is great for running random

00:34:59,859 --> 00:35:05,320
access workloads but for analytics it's

00:35:02,740 --> 00:35:07,240
between 20 times slower on one query and

00:35:05,320 --> 00:35:10,240
100 something times slower on other

00:35:07,240 --> 00:35:12,040
queries so if you want to use Phoenix as

00:35:10,240 --> 00:35:14,619
a front end for random access HBase

00:35:12,040 --> 00:35:18,160
great for analytics you should probably

00:35:14,619 --> 00:35:19,750
check out kudiye and the last one is a

00:35:18,160 --> 00:35:21,780
benchmark that we lose just to show that

00:35:19,750 --> 00:35:24,580
i'm not completely marketing this thing

00:35:21,780 --> 00:35:27,849
running why csb which is a purely random

00:35:24,580 --> 00:35:29,950
access workload we were between route

00:35:27,849 --> 00:35:32,140
neck and neck for uniform be workload

00:35:29,950 --> 00:35:32,839
and some cases up to four times slower

00:35:32,140 --> 00:35:35,210
than a

00:35:32,839 --> 00:35:37,609
space we've closed some of this gap in

00:35:35,210 --> 00:35:40,009
more recent versions but basically if

00:35:37,609 --> 00:35:42,170
you have purely no sequel random access

00:35:40,009 --> 00:35:44,029
only workload probably better to use

00:35:42,170 --> 00:35:47,440
something like HBase or Cassandra one of

00:35:44,029 --> 00:35:49,609
these more traditional no sequel stores

00:35:47,440 --> 00:35:51,739
alright so if you're convinced and you'd

00:35:49,609 --> 00:35:54,380
like to try out kudu we're an open

00:35:51,739 --> 00:35:55,880
source beta the latest release is being

00:35:54,380 --> 00:35:57,890
voted upon right now or part of the

00:35:55,880 --> 00:36:00,049
apache foundation so it's purely a

00:35:57,890 --> 00:36:01,460
community project I of course worked for

00:36:00,049 --> 00:36:04,940
a vendor but we are treating this like a

00:36:01,460 --> 00:36:06,680
pure open source project and a lot of

00:36:04,940 --> 00:36:08,599
people in the community contributing and

00:36:06,680 --> 00:36:12,410
using an early users running in

00:36:08,599 --> 00:36:15,380
production you can go download find out

00:36:12,410 --> 00:36:17,479
all the information I get kuduo email

00:36:15,380 --> 00:36:19,549
our mailing list or join our slack if

00:36:17,479 --> 00:36:20,989
you want to chat with us we're always on

00:36:19,549 --> 00:36:24,440
the slack happy to chat with anybody

00:36:20,989 --> 00:36:25,999
about use cases or getting started and

00:36:24,440 --> 00:36:27,499
if you want to help with developer

00:36:25,999 --> 00:36:29,690
you'll find these slides online you can

00:36:27,499 --> 00:36:32,749
click the links we're happy to have more

00:36:29,690 --> 00:36:34,239
developers working with us so I think we

00:36:32,749 --> 00:36:41,109
have three minutes left for questions

00:36:34,239 --> 00:36:41,109
that's all right yep thanks ok thank you

00:36:46,769 --> 00:36:58,719
okay so sorry this might be a knave

00:36:56,859 --> 00:37:02,680
question that you talked about random

00:36:58,719 --> 00:37:04,029
access so do you have indexes on your

00:37:02,680 --> 00:37:08,019
coolant in order to be able to identify

00:37:04,029 --> 00:37:10,450
a specific rules yes so there every

00:37:08,019 --> 00:37:12,579
table has a primary key I which can be a

00:37:10,450 --> 00:37:14,259
composite primary key so the example I

00:37:12,579 --> 00:37:16,660
gave with host metric time stamp makes

00:37:14,259 --> 00:37:19,359
up the primary key and that's indexed

00:37:16,660 --> 00:37:21,069
the other columns are not indexed but

00:37:19,359 --> 00:37:23,950
you get the single composite key primary

00:37:21,069 --> 00:37:26,019
key index okay and Marissa question was

00:37:23,950 --> 00:37:28,479
built you talked about the fact that

00:37:26,019 --> 00:37:32,469
packet would be faster on larger data

00:37:28,479 --> 00:37:34,599
for an analytical use case in which part

00:37:32,469 --> 00:37:37,809
of the design of kuru makes it slightly

00:37:34,599 --> 00:37:39,160
slow in that case so the two big

00:37:37,809 --> 00:37:42,519
differences that I've identified so far

00:37:39,160 --> 00:37:45,069
is park a uses much larger block sizes

00:37:42,519 --> 00:37:48,130
the default is 8 mega byte blocks that

00:37:45,069 --> 00:37:50,140
are red whereas kudu by default uses 256

00:37:48,130 --> 00:37:52,299
kilobytes so on spinning disks in

00:37:50,140 --> 00:37:54,640
particular the small iOS cause a lot

00:37:52,299 --> 00:37:56,499
more seeks so it's slower the other

00:37:54,640 --> 00:37:58,809
major difference is specifically with

00:37:56,499 --> 00:38:01,209
impalas Park a reader they have this i/o

00:37:58,809 --> 00:38:02,859
manager io scheduler which keeps all the

00:38:01,209 --> 00:38:05,709
disks like a hundred percent busy and

00:38:02,859 --> 00:38:07,089
really saturate CIO and kudu doesn't

00:38:05,709 --> 00:38:10,660
have the kind of advanced read-ahead

00:38:07,089 --> 00:38:12,009
scheduler that Impala does for parque so

00:38:10,660 --> 00:38:15,579
i think with some work we could probably

00:38:12,009 --> 00:38:17,469
close that gap significantly but at the

00:38:15,579 --> 00:38:18,969
end of the day parque is simpler it

00:38:17,469 --> 00:38:21,309
doesn't have index as it doesn't have

00:38:18,969 --> 00:38:23,259
all these features for random access so

00:38:21,309 --> 00:38:28,049
it probably will be a little bit faster

00:38:23,259 --> 00:38:28,049
for that use case thank you

00:38:33,689 --> 00:38:39,099
thanks for the talk there was somebody

00:38:37,029 --> 00:38:41,650
mentioning druid in an earlier talk and

00:38:39,099 --> 00:38:43,599
from from what you've been saying about

00:38:41,650 --> 00:38:45,640
kudu a lot of this sounds like this

00:38:43,599 --> 00:38:47,709
there's there's overlap boat and the I

00:38:45,640 --> 00:38:50,529
mean both in the target use case I guess

00:38:47,709 --> 00:38:52,809
and also in the techniques internally

00:38:50,529 --> 00:38:54,660
like the like the compression techniques

00:38:52,809 --> 00:38:57,430
look like they're very simple and so on

00:38:54,660 --> 00:38:59,469
what you what you would you say are the

00:38:57,430 --> 00:39:00,880
biggest differences and we're with the

00:38:59,469 --> 00:39:02,829
sweet spots are two different systems

00:39:00,880 --> 00:39:04,359
line yes I think I think druids

00:39:02,829 --> 00:39:05,499
interesting and really great project for

00:39:04,359 --> 00:39:08,289
what it's good at I think we do have

00:39:05,499 --> 00:39:09,999
some different use cases so druid for

00:39:08,289 --> 00:39:12,039
example doesn't really support these

00:39:09,999 --> 00:39:14,109
random lookups they're very much about

00:39:12,039 --> 00:39:15,759
aggregation pre aggregating along

00:39:14,109 --> 00:39:18,400
different dimensions in order to make

00:39:15,759 --> 00:39:19,959
the analytics fast but you can't go look

00:39:18,400 --> 00:39:23,140
up an individual row in one millisecond

00:39:19,959 --> 00:39:26,189
it's not really designed for that or do

00:39:23,140 --> 00:39:28,479
700,000 random iOS per second into druid

00:39:26,189 --> 00:39:30,640
one thing it does well is it integrates

00:39:28,479 --> 00:39:32,890
very well with s3 if you're on the cloud

00:39:30,640 --> 00:39:34,359
which kudu doesn't do another big

00:39:32,890 --> 00:39:36,039
difference is that druid has sort of

00:39:34,359 --> 00:39:38,410
combined the storage engine and the

00:39:36,039 --> 00:39:40,869
query execution you can't really run

00:39:38,410 --> 00:39:43,179
like a arbitrary Impala query with joins

00:39:40,869 --> 00:39:44,949
and order buys and group buys against

00:39:43,179 --> 00:39:47,650
drew it there's a sort of set of things

00:39:44,949 --> 00:39:49,660
that droid can execute whereas with kudu

00:39:47,650 --> 00:39:51,459
because we're only a storage engine we

00:39:49,660 --> 00:39:52,689
can run whatever you want and we have

00:39:51,459 --> 00:39:54,549
the raw data so you could build

00:39:52,689 --> 00:39:58,989
clustering you could do anything you can

00:39:54,549 --> 00:40:01,539
do a spark you can do on kudu okay thank

00:39:58,989 --> 00:40:04,059
you unfortunately we need to give time

00:40:01,539 --> 00:40:05,469
for a break and for people to change

00:40:04,059 --> 00:40:06,910
rooms but if you want to continue a

00:40:05,469 --> 00:40:11,699
discussion you are more than welcome to

00:40:06,910 --> 00:40:11,699

YouTube URL: https://www.youtube.com/watch?v=z3rApSRXNMw


