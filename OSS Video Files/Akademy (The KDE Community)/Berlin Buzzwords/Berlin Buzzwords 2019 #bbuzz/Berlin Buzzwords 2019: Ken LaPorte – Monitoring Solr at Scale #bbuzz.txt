Title: Berlin Buzzwords 2019: Ken LaPorte – Monitoring Solr at Scale #bbuzz
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	It’s 3am and your phone wakes you up. A service you own is having a problem. This is an all too familiar issue for application and infrastructure teams alike. When that team is Bloomberg's Search Infrastructure team and the global financial markets are relying on the services you provide, you have to get up and fix it right away.

But how did you learn about the problem or how severe it is in the first place? And how do you scale that for hundreds and thousands of services? What can you do to ensure your services are performing within your SLAs - despite peak load (a question that becomes even more interesting when many of those services are managed using Kubernetes)?

Bloomberg's Search Infrastructure team has created a holistic, extensible and configurable monitoring solution for large scale distributed systems. Our solution allows us to scale monitoring both horizontally (the types of services) and vertically (the number of services). In this talk, I will discuss how our approach has evolved as the number of services we monitor has increased dramatically. I will detail how we leveraged Apache Kafka to improve our reliability and unlink our monitoring and alarming solutions. Finally, I’ll demonstrate how ChatOps have helped us all get a good night's sleep.

Read more:
https://2019.berlinbuzzwords.de/19/session/monitoring-solr-scale

About Ken LaPorte:
https://2019.berlinbuzzwords.de/users/ken-laporte

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,160 --> 00:00:11,430
so yeah my name is kinda Laporte I'm the

00:00:08,990 --> 00:00:14,090
team leader at Bloomberg and the search

00:00:11,430 --> 00:00:14,090
apartment

00:00:14,320 --> 00:00:18,790
how we monitor solar and and what kind

00:00:16,690 --> 00:00:20,140
of actions we take and and basically our

00:00:18,790 --> 00:00:24,130
general philosophy as well as some of

00:00:20,140 --> 00:00:28,750
the software around it so the question

00:00:24,130 --> 00:00:31,149
that I have is has this sort of thing

00:00:28,750 --> 00:00:32,680
happened to you so you get a call in the

00:00:31,149 --> 00:00:35,710
middle of the night this happened to

00:00:32,680 --> 00:00:37,180
Antrim just last week I know two o'clock

00:00:35,710 --> 00:00:39,460
in the morning you get woken up you're

00:00:37,180 --> 00:00:41,140
like what's going on

00:00:39,460 --> 00:00:43,480
sidenote that is not the Bloomberg

00:00:41,140 --> 00:00:46,360
number did a little Photoshop editing

00:00:43,480 --> 00:00:48,760
there so don't try and call that so you

00:00:46,360 --> 00:00:51,850
get woken up they say hey there's a

00:00:48,760 --> 00:00:53,380
problem you log on mill the night you

00:00:51,850 --> 00:00:54,520
work for the next two hours you figure

00:00:53,380 --> 00:00:57,160
out the problem you roll it back

00:00:54,520 --> 00:00:59,260
everything is going great and then

00:00:57,160 --> 00:01:01,150
you're looking at the clock like if I go

00:00:59,260 --> 00:01:04,629
to sleep now I'll get thirty minutes of

00:01:01,150 --> 00:01:05,650
sleep you wake up then you get it to

00:01:04,629 --> 00:01:09,759
work the next day you're feeling

00:01:05,650 --> 00:01:11,409
terrible it's just a bad scene right so

00:01:09,759 --> 00:01:13,119
we're going to talk about how to fix

00:01:11,409 --> 00:01:16,210
some of that stuff how to make that that

00:01:13,119 --> 00:01:18,130
world a little bit better for you and

00:01:16,210 --> 00:01:20,680
we're going to do that by talking about

00:01:18,130 --> 00:01:22,120
different strategies that we employ how

00:01:20,680 --> 00:01:24,460
we visualize our data

00:01:22,120 --> 00:01:27,280
what's the monitoring and alarming that

00:01:24,460 --> 00:01:31,210
we do the backend architecture and some

00:01:27,280 --> 00:01:32,920
future work that we have going on before

00:01:31,210 --> 00:01:34,860
I start though I want to talk about what

00:01:32,920 --> 00:01:37,750
a Bloomberg is because especially for

00:01:34,860 --> 00:01:40,479
Berlin a lot of people don't have never

00:01:37,750 --> 00:01:42,640
even heard of one so the the main

00:01:40,479 --> 00:01:44,440
product is the Bloomberg terminal it is

00:01:42,640 --> 00:01:46,240
the Bloomberg is the world's largest

00:01:44,440 --> 00:01:49,450
provider of financial news and

00:01:46,240 --> 00:01:51,640
information providing that in that data

00:01:49,450 --> 00:01:54,310
as quickly and accurately as possible is

00:01:51,640 --> 00:01:56,740
key to our to our use cases and to our

00:01:54,310 --> 00:01:58,150
to our customers but it goes beyond that

00:01:56,740 --> 00:02:01,840
it goes to the financial markets as a

00:01:58,150 --> 00:02:04,540
whole there's a great story that I want

00:02:01,840 --> 00:02:06,280
to tell but my friends from Amazon I

00:02:04,540 --> 00:02:07,960
don't see you guys here today so that

00:02:06,280 --> 00:02:11,590
that's kind of good

00:02:07,960 --> 00:02:14,230
in June 2008 Amazon went down for a

00:02:11,590 --> 00:02:17,620
couple hours and it was estimated that

00:02:14,230 --> 00:02:18,970
they were losing 31 million 31 thousand

00:02:17,620 --> 00:02:21,340
dollars for every minute that they were

00:02:18,970 --> 00:02:25,060
down which is just an insane amount of

00:02:21,340 --> 00:02:25,569
money three years prior Bloomberg also

00:02:25,060 --> 00:02:28,090
went down

00:02:25,569 --> 00:02:30,910
Bloomberg went down for a few

00:02:28,090 --> 00:02:32,590
it was a total of two hours and because

00:02:30,910 --> 00:02:34,989
of that the Bank of England had to

00:02:32,590 --> 00:02:38,620
postpone a bond sale worth three billion

00:02:34,989 --> 00:02:41,920
dollars so the entire financial markets

00:02:38,620 --> 00:02:44,640
move on this and I kind of like a little

00:02:41,920 --> 00:02:46,780
demonstration of like how people felt

00:02:44,640 --> 00:02:48,849
there was also one great one about like

00:02:46,780 --> 00:02:50,470
hey Bloomberg's down let's go to the pub

00:02:48,849 --> 00:02:53,590
but I I didn't know if that was

00:02:50,470 --> 00:02:56,530
appropriate for here so bringing it back

00:02:53,590 --> 00:02:59,590
to solar what do we do so the search

00:02:56,530 --> 00:03:01,450
infrastructure team provides searches of

00:02:59,590 --> 00:03:04,180
service the rest of the company we have

00:03:01,450 --> 00:03:06,430
a diverse a set of use cases everything

00:03:04,180 --> 00:03:08,260
from Chios spatial search to tech search

00:03:06,430 --> 00:03:10,870
there was a talk in here just before

00:03:08,260 --> 00:03:13,599
about multi language search we do a ton

00:03:10,870 --> 00:03:14,170
of that we also do obviously financial

00:03:13,599 --> 00:03:16,180
data

00:03:14,170 --> 00:03:20,470
everything from analytics of that

00:03:16,180 --> 00:03:22,120
financial data bonds equities all that

00:03:20,470 --> 00:03:24,670
stuff that I actually don't understand

00:03:22,120 --> 00:03:26,620
the thing of and I throw up some numbers

00:03:24,670 --> 00:03:27,940
here this these numbers are here just to

00:03:26,620 --> 00:03:30,970
give you a sense of the scale the

00:03:27,940 --> 00:03:33,730
footprint that we're monitoring and to

00:03:30,970 --> 00:03:35,560
kind of say with all of this we really

00:03:33,730 --> 00:03:38,530
do have to have a serious amount of

00:03:35,560 --> 00:03:41,139
orchestration around this so that when

00:03:38,530 --> 00:03:45,459
something goes wrong we know about it we

00:03:41,139 --> 00:03:48,040
can react and what about our team what

00:03:45,459 --> 00:03:50,109
is our team do so throughout Bloomberg

00:03:48,040 --> 00:03:52,959
there's multiple teams involved in

00:03:50,109 --> 00:03:55,060
search most notably our team there's a

00:03:52,959 --> 00:03:57,489
new search team that does terrific work

00:03:55,060 --> 00:04:01,120
you might know Christine from that team

00:03:57,489 --> 00:04:03,940
and Daniel Collins and Rahm they do

00:04:01,120 --> 00:04:06,190
terrific work Denis Cove was on this

00:04:03,940 --> 00:04:09,519
team for a little while one of the solar

00:04:06,190 --> 00:04:13,060
committees in PMC member so all told we

00:04:09,519 --> 00:04:15,609
have three committers two people on the

00:04:13,060 --> 00:04:18,849
PMC and we've made major contributions

00:04:15,609 --> 00:04:22,780
to solar in the last few years basically

00:04:18,849 --> 00:04:25,570
in every version since for two I think

00:04:22,780 --> 00:04:27,130
we've had some changes a lot of it's bug

00:04:25,570 --> 00:04:29,740
fixes but we've had some major

00:04:27,130 --> 00:04:32,770
contributions to so for example Dennis

00:04:29,740 --> 00:04:36,370
created with the fine folks at alfresco

00:04:32,770 --> 00:04:38,190
like Mike and Joel the streaming

00:04:36,370 --> 00:04:43,230
expressions component

00:04:38,190 --> 00:04:45,150
we also from our discovery team we

00:04:43,230 --> 00:04:47,700
created the learning to rank component

00:04:45,150 --> 00:04:50,040
which has been really successful so far

00:04:47,700 --> 00:04:52,200
in fact I think that's that's even been

00:04:50,040 --> 00:04:55,530
move it over to elasticsearch and we use

00:04:52,200 --> 00:04:58,530
it in production to the tune of I think

00:04:55,530 --> 00:05:02,010
it's like 30,000 queries a second so

00:04:58,530 --> 00:05:03,330
really very heavily used and Huston who

00:05:02,010 --> 00:05:05,340
was here just a minute ago but had to

00:05:03,330 --> 00:05:07,380
step out he created an analytics

00:05:05,340 --> 00:05:11,250
component so that you can do spark like

00:05:07,380 --> 00:05:12,900
aggregations within so or so trig your

00:05:11,250 --> 00:05:16,350
data exists in both you can push all

00:05:12,900 --> 00:05:19,290
that push all that down all this to say

00:05:16,350 --> 00:05:22,200
is that we are major believers in not

00:05:19,290 --> 00:05:25,380
only a solar but also open-source we're

00:05:22,200 --> 00:05:27,360
trying to really like advocate for that

00:05:25,380 --> 00:05:33,150
at Bloomberg and so far we've been very

00:05:27,360 --> 00:05:34,290
successful all right so just a quick

00:05:33,150 --> 00:05:36,570
review of some of the terms that we're

00:05:34,290 --> 00:05:38,220
going to be talking about so solar cloud

00:05:36,570 --> 00:05:39,780
I think everyone's pretty familiar with

00:05:38,220 --> 00:05:42,420
that it's basically a cluster that it

00:05:39,780 --> 00:05:45,840
provides high availability and fault

00:05:42,420 --> 00:05:48,660
tolerance each cloud may be made up of

00:05:45,840 --> 00:05:51,630
multiple collections a collection is

00:05:48,660 --> 00:05:53,910
just a complete logical index some

00:05:51,630 --> 00:05:55,620
clouds might have one index sorry some

00:05:53,910 --> 00:05:58,080
clouds might have one collection some

00:05:55,620 --> 00:06:01,710
clouds might have many we run in very

00:05:58,080 --> 00:06:02,820
different scenarios there that that

00:06:01,710 --> 00:06:05,520
collection might be broken down to

00:06:02,820 --> 00:06:07,820
multiple shards each shard is just a lot

00:06:05,520 --> 00:06:10,410
to go piece of the index simple enough

00:06:07,820 --> 00:06:13,050
and then for each of those shards you

00:06:10,410 --> 00:06:14,310
might have replicas or many replicas for

00:06:13,050 --> 00:06:16,920
each replicas you have to have a leader

00:06:14,310 --> 00:06:19,500
and that leader is responsible to make

00:06:16,920 --> 00:06:21,660
sure that indexing happens should that

00:06:19,500 --> 00:06:25,710
leader go away you're in your ability to

00:06:21,660 --> 00:06:27,090
index new data becomes a problem we're

00:06:25,710 --> 00:06:31,310
going to talk some some more about that

00:06:27,090 --> 00:06:33,480
later some other key terms zookeeper

00:06:31,310 --> 00:06:37,080
zookeeper is a consensus management

00:06:33,480 --> 00:06:38,940
system that solar heavily relies on we

00:06:37,080 --> 00:06:40,850
use it internally also as offering that

00:06:38,940 --> 00:06:43,380
as a consent as a leader election

00:06:40,850 --> 00:06:46,530
service to our users and for

00:06:43,380 --> 00:06:48,480
distributive configuration management so

00:06:46,530 --> 00:06:49,770
throughout this talk I'll also mention

00:06:48,480 --> 00:06:50,800
how we monitor that because there's a

00:06:49,770 --> 00:06:55,030
lot of parallels

00:06:50,800 --> 00:06:56,979
between the two finally service provider

00:06:55,030 --> 00:06:59,710
that's me that's the guy who's offering

00:06:56,979 --> 00:07:01,270
the search as a service our tenants

00:06:59,710 --> 00:07:04,629
I think Anton put it really well

00:07:01,270 --> 00:07:06,909
yesterday there are hostile users there

00:07:04,629 --> 00:07:08,349
the thing they're the guys who have

00:07:06,909 --> 00:07:09,909
business use cases that they're trying

00:07:08,349 --> 00:07:11,229
to solve they're trying to address a

00:07:09,909 --> 00:07:13,750
business problem without necessarily

00:07:11,229 --> 00:07:15,159
having to fight against the

00:07:13,750 --> 00:07:19,030
functionality of the platform that

00:07:15,159 --> 00:07:21,310
exists I'm just kidding we actually love

00:07:19,030 --> 00:07:22,870
our users they really do push this and

00:07:21,310 --> 00:07:27,490
and I think they make our platform

00:07:22,870 --> 00:07:28,930
better they make solar better so we're

00:07:27,490 --> 00:07:31,440
going to obviously talk about monitoring

00:07:28,930 --> 00:07:33,669
but the alarming is also critical like

00:07:31,440 --> 00:07:34,900
when something happens how do you get

00:07:33,669 --> 00:07:37,659
how do you get notified how to get

00:07:34,900 --> 00:07:42,400
involved and had how do you finally come

00:07:37,659 --> 00:07:45,539
to a resolution so what does it mean to

00:07:42,400 --> 00:07:45,539
actually monitor solar

00:07:46,289 --> 00:07:50,020
this is a common question that we were

00:07:48,759 --> 00:07:52,479
getting when we were discussing like how

00:07:50,020 --> 00:07:54,699
giving this talk and a number of people

00:07:52,479 --> 00:07:58,240
were like well why not just use metrics

00:07:54,699 --> 00:08:00,520
why not just use Prometheus metrics JMX

00:07:58,240 --> 00:08:02,590
metrics and figure out like can you

00:08:00,520 --> 00:08:04,060
figure out the state of solar from that

00:08:02,590 --> 00:08:06,580
can you figure out how it's performing

00:08:04,060 --> 00:08:09,190
if everything is the problem if anything

00:08:06,580 --> 00:08:10,750
is a problem and then go from there

00:08:09,190 --> 00:08:12,699
what we've learned over time when we

00:08:10,750 --> 00:08:15,190
have some gear issues about this is that

00:08:12,699 --> 00:08:16,930
metrics can lie they can tell you things

00:08:15,190 --> 00:08:19,990
are fine when they're not they can tell

00:08:16,930 --> 00:08:22,779
you things are broken when you might

00:08:19,990 --> 00:08:26,139
just be in a degraded state you might be

00:08:22,779 --> 00:08:28,539
completely fine so we realized that

00:08:26,139 --> 00:08:31,779
metrics are not just one measure that we

00:08:28,539 --> 00:08:34,829
can rely on so instead we said okay

00:08:31,779 --> 00:08:37,570
we're gonna measure live nodes really

00:08:34,829 --> 00:08:40,419
most simply is solar running and is he

00:08:37,570 --> 00:08:42,219
connected to zookeeper so when solar I

00:08:40,419 --> 00:08:44,020
think most you guys know but when solar

00:08:42,219 --> 00:08:46,899
starts up it makes a connection to

00:08:44,020 --> 00:08:49,149
zookeeper and says I'm here should that

00:08:46,899 --> 00:08:51,070
solar and since go away that connection

00:08:49,149 --> 00:08:54,070
will time out and that node will

00:08:51,070 --> 00:08:56,019
disappear from zookeeper so we we

00:08:54,070 --> 00:08:59,589
actually watch this and say is that node

00:08:56,019 --> 00:09:02,290
there is it where I expect it to be we

00:08:59,589 --> 00:09:03,130
also use cluster state or now state JSON

00:09:02,290 --> 00:09:04,870
for monitoring

00:09:03,130 --> 00:09:07,240
this gives us a ton of information like

00:09:04,870 --> 00:09:09,430
you know what does solar think its

00:09:07,240 --> 00:09:12,160
current status not necessarily what it

00:09:09,430 --> 00:09:15,730
is but what does it think it is it tells

00:09:12,160 --> 00:09:19,270
us what's running where sometimes the

00:09:15,730 --> 00:09:21,880
infrastructure such as ours we we have a

00:09:19,270 --> 00:09:23,890
very declarative view on solar we know

00:09:21,880 --> 00:09:25,570
what should be running where and this

00:09:23,890 --> 00:09:26,710
helps us determine if there's a

00:09:25,570 --> 00:09:29,140
difference between what is running

00:09:26,710 --> 00:09:31,780
versus what should be running and we can

00:09:29,140 --> 00:09:34,870
alarm in a monitor off of that and

00:09:31,780 --> 00:09:39,670
finally is there a leader for each for

00:09:34,870 --> 00:09:43,030
each shard obviously very critical we do

00:09:39,670 --> 00:09:45,070
a lot of indexing if if we don't have

00:09:43,030 --> 00:09:50,170
leadership we are really going to have a

00:09:45,070 --> 00:09:53,230
problem so getting back to that metric

00:09:50,170 --> 00:09:55,150
stuff we do also monitor metrics we want

00:09:53,230 --> 00:09:57,870
to know like is it performing is

00:09:55,150 --> 00:10:01,330
everything operating the way it should

00:09:57,870 --> 00:10:03,430
we look at we look at PC a lot whenever

00:10:01,330 --> 00:10:05,710
there's a problem that's one of the

00:10:03,430 --> 00:10:06,970
first places that we turn to that was

00:10:05,710 --> 00:10:09,010
also mentioned in antonyms talk

00:10:06,970 --> 00:10:10,810
yesterday and a little shout out there's

00:10:09,010 --> 00:10:13,180
a company called tier one app that

00:10:10,810 --> 00:10:14,530
produces something called GC easy we've

00:10:13,180 --> 00:10:17,830
been using that for a little while to

00:10:14,530 --> 00:10:19,510
kind of analyze our GC logs and it's

00:10:17,830 --> 00:10:23,260
been able to point out some problems and

00:10:19,510 --> 00:10:25,300
way just to reconfigure solar how we run

00:10:23,260 --> 00:10:29,230
it that it made major differences in

00:10:25,300 --> 00:10:31,810
stability of some collections finally we

00:10:29,230 --> 00:10:33,790
do basic log analysis we're looking at

00:10:31,810 --> 00:10:36,910
our logs saying hey do we see a problem

00:10:33,790 --> 00:10:38,950
here are we seeing this error our racing

00:10:36,910 --> 00:10:44,140
a lot of these errors and then

00:10:38,950 --> 00:10:45,250
responding to that and finally like any

00:10:44,140 --> 00:10:47,320
good service provider we have a

00:10:45,250 --> 00:10:50,650
middleware we typically don't let users

00:10:47,320 --> 00:10:52,570
connect directly to solar so this metal

00:10:50,650 --> 00:10:54,160
where does everything from like hey let

00:10:52,570 --> 00:10:58,240
me do a health check Pig let me make

00:10:54,160 --> 00:10:59,860
sure everything is working and obviously

00:10:58,240 --> 00:11:02,230
if our middleware that our users are

00:10:59,860 --> 00:11:03,850
trying to connect to can't connect to

00:11:02,230 --> 00:11:04,630
solar that's just as big of a problem

00:11:03,850 --> 00:11:06,370
for us

00:11:04,630 --> 00:11:08,170
even if solar is in a healthy state if

00:11:06,370 --> 00:11:12,069
there's some network disconnect between

00:11:08,170 --> 00:11:13,869
the two we have a broken system

00:11:12,069 --> 00:11:16,269
and finally another shout-out to Antrim

00:11:13,869 --> 00:11:19,540
some of the work that he's going to do

00:11:16,269 --> 00:11:24,730
to or release I should say in terms of

00:11:19,540 --> 00:11:26,649
like capturing bad user will call

00:11:24,730 --> 00:11:29,709
behavior where they have too many

00:11:26,649 --> 00:11:32,559
documents going too high off of scale

00:11:29,709 --> 00:11:34,629
and whatnot if those are gonna go to

00:11:32,559 --> 00:11:38,679
logs and we'll add rules for our log

00:11:34,629 --> 00:11:40,990
analysis so all this is to say as new

00:11:38,679 --> 00:11:42,910
features to are added to solar we can

00:11:40,990 --> 00:11:47,410
then integrate those features into our

00:11:42,910 --> 00:11:48,639
own monitoring stack zookeeper zookeeper

00:11:47,410 --> 00:11:52,029
is pretty much the same thing

00:11:48,639 --> 00:11:54,879
so zookeeper has a simple are you okay

00:11:52,029 --> 00:11:56,610
for letter command question do you guys

00:11:54,879 --> 00:12:00,540
know what the four letter commands are

00:11:56,610 --> 00:12:00,540
one person awesome

00:12:01,029 --> 00:12:05,589
so one of the main ways up till 3 v of

00:12:04,119 --> 00:12:08,139
monitoring zookeeper was issuing the

00:12:05,589 --> 00:12:09,879
four letter commands these are simple

00:12:08,139 --> 00:12:12,490
things that it's a simple text interface

00:12:09,879 --> 00:12:14,439
that says you send it are you okay and

00:12:12,490 --> 00:12:18,429
it comes back I'm okay

00:12:14,439 --> 00:12:22,110
or hey I'm up but I'm not running as

00:12:18,429 --> 00:12:24,389
part of an ensemble or nothing at all

00:12:22,110 --> 00:12:26,980
this is the main way of monitoring

00:12:24,389 --> 00:12:28,959
zookeeper in addition to monitoring

00:12:26,980 --> 00:12:31,569
zookeeper that way you can also use JMX

00:12:28,959 --> 00:12:35,529
metrics but as I mentioned before they

00:12:31,569 --> 00:12:38,949
lie so there's another four-letter

00:12:35,529 --> 00:12:41,220
command MN TR MN t art gives you a lot

00:12:38,949 --> 00:12:44,470
of I think that's new in three three

00:12:41,220 --> 00:12:46,209
that gives you a lot of different data

00:12:44,470 --> 00:12:48,819
points that you can use to monitor solar

00:12:46,209 --> 00:12:51,819
everything from how many Z nodes do you

00:12:48,819 --> 00:12:55,689
have to how many watches the zookeeper

00:12:51,819 --> 00:12:58,329
version we had a problem about a year

00:12:55,689 --> 00:13:00,730
ago where turned out that we had

00:12:58,329 --> 00:13:03,220
upgraded a version of zookeeper

00:13:00,730 --> 00:13:05,709
somewhere and that up just one node

00:13:03,220 --> 00:13:10,240
didn't get bounced to pick up that new

00:13:05,709 --> 00:13:13,809
version this this cost basically timing

00:13:10,240 --> 00:13:15,459
issues between nodes once we discover

00:13:13,809 --> 00:13:16,839
that we're like okay we're going to add

00:13:15,459 --> 00:13:18,279
monitoring for that so that that never

00:13:16,839 --> 00:13:19,449
happens again

00:13:18,279 --> 00:13:22,629
and we created a little tool called

00:13:19,449 --> 00:13:24,670
version ater that does that and also

00:13:22,629 --> 00:13:26,980
just like solar

00:13:24,670 --> 00:13:27,850
keeper also has the sense of a leader so

00:13:26,980 --> 00:13:29,800
we want to make sure that we've

00:13:27,850 --> 00:13:32,730
identified the leader understand where

00:13:29,800 --> 00:13:32,730
it is and what it's doing

00:13:32,820 --> 00:13:38,500
back to metrics and logs we use these to

00:13:36,520 --> 00:13:40,960
understand what is the state of

00:13:38,500 --> 00:13:44,680
zookeeper how is it performing and what

00:13:40,960 --> 00:13:45,760
not and finally there's one last

00:13:44,680 --> 00:13:48,580
four-letter command that's worth

00:13:45,760 --> 00:13:51,160
pointing out we do this for tracking not

00:13:48,580 --> 00:13:53,670
for a really analysis but it's cons

00:13:51,160 --> 00:13:56,650
which shows you the connection status

00:13:53,670 --> 00:13:58,980
and it's also I don't know if you guys

00:13:56,650 --> 00:14:03,520
heard about a month ago a zookeeper 3/5

00:13:58,980 --> 00:14:05,170
was finally released the nice thing

00:14:03,520 --> 00:14:06,880
about this is it allows for dynamic

00:14:05,170 --> 00:14:09,970
reconfiguration of your zookeeper

00:14:06,880 --> 00:14:12,340
ensemble which is incredibly important

00:14:09,970 --> 00:14:13,950
once you give once you start up a

00:14:12,340 --> 00:14:16,660
process that relies on the zookeeper

00:14:13,950 --> 00:14:19,330
it's kind of assumed that that ensemble

00:14:16,660 --> 00:14:20,800
will never ever change this makes it

00:14:19,330 --> 00:14:22,570
very difficult from an operational

00:14:20,800 --> 00:14:26,500
perspective to kind of orchestrate

00:14:22,570 --> 00:14:28,750
around that the other nice thing is that

00:14:26,500 --> 00:14:31,770
they now have these four-letter commands

00:14:28,750 --> 00:14:36,580
available from a jetty rest endpoint so

00:14:31,770 --> 00:14:40,140
no more you know using NC or direct

00:14:36,580 --> 00:14:40,140
connections to the to these things

00:14:40,200 --> 00:14:46,360
alright so let's talk about how we

00:14:43,120 --> 00:14:49,960
visualize so or I kind of want to give

00:14:46,360 --> 00:14:52,570
you a history a bit see where we came

00:14:49,960 --> 00:14:55,300
from and kind of like also a little bit

00:14:52,570 --> 00:14:57,130
of where we're going so what you see

00:14:55,300 --> 00:14:59,140
here is part is a screen capture from a

00:14:57,130 --> 00:15:02,710
Bloomberg terminal I apologize it's a

00:14:59,140 --> 00:15:04,330
little funny or fuzzy I should say it's

00:15:02,710 --> 00:15:06,040
fuzzy because those are some of our

00:15:04,330 --> 00:15:08,040
server names and we have to we protect

00:15:06,040 --> 00:15:09,970
them so I blurred them out a little bit

00:15:08,040 --> 00:15:13,410
you might see that it through a couple

00:15:09,970 --> 00:15:15,880
of slides my apologies kind of necessary

00:15:13,410 --> 00:15:18,550
so getting back to this this is this was

00:15:15,880 --> 00:15:22,120
our initial version it showed a single

00:15:18,550 --> 00:15:24,010
cloud deal and it showed you across

00:15:22,120 --> 00:15:26,800
different environments in this case our

00:15:24,010 --> 00:15:29,590
dev alpha and prod and it was a snapshot

00:15:26,800 --> 00:15:32,110
in time this was very useful for us to

00:15:29,590 --> 00:15:33,840
just say hey something is someone is

00:15:32,110 --> 00:15:36,460
saying that there's a problem with this

00:15:33,840 --> 00:15:37,530
let us go look and see what we can

00:15:36,460 --> 00:15:39,240
figure out

00:15:37,530 --> 00:15:42,740
is this in the degraded State is

00:15:39,240 --> 00:15:46,050
everything okay really kind of crummy

00:15:42,740 --> 00:15:47,970
there it's a very very limited view and

00:15:46,050 --> 00:15:50,220
for as a service provider not very

00:15:47,970 --> 00:15:51,840
useful so we're like okay let's imagine

00:15:50,220 --> 00:15:55,140
how this would look if we're going to do

00:15:51,840 --> 00:15:57,930
this across multiple clouds so we have a

00:15:55,140 --> 00:15:59,670
new one also built into the terminal

00:15:57,930 --> 00:16:02,190
this was going to display all the

00:15:59,670 --> 00:16:03,510
different clouds we're going to be do

00:16:02,190 --> 00:16:05,820
really cool things like sort them by

00:16:03,510 --> 00:16:08,130
status of the the really bad ones are at

00:16:05,820 --> 00:16:11,610
the top and and we can kind of address

00:16:08,130 --> 00:16:12,330
those in order of importance we're going

00:16:11,610 --> 00:16:13,980
to be able to switch between

00:16:12,330 --> 00:16:15,840
environments so that you can see like

00:16:13,980 --> 00:16:17,400
let me see all my prod clusters let me

00:16:15,840 --> 00:16:21,210
see all my dev clusters alpha clusters

00:16:17,400 --> 00:16:23,460
whatnot you can do various commands you

00:16:21,210 --> 00:16:26,190
can do hey I'm gonna stop this note I'm

00:16:23,460 --> 00:16:28,950
gonna restart this rack I'm going to

00:16:26,190 --> 00:16:32,070
like operate at a scale so that we can

00:16:28,950 --> 00:16:33,780
kind of orchestrate big changes this was

00:16:32,070 --> 00:16:36,540
a big improvement from us for us from a

00:16:33,780 --> 00:16:39,990
workflow perspective but it still wasn't

00:16:36,540 --> 00:16:42,060
quite right it's still pointed time I

00:16:39,990 --> 00:16:43,800
had mentioned earlier that we have

00:16:42,060 --> 00:16:46,170
hundreds I think it's actually thousands

00:16:43,800 --> 00:16:50,540
now of different solar applications that

00:16:46,170 --> 00:16:53,160
we monitor this doesn't scale that way

00:16:50,540 --> 00:16:55,370
we this I think was fine when we had a

00:16:53,160 --> 00:17:01,430
couple we had like a hundred or two

00:16:55,370 --> 00:17:01,430
after that it kind of broke oops sorry

00:17:02,000 --> 00:17:07,319
so we created something called night owl

00:17:04,189 --> 00:17:10,050
night now is the name of both a

00:17:07,319 --> 00:17:11,850
front-end and back-end service the idea

00:17:10,050 --> 00:17:15,630
with a night owl is that it was going to

00:17:11,850 --> 00:17:17,040
actively monitor solar all the time this

00:17:15,630 --> 00:17:21,480
is what our monitoring platform is built

00:17:17,040 --> 00:17:24,060
on today the idea behind it is that we

00:17:21,480 --> 00:17:29,790
viewed solar as kind of this tree

00:17:24,060 --> 00:17:31,590
structure we said okay what do we what

00:17:29,790 --> 00:17:34,020
do we really need to monitor well we

00:17:31,590 --> 00:17:37,020
need to understand how the different

00:17:34,020 --> 00:17:40,740
leaf nodes act how they how they operate

00:17:37,020 --> 00:17:42,450
up the tree how one leaf node interacts

00:17:40,740 --> 00:17:45,900
with another and we'll talk about some

00:17:42,450 --> 00:17:47,490
of the specifics around that later but

00:17:45,900 --> 00:17:49,560
this this gave us a really good view

00:17:47,490 --> 00:17:51,150
this is like hey this is a fairly

00:17:49,560 --> 00:17:54,720
scalable way

00:17:51,150 --> 00:17:56,970
of seeing exactly how our infrastructure

00:17:54,720 --> 00:17:59,070
was doing but it was still limited

00:17:56,970 --> 00:18:01,050
this was our this was the the service

00:17:59,070 --> 00:18:02,490
provider for you this is our view we

00:18:01,050 --> 00:18:04,650
couldn't give this to tenants because it

00:18:02,490 --> 00:18:06,870
didn't provide them any value all they

00:18:04,650 --> 00:18:08,940
would see is like hey is there a major

00:18:06,870 --> 00:18:09,540
widespread problem or is it just my

00:18:08,940 --> 00:18:11,250
problem

00:18:09,540 --> 00:18:12,540
they didn't really care if it was a

00:18:11,250 --> 00:18:14,970
widespread problem they just cared that

00:18:12,540 --> 00:18:20,250
they had a problem and this didn't come

00:18:14,970 --> 00:18:22,560
close to doing it for us so I apologize

00:18:20,250 --> 00:18:25,040
every time I shake my hand the slide

00:18:22,560 --> 00:18:25,040
changes

00:18:36,670 --> 00:18:43,100
I'll put the mouse down so I don't do

00:18:38,810 --> 00:18:46,070
that anymore so we created Watchtower

00:18:43,100 --> 00:18:47,630
the idea behind Watchtower is that hey

00:18:46,070 --> 00:18:50,360
we're gonna lift the service providers

00:18:47,630 --> 00:18:51,830
and our clients see the same view they

00:18:50,360 --> 00:18:55,070
were going to be able to see what we

00:18:51,830 --> 00:18:56,390
could see with some exceptions we're

00:18:55,070 --> 00:18:58,130
going to be able to control everything

00:18:56,390 --> 00:19:01,190
from this our processes were going to be

00:18:58,130 --> 00:19:03,440
built to this we could integrate our to

00:19:01,190 --> 00:19:05,330
our second day things like hey we want

00:19:03,440 --> 00:19:07,430
to do a schema change we can do it

00:19:05,330 --> 00:19:09,260
through here you want to be able to

00:19:07,430 --> 00:19:11,210
monitor our metrics you can do it here

00:19:09,260 --> 00:19:13,810
you can configure your security policies

00:19:11,210 --> 00:19:16,250
everything can be done in this one view

00:19:13,810 --> 00:19:18,920
this was a major improvement from what

00:19:16,250 --> 00:19:21,860
we were doing before it allowed us as

00:19:18,920 --> 00:19:31,490
well as our tenants to really introspect

00:19:21,860 --> 00:19:36,760
their systems but so sorry bring this

00:19:31,490 --> 00:19:38,960
over here it also allowed us to

00:19:36,760 --> 00:19:42,380
configure our alarming and and do a

00:19:38,960 --> 00:19:44,030
little bit of interesting analysis so

00:19:42,380 --> 00:19:46,760
every time our monitoring software

00:19:44,030 --> 00:19:49,250
produced an event we're then logging

00:19:46,760 --> 00:19:51,320
we're then be able to display all these

00:19:49,250 --> 00:19:53,270
events as a cumulative view to our users

00:19:51,320 --> 00:19:56,620
so we can integrate things like this

00:19:53,270 --> 00:20:01,880
users stop this process this user

00:19:56,620 --> 00:20:04,940
created this schema job this cloud had

00:20:01,880 --> 00:20:07,100
an out of memory exception this cloud

00:20:04,940 --> 00:20:08,960
went into recovery and we can kind of

00:20:07,100 --> 00:20:12,440
visualize and capture all those events

00:20:08,960 --> 00:20:15,530
in one UI once we had this we're able to

00:20:12,440 --> 00:20:17,600
slice a dice a little bit so one of the

00:20:15,530 --> 00:20:19,280
big things that we found through being

00:20:17,600 --> 00:20:21,890
able to store this information and then

00:20:19,280 --> 00:20:24,380
visualize and and perform analytics on

00:20:21,890 --> 00:20:26,240
this is that we found out that over the

00:20:24,380 --> 00:20:29,060
course of the night one of our bigger

00:20:26,240 --> 00:20:32,510
tenants would go into recovery hundreds

00:20:29,060 --> 00:20:34,940
or thousands of times just in credit any

00:20:32,510 --> 00:20:36,850
alarms it was just one note going down

00:20:34,940 --> 00:20:41,390
and coming back up a few minutes later

00:20:36,850 --> 00:20:44,120
but we what we looked at is hey why is

00:20:41,390 --> 00:20:46,280
this happening really understanding like

00:20:44,120 --> 00:20:48,559
what is what is the youth what is the

00:20:46,280 --> 00:20:50,600
problem that's causing this to occur

00:20:48,559 --> 00:20:54,129
and when we dug into it it was all just

00:20:50,600 --> 00:20:56,299
just GC issues a node would go into GC

00:20:54,129 --> 00:20:59,080
15 seconds later it would pop back

00:20:56,299 --> 00:21:01,519
reconnect a zookeeper and it was happy

00:20:59,080 --> 00:21:05,179
we did some analysis we spent about a

00:21:01,519 --> 00:21:07,789
month on it and we found out hey CMS was

00:21:05,179 --> 00:21:10,789
causing the problem we switch to g1 GC

00:21:07,789 --> 00:21:13,039
and all of a sudden that went away that

00:21:10,789 --> 00:21:15,080
flutter that was overnight it stopped

00:21:13,039 --> 00:21:16,580
happening and then when we did that

00:21:15,080 --> 00:21:18,529
throughout all of our infrastructure

00:21:16,580 --> 00:21:20,389
everything immediately became more

00:21:18,529 --> 00:21:22,159
stable and we've been able to do that

00:21:20,389 --> 00:21:26,749
for a few different iterations that's

00:21:22,159 --> 00:21:30,590
just one fairly major case where this

00:21:26,749 --> 00:21:33,440
analysis has allowed us to find problems

00:21:30,590 --> 00:21:35,119
that were systemic throughout our system

00:21:33,440 --> 00:21:39,769
and then make the make changes to

00:21:35,119 --> 00:21:41,240
address those this view also allowed us

00:21:39,769 --> 00:21:43,429
to slice and dice the data in a

00:21:41,240 --> 00:21:46,639
different way it let us look at things

00:21:43,429 --> 00:21:48,830
like hey is there a host that's bad

00:21:46,639 --> 00:21:52,610
is there a network segment that's having

00:21:48,830 --> 00:21:55,100
problems is there a particular security

00:21:52,610 --> 00:21:57,320
policy is there a particular tenant is

00:21:55,100 --> 00:22:00,190
there a particular anything that would

00:21:57,320 --> 00:22:03,769
that we can identify as causing issues

00:22:00,190 --> 00:22:06,350
this ability to like have introspect the

00:22:03,769 --> 00:22:09,830
data in different ways has let us really

00:22:06,350 --> 00:22:10,549
enjoy our monitoring and orchestration

00:22:09,830 --> 00:22:13,580
frameworks

00:22:10,549 --> 00:22:15,409
it's prevented us from actually having

00:22:13,580 --> 00:22:17,570
outages because we can we can see

00:22:15,409 --> 00:22:23,269
patterns and trends that otherwise

00:22:17,570 --> 00:22:25,100
wouldn't be visible but it's still not

00:22:23,269 --> 00:22:27,110
right our tenants really don't like it

00:22:25,100 --> 00:22:29,570
so we've decided that we're gonna create

00:22:27,110 --> 00:22:31,309
one more view and this is the last one

00:22:29,570 --> 00:22:34,629
I'm sure it's the last one we're never

00:22:31,309 --> 00:22:34,629
going to create another UI after this

00:22:35,110 --> 00:22:40,039
we're also working with other service

00:22:37,460 --> 00:22:43,669
providers in our area who also have

00:22:40,039 --> 00:22:45,259
their own you eyes what you know we

00:22:43,669 --> 00:22:48,350
believe in the open source model in our

00:22:45,259 --> 00:22:51,350
group other teams as well do why can't

00:22:48,350 --> 00:22:54,409
our Cassandra service provider also

00:22:51,350 --> 00:22:55,340
share the same UI that solar does so

00:22:54,409 --> 00:22:57,110
that's one of the things that we're

00:22:55,340 --> 00:22:59,059
looking to accomplish here obviously

00:22:57,110 --> 00:23:02,240
we're still early in the wireframe

00:22:59,059 --> 00:23:04,280
stages but it's also going to be geared

00:23:02,240 --> 00:23:06,440
words are tenants themselves it's what

00:23:04,280 --> 00:23:10,370
they want to see so what they're focused

00:23:06,440 --> 00:23:14,090
on is is everything up how do I connect

00:23:10,370 --> 00:23:16,030
and what are my metrics they don't care

00:23:14,090 --> 00:23:18,710
about the other stuff that we had put in

00:23:16,030 --> 00:23:21,620
so we're working closely with them to

00:23:18,710 --> 00:23:23,630
figure out ok what do they care about

00:23:21,620 --> 00:23:31,700
what do they want and we're gonna try

00:23:23,630 --> 00:23:35,500
and deliver that for them alright so

00:23:31,700 --> 00:23:38,870
getting back to our our monitoring

00:23:35,500 --> 00:23:40,190
there's something that we created that I

00:23:38,870 --> 00:23:43,820
think is really making a big difference

00:23:40,190 --> 00:23:47,450
so we created a chat bot in slack called

00:23:43,820 --> 00:23:50,270
BFS bottom the idea of it is hey you get

00:23:47,450 --> 00:23:52,309
woken up in the middle of the night how

00:23:50,270 --> 00:23:54,020
do you react how can you identify how

00:23:52,309 --> 00:23:57,290
bad a problem is can you identify what

00:23:54,020 --> 00:23:59,660
the problem is and it's all with the

00:23:57,290 --> 00:24:00,830
goal of hey let me identify it let me

00:23:59,660 --> 00:24:02,720
address it and go back to sleep without

00:24:00,830 --> 00:24:05,920
ever leaving bed without ever opening up

00:24:02,720 --> 00:24:08,000
a laptop and from a security perspective

00:24:05,920 --> 00:24:10,610
we just want to make sure it was

00:24:08,000 --> 00:24:13,850
read-only Spotify did an interesting

00:24:10,610 --> 00:24:15,950
talking here yesterday where they threw

00:24:13,850 --> 00:24:18,290
a slack pot actually do like index

00:24:15,950 --> 00:24:19,160
rebuilds I don't think we'll ever be

00:24:18,290 --> 00:24:21,620
there yet

00:24:19,160 --> 00:24:24,250
just because effecting change through

00:24:21,620 --> 00:24:26,480
this kind of makes me a little uneasy

00:24:24,250 --> 00:24:30,590
should there ever be a security problem

00:24:26,480 --> 00:24:32,470
so keeping it read-only makes it a makes

00:24:30,590 --> 00:24:36,559
me feel a little bit safer

00:24:32,470 --> 00:24:37,760
so we can go in say sup sup tells us

00:24:36,559 --> 00:24:40,160
everything that is currently an

00:24:37,760 --> 00:24:40,550
integrated state if it's not active it's

00:24:40,160 --> 00:24:43,490
there

00:24:40,550 --> 00:24:45,740
it also lets us say how is this thing

00:24:43,490 --> 00:24:47,210
and they'll tell us in this case there

00:24:45,740 --> 00:24:48,920
was a zookeeper instance that was down

00:24:47,210 --> 00:24:51,800
it was down because we had a hard drive

00:24:48,920 --> 00:24:54,410
failure and we're able to say yep this

00:24:51,800 --> 00:24:55,640
is good I'm going back to sleep and

00:24:54,410 --> 00:24:57,380
we've started adding more and more

00:24:55,640 --> 00:25:01,190
functionality to this over this over the

00:24:57,380 --> 00:25:03,140
time we also publicize every event that

00:25:01,190 --> 00:25:05,390
happens to a slack channel every time

00:25:03,140 --> 00:25:09,230
something goes up or down we monitor

00:25:05,390 --> 00:25:10,940
that and post it all along with the

00:25:09,230 --> 00:25:12,530
information about like hey did we create

00:25:10,940 --> 00:25:13,740
a ticket about this is things are things

00:25:12,530 --> 00:25:17,190
getting better are things get

00:25:13,740 --> 00:25:19,080
worse you know it's one thing to wake up

00:25:17,190 --> 00:25:21,120
in the morning and see five events in

00:25:19,080 --> 00:25:22,620
this slack Channel it's another thing to

00:25:21,120 --> 00:25:25,140
wake up in the middle in the morning and

00:25:22,620 --> 00:25:27,690
see 5,000 events and you can just

00:25:25,140 --> 00:25:30,240
quantify like hey there are a lot of

00:25:27,690 --> 00:25:31,440
changes going on I need to get involved

00:25:30,240 --> 00:25:38,520
I need to understand what's going what's

00:25:31,440 --> 00:25:40,590
happening here all right so let's walk

00:25:38,520 --> 00:25:42,720
through a couple different scenarios so

00:25:40,590 --> 00:25:44,910
assuming you have a solar cloud with one

00:25:42,720 --> 00:25:47,250
collection for simplicity this

00:25:44,910 --> 00:25:52,050
collection has two shards each card has

00:25:47,250 --> 00:25:56,520
six replicas well for us we kind of go

00:25:52,050 --> 00:25:59,010
with the whole cattle not pets theory so

00:25:56,520 --> 00:26:01,320
when we go and we lose one we don't

00:25:59,010 --> 00:26:02,840
really care this is just normal

00:26:01,320 --> 00:26:06,330
operations this happens all the time

00:26:02,840 --> 00:26:08,780
should we lose two well maybe there's

00:26:06,330 --> 00:26:11,850
something that we want to know about

00:26:08,780 --> 00:26:14,130
should we lose three still don't really

00:26:11,850 --> 00:26:16,140
care we've notified some of that there's

00:26:14,130 --> 00:26:19,050
a problem we've lost one in another

00:26:16,140 --> 00:26:23,250
shard that other shard is in it is in

00:26:19,050 --> 00:26:24,870
just as degraded state as shard one so

00:26:23,250 --> 00:26:27,059
this is where we come back to being

00:26:24,870 --> 00:26:29,010
cloud aware understanding that Solar is

00:26:27,059 --> 00:26:30,690
kind of a the status of solar can be

00:26:29,010 --> 00:26:32,429
represented by a tree

00:26:30,690 --> 00:26:35,100
and understand that there's no

00:26:32,429 --> 00:26:38,790
rotational risk at this point by having

00:26:35,100 --> 00:26:41,940
that other node be down losing another

00:26:38,790 --> 00:26:43,920
node still not a problem we've created a

00:26:41,940 --> 00:26:46,380
ticket someone will look at it someone

00:26:43,920 --> 00:26:48,179
will figure out what's going on but then

00:26:46,380 --> 00:26:50,630
we lose that third node and that sounds

00:26:48,179 --> 00:26:54,530
like well half of us charges down

00:26:50,630 --> 00:26:56,520
someone's got to wake up and fix this

00:26:54,530 --> 00:26:58,950
the thing that I want to point out

00:26:56,520 --> 00:27:02,220
that's really important here is that a

00:26:58,950 --> 00:27:05,250
lot of our monitoring is meant to get us

00:27:02,220 --> 00:27:06,870
up get us looking at problems way before

00:27:05,250 --> 00:27:09,660
our users even notice that an issue

00:27:06,870 --> 00:27:11,970
happened at this point even for shard

00:27:09,660 --> 00:27:14,910
one you're still indexing you're still

00:27:11,970 --> 00:27:17,850
querying you know you might have these

00:27:14,910 --> 00:27:20,460
six nodes spread out across multiple

00:27:17,850 --> 00:27:23,890
available available --'tis ohms at this

00:27:20,460 --> 00:27:27,370
point you might still be dr- one

00:27:23,890 --> 00:27:28,799
in our case this is a scenario where you

00:27:27,370 --> 00:27:31,809
know we might have lost a data center

00:27:28,799 --> 00:27:33,250
okay we're still serving traffic but we

00:27:31,809 --> 00:27:37,059
have to get in and get involved in and

00:27:33,250 --> 00:27:40,150
deal with it so let's take a look at a

00:27:37,059 --> 00:27:42,160
different scenario leadership I

00:27:40,150 --> 00:27:43,809
mentioned was very important right well

00:27:42,160 --> 00:27:45,730
what happens if you actually lose your

00:27:43,809 --> 00:27:47,620
leader there are many reasons that you

00:27:45,730 --> 00:27:49,240
might do this one of them is that

00:27:47,620 --> 00:27:51,520
machine goes down you have an out of

00:27:49,240 --> 00:27:53,220
memory issue network connectivity any

00:27:51,520 --> 00:27:55,480
kind of thing that could have happened

00:27:53,220 --> 00:27:58,780
so as soon as that happens we start a

00:27:55,480 --> 00:28:02,080
clock and we say well we're gonna give

00:27:58,780 --> 00:28:04,510
zookeeper and solar 10 seconds 15

00:28:02,080 --> 00:28:07,390
seconds whatever to go and find a new

00:28:04,510 --> 00:28:09,340
leader if that doesn't happen we're

00:28:07,390 --> 00:28:10,630
gonna create a ticket and then we're

00:28:09,340 --> 00:28:11,710
gonna start another clock and say well

00:28:10,630 --> 00:28:14,049
we're gonna give it a little bit longer

00:28:11,710 --> 00:28:16,570
and if it still doesn't happen we're

00:28:14,049 --> 00:28:18,190
gonna wake someone up this is kind of

00:28:16,570 --> 00:28:21,250
like having different degrees of

00:28:18,190 --> 00:28:24,850
problems and and using timers to kind of

00:28:21,250 --> 00:28:27,610
figure out like hey 5 10 seconds is fine

00:28:24,850 --> 00:28:31,960
30 seconds we might have a problem that

00:28:27,610 --> 00:28:32,980
we need to look at all right so what

00:28:31,960 --> 00:28:34,990
does this all look like from an

00:28:32,980 --> 00:28:36,549
infrastructure perspective well as I

00:28:34,990 --> 00:28:38,020
mentioned before we have a ton of solar

00:28:36,549 --> 00:28:40,750
and zookeeper instances that we're

00:28:38,020 --> 00:28:43,450
monitoring all that is talking to night

00:28:40,750 --> 00:28:45,850
owl and monitors night owl and night I

00:28:43,450 --> 00:28:47,530
was doing all the monitoring we write

00:28:45,850 --> 00:28:51,040
all this to a status event solar

00:28:47,530 --> 00:28:54,370
instance taking that away for a minute

00:28:51,040 --> 00:28:57,240
we then have all of our alarms and all

00:28:54,370 --> 00:28:59,980
of our rules built into this one service

00:28:57,240 --> 00:29:01,540
when something happens that service

00:28:59,980 --> 00:29:03,940
calls out to the Bloomberg's alarming

00:29:01,540 --> 00:29:06,669
framework a service that we don't know

00:29:03,940 --> 00:29:08,290
in our control that just says hey this

00:29:06,669 --> 00:29:10,809
is a ticket someone should look into

00:29:08,290 --> 00:29:13,360
this hey this is a ticket someone needs

00:29:10,809 --> 00:29:14,220
to be woken up and they take care of the

00:29:13,360 --> 00:29:16,660
rest

00:29:14,220 --> 00:29:18,280
it doesn't matter what framework this is

00:29:16,660 --> 00:29:23,919
we could be using page or duty for this

00:29:18,280 --> 00:29:25,660
we could be using anything so what

00:29:23,919 --> 00:29:28,360
happens when we start adding a whole lot

00:29:25,660 --> 00:29:31,450
more solar and zookeeper instances this

00:29:28,360 --> 00:29:34,270
doesn't exactly scale so we had to kind

00:29:31,450 --> 00:29:36,830
of address that and the first way is we

00:29:34,270 --> 00:29:39,770
took Kafka and we kind of broke up

00:29:36,830 --> 00:29:42,800
the monitoring part from the alarming

00:29:39,770 --> 00:29:45,230
part and we use Kafka as kind of the

00:29:42,800 --> 00:29:47,930
intermediary between the two so we've

00:29:45,230 --> 00:29:51,110
separated out our rules engine from the

00:29:47,930 --> 00:29:52,820
actual monitoring this allows us to make

00:29:51,110 --> 00:29:55,670
changes on the rule side a little bit

00:29:52,820 --> 00:29:57,440
easier making sure that our alarming

00:29:55,670 --> 00:30:02,810
configurations are can be a lot more

00:29:57,440 --> 00:30:05,840
flexible it also allows us to have other

00:30:02,810 --> 00:30:07,160
event publishers say hey this is a

00:30:05,840 --> 00:30:08,990
monitoring event that I want to care

00:30:07,160 --> 00:30:11,150
about in effect it might affect my soul

00:30:08,990 --> 00:30:12,680
or instance this could be anything from

00:30:11,150 --> 00:30:16,010
one of our tenants saying hey there's a

00:30:12,680 --> 00:30:18,350
problem or it could be hey there's this

00:30:16,010 --> 00:30:20,450
network information that we might care

00:30:18,350 --> 00:30:22,280
about and all that can go into our

00:30:20,450 --> 00:30:24,740
alarming into our rules engine to

00:30:22,280 --> 00:30:26,060
determine like should should we handle

00:30:24,740 --> 00:30:27,800
this is there something that we need to

00:30:26,060 --> 00:30:32,060
is there something that we need to act

00:30:27,800 --> 00:30:34,550
on the flip side of that is now our our

00:30:32,060 --> 00:30:38,720
tenants can have consumers of that Kafka

00:30:34,550 --> 00:30:40,400
queue and they can say hey you guys have

00:30:38,720 --> 00:30:42,550
your own alarms that's great I want to

00:30:40,400 --> 00:30:46,040
integrate your alarms and your on your

00:30:42,550 --> 00:30:48,140
event into my own alarming system so I

00:30:46,040 --> 00:30:49,910
can say I can track this myself I can

00:30:48,140 --> 00:30:53,210
alarm on it from my own team if I care

00:30:49,910 --> 00:30:56,750
if I care to and bring it back that

00:30:53,210 --> 00:30:58,670
status events that all that stuff

00:30:56,750 --> 00:31:01,070
including the event publishers gets

00:30:58,670 --> 00:31:06,170
written to that same status status

00:31:01,070 --> 00:31:09,010
events service but you still notice we

00:31:06,170 --> 00:31:11,750
still have that one big monitoring block

00:31:09,010 --> 00:31:14,750
that's not a scalable solution that's

00:31:11,750 --> 00:31:15,950
not going to work forever so we've

00:31:14,750 --> 00:31:19,360
broken that up and now we're running

00:31:15,950 --> 00:31:22,130
that bitting in kubernetes so different

00:31:19,360 --> 00:31:24,490
each one of those is a single monitoring

00:31:22,130 --> 00:31:27,260
service similar to the way it was before

00:31:24,490 --> 00:31:29,570
except now it said okay when you start

00:31:27,260 --> 00:31:31,940
up you're told what your support what

00:31:29,570 --> 00:31:34,100
segment you're supposed to monitor maybe

00:31:31,940 --> 00:31:36,110
it's a specific tenancy maybe it's a

00:31:34,100 --> 00:31:38,960
group of in divide individual services

00:31:36,110 --> 00:31:40,720
whatever adds startup you're told

00:31:38,960 --> 00:31:43,280
exactly who you're supposed to monitor

00:31:40,720 --> 00:31:46,910
should you die someone will replace you

00:31:43,280 --> 00:31:48,230
and do exactly the same thing since

00:31:46,910 --> 00:31:50,150
doing this we've also been able to do

00:31:48,230 --> 00:31:52,550
things like use that same frame

00:31:50,150 --> 00:31:56,000
to monitor our admin services and our

00:31:52,550 --> 00:31:59,150
coop services so this has proved we can

00:31:56,000 --> 00:32:01,700
apply the same exact algorithms and

00:31:59,150 --> 00:32:03,800
design for monitoring everything that we

00:32:01,700 --> 00:32:08,240
all the pieces of infrastructure that we

00:32:03,800 --> 00:32:11,420
have so what's next so we had this great

00:32:08,240 --> 00:32:15,290
goal of creating a really dynamic rule

00:32:11,420 --> 00:32:16,310
configuration and unit front 2019 we

00:32:15,290 --> 00:32:17,750
were hoping to leverage some of the

00:32:16,310 --> 00:32:21,170
Kafka stream stuff that we learned about

00:32:17,750 --> 00:32:23,470
last year maybe even flink to figure out

00:32:21,170 --> 00:32:26,660
how to make that a really powerful

00:32:23,470 --> 00:32:28,790
alarming engine that work is ongoing but

00:32:26,660 --> 00:32:33,470
it's still not there yet

00:32:28,790 --> 00:32:35,360
I don't know once we got into the weeds

00:32:33,470 --> 00:32:36,860
of this we realized that it's actually a

00:32:35,360 --> 00:32:39,560
very complicated problem that were that

00:32:36,860 --> 00:32:42,710
we're trying to solve so hopefully we'll

00:32:39,560 --> 00:32:44,870
have news on that we want to also open

00:32:42,710 --> 00:32:47,780
sourced night-owl now that we've kind of

00:32:44,870 --> 00:32:50,600
broken out all the alarming from it and

00:32:47,780 --> 00:32:52,490
it's just the monitoring piece it's a

00:32:50,600 --> 00:32:54,650
lot easier to then go and say ok we're

00:32:52,490 --> 00:32:58,160
going to open-source this this actually

00:32:54,650 --> 00:32:59,840
adds value to the community but what we

00:32:58,160 --> 00:33:01,010
realized is that it's still tightly

00:32:59,840 --> 00:33:03,830
integrated into the Bloomberg

00:33:01,010 --> 00:33:05,600
architecture so hopefully for next year

00:33:03,830 --> 00:33:07,820
we'll be able to detangle some of that

00:33:05,600 --> 00:33:12,200
and then release that and make that

00:33:07,820 --> 00:33:12,560
usable for other people thank you

00:33:12,200 --> 00:33:15,970
everybody

00:33:12,560 --> 00:33:15,970
any questions

00:33:16,210 --> 00:33:23,909
[Applause]

00:33:25,770 --> 00:33:30,010
yeah

00:33:27,420 --> 00:33:33,730
I'm just wondering what your take is on

00:33:30,010 --> 00:33:35,920
Prometheus and outside of things I think

00:33:33,730 --> 00:33:38,620
Prometheus is great especially with some

00:33:35,920 --> 00:33:41,130
of the work that was done in solar 7 to

00:33:38,620 --> 00:33:44,980
make you know from easiest exporters and

00:33:41,130 --> 00:33:48,580
metrics greatly improved over solar 4 5

00:33:44,980 --> 00:33:50,140
& 6 we love what we've done we love

00:33:48,580 --> 00:33:51,700
what's been done and one of the

00:33:50,140 --> 00:33:54,250
contributions that we made just a few

00:33:51,700 --> 00:33:57,040
months ago is we rebuilt the solar

00:33:54,250 --> 00:33:58,900
metrics collector because it just wasn't

00:33:57,040 --> 00:34:01,060
doing it there were a lot of bugs in it

00:33:58,900 --> 00:34:03,490
so we kind of built that from rebuilt

00:34:01,060 --> 00:34:05,590
that from scratch and published it and

00:34:03,490 --> 00:34:07,120
that I'm sure is available now and we're

00:34:05,590 --> 00:34:09,159
running that for a number of our solar 7

00:34:07,120 --> 00:34:12,419
collections we're just starting to

00:34:09,159 --> 00:34:14,050
experiment so low 8 so hopefully that

00:34:12,419 --> 00:34:16,899
prometheus will help with our

00:34:14,050 --> 00:34:19,379
experimentation of that but yeah I love

00:34:16,899 --> 00:34:19,379
for me Thea's

00:34:26,970 --> 00:34:36,329
oh great talk thank you

00:34:32,839 --> 00:34:39,179
you mentioned that like when servers

00:34:36,329 --> 00:34:42,750
start going down and you you have slack

00:34:39,179 --> 00:34:45,000
alerts for every single state change at

00:34:42,750 --> 00:34:48,050
the scale that you're operating at if

00:34:45,000 --> 00:34:50,069
you're dealing with individual servers

00:34:48,050 --> 00:34:52,980
somebody once told me like you've

00:34:50,069 --> 00:34:55,409
already lost so what's what's your take

00:34:52,980 --> 00:34:57,750
on that like why is that valuable I

00:34:55,409 --> 00:35:02,579
don't understand that that's that's a

00:34:57,750 --> 00:35:04,020
great question so there there's a few

00:35:02,579 --> 00:35:05,970
parts so let me try and address it and

00:35:04,020 --> 00:35:09,240
then you tell me if I if I answered your

00:35:05,970 --> 00:35:10,650
question so the first part is if you're

00:35:09,240 --> 00:35:12,270
dealing with a single server

00:35:10,650 --> 00:35:15,030
you've already you've just missed the

00:35:12,270 --> 00:35:16,530
boat and I couldn't agree more

00:35:15,030 --> 00:35:19,260
one of the things that we over retune

00:35:16,530 --> 00:35:21,270
routinely do is say we're going to

00:35:19,260 --> 00:35:23,609
upgrade the firmware on the router on a

00:35:21,270 --> 00:35:26,190
rack and we'll just set the whole rack

00:35:23,609 --> 00:35:28,319
down and at this point we don't care one

00:35:26,190 --> 00:35:30,260
bit because when we when we place our

00:35:28,319 --> 00:35:33,780
services throughout our infrastructure

00:35:30,260 --> 00:35:35,400
we have some idea of like hey let's not

00:35:33,780 --> 00:35:39,000
put two of the same services in the same

00:35:35,400 --> 00:35:41,220
rack let's make sure that you know let

00:35:39,000 --> 00:35:42,950
me rephrase let's not put two replicas

00:35:41,220 --> 00:35:46,589
in the same rack

00:35:42,950 --> 00:35:48,260
so as we're running these things you

00:35:46,589 --> 00:35:51,480
know you lose a server you loser rack

00:35:48,260 --> 00:35:53,130
you don't really care in fact most of

00:35:51,480 --> 00:35:54,660
our infrastructure is built so that we

00:35:53,130 --> 00:35:58,200
could lose an entire data center and

00:35:54,660 --> 00:36:00,359
still not really care that's not to say

00:35:58,200 --> 00:36:04,980
that I don't want an alert saying that

00:36:00,359 --> 00:36:07,589
this happened you know hey we've seen

00:36:04,980 --> 00:36:08,180
issues where a server is just behaving

00:36:07,589 --> 00:36:12,510
badly

00:36:08,180 --> 00:36:16,140
we had a hard drive go bad about six

00:36:12,510 --> 00:36:18,420
months ago no alarms our s eries didn't

00:36:16,140 --> 00:36:19,710
notice anything there were no individual

00:36:18,420 --> 00:36:21,960
metrics that would have indicated that

00:36:19,710 --> 00:36:24,270
there was a problem but when you look at

00:36:21,960 --> 00:36:27,000
the queue times for that one instance on

00:36:24,270 --> 00:36:28,740
solar those queue times were exploding

00:36:27,000 --> 00:36:32,880
compared to the same query on other

00:36:28,740 --> 00:36:34,260
instances so we also found out that it

00:36:32,880 --> 00:36:37,200
was also going into recovery now and

00:36:34,260 --> 00:36:38,550
again and it was just acting flaky so

00:36:37,200 --> 00:36:39,720
while we don't really care about one

00:36:38,550 --> 00:36:40,490
server we still want to be able to

00:36:39,720 --> 00:36:44,110
detect that there

00:36:40,490 --> 00:36:46,400
a problem and kind of deal with that and

00:36:44,110 --> 00:36:49,190
as far as how the how that all

00:36:46,400 --> 00:36:52,730
integrates in slack sometimes it's hate

00:36:49,190 --> 00:36:55,850
there was a problem with the server you

00:36:52,730 --> 00:36:58,580
know I want to be able to issue the how

00:36:55,850 --> 00:37:00,710
is foo command and then just be able to

00:36:58,580 --> 00:37:02,330
monitor that slack channel and say yep

00:37:00,710 --> 00:37:04,880
nodes are coming back up

00:37:02,330 --> 00:37:07,970
how is yep everything's 100% active I'm

00:37:04,880 --> 00:37:11,660
good I can go back to sleep so it's more

00:37:07,970 --> 00:37:14,540
of a way to get that that feedback of

00:37:11,660 --> 00:37:16,100
what's going on from a remote remote

00:37:14,540 --> 00:37:20,090
location instead of having to open up

00:37:16,100 --> 00:37:22,310
your your laptop login I feel like

00:37:20,090 --> 00:37:25,460
that's really the difference between can

00:37:22,310 --> 00:37:27,650
I go back to sleep or am I then gonna be

00:37:25,460 --> 00:37:30,050
up for the next two hours once I decide

00:37:27,650 --> 00:37:33,890
I have to get up wake my son wake my

00:37:30,050 --> 00:37:35,720
wife up in the process you know it's

00:37:33,890 --> 00:37:39,320
just a bad sign if I can look in things

00:37:35,720 --> 00:37:41,900
and say yep we lost a leader it came

00:37:39,320 --> 00:37:43,940
back a minute and a half later there's

00:37:41,900 --> 00:37:45,590
another replica that's down but overall

00:37:43,940 --> 00:37:48,580
my system is in a good state and it's

00:37:45,590 --> 00:37:50,930
getting better I'm happy

00:37:48,580 --> 00:37:55,900
did that answer your question two thumbs

00:37:50,930 --> 00:37:55,900
up cool any other questions

00:37:58,150 --> 00:38:02,740
more light question do you know how s

00:38:00,609 --> 00:38:05,319
saris are happy now like to kind of

00:38:02,740 --> 00:38:07,420
follow up how this thing like evolved

00:38:05,319 --> 00:38:11,799
what what time did you say water is like

00:38:07,420 --> 00:38:14,140
you follow up s re fatigue like yep so

00:38:11,799 --> 00:38:16,809
it's it's interesting we have an SRE

00:38:14,140 --> 00:38:18,999
team that was that grew out of the

00:38:16,809 --> 00:38:21,130
search infrastructure team were

00:38:18,999 --> 00:38:25,299
originally one team and then they they

00:38:21,130 --> 00:38:27,930
separated off and now do s re providing

00:38:25,299 --> 00:38:30,460
they provide s re services for

00:38:27,930 --> 00:38:33,430
relational databases Cassandra or data

00:38:30,460 --> 00:38:35,980
science platform Hadoop HBase a bunch of

00:38:33,430 --> 00:38:37,660
different technologies but they also

00:38:35,980 --> 00:38:39,190
remember where they came from so we have

00:38:37,660 --> 00:38:43,599
a really close working relationship with

00:38:39,190 --> 00:38:44,920
them with that it's like hey can you

00:38:43,599 --> 00:38:47,619
guys look into this there's something

00:38:44,920 --> 00:38:49,589
wrong or they'll mention something that

00:38:47,619 --> 00:38:52,599
I'll say we've gotten this alert

00:38:49,589 --> 00:38:55,720
anything going on there as familiar with

00:38:52,599 --> 00:38:57,400
our tools as we are they've also put had

00:38:55,720 --> 00:39:00,279
a lot of input into how they're created

00:38:57,400 --> 00:39:03,009
so that as we're releasing new features

00:39:00,279 --> 00:39:05,559
they have input into what those features

00:39:03,009 --> 00:39:08,049
look like and it matches up with their

00:39:05,559 --> 00:39:09,670
workflows in fact a lot of what we're

00:39:08,049 --> 00:39:12,549
doing it on in our next version of the

00:39:09,670 --> 00:39:14,799
UI is building in tools so they can say

00:39:12,549 --> 00:39:17,400
take this host and put in maintenance

00:39:14,799 --> 00:39:19,989
mode and that integrates with their own

00:39:17,400 --> 00:39:22,089
services that say yep this house is

00:39:19,989 --> 00:39:23,650
going to be in maintenance mode I'm

00:39:22,089 --> 00:39:25,239
gonna stop all the services on it I'm

00:39:23,650 --> 00:39:27,339
gonna log about its maintenance I'm

00:39:25,239 --> 00:39:29,140
gonna let our users know that this host

00:39:27,339 --> 00:39:30,700
is and maintenance our monitoring is

00:39:29,140 --> 00:39:34,660
going to be aware of that so it's gonna

00:39:30,700 --> 00:39:39,970
it's going to be it's going to be more

00:39:34,660 --> 00:39:42,279
tolerant of that node being down so how

00:39:39,970 --> 00:39:45,339
that plays out is so for example we have

00:39:42,279 --> 00:39:47,140
a generic query UI and one of the things

00:39:45,339 --> 00:39:49,869
that we found is by building that

00:39:47,140 --> 00:39:51,579
generic query you our people were able

00:39:49,869 --> 00:39:53,769
to execute production queries that were

00:39:51,579 --> 00:39:57,609
that were never tested and could do

00:39:53,769 --> 00:40:00,519
really really bad things well so we said

00:39:57,609 --> 00:40:04,059
okay you can run that query as long as

00:40:00,519 --> 00:40:06,249
your entire cluster is active you bring

00:40:04,059 --> 00:40:08,680
down a node no big deal it'll come back

00:40:06,249 --> 00:40:09,910
up we'll deal with it then nobody's

00:40:08,680 --> 00:40:11,140
going to get woken up in the middle of

00:40:09,910 --> 00:40:13,220
the night cool

00:40:11,140 --> 00:40:17,089
well what happens when that note is down

00:40:13,220 --> 00:40:20,089
for maintenance sorry the one thing I

00:40:17,089 --> 00:40:22,460
forgot to say is once that note is down

00:40:20,089 --> 00:40:24,440
we block the user from doing any further

00:40:22,460 --> 00:40:26,440
queries because it's like hey what you

00:40:24,440 --> 00:40:29,299
did might have caused a node to be down

00:40:26,440 --> 00:40:30,650
so when a node comes out for maintenance

00:40:29,299 --> 00:40:32,960
we have to be able to detect and

00:40:30,650 --> 00:40:35,089
distinguish between those two things so

00:40:32,960 --> 00:40:36,920
that a user can still query even though

00:40:35,089 --> 00:40:39,700
that that host is down for a completely

00:40:36,920 --> 00:40:39,700
unrelated issue

00:40:42,760 --> 00:40:49,150

YouTube URL: https://www.youtube.com/watch?v=RdQ0yUOLDW4


