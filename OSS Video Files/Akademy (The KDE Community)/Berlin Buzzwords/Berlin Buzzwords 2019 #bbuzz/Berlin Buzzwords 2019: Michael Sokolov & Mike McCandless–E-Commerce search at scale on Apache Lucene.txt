Title: Berlin Buzzwords 2019: Michael Sokolov & Mike McCandlessâ€“E-Commerce search at scale on Apache Lucene
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	After many years running its own in-house C++ search engine, Amazon is exploring moving its customer facing e-commerce product search to Apache Lucene, serving millions of customers each day worldwide. Apache Solr, Elasticsearch and other Lucene derivatives have been used widely for many years at Amazon, but until now the .com product search has been powered by a proprietary in-house engine. 

We'll discuss why we decided to adopt open source for this vital technology and dive deep into the technical challenges we faced in replicating our legacy engine's behavior, pointing out novel uses of Lucene along the way. 

Read more:
https://2019.berlinbuzzwords.de/19/session/e-commerce-search-scale-apache-lucene-tm

About Michael Sokolov:
https://2019.berlinbuzzwords.de/users/michael-sokolov

About Mike McCandless:
https://2019.berlinbuzzwords.de/users/mike-mccandless

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,570 --> 00:00:10,410
so

00:00:08,069 --> 00:00:12,060
I am Mike McCandless I'm a longtime

00:00:10,410 --> 00:00:15,260
committer on the attack

00:00:12,060 --> 00:00:15,260
open-source project I

00:00:15,290 --> 00:00:21,860
should've seen in action Mike Sokolov

00:00:19,460 --> 00:00:24,560
search veteran worked many years with

00:00:21,860 --> 00:00:29,060
Lucene and just recently became a

00:00:24,560 --> 00:00:31,189
committer and we are the two lucky souls

00:00:29,060 --> 00:00:32,899
up here today but there is a large team

00:00:31,189 --> 00:00:34,430
at Amazon a large distributed team

00:00:32,899 --> 00:00:37,550
including people in Seattle

00:00:34,430 --> 00:00:39,500
Tokyo Dublin San Francisco Palo Alto who

00:00:37,550 --> 00:00:44,149
have contributed to this effort some of

00:00:39,500 --> 00:00:45,770
them are in the audience here so here's

00:00:44,149 --> 00:00:47,480
a quick outline of our talk first I'll

00:00:45,770 --> 00:00:49,100
give an overview of why we chose we've

00:00:47,480 --> 00:00:50,990
seen and what we're trying to solve at

00:00:49,100 --> 00:00:52,940
Amazon with Lucene then I'll describe

00:00:50,990 --> 00:00:55,490
our production architecture I'll

00:00:52,940 --> 00:00:57,350
describe how we measure performance the

00:00:55,490 --> 00:00:59,270
other mic will come up and describe some

00:00:57,350 --> 00:01:01,670
text analysis challenges we have

00:00:59,270 --> 00:01:03,740
indexing our catalogue he'll describe

00:01:01,670 --> 00:01:05,119
how we optimize for query latency

00:01:03,740 --> 00:01:08,299
because we're very concerned with the

00:01:05,119 --> 00:01:11,869
long pole query latencies p99 plus how

00:01:08,299 --> 00:01:13,970
we do ranking and then I will wrap up so

00:01:11,869 --> 00:01:15,649
I think most people are probably

00:01:13,970 --> 00:01:17,689
familiar with Amazon you're probably

00:01:15,649 --> 00:01:19,430
customers of Amazon you probably

00:01:17,689 --> 00:01:21,470
appreciate how difficult a search

00:01:19,430 --> 00:01:23,600
application this particular search

00:01:21,470 --> 00:01:26,450
application is Amazon has a very large

00:01:23,600 --> 00:01:28,130
catalogue it is a high velocity catalog

00:01:26,450 --> 00:01:30,520
it's constantly products are being added

00:01:28,130 --> 00:01:32,870
prices are changing with high frequency

00:01:30,520 --> 00:01:34,850
we have to keep up with that indexing

00:01:32,870 --> 00:01:36,770
rate adding new documents and deleting

00:01:34,850 --> 00:01:38,570
documents or replacing them but at the

00:01:36,770 --> 00:01:41,090
same time we have to handle a very high

00:01:38,570 --> 00:01:42,799
query rate all of the amazon shoppers

00:01:41,090 --> 00:01:44,510
and this is the search box you get at

00:01:42,799 --> 00:01:46,760
amazon.com when you go to the top and

00:01:44,510 --> 00:01:48,799
and type in a few search terms that's

00:01:46,760 --> 00:01:50,240
hitting our service when you search on

00:01:48,799 --> 00:01:52,970
the Amazon mobile app that's hitting our

00:01:50,240 --> 00:01:55,580
service so that query rate is very high

00:01:52,970 --> 00:01:57,229
we have very strong latency requirements

00:01:55,580 --> 00:01:58,909
we don't want customers to wait even at

00:01:57,229 --> 00:02:01,580
the the long pole query Layton sees we

00:01:58,909 --> 00:02:03,260
want to keep those very low and then to

00:02:01,580 --> 00:02:05,960
make matters even more challenging the

00:02:03,260 --> 00:02:07,820
query rate is extremely peaky there's

00:02:05,960 --> 00:02:09,530
there's a daily pattern there's a weekly

00:02:07,820 --> 00:02:11,510
pattern a monthly pattern and then

00:02:09,530 --> 00:02:13,250
there's prime day which is coming up

00:02:11,510 --> 00:02:14,689
very soon and that's a completely

00:02:13,250 --> 00:02:19,730
different thing that's very challenging

00:02:14,689 --> 00:02:21,170
to handle so and people were unsure when

00:02:19,730 --> 00:02:24,260
we started this project whether Lucene

00:02:21,170 --> 00:02:26,090
is up to the task so why did we pick

00:02:24,260 --> 00:02:29,510
we've seen well the scene is an amazing

00:02:26,090 --> 00:02:32,510
search engine it is 20 years old this

00:02:29,510 --> 00:02:34,580
in 1999 doug cutting donated his his

00:02:32,510 --> 00:02:36,380
visionary source code for this new

00:02:34,580 --> 00:02:39,230
search engine to source forage at the

00:02:36,380 --> 00:02:41,030
time and it just developed a massive

00:02:39,230 --> 00:02:43,760
community very passionate people

00:02:41,030 --> 00:02:44,990
constantly iterating it's hard to find a

00:02:43,760 --> 00:02:47,480
software project that is still

00:02:44,990 --> 00:02:49,430
successful after 20 years that has the

00:02:47,480 --> 00:02:51,290
innovation that Lucene has every major

00:02:49,430 --> 00:02:54,290
releases adding incredible new features

00:02:51,290 --> 00:02:57,110
block max wand of codec impacts showed

00:02:54,290 --> 00:02:58,580
up in the latest major release and it's

00:02:57,110 --> 00:03:00,290
a high performance search engine this

00:02:58,580 --> 00:03:01,790
isn't just a toy research engine that

00:03:00,290 --> 00:03:03,290
people like to just play with but never

00:03:01,790 --> 00:03:05,600
really use in practice it's used in

00:03:03,290 --> 00:03:08,300
practice all over the place if you look

00:03:05,600 --> 00:03:11,030
for a ride on uber if you do a search on

00:03:08,300 --> 00:03:13,040
Twitter or LinkedIn if you are searching

00:03:11,030 --> 00:03:14,720
for a date on tinder you're using

00:03:13,040 --> 00:03:16,880
leucine leucine is everywhere and now

00:03:14,720 --> 00:03:19,370
some searches on amazon.com are using

00:03:16,880 --> 00:03:21,110
Bluestein and it's because of this

00:03:19,370 --> 00:03:23,450
incredibly passionate community and the

00:03:21,110 --> 00:03:24,800
20 years of compounded growth that

00:03:23,450 --> 00:03:29,650
leucine is such an incredible search

00:03:24,800 --> 00:03:31,670
engine leucine is a pure Java

00:03:29,650 --> 00:03:34,190
architecture and at the time that was

00:03:31,670 --> 00:03:36,470
really shocking in 1999 Java was this

00:03:34,190 --> 00:03:37,850
newfangled language but today that's a

00:03:36,470 --> 00:03:39,650
really great property you don't have

00:03:37,850 --> 00:03:41,900
memory leaks in the traditional sense of

00:03:39,650 --> 00:03:44,240
memory leaks anymore leucine is an on

00:03:41,900 --> 00:03:45,830
disk index so it can scale to indices

00:03:44,240 --> 00:03:49,280
much larger than the free RAM on a

00:03:45,830 --> 00:03:52,070
computer and still have good latency it

00:03:49,280 --> 00:03:54,410
has amazing concurrency so as computers

00:03:52,070 --> 00:03:56,360
in 1999 computers didn't have that much

00:03:54,410 --> 00:03:58,640
concurrency these days they have amazing

00:03:56,360 --> 00:04:00,800
concurrency you get 72 cores on a single

00:03:58,640 --> 00:04:02,540
box the solid state disks that are in

00:04:00,800 --> 00:04:04,160
machines they are really fast they are

00:04:02,540 --> 00:04:05,840
also incredibly concurrent under the

00:04:04,160 --> 00:04:08,870
hood they have many different channels

00:04:05,840 --> 00:04:11,060
that they can handle saturated i/o so

00:04:08,870 --> 00:04:12,860
leucine taps into all of that of that

00:04:11,060 --> 00:04:15,020
capability it handles that concurrency

00:04:12,860 --> 00:04:16,549
very well and it has a segmented

00:04:15,020 --> 00:04:18,620
architecture and if you're curious how

00:04:16,549 --> 00:04:19,940
leucine deals with merging segments

00:04:18,620 --> 00:04:21,739
because new segments are written and

00:04:19,940 --> 00:04:23,060
they're small and over time you get too

00:04:21,739 --> 00:04:24,680
many little segments it merges them

00:04:23,060 --> 00:04:26,720
together in the background into bigger

00:04:24,680 --> 00:04:28,400
segments there's a fun YouTube video

00:04:26,720 --> 00:04:31,310
that shows you how that process works

00:04:28,400 --> 00:04:33,290
and it has a very nice near real-time

00:04:31,310 --> 00:04:34,760
architecture used so you can search all

00:04:33,290 --> 00:04:36,800
the documents you indexed up until a

00:04:34,760 --> 00:04:39,110
point and you can continue indexing

00:04:36,800 --> 00:04:40,700
while you're searching and then every

00:04:39,110 --> 00:04:42,650
few seconds you can just open a new

00:04:40,700 --> 00:04:45,139
reader and it will search the point in

00:04:42,650 --> 00:04:46,850
time view of that index as of when you

00:04:45,139 --> 00:04:48,290
refreshed it so that and that that

00:04:46,850 --> 00:04:50,000
refresh operation is not that costly

00:04:48,290 --> 00:04:51,889
it's in proportion to how many documents

00:04:50,000 --> 00:04:54,440
were indexed since you last refreshed

00:04:51,889 --> 00:04:56,240
and that right once designed those

00:04:54,440 --> 00:04:58,970
segments are written once and they never

00:04:56,240 --> 00:05:01,070
change and that gives very good index

00:04:58,970 --> 00:05:02,690
compression leucine is able to very

00:05:01,070 --> 00:05:04,699
compactly write the values because it

00:05:02,690 --> 00:05:07,070
knows the values that just wrote won't

00:05:04,699 --> 00:05:11,900
be updated instead another segment will

00:05:07,070 --> 00:05:13,820
be written in the future so we in

00:05:11,900 --> 00:05:15,500
building this this search application on

00:05:13,820 --> 00:05:17,570
top of leucine we are using a huge

00:05:15,500 --> 00:05:19,190
number of leucine features and so this

00:05:17,570 --> 00:05:21,169
is one of the important characteristics

00:05:19,190 --> 00:05:23,060
that leucine had for our search problem

00:05:21,169 --> 00:05:25,160
I mentioned the near-real-time

00:05:23,060 --> 00:05:26,990
capability we're actually using a near

00:05:25,160 --> 00:05:28,760
real-time segment replication to

00:05:26,990 --> 00:05:30,770
distribute the index across many

00:05:28,760 --> 00:05:33,590
replicas and this is different from how

00:05:30,770 --> 00:05:35,270
elastic search and solar work today we

00:05:33,590 --> 00:05:38,360
are using a feature called concurrent

00:05:35,270 --> 00:05:41,300
searching so modern computers that have

00:05:38,360 --> 00:05:42,620
massive concurrency we have hard queries

00:05:41,300 --> 00:05:45,590
to answer they take a lot of computation

00:05:42,620 --> 00:05:47,539
so we now use multiple threads to answer

00:05:45,590 --> 00:05:49,130
that that query and that's a feature

00:05:47,539 --> 00:05:51,020
that's available in busine but it hasn't

00:05:49,130 --> 00:05:51,550
been exposed in elasticsearch or solar

00:05:51,020 --> 00:05:55,400
yet

00:05:51,550 --> 00:05:57,620
we use index sorting so we have this

00:05:55,400 --> 00:05:59,419
notion of whether a product is a good

00:05:57,620 --> 00:06:01,550
product lots of people are looking at it

00:05:59,419 --> 00:06:02,780
and purchasing it or a not good product

00:06:01,550 --> 00:06:05,300
it's not really getting much engagement

00:06:02,780 --> 00:06:07,490
we sort the index by that criteria and

00:06:05,300 --> 00:06:10,669
it allows us to do early termination on

00:06:07,490 --> 00:06:12,380
difficult queries we're using index time

00:06:10,669 --> 00:06:14,300
joins to handle offer informations

00:06:12,380 --> 00:06:16,220
because every every product has multiple

00:06:14,300 --> 00:06:18,289
offers and that's an index time join for

00:06:16,220 --> 00:06:20,120
us it's very efficient dimensional

00:06:18,289 --> 00:06:21,380
points was added in leucine 6 people

00:06:20,120 --> 00:06:23,510
think dimensional points is the

00:06:21,380 --> 00:06:25,669
geospatial filter and it does a very

00:06:23,510 --> 00:06:27,710
good job at that it's also a very

00:06:25,669 --> 00:06:29,900
generic multi-dimensional index and

00:06:27,710 --> 00:06:33,010
we're using that to handle the kinds of

00:06:29,900 --> 00:06:35,630
specials that we get on prime day

00:06:33,010 --> 00:06:37,580
Lucien's extensibility API is a

00:06:35,630 --> 00:06:39,199
collectors doc value source queries

00:06:37,580 --> 00:06:41,360
leucine has a it's a very modular

00:06:39,199 --> 00:06:43,190
architecture and it's very easy to build

00:06:41,360 --> 00:06:45,380
your own custom implementations of these

00:06:43,190 --> 00:06:47,720
classes those are great touch points to

00:06:45,380 --> 00:06:51,050
customize the behavior custom term

00:06:47,720 --> 00:06:53,210
frequencies for representing how how

00:06:51,050 --> 00:06:54,620
custom how engaged in Aysen is so that

00:06:53,210 --> 00:06:56,180
was a feature that was not available in

00:06:54,620 --> 00:06:57,620
leucine in the beginning and we

00:06:56,180 --> 00:06:59,330
that we contributed that feature back

00:06:57,620 --> 00:07:00,380
upstream and people have since built on

00:06:59,330 --> 00:07:01,479
top of it and done all sorts of

00:07:00,380 --> 00:07:03,740
interesting things

00:07:01,479 --> 00:07:05,440
faceting multi-phase ranking expression

00:07:03,740 --> 00:07:07,910
so the list goes on and on and

00:07:05,440 --> 00:07:09,020
especially because those two top two

00:07:07,910 --> 00:07:11,090
features were not available in

00:07:09,020 --> 00:07:13,039
elasticsearch and solar but also because

00:07:11,090 --> 00:07:15,320
we're heavily customizing many different

00:07:13,039 --> 00:07:16,900
parts of Lucene we are building directly

00:07:15,320 --> 00:07:21,759
on top of leucine we're not using

00:07:16,900 --> 00:07:23,930
elasticsearch or solar today and

00:07:21,759 --> 00:07:25,729
thinking back to Isabelle's keynote this

00:07:23,930 --> 00:07:27,770
morning she mentioned she had a point in

00:07:25,729 --> 00:07:30,020
her talk where she said contributions

00:07:27,770 --> 00:07:31,759
back upstream are dependent on your

00:07:30,020 --> 00:07:32,750
situation and where you are at your

00:07:31,759 --> 00:07:35,120
company and that's what happened at

00:07:32,750 --> 00:07:36,860
Amazon too early on as we were building

00:07:35,120 --> 00:07:39,259
this search engine on top of leucine we

00:07:36,860 --> 00:07:40,789
hit a bunch of rush corners a few small

00:07:39,259 --> 00:07:43,250
bugs you know some performance issues

00:07:40,789 --> 00:07:45,500
and early on we pushed a lot of good

00:07:43,250 --> 00:07:47,060
stuff back upstream and that benefits

00:07:45,500 --> 00:07:48,650
the whole community benefits everyone is

00:07:47,060 --> 00:07:50,570
using the scene elasticsearch and solar

00:07:48,650 --> 00:07:53,750
so that's a healthy process that works

00:07:50,570 --> 00:07:55,250
well inside our team and less so lately

00:07:53,750 --> 00:07:59,690
because leucine is working really well

00:07:55,250 --> 00:08:00,919
for us right now okay I'm going to

00:07:59,690 --> 00:08:02,539
switch gears and talk about our

00:08:00,919 --> 00:08:04,789
production architectures sort of at a

00:08:02,539 --> 00:08:06,320
high level first I wanted to dive into

00:08:04,789 --> 00:08:09,320
that first feature that I listed near

00:08:06,320 --> 00:08:10,970
real-time segment replication so when

00:08:09,320 --> 00:08:12,590
you have a search index that's too big

00:08:10,970 --> 00:08:14,150
for one computer to handle even with all

00:08:12,590 --> 00:08:16,490
of its concurrency and solid state disks

00:08:14,150 --> 00:08:18,340
you shard it you just take the index and

00:08:16,490 --> 00:08:21,320
you break it up evenly into n pieces

00:08:18,340 --> 00:08:24,289
then if each if one of those n pieces is

00:08:21,320 --> 00:08:25,669
still too hard to handle the query load

00:08:24,289 --> 00:08:27,830
how many queries per seconds you have to

00:08:25,669 --> 00:08:29,509
handle then you replicate that index so

00:08:27,830 --> 00:08:31,669
you have a matrix structure for search

00:08:29,509 --> 00:08:33,020
clusters and that's how elastic search

00:08:31,669 --> 00:08:36,650
and solar and our production

00:08:33,020 --> 00:08:38,900
architecture work as well however when

00:08:36,650 --> 00:08:40,459
you add documents to that index you have

00:08:38,900 --> 00:08:43,370
to have a way to get the new documents

00:08:40,459 --> 00:08:44,480
to all of the replicas so you had when

00:08:43,370 --> 00:08:46,520
the document comes in you have to figure

00:08:44,480 --> 00:08:47,900
out what shard it goes to and then in

00:08:46,520 --> 00:08:50,870
the case of elastic search and solar

00:08:47,900 --> 00:08:53,360
cloud the document is reindex across all

00:08:50,870 --> 00:08:55,040
the replicas that it wouldn't really

00:08:53,360 --> 00:08:56,870
scale very well for us because we have

00:08:55,040 --> 00:08:58,610
very deep replicas the search traffic is

00:08:56,870 --> 00:09:01,130
really peaky especially on prime days

00:08:58,610 --> 00:09:02,510
and Black Friday's so we use a feature

00:09:01,130 --> 00:09:04,430
called near real-time segment

00:09:02,510 --> 00:09:06,740
replication which takes advantage of

00:09:04,430 --> 00:09:08,660
leucine write-once architecture so loose

00:09:06,740 --> 00:09:09,769
one indexer node will write these new

00:09:08,660 --> 00:09:11,300
segments and

00:09:09,769 --> 00:09:13,399
then we copy those segments out to all

00:09:11,300 --> 00:09:17,269
of the replicas and we take advantage of

00:09:13,399 --> 00:09:18,860
AWS is as three it's fast networks it's

00:09:17,269 --> 00:09:20,989
a very fast operation for us to

00:09:18,860 --> 00:09:25,100
distribute those new checkpoints out to

00:09:20,989 --> 00:09:26,989
all the replicas and then we rely on

00:09:25,100 --> 00:09:30,049
Kinesis to make sure we don't lose any

00:09:26,989 --> 00:09:31,549
documents so if an indexer goes down we

00:09:30,049 --> 00:09:37,489
can fall back and the Kinesis queue and

00:09:31,549 --> 00:09:40,009
replay the updates so in addition to

00:09:37,489 --> 00:09:41,480
Kinesis we're using a ton of AWS

00:09:40,009 --> 00:09:42,739
features it's a great time to build a

00:09:41,480 --> 00:09:45,290
search application because you can just

00:09:42,739 --> 00:09:46,369
stand on the shoulders of giants all of

00:09:45,290 --> 00:09:47,749
these amazing services that are

00:09:46,369 --> 00:09:49,819
available in AWS you don't have to

00:09:47,749 --> 00:09:52,369
reinvent anymore so we use elastic

00:09:49,819 --> 00:09:55,160
container service to run each component

00:09:52,369 --> 00:09:56,839
of our architecture DynamoDB is used to

00:09:55,160 --> 00:09:58,699
hold the entire catalog that we're

00:09:56,839 --> 00:10:02,149
indexing so we can sort of pull from

00:09:58,699 --> 00:10:04,610
that to reindex the whole dock the whole

00:10:02,149 --> 00:10:06,980
index Kinesis queues are bringing in the

00:10:04,610 --> 00:10:08,629
real-time changes we use girfs EAP is to

00:10:06,980 --> 00:10:12,799
tell the nodes when they should refresh

00:10:08,629 --> 00:10:15,379
and and save snapshots to s3 and one

00:10:12,799 --> 00:10:18,499
unique property of our infrastructure is

00:10:15,379 --> 00:10:20,509
we rebuild the entire search index every

00:10:18,499 --> 00:10:22,369
time we push any version any software

00:10:20,509 --> 00:10:23,749
change it could be a trivial change it

00:10:22,369 --> 00:10:25,579
could just be a Java doc change or

00:10:23,749 --> 00:10:28,009
readme file or it could be a drastic

00:10:25,579 --> 00:10:30,319
change introducing a different approach

00:10:28,009 --> 00:10:31,790
for synonyms so because it's kind of

00:10:30,319 --> 00:10:33,619
risky to determine which software

00:10:31,790 --> 00:10:35,809
changes might have impacted the index in

00:10:33,619 --> 00:10:38,299
which which haven't we just rebuild on

00:10:35,809 --> 00:10:40,009
every on every push and it's Lucene is

00:10:38,299 --> 00:10:41,569
incredibly fast a tree indexing so that

00:10:40,009 --> 00:10:43,939
architecture works fine for us and

00:10:41,569 --> 00:10:45,860
because we're doing near real-time

00:10:43,939 --> 00:10:47,629
second replication we can afford to just

00:10:45,860 --> 00:10:51,860
do that on one node and copy the

00:10:47,629 --> 00:10:53,480
segment's out to the replicas so this is

00:10:51,860 --> 00:10:55,669
a picture that shows roughly how it

00:10:53,480 --> 00:10:57,470
works I'll go through it really quickly

00:10:55,669 --> 00:10:59,480
a query comes in at the top that's one

00:10:57,470 --> 00:11:01,549
request into a component we call the

00:10:59,480 --> 00:11:03,619
blender the blender fans out that

00:11:01,549 --> 00:11:07,040
request to multiple departments in

00:11:03,619 --> 00:11:09,079
Amazon's catalog each department has a

00:11:07,040 --> 00:11:11,419
collator in front and the collator

00:11:09,079 --> 00:11:13,399
I think is kind of like the coordinating

00:11:11,419 --> 00:11:15,499
node and elasticsearch it receives a

00:11:13,399 --> 00:11:17,329
query it figures out which charge in

00:11:15,499 --> 00:11:19,069
which replicas it's going to talk to it

00:11:17,329 --> 00:11:20,299
waits for them to reply it might retry

00:11:19,069 --> 00:11:22,279
if something goes wrong and then it

00:11:20,299 --> 00:11:23,730
merges the results back so that's a

00:11:22,279 --> 00:11:26,339
collator coming down here

00:11:23,730 --> 00:11:28,410
picking a replica in every column each

00:11:26,339 --> 00:11:29,999
shard is one column here and then

00:11:28,410 --> 00:11:33,029
sending the response back to the

00:11:29,999 --> 00:11:34,829
customer these are the indexing nodes so

00:11:33,029 --> 00:11:36,839
on top of each column we have an indexer

00:11:34,829 --> 00:11:40,410
who's taking consuming the Kinesis cues

00:11:36,839 --> 00:11:42,569
and DynamoDB pushing checkpoints or

00:11:40,410 --> 00:11:44,699
snapshots into s3 and then the replicas

00:11:42,569 --> 00:11:50,669
are copying from s3 and lighting those

00:11:44,699 --> 00:11:51,839
new segments for searching so when Lucy

00:11:50,669 --> 00:11:54,029
needs to search its index

00:11:51,839 --> 00:11:56,279
it's a segmented architecture the query

00:11:54,029 --> 00:11:58,679
comes in the query checks at each

00:11:56,279 --> 00:12:00,600
segment gets the best hits from every

00:11:58,679 --> 00:12:02,970
segment merges those results together

00:12:00,600 --> 00:12:04,259
and sends it back it's just like a shard

00:12:02,970 --> 00:12:09,449
at architecture but it's happening with

00:12:04,259 --> 00:12:10,829
happening within one Lucene index but if

00:12:09,449 --> 00:12:13,559
you do that sequentially which is the

00:12:10,829 --> 00:12:15,660
default for for leucine and and is how

00:12:13,559 --> 00:12:17,850
elasticsearch and solar work you pay a

00:12:15,660 --> 00:12:19,799
latency cost for all of those segments

00:12:17,850 --> 00:12:22,290
and if your query is costly and your

00:12:19,799 --> 00:12:24,089
shard is big enough that latency starts

00:12:22,290 --> 00:12:28,169
to push into uncomfortable numbers of

00:12:24,089 --> 00:12:30,269
milliseconds so we use the the leucine

00:12:28,169 --> 00:12:32,459
feature that allows us you just pass an

00:12:30,269 --> 00:12:34,379
executives executor service to index

00:12:32,459 --> 00:12:36,149
searcher and when that when you pass

00:12:34,379 --> 00:12:39,209
that index searcher will take the query

00:12:36,149 --> 00:12:41,730
and dispatch the query across multiple

00:12:39,209 --> 00:12:43,769
segments concurrently the small segments

00:12:41,730 --> 00:12:45,239
are coalesced into one work unit so one

00:12:43,769 --> 00:12:47,249
thread will do the small segments and

00:12:45,239 --> 00:12:48,989
then when those threads have all

00:12:47,249 --> 00:12:50,759
finished it does does it Joe in on all

00:12:48,989 --> 00:12:53,639
the results and merges them and returns

00:12:50,759 --> 00:12:55,350
them back this is really really good

00:12:53,639 --> 00:12:57,779
useful for us this allows us to make our

00:12:55,350 --> 00:13:00,689
shards larger and keep our query

00:12:57,779 --> 00:13:02,910
latencies lower until you we approach

00:13:00,689 --> 00:13:05,999
capacity so as we get close to red line

00:13:02,910 --> 00:13:07,230
QPS this feature hurts us because of the

00:13:05,999 --> 00:13:09,419
thread switching the overhead of

00:13:07,230 --> 00:13:11,970
switching between threads adds some cost

00:13:09,419 --> 00:13:14,239
and the red line QPS the red line

00:13:11,970 --> 00:13:16,470
capacity is a little worse as a result

00:13:14,239 --> 00:13:17,910
but it's a good trade-off from our

00:13:16,470 --> 00:13:19,259
standpoint we don't try to run our

00:13:17,910 --> 00:13:21,089
clusters at red line and we have other

00:13:19,259 --> 00:13:23,339
tools we can use to deal with a red line

00:13:21,089 --> 00:13:24,869
situation so it's far more important

00:13:23,339 --> 00:13:30,660
that we bring down our query Layton sees

00:13:24,869 --> 00:13:32,369
below red line we noticed one strange

00:13:30,660 --> 00:13:33,989
thing when we first launched our service

00:13:32,369 --> 00:13:36,419
we have all sorts of wonderful metrics

00:13:33,989 --> 00:13:37,170
this is a chart showing you two metrics

00:13:36,419 --> 00:13:39,179
the

00:13:37,170 --> 00:13:40,589
blue line which was the first metric we

00:13:39,179 --> 00:13:44,609
looked at we didn't know about the Green

00:13:40,589 --> 00:13:47,759
Line yet is our p99 9 three 9s query

00:13:44,609 --> 00:13:49,410
latency and we had to erase the axes I'm

00:13:47,759 --> 00:13:51,959
sorry we weren't allowed to show exact

00:13:49,410 --> 00:13:53,699
numbers here but the sawtooth pattern is

00:13:51,959 --> 00:13:55,019
what was very strange about this we all

00:13:53,699 --> 00:13:56,279
sort of scratched our heads and looked

00:13:55,019 --> 00:13:58,589
at our service and why would it have

00:13:56,279 --> 00:14:00,149
such a sharp behavior where suddenly the

00:13:58,589 --> 00:14:02,910
latency jumps and then kind of goes down

00:14:00,149 --> 00:14:04,739
again so then we realized we had a

00:14:02,910 --> 00:14:07,529
theory that it was it was related to

00:14:04,739 --> 00:14:09,149
leucine segmented architecture and could

00:14:07,529 --> 00:14:11,549
are concurrent and how we take advantage

00:14:09,149 --> 00:14:14,009
of concurrent search and what is

00:14:11,549 --> 00:14:16,109
happening is the Green Line is showing

00:14:14,009 --> 00:14:18,329
how many bytes were just copied out to

00:14:16,109 --> 00:14:21,319
the replicas so the spikes in the Green

00:14:18,329 --> 00:14:23,609
Line are large segment merges so

00:14:21,319 --> 00:14:25,470
normally when leucine merges small

00:14:23,609 --> 00:14:27,149
segments into a big one that's a good

00:14:25,470 --> 00:14:28,799
thing because because you've taken ten

00:14:27,149 --> 00:14:30,119
small segments and made a big one you

00:14:28,799 --> 00:14:31,889
don't have to you don't open so many

00:14:30,119 --> 00:14:33,149
files you don't have to visit all these

00:14:31,889 --> 00:14:35,160
little segments it's supposed to be a

00:14:33,149 --> 00:14:36,779
good thing and overall it is a good

00:14:35,160 --> 00:14:40,259
thing it brings you it brings up your

00:14:36,779 --> 00:14:42,059
capacity but in our case because we do

00:14:40,259 --> 00:14:44,160
concurrent search across segments those

00:14:42,059 --> 00:14:46,799
large segments were caused causing us

00:14:44,160 --> 00:14:48,809
more latency because now we lost some

00:14:46,799 --> 00:14:50,429
concurrency previously when we have the

00:14:48,809 --> 00:14:51,899
ted segments we would allow ten threads

00:14:50,429 --> 00:14:54,660
to search but now we have only one

00:14:51,899 --> 00:14:56,040
thread searching so I think there are

00:14:54,660 --> 00:14:57,540
some improvements we can do here and

00:14:56,040 --> 00:14:59,759
we're looking at whether we can use

00:14:57,540 --> 00:15:01,949
multiple threads even inside one large

00:14:59,759 --> 00:15:03,569
segment and then that would sort of

00:15:01,949 --> 00:15:06,869
smooth away those those sawtooth

00:15:03,569 --> 00:15:08,249
patterns okay

00:15:06,869 --> 00:15:11,970
changing gears to how we measure

00:15:08,249 --> 00:15:14,699
performance so we all have wonderful

00:15:11,970 --> 00:15:16,589
unit tests integration tests all sorts

00:15:14,699 --> 00:15:18,329
of Approval steps that we have in our

00:15:16,589 --> 00:15:20,759
pipelines there's all these things we

00:15:18,329 --> 00:15:22,169
have to catch functional errors but

00:15:20,759 --> 00:15:25,319
catching performance regressions is a

00:15:22,169 --> 00:15:27,089
lot harder so to help us with that and

00:15:25,319 --> 00:15:29,489
it's not a perfect solution but it works

00:15:27,089 --> 00:15:30,869
really well we have a set of internal

00:15:29,489 --> 00:15:33,149
benchmarks that are similar to the

00:15:30,869 --> 00:15:35,459
public Lucene benchmarks that run every

00:15:33,149 --> 00:15:37,919
night and these benchmarks they wake up

00:15:35,459 --> 00:15:39,869
they take up a recent snapshot of the

00:15:37,919 --> 00:15:42,119
catalog they take a recent snapshot of

00:15:39,869 --> 00:15:43,739
actual customer queries they index the

00:15:42,119 --> 00:15:44,999
catalog they send the queries to that

00:15:43,739 --> 00:15:47,819
index and they measure all sorts of

00:15:44,999 --> 00:15:50,069
metrics our long toll query latency our

00:15:47,819 --> 00:15:50,790
throughput memory usage number of

00:15:50,069 --> 00:15:53,160
garbage collection

00:15:50,790 --> 00:15:55,620
all sorts of metrics and chart those in

00:15:53,160 --> 00:15:57,390
nightly graphs which we go and look at

00:15:55,620 --> 00:15:59,640
every so often to catch accidental

00:15:57,390 --> 00:16:01,350
regressions they have caught a lot of

00:15:59,640 --> 00:16:02,880
accidental regressions it's easy to make

00:16:01,350 --> 00:16:04,200
a wonderful-looking change that passes

00:16:02,880 --> 00:16:08,390
all tests and everything's fine and you

00:16:04,200 --> 00:16:11,010
notice it had a 20% hit to your p99

00:16:08,390 --> 00:16:13,140
we also measure functionality so it's

00:16:11,010 --> 00:16:15,120
not just performance but whether a

00:16:13,140 --> 00:16:17,160
change that should have just been an

00:16:15,120 --> 00:16:19,950
optimization altered the search results

00:16:17,160 --> 00:16:21,450
so that's a very bad situation if we if

00:16:19,950 --> 00:16:23,400
we change the search order and we didn't

00:16:21,450 --> 00:16:25,290
expect to our benchmarks will help us

00:16:23,400 --> 00:16:27,150
catch that so these same benchmarks that

00:16:25,290 --> 00:16:29,250
run in a nightly machine they also are

00:16:27,150 --> 00:16:30,840
available for developers to run so that

00:16:29,250 --> 00:16:32,220
if they made a performance change they

00:16:30,840 --> 00:16:33,300
want to see what the impact was they can

00:16:32,220 --> 00:16:34,740
do that in the privacy of their

00:16:33,300 --> 00:16:37,470
workspace they can iterate and fix it

00:16:34,740 --> 00:16:40,830
and makes it a lot easier and lot safer

00:16:37,470 --> 00:16:42,210
to make exciting changes so I mentioned

00:16:40,830 --> 00:16:43,590
that long pole query latency is

00:16:42,210 --> 00:16:46,050
something we really pay attention to

00:16:43,590 --> 00:16:47,430
when we measure our long pole latency we

00:16:46,050 --> 00:16:49,280
use a load testing client it's an

00:16:47,430 --> 00:16:52,350
in-house load testing client that uses

00:16:49,280 --> 00:16:54,960
it's an open loop client so an open loop

00:16:52,350 --> 00:16:57,390
client is one that sends the request and

00:16:54,960 --> 00:16:59,040
doesn't wait further response it goes

00:16:57,390 --> 00:17:00,510
back to its thread pool and it sends

00:16:59,040 --> 00:17:03,690
another request when it's time to send

00:17:00,510 --> 00:17:05,970
it there are quite a few frustratingly

00:17:03,690 --> 00:17:07,440
high number of load testing clients that

00:17:05,970 --> 00:17:09,030
don't do this they use a closed loop

00:17:07,440 --> 00:17:11,610
tests to measure query latencies and

00:17:09,030 --> 00:17:13,140
that will lie you will get rosy looking

00:17:11,610 --> 00:17:17,000
results when in fact you have horrible

00:17:13,140 --> 00:17:18,690
results if you if you look up gilt na

00:17:17,000 --> 00:17:20,100
did a great talk

00:17:18,690 --> 00:17:21,630
describing why your load testing for the

00:17:20,100 --> 00:17:23,220
client is probably lying to you so if

00:17:21,630 --> 00:17:25,110
your load testing client is not using an

00:17:23,220 --> 00:17:27,540
open loop go back and look at it watch

00:17:25,110 --> 00:17:31,230
that talk and and find a better load

00:17:27,540 --> 00:17:32,760
testing client redline QPS we measure

00:17:31,230 --> 00:17:34,020
with a closed loop client so a closed to

00:17:32,760 --> 00:17:36,780
the client will just send the request

00:17:34,020 --> 00:17:38,550
and wait for the response and then when

00:17:36,780 --> 00:17:40,440
it gets it send another request and if

00:17:38,550 --> 00:17:43,590
you if you run with enough clients that

00:17:40,440 --> 00:17:45,210
the the reports the metrics measured by

00:17:43,590 --> 00:17:48,120
that close the plant will be close to

00:17:45,210 --> 00:17:50,040
your red line capacity when we measure

00:17:48,120 --> 00:17:51,600
latency there's a we sort of struggle

00:17:50,040 --> 00:17:53,100
with what at what load should we measure

00:17:51,600 --> 00:17:54,390
latency because if you measure latency

00:17:53,100 --> 00:17:56,250
at red line that's not so interesting

00:17:54,390 --> 00:17:58,830
there's just a lot of waiting happening

00:17:56,250 --> 00:18:01,350
if you measure at 50% performance with a

00:17:58,830 --> 00:18:04,620
Poisson process which is a mathematical

00:18:01,350 --> 00:18:06,630
model of how queries arrive in practice

00:18:04,620 --> 00:18:09,360
then you're going to also measure a lot

00:18:06,630 --> 00:18:11,100
of contention so we've sort of changed

00:18:09,360 --> 00:18:13,350
where we measure latency we measure at a

00:18:11,100 --> 00:18:14,760
fairly low rate because we really want

00:18:13,350 --> 00:18:16,710
to measure whether the software got any

00:18:14,760 --> 00:18:22,440
slower at replying at computing the

00:18:16,710 --> 00:18:24,059
answer to a query when we looked at our

00:18:22,440 --> 00:18:25,470
benchmarks this was a very interesting

00:18:24,059 --> 00:18:28,200
thing we noticed

00:18:25,470 --> 00:18:30,210
Lusine had absolutely horrible refresh

00:18:28,200 --> 00:18:32,460
times and which was very surprising

00:18:30,210 --> 00:18:33,750
because I know from Lu Singh's nightly

00:18:32,460 --> 00:18:36,870
benchmarks it doesn't have horrible

00:18:33,750 --> 00:18:38,789
refresh times so we realized that the

00:18:36,870 --> 00:18:41,820
way we were using the scene and how Lu

00:18:38,789 --> 00:18:43,980
seems concurrency model works weren't

00:18:41,820 --> 00:18:46,470
wasn't a good match so with Lu Singh's

00:18:43,980 --> 00:18:47,610
indexing if you have lots of threads

00:18:46,470 --> 00:18:49,649
doing indexing it's wonderfully

00:18:47,610 --> 00:18:52,320
concurrent you're saturating CPU or i/o

00:18:49,649 --> 00:18:53,760
you're getting amazing throughput but

00:18:52,320 --> 00:18:55,140
then if you stop if you pause your

00:18:53,760 --> 00:18:56,940
indexing threads and wait for them to

00:18:55,140 --> 00:18:58,289
all finish which is a common case you

00:18:56,940 --> 00:19:00,179
know you indexed all of your catalogue

00:18:58,289 --> 00:19:02,399
and once it's all indexed then you ask

00:19:00,179 --> 00:19:05,070
who seemed to commit that is a single

00:19:02,399 --> 00:19:06,840
threaded operation in Lucene today which

00:19:05,070 --> 00:19:08,070
is really weird because the indexing was

00:19:06,840 --> 00:19:10,409
nice and concurrent and now you do a

00:19:08,070 --> 00:19:11,610
refresh and it's really slow so we in

00:19:10,409 --> 00:19:12,990
our benchmarks we noticed that was

00:19:11,610 --> 00:19:14,909
incredibly slow we opened an issue

00:19:12,990 --> 00:19:16,200
Simon replied on the issue and said this

00:19:14,909 --> 00:19:18,270
is how you can fix it with an existing

00:19:16,200 --> 00:19:20,820
Lucienne api we made that fix and it's

00:19:18,270 --> 00:19:22,860
much faster going from a single thread

00:19:20,820 --> 00:19:27,899
to 64 threads committing is a gigantic

00:19:22,860 --> 00:19:29,880
improvement in your refresh times the

00:19:27,899 --> 00:19:31,710
benchmarks that we use are able to tap

00:19:29,880 --> 00:19:34,409
into some amazing metrics that would

00:19:31,710 --> 00:19:35,940
normally be quite hard to get except for

00:19:34,409 --> 00:19:38,070
the fact that leucine has really good

00:19:35,940 --> 00:19:40,350
abstractions directory reader is the

00:19:38,070 --> 00:19:42,480
abstraction for reading an index from a

00:19:40,350 --> 00:19:44,220
disk directory we use that to count how

00:19:42,480 --> 00:19:45,779
many term dictionary lookups we're doing

00:19:44,220 --> 00:19:47,580
because that's a fairly costly operation

00:19:45,779 --> 00:19:49,200
in Liu's team we have a bunch of custom

00:19:47,580 --> 00:19:50,730
code that does terms dictionary lookups

00:19:49,200 --> 00:19:52,500
and that that's something we chart is

00:19:50,730 --> 00:19:55,049
how many times a single query had to

00:19:52,500 --> 00:19:57,809
look up terms we wrap directory and

00:19:55,049 --> 00:19:59,460
index input to gather IO counters so we

00:19:57,809 --> 00:20:02,399
have a metric that tells us how many I

00:19:59,460 --> 00:20:04,110
ops did the query do each query we have

00:20:02,399 --> 00:20:06,899
this metric how many bytes did it read

00:20:04,110 --> 00:20:08,549
and if we push a change that double the

00:20:06,899 --> 00:20:13,890
number of bytes then we want to go back

00:20:08,549 --> 00:20:15,600
and understand why that happened so

00:20:13,890 --> 00:20:17,669
those metrics make those abstractions

00:20:15,600 --> 00:20:18,000
which Doug cutting a long time ago had

00:20:17,669 --> 00:20:19,770
the four

00:20:18,000 --> 00:20:21,300
site to create most of his abstractions

00:20:19,770 --> 00:20:23,160
are still in loosing today those are

00:20:21,300 --> 00:20:26,460
incredible tools for doing gathering

00:20:23,160 --> 00:20:27,690
custom metrics for for search engine we

00:20:26,460 --> 00:20:29,160
learned the lesson that everyone else

00:20:27,690 --> 00:20:30,270
has learned already that full garbage

00:20:29,160 --> 00:20:32,010
collection is bad

00:20:30,270 --> 00:20:33,990
Lusine is an amazingly lightweight

00:20:32,010 --> 00:20:36,690
search engine it does not allocate much

00:20:33,990 --> 00:20:38,040
memory to handle a query and and many

00:20:36,690 --> 00:20:40,350
concurrent queries in flight really

00:20:38,040 --> 00:20:42,720
don't allocate that much memory we made

00:20:40,350 --> 00:20:44,280
that we broke that because we had some

00:20:42,720 --> 00:20:46,290
places that we're using a lot of heap

00:20:44,280 --> 00:20:47,910
places we didn't think would be so bad

00:20:46,290 --> 00:20:50,940
but under load it turned out to be quite

00:20:47,910 --> 00:20:53,160
bad and and we don't trust G 1 GC yet

00:20:50,940 --> 00:20:55,230
although Wei told me I should start

00:20:53,160 --> 00:20:56,850
trusting it we still use the depth the

00:20:55,230 --> 00:20:58,560
now deprecated concurrent collector

00:20:56,850 --> 00:21:00,240
because we want low pause times and we

00:20:58,560 --> 00:21:03,390
don't mind trading off some throughput

00:21:00,240 --> 00:21:05,880
to get that Azul has an awesome tool

00:21:03,390 --> 00:21:08,100
called J hiccup this is a way to monitor

00:21:05,880 --> 00:21:09,660
the pauses you see in production not

00:21:08,100 --> 00:21:11,910
just from garbage collection but from

00:21:09,660 --> 00:21:13,500
the operating system scheduler your i/o

00:21:11,910 --> 00:21:16,170
devices if there are any unexpected

00:21:13,500 --> 00:21:18,120
pauses that will tell you the best you

00:21:16,170 --> 00:21:19,440
can do on your long pull query latencies

00:21:18,120 --> 00:21:20,820
because if the JVM is pausing the

00:21:19,440 --> 00:21:22,230
machine is pausing you you can't do any

00:21:20,820 --> 00:21:25,590
better than that no matter how fast you

00:21:22,230 --> 00:21:27,030
make your code so when we when we had

00:21:25,590 --> 00:21:28,700
problems with garbage collector we were

00:21:27,030 --> 00:21:30,780
hitting full stuff the world events we

00:21:28,700 --> 00:21:32,130
fixed a few places that we're holding on

00:21:30,780 --> 00:21:34,350
to too much heap we increased our heap

00:21:32,130 --> 00:21:35,970
size and we post a few parameters from

00:21:34,350 --> 00:21:37,920
elasticsearch that caused the concurrent

00:21:35,970 --> 00:21:39,840
collector to work a little harder to

00:21:37,920 --> 00:21:43,410
kick in a little sooner and that was a

00:21:39,840 --> 00:21:45,510
big improvement for us and this chart is

00:21:43,410 --> 00:21:47,520
not our benchmark chart this is from the

00:21:45,510 --> 00:21:49,890
scenes nightly benchmarks just an

00:21:47,520 --> 00:21:51,990
illustration of how much you have to pay

00:21:49,890 --> 00:21:54,060
attention to your garbage collector this

00:21:51,990 --> 00:21:56,400
is what's showing the performance of a

00:21:54,060 --> 00:21:58,110
sloppy phrase query on a full Wikipedia

00:21:56,400 --> 00:22:00,390
index so it's kind of a costly query for

00:21:58,110 --> 00:22:02,220
Lucene to run it was sort of find you

00:22:00,390 --> 00:22:04,140
know through months that it was fine for

00:22:02,220 --> 00:22:05,970
quite a while back before this but then

00:22:04,140 --> 00:22:08,970
suddenly we have created lou singh's

00:22:05,970 --> 00:22:12,030
nightly benchmarks to JDK 11 and saw a

00:22:08,970 --> 00:22:14,640
substantial in 12% drop in the in the

00:22:12,030 --> 00:22:16,140
QPS performance of this one query there

00:22:14,640 --> 00:22:17,370
was a discussion on the dev list

00:22:16,140 --> 00:22:19,410
huwway said it must be the garbage

00:22:17,370 --> 00:22:21,540
collector and so I went back and fixed

00:22:19,410 --> 00:22:24,360
the Java invocation to go back to the

00:22:21,540 --> 00:22:26,670
the throughput collector parallel GC and

00:22:24,360 --> 00:22:27,990
that restored quite a bit of our

00:22:26,670 --> 00:22:29,970
performance not all of it so there's

00:22:27,990 --> 00:22:31,680
still something missing there but

00:22:29,970 --> 00:22:35,220
garbage collection

00:22:31,680 --> 00:22:39,390
is difficult okay I'm gonna hand it over

00:22:35,220 --> 00:22:40,530
to Sparky you can still call me Mike

00:22:39,390 --> 00:22:42,570
it's okay

00:22:40,530 --> 00:22:43,830
okay well Michael come back at the end

00:22:42,570 --> 00:22:45,330
and we'll try to leave some room for

00:22:43,830 --> 00:22:47,900
questions but I'm gonna take the rest of

00:22:45,330 --> 00:22:50,250
this so um

00:22:47,900 --> 00:22:52,080
analysis I mean everybody has to deal

00:22:50,250 --> 00:22:53,640
with text text is you know one of the

00:22:52,080 --> 00:22:56,160
great things about Lucene is it gives

00:22:53,640 --> 00:22:59,160
you so many tools for dealing with text

00:22:56,160 --> 00:23:01,410
you know there's an analyzer for every

00:22:59,160 --> 00:23:04,590
language there are many many tools you

00:23:01,410 --> 00:23:06,270
can apply however every application is

00:23:04,590 --> 00:23:10,350
unique and it's in its own special way

00:23:06,270 --> 00:23:12,210
and we're no different we found we had

00:23:10,350 --> 00:23:13,920
some kind of special challenges to deal

00:23:12,210 --> 00:23:15,900
with part of it part of this also comes

00:23:13,920 --> 00:23:17,310
from trying to maintain the history of

00:23:15,900 --> 00:23:19,380
an engine that was already built

00:23:17,310 --> 00:23:22,230
in-house over 10 years and maintain

00:23:19,380 --> 00:23:26,100
compatibility because we we had a kind

00:23:22,230 --> 00:23:27,870
of a goal not to change the user search

00:23:26,100 --> 00:23:29,040
results too much during this port

00:23:27,870 --> 00:23:31,260
because we don't want to test too many

00:23:29,040 --> 00:23:33,120
things all at once so how do we maintain

00:23:31,260 --> 00:23:34,620
that while completely shifting you know

00:23:33,120 --> 00:23:37,710
the way that the analysis is being done

00:23:34,620 --> 00:23:41,280
underneath we committed to to doing

00:23:37,710 --> 00:23:42,780
basically a leucine native analyzer I

00:23:41,280 --> 00:23:44,520
mean early on we said well we could just

00:23:42,780 --> 00:23:46,080
port the existing code wrap it all up as

00:23:44,520 --> 00:23:48,120
a tokenizer and say well that's our

00:23:46,080 --> 00:23:49,590
that's our analysis chain instead we

00:23:48,120 --> 00:23:51,990
decided to break it up into pieces and

00:23:49,590 --> 00:23:55,560
that was more challenging I'm gonna give

00:23:51,990 --> 00:23:56,610
you a few examples here's one what does

00:23:55,560 --> 00:23:58,710
plain mean to you

00:23:56,610 --> 00:24:00,930
well obviously like many words it has

00:23:58,710 --> 00:24:03,090
many meanings you know even in the

00:24:00,930 --> 00:24:05,670
context of shopping could mean many

00:24:03,090 --> 00:24:08,010
different things you know it could be it

00:24:05,670 --> 00:24:10,230
could be an airplane Amazon doesn't sell

00:24:08,010 --> 00:24:13,200
airplanes but it does sell toy airplanes

00:24:10,230 --> 00:24:15,840
and airplane keychains I guess could

00:24:13,200 --> 00:24:20,130
also mean you know in English a tool a

00:24:15,840 --> 00:24:21,750
bench plane or a plane so when someone's

00:24:20,130 --> 00:24:23,100
searching for for that we want to give

00:24:21,750 --> 00:24:24,870
them the right the right answer well

00:24:23,100 --> 00:24:26,070
what is it well the thing is what we'd

00:24:24,870 --> 00:24:27,990
like to be able to do is apply some

00:24:26,070 --> 00:24:30,600
synonyms if someone searches for

00:24:27,990 --> 00:24:32,730
airplane but that document only said

00:24:30,600 --> 00:24:34,200
plane you know we'd like it to match so

00:24:32,730 --> 00:24:35,790
we want to apply an airplane to plane

00:24:34,200 --> 00:24:37,560
synonym but if we do that we don't want

00:24:35,790 --> 00:24:39,120
when they search for airplane for them

00:24:37,560 --> 00:24:40,470
to be finding bench planes so this is

00:24:39,120 --> 00:24:42,660
kind of a challenging problem

00:24:40,470 --> 00:24:45,150
the good news is we're doing our

00:24:42,660 --> 00:24:45,990
synonyms at index time we know at the

00:24:45,150 --> 00:24:48,420
document

00:24:45,990 --> 00:24:51,780
I mean we know for example that this is

00:24:48,420 --> 00:24:53,190
not a toy we know it's a tool so we can

00:24:51,780 --> 00:24:53,880
apply different synonyms in different

00:24:53,190 --> 00:24:55,560
contexts

00:24:53,880 --> 00:24:58,170
that's that's the sort of the theme here

00:24:55,560 --> 00:25:00,090
is context-sensitive analysis we want to

00:24:58,170 --> 00:25:02,250
be able to apply different analysis

00:25:00,090 --> 00:25:03,780
chains depending on maybe the value that

00:25:02,250 --> 00:25:05,520
comes from another field in this case

00:25:03,780 --> 00:25:08,310
the category that the document is in

00:25:05,520 --> 00:25:10,590
that's kind of challenging to do or has

00:25:08,310 --> 00:25:12,420
been challenging to do in lieu seems API

00:25:10,590 --> 00:25:14,790
is because you the way that you

00:25:12,420 --> 00:25:17,400
typically apply an analyzer is by

00:25:14,790 --> 00:25:19,320
specifying which field it applies to so

00:25:17,400 --> 00:25:21,780
you say ok I'm gonna use this analyzer

00:25:19,320 --> 00:25:23,190
for the title this one for the full-body

00:25:21,780 --> 00:25:24,810
text but it doesn't really give you a

00:25:23,190 --> 00:25:26,880
way of combining information that come

00:25:24,810 --> 00:25:29,040
from other fields so we had to write

00:25:26,880 --> 00:25:31,080
some custom code around that and then at

00:25:29,040 --> 00:25:32,670
some point discussion on the mailing

00:25:31,080 --> 00:25:34,650
list led to some entirely new way of

00:25:32,670 --> 00:25:36,660
handling this which is a conditional

00:25:34,650 --> 00:25:38,670
token filter which we didn't have time

00:25:36,660 --> 00:25:41,670
to use because came out later but I

00:25:38,670 --> 00:25:44,940
think that'll be a nice new way that you

00:25:41,670 --> 00:25:47,450
can do this kind of thing yeah I think

00:25:44,940 --> 00:25:50,220
that's all I have to say about synonyms

00:25:47,450 --> 00:25:52,890
another thing that you know was kind of

00:25:50,220 --> 00:25:56,690
unique in our use case in the analysis

00:25:52,890 --> 00:25:59,910
chain is the way we handle numbers like

00:25:56,690 --> 00:26:03,090
searching for products on in a catalog

00:25:59,910 --> 00:26:06,050
people really care about the sizes the

00:26:03,090 --> 00:26:10,290
age that the products are for the prices

00:26:06,050 --> 00:26:12,150
you know the the voltage that the tool

00:26:10,290 --> 00:26:13,350
works at and so on so the numbers are

00:26:12,150 --> 00:26:16,260
really important in a way that they

00:26:13,350 --> 00:26:19,530
aren't so much in a traditional you know

00:26:16,260 --> 00:26:21,150
let's say full-text search so some of

00:26:19,530 --> 00:26:22,500
the things we do are like twenty four

00:26:21,150 --> 00:26:24,420
three year old we wanted to match this

00:26:22,500 --> 00:26:26,220
age range two to four years but nowhere

00:26:24,420 --> 00:26:28,800
in the text of that document does it say

00:26:26,220 --> 00:26:30,930
three so you know we expand the range to

00:26:28,800 --> 00:26:33,350
include all the intermediate numbers you

00:26:30,930 --> 00:26:35,490
know we want to do number unit

00:26:33,350 --> 00:26:39,030
translations so that you can you can

00:26:35,490 --> 00:26:41,520
search for you know 1.5 liters and we

00:26:39,030 --> 00:26:44,760
don't match things that are about 1.5

00:26:41,520 --> 00:26:46,800
volts or what have you and then handling

00:26:44,760 --> 00:26:50,160
decimals and fractions and other kinds

00:26:46,800 --> 00:26:52,740
of numbers with punctuation you know is

00:26:50,160 --> 00:26:54,690
is is a challenge I asked people to ask

00:26:52,740 --> 00:26:56,820
me about tires today only only a few

00:26:54,690 --> 00:26:59,160
people did but we you know we had a bug

00:26:56,820 --> 00:27:01,200
with tire sizes because

00:26:59,160 --> 00:27:02,730
our sizes have slashes in them they're

00:27:01,200 --> 00:27:05,970
often written as several numbers with

00:27:02,730 --> 00:27:07,740
slashes in them and you know we confuse

00:27:05,970 --> 00:27:09,810
them with fractions so there's just all

00:27:07,740 --> 00:27:11,460
these subtleties and and it's really

00:27:09,810 --> 00:27:14,010
tricky building analysis right because

00:27:11,460 --> 00:27:15,300
you you um change one thing when one

00:27:14,010 --> 00:27:17,040
place and it changes everything

00:27:15,300 --> 00:27:19,500
somewhere else even though it's a nice

00:27:17,040 --> 00:27:21,120
modular chain of filters right there

00:27:19,500 --> 00:27:23,400
they impact each other I think we all

00:27:21,120 --> 00:27:25,200
know that if we fiddled with with

00:27:23,400 --> 00:27:26,490
analysis one of the things the

00:27:25,200 --> 00:27:28,710
consequences of having all this

00:27:26,490 --> 00:27:30,210
specialized punctuation handling is that

00:27:28,710 --> 00:27:32,250
we can't really use standard tokenizer

00:27:30,210 --> 00:27:34,020
standard tokenizer is what most people

00:27:32,250 --> 00:27:35,880
use it it's works great unless you're

00:27:34,020 --> 00:27:37,620
very fussy about punctuation but then

00:27:35,880 --> 00:27:39,810
you can't use it because it drops all

00:27:37,620 --> 00:27:42,000
your punctuation so instead we use you

00:27:39,810 --> 00:27:43,800
know basic a basic tokenizer with

00:27:42,000 --> 00:27:45,810
whitespace and then later on we apply

00:27:43,800 --> 00:27:47,280
word delimiter graph filter where

00:27:45,810 --> 00:27:49,920
delimiter graph filters like the Swiss

00:27:47,280 --> 00:27:52,970
Army knife of an analysis chains I mean

00:27:49,920 --> 00:27:56,100
it handles a special case of sort of

00:27:52,970 --> 00:27:57,660
secondary tokenization right you can do

00:27:56,100 --> 00:27:59,430
it if you didn't tokenize on those

00:27:57,660 --> 00:28:01,110
things early the punctuation or the non

00:27:59,430 --> 00:28:03,030
letter number characters or whatever

00:28:01,110 --> 00:28:04,890
it'll split them up later but it leads

00:28:03,030 --> 00:28:06,870
it leads to trouble like if it's it's

00:28:04,890 --> 00:28:09,410
useful but if you don't have to use it I

00:28:06,870 --> 00:28:11,730
would say don't use it because it causes

00:28:09,410 --> 00:28:14,180
Lucene gets a little bit unhappy if you

00:28:11,730 --> 00:28:16,830
have multiple things that split your

00:28:14,180 --> 00:28:20,220
text into tokens anyway but you can use

00:28:16,830 --> 00:28:22,650
it and then we do it's pretty good but

00:28:20,220 --> 00:28:24,150
one takeaway from all this analysis

00:28:22,650 --> 00:28:25,710
stuff there's there's not really a theme

00:28:24,150 --> 00:28:27,780
here because it's just a lot of little

00:28:25,710 --> 00:28:29,160
problems one after another right and you

00:28:27,780 --> 00:28:30,810
have to solve them and keep working at

00:28:29,160 --> 00:28:32,970
it until you get it right and we did

00:28:30,810 --> 00:28:34,920
that but I would love it if we could

00:28:32,970 --> 00:28:36,000
like isn't machine learning the sauce

00:28:34,920 --> 00:28:38,130
that's supposed to solve all our

00:28:36,000 --> 00:28:40,110
problems like with somebody please apply

00:28:38,130 --> 00:28:42,090
it to tokenization I would love I would

00:28:40,110 --> 00:28:43,590
love that I think we need that there's

00:28:42,090 --> 00:28:45,870
an opportunity here I mean this is like

00:28:43,590 --> 00:28:48,810
the oldest part of search but it needs

00:28:45,870 --> 00:28:51,660
some work okay so that's enough about

00:28:48,810 --> 00:28:53,670
analysis query optimization so I'm gonna

00:28:51,660 --> 00:28:56,940
Mike refer to a few of these things at

00:28:53,670 --> 00:28:57,920
all before sorry and I'll just dive a

00:28:56,940 --> 00:29:00,450
little deeper

00:28:57,920 --> 00:29:02,640
you know latency is important for us our

00:29:00,450 --> 00:29:04,020
customers are humans we don't want them

00:29:02,640 --> 00:29:05,400
to have to wait if they wait too long

00:29:04,020 --> 00:29:06,240
they'll go away they'll buy things

00:29:05,400 --> 00:29:08,190
somewhere else

00:29:06,240 --> 00:29:10,050
speed is super important you know it's

00:29:08,190 --> 00:29:11,730
important in in every dimension delivery

00:29:10,050 --> 00:29:12,600
has always been one of Amazon's like

00:29:11,730 --> 00:29:14,370
prime

00:29:12,600 --> 00:29:15,720
promises we'll get it to you fast but

00:29:14,370 --> 00:29:17,700
you know it's also part of the search

00:29:15,720 --> 00:29:19,440
experience and like if you have to wait

00:29:17,700 --> 00:29:23,730
for three seconds you know it's just not

00:29:19,440 --> 00:29:25,320
good so you know one of the tricks this

00:29:23,730 --> 00:29:27,480
was this was a cool idea that I think

00:29:25,320 --> 00:29:29,460
many people can use and you know I'm

00:29:27,480 --> 00:29:30,870
it's called index queries here there's a

00:29:29,460 --> 00:29:33,060
couple of different ideas here they both

00:29:30,870 --> 00:29:35,730
come out of like what I would call

00:29:33,060 --> 00:29:37,980
adaptive indexing so the basic idea is

00:29:35,730 --> 00:29:40,020
look at your queries see what people are

00:29:37,980 --> 00:29:41,970
searching for commonly and then use that

00:29:40,020 --> 00:29:44,280
to inform your indexing process and it's

00:29:41,970 --> 00:29:45,990
a circular thing and it it's tricky but

00:29:44,280 --> 00:29:48,000
you can get some nice gains from it so

00:29:45,990 --> 00:29:51,330
what we did here is we went to our query

00:29:48,000 --> 00:29:53,460
logs you know printed the Lucene query

00:29:51,330 --> 00:29:56,310
that it was we were parsing out of after

00:29:53,460 --> 00:29:58,560
doing all our processing and we thought

00:29:56,310 --> 00:30:01,230
well hey like all our queries have the

00:29:58,560 --> 00:30:02,790
same you know set of filters they're all

00:30:01,230 --> 00:30:04,050
filtering on you know these are some

00:30:02,790 --> 00:30:06,480
very common things that say this

00:30:04,050 --> 00:30:08,550
document is a product it was not

00:30:06,480 --> 00:30:10,560
suppressed by some rule that some

00:30:08,550 --> 00:30:12,660
business rules someone applied it's not

00:30:10,560 --> 00:30:15,570
an adult product most queries are like

00:30:12,660 --> 00:30:17,040
this right and there are there are more

00:30:15,570 --> 00:30:18,690
filters than that I didn't show you all

00:30:17,040 --> 00:30:21,300
of them that are repeated over and over

00:30:18,690 --> 00:30:23,880
and you know leucine is really fast at

00:30:21,300 --> 00:30:25,890
matching these very simple term queries

00:30:23,880 --> 00:30:27,960
and then conjoining them together

00:30:25,890 --> 00:30:31,830
finding the documents that match all of

00:30:27,960 --> 00:30:33,210
them but it still wasted effort like if

00:30:31,830 --> 00:30:35,520
we can replace all of those with a

00:30:33,210 --> 00:30:37,800
single term hey we can we can go faster

00:30:35,520 --> 00:30:39,720
so basically what we do is we treat the

00:30:37,800 --> 00:30:42,030
queries as documents we look for

00:30:39,720 --> 00:30:43,620
commonly occurring sub queries we factor

00:30:42,030 --> 00:30:46,050
them out of the queries and the

00:30:43,620 --> 00:30:48,000
documents so when we're indexing we find

00:30:46,050 --> 00:30:49,530
documents that match these commonly

00:30:48,000 --> 00:30:51,450
occurring sub queries and we index a

00:30:49,530 --> 00:30:53,100
single term to represent that and then a

00:30:51,450 --> 00:30:56,130
query time we do the same thing replace

00:30:53,100 --> 00:30:57,810
all those terms that whole term thing

00:30:56,130 --> 00:31:00,840
with a single term query and we got a

00:30:57,810 --> 00:31:02,610
nice speed up from that this is a little

00:31:00,840 --> 00:31:03,870
more about how that works I mean in

00:31:02,610 --> 00:31:06,120
general the problem is hard because

00:31:03,870 --> 00:31:08,910
boolean expressions are trees you know

00:31:06,120 --> 00:31:10,800
you them it's kind of a challenging

00:31:08,910 --> 00:31:13,380
problem but our queries are simpler you

00:31:10,800 --> 00:31:15,120
know we have a process that generates

00:31:13,380 --> 00:31:18,000
them that doesn't create these very deep

00:31:15,120 --> 00:31:19,920
trees so that helped also I'll just give

00:31:18,000 --> 00:31:21,330
a shout-out to this FB growth algorithm

00:31:19,920 --> 00:31:23,460
I don't know if anybody's encountered it

00:31:21,330 --> 00:31:26,190
I learned about it during the course of

00:31:23,460 --> 00:31:29,250
this work and it's pretty cool it

00:31:26,190 --> 00:31:31,169
commonly occurring sub patterns you know

00:31:29,250 --> 00:31:34,289
inquiries it's commonly applied like in

00:31:31,169 --> 00:31:38,850
database world and it sped things up on

00:31:34,289 --> 00:31:40,200
our index inside so yeah this is kind of

00:31:38,850 --> 00:31:42,450
the results that we saw this was early

00:31:40,200 --> 00:31:43,950
on you know but we saw a nice boost at

00:31:42,450 --> 00:31:46,049
that put time thirty percent in

00:31:43,950 --> 00:31:47,879
improvement to our QP s you know by

00:31:46,049 --> 00:31:51,299
avoiding all this repetitive work and

00:31:47,879 --> 00:31:52,559
latency went way down I think maybe a

00:31:51,299 --> 00:31:55,230
little different now but it's it's a

00:31:52,559 --> 00:31:57,659
it's a nice boost we did a similar thing

00:31:55,230 --> 00:31:59,519
with full text so the thing I showed you

00:31:57,659 --> 00:32:02,340
before was really just for kind of

00:31:59,519 --> 00:32:04,230
filters you know on very simple terms

00:32:02,340 --> 00:32:07,980
but you can also apply similar

00:32:04,230 --> 00:32:08,909
optimization to you know text it's a

00:32:07,980 --> 00:32:10,590
little different here because the

00:32:08,909 --> 00:32:12,000
cardinalities of the terms are different

00:32:10,590 --> 00:32:13,799
you have to kind of set different

00:32:12,000 --> 00:32:15,720
thresholds about how many of these

00:32:13,799 --> 00:32:19,740
things you index so you don't blow out

00:32:15,720 --> 00:32:21,809
various limits but I wanted to show

00:32:19,740 --> 00:32:23,700
these partly just so it's a cool idea

00:32:21,809 --> 00:32:25,679
but also because the terms are kind of

00:32:23,700 --> 00:32:27,389
interesting I think and it highlights a

00:32:25,679 --> 00:32:29,250
few things about our system one of them

00:32:27,389 --> 00:32:30,720
is that hey this data is changing all

00:32:29,250 --> 00:32:32,730
the time I mean you can see Valentine's

00:32:30,720 --> 00:32:34,200
Day down there at the bottom it's not

00:32:32,730 --> 00:32:37,230
all that interesting for us to index

00:32:34,200 --> 00:32:39,210
Valentine's Day most of the year so if

00:32:37,230 --> 00:32:42,210
we build this index you know this this

00:32:39,210 --> 00:32:44,970
kind of set of indexed tuples and

00:32:42,210 --> 00:32:46,860
queries once and leave it then we're our

00:32:44,970 --> 00:32:48,149
performance is going to slowly drop and

00:32:46,860 --> 00:32:49,950
that's kind of a dangerous thing to have

00:32:48,149 --> 00:32:51,509
in a production system you know you

00:32:49,950 --> 00:32:53,009
thought your performance was this but if

00:32:51,509 --> 00:32:54,419
you just leave it alone and don't do

00:32:53,009 --> 00:32:56,279
anything it's slowly going to degrade

00:32:54,419 --> 00:32:56,759
right oh that's not a good situation to

00:32:56,279 --> 00:32:59,340
be in

00:32:56,759 --> 00:33:02,039
so so you know you really need if you do

00:32:59,340 --> 00:33:04,919
this kind of circular kind of indexing

00:33:02,039 --> 00:33:06,269
of you know dynamic indexing you have to

00:33:04,919 --> 00:33:08,370
have an automated process that's

00:33:06,269 --> 00:33:10,080
continually updating it and I think the

00:33:08,370 --> 00:33:13,139
the faster that loop works the better

00:33:10,080 --> 00:33:15,779
off you are because you know we reach

00:33:13,139 --> 00:33:17,399
patterns change very quickly new product

00:33:15,779 --> 00:33:20,009
drops suddenly there all these queries

00:33:17,399 --> 00:33:22,799
for a thing you never saw before anyway

00:33:20,009 --> 00:33:24,570
yeah that's kind of a oh yeah another

00:33:22,799 --> 00:33:26,129
optimization this is this is cool I mean

00:33:24,570 --> 00:33:28,350
Mike talked a little bit about this but

00:33:26,129 --> 00:33:30,600
just to elaborate so lightning deals

00:33:28,350 --> 00:33:33,720
with what are they basically we want to

00:33:30,600 --> 00:33:36,869
sell you know at a discount on for two

00:33:33,720 --> 00:33:38,580
hours you know buy it now get it for 15%

00:33:36,869 --> 00:33:39,720
less or whatever so they have all these

00:33:38,580 --> 00:33:40,980
deals

00:33:39,720 --> 00:33:42,990
but searching for those deals

00:33:40,980 --> 00:33:44,880
traditionally was very expensive they

00:33:42,990 --> 00:33:46,500
were implemented as a post filter you'd

00:33:44,880 --> 00:33:48,600
find all the matching documents and then

00:33:46,500 --> 00:33:50,280
after the fact you'd search them all to

00:33:48,600 --> 00:33:51,330
see if they were if you know they

00:33:50,280 --> 00:33:54,929
matched your deal

00:33:51,330 --> 00:33:57,210
well we've replaced that using the

00:33:54,929 --> 00:33:59,400
dimensional points and leucine we saw it

00:33:57,210 --> 00:34:01,770
very nice speed up the funny thing about

00:33:59,400 --> 00:34:03,900
these dimensional points as as you know

00:34:01,770 --> 00:34:05,700
maybe or as Mike said before they they

00:34:03,900 --> 00:34:07,409
were originally designed for geo queries

00:34:05,700 --> 00:34:10,440
so like for example in that region there

00:34:07,409 --> 00:34:13,070
you see a red you know region the query

00:34:10,440 --> 00:34:15,510
might find anything in that region by

00:34:13,070 --> 00:34:17,899
essentially breaking it up into little

00:34:15,510 --> 00:34:20,639
rectangular sub regions and then

00:34:17,899 --> 00:34:22,530
subdividing them to match to match the

00:34:20,639 --> 00:34:25,679
boundary using this tree like structure

00:34:22,530 --> 00:34:27,389
in multiple dimensions to here but you

00:34:25,679 --> 00:34:29,369
can use it for other kinds of data so

00:34:27,389 --> 00:34:31,590
for this example the Lightning Deals we

00:34:29,369 --> 00:34:34,250
have really three dimensions we've got

00:34:31,590 --> 00:34:36,419
the start time the end time and an

00:34:34,250 --> 00:34:39,599
identifier you know which is just which

00:34:36,419 --> 00:34:41,010
deal is it and we actually like the

00:34:39,599 --> 00:34:42,419
weird idea was well that identifier

00:34:41,010 --> 00:34:43,980
isn't a number but we can throw it in

00:34:42,419 --> 00:34:45,780
there you know and it doesn't have any

00:34:43,980 --> 00:34:47,339
ordering properties but it's still a

00:34:45,780 --> 00:34:50,550
dimension and it actually works out

00:34:47,339 --> 00:34:52,409
great so we're hoping that we see that

00:34:50,550 --> 00:34:55,200
we you know this saves us from a prime

00:34:52,409 --> 00:34:58,380
day disaster no it's we've already seen

00:34:55,200 --> 00:35:00,900
good speed-up so that'll be good okay

00:34:58,380 --> 00:35:03,750
Wow am i doing it in time forty minutes

00:35:00,900 --> 00:35:06,480
in I want to save some time for

00:35:03,750 --> 00:35:07,859
questions six minutes left so I'm

00:35:06,480 --> 00:35:09,780
probably not going to get all the way

00:35:07,859 --> 00:35:11,490
into this last thing because it's kind

00:35:09,780 --> 00:35:14,670
of complicated but I'll give it just a

00:35:11,490 --> 00:35:16,980
gloss on it basically the story here is

00:35:14,670 --> 00:35:19,770
we have some very expensive machine

00:35:16,980 --> 00:35:21,750
learned models that we want to apply but

00:35:19,770 --> 00:35:23,849
you know we also have a lot of documents

00:35:21,750 --> 00:35:25,830
we can't score those those models across

00:35:23,849 --> 00:35:27,750
all our documents we have these tight

00:35:25,830 --> 00:35:29,460
latency guarantees so how do we do it

00:35:27,750 --> 00:35:33,210
basically we do multi-phase ranking

00:35:29,460 --> 00:35:35,280
first phase let's see what did I this

00:35:33,210 --> 00:35:36,990
isn't really what the slide says but why

00:35:35,280 --> 00:35:39,480
are they expensive well basically they

00:35:36,990 --> 00:35:41,940
just do a lot of stuff a one funny story

00:35:39,480 --> 00:35:44,010
is we initially tried to model these

00:35:41,940 --> 00:35:45,510
these scoring models as leucine

00:35:44,010 --> 00:35:46,980
expressions I mean a lot of people do

00:35:45,510 --> 00:35:49,260
that right you take you take your

00:35:46,980 --> 00:35:50,820
recency time and your relevance score

00:35:49,260 --> 00:35:52,619
from the text and you kind of fold them

00:35:50,820 --> 00:35:53,670
together into a function and leucine

00:35:52,619 --> 00:35:55,530
will pretty efficiently come

00:35:53,670 --> 00:35:57,599
shoot that by compiling you know this

00:35:55,530 --> 00:35:59,790
javascript looking expression into java

00:35:57,599 --> 00:36:02,549
and then it's very fast but we

00:35:59,790 --> 00:36:05,190
discovered that you can your expression

00:36:02,549 --> 00:36:07,589
has a limit a function a Java function

00:36:05,190 --> 00:36:10,680
cannot have more than 64,000 expressions

00:36:07,589 --> 00:36:12,900
in it so you know otherwise it runs into

00:36:10,680 --> 00:36:14,609
JVM limits and these are really big

00:36:12,900 --> 00:36:16,200
these things so it just wasn't it just

00:36:14,609 --> 00:36:17,670
wasn't working you know so we had to

00:36:16,200 --> 00:36:19,440
write custom code for that part of it

00:36:17,670 --> 00:36:21,750
but but anyway it just gives illustrates

00:36:19,440 --> 00:36:23,040
that they're kind of expensive so what

00:36:21,750 --> 00:36:24,750
do we do is we do this multi-phase

00:36:23,040 --> 00:36:28,200
ranking thing the first phase we do

00:36:24,750 --> 00:36:30,660
index you know static rank that's that's

00:36:28,200 --> 00:36:32,490
done at indexing time we just pick the

00:36:30,660 --> 00:36:34,589
top end documents from that and we say

00:36:32,490 --> 00:36:37,020
well we'll just rescore those with our

00:36:34,589 --> 00:36:38,640
less or more expensive model we actually

00:36:37,020 --> 00:36:42,920
do two phases of that we have a kind of

00:36:38,640 --> 00:36:46,589
intermediate expense model and I think

00:36:42,920 --> 00:36:48,569
that in the interests of saving time for

00:36:46,589 --> 00:36:50,250
questions I'm not gonna tell you about

00:36:48,569 --> 00:36:51,809
this really cool thing that we did but

00:36:50,250 --> 00:36:53,910
I'll just I'll just leave you with a

00:36:51,809 --> 00:36:55,950
problem to think about you may remember

00:36:53,910 --> 00:36:58,020
and this illustrates a little bit when

00:36:55,950 --> 00:36:59,819
Mike talked about concurrent searching

00:36:58,020 --> 00:37:02,160
across multiple segments before that he

00:36:59,819 --> 00:37:03,750
showed that picture which may not be in

00:37:02,160 --> 00:37:05,309
your head anymore but there were you

00:37:03,750 --> 00:37:06,869
know how do we collect all these

00:37:05,309 --> 00:37:08,849
documents across all the segments then

00:37:06,869 --> 00:37:10,859
we merge sort them right well when we

00:37:08,849 --> 00:37:12,359
did that we collected let's say we want

00:37:10,859 --> 00:37:14,369
an end documents we collected n

00:37:12,359 --> 00:37:17,040
documents per segment and then we threw

00:37:14,369 --> 00:37:18,780
most of them away well we probably

00:37:17,040 --> 00:37:20,339
didn't need to do that I mean most of

00:37:18,780 --> 00:37:21,900
them came out of the big segments fewer

00:37:20,339 --> 00:37:23,849
of them came out of the small segments

00:37:21,900 --> 00:37:26,510
so anyway we did some stuff to prorate

00:37:23,849 --> 00:37:28,799
that and got a little speed-up

00:37:26,510 --> 00:37:31,170
so that was kind of a whirlwind tour

00:37:28,799 --> 00:37:32,819
this is more about showing you how the

00:37:31,170 --> 00:37:36,240
collection works when it's proportional

00:37:32,819 --> 00:37:37,980
and you know you run the risk of not

00:37:36,240 --> 00:37:39,390
getting quite the same results you would

00:37:37,980 --> 00:37:41,069
have done otherwise but you've got a

00:37:39,390 --> 00:37:44,369
speed-up from it so that was a kind of a

00:37:41,069 --> 00:37:46,920
neat thing yeah I think I'll leave that

00:37:44,369 --> 00:37:52,319
there and ask Mike to come up and wrap

00:37:46,920 --> 00:37:54,089
us up ok real quick so yeah it turns out

00:37:52,319 --> 00:37:55,500
Lucene can handle this search

00:37:54,089 --> 00:37:57,359
application it's been really challenging

00:37:55,500 --> 00:37:58,980
we're using all kinds of features we

00:37:57,359 --> 00:38:00,390
push changes back upstream but it's

00:37:58,980 --> 00:38:02,700
working really well if you go and search

00:38:00,390 --> 00:38:04,319
on amazon.com today you're using Lu

00:38:02,700 --> 00:38:05,740
seems sometimes it's not a hundred

00:38:04,319 --> 00:38:07,760
percent out there yet

00:38:05,740 --> 00:38:09,170
segment replication is incredibly

00:38:07,760 --> 00:38:11,569
efficient if you have deep replicas

00:38:09,170 --> 00:38:13,490
count so that's something I hope someday

00:38:11,569 --> 00:38:16,160
elasticsearch will offer it as an option

00:38:13,490 --> 00:38:18,829
if you have really deep replicas it

00:38:16,160 --> 00:38:20,420
would be a big win we're using multiple

00:38:18,829 --> 00:38:22,039
threads to handle one query is

00:38:20,420 --> 00:38:23,660
incredibly incredibly important if you

00:38:22,039 --> 00:38:27,109
care about latency and you're not always

00:38:23,660 --> 00:38:28,130
running at redline and come join us if

00:38:27,109 --> 00:38:30,319
you like working on open source software

00:38:28,130 --> 00:38:45,289
on really challenging high scale search

00:38:30,319 --> 00:38:47,030
problems we are hiring so unfortunately

00:38:45,289 --> 00:38:49,130
we probably only have time for one

00:38:47,030 --> 00:38:51,289
question and this was an amazing talk so

00:38:49,130 --> 00:38:53,210
show of hands who wants to ask a

00:38:51,289 --> 00:39:00,799
question and Mike and Mike will pick

00:38:53,210 --> 00:39:02,089
from the audience one two okay you can

00:39:00,799 --> 00:39:04,339
come to our booth we have a booth

00:39:02,089 --> 00:39:06,200
downstairs so if you want to ask us more

00:39:04,339 --> 00:39:09,020
challenging questions come and ask us

00:39:06,200 --> 00:39:11,059
thank you for the talk I just have one

00:39:09,020 --> 00:39:13,880
question related to synonyms handling so

00:39:11,059 --> 00:39:16,279
you mentioned you do synonyms during

00:39:13,880 --> 00:39:18,799
indexing time do you think you are

00:39:16,279 --> 00:39:21,920
missing the context of the user or and

00:39:18,799 --> 00:39:24,260
the user query or do you also like a

00:39:21,920 --> 00:39:31,520
different type of synonyms during query

00:39:24,260 --> 00:39:33,380
time so yeah probably I think it's done

00:39:31,520 --> 00:39:35,270
in the interest of efficiency for the

00:39:33,380 --> 00:39:36,770
most part you know although this this

00:39:35,270 --> 00:39:38,329
document context is something we

00:39:36,770 --> 00:39:39,770
wouldn't have a query time so there's a

00:39:38,329 --> 00:39:42,319
trade-off I think there's a there's a

00:39:39,770 --> 00:39:44,029
role for both there are other processes

00:39:42,319 --> 00:39:45,500
that we didn't talk about which run kind

00:39:44,029 --> 00:39:47,569
of prior to the search engine as well

00:39:45,500 --> 00:39:49,250
and they do some of that work too

00:39:47,569 --> 00:39:51,079
there's some kind of query rewriting

00:39:49,250 --> 00:39:57,619
that happens before we see it but but I

00:39:51,079 --> 00:40:00,109
think there's room for more yeah okay in

00:39:57,619 --> 00:40:07,670
theory we have one minute left so any

00:40:00,109 --> 00:40:10,880
other question Tuesday thanks for a nice

00:40:07,670 --> 00:40:13,309
talk so you said it leucine powers most

00:40:10,880 --> 00:40:14,839
of the searches what else what else do

00:40:13,309 --> 00:40:17,150
used for powering the rest on searches

00:40:14,839 --> 00:40:18,650
well so so far

00:40:17,150 --> 00:40:19,460
Lusine is used all over the place the

00:40:18,650 --> 00:40:21,230
Amazon for

00:40:19,460 --> 00:40:23,450
situations that aren't product search

00:40:21,230 --> 00:40:24,950
busine elasticsearch and solar so we're

00:40:23,450 --> 00:40:27,740
just talking about in the product search

00:40:24,950 --> 00:40:29,090
situation Amazon started with an

00:40:27,740 --> 00:40:31,430
in-house search engine they developed

00:40:29,090 --> 00:40:33,170
years ago it's highly tuned to what

00:40:31,430 --> 00:40:35,120
Amazon customers need for search and it

00:40:33,170 --> 00:40:37,520
was highly challenging to replicate its

00:40:35,120 --> 00:40:40,660
behavior on top of leucine so that's

00:40:37,520 --> 00:40:40,660

YouTube URL: https://www.youtube.com/watch?v=EkkzSLstSAE


