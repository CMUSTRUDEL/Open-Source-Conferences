Title: Berlin Buzzwords 2019: David Moravekâ€“Apache Beam pipelines at 100TB+ scale using Apache Spark #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.

This talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.

Read more:
https://2019.berlinbuzzwords.de/19/session/apache-beam-pipelines-100tb-scale-using-apache-spark

About David Moravek:
https://2019.berlinbuzzwords.de/users/david-moravek

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,080 --> 00:00:11,560
hi I'm David I'm lead engineer at says

00:00:10,620 --> 00:00:12,930
Nam

00:00:11,560 --> 00:00:16,190
does anyone know

00:00:12,930 --> 00:00:22,020
the company except the guys from system

00:00:16,190 --> 00:00:26,160
No okay so says nom is a search engine

00:00:22,020 --> 00:00:28,680
which is local to Czech Republic like

00:00:26,160 --> 00:00:31,230
for those who are not familiar with the

00:00:28,680 --> 00:00:33,150
search engine market like it's pretty

00:00:31,230 --> 00:00:35,040
much dominated by Google all over the

00:00:33,150 --> 00:00:37,710
world and there are very few countries

00:00:35,040 --> 00:00:39,540
where there is actually some competition

00:00:37,710 --> 00:00:42,540
and the Czech Republic is one of them

00:00:39,540 --> 00:00:44,450
and we're very proud to that profit of

00:00:42,540 --> 00:00:48,149
that

00:00:44,450 --> 00:00:52,260
thanks we still maintain like 30 percent

00:00:48,149 --> 00:00:55,399
market share which is pretty cool okay

00:00:52,260 --> 00:00:57,989
so as I said I work on a search engine

00:00:55,399 --> 00:01:01,290
as well like especially on the crawler

00:00:57,989 --> 00:01:05,430
part and the most important thing that

00:01:01,290 --> 00:01:10,619
crawler needs is to process like

00:01:05,430 --> 00:01:13,170
incredible amount of data like we yeah

00:01:10,619 --> 00:01:16,470
we just need to like score them select

00:01:13,170 --> 00:01:19,140
all of the like most relevant documents

00:01:16,470 --> 00:01:24,690
to the user and we just send it to index

00:01:19,140 --> 00:01:28,020
and that's where our job ends ok so

00:01:24,690 --> 00:01:32,250
here's some timeline we started using

00:01:28,020 --> 00:01:35,310
MapReduce in 2010 and we managed to get

00:01:32,250 --> 00:01:39,020
on pretty decent scale we have like 40

00:01:35,310 --> 00:01:43,350
billions rows in database which is HBase

00:01:39,020 --> 00:01:48,470
it has like about four hundred terabytes

00:01:43,350 --> 00:01:48,470
without replicas so it's like pretty big

00:01:48,500 --> 00:01:57,630
we download 300 million webpages every

00:01:53,790 --> 00:02:01,920
single day the Iran on bare metal

00:01:57,630 --> 00:02:05,310
servers over to data centers we have

00:02:01,920 --> 00:02:08,429
like over 13 petabytes of storage and

00:02:05,310 --> 00:02:10,410
over 50 terabytes of memory to do the

00:02:08,429 --> 00:02:17,700
computation on so it's pretty decent

00:02:10,410 --> 00:02:20,600
scale and it comes with a price so we

00:02:17,700 --> 00:02:23,519
were able to use MapReduce for years and

00:02:20,600 --> 00:02:26,580
we found out that it's really slow and

00:02:23,519 --> 00:02:29,190
expensive it's in bold

00:02:26,580 --> 00:02:31,920
enough terminals of like development

00:02:29,190 --> 00:02:35,730
price because it's really inflexible if

00:02:31,920 --> 00:02:40,110
you want to express what you really want

00:02:35,730 --> 00:02:42,810
to do and if you have like more complex

00:02:40,110 --> 00:02:45,240
business logic you need some like job

00:02:42,810 --> 00:02:46,770
chain of MapReduce jobs and like between

00:02:45,240 --> 00:02:49,890
each jobs you need to write everything

00:02:46,770 --> 00:02:51,810
on HDFS you need to replicate it three

00:02:49,890 --> 00:02:55,910
times you need to send it over the

00:02:51,810 --> 00:02:59,550
network and it's really really slow so

00:02:55,910 --> 00:03:03,030
we were seeking for a replacement so in

00:02:59,550 --> 00:03:09,600
2014 we started a project that we called

00:03:03,030 --> 00:03:13,140
euphoria API which was dislike engine

00:03:09,600 --> 00:03:15,980
independent programming model that could

00:03:13,140 --> 00:03:18,770
do both batch and streaming pipelines

00:03:15,980 --> 00:03:22,320
because like we had to maintain

00:03:18,770 --> 00:03:26,400
basically two code bases one for the

00:03:22,320 --> 00:03:29,130
like slow slow path which had really

00:03:26,400 --> 00:03:32,190
really high throughput and one for low

00:03:29,130 --> 00:03:35,730
latency one so we kind of wanted to

00:03:32,190 --> 00:03:41,430
write all the business logic just once

00:03:35,730 --> 00:03:44,520
and run it on both batch and stream so

00:03:41,430 --> 00:03:47,880
it was basically a Java SDK that then

00:03:44,520 --> 00:03:52,110
translated to either spark or fling does

00:03:47,880 --> 00:03:54,420
this sound familiar to anyone it should

00:03:52,110 --> 00:03:57,570
so in 2016

00:03:54,420 --> 00:04:01,080
Apache Bean was open sourced and you are

00:03:57,570 --> 00:04:05,490
like okay maybe they do very similar

00:04:01,080 --> 00:04:08,250
thing as we're trying to do so it's

00:04:05,490 --> 00:04:12,950
basically the same as I described except

00:04:08,250 --> 00:04:17,100
there they went a little bit further

00:04:12,950 --> 00:04:20,790
like they want you to be able to write

00:04:17,100 --> 00:04:23,550
your pipelines in any SDK in any

00:04:20,790 --> 00:04:25,470
language that you can imagine and then

00:04:23,550 --> 00:04:27,540
it just translates to the B model and

00:04:25,470 --> 00:04:30,300
you can run it anywhere you can run it

00:04:27,540 --> 00:04:33,930
on spark you can run it on flame on data

00:04:30,300 --> 00:04:36,630
flow and there are so many emerging

00:04:33,930 --> 00:04:39,380
runners right now that you can you'll be

00:04:36,630 --> 00:04:43,320
able to use in near future

00:04:39,380 --> 00:04:45,780
so if you're interested in like running

00:04:43,320 --> 00:04:49,320
pipelines in any language there would be

00:04:45,780 --> 00:04:50,010
really great talk tomorrow by Max and

00:04:49,320 --> 00:04:53,190
Ismail

00:04:50,010 --> 00:04:55,080
it's called peyten Java or go it's your

00:04:53,190 --> 00:04:58,380
choice but Apache being so you guys

00:04:55,080 --> 00:05:04,650
should definitely go there I'll

00:04:58,380 --> 00:05:06,570
definitely go there okay so then we were

00:05:04,650 --> 00:05:09,660
kind of like struggling what to do next

00:05:06,570 --> 00:05:12,480
and we finally realize it doesn't make

00:05:09,660 --> 00:05:14,940
any sense to maintain our project

00:05:12,480 --> 00:05:18,690
anymore because like we needed to write

00:05:14,940 --> 00:05:20,430
our own runners and it had like really

00:05:18,690 --> 00:05:25,650
large development cost for the company

00:05:20,430 --> 00:05:27,720
so in 2018 we managed to merge euphoria

00:05:25,650 --> 00:05:31,500
into being there is a JIRA issue if

00:05:27,720 --> 00:05:33,300
anyone is interested and we have like

00:05:31,500 --> 00:05:38,700
really great documentation on the bean

00:05:33,300 --> 00:05:41,190
website and like if anyone wants to

00:05:38,700 --> 00:05:42,810
contribute to being it's like really

00:05:41,190 --> 00:05:46,830
great community and they are very

00:05:42,810 --> 00:05:54,210
welcoming so it's really easy to start

00:05:46,830 --> 00:05:59,340
with it okay so our largest pipeline

00:05:54,210 --> 00:06:01,169
that we have is basically doing that

00:05:59,340 --> 00:06:04,530
it's turning the internet upside down

00:06:01,169 --> 00:06:06,900
because like anytime you download a new

00:06:04,530 --> 00:06:10,230
document document in terms of search

00:06:06,900 --> 00:06:12,780
engine is it can be either like HTML

00:06:10,230 --> 00:06:16,620
page it can be an image it can be a

00:06:12,780 --> 00:06:20,490
video video PDF like many different

00:06:16,620 --> 00:06:22,919
things and you only see like the links

00:06:20,490 --> 00:06:25,880
that are like going forward from the

00:06:22,919 --> 00:06:29,820
document so you just like you know only

00:06:25,880 --> 00:06:31,669
forward Network and what you read the

00:06:29,820 --> 00:06:34,680
question you really want to answer is

00:06:31,669 --> 00:06:37,260
what are the documents that I that are

00:06:34,680 --> 00:06:39,330
pointing and to me and what I know about

00:06:37,260 --> 00:06:41,070
them like for example if they are all in

00:06:39,330 --> 00:06:44,580
English there is a really high

00:06:41,070 --> 00:06:47,870
probability that that the document it's

00:06:44,580 --> 00:06:50,640
pointing to is also in English or if all

00:06:47,870 --> 00:06:53,129
the documents pointing at it are link

00:06:50,640 --> 00:06:57,209
farms it's really high probability

00:06:53,129 --> 00:07:00,209
that it would be a span so like we

00:06:57,209 --> 00:07:02,339
basically use this to like calculate

00:07:00,209 --> 00:07:06,169
some really really good signals that can

00:07:02,339 --> 00:07:10,050
be then used by machine learning models

00:07:06,169 --> 00:07:13,769
for like search relevance so it's really

00:07:10,050 --> 00:07:16,789
important and these jobs this pipeline

00:07:13,769 --> 00:07:20,429
it runs incremental every single day and

00:07:16,789 --> 00:07:23,339
the inputs it needs to process it ranges

00:07:20,429 --> 00:07:27,869
from 50 terabytes to like hundred

00:07:23,339 --> 00:07:30,409
terabytes each day so it we actually

00:07:27,869 --> 00:07:33,330
tried to run it on spark and we

00:07:30,409 --> 00:07:36,330
struggled a lot for like two years so

00:07:33,330 --> 00:07:38,159
this is what we're going to go through

00:07:36,330 --> 00:07:43,529
so you guys don't make the same mistake

00:07:38,159 --> 00:07:46,469
as we did okay so the biggest issue we

00:07:43,529 --> 00:07:50,939
encountered was the exponential data

00:07:46,469 --> 00:07:54,809
skew is anyone familiar with it there

00:07:50,939 --> 00:07:58,289
are a few people okay so an example

00:07:54,809 --> 00:08:01,169
would be joining documents that's what

00:07:58,289 --> 00:08:03,959
we already talked about and it would be

00:08:01,169 --> 00:08:10,019
like joining it with some metadata about

00:08:03,959 --> 00:08:11,819
domains that it's located on so if you

00:08:10,019 --> 00:08:15,360
look at the distribution how it looks

00:08:11,819 --> 00:08:19,159
like probably most of the websites would

00:08:15,360 --> 00:08:22,469
have like around 100 documents on it

00:08:19,159 --> 00:08:27,019
those are like mostly like blocks and

00:08:22,469 --> 00:08:30,899
those kind of small websites and then

00:08:27,019 --> 00:08:33,029
there is for example YouTube which has

00:08:30,899 --> 00:08:36,569
like hundreds of millions of documents

00:08:33,029 --> 00:08:41,250
and if you want to join it to a single

00:08:36,569 --> 00:08:44,790
key you'll struggle a lot so what are

00:08:41,250 --> 00:08:49,050
the solutions yeah this is what will

00:08:44,790 --> 00:08:52,410
happen on spark you can see that like

00:08:49,050 --> 00:08:55,350
most of the splits would finish in like

00:08:52,410 --> 00:08:57,660
five minutes and then there would be one

00:08:55,350 --> 00:09:00,959
split which would finish in 30 minutes

00:08:57,660 --> 00:09:03,480
so it would like really make your job

00:09:00,959 --> 00:09:06,149
competition very longer than it needs to

00:09:03,480 --> 00:09:07,110
be so what we want is to evenly

00:09:06,149 --> 00:09:11,510
distribute

00:09:07,110 --> 00:09:16,260
data data amongst plates so one solution

00:09:11,510 --> 00:09:19,230
would be not to shuffle it at all so if

00:09:16,260 --> 00:09:22,590
in does is anyone familiar with maps I

00:09:19,230 --> 00:09:27,570
join like three four people

00:09:22,590 --> 00:09:29,580
okay so Maps I joined basically if you

00:09:27,570 --> 00:09:33,450
want to join two sides left and right

00:09:29,580 --> 00:09:36,360
and if one side fits in memory you can

00:09:33,450 --> 00:09:38,610
just take it you can broadcast it to all

00:09:36,360 --> 00:09:40,470
the executors and you can just like map

00:09:38,610 --> 00:09:43,290
through the left side and just do in

00:09:40,470 --> 00:09:46,070
memory lookup to the hash table that you

00:09:43,290 --> 00:09:50,340
have in memory so you just collect it

00:09:46,070 --> 00:09:52,890
broadcast it and then when you're going

00:09:50,340 --> 00:09:56,070
through like single elements you just

00:09:52,890 --> 00:09:58,860
look it up and you have your result and

00:09:56,070 --> 00:10:01,190
you didn't have to send like hundreds of

00:09:58,860 --> 00:10:05,220
terabytes over the network just to do

00:10:01,190 --> 00:10:09,660
join with like 10 megabyte data set but

00:10:05,220 --> 00:10:12,660
this is usually not the case so another

00:10:09,660 --> 00:10:14,760
solution would be you could split the

00:10:12,660 --> 00:10:17,970
large keys for example if you have

00:10:14,760 --> 00:10:20,970
youtube you can just like copy the key

00:10:17,970 --> 00:10:23,730
which is on left side you can partition

00:10:20,970 --> 00:10:29,640
it and then you can evenly distribute

00:10:23,730 --> 00:10:32,640
the data among those partitions this is

00:10:29,640 --> 00:10:36,300
heart if you have an exponential data SQ

00:10:32,640 --> 00:10:39,720
because because you have to like

00:10:36,300 --> 00:10:41,550
calculate first like vitamin domains you

00:10:39,720 --> 00:10:45,590
want to split and how much you want to

00:10:41,550 --> 00:10:49,650
split them but it's definitely worth it

00:10:45,590 --> 00:10:51,750
ok so another huge issue with SPARC all

00:10:49,650 --> 00:10:55,800
values for a single key must fit in

00:10:51,750 --> 00:10:58,800
memory who is familiar with the

00:10:55,800 --> 00:11:05,190
difference of guru by key and reduce by

00:10:58,800 --> 00:11:07,260
key calls in spark ok ok so I hope that

00:11:05,190 --> 00:11:10,200
everyone is familiar with MapReduce

00:11:07,260 --> 00:11:14,940
right so if you want to do a word count

00:11:10,200 --> 00:11:19,170
for example you just like need to send

00:11:14,940 --> 00:11:20,790
all the same words with a value 1 over

00:11:19,170 --> 00:11:24,270
the network and if you use

00:11:20,790 --> 00:11:26,550
in memory combiner you can combine it

00:11:24,270 --> 00:11:28,620
mops eye on mop site and then you can

00:11:26,550 --> 00:11:32,160
just like send the combined residual

00:11:28,620 --> 00:11:34,830
results of the reducer and you saved a

00:11:32,160 --> 00:11:38,730
lot of time a lot of network traffic and

00:11:34,830 --> 00:11:41,520
a lot of iOS this is basically the

00:11:38,730 --> 00:11:43,110
difference between reduce by kyun grew

00:11:41,520 --> 00:11:45,360
by key Reggie's bike he just takes

00:11:43,110 --> 00:11:49,410
combiner and it does in memory combined

00:11:45,360 --> 00:11:51,840
but the issue is that grew by key is

00:11:49,410 --> 00:11:55,470
implemented using reduce by key which

00:11:51,840 --> 00:11:58,290
means you get like list combiner and any

00:11:55,470 --> 00:11:59,610
value that goes to group by key you just

00:11:58,290 --> 00:12:01,920
add it to the list

00:11:59,610 --> 00:12:04,470
so it means like at the end of the day

00:12:01,920 --> 00:12:08,610
you'll just need to load the whole list

00:12:04,470 --> 00:12:12,960
in memory at once which is obviously

00:12:08,610 --> 00:12:15,380
problems sometimes okay so there is a

00:12:12,960 --> 00:12:18,020
little quiz about group by key does

00:12:15,380 --> 00:12:20,670
anyone know what's wrong with this code

00:12:18,020 --> 00:12:33,120
because like we really struggled with

00:12:20,670 --> 00:12:35,910
this okay so so what happens if you take

00:12:33,120 --> 00:12:40,980
a byte array and new shop and you use it

00:12:35,910 --> 00:12:43,470
as a shuffle key like in Java it just

00:12:40,980 --> 00:12:46,710
like defaults to object hash code so it

00:12:43,470 --> 00:12:48,930
will be completely random so you don't

00:12:46,710 --> 00:12:52,080
have any guarantees where the key will

00:12:48,930 --> 00:12:54,330
end up so this is something to be aware

00:12:52,080 --> 00:12:58,430
of also if you're using composite keys

00:12:54,330 --> 00:13:01,920
that contains a enum is the same thing

00:12:58,430 --> 00:13:05,880
should be really careful about that okay

00:13:01,920 --> 00:13:09,030
so can we do more efficient of course we

00:13:05,880 --> 00:13:11,520
can we can just like repartition and

00:13:09,030 --> 00:13:14,850
sort everything it's kind of like going

00:13:11,520 --> 00:13:18,090
back to the MapReduce but it works so

00:13:14,850 --> 00:13:21,810
you just sort everything by key and you

00:13:18,090 --> 00:13:24,660
just go key by key and only think you

00:13:21,810 --> 00:13:27,600
need to load in memory is a single key

00:13:24,660 --> 00:13:29,370
at a time which is perfect so now

00:13:27,600 --> 00:13:33,830
everything should be okay right

00:13:29,370 --> 00:13:33,830
and we don't have any more problems

00:13:34,509 --> 00:13:40,910
yeah then we get exception like this you

00:13:39,439 --> 00:13:44,240
can see that like right now the

00:13:40,910 --> 00:13:48,680
distribution of data is pretty even but

00:13:44,240 --> 00:13:52,429
there is still like one one task that

00:13:48,680 --> 00:13:55,339
completed in 60 minutes which is like

00:13:52,429 --> 00:14:01,209
order of magnitude worse than the rest

00:13:55,339 --> 00:14:04,939
and we can also see there is something

00:14:01,209 --> 00:14:07,249
called Shaffer read blocked time and we

00:14:04,939 --> 00:14:10,389
we can see that shuffle was blocked for

00:14:07,249 --> 00:14:14,540
20 minutes this is like one of the

00:14:10,389 --> 00:14:16,850
biggest scaling issues with spark there

00:14:14,540 --> 00:14:19,879
is great paper from Facebook

00:14:16,850 --> 00:14:22,399
it's called riffle optimized shuffle

00:14:19,879 --> 00:14:26,209
service for large-scale data analytics

00:14:22,399 --> 00:14:29,300
and it basically says that within with

00:14:26,209 --> 00:14:31,999
an increasing number of tasks shuffle

00:14:29,300 --> 00:14:36,350
time and i/o requests you need to do

00:14:31,999 --> 00:14:39,470
they grow quadratically and the size of

00:14:36,350 --> 00:14:42,079
Io requests they decrease quadratically

00:14:39,470 --> 00:14:46,279
it basically means that like every task

00:14:42,079 --> 00:14:49,040
that completes just ends up with one one

00:14:46,279 --> 00:14:52,370
shuffle file on disk so if you have like

00:14:49,040 --> 00:14:55,009
I don't know 100 thousand tasks you will

00:14:52,370 --> 00:14:57,259
end up with a hundred thousands shuffle

00:14:55,009 --> 00:15:01,819
files and you you are doing a lot of

00:14:57,259 --> 00:15:05,389
random i/o seeks and most of the expired

00:15:01,819 --> 00:15:08,689
clusters are just like on the yarn and

00:15:05,389 --> 00:15:11,059
on hdds so it's not really random access

00:15:08,689 --> 00:15:12,410
friendly and it will just kill the

00:15:11,059 --> 00:15:16,579
performance of the whole cluster

00:15:12,410 --> 00:15:18,199
completely and it also has really bad

00:15:16,579 --> 00:15:22,730
impact on other jobs that are running

00:15:18,199 --> 00:15:28,069
there so what you can do you can kind of

00:15:22,730 --> 00:15:29,899
decrease the number of map tasks but how

00:15:28,069 --> 00:15:33,799
you do it you just like have larger

00:15:29,899 --> 00:15:35,839
spreads for a single task which means it

00:15:33,799 --> 00:15:39,619
will run but it will run way slower

00:15:35,839 --> 00:15:42,620
because you'll need to do like lot of

00:15:39,619 --> 00:15:46,640
disk spells so it's a trade-off but it's

00:15:42,620 --> 00:15:48,950
runs I think like Facebook has

00:15:46,640 --> 00:15:51,830
some ongoing work on this and hopefully

00:15:48,950 --> 00:15:53,920
they will contribute it back to spark it

00:15:51,830 --> 00:15:59,050
would be really awesome because

00:15:53,920 --> 00:16:02,240
otherwise yeah it doesn't really scale

00:15:59,050 --> 00:16:05,000
then like one really huge performance

00:16:02,240 --> 00:16:07,790
gain we got from being was something

00:16:05,000 --> 00:16:11,000
called bite based shuffle because being

00:16:07,790 --> 00:16:14,090
kind of like have an abstract concept of

00:16:11,000 --> 00:16:17,870
serialization so only data that spark

00:16:14,090 --> 00:16:19,970
sees are actually bytes so like during

00:16:17,870 --> 00:16:22,340
shuffle spark just doesn't need to

00:16:19,970 --> 00:16:24,320
deserialize it or all the time and

00:16:22,340 --> 00:16:28,280
doesn't need to use any like custom

00:16:24,320 --> 00:16:32,480
comparators so this was for like really

00:16:28,280 --> 00:16:34,280
really huge performance game and what

00:16:32,480 --> 00:16:37,880
you can do if you want to debug spark

00:16:34,280 --> 00:16:40,610
pipelines there is this really great

00:16:37,880 --> 00:16:43,760
tool it's called Babar it's from

00:16:40,610 --> 00:16:47,450
righty-o I don't know much about the

00:16:43,760 --> 00:16:49,940
company but it gives you like all of the

00:16:47,450 --> 00:16:52,820
information about like containers that

00:16:49,940 --> 00:16:57,350
were allocated about memory your job was

00:16:52,820 --> 00:16:59,990
using about about CPU about garbage

00:16:57,350 --> 00:17:02,840
collection and the best thing it gives

00:16:59,990 --> 00:17:06,410
you it gives you a flame graph of your

00:17:02,840 --> 00:17:09,470
job this is really great tool if you

00:17:06,410 --> 00:17:12,020
want to profile it and if you were like

00:17:09,470 --> 00:17:15,860
trying to find out why the job

00:17:12,020 --> 00:17:18,560
competition takes so long for example we

00:17:15,860 --> 00:17:22,220
were able to find that there was like

00:17:18,560 --> 00:17:25,610
really nasty issue with combiners in

00:17:22,220 --> 00:17:27,709
being that like every single time you

00:17:25,610 --> 00:17:31,220
added something to combined accumulator

00:17:27,709 --> 00:17:33,710
you just need to serialize and

00:17:31,220 --> 00:17:36,440
deserialize it and it was really easy

00:17:33,710 --> 00:17:38,270
fix but it's really too hard to know

00:17:36,440 --> 00:17:40,550
that you have to fix it without like

00:17:38,270 --> 00:17:43,790
properly profiling the pipeline because

00:17:40,550 --> 00:17:48,500
this is something that tests just cannot

00:17:43,790 --> 00:17:53,960
catch and of course Hipp dumps are very

00:17:48,500 --> 00:18:00,380
useful so like right now we were able to

00:17:53,960 --> 00:18:02,810
run our like biggest pipeline we had on

00:18:00,380 --> 00:18:06,170
park runners so I would say it's

00:18:02,810 --> 00:18:08,840
production-ready right now for batch for

00:18:06,170 --> 00:18:13,730
streaming there are some ongoing works

00:18:08,840 --> 00:18:16,840
and if you're more interested in Apache

00:18:13,730 --> 00:18:20,150
beam there will be another great talk

00:18:16,840 --> 00:18:24,080
the next session it will be by Tomas is

00:18:20,150 --> 00:18:29,990
about streaming your share right so it's

00:18:24,080 --> 00:18:32,920
like and then there is a beam summit

00:18:29,990 --> 00:18:34,160
Europe happening this Wednesday and

00:18:32,920 --> 00:18:36,920
Thursday

00:18:34,160 --> 00:18:39,980
it's actually at the same place as

00:18:36,920 --> 00:18:42,410
barreling buzzwords so and it's free so

00:18:39,980 --> 00:18:46,730
you're free to like register and you

00:18:42,410 --> 00:18:50,720
will be very welcome to show up ok so

00:18:46,730 --> 00:18:52,610
that's all from me and if you if you'll

00:18:50,720 --> 00:18:56,910
have any questions I'll just be around

00:18:52,610 --> 00:19:01,539
and you can ask me anytime thank you

00:18:56,910 --> 00:19:01,539

YouTube URL: https://www.youtube.com/watch?v=rJIpva0tD0g


