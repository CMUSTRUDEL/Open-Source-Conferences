Title: Berlin Buzzwords 2019: Houston Putman – Running Solr within Kubernetes at Scale #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	The Search Infrastructure team at Bloomberg runs over a thousand Solr clouds spread across hundreds of machines. This scale creates significant challenges in managing hardware resources. Beyond just managing where clouds are allocated and the resources available across the cluster, tasks that affect multiple clouds, such as upgrading OS versions or taking a machine down for maintenance, can grow into serious undertakings.

Kubernetes is a system designed to help orchestrate large scale applications. However, it has room for improvement in use cases such as running on physical hardware or managing stateful applications. In this talk, Houston Putman will detail how the team has addressed these issues and begun to run production Solr clouds on Kubernetes. He will also share his experience and the performance characteristics when running Solr on Kubernetes vs. on bare metal.

Read more:
https://2019.berlinbuzzwords.de/19/session/running-solr-within-kubernetes-scale

About Houston Putman:
https://2019.berlinbuzzwords.de/users/houston-putman-0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,220 --> 00:00:12,960
hello everyone thanks for coming today

00:00:09,660 --> 00:00:14,340
as was mentioned I'm Houston I'm a

00:00:12,960 --> 00:00:15,930
software developer on the search

00:00:14,340 --> 00:00:17,910
infrastructure team at Bloomberg and

00:00:15,930 --> 00:00:21,570
here I'm in talk to you today about

00:00:17,910 --> 00:00:23,570
running solar on kubernetes um just kind

00:00:21,570 --> 00:00:27,210
of brief into a Bloomberg and what we do

00:00:23,570 --> 00:00:29,640
we're the largest private provider of

00:00:27,210 --> 00:00:34,440
news and business information around the

00:00:29,640 --> 00:00:38,390
world we power tools that help finance

00:00:34,440 --> 00:00:40,740
finance government and law professionals

00:00:38,390 --> 00:00:43,560
throughout the globe and at a very large

00:00:40,740 --> 00:00:45,600
scale having to deliver kind of high

00:00:43,560 --> 00:00:47,670
performance applications that scale and

00:00:45,600 --> 00:00:50,280
are reliable due to the customers that

00:00:47,670 --> 00:00:53,070
we try to support um this kind of leads

00:00:50,280 --> 00:00:55,050
into search infrastructure team we just

00:00:53,070 --> 00:00:56,760
manage a lot of search infrastructure at

00:00:55,050 --> 00:00:58,710
Bloomberg because of the amount of data

00:00:56,760 --> 00:01:02,370
and kind of the reliability that our

00:00:58,710 --> 00:01:04,799
customers kind of rely on I mean so

00:01:02,370 --> 00:01:07,650
because of the scale you can imagine

00:01:04,799 --> 00:01:10,679
that running all of these thousands of

00:01:07,650 --> 00:01:13,140
search engines would become kind of

00:01:10,679 --> 00:01:14,219
cumbersome and lead to a lot of monday

00:01:13,140 --> 00:01:16,710
and activities that you don't really

00:01:14,219 --> 00:01:18,569
want to have to deal with that kind of

00:01:16,710 --> 00:01:20,369
gets into our talk today we're gonna

00:01:18,569 --> 00:01:22,710
first kind of give some baseline

00:01:20,369 --> 00:01:26,090
knowledge about kind of managing solar

00:01:22,710 --> 00:01:28,079
clouds and a kubernetes introduction I'm

00:01:26,090 --> 00:01:30,060
sure that a lot of you all kind of

00:01:28,079 --> 00:01:32,310
understand how solar clouds are managed

00:01:30,060 --> 00:01:35,520
and kind of the basics of kubernetes but

00:01:32,310 --> 00:01:37,229
please kind of deal with the

00:01:35,520 --> 00:01:38,700
introductions because I want to get

00:01:37,229 --> 00:01:40,109
everyone on a baseline knowledge before

00:01:38,700 --> 00:01:42,929
I kind of talk about what we've built

00:01:40,109 --> 00:01:46,789
and the solar cloud operator that we've

00:01:42,929 --> 00:01:49,289
opened source that I'll end with cool so

00:01:46,789 --> 00:01:50,759
imagine managing solar clouds at

00:01:49,289 --> 00:01:53,130
Bloomberg we manage solar clouds at a

00:01:50,759 --> 00:01:56,279
very large scale we have thousands of

00:01:53,130 --> 00:01:58,549
search engines managing tens of billions

00:01:56,279 --> 00:02:01,350
fifties of billions of documents daily

00:01:58,549 --> 00:02:03,029
and so basically there are two different

00:02:01,350 --> 00:02:04,590
ways that we manage our search

00:02:03,029 --> 00:02:06,569
infrastructure and basically two

00:02:04,590 --> 00:02:08,550
different ways of managing solar clouds

00:02:06,569 --> 00:02:10,500
there's the physical management as well

00:02:08,550 --> 00:02:12,930
as the logical management physical

00:02:10,500 --> 00:02:14,640
management is where does data live what

00:02:12,930 --> 00:02:16,230
processes are running connected to

00:02:14,640 --> 00:02:19,680
zookeeper creating these solar clouds

00:02:16,230 --> 00:02:20,400
and the logical management is what is

00:02:19,680 --> 00:02:22,200
the data the

00:02:20,400 --> 00:02:25,260
in these other clouds how is it broken

00:02:22,200 --> 00:02:26,790
up what is the schema define these kind

00:02:25,260 --> 00:02:29,970
of things that you normally think about

00:02:26,790 --> 00:02:33,689
when using search as a kind of consumer

00:02:29,970 --> 00:02:36,060
instead of as a provider so first I'll

00:02:33,689 --> 00:02:37,909
kind of dive into what this physical

00:02:36,060 --> 00:02:40,140
topology that I mentioned before is

00:02:37,909 --> 00:02:42,329
basically this is your infrastructure

00:02:40,140 --> 00:02:45,000
this is your list of servers or virtual

00:02:42,329 --> 00:02:47,250
machines that are running a Java

00:02:45,000 --> 00:02:49,079
processes that are running solar cloud

00:02:47,250 --> 00:02:53,280
these little clouds are connected to

00:02:49,079 --> 00:02:55,889
zookeeper and they are storing solar

00:02:53,280 --> 00:03:00,840
cores which is just parts of the solar

00:02:55,889 --> 00:03:02,819
index that you're using to search next

00:03:00,840 --> 00:03:04,680
you have the logical topology this is

00:03:02,819 --> 00:03:07,470
the other side of how you manage the

00:03:04,680 --> 00:03:09,120
data here data is broken down into

00:03:07,470 --> 00:03:11,849
collections which is just a logical

00:03:09,120 --> 00:03:14,010
grouping of documents that have the same

00:03:11,849 --> 00:03:16,650
schema and that you can do common

00:03:14,010 --> 00:03:21,120
operations on these are further broken

00:03:16,650 --> 00:03:23,280
down by shards which are kind of logical

00:03:21,120 --> 00:03:26,699
splitting of the data so that it can be

00:03:23,280 --> 00:03:28,739
replicated and so solar shards can only

00:03:26,699 --> 00:03:30,840
store up to two billion documents so at

00:03:28,739 --> 00:03:32,519
some point you need to add shards to

00:03:30,840 --> 00:03:35,160
your collection in order to store kind

00:03:32,519 --> 00:03:37,530
of large amounts of data replicas are

00:03:35,160 --> 00:03:40,730
kind of the last level there and this is

00:03:37,530 --> 00:03:42,870
a copy of a shards data that is

00:03:40,730 --> 00:03:46,109
replicated as many times as you want

00:03:42,870 --> 00:03:50,040
just to add in resiliency to your solar

00:03:46,109 --> 00:03:51,900
cloud and this is basically how our

00:03:50,040 --> 00:03:54,150
logical and physical technologies meet

00:03:51,900 --> 00:03:57,959
they house solar cores which is just a

00:03:54,150 --> 00:04:00,120
copy of the index so as I mentioned

00:03:57,959 --> 00:04:02,280
before this our logical topology and so

00:04:00,120 --> 00:04:04,530
basically if we take away the clouds and

00:04:02,280 --> 00:04:07,079
the collections in shards we are left

00:04:04,530 --> 00:04:09,180
with just replicas we can shuffle these

00:04:07,079 --> 00:04:12,209
replicas around however we want and they

00:04:09,180 --> 00:04:14,569
just remain replicas once we bring up

00:04:12,209 --> 00:04:17,489
the physical topology layer you can see

00:04:14,569 --> 00:04:19,440
that the server and solar nodes view

00:04:17,489 --> 00:04:22,199
which is the physical topology view is

00:04:19,440 --> 00:04:25,620
basically just a reimagining of the same

00:04:22,199 --> 00:04:29,070
data so that it's seen in a different

00:04:25,620 --> 00:04:32,729
way so these men these topologies are

00:04:29,070 --> 00:04:34,380
managed in very different ways logical

00:04:32,729 --> 00:04:37,530
has a much better

00:04:34,380 --> 00:04:39,450
kind of set of defined api's and

00:04:37,530 --> 00:04:41,460
commands within sulla that lets you

00:04:39,450 --> 00:04:44,640
manage it physical is a little lacking

00:04:41,460 --> 00:04:47,010
and so with the logical topology you get

00:04:44,640 --> 00:04:48,630
a lot of implicit features in solar this

00:04:47,010 --> 00:04:50,310
is through the auto scaling component

00:04:48,630 --> 00:04:52,980
which lets you add remove replicas to

00:04:50,310 --> 00:04:55,920
based on load and other things as well

00:04:52,980 --> 00:04:58,230
as time routed aliases which based on

00:04:55,920 --> 00:05:00,270
the data within your collection spins up

00:04:58,230 --> 00:05:02,490
new collections or deletes old ones if

00:05:00,270 --> 00:05:04,680
you don't need that data anymore or you

00:05:02,490 --> 00:05:06,930
have like new data coming in with newer

00:05:04,680 --> 00:05:09,240
dates that you want there's a lot of

00:05:06,930 --> 00:05:11,130
also explicit API is provided by the

00:05:09,240 --> 00:05:13,260
collections API that lets you manage

00:05:11,130 --> 00:05:15,150
this logical topology such as crud

00:05:13,260 --> 00:05:17,040
operations about around collections

00:05:15,150 --> 00:05:21,360
replicas and shards creating them

00:05:17,040 --> 00:05:23,580
splitting them and so on as well as kind

00:05:21,360 --> 00:05:26,100
of managing your schemas in these

00:05:23,580 --> 00:05:28,710
collections as well the physical side of

00:05:26,100 --> 00:05:31,590
the topology this is like dealing around

00:05:28,710 --> 00:05:36,360
servers solar nodes solar processes this

00:05:31,590 --> 00:05:38,730
has a far kind of sparser API set so

00:05:36,360 --> 00:05:40,830
you're able to do a few a little amount

00:05:38,730 --> 00:05:43,320
of auto scaling which is shuffling

00:05:40,830 --> 00:05:46,380
replicas or cores across solar nodes as

00:05:43,320 --> 00:05:48,720
well as a couple explicit commands to

00:05:46,380 --> 00:05:50,820
the collecting API such as removing a

00:05:48,720 --> 00:05:52,650
node migrating anode which is migrates

00:05:50,820 --> 00:05:54,500
all the cores from one solar node to

00:05:52,650 --> 00:05:57,780
another solar node but in general

00:05:54,500 --> 00:06:00,540
there's not much there and so when

00:05:57,780 --> 00:06:03,450
you're running thousands or even just

00:06:00,540 --> 00:06:06,570
hundreds or dozens of solar clouds doing

00:06:03,450 --> 00:06:08,610
this physical management of solar clouds

00:06:06,570 --> 00:06:09,660
can take a lot of time and a lot of

00:06:08,610 --> 00:06:11,910
energy that you don't really want to

00:06:09,660 --> 00:06:13,830
spend and so that kind of brings us into

00:06:11,910 --> 00:06:16,620
the next section of our talk which is

00:06:13,830 --> 00:06:18,450
kubernetes communities is basically if

00:06:16,620 --> 00:06:20,930
you haven't heard of it an open source

00:06:18,450 --> 00:06:24,210
framework that allows for scheduling and

00:06:20,930 --> 00:06:26,520
operating of processes in a cloud

00:06:24,210 --> 00:06:29,100
platform a generic cloud platform

00:06:26,520 --> 00:06:30,990
basically letting you see your set of

00:06:29,100 --> 00:06:32,880
servers as one giant machine that

00:06:30,990 --> 00:06:35,180
everything is running on sharing volumes

00:06:32,880 --> 00:06:38,180
and so on and we'll get into kind of

00:06:35,180 --> 00:06:41,400
what is communities in a little bit but

00:06:38,180 --> 00:06:43,380
basically you can just see it as a giant

00:06:41,400 --> 00:06:46,919
machine that you're running at all your

00:06:43,380 --> 00:06:48,210
things on and a kind of permeated

00:06:46,919 --> 00:06:50,819
throughout the industry

00:06:48,210 --> 00:06:53,340
all the major cloud providers can

00:06:50,819 --> 00:06:55,350
support it natively as well as kind of

00:06:53,340 --> 00:06:58,800
provide custom interfaces within it to

00:06:55,350 --> 00:07:00,750
make the experience better it's kind of

00:06:58,800 --> 00:07:02,970
important to mention that in kubernetes

00:07:00,750 --> 00:07:05,190
applications are run in a declarative

00:07:02,970 --> 00:07:06,990
fashion meaning you give some set of

00:07:05,190 --> 00:07:09,120
configuration telling humidities what

00:07:06,990 --> 00:07:11,099
you want your what you want it to run

00:07:09,120 --> 00:07:14,130
and then kubernetes makes that state

00:07:11,099 --> 00:07:17,160
happen but as kind of a long side that

00:07:14,130 --> 00:07:21,330
there's kind of ways that you can have

00:07:17,160 --> 00:07:23,490
automated processes that manipulate this

00:07:21,330 --> 00:07:25,889
set of configuration as well which kind

00:07:23,490 --> 00:07:27,900
of adds a lot of power within

00:07:25,889 --> 00:07:29,639
communities and lets us do things such

00:07:27,900 --> 00:07:31,280
as manage solar cloud within it and

00:07:29,639 --> 00:07:34,110
we'll get to that later

00:07:31,280 --> 00:07:36,300
so just kind of establishing some

00:07:34,110 --> 00:07:38,970
terminology this technology we like to

00:07:36,300 --> 00:07:40,680
overload terms there's two terms here

00:07:38,970 --> 00:07:43,289
that are used within communities and

00:07:40,680 --> 00:07:46,500
Sylla that I kind of want to get out

00:07:43,289 --> 00:07:48,389
there so node node is used in both solar

00:07:46,500 --> 00:07:49,169
in communities and solar I've used it a

00:07:48,389 --> 00:07:51,389
couple times already

00:07:49,169 --> 00:07:53,310
it means it's a solar solar cloud

00:07:51,389 --> 00:07:56,370
process that is running and connected to

00:07:53,310 --> 00:07:59,520
zookeeper it can house cores issue quit

00:07:56,370 --> 00:08:00,930
I respond to queries and indexing but in

00:07:59,520 --> 00:08:03,960
kubernetes that means I'm very different

00:08:00,930 --> 00:08:06,060
this just means a like a server or a

00:08:03,960 --> 00:08:08,699
virtual machine that is running within a

00:08:06,060 --> 00:08:10,919
coop and East Network replicas is a

00:08:08,699 --> 00:08:13,229
little more similar between the two and

00:08:10,919 --> 00:08:15,599
solar we've already kind of define it if

00:08:13,229 --> 00:08:18,090
a copy of one charge the data that

00:08:15,599 --> 00:08:21,900
houses the solar core but in kubernetes

00:08:18,090 --> 00:08:24,780
it means an instantiated in session Stan

00:08:21,900 --> 00:08:26,039
Shi ation of a pod specification and

00:08:24,780 --> 00:08:29,060
we'll kind of get into what that means

00:08:26,039 --> 00:08:31,800
in a little bit so I'm gonna kind of

00:08:29,060 --> 00:08:34,140
give you a kind of introduction to

00:08:31,800 --> 00:08:36,450
kubernetes and the things you can do in

00:08:34,140 --> 00:08:39,089
it we're going to start with the

00:08:36,450 --> 00:08:41,279
low-level kind of objects that you can

00:08:39,089 --> 00:08:45,270
create and see how they all build up on

00:08:41,279 --> 00:08:46,800
top of each other so I'm assuming a

00:08:45,270 --> 00:08:49,020
lobby I'll have used containerized

00:08:46,800 --> 00:08:52,020
services in the past containers such as

00:08:49,020 --> 00:08:54,300
docker just kind of a way of isolating

00:08:52,020 --> 00:08:56,790
an environment and running a process

00:08:54,300 --> 00:09:00,089
within an isolated environment it helps

00:08:56,790 --> 00:09:02,040
you kind of tell what your dependencies

00:09:00,089 --> 00:09:04,589
are load them in beforehand to make sure

00:09:02,040 --> 00:09:06,540
that even no matter what is running on

00:09:04,589 --> 00:09:08,970
your server you can make sure that the

00:09:06,540 --> 00:09:11,630
correct things are running for your

00:09:08,970 --> 00:09:14,970
application within this docker container

00:09:11,630 --> 00:09:17,779
pods are kind of an abstraction on top

00:09:14,970 --> 00:09:20,730
of containers and manage a lot of the

00:09:17,779 --> 00:09:23,310
networking volume or like data

00:09:20,730 --> 00:09:25,380
management around them as well as kind

00:09:23,310 --> 00:09:28,500
of help make a consistent interface

00:09:25,380 --> 00:09:30,600
depending on what kind of container

00:09:28,500 --> 00:09:33,720
using be it dock or be it rocket be any

00:09:30,600 --> 00:09:36,509
of the other ones and so basically it

00:09:33,720 --> 00:09:38,730
also lets you add health checks and to

00:09:36,509 --> 00:09:40,110
make sure that no matter what kind of

00:09:38,730 --> 00:09:43,199
container you're running within your pod

00:09:40,110 --> 00:09:44,880
it has a kind of standard way of knowing

00:09:43,199 --> 00:09:47,880
whether that container is healthy

00:09:44,880 --> 00:09:52,410
whether it died any of these common

00:09:47,880 --> 00:09:53,910
operations so that's nice but whenever

00:09:52,410 --> 00:09:55,829
you're running lots of these pods are

00:09:53,910 --> 00:09:59,100
lots these containers you want to make

00:09:55,829 --> 00:10:00,839
sure that if your pod dies it comes back

00:09:59,100 --> 00:10:03,360
you don't want to just have to manually

00:10:00,839 --> 00:10:05,310
listen to see oh my application stopped

00:10:03,360 --> 00:10:05,819
working let me go and spin that back up

00:10:05,310 --> 00:10:08,069
again

00:10:05,819 --> 00:10:10,380
replica sets of the way that kubernetes

00:10:08,069 --> 00:10:13,800
kind of allows you to build an resilient

00:10:10,380 --> 00:10:16,319
resiliency around pods so that you can

00:10:13,800 --> 00:10:18,360
make sure that in of in set of your pods

00:10:16,319 --> 00:10:23,310
are running at any given time so

00:10:18,360 --> 00:10:25,889
whenever say a pod dies or goes whenever

00:10:23,310 --> 00:10:28,170
a pod dies you guys down

00:10:25,889 --> 00:10:29,790
then the pod is deleted the replica set

00:10:28,170 --> 00:10:33,630
will make sure that a new pod is

00:10:29,790 --> 00:10:35,850
scheduled and ran whenever that happens

00:10:33,630 --> 00:10:39,449
so at any given time you can expect say

00:10:35,850 --> 00:10:41,970
six of your pods to be running so that's

00:10:39,449 --> 00:10:43,680
nice however it doesn't really do

00:10:41,970 --> 00:10:46,589
everything you want say you want to

00:10:43,680 --> 00:10:48,689
upgrade your pod to run a new version of

00:10:46,589 --> 00:10:50,310
your software you don't want to just

00:10:48,689 --> 00:10:52,110
upgrade at all though your pods at the

00:10:50,310 --> 00:10:55,430
same time you want to upgrade in a very

00:10:52,110 --> 00:10:57,329
safe fashion say a rolling upgrade

00:10:55,430 --> 00:10:59,910
deployments are a way that cube minis

00:10:57,329 --> 00:11:01,500
lets you do this it basically manages

00:10:59,910 --> 00:11:02,790
replica sets as you can see we're just

00:11:01,500 --> 00:11:04,339
kind of building on top of each other

00:11:02,790 --> 00:11:07,110
here replica sets manage pods

00:11:04,339 --> 00:11:08,850
deployments manage replica sets and so

00:11:07,110 --> 00:11:11,449
it will spin up whenever you want to

00:11:08,850 --> 00:11:13,290
deploy a new version of your application

00:11:11,449 --> 00:11:16,079
deployments will spin up a new replica

00:11:13,290 --> 00:11:19,800
set and very safely

00:11:16,079 --> 00:11:22,529
delete and add one by one from these

00:11:19,800 --> 00:11:24,959
repla sets so at first it'll start to

00:11:22,529 --> 00:11:26,819
bring up a new pod in the new version of

00:11:24,959 --> 00:11:29,759
your replica set and bring down one pot

00:11:26,819 --> 00:11:31,619
in previous version if that doesn't work

00:11:29,759 --> 00:11:33,660
it the upgrade will stop and you'll

00:11:31,619 --> 00:11:36,689
still have two pods available in your

00:11:33,660 --> 00:11:37,290
previous repla set however if it does

00:11:36,689 --> 00:11:39,540
work

00:11:37,290 --> 00:11:41,369
the deployment won't continue trying to

00:11:39,540 --> 00:11:43,439
increase the amount of new pods you have

00:11:41,369 --> 00:11:45,269
and decrease the amount of old pods so

00:11:43,439 --> 00:11:47,759
that your upgrade works kind of

00:11:45,269 --> 00:11:50,720
flawlessly seamlessly and so you don't

00:11:47,759 --> 00:11:52,829
really have to manage anything about

00:11:50,720 --> 00:11:56,369
making sure your upgrades are safe and

00:11:52,829 --> 00:11:58,619
reliable and kind of the thing to note

00:11:56,369 --> 00:12:00,329
here is that even though the old replica

00:11:58,619 --> 00:12:02,279
set no longer has any running pods

00:12:00,329 --> 00:12:04,769
crewman Hayes will keep it around just

00:12:02,279 --> 00:12:07,379
in case you want to rollback versions if

00:12:04,769 --> 00:12:11,399
something bad happened in your new kind

00:12:07,379 --> 00:12:13,920
of software upgrade so that's nice we

00:12:11,399 --> 00:12:15,929
can now kind of consistently run manage

00:12:13,920 --> 00:12:20,670
applications in a safe and consistent

00:12:15,929 --> 00:12:22,879
way however while kubernetes gives us

00:12:20,670 --> 00:12:25,679
kind of like nice tooling around this

00:12:22,879 --> 00:12:28,319
kubernetes it's not a fix-all solution

00:12:25,679 --> 00:12:30,240
for all of your problems there are kind

00:12:28,319 --> 00:12:33,779
of give and takes and networking is one

00:12:30,240 --> 00:12:36,779
of those big give you one of the give or

00:12:33,779 --> 00:12:38,910
takes and so basically networking is not

00:12:36,779 --> 00:12:41,459
necessarily easier within kubernetes and

00:12:38,910 --> 00:12:44,069
outside of communities and so services

00:12:41,459 --> 00:12:47,929
are one way which kubernetes has kind of

00:12:44,069 --> 00:12:51,149
try to introduce basic networking within

00:12:47,929 --> 00:12:52,980
this framework so as you see before we

00:12:51,149 --> 00:12:54,540
have our deployment which maps to a

00:12:52,980 --> 00:12:57,089
replica set which isn't shown and that's

00:12:54,540 --> 00:13:00,029
eventually mapped pods services are a

00:12:57,089 --> 00:13:01,860
way of kind of letting pods be

00:13:00,029 --> 00:13:03,869
addressable so you have all these

00:13:01,860 --> 00:13:06,449
applications running say that they're

00:13:03,869 --> 00:13:10,379
HTTP servers you want to be able to

00:13:06,449 --> 00:13:12,689
reach these pods from other pods and in

00:13:10,379 --> 00:13:14,369
kubernetes due to security reasons pods

00:13:12,689 --> 00:13:16,619
aren't addressable you're aren't able to

00:13:14,369 --> 00:13:17,910
just reach a pod directly you have to go

00:13:16,619 --> 00:13:19,919
through something that's called a

00:13:17,910 --> 00:13:23,160
service and so basically services are

00:13:19,919 --> 00:13:27,869
there just to provide a mapping from a

00:13:23,160 --> 00:13:29,760
DNS namespace a DNS or a IP address to a

00:13:27,869 --> 00:13:32,250
set of pods

00:13:29,760 --> 00:13:34,529
services are given and mapping of pod

00:13:32,250 --> 00:13:37,050
like a pod selector which basically

00:13:34,529 --> 00:13:38,399
tells you which podsnap - it doesn't

00:13:37,050 --> 00:13:39,930
have to go through deployments you can

00:13:38,399 --> 00:13:42,329
just be running your own individual pods

00:13:39,930 --> 00:13:44,070
and services will still work with it we

00:13:42,329 --> 00:13:46,380
just managed it be a deployments because

00:13:44,070 --> 00:13:48,420
that's generally what's done and so

00:13:46,380 --> 00:13:50,279
basically you can also create another

00:13:48,420 --> 00:13:52,529
pod selector give with a new service

00:13:50,279 --> 00:13:55,170
service B that overlaps with some of the

00:13:52,529 --> 00:13:58,410
existing pods that are mapped to be a

00:13:55,170 --> 00:13:59,639
service a this is very useful as we'll

00:13:58,410 --> 00:14:03,329
see later in the presentation when

00:13:59,639 --> 00:14:06,269
creating our solar cloud but the

00:14:03,329 --> 00:14:10,800
important thing here is that pods can be

00:14:06,269 --> 00:14:12,630
pods are making me map to buy multiple

00:14:10,800 --> 00:14:14,820
services no services one service they

00:14:12,630 --> 00:14:16,800
don't really care the services work on a

00:14:14,820 --> 00:14:21,000
different layer and just provide basic

00:14:16,800 --> 00:14:23,820
networking however services only really

00:14:21,000 --> 00:14:26,910
work within a Cooper Nettie's cluster so

00:14:23,820 --> 00:14:28,980
a client traffic cannot really hit

00:14:26,910 --> 00:14:30,389
services from outside the cluster this

00:14:28,980 --> 00:14:33,510
isn't always true you can use load

00:14:30,389 --> 00:14:35,160
balancing services but this kind of is

00:14:33,510 --> 00:14:37,470
dependent on the cloud provider that you

00:14:35,160 --> 00:14:39,750
run kubernetes in and if you run it on

00:14:37,470 --> 00:14:43,470
bare metal hardware it doesn't really

00:14:39,750 --> 00:14:45,300
work as well so there's another product

00:14:43,470 --> 00:14:49,110
that communities kind of offers which is

00:14:45,300 --> 00:14:50,670
ingress --is and ingresses are a another

00:14:49,110 --> 00:14:53,430
application that's running inside

00:14:50,670 --> 00:14:56,010
kubernetes and listens to client traffic

00:14:53,430 --> 00:14:57,630
coming from outside communities and once

00:14:56,010 --> 00:15:00,779
the client traffic hits this ingress

00:14:57,630 --> 00:15:04,470
controller you have defined rules on how

00:15:00,779 --> 00:15:06,480
to route that traffic it's an important

00:15:04,470 --> 00:15:08,430
note ingress is only work with HTTP

00:15:06,480 --> 00:15:11,819
traffic they don't work with based TCP

00:15:08,430 --> 00:15:14,430
or any of the other standards and so you

00:15:11,819 --> 00:15:16,529
get kind of because it's HTTP you can do

00:15:14,430 --> 00:15:19,019
kind of complex routing you can do

00:15:16,529 --> 00:15:22,380
routing based on hostname so host 1 is

00:15:19,019 --> 00:15:25,560
not routed to service a host 2 is routed

00:15:22,380 --> 00:15:27,959
to service B or you can do a path level

00:15:25,560 --> 00:15:29,459
routing so host the reason that to

00:15:27,959 --> 00:15:32,339
anything you start looking at the path

00:15:29,459 --> 00:15:35,850
and we see that API v2 is routed back to

00:15:32,339 --> 00:15:38,220
service a so much like services ingress

00:15:35,850 --> 00:15:40,529
rules can map to multiple services or no

00:15:38,220 --> 00:15:42,569
services but basically you have to

00:15:40,529 --> 00:15:43,560
provide a service for ingress - ow - you

00:15:42,569 --> 00:15:46,020
can't just provide

00:15:43,560 --> 00:15:48,740
pods because now all networking within

00:15:46,020 --> 00:15:51,750
communities has to go through services

00:15:48,740 --> 00:15:54,510
so I guess the question here is we have

00:15:51,750 --> 00:15:56,820
all these pieces of infrastructure that

00:15:54,510 --> 00:16:00,420
kubernetes provides us can we build a

00:15:56,820 --> 00:16:02,760
solar cloud with these pieces and once

00:16:00,420 --> 00:16:05,580
we start thinking about that as a few

00:16:02,760 --> 00:16:07,770
issues start to come up solar nodes or

00:16:05,580 --> 00:16:10,950
we'd be running them in communities pods

00:16:07,770 --> 00:16:12,779
have data that is unique to them this is

00:16:10,950 --> 00:16:14,250
not just the Scylla cores that they're

00:16:12,779 --> 00:16:16,220
running even though that is unique to

00:16:14,250 --> 00:16:18,960
them and needs to be stored persistently

00:16:16,220 --> 00:16:22,140
they have a unique name and address

00:16:18,960 --> 00:16:24,870
whenever you search little cloud up in

00:16:22,140 --> 00:16:27,270
zookeeper it's so it stores its kind of

00:16:24,870 --> 00:16:30,000
the address to locate it and the name

00:16:27,270 --> 00:16:32,010
that's running under and whenever it

00:16:30,000 --> 00:16:34,650
restarts it needs to have a consistent

00:16:32,010 --> 00:16:36,089
name and address for the other nodes in

00:16:34,650 --> 00:16:37,710
your solar cloud to be able to

00:16:36,089 --> 00:16:40,200
communicate with it

00:16:37,710 --> 00:16:42,960
and so the name and address and solar

00:16:40,200 --> 00:16:46,080
cores that are running I kind of need to

00:16:42,960 --> 00:16:47,700
be persisted through outages through pod

00:16:46,080 --> 00:16:50,339
restarts through any of these things

00:16:47,700 --> 00:16:51,900
I mean having any of them change

00:16:50,339 --> 00:16:54,360
whenever the pod research does kind of

00:16:51,900 --> 00:16:56,940
break solar cloud so how are we gonna

00:16:54,360 --> 00:16:59,760
deal with that so there's a couple of

00:16:56,940 --> 00:17:02,670
rooms for improvement with kubernetes

00:16:59,760 --> 00:17:04,589
running stateful applications and the

00:17:02,670 --> 00:17:07,709
workflow that we've defined already with

00:17:04,589 --> 00:17:09,839
these deployments the replica sets these

00:17:07,709 --> 00:17:11,730
services they work very well with

00:17:09,839 --> 00:17:13,709
stateless applications and you can think

00:17:11,730 --> 00:17:15,920
of the solar prometheus exporter as one

00:17:13,709 --> 00:17:20,150
of these things which just basically

00:17:15,920 --> 00:17:23,120
runs one pod and it queries solar

00:17:20,150 --> 00:17:25,530
metrics and queries it's other things in

00:17:23,120 --> 00:17:27,270
exports such Prometheus as you can see

00:17:25,530 --> 00:17:29,310
that doesn't have any state associated

00:17:27,270 --> 00:17:32,370
with it it's very easy to run via

00:17:29,310 --> 00:17:33,630
deployments but these deployments were

00:17:32,370 --> 00:17:34,740
up because that's really do have issues

00:17:33,630 --> 00:17:37,020
with the things that we're talking about

00:17:34,740 --> 00:17:39,450
before which are data persistence and

00:17:37,020 --> 00:17:41,460
locality kind of consistent naming

00:17:39,450 --> 00:17:43,890
through pot restarts and kind of

00:17:41,460 --> 00:17:45,750
consistent addressability as well so

00:17:43,890 --> 00:17:47,960
what does communities provide us to be

00:17:45,750 --> 00:17:50,970
able to deal with these problems

00:17:47,960 --> 00:17:54,980
persistent data has a story in Cuban ace

00:17:50,970 --> 00:17:57,450
the story is not perfect but it exists

00:17:54,980 --> 00:17:59,789
there are persistent volumes

00:17:57,450 --> 00:18:02,190
so if you've used docker docker has

00:17:59,789 --> 00:18:05,570
volumes that you mountain as directories

00:18:02,190 --> 00:18:06,720
or other environment variables and

00:18:05,570 --> 00:18:08,159
docker

00:18:06,720 --> 00:18:10,289
I mean communities does have a way of

00:18:08,159 --> 00:18:12,179
kind of managing this they have

00:18:10,289 --> 00:18:14,690
persistent volumes which are a way of

00:18:12,179 --> 00:18:16,950
staying storing persistent state and

00:18:14,690 --> 00:18:19,620
there's a lot of different providers of

00:18:16,950 --> 00:18:21,600
these there's ones that are just storing

00:18:19,620 --> 00:18:24,299
local data on your box and on your

00:18:21,600 --> 00:18:26,100
server but also as your a DBS all the

00:18:24,299 --> 00:18:27,750
big cloud providers have created their

00:18:26,100 --> 00:18:30,690
own persistent volumes to be able to use

00:18:27,750 --> 00:18:32,899
it within their ecosystem other kind of

00:18:30,690 --> 00:18:36,330
more open source he things such as in FS

00:18:32,899 --> 00:18:39,299
I think I can't core OS so a couple

00:18:36,330 --> 00:18:42,779
other ones have their own persistent

00:18:39,299 --> 00:18:45,149
volume well as well but they all kind of

00:18:42,779 --> 00:18:47,779
implement this base persistent volume

00:18:45,149 --> 00:18:50,760
interface that kubernetes can make specs

00:18:47,779 --> 00:18:52,919
and a persistent volume claim is kind of

00:18:50,760 --> 00:18:55,070
coupled with this so a persistent volume

00:18:52,919 --> 00:18:57,750
claim let's something within kubernetes

00:18:55,070 --> 00:19:00,720
basically asked for a persistent volume

00:18:57,750 --> 00:19:01,919
resource and kubernetes will then find a

00:19:00,720 --> 00:19:03,480
persistent volume that has been

00:19:01,919 --> 00:19:05,429
registered in the same kubernetes

00:19:03,480 --> 00:19:08,000
cluster and kind of link it to that

00:19:05,429 --> 00:19:12,419
persistent volume claim so that when a

00:19:08,000 --> 00:19:15,750
pod that uses that claim restarts and

00:19:12,419 --> 00:19:17,100
when he comes back up the persistent

00:19:15,750 --> 00:19:20,250
volume will still be there and be able

00:19:17,100 --> 00:19:23,970
to kind of be added back onto the pod

00:19:20,250 --> 00:19:28,019
once it's restarted so how is this used

00:19:23,970 --> 00:19:31,860
so staple sets are kind of a reimagining

00:19:28,019 --> 00:19:34,320
of replica sets replica sets were ways

00:19:31,860 --> 00:19:36,149
of managing in replicas of the pod

00:19:34,320 --> 00:19:38,760
specification staples has to do the same

00:19:36,149 --> 00:19:39,539
thing however staple sets do other

00:19:38,760 --> 00:19:42,029
things swell

00:19:39,539 --> 00:19:44,429
they now kind of have a standard pod

00:19:42,029 --> 00:19:47,610
naming structure so that whenever a pod

00:19:44,429 --> 00:19:49,260
goes down and gets deleted it comes back

00:19:47,610 --> 00:19:51,059
up with the same name which is very

00:19:49,260 --> 00:19:52,289
important for things like solar where we

00:19:51,059 --> 00:19:55,789
need consistent naming and addressing

00:19:52,289 --> 00:19:59,039
through out pod restarts and kind of

00:19:55,789 --> 00:20:01,200
additional upgrades and stuff and these

00:19:59,039 --> 00:20:03,360
persistent volume claims are then now

00:20:01,200 --> 00:20:06,179
set up natively with stateful sets so

00:20:03,360 --> 00:20:07,769
that we you pass when you pass in the

00:20:06,179 --> 00:20:10,200
pod specification that you want to run

00:20:07,769 --> 00:20:11,070
you also pass in a persistent volume

00:20:10,200 --> 00:20:13,320
claim that you want to

00:20:11,070 --> 00:20:15,960
alongside these pods and so as you can

00:20:13,320 --> 00:20:18,630
see you're here a pod

00:20:15,960 --> 00:20:21,029
a persistent volume claim is linked to

00:20:18,630 --> 00:20:23,039
each pod and so whenever our pod went

00:20:21,029 --> 00:20:25,019
down earlier the persistent volume claim

00:20:23,039 --> 00:20:27,179
didn't get deleted alongside it it

00:20:25,019 --> 00:20:29,130
stayed and it's a link to the persistent

00:20:27,179 --> 00:20:31,500
volume that kubernetes gave it so that

00:20:29,130 --> 00:20:33,600
when the pod comes back up you can see

00:20:31,500 --> 00:20:35,009
that has not lost its data our solar

00:20:33,600 --> 00:20:38,750
cores are still there and they're still

00:20:35,009 --> 00:20:42,029
be able to be indexed which is nice

00:20:38,750 --> 00:20:44,340
headless services are okay so we've kind

00:20:42,029 --> 00:20:46,200
of dealt with these deployments of these

00:20:44,340 --> 00:20:47,789
staple services having data associated

00:20:46,200 --> 00:20:48,960
with these stateless services now we

00:20:47,789 --> 00:20:51,090
kind of have to deal with the networking

00:20:48,960 --> 00:20:53,850
part of it so headless services are a

00:20:51,090 --> 00:20:56,100
way that communities has allowed these

00:20:53,850 --> 00:20:58,110
stateful applications to be able to

00:20:56,100 --> 00:20:59,850
individually address solar PAH I'm

00:20:58,110 --> 00:21:02,129
miserable applause terminate these pods

00:20:59,850 --> 00:21:03,330
within these staple sets so whenever you

00:21:02,129 --> 00:21:05,159
have a headless service it gives a

00:21:03,330 --> 00:21:08,309
different host name for each pod in the

00:21:05,159 --> 00:21:11,220
staple set this is just kind of services

00:21:08,309 --> 00:21:13,919
manipulating the dns to just add entries

00:21:11,220 --> 00:21:15,659
for each pod and so whenever the solar

00:21:13,919 --> 00:21:19,889
pod restarts since the pod has a

00:21:15,659 --> 00:21:21,899
consistent name the address for that pod

00:21:19,889 --> 00:21:24,990
will stay the same since it's just the

00:21:21,899 --> 00:21:28,230
pod named service name name space so at

00:21:24,990 --> 00:21:30,629
this point we have kind of consistent

00:21:28,230 --> 00:21:33,330
addressability consistent naming and

00:21:30,629 --> 00:21:35,220
persistent volumes persistent storage

00:21:33,330 --> 00:21:38,549
that solar kind of can work more

00:21:35,220 --> 00:21:40,289
natively with kubernetes and but this

00:21:38,549 --> 00:21:41,820
does kind of introduce limitation

00:21:40,289 --> 00:21:44,730
headless services do have limitations

00:21:41,820 --> 00:21:46,529
associated with them and they don't work

00:21:44,730 --> 00:21:48,570
with ingress is for one so it doesn't

00:21:46,529 --> 00:21:50,340
really help us get data from outside of

00:21:48,570 --> 00:21:53,519
kubernetes back into the kubernetes

00:21:50,340 --> 00:21:55,289
cluster and they can't also be use of

00:21:53,519 --> 00:21:57,659
load balancing which is another way of

00:21:55,289 --> 00:22:00,059
getting services addressed outside of

00:21:57,659 --> 00:22:02,039
the communities cluster so if we want to

00:22:00,059 --> 00:22:04,919
run our communities I mean our solar

00:22:02,039 --> 00:22:07,169
cloud safer in to close community

00:22:04,919 --> 00:22:08,879
clusters or address our solar cloud

00:22:07,169 --> 00:22:10,980
nodes individually from outside the

00:22:08,879 --> 00:22:14,879
community lustre headless services don't

00:22:10,980 --> 00:22:16,590
really help us very much so and why is

00:22:14,879 --> 00:22:19,049
this really needed why do we need to run

00:22:16,590 --> 00:22:20,669
sort of clouds across kubernetes or have

00:22:19,049 --> 00:22:24,780
clients access it from outside of

00:22:20,669 --> 00:22:26,160
kubernetes one you won't be able to have

00:22:24,780 --> 00:22:27,870
rollouts of every piece of your

00:22:26,160 --> 00:22:30,200
infrastructure and so if you want to

00:22:27,870 --> 00:22:33,930
have upgrade your kubernetes cluster

00:22:30,200 --> 00:22:36,210
communities kind of plays a new update

00:22:33,930 --> 00:22:39,690
every six months usually a big major

00:22:36,210 --> 00:22:41,520
release and so it's kind of fairly

00:22:39,690 --> 00:22:44,220
common to see yourself upgrade you know

00:22:41,520 --> 00:22:45,930
a kubernetes cluster and so if you want

00:22:44,220 --> 00:22:47,670
to kind of have a staged rollout you

00:22:45,930 --> 00:22:49,740
need to be able to have multiple

00:22:47,670 --> 00:22:51,840
kubernetes clusters running your solar

00:22:49,740 --> 00:22:55,800
applications and you want to have these

00:22:51,840 --> 00:22:58,080
solar clouds run kind of into the same

00:22:55,800 --> 00:23:00,390
solar cloud run into different

00:22:58,080 --> 00:23:06,510
kubernetes clusters talking across them

00:23:00,390 --> 00:23:08,610
on these staged rollouts don't just

00:23:06,510 --> 00:23:10,440
include kubernetes they include other

00:23:08,610 --> 00:23:12,720
pieces of software running in companies

00:23:10,440 --> 00:23:15,570
and so we're gonna introduce the solar

00:23:12,720 --> 00:23:17,550
operator later and whenever you're using

00:23:15,570 --> 00:23:19,770
the solar operator to manage your solar

00:23:17,550 --> 00:23:21,720
clouds you want to be able to do a

00:23:19,770 --> 00:23:24,150
staged rollout a set of that as well

00:23:21,720 --> 00:23:26,010
which also requires multiple kubernetes

00:23:24,150 --> 00:23:28,890
clusters running multiple solar

00:23:26,010 --> 00:23:30,390
operators and in general your

00:23:28,890 --> 00:23:32,160
applications aren't necessarily going to

00:23:30,390 --> 00:23:34,830
be running the same so a community

00:23:32,160 --> 00:23:36,450
cluster that your solar clouds are

00:23:34,830 --> 00:23:38,160
running in and so if you want to use

00:23:36,450 --> 00:23:40,230
things that's just a solar Jake client

00:23:38,160 --> 00:23:42,750
I'm sure PI solar has similar

00:23:40,230 --> 00:23:44,850
capabilities but generally being able to

00:23:42,750 --> 00:23:46,320
address nodes individual nodes from

00:23:44,850 --> 00:23:47,490
outside the communities cluster we're

00:23:46,320 --> 00:23:50,940
not gonna be able to use these headless

00:23:47,490 --> 00:23:52,800
services so what can we do there's two

00:23:50,940 --> 00:23:55,560
different ways we can do this but in

00:23:52,800 --> 00:23:57,210
general the solution is unfortunately

00:23:55,560 --> 00:23:59,070
you just need to create a separate

00:23:57,210 --> 00:24:01,800
service for every solar node and we'll

00:23:59,070 --> 00:24:03,720
kind of explain how this works later but

00:24:01,800 --> 00:24:05,190
once you have a separate service for

00:24:03,720 --> 00:24:06,900
every solar node there's two ways to

00:24:05,190 --> 00:24:08,700
make it addressable from outside we

00:24:06,900 --> 00:24:11,580
create a load balancer within that

00:24:08,700 --> 00:24:14,030
service which creates an IP address that

00:24:11,580 --> 00:24:16,890
Maps digest that individual solar node

00:24:14,030 --> 00:24:18,510
or we create an ingress which allows for

00:24:16,890 --> 00:24:22,290
a kind of more complex

00:24:18,510 --> 00:24:25,410
routing of path and hostname to that

00:24:22,290 --> 00:24:28,800
solar pod through a common IP address I

00:24:25,410 --> 00:24:30,840
will say at Bloomberg we kind of prove

00:24:28,800 --> 00:24:32,370
we like the ingress solution more

00:24:30,840 --> 00:24:34,740
because we don't like spending up

00:24:32,370 --> 00:24:36,240
thousands of IP addresses because we run

00:24:34,740 --> 00:24:38,250
thousands of solar notes within our

00:24:36,240 --> 00:24:41,280
company

00:24:38,250 --> 00:24:44,220
so as a because we like ingress more all

00:24:41,280 --> 00:24:48,059
my solutions will kind of have ingress

00:24:44,220 --> 00:24:49,890
in there so just that's a caveat and so

00:24:48,059 --> 00:24:51,960
as you can see here we have our solar

00:24:49,890 --> 00:24:53,669
cloud running and we have client traffic

00:24:51,960 --> 00:24:55,980
hitting it from the outside through an

00:24:53,669 --> 00:24:58,470
ingress these ingress rules then map to

00:24:55,980 --> 00:25:01,320
different note services we have one

00:24:58,470 --> 00:25:03,570
common service that routes to all the

00:25:01,320 --> 00:25:07,530
pods in our solar cloud and then we have

00:25:03,570 --> 00:25:11,580
individual services that Knapp kind of

00:25:07,530 --> 00:25:14,220
data and user requests from to an

00:25:11,580 --> 00:25:18,539
individual pod within solar so the node

00:25:14,220 --> 00:25:20,159
5 node 6 and so on once we're using

00:25:18,539 --> 00:25:24,419
these individual node services we can

00:25:20,159 --> 00:25:28,830
then have the individual pods use the

00:25:24,419 --> 00:25:32,220
same kind of client request path to end

00:25:28,830 --> 00:25:35,340
address notes themself so pod 5 would in

00:25:32,220 --> 00:25:37,470
if it's doing peer sync with pod 6 would

00:25:35,340 --> 00:25:40,080
go back out to the ingress and have that

00:25:37,470 --> 00:25:42,419
request routed down to the service and

00:25:40,080 --> 00:25:44,490
back to down to pod 6 so this way we can

00:25:42,419 --> 00:25:47,220
run solar across multiple kubernetes

00:25:44,490 --> 00:25:49,950
clusters and it will be able to kind of

00:25:47,220 --> 00:25:51,780
work natively and talk to pods within

00:25:49,950 --> 00:25:54,750
its own communities cluster another one

00:25:51,780 --> 00:25:59,159
and it doesn't really care which one is

00:25:54,750 --> 00:26:01,020
running in so how have we built the

00:25:59,159 --> 00:26:02,880
solar cloud in communities as you can

00:26:01,020 --> 00:26:05,880
see here at the bottom we have a staple

00:26:02,880 --> 00:26:08,159
set that staple set has four pods

00:26:05,880 --> 00:26:10,830
running in it these are our four solar

00:26:08,159 --> 00:26:12,809
cloud pods and then we have a lot of

00:26:10,830 --> 00:26:15,030
different services these services are

00:26:12,809 --> 00:26:18,419
one each for all of our solar cloud

00:26:15,030 --> 00:26:20,460
nodes as well as one common endpoint for

00:26:18,419 --> 00:26:22,200
all of our nodes together and then a

00:26:20,460 --> 00:26:24,240
headless service that isn't really used

00:26:22,200 --> 00:26:26,909
for much at the bottom we have an

00:26:24,240 --> 00:26:29,460
ingress oh and this ingress has multiple

00:26:26,909 --> 00:26:31,890
hosts associated with it on the

00:26:29,460 --> 00:26:34,049
basically the common service of the

00:26:31,890 --> 00:26:36,690
common solar cloud host as well as

00:26:34,049 --> 00:26:38,940
individual node hosts as well this is

00:26:36,690 --> 00:26:40,440
how we route traffic into the kubernetes

00:26:38,940 --> 00:26:46,320
cluster and into our individual solar

00:26:40,440 --> 00:26:48,899
nodes so running all these things kind

00:26:46,320 --> 00:26:51,630
of lets us build a solar cloud within

00:26:48,899 --> 00:26:52,110
kubernetes however managing all of these

00:26:51,630 --> 00:26:54,960
different

00:26:52,110 --> 00:26:56,790
objects can become cumbersome and as I

00:26:54,960 --> 00:26:58,200
mentioned before we run thousands of

00:26:56,790 --> 00:27:00,150
solar clouds we don't want to have to go

00:26:58,200 --> 00:27:02,010
and manually create these staple sets

00:27:00,150 --> 00:27:03,840
may nearly create these services these

00:27:02,010 --> 00:27:06,450
in grasses and so on and have a whole

00:27:03,840 --> 00:27:08,460
management system for that we want

00:27:06,450 --> 00:27:12,299
communities to be able to do that for us

00:27:08,460 --> 00:27:14,010
and so communities has these objects

00:27:12,299 --> 00:27:16,590
called custom researched

00:27:14,010 --> 00:27:19,500
custom research definitions which allow

00:27:16,590 --> 00:27:21,450
you to create objects within communities

00:27:19,500 --> 00:27:24,120
as if they were native to communities

00:27:21,450 --> 00:27:26,880
such as deployment services staple sets

00:27:24,120 --> 00:27:28,290
etc and so these custom objects and

00:27:26,880 --> 00:27:30,840
communities can be anything they can be

00:27:28,290 --> 00:27:33,150
solar zookeeper Kafka anything we want

00:27:30,840 --> 00:27:35,040
to create and then controllers are

00:27:33,150 --> 00:27:37,890
applications that are built on top of

00:27:35,040 --> 00:27:40,110
these custom resources that allow you to

00:27:37,890 --> 00:27:42,990
kind of listen to the API server and

00:27:40,110 --> 00:27:45,210
kubernetes for changes in different like

00:27:42,990 --> 00:27:47,880
kind of updates creations deletions

00:27:45,210 --> 00:27:50,780
different operations on instantiation of

00:27:47,880 --> 00:27:53,610
this custom resource definition and then

00:27:50,780 --> 00:27:55,559
the controllers they can manipulate the

00:27:53,610 --> 00:27:59,400
other communities objects to make that

00:27:55,559 --> 00:28:01,740
state happen so say we have a solar

00:27:59,400 --> 00:28:03,330
cloud specification the controller would

00:28:01,740 --> 00:28:07,020
then make sure that all of these things

00:28:03,330 --> 00:28:09,960
have been created once that kind of CRD

00:28:07,020 --> 00:28:13,650
has been pushed kubernetes pause so you

00:28:09,960 --> 00:28:15,270
can take okay so operators are nothing

00:28:13,650 --> 00:28:18,929
more than just a grouping of controllers

00:28:15,270 --> 00:28:21,600
they allow for a common kind of actions

00:28:18,929 --> 00:28:23,880
on a set of technologies so for example

00:28:21,600 --> 00:28:26,549
solar cloud the silikal operator would

00:28:23,880 --> 00:28:28,350
have a so a cloud controller a solar

00:28:26,549 --> 00:28:30,510
cloud backup controller a restoring

00:28:28,350 --> 00:28:32,610
controller these are just kind of the

00:28:30,510 --> 00:28:35,220
common operations and other technologies

00:28:32,610 --> 00:28:38,940
have more kind of technology specific

00:28:35,220 --> 00:28:40,260
actions associated with them so this

00:28:38,940 --> 00:28:42,720
specification that we've been talking

00:28:40,260 --> 00:28:44,760
about the solar cloud resource that

00:28:42,720 --> 00:28:46,230
we've created has a more simple

00:28:44,760 --> 00:28:49,679
specification than some of the more

00:28:46,230 --> 00:28:51,900
mature customer resources out there but

00:28:49,679 --> 00:28:55,350
there's more in there but kind of taking

00:28:51,900 --> 00:28:57,200
out the kind of unimportant things and

00:28:55,350 --> 00:28:59,580
left the ones that are really important

00:28:57,200 --> 00:29:01,530
basically the main things you need to

00:28:59,580 --> 00:29:04,140
run solar are the number of solar nodes

00:29:01,530 --> 00:29:05,430
that you want to run the version of solo

00:29:04,140 --> 00:29:06,960
that you want to run and

00:29:05,430 --> 00:29:10,950
as you can see we're just using the

00:29:06,960 --> 00:29:14,130
default docker solar container that's

00:29:10,950 --> 00:29:17,880
published in the most recent version and

00:29:14,130 --> 00:29:19,890
then a zookeeper to connect to the solar

00:29:17,880 --> 00:29:22,890
cloud operator can create you a

00:29:19,890 --> 00:29:24,900
zookeeper in the cluster dynamically if

00:29:22,890 --> 00:29:26,610
you want it to but it's kind of nice to

00:29:24,900 --> 00:29:28,440
have an external zookeeper that you know

00:29:26,610 --> 00:29:31,940
is going to be there and reliable that's

00:29:28,440 --> 00:29:35,670
not running in the same kind of

00:29:31,940 --> 00:29:37,770
cholesterol or cloud is and so here we

00:29:35,670 --> 00:29:40,050
pass it an external connection string as

00:29:37,770 --> 00:29:43,170
well as the CH fruit that we want solar

00:29:40,050 --> 00:29:46,020
cloud to connect to once this is done so

00:29:43,170 --> 00:29:47,850
cloud will create the objects of objects

00:29:46,020 --> 00:29:49,950
that we talked about earlier and return

00:29:47,850 --> 00:29:51,900
back as status so CRD use have both the

00:29:49,950 --> 00:29:54,930
specification and a status

00:29:51,900 --> 00:29:57,000
so once the CRD is created it will

00:29:54,930 --> 00:29:58,800
basically populate the status section it

00:29:57,000 --> 00:30:00,720
will tell you how to connect to the

00:29:58,800 --> 00:30:03,210
solar cloud via maybe zookeeper at the

00:30:00,720 --> 00:30:05,520
bottom or just a URL at the top as well

00:30:03,210 --> 00:30:07,980
as the solar nodes that are running and

00:30:05,520 --> 00:30:09,750
the versions that they're running the

00:30:07,980 --> 00:30:11,820
status of the solar nodes as well as

00:30:09,750 --> 00:30:14,130
individual connection strings for those

00:30:11,820 --> 00:30:16,410
nodes and as you see here at the bottom

00:30:14,130 --> 00:30:18,660
this cloud is currently doing an upgrade

00:30:16,410 --> 00:30:21,210
from solar eight point one point zero to

00:30:18,660 --> 00:30:22,410
eight point one point one and the status

00:30:21,210 --> 00:30:24,540
section will kind of tell you this

00:30:22,410 --> 00:30:26,700
status of the nodes through the upgrade

00:30:24,540 --> 00:30:28,110
and so we have one node that has been

00:30:26,700 --> 00:30:30,510
upgraded one note that hasn't been

00:30:28,110 --> 00:30:32,490
updated yet and it seems to be working

00:30:30,510 --> 00:30:34,920
since the node that is upgraded it's

00:30:32,490 --> 00:30:38,910
currently ready so soon the last one

00:30:34,920 --> 00:30:40,800
will be upgraded so I guess kind of

00:30:38,910 --> 00:30:44,130
finally we've created this little cloud

00:30:40,800 --> 00:30:46,080
operator it's recently published as open

00:30:44,130 --> 00:30:48,210
source we finally got it out this last

00:30:46,080 --> 00:30:50,730
Friday so the builds are not necessarily

00:30:48,210 --> 00:30:53,340
working but the codes out there would

00:30:50,730 --> 00:30:55,500
love contributions basically we want

00:30:53,340 --> 00:30:57,180
this to be the way that Bloomberg runs

00:30:55,500 --> 00:30:58,680
solar cloud in the future and that have

00:30:57,180 --> 00:31:01,740
a lot of companies that manage a lot of

00:30:58,680 --> 00:31:03,030
solar clouds run in the future these

00:31:01,740 --> 00:31:04,890
slides will be published at some point

00:31:03,030 --> 00:31:07,530
and if you search Solar operator this

00:31:04,890 --> 00:31:10,620
not a whole lot out there and I think

00:31:07,530 --> 00:31:13,590
this is one of the top results so we

00:31:10,620 --> 00:31:16,680
want as many contributions as possible

00:31:13,590 --> 00:31:18,870
Bloomberg is kind of big into making

00:31:16,680 --> 00:31:20,370
open-source things so that we kind of

00:31:18,870 --> 00:31:22,650
should be the key to the community and

00:31:20,370 --> 00:31:28,080
that everyone can have kind of the best

00:31:22,650 --> 00:31:29,760
way to run solar as possible so what is

00:31:28,080 --> 00:31:31,740
kind of the future of this project

00:31:29,760 --> 00:31:34,950
there's kind of a lot of things that we

00:31:31,740 --> 00:31:37,110
need to do the data persistent is the

00:31:34,950 --> 00:31:38,880
data persistence is there but in general

00:31:37,110 --> 00:31:40,110
we don't have a good story around it we

00:31:38,880 --> 00:31:41,490
haven't done a whole lot of testing

00:31:40,110 --> 00:31:43,140
around the local persistent volumes

00:31:41,490 --> 00:31:45,150
which are still kind of in their infancy

00:31:43,140 --> 00:31:46,830
in kubernetes and we haven't done a

00:31:45,150 --> 00:31:49,380
whole lot of testing around the remote

00:31:46,830 --> 00:31:50,820
storage capabilities that will

00:31:49,380 --> 00:31:53,520
definitely be something that we start

00:31:50,820 --> 00:31:55,640
looking into the summer as we run more

00:31:53,520 --> 00:31:57,420
things in communities within Bloomberg

00:31:55,640 --> 00:31:58,860
there's additional operator

00:31:57,420 --> 00:32:00,660
functionality that we need such as

00:31:58,860 --> 00:32:01,830
backup and restoring which we're

00:32:00,660 --> 00:32:03,900
definitely gonna get around to this

00:32:01,830 --> 00:32:05,429
month and basically a lot of the

00:32:03,900 --> 00:32:07,590
different operators around there have

00:32:05,429 --> 00:32:09,179
backup restore capabilities that we kind

00:32:07,590 --> 00:32:11,820
of want to build a naval it natively to

00:32:09,179 --> 00:32:13,980
the solar cloud operator and we also

00:32:11,820 --> 00:32:15,929
want metrics to be provided kind of as

00:32:13,980 --> 00:32:18,300
easily as possible so spinning up a

00:32:15,929 --> 00:32:20,160
deployment of the Prometheus exporter

00:32:18,300 --> 00:32:23,010
would be awesome to do a long cities

00:32:20,160 --> 00:32:25,710
cloud we just need to kind of get that

00:32:23,010 --> 00:32:28,320
in there so that's basically what we're

00:32:25,710 --> 00:32:30,710
doing and what we've done give any

00:32:28,320 --> 00:32:30,710
questions

00:32:30,930 --> 00:32:39,170
[Applause]

00:32:49,860 --> 00:32:57,850
hi okay when you were talking about the

00:32:54,129 --> 00:33:01,059
ingress rules have you guys tried doing

00:32:57,850 --> 00:33:04,600
any routing based on collection name at

00:33:01,059 --> 00:33:06,669
the ingress rule level for like queries

00:33:04,600 --> 00:33:16,450
and updates going directly to the

00:33:06,669 --> 00:33:18,039
correct node a lot of running here so no

00:33:16,450 --> 00:33:19,749
we have not currently done anything

00:33:18,039 --> 00:33:24,429
around that that'd be saying that I

00:33:19,749 --> 00:33:27,789
would 100 100 percent B I think that'd

00:33:24,429 --> 00:33:28,749
be awesome to add to the operator the we

00:33:27,789 --> 00:33:31,029
have been working around some other

00:33:28,749 --> 00:33:33,909
things but this is open source so please

00:33:31,029 --> 00:33:51,960
help us if you want to have that in

00:33:33,909 --> 00:33:55,509
there thank you

00:33:51,960 --> 00:33:57,970
so you mentioned situations where our

00:33:55,509 --> 00:34:01,629
server dies or a pod dies or stuff like

00:33:57,970 --> 00:34:04,240
that but how do you tackle situations

00:34:01,629 --> 00:34:07,840
where the the service is not really dead

00:34:04,240 --> 00:34:10,690
so it just has like some issues it

00:34:07,840 --> 00:34:13,800
responds badly it's it's taking long a

00:34:10,690 --> 00:34:16,839
long time to respond it behaves

00:34:13,800 --> 00:34:20,319
unnaturally and these situations are

00:34:16,839 --> 00:34:22,060
more difficult to detect and they there

00:34:20,319 --> 00:34:26,250
are situation where you actually need to

00:34:22,060 --> 00:34:26,250
kill the server and to start on a drone

00:34:26,399 --> 00:34:34,929
and whoever wants to ask a question next

00:34:29,889 --> 00:34:36,460
should go right here that's a very good

00:34:34,929 --> 00:34:38,740
question and I agree a hundred percent

00:34:36,460 --> 00:34:41,409
there's a lot of issues with solar that

00:34:38,740 --> 00:34:45,579
isn't solar dies and these things are

00:34:41,409 --> 00:34:47,319
like om in garbage collection issues it

00:34:45,579 --> 00:34:51,190
can't connect to zookeeper these kind of

00:34:47,319 --> 00:34:57,910
things on the solar I mean the go back

00:34:51,190 --> 00:35:00,190
here the kind of POD specification lets

00:34:57,910 --> 00:35:01,270
you define different health checks and

00:35:00,190 --> 00:35:04,350
status checks so these be

00:35:01,270 --> 00:35:07,030
different things and so you can define

00:35:04,350 --> 00:35:08,950
health checks for your pods to say that

00:35:07,030 --> 00:35:10,870
if these things don't respond in the way

00:35:08,950 --> 00:35:12,100
that I want them to kill the pod but

00:35:10,870 --> 00:35:15,880
there's other things such as status

00:35:12,100 --> 00:35:17,620
check so if the I don't I'm not an

00:35:15,880 --> 00:35:18,550
expert in communities let me start with

00:35:17,620 --> 00:35:22,210
this so if any of you all have

00:35:18,550 --> 00:35:24,790
Corrections I will take them so you can

00:35:22,210 --> 00:35:27,430
define like kind of more granular things

00:35:24,790 --> 00:35:29,890
there so if the pods don't respond in a

00:35:27,430 --> 00:35:32,800
certain way also kill the pod it's not

00:35:29,890 --> 00:35:34,570
just like if this HCP endpoint is

00:35:32,800 --> 00:35:35,770
available keep it up in life there's a

00:35:34,570 --> 00:35:39,340
lot more things you can do around that

00:35:35,770 --> 00:35:41,350
and I think given the different versions

00:35:39,340 --> 00:35:43,360
of solar solar 8 has a better health

00:35:41,350 --> 00:35:45,310
check Handler than solar 7 and before

00:35:43,360 --> 00:35:46,510
where it actually checks whether it's

00:35:45,310 --> 00:35:49,020
connected to zookeeper in these

00:35:46,510 --> 00:35:51,400
different things if the course already

00:35:49,020 --> 00:35:52,510
and so there's different things that we

00:35:51,400 --> 00:35:54,580
can build into the operator and

00:35:52,510 --> 00:35:59,370
different ways that we can manage this

00:35:54,580 --> 00:35:59,370
health hopefully that answer

00:36:05,940 --> 00:36:09,930
okay I'm gonna give you two questions

00:36:07,770 --> 00:36:11,369
and you can answer either or both or

00:36:09,930 --> 00:36:13,829
whatever you want one is just about

00:36:11,369 --> 00:36:15,980
scaling limits if you've run into any or

00:36:13,829 --> 00:36:18,809
if you know like how big can you make

00:36:15,980 --> 00:36:21,150
each thing and then the other question

00:36:18,809 --> 00:36:23,400
is about if you've done anything about

00:36:21,150 --> 00:36:24,780
schema updates you know you need to

00:36:23,400 --> 00:36:26,369
reenacts your whole cluster do you

00:36:24,780 --> 00:36:34,980
provide any support for that in the

00:36:26,369 --> 00:36:38,160
operator so um that thing blinded me I

00:36:34,980 --> 00:36:40,319
forgot a lot but um the first question I

00:36:38,160 --> 00:36:42,109
ll enter the second question for schema

00:36:40,319 --> 00:36:44,819
updates and having to rien to cure data

00:36:42,109 --> 00:36:46,260
I'm not I think that there's some cheer

00:36:44,819 --> 00:36:48,270
tickets out there to make it better in

00:36:46,260 --> 00:36:49,890
solar itself there's not a whole lot

00:36:48,270 --> 00:36:55,940
that the operator does for you there

00:36:49,890 --> 00:36:58,710
because this is it's more of a kind of

00:36:55,940 --> 00:37:00,150
logical topology management - similar as

00:36:58,710 --> 00:37:02,490
the operators more the physical side of

00:37:00,150 --> 00:37:05,700
actually kind of managing your nodes

00:37:02,490 --> 00:37:07,559
your processes not the schemas within

00:37:05,700 --> 00:37:10,319
that's not saying that we can't build it

00:37:07,559 --> 00:37:11,819
in there it's just not something that's

00:37:10,319 --> 00:37:14,160
currently there and this is once again

00:37:11,819 --> 00:37:15,990
in its infancy we want to make it a lot

00:37:14,160 --> 00:37:17,490
more powerful and solve a lot more

00:37:15,990 --> 00:37:21,150
problems and I now remember the first

00:37:17,490 --> 00:37:22,829
question scale um so we've run solar

00:37:21,150 --> 00:37:25,890
clouds we haven't done like a whole lot

00:37:22,829 --> 00:37:28,770
of scaling with our operator but I know

00:37:25,890 --> 00:37:31,680
that we've run within two to kubernetes

00:37:28,770 --> 00:37:35,130
clusters 20 solar nodes each within the

00:37:31,680 --> 00:37:37,950
same cloud so 40 total which is far more

00:37:35,130 --> 00:37:39,869
than we run and not far more that's

00:37:37,950 --> 00:37:41,279
basically what we top out at Bloomberg

00:37:39,869 --> 00:37:43,589
right now so I know there's more of

00:37:41,279 --> 00:37:46,890
y'all that run thousands so nodes in a

00:37:43,589 --> 00:37:49,010
cloud and hopefully you can help us test

00:37:46,890 --> 00:37:52,309
that but I'm not sure how he'll do

00:37:49,010 --> 00:37:52,309
hopefully well

00:37:59,740 --> 00:38:02,980
you mentioned on the last slide that the

00:38:01,240 --> 00:38:05,410
like potential audit remote storage

00:38:02,980 --> 00:38:06,850
might potentially be too slow which type

00:38:05,410 --> 00:38:08,890
of storage do you currently use because

00:38:06,850 --> 00:38:11,110
like what we noticed briefly scene based

00:38:08,890 --> 00:38:12,520
application on top of Amazon is that

00:38:11,110 --> 00:38:15,190
they work really really fast if you use

00:38:12,520 --> 00:38:16,770
locally SSDs as soon as we one like a

00:38:15,190 --> 00:38:19,000
remote filesystem or something like this

00:38:16,770 --> 00:38:20,110
performance is completely like one order

00:38:19,000 --> 00:38:26,860
of magnitude Wells or something like

00:38:20,110 --> 00:38:29,290
this so yeah the reason I put the local

00:38:26,860 --> 00:38:30,850
persistent volumes in there before the

00:38:29,290 --> 00:38:34,120
remote storage is because we currently

00:38:30,850 --> 00:38:36,850
use local persistent volumes in general

00:38:34,120 --> 00:38:39,580
they're not really implemented very well

00:38:36,850 --> 00:38:41,380
in kubernetes right now I think they got

00:38:39,580 --> 00:38:44,710
in in the last two releases so they're

00:38:41,380 --> 00:38:46,750
pretty beta at this point and we have

00:38:44,710 --> 00:38:48,670
not done any remote storage yet that I

00:38:46,750 --> 00:38:50,580
think we're kind of looking to do that

00:38:48,670 --> 00:38:53,800
for the backup and restoring right now

00:38:50,580 --> 00:38:55,270
but in the future I don't see I don't

00:38:53,800 --> 00:38:56,860
think that we're gonna be running with

00:38:55,270 --> 00:38:59,050
anything other than local persistent

00:38:56,860 --> 00:39:00,430
volumes because I mean if you're using

00:38:59,050 --> 00:39:03,370
solo you want it to be pretty fast

00:39:00,430 --> 00:39:05,200
probably we haven't done any testing

00:39:03,370 --> 00:39:07,060
with remote storage yet we probably will

00:39:05,200 --> 00:39:16,690
but I'm not expecting very much out of

00:39:07,060 --> 00:39:18,490
it there any particular versions of

00:39:16,690 --> 00:39:20,560
solar that this would depend on or are

00:39:18,490 --> 00:39:25,000
there any upcoming features in solar

00:39:20,560 --> 00:39:27,580
that would benefit this as I mentioned

00:39:25,000 --> 00:39:29,740
before the like solar has a better kind

00:39:27,580 --> 00:39:34,330
of liveness check in solar aid and

00:39:29,740 --> 00:39:35,920
beyond but I think this works with any

00:39:34,330 --> 00:39:38,740
of the docker containers currently

00:39:35,920 --> 00:39:41,800
published which i think is solar 5 5

00:39:38,740 --> 00:39:46,330
something on I might be wrong with that

00:39:41,800 --> 00:39:49,780
another some 5 some 6m7 some 8 yeah we

00:39:46,330 --> 00:39:51,430
run it currently 7 and 8 and I I know

00:39:49,780 --> 00:39:53,950
that it works with those I think solar 8

00:39:51,430 --> 00:39:56,140
has better liveness as I said before but

00:39:53,950 --> 00:39:58,620
I'm not sure that there I've not seen

00:39:56,140 --> 00:40:01,840
any features that kind of directly

00:39:58,620 --> 00:40:04,690
contribute to this but I'm I might be

00:40:01,840 --> 00:40:10,059
wrong there thank you

00:40:04,690 --> 00:40:10,059

YouTube URL: https://www.youtube.com/watch?v=XUnYvmAeeNU


