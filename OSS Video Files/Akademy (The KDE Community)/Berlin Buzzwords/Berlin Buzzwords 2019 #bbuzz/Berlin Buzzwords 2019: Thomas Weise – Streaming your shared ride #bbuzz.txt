Title: Berlin Buzzwords 2019: Thomas Weise â€“ Streaming your shared ride #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.

Enablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.

Topics covered in this talk include:
-Overview of use cases and platform architecture
-Streaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping
-Stateful streaming computation with scalability, high availability and low latency processing on Apache Flink
-Development frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python
-Python with Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink
-Kubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications

Read more:
https://2019.berlinbuzzwords.de/19/session/streaming-your-shared-ride

About Thomas Weise:
https://2019.berlinbuzzwords.de/users/thomas-weise

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,310 --> 00:00:15,010
so the week

00:00:10,420 --> 00:00:17,890
of all then what we do in

00:00:15,010 --> 00:00:20,890
area an example for an application

00:00:17,890 --> 00:00:23,200
integrations connect us and finally also

00:00:20,890 --> 00:00:26,680
how we deploy things in our

00:00:23,200 --> 00:00:31,510
infrastructure so first of all data at

00:00:26,680 --> 00:00:34,510
lift is essential for the business to

00:00:31,510 --> 00:00:36,220
operate and specifically because if

00:00:34,510 --> 00:00:37,809
you're familiar is right sharing you

00:00:36,220 --> 00:00:40,870
have an application and you expect

00:00:37,809 --> 00:00:44,650
information to be up-to-date the goal is

00:00:40,870 --> 00:00:46,710
to match passengers with drivers and we

00:00:44,650 --> 00:00:49,449
want to ensure that our users both

00:00:46,710 --> 00:00:52,360
drivers and passengers have a good

00:00:49,449 --> 00:00:54,430
experience which means the ETA

00:00:52,360 --> 00:00:57,250
information that you see for your ride

00:00:54,430 --> 00:00:59,290
should be accurate and up-to-date the

00:00:57,250 --> 00:01:03,100
pricing should be fair and drivers

00:00:59,290 --> 00:01:06,430
should receive fair earnings and

00:01:03,100 --> 00:01:08,650
notifications should be timely so for

00:01:06,430 --> 00:01:10,120
also for the users we want to make sure

00:01:08,650 --> 00:01:12,220
that they are getting notified in a

00:01:10,120 --> 00:01:13,990
timely manner when there are changes in

00:01:12,220 --> 00:01:17,170
the conditions like traffic delays route

00:01:13,990 --> 00:01:20,320
changes and so on here you see a few

00:01:17,170 --> 00:01:21,790
examples for example pricing pricing can

00:01:20,320 --> 00:01:24,490
change and I'm going to say more about

00:01:21,790 --> 00:01:27,940
that use case later but also fraud

00:01:24,490 --> 00:01:30,880
detection is one area and then just the

00:01:27,940 --> 00:01:34,060
general area of notifications for users

00:01:30,880 --> 00:01:38,920
and the overall experience in this

00:01:34,060 --> 00:01:41,410
application so looking at the data we

00:01:38,920 --> 00:01:45,220
have as different user audiences so

00:01:41,410 --> 00:01:48,730
their data modelers analysts product

00:01:45,220 --> 00:01:50,770
managers and so on and in this talk

00:01:48,730 --> 00:01:52,810
we're going to focus more on two of

00:01:50,770 --> 00:01:56,530
these audiences data scientists and

00:01:52,810 --> 00:01:58,870
engineers the overall platform

00:01:56,530 --> 00:02:01,420
architecture it starts on the left side

00:01:58,870 --> 00:02:04,560
with the data that is being produced and

00:02:01,420 --> 00:02:08,819
being ingested stored in a pops-up

00:02:04,560 --> 00:02:11,530
platform in the form of streams then

00:02:08,819 --> 00:02:14,800
processed by a stream processing engine

00:02:11,530 --> 00:02:20,140
eventually landed in a permanent storage

00:02:14,800 --> 00:02:23,260
for offline analytics we also have you

00:02:20,140 --> 00:02:25,180
can see here tools like a spark and app

00:02:23,260 --> 00:02:28,480
flow and also flight which is another

00:02:25,180 --> 00:02:31,989
scheduler for ETL processing in

00:02:28,480 --> 00:02:34,629
and then finally to make the data

00:02:31,989 --> 00:02:37,810
accessible interactively to use us with

00:02:34,629 --> 00:02:40,510
query engines which presto is most

00:02:37,810 --> 00:02:42,310
workers into presto nowadays and then

00:02:40,510 --> 00:02:46,780
the front-end to is finally to make this

00:02:42,310 --> 00:02:51,280
accessible to users and our focus is on

00:02:46,780 --> 00:02:54,819
the streaming site so for the pops up

00:02:51,280 --> 00:02:59,140
system so the transport of messages from

00:02:54,819 --> 00:03:01,900
producer to consumer we are going from

00:02:59,140 --> 00:03:03,970
Kinesis to Kafka lyft traditionally had

00:03:01,900 --> 00:03:06,549
everything running on Kinesis and there

00:03:03,970 --> 00:03:08,310
have been challenges as business is

00:03:06,549 --> 00:03:11,769
growing and the data volumes are crowing

00:03:08,310 --> 00:03:15,130
also the requirements for faster

00:03:11,769 --> 00:03:18,069
processing also growing so one key issue

00:03:15,130 --> 00:03:21,099
is latency where Kinesis has a very high

00:03:18,069 --> 00:03:23,860
tail latency and when you compare that

00:03:21,099 --> 00:03:27,280
to Kafka with Kafka we can have much

00:03:23,860 --> 00:03:30,579
fast or durable writes and the emphasis

00:03:27,280 --> 00:03:34,090
is on durability we want to achieve a

00:03:30,579 --> 00:03:36,880
good latency SLA but also reliable

00:03:34,090 --> 00:03:39,160
system that we don't lose messages and

00:03:36,880 --> 00:03:40,090
with Kinesis it's hard you have to trade

00:03:39,160 --> 00:03:41,799
one for the other

00:03:40,090 --> 00:03:45,010
essentially with Kafka we don't have to

00:03:41,799 --> 00:03:47,410
make that choice the second part is the

00:03:45,010 --> 00:03:51,370
fan-out limitation fan-out limitation

00:03:47,410 --> 00:03:53,700
means that with how kindnesses is a set

00:03:51,370 --> 00:03:56,380
up and exposed to the users there is

00:03:53,700 --> 00:03:58,900
restrictions how much data can be taken

00:03:56,380 --> 00:04:01,930
out of a stream or a shot in the stream

00:03:58,900 --> 00:04:03,730
and because of that it's not possible to

00:04:01,930 --> 00:04:05,440
do a fan-out on a heist report stream

00:04:03,730 --> 00:04:07,980
because there can only be one consumer

00:04:05,440 --> 00:04:10,630
essentially with the limits that it has

00:04:07,980 --> 00:04:13,720
even though Kinesis makes some

00:04:10,630 --> 00:04:16,479
improvements with the new 2.0 api it's

00:04:13,720 --> 00:04:20,200
still limited to five consumers but what

00:04:16,479 --> 00:04:21,430
you see in a Kafka world very often is

00:04:20,200 --> 00:04:23,349
that you have many consumers for the

00:04:21,430 --> 00:04:26,020
same stream processing the data in

00:04:23,349 --> 00:04:28,630
different ways so that's something that

00:04:26,020 --> 00:04:32,020
we need to and finally the scalability

00:04:28,630 --> 00:04:33,669
limitation there are restrictions on the

00:04:32,020 --> 00:04:36,330
number of shots and I believe the

00:04:33,669 --> 00:04:38,770
default is 500 but in a scale of

00:04:36,330 --> 00:04:41,860
processing we have it lived we are in

00:04:38,770 --> 00:04:44,349
the thousands of shots you cannot easily

00:04:41,860 --> 00:04:46,569
increase those and it requires manual

00:04:44,349 --> 00:04:50,680
intervention it requires approvals also

00:04:46,569 --> 00:04:52,120
for the account by AWS so and at the end

00:04:50,680 --> 00:04:54,099
of the day when you increase the number

00:04:52,120 --> 00:04:56,289
of shots you also increase the cost so

00:04:54,099 --> 00:05:01,780
those are all problems that we can

00:04:56,289 --> 00:05:07,060
overcome with Kafka so for the streaming

00:05:01,780 --> 00:05:09,939
compute we are using fling and fling as

00:05:07,060 --> 00:05:14,729
different API it has a sequel of fling

00:05:09,939 --> 00:05:18,370
sequel API you can write Java flick jobs

00:05:14,729 --> 00:05:20,830
we are also using beam to make fling

00:05:18,370 --> 00:05:27,400
available to other languages I will say

00:05:20,830 --> 00:05:30,969
more about that later so with that we

00:05:27,400 --> 00:05:35,020
have a platform to define processing

00:05:30,969 --> 00:05:36,490
workloads in various ways and flink 10

00:05:35,020 --> 00:05:38,139
needs to be deployed in the

00:05:36,490 --> 00:05:40,479
infrastructure and that follows pretty

00:05:38,139 --> 00:05:43,409
much the tooling that's already there at

00:05:40,479 --> 00:05:49,000
lift currently for observability and

00:05:43,409 --> 00:05:51,279
other aspects and our deployment of link

00:05:49,000 --> 00:05:53,919
is currently standalone cluster that

00:05:51,279 --> 00:05:56,409
means we have instances on ec2 and then

00:05:53,919 --> 00:05:57,969
we put the flink processes on those the

00:05:56,409 --> 00:06:00,279
task managers and job managers but

00:05:57,969 --> 00:06:02,199
that's not the future the future of

00:06:00,279 --> 00:06:06,550
deployment is kubernetes which we will

00:06:02,199 --> 00:06:09,180
take a closer look in a bit so the flink

00:06:06,550 --> 00:06:12,699
abstraction levels that were shown there

00:06:09,180 --> 00:06:14,400
you can write jobs using sequel and a

00:06:12,699 --> 00:06:17,349
lot of effort has gone into sequel

00:06:14,400 --> 00:06:19,210
lately and this is going to continue and

00:06:17,349 --> 00:06:24,219
not just in fling but in other systems

00:06:19,210 --> 00:06:27,279
to a level below that is your Java

00:06:24,219 --> 00:06:30,520
program and now you write your data

00:06:27,279 --> 00:06:33,550
stream or data set in the case of batch

00:06:30,520 --> 00:06:36,339
flink application with that and requires

00:06:33,550 --> 00:06:38,919
programming right and then even more

00:06:36,339 --> 00:06:41,080
flexibility and access to the state and

00:06:38,919 --> 00:06:43,599
time and and auto details

00:06:41,080 --> 00:06:45,699
flink provides those abstraction levels

00:06:43,599 --> 00:06:47,770
too so depending on the use case you can

00:06:45,699 --> 00:06:51,849
go down and the level of abstraction and

00:06:47,770 --> 00:06:53,409
gain more flexibility for more work and

00:06:51,849 --> 00:06:56,520
a higher skill set that is obviously

00:06:53,409 --> 00:06:59,350
needed to work with it as well

00:06:56,520 --> 00:07:01,510
about categorizing the use cases that we

00:06:59,350 --> 00:07:04,330
have at lift broadly we can divide it

00:07:01,510 --> 00:07:06,880
into analytics steering analytics and

00:07:04,330 --> 00:07:10,000
then streaming applications and in

00:07:06,880 --> 00:07:14,590
analytics there are many use cases that

00:07:10,000 --> 00:07:17,470
have simple needs think of aggregation

00:07:14,590 --> 00:07:19,590
simple ETL simple ETL jobs where you

00:07:17,470 --> 00:07:22,120
just want to compute some aggregates and

00:07:19,590 --> 00:07:23,950
then store them somewhere so that can be

00:07:22,120 --> 00:07:25,780
expressed actually whistling sequel

00:07:23,950 --> 00:07:29,590
there's no need to write a Java program

00:07:25,780 --> 00:07:32,560
for it and then the fling table API also

00:07:29,590 --> 00:07:35,500
gains a lot of attention now and so

00:07:32,560 --> 00:07:39,310
let's equate to a set to define a large

00:07:35,500 --> 00:07:43,240
number of simple flink jobs without

00:07:39,310 --> 00:07:44,860
going into the programming realm many

00:07:43,240 --> 00:07:47,200
things are automatic if you use those

00:07:44,860 --> 00:07:50,170
API optimizations or done by the system

00:07:47,200 --> 00:07:54,010
state and triggers and time are managed

00:07:50,170 --> 00:07:58,080
automatically what you get as the user

00:07:54,010 --> 00:08:03,100
is a faster time implementation time and

00:07:58,080 --> 00:08:05,980
in the case of lift we have combined a

00:08:03,100 --> 00:08:08,320
fling sequel with a management platform

00:08:05,980 --> 00:08:10,000
that also takes care of all the

00:08:08,320 --> 00:08:10,510
deployment of linked jobs so that to the

00:08:10,000 --> 00:08:12,510
user

00:08:10,510 --> 00:08:15,370
there's no fleeing it's just a

00:08:12,510 --> 00:08:18,010
declaration for the job on the other

00:08:15,370 --> 00:08:20,500
side our applications that need

00:08:18,010 --> 00:08:23,040
programming complex use cases that are

00:08:20,500 --> 00:08:25,690
special that require special logic and

00:08:23,040 --> 00:08:27,790
more flexibility in the form of finer

00:08:25,690 --> 00:08:30,130
grained control and those would be

00:08:27,790 --> 00:08:32,710
written in Java the problem we have a

00:08:30,130 --> 00:08:34,990
lift is that most teams don't work with

00:08:32,710 --> 00:08:38,200
Java traditionally a lot of it was done

00:08:34,990 --> 00:08:41,020
this Python so and so beam is then the

00:08:38,200 --> 00:08:45,430
route to enable other languages on top

00:08:41,020 --> 00:08:49,570
of link so let's look at the analytics

00:08:45,430 --> 00:08:52,750
first the system we call it internally

00:08:49,570 --> 00:08:56,290
drift the goal of the system is to

00:08:52,750 --> 00:09:00,310
enable future generation broadly in a

00:08:56,290 --> 00:09:03,780
unified way for machine learning we want

00:09:00,310 --> 00:09:06,940
to have one system that can do that for

00:09:03,780 --> 00:09:09,400
bulk processing how to generate features

00:09:06,940 --> 00:09:11,320
for for training mostly

00:09:09,400 --> 00:09:14,740
but also for the real-time processing

00:09:11,320 --> 00:09:19,380
for the current events for scoring or

00:09:14,740 --> 00:09:22,660
model inference we are using sequel to

00:09:19,380 --> 00:09:24,970
the user is using sequel rather to

00:09:22,660 --> 00:09:26,920
define what is what those pipelines

00:09:24,970 --> 00:09:29,110
should do and then they give this job

00:09:26,920 --> 00:09:31,870
declaration to a fully managed service

00:09:29,110 --> 00:09:34,420
that will ensure that it runs gets

00:09:31,870 --> 00:09:37,510
deployed in the right way and runs in in

00:09:34,420 --> 00:09:39,130
a cluster so this is what you would see

00:09:37,510 --> 00:09:41,770
as a user in that case so there's a

00:09:39,130 --> 00:09:44,800
configuration file that describes where

00:09:41,770 --> 00:09:46,390
the sequel query is what the sources are

00:09:44,800 --> 00:09:48,430
and what the features are that are being

00:09:46,390 --> 00:09:49,900
created on the right side this is

00:09:48,430 --> 00:09:54,880
something that is flink sequel

00:09:49,900 --> 00:09:58,840
essentially so then there are two modes

00:09:54,880 --> 00:10:00,640
of execution one is batch quote/unquote

00:09:58,840 --> 00:10:03,130
because everything runs in streaming in

00:10:00,640 --> 00:10:04,960
this case but it's semantically patched

00:10:03,130 --> 00:10:08,710
because it's about the data set of past

00:10:04,960 --> 00:10:10,960
data or current data and in the in the

00:10:08,710 --> 00:10:14,410
case of processing all day that the data

00:10:10,960 --> 00:10:16,360
would come from s3 and the sync is maybe

00:10:14,410 --> 00:10:19,210
different from streaming it could also

00:10:16,360 --> 00:10:21,310
be s3 where the results are stored the

00:10:19,210 --> 00:10:23,590
sequel part in the middle is exactly the

00:10:21,310 --> 00:10:26,290
same now looking at the real-time pass

00:10:23,590 --> 00:10:29,890
the sources Kafka topic or Kinesis

00:10:26,290 --> 00:10:31,570
stream sequel statement remains same but

00:10:29,890 --> 00:10:33,280
the sync is something different maybe

00:10:31,570 --> 00:10:37,780
it's a new stream in Kafka or it's a

00:10:33,280 --> 00:10:39,220
dynamo DP or multiple options but

00:10:37,780 --> 00:10:42,310
there's also an interesting case where

00:10:39,220 --> 00:10:45,130
our historic data processing and

00:10:42,310 --> 00:10:47,560
real-time processing intersect and when

00:10:45,130 --> 00:10:50,920
you think of the scenario let's say you

00:10:47,560 --> 00:10:53,050
want to go back 60 days and you want to

00:10:50,920 --> 00:10:56,530
compute a feature that needs 60 days of

00:10:53,050 --> 00:11:00,040
data to do that you need 60 days of data

00:10:56,530 --> 00:11:02,380
now to get an answer immediately all

00:11:00,040 --> 00:11:05,220
right because we have to go back and in

00:11:02,380 --> 00:11:07,750
the past to even get an answer right now

00:11:05,220 --> 00:11:11,530
that cannot be done with the current

00:11:07,750 --> 00:11:13,840
state of pops up at least how we have it

00:11:11,530 --> 00:11:16,240
not always kinesin always Kafka because

00:11:13,840 --> 00:11:19,630
of data retention so what we need to do

00:11:16,240 --> 00:11:21,610
is we need to have a hybrid source which

00:11:19,630 --> 00:11:22,960
you see here part of the data comes from

00:11:21,610 --> 00:11:25,170
s3 and

00:11:22,960 --> 00:11:28,420
then at some point it cuts over to the

00:11:25,170 --> 00:11:31,600
head data that comes through the pop sub

00:11:28,420 --> 00:11:33,160
system and computes the features there

00:11:31,600 --> 00:11:35,080
are more details and the slides are

00:11:33,160 --> 00:11:37,450
shared so you can take a look at the

00:11:35,080 --> 00:11:40,840
presentation that is linked here our

00:11:37,450 --> 00:11:43,000
benefits of doing things this way is

00:11:40,840 --> 00:11:45,370
that we enable low latency computation

00:11:43,000 --> 00:11:47,890
and streaming data the teams that use it

00:11:45,370 --> 00:11:49,780
internally they have a faster onboarding

00:11:47,890 --> 00:11:55,240
they don't need to be Java programmers

00:11:49,780 --> 00:11:58,810
the development time is small and no

00:11:55,240 --> 00:12:02,140
awareness of deployment to the user they

00:11:58,810 --> 00:12:05,830
don't need to know how flink runs on the

00:12:02,140 --> 00:12:10,510
machines on on ec2 it's self-serve and

00:12:05,830 --> 00:12:12,910
the goal is to be reliable of course on

00:12:10,510 --> 00:12:16,480
the other hand application use case and

00:12:12,910 --> 00:12:18,250
this is now specialized because when we

00:12:16,480 --> 00:12:20,620
look at it we understand why I mean

00:12:18,250 --> 00:12:23,110
dynamic pricing is one use case dynamic

00:12:20,620 --> 00:12:25,900
pricing means that the price can change

00:12:23,110 --> 00:12:29,230
at a given location and time depending

00:12:25,900 --> 00:12:32,970
on the contextual information supply and

00:12:29,230 --> 00:12:36,550
demand which are continuously evaluated

00:12:32,970 --> 00:12:38,440
so if the market isn't balanced then

00:12:36,550 --> 00:12:40,930
it's not good for traverse and not good

00:12:38,440 --> 00:12:43,270
for passengers either because you may

00:12:40,930 --> 00:12:45,940
not get a car if there's too much demand

00:12:43,270 --> 00:12:49,030
there long wait times and drivers have

00:12:45,940 --> 00:12:51,970
no reason to fill the demand shortage

00:12:49,030 --> 00:12:56,200
because they get paid the same price so

00:12:51,970 --> 00:13:00,160
price is the level to bring this into

00:12:56,200 --> 00:13:02,110
balance so primetime we call this

00:13:00,160 --> 00:13:04,900
mechanism primetime primetime is a

00:13:02,110 --> 00:13:07,810
multiplier on the base price for a given

00:13:04,900 --> 00:13:10,270
location and time and to determine that

00:13:07,810 --> 00:13:12,280
multiplier we need to look at millions

00:13:10,270 --> 00:13:14,710
of geohashes so here it's the location

00:13:12,280 --> 00:13:16,750
is identified with the geo hash and we

00:13:14,710 --> 00:13:19,600
need to look at millions of geohashes to

00:13:16,750 --> 00:13:21,220
update this for every location because

00:13:19,600 --> 00:13:22,990
the conditions are different at every

00:13:21,220 --> 00:13:26,350
location and they change fast

00:13:22,990 --> 00:13:28,450
so it's scale and it's also latency that

00:13:26,350 --> 00:13:30,670
matters here

00:13:28,450 --> 00:13:34,209
in the legacy architecture this is a

00:13:30,670 --> 00:13:36,279
series of cron jobs that do stage

00:13:34,209 --> 00:13:38,290
processing so in the stage one

00:13:36,279 --> 00:13:41,380
some aggregation is performed then the

00:13:38,290 --> 00:13:43,660
results get stuck into Redis then

00:13:41,380 --> 00:13:46,420
another cron job at fixed interval kicks

00:13:43,660 --> 00:13:48,519
off takes those does some model

00:13:46,420 --> 00:13:51,519
computation when an already trained

00:13:48,519 --> 00:13:54,040
model again store some results into

00:13:51,519 --> 00:13:57,160
Redis a third phase kicks off computes

00:13:54,040 --> 00:13:59,139
the information for the pricing service

00:13:57,160 --> 00:14:02,380
and passes that on of course this means

00:13:59,139 --> 00:14:04,240
latency in every stage it's not data

00:14:02,380 --> 00:14:06,279
driven it's driven by a cron schedule

00:14:04,240 --> 00:14:08,709
and the system architecture is fairly

00:14:06,279 --> 00:14:15,750
brittle because it's it cannot be easily

00:14:08,709 --> 00:14:17,800
changed so and in addition to that

00:14:15,750 --> 00:14:21,120
adding new features is also difficult

00:14:17,800 --> 00:14:24,370
code complexity comes from the fact that

00:14:21,120 --> 00:14:26,800
we have to write code for all this

00:14:24,370 --> 00:14:28,510
scheduling and coordination that

00:14:26,800 --> 00:14:33,010
normally you would not find in a

00:14:28,510 --> 00:14:35,170
streaming pipeline so why not use

00:14:33,010 --> 00:14:38,250
streaming when the team looked at at

00:14:35,170 --> 00:14:40,990
first there was no pattern for flink

00:14:38,250 --> 00:14:42,699
flink has a Python API but it's tight

00:14:40,990 --> 00:14:45,190
and it's not really pi - no we need

00:14:42,699 --> 00:14:47,680
something where we can use the full C

00:14:45,190 --> 00:14:51,730
pite and ecosystem of libraries just as

00:14:47,680 --> 00:14:54,519
it existed already so what can we do

00:14:51,730 --> 00:14:57,029
about that is Apache beam source

00:14:54,519 --> 00:14:59,949
promises to solve that problem its

00:14:57,029 --> 00:15:01,360
promises to have language portability

00:14:59,949 --> 00:15:03,250
which means that you can choose the

00:15:01,360 --> 00:15:05,079
programming language that is appropriate

00:15:03,250 --> 00:15:07,000
for your use case now something like the

00:15:05,079 --> 00:15:08,470
dynamic pricing we don't want to write

00:15:07,000 --> 00:15:11,709
in Java because we already have the

00:15:08,470 --> 00:15:14,640
machine learning models and the people

00:15:11,709 --> 00:15:16,930
working on it don't even know Java so

00:15:14,640 --> 00:15:20,680
beam can help you the multi-language

00:15:16,930 --> 00:15:22,060
support became a reality over the past

00:15:20,680 --> 00:15:24,639
couple of years

00:15:22,060 --> 00:15:30,069
end of last year it was possible to run

00:15:24,639 --> 00:15:31,180
Python for the first time which is what

00:15:30,069 --> 00:15:32,649
we need at lift

00:15:31,180 --> 00:15:37,110
there is another SDK which is

00:15:32,649 --> 00:15:37,110
experimental write notes for go but

00:15:37,790 --> 00:15:45,569
in theory you can learn or what SDK at

00:15:42,119 --> 00:15:47,429
at the beam summit in a couple of days

00:15:45,569 --> 00:15:52,049
from the gentleman with the blue shirt

00:15:47,429 --> 00:15:54,179
in the back and so there are some

00:15:52,049 --> 00:15:56,069
choices in language right you pick what

00:15:54,179 --> 00:15:57,540
is best for your use case maybe you have

00:15:56,069 --> 00:15:59,549
already written code maybe you have some

00:15:57,540 --> 00:16:01,410
libraries and then you run it on your

00:15:59,549 --> 00:16:04,169
run off choice even if the runners like

00:16:01,410 --> 00:16:05,850
flink Java so that's the beam the

00:16:04,169 --> 00:16:07,769
promise of beam and we are making

00:16:05,850 --> 00:16:09,389
progress on that path if you write

00:16:07,769 --> 00:16:12,359
something in part and it looks like this

00:16:09,389 --> 00:16:15,419
so simple example reading from a text

00:16:12,359 --> 00:16:17,730
decide what windowing we want if you do

00:16:15,419 --> 00:16:20,009
an aggregation define how we want to

00:16:17,730 --> 00:16:21,660
trigger results with different types of

00:16:20,009 --> 00:16:24,179
triggers there are three examples here

00:16:21,660 --> 00:16:26,699
and the computation is just a summation

00:16:24,179 --> 00:16:28,799
and then write the results to text so

00:16:26,699 --> 00:16:32,369
just to give a feel of what you would

00:16:28,799 --> 00:16:34,379
see as a Python program and you can of

00:16:32,369 --> 00:16:36,269
course then plug your user code in

00:16:34,379 --> 00:16:39,980
instead of doing a summation you can

00:16:36,269 --> 00:16:43,220
perform some out of function call models

00:16:39,980 --> 00:16:47,730
use any library that you would like

00:16:43,220 --> 00:16:49,739
running this on fling looks on the right

00:16:47,730 --> 00:16:52,230
side you see a fling cluster but before

00:16:49,739 --> 00:16:54,539
we are ready to go to the flink lust or

00:16:52,230 --> 00:16:56,759
the client-side is a Titan program that

00:16:54,539 --> 00:17:00,749
you run like any other pattern program

00:16:56,759 --> 00:17:03,720
one parameter that identifies job server

00:17:00,749 --> 00:17:05,279
now this is a language boundary so

00:17:03,720 --> 00:17:09,829
you're going from the Python world to

00:17:05,279 --> 00:17:13,409
the Java world in the case of link and

00:17:09,829 --> 00:17:14,819
the way this works is that the pipeline

00:17:13,409 --> 00:17:17,279
that was written in Titan gets

00:17:14,819 --> 00:17:20,490
translated into a language agnostic

00:17:17,279 --> 00:17:24,149
format and photograph then it's being

00:17:20,490 --> 00:17:26,490
sent over that endpoint to the run to

00:17:24,149 --> 00:17:28,289
the job server which contains the run

00:17:26,490 --> 00:17:30,809
out of link run all the flink Runner

00:17:28,289 --> 00:17:32,519
then takes this portable pipeline

00:17:30,809 --> 00:17:36,090
definition and translates it into its

00:17:32,519 --> 00:17:39,750
own job graph becomes a flink job in the

00:17:36,090 --> 00:17:41,789
case of streaming a data stream job and

00:17:39,750 --> 00:17:43,649
this is being passed over to fling like

00:17:41,789 --> 00:17:48,629
a lot of link anything application would

00:17:43,649 --> 00:17:50,340
run but there is a pieces of Python in

00:17:48,629 --> 00:17:51,929
that job definition and they need to

00:17:50,340 --> 00:17:53,850
executed somehow they cannot run in the

00:17:51,929 --> 00:17:56,130
task manager so we have these SDK

00:17:53,850 --> 00:17:58,200
workers the separate processes that not

00:17:56,130 --> 00:18:01,470
just to run the Python code and they

00:17:58,200 --> 00:18:03,570
need to communicate with the flink side

00:18:01,470 --> 00:18:06,419
which is Java which runs in the task

00:18:03,570 --> 00:18:09,240
manager and that API is called FN

00:18:06,419 --> 00:18:11,789
services with various planes to drive

00:18:09,240 --> 00:18:13,770
the execution as records arrive they are

00:18:11,789 --> 00:18:15,720
being passed over to the SDK via our

00:18:13,770 --> 00:18:18,120
results go back from SDK worker to fling

00:18:15,720 --> 00:18:20,309
chrono fling chrono is responsible for

00:18:18,120 --> 00:18:22,049
the distribution it's also responsible

00:18:20,309 --> 00:18:23,669
for the state management for timer

00:18:22,049 --> 00:18:26,419
management and everything the worker is

00:18:23,669 --> 00:18:29,149
really just a like a stateless service

00:18:26,419 --> 00:18:35,100
that is disposable it just executes

00:18:29,149 --> 00:18:38,700
records or bundles in beam so after the

00:18:35,100 --> 00:18:40,679
pricing system switch to beam it looks

00:18:38,700 --> 00:18:44,010
conceptually like this it's simplified

00:18:40,679 --> 00:18:46,679
but still reading data from Kinesis and

00:18:44,010 --> 00:18:49,140
now for every record it passes that on

00:18:46,679 --> 00:18:52,590
to the next function is data driven is a

00:18:49,140 --> 00:18:54,059
pipeline filtering aggregation all of

00:18:52,590 --> 00:18:55,710
that can happen in incremental way

00:18:54,059 --> 00:18:59,580
there's no more cron scheduler here

00:18:55,710 --> 00:19:03,029
involved it's just the data processing

00:18:59,580 --> 00:19:05,070
graph and no intermediate state storage

00:19:03,029 --> 00:19:06,179
is required because the data flows

00:19:05,070 --> 00:19:08,580
through the pipeline

00:19:06,179 --> 00:19:15,210
instead of using Redis as a intermediate

00:19:08,580 --> 00:19:17,880
hop past data so with that they managed

00:19:15,210 --> 00:19:19,559
to get the latency reduction and the

00:19:17,880 --> 00:19:22,110
interesting part about the latency still

00:19:19,559 --> 00:19:24,029
looks high right 30 seconds but it's

00:19:22,110 --> 00:19:25,919
mostly bounded by the model execution

00:19:24,029 --> 00:19:29,039
now not by the system around it and not

00:19:25,919 --> 00:19:30,960
by the Krone scheduler or not by the

00:19:29,039 --> 00:19:33,600
streaming that is happening now it's the

00:19:30,960 --> 00:19:35,159
model execution which would need to be

00:19:33,600 --> 00:19:38,070
worked on to get the latency further

00:19:35,159 --> 00:19:41,279
down but it's possible the model code

00:19:38,070 --> 00:19:43,500
could be reused want no change to it but

00:19:41,279 --> 00:19:45,419
lines of code reduce because we got rid

00:19:43,500 --> 00:19:48,809
of all this boilerplate code just to

00:19:45,419 --> 00:19:51,570
stitch multiple jobs together and it was

00:19:48,809 --> 00:19:52,740
also with a reduction in the resources

00:19:51,570 --> 00:19:57,980
that are needed

00:19:52,740 --> 00:20:01,080
mr. AWS instances so now integrations

00:19:57,980 --> 00:20:02,630
integrations means the pieces that help

00:20:01,080 --> 00:20:04,150
you to get the data into your streaming

00:20:02,630 --> 00:20:06,970
application and all

00:20:04,150 --> 00:20:11,200
right at Howard sources and sinks in

00:20:06,970 --> 00:20:12,790
beam so we are using Flinx connect us

00:20:11,200 --> 00:20:14,500
even in beam they are using Flinx

00:20:12,790 --> 00:20:16,570
connectors because we did the work once

00:20:14,500 --> 00:20:19,630
to make those really work inside lift

00:20:16,570 --> 00:20:22,960
and now we try to capitalize on that

00:20:19,630 --> 00:20:25,420
effort so we have exposed the flink

00:20:22,960 --> 00:20:29,980
Kinesis consumer also as a beam source

00:20:25,420 --> 00:20:33,100
in our beam fork we are planning to do

00:20:29,980 --> 00:20:34,960
the same for consumer and producer but

00:20:33,100 --> 00:20:37,180
this is not only for beam this is for

00:20:34,960 --> 00:20:40,300
also for all Java pipelines we use the

00:20:37,180 --> 00:20:41,860
same set of connectors we need to read

00:20:40,300 --> 00:20:43,870
from cough-cough we need to write to

00:20:41,860 --> 00:20:46,090
Kafka we need to reach from s3 we need

00:20:43,870 --> 00:20:47,950
to write to s3 we also need to write to

00:20:46,090 --> 00:20:50,380
elasticsearch we need to consume from

00:20:47,950 --> 00:20:53,080
DynamoDB streams which is special flink

00:20:50,380 --> 00:20:54,910
and is consumer and then very important

00:20:53,080 --> 00:20:55,660
in the flink is checkpointing the check

00:20:54,910 --> 00:20:57,280
pointed

00:20:55,660 --> 00:21:00,010
state needs to be written somewhere

00:20:57,280 --> 00:21:03,970
which is s3 in the case in our case we

00:21:00,010 --> 00:21:06,600
had a lot of interesting experiences and

00:21:03,970 --> 00:21:11,830
a lot of learning with writing

00:21:06,600 --> 00:21:14,680
checkpoints to s3 well as three may look

00:21:11,830 --> 00:21:18,310
like a filesystem to fling but it's not

00:21:14,680 --> 00:21:20,010
a filesystem when it really works so

00:21:18,310 --> 00:21:22,150
here are some of the challenges

00:21:20,010 --> 00:21:24,700
generally connect us and flink are

00:21:22,150 --> 00:21:27,160
pretty good but usually you hit some

00:21:24,700 --> 00:21:29,730
bumps when it comes to certain

00:21:27,160 --> 00:21:32,410
production readiness aspects we found

00:21:29,730 --> 00:21:34,060
issues with observability which means

00:21:32,410 --> 00:21:36,610
that the metrics and the locks are

00:21:34,060 --> 00:21:38,650
helpful diagnosing issues also that we

00:21:36,610 --> 00:21:41,020
are able to configure everything that we

00:21:38,650 --> 00:21:42,870
need or the underlying client api

00:21:41,020 --> 00:21:45,550
configuration parameters are exposed

00:21:42,870 --> 00:21:47,110
sufficiently and then also performance

00:21:45,550 --> 00:21:50,230
when you run it at scale you find things

00:21:47,110 --> 00:21:52,720
that nobody has hit when you're the

00:21:50,230 --> 00:21:55,090
first one so in the case of Kinesis i

00:21:52,720 --> 00:21:58,750
think we were probably the first one

00:21:55,090 --> 00:22:00,460
running it at that scale level AWS

00:21:58,750 --> 00:22:03,820
integration is also a little bit

00:22:00,460 --> 00:22:05,920
interesting because it behaves in

00:22:03,820 --> 00:22:07,930
unexpected ways sometimes they are

00:22:05,920 --> 00:22:10,420
transient service errors that are

00:22:07,930 --> 00:22:12,640
bubbling up and if those are not handled

00:22:10,420 --> 00:22:14,170
correctly with retries then it means

00:22:12,640 --> 00:22:15,550
that your pipeline is interrupted every

00:22:14,170 --> 00:22:17,920
time such a thing happens it will

00:22:15,550 --> 00:22:19,540
basically fail recover from it

00:22:17,920 --> 00:22:21,580
point that means in the mean time does

00:22:19,540 --> 00:22:23,050
no processing which is of course not

00:22:21,580 --> 00:22:25,990
what we want so what we want is

00:22:23,050 --> 00:22:28,480
identified those retrial exceptions and

00:22:25,990 --> 00:22:32,740
then just continue with a with a retry

00:22:28,480 --> 00:22:34,630
and a small time out and dealing with s3

00:22:32,740 --> 00:22:38,500
is also interesting for checkpointing

00:22:34,630 --> 00:22:41,860
with large state because if you hit the

00:22:38,500 --> 00:22:45,100
same as three shot for many subtasks

00:22:41,860 --> 00:22:47,080
then you get the hot spotting symptom

00:22:45,100 --> 00:22:51,460
and there is a way to work around that

00:22:47,080 --> 00:22:54,090
with modifying augmenting the file path

00:22:51,460 --> 00:22:56,740
just to make it go over multiple shots

00:22:54,090 --> 00:23:00,160
that changed went into the last link

00:22:56,740 --> 00:23:02,890
release then also we need event time

00:23:00,160 --> 00:23:06,280
processing it's really important and so

00:23:02,890 --> 00:23:08,230
source water marking we added that to

00:23:06,280 --> 00:23:09,970
the kindnesses consumer in flink the

00:23:08,230 --> 00:23:11,760
kafka consumer has that already it

00:23:09,970 --> 00:23:14,140
wasn't there and the Kinesis consumer

00:23:11,760 --> 00:23:16,180
also that water marking is done

00:23:14,140 --> 00:23:18,310
correctly while reading from multiple

00:23:16,180 --> 00:23:20,440
shots merging data for multiple shots

00:23:18,310 --> 00:23:23,080
the correct computation of the watermark

00:23:20,440 --> 00:23:24,850
is actually can lead to surprising

00:23:23,080 --> 00:23:27,460
effects if that's not done in the source

00:23:24,850 --> 00:23:29,590
and watermarks q I'm going to say

00:23:27,460 --> 00:23:31,120
something about that problem because

00:23:29,590 --> 00:23:34,330
that's a seems to be a quite generic

00:23:31,120 --> 00:23:36,940
issue also I said at the beginning that

00:23:34,330 --> 00:23:38,740
was Kafka you can you can process a lot

00:23:36,940 --> 00:23:41,470
of data very fast which is true now you

00:23:38,740 --> 00:23:43,450
get the opposite issue with Kafka the

00:23:41,470 --> 00:23:45,460
bandwidth so can be so high to an

00:23:43,450 --> 00:23:47,800
individual consumer that the other

00:23:45,460 --> 00:23:50,620
consumers get stopped and then you have

00:23:47,800 --> 00:23:55,660
to think about rate controls on the

00:23:50,620 --> 00:23:59,050
consumer so the bottom oxq is the issue

00:23:55,660 --> 00:24:03,430
of the watermark of individual

00:23:59,050 --> 00:24:05,560
partitions and Kafka or shots in Chinese

00:24:03,430 --> 00:24:08,050
is not being Alliant that means one of

00:24:05,560 --> 00:24:11,860
those in this example gets consumed or

00:24:08,050 --> 00:24:14,890
gets consumed slower than the others and

00:24:11,860 --> 00:24:18,220
so you can see the red data is still

00:24:14,890 --> 00:24:20,170
being accumulated if this is a windowing

00:24:18,220 --> 00:24:22,630
use case because the watermark has not

00:24:20,170 --> 00:24:25,180
advanced but in partition two and three

00:24:22,630 --> 00:24:26,350
we are also reading newer data so what

00:24:25,180 --> 00:24:28,150
is happening here is that you are

00:24:26,350 --> 00:24:29,620
accumulating state in the flink

00:24:28,150 --> 00:24:32,350
application

00:24:29,620 --> 00:24:34,690
and if that is large scale and that skew

00:24:32,350 --> 00:24:36,430
becomes very large then it becomes a

00:24:34,690 --> 00:24:38,559
problem because the checkpoint state

00:24:36,430 --> 00:24:40,540
becomes so large the application might

00:24:38,559 --> 00:24:42,610
come to a grinding halt on checkpoint

00:24:40,540 --> 00:24:44,710
fail checkpointing and at some point not

00:24:42,610 --> 00:24:47,380
pauses at all and then there's also no

00:24:44,710 --> 00:24:49,270
way out of that because the consumer

00:24:47,380 --> 00:24:50,740
offsets or checkpoint and even if you

00:24:49,270 --> 00:24:54,070
restart you will still have the same

00:24:50,740 --> 00:24:55,930
issue so it is important that there is

00:24:54,070 --> 00:24:57,940
some synchronization mechanism and this

00:24:55,930 --> 00:25:01,420
queue can be caused by variety of

00:24:57,940 --> 00:25:03,280
factors it could be a difference in the

00:25:01,420 --> 00:25:06,220
speed of the consumers it could be a

00:25:03,280 --> 00:25:08,610
difference in the speed of the serving

00:25:06,220 --> 00:25:12,490
system like kafka protocols or Kinesis

00:25:08,610 --> 00:25:15,010
shots or it could also be the density of

00:25:12,490 --> 00:25:18,640
the data so it's it's important to have

00:25:15,010 --> 00:25:21,910
this ability to align so we implemented

00:25:18,640 --> 00:25:24,820
this mechanism of based on a global

00:25:21,910 --> 00:25:28,179
watermark that all subtasks can flink

00:25:24,820 --> 00:25:30,760
all consumers are aware of by knowing

00:25:28,179 --> 00:25:33,220
where everybody else is so the minimum

00:25:30,760 --> 00:25:36,730
of all the watermarks consumers can

00:25:33,220 --> 00:25:39,730
adjust the speed so consumer that runs

00:25:36,730 --> 00:25:42,100
ahead and say hey I need to slow down

00:25:39,730 --> 00:25:44,740
you know I need to pause I'm going to do

00:25:42,100 --> 00:25:46,240
this until others have caught up so

00:25:44,740 --> 00:25:49,960
there's a little bit of a stage sharing

00:25:46,240 --> 00:25:51,850
mechanism that is now part of link that

00:25:49,960 --> 00:25:56,650
we utilize this and then we have built

00:25:51,850 --> 00:25:59,020
this into it the Kinesis consumer to

00:25:56,650 --> 00:26:02,260
illustrate this on the left side you see

00:25:59,020 --> 00:26:04,570
the skew the effect of the skew so in

00:26:02,260 --> 00:26:06,160
the middle you see checkpoint size and

00:26:04,570 --> 00:26:08,940
you see as the application is running

00:26:06,160 --> 00:26:11,500
the checkpoint size is climbing and the

00:26:08,940 --> 00:26:13,059
exact same execution on the right side

00:26:11,500 --> 00:26:16,360
doesn't have that problem you see this

00:26:13,059 --> 00:26:18,190
is fairly flat the right side is with

00:26:16,360 --> 00:26:20,160
synchronization the left side is without

00:26:18,190 --> 00:26:23,050
synchronization and when you look at the

00:26:20,160 --> 00:26:27,429
iterator age in Kinesis you see that it

00:26:23,050 --> 00:26:31,000
the band widens for for the processing

00:26:27,429 --> 00:26:33,929
without synchronization because they are

00:26:31,000 --> 00:26:37,750
being consumed at different speeds and

00:26:33,929 --> 00:26:39,670
it over time it wide it widens and this

00:26:37,750 --> 00:26:41,590
is a replay scenario where there are a

00:26:39,670 --> 00:26:43,610
lot of data is read in a short amount of

00:26:41,590 --> 00:26:46,460
time

00:26:43,610 --> 00:26:49,669
and once it returns to the head read

00:26:46,460 --> 00:26:52,580
then it's flat but on the right side you

00:26:49,669 --> 00:26:56,380
can see that this band is narrow the

00:26:52,580 --> 00:27:00,049
checkpoint size is has a ceiling and

00:26:56,380 --> 00:27:01,490
that's what we want to see so that we

00:27:00,049 --> 00:27:06,049
can also replay large amounts of data

00:27:01,490 --> 00:27:08,179
without running into memory issues so we

00:27:06,049 --> 00:27:09,830
contributed that the flink the next

00:27:08,179 --> 00:27:11,390
release will have T synchronization in

00:27:09,830 --> 00:27:13,250
the Kinesis consumer we will built it

00:27:11,390 --> 00:27:15,710
also for the Kafka consumer it's a bit

00:27:13,250 --> 00:27:17,210
more work because Kafka consumer of link

00:27:15,710 --> 00:27:19,400
after consumer has a different consumer

00:27:17,210 --> 00:27:22,640
model compared to Kinesis consumer but

00:27:19,400 --> 00:27:25,580
in the future there's also flip 27 which

00:27:22,640 --> 00:27:29,660
will provide a new framework for sources

00:27:25,580 --> 00:27:31,760
in flink that will account for all of

00:27:29,660 --> 00:27:33,250
these requirements have a correct event

00:27:31,760 --> 00:27:38,390
time consumption

00:27:33,250 --> 00:27:40,400
so now deployment for deployment we are

00:27:38,390 --> 00:27:43,309
moving from our standalone flink

00:27:40,400 --> 00:27:45,590
deployment on ec2 instances to

00:27:43,309 --> 00:27:47,840
kubernetes so everything it lifts will

00:27:45,590 --> 00:27:51,280
run on kubernetes eventually which

00:27:47,840 --> 00:27:54,260
provides more stability more flexibility

00:27:51,280 --> 00:27:58,160
shorter wait times when it comes to

00:27:54,260 --> 00:27:59,840
starting up a new fling deployment

00:27:58,160 --> 00:28:02,059
because if Lync deployment is always two

00:27:59,840 --> 00:28:03,470
things even if there's just one job

00:28:02,059 --> 00:28:05,780
running on it it's always a fling

00:28:03,470 --> 00:28:08,240
cluster and then a job that is running

00:28:05,780 --> 00:28:10,160
in the fling cluster for us it mean

00:28:08,240 --> 00:28:11,720
always means you're only interested in

00:28:10,160 --> 00:28:15,830
one thing we don't want to run multiple

00:28:11,720 --> 00:28:19,669
flink jobs in one flink cluster which

00:28:15,830 --> 00:28:22,280
has other issues with isolation so the

00:28:19,669 --> 00:28:24,020
we are building a flink operator that is

00:28:22,280 --> 00:28:26,830
open source you can go and check it out

00:28:24,020 --> 00:28:29,840
and maybe even contribute to it

00:28:26,830 --> 00:28:31,130
kubernetes flink operator that

00:28:29,840 --> 00:28:33,880
understands how to manage Lync

00:28:31,130 --> 00:28:36,230
applications and supports its own

00:28:33,880 --> 00:28:38,870
deployment descriptor that describes the

00:28:36,230 --> 00:28:40,970
flink application so you can see an

00:28:38,870 --> 00:28:44,179
example here this is very simple of

00:28:40,970 --> 00:28:46,510
course but it's a custom resource

00:28:44,179 --> 00:28:49,820
descriptor that is understood by the

00:28:46,510 --> 00:28:51,500
flink Kinesis operator only which is

00:28:49,820 --> 00:28:53,150
always active in the company this

00:28:51,500 --> 00:28:56,000
cluster

00:28:53,150 --> 00:28:56,269
it describes a single job the all the

00:28:56,000 --> 00:28:58,609
applet

00:28:56,269 --> 00:29:00,499
code is packaged in a docker image that

00:28:58,609 --> 00:29:02,779
token which contains all dependencies

00:29:00,499 --> 00:29:04,879
and we are using source to image

00:29:02,779 --> 00:29:07,669
internally to produce this from there's

00:29:04,879 --> 00:29:09,679
a base image and a user's add the

00:29:07,669 --> 00:29:12,349
application code to it and to get an

00:29:09,679 --> 00:29:14,239
application image that is then being

00:29:12,349 --> 00:29:19,669
specified in the customer sauce

00:29:14,239 --> 00:29:23,749
descriptor and any time this descriptor

00:29:19,669 --> 00:29:25,219
changes the operator has to update so it

00:29:23,749 --> 00:29:26,659
could be any change it could be a change

00:29:25,219 --> 00:29:28,249
in the parallelism but it also could be

00:29:26,659 --> 00:29:29,570
a change in the code and this operator

00:29:28,249 --> 00:29:31,519
needs to understand how the update of

00:29:29,570 --> 00:29:33,109
think job fling shop is different from a

00:29:31,519 --> 00:29:35,749
micro service micro service you can just

00:29:33,109 --> 00:29:38,179
change the replicas and everything will

00:29:35,749 --> 00:29:39,799
happen in the right way if link job is a

00:29:38,179 --> 00:29:42,019
stateful application so we need to do

00:29:39,799 --> 00:29:45,649
some more work here there's an example

00:29:42,019 --> 00:29:48,079
so the operator the kubernetes operator

00:29:45,649 --> 00:29:51,649
will detect a change in the CID it will

00:29:48,079 --> 00:29:53,299
now go into nap updating State and that

00:29:51,649 --> 00:29:54,739
means it will have to take a safe point

00:29:53,299 --> 00:29:56,479
because we want to carry over the state

00:29:54,739 --> 00:29:59,359
from the current running application to

00:29:56,479 --> 00:30:01,519
the new job that will be launched in

00:29:59,359 --> 00:30:03,589
parallel it will create already the new

00:30:01,519 --> 00:30:05,419
fling cluster which is the new set of

00:30:03,589 --> 00:30:07,309
task manager and job manager processes

00:30:05,419 --> 00:30:09,739
because we don't want to wait later

00:30:07,309 --> 00:30:13,519
while it's safe finding it can already

00:30:09,739 --> 00:30:16,219
create this new flink deployment when

00:30:13,519 --> 00:30:19,039
the safe pointing completes then we have

00:30:16,219 --> 00:30:21,349
everything that we needed to start the

00:30:19,039 --> 00:30:23,119
new version of the job so the old one is

00:30:21,349 --> 00:30:25,309
now canceled the safe point was written

00:30:23,119 --> 00:30:27,440
the new fling shop comes up from that

00:30:25,309 --> 00:30:30,969
safe point on the new set of processes

00:30:27,440 --> 00:30:34,039
and so that would be a stateful update

00:30:30,969 --> 00:30:38,809
the user should not be aware of the

00:30:34,039 --> 00:30:42,019
different phases that this that is

00:30:38,809 --> 00:30:43,579
contains we also currently have

00:30:42,019 --> 00:30:46,009
deployment tooling that is doing similar

00:30:43,579 --> 00:30:48,919
things but with the kubernetes operator

00:30:46,009 --> 00:30:53,570
this is nicely encapsulated and more

00:30:48,919 --> 00:30:57,679
predictable so finally last beam submit

00:30:53,570 --> 00:30:59,509
is coming up in day after tomorrow

00:30:57,679 --> 00:31:02,479
so if you want to learn more about beam

00:30:59,509 --> 00:31:04,879
then please stop by and any of the beam

00:31:02,479 --> 00:31:07,179
things that I quickly run through here

00:31:04,879 --> 00:31:09,440
you will get great amount of detail

00:31:07,179 --> 00:31:10,130
there's also a beam summit coming up in

00:31:09,440 --> 00:31:13,309
North America

00:31:10,130 --> 00:31:16,880
in September in Las Vegas at Apache con

00:31:13,309 --> 00:31:19,429
or the 20 patchy con so for those that

00:31:16,880 --> 00:31:21,260
came over here from North America or

00:31:19,429 --> 00:31:24,830
those that want to go there this might

00:31:21,260 --> 00:31:28,400
be a good thing to bookmark and we have

00:31:24,830 --> 00:31:39,789
a few minutes left for questions right

00:31:28,400 --> 00:31:39,789
that's pink the speaker other questions

00:31:40,390 --> 00:31:43,789
so I have a question regarding your

00:31:42,380 --> 00:31:46,909
connectors you mentioned you might you

00:31:43,789 --> 00:31:48,919
use fling connectors on top of beam how

00:31:46,909 --> 00:31:50,510
do you enable developers like to test

00:31:48,919 --> 00:31:52,580
and debug their code to do usually have

00:31:50,510 --> 00:31:54,020
a locally instance running but you have

00:31:52,580 --> 00:31:56,510
like another connector that they can use

00:31:54,020 --> 00:31:59,840
with dileep rima wanna yeah this is a

00:31:56,510 --> 00:32:01,640
good question so the way it is currently

00:31:59,840 --> 00:32:04,640
works the team that is working with beam

00:32:01,640 --> 00:32:07,130
they do indeed run the application they

00:32:04,640 --> 00:32:09,140
have for unit testing they have a way to

00:32:07,130 --> 00:32:10,970
stop this and replace it with something

00:32:09,140 --> 00:32:14,000
else for example you can read from a

00:32:10,970 --> 00:32:16,340
file file can also be like a stream and

00:32:14,000 --> 00:32:18,169
you can read some test data so this way

00:32:16,340 --> 00:32:20,270
you can test the logic without actually

00:32:18,169 --> 00:32:21,260
having to have that connector that's one

00:32:20,270 --> 00:32:23,419
way of doing it

00:32:21,260 --> 00:32:27,470
beam gives you other options you can

00:32:23,419 --> 00:32:29,299
replace you've got to learn more at beam

00:32:27,470 --> 00:32:31,640
summit about it you can replace any

00:32:29,299 --> 00:32:34,250
transform with something else of your

00:32:31,640 --> 00:32:38,150
choice by just redefining how your end

00:32:34,250 --> 00:32:39,830
gets translated that requires surgery on

00:32:38,150 --> 00:32:42,679
the runner this is what we do when we

00:32:39,830 --> 00:32:44,270
see is lift Kinesis consumer then we

00:32:42,679 --> 00:32:47,210
know what to put in we plug in the flink

00:32:44,270 --> 00:32:49,370
chineses consumer so for if you have a

00:32:47,210 --> 00:32:52,039
logo that's an interesting path because

00:32:49,370 --> 00:32:55,309
you can do pretty much anything but the

00:32:52,039 --> 00:32:57,770
route forward for beam itself would be

00:32:55,309 --> 00:33:00,260
cross language transform where you can

00:32:57,770 --> 00:33:03,049
use you write your pipeline your

00:33:00,260 --> 00:33:06,169
transformation code in Python and then

00:33:03,049 --> 00:33:09,049
for sources you have the option to use

00:33:06,169 --> 00:33:11,270
an existing beam Java IO their quotes

00:33:09,049 --> 00:33:15,169
and there's an example that we will show

00:33:11,270 --> 00:33:18,760
day after tomorrow for Kafka for reading

00:33:15,169 --> 00:33:21,400
from Kafka and for writing to Kafka

00:33:18,760 --> 00:33:25,180
there's no questions

00:33:21,400 --> 00:33:27,370
I don't wanna talk oh yeah so you were

00:33:25,180 --> 00:33:30,040
talking about how you want to unify have

00:33:27,370 --> 00:33:32,920
the feature engineering for both serving

00:33:30,040 --> 00:33:34,630
and training that sounded very much like

00:33:32,920 --> 00:33:38,050
you're building a feature story you

00:33:34,630 --> 00:33:40,090
didn't use that term and tf-x

00:33:38,050 --> 00:33:42,520
which is kind of the as you know the the

00:33:40,090 --> 00:33:43,809
layer on top of beam link for building

00:33:42,520 --> 00:33:45,100
machine learning pipelines doesn't have

00:33:43,809 --> 00:33:47,620
a feature store can you say anything

00:33:45,100 --> 00:33:48,850
about your roadmap and what way you

00:33:47,620 --> 00:33:50,770
think it's going is it going the

00:33:48,850 --> 00:33:52,420
features or way or that P FX pipeline

00:33:50,770 --> 00:33:54,070
way well we are not doing that because

00:33:52,420 --> 00:33:56,590
you're already doing it now I'm just

00:33:54,070 --> 00:33:58,450
joking so we do have a feature service

00:33:56,590 --> 00:34:02,950
which is a separate system and lift

00:33:58,450 --> 00:34:04,900
so the flink jobs that are running there

00:34:02,950 --> 00:34:06,670
and using sequel as a specification they

00:34:04,900 --> 00:34:08,740
will eventually output also one of the

00:34:06,670 --> 00:34:10,929
things is actually feature store well

00:34:08,740 --> 00:34:13,480
these things are being stored and then

00:34:10,929 --> 00:34:14,950
consumed but you're right I mean this

00:34:13,480 --> 00:34:17,560
could be an interesting path in the

00:34:14,950 --> 00:34:22,290
future to do something like this based

00:34:17,560 --> 00:34:25,330
on existing tools but this was created

00:34:22,290 --> 00:34:28,510
years ago and this current feature

00:34:25,330 --> 00:34:30,250
service we call it internally probably

00:34:28,510 --> 00:34:32,470
at some point it will be touched when

00:34:30,250 --> 00:34:41,830
the systems that are coming up or made

00:34:32,470 --> 00:34:44,550
sure enough yeah more questions there's

00:34:41,830 --> 00:34:44,550
money in the center

00:34:46,480 --> 00:34:53,359
I'm about the analytics parts you said

00:34:50,080 --> 00:34:54,980
some people can write Taipan only using

00:34:53,359 --> 00:34:57,680
sequel so that's great because more

00:34:54,980 --> 00:35:00,410
people can write pipelines do you have

00:34:57,680 --> 00:35:02,359
any safeguards in case they don't fully

00:35:00,410 --> 00:35:04,100
understand what's being behind or it

00:35:02,359 --> 00:35:05,540
just like the trap is going to be super

00:35:04,100 --> 00:35:09,440
long so they're going to figure out it's

00:35:05,540 --> 00:35:11,420
not the ideal way well it's really the

00:35:09,440 --> 00:35:13,550
question is whether you can express what

00:35:11,420 --> 00:35:15,740
you want to do with sequel really then

00:35:13,550 --> 00:35:17,630
that's rather that's basically

00:35:15,740 --> 00:35:19,820
independent of fleeing can i express the

00:35:17,630 --> 00:35:22,220
computation that I want to do there with

00:35:19,820 --> 00:35:24,470
a sequel and in many cases the answer is

00:35:22,220 --> 00:35:26,330
yes at least for us what what was

00:35:24,470 --> 00:35:28,580
written previously was Python for

00:35:26,330 --> 00:35:31,760
example doing just simple aggregations

00:35:28,580 --> 00:35:33,680
summations with no special trigger

00:35:31,760 --> 00:35:36,500
conditions and so on all of that can be

00:35:33,680 --> 00:35:38,480
expressed with sequel and most of the

00:35:36,500 --> 00:35:42,590
flink jobs are actually that we have at

00:35:38,480 --> 00:35:45,109
live today or in that category and the

00:35:42,590 --> 00:35:48,170
cloister is much faster because when you

00:35:45,109 --> 00:35:50,150
write a full-fledged beam with beam

00:35:48,170 --> 00:35:52,190
that's the case too but a blink Java job

00:35:50,150 --> 00:35:54,980
there's a quite a hurdle you have to

00:35:52,190 --> 00:35:57,859
have somebody very familiar with Java in

00:35:54,980 --> 00:35:59,560
flink first of all and also you have to

00:35:57,859 --> 00:36:01,580
have somebody who understands the

00:35:59,560 --> 00:36:03,980
deployment when you open this wide range

00:36:01,580 --> 00:36:07,070
of options how you write the code and

00:36:03,980 --> 00:36:09,020
what you have access to the flipside of

00:36:07,070 --> 00:36:12,650
that is that there are many more things

00:36:09,020 --> 00:36:16,280
that can go wrong and so somebody has to

00:36:12,650 --> 00:36:19,510
support that so yes I mean the use cases

00:36:16,280 --> 00:36:22,190
that can be served to sequel or many and

00:36:19,510 --> 00:36:24,260
the ones where you want to go down the

00:36:22,190 --> 00:36:26,210
programming route or hopefully few

00:36:24,260 --> 00:36:30,140
because it requires way more work and

00:36:26,210 --> 00:36:31,670
time all right very good

00:36:30,140 --> 00:36:33,780
don't know that's I think the speak

00:36:31,670 --> 00:36:35,270
again

00:36:33,780 --> 00:36:39,469
thank you

00:36:35,270 --> 00:36:39,469

YouTube URL: https://www.youtube.com/watch?v=2R0RXRH2eD4


