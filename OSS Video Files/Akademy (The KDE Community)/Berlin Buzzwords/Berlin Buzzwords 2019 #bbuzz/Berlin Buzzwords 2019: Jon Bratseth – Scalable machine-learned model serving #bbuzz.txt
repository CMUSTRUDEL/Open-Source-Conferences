Title: Berlin Buzzwords 2019: Jon Bratseth â€“ Scalable machine-learned model serving #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Applying machine learning in online applications requires solving the problem of model serving: Evaluating the machine-learned model over some data point(s) in real time while the user is waiting for a response. Solutions such as TensorFlow Serving are available to solve this problem where the model only needs to be evaluated over a one data point per user request, but what about the case where a model needs to be evaluated over many data points per request, such as in search and recommendation systems? 

This talk will show that this is a bandwidth constrained problem, and outline an architectural solution where computation is pushed down to data shards in parallel. It will demonstrate how this solution can be put into use with Vespa.ai - the open source big data serving engine - to achieve scalable model serving of TensorFlow and ONNX and show benchmarks comparing performance and scalability to TensorFlow Serving. 

Model serving with Vespa is used today for some of the worlds largest recommendation systems, such as serving personalized content on all Yahoo content pages, personalized ads in the worlds third largest ad network, and image search and retrieval by similarity in Flickr. These systems evaluate models over millions of data points per request for hundreds of thousands of requests per second.

Read more:
https://2019.berlinbuzzwords.de/19/session/scalable-machine-learned-model-serving

About Jon Bratseth:
https://2019.berlinbuzzwords.de/users/jon-bratseth

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,790 --> 00:00:11,990
all right I'm here

00:00:10,080 --> 00:00:15,320
what I call

00:00:11,990 --> 00:00:22,540
serving and

00:00:15,320 --> 00:00:26,599
that solves this problem in open-source

00:00:22,540 --> 00:00:30,169
so first time be talking about what I

00:00:26,599 --> 00:00:31,730
mean by big data serving what's the use

00:00:30,169 --> 00:00:33,820
cases what's the problems things like

00:00:31,730 --> 00:00:36,980
that and then I'll be talking about

00:00:33,820 --> 00:00:38,449
lesbo which is the open source engine we

00:00:36,980 --> 00:00:41,269
have created to solve these kind of

00:00:38,449 --> 00:00:43,580
problems and then as time permits I'll

00:00:41,269 --> 00:00:46,580
go deeper into architecture and

00:00:43,580 --> 00:00:48,829
capabilities and using this puzzle this

00:00:46,580 --> 00:00:52,149
is a lot of slide so a larder be talking

00:00:48,829 --> 00:00:55,430
really fast or skipping some parts I

00:00:52,149 --> 00:00:59,059
like to start by introduced by talking

00:00:55,430 --> 00:01:03,649
about the big data maturity level when

00:00:59,059 --> 00:01:06,410
organizations start out they usually

00:01:03,649 --> 00:01:07,580
create a lot of data in logs and so on

00:01:06,410 --> 00:01:09,890
but they don't actually use it for

00:01:07,580 --> 00:01:12,340
anything systematically that's what I

00:01:09,890 --> 00:01:15,890
call the latent phase at some point they

00:01:12,340 --> 00:01:19,280
understand that it's useful to look at

00:01:15,890 --> 00:01:21,799
these data systematically to use it as

00:01:19,280 --> 00:01:25,990
for decision-making so that's the

00:01:21,799 --> 00:01:28,369
analysis stage but they're still users

00:01:25,990 --> 00:01:30,740
actual employees looking at the data

00:01:28,369 --> 00:01:32,810
creating reports and so on right then

00:01:30,740 --> 00:01:36,979
you get to the learning stage where you

00:01:32,810 --> 00:01:40,220
try to automatically learn insights from

00:01:36,979 --> 00:01:42,070
your data right which is what people are

00:01:40,220 --> 00:01:45,259
doing with machine learning obviously

00:01:42,070 --> 00:01:47,420
them at last you get to the acting phase

00:01:45,259 --> 00:01:52,149
where you try to make decisions in real

00:01:47,420 --> 00:01:56,000
time based on your data and there's two

00:01:52,149 --> 00:01:58,579
subcategories so that one is what coal

00:01:56,000 --> 00:02:00,290
either stream processing or model

00:01:58,579 --> 00:02:03,170
serving depending on whether you do

00:02:00,290 --> 00:02:06,530
request response or streams and that is

00:02:03,170 --> 00:02:09,679
where it's efficient to look at a single

00:02:06,530 --> 00:02:11,810
data item to make a decision the other

00:02:09,679 --> 00:02:14,090
problem is where you need to look at

00:02:11,810 --> 00:02:17,750
many data items to make a single

00:02:14,090 --> 00:02:21,799
decision and that's what I mean by big

00:02:17,750 --> 00:02:23,989
data serving right so if we do a simple

00:02:21,799 --> 00:02:26,840
example down here which is something

00:02:23,989 --> 00:02:28,580
like Netflix a movie streaming sites at

00:02:26,840 --> 00:02:30,800
first maybe you just

00:02:28,580 --> 00:02:33,320
serve the movies and your log which

00:02:30,800 --> 00:02:35,240
users are looking at which movie then at

00:02:33,320 --> 00:02:38,450
some point you go to the analysis stage

00:02:35,240 --> 00:02:40,490
and you start to do analytics to

00:02:38,450 --> 00:02:42,290
discover what kind of users are looking

00:02:40,490 --> 00:02:44,660
at what movies so that you can create

00:02:42,290 --> 00:02:48,890
curated lists or movies to recommend

00:02:44,660 --> 00:02:51,050
right at some point you want to stop

00:02:48,890 --> 00:02:53,150
paying people to do that job and you

00:02:51,050 --> 00:02:56,000
start using machine learning to create

00:02:53,150 --> 00:02:59,480
those lists for you but you still do it

00:02:56,000 --> 00:03:02,120
offline right you either or module

00:02:59,480 --> 00:03:03,890
persons like that you look to all your

00:03:02,120 --> 00:03:05,270
data and you generate all this list of

00:03:03,890 --> 00:03:06,950
recommendations for different kinds of

00:03:05,270 --> 00:03:09,590
users and then you move into your

00:03:06,950 --> 00:03:12,260
serving system and you serve them then

00:03:09,590 --> 00:03:14,209
the last stage of the acting phase you

00:03:12,260 --> 00:03:17,270
defer the decision about what to

00:03:14,209 --> 00:03:19,700
recommend to every single user to the

00:03:17,270 --> 00:03:26,150
point where the user needs to

00:03:19,700 --> 00:03:28,070
recommendations and there are many good

00:03:26,150 --> 00:03:30,620
reasons to do that right

00:03:28,070 --> 00:03:33,230
if you defer the decision-making until

00:03:30,620 --> 00:03:35,360
the users actually need the decision to

00:03:33,230 --> 00:03:37,459
be made you can use all the up-to-date

00:03:35,360 --> 00:03:40,670
information you have both about the user

00:03:37,459 --> 00:03:42,560
about the data all the information you

00:03:40,670 --> 00:03:45,920
have can go into that decision-making

00:03:42,560 --> 00:03:48,650
it's not Auto date right secondly you

00:03:45,920 --> 00:03:51,080
don't waste computation because you

00:03:48,650 --> 00:03:52,340
never equally compute something that

00:03:51,080 --> 00:03:55,220
you're not actually going to need

00:03:52,340 --> 00:03:59,090
because you do it only in response to

00:03:55,220 --> 00:04:02,600
something happening right for that same

00:03:59,090 --> 00:04:04,970
reason your decision-making can be much

00:04:02,600 --> 00:04:07,750
more fine-grain in the movie example

00:04:04,970 --> 00:04:14,480
right you can move from recommending

00:04:07,750 --> 00:04:16,840
different lists of videos to adults tech

00:04:14,480 --> 00:04:19,519
interested people or whatever to

00:04:16,840 --> 00:04:21,380
creating a separate list for every

00:04:19,519 --> 00:04:23,600
single user right something that would

00:04:21,380 --> 00:04:28,070
be too expensive in most cases if you do

00:04:23,600 --> 00:04:31,810
it eagerly upfront and from an

00:04:28,070 --> 00:04:31,810
architectural point of view if you can

00:04:31,840 --> 00:04:37,250
tweet the system that do the real-time

00:04:34,970 --> 00:04:38,890
decision-making as a black box the your

00:04:37,250 --> 00:04:42,140
architecture becomes much simpler

00:04:38,890 --> 00:04:44,180
because usually to work around those

00:04:42,140 --> 00:04:47,300
problems I just mentioned you end up

00:04:44,180 --> 00:04:49,850
with a quite complicated system where

00:04:47,300 --> 00:04:51,440
you have some batch processes that learn

00:04:49,850 --> 00:04:53,330
something and smaller processes that

00:04:51,440 --> 00:04:55,220
move that data to your serving system

00:04:53,330 --> 00:04:59,230
but then it's out of date so you create

00:04:55,220 --> 00:05:02,270
additional systems on top that immense

00:04:59,230 --> 00:05:04,070
out of date decisions you made with

00:05:02,270 --> 00:05:04,880
real-time information that you put on

00:05:04,070 --> 00:05:09,050
top and things like that

00:05:04,880 --> 00:05:11,330
right so well that goes away if you just

00:05:09,050 --> 00:05:14,120
commit to doing everything with that

00:05:11,330 --> 00:05:15,920
right but why aren't everybody doing

00:05:14,120 --> 00:05:17,990
this it's because it's really hard right

00:05:15,920 --> 00:05:20,810
and why is it hard it's because you

00:05:17,990 --> 00:05:24,110
combine a bunch of things that are hard

00:05:20,810 --> 00:05:27,650
to do on scale into one single thing so

00:05:24,110 --> 00:05:30,110
we sort of get multiplication of the

00:05:27,650 --> 00:05:33,920
difficulties so you have you're dealing

00:05:30,110 --> 00:05:35,110
with state obviously because you need to

00:05:33,920 --> 00:05:39,130
store the state that you do

00:05:35,110 --> 00:05:43,910
decision-making over and because you are

00:05:39,130 --> 00:05:46,220
fanning out because you need to compute

00:05:43,910 --> 00:05:49,550
over low latency budget over lots of

00:05:46,220 --> 00:05:51,320
data you need to fan out to multiple

00:05:49,550 --> 00:05:52,910
nodes working in parallel so you had the

00:05:51,320 --> 00:05:56,570
problem of doing scale together with a

00:05:52,910 --> 00:05:58,460
low latency budget and you need since

00:05:56,570 --> 00:06:00,350
you're on line serving directly to users

00:05:58,460 --> 00:06:05,960
you need this whole thing to have high

00:06:00,350 --> 00:06:08,000
availability right so the features you

00:06:05,960 --> 00:06:10,520
need you need to be able to find data

00:06:08,000 --> 00:06:14,240
and make inferences in typically a few

00:06:10,520 --> 00:06:16,850
tens of milliseconds you need at the

00:06:14,240 --> 00:06:21,410
same time to be able to have real-time

00:06:16,850 --> 00:06:23,240
updates at a high continuous rate you

00:06:21,410 --> 00:06:25,520
need to be able to have large request

00:06:23,240 --> 00:06:27,290
rates over the data because you're

00:06:25,520 --> 00:06:29,450
dealing directly with end-users and

00:06:27,290 --> 00:06:31,490
there's a lot more end-users than for

00:06:29,450 --> 00:06:34,640
example employees right so you need to

00:06:31,490 --> 00:06:36,410
be able to handle high request rates and

00:06:34,640 --> 00:06:39,440
as I mention you need to be always

00:06:36,410 --> 00:06:42,340
available because your end-users out in

00:06:39,440 --> 00:06:44,630
the world typically won't tolerate

00:06:42,340 --> 00:06:47,120
downtime or it won't be good for your

00:06:44,630 --> 00:06:48,950
company and for the same reason you need

00:06:47,120 --> 00:06:50,560
everything to be able to evolve while

00:06:48,950 --> 00:06:54,200
it's serving you need to be able to

00:06:50,560 --> 00:06:56,030
change your schemas change your logic

00:06:54,200 --> 00:06:58,220
components running in the system

00:06:56,030 --> 00:07:00,470
change your data change the hardware's

00:06:58,220 --> 00:07:05,420
running on everything without ever being

00:07:00,470 --> 00:07:08,090
down right and since this is just a

00:07:05,420 --> 00:07:09,560
serving part of dealing with big data

00:07:08,090 --> 00:07:12,110
problems you need to integrate with

00:07:09,560 --> 00:07:14,270
other parts of the stack like machine

00:07:12,110 --> 00:07:19,430
learning systems offline processing

00:07:14,270 --> 00:07:21,290
systems and so on so how can we solve

00:07:19,430 --> 00:07:24,890
this problem we can use less power which

00:07:21,290 --> 00:07:29,090
is open source platform open source on

00:07:24,890 --> 00:07:33,740
restful AI that delivers the features

00:07:29,090 --> 00:07:37,010
are just mentioned for you best poles

00:07:33,740 --> 00:07:40,190
actually started a long long time ago

00:07:37,010 --> 00:07:42,919
and the problem we were attacking back

00:07:40,190 --> 00:07:45,020
in those days were web search this was

00:07:42,919 --> 00:07:48,880
before Google so there was lots of

00:07:45,020 --> 00:07:51,560
different web search engines right so

00:07:48,880 --> 00:07:54,620
search and in particular web search is a

00:07:51,560 --> 00:07:59,240
prototypical big data serving use case

00:07:54,620 --> 00:08:01,280
right because there are close to an

00:07:59,240 --> 00:08:04,280
infinite number of potentially user

00:08:01,280 --> 00:08:06,340
queries you can't compute the answer to

00:08:04,280 --> 00:08:10,250
every query that you'll see up front

00:08:06,340 --> 00:08:12,650
obviously and because the relevance is

00:08:10,250 --> 00:08:15,100
really important you need to be able to

00:08:12,650 --> 00:08:17,390
compute machine learning models over the

00:08:15,100 --> 00:08:19,240
documents that you're matching so you

00:08:17,390 --> 00:08:22,310
have that part of the problem as well

00:08:19,240 --> 00:08:24,020
and you need low latency because users

00:08:22,310 --> 00:08:27,169
really care about latency once you get

00:08:24,020 --> 00:08:32,419
past about 400 milliseconds and gem

00:08:27,169 --> 00:08:35,030
right and luckily there at some point

00:08:32,419 --> 00:08:37,339
starting at the turn of the century

00:08:35,030 --> 00:08:39,940
there was a lot of money flowing into

00:08:37,339 --> 00:08:42,560
web search mint which means we could use

00:08:39,940 --> 00:08:45,980
lots of resources to solve these

00:08:42,560 --> 00:08:48,620
difficult problems which is why the big

00:08:45,980 --> 00:08:53,240
data surveying systems go Google to web

00:08:48,620 --> 00:08:58,339
search right so we worked on these

00:08:53,240 --> 00:09:01,250
problems in Yahoo search and we made two

00:08:58,339 --> 00:09:04,280
systems at the same time more or less

00:09:01,250 --> 00:09:08,690
the same time to solve these problems

00:09:04,280 --> 00:09:12,320
and they both had the same basic idea

00:09:08,690 --> 00:09:15,290
rather than looking up data and sending

00:09:12,320 --> 00:09:21,890
it somewhere for computation which is

00:09:15,290 --> 00:09:24,770
what you typically do in in two-tier

00:09:21,890 --> 00:09:28,190
systems that needs to work on data right

00:09:24,770 --> 00:09:29,540
you create a representation of the

00:09:28,190 --> 00:09:31,100
computation that you want to make and

00:09:29,540 --> 00:09:35,960
you ship the computation to the data

00:09:31,100 --> 00:09:36,650
right so we created Hadoop to do exactly

00:09:35,960 --> 00:09:38,810
that

00:09:36,650 --> 00:09:42,850
on the offline side and rest but to do

00:09:38,810 --> 00:09:42,850
it on the online side right

00:09:42,880 --> 00:09:49,130
Hadoop was open source from the

00:09:45,830 --> 00:09:51,470
beginning while Vesper we were unable to

00:09:49,130 --> 00:09:57,010
support to open source because of the

00:09:51,470 --> 00:10:00,230
complex IP rights around search but

00:09:57,010 --> 00:10:06,770
finally about one and a half year ago we

00:10:00,230 --> 00:10:11,150
were able to open source it so the

00:10:06,770 --> 00:10:14,510
company I know best which uses Vespa is

00:10:11,150 --> 00:10:17,390
my own company which used to be Yahoo

00:10:14,510 --> 00:10:22,610
and is now called Verizon media so we

00:10:17,390 --> 00:10:24,800
have a cloud system a cloud service

00:10:22,610 --> 00:10:28,150
running Vespa for all the use cases in

00:10:24,800 --> 00:10:31,400
this company so it's a couple hundred

00:10:28,150 --> 00:10:34,250
applications running Vespa we all run

00:10:31,400 --> 00:10:37,520
them from my team in seven data centers

00:10:34,250 --> 00:10:41,360
around the world in total we serve over

00:10:37,520 --> 00:10:44,300
a billion users and about 250,000

00:10:41,360 --> 00:10:46,490
queries per second and one of the use

00:10:44,300 --> 00:10:48,220
cases here is the third largest ad

00:10:46,490 --> 00:10:52,160
networks in the work

00:10:48,220 --> 00:10:54,410
nobody likes ads but they are what makes

00:10:52,160 --> 00:10:56,210
it possible for poor people to use all

00:10:54,410 --> 00:10:59,180
the services on the internet for free so

00:10:56,210 --> 00:11:01,070
they have some episode as well so one

00:10:59,180 --> 00:11:03,770
example use case actually you can see

00:11:01,070 --> 00:11:05,720
two here on this page this I talked

00:11:03,770 --> 00:11:08,360
about movie recommendation and this is

00:11:05,720 --> 00:11:10,900
another case of recommendation if a user

00:11:08,360 --> 00:11:15,770
visitors we see it's one of the Yahoo

00:11:10,900 --> 00:11:18,350
patients he or she gets a list of videos

00:11:15,770 --> 00:11:20,150
and articles and those videos and

00:11:18,350 --> 00:11:22,490
articles are computed on the fly by

00:11:20,150 --> 00:11:26,959
RESPA based on the personalized profile

00:11:22,490 --> 00:11:29,060
that user and you can also see what's

00:11:26,959 --> 00:11:30,709
typically called a native ad in there

00:11:29,060 --> 00:11:35,930
which is a different application of

00:11:30,709 --> 00:11:39,380
WestBow that do kind of the same thing

00:11:35,930 --> 00:11:40,940
but additional stuff for doing a

00:11:39,380 --> 00:11:45,620
real-time auction and so on which is

00:11:40,940 --> 00:11:48,260
also interesting so so far I've been

00:11:45,620 --> 00:11:50,540
talking about two use cases so a big

00:11:48,260 --> 00:11:53,330
data serving platform one may search

00:11:50,540 --> 00:11:56,690
where the query is typically some

00:11:53,330 --> 00:11:58,520
keywords and the model machine learn

00:11:56,690 --> 00:12:01,910
model you want to evaluate is what

00:11:58,520 --> 00:12:03,770
computes the relevance and the items the

00:12:01,910 --> 00:12:05,990
data you actually want to return are the

00:12:03,770 --> 00:12:08,000
ones that have the highest relevant

00:12:05,990 --> 00:12:10,160
score according to your machine learn

00:12:08,000 --> 00:12:13,550
real ones model right and then you have

00:12:10,160 --> 00:12:15,310
the recommendation use case where the

00:12:13,550 --> 00:12:19,279
query is typically just some filters

00:12:15,310 --> 00:12:23,270
specifying what kind of data is eligible

00:12:19,279 --> 00:12:26,899
for that particular user or market or

00:12:23,270 --> 00:12:29,230
whatever and then particular is also

00:12:26,899 --> 00:12:31,490
some typically machine learn

00:12:29,230 --> 00:12:34,940
representation of the user like a vector

00:12:31,490 --> 00:12:37,010
or tensor embedding and your machine

00:12:34,940 --> 00:12:41,149
normal is your recommendation model

00:12:37,010 --> 00:12:43,490
obviously and again you just want to

00:12:41,149 --> 00:12:45,470
return on items that have the highest

00:12:43,490 --> 00:12:47,510
score it's actually typically a bit more

00:12:45,470 --> 00:12:48,920
complicated because you have diversity

00:12:47,510 --> 00:12:52,399
that you want the factory nothing's like

00:12:48,920 --> 00:12:56,630
that so those are the two obvious use

00:12:52,399 --> 00:12:59,630
cases that are keeping us busy at the

00:12:56,630 --> 00:13:02,720
moment but it's my belief that there are

00:12:59,630 --> 00:13:05,450
lots and lots of other use cases for

00:13:02,720 --> 00:13:07,250
this kind of thing where you can compute

00:13:05,450 --> 00:13:09,649
machine learning models over lots of

00:13:07,250 --> 00:13:14,690
data items with a latency budget of less

00:13:09,649 --> 00:13:16,279
than 100 milliseconds and now that we

00:13:14,690 --> 00:13:17,810
have a platform that can solve this kind

00:13:16,279 --> 00:13:20,180
of problems we can probably apply it to

00:13:17,810 --> 00:13:22,870
a lot more use cases so I just want to

00:13:20,180 --> 00:13:25,850
mention one example which is not

00:13:22,870 --> 00:13:30,290
entirely made up just obvious k to the

00:13:25,850 --> 00:13:33,540
beat say you have data items which are

00:13:30,290 --> 00:13:40,529
some kind of assets for example stocks

00:13:33,540 --> 00:13:46,769
and you have a machine learn predict the

00:13:40,529 --> 00:13:50,040
price of these assets based on a lot of

00:13:46,769 --> 00:13:51,959
data about each of the assets and about

00:13:50,040 --> 00:13:55,740
other data what other things happening

00:13:51,959 --> 00:13:59,639
in the world now you can create a query

00:13:55,740 --> 00:14:03,240
that supplies some of the values for

00:13:59,639 --> 00:14:06,420
this predictive model namely the stuff

00:14:03,240 --> 00:14:08,970
that's happening in the world and then

00:14:06,420 --> 00:14:11,430
you can evaluate that machine or model

00:14:08,970 --> 00:14:14,910
for each of your stocks and then find a

00:14:11,430 --> 00:14:17,220
new price where given your query which

00:14:14,910 --> 00:14:19,350
is sort of an update to what's happening

00:14:17,220 --> 00:14:23,029
in the work right and then you can

00:14:19,350 --> 00:14:27,839
select the items that have the biggest

00:14:23,029 --> 00:14:29,730
difference in price predicted from your

00:14:27,839 --> 00:14:32,339
old world state and a new world state

00:14:29,730 --> 00:14:35,550
that you send in the query and then as a

00:14:32,339 --> 00:14:39,000
result you are able to quicker than

00:14:35,550 --> 00:14:41,639
anybody else compute what's going to

00:14:39,000 --> 00:14:44,339
happen across all your all the assets

00:14:41,639 --> 00:14:48,149
that you're interested in the stocks if

00:14:44,339 --> 00:14:49,889
some event is happening and you can take

00:14:48,149 --> 00:14:52,680
your event stream with things that are

00:14:49,889 --> 00:14:55,260
actually happening and find the biggest

00:14:52,680 --> 00:14:56,910
price mover the the stocks that will

00:14:55,260 --> 00:14:58,860
move the most in response to these

00:14:56,910 --> 00:15:01,639
events and do that faster than anybody

00:14:58,860 --> 00:15:04,410
else and that's actually very valuable

00:15:01,639 --> 00:15:07,170
so this is just an example that you can

00:15:04,410 --> 00:15:09,060
if you abstract a problem like this you

00:15:07,170 --> 00:15:11,610
can probably come up with lots of

00:15:09,060 --> 00:15:15,990
examples so how to use an engine like

00:15:11,610 --> 00:15:22,560
this so one question I often get about

00:15:15,990 --> 00:15:27,360
respo is about is big data serving and

00:15:22,560 --> 00:15:29,910
RESPA for analytics and to me analytics

00:15:27,360 --> 00:15:33,540
is different problems it just definitely

00:15:29,910 --> 00:15:35,220
overlap and you can more or less if you

00:15:33,540 --> 00:15:38,279
have something that's created for one

00:15:35,220 --> 00:15:41,790
you can usually make it work for the

00:15:38,279 --> 00:15:43,410
other because of the OLAP but since the

00:15:41,790 --> 00:15:44,939
design points are different it will be

00:15:43,410 --> 00:15:47,250
harder to do one thing or the other and

00:15:44,939 --> 00:15:50,280
in particular harder to

00:15:47,250 --> 00:15:52,590
make it performance right so in

00:15:50,280 --> 00:15:56,070
analytics you typically have response

00:15:52,590 --> 00:15:57,780
times in low seconds because your users

00:15:56,070 --> 00:16:01,170
or employees and they can tolerate

00:15:57,780 --> 00:16:03,600
waiting a bit right while we're big data

00:16:01,170 --> 00:16:05,850
serving because you're dealing with end

00:16:03,600 --> 00:16:08,040
users typically your latency budget is

00:16:05,850 --> 00:16:09,570
less than a hundred milliseconds for

00:16:08,040 --> 00:16:13,680
some reason you have low workwear rate

00:16:09,570 --> 00:16:16,080
with analytics usually you are dealing

00:16:13,680 --> 00:16:17,940
with time series data which means you

00:16:16,080 --> 00:16:22,140
can use special optimizations that we

00:16:17,940 --> 00:16:23,940
assume we cannot do at least in respond

00:16:22,140 --> 00:16:29,070
because we support random writes to all

00:16:23,940 --> 00:16:30,930
the data and high availability is much

00:16:29,070 --> 00:16:32,940
more important in big data serving as I

00:16:30,930 --> 00:16:35,250
mentioned so all of those sort things

00:16:32,940 --> 00:16:37,620
that are better in something like Vespa

00:16:35,250 --> 00:16:40,050
but the flipside is that since we can't

00:16:37,620 --> 00:16:43,110
do a lot of optimizations that you can

00:16:40,050 --> 00:16:45,270
do with analytics it gets more expensive

00:16:43,110 --> 00:16:50,250
once you get to the trillions of

00:16:45,270 --> 00:16:52,860
documents ok so that was a brief

00:16:50,250 --> 00:16:54,960
introduction to the problem or Big Data

00:16:52,860 --> 00:17:00,120
serving now I'll move to talking a bit

00:16:54,960 --> 00:17:01,800
about how Vespa solstice problem so a

00:17:00,120 --> 00:17:04,680
bit more detail on the features that

00:17:01,800 --> 00:17:09,930
westphall provides it provides text

00:17:04,680 --> 00:17:11,130
search and structured data selection at

00:17:09,930 --> 00:17:14,690
the same time you know

00:17:11,130 --> 00:17:17,520
represented in a query it supports

00:17:14,690 --> 00:17:18,959
evaluating machine Luhrmann's scoring

00:17:17,520 --> 00:17:22,170
relevance inference things like that

00:17:18,959 --> 00:17:25,140
using natural language features advanced

00:17:22,170 --> 00:17:29,490
machine learning models integration with

00:17:25,140 --> 00:17:33,750
terms of flow and so on you can query

00:17:29,490 --> 00:17:37,920
time organize and aggregate data across

00:17:33,750 --> 00:17:40,350
all the data that is selected in your

00:17:37,920 --> 00:17:41,790
query without actually sending all the

00:17:40,350 --> 00:17:45,030
data to one place and doing the

00:17:41,790 --> 00:17:48,930
aggregation and organization and you can

00:17:45,030 --> 00:17:51,960
do all of this while you sustain high

00:17:48,930 --> 00:17:54,630
wheel time right way so typically you

00:17:51,960 --> 00:17:56,250
can do from a few thousand to a few tens

00:17:54,630 --> 00:17:58,880
of thousands of writes per second per

00:17:56,250 --> 00:17:58,880
node sustained

00:17:58,940 --> 00:18:04,409
the clusters are elastic and also

00:18:01,860 --> 00:18:06,450
recovering so if you loosen all the data

00:18:04,409 --> 00:18:08,279
will automatically rebalance and if you

00:18:06,450 --> 00:18:11,179
add or remove hardware the data will

00:18:08,279 --> 00:18:14,340
automatically rebalance in the same way

00:18:11,179 --> 00:18:17,940
as part of the architecture is also

00:18:14,340 --> 00:18:22,470
stateless Java container of people used

00:18:17,940 --> 00:18:25,080
to plug in their own data and query and

00:18:22,470 --> 00:18:28,740
result processing logic and because

00:18:25,080 --> 00:18:31,320
these systems can be large you end up

00:18:28,740 --> 00:18:33,299
with lots of clusters lots of nodes and

00:18:31,320 --> 00:18:36,690
processes and so on and setting up all

00:18:33,299 --> 00:18:39,210
of that manually is just way too hard

00:18:36,690 --> 00:18:41,730
so the clusters are managed and what end

00:18:39,210 --> 00:18:45,450
users are seeing is more abstracted

00:18:41,730 --> 00:18:49,380
representation of the system you want to

00:18:45,450 --> 00:18:51,419
run so architecture on the highest level

00:18:49,380 --> 00:18:52,799
let's place a two-tier system as I

00:18:51,419 --> 00:18:55,260
mentioned there's a stateless Java

00:18:52,799 --> 00:19:00,419
container tier on top where you can plug

00:18:55,260 --> 00:19:02,610
in your own logic and we're also runs

00:19:00,419 --> 00:19:04,980
the stateless part of the logic of

00:19:02,610 --> 00:19:07,730
executing queries sending incoming data

00:19:04,980 --> 00:19:10,080
and so on and then we have what we call

00:19:07,730 --> 00:19:13,950
content nodes in content clusters which

00:19:10,080 --> 00:19:17,639
are all implemented in C++ because we

00:19:13,950 --> 00:19:20,399
don't really like limitations and

00:19:17,639 --> 00:19:23,909
problems of working with lots and lots

00:19:20,399 --> 00:19:27,539
so data in a single process in Java so

00:19:23,909 --> 00:19:29,690
this part is all in C++ and this is the

00:19:27,539 --> 00:19:33,480
part where we actually stores the data

00:19:29,690 --> 00:19:37,580
managers redistribution execute the

00:19:33,480 --> 00:19:39,539
distributed porch or queries and so on

00:19:37,580 --> 00:19:41,659
lastly as I mention we have an

00:19:39,539 --> 00:19:44,210
administration and configuration

00:19:41,659 --> 00:19:47,970
subsystem which is another cluster over

00:19:44,210 --> 00:19:50,850
zookeeper that manages the other

00:19:47,970 --> 00:19:52,860
clusters for you and what the user is

00:19:50,850 --> 00:19:53,940
seeing is what we call an application

00:19:52,860 --> 00:19:55,679
package which is a high-level

00:19:53,940 --> 00:19:58,139
description of the system that you want

00:19:55,679 --> 00:20:01,529
to run the clusters what features should

00:19:58,139 --> 00:20:06,530
they have and so on as well as your Java

00:20:01,529 --> 00:20:09,169
components machine learn models all that

00:20:06,530 --> 00:20:10,880
so all that is what we got an

00:20:09,169 --> 00:20:13,400
application package which is deployed to

00:20:10,880 --> 00:20:16,100
Vespa to the administration and

00:20:13,400 --> 00:20:18,110
configuration system which will then set

00:20:16,100 --> 00:20:19,730
up the system for you if you change the

00:20:18,110 --> 00:20:22,580
application package and deploy again the

00:20:19,730 --> 00:20:24,500
system will change to accommodate the

00:20:22,580 --> 00:20:28,840
changes you made without taking anything

00:20:24,500 --> 00:20:32,960
down or anything like that

00:20:28,840 --> 00:20:37,669
so the key point the key purpose of ESPA

00:20:32,960 --> 00:20:40,039
is to achieve low latency computation

00:20:37,669 --> 00:20:44,000
over data and there are three strategies

00:20:40,039 --> 00:20:47,380
we use for that which is quite obvious

00:20:44,000 --> 00:20:51,470
one is parallelization the queries are

00:20:47,380 --> 00:20:53,630
scattered to a bunch of content nodes in

00:20:51,470 --> 00:20:55,929
parallel to execute over charts of the

00:20:53,630 --> 00:21:02,470
data on each of the nodes we dynamically

00:20:55,929 --> 00:21:05,840
short over many course working separate

00:21:02,470 --> 00:21:08,570
subspaces of the data while exchanging

00:21:05,840 --> 00:21:12,020
some information about what subspace to

00:21:08,570 --> 00:21:16,909
work on next and things like that

00:21:12,020 --> 00:21:18,710
secondly we prepare data structures at

00:21:16,909 --> 00:21:22,750
right time as well as in the background

00:21:18,710 --> 00:21:25,669
to make read time faster and the

00:21:22,750 --> 00:21:28,429
simplest example that you know well that

00:21:25,669 --> 00:21:31,580
is reversing this is for text search

00:21:28,429 --> 00:21:35,360
right but also other examples of this

00:21:31,580 --> 00:21:38,000
like using a level DB light structured

00:21:35,360 --> 00:21:39,950
for the raw data so that you end up with

00:21:38,000 --> 00:21:43,250
more and more sorted data as it goes

00:21:39,950 --> 00:21:45,169
older things like that and as I mention

00:21:43,250 --> 00:21:48,020
we move as much of the execution as

00:21:45,169 --> 00:21:49,640
possible to the data nodes so for

00:21:48,020 --> 00:21:51,919
example if you want to evaluate a

00:21:49,640 --> 00:21:54,080
machine learn model that model a spark

00:21:51,919 --> 00:21:58,360
to your application package when you

00:21:54,080 --> 00:22:00,830
deploy it the model is copied to all the

00:21:58,360 --> 00:22:03,380
content nodes so that we can evaluate

00:22:00,830 --> 00:22:05,559
machine learning models locally on all

00:22:03,380 --> 00:22:07,789
the content nodes in parallel without

00:22:05,559 --> 00:22:10,760
shipping the data anywhere for

00:22:07,789 --> 00:22:15,010
computation right and just to drive that

00:22:10,760 --> 00:22:20,290
point home here is an example where we

00:22:15,010 --> 00:22:22,690
compare scalability or using in terms of

00:22:20,290 --> 00:22:26,790
serving with Vespa for serving the same

00:22:22,690 --> 00:22:29,890
model the partitions here are different

00:22:26,790 --> 00:22:31,720
content nodes once we add more content

00:22:29,890 --> 00:22:35,260
nodes with less power you get more or

00:22:31,720 --> 00:22:37,660
less linear scaling while with

00:22:35,260 --> 00:22:39,630
tensorflow what you need to do is to

00:22:37,660 --> 00:22:43,540
look up the data and Vespa and then

00:22:39,630 --> 00:22:46,570
evaluate your tensor flow model in the

00:22:43,540 --> 00:22:49,360
state list here which very quickly runs

00:22:46,570 --> 00:22:51,430
into the bandwidth bottleneck at which

00:22:49,360 --> 00:22:54,310
point it doesn't help you at all to add

00:22:51,430 --> 00:22:56,590
more content partitions so your data

00:22:54,310 --> 00:22:59,680
doesn't really your computation doesn't

00:22:56,590 --> 00:23:02,410
really scale with the data anymore while

00:22:59,680 --> 00:23:06,790
we the best way it does so with more

00:23:02,410 --> 00:23:10,930
about all Vespa is implemented under

00:23:06,790 --> 00:23:13,390
hood we use in most cases apart from

00:23:10,930 --> 00:23:15,130
some specific optimizations which was

00:23:13,390 --> 00:23:16,930
called document at the time emulation

00:23:15,130 --> 00:23:19,120
overall the query operators in your

00:23:16,930 --> 00:23:24,460
query and that's because we want to be

00:23:19,120 --> 00:23:28,510
able to compute nonlinear relevance

00:23:24,460 --> 00:23:30,970
models we have two kinds of fields we

00:23:28,510 --> 00:23:33,280
have index fields which is used for text

00:23:30,970 --> 00:23:38,020
search where you have positional text

00:23:33,280 --> 00:23:40,740
synthesis for the old data and we use be

00:23:38,020 --> 00:23:45,990
trees in memory for the recent changes

00:23:40,740 --> 00:23:49,810
so we we are not using the old style

00:23:45,990 --> 00:23:51,880
trick where you have a small reverse

00:23:49,810 --> 00:23:55,650
index for your changes and then a lot

00:23:51,880 --> 00:23:59,080
larger reverse index and the new version

00:23:55,650 --> 00:24:00,790
instead we use B trees which are more

00:23:59,080 --> 00:24:03,580
like databases for the recent changes

00:24:00,790 --> 00:24:08,620
and then we flush those to disk in the

00:24:03,580 --> 00:24:10,840
background and work for structured data

00:24:08,620 --> 00:24:12,610
we you typically use what's called

00:24:10,840 --> 00:24:16,570
attribute fields which are in memory

00:24:12,610 --> 00:24:19,480
forward dense data where you can also

00:24:16,570 --> 00:24:21,490
optionally have in memory B trees over

00:24:19,480 --> 00:24:25,270
the data if you want to use them a

00:24:21,490 --> 00:24:27,100
strong criteria in queries in addition

00:24:25,270 --> 00:24:30,490
to this we have a transaction log for

00:24:27,100 --> 00:24:34,269
persistence and replay if you crush and

00:24:30,490 --> 00:24:36,789
we have a separate store in a level dB

00:24:34,269 --> 00:24:39,429
similar structure for the actual raw

00:24:36,789 --> 00:24:43,029
data of all the documents that we use

00:24:39,429 --> 00:24:47,830
for recovery data redistribution things

00:24:43,029 --> 00:24:51,639
like that but also for returning the

00:24:47,830 --> 00:24:54,820
payload data of the response if you have

00:24:51,639 --> 00:25:01,929
multiple doc schemas we have one copy of

00:24:54,820 --> 00:25:04,209
all of this for for each chemo so I

00:25:01,929 --> 00:25:06,969
mentioned that vespa distribute state

00:25:04,209 --> 00:25:10,589
over the content nodes in a colon

00:25:06,969 --> 00:25:14,820
cluster that happens completely

00:25:10,589 --> 00:25:18,959
automatically in RESPA you never ever

00:25:14,820 --> 00:25:21,279
manually worry about what your shorts or

00:25:18,959 --> 00:25:25,570
wheesh are doing things like that

00:25:21,279 --> 00:25:29,739
that's things nobody using best to have

00:25:25,570 --> 00:25:32,769
worried about for at least 10 years and

00:25:29,739 --> 00:25:34,719
there's no way to do offline indexing

00:25:32,769 --> 00:25:38,289
the reason we used to do offline

00:25:34,719 --> 00:25:43,899
indexing back in all this is because it

00:25:38,289 --> 00:25:48,429
was CPU costly while since them CPU cost

00:25:43,899 --> 00:25:50,399
has grown much faster than sorry the CPU

00:25:48,429 --> 00:25:53,799
performance has grown much faster than

00:25:50,399 --> 00:25:57,039
bandwidth performance both water

00:25:53,799 --> 00:25:58,509
bandwidth and memory based bandwidth so

00:25:57,039 --> 00:26:02,129
because of that it's just not a

00:25:58,509 --> 00:26:04,589
bottleneck anymore so it's much more

00:26:02,129 --> 00:26:07,570
efficient to actually do all the or

00:26:04,589 --> 00:26:12,879
indexing locally on the node that will

00:26:07,570 --> 00:26:15,369
serve the data so in West Point you just

00:26:12,879 --> 00:26:17,739
list the nodes that goes into a content

00:26:15,369 --> 00:26:19,869
cluster and RESPA will distribute the

00:26:17,739 --> 00:26:21,519
data over those content nodes we had a

00:26:19,869 --> 00:26:25,179
certain replication Factory at you can

00:26:21,519 --> 00:26:27,489
set if you have a high query rate you

00:26:25,179 --> 00:26:30,190
can also do multiple groups inside the

00:26:27,489 --> 00:26:31,899
content cluster which each we keep some

00:26:30,190 --> 00:26:35,649
number of replicas of all the data so

00:26:31,899 --> 00:26:39,029
that you can do round-robin or load

00:26:35,649 --> 00:26:42,879
balance distribution across those groups

00:26:39,029 --> 00:26:44,799
to scale to higher Kweli queries and

00:26:42,879 --> 00:26:46,870
optionally in some use cases you can

00:26:44,799 --> 00:26:49,560
also

00:26:46,870 --> 00:26:53,710
Kolak ate some data with some property

00:26:49,560 --> 00:26:56,710
on specific nodes which is useful for

00:26:53,710 --> 00:26:59,830
things like personal search where each

00:26:56,710 --> 00:27:02,800
of the queries only searches a given

00:26:59,830 --> 00:27:04,510
users private data and in that case you

00:27:02,800 --> 00:27:06,700
don't want to scatter all the queries

00:27:04,510 --> 00:27:12,180
over all nodes you want to co-locate the

00:27:06,700 --> 00:27:15,430
data on some small number of nodes and

00:27:12,180 --> 00:27:17,380
only send a query for a given user to

00:27:15,430 --> 00:27:19,590
that smaller number of nodes where the

00:27:17,380 --> 00:27:22,360
number of nodes is a trade-off between

00:27:19,590 --> 00:27:28,150
how much scatter gather you need to do

00:27:22,360 --> 00:27:32,710
and the latency you achieve right and if

00:27:28,150 --> 00:27:34,960
you change your configuration to add or

00:27:32,710 --> 00:27:37,060
remove groups or replication factor or

00:27:34,960 --> 00:27:39,960
whatever let's probably read this beauty

00:27:37,060 --> 00:27:42,130
data in in the background while you are

00:27:39,960 --> 00:27:43,840
serving and handling writes and the same

00:27:42,130 --> 00:27:45,910
thing will happen if you add nodes or

00:27:43,840 --> 00:27:52,600
remove notes or some node die on your

00:27:45,910 --> 00:27:55,750
things like that another big thing in

00:27:52,600 --> 00:27:58,150
vespa is the inference engine which

00:27:55,750 --> 00:28:01,780
underlies support for machine learning

00:27:58,150 --> 00:28:07,210
models and so on so it's part of WestBow

00:28:01,780 --> 00:28:12,160
we have a tensor data model it used to

00:28:07,210 --> 00:28:14,050
be somewhat cumbersome to explain

00:28:12,160 --> 00:28:16,180
tensors to people why we need them and

00:28:14,050 --> 00:28:19,420
so on but after transform came up it's

00:28:16,180 --> 00:28:22,020
much simpler so as you probably know

00:28:19,420 --> 00:28:24,430
tensor is just a multi-dimensional

00:28:22,020 --> 00:28:27,310
collection on numbers at least if you

00:28:24,430 --> 00:28:29,890
ask a computer scientist and you can add

00:28:27,310 --> 00:28:33,160
these tensors to queries documents and

00:28:29,890 --> 00:28:35,650
models and use them to represent vectors

00:28:33,160 --> 00:28:36,790
matrixes higher level a higher

00:28:35,650 --> 00:28:40,990
dimensional data and so on

00:28:36,790 --> 00:28:43,390
so each tensor dimension in vespa can be

00:28:40,990 --> 00:28:45,430
either indexed which we use for dense

00:28:43,390 --> 00:28:49,830
data or it can be mapped which you use

00:28:45,430 --> 00:28:52,630
for sparse data and then we have a

00:28:49,830 --> 00:28:56,050
tensor mathematical language that allows

00:28:52,630 --> 00:28:58,830
you to do to compute much machines or

00:28:56,050 --> 00:29:03,810
models or even handwritten models or

00:28:58,830 --> 00:29:06,810
censors and that mouth language contains

00:29:03,810 --> 00:29:09,450
just six operators which is pretty cool

00:29:06,810 --> 00:29:13,590
based on those six simple operators we

00:29:09,450 --> 00:29:18,150
can combine them to provide all those

00:29:13,590 --> 00:29:21,630
higher level functions that you have in

00:29:18,150 --> 00:29:23,280
things like neural nets and so on so we

00:29:21,630 --> 00:29:24,960
have you provide those higher level

00:29:23,280 --> 00:29:26,550
functions out of the box as well so

00:29:24,960 --> 00:29:29,970
there's a long list in the documentation

00:29:26,550 --> 00:29:33,090
which is on the left hand side here but

00:29:29,970 --> 00:29:38,660
they are all just implemented in terms

00:29:33,090 --> 00:29:41,160
of those six core functions in Vespa

00:29:38,660 --> 00:29:44,700
which is very nice for us because it

00:29:41,160 --> 00:29:46,590
makes it simple for us to optimize

00:29:44,700 --> 00:29:47,970
because we can optimize those core

00:29:46,590 --> 00:29:54,810
functions and it will work for all the

00:29:47,970 --> 00:29:56,580
higher level functions right so people

00:29:54,810 --> 00:30:00,150
can handwrite these mathematical

00:29:56,580 --> 00:30:03,600
expressions to achieve whatever

00:30:00,150 --> 00:30:06,090
computation they want but in most cases

00:30:03,600 --> 00:30:08,930
people find that too difficult so to

00:30:06,090 --> 00:30:11,700
solve that problem we also provide

00:30:08,930 --> 00:30:15,600
integration of that box with tencel

00:30:11,700 --> 00:30:17,490
outside models so you can have you can

00:30:15,600 --> 00:30:19,290
store your tensorflow save models

00:30:17,490 --> 00:30:21,780
directly and respond respond random for

00:30:19,290 --> 00:30:23,820
you by converting them to mathematical

00:30:21,780 --> 00:30:25,710
expressions in this language and you can

00:30:23,820 --> 00:30:29,790
do that the same thing with models

00:30:25,710 --> 00:30:32,760
inside cake and pie torsion so by saving

00:30:29,790 --> 00:30:38,220
them in the common or next format which

00:30:32,760 --> 00:30:41,160
is also read by Vespa so just to give

00:30:38,220 --> 00:30:45,450
some intuition on that this thing on the

00:30:41,160 --> 00:30:47,400
left is a simple graph model in terms of

00:30:45,450 --> 00:30:51,500
flow and on the right you have the

00:30:47,400 --> 00:30:55,640
equivalent expression in the

00:30:51,500 --> 00:30:55,640
mathematical language inside

00:30:59,390 --> 00:31:06,919
just a few words on releases we do all

00:31:04,549 --> 00:31:09,200
development in the open or WestBow on

00:31:06,919 --> 00:31:13,610
github so it's on less pangaean slash

00:31:09,200 --> 00:31:15,380
Westlaw there's no internal thing we are

00:31:13,610 --> 00:31:19,039
doing and then sinking or and you know

00:31:15,380 --> 00:31:21,669
that is all in open and we create new

00:31:19,039 --> 00:31:27,320
production releases Monday to Thursday

00:31:21,669 --> 00:31:29,269
every week we for each release we each

00:31:27,320 --> 00:31:31,580
release will first have passed or zuto

00:31:29,269 --> 00:31:36,169
performance tests and functional tests

00:31:31,580 --> 00:31:39,620
and so on but they will also already be

00:31:36,169 --> 00:31:43,220
running the 150 or so applications in

00:31:39,620 --> 00:31:45,860
our own vespa cloud service so that

00:31:43,220 --> 00:31:47,929
wants to release is in the public it's

00:31:45,860 --> 00:31:50,600
already proven on all these applications

00:31:47,929 --> 00:31:53,889
so you can safely use it so I recommend

00:31:50,600 --> 00:31:56,450
everybody using respite - upgrades

00:31:53,889 --> 00:31:58,820
create a process to upgrade at least

00:31:56,450 --> 00:32:05,000
once a week using these releases rather

00:31:58,820 --> 00:32:09,409
than ending up being behind okay so that

00:32:05,000 --> 00:32:11,929
was introduction to vespa I'll summarize

00:32:09,409 --> 00:32:18,590
that and then do just a few more slides

00:32:11,929 --> 00:32:21,049
and then we'll be done so in summary if

00:32:18,590 --> 00:32:23,179
you want to use Big Data the best way

00:32:21,049 --> 00:32:26,870
you need to be able to make decisions in

00:32:23,179 --> 00:32:30,799
real time over all the data and West by

00:32:26,870 --> 00:32:33,139
senji nuts optimized for doing this you

00:32:30,799 --> 00:32:35,510
can find it on RESPA dots AI and there

00:32:33,139 --> 00:32:37,940
you can also find a QuickStart tutorial

00:32:35,510 --> 00:32:40,880
that allows you to run it on your laptop

00:32:37,940 --> 00:32:44,299
or on abs in less than 10 minutes you

00:32:40,880 --> 00:32:47,000
also have a big tutorial that lets you

00:32:44,299 --> 00:32:49,639
build the blog search and recommendation

00:32:47,000 --> 00:32:53,870
engine ending with recommendation using

00:32:49,639 --> 00:32:55,880
neural network that you can follow that

00:32:53,870 --> 00:32:57,649
takes something like a day but it starts

00:32:55,880 --> 00:32:59,649
from just a raw data and wins the whole

00:32:57,649 --> 00:33:03,440
thing which is pretty cool

00:32:59,649 --> 00:33:05,139
ok so I'll stop soon and move to

00:33:03,440 --> 00:33:07,850
questions but a few more things about

00:33:05,139 --> 00:33:10,929
how to use respo veera leaves RPM

00:33:07,850 --> 00:33:13,299
packages and docker images

00:33:10,929 --> 00:33:18,070
you install the same thing on all your

00:33:13,299 --> 00:33:20,830
nodes and you set one variable that

00:33:18,070 --> 00:33:23,130
points to the administration subsystem

00:33:20,830 --> 00:33:26,020
and the rest is creating your

00:33:23,130 --> 00:33:28,500
application package which typically

00:33:26,020 --> 00:33:32,110
corresponds to something like a git repo

00:33:28,500 --> 00:33:36,450
representing your application it needs

00:33:32,110 --> 00:33:39,070
to have three files one file at least

00:33:36,450 --> 00:33:41,169
describes clusters that you want to run

00:33:39,070 --> 00:33:44,020
and one listing the nodes you have

00:33:41,169 --> 00:33:47,399
available and then you need schemas for

00:33:44,020 --> 00:33:50,649
this and that the schemas also contains

00:33:47,399 --> 00:33:52,440
models you want to run or pointers to

00:33:50,649 --> 00:33:57,700
the models you want to run because

00:33:52,440 --> 00:34:00,100
models are tied to the schema so this to

00:33:57,700 --> 00:34:04,630
slice is a complete simple application

00:34:00,100 --> 00:34:06,490
package so we provide HTTP interfaces to

00:34:04,630 --> 00:34:09,730
do everything towards respond there's

00:34:06,490 --> 00:34:19,260
also some other alternatives in some

00:34:09,730 --> 00:34:19,260
cases yeah I won't I'll skip over this

00:34:25,570 --> 00:34:30,409
yeah maybe I can mention this because

00:34:28,700 --> 00:34:34,220
there's a lot of search people here I've

00:34:30,409 --> 00:34:37,780
noticed so typically when you do text

00:34:34,220 --> 00:34:37,780
search should do supervised learning

00:34:38,320 --> 00:34:44,450
using the structured data right

00:34:41,270 --> 00:34:47,000
while in recommendation it's more common

00:34:44,450 --> 00:34:50,690
to use some kind of vector tensor

00:34:47,000 --> 00:34:54,130
embedding or both users or whatever and

00:34:50,690 --> 00:34:57,970
as well as your documents and then

00:34:54,130 --> 00:35:01,130
evaluate overall your data items and

00:34:57,970 --> 00:35:02,570
also in as much as possible use

00:35:01,130 --> 00:35:04,250
reinforcement learning because you

00:35:02,570 --> 00:35:07,000
typically don't have a good idea about

00:35:04,250 --> 00:35:12,080
what right

00:35:07,000 --> 00:35:15,170
ranking is there's a certain movement in

00:35:12,080 --> 00:35:17,650
search now where you from for text

00:35:15,170 --> 00:35:21,920
search also are moving away from

00:35:17,650 --> 00:35:23,210
symbolic computation towards vector

00:35:21,920 --> 00:35:25,970
representations and things like that

00:35:23,210 --> 00:35:29,990
which is what people call some people at

00:35:25,970 --> 00:35:33,770
least call it search - oh it's

00:35:29,990 --> 00:35:35,870
interesting and in practice the best

00:35:33,770 --> 00:35:39,020
solution is kind of both at the same

00:35:35,870 --> 00:35:42,220
time but in any case vespa supports both

00:35:39,020 --> 00:35:44,300
this kind of use cases and we have

00:35:42,220 --> 00:35:47,210
interesting applications doing both

00:35:44,300 --> 00:35:49,700
approaches which are pretty good so one

00:35:47,210 --> 00:35:53,570
common thing that people are using with

00:35:49,700 --> 00:35:55,520
text search is what's called gradient

00:35:53,570 --> 00:36:00,430
boosted decision trees that has been

00:35:55,520 --> 00:36:03,320
sort of the benchmark for text redlands

00:36:00,430 --> 00:36:05,360
for a good while you can do that in

00:36:03,320 --> 00:36:10,580
vespa by writing a mathematical

00:36:05,360 --> 00:36:12,410
expression like expressing your forest

00:36:10,580 --> 00:36:15,410
that comes out from the gdt training

00:36:12,410 --> 00:36:17,090
manually but you can also use extra

00:36:15,410 --> 00:36:18,950
boost and just put the model directly

00:36:17,090 --> 00:36:20,150
and respond less probably understand it

00:36:18,950 --> 00:36:22,280
and convert it for you

00:36:20,150 --> 00:36:28,130
these expressions are really expensive

00:36:22,280 --> 00:36:31,370
to run the vespa contains spent a lot of

00:36:28,130 --> 00:36:33,560
effort into optimizing these expressions

00:36:31,370 --> 00:36:36,050
to make them faster run so Westfall

00:36:33,560 --> 00:36:38,030
recognized this shape or mathematical

00:36:36,050 --> 00:36:42,500
expressions use very

00:36:38,030 --> 00:36:44,360
specific optimizations for those so

00:36:42,500 --> 00:36:46,910
that's cool but then we have papers like

00:36:44,360 --> 00:36:52,160
this which says that you can train your

00:36:46,910 --> 00:36:55,760
G ability model on them train neural

00:36:52,160 --> 00:36:58,070
nets that mimics your model and that

00:36:55,760 --> 00:37:00,260
will give you the same results for about

00:36:58,070 --> 00:37:03,530
a hundredth of the cost so some people

00:37:00,260 --> 00:37:05,480
are doing this as well and that sort of

00:37:03,530 --> 00:37:07,790
makes sense when you think about it so

00:37:05,480 --> 00:37:11,270
maybe that's what people will do more or

00:37:07,790 --> 00:37:13,460
in the future in any case you can

00:37:11,270 --> 00:37:17,120
express both these kind of models quite

00:37:13,460 --> 00:37:18,830
simply in respond lots of interesting

00:37:17,120 --> 00:37:22,430
stuff here you can find these slides

00:37:18,830 --> 00:37:24,790
online if you really want to or even if

00:37:22,430 --> 00:37:27,770
you just want to a little bit actually

00:37:24,790 --> 00:37:29,780
but I'll stop there and move to

00:37:27,770 --> 00:37:33,800
questions we have three minutes if you

00:37:29,780 --> 00:37:39,860
are any questions hands up if you have a

00:37:33,800 --> 00:37:41,390
question so I was just wondering I you

00:37:39,860 --> 00:37:42,920
were showing some of the models from

00:37:41,390 --> 00:37:45,920
tensorflow and they're actually living

00:37:42,920 --> 00:37:47,570
in the vespa runtime so how do you deal

00:37:45,920 --> 00:37:49,790
there with the amount of memory that

00:37:47,570 --> 00:37:51,830
that model will actually take up it

00:37:49,790 --> 00:37:53,660
seems to be a user facing it seems to be

00:37:51,830 --> 00:37:57,680
running a query time if I read it

00:37:53,660 --> 00:37:59,630
correctly what so you were taking as

00:37:57,680 --> 00:38:02,120
some deep models and you were putting

00:37:59,630 --> 00:38:04,340
them in the the rancor yeah and I'm just

00:38:02,120 --> 00:38:05,690
wondering how does that perform because

00:38:04,340 --> 00:38:08,810
generally whenever I do this like a

00:38:05,690 --> 00:38:10,850
models maybe four gigabytes and so how

00:38:08,810 --> 00:38:15,130
does vespa deal with this oh is it

00:38:10,850 --> 00:38:17,690
yeah if you need to your your models or

00:38:15,130 --> 00:38:19,280
more specifically the tensors that are

00:38:17,690 --> 00:38:22,340
part of the model needs to fit in memory

00:38:19,280 --> 00:38:24,260
on all the nodes otherwise is possible

00:38:22,340 --> 00:38:27,200
to run it but it will just be way too

00:38:24,260 --> 00:38:28,550
slow but typically models aren't larger

00:38:27,200 --> 00:38:31,070
than what we mentioned a couple of

00:38:28,550 --> 00:38:35,210
gigabytes and that's fine really

00:38:31,070 --> 00:38:37,510
so that bottleneck really for at least

00:38:35,210 --> 00:38:40,220
four simple models the bottleneck become

00:38:37,510 --> 00:38:42,500
memory bandwidth actually when you

00:38:40,220 --> 00:38:46,240
evaluate is which is what you want

00:38:42,500 --> 00:38:46,240
because it's a scarce resource

00:38:49,940 --> 00:38:56,460
okay I guess everything will Sparkle on

00:38:52,770 --> 00:38:58,790
my earlier you mentioned reinforcement

00:38:56,460 --> 00:39:02,100
learning in recommendations algorithm

00:38:58,790 --> 00:39:04,500
module is it does it contain some sort

00:39:02,100 --> 00:39:07,800
of Explorer exploit techniques for

00:39:04,500 --> 00:39:10,860
recommendations right yeah explore

00:39:07,800 --> 00:39:14,250
exploit so when you do reinforcement

00:39:10,860 --> 00:39:16,470
learning you want to if you're not

00:39:14,250 --> 00:39:18,600
familiar with it you want to you have

00:39:16,470 --> 00:39:21,450
some model and you want to exploit that

00:39:18,600 --> 00:39:26,040
model to return results but you also

00:39:21,450 --> 00:39:27,930
want to for some of your traffic explore

00:39:26,040 --> 00:39:29,730
other options so that you can learn a

00:39:27,930 --> 00:39:32,550
better model right so that's what

00:39:29,730 --> 00:39:34,500
Explorer exploited and that's case the

00:39:32,550 --> 00:39:35,070
question is just west of us support is

00:39:34,500 --> 00:39:38,490
of the box

00:39:35,070 --> 00:39:41,280
no we do not we have you can plug in

00:39:38,490 --> 00:39:45,270
functionality like that pretty easily

00:39:41,280 --> 00:39:48,330
and I actually wrote a blog post you can

00:39:45,270 --> 00:39:51,210
find on medium which goes to details so

00:39:48,330 --> 00:39:54,500
how one use case that is running in

00:39:51,210 --> 00:39:57,060
production to do comment ranking using

00:39:54,500 --> 00:39:59,100
reinforcement learning is doing this so

00:39:57,060 --> 00:40:05,660
if you're interested in technical

00:39:59,100 --> 00:40:08,160
details you can look at that anymore

00:40:05,660 --> 00:40:09,690
okay in that case could you all put your

00:40:08,160 --> 00:40:10,590
hands together and thank John for a

00:40:09,690 --> 00:40:13,820
really interesting talk

00:40:10,590 --> 00:40:13,820

YouTube URL: https://www.youtube.com/watch?v=tTIATQk-V00


