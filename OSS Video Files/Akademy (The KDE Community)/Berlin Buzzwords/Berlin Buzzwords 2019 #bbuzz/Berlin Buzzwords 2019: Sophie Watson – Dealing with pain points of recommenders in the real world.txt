Title: Berlin Buzzwords 2019: Sophie Watson – Dealing with pain points of recommenders in the real world
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Off-the-shelf recommenders are a necessary building block for developing a personalised recommendation system, but they are not sufficient to solve personalisation problems by themselves. Such systems are designed to use either explicit or implicit data, but never both. They are unable to identify anomalous user activity, and cannot determine when one account is put to multiple distinct uses, for example combined personal and business purchases, or many users in a household sharing streaming media.

This talk will look at the pain points of recommendation algorithms and cover ways to overcome them in practise. We will dive into the example of data drift in recommendation using data from a music streaming service. A user’s behaviour is likely to change over time for multiple reasons: users might discover a new genre they like, they may associate negative memories with a song they used to love, or their taste may simply change as they grow up. Given a profile of a user’s tastes over time, it’s relatively straightforward to recommend content that will spark nostalgia.  However, “nostalgia” comes from Greek words meaning both “returning home” and “pain,” and the songs we loved once may not bring us joy today!

We’ll talk about how to identify changing tastes, find content which will feel like returning home without the pain, and give some simple suggestions for how to incorporate these findings into off-the-shelf recommenders to give a more robust user experience.

Read more:
https://2019.berlinbuzzwords.de/19/session/dealing-pain-points-recommenders-real-world

About Sophie Watson:
https://2019.berlinbuzzwords.de/users/sophie-watson

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,859 --> 00:00:12,490
thank you so hi I'm Sophie Watson I work

00:00:11,309 --> 00:00:14,040
at Red Hat

00:00:12,490 --> 00:00:16,379
the team our forward

00:00:14,040 --> 00:00:18,210
engineers so we focus on helping

00:00:16,379 --> 00:00:20,190
customers solve their business problems

00:00:18,210 --> 00:00:22,290
using data science and get their machine

00:00:20,190 --> 00:00:24,720
learning and their AI workloads running

00:00:22,290 --> 00:00:26,400
in the cloud over the past year or so

00:00:24,720 --> 00:00:28,650
I've been thinking quite a lot about

00:00:26,400 --> 00:00:30,000
recommendation engines and today I want

00:00:28,650 --> 00:00:32,460
to share with you some of the thoughts

00:00:30,000 --> 00:00:34,649
and ideas that I've had and specifically

00:00:32,460 --> 00:00:36,780
I'm gonna point out hurdles that we've

00:00:34,649 --> 00:00:39,600
had to get across and issues that we've

00:00:36,780 --> 00:00:44,570
seen along the way I'll also suggest

00:00:39,600 --> 00:00:44,570
some fixes and some potential solutions

00:00:45,230 --> 00:00:49,680
many of the services companies and

00:00:47,760 --> 00:00:51,510
products that we interact with on a

00:00:49,680 --> 00:00:54,360
daily basis and are providing

00:00:51,510 --> 00:00:57,270
personalized content to users and giving

00:00:54,360 --> 00:00:58,920
personal recommendations to us and if

00:00:57,270 --> 00:01:00,780
there's companies out there who aren't

00:00:58,920 --> 00:01:03,210
striving to give that personalized

00:01:00,780 --> 00:01:04,559
experience then it's pretty likely that

00:01:03,210 --> 00:01:07,229
they're going to fall behind in their

00:01:04,559 --> 00:01:09,030
market pretty soon so the types of

00:01:07,229 --> 00:01:11,340
recommendations that are made to us as

00:01:09,030 --> 00:01:15,149
users come from a huge range of

00:01:11,340 --> 00:01:18,180
applications Spotify for example curates

00:01:15,149 --> 00:01:21,870
daily playlists for me so if I log in I

00:01:18,180 --> 00:01:24,240
can see a suggestive playlist grouped by

00:01:21,870 --> 00:01:27,120
genre there'll be a pop playlist a

00:01:24,240 --> 00:01:30,210
classical playlist an indie playlist and

00:01:27,120 --> 00:01:33,150
so on and these are a mix of songs that

00:01:30,210 --> 00:01:36,120
I've listened to previously and I like

00:01:33,150 --> 00:01:38,250
from that genre and other songs that I

00:01:36,120 --> 00:01:42,540
haven't heard of before but they fit

00:01:38,250 --> 00:01:46,350
well into that playlist Apple knows that

00:01:42,540 --> 00:01:49,350
last month I bought an iPad so all of my

00:01:46,350 --> 00:01:51,150
adverts and emails are now suggesting

00:01:49,350 --> 00:01:53,100
that I go out and purchase some very

00:01:51,150 --> 00:01:55,229
expensive adapters which mean that I

00:01:53,100 --> 00:01:58,710
could plug my iPad into just about

00:01:55,229 --> 00:02:00,479
anything and video streaming services

00:01:58,710 --> 00:02:02,549
make perhaps what we think of is more

00:02:00,479 --> 00:02:05,970
classical recommendations using our

00:02:02,549 --> 00:02:09,479
listening history or our watching

00:02:05,970 --> 00:02:14,819
history to predict which films we should

00:02:09,479 --> 00:02:17,010
go ahead and check out but underlying

00:02:14,819 --> 00:02:18,659
all of these forms in which I see

00:02:17,010 --> 00:02:21,030
recommendations made to me there's some

00:02:18,659 --> 00:02:23,010
recommendation algorithm so if you look

00:02:21,030 --> 00:02:25,150
at the literature on recommendation

00:02:23,010 --> 00:02:28,030
engines all of the algorithms are

00:02:25,150 --> 00:02:30,849
are evaluated on how well they perform

00:02:28,030 --> 00:02:32,709
in terms of minimizing some error and

00:02:30,849 --> 00:02:35,500
there's an assumption that lower error

00:02:32,709 --> 00:02:38,230
means better real-world predictions for

00:02:35,500 --> 00:02:40,629
users in practice that's not really case

00:02:38,230 --> 00:02:42,670
so are these underlying metrics or

00:02:40,629 --> 00:02:44,670
errors or things like log loss or mean

00:02:42,670 --> 00:02:47,879
squared error and they don't necessarily

00:02:44,670 --> 00:02:52,150
directly correlate with what we as users

00:02:47,879 --> 00:02:55,540
want from our experience they're just

00:02:52,150 --> 00:02:57,310
not capturing the full picture once you

00:02:55,540 --> 00:02:59,290
put your recommendation model into

00:02:57,310 --> 00:03:01,720
production it's true that you can see

00:02:59,290 --> 00:03:04,090
how 'yes' interacts with that and then

00:03:01,720 --> 00:03:06,370
you can go ahead and improve your

00:03:04,090 --> 00:03:09,400
algorithm based on the way in which

00:03:06,370 --> 00:03:10,659
users are behaving and but for today

00:03:09,400 --> 00:03:12,459
we're going to think about the stage

00:03:10,659 --> 00:03:14,560
before production and we're going to

00:03:12,459 --> 00:03:17,010
send think about some things that we

00:03:14,560 --> 00:03:21,750
could do to improve recommendations

00:03:17,010 --> 00:03:23,799
before we get going into production so

00:03:21,750 --> 00:03:25,659
we'll start by talking about the

00:03:23,799 --> 00:03:27,909
problems which recommendation algorithms

00:03:25,659 --> 00:03:29,769
don't usually address and to do this

00:03:27,909 --> 00:03:32,370
we'll introduce alternating least

00:03:29,769 --> 00:03:35,980
squares so this is a very widely used

00:03:32,370 --> 00:03:37,239
recommendation algorithm from there

00:03:35,980 --> 00:03:39,400
we'll see what problems in

00:03:37,239 --> 00:03:41,799
recommendation are not solved by

00:03:39,400 --> 00:03:44,950
alternating least squares will select

00:03:41,799 --> 00:03:46,930
will suggest solutions for some and

00:03:44,950 --> 00:03:50,650
we'll dive deep into the problem of data

00:03:46,930 --> 00:03:53,949
drift we'll talk about a solution which

00:03:50,650 --> 00:03:56,440
uses approximate set similarity to make

00:03:53,949 --> 00:03:59,199
recommendations and we'll see how these

00:03:56,440 --> 00:04:01,989
set representations enable us to make

00:03:59,199 --> 00:04:03,400
sensible and robust recommendations to

00:04:01,989 --> 00:04:05,290
users with an added layer of

00:04:03,400 --> 00:04:09,040
intelligence over that initial

00:04:05,290 --> 00:04:10,870
alternating least squares algorithm and

00:04:09,040 --> 00:04:12,790
we'll see how this is done in a way

00:04:10,870 --> 00:04:17,139
which scales nicely as the number of

00:04:12,790 --> 00:04:18,780
users of a system grows so let's start

00:04:17,139 --> 00:04:21,760
with alternating least squares

00:04:18,780 --> 00:04:23,650
alternating least squares or als as it's

00:04:21,760 --> 00:04:25,690
more affectionately known is the

00:04:23,650 --> 00:04:27,360
industry standard recommendation

00:04:25,690 --> 00:04:29,380
algorithm and there's off-the-shelf

00:04:27,360 --> 00:04:34,110
implementations that you can plug and

00:04:29,380 --> 00:04:34,110
play written in scala - and off

00:04:34,150 --> 00:04:39,100
in its original form the algorithm

00:04:36,040 --> 00:04:41,290
itself deals with explicit data so if we

00:04:39,100 --> 00:04:43,780
stick with the movie streaming service

00:04:41,290 --> 00:04:46,030
example an example of explicit data

00:04:43,780 --> 00:04:50,770
would be something like user one gave

00:04:46,030 --> 00:04:55,530
film five three stars ALS and takes this

00:04:50,770 --> 00:04:55,530
data and puts it into a matrix like so

00:04:57,480 --> 00:05:02,910
and you can populate that matrix with

00:05:00,160 --> 00:05:05,320
any other recorded data that you have

00:05:02,910 --> 00:05:10,240
from there we're ready to implement the

00:05:05,320 --> 00:05:12,670
algorithm all in English squares

00:05:10,240 --> 00:05:15,310
factorizes this matrix into two smaller

00:05:12,670 --> 00:05:17,710
matrices it does this by some iterative

00:05:15,310 --> 00:05:19,180
process that for the purposes of this

00:05:17,710 --> 00:05:21,400
talk we don't need to worry too much

00:05:19,180 --> 00:05:23,410
about today but if anyone wants to talk

00:05:21,400 --> 00:05:26,710
in more detail about ALS and how it

00:05:23,410 --> 00:05:29,290
works then hit me up offline so one of

00:05:26,710 --> 00:05:31,960
these two matrices represents the users

00:05:29,290 --> 00:05:35,170
whilst the other represents the items or

00:05:31,960 --> 00:05:37,510
in this case the films so each row of

00:05:35,170 --> 00:05:41,800
the user matrix corresponds to one user

00:05:37,510 --> 00:05:45,490
each column of a film matrix corresponds

00:05:41,800 --> 00:05:47,740
to one film once the algorithm has

00:05:45,490 --> 00:05:50,140
factorized the data into these two

00:05:47,740 --> 00:05:52,180
matrices it becomes really simple to go

00:05:50,140 --> 00:05:55,750
ahead animate predictions for a given

00:05:52,180 --> 00:06:00,310
user so if we want to estimate how user

00:05:55,750 --> 00:06:04,210
one which rate film - we can do that by

00:06:00,310 --> 00:06:07,060
simply taking the dot product of the row

00:06:04,210 --> 00:06:11,250
corresponding to user 1 with the column

00:06:07,060 --> 00:06:14,740
corresponding to you a to film number 2

00:06:11,250 --> 00:06:16,870
and the ability to estimate how a

00:06:14,740 --> 00:06:19,060
particular user will rate a particular

00:06:16,870 --> 00:06:21,130
film enables you to do things like we

00:06:19,060 --> 00:06:26,560
recommend the ten films which a user

00:06:21,130 --> 00:06:29,440
will most like alternating least squares

00:06:26,560 --> 00:06:31,030
also allows you to identify users who

00:06:29,440 --> 00:06:35,260
are similar to each other who have

00:06:31,030 --> 00:06:37,330
similar tastes if we look at rows of the

00:06:35,260 --> 00:06:40,390
matrix of users and we compare the

00:06:37,330 --> 00:06:43,230
similarity of them then we can do that

00:06:40,390 --> 00:06:46,440
by comparing the vectors in the rows and

00:06:43,230 --> 00:06:48,180
in the same manner we can compare movies

00:06:46,440 --> 00:06:51,000
by looking at the columns of these

00:06:48,180 --> 00:06:53,250
matrices and identify movies which the

00:06:51,000 --> 00:06:55,440
algorithm deems similar and this might

00:06:53,250 --> 00:06:57,540
help you save some computational cost so

00:06:55,440 --> 00:06:59,550
for example if you knew that two users

00:06:57,540 --> 00:07:01,320
were exceptionally similar and you'd

00:06:59,550 --> 00:07:03,240
already computed how you one of them

00:07:01,320 --> 00:07:06,270
would react to a particular film you

00:07:03,240 --> 00:07:11,610
could just represent that recommendation

00:07:06,270 --> 00:07:14,340
to the other user so that example was

00:07:11,610 --> 00:07:17,340
for explicit day sir but what if I data

00:07:14,340 --> 00:07:20,580
is implicit if we think about the music

00:07:17,340 --> 00:07:22,230
streaming service as scenario what does

00:07:20,580 --> 00:07:24,330
it mean if you'd listened to a song so

00:07:22,230 --> 00:07:32,550
say I listened to a song once do I like

00:07:24,330 --> 00:07:34,410
it right we don't know would no idea if

00:07:32,550 --> 00:07:36,120
I listened to a different song ten times

00:07:34,410 --> 00:07:38,730
you would like your algorithm to be much

00:07:36,120 --> 00:07:42,300
more confident that indeed I probably

00:07:38,730 --> 00:07:45,210
liked that song and moving on if I

00:07:42,300 --> 00:07:47,160
listen to a song 100 times then we want

00:07:45,210 --> 00:07:49,740
to be even more confident than I like it

00:07:47,160 --> 00:07:52,800
so there's an implicit version of ALS

00:07:49,740 --> 00:07:55,320
which works on such data it requires

00:07:52,800 --> 00:07:57,660
that you define a mapping from your

00:07:55,320 --> 00:08:00,690
recording to how confident you are that

00:07:57,660 --> 00:08:03,210
the user likes that item so in our case

00:08:00,690 --> 00:08:05,940
this would be a mapping that mapped from

00:08:03,210 --> 00:08:07,290
number of plays through confidence but

00:08:05,940 --> 00:08:09,690
other than that it's pretty much the

00:08:07,290 --> 00:08:12,960
same as the implicit the explicit

00:08:09,690 --> 00:08:15,120
algorithm ok great so now we can deal

00:08:12,960 --> 00:08:18,090
with explicit data we can deal with

00:08:15,120 --> 00:08:21,180
implicit data we can quickly make point

00:08:18,090 --> 00:08:25,320
recommendations to users and we can

00:08:21,180 --> 00:08:27,180
identify similar users but that doesn't

00:08:25,320 --> 00:08:29,430
necessarily mean that you should just go

00:08:27,180 --> 00:08:31,230
ahead and push you're alternating least

00:08:29,430 --> 00:08:33,390
squares recommendation algorithm to

00:08:31,230 --> 00:08:35,160
production so in this next section we'll

00:08:33,390 --> 00:08:39,390
think about what alternating least

00:08:35,160 --> 00:08:41,160
squares is not providing us with if we

00:08:39,390 --> 00:08:43,680
go back to this emergency that we've got

00:08:41,160 --> 00:08:46,260
two matrices one for users and one for

00:08:43,680 --> 00:08:51,420
products but what happens when a new

00:08:46,260 --> 00:08:54,480
product is added to the market no users

00:08:51,420 --> 00:08:55,940
have rated that product yet so we don't

00:08:54,480 --> 00:08:58,220
know how they're going to interact

00:08:55,940 --> 00:09:01,280
and because the components of these

00:08:58,220 --> 00:09:03,650
feature vectors in ALS don't correspond

00:09:01,280 --> 00:09:06,560
to explainable features we can't just go

00:09:03,650 --> 00:09:09,650
ahead and generate one for this new film

00:09:06,560 --> 00:09:12,050
we need that ratings data in order to be

00:09:09,650 --> 00:09:14,500
able to predict the future vector for

00:09:12,050 --> 00:09:18,010
the film and thus make recommendations

00:09:14,500 --> 00:09:21,080
and in the same way that products arrive

00:09:18,010 --> 00:09:23,060
some products go off the market or fall

00:09:21,080 --> 00:09:25,670
out of favour with the whole population

00:09:23,060 --> 00:09:27,980
so this might be for political reasons

00:09:25,670 --> 00:09:31,010
ethical reasons or it could be because

00:09:27,980 --> 00:09:36,380
of the media storm or so on and out of

00:09:31,010 --> 00:09:38,090
the box als doesn't handle this all in

00:09:36,380 --> 00:09:40,670
English words also doesn't take into

00:09:38,090 --> 00:09:43,160
account changes in users opinions over

00:09:40,670 --> 00:09:45,680
time so it's not uncommon for people to

00:09:43,160 --> 00:09:47,870
have the same accounts now for years and

00:09:45,680 --> 00:09:51,140
years I've had the same Spotify account

00:09:47,870 --> 00:09:54,620
since 2008 and basically used it daily

00:09:51,140 --> 00:09:57,590
but my change in taste in music is very

00:09:54,620 --> 00:09:59,690
apparent and the music I listen to is

00:09:57,590 --> 00:10:01,400
much broader now and alternatingly

00:09:59,690 --> 00:10:04,490
squares just does not consider that at

00:10:01,400 --> 00:10:06,890
all there's not just one way in which

00:10:04,490 --> 00:10:09,620
cases can change so we usually think

00:10:06,890 --> 00:10:12,200
about change in terms of aging and I'm

00:10:09,620 --> 00:10:15,680
much less likely to listen to club music

00:10:12,200 --> 00:10:18,200
now than I was when I was 18 but there's

00:10:15,680 --> 00:10:19,940
also some seasonal changes in taste so

00:10:18,200 --> 00:10:22,580
this might be seasonal in the

00:10:19,940 --> 00:10:24,980
traditional traditional sense people

00:10:22,580 --> 00:10:28,070
only watch Christmas films in December

00:10:24,980 --> 00:10:30,230
for example and the type of music that

00:10:28,070 --> 00:10:32,020
is often released in summer and thus

00:10:30,230 --> 00:10:34,400
that people listen to in summer is

00:10:32,020 --> 00:10:41,300
tragically different from the things

00:10:34,400 --> 00:10:45,260
that are released in winter but you

00:10:41,300 --> 00:10:48,260
might also see changes that indicate

00:10:45,260 --> 00:10:50,420
seasons of life so there's likely some

00:10:48,260 --> 00:10:52,040
correlation positive or otherwise

00:10:50,420 --> 00:10:54,710
between a change in someone's

00:10:52,040 --> 00:10:58,790
relationship status and their interest

00:10:54,710 --> 00:11:00,950
in romantic films another thing which

00:10:58,790 --> 00:11:03,890
ALS can't handle it all is anomalous

00:11:00,950 --> 00:11:05,840
recordings so these crop up perhaps when

00:11:03,890 --> 00:11:07,850
you just hit the wrong star rating when

00:11:05,840 --> 00:11:09,270
you are watching a film or maybe there

00:11:07,850 --> 00:11:11,820
is some real anomaly

00:11:09,270 --> 00:11:16,530
in your opinion perhaps you hates all

00:11:11,820 --> 00:11:18,330
horror films except for one all in

00:11:16,530 --> 00:11:21,210
english squares is unnecessarily

00:11:18,330 --> 00:11:23,970
sensitive to these anomalous recordings

00:11:21,210 --> 00:11:26,910
so in the algorithm anomalies are fed

00:11:23,970 --> 00:11:29,940
back into the main matrix which is then

00:11:26,910 --> 00:11:33,480
used to create your user vector and as

00:11:29,940 --> 00:11:37,520
such all of your future recommendations

00:11:33,480 --> 00:11:40,350
are influenced by that one anomaly

00:11:37,520 --> 00:11:42,120
so although OLS can provide us with

00:11:40,350 --> 00:11:44,340
these personalized recommendations

00:11:42,120 --> 00:11:46,320
quickly and easily it would be nice if

00:11:44,340 --> 00:11:47,970
we could solve some of these issues that

00:11:46,320 --> 00:11:51,930
it doesn't address before we ship our

00:11:47,970 --> 00:11:54,140
engine to production now some of these

00:11:51,930 --> 00:11:57,480
problems can be simply solved by

00:11:54,140 --> 00:12:00,270
post-processing so if we think about the

00:11:57,480 --> 00:12:02,940
ALS algorithm as our model it takes in

00:12:00,270 --> 00:12:07,080
data and it returns recommendations to

00:12:02,940 --> 00:12:09,990
users a post-processing microservice

00:12:07,080 --> 00:12:13,020
could be placed in between the model and

00:12:09,990 --> 00:12:15,870
the recommendations and solve some of

00:12:13,020 --> 00:12:17,730
the issues for example we could use it

00:12:15,870 --> 00:12:20,100
to filter out these seasonal

00:12:17,730 --> 00:12:24,900
recommendations therefore not recommend

00:12:20,100 --> 00:12:27,630
Christmas films in July for example it

00:12:24,900 --> 00:12:30,000
could be also used to ensure that you

00:12:27,630 --> 00:12:34,280
recommend any new or any promoted

00:12:30,000 --> 00:12:36,990
products to the whole population and the

00:12:34,280 --> 00:12:39,330
outdated products are never recommended

00:12:36,990 --> 00:12:41,990
now you probably just don't want to

00:12:39,330 --> 00:12:44,520
remove all of the data you have about

00:12:41,990 --> 00:12:46,140
outdated products from your data store

00:12:44,520 --> 00:12:48,630
because these do give you insight into

00:12:46,140 --> 00:12:50,790
how the user may react to other products

00:12:48,630 --> 00:12:55,380
so we do just want to filter those out

00:12:50,790 --> 00:12:58,020
at the post-processing stage what we

00:12:55,380 --> 00:13:01,680
can't do with post-processing is use it

00:12:58,020 --> 00:13:03,030
to identify changes in user taste so

00:13:01,680 --> 00:13:05,370
it's not able to figure out which

00:13:03,030 --> 00:13:07,500
products or media will spark nostalgia

00:13:05,370 --> 00:13:09,930
and identify those from the ones that

00:13:07,500 --> 00:13:11,700
you don't mind if you ever see again it

00:13:09,930 --> 00:13:14,130
can't tell if you watched a load of sad

00:13:11,700 --> 00:13:15,900
breakup movies in a very short period of

00:13:14,130 --> 00:13:17,850
time or whether you're actually just

00:13:15,900 --> 00:13:20,520
quite like sad breakup movies and you

00:13:17,850 --> 00:13:21,649
watch them frequently and it's not able

00:13:20,520 --> 00:13:24,019
to identify

00:13:21,649 --> 00:13:25,879
anomalous behavior and then prevent that

00:13:24,019 --> 00:13:30,769
from influencing these recommendations

00:13:25,879 --> 00:13:32,540
further so lovely for us there's another

00:13:30,769 --> 00:13:34,819
class of algorithms which we can use to

00:13:32,540 --> 00:13:38,360
make recommendations and these use

00:13:34,819 --> 00:13:40,610
composable signatures in this next

00:13:38,360 --> 00:13:43,929
section we'll introduce the class we'll

00:13:40,610 --> 00:13:46,839
sure why it solves a few more problems

00:13:43,929 --> 00:13:53,660
so let's go back to our music

00:13:46,839 --> 00:13:56,509
recommendations scenario here we have

00:13:53,660 --> 00:13:58,519
three users the first two users have

00:13:56,509 --> 00:14:00,470
made me listen to the same stuff so I

00:13:58,519 --> 00:14:02,449
think I hope you would all agree that

00:14:00,470 --> 00:14:03,980
it's reasonable to recommend to use the

00:14:02,449 --> 00:14:06,290
two that they go out and listen to Billy

00:14:03,980 --> 00:14:10,189
Bragg and recommend new order to use a

00:14:06,290 --> 00:14:12,139
one but if we look at user three the

00:14:10,189 --> 00:14:13,759
only house Taylor Swift in common with

00:14:12,139 --> 00:14:16,459
the other two users so we're much less

00:14:13,759 --> 00:14:18,470
likely to tell users one or two to go

00:14:16,459 --> 00:14:20,839
and listen to Spice Girls or recommend

00:14:18,470 --> 00:14:25,670
that user three goes out and buys the

00:14:20,839 --> 00:14:27,589
Smiths Greatest Hits and in fact there's

00:14:25,670 --> 00:14:29,870
a very nice statistic that we can use to

00:14:27,589 --> 00:14:32,360
quantify this level of similarity

00:14:29,870 --> 00:14:35,420
between two sets so this is known as the

00:14:32,360 --> 00:14:37,610
jacquard index and it's computed as the

00:14:35,420 --> 00:14:40,220
union of those two sets over the

00:14:37,610 --> 00:14:42,439
intersection so if we were going to

00:14:40,220 --> 00:14:44,529
compute it fuses 1 & 2 we'd see that

00:14:42,439 --> 00:14:47,209
they've listened to the same artists

00:14:44,529 --> 00:14:49,339
four of the same artists and they've

00:14:47,209 --> 00:14:51,649
listened to a total of six artists

00:14:49,339 --> 00:14:56,990
between them and so their jacquard index

00:14:51,649 --> 00:14:59,720
is four sixths for users one and three

00:14:56,990 --> 00:15:01,129
they only have one artist in common and

00:14:59,720 --> 00:15:05,449
between them they've listened to nine

00:15:01,129 --> 00:15:07,220
artists hence we get a nice and if you

00:15:05,449 --> 00:15:09,290
compute your jacquard index for the

00:15:07,220 --> 00:15:11,059
music history of lots of different users

00:15:09,290 --> 00:15:13,309
with user number one then you can

00:15:11,059 --> 00:15:14,990
identify users that have similar tastes

00:15:13,309 --> 00:15:17,389
to number one and as such you can

00:15:14,990 --> 00:15:21,049
recommend some use music to number 1

00:15:17,389 --> 00:15:23,329
which would be songs that the people

00:15:21,049 --> 00:15:27,429
that they were deemed close to have

00:15:23,329 --> 00:15:27,429
listened to but they previously have not

00:15:28,100 --> 00:15:34,100
so now we have a way to numerically

00:15:30,160 --> 00:15:36,350
quantify the similarity of sets but

00:15:34,100 --> 00:15:38,870
storing all of the artists that every

00:15:36,350 --> 00:15:41,210
user has listened to in a set is not

00:15:38,870 --> 00:15:44,510
very efficient so we can extend this

00:15:41,210 --> 00:15:46,580
idea of storing a user's music history

00:15:44,510 --> 00:15:49,930
in a set by storing it in a bit vector

00:15:46,580 --> 00:15:52,460
so here each bit or box as they're shown

00:15:49,930 --> 00:15:55,100
represents a band or an artist and the

00:15:52,460 --> 00:15:58,160
bits are set or colored in if the user

00:15:55,100 --> 00:16:00,680
has listened to that artist if we

00:15:58,160 --> 00:16:02,540
compute these for every user then we can

00:16:00,680 --> 00:16:08,810
compare users by comparing their bit

00:16:02,540 --> 00:16:11,240
vectors and the jacquard index of bit

00:16:08,810 --> 00:16:15,020
vectors is computed in the same way as

00:16:11,240 --> 00:16:18,170
it was for sets so the intersection is

00:16:15,020 --> 00:16:20,570
just how many bits are set in both of

00:16:18,170 --> 00:16:23,630
the vectors in this case it's three and

00:16:20,570 --> 00:16:26,750
the Union is how many bits are set in

00:16:23,630 --> 00:16:31,790
any of the vectors in this case it's ten

00:16:26,750 --> 00:16:33,710
hence the jacquard index is 3/10 bit

00:16:31,790 --> 00:16:36,590
vectors provide a much more efficient

00:16:33,710 --> 00:16:39,200
representation of sets than the set

00:16:36,590 --> 00:16:41,780
themselves but I probably haven't yet

00:16:39,200 --> 00:16:44,090
done them justice so one really nice

00:16:41,780 --> 00:16:45,920
property of bit vectors that we really

00:16:44,090 --> 00:16:48,590
want to capitalize on today is that

00:16:45,920 --> 00:16:50,870
they're composable what that means for

00:16:48,590 --> 00:16:52,010
us is that it's very easy to merge these

00:16:50,870 --> 00:16:55,550
bit vectors together

00:16:52,010 --> 00:16:58,580
so suppose user one listens to music not

00:16:55,550 --> 00:17:01,670
only on their computer but also on their

00:16:58,580 --> 00:17:05,270
phone and each device keeps track of a

00:17:01,670 --> 00:17:07,520
bit vector for itself it's really easy

00:17:05,270 --> 00:17:09,500
to merge these into one global vector

00:17:07,520 --> 00:17:12,740
for the user and gather all of that

00:17:09,500 --> 00:17:14,570
information together all perhaps you

00:17:12,740 --> 00:17:17,390
want to keep these two bit vectors

00:17:14,570 --> 00:17:19,490
separate so maybe the user listens to

00:17:17,390 --> 00:17:21,500
podcasts on their commute to work on

00:17:19,490 --> 00:17:23,420
their phone but listen to music once

00:17:21,500 --> 00:17:26,300
they're sat at their desk and so you can

00:17:23,420 --> 00:17:27,880
use those bit vectors that are separated

00:17:26,300 --> 00:17:30,110
to go ahead and make relevant

00:17:27,880 --> 00:17:32,570
recommendations on the appropriate

00:17:30,110 --> 00:17:35,750
device but it's still nice to be able to

00:17:32,570 --> 00:17:39,390
obtain simply that global overview of

00:17:35,750 --> 00:17:41,820
the user using the joined vector

00:17:39,390 --> 00:17:43,950
where this composability really gives us

00:17:41,820 --> 00:17:46,200
great improvements over alternatingly

00:17:43,950 --> 00:17:48,360
squares is when we use different bit

00:17:46,200 --> 00:17:51,360
vectors for different time periods

00:17:48,360 --> 00:17:52,920
it's these time-dependent bit vectors

00:17:51,360 --> 00:17:54,990
which allow us to do things like map

00:17:52,920 --> 00:17:57,360
changes and user's behavior over time

00:17:54,990 --> 00:17:59,040
and identify the music that you're going

00:17:57,360 --> 00:18:01,320
to keep coming back to you again and

00:17:59,040 --> 00:18:04,320
again and ultimately figure out what's

00:18:01,320 --> 00:18:06,270
gonna stop the spark nostalgia so you

00:18:04,320 --> 00:18:08,820
could keep a different bit vector for

00:18:06,270 --> 00:18:11,669
every month every day every week for a

00:18:08,820 --> 00:18:14,190
user and you can see the changes in

00:18:11,669 --> 00:18:16,110
behavior there you can catch new trends

00:18:14,190 --> 00:18:20,429
and you can see what songs people keep

00:18:16,110 --> 00:18:22,410
returning to now suddenly we can't just

00:18:20,429 --> 00:18:25,290
stop there this isn't yet a functional

00:18:22,410 --> 00:18:26,179
solution to our problems for a couple of

00:18:25,290 --> 00:18:28,500
reasons

00:18:26,179 --> 00:18:30,960
although the bit vectors take up a lot

00:18:28,500 --> 00:18:32,880
less space than explicit sets the

00:18:30,960 --> 00:18:35,370
downside is that you have to visit every

00:18:32,880 --> 00:18:39,000
bit in the vector in order to perform

00:18:35,370 --> 00:18:42,059
the set operations the jacquard index is

00:18:39,000 --> 00:18:44,010
still relatively cheap to compute but

00:18:42,059 --> 00:18:46,890
the real problem is that to find the

00:18:44,010 --> 00:18:48,990
most similar users to any given user

00:18:46,890 --> 00:18:53,480
we'd have to compute an acceptable

00:18:48,990 --> 00:18:57,330
number of jacquard indexes Spotify has

00:18:53,480 --> 00:18:59,640
270 million users so even if you want to

00:18:57,330 --> 00:19:02,010
compute the similarity between just one

00:18:59,640 --> 00:19:04,470
of them and all the other users it's

00:19:02,010 --> 00:19:06,740
going to be really expensive so what's

00:19:04,470 --> 00:19:06,740
next

00:19:09,950 --> 00:19:14,700
so the algorithm that we're going to use

00:19:12,179 --> 00:19:17,160
to solve our problems here is called min

00:19:14,700 --> 00:19:19,890
hash min hash is widely used for

00:19:17,160 --> 00:19:21,870
identifying similar documents at scale

00:19:19,890 --> 00:19:24,090
but we'll see that we can use it to

00:19:21,870 --> 00:19:28,350
determine set similarity and we'll also

00:19:24,090 --> 00:19:30,120
use it to make recommendations min hash

00:19:28,350 --> 00:19:32,549
takes that massive vector which

00:19:30,120 --> 00:19:34,799
indicates the artists that the user has

00:19:32,549 --> 00:19:38,190
listened to and that sit down to a much

00:19:34,799 --> 00:19:41,910
smaller structure we call this a min had

00:19:38,190 --> 00:19:45,570
min hash signature so how do we generate

00:19:41,910 --> 00:19:48,210
one of these well what we need is n hash

00:19:45,570 --> 00:19:51,059
functions where n is the size of your

00:19:48,210 --> 00:19:52,150
min hash signature so here we have five

00:19:51,059 --> 00:19:54,220
of them

00:19:52,150 --> 00:19:57,610
each of these functions maps to some

00:19:54,220 --> 00:19:59,920
large set of numbers to begin with every

00:19:57,610 --> 00:20:03,190
entry in the pit back in the main

00:19:59,920 --> 00:20:06,070
hashtag mature is set to infinity and

00:20:03,190 --> 00:20:09,400
what we do is we move along the bit

00:20:06,070 --> 00:20:13,720
vector until we find a set bit the first

00:20:09,400 --> 00:20:16,390
place empty so we carry on when we reach

00:20:13,720 --> 00:20:17,980
a non empty bit we do the following so

00:20:16,390 --> 00:20:20,500
what we do is we first look at what

00:20:17,980 --> 00:20:22,450
number bit that is if I use computer

00:20:20,500 --> 00:20:24,550
science counting then that fit number

00:20:22,450 --> 00:20:28,330
one because we start counting at zero

00:20:24,550 --> 00:20:31,080
apparently so I pass that number in our

00:20:28,330 --> 00:20:36,460
case 1 through the hash functions and

00:20:31,080 --> 00:20:39,220
each of them returns an integer we then

00:20:36,460 --> 00:20:42,820
update the min hash signature to take

00:20:39,220 --> 00:20:45,100
the minimum of for each role what's

00:20:42,820 --> 00:20:48,280
already in the signature and the new

00:20:45,100 --> 00:20:55,090
number so the minimum of 6 and infinity

00:20:48,280 --> 00:20:57,370
is sweet okay so we keep the infinity

00:20:55,090 --> 00:21:01,870
out there we put the 6 in the minimum of

00:20:57,370 --> 00:21:03,910
12,000 and 48 and infinity is all right

00:21:01,870 --> 00:21:05,920
I'm not gonna okay so hopefully you get

00:21:03,910 --> 00:21:08,830
the idea of how we get started with min

00:21:05,920 --> 00:21:10,570
hash next we go back to our bit vector

00:21:08,830 --> 00:21:12,820
and we keep moving along until we find

00:21:10,570 --> 00:21:14,860
the next set bit so in this case it's

00:21:12,820 --> 00:21:17,770
bit number 4 because we start counting

00:21:14,860 --> 00:21:20,410
at 0 and we pass 4 through each of these

00:21:17,770 --> 00:21:22,360
hash functions and we then just compute

00:21:20,410 --> 00:21:24,760
the minimum of what's already in the

00:21:22,360 --> 00:21:27,400
vector and what the hash functions gave

00:21:24,760 --> 00:21:31,060
us so in this case we'd update the

00:21:27,400 --> 00:21:33,280
second and the fourth rows if I start

00:21:31,060 --> 00:21:35,950
counting at warn in that case sorry for

00:21:33,280 --> 00:21:38,440
any confusion so we continue this

00:21:35,950 --> 00:21:45,070
process and what you end up with is a

00:21:38,440 --> 00:21:46,930
min hash signature for your user some

00:21:45,070 --> 00:21:49,120
min hash signatures are by construction

00:21:46,930 --> 00:21:51,370
shorter than your bit vector so it's

00:21:49,120 --> 00:21:53,970
faster to compare 2 min hash signatures

00:21:51,370 --> 00:21:56,500
than it is the compareto bit vectors and

00:21:53,970 --> 00:21:58,990
the way in which we compare min hash

00:21:56,500 --> 00:22:01,300
signatures is that we just can't the

00:21:58,990 --> 00:22:04,510
proportion of elements which are the

00:22:01,300 --> 00:22:10,270
same in both of them so in this case

00:22:04,510 --> 00:22:11,740
we've got two matches there so the

00:22:10,270 --> 00:22:16,059
similarity here would be deemed

00:22:11,740 --> 00:22:17,919
two-fifths or not point four now a min

00:22:16,059 --> 00:22:20,110
hash signature is an approximate

00:22:17,919 --> 00:22:23,740
representation of a set that allows us

00:22:20,110 --> 00:22:26,710
to compare those sets it is possible for

00:22:23,740 --> 00:22:29,919
two sets which are not the same to have

00:22:26,710 --> 00:22:32,440
the same min hash signature in practice

00:22:29,919 --> 00:22:34,290
this very rarely happens but it's worth

00:22:32,440 --> 00:22:37,900
remembering that you're never gonna

00:22:34,290 --> 00:22:42,820
underestimate similarity with min hash

00:22:37,900 --> 00:22:44,650
but you may overestimate it for

00:22:42,820 --> 00:22:46,240
recommendation purposes this over

00:22:44,650 --> 00:22:48,100
estimation isn't going to cause us too

00:22:46,240 --> 00:22:50,910
many problems if we think about what we

00:22:48,100 --> 00:22:54,700
do once we have identified similar users

00:22:50,910 --> 00:22:56,110
we'd go back to their bit vectors and we

00:22:54,700 --> 00:22:58,660
take the difference of the two bit

00:22:56,110 --> 00:23:00,340
vectors we'd look at which songs one has

00:22:58,660 --> 00:23:02,200
listened to that the other has not and

00:23:00,340 --> 00:23:05,730
from there we could make recommendations

00:23:02,200 --> 00:23:07,990
so the pink bits here represent the the

00:23:05,730 --> 00:23:10,960
artists that we would recommend to the

00:23:07,990 --> 00:23:12,790
other user suppose we looked at the

00:23:10,960 --> 00:23:14,860
difference between those two bit vectors

00:23:12,790 --> 00:23:16,950
and saw that it was actually large in

00:23:14,860 --> 00:23:20,049
such a situation we'd likely

00:23:16,950 --> 00:23:24,820
overestimated the similarity and so we

00:23:20,049 --> 00:23:27,040
take a step back so now we've got a way

00:23:24,820 --> 00:23:29,500
to compare two users without having to

00:23:27,040 --> 00:23:31,990
make as many pairwise comparisons and

00:23:29,500 --> 00:23:34,809
min hash also has that composability

00:23:31,990 --> 00:23:36,669
property that the vectors had so if we

00:23:34,809 --> 00:23:39,100
have min hash signatures for a couple of

00:23:36,669 --> 00:23:46,059
devices we can combine them just by

00:23:39,100 --> 00:23:48,010
taking the real wise minimum and the

00:23:46,059 --> 00:23:49,840
process of computing these min hash

00:23:48,010 --> 00:23:52,480
vectors doesn't actually have to start

00:23:49,840 --> 00:23:55,059
from the bit vector you could just hash

00:23:52,480 --> 00:23:57,340
based on the artists names or the song

00:23:55,059 --> 00:24:00,160
names for example as they're streaming

00:23:57,340 --> 00:24:02,320
in so this gives you a nice way to

00:24:00,160 --> 00:24:04,510
quickly update your min hash signature

00:24:02,320 --> 00:24:06,880
as the user is using as the streaming

00:24:04,510 --> 00:24:08,620
service it also helps out when you're

00:24:06,880 --> 00:24:12,790
thinking about the issue of adding new

00:24:08,620 --> 00:24:14,409
items to the market we don't have to

00:24:12,790 --> 00:24:16,480
change the size of our min hash

00:24:14,409 --> 00:24:17,830
signature we can hash whatever we pass

00:24:16,480 --> 00:24:21,200
in

00:24:17,830 --> 00:24:23,510
so that solves the problem of us having

00:24:21,200 --> 00:24:25,850
to compare huge bit factors but that

00:24:23,510 --> 00:24:26,810
wasn't really the problem that we cared

00:24:25,850 --> 00:24:29,900
about that wasn't where the

00:24:26,810 --> 00:24:31,970
computational drain was lying we still

00:24:29,900 --> 00:24:33,500
haven't made any headway on trying to

00:24:31,970 --> 00:24:35,840
reduce the number of pairwise

00:24:33,500 --> 00:24:37,670
comparisons which we have to make in

00:24:35,840 --> 00:24:42,260
order to find these users that have

00:24:37,670 --> 00:24:44,470
similar behavior luckily for us

00:24:42,260 --> 00:24:49,070
something known as locality-sensitive

00:24:44,470 --> 00:24:51,980
min hash exists so locality sensitive

00:24:49,070 --> 00:24:54,730
moon hash looks at subsets of the users

00:24:51,980 --> 00:24:57,380
min hash signatures and if any to have

00:24:54,730 --> 00:25:00,230
identical signatures in any of the

00:24:57,380 --> 00:25:02,930
subsets then these users are considered

00:25:00,230 --> 00:25:05,630
a candidate pair from there you would go

00:25:02,930 --> 00:25:07,850
on and compute their similarity or their

00:25:05,630 --> 00:25:10,250
approximate jacquard index by looking at

00:25:07,850 --> 00:25:12,080
their min hash signatures as a whole to

00:25:10,250 --> 00:25:15,200
determine how similar they are and if

00:25:12,080 --> 00:25:17,150
you do want to make recommendations the

00:25:15,200 --> 00:25:19,070
way in which locality sensitive min

00:25:17,150 --> 00:25:22,040
hashing works is by splitting the min

00:25:19,070 --> 00:25:24,650
hash signature into bands so here we

00:25:22,040 --> 00:25:29,240
have five bands each containing two

00:25:24,650 --> 00:25:31,790
roles a new hash function then maps from

00:25:29,240 --> 00:25:34,070
the contents of the Bands into some

00:25:31,790 --> 00:25:36,680
buckets so for us our hash function

00:25:34,070 --> 00:25:39,080
would take in vectors of length two

00:25:36,680 --> 00:25:45,530
since the bands have two roles in each

00:25:39,080 --> 00:25:47,300
of them if in any band to users map to

00:25:45,530 --> 00:25:49,790
the same bucket then they're going to be

00:25:47,300 --> 00:25:51,860
considered a candidate pair and we go

00:25:49,790 --> 00:25:55,190
back and look at the min hash signatures

00:25:51,860 --> 00:25:56,990
and see how similar the users are thus

00:25:55,190 --> 00:25:59,240
we only have to compute similarity of

00:25:56,990 --> 00:26:01,490
the monastic matures for a subset of the

00:25:59,240 --> 00:26:05,450
whole population so if you think about

00:26:01,490 --> 00:26:08,330
the Spotify case if we reduce from 217

00:26:05,450 --> 00:26:09,950
million potential users that are the

00:26:08,330 --> 00:26:11,780
same as mymusictaste

00:26:09,950 --> 00:26:16,940
even just down to a hundred thousand

00:26:11,780 --> 00:26:20,420
that's a massive computational saving so

00:26:16,940 --> 00:26:22,850
Manoj and locality-sensitive min hash

00:26:20,420 --> 00:26:24,920
solves a lot of our problems but perhaps

00:26:22,850 --> 00:26:27,560
and surprisingly I haven't given you

00:26:24,920 --> 00:26:29,720
solutions to all of your recommendation

00:26:27,560 --> 00:26:30,690
problems of the last half an hour so

00:26:29,720 --> 00:26:32,340
here's something

00:26:30,690 --> 00:26:34,790
that we didn't have time to touch on

00:26:32,340 --> 00:26:39,180
when some suggestions for how to

00:26:34,790 --> 00:26:40,710
approach these problems going forward so

00:26:39,180 --> 00:26:42,690
the first is that you might have noticed

00:26:40,710 --> 00:26:45,630
that min hash example we did was for

00:26:42,690 --> 00:26:47,820
implicit data we essentially assumed

00:26:45,630 --> 00:26:51,150
that if someone listened to a song they

00:26:47,820 --> 00:26:53,160
liked it but earlier we were all in

00:26:51,150 --> 00:26:56,220
agreement that we're not entirely sure

00:26:53,160 --> 00:26:58,230
that that's the case so one way round

00:26:56,220 --> 00:27:01,260
this could be to only record the

00:26:58,230 --> 00:27:03,450
information in the min hash signature or

00:27:01,260 --> 00:27:06,660
in the bit vector if a user listens to

00:27:03,450 --> 00:27:08,910
that artists multiple times so here we

00:27:06,660 --> 00:27:12,330
wouldn't put song again but we would put

00:27:08,910 --> 00:27:14,160
song B and song C we're introducing some

00:27:12,330 --> 00:27:15,990
sort of threshold number and we're

00:27:14,160 --> 00:27:18,270
saying okay if you've interacted with

00:27:15,990 --> 00:27:20,970
this product for so many minutes or

00:27:18,270 --> 00:27:24,570
you've explicitly told us that you do

00:27:20,970 --> 00:27:27,840
like it then we can make that hash if

00:27:24,570 --> 00:27:30,420
the data is in fact explicit if the user

00:27:27,840 --> 00:27:32,400
does say okay I give this film five

00:27:30,420 --> 00:27:34,290
stars we might want to deal with that

00:27:32,400 --> 00:27:36,780
slightly differently so one thing that

00:27:34,290 --> 00:27:39,360
you could do is keep two min hash

00:27:36,780 --> 00:27:40,860
signatures for every user these are

00:27:39,360 --> 00:27:43,740
cheap to store because they're

00:27:40,860 --> 00:27:45,150
relatively small and in one of these we

00:27:43,740 --> 00:27:47,250
could have things that we know the user

00:27:45,150 --> 00:27:49,980
likes and then the other things we know

00:27:47,250 --> 00:27:52,290
the user dislikes I think we could

00:27:49,980 --> 00:27:55,380
probably all have a reasonable argument

00:27:52,290 --> 00:27:56,910
that if you dislike some things there's

00:27:55,380 --> 00:27:58,380
probably some correlation there with the

00:27:56,910 --> 00:28:00,420
things that you do like so that

00:27:58,380 --> 00:28:06,300
information is still important you can

00:28:00,420 --> 00:28:08,600
use it to make recommendations or maybe

00:28:06,300 --> 00:28:08,600
there's

00:28:12,790 --> 00:28:18,190
yeah so another problem that we haven't

00:28:16,570 --> 00:28:19,930
addressed is that of making

00:28:18,190 --> 00:28:22,600
recommendations in the case where

00:28:19,930 --> 00:28:24,700
multiple users share one account

00:28:22,600 --> 00:28:32,260
who here shares some sort of streaming

00:28:24,700 --> 00:28:34,210
accounts with somebody else and so it

00:28:32,260 --> 00:28:39,220
might be obvious in this case because of

00:28:34,210 --> 00:28:41,950
different device usage that you can

00:28:39,220 --> 00:28:43,840
identify who is using what and make the

00:28:41,950 --> 00:28:45,580
recommendations accordingly in the same

00:28:43,840 --> 00:28:47,350
way that we discussed perhaps someone

00:28:45,580 --> 00:28:52,090
commuting using their phone and then

00:28:47,350 --> 00:28:53,440
being static and using their computer or

00:28:52,090 --> 00:28:55,660
you might have to do something more

00:28:53,440 --> 00:28:58,690
clever like trying to cluster users

00:28:55,660 --> 00:29:00,910
behavior and when you think about

00:28:58,690 --> 00:29:02,740
watching movies I think there's an extra

00:29:00,910 --> 00:29:04,870
layer there so often if you watch a

00:29:02,740 --> 00:29:07,510
movie with someone else the movie that

00:29:04,870 --> 00:29:09,880
you watch might be a compromise of what

00:29:07,510 --> 00:29:13,720
they like and what you like and it might

00:29:09,880 --> 00:29:16,870
not be the same so from some angle it's

00:29:13,720 --> 00:29:19,240
your taste but head-on not exactly so we

00:29:16,870 --> 00:29:20,860
want to see how to deal with this I

00:29:19,240 --> 00:29:24,090
would argue that this is a bit of a

00:29:20,860 --> 00:29:27,610
lesser version of an outlier and

00:29:24,090 --> 00:29:30,220
although outliers don't influence all

00:29:27,610 --> 00:29:32,140
recommendations when using main hash in

00:29:30,220 --> 00:29:34,270
the same way that they do in alternating

00:29:32,140 --> 00:29:38,470
least squares we haven't charged about

00:29:34,270 --> 00:29:41,500
how to identify them so it might be

00:29:38,470 --> 00:29:45,070
plausible to just look at the end most

00:29:41,500 --> 00:29:46,780
similar users to a given user and say ok

00:29:45,070 --> 00:29:49,510
we've got this new recording for our

00:29:46,780 --> 00:29:52,060
user did anybody that we deem similar to

00:29:49,510 --> 00:29:55,210
them also interact with this product in

00:29:52,060 --> 00:29:56,860
the same way and then from there you

00:29:55,210 --> 00:30:01,180
could decide whether or not you wanted

00:29:56,860 --> 00:30:03,580
to record that data and finally we

00:30:01,180 --> 00:30:05,860
haven't talked about users changing

00:30:03,580 --> 00:30:09,340
their opinion so suppose I wrote from

00:30:05,860 --> 00:30:12,130
Haile and then the next year I changed

00:30:09,340 --> 00:30:14,080
that rating to a dislike can you get

00:30:12,130 --> 00:30:16,810
some extra information from that so will

00:30:14,080 --> 00:30:19,060
I now dislike all of the movies that I

00:30:16,810 --> 00:30:21,040
watched of the same genre in the same

00:30:19,060 --> 00:30:24,620
time period that I first watched that

00:30:21,040 --> 00:30:27,620
initial movie or perhaps I need

00:30:24,620 --> 00:30:30,130
damn wait that behavior from that period

00:30:27,620 --> 00:30:32,419
until we can figure out what's happening

00:30:30,130 --> 00:30:34,580
so that's some ideas for the problems

00:30:32,419 --> 00:30:36,890
that we didn't have time to talk about

00:30:34,580 --> 00:30:41,419
in detail today but let's review what we

00:30:36,890 --> 00:30:44,470
did we've seen that min hash gives us a

00:30:41,419 --> 00:30:47,659
way to obtain summaries of a user's

00:30:44,470 --> 00:30:49,789
interactions with a set of items and

00:30:47,659 --> 00:30:51,880
these summaries are composable thus

00:30:49,789 --> 00:30:55,100
allowing us to capture trends and

00:30:51,880 --> 00:30:57,049
identify changes in users behavior and

00:30:55,100 --> 00:31:00,770
identify different behavior across

00:30:57,049 --> 00:31:03,140
different devices they enable us to look

00:31:00,770 --> 00:31:05,090
at users listening history over periods

00:31:03,140 --> 00:31:07,309
of time and make time sensitive

00:31:05,090 --> 00:31:08,990
recommendations accordingly and this is

00:31:07,309 --> 00:31:12,440
something that we couldn't achieve with

00:31:08,990 --> 00:31:14,210
alternating least-squares this is really

00:31:12,440 --> 00:31:18,080
going to help to make any recommendation

00:31:14,210 --> 00:31:25,669
service more personable more adaptive

00:31:18,080 --> 00:31:27,620
and thus more successful if this week we

00:31:25,669 --> 00:31:30,110
see that one of our users is playing

00:31:27,620 --> 00:31:32,390
lots of breakup songs we might wait this

00:31:30,110 --> 00:31:34,549
highly for a while and recommend them

00:31:32,390 --> 00:31:36,529
more breakup songs until they stop

00:31:34,549 --> 00:31:38,899
listening to them and then we might want

00:31:36,529 --> 00:31:43,070
to be quite kind and remove that chunk

00:31:38,899 --> 00:31:46,340
of music history from their user history

00:31:43,070 --> 00:31:49,760
I saw eyes do not influence further

00:31:46,340 --> 00:31:51,980
recommendations and we also saw how a

00:31:49,760 --> 00:31:54,740
locality sensitive hashing enables us to

00:31:51,980 --> 00:31:56,360
identify candidate pairs of similar

00:31:54,740 --> 00:31:59,210
users and this prevents us from

00:31:56,360 --> 00:32:02,539
comparing every user of a system in

00:31:59,210 --> 00:32:04,460
order to find similar users so please

00:32:02,539 --> 00:32:05,840
stay in touch I think we might have some

00:32:04,460 --> 00:32:09,289
time now for questions

00:32:05,840 --> 00:32:10,850
I'm also around for the next two days so

00:32:09,289 --> 00:32:12,890
if anyone wants to chat about

00:32:10,850 --> 00:32:15,970
recommendation engines then that would

00:32:12,890 --> 00:32:15,970
be great thanks

00:32:20,140 --> 00:32:29,080
I think was remarkable any questions in

00:32:25,150 --> 00:32:32,110
the recommendations there hi sorry yeah

00:32:29,080 --> 00:32:34,090
thank you for the good talk and like you

00:32:32,110 --> 00:32:36,220
said in the beginning that the offline

00:32:34,090 --> 00:32:38,230
measures don't work and a B testing

00:32:36,220 --> 00:32:43,150
works but a/b testing is like really

00:32:38,230 --> 00:32:45,610
very expensive and like so there are

00:32:43,150 --> 00:32:48,220
some learning to rank matrix like

00:32:45,610 --> 00:32:50,350
procedure at rate K map mean a fresh

00:32:48,220 --> 00:32:53,920
precision which are used in all

00:32:50,350 --> 00:32:55,540
recommendation competitions finally they

00:32:53,920 --> 00:32:57,970
are like they are proved to be useful

00:32:55,540 --> 00:33:01,630
over time okay

00:32:57,970 --> 00:33:07,360
and like they also correlate or a good

00:33:01,630 --> 00:33:11,320
proxy for AP testing okay yeah okay and

00:33:07,360 --> 00:33:13,560
also in the plain vanilla alternating

00:33:11,320 --> 00:33:17,980
least squares the matrix factorization

00:33:13,560 --> 00:33:19,810
there is a way to decay the time and yes

00:33:17,980 --> 00:33:22,840
in the equation I don't know if you had

00:33:19,810 --> 00:33:27,070
time to I mean that also handles time

00:33:22,840 --> 00:33:29,740
decay the matrix factorization in the

00:33:27,070 --> 00:33:31,660
beginning which you talked about yes so

00:33:29,740 --> 00:33:44,050
that does get worse over time that's

00:33:31,660 --> 00:33:47,830
certainly the case yeah yeah let's check

00:33:44,050 --> 00:33:49,920
out that offline thank you I think I

00:33:47,830 --> 00:33:52,420
missed something in the presentation how

00:33:49,920 --> 00:33:54,760
exactly did you tackle the item called

00:33:52,420 --> 00:34:00,430
start problem there because you talked

00:33:54,760 --> 00:34:03,480
about it and then yes so the cold start

00:34:00,430 --> 00:34:06,720
problem is certainly something that

00:34:03,480 --> 00:34:10,390
alternatingly squares deals with

00:34:06,720 --> 00:34:13,600
terribly because you don't have the

00:34:10,390 --> 00:34:15,460
space allocated in your matrices to

00:34:13,600 --> 00:34:20,070
store the information about these new

00:34:15,460 --> 00:34:20,070
items for example and

00:34:21,809 --> 00:34:28,299
whereas so it would the algorithm would

00:34:26,020 --> 00:34:29,710
stumble if you passed it something it

00:34:28,299 --> 00:34:32,079
hadn't seen before there are some

00:34:29,710 --> 00:34:34,569
streaming implementations that are out

00:34:32,079 --> 00:34:36,490
there and they can be used but it takes

00:34:34,569 --> 00:34:38,109
a good while to get some good

00:34:36,490 --> 00:34:40,929
recommendations out of it

00:34:38,109 --> 00:34:43,059
I guess the advantage of the composable

00:34:40,929 --> 00:34:45,579
techniques is because of the hashing

00:34:43,059 --> 00:34:47,950
there's just no stumbling you don't have

00:34:45,579 --> 00:34:50,770
to worry so much so you would have to

00:34:47,950 --> 00:34:52,839
increase the size of the bit vectors to

00:34:50,770 --> 00:34:56,020
acknowledge that a new item had been

00:34:52,839 --> 00:34:57,609
added to the system but you can use the

00:34:56,020 --> 00:34:59,740
same Manoj Singh nature that you're

00:34:57,609 --> 00:35:04,030
already using and just update it with

00:34:59,740 --> 00:35:08,980
this new hashed item without it causing

00:35:04,030 --> 00:35:11,230
a problem could they read more about

00:35:08,980 --> 00:35:13,750
this approach in a scientific paper yes

00:35:11,230 --> 00:35:17,099
why not drop me an email and I'll send

00:35:13,750 --> 00:35:20,410
you some citation okay thanks

00:35:17,099 --> 00:35:22,180
any other questions yes please as a

00:35:20,410 --> 00:35:26,650
question of the opinion otherwise it's

00:35:22,180 --> 00:35:28,780
confusing okay actually I implemented

00:35:26,650 --> 00:35:31,180
something like this for item item

00:35:28,780 --> 00:35:32,710
recommendation not for a user but I'm

00:35:31,180 --> 00:35:35,290
curious how do you handle the fact that

00:35:32,710 --> 00:35:37,390
person never listened or never even seen

00:35:35,290 --> 00:35:44,079
that there is a stream that you can see

00:35:37,390 --> 00:35:46,240
they're speaking about goals and sees an

00:35:44,079 --> 00:35:48,640
item yeah the question is if the person

00:35:46,240 --> 00:35:55,660
ever found the item how do you deal with

00:35:48,640 --> 00:35:58,329
it so let me just double check so a

00:35:55,660 --> 00:36:01,599
person they just don't know that it

00:35:58,329 --> 00:36:04,780
exists decides yeah exactly

00:36:01,599 --> 00:36:07,059
so in your case if the person sees it vd

00:36:04,780 --> 00:36:09,040
for more than two or three four times

00:36:07,059 --> 00:36:12,670
you say okay this is great the guy likes

00:36:09,040 --> 00:36:15,190
it yeah but if I never found that music

00:36:12,670 --> 00:36:17,440
Wow how do you handle with it I'm seeing

00:36:15,190 --> 00:36:19,630
myself for instance in Netflix I like

00:36:17,440 --> 00:36:21,400
some some movies but maybe I would not

00:36:19,630 --> 00:36:30,800
like some other movies but I simply

00:36:21,400 --> 00:36:32,720
don't know that they exist okay so if if

00:36:30,800 --> 00:36:34,400
if no one in the population knows that

00:36:32,720 --> 00:36:37,190
they exist then it's certainly the case

00:36:34,400 --> 00:36:39,890
they would never be recommended to you I

00:36:37,190 --> 00:37:00,440
mean hopefully if somebody but similar

00:36:39,890 --> 00:37:02,180
user behavior has okay so in terms of

00:37:00,440 --> 00:37:05,420
when you do your factorization in

00:37:02,180 --> 00:37:08,330
alternating least squares then in the

00:37:05,420 --> 00:37:10,190
composable okay let's let's chat offline

00:37:08,330 --> 00:37:12,260
and I'll make sure I know what's

00:37:10,190 --> 00:37:16,270
happening I think this is a problem

00:37:12,260 --> 00:37:19,640
about exploration and exploitation

00:37:16,270 --> 00:37:22,790
because when a new item is in the system

00:37:19,640 --> 00:37:25,670
you kind of want to force it to get the

00:37:22,790 --> 00:37:29,420
information from it so you want to show

00:37:25,670 --> 00:37:31,820
it to some users to get some grades and

00:37:29,420 --> 00:37:34,310
maybe later put it in a different face

00:37:31,820 --> 00:37:37,520
like I see so perhaps we could approach

00:37:34,310 --> 00:37:42,760
it in the post processing situation as

00:37:37,520 --> 00:37:46,240
well okay so suggest it to many users

00:37:42,760 --> 00:37:46,240
okay thank you

00:37:46,780 --> 00:37:58,280
the last one maybe well thanks so a

00:37:54,470 --> 00:38:01,180
really great talk have you investigated

00:37:58,280 --> 00:38:05,000
any ways or done any work around

00:38:01,180 --> 00:38:07,550
potential ways to include context and

00:38:05,000 --> 00:38:09,560
metadata into this scheme so at the

00:38:07,550 --> 00:38:12,109
moment it's it's based purely on the

00:38:09,560 --> 00:38:14,119
co-occurrence which is potentially one

00:38:12,109 --> 00:38:16,430
of the downsides of pure ALS but there's

00:38:14,119 --> 00:38:19,760
yeah you have a lot of rich context

00:38:16,430 --> 00:38:21,530
around you know things like he you've

00:38:19,760 --> 00:38:26,210
got a little bit of it there with time

00:38:21,530 --> 00:38:28,130
and with with with devices but are there

00:38:26,210 --> 00:38:30,140
other ways to include item metadata to a

00:38:28,130 --> 00:38:31,790
user metadata that you're all about is

00:38:30,140 --> 00:38:34,280
it just a case of having different mesh

00:38:31,790 --> 00:38:36,680
structures you know group are keys or

00:38:34,280 --> 00:38:38,150
are there ways to combine and we'll

00:38:36,680 --> 00:38:40,520
build them into the signature you

00:38:38,150 --> 00:38:43,369
thinking like that yes

00:38:40,520 --> 00:38:43,700
so thanks Nick that's a great question

00:38:43,369 --> 00:38:45,920
it's

00:38:43,700 --> 00:38:48,740
absolutely not something that we have

00:38:45,920 --> 00:38:53,060
implemented but it certainly seems like

00:38:48,740 --> 00:38:54,800
it would be possible to investigate what

00:38:53,060 --> 00:38:56,839
the benefits of keeping different

00:38:54,800 --> 00:38:59,710
Menasha signatures for different genres

00:38:56,839 --> 00:39:03,410
are for example and so on

00:38:59,710 --> 00:39:08,270
yeah that would be that would be good to

00:39:03,410 --> 00:39:09,980
see you sort of only thought about using

00:39:08,270 --> 00:39:12,500
that extra metadata in terms of

00:39:09,980 --> 00:39:14,420
post-processing really rather than

00:39:12,500 --> 00:39:15,740
embedding it into different min hash

00:39:14,420 --> 00:39:17,540
signatures so if anyone has any

00:39:15,740 --> 00:39:22,190
experience in this let us know because

00:39:17,540 --> 00:39:24,620
we'd be interested okay okay thank you

00:39:22,190 --> 00:39:29,539
awesome thank you very much

00:39:24,620 --> 00:39:29,539

YouTube URL: https://www.youtube.com/watch?v=9o2q88UUmTM


