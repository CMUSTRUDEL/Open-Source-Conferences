Title: Berlin Buzzwords 2019: Jakub Piasecki & Fabian Hueske â€“ 7 reasons to use Apache Flink #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Jakub Piasecki and Fabian Hueske talking about "7 Reasons to use Apache Flink for your IoT Project - How We Built a Real-time Asset Tracking System".

IoT data poses several challenges to data processing systems. The volume of machine-generated data is huge, users expect timely reactions as soon as real-world events are detected by remote sensors, and connections to edge devices often suffer from varying and often high transfer latencies, resulting in data arriving out-of-order. Apache Flink is an open-source stream processor, that addresses the challenges that IoT data presents. Flink applications run in production at a massive scale at many enterprises and companies, including Alibaba, Netflix, and Uber. In this talk, we will discuss seven reasons why Apache Flink is well-suited for your IoT data project and present how we built a system for real-time RFID asset tracking that is backed by Apache Flink.

Read more:
https://2019.berlinbuzzwords.de/19/session/7-reasons-use-apache-flink-your-iot-project-how-we-built-real-time-asset-tracking-system

About Jakub Piasecki:
https://2019.berlinbuzzwords.de/users/jakub-piasecki

About Fabian Hueske:
https://2019.berlinbuzzwords.de/users/fabian-hueske-1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,750 --> 00:00:15,180
hello and

00:00:11,680 --> 00:00:18,360
fine thanks for sticking around with us

00:00:15,180 --> 00:00:21,360
our so I think that or I assume that we

00:00:18,360 --> 00:00:25,710
have a roomful of Apache think or alt

00:00:21,360 --> 00:00:28,170
fans which is awesome so we've been

00:00:25,710 --> 00:00:30,630
already introduced a microcassette ski

00:00:28,170 --> 00:00:32,809
working for Freeport metrics that I'll

00:00:30,630 --> 00:00:37,610
be presenting for my company today

00:00:32,809 --> 00:00:40,530
co-presenting with fabian Houska

00:00:37,610 --> 00:00:42,300
co-creator of flying working for very

00:00:40,530 --> 00:00:46,230
very very very car

00:00:42,300 --> 00:00:51,420
ok so today first I will talk a little

00:00:46,230 --> 00:00:53,940
bit about IOT data and how IOT

00:00:51,420 --> 00:00:56,399
applications are a bit different from

00:00:53,940 --> 00:00:58,680
other applications then Fabian will talk

00:00:56,399 --> 00:01:01,500
a little bit about Apache flying and how

00:00:58,680 --> 00:01:04,739
it helps with IOT then I will take over

00:01:01,500 --> 00:01:07,200
to Joe to talk just a little bit about

00:01:04,739 --> 00:01:09,570
how we used as report metrics Apache

00:01:07,200 --> 00:01:12,420
flame Frank to build an asset tracking

00:01:09,570 --> 00:01:14,070
system for one of our clients and then I

00:01:12,420 --> 00:01:17,490
will pass it down again to Fabian to

00:01:14,070 --> 00:01:20,640
talk about different oil to use cases ok

00:01:17,490 --> 00:01:26,850
so what is so special about IOT data and

00:01:20,640 --> 00:01:28,740
IOT applications when I started creating

00:01:26,850 --> 00:01:32,810
this presentation the first question I

00:01:28,740 --> 00:01:36,170
asked to myself was how much data do i

00:01:32,810 --> 00:01:41,250
generate consciously or unconsciously

00:01:36,170 --> 00:01:43,409
and it surprised me when I thought about

00:01:41,250 --> 00:01:46,530
it and not not to say they discard me

00:01:43,409 --> 00:01:48,710
but I like to i liked you to walk

00:01:46,530 --> 00:01:51,780
through this exercise with me today and

00:01:48,710 --> 00:01:55,619
think about how much data data do you

00:01:51,780 --> 00:01:58,439
generate maybe you drove through a an

00:01:55,619 --> 00:02:03,500
automated table of other highway or

00:01:58,439 --> 00:02:06,780
maybe you rented an electric scooter and

00:02:03,500 --> 00:02:11,780
when I when I felt like what data I

00:02:06,780 --> 00:02:13,980
produced like more frequently maybe you

00:02:11,780 --> 00:02:16,980
didn't like a self check out at the

00:02:13,980 --> 00:02:18,959
grocery store or maybe you used in GPS

00:02:16,980 --> 00:02:21,390
and your data was was sent to some

00:02:18,959 --> 00:02:25,310
servers to create traffic information

00:02:21,390 --> 00:02:28,380
that is sent to other users or maybe you

00:02:25,310 --> 00:02:30,480
interacted with the smart home device

00:02:28,380 --> 00:02:32,150
and then when you think like what kind

00:02:30,480 --> 00:02:35,670
of data you produce basically every day

00:02:32,150 --> 00:02:38,840
some data this is sent and start outside

00:02:35,670 --> 00:02:42,660
and are processed in some data centers

00:02:38,840 --> 00:02:45,360
and maybe your utility company is using

00:02:42,660 --> 00:02:48,300
smart meters and when you woke up today

00:02:45,360 --> 00:02:52,010
morning and you started your coffee

00:02:48,300 --> 00:02:55,710
machine this spike in energy usage was

00:02:52,010 --> 00:02:59,630
start somewhere or maybe you are using a

00:02:55,710 --> 00:03:03,120
fitness tracker and your pulse is

00:02:59,630 --> 00:03:04,470
measured every five minutes let's say or

00:03:03,120 --> 00:03:09,240
maybe there are applications setting

00:03:04,470 --> 00:03:11,640
your locations in background okay so we

00:03:09,240 --> 00:03:15,000
just covered like some examples from

00:03:11,640 --> 00:03:18,390
everyday life but there is another type

00:03:15,000 --> 00:03:21,570
of IOT data which is industrial data and

00:03:18,390 --> 00:03:24,420
some use cases can be like measuring so

00:03:21,570 --> 00:03:26,880
arm pardonez data on maybe when you are

00:03:24,420 --> 00:03:30,690
right here at Berlin your luggage was

00:03:26,880 --> 00:03:32,520
automatically tracked by a system or

00:03:30,690 --> 00:03:36,300
maybe you can imagine sensors along a

00:03:32,520 --> 00:03:39,090
production line and I believe that we

00:03:36,300 --> 00:03:40,950
will see more and more you know to use

00:03:39,090 --> 00:03:44,610
casings coming because there are more

00:03:40,950 --> 00:03:47,310
sensors all around that some of them

00:03:44,610 --> 00:03:49,080
like more interesting use cases are in

00:03:47,310 --> 00:03:51,840
my opinion like predictive made

00:03:49,080 --> 00:03:56,130
maintenance all the use cases related to

00:03:51,840 --> 00:03:57,960
smart cities and smart buildings there

00:03:56,130 --> 00:04:06,930
is a lot of going down in half course

00:03:57,960 --> 00:04:10,040
well like use like you so like those use

00:04:06,930 --> 00:04:12,630
cases create a very broad spectrum and

00:04:10,040 --> 00:04:14,880
some people may argue that some of the

00:04:12,630 --> 00:04:18,090
examples I presented are not strictly

00:04:14,880 --> 00:04:20,820
IOT data to be honest I spent some time

00:04:18,090 --> 00:04:23,610
before the talk to verify whether people

00:04:20,820 --> 00:04:25,950
consider mobile phone produced data IOT

00:04:23,610 --> 00:04:26,670
data and many don't but all of this data

00:04:25,950 --> 00:04:30,210
sure

00:04:26,670 --> 00:04:32,550
come on properties one common properties

00:04:30,210 --> 00:04:36,780
is that there is just a lot of data

00:04:32,550 --> 00:04:38,130
produced and Cisco predicts that in 2019

00:04:36,780 --> 00:04:40,130
there's going to be five hundred

00:04:38,130 --> 00:04:41,690
zettabytes

00:04:40,130 --> 00:04:44,870
produce there's a lot of data this

00:04:41,690 --> 00:04:46,970
number may seem abstract what I think is

00:04:44,870 --> 00:04:51,110
important that even if you assume that

00:04:46,970 --> 00:04:53,030
ten percent of this data is useful it's

00:04:51,110 --> 00:04:55,520
still much more that it can be stored in

00:04:53,030 --> 00:04:59,960
data centers so some of this data needs

00:04:55,520 --> 00:05:02,300
to be processed or there's a chance for

00:04:59,960 --> 00:05:06,130
this data to be processed in real time

00:05:02,300 --> 00:05:09,020
and still be useful of course like the

00:05:06,130 --> 00:05:12,440
for a regular application developer

00:05:09,020 --> 00:05:15,050
that's another problem but even on a

00:05:12,440 --> 00:05:17,560
smaller scale you can see that the

00:05:15,050 --> 00:05:20,780
amount of data that is generated

00:05:17,560 --> 00:05:24,170
automatically can be pretty large the

00:05:20,780 --> 00:05:27,140
second property that is coming to one of

00:05:24,170 --> 00:05:30,200
the use cases is that you need to deal

00:05:27,140 --> 00:05:33,260
with latency like latencies and data

00:05:30,200 --> 00:05:37,600
that comes out of order especially when

00:05:33,260 --> 00:05:40,250
you transfer it through a mobile network

00:05:37,600 --> 00:05:42,230
oftentimes in industrial use cases you

00:05:40,250 --> 00:05:44,240
have Kate with great weight devices that

00:05:42,230 --> 00:05:46,460
write added own latency and buffer data

00:05:44,240 --> 00:05:49,400
and of course you need to deal with

00:05:46,460 --> 00:05:51,950
failures the first properties that flow

00:05:49,400 --> 00:05:53,360
of the data is continuous which means

00:05:51,950 --> 00:05:54,950
that you want your system to be

00:05:53,360 --> 00:05:59,960
available to ingest this data and

00:05:54,950 --> 00:06:02,450
process it as soon as possible and I

00:05:59,960 --> 00:06:06,830
think like like for all the streaming

00:06:02,450 --> 00:06:11,630
applications basically users don't want

00:06:06,830 --> 00:06:14,890
and sometimes cannot wait data is

00:06:11,630 --> 00:06:17,960
covered in reward and needs to be

00:06:14,890 --> 00:06:20,090
processed and influenced the real world

00:06:17,960 --> 00:06:23,960
what I mean by that let's imagine you

00:06:20,090 --> 00:06:26,660
have like an valuable piece of equipment

00:06:23,960 --> 00:06:29,210
the new track so you would rather know

00:06:26,660 --> 00:06:30,860
sooner sooner than later and then this

00:06:29,210 --> 00:06:33,200
piece of equipment is leaving your

00:06:30,860 --> 00:06:35,150
facility out let's say in case of

00:06:33,200 --> 00:06:37,430
predictive maintenance the sooner you

00:06:35,150 --> 00:06:43,550
stop the machine that you think may fail

00:06:37,430 --> 00:06:48,080
the the higher chance of for the machine

00:06:43,550 --> 00:06:51,800
that it not fail so as we covered the

00:06:48,080 --> 00:06:53,840
basic properties of IOT data we hand

00:06:51,800 --> 00:06:58,910
over to forensic and talk about how

00:06:53,840 --> 00:07:01,490
think absolute yeah thanks so yeah so

00:06:58,910 --> 00:07:03,440
I'm we right now cover basically what is

00:07:01,490 --> 00:07:08,000
what are the properties of IOT data and

00:07:03,440 --> 00:07:09,050
IOT applications and yeah I'm gonna talk

00:07:08,000 --> 00:07:13,490
about Apache fleeing and how it

00:07:09,050 --> 00:07:16,400
addresses these requirements so for

00:07:13,490 --> 00:07:19,070
those of you who not familiar with the

00:07:16,400 --> 00:07:21,320
Petri flank I have a like brief intro

00:07:19,070 --> 00:07:22,460
slide here to like give you the right

00:07:21,320 --> 00:07:24,470
context

00:07:22,460 --> 00:07:28,580
Apache think is a distributed system for

00:07:24,470 --> 00:07:31,160
stateful stream processing so it ingests

00:07:28,580 --> 00:07:34,040
streams of data and process them

00:07:31,160 --> 00:07:38,110
processes them as the data is basically

00:07:34,040 --> 00:07:40,729
arriving in this system it's stateful so

00:07:38,110 --> 00:07:44,620
certain computations require that you

00:07:40,729 --> 00:07:46,669
keep some state around so imagine

00:07:44,620 --> 00:07:48,889
disabilities simple as example is you

00:07:46,669 --> 00:07:49,970
want to count how many how many events

00:07:48,889 --> 00:07:52,729
you arrive from a stream

00:07:49,970 --> 00:07:54,650
so there's account some some counter

00:07:52,729 --> 00:07:55,820
that you need to increment and whenever

00:07:54,650 --> 00:07:56,990
you get a new event you have to

00:07:55,820 --> 00:07:59,470
increment the count and then this

00:07:56,990 --> 00:08:02,900
variable is basically the state that you

00:07:59,470 --> 00:08:04,580
that this application uses and as it is

00:08:02,900 --> 00:08:06,410
a distributed system you also kind of

00:08:04,580 --> 00:08:08,270
like need to ensure that this state is

00:08:06,410 --> 00:08:11,090
not lost in case something goes wrong

00:08:08,270 --> 00:08:14,599
so flick applications are also disputed

00:08:11,090 --> 00:08:16,669
on tens or hundreds of machines so it's

00:08:14,599 --> 00:08:18,650
very common that something goes wrong

00:08:16,669 --> 00:08:20,570
and goes wrong and a process occur

00:08:18,650 --> 00:08:23,539
process goes down so you need to also

00:08:20,570 --> 00:08:25,849
ensure that this state has been able to

00:08:23,539 --> 00:08:28,270
to be recovered so it's a disputed

00:08:25,849 --> 00:08:31,070
system it processes data streams and

00:08:28,270 --> 00:08:33,140
yeah that's kinda like what Apache

00:08:31,070 --> 00:08:36,409
flingers and now I'm talking a bit about

00:08:33,140 --> 00:08:42,070
the that the properties that make it a

00:08:36,409 --> 00:08:45,680
good fit for IT data so number one is

00:08:42,070 --> 00:08:48,470
obviously that effectively in processes

00:08:45,680 --> 00:08:50,330
data with low latency so we are before

00:08:48,470 --> 00:08:53,810
that many of the applications require

00:08:50,330 --> 00:08:56,510
timely response to to to certain things

00:08:53,810 --> 00:08:59,779
that were measured by sensors GPS

00:08:56,510 --> 00:09:02,300
sensors or I've RFID tag readers or

00:08:59,779 --> 00:09:05,089
whatever whatever the source of the data

00:09:02,300 --> 00:09:07,520
is but oftentimes you want the

00:09:05,089 --> 00:09:12,320
application to respond

00:09:07,520 --> 00:09:15,080
faster faster this and if link has a

00:09:12,320 --> 00:09:18,709
couple of properties that make that that

00:09:15,080 --> 00:09:22,100
make a processing of data possible with

00:09:18,709 --> 00:09:24,110
low latency first of all a state is

00:09:22,100 --> 00:09:27,080
always locally maintained so if link

00:09:24,110 --> 00:09:28,970
does not store state in a distributed

00:09:27,080 --> 00:09:30,890
database for instance like the count

00:09:28,970 --> 00:09:32,300
that I was talking about is always kept

00:09:30,890 --> 00:09:36,010
on the local machine that is processing

00:09:32,300 --> 00:09:38,570
the events so you don't not call into an

00:09:36,010 --> 00:09:41,089
remote key value store - hey what's the

00:09:38,570 --> 00:09:42,550
latest counter you get the RISC result

00:09:41,089 --> 00:09:44,600
back you increment it and write it back

00:09:42,550 --> 00:09:48,290
instead all the state is locally

00:09:44,600 --> 00:09:50,330
maintained either in memory of if your

00:09:48,290 --> 00:09:53,440
state grows too large you can also put

00:09:50,330 --> 00:09:56,870
it on rocks to be on the local hard disk

00:09:53,440 --> 00:10:00,490
the network status is optimized for low

00:09:56,870 --> 00:10:04,580
latency so we recently improved this tag

00:10:00,490 --> 00:10:08,120
having a mechanism called a credit based

00:10:04,580 --> 00:10:12,260
flow control that basically helps the

00:10:08,120 --> 00:10:14,149
system - - to know from which channels

00:10:12,260 --> 00:10:16,190
to consume later there's a pretty much

00:10:14,149 --> 00:10:17,810
pretty detailed blog post that we

00:10:16,190 --> 00:10:19,160
recently published on the fling block so

00:10:17,810 --> 00:10:23,540
if you're interested in this I would

00:10:19,160 --> 00:10:26,990
just point you to this blog post and the

00:10:23,540 --> 00:10:29,750
way that flink supports state

00:10:26,990 --> 00:10:31,970
consistency is check pointing the check

00:10:29,750 --> 00:10:34,279
party mechanism is also designed in a

00:10:31,970 --> 00:10:36,410
way that whenever the system takes a

00:10:34,279 --> 00:10:37,910
periodic check point there's are as

00:10:36,410 --> 00:10:39,890
little intervention in the regular

00:10:37,910 --> 00:10:42,680
processing as possible and we achieved

00:10:39,890 --> 00:10:46,010
that by doing these check points in an

00:10:42,680 --> 00:10:52,420
asynchronous fashion and also support

00:10:46,010 --> 00:10:55,490
for incommoded check points so the next

00:10:52,420 --> 00:10:57,230
reason number two is fling City

00:10:55,490 --> 00:11:00,589
submitted system at its game can scale

00:10:57,230 --> 00:11:02,860
to large data volumes what this

00:11:00,589 --> 00:11:06,500
basically means is that data streams are

00:11:02,860 --> 00:11:07,610
partitioned so it's and therefore we

00:11:06,500 --> 00:11:11,000
distribute the data and we also

00:11:07,610 --> 00:11:14,260
distribute the computation to possibly

00:11:11,000 --> 00:11:17,150
many machines in a computer cluster and

00:11:14,260 --> 00:11:19,760
basically by just routing the data to

00:11:17,150 --> 00:11:20,759
two different processes we can scale out

00:11:19,760 --> 00:11:23,279
the computer

00:11:20,759 --> 00:11:24,120
flink applications cannot run at very

00:11:23,279 --> 00:11:27,720
large scale

00:11:24,120 --> 00:11:30,480
so at ten thousand-plus chorus and also

00:11:27,720 --> 00:11:32,930
process huge amounts of events for

00:11:30,480 --> 00:11:35,850
instance Netflix is using fling at

00:11:32,930 --> 00:11:37,589
pretty pretty large scale processing

00:11:35,850 --> 00:11:42,180
five trolling events every day which is

00:11:37,589 --> 00:11:46,529
like about 50 50 million events per

00:11:42,180 --> 00:11:49,110
second and as I said by like disputing

00:11:46,529 --> 00:11:51,059
the this state we can also like you can

00:11:49,110 --> 00:11:52,800
think of this that this state is like

00:11:51,059 --> 00:11:57,449
distributed in the commute classic a

00:11:52,800 --> 00:12:00,480
similar to a key value store so we route

00:11:57,449 --> 00:12:03,529
the data to where the state is modify

00:12:00,480 --> 00:12:06,089
the state locally and then keep on going

00:12:03,529 --> 00:12:10,829
fleeing is also support for scaling

00:12:06,089 --> 00:12:15,149
application out and in so that is also

00:12:10,829 --> 00:12:19,079
possible if in case you your application

00:12:15,149 --> 00:12:21,149
has some kind of like a pattern where

00:12:19,079 --> 00:12:23,610
you need to ingest more data or less

00:12:21,149 --> 00:12:29,999
data over time you can also adjust the

00:12:23,610 --> 00:12:33,929
parallelism reason number two three

00:12:29,999 --> 00:12:36,509
as we heard IOT data is often well not

00:12:33,929 --> 00:12:38,879
of the not of the best quality it arise

00:12:36,509 --> 00:12:43,069
out of order sometimes sends a produce a

00:12:38,879 --> 00:12:45,839
bit of messy data flink is able to

00:12:43,069 --> 00:12:48,439
perform computations in avenged time

00:12:45,839 --> 00:12:50,910
what this basically means is that fling

00:12:48,439 --> 00:12:54,290
processes data based on a timestamp that

00:12:50,910 --> 00:12:58,339
is encoded in the data instead of

00:12:54,290 --> 00:13:00,809
processing data when it arrives at the

00:12:58,339 --> 00:13:04,110
processing machine so imagine you want

00:13:00,809 --> 00:13:05,999
to for instance account something every

00:13:04,110 --> 00:13:08,970
five minutes or for five minutes you

00:13:05,999 --> 00:13:11,639
want to know how many how many events

00:13:08,970 --> 00:13:13,800
you received the easy way to do it is to

00:13:11,639 --> 00:13:16,620
look at the watch or let the computer

00:13:13,800 --> 00:13:19,319
check what is the time now and then

00:13:16,620 --> 00:13:21,929
collect all the events on 2 or 5 minutes

00:13:19,319 --> 00:13:24,269
and then you're done but this technique

00:13:21,929 --> 00:13:26,879
obviously depends on how much the

00:13:24,269 --> 00:13:27,569
machine can consume if there is like too

00:13:26,879 --> 00:13:30,209
much pressure

00:13:27,569 --> 00:13:31,589
it might get bottlenecked and the

00:13:30,209 --> 00:13:33,670
correct way of doing this is really

00:13:31,589 --> 00:13:37,360
using the time stamps in the data and to

00:13:33,670 --> 00:13:39,820
these computations there is this concept

00:13:37,360 --> 00:13:44,110
of watermarks which basically control

00:13:39,820 --> 00:13:47,890
gives is a mechanism to let the system

00:13:44,110 --> 00:13:50,740
know about the progress of time in the

00:13:47,890 --> 00:13:53,920
stream so we said that every record has

00:13:50,740 --> 00:13:57,430
has a time stamp and with water marks

00:13:53,920 --> 00:14:00,490
Lincoln or a system that uses what I'm

00:13:57,430 --> 00:14:04,600
asking reason about what time stems to

00:14:00,490 --> 00:14:07,270
to to expect so it gives some gives

00:14:04,600 --> 00:14:08,940
gives an application a way of reasoning

00:14:07,270 --> 00:14:12,160
about the completeness of a stream and

00:14:08,940 --> 00:14:14,140
by that it can perform a certain

00:14:12,160 --> 00:14:16,300
computation that it knows okay my stream

00:14:14,140 --> 00:14:19,600
is now at 12 o'clock it can perform all

00:14:16,300 --> 00:14:21,400
computations that received all the data

00:14:19,600 --> 00:14:25,870
until twelve o'clock and then also

00:14:21,400 --> 00:14:27,640
discard the state possibly it was linked

00:14:25,870 --> 00:14:29,770
it's also some support and API

00:14:27,640 --> 00:14:32,830
primitives that make it very easy to

00:14:29,770 --> 00:14:36,970
smoothen in a cratering imprecise sensor

00:14:32,830 --> 00:14:39,790
data like GPS signals or temperature

00:14:36,970 --> 00:14:41,710
sensors so you can like also smooth that

00:14:39,790 --> 00:14:49,680
out with it just like one command

00:14:41,710 --> 00:14:53,400
basically yeah IOT applications should

00:14:49,680 --> 00:14:56,860
some of these provide mission critical

00:14:53,400 --> 00:15:00,820
infrastructure and also you basically

00:14:56,860 --> 00:15:04,330
want them to run continuously and in

00:15:00,820 --> 00:15:05,650
flink we achieved that by having these

00:15:04,330 --> 00:15:09,640
checkpoints that was talking earlier

00:15:05,650 --> 00:15:11,170
about checkpoints basically guaranteeing

00:15:09,640 --> 00:15:13,680
that the state that an application has

00:15:11,170 --> 00:15:13,680
is never lost

00:15:13,800 --> 00:15:19,180
instead fleeing provides exactly once

00:15:16,600 --> 00:15:21,100
guarantees for the state if something

00:15:19,180 --> 00:15:23,170
goes down fling buried if link

00:15:21,100 --> 00:15:25,870
application will basically load this

00:15:23,170 --> 00:15:29,440
state that is has been written to some

00:15:25,870 --> 00:15:31,780
persistent storage in HDFS or s3 will

00:15:29,440 --> 00:15:32,950
load this copy of the state loaded again

00:15:31,780 --> 00:15:36,430
into the application and then the

00:15:32,950 --> 00:15:36,970
application will continue as if nothing

00:15:36,430 --> 00:15:39,160
ever happened

00:15:36,970 --> 00:15:41,320
so the state also includes the reading

00:15:39,160 --> 00:15:44,200
positions in the source dreams like

00:15:41,320 --> 00:15:46,860
Kafka offsets for instance

00:15:44,200 --> 00:15:52,660
yeah flink is also support for

00:15:46,860 --> 00:15:54,430
highly available setups and can also run

00:15:52,660 --> 00:15:59,020
in different resource managers together

00:15:54,430 --> 00:16:01,350
with zookeeper flink will also be able

00:15:59,020 --> 00:16:04,630
to recover from massive failures and

00:16:01,350 --> 00:16:07,930
just deploy the new new processes and

00:16:04,630 --> 00:16:14,860
again continue as if the error didn't

00:16:07,930 --> 00:16:15,790
happen this is a very nice feature and

00:16:14,860 --> 00:16:22,750
often inept

00:16:15,790 --> 00:16:23,940
coyote applications these these some of

00:16:22,750 --> 00:16:27,180
some of the applications basically

00:16:23,940 --> 00:16:30,490
measure real time events and you want to

00:16:27,180 --> 00:16:33,730
figure out whether or you have a certain

00:16:30,490 --> 00:16:40,030
pattern in mind in which events should

00:16:33,730 --> 00:16:42,100
happen for instance your you have an

00:16:40,030 --> 00:16:43,270
application that receives an order and

00:16:42,100 --> 00:16:45,130
then the order should be processed into

00:16:43,270 --> 00:16:46,510
only and finally should be shipped so

00:16:45,130 --> 00:16:50,050
it's like three bands that you want

00:16:46,510 --> 00:16:52,150
these to happen in a certain in a

00:16:50,050 --> 00:16:55,690
certain order and also within a certain

00:16:52,150 --> 00:17:01,420
time frame then you can define such a

00:16:55,690 --> 00:17:04,750
pattern in flink has support for so

00:17:01,420 --> 00:17:06,760
called CP library that is based on the

00:17:04,750 --> 00:17:09,790
data stream API in which you can define

00:17:06,760 --> 00:17:12,670
a pattern basically saying hey I want

00:17:09,790 --> 00:17:16,089
first event a to happen then I want

00:17:12,670 --> 00:17:18,579
event B or C to happen and finally I

00:17:16,089 --> 00:17:20,380
wanted an event D to happen so you can

00:17:18,579 --> 00:17:23,290
define these like reg acts like patterns

00:17:20,380 --> 00:17:25,480
and when you then deploy such an

00:17:23,290 --> 00:17:26,980
application if we were like just monitor

00:17:25,480 --> 00:17:31,180
the stream and whenever the pattern

00:17:26,980 --> 00:17:34,750
matches you get a you get a new event

00:17:31,180 --> 00:17:38,470
and can implement an application that

00:17:34,750 --> 00:17:40,600
just acts on this event there's also you

00:17:38,470 --> 00:17:44,080
cannot only do this with this CP library

00:17:40,600 --> 00:17:47,440
there's also a recent extension of the

00:17:44,080 --> 00:17:50,170
sequel standard so sequel or 2016 there

00:17:47,440 --> 00:17:53,590
was this match recognized clause added

00:17:50,170 --> 00:17:55,840
and this is pretty much exactly this so

00:17:53,590 --> 00:17:59,950
you basically defined a pattern over

00:17:55,840 --> 00:18:01,570
some audit relation and

00:17:59,950 --> 00:18:03,100
can can react to that so there's

00:18:01,570 --> 00:18:07,420
something like a prime use case for a

00:18:03,100 --> 00:18:09,400
sequel for four for streaming sequel and

00:18:07,420 --> 00:18:11,530
flink supports or streaming secrets so

00:18:09,400 --> 00:18:14,380
you can even implement these these kinds

00:18:11,530 --> 00:18:15,880
of patterns in a secret theory and the

00:18:14,380 --> 00:18:18,130
really nice thing is that you can

00:18:15,880 --> 00:18:22,720
combine it also with data analytics so

00:18:18,130 --> 00:18:24,910
you can either first have this pattern

00:18:22,720 --> 00:18:26,500
and then basic count how often did this

00:18:24,910 --> 00:18:30,150
pattern occur in five minutes but you

00:18:26,500 --> 00:18:30,150
can also turn it around and first to

00:18:30,180 --> 00:18:37,000
perform some aggregation and create a

00:18:34,330 --> 00:18:40,030
new event whenever a certain threshold

00:18:37,000 --> 00:18:44,110
was was exceeded and then have a pattern

00:18:40,030 --> 00:18:46,240
depending on these threads threshold

00:18:44,110 --> 00:18:48,940
exceeding events so I could like to

00:18:46,240 --> 00:18:52,930
figure out if something violated here

00:18:48,940 --> 00:18:54,970
SLA twice within a certain time frame so

00:18:52,930 --> 00:18:58,050
I think this is a pretty pretty good

00:18:54,970 --> 00:19:00,820
match for from any way to use cases

00:18:58,050 --> 00:19:06,480
finally or not finally it's number six

00:19:00,820 --> 00:19:09,730
so it's very connected with many other

00:19:06,480 --> 00:19:12,190
systems in the in the Big Data space and

00:19:09,730 --> 00:19:18,750
also with messaging systems like cough

00:19:12,190 --> 00:19:21,430
car kinases pulsar it is Lincoln right -

00:19:18,750 --> 00:19:23,530
Cassandra elasticsearch or JDBC

00:19:21,430 --> 00:19:25,720
databases but also too far it's a

00:19:23,530 --> 00:19:28,080
different file formats so there's a

00:19:25,720 --> 00:19:31,600
bunch of connectors available and

00:19:28,080 --> 00:19:34,410
finally I would argue that data

00:19:31,600 --> 00:19:38,950
streaming is like conceptually simple

00:19:34,410 --> 00:19:40,900
and this is basically also I would argue

00:19:38,950 --> 00:19:43,300
the natural way to think about handing

00:19:40,900 --> 00:19:46,000
out events so you get a consistent

00:19:43,300 --> 00:19:48,430
stream of data that you want to process

00:19:46,000 --> 00:19:51,040
one by one and this is basically

00:19:48,430 --> 00:19:51,550
something that flink makes makes makes

00:19:51,040 --> 00:19:56,370
quite easy

00:19:51,550 --> 00:19:58,960
due to its API the data stream API this

00:19:56,370 --> 00:20:02,740
has a couple of high level primitives

00:19:58,960 --> 00:20:04,870
like for instance joins or window

00:20:02,740 --> 00:20:09,070
aggregations but also allows you to go

00:20:04,870 --> 00:20:10,810
deep into into into like the core

00:20:09,070 --> 00:20:12,480
primitives of stream processing like

00:20:10,810 --> 00:20:16,140
state and state in time

00:20:12,480 --> 00:20:17,940
and by implementing your application

00:20:16,140 --> 00:20:21,780
against the status from API you can like

00:20:17,940 --> 00:20:24,540
scale it out basically to any size all

00:20:21,780 --> 00:20:27,210
right with this introduction to fling

00:20:24,540 --> 00:20:30,360
and handing over to yeah coop again who

00:20:27,210 --> 00:20:35,360
will tell us how he built on as a

00:20:30,360 --> 00:20:38,760
tracking system with Lincoln thank you

00:20:35,360 --> 00:20:40,770
so like five introduced a little bit

00:20:38,760 --> 00:20:44,100
about how we had people metrics bills

00:20:40,770 --> 00:20:48,990
and as a tracking system for our client

00:20:44,100 --> 00:20:51,150
ivanhoe a little bit where we are coming

00:20:48,990 --> 00:20:53,810
from at Freeport metrics and we are a

00:20:51,150 --> 00:20:56,880
little bit did sell digital products

00:20:53,810 --> 00:21:00,120
development company so to decipher it

00:20:56,880 --> 00:21:02,180
repeated software for companies that's

00:21:00,120 --> 00:21:06,360
out this substrate other companies and

00:21:02,180 --> 00:21:11,130
over the years we've worked on quite a

00:21:06,360 --> 00:21:15,000
few projects related to industrial data

00:21:11,130 --> 00:21:17,850
processing or IOT data some of it were

00:21:15,000 --> 00:21:21,390
work on projects for solar and wind

00:21:17,850 --> 00:21:24,230
energy farm analytics we worked on a

00:21:21,390 --> 00:21:28,800
couple inventory and warehouse systems

00:21:24,230 --> 00:21:32,430
we worked on automated retail kiosks but

00:21:28,800 --> 00:21:34,470
I think the important part is that this

00:21:32,430 --> 00:21:36,690
was our first project with playing and

00:21:34,470 --> 00:21:40,170
before that we were relying on more

00:21:36,690 --> 00:21:42,690
traditional ETL approaches or our custom

00:21:40,170 --> 00:21:48,120
code for ingesting events or on

00:21:42,690 --> 00:21:52,710
processing them okay so what is an asset

00:21:48,120 --> 00:21:56,070
tracking system to give you some use

00:21:52,710 --> 00:21:58,080
cases the most straightforward use case

00:21:56,070 --> 00:22:00,060
is inventory management so basically you

00:21:58,080 --> 00:22:03,750
want to track if you have all the things

00:22:00,060 --> 00:22:07,620
to do should have the second use cases

00:22:03,750 --> 00:22:10,620
uterine contractions in a warehouse but

00:22:07,620 --> 00:22:13,260
I think the pretty coal use case is an

00:22:10,620 --> 00:22:17,690
actually one of our first pallets of our

00:22:13,260 --> 00:22:20,550
client ease that you hand out RFID

00:22:17,690 --> 00:22:22,860
wristbands to patients and families in

00:22:20,550 --> 00:22:25,410
waiting rooms in hospital can see how

00:22:22,860 --> 00:22:26,930
those patients go for different types of

00:22:25,410 --> 00:22:31,680
medical procedure

00:22:26,930 --> 00:22:33,390
and what's the source of data in in an

00:22:31,680 --> 00:22:36,990
asset tracking system first of all you

00:22:33,390 --> 00:22:40,230
have RFID taxed and can be many of them

00:22:36,990 --> 00:22:43,470
like hundreds of of thousands and those

00:22:40,230 --> 00:22:47,220
tags are tracked by arif a pair of RFID

00:22:43,470 --> 00:22:50,940
antennas they are sent to gateways and

00:22:47,220 --> 00:22:54,480
then to the backend systems by besides

00:22:50,940 --> 00:22:57,570
those tax they are tracked automatically

00:22:54,480 --> 00:23:00,630
you have also users with handheld

00:22:57,570 --> 00:23:08,810
barcode scanners and of course obviously

00:23:00,630 --> 00:23:11,250
like mobile and web interfaces ok so I

00:23:08,810 --> 00:23:12,900
Fabian is definitely like an expert in

00:23:11,250 --> 00:23:16,350
the first thing so I will talk about

00:23:12,900 --> 00:23:19,770
about think that I think was the most

00:23:16,350 --> 00:23:21,180
useful for us in of people batteries

00:23:19,770 --> 00:23:25,580
when you just blink and I think this is

00:23:21,180 --> 00:23:27,990
the computational model of fleeing and

00:23:25,580 --> 00:23:30,720
what I think the most powerful features

00:23:27,990 --> 00:23:32,790
event time like Fabian mentioned like

00:23:30,720 --> 00:23:36,420
you when you have data from multiple

00:23:32,790 --> 00:23:38,010
sources this data will come out of order

00:23:36,420 --> 00:23:40,650
in our case we have alpha in the

00:23:38,010 --> 00:23:43,910
antennas we have users using mobile

00:23:40,650 --> 00:23:46,080
devices you get all those different

00:23:43,910 --> 00:23:49,710
latencies from those different sources

00:23:46,080 --> 00:23:51,870
and to be able to make any sensible

00:23:49,710 --> 00:23:55,110
results in it order it accordingly

00:23:51,870 --> 00:23:59,910
the second thing is referring towards

00:23:55,110 --> 00:24:01,890
fight against that window which is

00:23:59,910 --> 00:24:06,750
always very a very useful feature for

00:24:01,890 --> 00:24:08,910
cleaning up your data in in the use case

00:24:06,750 --> 00:24:11,010
that report on you may have like

00:24:08,910 --> 00:24:14,250
overlapping antennas so the same antenna

00:24:11,010 --> 00:24:16,110
is reading so two antennas are reading

00:24:14,250 --> 00:24:18,360
the same tag like almost at the same

00:24:16,110 --> 00:24:21,990
time so using window you can apply some

00:24:18,360 --> 00:24:26,540
heuristics and to figure out when words

00:24:21,990 --> 00:24:32,040
the tagger really is the second thing is

00:24:26,540 --> 00:24:35,490
state and I think the powerful feature

00:24:32,040 --> 00:24:37,800
of Flanagan is that you'd like forces

00:24:35,490 --> 00:24:39,210
you to partition your state and it'll

00:24:37,800 --> 00:24:42,480
Casey

00:24:39,210 --> 00:24:44,970
we could partition all divins related to

00:24:42,480 --> 00:24:49,049
one assets related to dock to tag into

00:24:44,970 --> 00:24:52,049
like one let's call it group and but

00:24:49,049 --> 00:24:54,480
what you get from that it that it is

00:24:52,049 --> 00:24:57,029
automatically parallelizable and of

00:24:54,480 --> 00:24:59,460
course it affects performance the second

00:24:57,029 --> 00:25:01,620
thing about state is when you have

00:24:59,460 --> 00:25:05,460
stayed you can build state machines and

00:25:01,620 --> 00:25:08,130
some more complex logic in our case we

00:25:05,460 --> 00:25:10,770
built a simple version of like a

00:25:08,130 --> 00:25:13,350
business process modeling tool which i

00:25:10,770 --> 00:25:15,480
thought let us modeled situations like

00:25:13,350 --> 00:25:17,640
let's say you have a patient that enters

00:25:15,480 --> 00:25:21,720
a waiting room and the doctor's office

00:25:17,640 --> 00:25:23,700
and then recovery area and you can also

00:25:21,720 --> 00:25:29,010
use time like if ten minutes part we

00:25:23,700 --> 00:25:31,950
know that the procedure is over a little

00:25:29,010 --> 00:25:34,020
bit about our experience like developers

00:25:31,950 --> 00:25:36,059
experience with using a passive think

00:25:34,020 --> 00:25:39,029
first of all we had a pretty small team

00:25:36,059 --> 00:25:42,960
to work alongside our clients team but

00:25:39,029 --> 00:25:45,690
that that were not many of us and so we

00:25:42,960 --> 00:25:47,190
didn't spend that much time like on on

00:25:45,690 --> 00:25:50,760
this event processing part thanks to

00:25:47,190 --> 00:25:52,350
first of all the functionality that Fink

00:25:50,760 --> 00:25:54,000
provides so that was very useful for us

00:25:52,350 --> 00:25:56,730
and that has progressed on other parts

00:25:54,000 --> 00:25:59,429
of the system second created to that we

00:25:56,730 --> 00:26:03,890
could focus mostly on our our core logic

00:25:59,429 --> 00:26:05,429
of the application another thing is that

00:26:03,890 --> 00:26:09,299
flink provides

00:26:05,429 --> 00:26:11,460
different levels of different different

00:26:09,299 --> 00:26:13,380
levels of abstractions and even the

00:26:11,460 --> 00:26:17,340
api's and I also think this is very

00:26:13,380 --> 00:26:19,140
useful you can work on higher level or

00:26:17,340 --> 00:26:23,549
you need to cast a customized on

00:26:19,140 --> 00:26:26,640
processing function level to and adjust

00:26:23,549 --> 00:26:28,649
it to your use case and needs to say the

00:26:26,640 --> 00:26:32,370
integration with with with tools and

00:26:28,649 --> 00:26:37,649
Kafka is very good and the challenge is

00:26:32,370 --> 00:26:40,320
the interesting part referring to the

00:26:37,649 --> 00:26:42,500
flink being conceptually simple I have

00:26:40,320 --> 00:26:45,390
to say like for our team it was like a

00:26:42,500 --> 00:26:49,500
new way of thinking but I think like

00:26:45,390 --> 00:26:53,130
it's well worth investing after you get

00:26:49,500 --> 00:26:55,290
used to this new programming model it's

00:26:53,130 --> 00:27:00,690
appears like the natural ways of doing

00:26:55,290 --> 00:27:03,510
things second thing is that it I think a

00:27:00,690 --> 00:27:05,130
new feature may require serious planning

00:27:03,510 --> 00:27:07,530
I don't have to convince your

00:27:05,130 --> 00:27:10,740
discomforts that distributed systems are

00:27:07,530 --> 00:27:14,820
hard but also in to get understanding

00:27:10,740 --> 00:27:17,610
from all your team members that the

00:27:14,820 --> 00:27:20,460
change that may seem simple from user

00:27:17,610 --> 00:27:22,410
perspective may require like significant

00:27:20,460 --> 00:27:24,600
adjustment to the system under the hood

00:27:22,410 --> 00:27:27,560
to give you some examples it is very

00:27:24,600 --> 00:27:31,320
important what you use to partition the

00:27:27,560 --> 00:27:36,360
partitioning of your data and/or how do

00:27:31,320 --> 00:27:38,250
you work with latency in the system and

00:27:36,360 --> 00:27:42,410
the fact these users that want to see

00:27:38,250 --> 00:27:44,880
some events immediately or can update

00:27:42,410 --> 00:27:47,340
application from user interfaces that

00:27:44,880 --> 00:27:50,550
require cos likeness and internally

00:27:47,340 --> 00:27:52,170
interaction and the last item is like

00:27:50,550 --> 00:27:54,840
three years ago when we started working

00:27:52,170 --> 00:27:59,040
on the system with our clients other

00:27:54,840 --> 00:28:02,790
than how I would say that access to

00:27:59,040 --> 00:28:04,380
learning materials was emitted but it

00:28:02,790 --> 00:28:07,470
cuts so much better in the last couple

00:28:04,380 --> 00:28:10,560
days because Alaska for last couple

00:28:07,470 --> 00:28:12,780
years I have to say especially with like

00:28:10,560 --> 00:28:15,450
all the couple like video materials

00:28:12,780 --> 00:28:17,610
available and which are all in the last

00:28:15,450 --> 00:28:20,790
couple months I can say like five and

00:28:17,610 --> 00:28:26,640
published at work so so so I wish I had

00:28:20,790 --> 00:28:28,290
fears the girl but that's exactly so I

00:28:26,640 --> 00:28:33,830
think that it's a good point to hand

00:28:28,290 --> 00:28:37,250
over to five and again oh yeah thanks

00:28:33,830 --> 00:28:39,660
alright so we heard about one

00:28:37,250 --> 00:28:45,390
interesting and challenging use case for

00:28:39,660 --> 00:28:47,100
for IOT later and the Petri flank I was

00:28:45,390 --> 00:28:48,810
like when we hit this presentation I

00:28:47,100 --> 00:28:51,150
said well we also have a couple of other

00:28:48,810 --> 00:28:54,810
interesting use cases that I want to

00:28:51,150 --> 00:28:56,340
briefly mention we basically collect

00:28:54,810 --> 00:28:59,330
most of them

00:28:56,340 --> 00:28:59,330
oh there's a nice type of

00:28:59,990 --> 00:29:06,540
we collected a couple of a nice example

00:29:04,710 --> 00:29:10,140
mostly from the fling forward conference

00:29:06,540 --> 00:29:13,430
like one of them is in data-driven

00:29:10,140 --> 00:29:17,400
agriculture by the company John Deere

00:29:13,430 --> 00:29:19,730
they presented this use case at field

00:29:17,400 --> 00:29:23,010
forward San Francisco earlier this year

00:29:19,730 --> 00:29:24,240
John Deere as many of you might know is

00:29:23,010 --> 00:29:27,170
a manufacturer of machines for

00:29:24,240 --> 00:29:29,880
agriculture construction and forestry

00:29:27,170 --> 00:29:32,070
and they also run a data platform that

00:29:29,880 --> 00:29:35,550
provides like data services for farmers

00:29:32,070 --> 00:29:37,260
so what they basically do is their

00:29:35,550 --> 00:29:38,550
machines or at least I guess the newer

00:29:37,260 --> 00:29:43,220
generation of their machines are

00:29:38,550 --> 00:29:45,840
collecting lots of data data that is

00:29:43,220 --> 00:29:49,410
geospatial obviously but also temporal

00:29:45,840 --> 00:29:51,290
and they measure different different

00:29:49,410 --> 00:29:55,350
things like for instance how fast a

00:29:51,290 --> 00:29:57,720
planter is seeding new plants or what's

00:29:55,350 --> 00:30:00,870
the humidity or whatever so they're

00:29:57,720 --> 00:30:02,670
collecting lots of lots of data and this

00:30:00,870 --> 00:30:04,530
is really like at a large scale like a

00:30:02,670 --> 00:30:07,260
single planting machine produces for

00:30:04,530 --> 00:30:09,990
instance like more than 2,000 sensor

00:30:07,260 --> 00:30:11,820
sensor measurements per second and they

00:30:09,990 --> 00:30:14,580
collect all of this data basically and

00:30:11,820 --> 00:30:18,740
then make it available for the farmers

00:30:14,580 --> 00:30:23,250
to analyze it basically to to

00:30:18,740 --> 00:30:27,030
investigate is how much in how much does

00:30:23,250 --> 00:30:31,400
the planting speed influence or the

00:30:27,030 --> 00:30:36,020
yield in this area and so on so this is

00:30:31,400 --> 00:30:38,610
really meant to help the farmers to

00:30:36,020 --> 00:30:41,100
basically increase the years of the era

00:30:38,610 --> 00:30:45,120
of the fields what they're doing is

00:30:41,100 --> 00:30:48,930
basically they're apparently using fling

00:30:45,120 --> 00:30:52,140
con on AWS so they're interesting the

00:30:48,930 --> 00:30:54,660
data from from kinases processing it

00:30:52,140 --> 00:30:57,480
with fleeing and then writing it into a

00:30:54,660 --> 00:31:02,480
into a data like that is based on as

00:30:57,480 --> 00:31:06,240
three or DynamoDB another use case is

00:31:02,480 --> 00:31:11,040
stand by by here this is what they call

00:31:06,240 --> 00:31:11,680
living Maps this is basically static map

00:31:11,040 --> 00:31:15,550
data

00:31:11,680 --> 00:31:18,430
gets enriched with real-time data that

00:31:15,550 --> 00:31:21,370
is collected by cars like for instance

00:31:18,430 --> 00:31:24,790
they can figure out they basically they

00:31:21,370 --> 00:31:26,410
have like getting data from the canvas

00:31:24,790 --> 00:31:28,060
of the car so they can figure out

00:31:26,410 --> 00:31:31,480
something with a road is slippery

00:31:28,060 --> 00:31:34,390
weather like this the science changed

00:31:31,480 --> 00:31:39,730
you to a construction accidents they

00:31:34,390 --> 00:31:41,620
even measure for for for the side side

00:31:39,730 --> 00:31:43,630
sends us whether there is a parking spot

00:31:41,620 --> 00:31:45,370
and can make this information available

00:31:43,630 --> 00:31:50,500
for somebody who's looking for a parking

00:31:45,370 --> 00:31:53,650
spot so this also is is a lot of data

00:31:50,500 --> 00:31:57,190
that has been being being generated and

00:31:53,650 --> 00:32:01,180
being made available by by here so what

00:31:57,190 --> 00:32:04,360
they actually do is they have a platform

00:32:01,180 --> 00:32:05,680
with a data data data marketplace where

00:32:04,360 --> 00:32:08,080
they offer Frank as a service and you

00:32:05,680 --> 00:32:11,740
can basically write applications using

00:32:08,080 --> 00:32:15,810
their data streams to yeah

00:32:11,740 --> 00:32:18,490
get get the data that you need there's

00:32:15,810 --> 00:32:20,290
also I think this is a kind of kind of

00:32:18,490 --> 00:32:23,020
similar to the to the use case that

00:32:20,290 --> 00:32:25,630
Yakka presented it's a fleet management

00:32:23,020 --> 00:32:28,780
construction it's a company called Trek

00:32:25,630 --> 00:32:31,660
unit they're providing telematics

00:32:28,780 --> 00:32:33,570
solutions for yeah for fleet management

00:32:31,660 --> 00:32:38,580
mostly in the construction industry and

00:32:33,570 --> 00:32:42,220
so they basically track where all the

00:32:38,580 --> 00:32:46,150
all the machinery is how it is used

00:32:42,220 --> 00:32:50,020
whether it's used or not used and can

00:32:46,150 --> 00:32:53,320
basically then the customers of this of

00:32:50,020 --> 00:32:55,690
this service can basically figure out

00:32:53,320 --> 00:32:57,630
yeah where the machines are basically

00:32:55,690 --> 00:33:01,270
optimize the usage patterns and also

00:32:57,630 --> 00:33:04,360
make some inference about the

00:33:01,270 --> 00:33:06,330
maintenance intervals and finally this

00:33:04,360 --> 00:33:09,370
is well this is not another real

00:33:06,330 --> 00:33:12,370
application but it's it's on the AWS

00:33:09,370 --> 00:33:16,330
blog so I found it quite interesting use

00:33:12,370 --> 00:33:18,790
case as well this is a basically a blog

00:33:16,330 --> 00:33:22,690
post that describes how to

00:33:18,790 --> 00:33:24,809
how to do best fire detection so they

00:33:22,690 --> 00:33:27,580
kinda like assume like a multi hope

00:33:24,809 --> 00:33:29,350
network of sensors they basically talk

00:33:27,580 --> 00:33:30,850
to each other and then basically have

00:33:29,350 --> 00:33:33,370
some kind of propagation protocol that

00:33:30,850 --> 00:33:37,000
one sends us broadcasts information and

00:33:33,370 --> 00:33:40,240
then it goes on and on until to some to

00:33:37,000 --> 00:33:45,400
some station that then uploads the data

00:33:40,240 --> 00:33:47,800
to to the cloud and then they have a

00:33:45,400 --> 00:33:50,020
demo using this CP library that I was

00:33:47,800 --> 00:33:51,400
talking earlier about basically to

00:33:50,020 --> 00:33:54,520
figure out whether there is a brushfire

00:33:51,400 --> 00:33:57,070
or not and maybe making this inference

00:33:54,520 --> 00:33:59,530
obviously based on temperature in bands

00:33:57,070 --> 00:34:06,100
and then there is like a visualization

00:33:59,530 --> 00:34:08,550
with a heat map using using Cubana yeah

00:34:06,100 --> 00:34:15,669
so the demo you can also try that out on

00:34:08,550 --> 00:34:17,560
AWS alright so to conclude so like I

00:34:15,669 --> 00:34:20,679
would say if link this time without a

00:34:17,560 --> 00:34:25,720
type of meets the demands of IOT

00:34:20,679 --> 00:34:28,679
applications it's it serves the the

00:34:25,720 --> 00:34:31,540
latency and and throughput requirements

00:34:28,679 --> 00:34:34,050
quite quite well a low latency through

00:34:31,540 --> 00:34:36,639
our local state management

00:34:34,050 --> 00:34:40,419
volume by scaling out the computation

00:34:36,639 --> 00:34:42,879
it has good api's to give you like basic

00:34:40,419 --> 00:34:44,919
control for the for the right level of

00:34:42,879 --> 00:34:47,620
abstraction that you want to deal with

00:34:44,919 --> 00:34:51,190
event time helps a lot with dealing

00:34:47,620 --> 00:34:55,210
dealing with out of order data and it's

00:34:51,190 --> 00:34:58,810
been used by several IT projects and if

00:34:55,210 --> 00:35:02,790
you have something in this in this

00:34:58,810 --> 00:35:05,830
domain you should also give it a try

00:35:02,790 --> 00:35:08,350
alright so this is a briefly mentioned

00:35:05,830 --> 00:35:11,290
this this is the book that I wrote it

00:35:08,350 --> 00:35:14,200
came out I think like one and a

00:35:11,290 --> 00:35:16,930
half-year month ago I'm signing copies

00:35:14,200 --> 00:35:20,470
if you have a copy I would be in the

00:35:16,930 --> 00:35:22,000
booth area if you don't have a copy you

00:35:20,470 --> 00:35:24,730
can also get one and there it's not at

00:35:22,000 --> 00:35:28,810
the registration test but within the the

00:35:24,730 --> 00:35:36,550
booth area as well right after this talk

00:35:28,810 --> 00:35:48,490
thank you thank you guys and do you have

00:35:36,550 --> 00:35:49,810
some questions young no yeah present was

00:35:48,490 --> 00:35:54,910
really cool talk I didn't know anything

00:35:49,810 --> 00:35:57,660
about flink so we use kappa at Shopify a

00:35:54,910 --> 00:36:01,720
lot but would you say like the main like

00:35:57,660 --> 00:36:02,860
like proponents of flink / Kafka and I

00:36:01,720 --> 00:36:13,780
guess like the other way around and I

00:36:02,860 --> 00:36:17,140
like yeah yeah so so you're asking like

00:36:13,780 --> 00:36:19,240
what's the the the difference - - Kafka

00:36:17,140 --> 00:36:21,070
right so I mean Kafka is a large project

00:36:19,240 --> 00:36:22,870
which is also doing lots of things flink

00:36:21,070 --> 00:36:26,530
is not a message message queue right so

00:36:22,870 --> 00:36:31,320
it's not you you can't use it to route

00:36:26,530 --> 00:36:33,820
data Kafka and the Kafka project is the

00:36:31,320 --> 00:36:40,060
sum stream processing with African

00:36:33,820 --> 00:36:42,820
streams right so flink is I would argue

00:36:40,060 --> 00:36:45,100
think as I didn't really talk about it

00:36:42,820 --> 00:36:47,260
because it's a bit maybe no that not

00:36:45,100 --> 00:36:49,420
that important in the context of of IOT

00:36:47,260 --> 00:36:52,900
data but flick is also able to do batch

00:36:49,420 --> 00:36:57,010
processing so it's not only doing stream

00:36:52,900 --> 00:36:58,300
processing so we actually see well

00:36:57,010 --> 00:37:00,370
that's a special case of stream

00:36:58,300 --> 00:37:01,990
processing so you can also do stream

00:37:00,370 --> 00:37:04,030
processing and lots of the api's or lots

00:37:01,990 --> 00:37:07,380
of the things where the community is

00:37:04,030 --> 00:37:09,550
currently working on are going into

00:37:07,380 --> 00:37:12,550
unifying veteran stream processing and

00:37:09,550 --> 00:37:15,520
this is like I think an important thing

00:37:12,550 --> 00:37:18,640
if you look at things like backfilling

00:37:15,520 --> 00:37:20,760
of results that you want to repair

00:37:18,640 --> 00:37:23,020
because you identified it back in your

00:37:20,760 --> 00:37:26,160
somewhere in your in your code you just

00:37:23,020 --> 00:37:28,420
then run the code also the historic data

00:37:26,160 --> 00:37:31,090
using efficient batch processing

00:37:28,420 --> 00:37:35,620
techniques but also deploy for for your

00:37:31,090 --> 00:37:38,500
life life pipelines you can also do

00:37:35,620 --> 00:37:39,820
things like statement stripping which is

00:37:38,500 --> 00:37:41,260
also important so you don't want to

00:37:39,820 --> 00:37:42,010
start from zero you want to start like

00:37:41,260 --> 00:37:46,930
with history

00:37:42,010 --> 00:37:49,690
of like say six-month and bootstrap the

00:37:46,930 --> 00:37:53,380
state using using this so flink is

00:37:49,690 --> 00:37:56,710
really like a system that what it is

00:37:53,380 --> 00:37:58,810
designed or is aiming to like cover the

00:37:56,710 --> 00:38:00,490
full spectrum of of data processing

00:37:58,810 --> 00:38:04,110
whereas like Kafka streams focuses on

00:38:00,490 --> 00:38:04,110

YouTube URL: https://www.youtube.com/watch?v=JcXWcBA2dx4


