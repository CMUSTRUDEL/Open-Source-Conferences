Title: Berlin Buzzwords 2019: Simon Willnauer â€“ What's evolving in Elasticsearch and Lucene? #bbuzz
Publication date: 2019-06-27
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	This talk will walk through the latest developments in Elasticsearch and related changes in Apache Lucene. We will look into significant query performance improvements, frozen indices, usage of the new soft-deletes feature for cross data-center replication and changes APIs and usability changes that makes search-as-you-type almost trivial.

The talk also presents the results of some long standing changes that we made in Lucene which are now paying off in geo-search use-cases and how they are integrated in Elasticsearch. Folks that are interested in distributed aspects of Elasticsearch will also hear about improvements in Zen 2 and Index-lifecycle-Management.

Read more:
https://2019.berlinbuzzwords.de/19/session/whats-evolving-elasticsearch-and-lucene

About Simon Willnauer:
https://2019.berlinbuzzwords.de/users/simon-willnauer

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,090 --> 00:00:11,760
okay cool well first of all thank you

00:00:09,940 --> 00:00:14,840
for having me because I was

00:00:11,760 --> 00:00:17,070
not supposed to speak at this conference

00:00:14,840 --> 00:00:19,500
tat dining was supposed to speak here

00:00:17,070 --> 00:00:21,300
but he couldn't make it and Nina called

00:00:19,500 --> 00:00:24,180
me last week and said like hey can you

00:00:21,300 --> 00:00:27,180
can you like find some slides and they

00:00:24,180 --> 00:00:29,640
would talk and say yeah sure okay

00:00:27,180 --> 00:00:31,830
a couple words to myself I'm a I'm a

00:00:29,640 --> 00:00:34,800
founder of discomfort as well I started

00:00:31,830 --> 00:00:37,230
this with Isabel and Yan in 2010 and I'm

00:00:34,800 --> 00:00:40,860
super happy that this conference found

00:00:37,230 --> 00:00:42,629
its place and it's it's like a little

00:00:40,860 --> 00:00:44,519
family I see a lot of faces that I've

00:00:42,629 --> 00:00:46,409
seen like ten years ago and that I

00:00:44,519 --> 00:00:49,110
worked with and that I work with online

00:00:46,409 --> 00:00:52,100
and open source that's great

00:00:49,110 --> 00:00:55,140
I'm a loosing committer since I think

00:00:52,100 --> 00:00:58,620
2006 right around the time the bike

00:00:55,140 --> 00:01:00,449
started as well and I found it elastic

00:00:58,620 --> 00:01:02,760
the company together with Shia Benin and

00:01:00,449 --> 00:01:05,159
and two other people

00:01:02,760 --> 00:01:08,490
yeah and I'm I'm still working on that

00:01:05,159 --> 00:01:11,150
stuff it's still fun and let me let me

00:01:08,490 --> 00:01:11,150
show you what we're doing

00:01:11,450 --> 00:01:17,760
elasticsearch is is known for executing

00:01:15,119 --> 00:01:19,500
crews really really fast and under under

00:01:17,760 --> 00:01:24,680
the hood we use leucine quite heavily

00:01:19,500 --> 00:01:27,270
for all our queries that we execute and

00:01:24,680 --> 00:01:28,410
query times will always depend on what

00:01:27,270 --> 00:01:32,250
kind of benchmarks you're running

00:01:28,410 --> 00:01:34,890
they're pretty good and every time we do

00:01:32,250 --> 00:01:36,510
an improvement I have the impression

00:01:34,890 --> 00:01:38,400
like we can't do any better like that's

00:01:36,510 --> 00:01:42,240
it like there's no way we can improve

00:01:38,400 --> 00:01:44,310
this but lately a couple of

00:01:42,240 --> 00:01:51,330
elasticsearch engineers started to look

00:01:44,310 --> 00:01:53,940
into a paper from I think 2012 that that

00:01:51,330 --> 00:01:57,659
a person that is here from Berlin Stefan

00:01:53,940 --> 00:01:59,310
pol worked on that had a couple of

00:01:57,659 --> 00:02:01,170
really nice ideas about how we can

00:01:59,310 --> 00:02:03,510
actually pre compute the maximum score

00:02:01,170 --> 00:02:08,069
Corey can produce and with this

00:02:03,510 --> 00:02:11,909
optimization we were able to bump this

00:02:08,069 --> 00:02:15,360
up quite significantly in in that sin in

00:02:11,909 --> 00:02:19,420
elasticsearch seven and elasticsearch

00:02:15,360 --> 00:02:22,870
seven and Lucine eight

00:02:19,420 --> 00:02:24,910
yeah and this these these performance

00:02:22,870 --> 00:02:28,150
improvements are just stunning right

00:02:24,910 --> 00:02:30,430
it's like the the 1160 queries per

00:02:28,150 --> 00:02:33,250
second on a terms query that's just

00:02:30,430 --> 00:02:34,630
insane when we saw the numbers we

00:02:33,250 --> 00:02:35,200
couldn't we just literally couldn't

00:02:34,630 --> 00:02:38,890
believe it

00:02:35,200 --> 00:02:40,810
the reason why this is not like super

00:02:38,890 --> 00:02:42,520
significant of us of course also other

00:02:40,810 --> 00:02:43,959
queries not the queries on here is that

00:02:42,520 --> 00:02:45,730
we we using we don't use this

00:02:43,959 --> 00:02:47,080
optimization everywhere yet like we're

00:02:45,730 --> 00:02:49,000
still working on adding it to more

00:02:47,080 --> 00:02:51,820
queries and how we can bubble it up and

00:02:49,000 --> 00:02:57,790
down the query tree the optimization is

00:02:51,820 --> 00:02:59,920
called the magic wind it's a it's a week

00:02:57,790 --> 00:03:02,590
and optimization and what we basically

00:02:59,920 --> 00:03:04,989
doing there is and I'm trying to explain

00:03:02,590 --> 00:03:06,430
it on a higher level at the entire talk

00:03:04,989 --> 00:03:08,680
could be filled with the details so

00:03:06,430 --> 00:03:10,870
please do bear with me and there's also

00:03:08,680 --> 00:03:12,760
a lot of people here that can give you

00:03:10,870 --> 00:03:14,980
all the all the details after effect and

00:03:12,760 --> 00:03:17,260
afterwards if if you're curious so let's

00:03:14,980 --> 00:03:20,709
look at this great elastic searching

00:03:17,260 --> 00:03:24,180
we've seen and we are basically having a

00:03:20,709 --> 00:03:26,530
function to calculate the maximum score

00:03:24,180 --> 00:03:29,470
each of the query terms can contribute

00:03:26,530 --> 00:03:32,160
to the overall score and in that

00:03:29,470 --> 00:03:33,910
situation when we look at the top 10

00:03:32,160 --> 00:03:35,620
documents of a career that's what we've

00:03:33,910 --> 00:03:37,209
seen does it doesn't give you all the

00:03:35,620 --> 00:03:40,410
results it gives you the top 10 very

00:03:37,209 --> 00:03:43,060
very efficiently when we do that and

00:03:40,410 --> 00:03:45,010
collect our doctor documents we can we

00:03:43,060 --> 00:03:46,980
can say like what is what's the minimum

00:03:45,010 --> 00:03:50,760
competitive score that we have to have

00:03:46,980 --> 00:03:55,239
in order to make it to the top 10 and

00:03:50,760 --> 00:03:56,980
the further we collect documents the the

00:03:55,239 --> 00:03:59,430
higher the score needs to be in order to

00:03:56,980 --> 00:04:02,769
get there which allows us to ignore

00:03:59,430 --> 00:04:04,780
entire terms down the road with

00:04:02,769 --> 00:04:06,670
collecting documents which means

00:04:04,780 --> 00:04:09,130
essentially that's why the the single

00:04:06,670 --> 00:04:11,230
term optimization is so high like we at

00:04:09,130 --> 00:04:13,989
some point we can Susie say we skipping

00:04:11,230 --> 00:04:15,730
entire segments or entire shot and we

00:04:13,989 --> 00:04:18,970
don't even need to look at the the data

00:04:15,730 --> 00:04:20,530
under the hood all at some point where

00:04:18,970 --> 00:04:25,840
we know we can stop collecting and then

00:04:20,530 --> 00:04:27,789
pre-emptive query the weekend doesn't

00:04:25,840 --> 00:04:30,099
work all the time unfortunately it

00:04:27,789 --> 00:04:31,840
doesn't work when you use aggregations

00:04:30,099 --> 00:04:33,280
on elastic search because aggregations

00:04:31,840 --> 00:04:35,740
by definition need this

00:04:33,280 --> 00:04:37,600
every documentary matches but if you

00:04:35,740 --> 00:04:39,580
have a pure curry that that only is

00:04:37,600 --> 00:04:41,590
there for matching and not calculating

00:04:39,580 --> 00:04:43,180
that only the top ten we can't apply

00:04:41,590 --> 00:04:48,520
this optimization because we're skipping

00:04:43,180 --> 00:04:49,900
a ton of documents the other situation

00:04:48,520 --> 00:04:51,550
where we can't do this inelastic

00:04:49,900 --> 00:04:53,620
searches there's an option called track

00:04:51,550 --> 00:04:55,300
total hits if you have this little

00:04:53,620 --> 00:05:01,300
number on top of your websites and like

00:04:55,300 --> 00:05:02,650
we found 15 million 745 documents then

00:05:01,300 --> 00:05:05,380
this optimization doesn't work for you

00:05:02,650 --> 00:05:07,330
because we can give you an estimate of

00:05:05,380 --> 00:05:10,120
how many documents at least matched

00:05:07,330 --> 00:05:11,860
that's the default in in in elastic

00:05:10,120 --> 00:05:13,690
search that's used to be the default in

00:05:11,860 --> 00:05:16,419
elastic search you get the total hits

00:05:13,690 --> 00:05:18,730
but now we move to a different option

00:05:16,419 --> 00:05:23,140
here in lost or seven where we say okay

00:05:18,730 --> 00:05:24,669
we have at least 10,000 hits collected

00:05:23,140 --> 00:05:28,419
if that's something that you can work

00:05:24,669 --> 00:05:30,580
with that is done of the weekend

00:05:28,419 --> 00:05:33,780
optimization we'll kick it for you you

00:05:30,580 --> 00:05:36,040
can set this number that we at least

00:05:33,780 --> 00:05:37,750
collecting 10,000 hits or at least

00:05:36,040 --> 00:05:40,780
collecting 50,000 it's the further you

00:05:37,750 --> 00:05:45,220
go well the less of the performance

00:05:40,780 --> 00:05:48,610
boost you'll get cheer shapes to your

00:05:45,220 --> 00:05:50,740
types is a it's it's one of the totally

00:05:48,610 --> 00:05:53,890
underestimated capabilities of Racine

00:05:50,740 --> 00:05:55,419
and I've talked to a lot of a lot of geo

00:05:53,890 --> 00:05:57,910
people and from a lot of different

00:05:55,419 --> 00:06:00,220
angles I get like machine elasticsearch

00:05:57,910 --> 00:06:03,310
and all the functionality on top of it

00:06:00,220 --> 00:06:05,530
is is one of the best you can use for 40

00:06:03,310 --> 00:06:09,340
or search these days let me look at a

00:06:05,530 --> 00:06:13,300
little bit in a history we started and

00:06:09,340 --> 00:06:19,440
leucine 5 I think with adding be KD

00:06:13,300 --> 00:06:21,700
trees 6 ok 6 and elasticsearch - anyways

00:06:19,440 --> 00:06:23,830
be KD trees which is a different data

00:06:21,700 --> 00:06:27,130
structure represent numbers until then

00:06:23,830 --> 00:06:30,490
we were stuck with the prefix try that

00:06:27,130 --> 00:06:33,010
over built - to represent numbers that

00:06:30,490 --> 00:06:36,460
was working great but it wasn't as

00:06:33,010 --> 00:06:38,260
scalable as the bikini trees are in the

00:06:36,460 --> 00:06:40,419
beginning allow search try to pick it up

00:06:38,260 --> 00:06:42,640
for one dimension for numbers and dates

00:06:40,419 --> 00:06:44,890
and the further we got in last search

00:06:42,640 --> 00:06:46,330
was in 5 we use two dimensions for geo

00:06:44,890 --> 00:06:48,730
points

00:06:46,330 --> 00:06:51,790
two dimensions for dates numbers and

00:06:48,730 --> 00:06:54,970
date ranges calendars and things like

00:06:51,790 --> 00:06:57,700
that and in Alaska 36.7 which is pretty

00:06:54,970 --> 00:07:01,810
recent we started to use seven

00:06:57,700 --> 00:07:03,430
dimensions for geo shapes the what

00:07:01,810 --> 00:07:04,980
happens under the hood is we like we

00:07:03,430 --> 00:07:08,890
bullying triangles and try to represent

00:07:04,980 --> 00:07:10,600
the cheer shapes as triangles and the I

00:07:08,890 --> 00:07:13,420
got some performance numbers from some

00:07:10,600 --> 00:07:17,800
users that were able to improve their

00:07:13,420 --> 00:07:19,600
indexing pipeline by a factor of 35 the

00:07:17,800 --> 00:07:22,270
search performance is much better we can

00:07:19,600 --> 00:07:24,430
represent shapes that were that they're

00:07:22,270 --> 00:07:26,440
now indexed in a couple seconds that

00:07:24,430 --> 00:07:29,470
will not be able to represent before in

00:07:26,440 --> 00:07:30,970
memory on a 64 gig machine it's more

00:07:29,470 --> 00:07:33,010
memory efficient there's a little

00:07:30,970 --> 00:07:35,800
massive improvement it's accurate to one

00:07:33,010 --> 00:07:39,690
millimeter versus 50 meters this index

00:07:35,800 --> 00:07:44,140
is much smaller indexing is super fast

00:07:39,690 --> 00:07:45,850
queries are 50% faster plus T that the

00:07:44,140 --> 00:07:50,770
Gio points also getting a performance

00:07:45,850 --> 00:07:53,800
hit here I only have 40 minutes so I'm

00:07:50,770 --> 00:07:57,100
just moving okay yeah I hope I hope I

00:07:53,800 --> 00:08:00,760
make it to get some questions the

00:07:57,100 --> 00:08:02,320
changes API who knows the who has ever

00:08:00,760 --> 00:08:05,770
heard about the changes API on on

00:08:02,320 --> 00:08:08,530
couchdb it's it's quite an interesting

00:08:05,770 --> 00:08:10,300
thing you can just like subscribe to a

00:08:08,530 --> 00:08:12,310
stream of changes in you getting all the

00:08:10,300 --> 00:08:14,080
changes that are indexed into or or

00:08:12,310 --> 00:08:18,130
edits to the to the database and you can

00:08:14,080 --> 00:08:20,890
consume it like a like a big log file if

00:08:18,130 --> 00:08:24,040
you will and it's it's one of the

00:08:20,890 --> 00:08:26,140
changes that elasticsearch wanted to do

00:08:24,040 --> 00:08:28,560
since we founded a company and we didn't

00:08:26,140 --> 00:08:31,330
know how to do it for a very long time

00:08:28,560 --> 00:08:34,030
and after like seven years into the game

00:08:31,330 --> 00:08:37,240
we built something else called soft

00:08:34,030 --> 00:08:39,130
deletes in Lucene that we use for cross

00:08:37,240 --> 00:08:43,000
data center application that made this

00:08:39,130 --> 00:08:45,760
feature absolutely trivial to do and in

00:08:43,000 --> 00:08:47,560
contrast to what couch TV does in

00:08:45,760 --> 00:08:49,300
elasticsearch you will be able once we

00:08:47,560 --> 00:08:51,400
have to change the API release you will

00:08:49,300 --> 00:08:54,040
be able to filter the changes as well

00:08:51,400 --> 00:08:56,560
with any kinds of curry you have that is

00:08:54,040 --> 00:08:58,870
a super powerful feature that you can

00:08:56,560 --> 00:09:00,130
say like I know I want to be informed of

00:08:58,870 --> 00:09:01,900
any changes that are in sir

00:09:00,130 --> 00:09:03,790
gyah range I want to be informative

00:09:01,900 --> 00:09:06,970
certain certain changes with a certain

00:09:03,790 --> 00:09:09,400
category those kind of things full text

00:09:06,970 --> 00:09:14,650
queries against it they're going to be

00:09:09,400 --> 00:09:16,980
super powerful we try to not call it

00:09:14,650 --> 00:09:19,840
send to but I call this end to anyways

00:09:16,980 --> 00:09:22,090
sin is all we need to filter even if you

00:09:19,840 --> 00:09:23,950
change the filter yeah it's important I

00:09:22,090 --> 00:09:29,560
guess but this is unrelated to the

00:09:23,950 --> 00:09:32,410
elasticsearch change sin is our is the

00:09:29,560 --> 00:09:35,500
name of the elasticsearch distributed

00:09:32,410 --> 00:09:37,270
layer that try to figure out like who

00:09:35,500 --> 00:09:40,060
know which note belongs to the cluster

00:09:37,270 --> 00:09:43,000
end of force and we have a long history

00:09:40,060 --> 00:09:45,790
of edge cases and problems and split

00:09:43,000 --> 00:09:47,680
brains and we reduced the number of

00:09:45,790 --> 00:09:49,240
spins dramatically but there were a

00:09:47,680 --> 00:09:51,220
couple of scenarios that were just not

00:09:49,240 --> 00:09:54,040
fixable with with the model that we had

00:09:51,220 --> 00:09:56,830
so we spent about two years investing

00:09:54,040 --> 00:09:59,110
into a new distribution layer that is

00:09:56,830 --> 00:10:02,560
partially based on on research that is

00:09:59,110 --> 00:10:06,190
complete formally proven and as very

00:10:02,560 --> 00:10:07,870
very test well tested and it removes all

00:10:06,190 --> 00:10:12,040
the pain points that we had before

00:10:07,870 --> 00:10:14,230
especially with configuration if you've

00:10:12,040 --> 00:10:17,170
used the elastic search you hopefully

00:10:14,230 --> 00:10:19,540
have set minimum master notes to a

00:10:17,170 --> 00:10:22,000
quorum level or to a quorum number of

00:10:19,540 --> 00:10:24,040
the number of nodes in the cluster well

00:10:22,000 --> 00:10:26,350
this is important because if you lose

00:10:24,040 --> 00:10:28,480
one of the nodes in in this scenario

00:10:26,350 --> 00:10:29,830
will you'll or if you use a majority of

00:10:28,480 --> 00:10:32,530
the nodes in this scenario didn't they

00:10:29,830 --> 00:10:34,750
not be able to form a cluster that will

00:10:32,530 --> 00:10:37,900
prevent your split-brain the problem

00:10:34,750 --> 00:10:39,190
with this is if you add a new node like

00:10:37,900 --> 00:10:41,290
you have this moment where you have to

00:10:39,190 --> 00:10:43,450
change the value to be correct and

00:10:41,290 --> 00:10:46,540
that's that is actually the moment when

00:10:43,450 --> 00:10:49,480
you get loose data and what happens in

00:10:46,540 --> 00:10:51,220
practice is people don't set it people

00:10:49,480 --> 00:10:54,640
don't set it when you leave when notes

00:10:51,220 --> 00:10:58,090
leave either so you very likely have a

00:10:54,640 --> 00:11:01,360
misconfigured cluster at some point and

00:10:58,090 --> 00:11:04,000
you have problems if there's only one

00:11:01,360 --> 00:11:06,340
note or one node leaving there is so

00:11:04,000 --> 00:11:07,780
many situations here that the best

00:11:06,340 --> 00:11:09,730
option for us is just remove this

00:11:07,780 --> 00:11:11,950
setting altogether and replace it with

00:11:09,730 --> 00:11:13,810
sent to where you have to set it only

00:11:11,950 --> 00:11:15,970
once when you boot step you plus

00:11:13,810 --> 00:11:17,529
initially you would start your cluster

00:11:15,970 --> 00:11:18,760
and you say okay I want to start a new

00:11:17,529 --> 00:11:22,180
cluster with six nodes

00:11:18,760 --> 00:11:24,700
I set the initial master nodes to four

00:11:22,180 --> 00:11:26,260
and from then on you can scale up and

00:11:24,700 --> 00:11:28,390
down and shut down a cluster and we

00:11:26,260 --> 00:11:33,010
started in elasticsearch we'll take her

00:11:28,390 --> 00:11:35,080
off the right well used for you that is

00:11:33,010 --> 00:11:37,630
probably going to remove ninety percent

00:11:35,080 --> 00:11:38,800
of the of the split pre-order or the

00:11:37,630 --> 00:11:41,880
data loss situations we had

00:11:38,800 --> 00:11:41,880
elasticsearch altogether

00:11:42,180 --> 00:11:47,500
adaptive replica selection is a feature

00:11:44,890 --> 00:11:49,750
that I think we started to work on in

00:11:47,500 --> 00:11:52,750
last search five and we had some parts

00:11:49,750 --> 00:11:55,330
of it already committed in in the five

00:11:52,750 --> 00:11:57,490
series and it's it was already done in

00:11:55,330 --> 00:11:59,560
six but not enabled in seven it's the

00:11:57,490 --> 00:12:03,279
default what is this

00:11:59,560 --> 00:12:05,020
so in elasticsearch replicas I think I

00:12:03,279 --> 00:12:07,750
have a pointer here like replicas as

00:12:05,020 --> 00:12:09,430
well as primaries conserved requests if

00:12:07,750 --> 00:12:11,440
the coordinating err gets a request it

00:12:09,430 --> 00:12:13,480
picks one of the replicas either the

00:12:11,440 --> 00:12:18,490
primary or this replica and fires up the

00:12:13,480 --> 00:12:21,100
request on this back channel here we are

00:12:18,490 --> 00:12:22,930
able to collect information how long's

00:12:21,100 --> 00:12:26,410
the queue how long did it take to a serf

00:12:22,930 --> 00:12:28,480
the request did we actually have we have

00:12:26,410 --> 00:12:30,940
we actually been queued on the node etc

00:12:28,480 --> 00:12:33,670
and that gives us a very good indication

00:12:30,940 --> 00:12:37,600
if the node is on the load or not and in

00:12:33,670 --> 00:12:39,970
some situations you might get slow

00:12:37,600 --> 00:12:43,180
responses and then you actually want to

00:12:39,970 --> 00:12:44,770
give this node here a pause you don't

00:12:43,180 --> 00:12:47,430
want to overload that and that's that's

00:12:44,770 --> 00:12:53,200
to reduce your 95th and 99th percentile

00:12:47,430 --> 00:12:54,580
latency the adaptive replicas selection

00:12:53,200 --> 00:12:55,960
is default in the last through seven if

00:12:54,580 --> 00:12:57,130
you want to try give it a try and if you

00:12:55,960 --> 00:12:59,020
search use casing you sometimes

00:12:57,130 --> 00:13:03,459
suffering from spikes you can enable it

00:12:59,020 --> 00:13:05,020
in last year it's six as well in the

00:13:03,459 --> 00:13:07,209
history of elastic search we had

00:13:05,020 --> 00:13:11,890
something called the tribe note the

00:13:07,209 --> 00:13:13,480
tribe note had tons of problems I don't

00:13:11,890 --> 00:13:16,000
think anybody has ever used it without

00:13:13,480 --> 00:13:17,650
having a problem that's something that

00:13:16,000 --> 00:13:19,000
we realized pretty late then we wanted

00:13:17,650 --> 00:13:21,430
to replace it with something that is

00:13:19,000 --> 00:13:24,040
much more stable but at the same time

00:13:21,430 --> 00:13:25,510
it's also much less powerful we replaced

00:13:24,040 --> 00:13:27,400
it with something called cross cluster

00:13:25,510 --> 00:13:29,800
search which allows you to have

00:13:27,400 --> 00:13:32,530
clusters at different locations these

00:13:29,800 --> 00:13:36,070
locations are the cities now like

00:13:32,530 --> 00:13:37,810
searching across data centers is has its

00:13:36,070 --> 00:13:40,000
own details and its own Devils

00:13:37,810 --> 00:13:41,440
so you but if you have multiple clusters

00:13:40,000 --> 00:13:43,120
in a single data center it's only

00:13:41,440 --> 00:13:47,620
something that that works very very

00:13:43,120 --> 00:13:51,130
performant Qabbani can now go and talk

00:13:47,620 --> 00:13:53,050
to the cluster in London and say okay I

00:13:51,130 --> 00:13:54,640
want to search some indices in New York

00:13:53,050 --> 00:13:56,530
and some indices in Tokyo and then get

00:13:54,640 --> 00:13:58,990
it back as a result as if it's from a

00:13:56,530 --> 00:14:00,820
single cluster the really cool part

00:13:58,990 --> 00:14:03,190
about this is that we have our

00:14:00,820 --> 00:14:06,610
compatibility with from the current

00:14:03,190 --> 00:14:07,120
major to the previous majors latest

00:14:06,610 --> 00:14:09,730
minor

00:14:07,120 --> 00:14:12,100
that's what we guarantee so from 7 X

00:14:09,730 --> 00:14:15,460
we'll always be able to search 26.7 and

00:14:12,100 --> 00:14:18,550
from 6 dots X we will always be able to

00:14:15,460 --> 00:14:20,680
work to search to seek 5.6 so you are

00:14:18,550 --> 00:14:23,260
able to have three major versions

00:14:20,680 --> 00:14:25,540
different clusters without doing any

00:14:23,260 --> 00:14:27,430
upgrades we're able to search your data

00:14:25,540 --> 00:14:29,440
can h out if you have retention policies

00:14:27,430 --> 00:14:34,930
you can HR the entire cluster your

00:14:29,440 --> 00:14:38,680
upgrade process might be very slick CCR

00:14:34,930 --> 00:14:41,020
is all the writing part of CCS cross

00:14:38,680 --> 00:14:45,400
coastal search it's a it's a feature

00:14:41,020 --> 00:14:47,100
that is it's not free it's a it's a paid

00:14:45,400 --> 00:14:50,590
as a premium feature in elastic search

00:14:47,100 --> 00:14:52,270
and it allows you to basically have a

00:14:50,590 --> 00:14:54,430
leader index and a follower index in

00:14:52,270 --> 00:14:56,380
different cluster these clusters can

00:14:54,430 --> 00:15:00,490
also be in different data centers so you

00:14:56,380 --> 00:15:04,090
can have like a the London sales index

00:15:00,490 --> 00:15:08,080
be replicated in and across data centers

00:15:04,090 --> 00:15:09,610
into New York as well as Tokyo or you

00:15:08,080 --> 00:15:11,410
can have like a central location where

00:15:09,610 --> 00:15:13,600
you have a copy of the neutral data

00:15:11,410 --> 00:15:17,110
centers it works both directions it's

00:15:13,600 --> 00:15:20,560
per index you can do this all together

00:15:17,110 --> 00:15:23,200
and it's really relatively easy also to

00:15:20,560 --> 00:15:24,880
configure you can have auto follow

00:15:23,200 --> 00:15:26,320
follow patterns if you have like data

00:15:24,880 --> 00:15:28,300
indices and you're rolling over new

00:15:26,320 --> 00:15:29,620
images every day the the replication

00:15:28,300 --> 00:15:35,130
will kick in automatically if you

00:15:29,620 --> 00:15:35,130
specify a an auto following pattern

00:15:35,709 --> 00:15:42,529
frozen indices it's it's part of our

00:15:38,989 --> 00:15:44,629
making things slow effort which is like

00:15:42,529 --> 00:15:46,039
it makes me sad like 15 years of my life

00:15:44,629 --> 00:15:47,799
I try to make things fast and now

00:15:46,039 --> 00:15:50,719
everybody's asking for making them slow

00:15:47,799 --> 00:15:52,309
why do you want to do that anyway it's

00:15:50,719 --> 00:15:56,599
it's another area where you can innovate

00:15:52,309 --> 00:15:59,689
and frozen any ceases is is a solution

00:15:56,599 --> 00:16:02,029
where we try to actually utilize our

00:15:59,689 --> 00:16:04,249
resources much much better like if you

00:16:02,029 --> 00:16:07,039
have elasticsearch allocated allocating

00:16:04,249 --> 00:16:09,559
charlotte's on a note all you only you

00:16:07,039 --> 00:16:10,999
have a certain amount of disk space left

00:16:09,559 --> 00:16:13,429
here right not a really good utilization

00:16:10,999 --> 00:16:15,709
down here in the disk space but under

00:16:13,429 --> 00:16:17,749
heap you already filled up because the

00:16:15,709 --> 00:16:19,279
the the each index needs a certain

00:16:17,749 --> 00:16:21,109
amount of memory in this clip for

00:16:19,279 --> 00:16:24,439
mappings and segment memory and stuff

00:16:21,109 --> 00:16:25,939
like this with frozen indices you might

00:16:24,439 --> 00:16:29,089
be able to fill your disk up completely

00:16:25,939 --> 00:16:30,679
and the frozen indices are only searched

00:16:29,089 --> 00:16:32,359
in a serial manner they go through a

00:16:30,679 --> 00:16:33,979
like a what we call a throttle thread

00:16:32,359 --> 00:16:36,709
pool which only has one thread so we

00:16:33,979 --> 00:16:39,139
execute one after another and we lazily

00:16:36,709 --> 00:16:42,619
loading them into memory to not occupy

00:16:39,139 --> 00:16:44,209
too much memory that's slower in some

00:16:42,619 --> 00:16:48,289
situations if file system cache is

00:16:44,209 --> 00:16:52,189
actually hot it's like only a couple

00:16:48,289 --> 00:16:55,599
like 10 15 20 percent of a difference

00:16:52,189 --> 00:16:57,819
that it makes in insert execution speed

00:16:55,599 --> 00:17:00,949
while it's recommended to have these

00:16:57,819 --> 00:17:03,409
indices all being optimized for Smerch

00:17:00,949 --> 00:17:04,789
having a single segment because if they

00:17:03,409 --> 00:17:06,230
don't have a single segment and we need

00:17:04,789 --> 00:17:07,850
to build additional accelerate data

00:17:06,230 --> 00:17:11,319
structures for like global ordinals to

00:17:07,850 --> 00:17:11,319
do aggregations which is very costly

00:17:11,799 --> 00:17:17,269
that ties very much into it that's

00:17:14,329 --> 00:17:18,740
something that I most research use cases

00:17:17,269 --> 00:17:21,679
don't really need but if you're like

00:17:18,740 --> 00:17:23,299
analog in matrix security analytics use

00:17:21,679 --> 00:17:25,159
cases the next life cycle management is

00:17:23,299 --> 00:17:27,049
something that that you either do

00:17:25,159 --> 00:17:29,869
yourself or you always wish that ostrich

00:17:27,049 --> 00:17:32,509
would do for you in this example you

00:17:29,869 --> 00:17:35,419
have a hot warm and cold node

00:17:32,509 --> 00:17:37,850
architecture these nodes might have

00:17:35,419 --> 00:17:39,919
different discs and different different

00:17:37,850 --> 00:17:41,299
GPUs and things like that and your

00:17:39,919 --> 00:17:44,929
indexing into your hot no it's full

00:17:41,299 --> 00:17:48,200
speed and after every night it at

00:17:44,929 --> 00:17:51,740
midnight you go and roll over

00:17:48,200 --> 00:17:53,779
index and then move some of the some of

00:17:51,740 --> 00:17:55,700
the charts to to the warmth here and

00:17:53,779 --> 00:17:59,029
then go and shrink them together in one

00:17:55,700 --> 00:18:00,679
chart that that's the inverse of a split

00:17:59,029 --> 00:18:02,779
function we can split and shrink in

00:18:00,679 --> 00:18:04,429
elasticsearch the shrink function is

00:18:02,779 --> 00:18:06,440
actually very fast because we're using

00:18:04,429 --> 00:18:08,179
hard links under the hoods it's uh most

00:18:06,440 --> 00:18:10,010
of the time if you allocate them all on

00:18:08,179 --> 00:18:14,299
the same hard disk it's a it's an

00:18:10,010 --> 00:18:15,590
instant operation super quick if you

00:18:14,299 --> 00:18:17,240
force merge time you're getting smaller

00:18:15,590 --> 00:18:19,700
and at some point you want to have moved

00:18:17,240 --> 00:18:22,070
it to the cold tier and then freeze it

00:18:19,700 --> 00:18:24,950
and that's the what's in X lifecycle

00:18:22,070 --> 00:18:26,120
management does for you on some point

00:18:24,950 --> 00:18:27,830
you want to delete it I hope you don't

00:18:26,120 --> 00:18:32,330
forget about that the leading is the

00:18:27,830 --> 00:18:33,799
best way to save space if you use lock

00:18:32,330 --> 00:18:36,470
stash or beats or anything like that

00:18:33,799 --> 00:18:37,970
they install their their own policies

00:18:36,470 --> 00:18:39,860
and there's a next life cycle management

00:18:37,970 --> 00:18:44,059
you eyes in cabana it allow you to do

00:18:39,860 --> 00:18:45,139
that that's all free yeah nanosecond

00:18:44,059 --> 00:18:46,700
time steps you would think of

00:18:45,139 --> 00:18:49,519
milliseconds are enough right but

00:18:46,700 --> 00:18:52,279
there's a ton of things that can happen

00:18:49,519 --> 00:18:54,470
in a millisecond right so if you look at

00:18:52,279 --> 00:18:56,090
this granularity with date nanos you

00:18:54,470 --> 00:18:57,950
will be able to also have a melissa

00:18:56,090 --> 00:19:01,220
nanosecond granularity and elasticsearch

00:18:57,950 --> 00:19:03,860
in you will be able searched with this

00:19:01,220 --> 00:19:05,630
in similar schemas if you have date

00:19:03,860 --> 00:19:07,909
nanos or date melees it doesn't matter

00:19:05,630 --> 00:19:13,610
you'll be able to aggregate on these on

00:19:07,909 --> 00:19:15,289
these fields full-text searches used to

00:19:13,610 --> 00:19:18,080
be bread and butter and then we also

00:19:15,289 --> 00:19:20,870
doing a lot of other things but we we

00:19:18,080 --> 00:19:23,149
wanted to make things simpler think of a

00:19:20,870 --> 00:19:26,510
like a searches to type use case how

00:19:23,149 --> 00:19:29,269
hard it was in the past to build that

00:19:26,510 --> 00:19:31,760
and how much energy we put in through

00:19:29,269 --> 00:19:33,200
leucine and like build ffs theses

00:19:31,760 --> 00:19:34,970
jesters and we'll put them into memory

00:19:33,200 --> 00:19:36,860
they're super heavy to build they're

00:19:34,970 --> 00:19:40,159
super heavy too heavy to have a memory

00:19:36,860 --> 00:19:41,750
either blazing fast but our term

00:19:40,159 --> 00:19:44,690
dictionary is doing a really good job as

00:19:41,750 --> 00:19:47,929
well if you do it right in elasticsearch

00:19:44,690 --> 00:19:49,549
now you have these index prefixes as an

00:19:47,929 --> 00:19:52,580
option on the field and that will allow

00:19:49,549 --> 00:19:57,289
you if you index like quick it'll also

00:19:52,580 --> 00:19:59,029
put qi qi qi c and so forth the prefix

00:19:57,289 --> 00:20:00,980
is up to like four characters into your

00:19:59,029 --> 00:20:02,000
index to have really really fast

00:20:00,980 --> 00:20:04,340
prefix sir

00:20:02,000 --> 00:20:06,620
out of the box the nice thing is if you

00:20:04,340 --> 00:20:08,899
do that out of the box it compresses

00:20:06,620 --> 00:20:10,399
really well like our turn the trees are

00:20:08,899 --> 00:20:12,860
really made for these kind of things

00:20:10,399 --> 00:20:15,080
we call these edge engrams and by

00:20:12,860 --> 00:20:19,100
default now we would with a single

00:20:15,080 --> 00:20:20,659
second you can do that but for a search

00:20:19,100 --> 00:20:22,250
as you type you also need the phrases

00:20:20,659 --> 00:20:24,200
once you finish typing the first word

00:20:22,250 --> 00:20:26,330
right you need the phrases the same

00:20:24,200 --> 00:20:28,309
option exists for the phrases and we

00:20:26,330 --> 00:20:29,779
also doing engrams here but we that's

00:20:28,309 --> 00:20:33,559
what we in the scene and last we call

00:20:29,779 --> 00:20:35,809
them shingles or token-based engrams to

00:20:33,559 --> 00:20:37,639
have really really fast phrase search

00:20:35,809 --> 00:20:42,649
which will improve your phrase search is

00:20:37,639 --> 00:20:44,570
quite dramatically if you do that I can

00:20:42,649 --> 00:20:46,730
in this case you can you can just type

00:20:44,570 --> 00:20:48,590
these in in the match phrase prefix or

00:20:46,730 --> 00:20:50,570
in a match phrase query and these

00:20:48,590 --> 00:20:52,279
queries will actually execute very quick

00:20:50,570 --> 00:20:54,320
they look at the mappings it's like okay

00:20:52,279 --> 00:20:57,320
I have prefixes here or I can just go

00:20:54,320 --> 00:20:59,779
and and use the prefix diagrams and to

00:20:57,320 --> 00:21:02,259
to do the search and I don't have to do

00:20:59,779 --> 00:21:04,220
a wild-card search which is basically

00:21:02,259 --> 00:21:07,750
positioning in the term dictionary and

00:21:04,220 --> 00:21:07,750
then scanning until your front terms

00:21:08,049 --> 00:21:14,870
interval queries so it's we we used to

00:21:12,950 --> 00:21:16,610
have span queries or we still have

00:21:14,870 --> 00:21:18,379
spankers in the last version you seen

00:21:16,610 --> 00:21:19,879
but one of the biggest downsides with

00:21:18,379 --> 00:21:23,019
with them was that there

00:21:19,879 --> 00:21:25,669
the scoring was quite difficult the

00:21:23,019 --> 00:21:28,220
extending them was hard like using them

00:21:25,669 --> 00:21:30,860
with other queries impossible

00:21:28,220 --> 00:21:32,720
they only taken analyzed text which is

00:21:30,860 --> 00:21:34,639
also difficult because you're the

00:21:32,720 --> 00:21:36,110
analyzers are defined in in elastic

00:21:34,639 --> 00:21:39,409
surgeon you have to like somehow have

00:21:36,110 --> 00:21:41,779
them in two places and it's very close

00:21:39,409 --> 00:21:43,610
to my heart because it's one it's a 10

00:21:41,779 --> 00:21:45,799
year old issue I started just like 10

00:21:43,610 --> 00:21:51,110
years ago and Ellen Thank You Ellen for

00:21:45,799 --> 00:21:52,850
finishing it up after after standing in

00:21:51,110 --> 00:21:54,110
the shoulder giants like there had a lot

00:21:52,850 --> 00:21:55,820
of other things had to that's sometimes

00:21:54,110 --> 00:21:57,799
it's a very funny anecdote of open

00:21:55,820 --> 00:22:00,350
source like you have a good idea and you

00:21:57,799 --> 00:22:01,850
started and at the point at a time it

00:22:00,350 --> 00:22:04,519
just doesn't work

00:22:01,850 --> 00:22:06,830
I just dream just missing api's I'm

00:22:04,519 --> 00:22:08,840
missing other features and then 5 6 10

00:22:06,830 --> 00:22:11,840
years later you have all these pieces

00:22:08,840 --> 00:22:14,360
like now now we could actually do it in

00:22:11,840 --> 00:22:16,380
a nice way and and that's that's one of

00:22:14,360 --> 00:22:18,030
these things it's you seen 2870

00:22:16,380 --> 00:22:19,890
eight it's one of these issues that I

00:22:18,030 --> 00:22:22,470
know by hard and I probably will never

00:22:19,890 --> 00:22:25,470
forget it okay so what can we do with

00:22:22,470 --> 00:22:27,420
this if we have text like this we could

00:22:25,470 --> 00:22:30,240
we can search for like quake in your fox

00:22:27,420 --> 00:22:31,860
and then the whole thing needs to be

00:22:30,240 --> 00:22:34,980
near lazy and thought but they need to

00:22:31,860 --> 00:22:37,770
be at Jetsons right next together within

00:22:34,980 --> 00:22:39,360
like five characters where the spans

00:22:37,770 --> 00:22:40,770
this was quite difficult because you had

00:22:39,360 --> 00:22:42,090
to put like the analyze text in there

00:22:40,770 --> 00:22:46,620
now we can just put the text in there

00:22:42,090 --> 00:22:50,190
and do it and yeah that that's that's

00:22:46,620 --> 00:22:54,510
pretty much it and I think I have about

00:22:50,190 --> 00:22:55,350
10 minutes left something like 17

00:22:54,510 --> 00:22:59,250
minutes left

00:22:55,350 --> 00:23:00,780
oh that's awesome I I can I can talk

00:22:59,250 --> 00:23:04,980
about a lot of other things too

00:23:00,780 --> 00:23:11,340
do we have questions do we have

00:23:04,980 --> 00:23:14,000
questions please all kinds of elastic

00:23:11,340 --> 00:23:17,220
searching to seeing questions

00:23:14,000 --> 00:23:20,520
hello yeah so this is a question about

00:23:17,220 --> 00:23:22,160
the cross cluster replication yeah I'm

00:23:20,520 --> 00:23:25,320
just wondering under the hood is that

00:23:22,160 --> 00:23:27,900
document replication I hope not yes it

00:23:25,320 --> 00:23:30,950
is okay yes yes it's document

00:23:27,900 --> 00:23:34,290
replication so in elasticsearch we have

00:23:30,950 --> 00:23:36,540
added something in Luster's 5 & 6 called

00:23:34,290 --> 00:23:39,450
sequence IDs or every change has an ID

00:23:36,540 --> 00:23:42,420
assigned to it it's broken into two

00:23:39,450 --> 00:23:44,790
pieces it's a primary term the primary

00:23:42,420 --> 00:23:46,980
term comes from the current active

00:23:44,790 --> 00:23:52,020
primary of a shard and the sequence ID

00:23:46,980 --> 00:23:54,000
is a unique identifier per chart and

00:23:52,020 --> 00:23:57,780
these two numbers together allow you to

00:23:54,000 --> 00:23:59,760
have a total ordering of we went and we

00:23:57,780 --> 00:24:01,380
we guarantee you that this total

00:23:59,760 --> 00:24:03,510
ordering of vince doesn't have even

00:24:01,380 --> 00:24:05,250
doesn't have holes so we guarantee you

00:24:03,510 --> 00:24:07,980
go elasticity will always maintain a

00:24:05,250 --> 00:24:11,040
full history if a document has been

00:24:07,980 --> 00:24:12,300
acknowledged to the user and cross data

00:24:11,040 --> 00:24:15,810
center replication

00:24:12,300 --> 00:24:19,290
makes use of this of this property where

00:24:15,810 --> 00:24:21,540
we can say and a follower index we can

00:24:19,290 --> 00:24:25,260
say I have a full history up to sequence

00:24:21,540 --> 00:24:28,430
ID 15 now give me everything that you

00:24:25,260 --> 00:24:30,180
have that's in between your latest

00:24:28,430 --> 00:24:31,830
highest consecutive

00:24:30,180 --> 00:24:34,100
sequence idea that's what we call the

00:24:31,830 --> 00:24:36,390
global versus the local checkpoint and

00:24:34,100 --> 00:24:38,430
then we can go and catch up with the

00:24:36,390 --> 00:24:40,260
history that's not just used for cross

00:24:38,430 --> 00:24:42,780
data's on a replication we also used

00:24:40,260 --> 00:24:44,460
this for catching up with a replica so a

00:24:42,780 --> 00:24:46,350
rapid if you shut down and out and start

00:24:44,460 --> 00:24:48,630
it up again and you've indexed something

00:24:46,350 --> 00:24:50,190
in in in between like you lost like five

00:24:48,630 --> 00:24:52,650
minutes of your history the replicas

00:24:50,190 --> 00:24:54,690
will go to the primaries like hey I'm

00:24:52,650 --> 00:24:58,410
out of sync give me give me everything

00:24:54,690 --> 00:25:01,490
since sequence ID 150 25 and your latest

00:24:58,410 --> 00:25:03,840
checkpoint and then we will start

00:25:01,490 --> 00:25:05,400
indexing new documents into it so we

00:25:03,840 --> 00:25:07,830
could completely complete out of order

00:25:05,400 --> 00:25:11,010
delivery here and we will try to catch

00:25:07,830 --> 00:25:13,170
up with the primary now there's some

00:25:11,010 --> 00:25:15,570
trickiness to it and I said this before

00:25:13,170 --> 00:25:18,600
we have a feature in the scene it's

00:25:15,570 --> 00:25:21,300
called soft deletes soft deletes are

00:25:18,600 --> 00:25:23,190
very similar to delete sin in Lucene we

00:25:21,300 --> 00:25:26,880
don't really delete a document which

00:25:23,190 --> 00:25:30,570
market has deleted and when we merge we

00:25:26,880 --> 00:25:32,190
prune them we drop them for across data

00:25:30,570 --> 00:25:34,170
center replication and also for the

00:25:32,190 --> 00:25:36,930
changes API we needed to maintain

00:25:34,170 --> 00:25:38,880
history we couldn't just like drop them

00:25:36,930 --> 00:25:41,670
on the floor and remove them we need to

00:25:38,880 --> 00:25:44,070
hold on to them across mergers and the

00:25:41,670 --> 00:25:47,280
soft deletes allow you to do that allow

00:25:44,070 --> 00:25:49,590
you to basically specify a curry that

00:25:47,280 --> 00:25:51,030
tells the scene which documents it can

00:25:49,590 --> 00:25:52,890
which of the deleted occupancy can

00:25:51,030 --> 00:25:56,850
actually prune under under merge in

00:25:52,890 --> 00:25:59,190
which it has to keep and this allows us

00:25:56,850 --> 00:26:01,320
to maintain what we call our retention

00:25:59,190 --> 00:26:03,780
lease on each of the shards and if I'm a

00:26:01,320 --> 00:26:05,550
replica and and I'm talking to my

00:26:03,780 --> 00:26:06,990
primary I tell like okay I have a

00:26:05,550 --> 00:26:10,620
potentially for five minutes and my

00:26:06,990 --> 00:26:12,570
minimum sequence ID is 15 that means for

00:26:10,620 --> 00:26:14,100
the next five minutes that chart the

00:26:12,570 --> 00:26:16,200
primary will definitely guarantee you

00:26:14,100 --> 00:26:21,060
that it has the full history between 15

00:26:16,200 --> 00:26:26,210
and its global checkpoint Mike this is

00:26:21,060 --> 00:26:26,210
good okay more questions

00:26:32,680 --> 00:26:40,840
I is there also involvement in adjusting

00:26:36,490 --> 00:26:44,220
the highlighter I'm quite struggling

00:26:40,840 --> 00:26:48,730
with unified and all the other

00:26:44,220 --> 00:26:51,970
highlighter types list are any I don't

00:26:48,730 --> 00:26:55,350
think we have any massive new

00:26:51,970 --> 00:26:57,490
developments along those lines Ellen

00:26:55,350 --> 00:27:06,610
maybe Ellen can answer this question

00:26:57,490 --> 00:27:08,410
better oh yes so we the medicine changes

00:27:06,610 --> 00:27:11,410
in Toulouse scene itself there's a new

00:27:08,410 --> 00:27:13,930
matches API which will now allow queries

00:27:11,410 --> 00:27:16,180
to return exact positions of where

00:27:13,930 --> 00:27:17,530
they've matched something the unified

00:27:16,180 --> 00:27:21,490
highlighter is beginning to make use of

00:27:17,530 --> 00:27:24,580
that but we also have Jim Frenchy it

00:27:21,490 --> 00:27:25,510
works realistic is also working on a new

00:27:24,580 --> 00:27:27,880
highlighting which will be entirely

00:27:25,510 --> 00:27:29,590
based on that and will basically remove

00:27:27,880 --> 00:27:30,970
all of the bells and whistles from

00:27:29,590 --> 00:27:33,340
everything and just have it now this is

00:27:30,970 --> 00:27:35,410
a very plain highlighter use this and

00:27:33,340 --> 00:27:38,170
they will tell you what what matched and

00:27:35,410 --> 00:27:41,890
both tribes of the cut down on a lot of

00:27:38,170 --> 00:27:43,690
the they're very complicated settings

00:27:41,890 --> 00:27:45,250
that the current highlighters have the

00:27:43,690 --> 00:27:47,440
idea is eventually to hopefully just had

00:27:45,250 --> 00:27:49,830
that single highlighter and remove all

00:27:47,440 --> 00:27:49,830
the other ones

00:27:53,750 --> 00:28:00,120
yes okay two things I think you didn't

00:27:58,590 --> 00:28:01,680
mention another amazing thing that

00:28:00,120 --> 00:28:03,900
showed up in leucine and elasticsearch

00:28:01,680 --> 00:28:06,540
in the past year or maybe my timings off

00:28:03,900 --> 00:28:08,460
ranges so you can now arrange the

00:28:06,540 --> 00:28:10,680
first-class field you can put onto an

00:28:08,460 --> 00:28:14,190
element has a start and an end I think I

00:28:10,680 --> 00:28:25,770
mentioned it briefly in the when we used

00:28:14,190 --> 00:28:29,580
how we used the kitty trees that date

00:28:25,770 --> 00:28:31,230
ranges is here and 5.3 yeah that that's

00:28:29,580 --> 00:28:32,730
that is true like you have like

00:28:31,230 --> 00:28:34,920
searching data ranges of first class

00:28:32,730 --> 00:28:37,410
citizen in leucine or and in

00:28:34,920 --> 00:28:41,310
elasticsearch yeah good this is by the

00:28:37,410 --> 00:28:43,440
way a lot of these changes originated

00:28:41,310 --> 00:28:46,560
from developments in elastics which were

00:28:43,440 --> 00:28:49,260
we try to always add these changes where

00:28:46,560 --> 00:28:51,420
they belong like there is no friction

00:28:49,260 --> 00:28:52,920
between elasticsearch leucine and and

00:28:51,420 --> 00:28:54,660
other people using your scene because

00:28:52,920 --> 00:28:55,980
like we strongly believe that these

00:28:54,660 --> 00:28:59,700
changes should be made on the right

00:28:55,980 --> 00:29:02,310
level and this is one of the perfect

00:28:59,700 --> 00:29:04,620
examples right this is this whole be KD

00:29:02,310 --> 00:29:07,320
trees you today we think about is like i

00:29:04,620 --> 00:29:10,890
we've added it there but it surely

00:29:07,320 --> 00:29:12,690
started in 2013 hiring two people to do

00:29:10,890 --> 00:29:14,370
that and then like working on the

00:29:12,690 --> 00:29:16,590
structure for two and a half years and

00:29:14,370 --> 00:29:19,350
then another two and a half years to

00:29:16,590 --> 00:29:22,320
actually get here right there is so

00:29:19,350 --> 00:29:24,300
there's a lot of work and a lot of also

00:29:22,320 --> 00:29:26,010
cross community like back then when we

00:29:24,300 --> 00:29:28,500
started Mikey were working for elastic

00:29:26,010 --> 00:29:30,000
and then no but even do you depart it to

00:29:28,500 --> 00:29:31,380
your next adventure and we kept on

00:29:30,000 --> 00:29:33,660
working on it and that is that's a

00:29:31,380 --> 00:29:35,640
that's that's something that I'm I'm

00:29:33,660 --> 00:29:38,430
super proud of that's a very cool

00:29:35,640 --> 00:29:41,160
feature of this community so then an

00:29:38,430 --> 00:29:42,830
unrelated follow-up question a year from

00:29:41,160 --> 00:29:44,880
now when you're giving this talk again

00:29:42,830 --> 00:29:50,220
what are the things you're gonna tell us

00:29:44,880 --> 00:29:52,970
that happened in the following year what

00:29:50,220 --> 00:29:52,970
can I say

00:29:52,980 --> 00:29:58,260
let's let's take a step back and I'm

00:29:55,620 --> 00:30:02,580
hoping I'm hoping I'm not like Nick's

00:29:58,260 --> 00:30:04,140
gonna kill me for that but like when you

00:30:02,580 --> 00:30:06,060
look at it what could you do with vkd

00:30:04,140 --> 00:30:06,730
trees you could for instance look into

00:30:06,060 --> 00:30:08,710
and that's why

00:30:06,730 --> 00:30:11,710
don't take it as a promise right it

00:30:08,710 --> 00:30:13,450
might be years out look at the data

00:30:11,710 --> 00:30:15,820
structures that are that can represent

00:30:13,450 --> 00:30:19,179
graphs like neighbor trees for instance

00:30:15,820 --> 00:30:22,210
and have a very very efficient graph

00:30:19,179 --> 00:30:23,919
search like brass search that is so

00:30:22,210 --> 00:30:27,280
blazing fast that you haven't seen it

00:30:23,919 --> 00:30:30,549
before that that's something that I can

00:30:27,280 --> 00:30:32,950
totally imagine being at a base of these

00:30:30,549 --> 00:30:37,210
data structures but there's there's a

00:30:32,950 --> 00:30:38,950
lot of ideas and we also what we

00:30:37,210 --> 00:30:41,080
constantly asking ourselves so what is

00:30:38,950 --> 00:30:43,020
the next next we've seen major oceans

00:30:41,080 --> 00:30:45,340
big change like what are we

00:30:43,020 --> 00:30:46,990
none of these big changes have been

00:30:45,340 --> 00:30:49,419
planned right it's like they just

00:30:46,990 --> 00:30:52,120
suddenly show up that that is very

00:30:49,419 --> 00:30:53,500
interesting like the indexing

00:30:52,120 --> 00:30:56,140
improvements with documents right upper

00:30:53,500 --> 00:30:57,610
thread interval queries soft deletes

00:30:56,140 --> 00:30:58,809
they like we have a problem we try to

00:30:57,610 --> 00:31:00,850
solve the problem what's the best

00:30:58,809 --> 00:31:03,100
solution and sometimes it takes a couple

00:31:00,850 --> 00:31:05,559
years but yeah that's that's that's the

00:31:03,100 --> 00:31:10,900
outcome this is answer your question

00:31:05,559 --> 00:31:17,730
Mike okay okay discussions there's one

00:31:10,900 --> 00:31:20,049
over here we have enough time luckily

00:31:17,730 --> 00:31:23,110
thank you for the talk I have a question

00:31:20,049 --> 00:31:25,120
related to growth cluster search yeah is

00:31:23,110 --> 00:31:27,070
it faster than calling the three

00:31:25,120 --> 00:31:32,950
clusters in barrel and retrieves the

00:31:27,070 --> 00:31:34,809
results if it's faster didn't I don't

00:31:32,950 --> 00:31:39,010
think you have a performance advantage

00:31:34,809 --> 00:31:41,080
nor downside one thing that you cannot

00:31:39,010 --> 00:31:42,910
do outside of elasticsearch is reduce

00:31:41,080 --> 00:31:45,640
the results together if you have

00:31:42,910 --> 00:31:48,340
aggregations that span multiple serve

00:31:45,640 --> 00:31:50,200
multiple indices like merging them

00:31:48,340 --> 00:31:52,840
together can only elastic search can do

00:31:50,200 --> 00:31:55,210
that otherwise yes it's an obvious task

00:31:52,840 --> 00:31:57,760
for a client I'm completely with you but

00:31:55,210 --> 00:31:59,559
if you if you have like a histogram

00:31:57,760 --> 00:32:01,299
aggregation how do you merge them

00:31:59,559 --> 00:32:04,419
together if you do it outside of elastic

00:32:01,299 --> 00:32:07,570
search but it's in there under the hood

00:32:04,419 --> 00:32:10,299
it works exactly the same as if the

00:32:07,570 --> 00:32:12,850
connection is within the cluster we in

00:32:10,299 --> 00:32:14,169
on on the nodes that can do cross

00:32:12,850 --> 00:32:18,540
coastal search it's a lot like

00:32:14,169 --> 00:32:20,940
regularity here we establish a one

00:32:18,540 --> 00:32:23,280
an unidirectional connection in

00:32:20,940 --> 00:32:25,230
elasticsearch one clusters the star

00:32:23,280 --> 00:32:27,540
network like every node connects to

00:32:25,230 --> 00:32:29,400
every other node and and and vice versa

00:32:27,540 --> 00:32:31,140
cross clusters are just different we

00:32:29,400 --> 00:32:32,669
only connect in one direction and we

00:32:31,140 --> 00:32:34,830
maintain a smaller connection pool but

00:32:32,669 --> 00:32:39,150
these are like keep alive TCP

00:32:34,830 --> 00:32:42,120
connections and the connections as fast

00:32:39,150 --> 00:32:47,730
as your network goes yeah this is a

00:32:42,120 --> 00:32:50,309
question thank you hello and I have a

00:32:47,730 --> 00:32:51,750
question regarding the Big D trees so

00:32:50,309 --> 00:32:54,780
currently it's limited to seven

00:32:51,750 --> 00:32:57,840
dimensions and why it's seven so I see a

00:32:54,780 --> 00:32:59,490
particular use Kies useful for me is to

00:32:57,840 --> 00:33:01,679
compute a document similarity for

00:32:59,490 --> 00:33:03,450
example but usually seven dimensions for

00:33:01,679 --> 00:33:06,169
that is not enough but seven is hard

00:33:03,450 --> 00:33:10,230
coded in loosenin you cannot go further

00:33:06,169 --> 00:33:11,730
so white seven I I think seven is the

00:33:10,230 --> 00:33:15,090
limit where we were able to make it

00:33:11,730 --> 00:33:21,150
performance but I probably might need to

00:33:15,090 --> 00:33:23,160
answer this question the limit is eight

00:33:21,150 --> 00:33:25,590
in the scene I'm not sure why it's seven

00:33:23,160 --> 00:33:27,210
in elasticsearch and eight honestly that

00:33:25,590 --> 00:33:29,070
limit wasn't picked for any particularly

00:33:27,210 --> 00:33:31,950
good reason it was just placed there as

00:33:29,070 --> 00:33:33,720
a temporary limit because we didn't know

00:33:31,950 --> 00:33:36,330
what would happen if we pushed it beyond

00:33:33,720 --> 00:33:37,980
that but KD trees don't really scale

00:33:36,330 --> 00:33:39,570
that high if you try to do a very high

00:33:37,980 --> 00:33:41,760
dimensional KD tree it's sort of

00:33:39,570 --> 00:33:44,820
degrades into a linear search so there

00:33:41,760 --> 00:33:46,740
is a point where it's not worthwhile so

00:33:44,820 --> 00:33:48,690
if that's a limit for your use case and

00:33:46,740 --> 00:33:50,160
it's compelling to relax it open an

00:33:48,690 --> 00:33:51,630
issue start a discussion it doesn't have

00:33:50,160 --> 00:33:54,570
to be eight it could go higher than that

00:33:51,630 --> 00:33:56,340
at leucine or elasticsearch and the

00:33:54,570 --> 00:33:59,040
limit is not seven it's just because

00:33:56,340 --> 00:34:01,200
we're using seven dimensions for that

00:33:59,040 --> 00:34:04,770
geo shape like we could also use eight

00:34:01,200 --> 00:34:07,190
but that's that that data starting to

00:34:04,770 --> 00:34:07,190
fit in there

00:34:10,339 --> 00:34:16,200
hello my question relates to the prefix

00:34:13,859 --> 00:34:19,349
searching with the phrase queries I was

00:34:16,200 --> 00:34:21,979
wondering if that prefix in Graham was

00:34:19,349 --> 00:34:24,450
enabled by default on like all the text

00:34:21,979 --> 00:34:27,149
no yeah you have to enable it in a

00:34:24,450 --> 00:34:29,999
mapping and is the phrase wildcard

00:34:27,149 --> 00:34:33,539
phrase searching then only enabled for

00:34:29,999 --> 00:34:36,839
the in Graham fields no no you can you

00:34:33,539 --> 00:34:38,819
can do that on any field it'll just

00:34:36,839 --> 00:34:40,799
basically look at the mapping is the

00:34:38,819 --> 00:34:42,539
optimization enable if the optimization

00:34:40,799 --> 00:34:45,809
enabled the query that we execute is

00:34:42,539 --> 00:34:47,190
different okay thank you right so we got

00:34:45,809 --> 00:34:54,869
the same result but if you enable the

00:34:47,190 --> 00:34:59,779
optimization will be faster hey so I

00:34:54,869 --> 00:34:59,779
have a question about the weekend week

00:35:01,309 --> 00:35:05,759
I'm terribly familiar with that but I'm

00:35:04,559 --> 00:35:07,499
you mentioned that there are multiple

00:35:05,759 --> 00:35:10,079
constructs that can actually disable

00:35:07,499 --> 00:35:11,729
this optimization it doesn't it hasn't

00:35:10,079 --> 00:35:14,309
propagated to all the types of queries

00:35:11,729 --> 00:35:15,989
yet you mention aggregations as example

00:35:14,309 --> 00:35:18,779
and also like tracking total counts

00:35:15,989 --> 00:35:21,839
results counts and then my question is

00:35:18,779 --> 00:35:23,609
like if I write my words myself and I

00:35:21,839 --> 00:35:25,769
maybe use a construct that actually

00:35:23,609 --> 00:35:29,160
prevents this optimization for kicking

00:35:25,769 --> 00:35:30,839
in how can I easily found because maybe

00:35:29,160 --> 00:35:32,249
I can achieve what I want with a

00:35:30,839 --> 00:35:34,739
different construct which does not

00:35:32,249 --> 00:35:39,109
disable this optimization is a profile

00:35:34,739 --> 00:35:39,109
API the right way to look at to find out

00:35:39,319 --> 00:35:46,460
like so if you get this before this yeah

00:35:47,269 --> 00:35:57,269
so if you got the relation to be exact

00:35:53,640 --> 00:35:59,910
in the result you know that we didn't

00:35:57,269 --> 00:36:01,499
apply the optimization yeah buddy for

00:35:59,910 --> 00:36:02,880
example maybe I use some because I

00:36:01,499 --> 00:36:05,309
understand there are more constructs

00:36:02,880 --> 00:36:09,670
that disable these optimism it's only

00:36:05,309 --> 00:36:11,559
only this aggregation dirt again

00:36:09,670 --> 00:36:14,319
yeah you know what I mean but also

00:36:11,559 --> 00:36:19,270
scripts Corizon might disable this

00:36:14,319 --> 00:36:23,200
optimization if I have very yes yes we

00:36:19,270 --> 00:36:25,210
not using the optimization when script

00:36:23,200 --> 00:36:28,599
store is in the creek that is correct

00:36:25,210 --> 00:36:30,579
depending on the creek also I guess okay

00:36:28,599 --> 00:36:33,309
I have also have a second questions is

00:36:30,579 --> 00:36:35,890
Mike and Michael yesterday had a talk

00:36:33,309 --> 00:36:37,990
how they usually seen at Amazon and they

00:36:35,890 --> 00:36:39,880
mentioned multiple times that they use

00:36:37,990 --> 00:36:42,790
some features that are exposing the the

00:36:39,880 --> 00:36:44,049
scene but exist new scene but don't

00:36:42,790 --> 00:36:46,780
exist in the last search near

00:36:44,049 --> 00:36:49,420
elasticsearch on Ursula one of those

00:36:46,780 --> 00:36:51,990
features was Lucy searching across

00:36:49,420 --> 00:36:55,119
multiple segments with different threads

00:36:51,990 --> 00:36:58,809
is that what we might see one day in the

00:36:55,119 --> 00:37:00,940
last search maybe yeah maybe okay I mean

00:36:58,809 --> 00:37:05,049
Mike and I talked about it so the

00:37:00,940 --> 00:37:07,329
biggest problem is that how do you do

00:37:05,049 --> 00:37:09,460
the scheduling which quick quest gets

00:37:07,329 --> 00:37:11,859
the most threads like the concurrency in

00:37:09,460 --> 00:37:14,260
elasticsearch Lassiter's bill to get

00:37:11,859 --> 00:37:16,240
concurrency through queries like

00:37:14,260 --> 00:37:18,849
executing a lot of queries in parallel

00:37:16,240 --> 00:37:22,900
rather than executing one query in

00:37:18,849 --> 00:37:25,089
parallel but yeah as a I I started to

00:37:22,900 --> 00:37:26,470
look into this yesterday and then we I

00:37:25,089 --> 00:37:30,339
think we have enough constructs in

00:37:26,470 --> 00:37:33,309
elasticsearch to to allow this on demand

00:37:30,339 --> 00:37:34,750
like that you know if you don't have a

00:37:33,309 --> 00:37:38,730
lot of conquering surgeries will give

00:37:34,750 --> 00:37:38,730
some threats to the search execution

00:37:38,819 --> 00:37:45,099
there's there's a lot of corners here I

00:37:41,589 --> 00:37:46,930
guess but this is something that I I'm

00:37:45,099 --> 00:37:49,480
personally not a huge fan of settings

00:37:46,930 --> 00:37:52,900
I would like to enable this feature when

00:37:49,480 --> 00:37:54,880
we can write if you have 100 queries a

00:37:52,900 --> 00:37:57,190
second on a note it doesn't make sense

00:37:54,880 --> 00:37:58,900
to use multiple threats but you have

00:37:57,190 --> 00:38:01,660
three queries the second I don't know it

00:37:58,900 --> 00:38:02,950
makes total sense it's not a week we

00:38:01,660 --> 00:38:05,140
know we have this information we have

00:38:02,950 --> 00:38:07,569
queue length with utilization we have

00:38:05,140 --> 00:38:09,910
the time as curry spending the queue if

00:38:07,569 --> 00:38:12,150
it's cute at all we can use the

00:38:09,910 --> 00:38:14,559
statistic to me to make that work and

00:38:12,150 --> 00:38:16,900
again that's it's a it's a classical

00:38:14,559 --> 00:38:18,460
example of like if I would have tried to

00:38:16,900 --> 00:38:20,260
implement this four years ago I would

00:38:18,460 --> 00:38:21,970
have been a disaster and very hard but

00:38:20,260 --> 00:38:23,380
today might be different and then these

00:38:21,970 --> 00:38:26,400
ideas come up and that's why we come

00:38:23,380 --> 00:38:33,059
his conferences and talk and yeah so I

00:38:26,400 --> 00:38:35,859
know another promise but it yeah yeah

00:38:33,059 --> 00:38:39,519
perhaps the last question because we are

00:38:35,859 --> 00:38:43,089
running out of time hello hey thank you

00:38:39,519 --> 00:38:46,809
so still a question about cross cluster

00:38:43,089 --> 00:38:49,630
search so is is the cross cluster served

00:38:46,809 --> 00:38:51,609
a new way of distributing the cluster

00:38:49,630 --> 00:38:54,009
stage like should we use it even if we

00:38:51,609 --> 00:38:56,230
don't have multiple locations but just

00:38:54,009 --> 00:38:58,119
to distribute the cluster tip so the

00:38:56,230 --> 00:39:00,910
cluster state is not distributed with

00:38:58,119 --> 00:39:02,140
cross cluster search its stateless that

00:39:00,910 --> 00:39:04,390
is one of the biggest differences

00:39:02,140 --> 00:39:06,279
between tribe node and cross cluster

00:39:04,390 --> 00:39:07,809
search course cluster search is a pure

00:39:06,279 --> 00:39:09,220
read only right you don't have a

00:39:07,809 --> 00:39:11,259
coordination if you make a change like

00:39:09,220 --> 00:39:13,539
add a news index or mapping change in

00:39:11,259 --> 00:39:15,490
one cluster that that is the other

00:39:13,539 --> 00:39:16,990
clusters were not affected they don't

00:39:15,490 --> 00:39:18,220
get any information about this there's

00:39:16,990 --> 00:39:19,450
one of the biggest problems which tribe

00:39:18,220 --> 00:39:21,519
note is like you have one cluster

00:39:19,450 --> 00:39:23,109
updates the cluster stayed and it gets

00:39:21,519 --> 00:39:26,859
propagated to all the tribe notes that

00:39:23,109 --> 00:39:29,380
connected to it cross cluster search was

00:39:26,859 --> 00:39:31,119
solely made for this one purpose that

00:39:29,380 --> 00:39:33,130
you'll be able to search across multiple

00:39:31,119 --> 00:39:35,349
clusters if these clusters are in the

00:39:33,130 --> 00:39:36,819
same data center or in different data

00:39:35,349 --> 00:39:38,380
centers in the same region or in

00:39:36,819 --> 00:39:42,579
different regions is completely

00:39:38,380 --> 00:39:44,890
irrelevant if you ask me for the my

00:39:42,579 --> 00:39:47,349
recommendation I would totally recommend

00:39:44,890 --> 00:39:49,990
you to have more smaller clusters than

00:39:47,349 --> 00:39:52,180
one big one for the for a couple of

00:39:49,990 --> 00:39:53,740
reasons like if you you upgrading well

00:39:52,180 --> 00:39:56,619
you can do one by one you're learning

00:39:53,740 --> 00:39:58,809
about the the pitfalls earlier if one

00:39:56,619 --> 00:40:01,089
cluster breaks down and you still have

00:39:58,809 --> 00:40:03,220
the other ones to serve requests if you

00:40:01,089 --> 00:40:06,339
want to etch out data over time you can

00:40:03,220 --> 00:40:09,190
do multiple clusters the granularity how

00:40:06,339 --> 00:40:12,210
big one of these closest is is a very it

00:40:09,190 --> 00:40:12,210

YouTube URL: https://www.youtube.com/watch?v=17fEK1eTck0


