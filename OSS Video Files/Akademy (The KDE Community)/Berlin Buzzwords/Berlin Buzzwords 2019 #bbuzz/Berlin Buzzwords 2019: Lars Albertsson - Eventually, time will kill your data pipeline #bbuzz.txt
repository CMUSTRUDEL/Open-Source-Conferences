Title: Berlin Buzzwords 2019: Lars Albertsson - Eventually, time will kill your data pipeline #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Race conditions and intermittent failures, daylight saving time, time zones, leap seconds, and overload conditions - time is a factor in many of the most annoying problems in computer systems. Data engineering is not exempt from problems caused by time, but also has a slew of unique problems. 

In this presentation, we will enumerate the time-related problems that we have seen cause trouble in data processing system components, including data collection, batch processing, workflow orchestration, and stream processing. We will provide examples of time-related incidents, and also tools and tricks to avoid timing issues in data processing systems.

Read more:
https://2019.berlinbuzzwords.de/19/session/eventually-time-will-kill-your-data-pipeline

About Lars Albertsson:
https://2019.berlinbuzzwords.de/users/lars-albertsson

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,609 --> 00:00:10,000
thank you everyone

00:00:11,400 --> 00:00:15,719
and a little time that we have here on

00:00:13,860 --> 00:00:19,200
earth we spend writing data pipelines

00:00:15,719 --> 00:00:21,090
one of my favorite things to do on in a

00:00:19,200 --> 00:00:23,340
professional aspect just to come here to

00:00:21,090 --> 00:00:25,050
this conference because it's such a

00:00:23,340 --> 00:00:27,869
technical ordinance you can really D

00:00:25,050 --> 00:00:31,320
keen on something and go deep and this

00:00:27,869 --> 00:00:33,660
year I've chosen to geek in on the

00:00:31,320 --> 00:00:37,649
troubles that time I caused and how to

00:00:33,660 --> 00:00:39,329
deal with them so at support I saw a

00:00:37,649 --> 00:00:41,460
couple of slide or a couple of graphs

00:00:39,329 --> 00:00:43,170
that look like this these graphs tend to

00:00:41,460 --> 00:00:45,359
freak people out in particular if you're

00:00:43,170 --> 00:00:47,460
at the dip and somebody had launched a

00:00:45,359 --> 00:00:50,370
campaign or launched in a new country or

00:00:47,460 --> 00:00:52,890
something I'll also send a graph that

00:00:50,370 --> 00:00:56,069
looks like this there's a mysterious dip

00:00:52,890 --> 00:00:59,179
I kept midnight every day doesn't seem

00:00:56,069 --> 00:01:01,589
to match what we expect or this

00:00:59,179 --> 00:01:04,220
mysterious bump in the middle of the

00:01:01,589 --> 00:01:08,370
night one about that what that might be

00:01:04,220 --> 00:01:10,500
or this one only premium users are

00:01:08,370 --> 00:01:12,870
supposed to access premium services but

00:01:10,500 --> 00:01:15,600
apparently some free users do that but

00:01:12,870 --> 00:01:18,450
then they stop at midnight and then they

00:01:15,600 --> 00:01:22,860
start again that's weird these are

00:01:18,450 --> 00:01:25,890
examples of things that do not really

00:01:22,860 --> 00:01:28,260
match what the users did but are caused

00:01:25,890 --> 00:01:32,100
by our mismanagement of time and data

00:01:28,260 --> 00:01:33,870
processing pipelines so I will mention a

00:01:32,100 --> 00:01:35,910
bunch of issues that I've seen a bunch

00:01:33,870 --> 00:01:37,680
of and patterns there were we have dealt

00:01:35,910 --> 00:01:41,640
with things badly and a couple of

00:01:37,680 --> 00:01:44,340
patterns that I have come to conclude to

00:01:41,640 --> 00:01:46,530
use in order to mitigate the problems

00:01:44,340 --> 00:01:48,480
these are quite I'm quite opinionated in

00:01:46,530 --> 00:01:51,150
my pattern so take it with a grain of

00:01:48,480 --> 00:01:53,480
salt my hope is that you will become

00:01:51,150 --> 00:01:55,890
more aware of the issues that might

00:01:53,480 --> 00:01:59,520
occur and that I will share some of the

00:01:55,890 --> 00:02:02,040
tools that I have in my toolbox first

00:01:59,520 --> 00:02:03,780
looking at what from a time perspective

00:02:02,040 --> 00:02:05,520
what different types of data we have we

00:02:03,780 --> 00:02:07,230
have facts these are either or events

00:02:05,520 --> 00:02:10,200
that happen or like observations

00:02:07,230 --> 00:02:11,970
measurements out in the wild these have

00:02:10,200 --> 00:02:14,190
a time stamp they happen at a particular

00:02:11,970 --> 00:02:15,690
time and they communicate like a

00:02:14,190 --> 00:02:18,240
continuous stream we don't really

00:02:15,690 --> 00:02:19,980
control the time something else does we

00:02:18,240 --> 00:02:23,430
also have state that we have on our

00:02:19,980 --> 00:02:26,189
system and we have usually dump to the

00:02:23,430 --> 00:02:27,870
I can explain why later and these are

00:02:26,189 --> 00:02:30,079
typically dumped that like regular

00:02:27,870 --> 00:02:33,329
intervals daily or hourly or something

00:02:30,079 --> 00:02:35,400
and then we have claims which is a

00:02:33,329 --> 00:02:37,650
statement about something that has

00:02:35,400 --> 00:02:42,290
happened previously I'll give a couple

00:02:37,650 --> 00:02:44,670
of examples anymore and these claims

00:02:42,290 --> 00:02:46,469
always have a windows a time window

00:02:44,670 --> 00:02:51,359
scope like their claims about a

00:02:46,469 --> 00:02:54,150
particular time period these events and

00:02:51,359 --> 00:02:57,419
state dumps and and claims have

00:02:54,150 --> 00:02:59,849
different time scopes it's more mostly

00:02:57,419 --> 00:03:02,010
complicated for for events you have the

00:02:59,849 --> 00:03:04,139
event time the time at which the event

00:03:02,010 --> 00:03:06,209
occurred and they have some registration

00:03:04,139 --> 00:03:07,500
time when you see the event coming into

00:03:06,209 --> 00:03:09,540
your back-end systems where you can

00:03:07,500 --> 00:03:12,120
trust the clocks and then you have the

00:03:09,540 --> 00:03:13,889
ingestion time where which is when you

00:03:12,120 --> 00:03:17,250
put it into your data like for for

00:03:13,889 --> 00:03:19,379
eternal preservation in a single DC

00:03:17,250 --> 00:03:20,609
environment the registration ingest this

00:03:19,379 --> 00:03:21,659
usually the same time in a multi

00:03:20,609 --> 00:03:24,359
disinvite meant you might have a

00:03:21,659 --> 00:03:26,340
propagation delay and then later we

00:03:24,359 --> 00:03:30,750
process the data so we have a processing

00:03:26,340 --> 00:03:34,319
time as well for for state we only have

00:03:30,750 --> 00:03:36,389
the ingest time and then for claims we

00:03:34,319 --> 00:03:39,510
have the word I call the domain time the

00:03:36,389 --> 00:03:41,340
time scope which the claim refers to for

00:03:39,510 --> 00:03:43,019
example these were the users that we

00:03:41,340 --> 00:03:49,349
thought were fraudulent during this

00:03:43,019 --> 00:03:51,750
particular time we know we use clocks to

00:03:49,349 --> 00:03:53,970
measure time they don't actually really

00:03:51,750 --> 00:03:58,139
tell us what time it is they tell us how

00:03:53,970 --> 00:03:59,280
much time has elapsed and but in order

00:03:58,139 --> 00:04:00,810
to deal with them we first tell them

00:03:59,280 --> 00:04:03,329
that more time it is and then we get a

00:04:00,810 --> 00:04:04,919
new machine it back we have clocks that

00:04:03,329 --> 00:04:06,389
our high quality usually our back and

00:04:04,919 --> 00:04:09,120
stuff clocks that are of low quality

00:04:06,389 --> 00:04:12,780
like in IOT devices mobile devices and

00:04:09,120 --> 00:04:15,090
so forth we also have wrong clocks which

00:04:12,780 --> 00:04:16,590
is when we look at the different time

00:04:15,090 --> 00:04:18,630
scope than the one we actually care

00:04:16,590 --> 00:04:22,830
about sometimes this is okay we used the

00:04:18,630 --> 00:04:26,250
wrong time scope as a proxy and some

00:04:22,830 --> 00:04:29,430
clocks we control some clocks we can

00:04:26,250 --> 00:04:30,840
trust in their value evaluation their

00:04:29,430 --> 00:04:32,490
measurement of time but we don't really

00:04:30,840 --> 00:04:35,539
control the how they are set in terms of

00:04:32,490 --> 00:04:35,539
time zones and so forth

00:04:37,060 --> 00:04:44,060
clock time maps the calendars which is

00:04:40,970 --> 00:04:46,220
like your more a human invention that

00:04:44,060 --> 00:04:49,460
maps to the astronomical and social

00:04:46,220 --> 00:04:51,530
domains and if I were to ask my son how

00:04:49,460 --> 00:04:53,330
the definition of a calendar this is

00:04:51,530 --> 00:04:55,100
what he might come up with this is also

00:04:53,330 --> 00:04:59,539
what a computer comes up with if you if

00:04:55,100 --> 00:05:03,199
you ask it Maeve Lee now reality is more

00:04:59,539 --> 00:05:05,509
difficult in a naive calendar you might

00:05:03,199 --> 00:05:08,600
think that these properties hold like is

00:05:05,509 --> 00:05:11,240
a day has a certain number of seconds if

00:05:08,600 --> 00:05:14,360
you go 45 years in the future you end up

00:05:11,240 --> 00:05:15,979
in what year you are at plus 45 if you

00:05:14,360 --> 00:05:16,580
start at the beginning of the year and

00:05:15,979 --> 00:05:19,130
you go

00:05:16,580 --> 00:05:22,400
36 364 days for do you think you might

00:05:19,130 --> 00:05:24,500
end up in December if you have 60

00:05:22,400 --> 00:05:27,259
minutes forward you might end up in the

00:05:24,500 --> 00:05:29,930
next hour you think and if you shift

00:05:27,259 --> 00:05:32,000
timezone you would perhaps think that

00:05:29,930 --> 00:05:35,810
you at most shift today and that the

00:05:32,000 --> 00:05:38,419
number of minutes is preserved and if

00:05:35,810 --> 00:05:40,760
you are at December 29th you might think

00:05:38,419 --> 00:05:44,000
that the next day is December 30th or

00:05:40,760 --> 00:05:48,919
that January or February has like 28 or

00:05:44,000 --> 00:05:51,590
29 days now I'm going to test you how

00:05:48,919 --> 00:05:55,220
obviously since I put them up these are

00:05:51,590 --> 00:05:56,930
at some point in time wrong how many

00:05:55,220 --> 00:06:00,789
people can find at least one

00:05:56,930 --> 00:06:00,789
counterexample for these things Oh

00:06:04,430 --> 00:06:10,610
how many people can find at least two

00:06:06,169 --> 00:06:20,180
keep your hands up at least three at

00:06:10,610 --> 00:06:21,650
least for at least five six oh it's not

00:06:20,180 --> 00:06:24,130
so easy I had to Google a bit on this

00:06:21,650 --> 00:06:27,229
one it's performance the leap seconds

00:06:24,130 --> 00:06:29,389
second one is year zero after year minus

00:06:27,229 --> 00:06:30,889
one comes year plus one in historical

00:06:29,389 --> 00:06:32,840
time counting there's an astronomical

00:06:30,889 --> 00:06:35,360
just to confuse things where there is a

00:06:32,840 --> 00:06:39,169
year zero there was a switch between

00:06:35,360 --> 00:06:41,030
Julian and Gregorian calendars when you

00:06:39,169 --> 00:06:42,349
might ask well it stretched over 300

00:06:41,030 --> 00:06:44,419
years in different countries and it

00:06:42,349 --> 00:06:46,190
stretched your way into the 20th century

00:06:44,419 --> 00:06:48,730
daylight savings time you're familiar

00:06:46,190 --> 00:06:50,980
with time zones are not

00:06:48,730 --> 00:06:52,360
not really on a full hour and they span

00:06:50,980 --> 00:06:54,280
more than 24 hours

00:06:52,360 --> 00:06:57,880
some countries jump back and forth

00:06:54,280 --> 00:07:00,490
between on the date line so they

00:06:57,880 --> 00:07:03,490
sometimes skip a day and I proudly

00:07:00,490 --> 00:07:08,260
present the only country to have had the

00:07:03,490 --> 00:07:10,690
30th of February this was as part of the

00:07:08,260 --> 00:07:13,390
confusion in the Gregorian Jolanda

00:07:10,690 --> 00:07:14,730
gorian calendars which fortunately you

00:07:13,390 --> 00:07:17,470
don't have to know most of these things

00:07:14,730 --> 00:07:19,990
they rarely come up unless you're doing

00:07:17,470 --> 00:07:22,840
real historical data leap seconds might

00:07:19,990 --> 00:07:25,900
go up it took down a couple of big sites

00:07:22,840 --> 00:07:28,870
a few years ago for data processing it's

00:07:25,900 --> 00:07:31,840
probably not a huge deal you might have

00:07:28,870 --> 00:07:34,360
to prepare that that the timestamps

00:07:31,840 --> 00:07:36,040
might have the second number 60 rather

00:07:34,360 --> 00:07:38,170
than that you should are probably fine

00:07:36,040 --> 00:07:39,850
if you're in Google they smear the leap

00:07:38,170 --> 00:07:42,030
seconds out very elegant solution I

00:07:39,850 --> 00:07:45,010
don't know what l other cloudforest do

00:07:42,030 --> 00:07:50,500
daylight savings town time on the other

00:07:45,010 --> 00:07:53,380
hand that for sure one will bite you it

00:07:50,500 --> 00:07:55,390
even kills people are you aware that the

00:07:53,380 --> 00:07:57,570
in spring when you lose an hour there's

00:07:55,390 --> 00:07:59,440
an elevated number of heart attacks and

00:07:57,570 --> 00:08:01,270
there's a lower number of heart attacks

00:07:59,440 --> 00:08:04,020
in autumn but they don't add up

00:08:01,270 --> 00:08:06,940
so daylight savings time killed people

00:08:04,020 --> 00:08:10,240
and of course they'll attend his time is

00:08:06,940 --> 00:08:12,910
the explanation for this counter graph

00:08:10,240 --> 00:08:15,370
where suddenly the hour the day has like

00:08:12,910 --> 00:08:19,620
24 hours and there's a corresponding dip

00:08:15,370 --> 00:08:22,060
in the awesome now how might this affect

00:08:19,620 --> 00:08:23,350
technical systems this is a scenario we

00:08:22,060 --> 00:08:27,220
had recently

00:08:23,350 --> 00:08:28,780
this is a typical ingestion pattern

00:08:27,220 --> 00:08:31,570
called the loading dock you have a

00:08:28,780 --> 00:08:34,000
legacy system it pushes data on a

00:08:31,570 --> 00:08:36,910
regular basis and this gains hourly to a

00:08:34,000 --> 00:08:39,550
neutral ground a file system a Google

00:08:36,910 --> 00:08:41,500
cloud bucket in this case and they

00:08:39,550 --> 00:08:46,960
ingest your mechanism pulls it and

00:08:41,500 --> 00:08:49,870
compensate to the data like usually the

00:08:46,960 --> 00:08:51,250
legacy systems are more difficult to

00:08:49,870 --> 00:08:53,380
change so you adapt to whatever

00:08:51,250 --> 00:08:57,640
conventions they haven't just accept

00:08:53,380 --> 00:09:01,780
them in this case by the way I included

00:08:57,640 --> 00:09:03,700
some Luigi workflow code because I've

00:09:01,780 --> 00:09:04,990
I've seen people stumble with these

00:09:03,700 --> 00:09:08,940
types of patterns so here's an example

00:09:04,990 --> 00:09:11,740
you can get sliced later if you want in

00:09:08,940 --> 00:09:15,040
this case it turned out that the time

00:09:11,740 --> 00:09:16,750
stamps for a local time would including

00:09:15,040 --> 00:09:19,780
local timezone which doesn't matter all

00:09:16,750 --> 00:09:21,310
that much until there's a shift and then

00:09:19,780 --> 00:09:24,310
in spring suddenly there's one hour

00:09:21,310 --> 00:09:27,370
missing and then in autumn if we're at

00:09:24,310 --> 00:09:29,020
the code the work local this way it will

00:09:27,370 --> 00:09:30,760
pick up the first data set it will

00:09:29,020 --> 00:09:34,570
ignore the second data set and we have a

00:09:30,760 --> 00:09:36,520
silent data loss you can compensate

00:09:34,570 --> 00:09:38,560
these either on the source systems by

00:09:36,520 --> 00:09:41,350
not including the daylight savings time

00:09:38,560 --> 00:09:47,830
or the time zone or by with some hacker

00:09:41,350 --> 00:09:50,740
of course in the receiving system so one

00:09:47,830 --> 00:09:52,150
principle that I try to push to to all

00:09:50,740 --> 00:09:54,670
the clients and people that I worked

00:09:52,150 --> 00:09:56,860
with is to separate the online world

00:09:54,670 --> 00:09:59,470
from the offline world in the online

00:09:56,860 --> 00:10:01,240
world processing time and event time are

00:09:59,470 --> 00:10:04,210
tightly connected you process things

00:10:01,240 --> 00:10:06,190
right away after they happened in the

00:10:04,210 --> 00:10:08,500
offline world you process them later

00:10:06,190 --> 00:10:13,240
with streaming just a little later with

00:10:08,500 --> 00:10:15,070
batch perhaps much later and it can be

00:10:13,240 --> 00:10:17,650
tricky when you mix these world due to

00:10:15,070 --> 00:10:20,339
the different time scopes when you're

00:10:17,650 --> 00:10:22,960
batch processing there are a number of

00:10:20,339 --> 00:10:25,390
principles essentially inherited from

00:10:22,960 --> 00:10:27,880
functional programming that I found to

00:10:25,390 --> 00:10:30,520
be really valuable and to adhere to to

00:10:27,880 --> 00:10:34,080
make things simpler I will explain why

00:10:30,520 --> 00:10:38,500
these are so important in the very end

00:10:34,080 --> 00:10:41,680
essentially you want your batch process

00:10:38,500 --> 00:10:44,560
to be a pure function it should be the

00:10:41,680 --> 00:10:46,810
output should be a function only of the

00:10:44,560 --> 00:10:49,780
input and of the code of nothing else

00:10:46,810 --> 00:10:51,370
not state and databases no randomness no

00:10:49,780 --> 00:10:54,670
wall clock time don't look at the

00:10:51,370 --> 00:10:57,370
processing time and so forth and you

00:10:54,670 --> 00:10:59,800
want the amount of input data to be

00:10:57,370 --> 00:11:02,170
known and bounded this is a common like

00:10:59,800 --> 00:11:04,480
beginner mistake and data processing

00:11:02,170 --> 00:11:06,220
violence that I see often we process

00:11:04,480 --> 00:11:08,410
whatever data happens to be in this

00:11:06,220 --> 00:11:10,830
bucket or in this bigquery data set and

00:11:08,410 --> 00:11:10,830
so forth

00:11:11,490 --> 00:11:15,689
in order to achieve this if you want

00:11:14,129 --> 00:11:17,459
state from a database

00:11:15,689 --> 00:11:19,019
we cannot query the database so

00:11:17,459 --> 00:11:23,790
therefore we dumped the database to like

00:11:19,019 --> 00:11:27,529
on a regular basis it seems simple I've

00:11:23,790 --> 00:11:30,929
seen lots of ways to fumble with this

00:11:27,529 --> 00:11:32,579
one is to direct your cluster to dump

00:11:30,929 --> 00:11:34,889
from the database which is fine if the

00:11:32,579 --> 00:11:36,929
data is small but if the data is large

00:11:34,889 --> 00:11:39,360
this is essentially a denial of service

00:11:36,929 --> 00:11:40,980
attack this is one of my oldest slides

00:11:39,360 --> 00:11:43,740
as you can tell from the references to

00:11:40,980 --> 00:11:45,629
scoop and MapReduce some of you might

00:11:43,740 --> 00:11:47,519
have seen it before I've seen a couple

00:11:45,629 --> 00:11:50,519
of systems go down this way and you're

00:11:47,519 --> 00:11:52,139
essentially mixing up the offline world

00:11:50,519 --> 00:11:53,639
separation of processing time was

00:11:52,139 --> 00:11:55,889
processing time and event time are

00:11:53,639 --> 00:11:57,869
separated into the online world where

00:11:55,889 --> 00:11:59,730
they are the same and the offline world

00:11:57,869 --> 00:12:01,759
wants all of the events at once and

00:11:59,730 --> 00:12:05,399
process it as soon as possible

00:12:01,759 --> 00:12:07,139
so here's a a pattern corresponding

00:12:05,399 --> 00:12:08,879
pattern that you can do you are very

00:12:07,139 --> 00:12:09,329
careful taking the data to the offline

00:12:08,879 --> 00:12:11,759
world

00:12:09,329 --> 00:12:13,230
and in and I recommend using some kind

00:12:11,759 --> 00:12:16,049
of replica you can have a library liquor

00:12:13,230 --> 00:12:18,179
in this case I suggest taken the backups

00:12:16,049 --> 00:12:20,459
from your databases restoring in an

00:12:18,179 --> 00:12:25,470
offline database and you have separated

00:12:20,459 --> 00:12:26,309
the online and the offline world now

00:12:25,470 --> 00:12:27,929
what do you want to do with your

00:12:26,309 --> 00:12:30,299
snapshots in many cases you want to

00:12:27,929 --> 00:12:31,829
decorate your events you have your users

00:12:30,299 --> 00:12:33,389
in the database and your all of your

00:12:31,829 --> 00:12:35,759
events have a user ID so you want to

00:12:33,389 --> 00:12:37,589
decorate them and throw some remote

00:12:35,759 --> 00:12:40,920
demographic information in there and so

00:12:37,589 --> 00:12:44,040
forth but the events are a continuous

00:12:40,920 --> 00:12:47,129
stream continues over time and this the

00:12:44,040 --> 00:12:50,009
snapshots that you have come at regular

00:12:47,129 --> 00:12:51,839
intervals so when you join you will

00:12:50,009 --> 00:12:57,029
always join information from two

00:12:51,839 --> 00:12:58,769
different times so this you can usually

00:12:57,029 --> 00:13:00,929
live with this this is acceptable but

00:12:58,769 --> 00:13:02,579
you need to be aware and it's often

00:13:00,929 --> 00:13:04,829
easier to have the mismatch only in one

00:13:02,579 --> 00:13:07,769
direction it's common to to join with a

00:13:04,829 --> 00:13:09,209
previous snapshot or the user database

00:13:07,769 --> 00:13:13,980
for example otherwise you have to wait

00:13:09,209 --> 00:13:18,139
for an hour or 24 hours but this

00:13:13,980 --> 00:13:23,730
mismatch is what causes graphs like this

00:13:18,139 --> 00:13:26,009
because here the user we was free a free

00:13:23,730 --> 00:13:27,839
user here upgraded to premium here and

00:13:26,009 --> 00:13:29,910
then started using premium services here

00:13:27,839 --> 00:13:32,790
but when you join you have the mismatch

00:13:29,910 --> 00:13:38,040
and you have this weakness as long as

00:13:32,790 --> 00:13:41,160
you where it's usually fine if you're

00:13:38,040 --> 00:13:46,399
doing batch you your events or bucket

00:13:41,160 --> 00:13:49,769
and in two batches and if that batch

00:13:46,399 --> 00:13:53,490
doesn't really align with the dump time

00:13:49,769 --> 00:13:55,799
of the database you might end up with

00:13:53,490 --> 00:13:59,669
having wrong information in two time

00:13:55,799 --> 00:14:01,829
directions usually okay but if you care

00:13:59,669 --> 00:14:03,389
about being the data being wrong in only

00:14:01,829 --> 00:14:07,559
one direction you might want to shift

00:14:03,389 --> 00:14:11,730
this window and a couple of occasions I

00:14:07,559 --> 00:14:14,309
try to understand a complex system where

00:14:11,730 --> 00:14:16,439
and the different different situations

00:14:14,309 --> 00:14:17,999
with how we bucket and where the data is

00:14:16,439 --> 00:14:20,220
copied from and the other data and

00:14:17,999 --> 00:14:22,049
perhaps we can align to make the error

00:14:20,220 --> 00:14:24,269
smaller and the result has always been

00:14:22,049 --> 00:14:25,889
no because of some reason we're screwed

00:14:24,269 --> 00:14:31,049
anyway we just live with it with the

00:14:25,889 --> 00:14:34,799
information loss you can avoid this by

00:14:31,049 --> 00:14:36,749
adopting a paradigm called event

00:14:34,799 --> 00:14:38,850
sourcing where you say that the the

00:14:36,749 --> 00:14:42,209
state and data basis is not the truth

00:14:38,850 --> 00:14:45,329
the only truth out there is the history

00:14:42,209 --> 00:14:47,100
of events that has happened and we do

00:14:45,329 --> 00:14:49,379
use databases but they are just the

00:14:47,100 --> 00:14:54,419
cached view of a certain aggregation of

00:14:49,379 --> 00:14:57,720
the history of events now if you only

00:14:54,419 --> 00:15:01,459
look at if you join now you can choose

00:14:57,720 --> 00:15:06,029
to play as many events as you want to

00:15:01,459 --> 00:15:07,769
match the time where you want to join so

00:15:06,029 --> 00:15:11,100
that you don't have these mismatches

00:15:07,769 --> 00:15:13,499
anymore unfortunately this makes your

00:15:11,100 --> 00:15:15,209
code significantly more significantly

00:15:13,499 --> 00:15:17,699
more complex you don't you no longer

00:15:15,209 --> 00:15:22,769
just look up and a table and undo a

00:15:17,699 --> 00:15:25,400
plane join this point any streaming fans

00:15:22,769 --> 00:15:28,460
might say well this these are problem

00:15:25,400 --> 00:15:32,180
specific to batch right if we stream we

00:15:28,460 --> 00:15:38,030
can update a table all the time and join

00:15:32,180 --> 00:15:40,010
with that table so great we have one

00:15:38,030 --> 00:15:41,480
stream with it with the user events here

00:15:40,010 --> 00:15:45,080
perhaps and we have some other stream

00:15:41,480 --> 00:15:47,360
with a service events or playing of the

00:15:45,080 --> 00:15:49,730
films or whatever so we just update this

00:15:47,360 --> 00:15:52,580
table in the middle and join with that

00:15:49,730 --> 00:15:55,640
whatever a state is in that table and

00:15:52,580 --> 00:15:59,060
this will give you the right data under

00:15:55,640 --> 00:16:02,420
the naive assumption that these streams

00:15:59,060 --> 00:16:05,210
are in sync and practice it's difficult

00:16:02,420 --> 00:16:07,130
to make streams being sync so every join

00:16:05,210 --> 00:16:11,500
here will be a race condition to see

00:16:07,130 --> 00:16:11,500
which event goes first to the table

00:16:13,480 --> 00:16:18,020
let's continue on the streaming track in

00:16:15,860 --> 00:16:20,200
order to avoid these types of race

00:16:18,020 --> 00:16:22,130
conditions you usually look at Windows

00:16:20,200 --> 00:16:23,840
when you're doing stream processing

00:16:22,130 --> 00:16:26,990
whether you do in aggregations on the

00:16:23,840 --> 00:16:30,310
left or whether you're joining here on

00:16:26,990 --> 00:16:32,960
the right in stream you kind of look at

00:16:30,310 --> 00:16:35,840
larger windows of time on both the

00:16:32,960 --> 00:16:38,180
streams and try to match them you have

00:16:35,840 --> 00:16:40,100
some choices when when making the

00:16:38,180 --> 00:16:41,870
windows you have to choice whether to do

00:16:40,100 --> 00:16:45,890
sliding windows or tumbling windows and

00:16:41,870 --> 00:16:49,610
so forth in addition to this you have to

00:16:45,890 --> 00:16:53,390
Cho choose what to window over which

00:16:49,610 --> 00:16:55,850
time or in the upper case here the

00:16:53,390 --> 00:16:57,350
number of events is also an option the

00:16:55,850 --> 00:16:59,120
early stream processing systems were

00:16:57,350 --> 00:17:02,630
only supporting like number of events

00:16:59,120 --> 00:17:05,470
and processing time now those are

00:17:02,630 --> 00:17:07,160
easiest to implement but often not

00:17:05,470 --> 00:17:08,900
necessarily what you need because you

00:17:07,160 --> 00:17:12,490
have a trade-off here with the size of

00:17:08,900 --> 00:17:14,810
the window and the accuracy and if you

00:17:12,490 --> 00:17:17,690
window over a number of events for

00:17:14,810 --> 00:17:19,370
example then if there's a spike in

00:17:17,690 --> 00:17:22,160
events your effective time window will

00:17:19,370 --> 00:17:25,870
shrink or if you window over processing

00:17:22,160 --> 00:17:29,360
time you lose the nice

00:17:25,870 --> 00:17:30,950
property of reproducibility if you ever

00:17:29,360 --> 00:17:33,200
try to replay again you will get a

00:17:30,950 --> 00:17:35,000
different result which breaks these

00:17:33,200 --> 00:17:37,510
functional principles that I happen to

00:17:35,000 --> 00:17:37,510
like a lot

00:17:37,720 --> 00:17:44,239
so lately we've seen support for joining

00:17:41,600 --> 00:17:45,889
or for windowing over event time instead

00:17:44,239 --> 00:17:48,220
which is usually what you want from a

00:17:45,889 --> 00:17:50,929
business logic perspective

00:17:48,220 --> 00:17:53,360
unfortunately you no longer know how

00:17:50,929 --> 00:17:55,850
much resources you need if you have a

00:17:53,360 --> 00:17:58,669
spike then your resource your memory

00:17:55,850 --> 00:18:01,039
consumption will advise and if you have

00:17:58,669 --> 00:18:04,429
all the events times like some bad clock

00:18:01,039 --> 00:18:06,080
stating that it is now next year then

00:18:04,429 --> 00:18:07,639
you've certainly flush all of your

00:18:06,080 --> 00:18:09,080
windows and not do what you want so you

00:18:07,639 --> 00:18:09,940
have to do some time to check in and so

00:18:09,080 --> 00:18:13,159
forth

00:18:09,940 --> 00:18:14,749
so no matter what you do you have some

00:18:13,159 --> 00:18:17,960
interesting trade-offs and this is not

00:18:14,749 --> 00:18:19,639
so easy if anybody still thinks that

00:18:17,960 --> 00:18:22,279
this might be easy I suggest you watch

00:18:19,639 --> 00:18:25,340
hawk anomalous talk from from last year

00:18:22,279 --> 00:18:27,200
which could have been titled it is

00:18:25,340 --> 00:18:28,820
surprisingly difficult to join two

00:18:27,200 --> 00:18:33,769
streams but it wasn't it had another

00:18:28,820 --> 00:18:38,179
title lots of interesting learnings so

00:18:33,769 --> 00:18:40,369
let's try and see if we can sort of do

00:18:38,179 --> 00:18:41,749
better with batch instead and and see

00:18:40,369 --> 00:18:48,590
how many problems we can avoid in the

00:18:41,749 --> 00:18:51,019
batch world we take the events in we put

00:18:48,590 --> 00:18:54,409
them into like our buckets for example

00:18:51,019 --> 00:18:56,869
or daily buckets so we need to again we

00:18:54,409 --> 00:18:58,249
need to decide to window over what

00:18:56,869 --> 00:19:01,970
because these these buckets are also

00:18:58,249 --> 00:19:05,359
windows and also we need to decide when

00:19:01,970 --> 00:19:07,159
we when these windows these buckets are

00:19:05,359 --> 00:19:12,109
complete when can we start the

00:19:07,159 --> 00:19:14,659
processing downstream so this is a data

00:19:12,109 --> 00:19:18,230
collection system I was working with

00:19:14,659 --> 00:19:20,450
from many years ago we had logs going

00:19:18,230 --> 00:19:23,509
events going down of files in the

00:19:20,450 --> 00:19:25,580
different services and we partitioned in

00:19:23,509 --> 00:19:27,799
my hour and we copied them with with or

00:19:25,580 --> 00:19:30,739
sync or SSH this even predates the

00:19:27,799 --> 00:19:34,399
existence of Kafka hence the ancient

00:19:30,739 --> 00:19:36,619
tools and then we put them over there in

00:19:34,399 --> 00:19:38,809
hourly buckets and then we decided to

00:19:36,619 --> 00:19:43,159
start the processing when all of the

00:19:38,809 --> 00:19:45,679
hosts had reported their data now as the

00:19:43,159 --> 00:19:47,509
number of hosts grew this got more and

00:19:45,679 --> 00:19:49,430
more fragile and we were processing

00:19:47,509 --> 00:19:53,960
later and later later

00:19:49,430 --> 00:19:57,440
not scalable and we were trying to add

00:19:53,960 --> 00:19:58,970
add on various hacks with looking at the

00:19:57,440 --> 00:20:00,950
monitoring system to figure which shows

00:19:58,970 --> 00:20:02,630
were up or down and so forth and just

00:20:00,950 --> 00:20:04,250
get more and more and more complex and

00:20:02,630 --> 00:20:06,530
we all like the feeling this is not

00:20:04,250 --> 00:20:08,690
supposed to be this complex and it isn't

00:20:06,530 --> 00:20:11,090
but we didn't figure that out at the

00:20:08,690 --> 00:20:13,520
time so another Swedish company had

00:20:11,090 --> 00:20:15,470
another trick they they optimistically

00:20:13,520 --> 00:20:17,840
started the batch processing downstream

00:20:15,470 --> 00:20:20,480
when the hours through and then they

00:20:17,840 --> 00:20:22,190
measured how much new data has arrived

00:20:20,480 --> 00:20:25,370
and I mean if you went over a certain

00:20:22,190 --> 00:20:29,600
threshold they triggered a reprocessing

00:20:25,370 --> 00:20:32,380
downstream which is also not great

00:20:29,600 --> 00:20:35,330
because then you have a data collection

00:20:32,380 --> 00:20:37,730
determined or decided for every use case

00:20:35,330 --> 00:20:40,700
downstream what the quality requirements

00:20:37,730 --> 00:20:43,190
were which worked for one or two use

00:20:40,700 --> 00:20:46,550
cases but it didn't work in the end for

00:20:43,190 --> 00:20:49,480
for many use cases there are tools and

00:20:46,550 --> 00:20:51,860
structures where you can do reprocessing

00:20:49,480 --> 00:20:53,660
googily is is doing something along

00:20:51,860 --> 00:20:56,360
these lines but they have very good

00:20:53,660 --> 00:20:58,520
internal tools we they've exposed some

00:20:56,360 --> 00:21:00,950
of that tooling in turn in terms of a

00:20:58,520 --> 00:21:04,390
patchy beam where you can say hey now I

00:21:00,950 --> 00:21:07,670
now have an updated data set so forth

00:21:04,390 --> 00:21:10,160
the I have so far

00:21:07,670 --> 00:21:14,020
not used or deselecting those tools

00:21:10,160 --> 00:21:17,090
because it turns your batch processing

00:21:14,020 --> 00:21:19,040
operationally into a stream and batch is

00:21:17,090 --> 00:21:20,660
much easier from relational perspectives

00:21:19,040 --> 00:21:21,920
I try to try to stay with the batches

00:21:20,660 --> 00:21:23,930
master much as I can

00:21:21,920 --> 00:21:26,060
they're also tools in terms of our

00:21:23,930 --> 00:21:27,980
versioning you you generate new versions

00:21:26,060 --> 00:21:30,110
of datasets downstream and so forth or

00:21:27,980 --> 00:21:31,820
or in provenance keeping really good

00:21:30,110 --> 00:21:34,880
track of what was used for what and so

00:21:31,820 --> 00:21:36,980
forth I again try to avoid those tools I

00:21:34,880 --> 00:21:38,570
because I feel that the with the tooling

00:21:36,980 --> 00:21:41,030
that exists today the the cure is worse

00:21:38,570 --> 00:21:45,200
than worse than the disease and I think

00:21:41,030 --> 00:21:49,640
there are simpler patterns so here's

00:21:45,200 --> 00:21:51,350
what I try to do these days we have some

00:21:49,640 --> 00:21:54,440
different times scopes that we can

00:21:51,350 --> 00:21:57,790
choose to bucket from and just choose

00:21:54,440 --> 00:22:02,120
the closest one the ingestion time

00:21:57,790 --> 00:22:03,010
because if you do that it's a local

00:22:02,120 --> 00:22:06,460
clock in the

00:22:03,010 --> 00:22:09,220
cool system closest to the data lake and

00:22:06,460 --> 00:22:11,890
that means that you can close and seal

00:22:09,220 --> 00:22:13,960
the datasets very quickly after the hour

00:22:11,890 --> 00:22:16,000
is through so you can start processing

00:22:13,960 --> 00:22:17,440
at that point so you don't have these

00:22:16,000 --> 00:22:20,640
situations where we're waiting for more

00:22:17,440 --> 00:22:20,640
data done before processing

00:22:20,940 --> 00:22:26,140
unfortunately if you're doing it

00:22:23,530 --> 00:22:30,820
analytics it leads to graphs like

00:22:26,140 --> 00:22:34,300
therefore one and then per use case you

00:22:30,820 --> 00:22:37,960
decide if that dip comes what do you

00:22:34,300 --> 00:22:39,460
want do you want to see the dip with the

00:22:37,960 --> 00:22:42,040
data that you have or do you want to

00:22:39,460 --> 00:22:43,270
wait until you have better data you can

00:22:42,040 --> 00:22:46,720
decide whether you have better or not

00:22:43,270 --> 00:22:49,480
based on the event time this should be a

00:22:46,720 --> 00:22:50,740
decision per use case and in the inner

00:22:49,480 --> 00:22:52,600
dashboard where you might want both but

00:22:50,740 --> 00:22:56,050
but for machinery downstream we have to

00:22:52,600 --> 00:22:57,790
make a choice and this is what it might

00:22:56,050 --> 00:23:01,450
look like Luigi code to the left bar

00:22:57,790 --> 00:23:04,390
code on the right we if we want to wait

00:23:01,450 --> 00:23:08,490
and get better data we delay the

00:23:04,390 --> 00:23:12,040
processing with n number of hours and

00:23:08,490 --> 00:23:14,530
then in the in the spot code we just we

00:23:12,040 --> 00:23:16,630
just take all of the wide window shuffle

00:23:14,530 --> 00:23:18,490
and spit out the the hour that were

00:23:16,630 --> 00:23:20,410
actually interested but since we waited

00:23:18,490 --> 00:23:24,550
for a while we have some more better

00:23:20,410 --> 00:23:27,730
data what if we're wrong we might be

00:23:24,550 --> 00:23:31,450
wrong and that's ok as long as we know

00:23:27,730 --> 00:23:35,080
how wrong we are so we count whenever

00:23:31,450 --> 00:23:38,350
our assumptions fail then and the data

00:23:35,080 --> 00:23:40,210
goes at the event time falls out of the

00:23:38,350 --> 00:23:41,980
window that we're looking at we count

00:23:40,210 --> 00:23:44,230
those as cameras so after the fact if we

00:23:41,980 --> 00:23:46,270
have a report we might know whether it

00:23:44,230 --> 00:23:54,340
was a good report or not and if it

00:23:46,270 --> 00:23:56,470
wasn't we can increase the delay and we

00:23:54,340 --> 00:23:58,480
compute different delay hours for to

00:23:56,470 --> 00:24:00,070
cater for all the use cases that we have

00:23:58,480 --> 00:24:03,010
so we usually do a fast one without

00:24:00,070 --> 00:24:05,590
delay delay our say equals zero and then

00:24:03,010 --> 00:24:08,620
some more and further for the really for

00:24:05,590 --> 00:24:11,920
the cases where quality is really

00:24:08,620 --> 00:24:15,480
important like financial reporting and

00:24:11,920 --> 00:24:15,480
so forth we wait for really long time

00:24:17,780 --> 00:24:22,680
so then we have some kind of land

00:24:20,550 --> 00:24:24,480
architecture in batch usually the lambo

00:24:22,680 --> 00:24:26,280
architecture is is you have you assumed

00:24:24,480 --> 00:24:28,230
that your streaming is unreliable so you

00:24:26,280 --> 00:24:30,210
redo the same thing about here we assume

00:24:28,230 --> 00:24:31,770
that the first batch processing is

00:24:30,210 --> 00:24:34,620
unreliable because it doesn't have all

00:24:31,770 --> 00:24:37,590
the data so we add another one or two or

00:24:34,620 --> 00:24:39,750
more pipelines on the side

00:24:37,590 --> 00:24:41,610
this might seem complex it's actually

00:24:39,750 --> 00:24:44,400
not because you can essentially use all

00:24:41,610 --> 00:24:46,110
the code it just at which parameter I've

00:24:44,400 --> 00:24:49,050
been in cases where we are dealt with

00:24:46,110 --> 00:24:50,970
like financial reporting and also some

00:24:49,050 --> 00:24:53,670
new data sets come in afterwards like

00:24:50,970 --> 00:24:55,590
that like the fraudulent users or

00:24:53,670 --> 00:24:58,710
something that we want to include in the

00:24:55,590 --> 00:25:05,820
high quality reporting so this and

00:24:58,710 --> 00:25:08,190
that's shown in this example here so

00:25:05,820 --> 00:25:11,550
those fraudulent users are examples of

00:25:08,190 --> 00:25:15,960
data that is delayed not by machinery

00:25:11,550 --> 00:25:17,790
but by humans claims is also a form of

00:25:15,960 --> 00:25:20,910
data that is delayed by humans the

00:25:17,790 --> 00:25:22,440
humans on inside your company or outside

00:25:20,910 --> 00:25:24,420
your company have not yet figured out

00:25:22,440 --> 00:25:26,760
what they are supposed to claim like you

00:25:24,420 --> 00:25:29,250
compensate economical compensation or

00:25:26,760 --> 00:25:31,140
whatever and this might convert late

00:25:29,250 --> 00:25:33,480
I've had I've had scenarios where

00:25:31,140 --> 00:25:36,300
according to the contrast contracts

00:25:33,480 --> 00:25:37,770
external parties might two years after

00:25:36,300 --> 00:25:40,470
the fact to come and claim things and we

00:25:37,770 --> 00:25:42,690
have to deal with that so you can't wait

00:25:40,470 --> 00:25:44,580
two years and until you spit out reports

00:25:42,690 --> 00:25:46,380
so you want preliminary reports but at

00:25:44,580 --> 00:25:50,310
the end after all claims you want

00:25:46,380 --> 00:25:54,090
accurate reports and as I've explained

00:25:50,310 --> 00:25:57,990
already reprocessing no good instead

00:25:54,090 --> 00:26:04,160
what you do is to explicitly model the

00:25:57,990 --> 00:26:09,170
time domain or the domain time and the

00:26:04,160 --> 00:26:12,540
registration or arrival time so for each

00:26:09,170 --> 00:26:16,880
domain time scope in the past like like

00:26:12,540 --> 00:26:18,510
the March this year you have each day

00:26:16,880 --> 00:26:21,120
build-a-report

00:26:18,510 --> 00:26:23,520
data set that states what you knew about

00:26:21,120 --> 00:26:25,920
that window and time on this particular

00:26:23,520 --> 00:26:27,929
day and then tomorrow we do another

00:26:25,920 --> 00:26:30,990
computation if new things have arrived

00:26:27,929 --> 00:26:33,600
about what we knew at what we knew on

00:26:30,990 --> 00:26:34,409
this day about that time that this turns

00:26:33,600 --> 00:26:37,080
out to make the whole thing

00:26:34,409 --> 00:26:39,330
deterministic and explicit and it also

00:26:37,080 --> 00:26:41,909
allows us to go back and all it things

00:26:39,330 --> 00:26:43,950
afterwards the dependency tree might

00:26:41,909 --> 00:26:45,450
blow up in these cases so so your your

00:26:43,950 --> 00:26:47,190
workflow Orchestrator might have a bit

00:26:45,450 --> 00:26:49,259
difficult time then there are you can do

00:26:47,190 --> 00:26:54,960
hacks to make the the tree spar since

00:26:49,259 --> 00:26:58,289
then and so forth in some cases it turns

00:26:54,960 --> 00:27:01,590
out to be convenient to depend on

00:26:58,289 --> 00:27:03,269
yesterday's data set we had this is also

00:27:01,590 --> 00:27:05,820
one of recent examples where we were

00:27:03,269 --> 00:27:07,499
looking at stocking in a store and we

00:27:05,820 --> 00:27:09,749
are getting updates with you know new

00:27:07,499 --> 00:27:12,299
stocks but only for a portion of the

00:27:09,749 --> 00:27:13,499
stock so we aggregate by just taking the

00:27:12,299 --> 00:27:17,659
latest information for a particular

00:27:13,499 --> 00:27:20,070
stock item if you depend on on

00:27:17,659 --> 00:27:23,429
yesterday's data then you have a

00:27:20,070 --> 00:27:25,769
recursive infinite dependency to to some

00:27:23,429 --> 00:27:28,799
kind of starting point this is usually

00:27:25,769 --> 00:27:30,720
okay but it represents an operational

00:27:28,799 --> 00:27:32,549
risk if you decide at some point that

00:27:30,720 --> 00:27:34,470
you need these will these calculations

00:27:32,549 --> 00:27:37,110
wrong you need to go and backfill then

00:27:34,470 --> 00:27:38,970
you need to go back all the time this

00:27:37,110 --> 00:27:40,769
has often you can get away with this

00:27:38,970 --> 00:27:43,139
I've had number of occasions where this

00:27:40,769 --> 00:27:45,749
was actually a practical problem and

00:27:43,139 --> 00:27:49,590
cost days of delay for for financial

00:27:45,749 --> 00:27:52,980
reporting and so forth so there is a

00:27:49,590 --> 00:27:57,139
mitigation here that I used at some

00:27:52,980 --> 00:28:01,889
point which is to do the recursion in

00:27:57,139 --> 00:28:05,519
jumps so for every first of the month

00:28:01,889 --> 00:28:07,740
I wrote to a job that depended on the

00:28:05,519 --> 00:28:10,230
first of the last last month and then

00:28:07,740 --> 00:28:13,259
everything in between and then so forth

00:28:10,230 --> 00:28:14,999
which made you stride quicker forwards

00:28:13,259 --> 00:28:16,470
in time and then for the other day so

00:28:14,999 --> 00:28:18,330
the managers just depend on the first

00:28:16,470 --> 00:28:19,710
and whatever was incremental so it's

00:28:18,330 --> 00:28:21,899
kind of like MPEG encoding where you're

00:28:19,710 --> 00:28:25,019
a liar frames and P frames and things

00:28:21,899 --> 00:28:31,529
and this cuts down the the operational

00:28:25,019 --> 00:28:34,409
risk here in some cases your business

00:28:31,529 --> 00:28:38,039
logic the texts that you need to look

00:28:34,409 --> 00:28:39,600
back in time in order to figure out what

00:28:38,039 --> 00:28:41,400
the output should be and the simplest

00:28:39,600 --> 00:28:43,080
example is

00:28:41,400 --> 00:28:46,530
where you had users to click on your

00:28:43,080 --> 00:28:51,030
webpage or you use a mobile phone and so

00:28:46,530 --> 00:28:53,370
forth and you need to you want to plump

00:28:51,030 --> 00:28:57,150
lump that into sessions with of a

00:28:53,370 --> 00:28:59,310
certain length and so forth this is an

00:28:57,150 --> 00:29:00,270
example I taken I hold the course of

00:28:59,310 --> 00:29:02,370
several months and while this is an

00:29:00,270 --> 00:29:05,100
example I take in order to make people

00:29:02,370 --> 00:29:07,890
think about dependencies so let's say

00:29:05,100 --> 00:29:11,970
let's say there you have a different

00:29:07,890 --> 00:29:13,710
definition of sessions so which hours of

00:29:11,970 --> 00:29:15,300
data do you need in order to figure out

00:29:13,710 --> 00:29:17,240
the sessions here well if for this

00:29:15,300 --> 00:29:19,290
definition you need infinite history

00:29:17,240 --> 00:29:21,150
because the sessions might be very long

00:29:19,290 --> 00:29:24,480
well long sessions might not make sense

00:29:21,150 --> 00:29:25,920
they're bots anyway so you might cut off

00:29:24,480 --> 00:29:27,960
here and say that no more than three

00:29:25,920 --> 00:29:30,060
hours good excellent excellent now you

00:29:27,960 --> 00:29:32,460
you might think that you need only four

00:29:30,060 --> 00:29:33,480
hours or data and the sessions will look

00:29:32,460 --> 00:29:35,310
something like this

00:29:33,480 --> 00:29:38,940
it turns out if you go to the border

00:29:35,310 --> 00:29:41,910
here you have a session in the beginning

00:29:38,940 --> 00:29:43,800
but you nevertheless don't know well

00:29:41,910 --> 00:29:46,380
that session started at that point or

00:29:43,800 --> 00:29:48,570
not so depending on the history back you

00:29:46,380 --> 00:29:50,310
you might have a cutoff date or not and

00:29:48,570 --> 00:29:53,420
so forth it might be a contrived example

00:29:50,310 --> 00:29:57,090
but it illustrates that it is often

00:29:53,420 --> 00:29:59,250
difficult to figure out how much data

00:29:57,090 --> 00:30:01,200
are you actually what time window you

00:29:59,250 --> 00:30:03,210
actually need in order to do a

00:30:01,200 --> 00:30:04,830
particular job and this graph that we

00:30:03,210 --> 00:30:07,350
saw where there was like a ten percent

00:30:04,830 --> 00:30:09,510
dip at midnight was a failure to realize

00:30:07,350 --> 00:30:13,320
that we needed more data than we

00:30:09,510 --> 00:30:15,060
actually thought we needed what to do

00:30:13,320 --> 00:30:17,010
about it where you can do the recursive

00:30:15,060 --> 00:30:19,290
strides as mentioned previously and just

00:30:17,010 --> 00:30:21,030
take all of the data and but express it

00:30:19,290 --> 00:30:23,460
in a different way or you can introduce

00:30:21,030 --> 00:30:24,780
like limits we're going to cut all the

00:30:23,460 --> 00:30:27,360
sessions at midnight to something that

00:30:24,780 --> 00:30:31,110
align you to your artificial batch

00:30:27,360 --> 00:30:33,330
boundaries and then you are losing some

00:30:31,110 --> 00:30:34,920
information but you are making it more

00:30:33,330 --> 00:30:41,430
practical and less realistic it's

00:30:34,920 --> 00:30:43,230
passing a use case why do I care about

00:30:41,430 --> 00:30:46,620
so much about these properties of repres

00:30:43,230 --> 00:30:50,910
ability and functional properties like

00:30:46,620 --> 00:30:53,970
immutability and so forth these are some

00:30:50,910 --> 00:30:55,230
aspects that I have found that not so

00:30:53,970 --> 00:30:59,490
many people follow

00:30:55,230 --> 00:31:01,529
but turn out to be really to really make

00:30:59,490 --> 00:31:04,230
a difference in whether you get value

00:31:01,529 --> 00:31:07,429
from your data engineering or not I've

00:31:04,230 --> 00:31:11,880
learned to gravitate heavily towards

00:31:07,429 --> 00:31:15,149
simplicity and towards slow data so I

00:31:11,880 --> 00:31:18,899
always use batch processing if I can get

00:31:15,149 --> 00:31:22,139
away with it if the use cases can handle

00:31:18,899 --> 00:31:23,880
it or have the latency because the cost

00:31:22,139 --> 00:31:26,039
of operations is much smaller and the

00:31:23,880 --> 00:31:29,940
smaller cost of operations mean that you

00:31:26,039 --> 00:31:32,549
can innovate faster move faster well in

00:31:29,940 --> 00:31:35,000
this presentations I've focused on

00:31:32,549 --> 00:31:37,529
techniques for defending these

00:31:35,000 --> 00:31:39,059
functional architecture principles and

00:31:37,529 --> 00:31:40,529
I've solved some of the problems that

00:31:39,059 --> 00:31:44,519
are bumped up with workflow

00:31:40,529 --> 00:31:48,419
orchestration why are these principles

00:31:44,519 --> 00:31:51,090
so important because they support high

00:31:48,419 --> 00:31:54,090
team concurrency we have learned that

00:31:51,090 --> 00:31:58,049
immutable data and work and expressing

00:31:54,090 --> 00:32:00,389
things in in functional pipelines is

00:31:58,049 --> 00:32:02,580
good for thread concurrency and for

00:32:00,389 --> 00:32:04,789
computational concurrency it turns out

00:32:02,580 --> 00:32:07,529
that immutable data sets and data

00:32:04,789 --> 00:32:10,039
pipelines as the means of collaboration

00:32:07,529 --> 00:32:12,299
allows for high team concurrency

00:32:10,039 --> 00:32:15,600
different people can work with the data

00:32:12,299 --> 00:32:17,730
without applying organized operational

00:32:15,600 --> 00:32:20,159
risk on the teams that own the data

00:32:17,730 --> 00:32:22,590
because the data is immutable and so

00:32:20,159 --> 00:32:24,570
forth and reproduce ability it cuts down

00:32:22,590 --> 00:32:26,760
your operational risk because if you

00:32:24,570 --> 00:32:28,649
need to rerun things you know that you

00:32:26,760 --> 00:32:30,059
get the same thing so you can remove

00:32:28,649 --> 00:32:37,080
things and rerun them as soon as you

00:32:30,059 --> 00:32:39,269
want and there's some sometimes talk

00:32:37,080 --> 00:32:45,950
about the reproducibility crisis and so

00:32:39,269 --> 00:32:49,830
forth and this this is this crisis is

00:32:45,950 --> 00:32:54,230
aggregated by or increased by failing to

00:32:49,830 --> 00:32:58,490
comply with these principles in order to

00:32:54,230 --> 00:33:01,470
do machine learning on a repeatable in

00:32:58,490 --> 00:33:03,120
order to get lots of value from machine

00:33:01,470 --> 00:33:05,639
learning in your products you need to be

00:33:03,120 --> 00:33:07,230
able to run many experiments and if you

00:33:05,639 --> 00:33:08,640
run experiments and they are not

00:33:07,230 --> 00:33:10,530
reproducible they

00:33:08,640 --> 00:33:11,940
know whether your change made a

00:33:10,530 --> 00:33:14,100
difference or whether it was different

00:33:11,940 --> 00:33:16,320
data or just some some of the face of

00:33:14,100 --> 00:33:21,300
the moon or something that affected the

00:33:16,320 --> 00:33:25,800
results and the democratization of data

00:33:21,300 --> 00:33:27,870
and a ability to collaborate is the real

00:33:25,800 --> 00:33:29,670
value of big data it's not the size of

00:33:27,870 --> 00:33:32,010
the data it's not the machine learning

00:33:29,670 --> 00:33:33,720
and shining things this is what in all

00:33:32,010 --> 00:33:36,180
of the organizations that I've seen is

00:33:33,720 --> 00:33:38,400
what make the big difference the that

00:33:36,180 --> 00:33:40,650
you're breaking down silos you don't

00:33:38,400 --> 00:33:42,750
have to coordinate with lots of people

00:33:40,650 --> 00:33:45,390
because the data is readily available

00:33:42,750 --> 00:33:51,000
and that's why I soo-min focus on these

00:33:45,390 --> 00:33:54,120
things some credits some good timing

00:33:51,000 --> 00:33:58,110
libraries that I usually use and if you

00:33:54,120 --> 00:33:59,700
are having questions about the the batch

00:33:58,110 --> 00:34:00,960
versus streaming and the operational

00:33:59,700 --> 00:34:02,520
trade-offs and so on I have a

00:34:00,960 --> 00:34:06,690
presentation about that that you might

00:34:02,520 --> 00:34:09,770
wanna watch it seems that we do have a

00:34:06,690 --> 00:34:09,770
few minutes for questions

00:34:10,600 --> 00:34:16,030
[Applause]

00:34:19,880 --> 00:34:27,170
who has got some questions please put

00:34:25,170 --> 00:34:32,240
your hand up as long as I cannot see you

00:34:27,170 --> 00:34:32,240
no come on I was really exciting talk so

00:34:35,420 --> 00:34:42,360
yes how do you deal with them like when

00:34:40,680 --> 00:34:46,410
the members of a team wants to do

00:34:42,360 --> 00:34:48,870
screaming because it's cool hands oh I'm

00:34:46,410 --> 00:34:51,780
very much down-to-earth and I have I

00:34:48,870 --> 00:34:54,180
used to love complexity right but I

00:34:51,780 --> 00:34:55,650
started in big date with big negative

00:34:54,180 --> 00:35:00,540
stuff fairly early and now I hate the

00:34:55,650 --> 00:35:03,840
complexity and some of course some in

00:35:00,540 --> 00:35:05,310
there are cases where we're streaming is

00:35:03,840 --> 00:35:08,100
the right trade-off right you need the

00:35:05,310 --> 00:35:09,570
low latency and that's fine and also you

00:35:08,100 --> 00:35:11,300
usually have some kind of streaming in

00:35:09,570 --> 00:35:18,720
your in your data collection pipeline

00:35:11,300 --> 00:35:22,110
okay so forth I find that you need to

00:35:18,720 --> 00:35:28,140
have a cultural balance and sufficient

00:35:22,110 --> 00:35:30,890
focus on value delivery and product

00:35:28,140 --> 00:35:33,090
ownership and business value focus

00:35:30,890 --> 00:35:36,090
represented in the teams if you throw a

00:35:33,090 --> 00:35:37,350
bunch of engineers a way to do something

00:35:36,090 --> 00:35:40,080
they're going to do really cool things

00:35:37,350 --> 00:35:43,740
so the successful teams that I've worked

00:35:40,080 --> 00:35:46,530
with have had a balance of engineering

00:35:43,740 --> 00:35:47,880
domain expertise and product ownership

00:35:46,530 --> 00:35:51,720
and in the case of some machine learning

00:35:47,880 --> 00:35:54,180
a data scientist I've never been in a

00:35:51,720 --> 00:36:00,720
successful team that delivers feature

00:35:54,180 --> 00:36:04,490
that was not cross-functional anybody

00:36:00,720 --> 00:36:04,490
else yes

00:36:06,540 --> 00:36:12,210
you talk about workflow orchestration

00:36:09,180 --> 00:36:15,210
but I mean it's still quite problematic

00:36:12,210 --> 00:36:17,100
to find a scheduler that does all the

00:36:15,210 --> 00:36:19,290
workflow orchestration for you do you

00:36:17,100 --> 00:36:22,410
have any recommendations there are two

00:36:19,290 --> 00:36:24,180
reasonable options there is the examples

00:36:22,410 --> 00:36:26,310
that I showed here it is Luigi from

00:36:24,180 --> 00:36:30,320
Spotify the other reasonable option is

00:36:26,310 --> 00:36:35,100
r4 from RB me I've not seen any other

00:36:30,320 --> 00:36:37,230
reasonable options those are both

00:36:35,100 --> 00:36:39,720
implemented in Python you need a real

00:36:37,230 --> 00:36:41,520
language in order to express the things

00:36:39,720 --> 00:36:43,380
that are non-trivial so many of the

00:36:41,520 --> 00:36:45,030
examples that I put up here if you go to

00:36:43,380 --> 00:36:49,800
something like OC you cannot express

00:36:45,030 --> 00:36:52,109
them which one should you take well I am

00:36:49,800 --> 00:36:58,950
biased I used Luigi since long before

00:36:52,109 --> 00:37:01,619
our flow existed the it kind of depends

00:36:58,950 --> 00:37:06,060
on whether you are a buy things or build

00:37:01,619 --> 00:37:08,850
things organization airflow is has a

00:37:06,060 --> 00:37:11,130
wider scope it gives you some monitoring

00:37:08,850 --> 00:37:14,250
it gives you the the scheduling

00:37:11,130 --> 00:37:16,430
mechanism and so forth Luigi has a

00:37:14,250 --> 00:37:21,390
narrow scope it's a UNIX philosophy tool

00:37:16,430 --> 00:37:23,040
do think one thing well but it also it's

00:37:21,390 --> 00:37:28,410
more versatile in sense that it allows

00:37:23,040 --> 00:37:29,640
you to express more complex things so if

00:37:28,410 --> 00:37:31,440
if you were good at our flow you

00:37:29,640 --> 00:37:32,700
probably able to express some of the

00:37:31,440 --> 00:37:34,770
things I showed you but some of the

00:37:32,700 --> 00:37:36,420
things would be difficult I am are doing

00:37:34,770 --> 00:37:41,340
user flow son expert so those are your

00:37:36,420 --> 00:37:44,670
two options and thanks anybody for the

00:37:41,340 --> 00:37:45,740
last question No perfect thank you very

00:37:44,670 --> 00:37:50,540
much miss amazing

00:37:45,740 --> 00:37:50,540

YouTube URL: https://www.youtube.com/watch?v=1spKXX2W7Eo


