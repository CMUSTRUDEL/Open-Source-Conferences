Title: #Haystack: Trey Grainger - Thought Vectors, Knowledge Graphs, and Curious Death(?) of Keyword Search
Publication date: 2020-07-02
Playlist: Haystack - Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/thought-vectors-knowledge-graphs-and-curious-death-keyword-search

The world of information retrieval is changing. BERT, Elmo, and the Sesame Street gang are moving in, shouting the gospel of "thought vectors" as a replacement for traditional keyword search. Meanwhile many search teams are now automatically extracting graph representations of the world, trying their best to also provide more structured answers in the search experience. Poor old keyword search seems so outdated by comparison - is it dead, dying, or simply misunderstood? Contrary to popular belief, embeddings and thought vectors only solve a small subset of search problems, and each of these three tools (keyword search, thought vectors, and knowledge graphs) actually serves a critical role in building the next generation of search experiences. In this talk, we'll define and highlight the strengths and weaknesses of each of these search methodologies, discuss the role each should play in a modern search solution, and demonstrate where each fails to get the job done and also how they can best complement each other to optimize relevance. We'll walk through interactive, open source demos showing each of these three types of search in action, demonstrating how to balance the strengths, weaknesses and tradeoffs between them for different user intents and query types.
Captions: 
	00:00:07,590 --> 00:00:11,760
hi everybody really glad to be here

00:00:10,290 --> 00:00:14,510
thanks for the great introduction

00:00:11,760 --> 00:00:17,040
Charlie I'm going to talk today about

00:00:14,510 --> 00:00:19,280
thought vectors knowledge graphs and the

00:00:17,040 --> 00:00:22,260
curious death of keyword search

00:00:19,280 --> 00:00:25,350
so as Charlie mentioned I am the founder

00:00:22,260 --> 00:00:26,660
of search kernel a company helping a

00:00:25,350 --> 00:00:30,780
consultancy helping other companies

00:00:26,660 --> 00:00:32,640
build next-generation search formerly

00:00:30,780 --> 00:00:35,160
chief evidence officer lucid works

00:00:32,640 --> 00:00:37,739
author of a I powered search which is a

00:00:35,160 --> 00:00:42,050
work in progress and an advisor to pre

00:00:37,739 --> 00:00:44,910
search the decentralized search engine

00:00:42,050 --> 00:00:47,370
very briefly search kernel we just

00:00:44,910 --> 00:00:49,350
mentioned it you know if you're

00:00:47,370 --> 00:00:50,460
interested in help with some of the

00:00:49,350 --> 00:00:52,350
techniques that I'm going to talk about

00:00:50,460 --> 00:00:55,440
today would be would love to chat with

00:00:52,350 --> 00:00:59,729
you pre search meant for many of you who

00:00:55,440 --> 00:01:02,280
are unfamiliar is a decentralized search

00:00:59,729 --> 00:01:04,170
engine think of it as ultimately a

00:01:02,280 --> 00:01:07,880
decentralized open source version of

00:01:04,170 --> 00:01:11,400
Google that's a very audacious goal but

00:01:07,880 --> 00:01:12,750
where you know in fairly early stages of

00:01:11,400 --> 00:01:14,040
building out something that I think is

00:01:12,750 --> 00:01:18,180
going to be really important to the

00:01:14,040 --> 00:01:19,590
world so stay tuned for more on that and

00:01:18,180 --> 00:01:21,299
of course a ad powered search is my

00:01:19,590 --> 00:01:23,399
newest book that I am currently working

00:01:21,299 --> 00:01:25,380
on if you're interested in getting a

00:01:23,399 --> 00:01:28,229
copy there's a discount code here at the

00:01:25,380 --> 00:01:31,320
bottom they add powered 40 yet on

00:01:28,229 --> 00:01:34,950
manning's website so the agenda for

00:01:31,320 --> 00:01:35,939
today is pretty simple it's the title of

00:01:34,950 --> 00:01:37,170
the talk we're going to talk about

00:01:35,939 --> 00:01:38,520
thought vectors we're going to talk

00:01:37,170 --> 00:01:40,109
about knowledge graphs and then we're

00:01:38,520 --> 00:01:42,570
going to talk about keyword search and

00:01:40,109 --> 00:01:47,250
how thought vectors and knowledge graphs

00:01:42,570 --> 00:01:50,189
replace or augment keyword search so to

00:01:47,250 --> 00:01:52,229
get started you know before we started

00:01:50,189 --> 00:01:53,549
talking about thought vectors and what

00:01:52,229 --> 00:01:56,100
they really are which I'll get into in

00:01:53,549 --> 00:01:57,929
just a minute we had keyword search so

00:01:56,100 --> 00:02:00,600
this as many of you will be familiar

00:01:57,929 --> 00:02:04,590
with is an example of an inverted index

00:02:00,600 --> 00:02:06,659
where I have documents the documents

00:02:04,590 --> 00:02:08,670
have keywords in them so for in this

00:02:06,659 --> 00:02:10,380
example the word Apple the word donut

00:02:08,670 --> 00:02:13,500
and the word juice are all terms that

00:02:10,380 --> 00:02:18,540
appear within these documents and then

00:02:13,500 --> 00:02:21,480
we pre index a mapping of every possible

00:02:18,540 --> 00:02:22,920
key word into a postings list of

00:02:21,480 --> 00:02:24,659
documents that that keyword appears

00:02:22,920 --> 00:02:27,870
within so that when somebody searches

00:02:24,659 --> 00:02:30,090
for the you know the search apple juice

00:02:27,870 --> 00:02:31,799
we look up apple find all the documents

00:02:30,090 --> 00:02:34,170
that's in lookup juice find all the

00:02:31,799 --> 00:02:35,879
documents that contain juice and then do

00:02:34,170 --> 00:02:37,739
a set intersection and ultimately find

00:02:35,879 --> 00:02:40,620
all of the documents that contain the

00:02:37,739 --> 00:02:42,810
words Apple and juice from there you

00:02:40,620 --> 00:02:44,340
know this is the standard be m25 formula

00:02:42,810 --> 00:02:46,739
which I won't get into but we

00:02:44,340 --> 00:02:49,290
essentially take every document and we

00:02:46,739 --> 00:02:51,150
score it relative to those terms Apple

00:02:49,290 --> 00:02:53,280
and juice to try to figure out which

00:02:51,150 --> 00:02:55,950
documents are the most related to the

00:02:53,280 --> 00:02:58,129
grade that comes in this is traditional

00:02:55,950 --> 00:03:00,840
keyword search most folks watching this

00:02:58,129 --> 00:03:04,890
presentation are probably very familiar

00:03:00,840 --> 00:03:07,579
with with this technique of course you

00:03:04,890 --> 00:03:12,299
know in recent years there's been a

00:03:07,579 --> 00:03:14,129
massive effort to improve search and

00:03:12,299 --> 00:03:16,650
bertÃ© which many of you have probably

00:03:14,129 --> 00:03:17,760
heard about is Google's attempt that

00:03:16,650 --> 00:03:20,340
they've actually rolled out into their

00:03:17,760 --> 00:03:22,290
search engine to do so and so this is

00:03:20,340 --> 00:03:24,060
the article from when Burt was first

00:03:22,290 --> 00:03:26,430
released in Google search results and

00:03:24,060 --> 00:03:27,989
just to give you a sense the article

00:03:26,430 --> 00:03:30,060
says Google is making one of the biggest

00:03:27,989 --> 00:03:31,829
changes to its ranking algorithm as it

00:03:30,060 --> 00:03:33,630
gets artificial intelligence a deepening

00:03:31,829 --> 00:03:35,730
role in the world's most popular search

00:03:33,630 --> 00:03:37,500
engine the change which Google described

00:03:35,730 --> 00:03:39,840
as its most significant revision and at

00:03:37,500 --> 00:03:41,730
least five years uses a new form of

00:03:39,840 --> 00:03:44,880
language analysis to understand users

00:03:41,730 --> 00:03:46,590
queries better it is set to until now

00:03:44,880 --> 00:03:47,940
Google's algorithm has tried to single

00:03:46,590 --> 00:03:50,669
out the most important words in any

00:03:47,940 --> 00:03:53,010
search ignoring smaller or common words

00:03:50,669 --> 00:03:54,720
that seem less significant this enables

00:03:53,010 --> 00:03:56,459
that to zero in on the main subject but

00:03:54,720 --> 00:03:58,530
often results in it misunderstanding its

00:03:56,459 --> 00:04:00,930
precise request the new technique known

00:03:58,530 --> 00:04:03,810
as Bert relies on language model built

00:04:00,930 --> 00:04:05,910
up from analysis of vast amounts of text

00:04:03,810 --> 00:04:07,530
online rather than reading a string of

00:04:05,910 --> 00:04:09,419
searched words sequentially it analyzes

00:04:07,530 --> 00:04:10,769
them all at the same time including

00:04:09,419 --> 00:04:13,349
smaller words that would have been

00:04:10,769 --> 00:04:15,209
ignored before one example Google gave

00:04:13,349 --> 00:04:17,970
of the types of questions that could now

00:04:15,209 --> 00:04:20,760
handle was how old was Taylor Swift when

00:04:17,970 --> 00:04:22,260
Kanye jumped on the stage this points to

00:04:20,760 --> 00:04:24,389
more complex queries that have been

00:04:22,260 --> 00:04:26,130
beyond its reach before the update marks

00:04:24,389 --> 00:04:27,470
the first application of the piece of

00:04:26,130 --> 00:04:29,520
research on natural language processing

00:04:27,470 --> 00:04:32,700
from last year that has drawn

00:04:29,520 --> 00:04:34,200
considerable attention in AI circles and

00:04:32,700 --> 00:04:35,370
so I keep mentioning thought vectors

00:04:34,200 --> 00:04:36,660
what our thought vectors

00:04:35,370 --> 00:04:39,840
you've probably heard the word

00:04:36,660 --> 00:04:41,820
embeddings before but there's many kinds

00:04:39,840 --> 00:04:43,470
of embeddings so thought vectors and

00:04:41,820 --> 00:04:44,580
compass all of those kinds so for

00:04:43,470 --> 00:04:46,830
example you can have a word or phrase

00:04:44,580 --> 00:04:49,680
embedding that essentially means that we

00:04:46,830 --> 00:04:51,930
take the words or phrases that are you

00:04:49,680 --> 00:04:54,360
know within documents and we map them

00:04:51,930 --> 00:04:56,639
into numerical vectors representing the

00:04:54,360 --> 00:04:59,130
meaning of those words and phrases so we

00:04:56,639 --> 00:05:00,840
can have word or phrase embeddings but

00:04:59,130 --> 00:05:02,280
you can also do things like sentence

00:05:00,840 --> 00:05:04,410
embeddings where you take an entire

00:05:02,280 --> 00:05:06,060
sentence and try to map it into a vector

00:05:04,410 --> 00:05:07,800
representing the meaning of the sentence

00:05:06,060 --> 00:05:09,150
you can do the same thing with

00:05:07,800 --> 00:05:09,720
paragraphs to create paragraph

00:05:09,150 --> 00:05:13,169
embeddings

00:05:09,720 --> 00:05:14,760
a vector that you know maps to the

00:05:13,169 --> 00:05:16,590
meaning of an entire paragraph when

00:05:14,760 --> 00:05:18,810
taken as a whole and of course you can

00:05:16,590 --> 00:05:20,400
take an entire document and break up all

00:05:18,810 --> 00:05:21,990
of the pieces within the document and

00:05:20,400 --> 00:05:23,580
try to map it to a vector that

00:05:21,990 --> 00:05:26,880
represents the overall meaning of the

00:05:23,580 --> 00:05:28,979
document if you think of this in terms

00:05:26,880 --> 00:05:32,070
of the way we think of an inverted index

00:05:28,979 --> 00:05:34,050
historically you know where as you might

00:05:32,070 --> 00:05:35,550
have a query that comes in so for

00:05:34,050 --> 00:05:38,460
example on the left here these are query

00:05:35,550 --> 00:05:40,979
keywords you know Apple cheese juice etc

00:05:38,460 --> 00:05:43,410
and then you have an inverted index like

00:05:40,979 --> 00:05:45,419
we looked at before which has you know a

00:05:43,410 --> 00:05:47,160
bunch of individual words mapped to

00:05:45,419 --> 00:05:50,940
whether they exist within documents or

00:05:47,160 --> 00:05:53,970
not in this case you you could actually

00:05:50,940 --> 00:05:57,300
represent a query for example the query

00:05:53,970 --> 00:05:59,820
for Apple as a vector where the vector

00:05:57,300 --> 00:06:01,979
has a 1 in the position where Apple

00:05:59,820 --> 00:06:03,600
exists and a 0 in every other position

00:06:01,979 --> 00:06:06,539
and the length of this vector

00:06:03,600 --> 00:06:09,270
essentially has one element for every

00:06:06,539 --> 00:06:11,460
possible word that exists in any of your

00:06:09,270 --> 00:06:15,150
documents now this is known as one hot

00:06:11,460 --> 00:06:17,310
encoding what you can then do if you

00:06:15,150 --> 00:06:19,770
represent each query as a vector so

00:06:17,310 --> 00:06:21,600
Apple has you know one in the position

00:06:19,770 --> 00:06:23,820
for apple juice has a one in the

00:06:21,600 --> 00:06:25,740
position for juice for that feature then

00:06:23,820 --> 00:06:27,900
you can ultimately add those two vectors

00:06:25,740 --> 00:06:29,820
together to get the vector for apple

00:06:27,900 --> 00:06:31,590
juice or apple and juice both appear

00:06:29,820 --> 00:06:33,120
this is similar to the set intersection

00:06:31,590 --> 00:06:36,600
we saw a few minutes ago with the

00:06:33,120 --> 00:06:39,060
inverted index you can also do multi

00:06:36,600 --> 00:06:41,280
term searches using that technique so

00:06:39,060 --> 00:06:43,500
now Apple and juice has a 1 in both of

00:06:41,280 --> 00:06:45,900
these positions we can you know think of

00:06:43,500 --> 00:06:49,110
this vector as the query for apple juice

00:06:45,900 --> 00:06:51,600
I mean so far these look

00:06:49,110 --> 00:06:53,690
almost identical right we've got our

00:06:51,600 --> 00:06:56,780
inverted index here mapping terms to

00:06:53,690 --> 00:07:00,600
documents and then over here we've got

00:06:56,780 --> 00:07:01,860
you know queries and the same thing I

00:07:00,600 --> 00:07:03,300
happen to be up with that which is query

00:07:01,860 --> 00:07:04,590
and over here we've got queries and

00:07:03,300 --> 00:07:08,100
we're mapping those two vectors that

00:07:04,590 --> 00:07:10,200
represent you know where they exist but

00:07:08,100 --> 00:07:12,390
then the magic comes into play with

00:07:10,200 --> 00:07:13,860
thought vectors with these embeddings

00:07:12,390 --> 00:07:16,890
which is the idea of dimensionality

00:07:13,860 --> 00:07:20,190
reduction so whereas before we had one

00:07:16,890 --> 00:07:22,980
feature or you know dimension and this

00:07:20,190 --> 00:07:24,510
vector for every possible word what if

00:07:22,980 --> 00:07:26,850
we don't want to have that many

00:07:24,510 --> 00:07:29,340
dimensions we can apply dimensionality

00:07:26,850 --> 00:07:32,400
reduction so that in this case instead

00:07:29,340 --> 00:07:34,920
of having each query mapped to the

00:07:32,400 --> 00:07:37,560
specific words that it contains instead

00:07:34,920 --> 00:07:41,640
we have it mapped to a reduced set of

00:07:37,560 --> 00:07:43,050
features which describe in some way the

00:07:41,640 --> 00:07:44,790
things we're searching for so in this

00:07:43,050 --> 00:07:47,310
case I've created eight different

00:07:44,790 --> 00:07:51,000
categories or features one of them being

00:07:47,310 --> 00:07:52,470
whether the query over here represents

00:07:51,000 --> 00:07:54,540
the term over here represents food

00:07:52,470 --> 00:07:56,670
whether it represents you know something

00:07:54,540 --> 00:07:58,710
you can drink whether it contains dairy

00:07:56,670 --> 00:08:00,360
bread caffeine whether it's sweet

00:07:58,710 --> 00:08:02,580
whether it has calories or whether it's

00:08:00,360 --> 00:08:05,460
considered healthy so you can see in

00:08:02,580 --> 00:08:07,770
this case for example a doughnut is food

00:08:05,460 --> 00:08:10,110
it's not a drink contains a little bit

00:08:07,770 --> 00:08:11,430
of dairy contains a lot of bread it is

00:08:10,110 --> 00:08:14,130
very sweet

00:08:11,430 --> 00:08:16,430
it contains the the highest number of

00:08:14,130 --> 00:08:19,710
calories of everything on here and it's

00:08:16,430 --> 00:08:22,410
not healthy at all and you could you

00:08:19,710 --> 00:08:24,930
know see the rest map to a similar space

00:08:22,410 --> 00:08:28,260
in terms of how they map to these

00:08:24,930 --> 00:08:30,030
features so once we've done that and

00:08:28,260 --> 00:08:33,390
we've essentially represented all of our

00:08:30,030 --> 00:08:35,100
terms or phrases we create these vectors

00:08:33,390 --> 00:08:36,770
that we just saw on the last screen that

00:08:35,100 --> 00:08:39,300
represent each of those terms or phrases

00:08:36,770 --> 00:08:42,030
these are known as our embeddings or our

00:08:39,300 --> 00:08:44,730
thought vectors representing words and

00:08:42,030 --> 00:08:47,130
phrases in this case once we've done

00:08:44,730 --> 00:08:50,280
that we can take a similarity between

00:08:47,130 --> 00:08:51,720
any two of these vectors you know using

00:08:50,280 --> 00:08:53,910
something like the dot product or a

00:08:51,720 --> 00:08:55,920
cosine or you know Euclidean distance or

00:08:53,910 --> 00:08:58,440
any number of similarity measures and

00:08:55,920 --> 00:09:00,660
ultimately we generate similarity scores

00:08:58,440 --> 00:09:02,819
for each of these so for example if I

00:09:00,660 --> 00:09:05,729
search for cheese pizza and I

00:09:02,819 --> 00:09:09,149
grab the vector for cheese pizza and

00:09:05,729 --> 00:09:10,919
then I do a cosine similarity between

00:09:09,149 --> 00:09:12,749
that vector and all these other vectors

00:09:10,919 --> 00:09:13,769
I can get a ranked set of results

00:09:12,749 --> 00:09:15,869
showing me that

00:09:13,769 --> 00:09:17,489
for example cheese breadsticks are the

00:09:15,869 --> 00:09:19,439
most similar thing to cheese pizza

00:09:17,489 --> 00:09:21,809
cinnamon breadsticks are the most

00:09:19,439 --> 00:09:23,369
similar thing to or the second with

00:09:21,809 --> 00:09:26,999
similar thing to cheese pizza and so on

00:09:23,369 --> 00:09:29,069
with water being the least similar if I

00:09:26,999 --> 00:09:31,169
take green tea you'll see that water and

00:09:29,069 --> 00:09:32,609
cappuccino and latte and then apple

00:09:31,169 --> 00:09:34,949
juice are the most similar these all

00:09:32,609 --> 00:09:37,289
have to do with being drinks going from

00:09:34,949 --> 00:09:39,449
most healthy to least healthy and then

00:09:37,289 --> 00:09:41,970
donut is the thing that is the farthest

00:09:39,449 --> 00:09:46,319
away in terms of similarity similarity

00:09:41,970 --> 00:09:47,910
to green tea and so implementing this

00:09:46,319 --> 00:09:49,079
kind of vector search those were simple

00:09:47,910 --> 00:09:51,989
examples we'll do something more

00:09:49,079 --> 00:09:55,079
complicated in a minute requires dense

00:09:51,989 --> 00:09:58,289
vectors support in your search engine

00:09:55,079 --> 00:10:00,539
and so you know all of the open source

00:09:58,289 --> 00:10:02,369
or at least partially open source search

00:10:00,539 --> 00:10:05,369
engines have solar elastic search

00:10:02,369 --> 00:10:08,899
festival etc all contain some level of

00:10:05,369 --> 00:10:11,819
vector dense vector support I'm

00:10:08,899 --> 00:10:13,019
typically using solar so I'm going to

00:10:11,819 --> 00:10:16,709
show you some examples of how to do this

00:10:13,019 --> 00:10:18,899
with solar today and what one of the

00:10:16,709 --> 00:10:24,059
first ways is through solar streaming

00:10:18,899 --> 00:10:25,979
expressions capabilities as a neguin

00:10:24,059 --> 00:10:28,019
expressions I can pass in documents

00:10:25,979 --> 00:10:30,209
where you know these are the exact

00:10:28,019 --> 00:10:32,309
documents I was just looking at and I

00:10:30,209 --> 00:10:34,979
can pass in a vector I'm using a

00:10:32,309 --> 00:10:37,529
multivalued float field where I just

00:10:34,979 --> 00:10:39,989
pass in each of the dimensions as the

00:10:37,529 --> 00:10:43,319
the multiple values going in once I've

00:10:39,989 --> 00:10:45,389
done that I can use a streaming

00:10:43,319 --> 00:10:49,019
expression like this where I'm selecting

00:10:45,389 --> 00:10:53,269
the top apologies

00:10:49,019 --> 00:10:58,859
somebody just rang my doorbell I can

00:10:53,269 --> 00:11:02,189
search for the top from this query where

00:10:58,859 --> 00:11:05,249
I'm searching for my food collection and

00:11:02,189 --> 00:11:08,579
trying to do a cosine similarity with

00:11:05,249 --> 00:11:10,259
this vector you know this being a query

00:11:08,579 --> 00:11:11,749
for actually the donut vector and do a

00:11:10,259 --> 00:11:14,309
cosine similarity and what I see is that

00:11:11,749 --> 00:11:16,830
the donut document is the most similar

00:11:14,309 --> 00:11:18,720
to the donut vector followed by

00:11:16,830 --> 00:11:23,460
in breadsticks and cheese pizza I'm and

00:11:18,720 --> 00:11:25,800
so on another thing that I could do is I

00:11:23,460 --> 00:11:27,270
can search for oh sorry

00:11:25,800 --> 00:11:29,730
another technique I can use if I want to

00:11:27,270 --> 00:11:31,440
use a vector search in my main line

00:11:29,730 --> 00:11:34,980
query not as a streaming expression is

00:11:31,440 --> 00:11:36,870
to use solar one four three nine seven

00:11:34,980 --> 00:11:39,810
which is the vector fields and functions

00:11:36,870 --> 00:11:41,130
in solar patch that I'm personally

00:11:39,810 --> 00:11:46,680
currently working on and I'll show you

00:11:41,130 --> 00:11:48,180
how to use that so same thing here if

00:11:46,680 --> 00:11:51,450
you you know just want to pull the code

00:11:48,180 --> 00:11:52,890
a build solar and start solar you create

00:11:51,450 --> 00:11:56,850
a collection once you've created a

00:11:52,890 --> 00:12:00,030
collection you add your you know dense

00:11:56,850 --> 00:12:02,700
vector field type to your schema and

00:12:00,030 --> 00:12:05,280
then you create an actual vector field

00:12:02,700 --> 00:12:07,590
so in this case I'm naming my field

00:12:05,280 --> 00:12:11,310
vectors underscore V making it a dense

00:12:07,590 --> 00:12:15,570
vector and going from there once I've

00:12:11,310 --> 00:12:19,040
done that then I can add documents just

00:12:15,570 --> 00:12:21,030
like before where I've got you know

00:12:19,040 --> 00:12:24,120
doughnut apple juice

00:12:21,030 --> 00:12:26,700
cappuccino cheese pizza green tea etc

00:12:24,120 --> 00:12:28,560
here here's all my documents the one

00:12:26,700 --> 00:12:30,270
interesting thing to note and the solar

00:12:28,560 --> 00:12:32,850
implementation here is that I can

00:12:30,270 --> 00:12:35,520
actually pass in multiple vectors per

00:12:32,850 --> 00:12:39,720
field within the document so in this

00:12:35,520 --> 00:12:41,220
case you know when I do my vector

00:12:39,720 --> 00:12:43,140
similarity scoring instead of being

00:12:41,220 --> 00:12:45,840
limited to one vector per document which

00:12:43,140 --> 00:12:48,210
would be a document thought vector our

00:12:45,840 --> 00:12:50,250
document embedding I can actually you

00:12:48,210 --> 00:12:51,960
know potentially do you know a vector

00:12:50,250 --> 00:12:54,870
per paragraph or a vector per sentence

00:12:51,960 --> 00:12:59,100
or even a vector per word once I've done

00:12:54,870 --> 00:13:01,320
this I can now come in and you know run

00:12:59,100 --> 00:13:04,740
my search so in this case I want to sort

00:13:01,320 --> 00:13:07,350
by and return the vector similarity so

00:13:04,740 --> 00:13:10,440
I've got my in this case my query is

00:13:07,350 --> 00:13:12,720
going to be for this vector for that

00:13:10,440 --> 00:13:14,700
represents a doughnut my cosine

00:13:12,720 --> 00:13:18,210
similarity function is here this vector

00:13:14,700 --> 00:13:21,960
cosine function I'm going to return that

00:13:18,210 --> 00:13:23,670
cosine score and I'm going to sort my

00:13:21,960 --> 00:13:26,520
results based upon that cosine

00:13:23,670 --> 00:13:27,960
similarity calculation so in this case

00:13:26,520 --> 00:13:30,870
when I search for the doughnut vector

00:13:27,960 --> 00:13:33,090
what I get back is you know the toppers

00:13:30,870 --> 00:13:35,580
with a perfect score is the donut

00:13:33,090 --> 00:13:38,120
document because this first vector in

00:13:35,580 --> 00:13:40,830
that document got a perfect score

00:13:38,120 --> 00:13:43,290
cinnamon bread 6 our next cheese pizza

00:13:40,830 --> 00:13:46,320
and and so on and so forth so this is an

00:13:43,290 --> 00:13:49,280
example of using the cosine similarity

00:13:46,320 --> 00:13:54,720
to score documents based upon the

00:13:49,280 --> 00:13:58,410
similarity of the vectors in this next

00:13:54,720 --> 00:14:00,090
example I'm doing something similar but

00:13:58,410 --> 00:14:01,890
instead of only relying on the vector

00:14:00,090 --> 00:14:05,880
similarity calculation I'm actually

00:14:01,890 --> 00:14:07,860
combining the vector similarity with

00:14:05,880 --> 00:14:10,350
traditional keyboard search so in this

00:14:07,860 --> 00:14:15,120
case actually search for the term bread

00:14:10,350 --> 00:14:17,160
and I'm gonna match that like a normal

00:14:15,120 --> 00:14:20,460
keyword search and rank based upon the

00:14:17,160 --> 00:14:23,130
the BM 25 score for bread but then I'm

00:14:20,460 --> 00:14:24,690
also going to boost the ranking of my

00:14:23,130 --> 00:14:27,600
search results based upon the cosine

00:14:24,690 --> 00:14:29,070
similarity between the vectors so in

00:14:27,600 --> 00:14:30,540
this case you have same thing here's my

00:14:29,070 --> 00:14:32,910
doughnut vector here's my sicko sine

00:14:30,540 --> 00:14:34,860
similarity calculation still returning

00:14:32,910 --> 00:14:37,130
the fields or in this case I'm doing a

00:14:34,860 --> 00:14:40,170
boost based upon the cosine similarity

00:14:37,130 --> 00:14:41,460
and a normal query based upon bread and

00:14:40,170 --> 00:14:43,410
so this case what you see is that

00:14:41,460 --> 00:14:45,330
because I matched on the term bread and

00:14:43,410 --> 00:14:46,920
my query that I only get the two

00:14:45,330 --> 00:14:48,570
documents that contain the word bread

00:14:46,920 --> 00:14:50,190
and them so cinnamon breadsticks and

00:14:48,570 --> 00:14:53,190
cheese breadsticks but I'm actually

00:14:50,190 --> 00:14:55,470
boosting because I search on the vector

00:14:53,190 --> 00:14:57,240
for donut I'm boosting on the one that

00:14:55,470 --> 00:14:59,370
is the most related to donut in this

00:14:57,240 --> 00:15:01,200
case cinnamon breadsticks is more

00:14:59,370 --> 00:15:04,020
related to donut than cheese breadsticks

00:15:01,200 --> 00:15:06,660
and you can see in the explain down here

00:15:04,020 --> 00:15:09,030
for the calculation that's indeed it

00:15:06,660 --> 00:15:12,150
matched indented BM 25 score on this

00:15:09,030 --> 00:15:17,570
term bread and then boosted based upon

00:15:12,150 --> 00:15:20,670
the cosine similarity for the vectors

00:15:17,570 --> 00:15:23,550
this next example allows me to

00:15:20,670 --> 00:15:28,610
essentially use the vector cosine

00:15:23,550 --> 00:15:31,050
similarity as the score and to actually

00:15:28,610 --> 00:15:34,560
cut off the results beyond a certain

00:15:31,050 --> 00:15:37,400
score so unlike BM 25 I'm in traditional

00:15:34,560 --> 00:15:40,680
search relevancy which has all of the

00:15:37,400 --> 00:15:42,470
the scores relative to each other where

00:15:40,680 --> 00:15:44,280
there's no sort of absolute score

00:15:42,470 --> 00:15:46,110
indicating

00:15:44,280 --> 00:15:49,050
the quality of the match when you're

00:15:46,110 --> 00:15:52,200
doing something like a vector similarity

00:15:49,050 --> 00:15:54,180
and cosine in this case the scores are

00:15:52,200 --> 00:15:56,910
actually more absolute where a1 is a

00:15:54,180 --> 00:15:59,190
perfect score and a zero is you know no

00:15:56,910 --> 00:16:01,950
match at all and so in this case what

00:15:59,190 --> 00:16:03,990
I'm doing is I'm running my query my

00:16:01,950 --> 00:16:07,200
query itself is the cosine similarity

00:16:03,990 --> 00:16:09,120
value that is calculated everything else

00:16:07,200 --> 00:16:12,090
and here's exactly the same but now I'm

00:16:09,120 --> 00:16:14,640
applying a filter which limits the rate

00:16:12,090 --> 00:16:17,220
the documents coming back to only

00:16:14,640 --> 00:16:20,580
documents that have greater than 0.75

00:16:17,220 --> 00:16:22,500
cosine similarity so in this case the

00:16:20,580 --> 00:16:24,570
results I get back are ordered by

00:16:22,500 --> 00:16:28,920
relevance in this case the cosine

00:16:24,570 --> 00:16:31,290
similarity score and they're cut off so

00:16:28,920 --> 00:16:33,180
that no documents with a score less than

00:16:31,290 --> 00:16:35,460
0.75 come back so in this case I only

00:16:33,180 --> 00:16:36,990
get my top-ranked documents donuts

00:16:35,460 --> 00:16:38,040
cinnamon bread six cheese pizza and

00:16:36,990 --> 00:16:40,650
cheese breadsticks

00:16:38,040 --> 00:16:43,950
so that should give you a general sense

00:16:40,650 --> 00:16:45,600
of how to implement you know once you've

00:16:43,950 --> 00:16:48,270
got these thought vectors how to

00:16:45,600 --> 00:16:50,640
actually implement them in Apache Solr

00:16:48,270 --> 00:16:53,070
but of course those are all fairly

00:16:50,640 --> 00:16:55,410
contrived examples using you know food

00:16:53,070 --> 00:16:58,440
items and attributes of food that's not

00:16:55,410 --> 00:17:00,390
a real-world application and I also came

00:16:58,440 --> 00:17:02,550
up with those categories by hand so the

00:17:00,390 --> 00:17:05,850
question then becomes how do i encode my

00:17:02,550 --> 00:17:08,459
documents and queries into vectors so

00:17:05,850 --> 00:17:10,740
that's where vector encoders come in so

00:17:08,459 --> 00:17:13,470
vector encoders take queries documents

00:17:10,740 --> 00:17:15,270
as documents instance paragraphs etc and

00:17:13,470 --> 00:17:17,160
transformed them into thought vectors or

00:17:15,270 --> 00:17:18,930
embeddings usually they leverage deep

00:17:17,160 --> 00:17:20,939
learning which can discover rich

00:17:18,930 --> 00:17:22,500
language usage rules and semantics and

00:17:20,939 --> 00:17:24,120
the map them to combinations of features

00:17:22,500 --> 00:17:27,030
in the vector and there's lots of

00:17:24,120 --> 00:17:28,380
different libraries that do this and to

00:17:27,030 --> 00:17:30,180
give you a sense of those libraries I

00:17:28,380 --> 00:17:32,580
think everybody is pretty familiar with

00:17:30,180 --> 00:17:34,940
word Tyvek came out in 2013

00:17:32,580 --> 00:17:38,010
sort of took the NLP world by storm

00:17:34,940 --> 00:17:40,500
glove came out about a year later and

00:17:38,010 --> 00:17:43,560
improved upon what word to Becca do fast

00:17:40,500 --> 00:17:47,580
text a few years later and then in 2017

00:17:43,560 --> 00:17:50,040
the this paper about all you need is

00:17:47,580 --> 00:17:53,900
attention came out they introduced

00:17:50,040 --> 00:17:56,250
transformers which ultimately took us a

00:17:53,900 --> 00:17:57,730
generation beyond what words a Beck and

00:17:56,250 --> 00:17:59,770
the others did where

00:17:57,730 --> 00:18:01,300
whereas word Tyvek came up with a static

00:17:59,770 --> 00:18:03,280
representation of a vector that

00:18:01,300 --> 00:18:05,020
represented a you know a word

00:18:03,280 --> 00:18:07,360
what transformers did is they took the

00:18:05,020 --> 00:18:09,160
idea of context into consideration so

00:18:07,360 --> 00:18:11,500
that words and phrases could have

00:18:09,160 --> 00:18:13,720
different meaning within the context in

00:18:11,500 --> 00:18:15,430
which they appeared and so shortly

00:18:13,720 --> 00:18:16,960
thereafter we got Elmo

00:18:15,430 --> 00:18:19,630
we got the universal Simmons encoder

00:18:16,960 --> 00:18:21,040
which didn't necessarily you follow the

00:18:19,630 --> 00:18:22,360
transformer motto but then almost

00:18:21,040 --> 00:18:25,390
everything you saw from that point on

00:18:22,360 --> 00:18:29,050
leverage the transformer model a uln fit

00:18:25,390 --> 00:18:31,500
really made transfer learning which is

00:18:29,050 --> 00:18:34,900
the ability to start with a base

00:18:31,500 --> 00:18:38,290
language model and then retrain the only

00:18:34,900 --> 00:18:40,150
a portion of it to based upon the the

00:18:38,290 --> 00:18:41,800
content your domain that's where it

00:18:40,150 --> 00:18:45,790
really became popular here in these

00:18:41,800 --> 00:18:47,830
models and then of course Bert was a the

00:18:45,790 --> 00:18:50,080
next major breakthrough you know which

00:18:47,830 --> 00:18:51,670
Google has leveraged very heavily and

00:18:50,080 --> 00:18:53,560
then you know you can see from that

00:18:51,670 --> 00:18:55,450
point on there's just been an explosion

00:18:53,560 --> 00:18:58,390
of different models that have come out

00:18:55,450 --> 00:19:00,640
from all of these you know not even

00:18:58,390 --> 00:19:01,570
including anything that came into being

00:19:00,640 --> 00:19:03,820
in 2020

00:19:01,570 --> 00:19:06,760
lots of Sesame Street characters that

00:19:03,820 --> 00:19:09,010
the theming is fun there but but that's

00:19:06,760 --> 00:19:10,900
sort of what we what we see now and I

00:19:09,010 --> 00:19:13,000
haven't even included any of the 20/20

00:19:10,900 --> 00:19:16,120
stuff other than I did want to mention

00:19:13,000 --> 00:19:18,580
GPT 3 which just came out I think at the

00:19:16,120 --> 00:19:20,410
beginning of June this month which we'll

00:19:18,580 --> 00:19:22,000
talk about in just a little bit and you

00:19:20,410 --> 00:19:23,980
can actually see this is the leaderboard

00:19:22,000 --> 00:19:26,110
for the Glu benchmark which is one of

00:19:23,980 --> 00:19:29,320
the benchmarks that measures the success

00:19:26,110 --> 00:19:31,000
rates of each of these models and half

00:19:29,320 --> 00:19:32,770
of these I don't even recognize the the

00:19:31,000 --> 00:19:35,650
field is just growing by leaps and

00:19:32,770 --> 00:19:37,810
bounds so quickly and so we mentioned

00:19:35,650 --> 00:19:39,850
transformers which most of the modern

00:19:37,810 --> 00:19:42,090
models follow you know what is a

00:19:39,850 --> 00:19:45,850
transformer a transformer ultimately

00:19:42,090 --> 00:19:48,040
allows you to take incoming texts so in

00:19:45,850 --> 00:19:51,220
this case I've got the she went for a

00:19:48,040 --> 00:19:54,030
walk outdoors to encode it into a vector

00:19:51,220 --> 00:19:56,230
representation or an embedding and then

00:19:54,030 --> 00:19:58,270
once I've done that I can take that

00:19:56,230 --> 00:20:00,550
representation that in theory represents

00:19:58,270 --> 00:20:02,410
the meaning of the the input and then

00:20:00,550 --> 00:20:04,690
decode it back into something that's

00:20:02,410 --> 00:20:06,970
more you know human understandable so in

00:20:04,690 --> 00:20:09,430
this case I might get something out like

00:20:06,970 --> 00:20:11,710
the woman went outside to walk if my

00:20:09,430 --> 00:20:14,410
input was she went for a walk out door

00:20:11,710 --> 00:20:17,280
the meaning was sort of communicated

00:20:14,410 --> 00:20:19,780
here and then we translate it back out

00:20:17,280 --> 00:20:21,550
you could also build a different decoder

00:20:19,780 --> 00:20:24,190
so maybe I encoded this way but then I

00:20:21,550 --> 00:20:26,170
had a decoder built for the German

00:20:24,190 --> 00:20:28,150
language and I can automatically get a

00:20:26,170 --> 00:20:30,460
translation out that that is pretty good

00:20:28,150 --> 00:20:34,660
because the meaning has been preserved

00:20:30,460 --> 00:20:37,300
in the embedding the when we're doing

00:20:34,660 --> 00:20:39,940
search implementations and trying to do

00:20:37,300 --> 00:20:41,980
similarity for search typically we don't

00:20:39,940 --> 00:20:43,930
use the decoder part of the transformer

00:20:41,980 --> 00:20:46,540
and we focus on just on the encoder and

00:20:43,930 --> 00:20:48,850
so in this case documents will go into

00:20:46,540 --> 00:20:51,160
the encoder and for each document I will

00:20:48,850 --> 00:20:53,850
get you know a document vector or maybe

00:20:51,160 --> 00:20:57,640
a you know paragraph vector sentence and

00:20:53,850 --> 00:20:59,470
vectors etc those all get indexed into

00:20:57,640 --> 00:21:03,640
the search engine and then whenever a

00:20:59,470 --> 00:21:05,320
query comes in I you know similarly get

00:21:03,640 --> 00:21:06,910
the embedding for it and then I do the

00:21:05,320 --> 00:21:10,120
similarity calculation that I showed you

00:21:06,910 --> 00:21:13,060
a few few slides back that allows you to

00:21:10,120 --> 00:21:15,160
just score each query vector relevant to

00:21:13,060 --> 00:21:18,220
all of the vectors represented in your

00:21:15,160 --> 00:21:19,960
documents there's some performance

00:21:18,220 --> 00:21:21,670
considerations however whenever we're

00:21:19,960 --> 00:21:25,000
doing this for real-time search so for

00:21:21,670 --> 00:21:27,430
example the vector scoring is slow it's

00:21:25,000 --> 00:21:30,280
a whereas with the traditional inverted

00:21:27,430 --> 00:21:31,890
index you look up the documents that you

00:21:30,280 --> 00:21:35,410
need to score and then only score those

00:21:31,890 --> 00:21:37,000
with vector similarities you have to

00:21:35,410 --> 00:21:39,130
actually score every single document

00:21:37,000 --> 00:21:42,520
that you're considering to figure out if

00:21:39,130 --> 00:21:44,500
it's a match or not and so this is

00:21:42,520 --> 00:21:45,730
equivalent to doing a table scan in a

00:21:44,500 --> 00:21:48,340
database so it can be very very

00:21:45,730 --> 00:21:50,230
expensive and very slow so the solution

00:21:48,340 --> 00:21:52,180
set to that is this notion of quantized

00:21:50,230 --> 00:21:55,000
vectors or Proximus nearest neighbors

00:21:52,180 --> 00:21:58,360
approximate nearest neighbors the idea

00:21:55,000 --> 00:22:00,850
here is that you map the vectors into a

00:21:58,360 --> 00:22:02,470
set of discrete values ultimately

00:22:00,850 --> 00:22:04,810
creating tokens that you can put into

00:22:02,470 --> 00:22:07,270
your inverted index to match on to

00:22:04,810 --> 00:22:08,530
reduce the number of documents that you

00:22:07,270 --> 00:22:12,010
have to actually do the full vector

00:22:08,530 --> 00:22:14,620
scoring on and that enables you to

00:22:12,010 --> 00:22:17,590
really combine vector scoring with

00:22:14,620 --> 00:22:20,050
traditional search while not being

00:22:17,590 --> 00:22:21,880
painfully slow so the recommended

00:22:20,050 --> 00:22:25,330
approach when you're implementing this

00:22:21,880 --> 00:22:27,120
in a search engine is typically to

00:22:25,330 --> 00:22:29,260
do approximate nearest neighbor search

00:22:27,120 --> 00:22:32,490
leveraging one of these quantization

00:22:29,260 --> 00:22:34,900
techniques and then after me you found a

00:22:32,490 --> 00:22:37,210
potential candidate set of documents

00:22:34,900 --> 00:22:39,550
that should be good matches you then

00:22:37,210 --> 00:22:43,240
rear ank those documents leveraging the

00:22:39,550 --> 00:22:44,800
vector similarity score and so you know

00:22:43,240 --> 00:22:47,230
similarly in solar if you want to

00:22:44,800 --> 00:22:50,170
implement this you know same general

00:22:47,230 --> 00:22:52,090
setup as before except for in this case

00:22:50,170 --> 00:22:54,460
I'm saying that instead of scoring all

00:22:52,090 --> 00:22:56,950
of the documents I only want to bring

00:22:54,460 --> 00:22:58,840
back the top I only want to bring back

00:22:56,950 --> 00:23:01,270
the top five documents that are returned

00:22:58,840 --> 00:23:04,270
and of those five documents I'm then

00:23:01,270 --> 00:23:06,880
going to run my vector cosine similarity

00:23:04,270 --> 00:23:10,060
on those so you can see my example here

00:23:06,880 --> 00:23:12,310
document one these first

00:23:10,060 --> 00:23:15,370
I guess sorry I probably should've been

00:23:12,310 --> 00:23:16,510
four but these top documents have a

00:23:15,370 --> 00:23:18,940
higher score because they've been

00:23:16,510 --> 00:23:20,890
rewrapped and from my donut search you

00:23:18,940 --> 00:23:22,630
see that donut cheese pizza apple juice

00:23:20,890 --> 00:23:25,870
and cappuccino where the top ranked out

00:23:22,630 --> 00:23:28,150
of those first five that came back and

00:23:25,870 --> 00:23:30,250
then you can see that now I'm thinking

00:23:28,150 --> 00:23:31,570
so typo should be four but and then you

00:23:30,250 --> 00:23:33,340
can see that for the rest of the

00:23:31,570 --> 00:23:34,930
documents they just appear and you know

00:23:33,340 --> 00:23:37,480
document order because they didn't

00:23:34,930 --> 00:23:39,430
actually get scored and so in practice

00:23:37,480 --> 00:23:41,530
you had just a wild-card search here to

00:23:39,430 --> 00:23:45,160
show the technique but in practice if

00:23:41,530 --> 00:23:47,350
you've got a mechanism to either do an

00:23:45,160 --> 00:23:49,120
initial keyword search essentially as

00:23:47,350 --> 00:23:52,000
your approximate nearest neighbor query

00:23:49,120 --> 00:23:54,940
or to build some approximate nearest

00:23:52,000 --> 00:23:56,800
neighbors or clustering capability that

00:23:54,940 --> 00:23:59,440
you can then tag on the documents so

00:23:56,800 --> 00:24:01,390
that you can then filter on them before

00:23:59,440 --> 00:24:04,090
the scoring you would just you know pass

00:24:01,390 --> 00:24:05,830
in the a n n filter as either your query

00:24:04,090 --> 00:24:07,900
if there's some relevance associated

00:24:05,830 --> 00:24:09,400
with it or just as a filter if you just

00:24:07,900 --> 00:24:12,670
want to filter down to a candidate set

00:24:09,400 --> 00:24:14,110
of documents and then rewrite them so

00:24:12,670 --> 00:24:16,510
there's lots of different approximate

00:24:14,110 --> 00:24:19,210
nearest neighbors approaches this gives

00:24:16,510 --> 00:24:22,240
you an example of you know some of the

00:24:19,210 --> 00:24:23,410
benchmarks there's several that are

00:24:22,240 --> 00:24:26,290
currently in the process of being

00:24:23,410 --> 00:24:28,560
implemented in Apache Lucene so I'm

00:24:26,290 --> 00:24:31,110
actually having vector fields and Lucene

00:24:28,560 --> 00:24:33,100
that internally can implement

00:24:31,110 --> 00:24:35,440
approximate nearest neighbors indexing

00:24:33,100 --> 00:24:36,790
to make search faster I'm not going to

00:24:35,440 --> 00:24:39,309
go over those but definitely check them

00:24:36,790 --> 00:24:41,139
out and then of course the

00:24:39,309 --> 00:24:44,080
you know some limitations for these

00:24:41,139 --> 00:24:45,429
embedding models so this article you

00:24:44,080 --> 00:24:47,350
know we talked looked at earlier about

00:24:45,429 --> 00:24:51,220
Burt one of the things I didn't mention

00:24:47,350 --> 00:24:53,080
before is that it actually only is being

00:24:51,220 --> 00:24:56,889
used by Google and one of one out of ten

00:24:53,080 --> 00:24:59,230
searches so it's not a solution to solve

00:24:56,889 --> 00:25:03,370
every problem but in 1 out of 10

00:24:59,230 --> 00:25:04,389
searches it is you know so 90% of

00:25:03,370 --> 00:25:11,019
searches are still using other

00:25:04,389 --> 00:25:12,700
techniques additionally you know the

00:25:11,019 --> 00:25:14,230
search companies Google said the bird

00:25:12,700 --> 00:25:16,149
the bird technique would return more

00:25:14,230 --> 00:25:17,830
useful responses to many queries that

00:25:16,149 --> 00:25:19,570
are also said that in some instances the

00:25:17,830 --> 00:25:22,289
new algorithm had produced worse results

00:25:19,570 --> 00:25:26,860
than before so it doesn't necessarily

00:25:22,289 --> 00:25:28,269
always improve things further GPT 3

00:25:26,860 --> 00:25:31,720
which just came out at the beginning of

00:25:28,269 --> 00:25:33,100
June it is a major breakthrough there's

00:25:31,720 --> 00:25:34,899
something like a hundred and seventy

00:25:33,100 --> 00:25:37,090
five billion parameters that use this to

00:25:34,899 --> 00:25:40,210
train but in the paper they actually

00:25:37,090 --> 00:25:42,639
identify some general issues with the

00:25:40,210 --> 00:25:44,289
use of these kind of language models so

00:25:42,639 --> 00:25:45,850
for this article in this article says

00:25:44,289 --> 00:25:48,369
despite the strong quantitative and

00:25:45,850 --> 00:25:49,990
qualitative improvements of GPT 3 it

00:25:48,369 --> 00:25:51,490
still has notable weaknesses those

00:25:49,990 --> 00:25:53,169
weaknesses include an inability to

00:25:51,490 --> 00:25:57,759
achieve significant accuracy on

00:25:53,169 --> 00:25:59,169
adversarial nli which is you know where

00:25:57,759 --> 00:26:01,929
programs need to determine the

00:25:59,169 --> 00:26:04,119
relationship between two sentences GPT 3

00:26:01,929 --> 00:26:06,429
does little better than chance on things

00:26:04,119 --> 00:26:08,769
like adversarial in Li the authors write

00:26:06,429 --> 00:26:11,529
worse having amped up the processing

00:26:08,769 --> 00:26:13,269
power 275 billion weights the authors

00:26:11,529 --> 00:26:15,610
weren't exactly sure why it was coming

00:26:13,269 --> 00:26:17,740
up short additionally and even more

00:26:15,610 --> 00:26:19,059
startling is the next observation the

00:26:17,740 --> 00:26:21,220
whole practice of trying to predict

00:26:19,059 --> 00:26:25,480
what's going to happen with language may

00:26:21,220 --> 00:26:26,919
be the wrong approach and you know these

00:26:25,480 --> 00:26:29,619
may be the wrong types of models for

00:26:26,919 --> 00:26:32,440
this and so if you look at traditional

00:26:29,619 --> 00:26:35,259
keyword search versus vector search we

00:26:32,440 --> 00:26:37,119
can actually note if you throw out a few

00:26:35,259 --> 00:26:40,210
examples where where each one is likely

00:26:37,119 --> 00:26:42,309
to do well or to fail so for example for

00:26:40,210 --> 00:26:44,379
obscured keyword combinations like this

00:26:42,309 --> 00:26:46,740
boolean query keyword search does really

00:26:44,379 --> 00:26:49,840
well and vector search completely fails

00:26:46,740 --> 00:26:51,490
for specific identifiers and attributes

00:26:49,840 --> 00:26:53,470
for example if somebody searched them

00:26:51,490 --> 00:26:55,000
for a specific product or attribute

00:26:53,470 --> 00:26:57,510
a keyword search will succeed because

00:26:55,000 --> 00:26:59,530
it's just filtering down unknown

00:26:57,510 --> 00:27:01,570
attributes where this vector search will

00:26:59,530 --> 00:27:02,830
likely fail depending upon how it was

00:27:01,570 --> 00:27:05,410
trained and and whether there's

00:27:02,830 --> 00:27:06,940
overfitting or not for natural language

00:27:05,410 --> 00:27:09,460
queries like can my spouse drive on my

00:27:06,940 --> 00:27:10,930
insurance keyword search might get lucky

00:27:09,460 --> 00:27:12,460
but is probably gonna fail pretty

00:27:10,930 --> 00:27:13,900
miserably because it doesn't understand

00:27:12,460 --> 00:27:14,470
the words whereas vector search will

00:27:13,900 --> 00:27:16,840
succeed

00:27:14,470 --> 00:27:18,910
fuzzy language queries like famous

00:27:16,840 --> 00:27:21,070
French tower are things that keyword

00:27:18,910 --> 00:27:23,230
search will mismatch because it again is

00:27:21,070 --> 00:27:25,530
just looking in individual keywords but

00:27:23,230 --> 00:27:27,910
where vector search will will succeed

00:27:25,530 --> 00:27:30,070
but then you've got this issue of

00:27:27,910 --> 00:27:32,890
structured relationship queries so

00:27:30,070 --> 00:27:36,340
things like a popular barbecue near near

00:27:32,890 --> 00:27:37,660
haystack where you actually have meaning

00:27:36,340 --> 00:27:39,220
associated with each of these and you're

00:27:37,660 --> 00:27:42,340
looking at the interrelationship between

00:27:39,220 --> 00:27:44,050
them in this case keyword search will

00:27:42,340 --> 00:27:46,210
fail and vector search will fail because

00:27:44,050 --> 00:27:49,390
vector search won't necessarily know

00:27:46,210 --> 00:27:51,190
that haystack is a is an event at a

00:27:49,390 --> 00:27:54,610
particular location and I'm looking for

00:27:51,190 --> 00:27:56,680
a geospatial search for trying to do all

00:27:54,610 --> 00:27:59,670
these relationships just vector search

00:27:56,680 --> 00:28:01,420
just doesn't quite pull it off and so

00:27:59,670 --> 00:28:03,790
you know that's where we get into

00:28:01,420 --> 00:28:05,380
knowledge graphs and I'm gonna skip over

00:28:03,790 --> 00:28:07,420
this sort of what is a knowledge graph

00:28:05,380 --> 00:28:08,740
piece of it but there's one particular

00:28:07,420 --> 00:28:10,270
kind of knowledge graph that I've worked

00:28:08,740 --> 00:28:13,540
on called a semantic knowledge graph

00:28:10,270 --> 00:28:15,640
that essentially allows us to take the

00:28:13,540 --> 00:28:18,580
keywords that we have in our documents

00:28:15,640 --> 00:28:20,470
and relate them statistically to try to

00:28:18,580 --> 00:28:21,910
figure out other related keywords now

00:28:20,470 --> 00:28:23,890
presented on this in the past so I'm not

00:28:21,910 --> 00:28:26,770
going to spend much time on it today but

00:28:23,890 --> 00:28:28,570
I can essentially take a term look

00:28:26,770 --> 00:28:30,550
through documents to find other related

00:28:28,570 --> 00:28:33,190
terms and then you know sort of traverse

00:28:30,550 --> 00:28:35,860
through the graph that way and so one

00:28:33,190 --> 00:28:37,810
way I've used this and since this is a

00:28:35,860 --> 00:28:39,790
remote conference I didn't have a

00:28:37,810 --> 00:28:42,880
location so I used a previous conference

00:28:39,790 --> 00:28:44,170
that I spoken out which is activate but

00:28:42,880 --> 00:28:45,970
what I can actually do is query the

00:28:44,170 --> 00:28:47,920
semantic knowledge graph or query my

00:28:45,970 --> 00:28:50,680
inverted index and ask it questions like

00:28:47,920 --> 00:28:52,990
hey what does barbecue mean and what it

00:28:50,680 --> 00:28:55,450
can tell me is that hey based upon the

00:28:52,990 --> 00:28:56,980
content in my inverted index barbecue is

00:28:55,450 --> 00:28:59,620
most related to the category of

00:28:56,980 --> 00:29:02,170
documented restaurants and within that

00:28:59,620 --> 00:29:05,020
category you know it means things like

00:29:02,170 --> 00:29:06,910
barbecue brisket and ribs and pork and

00:29:05,020 --> 00:29:09,240
the second-most common categories

00:29:06,910 --> 00:29:11,590
to equipment and within that category

00:29:09,240 --> 00:29:15,070
barbecue is related to things like grill

00:29:11,590 --> 00:29:17,050
charcoal and propane I can then combine

00:29:15,070 --> 00:29:20,020
that with a structured knowledge graph

00:29:17,050 --> 00:29:23,260
so in this case my Acree for barbecuing

00:29:20,020 --> 00:29:24,970
or activate I can do things like say you

00:29:23,260 --> 00:29:26,380
know let me search for let me find the

00:29:24,970 --> 00:29:28,900
word near see the decimal in two

00:29:26,380 --> 00:29:30,430
location distance traverse from there to

00:29:28,900 --> 00:29:33,390
you know activate which is the

00:29:30,430 --> 00:29:36,520
conference figure out that activate is

00:29:33,390 --> 00:29:38,430
was when it was last held at the what

00:29:36,520 --> 00:29:41,260
was held at the Washington Hilton and

00:29:38,430 --> 00:29:43,720
then you know the the Washington Hilton

00:29:41,260 --> 00:29:45,160
I can look up a location for that and so

00:29:43,720 --> 00:29:46,990
this is an example I'm not going to go

00:29:45,160 --> 00:29:49,900
through the details but an example of me

00:29:46,990 --> 00:29:52,630
taking the keyword activate and mapping

00:29:49,900 --> 00:29:54,430
it using a knowledge graph traversal on

00:29:52,630 --> 00:29:57,160
solar to the location that activate was

00:29:54,430 --> 00:29:58,600
held at and then you know this example

00:29:57,160 --> 00:30:00,550
again the slides will be available

00:29:58,600 --> 00:30:02,920
afterwards allows me to take that

00:30:00,550 --> 00:30:04,270
keyword of barbecue look that up and the

00:30:02,920 --> 00:30:04,900
semantic knowledge graph we did just a

00:30:04,270 --> 00:30:07,150
minute ago

00:30:04,900 --> 00:30:08,830
take the keyword of activate do one

00:30:07,150 --> 00:30:12,270
query to solar to traverse the knowledge

00:30:08,830 --> 00:30:14,860
graph and ultimately bring back a search

00:30:12,270 --> 00:30:17,740
build a search query that actually goes

00:30:14,860 --> 00:30:20,680
to solar that looks like barbecue ribs

00:30:17,740 --> 00:30:22,720
brisket dot type of restaurant and then

00:30:20,680 --> 00:30:24,430
filtering to the location that activate

00:30:22,720 --> 00:30:27,070
was held at and all of that can be done

00:30:24,430 --> 00:30:28,720
with one graph you know look up in the

00:30:27,070 --> 00:30:30,900
knowledge graph and then a subsequent

00:30:28,720 --> 00:30:32,920
query to actually rank the results

00:30:30,900 --> 00:30:36,040
similarly you know it's great for top

00:30:32,920 --> 00:30:39,490
barbecue in Berlin realizes that top

00:30:36,040 --> 00:30:41,410
actually means a you know a boost on

00:30:39,490 --> 00:30:42,790
popularity of documents and then

00:30:41,410 --> 00:30:45,520
everything else here is very similar to

00:30:42,790 --> 00:30:47,530
before and we talked about the

00:30:45,520 --> 00:30:50,170
transformer models before where context

00:30:47,530 --> 00:30:51,850
really matters I'm in this case you know

00:30:50,170 --> 00:30:54,250
you can see that when I search for BBQ

00:30:51,850 --> 00:30:56,230
near activate that it understands that

00:30:54,250 --> 00:30:58,420
I'm looking for a restaurant but if I

00:30:56,230 --> 00:30:59,890
search for barbecue grill it understands

00:30:58,420 --> 00:31:02,230
a different meaning of the words and

00:30:59,890 --> 00:31:04,360
also that grill is a sort of outdoor

00:31:02,230 --> 00:31:06,970
appliance and the the category of grill

00:31:04,360 --> 00:31:09,010
and so the same kind of nuance that we

00:31:06,970 --> 00:31:10,900
get with thought vectors and these

00:31:09,010 --> 00:31:12,220
transformer models we can actually get

00:31:10,900 --> 00:31:14,770
through our knowledge graphs as well if

00:31:12,220 --> 00:31:16,510
we can shuck them appropriately and so

00:31:14,770 --> 00:31:19,660
with the knowledge graph approach

00:31:16,510 --> 00:31:20,600
answering a query like four-star hotels

00:31:19,660 --> 00:31:22,430
near movie theater

00:31:20,600 --> 00:31:24,440
open after midnight in Berlin is

00:31:22,430 --> 00:31:26,630
something we can handle pretty easily

00:31:24,440 --> 00:31:27,770
whereas with either keyword search or

00:31:26,630 --> 00:31:30,800
thought vectors that would be

00:31:27,770 --> 00:31:32,150
challenging and so you know just a

00:31:30,800 --> 00:31:34,820
couple of thoughts

00:31:32,150 --> 00:31:37,250
here's we wrap up on semantic vector

00:31:34,820 --> 00:31:39,260
spaces so thought vectors are embeddings

00:31:37,250 --> 00:31:41,240
and knowledge graphs and keyword search

00:31:39,260 --> 00:31:43,730
ultimately all resolve to the same

00:31:41,240 --> 00:31:45,860
overlapping semantic vector spaces we're

00:31:43,730 --> 00:31:47,260
still dealing with the same content and

00:31:45,860 --> 00:31:50,000
the same relationships within the

00:31:47,260 --> 00:31:52,310
meaning of the things in our domain it's

00:31:50,000 --> 00:31:54,200
just these are different representations

00:31:52,310 --> 00:31:57,410
or techniques we can use to get at that

00:31:54,200 --> 00:31:58,820
understanding at the semantics each of

00:31:57,410 --> 00:32:00,380
these are really just different ways of

00:31:58,820 --> 00:32:02,930
looking at the same relationships that

00:32:00,380 --> 00:32:05,240
exist within our content likewise it's

00:32:02,930 --> 00:32:07,850
also possible to model user behavior

00:32:05,240 --> 00:32:10,010
signals into either numerical vectors

00:32:07,850 --> 00:32:11,690
tokens on documents or as nodes and

00:32:10,010 --> 00:32:14,150
edges in a graph again each of these

00:32:11,690 --> 00:32:16,400
techniques can be used both for content

00:32:14,150 --> 00:32:18,140
and for user behavior and then finally

00:32:16,400 --> 00:32:20,240
certain types of attributes are much

00:32:18,140 --> 00:32:23,540
easier to represent as numerical vectors

00:32:20,240 --> 00:32:24,140
such as image features and you know

00:32:23,540 --> 00:32:26,450
things like that

00:32:24,140 --> 00:32:28,340
this makes multimodal learning or joint

00:32:26,450 --> 00:32:29,690
learning much easier to accomplish then

00:32:28,340 --> 00:32:32,180
in the past with just keywords and

00:32:29,690 --> 00:32:33,890
knowledge graphs and so if you sort of

00:32:32,180 --> 00:32:36,170
think of this as different approaches if

00:32:33,890 --> 00:32:37,700
I take a query like this one here

00:32:36,170 --> 00:32:39,260
machine learning research and

00:32:37,700 --> 00:32:42,170
development Portland Oregon software

00:32:39,260 --> 00:32:43,430
engineer and new to Java traditional

00:32:42,170 --> 00:32:46,070
keyword search would represent it this

00:32:43,430 --> 00:32:48,170
way a knowledge graph based query might

00:32:46,070 --> 00:32:50,330
expand that based upon the relationships

00:32:48,170 --> 00:32:52,400
that it understands and a thought vector

00:32:50,330 --> 00:32:56,390
would ultimately try to map the entire

00:32:52,400 --> 00:32:58,370
query into one vector of meaning and try

00:32:56,390 --> 00:33:02,600
to search on that to probabilistically

00:32:58,370 --> 00:33:05,540
find the results I like to think of

00:33:02,600 --> 00:33:07,490
things in terms of though when we're

00:33:05,540 --> 00:33:09,320
solving search problems in terms of

00:33:07,490 --> 00:33:11,360
they're our ultimate goal being to

00:33:09,320 --> 00:33:12,890
understand user intent which ultimately

00:33:11,360 --> 00:33:14,510
means understanding our content

00:33:12,890 --> 00:33:17,690
understanding our users and

00:33:14,510 --> 00:33:19,610
understanding our domain knowledge

00:33:17,690 --> 00:33:22,490
graphs and keyword search are key ways

00:33:19,610 --> 00:33:25,130
that we we do that and I like to think

00:33:22,490 --> 00:33:26,840
of you know dense vector search not as a

00:33:25,130 --> 00:33:28,700
new dimension but essentially as a

00:33:26,840 --> 00:33:31,640
technique so we can use the inverted

00:33:28,700 --> 00:33:34,279
index to match on tokens we can use

00:33:31,640 --> 00:33:35,779
dense vector search to

00:33:34,279 --> 00:33:37,700
score and then match based upon high

00:33:35,779 --> 00:33:39,469
scores or we can use a hybrid approach

00:33:37,700 --> 00:33:42,109
which I showed earlier when you're

00:33:39,469 --> 00:33:44,960
trying to combine both together and so a

00:33:42,109 --> 00:33:47,029
couple of takeaways first thought

00:33:44,960 --> 00:33:48,379
vectors are embeddings work very well

00:33:47,029 --> 00:33:50,419
for natural language queries and

00:33:48,379 --> 00:33:52,159
questions or for modeling non textual

00:33:50,419 --> 00:33:54,739
data like image features and user

00:33:52,159 --> 00:33:57,109
signals dense vectors also make it

00:33:54,739 --> 00:33:58,969
easier to enable joint learning modeling

00:33:57,109 --> 00:34:01,369
text content user behavioral signals

00:33:58,969 --> 00:34:03,169
images and so on to a shared vector

00:34:01,369 --> 00:34:05,389
space the keep in mind that even Google

00:34:03,169 --> 00:34:07,940
only uses the technique currently for 10

00:34:05,389 --> 00:34:09,619
percent of their queries knowledge

00:34:07,940 --> 00:34:11,869
graphs on the other hand work best for

00:34:09,619 --> 00:34:13,339
relational queries including reasoning

00:34:11,869 --> 00:34:15,710
about known entities dealing with

00:34:13,339 --> 00:34:18,889
hierarchies and multi level inference a

00:34:15,710 --> 00:34:21,440
keyword search is not dead it works best

00:34:18,889 --> 00:34:23,289
for longtail keywords specific item or

00:34:21,440 --> 00:34:25,579
attribute search like names and ID's

00:34:23,289 --> 00:34:28,069
obscure keywords boolean type query

00:34:25,579 --> 00:34:28,760
needs and is the safest catch-all or

00:34:28,069 --> 00:34:31,250
fallback

00:34:28,760 --> 00:34:32,980
for most queries and then finally it

00:34:31,250 --> 00:34:35,990
should be possible to actually enable

00:34:32,980 --> 00:34:38,510
explainable AI on trained inspector

00:34:35,990 --> 00:34:40,819
representations by mapping similar

00:34:38,510 --> 00:34:42,710
documents back to overlapping semantic

00:34:40,819 --> 00:34:44,629
vector spaces at the keyword and

00:34:42,710 --> 00:34:45,770
attribute level such as through the

00:34:44,629 --> 00:34:48,440
semantic knowledge graph I showed

00:34:45,770 --> 00:34:50,389
earlier essentially if we understand

00:34:48,440 --> 00:34:52,669
that each of this these techniques is

00:34:50,389 --> 00:34:55,480
ultimately trying to get at the same

00:34:52,669 --> 00:34:58,549
semantics in the same semantic space

00:34:55,480 --> 00:35:00,260
vector space then we should in theory be

00:34:58,549 --> 00:35:02,240
able to overlap these techniques on top

00:35:00,260 --> 00:35:03,440
of each other and use one to explain the

00:35:02,240 --> 00:35:05,210
other and so that's an area that I

00:35:03,440 --> 00:35:08,210
personally plan to spend some time

00:35:05,210 --> 00:35:09,680
researching in the near future so I

00:35:08,210 --> 00:35:11,359
think at the end of the day the best

00:35:09,680 --> 00:35:12,849
systems will likely use a combination of

00:35:11,359 --> 00:35:17,569
all these approaches blended together

00:35:12,849 --> 00:35:20,150
and hopefully more to come soon on all

00:35:17,569 --> 00:35:22,970
of that so that's it for me thank you

00:35:20,150 --> 00:35:24,170
Charlie if you wanna know if we have any

00:35:22,970 --> 00:35:26,539
questions if we've got a couple of

00:35:24,170 --> 00:35:29,380
minutes maybe take a few of those but

00:35:26,539 --> 00:35:29,380
yeah thank you everybody

00:35:34,950 --> 00:35:40,869
Charlie you're muted I think thank you

00:35:39,160 --> 00:35:42,430
Trey fascinating as always

00:35:40,869 --> 00:35:45,040
unsurprisingly we do have a few

00:35:42,430 --> 00:35:46,390
questions I not sure we'll get to all of

00:35:45,040 --> 00:35:48,640
them but there is Hannah there's a

00:35:46,390 --> 00:35:49,960
breakout room and I'm sure Trey would be

00:35:48,640 --> 00:35:52,510
happy to answer some questions in this

00:35:49,960 --> 00:35:53,770
lack as well so learn I'm gonna start at

00:35:52,510 --> 00:35:56,770
the top and see what we can work through

00:35:53,770 --> 00:35:59,920
the next few minutes so a couple of

00:35:56,770 --> 00:36:03,099
combined questions here mateo asks are

00:35:59,920 --> 00:36:06,790
vectors always composed in by non-

00:36:03,099 --> 00:36:09,460
components and peter as to that about

00:36:06,790 --> 00:36:13,630
using dot product and cosine distances

00:36:09,460 --> 00:36:15,580
and search so in lucy nate- schools are

00:36:13,630 --> 00:36:17,109
prohibited but dot product and cosine

00:36:15,580 --> 00:36:21,270
distance return return negatives how do

00:36:17,109 --> 00:36:23,170
we work around this yeah so usually

00:36:21,270 --> 00:36:25,090
because of the limitations that we've

00:36:23,170 --> 00:36:27,340
seen they're usually if you need to

00:36:25,090 --> 00:36:30,460
include negatives you can essentially

00:36:27,340 --> 00:36:31,660
just cut off at zero and then move

00:36:30,460 --> 00:36:33,970
everything that was negative to the

00:36:31,660 --> 00:36:36,670
positive side so that's that's the

00:36:33,970 --> 00:36:39,760
technique you would typically use it's

00:36:36,670 --> 00:36:42,640
you know the hacker workaround but it

00:36:39,760 --> 00:36:43,839
can definitely be done just you at the

00:36:42,640 --> 00:36:45,280
end of the day you can just lose half

00:36:43,839 --> 00:36:48,130
your precision from from the from the

00:36:45,280 --> 00:36:50,140
negative side but so that's how you

00:36:48,130 --> 00:36:52,960
would do that in terms of the other

00:36:50,140 --> 00:36:56,740
question was dot product and cosine what

00:36:52,960 --> 00:36:58,510
was the actual question well when you do

00:36:56,740 --> 00:36:59,560
do dot products or cosine calculations

00:36:58,510 --> 00:37:02,760
you often end up with negative numbers

00:36:59,560 --> 00:37:05,830
so how do you how do you then use them

00:37:02,760 --> 00:37:07,240
so if you if you then end up with

00:37:05,830 --> 00:37:09,339
everything on the positive side and

00:37:07,240 --> 00:37:12,040
you're doing your your cosine there then

00:37:09,339 --> 00:37:15,580
you basically focus on the the range

00:37:12,040 --> 00:37:19,599
between 0 & 1 okay okay thank you yep so

00:37:15,580 --> 00:37:21,520
on our next question aya asks how is the

00:37:19,599 --> 00:37:23,619
similarity score calculated if there are

00:37:21,520 --> 00:37:25,780
multiple vectors for a document field do

00:37:23,619 --> 00:37:30,430
you take a max score yeah great question

00:37:25,780 --> 00:37:33,869
so in the implementation that I was

00:37:30,430 --> 00:37:36,310
showing that I'm working on essentially

00:37:33,869 --> 00:37:37,990
you know I won't pull the screen back up

00:37:36,310 --> 00:37:40,000
but essentially a one of the parameters

00:37:37,990 --> 00:37:44,490
you pass into the vector cosine function

00:37:40,000 --> 00:37:46,990
is a selector so it can either be max

00:37:44,490 --> 00:37:49,830
min first

00:37:46,990 --> 00:37:53,110
last or average so if there's you know

00:37:49,830 --> 00:37:54,850
ten vectors in the document you can make

00:37:53,110 --> 00:37:56,530
the store coming out of the function the

00:37:54,850 --> 00:37:59,760
average of the similarity of each

00:37:56,530 --> 00:38:01,810
between each of those vectors or

00:37:59,760 --> 00:38:04,090
alternatively take the max score the min

00:38:01,810 --> 00:38:06,430
score or if you for efficiency reasons

00:38:04,090 --> 00:38:07,869
want to just take the first vector

00:38:06,430 --> 00:38:09,070
maybe I've indexed a bunch but I only

00:38:07,869 --> 00:38:10,690
want to use the first you can say first

00:38:09,070 --> 00:38:12,280
and then it will ignore all the

00:38:10,690 --> 00:38:13,930
additional vectors and up or you can do

00:38:12,280 --> 00:38:15,880
last but that's probably less less

00:38:13,930 --> 00:38:20,440
common but yeah there's just a selector

00:38:15,880 --> 00:38:22,840
that you pass in okay next question from

00:38:20,440 --> 00:38:24,490
Charles if you have suggestions or case

00:38:22,840 --> 00:38:27,490
studies on how to tune the relative

00:38:24,490 --> 00:38:29,560
weighting of I are scoring versus the

00:38:27,490 --> 00:38:33,970
thought vector cosine distance scoring

00:38:29,560 --> 00:38:36,010
so even just rules of thumb be useful so

00:38:33,970 --> 00:38:38,290
so I had a basically if you're doing a

00:38:36,010 --> 00:38:41,530
keyword search and using like being 25

00:38:38,290 --> 00:38:41,920
weighting that versus the the the vector

00:38:41,530 --> 00:38:44,550
score

00:38:41,920 --> 00:38:46,660
that's the question huh yeah um so I

00:38:44,550 --> 00:38:49,510
don't and I don't know that there's

00:38:46,660 --> 00:38:51,190
there's not gonna be a fixed answer I'm

00:38:49,510 --> 00:38:53,080
just gonna depend upon your domain and

00:38:51,190 --> 00:38:55,119
and what your users are doing and the

00:38:53,080 --> 00:38:56,950
kinds of queries are sending in what I

00:38:55,119 --> 00:39:00,850
would say is it's it's not gonna be

00:38:56,950 --> 00:39:04,390
based upon a you know always using let's

00:39:00,850 --> 00:39:07,660
say 50% keyword score and 50% vector

00:39:04,390 --> 00:39:09,310
similarity score I think it's really

00:39:07,660 --> 00:39:11,020
gonna be based upon the kind of query

00:39:09,310 --> 00:39:15,340
that comes in so you might have queries

00:39:11,020 --> 00:39:18,869
that look like questions you know who is

00:39:15,340 --> 00:39:21,040
the president of so-and-so University

00:39:18,869 --> 00:39:23,890
that is probably going to be best

00:39:21,040 --> 00:39:26,290
answered by the vector similarity score

00:39:23,890 --> 00:39:27,869
versus the keyword score and so in that

00:39:26,290 --> 00:39:30,100
case you would want to weight the vector

00:39:27,869 --> 00:39:32,560
similarity very high and the keyword

00:39:30,100 --> 00:39:35,050
similarity either non-existent or very

00:39:32,560 --> 00:39:37,240
low in another case somebody might type

00:39:35,050 --> 00:39:39,730
in a query that is just a product ID or

00:39:37,240 --> 00:39:41,170
the name of a product in that case you

00:39:39,730 --> 00:39:43,000
would want the keyword match to be very

00:39:41,170 --> 00:39:45,550
high and the vector score to be low so I

00:39:43,000 --> 00:39:48,390
think ultimately the the best techniques

00:39:45,550 --> 00:39:51,280
are going to involve getting a sort of

00:39:48,390 --> 00:39:53,920
let's call it a query intent classifier

00:39:51,280 --> 00:39:56,410
up front where the first step of this is

00:39:53,920 --> 00:39:59,290
going to be taking the query and mapping

00:39:56,410 --> 00:40:00,820
it into a decision tree about how to

00:39:59,290 --> 00:40:02,800
actually choose the

00:40:00,820 --> 00:40:04,510
relative weights I didn't cover that in

00:40:02,800 --> 00:40:07,570
this talk but I think ultimately that

00:40:04,510 --> 00:40:09,580
decision is very important and it's

00:40:07,570 --> 00:40:12,490
gonna be very nuanced and changed on a

00:40:09,580 --> 00:40:14,050
per query basis okay great

00:40:12,490 --> 00:40:16,360
I think I can probably squeeze in one

00:40:14,050 --> 00:40:19,120
more question so I'm asked it on behalf

00:40:16,360 --> 00:40:23,130
of Senate who asks how does the vector

00:40:19,120 --> 00:40:28,630
proach fare against learning to rank

00:40:23,130 --> 00:40:30,790
it's a great question so there so

00:40:28,630 --> 00:40:32,080
ultimately they're sort of solving two

00:40:30,790 --> 00:40:33,370
problems so when you implement learning

00:40:32,080 --> 00:40:36,190
to rank you're building a ranking

00:40:33,370 --> 00:40:37,450
classifier that is learning attributes

00:40:36,190 --> 00:40:40,690
and then ranking based upon those

00:40:37,450 --> 00:40:43,930
attributes typically it also follows

00:40:40,690 --> 00:40:47,140
this rear rank approach like I showed as

00:40:43,930 --> 00:40:49,330
a suggestion for performance for from

00:40:47,140 --> 00:40:50,770
the dense vector search in the case of

00:40:49,330 --> 00:40:52,300
dense vector search the way I would

00:40:50,770 --> 00:40:54,520
think of it with these transformer

00:40:52,300 --> 00:40:56,620
language models they are they're

00:40:54,520 --> 00:40:58,570
essentially learning two things they're

00:40:56,620 --> 00:41:01,270
learning and understanding of language

00:40:58,570 --> 00:41:03,430
itself so the way that words and phrases

00:41:01,270 --> 00:41:06,070
interact how questions are asked and

00:41:03,430 --> 00:41:09,400
answered things like that which is

00:41:06,070 --> 00:41:11,920
generally transferable to most domains

00:41:09,400 --> 00:41:12,970
that you know exist in that language for

00:41:11,920 --> 00:41:14,680
example English or German or

00:41:12,970 --> 00:41:17,160
what-have-you and then separately

00:41:14,680 --> 00:41:19,810
they're learning the domain-specific

00:41:17,160 --> 00:41:23,470
terminology that is used in the data set

00:41:19,810 --> 00:41:25,540
that they're trained upon and so for in

00:41:23,470 --> 00:41:27,280
the former case the understanding of

00:41:25,540 --> 00:41:29,020
language itself is something that

00:41:27,280 --> 00:41:30,340
learning to rank typically is not going

00:41:29,020 --> 00:41:33,010
to get you learning to rank stip eclis

00:41:30,340 --> 00:41:35,770
more focused on engineered features that

00:41:33,010 --> 00:41:36,840
it can match on but to the degree that

00:41:35,770 --> 00:41:40,150
you get into domain-specific

00:41:36,840 --> 00:41:42,490
understanding there's going to be some

00:41:40,150 --> 00:41:43,870
overlap if you use any textual based

00:41:42,490 --> 00:41:45,400
features in your learn to rank algorithm

00:41:43,870 --> 00:41:48,430
they're probably both going to be sort

00:41:45,400 --> 00:41:50,200
of optimizing the weights in a very

00:41:48,430 --> 00:41:52,270
similar way but what learning to rank

00:41:50,200 --> 00:41:54,850
will get you beyond that is any other

00:41:52,270 --> 00:41:57,340
engineered features for example a Geo

00:41:54,850 --> 00:41:59,740
distance boosts or a category boost or

00:41:57,340 --> 00:42:01,510
maybe there's other features that you

00:41:59,740 --> 00:42:03,880
wouldn't necessarily learn from your

00:42:01,510 --> 00:42:06,340
language model then that matter in your

00:42:03,880 --> 00:42:07,870
domain may be a you know recency score

00:42:06,340 --> 00:42:09,600
or popularity boosts or something like

00:42:07,870 --> 00:42:11,290
that which isn't related to your text

00:42:09,600 --> 00:42:14,779
learning sharing is going to be able to

00:42:11,290 --> 00:42:16,279
optimize those non semantic features

00:42:14,779 --> 00:42:19,489
and give you better scoring windows

00:42:16,279 --> 00:42:21,619
better fantastic well thank you very

00:42:19,489 --> 00:42:23,179
much tre we're going to leave it there

00:42:21,619 --> 00:42:25,400
but I'm sure you'll be around to answer

00:42:23,179 --> 00:42:27,380
questions in the slack and there's also

00:42:25,400 --> 00:42:28,969
the breakout room we're going to take a

00:42:27,380 --> 00:42:32,799
quick break now and we'll be back in a

00:42:28,969 --> 00:42:32,799
few minutes with Tim Alison

00:42:38,530 --> 00:42:40,590

YouTube URL: https://www.youtube.com/watch?v=1EVRWQLufNE


