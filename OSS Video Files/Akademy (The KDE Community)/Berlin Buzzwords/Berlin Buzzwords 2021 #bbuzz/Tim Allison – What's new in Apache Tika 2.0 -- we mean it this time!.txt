Title: Tim Allison – What's new in Apache Tika 2.0 -- we mean it this time!
Publication date: 2021-06-29
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Apache Tika is used in big data document processing pipelines to extract text and metadata from numerous file formats. Text extraction is a critical component for search systems.  While work on 2.0 has been ongoing for years, the Tika team released 2.0.0-ALPHA in January and will release 2.0.0 before Buzzwords 2021. In addition to dramatically increased modularization, there are new components to improve scaling, integration and robustness. This talk will offer an overview of the changes in Tika 2.0 with a deep dive on the new tika-pipes module that enables synchronous and asynchronous fetching from numerous data sources (jdbc, fileshare, S3), parsing and then emitting to other endpoints (fileshare, S3, Solr, Elasticsearch, etc).

Speaker:
Tim Alisson – https://2021.berlinbuzzwords.de/member/tim-allison

More: https://2021.berlinbuzzwords.de/session/whats-new-apache-tika-20-we-mean-it-time
Captions: 
	00:00:07,120 --> 00:00:11,440
off i go to talk about apache tika 2.0

00:00:10,080 --> 00:00:13,599
the title of the talk was we mean at

00:00:11,440 --> 00:00:15,360
this time and we did

00:00:13,599 --> 00:00:18,080
but we did release at least a 2.0 beta

00:00:15,360 --> 00:00:21,840
and i'll talk about that shortly

00:00:18,080 --> 00:00:25,119
all right so uh

00:00:21,840 --> 00:00:28,400
tca is a overall project that

00:00:25,119 --> 00:00:30,560
deals with file processing

00:00:28,400 --> 00:00:32,640
it's a framework for file type detection

00:00:30,560 --> 00:00:34,320
and then extraction of text and metadata

00:00:32,640 --> 00:00:35,280
you send it bytes you get back uh

00:00:34,320 --> 00:00:36,640
normalized uh

00:00:35,280 --> 00:00:38,800
output across all of those different

00:00:36,640 --> 00:00:40,239
file formats um

00:00:38,800 --> 00:00:41,680
one of the things that got me into tikka

00:00:40,239 --> 00:00:43,280
are several of the things it's easy to

00:00:41,680 --> 00:00:44,960
add new file types for detection it's

00:00:43,280 --> 00:00:46,640
easy to add new parsers works

00:00:44,960 --> 00:00:48,079
recursively with embedded files which is

00:00:46,640 --> 00:00:50,160
really important for a lot of

00:00:48,079 --> 00:00:53,280
complex file types and it has a nice

00:00:50,160 --> 00:00:55,360
integration with tesseract ocr

00:00:53,280 --> 00:00:56,399
this is an example of files that can be

00:00:55,360 --> 00:00:59,120
embedded within

00:00:56,399 --> 00:01:00,399
files and i have seen this in the wild

00:00:59,120 --> 00:01:02,079
people do crazy things

00:01:00,399 --> 00:01:03,440
uh and it's important to have be able to

00:01:02,079 --> 00:01:04,960
have parsers that can understand these

00:01:03,440 --> 00:01:06,640
file formats and then

00:01:04,960 --> 00:01:08,479
be able to process all the different

00:01:06,640 --> 00:01:09,040
files that can be stuck inside of other

00:01:08,479 --> 00:01:11,360
files

00:01:09,040 --> 00:01:12,080
recursively all right so that's kind of

00:01:11,360 --> 00:01:14,159
a quick overview

00:01:12,080 --> 00:01:15,280
of tikka uh for those who aren't

00:01:14,159 --> 00:01:17,600
familiar with it

00:01:15,280 --> 00:01:19,600
uh so the outline of the focus with a

00:01:17,600 --> 00:01:21,439
status of where we are on tikka

00:01:19,600 --> 00:01:24,159
i'll talk about breaking changes in 2.0

00:01:21,439 --> 00:01:26,400
and talk about maven modularization

00:01:24,159 --> 00:01:28,479
a bit of motivation for why we added the

00:01:26,400 --> 00:01:31,280
tikka pipes module and then i'll talk

00:01:28,479 --> 00:01:32,079
go in great detail about tika pipes so

00:01:31,280 --> 00:01:34,960
off we go so

00:01:32,079 --> 00:01:35,759
status we released a 2.0.0 alpha uh

00:01:34,960 --> 00:01:37,920
january

00:01:35,759 --> 00:01:40,000
uh beta came out towards the end of may

00:01:37,920 --> 00:01:41,360
we were thinking about getting a 2.0

00:01:40,000 --> 00:01:43,360
out by the conference but that didn't

00:01:41,360 --> 00:01:44,479
happen all good progress is being made

00:01:43,360 --> 00:01:46,000
we want to make sure that everything is

00:01:44,479 --> 00:01:48,560
working well

00:01:46,000 --> 00:01:49,600
the other major part of the project is

00:01:48,560 --> 00:01:52,320
we recently added

00:01:49,600 --> 00:01:54,159
nicholas de paza di piazza as a

00:01:52,320 --> 00:01:56,479
committer and pmc member

00:01:54,159 --> 00:01:58,079
uh nicholas and i quite a bit on the

00:01:56,479 --> 00:01:58,640
tika pipes module and i'd like to thank

00:01:58,079 --> 00:02:00,079
him now

00:01:58,640 --> 00:02:01,759
publicly uh for all of this uh

00:02:00,079 --> 00:02:03,520
collaboration on that

00:02:01,759 --> 00:02:04,880
and for his many contributions to apache

00:02:03,520 --> 00:02:07,840
tiga

00:02:04,880 --> 00:02:08,879
all right so tika 2.0 breaking changes

00:02:07,840 --> 00:02:10,800
at a high level

00:02:08,879 --> 00:02:12,800
uh one is that if you have tesseract

00:02:10,800 --> 00:02:14,000
installed the pdf parser will call

00:02:12,800 --> 00:02:16,400
tesseract

00:02:14,000 --> 00:02:17,920
in auto mode by default so that means

00:02:16,400 --> 00:02:21,040
that if it doesn't find enough

00:02:17,920 --> 00:02:22,080
text on a page or if it finds bad text

00:02:21,040 --> 00:02:23,599
on a page

00:02:22,080 --> 00:02:26,720
it will run tesseract which will be

00:02:23,599 --> 00:02:28,400
surprising from a performance standpoint

00:02:26,720 --> 00:02:30,239
uh all of the parsers have now been

00:02:28,400 --> 00:02:32,000
maven modularized and moved around

00:02:30,239 --> 00:02:34,000
these are not jigsaw modularized yet but

00:02:32,000 --> 00:02:35,440
they are at least maven modularized

00:02:34,000 --> 00:02:37,760
and i'll talk about that in some detail

00:02:35,440 --> 00:02:39,040
in the next slides uh tikka server

00:02:37,760 --> 00:02:39,680
operates in spawn child mode and i'll

00:02:39,040 --> 00:02:41,599
talk about

00:02:39,680 --> 00:02:43,200
that in some slides and the metadata has

00:02:41,599 --> 00:02:46,080
been streamlined with a preference for

00:02:43,200 --> 00:02:48,080
uh standards like dublin core and so on

00:02:46,080 --> 00:02:49,200
okay so may even modularize all the

00:02:48,080 --> 00:02:52,480
things

00:02:49,200 --> 00:02:54,560
so tikka one point x has

00:02:52,480 --> 00:02:56,400
a massive directory with all of the

00:02:54,560 --> 00:02:57,200
different parsers all of the unit tests

00:02:56,400 --> 00:02:58,480
are

00:02:57,200 --> 00:03:00,959
all the test documents are in one

00:02:58,480 --> 00:03:03,519
directory uh it was getting unwieldy

00:03:00,959 --> 00:03:05,200
so we decided to break the parsers into

00:03:03,519 --> 00:03:08,319
three sub modules

00:03:05,200 --> 00:03:11,360
tikka parser standard which are the

00:03:08,319 --> 00:03:14,720
java only uh fairly lightweight

00:03:11,360 --> 00:03:15,680
uh parsers are in the antique pressure

00:03:14,720 --> 00:03:17,200
standard module

00:03:15,680 --> 00:03:18,800
t-comparisons extended or for those

00:03:17,200 --> 00:03:19,519
parsers which probably aren't used by a

00:03:18,800 --> 00:03:21,280
lot of people

00:03:19,519 --> 00:03:22,800
necessarily but they may require heavy

00:03:21,280 --> 00:03:25,040
dependencies

00:03:22,800 --> 00:03:26,000
external network calls to call external

00:03:25,040 --> 00:03:27,440
resources

00:03:26,000 --> 00:03:29,840
or they might require native libs like

00:03:27,440 --> 00:03:31,920
sqloid 3. then the third

00:03:29,840 --> 00:03:33,840
module is tikka parser's advanced and

00:03:31,920 --> 00:03:34,879
that is for super heavy dependencies

00:03:33,840 --> 00:03:37,360
like dl4j

00:03:34,879 --> 00:03:38,000
uh or other heavy-duty processing like

00:03:37,360 --> 00:03:39,760
text

00:03:38,000 --> 00:03:41,680
from bytes image recognition nlp that

00:03:39,760 --> 00:03:43,519
kind of stuff so that's kind of the

00:03:41,680 --> 00:03:44,159
breakdown of the three sub modules the

00:03:43,519 --> 00:03:46,560
new three

00:03:44,159 --> 00:03:48,400
sub modules of tica parsers uh we did

00:03:46,560 --> 00:03:50,720
integrate tesseract ocr back in

00:03:48,400 --> 00:03:51,760
up in tk parso's standard because that's

00:03:50,720 --> 00:03:53,599
um just

00:03:51,760 --> 00:03:54,799
feels like it's such a fundamental thing

00:03:53,599 --> 00:03:56,959
to have available

00:03:54,799 --> 00:03:58,959
and we're not shipping uh tikka with

00:03:56,959 --> 00:04:01,200
tesseract users have to install it or at

00:03:58,959 --> 00:04:03,439
least pull it in with tikka server

00:04:01,200 --> 00:04:04,319
and docker so it's it's not something

00:04:03,439 --> 00:04:06,560
that um

00:04:04,319 --> 00:04:07,360
we're shipping uh with with it which is

00:04:06,560 --> 00:04:09,360
why we're

00:04:07,360 --> 00:04:10,959
uh incorporating it into tco parser

00:04:09,360 --> 00:04:12,319
standard

00:04:10,959 --> 00:04:15,280
all right so all of those so we've then

00:04:12,319 --> 00:04:17,680
subdivided even further into

00:04:15,280 --> 00:04:19,120
sub modules for the different file types

00:04:17,680 --> 00:04:20,639
uh you can see t comparison

00:04:19,120 --> 00:04:22,160
so that's ticker parser's classic or

00:04:20,639 --> 00:04:24,800
standard excuse me on the left

00:04:22,160 --> 00:04:26,479
um t capacitor is extended on the right

00:04:24,800 --> 00:04:29,600
with the scientific module

00:04:26,479 --> 00:04:32,479
advanced we have the deep learning

00:04:29,600 --> 00:04:34,479
we have captioning uh object recognition

00:04:32,479 --> 00:04:35,840
and a number of other things in advance

00:04:34,479 --> 00:04:37,759
we also now have thanks to louis

00:04:35,840 --> 00:04:41,120
mcgivern team a

00:04:37,759 --> 00:04:44,800
a integration for speech to text

00:04:41,120 --> 00:04:49,360
or automatic speech recognition uh which

00:04:44,800 --> 00:04:52,560
reaches out to amazon's asr

00:04:49,360 --> 00:04:55,199
platform okay so

00:04:52,560 --> 00:04:57,040
we the the key difference the key

00:04:55,199 --> 00:04:58,720
surprise for some folks will be that we

00:04:57,040 --> 00:05:00,320
are now only including t comparison

00:04:58,720 --> 00:05:03,039
standard in tikka app

00:05:00,320 --> 00:05:04,960
and also in tikka server so that if you

00:05:03,039 --> 00:05:06,720
want those extended uh sub modules for

00:05:04,960 --> 00:05:09,120
scientific format parsing or the

00:05:06,720 --> 00:05:10,800
sqlite 3 you'll need to add those but

00:05:09,120 --> 00:05:12,560
that makes for a lighter tikka app a

00:05:10,800 --> 00:05:13,919
lighter tika server fewer dependencies

00:05:12,560 --> 00:05:16,240
no network calls no

00:05:13,919 --> 00:05:17,440
http client none of that antica parts

00:05:16,240 --> 00:05:21,039
are standard

00:05:17,440 --> 00:05:21,919
um we've modulized tikka server uh in in

00:05:21,039 --> 00:05:25,280
the same way

00:05:21,919 --> 00:05:27,199
so see tikka server core is just the

00:05:25,280 --> 00:05:28,240
server with absolute with no parsers but

00:05:27,199 --> 00:05:30,000
then we also have the tika server

00:05:28,240 --> 00:05:30,400
standard which has the standard parsers

00:05:30,000 --> 00:05:32,400
in it

00:05:30,400 --> 00:05:34,240
and again users will have to add sub

00:05:32,400 --> 00:05:36,160
modules for scientific format parsing or

00:05:34,240 --> 00:05:37,919
sql e3

00:05:36,160 --> 00:05:39,039
we've modularized language detection so

00:05:37,919 --> 00:05:39,680
you only have to use one language

00:05:39,039 --> 00:05:41,120
detector

00:05:39,680 --> 00:05:42,720
detection module you don't have to pull

00:05:41,120 --> 00:05:44,880
all of them in and then pick

00:05:42,720 --> 00:05:46,080
only a small portion of that code we've

00:05:44,880 --> 00:05:47,280
done the same with tika server so we

00:05:46,080 --> 00:05:48,479
have tk server core

00:05:47,280 --> 00:05:50,320
you can server classic and we're

00:05:48,479 --> 00:05:51,680
starting to build out some tika server

00:05:50,320 --> 00:05:53,440
client

00:05:51,680 --> 00:05:55,520
we've modularized tk eval and let me

00:05:53,440 --> 00:05:58,479
take a short break on this

00:05:55,520 --> 00:06:00,080
so now you can drop the tkeval core jar

00:05:58,479 --> 00:06:01,600
in your class path for tk server

00:06:00,080 --> 00:06:03,840
and you will automatically get tikka

00:06:01,600 --> 00:06:05,280
eval stats run on your files

00:06:03,840 --> 00:06:07,360
this is useful when you want to do

00:06:05,280 --> 00:06:10,639
automatic detection of garbled text

00:06:07,360 --> 00:06:12,880
um the column on the left tig 114

00:06:10,639 --> 00:06:15,120
was the text that we pulled out of a

00:06:12,880 --> 00:06:17,199
file with t114

00:06:15,120 --> 00:06:19,280
language id chinese there were zero

00:06:17,199 --> 00:06:21,039
common chinese words in that

00:06:19,280 --> 00:06:22,560
and the top tokens if anybody can reach

00:06:21,039 --> 00:06:23,360
it for for those who can read chinese is

00:06:22,560 --> 00:06:25,199
all garbage

00:06:23,360 --> 00:06:26,960
the ticket 115 is like changing coding

00:06:25,199 --> 00:06:28,880
detection we're now pulling out german

00:06:26,960 --> 00:06:30,160
with much better oov or out of

00:06:28,880 --> 00:06:31,520
vocabulary rate

00:06:30,160 --> 00:06:33,039
so these are some statistics they're

00:06:31,520 --> 00:06:34,880
available in tk eval you can get those

00:06:33,039 --> 00:06:37,280
now easily in tikka server

00:06:34,880 --> 00:06:39,120
simply by dropping the eval core jar in

00:06:37,280 --> 00:06:41,039
your class path

00:06:39,120 --> 00:06:42,319
and you can use this in action uh or in

00:06:41,039 --> 00:06:44,000
production um

00:06:42,319 --> 00:06:46,160
in one particular file the text that was

00:06:44,000 --> 00:06:47,919
stored in the pdf came out as the top

00:06:46,160 --> 00:06:49,199
um the out of vocabulary on that is you

00:06:47,919 --> 00:06:51,520
know 100

00:06:49,199 --> 00:06:53,199
probably uh and then based on that

00:06:51,520 --> 00:06:54,560
statistic you can choose to run ocr in

00:06:53,199 --> 00:06:56,160
it um you can see that the eos error is

00:06:54,560 --> 00:06:57,680
not perfect but it's far better than the

00:06:56,160 --> 00:07:00,400
text that was extracted from that

00:06:57,680 --> 00:07:01,520
pdf all right so now i'm going to talk

00:07:00,400 --> 00:07:05,039
about

00:07:01,520 --> 00:07:07,840
why this matters and talk about

00:07:05,039 --> 00:07:08,639
why we've added tika pipes so thank you

00:07:07,840 --> 00:07:11,680
nick birch for

00:07:08,639 --> 00:07:13,199
crashing jvms at scale so

00:07:11,680 --> 00:07:14,560
there are the usual you know catch the

00:07:13,199 --> 00:07:15,199
exceptions kinds of things the parser

00:07:14,560 --> 00:07:16,800
had

00:07:15,199 --> 00:07:18,319
wasn't very happy with things but then

00:07:16,800 --> 00:07:19,919
they're also more catastrophic things

00:07:18,319 --> 00:07:21,360
like out of memory errors infinite loops

00:07:19,919 --> 00:07:23,599
memory leaks

00:07:21,360 --> 00:07:24,560
just code runaway forward processes all

00:07:23,599 --> 00:07:26,160
of those wonderful things that can

00:07:24,560 --> 00:07:28,479
happen with parsers

00:07:26,160 --> 00:07:30,000
and i used to feel awful about this but

00:07:28,479 --> 00:07:33,919
then i realized that tika really isn't

00:07:30,000 --> 00:07:35,440
alone and that uh parsers generally

00:07:33,919 --> 00:07:37,440
uh software generally has

00:07:35,440 --> 00:07:40,479
vulnerabilities but parsers in specific

00:07:37,440 --> 00:07:43,680
are extremely uh prone to

00:07:40,479 --> 00:07:45,599
uh security issues uh whether that's

00:07:43,680 --> 00:07:47,199
denial of service or

00:07:45,599 --> 00:07:48,639
remote code execution or other fun

00:07:47,199 --> 00:07:49,520
things kathleen fisher recently had a

00:07:48,639 --> 00:07:50,319
great keynote

00:07:49,520 --> 00:07:52,720
where she was talking about how

00:07:50,319 --> 00:07:54,560
dangerous parsons can be these stats are

00:07:52,720 --> 00:07:56,879
from her talk

00:07:54,560 --> 00:07:58,560
parsers are dangerous especially when

00:07:56,879 --> 00:07:59,440
you're running untrusted parsers on

00:07:58,560 --> 00:08:02,639
untrusted

00:07:59,440 --> 00:08:03,280
uh inputs bad things can happen um font

00:08:02,639 --> 00:08:06,639
parsers

00:08:03,280 --> 00:08:09,280
um four of the cves mentioned in this um

00:08:06,639 --> 00:08:10,479
exploit chain are font parsers uh so i

00:08:09,280 --> 00:08:13,120
found that rather amusing

00:08:10,479 --> 00:08:14,639
anyways parsing's dangerous this also

00:08:13,120 --> 00:08:15,840
came up recently rogue document might

00:08:14,639 --> 00:08:18,240
bring the process to a halt

00:08:15,840 --> 00:08:18,960
um that should never happen to you uh if

00:08:18,240 --> 00:08:22,319
it does

00:08:18,960 --> 00:08:24,000
um we should talk uh and you know with

00:08:22,319 --> 00:08:25,120
antiqua we have had these problems we

00:08:24,000 --> 00:08:26,720
have had

00:08:25,120 --> 00:08:28,160
infinite loops and other issues which we

00:08:26,720 --> 00:08:31,120
try to fix as we can

00:08:28,160 --> 00:08:32,719
but it's it it really is a systemic

00:08:31,120 --> 00:08:34,399
issue and are infinite

00:08:32,719 --> 00:08:35,760
loops really that bad yes they are they

00:08:34,399 --> 00:08:38,080
really really are

00:08:35,760 --> 00:08:39,919
for your cpus for the environment for

00:08:38,080 --> 00:08:41,200
everything they really do run for a long

00:08:39,919 --> 00:08:44,159
long time

00:08:41,200 --> 00:08:44,720
some might see infinitely all right um

00:08:44,159 --> 00:08:47,600
so

00:08:44,720 --> 00:08:48,240
in tikka we until we get verified secure

00:08:47,600 --> 00:08:50,080
parsers

00:08:48,240 --> 00:08:51,839
which aren't on the horizon in the near

00:08:50,080 --> 00:08:54,320
term we're trying to mitigate

00:08:51,839 --> 00:08:56,399
catastrophes as we can we're doing all

00:08:54,320 --> 00:08:56,720
sorts of things uh code reviews uh we're

00:08:56,399 --> 00:08:58,480
having

00:08:56,720 --> 00:08:59,600
we have a file format or fuzzy module

00:08:58,480 --> 00:09:01,120
which we just started we have two

00:08:59,600 --> 00:09:03,440
terabytes of regression corpora

00:09:01,120 --> 00:09:04,560
we have a mock parser uh which allows

00:09:03,440 --> 00:09:05,920
you to try out

00:09:04,560 --> 00:09:08,160
uh what would happen if a miss b8 and

00:09:05,920 --> 00:09:11,279
parser misbehave for you so here you

00:09:08,160 --> 00:09:12,080
you add the tikka core test jar to your

00:09:11,279 --> 00:09:13,760
class path

00:09:12,080 --> 00:09:15,120
and then you can send an xml file with a

00:09:13,760 --> 00:09:17,040
mock element in it

00:09:15,120 --> 00:09:19,760
and you can have the parser do things

00:09:17,040 --> 00:09:23,360
like throw an oom or do a system exit

00:09:19,760 --> 00:09:24,959
which can be quite exciting uh so to run

00:09:23,360 --> 00:09:26,800
tikka safely we have the fork parser

00:09:24,959 --> 00:09:28,399
which forks uh

00:09:26,800 --> 00:09:30,080
the parsing into another process tikka

00:09:28,399 --> 00:09:31,360
batch which does the same thing pretty

00:09:30,080 --> 00:09:33,200
much in a different way

00:09:31,360 --> 00:09:35,600
tika server i'll talk about in detail

00:09:33,200 --> 00:09:38,320
and now in 2x we have pipes and async

00:09:35,600 --> 00:09:38,640
which both uh use forked processes to do

00:09:38,320 --> 00:09:41,600
the

00:09:38,640 --> 00:09:43,839
the hard uh the the risky part of the

00:09:41,600 --> 00:09:45,519
parsing

00:09:43,839 --> 00:09:47,680
our overall goal is to be boring we want

00:09:45,519 --> 00:09:49,920
to be so boring that um i can stop

00:09:47,680 --> 00:09:51,920
giving talks on tica and it just works

00:09:49,920 --> 00:09:54,000
there are obviously will always be

00:09:51,920 --> 00:09:55,200
problems uh there will be out of memory

00:09:54,000 --> 00:09:57,519
errors there'll be infinite loops

00:09:55,200 --> 00:09:58,320
but should be able to control those by

00:09:57,519 --> 00:10:00,399
itself

00:09:58,320 --> 00:10:02,480
all right so evolution of tikka server

00:10:00,399 --> 00:10:05,279
is tk server safe

00:10:02,480 --> 00:10:07,440
well so in the beginning uh you had a

00:10:05,279 --> 00:10:10,480
tikka you had client calling tikka

00:10:07,440 --> 00:10:12,399
and when it went out uh with a crash uh

00:10:10,480 --> 00:10:14,000
it went there's nothing there to restart

00:10:12,399 --> 00:10:15,200
it uh people around the world had to

00:10:14,000 --> 00:10:18,000
restart their own

00:10:15,200 --> 00:10:19,920
uh tikka servers we added spawn child

00:10:18,000 --> 00:10:20,720
which will become default in tikka 2x

00:10:19,920 --> 00:10:24,320
which means that

00:10:20,720 --> 00:10:26,240
um a watcher process starts the server

00:10:24,320 --> 00:10:27,519
uh when the client sends a file it

00:10:26,240 --> 00:10:30,160
causes a problem uh

00:10:27,519 --> 00:10:30,800
t couldn't crash but the watcher will

00:10:30,160 --> 00:10:32,399
restart

00:10:30,800 --> 00:10:35,920
the process it will also look for

00:10:32,399 --> 00:10:39,040
timeouts um uh after memory errors and

00:10:35,920 --> 00:10:40,560
and crashes so um

00:10:39,040 --> 00:10:42,560
yeah so that that's where we that will

00:10:40,560 --> 00:10:44,320
now become default in tikka 2x we've had

00:10:42,560 --> 00:10:44,800
the spawn child mode in tega 1x for a

00:10:44,320 --> 00:10:46,399
while

00:10:44,800 --> 00:10:49,279
all right so the tika pipes module looks

00:10:46,399 --> 00:10:50,720
to solve a lot of these uh problems and

00:10:49,279 --> 00:10:52,959
make

00:10:50,720 --> 00:10:54,079
make make things much more uh robust

00:10:52,959 --> 00:10:56,399
scalable and

00:10:54,079 --> 00:10:58,399
safe the key thing is to isolate parsing

00:10:56,399 --> 00:10:59,440
into its own process

00:10:58,399 --> 00:11:01,519
we want to keep the iterator and the

00:10:59,440 --> 00:11:03,200
command module the client separate from

00:11:01,519 --> 00:11:05,200
uh the process that's uh doing the

00:11:03,200 --> 00:11:07,120
parsing um we want to allow

00:11:05,200 --> 00:11:08,320
for robust timing timeouts and we also

00:11:07,120 --> 00:11:08,800
want to allow for really long parse

00:11:08,320 --> 00:11:10,480
times

00:11:08,800 --> 00:11:11,839
so let's say you want to run ocr on 100

00:11:10,480 --> 00:11:12,800
page pdf you're not going to send that

00:11:11,839 --> 00:11:14,720
to tkka server

00:11:12,800 --> 00:11:16,079
because currently you have to keep the

00:11:14,720 --> 00:11:18,959
http connection open

00:11:16,079 --> 00:11:20,399
uh to get the response back so fetchers

00:11:18,959 --> 00:11:21,839
and emitters uh the notion here is that

00:11:20,399 --> 00:11:24,240
you fetch data from someplace do some

00:11:21,839 --> 00:11:25,920
processing and then emit the output

00:11:24,240 --> 00:11:27,440
uh fetcher looks like this uh here's a

00:11:25,920 --> 00:11:28,079
file system fetcher it has a name and it

00:11:27,440 --> 00:11:31,200
has a path

00:11:28,079 --> 00:11:33,920
to where it should pull files from

00:11:31,200 --> 00:11:35,839
uh this is kind of the output uh this is

00:11:33,920 --> 00:11:37,200
all recursive metadata parser output

00:11:35,839 --> 00:11:38,079
where you get the metadata and then the

00:11:37,200 --> 00:11:42,000
content is stuck

00:11:38,079 --> 00:11:44,480
in the xtiga content key

00:11:42,000 --> 00:11:45,680
we have metadata filters started in 1x

00:11:44,480 --> 00:11:47,200
so you can say

00:11:45,680 --> 00:11:48,720
from what comes out of tikka i want you

00:11:47,200 --> 00:11:50,800
to map that file that

00:11:48,720 --> 00:11:52,480
metadata name to something else and we

00:11:50,800 --> 00:11:54,240
have emitter so it says the first

00:11:52,480 --> 00:11:56,320
emitter is the solar one emitter and

00:11:54,240 --> 00:11:58,560
sends something off to solar or you can

00:11:56,320 --> 00:11:59,839
configure a file system emitter which

00:11:58,560 --> 00:12:02,240
will send the output

00:11:59,839 --> 00:12:04,000
file system this is what you send to

00:12:02,240 --> 00:12:05,279
teka to say here's my fetcher here's my

00:12:04,000 --> 00:12:06,720
emitter here's the key that i want to

00:12:05,279 --> 00:12:08,480
use to fetch the file here's my key that

00:12:06,720 --> 00:12:10,480
i want to admit the file you can

00:12:08,480 --> 00:12:11,680
inject your own user metadata and you

00:12:10,480 --> 00:12:13,360
can tell it what to do on a parse

00:12:11,680 --> 00:12:15,440
exception

00:12:13,360 --> 00:12:16,399
uh these this all of this stuff works

00:12:15,440 --> 00:12:18,639
with tka server

00:12:16,399 --> 00:12:20,160
with pipes and async pipes is you send

00:12:18,639 --> 00:12:21,920
something and get a response back async

00:12:20,160 --> 00:12:24,240
is you just send a bunch of responses

00:12:21,920 --> 00:12:25,920
uh and hope everything works out okay

00:12:24,240 --> 00:12:27,839
okay so the current state

00:12:25,920 --> 00:12:30,079
from a scalability standpoint is a

00:12:27,839 --> 00:12:31,600
client um let's say goes to s3 grabs a

00:12:30,079 --> 00:12:33,680
bunch of bytes sends those tikka gets

00:12:31,600 --> 00:12:36,240
the text back and sends those to solar

00:12:33,680 --> 00:12:37,279
scaling wise this is horrible because

00:12:36,240 --> 00:12:38,639
the client is

00:12:37,279 --> 00:12:41,120
sloshing all these bytes all over the

00:12:38,639 --> 00:12:45,040
place bad for the data center

00:12:41,120 --> 00:12:47,839
um in the factory emitter idea

00:12:45,040 --> 00:12:50,160
the client sends a json fetching the

00:12:47,839 --> 00:12:53,040
topple tatika which sends that json to

00:12:50,160 --> 00:12:55,120
a separate jbm which does which pulls

00:12:53,040 --> 00:12:56,959
the bytes out of s3 does the parsing and

00:12:55,120 --> 00:13:01,120
then sends those off to solar

00:12:56,959 --> 00:13:04,160
so when bad things happen um

00:13:01,120 --> 00:13:06,560
it will it will restart that jvm and

00:13:04,160 --> 00:13:07,279
antique will be good to go and also you

00:13:06,560 --> 00:13:09,760
can have

00:13:07,279 --> 00:13:11,279
obviously multiple uh worker jvms uh

00:13:09,760 --> 00:13:12,880
going at the same time so you can set a

00:13:11,279 --> 00:13:15,760
bunch of uh requests

00:13:12,880 --> 00:13:16,880
and all of those will work in parallel

00:13:15,760 --> 00:13:18,480
all right

00:13:16,880 --> 00:13:19,839
um and the great thing is then you can

00:13:18,480 --> 00:13:21,200
scale this across a cluster easily

00:13:19,839 --> 00:13:22,880
because the client's no longer pulling

00:13:21,200 --> 00:13:24,160
in all the bytes from a data source and

00:13:22,880 --> 00:13:24,639
then sending all those bytes across the

00:13:24,160 --> 00:13:27,040
cluster

00:13:24,639 --> 00:13:28,480
you're just sending json and then tk is

00:13:27,040 --> 00:13:30,560
pulling the appropriate

00:13:28,480 --> 00:13:31,600
bytes doing the processing and then

00:13:30,560 --> 00:13:33,600
forwarding those to

00:13:31,600 --> 00:13:35,600
uh to an endpoint or emitting those to

00:13:33,600 --> 00:13:36,480
an endpoint and we also have the notion

00:13:35,600 --> 00:13:37,600
of a

00:13:36,480 --> 00:13:40,480
fetch iterator which just kind of

00:13:37,600 --> 00:13:43,360
automates this so this is a file system

00:13:40,480 --> 00:13:45,360
pipes iterator which will iterate

00:13:43,360 --> 00:13:47,839
through files in a file share

00:13:45,360 --> 00:13:49,279
it will use the fetcher as specified and

00:13:47,839 --> 00:13:50,639
we'll also use the emitter so here's an

00:13:49,279 --> 00:13:53,199
example of configuring

00:13:50,639 --> 00:13:54,639
a little bit of xml to say go to this

00:13:53,199 --> 00:13:56,000
directory

00:13:54,639 --> 00:13:58,320
pull all of those files and send them

00:13:56,000 --> 00:14:00,399
off to solar

00:13:58,320 --> 00:14:01,760
next step we need more tests and

00:14:00,399 --> 00:14:03,839
documentation

00:14:01,760 --> 00:14:05,519
more tests and documentation um

00:14:03,839 --> 00:14:07,360
especially with uh dockerized

00:14:05,519 --> 00:14:09,279
mod s3 solar we're going to add open

00:14:07,360 --> 00:14:10,160
search and hopefully vespa fairly

00:14:09,279 --> 00:14:11,120
shortly

00:14:10,160 --> 00:14:13,120
we also need to figure out a way to

00:14:11,120 --> 00:14:13,440
package jars for tika server all right

00:14:13,120 --> 00:14:15,680
so

00:14:13,440 --> 00:14:17,440
in general i think i ran through that at

00:14:15,680 --> 00:14:19,600
twice the speed

00:14:17,440 --> 00:14:22,480
sorry about that but off we go so please

00:14:19,600 --> 00:14:25,680
join the fun uh here are some links

00:14:22,480 --> 00:14:25,680
and off we go to questions

00:14:26,480 --> 00:14:29,839
okay thank you very much tim i think

00:14:28,480 --> 00:14:33,440
this leaves uh

00:14:29,839 --> 00:14:35,440
quite some time for questions so please

00:14:33,440 --> 00:14:39,120
don't be shy type in your questions

00:14:35,440 --> 00:14:42,880
uh in to the chat next to the

00:14:39,120 --> 00:14:43,920
um to the stage uh my question tim would

00:14:42,880 --> 00:14:47,839
be

00:14:43,920 --> 00:14:49,120
let's say i run a enterprise search and

00:14:47,839 --> 00:14:52,240
i've used tikka

00:14:49,120 --> 00:14:54,720
for parsing documents uh so far

00:14:52,240 --> 00:14:57,760
so what would be what would the

00:14:54,720 --> 00:15:00,160
migration path look like yeah so

00:14:57,760 --> 00:15:02,639
i i understand that i have to look at

00:15:00,160 --> 00:15:05,440
all the modules that are available now

00:15:02,639 --> 00:15:06,639
um would this be the path or how would

00:15:05,440 --> 00:15:09,120
you approach it

00:15:06,639 --> 00:15:09,760
sure so it at the at the very least you

00:15:09,120 --> 00:15:12,000
can

00:15:09,760 --> 00:15:14,560
pretty much drop in tikka server

00:15:12,000 --> 00:15:16,079
standard for the old tica server

00:15:14,560 --> 00:15:17,680
with the one caveat that you will no

00:15:16,079 --> 00:15:19,360
longer have the um

00:15:17,680 --> 00:15:21,279
the science parser modules and the

00:15:19,360 --> 00:15:22,480
sqlite three and with it so you have to

00:15:21,279 --> 00:15:26,079
add those manually

00:15:22,480 --> 00:15:28,079
but the tica server standard should

00:15:26,079 --> 00:15:29,680
should be should act the same there are

00:15:28,079 --> 00:15:31,279
some differences

00:15:29,680 --> 00:15:33,680
for those who want to move into the

00:15:31,279 --> 00:15:36,000
pipes mode though um

00:15:33,680 --> 00:15:37,680
that's where you'll have to see if you

00:15:36,000 --> 00:15:38,480
know if we have a fetcher that meets

00:15:37,680 --> 00:15:41,440
your needs

00:15:38,480 --> 00:15:42,880
we currently cover s3 local file share

00:15:41,440 --> 00:15:44,880
we want to add some other

00:15:42,880 --> 00:15:46,560
fetchers and if there's an endpoint an

00:15:44,880 --> 00:15:48,560
emitter that covers your needs

00:15:46,560 --> 00:15:49,839
if that's the case then you can start

00:15:48,560 --> 00:15:52,079
experimenting with

00:15:49,839 --> 00:15:53,680
scaling that out making sure that it

00:15:52,079 --> 00:15:56,399
works in your environment

00:15:53,680 --> 00:15:57,199
and is performant i would encourage

00:15:56,399 --> 00:15:59,920
everybody

00:15:57,199 --> 00:16:00,720
to use the mock parser in uh on your dev

00:15:59,920 --> 00:16:03,199
system

00:16:00,720 --> 00:16:04,560
uh to see what happens when a parser

00:16:03,199 --> 00:16:06,160
calls a system exit

00:16:04,560 --> 00:16:08,399
which most parsers shouldn't we did have

00:16:06,160 --> 00:16:10,959
one that did at one point

00:16:08,399 --> 00:16:11,519
but that helps uh imitate what can

00:16:10,959 --> 00:16:14,240
happen

00:16:11,519 --> 00:16:15,920
with um when an om killer on the

00:16:14,240 --> 00:16:17,920
operating system decides that

00:16:15,920 --> 00:16:19,680
a parser is doing something horrible for

00:16:17,920 --> 00:16:23,199
the for the

00:16:19,680 --> 00:16:26,959
survival of the os so those are the

00:16:23,199 --> 00:16:29,120
that the in in short that's how i would

00:16:26,959 --> 00:16:32,399
proceed with with upgrading

00:16:29,120 --> 00:16:35,440
okay uh so now there are a few questions

00:16:32,399 --> 00:16:38,639
uh in the chat so the first one

00:16:35,440 --> 00:16:40,560
uh does ticker 2.0 also modularize

00:16:38,639 --> 00:16:42,450
the mime type detection or just the

00:16:40,560 --> 00:16:45,199
extraction and passing

00:16:42,450 --> 00:16:48,240
[Applause]

00:16:45,199 --> 00:16:50,320
it does um it's we don't have a cl a

00:16:48,240 --> 00:16:53,040
clean way of doing it um

00:16:50,320 --> 00:16:55,040
but yes so if you are using the sub sub

00:16:53,040 --> 00:16:57,839
modules so if you only want the

00:16:55,040 --> 00:16:59,279
tca microsoft office parsers for example

00:16:57,839 --> 00:17:00,959
um or if you let's say you only want the

00:16:59,279 --> 00:17:02,880
pdf parser but you want to be able to

00:17:00,959 --> 00:17:04,319
detect whether it's a powerpoint or an

00:17:02,880 --> 00:17:06,880
xlsx file

00:17:04,319 --> 00:17:08,319
um you would then need to you would you

00:17:06,880 --> 00:17:10,079
if you're not using the tika standard

00:17:08,319 --> 00:17:11,199
stuff which most people should

00:17:10,079 --> 00:17:13,199
but you're using those little sub

00:17:11,199 --> 00:17:13,760
modules you would have to include the

00:17:13,199 --> 00:17:15,919
tikka

00:17:13,760 --> 00:17:18,000
microsoft parsers module because that's

00:17:15,919 --> 00:17:20,160
what is now doing the detection

00:17:18,000 --> 00:17:21,600
for the subtype detection of office

00:17:20,160 --> 00:17:23,600
files

00:17:21,600 --> 00:17:25,439
so it is modularized it's not as neat as

00:17:23,600 --> 00:17:27,520
we would like because we want to have a

00:17:25,439 --> 00:17:30,400
lot of the stuff

00:17:27,520 --> 00:17:32,080
available in tikka core for at least the

00:17:30,400 --> 00:17:32,799
my magic detection which just looks at

00:17:32,080 --> 00:17:35,039
the first couple of

00:17:32,799 --> 00:17:35,919
the first thousand bytes of a file um

00:17:35,039 --> 00:17:39,919
but yes we have

00:17:35,919 --> 00:17:42,000
uh we have tried to um uh to modularize

00:17:39,919 --> 00:17:43,840
uh detection as we can

00:17:42,000 --> 00:17:45,360
again right sorry for people using

00:17:43,840 --> 00:17:47,760
standard tika standard

00:17:45,360 --> 00:17:48,480
you you won't notice any differences

00:17:47,760 --> 00:17:51,520
okay

00:17:48,480 --> 00:17:53,440
that sounds cool next question is um are

00:17:51,520 --> 00:17:54,240
you seeing parsers for any of the cloud

00:17:53,440 --> 00:17:57,520
provider

00:17:54,240 --> 00:18:00,880
documents being created so uh like

00:17:57,520 --> 00:18:03,280
google docs on google drive no

00:18:00,880 --> 00:18:05,440
uh we i haven't um the closest that

00:18:03,280 --> 00:18:07,360
comes to that is nicholas d piazza who i

00:18:05,440 --> 00:18:08,640
mentioned earlier i recently opened a

00:18:07,360 --> 00:18:12,480
ticket for um

00:18:08,640 --> 00:18:14,880
microsoft exchange 365 uh um

00:18:12,480 --> 00:18:16,000
onenote files uh because apparently the

00:18:14,880 --> 00:18:17,200
00:18:16,000 --> 00:18:19,520
onenotes are different from the regular

00:18:17,200 --> 00:18:22,160
onenotes but no i haven't seen

00:18:19,520 --> 00:18:22,640
folks contributing parsers or even heard

00:18:22,160 --> 00:18:26,000
of

00:18:22,640 --> 00:18:29,120
the need yet for those file formats

00:18:26,000 --> 00:18:32,640
but committers are standing by

00:18:29,120 --> 00:18:34,320
okay all right yeah okay

00:18:32,640 --> 00:18:36,960
uh i mean if i look at the google drive

00:18:34,320 --> 00:18:39,520
search maybe there's a place for that

00:18:36,960 --> 00:18:41,760
okay um so next question is uh will

00:18:39,520 --> 00:18:43,679
ticker to be a drop-in replacement for

00:18:41,760 --> 00:18:46,240
use of solar who use the tikka

00:18:43,679 --> 00:18:46,240
integration

00:18:47,360 --> 00:18:54,480
yes it should be yeah

00:18:50,720 --> 00:18:57,520
um for yes it it absolutely should be

00:18:54,480 --> 00:18:59,120
again with the i'm pretty sure you're

00:18:57,520 --> 00:19:00,799
not

00:18:59,120 --> 00:19:02,559
bringing in the sql light dependency so

00:19:00,799 --> 00:19:04,160
you're not going to lose that um

00:19:02,559 --> 00:19:05,600
and again this the same thing applies

00:19:04,160 --> 00:19:06,400
within with the science scientific

00:19:05,600 --> 00:19:08,080
parsers

00:19:06,400 --> 00:19:11,039
uh but it should be much cleaner because

00:19:08,080 --> 00:19:13,520
the tikka standard is not pulling in

00:19:11,039 --> 00:19:15,360
hdb components and other things that

00:19:13,520 --> 00:19:18,160
used to clash with solar

00:19:15,360 --> 00:19:19,840
i would still encourage you not to parse

00:19:18,160 --> 00:19:20,559
files in the same jvm that solar's

00:19:19,840 --> 00:19:22,799
running in

00:19:20,559 --> 00:19:26,080
that's just a recipe for disaster no

00:19:22,799 --> 00:19:26,080
matter what eric p will tell you

00:19:26,480 --> 00:19:32,240
yeah that's so muscle okay okay uh

00:19:29,919 --> 00:19:34,160
next question then maybe somehow related

00:19:32,240 --> 00:19:36,080
uh for archival data

00:19:34,160 --> 00:19:38,160
what are the pros and cons of parsing

00:19:36,080 --> 00:19:42,559
once and storing the past data

00:19:38,160 --> 00:19:42,559
versus re-parsing when re-indexing

00:19:44,559 --> 00:19:48,640
i recommend something that i've never

00:19:45,840 --> 00:19:52,080
seen happen in practice

00:19:48,640 --> 00:19:54,160
i recommend re-parsing uh and perhaps

00:19:52,080 --> 00:19:55,280
using the eval out of vocabulary stuff

00:19:54,160 --> 00:19:57,280
to figure out if you're doing a better

00:19:55,280 --> 00:19:59,120
job or a worse job

00:19:57,280 --> 00:20:00,720
as our parsers get better we are pulling

00:19:59,120 --> 00:20:02,640
out more text we're pulling out

00:20:00,720 --> 00:20:03,760
more reliable text and again we measure

00:20:02,640 --> 00:20:05,679
reliability with

00:20:03,760 --> 00:20:07,120
that outer vocabulary statistic so we

00:20:05,679 --> 00:20:09,440
know that we are getting better

00:20:07,120 --> 00:20:11,280
on most files there's always the chance

00:20:09,440 --> 00:20:14,240
when you move especially with

00:20:11,280 --> 00:20:15,600
pdfs that as you improve generally that

00:20:14,240 --> 00:20:17,520
you still might have regressions on a

00:20:15,600 --> 00:20:18,480
file here or there so from an archival

00:20:17,520 --> 00:20:19,919
standpoint

00:20:18,480 --> 00:20:21,440
i would want to be very careful about

00:20:19,919 --> 00:20:22,080
throwing out old parses where you might

00:20:21,440 --> 00:20:24,400
have had good

00:20:22,080 --> 00:20:25,919
good texts that you're not getting now

00:20:24,400 --> 00:20:28,320
um yeah

00:20:25,919 --> 00:20:30,080
so look into tk eval and and keep keep

00:20:28,320 --> 00:20:33,280
refreshing uh those parses

00:20:30,080 --> 00:20:35,360
as as you deem valuable

00:20:33,280 --> 00:20:49,840
to your need and if you have the budget

00:20:35,360 --> 00:20:49,840
to do it

00:20:57,200 --> 00:20:59,280

YouTube URL: https://www.youtube.com/watch?v=OoEHQUlu16w


