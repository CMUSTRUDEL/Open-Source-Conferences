Title: Gunnar Morling & Hans-Peter Grahsl – Change Data Streaming Patterns in Distributed Systems
Publication date: 2021-06-30
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Microservices are one of the big trends in software engineering of the last few years; organising business functionality in several self-contained, loosely coupled services helps teams to work efficiently, make the most suitable technical decisions, and react quickly to new business requirements.

In this session we'll discuss and showcase how open-source change data capture (CDC) with Debezium can help developers with typical challenges they often face when working on microservices. Come and join us to learn how to:

* Employ the outbox pattern for reliable, eventually consistent data exchange between microservices, without incurring unsafe dual writes or tight coupling

* Gradually extract microservices from existing monolithic applications, using CDC and the strangler fig pattern

* Coordinate long-running business transactions across multiple services using CDC-based saga orchestration, ensuring such activity gets consistently applied or aborted by all participating services

Speaker:
Gunnar Morling – 
Hans-Peter Grahsl – 

More: https://2021.berlinbuzzwords.de/session/change-data-streaming-patterns-distributed-systems
Captions: 
	00:00:06,399 --> 00:00:09,280
welcome to this

00:00:07,440 --> 00:00:10,719
session um thank you so much uh for

00:00:09,280 --> 00:00:12,240
having us um

00:00:10,719 --> 00:00:13,440
it's my first time at building buzzwords

00:00:12,240 --> 00:00:14,799
actually and i live in hamburg so it's

00:00:13,440 --> 00:00:16,320
not that far away but for some reason i

00:00:14,799 --> 00:00:18,800
never made it so i'm very happy

00:00:16,320 --> 00:00:20,160
to be here and the idea for this talk

00:00:18,800 --> 00:00:22,320
too is to

00:00:20,160 --> 00:00:23,359
talk about three common challenges which

00:00:22,320 --> 00:00:25,599
we encounter

00:00:23,359 --> 00:00:27,439
in distributed systems or microservices

00:00:25,599 --> 00:00:29,279
and patterns for overcoming

00:00:27,439 --> 00:00:32,079
those challenges and implementing those

00:00:29,279 --> 00:00:35,120
patterns with change data capture

00:00:32,079 --> 00:00:36,640
so in more depth um what can you expect

00:00:35,120 --> 00:00:38,239
i hope you see my slides by the way so

00:00:36,640 --> 00:00:39,600
we are talking ab or we are going to

00:00:38,239 --> 00:00:40,640
talk about three different patterns the

00:00:39,600 --> 00:00:43,040
first would be the

00:00:40,640 --> 00:00:45,360
outbox pattern which allows us to

00:00:43,040 --> 00:00:46,640
exchange messages between microservices

00:00:45,360 --> 00:00:48,000
in a safe way

00:00:46,640 --> 00:00:50,160
then we are going to talk about this

00:00:48,000 --> 00:00:50,719
trailer fig pattern which allows us to

00:00:50,160 --> 00:00:52,800
move

00:00:50,719 --> 00:00:54,079
to microservices coming from a world of

00:00:52,800 --> 00:00:55,360
model of a monolith

00:00:54,079 --> 00:00:57,199
and lastly we are going to talk about

00:00:55,360 --> 00:00:59,440
the saga pattern which allows us

00:00:57,199 --> 00:01:00,480
to um coordinate long-running

00:00:59,440 --> 00:01:04,000
transaction flows

00:01:00,480 --> 00:01:06,320
across multiple services and now when i

00:01:04,000 --> 00:01:08,080
say we i'm not here alone i'm here with

00:01:06,320 --> 00:01:09,760
my good friend hunt peter so who am i i

00:01:08,080 --> 00:01:11,520
work as a software engineer at reddit

00:01:09,760 --> 00:01:13,040
and i'm the lead of the division project

00:01:11,520 --> 00:01:15,040
which is an open source

00:01:13,040 --> 00:01:16,960
implementation of change there capture

00:01:15,040 --> 00:01:17,360
and i have the honor to be here today

00:01:16,960 --> 00:01:19,360
with

00:01:17,360 --> 00:01:21,119
hans peter grazel who is a technical

00:01:19,360 --> 00:01:23,200
trainer at mid economy

00:01:21,119 --> 00:01:24,400
and he also works as an independent

00:01:23,200 --> 00:01:26,479
consultant and

00:01:24,400 --> 00:01:28,000
software engineer so let's get started

00:01:26,479 --> 00:01:30,799
right into it

00:01:28,000 --> 00:01:32,320
um and before actually i dive or i talk

00:01:30,799 --> 00:01:34,960
or before we talk about those

00:01:32,320 --> 00:01:35,520
particular patterns um let me just give

00:01:34,960 --> 00:01:37,119
give you

00:01:35,520 --> 00:01:38,960
briefly an overview of what the change

00:01:37,119 --> 00:01:39,680
data capture is about and what dimension

00:01:38,960 --> 00:01:42,399
is about

00:01:39,680 --> 00:01:44,240
and the idea is real simple essentially

00:01:42,399 --> 00:01:46,720
you have a database and whenever there's

00:01:44,240 --> 00:01:48,560
an insert or an update or a delete um

00:01:46,720 --> 00:01:49,040
well you would like to react to those

00:01:48,560 --> 00:01:50,960
events

00:01:49,040 --> 00:01:52,479
so if there's a new customer created

00:01:50,960 --> 00:01:54,079
purchase order gets updated

00:01:52,479 --> 00:01:56,640
or something gets deleted you would like

00:01:54,079 --> 00:01:57,280
to react to that and well the canonical

00:01:56,640 --> 00:01:59,360
source

00:01:57,280 --> 00:02:01,439
for getting changes from the database is

00:01:59,360 --> 00:02:03,040
the databases transaction log

00:02:01,439 --> 00:02:05,360
each database has a transaction log and

00:02:03,040 --> 00:02:07,040
it's depend only log whenever something

00:02:05,360 --> 00:02:09,039
changes in the database it will append

00:02:07,040 --> 00:02:10,640
an event to this log and this is what

00:02:09,039 --> 00:02:12,480
the bsm uses as the change of end source

00:02:10,640 --> 00:02:14,400
so it taps into the transaction log

00:02:12,480 --> 00:02:16,160
extracts the changes and sends those

00:02:14,400 --> 00:02:18,000
events to consumers

00:02:16,160 --> 00:02:20,319
and typically this is done by apache

00:02:18,000 --> 00:02:21,120
kafka which allows us to have some nice

00:02:20,319 --> 00:02:22,879
decoupling

00:02:21,120 --> 00:02:24,480
between the event source which would be

00:02:22,879 --> 00:02:26,560
the database and the museum

00:02:24,480 --> 00:02:28,800
and the event consumers and then

00:02:26,560 --> 00:02:31,360
consumers can subscribe to those topics

00:02:28,800 --> 00:02:33,360
by default we would have one topic in

00:02:31,360 --> 00:02:34,959
kafka per table we are capturing

00:02:33,360 --> 00:02:36,560
and they could react to those change

00:02:34,959 --> 00:02:37,440
rates we could use those changements to

00:02:36,560 --> 00:02:40,000
update the cache

00:02:37,440 --> 00:02:40,560
a search index um to run streaming

00:02:40,000 --> 00:02:42,160
queries

00:02:40,560 --> 00:02:44,000
or to exchange data between

00:02:42,160 --> 00:02:44,720
microservices which is what we are going

00:02:44,000 --> 00:02:47,840
to focus

00:02:44,720 --> 00:02:50,000
on in this talk today so let's

00:02:47,840 --> 00:02:51,920
change the capture in a nutshell and i

00:02:50,000 --> 00:02:54,720
very briefly should talk about

00:02:51,920 --> 00:02:55,360
how those change event uh change events

00:02:54,720 --> 00:02:57,200
look like

00:02:55,360 --> 00:02:59,040
and in this case in case of dimension

00:02:57,200 --> 00:02:59,519
this is how such an event would look

00:02:59,040 --> 00:03:01,360
like

00:02:59,519 --> 00:03:03,440
this actually has three big parts like

00:03:01,360 --> 00:03:04,239
the old and the new state of the changed

00:03:03,440 --> 00:03:07,360
row

00:03:04,239 --> 00:03:08,159
some metadata and the operation type and

00:03:07,360 --> 00:03:10,879
timestamp

00:03:08,159 --> 00:03:12,800
so in the alt and new row state there we

00:03:10,879 --> 00:03:14,480
would have

00:03:12,800 --> 00:03:16,400
essentially a structure which resembles

00:03:14,480 --> 00:03:18,159
the tables we are capturing so for each

00:03:16,400 --> 00:03:21,440
column in our capture tables

00:03:18,159 --> 00:03:23,280
we would see one field in those parts of

00:03:21,440 --> 00:03:24,879
the message and now if this is an insert

00:03:23,280 --> 00:03:26,720
we only would have the after state for

00:03:24,879 --> 00:03:30,560
an update for instance we would have

00:03:26,720 --> 00:03:32,720
both so that's all the neural state

00:03:30,560 --> 00:03:34,080
in terms of metadata there's things like

00:03:32,720 --> 00:03:35,840
what's the table where this

00:03:34,080 --> 00:03:38,000
event is coming from what's the database

00:03:35,840 --> 00:03:38,799
uh maybe the query which caused this

00:03:38,000 --> 00:03:40,239
change um

00:03:38,799 --> 00:03:42,080
does this come from what we call an

00:03:40,239 --> 00:03:42,480
initial snapshot of the data and a few

00:03:42,080 --> 00:03:44,000
more

00:03:42,480 --> 00:03:46,080
metadata like this or positioning the

00:03:44,000 --> 00:03:46,720
log file transaction id and so on and

00:03:46,080 --> 00:03:48,319
lastly

00:03:46,720 --> 00:03:49,840
we have this additional metadata like

00:03:48,319 --> 00:03:51,680
operation type so is it an

00:03:49,840 --> 00:03:53,200
update is it an insert uh sorry is it an

00:03:51,680 --> 00:03:53,760
update or it is a great event for

00:03:53,200 --> 00:03:56,080
instance

00:03:53,760 --> 00:03:57,840
and the time step when did this change

00:03:56,080 --> 00:03:58,879
happen so those are the changements

00:03:57,840 --> 00:04:00,959
which are sent

00:03:58,879 --> 00:04:02,000
from the database or which are extracted

00:04:00,959 --> 00:04:04,560
from the database

00:04:02,000 --> 00:04:06,080
by the museum and sent to consumers via

00:04:04,560 --> 00:04:09,680
kafka so now let's see

00:04:06,080 --> 00:04:12,239
how we can um use those change events

00:04:09,680 --> 00:04:13,360
for implementing those distributed

00:04:12,239 --> 00:04:14,959
interaction patterns

00:04:13,360 --> 00:04:17,199
and the first one would be the outbox

00:04:14,959 --> 00:04:18,639
pattern and well

00:04:17,199 --> 00:04:20,239
for each of the patterns we are going to

00:04:18,639 --> 00:04:21,759
talk about briefly what is the

00:04:20,239 --> 00:04:23,759
problem which we would like to solve

00:04:21,759 --> 00:04:25,360
here right so and here in this case very

00:04:23,759 --> 00:04:27,759
often we have a situation where

00:04:25,360 --> 00:04:28,560
a service needs to update its own

00:04:27,759 --> 00:04:30,160
database

00:04:28,560 --> 00:04:31,600
and then at the same time it also would

00:04:30,160 --> 00:04:34,479
like to notify

00:04:31,600 --> 00:04:35,759
other services about uh this uh change

00:04:34,479 --> 00:04:37,520
which happened which happened

00:04:35,759 --> 00:04:38,880
so our database should be updated and we

00:04:37,520 --> 00:04:40,720
would like to send a message to other

00:04:38,880 --> 00:04:43,520
consumers maybe via kafka

00:04:40,720 --> 00:04:44,960
now the thing is um well we would like

00:04:43,520 --> 00:04:46,800
to do this consistently right so we

00:04:44,960 --> 00:04:48,639
don't we want to avoid the situation

00:04:46,800 --> 00:04:49,840
where we for instance do the database

00:04:48,639 --> 00:04:51,919
change but not

00:04:49,840 --> 00:04:53,360
the right of the message to kafka so

00:04:51,919 --> 00:04:55,440
that's the inconsistencies we would like

00:04:53,360 --> 00:04:57,040
to avoid

00:04:55,440 --> 00:04:58,560
and the traveling approach or the

00:04:57,040 --> 00:05:00,800
apparent approach maybe would be

00:04:58,560 --> 00:05:02,320
um well to just do what's called a dual

00:05:00,800 --> 00:05:04,240
write right so our service would

00:05:02,320 --> 00:05:05,680
try and update its database and at the

00:05:04,240 --> 00:05:07,759
same time it would send a message

00:05:05,680 --> 00:05:09,280
um via kafka to consumers but the

00:05:07,759 --> 00:05:12,479
problem is this what's called

00:05:09,280 --> 00:05:13,360
dual rides it's not reliable so as those

00:05:12,479 --> 00:05:15,199
two actions

00:05:13,360 --> 00:05:17,039
don't happen within one shared

00:05:15,199 --> 00:05:17,759
transaction it could happen one of them

00:05:17,039 --> 00:05:19,759
gets applied

00:05:17,759 --> 00:05:21,199
and the other one fails and now we end

00:05:19,759 --> 00:05:22,560
up with an inconsistent situation so

00:05:21,199 --> 00:05:24,080
maybe we already have received and

00:05:22,560 --> 00:05:26,639
persisted the purchase order

00:05:24,080 --> 00:05:27,520
in our database but then we forgot or we

00:05:26,639 --> 00:05:29,199
just failed

00:05:27,520 --> 00:05:30,880
to notify the shipment service about

00:05:29,199 --> 00:05:32,639
this order and obviously that's a bad

00:05:30,880 --> 00:05:34,240
situation so don't do dual rights

00:05:32,639 --> 00:05:35,840
now the question is how can we avoid

00:05:34,240 --> 00:05:37,280
this how can we overcome this

00:05:35,840 --> 00:05:39,199
and this is where the outbox pattern

00:05:37,280 --> 00:05:40,800
comes into the picture so the idea there

00:05:39,199 --> 00:05:42,720
is well if we cannot update multiple

00:05:40,800 --> 00:05:44,320
resources we always can update a single

00:05:42,720 --> 00:05:45,280
one right we always can go to our

00:05:44,320 --> 00:05:48,320
database

00:05:45,280 --> 00:05:49,600
and that's the idea so if a new request

00:05:48,320 --> 00:05:50,560
comes in let's say for placing a

00:05:49,600 --> 00:05:52,560
purchase order

00:05:50,560 --> 00:05:54,880
this service would update its internal

00:05:52,560 --> 00:05:56,639
table table model so like

00:05:54,880 --> 00:05:58,240
its order table or the lines table and

00:05:56,639 --> 00:06:00,240
so on and then

00:05:58,240 --> 00:06:02,240
within the same transaction it also

00:06:00,240 --> 00:06:03,919
would write a record to another table

00:06:02,240 --> 00:06:06,479
which we call the outbox table

00:06:03,919 --> 00:06:07,919
and this outbox table contains messages

00:06:06,479 --> 00:06:10,960
which are meant to be sent

00:06:07,919 --> 00:06:13,039
to external consumers

00:06:10,960 --> 00:06:14,800
then we would use division to capture

00:06:13,039 --> 00:06:16,319
the changes just from this outbox table

00:06:14,800 --> 00:06:17,759
so we would not capture changes from the

00:06:16,319 --> 00:06:18,560
actual business tables just from this

00:06:17,759 --> 00:06:20,800
outbox table

00:06:18,560 --> 00:06:21,759
and we would send them towards those

00:06:20,800 --> 00:06:23,840
consumers

00:06:21,759 --> 00:06:24,880
so how would this outbox table look like

00:06:23,840 --> 00:06:27,120
well in this case

00:06:24,880 --> 00:06:28,880
um it's a bit based on the ideas of a

00:06:27,120 --> 00:06:30,319
domain driven design so you have columns

00:06:28,880 --> 00:06:32,160
there like aggregate type

00:06:30,319 --> 00:06:33,680
which describes what's the kind of

00:06:32,160 --> 00:06:36,080
disaggregate is it in order

00:06:33,680 --> 00:06:38,160
is the customer is it i don't know a

00:06:36,080 --> 00:06:40,000
recipe or whatever your domain is about

00:06:38,160 --> 00:06:41,520
we have things like an aggregate id

00:06:40,000 --> 00:06:44,400
which comes in handy for

00:06:41,520 --> 00:06:46,240
routing events to make sure we ensure we

00:06:44,400 --> 00:06:48,479
have a consistent order of those events

00:06:46,240 --> 00:06:50,240
for the same aggregate and most

00:06:48,479 --> 00:06:50,880
importantly we have the payload and the

00:06:50,240 --> 00:06:52,880
payload

00:06:50,880 --> 00:06:54,639
that really is a structure which you

00:06:52,880 --> 00:06:56,560
define so it could be anything in this

00:06:54,639 --> 00:06:57,199
case it's a like json example a json

00:06:56,560 --> 00:06:58,720
structure

00:06:57,199 --> 00:07:01,280
and this is now the message which is

00:06:58,720 --> 00:07:03,120
sent towards your external consumers

00:07:01,280 --> 00:07:04,800
and now as you do this insert in your

00:07:03,120 --> 00:07:07,039
database as part

00:07:04,800 --> 00:07:08,880
of the same transaction in which you

00:07:07,039 --> 00:07:10,880
also updated your business tables

00:07:08,880 --> 00:07:12,639
you have uh well transactional

00:07:10,880 --> 00:07:14,800
guarantees and now

00:07:12,639 --> 00:07:16,479
all this is applied using at least one

00:07:14,800 --> 00:07:18,000
semantics so the beast will extract the

00:07:16,479 --> 00:07:19,919
changes from transaction log

00:07:18,000 --> 00:07:21,680
and in case of a failure it might go

00:07:19,919 --> 00:07:23,759
back and read a change which

00:07:21,680 --> 00:07:25,759
which it already read before a second

00:07:23,759 --> 00:07:26,800
time so consumers need to be prepared to

00:07:25,759 --> 00:07:28,560
see duplicates

00:07:26,800 --> 00:07:30,800
but we will never miss anything we will

00:07:28,560 --> 00:07:31,919
never end up with inconsistent situation

00:07:30,800 --> 00:07:34,479
where we have

00:07:31,919 --> 00:07:35,919
updated our database and not send

00:07:34,479 --> 00:07:37,280
something to consumers

00:07:35,919 --> 00:07:39,199
so that's the outbox pattern we will

00:07:37,280 --> 00:07:41,199
later on see how we can use this uh for

00:07:39,199 --> 00:07:44,000
more complex interactions

00:07:41,199 --> 00:07:44,720
for now let's talk about the strength

00:07:44,000 --> 00:07:46,800
pattern and

00:07:44,720 --> 00:07:49,039
hans peter can tell you more about that

00:07:46,800 --> 00:07:51,759
and spit it over to you

00:07:49,039 --> 00:07:54,000
all right thanks guna so uh the nice

00:07:51,759 --> 00:07:56,400
thing about the strangler fig pattern is

00:07:54,000 --> 00:07:58,400
that it can be explained based on a

00:07:56,400 --> 00:07:59,280
rather illustrative analogy we find in

00:07:58,400 --> 00:08:01,199
nature so

00:07:59,280 --> 00:08:02,319
what you see in the background of this

00:08:01,199 --> 00:08:05,440
image of this

00:08:02,319 --> 00:08:07,759
slide here is uh a tree structure

00:08:05,440 --> 00:08:09,440
and this tree itself is wrapped into

00:08:07,759 --> 00:08:10,639
some other species of plants and they

00:08:09,440 --> 00:08:13,840
are called stranglerfix

00:08:10,639 --> 00:08:15,599
and the interesting thing about them is

00:08:13,840 --> 00:08:17,280
their special growing behavior because

00:08:15,599 --> 00:08:19,039
those strangler figs

00:08:17,280 --> 00:08:20,560
they see it in the upper part of their

00:08:19,039 --> 00:08:22,720
host trees and then

00:08:20,560 --> 00:08:23,919
they grow from top to bottom until they

00:08:22,720 --> 00:08:26,240
see in the

00:08:23,919 --> 00:08:27,599
until they root in the soil themselves

00:08:26,240 --> 00:08:29,919
now

00:08:27,599 --> 00:08:32,240
what we can do is we can borrow from

00:08:29,919 --> 00:08:34,800
this idea we can mimic this special

00:08:32,240 --> 00:08:36,880
growing behavior in a technical context

00:08:34,800 --> 00:08:38,000
namely that of a software project

00:08:36,880 --> 00:08:40,640
migration

00:08:38,000 --> 00:08:42,800
so we want to show you uh how you can

00:08:40,640 --> 00:08:46,160
use this pattern again to

00:08:42,800 --> 00:08:48,959
uh migrate your applications and

00:08:46,160 --> 00:08:50,640
uh let's uh assume or i think many of

00:08:48,959 --> 00:08:52,720
you know that there are many full

00:08:50,640 --> 00:08:55,440
challenges involved when you

00:08:52,720 --> 00:08:56,240
are um tackled with the task to migrate

00:08:55,440 --> 00:08:58,240
an existing

00:08:56,240 --> 00:08:59,920
application into some newer form let's

00:08:58,240 --> 00:09:02,720
say we want to replace

00:08:59,920 --> 00:09:04,880
an old monolithic application uh with

00:09:02,720 --> 00:09:06,080
some more modern micro services based

00:09:04,880 --> 00:09:08,959
architecture

00:09:06,080 --> 00:09:11,839
and uh very rarely it is a good idea to

00:09:08,959 --> 00:09:15,120
try and do this migration in one big

00:09:11,839 --> 00:09:18,000
uh chunk um so writing everything in

00:09:15,120 --> 00:09:19,760
uh your micro service stack and then uh

00:09:18,000 --> 00:09:20,560
trying to have this hard cut over

00:09:19,760 --> 00:09:23,680
approach

00:09:20,560 --> 00:09:25,760
is oftentimes doomed to failure

00:09:23,680 --> 00:09:27,760
now instead we should do it in this

00:09:25,760 --> 00:09:29,680
strangler fixed way and try to find a

00:09:27,760 --> 00:09:32,959
way to smoothly and gradual

00:09:29,680 --> 00:09:35,680
gradually evolve our old system into

00:09:32,959 --> 00:09:36,080
the newer architecture and when we do

00:09:35,680 --> 00:09:38,080
that

00:09:36,080 --> 00:09:40,240
in a step-by-step way this basically

00:09:38,080 --> 00:09:40,720
means that we have to find a way such

00:09:40,240 --> 00:09:43,839
that

00:09:40,720 --> 00:09:46,959
both systems our monolith that we are

00:09:43,839 --> 00:09:48,959
gradually uh migrating over to services

00:09:46,959 --> 00:09:52,160
can co-exist with this

00:09:48,959 --> 00:09:53,680
new services now uh how might this

00:09:52,160 --> 00:09:56,480
actually work you might wonder

00:09:53,680 --> 00:09:57,920
and uh for that let's discuss a a

00:09:56,480 --> 00:09:59,920
concrete example here

00:09:57,920 --> 00:10:01,040
what we see is a fictional monolithic

00:09:59,920 --> 00:10:04,160
application in the

00:10:01,040 --> 00:10:07,279
in the domain of e-commerce here we see

00:10:04,160 --> 00:10:09,920
a a couple of modules uh three

00:10:07,279 --> 00:10:10,480
uh to pick to pre-precise and let's

00:10:09,920 --> 00:10:13,519
assume

00:10:10,480 --> 00:10:13,839
we want to now uh migrate this monolith

00:10:13,519 --> 00:10:17,040
uh

00:10:13,839 --> 00:10:18,720
into uh microservices so let's say we

00:10:17,040 --> 00:10:20,800
start this migration based on the

00:10:18,720 --> 00:10:23,360
customer module in the middle

00:10:20,800 --> 00:10:25,760
now in a first step we would introduce a

00:10:23,360 --> 00:10:27,839
proxy mechanism could be nginx or

00:10:25,760 --> 00:10:31,040
anything else that acts as a proxy

00:10:27,839 --> 00:10:31,760
and at that point in time all the proxy

00:10:31,040 --> 00:10:34,800
would do

00:10:31,760 --> 00:10:36,959
is uh just a pass through all read and

00:10:34,800 --> 00:10:40,959
write requests to the monolith

00:10:36,959 --> 00:10:44,320
as if nothing is uh there yet

00:10:40,959 --> 00:10:47,120
so then in a second step let's

00:10:44,320 --> 00:10:47,519
focus on the persistence layer so here

00:10:47,120 --> 00:10:49,839
uh

00:10:47,519 --> 00:10:51,680
we would need to find a way again we are

00:10:49,839 --> 00:10:54,480
talking about the customer module

00:10:51,680 --> 00:10:57,040
to bring all those customer related data

00:10:54,480 --> 00:10:59,279
into its own separate data store

00:10:57,040 --> 00:11:01,040
because this is how you typically do it

00:10:59,279 --> 00:11:04,079
you should avoid to have

00:11:01,040 --> 00:11:04,959
a database for your service or multiple

00:11:04,079 --> 00:11:08,240
services

00:11:04,959 --> 00:11:10,640
uh that is shared so we could uh in

00:11:08,240 --> 00:11:12,160
we could configure uh and use kafka

00:11:10,640 --> 00:11:14,160
connect we could configure

00:11:12,160 --> 00:11:15,440
at the bezium source connector that

00:11:14,160 --> 00:11:17,360
would for instance take

00:11:15,440 --> 00:11:18,640
all the existing data from a relational

00:11:17,360 --> 00:11:21,839
database system

00:11:18,640 --> 00:11:24,000
bring it into uh kafka topics uh

00:11:21,839 --> 00:11:25,279
and uh from there we could propagate it

00:11:24,000 --> 00:11:26,560
further with the so-called sync

00:11:25,279 --> 00:11:28,480
connector

00:11:26,560 --> 00:11:30,160
depending on what our data store at the

00:11:28,480 --> 00:11:32,399
target is we can do that

00:11:30,160 --> 00:11:34,399
with out of the box connectors just by

00:11:32,399 --> 00:11:36,560
means of configuration

00:11:34,399 --> 00:11:39,279
so the bcm will not only bring those

00:11:36,560 --> 00:11:41,920
existing data over to uh kafka topics

00:11:39,279 --> 00:11:45,200
but it will continuously listen to this

00:11:41,920 --> 00:11:46,800
to this transaction log and propagate

00:11:45,200 --> 00:11:49,040
all changes further

00:11:46,800 --> 00:11:51,040
once we have the customer relevant data

00:11:49,040 --> 00:11:52,560
over there we could shift our focus back

00:11:51,040 --> 00:11:55,519
to do the actual migration

00:11:52,560 --> 00:11:56,240
we could write our customer micro

00:11:55,519 --> 00:11:58,240
service

00:11:56,240 --> 00:11:59,760
and we could also do that in a stepwise

00:11:58,240 --> 00:12:00,320
fashion let's say we want to first

00:11:59,760 --> 00:12:03,040
introdu

00:12:00,320 --> 00:12:04,000
uh uh support just uh read based

00:12:03,040 --> 00:12:05,839
scenarios

00:12:04,000 --> 00:12:07,760
so we could do that and once this is

00:12:05,839 --> 00:12:09,600
ready we would then reconfigure the

00:12:07,760 --> 00:12:11,200
proxy and the proxy would make sure

00:12:09,600 --> 00:12:12,959
that it would route those client

00:12:11,200 --> 00:12:15,279
requests that are

00:12:12,959 --> 00:12:18,240
have originally targeted the monolith

00:12:15,279 --> 00:12:20,720
for reads over to our new microservice

00:12:18,240 --> 00:12:23,120
in the second step we could then say we

00:12:20,720 --> 00:12:24,560
want to extend the microservice and also

00:12:23,120 --> 00:12:27,200
support write

00:12:24,560 --> 00:12:29,279
workloads and again once this is done we

00:12:27,200 --> 00:12:32,399
reconfigure the proxy once more

00:12:29,279 --> 00:12:35,600
and then our microservice would serve

00:12:32,399 --> 00:12:36,000
all reads and rights that are relevant

00:12:35,600 --> 00:12:38,320
for

00:12:36,000 --> 00:12:40,639
for this customer service at the same

00:12:38,320 --> 00:12:43,519
time this means that we could

00:12:40,639 --> 00:12:44,000
more or less shut down or or get rid of

00:12:43,519 --> 00:12:47,040
this

00:12:44,000 --> 00:12:49,279
particular functionality in the monolith

00:12:47,040 --> 00:12:51,519
and then once we write to the

00:12:49,279 --> 00:12:52,240
microservice of course it will happen in

00:12:51,519 --> 00:12:54,959
practice

00:12:52,240 --> 00:12:56,800
that other modules in our monolith might

00:12:54,959 --> 00:12:57,440
need to be aware of the changes

00:12:56,800 --> 00:12:59,760
happening

00:12:57,440 --> 00:13:01,360
in this new micro service and its own

00:12:59,760 --> 00:13:03,040
separate database

00:13:01,360 --> 00:13:05,200
for that we would then need to find a

00:13:03,040 --> 00:13:06,160
way to propagate those changes back into

00:13:05,200 --> 00:13:08,639
the monolith

00:13:06,160 --> 00:13:09,519
and again we could then just configure

00:13:08,639 --> 00:13:12,720
another

00:13:09,519 --> 00:13:14,079
uh change data capture uh connector uh

00:13:12,720 --> 00:13:16,720
based on the bezium

00:13:14,079 --> 00:13:17,120
which uh basically brings the data back

00:13:16,720 --> 00:13:19,279
to

00:13:17,120 --> 00:13:20,240
the monolith of course first into kafka

00:13:19,279 --> 00:13:22,560
and then from there

00:13:20,240 --> 00:13:23,760
we have a sync connector uh let's

00:13:22,560 --> 00:13:27,040
briefly reflect

00:13:23,760 --> 00:13:29,920
what this approach would give us uh so

00:13:27,040 --> 00:13:31,440
as a first uh of course main benefit we

00:13:29,920 --> 00:13:34,720
can say that we can reach

00:13:31,440 --> 00:13:37,040
our ultimate goal of migrating a larger

00:13:34,720 --> 00:13:39,519
monolithic application

00:13:37,040 --> 00:13:41,440
uh in an incremental fashion this means

00:13:39,519 --> 00:13:42,480
we can take these baby steps we can

00:13:41,440 --> 00:13:45,040
extract

00:13:42,480 --> 00:13:46,240
service by a feature by feature or

00:13:45,040 --> 00:13:49,360
module by module

00:13:46,240 --> 00:13:50,079
into their own services um so uh this

00:13:49,360 --> 00:13:52,959
means uh

00:13:50,079 --> 00:13:54,480
inherently we can uh based on how we uh

00:13:52,959 --> 00:13:57,120
just discussed it support

00:13:54,480 --> 00:13:57,920
this coexistence and another thing is

00:13:57,120 --> 00:14:00,480
that we

00:13:57,920 --> 00:14:01,680
can pause our migration after more or

00:14:00,480 --> 00:14:04,160
less every step

00:14:01,680 --> 00:14:05,680
along the journey we could even say we

00:14:04,160 --> 00:14:07,920
want to completely stop

00:14:05,680 --> 00:14:10,000
the migration because maybe it was never

00:14:07,920 --> 00:14:11,040
the idea to migrate really the whole

00:14:10,000 --> 00:14:13,680
monolith but

00:14:11,040 --> 00:14:14,560
only parts of it so also this hybrid

00:14:13,680 --> 00:14:16,800
scenario

00:14:14,560 --> 00:14:18,639
is very well supported with that

00:14:16,800 --> 00:14:20,800
strangler fake pattern approach

00:14:18,639 --> 00:14:23,040
and depending on how we do it and if we

00:14:20,800 --> 00:14:25,600
get it right we can also support

00:14:23,040 --> 00:14:28,240
rollbacks if we need to reverse some

00:14:25,600 --> 00:14:30,959
functionality because of some issues

00:14:28,240 --> 00:14:31,600
that would happen we could then uh just

00:14:30,959 --> 00:14:34,160
go back

00:14:31,600 --> 00:14:35,360
uh and migrate the functionality back

00:14:34,160 --> 00:14:37,440
into the monolith

00:14:35,360 --> 00:14:38,800
again the bottom line here is that what

00:14:37,440 --> 00:14:41,760
we want to achieve uh

00:14:38,800 --> 00:14:42,240
is considerably lower migration risk

00:14:41,760 --> 00:14:44,480
than

00:14:42,240 --> 00:14:45,279
uh when we would contrast that with the

00:14:44,480 --> 00:14:48,880
big bang

00:14:45,279 --> 00:14:49,680
migration now uh when we take a second

00:14:48,880 --> 00:14:52,560
closer look

00:14:49,680 --> 00:14:53,279
on uh the cdc part of such a solution we

00:14:52,560 --> 00:14:55,600
might get a

00:14:53,279 --> 00:14:59,120
slightly more nuanced view remember in

00:14:55,600 --> 00:15:01,519
its uh original and most a basic form

00:14:59,120 --> 00:15:03,360
we learned today that cdc gives us this

00:15:01,519 --> 00:15:05,600
one-to-one replication

00:15:03,360 --> 00:15:07,760
bit of data between any two systems and

00:15:05,600 --> 00:15:09,600
we we do that basically on a record by

00:15:07,760 --> 00:15:13,440
record or change by change

00:15:09,600 --> 00:15:14,959
uh fashion also we get separate topics

00:15:13,440 --> 00:15:17,199
for separate tables

00:15:14,959 --> 00:15:18,480
usually from such cdc solutions as

00:15:17,199 --> 00:15:20,800
dibysium

00:15:18,480 --> 00:15:22,480
and this raises a couple of questions

00:15:20,800 --> 00:15:23,760
when we would transfer data in a

00:15:22,480 --> 00:15:26,240
one-to-one fashion

00:15:23,760 --> 00:15:28,160
aren't we somehow leaking our data model

00:15:26,240 --> 00:15:30,320
from either side and thereby pollute

00:15:28,160 --> 00:15:33,360
each other's domain model

00:15:30,320 --> 00:15:35,440
or what about uh if we want to break

00:15:33,360 --> 00:15:35,920
free from that restriction of only

00:15:35,440 --> 00:15:39,279
having

00:15:35,920 --> 00:15:41,920
uh the possibility to transfer records

00:15:39,279 --> 00:15:44,240
in a one-to-one fashion maybe we want to

00:15:41,920 --> 00:15:46,240
join records in flight maybe we want to

00:15:44,240 --> 00:15:47,519
uh in general build any kind of

00:15:46,240 --> 00:15:49,839
aggregate structure

00:15:47,519 --> 00:15:51,600
uh and the good news is that while all

00:15:49,839 --> 00:15:52,320
of these are legitimate concerns we can

00:15:51,600 --> 00:15:54,320
address them

00:15:52,320 --> 00:15:55,600
uh and the way we would do that is we

00:15:54,320 --> 00:15:58,160
would enhance this

00:15:55,600 --> 00:16:00,000
change data capture pipeline uh and as a

00:15:58,160 --> 00:16:02,880
first improvement we would uh

00:16:00,000 --> 00:16:04,959
for instance uh bring in single message

00:16:02,880 --> 00:16:06,720
transforms or smts for short

00:16:04,959 --> 00:16:09,440
what they allow us to do is we can

00:16:06,720 --> 00:16:10,480
manipulate the cdc payload in flight and

00:16:09,440 --> 00:16:13,519
we can do that

00:16:10,480 --> 00:16:16,480
with many out-of-the-box

00:16:13,519 --> 00:16:17,759
smt's uh just by means of configuration

00:16:16,480 --> 00:16:20,240
uh typical uh

00:16:17,759 --> 00:16:20,959
modifications are to just include or

00:16:20,240 --> 00:16:24,880
exclude

00:16:20,959 --> 00:16:27,279
a particular subset of the whole

00:16:24,880 --> 00:16:29,519
change event payload or we could rename

00:16:27,279 --> 00:16:30,160
fields we could change data types we

00:16:29,519 --> 00:16:32,160
could

00:16:30,160 --> 00:16:33,600
mask sensitive data we could fully

00:16:32,160 --> 00:16:36,240
encrypt data

00:16:33,600 --> 00:16:37,040
and many many more things also important

00:16:36,240 --> 00:16:40,240
to understand

00:16:37,040 --> 00:16:42,800
is that we can apply uh smds on both

00:16:40,240 --> 00:16:45,040
sides of our data flow we can use them

00:16:42,800 --> 00:16:46,720
on the way into kafka so on the source

00:16:45,040 --> 00:16:49,360
side or we can use them

00:16:46,720 --> 00:16:50,160
on the from the way out of kafka on the

00:16:49,360 --> 00:16:52,880
sink side

00:16:50,160 --> 00:16:54,480
wherever it is a better fit for our use

00:16:52,880 --> 00:16:57,040
case

00:16:54,480 --> 00:16:58,399
remember i said we want to act upon more

00:16:57,040 --> 00:17:00,639
than one record

00:16:58,399 --> 00:17:01,519
and a very commonly found use case here

00:17:00,639 --> 00:17:03,519
is that you need

00:17:01,519 --> 00:17:05,360
to somehow find a way to join

00:17:03,519 --> 00:17:06,559
parent-child relationships as they are

00:17:05,360 --> 00:17:09,439
commonly found in

00:17:06,559 --> 00:17:10,799
relational databases together uh and

00:17:09,439 --> 00:17:12,880
maybe you want them to

00:17:10,799 --> 00:17:14,640
have this coherent kind of aggregate

00:17:12,880 --> 00:17:15,760
consisting of the parent and all its

00:17:14,640 --> 00:17:18,000
child records

00:17:15,760 --> 00:17:19,280
and want to propagate that further

00:17:18,000 --> 00:17:21,439
towards your sink

00:17:19,280 --> 00:17:23,039
and this is where stream processes would

00:17:21,439 --> 00:17:25,039
come into play so we could write for

00:17:23,039 --> 00:17:28,079
instance a kafka streams application

00:17:25,039 --> 00:17:30,640
which does exactly that uh for namely

00:17:28,079 --> 00:17:32,880
joining records based on their foreign

00:17:30,640 --> 00:17:36,880
key relationship that they have

00:17:32,880 --> 00:17:38,559
in the data source here

00:17:36,880 --> 00:17:40,880
when you think about that when you

00:17:38,559 --> 00:17:42,640
extracted multiple services now

00:17:40,880 --> 00:17:44,320
you might reach the point where you

00:17:42,640 --> 00:17:46,320
might want to execute

00:17:44,320 --> 00:17:48,080
transactions across multiple services

00:17:46,320 --> 00:17:50,480
and this is in general not easy

00:17:48,080 --> 00:17:52,080
you try to avoid it as good as you can

00:17:50,480 --> 00:17:52,640
but sometimes there is just no way

00:17:52,080 --> 00:17:54,640
around

00:17:52,640 --> 00:17:57,120
and for that we need uh something more

00:17:54,640 --> 00:17:59,039
sophisticated and uh this brings me to

00:17:57,120 --> 00:18:01,120
the third and final pattern of today

00:17:59,039 --> 00:18:04,000
which is all about sagas so with that

00:18:01,120 --> 00:18:04,000
back to you guna

00:18:05,360 --> 00:18:08,480
all right thank you so much hans peter

00:18:08,080 --> 00:18:11,440
um

00:18:08,480 --> 00:18:12,480
yes exactly so i mean if we move to

00:18:11,440 --> 00:18:15,200
microservices

00:18:12,480 --> 00:18:16,880
from a world of coming from a monolith

00:18:15,200 --> 00:18:18,000
well we would like to as you say would

00:18:16,880 --> 00:18:21,679
like to avoid

00:18:18,000 --> 00:18:24,720
this need for multiple services

00:18:21,679 --> 00:18:26,480
to you know interact and to

00:18:24,720 --> 00:18:27,840
achieve on one consistent outcome but

00:18:26,480 --> 00:18:31,520
sometimes there's just no way

00:18:27,840 --> 00:18:34,559
around it um no matter how you try and

00:18:31,520 --> 00:18:36,480
desire to work on your domain um you

00:18:34,559 --> 00:18:38,080
know to really decouple your services at

00:18:36,480 --> 00:18:40,080
the end of the day there will be cases

00:18:38,080 --> 00:18:41,760
where multiple services need to

00:18:40,080 --> 00:18:43,919
collaborate and they need to

00:18:41,760 --> 00:18:45,360
achieve one consistent outcome so that's

00:18:43,919 --> 00:18:47,360
that's the problem so we have those

00:18:45,360 --> 00:18:48,559
what we could call long-running business

00:18:47,360 --> 00:18:50,240
transactions

00:18:48,559 --> 00:18:52,160
and now the problem is we don't

00:18:50,240 --> 00:18:54,799
typically have protocols like

00:18:52,160 --> 00:18:55,679
xa to face uh commit protocols which

00:18:54,799 --> 00:18:57,039
would

00:18:55,679 --> 00:18:58,720
have been used in the past for

00:18:57,039 --> 00:19:00,480
implementing such a logic which

00:18:58,720 --> 00:19:02,160
is distributed across multiple databases

00:19:00,480 --> 00:19:03,840
so typically we don't have

00:19:02,160 --> 00:19:05,520
such protocols in a world of

00:19:03,840 --> 00:19:07,440
microservices um

00:19:05,520 --> 00:19:09,440
so still we have this need for

00:19:07,440 --> 00:19:11,600
implementing such flows and of course

00:19:09,440 --> 00:19:13,200
again it's very important to think about

00:19:11,600 --> 00:19:14,559
failure cases right so everything is

00:19:13,200 --> 00:19:16,080
always easy and nice

00:19:14,559 --> 00:19:17,840
if you just think about the happy path

00:19:16,080 --> 00:19:19,520
but you also need to think about

00:19:17,840 --> 00:19:21,120
failure cases what happens if one of the

00:19:19,520 --> 00:19:23,760
services isn't available

00:19:21,120 --> 00:19:25,200
or if it fails um so how can we ensure

00:19:23,760 --> 00:19:27,520
we still have a consistent

00:19:25,200 --> 00:19:29,440
data outcome so that's where the saga

00:19:27,520 --> 00:19:31,440
pattern comes into the picture

00:19:29,440 --> 00:19:34,240
to give you one example so again we're

00:19:31,440 --> 00:19:36,000
in this domain of e-commerce application

00:19:34,240 --> 00:19:37,360
and let's say we have three services

00:19:36,000 --> 00:19:39,200
which are part of this

00:19:37,360 --> 00:19:40,720
interaction so we have an order service

00:19:39,200 --> 00:19:41,679
we have a customer service and we have a

00:19:40,720 --> 00:19:43,360
payment service

00:19:41,679 --> 00:19:44,720
and now what happens is this order

00:19:43,360 --> 00:19:46,960
service it receives a

00:19:44,720 --> 00:19:48,640
request for placing a new purchase order

00:19:46,960 --> 00:19:50,400
and now it needs to interact with those

00:19:48,640 --> 00:19:52,160
other two services so it needs to reach

00:19:50,400 --> 00:19:54,160
out to this customer service

00:19:52,160 --> 00:19:55,600
to do some sort of credit limit check so

00:19:54,160 --> 00:19:57,919
we would like to know

00:19:55,600 --> 00:19:59,679
do we want to accept this order or you

00:19:57,919 --> 00:20:01,200
know does this customer already have too

00:19:59,679 --> 00:20:02,080
many pending orders and we don't want to

00:20:01,200 --> 00:20:03,679
give him another

00:20:02,080 --> 00:20:06,240
pending order so that's this credit

00:20:03,679 --> 00:20:06,799
limit check and provided this is okay so

00:20:06,240 --> 00:20:08,640
we

00:20:06,799 --> 00:20:10,159
essentially we allocate some credit

00:20:08,640 --> 00:20:11,200
limit in the customer service for this

00:20:10,159 --> 00:20:12,799
customer

00:20:11,200 --> 00:20:14,240
provided this is okay we would like to

00:20:12,799 --> 00:20:16,320
go to the payment service and

00:20:14,240 --> 00:20:17,919
initiate the payment and then the order

00:20:16,320 --> 00:20:19,039
would be in some sort of accepted state

00:20:17,919 --> 00:20:21,200
so that's the happy path

00:20:19,039 --> 00:20:22,799
but now let's assume this payment step

00:20:21,200 --> 00:20:24,240
fails so for for whatever reason the

00:20:22,799 --> 00:20:25,600
payment doesn't go through maybe the

00:20:24,240 --> 00:20:27,360
credit card number is involved

00:20:25,600 --> 00:20:29,600
something like that well then we need to

00:20:27,360 --> 00:20:29,919
go back to the customer service and we

00:20:29,600 --> 00:20:32,559
need

00:20:29,919 --> 00:20:33,120
to compensate we need to undo what this

00:20:32,559 --> 00:20:34,640
has

00:20:33,120 --> 00:20:36,720
been doing before so in this case we

00:20:34,640 --> 00:20:38,159
would need to release this credit amount

00:20:36,720 --> 00:20:39,760
which had been allocated

00:20:38,159 --> 00:20:42,400
then it would be in some sort of order

00:20:39,760 --> 00:20:44,320
rejected state and the overall saga

00:20:42,400 --> 00:20:46,000
would be in consistent state so all the

00:20:44,320 --> 00:20:46,720
services have agreed on one final

00:20:46,000 --> 00:20:48,320
outcome

00:20:46,720 --> 00:20:50,559
what's important to understand is

00:20:48,320 --> 00:20:52,720
there's some reduced uh

00:20:50,559 --> 00:20:53,679
guarantees in terms of the classical

00:20:52,720 --> 00:20:55,200
transaction

00:20:53,679 --> 00:20:57,039
semantics so if you think about asset

00:20:55,200 --> 00:20:57,760
transactions what in particular what you

00:20:57,039 --> 00:20:59,679
don't have

00:20:57,760 --> 00:21:01,520
is isolation so if you think about it

00:20:59,679 --> 00:21:03,919
this customer service

00:21:01,520 --> 00:21:05,039
once it allocated this credit amount um

00:21:03,919 --> 00:21:08,720
this already would

00:21:05,039 --> 00:21:10,799
be visible to other clients

00:21:08,720 --> 00:21:11,919
also while this entire flow still is

00:21:10,799 --> 00:21:13,440
running and then maybe

00:21:11,919 --> 00:21:15,760
this payment check or this payment

00:21:13,440 --> 00:21:17,039
service you know it agrees or doesn't

00:21:15,760 --> 00:21:20,159
agree to the overflow

00:21:17,039 --> 00:21:21,520
and then we might want to undo this

00:21:20,159 --> 00:21:24,080
change in the customer service

00:21:21,520 --> 00:21:24,640
but for some time the change in the

00:21:24,080 --> 00:21:27,360
customer

00:21:24,640 --> 00:21:27,919
service already was visible to outs to

00:21:27,360 --> 00:21:31,679
external

00:21:27,919 --> 00:21:34,880
clients so we have reduced um isolation

00:21:31,679 --> 00:21:37,520
guarantees here so that's the uh

00:21:34,880 --> 00:21:40,240
that's an example so now how could this

00:21:37,520 --> 00:21:42,000
um implement it using change.capture and

00:21:40,240 --> 00:21:44,000
maybe you already already can guess it

00:21:42,000 --> 00:21:46,240
so we have this very powerful

00:21:44,000 --> 00:21:47,679
tool of the outbox pattern so the idea

00:21:46,240 --> 00:21:50,000
is to

00:21:47,679 --> 00:21:51,440
use the outbox pattern for coordinating

00:21:50,000 --> 00:21:54,000
this flow so each service

00:21:51,440 --> 00:21:55,360
would always go to its own database do

00:21:54,000 --> 00:21:57,600
whatever it needs to do

00:21:55,360 --> 00:21:59,200
in terms of updating its domain model

00:21:57,600 --> 00:22:01,840
updating its own local state and then it

00:21:59,200 --> 00:22:04,960
would send a message to the next service

00:22:01,840 --> 00:22:06,480
via its outbox table which then takes

00:22:04,960 --> 00:22:08,400
over the flow so the order service would

00:22:06,480 --> 00:22:10,720
send a message to the customer service

00:22:08,400 --> 00:22:11,840
this will reply then the order service

00:22:10,720 --> 00:22:13,679
would send another message to the

00:22:11,840 --> 00:22:14,000
payment service and this would reply and

00:22:13,679 --> 00:22:17,200
then

00:22:14,000 --> 00:22:19,280
this flow would be successfully finished

00:22:17,200 --> 00:22:22,080
or in case of a failure it would look

00:22:19,280 --> 00:22:24,720
like that so again the order service

00:22:22,080 --> 00:22:26,400
would do what it has to do it would then

00:22:24,720 --> 00:22:27,120
pass over control to the customer

00:22:26,400 --> 00:22:30,080
service

00:22:27,120 --> 00:22:31,600
um this says okay that's fine i can

00:22:30,080 --> 00:22:33,600
allocate this credit amount

00:22:31,600 --> 00:22:35,039
so the order service reaches out now or

00:22:33,600 --> 00:22:35,840
it sends a message to the payment

00:22:35,039 --> 00:22:38,320
service and now

00:22:35,840 --> 00:22:39,120
this payment fails so what we need to do

00:22:38,320 --> 00:22:41,039
now is we need to

00:22:39,120 --> 00:22:42,559
again go to the customer service and

00:22:41,039 --> 00:22:44,960
compensate this

00:22:42,559 --> 00:22:46,799
uh previously executed action so to

00:22:44,960 --> 00:22:49,440
release the credit amount

00:22:46,799 --> 00:22:51,440
so that's how if saga flow could look

00:22:49,440 --> 00:22:54,640
like in sort in in terms of a

00:22:51,440 --> 00:22:56,080
failed saga execution so

00:22:54,640 --> 00:22:57,679
to make it a bit more tangible let's

00:22:56,080 --> 00:22:58,480
take a look at a demo which i've

00:22:57,679 --> 00:23:00,640
prepared and

00:22:58,480 --> 00:23:02,400
um for the sake of time i'm not doing

00:23:00,640 --> 00:23:04,400
this live but i'm playing back a video

00:23:02,400 --> 00:23:05,520
so and i'm going to run you through

00:23:04,400 --> 00:23:08,400
what's happening here

00:23:05,520 --> 00:23:09,200
so there's a few things running here and

00:23:08,400 --> 00:23:11,919
um

00:23:09,200 --> 00:23:13,280
i i got all this running wire compose so

00:23:11,919 --> 00:23:14,960
we can see what's in there so we got

00:23:13,280 --> 00:23:17,200
zookeeper we got kafka

00:23:14,960 --> 00:23:18,400
for our message exchange we got kafka

00:23:17,200 --> 00:23:19,360
connect which has the division

00:23:18,400 --> 00:23:21,280
connectors

00:23:19,360 --> 00:23:22,880
and then we have for each of the

00:23:21,280 --> 00:23:26,080
services order

00:23:22,880 --> 00:23:26,720
customer and payment we have um a

00:23:26,080 --> 00:23:28,240
database

00:23:26,720 --> 00:23:29,919
which is a postcross database here for

00:23:28,240 --> 00:23:31,360
the sake of the example and we have the

00:23:29,919 --> 00:23:33,840
actual service implementation

00:23:31,360 --> 00:23:35,360
which in this case is a caucus-based

00:23:33,840 --> 00:23:37,440
microservice

00:23:35,360 --> 00:23:39,360
quadcores being a stack for building

00:23:37,440 --> 00:23:41,520
cloud native microservices

00:23:39,360 --> 00:23:43,600
so all those things are running the next

00:23:41,520 --> 00:23:45,760
step would be to register the debesium

00:23:43,600 --> 00:23:47,679
cdc connectors and we can take a look

00:23:45,760 --> 00:23:48,960
into the configuration of one of them

00:23:47,679 --> 00:23:50,320
and here

00:23:48,960 --> 00:23:52,640
so that's the instance of the division

00:23:50,320 --> 00:23:54,480
postgres connector it's going to the

00:23:52,640 --> 00:23:56,640
order to be host it's using those

00:23:54,480 --> 00:23:58,559
credentials order user and so on

00:23:56,640 --> 00:24:00,000
and then by means of those include

00:23:58,559 --> 00:24:01,440
filters we are limiting

00:24:00,000 --> 00:24:03,039
what we are capturing just to this

00:24:01,440 --> 00:24:04,400
outbox events table so that's the only

00:24:03,039 --> 00:24:06,080
thing we are interested in here

00:24:04,400 --> 00:24:08,080
and then we are using a transformation

00:24:06,080 --> 00:24:09,600
which is called the outbox image router

00:24:08,080 --> 00:24:12,080
which essentially makes sure that the

00:24:09,600 --> 00:24:15,039
events from this single outbox table

00:24:12,080 --> 00:24:16,559
are routed into different topics based

00:24:15,039 --> 00:24:17,279
on the aggregate type so that the

00:24:16,559 --> 00:24:19,520
consumer

00:24:17,279 --> 00:24:21,600
can subscribe to just the changes of a

00:24:19,520 --> 00:24:24,400
particular aggregate type

00:24:21,600 --> 00:24:25,360
so now we can place a purchase order and

00:24:24,400 --> 00:24:27,600
i'm

00:24:25,360 --> 00:24:28,559
using this very basic rest api in this

00:24:27,600 --> 00:24:31,120
order service

00:24:28,559 --> 00:24:33,360
and now i can take a look into the kafka

00:24:31,120 --> 00:24:34,240
topic or the kafka topics which are used

00:24:33,360 --> 00:24:36,240
for this message

00:24:34,240 --> 00:24:37,520
message exchange between the services so

00:24:36,240 --> 00:24:38,799
the first one would be this credit

00:24:37,520 --> 00:24:40,480
approval request topic

00:24:38,799 --> 00:24:42,240
so this has the messages which are sent

00:24:40,480 --> 00:24:45,440
from order to customer

00:24:42,240 --> 00:24:47,360
and now as i place orders

00:24:45,440 --> 00:24:49,120
purchase orders in the other servers we

00:24:47,360 --> 00:24:52,159
will see obviously that the

00:24:49,120 --> 00:24:53,919
message is sent to the customer

00:24:52,159 --> 00:24:55,600
uh to the customer service and we also

00:24:53,919 --> 00:24:57,919
can take a look into the response

00:24:55,600 --> 00:24:58,960
topic so now this those are the messages

00:24:57,919 --> 00:25:01,120
which are sent from

00:24:58,960 --> 00:25:02,240
customer back to order and again i'm

00:25:01,120 --> 00:25:04,400
placing another order

00:25:02,240 --> 00:25:06,480
and i get another response back from

00:25:04,400 --> 00:25:08,320
customer to order now all those are the

00:25:06,480 --> 00:25:08,720
proof in the approved status so that's

00:25:08,320 --> 00:25:10,559
good

00:25:08,720 --> 00:25:12,000
i could do the same for the exchange for

00:25:10,559 --> 00:25:13,520
the payment service but really it's the

00:25:12,000 --> 00:25:15,919
same so it's not so exciting

00:25:13,520 --> 00:25:16,960
instead let's take a look into what's

00:25:15,919 --> 00:25:18,640
called the

00:25:16,960 --> 00:25:20,400
saga execution lock and this is

00:25:18,640 --> 00:25:21,440
essentially a table which lives within

00:25:20,400 --> 00:25:23,760
the order service

00:25:21,440 --> 00:25:25,200
and this keeps track of the state of

00:25:23,760 --> 00:25:26,880
this saga

00:25:25,200 --> 00:25:28,559
and now well again i can use the

00:25:26,880 --> 00:25:30,480
dybusium to extract the changes from

00:25:28,559 --> 00:25:30,880
this saga execution log table and this

00:25:30,480 --> 00:25:32,400
is

00:25:30,880 --> 00:25:34,480
you know we do it here for debugging or

00:25:32,400 --> 00:25:35,520
exploration purposes so to see what's

00:25:34,480 --> 00:25:37,600
happening in this

00:25:35,520 --> 00:25:39,200
saga table there's two fields which are

00:25:37,600 --> 00:25:41,120
interesting there so this is the

00:25:39,200 --> 00:25:43,200
zaga status which describes the entire

00:25:41,120 --> 00:25:45,279
status of this of desire

00:25:43,200 --> 00:25:47,679
and then the step status which describes

00:25:45,279 --> 00:25:49,520
the status of the particular steps so

00:25:47,679 --> 00:25:51,760
now first of all this is started

00:25:49,520 --> 00:25:52,960
and then the credit approval step this

00:25:51,760 --> 00:25:54,559
has been started

00:25:52,960 --> 00:25:56,159
the credit approval step is in the

00:25:54,559 --> 00:25:59,279
succeeded state

00:25:56,159 --> 00:26:00,240
payment is started and lastly payment is

00:25:59,279 --> 00:26:03,440
succeeded

00:26:00,240 --> 00:26:05,600
and the overall saga is completed

00:26:03,440 --> 00:26:07,120
so that's a successful uh sagar

00:26:05,600 --> 00:26:07,760
execution now this order would be

00:26:07,120 --> 00:26:10,400
accepted

00:26:07,760 --> 00:26:11,600
you could process we could fulfill it

00:26:10,400 --> 00:26:13,760
but also let's take a look at

00:26:11,600 --> 00:26:15,279
a failed order now in this case um i

00:26:13,760 --> 00:26:15,679
place an order which fails because the

00:26:15,279 --> 00:26:17,360
credit

00:26:15,679 --> 00:26:19,039
card number is invalid for the sake of

00:26:17,360 --> 00:26:20,480
the example so how does that look like

00:26:19,039 --> 00:26:22,720
so again first of all it's in the

00:26:20,480 --> 00:26:26,080
started state the credit approval step

00:26:22,720 --> 00:26:27,360
that started um credit approval is

00:26:26,080 --> 00:26:29,840
succeeded and payment

00:26:27,360 --> 00:26:32,159
started and now this payment step fails

00:26:29,840 --> 00:26:34,080
due to this invalid credit card number

00:26:32,159 --> 00:26:36,480
and now we see this credit approval step

00:26:34,080 --> 00:26:38,000
which was successfully

00:26:36,480 --> 00:26:39,760
executed before this now this

00:26:38,000 --> 00:26:41,600
compensating step so which means

00:26:39,760 --> 00:26:43,279
we need to go back and we need to undo

00:26:41,600 --> 00:26:46,080
it um

00:26:43,279 --> 00:26:47,919
so um it's compensating and the overall

00:26:46,080 --> 00:26:50,799
saga that's aborting so we know

00:26:47,919 --> 00:26:52,080
we need to go back to this uh to some

00:26:50,799 --> 00:26:54,000
sort of importance thing

00:26:52,080 --> 00:26:56,000
and lastly the credit approval step

00:26:54,000 --> 00:26:57,919
that's is now compensated

00:26:56,000 --> 00:26:59,360
payment has failed and the overalls are

00:26:57,919 --> 00:27:00,159
like it's in the failed sales and of

00:26:59,360 --> 00:27:02,720
course we could now

00:27:00,159 --> 00:27:03,200
explore why is this in a failed state um

00:27:02,720 --> 00:27:06,240
but

00:27:03,200 --> 00:27:07,600
that's that's the basic idea um

00:27:06,240 --> 00:27:09,840
stopping the demo here for the sake of

00:27:07,600 --> 00:27:12,480
time there's some mornings to this

00:27:09,840 --> 00:27:13,840
we could talk about item potency here

00:27:12,480 --> 00:27:16,159
which is done about

00:27:13,840 --> 00:27:17,600
by means of keeping track of the ids of

00:27:16,159 --> 00:27:19,520
processed messages

00:27:17,600 --> 00:27:20,880
you can see this in the entire demo

00:27:19,520 --> 00:27:22,720
which we will share this

00:27:20,880 --> 00:27:24,960
link to at the end of the talk so you

00:27:22,720 --> 00:27:26,159
can see how this is implemented

00:27:24,960 --> 00:27:28,399
all right so let me go back to the

00:27:26,159 --> 00:27:30,080
slides and um

00:27:28,399 --> 00:27:31,840
we are pretty much done i'm just sending

00:27:30,080 --> 00:27:34,799
over once more to hans peter

00:27:31,840 --> 00:27:35,840
who will give us a wrap up and some

00:27:34,799 --> 00:27:38,159
summary of this talk

00:27:35,840 --> 00:27:39,919
over to you on speedo thank you guna for

00:27:38,159 --> 00:27:42,000
the nice demo so yeah we are about to

00:27:39,919 --> 00:27:44,480
wrap this up so i think the

00:27:42,000 --> 00:27:46,399
uh most important takeaway uh for this

00:27:44,480 --> 00:27:49,440
session today is hopefully that we

00:27:46,399 --> 00:27:51,520
we were able to show you that uh cdc is

00:27:49,440 --> 00:27:52,159
a really powerful tool in fact this it's

00:27:51,520 --> 00:27:54,320
such a

00:27:52,159 --> 00:27:56,480
fundamental tool when it comes to

00:27:54,320 --> 00:27:58,640
building uh event-driven architectures

00:27:56,480 --> 00:28:01,840
or or microservices

00:27:58,640 --> 00:28:04,320
and we should really uh use it

00:28:01,840 --> 00:28:06,240
beyond those uh simplistic one-to-one

00:28:04,320 --> 00:28:08,159
replication kind of use cases that you

00:28:06,240 --> 00:28:10,720
very often see so i hope this

00:28:08,159 --> 00:28:12,320
uh explanation of uh these three

00:28:10,720 --> 00:28:15,279
patterns namely outbox

00:28:12,320 --> 00:28:16,159
strangler fig and saga patterns uh gave

00:28:15,279 --> 00:28:18,799
you some some

00:28:16,159 --> 00:28:19,840
inspiration some ideas how you could

00:28:18,799 --> 00:28:23,039
make use of

00:28:19,840 --> 00:28:26,000
these uh mechanisms in your own

00:28:23,039 --> 00:28:28,080
uh real world applications uh also if

00:28:26,000 --> 00:28:30,240
you want to dig deeper uh i would really

00:28:28,080 --> 00:28:33,039
encourage you to try out a

00:28:30,240 --> 00:28:33,679
cdc for yourself uh based on the bc

00:28:33,039 --> 00:28:35,520
which is

00:28:33,679 --> 00:28:37,279
among the leading change data capture

00:28:35,520 --> 00:28:40,480
solutions fully open source

00:28:37,279 --> 00:28:43,039
uh in addition to just being a cdc

00:28:40,480 --> 00:28:45,360
it comes with uh handy smds very

00:28:43,039 --> 00:28:47,200
convenient extensions such as

00:28:45,360 --> 00:28:49,679
outbox pattern support that we discussed

00:28:47,200 --> 00:28:50,320
today uh and in that regard actually we

00:28:49,679 --> 00:28:52,880
would have

00:28:50,320 --> 00:28:53,919
a a call to action for every one of you

00:28:52,880 --> 00:28:56,399
so if you

00:28:53,919 --> 00:28:57,360
would like to see um support for the

00:28:56,399 --> 00:28:59,039
saga pattern

00:28:57,360 --> 00:29:00,480
similar to what we have seen in in the

00:28:59,039 --> 00:29:03,440
demo

00:29:00,480 --> 00:29:03,919
from guna please uh more or less let us

00:29:03,440 --> 00:29:05,679
know

00:29:03,919 --> 00:29:08,159
via the project's mailing list which you

00:29:05,679 --> 00:29:10,000
can find on the bcm dot io

00:29:08,159 --> 00:29:11,440
with that uh we want to point you just

00:29:10,000 --> 00:29:14,240
to some further resources

00:29:11,440 --> 00:29:16,799
again uh two very elaborate uh blog

00:29:14,240 --> 00:29:19,279
posts about the outbox pattern and saga

00:29:16,799 --> 00:29:20,480
pattern uh step-by-step instructions the

00:29:19,279 --> 00:29:22,000
demo repository

00:29:20,480 --> 00:29:24,240
should be great because the demo you

00:29:22,000 --> 00:29:25,039
have seen today you can try it out on

00:29:24,240 --> 00:29:27,039
your own so

00:29:25,039 --> 00:29:28,720
everything is there for you to run the

00:29:27,039 --> 00:29:32,240
demos uh

00:29:28,720 --> 00:29:35,120
at uh the convenience uh that uh

00:29:32,240 --> 00:29:36,320
you can use it from your home just by

00:29:35,120 --> 00:29:39,520
using docker

00:29:36,320 --> 00:29:41,919
compose uh which is all you basically

00:29:39,520 --> 00:29:43,600
need to run those demos uh with that we

00:29:41,919 --> 00:29:44,880
want to thank you for joining again

00:29:43,600 --> 00:29:48,320
thanks for taking the time

00:29:44,880 --> 00:29:50,000
we would appreciate uh uh to uh to

00:29:48,320 --> 00:29:51,440
get some questions and hopefully be able

00:29:50,000 --> 00:29:54,000
to answer them um

00:29:51,440 --> 00:29:58,080
and yeah let's do the live q a right now

00:29:54,000 --> 00:29:58,080
or discuss later in the breakout channel

00:30:06,840 --> 00:30:09,840
thanks

00:30:16,000 --> 00:30:18,080

YouTube URL: https://www.youtube.com/watch?v=CLv2EcYnr2g


