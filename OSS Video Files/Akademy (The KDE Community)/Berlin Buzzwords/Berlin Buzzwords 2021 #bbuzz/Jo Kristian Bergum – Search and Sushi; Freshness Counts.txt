Title: Jo Kristian Bergum – Search and Sushi; Freshness Counts
Publication date: 2021-07-01
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Search and ranking over datasets which are constantly evolving in real time is a challenging problem at scale. Updating the documents in the index with real time signals like inventory status and click through rates can improve the search experience considerably. The fields which needs to be updated at scale can be used as hard filters as part of the retrieval strategy or as another ranking signal. 

In this talk we’ll present an overview of the real time indexing architecture of Vespa.ai which supports true in-place partial updates of searchable fields, including tensor fields. We also compare the real time indexing architecture of Vespa.ai with search engines built on the Apache Lucene library.

Speaker:
Jo Kristian Bergum – https://2021.berlinbuzzwords.de/member/jo-kristian-bergum

More: https://2021.berlinbuzzwords.de/session/search-and-sushi-freshness-counts
Captions: 
	00:00:07,680 --> 00:00:11,920
welcome to this talk on search and sushi

00:00:10,240 --> 00:00:14,559
freshness counts my name is joe

00:00:11,920 --> 00:00:16,800
christian burgum and i work for

00:00:14,559 --> 00:00:18,240
various media on the vespa team um also

00:00:16,800 --> 00:00:20,720
on twitter

00:00:18,240 --> 00:00:21,439
um yeah so this talk uh i'll give a

00:00:20,720 --> 00:00:24,960
brief

00:00:21,439 --> 00:00:27,199
uh introduction to to vespa and

00:00:24,960 --> 00:00:28,320
searching and ranking over evolving data

00:00:27,199 --> 00:00:31,519
sets

00:00:28,320 --> 00:00:32,399
and i'll do um a little bit of a deep

00:00:31,519 --> 00:00:34,800
dive into

00:00:32,399 --> 00:00:37,440
the vespa real-time indexing

00:00:34,800 --> 00:00:37,440
architecture

00:00:37,840 --> 00:00:41,680
so um what is vespa really um it's the

00:00:40,640 --> 00:00:44,000
open source

00:00:41,680 --> 00:00:45,200
platform for low latency computations

00:00:44,000 --> 00:00:47,680
over large

00:00:45,200 --> 00:00:48,719
evolving data and it's a partially tool

00:00:47,680 --> 00:00:52,079
licensed

00:00:48,719 --> 00:00:54,239
and it has a rich set of features

00:00:52,079 --> 00:00:56,320
you can search filter and rank both

00:00:54,239 --> 00:00:59,359
structured and unstructured data

00:00:56,320 --> 00:01:01,920
it also has vector search capabilities

00:00:59,359 --> 00:01:02,719
to approximate nearest neighbor search

00:01:01,920 --> 00:01:04,479
you can also

00:01:02,719 --> 00:01:07,119
combine approximate nearest neighbors

00:01:04,479 --> 00:01:09,360
search with regular search filters or

00:01:07,119 --> 00:01:12,640
rankings so you can have a hybrid

00:01:09,360 --> 00:01:15,280
retrieval and ranking pipeline it's

00:01:12,640 --> 00:01:16,799
very scalable it's also very fast so

00:01:15,280 --> 00:01:20,080
it's cost effective

00:01:16,799 --> 00:01:21,280
and also has advanced multi-phase

00:01:20,080 --> 00:01:24,320
ranking and retrieval

00:01:21,280 --> 00:01:26,320
and also tensors are first-class

00:01:24,320 --> 00:01:27,439
citizens in the in the vespa document

00:01:26,320 --> 00:01:31,200
model

00:01:27,439 --> 00:01:34,560
and has built-in support for importing

00:01:31,200 --> 00:01:36,079
machine learning models from a wide

00:01:34,560 --> 00:01:38,159
variety of popular

00:01:36,079 --> 00:01:40,159
machine learning frameworks like

00:01:38,159 --> 00:01:43,200
tensorflow pytorch

00:01:40,159 --> 00:01:46,880
on x and also the gbdt family

00:01:43,200 --> 00:01:49,840
executes and light gbm and in this talk

00:01:46,880 --> 00:01:52,240
i'll focus on the real-time indexing

00:01:49,840 --> 00:01:55,680
support investbar and also the true

00:01:52,240 --> 00:01:58,640
partial update uh support um

00:01:55,680 --> 00:01:59,680
but first let's look at the history uh

00:01:58,640 --> 00:02:04,320
of wespa

00:01:59,680 --> 00:02:06,560
so going back actually to 1998

00:02:04,320 --> 00:02:08,959
the team here in trondheim you used

00:02:06,560 --> 00:02:11,120
around all the web.com which was a a

00:02:08,959 --> 00:02:14,959
large web search engine

00:02:11,120 --> 00:02:16,319
competing with google inktomy altavista

00:02:14,959 --> 00:02:19,360
and other web search players

00:02:16,319 --> 00:02:21,599
at that time in 2003

00:02:19,360 --> 00:02:22,480
yaoi decided that they also wanted to

00:02:21,599 --> 00:02:25,440
build

00:02:22,480 --> 00:02:26,560
web search technology um they wanted to

00:02:25,440 --> 00:02:28,800
get rid of google

00:02:26,560 --> 00:02:29,840
so they went out and bought inktomy

00:02:28,800 --> 00:02:33,360
overture

00:02:29,840 --> 00:02:35,519
after vista and also this office

00:02:33,360 --> 00:02:37,519
who was powering the web search of

00:02:35,519 --> 00:02:39,840
oldweb.com

00:02:37,519 --> 00:02:40,879
and the team here in trondheim we were

00:02:39,840 --> 00:02:43,120
set up to

00:02:40,879 --> 00:02:44,640
build the next generation vertical

00:02:43,120 --> 00:02:46,000
search platform and that's where the

00:02:44,640 --> 00:02:49,040
name vespa is from

00:02:46,000 --> 00:02:52,319
and west was born in 2004

00:02:49,040 --> 00:02:53,440
fast forward to 2010 uh vespa 5x came

00:02:52,319 --> 00:02:55,760
out with

00:02:53,440 --> 00:02:56,720
improved and new real-time indexing

00:02:55,760 --> 00:02:59,760
support

00:02:56,720 --> 00:03:02,400
and finally in 2017

00:02:59,760 --> 00:03:04,480
uh we open sourced vespa to the world uh

00:03:02,400 --> 00:03:06,640
using an apache 2o license

00:03:04,480 --> 00:03:08,159
and all the development of the vespa

00:03:06,640 --> 00:03:11,120
engine is now in the open

00:03:08,159 --> 00:03:12,000
on github and this year we also

00:03:11,120 --> 00:03:13,440
announced

00:03:12,000 --> 00:03:15,680
the availability or the general

00:03:13,440 --> 00:03:16,959
availability of the vespa cloud which

00:03:15,680 --> 00:03:21,920
allows you to

00:03:16,959 --> 00:03:21,920
run web applications on a hosted cloud

00:03:22,080 --> 00:03:27,760
the scale we operate westport

00:03:25,120 --> 00:03:28,879
at the aortic media is pretty

00:03:27,760 --> 00:03:31,280
significant

00:03:28,879 --> 00:03:32,080
we serve about 25 billion real-time

00:03:31,280 --> 00:03:35,120
queries per

00:03:32,080 --> 00:03:36,720
day and we do about 75 rights or

00:03:35,120 --> 00:03:38,480
primarily updates

00:03:36,720 --> 00:03:40,640
there's around 150 different

00:03:38,480 --> 00:03:43,840
applications across yahoo and various

00:03:40,640 --> 00:03:47,040
media that are using vespa that includes

00:03:43,840 --> 00:03:49,040
gemini native ads home page local search

00:03:47,040 --> 00:03:50,080
news finance and so on the list is

00:03:49,040 --> 00:03:52,720
pretty long

00:03:50,080 --> 00:03:54,640
if you go to blog westby ai you can read

00:03:52,720 --> 00:03:56,000
about some of the interesting use cases

00:03:54,640 --> 00:03:58,799
that vespa are used for

00:03:56,000 --> 00:04:02,400
in in yahoo including tensor ranking for

00:03:58,799 --> 00:04:02,400
home page recommendations

00:04:02,879 --> 00:04:07,840
here is an overview figure of the vespa

00:04:05,920 --> 00:04:11,120
architecture of a single deployment

00:04:07,840 --> 00:04:12,720
of a vespa architecture so on the left

00:04:11,120 --> 00:04:13,599
side you have the vespa application

00:04:12,720 --> 00:04:15,360
package

00:04:13,599 --> 00:04:17,120
this is where you define your vespa

00:04:15,360 --> 00:04:20,239
application it includes

00:04:17,120 --> 00:04:22,960
the kind of deployment specification it

00:04:20,239 --> 00:04:24,639
also includes the the document schemas

00:04:22,960 --> 00:04:26,240
if you have custom models machine

00:04:24,639 --> 00:04:28,320
learning models you also import these

00:04:26,240 --> 00:04:29,840
into the application package and if you

00:04:28,320 --> 00:04:32,240
have custom code

00:04:29,840 --> 00:04:34,160
this application package is uploaded to

00:04:32,240 --> 00:04:36,320
the vespa configuration system

00:04:34,160 --> 00:04:37,600
which translates this high level

00:04:36,320 --> 00:04:39,680
configuration

00:04:37,600 --> 00:04:42,160
into an actual running deployment and

00:04:39,680 --> 00:04:44,160
running configuration

00:04:42,160 --> 00:04:45,440
on the top here we have the stateless

00:04:44,160 --> 00:04:48,240
container cluster

00:04:45,440 --> 00:04:48,720
which is a java container where you can

00:04:48,240 --> 00:04:50,800
plug

00:04:48,720 --> 00:04:52,000
custom query processors working on the

00:04:50,800 --> 00:04:54,240
queries

00:04:52,000 --> 00:04:56,240
custom document processors working on

00:04:54,240 --> 00:04:58,720
the documents and also generally

00:04:56,240 --> 00:05:00,000
components on top of this there's a set

00:04:58,720 --> 00:05:03,039
of native apis

00:05:00,000 --> 00:05:05,039
both http and http to go

00:05:03,039 --> 00:05:06,639
you can also build your own apis on top

00:05:05,039 --> 00:05:08,320
of this because it comes with

00:05:06,639 --> 00:05:10,000
http handlers and so on so you can

00:05:08,320 --> 00:05:12,080
actually build nice

00:05:10,000 --> 00:05:13,680
feature-rich apis directly on the

00:05:12,080 --> 00:05:15,360
serving container

00:05:13,680 --> 00:05:17,440
then there's the content layer in the

00:05:15,360 --> 00:05:19,280
content cluster where the magic is

00:05:17,440 --> 00:05:21,120
happening this is where we store the

00:05:19,280 --> 00:05:23,440
content and index the content and

00:05:21,120 --> 00:05:25,680
distribute the content and also has

00:05:23,440 --> 00:05:26,479
the distributed query execution so that

00:05:25,680 --> 00:05:28,400
you can

00:05:26,479 --> 00:05:30,800
fan a carry out and get results from

00:05:28,400 --> 00:05:33,280
multiple nodes in the cluster

00:05:30,800 --> 00:05:35,840
westman runs you can run west by using

00:05:33,280 --> 00:05:38,720
rpms or you can also use docker

00:05:35,840 --> 00:05:42,320
the docker image name is there you can

00:05:38,720 --> 00:05:45,280
run it even on your on your laptop

00:05:42,320 --> 00:05:47,520
so back to the the thing about searching

00:05:45,280 --> 00:05:50,080
and ranking over evolving data sets

00:05:47,520 --> 00:05:52,080
so obviously vespa supports the basic

00:05:50,080 --> 00:05:55,520
crude operations so create

00:05:52,080 --> 00:05:57,680
read or search update and delete

00:05:55,520 --> 00:05:59,919
in any given search applications they're

00:05:57,680 --> 00:06:02,160
usually some hard filters

00:05:59,919 --> 00:06:03,840
those filters could be explicit by the

00:06:02,160 --> 00:06:05,199
user so the user get the choice to

00:06:03,840 --> 00:06:06,960
filter the search results

00:06:05,199 --> 00:06:09,360
but they could also be hidden for the

00:06:06,960 --> 00:06:11,919
user built in the application layer

00:06:09,360 --> 00:06:14,400
for instance spam filtering offensive

00:06:11,919 --> 00:06:16,880
content and so on are examples of

00:06:14,400 --> 00:06:18,560
a way of filtering away results that we

00:06:16,880 --> 00:06:20,400
don't want to show to the end user but

00:06:18,560 --> 00:06:23,520
these results can also be

00:06:20,400 --> 00:06:25,440
filtered by the users in some cases when

00:06:23,520 --> 00:06:28,160
you have a very large document volume

00:06:25,440 --> 00:06:30,080
and you want to update some parts of the

00:06:28,160 --> 00:06:32,720
documents for instance the in-stock

00:06:30,080 --> 00:06:34,639
status or the price or some rating

00:06:32,720 --> 00:06:36,000
it's very troublesome to actually have

00:06:34,639 --> 00:06:37,280
if you have a very large content volume

00:06:36,000 --> 00:06:40,720
if you have to re-index

00:06:37,280 --> 00:06:42,800
all the data just to change a few fields

00:06:40,720 --> 00:06:44,319
so that's one important aspect and

00:06:42,800 --> 00:06:46,960
westbound solves this

00:06:44,319 --> 00:06:48,160
for you also there there's another thing

00:06:46,960 --> 00:06:50,880
what i call basically

00:06:48,160 --> 00:06:52,800
soft filtering it's not really filtered

00:06:50,880 --> 00:06:54,639
in the explicit way but

00:06:52,800 --> 00:06:56,080
some properties of the documents will

00:06:54,639 --> 00:06:57,919
make the document rank

00:06:56,080 --> 00:06:59,759
much lower so it's actually not surface

00:06:57,919 --> 00:07:01,440
to the users

00:06:59,759 --> 00:07:03,440
some singles that could be used in the

00:07:01,440 --> 00:07:05,440
ranking model could include for instance

00:07:03,440 --> 00:07:06,800
click feedback from users what are the

00:07:05,440 --> 00:07:09,199
users clicking for and

00:07:06,800 --> 00:07:10,080
what are the users not clicking for this

00:07:09,199 --> 00:07:13,199
information

00:07:10,080 --> 00:07:13,919
can be fed back to the index so that the

00:07:13,199 --> 00:07:15,840
ranking is

00:07:13,919 --> 00:07:18,479
updated based on how the users are

00:07:15,840 --> 00:07:18,479
interacting

00:07:20,400 --> 00:07:25,280
now i will go into the real-time

00:07:23,120 --> 00:07:28,080
indexing architecture in the vespa and

00:07:25,280 --> 00:07:30,240
i will also give you some history about

00:07:28,080 --> 00:07:30,639
how the indexing architecture of vespa

00:07:30,240 --> 00:07:34,000
actually

00:07:30,639 --> 00:07:36,000
developed over time so when we

00:07:34,000 --> 00:07:37,599
made real-time indexing architecture or

00:07:36,000 --> 00:07:39,440
the real-time indexing parts in

00:07:37,599 --> 00:07:41,520
vespa there were some high-level goals

00:07:39,440 --> 00:07:43,759
that we wanted to meet we wanted to have

00:07:41,520 --> 00:07:45,360
very low latency measured in the single

00:07:43,759 --> 00:07:48,720
digit milliseconds

00:07:45,360 --> 00:07:50,879
we wanted to have operations visible in

00:07:48,720 --> 00:07:54,400
the search results immediately

00:07:50,879 --> 00:07:56,479
when the operation was acknowledged so

00:07:54,400 --> 00:07:58,479
if you feed the operation to vespa and

00:07:56,479 --> 00:08:00,479
you get acknowledged back that hey

00:07:58,479 --> 00:08:02,400
i've taken care of the document then

00:08:00,479 --> 00:08:04,080
that operation is actually visible in

00:08:02,400 --> 00:08:06,560
the results

00:08:04,080 --> 00:08:08,800
we also wanted to have a reasonably high

00:08:06,560 --> 00:08:10,240
throughput of operations even if we have

00:08:08,800 --> 00:08:12,960
very low latency

00:08:10,240 --> 00:08:15,280
and also importantly we want to have a

00:08:12,960 --> 00:08:17,759
low impact on search serving latency

00:08:15,280 --> 00:08:18,720
so that in case we do operations and

00:08:17,759 --> 00:08:20,879
feed and do

00:08:18,720 --> 00:08:23,360
updates of our index we don't want that

00:08:20,879 --> 00:08:24,160
to ruin the search latency or the search

00:08:23,360 --> 00:08:27,840
experience or the

00:08:24,160 --> 00:08:27,840
service level quality

00:08:28,479 --> 00:08:31,680
before diving a little bit into this i

00:08:30,479 --> 00:08:33,919
know that a lot of you

00:08:31,680 --> 00:08:35,519
are search experts and you know the in

00:08:33,919 --> 00:08:37,440
and out of search and how search

00:08:35,519 --> 00:08:39,120
work but let's take a step back and look

00:08:37,440 --> 00:08:40,560
at the classic inverted index data

00:08:39,120 --> 00:08:43,039
structure

00:08:40,560 --> 00:08:45,040
so basically you take a set of documents

00:08:43,039 --> 00:08:45,760
and to index them to speed up query

00:08:45,040 --> 00:08:47,680
evaluation

00:08:45,760 --> 00:08:49,680
you invert the documents so that you

00:08:47,680 --> 00:08:51,440
first tokenize the documents you figure

00:08:49,680 --> 00:08:53,040
out which are the vocabulary

00:08:51,440 --> 00:08:55,279
and then you build a dictionary of the

00:08:53,040 --> 00:08:57,200
unique set of words or tokens that is

00:08:55,279 --> 00:08:59,440
occurring in your documents

00:08:57,200 --> 00:09:01,600
the dictionary contains a pointer to

00:08:59,440 --> 00:09:03,440
what we call a posting list

00:09:01,600 --> 00:09:05,680
here in this case there are three words

00:09:03,440 --> 00:09:08,880
and they're occurring in some documents

00:09:05,680 --> 00:09:10,399
these structures can help speed up

00:09:08,880 --> 00:09:12,480
searches and there are some examples

00:09:10,399 --> 00:09:15,760
here for instance using or

00:09:12,480 --> 00:09:18,000
and and phrase the posting list might

00:09:15,760 --> 00:09:18,800
contain different granularity of

00:09:18,000 --> 00:09:20,720
information

00:09:18,800 --> 00:09:22,000
for example some posting lists can just

00:09:20,720 --> 00:09:23,760
say is the dark

00:09:22,000 --> 00:09:25,600
is the term present in the document or

00:09:23,760 --> 00:09:27,519
not more

00:09:25,600 --> 00:09:29,360
information can be added for instance

00:09:27,519 --> 00:09:32,160
how many times does the term occur

00:09:29,360 --> 00:09:33,760
and also at which positions i mean

00:09:32,160 --> 00:09:35,360
posting lists and dictionaries and

00:09:33,760 --> 00:09:37,120
the classic inverted structures there's

00:09:35,360 --> 00:09:38,880
been a lot of resources old-fashioned

00:09:37,120 --> 00:09:40,160
data structure it's been around for a

00:09:38,880 --> 00:09:41,920
long time

00:09:40,160 --> 00:09:43,440
one downside of the classic index

00:09:41,920 --> 00:09:44,640
structure is that it's difficult to

00:09:43,440 --> 00:09:46,640
update this structure

00:09:44,640 --> 00:09:48,240
because if a new document is added you

00:09:46,640 --> 00:09:51,839
both need to update the dictionary and

00:09:48,240 --> 00:09:51,839
you need to update the posting lists

00:09:53,120 --> 00:09:56,160
our first take at solving real-time

00:09:55,519 --> 00:09:59,360
indexing

00:09:56,160 --> 00:10:01,600
was around 2004. the way we did this was

00:09:59,360 --> 00:10:04,000
that we had a hierarchy of indexes with

00:10:01,600 --> 00:10:06,320
the gradually increasing size

00:10:04,000 --> 00:10:07,200
here in this case we have three active

00:10:06,320 --> 00:10:09,839
indexes

00:10:07,200 --> 00:10:10,640
and this is basically a batch immutable

00:10:09,839 --> 00:10:13,120
index segment

00:10:10,640 --> 00:10:13,839
once the index is built the index cannot

00:10:13,120 --> 00:10:15,600
change

00:10:13,839 --> 00:10:16,880
because then we are taking the documents

00:10:15,600 --> 00:10:20,000
and we have inverted them

00:10:16,880 --> 00:10:22,480
and the index is basically frozen and

00:10:20,000 --> 00:10:24,240
operations against this was operations

00:10:22,480 --> 00:10:25,839
where you don't you didn't really know

00:10:24,240 --> 00:10:27,440
when they actually become searchable the

00:10:25,839 --> 00:10:28,000
search engine says okay i take a new

00:10:27,440 --> 00:10:30,800
document

00:10:28,000 --> 00:10:32,160
it might become searchable later and the

00:10:30,800 --> 00:10:34,480
queries need to fan out

00:10:32,160 --> 00:10:36,560
all the active indexes and this means

00:10:34,480 --> 00:10:37,440
that each individual index needs to do a

00:10:36,560 --> 00:10:38,959
lot of matching

00:10:37,440 --> 00:10:40,640
looking up in the dictionary reading

00:10:38,959 --> 00:10:42,240
posting lists and then finally the

00:10:40,640 --> 00:10:44,560
results emerge

00:10:42,240 --> 00:10:46,160
in order to not run into a situation

00:10:44,560 --> 00:10:48,560
where you have hundreds of thousands

00:10:46,160 --> 00:10:49,600
of indexes you need to merge these in in

00:10:48,560 --> 00:10:52,480
the background

00:10:49,600 --> 00:10:53,760
to lower the the the serving cost of the

00:10:52,480 --> 00:10:56,720
queries

00:10:53,760 --> 00:10:58,959
and this also puts a cost on the system

00:10:56,720 --> 00:11:01,120
in terms of io rights and and

00:10:58,959 --> 00:11:02,959
and and might impact search performance

00:11:01,120 --> 00:11:03,920
so in our experience this indexing

00:11:02,959 --> 00:11:06,800
architecture

00:11:03,920 --> 00:11:08,720
did not really uh it didn't provide a

00:11:06,800 --> 00:11:09,200
really low indexing legacy and we also

00:11:08,720 --> 00:11:12,240
saw

00:11:09,200 --> 00:11:15,839
uh quite significantly impact on the

00:11:12,240 --> 00:11:15,839
serving latency

00:11:16,560 --> 00:11:19,600
this illustrates uh the kind of the

00:11:18,640 --> 00:11:22,160
index fusion

00:11:19,600 --> 00:11:22,959
and how a new index is built when when

00:11:22,160 --> 00:11:25,200
you have to

00:11:22,959 --> 00:11:26,320
merge indexes then the old one can be

00:11:25,200 --> 00:11:27,760
deleted

00:11:26,320 --> 00:11:30,079
then you start serving credits on the

00:11:27,760 --> 00:11:32,640
new one but

00:11:30,079 --> 00:11:34,880
hardware really evolved so we started in

00:11:32,640 --> 00:11:35,839
the search business around 1999. this

00:11:34,880 --> 00:11:39,519
chart shows

00:11:35,839 --> 00:11:43,240
uh how much the price of one gigabyte of

00:11:39,519 --> 00:11:44,640
memory uh over the time since 1995 to

00:11:43,240 --> 00:11:47,120
2015.

00:11:44,640 --> 00:11:48,800
as you can see there's a reflect at the

00:11:47,120 --> 00:11:51,920
roughly cost of one gigabyte

00:11:48,800 --> 00:11:54,839
in 2000 was one thousand dollar that

00:11:51,920 --> 00:11:56,240
dropped to ten dollars per gigabyte in

00:11:54,839 --> 00:11:59,360
2010.

00:11:56,240 --> 00:12:01,680
so we were operating vespa or the

00:11:59,360 --> 00:12:03,920
predecessor of west by 1999 we were

00:12:01,680 --> 00:12:07,040
running on on systems with one

00:12:03,920 --> 00:12:09,440
less than one gig of memory in 2010 or

00:12:07,040 --> 00:12:11,839
2008 we started seeing systems with 16

00:12:09,440 --> 00:12:13,680
gigs of ram 24 gigs of ram so we thought

00:12:11,839 --> 00:12:16,959
you know we're getting a lot more ram

00:12:13,680 --> 00:12:16,959
now what can we do with this

00:12:19,120 --> 00:12:23,680
we decided that we wanted to move away

00:12:21,200 --> 00:12:25,839
from this multi index instead we built a

00:12:23,680 --> 00:12:26,639
mutable memory index structure where we

00:12:25,839 --> 00:12:28,320
actually can

00:12:26,639 --> 00:12:30,079
update the dictionary and the posting

00:12:28,320 --> 00:12:33,360
lists in place

00:12:30,079 --> 00:12:35,600
in memory and then back this with an

00:12:33,360 --> 00:12:38,240
immutable index

00:12:35,600 --> 00:12:39,760
then if the memory index is full then we

00:12:38,240 --> 00:12:41,680
flush it in the background

00:12:39,760 --> 00:12:44,000
and then we can merge that with the with

00:12:41,680 --> 00:12:45,680
the immutable index similar that we did

00:12:44,000 --> 00:12:48,240
in the previous architecture

00:12:45,680 --> 00:12:49,440
but this adds a much better buffer so

00:12:48,240 --> 00:12:51,680
that you can actually do

00:12:49,440 --> 00:12:53,920
in place updates in the in the memory

00:12:51,680 --> 00:12:55,600
structures

00:12:53,920 --> 00:12:57,760
there's also something called attribute

00:12:55,600 --> 00:13:00,079
data which is really a forward index

00:12:57,760 --> 00:13:01,920
this is a place where we can store

00:13:00,079 --> 00:13:03,680
certain

00:13:01,920 --> 00:13:05,120
fields that we annotate in the document

00:13:03,680 --> 00:13:07,040
schema that these fields

00:13:05,120 --> 00:13:08,480
are attributes and the attribute in the

00:13:07,040 --> 00:13:11,440
field should be in memory

00:13:08,480 --> 00:13:12,639
so that we can access them for ranking

00:13:11,440 --> 00:13:15,440
grouping

00:13:12,639 --> 00:13:18,720
and and also for sorting so that that

00:13:15,440 --> 00:13:21,440
data is actually in memory

00:13:18,720 --> 00:13:22,079
here is an example of a vespa document

00:13:21,440 --> 00:13:24,160
schema

00:13:22,079 --> 00:13:26,000
there's a document tweet which has a

00:13:24,160 --> 00:13:28,160
field text it's a type string

00:13:26,000 --> 00:13:29,920
and we say that we're gonna indexing

00:13:28,160 --> 00:13:32,000
this it's an index

00:13:29,920 --> 00:13:34,320
this type of field will be in the

00:13:32,000 --> 00:13:35,839
mutable memory index

00:13:34,320 --> 00:13:37,360
will be indexed in the mutable memory

00:13:35,839 --> 00:13:39,440
index first and then it will be

00:13:37,360 --> 00:13:40,160
gradually flushed into the immutable

00:13:39,440 --> 00:13:42,720
index

00:13:40,160 --> 00:13:44,639
the attribute field here of a type long

00:13:42,720 --> 00:13:49,839
will be in the attribute data which is

00:13:44,639 --> 00:13:49,839
the forward index

00:13:51,360 --> 00:13:54,720
this is the vespa field view inside the

00:13:53,440 --> 00:13:57,519
content node

00:13:54,720 --> 00:13:58,720
so when the operation and this does not

00:13:57,519 --> 00:14:01,680
cover the kind of

00:13:58,720 --> 00:14:02,800
distribution mechanisms in westbound how

00:14:01,680 --> 00:14:05,600
we do

00:14:02,800 --> 00:14:06,639
across multiple nodes this is inside one

00:14:05,600 --> 00:14:08,480
content node

00:14:06,639 --> 00:14:10,160
you have a transaction log so you really

00:14:08,480 --> 00:14:11,519
need to have a transaction log because

00:14:10,160 --> 00:14:12,560
we are dealing with some memory

00:14:11,519 --> 00:14:15,279
structures here

00:14:12,560 --> 00:14:17,040
if we lose power we need to be able to

00:14:15,279 --> 00:14:18,959
recover the ins the index from the

00:14:17,040 --> 00:14:20,639
transaction log

00:14:18,959 --> 00:14:22,959
then there's the immutable index which

00:14:20,639 --> 00:14:26,320
we already touched on which is basically

00:14:22,959 --> 00:14:28,560
one large index then there's the

00:14:26,320 --> 00:14:31,199
document store where we actually store

00:14:28,560 --> 00:14:32,399
the actual document contents and in the

00:14:31,199 --> 00:14:35,120
the document contents

00:14:32,399 --> 00:14:36,480
uh we basically store all of the data

00:14:35,120 --> 00:14:37,680
for the document because this also

00:14:36,480 --> 00:14:39,680
allows us to

00:14:37,680 --> 00:14:41,120
in case we want to redistribute data

00:14:39,680 --> 00:14:42,959
over multiple nodes

00:14:41,120 --> 00:14:45,199
uh we have the source data in the

00:14:42,959 --> 00:14:48,000
document store

00:14:45,199 --> 00:14:49,519
then the memory index like introduced

00:14:48,000 --> 00:14:51,040
and attribute data

00:14:49,519 --> 00:14:53,680
and there's also a mapping into the

00:14:51,040 --> 00:14:55,600
document store which is in memory

00:14:53,680 --> 00:14:57,600
and also attribute data where you

00:14:55,600 --> 00:14:58,000
specify a specific setting in the schema

00:14:57,600 --> 00:15:00,160
we'll get

00:14:58,000 --> 00:15:02,639
to that you can add also posting lists

00:15:00,160 --> 00:15:04,240
on top of the attribute data

00:15:02,639 --> 00:15:06,399
then there's a feed view so when an

00:15:04,240 --> 00:15:08,639
operation comes into the system

00:15:06,399 --> 00:15:10,959
it gets written to the transaction log

00:15:08,639 --> 00:15:12,959
so that we persist the operation in case

00:15:10,959 --> 00:15:14,000
is a power failure or something that we

00:15:12,959 --> 00:15:16,079
need to recover

00:15:14,000 --> 00:15:17,279
let me write it into the memory index

00:15:16,079 --> 00:15:20,959
attribute data

00:15:17,279 --> 00:15:24,079
and in the document summary store

00:15:20,959 --> 00:15:26,800
and the search view similar we have a

00:15:24,079 --> 00:15:27,279
queries coming in and the queries have

00:15:26,800 --> 00:15:31,839
these

00:15:27,279 --> 00:15:34,160
components to um to research so a query

00:15:31,839 --> 00:15:35,680
will in parallel search the memory index

00:15:34,160 --> 00:15:37,120
and the immutable index and the

00:15:35,680 --> 00:15:39,040
attribute data

00:15:37,120 --> 00:15:40,639
if they are included in the query or the

00:15:39,040 --> 00:15:42,399
attributes with the fast search

00:15:40,639 --> 00:15:43,920
so this is the way the query is set up

00:15:42,399 --> 00:15:46,880
so that it's fanned out to all these

00:15:43,920 --> 00:15:46,880
different components

00:15:48,639 --> 00:15:53,440
now we touched on the kind of basics of

00:15:51,680 --> 00:15:55,360
around the indexing and the nodes now we

00:15:53,440 --> 00:15:56,079
look at actually how users are

00:15:55,360 --> 00:15:58,880
configuring

00:15:56,079 --> 00:16:00,000
how users are controlling their

00:15:58,880 --> 00:16:04,000
application

00:16:00,000 --> 00:16:05,040
in this case we have a schema which has

00:16:04,000 --> 00:16:06,880
a document tweet

00:16:05,040 --> 00:16:08,320
and i'm listing a set of name fields

00:16:06,880 --> 00:16:10,959
here there's id

00:16:08,320 --> 00:16:12,639
field type long we specify that this is

00:16:10,959 --> 00:16:13,199
going to be a summary that means that it

00:16:12,639 --> 00:16:14,800
will be

00:16:13,199 --> 00:16:16,320
also returned in the search engine

00:16:14,800 --> 00:16:17,920
result page it's

00:16:16,320 --> 00:16:19,920
attribute which means it's in the

00:16:17,920 --> 00:16:20,560
attribute storage so we can have fast

00:16:19,920 --> 00:16:22,880
access to

00:16:20,560 --> 00:16:23,920
this field both for grouping searching

00:16:22,880 --> 00:16:26,560
and ranking

00:16:23,920 --> 00:16:28,240
there's a text field type string here we

00:16:26,560 --> 00:16:30,720
specify we're going to index it

00:16:28,240 --> 00:16:32,160
that means that we will we will tokenize

00:16:30,720 --> 00:16:34,880
it we will stem it

00:16:32,160 --> 00:16:35,279
and do the regular indexing to be able

00:16:34,880 --> 00:16:38,480
to

00:16:35,279 --> 00:16:40,959
support textile matching

00:16:38,480 --> 00:16:42,880
then there is a created ad field which

00:16:40,959 --> 00:16:45,759
is basically the timestamp when the

00:16:42,880 --> 00:16:47,040
in unix epoch for when the tweet was

00:16:45,759 --> 00:16:48,959
actually created

00:16:47,040 --> 00:16:51,279
and in this case we have also attribute

00:16:48,959 --> 00:16:54,320
but here we add attribute fast search

00:16:51,279 --> 00:16:55,759
that means that we will add a b3

00:16:54,320 --> 00:16:57,519
indexing structure on top of the

00:16:55,759 --> 00:17:00,639
attribute so you can have

00:16:57,519 --> 00:17:03,759
a fast search using posting lists

00:17:00,639 --> 00:17:06,880
by default attribute field like the

00:17:03,759 --> 00:17:08,799
id here does not have that structure so

00:17:06,880 --> 00:17:11,600
if you actually try to just search for

00:17:08,799 --> 00:17:14,079
the id using this schema that will be a

00:17:11,600 --> 00:17:15,760
linear scan

00:17:14,079 --> 00:17:17,600
and there's a key distinction here

00:17:15,760 --> 00:17:20,720
between index and attribute

00:17:17,600 --> 00:17:23,199
there's also a likes field here and the

00:17:20,720 --> 00:17:26,319
topics which is a tensor field where

00:17:23,199 --> 00:17:28,720
we can store what topics is this tweet

00:17:26,319 --> 00:17:33,039
about using a tensor or sparse

00:17:28,720 --> 00:17:35,840
sparse tensor in this case

00:17:33,039 --> 00:17:37,760
vespa ranking similar you configure

00:17:35,840 --> 00:17:39,840
vespa ranking in the document schema

00:17:37,760 --> 00:17:41,840
it's very flexible so you can write

00:17:39,840 --> 00:17:43,919
handwrite your own expressions or you

00:17:41,840 --> 00:17:46,720
can use machine learning models

00:17:43,919 --> 00:17:47,280
but the magic here is that the the

00:17:46,720 --> 00:17:49,840
machine

00:17:47,280 --> 00:17:52,080
or the ranking is able to use these

00:17:49,840 --> 00:17:52,880
fresh signals reading from the attribute

00:17:52,080 --> 00:17:56,240
store which

00:17:52,880 --> 00:17:58,640
can be updated in real time

00:17:56,240 --> 00:18:00,320
here there's an example of a simple text

00:17:58,640 --> 00:18:03,600
freshness rank profile which

00:18:00,320 --> 00:18:06,880
does a combination of freshness and bm25

00:18:03,600 --> 00:18:09,280
so bm25 is a built-in ranking feature

00:18:06,880 --> 00:18:10,320
there's a topic ranking here given that

00:18:09,280 --> 00:18:12,160
the user has

00:18:10,320 --> 00:18:15,120
declared some interest in the certain

00:18:12,160 --> 00:18:16,799
topics we compute the sparse dot product

00:18:15,120 --> 00:18:19,120
between the user interest

00:18:16,799 --> 00:18:20,160
and the topics in the document so so in

00:18:19,120 --> 00:18:23,200
order to kind of

00:18:20,160 --> 00:18:26,720
show the user uh topics that he

00:18:23,200 --> 00:18:30,240
is interested in and finally there's

00:18:26,720 --> 00:18:30,799
an example of actually combining a x-key

00:18:30,240 --> 00:18:34,000
boost

00:18:30,799 --> 00:18:38,880
model this is a gbt model with

00:18:34,000 --> 00:18:38,880
on-x deep neural network model

00:18:39,200 --> 00:18:42,720
so back to the attribute versus index

00:18:41,679 --> 00:18:45,840
this table

00:18:42,720 --> 00:18:46,799
kind of summarize uh the the the

00:18:45,840 --> 00:18:48,799
features

00:18:46,799 --> 00:18:50,720
that you get when you declare a field as

00:18:48,799 --> 00:18:53,919
an index or attribute

00:18:50,720 --> 00:18:56,240
attribute fields are only fast

00:18:53,919 --> 00:18:57,200
to match over if they have been set with

00:18:56,240 --> 00:18:58,799
fast search

00:18:57,200 --> 00:19:01,039
or if there are no other more

00:18:58,799 --> 00:19:05,120
restrictive terms in the query

00:19:01,039 --> 00:19:05,120
but they're very fast to update

00:19:05,600 --> 00:19:10,000
now i get a few api examples so you get

00:19:08,240 --> 00:19:13,280
the feel of how you interact with

00:19:10,000 --> 00:19:14,160
the with the apis of vespa here there's

00:19:13,280 --> 00:19:17,360
a simple

00:19:14,160 --> 00:19:19,200
curl command using the vespa http api to

00:19:17,360 --> 00:19:21,120
create a new document so

00:19:19,200 --> 00:19:22,720
we put the document into we have some

00:19:21,120 --> 00:19:25,600
text here we have id

00:19:22,720 --> 00:19:26,320
and we have a created timestamp that's

00:19:25,600 --> 00:19:28,320
fine

00:19:26,320 --> 00:19:30,320
then we want to update the document and

00:19:28,320 --> 00:19:32,799
in this case we update the number of

00:19:30,320 --> 00:19:36,720
likes and assign the value one

00:19:32,799 --> 00:19:39,760
this operation is not causing rest but

00:19:36,720 --> 00:19:42,480
you have to read the original document

00:19:39,760 --> 00:19:42,799
and then apply the number of likes and

00:19:42,480 --> 00:19:45,520
then

00:19:42,799 --> 00:19:46,559
write it back this is applied directly

00:19:45,520 --> 00:19:48,799
in the attribute

00:19:46,559 --> 00:19:49,919
right so that is the the crucial part

00:19:48,799 --> 00:19:52,640
here to

00:19:49,919 --> 00:19:54,240
have a high throughput of these partial

00:19:52,640 --> 00:19:56,400
updates

00:19:54,240 --> 00:19:58,240
here's another one where we can imagine

00:19:56,400 --> 00:20:00,160
that we have some machine learning

00:19:58,240 --> 00:20:01,919
process going in the background it will

00:20:00,160 --> 00:20:03,039
either define which tweets are about

00:20:01,919 --> 00:20:05,120
which topics

00:20:03,039 --> 00:20:07,600
and then we can go back and update a

00:20:05,120 --> 00:20:10,480
large volume of the content pool

00:20:07,600 --> 00:20:12,240
with this value in this case we assign

00:20:10,480 --> 00:20:14,159
this tweet

00:20:12,240 --> 00:20:17,440
it's about search and it's about machine

00:20:14,159 --> 00:20:20,080
learning and it has some scores

00:20:17,440 --> 00:20:22,640
in this case we have a pro account of

00:20:20,080 --> 00:20:26,159
twitter which is actually updating

00:20:22,640 --> 00:20:28,880
the tweet text this is not uh handle

00:20:26,159 --> 00:20:30,400
this is handled by reading the document

00:20:28,880 --> 00:20:33,280
from the document summary store

00:20:30,400 --> 00:20:34,240
and writing it back to the index so this

00:20:33,280 --> 00:20:37,520
one is not

00:20:34,240 --> 00:20:39,039
in place this needs access to the other

00:20:37,520 --> 00:20:41,600
fields that were originally in the

00:20:39,039 --> 00:20:41,600
document

00:20:42,320 --> 00:20:49,360
and here is an example of the query api

00:20:46,159 --> 00:20:51,200
using the yahoo query language where you

00:20:49,360 --> 00:20:55,200
specify the application logic

00:20:51,200 --> 00:20:58,240
in this sql-like syntax

00:20:55,200 --> 00:20:58,880
where you want to have a filter on on a

00:20:58,240 --> 00:21:00,559
date

00:20:58,880 --> 00:21:02,640
which is application specific for

00:21:00,559 --> 00:21:05,679
instance only search in tweets

00:21:02,640 --> 00:21:07,679
the last 24 hours and the user query

00:21:05,679 --> 00:21:09,360
and in this case the user couries berlin

00:21:07,679 --> 00:21:11,360
buzzwords and we choose

00:21:09,360 --> 00:21:14,080
the ranking profile and how many hits we

00:21:11,360 --> 00:21:14,080
want to return

00:21:14,240 --> 00:21:17,360
so everybody's wondering about

00:21:15,679 --> 00:21:20,000
performance obviously

00:21:17,360 --> 00:21:21,360
so this depends a little bit on the type

00:21:20,000 --> 00:21:22,799
of hardware you're running on but this

00:21:21,360 --> 00:21:24,480
is one example where you're doing

00:21:22,799 --> 00:21:27,919
partial updates of a single

00:21:24,480 --> 00:21:30,640
uh integer field where you get single

00:21:27,919 --> 00:21:33,919
digit millisecond latency and we can do

00:21:30,640 --> 00:21:37,120
50 000 updates per second per node

00:21:33,919 --> 00:21:38,640
so that's a really high number but note

00:21:37,120 --> 00:21:40,960
you also have to write to the

00:21:38,640 --> 00:21:42,880
transaction log and that requires a high

00:21:40,960 --> 00:21:44,400
i o write capacity

00:21:42,880 --> 00:21:46,480
and if you're running westbound for

00:21:44,400 --> 00:21:48,080
example a network attached storage

00:21:46,480 --> 00:21:49,919
doing a sync operation against that

00:21:48,080 --> 00:21:51,840
storage cost a lot

00:21:49,919 --> 00:21:54,240
so but the default sync operation in

00:21:51,840 --> 00:21:56,880
vespa is that we do actually try to sync

00:21:54,240 --> 00:21:58,960
sync to storage for for the operation so

00:21:56,880 --> 00:22:00,080
that they can be durable in case of a

00:21:58,960 --> 00:22:03,200
power failure

00:22:00,080 --> 00:22:03,600
you can however tweak this and and set

00:22:03,200 --> 00:22:06,720
this

00:22:03,600 --> 00:22:09,039
to false uh the puts against the memory

00:22:06,720 --> 00:22:10,480
index depends a lot on the size of the

00:22:09,039 --> 00:22:12,720
input text

00:22:10,480 --> 00:22:14,480
obviously and also in this case we're

00:22:12,720 --> 00:22:16,720
going to have multiple threads

00:22:14,480 --> 00:22:17,679
working on on the memory index so in

00:22:16,720 --> 00:22:19,600
this case

00:22:17,679 --> 00:22:21,280
it depends really on the size but

00:22:19,600 --> 00:22:23,600
usually numbers that we observe on

00:22:21,280 --> 00:22:25,200
similar type of hardware is from 1000 to

00:22:23,600 --> 00:22:28,000
8000 but remember

00:22:25,200 --> 00:22:30,080
this is not batch oriented indexing this

00:22:28,000 --> 00:22:31,600
is with low latency with a single digit

00:22:30,080 --> 00:22:33,600
millisecond latent so once you

00:22:31,600 --> 00:22:34,720
put the document into the index you get

00:22:33,600 --> 00:22:36,799
the acknowledge

00:22:34,720 --> 00:22:38,960
then the actual document is there

00:22:36,799 --> 00:22:41,280
similar if you look at elasticsearch

00:22:38,960 --> 00:22:42,000
you will have to pass this refresh

00:22:41,280 --> 00:22:44,320
setting

00:22:42,000 --> 00:22:48,400
to true in order to have the same kind

00:22:44,320 --> 00:22:48,400
of functionality that you have in vespa

00:22:50,400 --> 00:22:55,360
that was what i had i included a few

00:22:53,039 --> 00:22:58,720
resources here

00:22:55,360 --> 00:23:00,880
you can go we read more about vespa you

00:22:58,720 --> 00:23:04,000
can go we have a slack space

00:23:00,880 --> 00:23:06,559
we even have a free cloud cloud

00:23:04,000 --> 00:23:08,240
trial so you can check out our hosted

00:23:06,559 --> 00:23:10,000
cloud offering

00:23:08,240 --> 00:23:11,360
we have a twitter account i'm also on

00:23:10,000 --> 00:23:13,919
twitter and

00:23:11,360 --> 00:23:15,120
you can also follow us on github and

00:23:13,919 --> 00:23:17,760
tomorrow there's

00:23:15,120 --> 00:23:19,679
going to be a search engine debate with

00:23:17,760 --> 00:23:21,120
me and josh and angel

00:23:19,679 --> 00:23:23,440
and i'm really looking forward for that

00:23:21,120 --> 00:23:26,880
so i hope you will take time

00:23:23,440 --> 00:23:31,440
to join so that was what i had

00:23:26,880 --> 00:23:31,440
um i'm open to questions

00:23:32,080 --> 00:23:35,440
thank you joe christian that was great a

00:23:34,159 --> 00:23:37,760
nice tour of vespa

00:23:35,440 --> 00:23:39,280
uh and one of you well i think one of

00:23:37,760 --> 00:23:42,320
your favorite topics to talk about

00:23:39,280 --> 00:23:44,559
is partial updates um

00:23:42,320 --> 00:23:45,760
we have one question from andreas uh in

00:23:44,559 --> 00:23:47,600
the chat and it's a

00:23:45,760 --> 00:23:49,039
bit more of a general vespa question so

00:23:47,600 --> 00:23:50,960
he's asking um

00:23:49,039 --> 00:23:52,799
how can i modify or add analyzers

00:23:50,960 --> 00:23:55,840
stemming tokenization etc

00:23:52,799 --> 00:23:58,799
in bespo what does that look like

00:23:55,840 --> 00:23:59,279
do you have a handy example to show yeah

00:23:58,799 --> 00:24:00,880
so that's

00:23:59,279 --> 00:24:02,799
that's a great example so basketball by

00:24:00,880 --> 00:24:06,400
default we integrate uh

00:24:02,799 --> 00:24:09,440
with apache open nlp for stemming and

00:24:06,400 --> 00:24:11,440
tokenization and all of this is

00:24:09,440 --> 00:24:15,279
happening in java so you can

00:24:11,440 --> 00:24:16,960
actually take the linguistic class that

00:24:15,279 --> 00:24:18,880
is there and you can subclass it

00:24:16,960 --> 00:24:20,559
and make your own uh stammer and

00:24:18,880 --> 00:24:23,039
tokenize so if you want to kind of

00:24:20,559 --> 00:24:25,360
extend it so that's definitely

00:24:23,039 --> 00:24:28,240
definitely possible

00:24:25,360 --> 00:24:28,960
excellent um so i had a couple of

00:24:28,240 --> 00:24:31,200
questions

00:24:28,960 --> 00:24:32,720
you don't mind so you mentioned recovery

00:24:31,200 --> 00:24:33,679
using transaction log which makes a lot

00:24:32,720 --> 00:24:36,400
of sense

00:24:33,679 --> 00:24:36,720
um i'm curious and maybe i missed it uh

00:24:36,400 --> 00:24:38,799
on

00:24:36,720 --> 00:24:39,840
the last slide but how often is the

00:24:38,799 --> 00:24:42,720
immutable

00:24:39,840 --> 00:24:43,679
memory index flushed and merged into the

00:24:42,720 --> 00:24:46,000
immutable disk

00:24:43,679 --> 00:24:47,600
index yeah that's that's that's that's a

00:24:46,000 --> 00:24:50,240
great question um

00:24:47,600 --> 00:24:51,440
so uh that will depend on the feed rate

00:24:50,240 --> 00:24:54,640
because there's a target

00:24:51,440 --> 00:24:57,120
for the maximum size right so

00:24:54,640 --> 00:24:58,960
if it's if it becomes too big then it's

00:24:57,120 --> 00:25:01,440
going to be flushed

00:24:58,960 --> 00:25:03,440
right so typically the default setting i

00:25:01,440 --> 00:25:06,400
think is uh we use one gig

00:25:03,440 --> 00:25:07,840
uh for for the mapper index okay so it

00:25:06,400 --> 00:25:10,640
definitely will depend on the on the on

00:25:07,840 --> 00:25:12,720
the feed rate and the update rate right

00:25:10,640 --> 00:25:14,000
makes sense all right a couple more

00:25:12,720 --> 00:25:17,200
questions for you yeah

00:25:14,000 --> 00:25:21,120
great um is there any way to uh

00:25:17,200 --> 00:25:23,760
encrypt documents being stored

00:25:21,120 --> 00:25:24,880
right so uh we rely on the file system

00:25:23,760 --> 00:25:27,360
to do the encryption

00:25:24,880 --> 00:25:28,240
uh yeah we do that i think that's the

00:25:27,360 --> 00:25:31,360
normal

00:25:28,240 --> 00:25:34,000
normal procedure makes sense

00:25:31,360 --> 00:25:35,679
um so another question how does the

00:25:34,000 --> 00:25:38,480
indexing performance change

00:25:35,679 --> 00:25:39,919
as read traffic increases how easy is it

00:25:38,480 --> 00:25:42,799
to take snapshots

00:25:39,919 --> 00:25:44,000
of the index okay so those are two great

00:25:42,799 --> 00:25:47,360
examples so

00:25:44,000 --> 00:25:49,200
on the read performance versus

00:25:47,360 --> 00:25:51,120
right performance so basically a lot of

00:25:49,200 --> 00:25:54,240
these operations are

00:25:51,120 --> 00:25:56,559
uh cpus the way we throttle it is that

00:25:54,240 --> 00:25:58,640
there's a parameter called concurrency

00:25:56,559 --> 00:26:00,480
so it's a high level parameter where you

00:25:58,640 --> 00:26:02,640
set aside you know how many

00:26:00,480 --> 00:26:04,080
the sizes of the tread bulls which is

00:26:02,640 --> 00:26:06,080
depending on

00:26:04,080 --> 00:26:09,120
the number of cpus you have on the

00:26:06,080 --> 00:26:12,159
system so typically these are

00:26:09,120 --> 00:26:13,200
limited uh number of cpus you you don't

00:26:12,159 --> 00:26:15,760
get to use like

00:26:13,200 --> 00:26:16,400
100 cpu just for the indexing part so

00:26:15,760 --> 00:26:19,679
that's

00:26:16,400 --> 00:26:23,120
how we kind of balance uh search versus

00:26:19,679 --> 00:26:26,000
uh versus uh indexing

00:26:23,120 --> 00:26:27,760
the other question well i forgot it i

00:26:26,000 --> 00:26:30,080
forgot i'm sorry

00:26:27,760 --> 00:26:31,360
no it's okay um so there's quite a few

00:26:30,080 --> 00:26:33,279
questions rolling in

00:26:31,360 --> 00:26:34,720
uh we're at the top of the hour so i'm

00:26:33,279 --> 00:26:35,760
also gonna suggest we'll ask one more

00:26:34,720 --> 00:26:38,880
question and then

00:26:35,760 --> 00:26:41,039
we'll go over to the breakout room

00:26:38,880 --> 00:26:42,720
yeah that's breakout room france salon

00:26:41,039 --> 00:26:44,000
and we can i can copy some of those

00:26:42,720 --> 00:26:47,440
questions over

00:26:44,000 --> 00:26:47,760
um one more question uh maybe a short

00:26:47,440 --> 00:26:50,880
one

00:26:47,760 --> 00:26:51,120
on partial updates are all fields read

00:26:50,880 --> 00:26:53,440
and

00:26:51,120 --> 00:26:55,600
indexed or just the one field you are

00:26:53,440 --> 00:26:58,240
updating

00:26:55,600 --> 00:26:58,960
it's uh only yeah so if it's attribute

00:26:58,240 --> 00:27:02,080
field

00:26:58,960 --> 00:27:02,720
uh we don't uh read the original data

00:27:02,080 --> 00:27:05,840
from

00:27:02,720 --> 00:27:06,799
uh the data store so you upload that in

00:27:05,840 --> 00:27:08,720
place you don't

00:27:06,799 --> 00:27:21,840
need to read the entire document and

00:27:08,720 --> 00:27:21,840
re-index it back

00:27:30,399 --> 00:27:32,480

YouTube URL: https://www.youtube.com/watch?v=vFu5g44-VaY


