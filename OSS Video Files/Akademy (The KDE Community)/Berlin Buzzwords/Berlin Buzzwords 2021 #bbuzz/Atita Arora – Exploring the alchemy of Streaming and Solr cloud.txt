Title: Atita Arora – Exploring the alchemy of Streaming and Solr cloud
Publication date: 2021-06-24
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	This is a normal E-commerce use case that had been relying on earlier standalone Solr and later Master-Slave for ages.
Everything was running smoothly with no performance glitches until one day we had this requirement to support “Unlimited Products” in our shops.
We were confident that things would be smooth with full reindex 2 times a day but looking at the index pipeline duration of 4+ hrs, we realized it needed some cloud magic.
This is where we started and transformed a normal batch index pipeline to a stream index pipeline leveraging Kafka and Redis and achieved a NRT indexing.
Along with that, we did not shy to give our infra a bonus upgrade from Solr 6.6.2 to Solr 8.7 and an add-on migration from the existing Master-Slave to Cloud.
We want to talk about our journey and important lessons learned during this process.

Speaker
Atita Arora – https://2021.berlinbuzzwords.de/member/atita-arora

More: https://2021.berlinbuzzwords.de/session/exploring-alchemy-streaming-and-solr-cloud
Captions: 
	00:00:08,160 --> 00:00:12,480
hello everyone my name is atita arura

00:00:10,400 --> 00:00:13,440
and i'm an active contributor in search

00:00:12,480 --> 00:00:15,839
community

00:00:13,440 --> 00:00:17,359
i currently work with my toys group and

00:00:15,839 --> 00:00:18,960
today we're going to be talking about

00:00:17,359 --> 00:00:21,760
exploring the alchemy of

00:00:18,960 --> 00:00:22,160
kafka streaming along with solar cloud i

00:00:21,760 --> 00:00:24,640
hope

00:00:22,160 --> 00:00:26,320
i make use of enough time because my

00:00:24,640 --> 00:00:28,320
slides are a little longer

00:00:26,320 --> 00:00:29,679
so my agenda of course i would be

00:00:28,320 --> 00:00:31,119
discussing about the problem the

00:00:29,679 --> 00:00:32,880
proposed solution

00:00:31,119 --> 00:00:34,399
some improvements that we made and i

00:00:32,880 --> 00:00:36,000
hope we leave some time for questions

00:00:34,399 --> 00:00:38,960
and suggestions

00:00:36,000 --> 00:00:40,879
so i would skip this part just to give

00:00:38,960 --> 00:00:43,680
you a brief that my toys

00:00:40,879 --> 00:00:44,719
tmbh was founded in 1999 at subsidiary

00:00:43,680 --> 00:00:46,399
of auto group

00:00:44,719 --> 00:00:49,039
and we have various other shops that we

00:00:46,399 --> 00:00:51,280
support which have products ranging from

00:00:49,039 --> 00:00:53,039
pregnancy to everything about kids wide

00:00:51,280 --> 00:00:53,920
range of shoes and home furnishing to

00:00:53,039 --> 00:00:57,440
check out

00:00:53,920 --> 00:00:59,280
we have over 4 million active users and

00:00:57,440 --> 00:01:01,039
statistically speaking we serve about

00:00:59,280 --> 00:01:02,480
1.3 million searches per day have

00:01:01,039 --> 00:01:05,360
conversion rate of

00:01:02,480 --> 00:01:07,280
around nine percent uh now small

00:01:05,360 --> 00:01:09,840
snapshot from the last uh black friday

00:01:07,280 --> 00:01:11,600
we served about 26.2 million queries in

00:01:09,840 --> 00:01:13,600
one single day

00:01:11,600 --> 00:01:15,920
so after that introduction a little bit

00:01:13,600 --> 00:01:16,240
about our platform that we have changed

00:01:15,920 --> 00:01:17,759
so

00:01:16,240 --> 00:01:19,840
previously it used to look like this

00:01:17,759 --> 00:01:21,759
that we had the data source

00:01:19,840 --> 00:01:23,280
uh the data that would consist of

00:01:21,759 --> 00:01:24,400
everything from the product data

00:01:23,280 --> 00:01:25,920
promotional data

00:01:24,400 --> 00:01:27,520
data from data warehousing and

00:01:25,920 --> 00:01:29,600
everything and whatnot

00:01:27,520 --> 00:01:30,880
all of this data was consumed by our

00:01:29,600 --> 00:01:33,040
batch processors

00:01:30,880 --> 00:01:35,280
uh which is called solar importer the

00:01:33,040 --> 00:01:37,520
main idea of the solar importer was to

00:01:35,280 --> 00:01:38,240
transform the data based on the business

00:01:37,520 --> 00:01:40,320
logic

00:01:38,240 --> 00:01:41,759
and put the documents into solar which

00:01:40,320 --> 00:01:43,119
are then served on the portal through

00:01:41,759 --> 00:01:44,960
the product service

00:01:43,119 --> 00:01:47,119
uh the subsidiary process was also

00:01:44,960 --> 00:01:48,560
supported here for the price and stock

00:01:47,119 --> 00:01:49,520
of course for e-commerce that's a very

00:01:48,560 --> 00:01:52,640
essential thing

00:01:49,520 --> 00:01:54,399
to maintain the uh challenging uh prices

00:01:52,640 --> 00:01:56,560
uh you know attractive prices and the

00:01:54,399 --> 00:01:58,320
stock update but to mention here that

00:01:56,560 --> 00:01:58,960
these stock updates were not processed

00:01:58,320 --> 00:02:01,119
when this

00:01:58,960 --> 00:02:02,799
batch process was running that was one

00:02:01,119 --> 00:02:06,320
of the biggest crucial

00:02:02,799 --> 00:02:08,080
disadvantages to add here is also that

00:02:06,320 --> 00:02:09,759
there were some shortcomings uh that we

00:02:08,080 --> 00:02:11,120
were able to support only the limited

00:02:09,759 --> 00:02:12,000
number of products due to limited

00:02:11,120 --> 00:02:14,239
resources

00:02:12,000 --> 00:02:15,520
the complete catalog reindex which was

00:02:14,239 --> 00:02:18,239
happening twice a day

00:02:15,520 --> 00:02:19,520
was taking close to four hours each time

00:02:18,239 --> 00:02:21,120
and as i just mentioned

00:02:19,520 --> 00:02:22,720
that we were not processing the price

00:02:21,120 --> 00:02:24,000
and stock updates every time this batch

00:02:22,720 --> 00:02:25,920
processes was running

00:02:24,000 --> 00:02:27,520
which means that the know that there

00:02:25,920 --> 00:02:29,599
were no product updates

00:02:27,520 --> 00:02:30,800
while the catalog index was happening

00:02:29,599 --> 00:02:32,319
scaling of course

00:02:30,800 --> 00:02:33,599
does looks worrisome in such kind of

00:02:32,319 --> 00:02:34,959
approach and there was no disaster

00:02:33,599 --> 00:02:36,640
recovery mechanism

00:02:34,959 --> 00:02:38,400
another bad thing is that we had the

00:02:36,640 --> 00:02:40,879
bulky document size because we are

00:02:38,400 --> 00:02:44,080
sowing everything from solar

00:02:40,879 --> 00:02:45,360
so to counter this we wanted to plan

00:02:44,080 --> 00:02:47,440
something that would help us

00:02:45,360 --> 00:02:49,120
in the long run so to begin with we

00:02:47,440 --> 00:02:51,440
wanted to keep only the searchable data

00:02:49,120 --> 00:02:52,400
in solar to reduce the number of fields

00:02:51,440 --> 00:02:55,440
in solar

00:02:52,400 --> 00:02:56,640
and we plan to use redis for supporting

00:02:55,440 --> 00:02:59,440
solar with this thing

00:02:56,640 --> 00:03:01,280
we wanted to reduce the pipeline time to

00:02:59,440 --> 00:03:02,800
not spend four hours but rather should

00:03:01,280 --> 00:03:04,959
be done in the near real time

00:03:02,800 --> 00:03:06,560
we managed to reduce the index footprint

00:03:04,959 --> 00:03:09,280
by reducing the number of fields from

00:03:06,560 --> 00:03:10,000
548 fields to 78 fields for all the

00:03:09,280 --> 00:03:12,159
shops

00:03:10,000 --> 00:03:14,000
the disaster recovery management was

00:03:12,159 --> 00:03:15,840
achieved through the kafka replay using

00:03:14,000 --> 00:03:16,959
the kafka connect that i will i'll be

00:03:15,840 --> 00:03:19,760
showing you in the

00:03:16,959 --> 00:03:21,760
next slide of course there was no wait

00:03:19,760 --> 00:03:23,599
for the full re-index anymore

00:03:21,760 --> 00:03:25,760
we are processing everything in real

00:03:23,599 --> 00:03:26,400
time and the infrastructure scalability

00:03:25,760 --> 00:03:29,280
management

00:03:26,400 --> 00:03:30,959
was achieved through the kubernetes so

00:03:29,280 --> 00:03:33,519
to give you a sneak peek as to how

00:03:30,959 --> 00:03:38,080
exactly the architecture looks like

00:03:33,519 --> 00:03:42,319
i hope i can do justice i would have to

00:03:38,080 --> 00:03:42,319
go back to the architecture

00:03:43,040 --> 00:03:50,400
okay come up come up

00:03:46,720 --> 00:03:53,200
okay i guess everyone can see this

00:03:50,400 --> 00:03:55,280
so here the data source stays intact as

00:03:53,200 --> 00:03:56,159
it was instead of being consumed by the

00:03:55,280 --> 00:03:58,640
batch process

00:03:56,159 --> 00:04:00,239
we are publishing the respective data

00:03:58,640 --> 00:04:02,239
into the respective topic

00:04:00,239 --> 00:04:04,319
topics and this is consumed by the

00:04:02,239 --> 00:04:04,720
product data topology which processes

00:04:04,319 --> 00:04:06,879
now

00:04:04,720 --> 00:04:08,560
the data as per the business logic and

00:04:06,879 --> 00:04:10,400
combines them and publishes them into

00:04:08,560 --> 00:04:11,280
one single topic which is then consumed

00:04:10,400 --> 00:04:13,200
by the

00:04:11,280 --> 00:04:14,560
master and the variant topology these

00:04:13,200 --> 00:04:17,919
are very heavyweight

00:04:14,560 --> 00:04:19,919
and highly available and heavy data

00:04:17,919 --> 00:04:22,320
processing topologies that we have

00:04:19,919 --> 00:04:23,199
over here you see is the solar adapter

00:04:22,320 --> 00:04:25,120
topology

00:04:23,199 --> 00:04:26,560
the main aim here is to transform the

00:04:25,120 --> 00:04:28,560
data into

00:04:26,560 --> 00:04:29,759
the solar documents and publish them

00:04:28,560 --> 00:04:33,440
into the respective

00:04:29,759 --> 00:04:34,160
shop topic so after this the data is

00:04:33,440 --> 00:04:36,320
being read

00:04:34,160 --> 00:04:38,639
from here into the kafka connect cluster

00:04:36,320 --> 00:04:39,919
which is then consumed by the solar sync

00:04:38,639 --> 00:04:42,000
and pushed into the solar which is

00:04:39,919 --> 00:04:43,759
served through the search service the

00:04:42,000 --> 00:04:45,680
other data which is non-searchable

00:04:43,759 --> 00:04:47,199
is published into the redis and it

00:04:45,680 --> 00:04:48,479
served through redis through the product

00:04:47,199 --> 00:04:51,199
service

00:04:48,479 --> 00:04:52,880
so that was of course the major

00:04:51,199 --> 00:04:55,199
transformation that we did

00:04:52,880 --> 00:04:57,520
to the uh platform of course we were

00:04:55,199 --> 00:04:59,360
expecting to see bigger challenges

00:04:57,520 --> 00:05:01,680
so we did face some challenges on the

00:04:59,360 --> 00:05:02,960
kafka streams first and the foremost was

00:05:01,680 --> 00:05:04,560
the slow and expensive

00:05:02,960 --> 00:05:06,880
kafka state restoration because we're

00:05:04,560 --> 00:05:08,960
talking about the data magnitude of

00:05:06,880 --> 00:05:10,639
almost 100 gb so we use cold

00:05:08,960 --> 00:05:12,080
bootstrapping to help us here i'm going

00:05:10,639 --> 00:05:13,039
to be showing you how we did that in the

00:05:12,080 --> 00:05:14,560
next slide

00:05:13,039 --> 00:05:16,400
uh we had to make a lot of

00:05:14,560 --> 00:05:17,759
customizations to the kafka solar

00:05:16,400 --> 00:05:19,680
connect because uh

00:05:17,759 --> 00:05:21,520
we ran into the race condition with the

00:05:19,680 --> 00:05:23,759
document addition and deletions

00:05:21,520 --> 00:05:25,440
so we had to modify the behavior as well

00:05:23,759 --> 00:05:28,080
for horizontal scalability

00:05:25,440 --> 00:05:30,000
we based it out of matrix used uh uh

00:05:28,080 --> 00:05:31,680
metrics here was consumer lags if the

00:05:30,000 --> 00:05:33,919
consumer lacks increases

00:05:31,680 --> 00:05:34,960
the certain number we spin up a new pot

00:05:33,919 --> 00:05:36,720
in the kubernetes

00:05:34,960 --> 00:05:39,280
to make sure that the consumer lag is

00:05:36,720 --> 00:05:40,800
taken care of the rock cd customization

00:05:39,280 --> 00:05:42,240
was done to ensure the cold

00:05:40,800 --> 00:05:44,639
bootstrapping is supported

00:05:42,240 --> 00:05:46,400
i would be showing you that uh of course

00:05:44,639 --> 00:05:48,400
as i also already mentioned that we are

00:05:46,400 --> 00:05:49,280
maintaining four high volume topology so

00:05:48,400 --> 00:05:51,280
managing

00:05:49,280 --> 00:05:52,479
such big amount of data was very

00:05:51,280 --> 00:05:54,800
challenging

00:05:52,479 --> 00:05:57,759
we were using the self-managed kafka

00:05:54,800 --> 00:05:59,759
here but we migrated to msk here

00:05:57,759 --> 00:06:00,800
we used mirror maker to come to our

00:05:59,759 --> 00:06:02,400
rescue

00:06:00,800 --> 00:06:04,000
so this is what i was talking about that

00:06:02,400 --> 00:06:06,960
by default uh

00:06:04,000 --> 00:06:07,360
in kafka we use change lock topic uh for

00:06:06,960 --> 00:06:11,039
the

00:06:07,360 --> 00:06:12,800
uh state recovery and replay but uh

00:06:11,039 --> 00:06:15,039
and in this case uh we were taking

00:06:12,800 --> 00:06:15,360
almost close to like 24 hours to recover

00:06:15,039 --> 00:06:17,440
our

00:06:15,360 --> 00:06:19,759
kind of data so we had to modify this

00:06:17,440 --> 00:06:20,479
default behavior instead of using change

00:06:19,759 --> 00:06:23,440
log topic

00:06:20,479 --> 00:06:24,560
for recovery data and replay we used the

00:06:23,440 --> 00:06:27,440
active

00:06:24,560 --> 00:06:27,840
rocksdb and every time the standby went

00:06:27,440 --> 00:06:29,759
down

00:06:27,840 --> 00:06:30,960
instead of replaying everything from

00:06:29,759 --> 00:06:33,199
changelog topic

00:06:30,960 --> 00:06:34,800
we copied the rocksdb from the active

00:06:33,199 --> 00:06:37,520
into the standby

00:06:34,800 --> 00:06:38,880
with this approach the data recovery was

00:06:37,520 --> 00:06:41,199
lightning fast

00:06:38,880 --> 00:06:42,240
uh it's like eight seconds for 100 gb of

00:06:41,199 --> 00:06:44,639
data

00:06:42,240 --> 00:06:45,280
and we were able to recover this uh

00:06:44,639 --> 00:06:47,759
challenge

00:06:45,280 --> 00:06:48,800
pretty well so this is called cold boots

00:06:47,759 --> 00:06:52,319
trapping

00:06:48,800 --> 00:06:54,639
and what did we improve in general as

00:06:52,319 --> 00:06:55,440
a whole uh transformation that we were

00:06:54,639 --> 00:06:58,160
able to

00:06:55,440 --> 00:07:00,319
update the products in real time we got

00:06:58,160 --> 00:07:01,759
rid of the index pipeline which took

00:07:00,319 --> 00:07:03,199
four hours now everything is getting

00:07:01,759 --> 00:07:05,599
updated in real time

00:07:03,199 --> 00:07:06,639
we upgraded solar from solar 6 to solar

00:07:05,599 --> 00:07:08,479
00:07:06,639 --> 00:07:10,560
we also moved from master slave to solar

00:07:08,479 --> 00:07:14,240
cloud we managed to switch to manage

00:07:10,560 --> 00:07:16,160
kafka and now we have uh corrected

00:07:14,240 --> 00:07:18,000
the architectural flaw that everything

00:07:16,160 --> 00:07:19,280
was served from solar now we're using

00:07:18,000 --> 00:07:20,960
reddish to support solar with the

00:07:19,280 --> 00:07:22,560
non-changing fields

00:07:20,960 --> 00:07:24,080
just in case if you have questions about

00:07:22,560 --> 00:07:25,599
the future work we are planning to open

00:07:24,080 --> 00:07:28,319
up to community and we are

00:07:25,599 --> 00:07:29,919
doing an intensive review of the changes

00:07:28,319 --> 00:07:31,440
that we have made and we are going to be

00:07:29,919 --> 00:07:32,960
opening up to the community for the

00:07:31,440 --> 00:07:34,000
customizations that we have made to the

00:07:32,960 --> 00:07:37,199
kafka connect

00:07:34,000 --> 00:07:38,160
the state recovery library slash cold

00:07:37,199 --> 00:07:41,120
bootstrapping

00:07:38,160 --> 00:07:42,960
and the kafka connect customizations so

00:07:41,120 --> 00:07:43,680
just in case you have any questions

00:07:42,960 --> 00:07:45,759
please

00:07:43,680 --> 00:07:47,039
feel free to reach out to me and some

00:07:45,759 --> 00:07:50,080
references for your

00:07:47,039 --> 00:07:52,240
online offline references

00:07:50,080 --> 00:07:53,840
so thank you i hope i was able to cover

00:07:52,240 --> 00:07:57,280
this in

00:07:53,840 --> 00:08:00,479
five minutes any questions

00:07:57,280 --> 00:08:02,319
thank you so much uh that was uh

00:08:00,479 --> 00:08:03,840
you covered such a lot in a short time

00:08:02,319 --> 00:08:04,639
uh i don't see questions here but i've

00:08:03,840 --> 00:08:07,360
got a few uh

00:08:04,639 --> 00:08:08,560
firstly thanks for uh yeah sharing those

00:08:07,360 --> 00:08:10,319
upstream changes that

00:08:08,560 --> 00:08:11,680
uh you're anticipating coming back to

00:08:10,319 --> 00:08:12,400
the community that's that's wonderful to

00:08:11,680 --> 00:08:14,240
hear

00:08:12,400 --> 00:08:15,680
and you also listed some challenges

00:08:14,240 --> 00:08:17,039
there the major challenges that you

00:08:15,680 --> 00:08:18,400
faced with the

00:08:17,039 --> 00:08:19,840
uh yeah with the new rollout that you

00:08:18,400 --> 00:08:20,960
described i wonder how did those

00:08:19,840 --> 00:08:22,720
challenges

00:08:20,960 --> 00:08:25,120
fit with the anticipated challenges so

00:08:22,720 --> 00:08:26,479
like expectations versus reality

00:08:25,120 --> 00:08:28,479
were the hardest parts the thought the

00:08:26,479 --> 00:08:29,680
part that you expected to be or yeah was

00:08:28,479 --> 00:08:31,280
it really quite unpredictable in the end

00:08:29,680 --> 00:08:32,959
well i i would say that that's a very

00:08:31,280 --> 00:08:35,440
good question and in fact uh

00:08:32,959 --> 00:08:37,680
we were anticipating changes we were in

00:08:35,440 --> 00:08:39,360
fact you know when we did the the poc we

00:08:37,680 --> 00:08:40,479
were not anticipating the changes like

00:08:39,360 --> 00:08:42,399
the race condition that it

00:08:40,479 --> 00:08:43,919
spoke about we were not even looking

00:08:42,399 --> 00:08:44,560
forward to such kind of race condition

00:08:43,919 --> 00:08:47,040
that the

00:08:44,560 --> 00:08:49,279
deletions would come before the editions

00:08:47,040 --> 00:08:52,560
and we would run into like the blocked

00:08:49,279 --> 00:08:53,279
uh infra completely similar to that was

00:08:52,560 --> 00:08:55,440
that

00:08:53,279 --> 00:08:56,399
the bootstrapping that i spoke about the

00:08:55,440 --> 00:08:58,399
bootstrapping took

00:08:56,399 --> 00:09:00,880
close to like 24 hours the first time we

00:08:58,399 --> 00:09:03,120
tried we were not even anticipating that

00:09:00,880 --> 00:09:04,480
because we did not understand or we did

00:09:03,120 --> 00:09:06,480
not actually estimate

00:09:04,480 --> 00:09:08,000
the data would grow as big as like 100

00:09:06,480 --> 00:09:10,080
gb so yes

00:09:08,000 --> 00:09:12,399
i would say that a lot of places we did

00:09:10,080 --> 00:09:15,600
not anticipate all of this to go

00:09:12,399 --> 00:09:17,360
uh haywire it took us a lot of time it

00:09:15,600 --> 00:09:20,080
took us about eight months

00:09:17,360 --> 00:09:22,399
to implement this but we're pretty happy

00:09:20,080 --> 00:09:23,440
how it looks right now

00:09:22,399 --> 00:09:25,040
that's great here it's always

00:09:23,440 --> 00:09:26,640
encouraging to hear how you know how it

00:09:25,040 --> 00:09:27,279
works in the real world in eight months

00:09:26,640 --> 00:09:28,640
yeah

00:09:27,279 --> 00:09:29,920
it sounds quite reasonable considering

00:09:28,640 --> 00:09:30,560
that the challenges that you face but

00:09:29,920 --> 00:09:32,320
it's

00:09:30,560 --> 00:09:37,839
always helpful to benchmark with

00:09:32,320 --> 00:09:37,839
organizations like that so thank you

00:09:53,600 --> 00:09:55,680

YouTube URL: https://www.youtube.com/watch?v=wush4EBP1bE


