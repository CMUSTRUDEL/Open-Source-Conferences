Title: Frank Conrad – Kubernetes and the dynamic world in the cloud
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Kubernetes get the container running platform to use with really easy add or remove resources. For real 12 factor app this is strait forward and the good way to go.

The challenge comes if you have long running (stateful) apps or big data apps like hadoop, spark, flink,... 

This talk show strategies to meditate this challenges and take advantages out of it. 

For example instead of run small cluster 24h, run a 12 times bigger cluster only for short intervals(~2h). If your jobs are scalable you get you results up to 12 times faster and bigger business value for same resource consumption. Or run a live recording, you need stay until event ends even it is a 24h. Run an AI training which don’t usually work so well with snapshots for recovery.

- use cloud in dynamic way (scale 100x of capacity in minutes is possible)
- per job cluster with the fitting sizing 
- leverage multiple node pools/groups 
- help from k8s operators for deployment 
- how this can work together with workflows like airflow
- k8s cluster auto scaler, how to leverage him and pitfalls
- k8s scheduler, alternatives , options to consider 

Speaker:
Frank Conrad – https://2021.berlinbuzzwords.de/member/frank-conrad

More: https://2021.berlinbuzzwords.de/session/kubernetes-and-dynamic-world-cloud
Captions: 
	00:00:07,040 --> 00:00:12,000
thank you for

00:00:08,639 --> 00:00:14,400
joining my talk yeah it's

00:00:12,000 --> 00:00:15,679
it's about kubernetes and the dynamic

00:00:14,400 --> 00:00:18,720
world in the cloud

00:00:15,679 --> 00:00:22,080
and i will talk a lot a lot about some

00:00:18,720 --> 00:00:23,359
special things about that me personally

00:00:22,080 --> 00:00:26,160
i co-founded

00:00:23,359 --> 00:00:27,279
7d and wonderloop and was working for 15

00:00:26,160 --> 00:00:29,519
years

00:00:27,279 --> 00:00:31,439
in the online advertising space and

00:00:29,519 --> 00:00:33,920
science three years i'm working

00:00:31,439 --> 00:00:34,960
in the kubernetes world and run their

00:00:33,920 --> 00:00:37,840
workloads

00:00:34,960 --> 00:00:38,800
at scale in in kubernetes in a dynamic

00:00:37,840 --> 00:00:41,280
way

00:00:38,800 --> 00:00:43,200
my current i'm currently working at

00:00:41,280 --> 00:00:46,079
cisco in the webex ai

00:00:43,200 --> 00:00:46,559
team and my patient has really large

00:00:46,079 --> 00:00:49,920
scale

00:00:46,559 --> 00:00:53,120
low latency processing for worldwide

00:00:49,920 --> 00:00:53,120
distributed systems

00:00:53,840 --> 00:00:57,520
what is the challenge what we typically

00:00:55,760 --> 00:01:00,640
have this is a

00:00:57,520 --> 00:01:02,480
business need dynamic scale even though

00:01:00,640 --> 00:01:04,320
they don't say it because they want to

00:01:02,480 --> 00:01:07,600
have the sla always

00:01:04,320 --> 00:01:09,360
in time and that you achieve it even on

00:01:07,600 --> 00:01:11,200
black friday and special

00:01:09,360 --> 00:01:12,560
events on the other side they want to

00:01:11,200 --> 00:01:15,600
have the operational

00:01:12,560 --> 00:01:17,600
costs under control and

00:01:15,600 --> 00:01:19,040
likely only this pay what they're really

00:01:17,600 --> 00:01:21,439
right now using

00:01:19,040 --> 00:01:24,000
and they want to have a reliable system

00:01:21,439 --> 00:01:26,960
what can deliver new features fast

00:01:24,000 --> 00:01:27,759
and as always time is money to get

00:01:26,960 --> 00:01:30,799
things

00:01:27,759 --> 00:01:33,840
fast out of the door the

00:01:30,799 --> 00:01:36,799
the operational challenges which coming

00:01:33,840 --> 00:01:38,960
to us as a in engineers and operations

00:01:36,799 --> 00:01:42,240
guys to that is basically

00:01:38,960 --> 00:01:44,960
that the the resource requirements of

00:01:42,240 --> 00:01:45,360
jobs are very different from job to drop

00:01:44,960 --> 00:01:48,399
and

00:01:45,360 --> 00:01:51,119
even on the each day or week or month

00:01:48,399 --> 00:01:52,079
or quarterly they can be have very

00:01:51,119 --> 00:01:55,520
different

00:01:52,079 --> 00:01:57,280
resource profiles and also

00:01:55,520 --> 00:01:59,600
if we have problems and needs to catch

00:01:57,280 --> 00:02:00,560
up or do some reprocessing because of

00:01:59,600 --> 00:02:03,840
new features

00:02:00,560 --> 00:02:03,840
or bug fixes

00:02:04,880 --> 00:02:11,440
we need to want to do this

00:02:08,319 --> 00:02:14,000
as fast as possible and with fixed size

00:02:11,440 --> 00:02:17,120
clusters which we have traditionally

00:02:14,000 --> 00:02:20,720
you um yeah you have to schedule the

00:02:17,120 --> 00:02:22,959
uh to schedule optimizations

00:02:20,720 --> 00:02:24,480
a lot the jobs influencing and watching

00:02:22,959 --> 00:02:27,440
each other

00:02:24,480 --> 00:02:29,599
and on uninterrupted jobs you also have

00:02:27,440 --> 00:02:33,120
some problems

00:02:29,599 --> 00:02:36,319
and what we

00:02:33,120 --> 00:02:39,440
ideally wants to have is

00:02:36,319 --> 00:02:42,480
that each job gets the right resources

00:02:39,440 --> 00:02:45,040
what it needs that's independent as

00:02:42,480 --> 00:02:45,920
as possible and have less influence

00:02:45,040 --> 00:02:49,920
against

00:02:45,920 --> 00:02:51,760
each other that you can can change

00:02:49,920 --> 00:02:54,720
existing jobs and you draw

00:02:51,760 --> 00:02:56,080
at new jobs with minimal effects to the

00:02:54,720 --> 00:02:58,480
runtime

00:02:56,080 --> 00:03:00,000
that you don't care so much about idle

00:02:58,480 --> 00:03:02,480
time or of your servers

00:03:00,000 --> 00:03:03,360
because they would go away that you only

00:03:02,480 --> 00:03:06,560
pay what you

00:03:03,360 --> 00:03:09,120
need and

00:03:06,560 --> 00:03:11,360
for that to get this you need a system

00:03:09,120 --> 00:03:12,319
where you can add and remove resources

00:03:11,360 --> 00:03:15,440
in real time

00:03:12,319 --> 00:03:18,000
means scale up and scale down

00:03:15,440 --> 00:03:20,239
and then you only pay what you are

00:03:18,000 --> 00:03:24,560
currently running

00:03:20,239 --> 00:03:26,799
and what effects can can we have such as

00:03:24,560 --> 00:03:30,080
a dynamic scaling system which

00:03:26,799 --> 00:03:31,680
can grow and shrink it's

00:03:30,080 --> 00:03:34,000
it's basically if you say you have a

00:03:31,680 --> 00:03:36,319
fixed budget and you run a job normally

00:03:34,000 --> 00:03:40,720
bisana tpu's a full day

00:03:36,319 --> 00:03:40,720
all the time the class of uh

00:03:42,000 --> 00:03:47,599
then you can also say i run an eight

00:03:45,200 --> 00:03:49,200
times bigger cluster only four hours and

00:03:47,599 --> 00:03:52,560
it cost you the same

00:03:49,200 --> 00:03:55,760
in the cloud and you get your results

00:03:52,560 --> 00:03:58,799
eight eight times earlier and

00:03:55,760 --> 00:04:02,080
if you say okay i i only need uh

00:03:58,799 --> 00:04:04,000
two uh two hours 800 cpus then i can

00:04:02,080 --> 00:04:07,360
spend the rest also

00:04:04,000 --> 00:04:10,560
as 36 cpus and

00:04:07,360 --> 00:04:14,720
running some small stuff there or or

00:04:10,560 --> 00:04:17,600
do some ui responsive things to that

00:04:14,720 --> 00:04:18,880
and you still under the same operational

00:04:17,600 --> 00:04:21,759
costs

00:04:18,880 --> 00:04:24,560
and on the other view to it is you have

00:04:21,759 --> 00:04:27,040
a static system

00:04:24,560 --> 00:04:28,639
where you only need basically the full

00:04:27,040 --> 00:04:30,560
capacity for eight hours

00:04:28,639 --> 00:04:31,919
and the rest of the day you only need

00:04:30,560 --> 00:04:35,040
for uh 40

00:04:31,919 --> 00:04:36,720
of the cpus and if you do this in a

00:04:35,040 --> 00:04:39,919
dynamic way you would save

00:04:36,720 --> 00:04:42,240
40 of your costs

00:04:39,919 --> 00:04:44,560
and if you then also think about oh i

00:04:42,240 --> 00:04:47,199
need to add something for peak events

00:04:44,560 --> 00:04:48,320
like back friday or other things if this

00:04:47,199 --> 00:04:50,960
is 30

00:04:48,320 --> 00:04:52,160
more what you want to need to add you

00:04:50,960 --> 00:04:56,080
save

00:04:52,160 --> 00:05:00,400
54 and if you need to add 100

00:04:56,080 --> 00:05:04,960
you even save 70 so it's a

00:05:00,400 --> 00:05:08,560
it's a simple way to save

00:05:04,960 --> 00:05:11,919
to save costs

00:05:08,560 --> 00:05:12,400
and what what basically is a cloud give

00:05:11,919 --> 00:05:14,960
us

00:05:12,400 --> 00:05:15,840
that you really only pay what you are

00:05:14,960 --> 00:05:18,400
using

00:05:15,840 --> 00:05:20,240
you can add and remove resources and

00:05:18,400 --> 00:05:22,560
this is typically also with a minimum

00:05:20,240 --> 00:05:24,560
no or minimal lead times we are not

00:05:22,560 --> 00:05:27,759
talking about minutes normally

00:05:24,560 --> 00:05:29,039
you which allows you to change cpu

00:05:27,759 --> 00:05:32,800
memory

00:05:29,039 --> 00:05:35,039
temp space ssds on on a on a frequent

00:05:32,800 --> 00:05:35,039
way

00:05:35,120 --> 00:05:40,800
to do this is normally a problem that

00:05:37,680 --> 00:05:43,919
you have to do it very provider specific

00:05:40,800 --> 00:05:47,120
and yeah this

00:05:43,919 --> 00:05:51,360
is sometimes a challenge

00:05:47,120 --> 00:05:52,560
so the most real cost savings out of the

00:05:51,360 --> 00:05:54,479
cloud you get really

00:05:52,560 --> 00:05:56,560
if you leverage the cloud in a dynamic

00:05:54,479 --> 00:06:00,319
way means scaling up and down

00:05:56,560 --> 00:06:03,759
and and in terms if you run starting

00:06:00,319 --> 00:06:06,400
systems they are most likely more

00:06:03,759 --> 00:06:08,720
expensive than on-prems over time

00:06:06,400 --> 00:06:10,960
if you are ignoring capex and opex

00:06:08,720 --> 00:06:13,840
effects

00:06:10,960 --> 00:06:16,560
but there's nothing nothing for free if

00:06:13,840 --> 00:06:19,840
you run a dynamic system you need to

00:06:16,560 --> 00:06:21,120
monitor your cost because you pay us

00:06:19,840 --> 00:06:24,880
what you use

00:06:21,120 --> 00:06:28,560
and if your system

00:06:24,880 --> 00:06:29,600
uh runs under high load in a traditional

00:06:28,560 --> 00:06:31,600
system you would get

00:06:29,600 --> 00:06:32,800
alerts that the system get overloaded

00:06:31,600 --> 00:06:36,000
your it might

00:06:32,800 --> 00:06:37,919
even break your sla in a dynamic world

00:06:36,000 --> 00:06:39,520
you would get the resources you still

00:06:37,919 --> 00:06:41,600
need your sla

00:06:39,520 --> 00:06:42,880
but you would get the cost alert because

00:06:41,600 --> 00:06:46,000
you're spending more

00:06:42,880 --> 00:06:46,000
than you have planned for

00:06:46,840 --> 00:06:53,280
and in this way what uh

00:06:50,080 --> 00:06:54,080
if you use kubernetes in the cloud what

00:06:53,280 --> 00:06:57,199
it will

00:06:54,080 --> 00:06:59,919
bring you kubernetes looks like a

00:06:57,199 --> 00:07:02,400
another cluster but we already had in

00:06:59,919 --> 00:07:05,440
the past the spark or hadoop clusters

00:07:02,400 --> 00:07:05,440
of link clusters

00:07:06,560 --> 00:07:09,599
but it has much better cloud support

00:07:09,120 --> 00:07:12,240
things

00:07:09,599 --> 00:07:14,880
it can really do this dynamic up and

00:07:12,240 --> 00:07:18,560
down scaling via cluster autoscaler

00:07:14,880 --> 00:07:20,800
it's in this way really dynamic

00:07:18,560 --> 00:07:23,039
and the other nice thing is you can also

00:07:20,800 --> 00:07:24,080
run your rest of your workloads which is

00:07:23,039 --> 00:07:27,440
not big data

00:07:24,080 --> 00:07:30,479
on on kubernetes clusters and

00:07:27,440 --> 00:07:32,639
makes them also in a very dynamic way

00:07:30,479 --> 00:07:33,680
so you get basically one deployment

00:07:32,639 --> 00:07:36,960
system

00:07:33,680 --> 00:07:38,960
for for everything and

00:07:36,960 --> 00:07:40,479
the other nice thing is that basically

00:07:38,960 --> 00:07:42,639
kubernetes covers you

00:07:40,479 --> 00:07:44,080
a lot of the provider dependent scale up

00:07:42,639 --> 00:07:46,479
and down

00:07:44,080 --> 00:07:49,039
mechanisms because they implemented this

00:07:46,479 --> 00:07:51,360
one time and you simply only use it

00:07:49,039 --> 00:07:53,680
and you have on your side only

00:07:51,360 --> 00:07:57,039
kubernetes dependent deployments

00:07:53,680 --> 00:08:00,080
which makes you in a way also provide a

00:07:57,039 --> 00:08:04,000
independent and this

00:08:00,080 --> 00:08:04,639
with this you go from from the vm world

00:08:04,000 --> 00:08:07,680
or app

00:08:04,639 --> 00:08:10,960
per vm world to the container world

00:08:07,680 --> 00:08:12,000
there are some uh all the cloud

00:08:10,960 --> 00:08:15,360
providers

00:08:12,000 --> 00:08:16,479
giving you some hosted kubernetes

00:08:15,360 --> 00:08:20,000
clusters

00:08:16,479 --> 00:08:24,840
this is an aws eks in asia aks

00:08:20,000 --> 00:08:26,720
and google gke which is most

00:08:24,840 --> 00:08:28,720
sophisticated and

00:08:26,720 --> 00:08:29,840
what kubernetes also introduce the

00:08:28,720 --> 00:08:32,240
operator

00:08:29,840 --> 00:08:33,279
pattern to make it simpler to run

00:08:32,240 --> 00:08:36,800
complex system

00:08:33,279 --> 00:08:40,000
like cassandra kafka in a

00:08:36,800 --> 00:08:43,680
kubernetes environment and this operator

00:08:40,000 --> 00:08:44,159
pattern it basically takes operations

00:08:43,680 --> 00:08:48,480
and

00:08:44,159 --> 00:08:49,920
sae knowledge of running complex systems

00:08:48,480 --> 00:08:52,080
and put it into code

00:08:49,920 --> 00:08:54,080
so there are operators available for

00:08:52,080 --> 00:08:55,760
kafka for databases and also for

00:08:54,080 --> 00:08:59,200
monitoring stuff

00:08:55,760 --> 00:09:01,519
and there they get controlled via custom

00:08:59,200 --> 00:09:02,560
resource definitions which are which are

00:09:01,519 --> 00:09:05,760
objects in

00:09:02,560 --> 00:09:06,720
inside the kubernetes system and you

00:09:05,760 --> 00:09:09,120
find them on

00:09:06,720 --> 00:09:11,600
operator hub or awesome operator and a

00:09:09,120 --> 00:09:15,200
couple of other sources google's helps

00:09:11,600 --> 00:09:16,399
google helps a lot and i get mostly

00:09:15,200 --> 00:09:19,600
installed via

00:09:16,399 --> 00:09:22,720
via helm which is the standard packet

00:09:19,600 --> 00:09:26,320
management for for kubernetes and

00:09:22,720 --> 00:09:29,600
tools like hand file car customize

00:09:26,320 --> 00:09:31,360
simplify your the usage

00:09:29,600 --> 00:09:33,920
of that and making also the

00:09:31,360 --> 00:09:36,560
configuration more dry

00:09:33,920 --> 00:09:37,200
what are the big data benefits what we

00:09:36,560 --> 00:09:41,279
see

00:09:37,200 --> 00:09:44,640
here is yeah for your scalable

00:09:41,279 --> 00:09:45,200
jobs you can produce far faster results

00:09:44,640 --> 00:09:46,959
because

00:09:45,200 --> 00:09:50,399
you're putting more resources for a

00:09:46,959 --> 00:09:53,279
short time in it and you get

00:09:50,399 --> 00:09:55,279
the results earlier you have a chance

00:09:53,279 --> 00:09:58,000
that this data size or load

00:09:55,279 --> 00:09:58,959
the compute can grow dynamically you

00:09:58,000 --> 00:10:01,120
only pay

00:09:58,959 --> 00:10:02,160
what you really allocate at that time

00:10:01,120 --> 00:10:05,920
you don't

00:10:02,160 --> 00:10:08,800
so much need to pre-allocate things for

00:10:05,920 --> 00:10:10,399
stuff what you only need in a rod uh for

00:10:08,800 --> 00:10:13,600
a short time

00:10:10,399 --> 00:10:16,160
like black friday and uh

00:10:13,600 --> 00:10:17,200
also if you have to recover uh from

00:10:16,160 --> 00:10:20,160
failed jobs

00:10:17,200 --> 00:10:21,839
you can add resources to make those

00:10:20,160 --> 00:10:25,040
faster

00:10:21,839 --> 00:10:28,079
things to give you some

00:10:25,040 --> 00:10:31,279
some numbers what it means to run

00:10:28,079 --> 00:10:34,480
a kubernetes cluster autoscaler

00:10:31,279 --> 00:10:37,680
how fast you get something so if here an

00:10:34,480 --> 00:10:41,440
example is a gke cluster

00:10:37,680 --> 00:10:43,920
1.18 with 18 node pools defined

00:10:41,440 --> 00:10:44,880
and they are all scale via cluster auto

00:10:43,920 --> 00:10:47,920
scaler

00:10:44,880 --> 00:10:51,519
if a part triggers in need of a new node

00:10:47,920 --> 00:10:53,760
it takes between 30 and 45 seconds

00:10:51,519 --> 00:10:55,200
to get support pot running on this new

00:10:53,760 --> 00:10:58,720
node

00:10:55,200 --> 00:10:59,760
if you try to start 3 000 pods which

00:10:58,720 --> 00:11:03,040
triggers

00:10:59,760 --> 00:11:05,200
basically thousand new nodes it takes

00:11:03,040 --> 00:11:08,640
four minutes to get them running

00:11:05,200 --> 00:11:09,279
if you need to start eighteen thousand

00:11:08,640 --> 00:11:12,720
pots

00:11:09,279 --> 00:11:15,680
which large images which also trigger a

00:11:12,720 --> 00:11:18,560
thousand nodes it takes approximately 17

00:11:15,680 --> 00:11:20,160
minutes to start them and so overhead of

00:11:18,560 --> 00:11:24,160
a node is typically

00:11:20,160 --> 00:11:25,680
odor 2 cpus and 2.7 gigabytes of five

00:11:24,160 --> 00:11:29,839
percent of the memory

00:11:25,680 --> 00:11:29,839
but you have an as an overhead per node

00:11:31,760 --> 00:11:35,040
what what is the cluster autoscaler

00:11:34,079 --> 00:11:38,160
really doing

00:11:35,040 --> 00:11:42,079
he is responsible to add and remove

00:11:38,160 --> 00:11:44,640
nodes to a cluster and for scale scale

00:11:42,079 --> 00:11:45,680
up it basically looks for unscheduleable

00:11:44,640 --> 00:11:48,240
pots

00:11:45,680 --> 00:11:49,040
runs the simulation to find the right

00:11:48,240 --> 00:11:51,760
node pool

00:11:49,040 --> 00:11:54,000
multiple are groups of nodes which have

00:11:51,760 --> 00:11:58,000
the same profile

00:11:54,000 --> 00:12:01,839
and do the right decision

00:11:58,000 --> 00:12:04,639
by uh via a weight in a similar way as a

00:12:01,839 --> 00:12:05,040
as the scheduler is doing and on gke it

00:12:04,639 --> 00:12:08,399
also

00:12:05,040 --> 00:12:11,120
includes the price of the nodes and

00:12:08,399 --> 00:12:12,399
the node with the highest weight or node

00:12:11,120 --> 00:12:15,920
pool with the highest weld

00:12:12,399 --> 00:12:18,800
will be used to add the new node

00:12:15,920 --> 00:12:20,000
for scaling down it looks for the

00:12:18,800 --> 00:12:23,360
underutilized

00:12:20,000 --> 00:12:27,200
nodes and see if they can

00:12:23,360 --> 00:12:30,720
be deleted and to always

00:12:27,200 --> 00:12:34,560
get the resources what you need you you

00:12:30,720 --> 00:12:39,839
should set the limits high enough for

00:12:34,560 --> 00:12:39,839
the specific node pools

00:12:40,639 --> 00:12:46,160
what is always the most more complicated

00:12:43,200 --> 00:12:48,720
than scale up is scale down

00:12:46,160 --> 00:12:49,200
you want to do a graceful shutdown so

00:12:48,720 --> 00:12:51,360
means

00:12:49,200 --> 00:12:52,639
you don't want to interrupt your stuff

00:12:51,360 --> 00:12:55,279
if if notes

00:12:52,639 --> 00:12:56,639
once have to go down because they are

00:12:55,279 --> 00:12:58,480
underutilized

00:12:56,639 --> 00:13:00,320
for that the cluster autoscaler is

00:12:58,480 --> 00:13:02,800
looking

00:13:00,320 --> 00:13:04,480
to that the node is getting

00:13:02,800 --> 00:13:07,440
underutilized

00:13:04,480 --> 00:13:09,279
it's then also checking are there pots

00:13:07,440 --> 00:13:11,360
with local storage

00:13:09,279 --> 00:13:13,600
or have no controller which means they

00:13:11,360 --> 00:13:16,959
would not automatically get restarted

00:13:13,600 --> 00:13:19,600
have a special annotation uh

00:13:16,959 --> 00:13:22,079
cluster auto seller kubernetes safe to

00:13:19,600 --> 00:13:25,600
evict false

00:13:22,079 --> 00:13:27,600
and and also looking are there resources

00:13:25,600 --> 00:13:31,519
somewhere else where the pot can run

00:13:27,600 --> 00:13:34,959
and only if this all fulfills

00:13:31,519 --> 00:13:38,399
in a will shut down so the

00:13:34,959 --> 00:13:41,600
safest way to avoid that your pot get

00:13:38,399 --> 00:13:44,320
uh get evicted from

00:13:41,600 --> 00:13:45,760
from a node is to set this annotation

00:13:44,320 --> 00:13:49,120
and even on the

00:13:45,760 --> 00:13:52,240
scale scale down if this has happened as

00:13:49,120 --> 00:13:53,600
the pot disruption budget gets respected

00:13:52,240 --> 00:13:57,120
as well as the

00:13:53,600 --> 00:14:00,399
graceful termination up to 10 minutes

00:13:57,120 --> 00:14:03,360
which which are both mechanisms

00:14:00,399 --> 00:14:07,600
for normal up applications to make sure

00:14:03,360 --> 00:14:07,600
that you do a really graceful shutdown

00:14:07,680 --> 00:14:12,720
what this means for for our big data

00:14:11,440 --> 00:14:14,959
jobs

00:14:12,720 --> 00:14:16,959
big data jobs are they are slightly

00:14:14,959 --> 00:14:20,160
different they are mostly really state

00:14:16,959 --> 00:14:22,880
stateful and the the

00:14:20,160 --> 00:14:23,760
failure recovery mechanisms are designed

00:14:22,880 --> 00:14:26,079
for

00:14:23,760 --> 00:14:27,839
that a few nodes get some problems and

00:14:26,079 --> 00:14:30,480
get restarted or

00:14:27,839 --> 00:14:31,760
things things happened but if if

00:14:30,480 --> 00:14:35,120
basically

00:14:31,760 --> 00:14:38,480
a bigger amount of uh of the

00:14:35,120 --> 00:14:40,720
uh pots get affected

00:14:38,480 --> 00:14:42,880
there's they're really starting

00:14:40,720 --> 00:14:44,959
struggling on it and especially

00:14:42,880 --> 00:14:46,079
if they would do a rolling update it

00:14:44,959 --> 00:14:48,079
would be probable

00:14:46,079 --> 00:14:49,440
probably for your spark jobs the worst

00:14:48,079 --> 00:14:52,639
thing worse thing

00:14:49,440 --> 00:14:55,120
to do and to

00:14:52,639 --> 00:14:55,680
uh to avoid this happened as i mentioned

00:14:55,120 --> 00:14:58,560
before

00:14:55,680 --> 00:14:58,959
set this annotation and then a node will

00:14:58,560 --> 00:15:03,920
run

00:14:58,959 --> 00:15:03,920
until your your ports are done

00:15:06,480 --> 00:15:12,160
the kubernetes scheduler which is a guy

00:15:08,639 --> 00:15:15,920
who's responsible to run

00:15:12,160 --> 00:15:16,800
to decide which node a port should run

00:15:15,920 --> 00:15:20,880
on it

00:15:16,800 --> 00:15:23,440
and it's basically looking for all nodes

00:15:20,880 --> 00:15:25,360
and to the heart filtering of the

00:15:23,440 --> 00:15:26,880
available resources and all the other

00:15:25,360 --> 00:15:30,480
criterias

00:15:26,880 --> 00:15:34,000
and if it figure find no node

00:15:30,480 --> 00:15:37,120
where the port can run on it

00:15:34,000 --> 00:15:41,279
markets as unschedulable which triggers

00:15:37,120 --> 00:15:45,040
then the cluster autoscaler to find

00:15:41,279 --> 00:15:48,240
or generate a new node otherwise

00:15:45,040 --> 00:15:52,880
it basically calculates

00:15:48,240 --> 00:15:56,160
a priority on on which node

00:15:52,880 --> 00:15:59,519
a pod should run and uh

00:15:56,160 --> 00:16:02,240
by as a default behavior uh

00:15:59,519 --> 00:16:03,360
it tries to do a very dist well

00:16:02,240 --> 00:16:07,040
distributed

00:16:03,360 --> 00:16:10,160
load across your current clusters and

00:16:07,040 --> 00:16:12,560
um the uh

00:16:10,160 --> 00:16:13,279
this is this means if you run a lot of

00:16:12,560 --> 00:16:16,959
low

00:16:13,279 --> 00:16:21,759
notes which have low low load

00:16:16,959 --> 00:16:25,360
they have get all evenly load on that

00:16:21,759 --> 00:16:28,240
this is done pot by pot

00:16:25,360 --> 00:16:28,560
which can introduce a lot of latency if

00:16:28,240 --> 00:16:31,759
you

00:16:28,560 --> 00:16:35,680
start a high number uh of pots

00:16:31,759 --> 00:16:38,639
and as i mentioned the number before a

00:16:35,680 --> 00:16:40,399
part of of this high high number on the

00:16:38,639 --> 00:16:42,240
high number of parts coming out of the

00:16:40,399 --> 00:16:44,000
scheduler latency

00:16:42,240 --> 00:16:47,199
there are some some ways where you can

00:16:44,000 --> 00:16:50,240
influence us by priority classes

00:16:47,199 --> 00:16:53,680
but at the end um yeah it's in

00:16:50,240 --> 00:16:55,759
pot-by-pot scheduling but which is a

00:16:53,680 --> 00:16:56,720
challenge for us for the big data jobs

00:16:55,759 --> 00:16:59,120
where you

00:16:56,720 --> 00:16:59,839
basically want mostly wants to have

00:16:59,120 --> 00:17:02,720
something

00:16:59,839 --> 00:17:03,440
all or nothing start all my pots of a

00:17:02,720 --> 00:17:07,280
job

00:17:03,440 --> 00:17:09,199
or or none of them if

00:17:07,280 --> 00:17:10,720
if their only party gets started your

00:17:09,199 --> 00:17:13,919
job will pro likely

00:17:10,720 --> 00:17:16,000
not good never finish and especially

00:17:13,919 --> 00:17:18,319
if you run multiple jobs at the same

00:17:16,000 --> 00:17:19,520
time which get affected by that you can

00:17:18,319 --> 00:17:22,720
even reach into the

00:17:19,520 --> 00:17:23,199
deadlock situation the solutions to that

00:17:22,720 --> 00:17:25,199
is

00:17:23,199 --> 00:17:27,280
one to use the cluster autoscaler and

00:17:25,199 --> 00:17:30,240
always the needed resources to that

00:17:27,280 --> 00:17:31,600
so that you always can run your pots or

00:17:30,240 --> 00:17:34,480
use another

00:17:31,600 --> 00:17:38,480
scheduler which addresses this problem

00:17:34,480 --> 00:17:38,480
which is basically a game schedule

00:17:38,799 --> 00:17:42,799
this uh to use other schedulers is

00:17:41,360 --> 00:17:44,720
basically the solution on

00:17:42,799 --> 00:17:47,440
very large clusters or if you have

00:17:44,720 --> 00:17:51,760
limited resources

00:17:47,440 --> 00:17:54,000
and why is very large also because

00:17:51,760 --> 00:17:56,080
very large means you hit limits and at

00:17:54,000 --> 00:17:58,960
some point you can't schedule any more

00:17:56,080 --> 00:18:00,960
so it's it's the same use case as

00:17:58,960 --> 00:18:02,880
limited resources

00:18:00,960 --> 00:18:04,880
and for that you need something like a

00:18:02,880 --> 00:18:07,679
gangster scheduler

00:18:04,880 --> 00:18:09,280
potentially also this priority cues

00:18:07,679 --> 00:18:12,840
something what you

00:18:09,280 --> 00:18:14,240
know from your old yarn clusters for

00:18:12,840 --> 00:18:16,559
example

00:18:14,240 --> 00:18:18,000
with with kubernetes you can have

00:18:16,559 --> 00:18:20,480
multiple schedulers

00:18:18,000 --> 00:18:21,280
running and there's also some scheduler

00:18:20,480 --> 00:18:24,400
plugins

00:18:21,280 --> 00:18:28,720
available for customizations to use that

00:18:24,400 --> 00:18:30,640
has some pros and cons and

00:18:28,720 --> 00:18:32,000
the things what you need to care of is

00:18:30,640 --> 00:18:35,039
each pot must

00:18:32,000 --> 00:18:37,440
to have have a scheduler assignment and

00:18:35,039 --> 00:18:39,840
much more important

00:18:37,440 --> 00:18:41,600
a node pool or a node should be only

00:18:39,840 --> 00:18:43,520
managed by one scheduler

00:18:41,600 --> 00:18:45,679
so you have to make sure that the

00:18:43,520 --> 00:18:47,360
default scheduler or the other scalia

00:18:45,679 --> 00:18:51,280
are not managing

00:18:47,360 --> 00:18:54,400
load load on on the same nodes otherwise

00:18:51,280 --> 00:18:56,880
it's planned for a disaster

00:18:54,400 --> 00:18:57,600
and there are sometimes some challenges

00:18:56,880 --> 00:19:01,840
if you use

00:18:57,600 --> 00:19:06,000
provided a uh based kubernetes clusters

00:19:01,840 --> 00:19:08,480
on on that um

00:19:06,000 --> 00:19:10,000
there are there are some schedulers

00:19:08,480 --> 00:19:13,600
available the

00:19:10,000 --> 00:19:16,160
most common use is the q patch scheduler

00:19:13,600 --> 00:19:17,600
there's base based on that much more

00:19:16,160 --> 00:19:21,440
well integrated things

00:19:17,600 --> 00:19:25,600
is a vulcan carno one

00:19:21,440 --> 00:19:27,760
then is also apache unicorn

00:19:25,600 --> 00:19:29,200
which is also a gang scheduler and

00:19:27,760 --> 00:19:32,880
target really large

00:19:29,200 --> 00:19:35,840
large scale and fast scheduling and

00:19:32,880 --> 00:19:36,320
in the kubernetes scheduling framework

00:19:35,840 --> 00:19:39,280
there are

00:19:36,320 --> 00:19:41,520
a couple of scheduler plugins available

00:19:39,280 --> 00:19:46,559
to do customizations

00:19:41,520 --> 00:19:50,320
and to get the scheduling

00:19:46,559 --> 00:19:52,640
influenced in the way you want

00:19:50,320 --> 00:19:54,320
the alternative way to use the cluster

00:19:52,640 --> 00:19:57,280
autoscaler is my

00:19:54,320 --> 00:19:57,679
my my preferred way because you always

00:19:57,280 --> 00:20:00,799
get

00:19:57,679 --> 00:20:03,760
at the capacity what you need

00:20:00,799 --> 00:20:04,880
and with that uh we said you don't have

00:20:03,760 --> 00:20:08,559
all the challenges

00:20:04,880 --> 00:20:12,799
with the uh to run

00:20:08,559 --> 00:20:15,520
a second scheduler in your cluster

00:20:12,799 --> 00:20:16,960
and this works very well with uh with

00:20:15,520 --> 00:20:21,600
all the cloud environments

00:20:16,960 --> 00:20:25,360
because you get the resources

00:20:21,600 --> 00:20:27,280
you want in in your limits in your quota

00:20:25,360 --> 00:20:31,840
where you have to look to

00:20:27,280 --> 00:20:34,880
and you have to set the

00:20:31,840 --> 00:20:37,919
max nodes per node pool

00:20:34,880 --> 00:20:41,039
high enough that you always have

00:20:37,919 --> 00:20:41,440
capacity available and have to monitor

00:20:41,039 --> 00:20:44,720
on

00:20:41,440 --> 00:20:47,120
for sure your quota and your cost

00:20:44,720 --> 00:20:48,640
there's a challenges especially for

00:20:47,120 --> 00:20:52,559
scale down if you

00:20:48,640 --> 00:20:52,559
run more than one pot per node

00:20:52,799 --> 00:20:56,960
why is that that has to do with the cube

00:20:55,200 --> 00:21:00,080
scheduler

00:20:56,960 --> 00:21:01,760
behavior of the weld distribution what i

00:21:00,080 --> 00:21:03,679
come

00:21:01,760 --> 00:21:05,200
come to it so the preferred way is

00:21:03,679 --> 00:21:08,240
really that you run

00:21:05,200 --> 00:21:10,880
one part per node because this makes

00:21:08,240 --> 00:21:11,760
also scale down and so on efficient and

00:21:10,880 --> 00:21:14,799
with that

00:21:11,760 --> 00:21:14,799
you really only

00:21:15,200 --> 00:21:19,440
i have the running resources what you

00:21:17,200 --> 00:21:21,600
need at the time

00:21:19,440 --> 00:21:23,280
the challenge if you are running

00:21:21,600 --> 00:21:25,440
multiple pods is basically

00:21:23,280 --> 00:21:26,320
you have to wait until the last port is

00:21:25,440 --> 00:21:29,600
gone

00:21:26,320 --> 00:21:32,080
uh on the uh

00:21:29,600 --> 00:21:34,080
from the node until they can go down

00:21:32,080 --> 00:21:34,480
means especially if you run multiple

00:21:34,080 --> 00:21:38,080
jobs

00:21:34,480 --> 00:21:41,760
at the same time the longest running job

00:21:38,080 --> 00:21:43,760
is dictating this and also if your

00:21:41,760 --> 00:21:46,559
pods ending and you are starting on the

00:21:43,760 --> 00:21:48,159
already running notes

00:21:46,559 --> 00:21:50,480
you run into the problem that they

00:21:48,159 --> 00:21:53,679
evenly get distributed by the

00:21:50,480 --> 00:21:55,679
by the scheduler which also

00:21:53,679 --> 00:21:56,960
holds a lot no it's longer than it's

00:21:55,679 --> 00:22:00,320
really needed

00:21:56,960 --> 00:22:01,039
you can optimize that by getting strong

00:22:00,320 --> 00:22:04,400
being

00:22:01,039 --> 00:22:05,520
packing on your kubernetes clusters you

00:22:04,400 --> 00:22:08,240
can set

00:22:05,520 --> 00:22:11,280
the auto scaling profile to optimize

00:22:08,240 --> 00:22:14,720
your utilization in gke for example

00:22:11,280 --> 00:22:14,720
which solves this problem

00:22:16,240 --> 00:22:20,480
then then the next thing to optimize the

00:22:18,640 --> 00:22:22,960
things is really to use

00:22:20,480 --> 00:22:25,520
dedicated node pools means the node pool

00:22:22,960 --> 00:22:28,559
which only runs certain workloads

00:22:25,520 --> 00:22:31,360
what you want to it that is

00:22:28,559 --> 00:22:32,880
sounds a little bit strange because and

00:22:31,360 --> 00:22:35,440
in many how to's you

00:22:32,880 --> 00:22:36,640
you you read you should not do it you

00:22:35,440 --> 00:22:39,760
should

00:22:36,640 --> 00:22:41,440
let kubernetes figure out the stuff my

00:22:39,760 --> 00:22:44,559
experience is

00:22:41,440 --> 00:22:45,840
it's better better in the dynamic world

00:22:44,559 --> 00:22:49,120
it's better to use

00:22:45,840 --> 00:22:51,200
dedicated notebooks for a lot more load

00:22:49,120 --> 00:22:53,200
to make sure that they are also scaling

00:22:51,200 --> 00:22:56,320
down in a way

00:22:53,200 --> 00:22:59,600
you are um you are thinking and you are

00:22:56,320 --> 00:23:00,559
also much more controlled the scheduling

00:22:59,600 --> 00:23:04,320
by that

00:23:00,559 --> 00:23:08,559
and creating node pools basically

00:23:04,320 --> 00:23:11,679
you add your own labels

00:23:08,559 --> 00:23:14,720
to it to which are there to bind

00:23:11,679 --> 00:23:18,000
a pot to this particular node pool

00:23:14,720 --> 00:23:18,799
and only one label helps you also to do

00:23:18,000 --> 00:23:21,120
upgrades

00:23:18,799 --> 00:23:24,000
updates of node pools because you create

00:23:21,120 --> 00:23:26,640
a new one which has the same labels and

00:23:24,000 --> 00:23:27,679
um and you can then migrate the workload

00:23:26,640 --> 00:23:30,720
over that

00:23:27,679 --> 00:23:33,120
and also you have the names under

00:23:30,720 --> 00:23:36,159
control so you are not depending on

00:23:33,120 --> 00:23:36,720
some kubernetes changes or provider

00:23:36,159 --> 00:23:39,760
changes

00:23:36,720 --> 00:23:42,240
of the labels and

00:23:39,760 --> 00:23:43,760
you need also the obtains which are

00:23:42,240 --> 00:23:47,120
needed to block

00:23:43,760 --> 00:23:49,679
unwanted ports from from

00:23:47,120 --> 00:23:50,240
from your nodes you should say set the

00:23:49,679 --> 00:23:53,200
minimum

00:23:50,240 --> 00:23:54,960
size of a notebook always to zero that

00:23:53,200 --> 00:23:58,000
if you don't use a node pool

00:23:54,960 --> 00:24:00,799
you don't have to pay anything and

00:23:58,000 --> 00:24:01,600
the max as i said before you should set

00:24:00,799 --> 00:24:05,600
to

00:24:01,600 --> 00:24:09,039
to something that you never never reach

00:24:05,600 --> 00:24:09,039
but don't forget your quota

00:24:09,919 --> 00:24:17,600
the next optimization piece what we

00:24:13,600 --> 00:24:17,600
can do is leverage

00:24:17,840 --> 00:24:22,080
uh to to leverage the default cloud way

00:24:21,600 --> 00:24:25,120
to

00:24:22,080 --> 00:24:28,000
separate storage from compute

00:24:25,120 --> 00:24:30,080
because this allows you basically to

00:24:28,000 --> 00:24:33,120
restart your port

00:24:30,080 --> 00:24:36,640
uh with a different resource requests

00:24:33,120 --> 00:24:40,000
and getting the same data back back from

00:24:36,640 --> 00:24:42,480
from the storage um and

00:24:40,000 --> 00:24:43,840
it could be you also with a trick help

00:24:42,480 --> 00:24:47,919
you to

00:24:43,840 --> 00:24:51,279
save cross zones network charges

00:24:47,919 --> 00:24:54,720
if if they are a problem for you

00:24:51,279 --> 00:24:58,400
so um

00:24:54,720 --> 00:25:00,960
as i said before big data jobs

00:24:58,400 --> 00:25:03,919
struggling if if they're getting

00:25:00,960 --> 00:25:05,760
a huge percentage of ports failing which

00:25:03,919 --> 00:25:09,120
has happened if a zone is failing

00:25:05,760 --> 00:25:11,039
and you run it across multiple zones

00:25:09,120 --> 00:25:12,480
for example three means thirty percent

00:25:11,039 --> 00:25:15,760
of your pots are gone

00:25:12,480 --> 00:25:18,799
or or getting problems and

00:25:15,760 --> 00:25:21,520
um there's basically a trick if you're

00:25:18,799 --> 00:25:24,080
using storage which is accessible from

00:25:21,520 --> 00:25:27,279
multiple zones like object stores

00:25:24,080 --> 00:25:30,400
network file systems or in

00:25:27,279 --> 00:25:32,080
gke you can use regional persistent

00:25:30,400 --> 00:25:34,720
disks

00:25:32,080 --> 00:25:36,799
which means you start your job in one

00:25:34,720 --> 00:25:37,760
zone and if the zone get a problem you

00:25:36,799 --> 00:25:40,880
restart it

00:25:37,760 --> 00:25:44,559
in in another zone which helps you

00:25:40,880 --> 00:25:47,760
to minimize the network costs as well

00:25:44,559 --> 00:25:51,200
as reduce the latency in the

00:25:47,760 --> 00:25:51,200
communication between the nodes

00:25:52,080 --> 00:25:57,440
this is a stateful set which is

00:25:55,440 --> 00:25:59,520
manage basically your deployment with

00:25:57,440 --> 00:26:04,720
dedicated volumes per

00:25:59,520 --> 00:26:08,320
per pot basically stateful deployments

00:26:04,720 --> 00:26:11,840
you have also the possibility to change

00:26:08,320 --> 00:26:11,840
the resource requests

00:26:11,919 --> 00:26:19,039
or also node affinities tolerations

00:26:16,320 --> 00:26:21,200
and if you change this stay as a

00:26:19,039 --> 00:26:25,440
stateful set will then take care

00:26:21,200 --> 00:26:29,039
to graceful restart all your pots

00:26:25,440 --> 00:26:29,039
which allows you then to

00:26:30,080 --> 00:26:36,720
add cpu capacity during the day

00:26:33,600 --> 00:26:39,760
or at memory on kafka for example

00:26:36,720 --> 00:26:40,400
you do a rolling restart and double the

00:26:39,760 --> 00:26:43,360
memory

00:26:40,400 --> 00:26:44,640
for the high traffic hours for example

00:26:43,360 --> 00:26:47,679
and because

00:26:44,640 --> 00:26:48,880
it depends on the workload how how how

00:26:47,679 --> 00:26:50,960
fast this goes

00:26:48,880 --> 00:26:52,159
but you can't even do it multiple times

00:26:50,960 --> 00:26:55,600
a day and

00:26:52,159 --> 00:27:00,720
basically allows you again to

00:26:55,600 --> 00:27:00,720
adapt your resource definitions to

00:27:02,080 --> 00:27:06,480
to your need and the similar stuff can

00:27:05,600 --> 00:27:09,120
be done

00:27:06,480 --> 00:27:11,600
with the uh most of the operators which

00:27:09,120 --> 00:27:15,279
are also triggering then restarts

00:27:11,600 --> 00:27:19,520
which are yeah operator and

00:27:15,279 --> 00:27:23,120
uh and so on so so so this is

00:27:19,520 --> 00:27:26,159
big big data drops like

00:27:23,120 --> 00:27:28,159
spark flink running at the end in their

00:27:26,159 --> 00:27:31,600
own sparkling cluster

00:27:28,159 --> 00:27:32,880
and in kubernetes you can spin up those

00:27:31,600 --> 00:27:36,320
clusters basically

00:27:32,880 --> 00:27:37,600
on demand and if you run one one job in

00:27:36,320 --> 00:27:40,799
the cluster

00:27:37,600 --> 00:27:42,480
and this allows you to specify the size

00:27:40,799 --> 00:27:46,240
of the cluster per job

00:27:42,480 --> 00:27:49,760
you have the cluster per job on demand

00:27:46,240 --> 00:27:53,120
and this allows you the right sizing

00:27:49,760 --> 00:27:56,399
and with different node profiles

00:27:53,120 --> 00:27:59,679
on on on different node pools you can

00:27:56,399 --> 00:28:01,600
adjust the capacity there and the

00:27:59,679 --> 00:28:03,520
cluster autoscaler will take care that

00:28:01,600 --> 00:28:05,600
the resources are available

00:28:03,520 --> 00:28:08,159
and the operators makes the deployment

00:28:05,600 --> 00:28:10,640
very very simple

00:28:08,159 --> 00:28:12,480
and and for spark there are currently

00:28:10,640 --> 00:28:14,840
two install

00:28:12,480 --> 00:28:16,720
interesting operators what i mentioned

00:28:14,840 --> 00:28:19,679
here

00:28:16,720 --> 00:28:21,360
i have good experience with the google

00:28:19,679 --> 00:28:25,760
one because it's a very good

00:28:21,360 --> 00:28:25,760
integration in all the kubernetes stuff

00:28:25,919 --> 00:28:29,440
and if you use airflow you have even a

00:28:28,880 --> 00:28:32,640
native

00:28:29,440 --> 00:28:34,640
integration to easily use it and

00:28:32,640 --> 00:28:36,159
on the others you have to do a little

00:28:34,640 --> 00:28:39,600
bit

00:28:36,159 --> 00:28:43,279
more yeah airflow

00:28:39,600 --> 00:28:45,760
is a very very integrated

00:28:43,279 --> 00:28:47,600
now with this kubernetes stuff and

00:28:45,760 --> 00:28:51,600
allows you

00:28:47,600 --> 00:28:54,960
to create spark

00:28:51,600 --> 00:28:57,919
up applications on demand there are

00:28:54,960 --> 00:29:00,080
some some other tools allow similar

00:28:57,919 --> 00:29:02,720
stuff

00:29:00,080 --> 00:29:03,919
yeah flink operators are also available

00:29:02,720 --> 00:29:07,120
for flink jobs

00:29:03,919 --> 00:29:07,840
and also as as in in the talk before

00:29:07,120 --> 00:29:11,039
mentioned

00:29:07,840 --> 00:29:14,159
there are others as well available

00:29:11,039 --> 00:29:16,159
so for for storage there are some

00:29:14,159 --> 00:29:18,000
some some hints especially if you use

00:29:16,159 --> 00:29:21,679
object stores

00:29:18,000 --> 00:29:25,200
uh because they're scaling right now all

00:29:21,679 --> 00:29:27,360
automatically you ha you should always

00:29:25,200 --> 00:29:29,279
reuse your buckets always use the same

00:29:27,360 --> 00:29:31,760
buckets for the same jobs

00:29:29,279 --> 00:29:33,840
use the same pattern even do some time

00:29:31,760 --> 00:29:36,880
pre-conditioning or warm-up

00:29:33,840 --> 00:29:40,399
to allow the high parallelisms what

00:29:36,880 --> 00:29:44,159
big data jobs typically need

00:29:40,399 --> 00:29:46,159
needing and also avoid writing to local

00:29:44,159 --> 00:29:51,039
images and use us

00:29:46,159 --> 00:29:55,840
for temp local ssds or things like that

00:29:51,039 --> 00:29:55,840
and for the for the images

00:29:56,000 --> 00:30:00,240
avoid large large images whenever is

00:29:59,120 --> 00:30:03,360
possible

00:30:00,240 --> 00:30:05,440
so try to load the data some somewhere

00:30:03,360 --> 00:30:09,200
else like an nfs server

00:30:05,440 --> 00:30:10,480
afs or gcp file store or even loaded

00:30:09,200 --> 00:30:14,960
downloaded

00:30:10,480 --> 00:30:14,960
from from an from an object store

00:30:16,000 --> 00:30:23,919
for uninterruptable jobs

00:30:19,840 --> 00:30:24,880
on on kubernetes run them as a job or as

00:30:23,919 --> 00:30:28,159
a static port

00:30:24,880 --> 00:30:30,559
set the limits and the requests equally

00:30:28,159 --> 00:30:33,039
to make sure that they're not getting

00:30:30,559 --> 00:30:37,039
get affected much by other pots

00:30:33,039 --> 00:30:39,440
and the annotation

00:30:37,039 --> 00:30:39,440
again

00:30:40,480 --> 00:30:46,000
and if you want to to run such a thing

00:30:43,520 --> 00:30:47,760
get the kubernetes cluster like a gce

00:30:46,000 --> 00:30:50,240
have make sure that you have the

00:30:47,760 --> 00:30:51,919
cluster autoscaler and add a couple of

00:30:50,240 --> 00:30:54,880
stuff

00:30:51,919 --> 00:30:56,799
stuff then there is here example hand

00:30:54,880 --> 00:31:00,480
shots to use

00:30:56,799 --> 00:31:03,519
yeah and that

00:31:00,480 --> 00:31:19,840
said bosses thank you for listening

00:31:03,519 --> 00:31:19,840
and if you have questions let me know

00:31:26,880 --> 00:31:28,960

YouTube URL: https://www.youtube.com/watch?v=iY9H5luFwns


