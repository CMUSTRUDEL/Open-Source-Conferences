Title: Berlin Buzzwords 2017: Marcin Szymaniuk - Apache Spark? If only it worked #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Do you have plans to start working with Apache Spark? Are you already working with Spark but you haven’t gotten the expected performance and stability and you are not sure where to look for a fix?

Spark has a very nice API and it promises high performance for crunching large datasets. It’s really easy to write an app in Spark, unfortunately, it’s also easy to write one which doesn’t perform the way you would expect or just fails for no obvious reason.

This talk will consist of multiple common problems you might face when running Spark at full scale and, of course, solutions for solving them. Each of the problems I will cover will come with well-described background and examples so that it will be understood by people with no Spark experience. However, people who are working with Spark are the main audience. The ultimate objective is to give the audience a practical framework for optimizing the most common problems with Spark applications.

Read more:
https://2017.berlinbuzzwords.de/17/session/apache-spark-if-only-it-worked

About Marcin Szymaniuk:
https://2017.berlinbuzzwords.de/users/marcin-szymaniuk

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,440 --> 00:00:09,990
do you plan to start data analysis and

00:00:09,100 --> 00:00:13,710
your consider

00:00:09,990 --> 00:00:16,470
using Sparkle read for it have you just

00:00:13,710 --> 00:00:19,770
started using spark and you want to

00:00:16,470 --> 00:00:22,529
avoid pitfalls are you already using

00:00:19,770 --> 00:00:25,160
spark but you are not really sure or

00:00:22,529 --> 00:00:29,670
happy about it performance or stability

00:00:25,160 --> 00:00:32,309
by the next of by the end of next half

00:00:29,670 --> 00:00:34,949
an hour I'll you'll have an overview of

00:00:32,309 --> 00:00:38,040
most common problem faced when running

00:00:34,949 --> 00:00:42,750
spark on production and solutions for

00:00:38,040 --> 00:00:45,510
them my name is marcin I'm data engineer

00:00:42,750 --> 00:00:48,089
act on tooth data I have almost a decade

00:00:45,510 --> 00:00:51,960
of experience I've worked for companies

00:00:48,089 --> 00:00:57,570
like Spotify Apple as you small smaller

00:00:51,960 --> 00:01:01,589
startups so I have quite some experience

00:00:57,570 --> 00:01:03,120
in data and today also the spark part of

00:01:01,589 --> 00:01:04,460
it

00:01:03,120 --> 00:01:07,979
[Applause]

00:01:04,460 --> 00:01:11,850
apache spark if only it worked it's

00:01:07,979 --> 00:01:14,659
quite controversial but it was inspired

00:01:11,850 --> 00:01:17,159
by a blog post I found a few years ago

00:01:14,659 --> 00:01:19,530
so when I was writing my first spark

00:01:17,159 --> 00:01:21,780
application I was looking for solution

00:01:19,530 --> 00:01:23,969
for my problems and I found this blog

00:01:21,780 --> 00:01:27,420
post and author these blog posts was

00:01:23,969 --> 00:01:29,280
giving quite a lot of advices but also

00:01:27,420 --> 00:01:33,450
he was complaining quite a lot about how

00:01:29,280 --> 00:01:38,159
hard is to make spark work reliably on

00:01:33,450 --> 00:01:41,310
production so honestly at that point I

00:01:38,159 --> 00:01:44,789
felt exactly the same but fortunately

00:01:41,310 --> 00:01:46,619
quite a lot has changed since then I've

00:01:44,789 --> 00:01:49,049
learned quite a lot and also sparking

00:01:46,619 --> 00:01:51,450
proof quite a lot so today I'm not going

00:01:49,049 --> 00:01:53,069
to put you off from spark I'm going to

00:01:51,450 --> 00:01:58,319
share my lesson learned so it will be

00:01:53,069 --> 00:02:00,479
easier for you we will start with spark

00:01:58,319 --> 00:02:04,469
execution model and that will be our

00:02:00,479 --> 00:02:08,300
foundation two reason another understand

00:02:04,469 --> 00:02:11,039
various problems you might face on scale

00:02:08,300 --> 00:02:13,950
we will be looking at various classes of

00:02:11,039 --> 00:02:17,730
problems like sizing executors skew data

00:02:13,950 --> 00:02:23,810
locality caching I'll also mention a few

00:02:17,730 --> 00:02:23,810
words about testing and debugging spark

00:02:24,320 --> 00:02:31,920
so what is spark is a general engine for

00:02:29,100 --> 00:02:36,810
distributed data processing it has

00:02:31,920 --> 00:02:40,290
support for various various programming

00:02:36,810 --> 00:02:42,510
languages it supports an API for for

00:02:40,290 --> 00:02:44,310
various program programming languages it

00:02:42,510 --> 00:02:46,560
has support for sequel

00:02:44,310 --> 00:02:48,990
it has streaming library it has machine

00:02:46,560 --> 00:02:53,280
learning library but today we're going

00:02:48,990 --> 00:02:56,940
to focus just on the core of SPARC yeah

00:02:53,280 --> 00:03:00,200
but what it really is why why would you

00:02:56,940 --> 00:03:03,150
need it so like one of the very common

00:03:00,200 --> 00:03:06,209
use cases first part is that you have an

00:03:03,150 --> 00:03:09,270
application you write your data and you

00:03:06,209 --> 00:03:10,920
have some analytics who are connecting

00:03:09,270 --> 00:03:15,150
to the data in order to understand

00:03:10,920 --> 00:03:17,580
what's going on with your users how how

00:03:15,150 --> 00:03:20,459
they are using the application how to

00:03:17,580 --> 00:03:22,200
make them pay why don't they pay to get

00:03:20,459 --> 00:03:25,550
basically some insight from your

00:03:22,200 --> 00:03:28,560
application but that is tricky to scale

00:03:25,550 --> 00:03:30,420
that is also a bit risky because you're

00:03:28,560 --> 00:03:32,970
on analytics might be a bit too greedy

00:03:30,420 --> 00:03:38,730
and and and then then you have a problem

00:03:32,970 --> 00:03:41,519
with with your with your app so so

00:03:38,730 --> 00:03:44,340
another approach very very common very

00:03:41,519 --> 00:03:47,489
very standard one is that you store your

00:03:44,340 --> 00:03:50,850
analytics data somewhere completely else

00:03:47,489 --> 00:03:53,250
so for instance in HDFS and you use

00:03:50,850 --> 00:03:58,799
SPARC to actually run analysis on top

00:03:53,250 --> 00:04:03,959
referrers of your data okay let's spark

00:03:58,799 --> 00:04:06,600
with an execution model sparks

00:04:03,959 --> 00:04:09,510
introduces resilient distributed data

00:04:06,600 --> 00:04:12,600
set and it's basically an abstraction on

00:04:09,510 --> 00:04:14,750
top of distributed data set the

00:04:12,600 --> 00:04:18,690
resilient part of it means it can be

00:04:14,750 --> 00:04:20,910
recalculated if the data is lost so we

00:04:18,690 --> 00:04:23,130
have a data set which which is split

00:04:20,910 --> 00:04:26,550
into multiple partitions into multiple

00:04:23,130 --> 00:04:29,340
nodes and it's just transparent to you

00:04:26,550 --> 00:04:31,080
that is distributed and then when you

00:04:29,340 --> 00:04:34,680
run your SPARC application SPARC

00:04:31,080 --> 00:04:36,210
pipelines operations until they can be

00:04:34,680 --> 00:04:39,360
done in context of just

00:04:36,210 --> 00:04:41,490
single partition until operation on

00:04:39,360 --> 00:04:44,250
given partition don't need data from

00:04:41,490 --> 00:04:48,720
another ones in such a group of

00:04:44,250 --> 00:04:52,349
pipelines operations is called state but

00:04:48,720 --> 00:04:54,389
eventually you want to exchange data

00:04:52,349 --> 00:04:57,470
between partitions eventually you do a

00:04:54,389 --> 00:05:00,630
an operation with like group by key and

00:04:57,470 --> 00:05:04,919
such kind of operation is called shuffle

00:05:00,630 --> 00:05:07,039
and that one we will be actually focus

00:05:04,919 --> 00:05:09,539
focusing on quite a lot because that is

00:05:07,039 --> 00:05:13,699
an expensive one and also that is

00:05:09,539 --> 00:05:17,039
causing quite a lot of problems and your

00:05:13,699 --> 00:05:23,370
your application usually consists of

00:05:17,039 --> 00:05:26,789
multiple stages multiple shuffles so so

00:05:23,370 --> 00:05:30,000
far I was just saying spurred us this

00:05:26,789 --> 00:05:34,500
and that but the actual execution you

00:05:30,000 --> 00:05:37,500
need in spark is tasks tasks consists of

00:05:34,500 --> 00:05:41,550
a code which is supposed to be run on

00:05:37,500 --> 00:05:45,060
your data and a piece of data so here

00:05:41,550 --> 00:05:47,520
you can see a block in HDFS so single

00:05:45,060 --> 00:05:49,889
block in HDFS and end code which is

00:05:47,520 --> 00:05:52,560
supposed to be run on it is a task and

00:05:49,889 --> 00:05:58,849
each stage consists of multiple

00:05:52,560 --> 00:06:03,900
independent tasks tasks get run in

00:05:58,849 --> 00:06:08,460
executors each executor can run one or

00:06:03,900 --> 00:06:11,940
more tasks at a time and the at a time

00:06:08,460 --> 00:06:15,810
part I want to emphasize on because that

00:06:11,940 --> 00:06:18,509
means spark can reuse the same executor

00:06:15,810 --> 00:06:21,060
to run task after task after task it can

00:06:18,509 --> 00:06:25,469
pick up dust from from the pending tasks

00:06:21,060 --> 00:06:29,729
view and it's a quite nice improvement

00:06:25,469 --> 00:06:32,370
comparing to to Hadoop so you don't have

00:06:29,729 --> 00:06:35,849
to spawn new JVM for each task you can

00:06:32,370 --> 00:06:39,949
reuse the same JVM and everything is

00:06:35,849 --> 00:06:42,949
orchestrated by a single single driver

00:06:39,949 --> 00:06:42,949
process

00:06:43,620 --> 00:06:50,740
so let's zoom in to shuffle you might

00:06:48,520 --> 00:06:53,290
ask why do I need to know what's going

00:06:50,740 --> 00:06:56,290
on under the hood why don't I just rely

00:06:53,290 --> 00:06:58,900
on the very nice spark API

00:06:56,290 --> 00:07:01,960
why don't I simply let spark do the job

00:06:58,900 --> 00:07:03,370
and the answer is you really need to

00:07:01,960 --> 00:07:07,570
understand what's going on under the

00:07:03,370 --> 00:07:13,920
hood in order to understand problems you

00:07:07,570 --> 00:07:17,860
might face and eventually solve them so

00:07:13,920 --> 00:07:20,800
there is a task 1 which is processing

00:07:17,860 --> 00:07:23,110
some partition it pipelines operation on

00:07:20,800 --> 00:07:27,690
that partition so it pipelines

00:07:23,110 --> 00:07:30,970
operations like map flatmap filter and

00:07:27,690 --> 00:07:35,170
eventually we get a result the result

00:07:30,970 --> 00:07:38,620
gets stored to a local disk and it gets

00:07:35,170 --> 00:07:42,360
stored into multiple buckets each bucket

00:07:38,620 --> 00:07:45,670
is responsible for certain group of keys

00:07:42,360 --> 00:07:50,650
lookup the other tasks they do exactly

00:07:45,670 --> 00:07:55,120
the same so at the end of stage 1 we

00:07:50,650 --> 00:08:00,280
have the result of tasks of tasks

00:07:55,120 --> 00:08:04,720
written in local disks so they can be

00:08:00,280 --> 00:08:06,370
pulled together all the red buckets are

00:08:04,720 --> 00:08:08,290
responsible for the same group of keys

00:08:06,370 --> 00:08:11,680
they can be pulled together and process

00:08:08,290 --> 00:08:14,490
and and stage number 2 can start

00:08:11,680 --> 00:08:18,370
processing it from there

00:08:14,490 --> 00:08:21,220
so as you can see there is quite a lot

00:08:18,370 --> 00:08:23,380
of i/o so you're writing data to this

00:08:21,220 --> 00:08:26,470
we're reading from it we are sending

00:08:23,380 --> 00:08:28,660
data over the network it's expensive but

00:08:26,470 --> 00:08:30,060
let's not focus on it for now because it

00:08:28,660 --> 00:08:33,460
might be that your application actually

00:08:30,060 --> 00:08:35,740
need multiple shuffles and you can

00:08:33,460 --> 00:08:36,370
cannot really avoid it you simply have

00:08:35,740 --> 00:08:42,250
to do it

00:08:36,370 --> 00:08:48,460
so what could possibly go wrong like

00:08:42,250 --> 00:08:50,860
famous last sentence so imagine you have

00:08:48,460 --> 00:08:52,450
your application ready you have tested

00:08:50,860 --> 00:08:55,420
in log it locally you are quite

00:08:52,450 --> 00:08:57,220
confident about your business logic and

00:08:55,420 --> 00:08:59,769
go to a cluster your

00:08:57,220 --> 00:09:01,689
in large data set and you end up with

00:08:59,769 --> 00:09:05,170
one of these problems spark is

00:09:01,689 --> 00:09:07,029
complaining about two gigabytes limit

00:09:05,170 --> 00:09:09,509
for for the bucket I was showing you

00:09:07,029 --> 00:09:12,009
spark is complaining about timeouts

00:09:09,509 --> 00:09:14,050
Sparky is complaining about some memory

00:09:12,009 --> 00:09:17,800
related problems or executor lost

00:09:14,050 --> 00:09:20,829
failure none of them is really related

00:09:17,800 --> 00:09:23,350
to your business logic but but you still

00:09:20,829 --> 00:09:27,220
can see such kind of problems and you

00:09:23,350 --> 00:09:30,490
still have to tackle them so why why do

00:09:27,220 --> 00:09:32,949
we have them it's quite often that you

00:09:30,490 --> 00:09:36,430
can see such group of problems when your

00:09:32,949 --> 00:09:39,550
executors are when your tasks are

00:09:36,430 --> 00:09:41,170
processing too much data so the tasks

00:09:39,550 --> 00:09:42,819
are simply talking with the data it's

00:09:41,170 --> 00:09:45,040
either somewhere inside the task

00:09:42,819 --> 00:09:48,519
somewhere with garbage collection or

00:09:45,040 --> 00:09:50,529
maybe the tasks are actually fine but

00:09:48,519 --> 00:09:56,680
there are problems when the data gets

00:09:50,529 --> 00:09:58,959
sent over the network so there are so

00:09:56,680 --> 00:10:03,970
where do you look for cooperate how do

00:09:58,959 --> 00:10:07,480
you actually investigate it you have a

00:10:03,970 --> 00:10:09,910
pretty nice spark UI for it and it gives

00:10:07,480 --> 00:10:11,980
you a lot of informations but the

00:10:09,910 --> 00:10:15,930
information I want to focus on right now

00:10:11,980 --> 00:10:18,579
is a stage overview which gives you a

00:10:15,930 --> 00:10:21,399
dug visualization which is kind of an

00:10:18,579 --> 00:10:28,829
execution plan but also it gives you

00:10:21,399 --> 00:10:37,230
metrics for each individual tasks so

00:10:28,829 --> 00:10:37,230
yeah all really works for me

00:10:37,529 --> 00:10:46,480
yes once again there are multiple tabs

00:10:41,710 --> 00:10:49,240
er you have you have an overview of what

00:10:46,480 --> 00:10:53,910
your execution plan looks like now I

00:10:49,240 --> 00:10:59,770
have to go back yeah and you get a

00:10:53,910 --> 00:11:01,900
overview of matrix third task so the

00:10:59,770 --> 00:11:04,630
ones you definitely want to focus on is

00:11:01,900 --> 00:11:07,390
status of your tasks so if you can see

00:11:04,630 --> 00:11:09,520
tasks failing you obviously need to go

00:11:07,390 --> 00:11:12,870
and dig into it and look for a reason

00:11:09,520 --> 00:11:15,640
they are failing but another matrix

00:11:12,870 --> 00:11:18,010
which are super important for you is the

00:11:15,640 --> 00:11:24,040
duration of your tasks garbage

00:11:18,010 --> 00:11:27,940
collection time which tasks spend on the

00:11:24,040 --> 00:11:34,480
garbage collection and last but not

00:11:27,940 --> 00:11:37,089
least amount of data sent do sent or or

00:11:34,480 --> 00:11:40,779
created created during the shuffle so

00:11:37,089 --> 00:11:43,750
you want this value values to be not too

00:11:40,779 --> 00:11:47,470
large and you usually want this value to

00:11:43,750 --> 00:11:50,170
be kind of equal so if all of the values

00:11:47,470 --> 00:11:53,760
are large or some of the some of some of

00:11:50,170 --> 00:11:56,770
your values are really large that means

00:11:53,760 --> 00:11:59,050
you you have a problem that that means

00:11:56,770 --> 00:12:03,250
you have you have something to

00:11:59,050 --> 00:12:05,709
investigate and if you can see if you

00:12:03,250 --> 00:12:11,950
suspect your task or processing too much

00:12:05,709 --> 00:12:14,800
data what you could do is you could

00:12:11,950 --> 00:12:18,600
control you could try to control the

00:12:14,800 --> 00:12:21,279
level of parallelism so if you pass a

00:12:18,600 --> 00:12:25,690
non partition parameter to any method

00:12:21,279 --> 00:12:30,670
which is triggering shuffle you

00:12:25,690 --> 00:12:32,830
basically tells Park oh I want the group

00:12:30,670 --> 00:12:36,610
by to produce this many buckets this

00:12:32,830 --> 00:12:39,279
many tasks so I want to process the

00:12:36,610 --> 00:12:43,600
result of group by key in that many

00:12:39,279 --> 00:12:46,540
tasks of course you could also try to

00:12:43,600 --> 00:12:48,950
give give your executor more memory and

00:12:46,540 --> 00:12:51,230
sometimes that works but but

00:12:48,950 --> 00:12:53,360
you are limited that doesn't scale scale

00:12:51,230 --> 00:12:56,090
very well you cannot do it every single

00:12:53,360 --> 00:12:59,330
time and one more thing you could do is

00:12:56,090 --> 00:13:02,750
you could trigger an artificial shuffle

00:12:59,330 --> 00:13:05,390
so you could trigger repartition

00:13:02,750 --> 00:13:07,460
to do exclusively tells park that you

00:13:05,390 --> 00:13:08,870
want to even though you don't need it

00:13:07,460 --> 00:13:10,310
from the point of your business or if

00:13:08,870 --> 00:13:16,130
you want to report even the data in

00:13:10,310 --> 00:13:18,530
order to make sparks life easier so look

00:13:16,130 --> 00:13:20,090
we have stage which is processing data

00:13:18,530 --> 00:13:24,710
and three tasks and those tasks are

00:13:20,090 --> 00:13:27,410
talking so what you do you pass larger

00:13:24,710 --> 00:13:30,830
number of partition as a parameter and

00:13:27,410 --> 00:13:32,540
you end up with more tasks but each of

00:13:30,830 --> 00:13:34,730
them is smaller each of them is

00:13:32,540 --> 00:13:39,020
processing less data each of them is

00:13:34,730 --> 00:13:44,170
producing smaller buckets so so the

00:13:39,020 --> 00:13:44,170
stated dance team has easier job as well

00:13:44,740 --> 00:13:53,830
but it's not always that simple

00:13:48,550 --> 00:13:56,780
sometimes you you tell spark to to use

00:13:53,830 --> 00:13:58,520
to process the data in a lot of tasks

00:13:56,780 --> 00:14:01,880
and most of the tasks are doing

00:13:58,520 --> 00:14:05,990
basically nothing but one of them or a

00:14:01,880 --> 00:14:07,160
few of them are or super heavy so this

00:14:05,990 --> 00:14:11,150
kind of problems are called

00:14:07,160 --> 00:14:13,370
caused by skew in your data and to give

00:14:11,150 --> 00:14:15,680
you an example if you are processing

00:14:13,370 --> 00:14:18,040
data per country some of your countries

00:14:15,680 --> 00:14:22,070
are just heavier some of your countries

00:14:18,040 --> 00:14:25,130
has more transactions more users and and

00:14:22,070 --> 00:14:27,830
if you group by by the country code you

00:14:25,130 --> 00:14:33,530
end up with with different sizes of your

00:14:27,830 --> 00:14:36,650
tasks another example is key being now

00:14:33,530 --> 00:14:38,480
so if you allow your keys to be now and

00:14:36,650 --> 00:14:40,880
if you group by it you might end up with

00:14:38,480 --> 00:14:43,220
like 50 percent of your data ending up

00:14:40,880 --> 00:14:46,370
in the same task and regardless of how

00:14:43,220 --> 00:14:51,710
many of them you have most of the job is

00:14:46,370 --> 00:14:54,560
job is done in in one in one task so

00:14:51,710 --> 00:14:59,260
very general technique to to deal with

00:14:54,560 --> 00:15:02,060
such kind of problem is introducing salt

00:14:59,260 --> 00:15:02,720
to your keys introducing some randomness

00:15:02,060 --> 00:15:08,360
to your key

00:15:02,720 --> 00:15:10,430
so look we have a few keys through and

00:15:08,360 --> 00:15:12,680
let's say we have too many too many of

00:15:10,430 --> 00:15:14,569
them and we want to split them split

00:15:12,680 --> 00:15:16,810
them in between multiple tasks we don't

00:15:14,569 --> 00:15:20,600
want them all to be processed in one

00:15:16,810 --> 00:15:23,120
task so we add some random random values

00:15:20,600 --> 00:15:25,790
to them so then they can be processed in

00:15:23,120 --> 00:15:29,899
separation and then you are responsible

00:15:25,790 --> 00:15:33,259
for making sure you you merge the result

00:15:29,899 --> 00:15:36,860
back together you clear the randomness

00:15:33,259 --> 00:15:44,410
you introduced and making sure that your

00:15:36,860 --> 00:15:47,629
business logic stays correct another

00:15:44,410 --> 00:15:50,990
subject which is very important when

00:15:47,629 --> 00:15:54,500
when you want to actually benefit from

00:15:50,990 --> 00:15:57,259
using spark is caching so I mentioned

00:15:54,500 --> 00:16:01,519
RDD so far but I have not mentioned that

00:15:57,259 --> 00:16:05,120
rdd's are lately evaluated so spark

00:16:01,519 --> 00:16:08,449
tries to pipeline pipeline as many as

00:16:05,120 --> 00:16:10,490
many operations as possible it runs them

00:16:08,449 --> 00:16:14,689
when when the data actually has to be

00:16:10,490 --> 00:16:18,139
materialized so it does not run them on

00:16:14,689 --> 00:16:20,120
the fly does not run them it does not

00:16:18,139 --> 00:16:24,410
store any intermediate result to disk as

00:16:20,120 --> 00:16:26,779
well so we have the blue RDD one which

00:16:24,410 --> 00:16:31,189
is calculated out of multiple operations

00:16:26,779 --> 00:16:35,389
and then we reuse them we reuse already

00:16:31,189 --> 00:16:37,310
one so we call some some more operations

00:16:35,389 --> 00:16:39,860
on RDD one and we store the result to

00:16:37,310 --> 00:16:41,600
disk and let's say we do exactly the

00:16:39,860 --> 00:16:45,259
same to our DD one or maybe some other

00:16:41,600 --> 00:16:48,649
operation but but but we we we again

00:16:45,259 --> 00:16:51,170
call more operations or our DD one we

00:16:48,649 --> 00:16:55,639
store the result to disk and it works

00:16:51,170 --> 00:16:58,750
fine with one caveat the blue part gets

00:16:55,639 --> 00:17:01,879
executed twice and you might be

00:16:58,750 --> 00:17:05,799
disappointed you might ask why but this

00:17:01,879 --> 00:17:10,010
is simply what spark does it avoids

00:17:05,799 --> 00:17:11,360
storing intermediate data to disk and if

00:17:10,010 --> 00:17:14,179
you don't have the result of calculation

00:17:11,360 --> 00:17:15,540
of our DD one it has to recalculate

00:17:14,179 --> 00:17:17,880
everything

00:17:15,540 --> 00:17:21,120
but spark gives you a mechanism to

00:17:17,880 --> 00:17:23,820
actually control it so spark gives you

00:17:21,120 --> 00:17:26,940
caching mechanism so whenever you see a

00:17:23,820 --> 00:17:28,650
situation like that when you have a

00:17:26,940 --> 00:17:32,010
branch in your execution plan when you

00:17:28,650 --> 00:17:35,730
really use some LEDs you can catch the

00:17:32,010 --> 00:17:38,820
result and you can cache it to memory

00:17:35,730 --> 00:17:41,940
you can cache it to disk you can cast it

00:17:38,820 --> 00:17:44,790
to HDFS you can control the replication

00:17:41,940 --> 00:17:49,530
factor so into how many notes given

00:17:44,790 --> 00:17:52,740
partition goes and it's up to you what

00:17:49,530 --> 00:17:54,990
decision you make so first of all as you

00:17:52,740 --> 00:17:56,970
probably already noticed your your

00:17:54,990 --> 00:17:59,400
memory will be will be limited so you

00:17:56,970 --> 00:18:03,660
cannot cash everything and the question

00:17:59,400 --> 00:18:06,300
is what do I cash so in order to decide

00:18:03,660 --> 00:18:09,650
what to cash you have to know that you

00:18:06,300 --> 00:18:13,080
cannot pin an RDD to memory you cannot

00:18:09,650 --> 00:18:16,560
prioritize rdd's it's just all are you

00:18:13,080 --> 00:18:18,900
algorithm so caching one thing might

00:18:16,560 --> 00:18:21,660
mean you losing another thing so you

00:18:18,900 --> 00:18:24,780
have to make sure that what you're

00:18:21,660 --> 00:18:26,940
caching is the important one is the

00:18:24,780 --> 00:18:29,220
heavy to calculate one the one thing

00:18:26,940 --> 00:18:32,460
which you actually want to keep and you

00:18:29,220 --> 00:18:37,290
have to make sure that we are not to

00:18:32,460 --> 00:18:40,590
Brede so so you are not losing some some

00:18:37,290 --> 00:18:43,890
some not that heavy to alkalize later DD

00:18:40,590 --> 00:18:45,360
so sorry you are not using the heavy to

00:18:43,890 --> 00:18:48,720
calculator really because you causing

00:18:45,360 --> 00:18:51,870
something not that important if you are

00:18:48,720 --> 00:18:53,820
not sure what to expect what sizes your

00:18:51,870 --> 00:18:58,980
arteries are you can always go to spark

00:18:53,820 --> 00:19:06,510
UI and it tells you how much how much

00:18:58,980 --> 00:19:09,270
memory LEDs are taking in in cash one

00:19:06,510 --> 00:19:11,850
more important thing is that you don't

00:19:09,270 --> 00:19:14,340
necessarily have to fit all the already

00:19:11,850 --> 00:19:17,160
in memory you can cash this just part of

00:19:14,340 --> 00:19:20,100
it I mean spark will do it for you the

00:19:17,160 --> 00:19:22,950
only requirement is that the whole

00:19:20,100 --> 00:19:24,990
partition of an RDD has to fit in memory

00:19:22,950 --> 00:19:26,360
so spark either causes the whole

00:19:24,990 --> 00:19:31,340
partition

00:19:26,360 --> 00:19:35,680
or just nothing okay so we have a

00:19:31,340 --> 00:19:40,480
strategy for for caching data in memory

00:19:35,680 --> 00:19:44,540
but why don't - why don't we just cache

00:19:40,480 --> 00:19:49,100
everything we reuse to disk and it might

00:19:44,540 --> 00:19:52,040
be a bit counterintuitive but storing

00:19:49,100 --> 00:19:54,740
data to disk sometimes can be just more

00:19:52,040 --> 00:19:57,380
expensive than than just recalculating

00:19:54,740 --> 00:20:00,860
the data so it sounds like a waste but

00:19:57,380 --> 00:20:04,370
still sometimes for calculation of of

00:20:00,860 --> 00:20:05,900
your RDD is completely fine and it it's

00:20:04,370 --> 00:20:08,090
especially true when when when you are

00:20:05,900 --> 00:20:12,710
considering customer application factor

00:20:08,090 --> 00:20:17,390
or caching with caching in HDFS which

00:20:12,710 --> 00:20:19,640
will be much more much more expensive

00:20:17,390 --> 00:20:23,960
because because then then you have to

00:20:19,640 --> 00:20:26,660
deal with with network and so on last

00:20:23,960 --> 00:20:28,940
thing to remember is that the buckets I

00:20:26,660 --> 00:20:31,640
was showing you in shuffle they get

00:20:28,940 --> 00:20:33,260
stored and they are kept by spark so

00:20:31,640 --> 00:20:35,060
they can be reused so when you

00:20:33,260 --> 00:20:37,760
recalculate when spark recalculate your

00:20:35,060 --> 00:20:40,190
data it doesn't we calculate it from

00:20:37,760 --> 00:20:46,510
from the very beginning it we calculated

00:20:40,190 --> 00:20:46,510
it we calculated from the last shuffle

00:20:47,530 --> 00:20:56,960
all right let's have a look at what size

00:20:54,950 --> 00:21:00,050
your executor should be because that's

00:20:56,960 --> 00:21:02,780
that's a very common common problem so

00:21:00,050 --> 00:21:07,520
you control how many CPUs you are giving

00:21:02,780 --> 00:21:10,640
to your executors and that means that

00:21:07,520 --> 00:21:13,190
means number of tasks run run in

00:21:10,640 --> 00:21:15,950
parallel per an executor and you also

00:21:13,190 --> 00:21:17,960
control amount of memory and you have a

00:21:15,950 --> 00:21:20,690
choice of running very small ones which

00:21:17,960 --> 00:21:22,670
are and many of them per node and you

00:21:20,690 --> 00:21:25,550
can have very large one which is

00:21:22,670 --> 00:21:32,030
occupying most of the node resources you

00:21:25,550 --> 00:21:35,240
can all also run anything in between the

00:21:32,030 --> 00:21:38,090
actual decision is very much dependent

00:21:35,240 --> 00:21:39,530
on your workload but there are a couple

00:21:38,090 --> 00:21:42,980
of bullet points

00:21:39,530 --> 00:21:45,350
I want to I want to work on so first of

00:21:42,980 --> 00:21:50,750
all parks can benefit from running

00:21:45,350 --> 00:21:53,300
multiple multiple tasks in the same JVM

00:21:50,750 --> 00:21:56,300
so it can serve some variables across

00:21:53,300 --> 00:22:00,200
tasks so the review just one copy of

00:21:56,300 --> 00:22:04,610
this variable instead of a a copy per

00:22:00,200 --> 00:22:07,070
task also when you when you are heavily

00:22:04,610 --> 00:22:10,520
cashing things it's just easier for

00:22:07,070 --> 00:22:12,860
SPARC to feed the cached data in memory

00:22:10,520 --> 00:22:17,420
if you have like one big chunk of memory

00:22:12,860 --> 00:22:21,740
instead of instead of smaller ones on

00:22:17,420 --> 00:22:24,230
the other hand when you run when you run

00:22:21,740 --> 00:22:26,720
very large executors it's very likely

00:22:24,230 --> 00:22:31,820
that we will get into garbage collection

00:22:26,720 --> 00:22:34,610
problems and also one more notion about

00:22:31,820 --> 00:22:37,550
the large ones if you if your job is not

00:22:34,610 --> 00:22:40,340
that large and you and you still want to

00:22:37,550 --> 00:22:43,130
run large executors so let's say you

00:22:40,340 --> 00:22:45,620
have hundreds of hundreds of nodes in

00:22:43,130 --> 00:22:48,410
your cluster and you run just tens of

00:22:45,620 --> 00:22:52,550
executors but large ones that means you

00:22:48,410 --> 00:22:55,430
don't utilize you don't utilize all the

00:22:52,550 --> 00:22:58,220
resources you don't you also might have

00:22:55,430 --> 00:23:03,020
problems with locality which I'll I'll

00:22:58,220 --> 00:23:07,340
get to so the general hints are if if

00:23:03,020 --> 00:23:09,560
your workload is like ETL I would say in

00:23:07,340 --> 00:23:12,620
most cases there is no point in in

00:23:09,560 --> 00:23:15,890
playing with very large executors and

00:23:12,620 --> 00:23:18,560
and and risking garbage collection

00:23:15,890 --> 00:23:22,310
problem I will start with small ones on

00:23:18,560 --> 00:23:24,350
the other hand if you if you very much

00:23:22,310 --> 00:23:27,500
rely on caching or if you are using

00:23:24,350 --> 00:23:30,020
broadcast variables you might need

00:23:27,500 --> 00:23:33,410
larger ones so you will have to play

00:23:30,020 --> 00:23:38,420
large executors and and make sure you

00:23:33,410 --> 00:23:43,160
not end up with problems before we go

00:23:38,420 --> 00:23:45,920
further let's have a quick look at spark

00:23:43,160 --> 00:23:47,930
memory model so you decide how much JVM

00:23:45,920 --> 00:23:51,740
Menor you give you give to an executor

00:23:47,930 --> 00:23:52,860
this area this space is split into three

00:23:51,740 --> 00:23:57,170
areas

00:23:52,860 --> 00:23:59,850
area for your user program area for

00:23:57,170 --> 00:24:03,660
intermediate buffer buffers used during

00:23:59,850 --> 00:24:07,020
shuffle and area for cashing up until

00:24:03,660 --> 00:24:11,520
SPARC 1.6 it could you had to actually

00:24:07,020 --> 00:24:13,620
control that from 1.6 park tries to

00:24:11,520 --> 00:24:16,320
balance it but you still can can switch

00:24:13,620 --> 00:24:19,770
back and and and decide that you are

00:24:16,320 --> 00:24:23,370
smarter than spark and and assign some

00:24:19,770 --> 00:24:26,130
buddies to it another very important

00:24:23,370 --> 00:24:28,860
memory which is outside of the heap is

00:24:26,130 --> 00:24:34,560
memory overhead this is a memory needed

00:24:28,860 --> 00:24:37,710
by a container but also this area is

00:24:34,560 --> 00:24:39,540
where all the off hit memory goes so if

00:24:37,710 --> 00:24:41,580
you know that you are allocating off

00:24:39,540 --> 00:24:43,860
heap memory or if you are using the

00:24:41,580 --> 00:24:48,750
library which does that you have to make

00:24:43,860 --> 00:24:49,650
sure you make it large enough so keep

00:24:48,750 --> 00:24:52,590
that in mind

00:24:49,650 --> 00:24:55,110
also keep marquee keep in mind the

00:24:52,590 --> 00:24:57,150
operating system so don't be too greedy

00:24:55,110 --> 00:24:59,550
leave some resources for the operating

00:24:57,150 --> 00:25:01,980
system if you are using on the yarn ORS

00:24:59,550 --> 00:25:04,140
or system like that probably your admin

00:25:01,980 --> 00:25:07,970
took care of that but otherwise make

00:25:04,140 --> 00:25:07,970
sure you you don't allocate everything

00:25:08,450 --> 00:25:15,660
if you are not sure about the memory

00:25:12,180 --> 00:25:18,210
consumption you can always just play

00:25:15,660 --> 00:25:21,180
around cousin already check how much it

00:25:18,210 --> 00:25:25,740
takes so you get an overview of what

00:25:21,180 --> 00:25:28,650
you're dealing with one more thing which

00:25:25,740 --> 00:25:31,320
is worth trying if you are not sure how

00:25:28,650 --> 00:25:33,390
many executors you want how how much

00:25:31,320 --> 00:25:36,300
resources you want to give so let's say

00:25:33,390 --> 00:25:38,790
you have a job with the same job which

00:25:36,300 --> 00:25:40,860
is sat which sometimes is taking small

00:25:38,790 --> 00:25:43,560
inputs sometimes instead of taking very

00:25:40,860 --> 00:25:46,170
large input you can play with dynamic

00:25:43,560 --> 00:25:48,750
resource allocation in that situation so

00:25:46,170 --> 00:25:50,940
spark will start with small number of

00:25:48,750 --> 00:26:00,000
executors and it will just bump it up

00:25:50,940 --> 00:26:02,010
while it sees pending tasks locality

00:26:00,000 --> 00:26:05,160
I've mentioned that already so the

00:26:02,010 --> 00:26:07,270
concept is borrowed from Hadoop and the

00:26:05,160 --> 00:26:11,330
concept is super simple

00:26:07,270 --> 00:26:16,340
note 1 contains block 1 and you want to

00:26:11,330 --> 00:26:19,550
process that block so it's easier to to

00:26:16,340 --> 00:26:22,809
move the execution to the node 1 rather

00:26:19,550 --> 00:26:26,530
than moving the data somewhere else and

00:26:22,809 --> 00:26:30,950
SPARC does does it for you automatically

00:26:26,530 --> 00:26:33,080
SPARC tries to achieve us highest

00:26:30,950 --> 00:26:36,320
locality as high local locality as

00:26:33,080 --> 00:26:39,200
possible but in some situations it's not

00:26:36,320 --> 00:26:42,080
possible but you can you can help spark

00:26:39,200 --> 00:26:45,140
to achieve it so let's say you have 100

00:26:42,080 --> 00:26:47,720
nodes and you have just 10 executors and

00:26:45,140 --> 00:26:51,050
you have and you have IDF size blocks on

00:26:47,720 --> 00:26:53,050
all 100 of the nodes so it's not

00:26:51,050 --> 00:26:57,200
possible to run everything locally

00:26:53,050 --> 00:27:01,580
because simply have just execution on 10

00:26:57,200 --> 00:27:04,370
nodes so the rest 90 nodes have to have

00:27:01,580 --> 00:27:07,220
to transfer the data what you could do

00:27:04,370 --> 00:27:11,450
in order to help spark is you could

00:27:07,220 --> 00:27:14,660
increase number of exhibitors so you

00:27:11,450 --> 00:27:17,090
make it more likely that spark has an

00:27:14,660 --> 00:27:22,660
executor which is waiting and ready to

00:27:17,090 --> 00:27:22,660
pick up a task in certain locality level

00:27:23,140 --> 00:27:28,490
if your job is not that large on the

00:27:26,030 --> 00:27:31,990
other hand maybe it's just better to

00:27:28,490 --> 00:27:34,850
leave it as is if you really want to

00:27:31,990 --> 00:27:37,460
control the locality because play with

00:27:34,850 --> 00:27:40,760
spark locality white parameter so that's

00:27:37,460 --> 00:27:44,750
a dub-dub param tall spark for how long

00:27:40,760 --> 00:27:47,330
it should wait for for running a job on

00:27:44,750 --> 00:27:50,990
given locality level until it just gives

00:27:47,330 --> 00:27:54,500
up and runs it somewhere else so by

00:27:50,990 --> 00:27:57,050
using this param you can enforce

00:27:54,500 --> 00:28:00,380
locality you can give it up entirely but

00:27:57,050 --> 00:28:04,309
it's super super tricky and the locality

00:28:00,380 --> 00:28:07,010
level also we can check in in spark UI

00:28:04,309 --> 00:28:10,040
and it's a property per task so you can

00:28:07,010 --> 00:28:14,080
see what locality level each task run on

00:28:10,040 --> 00:28:18,010
and you usually want to see no local not

00:28:14,080 --> 00:28:18,010
not something different

00:28:20,010 --> 00:28:26,770
and let's have a similar exercise we've

00:28:25,720 --> 00:28:32,080
done with

00:28:26,770 --> 00:28:34,899
shuffle let's do it with join so the

00:28:32,080 --> 00:28:39,039
regular shuffle join works very similar

00:28:34,899 --> 00:28:42,220
to what happened in in group by so in

00:28:39,039 --> 00:28:44,950
both stages spark has to split the

00:28:42,220 --> 00:28:47,860
result of each task in two buckets and

00:28:44,950 --> 00:28:49,480
then all the same like buckets

00:28:47,860 --> 00:28:52,330
responsible for the same keys are going

00:28:49,480 --> 00:28:59,549
together and they get joined in stage

00:28:52,330 --> 00:29:01,929
three and as I already mentioned

00:28:59,549 --> 00:29:05,230
that's an expensive one sometimes you

00:29:01,929 --> 00:29:10,149
cannot you cannot avoid it but let's see

00:29:05,230 --> 00:29:15,640
how we could improve it and improving

00:29:10,149 --> 00:29:19,350
shuffle very often means avoiding it so

00:29:15,640 --> 00:29:23,740
look at this execution plan Stage one is

00:29:19,350 --> 00:29:27,789
doing group by key followed by map Stage

00:29:23,740 --> 00:29:30,610
three is doing exactly the same and then

00:29:27,789 --> 00:29:36,429
Stage four is joining the results of

00:29:30,610 --> 00:29:38,559
them two together but you can do some

00:29:36,429 --> 00:29:42,100
tricks you can be nicer to spark in

00:29:38,559 --> 00:29:45,580
certain situations so if you know that

00:29:42,100 --> 00:29:48,159
the map after group by is not modifying

00:29:45,580 --> 00:29:51,700
the keys you can tell it to spark by

00:29:48,159 --> 00:29:54,880
using map values instead of map so if

00:29:51,700 --> 00:29:59,350
you're if you are modifying just values

00:29:54,880 --> 00:30:02,110
in your key value Spurs the other the

00:29:59,350 --> 00:30:04,029
other thing you could do is to make sure

00:30:02,110 --> 00:30:06,520
you are using the same number of

00:30:04,029 --> 00:30:11,080
partitions in involve group bys

00:30:06,520 --> 00:30:13,090
so then spark knows that we have exactly

00:30:11,080 --> 00:30:15,399
the same partitioning scheme in both

00:30:13,090 --> 00:30:19,059
group bys and we have not modified the

00:30:15,399 --> 00:30:21,580
keys so the the result partitioning

00:30:19,059 --> 00:30:26,139
scheme is exactly the same so it can use

00:30:21,580 --> 00:30:27,730
this fact and end up with such an

00:30:26,139 --> 00:30:31,870
execution plan

00:30:27,730 --> 00:30:34,690
so we used to have five stages and four

00:30:31,870 --> 00:30:36,850
shuttles between them now we have two

00:30:34,690 --> 00:30:40,750
shuttles less which is usually quite a

00:30:36,850 --> 00:30:43,779
big win and what happens here is the

00:30:40,750 --> 00:30:47,889
group by key then followed by map values

00:30:43,779 --> 00:30:52,000
and then join happens all in the same

00:30:47,889 --> 00:30:58,830
place there is no data needed to be sent

00:30:52,000 --> 00:31:01,690
over the network another technique to

00:30:58,830 --> 00:31:04,029
improve shuffle and again improving

00:31:01,690 --> 00:31:07,120
shuffle means avoiding it is broadcast

00:31:04,029 --> 00:31:09,190
variable broadcast variable is a

00:31:07,120 --> 00:31:12,419
variable which you can send from a

00:31:09,190 --> 00:31:16,360
driver to every single to every single

00:31:12,419 --> 00:31:19,299
executor it has to fit in memory so

00:31:16,360 --> 00:31:21,429
imagine you have to join our dd1 which

00:31:19,299 --> 00:31:24,039
is very very large in consists of many

00:31:21,429 --> 00:31:27,909
partitions and you have our DD 2 which

00:31:24,039 --> 00:31:30,340
is tiny so instead of shuffling already

00:31:27,909 --> 00:31:32,799
one traffic already - and then joining

00:31:30,340 --> 00:31:36,399
them together what you could do you

00:31:32,799 --> 00:31:40,990
could broadcast already - so it resists

00:31:36,399 --> 00:31:45,100
in each executor and then join our DD -

00:31:40,990 --> 00:31:48,549
to just single partition of our DD 1 in

00:31:45,100 --> 00:31:52,000
memory and then you end up with with the

00:31:48,549 --> 00:31:56,049
join result without shuffling articles

00:31:52,000 --> 00:31:59,019
ever large our DD 1 so just a quick

00:31:56,049 --> 00:32:02,380
recap if you control number of

00:31:59,019 --> 00:32:04,539
partitions and and if you are using map

00:32:02,380 --> 00:32:07,179
values if you know that you you don't

00:32:04,539 --> 00:32:10,510
modify your keys that might be super

00:32:07,179 --> 00:32:12,730
helpful to start to avoid shuffles if

00:32:10,510 --> 00:32:15,159
you are joining small rdd's with very

00:32:12,730 --> 00:32:17,710
large one consider using broadcast

00:32:15,159 --> 00:32:21,010
variable consider broadcasting small

00:32:17,710 --> 00:32:25,840
rdd's filter before the shuffle not

00:32:21,010 --> 00:32:27,940
after it that's quite obvious use reduce

00:32:25,840 --> 00:32:32,200
by key where you can so I was referring

00:32:27,940 --> 00:32:36,580
to group by key all the time but but

00:32:32,200 --> 00:32:39,740
reduced like you can can actually limit

00:32:36,580 --> 00:32:42,620
amount of data sent over the network

00:32:39,740 --> 00:32:48,440
hums somehow related to toward combiner

00:32:42,620 --> 00:32:51,620
it in Hadoop does test your code I

00:32:48,440 --> 00:32:53,720
cannot stress it more I mean it's not

00:32:51,620 --> 00:32:56,210
only about your qualities also about how

00:32:53,720 --> 00:32:57,890
fast you can iterate if you if you

00:32:56,210 --> 00:33:00,590
testing you're testing your code locally

00:32:57,890 --> 00:33:04,040
you avoid waiting for resources going

00:33:00,590 --> 00:33:06,590
for a class or waiting until it it

00:33:04,040 --> 00:33:08,690
finishes the job and then checking the

00:33:06,590 --> 00:33:10,730
the very large data set test as much as

00:33:08,690 --> 00:33:15,320
you can locally and SPARC actually

00:33:10,730 --> 00:33:18,610
supports that pretty well make sure you

00:33:15,320 --> 00:33:21,710
know what you are optimized for do you

00:33:18,610 --> 00:33:24,650
do you want to see good work time do you

00:33:21,710 --> 00:33:26,870
want to see good resource resource users

00:33:24,650 --> 00:33:28,970
or maybe your time is that creatures

00:33:26,870 --> 00:33:33,740
that you don't want to rub it hold on on

00:33:28,970 --> 00:33:35,660
some small improvements also make sure

00:33:33,740 --> 00:33:37,910
you know the priorities of your jobs so

00:33:35,660 --> 00:33:40,760
so make sure that you are not hogging

00:33:37,910 --> 00:33:42,770
the the whole cluster and and other

00:33:40,760 --> 00:33:48,770
people maybe more important jobs are

00:33:42,770 --> 00:33:52,250
waiting for you make sure you have some

00:33:48,770 --> 00:33:54,470
guidelines because it's very likely that

00:33:52,250 --> 00:33:57,080
in your team this is you as you

00:33:54,470 --> 00:33:58,809
developers and maybe a few analytics and

00:33:57,080 --> 00:34:00,980
maybe a few people who wants to run

00:33:58,809 --> 00:34:04,820
sequel from time to time and they are

00:34:00,980 --> 00:34:07,490
not that much into or going on under the

00:34:04,820 --> 00:34:09,560
hood but make sure you you sure the best

00:34:07,490 --> 00:34:12,700
practices so you don't dig into the same

00:34:09,560 --> 00:34:17,330
problems all over again and the last

00:34:12,700 --> 00:34:20,510
comment is somehow related to the title

00:34:17,330 --> 00:34:23,179
of this talk so SPARC actually works but

00:34:20,510 --> 00:34:24,770
it really helps to know what's going on

00:34:23,179 --> 00:34:27,200
under the hood and the more you

00:34:24,770 --> 00:34:32,300
understand the more you work with it the

00:34:27,200 --> 00:34:36,290
more you like it okay we I think we have

00:34:32,300 --> 00:34:39,580
a few minutes for questions right let's

00:34:36,290 --> 00:34:39,580
take the speaker first for the talk

00:34:39,850 --> 00:34:45,650
Martin and here all the questions in the

00:34:45,020 --> 00:34:49,910
audience

00:34:45,650 --> 00:34:52,970
is one in the phone yeah I yet this one

00:34:49,910 --> 00:34:55,900
sideway said that if you put on too many

00:34:52,970 --> 00:34:59,780
executors that you get into GC problems

00:34:55,900 --> 00:35:01,730
now it was if the executor is very large

00:34:59,780 --> 00:35:05,480
so if you let's say you you want to

00:35:01,730 --> 00:35:08,630
process a lot of tasks in context of the

00:35:05,480 --> 00:35:12,410
same machine so then you need to give it

00:35:08,630 --> 00:35:13,550
a lot of memory and then for large for

00:35:12,410 --> 00:35:16,670
large hits

00:35:13,550 --> 00:35:19,310
it doesn't really behave very well so I

00:35:16,670 --> 00:35:21,620
got it wrong because you can have

00:35:19,310 --> 00:35:23,990
actually big boxes with lots of course

00:35:21,620 --> 00:35:25,940
and you will benefit from them right yes

00:35:23,990 --> 00:35:27,950
yeah in certain situations yes but but

00:35:25,940 --> 00:35:30,020
for instance if you are running ETL you

00:35:27,950 --> 00:35:33,920
probably not that heavily relying on

00:35:30,020 --> 00:35:35,780
caching so I would say it doesn't make

00:35:33,920 --> 00:35:37,700
much difference if you small run once

00:35:35,780 --> 00:35:40,030
and then you know and you step back from

00:35:37,700 --> 00:35:46,550
all these garbage collection problems

00:35:40,030 --> 00:35:51,890
thanks more questions this one here on

00:35:46,550 --> 00:35:54,290
the writer right side for you yeah thank

00:35:51,890 --> 00:35:57,800
you um do you have any advice about

00:35:54,290 --> 00:36:01,100
running spark on yarn anything any

00:35:57,800 --> 00:36:04,850
pitfalls to to look at all anything like

00:36:01,100 --> 00:36:06,410
that any really a bit more specific I'm

00:36:04,850 --> 00:36:08,210
basically just someone who's going to

00:36:06,410 --> 00:36:09,830
quite need to spark and I just wondered

00:36:08,210 --> 00:36:11,720
if there's anything I should be worrying

00:36:09,830 --> 00:36:14,690
about if I was running it on top of yarn

00:36:11,720 --> 00:36:16,880
versus running it now if it was like

00:36:14,690 --> 00:36:19,760
three years ago I would say you should

00:36:16,880 --> 00:36:25,190
have been very worried but right now it

00:36:19,760 --> 00:36:28,000
works quite well thank you we got time

00:36:25,190 --> 00:36:28,000
for one more question

00:36:28,360 --> 00:36:35,150
what is the suggestion about runtime of

00:36:32,000 --> 00:36:37,520
each task should because we saw there

00:36:35,150 --> 00:36:41,720
only 25 milliseconds yeah that was a

00:36:37,520 --> 00:36:44,630
freak example so I would say again it's

00:36:41,720 --> 00:36:46,460
relate its each depending on our

00:36:44,630 --> 00:36:50,420
workload but I would say a few minutes

00:36:46,460 --> 00:36:52,760
probably probably a few minutes because

00:36:50,420 --> 00:36:55,790
usually at least if you are running data

00:36:52,760 --> 00:36:58,790
from a GDS su you are not each task is

00:36:55,790 --> 00:37:01,550
in the beginning reading readings like

00:36:58,790 --> 00:37:04,660
few hundred Meg's so so it doesn't mean

00:37:01,550 --> 00:37:07,820
if it runs for four for more time it's

00:37:04,660 --> 00:37:12,290
it's starting to started starting to be

00:37:07,820 --> 00:37:15,740
worrying but depend depending on your on

00:37:12,290 --> 00:37:17,480
your workload once you know you once you

00:37:15,740 --> 00:37:19,580
work with it with your specific

00:37:17,480 --> 00:37:21,050
application we get you get the feeling

00:37:19,580 --> 00:37:25,310
in general I would say few minutes up to

00:37:21,050 --> 00:37:32,690
ten minutes six all right let's think to

00:37:25,310 --> 00:37:34,880
speak again it was okay I just I just I

00:37:32,690 --> 00:37:36,800
just want to thank you very much if you

00:37:34,880 --> 00:37:39,950
have more questions that switch out to

00:37:36,800 --> 00:37:41,390
me I will be around if you have more

00:37:39,950 --> 00:37:43,220
questions about the presentation if you

00:37:41,390 --> 00:37:45,920
have more questions about your use case

00:37:43,220 --> 00:37:48,020
and also if you are interested in in

00:37:45,920 --> 00:37:50,800
consulting services or or training

00:37:48,020 --> 00:37:51,270
services reach out to me thank you

00:37:50,800 --> 00:37:54,360
[Applause]

00:37:51,270 --> 00:37:54,360

YouTube URL: https://www.youtube.com/watch?v=19OSxob6ntk


