Title: Berlin Buzzwords 2017: Fabian Hueske - Stream Analytics with SQL on Apache Flink #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	SQL is undoubtedly the most widely used language for data analytics. It is declarative, many database systems and query processors feature advanced query optimizers and highly efficient execution engines, and last but not least it is the standard that everybody knows and uses. 

With stream processing technology becoming mainstream a question arises: “Why isn’t SQL widely supported by open source stream processors?”. One answer is that SQL’s semantics and syntax have not been designed with the characteristics of streaming data in mind. Consequently, systems that want to provide support for SQL on data streams have to overcome a conceptual gap.

Apache Flink is a distributed stream processing system. Due to its support for event-time processing, exactly-once state semantics, and its high throughput capabilities, Flink is very well suited for streaming analytics. Since about a year, the Flink community is working on two relational APIs for unified stream and batch processing, the Table API and SQL. The Table API is a language-integrated relational API and the SQL interface is compliant with standard SQL. 

Both APIs are semantically compatible and share the same optimization and execution path based on Apache Calcite. A core principle of both APIs is to provide the same semantics for batch and streaming data sources, meaning that a query should compute the same result regardless whether it was executed on a static data set, such as a file, or on a data stream, like a Kafka topic.

Read more:
https://2017.berlinbuzzwords.de/17/session/stream-analytics-sql-apache-flink

About Fabian Hueske:
https://2017.berlinbuzzwords.de/users/fabian-hueske

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,440 --> 00:00:12,039
yeah thanks for the introduction

00:00:08,530 --> 00:00:15,460
welcome everybody yeah I'm going to talk

00:00:12,039 --> 00:00:17,259
about stream processing stream

00:00:15,460 --> 00:00:21,160
processing with a sequel on a page we've

00:00:17,259 --> 00:00:22,890
linked a Patrick link I guess you

00:00:21,160 --> 00:00:25,420
already heard about it a little bit it's

00:00:22,890 --> 00:00:28,960
basically a platform for distributed

00:00:25,420 --> 00:00:31,269
scheduled stream processing so the main

00:00:28,960 --> 00:00:33,100
characteristic is our it's faster you

00:00:31,269 --> 00:00:35,140
got low latency and high throughput at

00:00:33,100 --> 00:00:36,519
the same time so formally you have

00:00:35,140 --> 00:00:39,190
basically G choose whether to go with

00:00:36,519 --> 00:00:42,219
low latency or high throughput with a

00:00:39,190 --> 00:00:46,000
flink basically choose both of that it's

00:00:42,219 --> 00:00:47,260
accurate in the sense that it offers

00:00:46,000 --> 00:00:49,420
event time processing

00:00:47,260 --> 00:00:51,789
so you're processing data based on

00:00:49,420 --> 00:00:53,800
timestamps in your data and not based on

00:00:51,789 --> 00:00:57,850
the time when data arrives on your

00:00:53,800 --> 00:01:00,850
machines and it also provides exactly

00:00:57,850 --> 00:01:02,050
once guarantees for the stage that

00:01:00,850 --> 00:01:04,660
you're holding in your in your

00:01:02,050 --> 00:01:07,869
operations it's reliable so you can have

00:01:04,660 --> 00:01:10,240
a highly highly available or setup of a

00:01:07,869 --> 00:01:12,520
fling cluster and with you with the

00:01:10,240 --> 00:01:14,200
feature of snapshots you can basically

00:01:12,520 --> 00:01:17,770
take a snapshot of your application

00:01:14,200 --> 00:01:19,599
consistent snapshot and then arm start

00:01:17,770 --> 00:01:23,020
another application or the same

00:01:19,599 --> 00:01:25,869
application from that snapshot at any

00:01:23,020 --> 00:01:29,440
point in time later so these are like

00:01:25,869 --> 00:01:32,259
the key features of length fling is used

00:01:29,440 --> 00:01:33,190
in quite a few or by quite a few

00:01:32,259 --> 00:01:36,340
companies across very different

00:01:33,190 --> 00:01:39,069
industries these days so it's used by

00:01:36,340 --> 00:01:43,929
online retailers it's used by telcos in

00:01:39,069 --> 00:01:48,160
finance social media and also arm mobile

00:01:43,929 --> 00:01:49,929
gaming and many of these users run flink

00:01:48,160 --> 00:01:53,800
on very large scale they're processing

00:01:49,929 --> 00:01:57,810
billions of events per day and have

00:01:53,800 --> 00:02:01,030
state that grows to tear up exercise so

00:01:57,810 --> 00:02:07,929
from for many of these users this is

00:02:01,030 --> 00:02:10,989
like a very good set up the data stream

00:02:07,929 --> 00:02:12,970
API off link is basically the probably

00:02:10,989 --> 00:02:15,130
most well known API to to implements

00:02:12,970 --> 00:02:17,680
tree processing applications

00:02:15,130 --> 00:02:19,270
it's very expressive so basically all

00:02:17,680 --> 00:02:22,390
the logic that you implement goes

00:02:19,270 --> 00:02:25,000
to user different functions and you got

00:02:22,390 --> 00:02:27,220
you have this API of of many concepts

00:02:25,000 --> 00:02:29,410
that are very to the core of stream

00:02:27,220 --> 00:02:32,290
processing such as windows triggers you

00:02:29,410 --> 00:02:35,590
have some ways to deal with time and

00:02:32,290 --> 00:02:37,690
state you can do asynchronous codes so

00:02:35,590 --> 00:02:40,240
all of this is built in this API so it

00:02:37,690 --> 00:02:44,200
gives you a lot of control of about how

00:02:40,240 --> 00:02:45,940
you're processing streams and how I deal

00:02:44,200 --> 00:02:48,550
with state and time basically all those

00:02:45,940 --> 00:02:51,550
concepts which are pretty much to the

00:02:48,550 --> 00:02:53,860
core of stream processing on the other

00:02:51,550 --> 00:02:56,560
hand many applications follow follow

00:02:53,860 --> 00:02:59,350
similar patterns and don't really don't

00:02:56,560 --> 00:03:04,590
really need this expressiveness of the

00:02:59,350 --> 00:03:06,760
data stream API and these these

00:03:04,590 --> 00:03:10,150
applications can be can be implemented

00:03:06,760 --> 00:03:12,670
are more easily and more concisely with

00:03:10,150 --> 00:03:16,570
a domain-specific language and what's

00:03:12,670 --> 00:03:21,240
the most and most popular DSL for for

00:03:16,570 --> 00:03:25,360
data processing of course its sequel so

00:03:21,240 --> 00:03:28,510
that's why flink or the flink project of

00:03:25,360 --> 00:03:32,260
thing community is working on relational

00:03:28,510 --> 00:03:34,930
api's form for apache flink right now

00:03:32,260 --> 00:03:38,950
there are two different api's there is

00:03:34,930 --> 00:03:41,530
standard sequel and so called a table

00:03:38,950 --> 00:03:45,910
API which is a language integrated API

00:03:41,530 --> 00:03:48,430
for Java and Scala we'll see later how

00:03:45,910 --> 00:03:52,510
query looks like in this table API and

00:03:48,430 --> 00:03:54,220
both of these api's are integrated

00:03:52,510 --> 00:03:56,650
unified API is for beds and stream

00:03:54,220 --> 00:04:02,459
processing which basically means that if

00:03:56,650 --> 00:04:05,080
you specify a career in the the

00:04:02,459 --> 00:04:08,350
semantics of the query do not depend on

00:04:05,080 --> 00:04:11,440
whether the input is a stream or a batch

00:04:08,350 --> 00:04:13,360
input so it doesn't matter whether you

00:04:11,440 --> 00:04:16,480
read the data as a stream or reading a

00:04:13,360 --> 00:04:19,660
file the result that is computed by this

00:04:16,480 --> 00:04:22,210
query will be the same so this is a very

00:04:19,660 --> 00:04:24,160
important very important point that I'd

00:04:22,210 --> 00:04:26,440
like to stress it's all about the

00:04:24,160 --> 00:04:29,830
semantics here that you have the same

00:04:26,440 --> 00:04:33,229
API for fetching stream processing the

00:04:29,830 --> 00:04:35,599
to API is the sequel end table API

00:04:33,229 --> 00:04:37,909
have common translation layers so

00:04:35,599 --> 00:04:40,729
there's a lot of a lot of the internals

00:04:37,909 --> 00:04:43,249
also the random code is the same and all

00:04:40,729 --> 00:04:47,659
of this is based on a picture calcite

00:04:43,249 --> 00:04:53,300
which is a yeah clearly optimized a

00:04:47,659 --> 00:04:56,930
clear policy proposal and app educator

00:04:53,300 --> 00:04:59,779
is basically used to pass sequel and

00:04:56,930 --> 00:05:01,219
also to translate the table API cards

00:04:59,779 --> 00:05:03,590
into a common representation which is a

00:05:01,219 --> 00:05:06,919
logical theory plan then this logical

00:05:03,590 --> 00:05:09,020
query plan is optimized by the website

00:05:06,919 --> 00:05:13,370
optimizer with a custom routes depending

00:05:09,020 --> 00:05:16,729
on whether the input is it's streaming

00:05:13,370 --> 00:05:18,620
or badge and then the this physical plan

00:05:16,729 --> 00:05:21,139
gets translated either in a blink data

00:05:18,620 --> 00:05:23,180
set program if the input is batch input

00:05:21,139 --> 00:05:27,349
or I did a stream program if the input

00:05:23,180 --> 00:05:29,000
is stream streaming input so we have

00:05:27,349 --> 00:05:32,210
different translation part for batch and

00:05:29,000 --> 00:05:36,740
stream but the result is I said is it's

00:05:32,210 --> 00:05:39,409
the same alright so um how do curious

00:05:36,740 --> 00:05:42,710
look like in these in these api's so

00:05:39,409 --> 00:05:47,629
first of all on a table a query

00:05:42,710 --> 00:05:49,729
so you see we're starting here from so

00:05:47,629 --> 00:05:53,240
called table environment this pmf at the

00:05:49,729 --> 00:05:55,460
top on this we can call a method called

00:05:53,240 --> 00:05:58,759
scan we give a name here which is the

00:05:55,460 --> 00:06:02,110
name of a table we do some filtering on

00:05:58,759 --> 00:06:05,210
that so with the the table is a table of

00:06:02,110 --> 00:06:07,159
clickstream data so which has like a in

00:06:05,210 --> 00:06:09,050
common feeds they would expect so there

00:06:07,159 --> 00:06:11,569
some UL included in there and we aren't

00:06:09,050 --> 00:06:18,469
interested in anywhere else let's start

00:06:11,569 --> 00:06:20,240
with these dub-dub-dub xyz.com after

00:06:18,469 --> 00:06:23,779
that we do a group i we group by the

00:06:20,240 --> 00:06:26,599
user that did the click and after that

00:06:23,779 --> 00:06:32,419
we basically want to count how many how

00:06:26,599 --> 00:06:35,509
many links this user clicked that came

00:06:32,419 --> 00:06:38,120
came from this domain so if you do the

00:06:35,509 --> 00:06:41,680
same query in in sequel it has it looks

00:06:38,120 --> 00:06:45,080
like this so there's a little bit of

00:06:41,680 --> 00:06:46,580
code around but the secret viewer here

00:06:45,080 --> 00:06:48,470
basically looks like

00:06:46,580 --> 00:06:53,180
as we would expect it to look so we have

00:06:48,470 --> 00:06:54,680
a select which basically selects what

00:06:53,180 --> 00:06:57,650
you what put the result should contain

00:06:54,680 --> 00:06:59,030
the user and the kind of clicks in the

00:06:57,650 --> 00:07:01,129
from clause you define the table that

00:06:59,030 --> 00:07:03,860
you want to read if we're closed for the

00:07:01,129 --> 00:07:06,319
filter and the group by then tells how

00:07:03,860 --> 00:07:11,240
the record should be group for the

00:07:06,319 --> 00:07:13,099
aggregation so and it doesn't really

00:07:11,240 --> 00:07:15,490
matter whether clicks is a file or

00:07:13,099 --> 00:07:19,219
database table or stream as I said um

00:07:15,490 --> 00:07:22,849
this is this is a unified API that can

00:07:19,219 --> 00:07:28,969
be used both on beds input and on stream

00:07:22,849 --> 00:07:32,629
input so how does it look like if clicks

00:07:28,969 --> 00:07:35,020
is a file arm so we have this file here

00:07:32,629 --> 00:07:36,889
on the on the top left which gets

00:07:35,020 --> 00:07:39,229
translated in some kind of table

00:07:36,889 --> 00:07:42,529
representation then we call this query

00:07:39,229 --> 00:07:46,580
on that and the result is here so on on

00:07:42,529 --> 00:07:49,490
the on the right hand side this is

00:07:46,580 --> 00:07:52,430
basically what you would expect security

00:07:49,490 --> 00:07:55,159
do and what do we do if we get more

00:07:52,430 --> 00:07:57,560
click data so click click debtors you

00:07:55,159 --> 00:08:00,229
like produce enter stream what do we get

00:07:57,560 --> 00:08:02,539
it if we get more click data well we

00:08:00,229 --> 00:08:04,039
have to query run the query again so

00:08:02,539 --> 00:08:06,860
this is how you would do it basically in

00:08:04,039 --> 00:08:10,129
a bachelor right either you collect the

00:08:06,860 --> 00:08:11,419
data like an invention for each day and

00:08:10,129 --> 00:08:17,930
then your every day you run the query

00:08:11,419 --> 00:08:20,900
and yeah so this is how it would be done

00:08:17,930 --> 00:08:23,330
in a in a batch word but what if clicks

00:08:20,900 --> 00:08:25,539
is represented the stream so you don't

00:08:23,330 --> 00:08:28,819
write it to a file but we want to ingest

00:08:25,539 --> 00:08:30,650
I'm basically a directly the stream and

00:08:28,819 --> 00:08:34,640
compute the the results based on the

00:08:30,650 --> 00:08:36,979
stream we of course want to have the

00:08:34,640 --> 00:08:38,570
same result if we directly ingest the

00:08:36,979 --> 00:08:41,570
stream into into the query as if we

00:08:38,570 --> 00:08:43,699
would first write it to a batch file but

00:08:41,570 --> 00:08:48,339
then the question is how well does

00:08:43,699 --> 00:08:50,360
actually like sequel work on on streams

00:08:48,339 --> 00:08:51,829
so

00:08:50,360 --> 00:08:54,290
sequel was not really designed for

00:08:51,829 --> 00:08:58,130
screams so if you look at the concept

00:08:54,290 --> 00:09:01,940
you see relations are like bounded sets

00:08:58,130 --> 00:09:04,820
screams of infinite sequences the

00:09:01,940 --> 00:09:10,100
database management system is assumes

00:09:04,820 --> 00:09:12,140
that it can access all data off the

00:09:10,100 --> 00:09:14,360
table that it that it wants to process

00:09:12,140 --> 00:09:17,930
whereas stream data raised over time

00:09:14,360 --> 00:09:19,839
right so you don't know what what data

00:09:17,930 --> 00:09:22,579
you have to process in five minutes and

00:09:19,839 --> 00:09:26,540
a secret fury honor on a table or on a

00:09:22,579 --> 00:09:28,700
database table or file returns the

00:09:26,540 --> 00:09:31,279
result and it's done so it's completed

00:09:28,700 --> 00:09:32,779
whereas a streaming theory computes the

00:09:31,279 --> 00:09:38,230
result continuously based on the data

00:09:32,779 --> 00:09:41,899
that arrives and never really complete

00:09:38,230 --> 00:09:44,720
nonetheless database systems are doing

00:09:41,899 --> 00:09:47,390
something like that already so or more

00:09:44,720 --> 00:09:50,959
advanced database systems there's a

00:09:47,390 --> 00:09:52,490
feature called materialized views which

00:09:50,959 --> 00:09:54,740
are basically very similar to regular

00:09:52,490 --> 00:10:00,709
views so you basically define a query

00:09:54,740 --> 00:10:03,010
that on some some some tables and then

00:10:00,709 --> 00:10:06,649
you have like a virtual table of that

00:10:03,010 --> 00:10:08,420
but the difference between virtual views

00:10:06,649 --> 00:10:10,220
and materialized views is that the

00:10:08,420 --> 00:10:13,010
result of these clear is actually

00:10:10,220 --> 00:10:15,190
persisted as a natural database as an

00:10:13,010 --> 00:10:17,329
actual table inside the database and

00:10:15,190 --> 00:10:19,550
this feature these materialized views

00:10:17,329 --> 00:10:24,140
are usually used to speed up analytic

00:10:19,550 --> 00:10:26,540
appears and but this also means that the

00:10:24,140 --> 00:10:30,920
database system has to make sure that

00:10:26,540 --> 00:10:33,829
this materialized view is updated or as

00:10:30,920 --> 00:10:36,199
kept consistent with the base levels so

00:10:33,829 --> 00:10:38,899
if I would define a materialized view

00:10:36,199 --> 00:10:40,670
and I update the base tables then the

00:10:38,899 --> 00:10:45,800
database system is make sure that these

00:10:40,670 --> 00:10:48,019
updates are basically used to also

00:10:45,800 --> 00:10:49,790
update the materialized view and this

00:10:48,019 --> 00:10:51,279
this whole maintenance of materialized

00:10:49,790 --> 00:10:53,810
views is very similar to actually

00:10:51,279 --> 00:10:56,300
processing sibilant streams so if you

00:10:53,810 --> 00:10:59,930
think about it the base tables the base

00:10:56,300 --> 00:11:01,730
table updates like the insert delete or

00:10:59,930 --> 00:11:03,949
update statements they go to against the

00:11:01,730 --> 00:11:04,310
base tables at some kind of a stream if

00:11:03,949 --> 00:11:05,840
they

00:11:04,310 --> 00:11:10,760
about it and then the database system

00:11:05,840 --> 00:11:14,360
has to basically use or process to view

00:11:10,760 --> 00:11:16,430
these update statements and based on

00:11:14,360 --> 00:11:19,810
these update statements has to update

00:11:16,430 --> 00:11:23,540
the materialized view and this is

00:11:19,810 --> 00:11:29,540
actually pretty much what simulant

00:11:23,540 --> 00:11:31,690
streams is about in flink we recently

00:11:29,540 --> 00:11:35,990
added this feature of continuous queries

00:11:31,690 --> 00:11:38,029
and the concept of this also called

00:11:35,990 --> 00:11:40,130
dynamic tables dynamic tables are

00:11:38,029 --> 00:11:43,610
changing over time so you have a heavy

00:11:40,130 --> 00:11:47,330
table and as more data arrives or from

00:11:43,610 --> 00:11:50,720
from a stream these tables are changing

00:11:47,330 --> 00:11:52,760
and you can very dynamic tables and such

00:11:50,720 --> 00:11:55,130
a query on the dominating table produces

00:11:52,760 --> 00:11:57,920
another dynamic table and this dynamic

00:11:55,130 --> 00:12:00,260
table is updated based on the changes of

00:11:57,920 --> 00:12:02,060
the input data these curious

00:12:00,260 --> 00:12:05,960
discontinuous gears and dynamic table do

00:12:02,060 --> 00:12:07,580
not terminate they're basically just

00:12:05,960 --> 00:12:10,280
listening to the to the updates of the

00:12:07,580 --> 00:12:14,800
input tables and then keep the result

00:12:10,280 --> 00:12:17,030
table consistent with the changes on the

00:12:14,800 --> 00:12:19,540
with it with the changes on the on the

00:12:17,030 --> 00:12:19,540
input tables

00:12:25,960 --> 00:12:33,640
yeah so other queries that we rerun

00:12:31,760 --> 00:12:36,740
conceptually run on these dynamic tables

00:12:33,640 --> 00:12:37,850
however dynamic table is just a just a

00:12:36,740 --> 00:12:41,089
logical concept here

00:12:37,850 --> 00:12:42,680
so in fact linked internally doesn't

00:12:41,089 --> 00:12:44,990
really produce this dynamic tables but

00:12:42,680 --> 00:12:47,780
it's it's a nice mental model to think

00:12:44,990 --> 00:12:51,080
about how how these continuous queries

00:12:47,780 --> 00:12:54,980
are processed in order to integrate that

00:12:51,080 --> 00:12:57,320
with a stream processing we needn't need

00:12:54,980 --> 00:12:59,420
a way to turn a stream into a dynamic

00:12:57,320 --> 00:13:01,730
table and later turn the dynamic table

00:12:59,420 --> 00:13:03,589
back into a stream so it basically looks

00:13:01,730 --> 00:13:06,290
like this where we have a stream first

00:13:03,589 --> 00:13:08,630
it's logically converted into a dynamic

00:13:06,290 --> 00:13:10,580
table then we define or rendered

00:13:08,630 --> 00:13:14,270
continuous query which produces a new

00:13:10,580 --> 00:13:17,650
dynamic table and this result table is

00:13:14,270 --> 00:13:17,650
later converted back into a stream

00:13:18,830 --> 00:13:28,620
how can we convert a stream into a

00:13:26,280 --> 00:13:30,560
dynamic table well there are a couple of

00:13:28,620 --> 00:13:33,810
ways to do that one is the so called

00:13:30,560 --> 00:13:36,060
append mode we basically have a stream

00:13:33,810 --> 00:13:40,260
of events arriving and each of these

00:13:36,060 --> 00:13:43,200
events is simply appended to this

00:13:40,260 --> 00:13:45,120
dynamic table so this dynamic table is

00:13:43,200 --> 00:13:48,330
then continuously growing it grows as

00:13:45,120 --> 00:13:50,700
more data arise from the stream and we

00:13:48,330 --> 00:13:52,650
simply each of the of the events that

00:13:50,700 --> 00:13:55,310
arrives on the stream is just appended

00:13:52,650 --> 00:13:59,370
to the end of the table

00:13:55,310 --> 00:14:02,610
another mode to turn a stream into a

00:13:59,370 --> 00:14:08,310
dynamic table is the absurd absurd mode

00:14:02,610 --> 00:14:11,670
so here the input data which has a

00:14:08,310 --> 00:14:13,650
certain certain schemer has a composite

00:14:11,670 --> 00:14:15,990
key or has a key or composite key some

00:14:13,650 --> 00:14:18,270
some key attributes and all records that

00:14:15,990 --> 00:14:20,310
arrived from the stream either inserted

00:14:18,270 --> 00:14:24,660
if we've never seen that key before or

00:14:20,310 --> 00:14:28,980
we update the existing record with the

00:14:24,660 --> 00:14:32,160
same key so this is C here we have this

00:14:28,980 --> 00:14:34,950
this streaming of six events

00:14:32,160 --> 00:14:41,630
the first one is has the user one with a

00:14:34,950 --> 00:14:46,110
name Mary and later this this user with

00:14:41,630 --> 00:14:48,930
with ID you want is overwritten by by

00:14:46,110 --> 00:14:55,020
later later events that arrived on the

00:14:48,930 --> 00:14:59,340
stream so once we have this this dynamic

00:14:55,020 --> 00:15:02,190
table defined the question is how can we

00:14:59,340 --> 00:15:04,770
evaluate a query based on that that is a

00:15:02,190 --> 00:15:10,260
clear it against such a dynamic table

00:15:04,770 --> 00:15:13,110
and this basically looks like this so if

00:15:10,260 --> 00:15:14,790
we take this even a little bit

00:15:13,110 --> 00:15:19,580
simplified example here of a simple

00:15:14,790 --> 00:15:24,150
group account theory again this is the

00:15:19,580 --> 00:15:27,060
the schema of a click stream here and if

00:15:24,150 --> 00:15:29,550
now the data arrives in the in the

00:15:27,060 --> 00:15:31,860
clicks table on the left hand side

00:15:29,550 --> 00:15:34,170
this query will then update the result

00:15:31,860 --> 00:15:39,570
table on the right hand side so if we

00:15:34,170 --> 00:15:42,990
get the first record on the input clicks

00:15:39,570 --> 00:15:45,120
table we'll get immediately see that the

00:15:42,990 --> 00:15:48,810
result table is updated so as more data

00:15:45,120 --> 00:15:54,230
arrives we update the result table so

00:15:48,810 --> 00:15:59,070
with a11 enter here with the user of ID

00:15:54,230 --> 00:16:02,760
uu1 so we get here an entry with u1 and

00:15:59,070 --> 00:16:05,640
a count of 1 7 for you to and if then

00:16:02,760 --> 00:16:09,240
another record arrives with you 1 we

00:16:05,640 --> 00:16:10,950
update the count here 2 2 so what

00:16:09,240 --> 00:16:15,630
basically what what the query internally

00:16:10,950 --> 00:16:18,030
does it has the the result table

00:16:15,630 --> 00:16:20,340
internally it keeps that estate in flink

00:16:18,030 --> 00:16:24,120
and it's more data arises if more change

00:16:20,340 --> 00:16:28,310
data arrives it takes the the current

00:16:24,120 --> 00:16:31,830
state of the of the result table and

00:16:28,310 --> 00:16:34,140
uses the update on the base server to

00:16:31,830 --> 00:16:38,850
also update the result and if we now add

00:16:34,140 --> 00:16:44,100
more data the counts of this dynamic

00:16:38,850 --> 00:16:48,180
result table evolve one thing that is

00:16:44,100 --> 00:16:51,090
important to note here is that the rows

00:16:48,180 --> 00:16:53,280
of the result table are updated so this

00:16:51,090 --> 00:16:55,530
table is really dynamically changing

00:16:53,280 --> 00:16:59,550
it's not just appended it's really the

00:16:55,530 --> 00:17:01,530
rows are of the result table are updated

00:16:59,550 --> 00:17:05,160
and this is something that we later have

00:17:01,530 --> 00:17:08,250
to take into take into account so this

00:17:05,160 --> 00:17:09,630
is a simple query now you might ask

00:17:08,250 --> 00:17:11,310
yourself well some processing is all

00:17:09,630 --> 00:17:16,110
about windows right so here we don't see

00:17:11,310 --> 00:17:20,760
any windows so how how do windows do

00:17:16,110 --> 00:17:22,560
winners relate to this and flink sequel

00:17:20,760 --> 00:17:26,790
and also the table API also supports

00:17:22,560 --> 00:17:30,840
different types of windows this is one

00:17:26,790 --> 00:17:33,810
example where we do a tumbling window of

00:17:30,840 --> 00:17:36,000
one hour on the click stream and here

00:17:33,810 --> 00:17:39,630
with basics don't want to count how many

00:17:36,000 --> 00:17:40,950
links user visited since we started the

00:17:39,630 --> 00:17:42,250
query which is basically what the other

00:17:40,950 --> 00:17:44,740
query did but here

00:17:42,250 --> 00:17:47,230
want to count per for each hour how many

00:17:44,740 --> 00:17:52,630
links is user visited so we say again

00:17:47,230 --> 00:17:54,400
scan these links this clicks table then

00:17:52,630 --> 00:17:56,560
we define a window and the window is

00:17:54,400 --> 00:17:59,380
defined here is a tumbling window over

00:17:56,560 --> 00:18:01,630
one hour on the time column which is C

00:17:59,380 --> 00:18:03,640
time here and the terminal window is

00:18:01,630 --> 00:18:05,110
basically just a just the window which

00:18:03,640 --> 00:18:07,480
is evaluated every hour so we have a

00:18:05,110 --> 00:18:11,290
window from from 12 to 1 from 1 to 2

00:18:07,480 --> 00:18:15,460
from 2 to 3 and so on then we do a group

00:18:11,290 --> 00:18:17,500
I'm here on W which is an alias that we

00:18:15,460 --> 00:18:20,380
assigned to the window that we defined

00:18:17,500 --> 00:18:24,430
before we want to group I in addition on

00:18:20,380 --> 00:18:25,930
the user field and then we say ok the

00:18:24,430 --> 00:18:27,700
result that we want to compute from this

00:18:25,930 --> 00:18:29,890
is we want to have the user we want to

00:18:27,700 --> 00:18:35,140
have the end timestamp of the window and

00:18:29,890 --> 00:18:39,130
we want to to count how many links a

00:18:35,140 --> 00:18:40,630
user visited within this window if you

00:18:39,130 --> 00:18:43,660
look at the secret fear it basically

00:18:40,630 --> 00:18:46,660
looks very similar in the last release

00:18:43,660 --> 00:18:51,280
of carrot side care that edit these

00:18:46,660 --> 00:18:53,680
window functions and which we can add or

00:18:51,280 --> 00:18:55,360
group window functions that can add to

00:18:53,680 --> 00:18:57,730
the group by clause so here we see a

00:18:55,360 --> 00:18:59,920
tumble we give the time attribute see

00:18:57,730 --> 00:19:03,820
time and then we define how long the

00:18:59,920 --> 00:19:05,530
window should be and there is another

00:19:03,820 --> 00:19:08,020
function called tumble end which is

00:19:05,530 --> 00:19:10,630
function that returns the end timestamp

00:19:08,020 --> 00:19:13,600
of the window here and apart from that

00:19:10,630 --> 00:19:15,970
the query is very similar to to what we

00:19:13,600 --> 00:19:20,260
seen before so we say group by tumble

00:19:15,970 --> 00:19:22,510
end user from clicks and again we want

00:19:20,260 --> 00:19:26,170
to have the user end times of the window

00:19:22,510 --> 00:19:28,920
and also the count and the count of the

00:19:26,170 --> 00:19:34,480
links that are the user visited so if we

00:19:28,920 --> 00:19:37,840
run such a theory it looks like this we

00:19:34,480 --> 00:19:40,930
have here a couple of clicks in the

00:19:37,840 --> 00:19:43,560
range from 12 o'clock to to 1 o'clock

00:19:40,930 --> 00:19:46,720
all of these are aggregated by the query

00:19:43,560 --> 00:19:51,220
into these two records

00:19:46,720 --> 00:19:53,770
you see user u1 visited three links here

00:19:51,220 --> 00:19:57,520
or the green green fields

00:19:53,770 --> 00:20:02,320
user YouTube just visited one and the

00:19:57,520 --> 00:20:03,910
next hour from one to two user to again

00:20:02,320 --> 00:20:06,370
visited just one link and is a three

00:20:03,910 --> 00:20:10,840
visited two and so on so here we can

00:20:06,370 --> 00:20:13,480
basically compute these aggregates as

00:20:10,840 --> 00:20:17,290
they go and an important thing to note

00:20:13,480 --> 00:20:20,020
here is that the rows are appended to

00:20:17,290 --> 00:20:22,090
the results table so here once an entry

00:20:20,020 --> 00:20:23,800
has been added to the to the result

00:20:22,090 --> 00:20:27,610
table it's not updated anymore so this

00:20:23,800 --> 00:20:31,080
is where this query differs from the

00:20:27,610 --> 00:20:36,430
occur before we had where we emitted a

00:20:31,080 --> 00:20:41,680
row which was later than updated okay so

00:20:36,430 --> 00:20:44,260
if you now want to turn a dynamic table

00:20:41,680 --> 00:20:51,000
into a stream again what do we have to

00:20:44,260 --> 00:20:53,770
do so here the important point is that

00:20:51,000 --> 00:20:57,100
we somehow need to deal with the updates

00:20:53,770 --> 00:20:59,980
on the dynamic table right so if we have

00:20:57,100 --> 00:21:01,810
a table which updates its rows we cannot

00:20:59,980 --> 00:21:04,270
simply we somehow need to encode these

00:21:01,810 --> 00:21:07,510
updates in the stream that we send out

00:21:04,270 --> 00:21:10,570
to the don't downstream system if we

00:21:07,510 --> 00:21:12,970
have a table which simply append as in

00:21:10,570 --> 00:21:15,130
the end the window at example here then

00:21:12,970 --> 00:21:17,020
we can scan simply if we know we will

00:21:15,130 --> 00:21:19,540
never update any record that we have

00:21:17,020 --> 00:21:23,350
computed we can simply omit this to a

00:21:19,540 --> 00:21:24,760
stream and omit the records of the

00:21:23,350 --> 00:21:26,920
stream and we are we're good

00:21:24,760 --> 00:21:29,200
however if you know that we somehow are

00:21:26,920 --> 00:21:31,000
that everything that we computed might

00:21:29,200 --> 00:21:35,890
change in the future we someone need to

00:21:31,000 --> 00:21:39,310
need a mechanism to encode that and the

00:21:35,890 --> 00:21:41,290
way that flink does this is kind of

00:21:39,310 --> 00:21:43,960
inspired by databases or by by the

00:21:41,290 --> 00:21:49,870
logging mechanisms of database systems

00:21:43,960 --> 00:21:52,290
and database systems use these locks

00:21:49,870 --> 00:21:55,090
basically to to be able to restore

00:21:52,290 --> 00:21:58,120
restore databases and tables in case of

00:21:55,090 --> 00:22:00,970
a failure to have the data consistent

00:21:58,120 --> 00:22:03,720
and there are two techniques basically

00:22:00,970 --> 00:22:05,760
one is the redo lock which stores all

00:22:03,720 --> 00:22:08,880
or new records that should be added to

00:22:05,760 --> 00:22:10,860
the database arm in order to be able to

00:22:08,880 --> 00:22:12,780
redo changes in case of a failure in

00:22:10,860 --> 00:22:14,789
case these these changes have not been

00:22:12,780 --> 00:22:16,620
materialized to the disk yet when the

00:22:14,789 --> 00:22:18,900
failure happens and the other one is a

00:22:16,620 --> 00:22:22,470
so-called undo log which as to us the

00:22:18,900 --> 00:22:24,480
old records that were changed in the

00:22:22,470 --> 00:22:28,770
transaction in order to be able to undo

00:22:24,480 --> 00:22:31,500
changes in case will change the database

00:22:28,770 --> 00:22:33,900
has written to the table but not

00:22:31,500 --> 00:22:37,350
committed the transaction yet when a

00:22:33,900 --> 00:22:40,020
failure happened so we kind of like use

00:22:37,350 --> 00:22:45,510
the same terminology here for for

00:22:40,020 --> 00:22:47,610
storing old and new records yeah so one

00:22:45,510 --> 00:22:51,000
technique to convert the dynamic table

00:22:47,610 --> 00:22:58,230
into a stream is the what we call a redo

00:22:51,000 --> 00:23:01,710
undo conversion and in this in this

00:22:58,230 --> 00:23:04,429
example here we have again this simple

00:23:01,710 --> 00:23:09,600
simple query which is the non windowed

00:23:04,429 --> 00:23:11,640
group ikon theory and we have the input

00:23:09,600 --> 00:23:15,750
data here on the left-hand side we say

00:23:11,640 --> 00:23:20,070
use ID 1 link 1 use ID to use ID 1 and

00:23:15,750 --> 00:23:24,470
we see that the first entry here we

00:23:20,070 --> 00:23:28,289
basically omit two types of messages

00:23:24,470 --> 00:23:29,940
into to the resulting stream and these

00:23:28,289 --> 00:23:32,909
matches messages are insertion or

00:23:29,940 --> 00:23:34,260
deletion messages insertion is marked

00:23:32,909 --> 00:23:36,570
here with a plus sign and deletion is

00:23:34,260 --> 00:23:40,409
minus sign so if you get the first

00:23:36,570 --> 00:23:42,809
record here it goes into the query which

00:23:40,409 --> 00:23:44,850
produces conceptually this resolves

00:23:42,809 --> 00:23:46,380
dynamic Taylor and if we convert this

00:23:44,850 --> 00:23:51,530
result dynamic to every packet or stream

00:23:46,380 --> 00:23:55,950
we omit this insertion of user ID 1 and

00:23:51,530 --> 00:23:57,990
count equal 1 same for use ID 2 because

00:23:55,950 --> 00:24:00,780
we don't do any updates we again insert

00:23:57,990 --> 00:24:02,850
something in this and this dynamic cover

00:24:00,780 --> 00:24:07,770
but then if we get the threat record

00:24:02,850 --> 00:24:10,230
here with user ID 1 at this point we

00:24:07,770 --> 00:24:12,690
have to update a record that we

00:24:10,230 --> 00:24:14,880
previously computed and this is then

00:24:12,690 --> 00:24:17,610
done by invalidating the record that we

00:24:14,880 --> 00:24:20,400
previously admitted so we have mine

00:24:17,610 --> 00:24:22,200
you won one which invalidates the first

00:24:20,400 --> 00:24:25,080
record that we emitted and then we add a

00:24:22,200 --> 00:24:29,940
new record you a d1 with a counter - and

00:24:25,080 --> 00:24:31,080
this is how we encode the updates into

00:24:29,940 --> 00:24:33,179
this stream and then the downstream

00:24:31,080 --> 00:24:36,000
system would meet would need to

00:24:33,179 --> 00:24:38,549
basically be able to interpret all these

00:24:36,000 --> 00:24:42,120
messages and accordingly update it on

00:24:38,549 --> 00:24:45,000
its own website this is one way to

00:24:42,120 --> 00:24:48,270
include updates in a stream but the

00:24:45,000 --> 00:24:51,480
drawback here is that we get quite a few

00:24:48,270 --> 00:24:52,500
records right so whenever we update

00:24:51,480 --> 00:24:55,770
something or whenever we change

00:24:52,500 --> 00:25:00,150
something we have to admit omit to two

00:24:55,770 --> 00:25:03,630
records or a deletion at an insertion in

00:25:00,150 --> 00:25:07,260
many cases it's also possible to to not

00:25:03,630 --> 00:25:10,530
do that and just omit one one record and

00:25:07,260 --> 00:25:12,990
that's the case when the when the result

00:25:10,530 --> 00:25:15,450
is our resulting table has a unique key

00:25:12,990 --> 00:25:17,669
and in this case the user ID is a unique

00:25:15,450 --> 00:25:19,470
key because we only have one single row

00:25:17,669 --> 00:25:22,830
for each user ID because we grew up on

00:25:19,470 --> 00:25:25,710
that and in this case we encode the

00:25:22,830 --> 00:25:30,690
outgoing outgoing records in by three

00:25:25,710 --> 00:25:32,580
different kinds of messages that we made

00:25:30,690 --> 00:25:34,410
one is again an insertion which is very

00:25:32,580 --> 00:25:38,090
similar as before but then we have an

00:25:34,410 --> 00:25:42,570
update and delete message and these are

00:25:38,090 --> 00:25:44,940
always refer to their to the to the key

00:25:42,570 --> 00:25:47,070
attribute so when we get the first entry

00:25:44,940 --> 00:25:48,929
here with you one we again doesn't do an

00:25:47,070 --> 00:25:51,870
insertion the same for you too but then

00:25:48,929 --> 00:25:55,350
if we get the the this third record was

00:25:51,870 --> 00:25:56,790
just a second for u1 we again have to

00:25:55,350 --> 00:25:59,970
update a record and then we say okay

00:25:56,790 --> 00:26:03,840
here for this key for key u1 we have to

00:25:59,970 --> 00:26:05,730
update this record to the record where

00:26:03,840 --> 00:26:09,990
the count is two and this is how we can

00:26:05,730 --> 00:26:12,150
then encode these updates in an outgoing

00:26:09,990 --> 00:26:16,700
stream if we know that there is a unique

00:26:12,150 --> 00:26:16,700
key on there on the data results table

00:26:16,730 --> 00:26:25,290
so um can we run any crew on a dynamic

00:26:20,730 --> 00:26:27,000
table uncertain not so there are two

00:26:25,290 --> 00:26:29,309
types of constraints that we have to

00:26:27,000 --> 00:26:30,660
consider when we translate a secret

00:26:29,309 --> 00:26:31,350
clear on the stream so you cannot just

00:26:30,660 --> 00:26:34,830
simply

00:26:31,350 --> 00:26:38,630
run any arbitrary query on a on a stream

00:26:34,830 --> 00:26:42,840
and these constraints are in space and

00:26:38,630 --> 00:26:44,669
computational effort basically so the

00:26:42,840 --> 00:26:46,260
query that I've shown before the run

00:26:44,669 --> 00:26:48,030
example that we've that we've used all

00:26:46,260 --> 00:26:51,120
the time here it's basically a query

00:26:48,030 --> 00:26:53,600
that you cannot simply run in a

00:26:51,120 --> 00:26:56,690
distributed way unless you know that the

00:26:53,600 --> 00:27:03,270
that you're the amount of users you have

00:26:56,690 --> 00:27:06,000
does not does not significantly grow so

00:27:03,270 --> 00:27:07,950
the problem here is that as I said

00:27:06,000 --> 00:27:10,260
before when the cure is computed you

00:27:07,950 --> 00:27:12,630
have to check the current current state

00:27:10,260 --> 00:27:14,400
of the result table and update it with a

00:27:12,630 --> 00:27:17,520
result which means that the results

00:27:14,400 --> 00:27:22,789
table has to be kept in memory by the

00:27:17,520 --> 00:27:25,799
better streaming theorem and so this

00:27:22,789 --> 00:27:28,980
this state might grow over time if for

00:27:25,799 --> 00:27:30,419
instance we would not group by here on

00:27:28,980 --> 00:27:31,590
the user ID but we would group on

00:27:30,419 --> 00:27:34,799
something like a session ID which

00:27:31,590 --> 00:27:37,169
session ID which is unique then we would

00:27:34,799 --> 00:27:38,520
most likely run into a problem at some

00:27:37,169 --> 00:27:39,900
point in time because we get a new

00:27:38,520 --> 00:27:42,510
session IDs and new session T's we

00:27:39,900 --> 00:27:44,190
accumulate more and more state our

00:27:42,510 --> 00:27:46,140
internal state grows and grows and grows

00:27:44,190 --> 00:27:49,440
as most session IDs arrive and at some

00:27:46,140 --> 00:27:52,020
point in time we might run out of might

00:27:49,440 --> 00:27:54,330
run out of disk space or memory

00:27:52,020 --> 00:27:59,690
depending on what kind of step back and

00:27:54,330 --> 00:28:02,490
you choose so we have to make sure that

00:27:59,690 --> 00:28:04,440
that the state of a query that the Cure

00:28:02,490 --> 00:28:07,890
internally uses to to process its result

00:28:04,440 --> 00:28:09,419
does not does not grow to large and the

00:28:07,890 --> 00:28:11,490
other input we had the other constraint

00:28:09,419 --> 00:28:14,760
that we have to consider is as I said

00:28:11,490 --> 00:28:19,470
this computational effort if you look at

00:28:14,760 --> 00:28:22,309
this theory here where we have some kind

00:28:19,470 --> 00:28:26,880
of a users table and this users table

00:28:22,309 --> 00:28:29,580
records the last login of a user and we

00:28:26,880 --> 00:28:31,110
constantly want to know okay we want to

00:28:29,580 --> 00:28:32,880
rank our users by the last log and we

00:28:31,110 --> 00:28:35,730
also want to have always want to have

00:28:32,880 --> 00:28:37,559
like the user that locked in most

00:28:35,730 --> 00:28:40,280
recently to be on top of the of the

00:28:37,559 --> 00:28:45,179
results table or have a rank that is

00:28:40,280 --> 00:28:46,589
rank one then this would not work well

00:28:45,179 --> 00:28:48,960
if you would evaluate that in a stream

00:28:46,589 --> 00:28:51,960
because whenever we get a new record

00:28:48,960 --> 00:28:54,629
with a new login we have to change all

00:28:51,960 --> 00:28:56,249
other records so the amount of work that

00:28:54,629 --> 00:29:00,839
we have to do for a single update is

00:28:56,249 --> 00:29:02,909
very is very very big and this is

00:29:00,839 --> 00:29:07,830
nothing that would work very well in

00:29:02,909 --> 00:29:09,749
practice so where we can do something

00:29:07,830 --> 00:29:14,190
about the first problem with the state

00:29:09,749 --> 00:29:19,289
the the second the second problem of the

00:29:14,190 --> 00:29:22,049
computation effort is a lot more yeah

00:29:19,289 --> 00:29:23,429
it's basically based on the semantic

00:29:22,049 --> 00:29:28,859
system of the query and you cannot do a

00:29:23,429 --> 00:29:33,570
lot about it so what can we do to burn

00:29:28,859 --> 00:29:37,440
the state of a query so one thing that

00:29:33,570 --> 00:29:39,419
we can do is to arm play a little bit

00:29:37,440 --> 00:29:41,729
with the semantics of the query so we

00:29:39,419 --> 00:29:44,099
could for instance at a predicate here

00:29:41,729 --> 00:29:47,940
into the where clause we say for

00:29:44,099 --> 00:29:52,739
instance last click time interval one

00:29:47,940 --> 00:29:54,749
day which basically is something that or

00:29:52,739 --> 00:29:57,450
a function that I just made up but this

00:29:54,749 --> 00:29:59,489
could indicate that I'm only interested

00:29:57,450 --> 00:30:02,190
in the in the data of the last day so

00:29:59,489 --> 00:30:06,229
we're basically when I compute my query

00:30:02,190 --> 00:30:08,820
I would always compute the query on the

00:30:06,229 --> 00:30:10,830
last 24 hours of the stream in this as

00:30:08,820 --> 00:30:14,700
the stream evolves I always keep Basel

00:30:10,830 --> 00:30:17,580
II only the last last 24 hours and this

00:30:14,700 --> 00:30:19,469
would then also automatically limit the

00:30:17,580 --> 00:30:22,849
size of the state that I have to have to

00:30:19,469 --> 00:30:22,849
worry about when I evaluate the query

00:30:23,149 --> 00:30:29,639
another option to do this is to

00:30:26,580 --> 00:30:32,159
basically trade accuracy of the result

00:30:29,639 --> 00:30:37,219
for for the size of the start for the

00:30:32,159 --> 00:30:41,219
size of the state what I mean by that is

00:30:37,219 --> 00:30:44,549
actually if you want to compute a result

00:30:41,219 --> 00:30:48,089
of this query without the way across

00:30:44,549 --> 00:30:51,210
here absolutely correctly there is no

00:30:48,089 --> 00:30:52,889
way around to just keep the state for

00:30:51,210 --> 00:30:55,820
for each user arrives at any point in

00:30:52,889 --> 00:30:57,160
time they might arrive a record that

00:30:55,820 --> 00:30:59,559
accesses

00:30:57,160 --> 00:31:03,070
any user and then you would need to

00:30:59,559 --> 00:31:06,400
update this then you would need to

00:31:03,070 --> 00:31:10,150
update the corresponding record in the

00:31:06,400 --> 00:31:13,390
end your results table however if you

00:31:10,150 --> 00:31:15,910
know that over time some of the records

00:31:13,390 --> 00:31:18,910
become or some of the grouping Keys

00:31:15,910 --> 00:31:20,320
become become stale or inactive as for

00:31:18,910 --> 00:31:23,410
instance when I would group on on a

00:31:20,320 --> 00:31:28,840
session ID instead of a user then you

00:31:23,410 --> 00:31:30,789
can basically say okay I want to remove

00:31:28,840 --> 00:31:33,520
records which have been inactive for

00:31:30,789 --> 00:31:37,390
let's say 24 hours so I basically put a

00:31:33,520 --> 00:31:40,780
bound perky and whenever a key is

00:31:37,390 --> 00:31:43,059
updated I reduce the timer say okay if

00:31:40,780 --> 00:31:45,220
this is not updated within 24 hours then

00:31:43,059 --> 00:31:47,860
I can just remove the state if then it's

00:31:45,220 --> 00:31:50,860
at some point I receive another record

00:31:47,860 --> 00:31:54,330
for this for the same key that that I

00:31:50,860 --> 00:31:57,460
just removed that can like have like

00:31:54,330 --> 00:32:00,130
because in that case this would the

00:31:57,460 --> 00:32:02,470
system would basically see this as if

00:32:00,130 --> 00:32:06,419
the just received record would be the

00:32:02,470 --> 00:32:08,890
first one ever observed for the scheme

00:32:06,419 --> 00:32:10,150
but depending on the use cases set if

00:32:08,890 --> 00:32:12,789
you something like a unique session ID

00:32:10,150 --> 00:32:16,990
as a key this works very well to bound

00:32:12,789 --> 00:32:21,039
the state so um what is the current

00:32:16,990 --> 00:32:23,500
state of all of this the this relation

00:32:21,039 --> 00:32:26,409
ideas are rapidly evolving we've got

00:32:23,500 --> 00:32:31,450
many contributors contributing features

00:32:26,409 --> 00:32:34,000
and features and improvements to these

00:32:31,450 --> 00:32:36,490
API so we get more more built-in

00:32:34,000 --> 00:32:39,820
functions we get also core functionality

00:32:36,490 --> 00:32:41,650
being added and there are so many many

00:32:39,820 --> 00:32:45,100
people asking for these features on the

00:32:41,650 --> 00:32:47,200
on the mailing lists the API is actually

00:32:45,100 --> 00:32:52,330
used in production very large scale at

00:32:47,200 --> 00:32:53,980
Alibaba and Alibaba is also one of the

00:32:52,330 --> 00:32:58,169
contributors pushing us for these

00:32:53,980 --> 00:33:01,929
features and collaborating with us um

00:32:58,169 --> 00:33:03,190
recently linked 1 3 was released and we

00:33:01,929 --> 00:33:06,940
added a couple of exciting features

00:33:03,190 --> 00:33:08,770
there among those are window records for

00:33:06,940 --> 00:33:10,480
group I so basically this comber keyword

00:33:08,770 --> 00:33:12,450
that have just shown it's no

00:33:10,480 --> 00:33:15,460
available in 1/3 but you can also do

00:33:12,450 --> 00:33:18,309
popping windows or our certain windows

00:33:15,460 --> 00:33:21,490
so you can say I want a session with a

00:33:18,309 --> 00:33:24,190
gap of 30 minutes and then the link or

00:33:21,490 --> 00:33:29,530
the the tablet here will compute

00:33:24,190 --> 00:33:32,100
aggregate for all records that are not

00:33:29,530 --> 00:33:35,190
more than the gap apart from each other

00:33:32,100 --> 00:33:40,450
we also have these non-winner aggregate

00:33:35,190 --> 00:33:45,070
with update changes in flick 1:3 and

00:33:40,450 --> 00:33:46,720
user-defined aggregation functions so

00:33:45,070 --> 00:33:51,280
what kind of applications can a bit with

00:33:46,720 --> 00:33:53,500
this actually so our very calm use case

00:33:51,280 --> 00:33:58,120
is something like the continues ETL why

00:33:53,500 --> 00:34:01,510
where the query continuously ingests the

00:33:58,120 --> 00:34:03,700
data you play at transformations and and

00:34:01,510 --> 00:34:06,850
winner gets on it and then you write the

00:34:03,700 --> 00:34:12,010
data out to to do some kind of files to

00:34:06,850 --> 00:34:14,010
another Kafka topic that is later

00:34:12,010 --> 00:34:16,869
consumed by some some other processes

00:34:14,010 --> 00:34:19,119
for the data or you write into a

00:34:16,869 --> 00:34:22,240
database or HBase or whatever so this is

00:34:19,119 --> 00:34:25,990
basically kind some kind of like a yeah

00:34:22,240 --> 00:34:28,000
ETL ingestion transformation types of

00:34:25,990 --> 00:34:32,919
workloads that can be very easily

00:34:28,000 --> 00:34:35,230
specify with a sequel or the table API

00:34:32,919 --> 00:34:39,270
so you don't need to hand folders up for

00:34:35,230 --> 00:34:40,960
that it's much easier to use these api's

00:34:39,270 --> 00:34:43,750
further for these kinds of

00:34:40,960 --> 00:34:46,210
transformations another thing that you

00:34:43,750 --> 00:34:48,310
can't go through this is basically send

00:34:46,210 --> 00:34:51,940
it around this feature of updating

00:34:48,310 --> 00:34:53,800
updating tables and very very common use

00:34:51,940 --> 00:34:57,220
cases here are dashboards or reporting

00:34:53,800 --> 00:34:59,500
or but also on these types of event

00:34:57,220 --> 00:35:02,800
driven and event-driven different

00:34:59,500 --> 00:35:05,320
applications that always want to have

00:35:02,800 --> 00:35:07,240
low latency access to some kind of state

00:35:05,320 --> 00:35:10,150
which is a continuous computed from from

00:35:07,240 --> 00:35:12,190
arriving data so you have the query

00:35:10,150 --> 00:35:14,380
which maintains this kind of

00:35:12,190 --> 00:35:16,930
materialized view and this view can

00:35:14,380 --> 00:35:21,790
metric can be materialized to Cassandra

00:35:16,930 --> 00:35:22,980
or another key value store or relational

00:35:21,790 --> 00:35:24,810
database or compact

00:35:22,980 --> 00:35:29,310
Casca topic and then your applications

00:35:24,810 --> 00:35:32,160
basically always go against these this

00:35:29,310 --> 00:35:35,430
key value store or this data store and

00:35:32,160 --> 00:35:38,160
have the results which are updated as

00:35:35,430 --> 00:35:41,100
the stream is consumed where they buy

00:35:38,160 --> 00:35:43,680
they buy the continuous query and later

00:35:41,100 --> 00:35:46,380
also in a later version of link all

00:35:43,680 --> 00:35:48,390
these results will also be able to be

00:35:46,380 --> 00:35:50,580
maintained in curable state so in this

00:35:48,390 --> 00:35:52,590
case you don't even have to write use

00:35:50,580 --> 00:35:54,570
out into Cassandra in external data

00:35:52,590 --> 00:35:56,520
store you can just keep it inside of

00:35:54,570 --> 00:35:58,619
link inside of links gerber state and

00:35:56,520 --> 00:36:00,510
then the applications can go directly

00:35:58,619 --> 00:36:03,119
against this state and what this

00:36:00,510 --> 00:36:06,000
basically means is that Flinx somewhat

00:36:03,119 --> 00:36:09,109
becomes kind of like a database because

00:36:06,000 --> 00:36:12,390
it get exchanges as a stream ingested

00:36:09,109 --> 00:36:14,340
into in the flink link update compute

00:36:12,390 --> 00:36:18,150
the result keeps the updated state

00:36:14,340 --> 00:36:19,770
inside of inside of the curable state

00:36:18,150 --> 00:36:25,109
and then applications can go directly

00:36:19,770 --> 00:36:27,990
against that yeah so um table a pn 0 on

00:36:25,109 --> 00:36:30,480
support many use cases it's a high-level

00:36:27,990 --> 00:36:32,400
specification and declarative so it's

00:36:30,480 --> 00:36:35,700
fairly easy to use everybody knows how

00:36:32,400 --> 00:36:38,070
to use evil it's optimized it's

00:36:35,700 --> 00:36:39,330
efficiently executed and you can

00:36:38,070 --> 00:36:41,550
actually do quite a bit of quite a few

00:36:39,330 --> 00:36:43,170
of things because the table API and c

00:36:41,550 --> 00:36:45,150
will also support user-defined functions

00:36:43,170 --> 00:36:47,100
for scalar functions table functions

00:36:45,150 --> 00:36:49,530
aggregation functions so you can also

00:36:47,100 --> 00:36:54,570
encode a lot of functionality inside of

00:36:49,530 --> 00:36:58,140
these UDF and the the possibility to all

00:36:54,570 --> 00:37:00,000
have tables which which emit updates for

00:36:58,140 --> 00:37:03,600
for computed results and dematerialized

00:37:00,000 --> 00:37:07,100
ed to an external key value store this

00:37:03,600 --> 00:37:11,369
is enables a lots of lots of very very

00:37:07,100 --> 00:37:13,200
very exciting applications and yeah if

00:37:11,369 --> 00:37:16,350
you have some kind uses that might fit

00:37:13,200 --> 00:37:19,830
into these these things I encourage you

00:37:16,350 --> 00:37:23,160
to just try out in case you haven't

00:37:19,830 --> 00:37:26,400
heard this so data Adams is organizing a

00:37:23,160 --> 00:37:31,710
swing conference in September this year

00:37:26,400 --> 00:37:33,750
the core for paper ends soon and in

00:37:31,710 --> 00:37:36,609
about a week or so so if you have a

00:37:33,750 --> 00:37:40,650
topic that you'd like to talk about

00:37:36,609 --> 00:37:43,420
please submit and submit a talk there

00:37:40,650 --> 00:37:45,460
here's another famous plug arm I'm a

00:37:43,420 --> 00:37:47,859
quarter of this book stream processing

00:37:45,460 --> 00:37:51,219
an effective link it's available on all

00:37:47,859 --> 00:37:55,930
rightie early release so you can get it

00:37:51,219 --> 00:37:57,670
there and we yeah it should be if

00:37:55,930 --> 00:38:00,369
everything goes goes according to plan

00:37:57,670 --> 00:38:02,829
should be available sometime early next

00:38:00,369 --> 00:38:06,489
year completely but with the early

00:38:02,829 --> 00:38:08,799
release you have you can you can read

00:38:06,489 --> 00:38:11,680
what we have right now alright and

00:38:08,799 --> 00:38:13,450
finally we also hiring so in case you

00:38:11,680 --> 00:38:15,819
find any of this interesting or you'd

00:38:13,450 --> 00:38:18,600
like to work directly on fling

00:38:15,819 --> 00:38:20,000
come and talk to me thank you

00:38:18,600 --> 00:38:20,560
[Applause]

00:38:20,000 --> 00:38:24,770
[Music]

00:38:20,560 --> 00:38:24,770

YouTube URL: https://www.youtube.com/watch?v=c8J1O4qZRLA


