Title: Berlin Buzzwords 2017: Andrew Psaltis - Hands-on with Apache NiFi and MiNiFi #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	This talk will provide a hands on introduction to Apache NiFi and MiNiFi. Focusing on how to use them to securely, efficiently, and reliably collect data from the edge, curate and deliver it to other tiers in a streaming pipeline. This will be a very interactive session to ensure all participants understand the how and why of Apache NiFi and MiNiFi. We will cover the following topics:

Apache NiFi Overview
- Describe Apache NiFi and its use cases.
- Describe NiFi Architecture

Core Concepts
- Understand Nifi Features and Characteristics.
- Understand NiFi user interface in depth.
- Understand Processors
- Understand Connections
- Understand Processor Groups
- Understand Remote Processor Groups
- Explain Data Provenance in NiFi
- Understand Concepts of NiFi Cluster

Designing Data Flows
- Understand how to build a DataFlow using NiFi
- Learn how to optimize a DataFlow
- Learn how to use NiFi Expression language and its use.
- Learn about Attributes and Templates in NiFi

MiNiFi
- Overview
- Use Cases
- Integrating with NiFi

Read more:
https://2017.berlinbuzzwords.de/17/session/hands-apache-nifi-and-minifi

About Andrew Psaltis:
https://2017.berlinbuzzwords.de/users/andrew-psaltis

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,690 --> 00:00:10,880
thank you thanks for coming after lunch

00:00:08,710 --> 00:00:15,140
so we're going to talk about attaching

00:00:10,880 --> 00:00:16,160
knife identify sounds pretty loud so the

00:00:15,140 --> 00:00:17,750
first thing I want to talk about it's

00:00:16,160 --> 00:00:19,279
going to set the stage for just data

00:00:17,750 --> 00:00:24,020
flow and the associated problems

00:00:19,279 --> 00:00:25,400
minifying knife I aim to solve so think

00:00:24,020 --> 00:00:27,560
simplistically we kind of think about

00:00:25,400 --> 00:00:30,349
data flow this way that people just

00:00:27,560 --> 00:00:32,590
think that I'm ingesting some data and

00:00:30,349 --> 00:00:35,360
move it along and I store it somewhere

00:00:32,590 --> 00:00:38,120
reality is the world looks more like

00:00:35,360 --> 00:00:40,340
this alright everyone's got data all

00:00:38,120 --> 00:00:42,110
over the place some people you know

00:00:40,340 --> 00:00:43,610
smiling or laughing is this is kind of

00:00:42,110 --> 00:00:44,690
the world we live in right that it's all

00:00:43,610 --> 00:00:47,150
over the place

00:00:44,690 --> 00:00:49,940
we're doing things with it and then try

00:00:47,150 --> 00:00:52,640
and store it it's really this mess of

00:00:49,940 --> 00:00:55,400
data moving around that considered data

00:00:52,640 --> 00:00:59,390
flow that knife I minify are designed to

00:00:55,400 --> 00:01:03,380
solve it really gets summed up pretty

00:00:59,390 --> 00:01:05,360
well in this X SCD cartoon right that

00:01:03,380 --> 00:01:07,430
you end up with like these 14 competing

00:01:05,360 --> 00:01:08,750
standards get people that sit back and

00:01:07,430 --> 00:01:11,390
decide that they need to come up with

00:01:08,750 --> 00:01:14,780
one general standard you know 14 is not

00:01:11,390 --> 00:01:16,789
good six months later you end up with 15

00:01:14,780 --> 00:01:18,469
competing standards and we end up with

00:01:16,789 --> 00:01:20,780
the mess that we have of constantly

00:01:18,469 --> 00:01:23,689
different standards for moving data for

00:01:20,780 --> 00:01:29,119
storing data for encoding data for all

00:01:23,689 --> 00:01:32,719
sorts of things so if you then look at

00:01:29,119 --> 00:01:33,909
Apache knife I and it's Genesis and kind

00:01:32,719 --> 00:01:36,229
of where it's going

00:01:33,909 --> 00:01:38,689
it started out trying to solve that

00:01:36,229 --> 00:01:40,369
problem probably about 10 years ago at

00:01:38,689 --> 00:01:43,520
the NSA and these are some of the key

00:01:40,369 --> 00:01:45,770
features that were decided upon and

00:01:43,520 --> 00:01:47,840
still stay with it today as it continues

00:01:45,770 --> 00:01:49,909
to grow so some of the things as far as

00:01:47,840 --> 00:01:51,889
this to guarantee data delivery from the

00:01:49,909 --> 00:01:54,229
point of ingestion to the point of

00:01:51,889 --> 00:01:56,810
delivering data it's guaranteed of no

00:01:54,229 --> 00:02:00,109
loss you know to have data buffering

00:01:56,810 --> 00:02:02,119
it's not always is a consumer able to

00:02:00,109 --> 00:02:03,920
ingest data at the same rate so how do

00:02:02,119 --> 00:02:06,319
you buffer that how do you apply back

00:02:03,920 --> 00:02:09,649
pressure in the flow and how to relieve

00:02:06,319 --> 00:02:11,569
back pressure during the flow having

00:02:09,649 --> 00:02:14,180
prioritized queuing is so you see this

00:02:11,569 --> 00:02:17,020
whole data flow not all data is created

00:02:14,180 --> 00:02:19,510
the same not all data has the same

00:02:17,020 --> 00:02:21,280
importance you have some day that make

00:02:19,510 --> 00:02:23,379
a system that's an alarm that you need

00:02:21,280 --> 00:02:25,480
to act on faster and some data that

00:02:23,379 --> 00:02:27,750
could take slower so you could

00:02:25,480 --> 00:02:31,269
prioritize as a data move to the flow

00:02:27,750 --> 00:02:33,340
which is more important you have flow

00:02:31,269 --> 00:02:35,409
specific quality of service sometimes

00:02:33,340 --> 00:02:37,360
you prefer latency sometimes you prefer

00:02:35,409 --> 00:02:40,030
throughput and you can figure this as

00:02:37,360 --> 00:02:41,950
you go a variety other things too

00:02:40,030 --> 00:02:43,810
probably one that's most important is

00:02:41,950 --> 00:02:45,430
data provenance and we'll touch on a bit

00:02:43,810 --> 00:02:48,640
and kind of walk through and see how

00:02:45,430 --> 00:02:54,390
that works an inflow templating and

00:02:48,640 --> 00:02:58,450
clustering so knife eye is a web-based

00:02:54,390 --> 00:03:00,879
UI on top of this framework that allows

00:02:58,450 --> 00:03:04,840
you to drag and drop and interactively

00:03:00,879 --> 00:03:07,060
build these data flows ok so it's you

00:03:04,840 --> 00:03:08,680
could think about it as molding clay and

00:03:07,060 --> 00:03:10,329
we'll see as we kind of walk through and

00:03:08,680 --> 00:03:12,909
I'll build a couple of flows later on a

00:03:10,329 --> 00:03:15,849
demo that you get to interact with this

00:03:12,909 --> 00:03:17,500
data as it's moving and be able to

00:03:15,849 --> 00:03:20,950
change your mind be able to side things

00:03:17,500 --> 00:03:23,079
and move through so some of the things

00:03:20,950 --> 00:03:25,209
that need to think about it some the

00:03:23,079 --> 00:03:27,699
terminology that comes we start talking

00:03:25,209 --> 00:03:29,290
about 95 and minify and come and just

00:03:27,699 --> 00:03:31,359
get understanding as to the common

00:03:29,290 --> 00:03:34,530
things you'll hear the first is we'll

00:03:31,359 --> 00:03:36,459
hear a lot to talk about flow files

00:03:34,530 --> 00:03:38,139
anytime you look at anything to do with

00:03:36,459 --> 00:03:40,000
nya file you're always hear this concept

00:03:38,139 --> 00:03:42,340
of flow files you can really think about

00:03:40,000 --> 00:03:46,150
them as a unit of data that moves

00:03:42,340 --> 00:03:49,599
through 9 Phi or minify very similar to

00:03:46,150 --> 00:03:51,519
an HTTP response where an HTTP you'd

00:03:49,599 --> 00:03:55,120
have the data right so you have the data

00:03:51,519 --> 00:03:57,489
and you have the headers in minify or 95

00:03:55,120 --> 00:03:59,260
you would have the headers which would

00:03:57,489 --> 00:04:01,930
be called attributes then the data which

00:03:59,260 --> 00:04:04,000
is just the payload or the content so

00:04:01,930 --> 00:04:05,769
very similar type of concept to keep in

00:04:04,000 --> 00:04:08,079
your mind as we start looking at it that

00:04:05,769 --> 00:04:12,629
there's two different pieces the

00:04:08,079 --> 00:04:15,400
attributes and then the data itself a

00:04:12,629 --> 00:04:17,169
processor we saw that screenshot before

00:04:15,400 --> 00:04:18,820
you have these processors process is

00:04:17,169 --> 00:04:22,599
really just anything that you could use

00:04:18,820 --> 00:04:24,370
to ingress manipulate or egress data all

00:04:22,599 --> 00:04:26,080
of them are called the processor so

00:04:24,370 --> 00:04:29,800
anything that you do that takes action

00:04:26,080 --> 00:04:31,719
is a processor you have a connection

00:04:29,800 --> 00:04:32,660
between the processors this is where

00:04:31,719 --> 00:04:34,670
you'll set things like

00:04:32,660 --> 00:04:37,790
organization different types of back

00:04:34,670 --> 00:04:40,720
pressure I think you have a process

00:04:37,790 --> 00:04:43,070
group so this allows you to visually

00:04:40,720 --> 00:04:44,840
simplify things and group them together

00:04:43,070 --> 00:04:48,110
otherwise you end up with like this

00:04:44,840 --> 00:04:50,150
horse blanket of the typical ard of

00:04:48,110 --> 00:04:51,920
everything just all over the place using

00:04:50,150 --> 00:04:54,020
a processor group allows you to simplify

00:04:51,920 --> 00:04:57,860
that and make it clean as well as think

00:04:54,020 --> 00:04:59,090
about it as a component so if we that if

00:04:57,860 --> 00:05:02,060
you're familiar with the flow based

00:04:59,090 --> 00:05:04,610
programming model knife I get a lot of

00:05:02,060 --> 00:05:06,440
its ideas and a lot of it based on flow

00:05:04,610 --> 00:05:07,790
based programming so if you come across

00:05:06,440 --> 00:05:11,090
web based programming or you're familiar

00:05:07,790 --> 00:05:12,320
with it this is how it maps in the flow

00:05:11,090 --> 00:05:14,630
based programming world is what's called

00:05:12,320 --> 00:05:17,060
an information packet in knife is called

00:05:14,630 --> 00:05:19,520
a flow file you see things like a black

00:05:17,060 --> 00:05:25,550
box in flow-based programming in ninth i

00:05:19,520 --> 00:05:27,770
that's a flow file processor they can

00:05:25,550 --> 00:05:29,480
see the other items that are on here so

00:05:27,770 --> 00:05:31,550
if you have thoughts about where do they

00:05:29,480 --> 00:05:33,260
come from or different you know want to

00:05:31,550 --> 00:05:35,870
understand more about the background an

00:05:33,260 --> 00:05:37,760
eye-fi with you read in the flow based

00:05:35,870 --> 00:05:39,350
programming world you'll see a lot of

00:05:37,760 --> 00:05:44,540
similarities and this gives knife eyes

00:05:39,350 --> 00:05:45,980
based upon all those principles so if we

00:05:44,540 --> 00:05:49,340
put together some of these terminologies

00:05:45,980 --> 00:05:52,520
of a processor connection processor

00:05:49,340 --> 00:05:54,440
group here you have a generate flow file

00:05:52,520 --> 00:05:56,600
processor there's a connection between

00:05:54,440 --> 00:05:59,240
that's a success there's a router an

00:05:56,600 --> 00:06:00,410
attribute processor with an incoming

00:05:59,240 --> 00:06:02,930
that's going out of there

00:06:00,410 --> 00:06:05,720
and that goes says process data process

00:06:02,930 --> 00:06:08,300
group okay so each of these will be

00:06:05,720 --> 00:06:10,010
considered a this is processor rather an

00:06:08,300 --> 00:06:12,770
attribute of processor process data a

00:06:10,010 --> 00:06:14,660
process group and then where you see

00:06:12,770 --> 00:06:18,320
this cue will be the connection between

00:06:14,660 --> 00:06:20,030
us so I want to take a moment to talk a

00:06:18,320 --> 00:06:25,360
little bit more about some of the key

00:06:20,030 --> 00:06:27,710
features so the need for data provenance

00:06:25,360 --> 00:06:29,690
the often times people think about it as

00:06:27,710 --> 00:06:32,030
just its lineage and just kind of get an

00:06:29,690 --> 00:06:33,620
idea as to where things were and kind of

00:06:32,030 --> 00:06:36,200
the path through something but it really

00:06:33,620 --> 00:06:37,669
provides a lot more than that so you get

00:06:36,200 --> 00:06:40,250
this idea of having not just the

00:06:37,669 --> 00:06:42,560
traceability and lineage for an operator

00:06:40,250 --> 00:06:44,419
of understanding what happened but also

00:06:42,560 --> 00:06:46,029
from a compliance as well so you get a

00:06:44,419 --> 00:06:48,159
complete chain of

00:06:46,029 --> 00:06:49,930
and a complete understanding of what

00:06:48,159 --> 00:06:51,939
happened and every change to a piece of

00:06:49,930 --> 00:06:54,430
data so if you have situations where

00:06:51,939 --> 00:06:56,589
you're moving data across countries and

00:06:54,430 --> 00:06:58,300
certain data is not a lead not allowed

00:06:56,589 --> 00:07:00,969
to leave the boundaries of a country

00:06:58,300 --> 00:07:02,919
then you have Providence to ensure and

00:07:00,969 --> 00:07:03,610
provide audit that that actually never

00:07:02,919 --> 00:07:06,159
happened

00:07:03,610 --> 00:07:08,409
here we see financial customers that

00:07:06,159 --> 00:07:08,919
users understand what changed to a piece

00:07:08,409 --> 00:07:10,569
of data

00:07:08,919 --> 00:07:12,550
what computations were done and you can

00:07:10,569 --> 00:07:19,689
see that changed all the way through as

00:07:12,550 --> 00:07:21,789
a data flows this shows an example of it

00:07:19,689 --> 00:07:22,779
really just not lineage so this would be

00:07:21,789 --> 00:07:24,639
a way that you can look at the

00:07:22,779 --> 00:07:26,559
provenance this happens to be coming

00:07:24,639 --> 00:07:29,080
from a processor that's putting data

00:07:26,559 --> 00:07:32,050
into solar and you can look at this

00:07:29,080 --> 00:07:34,360
lineage tree as two that it was received

00:07:32,050 --> 00:07:36,580
there was a send event there's a drop

00:07:34,360 --> 00:07:38,979
event you see more details about this

00:07:36,580 --> 00:07:41,800
provenance event how long it was in

00:07:38,979 --> 00:07:44,020
lineage the UUID for the flow file of

00:07:41,800 --> 00:07:46,210
the size what it was you see the

00:07:44,020 --> 00:07:48,009
attributes to see the content and all

00:07:46,210 --> 00:07:50,080
this information you could also get out

00:07:48,009 --> 00:07:52,029
of nine file and send to another system

00:07:50,080 --> 00:07:54,189
so if you did want to use it for

00:07:52,029 --> 00:07:56,949
auditing and understanding everything

00:07:54,189 --> 00:07:58,599
done to data as it flowed through you

00:07:56,949 --> 00:08:00,819
could get this data sent it to solar

00:07:58,599 --> 00:08:02,469
scene people that take it to solar build

00:08:00,819 --> 00:08:05,169
dashboards on it to understand the flow

00:08:02,469 --> 00:08:06,789
of data people that work in a financial

00:08:05,169 --> 00:08:08,020
space that load it into HBase or

00:08:06,789 --> 00:08:10,120
something else they could then run

00:08:08,020 --> 00:08:16,149
audits on it we do a variety of things

00:08:10,120 --> 00:08:19,029
with this information if you look at

00:08:16,149 --> 00:08:21,520
security oftentimes people moving data

00:08:19,029 --> 00:08:24,639
that's sensitive but the security around

00:08:21,520 --> 00:08:26,889
it and it's really just not about having

00:08:24,639 --> 00:08:28,419
encrypted communication knife I supports

00:08:26,889 --> 00:08:30,550
encrypted communication whether it's

00:08:28,419 --> 00:08:32,949
from minified in ninth I or between

00:08:30,550 --> 00:08:35,620
knife I nodes or on the wire anywhere

00:08:32,949 --> 00:08:38,199
supports encryption also supports this

00:08:35,620 --> 00:08:40,930
notion of not all users have the same

00:08:38,199 --> 00:08:42,789
access to data so you could control what

00:08:40,930 --> 00:08:44,529
users have access to what parts of a

00:08:42,789 --> 00:08:51,190
flow what they could do to that flow

00:08:44,529 --> 00:08:52,630
what they get seeing the flow back

00:08:51,190 --> 00:08:55,060
pressure you can figure on this

00:08:52,630 --> 00:08:56,920
connection so if you right-click you'd

00:08:55,060 --> 00:08:58,500
see a dialog that comes up where it

00:08:56,920 --> 00:09:01,470
shows back pressure that object

00:08:58,500 --> 00:09:03,450
hold in that little picture is part of a

00:09:01,470 --> 00:09:05,310
screenshot from looking at that this

00:09:03,450 --> 00:09:08,430
allows you to do things to configure

00:09:05,310 --> 00:09:10,080
back pressure for the next processor so

00:09:08,430 --> 00:09:12,450
it's at every connection in between

00:09:10,080 --> 00:09:14,790
processors you can configure the back

00:09:12,450 --> 00:09:17,460
pressure based upon the number of flow

00:09:14,790 --> 00:09:20,010
files or the size which whichever one

00:09:17,460 --> 00:09:22,290
comes first so you can decide that in

00:09:20,010 --> 00:09:24,240
this case this log attribute has a

00:09:22,290 --> 00:09:28,050
default of it's ten thousand flow files

00:09:24,240 --> 00:09:30,150
or one gig in size if that connection

00:09:28,050 --> 00:09:33,120
builds up to have the ten thousand flow

00:09:30,150 --> 00:09:35,270
files or one gig the upstream processor

00:09:33,120 --> 00:09:37,560
knife I will stop scheduling it to run

00:09:35,270 --> 00:09:39,270
so will automatically stop it from

00:09:37,560 --> 00:09:41,310
running until that other processor keeps

00:09:39,270 --> 00:09:43,290
up in this case that processor is

00:09:41,310 --> 00:09:44,910
stopped so it's not going to do anything

00:09:43,290 --> 00:09:47,100
but you'll see as we kind of walk

00:09:44,910 --> 00:09:49,560
through a flow that you could stop a

00:09:47,100 --> 00:09:51,270
processor make changes that you need to

00:09:49,560 --> 00:09:52,620
change perhaps it's the regular

00:09:51,270 --> 00:09:54,330
expression you're using or you're

00:09:52,620 --> 00:09:56,160
evolving a JSON path you're doing

00:09:54,330 --> 00:09:58,020
something with it you get a chance to

00:09:56,160 --> 00:10:00,120
stop it make whatever changes you need

00:09:58,020 --> 00:10:02,580
start again and then the data start

00:10:00,120 --> 00:10:05,820
flowing and you get to do this at every

00:10:02,580 --> 00:10:07,080
processor all the way through get to

00:10:05,820 --> 00:10:12,480
make lots of different decisions and

00:10:07,080 --> 00:10:13,860
decide what you want to do it mentioned

00:10:12,480 --> 00:10:16,890
a little bit ago about prioritization

00:10:13,860 --> 00:10:18,420
you again sometimes not all the data is

00:10:16,890 --> 00:10:20,010
the same right if you're getting data

00:10:18,420 --> 00:10:22,080
that's coming off of a sensor and

00:10:20,010 --> 00:10:23,820
perhaps it has to be coming off of a

00:10:22,080 --> 00:10:26,060
heart rate monitor and you're trying to

00:10:23,820 --> 00:10:28,860
understand the health of somebody

00:10:26,060 --> 00:10:30,630
certain patterns of measurements coming

00:10:28,860 --> 00:10:32,400
off are going to be more important and

00:10:30,630 --> 00:10:34,650
perhaps signify that the person is not

00:10:32,400 --> 00:10:36,330
well than just normal activity or

00:10:34,650 --> 00:10:38,940
they're sleeping so you get to

00:10:36,330 --> 00:10:42,660
prioritize based upon either first-in

00:10:38,940 --> 00:10:45,300
first-out so size or the data or an

00:10:42,660 --> 00:10:47,580
attribute on that price on that piece of

00:10:45,300 --> 00:10:49,410
data that's important to you so you can

00:10:47,580 --> 00:10:51,870
decide that if you're looking at a value

00:10:49,410 --> 00:10:54,180
that is heart rate that that is more

00:10:51,870 --> 00:10:56,730
important than any other data in that

00:10:54,180 --> 00:11:00,480
packet and if the value is above a

00:10:56,730 --> 00:11:02,640
threshold and that has priority and if

00:11:00,480 --> 00:11:04,680
one of these prioritizes doesn't meet

00:11:02,640 --> 00:11:08,420
your needs and knife is completely

00:11:04,680 --> 00:11:08,420
extensible and you can add their own

00:11:11,110 --> 00:11:17,120
the latency first throughput again this

00:11:14,660 --> 00:11:18,819
is on a per processor basis and it

00:11:17,120 --> 00:11:21,709
allows you to make decisions and really

00:11:18,819 --> 00:11:24,079
give knife I a suggestion if you will as

00:11:21,709 --> 00:11:26,660
to whether that processor whether you

00:11:24,079 --> 00:11:28,310
prefer to have lower latency in which

00:11:26,660 --> 00:11:30,110
case every time there's a flow file in a

00:11:28,310 --> 00:11:32,089
connection it's going to get a chance to

00:11:30,110 --> 00:11:34,579
execute against that or if you prefer

00:11:32,089 --> 00:11:36,050
higher throughput in which case it's

00:11:34,579 --> 00:11:39,170
going to treat it more as like a batch

00:11:36,050 --> 00:11:41,480
and if you move that slider close to say

00:11:39,170 --> 00:11:43,009
the 1 second or two second it's not a

00:11:41,480 --> 00:11:44,810
guarantee it's going to happen at 1

00:11:43,009 --> 00:11:46,670
second to second and it's really going

00:11:44,810 --> 00:11:47,449
to be the number of flow files that are

00:11:46,670 --> 00:11:49,670
in that connection

00:11:47,449 --> 00:11:51,110
at about that amount of time and it's

00:11:49,670 --> 00:11:52,940
going to schedule that processor to run

00:11:51,110 --> 00:11:55,339
so in that case that processor is going

00:11:52,940 --> 00:11:57,860
to get a batch of flow files to operate

00:11:55,339 --> 00:11:59,870
on whereas if you skewed it towards the

00:11:57,860 --> 00:12:01,490
lower latency then it's going to run

00:11:59,870 --> 00:12:03,170
every single time there's a flow file in

00:12:01,490 --> 00:12:06,380
the connection so you get to make these

00:12:03,170 --> 00:12:08,630
choices at every processor level and you

00:12:06,380 --> 00:12:10,040
get to and how that goes and see other

00:12:08,630 --> 00:12:12,380
things too as far as like a scheduling

00:12:10,040 --> 00:12:14,540
strategy the concurrent task you can

00:12:12,380 --> 00:12:16,459
think about those really as if you will

00:12:14,540 --> 00:12:23,839
the threads that knife I will run in

00:12:16,459 --> 00:12:27,410
parallel for a given processor when you

00:12:23,839 --> 00:12:29,329
look at the extension integration pretty

00:12:27,410 --> 00:12:31,730
much everything that you see so the flow

00:12:29,329 --> 00:12:34,279
files you can build your own there's a

00:12:31,730 --> 00:12:36,589
maven archetype to get people started to

00:12:34,279 --> 00:12:39,050
build your own processor there's a

00:12:36,589 --> 00:12:40,100
notion of this reporting task when I

00:12:39,050 --> 00:12:42,649
mentioned getting the provenance

00:12:40,100 --> 00:12:44,300
information out of 9th I so there is oh

00:12:42,649 --> 00:12:46,310
it's called a site-to-site report

00:12:44,300 --> 00:12:48,410
provenance reporting task which just

00:12:46,310 --> 00:12:50,990
runs and will get data to another system

00:12:48,410 --> 00:12:54,380
soft in time to use reporting tasks for

00:12:50,990 --> 00:12:56,420
that there's a controller service notion

00:12:54,380 --> 00:12:58,009
of that really is much like a JDBC

00:12:56,420 --> 00:13:00,740
connection or any sort of connection

00:12:58,009 --> 00:13:03,860
pool or a connection pool to hive or to

00:13:00,740 --> 00:13:05,329
other expensive resources so you see the

00:13:03,860 --> 00:13:06,290
connections serve as a controller

00:13:05,329 --> 00:13:09,680
service being something that you're

00:13:06,290 --> 00:13:11,959
going to reuse across processors and the

00:13:09,680 --> 00:13:13,370
REST API everything that you saw in the

00:13:11,959 --> 00:13:15,560
UI and the stuff that we'll see in the

00:13:13,370 --> 00:13:17,810
UI and the demo it everything that the

00:13:15,560 --> 00:13:20,059
UI is doing is doing through a REST API

00:13:17,810 --> 00:13:21,500
that knife that exposes so anything that

00:13:20,059 --> 00:13:23,830
could be done in the UI and you could do

00:13:21,500 --> 00:13:27,000
via a REST API so often

00:13:23,830 --> 00:13:31,590
times people want to automate our tu-95

00:13:27,000 --> 00:13:31,590
so you could use the REST API to do that

00:13:33,120 --> 00:13:43,960
so something to keep in mind that is

00:13:36,070 --> 00:13:46,900
really just how 9/5 fits in I think

00:13:43,960 --> 00:13:49,330
often times as humans we want to put

00:13:46,900 --> 00:13:50,890
things into a box based upon our frame

00:13:49,330 --> 00:13:54,310
of reference and where we're coming from

00:13:50,890 --> 00:13:57,190
ix is no different oftentimes depending

00:13:54,310 --> 00:13:59,410
upon people's background and kind of

00:13:57,190 --> 00:14:01,210
what they work on in their environment 9

00:13:59,410 --> 00:14:05,880
side gets usually put into one of these

00:14:01,210 --> 00:14:10,060
four bubbles almost always some of it is

00:14:05,880 --> 00:14:11,710
definitely only confused by some of the

00:14:10,060 --> 00:14:13,300
terminology that's use and some of the

00:14:11,710 --> 00:14:15,670
processors are built that kind of

00:14:13,300 --> 00:14:18,000
continue to help muddy the water but it

00:14:15,670 --> 00:14:21,610
really kind of gets put into these it's

00:14:18,000 --> 00:14:23,650
not any one of those it really has bits

00:14:21,610 --> 00:14:24,940
and pieces of each of them does this

00:14:23,650 --> 00:14:27,250
take a moment to kind of walk through

00:14:24,940 --> 00:14:29,470
each of these and just kind of give you

00:14:27,250 --> 00:14:32,590
some ideas of how to think of nya Phi's

00:14:29,470 --> 00:14:33,850
it relates to those technologies so if

00:14:32,590 --> 00:14:35,710
you look at like the processing

00:14:33,850 --> 00:14:38,020
frameworks right you for a lot of stuff

00:14:35,710 --> 00:14:40,060
talk about flink you know the last two

00:14:38,020 --> 00:14:42,040
days about kafka streams hear people

00:14:40,060 --> 00:14:45,340
talk about SPARC those systems are

00:14:42,040 --> 00:14:47,350
fantastic for analyzing complex

00:14:45,340 --> 00:14:49,390
relationships doing complex event

00:14:47,350 --> 00:14:51,580
processing looking at data over windows

00:14:49,390 --> 00:14:54,150
at time joining streams the updated

00:14:51,580 --> 00:14:57,100
together and doing a lot of complex work

00:14:54,150 --> 00:14:59,740
knife-like doesn't do that it doesn't

00:14:57,100 --> 00:15:01,570
understand how to work across windows a

00:14:59,740 --> 00:15:03,940
time doesn't understand how to join

00:15:01,570 --> 00:15:06,070
streams of data together it's really

00:15:03,940 --> 00:15:07,480
working towards that flow so if you

00:15:06,070 --> 00:15:09,550
looked at it in an environment where

00:15:07,480 --> 00:15:11,890
you're using spark streaming or kafka

00:15:09,550 --> 00:15:13,480
streams or flink you look at knife eye

00:15:11,890 --> 00:15:16,780
as being that tool that you could use to

00:15:13,480 --> 00:15:19,210
get the data from a source to clean the

00:15:16,780 --> 00:15:22,120
data massaged it transform it really get

00:15:19,210 --> 00:15:24,850
it ready for something like link or

00:15:22,120 --> 00:15:27,010
spark or kafka streams and so instead of

00:15:24,850 --> 00:15:30,250
doing any sort of transformation logic

00:15:27,010 --> 00:15:32,920
and data cleansing if you will in a tool

00:15:30,250 --> 00:15:36,090
that's designed for analysis knife I

00:15:32,920 --> 00:15:36,090
really excels in that space

00:15:38,370 --> 00:15:43,630
knife I compared to like message buses

00:15:41,140 --> 00:15:45,640
and things like Kafka the Kafka in the

00:15:43,630 --> 00:15:47,500
related tools your fantastic job saying

00:15:45,640 --> 00:15:49,450
give me as much data as you can as fast

00:15:47,500 --> 00:15:51,820
as you can I'll hold onto it and give it

00:15:49,450 --> 00:15:54,220
back when you ask for it but that is all

00:15:51,820 --> 00:15:56,830
a black box right you produce a byte

00:15:54,220 --> 00:15:59,020
array into Kafka and you get back out

00:15:56,830 --> 00:16:01,180
this payload now if I could think of us

00:15:59,020 --> 00:16:03,970
when it comes in its what you want to do

00:16:01,180 --> 00:16:05,590
with that data once it arrives until you

00:16:03,970 --> 00:16:07,630
deliver it so it's really everything

00:16:05,590 --> 00:16:14,290
that's inside that's happening and it's

00:16:07,630 --> 00:16:17,320
how you handle this data flow when you

00:16:14,290 --> 00:16:20,500
look at ingestion frameworks like spring

00:16:17,320 --> 00:16:23,380
or flume camel a lot of those work great

00:16:20,500 --> 00:16:25,270
at orchestrating a workflow they're very

00:16:23,380 --> 00:16:28,240
developer oriented tools you have to

00:16:25,270 --> 00:16:29,980
build everything else around it ix is

00:16:28,240 --> 00:16:32,650
not available is just a jar that you

00:16:29,980 --> 00:16:35,080
could include in an application you add

00:16:32,650 --> 00:16:37,540
things to it you use it to manage a data

00:16:35,080 --> 00:16:39,400
flow and it provides all that tooling

00:16:37,540 --> 00:16:41,950
around it and when you look at it the

00:16:39,400 --> 00:16:44,110
first audience 4/9 is really the data

00:16:41,950 --> 00:16:46,060
flow manager and it's allowing someone

00:16:44,110 --> 00:16:48,490
in the enterprise that understands the

00:16:46,060 --> 00:16:50,590
flow of data at an infrastructure level

00:16:48,490 --> 00:16:52,660
and just the overall flow for them to

00:16:50,590 --> 00:16:54,850
manage it and to manage the flow of data

00:16:52,660 --> 00:16:57,190
without you having to write code or

00:16:54,850 --> 00:16:59,050
change code because something changed a

00:16:57,190 --> 00:17:00,670
different database is no longer

00:16:59,050 --> 00:17:02,080
available because it's there's

00:17:00,670 --> 00:17:03,760
maintenance going on how do you switch

00:17:02,080 --> 00:17:05,680
things in or out how do you stop the

00:17:03,760 --> 00:17:07,930
flow of data I do understand what

00:17:05,680 --> 00:17:11,470
happened to the data knife I really

00:17:07,930 --> 00:17:13,660
works in that space whereas in spring or

00:17:11,470 --> 00:17:18,180
flume or any of the tools like that

00:17:13,660 --> 00:17:18,180
you're left having to do all that work

00:17:20,010 --> 00:17:25,180
when you look at it compared to the ETL

00:17:22,420 --> 00:17:28,570
tools again we end up using the term

00:17:25,180 --> 00:17:30,790
like transforming data with knife I only

00:17:28,570 --> 00:17:32,560
leads to more problems and now the

00:17:30,790 --> 00:17:35,680
latest version 9 file is kind of a a

00:17:32,560 --> 00:17:37,990
poor man CDC if you own some other ETL

00:17:35,680 --> 00:17:40,470
stuff but it's really thinking about

00:17:37,990 --> 00:17:43,270
knife I as it's great at dealing with

00:17:40,470 --> 00:17:45,340
events and dealing with rows of data

00:17:43,270 --> 00:17:47,740
there's new processors that allow you to

00:17:45,340 --> 00:17:50,740
deal with many rows of data and to

00:17:47,740 --> 00:17:53,080
sequel queries over them but it unlike

00:17:50,740 --> 00:17:55,420
an ETL tool that's used to a warehouse

00:17:53,080 --> 00:17:58,570
or any data system is connecting to that

00:17:55,420 --> 00:18:00,190
has a schema ingesting all that data now

00:17:58,570 --> 00:18:02,230
if I could connect to databases that a

00:18:00,190 --> 00:18:04,660
problem and ingest it but you're not

00:18:02,230 --> 00:18:06,610
going to be able to say select all the

00:18:04,660 --> 00:18:09,130
rows from a customer table all the rows

00:18:06,610 --> 00:18:11,170
from an order table and join them or do

00:18:09,130 --> 00:18:13,570
some table like operations on them in

00:18:11,170 --> 00:18:15,760
masse now if I doesn't have that type of

00:18:13,570 --> 00:18:18,010
ability it's not designed in that ETL

00:18:15,760 --> 00:18:21,160
world you'll see some processors again

00:18:18,010 --> 00:18:24,280
that kind of somewhat start to bleed and

00:18:21,160 --> 00:18:26,110
blur that line more but it will never be

00:18:24,280 --> 00:18:28,360
something that you would use as a

00:18:26,110 --> 00:18:29,500
complete replacement for informatica if

00:18:28,360 --> 00:18:32,110
you're doing really heavy

00:18:29,500 --> 00:18:34,300
ETL working informatica knife is not

00:18:32,110 --> 00:18:36,820
that tool in the same breath though

00:18:34,300 --> 00:18:38,770
there's folks that use knife I to do

00:18:36,820 --> 00:18:42,250
image recognition and facial recognition

00:18:38,770 --> 00:18:44,830
images and ingest image data and then

00:18:42,250 --> 00:18:47,260
run like OpenCV against those images in

00:18:44,830 --> 00:18:49,330
a flow and that same flow you could then

00:18:47,260 --> 00:18:51,880
analyze Twitter messages you can connect

00:18:49,330 --> 00:18:53,620
to a database you connect to s3 as your

00:18:51,880 --> 00:18:57,070
to all these different sources of data

00:18:53,620 --> 00:18:58,660
and have them all go through one flow so

00:18:57,070 --> 00:19:01,059
you could operate on different types of

00:18:58,660 --> 00:19:07,120
data which is quite different than what

00:19:01,059 --> 00:19:10,150
you'll see in all the ETL tools so if we

00:19:07,120 --> 00:19:19,030
look at it minify and how this plays

00:19:10,150 --> 00:19:23,200
into things minify is a sub-project to

00:19:19,030 --> 00:19:25,750
knife i and really have this goal of zen

00:19:23,200 --> 00:19:27,910
let me get the key parts of knife i and

00:19:25,750 --> 00:19:31,929
move them as close to the edge as

00:19:27,910 --> 00:19:34,270
possible so knife is very comfortable to

00:19:31,929 --> 00:19:36,400
run in the data center and assumes that

00:19:34,270 --> 00:19:39,970
a machine is running on that it has full

00:19:36,400 --> 00:19:42,880
use of a CPU of the RAM of the i/o of

00:19:39,970 --> 00:19:45,910
everything that's there knife minify is

00:19:42,880 --> 00:19:49,900
designed much more as I'm an agent and a

00:19:45,910 --> 00:19:51,780
guest on an OS and how can you then take

00:19:49,900 --> 00:19:54,160
these same types of capabilities and

00:19:51,780 --> 00:19:55,540
move them all the way out how do you put

00:19:54,160 --> 00:19:57,340
them on a car how do you put them on an

00:19:55,540 --> 00:19:59,470
oil rig how do you embed them in a cash

00:19:57,340 --> 00:20:00,280
register how do you push this type of

00:19:59,470 --> 00:20:02,050
the

00:20:00,280 --> 00:20:08,140
Providence secured in everything all the

00:20:02,050 --> 00:20:10,360
way to the edge when you start to think

00:20:08,140 --> 00:20:11,710
through that and think through how do

00:20:10,360 --> 00:20:13,660
you move this code that's used to

00:20:11,710 --> 00:20:15,610
running in a data center and what the

00:20:13,660 --> 00:20:18,070
realities are of running out of the data

00:20:15,610 --> 00:20:21,220
center there's you know a couple of key

00:20:18,070 --> 00:20:23,290
things that come out you know obviously

00:20:21,220 --> 00:20:24,670
there's limited compute capabilities

00:20:23,290 --> 00:20:27,670
depending upon the hardware you're

00:20:24,670 --> 00:20:31,020
running on restricted software does

00:20:27,670 --> 00:20:34,000
often no UI scalability concerns

00:20:31,020 --> 00:20:35,560
security becomes a concern as well so

00:20:34,000 --> 00:20:37,420
you have a lot of these same things a

00:20:35,560 --> 00:20:40,000
lot of the other features you'd see but

00:20:37,420 --> 00:20:43,990
you got a lot of realities of what

00:20:40,000 --> 00:20:45,930
computing at the edge is like when you

00:20:43,990 --> 00:20:49,720
look at the features that minify has

00:20:45,930 --> 00:20:52,180
they're almost identical to knife I so

00:20:49,720 --> 00:20:54,340
guaranteed delivery same thing from the

00:20:52,180 --> 00:20:57,340
point of ingesting data of whether it's

00:20:54,340 --> 00:21:00,340
on a device somewhere to it being

00:20:57,340 --> 00:21:02,740
delivered is guaranteed the idea of data

00:21:00,340 --> 00:21:04,240
provenance so now you can understand

00:21:02,740 --> 00:21:06,310
what's happening to that piece of data

00:21:04,240 --> 00:21:09,190
from the time it's being generated all

00:21:06,310 --> 00:21:11,710
the way back being able to have back

00:21:09,190 --> 00:21:14,050
pressure and the pressure release the

00:21:11,710 --> 00:21:16,630
prioritization of data are these things

00:21:14,050 --> 00:21:19,450
things the key differences that you'll

00:21:16,630 --> 00:21:21,580
see with minify is that unlike knife I

00:21:19,450 --> 00:21:22,990
where you're pretty much molding clay

00:21:21,580 --> 00:21:25,480
and it's this command and control

00:21:22,990 --> 00:21:28,480
environment with minify it's very much a

00:21:25,480 --> 00:21:30,910
design and deploy so you would design

00:21:28,480 --> 00:21:35,590
the flow in knife i and then deploy it

00:21:30,910 --> 00:21:37,090
as a yamo file to minify with that

00:21:35,590 --> 00:21:38,980
there's also this notion of having warm

00:21:37,090 --> 00:21:41,380
readable three deploys so as you change

00:21:38,980 --> 00:21:43,240
the configuration and you push it to

00:21:41,380 --> 00:21:45,340
minify then you can have it kind of do a

00:21:43,240 --> 00:21:47,800
warm redeploy of that similar to

00:21:45,340 --> 00:21:49,540
producing a war file that runs in a day

00:21:47,800 --> 00:21:51,460
2 ee app server where it automatically

00:21:49,540 --> 00:21:57,460
just warm restarts similar type of

00:21:51,460 --> 00:21:59,950
concept and minify there are some things

00:21:57,460 --> 00:22:02,650
that take precedent from 9/5 that you'll

00:21:59,950 --> 00:22:05,320
see it minify the notion of site to site

00:22:02,650 --> 00:22:07,660
so psyched the site's a protocol that

00:22:05,320 --> 00:22:11,410
was developed for knife I it's available

00:22:07,660 --> 00:22:13,389
as a Java client library it's used in

00:22:11,410 --> 00:22:16,570
between nine and nine find

00:22:13,389 --> 00:22:19,089
census is used between minified and IFI

00:22:16,570 --> 00:22:23,019
can be used by anything that runs on the

00:22:19,089 --> 00:22:25,959
JVM and also now in C++ you'll see

00:22:23,019 --> 00:22:28,239
implementations and work that's been

00:22:25,959 --> 00:22:30,700
done to use site-to-site to integrate

00:22:28,239 --> 00:22:33,429
say knife I with flank or knife I would

00:22:30,700 --> 00:22:35,739
spark or storm or other systems usually

00:22:33,429 --> 00:22:37,509
not the best idea from a production

00:22:35,739 --> 00:22:39,279
standpoint you usually want something in

00:22:37,509 --> 00:22:41,349
the middle as a buffer not tie those two

00:22:39,279 --> 00:22:43,450
systems together but you could do it so

00:22:41,349 --> 00:22:45,909
if you had an idea of something that you

00:22:43,450 --> 00:22:47,820
want to build to communicate with 9/5

00:22:45,909 --> 00:22:49,599
whether to push data in or get data out

00:22:47,820 --> 00:22:56,559
then you could use a site-to-site

00:22:49,599 --> 00:22:59,109
library the providence information that

00:22:56,559 --> 00:23:02,139
again is the same type of idea that you

00:22:59,109 --> 00:23:04,690
see in knife I and you get fine-grain

00:23:02,139 --> 00:23:06,700
event level access so when something is

00:23:04,690 --> 00:23:09,190
created when attributes are modified

00:23:06,700 --> 00:23:12,119
when it's viewed when it's downloaded

00:23:09,190 --> 00:23:15,039
what's done to it everything is recorded

00:23:12,119 --> 00:23:17,169
all the Associated attributes and

00:23:15,039 --> 00:23:20,919
metadata about the event recorded as

00:23:17,169 --> 00:23:23,619
well so now you can end up with a map of

00:23:20,919 --> 00:23:25,629
this flow files journey from a device

00:23:23,619 --> 00:23:28,119
where it was the data was originated all

00:23:25,629 --> 00:23:30,279
the way back to knife I in a data center

00:23:28,119 --> 00:23:35,589
so you get to see a whole lifecycle of

00:23:30,279 --> 00:23:38,019
what happened to this some other stuff

00:23:35,589 --> 00:23:40,809
that just shows how you would look at

00:23:38,019 --> 00:23:42,969
that provenance event and where it

00:23:40,809 --> 00:23:46,839
starts to differ is trying to get the

00:23:42,969 --> 00:23:51,159
fit right so knife is built in Java runs

00:23:46,839 --> 00:23:53,440
on the JVM minify has to try and work in

00:23:51,159 --> 00:23:56,070
that same way so you'll see things that

00:23:53,440 --> 00:23:59,079
are a little bit smaller there's no UI

00:23:56,070 --> 00:24:01,479
so it's all just generated off of logs

00:23:59,079 --> 00:24:04,570
that the configuration is declarative

00:24:01,479 --> 00:24:09,639
using llamó and then a reduced set of

00:24:04,570 --> 00:24:11,829
bundle components so the scoping of it

00:24:09,639 --> 00:24:14,349
there's minified Java which is much

00:24:11,829 --> 00:24:16,209
smaller has a reduced set of components

00:24:14,349 --> 00:24:19,329
all the components that work with knife

00:24:16,209 --> 00:24:21,519
I will work with vinify Java you

00:24:19,329 --> 00:24:23,499
probably wouldn't want to though just

00:24:21,519 --> 00:24:27,160
because of the size and then there's

00:24:23,499 --> 00:24:28,720
minify C++ that's again smaller and

00:24:27,160 --> 00:24:31,570
this idea of making it smaller and

00:24:28,720 --> 00:24:33,880
writing it for particular processors in

00:24:31,570 --> 00:24:38,710
particular libraries and languages like

00:24:33,880 --> 00:24:40,720
iOS or Android so if we put the whole

00:24:38,710 --> 00:24:42,250
thing together of where it would fit so

00:24:40,720 --> 00:24:44,590
you have like all the way the edge is

00:24:42,250 --> 00:24:46,540
constrained high latency localized

00:24:44,590 --> 00:24:48,310
content environment you got stuff in the

00:24:46,540 --> 00:24:50,740
middle so you have these sources of data

00:24:48,310 --> 00:24:53,680
got to cut this regional infrastructure

00:24:50,740 --> 00:24:55,270
then you've got this core oftentimes you

00:24:53,680 --> 00:24:57,610
see like nine file in the core and

00:24:55,270 --> 00:25:00,730
either nine fire minify at the edge and

00:24:57,610 --> 00:25:03,430
it really that line gets blurred as far

00:25:00,730 --> 00:25:05,110
as where it makes sense in some cases

00:25:03,430 --> 00:25:08,080
this constrained device and only at the

00:25:05,110 --> 00:25:10,180
edge people still put minify or put a

00:25:08,080 --> 00:25:11,230
knife I and then in other cases it just

00:25:10,180 --> 00:25:13,360
doesn't make sense

00:25:11,230 --> 00:25:15,490
and minify makes more sense and you can

00:25:13,360 --> 00:25:19,720
kind of bleed back and forth as to where

00:25:15,490 --> 00:25:25,570
that goes all right so let's take a

00:25:19,720 --> 00:25:29,640
moment and kind of look at it running so

00:25:25,570 --> 00:25:29,640
let me just change my display here

00:25:36,660 --> 00:25:46,930
okay still visible okay so this I have

00:25:45,460 --> 00:25:48,910
one flow if we just looked at this

00:25:46,930 --> 00:25:51,430
quickly you know so this across board is

00:25:48,910 --> 00:25:54,250
the processors notice of input/output

00:25:51,430 --> 00:25:56,260
ports a process a group a remote

00:25:54,250 --> 00:25:58,480
processor group which that would be

00:25:56,260 --> 00:26:00,820
another instance of knife I that's

00:25:58,480 --> 00:26:03,220
running site-to-site or an application

00:26:00,820 --> 00:26:06,010
that you may have this idea of funneling

00:26:03,220 --> 00:26:07,360
templates and you can record stuff it

00:26:06,010 --> 00:26:09,040
just take notes so if you were to

00:26:07,360 --> 00:26:11,850
drag-and-drop you literally just drag

00:26:09,040 --> 00:26:16,960
and drop and pick a processor so this is

00:26:11,850 --> 00:26:18,490
1.2 release of knife I a 1.3 just was

00:26:16,960 --> 00:26:19,840
released the other day I don't know if

00:26:18,490 --> 00:26:21,820
there's more processors at it but

00:26:19,840 --> 00:26:24,850
there's somewhere around you know 219

00:26:21,820 --> 00:26:26,620
220 processors pretty much all sorts of

00:26:24,850 --> 00:26:29,590
things you want to do alright so

00:26:26,620 --> 00:26:33,100
everything from my sequel Kafka MQTT

00:26:29,590 --> 00:26:35,020
pop3 Windows Event log comes the stuff

00:26:33,100 --> 00:26:36,310
with Jason o RC so you can just keep

00:26:35,020 --> 00:26:38,350
cruising through of all these different

00:26:36,310 --> 00:26:41,260
things you want to do there's stuff to

00:26:38,350 --> 00:26:44,770
execute script as interesting if you

00:26:41,260 --> 00:26:49,090
were wanting to use that it supports

00:26:44,770 --> 00:26:51,100
Python groovy Lua JavaScript sometimes

00:26:49,090 --> 00:26:52,930
people will use them to have an idea as

00:26:51,100 --> 00:26:54,520
to how they want to handle data before

00:26:52,930 --> 00:26:58,000
investing in writing a processor

00:26:54,520 --> 00:27:00,040
sometimes writing it in Python or in

00:26:58,000 --> 00:27:04,180
JavaScript is good enough and you don't

00:27:00,040 --> 00:27:06,760
need another processor so this flow

00:27:04,180 --> 00:27:11,430
right here connects to the meetup API

00:27:06,760 --> 00:27:11,430
streaming API so if I were to run this

00:27:11,730 --> 00:27:17,590
you can see this is connecting over

00:27:14,520 --> 00:27:18,880
WebSockets to go and get data if I stop

00:27:17,590 --> 00:27:20,350
this here and so you know I don't just

00:27:18,880 --> 00:27:24,490
you know want to see what this looks

00:27:20,350 --> 00:27:26,440
like make it stop and you'll see if this

00:27:24,490 --> 00:27:28,600
kind of builds up it starts the buffer I

00:27:26,440 --> 00:27:31,120
could go here and I could list this and

00:27:28,600 --> 00:27:33,940
see what's there so now I can see here's

00:27:31,120 --> 00:27:36,430
what the data looks like here's the

00:27:33,940 --> 00:27:39,640
details about it here's the attributes

00:27:36,430 --> 00:27:42,280
that came in and these attributes

00:27:39,640 --> 00:27:45,210
continue to build up as you go through

00:27:42,280 --> 00:27:48,539
the flow if I then went and viewed it

00:27:45,210 --> 00:27:51,239
I can then view this content so this is

00:27:48,539 --> 00:27:52,440
a JSON I could view what it is and so I

00:27:51,239 --> 00:27:54,479
could go through and just start working

00:27:52,440 --> 00:28:01,379
through this and figure out what I want

00:27:54,479 --> 00:28:03,509
to do in this case this flow it's going

00:28:01,379 --> 00:28:05,700
to evaluate this JSON path I can look

00:28:03,509 --> 00:28:07,859
and see what this is and it's going to

00:28:05,700 --> 00:28:09,690
go through and from that JSON it's going

00:28:07,859 --> 00:28:11,489
to look at dollar being the route get

00:28:09,690 --> 00:28:13,739
event that event name and it's going to

00:28:11,489 --> 00:28:16,169
start plucking this out right and start

00:28:13,739 --> 00:28:20,009
filling these attributes with the data

00:28:16,169 --> 00:28:21,419
that defines here so if we look at the

00:28:20,009 --> 00:28:23,759
data provenance you can see where this

00:28:21,419 --> 00:28:26,940
has run and the data that's flowed

00:28:23,759 --> 00:28:28,440
through here of what's happened so now

00:28:26,940 --> 00:28:31,710
you can see that okay there were

00:28:28,440 --> 00:28:35,879
attributes modified here so now I could

00:28:31,710 --> 00:28:38,789
see there's a details there's the

00:28:35,879 --> 00:28:40,379
attributes so this is no value set when

00:28:38,789 --> 00:28:42,269
the data was coming in there was no

00:28:40,379 --> 00:28:44,249
value we set this attribute in this

00:28:42,269 --> 00:28:46,469
processor so you can see everything that

00:28:44,249 --> 00:28:48,450
went on you can then go and look at the

00:28:46,469 --> 00:28:50,549
content if you got something wrong in

00:28:48,450 --> 00:28:52,469
that mapping you can look at the input

00:28:50,549 --> 00:28:54,719
claim what the flow file look like

00:28:52,469 --> 00:28:57,119
coming in I think you look at the output

00:28:54,719 --> 00:28:58,710
as to what you did to it going out so

00:28:57,119 --> 00:29:00,330
you can see the before and after of

00:28:58,710 --> 00:29:02,609
everything that happened you could then

00:29:00,330 --> 00:29:04,169
replay this data if there's something

00:29:02,609 --> 00:29:06,269
that you fixed and you want to change it

00:29:04,169 --> 00:29:08,489
so you can see all this that's going on

00:29:06,269 --> 00:29:16,289
again all this data you could get out of

00:29:08,489 --> 00:29:18,149
nine five to another system as well so

00:29:16,289 --> 00:29:23,999
let me just show you another part of a

00:29:18,149 --> 00:29:26,429
flow so this one here I have minified

00:29:23,999 --> 00:29:28,019
running on this Raspberry Pi that's

00:29:26,429 --> 00:29:30,419
connected to my laptop and then I have

00:29:28,019 --> 00:29:31,950
this Texas Instruments sensor tag which

00:29:30,419 --> 00:29:33,299
has like a dozen different sensors on

00:29:31,950 --> 00:29:36,659
this thing you get them for like fifteen

00:29:33,299 --> 00:29:38,909
dollars right so I have minified running

00:29:36,659 --> 00:29:44,339
on its Raspberry Pi I got the sensor

00:29:38,909 --> 00:29:46,309
here if we start this thing running you

00:29:44,339 --> 00:29:48,749
see that I'll start moving data across

00:29:46,309 --> 00:29:51,419
you go over here let me see if this is

00:29:48,749 --> 00:29:54,169
my Raspberry Pi let me go ahead and

00:29:51,419 --> 00:29:54,169
start minify

00:30:01,650 --> 00:30:05,270
okay so that started up

00:30:13,190 --> 00:30:22,250
guys little no jsf oak of course helps

00:30:18,980 --> 00:30:24,769
up in the right place right that this is

00:30:22,250 --> 00:30:27,110
going to connect to this over Bluetooth

00:30:24,769 --> 00:30:28,549
Low Energy start getting data if you

00:30:27,110 --> 00:30:30,350
have this on something and someone

00:30:28,549 --> 00:30:31,159
starts to knock the package around or

00:30:30,350 --> 00:30:33,860
whatever it is

00:30:31,159 --> 00:30:38,960
drops it you can see this is getting

00:30:33,860 --> 00:30:40,789
stuff from drop it and make it fail you

00:30:38,960 --> 00:30:43,009
see it's getting stuff coming off of a

00:30:40,789 --> 00:30:45,080
gyroscope that's in here if you then

00:30:43,009 --> 00:30:47,659
went and looked at minify it looked at

00:30:45,080 --> 00:30:49,730
nine five this data is getting sent you

00:30:47,659 --> 00:30:53,620
know in this case is just logging it so

00:30:49,730 --> 00:31:01,909
if we stop this here stop one of these

00:30:53,620 --> 00:31:04,340
let's see if i could get it to lets them

00:31:01,909 --> 00:31:06,110
know we got data this coming through and

00:31:04,340 --> 00:31:08,269
this ends up being like x one sets you

00:31:06,110 --> 00:31:15,649
get the roll pitch and the yaw of what's

00:31:08,269 --> 00:31:19,340
coming off of the gyroscope and you'd

00:31:15,649 --> 00:31:20,960
see data that's flowing through but you

00:31:19,340 --> 00:31:22,549
could run it in all different things you

00:31:20,960 --> 00:31:25,399
could do a bunch of stuff with it so

00:31:22,549 --> 00:31:26,750
this does not have minify on it but it'd

00:31:25,399 --> 00:31:28,549
be an idea of how can you do it and

00:31:26,750 --> 00:31:31,129
oftentimes something like this doesn't

00:31:28,549 --> 00:31:33,049
have a full network stack so oftentimes

00:31:31,129 --> 00:31:35,299
like an IOT setup you see people that

00:31:33,049 --> 00:31:35,899
have something could be a pie or

00:31:35,299 --> 00:31:37,580
something else

00:31:35,899 --> 00:31:40,210
that would run as an IOT gateway that's

00:31:37,580 --> 00:31:43,460
connecting to sensors getting data

00:31:40,210 --> 00:31:45,379
gathering data on that sensor and from

00:31:43,460 --> 00:31:47,570
that IOT gateway and then sending it to

00:31:45,379 --> 00:31:49,820
minify or deny phi so in this case

00:31:47,570 --> 00:31:51,379
minified running on the pi that data

00:31:49,820 --> 00:31:53,750
provenance is from the point that it's

00:31:51,379 --> 00:31:55,879
ingesting this on the PI all the way

00:31:53,750 --> 00:31:57,590
back so then if I were to deliver that

00:31:55,879 --> 00:32:00,379
data into Kafka you'd have guaranteed

00:31:57,590 --> 00:32:02,240
delivery from the PI all the way to when

00:32:00,379 --> 00:32:04,549
the data was put into Kafka and you'd

00:32:02,240 --> 00:32:06,320
understand the whole path that data took

00:32:04,549 --> 00:32:08,389
from a Raspberry Pi that could be

00:32:06,320 --> 00:32:12,610
deployed in a manufacturing plant

00:32:08,389 --> 00:32:12,610
somewhere all the way back as it moved

00:32:16,260 --> 00:32:34,780
okay so if you want to learn of course

00:32:28,440 --> 00:32:36,730
more about it there's different

00:32:34,780 --> 00:32:38,770
resources join the minified Community

00:32:36,730 --> 00:32:41,260
knife eye community participate it's

00:32:38,770 --> 00:32:43,929
pretty active group there's a lot that

00:32:41,260 --> 00:32:45,460
goes on and it's pretty fun once you

00:32:43,929 --> 00:32:47,169
start writing it all the different

00:32:45,460 --> 00:32:48,820
things that you could do they've seen

00:32:47,169 --> 00:32:50,470
people that have done stuff on raspberry

00:32:48,820 --> 00:32:52,540
pies and putting like a collar on their

00:32:50,470 --> 00:32:54,490
dog that has a chip and determining when

00:32:52,540 --> 00:32:57,250
the dog comes in and out of the house to

00:32:54,490 --> 00:32:58,540
turn on or off the sprinkler system you

00:32:57,250 --> 00:32:59,500
know do different things you'll see

00:32:58,540 --> 00:33:01,150
stuff that Horton which is done with

00:32:59,500 --> 00:33:03,309
connected car there's videos out there

00:33:01,150 --> 00:33:05,770
of people controlling a sunroof based

00:33:03,309 --> 00:33:07,419
upon what the GPS says in the car

00:33:05,770 --> 00:33:09,370
whether you're it should be sunny where

00:33:07,419 --> 00:33:10,990
you are or raining of controlling the

00:33:09,370 --> 00:33:12,790
sunroof in a car so you could do a

00:33:10,990 --> 00:33:15,070
variety of things pretty easy to get

00:33:12,790 --> 00:33:18,280
going and everything that you see there

00:33:15,070 --> 00:33:24,419
is extensible either with Java or a REST

00:33:18,280 --> 00:33:27,220
API so that thank you for your time and

00:33:24,419 --> 00:33:33,760
guess that's the buzzer take questions

00:33:27,220 --> 00:33:36,630
if you have them Thank You Andre do you

00:33:33,760 --> 00:33:36,630
have any questions

00:33:44,050 --> 00:33:53,530
I think sitar mmm yeah

00:33:50,550 --> 00:33:55,990
what I'm puzzled about about a knife I

00:33:53,530 --> 00:33:58,200
it has a very nice user interface but

00:33:55,990 --> 00:34:01,270
for what kind of volumes is it suitable

00:33:58,200 --> 00:34:03,310
because um what I don't understand is

00:34:01,270 --> 00:34:05,140
that you know you're talking about very

00:34:03,310 --> 00:34:07,990
low latency and at the same time you're

00:34:05,140 --> 00:34:10,510
talking about slow files and it sounds

00:34:07,990 --> 00:34:13,480
to me like what what is it is it microbe

00:34:10,510 --> 00:34:15,429
etching what no engine does use to

00:34:13,480 --> 00:34:17,590
actually run the data can I run it at

00:34:15,429 --> 00:34:19,450
really large scale on a Hadoop

00:34:17,590 --> 00:34:21,700
environment or something like that right

00:34:19,450 --> 00:34:23,560
so the question was what types of

00:34:21,700 --> 00:34:25,149
volumes can knife I handle and how does

00:34:23,560 --> 00:34:26,950
it do scheduling and can you run it in a

00:34:25,149 --> 00:34:30,310
Hadoop environment did I capture that

00:34:26,950 --> 00:34:31,450
correct so I'll answer in Reverse or

00:34:30,310 --> 00:34:33,550
running on a Hadoop environment today

00:34:31,450 --> 00:34:35,590
knife I doesn't run on yarn and mezzos

00:34:33,550 --> 00:34:37,720
or anything in the future possibly will

00:34:35,590 --> 00:34:39,370
I think there is ideas for it to run on

00:34:37,720 --> 00:34:41,980
that so today does its own scheduling

00:34:39,370 --> 00:34:44,350
and really the basis for that is when it

00:34:41,980 --> 00:34:45,880
started ten years ago those other tools

00:34:44,350 --> 00:34:48,370
were not even a twinkle in someone's eye

00:34:45,880 --> 00:34:50,320
so it does all its own scheduling from a

00:34:48,370 --> 00:34:51,909
scale standpoint work with

00:34:50,320 --> 00:34:53,590
telecommunications companies that are

00:34:51,909 --> 00:34:56,770
running 50 megabytes of seconds of data

00:34:53,590 --> 00:35:00,400
through knife I so it runs at very high

00:34:56,770 --> 00:35:01,600
scale you know credit card company that

00:35:00,400 --> 00:35:03,010
you know if I ask you what's in your

00:35:01,600 --> 00:35:05,880
wallet you probably idea who they are

00:35:03,010 --> 00:35:08,170
they use it to capture all sorts of

00:35:05,880 --> 00:35:10,540
fraud information and cyber security

00:35:08,170 --> 00:35:13,300
they move somewhere around like 50

00:35:10,540 --> 00:35:16,210
thousand events a second through knife I

00:35:13,300 --> 00:35:18,430
going from on creme to off creme to the

00:35:16,210 --> 00:35:21,580
cloud so you could run it at very high

00:35:18,430 --> 00:35:23,410
speeds or and low latency or you can run

00:35:21,580 --> 00:35:26,050
it for bats and some people use it to

00:35:23,410 --> 00:35:28,060
ingest files it really depends the idea

00:35:26,050 --> 00:35:29,440
of a flow files and the reason for

00:35:28,060 --> 00:35:31,360
mentioning of keeping in mind that

00:35:29,440 --> 00:35:33,460
there's the attributes and there's the

00:35:31,360 --> 00:35:39,040
content the attributes that I showed in

00:35:33,460 --> 00:35:42,010
the UI so when you look at when you look

00:35:39,040 --> 00:35:44,710
at this and if we even just go to this

00:35:42,010 --> 00:35:49,960
other flow here and you look at any of

00:35:44,710 --> 00:35:51,280
these when you look at this view here

00:35:49,960 --> 00:35:54,820
right you get to click on this and look

00:35:51,280 --> 00:35:57,010
at this flow file these attributes are

00:35:54,820 --> 00:35:57,730
what move from processor to processor

00:35:57,010 --> 00:36:00,940
the con

00:35:57,730 --> 00:36:03,070
if at all possible stay still on disk so

00:36:00,940 --> 00:36:05,560
once knife I ingest it whether in this

00:36:03,070 --> 00:36:07,240
case is just a meet-up JSON object or

00:36:05,560 --> 00:36:08,770
whether it's a 10 gig file or it's

00:36:07,240 --> 00:36:11,890
something coming from you changing your

00:36:08,770 --> 00:36:14,080
channel on a set-top box the content is

00:36:11,890 --> 00:36:15,370
stored once when it comes in and as it

00:36:14,080 --> 00:36:17,170
moves as long as you're not doing

00:36:15,370 --> 00:36:18,850
something to cause knife I to have to

00:36:17,170 --> 00:36:21,490
rewrite it like compressing the content

00:36:18,850 --> 00:36:22,870
the content stays still the flow files

00:36:21,490 --> 00:36:25,300
have a pointer to where that content

00:36:22,870 --> 00:36:26,740
lives and just the little bits of data

00:36:25,300 --> 00:36:29,830
that's the attributes move from

00:36:26,740 --> 00:36:33,640
processor to processor so designed to

00:36:29,830 --> 00:36:35,820
move things at pretty high speed I got a

00:36:33,640 --> 00:36:35,820
question

00:36:46,770 --> 00:36:53,980
and so we're using nine five and one one

00:36:51,819 --> 00:36:58,180
of the problems that we that we face is

00:36:53,980 --> 00:36:59,980
ingesting from rabbit and cube why

00:36:58,180 --> 00:37:04,660
because if you want to ingest fast from

00:36:59,980 --> 00:37:10,210
rabbit you have to at least get get

00:37:04,660 --> 00:37:13,839
batches of information so multiple

00:37:10,210 --> 00:37:16,390
messages and we are not being able to do

00:37:13,839 --> 00:37:22,150
that with with my file because it's more

00:37:16,390 --> 00:37:25,660
oriented on message per message space is

00:37:22,150 --> 00:37:27,099
there any any way to tackle that or it's

00:37:25,660 --> 00:37:30,220
supposed to be like that message per

00:37:27,099 --> 00:37:33,970
message so which version knife I are

00:37:30,220 --> 00:37:36,819
using I can tell it okay so that would

00:37:33,970 --> 00:37:38,230
be if you can using like the consume JMS

00:37:36,819 --> 00:37:41,559
processor you want to see if that

00:37:38,230 --> 00:37:43,900
supports batching and it may be that

00:37:41,559 --> 00:37:45,970
processor that either it cannot ingest

00:37:43,900 --> 00:37:47,410
more than a message at a time if it

00:37:45,970 --> 00:37:50,410
supports batching it should be able to

00:37:47,410 --> 00:37:52,150
run and ingest more messages it be it

00:37:50,410 --> 00:37:57,670
the processor of what it can do so if

00:37:52,150 --> 00:38:00,400
AMQP not okay so but a good knife I

00:37:57,670 --> 00:38:03,010
support Sebastian right correct supports

00:38:00,400 --> 00:38:06,250
batching and if you're able to consume a

00:38:03,010 --> 00:38:07,750
batch of messages of supporting batching

00:38:06,250 --> 00:38:09,460
from its sources can depend on the

00:38:07,750 --> 00:38:12,670
processor and if that processor was

00:38:09,460 --> 00:38:14,170
written to support batching so it's

00:38:12,670 --> 00:38:16,089
going to be so if it's AMQP is can

00:38:14,170 --> 00:38:18,039
depend if whoever wrote that processor

00:38:16,089 --> 00:38:19,289
if it supports ingesting more in a

00:38:18,039 --> 00:38:21,609
message at a time

00:38:19,289 --> 00:38:22,990
so there's cases words like query

00:38:21,609 --> 00:38:25,000
database table and others that you could

00:38:22,990 --> 00:38:26,609
define the batch of records you want and

00:38:25,000 --> 00:38:29,859
the size of batches and things like that

00:38:26,609 --> 00:38:32,289
so it kind of depends I know like a PCP

00:38:29,859 --> 00:38:34,779
that's a similar if you set the batching

00:38:32,289 --> 00:38:36,240
then it'll send out flow files based

00:38:34,779 --> 00:38:39,039
upon the number of messages are received

00:38:36,240 --> 00:38:45,329
AMQP you want have to look and see with

00:38:39,039 --> 00:38:45,329
that processor thank you that questions

00:38:48,270 --> 00:38:53,710
you mentioned that you are right that

00:38:51,549 --> 00:38:55,839
they're writing to the local disk is

00:38:53,710 --> 00:38:59,410
there any redundancy on it so it's a

00:38:55,839 --> 00:39:02,049
server dies and what is if your data set

00:38:59,410 --> 00:39:03,789
is bigger than the disks on on each

00:39:02,049 --> 00:39:06,250
server or threa

00:39:03,789 --> 00:39:09,549
do I have to have PP servers for that

00:39:06,250 --> 00:39:11,349
all right so the question was at what

00:39:09,549 --> 00:39:14,589
about data redundancy since knife is

00:39:11,349 --> 00:39:16,089
writing locally so today there isn't H a

00:39:14,589 --> 00:39:17,859
from the software standpoint of 9th I

00:39:16,089 --> 00:39:19,990
sending bits of data all around a

00:39:17,859 --> 00:39:21,789
cluster it'll run the cluster there's a

00:39:19,990 --> 00:39:23,680
flow file repository that these flow

00:39:21,789 --> 00:39:24,940
files are being written to a Content

00:39:23,680 --> 00:39:27,520
repository and then a problems

00:39:24,940 --> 00:39:29,589
repository those are all local so today

00:39:27,520 --> 00:39:31,089
the best thing you could do is to have a

00:39:29,589 --> 00:39:34,119
raid configuration that gives you that

00:39:31,089 --> 00:39:36,460
redundancy AB there's work going on to

00:39:34,119 --> 00:39:39,940
have h a at the data level from an

00:39:36,460 --> 00:39:41,619
application of 95 working most likely

00:39:39,940 --> 00:39:43,599
similar to what you'd see Kafka is how

00:39:41,619 --> 00:39:45,760
it does replication and moves messages

00:39:43,599 --> 00:39:47,680
around you possibly see that there's a

00:39:45,760 --> 00:39:49,329
design for it on the Apache wiki for it

00:39:47,680 --> 00:39:52,420
that doesn't exist today

00:39:49,329 --> 00:39:55,000
so the best bet is at a hardware level

00:39:52,420 --> 00:39:57,309
of having raid oftentimes people are not

00:39:55,000 --> 00:40:01,990
using knife I as a data store the data

00:39:57,309 --> 00:40:04,510
is in transit so sometimes losing disk

00:40:01,990 --> 00:40:06,130
isn't that bad but you could configure

00:40:04,510 --> 00:40:09,730
it and then as far as running out of

00:40:06,130 --> 00:40:11,380
disk the amount of data stored for each

00:40:09,730 --> 00:40:15,779
of those repositories is configurable so

00:40:11,380 --> 00:40:15,779
you can configure it to meet your needs

00:40:16,710 --> 00:40:21,869
for your unnamed credit card company

00:40:19,569 --> 00:40:24,460
that you mentioned before what is the

00:40:21,869 --> 00:40:26,099
hardware stack for a cluster look like

00:40:24,460 --> 00:40:29,109
how big is it

00:40:26,099 --> 00:40:30,670
so for knife I it's typical that you see

00:40:29,109 --> 00:40:32,680
in this is I think it was on the slide

00:40:30,670 --> 00:40:34,690
about comparing knife I like storm spa

00:40:32,680 --> 00:40:36,339
or spark or flink where you see like

00:40:34,690 --> 00:40:39,089
thousands of nodes in the knife i case

00:40:36,339 --> 00:40:43,000
you see clusters that are you know tens

00:40:39,089 --> 00:40:44,799
sometimes less so in that case they have

00:40:43,000 --> 00:40:48,220
three nodes internally that run as a

00:40:44,799 --> 00:40:50,049
cluster and then six in AWS to kind of

00:40:48,220 --> 00:40:53,849
have the accommodation of matching their

00:40:50,049 --> 00:40:56,890
hardware to what's in the VMS and AWS

00:40:53,849 --> 00:41:00,220
rammer just 95 by default is going to be

00:40:56,890 --> 00:41:00,760
network bound and then IO bound as it's

00:41:00,220 --> 00:41:02,560
writing

00:41:00,760 --> 00:41:05,260
and then memory and CPUs can depend on

00:41:02,560 --> 00:41:06,760
what you're doing so but always is going

00:41:05,260 --> 00:41:10,330
to be network first of how fast can't

00:41:06,760 --> 00:41:12,760
move data okay thank you Alice Claudia

00:41:10,330 --> 00:41:16,410
silk and your answer yep I'll be around

00:41:12,760 --> 00:41:16,410

YouTube URL: https://www.youtube.com/watch?v=U998Z_i7heQ


