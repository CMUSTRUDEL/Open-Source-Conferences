Title: Berlin Buzzwords 2017: Marvin Justice - Migrating a Real-time News classification Engine to ...
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	As the world's leading provider of financial news, Bloomberg LP ingests on the order of 1 million news stories per day from over 100 thousand sources in over 40 languages. To facilitate users' ability to quickly retrieve news tailored to their specific interests, stories are run through a classification system containing hundreds of thousands of rules where they are tagged in real-time with a mean latency of under 50ms. 

In this talk I'll discuss the migration of the news classification engine from a legacy system to a solution based on Luwak/Lucene while retaining the query language of the existing corpus of rules.  

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,750 --> 00:00:11,040
okay so yeah get in right now okay all

00:00:08,490 --> 00:00:13,320
right so welcome to the talk migrating a

00:00:11,040 --> 00:00:16,619
real time new specification engine to

00:00:13,320 --> 00:00:19,590
lieu a key seeing here's what I'm going

00:00:16,619 --> 00:00:21,480
to try to cover first I'll give an

00:00:19,590 --> 00:00:23,970
overview of a rules-based news

00:00:21,480 --> 00:00:26,640
classification at Bloomberg then I'll

00:00:23,970 --> 00:00:28,980
describe the legacy engine that's

00:00:26,640 --> 00:00:32,160
currently in use and the OTL query

00:00:28,980 --> 00:00:33,600
language then I'll give a real quick

00:00:32,160 --> 00:00:35,400
intro to Lou ACK

00:00:33,600 --> 00:00:37,920
for those of you that are unfamiliar

00:00:35,400 --> 00:00:40,680
with it and that leads to the main part

00:00:37,920 --> 00:00:43,170
of the talk which is how we built the

00:00:40,680 --> 00:00:46,230
Luwak application that understands this

00:00:43,170 --> 00:00:49,260
OTL query language and reproduces the

00:00:46,230 --> 00:00:51,510
behavior of the legacy system and then

00:00:49,260 --> 00:00:54,750
I'll say a few words about a custom

00:00:51,510 --> 00:00:57,630
version of solar we used to test

00:00:54,750 --> 00:00:59,760
everything and conclude with the current

00:00:57,630 --> 00:01:03,930
status of the effort and give you some

00:00:59,760 --> 00:01:07,110
performance numbers okay so Bloomberg is

00:01:03,930 --> 00:01:09,299
a large company with many products the

00:01:07,110 --> 00:01:11,700
principal product is the Bloomberg

00:01:09,299 --> 00:01:15,360
professional service also known as the

00:01:11,700 --> 00:01:19,470
Bloomberg terminal the terminal consists

00:01:15,360 --> 00:01:22,590
of a UI that connects to Bloomberg data

00:01:19,470 --> 00:01:26,400
centers from a client's PC or Mac and

00:01:22,590 --> 00:01:29,300
along with the custom keyboard the UI

00:01:26,400 --> 00:01:32,010
has a command line or a function line

00:01:29,300 --> 00:01:35,220
that accepts almost an unlimited number

00:01:32,010 --> 00:01:37,200
of individual functions among which the

00:01:35,220 --> 00:01:40,770
most some of the most widely used or

00:01:37,200 --> 00:01:43,800
several that are news related in the

00:01:40,770 --> 00:01:48,360
data centers we ingest over a million

00:01:43,800 --> 00:01:50,100
new stories per day from thousands of

00:01:48,360 --> 00:01:53,070
sources around the world in over 50

00:01:50,100 --> 00:01:55,710
languages all for the Li the stories are

00:01:53,070 --> 00:01:57,990
indexed to search collections which were

00:01:55,710 --> 00:02:01,380
converted to solar in just the past few

00:01:57,990 --> 00:02:04,350
years since we're talking about news we

00:02:01,380 --> 00:02:08,819
don't necessarily have to keep every old

00:02:04,350 --> 00:02:10,319
story but any given time we maintain

00:02:08,819 --> 00:02:14,090
active collections containing

00:02:10,319 --> 00:02:17,760
approximately 500 million stories

00:02:14,090 --> 00:02:19,140
subscribers can enter up to 10 million

00:02:17,760 --> 00:02:22,200
new searches per day

00:02:19,140 --> 00:02:24,060
they can do keyword searches but it's

00:02:22,200 --> 00:02:27,030
more useful for us to tag the stories

00:02:24,060 --> 00:02:30,870
say by a company stock ticker such as

00:02:27,030 --> 00:02:32,970
Apple or by topics such as gold or M&A

00:02:30,870 --> 00:02:37,140
and allow the users to search for the

00:02:32,970 --> 00:02:39,390
tags instead for good overview of new

00:02:37,140 --> 00:02:43,470
circuit Bloomberg you can check out this

00:02:39,390 --> 00:02:46,140
talk from last year's buzz words okay

00:02:43,470 --> 00:02:48,390
here's a picture of a Bloomberg keyboard

00:02:46,140 --> 00:02:50,760
this is one of the older models but you

00:02:48,390 --> 00:02:55,200
still see them around note the special

00:02:50,760 --> 00:02:58,769
yellow and green keys if you were to

00:02:55,200 --> 00:03:02,010
type the company stock tip or ticker for

00:02:58,769 --> 00:03:05,519
Apple in the UI and hit the yellow key

00:03:02,010 --> 00:03:08,670
label equity and then select news from

00:03:05,519 --> 00:03:11,850
the pop-up menu this is an example of

00:03:08,670 --> 00:03:15,239
what you might see the default view is a

00:03:11,850 --> 00:03:17,700
time sorted list of headlines along with

00:03:15,239 --> 00:03:20,310
an optional display of the companies the

00:03:17,700 --> 00:03:23,269
stories are tagged with the wire source

00:03:20,310 --> 00:03:26,450
of the story and its time of last update

00:03:23,269 --> 00:03:29,370
so for example at the bottom we have a

00:03:26,450 --> 00:03:32,250
story about companies competing over the

00:03:29,370 --> 00:03:34,290
world supply of cobalt so it gets tagged

00:03:32,250 --> 00:03:36,239
with the companies mentioned in the

00:03:34,290 --> 00:03:38,579
story

00:03:36,239 --> 00:03:41,190
it looks like Tesla got the highest

00:03:38,579 --> 00:03:42,780
score here probably because Tesla was

00:03:41,190 --> 00:03:44,160
mentioned in the headline but if you

00:03:42,780 --> 00:03:46,829
were to hover your mouse over that

00:03:44,160 --> 00:03:51,269
ticker column you'd see Apple and also a

00:03:46,829 --> 00:03:53,400
few other companies so also if you leave

00:03:51,269 --> 00:03:56,579
this screen open for a while what you

00:03:53,400 --> 00:03:58,650
see is new stories satisfying the search

00:03:56,579 --> 00:04:01,760
criteria with scrolling from the top

00:03:58,650 --> 00:04:05,430
pushing older stories off the bottom

00:04:01,760 --> 00:04:08,160
that capability is provided by the news

00:04:05,430 --> 00:04:12,120
alerting subsystem which nowadays also

00:04:08,160 --> 00:04:15,359
runs on top of a little act ok so what

00:04:12,120 --> 00:04:20,880
is rules-based news classification it's

00:04:15,359 --> 00:04:24,870
part of the news ingest pipeline which

00:04:20,880 --> 00:04:27,240
receives stories from various fees and

00:04:24,870 --> 00:04:30,390
an average of around 15 stories per

00:04:27,240 --> 00:04:32,830
second but news can be very bursty so at

00:04:30,390 --> 00:04:36,900
any given time we might see

00:04:32,830 --> 00:04:36,900
Ickes of several many times that rate

00:04:38,820 --> 00:04:43,300
classification is also part of something

00:04:40,930 --> 00:04:46,000
called the primary build where we take

00:04:43,300 --> 00:04:47,500
all the active stories and Riaan jest

00:04:46,000 --> 00:04:50,170
them as rapidly as possible

00:04:47,500 --> 00:04:52,360
and we might want to do that if for

00:04:50,170 --> 00:04:54,670
example the classification rules are

00:04:52,360 --> 00:05:01,240
updated and we want that to be reflected

00:04:54,670 --> 00:05:02,830
in the historical stories in the in the

00:05:01,240 --> 00:05:06,100
pipeline the stories are processed

00:05:02,830 --> 00:05:12,370
through several stages before finally

00:05:06,100 --> 00:05:16,450
being indexed solar in the

00:05:12,370 --> 00:05:17,890
classification stage of the pipeline the

00:05:16,450 --> 00:05:21,760
stories are tagged against hundreds of

00:05:17,890 --> 00:05:24,850
thousands of rules the rules are written

00:05:21,760 --> 00:05:27,100
in the OTL query language which I'll

00:05:24,850 --> 00:05:30,970
describe in detail in the next few

00:05:27,100 --> 00:05:33,970
slides most of the rules are for

00:05:30,970 --> 00:05:36,370
companies and most of those are

00:05:33,970 --> 00:05:38,830
so-called templated rules we take

00:05:36,370 --> 00:05:41,890
information about a company from a

00:05:38,830 --> 00:05:45,700
database and then Auto generate a rule

00:05:41,890 --> 00:05:47,980
from a template but the larger and

00:05:45,700 --> 00:05:52,470
better known companies have handwritten

00:05:47,980 --> 00:05:55,330
rules and there are also thousands of

00:05:52,470 --> 00:06:00,730
handwritten rules for topics such as

00:05:55,330 --> 00:06:04,930
industry currency etc the rules are

00:06:00,730 --> 00:06:06,700
written and maintained by a group inside

00:06:04,930 --> 00:06:09,370
Bloomberg a group of domain experts

00:06:06,700 --> 00:06:15,190
inside Bloomberg called the content

00:06:09,370 --> 00:06:18,100
indexing team yeah so these stories pass

00:06:15,190 --> 00:06:20,140
through the pipeline it gets tagged

00:06:18,100 --> 00:06:23,050
against hundreds of thousands of rules

00:06:20,140 --> 00:06:26,110
and since we have so many rules a brute

00:06:23,050 --> 00:06:30,250
force approach isn't is it feasible so a

00:06:26,110 --> 00:06:32,950
more sophisticated approaches needed for

00:06:30,250 --> 00:06:35,130
the past ten plus years Bloomberg has

00:06:32,950 --> 00:06:38,710
been using a classification engine

00:06:35,130 --> 00:06:42,430
provided by Verity that was before my

00:06:38,710 --> 00:06:45,130
time but I believe Bloomberg began using

00:06:42,430 --> 00:06:46,879
Verity just a few months before it was

00:06:45,130 --> 00:06:49,589
acquired by autonomy

00:06:46,879 --> 00:06:51,779
and autonomy continue to support the

00:06:49,589 --> 00:06:53,719
product until they were eventually

00:06:51,779 --> 00:06:57,479
acquired by hewlett-packard and

00:06:53,719 --> 00:07:00,089
hewlett-packard continued to support us

00:06:57,479 --> 00:07:04,770
until just recently when they decided to

00:07:00,089 --> 00:07:07,379
drop the product the very

00:07:04,770 --> 00:07:10,469
declassification Inogen is provided as a

00:07:07,379 --> 00:07:13,289
32-bit native library the rules are

00:07:10,469 --> 00:07:15,210
written in the OTL query language which

00:07:13,289 --> 00:07:19,770
was invented by Verity as far as I know

00:07:15,210 --> 00:07:23,360
and overall the Verity engine has been a

00:07:19,770 --> 00:07:25,919
good workhorse for Bloomberg achieving

00:07:23,360 --> 00:07:28,680
average latencies of around 25

00:07:25,919 --> 00:07:30,449
milliseconds per story the maximum

00:07:28,680 --> 00:07:34,919
latency is not always so good however

00:07:30,449 --> 00:07:37,889
and that can be tens of seconds but the

00:07:34,919 --> 00:07:40,259
really high latency stories are not what

00:07:37,889 --> 00:07:43,259
we would consider urgent market-moving

00:07:40,259 --> 00:07:45,419
news we get a small but steady stream of

00:07:43,259 --> 00:07:48,209
stories that are really like a research

00:07:45,419 --> 00:07:51,569
reports that can have lots of large

00:07:48,209 --> 00:07:58,050
attachments and that can take a lot of

00:07:51,569 --> 00:08:01,830
classified okay ok L stands for outline

00:07:58,050 --> 00:08:05,069
topic language it is a complete IR query

00:08:01,830 --> 00:08:08,039
language that provides term and phrase

00:08:05,069 --> 00:08:10,979
leads but also has a phrase operator for

00:08:08,039 --> 00:08:12,569
writing multiple related phrases in a

00:08:10,979 --> 00:08:17,899
compact form which I'll illustrate on

00:08:12,569 --> 00:08:22,249
the next slide it has support for basic

00:08:17,899 --> 00:08:25,769
star and question mark wildcards plus a

00:08:22,249 --> 00:08:29,099
support for a limited subset of regular

00:08:25,769 --> 00:08:33,389
expressions it has the basic boolean

00:08:29,099 --> 00:08:36,510
operators and or not for proximity

00:08:33,389 --> 00:08:40,500
queries it has an ear but also a start

00:08:36,510 --> 00:08:43,259
zone in OTL fields are called zones in

00:08:40,500 --> 00:08:46,069
the start zone serves to limit a query

00:08:43,259 --> 00:08:50,310
to the first end positions of the field

00:08:46,069 --> 00:08:54,329
for frequency queries it has at least

00:08:50,310 --> 00:08:56,790
slash n which takes a single child and

00:08:54,329 --> 00:09:00,270
basically says match if this turn

00:08:56,790 --> 00:09:02,130
rephrase appears at least in time

00:09:00,270 --> 00:09:04,830
the count slashing operator takes

00:09:02,130 --> 00:09:07,740
multiple children and says matches any

00:09:04,830 --> 00:09:11,400
end of the children appear there

00:09:07,740 --> 00:09:13,260
miscellaneous operators such as sound x4

00:09:11,400 --> 00:09:15,810
phonetic clinic phonetically sounds like

00:09:13,260 --> 00:09:17,640
the Saurus for synonym which have never

00:09:15,810 --> 00:09:21,810
been used at Bloomberg in which we

00:09:17,640 --> 00:09:24,810
consequently decided not to implement in

00:09:21,810 --> 00:09:29,130
the new system very important is that

00:09:24,810 --> 00:09:31,680
oto provides for absolute scoring so

00:09:29,130 --> 00:09:33,360
this is a good for a classification

00:09:31,680 --> 00:09:36,090
problem which is just with a single

00:09:33,360 --> 00:09:37,950
document at a time and for which loosens

00:09:36,090 --> 00:09:39,440
rank based scoring doesn't entirely make

00:09:37,950 --> 00:09:42,390
sense

00:09:39,440 --> 00:09:45,390
there are several operators devoted

00:09:42,390 --> 00:09:49,190
solely to modifying the scoring behavior

00:09:45,390 --> 00:09:53,040
for example the any and many operators

00:09:49,190 --> 00:09:54,960
are exactly like the or operator when it

00:09:53,040 --> 00:09:58,410
comes to matching but have different

00:09:54,960 --> 00:10:02,010
scoring behaviors most of the operators

00:09:58,410 --> 00:10:04,340
have direct leucine analogues but not

00:10:02,010 --> 00:10:08,030
all of them the consequence is that

00:10:04,340 --> 00:10:10,830
we're going to have to we had to

00:10:08,030 --> 00:10:16,590
implement some custom leucine queries on

00:10:10,830 --> 00:10:19,200
our own for example leucine has a spans

00:10:16,590 --> 00:10:23,640
first query which is similar to the

00:10:19,200 --> 00:10:26,550
start zone our operator but at least as

00:10:23,640 --> 00:10:29,640
of solar 6.3 which is the version we're

00:10:26,550 --> 00:10:32,340
currently on saying first query doesn't

00:10:29,640 --> 00:10:35,490
support multi valued fields which our

00:10:32,340 --> 00:10:39,060
document is contained the count slash n

00:10:35,490 --> 00:10:42,080
is basically the same as a boolean

00:10:39,060 --> 00:10:46,020
minimum number said match in leucine

00:10:42,080 --> 00:10:47,970
except leucine at least as of 6.3 that's

00:10:46,020 --> 00:10:51,390
a plot it doesn't support a spans

00:10:47,970 --> 00:10:56,400
version of that query which we need a no

00:10:51,390 --> 00:11:00,720
TL ok so here is an example of what OPL

00:10:56,400 --> 00:11:03,960
looks like we have an N operator which

00:11:00,720 --> 00:11:07,290
serves to limit the query to a specific

00:11:03,960 --> 00:11:08,760
field headline in this case and OPL you

00:11:07,290 --> 00:11:10,470
don't necessarily have to specify a

00:11:08,760 --> 00:11:12,130
field in which case the query applies to

00:11:10,470 --> 00:11:14,590
all fields

00:11:12,130 --> 00:11:18,000
beneath the end we have a phrase

00:11:14,590 --> 00:11:20,490
operator containing to any list

00:11:18,000 --> 00:11:23,350
parent-child relationships between

00:11:20,490 --> 00:11:27,520
operators are specified by indentation

00:11:23,350 --> 00:11:31,860
with asterisks the phrase operator here

00:11:27,520 --> 00:11:35,710
acts like an outer product forming all

00:11:31,860 --> 00:11:39,040
possible ordered combinations of the any

00:11:35,710 --> 00:11:41,800
list so in this example we're saying

00:11:39,040 --> 00:11:44,880
match if any one of these 3 times 2

00:11:41,800 --> 00:11:47,380
equals 6 phrases appear in the headline

00:11:44,880 --> 00:11:52,540
so you might ask why not just write out

00:11:47,380 --> 00:11:55,450
the six individual phrases and it turns

00:11:52,540 --> 00:11:57,339
out that it's more efficient to specify

00:11:55,450 --> 00:12:00,010
it can be more efficient to specify them

00:11:57,339 --> 00:12:01,120
like this probably for six phrases it

00:12:00,010 --> 00:12:03,839
wouldn't make much difference but we

00:12:01,120 --> 00:12:07,570
have cases we have over a thousand

00:12:03,839 --> 00:12:13,680
related phrases and specifying them with

00:12:07,570 --> 00:12:13,680
a phrase and any's is noticeably faster

00:12:14,160 --> 00:12:19,360
this is yeah like I said this is a very

00:12:16,330 --> 00:12:22,230
simple example in our system the

00:12:19,360 --> 00:12:27,550
shortest queries or over 100 lines long

00:12:22,230 --> 00:12:32,460
with the longer ones clocking in at over

00:12:27,550 --> 00:12:35,110
8,000 lines ok so we have a

00:12:32,460 --> 00:12:37,660
classification engine with its own query

00:12:35,110 --> 00:12:39,520
language which is being retired by its

00:12:37,660 --> 00:12:42,640
vendor so we got to come up with a

00:12:39,520 --> 00:12:45,100
replacement first requirement is that we

00:12:42,640 --> 00:12:47,800
want to do this in such a way as to be a

00:12:45,100 --> 00:12:51,250
minimal burden for the content indexing

00:12:47,800 --> 00:12:53,770
teams there are literally tens of

00:12:51,250 --> 00:12:56,020
thousands of handwritten rules and we

00:12:53,770 --> 00:12:59,320
can't really ask these guys to rewrite

00:12:56,020 --> 00:13:01,030
their queries in a new language so the

00:12:59,320 --> 00:13:05,350
replacement must be able to interpret

00:13:01,030 --> 00:13:08,110
OTL input also there are several

00:13:05,350 --> 00:13:10,120
components downstream of classification

00:13:08,110 --> 00:13:15,100
that rely on the existing scoring scheme

00:13:10,120 --> 00:13:16,690
so we need to replicate that as well the

00:13:15,100 --> 00:13:19,930
next requirement is that the performance

00:13:16,690 --> 00:13:23,080
must be as good or better we are allowed

00:13:19,930 --> 00:13:24,130
to spend a bit more on hardware as long

00:13:23,080 --> 00:13:28,500
as it doesn't get out

00:13:24,130 --> 00:13:31,780
and there are two aspects to performance

00:13:28,500 --> 00:13:33,820
latency and load for latency we have the

00:13:31,780 --> 00:13:37,410
legacy systems average of 25

00:13:33,820 --> 00:13:40,330
milliseconds as our target for the load

00:13:37,410 --> 00:13:44,410
the ultimate test is whether we can keep

00:13:40,330 --> 00:13:47,110
up during primary build and finally we

00:13:44,410 --> 00:13:51,280
need a way to for content indexers to

00:13:47,110 --> 00:13:52,840
test new rules but Verity provided a

00:13:51,280 --> 00:13:55,540
nice authoring environment for both

00:13:52,840 --> 00:13:56,710
writing and testing rules we don't need

00:13:55,540 --> 00:13:59,410
to replace the whole authoring

00:13:56,710 --> 00:14:03,670
environment right away but we do need a

00:13:59,410 --> 00:14:05,320
way to allow content indexers to

00:14:03,670 --> 00:14:10,110
generate custom collections of stories

00:14:05,320 --> 00:14:12,580
and run the rules against them ok so

00:14:10,110 --> 00:14:15,000
where do we start well it turns out

00:14:12,580 --> 00:14:18,510
there's a open source library called

00:14:15,000 --> 00:14:22,540
Luwak that does exactly what we want and

00:14:18,510 --> 00:14:25,780
the quote here is from the github readme

00:14:22,540 --> 00:14:28,450
for Lu ACK simply put Lueck allows you

00:14:25,780 --> 00:14:30,490
to define a set of search queries and

00:14:28,450 --> 00:14:34,510
then monitor a stream of documents for

00:14:30,490 --> 00:14:37,210
any that might match Lueck is the

00:14:34,510 --> 00:14:40,810
brainchild of Alan Woodward who spoke

00:14:37,210 --> 00:14:44,500
here earlier today and his colleagues at

00:14:40,810 --> 00:14:46,750
flax in Cambridge England to really

00:14:44,500 --> 00:14:49,000
describe Lueck and any debts would

00:14:46,750 --> 00:14:50,860
require its own separate talk but

00:14:49,000 --> 00:14:54,760
fortunately Alan woodwork gave just such

00:14:50,860 --> 00:14:58,660
a talk at Berlin buzzwords 2014 which

00:14:54,760 --> 00:15:02,050
you can find online there briefly the

00:14:58,660 --> 00:15:06,130
Lueck is based on uses a query index and

00:15:02,050 --> 00:15:09,310
app researcher to do an inverted search

00:15:06,130 --> 00:15:14,970
where documents are treated as queries

00:15:09,310 --> 00:15:17,980
and queries are treated as documents of

00:15:14,970 --> 00:15:20,920
the document it's essentially turned

00:15:17,980 --> 00:15:23,020
into something like a giant boolean or

00:15:20,920 --> 00:15:25,870
at its terms and run against the query

00:15:23,020 --> 00:15:27,910
index in order to quickly select out

00:15:25,870 --> 00:15:32,350
only those queries that have any chance

00:15:27,910 --> 00:15:35,050
of matching and the queries that are

00:15:32,350 --> 00:15:37,600
left over after pre selection can then

00:15:35,050 --> 00:15:46,839
be run in brute-force parallel

00:15:37,600 --> 00:15:49,360
fashion prior to the migration of

00:15:46,839 --> 00:15:52,389
classification to Lueck bluemark had

00:15:49,360 --> 00:15:57,449
already migrated the news alerting

00:15:52,389 --> 00:16:00,399
subsystem to Lueck and Daniel Collins

00:15:57,449 --> 00:16:05,019
gave a nice presentation on that which

00:16:00,399 --> 00:16:06,160
you can find online their classification

00:16:05,019 --> 00:16:10,810
is a little bit more challenging than

00:16:06,160 --> 00:16:13,089
alerting because it uses the OTL query

00:16:10,810 --> 00:16:15,839
language whereas alerting uses a pretty

00:16:13,089 --> 00:16:19,660
basic query language

00:16:15,839 --> 00:16:23,790
okay so we have Lueck and we need a

00:16:19,660 --> 00:16:28,560
version that understands OTL and

00:16:23,790 --> 00:16:31,720
replicates the existing scoring here are

00:16:28,560 --> 00:16:34,750
the pieces the main pieces we're going

00:16:31,720 --> 00:16:38,920
to need to do that which I'll elaborate

00:16:34,750 --> 00:16:40,329
on the next slide next few slides first

00:16:38,920 --> 00:16:43,089
we're going to need a Nokia parser

00:16:40,329 --> 00:16:44,620
obviously we need analysis change which

00:16:43,089 --> 00:16:47,769
seems straightforward but there are

00:16:44,620 --> 00:16:52,649
complications in our case and since not

00:16:47,769 --> 00:16:54,699
all OTL constructs can be represented by

00:16:52,649 --> 00:16:57,430
existing lease inquiries we're going to

00:16:54,699 --> 00:16:59,649
have to implement some custom queries

00:16:57,430 --> 00:17:03,370
and figure out how to do the scoring and

00:16:59,649 --> 00:17:04,390
once when you implement custom queries

00:17:03,370 --> 00:17:07,240
then we're gonna have to do some

00:17:04,390 --> 00:17:09,909
extensions to Lueck finally when we have

00:17:07,240 --> 00:17:14,049
all that we need to integrate it into

00:17:09,909 --> 00:17:16,630
our pipeline so where do we start

00:17:14,049 --> 00:17:17,520
well the first thing we did was with the

00:17:16,630 --> 00:17:20,260
higher flag

00:17:17,520 --> 00:17:22,750
okay so Alan came on board as a

00:17:20,260 --> 00:17:25,720
consultant and worked for a few months

00:17:22,750 --> 00:17:30,159
to help get this project off the ground

00:17:25,720 --> 00:17:31,510
in particular Alice Allen provided us

00:17:30,159 --> 00:17:35,740
with nearly complete

00:17:31,510 --> 00:17:37,380
okie L parser for which he chose the

00:17:35,740 --> 00:17:41,760
atler for parser generator

00:17:37,380 --> 00:17:44,320
he wrote a grammar file from which

00:17:41,760 --> 00:17:48,670
lecturer parser listener components are

00:17:44,320 --> 00:17:51,390
generated using antler tools and then

00:17:48,670 --> 00:17:54,300
Allen wrote a query builder class to

00:17:51,390 --> 00:17:58,930
take these generating components and

00:17:54,300 --> 00:18:01,330
translate OTL into a loosie loosie query

00:17:58,930 --> 00:18:03,390
object in a top-down walk of the parse

00:18:01,330 --> 00:18:03,390
tree

00:18:04,410 --> 00:18:10,480
okay analysis changed the issue here is

00:18:08,260 --> 00:18:14,080
that the legacy system has its own

00:18:10,480 --> 00:18:16,030
proprietary analysis change and those

00:18:14,080 --> 00:18:19,630
are reflected in the way the rules are

00:18:16,030 --> 00:18:23,290
written since in verity queries are

00:18:19,630 --> 00:18:26,980
whitespace tokenized for example

00:18:23,290 --> 00:18:29,680
consider Twitter handles okay so when

00:18:26,980 --> 00:18:32,860
Twitter first began to become a big

00:18:29,680 --> 00:18:35,680
thing for News the legacy systems

00:18:32,860 --> 00:18:38,140
tokenizer config file was modified to

00:18:35,680 --> 00:18:41,530
not treat at symbols it's punctuation

00:18:38,140 --> 00:18:44,640
and specific Twitter handles were added

00:18:41,530 --> 00:18:47,080
to classification rules in some cases

00:18:44,640 --> 00:18:50,350
well that's a problem for our live scene

00:18:47,080 --> 00:18:52,410
which wants to treat the @ symbol is

00:18:50,350 --> 00:18:56,440
punctuation and just discard it

00:18:52,410 --> 00:18:58,300
so the solution we chose was to fork

00:18:56,440 --> 00:19:01,090
create a fork of leucine standard

00:18:58,300 --> 00:19:03,670
tokenizer and modify the underlying

00:19:01,090 --> 00:19:06,900
j-flex file so that at and a few other

00:19:03,670 --> 00:19:10,930
symbols aren't treated as punctuation

00:19:06,900 --> 00:19:13,000
another case was Korean where we had to

00:19:10,930 --> 00:19:13,660
create a separate Fork of the standard

00:19:13,000 --> 00:19:16,120
tokenizer

00:19:13,660 --> 00:19:19,420
in order to handle mixed character words

00:19:16,120 --> 00:19:21,580
where the legacy tokenizer

00:19:19,420 --> 00:19:25,210
and which sees the scenes tokenizer have

00:19:21,580 --> 00:19:28,960
different behavior for filtering we

00:19:25,210 --> 00:19:32,440
found that the combination of lower case

00:19:28,960 --> 00:19:35,140
filter and ASCII foley filter replicated

00:19:32,440 --> 00:19:38,320
the legacy system quite well except for

00:19:35,140 --> 00:19:40,840
a few cases where Lucy's ASCII fold and

00:19:38,320 --> 00:19:44,320
filter pulls a few more characters in

00:19:40,840 --> 00:19:46,450
the legacy system that did so to handle

00:19:44,320 --> 00:19:50,650
that we simply added exclusion list to

00:19:46,450 --> 00:19:52,810
the SD falling filter okay Chinese

00:19:50,650 --> 00:19:59,860
Japanese that's where the real problems

00:19:52,810 --> 00:20:02,010
occur so the legacy system used a

00:19:59,860 --> 00:20:05,970
dictionary based tokenizer

00:20:02,010 --> 00:20:09,030
from bassist act circa 2005 so we

00:20:05,970 --> 00:20:11,280
initially we reached out to bassist AK

00:20:09,030 --> 00:20:14,880
and asked if they could provide us a

00:20:11,280 --> 00:20:17,220
solar pluggable version of that exact

00:20:14,880 --> 00:20:18,840
tokenizer and they said they couldn't

00:20:17,220 --> 00:20:21,690
really do that would we like to try

00:20:18,840 --> 00:20:23,160
their latest version instead so we tried

00:20:21,690 --> 00:20:24,600
the latest version but it was just too

00:20:23,160 --> 00:20:28,350
different from the from the earlier

00:20:24,600 --> 00:20:31,110
version so then we tried all the

00:20:28,350 --> 00:20:33,840
available solar alternatives and found

00:20:31,110 --> 00:20:35,670
that the one that comes closest to the

00:20:33,840 --> 00:20:41,360
one we've been using is the ICU

00:20:35,670 --> 00:20:44,190
tokenizer the ICU tokenizer is also a

00:20:41,360 --> 00:20:47,970
dictionary based and empirically we can

00:20:44,190 --> 00:20:51,420
tell that there's a least a 70%

00:20:47,970 --> 00:20:55,920
agreement between the ice cute

00:20:51,420 --> 00:20:57,510
dictionary and the legacy dictionary but

00:20:55,920 --> 00:21:00,570
that's exacerbated the problem is

00:20:57,510 --> 00:21:05,130
decimated by the fact that in the legacy

00:21:00,570 --> 00:21:07,560
system when an indexer wants to search

00:21:05,130 --> 00:21:09,270
for a Chinese phrase for example the

00:21:07,560 --> 00:21:12,450
very authoring tool will insert

00:21:09,270 --> 00:21:16,890
whitespace where its dictionary says

00:21:12,450 --> 00:21:20,010
says it should go so one thing we can do

00:21:16,890 --> 00:21:22,560
to help for the new system is to abandon

00:21:20,010 --> 00:21:25,440
whitespace tokenization for chinese and

00:21:22,560 --> 00:21:27,540
japanese and instead to programmatically

00:21:25,440 --> 00:21:30,120
reto kanai's the OTL according to the

00:21:27,540 --> 00:21:34,560
new dictionary and that helps a lot that

00:21:30,120 --> 00:21:37,080
brings the agreement up to above 90% but

00:21:34,560 --> 00:21:38,520
it's still not good enough so one thing

00:21:37,080 --> 00:21:40,890
we've been playing around with recently

00:21:38,520 --> 00:21:43,200
is to customize the ICU dictionary

00:21:40,890 --> 00:21:45,450
itself to try and bring it into a better

00:21:43,200 --> 00:21:46,980
agreement with the old dictionary and

00:21:45,450 --> 00:21:50,880
we've had partial success with that

00:21:46,980 --> 00:21:52,530
approach still early and it's still an

00:21:50,880 --> 00:21:55,290
open question as to whether it's going

00:21:52,530 --> 00:21:56,940
to fix all our problems ultimately we

00:21:55,290 --> 00:22:01,110
may have to bend our requirements Evette

00:21:56,940 --> 00:22:05,160
and as the indexing team to adjust some

00:22:01,110 --> 00:22:09,120
of the chinese-japanese rules okay now

00:22:05,160 --> 00:22:12,900
on to custom queries and scoring as

00:22:09,120 --> 00:22:15,910
mentioned not all OTL operators have

00:22:12,900 --> 00:22:18,880
Lucene analogs so for those we had to

00:22:15,910 --> 00:22:22,030
by custom queries for example for the at

00:22:18,880 --> 00:22:24,310
least operator we wrote at least query

00:22:22,030 --> 00:22:27,100
by Sicily copying an existing Luke

00:22:24,310 --> 00:22:29,410
lucien query and then just modifying it

00:22:27,100 --> 00:22:31,930
so that its internal iterator checks the

00:22:29,410 --> 00:22:35,050
frequency of the sub score before

00:22:31,930 --> 00:22:36,640
deciding whether there's a match we also

00:22:35,050 --> 00:22:40,600
had to come up with a spans version of

00:22:36,640 --> 00:22:42,940
that one and span min match query for

00:22:40,600 --> 00:22:43,810
proximity queries containing count

00:22:42,940 --> 00:22:46,240
operators

00:22:43,810 --> 00:22:49,780
I'll cover the span starts query in

00:22:46,240 --> 00:22:52,660
detail on the next slide we had to write

00:22:49,780 --> 00:22:55,420
custom queries for all the scoring

00:22:52,660 --> 00:22:58,660
operators but that wasn't really too

00:22:55,420 --> 00:23:01,270
hard he just involved taking existing

00:22:58,660 --> 00:23:03,730
Lucene queries and changing the scoring

00:23:01,270 --> 00:23:05,710
methods for example it's just Junction a

00:23:03,730 --> 00:23:07,750
crew craters and a crew operator in

00:23:05,710 --> 00:23:10,030
ochio they're just judging a crew query

00:23:07,750 --> 00:23:12,160
is very similar to the existing

00:23:10,030 --> 00:23:17,530
disjunction max query except for the

00:23:12,160 --> 00:23:20,740
scoring methods even for operators that

00:23:17,530 --> 00:23:22,990
have direct leucine analogues we found

00:23:20,740 --> 00:23:26,170
that small tweaks to lose scene were

00:23:22,990 --> 00:23:29,620
required for example the scene has a

00:23:26,170 --> 00:23:32,430
disjunction score or class that among

00:23:29,620 --> 00:23:34,840
other things has a frequency method

00:23:32,430 --> 00:23:38,560
calculates frequencies by incrementing

00:23:34,840 --> 00:23:40,060
by one for each matching sub score well

00:23:38,560 --> 00:23:43,480
I needn't give the correct answer for

00:23:40,060 --> 00:23:46,060
OTL but by just modifying this this one

00:23:43,480 --> 00:23:50,560
line of code we're able to reproduce the

00:23:46,060 --> 00:23:55,870
legacy system pretty much exactly we had

00:23:50,560 --> 00:23:57,970
to tweak the slot formulas for near

00:23:55,870 --> 00:24:02,470
queries and I'll cover that in a further

00:23:57,970 --> 00:24:05,860
slide ran into a lot of difficulties

00:24:02,470 --> 00:24:08,950
with nested span queries which are very

00:24:05,860 --> 00:24:11,980
heavily used in our system in fact

00:24:08,950 --> 00:24:14,820
there's a open Lucene issue 7 3 9 8

00:24:11,980 --> 00:24:18,180
called nested span queries or buggy

00:24:14,820 --> 00:24:23,310
which pretty much sums up our experience

00:24:18,180 --> 00:24:26,320
but we found that most of those issues

00:24:23,310 --> 00:24:28,360
could be resolved by simple modification

00:24:26,320 --> 00:24:35,559
the sort order

00:24:28,360 --> 00:24:37,150
we'll see expand position queue okay so

00:24:35,559 --> 00:24:40,230
in this slide I'll try to walk you

00:24:37,150 --> 00:24:44,190
through our custom span starts query

00:24:40,230 --> 00:24:47,920
recall that TL has a start zone operator

00:24:44,190 --> 00:24:49,960
that serves to limit a query to the

00:24:47,920 --> 00:24:52,990
first end positions of a field

00:24:49,960 --> 00:24:56,260
well leucine has a span first query that

00:24:52,990 --> 00:24:59,530
you know sounds like it might work can

00:24:56,260 --> 00:25:02,160
we can we use that so okay for a test

00:24:59,530 --> 00:25:05,890
let's try a query simple query that says

00:25:02,160 --> 00:25:09,100
match is the term Lu act appears in the

00:25:05,890 --> 00:25:12,490
five first five positions of the

00:25:09,100 --> 00:25:15,190
attachment field okay here we have a

00:25:12,490 --> 00:25:17,950
document with two attachments of a

00:25:15,190 --> 00:25:21,669
thousand tokens a piece in the first

00:25:17,950 --> 00:25:23,470
Luwak appears at position 500 so that's

00:25:21,669 --> 00:25:25,150
no good but in a second

00:25:23,470 --> 00:25:26,799
you know Lueck it's within five

00:25:25,150 --> 00:25:30,880
positions of the start so yes this

00:25:26,799 --> 00:25:33,220
document should definitely match the

00:25:30,880 --> 00:25:36,820
span first query however uses a single

00:25:33,220 --> 00:25:40,120
iterator and it can only check whether

00:25:36,820 --> 00:25:42,910
the position of that iterator is within

00:25:40,120 --> 00:25:45,580
five positions of the start so it sees

00:25:42,910 --> 00:25:50,770
the second Luwak in position 1001 and

00:25:45,580 --> 00:25:52,390
says no this document doesn't match our

00:25:50,770 --> 00:25:55,000
custom span starts query on the other

00:25:52,390 --> 00:25:57,210
hand uses two iterators and relies on a

00:25:55,000 --> 00:26:00,280
trick we picked up from Allen Woodward

00:25:57,210 --> 00:26:02,679
so in the document analysis chain we

00:26:00,280 --> 00:26:04,660
have a custom token filter that is

00:26:02,679 --> 00:26:06,870
certain search to start anchor token at

00:26:04,660 --> 00:26:09,580
the beginning of each value of a field

00:26:06,870 --> 00:26:13,720
this token filter runs last in

00:26:09,580 --> 00:26:17,770
particular after the lower case filter

00:26:13,720 --> 00:26:20,380
so if we choose a start anchor with

00:26:17,770 --> 00:26:24,280
containing capital letters there's no

00:26:20,380 --> 00:26:26,980
chance that that token will ever be seen

00:26:24,280 --> 00:26:29,200
by an end user so then we can have one

00:26:26,980 --> 00:26:32,140
iterator iterate over the start tokens

00:26:29,200 --> 00:26:34,630
while the other iterator eighths over

00:26:32,140 --> 00:26:36,940
the Luwak tournament before and now if

00:26:34,630 --> 00:26:39,280
we subtract the positions of the two

00:26:36,940 --> 00:26:41,980
iterators we get we can get the correct

00:26:39,280 --> 00:26:46,690
answer one thousand three you might 1001

00:26:41,980 --> 00:26:52,419
it's less than five so it's a match okay

00:26:46,690 --> 00:26:54,820
so we had to modify the slot

00:26:52,419 --> 00:26:58,090
calculations for both ordered and

00:26:54,820 --> 00:26:59,830
unordered near queries and here I'll try

00:26:58,090 --> 00:27:03,850
to walk you through the unordered near

00:26:59,830 --> 00:27:07,870
case so we have a query fragment near

00:27:03,850 --> 00:27:09,850
slash for of three terms in OTL there's

00:27:07,870 --> 00:27:12,490
no such thing as nearest slash zero near

00:27:09,850 --> 00:27:14,350
slash one is as close as you can get so

00:27:12,490 --> 00:27:17,559
when we translate this to leucine it

00:27:14,350 --> 00:27:21,370
becomes an order unordered span near

00:27:17,559 --> 00:27:23,799
query with a slop of three so next

00:27:21,370 --> 00:27:26,980
consider this text fragment does it

00:27:23,799 --> 00:27:29,980
match well the three terms are all

00:27:26,980 --> 00:27:34,570
present spanning positions from zero to

00:27:29,980 --> 00:27:37,870
five what what's the slot if you look in

00:27:34,570 --> 00:27:40,029
the near spans and ordered class the

00:27:37,870 --> 00:27:42,639
slot is calculated as follows so you

00:27:40,029 --> 00:27:45,159
have the max in position in cell dot in

00:27:42,639 --> 00:27:47,799
position well that's six in this case

00:27:45,159 --> 00:27:51,820
because the in position of a turn spans

00:27:47,799 --> 00:27:54,159
is it start plus one minus the min

00:27:51,820 --> 00:27:56,889
position cells start position which is

00:27:54,159 --> 00:27:59,909
zero in this case minus the total span

00:27:56,889 --> 00:28:02,260
length well what's the total span length

00:27:59,909 --> 00:28:05,500
if you look where that's calculated it's

00:28:02,260 --> 00:28:07,630
just the sum of the lengths of the

00:28:05,500 --> 00:28:10,090
individual sub spans so that would be

00:28:07,630 --> 00:28:12,760
three in this case so leucine would

00:28:10,090 --> 00:28:16,000
calculate the slop at six minus zero

00:28:12,760 --> 00:28:19,779
minus three equals three and say yes

00:28:16,000 --> 00:28:22,419
this is a match in fact in the scene it

00:28:19,779 --> 00:28:25,720
would be possible to have a near / one

00:28:22,419 --> 00:28:27,340
say a hundred terms okay that's okay

00:28:25,720 --> 00:28:28,840
that's that's one way to do things but

00:28:27,340 --> 00:28:32,080
that's not how LTL works

00:28:28,840 --> 00:28:37,260
and OTL a new year / four means

00:28:32,080 --> 00:28:41,320
literally that there can be at most four

00:28:37,260 --> 00:28:43,179
positions between the start and the the

00:28:41,320 --> 00:28:45,840
first term and the last term as it

00:28:43,179 --> 00:28:49,389
appears in the document so we had to

00:28:45,840 --> 00:28:52,059
modify the slot formula in the near

00:28:49,389 --> 00:28:54,669
spans an ordered class to calculate the

00:28:52,059 --> 00:28:55,690
slots as follows and we get the fifth

00:28:54,669 --> 00:29:00,100
it's not a match

00:28:55,690 --> 00:29:02,680
and we're good to go okay so we've made

00:29:00,100 --> 00:29:05,350
some changes to the scene do we have to

00:29:02,680 --> 00:29:08,050
change it change any luat code the

00:29:05,350 --> 00:29:10,960
answer is no we can run stock glue act

00:29:08,050 --> 00:29:14,770
because Lueck is designed in such a way

00:29:10,960 --> 00:29:16,780
that if you add custom queries and all

00:29:14,770 --> 00:29:21,420
you need to do is extend some Louet

00:29:16,780 --> 00:29:26,140
classes so recall that Louis Lueck

00:29:21,420 --> 00:29:29,470
builds a query index and it does that by

00:29:26,140 --> 00:29:33,010
using a query extractor critics eckers

00:29:29,470 --> 00:29:40,030
so we had custom queries then all you

00:29:33,010 --> 00:29:41,860
need to do is add custom extractors so

00:29:40,030 --> 00:29:45,750
we also use a performance feature in

00:29:41,860 --> 00:29:49,380
Lueck called query decomposition where

00:29:45,750 --> 00:29:53,110
for example disjunction queries are

00:29:49,380 --> 00:29:55,270
decomposed into their individual sub

00:29:53,110 --> 00:29:57,670
clauses which can then can be selected

00:29:55,270 --> 00:29:59,740
independently by the pre searcher and so

00:29:57,670 --> 00:30:03,670
we just extended the query decomposer to

00:29:59,740 --> 00:30:05,410
an opioid query decomposer which knows

00:30:03,670 --> 00:30:08,130
how to decompose our custom queries

00:30:05,410 --> 00:30:11,830
likewise for highlighting functionality

00:30:08,130 --> 00:30:13,660
we extended the span rewriter class to

00:30:11,830 --> 00:30:21,520
an ocl span rewriter class to handle the

00:30:13,660 --> 00:30:24,250
custom queries okay so we have now a new

00:30:21,520 --> 00:30:29,080
engine and we need to integrate it into

00:30:24,250 --> 00:30:31,900
our pipeline how do we do that well the

00:30:29,080 --> 00:30:34,870
pipeline consists of a set of discrete

00:30:31,900 --> 00:30:38,500
Blumberg application services or bath

00:30:34,870 --> 00:30:42,210
services so we're going to need a bath

00:30:38,500 --> 00:30:42,210
service for our new engine

00:30:42,690 --> 00:30:47,650
traditionally bass has been a native

00:30:45,010 --> 00:30:49,270
code framework at Bloomberg and so with

00:30:47,650 --> 00:30:52,570
the legacy engine all we needed to do

00:30:49,270 --> 00:30:53,730
was link in the dot SL library provided

00:30:52,570 --> 00:30:57,100
by the vendor

00:30:53,730 --> 00:31:00,100
well Luwak and latina written in Java

00:30:57,100 --> 00:31:02,230
and at the time we started this project

00:31:00,100 --> 00:31:05,370
bass Java wasn't really available

00:31:02,230 --> 00:31:07,640
although it's sort of available now so

00:31:05,370 --> 00:31:11,600
we decided to do

00:31:07,640 --> 00:31:16,010
us to integrate the new Luwak based

00:31:11,600 --> 00:31:21,140
engine into a native bass using j'ni and

00:31:16,010 --> 00:31:23,330
we call that new service Oryx there

00:31:21,140 --> 00:31:25,580
would have been other alternatives for

00:31:23,330 --> 00:31:28,220
example we could have embedded Lu AK

00:31:25,580 --> 00:31:31,010
into a standalone web service and send

00:31:28,220 --> 00:31:31,730
an HTTP request to it many people do it

00:31:31,010 --> 00:31:34,550
that way

00:31:31,730 --> 00:31:36,890
we chose the j'ni approach you know it's

00:31:34,550 --> 00:31:39,280
worked well for us and so we're happy

00:31:36,890 --> 00:31:39,280
with it

00:31:41,000 --> 00:31:46,010
these auric services run on a farm of

00:31:44,120 --> 00:31:51,290
Linux machines spread across multiple

00:31:46,010 --> 00:31:53,150
data servers data centers the rules are

00:31:51,290 --> 00:31:56,600
charted four ways which we found to be

00:31:53,150 --> 00:31:58,940
sufficient so each each of the four

00:31:56,600 --> 00:32:01,910
shards is replicated across each machine

00:31:58,940 --> 00:32:05,540
of the farm and then the replicas are

00:32:01,910 --> 00:32:08,360
fed by intelligent routers which track

00:32:05,540 --> 00:32:11,600
the number of in-flight jobs and route

00:32:08,360 --> 00:32:14,150
to the replicas with a leaf load we

00:32:11,600 --> 00:32:18,590
currently have the parallel master

00:32:14,150 --> 00:32:22,820
thread count of Lueck sent to to we we

00:32:18,590 --> 00:32:25,460
tried sending it to four but found that

00:32:22,820 --> 00:32:27,920
that didn't really work for new spikes

00:32:25,460 --> 00:32:29,980
or primary build for those cases we

00:32:27,920 --> 00:32:33,170
really need to be able to classify

00:32:29,980 --> 00:32:36,170
20-story simultaneously per replicas and

00:32:33,170 --> 00:32:40,520
so to avoid CPU starvation we had to set

00:32:36,170 --> 00:32:44,960
the the matress thread count to two once

00:32:40,520 --> 00:32:48,140
the replica has 20 is working on 20

00:32:44,960 --> 00:32:50,480
stories then any subsequent requests are

00:32:48,140 --> 00:32:57,110
queued up but that's very rare in our

00:32:50,480 --> 00:33:01,850
system okay so the last piece of

00:32:57,110 --> 00:33:05,060
requirements was to provide a way for

00:33:01,850 --> 00:33:06,980
content indexers to generate collections

00:33:05,060 --> 00:33:09,140
and test their rules against them that's

00:33:06,980 --> 00:33:10,880
just the classic search problem you have

00:33:09,140 --> 00:33:16,640
one query you want to see which

00:33:10,880 --> 00:33:18,450
documents and matches and why so what we

00:33:16,640 --> 00:33:22,260
did was create a

00:33:18,450 --> 00:33:27,450
custom branches of solar containing the

00:33:22,260 --> 00:33:29,720
OTL parser the custom query the leucine

00:33:27,450 --> 00:33:32,010
twigs just like everything except Lueck

00:33:29,720 --> 00:33:35,090
the only additional change we made to

00:33:32,010 --> 00:33:39,390
solar was to modify it so it could

00:33:35,090 --> 00:33:41,640
handle so they had per language analysis

00:33:39,390 --> 00:33:44,610
changed then we threw together a

00:33:41,640 --> 00:33:47,910
Bloomberg UI and this is what that looks

00:33:44,610 --> 00:33:50,130
like so this is a purely internal screen

00:33:47,910 --> 00:33:52,770
a customer would never never be able to

00:33:50,130 --> 00:33:55,800
access this so we have a you know in the

00:33:52,770 --> 00:33:57,570
grid box we have several collections of

00:33:55,800 --> 00:34:00,270
stories ranging in size from a million

00:33:57,570 --> 00:34:02,160
all the way down one story there's a

00:34:00,270 --> 00:34:04,230
button where you can create a new

00:34:02,160 --> 00:34:06,540
collection if you want the yellow box

00:34:04,230 --> 00:34:10,710
you can write a short query in either

00:34:06,540 --> 00:34:13,740
OTL or plain so our format with the

00:34:10,710 --> 00:34:16,440
other tabs you can upload pre-existing

00:34:13,740 --> 00:34:20,540
OTL rules and then run them against the

00:34:16,440 --> 00:34:24,540
collection the results are displayed

00:34:20,540 --> 00:34:25,830
pretty much the same format as the what

00:34:24,540 --> 00:34:28,590
I shows the very beginning of the talk

00:34:25,830 --> 00:34:31,830
with the Apple search only if you click

00:34:28,590 --> 00:34:34,170
on a headline in this UI you'll see the

00:34:31,830 --> 00:34:39,230
query the result the story displayed

00:34:34,170 --> 00:34:44,150
with a query hits highlighted ok so

00:34:39,230 --> 00:34:47,340
that's it the current status is that

00:34:44,150 --> 00:34:49,950
we're now running parallel streams in

00:34:47,340 --> 00:34:53,460
the pipeline for both the old and new

00:34:49,950 --> 00:34:57,000
engine so far we only keep the results

00:34:53,460 --> 00:35:00,330
from the old engine except for a tie

00:34:57,000 --> 00:35:03,570
we've recently had interest for Tai

00:35:00,330 --> 00:35:05,130
language news and the legacy system does

00:35:03,570 --> 00:35:07,890
it support Tai so we went we've gone

00:35:05,130 --> 00:35:10,260
ahead and turned on Tai for the new

00:35:07,890 --> 00:35:13,200
system for the other languages we're

00:35:10,260 --> 00:35:17,460
doing the process of doing side-by-side

00:35:13,200 --> 00:35:19,590
comparisons the the two engines and what

00:35:17,460 --> 00:35:22,920
we found for for non Chinese Japanese

00:35:19,590 --> 00:35:26,370
language is that the new system is able

00:35:22,920 --> 00:35:29,220
to reproduce the results with a greater

00:35:26,370 --> 00:35:31,940
than 99% accuracy accuracy and that

00:35:29,220 --> 00:35:33,920
includes the scoring

00:35:31,940 --> 00:35:35,150
for Chinese Japanese it's not quite so

00:35:33,920 --> 00:35:38,660
good

00:35:35,150 --> 00:35:40,550
slightly above 90% and that's real very

00:35:38,660 --> 00:35:43,430
rule dependent however you know some

00:35:40,550 --> 00:35:45,320
some rule Chinese Japanese rules or you

00:35:43,430 --> 00:35:47,390
know above the 99% but others are

00:35:45,320 --> 00:35:49,849
significantly worse and the problem

00:35:47,390 --> 00:35:52,250
there goes back to tokenization which we

00:35:49,849 --> 00:35:54,200
still have to figure out how about

00:35:52,250 --> 00:35:59,140
performance well when we started we were

00:35:54,200 --> 00:36:02,300
on solar 4.8 and it was horribly slow

00:35:59,140 --> 00:36:07,520
then we officially upgraded to solar 5.3

00:36:02,300 --> 00:36:10,190
and things got miraculously better so

00:36:07,520 --> 00:36:12,410
today we're on solar 6.3 and we've

00:36:10,190 --> 00:36:14,510
achieved an average latency of 20

00:36:12,410 --> 00:36:17,500
milliseconds per story which is a nice

00:36:14,510 --> 00:36:23,119
improvement over the the old system and

00:36:17,500 --> 00:36:25,760
for large the larger stories we're way

00:36:23,119 --> 00:36:27,470
ahead the maximum latency of a few

00:36:25,760 --> 00:36:31,910
seconds as opposed to a few tens of

00:36:27,470 --> 00:36:34,820
seconds the new system handles load as

00:36:31,910 --> 00:36:37,099
well or better than the old system the

00:36:34,820 --> 00:36:40,609
only caveat is that it does use twice as

00:36:37,099 --> 00:36:42,200
much hardware and probably we could have

00:36:40,609 --> 00:36:44,240
got by it with a little bit less than

00:36:42,200 --> 00:36:45,950
that we put in the hard order order when

00:36:44,240 --> 00:36:46,970
we were on in solar 4.8 and we really

00:36:45,950 --> 00:36:50,359
thought we were going to need a lot

00:36:46,970 --> 00:36:52,339
that's a you know it's teardown so we're

00:36:50,359 --> 00:36:56,330
going to use it

00:36:52,339 --> 00:36:58,480
the only drawback to the Luwak system

00:36:56,330 --> 00:37:02,330
and then it starts up much slower than

00:36:58,480 --> 00:37:06,230
the old system barity had some magic

00:37:02,330 --> 00:37:09,200
where you could create a binary version

00:37:06,230 --> 00:37:11,720
of the query data offline so that when

00:37:09,200 --> 00:37:14,109
the classifier starts up it's ready to

00:37:11,720 --> 00:37:17,119
receive requests almost instantaneously

00:37:14,109 --> 00:37:19,550
with Lueck we have to index the queries

00:37:17,119 --> 00:37:22,280
every time we start a process and that

00:37:19,550 --> 00:37:23,869
can take a few minutes in our case but

00:37:22,280 --> 00:37:26,000
it's not you know it's not a big deal

00:37:23,869 --> 00:37:29,450
because we have a high degree of

00:37:26,000 --> 00:37:32,450
replication we can restart processes

00:37:29,450 --> 00:37:37,390
when we need to without causing any

00:37:32,450 --> 00:37:37,390
hiccups so that's it

00:37:37,540 --> 00:37:50,140
[Applause]

00:37:45,630 --> 00:37:54,309
cool thank you for the talk and now we

00:37:50,140 --> 00:37:56,970
have time for two free questions okay we

00:37:54,309 --> 00:37:56,970
have one here

00:37:58,980 --> 00:38:06,520
hello you explained how did you how

00:38:03,220 --> 00:38:09,369
ordering of spawnposition queue helps

00:38:06,520 --> 00:38:12,339
you fix the okay so Nestor it's funny

00:38:09,369 --> 00:38:14,589
you should read that is that that ticket

00:38:12,339 --> 00:38:16,030
so actually we figured this I think it's

00:38:14,589 --> 00:38:17,859
kind of obvious if you if you're like in

00:38:16,030 --> 00:38:20,589
a debugger and see what's going on

00:38:17,859 --> 00:38:22,390
we figured it out kind of on our own

00:38:20,589 --> 00:38:25,089
before I was even aware of this ticket

00:38:22,390 --> 00:38:27,160
but there's a comment by Alan actually

00:38:25,089 --> 00:38:28,930
about this in the ticket although it's

00:38:27,160 --> 00:38:32,170
not been submitted as a package for us I

00:38:28,930 --> 00:38:35,170
I know so the the spam visit so you sort

00:38:32,170 --> 00:38:37,210
by start position first the Louis start

00:38:35,170 --> 00:38:39,400
position and this is just a tie the

00:38:37,210 --> 00:38:42,010
existing code says lowest in position

00:38:39,400 --> 00:38:43,569
well you don't really what you want you

00:38:42,010 --> 00:38:46,390
really want to make it that big the

00:38:43,569 --> 00:38:49,510
greatest in positions and if you do that

00:38:46,390 --> 00:38:51,099
just you know changing a less-than side

00:38:49,510 --> 00:38:53,770
to a greater than sign everything much

00:38:51,099 --> 00:38:55,420
is overlapping you might actually okay

00:38:53,770 --> 00:38:57,190
there's a problem with overlapping stuff

00:38:55,420 --> 00:39:00,069
too and I said that you know this

00:38:57,190 --> 00:39:03,010
resolved most of our issues but not all

00:39:00,069 --> 00:39:05,079
so the other thing we do we kind of get

00:39:03,010 --> 00:39:08,829
get around some of the overlapping stuff

00:39:05,079 --> 00:39:11,440
but we don't we try to avoid in position

00:39:08,829 --> 00:39:15,220
as much as possible and just use start

00:39:11,440 --> 00:39:16,779
positions and that way we kind of but we

00:39:15,220 --> 00:39:20,020
still have issues with the end the

00:39:16,779 --> 00:39:23,069
overlapping stuff so there's something I

00:39:20,020 --> 00:39:26,440
didn't show here but you can have a near

00:39:23,069 --> 00:39:28,599
in OTL you can have a near of an all of

00:39:26,440 --> 00:39:30,579
that basically conjunction of you know

00:39:28,599 --> 00:39:33,309
so you can have all these terms you want

00:39:30,579 --> 00:39:35,260
to near a certain term right and the

00:39:33,309 --> 00:39:37,839
certain terms you want near they all can

00:39:35,260 --> 00:39:40,000
be in the middle of your all and then

00:39:37,839 --> 00:39:42,220
things get really strange we that's what

00:39:40,000 --> 00:39:44,270
the legacy system the very system

00:39:42,220 --> 00:39:48,560
totally cheated on that

00:39:44,270 --> 00:39:49,910
so they decided to forget about how

00:39:48,560 --> 00:39:52,190
things appear in the document and just

00:39:49,910 --> 00:39:55,180
they use the order things appear in the

00:39:52,190 --> 00:39:57,440
query right which is totally wrong but

00:39:55,180 --> 00:39:59,360
but yeah we do something slightly

00:39:57,440 --> 00:40:02,570
different that kind of gets that thing

00:39:59,360 --> 00:40:04,160
half right half the time and not the

00:40:02,570 --> 00:40:05,920
other half but we're still like hoping

00:40:04,160 --> 00:40:10,160
some some progress happens on that

00:40:05,920 --> 00:40:12,080
nested span ticket how actually deep

00:40:10,160 --> 00:40:15,590
your span core is like the the resulting

00:40:12,080 --> 00:40:18,230
span queries yeah well how large are

00:40:15,590 --> 00:40:20,780
they how how large are they resulting

00:40:18,230 --> 00:40:22,450
will be generated upon queries are like

00:40:20,780 --> 00:40:24,740
I'll be there

00:40:22,450 --> 00:40:28,430
howdy I mean I'm not sure what you're

00:40:24,740 --> 00:40:30,620
asking so yeah so we have span queries

00:40:28,430 --> 00:40:33,800
containing like you know not just like

00:40:30,620 --> 00:40:36,020
terms but like whole query trees so

00:40:33,800 --> 00:40:37,580
these content indexers you know they

00:40:36,020 --> 00:40:39,950
know their domain but they don't

00:40:37,580 --> 00:40:43,010
necessarily know how to write it

00:40:39,950 --> 00:40:46,100
optimize query so they write some really

00:40:43,010 --> 00:40:48,380
crazy looking stuff right and so we get

00:40:46,100 --> 00:40:51,050
some very hairy looking span queries in

00:40:48,380 --> 00:40:55,400
our system one more question and then we

00:40:51,050 --> 00:40:57,740
have break thanks for your talk I was

00:40:55,400 --> 00:40:59,720
just curious like this is a really a

00:40:57,740 --> 00:41:01,790
tough problem like how would you rate

00:40:59,720 --> 00:41:03,980
this one compared to like other

00:41:01,790 --> 00:41:05,750
companies for example Flipboard they're

00:41:03,980 --> 00:41:09,080
actually kind of clustering like this

00:41:05,750 --> 00:41:10,940
real-time streaming of documents and how

00:41:09,080 --> 00:41:14,180
would you say like any other methods

00:41:10,940 --> 00:41:16,130
versus this kind of look like like index

00:41:14,180 --> 00:41:17,600
so it's yeah okay that is the main thing

00:41:16,130 --> 00:41:19,460
is this this is a rules-based system

00:41:17,600 --> 00:41:22,070
right if you're going to do that yeah I

00:41:19,460 --> 00:41:24,440
think you know Lueck is like you know

00:41:22,070 --> 00:41:27,230
I'm not sure exactly how very that's

00:41:24,440 --> 00:41:29,000
proprietary but I'm guessing that you

00:41:27,230 --> 00:41:31,550
know under the hood it's doing something

00:41:29,000 --> 00:41:33,020
very similar to what Lueck does so I

00:41:31,550 --> 00:41:34,940
think this is like the way to go it's

00:41:33,020 --> 00:41:36,740
kind of like the obvious thing to do now

00:41:34,940 --> 00:41:38,690
in Bloomberg this is not the only

00:41:36,740 --> 00:41:41,690
classification system so there's a lot

00:41:38,690 --> 00:41:44,990
of work going around machine learning

00:41:41,690 --> 00:41:46,550
techniques for classification you know

00:41:44,990 --> 00:41:49,930
that's still kind of like not quite

00:41:46,550 --> 00:41:52,220
right for us not mature enough to

00:41:49,930 --> 00:41:54,170
completely use in the production so we

00:41:52,220 --> 00:41:56,040
really rely heavily on rules-based but

00:41:54,170 --> 00:41:59,910
yeah we're looking at other

00:41:56,040 --> 00:42:02,710
other approaches inside Bloomberg cool

00:41:59,910 --> 00:42:08,760
thank you for the talk again

00:42:02,710 --> 00:42:08,760

YouTube URL: https://www.youtube.com/watch?v=5GLlzlDYqg8


