Title: Berlin Buzzwords 2017: Michael HÃ¤usler - Integration Patterns for Big Data Applications #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Big Data technologies like distributed databases, queues, batch processors, and stream processors are fun and exciting to play with. Making them play nicely together can be challenging. Keeping it fun for engineers to continuously improve and operate them is hard. At ResearchGate, we run thousands of YARN applications every day to gain insights and to power user facing features. Of course, there are numerous integration challenges on the way:

- integrating batch and stream processors with operational systems
- ingesting data and playing back results while controlling performance crosstalk
- rolling out new versions of synchronous, stream, and batch applications and their respective data schemas
- controlling the amount of glue and adapter code between different technologies
- modeling cross-flow dependencies while handling failures gracefully and limiting their repercussions

In this talk we will discuss how ResearchGate has tackled those problems. We describe our ongoing journey in identifying patterns and principles to make our big data stack integrate well. Technologies to be covered will include MongoDB, Kafka, Hadoop (YARN), Hive (TEZ), Flink Batch, and Flink Streaming.

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,609 --> 00:00:10,310
thank you so much

00:00:08,180 --> 00:00:12,770
for the big audience here I'm super

00:00:10,310 --> 00:00:15,140
excited and to tell you a little bit

00:00:12,770 --> 00:00:16,900
about the big data integration patterns

00:00:15,140 --> 00:00:18,110
that we came up at ResearchGate

00:00:16,900 --> 00:00:20,300
ResearchGate

00:00:18,110 --> 00:00:26,090
is a professional network for scientists

00:00:20,300 --> 00:00:28,100
and we we research gate is built for

00:00:26,090 --> 00:00:31,070
scientists and give them the tools to

00:00:28,100 --> 00:00:36,410
connect to collaborate and to discover

00:00:31,070 --> 00:00:39,350
relevant research we have our mission is

00:00:36,410 --> 00:00:41,420
to connect the world of science and

00:00:39,350 --> 00:00:43,310
really make research open to all make

00:00:41,420 --> 00:00:47,690
all the research artifacts accessible

00:00:43,310 --> 00:00:50,059
and so far we have 12 million scientists

00:00:47,690 --> 00:00:53,180
who are registered users for such gate

00:00:50,059 --> 00:00:55,910
and our database contains more than 100

00:00:53,180 --> 00:00:57,920
million publications and so far we have

00:00:55,910 --> 00:01:00,170
discovered more than 1.5 billion

00:00:57,920 --> 00:01:02,930
citations that connect those

00:01:00,170 --> 00:01:05,390
publications so a big part of what we

00:01:02,930 --> 00:01:07,280
are doing is finding those connections

00:01:05,390 --> 00:01:10,100
with connections between publications

00:01:07,280 --> 00:01:12,890
between users between research topics

00:01:10,100 --> 00:01:15,260
and that that's what we are doing and

00:01:12,890 --> 00:01:17,260
that means big data processes in data

00:01:15,260 --> 00:01:20,000
processing is a large part of it

00:01:17,260 --> 00:01:21,619
analyzing that scientific content and

00:01:20,000 --> 00:01:25,700
then in the end making good

00:01:21,619 --> 00:01:28,909
recommendations to our users for that we

00:01:25,700 --> 00:01:33,110
started pretty early with doing Hadoop

00:01:28,909 --> 00:01:35,990
and other big data processing and we run

00:01:33,110 --> 00:01:37,759
two hadoop clusters in production one

00:01:35,990 --> 00:01:39,950
which we call the last cluster the

00:01:37,759 --> 00:01:42,259
near-real-time cluster which is

00:01:39,950 --> 00:01:44,750
optimized for latency and mainly ran a

00:01:42,259 --> 00:01:47,150
running HBase and flink streaming and we

00:01:44,750 --> 00:01:49,700
have the batch cluster so-called

00:01:47,150 --> 00:01:53,390
analytics cluster which mainly runs

00:01:49,700 --> 00:01:56,240
MapReduce hive and flink batch and both

00:01:53,390 --> 00:01:59,380
of these clusters are in the petabyte

00:01:56,240 --> 00:02:02,450
range so when we are talking about those

00:01:59,380 --> 00:02:05,390
applications so what we are doing we are

00:02:02,450 --> 00:02:08,350
running over 3,000 Yarn applications on

00:02:05,390 --> 00:02:12,590
a normal day roughly 70% of them are

00:02:08,350 --> 00:02:14,930
scheduled um applications that run every

00:02:12,590 --> 00:02:18,190
day and roughly 30 percent are

00:02:14,930 --> 00:02:22,210
interactive ad-hoc analysis

00:02:18,190 --> 00:02:25,510
and we do ingest more than 370 different

00:02:22,210 --> 00:02:27,310
data sources every day and we're 65

00:02:25,510 --> 00:02:31,600
engineers and that is everyone from

00:02:27,310 --> 00:02:35,020
front end to system operations and this

00:02:31,600 --> 00:02:37,090
means a lot of jobs quite a few

00:02:35,020 --> 00:02:38,860
engineers but this is still an amazing

00:02:37,090 --> 00:02:42,220
amount of work so what you think a lot

00:02:38,860 --> 00:02:44,470
about is develop a productivity so we

00:02:42,220 --> 00:02:47,860
want it to be fun to develop new jobs

00:02:44,470 --> 00:02:50,230
you should get your results quickly but

00:02:47,860 --> 00:02:51,910
it should also be fun to upgrade the job

00:02:50,230 --> 00:02:53,620
later when you want to build a new

00:02:51,910 --> 00:02:55,840
feature for it so ease of maintenance is

00:02:53,620 --> 00:02:59,140
very important and use of operations

00:02:55,840 --> 00:03:01,990
because no one wants to get woken up and

00:02:59,140 --> 00:03:06,000
wants to get cold at night if one of

00:03:01,990 --> 00:03:09,790
those 3,000 jobs might have a hiccup so

00:03:06,000 --> 00:03:11,950
we come up with some integration

00:03:09,790 --> 00:03:14,890
patterns and principles how we build our

00:03:11,950 --> 00:03:17,500
workflows and they are all about

00:03:14,890 --> 00:03:20,200
productivity about easy collaboration

00:03:17,500 --> 00:03:23,140
between teams basically coming up with

00:03:20,200 --> 00:03:25,300
building blocks that make it really easy

00:03:23,140 --> 00:03:28,390
and fun to build bigger systems out of

00:03:25,300 --> 00:03:31,000
it so speaking of reuse and developers

00:03:28,390 --> 00:03:32,770
should be able to reuse results from

00:03:31,000 --> 00:03:36,760
everywhere from other teams be it

00:03:32,770 --> 00:03:39,400
software or be it data yeah these

00:03:36,760 --> 00:03:41,530
patterns though of course they need to

00:03:39,400 --> 00:03:44,050
be strategic but they should really be

00:03:41,530 --> 00:03:46,600
driven by real-world use cases and they

00:03:44,050 --> 00:03:48,280
should really be driven by real-world

00:03:46,600 --> 00:03:50,440
pain points this is what you want to

00:03:48,280 --> 00:03:53,100
solve with them because that is what

00:03:50,440 --> 00:03:55,900
then developer productivity is about and

00:03:53,100 --> 00:04:01,209
they should not be dictated by a single

00:03:55,900 --> 00:04:02,860
technology so we try to focus on by with

00:04:01,209 --> 00:04:05,200
these principles on more the abstract

00:04:02,860 --> 00:04:08,440
level and really think about what's the

00:04:05,200 --> 00:04:10,480
idea behind it and not go too much into

00:04:08,440 --> 00:04:13,150
like how do we do with this with this

00:04:10,480 --> 00:04:15,910
specific framework and why is it so big

00:04:13,150 --> 00:04:18,250
data is still a fast-moving space like

00:04:15,910 --> 00:04:20,260
the data batch processing today is quite

00:04:18,250 --> 00:04:23,890
different to compared to five years ago

00:04:20,260 --> 00:04:27,550
and five years ago 2012 I still remember

00:04:23,890 --> 00:04:30,100
it was my first time to attend buzzwords

00:04:27,550 --> 00:04:31,700
as a guest and I still can wizardly

00:04:30,100 --> 00:04:33,500
recall the discussions that

00:04:31,700 --> 00:04:35,870
we're having service hey should we use

00:04:33,500 --> 00:04:38,690
high it or should we use pig for example

00:04:35,870 --> 00:04:41,210
or should we use HBase coprocessors or

00:04:38,690 --> 00:04:43,570
it was hey what is the best client

00:04:41,210 --> 00:04:46,850
library to talk to zookeeper with and

00:04:43,570 --> 00:04:49,340
that's not questions that keep us awake

00:04:46,850 --> 00:04:51,950
at night today so this whole space has

00:04:49,340 --> 00:04:55,550
evolved quickly so many new frameworks

00:04:51,950 --> 00:04:57,410
have happened and we are in a quite

00:04:55,550 --> 00:04:57,920
different world today than it was five

00:04:57,410 --> 00:05:01,130
years ago

00:04:57,920 --> 00:05:05,810
and while the batch processing world is

00:05:01,130 --> 00:05:07,850
slowly maturing and the speed of

00:05:05,810 --> 00:05:09,560
innovation goes a little bit down with

00:05:07,850 --> 00:05:11,090
the stream processing world it's we're

00:05:09,560 --> 00:05:12,530
still in the middle of it this is what's

00:05:11,090 --> 00:05:15,050
happening right now this is really

00:05:12,530 --> 00:05:19,280
exciting and what's happening today with

00:05:15,050 --> 00:05:21,020
streams will probably influence for

00:05:19,280 --> 00:05:24,650
example SQL on streams will influence

00:05:21,020 --> 00:05:26,480
what we will run on top of stream

00:05:24,650 --> 00:05:29,230
processors instead of batch processors

00:05:26,480 --> 00:05:31,430
for the upcoming years so this is

00:05:29,230 --> 00:05:35,290
evolving and that means the big data

00:05:31,430 --> 00:05:40,070
architecture also must evolve over time

00:05:35,290 --> 00:05:42,320
so as I said we wanted it to be use case

00:05:40,070 --> 00:05:44,120
driven and that's why I want to very

00:05:42,320 --> 00:05:47,270
quickly tell you about our first big

00:05:44,120 --> 00:05:49,220
data use case that we had early 2011 we

00:05:47,270 --> 00:05:51,080
implemented author analysis on her group

00:05:49,220 --> 00:05:54,440
back in the day that was Java MapReduce

00:05:51,080 --> 00:05:56,750
because few options available and since

00:05:54,440 --> 00:05:59,150
then early 2011 we're running Hadoop in

00:05:56,750 --> 00:06:01,190
production the problem is easily

00:05:59,150 --> 00:06:03,440
described you have two publications and

00:06:01,190 --> 00:06:05,420
imagine that both publications have a

00:06:03,440 --> 00:06:08,450
common author in this case for example

00:06:05,420 --> 00:06:10,850
yet modish and you want to know is this

00:06:08,450 --> 00:06:12,410
the same person or are these two

00:06:10,850 --> 00:06:14,900
different people who just could

00:06:12,410 --> 00:06:16,730
instantly share the same name so this is

00:06:14,900 --> 00:06:18,920
kind of a clustering problem a

00:06:16,730 --> 00:06:21,020
classification problem also named

00:06:18,920 --> 00:06:23,510
disambiguation problem all rolled up

00:06:21,020 --> 00:06:25,820
into one and it is a super high product

00:06:23,510 --> 00:06:28,010
impact is a user facing feature because

00:06:25,820 --> 00:06:30,050
if you find this out correctly then we

00:06:28,010 --> 00:06:32,710
can build beautiful publication detail

00:06:30,050 --> 00:06:35,450
pages where we connect the scientific

00:06:32,710 --> 00:06:40,190
content directly to the users who

00:06:35,450 --> 00:06:42,410
created it so one thing that's maybe a

00:06:40,190 --> 00:06:44,540
bit different from other first use cases

00:06:42,410 --> 00:06:44,969
with Hadoop is that we are talking about

00:06:44,540 --> 00:06:49,019
in

00:06:44,969 --> 00:06:50,819
reaching user-generated content so the

00:06:49,019 --> 00:06:52,589
users are creating those publications

00:06:50,819 --> 00:06:54,419
and we want to enrich them with some

00:06:52,589 --> 00:06:56,669
information so they're both working on

00:06:54,419 --> 00:06:58,919
the same data set and that's a little

00:06:56,669 --> 00:07:00,569
bit different from an analytical use

00:06:58,919 --> 00:07:02,039
case if you would have analytical use

00:07:00,569 --> 00:07:03,779
case you would start with users

00:07:02,039 --> 00:07:05,879
producing some data in your life

00:07:03,779 --> 00:07:07,499
database you would ingest that to your

00:07:05,879 --> 00:07:09,779
Hadoop cluster then you would run some

00:07:07,499 --> 00:07:12,509
analytics on it and that would in the

00:07:09,779 --> 00:07:15,029
result go for example to your bi tool or

00:07:12,509 --> 00:07:17,339
if you're for example building a

00:07:15,029 --> 00:07:19,999
recommendation model then again the

00:07:17,339 --> 00:07:22,979
users would click on your site you would

00:07:19,999 --> 00:07:26,369
record interesting signals on the live

00:07:22,979 --> 00:07:28,409
database and you would ingest these data

00:07:26,369 --> 00:07:30,209
ingest the signals and features build a

00:07:28,409 --> 00:07:32,999
recommendations model in Hadoop for

00:07:30,209 --> 00:07:35,039
example ship that model off again to

00:07:32,999 --> 00:07:37,469
production but it's not going to the

00:07:35,039 --> 00:07:40,110
same live database it's sitting next to

00:07:37,469 --> 00:07:42,439
it right that was a little bit of a

00:07:40,110 --> 00:07:45,959
special thing that we have to do there

00:07:42,439 --> 00:07:47,519
very quickly about the data model so

00:07:45,959 --> 00:07:49,619
this is just a small part of the data

00:07:47,519 --> 00:07:51,509
model at ResearchGate and what they're

00:07:49,619 --> 00:07:53,659
talking about basically is the

00:07:51,509 --> 00:07:57,719
connection here between publications

00:07:53,659 --> 00:07:59,759
authors and accounts and these live in

00:07:57,719 --> 00:08:02,610
and are owned by different micro

00:07:59,759 --> 00:08:04,919
services which means our first

00:08:02,610 --> 00:08:07,050
implementation looked a little bit like

00:08:04,919 --> 00:08:09,959
this you have data owned by different

00:08:07,050 --> 00:08:14,129
like services and we are ingesting that

00:08:09,959 --> 00:08:16,399
and we are doing offer analysis I'm

00:08:14,129 --> 00:08:19,110
cheating a little bit here this is an

00:08:16,399 --> 00:08:20,579
oversimplified representation what it

00:08:19,110 --> 00:08:24,360
really looked like was more like this

00:08:20,579 --> 00:08:26,729
but for the sake of this talk let's keep

00:08:24,360 --> 00:08:28,889
it at the simple level so what's going

00:08:26,729 --> 00:08:31,349
on here well you have the data sources

00:08:28,889 --> 00:08:33,689
and you ingest them which gives you some

00:08:31,349 --> 00:08:36,750
input data in your analytical system in

00:08:33,689 --> 00:08:38,939
our case HDFS and you running some data

00:08:36,750 --> 00:08:41,669
processing on top of it which gives you

00:08:38,939 --> 00:08:43,740
some intermediate results and then you

00:08:41,669 --> 00:08:44,970
might for example run at this one to see

00:08:43,740 --> 00:08:46,860
which of these results are really

00:08:44,970 --> 00:08:49,079
relevant for production and then your

00:08:46,860 --> 00:08:52,019
final results and you're exporting those

00:08:49,079 --> 00:08:53,970
results and it was fine for the very

00:08:52,019 --> 00:08:56,339
first use case but you can already see

00:08:53,970 --> 00:08:58,819
that some things are not optimal and

00:08:56,339 --> 00:09:02,299
that we quickly try to tackle and

00:08:58,819 --> 00:09:05,329
six and that was you should always be

00:09:02,299 --> 00:09:10,939
cup of data ingestion so what I mean by

00:09:05,329 --> 00:09:13,160
that you have here this flow and it kind

00:09:10,939 --> 00:09:15,379
of makes sense it was the first up flow

00:09:13,160 --> 00:09:16,910
that we filled there was no data in the

00:09:15,379 --> 00:09:19,069
cluster so the team who built the flow

00:09:16,910 --> 00:09:22,160
also built the data ingestion and just

00:09:19,069 --> 00:09:23,989
made it one right but that's not optimal

00:09:22,160 --> 00:09:25,819
let's say for example that one service

00:09:23,989 --> 00:09:27,799
is using false positive as a database

00:09:25,819 --> 00:09:30,709
and the other is using MongoDB as a

00:09:27,799 --> 00:09:32,629
database then that should be really an

00:09:30,709 --> 00:09:34,789
implementation secret of that service

00:09:32,629 --> 00:09:36,289
your flow should not be concerned about

00:09:34,789 --> 00:09:38,179
that and you should not have to

00:09:36,289 --> 00:09:40,100
implement connectors to all the

00:09:38,179 --> 00:09:42,829
different databases all over again and

00:09:40,100 --> 00:09:45,229
worst case be restricted in the choice

00:09:42,829 --> 00:09:46,910
of your processing framework because one

00:09:45,229 --> 00:09:49,039
processing framework might have a

00:09:46,910 --> 00:09:53,899
connector for your favorite database and

00:09:49,039 --> 00:09:56,089
the other might not so for another thing

00:09:53,899 --> 00:09:58,249
these imports are usually quite

00:09:56,089 --> 00:10:00,259
expensive so you want to reuse them as

00:09:58,249 --> 00:10:03,709
much as possible and one thing that

00:10:00,259 --> 00:10:06,139
surprised us a little bit and was it is

00:10:03,709 --> 00:10:08,119
really hard to the back if you don't

00:10:06,139 --> 00:10:10,999
separate that and this is what I want to

00:10:08,119 --> 00:10:13,999
explain a little bit more so imagine you

00:10:10,999 --> 00:10:16,399
have written a job and you did all the

00:10:13,999 --> 00:10:18,199
things in the right way so this means

00:10:16,399 --> 00:10:21,189
you have perfect unit tests and

00:10:18,199 --> 00:10:24,499
integration tests but still suddenly

00:10:21,189 --> 00:10:27,079
your job breaks on production and then

00:10:24,499 --> 00:10:28,970
how are you tackling this how are you

00:10:27,079 --> 00:10:31,519
finding the root cause and there are so

00:10:28,970 --> 00:10:33,139
many options is it a new constellation

00:10:31,519 --> 00:10:36,259
in your input data that you just never

00:10:33,139 --> 00:10:38,359
save seen before or it is a change on

00:10:36,259 --> 00:10:41,179
the cluster did someone roll out a

00:10:38,359 --> 00:10:43,819
security fix or change the configuration

00:10:41,179 --> 00:10:46,909
or is it a race condition that always

00:10:43,819 --> 00:10:48,889
was there but this day is just the very

00:10:46,909 --> 00:10:52,549
first time that you've observed that

00:10:48,889 --> 00:10:54,529
race condition and if you want to be

00:10:52,549 --> 00:10:57,499
back that root cause you need to have

00:10:54,529 --> 00:11:00,350
some crucial operational capabilities so

00:10:57,499 --> 00:11:03,619
it should be super easy to do a clock

00:11:00,350 --> 00:11:05,600
analysis of all the involves data you if

00:11:03,619 --> 00:11:07,579
you try to find out what the doc is you

00:11:05,600 --> 00:11:09,649
want to be able to run statistics on

00:11:07,579 --> 00:11:12,700
your input data on your intermediate

00:11:09,649 --> 00:11:16,280
results and on your final results

00:11:12,700 --> 00:11:18,290
even more important if the flow broke

00:11:16,280 --> 00:11:22,040
today but it worked very well yesterday

00:11:18,290 --> 00:11:24,350
what you want to be able to do is rerun

00:11:22,040 --> 00:11:26,780
with the current configuration of your

00:11:24,350 --> 00:11:29,960
cluster with the current state of the

00:11:26,780 --> 00:11:33,260
world rerun the same flow on yesterday's

00:11:29,960 --> 00:11:35,780
data and because with that you can rule

00:11:33,260 --> 00:11:38,750
out that it's a data issue and if you

00:11:35,780 --> 00:11:40,850
finally found and fix the bug you and

00:11:38,750 --> 00:11:43,970
you have come up with a hotfix you want

00:11:40,850 --> 00:11:47,420
to rerun it on today's data but you want

00:11:43,970 --> 00:11:49,970
to run it on exactly the same data that

00:11:47,420 --> 00:11:53,360
the first iteration run on because that

00:11:49,970 --> 00:11:55,340
might have triggered this might have

00:11:53,360 --> 00:11:58,730
less constellation you want to know for

00:11:55,340 --> 00:12:00,560
sure that this is fixed and you can

00:11:58,730 --> 00:12:04,190
sleep well

00:12:00,560 --> 00:12:06,260
during the next night so how do we

00:12:04,190 --> 00:12:08,420
decouple I mean we already said we don't

00:12:06,260 --> 00:12:11,420
want the flow to own the data ingestion

00:12:08,420 --> 00:12:13,370
some could it now argue well if the

00:12:11,420 --> 00:12:15,280
database is an implementation secret of

00:12:13,370 --> 00:12:17,930
the service the service should be

00:12:15,280 --> 00:12:23,270
responsible for pushing that information

00:12:17,930 --> 00:12:25,520
and that's not what we decided for so we

00:12:23,270 --> 00:12:28,910
decided for a dedicated data ingestion

00:12:25,520 --> 00:12:30,500
component and so the advantage here is

00:12:28,910 --> 00:12:32,440
that that flows don't have to have

00:12:30,500 --> 00:12:35,390
connectors for all the different

00:12:32,440 --> 00:12:37,670
database technologies but also that the

00:12:35,390 --> 00:12:39,950
services do not need to know about the

00:12:37,670 --> 00:12:42,320
different Big Data technology which is

00:12:39,950 --> 00:12:45,110
currently on work and fast changing and

00:12:42,320 --> 00:12:47,990
also it allows you on a one central

00:12:45,110 --> 00:12:50,600
place to implement features like hey

00:12:47,990 --> 00:12:52,880
let's for everything that we ingest

00:12:50,600 --> 00:12:55,190
let's always mount it to hive because

00:12:52,880 --> 00:12:58,340
that will allow the kind of ad hoc

00:12:55,190 --> 00:13:06,660
analytics and it also allows then that

00:12:58,340 --> 00:13:10,500
data to be reused by many flows okay

00:13:06,660 --> 00:13:13,230
so the important thing though is if you

00:13:10,500 --> 00:13:16,410
have such a dedicated component it needs

00:13:13,230 --> 00:13:18,900
to be generic so every team who hasn't

00:13:16,410 --> 00:13:21,420
need to ingest a new data source should

00:13:18,900 --> 00:13:24,450
be able to get it ingested so you don't

00:13:21,420 --> 00:13:26,580
want every request for data ingestion to

00:13:24,450 --> 00:13:29,700
be handled handed over to a central data

00:13:26,580 --> 00:13:31,380
engineering team and sit there in the in

00:13:29,700 --> 00:13:34,440
the gyro board for a few weeks until

00:13:31,380 --> 00:13:37,020
that is done you want every team to

00:13:34,440 --> 00:13:40,560
contribute to this central platform data

00:13:37,020 --> 00:13:42,870
import and every ingested data should

00:13:40,560 --> 00:13:45,720
immediately be available to other

00:13:42,870 --> 00:13:48,150
consumers as well so that once you have

00:13:45,720 --> 00:13:51,330
done the work of integrating it it

00:13:48,150 --> 00:13:53,760
should be available to any use case with

00:13:51,330 --> 00:13:56,600
any framework and including analytics

00:13:53,760 --> 00:13:59,490
and as we said before if you have this

00:13:56,600 --> 00:14:01,860
dedicated component you can easily have

00:13:59,490 --> 00:14:05,810
feature parity for all data sources for

00:14:01,860 --> 00:14:09,090
example mounting everything in hive so

00:14:05,810 --> 00:14:11,850
the next thing that we quickly realized

00:14:09,090 --> 00:14:15,120
is the importance of speaking a common

00:14:11,850 --> 00:14:17,670
format and what I mean with that is have

00:14:15,120 --> 00:14:19,530
at least one copy of all data in a

00:14:17,670 --> 00:14:22,020
comment format it's okay to have

00:14:19,530 --> 00:14:25,530
multiple formats but you should always

00:14:22,020 --> 00:14:26,610
have this common ground and formats

00:14:25,530 --> 00:14:28,380
ResearchGate

00:14:26,610 --> 00:14:30,450
developed very very quickly when we

00:14:28,380 --> 00:14:34,740
started first we use texts because that

00:14:30,450 --> 00:14:37,370
was what all the hello world and hello

00:14:34,740 --> 00:14:41,040
world in that case was word count

00:14:37,370 --> 00:14:43,770
examples did but it has low productivity

00:14:41,040 --> 00:14:46,050
low performance and it gave us a lot of

00:14:43,770 --> 00:14:48,480
bugs so we very quickly explained in

00:14:46,050 --> 00:14:50,070
exchange that the sequence files which

00:14:48,480 --> 00:14:52,950
gave us better performance and fewer

00:14:50,070 --> 00:14:55,530
bugs but also developer productivity and

00:14:52,950 --> 00:14:58,620
then we very very quickly moved over to

00:14:55,530 --> 00:15:01,560
a burrow and that was really helpful and

00:14:58,620 --> 00:15:03,300
it has an amazing set of features if you

00:15:01,560 --> 00:15:05,640
really look into it which you can build

00:15:03,300 --> 00:15:07,530
other integrational patterns on top of

00:15:05,640 --> 00:15:10,200
it so the schema evolution is really

00:15:07,530 --> 00:15:13,470
nice the fact that it's self describing

00:15:10,200 --> 00:15:16,140
is amazing because it's trivial to mount

00:15:13,470 --> 00:15:19,769
another data set in Drive for example

00:15:16,140 --> 00:15:22,050
and have full schema feed allergy

00:15:19,769 --> 00:15:24,239
it has a reflect date on reader which is

00:15:22,050 --> 00:15:27,389
really really underestimated in my

00:15:24,239 --> 00:15:28,949
opinion and it's flexible because of its

00:15:27,389 --> 00:15:32,040
robust nature you can use it for both

00:15:28,949 --> 00:15:33,839
batch and streaming so but we didn't

00:15:32,040 --> 00:15:35,550
want to stop there we wanted more we

00:15:33,839 --> 00:15:38,790
wanted better performance better

00:15:35,550 --> 00:15:41,160
compression that's why we added OSC as

00:15:38,790 --> 00:15:46,079
an additional format but we didn't drop

00:15:41,160 --> 00:15:48,029
the other one so you you can add

00:15:46,079 --> 00:15:50,220
additional formats that have specific

00:15:48,029 --> 00:15:53,189
features that are great for certain use

00:15:50,220 --> 00:15:58,610
cases pH for certain frameworks but

00:15:53,189 --> 00:16:02,279
still keep this common ground format so

00:15:58,610 --> 00:16:05,040
we think OSE is great for batch and we

00:16:02,279 --> 00:16:07,499
would love every framework to support it

00:16:05,040 --> 00:16:09,209
but that does not mean that we will drop

00:16:07,499 --> 00:16:11,910
Avro because that is still this one

00:16:09,209 --> 00:16:14,819
thing that everyone understands and it's

00:16:11,910 --> 00:16:17,489
flexible for both batch and streaming so

00:16:14,819 --> 00:16:20,220
have at least one copy of our data in a

00:16:17,489 --> 00:16:21,989
common format which means your choice of

00:16:20,220 --> 00:16:23,879
processing framework should not be

00:16:21,989 --> 00:16:26,279
limited by the format of existing data

00:16:23,879 --> 00:16:28,619
so if one team says hey we want to use

00:16:26,279 --> 00:16:30,600
this and for that we are importing it in

00:16:28,619 --> 00:16:32,610
parquet that's totally fine but still

00:16:30,600 --> 00:16:35,670
get the other one so that you can reuse

00:16:32,610 --> 00:16:37,829
the data in other use cases and which

00:16:35,670 --> 00:16:40,199
means then every ingested sauce will be

00:16:37,829 --> 00:16:42,360
available for all consumers and when

00:16:40,199 --> 00:16:46,850
optimizing for a framework you consider

00:16:42,360 --> 00:16:50,699
a copy the next one is a bit related

00:16:46,850 --> 00:16:53,309
that is speak a common language and what

00:16:50,699 --> 00:16:57,389
I mean by that is continuously propagate

00:16:53,309 --> 00:17:00,959
schema changes and if we go back to our

00:16:57,389 --> 00:17:02,879
previous example the question is you

00:17:00,959 --> 00:17:05,339
have some databases which have an

00:17:02,879 --> 00:17:07,199
inherent schema and some databases which

00:17:05,339 --> 00:17:09,809
are schema-less do we now have

00:17:07,199 --> 00:17:11,760
structured or unstructured data the

00:17:09,809 --> 00:17:13,829
thing is just because the data is in

00:17:11,760 --> 00:17:16,860
MongoDB does not mean it's unstructured

00:17:13,829 --> 00:17:19,199
for almost all of our use cases the

00:17:16,860 --> 00:17:21,360
service and you knows very well about

00:17:19,199 --> 00:17:24,449
the structure and knows exactly what

00:17:21,360 --> 00:17:26,069
their data looks like and so this is

00:17:24,449 --> 00:17:28,409
something we are really interested in

00:17:26,069 --> 00:17:32,550
and it goes a little bit to the data

00:17:28,409 --> 00:17:34,800
warehouse versus data Lake argument so

00:17:32,550 --> 00:17:36,720
the classic data warehouse would mean

00:17:34,800 --> 00:17:39,570
you enforce a schema at ingestion time

00:17:36,720 --> 00:17:43,440
to do this schema on right but this

00:17:39,570 --> 00:17:46,290
means you have to maintain all this ETL

00:17:43,440 --> 00:17:48,809
stuff all those transformations and the

00:17:46,290 --> 00:17:51,540
data lake in the classic sense assumes

00:17:48,809 --> 00:17:54,720
no schema at all and basically defers

00:17:51,540 --> 00:17:59,580
this task of discovering the schema to

00:17:54,720 --> 00:18:03,300
the consumer now we didn't really like

00:17:59,580 --> 00:18:05,460
both approaches because for the data

00:18:03,300 --> 00:18:07,320
warehouse it's way too expensive to

00:18:05,460 --> 00:18:10,559
Maine all these transformation processes

00:18:07,320 --> 00:18:15,120
and transformation does mean a potential

00:18:10,559 --> 00:18:17,040
loss of information of data so we didn't

00:18:15,120 --> 00:18:19,200
want to spend time on on these

00:18:17,040 --> 00:18:21,840
transformations we wanted to spend time

00:18:19,200 --> 00:18:24,000
on building great features but just as

00:18:21,840 --> 00:18:25,890
well we didn't like the data like the

00:18:24,000 --> 00:18:28,080
pure classic data Lake approach because

00:18:25,890 --> 00:18:30,540
losing schema information which is

00:18:28,080 --> 00:18:32,880
already known in a service is really bad

00:18:30,540 --> 00:18:35,490
and different consumers have to

00:18:32,880 --> 00:18:37,590
rediscover the schema over and over and

00:18:35,490 --> 00:18:39,390
over again and we don't have time for

00:18:37,590 --> 00:18:42,330
that because we want to build great

00:18:39,390 --> 00:18:44,550
features in that time instead so the

00:18:42,330 --> 00:18:47,370
question is can we have both and with

00:18:44,550 --> 00:18:50,760
both I mean the best properties of both

00:18:47,370 --> 00:18:52,559
words not the worst of course so can we

00:18:50,760 --> 00:18:54,360
preserve the schema is information that

00:18:52,559 --> 00:18:56,730
is already present sometimes the

00:18:54,360 --> 00:19:00,450
database level but in many times in our

00:18:56,730 --> 00:19:02,940
use case at the application level while

00:19:00,450 --> 00:19:05,970
at the same time always preserve the

00:19:02,940 --> 00:19:09,600
full data never lose data be truthful to

00:19:05,970 --> 00:19:12,240
our data source and by that meaning can

00:19:09,600 --> 00:19:14,280
be always propagate schema changes that

00:19:12,240 --> 00:19:17,010
you have in your source of truth

00:19:14,280 --> 00:19:19,590
database and the question is can we have

00:19:17,010 --> 00:19:24,330
something like a data lake house maybe

00:19:19,590 --> 00:19:27,870
and it turns out become so what we did

00:19:24,330 --> 00:19:30,090
is we went for a code first approach so

00:19:27,870 --> 00:19:32,100
the service knows what the data looks

00:19:30,090 --> 00:19:34,530
like and the service usually has

00:19:32,100 --> 00:19:39,030
entities which describe how that data

00:19:34,530 --> 00:19:40,830
looks like and we want those entities to

00:19:39,030 --> 00:19:42,540
define the schema and whenever a

00:19:40,830 --> 00:19:45,929
developer makes a change to those

00:19:42,540 --> 00:19:48,149
entities then we know if we can get the

00:19:45,929 --> 00:19:50,700
information from there we are always up

00:19:48,149 --> 00:19:53,970
to date so we want to automatically

00:19:50,700 --> 00:19:56,820
convert convert the schema which is

00:19:53,970 --> 00:20:00,960
inherent in the entities to another

00:19:56,820 --> 00:20:04,049
schema and that's possible and actually

00:20:00,960 --> 00:20:05,879
everyone is doing that already today so

00:20:04,049 --> 00:20:07,830
almost everyone is having some

00:20:05,879 --> 00:20:10,230
annotations in the entities of their

00:20:07,830 --> 00:20:12,509
services for Jackson for example to say

00:20:10,230 --> 00:20:15,840
how this is converted to a JSON format

00:20:12,509 --> 00:20:18,299
or you have entities from your object

00:20:15,840 --> 00:20:20,190
document meta which tell you how are you

00:20:18,299 --> 00:20:23,070
going to put this into the database and

00:20:20,190 --> 00:20:24,509
here for example you have a field and in

00:20:23,070 --> 00:20:26,429
your favorite programming language

00:20:24,509 --> 00:20:28,289
unfortunately abstract is a reserved

00:20:26,429 --> 00:20:31,080
word so you have to call it differently

00:20:28,289 --> 00:20:35,249
but in different systems you want to

00:20:31,080 --> 00:20:38,249
have it named properly and so what we

00:20:35,249 --> 00:20:40,470
did we implemented a few other mm and

00:20:38,249 --> 00:20:44,070
extra other annotations then make this

00:20:40,470 --> 00:20:47,549
really easy that's already a while ago I

00:20:44,070 --> 00:20:49,679
think it went into upper 175 so it's

00:20:47,549 --> 00:20:52,169
available to everyone so we have our

00:20:49,679 --> 00:20:54,600
name other ignore our alias a barometer

00:20:52,169 --> 00:20:57,570
and with that we can automatically

00:20:54,600 --> 00:21:06,720
generate another schema which has all

00:20:57,570 --> 00:21:08,690
that information so we want to

00:21:06,720 --> 00:21:13,019
continuously propagate schema changes

00:21:08,690 --> 00:21:16,200
which means we want to make our data

00:21:13,019 --> 00:21:19,139
ingestion process generic and driven by

00:21:16,200 --> 00:21:20,909
those other schemas which means changes

00:21:19,139 --> 00:21:23,009
in the other schemas are continuously

00:21:20,909 --> 00:21:25,379
propagated to the data ingestion process

00:21:23,009 --> 00:21:27,269
whenever you deploy an updated version

00:21:25,379 --> 00:21:30,419
of the entities in production to your

00:21:27,269 --> 00:21:33,200
service also update the other schema and

00:21:30,419 --> 00:21:36,090
the data ingestion process which means

00:21:33,200 --> 00:21:38,669
the which means the data ingestion

00:21:36,090 --> 00:21:41,249
process now can be completely generic

00:21:38,669 --> 00:21:44,429
and the way how to fetch it from the

00:21:41,249 --> 00:21:49,529
database is all encoded in in this other

00:21:44,429 --> 00:21:52,200
schema in metadata it also then allows

00:21:49,529 --> 00:21:54,059
Averell schema evolution is a is a

00:21:52,200 --> 00:21:56,669
really nice feature turned out because

00:21:54,059 --> 00:21:58,720
you don't have to upgrade all consumers

00:21:56,669 --> 00:22:00,730
straightaway you can still have cons you

00:21:58,720 --> 00:22:03,130
much out there flows out there which

00:22:00,730 --> 00:22:05,650
have the old schema deployed and our

00:22:03,130 --> 00:22:09,010
schema evolution will usually do a very

00:22:05,650 --> 00:22:11,710
good job of making your consumers read

00:22:09,010 --> 00:22:13,210
that data caveat is of course if you

00:22:11,710 --> 00:22:15,490
have a breaking change that is not

00:22:13,210 --> 00:22:17,890
covered by other schema resolution you

00:22:15,490 --> 00:22:20,890
still have to deal with that with a

00:22:17,890 --> 00:22:22,659
change management process but the

00:22:20,890 --> 00:22:24,070
important thing that we get out of this

00:22:22,659 --> 00:22:26,080
is that everyone speaks the same

00:22:24,070 --> 00:22:29,520
language and that was really surprising

00:22:26,080 --> 00:22:32,289
how much that affected the organization

00:22:29,520 --> 00:22:35,350
because the engineer who's building the

00:22:32,289 --> 00:22:37,630
service and providing for example the

00:22:35,350 --> 00:22:39,909
json api is talking about exactly the

00:22:37,630 --> 00:22:41,650
same fields as the engineers who are

00:22:39,909 --> 00:22:44,530
implementing the clients of these

00:22:41,650 --> 00:22:46,179
services so the engineers who are

00:22:44,530 --> 00:22:47,950
working with javascript for example

00:22:46,179 --> 00:22:50,350
they're also talking about exactly the

00:22:47,950 --> 00:22:52,690
same fields and engineers who are

00:22:50,350 --> 00:22:54,909
building the big data flows are talking

00:22:52,690 --> 00:22:57,789
about exactly the same fields and our

00:22:54,909 --> 00:23:00,549
product analysts which are using for

00:22:57,789 --> 00:23:03,100
example high F or ipython to do these

00:23:00,549 --> 00:23:05,860
analysis also talk about the same fields

00:23:03,100 --> 00:23:09,000
and this is of communications between

00:23:05,860 --> 00:23:11,919
the company just reduces so many

00:23:09,000 --> 00:23:14,770
potential friction and misunderstanding

00:23:11,919 --> 00:23:17,470
so one of the biggest advantages of that

00:23:14,770 --> 00:23:19,210
approach it turned out to be was the

00:23:17,470 --> 00:23:23,080
improved communication within the

00:23:19,210 --> 00:23:24,549
organization there was a small extra

00:23:23,080 --> 00:23:27,309
benefit that we did not immediately

00:23:24,549 --> 00:23:29,980
expect it's also coming out of this

00:23:27,309 --> 00:23:32,799
combination of using Averell reflex

00:23:29,980 --> 00:23:34,630
reader and using entities because if you

00:23:32,799 --> 00:23:37,090
have exactly the same schema in your

00:23:34,630 --> 00:23:39,460
life database as you have in your

00:23:37,090 --> 00:23:42,000
analytic system then you can actually

00:23:39,460 --> 00:23:45,220
share code and share a business logic

00:23:42,000 --> 00:23:48,280
when would you do this imagine you want

00:23:45,220 --> 00:23:51,100
to make a stricter validation rule on

00:23:48,280 --> 00:23:52,870
your service then you of course can

00:23:51,100 --> 00:23:54,940
deploy it there to production but you

00:23:52,870 --> 00:23:57,850
want what you want to find out is how

00:23:54,940 --> 00:24:01,080
many records will be affected by that

00:23:57,850 --> 00:24:03,700
and you can take this stricter

00:24:01,080 --> 00:24:05,559
validation rules just put it into a

00:24:03,700 --> 00:24:07,270
batch process run it over all the

00:24:05,559 --> 00:24:09,280
datasets that you have in your heart

00:24:07,270 --> 00:24:11,520
cluster and then you can get statistics

00:24:09,280 --> 00:24:13,820
what this change will do

00:24:11,520 --> 00:24:17,910
what this will mean for for your data

00:24:13,820 --> 00:24:24,710
and you can get this up front so that

00:24:17,910 --> 00:24:24,710
was speaking a common language one thing

00:24:25,100 --> 00:24:31,080
which which turned out to be really a

00:24:28,770 --> 00:24:34,680
tricky one was modeling data

00:24:31,080 --> 00:24:37,620
dependencies explicitly so we have split

00:24:34,680 --> 00:24:39,690
it up now we have this explicit data

00:24:37,620 --> 00:24:42,870
ingestion process but we're still not

00:24:39,690 --> 00:24:46,020
happy and we are still not happy because

00:24:42,870 --> 00:24:49,590
of that edge that is going from the from

00:24:46,020 --> 00:24:53,550
the red box to the orange box because

00:24:49,590 --> 00:24:57,450
this is still too tightly covered what

00:24:53,550 --> 00:25:00,570
are the potential issues with that is

00:24:57,450 --> 00:25:03,120
for example you want a flow to start as

00:25:00,570 --> 00:25:05,430
early as possible but you don't know how

00:25:03,120 --> 00:25:08,310
long the import will take how we solve

00:25:05,430 --> 00:25:10,590
this is with introducing memento which

00:25:08,310 --> 00:25:13,650
is a system which can be used to publish

00:25:10,590 --> 00:25:17,940
datasets so what happens the ingestion

00:25:13,650 --> 00:25:21,630
process provides all the data and puts

00:25:17,940 --> 00:25:24,060
it into HDFS and only after it's done it

00:25:21,630 --> 00:25:29,610
notifies and signals memento that this

00:25:24,060 --> 00:25:33,540
data is really there and the consumer

00:25:29,610 --> 00:25:36,360
can call for memento and memento as soon

00:25:33,540 --> 00:25:40,710
as this data is published will notify it

00:25:36,360 --> 00:25:44,400
and then you the consumer can work with

00:25:40,710 --> 00:25:47,820
the data and read it so I'm showing here

00:25:44,400 --> 00:25:49,890
the memento version to API this is still

00:25:47,820 --> 00:25:50,670
work in progress right now at

00:25:49,890 --> 00:25:53,820
ResearchGate

00:25:50,670 --> 00:25:55,890
so you would publish metadata in the

00:25:53,820 --> 00:25:57,630
data ingestion process for example in

00:25:55,890 --> 00:25:59,370
this case we are talking about the

00:25:57,630 --> 00:26:01,590
office collection in our refined

00:25:59,370 --> 00:26:04,550
database and you would say he'll you the

00:26:01,590 --> 00:26:07,680
type is HDFS and it's well it's from

00:26:04,550 --> 00:26:11,400
basically yesterday so it's from 2017 or

00:26:07,680 --> 00:26:16,170
611 and was actually imported this

00:26:11,400 --> 00:26:18,480
morning early on at all 42 and you're

00:26:16,170 --> 00:26:20,370
publishing that and your consumer can

00:26:18,480 --> 00:26:23,160
then pull with a certain waiting time

00:26:20,370 --> 00:26:24,960
and it can describe with a language the

00:26:23,160 --> 00:26:28,649
quality of the data that

00:26:24,960 --> 00:26:31,950
to get so you say hi I want this office

00:26:28,649 --> 00:26:35,070
collection and I require data from today

00:26:31,950 --> 00:26:37,380
and I want it in the HDFS format I want

00:26:35,070 --> 00:26:41,010
just the files with their files not the

00:26:37,380 --> 00:26:42,899
highest one and what you get back is you

00:26:41,010 --> 00:26:44,730
get from a mentor as soon as this is

00:26:42,899 --> 00:26:46,649
published the information where to find

00:26:44,730 --> 00:26:49,169
this so you don't have to know about

00:26:46,649 --> 00:26:51,360
these HDFS parts in theory this could

00:26:49,169 --> 00:26:53,580
mean we can even switch completely

00:26:51,360 --> 00:26:55,529
storage like a fetch list from a

00:26:53,580 --> 00:26:59,490
different cluster fetch list from s3

00:26:55,529 --> 00:27:02,240
instead and one important thing is

00:26:59,490 --> 00:27:04,620
you're getting a unique artifact ID

00:27:02,240 --> 00:27:07,679
that's really important because you can

00:27:04,620 --> 00:27:10,370
then always refer back to that specific

00:27:07,679 --> 00:27:13,380
version of the data that you requested

00:27:10,370 --> 00:27:16,049
so what this means are more flexible

00:27:13,380 --> 00:27:19,049
scheduling running you can run flows as

00:27:16,049 --> 00:27:22,169
early as possible you can have multiple

00:27:19,049 --> 00:27:26,130
ingestion or processing attempts while

00:27:22,169 --> 00:27:28,500
at the very same time retain immutable

00:27:26,130 --> 00:27:30,570
data so imagine something goes wrong

00:27:28,500 --> 00:27:33,090
with your data ingestion pipeline and

00:27:30,570 --> 00:27:34,770
you want to rerun it then you have to

00:27:33,090 --> 00:27:36,510
make a business decision what do you do

00:27:34,770 --> 00:27:38,460
with all the flows then have already

00:27:36,510 --> 00:27:41,820
started do you want to keep them running

00:27:38,460 --> 00:27:43,799
with the old data and only the new flows

00:27:41,820 --> 00:27:45,899
pick up on the newer version of the data

00:27:43,799 --> 00:27:46,559
if that is the case if that is a

00:27:45,899 --> 00:27:48,899
decision

00:27:46,559 --> 00:27:51,750
with the system you can do it you just

00:27:48,899 --> 00:27:53,610
load another attempt a better version of

00:27:51,750 --> 00:27:55,770
the data and the old flows can still

00:27:53,610 --> 00:27:59,970
refer to the old data basically they

00:27:55,770 --> 00:28:02,039
have repeatable reach and one very nice

00:27:59,970 --> 00:28:04,649
thing is it allows analysis of the

00:28:02,039 --> 00:28:07,770
dependency graph so you can find out

00:28:04,649 --> 00:28:10,529
which datasets are used by what slow

00:28:07,770 --> 00:28:15,710
again that helps a lot for the

00:28:10,529 --> 00:28:20,039
communication within the organization so

00:28:15,710 --> 00:28:22,890
we're still not happy because we learned

00:28:20,039 --> 00:28:25,230
to talk about decoupling export of

00:28:22,890 --> 00:28:26,789
results so we have the system in place

00:28:25,230 --> 00:28:29,640
we are talking to momenta for the

00:28:26,789 --> 00:28:33,450
ingestion but we are now not happy about

00:28:29,640 --> 00:28:36,410
that edge about that edge here which is

00:28:33,450 --> 00:28:43,740
about playing back the results

00:28:36,410 --> 00:28:46,470
so first of all you want to split your

00:28:43,740 --> 00:28:49,020
flows that it is very very easy to skip

00:28:46,470 --> 00:28:51,330
this export thing you should model your

00:28:49,020 --> 00:28:54,210
flows in a way that you can run them

00:28:51,330 --> 00:28:56,130
side-effect free why is it so imagine

00:28:54,210 --> 00:28:58,650
someone needs to install a security

00:28:56,130 --> 00:29:01,380
patch on the cluster then they might

00:28:58,650 --> 00:29:03,330
want to rerun the most critical flows to

00:29:01,380 --> 00:29:06,810
prove that the system is still working

00:29:03,330 --> 00:29:08,760
but we have already exported our data

00:29:06,810 --> 00:29:11,130
for today so we don't want to export it

00:29:08,760 --> 00:29:13,860
same thing you have a new development

00:29:11,130 --> 00:29:15,720
version of your flow and of course you

00:29:13,860 --> 00:29:18,000
test it with unit tests of course you

00:29:15,720 --> 00:29:20,160
test it with integration tests but if

00:29:18,000 --> 00:29:23,610
we're talking about big data you have to

00:29:20,160 --> 00:29:25,500
also do this test run on real production

00:29:23,610 --> 00:29:26,910
data because they might always be a data

00:29:25,500 --> 00:29:29,580
constellation that you have not

00:29:26,910 --> 00:29:31,710
anticipated and then you want to run a

00:29:29,580 --> 00:29:33,630
development version on production data

00:29:31,710 --> 00:29:36,180
of course you don't want to export that

00:29:33,630 --> 00:29:38,850
and what's important every flow should

00:29:36,180 --> 00:29:40,800
do this in the same way so that there is

00:29:38,850 --> 00:29:43,440
no surprises so if you have this kind of

00:29:40,800 --> 00:29:47,880
a dry run mode make sure that you do it

00:29:43,440 --> 00:29:50,690
in a consistent fashion so we split this

00:29:47,880 --> 00:29:53,460
up but the other thing is we don't want

00:29:50,690 --> 00:29:56,340
this job to write to the database

00:29:53,460 --> 00:29:58,140
directly this database should be owned

00:29:56,340 --> 00:30:00,900
by the service only the service should

00:29:58,140 --> 00:30:03,570
write to it and we have this small

00:30:00,900 --> 00:30:07,070
special requirement that I talked about

00:30:03,570 --> 00:30:10,020
in the beginning and that requirement is

00:30:07,070 --> 00:30:14,030
we are writing back to the same database

00:30:10,020 --> 00:30:18,210
that we have written from so this means

00:30:14,030 --> 00:30:19,740
we are that we are computing but while

00:30:18,210 --> 00:30:22,320
we are computing the world is still

00:30:19,740 --> 00:30:24,630
turning it might be that the result that

00:30:22,320 --> 00:30:26,850
the kind up with is no longer compatible

00:30:24,630 --> 00:30:30,090
with the state of the world as it is

00:30:26,850 --> 00:30:34,230
right now as it is a time of export so

00:30:30,090 --> 00:30:36,650
instead we want to throw those results

00:30:34,230 --> 00:30:39,090
back at the service and what we

00:30:36,650 --> 00:30:41,850
experimented with and then turned out to

00:30:39,090 --> 00:30:45,270
be very successful is just make HTTP

00:30:41,850 --> 00:30:48,140
calls all the time so pushing results

00:30:45,270 --> 00:30:50,480
via HTTP back to the service

00:30:48,140 --> 00:30:53,300
what this means is that this export of

00:30:50,480 --> 00:30:56,390
results is just becoming a client to the

00:30:53,300 --> 00:30:59,030
service like any other clients so this

00:30:56,390 --> 00:31:01,400
means it's it's much more it feels much

00:30:59,030 --> 00:31:03,650
more natural for the service developer

00:31:01,400 --> 00:31:06,080
also the service does not have to be

00:31:03,650 --> 00:31:08,690
aware of how do I read this data from

00:31:06,080 --> 00:31:12,650
HDFS how do I read this data from Kafka

00:31:08,690 --> 00:31:15,740
it gets it pushed just by HTTP and the

00:31:12,650 --> 00:31:18,500
service can validate each individual

00:31:15,740 --> 00:31:21,530
result and check hey is this still

00:31:18,500 --> 00:31:24,230
plausible is this still do I want to

00:31:21,530 --> 00:31:26,360
ingest this back into my life system at

00:31:24,230 --> 00:31:27,890
the current state of the world and you

00:31:26,360 --> 00:31:30,110
can do this with plausibility checks

00:31:27,890 --> 00:31:32,480
with business rules or with optimistic

00:31:30,110 --> 00:31:35,690
locking and last but not least it makes

00:31:32,480 --> 00:31:39,260
testing so much easier because it's so

00:31:35,690 --> 00:31:41,030
easy to just spin up a service and then

00:31:39,260 --> 00:31:43,340
on your developer notebook and then

00:31:41,030 --> 00:31:46,310
throw some HTTP calls against it and see

00:31:43,340 --> 00:31:47,990
how that behaves then to always have to

00:31:46,310 --> 00:31:50,030
put a file for example on your Hadoop

00:31:47,990 --> 00:31:52,070
cluster and then make it ingest from

00:31:50,030 --> 00:31:57,170
there so that's really helpful for

00:31:52,070 --> 00:31:59,450
testing as well so we came up with a

00:31:57,170 --> 00:32:01,970
component which turns Avro files into

00:31:59,450 --> 00:32:03,560
HTTP calls and we want that to be part

00:32:01,970 --> 00:32:07,010
of the flow but we want to it to be a

00:32:03,560 --> 00:32:09,860
standardized component so that we have

00:32:07,010 --> 00:32:12,700
this small building block where we can

00:32:09,860 --> 00:32:15,410
compose big resolutions out of it and

00:32:12,700 --> 00:32:17,750
this component treats every single

00:32:15,410 --> 00:32:19,880
record in the other file as in

00:32:17,750 --> 00:32:21,860
individual and if there's one million

00:32:19,880 --> 00:32:23,750
entries in the other file it will make 1

00:32:21,860 --> 00:32:25,820
million HTTP calls out of it

00:32:23,750 --> 00:32:27,830
it handles tracking of the progress it

00:32:25,820 --> 00:32:31,700
basically treats this input files as a

00:32:27,830 --> 00:32:33,860
queue and this also means you can stop

00:32:31,700 --> 00:32:37,070
this process at any point in time so you

00:32:33,860 --> 00:32:39,770
can stop it and do some maintenance work

00:32:37,070 --> 00:32:42,140
some operational work and then resume at

00:32:39,770 --> 00:32:44,630
any time it's also because it's a

00:32:42,140 --> 00:32:47,330
standardized component it allows us to

00:32:44,630 --> 00:32:51,080
have some standard features like for

00:32:47,330 --> 00:32:53,360
every with every HTTP request send the

00:32:51,080 --> 00:32:56,570
batch flow which from which this is

00:32:53,360 --> 00:32:59,720
originating and what we can also do then

00:32:56,570 --> 00:33:01,670
is the service can see oh this is not a

00:32:59,720 --> 00:33:04,000
call which is originating from

00:33:01,670 --> 00:33:06,980
human user this is a call which is

00:33:04,000 --> 00:33:09,830
coming from this batch flow I'm

00:33:06,980 --> 00:33:12,470
currently at my scaling limit I don't

00:33:09,830 --> 00:33:15,470
want to auto scale anymore and I just

00:33:12,470 --> 00:33:17,630
send a back pressure signal and this

00:33:15,470 --> 00:33:19,310
component I'll go to a chippy will then

00:33:17,630 --> 00:33:23,690
handle the back pressure signals from

00:33:19,310 --> 00:33:26,420
the service and this is my last one for

00:33:23,690 --> 00:33:28,520
today model slow orchestration

00:33:26,420 --> 00:33:32,750
explicitly and then make that a big bit

00:33:28,520 --> 00:33:34,940
faster so please consider using an

00:33:32,750 --> 00:33:37,460
execution system and take the system of

00:33:34,940 --> 00:33:39,920
your choice whether this is Azkaban or

00:33:37,460 --> 00:33:42,740
luigi or FL airflow or any of the others

00:33:39,920 --> 00:33:45,020
pick the system of your choice but don't

00:33:42,740 --> 00:33:47,630
stop there this is the one point where

00:33:45,020 --> 00:33:50,930
you can most easily create a maintenance

00:33:47,630 --> 00:33:53,690
nightmare to come for your system so you

00:33:50,930 --> 00:33:57,230
want to have extra standards here in

00:33:53,690 --> 00:33:59,210
place and something like inject paths

00:33:57,230 --> 00:34:01,850
always from the outside don't construct

00:33:59,210 --> 00:34:04,190
them in your flow inject calculation

00:34:01,850 --> 00:34:07,370
dates always from the outside never call

00:34:04,190 --> 00:34:10,640
now in any of your batch flows consider

00:34:07,370 --> 00:34:12,560
it making it functionally pure inject

00:34:10,640 --> 00:34:14,630
configure is a configuration settings

00:34:12,560 --> 00:34:17,230
never hard code something like hey this

00:34:14,630 --> 00:34:20,480
mapper needs full gigabyte of memory and

00:34:17,230 --> 00:34:22,160
foresee that you need different settings

00:34:20,480 --> 00:34:24,410
for different environments because your

00:34:22,160 --> 00:34:27,590
developer notebook will matter have less

00:34:24,410 --> 00:34:29,000
RAM and less data than your destined and

00:34:27,590 --> 00:34:30,470
the depth and environment will have less

00:34:29,000 --> 00:34:33,140
RAM and less data than the production

00:34:30,470 --> 00:34:35,320
environment and you don't want to waste

00:34:33,140 --> 00:34:38,030
resources on your developer notebook and

00:34:35,320 --> 00:34:40,790
this is 4c different environmental

00:34:38,030 --> 00:34:44,630
settings so always think about ease of

00:34:40,790 --> 00:34:49,160
operations tuning of settings and ease

00:34:44,630 --> 00:34:51,230
of upgrades now I talked a lot about

00:34:49,160 --> 00:34:53,840
batch processing and now you're probably

00:34:51,230 --> 00:34:57,140
thinking hey batch processing is so 2015

00:34:53,840 --> 00:34:59,600
what about stream processing and so

00:34:57,140 --> 00:35:01,520
let's look quickly at what kind of

00:34:59,600 --> 00:35:02,150
sources of streaming data we have at

00:35:01,520 --> 00:35:04,070
ResearchGate

00:35:02,150 --> 00:35:06,050
so there are some services which have

00:35:04,070 --> 00:35:08,660
natively which natively produce

00:35:06,050 --> 00:35:10,790
time-series data and we put this as Avro

00:35:08,660 --> 00:35:13,610
Andrew Kafka and then there are some

00:35:10,790 --> 00:35:14,830
other services whose native format is

00:35:13,610 --> 00:35:18,070
not so much

00:35:14,830 --> 00:35:20,470
time-series data and we're the most

00:35:18,070 --> 00:35:22,840
natural representation of your data

00:35:20,470 --> 00:35:24,790
would for example be a graph and of

00:35:22,840 --> 00:35:27,400
course you can turn everything into time

00:35:24,790 --> 00:35:29,020
series data I mean look at the Journal

00:35:27,400 --> 00:35:31,060
of your data base and you have every

00:35:29,020 --> 00:35:34,150
change in time series data but it's not

00:35:31,060 --> 00:35:36,100
the most natural way and so what I just

00:35:34,150 --> 00:35:37,990
said is exactly what we did so we build

00:35:36,100 --> 00:35:40,270
entity conveyor which is a system which

00:35:37,990 --> 00:35:42,760
looks at the data based replication and

00:35:40,270 --> 00:35:47,740
looks at every change that's happening

00:35:42,760 --> 00:35:49,840
there and can put that into a Kafka Q so

00:35:47,740 --> 00:35:52,750
what we can then do we can have stream

00:35:49,840 --> 00:35:55,360
processors which join that information

00:35:52,750 --> 00:35:57,250
which subscribe to both native time

00:35:55,360 --> 00:35:58,990
series data and the time series data

00:35:57,250 --> 00:36:02,800
that we provide from any other source

00:35:58,990 --> 00:36:05,350
and what we do is we write the results

00:36:02,800 --> 00:36:08,620
of that stream processor into a Casca

00:36:05,350 --> 00:36:11,590
queue again and as this was working out

00:36:08,620 --> 00:36:13,960
so well with the HTTP calls what we

00:36:11,590 --> 00:36:17,230
decided to do is we decided to implement

00:36:13,960 --> 00:36:20,860
a component that we call m to come which

00:36:17,230 --> 00:36:23,350
deprives to capital topics and contain

00:36:20,860 --> 00:36:25,560
convert those kafka topics to HTTP calls

00:36:23,350 --> 00:36:27,880
and put them back to the service and

00:36:25,560 --> 00:36:31,090
when you're thinking about it like this

00:36:27,880 --> 00:36:32,710
we are going through the same patterns

00:36:31,090 --> 00:36:36,640
we are going through the same principles

00:36:32,710 --> 00:36:39,400
again we D Kappa data ingestion we speak

00:36:36,640 --> 00:36:41,710
a common format we want all the data to

00:36:39,400 --> 00:36:43,660
be available in Kafka so we want all

00:36:41,710 --> 00:36:46,390
data to be available in Averell files

00:36:43,660 --> 00:36:49,090
for all batch stops you want all data to

00:36:46,390 --> 00:36:52,120
be available in Kafka topics as well for

00:36:49,090 --> 00:36:56,140
streaming jobs and inside there we want

00:36:52,120 --> 00:36:57,670
those records to be Avram then we want

00:36:56,140 --> 00:36:59,290
to speak a common language so we are

00:36:57,670 --> 00:37:02,320
listening directly to the database we

00:36:59,290 --> 00:37:04,690
want to retain this full information so

00:37:02,320 --> 00:37:07,060
when we are changing the entities of our

00:37:04,690 --> 00:37:09,040
service we are also letting entity

00:37:07,060 --> 00:37:14,230
conveyor know about that schema change

00:37:09,040 --> 00:37:18,760
and we are decoupling the export of

00:37:14,230 --> 00:37:22,090
results and throwing these back wire

00:37:18,760 --> 00:37:23,740
HTTP to the services so you can have a

00:37:22,090 --> 00:37:25,930
test version of the stream processor

00:37:23,740 --> 00:37:26,830
which just writes to a different topic

00:37:25,930 --> 00:37:28,330
and no

00:37:26,830 --> 00:37:31,180
and is listening to that but you can

00:37:28,330 --> 00:37:38,140
then do some analysis on it afterwards

00:37:31,180 --> 00:37:41,020
so then there's also a lot of execution

00:37:38,140 --> 00:37:42,550
specific things and those differ so I

00:37:41,020 --> 00:37:44,740
fear we're running out of time I'm not

00:37:42,550 --> 00:37:47,350
going into details with that one but

00:37:44,740 --> 00:37:49,600
what you're seeing here is that almost

00:37:47,350 --> 00:37:51,790
all of the rules that we came up with

00:37:49,600 --> 00:37:56,050
that systems also apply for our

00:37:51,790 --> 00:37:58,300
streaming systems and if you look

00:37:56,050 --> 00:37:59,980
carefully you see one is missing so what

00:37:58,300 --> 00:38:02,470
about number form model data

00:37:59,980 --> 00:38:04,090
dependencies explicitly that's something

00:38:02,470 --> 00:38:06,220
what we have not yet completely figured

00:38:04,090 --> 00:38:08,770
out for the streaming world but we think

00:38:06,220 --> 00:38:11,770
about it and potentially put Kafka

00:38:08,770 --> 00:38:14,860
topics into memento storing off sets of

00:38:11,770 --> 00:38:17,800
interest as decided by the producer

00:38:14,860 --> 00:38:19,180
there as well and we hope that this will

00:38:17,800 --> 00:38:20,830
facilitate switching between

00:38:19,180 --> 00:38:24,220
incompatible versions of stream

00:38:20,830 --> 00:38:28,480
processors so what I want to close this

00:38:24,220 --> 00:38:30,730
talk with here is you have this big data

00:38:28,480 --> 00:38:33,580
architecture and it's involving and of

00:38:30,730 --> 00:38:35,680
course a stream processor is a very

00:38:33,580 --> 00:38:38,830
different thing than a batch processor

00:38:35,680 --> 00:38:42,120
so they have special requirements but at

00:38:38,830 --> 00:38:45,160
the same time there's a lot of reuse if

00:38:42,120 --> 00:38:47,080
you think about about these building

00:38:45,160 --> 00:38:49,570
blocks and about these patterns early on

00:38:47,080 --> 00:38:51,880
and what I like to encourage you to

00:38:49,570 --> 00:38:53,830
think about is that there is not this

00:38:51,880 --> 00:38:55,870
batch camp and that there is this

00:38:53,830 --> 00:38:57,970
streaming camp and then there is no

00:38:55,870 --> 00:39:00,130
connection between them actually we

00:38:57,970 --> 00:39:03,610
think that they are going very very well

00:39:00,130 --> 00:39:07,630
together and you should be able to just

00:39:03,610 --> 00:39:10,330
do both as needed by your use case so

00:39:07,630 --> 00:39:11,740
that's about it thank you so much for

00:39:10,330 --> 00:39:15,300
your attention

00:39:11,740 --> 00:39:15,300
do you have any questions

00:39:22,890 --> 00:39:30,849
this one here so what were the factors

00:39:28,359 --> 00:39:33,900
that made you create momentum stead of

00:39:30,849 --> 00:39:36,940
using the existing workflow managers so

00:39:33,900 --> 00:39:39,069
what we have in the very beginning we

00:39:36,940 --> 00:39:41,049
tried to model the data dependency

00:39:39,069 --> 00:39:43,950
between two different flows as an

00:39:41,049 --> 00:39:46,900
azkaban edge which leads to a horrid

00:39:43,950 --> 00:39:49,750
horrible yeah horrific and huge graph

00:39:46,900 --> 00:39:52,540
that no one could look at anymore and it

00:39:49,750 --> 00:39:54,130
entangled this to one unit of deployment

00:39:52,540 --> 00:39:56,890
what you want is if you're running

00:39:54,130 --> 00:39:59,770
hundreds of flows with thousands of jobs

00:39:56,890 --> 00:40:03,579
you want that one team can safely deploy

00:39:59,770 --> 00:40:06,549
just one flow yeah and know that it

00:40:03,579 --> 00:40:09,460
affects only them if you try to model

00:40:06,549 --> 00:40:12,069
that with this wave for example in

00:40:09,460 --> 00:40:14,230
Azkaban edge then you always have this

00:40:12,069 --> 00:40:16,329
monolithic flow which describes all the

00:40:14,230 --> 00:40:19,359
data dependencies in your company and

00:40:16,329 --> 00:40:23,200
that was just too entangled into a

00:40:19,359 --> 00:40:25,420
restricted habit I say the companies

00:40:23,200 --> 00:40:29,140
used to Luigi and our flow to do what

00:40:25,420 --> 00:40:31,240
you describe could you so measure what

00:40:29,140 --> 00:40:33,819
you were not satisfied with those tools

00:40:31,240 --> 00:40:35,880
so we rebuilt this before Luigi and the

00:40:33,819 --> 00:40:42,640
airflow was out so that's a short answer

00:40:35,880 --> 00:40:46,510
so we started building this in 2000 can

00:40:42,640 --> 00:40:48,549
you help me curl in 2013-2014 so

00:40:46,510 --> 00:40:51,369
basically it was development I'm sure

00:40:48,549 --> 00:40:53,579
that other people have come up with the

00:40:51,369 --> 00:40:56,829
same problem and come up with their load

00:40:53,579 --> 00:40:58,510
this their solution for that what I'm

00:40:56,829 --> 00:41:01,420
saying here is not that you should all

00:40:58,510 --> 00:41:04,059
now use memento from now on what I'm

00:41:01,420 --> 00:41:06,130
saying is model your data dependencies

00:41:04,059 --> 00:41:09,160
explicit and really think about that how

00:41:06,130 --> 00:41:11,230
you model them and think about units of

00:41:09,160 --> 00:41:13,839
deployments you don't want one huge

00:41:11,230 --> 00:41:18,069
monolithic configuration which describes

00:41:13,839 --> 00:41:20,049
it all right well let's take what many

00:41:18,069 --> 00:41:22,260
questions offline let's think the

00:41:20,049 --> 00:41:22,620
speaker thank you so much

00:41:22,260 --> 00:41:28,830
[Music]

00:41:22,620 --> 00:41:28,830

YouTube URL: https://www.youtube.com/watch?v=Gz5enlaEjRk


