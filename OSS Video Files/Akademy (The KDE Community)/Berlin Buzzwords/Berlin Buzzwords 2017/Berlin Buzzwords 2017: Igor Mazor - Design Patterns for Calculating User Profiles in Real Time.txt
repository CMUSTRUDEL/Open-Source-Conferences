Title: Berlin Buzzwords 2017: Igor Mazor - Design Patterns for Calculating User Profiles in Real Time
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	It’s important to know your users’ preferences and behavior in the E-marketplace world. If you can quickly understand who your users are, you can optimize the user journey on the E-marketplace platform by presenting relevant products to the user, and by improving the relevance of search results. One way to leverage user preferences is to calculate a passive user profile based on the user’s interactions with the E-marketplace platform. 

At mobile.de, Germany's largest online vehicle marketplace, we include in the user profile information such as likelihood of a user to select different car colors, price distribution, mileage distribution, etc. The real challenge is designing a scalable system that can calculate profiles for different users in real-time and serve those profiles via a REST API to other stakeholders.

At mobile.de, we reviewed some of the most popular open-source stream processing solutions to consider possible architecture designs for the problem. In a nutshell updating user profile in real time is actually a stateful stream processing system in which the state is the user profile, the state key could be the user ID, and the state update operation can be simple as counter increment or an average/variance update.

This talk is sponsored by our Gold partner ebay tech.

Read more:
https://2017.berlinbuzzwords.de/17/session/design-patterns-calculating-user-profiles-real-time

About Igor Mazor:
https://2017.berlinbuzzwords.de/users/igor-mazor

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,450 --> 00:00:11,860
so I would not repeat the title again

00:00:09,880 --> 00:00:15,820
it's quite a long I would just add that

00:00:11,860 --> 00:00:18,610
i joined mobe ela which is part of ebay

00:00:15,820 --> 00:00:22,060
classified groups last year and my major

00:00:18,610 --> 00:00:23,710
goal is to leverage and to bring the

00:00:22,060 --> 00:00:26,169
mobile infrastructure from batch

00:00:23,710 --> 00:00:29,290
processing more towards real time and

00:00:26,169 --> 00:00:31,810
building scalable data products so a

00:00:29,290 --> 00:00:33,850
little bit about mobile a for those

00:00:31,810 --> 00:00:36,160
who've use it not familiar with mobile a

00:00:33,850 --> 00:00:37,090
if the Germany's largest online vehicle

00:00:36,160 --> 00:00:41,100
marketplace

00:00:37,090 --> 00:00:43,930
we have almost 1.6 million used cars and

00:00:41,100 --> 00:00:45,430
obviously we have also a lot of users

00:00:43,930 --> 00:00:47,590
each day millions of users that come

00:00:45,430 --> 00:00:50,190
into each to our plot from each day and

00:00:47,590 --> 00:00:52,180
looking for their next dream car and

00:00:50,190 --> 00:00:53,920
definitely when you have such a huge

00:00:52,180 --> 00:00:56,110
variety of cars finding to your next

00:00:53,920 --> 00:00:58,120
dream car is not an easy task and for

00:00:56,110 --> 00:01:00,070
that you will actually need a good data

00:00:58,120 --> 00:01:02,620
products and this is the main task of

00:01:00,070 --> 00:01:07,060
the data team at Mozilla to build good

00:01:02,620 --> 00:01:09,789
scalable amazing data products so today

00:01:07,060 --> 00:01:11,799
my focus would be exactly around one of

00:01:09,789 --> 00:01:14,229
the most challenging projects that we

00:01:11,799 --> 00:01:16,149
are working on and it is around the

00:01:14,229 --> 00:01:17,770
concept of the user profile so I would

00:01:16,149 --> 00:01:19,240
first start with some motivation why

00:01:17,770 --> 00:01:21,189
actually we need user profile

00:01:19,240 --> 00:01:22,719
we're from it cam what are the

00:01:21,189 --> 00:01:24,789
challenges that we had that you are

00:01:22,719 --> 00:01:27,100
still having some possible design

00:01:24,789 --> 00:01:28,990
patterns what eventually our

00:01:27,100 --> 00:01:33,490
architectures that we have selected and

00:01:28,990 --> 00:01:35,499
some final notes all right so let's

00:01:33,490 --> 00:01:38,319
start with motivation numbers everybody

00:01:35,499 --> 00:01:40,749
like numbers it's mobile our platform

00:01:38,319 --> 00:01:43,509
generates each day almost 100 million

00:01:40,749 --> 00:01:45,009
events so maybe it's not so much but for

00:01:43,509 --> 00:01:47,649
our companies quite huge amount of

00:01:45,009 --> 00:01:49,450
events and it's great each event

00:01:47,649 --> 00:01:51,880
represents some sort of interaction with

00:01:49,450 --> 00:01:53,889
the user in our platform for example

00:01:51,880 --> 00:01:56,409
when you're looking for specific car it

00:01:53,889 --> 00:01:58,359
triggers ad view event radiating when

00:01:56,409 --> 00:02:00,069
you park a car we have a park event it's

00:01:58,359 --> 00:02:03,639
like adding a car to a wish list so you

00:02:00,069 --> 00:02:05,770
can remember it later but 100 million

00:02:03,639 --> 00:02:08,200
events separately don't give us too much

00:02:05,770 --> 00:02:10,599
insight and we want to see how can we

00:02:08,200 --> 00:02:12,400
derive any kind of insights from those

00:02:10,599 --> 00:02:16,960
millions of events and reduce it to

00:02:12,400 --> 00:02:18,819
something more meaningful so we came up

00:02:16,960 --> 00:02:21,610
with the user profiles concept

00:02:18,819 --> 00:02:23,859
so eventually we take each day thus 100

00:02:21,610 --> 00:02:26,950
million of row events and reduce them to

00:02:23,859 --> 00:02:28,060
2 million user profiles all right so

00:02:26,950 --> 00:02:29,920
you're probably asking yourself all

00:02:28,060 --> 00:02:31,829
right what what is that user profile how

00:02:29,920 --> 00:02:34,000
it exactly look like how to expressed

00:02:31,829 --> 00:02:36,609
actually it's quite simple the user

00:02:34,000 --> 00:02:39,069
profile is just a set of attributes each

00:02:36,609 --> 00:02:41,379
attribute shows some kind of preference

00:02:39,069 --> 00:02:43,599
of the user towards those specific

00:02:41,379 --> 00:02:47,799
attributes for example an attribute can

00:02:43,599 --> 00:02:49,920
be color price range mileage range and

00:02:47,799 --> 00:02:53,319
each of those attributes we call

00:02:49,920 --> 00:02:55,000
dimension a profile dimension eventually

00:02:53,319 --> 00:02:57,269
we have approximately 20 dimensions that

00:02:55,000 --> 00:02:59,769
we collect for each user each dimension

00:02:57,269 --> 00:03:01,840
represent some sort of like loot of the

00:02:59,769 --> 00:03:06,099
user to favorite that attribute for

00:03:01,840 --> 00:03:09,310
example user had 20% red cards 80% blue

00:03:06,099 --> 00:03:17,409
cars and the distribution of the prices

00:03:09,310 --> 00:03:19,030
between 20,000 and 30,000 euro but that

00:03:17,409 --> 00:03:20,829
comes with the following question how do

00:03:19,030 --> 00:03:22,540
we calculate those user profiles why

00:03:20,829 --> 00:03:24,310
actually we need them in real time and

00:03:22,540 --> 00:03:25,930
actually the title here is new real time

00:03:24,310 --> 00:03:28,120
because real time is just by definition

00:03:25,930 --> 00:03:29,680
of the business for somebody real time

00:03:28,120 --> 00:03:32,109
is 10 seconds for other one second

00:03:29,680 --> 00:03:34,989
somebody else 30 seconds so why Neal

00:03:32,109 --> 00:03:37,120
real time we can just make our life easy

00:03:34,989 --> 00:03:38,739
and make a daily batch jobs that

00:03:37,120 --> 00:03:40,659
calculate the profiles over the night

00:03:38,739 --> 00:03:42,699
put them to a database then we have nice

00:03:40,659 --> 00:03:45,280
rest service some kind of micro service

00:03:42,699 --> 00:03:47,440
that serve them and our life is easy but

00:03:45,280 --> 00:03:49,959
no we were not satisfied with that what

00:03:47,440 --> 00:03:52,359
we wanted is once you take any kind of

00:03:49,959 --> 00:03:54,340
action on the website so you trigger any

00:03:52,359 --> 00:03:55,810
kind of edgy event that it would be

00:03:54,340 --> 00:03:57,760
reflected immediately on your eat

00:03:55,810 --> 00:03:59,799
journey which mean if you had now some

00:03:57,760 --> 00:04:02,500
kind of ad view and you go back again to

00:03:59,799 --> 00:04:04,479
the search page we want that all the

00:04:02,500 --> 00:04:06,340
views that you had before 2 minutes

00:04:04,479 --> 00:04:08,349
before 3 minutes before will reflect it

00:04:06,340 --> 00:04:10,569
right away on your journey and effect on

00:04:08,349 --> 00:04:13,000
the search results page in addition

00:04:10,569 --> 00:04:15,430
imagine a situation that new user coming

00:04:13,000 --> 00:04:17,620
to our website and he done is still

00:04:15,430 --> 00:04:19,289
don't have any kind of profile and we

00:04:17,620 --> 00:04:21,489
will view that profile over the night

00:04:19,289 --> 00:04:23,080
what we have done is that maybe the user

00:04:21,489 --> 00:04:25,300
will not come back again so the next day

00:04:23,080 --> 00:04:28,840
what that profile over the night will

00:04:25,300 --> 00:04:31,659
help me not at all that's the motivation

00:04:28,840 --> 00:04:32,740
for the real time all right so some of

00:04:31,659 --> 00:04:35,590
the challenges that we

00:04:32,740 --> 00:04:37,509
of so as I mentioned we reduce all our

00:04:35,590 --> 00:04:40,960
event each day to approximately two

00:04:37,509 --> 00:04:43,030
million daily user profiles and each

00:04:40,960 --> 00:04:44,620
profile have approximately not

00:04:43,030 --> 00:04:48,330
approximately exactly 20 profile

00:04:44,620 --> 00:04:51,099
dimensions such as price color mileage

00:04:48,330 --> 00:04:52,990
now one thing that important to mention

00:04:51,099 --> 00:04:55,000
here that it is daily profile which

00:04:52,990 --> 00:04:56,889
means that for each profile for each

00:04:55,000 --> 00:04:58,930
user we calculate his profile on daily

00:04:56,889 --> 00:05:00,610
base so if you have any kind of

00:04:58,930 --> 00:05:02,020
interaction today you will get a profile

00:05:00,610 --> 00:05:03,729
for today if you will come up to two

00:05:02,020 --> 00:05:05,380
more days and you will have interaction

00:05:03,729 --> 00:05:07,599
in the platform you will get another

00:05:05,380 --> 00:05:09,729
profile for that specific day and then

00:05:07,599 --> 00:05:11,919
each of those daily profiles we keep for

00:05:09,729 --> 00:05:14,199
a period of 60 days and I think white

00:05:11,919 --> 00:05:16,659
daily approach simple because we want to

00:05:14,199 --> 00:05:19,750
give the flexibility to our stakeholders

00:05:16,659 --> 00:05:23,409
the users of the users of the user

00:05:19,750 --> 00:05:25,479
profile series the option to say I want

00:05:23,409 --> 00:05:27,190
a user profile based on the last 10 days

00:05:25,479 --> 00:05:29,440
I want to use a profile based on the

00:05:27,190 --> 00:05:31,569
last 20 days we also want to give them

00:05:29,440 --> 00:05:33,580
the flexibility to apply different decay

00:05:31,569 --> 00:05:35,680
function between those profiles between

00:05:33,580 --> 00:05:39,430
those daily profiles so maybe we would

00:05:35,680 --> 00:05:41,830
like to boost the data of a user from

00:05:39,430 --> 00:05:44,110
the last week more than what happens in

00:05:41,830 --> 00:05:46,900
the last month so if you will do a small

00:05:44,110 --> 00:05:49,090
mass 2 million profiles 20 dimensions 60

00:05:46,900 --> 00:05:51,039
days you getting approximately 2.4

00:05:49,090 --> 00:05:53,740
billion records which is approximately

00:05:51,039 --> 00:05:55,300
350 gigabytes of data it's a production

00:05:53,740 --> 00:05:56,979
that is not some sort of unstructured

00:05:55,300 --> 00:05:59,500
data its production data that need to be

00:05:56,979 --> 00:06:01,360
maintained and updated constantly some

00:05:59,500 --> 00:06:03,490
of our more requirements were the user

00:06:01,360 --> 00:06:05,800
profile need to be updated up until 30

00:06:03,490 --> 00:06:07,509
seconds from the minute from the moment

00:06:05,800 --> 00:06:09,759
the user triggers any kind of event and

00:06:07,509 --> 00:06:14,280
we need to be able to serve the user

00:06:09,759 --> 00:06:14,280
profile in less than 50 milliseconds

00:06:14,550 --> 00:06:19,419
some of the more challenges that we have

00:06:17,020 --> 00:06:22,750
in scaling out the system so now we are

00:06:19,419 --> 00:06:24,460
keeping for 60 days 20 dimensions but

00:06:22,750 --> 00:06:25,930
imagine that tomorrow we decide no I

00:06:24,460 --> 00:06:29,680
want to keep it for 90 days

00:06:25,930 --> 00:06:31,780
no I want 30 dimensions the data size

00:06:29,680 --> 00:06:33,250
grows the user profile database is gross

00:06:31,780 --> 00:06:35,800
we need to make sure that our data

00:06:33,250 --> 00:06:38,530
storage can scale out easily without any

00:06:35,800 --> 00:06:41,110
kind of down times also for the moment

00:06:38,530 --> 00:06:43,510
we expecting 2,000 requests per second

00:06:41,110 --> 00:06:46,060
for the user profile service but it can

00:06:43,510 --> 00:06:46,420
easily go up tomorrow for 3,000 if more

00:06:46,060 --> 00:06:47,680
stick

00:06:46,420 --> 00:06:49,420
others in the company would like to use

00:06:47,680 --> 00:06:51,100
it we need to make sure our system

00:06:49,420 --> 00:06:55,300
scales good enough for that without

00:06:51,100 --> 00:06:56,530
downtimes high availability so imagine

00:06:55,300 --> 00:06:58,630
you have a system that have two major

00:06:56,530 --> 00:07:00,490
parts one part is updating the user

00:06:58,630 --> 00:07:02,920
profiles in real time another part is

00:07:00,490 --> 00:07:05,230
the one that serves them for us what was

00:07:02,920 --> 00:07:06,700
important is the serving part we don't

00:07:05,230 --> 00:07:08,710
want to have any kind of down times

00:07:06,700 --> 00:07:10,150
there it could be sent the stream

00:07:08,710 --> 00:07:12,010
processing system the system that

00:07:10,150 --> 00:07:14,020
updates the profiles failed from some

00:07:12,010 --> 00:07:15,820
reason doesn't important what kind of

00:07:14,020 --> 00:07:17,860
reason we still want to be able to

00:07:15,820 --> 00:07:19,360
continue in serve our profile even if

00:07:17,860 --> 00:07:22,180
it's not the most updated it's better

00:07:19,360 --> 00:07:25,270
than not serving it at all and if you

00:07:22,180 --> 00:07:27,010
intensive calculations so we don't

00:07:25,270 --> 00:07:28,630
really serve the profile as it is we

00:07:27,010 --> 00:07:30,670
actually using the basin approach we

00:07:28,630 --> 00:07:33,850
calculating posterior and prior for each

00:07:30,670 --> 00:07:36,370
user profile and by that we sort of

00:07:33,850 --> 00:07:38,950
creating likelihood a prediction how

00:07:36,370 --> 00:07:41,710
much a user will favor it a car with a

00:07:38,950 --> 00:07:43,450
specific set of dimensions those kind of

00:07:41,710 --> 00:07:46,060
calculations are quite intensive and the

00:07:43,450 --> 00:07:47,980
system need to be able to handle all of

00:07:46,060 --> 00:07:52,390
those amounts of traffic through the

00:07:47,980 --> 00:07:54,100
calculations all right so how we

00:07:52,390 --> 00:07:55,660
actually build such a scalable system

00:07:54,100 --> 00:07:57,370
that updates profile a new real-time

00:07:55,660 --> 00:07:59,650
serve them in real-time in a nutshell

00:07:57,370 --> 00:08:01,510
it's just stateful stream for setting

00:07:59,650 --> 00:08:03,870
system right so we have our events

00:08:01,510 --> 00:08:06,490
coming from caster this is our source

00:08:03,870 --> 00:08:09,720
stream source we have some kind of

00:08:06,490 --> 00:08:12,490
framework fling the cast of streams

00:08:09,720 --> 00:08:14,410
sparked right the processing our events

00:08:12,490 --> 00:08:16,570
and most of those frameworks have the

00:08:14,410 --> 00:08:19,270
support for keeping the stream state in

00:08:16,570 --> 00:08:20,890
some some manner and we have the stream

00:08:19,270 --> 00:08:23,110
state and Street and stead update

00:08:20,890 --> 00:08:25,330
operations so in the context of user

00:08:23,110 --> 00:08:27,520
profile the set can be simple as user

00:08:25,330 --> 00:08:29,590
profile the state key can be combined

00:08:27,520 --> 00:08:32,650
for example from user ID date and

00:08:29,590 --> 00:08:34,840
dimension because we would like to allow

00:08:32,650 --> 00:08:36,970
our stakeholders to request the user

00:08:34,840 --> 00:08:38,800
profile for specific date ranges or

00:08:36,970 --> 00:08:41,740
maybe using only specific set of

00:08:38,800 --> 00:08:43,510
dimensions and the set of debt operation

00:08:41,740 --> 00:08:45,460
are quite straightforward to the simple

00:08:43,510 --> 00:08:49,390
counter increment of some sort of

00:08:45,460 --> 00:08:51,220
average variance update okay

00:08:49,390 --> 00:08:54,070
but when visiting such a system there is

00:08:51,220 --> 00:08:56,290
one major decision to do how the state

00:08:54,070 --> 00:08:58,990
will be managed where the date will be

00:08:56,290 --> 00:09:00,160
stored and there are two major

00:08:58,990 --> 00:09:01,649
approaches

00:09:00,160 --> 00:09:03,220
more traditional approach is to use

00:09:01,649 --> 00:09:04,720
external storage

00:09:03,220 --> 00:09:07,509
I call it external storage global

00:09:04,720 --> 00:09:09,879
storage central storage you choose it

00:09:07,509 --> 00:09:12,430
but storages such as Cassandra and edge

00:09:09,879 --> 00:09:14,740
is a popular choice they are distributed

00:09:12,430 --> 00:09:16,329
and scalable by themselves another

00:09:14,740 --> 00:09:18,670
approach which is a little bit more

00:09:16,329 --> 00:09:21,069
already innovative and I see and read a

00:09:18,670 --> 00:09:22,389
lot of blogs and the confluent guys from

00:09:21,069 --> 00:09:24,339
castor will give a talk tomorrow about

00:09:22,389 --> 00:09:26,980
it changing the concept of micro

00:09:24,339 --> 00:09:28,449
services is to use local storage so

00:09:26,980 --> 00:09:30,430
local storage could be in the form of

00:09:28,449 --> 00:09:33,000
embedded key value storage for example

00:09:30,430 --> 00:09:37,870
rocks DB or any kind of simple in-memory

00:09:33,000 --> 00:09:40,120
map okay let's dive a little bit deeper

00:09:37,870 --> 00:09:43,569
so let's start with the local storage

00:09:40,120 --> 00:09:45,459
option so as much as most of you

00:09:43,569 --> 00:09:48,910
probably know a distributed stream

00:09:45,459 --> 00:09:51,759
processing system is just a cluster with

00:09:48,910 --> 00:09:54,430
multiple workers each worker is able to

00:09:51,759 --> 00:09:57,040
process a subset of your stream events

00:09:54,430 --> 00:09:59,379
and what you can do at that point that

00:09:57,040 --> 00:10:01,839
once you starting a new worker one is

00:09:59,379 --> 00:10:04,029
starting our stream processing system we

00:10:01,839 --> 00:10:06,279
can attach we can create a local storage

00:10:04,029 --> 00:10:08,139
to each of those workers if you have

00:10:06,279 --> 00:10:10,120
multiple executors on each node then

00:10:08,139 --> 00:10:12,759
each of these executors will have its

00:10:10,120 --> 00:10:16,329
own local storage it's not just local

00:10:12,759 --> 00:10:17,529
storage per machine ok and as I

00:10:16,329 --> 00:10:19,209
mentioned this local search can be

00:10:17,529 --> 00:10:20,889
embedded and can be on the file system

00:10:19,209 --> 00:10:23,800
you choose it what is more comfortable

00:10:20,889 --> 00:10:26,949
for you what the framework supports and

00:10:23,800 --> 00:10:29,560
then once a specific worker executors

00:10:26,949 --> 00:10:32,860
process a subset of the stream events he

00:10:29,560 --> 00:10:34,990
will keep also and update the subset of

00:10:32,860 --> 00:10:37,240
the stream state in that specific local

00:10:34,990 --> 00:10:38,800
storage and the way how we can expose

00:10:37,240 --> 00:10:41,740
that local storage in real-time is

00:10:38,800 --> 00:10:43,839
simply by exposing rest endpoint on top

00:10:41,740 --> 00:10:46,809
of that worker but one important thing

00:10:43,839 --> 00:10:48,790
to understand and remember that the

00:10:46,809 --> 00:10:51,189
worker and the rest point will have

00:10:48,790 --> 00:10:55,120
access only and only to that local

00:10:51,189 --> 00:10:57,189
storage on the other hand the more

00:10:55,120 --> 00:10:59,110
traditional approach that we're using is

00:10:57,189 --> 00:11:01,029
the external storage and the major

00:10:59,110 --> 00:11:04,180
differences between the external storage

00:11:01,029 --> 00:11:06,870
and local storage is in the external

00:11:04,180 --> 00:11:10,269
storage all of the workers

00:11:06,870 --> 00:11:13,160
subset of the workers can access in the

00:11:10,269 --> 00:11:15,529
same time your external storage ok

00:11:13,160 --> 00:11:17,360
so they continuously can continue an

00:11:15,529 --> 00:11:21,350
updating the external storage there is

00:11:17,360 --> 00:11:22,790
no kind of separation of their and it

00:11:21,350 --> 00:11:24,709
could be an edge scenario that for

00:11:22,790 --> 00:11:26,600
example two workers will try to update a

00:11:24,709 --> 00:11:28,970
specific user profile exactly at the

00:11:26,600 --> 00:11:30,740
same time and since our updates are

00:11:28,970 --> 00:11:33,199
counter update it can lead really much

00:11:30,740 --> 00:11:36,050
to rest conditions and eventually to

00:11:33,199 --> 00:11:37,639
wrong state this is important to

00:11:36,050 --> 00:11:39,290
remember and by the way this is a

00:11:37,639 --> 00:11:40,879
problem but it can be solved by

00:11:39,290 --> 00:11:42,800
corrected design and unfortunately I

00:11:40,879 --> 00:11:45,670
don't have enough time or it's not the

00:11:42,800 --> 00:11:48,670
focus of my talk today the other

00:11:45,670 --> 00:11:51,110
advantage of using local storage is

00:11:48,670 --> 00:11:53,029
decoupling so you can decouple fully

00:11:51,110 --> 00:11:55,069
between the system that updates the user

00:11:53,029 --> 00:11:57,350
profiles in real time and between the

00:11:55,069 --> 00:11:59,569
system that serves the profile so

00:11:57,350 --> 00:12:01,790
imagine that the system that updates

00:11:59,569 --> 00:12:04,550
profiles getting approximately let's say

00:12:01,790 --> 00:12:06,079
800 requests per second but the system

00:12:04,550 --> 00:12:08,300
that serves them needs to the handle

00:12:06,079 --> 00:12:10,850
with 3,000 requests per second it will

00:12:08,300 --> 00:12:12,769
be quite hard to scale the system if it

00:12:10,850 --> 00:12:14,420
contains both to the both of those

00:12:12,769 --> 00:12:16,009
operations together but if you have

00:12:14,420 --> 00:12:17,930
issue of the system separately you can

00:12:16,009 --> 00:12:20,389
scale it easily and you can debug it

00:12:17,930 --> 00:12:21,769
also easily it also removes some

00:12:20,389 --> 00:12:24,050
concerns for example for high

00:12:21,769 --> 00:12:26,779
availability if the part that updates

00:12:24,050 --> 00:12:29,060
the profile fail still you can continue

00:12:26,779 --> 00:12:30,980
serving those profiles because both of

00:12:29,060 --> 00:12:33,339
the systems are isolated from each other

00:12:30,980 --> 00:12:38,889
completely

00:12:33,339 --> 00:12:40,970
so some comparisons that I prepared so

00:12:38,889 --> 00:12:43,220
if you look about letting this

00:12:40,970 --> 00:12:44,930
definitely local storage is much better

00:12:43,220 --> 00:12:46,759
option because its local storage is in

00:12:44,930 --> 00:12:49,009
memory it's a file system we don't have

00:12:46,759 --> 00:12:50,600
any kind of remote calls over the wire

00:12:49,009 --> 00:12:53,019
we don't have any kind of network

00:12:50,600 --> 00:12:57,040
bottlenecks like it's typical to

00:12:53,019 --> 00:13:00,199
databases right so it's will win in that

00:12:57,040 --> 00:13:03,259
for that one he will win definitely from

00:13:00,199 --> 00:13:05,120
resource and maintains yeah maintaining

00:13:03,259 --> 00:13:08,149
a stream processing system without

00:13:05,120 --> 00:13:10,579
another piece of distributed database is

00:13:08,149 --> 00:13:12,319
definitely much easier but if you don't

00:13:10,579 --> 00:13:14,600
get from me all the five points because

00:13:12,319 --> 00:13:16,879
after all managing the distributed

00:13:14,600 --> 00:13:18,129
stream processing system it's not an

00:13:16,879 --> 00:13:20,540
easy task it can be quite challenging

00:13:18,129 --> 00:13:21,680
you need to monitor in to track you need

00:13:20,540 --> 00:13:24,439
to make sure that everything works fine

00:13:21,680 --> 00:13:25,939
so it is a dedicated resources and work

00:13:24,439 --> 00:13:27,639
however of

00:13:25,939 --> 00:13:29,720
if on top of it you add another

00:13:27,639 --> 00:13:31,339
distributed data storage you need a

00:13:29,720 --> 00:13:33,259
separate cluster for that you need maybe

00:13:31,339 --> 00:13:35,389
a separate DevOps that you'll support

00:13:33,259 --> 00:13:37,689
you and help you do that so it's more

00:13:35,389 --> 00:13:41,659
challenging from resources point of view

00:13:37,689 --> 00:13:43,789
recovery from failures so it's true that

00:13:41,659 --> 00:13:47,419
many of the stream processing systems

00:13:43,789 --> 00:13:49,549
slink and Casca streams and Apache

00:13:47,419 --> 00:13:51,799
things have different mechanism for how

00:13:49,549 --> 00:13:56,349
the recovery from failure one is better

00:13:51,799 --> 00:13:59,299
one is work depend on the use case but

00:13:56,349 --> 00:14:02,239
there are still not so much stable at

00:13:59,299 --> 00:14:04,789
least in our opinion as distributed data

00:14:02,239 --> 00:14:07,970
storages it can lead alpha easily to

00:14:04,789 --> 00:14:09,439
duplications so let's start with the

00:14:07,970 --> 00:14:11,169
example of duplications imagine you're

00:14:09,439 --> 00:14:14,809
processing your stream of events and

00:14:11,169 --> 00:14:16,789
your state is updated in memory but

00:14:14,809 --> 00:14:18,949
something happened your entire system

00:14:16,789 --> 00:14:22,099
fails and before you are able to commit

00:14:18,949 --> 00:14:23,959
the offset back to Kafka done you you're

00:14:22,099 --> 00:14:26,329
succeed to update your state but not to

00:14:23,959 --> 00:14:28,279
commit offsets now once you restart the

00:14:26,329 --> 00:14:30,109
stream processing system again what will

00:14:28,279 --> 00:14:31,939
happen that you will reprocess those

00:14:30,109 --> 00:14:33,589
events from Kafka again and will

00:14:31,939 --> 00:14:36,369
continue update your state so you will

00:14:33,589 --> 00:14:39,769
have over counting quite easily right

00:14:36,369 --> 00:14:42,229
also if your state is big 2.4 billion

00:14:39,769 --> 00:14:43,970
keys in the state it can be quite slow

00:14:42,229 --> 00:14:47,749
until you can recover from any kind of

00:14:43,970 --> 00:14:50,629
those failures on the other hand

00:14:47,749 --> 00:14:52,489
external storages distributed data

00:14:50,629 --> 00:14:54,859
storages more precise the HBase

00:14:52,489 --> 00:14:56,419
cassandra for example will build it from

00:14:54,859 --> 00:14:58,669
the beginning from the fundamentals of

00:14:56,419 --> 00:15:00,379
high availability and scalability so

00:14:58,669 --> 00:15:02,179
scaling those out

00:15:00,379 --> 00:15:05,949
they have much rubbish mechanism for

00:15:02,179 --> 00:15:08,269
recovering from any kind of failures and

00:15:05,949 --> 00:15:10,269
don't want to go too much deep into the

00:15:08,269 --> 00:15:12,979
mechanism of distributed data storages

00:15:10,269 --> 00:15:14,419
state availability as I mentioned if

00:15:12,979 --> 00:15:16,249
you're using the local storage you're

00:15:14,419 --> 00:15:17,839
actually getting one big system that

00:15:16,249 --> 00:15:20,179
contains all the moving parts you have a

00:15:17,839 --> 00:15:22,039
one system that contains the state

00:15:20,179 --> 00:15:24,949
updates the user profile update and the

00:15:22,039 --> 00:15:26,929
serving part if it fails you lose your

00:15:24,949 --> 00:15:28,999
profile you use the ability to serve the

00:15:26,929 --> 00:15:30,379
profile for some time or maybe for a

00:15:28,999 --> 00:15:32,749
longer time depends what kind of failure

00:15:30,379 --> 00:15:34,129
with the external storages you can just

00:15:32,749 --> 00:15:35,749
put your rest endpoints your micro

00:15:34,129 --> 00:15:38,089
services on top of the external storage

00:15:35,749 --> 00:15:39,320
you completely decoupled it from your

00:15:38,089 --> 00:15:42,769
systems that update

00:15:39,320 --> 00:15:44,949
the profit and you reduce the risk from

00:15:42,769 --> 00:15:47,930
race conditions okay I mentioned it

00:15:44,949 --> 00:15:49,910
multiple workers can access the external

00:15:47,930 --> 00:15:53,630
storage at the same time leading to race

00:15:49,910 --> 00:15:55,880
conditions and scaling out so don't

00:15:53,630 --> 00:15:58,459
forget the local storage attached to a

00:15:55,880 --> 00:16:00,980
specific worker and if your state size

00:15:58,459 --> 00:16:03,199
continuously growing you can be in a

00:16:00,980 --> 00:16:05,149
problem because the amount of space and

00:16:03,199 --> 00:16:07,550
memories that you can have dedicated to

00:16:05,149 --> 00:16:09,259
that local storage is connected directly

00:16:07,550 --> 00:16:12,860
to the worker and if the workers don't

00:16:09,259 --> 00:16:14,779
have enough space capacity your only

00:16:12,860 --> 00:16:15,259
problem the only way is to stop the

00:16:14,779 --> 00:16:17,089
process

00:16:15,259 --> 00:16:19,880
review your cluster but you have down

00:16:17,089 --> 00:16:21,920
times on the other hand using databases

00:16:19,880 --> 00:16:25,600
such as edge base with Cassandra you can

00:16:21,920 --> 00:16:28,430
just add more nodes and scale linearly

00:16:25,600 --> 00:16:30,680
so for us recovery from failures data

00:16:28,430 --> 00:16:32,930
availability and scaling out was the

00:16:30,680 --> 00:16:35,509
most important factors when we designing

00:16:32,930 --> 00:16:37,130
our user profile system that's why we

00:16:35,509 --> 00:16:39,709
decided to go with the external storage

00:16:37,130 --> 00:16:41,750
with the following configuration so use

00:16:39,709 --> 00:16:44,449
Cassandra as our external storage which

00:16:41,750 --> 00:16:46,069
is quite typical because it's quite

00:16:44,449 --> 00:16:48,019
battle proofed we have a lot of big

00:16:46,069 --> 00:16:50,319
community around it so you can get a lot

00:16:48,019 --> 00:16:52,639
of support we have 2.4 billion

00:16:50,319 --> 00:16:55,339
keys in our state which is quite a big

00:16:52,639 --> 00:16:56,899
state to manage and we saw that

00:16:55,339 --> 00:16:57,370
Cassandra able to manage even much

00:16:56,899 --> 00:17:00,470
better

00:16:57,370 --> 00:17:02,930
Cassandra structure allow really fast

00:17:00,470 --> 00:17:05,179
reads and even faster right so we felt

00:17:02,930 --> 00:17:07,610
that for us this is the perfect I will

00:17:05,179 --> 00:17:11,449
not hit the perfect the correct database

00:17:07,610 --> 00:17:13,220
to use for keeping our state's we select

00:17:11,449 --> 00:17:14,809
the SPARC streaming as our stream

00:17:13,220 --> 00:17:16,909
processing system the one that updates

00:17:14,809 --> 00:17:18,770
user profiles in real time and why

00:17:16,909 --> 00:17:20,990
exactly because of the near real-time

00:17:18,770 --> 00:17:23,150
the micro batch approach perfectly fit

00:17:20,990 --> 00:17:25,069
us we can create a micro basis of ten

00:17:23,150 --> 00:17:26,870
seconds of 20 seconds and achieve our

00:17:25,069 --> 00:17:29,780
requirements or update the user profile

00:17:26,870 --> 00:17:32,750
amp until 30 seconds in addition if the

00:17:29,780 --> 00:17:34,159
user have multiple events using the

00:17:32,750 --> 00:17:35,780
micro batch approach we can actually

00:17:34,159 --> 00:17:38,659
reduce all the amounts of those events

00:17:35,780 --> 00:17:42,370
into single event and by that reduce the

00:17:38,659 --> 00:17:46,340
load from our external storage on top of

00:17:42,370 --> 00:17:50,210
on top of Cassandra we using akka and

00:17:46,340 --> 00:17:52,220
Scala and more precisely akka HTTP in

00:17:50,210 --> 00:17:53,180
our opinion Scala is just a wonderful

00:17:52,220 --> 00:17:56,330
language for writing

00:17:53,180 --> 00:17:58,700
kind of asking services scalable

00:17:56,330 --> 00:18:01,450
services and we really didn't have too

00:17:58,700 --> 00:18:03,530
much thinking over there I mean we just

00:18:01,450 --> 00:18:06,200
considering different frameworks as

00:18:03,530 --> 00:18:07,580
there are HTTP of collateral play but

00:18:06,200 --> 00:18:09,590
eventually we saw that our cash TTP

00:18:07,580 --> 00:18:11,570
works quite fine and again I can have a

00:18:09,590 --> 00:18:13,580
good community so communities by the way

00:18:11,570 --> 00:18:14,750
always a big factor because especially

00:18:13,580 --> 00:18:16,250
once he work with open source

00:18:14,750 --> 00:18:21,260
technologies you would definitely would

00:18:16,250 --> 00:18:23,990
like to get a nice support and our micro

00:18:21,260 --> 00:18:26,660
services are managed by a massive

00:18:23,990 --> 00:18:29,930
cluster and well message is a great

00:18:26,660 --> 00:18:32,090
scheduler but it's not was like our

00:18:29,930 --> 00:18:34,010
choice simply when I came to the company

00:18:32,090 --> 00:18:35,510
methyls already was there all our micro

00:18:34,010 --> 00:18:38,210
services in the company are managed by

00:18:35,510 --> 00:18:39,980
mesos we have two data centers for now

00:18:38,210 --> 00:18:47,330
it works fine I don't have any kind of

00:18:39,980 --> 00:18:48,980
complaints so to conclude when you

00:18:47,330 --> 00:18:50,870
designing such a scalable system

00:18:48,980 --> 00:18:53,240
especially if you have any kind of those

00:18:50,870 --> 00:18:55,130
special requirements it is really

00:18:53,240 --> 00:18:56,450
important to understand all the

00:18:55,130 --> 00:18:57,950
different pros and cons of the

00:18:56,450 --> 00:18:59,290
technologies of the available

00:18:57,950 --> 00:19:02,300
technologies

00:18:59,290 --> 00:19:04,400
it's not easy choice to do right unit on

00:19:02,300 --> 00:19:07,940
Sand Village good which one works good

00:19:04,400 --> 00:19:09,380
which one works better and how but even

00:19:07,940 --> 00:19:11,270
much more important is also to

00:19:09,380 --> 00:19:12,980
understand your business requirements

00:19:11,270 --> 00:19:15,050
because it may be that you will find in

00:19:12,980 --> 00:19:17,870
technology that it's shooting for you

00:19:15,050 --> 00:19:21,380
but not for the business requirements

00:19:17,870 --> 00:19:23,840
for example our our requirement to have

00:19:21,380 --> 00:19:27,080
the user profile always available to be

00:19:23,840 --> 00:19:29,960
served so in my opinion the winning

00:19:27,080 --> 00:19:31,820
equation is to understand deeply all the

00:19:29,960 --> 00:19:34,430
different technological pros and cons of

00:19:31,820 --> 00:19:36,410
all available technologies which you

00:19:34,430 --> 00:19:38,240
heard about open source or something

00:19:36,410 --> 00:19:40,010
that you want to pay for to understand

00:19:38,240 --> 00:19:41,570
your use case the business requirements

00:19:40,010 --> 00:19:44,480
and only then you can build your

00:19:41,570 --> 00:19:46,670
scalable system and even then I still

00:19:44,480 --> 00:19:48,230
continuously to be honest always doubt

00:19:46,670 --> 00:19:50,360
myself and checking what kind of new

00:19:48,230 --> 00:19:52,370
technologies that are checking if I did

00:19:50,360 --> 00:19:54,350
any kind of mistake if then I redesign

00:19:52,370 --> 00:19:55,850
something cannot only by doubting

00:19:54,350 --> 00:19:58,100
yourself you can always continue to

00:19:55,850 --> 00:20:02,220
learn in finding bugs and improving your

00:19:58,100 --> 00:20:05,410
system thank you for being here clay

00:20:02,220 --> 00:20:05,410
[Applause]

00:20:08,019 --> 00:20:23,779
Thank You Hugo are there any questions

00:20:11,330 --> 00:20:26,210
to ego oh hello ego so my question is if

00:20:23,779 --> 00:20:30,470
so you explained in earlier side that

00:20:26,210 --> 00:20:32,230
when a new event from Kapaa comes you

00:20:30,470 --> 00:20:34,820
update the user profile in the real time

00:20:32,230 --> 00:20:36,919
but do you have any kind of snapshots

00:20:34,820 --> 00:20:40,070
kind of for mechanism like for example

00:20:36,919 --> 00:20:43,789
if you make a release in this user

00:20:40,070 --> 00:20:45,769
profile quote and if two weeks you fire

00:20:43,789 --> 00:20:48,049
realize that there is was a bug in last

00:20:45,769 --> 00:20:49,549
release and the user profiles in last

00:20:48,049 --> 00:20:52,549
two weeks updated are like kind of

00:20:49,549 --> 00:20:55,700
culminated like they are not some very

00:20:52,549 --> 00:20:56,929
correctly set up so yeah how do you

00:20:55,700 --> 00:20:59,240
correct them it's a good question

00:20:56,929 --> 00:21:00,830
definitely reprocessing is not always an

00:20:59,240 --> 00:21:02,690
easy task especially if your retention

00:21:00,830 --> 00:21:03,980
policy in Kafka is limited for example

00:21:02,690 --> 00:21:06,710
for seven days but you need to reprocess

00:21:03,980 --> 00:21:09,710
two days two weeks we have all the raw

00:21:06,710 --> 00:21:11,779
data always in Hadoop and also one of

00:21:09,710 --> 00:21:13,850
the reasons why we selected spark which

00:21:11,779 --> 00:21:16,010
is now still in going process is that we

00:21:13,850 --> 00:21:18,230
can easily leverage all the logic that

00:21:16,010 --> 00:21:21,799
you have and just hook a different

00:21:18,230 --> 00:21:24,380
source so instead of Kafka we can just

00:21:21,799 --> 00:21:26,720
use Hadoop so then we can reprocess all

00:21:24,380 --> 00:21:28,460
the events and push them to Cassandra

00:21:26,720 --> 00:21:30,710
definitely we will need to be careful

00:21:28,460 --> 00:21:32,059
there because if you push huge amounts

00:21:30,710 --> 00:21:34,039
of events right away - Cassandra it can

00:21:32,059 --> 00:21:41,419
be a problematic but it's a challenge

00:21:34,039 --> 00:21:43,279
that we are still working with yeah so

00:21:41,419 --> 00:21:45,440
you can just because of SPARC to have

00:21:43,279 --> 00:21:47,360
support for many different sources

00:21:45,440 --> 00:21:49,220
especially like a dupe and we proceed

00:21:47,360 --> 00:21:51,799
our raw data anyway on Hadoop for any

00:21:49,220 --> 00:21:54,110
other sort of analysis we can just reuse

00:21:51,799 --> 00:21:58,029
it and just instead of using Kafka we

00:21:54,110 --> 00:21:58,029
can use Hadoop to reprocess the events

00:21:58,059 --> 00:22:01,039
yeah thank you very much you go thank

00:22:00,710 --> 00:22:03,760
you

00:22:01,039 --> 00:22:03,760

YouTube URL: https://www.youtube.com/watch?v=uVOZbZXeVzY


