Title: Berlin Buzzwords 2017: Sean Braithwaite - Mechanics of Data Pipelines #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	This talk focused the topic on how to model data pipelines as retroactive, immutable data structures. It covers the topic of how do you build a data pipelines for a growing organization where different teams depend on each others data and need to be able to re-process data when errors occur upstream. 

I draw comparisons between the microservice architectures for both stream and batch processings and provide some guiding principals towards building resilient systems based on experience scaling out infrastructure at SoundCloud.

Read more:
https://2017.berlinbuzzwords.de/17/session/mechanics-data-pipelines

About Sean Braithwaite:
https://2017.berlinbuzzwords.de/users/sean-braithwaite

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,049 --> 00:00:11,290
okay so thank you for the introduction

00:00:09,790 --> 00:00:13,260
my name is Shawn

00:00:11,290 --> 00:00:15,160
I'm originally from Montreal in Canada

00:00:13,260 --> 00:00:18,070
but actually for the past five years

00:00:15,160 --> 00:00:21,520
I've been living in Berlin so it's super

00:00:18,070 --> 00:00:23,259
nice to do a presentation here and get

00:00:21,520 --> 00:00:26,470
to know a little bit of the scene

00:00:23,259 --> 00:00:28,090
happening for the past eight years or so

00:00:26,470 --> 00:00:30,310
I've been working with data in different

00:00:28,090 --> 00:00:32,920
capacities from like the infrastructure

00:00:30,310 --> 00:00:35,559
perspective maintaining databases to

00:00:32,920 --> 00:00:38,890
doing more distributed systems cassandra

00:00:35,559 --> 00:00:41,079
type work and most recently as a data

00:00:38,890 --> 00:00:44,970
scientist sort of producing I guess

00:00:41,079 --> 00:00:47,380
models of how consumers listen to music

00:00:44,970 --> 00:00:50,290
these different experiences have really

00:00:47,380 --> 00:00:52,660
informs the way I think about data and

00:00:50,290 --> 00:00:54,430
the stuff that we build and I've tried

00:00:52,660 --> 00:00:57,250
to synthesize some of those learnings

00:00:54,430 --> 00:00:59,920
into this presentation so the name of

00:00:57,250 --> 00:01:02,710
this talk is the mechanics of data

00:00:59,920 --> 00:01:04,360
pipeline and it's effectively for people

00:01:02,710 --> 00:01:09,880
who have been working with data and

00:01:04,360 --> 00:01:12,250
don't want to babysit ETL so first a

00:01:09,880 --> 00:01:13,990
little bit of context I work at a

00:01:12,250 --> 00:01:16,030
company called SoundCloud and SoundCloud

00:01:13,990 --> 00:01:17,830
is this you know music streaming

00:01:16,030 --> 00:01:21,700
platform similar to Spotify but

00:01:17,830 --> 00:01:23,229
distinctively different we have hundreds

00:01:21,700 --> 00:01:26,470
of millions of people you know all

00:01:23,229 --> 00:01:29,049
around the world consuming music but you

00:01:26,470 --> 00:01:32,020
know it sort of have gone through this

00:01:29,049 --> 00:01:34,810
tremendous phase of growth when I

00:01:32,020 --> 00:01:37,000
started we're maybe 75 people or

00:01:34,810 --> 00:01:40,600
something and are now I think four

00:01:37,000 --> 00:01:43,090
hundred we were initially using you know

00:01:40,600 --> 00:01:45,010
Google Analytics like I think most web

00:01:43,090 --> 00:01:48,490
properties and then inevitably started

00:01:45,010 --> 00:01:51,280
to build more things in-house so during

00:01:48,490 --> 00:01:53,440
this transition the the role of data

00:01:51,280 --> 00:01:55,689
also changed that we started you know

00:01:53,440 --> 00:01:58,420
doing machine learning we had to fix

00:01:55,689 --> 00:02:00,250
problems like spam and we also started

00:01:58,420 --> 00:02:03,520
taking it more seriously and allowing it

00:02:00,250 --> 00:02:05,530
to sort of drive our decisions so the

00:02:03,520 --> 00:02:07,060
context here you know that it's framing

00:02:05,530 --> 00:02:10,810
some of these recommendations and these

00:02:07,060 --> 00:02:13,599
thoughts about architecture are you know

00:02:10,810 --> 00:02:16,420
a diverse organization were different

00:02:13,599 --> 00:02:18,129
different domain experts by trying to

00:02:16,420 --> 00:02:20,620
collaborate with data to do something

00:02:18,129 --> 00:02:23,010
quite complex which is understand

00:02:20,620 --> 00:02:24,780
consumers

00:02:23,010 --> 00:02:26,760
so what are we talking about

00:02:24,780 --> 00:02:31,709
specifically well

00:02:26,760 --> 00:02:33,510
data pipelines are the software our data

00:02:31,709 --> 00:02:35,879
pipelines are the software structures

00:02:33,510 --> 00:02:38,340
which emerge to process and disseminate

00:02:35,879 --> 00:02:42,150
information but let's maybe break that

00:02:38,340 --> 00:02:44,069
down a little bit so theater pipelines

00:02:42,150 --> 00:02:45,810
are distributed systems so we need to

00:02:44,069 --> 00:02:48,569
talk about the ways in which they are

00:02:45,810 --> 00:02:51,450
distributed and what structures they

00:02:48,569 --> 00:02:53,849
form when we put them together and I say

00:02:51,450 --> 00:02:55,829
here I use the word emerge for a

00:02:53,849 --> 00:02:57,780
particular reason I would like to

00:02:55,829 --> 00:02:59,849
suggest that data pipelines are not

00:02:57,780 --> 00:03:02,910
something that could be designed upfront

00:02:59,849 --> 00:03:05,160
but are somehow structured and grow and

00:03:02,910 --> 00:03:07,530
we want to talk about in this

00:03:05,160 --> 00:03:12,829
presentation how to sort of shape that

00:03:07,530 --> 00:03:15,090
growth so here's an idealistic view of

00:03:12,829 --> 00:03:18,629
what one of these software structures

00:03:15,090 --> 00:03:21,060
looks like you have a single source so

00:03:18,629 --> 00:03:23,900
data coming from SoundCloud and then

00:03:21,060 --> 00:03:27,690
being disseminated to multiple stages

00:03:23,900 --> 00:03:31,380
each block is a stage and is owned by a

00:03:27,690 --> 00:03:33,930
particular team these teams very or

00:03:31,380 --> 00:03:36,150
these stages they vary in terms of run

00:03:33,930 --> 00:03:38,940
time the technology they use and the

00:03:36,150 --> 00:03:41,130
storage where they put the data based on

00:03:38,940 --> 00:03:43,639
sort of like distinct requirements of

00:03:41,130 --> 00:03:46,500
the domain of the team that holds them

00:03:43,639 --> 00:03:48,900
so each of these teams has diverse

00:03:46,500 --> 00:03:51,419
domain expertise which they use the

00:03:48,900 --> 00:03:54,569
Trent to enrich the data and in some

00:03:51,419 --> 00:03:57,000
kind of meaningful way each stage can

00:03:54,569 --> 00:03:59,519
consume from one or more previous stages

00:03:57,000 --> 00:04:04,470
and also provide data to multiple

00:03:59,519 --> 00:04:05,220
subsequent stages so why do we build

00:04:04,470 --> 00:04:08,220
these things

00:04:05,220 --> 00:04:10,739
well because problem domains like music

00:04:08,220 --> 00:04:14,160
like platforms with hundreds of millions

00:04:10,739 --> 00:04:16,229
of people are really complex to face

00:04:14,160 --> 00:04:18,479
these challenges you might split it up

00:04:16,229 --> 00:04:21,299
into multiple subdomains each with

00:04:18,479 --> 00:04:23,520
distinct sets of experts one way to

00:04:21,299 --> 00:04:25,830
think about data pipelines is as the

00:04:23,520 --> 00:04:29,630
software structure which enables experts

00:04:25,830 --> 00:04:29,630
from different domains to collaborate

00:04:29,820 --> 00:04:34,900
so how do we build them you know

00:04:32,080 --> 00:04:37,120
sometimes not so well

00:04:34,900 --> 00:04:40,180
these software structures have a

00:04:37,120 --> 00:04:43,030
tendency to become quite complex as they

00:04:40,180 --> 00:04:45,699
become more complex they take more time

00:04:43,030 --> 00:04:49,000
to manage and they turn people with

00:04:45,699 --> 00:04:51,930
expertise or people with PhDs into

00:04:49,000 --> 00:04:55,419
babysitter's or into operations people

00:04:51,930 --> 00:04:59,100
in the worst case they become impossible

00:04:55,419 --> 00:05:01,810
to extent there are several reasons why

00:04:59,100 --> 00:05:04,539
these these data pipelines become quite

00:05:01,810 --> 00:05:06,850
complex and they could be you know

00:05:04,539 --> 00:05:08,949
perhaps we don't understand the domain

00:05:06,850 --> 00:05:10,990
that we're trying to model so say if

00:05:08,949 --> 00:05:12,639
we're trying to solve spam we may not

00:05:10,990 --> 00:05:16,060
have the right expertise in spam just

00:05:12,639 --> 00:05:17,830
yet we might misunderstand the nature of

00:05:16,060 --> 00:05:20,190
the integration so how do we tie these

00:05:17,830 --> 00:05:23,020
together we might do in an ad hoc way

00:05:20,190 --> 00:05:26,080
which sort of proliferate complexity and

00:05:23,020 --> 00:05:28,510
we might misunderstand coordination we

00:05:26,080 --> 00:05:30,160
might not have a good idea of what are

00:05:28,510 --> 00:05:32,050
the times in which people need to

00:05:30,160 --> 00:05:33,550
collaborate and what are the times of

00:05:32,050 --> 00:05:38,710
which you want to automate that

00:05:33,550 --> 00:05:40,690
collaboration so given all this I hope

00:05:38,710 --> 00:05:44,080
by the end of my talk I will have

00:05:40,690 --> 00:05:46,419
convinced you of three things first that

00:05:44,080 --> 00:05:48,580
data pipelines emerge and need to be

00:05:46,419 --> 00:05:51,280
designed foundationally different than

00:05:48,580 --> 00:05:53,800
static software second that the air

00:05:51,280 --> 00:05:57,580
shapes by the organization and domains

00:05:53,800 --> 00:05:59,260
which produce them and lastly that what

00:05:57,580 --> 00:06:03,720
prevents data pipelines from growing are

00:05:59,260 --> 00:06:07,510
these cases of incidental coordination

00:06:03,720 --> 00:06:09,580
so to understand the architecture of

00:06:07,510 --> 00:06:12,820
data pipelines I think we need to map

00:06:09,580 --> 00:06:14,500
the conditions which produce them they

00:06:12,820 --> 00:06:17,050
say that people are the products of

00:06:14,500 --> 00:06:19,960
their environments but I also think this

00:06:17,050 --> 00:06:23,199
applies to software so Conway's law

00:06:19,960 --> 00:06:24,669
states that the software we build is

00:06:23,199 --> 00:06:26,229
somehow a reflection of the

00:06:24,669 --> 00:06:28,690
communication boundaries of the

00:06:26,229 --> 00:06:30,910
organizations which produce it in

00:06:28,690 --> 00:06:35,070
general you know I think this is this is

00:06:30,910 --> 00:06:35,070
true but perhaps doesn't go far enough

00:06:35,340 --> 00:06:42,419
I would I would argue that the way these

00:06:39,000 --> 00:06:45,120
boundaries form matter as well to make

00:06:42,419 --> 00:06:47,610
this argument I think we need to talk at

00:06:45,120 --> 00:06:49,740
this higher level abstraction from this

00:06:47,610 --> 00:06:52,110
level I think we get a better view of

00:06:49,740 --> 00:06:58,080
the forces shaping data pipelines and

00:06:52,110 --> 00:07:00,830
make better decisions about design so

00:06:58,080 --> 00:07:04,380
these things can start quite small

00:07:00,830 --> 00:07:06,660
typically with a single ETL so say you

00:07:04,380 --> 00:07:09,210
have some kind of events being written

00:07:06,660 --> 00:07:11,070
to HDFS every time a user plays a track

00:07:09,210 --> 00:07:14,190
so this is one of the fundamental things

00:07:11,070 --> 00:07:17,100
that soundcloud as a platform does so

00:07:14,190 --> 00:07:19,440
HDFS I hope as we we all know is the

00:07:17,100 --> 00:07:23,790
Hadoop file system which is really great

00:07:19,440 --> 00:07:26,539
at storing petabyte scale data sets it

00:07:23,790 --> 00:07:29,190
isn't so good however at querying data

00:07:26,539 --> 00:07:31,760
to make the data easier to query you

00:07:29,190 --> 00:07:34,050
might want to load this into a database

00:07:31,760 --> 00:07:35,850
this way you can figure out for instance

00:07:34,050 --> 00:07:38,550
how many plays you got yesterday and

00:07:35,850 --> 00:07:40,139
compare that to how many plays you got

00:07:38,550 --> 00:07:44,190
this day last year or something like

00:07:40,139 --> 00:07:48,090
this this is a perfect case for an ETL

00:07:44,190 --> 00:07:51,840
but what is it actually doing well it's

00:07:48,090 --> 00:07:55,650
creating this sort of split plays as a

00:07:51,840 --> 00:07:59,010
concept now exists in two places in HDFS

00:07:55,650 --> 00:08:01,979
and also in this queryable database but

00:07:59,010 --> 00:08:04,110
this split occurs on different levels so

00:08:01,979 --> 00:08:06,780
on the level of the storage system on

00:08:04,110 --> 00:08:09,800
the level of bounded context and also on

00:08:06,780 --> 00:08:12,270
the level of organizations and teams

00:08:09,800 --> 00:08:15,030
understanding these splits is important

00:08:12,270 --> 00:08:19,289
as it drives the evolution of a data

00:08:15,030 --> 00:08:22,289
pipeline and ends up shaping it so

00:08:19,289 --> 00:08:24,479
founded context is this idea developed

00:08:22,289 --> 00:08:26,729
from domain driven design how many

00:08:24,479 --> 00:08:29,520
people are somewhat familiar with domain

00:08:26,729 --> 00:08:31,500
driven design I think a large amount

00:08:29,520 --> 00:08:35,310
that's nice but let's actually define

00:08:31,500 --> 00:08:38,279
our terms here so specifically it refers

00:08:35,310 --> 00:08:40,500
to groups of people or possibly software

00:08:38,279 --> 00:08:42,630
which use a consistent language to

00:08:40,500 --> 00:08:46,020
describe domain objects so in this case

00:08:42,630 --> 00:08:47,510
play as a music platform SoundCloud

00:08:46,020 --> 00:08:50,570
records data

00:08:47,510 --> 00:08:53,329
and plays in particular we use this data

00:08:50,570 --> 00:08:55,220
for all sorts of things actually to

00:08:53,329 --> 00:08:56,720
train machine learning models to make

00:08:55,220 --> 00:08:59,120
recommendations of what people should

00:08:56,720 --> 00:09:02,240
listen to next but also to perform

00:08:59,120 --> 00:09:04,730
complex calculations to payout artists

00:09:02,240 --> 00:09:09,139
royalties based on how much they're

00:09:04,730 --> 00:09:12,110
engaging the platform but before we do

00:09:09,139 --> 00:09:15,290
either of these cases we do this attempt

00:09:12,110 --> 00:09:18,440
to filter out spam so now imagine we

00:09:15,290 --> 00:09:21,170
have two separate teams one which is

00:09:18,440 --> 00:09:23,839
sort of an expert in fighting spam they

00:09:21,170 --> 00:09:26,449
call them trust and safety and then

00:09:23,839 --> 00:09:28,959
another which is producing reports will

00:09:26,449 --> 00:09:32,570
call that team royalties and reporting

00:09:28,959 --> 00:09:34,790
the data that the spam fighters work

00:09:32,570 --> 00:09:37,339
with is completely different than the

00:09:34,790 --> 00:09:40,010
data we use to produce the reports but

00:09:37,339 --> 00:09:43,639
they use the same domain object right

00:09:40,010 --> 00:09:45,889
plays it just means different things so

00:09:43,639 --> 00:09:47,810
one is subject to spam and might have

00:09:45,889 --> 00:09:52,130
additional fields and the other is not

00:09:47,810 --> 00:09:54,529
so if you were to say take a list of the

00:09:52,130 --> 00:09:56,300
top ten countries that get plays the

00:09:54,529 --> 00:09:59,630
data set that has spam would look very

00:09:56,300 --> 00:10:02,000
different than the data set without so

00:09:59,630 --> 00:10:04,160
the this filtering of spam represents a

00:10:02,000 --> 00:10:05,930
split in the bounding context and which

00:10:04,160 --> 00:10:10,699
the play takes on these these two

00:10:05,930 --> 00:10:13,519
different meetings so having events like

00:10:10,699 --> 00:10:15,680
plays and HDFS is a great first step

00:10:13,519 --> 00:10:17,569
I could always write a MapReduce job and

00:10:15,680 --> 00:10:19,940
figure out how many plays I got each day

00:10:17,569 --> 00:10:21,529
that's easy but what if I want to build

00:10:19,940 --> 00:10:23,480
a web application to show these play

00:10:21,529 --> 00:10:26,089
counts the users say I want to embed it

00:10:23,480 --> 00:10:27,680
in every single widget on every single

00:10:26,089 --> 00:10:31,399
platform that SoundCloud is available

00:10:27,680 --> 00:10:34,550
well in this case you know HDFS is not a

00:10:31,399 --> 00:10:37,250
suitable option so differences in access

00:10:34,550 --> 00:10:40,550
patterns drive the introduction of these

00:10:37,250 --> 00:10:43,490
new storage mechanisms so HDFS

00:10:40,550 --> 00:10:45,199
relational databases column restores key

00:10:43,490 --> 00:10:49,220
value stores are all good at different

00:10:45,199 --> 00:10:51,139
things as use cases evolve we often need

00:10:49,220 --> 00:10:53,089
to introduce new kinds of databases and

00:10:51,139 --> 00:10:55,930
the data pipeline is then used to

00:10:53,089 --> 00:10:55,930
connect these together

00:10:57,450 --> 00:11:02,250
so teams grow to the size that they

00:10:59,700 --> 00:11:04,950
could somehow manage to communicate and

00:11:02,250 --> 00:11:08,370
collaborate efficiently there are at

00:11:04,950 --> 00:11:10,380
least two reasons why team split first

00:11:08,370 --> 00:11:12,180
when teams get so large that they

00:11:10,380 --> 00:11:14,220
naturally partition into groups that

00:11:12,180 --> 00:11:17,330
work more closely together than they do

00:11:14,220 --> 00:11:20,580
with with it with the rest of the team

00:11:17,330 --> 00:11:24,090
and the second is when a problem domain

00:11:20,580 --> 00:11:25,920
becomes so complex that the team

00:11:24,090 --> 00:11:28,290
naturally needs to split to have a

00:11:25,920 --> 00:11:31,290
coherent image of what they were doing

00:11:28,290 --> 00:11:33,150
you know one case of this would be when

00:11:31,290 --> 00:11:34,860
most startups begin everyone could sit

00:11:33,150 --> 00:11:37,230
at the same table and you know there's

00:11:34,860 --> 00:11:39,360
just like the engineering team and then

00:11:37,230 --> 00:11:40,890
eventually maybe someone is more working

00:11:39,360 --> 00:11:42,660
on data or someone's more working on

00:11:40,890 --> 00:11:44,700
front-end and they decide that they need

00:11:42,660 --> 00:11:46,890
to form separate teams and we need to

00:11:44,700 --> 00:11:51,900
sort of facilitate communication between

00:11:46,890 --> 00:11:54,480
them so sometimes that these splits are

00:11:51,900 --> 00:11:58,560
also represented by differences in

00:11:54,480 --> 00:12:00,300
bounded context and the reason for that

00:11:58,560 --> 00:12:03,960
is because sometimes these splits are

00:12:00,300 --> 00:12:07,380
actually nested they are hierarchical in

00:12:03,960 --> 00:12:09,960
a way teams contain one or many bounded

00:12:07,380 --> 00:12:12,510
contexts which are represented on one or

00:12:09,960 --> 00:12:15,120
many different storage systems you know

00:12:12,510 --> 00:12:17,670
transgressions of this hierarchy incur a

00:12:15,120 --> 00:12:20,190
high cost of coordination so for

00:12:17,670 --> 00:12:22,920
instance if I were to share a database

00:12:20,190 --> 00:12:26,100
with a different team and I wanted to

00:12:22,920 --> 00:12:28,620
change maybe add a column remove a

00:12:26,100 --> 00:12:31,410
column or even just change the semantics

00:12:28,620 --> 00:12:33,330
of a column I could wreak havoc on every

00:12:31,410 --> 00:12:38,160
query that was somehow dependent on that

00:12:33,330 --> 00:12:39,720
schema so this is this case of incident

00:12:38,160 --> 00:12:43,110
coordination and it's what we're trying

00:12:39,720 --> 00:12:44,910
to avoid so let's look at reasonable

00:12:43,110 --> 00:12:47,670
ways that we could aim to cross these

00:12:44,910 --> 00:12:51,990
boundaries of storage systems teams and

00:12:47,670 --> 00:12:55,680
bounded contexts so these splits that

00:12:51,990 --> 00:12:57,600
were sort of outlined are important but

00:12:55,680 --> 00:13:01,440
they need to be crossed to enable

00:12:57,600 --> 00:13:03,390
collaboration so these splits are what

00:13:01,440 --> 00:13:06,150
is connected by the data pipeline and

00:13:03,390 --> 00:13:07,850
they require somehow a mapping on each

00:13:06,150 --> 00:13:12,290
level of the split

00:13:07,850 --> 00:13:14,090
the goal of designing a data pipeline as

00:13:12,290 --> 00:13:16,790
sort of like an aggregate software

00:13:14,090 --> 00:13:18,800
structure right is really to capture all

00:13:16,790 --> 00:13:24,560
these interactions between the entities

00:13:18,800 --> 00:13:26,780
at the different levels so in the case

00:13:24,560 --> 00:13:28,910
in which we are loading data from HDFS

00:13:26,780 --> 00:13:31,070
into a relational database we must admit

00:13:28,910 --> 00:13:33,230
that these storage systems differ not

00:13:31,070 --> 00:13:35,660
only in how we use them but how they

00:13:33,230 --> 00:13:38,210
work so if you were here for Steve's

00:13:35,660 --> 00:13:42,440
talk that preceded this one you get this

00:13:38,210 --> 00:13:44,420
really good example about how s3 might

00:13:42,440 --> 00:13:47,420
describe the operation might have a

00:13:44,420 --> 00:13:49,100
similar API to a POSIX system but

00:13:47,420 --> 00:13:51,590
actually what's happening underneath the

00:13:49,100 --> 00:13:53,810
hood is fundamentally different this

00:13:51,590 --> 00:13:56,600
idea of being able to get a consistent

00:13:53,810 --> 00:14:00,800
view of data is not available on every

00:13:56,600 --> 00:14:03,860
storage system so when we build an ETL

00:14:00,800 --> 00:14:05,870
which is somehow connecting maybe a

00:14:03,860 --> 00:14:07,820
consistent storage system to an

00:14:05,870 --> 00:14:09,830
inconsistent storage system we are

00:14:07,820 --> 00:14:12,050
mapping fundamentally the mechanics

00:14:09,830 --> 00:14:17,930
between the between how these these

00:14:12,050 --> 00:14:20,780
systems work so if we follow this

00:14:17,930 --> 00:14:22,760
domain-driven design model communication

00:14:20,780 --> 00:14:25,520
between bounded contexts is done with

00:14:22,760 --> 00:14:28,270
events events are these immutable

00:14:25,520 --> 00:14:30,740
objects which represent state change and

00:14:28,270 --> 00:14:33,980
working with events really structures

00:14:30,740 --> 00:14:36,260
the way we think about state by sharing

00:14:33,980 --> 00:14:38,360
different events different bounded

00:14:36,260 --> 00:14:40,790
contexts are able to construct distinct

00:14:38,360 --> 00:14:43,550
views of the data which are defined by

00:14:40,790 --> 00:14:45,350
their own internal logic in this way

00:14:43,550 --> 00:14:48,650
state is something which is computed

00:14:45,350 --> 00:14:50,930
instead of shared in the case of spam

00:14:48,650 --> 00:14:53,390
and plays a team with domain expertise

00:14:50,930 --> 00:14:56,510
in filtering spam would provide a record

00:14:53,390 --> 00:14:59,870
for every play event augmented with a

00:14:56,510 --> 00:15:02,810
classification and possibly a confidence

00:14:59,870 --> 00:15:05,150
rating people consuming that data set

00:15:02,810 --> 00:15:07,970
would then both have you know know what

00:15:05,150 --> 00:15:10,730
is spam and what is not but it'd be able

00:15:07,970 --> 00:15:13,490
to modulate their tolerance for spam

00:15:10,730 --> 00:15:15,290
based on their particular use case so if

00:15:13,490 --> 00:15:17,570
all you're trying to do is display some

00:15:15,290 --> 00:15:19,910
counts to a bunch of users then you

00:15:17,570 --> 00:15:20,840
might have a higher high tolerance for

00:15:19,910 --> 00:15:24,320
spam

00:15:20,840 --> 00:15:27,620
in exchange for timeliness but if you

00:15:24,320 --> 00:15:29,900
are paying money based on usage you

00:15:27,620 --> 00:15:31,640
might have a very low tolerance for spam

00:15:29,900 --> 00:15:33,440
and you might accept that things are

00:15:31,640 --> 00:15:35,720
going to take longer to process if it

00:15:33,440 --> 00:15:40,520
gives you more confidence in the

00:15:35,720 --> 00:15:44,240
classification so back coming back to

00:15:40,520 --> 00:15:47,930
this case of spam and royalties what

00:15:44,240 --> 00:15:52,910
happens when you see a spike of plays on

00:15:47,930 --> 00:15:54,560
a single day no is it because we were

00:15:52,910 --> 00:15:57,410
subject to a new kind of spam attack

00:15:54,560 --> 00:16:01,160
which we are unfamiliar with and were

00:15:57,410 --> 00:16:04,220
unable to mitigate or did kanye west's

00:16:01,160 --> 00:16:05,810
release a track you know this is one of

00:16:04,220 --> 00:16:09,020
this is a clear violation of the

00:16:05,810 --> 00:16:11,870
intended isolation provided by violent

00:16:09,020 --> 00:16:14,300
contacts and the result is this case of

00:16:11,870 --> 00:16:16,600
incidental coordination where case or

00:16:14,300 --> 00:16:18,740
teams you know have to get together and

00:16:16,600 --> 00:16:21,260
reconcile their view of the data and

00:16:18,740 --> 00:16:23,870
sort of come up with a new definition of

00:16:21,260 --> 00:16:26,900
what is right and wrong before they

00:16:23,870 --> 00:16:31,310
split again and become independent or

00:16:26,900 --> 00:16:33,320
continue to operate independently so how

00:16:31,310 --> 00:16:36,650
do we form these contracts between

00:16:33,320 --> 00:16:39,620
stages in a data pipeline actually so

00:16:36,650 --> 00:16:42,020
every team involved needs the same thing

00:16:39,620 --> 00:16:44,000
then it's quite simple for example data

00:16:42,020 --> 00:16:46,490
recorded yesterday will be available the

00:16:44,000 --> 00:16:48,500
following morning in HDFS but this

00:16:46,490 --> 00:16:51,380
doesn't work because not everyone needs

00:16:48,500 --> 00:16:53,240
needs the same storage system and also

00:16:51,380 --> 00:16:56,240
as an international company there's no

00:16:53,240 --> 00:16:57,950
unified concept of mourning or one

00:16:56,240 --> 00:17:00,910
person's concept good morning might

00:16:57,950 --> 00:17:04,490
certainly not work for someone else

00:17:00,910 --> 00:17:06,550
so these contracts between stages also

00:17:04,490 --> 00:17:09,319
in some way need to be compatible as

00:17:06,550 --> 00:17:11,480
ETLs become composed sort of the

00:17:09,319 --> 00:17:13,970
transitive nature of the interaction

00:17:11,480 --> 00:17:16,220
means that if I produce data at 2:00

00:17:13,970 --> 00:17:18,680
a.m. the soonest that I can make data

00:17:16,220 --> 00:17:23,030
available would be at 201 given some job

00:17:18,680 --> 00:17:25,100
takes 1 minute to process if we do not

00:17:23,030 --> 00:17:27,230
build these coordination facilities into

00:17:25,100 --> 00:17:31,000
our software we'll have two coronate at

00:17:27,230 --> 00:17:33,740
this much slower rate you know as people

00:17:31,000 --> 00:17:34,659
these are these cases of incidental

00:17:33,740 --> 00:17:36,729
coordination that we're trying

00:17:34,659 --> 00:17:38,679
to avoid and we're trying to design

00:17:36,729 --> 00:17:41,129
around in our sort of conception of data

00:17:38,679 --> 00:17:41,129
pipelines

00:17:45,539 --> 00:17:51,309
oops events as the sole communication

00:17:49,509 --> 00:17:53,769
method which can go between bounded

00:17:51,309 --> 00:17:56,889
context are owned by a single team I

00:17:53,769 --> 00:17:58,599
mean they alone can change the

00:17:56,889 --> 00:18:02,789
definition of that data but it has to be

00:17:58,599 --> 00:18:05,229
somehow coordinated with with consumers

00:18:02,789 --> 00:18:07,359
so when a team decides to change their

00:18:05,229 --> 00:18:09,070
definition how are these changes

00:18:07,359 --> 00:18:11,409
propagated throughout the pipeline is

00:18:09,070 --> 00:18:13,899
really the question are there mechanisms

00:18:11,409 --> 00:18:15,789
for embedding these expectations and

00:18:13,899 --> 00:18:17,440
contracts and what recourse the

00:18:15,789 --> 00:18:19,869
dependencies have when these contracts

00:18:17,440 --> 00:18:22,389
are violated either we provide a formal

00:18:19,869 --> 00:18:24,309
mechanism to evolve definitions or again

00:18:22,389 --> 00:18:28,899
we incur this cost of incidental

00:18:24,309 --> 00:18:31,629
coordination so individually TLS can

00:18:28,899 --> 00:18:33,639
also be difficult to operate when they

00:18:31,629 --> 00:18:37,090
try to model an unbounded amount of data

00:18:33,639 --> 00:18:40,059
you know a single bad record could stall

00:18:37,090 --> 00:18:42,849
a pipeline but the question is should it

00:18:40,059 --> 00:18:45,549
you know how many bad records would make

00:18:42,849 --> 00:18:47,499
the result unusable this is only a

00:18:45,549 --> 00:18:49,809
question that we could answer through

00:18:47,499 --> 00:18:50,200
coordination between producer and

00:18:49,809 --> 00:18:52,539
consumer

00:18:50,200 --> 00:18:54,609
but once agreed upon you know it's the

00:18:52,539 --> 00:18:56,229
job of the software we build to enforce

00:18:54,609 --> 00:19:00,999
these contracts and to provide

00:18:56,229 --> 00:19:03,009
mechanisms for remediation when a job

00:19:00,999 --> 00:19:06,220
does fail you know the question is then

00:19:03,009 --> 00:19:08,080
who is responsible and what recourse did

00:19:06,220 --> 00:19:10,119
they have in the case of manual

00:19:08,080 --> 00:19:13,299
intervention should every downstream

00:19:10,119 --> 00:19:15,429
consumer be called upon when something

00:19:13,299 --> 00:19:17,649
upstream goes wrong I don't think so

00:19:15,429 --> 00:19:20,229
instead we need to automate this process

00:19:17,649 --> 00:19:22,299
and provide a mechanism which not only

00:19:20,229 --> 00:19:25,599
responds to failure but provides a

00:19:22,299 --> 00:19:27,460
method of failure resolution when a fix

00:19:25,599 --> 00:19:29,590
is applied to an upstream job the

00:19:27,460 --> 00:19:34,149
pipeline should be able to continue to

00:19:29,590 --> 00:19:36,159
run automatically okay so the setting

00:19:34,149 --> 00:19:39,549
that we've used so far has been quite

00:19:36,159 --> 00:19:41,200
abstract you know the purpose was to map

00:19:39,549 --> 00:19:43,869
out the conditions which shape the

00:19:41,200 --> 00:19:46,359
software we built in this mapping and

00:19:43,869 --> 00:19:47,140
mapping this out we were able to outline

00:19:46,359 --> 00:19:49,260
what

00:19:47,140 --> 00:19:52,650
are about working with data pipelines

00:19:49,260 --> 00:19:55,510
specifically designing systems which are

00:19:52,650 --> 00:19:59,710
expanding in terms of technology domain

00:19:55,510 --> 00:20:01,870
and people simultaneously and it's with

00:19:59,710 --> 00:20:03,250
this context that we can set upon the

00:20:01,870 --> 00:20:06,670
task of providing concrete

00:20:03,250 --> 00:20:08,650
recommendations first we'll lay out the

00:20:06,670 --> 00:20:11,380
anatomy of an ETL just to define our

00:20:08,650 --> 00:20:14,020
terms and then we're decide will

00:20:11,380 --> 00:20:17,050
describe some some sane ways of gluing

00:20:14,020 --> 00:20:20,320
it all together this glue will serve as

00:20:17,050 --> 00:20:23,080
a protocol which specifies how and when

00:20:20,320 --> 00:20:28,410
we read and write data to minimize this

00:20:23,080 --> 00:20:31,240
risk of incidental coordination so in a

00:20:28,410 --> 00:20:33,430
single ETL is pretty straightforward it

00:20:31,240 --> 00:20:36,190
has a source some kind of computation or

00:20:33,430 --> 00:20:38,740
job what kind of use this job or stage

00:20:36,190 --> 00:20:40,900
word interchangeably and then some kind

00:20:38,740 --> 00:20:43,870
of destination the source and

00:20:40,900 --> 00:20:45,460
destination are storage systems they

00:20:43,870 --> 00:20:51,460
could vary so one could be a database

00:20:45,460 --> 00:20:53,380
and one can be HDFS the ETL here can

00:20:51,460 --> 00:20:56,500
also be seen as a job which executes

00:20:53,380 --> 00:20:58,570
with some regularity in a streaming

00:20:56,500 --> 00:21:00,820
world this would mean when a batch which

00:20:58,570 --> 00:21:02,760
is a certain size and in a batch world

00:21:00,820 --> 00:21:06,060
this would mean based on some schedule

00:21:02,760 --> 00:21:09,550
but each ETL represents not only

00:21:06,060 --> 00:21:13,030
computation but effectively a domain

00:21:09,550 --> 00:21:15,910
model so when we filter spam plays we

00:21:13,030 --> 00:21:19,380
are expressing a model which attempts to

00:21:15,910 --> 00:21:21,730
capture you know what is and is not spam

00:21:19,380 --> 00:21:23,550
this model then produces a new

00:21:21,730 --> 00:21:26,520
representation of the input set

00:21:23,550 --> 00:21:29,050
augmented with this internal logic

00:21:26,520 --> 00:21:31,180
within this transformation is also a

00:21:29,050 --> 00:21:33,700
domain-specific notion of correctness

00:21:31,180 --> 00:21:38,080
and sometimes this can be difficult to

00:21:33,700 --> 00:21:42,160
share so how do we know when a job has

00:21:38,080 --> 00:21:43,840
succeeded actually but Big Data the

00:21:42,160 --> 00:21:46,950
computations we design have to account

00:21:43,840 --> 00:21:49,930
for this enormous amount of variety with

00:21:46,950 --> 00:21:53,260
this variety become comes the risk of

00:21:49,930 --> 00:21:55,450
partial failure a job may complete in

00:21:53,260 --> 00:21:57,250
terms of process completion but the

00:21:55,450 --> 00:21:59,920
output might not reflect our

00:21:57,250 --> 00:22:01,470
expectations so think about a job

00:21:59,920 --> 00:22:05,830
completing but

00:22:01,470 --> 00:22:09,220
producing maybe 10 kilobytes instead of

00:22:05,830 --> 00:22:11,620
the typical three gigabytes so in this

00:22:09,220 --> 00:22:14,920
case of spam you know a machine learning

00:22:11,620 --> 00:22:18,310
model tasks which has no ground truth

00:22:14,920 --> 00:22:21,280
there is very little objective measure

00:22:18,310 --> 00:22:22,810
of what is and is not spam and the

00:22:21,280 --> 00:22:24,610
result of this it means you know

00:22:22,810 --> 00:22:25,240
deploying changes is actually quite

00:22:24,610 --> 00:22:28,270
terrifying

00:22:25,240 --> 00:22:30,580
on one hand we might be protecting users

00:22:28,270 --> 00:22:33,250
from the various agents on the other

00:22:30,580 --> 00:22:36,310
hand we might be penalizing legitimate

00:22:33,250 --> 00:22:38,500
engagement so this ambiguous notion of

00:22:36,310 --> 00:22:40,660
correctness has the effect of making us

00:22:38,500 --> 00:22:43,540
as software developers reluctant to

00:22:40,660 --> 00:22:45,880
change even if we suspect improvements

00:22:43,540 --> 00:22:50,070
can be made we need to ensure that our

00:22:45,880 --> 00:22:50,070
changes are objectively better better

00:22:50,310 --> 00:22:55,090
from a job perspective we can produce

00:22:52,930 --> 00:22:58,240
this fingerprint so this is a new term

00:22:55,090 --> 00:23:01,420
that I'm introducing and sometimes

00:22:58,240 --> 00:23:04,090
somehow it tries to capture a collection

00:23:01,420 --> 00:23:06,910
of summary statistics which model the

00:23:04,090 --> 00:23:09,700
underlying domain these statistics

00:23:06,910 --> 00:23:12,670
should aim to capture both the variants

00:23:09,700 --> 00:23:14,410
and the invariants in the data set so

00:23:12,670 --> 00:23:17,320
for instance the spent the amount of

00:23:14,410 --> 00:23:19,420
spam that's produced each day might not

00:23:17,320 --> 00:23:21,640
be constant you might have you know good

00:23:19,420 --> 00:23:24,130
days and bad days but what if we could

00:23:21,640 --> 00:23:26,440
produce you know a summary statistic

00:23:24,130 --> 00:23:28,840
which is invariant what if we look at

00:23:26,440 --> 00:23:31,990
for instance the distribution of IP

00:23:28,840 --> 00:23:35,050
addresses by country or the amount of

00:23:31,990 --> 00:23:36,880
user agents we can capture this notion

00:23:35,050 --> 00:23:40,030
of variants using this measure of

00:23:36,880 --> 00:23:45,640
entropy which is somehow the amount of

00:23:40,030 --> 00:23:47,710
chaos or structure and a signal in

00:23:45,640 --> 00:23:49,570
either case we need this notion of

00:23:47,710 --> 00:23:52,290
correctness it's important that

00:23:49,570 --> 00:23:55,060
definitions be bound to the right domain

00:23:52,290 --> 00:23:56,920
using the operational domain so like the

00:23:55,060 --> 00:23:58,630
exit code of the status or what Hadoop

00:23:56,920 --> 00:24:00,280
gives you back you know is really

00:23:58,630 --> 00:24:01,660
insufficient for coordinating

00:24:00,280 --> 00:24:04,900
correctness between producers and

00:24:01,660 --> 00:24:06,610
consumers instead you know correctness

00:24:04,900 --> 00:24:08,740
should be modeled in the business domain

00:24:06,610 --> 00:24:12,110
and reflect the knowledge of domain

00:24:08,740 --> 00:24:14,990
experts even as this knowledge changes

00:24:12,110 --> 00:24:16,610
a notion of correctness captured by

00:24:14,990 --> 00:24:20,480
outputting summary statistics is

00:24:16,610 --> 00:24:23,600
essential for collaboration as ETLs are

00:24:20,480 --> 00:24:25,820
composed fingerprints can be consumed by

00:24:23,600 --> 00:24:28,160
downstream jobs to assert their own

00:24:25,820 --> 00:24:33,260
notion of correctness making the nature

00:24:28,160 --> 00:24:35,870
of that collaboration much richer so

00:24:33,260 --> 00:24:37,929
when we compose ETLs we allow this

00:24:35,870 --> 00:24:40,520
collaboration between additional domains

00:24:37,929 --> 00:24:43,160
the example we've used so far has been

00:24:40,520 --> 00:24:45,440
mostly about spam and licensing but as a

00:24:43,160 --> 00:24:48,860
larger business of course you have many

00:24:45,440 --> 00:24:50,809
domains communication between stages is

00:24:48,860 --> 00:24:53,559
done with these sets of immutable events

00:24:50,809 --> 00:24:56,410
which flow to the pipeline as each stage

00:24:53,559 --> 00:24:59,360
augments them with some domain expertise

00:24:56,410 --> 00:25:01,040
you know by chaining etl together we are

00:24:59,360 --> 00:25:06,290
also creating this operational

00:25:01,040 --> 00:25:08,210
dependency so problems at you know one

00:25:06,290 --> 00:25:11,750
stage of the pipeline have a direct

00:25:08,210 --> 00:25:13,940
impact downstream the the transitive

00:25:11,750 --> 00:25:16,000
nature of these dependency means that

00:25:13,940 --> 00:25:19,340
it's often not clear who is responsible

00:25:16,000 --> 00:25:23,179
and who should end more importantly who

00:25:19,340 --> 00:25:24,980
should be involved in in fixing it if we

00:25:23,179 --> 00:25:28,760
don't design our pipelines correctly

00:25:24,980 --> 00:25:32,210
fixing broken jobs can require tedious

00:25:28,760 --> 00:25:33,980
and expensive intervention by teams you

00:25:32,210 --> 00:25:35,929
know these cases of internal

00:25:33,980 --> 00:25:38,150
coordination have many side effects but

00:25:35,929 --> 00:25:40,610
I think one of them is is particularly

00:25:38,150 --> 00:25:44,870
worth calling out and that's you know

00:25:40,610 --> 00:25:46,850
erosion of trust if the if the same job

00:25:44,870 --> 00:25:49,220
keeps breaking despite the best efforts

00:25:46,850 --> 00:25:51,530
of the person who owns it or the team

00:25:49,220 --> 00:25:53,720
who owns it you know those downstream

00:25:51,530 --> 00:25:56,690
consumers can become increasingly

00:25:53,720 --> 00:25:59,299
frustrated these frustrations can often

00:25:56,690 --> 00:26:01,880
lead people to effectively design around

00:25:59,299 --> 00:26:04,940
failing jobs creating more complex

00:26:01,880 --> 00:26:08,090
Aleutians which are dramatically more

00:26:04,940 --> 00:26:10,549
complex than than need be you know this

00:26:08,090 --> 00:26:12,590
case of erosion of trust and designing

00:26:10,549 --> 00:26:16,160
around failure is something we really

00:26:12,590 --> 00:26:22,320
need to avoid instead we want to design

00:26:16,160 --> 00:26:25,320
for failure in an explicit way so

00:26:22,320 --> 00:26:27,330
change which happens at one stage of the

00:26:25,320 --> 00:26:30,720
pipeline has direct impact downstream we

00:26:27,330 --> 00:26:32,970
know this now but what we want is a way

00:26:30,720 --> 00:26:35,880
to respond to this change in an explicit

00:26:32,970 --> 00:26:39,090
and automated way to do this what we

00:26:35,880 --> 00:26:41,250
need is a dependency structure but this

00:26:39,090 --> 00:26:44,730
dependency structure exists at two

00:26:41,250 --> 00:26:47,010
distinct levels so first we try and

00:26:44,730 --> 00:26:49,080
capture the dependency between jobs I

00:26:47,010 --> 00:26:53,960
think that's the easy part but the

00:26:49,080 --> 00:26:53,960
second captures dependency between data

00:26:54,080 --> 00:26:59,220
so dependencies between jobs can really

00:26:57,540 --> 00:27:02,310
be handled by a typical scheduling

00:26:59,220 --> 00:27:04,110
system like air flow or Luigi these

00:27:02,310 --> 00:27:06,060
systems ensure that different stages of

00:27:04,110 --> 00:27:08,670
a pipeline can easily be chained

00:27:06,060 --> 00:27:10,650
together these systems allow it the

00:27:08,670 --> 00:27:13,500
parametrized execution and specify

00:27:10,650 --> 00:27:16,260
things like you know the input folder on

00:27:13,500 --> 00:27:18,060
HDFS versus the output folder and in

00:27:16,260 --> 00:27:21,900
this way they have like a rough

00:27:18,060 --> 00:27:23,910
understanding of the dependency between

00:27:21,900 --> 00:27:26,250
data but maybe we need to be a little

00:27:23,910 --> 00:27:29,510
bit more explicit than that and actually

00:27:26,250 --> 00:27:31,820
we had a very nice talk last week by

00:27:29,510 --> 00:27:35,340
Michael Houser from research gates

00:27:31,820 --> 00:27:37,830
presenting momento which actually solves

00:27:35,340 --> 00:27:39,390
this problem I would advise all of you

00:27:37,830 --> 00:27:44,520
check out the slides when they're

00:27:39,390 --> 00:27:46,740
released but what about the case in

00:27:44,520 --> 00:27:49,110
which change doesn't follow a

00:27:46,740 --> 00:27:53,460
traditional schedule so what happens

00:27:49,110 --> 00:27:55,200
when data arrives late or report has

00:27:53,460 --> 00:27:57,320
already been generated and then we

00:27:55,200 --> 00:27:59,990
realized that it was missing some data

00:27:57,320 --> 00:28:02,580
does it become the case first of

00:27:59,990 --> 00:28:05,400
recognizing late arrival and then

00:28:02,580 --> 00:28:08,070
contacting every team of every stage of

00:28:05,400 --> 00:28:11,760
the pipeline to rerun the jobs you know

00:28:08,070 --> 00:28:13,590
I hope not instead pipelines should be

00:28:11,760 --> 00:28:16,020
able to use this dependency structure to

00:28:13,590 --> 00:28:18,510
run automatically I'm introducing this

00:28:16,020 --> 00:28:20,880
term here convergence as it captures the

00:28:18,510 --> 00:28:23,220
idea that data is the conclusion of some

00:28:20,880 --> 00:28:25,140
process this process should be

00:28:23,220 --> 00:28:29,360
indifference to the number of steps it

00:28:25,140 --> 00:28:29,360
takes actually it should simply converge

00:28:29,809 --> 00:28:34,350
conversions you know it ensures that our

00:28:32,519 --> 00:28:36,840
jobs update automatically when change

00:28:34,350 --> 00:28:38,970
happens upstream it requires these

00:28:36,840 --> 00:28:40,940
explicit links between jobs and the data

00:28:38,970 --> 00:28:43,889
they generate with the benefit of

00:28:40,940 --> 00:28:46,399
limiting manual intervention but some

00:28:43,889 --> 00:28:48,990
kinds of intervention cannot be avoided

00:28:46,399 --> 00:28:54,389
in particular when our notion of

00:28:48,990 --> 00:28:57,090
correctness changes so when working with

00:28:54,389 --> 00:28:59,970
data oftentimes our notion of

00:28:57,090 --> 00:29:02,760
correctness is mutable updates the

00:28:59,970 --> 00:29:05,580
domain model need to be reflected in the

00:29:02,760 --> 00:29:06,330
data which represents them in the case

00:29:05,580 --> 00:29:08,190
of spam

00:29:06,330 --> 00:29:09,960
you know definitions of spent what is

00:29:08,190 --> 00:29:12,269
and is not spam are changing all the

00:29:09,960 --> 00:29:16,679
time as we get exposed to new kinds of

00:29:12,269 --> 00:29:18,450
attack and just continue to learn but

00:29:16,679 --> 00:29:19,980
one our definition of spam changes it

00:29:18,450 --> 00:29:21,720
means that data we might have been

00:29:19,980 --> 00:29:24,149
certain about in the past can no longer

00:29:21,720 --> 00:29:29,460
be trusted and we need to redo old

00:29:24,149 --> 00:29:31,889
computations so data pipelines which can

00:29:29,460 --> 00:29:35,159
redo computations are called retroactive

00:29:31,889 --> 00:29:37,799
this property allows domain experts to

00:29:35,159 --> 00:29:40,679
effectively change their mind when they

00:29:37,799 --> 00:29:44,130
do we can then ensure that this changes

00:29:40,679 --> 00:29:46,649
are materialized in the data I know this

00:29:44,130 --> 00:29:48,840
might sound extreme but I would say that

00:29:46,649 --> 00:29:52,139
data pipelines which don't have this

00:29:48,840 --> 00:29:53,820
property can not really be trusted there

00:29:52,139 --> 00:29:56,039
would always be this risk that the data

00:29:53,820 --> 00:29:59,700
these pipelines provide is a reflection

00:29:56,039 --> 00:30:00,960
of bad assumptions on the contrary you

00:29:59,700 --> 00:30:03,960
know data pipelines which are

00:30:00,960 --> 00:30:05,909
retroactive can be trusted as they can

00:30:03,960 --> 00:30:11,639
actually represent you know the best of

00:30:05,909 --> 00:30:13,679
our knowledge you know advocating for

00:30:11,639 --> 00:30:18,179
mutation especially in saw this highly

00:30:13,679 --> 00:30:20,970
distributed world that we're in can seem

00:30:18,179 --> 00:30:24,929
quite scary but maybe it doesn't have to

00:30:20,970 --> 00:30:26,490
be we know that we have to change the

00:30:24,929 --> 00:30:29,250
data to reflect changes in our domain

00:30:26,490 --> 00:30:32,039
model but it's not always obvious how to

00:30:29,250 --> 00:30:33,960
do this with big data systems given that

00:30:32,039 --> 00:30:36,029
our data pipeline spans multiple storage

00:30:33,960 --> 00:30:37,799
services there's no guarantee that we

00:30:36,029 --> 00:30:39,890
could simply update records like we can

00:30:37,799 --> 00:30:42,510
in a database

00:30:39,890 --> 00:30:44,430
instead I would advocate for thinking

00:30:42,510 --> 00:30:47,070
about data pipelines as immutable data

00:30:44,430 --> 00:30:49,830
structures immutable data structures

00:30:47,070 --> 00:30:53,370
never override data but instead produce

00:30:49,830 --> 00:30:55,740
multiple versions these versions of data

00:30:53,370 --> 00:30:58,890
can actually be tied to the software

00:30:55,740 --> 00:31:00,300
which produce them since each version of

00:30:58,890 --> 00:31:04,230
the data comes with a fingerprint

00:31:00,300 --> 00:31:05,910
summarized in the data it contains or we

00:31:04,230 --> 00:31:08,220
can quickly measure if we've actually

00:31:05,910 --> 00:31:10,080
made improvements so say our fingerprint

00:31:08,220 --> 00:31:12,000
includes like our classification rate

00:31:10,080 --> 00:31:13,770
and then we could make updates to the

00:31:12,000 --> 00:31:17,640
software to bring that into what we

00:31:13,770 --> 00:31:20,520
consider to be a reasonable balance as

00:31:17,640 --> 00:31:22,830
an individual owning a single job these

00:31:20,520 --> 00:31:25,740
facilities provide me with a way to be

00:31:22,830 --> 00:31:27,810
confident in making changes without the

00:31:25,740 --> 00:31:30,300
risk of overwriting data or inciting a

00:31:27,810 --> 00:31:32,600
case of incidental coordination you know

00:31:30,300 --> 00:31:35,370
I could deploy my changes automatically

00:31:32,600 --> 00:31:37,680
in combination with output fingerprints

00:31:35,370 --> 00:31:40,290
I could put my software in a continuous

00:31:37,680 --> 00:31:42,630
delivery pipeline and every time I make

00:31:40,290 --> 00:31:45,690
a change into code I could actually make

00:31:42,630 --> 00:31:47,850
a change to the data in this way data

00:31:45,690 --> 00:31:50,550
pipelines as distributed software

00:31:47,850 --> 00:31:52,080
structures can start utilizing a lot of

00:31:50,550 --> 00:31:55,020
the development that's been happening in

00:31:52,080 --> 00:31:59,580
other fields in particularly in micro

00:31:55,020 --> 00:32:02,190
services so taking a step back these

00:31:59,580 --> 00:32:05,010
properties I've defined are not simply

00:32:02,190 --> 00:32:07,380
about individual jobs instead they

00:32:05,010 --> 00:32:09,180
describe primarily the interaction

00:32:07,380 --> 00:32:11,640
between jobs and in combination

00:32:09,180 --> 00:32:14,760
hopefully the aggregate behavior of the

00:32:11,640 --> 00:32:17,190
system making an individual stage

00:32:14,760 --> 00:32:19,160
idempotent or capable of scheduling

00:32:17,190 --> 00:32:22,170
itself when the underlying data changes

00:32:19,160 --> 00:32:25,170
it's very useful it's something I would

00:32:22,170 --> 00:32:27,890
certainly advocate that you do however

00:32:25,170 --> 00:32:30,210
I'm not sure that goes far enough

00:32:27,890 --> 00:32:32,060
expectations of operational behavior

00:32:30,210 --> 00:32:34,470
shouldn't vary throughout the pipeline

00:32:32,060 --> 00:32:36,870
this will create those preferences of

00:32:34,470 --> 00:32:39,840
what people what data people are

00:32:36,870 --> 00:32:41,910
comfortable depending on instead these

00:32:39,840 --> 00:32:44,130
properties that I outline our objectives

00:32:41,910 --> 00:32:46,970
for a collective common protocol for

00:32:44,130 --> 00:32:46,970
collaborating with data

00:32:48,050 --> 00:32:55,550
okay so in this talk I try to present

00:32:51,860 --> 00:32:58,070
data pipelines in in a new way I took a

00:32:55,550 --> 00:33:00,350
step back and defined them not as

00:32:58,070 --> 00:33:03,740
incidental Frankenstein compositions of

00:33:00,350 --> 00:33:06,640
software but structures these structures

00:33:03,740 --> 00:33:10,100
emerge for a good reason to connect and

00:33:06,640 --> 00:33:11,870
distribute domain expertise we looked at

00:33:10,100 --> 00:33:13,910
the specific forces governing the

00:33:11,870 --> 00:33:17,000
evolution of data pipelines over three

00:33:13,910 --> 00:33:20,210
levels storage systems founded contexts

00:33:17,000 --> 00:33:22,010
and teams this evolution came with a

00:33:20,210 --> 00:33:24,680
need to map between separate entities

00:33:22,010 --> 00:33:27,800
and acts which came with a consequence

00:33:24,680 --> 00:33:29,570
right coordination we then tried to make

00:33:27,800 --> 00:33:31,940
the distinction between incidental

00:33:29,570 --> 00:33:35,440
coordination and the mechanisms we could

00:33:31,940 --> 00:33:37,970
build into our software to avoid them

00:33:35,440 --> 00:33:40,610
avoiding internal coordination came from

00:33:37,970 --> 00:33:42,580
three suggestions making dependency

00:33:40,610 --> 00:33:44,630
structures of jobs and data explicit

00:33:42,580 --> 00:33:47,090
ensuring the output of jobs never

00:33:44,630 --> 00:33:50,000
overrides data but simply provides a

00:33:47,090 --> 00:33:51,290
newer version and finally by baking

00:33:50,000 --> 00:33:53,330
baking in the assumption that

00:33:51,290 --> 00:33:55,490
calculations we perform on data are

00:33:53,330 --> 00:33:56,930
going to be wrong and that we most

00:33:55,490 --> 00:34:00,740
certainly need the facilities to make

00:33:56,930 --> 00:34:02,810
them right so finally you know data

00:34:00,740 --> 00:34:04,220
pipelines connect and enable

00:34:02,810 --> 00:34:06,710
collaboration between diverse

00:34:04,220 --> 00:34:08,720
perspectives their growth is a

00:34:06,710 --> 00:34:10,700
reflection of our need to form a richer

00:34:08,720 --> 00:34:15,190
view of the world and we most certainly

00:34:10,700 --> 00:34:15,190
need to build them better thank you

00:34:21,340 --> 00:34:29,600
we have time for a few questions thank

00:34:27,470 --> 00:34:33,110
you very much I thought it was extremely

00:34:29,600 --> 00:34:35,840
insightful and interesting talk I have a

00:34:33,110 --> 00:34:37,580
question if you could maybe say a few

00:34:35,840 --> 00:34:39,830
words about two domains that are

00:34:37,580 --> 00:34:42,200
connected not by an ETL type pipeline

00:34:39,830 --> 00:34:44,629
but simply by data replication in which

00:34:42,200 --> 00:34:47,240
of your thoughts are similar or

00:34:44,629 --> 00:34:50,000
different on that kind of system data

00:34:47,240 --> 00:34:53,000
replication so I imagine maybe your

00:34:50,000 --> 00:34:55,669
ingestion and yeah and the spam

00:34:53,000 --> 00:34:57,920
detection might actually share data that

00:34:55,669 --> 00:35:00,560
aren't transformed in any way well I

00:34:57,920 --> 00:35:02,990
think the ingestion phase right which is

00:35:00,560 --> 00:35:05,360
somehow summarized in a single block in

00:35:02,990 --> 00:35:07,970
the diagram of course that's actually

00:35:05,360 --> 00:35:11,360
multiple stages so one of these things

00:35:07,970 --> 00:35:13,730
would be encoding so we take data that

00:35:11,360 --> 00:35:15,800
might arrive from JSON as sort of the

00:35:13,730 --> 00:35:17,990
the lowest common denominator of formats

00:35:15,800 --> 00:35:20,840
between different clients and then we

00:35:17,990 --> 00:35:23,180
might encode it as a protobuf at

00:35:20,840 --> 00:35:26,450
ingestion another thing that we do at

00:35:23,180 --> 00:35:29,000
the ingestion stage is account for some

00:35:26,450 --> 00:35:31,310
notion of deduplication that happens

00:35:29,000 --> 00:35:34,460
from up for operational reasons instead

00:35:31,310 --> 00:35:36,490
of semantic reasons so the a lot of the

00:35:34,460 --> 00:35:39,440
tools we use like Kafka you know they

00:35:36,490 --> 00:35:42,380
they get their delivery guarantees

00:35:39,440 --> 00:35:44,410
through retries so we try and account

00:35:42,380 --> 00:35:47,270
for that at the ingestion phase and

00:35:44,410 --> 00:35:49,190
isolates sort of different domain

00:35:47,270 --> 00:35:52,930
expertise from these operational

00:35:49,190 --> 00:36:02,450
complexities does that sort of capture

00:35:52,930 --> 00:36:04,990
the replication question yeah please any

00:36:02,450 --> 00:36:04,990
other questions

00:36:05,490 --> 00:36:15,240
I really like the fingerprinting idea

00:36:11,940 --> 00:36:18,480
and it sounded I really understood how

00:36:15,240 --> 00:36:21,420
you can use it for your everyday traffic

00:36:18,480 --> 00:36:23,040
let's say but you mentioned that you

00:36:21,420 --> 00:36:25,320
have to differentiate between a Kanye

00:36:23,040 --> 00:36:28,859
West releasing a new track or is it a

00:36:25,320 --> 00:36:30,780
spam attack so I imagine that the stats

00:36:28,859 --> 00:36:33,330
there would change yeah so how do you

00:36:30,780 --> 00:36:36,270
know what to do so this is this case of

00:36:33,330 --> 00:36:38,099
incidental coordination actually sitting

00:36:36,270 --> 00:36:40,530
in front of you is someone were working

00:36:38,099 --> 00:36:43,680
with on this very problem so maybe you

00:36:40,530 --> 00:36:47,430
could catch up with her afterwards but

00:36:43,680 --> 00:36:49,589
truly every SoundCloud is a really

00:36:47,430 --> 00:36:52,500
interesting problem domain because of

00:36:49,589 --> 00:36:53,880
this variance of signal right it's

00:36:52,500 --> 00:36:55,740
incredible the things that you could

00:36:53,880 --> 00:36:59,730
arrive like we have to remind ourselves

00:36:55,740 --> 00:37:01,500
every year of Ramadan to understand even

00:36:59,730 --> 00:37:05,820
from an operational perspective we see

00:37:01,500 --> 00:37:09,180
the the world change I think when when

00:37:05,820 --> 00:37:12,900
someone releases a big attract like some

00:37:09,180 --> 00:37:15,390
years ago was Miles mill had a has some

00:37:12,900 --> 00:37:17,599
kind of battle with with Drake or

00:37:15,390 --> 00:37:21,030
whatever and I think our ingestion

00:37:17,599 --> 00:37:23,580
infrastructure like scaled by an order

00:37:21,030 --> 00:37:26,810
of magnitude or something like that and

00:37:23,580 --> 00:37:29,820
in this case you know it really is

00:37:26,810 --> 00:37:33,060
everyone getting paged jumping on a

00:37:29,820 --> 00:37:35,730
channel and speculation you know there's

00:37:33,060 --> 00:37:37,800
very labor-intensive cost because people

00:37:35,730 --> 00:37:40,170
are not doing the regular work but

00:37:37,800 --> 00:37:42,240
actually debugging and I guess the the

00:37:40,170 --> 00:37:44,400
frustrating part with these cases of

00:37:42,240 --> 00:37:46,470
incident coordination is you don't know

00:37:44,400 --> 00:37:49,800
if you're doing the right thing actually

00:37:46,470 --> 00:37:53,700
like should I be looking is it is it you

00:37:49,800 --> 00:37:57,180
like is it is it this person but the

00:37:53,700 --> 00:37:59,880
reality it happens and I hope in this

00:37:57,180 --> 00:38:02,099
talk I didn't really present mechanism I

00:37:59,880 --> 00:38:04,290
didn't provide any recommendation on how

00:38:02,099 --> 00:38:07,770
to detect it but instead how to

00:38:04,290 --> 00:38:09,630
remediate it that once you if you say

00:38:07,770 --> 00:38:11,720
that is a spam attack you know that data

00:38:09,630 --> 00:38:15,420
is going to have to be reprocessed and

00:38:11,720 --> 00:38:17,910
that part you can automate but figuring

00:38:15,420 --> 00:38:18,779
out as a company you know that you might

00:38:17,910 --> 00:38:20,999
be under

00:38:18,779 --> 00:38:24,059
attack from a state actor or whatever

00:38:20,999 --> 00:38:27,119
it's really expensive it's all hands on

00:38:24,059 --> 00:38:37,319
deck and it's an amazing process to go

00:38:27,119 --> 00:38:39,739
through actually I agree any other

00:38:37,319 --> 00:38:39,739
questions

00:38:41,419 --> 00:38:52,690
let's thank the speaker again

00:38:43,770 --> 00:38:52,690

YouTube URL: https://www.youtube.com/watch?v=IHbh_MINeIM


