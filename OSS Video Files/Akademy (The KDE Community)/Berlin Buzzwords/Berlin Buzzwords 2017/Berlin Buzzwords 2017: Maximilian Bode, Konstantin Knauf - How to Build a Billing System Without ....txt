Title: Berlin Buzzwords 2017: Maximilian Bode, Konstantin Knauf - How to Build a Billing System Without ...
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Traditionally, big data applications rely on the Lambda Architecture in order to achieve low latency as well as completeness. A streaming layer provides real-time previews while a complementary batch layer retrospectively recomputes the correct results. Using a robust stream processor like Apache Flink, we can do without the latter. But can we take it even one step further? This talk will discuss one of the upcoming features of Apache Flink with the potential to do just that.

As a real-world example we have built a prototype for a robust billing system based on Flink and Queryable State. On the one hand, the system exposes the current monthly subtotals in real-time to front-end applications, on the other hand it reports the complete results to downstream systems, e.g. for invoicing. As completeness and correctness are core requirements for a billing system, we will demonstrate the system in multiple failure scenarios, including taskmanager and jobmanager failures as well as unavailability of downstream systems.

This talk will give you an idea of how "Queryable State" combined with a robust stream processor enables new streaming use cases and changes the future of streaming application architecture.

Read more:
https://2017.berlinbuzzwords.de/17/session/queryable-state-or-how-build-billing-system-without-database

About Maximilian Bode:
https://2017.berlinbuzzwords.de/users/maximilian-bode

About Konstantin Knauf:
https://2017.berlinbuzzwords.de/users/konstantin-knauf

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,790 --> 00:00:12,880
yeah hi and welcome to this session with

00:00:10,990 --> 00:00:16,420
my colleague Konstantin my name is

00:00:12,880 --> 00:00:18,400
Maximilian and we both work for T and G

00:00:16,420 --> 00:00:19,800
technology consulting which is

00:00:18,400 --> 00:00:26,140
munich-based

00:00:19,800 --> 00:00:29,619
and focused on high-end IT consulting we

00:00:26,140 --> 00:00:31,419
are currently I'm especially focused on

00:00:29,619 --> 00:00:34,680
distributed systems and particular

00:00:31,419 --> 00:00:39,160
real-time processing we both work for

00:00:34,680 --> 00:00:41,770
four telefÃ³nica Germany where we have

00:00:39,160 --> 00:00:43,540
built a platform that's processing

00:00:41,770 --> 00:00:45,460
billions of events a day and we are

00:00:43,540 --> 00:00:47,620
proud to say that we are running linked

00:00:45,460 --> 00:00:52,180
in production since the beginning of

00:00:47,620 --> 00:00:54,850
last year what is this talk going to be

00:00:52,180 --> 00:00:59,410
about and you have already from the

00:00:54,850 --> 00:01:02,410
title it's about variable to date so in

00:00:59,410 --> 00:01:05,049
the first part we want to talk a little

00:01:02,410 --> 00:01:08,979
bit about what that is give you a bit of

00:01:05,049 --> 00:01:12,729
motivation secondly we want to share

00:01:08,979 --> 00:01:15,819
with you a use case um in this context

00:01:12,729 --> 00:01:18,460
and requirements of this use case we

00:01:15,819 --> 00:01:23,020
build a prototype um for billing system

00:01:18,460 --> 00:01:25,829
and in the third and a large part of the

00:01:23,020 --> 00:01:28,990
talk we are going to take this prototype

00:01:25,829 --> 00:01:32,409
through different failure scenarios and

00:01:28,990 --> 00:01:39,399
show you how it behaves it's Mike

00:01:32,409 --> 00:01:42,700
somewhat off working yeah okay can I

00:01:39,399 --> 00:01:44,920
continue yes okay so let's give Zara's

00:01:42,700 --> 00:01:48,009
variable stage it's as I've already told

00:01:44,920 --> 00:01:52,179
you it's a feature that was introduced

00:01:48,009 --> 00:01:55,450
in swing one or two um and I want to

00:01:52,179 --> 00:01:58,779
give firstly a quick motivation about

00:01:55,450 --> 00:02:01,810
that um so in the next few slides

00:01:58,779 --> 00:02:04,569
um let's together consider a system with

00:02:01,810 --> 00:02:07,679
two somewhat competing some requirements

00:02:04,569 --> 00:02:11,470
on the one hand you need the system to

00:02:07,679 --> 00:02:12,790
be able to guarantee correctness but

00:02:11,470 --> 00:02:17,680
then on the other hand you want to be

00:02:12,790 --> 00:02:19,220
able to generate low latency inside so

00:02:17,680 --> 00:02:22,430
for example

00:02:19,220 --> 00:02:24,590
of some kind of a webshop where for each

00:02:22,430 --> 00:02:28,460
user you want to know exactly how many

00:02:24,590 --> 00:02:32,930
clicks he had in a day and you want this

00:02:28,460 --> 00:02:36,500
number to be exact but then you also you

00:02:32,930 --> 00:02:38,870
also need um let's say every minute the

00:02:36,500 --> 00:02:42,320
average clicks across users or something

00:02:38,870 --> 00:02:44,270
like that so traditionally what people

00:02:42,320 --> 00:02:46,880
have used in order to solve problems

00:02:44,270 --> 00:02:48,980
like this is a dual approach known as

00:02:46,880 --> 00:02:51,760
the lambda architecture I'm sure many of

00:02:48,980 --> 00:02:56,750
you will know this um so here you have

00:02:51,760 --> 00:03:01,250
two dual layers feed layer with a stream

00:02:56,750 --> 00:03:06,020
processor for example storm but this

00:03:01,250 --> 00:03:08,510
this delivers low latency results but

00:03:06,020 --> 00:03:11,990
can't really be trusted correctness wise

00:03:08,510 --> 00:03:15,170
and then you have a batch layer running

00:03:11,990 --> 00:03:17,480
MapReduce jobs one today and that that

00:03:15,170 --> 00:03:21,260
really computes correct results to be

00:03:17,480 --> 00:03:23,450
trusted right and and so one could see

00:03:21,260 --> 00:03:25,280
some some problems with this kind of

00:03:23,450 --> 00:03:27,410
architecture you have this separate

00:03:25,280 --> 00:03:29,930
systems that you need to develop for in

00:03:27,410 --> 00:03:31,190
different frameworks you have to operate

00:03:29,930 --> 00:03:36,820
the system separately

00:03:31,190 --> 00:03:49,700
um so modern stream processors yeah

00:03:36,820 --> 00:03:53,030
super off okay come video do you want to

00:03:49,700 --> 00:03:59,150
talk to me yeah I think you can give me

00:03:53,030 --> 00:04:00,769
any ways no okay yeah so where was I

00:03:59,150 --> 00:04:04,250
modern stream process is like a kerchief

00:04:00,769 --> 00:04:06,110
link um that are fault tolerant and

00:04:04,250 --> 00:04:09,769
resilient against failures enable us to

00:04:06,110 --> 00:04:12,709
do away with this patch layer so in in

00:04:09,769 --> 00:04:15,010
flink you could just aggregate the those

00:04:12,709 --> 00:04:17,989
minute wise aggregations I talked about

00:04:15,010 --> 00:04:19,729
and put it into a key value store but

00:04:17,989 --> 00:04:22,070
flink can guarantee you also under

00:04:19,729 --> 00:04:25,070
failures and that the results will be

00:04:22,070 --> 00:04:28,430
correct at the end of the day so this is

00:04:25,070 --> 00:04:30,440
the step forward but still um one has

00:04:28,430 --> 00:04:32,070
this external key value store for

00:04:30,440 --> 00:04:34,500
example Redis or whatever

00:04:32,070 --> 00:04:36,270
that needs to be operated operated that

00:04:34,500 --> 00:04:40,020
needs to be scared independently of your

00:04:36,270 --> 00:04:42,030
processing and your stream processor in

00:04:40,020 --> 00:04:44,610
order to give these strong guarantees

00:04:42,030 --> 00:04:46,710
already has to be able to keep track of

00:04:44,610 --> 00:04:48,660
the state internally so why don't we

00:04:46,710 --> 00:04:51,750
just go directly to the stream processor

00:04:48,660 --> 00:04:54,210
and that is what we will state does so

00:04:51,750 --> 00:04:56,460
if you know want to see what what is the

00:04:54,210 --> 00:04:59,390
current average over the last minute of

00:04:56,460 --> 00:05:02,520
clicks you just ask the stream processor

00:04:59,390 --> 00:05:06,650
that's the idea about variable States I

00:05:02,520 --> 00:05:08,190
hope I could give you some motivation

00:05:06,650 --> 00:05:09,870
even though there were technical

00:05:08,190 --> 00:05:11,850
problems but now Constantine is going to

00:05:09,870 --> 00:05:15,260
talk about how its implemented in flink

00:05:11,850 --> 00:05:20,340
right good afternoon for myself as well

00:05:15,260 --> 00:05:23,450
so I'm guessing most of you know what

00:05:20,340 --> 00:05:26,910
Apache Fling is just to just to recap

00:05:23,450 --> 00:05:28,170
the different components they are you

00:05:26,910 --> 00:05:31,020
have the chart manager which is

00:05:28,170 --> 00:05:33,240
basically the master node so to say

00:05:31,020 --> 00:05:35,580
which does all the coordination between

00:05:33,240 --> 00:05:37,200
the different task managers but it

00:05:35,580 --> 00:05:40,440
doesn't do any of the work itself and

00:05:37,200 --> 00:05:45,960
doesn't hold any state and then there

00:05:40,440 --> 00:05:48,390
you have a lot of you have a lot of task

00:05:45,960 --> 00:05:51,390
managers multiple task managers which

00:05:48,390 --> 00:05:53,820
actually do processing which are the

00:05:51,390 --> 00:05:55,440
workers and the case of Kerrville state

00:05:53,820 --> 00:05:58,320
you also have the Kerrville State client

00:05:55,440 --> 00:06:00,450
which is just some process outside of

00:05:58,320 --> 00:06:02,750
the cluster I think right now it's only

00:06:00,450 --> 00:06:06,710
possible with the Java process but for

00:06:02,750 --> 00:06:10,500
only Java client is feasible but

00:06:06,710 --> 00:06:12,390
separated from everything else okay

00:06:10,500 --> 00:06:14,700
let's look a little bit into more detail

00:06:12,390 --> 00:06:19,530
how this thing works for querulous State

00:06:14,700 --> 00:06:23,310
so as I said the status so we'll focus

00:06:19,530 --> 00:06:25,890
on the green boxes right now so the

00:06:23,310 --> 00:06:27,480
curve will state as I said is only what

00:06:25,890 --> 00:06:30,690
the state is only located in the task

00:06:27,480 --> 00:06:33,450
managers so for the Kyrgyzstan client to

00:06:30,690 --> 00:06:36,240
retrieve the state it goes to the task

00:06:33,450 --> 00:06:38,220
manager directly so the query will state

00:06:36,240 --> 00:06:39,840
client which is in the client code the

00:06:38,220 --> 00:06:43,580
key value state client goes to the key

00:06:39,840 --> 00:06:43,580
value state server of the task managers

00:06:44,240 --> 00:06:51,500
the most frequently used sort of state

00:06:47,840 --> 00:06:53,509
and flink is keep state so depending on

00:06:51,500 --> 00:06:56,539
your key the state is distributed about

00:06:53,509 --> 00:06:58,430
among multiple task managers so if you

00:06:56,539 --> 00:07:01,400
now think of the key s whistle

00:06:58,430 --> 00:07:02,930
subscriber then for different

00:07:01,400 --> 00:07:04,610
subscribers the key is located on

00:07:02,930 --> 00:07:07,699
different task managers now the key

00:07:04,610 --> 00:07:10,130
values the cruellest a client needs to

00:07:07,699 --> 00:07:13,569
know which cat has managed to go to for

00:07:10,130 --> 00:07:18,229
a particular for a particular subscriber

00:07:13,569 --> 00:07:21,169
so since the tasman is or since the duck

00:07:18,229 --> 00:07:23,380
has no way to know it has this key value

00:07:21,169 --> 00:07:26,030
state location lookup service which

00:07:23,380 --> 00:07:29,330
exactly does that it asked the job

00:07:26,030 --> 00:07:31,550
manager where does subscriber a has its

00:07:29,330 --> 00:07:35,120
state and then it goes directly for the

00:07:31,550 --> 00:07:38,509
task manager for the actual data these

00:07:35,120 --> 00:07:40,310
are basically the purple boxes so how

00:07:38,509 --> 00:07:42,819
does the job manager know where each

00:07:40,310 --> 00:07:47,360
state is located on the task manager

00:07:42,819 --> 00:07:50,449
that's just through the usual occur

00:07:47,360 --> 00:07:52,130
based notification services between the

00:07:50,449 --> 00:07:54,830
task management job manager it's just a

00:07:52,130 --> 00:07:56,539
different account different occur

00:07:54,830 --> 00:07:59,599
message that the husband just sent to

00:07:56,539 --> 00:08:03,880
the job manager same way as for example

00:07:59,599 --> 00:08:10,550
I finished processing this task or I'm

00:08:03,880 --> 00:08:12,320
I'm currently opening this operator so

00:08:10,550 --> 00:08:14,300
then the only thing the Kerrville state

00:08:12,320 --> 00:08:16,430
client needs to know is the job manager

00:08:14,300 --> 00:08:18,469
and if you have a high availability set

00:08:16,430 --> 00:08:20,409
up their mouth full job managers so it

00:08:18,469 --> 00:08:24,740
has to know the active job manager

00:08:20,409 --> 00:08:27,500
because only this job manager gets all

00:08:24,740 --> 00:08:30,199
the updates and for this this the leader

00:08:27,500 --> 00:08:31,699
retrieval service it's now also wrapped

00:08:30,199 --> 00:08:34,669
into a high availability service I think

00:08:31,699 --> 00:08:36,079
but basically this we need a retrieval

00:08:34,669 --> 00:08:40,610
service which is called every time

00:08:36,079 --> 00:08:42,800
there's a job manager failover so what

00:08:40,610 --> 00:08:47,440
you should take away from this slide is

00:08:42,800 --> 00:08:50,029
basically that global State is over or

00:08:47,440 --> 00:08:52,850
usually it's only communication between

00:08:50,029 --> 00:08:55,070
the client and the task managers and the

00:08:52,850 --> 00:08:57,890
top managers only ask once where does

00:08:55,070 --> 00:09:02,209
this key live and then this is cached

00:08:57,890 --> 00:09:04,370
because so that's important basically to

00:09:02,209 --> 00:09:05,720
understand the behavior and some of the

00:09:04,370 --> 00:09:08,209
failure scenarios we will see in the

00:09:05,720 --> 00:09:12,440
demo when we kill task managers when we

00:09:08,209 --> 00:09:15,070
kill shell managers okay ila any

00:09:12,440 --> 00:09:15,070
questions so far

00:09:15,310 --> 00:09:20,630
all right

00:09:16,700 --> 00:09:24,230
so just a quick introduction of our use

00:09:20,630 --> 00:09:28,700
case we called it queryable billing and

00:09:24,230 --> 00:09:31,279
basically a prototypical use case which

00:09:28,700 --> 00:09:33,680
we have condensed from mainly one

00:09:31,279 --> 00:09:36,890
project we've seen at our client not our

00:09:33,680 --> 00:09:40,339
own project but a yeah

00:09:36,890 --> 00:09:42,230
adjacent project so to say which was

00:09:40,339 --> 00:09:43,610
solve completely differently but then we

00:09:42,230 --> 00:09:46,130
heard about core of the state and flink

00:09:43,610 --> 00:09:48,290
and we're like ok most of that we could

00:09:46,130 --> 00:09:51,820
have done both links and we just wanted

00:09:48,290 --> 00:09:54,649
to try out how how could one solve that

00:09:51,820 --> 00:09:57,160
that's for the background so what were

00:09:54,649 --> 00:09:59,450
the requirements

00:09:57,160 --> 00:10:01,160
basically you have a telecommunication

00:09:59,450 --> 00:10:03,649
Network and this telecommunication

00:10:01,160 --> 00:10:08,149
network generates a couple of events a

00:10:03,649 --> 00:10:10,490
lot of events usually and these events

00:10:08,149 --> 00:10:13,550
need to be built so if there's a call

00:10:10,490 --> 00:10:15,800
there's a text message data usage Peck

00:10:13,550 --> 00:10:19,699
texts which are booked like daily Fred

00:10:15,800 --> 00:10:23,089
sled race and this kind of stuff and all

00:10:19,699 --> 00:10:27,199
these events are collected from the

00:10:23,089 --> 00:10:29,959
network component and they go into the

00:10:27,199 --> 00:10:31,730
system and the main purpose of this

00:10:29,959 --> 00:10:33,829
system is to aggregate them over months

00:10:31,730 --> 00:10:35,089
aggregate the usage and then forward

00:10:33,829 --> 00:10:37,279
this information to the downstream

00:10:35,089 --> 00:10:40,360
system which in our case we said is the

00:10:37,279 --> 00:10:45,070
invoicing system which is able to send

00:10:40,360 --> 00:10:48,050
mail in mail invoices to customers

00:10:45,070 --> 00:10:51,250
besides this main use case we also have

00:10:48,050 --> 00:10:55,310
a site use case which is on the bottom

00:10:51,250 --> 00:10:56,990
we also want our clients to give our

00:10:55,310 --> 00:10:59,120
clients the opportunity to query their

00:10:56,990 --> 00:11:02,510
current usage in the month by a web

00:10:59,120 --> 00:11:05,990
application and you can probably already

00:11:02,510 --> 00:11:08,800
guess variable state I'm just coming in

00:11:05,990 --> 00:11:11,000
here the one at the bottom is very

00:11:08,800 --> 00:11:15,040
similar from technical

00:11:11,000 --> 00:11:17,900
a few very similar requirement probably

00:11:15,040 --> 00:11:19,820
wants management has heard that you can

00:11:17,900 --> 00:11:21,710
do this kind of stuff they want some

00:11:19,820 --> 00:11:23,600
dashboards to do some real-time

00:11:21,710 --> 00:11:24,740
monitoring monitoring on marketing

00:11:23,600 --> 00:11:27,230
campaigns and so on

00:11:24,740 --> 00:11:30,560
maybe you split by different usage

00:11:27,230 --> 00:11:35,630
scenarios textbook packages which is

00:11:30,560 --> 00:11:37,160
booked and so on and so on so these are

00:11:35,630 --> 00:11:38,630
basically the functional requirements

00:11:37,160 --> 00:11:41,450
but there are also some some quality

00:11:38,630 --> 00:11:43,720
goals first of all there's correctness

00:11:41,450 --> 00:11:48,250
we're dealing with invoices here if you

00:11:43,720 --> 00:11:50,510
send out a lot of invoices with which

00:11:48,250 --> 00:11:53,600
over count events and stuff like that

00:11:50,510 --> 00:11:58,280
you will have a lot of a lot of traffic

00:11:53,600 --> 00:12:00,020
in your user desk and ya lose money this

00:11:58,280 --> 00:12:06,610
way the other way around you lose money

00:12:00,020 --> 00:12:09,110
directly by charging people um to view

00:12:06,610 --> 00:12:12,020
so that's correctness that's number one

00:12:09,110 --> 00:12:14,330
criterion then you also have robustness

00:12:12,020 --> 00:12:16,040
if we're dealing with with a distributed

00:12:14,330 --> 00:12:18,560
system here you always have partial

00:12:16,040 --> 00:12:21,950
false task managers go down shop

00:12:18,560 --> 00:12:25,520
managers go down um maybe the the

00:12:21,950 --> 00:12:27,200
invoicing system goes down so it needs

00:12:25,520 --> 00:12:29,990
to be robust to these kind of failures

00:12:27,200 --> 00:12:32,990
but it also needs to be robust to out of

00:12:29,990 --> 00:12:34,520
order data to late events especially

00:12:32,990 --> 00:12:36,860
events are coming from such a

00:12:34,520 --> 00:12:38,420
heterogeneous network as a

00:12:36,860 --> 00:12:41,990
telecommunication networks were also

00:12:38,420 --> 00:12:45,620
always some qualified can have a lag of

00:12:41,990 --> 00:12:47,839
half an hour or events just don't and

00:12:45,620 --> 00:12:51,860
look forward to through this data center

00:12:47,839 --> 00:12:53,390
anymore availability is not that much of

00:12:51,860 --> 00:12:56,060
an issue if you only want to write

00:12:53,390 --> 00:12:57,230
invoices I mean it only has to be

00:12:56,060 --> 00:13:00,740
available at the end of the month

00:12:57,230 --> 00:13:02,900
basically but once you want to also

00:13:00,740 --> 00:13:05,690
enable the customers to query their life

00:13:02,900 --> 00:13:08,560
usage you want the system to be up and

00:13:05,690 --> 00:13:11,360
running most of the time at least

00:13:08,560 --> 00:13:14,480
scalability is not a core requirement

00:13:11,360 --> 00:13:16,460
here and of course it's nice if it's

00:13:14,480 --> 00:13:17,600
elastic because then you can scale it

00:13:16,460 --> 00:13:22,089
down during the night where they are

00:13:17,600 --> 00:13:24,790
much less event but yeah we will only

00:13:22,089 --> 00:13:29,140
basically we won't do that you

00:13:24,790 --> 00:13:32,470
okay now let's let's talk about the

00:13:29,140 --> 00:13:35,230
architecture and Maxwell do that so so

00:13:32,470 --> 00:13:37,030
as far as the technology spec is

00:13:35,230 --> 00:13:37,990
concerned that we chose to use for this

00:13:37,030 --> 00:13:40,780
prototype

00:13:37,990 --> 00:13:42,430
so your basic sketch of that so on on

00:13:40,780 --> 00:13:44,770
the left hand side you have antennae

00:13:42,430 --> 00:13:48,010
which signalize will just probably

00:13:44,770 --> 00:13:52,240
stands for data generator and that we

00:13:48,010 --> 00:13:55,270
use to generate events that then are to

00:13:52,240 --> 00:13:58,870
be built somehow and this the data

00:13:55,270 --> 00:14:01,030
generator has some amount of determinism

00:13:58,870 --> 00:14:02,800
inside it but also some amount of

00:14:01,030 --> 00:14:04,390
randomness we are going to go into more

00:14:02,800 --> 00:14:07,360
detail on that later

00:14:04,390 --> 00:14:09,070
and then those events are certainly

00:14:07,360 --> 00:14:11,860
written to to Kafka which is a

00:14:09,070 --> 00:14:14,110
distributed message message broker might

00:14:11,860 --> 00:14:18,160
ask you if you haven't heard of that and

00:14:14,110 --> 00:14:21,610
from where our main component reads in

00:14:18,160 --> 00:14:24,310
those events which is a fling job the

00:14:21,610 --> 00:14:26,080
little squirrel over there and then the

00:14:24,310 --> 00:14:29,080
invoices are written to a distributed

00:14:26,080 --> 00:14:32,620
file system and the idea is that from

00:14:29,080 --> 00:14:34,720
this distributed file system the another

00:14:32,620 --> 00:14:37,270
adjacent system that then sends out

00:14:34,720 --> 00:14:40,420
mails a mail with the bills to our

00:14:37,270 --> 00:14:43,620
customers could could fetch those final

00:14:40,420 --> 00:14:45,700
bills and not not to remarks

00:14:43,620 --> 00:14:47,380
particularly interesting for people who

00:14:45,700 --> 00:14:51,900
are already somewhat familiar with link

00:14:47,380 --> 00:14:54,850
um first notice that this configuration

00:14:51,900 --> 00:14:57,070
reading from Kafka and writing to a

00:14:54,850 --> 00:14:59,290
distributed file system it's one where

00:14:57,070 --> 00:15:01,740
exactly once as possible right because

00:14:59,290 --> 00:15:05,020
in the case of failures you can restore

00:15:01,740 --> 00:15:07,000
from a checkpoint and and then you can

00:15:05,020 --> 00:15:09,490
on the one hand rewind your Kafka source

00:15:07,000 --> 00:15:11,530
and also truncate the file system and

00:15:09,490 --> 00:15:14,530
then you can start reprocessing without

00:15:11,530 --> 00:15:17,650
duplicating any events right and then

00:15:14,530 --> 00:15:20,410
secondly um well minor point but this

00:15:17,650 --> 00:15:21,790
this the fact that we are using a

00:15:20,410 --> 00:15:24,850
distributed file system well we chose

00:15:21,790 --> 00:15:28,660
that for this prototype but one could

00:15:24,850 --> 00:15:30,790
have as well this invoicing system yeah

00:15:28,660 --> 00:15:33,550
being implemented as a service that

00:15:30,790 --> 00:15:36,520
accepts idempotent HTTP requests yeah

00:15:33,550 --> 00:15:37,170
and and it worked worked just exactly

00:15:36,520 --> 00:15:40,100
the same

00:15:37,170 --> 00:15:43,470
so that was the horizontal lane of

00:15:40,100 --> 00:15:45,959
invoice creation now for the for the

00:15:43,470 --> 00:15:50,309
part that's actually concerned with

00:15:45,959 --> 00:15:53,779
queryable state from our perspective the

00:15:50,309 --> 00:15:56,279
variable state lines is still a little

00:15:53,779 --> 00:15:58,559
rough around the edges you probably

00:15:56,279 --> 00:16:01,230
wouldn't want to put put it directly in

00:15:58,559 --> 00:16:04,499
your front end so what we did is wrap it

00:16:01,230 --> 00:16:07,169
in a spring good application and then

00:16:04,499 --> 00:16:09,660
the JavaScript front-end can fetch its

00:16:07,169 --> 00:16:12,480
data from there and also here on the

00:16:09,660 --> 00:16:16,049
right hand side you could build a

00:16:12,480 --> 00:16:18,209
dashboard and it was like Kahana and but

00:16:16,049 --> 00:16:19,799
we aren't going to go into more detail

00:16:18,209 --> 00:16:23,279
here because it's functionally just the

00:16:19,799 --> 00:16:28,259
same and so rather let's zoom in on the

00:16:23,279 --> 00:16:30,149
flink job the job graph for those

00:16:28,259 --> 00:16:31,790
already familiar with pictures like this

00:16:30,149 --> 00:16:35,609
is what a fling job graph looks like

00:16:31,790 --> 00:16:38,069
isn't super spectacular right so on the

00:16:35,609 --> 00:16:40,470
left hand side the events from Kafka are

00:16:38,069 --> 00:16:42,299
all right in timestamps are assigned

00:16:40,470 --> 00:16:45,499
watermarks are emitted and so on and

00:16:42,299 --> 00:16:48,809
then we we want to aggregate a

00:16:45,499 --> 00:16:51,660
customer's usage for the month so we do

00:16:48,809 --> 00:16:54,540
30 day time windows and follows all

00:16:51,660 --> 00:16:57,299
these events up um basically sum up the

00:16:54,540 --> 00:16:59,699
billable amounts and then write out

00:16:57,299 --> 00:17:02,040
using marketing thing which is strings

00:16:59,699 --> 00:17:05,579
standard way of writing to a distributed

00:17:02,040 --> 00:17:07,740
file system and then what one actually

00:17:05,579 --> 00:17:11,069
would like to do in an ideal world is

00:17:07,740 --> 00:17:13,409
make this state in those windows

00:17:11,069 --> 00:17:14,699
variable directly unfortunately right

00:17:13,409 --> 00:17:17,490
now that's not possible

00:17:14,699 --> 00:17:20,130
um so just as a quick technical aside

00:17:17,490 --> 00:17:23,250
how did we do with this I'm basically

00:17:20,130 --> 00:17:27,959
duplicated the whole logic with a false

00:17:23,250 --> 00:17:29,760
function and trigger the windows on each

00:17:27,959 --> 00:17:31,710
event that comes in and then in the

00:17:29,760 --> 00:17:34,669
window function that is applied on each

00:17:31,710 --> 00:17:37,649
event and this date can be made credible

00:17:34,669 --> 00:17:39,480
so this is the second block that you see

00:17:37,649 --> 00:17:41,850
on the right-hand side of the graph and

00:17:39,480 --> 00:17:44,909
slot is basically the same but not keep

00:17:41,850 --> 00:17:47,429
by customer but keep by event type so

00:17:44,909 --> 00:17:48,890
how many calls whether and this month

00:17:47,429 --> 00:17:51,020
how many

00:17:48,890 --> 00:17:54,760
messages that actually builds how many

00:17:51,020 --> 00:17:57,170
bookings or flat rates or whatever yeah

00:17:54,760 --> 00:17:58,910
now we are going to go into the most

00:17:57,170 --> 00:18:04,760
interesting part so talk

00:17:58,910 --> 00:18:09,530
yes our demo so we basically set up the

00:18:04,760 --> 00:18:12,020
whole system in a darker setup so let's

00:18:09,530 --> 00:18:15,080
just do to give you a rough idea how

00:18:12,020 --> 00:18:17,210
it's all set up let's see what

00:18:15,080 --> 00:18:19,010
containers there are and if there are

00:18:17,210 --> 00:18:21,860
any questions during the demo anything

00:18:19,010 --> 00:18:23,690
looks suspicious or dubious please ask

00:18:21,860 --> 00:18:27,950
right away many people from the back

00:18:23,690 --> 00:18:31,490
reef resource or yeah okay so as you can

00:18:27,950 --> 00:18:32,900
see we have I think 10 container there

00:18:31,490 --> 00:18:35,000
are four containers which are just a

00:18:32,900 --> 00:18:39,140
fling cluster to job managers and to

00:18:35,000 --> 00:18:43,430
task managers so that we can kill one of

00:18:39,140 --> 00:18:46,280
them and it's still somehow working then

00:18:43,430 --> 00:18:48,070
there's a one casket broker which makes

00:18:46,280 --> 00:18:50,540
up the whole casket cluster and

00:18:48,070 --> 00:18:52,580
zookeeper at the bottom which supports

00:18:50,540 --> 00:18:55,340
Kafka and also the job manager high

00:18:52,580 --> 00:18:58,040
availability of link and then there we

00:18:55,340 --> 00:19:01,700
have four containers which are basically

00:18:58,040 --> 00:19:04,010
the queryable billing from the data

00:19:01,700 --> 00:19:07,040
generator which just generates these

00:19:04,010 --> 00:19:10,490
data all the time these events and put

00:19:07,040 --> 00:19:13,160
them into Kafka then there is the job

00:19:10,490 --> 00:19:15,020
itself the QB job which is just the

00:19:13,160 --> 00:19:16,100
client which submitted to the job to the

00:19:15,020 --> 00:19:19,670
cluster and doesn't do anything

00:19:16,100 --> 00:19:21,530
afterwards there's the QB server which

00:19:19,670 --> 00:19:25,070
is this small spring boot application

00:19:21,530 --> 00:19:27,530
and the front end is also a separate

00:19:25,070 --> 00:19:32,470
container which just serve statically

00:19:27,530 --> 00:19:36,950
the the content for the web application

00:19:32,470 --> 00:19:39,530
ok so let's first recap the requirements

00:19:36,950 --> 00:19:43,010
and we will basically go through them

00:19:39,530 --> 00:19:44,330
and check them during the demo so we

00:19:43,010 --> 00:19:46,610
have the functional requirements which

00:19:44,330 --> 00:19:50,180
are on one hand correct invoices on the

00:19:46,610 --> 00:19:54,190
other hand live updates verbal wireless

00:19:50,180 --> 00:19:56,879
API then we have the non-functional

00:19:54,190 --> 00:19:58,889
quality goals

00:19:56,879 --> 00:20:00,299
which is correctness availability we

00:19:58,889 --> 00:20:02,490
won't look at scalability because we

00:20:00,299 --> 00:20:05,129
only have nine customers in this demo so

00:20:02,490 --> 00:20:10,100
we can say anything about scalability in

00:20:05,129 --> 00:20:14,460
terms of keys and was only one laptop so

00:20:10,100 --> 00:20:16,440
yeah robustness at the other hand we

00:20:14,460 --> 00:20:18,539
will look at later driving event out of

00:20:16,440 --> 00:20:21,419
ordinance we'll look at tasks manage and

00:20:18,539 --> 00:20:23,759
job manager failures and also we'll look

00:20:21,419 --> 00:20:28,440
at failures of the downstream system so

00:20:23,759 --> 00:20:30,990
just basically the billing system the

00:20:28,440 --> 00:20:35,850
invoicing system okay let's start with

00:20:30,990 --> 00:20:40,049
just invoice generation so - so what you

00:20:35,850 --> 00:20:42,809
can see here is first of all the link UI

00:20:40,049 --> 00:20:46,100
up there the job is running for 24

00:20:42,809 --> 00:20:48,570
minutes now and data is being generated

00:20:46,100 --> 00:20:58,289
so let's let's have a look at the stream

00:20:48,570 --> 00:21:02,580
of data which goes into can we so it's

00:20:58,289 --> 00:21:05,940
that readable from the back yeah okay

00:21:02,580 --> 00:21:08,820
so basically the events just have a

00:21:05,940 --> 00:21:12,330
timestamp they have a name Emma Sophia

00:21:08,820 --> 00:21:17,269
Noah and so on there's some euro amount

00:21:12,330 --> 00:21:19,769
and a type package message and so on and

00:21:17,269 --> 00:21:23,340
what the state of generator basically

00:21:19,769 --> 00:21:25,019
does is it generates random amounts but

00:21:23,340 --> 00:21:28,110
they always add up to the same value for

00:21:25,019 --> 00:21:30,659
each customer for each month so it's for

00:21:28,110 --> 00:21:32,490
Emma for example is always 100 euros at

00:21:30,659 --> 00:21:35,340
the end of the month everything went

00:21:32,490 --> 00:21:38,580
well so this is our way now in this demo

00:21:35,340 --> 00:21:41,370
- to see if we lost any elements or if

00:21:38,580 --> 00:21:43,320
we over counted if at the end of the

00:21:41,370 --> 00:21:48,659
month it's always an even amount for

00:21:43,320 --> 00:21:50,129
each customer then that looks good so we

00:21:48,659 --> 00:21:52,889
said we were writing to a distributed

00:21:50,129 --> 00:21:54,000
file system for this docker setup you

00:21:52,889 --> 00:21:55,470
might have noticed we didn't set up

00:21:54,000 --> 00:21:58,799
anything like HDFS or something like

00:21:55,470 --> 00:22:00,240
that so what we are just did is we

00:21:58,799 --> 00:22:02,309
mounted a darker volume to all the

00:22:00,240 --> 00:22:05,759
containers or to the flame containers

00:22:02,309 --> 00:22:08,960
and we use this darker volume as kind of

00:22:05,759 --> 00:22:14,340
a mock distributed file system

00:22:08,960 --> 00:22:18,200
it behaves surprisingly similar in some

00:22:14,340 --> 00:22:21,630
case yeah actually there are some

00:22:18,200 --> 00:22:24,299
problems but yeah so here we have then

00:22:21,630 --> 00:22:30,650
one folder for each month and if we look

00:22:24,299 --> 00:22:35,220
into it it just gives you the month the

00:22:30,650 --> 00:22:39,390
subscriber user and then the sum so as

00:22:35,220 --> 00:22:41,490
you can see it's all even amount but

00:22:39,390 --> 00:22:45,570
there's actually one amount which is not

00:22:41,490 --> 00:22:49,020
even which is William in March 89 we

00:22:45,570 --> 00:22:51,450
started at at the FO 1970 so let's see

00:22:49,020 --> 00:22:54,299
how how we get about 30 seconds per

00:22:51,450 --> 00:22:57,660
month the event time progress in this

00:22:54,299 --> 00:22:59,190
demo so William had two hundred forty

00:22:57,660 --> 00:23:01,530
nine ninety four but then there was a

00:22:59,190 --> 00:23:02,940
late arriving event so the window got

00:23:01,530 --> 00:23:05,820
fired again if you're familiar with

00:23:02,940 --> 00:23:09,419
things way of dealing with lateness

00:23:05,820 --> 00:23:12,140
so we window always fires when the check

00:23:09,419 --> 00:23:16,380
point for when the watermark for this

00:23:12,140 --> 00:23:19,470
time passes but then you can also say I

00:23:16,380 --> 00:23:20,640
want to keep the stayed around for some

00:23:19,470 --> 00:23:22,290
more time which is called a loud

00:23:20,640 --> 00:23:22,980
maintenance and if another event comes

00:23:22,290 --> 00:23:25,320
after that

00:23:22,980 --> 00:23:28,440
it will fire again and that's what you

00:23:25,320 --> 00:23:31,080
see now here in case of severe when

00:23:28,440 --> 00:23:33,900
there was one event event missing and

00:23:31,080 --> 00:23:35,790
the window got already fired but then

00:23:33,900 --> 00:23:40,140
there was an update afterwards with late

00:23:35,790 --> 00:23:43,250
and arriving events so if we go back to

00:23:40,140 --> 00:23:47,340
our requirements inverse generation

00:23:43,250 --> 00:23:50,490
works I mean it's pretty basic but it

00:23:47,340 --> 00:23:52,110
works correctness works at least in the

00:23:50,490 --> 00:23:56,460
case where everything goes right which

00:23:52,110 --> 00:23:59,820
is easy and robustness late events work

00:23:56,460 --> 00:24:01,530
out of ordinates also work other works

00:23:59,820 --> 00:24:03,360
so this event stream we didn't see it

00:24:01,530 --> 00:24:06,390
but the event stream is pretty out of

00:24:03,360 --> 00:24:10,320
order actually and there are some late

00:24:06,390 --> 00:24:12,799
events as well these we saw so let's

00:24:10,320 --> 00:24:15,780
also have a look at the live updates now

00:24:12,799 --> 00:24:18,450
so as we set this spring food

00:24:15,780 --> 00:24:20,940
application basically exposes more wraps

00:24:18,450 --> 00:24:21,570
the queryable state client so we can do

00:24:20,940 --> 00:24:24,470
a

00:24:21,570 --> 00:24:29,210
you can just squirrel this service and

00:24:24,470 --> 00:24:33,390
here for example we get the current we

00:24:29,210 --> 00:24:35,520
basically going to customers Emer and we

00:24:33,390 --> 00:24:38,130
get the current amount and this amount

00:24:35,520 --> 00:24:40,830
increases increases and at at some point

00:24:38,130 --> 00:24:46,560
it's at 100 and then it starts for the

00:24:40,830 --> 00:24:50,400
next month again um so this this also

00:24:46,560 --> 00:24:53,970
works but - we can also have a look at

00:24:50,400 --> 00:24:56,430
at the type yeah you can also query for

00:24:53,970 --> 00:25:01,050
the type call for the type text message

00:24:56,430 --> 00:25:03,300
I think yeah but that's not that

00:25:01,050 --> 00:25:05,130
convenient for the demo now because we

00:25:03,300 --> 00:25:08,820
want to see how does it behave if we

00:25:05,130 --> 00:25:12,050
kill a task manager for example so we

00:25:08,820 --> 00:25:14,760
basically just have this very small

00:25:12,050 --> 00:25:17,450
front-end which every two seconds

00:25:14,760 --> 00:25:22,650
queries this spring good application and

00:25:17,450 --> 00:25:27,620
it shows the current the current usage

00:25:22,650 --> 00:25:30,510
of this customer as long as this

00:25:27,620 --> 00:25:33,630
timestamp is screen it means that it

00:25:30,510 --> 00:25:39,840
reached the service and the service was

00:25:33,630 --> 00:25:42,300
able to query this link cluster so yeah

00:25:39,840 --> 00:25:45,180
right now it's running so I think we can

00:25:42,300 --> 00:25:48,390
go back to the requirements and check

00:25:45,180 --> 00:25:51,240
live updates and now start with the more

00:25:48,390 --> 00:25:54,270
interesting stuff killing the D

00:25:51,240 --> 00:25:58,530
component so let's start with the task

00:25:54,270 --> 00:26:01,770
manager so we were just going to kill

00:25:58,530 --> 00:26:04,610
one of the top managers so and since

00:26:01,770 --> 00:26:09,390
this clusters very poorly provisioned

00:26:04,610 --> 00:26:10,140
this will render the job yeah not

00:26:09,390 --> 00:26:11,760
restartable

00:26:10,140 --> 00:26:14,850
because we don't have enough slots of

00:26:11,760 --> 00:26:17,910
course in any setup you wouldn't want to

00:26:14,850 --> 00:26:20,450
do that but here gives us a little bit

00:26:17,910 --> 00:26:23,100
more control when the job starts again

00:26:20,450 --> 00:26:27,450
for those of you who paid attention you

00:26:23,100 --> 00:26:30,300
saw that either was life a little bit

00:26:27,450 --> 00:26:31,680
more longer than Emma that was because

00:26:30,300 --> 00:26:33,870
we killed the task manager where the

00:26:31,680 --> 00:26:35,070
state of Emma resided and the other task

00:26:33,870 --> 00:26:36,509
manager that's the the

00:26:35,070 --> 00:26:39,330
beauty of distributed system it didn't

00:26:36,509 --> 00:26:41,580
know that the other task manager was

00:26:39,330 --> 00:26:43,710
killed and keeps running the job and was

00:26:41,580 --> 00:26:45,960
still able to serve the request by the

00:26:43,710 --> 00:26:47,759
quill estate client and only when the

00:26:45,960 --> 00:26:50,240
job manager realized okay the staff

00:26:47,759 --> 00:26:52,710
manager has gone I need to really

00:26:50,240 --> 00:26:54,450
restart the job job from a check point

00:26:52,710 --> 00:26:57,210
it killed the job on the other task

00:26:54,450 --> 00:26:59,480
manager and either wasn't able to serve

00:26:57,210 --> 00:27:02,639
these requests or the task manager

00:26:59,480 --> 00:27:03,830
wasn't able to serve the request for

00:27:02,639 --> 00:27:09,570
either as well

00:27:03,830 --> 00:27:11,610
so now let's let's restart the task

00:27:09,570 --> 00:27:15,779
manager so that we look at the inverse

00:27:11,610 --> 00:27:18,659
we can look at the universal yes so we

00:27:15,779 --> 00:27:22,830
are in the last last in words we got

00:27:18,659 --> 00:27:27,659
were from November 9 80 83 and if you

00:27:22,830 --> 00:27:30,330
look at the current date and the current

00:27:27,659 --> 00:27:34,409
date in the lock we have 90 with

00:27:30,330 --> 00:27:37,320
February 84 so what we would expect us

00:27:34,409 --> 00:27:40,340
that it basically crimes through the

00:27:37,320 --> 00:27:43,799
catalog and outputs all the invoices

00:27:40,340 --> 00:27:45,330
correctly so let's restart the task

00:27:43,799 --> 00:27:47,539
manager and see if the job comes up

00:27:45,330 --> 00:27:47,539
again

00:27:58,740 --> 00:28:06,920
okay it's run again and yeah

00:28:02,790 --> 00:28:10,220
Cuervo states also accessible and again

00:28:06,920 --> 00:28:14,790
it's basically you can already see a

00:28:10,220 --> 00:28:19,770
1984 for April 1984 so it also already

00:28:14,790 --> 00:28:24,210
went through the backlog so let's have a

00:28:19,770 --> 00:28:27,210
look at the invoices all even amount I

00:28:24,210 --> 00:28:30,120
think so that looks pretty good

00:28:27,210 --> 00:28:33,360
and we're also up to date so we have

00:28:30,120 --> 00:28:36,860
event from April and last invoice we

00:28:33,360 --> 00:28:40,679
have are from March so that looks good

00:28:36,860 --> 00:28:44,040
okay so Tasman is a failure it seems to

00:28:40,679 --> 00:28:45,809
work availability is is compromised

00:28:44,040 --> 00:28:48,890
though so as long as the job is not

00:28:45,809 --> 00:28:51,980
running the crew states also not

00:28:48,890 --> 00:28:54,179
accessible but keep in mind that

00:28:51,980 --> 00:28:56,040
normally this job would be restarted

00:28:54,179 --> 00:28:58,190
pretty much instantaneously as long as

00:28:56,040 --> 00:28:59,940
they are not enough task club and

00:28:58,190 --> 00:29:02,370
usually your cluster would be

00:28:59,940 --> 00:29:04,350
provisioned in a way that you can cope

00:29:02,370 --> 00:29:07,559
with a couple of task manager failures

00:29:04,350 --> 00:29:08,970
and the job can be restarted immediately

00:29:07,559 --> 00:29:15,420
but there is a small downtime

00:29:08,970 --> 00:29:20,250
that's not any case okay let's look at

00:29:15,420 --> 00:29:22,640
shop manager failure so um which job

00:29:20,250 --> 00:29:25,740
manager are we currently running on to

00:29:22,640 --> 00:29:31,160
job manager - okay yeah then we kill

00:29:25,740 --> 00:29:31,160
jump venture - and see what happens so

00:29:31,610 --> 00:29:37,380
again the jump managers is that but we

00:29:35,429 --> 00:29:39,360
will State still works because it

00:29:37,380 --> 00:29:44,790
doesn't communicate with the job manager

00:29:39,360 --> 00:29:46,650
once it's running so only when the time

00:29:44,790 --> 00:29:47,130
out of the leader election service

00:29:46,650 --> 00:29:50,100
basically

00:29:47,130 --> 00:29:51,600
kicks in and realizes okay there is no

00:29:50,100 --> 00:29:53,670
job manage anymore the task managers

00:29:51,600 --> 00:29:56,850
will be killed by the new job manager

00:29:53,670 --> 00:30:00,630
and then the cruellest state will not be

00:29:56,850 --> 00:30:04,600
accessible anymore so again the same

00:30:00,630 --> 00:30:12,150
like distributed system it's

00:30:04,600 --> 00:30:15,640
yeah at its best that's irony vanuit

00:30:12,150 --> 00:30:19,360
okay now we can have a look at your

00:30:15,640 --> 00:30:20,830
manager one I think yeah so new job

00:30:19,360 --> 00:30:30,480
managers here but it hasn't recovered

00:30:20,830 --> 00:30:33,550
the job yet it will eventually yeah I

00:30:30,480 --> 00:30:36,850
know the core was declined connected to

00:30:33,550 --> 00:30:39,460
the new job manager and basically run

00:30:36,850 --> 00:30:42,580
through the all the events from the last

00:30:39,460 --> 00:30:46,150
checkpoint we can also have a look at

00:30:42,580 --> 00:30:48,100
the invoices again there are any and

00:30:46,150 --> 00:30:49,660
even numbers there is one uneven number

00:30:48,100 --> 00:30:54,190
but there is another update afterwards

00:30:49,660 --> 00:31:00,910
which brings it up to 80 again same here

00:30:54,190 --> 00:31:05,520
for Emma yeah looks good so correctness

00:31:00,910 --> 00:31:05,520
also in case of job manager failures

00:31:06,540 --> 00:31:11,110
availability again is a little bit

00:31:08,530 --> 00:31:13,800
compromised during the switchover but

00:31:11,110 --> 00:31:16,660
that's also a very short amount of time

00:31:13,800 --> 00:31:18,550
okay last but not least failure of a

00:31:16,660 --> 00:31:23,200
downstream system so what does

00:31:18,550 --> 00:31:25,780
downstream system mean in our case we're

00:31:23,200 --> 00:31:28,180
writing to the file system so our dancin

00:31:25,780 --> 00:31:31,450
system is basically the file system if

00:31:28,180 --> 00:31:34,090
we were writing or if we would do

00:31:31,450 --> 00:31:39,190
idempotent calls through some REST API

00:31:34,090 --> 00:31:42,280
it would be the in this service the

00:31:39,190 --> 00:31:45,940
behavior is pretty much the same so what

00:31:42,280 --> 00:31:48,760
would we want to have to happen since in

00:31:45,940 --> 00:31:51,610
these architectures basically Kafka is

00:31:48,760 --> 00:31:54,150
always the fallback layer we would

00:31:51,610 --> 00:32:00,160
actually want the processing to stop and

00:31:54,150 --> 00:32:01,960
the events being yeah saved in Kafka

00:32:00,160 --> 00:32:03,700
persisted in Kafka and once the

00:32:01,960 --> 00:32:07,270
downstream system is available again we

00:32:03,700 --> 00:32:09,540
want to run through the backlog and send

00:32:07,270 --> 00:32:13,540
all the events to the downstream system

00:32:09,540 --> 00:32:16,800
so what we are doing here now is as I

00:32:13,540 --> 00:32:20,060
said our distributed file system is this

00:32:16,800 --> 00:32:22,800
this docker volume which is called

00:32:20,060 --> 00:32:24,300
invoices I think or the folder is called

00:32:22,800 --> 00:32:26,490
invoice and we'll just move it to

00:32:24,300 --> 00:32:30,660
invoice archive and see what happened

00:32:26,490 --> 00:32:33,570
and what happens so nothing happens

00:32:30,660 --> 00:32:37,380
because the month is not over yet so it

00:32:33,570 --> 00:32:39,410
doesn't try to write out that's why

00:32:37,380 --> 00:32:44,220
everything is going well right now and

00:32:39,410 --> 00:32:46,680
at some point it will fail we can

00:32:44,220 --> 00:32:52,650
already look what the invoice archive

00:32:46,680 --> 00:32:55,620
last month is now I try to write out and

00:32:52,650 --> 00:33:00,720
the job fails and is now basically stuck

00:32:55,620 --> 00:33:04,320
in this restarting restarting failing

00:33:00,720 --> 00:33:06,870
loop because now it when it starts up I

00:33:04,320 --> 00:33:08,400
think it checks whether the file system

00:33:06,870 --> 00:33:11,700
is available and it's not available in

00:33:08,400 --> 00:33:15,180
this case if it wouldn't do the check it

00:33:11,700 --> 00:33:17,040
would fail directly in any case because

00:33:15,180 --> 00:33:19,860
it then goes for all the event and calf

00:33:17,040 --> 00:33:22,380
guard is then stuck when writing out

00:33:19,860 --> 00:33:25,860
again so it's running again it's and so

00:33:22,380 --> 00:33:28,920
on and during the whole time horrible

00:33:25,860 --> 00:33:32,790
State client test is no way of

00:33:28,920 --> 00:33:33,920
retrieving the current current usage of

00:33:32,790 --> 00:33:37,800
the customer

00:33:33,920 --> 00:33:45,180
okay let's recreate the directory to to

00:33:37,800 --> 00:33:48,750
end it okay now it's successfully

00:33:45,180 --> 00:33:53,820
restarted and again go through the

00:33:48,750 --> 00:33:56,880
catalog yeah let's what was last month

00:33:53,820 --> 00:34:00,780
it was in November 84 so the last one

00:33:56,880 --> 00:34:04,080
month which we moved and now it starts

00:34:00,780 --> 00:34:08,820
with December and all even amounts so

00:34:04,080 --> 00:34:15,380
again no duplication and no I know lost

00:34:08,820 --> 00:34:18,900
messages so again it correctness is not

00:34:15,380 --> 00:34:21,780
all correct ensign in these cases the

00:34:18,900 --> 00:34:24,510
invoices are correct and complete but

00:34:21,780 --> 00:34:28,530
availability is pretty much compromised

00:34:24,510 --> 00:34:30,760
to some it's basically your couple your

00:34:28,530 --> 00:34:33,760
current state of your computation to you

00:34:30,760 --> 00:34:36,010
computation so you don't have any any

00:34:33,760 --> 00:34:38,830
fallback right now if you if you wish

00:34:36,010 --> 00:34:43,179
stopped summing up you don't have access

00:34:38,830 --> 00:34:45,250
to your current sum which is a lot

00:34:43,179 --> 00:34:46,810
different as if you were writing the sum

00:34:45,250 --> 00:34:48,580
to register for example and we're just

00:34:46,810 --> 00:34:50,110
updating there then at least it's

00:34:48,580 --> 00:34:56,740
filling cluster goes down you still have

00:34:50,110 --> 00:35:01,780
the last you last some in red

00:34:56,740 --> 00:35:05,160
okay so let's I already started a little

00:35:01,780 --> 00:35:08,740
bit let's look at some of the the

00:35:05,160 --> 00:35:11,680
limitations we have with kuru state

00:35:08,740 --> 00:35:16,540
right now so first of all some of you

00:35:11,680 --> 00:35:18,910
might have noticed the way we built the

00:35:16,540 --> 00:35:21,820
Kerrville state now or and the way we

00:35:18,910 --> 00:35:24,220
use the group will the state now is that

00:35:21,820 --> 00:35:27,670
we have one state perky but it's not

00:35:24,220 --> 00:35:29,530
scoped to the window so it's for someone

00:35:27,670 --> 00:35:33,250
for those of you who are more familiar

00:35:29,530 --> 00:35:35,800
with link it's basically only heat but

00:35:33,250 --> 00:35:41,170
the name space is always the void main

00:35:35,800 --> 00:35:46,240
main same space so there's no way to

00:35:41,170 --> 00:35:48,460
query the sum for two months at the same

00:35:46,240 --> 00:35:50,340
time for one customer it's always just

00:35:48,460 --> 00:35:54,730
the latest the month of the latest event

00:35:50,340 --> 00:35:56,920
but this will there we saw that that

00:35:54,730 --> 00:35:58,450
they change it with one three but there

00:35:56,920 --> 00:36:00,010
wasn't anything done with Kerrville

00:35:58,450 --> 00:36:02,500
state and one three so I think it was

00:36:00,010 --> 00:36:05,290
all postponed to one four but it's

00:36:02,500 --> 00:36:09,070
pretty small six so are pretty small

00:36:05,290 --> 00:36:11,650
additional feature see and I guess they

00:36:09,070 --> 00:36:13,330
will leave it in one for the client API

00:36:11,650 --> 00:36:15,760
max already said it it's pretty

00:36:13,330 --> 00:36:17,470
cumbersome right now so basically you

00:36:15,760 --> 00:36:19,090
need to link configuration you need to

00:36:17,470 --> 00:36:21,970
need to do all the flink type

00:36:19,090 --> 00:36:23,890
serialization stuff which is not not

00:36:21,970 --> 00:36:27,730
really convenient so you would want it

00:36:23,890 --> 00:36:29,260
to be basically aresko I think because

00:36:27,730 --> 00:36:31,780
then you can really use different

00:36:29,260 --> 00:36:33,760
clients and not just Java or Scala

00:36:31,780 --> 00:36:35,580
client where you can basically use these

00:36:33,760 --> 00:36:37,600
class

00:36:35,580 --> 00:36:42,490
state size of course is also a

00:36:37,600 --> 00:36:45,460
limitation so if your if your job

00:36:42,490 --> 00:36:48,700
I don't know who was in the Stefan's

00:36:45,460 --> 00:36:52,150
talk a last session but basically as

00:36:48,700 --> 00:36:53,800
long as as long as you have if you don't

00:36:52,150 --> 00:36:56,140
have the throughput requirements that

00:36:53,800 --> 00:36:57,970
you cannot use for CB then you have no

00:36:56,140 --> 00:36:59,800
limit here but there might be a

00:36:57,970 --> 00:37:02,950
performance penalty at some point for

00:36:59,800 --> 00:37:05,230
critical state but if you need to use

00:37:02,950 --> 00:37:08,680
the memory memories state back-end for

00:37:05,230 --> 00:37:11,050
performance reasons then your state size

00:37:08,680 --> 00:37:16,240
cannot be larger than your main memory

00:37:11,050 --> 00:37:18,700
and last I already touched upon it a bit

00:37:16,240 --> 00:37:22,780
availability in case of job failures is

00:37:18,700 --> 00:37:24,730
basically always a problem if your job

00:37:22,780 --> 00:37:26,980
doesn't come up immediately I mean can

00:37:24,730 --> 00:37:29,200
do caching and all other stuff so in a

00:37:26,980 --> 00:37:31,570
lot of case it should work but it's

00:37:29,200 --> 00:37:33,700
definitely you definitely couple your

00:37:31,570 --> 00:37:36,369
results to your ongoing computation

00:37:33,700 --> 00:37:39,820
which is something you should think

00:37:36,369 --> 00:37:41,680
about ok that's basically it you can

00:37:39,820 --> 00:37:44,470
check out the code in the slides on

00:37:41,680 --> 00:37:47,350
github play around with it yourself a

00:37:44,470 --> 00:37:50,050
little bit and yeah we're happy to take

00:37:47,350 --> 00:37:56,080
any questions yeah thank you very much

00:37:50,050 --> 00:38:01,840
much in confirming Maxis the man in the

00:37:56,080 --> 00:38:07,630
dark here at content in action ok so any

00:38:01,840 --> 00:38:09,190
questions yeah so could you maybe

00:38:07,630 --> 00:38:10,810
elaborate a little bit on the

00:38:09,190 --> 00:38:12,940
limitations you said with state size

00:38:10,810 --> 00:38:16,330
because I'm interested you said you had

00:38:12,940 --> 00:38:18,490
a your window was basically 30 days so I

00:38:16,330 --> 00:38:20,500
would imagine that your very large

00:38:18,490 --> 00:38:21,970
amount of data in the window and also

00:38:20,500 --> 00:38:24,400
did you have maybe some problems with

00:38:21,970 --> 00:38:25,930
the slide of the window because we ran

00:38:24,400 --> 00:38:29,950
into some problems with having a window

00:38:25,930 --> 00:38:31,119
of 24 hours but it had to be thought had

00:38:29,950 --> 00:38:33,310
to slide by one minute

00:38:31,119 --> 00:38:34,480
so a lot of copies of the same objects

00:38:33,310 --> 00:38:36,430
in the data and we had a lot of garbage

00:38:34,480 --> 00:38:37,990
collection issues whatever so I don't

00:38:36,430 --> 00:38:40,990
know maybe just some of your experiences

00:38:37,990 --> 00:38:43,030
with these types of issues so in this

00:38:40,990 --> 00:38:47,859
case there weren't any any issues

00:38:43,030 --> 00:38:50,350
because only nine keys and also it's

00:38:47,859 --> 00:38:53,050
even for for a lot of customers

00:38:50,350 --> 00:38:54,230
so think about 40 million or something

00:38:53,050 --> 00:38:56,060
like that

00:38:54,230 --> 00:38:58,940
it still shouldn't be too much data

00:38:56,060 --> 00:39:00,980
because there's always a fault operator

00:38:58,940 --> 00:39:04,460
as or in this case that it's just one

00:39:00,980 --> 00:39:06,290
number so it shouldn't be in the

00:39:04,460 --> 00:39:10,609
gigabyte let's say the state even for

00:39:06,290 --> 00:39:15,859
440 million yeah object overhead and so

00:39:10,609 --> 00:39:18,500
on but yeah we didn't we don't have any

00:39:15,859 --> 00:39:22,730
sliding windows here I think I saw the

00:39:18,500 --> 00:39:28,550
mailing list threat on on these here on

00:39:22,730 --> 00:39:31,520
the yeah many slide yeah it's definitely

00:39:28,550 --> 00:39:35,270
limitation but I don't have any solution

00:39:31,520 --> 00:39:36,890
on it I think it's yes it's basically

00:39:35,270 --> 00:39:38,869
how the windows work and think right now

00:39:36,890 --> 00:39:41,330
that if you assign it to multiple

00:39:38,869 --> 00:39:45,619
windows then it's always duplicates the

00:39:41,330 --> 00:39:46,880
state and that's especially problem if

00:39:45,619 --> 00:39:49,850
you don't have a fault or reduce

00:39:46,880 --> 00:39:57,280
functioning for pre aggregation of the

00:39:49,850 --> 00:39:57,280
window state there's I think talk from

00:39:57,340 --> 00:40:03,320
from from Stefan as well who's talking a

00:40:00,800 --> 00:40:05,800
little bit more about how will the state

00:40:03,320 --> 00:40:09,200
scales in terms of number of keys and

00:40:05,800 --> 00:40:11,240
state size so it's if you're interested

00:40:09,200 --> 00:40:15,710
or anyone is interesting especially into

00:40:11,240 --> 00:40:17,450
a horrible state scales then you should

00:40:15,710 --> 00:40:18,920
check out check out this this talk I

00:40:17,450 --> 00:40:22,070
don't know the name but you'll probably

00:40:18,920 --> 00:40:23,359
find it if if you look for the screen

00:40:22,070 --> 00:40:28,090
processor at the database

00:40:23,359 --> 00:40:28,090
Stefan Devon or something less okay

00:40:40,590 --> 00:40:46,320
so I may not be super familiar with

00:40:42,360 --> 00:40:49,980
blank but sports the query ability of

00:40:46,320 --> 00:40:53,760
this data set how flexible is it like in

00:40:49,980 --> 00:40:56,910
terms of aggregate aggregation or the

00:40:53,760 --> 00:40:58,710
sort of the time window terms are cactus

00:40:56,910 --> 00:41:02,310
appetizers it may be a similar question

00:40:58,710 --> 00:41:03,840
is what someone just asked but I'm

00:41:02,310 --> 00:41:07,320
assuming because it's a key/value kind

00:41:03,840 --> 00:41:08,970
of a store a lot of the querying

00:41:07,320 --> 00:41:10,500
essentially just give me everything for

00:41:08,970 --> 00:41:12,150
this key for this time period and then

00:41:10,500 --> 00:41:15,380
just do all the aggregation in code

00:41:12,150 --> 00:41:18,720
essentially is that it's a fair way of

00:41:15,380 --> 00:41:21,150
yeah yeah good question so basically at

00:41:18,720 --> 00:41:23,160
runtime or a query time it's not

00:41:21,150 --> 00:41:25,110
flexible at all so the way it's

00:41:23,160 --> 00:41:27,660
implemented right now you the whole

00:41:25,110 --> 00:41:31,140
aggregation and the time periods has to

00:41:27,660 --> 00:41:33,060
be um has to be hard-coded or made

00:41:31,140 --> 00:41:36,270
configure all or whatever but you you

00:41:33,060 --> 00:41:39,030
can't decide at at very time now I want

00:41:36,270 --> 00:41:41,490
to do this kind of aggregation um you

00:41:39,030 --> 00:41:45,680
could of course just fetch the raw data

00:41:41,490 --> 00:41:45,680
and then aggregate in some fashion but

00:41:46,310 --> 00:41:54,240
but the objects basically can be any any

00:41:49,980 --> 00:41:56,280
Java object which is state so it's just

00:41:54,240 --> 00:42:00,090
need to be able to to serialize and

00:41:56,280 --> 00:42:02,010
deserialize and the same serialization

00:42:00,090 --> 00:42:07,710
deserialization penalties apply which

00:42:02,010 --> 00:42:10,920
apply to all the state animation okay

00:42:07,710 --> 00:42:12,660
then thank you very much focus talk

00:42:10,920 --> 00:42:18,760
thank you thank you

00:42:12,660 --> 00:42:18,760

YouTube URL: https://www.youtube.com/watch?v=QPnJfumBOl4


