Title: Berlin Buzzwords 2017: Hagen Tönnies - Containerizing Distributed Pipes #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	In this talk I will present how we enable distributed, Unix style programming using Docker and Apache Kafka. We will show how we can take the famous Unix Pipe Pattern and apply it to a Distributed Computing System. We will demonstrate the development of simple applications with the focus on "Do One Thing and Do It Well."

Afterwards we demonstrate how we make these two programs work to together using Apache Kafka. By encapsulating our applications in containers we will also show how that enables us to go from the limited resources of a development machine to cluster of computers in a data center without changing our applications or containers.

Read more:
https://2017.berlinbuzzwords.de/17/session/containerizing-distributed-pipes

About Hagen Tönnies:
https://2017.berlinbuzzwords.de/users/hagen-tonnies

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,520 --> 00:00:12,450
yeah thanks so much yeah container is

00:00:09,600 --> 00:00:16,500
beautifies I'm hiring you can find me on

00:00:12,450 --> 00:00:20,070
LinkedIn I work for Sony we put

00:00:16,500 --> 00:00:21,810
PlayStation chips in racks and then we

00:00:20,070 --> 00:00:23,460
give you a streaming service where you

00:00:21,810 --> 00:00:26,180
can basically subscribe to PlayStation

00:00:23,460 --> 00:00:28,680
games and stream it to your device and

00:00:26,180 --> 00:00:30,990
so we have quite a fleet of computers

00:00:28,680 --> 00:00:32,879
and what I'm showing here though is just

00:00:30,990 --> 00:00:35,040
a proof of concept that's not actually

00:00:32,879 --> 00:00:37,800
related to what we do in production but

00:00:35,040 --> 00:00:40,129
it's kind of a research thingy that is

00:00:37,800 --> 00:00:42,510
ongoing see if we can make use of

00:00:40,129 --> 00:00:44,070
container writing distributed pipes and

00:00:42,510 --> 00:00:45,989
I have to give special thanks to

00:00:44,070 --> 00:00:49,199
Christian key that's the Twitter handle

00:00:45,989 --> 00:00:50,640
there he supports me a lot with the

00:00:49,199 --> 00:00:52,949
container writing part of everything

00:00:50,640 --> 00:00:56,430
basically but I'm just feeling his work

00:00:52,949 --> 00:00:59,370
and just use it so I give a bit of

00:00:56,430 --> 00:01:01,739
background then we talk about a tool

00:00:59,370 --> 00:01:02,940
chain which should run distributed and

00:01:01,739 --> 00:01:05,390
then we talked a little bit about

00:01:02,940 --> 00:01:09,210
container izing and then we have a recap

00:01:05,390 --> 00:01:11,340
so the background buzzwords first of

00:01:09,210 --> 00:01:13,470
course there was this talk from our team

00:01:11,340 --> 00:01:15,689
clapman way said the UNIX philosophy of

00:01:13,470 --> 00:01:17,159
distributed data and I got a bit of

00:01:15,689 --> 00:01:20,430
hooked and I thought oh that's kind of a

00:01:17,159 --> 00:01:23,039
cool idea can I actually use it and by

00:01:20,430 --> 00:01:25,350
that time he was doing Samsa and I was

00:01:23,039 --> 00:01:28,170
like are now sonzai runs on yarn and I

00:01:25,350 --> 00:01:32,670
needed to do but everything is ok not

00:01:28,170 --> 00:01:35,009
now and and then there were Kafka and

00:01:32,670 --> 00:01:36,689
Kepler came up with this streams API and

00:01:35,009 --> 00:01:38,939
I thought now now I can try it now it's

00:01:36,689 --> 00:01:40,829
every everything is simpler apparently

00:01:38,939 --> 00:01:44,219
so

00:01:40,829 --> 00:01:47,159
I thought okay what's the UNIX pipe so

00:01:44,219 --> 00:01:49,409
UNIX pipe is a process chain where the

00:01:47,159 --> 00:01:51,719
output of one process is the input of

00:01:49,409 --> 00:01:55,320
the next and I guess we all know it but

00:01:51,719 --> 00:01:58,679
I guess we also lost it a bit in the

00:01:55,320 --> 00:02:00,689
industry actually because programs that

00:01:58,679 --> 00:02:02,369
do one thing and do one thing well who

00:02:00,689 --> 00:02:04,619
has written a program lately that does

00:02:02,369 --> 00:02:12,000
this

00:02:04,619 --> 00:02:13,769
really good work together and I know

00:02:12,000 --> 00:02:16,290
working together kind of like yeah yeah

00:02:13,769 --> 00:02:19,290
we have an API we go this but using an

00:02:16,290 --> 00:02:23,549
API always couples our code to that API

00:02:19,290 --> 00:02:26,459
right so the other means basically by

00:02:23,549 --> 00:02:29,430
LNK we're working together in something

00:02:26,459 --> 00:02:31,470
a bit different and I think we need to

00:02:29,430 --> 00:02:34,980
go back and be simpler with everything

00:02:31,470 --> 00:02:36,750
to make that actually happen and then if

00:02:34,980 --> 00:02:40,110
here's something controversial a guy

00:02:36,750 --> 00:02:41,579
from Copenhagen BSD developers said use

00:02:40,110 --> 00:02:43,140
text streams because that's the

00:02:41,579 --> 00:02:44,840
universal interface and there's a lot of

00:02:43,140 --> 00:02:47,670
stuff on the internet that says app

00:02:44,840 --> 00:02:50,640
string oriented programming not my

00:02:47,670 --> 00:02:55,170
saying use types and then you're fine

00:02:50,640 --> 00:02:57,690
well I don't know the guy from the 70s

00:02:55,170 --> 00:02:59,670
has a lot of wisdom for me so I just put

00:02:57,690 --> 00:03:03,239
it out there because I think it's it's

00:02:59,670 --> 00:03:06,480
controversial and nice ok short recap on

00:03:03,239 --> 00:03:07,829
Kafka it's this thing that you put in

00:03:06,480 --> 00:03:11,310
your infrastructure and in your good

00:03:07,829 --> 00:03:12,690
because you can now shift around you can

00:03:11,310 --> 00:03:16,650
shift around tons of data and you don't

00:03:12,690 --> 00:03:19,290
worry just runs perfectly there are some

00:03:16,650 --> 00:03:22,470
some some problems with that I guess but

00:03:19,290 --> 00:03:23,760
everybody has to figure out my operation

00:03:22,470 --> 00:03:25,530
teams always said we don't have any

00:03:23,760 --> 00:03:27,780
problems with Kafka and I'm happy about

00:03:25,530 --> 00:03:30,359
that and I just believe them and

00:03:27,780 --> 00:03:33,139
actually we have no production outage of

00:03:30,359 --> 00:03:35,310
Kafka it's just just seems to be good

00:03:33,139 --> 00:03:36,989
yeah but you can you can travel around

00:03:35,310 --> 00:03:39,419
data in your organization and this is a

00:03:36,989 --> 00:03:41,700
data hub basically and it's also an

00:03:39,419 --> 00:03:43,500
append lock it looks something like that

00:03:41,700 --> 00:03:45,389
you have a zookeeper ensemble and you

00:03:43,500 --> 00:03:48,209
have a bunch of program that's like your

00:03:45,389 --> 00:03:50,639
cluster then you have the notion of a

00:03:48,209 --> 00:03:53,340
topic that's just a queue where you have

00:03:50,639 --> 00:03:55,349
your messages in then cascade has also

00:03:53,340 --> 00:03:58,560
this notion of partitioning where the

00:03:55,349 --> 00:04:00,510
topics get partitioned and that within a

00:03:58,560 --> 00:04:03,239
petition therefore strictly ordering

00:04:00,510 --> 00:04:05,190
some not too toxic itself is a straight

00:04:03,239 --> 00:04:08,160
ordering but the partition themselves

00:04:05,190 --> 00:04:10,049
they have so here is this what kind of

00:04:08,160 --> 00:04:11,400
petition key are you using and what does

00:04:10,049 --> 00:04:13,230
it mean for your data processing

00:04:11,400 --> 00:04:16,919
pipeline but

00:04:13,230 --> 00:04:18,510
so calcio will do also replications of

00:04:16,919 --> 00:04:21,060
your petition meaning I have one

00:04:18,510 --> 00:04:23,970
petition then you can say make it three

00:04:21,060 --> 00:04:27,090
times and then shift it around in the

00:04:23,970 --> 00:04:29,190
cluster does that for you very nicely

00:04:27,090 --> 00:04:30,750
actually and then it would look like

00:04:29,190 --> 00:04:32,880
this here if your broker and then you

00:04:30,750 --> 00:04:34,650
have your petitions and the copies of

00:04:32,880 --> 00:04:38,430
your petition so there's also a notion

00:04:34,650 --> 00:04:41,100
of data locality then of course you can

00:04:38,430 --> 00:04:43,169
produce messages to the topic you can

00:04:41,100 --> 00:04:45,440
consume messages from the topic and then

00:04:43,169 --> 00:04:48,330
here's the nice thing you can actually

00:04:45,440 --> 00:04:50,940
build groups of consumers where you can

00:04:48,330 --> 00:04:54,150
then read from a topic and then say this

00:04:50,940 --> 00:04:57,900
one group a that's your bi team reading

00:04:54,150 --> 00:04:59,970
your events if you want and then group

00:04:57,900 --> 00:05:02,430
two is your every team also reading the

00:04:59,970 --> 00:05:06,120
events but very differently but they can

00:05:02,430 --> 00:05:07,620
do this now without running into trouble

00:05:06,120 --> 00:05:10,650
or I have any synchronization between

00:05:07,620 --> 00:05:15,419
them they just can do this independently

00:05:10,650 --> 00:05:17,970
of each other here the streaming API

00:05:15,419 --> 00:05:19,919
just like you should go read the docs

00:05:17,970 --> 00:05:23,789
that's highly simplification and highly

00:05:19,919 --> 00:05:26,340
simplified here but whoever has used

00:05:23,789 --> 00:05:29,640
storm in the beginning or other

00:05:26,340 --> 00:05:31,530
processing libraries knows we have this

00:05:29,640 --> 00:05:33,630
notion of a topology how should our data

00:05:31,530 --> 00:05:35,460
processing pipeline look like and you

00:05:33,630 --> 00:05:36,960
have it in a low-level API for Kafka

00:05:35,460 --> 00:05:38,400
streams then you can build custom

00:05:36,960 --> 00:05:42,300
aggregators you can build custom

00:05:38,400 --> 00:05:44,729
processes which I think it actually

00:05:42,300 --> 00:05:47,520
looks very similar to what storm offers

00:05:44,729 --> 00:05:49,979
you from API wise but then you have also

00:05:47,520 --> 00:05:52,530
high level API and there you have a

00:05:49,979 --> 00:05:54,270
notion of streams and tables and you can

00:05:52,530 --> 00:05:58,919
interchange them because if you have a

00:05:54,270 --> 00:06:01,620
stream of changes then that actually is

00:05:58,919 --> 00:06:05,669
always a view on a certain table at a

00:06:01,620 --> 00:06:08,850
given time the high level API also comes

00:06:05,669 --> 00:06:11,160
with something flat map and map and

00:06:08,850 --> 00:06:13,440
reduce I guess and you can have joins

00:06:11,160 --> 00:06:15,960
meaning if you convert a stream to a

00:06:13,440 --> 00:06:18,090
table and another stream also to a table

00:06:15,960 --> 00:06:20,820
and you can go join them and then

00:06:18,090 --> 00:06:23,520
means a different stream or always look

00:06:20,820 --> 00:06:28,290
at pairs of data from different Calcutta

00:06:23,520 --> 00:06:31,590
topics very nice so the stream table

00:06:28,290 --> 00:06:32,970
thingy looks a bit like okay this is my

00:06:31,590 --> 00:06:34,800
first change then the table looks like

00:06:32,970 --> 00:06:37,320
this is my second change my table looks

00:06:34,800 --> 00:06:42,510
like this third change tail looks like

00:06:37,320 --> 00:06:45,030
this so you can interchange them or you

00:06:42,510 --> 00:06:49,080
have to change them streamflow setting

00:06:45,030 --> 00:06:51,960
recap this is actually I guess from from

00:06:49,080 --> 00:06:53,340
at equipment store or normal from some

00:06:51,960 --> 00:06:55,979
talk from constant I all country

00:06:53,340 --> 00:06:58,350
meddling it so in the beginning there

00:06:55,979 --> 00:07:01,760
was a patchy storm and I used it quite a

00:06:58,350 --> 00:07:03,930
bit we actually use it at Sony I guess

00:07:01,760 --> 00:07:06,810
then we have spark and fleeing

00:07:03,930 --> 00:07:09,870
I know there are up to 10 stream

00:07:06,810 --> 00:07:11,039
processors now actually but these are

00:07:09,870 --> 00:07:15,300
the most important one they are quite

00:07:11,039 --> 00:07:17,190
capable meaning the api's are vast broad

00:07:15,300 --> 00:07:19,830
powerful I don't know if it's actually

00:07:17,190 --> 00:07:22,590
powerful as they are that big but you

00:07:19,830 --> 00:07:25,680
can do a lot about this and then

00:07:22,590 --> 00:07:29,120
simplicity I don't know who had somebody

00:07:25,680 --> 00:07:33,990
debug a storm topology in production

00:07:29,120 --> 00:07:37,530
nice experience so yeah it's it's not

00:07:33,990 --> 00:07:38,880
that easy for spark we had a talk today

00:07:37,530 --> 00:07:42,630
and it's kind of like you need to know

00:07:38,880 --> 00:07:43,950
the not source link I have never been in

00:07:42,630 --> 00:07:45,810
touch with link actually I don't

00:07:43,950 --> 00:07:48,330
I hope they learned a lot from the

00:07:45,810 --> 00:07:52,110
former attempts and so everything is not

00:07:48,330 --> 00:07:53,910
simple and easy and then on that on

00:07:52,110 --> 00:07:55,979
these two dimensions character would go

00:07:53,910 --> 00:07:57,599
and be very very simply because if you

00:07:55,979 --> 00:07:59,789
have a running craft anyways what you do

00:07:57,599 --> 00:08:02,900
is basically write your stream processor

00:07:59,789 --> 00:08:06,150
put it in a jar and run the jar and done

00:08:02,900 --> 00:08:10,169
you don't need like 10 s eries running

00:08:06,150 --> 00:08:15,780
your Hadoop infrastructure so yeah you

00:08:10,169 --> 00:08:19,289
don't so coming back to distributed

00:08:15,780 --> 00:08:22,139
piping so one can think of a pipe as a

00:08:19,289 --> 00:08:24,780
casket topic or petition depending on

00:08:22,139 --> 00:08:28,020
how you layout your data and then the

00:08:24,780 --> 00:08:29,159
function which processes the output of

00:08:28,020 --> 00:08:32,550
one pipe

00:08:29,159 --> 00:08:34,979
so in this case the pipe would be the

00:08:32,550 --> 00:08:36,810
message broker Kafka and a string

00:08:34,979 --> 00:08:39,630
processing job would be of course a

00:08:36,810 --> 00:08:40,919
closure applications because nobody is

00:08:39,630 --> 00:08:44,670
like why would you use any other

00:08:40,919 --> 00:08:47,430
language and then you can have like this

00:08:44,670 --> 00:08:50,250
would be like okay we we know this this

00:08:47,430 --> 00:08:53,370
is Linux UNIX command-line stuff right

00:08:50,250 --> 00:08:58,440
you do you do kind of this with cat and

00:08:53,370 --> 00:08:59,790
a WK and less and whatever so this is

00:08:58,440 --> 00:09:04,560
the goal can we build something that

00:08:59,790 --> 00:09:08,430
looks like this and T is very similar so

00:09:04,560 --> 00:09:10,160
to do that I built some tools and I call

00:09:08,430 --> 00:09:12,360
them distributed because I can basically

00:09:10,160 --> 00:09:17,240
have this one implementation and then

00:09:12,360 --> 00:09:19,980
start multiple instances of my tool and

00:09:17,240 --> 00:09:23,579
yep there will be Lisp lots of

00:09:19,980 --> 00:09:25,439
parentheses but the good thing is after

00:09:23,579 --> 00:09:28,139
half a year of doing that the

00:09:25,439 --> 00:09:30,180
parentheses are no longer exist they

00:09:28,139 --> 00:09:34,079
just vanish and then everything is very

00:09:30,180 --> 00:09:36,149
clear so I built this cutter he takes a

00:09:34,079 --> 00:09:40,110
string and he comes up with the list of

00:09:36,149 --> 00:09:42,529
tokens and with some error handling and

00:09:40,110 --> 00:09:45,899
some blah blah blah it's basically just

00:09:42,529 --> 00:09:49,620
one function right you split anything at

00:09:45,899 --> 00:09:53,040
the space and then for the string mapper

00:09:49,620 --> 00:09:55,800
or the string builder it's called it's

00:09:53,040 --> 00:09:58,410
the does that work if I wanted here so

00:09:55,800 --> 00:10:04,079
you have the stream builder and then

00:09:58,410 --> 00:10:06,060
what wrong button you need to do the

00:10:04,079 --> 00:10:08,699
serialization and configure it and then

00:10:06,060 --> 00:10:10,949
you say ah this is my input topic then

00:10:08,699 --> 00:10:14,040
you can do some processing I mentioned a

00:10:10,949 --> 00:10:16,560
sled math map and then at the end you

00:10:14,040 --> 00:10:18,240
put it out and you can change the

00:10:16,560 --> 00:10:22,860
realization here if you want but because

00:10:18,240 --> 00:10:25,680
everything is string in Denmark we can

00:10:22,860 --> 00:10:27,060
this is also a just string and then you

00:10:25,680 --> 00:10:30,389
start your stream and that's about it

00:10:27,060 --> 00:10:32,160
and I think it's in Java it would be

00:10:30,389 --> 00:10:33,810
look a little bit different I guess in

00:10:32,160 --> 00:10:35,530
scarlet would be way tighter and way

00:10:33,810 --> 00:10:40,390
better and faster of course

00:10:35,530 --> 00:10:45,280
I prefer this then I've got a heavy

00:10:40,390 --> 00:10:47,880
hitter which is he takes in this list of

00:10:45,280 --> 00:10:50,740
tokens and he estimates the frequencies

00:10:47,880 --> 00:10:53,350
lips what have you hit at us and it uses

00:10:50,740 --> 00:10:55,660
a count min sketch data structure where

00:10:53,350 --> 00:10:57,490
you where you have your item and you

00:10:55,660 --> 00:10:59,500
have a bunch of hash functions and the

00:10:57,490 --> 00:11:02,050
table size basically and then you go and

00:10:59,500 --> 00:11:04,330
hash that item through all hash

00:11:02,050 --> 00:11:06,520
functions and place them at the given

00:11:04,330 --> 00:11:08,830
position in your table and when you

00:11:06,520 --> 00:11:10,720
retrieve them you get all your values

00:11:08,830 --> 00:11:13,690
back and then choose to the minimum and

00:11:10,720 --> 00:11:15,250
magically there's some F behind it it's

00:11:13,690 --> 00:11:19,690
actually a good estimate of the

00:11:15,250 --> 00:11:21,580
frequency of that token and then for

00:11:19,690 --> 00:11:25,180
having a heavy hitter you just go and

00:11:21,580 --> 00:11:29,620
take Tom in every X window time if you

00:11:25,180 --> 00:11:34,480
want and then you have at 20 p.m. that

00:11:29,620 --> 00:11:36,630
was my 5 heavy hitters and that looks

00:11:34,480 --> 00:11:41,170
something like this so he I'm using the

00:11:36,630 --> 00:11:43,630
low-level API because I need to add a

00:11:41,170 --> 00:11:45,490
processor and I cheated a bit because

00:11:43,630 --> 00:11:51,850
there's this tiny function where I said

00:11:45,490 --> 00:11:53,830
get processor just does that and here's

00:11:51,850 --> 00:11:57,070
something very cool which is also in Sam

00:11:53,830 --> 00:11:59,470
so you can have a state store so every

00:11:57,070 --> 00:12:04,030
time your processor is doing an handling

00:11:59,470 --> 00:12:06,190
state in this case my data structure for

00:12:04,030 --> 00:12:08,110
the heavy hitter it goes yeah yeah yeah

00:12:06,190 --> 00:12:11,620
taking and taking and taking and now

00:12:08,110 --> 00:12:14,260
zinc and then it spits out that state on

00:12:11,620 --> 00:12:16,690
to another topic in Kafka and my other

00:12:14,260 --> 00:12:19,600
processors will actually share that

00:12:16,690 --> 00:12:21,490
state and so it's the bad new to shared

00:12:19,600 --> 00:12:23,170
state but I guess if you do it in Kafka

00:12:21,490 --> 00:12:24,940
it's completely fine and you don't need

00:12:23,170 --> 00:12:29,920
to worry about it

00:12:24,940 --> 00:12:34,630
and then yeah you just I guess it's on

00:12:29,920 --> 00:12:36,940
the next slide here's the processor so

00:12:34,630 --> 00:12:38,440
you have this init function it's it's

00:12:36,940 --> 00:12:40,450
very Java raised right you have to

00:12:38,440 --> 00:12:42,310
implement something here the processor

00:12:40,450 --> 00:12:44,440
of course you have to do that and then

00:12:42,310 --> 00:12:47,660
if you're in a function that's this one

00:12:44,440 --> 00:12:49,819
here and then I set up my

00:12:47,660 --> 00:12:52,699
have you hit a data structure and my min

00:12:49,819 --> 00:12:54,949
sketch thingy then I process every value

00:12:52,699 --> 00:12:57,620
that's coming through actually key value

00:12:54,949 --> 00:13:00,019
pair sketch put to heavy hitter and on

00:12:57,620 --> 00:13:02,810
punctuate I go and flush the data

00:13:00,019 --> 00:13:06,560
structure on to the stream so back into

00:13:02,810 --> 00:13:11,149
another topic and if the system closes

00:13:06,560 --> 00:13:15,560
down I can gracefully shut down I forgot

00:13:11,149 --> 00:13:17,509
I made this then at the end I need to

00:13:15,560 --> 00:13:19,459
aggregate my results right so because I

00:13:17,509 --> 00:13:23,269
have a distant I come back and have

00:13:19,459 --> 00:13:25,750
these alvey's token eight times

00:13:23,269 --> 00:13:28,300
estimated and 10 time estimated and

00:13:25,750 --> 00:13:33,620
that's not going to work so I need to

00:13:28,300 --> 00:13:37,699
actually group by key and then count so

00:13:33,620 --> 00:13:39,620
I need this refi educator I need to do

00:13:37,699 --> 00:13:42,170
my custom aggregator here because

00:13:39,620 --> 00:13:44,810
obviously you know cannot just simply

00:13:42,170 --> 00:13:48,139
add I don't know if there are actually

00:13:44,810 --> 00:13:52,610
high-level api's that don't require you

00:13:48,139 --> 00:13:55,040
to implement an ad function but it's the

00:13:52,610 --> 00:13:56,689
same thing you build your stream your

00:13:55,040 --> 00:13:59,089
aggregate by key whatever you would like

00:13:56,689 --> 00:14:02,000
to do you need an initializer engine

00:13:59,089 --> 00:14:04,250
aggregator here for that serialization

00:14:02,000 --> 00:14:06,439
again then you put it into a stream then

00:14:04,250 --> 00:14:09,230
you map it back again I want to create a

00:14:06,439 --> 00:14:11,300
jason here and then i put it up again so

00:14:09,230 --> 00:14:14,600
it looks very similar like the first one

00:14:11,300 --> 00:14:18,350
that's there's not much to it actually

00:14:14,600 --> 00:14:20,990
it's just the api and at the end I think

00:14:18,350 --> 00:14:23,750
everything to elasticsearch so I have

00:14:20,990 --> 00:14:26,750
those estimated aggregated values ten

00:14:23,750 --> 00:14:30,430
plus eight and then I come up with this

00:14:26,750 --> 00:14:33,500
JSON structure and then to make more

00:14:30,430 --> 00:14:36,380
propaganda about closure we steal this

00:14:33,500 --> 00:14:38,660
goal thingy from the paper set the

00:14:36,380 --> 00:14:40,819
author but I guess he just was envy of

00:14:38,660 --> 00:14:43,130
golang so I guess it's that's the real

00:14:40,819 --> 00:14:46,730
reason we have go loops which is awesome

00:14:43,130 --> 00:14:50,899
you just consume from a Kafka topic get

00:14:46,730 --> 00:14:56,480
your Jason's and with an es connection

00:14:50,899 --> 00:14:59,440
and then just index it that's trivial so

00:14:56,480 --> 00:15:01,810
it's almost keep it stupid simple

00:14:59,440 --> 00:15:03,370
it's not because it's double gvn stuff

00:15:01,810 --> 00:15:07,060
and there are billion lines of code

00:15:03,370 --> 00:15:11,379
underneath you but from the top it looks

00:15:07,060 --> 00:15:13,120
simple I think so recap we have the

00:15:11,379 --> 00:15:14,680
string we have the list of tokens we

00:15:13,120 --> 00:15:16,089
have the estimated values and then we

00:15:14,680 --> 00:15:18,870
have the aggregated values and then we

00:15:16,089 --> 00:15:22,990
end up with a JSON and elasticsearch

00:15:18,870 --> 00:15:25,689
okay so that was a tooling the pipe now

00:15:22,990 --> 00:15:27,129
how do we put this in containers and we

00:15:25,689 --> 00:15:29,379
had to fiddle around with that a lots of

00:15:27,129 --> 00:15:31,990
meaning we Christian and me and because

00:15:29,379 --> 00:15:33,939
we had two attempts basically and when I

00:15:31,990 --> 00:15:36,759
get to know Christian he was always

00:15:33,939 --> 00:15:39,550
using console for service discovery and

00:15:36,759 --> 00:15:42,449
I was gonna so complicated why do we

00:15:39,550 --> 00:15:46,209
need another system to do that I don't

00:15:42,449 --> 00:15:46,540
yeah but so the first over we will get

00:15:46,209 --> 00:15:48,879
to that

00:15:46,540 --> 00:15:51,310
so we basically tried two things how to

00:15:48,879 --> 00:15:55,300
containerize and we come to that in a

00:15:51,310 --> 00:15:58,300
minute but first of all recap operating

00:15:55,300 --> 00:16:00,430
system-level virtualization multiple

00:15:58,300 --> 00:16:02,800
isolated user spaces I know there's some

00:16:00,430 --> 00:16:06,220
controversy about what actually means

00:16:02,800 --> 00:16:08,680
isolated user space because if I am root

00:16:06,220 --> 00:16:09,790
in a container will be rude on the host

00:16:08,680 --> 00:16:11,800
and that's evil

00:16:09,790 --> 00:16:13,389
anyway it's evil to be rude it has

00:16:11,800 --> 00:16:15,759
nothing to do with a container or docker

00:16:13,389 --> 00:16:20,259
it's shouldn't be rude you shouldn't run

00:16:15,759 --> 00:16:21,430
it through so in the old school days

00:16:20,259 --> 00:16:23,170
because yesterday I had a nice

00:16:21,430 --> 00:16:25,329
conversation I was basically yeah we

00:16:23,170 --> 00:16:28,420
have this container and the client wants

00:16:25,329 --> 00:16:30,430
us to put Windows XP in the container so

00:16:28,420 --> 00:16:32,829
they contain has some couple of tens of

00:16:30,430 --> 00:16:36,430
gigabytes big and it kind of something

00:16:32,829 --> 00:16:38,709
is gone wrong here but nobody notices so

00:16:36,430 --> 00:16:40,180
the old school was basically yes that's

00:16:38,709 --> 00:16:42,339
virtualization you have your server you

00:16:40,180 --> 00:16:45,130
have your host corner then you have the

00:16:42,339 --> 00:16:47,319
host user land where you do all your

00:16:45,130 --> 00:16:49,389
magic so that meant stuff and then you

00:16:47,319 --> 00:16:51,850
use the hypervisor and maybe you've got

00:16:49,389 --> 00:16:55,839
an internship and then your hypervisors

00:16:51,850 --> 00:16:58,240
also have a very efficient and then you

00:16:55,839 --> 00:17:00,309
get the host corners and the user land

00:16:58,240 --> 00:17:03,790
and the service on top so this is VM

00:17:00,309 --> 00:17:05,980
virtualization and you know with docker

00:17:03,790 --> 00:17:08,290
or containers we don't need it anymore

00:17:05,980 --> 00:17:09,910
was it looks like this we have a host

00:17:08,290 --> 00:17:11,709
colonel and we have the server and then

00:17:09,910 --> 00:17:13,630
we still have the userland of the host

00:17:11,709 --> 00:17:16,780
and maybe we have a service they are

00:17:13,630 --> 00:17:18,760
like log collection or metric collection

00:17:16,780 --> 00:17:20,709
or something but then we can have all

00:17:18,760 --> 00:17:23,829
our containers with their own user land

00:17:20,709 --> 00:17:25,990
and air service implementation and yet

00:17:23,829 --> 00:17:27,669
so it's simpler and we all love

00:17:25,990 --> 00:17:31,080
simplicity and you get rid of the whole

00:17:27,669 --> 00:17:31,080
bunch of lines of code right because

00:17:31,230 --> 00:17:35,110
there's a user let missing and a

00:17:33,280 --> 00:17:37,650
hypervisor missing and a second instance

00:17:35,110 --> 00:17:42,190
of the host color is also missing so I

00:17:37,650 --> 00:17:45,100
think it's worth it now

00:17:42,190 --> 00:17:47,530
the execution the execution of me as a

00:17:45,100 --> 00:17:47,950
Java developer for some couple of years

00:17:47,530 --> 00:17:52,299
now

00:17:47,950 --> 00:17:55,900
Java - jar and then talk I run I don't

00:17:52,299 --> 00:17:59,530
even need this ie I guess I want to put

00:17:55,900 --> 00:18:01,299
a minus RM minus minus RM but it looks

00:17:59,530 --> 00:18:04,150
very similar for mutant I execute

00:18:01,299 --> 00:18:06,220
something and with docker I can just do

00:18:04,150 --> 00:18:09,250
that and with a composed syntax which is

00:18:06,220 --> 00:18:12,760
this yellow specification it still looks

00:18:09,250 --> 00:18:16,600
something similar to just the command

00:18:12,760 --> 00:18:20,890
line so I think it's actually good

00:18:16,600 --> 00:18:24,520
looking yeah so for the development

00:18:20,890 --> 00:18:27,970
setup our first attempt how do i as a

00:18:24,520 --> 00:18:30,640
developer work with distributed pipes

00:18:27,970 --> 00:18:32,470
and their applications so everything

00:18:30,640 --> 00:18:35,799
needs to run on my one dhaka team and i

00:18:32,470 --> 00:18:40,150
have on my laptop and then here's those

00:18:35,799 --> 00:18:42,880
consoles yeah how to how to tell Kafka

00:18:40,150 --> 00:18:46,090
about zookeeper and how to tell the

00:18:42,880 --> 00:18:48,160
streaming application about my kaskell

00:18:46,090 --> 00:18:51,940
zookeeper and elasticsearch without

00:18:48,160 --> 00:18:53,500
having a domain name really or an IP

00:18:51,940 --> 00:18:55,540
address of course I can configure the

00:18:53,500 --> 00:18:57,940
network so that it binds to my host but

00:18:55,540 --> 00:19:01,600
then what happens if I add more docker

00:18:57,940 --> 00:19:03,309
engines to it okay consul to the rescue

00:19:01,600 --> 00:19:05,110
there's a key value store you start up

00:19:03,309 --> 00:19:07,570
you register yourself everybody else

00:19:05,110 --> 00:19:09,940
asked yourself you put in templates in

00:19:07,570 --> 00:19:11,770
your container then use console template

00:19:09,940 --> 00:19:13,960
language to render your configuration

00:19:11,770 --> 00:19:16,900
and there you have it and you block as

00:19:13,960 --> 00:19:19,120
long as console is not absolutely will

00:19:16,900 --> 00:19:20,500
council first and it was a start

00:19:19,120 --> 00:19:22,690
zookeeper then you will start the

00:19:20,500 --> 00:19:25,690
brokers and they come come up as a

00:19:22,690 --> 00:19:27,880
cluster and you go and add elasticsearch

00:19:25,690 --> 00:19:29,260
when you have more or less exertional

00:19:27,880 --> 00:19:32,440
they were joined a cluster and then

00:19:29,260 --> 00:19:35,500
afterwards you spawn out your individual

00:19:32,440 --> 00:19:42,640
applications yeah of course you need to

00:19:35,500 --> 00:19:43,270
set this chakra home spinning nothing

00:19:42,640 --> 00:19:46,270
unusual

00:19:43,270 --> 00:19:50,410
I guess composed some sects is quite

00:19:46,270 --> 00:19:52,360
clear you want to expose some ports with

00:19:50,410 --> 00:19:54,430
compost too and I get it still working

00:19:52,360 --> 00:19:56,050
with Campos three specification you can

00:19:54,430 --> 00:19:58,480
actually extend the base thing so in

00:19:56,050 --> 00:20:02,800
that base yema there's a network

00:19:58,480 --> 00:20:05,190
configuration it's not shown here again

00:20:02,800 --> 00:20:07,390
you load up your systems

00:20:05,190 --> 00:20:09,340
yeah and that's actually nice you load

00:20:07,390 --> 00:20:11,320
up your system but still it's only

00:20:09,340 --> 00:20:14,170
processes right you don't there's not a

00:20:11,320 --> 00:20:21,220
Linux running it's just the process and

00:20:14,170 --> 00:20:23,470
your host is providing the kernel okay

00:20:21,220 --> 00:20:26,410
so that looks like that it was looking

00:20:23,470 --> 00:20:29,350
on my laptop then you add the

00:20:26,410 --> 00:20:32,560
applications we had a couple of minutes

00:20:29,350 --> 00:20:35,140
ago very simple you have a base image

00:20:32,560 --> 00:20:37,900
with Java and then you add your jar and

00:20:35,140 --> 00:20:39,700
then you say entry point when Christian

00:20:37,900 --> 00:20:42,550
was sitting here he would say basically

00:20:39,700 --> 00:20:46,150
hugging that stupid don't do that in the

00:20:42,550 --> 00:20:48,520
entry point don't do that bad practice

00:20:46,150 --> 00:20:51,040
so everybody I have said that that's bad

00:20:48,520 --> 00:20:54,220
practice I like it because I still see

00:20:51,040 --> 00:20:54,580
what I'm doing and there's no hidden in

00:20:54,220 --> 00:20:56,980
it

00:20:54,580 --> 00:21:00,130
FH script in my container mounted

00:20:56,980 --> 00:21:01,780
somewhere that it does it's magic or I

00:21:00,130 --> 00:21:03,820
inherit and then I don't know what to do

00:21:01,780 --> 00:21:06,130
or I need to relook into that container

00:21:03,820 --> 00:21:09,850
and fix it and see what's my shell

00:21:06,130 --> 00:21:13,150
script doing for me this is simple and

00:21:09,850 --> 00:21:15,430
then to just get the container out there

00:21:13,150 --> 00:21:17,200
we just okay I have my IDE I have my

00:21:15,430 --> 00:21:19,510
builded my jar and then I check my

00:21:17,200 --> 00:21:22,210
container I log into docker and then I

00:21:19,510 --> 00:21:23,980
publish my container we all do this

00:21:22,210 --> 00:21:26,110
without signing of course and we don't

00:21:23,980 --> 00:21:27,150
have our own registries or find it's all

00:21:26,110 --> 00:21:30,630
secure

00:21:27,150 --> 00:21:35,400
don't wear is here so the complete tool

00:21:30,630 --> 00:21:40,260
chain now looks like this and if you get

00:21:35,400 --> 00:21:42,540
rid of this and just use the command we

00:21:40,260 --> 00:21:47,309
are still at the command line interface

00:21:42,540 --> 00:21:50,610
from what we just need to now say images

00:21:47,309 --> 00:21:52,650
host name container name I just do this

00:21:50,610 --> 00:21:55,260
actually I think you can get rid of it

00:21:52,650 --> 00:21:56,970
if you want to but I think it kind of

00:21:55,260 --> 00:22:01,140
works because I change together my

00:21:56,970 --> 00:22:02,790
processes and because I did my programs

00:22:01,140 --> 00:22:08,010
in that way that I can have the input

00:22:02,790 --> 00:22:10,050
topic and the outputs are baked so I I

00:22:08,010 --> 00:22:11,790
wouldn't model the pipe in here in

00:22:10,050 --> 00:22:15,270
between these specifications right I

00:22:11,790 --> 00:22:16,830
have to still do string args to say

00:22:15,270 --> 00:22:20,420
where you're coming from what you're

00:22:16,830 --> 00:22:23,850
reading it and where you're writing to

00:22:20,420 --> 00:22:25,740
but then in the data center setup we

00:22:23,850 --> 00:22:27,929
want to now have more talk engines and

00:22:25,740 --> 00:22:32,880
want to distribute the containers across

00:22:27,929 --> 00:22:34,590
more talk engines and now we figured out

00:22:32,880 --> 00:22:39,290
we can something we can do something

00:22:34,590 --> 00:22:42,660
else with compose 3 we can basically

00:22:39,290 --> 00:22:48,540
launch services in program and maybe we

00:22:42,660 --> 00:22:51,059
don't need console anymore because we

00:22:48,540 --> 00:22:53,280
can now use some something inside the

00:22:51,059 --> 00:22:55,740
swarm engine that gives us the server

00:22:53,280 --> 00:22:58,710
name or the service names and the task

00:22:55,740 --> 00:23:02,160
ID to identify a certain service and

00:22:58,710 --> 00:23:06,780
that's what we do in the version 3

00:23:02,160 --> 00:23:10,650
compose file yeah so you join you build

00:23:06,780 --> 00:23:12,690
a network you then be on the network and

00:23:10,650 --> 00:23:14,340
then for deploy doctor services you can

00:23:12,690 --> 00:23:17,040
actually say how many replicas do you

00:23:14,340 --> 00:23:23,070
want and what's the resource constraints

00:23:17,040 --> 00:23:25,800
for any given service um here's another

00:23:23,070 --> 00:23:27,780
thing where you can have the update

00:23:25,800 --> 00:23:30,059
consequences when I do a rolling upgrade

00:23:27,780 --> 00:23:31,590
of your service what would be the

00:23:30,059 --> 00:23:33,330
intermediate delay and what's the

00:23:31,590 --> 00:23:36,060
parallelism so how many shutdowns I do

00:23:33,330 --> 00:23:39,630
at one given time imagined

00:23:36,060 --> 00:23:42,210
this live feed fake producer would be

00:23:39,630 --> 00:23:44,640
there 10 times and I said they do a

00:23:42,210 --> 00:23:46,770
rolling upgrade and it would go one by

00:23:44,640 --> 00:23:48,150
one by one or it can go I shut

00:23:46,770 --> 00:23:49,740
everything down and inside everything

00:23:48,150 --> 00:23:52,740
you again you have control over that

00:23:49,740 --> 00:23:55,950
with compulsory detention and then you

00:23:52,740 --> 00:23:57,930
have a restart policy do you want to try

00:23:55,950 --> 00:24:00,150
once and then say forever do you want to

00:23:57,930 --> 00:24:03,390
always when you die you get restarted

00:24:00,150 --> 00:24:05,220
again your control over that I think

00:24:03,390 --> 00:24:07,380
it's very nice and then in the data

00:24:05,220 --> 00:24:13,110
center it would look something like oops

00:24:07,380 --> 00:24:15,060
sorry I create your network deploy the

00:24:13,110 --> 00:24:18,030
stack and I've splitted my stake into

00:24:15,060 --> 00:24:21,030
the back ends the distributed pipe the

00:24:18,030 --> 00:24:24,440
Caprica thingy and the stream processors

00:24:21,030 --> 00:24:26,760
and the front end so you can have both

00:24:24,440 --> 00:24:28,290
running and because in that

00:24:26,760 --> 00:24:32,160
specification they share the same

00:24:28,290 --> 00:24:35,490
attachable network they all see each

00:24:32,160 --> 00:24:41,430
other and they work together okay

00:24:35,490 --> 00:24:44,490
having the recap it's basically a pipe

00:24:41,430 --> 00:24:46,350
and a function can be expressed as a

00:24:44,490 --> 00:24:49,460
message broker and a string processor

00:24:46,350 --> 00:24:54,330
and you can put them all into Tokra and

00:24:49,460 --> 00:24:59,520
then be happy now there are some things

00:24:54,330 --> 00:25:00,810
that that I think are for tooling it's

00:24:59,520 --> 00:25:02,070
good enough but I would never try this

00:25:00,810 --> 00:25:04,860
actually in production I guess and I'm

00:25:02,070 --> 00:25:07,130
fine with my I saris holding off and

00:25:04,860 --> 00:25:11,070
saying hang on to that

00:25:07,130 --> 00:25:14,250
so yeah the counting how do you count

00:25:11,070 --> 00:25:15,630
something when you see something you

00:25:14,250 --> 00:25:18,090
don't know how often you see something

00:25:15,630 --> 00:25:20,580
that's how do you count what's ten now

00:25:18,090 --> 00:25:22,710
this is ten eleven is ten twenty ten one

00:25:20,580 --> 00:25:24,180
hundred something has gone wrong your

00:25:22,710 --> 00:25:25,560
broker would be just restarting you

00:25:24,180 --> 00:25:27,390
upgrade the codec if you were rolling

00:25:25,560 --> 00:25:31,230
upgrade and boom ten thousand messages

00:25:27,390 --> 00:25:34,590
are duplicated it said now ten thousand

00:25:31,230 --> 00:25:39,090
oh one I estimated 10,000 with my heavy

00:25:34,590 --> 00:25:42,180
data here but exactly once is coming

00:25:39,090 --> 00:25:45,279
there are some transaction API for Kafka

00:25:42,180 --> 00:25:46,960
I saw one talk about it

00:25:45,279 --> 00:25:48,309
didn't fully grasp but I guess it's

00:25:46,960 --> 00:25:52,059
coming and that's obviously something

00:25:48,309 --> 00:25:54,490
that fling provides you and yeah for

00:25:52,059 --> 00:25:56,230
doing some exact calculation field you

00:25:54,490 --> 00:25:58,769
need exactly once and it's not like

00:25:56,230 --> 00:26:01,809
shouldn't be like that

00:25:58,769 --> 00:26:03,700
so going from a laptop to a data center

00:26:01,809 --> 00:26:07,149
you still need capacity planning there's

00:26:03,700 --> 00:26:08,679
no way that you go I just like how do

00:26:07,149 --> 00:26:10,179
you partition your data how many

00:26:08,679 --> 00:26:11,379
processes can do what

00:26:10,179 --> 00:26:13,749
where are your blocking where you

00:26:11,379 --> 00:26:16,509
waiting on what you still need to do

00:26:13,749 --> 00:26:18,129
that it's not like you like it it's a

00:26:16,509 --> 00:26:20,529
difference if you try to process 10

00:26:18,129 --> 00:26:23,740
terabytes or 10 gigabytes of data and

00:26:20,529 --> 00:26:25,269
how many brokers do you want to apply

00:26:23,740 --> 00:26:27,100
and how much stream processors do you

00:26:25,269 --> 00:26:32,019
need and which stage is very costly and

00:26:27,100 --> 00:26:34,600
whatnot and that's still not trivial and

00:26:32,019 --> 00:26:36,070
there's still nothing I can see that

00:26:34,600 --> 00:26:39,039
that helps you better or do it

00:26:36,070 --> 00:26:41,860
automatically and testing and debugging

00:26:39,039 --> 00:26:44,110
of course it runs on docker you can have

00:26:41,860 --> 00:26:45,909
integration says you can have your

00:26:44,110 --> 00:26:47,350
colleague and you build a docker swarm

00:26:45,909 --> 00:26:48,730
and then you can go a bit bigger and

00:26:47,350 --> 00:26:51,850
then you can take the data and then you

00:26:48,730 --> 00:26:55,840
can look at it even and blah blah but in

00:26:51,850 --> 00:26:57,970
production what about this consistency

00:26:55,840 --> 00:27:00,190
in the state storage so it has the state

00:26:57,970 --> 00:27:01,840
storage and it's a multi bi case by

00:27:00,190 --> 00:27:03,909
default it's a key value store and then

00:27:01,840 --> 00:27:05,409
it gets swing over the network and the

00:27:03,909 --> 00:27:08,769
changes will be applied to all other

00:27:05,409 --> 00:27:10,240
instances but it still kept that kicks

00:27:08,769 --> 00:27:11,889
in and then it's still kind of like how

00:27:10,240 --> 00:27:14,470
do I know in a production scenario that

00:27:11,889 --> 00:27:17,470
everything is correct then it's not

00:27:14,470 --> 00:27:22,440
trivial to debug in the you have now a

00:27:17,470 --> 00:27:24,759
nice API but doing that it's necessary

00:27:22,440 --> 00:27:28,779
processing time versus ease and time are

00:27:24,759 --> 00:27:32,529
you sure that what you're counting here

00:27:28,779 --> 00:27:35,649
is happening in the same time interval

00:27:32,529 --> 00:27:37,840
or are you looking at something that the

00:27:35,649 --> 00:27:39,340
event claimed yesterday or was created

00:27:37,840 --> 00:27:42,159
yesterday but I'm now counting it in

00:27:39,340 --> 00:27:45,909
that window of now does that make sense

00:27:42,159 --> 00:27:48,100
so you need to be aware of where do I

00:27:45,909 --> 00:27:50,710
put in my processing time when when do i

00:27:48,100 --> 00:27:52,300
time stand like this is now and this is

00:27:50,710 --> 00:27:56,380
for me now and then these

00:27:52,300 --> 00:27:58,330
is yesterday or today like that there

00:27:56,380 --> 00:28:00,900
are some tricky thing is that can go

00:27:58,330 --> 00:28:05,650
with wrong with your sprinkler system

00:28:00,900 --> 00:28:07,660
and what about Amdahl's law so yeah sure

00:28:05,650 --> 00:28:09,910
I can build a hundred topics and cuff

00:28:07,660 --> 00:28:11,860
that would happily do that and I

00:28:09,910 --> 00:28:13,630
couldn't also process and through a

00:28:11,860 --> 00:28:15,460
hundred car topic and it will heavily do

00:28:13,630 --> 00:28:19,240
that but now I'm doing this a cricket by

00:28:15,460 --> 00:28:21,130
key what's now like is this a single

00:28:19,240 --> 00:28:23,620
point of aggregation and everybody wait

00:28:21,130 --> 00:28:26,590
and hold off until my one job gets

00:28:23,620 --> 00:28:29,770
together and basically has everything or

00:28:26,590 --> 00:28:32,950
do we build these graphs where you go

00:28:29,770 --> 00:28:34,780
and okay that's the first map step then

00:28:32,950 --> 00:28:37,420
you reduce then you reduce further then

00:28:34,780 --> 00:28:39,480
you reduce further are you need to

00:28:37,420 --> 00:28:42,250
rebuild this now with kafka streams or

00:28:39,480 --> 00:28:47,080
Kenya it does it just magically works I

00:28:42,250 --> 00:28:48,790
think at a certain size I'm not law will

00:28:47,080 --> 00:28:51,940
get you because there will be threads

00:28:48,790 --> 00:28:56,560
waiting to get get the aggregation or

00:28:51,940 --> 00:29:00,910
the final step done and then docker

00:28:56,560 --> 00:29:04,300
volumes so you have a bare-metal hosts

00:29:00,910 --> 00:29:08,050
and then you say yeah that's my root

00:29:04,300 --> 00:29:10,840
server big data directory and everything

00:29:08,050 --> 00:29:12,520
every container you build mounts into

00:29:10,840 --> 00:29:15,130
that and puts in the application name

00:29:12,520 --> 00:29:18,370
and then you write to the disk and the

00:29:15,130 --> 00:29:20,110
container dies eventually he will be

00:29:18,370 --> 00:29:25,480
back up again and hook to that same

00:29:20,110 --> 00:29:28,000
volume and everything is good but for

00:29:25,480 --> 00:29:30,040
elasticsearch does that work does it

00:29:28,000 --> 00:29:32,410
actually will it pick up and then the

00:29:30,040 --> 00:29:34,450
indexes and and then will elastic search

00:29:32,410 --> 00:29:37,210
happily do the reallocation of the

00:29:34,450 --> 00:29:38,770
shards again because it has now in

00:29:37,210 --> 00:29:41,740
different note ID running on the same

00:29:38,770 --> 00:29:47,050
volumes with the same I guess not

00:29:41,740 --> 00:29:49,090
and then Kafka also can you like no so

00:29:47,050 --> 00:29:52,210
docker volumes and like container rising

00:29:49,090 --> 00:29:56,560
these these data systems still has some

00:29:52,210 --> 00:29:58,120
issues and it's there were one idea I

00:29:56,560 --> 00:30:00,580
don't know if it was in December or

00:29:58,120 --> 00:30:03,670
January where somebody said basically

00:30:00,580 --> 00:30:04,559
all our volumes now live in zest so this

00:30:03,670 --> 00:30:07,860
is it

00:30:04,559 --> 00:30:10,889
also distributed file system a bit

00:30:07,860 --> 00:30:12,629
quicker than HDFS and then they would

00:30:10,889 --> 00:30:14,399
put their volumes in there and they

00:30:12,629 --> 00:30:16,649
could pinpoint the volumes to docker

00:30:14,399 --> 00:30:19,169
container IDs and then when a container

00:30:16,649 --> 00:30:21,600
dies we will come up at a different node

00:30:19,169 --> 00:30:23,669
and then we'll know my volume is F and F

00:30:21,600 --> 00:30:27,389
goes out I'm actually in your wreck here

00:30:23,669 --> 00:30:34,679
you have your volume and that seems to

00:30:27,389 --> 00:30:38,909
be a promising idea so I was a bit so I

00:30:34,679 --> 00:30:41,249
think my my little example didn't work

00:30:38,909 --> 00:30:45,269
for me I can do that and I will do that

00:30:41,249 --> 00:30:47,610
for not production stuff but if I need

00:30:45,269 --> 00:30:49,320
some big data processing cleaning up

00:30:47,610 --> 00:30:53,249
some data I will actually write a small

00:30:49,320 --> 00:30:55,289
stream processor and use my taka stack

00:30:53,249 --> 00:31:00,059
here and then work with that in that way

00:30:55,289 --> 00:31:04,649
but more importantly I think what it

00:31:00,059 --> 00:31:07,860
also shows is you just need something

00:31:04,649 --> 00:31:09,840
and you not often you need really Hadoop

00:31:07,860 --> 00:31:11,639
I mean most companies have it but are

00:31:09,840 --> 00:31:13,830
you really needing it for what you are

00:31:11,639 --> 00:31:16,649
trying to solve here or just using it

00:31:13,830 --> 00:31:18,929
because it's already there so yes

00:31:16,649 --> 00:31:21,059
I'm not against a dig at all maybe it

00:31:18,929 --> 00:31:23,879
comes around wrongly but it's just that

00:31:21,059 --> 00:31:26,070
in order to have a simple system there

00:31:23,879 --> 00:31:28,799
are lot of lines of code in the little

00:31:26,070 --> 00:31:31,470
snake and it's not exactly a fresh

00:31:28,799 --> 00:31:34,830
technology by now anymore so maybe you

00:31:31,470 --> 00:31:37,499
don't need and maybe you can't can do

00:31:34,830 --> 00:31:40,740
something with it without it and then

00:31:37,499 --> 00:31:43,470
other Einstein also things that you need

00:31:40,740 --> 00:31:51,320
to do the stuff needs to be simple but

00:31:43,470 --> 00:31:53,730
not too simple I just let that sink and

00:31:51,320 --> 00:31:58,549
then what what I told me at my

00:31:53,730 --> 00:32:01,470
university was the sky very more calm

00:31:58,549 --> 00:32:06,119
the fuels are some assumption right we

00:32:01,470 --> 00:32:11,249
tend to do everything based on a lot of

00:32:06,119 --> 00:32:13,409
assumptions and clean that out get rid

00:32:11,249 --> 00:32:15,029
of it like you really if you look at

00:32:13,409 --> 00:32:15,960
something if you want to have that fire

00:32:15,029 --> 00:32:17,580
here it's

00:32:15,960 --> 00:32:21,690
removing that fire there's no need to

00:32:17,580 --> 00:32:23,790
spin up what's that called it's called a

00:32:21,690 --> 00:32:25,740
 laughter I heard you can shift

00:32:23,790 --> 00:32:28,140
files around from one this to another or

00:32:25,740 --> 00:32:30,270
spring boot you can use spring to copy

00:32:28,140 --> 00:32:34,230
as files around yeah you can do that but

00:32:30,270 --> 00:32:35,910
you shouldn't and wherever you think of

00:32:34,230 --> 00:32:37,230
like how that's complicated maybe your

00:32:35,910 --> 00:32:40,440
feeling is exactly right at that point

00:32:37,230 --> 00:32:42,420
and yeah don't do it then we have enough

00:32:40,440 --> 00:32:45,330
lines of code and they bothering all our

00:32:42,420 --> 00:32:48,570
day it's like it's not worth it just if

00:32:45,330 --> 00:32:49,920
you don't like also but now I'm getting

00:32:48,570 --> 00:32:52,950
off the rails of it if you have a

00:32:49,920 --> 00:32:55,860
five-week working week like five days of

00:32:52,950 --> 00:32:58,680
work spend a Friday not committing a

00:32:55,860 --> 00:33:02,010
line of code that would be your Friday

00:32:58,680 --> 00:33:05,640
go not adding code just too much of them

00:33:02,010 --> 00:33:10,250
so we have five minutes left but I guess

00:33:05,640 --> 00:33:10,250
it's done thanks for this

00:33:15,979 --> 00:33:20,369
why are any questions yeah so we have

00:33:18,479 --> 00:33:25,460
some questions we have exactly five

00:33:20,369 --> 00:33:25,460
minutes so there is some time first one

00:33:28,429 --> 00:33:34,469
thank you for the talk first question is

00:33:31,859 --> 00:33:38,249
have you considered using actually UNIX

00:33:34,469 --> 00:33:41,700
pipes when when running lots of

00:33:38,249 --> 00:33:43,919
container locally and maybe you don't

00:33:41,700 --> 00:33:46,559
need containers because you have servers

00:33:43,919 --> 00:33:48,619
with tens of processors and you can do

00:33:46,559 --> 00:33:51,599
lots of parallel things just in one

00:33:48,619 --> 00:33:55,169
single server the exact questioning yeah

00:33:51,599 --> 00:33:57,359
no I get teached by my injuries

00:33:55,169 --> 00:33:59,639
engineers all the time how to do not

00:33:57,359 --> 00:34:02,070
that just do it on our house we have

00:33:59,639 --> 00:34:04,320
pretty big holes and yeah it would work

00:34:02,070 --> 00:34:09,149
the problem is we already put everything

00:34:04,320 --> 00:34:12,960
in Hadoop and Kafka so I would need

00:34:09,149 --> 00:34:13,619
something Bosch Kafka tale maybe a go

00:34:12,960 --> 00:34:15,329
language

00:34:13,619 --> 00:34:17,520
goldang program would solvent now

00:34:15,329 --> 00:34:19,889
absolutely right of course for some of

00:34:17,520 --> 00:34:22,250
this stuff you don't even need this but

00:34:19,889 --> 00:34:26,069
like in some organizations we have this

00:34:22,250 --> 00:34:28,349
data pipeline already here and you just

00:34:26,069 --> 00:34:32,659
hook into it and you can maybe gain some

00:34:28,349 --> 00:34:32,659
simplicity by using cough cough streams

00:34:38,560 --> 00:34:47,760
Gracie thank you very much

00:34:42,320 --> 00:34:47,760

YouTube URL: https://www.youtube.com/watch?v=-hlblcL46cQ


