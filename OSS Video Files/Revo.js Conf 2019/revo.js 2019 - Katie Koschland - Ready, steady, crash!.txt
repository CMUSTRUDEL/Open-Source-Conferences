Title: revo.js 2019 - Katie Koschland - Ready, steady, crash!
Publication date: 2019-10-25
Playlist: Revo.js Conf 2019
Description: 
	Ready, steady, crash!
Presented by Katie Koschland, at revo.js 2019
https://revojs.ro/2019/speakers/katie-koschland
---
The key to finding the optimal solution to a problem is to threefold: maximise the number of tools in your ‘personal tool box’; know all their capabilities and uses; and to know which tool is best suited to fix the particular problem in front of you.

This talk will enable you to grow all three aspects around load testing.

It will look at what it means to ‘load test’ your application, how to get started with writing your first script and provide you with a case study and an insight as to the tools and techniques that we use here at the FT to help identify such performance issues.
Captions: 
	00:00:00,720 --> 00:00:08,230
[Music]

00:00:09,390 --> 00:00:17,490
[Applause]

00:00:14,120 --> 00:00:19,410
hi um before we go any further I just

00:00:17,490 --> 00:00:22,710
want to warn you that the presentation

00:00:19,410 --> 00:00:24,900
does contain flashing content so if that

00:00:22,710 --> 00:00:29,609
makes you feel uncomfortable please do

00:00:24,900 --> 00:00:33,570
let me know now okay so let's get

00:00:29,609 --> 00:00:35,969
started have anyone in the audience ever

00:00:33,570 --> 00:00:39,540
come across a narrow page like this

00:00:35,969 --> 00:00:41,940
before where it says hang on a sec our

00:00:39,540 --> 00:00:44,969
sites experiencing more traffic than

00:00:41,940 --> 00:00:47,850
usual but you know you should be able to

00:00:44,969 --> 00:00:50,879
get back to shopping shortly raise your

00:00:47,850 --> 00:00:52,379
hands if you had okay

00:00:50,879 --> 00:00:55,350
so there's actually quite a few of you

00:00:52,379 --> 00:00:58,829
in the audience anam I did some research

00:00:55,350 --> 00:01:00,809
about this and it is fairly common even

00:00:58,829 --> 00:01:03,030
the most reputable sites displayed this

00:01:00,809 --> 00:01:05,400
type of error message about not being

00:01:03,030 --> 00:01:09,119
able to handle the level of traffic

00:01:05,400 --> 00:01:12,270
that's hitting their application even

00:01:09,119 --> 00:01:14,700
Amazon on Amazon Prime day as I'm sure

00:01:12,270 --> 00:01:17,220
you can all imagine and there was this

00:01:14,700 --> 00:01:19,920
huge spike in user traffic hitting their

00:01:17,220 --> 00:01:22,500
site and as a result users were just

00:01:19,920 --> 00:01:24,509
unable to access the site and they

00:01:22,500 --> 00:01:27,000
couldn't make the purchases they wanted

00:01:24,509 --> 00:01:31,409
to make think about the loss of sales as

00:01:27,000 --> 00:01:33,840
a consequence but going back to the

00:01:31,409 --> 00:01:34,470
j.crew experience I have to be honest

00:01:33,840 --> 00:01:38,340
about it

00:01:34,470 --> 00:01:39,930
I was slightly embarrassed if not

00:01:38,340 --> 00:01:43,290
disappointed in myself

00:01:39,930 --> 00:01:45,600
when I visited the site I visited the

00:01:43,290 --> 00:01:48,570
site and thought about my own personal

00:01:45,600 --> 00:01:51,680
experience not being able to make the

00:01:48,570 --> 00:01:54,240
patches I really want just make but I

00:01:51,680 --> 00:01:58,079
didn't really think about it from the

00:01:54,240 --> 00:02:01,530
other perspective I build websites just

00:01:58,079 --> 00:02:04,590
like the ones I've mentioned and never

00:02:01,530 --> 00:02:09,300
once did I think could my users be

00:02:04,590 --> 00:02:12,000
having a similar experience hi my name

00:02:09,300 --> 00:02:14,970
is Katie Carson I'm an engineer and

00:02:12,000 --> 00:02:17,940
who lead and I work at the Financial

00:02:14,970 --> 00:02:19,560
Times the Financial Times is one of the

00:02:17,940 --> 00:02:22,140
world's leading business news

00:02:19,560 --> 00:02:24,650
organizations and it's recognized

00:02:22,140 --> 00:02:28,890
internationally for its authority

00:02:24,650 --> 00:02:31,020
integrity and accuracy and what I'm

00:02:28,890 --> 00:02:34,560
gonna do today is I'm gonna talk about

00:02:31,020 --> 00:02:37,230
my load-testing journey and I'm gonna

00:02:34,560 --> 00:02:40,920
aim to cover the core concepts behind

00:02:37,230 --> 00:02:43,140
new tastic first I'll start with a

00:02:40,920 --> 00:02:46,650
challenge and this is how I came across

00:02:43,140 --> 00:02:48,750
the topic of lay tastic I'll then go on

00:02:46,650 --> 00:02:51,390
to explain what it means to load test

00:02:48,750 --> 00:02:53,750
your application and how you can get

00:02:51,390 --> 00:02:56,459
started with writing your first script

00:02:53,750 --> 00:02:58,590
then I'll go on to explain how you can

00:02:56,459 --> 00:03:00,810
use the metrics to help identify

00:02:58,590 --> 00:03:03,450
performance issues within your

00:03:00,810 --> 00:03:06,270
application and discuss the tools and

00:03:03,450 --> 00:03:09,030
techniques we use at the FT to help

00:03:06,270 --> 00:03:12,900
troubleshoot and improve such issues and

00:03:09,030 --> 00:03:15,780
then I'll conclude with some outcomes so

00:03:12,900 --> 00:03:17,280
the challenge I recently finished as

00:03:15,780 --> 00:03:19,440
three months of comment in the

00:03:17,280 --> 00:03:21,870
operations and reliability engineering

00:03:19,440 --> 00:03:25,080
team where we were developing the

00:03:21,870 --> 00:03:28,590
business API the purpose of the business

00:03:25,080 --> 00:03:31,970
API was to provide a central store of

00:03:28,590 --> 00:03:35,310
all business information across the ft

00:03:31,970 --> 00:03:37,530
above illustrates the model and the type

00:03:35,310 --> 00:03:40,680
of information that we that would be

00:03:37,530 --> 00:03:43,080
retrieved from the business API so you

00:03:40,680 --> 00:03:45,959
can see here there's information about

00:03:43,080 --> 00:03:48,390
systems how those systems relate to

00:03:45,959 --> 00:03:51,510
other systems and how this then go on to

00:03:48,390 --> 00:03:54,840
relate to groups teams and people across

00:03:51,510 --> 00:03:56,850
the FT and so this API and application

00:03:54,840 --> 00:03:59,430
was gonna used it was gonna be used

00:03:56,850 --> 00:04:00,780
frequently across the company and so we

00:03:59,430 --> 00:04:03,230
really needed to understand the

00:04:00,780 --> 00:04:05,280
technical limits of the application so

00:04:03,230 --> 00:04:07,440
that was my challenge

00:04:05,280 --> 00:04:10,500
it was testing the technical limits of

00:04:07,440 --> 00:04:12,930
our application but more specifically

00:04:10,500 --> 00:04:17,250
than that could I answer the following

00:04:12,930 --> 00:04:23,200
three questions will our application

00:04:17,250 --> 00:04:27,100
crash if so can our application recover

00:04:23,200 --> 00:04:29,920
um what exactly happens to that user

00:04:27,100 --> 00:04:33,340
experience once we found the limit of

00:04:29,920 --> 00:04:35,730
our application and push past it does Li

00:04:33,340 --> 00:04:39,420
use experience degrade gracefully or

00:04:35,730 --> 00:04:44,800
does it just cause an absolute meltdown

00:04:39,420 --> 00:04:48,570
and I was told that the answer could be

00:04:44,800 --> 00:04:52,690
found by Li testing our application so

00:04:48,570 --> 00:04:55,450
what exactly is load testing a colleague

00:04:52,690 --> 00:04:56,830
of mine recommended an excellent talk by

00:04:55,450 --> 00:04:59,680
jad Micucci

00:04:56,830 --> 00:05:02,800
where he describes li testing in simple

00:04:59,680 --> 00:05:05,620
terms it's about simulating ordinary

00:05:02,800 --> 00:05:09,280
user activity and then applying enough

00:05:05,620 --> 00:05:11,680
stress until it reaches failure I really

00:05:09,280 --> 00:05:15,550
like this definition but it kind of led

00:05:11,680 --> 00:05:17,380
me to my next question which is so how

00:05:15,550 --> 00:05:20,860
can you actually go about and apply that

00:05:17,380 --> 00:05:22,720
so-called stress to the application so

00:05:20,860 --> 00:05:25,120
that's what you need to do is you need

00:05:22,720 --> 00:05:26,680
to decide on a testing framework there

00:05:25,120 --> 00:05:29,820
are many frameworks out there um

00:05:26,680 --> 00:05:33,970
including popular ones like Apache bench

00:05:29,820 --> 00:05:36,610
Gatling Lucas artillery WAC but I

00:05:33,970 --> 00:05:40,840
decided to go with artillery and it was

00:05:36,610 --> 00:05:43,840
for the following five reasons first it

00:05:40,840 --> 00:05:45,970
could be used and installed using MTF

00:05:43,840 --> 00:05:47,890
which was really great if I want to get

00:05:45,970 --> 00:05:51,310
started quickly as we were building a

00:05:47,890 --> 00:05:53,260
new application the other thing was it

00:05:51,310 --> 00:05:56,830
allowed you to customize JavaScript code

00:05:53,260 --> 00:05:59,050
so what you could do was you could write

00:05:56,830 --> 00:06:01,510
a random you could write the function

00:05:59,050 --> 00:06:03,880
separately and in our case to generate

00:06:01,510 --> 00:06:05,980
random write queries and then cool it in

00:06:03,880 --> 00:06:07,470
your script each time of actual user was

00:06:05,980 --> 00:06:10,930
to make a request

00:06:07,470 --> 00:06:12,850
you could also and you could write for

00:06:10,930 --> 00:06:15,430
example a JavaScript function that sets

00:06:12,850 --> 00:06:17,110
headers or it sets cookies and then each

00:06:15,430 --> 00:06:19,540
time your virtual user makes a request

00:06:17,110 --> 00:06:22,030
in your script you can just import the

00:06:19,540 --> 00:06:23,950
function and call it and and that was

00:06:22,030 --> 00:06:25,840
really helpful and you can see above a

00:06:23,950 --> 00:06:26,610
snapshot of how we went about and did

00:06:25,840 --> 00:06:29,110
that

00:06:26,610 --> 00:06:31,930
another advantage was that scripts could

00:06:29,110 --> 00:06:34,420
be configured using Yammer and again

00:06:31,930 --> 00:06:36,070
this is just a snapshot but what you can

00:06:34,420 --> 00:06:38,380
see is by writing it in

00:06:36,070 --> 00:06:41,830
yama it makes those scripts really easy

00:06:38,380 --> 00:06:44,350
to read maintain across your tee and you

00:06:41,830 --> 00:06:46,690
can see here that I'm calling the set

00:06:44,350 --> 00:06:49,990
header section I mentioned previously

00:06:46,690 --> 00:06:52,960
and also it's targeted towards

00:06:49,990 --> 00:06:54,700
continuous integration unfortunately I

00:06:52,960 --> 00:06:55,480
didn't have a chance to implement this

00:06:54,700 --> 00:06:56,830
ourselves

00:06:55,480 --> 00:06:59,080
but isn't something I'm particularly

00:06:56,830 --> 00:07:02,320
interested in and would really like to

00:06:59,080 --> 00:07:04,930
do in our next project if performance is

00:07:02,320 --> 00:07:07,450
a high priority within your application

00:07:04,930 --> 00:07:09,910
what you can do is you can configure

00:07:07,450 --> 00:07:13,990
your continuous integration let's say

00:07:09,910 --> 00:07:16,330
circle CI and run your script each time

00:07:13,990 --> 00:07:18,130
you trigger a build what this allows you

00:07:16,330 --> 00:07:21,010
to do is that you're able to just have

00:07:18,130 --> 00:07:23,650
at each point in your application and

00:07:21,010 --> 00:07:26,680
your development process how that future

00:07:23,650 --> 00:07:29,320
directly impacts performance and so you

00:07:26,680 --> 00:07:31,990
can detect any regressions in your

00:07:29,320 --> 00:07:34,660
application and finally it provides

00:07:31,990 --> 00:07:37,570
detailed metrics and I'll go on to

00:07:34,660 --> 00:07:39,520
explain that shortly so once you've

00:07:37,570 --> 00:07:42,280
decided on your testing framework and

00:07:39,520 --> 00:07:43,570
your then need to think about the type

00:07:42,280 --> 00:07:47,140
of requests that be hitting your

00:07:43,570 --> 00:07:49,350
application so what your scripts need to

00:07:47,140 --> 00:07:53,470
do is they need to be able to represent

00:07:49,350 --> 00:07:54,910
ordinary user activity so what you want

00:07:53,470 --> 00:07:56,650
to do is you want to think about the

00:07:54,910 --> 00:07:59,680
common user journeys that are hitting

00:07:56,650 --> 00:08:02,200
your application and try to mirror what

00:07:59,680 --> 00:08:04,660
your real users are making and capture

00:08:02,200 --> 00:08:06,790
that behavior so your virtual users are

00:08:04,660 --> 00:08:10,870
making the same type of requests within

00:08:06,790 --> 00:08:12,880
the application and after that you need

00:08:10,870 --> 00:08:15,310
to decide on the shape of load that's

00:08:12,880 --> 00:08:17,320
hitting your application you're probably

00:08:15,310 --> 00:08:19,030
thinking like what do we actually mean

00:08:17,320 --> 00:08:22,090
by the shape of load and they'll go on

00:08:19,030 --> 00:08:24,790
to explain this if you take this graph

00:08:22,090 --> 00:08:26,590
above as an example it illustrates the

00:08:24,790 --> 00:08:30,910
shape of load that was hitting the

00:08:26,590 --> 00:08:34,719
ft.com homepage over a 24 hour during

00:08:30,910 --> 00:08:37,000
the 2017 election results so what you

00:08:34,719 --> 00:08:39,070
have is you have time across the x-axis

00:08:37,000 --> 00:08:41,830
and then you've got the number of user

00:08:39,070 --> 00:08:44,050
requests along the y-axis and you can

00:08:41,830 --> 00:08:47,260
see here that the shape of load changes

00:08:44,050 --> 00:08:47,930
drastically over time you've got Peaks

00:08:47,260 --> 00:08:49,850
where

00:08:47,930 --> 00:08:52,190
the number of concurrent requests per

00:08:49,850 --> 00:08:54,410
minute are extremely high and then you

00:08:52,190 --> 00:08:57,710
have trucks weathers like this very low

00:08:54,410 --> 00:08:59,060
level of concurrent requests so what you

00:08:57,710 --> 00:09:01,430
really want to do is you want your

00:08:59,060 --> 00:09:03,500
scripts to be able to capture the

00:09:01,430 --> 00:09:05,210
different shapes of load that could

00:09:03,500 --> 00:09:08,900
potentially be hitting your application

00:09:05,210 --> 00:09:11,630
over a production cycle um this can be

00:09:08,900 --> 00:09:13,490
really challenging and before production

00:09:11,630 --> 00:09:15,740
you know it's really difficult to

00:09:13,490 --> 00:09:17,780
predict the number of concurrent users

00:09:15,740 --> 00:09:20,840
and how they would essentially ramp in

00:09:17,780 --> 00:09:23,000
over time but it's really crucial if you

00:09:20,840 --> 00:09:27,290
want your scripts to be as robust as

00:09:23,000 --> 00:09:29,360
possible so if your applications already

00:09:27,290 --> 00:09:30,530
in production you might already know the

00:09:29,360 --> 00:09:34,460
shape of load that's hitting your

00:09:30,530 --> 00:09:36,050
application for example and a situation

00:09:34,460 --> 00:09:38,300
where you can predict the shape of load

00:09:36,050 --> 00:09:43,040
beforehand is before the release of a

00:09:38,300 --> 00:09:45,380
push notification so last week and the

00:09:43,040 --> 00:09:49,430
FT mobile apps and how a personification

00:09:45,380 --> 00:09:51,860
to notify users that the UK's Supreme

00:09:49,430 --> 00:09:55,640
Court ruled against Boris Johnson's

00:09:51,860 --> 00:09:57,740
suspension of Parliament before the

00:09:55,640 --> 00:10:00,860
personal notification went out the app

00:09:57,740 --> 00:10:03,890
was doing approximately 250 requests per

00:10:00,860 --> 00:10:07,340
minute and shortly after the pitch what

00:10:03,890 --> 00:10:10,940
you can see is it reached 2.6 million

00:10:07,340 --> 00:10:13,460
requests per minute and it wasn't just

00:10:10,940 --> 00:10:15,380
the app it impacted it was also the

00:10:13,460 --> 00:10:17,600
website ft.com

00:10:15,380 --> 00:10:21,020
before the patient notification was

00:10:17,600 --> 00:10:25,060
doing 200,000 requests per minute and

00:10:21,020 --> 00:10:28,460
shortly after the push it reached over

00:10:25,060 --> 00:10:31,160
550,000 requests per minute so in this

00:10:28,460 --> 00:10:33,260
situation we can predict you know the

00:10:31,160 --> 00:10:35,750
shape of there is going to be a huge

00:10:33,260 --> 00:10:39,500
spike in user traffic over this very

00:10:35,750 --> 00:10:41,540
short period of time but you know it's

00:10:39,500 --> 00:10:44,540
also okay if you actually don't know the

00:10:41,540 --> 00:10:46,280
shape of load as the business API was

00:10:44,540 --> 00:10:48,710
still in the development stage you know

00:10:46,280 --> 00:10:50,960
as I mentioned it's difficult to predict

00:10:48,710 --> 00:10:53,570
the number of concurrent users and how

00:10:50,960 --> 00:10:57,380
they ramp in but you know it's really

00:10:53,570 --> 00:10:58,880
crucial so if this is the case what you

00:10:57,380 --> 00:11:01,379
want to do is you might want to consider

00:10:58,880 --> 00:11:04,139
splitting your script into four separate

00:11:01,379 --> 00:11:07,619
eases what this is doing is you're

00:11:04,139 --> 00:11:10,559
allowing for your scripts to capture the

00:11:07,619 --> 00:11:13,009
potential different types of load that

00:11:10,559 --> 00:11:14,819
would be hitting the application and

00:11:13,009 --> 00:11:17,369
depending on the size of your

00:11:14,819 --> 00:11:20,039
application will depend on how you to

00:11:17,369 --> 00:11:22,829
find these four separate phases but I'll

00:11:20,039 --> 00:11:24,419
and I'll explain to you what I thought

00:11:22,829 --> 00:11:28,199
was appropriate for the scale of our

00:11:24,419 --> 00:11:30,809
application so first you have um a

00:11:28,199 --> 00:11:33,689
warm-up phase and I defined this as the

00:11:30,809 --> 00:11:37,499
arrival rate of ten virtual users per

00:11:33,689 --> 00:11:39,959
second that lasted for 60 seconds then I

00:11:37,499 --> 00:11:42,179
defined a ramp up phase this is when it

00:11:39,959 --> 00:11:45,419
went from an arrival rate of ten to

00:11:42,179 --> 00:11:50,189
twenty five new virtual users per second

00:11:45,419 --> 00:11:52,709
for 120 seconds then I defined a cruise

00:11:50,189 --> 00:11:56,579
phase and this was the arrival rate of

00:11:52,709 --> 00:12:00,029
25 virtual users per second for twelve

00:11:56,579 --> 00:12:02,999
hundred seconds and then finally I

00:12:00,029 --> 00:12:04,859
defined a crash phase and this is really

00:12:02,999 --> 00:12:07,709
the concept of you know finding that

00:12:04,859 --> 00:12:09,899
limit I'm really pushing past it and

00:12:07,709 --> 00:12:12,569
through our application this was the

00:12:09,899 --> 00:12:16,049
arrival rate of a hundred virtual user

00:12:12,569 --> 00:12:19,649
per second for 30 seconds so it's a very

00:12:16,049 --> 00:12:21,749
short best of time but what's really

00:12:19,649 --> 00:12:24,629
important to note here is that there's

00:12:21,749 --> 00:12:27,269
no point hitting your application with a

00:12:24,629 --> 00:12:30,359
load you know that it can handle the

00:12:27,269 --> 00:12:33,539
purpose of this phase is to make the

00:12:30,359 --> 00:12:36,089
application crash what you want to do is

00:12:33,539 --> 00:12:38,699
you want to be able to understand the is

00:12:36,089 --> 00:12:41,159
that behavior of your application once

00:12:38,699 --> 00:12:43,079
you've pushed past the limit you want to

00:12:41,159 --> 00:12:46,559
be able to answer questions like you

00:12:43,079 --> 00:12:49,289
know will your application recover does

00:12:46,559 --> 00:12:51,179
it recover on its own will it require an

00:12:49,289 --> 00:12:53,549
engineer to come in late at night to

00:12:51,179 --> 00:12:55,979
make the fix and how long does that end

00:12:53,549 --> 00:12:57,869
up taking these are the kind of

00:12:55,979 --> 00:13:02,399
questions you really want to be able to

00:12:57,869 --> 00:13:04,349
answer from the crash phase and to

00:13:02,399 --> 00:13:06,329
visualize this what I've done is I've

00:13:04,349 --> 00:13:09,119
identified these four phases in a

00:13:06,329 --> 00:13:11,220
production cycle again it's a graph

00:13:09,119 --> 00:13:14,730
showing and the number of requests per

00:13:11,220 --> 00:13:17,610
second hitting the ftom homepage over

00:13:14,730 --> 00:13:21,149
money for our period and what you can

00:13:17,610 --> 00:13:23,730
see is from 12th or 12:30 a.m. to 6 a.m.

00:13:21,149 --> 00:13:25,680
there's this very low level of five

00:13:23,730 --> 00:13:28,769
requests per second and that's the

00:13:25,680 --> 00:13:31,260
warm-up phase then from 6 a.m. to 9 a.m.

00:13:28,769 --> 00:13:34,260
there's this gradual increase in the

00:13:31,260 --> 00:13:37,470
number of requests per second from 5 to

00:13:34,260 --> 00:13:40,079
11 and this is clearly during our users

00:13:37,470 --> 00:13:43,529
daily to me and what I've defined as the

00:13:40,079 --> 00:13:45,870
ramp up phase then from 9 a.m. to midday

00:13:43,529 --> 00:13:48,089
there's this constant rate of 11

00:13:45,870 --> 00:13:51,029
requests per second this is what I

00:13:48,089 --> 00:13:52,680
discusses the cruise phase and then what

00:13:51,029 --> 00:13:54,449
I've done is they've extrapolated the

00:13:52,680 --> 00:13:56,820
graph to what I think would be

00:13:54,449 --> 00:13:59,519
appropriate for the size of application

00:13:56,820 --> 00:14:04,560
for the crash phase and that's about 40

00:13:59,519 --> 00:14:06,449
requests per second so okay you made the

00:14:04,560 --> 00:14:08,790
sigils on the testing framework the type

00:14:06,449 --> 00:14:10,050
of requests and the shape of load you

00:14:08,790 --> 00:14:12,959
know you're ready to write your first

00:14:10,050 --> 00:14:16,230
spray so let's discuss how you can get

00:14:12,959 --> 00:14:18,510
started what should your first script

00:14:16,230 --> 00:14:21,750
look back so the script should be split

00:14:18,510 --> 00:14:24,449
into two separate sections first you

00:14:21,750 --> 00:14:26,399
have the config section and what you do

00:14:24,449 --> 00:14:28,800
is you start by choosing the target of

00:14:26,399 --> 00:14:31,800
your load test so the address of the API

00:14:28,800 --> 00:14:34,800
server under test for us it was the

00:14:31,800 --> 00:14:37,019
visits API staging endpoint but because

00:14:34,800 --> 00:14:40,230
if the staging environment was an exact

00:14:37,019 --> 00:14:42,480
replication of production it was a fair

00:14:40,230 --> 00:14:44,279
test and we could use the staging and be

00:14:42,480 --> 00:14:47,370
confident that it would produced similar

00:14:44,279 --> 00:14:50,790
results and then you can specify any

00:14:47,370 --> 00:14:52,769
plugins you'd like to use and we use a

00:14:50,790 --> 00:14:55,230
plugin that allowed us to send the

00:14:52,769 --> 00:14:57,569
output and of our load test to graphite

00:14:55,230 --> 00:14:59,910
and graph our know what graphite is in

00:14:57,569 --> 00:15:02,190
some metrics datastore angriff honors

00:14:59,910 --> 00:15:05,790
the presentation and data visualization

00:15:02,190 --> 00:15:09,089
layer and what this did was it really

00:15:05,790 --> 00:15:10,860
helped us monitor over time and create

00:15:09,089 --> 00:15:12,630
graphs that later helped identify

00:15:10,860 --> 00:15:15,329
performance issues and troubleshoot

00:15:12,630 --> 00:15:17,519
along the way and after you've done that

00:15:15,329 --> 00:15:21,029
what you do is you define the load

00:15:17,519 --> 00:15:22,890
phases and and this is what we discussed

00:15:21,029 --> 00:15:25,110
previously you know about the number of

00:15:22,890 --> 00:15:26,730
concurrent virtual users and the method

00:15:25,110 --> 00:15:29,880
of how they go about round

00:15:26,730 --> 00:15:32,730
again and then what you have is the

00:15:29,880 --> 00:15:35,040
processor this is where you import any

00:15:32,730 --> 00:15:38,040
JavaScript files that contain functions

00:15:35,040 --> 00:15:40,440
you want to use in your script and so

00:15:38,040 --> 00:15:42,300
above what you can see is a snapshot of

00:15:40,440 --> 00:15:46,380
essentially what the config section

00:15:42,300 --> 00:15:49,470
should look like after that you have the

00:15:46,380 --> 00:15:51,990
scenario section and this is where you

00:15:49,470 --> 00:15:54,690
identify what the virtual users

00:15:51,990 --> 00:15:56,550
behaviors going to be during the test so

00:15:54,690 --> 00:15:59,250
you define the type of request your

00:15:56,550 --> 00:16:02,580
users going to make so you start with

00:15:59,250 --> 00:16:04,560
naming the virtual user request in this

00:16:02,580 --> 00:16:07,530
case the user request was going to

00:16:04,560 --> 00:16:10,440
retrieve a list of all systems at the ft

00:16:07,530 --> 00:16:12,150
and their corresponding properties by

00:16:10,440 --> 00:16:12,990
adding a name and providing a

00:16:12,150 --> 00:16:14,700
description

00:16:12,990 --> 00:16:17,580
it's very quickly easy to identify

00:16:14,700 --> 00:16:21,990
during the low test how many times that

00:16:17,580 --> 00:16:24,090
query has wrapped and then what you do

00:16:21,990 --> 00:16:26,520
is you define the flow and that's the

00:16:24,090 --> 00:16:28,950
array of operations that a virtual user

00:16:26,520 --> 00:16:32,520
will perform during your script in your

00:16:28,950 --> 00:16:34,440
load test in the example here you can

00:16:32,520 --> 00:16:38,520
see what we're doing is making a and

00:16:34,440 --> 00:16:41,100
post request with a graph QL and Jason

00:16:38,520 --> 00:16:44,220
buddy that contains a graph QL query and

00:16:41,100 --> 00:16:46,200
to the route graph Q app and you can see

00:16:44,220 --> 00:16:48,570
here again I'm calling the set headers

00:16:46,200 --> 00:16:51,000
function I mentioned previously and that

00:16:48,570 --> 00:16:53,220
you don't have to just do one query this

00:16:51,000 --> 00:16:56,910
is a snapshot you can do multiple

00:16:53,220 --> 00:16:59,070
queries within the same script and it's

00:16:56,910 --> 00:17:01,350
also really important to note head that

00:16:59,070 --> 00:17:04,110
you know if you wanted to you could also

00:17:01,350 --> 00:17:06,030
and assign weights to the queries so

00:17:04,110 --> 00:17:09,329
what a weight does is it allows the

00:17:06,030 --> 00:17:11,130
probability for one query or probability

00:17:09,329 --> 00:17:13,880
of a scenario being picked

00:17:11,130 --> 00:17:18,120
iein you've actually user to be waived

00:17:13,880 --> 00:17:20,220
relative to other scenarios so you could

00:17:18,120 --> 00:17:23,339
assign a weight of two to one query and

00:17:20,220 --> 00:17:26,760
one to the other query and and it's as

00:17:23,339 --> 00:17:29,130
simple as that and once you're happy

00:17:26,760 --> 00:17:30,900
with the script and you've prepared the

00:17:29,130 --> 00:17:34,860
config and scenario section together

00:17:30,900 --> 00:17:36,840
you're ready to get and you can run the

00:17:34,860 --> 00:17:39,610
load test using the following command

00:17:36,840 --> 00:17:43,390
artillery brand developer and

00:17:39,610 --> 00:17:45,790
name of the fat okay cool so you've now

00:17:43,390 --> 00:17:48,580
run the scrap but you know what happens

00:17:45,790 --> 00:17:50,710
after you're on the script so what

00:17:48,580 --> 00:17:53,710
artillery will do is it provides you

00:17:50,710 --> 00:17:55,870
with detailed metrics and and that helps

00:17:53,710 --> 00:17:58,420
analyze the behavior of your application

00:17:55,870 --> 00:18:00,940
when it's under load so you have

00:17:58,420 --> 00:18:02,710
scenarios launch that's the number of

00:18:00,940 --> 00:18:05,380
virtual users that have launched a

00:18:02,710 --> 00:18:07,929
scenario throughout the test you've got

00:18:05,380 --> 00:18:10,059
scenarios completed that's a number of

00:18:07,929 --> 00:18:12,970
virtual users that have completed a

00:18:10,059 --> 00:18:15,700
scenario throughout the test you've got

00:18:12,970 --> 00:18:18,809
requests completed the total number of

00:18:15,700 --> 00:18:22,030
HTTP requests and responses sent

00:18:18,809 --> 00:18:24,130
requests per second the average number

00:18:22,030 --> 00:18:27,040
of requests per second completed

00:18:24,130 --> 00:18:29,770
throughout the test and status codes

00:18:27,040 --> 00:18:32,309
HTTP status code responses from each

00:18:29,770 --> 00:18:36,520
request that has been completed and

00:18:32,309 --> 00:18:38,919
finally request latency how long it

00:18:36,520 --> 00:18:41,320
takes a server to receive and process a

00:18:38,919 --> 00:18:45,340
request and that's measured in

00:18:41,320 --> 00:18:47,559
milliseconds so okay we understand now

00:18:45,340 --> 00:18:50,230
what the metrics mean like how do we

00:18:47,559 --> 00:18:53,980
actually use the metrics and to help

00:18:50,230 --> 00:18:55,870
identify a performance issue so above is

00:18:53,980 --> 00:18:59,049
an example of one of the early summary

00:18:55,870 --> 00:19:00,790
reports from our script and you can see

00:18:59,049 --> 00:19:03,910
here that we notice that the request

00:19:00,790 --> 00:19:06,760
latency started to grade rapidly during

00:19:03,910 --> 00:19:09,850
our load test it went from about sixty

00:19:06,760 --> 00:19:11,620
seven and point six milliseconds and

00:19:09,850 --> 00:19:14,710
then suddenly ramped up to exceeding

00:19:11,620 --> 00:19:17,500
past thirty thousand milliseconds and

00:19:14,710 --> 00:19:19,450
this was really concerning because we

00:19:17,500 --> 00:19:22,360
were only sending on average twenty

00:19:19,450 --> 00:19:24,280
requests per second as we were expecting

00:19:22,360 --> 00:19:27,250
our applications be able to handle a

00:19:24,280 --> 00:19:29,919
much higher level of requests per second

00:19:27,250 --> 00:19:32,470
and out of the six thousand scenarios

00:19:29,919 --> 00:19:34,929
launched and completed more than 90

00:19:32,470 --> 00:19:37,780
percent of requests received an HTTP

00:19:34,929 --> 00:19:40,240
status code five hundred indicating the

00:19:37,780 --> 00:19:42,960
server was aware of an error or simply

00:19:40,240 --> 00:19:46,030
incapable of performing the request or

00:19:42,960 --> 00:19:49,870
583 indicating it actually just can't

00:19:46,030 --> 00:19:51,810
hit the server at all and so the metrics

00:19:49,870 --> 00:19:54,360
about response times three

00:19:51,810 --> 00:19:56,640
rates status codes helped identify a

00:19:54,360 --> 00:19:58,830
serious performance issue early on in

00:19:56,640 --> 00:20:00,690
our development process that could

00:19:58,830 --> 00:20:03,870
essentially just stop our application

00:20:00,690 --> 00:20:06,690
from running so we've got this

00:20:03,870 --> 00:20:08,910
performance issue how what do we do next

00:20:06,690 --> 00:20:12,330
how do we find the culprit

00:20:08,910 --> 00:20:14,640
so we round several tasks singling out

00:20:12,330 --> 00:20:17,910
each query and sending the metrics to

00:20:14,640 --> 00:20:20,280
graph a in graph on ax and after a

00:20:17,910 --> 00:20:24,450
couple of attempts we had found the

00:20:20,280 --> 00:20:27,150
culprit it was this graph QL query which

00:20:24,450 --> 00:20:29,790
was trying to retrieve the fast 600

00:20:27,150 --> 00:20:33,750
systems that had a Bruns Service tear at

00:20:29,790 --> 00:20:35,930
the Assateague in fact what we realized

00:20:33,750 --> 00:20:39,060
was we were reaching a hundred percent

00:20:35,930 --> 00:20:41,640
CPU when running this query and it was

00:20:39,060 --> 00:20:43,950
at this very low level of 10 requests

00:20:41,640 --> 00:20:47,610
per second the fact that we were

00:20:43,950 --> 00:20:49,320
reaching this 100% CPU utilization with

00:20:47,610 --> 00:20:51,990
this very low level of requests per

00:20:49,320 --> 00:20:54,410
second led us to believe that you know

00:20:51,990 --> 00:20:57,150
perhaps it could be an application issue

00:20:54,410 --> 00:20:59,460
we wanted to determine whether something

00:20:57,150 --> 00:21:03,450
in our code base that was causing the

00:20:59,460 --> 00:21:06,120
CPU to max out so a good starting point

00:21:03,450 --> 00:21:09,660
was to generate a flim-flam graph in

00:21:06,120 --> 00:21:11,250
order to profile the CPU usage and what

00:21:09,660 --> 00:21:15,150
we did was we use this plugin called

00:21:11,250 --> 00:21:17,430
zero acts and with a fair flame you can

00:21:15,150 --> 00:21:20,040
install it using NPM which is nice and

00:21:17,430 --> 00:21:22,620
quick and with a second command above

00:21:20,040 --> 00:21:24,960
what it does is it will generate the

00:21:22,620 --> 00:21:27,300
flame graph for you and it will in fact

00:21:24,960 --> 00:21:30,200
open it in a separate browser so you

00:21:27,300 --> 00:21:33,750
know it's really quick to get started

00:21:30,200 --> 00:21:37,080
and above as the flame graph we

00:21:33,750 --> 00:21:39,540
generated and with a flame grass all the

00:21:37,080 --> 00:21:42,270
data and is on the screen at the same

00:21:39,540 --> 00:21:44,220
time and the hottest code paths are

00:21:42,270 --> 00:21:47,310
immediately obvious as the widest

00:21:44,220 --> 00:21:49,920
functions annoyingly though there was

00:21:47,310 --> 00:21:52,320
nothing unusual being displayed there

00:21:49,920 --> 00:21:54,510
was no one single line of code that was

00:21:52,320 --> 00:21:57,090
using at all of the CPU that we could

00:21:54,510 --> 00:21:59,220
just go in and fix so we really needed

00:21:57,090 --> 00:22:01,020
to think about you know what else could

00:21:59,220 --> 00:22:04,920
essentially be causing that performance

00:22:01,020 --> 00:22:07,860
issue and so our database

00:22:04,920 --> 00:22:11,100
new forge a instance run by graphene DB

00:22:07,860 --> 00:22:13,650
and when a user meets a query using

00:22:11,100 --> 00:22:16,920
graphs cue out what happens as it hits

00:22:13,650 --> 00:22:20,280
an execution layer in our cases NPM

00:22:16,920 --> 00:22:23,880
module called neo4j graph QL J ass and

00:22:20,280 --> 00:22:27,300
that translates the graph QL query into

00:22:23,880 --> 00:22:29,550
a cipher query so this was the query in

00:22:27,300 --> 00:22:32,040
graph QL that was causing a performance

00:22:29,550 --> 00:22:35,340
issue and you can see that it gets

00:22:32,040 --> 00:22:37,860
translated into this cipher query so

00:22:35,340 --> 00:22:40,230
what happens is it translates it into

00:22:37,860 --> 00:22:42,030
the cipher query and then it would send

00:22:40,230 --> 00:22:44,760
the request directly to our neo4j

00:22:42,030 --> 00:22:47,040
instance so we decided to look into the

00:22:44,760 --> 00:22:48,600
configuration of this NPM module and see

00:22:47,040 --> 00:22:51,300
whether we could identify a performance

00:22:48,600 --> 00:22:53,340
issue that and what we noticed was that

00:22:51,300 --> 00:22:55,890
the driver had a maximum pool size of a

00:22:53,340 --> 00:22:58,470
hundred connections so this meant that

00:22:55,890 --> 00:23:01,080
if a session and try to acquire a

00:22:58,470 --> 00:23:03,000
connection with the pools at capacity

00:23:01,080 --> 00:23:05,430
you'd have to wait until a free

00:23:03,000 --> 00:23:08,580
connection was available or you know we

00:23:05,430 --> 00:23:11,280
would see an HTTP status code 503 which

00:23:08,580 --> 00:23:13,680
we had seen in our load test so you know

00:23:11,280 --> 00:23:16,860
we increase the pool size we decrease

00:23:13,680 --> 00:23:19,080
the retry timeout but unfortunately we

00:23:16,860 --> 00:23:20,850
weren't hitting the capacity of the pool

00:23:19,080 --> 00:23:24,930
size and so increase in the pool size

00:23:20,850 --> 00:23:26,580
had no impact on the performance so what

00:23:24,930 --> 00:23:29,190
we did next was we looked into the

00:23:26,580 --> 00:23:32,730
cipher query and so for tuning more

00:23:29,190 --> 00:23:35,610
specifically with neo4j what you can do

00:23:32,730 --> 00:23:39,140
is you can see how a particular query

00:23:35,610 --> 00:23:41,580
performance by prefixing it with profile

00:23:39,140 --> 00:23:43,890
so we round profile in the particular

00:23:41,580 --> 00:23:46,860
query that was causing a problem and

00:23:43,890 --> 00:23:49,110
this was the output what we're really

00:23:46,860 --> 00:23:51,450
interested to see here is the total

00:23:49,110 --> 00:23:55,140
number of database hits from this query

00:23:51,450 --> 00:23:57,900
and that was approximately 14,000 what

00:23:55,140 --> 00:24:00,480
the objective of query query profiling

00:23:57,900 --> 00:24:02,280
is is to find a way in which you can

00:24:00,480 --> 00:24:04,950
reduce the number of database hits

00:24:02,280 --> 00:24:08,790
required to perform the task whilst

00:24:04,950 --> 00:24:11,280
keeping the output level constant so

00:24:08,790 --> 00:24:13,530
what we did was we created index on

00:24:11,280 --> 00:24:15,090
service tear with the hope that you know

00:24:13,530 --> 00:24:17,400
that would improve the performance of

00:24:15,090 --> 00:24:19,740
the query I create

00:24:17,400 --> 00:24:22,650
and in debts you reduce those number of

00:24:19,740 --> 00:24:24,810
database heads and we're on profile

00:24:22,650 --> 00:24:27,120
again and it worked it did significantly

00:24:24,810 --> 00:24:29,250
reduce the number of database head

00:24:27,120 --> 00:24:31,680
swells keeping the output constant

00:24:29,250 --> 00:24:34,110
however it didn't resolve the

00:24:31,680 --> 00:24:37,770
performance issue in fact it had no

00:24:34,110 --> 00:24:39,630
impact so what we did finally was we

00:24:37,770 --> 00:24:41,790
looked at the configuration of MU for J

00:24:39,630 --> 00:24:44,100
instance what was the size of the

00:24:41,790 --> 00:24:46,920
instance did we have the correct round

00:24:44,100 --> 00:24:47,550
size and how many caused we have on the

00:24:46,920 --> 00:24:50,370
server

00:24:47,550 --> 00:24:53,580
was it the correct amount of course um

00:24:50,370 --> 00:24:56,310
so what we did was we ran system info in

00:24:53,580 --> 00:24:58,290
the new 4G browser and this provided us

00:24:56,310 --> 00:25:02,970
with information about the size of our

00:24:58,290 --> 00:25:06,240
database from this we use the new for J

00:25:02,970 --> 00:25:08,820
Hardware sizing calculator and what it

00:25:06,240 --> 00:25:11,220
recommended was to increase the size of

00:25:08,820 --> 00:25:13,830
total ram increase the number of cores

00:25:11,220 --> 00:25:17,280
on our server and you know that could

00:25:13,830 --> 00:25:18,840
explain why the CPU was maxing out so

00:25:17,280 --> 00:25:21,090
that's what we did we elected there

00:25:18,840 --> 00:25:24,870
different plans improved our plan to

00:25:21,090 --> 00:25:27,450
performance one and this had eight

00:25:24,870 --> 00:25:29,130
gigabytes of RAM two cores on the server

00:25:27,450 --> 00:25:31,410
so we were kind of convinced you know

00:25:29,130 --> 00:25:33,990
this will definitely have an impact and

00:25:31,410 --> 00:25:35,670
improve the overall performance so you

00:25:33,990 --> 00:25:39,510
know we made the change we ran the

00:25:35,670 --> 00:25:41,340
scripts again confusingly even from

00:25:39,510 --> 00:25:43,710
moving from a shared resource to a

00:25:41,340 --> 00:25:46,710
dedicated resource and increasing the

00:25:43,710 --> 00:25:49,260
RAM size and the number of course it

00:25:46,710 --> 00:25:51,840
didn't help them improve the performance

00:25:49,260 --> 00:25:56,070
in fact it actually made the performance

00:25:51,840 --> 00:25:57,870
worse and so then you know we thought

00:25:56,070 --> 00:26:01,470
okay we've kind of thought of a lot of

00:25:57,870 --> 00:26:04,140
different techniques perhaps you know it

00:26:01,470 --> 00:26:05,820
could be a network input output issue so

00:26:04,140 --> 00:26:09,150
you know when the database results are

00:26:05,820 --> 00:26:11,400
queued for processing by CPU if the MPA

00:26:09,150 --> 00:26:13,410
and outputs limited you know the queue

00:26:11,400 --> 00:26:16,340
of database results will back up

00:26:13,410 --> 00:26:20,430
potentially causing a struggling CPA and

00:26:16,340 --> 00:26:22,800
so we were gonna look into the snacks so

00:26:20,430 --> 00:26:24,420
you're probably really thinking okay

00:26:22,800 --> 00:26:26,100
come on already like you've spoken about

00:26:24,420 --> 00:26:28,620
all of these troubleshooting techniques

00:26:26,100 --> 00:26:30,340
like you know what was the effects like

00:26:28,620 --> 00:26:33,880
what did you do to resolve the

00:26:30,340 --> 00:26:36,640
performance issues and unfortunately

00:26:33,880 --> 00:26:43,170
from one day to the next the performance

00:26:36,640 --> 00:26:46,360
issue just disappeared yeah I know

00:26:43,170 --> 00:26:50,020
engineering we were all of a sudden able

00:26:46,360 --> 00:26:53,230
to make 25 requests per second without

00:26:50,020 --> 00:26:55,450
the application crash a so for this

00:26:53,230 --> 00:26:57,490
particular example we were actually

00:26:55,450 --> 00:27:01,630
unable to find the solution to this

00:26:57,490 --> 00:27:03,820
problem so you're probably thinking why

00:27:01,630 --> 00:27:05,650
have you shared this particular example

00:27:03,820 --> 00:27:08,130
with me not one where you know you've

00:27:05,650 --> 00:27:11,680
significantly improved performance and

00:27:08,130 --> 00:27:13,540
um the reason is I I believe that what

00:27:11,680 --> 00:27:15,220
I've learned is really invaluable and I

00:27:13,540 --> 00:27:17,560
want to be able to share that part with

00:27:15,220 --> 00:27:19,630
you you know I've been able to build a

00:27:17,560 --> 00:27:22,120
personal toolbox with tools and

00:27:19,630 --> 00:27:25,570
techniques that will help me eradicate

00:27:22,120 --> 00:27:27,850
and identify future culprits but also

00:27:25,570 --> 00:27:33,850
that you know there were practical

00:27:27,850 --> 00:27:37,090
outcomes to this first you know we were

00:27:33,850 --> 00:27:39,280
able to measure assess and improve the

00:27:37,090 --> 00:27:43,060
overall performance of our application

00:27:39,280 --> 00:27:45,580
even if it was very slightly and by load

00:27:43,060 --> 00:27:48,340
testing you know we were able to detect

00:27:45,580 --> 00:27:51,430
a performance issue early on and as a

00:27:48,340 --> 00:27:53,950
result dedicate time to help improve the

00:27:51,430 --> 00:27:57,810
performance of the application before it

00:27:53,950 --> 00:28:01,030
went into production we knew the limits

00:27:57,810 --> 00:28:04,270
we were also able to assess the

00:28:01,030 --> 00:28:07,120
reliability of our application by load

00:28:04,270 --> 00:28:10,270
testing we increased our confidence that

00:28:07,120 --> 00:28:13,210
our application could stay online given

00:28:10,270 --> 00:28:15,340
a certain level of traffic and we knew

00:28:13,210 --> 00:28:17,890
exactly at what point we would need to

00:28:15,340 --> 00:28:20,310
consider scaling the application in

00:28:17,890 --> 00:28:23,950
order for it to stay or not

00:28:20,310 --> 00:28:25,900
so to conclude what I'd like to say is

00:28:23,950 --> 00:28:28,120
if if you really want to fully

00:28:25,900 --> 00:28:30,640
understand the performance and

00:28:28,120 --> 00:28:32,590
reliability of what you're building you

00:28:30,640 --> 00:28:35,290
should be considering load testing your

00:28:32,590 --> 00:28:37,690
application and it doesn't just have to

00:28:35,290 --> 00:28:41,440
be in production you should start from

00:28:37,690 --> 00:28:42,100
the development stage so thank you for

00:28:41,440 --> 00:28:44,500
listening

00:28:42,100 --> 00:28:47,950
and and we're also hiring so either

00:28:44,500 --> 00:28:50,240
visit this URL or and just come and chat

00:28:47,950 --> 00:28:53,170
to me after thank you

00:28:50,240 --> 00:28:58,279
[Applause]

00:28:53,170 --> 00:28:58,279

YouTube URL: https://www.youtube.com/watch?v=GT0V1SucScs


