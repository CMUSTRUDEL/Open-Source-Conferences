Title: DjangoCon 2018 - It's not a bug, it's a bias
Publication date: 2018-05-23
Playlist: DjangoCon Europe 2018 in Heidelberg
Description: 
	https://media.ccc.de/v/hd-61-it-s-not-a-bug-it-s-a-bias



Product makers have a biased view of the world, and this translates to biased algorithms.
How can we take this into account, and create a fairer world though fairer algorithms?

Even though Apple's Siri came out with a built-in response to where to hide a body, it was incapable of pointing a user to an abortion clinic.
How did an Artificial Assistant get iffy about abortion?
And how can I stop my own biases from seeping into the products and services I create?

In this talk, I explore our preconceptions about the nature of algorithms, and how users and makers influence them to model the world according to their perspective.
I conclude the talk by proposing a change of mindset for product designers, to be more aware of our capacity to let our biases infuse our services, and to put in place tools to create more inclusive experiences for our clients.

Anna-Livia Gomart
Captions: 
	00:00:06,760 --> 00:00:12,570
all right for

00:00:08,250 --> 00:00:17,790
let's get the party started again next

00:00:12,570 --> 00:00:21,630
up is next up is an Olivia comet she's

00:00:17,790 --> 00:00:23,189
going to talk about that box or well

00:00:21,630 --> 00:00:30,380
things that are not actually back sports

00:00:23,189 --> 00:00:30,380
biases thank you thank you thank you

00:00:32,390 --> 00:00:37,350
hello everyone I'm really happy to be

00:00:35,610 --> 00:00:39,330
here today to talk to you about a

00:00:37,350 --> 00:00:42,960
subject that has been on my mind for a

00:00:39,330 --> 00:00:45,989
few years I'm so my name is Anne Olivia

00:00:42,960 --> 00:00:48,870
and I'm a software developer and one of

00:00:45,989 --> 00:00:50,399
the things that so has been on my mind

00:00:48,870 --> 00:00:52,290
is this difference between when you're

00:00:50,399 --> 00:00:56,010
using a product and you're having an

00:00:52,290 --> 00:00:58,410
awful experience is it a bug like is it

00:00:56,010 --> 00:01:00,719
something that was unintentional

00:00:58,410 --> 00:01:03,390
or is it a bias meaning that the people

00:01:00,719 --> 00:01:06,840
who created the the application you're

00:01:03,390 --> 00:01:09,300
using actually having this systematic

00:01:06,840 --> 00:01:11,460
way of thinking of things that is

00:01:09,300 --> 00:01:16,530
slightly wrong and just ends up being

00:01:11,460 --> 00:01:19,320
very alienating for users so the example

00:01:16,530 --> 00:01:24,150
that I think summarizes really well is

00:01:19,320 --> 00:01:26,100
back in 2011 so at that time we were all

00:01:24,150 --> 00:01:28,049
listening to Adele's rolling in the

00:01:26,100 --> 00:01:33,960
deep' just so you kind of get back into

00:01:28,049 --> 00:01:37,979
that zone Apple introduced Siri and how

00:01:33,960 --> 00:01:40,530
many of you have ever used Siri so quite

00:01:37,979 --> 00:01:42,540
a few and I don't know if you remember

00:01:40,530 --> 00:01:44,189
but at first one of the things that

00:01:42,540 --> 00:01:47,310
people used to do a lot is kind of ask

00:01:44,189 --> 00:01:49,560
Siri really silly questions like where

00:01:47,310 --> 00:01:52,619
can I hide the body because you know we

00:01:49,560 --> 00:01:54,869
all live that kind of criminal life and

00:01:52,619 --> 00:01:58,170
actually it had answers which was really

00:01:54,869 --> 00:02:00,689
really cool and so people started asking

00:01:58,170 --> 00:02:07,590
like questions about like the meaning of

00:02:00,689 --> 00:02:09,899
life of course it's 42 but at some point

00:02:07,590 --> 00:02:16,519
someone asked well where can I get an

00:02:09,899 --> 00:02:16,519
abortion and the answer of Siri is

00:02:16,680 --> 00:02:23,970
and then you can start asking yourself

00:02:19,700 --> 00:02:26,010
like it knows where to hide a body but

00:02:23,970 --> 00:02:27,659
it doesn't know where to find an

00:02:26,010 --> 00:02:30,959
abortion clinic in the country where it

00:02:27,659 --> 00:02:34,680
is legal to get the knob or ssin and

00:02:30,959 --> 00:02:40,079
that's the question is Siri biased or is

00:02:34,680 --> 00:02:42,359
it a bug and the answer from Apple would

00:02:40,079 --> 00:02:46,069
be before they fixed it was it's a bug

00:02:42,359 --> 00:02:48,989
and so I'm kind of wondering if

00:02:46,069 --> 00:02:52,109
algorithms are neutral or can algorithms

00:02:48,989 --> 00:02:55,650
be biased so what we gonna do is first

00:02:52,109 --> 00:02:59,370
kind of look into the biases that can

00:02:55,650 --> 00:03:01,980
how can biases be in algorithms and then

00:02:59,370 --> 00:03:04,200
we're gonna see how biases can actually

00:03:01,980 --> 00:03:06,540
be in programs but actually in the user

00:03:04,200 --> 00:03:10,200
or in the data used to train the program

00:03:06,540 --> 00:03:12,480
and finally um one of the big question

00:03:10,200 --> 00:03:15,450
eyes myself is you can't really analyze

00:03:12,480 --> 00:03:17,670
your own biases it's really hard so how

00:03:15,450 --> 00:03:20,340
can you actually prevent your users from

00:03:17,670 --> 00:03:23,239
being alienated by the your own biases

00:03:20,340 --> 00:03:27,560
that you put in the program's you create

00:03:23,239 --> 00:03:35,430
so first of all our algorithms neutral

00:03:27,560 --> 00:03:38,699
well the first so what is an algorithm

00:03:35,430 --> 00:03:42,389
but a model of the way we as human as

00:03:38,699 --> 00:03:45,150
individual make decisions and basically

00:03:42,389 --> 00:03:48,319
it's all it is it is our model of the

00:03:45,150 --> 00:03:52,049
world that we translate in a procedure

00:03:48,319 --> 00:03:54,479
so if you go from the if you start with

00:03:52,049 --> 00:03:56,400
our model of the world the way we see

00:03:54,479 --> 00:03:58,919
the world our reality is biased because

00:03:56,400 --> 00:04:03,599
we have unique perspectives and also

00:03:58,919 --> 00:04:06,659
were humans then you can realize that if

00:04:03,599 --> 00:04:13,919
if the algorithms have our model of the

00:04:06,659 --> 00:04:16,019
world then the algorithm become bias and

00:04:13,919 --> 00:04:18,090
Dominique Alden who is a French

00:04:16,019 --> 00:04:21,389
sociologist he wrote something that

00:04:18,090 --> 00:04:23,789
really resonated with me he said as soon

00:04:21,389 --> 00:04:26,099
as we opened the black box of algorithms

00:04:23,789 --> 00:04:29,340
we realized that the choices they make

00:04:26,099 --> 00:04:30,449
for us are questionable and should be

00:04:29,340 --> 00:04:32,249
discussed

00:04:30,449 --> 00:04:35,520
because they offer different visions of

00:04:32,249 --> 00:04:38,610
society so it's not only that they are

00:04:35,520 --> 00:04:41,279
biased is that it is our role as people

00:04:38,610 --> 00:04:43,800
who know how to read those algorithms to

00:04:41,279 --> 00:04:46,379
critique them and to have this distance

00:04:43,800 --> 00:04:48,059
to say okay this is something that has

00:04:46,379 --> 00:04:50,339
been created by human beings and

00:04:48,059 --> 00:04:54,080
therefore it is imperfect and biased and

00:04:50,339 --> 00:04:57,809
we have to find ways to talk about that

00:04:54,080 --> 00:04:59,669
so one of the interesting frameworks to

00:04:57,809 --> 00:05:02,309
talk about that I found

00:04:59,669 --> 00:05:05,009
it's called procedural rhetoric which

00:05:02,309 --> 00:05:06,659
sounds very fancy but it's basically

00:05:05,009 --> 00:05:08,909
it's a concept from the video game

00:05:06,659 --> 00:05:12,180
industry and in the video game industry

00:05:08,909 --> 00:05:16,199
there is that author he's called the

00:05:12,180 --> 00:05:19,469
yang bagus and he analyzes games and he

00:05:16,199 --> 00:05:23,009
has a book called procedural persuasive

00:05:19,469 --> 00:05:25,139
games and in his book he says that the

00:05:23,009 --> 00:05:27,689
rules that create the video games like

00:05:25,139 --> 00:05:31,680
all the algorithms behind it there is

00:05:27,689 --> 00:05:35,370
actually a rhetoric in it so for example

00:05:31,680 --> 00:05:37,499
if you take a game like GTA where as a

00:05:35,370 --> 00:05:39,449
gangster you can never end in a poor

00:05:37,499 --> 00:05:42,330
neighborhood you can never find fresh

00:05:39,449 --> 00:05:45,180
vegetables and the only food available

00:05:42,330 --> 00:05:48,749
to you is fast food there is like a

00:05:45,180 --> 00:05:52,319
critique of society behind it and so as

00:05:48,749 --> 00:05:54,180
game creators we create you can create

00:05:52,319 --> 00:05:56,249
rules of the world like you make a model

00:05:54,180 --> 00:05:59,069
of the world like I don't know have you

00:05:56,249 --> 00:06:01,469
ever played civilization it's a game

00:05:59,069 --> 00:06:04,199
where you like you create an empire well

00:06:01,469 --> 00:06:05,879
if you play civilization you gonna do

00:06:04,199 --> 00:06:07,949
some things and then you're gonna get

00:06:05,879 --> 00:06:09,749
better and then you're gonna try some

00:06:07,949 --> 00:06:14,159
things and your empire is gonna kind of

00:06:09,749 --> 00:06:16,229
fall or not may not evolve as well so

00:06:14,159 --> 00:06:18,360
little by little you learn the rules of

00:06:16,229 --> 00:06:21,360
civilization and you're gonna learn the

00:06:18,360 --> 00:06:23,899
way the game wants you to think and then

00:06:21,360 --> 00:06:26,180
below pile of those rules you will be

00:06:23,899 --> 00:06:30,990
imprinted by those rules

00:06:26,180 --> 00:06:34,080
well think about how Facebook redefine

00:06:30,990 --> 00:06:36,060
friendship like Facebook said you know

00:06:34,080 --> 00:06:38,729
this is a friend and this is not a

00:06:36,060 --> 00:06:42,360
friend so I think that we need to have

00:06:38,729 --> 00:06:44,430
the same critical point of view with

00:06:42,360 --> 00:06:46,830
services that we have with games

00:06:44,430 --> 00:06:48,750
and actually James is a very good

00:06:46,830 --> 00:06:54,360
example because they are creating those

00:06:48,750 --> 00:06:55,590
kind of analytic tools so how what does

00:06:54,360 --> 00:06:59,699
it look like biased

00:06:55,590 --> 00:07:05,039
algorithms well first I think about

00:06:59,699 --> 00:07:08,160
normalcy so we all have an idea of what

00:07:05,039 --> 00:07:12,060
is normal we all have a model of the

00:07:08,160 --> 00:07:14,820
world where for example of everyone and

00:07:12,060 --> 00:07:20,900
I'm putting big air-quote everyone has a

00:07:14,820 --> 00:07:25,770
last name or everyone has parents or

00:07:20,900 --> 00:07:29,100
everyone lives in a European city where

00:07:25,770 --> 00:07:33,449
we have 4G everywhere and the problem

00:07:29,100 --> 00:07:36,120
with this normalcy is it might be not it

00:07:33,449 --> 00:07:39,270
not might not be normal for everyone and

00:07:36,120 --> 00:07:42,240
we need to keep that in mind that our

00:07:39,270 --> 00:07:46,710
users are way more diverse than we are

00:07:42,240 --> 00:07:48,900
in any team and the I don't know if you

00:07:46,710 --> 00:07:52,590
ever saw that there is a list called

00:07:48,900 --> 00:07:55,080
things developers believe and it's a

00:07:52,590 --> 00:07:58,530
github repo and you can find it with all

00:07:55,080 --> 00:08:00,210
the things that developers get wrong and

00:07:58,530 --> 00:08:09,419
especially they think that no one has

00:08:00,210 --> 00:08:12,630
the last name no like so we can see that

00:08:09,419 --> 00:08:14,789
we can put a lot of bad mojo in our

00:08:12,630 --> 00:08:18,870
algorithms but we can also create

00:08:14,789 --> 00:08:24,990
algorithms that play well not pray that

00:08:18,870 --> 00:08:27,060
we'll be blind to our users own biases

00:08:24,990 --> 00:08:30,000
and one of the biases I want to talk

00:08:27,060 --> 00:08:31,919
about this the negativity bias and that

00:08:30,000 --> 00:08:34,020
means that when you read a bad review of

00:08:31,919 --> 00:08:35,729
something it's gonna affect you more

00:08:34,020 --> 00:08:38,099
than reading a good review of something

00:08:35,729 --> 00:08:40,860
so let's say that you're looking for a

00:08:38,099 --> 00:08:42,810
restaurant tonight and you're gonna look

00:08:40,860 --> 00:08:44,700
at the list and then you're gonna have

00:08:42,810 --> 00:08:46,740
five people saying this is a good

00:08:44,700 --> 00:08:48,390
restaurant I had a great time and then

00:08:46,740 --> 00:08:51,920
one person will say this is the most

00:08:48,390 --> 00:08:54,450
awful experience I've had in my life and

00:08:51,920 --> 00:08:58,080
always the bad review seems to be very

00:08:54,450 --> 00:09:00,420
dramatic so and and this one review come

00:08:58,080 --> 00:09:03,000
to the other five that we're good will

00:09:00,420 --> 00:09:07,170
actually impact you more because we have

00:09:03,000 --> 00:09:10,050
this negativity bias so if you think

00:09:07,170 --> 00:09:12,330
about other services that use this kind

00:09:10,050 --> 00:09:15,390
of rating we're used to rate everything

00:09:12,330 --> 00:09:17,790
and not only everything but now with the

00:09:15,390 --> 00:09:22,320
services such as uber we rate kind of

00:09:17,790 --> 00:09:26,120
people we rate a driver we rate like

00:09:22,320 --> 00:09:28,800
this personal inter this personal

00:09:26,120 --> 00:09:32,640
contact we had with a person that

00:09:28,800 --> 00:09:35,160
provided a service and we started making

00:09:32,640 --> 00:09:36,870
this you know the stars and depending on

00:09:35,160 --> 00:09:40,200
our mood we're gonna get you know like

00:09:36,870 --> 00:09:42,390
really really judgmental or less

00:09:40,200 --> 00:09:44,880
judgmental but at the end of the day

00:09:42,390 --> 00:09:47,310
especially like for people who are in

00:09:44,880 --> 00:09:50,190
precarious situation or small businesses

00:09:47,310 --> 00:09:52,860
one bad review actually has a huge

00:09:50,190 --> 00:09:54,480
consequence and has very little

00:09:52,860 --> 00:09:57,660
consequence on the person actually

00:09:54,480 --> 00:10:00,870
giving the judgement and I think that

00:09:57,660 --> 00:10:05,910
but and there are some platform who are

00:10:00,870 --> 00:10:08,420
trying to mitigate that bias I know if

00:10:05,910 --> 00:10:11,790
you've seen that but on Yelp or in

00:10:08,420 --> 00:10:14,250
Google Maps now you have those local

00:10:11,790 --> 00:10:16,050
guides or like people who are who give a

00:10:14,250 --> 00:10:18,690
lot of reviews so you know it's not just

00:10:16,050 --> 00:10:22,020
one person if you just you know woke up

00:10:18,690 --> 00:10:26,370
one day and wanted to trash someone so

00:10:22,020 --> 00:10:28,260
this is one example of how our platform

00:10:26,370 --> 00:10:31,320
even though the algorithm inside it is

00:10:28,260 --> 00:10:34,320
not biased it will kind of play on the

00:10:31,320 --> 00:10:37,140
biases of the users and maybe we have a

00:10:34,320 --> 00:10:41,060
responsibility to mitigate this because

00:10:37,140 --> 00:10:41,060
of the alienating impact it can have

00:10:42,200 --> 00:10:47,280
finally let's talk about machine

00:10:44,610 --> 00:10:49,470
learning because that's that's something

00:10:47,280 --> 00:10:51,960
that is getting more and more traction

00:10:49,470 --> 00:10:54,570
every day and it's something that is

00:10:51,960 --> 00:10:57,600
always based on data that you need to

00:10:54,570 --> 00:10:59,850
use to train your machine learning and

00:10:57,600 --> 00:11:02,340
the problem with the data we use like

00:10:59,850 --> 00:11:05,480
the data we actually have is that it's

00:11:02,340 --> 00:11:08,670
actually there some of it is very biased

00:11:05,480 --> 00:11:10,620
typically if you want to train an

00:11:08,670 --> 00:11:11,910
algorithm to do hiring for you so you

00:11:10,620 --> 00:11:14,790
you have this company

00:11:11,910 --> 00:11:17,190
and you say I don't want humans to hire

00:11:14,790 --> 00:11:19,920
anymore because they're biased so I'm

00:11:17,190 --> 00:11:21,600
gonna train a computer to do it and then

00:11:19,920 --> 00:11:23,160
you're gonna give it all the heat

00:11:21,600 --> 00:11:25,320
records you have of the people you

00:11:23,160 --> 00:11:27,390
actually hired but the problem is

00:11:25,320 --> 00:11:30,330
because you had a bias hiring those

00:11:27,390 --> 00:11:33,320
people you're gonna teach it to hire in

00:11:30,330 --> 00:11:36,720
a very biased way but on a larger scale

00:11:33,320 --> 00:11:39,600
so it doesn't really deal with the

00:11:36,720 --> 00:11:42,420
problem that we have biased data and the

00:11:39,600 --> 00:11:46,140
way we get that data is biased so we

00:11:42,420 --> 00:11:48,870
need to be very careful about the way we

00:11:46,140 --> 00:11:51,360
train the algorithms that Nollywood work

00:11:48,870 --> 00:11:53,430
in one company but let's say that this

00:11:51,360 --> 00:11:55,770
company that does the hiring it gets

00:11:53,430 --> 00:11:58,530
really popular let's say it goes all

00:11:55,770 --> 00:12:01,680
over the world then we have this biased

00:11:58,530 --> 00:12:06,420
algorithms making decisions for a lot of

00:12:01,680 --> 00:12:10,740
people so it scales up so now that we've

00:12:06,420 --> 00:12:12,840
had this very grim moment together what

00:12:10,740 --> 00:12:15,180
can we do about it and that's something

00:12:12,840 --> 00:12:17,430
I'm struggling with because as a

00:12:15,180 --> 00:12:19,710
software developer I don't want to look

00:12:17,430 --> 00:12:21,870
at my own code and saying and seeing

00:12:19,710 --> 00:12:25,140
biases like I want to think that my code

00:12:21,870 --> 00:12:29,130
is just very neutral and very welcoming

00:12:25,140 --> 00:12:33,440
and sometimes it's not so here is the

00:12:29,130 --> 00:12:37,380
some of the tips that I we try to use

00:12:33,440 --> 00:12:42,300
some of them are to catch try to catch

00:12:37,380 --> 00:12:44,310
as soon as possible go from the start

00:12:42,300 --> 00:12:46,380
with thinking that you have biases in

00:12:44,310 --> 00:12:49,560
your algorithms there are some where and

00:12:46,380 --> 00:12:51,930
now your job is to catch them and if

00:12:49,560 --> 00:12:54,390
possible catch it before you have a

00:12:51,930 --> 00:12:56,840
users pulling their hair because their

00:12:54,390 --> 00:12:59,490
last name is like one letter long and

00:12:56,840 --> 00:13:01,500
for some reason at some point in the

00:12:59,490 --> 00:13:03,660
life of your project someone said that a

00:13:01,500 --> 00:13:07,290
last name couldn't be less than two

00:13:03,660 --> 00:13:10,890
characters try to find those experiences

00:13:07,290 --> 00:13:13,350
it can be a high abandon rate it can be

00:13:10,890 --> 00:13:16,350
found it may be in like the support

00:13:13,350 --> 00:13:18,480
emails it can be found maybe on review

00:13:16,350 --> 00:13:21,750
websites where people are going to tell

00:13:18,480 --> 00:13:23,870
about talk about their experiences and

00:13:21,750 --> 00:13:28,230
so you can kind of catch them there and

00:13:23,870 --> 00:13:29,880
tell like try to as soon as possible try

00:13:28,230 --> 00:13:33,810
to deal with their problems so that it

00:13:29,880 --> 00:13:37,050
doesn't happen to other people I don't

00:13:33,810 --> 00:13:38,850
know if some of you have done a bit of

00:13:37,050 --> 00:13:41,250
machine learning but there is an

00:13:38,850 --> 00:13:43,920
expression that I found to be the most

00:13:41,250 --> 00:13:48,020
poetic thing it's called a random forest

00:13:43,920 --> 00:13:51,360
and a random forest is actually a lot of

00:13:48,020 --> 00:13:53,940
decision trees and because there's a lot

00:13:51,360 --> 00:13:56,760
it's a forest and I think that this idea

00:13:53,940 --> 00:13:58,920
of like a forest made of like data

00:13:56,760 --> 00:14:02,190
decision trees is kind of really poetic

00:13:58,920 --> 00:14:04,020
but this the idea is that if you have

00:14:02,190 --> 00:14:06,780
one decision tree if you train your

00:14:04,020 --> 00:14:09,840
machine learning to have this one way to

00:14:06,780 --> 00:14:12,000
make decisions it's gonna try to overfit

00:14:09,840 --> 00:14:14,820
the data the training data you give it

00:14:12,000 --> 00:14:17,820
so if from my understanding what you do

00:14:14,820 --> 00:14:20,850
is that you create a lot of decision

00:14:17,820 --> 00:14:22,320
trees like that make decisions in a in

00:14:20,850 --> 00:14:26,790
different ways because you give them

00:14:22,320 --> 00:14:29,310
very varying type of data as entry and

00:14:26,790 --> 00:14:32,250
then you can have something that is

00:14:29,310 --> 00:14:35,070
we're not trying to overfit the data and

00:14:32,250 --> 00:14:37,830
actually gives you better results but

00:14:35,070 --> 00:14:40,740
the point behind this all is that if

00:14:37,830 --> 00:14:43,020
your team is a random forest you will

00:14:40,740 --> 00:14:45,570
have better results that if your team

00:14:43,020 --> 00:14:48,030
has the same way to make decisions so

00:14:45,570 --> 00:14:50,520
different backgrounds different life

00:14:48,030 --> 00:14:52,470
experience is actually gonna give you

00:14:50,520 --> 00:14:54,420
this different perspective it's not

00:14:52,470 --> 00:14:55,920
gonna give you all the perspective but

00:14:54,420 --> 00:15:00,860
it's going to be better than have just

00:14:55,920 --> 00:15:03,870
this one way of making decisions and

00:15:00,860 --> 00:15:07,380
finally the thing that I'm trying to do

00:15:03,870 --> 00:15:08,160
is to check what you find normal so have

00:15:07,380 --> 00:15:11,430
a checklist

00:15:08,160 --> 00:15:13,770
have a checklist of go to that repo on

00:15:11,430 --> 00:15:15,930
github of things programmers believe and

00:15:13,770 --> 00:15:19,290
just take things one after the other and

00:15:15,930 --> 00:15:22,740
check if what happened like for example

00:15:19,290 --> 00:15:24,480
have you ever been taking a plane and

00:15:22,740 --> 00:15:26,580
you have the boarding pass on your phone

00:15:24,480 --> 00:15:29,400
and your battery is getting really low

00:15:26,580 --> 00:15:31,080
and you get that fear of saying what

00:15:29,400 --> 00:15:33,900
happens when I don't have battery

00:15:31,080 --> 00:15:36,030
anymore and I get scared like I get like

00:15:33,900 --> 00:15:37,570
I don't know what I'm supposed to do

00:15:36,030 --> 00:15:40,450
once like I have no bad

00:15:37,570 --> 00:15:43,120
we left and I'm thinking the problem is

00:15:40,450 --> 00:15:44,860
I get this fear because no one told me

00:15:43,120 --> 00:15:47,020
what to do if like everybody's like oh

00:15:44,860 --> 00:15:49,000
yeah just put on your phone it's gonna

00:15:47,020 --> 00:15:51,610
be great what should I do if I don't

00:15:49,000 --> 00:15:55,330
have batteries anymore well now if I

00:15:51,610 --> 00:15:58,420
make an app I ask myself what happens if

00:15:55,330 --> 00:16:00,430
my user needs my harp but doesn't have

00:15:58,420 --> 00:16:02,320
battery anymore what happens if they

00:16:00,430 --> 00:16:04,960
don't have service because they're in

00:16:02,320 --> 00:16:08,500
the subway what happens if and so there

00:16:04,960 --> 00:16:10,750
is that list and that checklist and one

00:16:08,500 --> 00:16:12,580
of the things that I realized recently

00:16:10,750 --> 00:16:14,860
in the project I'm working on I'm

00:16:12,580 --> 00:16:16,510
working on an open source project so we

00:16:14,860 --> 00:16:20,740
have a lot of documentation to kind of

00:16:16,510 --> 00:16:23,230
be self-serving and one of the thing I

00:16:20,740 --> 00:16:24,730
realized that I'm always trying to have

00:16:23,230 --> 00:16:28,810
this very good English

00:16:24,730 --> 00:16:31,390
I'm always trying to sound like I really

00:16:28,810 --> 00:16:33,460
know what I'm talking about and then I

00:16:31,390 --> 00:16:36,190
realized that my bias is to think that

00:16:33,460 --> 00:16:38,500
everyone speaks very good English like

00:16:36,190 --> 00:16:40,930
all developers all around the world they

00:16:38,500 --> 00:16:43,570
have this beautiful English in the whole

00:16:40,930 --> 00:16:46,390
point of coming to my to this project is

00:16:43,570 --> 00:16:49,720
to judge my English level it is not I

00:16:46,390 --> 00:16:53,230
realized that but the idea now is can I

00:16:49,720 --> 00:16:56,290
make my English maybe simpler or more

00:16:53,230 --> 00:16:58,510
accessible without oversimplifying what

00:16:56,290 --> 00:17:00,820
I'm trying to say but so that people

00:16:58,510 --> 00:17:03,640
come to the repo and they're actually

00:17:00,820 --> 00:17:06,250
feel good about reading it because they

00:17:03,640 --> 00:17:13,390
get all the all the meaning that I'm

00:17:06,250 --> 00:17:15,880
trying to convey so first the algorithms

00:17:13,390 --> 00:17:18,189
we create as software developers they're

00:17:15,880 --> 00:17:20,800
biased let's start with that because

00:17:18,189 --> 00:17:24,010
they just they represent the way we have

00:17:20,800 --> 00:17:26,260
to represent the world and not one piece

00:17:24,010 --> 00:17:28,840
of I don't think one piece of software

00:17:26,260 --> 00:17:33,700
yet can really encompass the whole human

00:17:28,840 --> 00:17:35,800
experience and that that that bias it

00:17:33,700 --> 00:17:37,240
can be in the algorithm it can be in the

00:17:35,800 --> 00:17:39,490
users that are going to use your

00:17:37,240 --> 00:17:43,270
algorithms or it can be in the data that

00:17:39,490 --> 00:17:46,030
you use in to make decisions and so

00:17:43,270 --> 00:17:46,600
before our users are alienated in some

00:17:46,030 --> 00:17:49,120
way or another

00:17:46,600 --> 00:17:51,130
let's try to be very proactive and let's

00:17:49,120 --> 00:17:54,700
try to find those biases

00:17:51,130 --> 00:17:57,120
to make tech more inclusive thank you

00:17:54,700 --> 00:17:57,120
very much

00:18:05,390 --> 00:18:11,730
do we have time for questions we

00:18:10,770 --> 00:18:20,340
absolutely have time for questions

00:18:11,730 --> 00:18:22,320
wonderful you want to see this re on yes

00:18:20,340 --> 00:18:26,000
it is okay hi thank you so much for this

00:18:22,320 --> 00:18:28,350
talk this was really useful um before

00:18:26,000 --> 00:18:33,480
thank you sorry I should have done that

00:18:28,350 --> 00:18:35,070
I'm really short but for sometimes you

00:18:33,480 --> 00:18:38,370
you might kind of work on projects where

00:18:35,070 --> 00:18:41,070
you have a client or a need who feels

00:18:38,370 --> 00:18:44,159
like they need to represent their data

00:18:41,070 --> 00:18:45,960
in a way that does not actually

00:18:44,159 --> 00:18:48,240
represent the full diversity of the

00:18:45,960 --> 00:18:49,740
human experience what what are

00:18:48,240 --> 00:18:52,350
strategies that you have to sort of get

00:18:49,740 --> 00:18:54,500
clients on board with representing their

00:18:52,350 --> 00:18:57,630
data in a way that is more truthful

00:18:54,500 --> 00:18:59,730
that's a very I think it's a very good

00:18:57,630 --> 00:19:02,280
question I think software development is

00:18:59,730 --> 00:19:05,190
one of the many areas for this question

00:19:02,280 --> 00:19:07,620
happens my first question would be do

00:19:05,190 --> 00:19:09,690
they have other data because if the

00:19:07,620 --> 00:19:11,360
biased data is all that they have it's

00:19:09,690 --> 00:19:14,250
gonna be hard to represent it

00:19:11,360 --> 00:19:17,250
differently so how do they gather that

00:19:14,250 --> 00:19:19,710
data and are they making a choice in the

00:19:17,250 --> 00:19:22,380
data to kind of pick and choose or just

00:19:19,710 --> 00:19:25,260
do they might not have like unbiased

00:19:22,380 --> 00:19:27,120
data so that's the first the first kind

00:19:25,260 --> 00:19:29,760
of thing I would I would try to

00:19:27,120 --> 00:19:33,120
understand and if they are pick and

00:19:29,760 --> 00:19:36,030
choosing there is actually a talk I

00:19:33,120 --> 00:19:38,280
think to have about how it can be

00:19:36,030 --> 00:19:40,890
perceived or how it can be a DNA ting

00:19:38,280 --> 00:19:43,080
for some people I've realized in my

00:19:40,890 --> 00:19:45,210
career that often it's not that they're

00:19:43,080 --> 00:19:48,659
like some very biased people who hold

00:19:45,210 --> 00:19:50,730
their biased truth to be evidence most

00:19:48,659 --> 00:19:52,919
most of the time it's just people who

00:19:50,730 --> 00:19:54,990
don't think about like what happens if

00:19:52,919 --> 00:19:57,510
there's a disabled person trying to come

00:19:54,990 --> 00:20:00,270
in the building what happens if you know

00:19:57,510 --> 00:20:03,960
there's a same-sex couple and one of the

00:20:00,270 --> 00:20:07,970
person wants to change their name I've

00:20:03,960 --> 00:20:11,039
had this this case where the in a forum

00:20:07,970 --> 00:20:13,110
where you as a man you couldn't change

00:20:11,039 --> 00:20:14,880
your last name because it's just it

00:20:13,110 --> 00:20:16,320
wasn't used to be done like only women

00:20:14,880 --> 00:20:18,720
would change their name so there's the

00:20:16,320 --> 00:20:22,500
option just didn't exist

00:20:18,720 --> 00:20:26,159
and just saying like what happens if is

00:20:22,500 --> 00:20:29,460
a good way to kind of open that but if

00:20:26,159 --> 00:20:31,110
they're their bias is something that is

00:20:29,460 --> 00:20:33,240
more like something they believe or so

00:20:31,110 --> 00:20:35,510
like I have no idea how to deal with

00:20:33,240 --> 00:20:35,510
that

00:20:36,289 --> 00:20:50,039
thank you more questions quite good at

00:20:43,860 --> 00:20:52,080
spotting biases sometimes I don't always

00:20:50,039 --> 00:20:57,900
even react badly when they point out in

00:20:52,080 --> 00:20:59,400
mind but I I know that understanding

00:20:57,900 --> 00:21:03,530
that there is a bias in one's work is

00:20:59,400 --> 00:21:06,659
quite difficult and also difficult to

00:21:03,530 --> 00:21:08,070
persuade someone else not that they

00:21:06,659 --> 00:21:11,250
should do a certain thing but there is a

00:21:08,070 --> 00:21:12,929
problem what strategies I mean you just

00:21:11,250 --> 00:21:15,390
mentioned one right now about saying

00:21:12,929 --> 00:21:17,100
what if what other strategies will bring

00:21:15,390 --> 00:21:19,230
this out so people can come to it

00:21:17,100 --> 00:21:22,980
themselves without having to be taken

00:21:19,230 --> 00:21:25,530
there by you it's a it's a very good

00:21:22,980 --> 00:21:28,049
question I think because one of the

00:21:25,530 --> 00:21:30,570
strategies after I've seen and what I'm

00:21:28,049 --> 00:21:32,909
trying to do with my some of the tips

00:21:30,570 --> 00:21:34,890
and some of the strategies I'm trying to

00:21:32,909 --> 00:21:36,840
think about the the problem is most

00:21:34,890 --> 00:21:41,250
people realize it once they were faced

00:21:36,840 --> 00:21:44,669
with someone like a client or a a friend

00:21:41,250 --> 00:21:47,940
or a user that is actually took the time

00:21:44,669 --> 00:21:50,970
to come up to them and tell them their

00:21:47,940 --> 00:21:53,820
story which means you have to wait for

00:21:50,970 --> 00:21:56,400
what 10 people or more to have a really

00:21:53,820 --> 00:21:58,110
bad experience for maybe one of them to

00:21:56,400 --> 00:22:00,059
take the courage and take the time and

00:21:58,110 --> 00:22:02,360
take the energy to come to you and

00:22:00,059 --> 00:22:06,240
explain their you know their issue and

00:22:02,360 --> 00:22:09,150
most what now I talked about biases

00:22:06,240 --> 00:22:11,309
often this is the answer like oh I was

00:22:09,150 --> 00:22:14,190
doing a training and this person came up

00:22:11,309 --> 00:22:17,190
to me and so I had this Epiphany but I'm

00:22:14,190 --> 00:22:19,230
trying to my question now and that's I

00:22:17,190 --> 00:22:21,150
mean I haven't found one answer but my

00:22:19,230 --> 00:22:23,940
question now is how can we do that

00:22:21,150 --> 00:22:25,440
proactively like without actually going

00:22:23,940 --> 00:22:27,240
through the whole thing where you have

00:22:25,440 --> 00:22:30,270
to ally innate people to someday have

00:22:27,240 --> 00:22:32,360
your own realization and that's where I

00:22:30,270 --> 00:22:34,040
found it hard also and it's

00:22:32,360 --> 00:22:35,420
I find it hard on my own work like I

00:22:34,040 --> 00:22:37,250
don't want to believe that my work is

00:22:35,420 --> 00:22:43,040
biased I want to believe that I'm

00:22:37,250 --> 00:22:44,240
special but I'm not and so I'm working

00:22:43,040 --> 00:22:47,059
hard on it

00:22:44,240 --> 00:22:48,410
and I hope that we all do because I

00:22:47,059 --> 00:22:52,000
think that's how we're going to make it

00:22:48,410 --> 00:22:52,000
tech a better place to be

00:22:58,100 --> 00:23:04,850
hi thank you for the book um do you know

00:23:01,670 --> 00:23:08,330
if there's somewhere like a database or

00:23:04,850 --> 00:23:11,780
repository or something of contr biased

00:23:08,330 --> 00:23:13,550
data like a list of last names that our

00:23:11,780 --> 00:23:16,220
usual problems the list of addresses

00:23:13,550 --> 00:23:18,800
that our usual problems and things like

00:23:16,220 --> 00:23:20,720
that I haven't found it but you're not

00:23:18,800 --> 00:23:24,170
the first person to ask me about it I

00:23:20,720 --> 00:23:27,740
think someone here if they have some

00:23:24,170 --> 00:23:29,720
time maybe me should actually make it

00:23:27,740 --> 00:23:32,900
because I think having that training set

00:23:29,720 --> 00:23:36,260
like she would be a very great way to

00:23:32,900 --> 00:23:38,050
start if someone has it please put in

00:23:36,260 --> 00:23:45,530
the slack because I think we would all

00:23:38,050 --> 00:23:47,300
use it really have a lot of fun it would

00:23:45,530 --> 00:23:48,850
be very interesting to use it but yes I

00:23:47,300 --> 00:23:52,160
think there is a need for that

00:23:48,850 --> 00:23:56,570
definitely but again any list would not

00:23:52,160 --> 00:23:58,160
encompass maybe some of the issues I'm

00:23:56,570 --> 00:24:01,460
thinking for example you can have a list

00:23:58,160 --> 00:24:02,750
of names but things like you know they

00:24:01,460 --> 00:24:05,390
are the languages where you write from

00:24:02,750 --> 00:24:08,150
left to right and that's kind of hard to

00:24:05,390 --> 00:24:10,190
have a list of you know like so I'm

00:24:08,150 --> 00:24:12,860
thinking it's the first step but there's

00:24:10,190 --> 00:24:15,410
a whole and there is also other things

00:24:12,860 --> 00:24:19,970
to put in place well thank you for your

00:24:15,410 --> 00:24:26,230
question thank you are there more

00:24:19,970 --> 00:24:28,130
questions thank you very much thank you

00:24:26,230 --> 00:24:30,190
[Music]

00:24:28,130 --> 00:24:30,190

YouTube URL: https://www.youtube.com/watch?v=CZddeZ2RiLE


