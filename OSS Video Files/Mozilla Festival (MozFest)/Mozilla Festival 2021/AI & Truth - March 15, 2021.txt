Title: AI & Truth - March 15, 2021
Publication date: 2021-02-05
Playlist: Mozilla Festival 2021
Description: 
	The AI powering our most influential communications platforms has a critical vulnerability: it rewards engagement, not truth or civility. Can AI be designed differently to avoid this pitfall? And if so, what conditions and incentives must be changed for this to be the case? A panel featuring:

Jacquelyn Mason, Senior Investigative Researcher, First Draft News

Dr. Peaks Krafft, Lead, Creative Computing Institute at UAL

Mutale Nkonde, CEO, AI for the People

Rasha Abdul-Rahim, Acting Director, Amnesty Tech

Moderator Brandi Geurkink, Senior Campaigner, Mozilla
Captions: 
	00:00:00,060 --> 00:00:47,030
[Music]

00:00:49,660 --> 00:01:08,789
[Music]

00:01:15,180 --> 00:02:24,210
[Music]

00:02:26,820 --> 00:02:45,959
[Music]

00:02:52,360 --> 00:04:01,380
[Music]

00:04:04,010 --> 00:04:23,139
[Music]

00:04:29,580 --> 00:04:50,199
[Music]

00:04:54,840 --> 00:05:38,559
so

00:04:57,110 --> 00:05:38,559
[Music]

00:05:41,190 --> 00:06:00,320
[Music]

00:06:06,710 --> 00:07:15,740
[Music]

00:07:18,360 --> 00:07:37,490
[Music]

00:07:43,890 --> 00:08:52,910
[Music]

00:08:55,540 --> 00:09:14,670
[Music]

00:09:21,110 --> 00:10:30,089
[Music]

00:10:32,720 --> 00:10:51,850
[Music]

00:10:58,240 --> 00:12:07,270
[Music]

00:12:09,890 --> 00:12:29,020
[Music]

00:12:35,420 --> 00:13:44,440
[Music]

00:13:47,070 --> 00:14:06,200
[Music]

00:14:12,640 --> 00:15:03,349
[Music]

00:15:03,519 --> 00:15:06,560
hi everyone thank you for joining us

00:15:05,680 --> 00:15:09,120
today

00:15:06,560 --> 00:15:12,480
i am brandi gerkink senior campaigner at

00:15:09,120 --> 00:15:14,639
mozilla and your host for today's panel

00:15:12,480 --> 00:15:16,639
we have a vital discussion planned for

00:15:14,639 --> 00:15:19,519
today as part of our mozfest

00:15:16,639 --> 00:15:21,680
dialogues and debates series we're going

00:15:19,519 --> 00:15:23,680
to unpack an issue that's underpinning

00:15:21,680 --> 00:15:26,320
so many of the problems online

00:15:23,680 --> 00:15:26,800
and offline today the relationship

00:15:26,320 --> 00:15:30,240
between

00:15:26,800 --> 00:15:31,920
ai and truth the ai

00:15:30,240 --> 00:15:33,839
powering our most influential

00:15:31,920 --> 00:15:37,360
communications platforms

00:15:33,839 --> 00:15:40,480
has a critical vulnerability it rewards

00:15:37,360 --> 00:15:41,279
engagement not truth or civility the

00:15:40,480 --> 00:15:43,680
result

00:15:41,279 --> 00:15:45,680
can be that misinformation harassment

00:15:43,680 --> 00:15:48,160
and polarization

00:15:45,680 --> 00:15:49,920
have massive speed and spread at massive

00:15:48,160 --> 00:15:52,639
speed and scale

00:15:49,920 --> 00:15:53,600
to give you just one example a couple of

00:15:52,639 --> 00:15:57,120
years ago

00:15:53,600 --> 00:15:58,000
advocacy group hope not hate analyzed 5

00:15:57,120 --> 00:16:00,320
00:15:58,000 --> 00:16:01,440
anti-meghan markle tweets and found that

00:16:00,320 --> 00:16:05,040
only 20

00:16:01,440 --> 00:16:07,120
accounts were responsible for sharing 70

00:16:05,040 --> 00:16:08,880
of negative tweets containing abusive

00:16:07,120 --> 00:16:12,320
pictures hashtags

00:16:08,880 --> 00:16:14,480
and memes we're seeing similar abuse

00:16:12,320 --> 00:16:16,000
proliferate now in the aftermath of the

00:16:14,480 --> 00:16:17,120
interview that megan and prince harry

00:16:16,000 --> 00:16:19,120
did with oprah

00:16:17,120 --> 00:16:20,959
and many of these racist and abusive

00:16:19,120 --> 00:16:22,480
tweets are then retargeted

00:16:20,959 --> 00:16:24,399
to the black community meaning the

00:16:22,480 --> 00:16:26,160
harassment spreads even further than its

00:16:24,399 --> 00:16:28,880
original target

00:16:26,160 --> 00:16:29,600
we're going to unpack this and lots more

00:16:28,880 --> 00:16:32,800
today

00:16:29,600 --> 00:16:34,399
and we're also going to ask can ai be

00:16:32,800 --> 00:16:35,440
designed differently to avoid this

00:16:34,399 --> 00:16:38,000
pitfall

00:16:35,440 --> 00:16:39,199
if so what conditions and incentives

00:16:38,000 --> 00:16:42,480
must be changed

00:16:39,199 --> 00:16:43,680
for this to be the case i'm super super

00:16:42,480 --> 00:16:46,160
excited to announce

00:16:43,680 --> 00:16:48,240
a group of amazing panelists that have

00:16:46,160 --> 00:16:51,360
uh are joining me here today

00:16:48,240 --> 00:16:53,680
so first off is jacqueline mason a

00:16:51,360 --> 00:16:54,320
senior investigative researcher at first

00:16:53,680 --> 00:16:56,880
draft

00:16:54,320 --> 00:16:59,839
her research focuses on disinformation

00:16:56,880 --> 00:17:03,440
targeting black and latinx communities

00:16:59,839 --> 00:17:04,319
second is mutale nakonde ceo of ai for

00:17:03,440 --> 00:17:06,000
the people

00:17:04,319 --> 00:17:07,520
which explores how artificial

00:17:06,000 --> 00:17:10,959
intelligence technology

00:17:07,520 --> 00:17:12,240
can be used for the social good third is

00:17:10,959 --> 00:17:14,720
dr peaks kraft

00:17:12,240 --> 00:17:15,439
a senior lecturer and m a internet

00:17:14,720 --> 00:17:17,760
inequal

00:17:15,439 --> 00:17:18,480
internet equalities course leader at the

00:17:17,760 --> 00:17:21,760
ual

00:17:18,480 --> 00:17:24,240
creative computing institute and

00:17:21,760 --> 00:17:25,520
finally rasha abdul rahim director of

00:17:24,240 --> 00:17:27,919
amnesty tech

00:17:25,520 --> 00:17:29,280
a globally distributed team at amnesty

00:17:27,919 --> 00:17:31,600
international

00:17:29,280 --> 00:17:34,400
working to ensure technology benefits

00:17:31,600 --> 00:17:36,480
rather than erodes human rights

00:17:34,400 --> 00:17:38,400
one last thing before you dive in we're

00:17:36,480 --> 00:17:41,840
taking questions for this panel

00:17:38,400 --> 00:17:45,919
um in real time so tweet them at mozilla

00:17:41,840 --> 00:17:47,280
with the hashtag dialogues and debates

00:17:45,919 --> 00:17:49,200
so to kick off

00:17:47,280 --> 00:17:50,320
i first want to pose a question to

00:17:49,200 --> 00:17:53,039
mutale

00:17:50,320 --> 00:17:55,280
your groundbreaking research into

00:17:53,039 --> 00:17:58,160
or american descendants of slavery

00:17:55,280 --> 00:18:00,240
online network gave way to a new term in

00:17:58,160 --> 00:18:02,799
the disinformation space called

00:18:00,240 --> 00:18:04,960
disinformation creep which refers to a

00:18:02,799 --> 00:18:05,679
general phenomenon wherein marginalized

00:18:04,960 --> 00:18:07,039
communities

00:18:05,679 --> 00:18:09,360
whose interests and legitimate

00:18:07,039 --> 00:18:10,000
grievances are ignored by mainstream

00:18:09,360 --> 00:18:12,559
narratives

00:18:10,000 --> 00:18:13,280
are then targeted by misinformation

00:18:12,559 --> 00:18:14,960
narratives

00:18:13,280 --> 00:18:16,320
can you tell us a little bit more about

00:18:14,960 --> 00:18:19,840
this research metalli

00:18:16,320 --> 00:18:19,840
um as well as its reception

00:18:22,320 --> 00:18:26,400
when i'm not muted um yes sure and thank

00:18:25,679 --> 00:18:30,000
you for

00:18:26,400 --> 00:18:34,000
inviting me everybody so about

00:18:30,000 --> 00:18:35,039
two years ago um i prior to even ai for

00:18:34,000 --> 00:18:37,440
the people start

00:18:35,039 --> 00:18:39,520
starting i was really kind of shocked

00:18:37,440 --> 00:18:42,679
and stunned that the most

00:18:39,520 --> 00:18:44,080
targeted group of um online users in

00:18:42,679 --> 00:18:46,640
00:18:44,080 --> 00:18:47,919
had been african-americans and this was

00:18:46,640 --> 00:18:49,679
a finding of the

00:18:47,919 --> 00:18:51,280
mueller report that was looking into

00:18:49,679 --> 00:18:53,520
election interference and

00:18:51,280 --> 00:18:55,919
at that point and so one of the things

00:18:53,520 --> 00:18:56,880
that we wanted to do in 2020 was to try

00:18:55,919 --> 00:18:59,120
and figure out

00:18:56,880 --> 00:19:02,240
what what did that look like what could

00:18:59,120 --> 00:19:06,640
that look like in 2020 and at that time

00:19:02,240 --> 00:19:10,559
in 2016 it had been russian troll farms

00:19:06,640 --> 00:19:11,039
creating a fake social media communities

00:19:10,559 --> 00:19:14,480
like

00:19:11,039 --> 00:19:16,799
woke blacks uh brown and down other

00:19:14,480 --> 00:19:18,720
other groups um and then creating

00:19:16,799 --> 00:19:19,840
content that was targeted to black

00:19:18,720 --> 00:19:22,080
communities

00:19:19,840 --> 00:19:23,919
but one of the things that we did this

00:19:22,080 --> 00:19:24,640
year was to try and find out what were

00:19:23,919 --> 00:19:26,799
the

00:19:24,640 --> 00:19:27,679
dominant black narratives in the age of

00:19:26,799 --> 00:19:30,880
covid

00:19:27,679 --> 00:19:33,760
and expecting to find that in

00:19:30,880 --> 00:19:35,840
online spaces we would be thinking about

00:19:33,760 --> 00:19:38,240
covid as a racial justice issue

00:19:35,840 --> 00:19:38,960
and with the particular network what we

00:19:38,240 --> 00:19:41,600
did find

00:19:38,960 --> 00:19:43,840
was that they were using breaking news

00:19:41,600 --> 00:19:46,559
specifically breaking news

00:19:43,840 --> 00:19:48,960
around culture and nativism so lots of

00:19:46,559 --> 00:19:51,679
questions about who was really black

00:19:48,960 --> 00:19:52,799
um kamala harris being kind of what

00:19:51,679 --> 00:19:54,720
piqued our initial

00:19:52,799 --> 00:19:56,000
interests and conversations about her

00:19:54,720 --> 00:19:59,039
not being black

00:19:56,000 --> 00:20:02,280
and then really turning the corner

00:19:59,039 --> 00:20:04,799
to have other conversations around

00:20:02,280 --> 00:20:07,039
reparations uh was that really the big

00:20:04,799 --> 00:20:09,520
conversation in 2020

00:20:07,039 --> 00:20:10,400
but they would always end with and then

00:20:09,520 --> 00:20:12,400
don't vote

00:20:10,400 --> 00:20:13,760
in the presidential and this is really

00:20:12,400 --> 00:20:15,679
in line with

00:20:13,760 --> 00:20:17,440
traditional uh definitions of

00:20:15,679 --> 00:20:20,000
disinformation

00:20:17,440 --> 00:20:21,760
and offered by liza bittman and other

00:20:20,000 --> 00:20:24,880
kgb officers

00:20:21,760 --> 00:20:28,480
where it was this truth this truth

00:20:24,880 --> 00:20:30,880
racism does exist this uh lie

00:20:28,480 --> 00:20:31,520
that you have to be uh you have to have

00:20:30,880 --> 00:20:34,559
a

00:20:31,520 --> 00:20:34,960
particular birthright to be considered

00:20:34,559 --> 00:20:37,360
black

00:20:34,960 --> 00:20:38,480
politically in america that's not true

00:20:37,360 --> 00:20:40,559
ask you know

00:20:38,480 --> 00:20:43,039
um amidu diallo who was shot by the

00:20:40,559 --> 00:20:44,480
police here in new york city in 1999 he

00:20:43,039 --> 00:20:46,000
was black enough to be shot by the

00:20:44,480 --> 00:20:46,880
police right even though he was from

00:20:46,000 --> 00:20:49,280
guinea

00:20:46,880 --> 00:20:50,320
and then this manipulation of behavior

00:20:49,280 --> 00:20:53,360
which was

00:20:50,320 --> 00:20:55,919
um don't vote so we are our data set was

00:20:53,360 --> 00:20:58,240
3.8 million tweets but the universe that

00:20:55,919 --> 00:21:01,600
we looked at was 1.3 million tweets

00:20:58,240 --> 00:21:04,159
starting november 2019

00:21:01,600 --> 00:21:05,120
through to election week and that we saw

00:21:04,159 --> 00:21:06,559
a clear

00:21:05,120 --> 00:21:09,039
pattern of what we believe to be

00:21:06,559 --> 00:21:11,919
networked activity pushing

00:21:09,039 --> 00:21:13,360
black um black people to the right on

00:21:11,919 --> 00:21:16,159
the twitter platform

00:21:13,360 --> 00:21:17,200
but using this kind of disinformation

00:21:16,159 --> 00:21:20,880
creep

00:21:17,200 --> 00:21:22,559
um you know strategy which would be

00:21:20,880 --> 00:21:25,039
hey kamala harris is running for

00:21:22,559 --> 00:21:25,679
president oh and by the way she's not

00:21:25,039 --> 00:21:27,919
black

00:21:25,679 --> 00:21:30,320
oh and therefore don't vote for her and

00:21:27,919 --> 00:21:33,440
then when she was named vice president

00:21:30,320 --> 00:21:35,039
by extension don't vote for um the

00:21:33,440 --> 00:21:36,320
academy which we thought was interesting

00:21:35,039 --> 00:21:38,240
but what really got us

00:21:36,320 --> 00:21:39,760
was when um unfortunately chadwick

00:21:38,240 --> 00:21:42,480
boseman died

00:21:39,760 --> 00:21:44,480
who for any black person in any corner

00:21:42,480 --> 00:21:46,320
of the earth is the black panther

00:21:44,480 --> 00:21:48,240
wakanda is a real place don't ask us

00:21:46,320 --> 00:21:50,320
about it we're going with it

00:21:48,240 --> 00:21:52,000
and their tweets around that were like

00:21:50,320 --> 00:21:52,480
well that kind of serves him right

00:21:52,000 --> 00:21:54,960
because

00:21:52,480 --> 00:21:56,320
it was in africa and if he was a true

00:21:54,960 --> 00:21:58,480
american then he wouldn't have been in

00:21:56,320 --> 00:21:59,919
the movie so don't vote at which point

00:21:58,480 --> 00:22:01,840
we were like aha

00:21:59,919 --> 00:22:03,919
you know we have something and we were

00:22:01,840 --> 00:22:04,880
very specifically looking at how they

00:22:03,919 --> 00:22:08,320
were targeting

00:22:04,880 --> 00:22:11,440
a local news ecosystem um

00:22:08,320 --> 00:22:13,760
in the philadelphia news ecosystem so

00:22:11,440 --> 00:22:15,440
it was quite ride and then lastly in

00:22:13,760 --> 00:22:17,919
terms of reception

00:22:15,440 --> 00:22:19,919
the internet was really mad like people

00:22:17,919 --> 00:22:23,679
that identified with that hashtag

00:22:19,919 --> 00:22:27,039
came after us uh one of my co-authors

00:22:23,679 --> 00:22:28,799
and had to block 600 accounts one of my

00:22:27,039 --> 00:22:31,679
other co-authors

00:22:28,799 --> 00:22:33,440
and got threats another co-op another

00:22:31,679 --> 00:22:36,720
person that we cite

00:22:33,440 --> 00:22:38,559
um she's told us that people previous

00:22:36,720 --> 00:22:39,120
prior to this had actually turned up at

00:22:38,559 --> 00:22:42,960
her house

00:22:39,120 --> 00:22:44,080
doxing her and that activity reacts so

00:22:42,960 --> 00:22:47,840
we were lucky

00:22:44,080 --> 00:22:50,480
um to be to be connected to

00:22:47,840 --> 00:22:52,640
folks um at twitter through great

00:22:50,480 --> 00:22:55,039
relationships with russia

00:22:52,640 --> 00:22:56,559
and her team and their colleagues at

00:22:55,039 --> 00:22:58,799
amnesty usa

00:22:56,559 --> 00:23:01,280
and start a conversation and i just

00:22:58,799 --> 00:23:04,320
found out today that i'm still being

00:23:01,280 --> 00:23:05,039
a shadow band uh by what i suspect to be

00:23:04,320 --> 00:23:07,600
that group

00:23:05,039 --> 00:23:09,039
so if you receive a tweet from me like

00:23:07,600 --> 00:23:11,679
beyonce should have got

00:23:09,039 --> 00:23:12,240
album for the year and i still you know

00:23:11,679 --> 00:23:14,000
like

00:23:12,240 --> 00:23:15,440
i still don't care about the grammys

00:23:14,000 --> 00:23:16,559
you're gonna find a warning on that

00:23:15,440 --> 00:23:19,120
tweet saying that

00:23:16,559 --> 00:23:20,320
um this is sensitive um content so it's

00:23:19,120 --> 00:23:21,919
an ongoing

00:23:20,320 --> 00:23:24,400
it's an ongoing conversation with the

00:23:21,919 --> 00:23:24,400
platform

00:23:27,840 --> 00:23:34,080
thanks brutally and you talked about how

00:23:30,960 --> 00:23:35,120
the the kind of outcome of that was and

00:23:34,080 --> 00:23:37,039
so don't vote

00:23:35,120 --> 00:23:38,640
you know and so linking back to this

00:23:37,039 --> 00:23:41,440
kind of narrative

00:23:38,640 --> 00:23:42,400
and is being being perpetuated and

00:23:41,440 --> 00:23:44,559
jacqueline i think

00:23:42,400 --> 00:23:45,840
um in reviewing some of some of your

00:23:44,559 --> 00:23:48,320
research on

00:23:45,840 --> 00:23:50,159
which is uh different you looked at

00:23:48,320 --> 00:23:52,400
vaccine misinformation

00:23:50,159 --> 00:23:54,240
but i i think i saw sort of similar

00:23:52,400 --> 00:23:55,840
similar threads there with re you know

00:23:54,240 --> 00:23:57,919
the ability for these messages to sort

00:23:55,840 --> 00:23:59,840
of reinforce a particular

00:23:57,919 --> 00:24:01,919
narrative and i know you looked into

00:23:59,840 --> 00:24:03,919
vaccine misinformation

00:24:01,919 --> 00:24:05,200
in the philippines and i was wondering

00:24:03,919 --> 00:24:07,919
what you could tell us about

00:24:05,200 --> 00:24:09,039
um the interplay between misinformation

00:24:07,919 --> 00:24:12,000
that spreads on

00:24:09,039 --> 00:24:14,080
social platforms and the wider media

00:24:12,000 --> 00:24:16,400
ecosystem that contributed to

00:24:14,080 --> 00:24:18,320
to these narratives um are you seeing

00:24:16,400 --> 00:24:20,960
any similar trends play out with

00:24:18,320 --> 00:24:23,279
covid vaccine misinformation do you have

00:24:20,960 --> 00:24:26,240
any recommendations on how we should

00:24:23,279 --> 00:24:27,520
consider this problem okay great thank

00:24:26,240 --> 00:24:29,679
you brandi thank you for inviting me

00:24:27,520 --> 00:24:31,120
here so excited to talk with all of you

00:24:29,679 --> 00:24:32,799
um yeah so i will talk a little bit

00:24:31,120 --> 00:24:35,039
about my research um on

00:24:32,799 --> 00:24:36,480
dang dang fever in the philippines that

00:24:35,039 --> 00:24:38,320
i did a couple of years ago

00:24:36,480 --> 00:24:39,760
uh so for those of you not familiar uh

00:24:38,320 --> 00:24:42,480
deng's a viral infection

00:24:39,760 --> 00:24:42,799
um in tropical and subtropical climates

00:24:42,480 --> 00:24:44,880
and

00:24:42,799 --> 00:24:46,000
it is currently the most rapid spreading

00:24:44,880 --> 00:24:48,240
mosquito-borne viral

00:24:46,000 --> 00:24:49,360
disease in the world so in november of

00:24:48,240 --> 00:24:51,840
00:24:49,360 --> 00:24:54,400
sanofi which is a pharmaceutical company

00:24:51,840 --> 00:24:56,240
developed a drug called dengvaxia

00:24:54,400 --> 00:24:58,240
which was meant to prevent dang fever

00:24:56,240 --> 00:25:00,640
however shortly after

00:24:58,240 --> 00:25:01,840
cenopy released an updated analysis on

00:25:00,640 --> 00:25:03,200
dengvaxia

00:25:01,840 --> 00:25:05,600
um but only after it had been

00:25:03,200 --> 00:25:08,559
administered to thousands of filipinos

00:25:05,600 --> 00:25:08,960
uh basically the analysis suggested that

00:25:08,559 --> 00:25:10,799
um

00:25:08,960 --> 00:25:13,440
children who've never been exposed to

00:25:10,799 --> 00:25:14,080
dang who would take vengeance could then

00:25:13,440 --> 00:25:16,720
develop

00:25:14,080 --> 00:25:18,159
a more serious form of dang and that

00:25:16,720 --> 00:25:20,000
more serious form of deng could even

00:25:18,159 --> 00:25:21,360
lead to death considering how dangerous

00:25:20,000 --> 00:25:24,000
dengvaxia is

00:25:21,360 --> 00:25:25,600
uh so dang vaccinations were officially

00:25:24,000 --> 00:25:28,720
suspended in the philippines

00:25:25,600 --> 00:25:30,720
shortly thereafter this also led to a

00:25:28,720 --> 00:25:31,919
rapid decline in vaccine confidence

00:25:30,720 --> 00:25:33,919
among filipinos

00:25:31,919 --> 00:25:35,360
and years following the incident uh

00:25:33,919 --> 00:25:37,520
there was still a lack of clear

00:25:35,360 --> 00:25:39,440
consensus from the actual government or

00:25:37,520 --> 00:25:42,480
the medical community in the philippines

00:25:39,440 --> 00:25:44,320
so this really encouraged a cacophony of

00:25:42,480 --> 00:25:46,400
voices and claims online and

00:25:44,320 --> 00:25:47,520
conspiracies that dengvaxia was

00:25:46,400 --> 00:25:49,520
dangerous

00:25:47,520 --> 00:25:51,200
and they began to thrive online

00:25:49,520 --> 00:25:54,559
particularly on facebook

00:25:51,200 --> 00:25:55,520
bullied by this uh uh hashtag called

00:25:54,559 --> 00:25:58,320
dengate

00:25:55,520 --> 00:26:00,320
right um this is a very popular hashtag

00:25:58,320 --> 00:26:02,080
used to associate misinformation about

00:26:00,320 --> 00:26:04,240
dangbaxio with deng

00:26:02,080 --> 00:26:06,159
um to proliferate online and this was a

00:26:04,240 --> 00:26:07,760
huge trauma for the philippines

00:26:06,159 --> 00:26:09,679
um there were videos that were

00:26:07,760 --> 00:26:12,320
repurposed showing mass

00:26:09,679 --> 00:26:14,559
deaths mass graves um people you know in

00:26:12,320 --> 00:26:15,440
hysterics over the death of many of any

00:26:14,559 --> 00:26:18,000
children

00:26:15,440 --> 00:26:19,440
um so online anti-vaccine social

00:26:18,000 --> 00:26:20,799
disinformation agents were basically

00:26:19,440 --> 00:26:23,039
thriving off of this

00:26:20,799 --> 00:26:25,120
and they attached the deaths of

00:26:23,039 --> 00:26:27,600
thousands of filipinos if not hundreds

00:26:25,120 --> 00:26:29,679
of thousands of filipinos to dengvaxia

00:26:27,600 --> 00:26:31,440
and a majority if not all of the posts

00:26:29,679 --> 00:26:33,360
um not being taken down

00:26:31,440 --> 00:26:34,480
actually not really many taken down

00:26:33,360 --> 00:26:37,520
until recently

00:26:34,480 --> 00:26:38,720
uh in the wake of cobin 19. so four

00:26:37,520 --> 00:26:40,880
years later

00:26:38,720 --> 00:26:43,360
you know some of those accounts are

00:26:40,880 --> 00:26:45,840
still up a lot of those posts have been

00:26:43,360 --> 00:26:46,799
taken down however those spreading

00:26:45,840 --> 00:26:48,960
vaccine

00:26:46,799 --> 00:26:49,919
missing disinformation are still using

00:26:48,960 --> 00:26:52,480
death

00:26:49,919 --> 00:26:53,919
and in a lot of cases depopulation as a

00:26:52,480 --> 00:26:56,240
main talking point

00:26:53,919 --> 00:26:58,480
uh so as we know i think last month

00:26:56,240 --> 00:27:00,720
facebook pledged to crack down on cobit

00:26:58,480 --> 00:27:02,799
19 vaccine misinformation

00:27:00,720 --> 00:27:04,640
um and while some posts have been taken

00:27:02,799 --> 00:27:06,320
down false claims continue to

00:27:04,640 --> 00:27:08,880
proliferate on the platform

00:27:06,320 --> 00:27:09,520
uh for example um people are using bit

00:27:08,880 --> 00:27:12,240
chutes

00:27:09,520 --> 00:27:14,080
uh to have links about copic 19 which

00:27:12,240 --> 00:27:16,159
spread false information that

00:27:14,080 --> 00:27:18,320
thousands of people have died even

00:27:16,159 --> 00:27:20,320
numbers as high as 56 000 people have

00:27:18,320 --> 00:27:20,799
died from vaccines right and he's been

00:27:20,320 --> 00:27:23,440
pushed

00:27:20,799 --> 00:27:25,279
over 1300 times since the ban um

00:27:23,440 --> 00:27:27,440
post-which plain people

00:27:25,279 --> 00:27:29,279
specifically celebrities have died or

00:27:27,440 --> 00:27:31,600
become ill from coveted vaccines

00:27:29,279 --> 00:27:33,200
remain on the platform uh specifically

00:27:31,600 --> 00:27:35,360
surrounding hank aaron

00:27:33,200 --> 00:27:37,279
um especially within the black community

00:27:35,360 --> 00:27:38,320
especially within the nation of islam

00:27:37,279 --> 00:27:40,240
and other groups

00:27:38,320 --> 00:27:42,000
are spreading missing disinformation

00:27:40,240 --> 00:27:43,840
that remains on the platform that hank

00:27:42,000 --> 00:27:46,080
aaron has died from covet vaccines now

00:27:43,840 --> 00:27:48,159
some of these posts are labeled

00:27:46,080 --> 00:27:50,000
by the cdc but it's just truly not

00:27:48,159 --> 00:27:51,679
enough right they need to be taken down

00:27:50,000 --> 00:27:52,799
but they are not and they continue to

00:27:51,679 --> 00:27:55,200
proliferate

00:27:52,799 --> 00:27:57,039
um so as we know there are informed

00:27:55,200 --> 00:27:58,960
sources particularly on facebook

00:27:57,039 --> 00:28:00,399
um that reached users contents about

00:27:58,960 --> 00:28:01,919
vaccines which i'll talk about a little

00:28:00,399 --> 00:28:04,799
bit more later like yes they have the

00:28:01,919 --> 00:28:07,200
cdc and who post but it's not enough

00:28:04,799 --> 00:28:08,880
these pages are often eclipsed by much

00:28:07,200 --> 00:28:11,919
conspiratorial comment

00:28:08,880 --> 00:28:14,000
um content and so i just think that my

00:28:11,919 --> 00:28:15,679
suggestion is that health authorities

00:28:14,000 --> 00:28:17,600
and trusted news sites

00:28:15,679 --> 00:28:19,440
be trained um much like we do with

00:28:17,600 --> 00:28:20,960
community-based organizations at first

00:28:19,440 --> 00:28:23,360
draft and best practices

00:28:20,960 --> 00:28:24,960
on social media monitoring uh debunking

00:28:23,360 --> 00:28:27,679
disinformation and propaganda and

00:28:24,960 --> 00:28:31,840
responsible reporting on these hoaxes

00:28:27,679 --> 00:28:31,840
in this age of disinformation

00:28:33,760 --> 00:28:38,640
thank you so much jacqueline yeah and i

00:28:35,520 --> 00:28:41,520
think um i definitely want to dive into

00:28:38,640 --> 00:28:42,080
sort of some questions i have around uh

00:28:41,520 --> 00:28:43,679
you know

00:28:42,080 --> 00:28:45,919
digging in a little bit more on what

00:28:43,679 --> 00:28:47,919
community-based solutions to

00:28:45,919 --> 00:28:49,679
to misinformation look like i think we

00:28:47,919 --> 00:28:52,799
can we can definitely start to

00:28:49,679 --> 00:28:53,679
to discuss that um in just a few uh i

00:28:52,799 --> 00:28:57,200
want to

00:28:53,679 --> 00:29:00,080
direct now to peaks um dr kraft you're

00:28:57,200 --> 00:29:00,640
currently recruiting for a master's

00:29:00,080 --> 00:29:02,880
program

00:29:00,640 --> 00:29:03,919
um in internet equality is focused on

00:29:02,880 --> 00:29:05,919
exploring how

00:29:03,919 --> 00:29:07,919
power relations are organized this is

00:29:05,919 --> 00:29:09,039
like sort of a meta look right at some

00:29:07,919 --> 00:29:10,399
of the things that we've already been

00:29:09,039 --> 00:29:12,640
talking about um

00:29:10,399 --> 00:29:14,480
how they are embedded and perpetuated in

00:29:12,640 --> 00:29:16,640
internet technologies

00:29:14,480 --> 00:29:17,520
and how they can be reorganized and

00:29:16,640 --> 00:29:19,600
challenged

00:29:17,520 --> 00:29:20,720
through critical creative activist

00:29:19,600 --> 00:29:22,159
practice

00:29:20,720 --> 00:29:23,760
uh i was wondering if you could tell us

00:29:22,159 --> 00:29:25,919
a little bit more about

00:29:23,760 --> 00:29:27,200
how the course came about who it's

00:29:25,919 --> 00:29:29,679
designed for

00:29:27,200 --> 00:29:31,360
the change that you think it can bring

00:29:29,679 --> 00:29:32,320
about within the industry we've already

00:29:31,360 --> 00:29:34,799
heard a couple of

00:29:32,320 --> 00:29:35,679
examples of areas where change is really

00:29:34,799 --> 00:29:37,120
really needed

00:29:35,679 --> 00:29:39,360
so i'm hoping you can speak to this a

00:29:37,120 --> 00:29:42,240
bit and also if there are any

00:29:39,360 --> 00:29:42,960
particular emerging issues with regards

00:29:42,240 --> 00:29:45,760
to

00:29:42,960 --> 00:29:48,080
algorithmic equity that this course uh

00:29:45,760 --> 00:29:50,080
will explore

00:29:48,080 --> 00:29:52,159
yeah thanks so much brandi and lovely to

00:29:50,080 --> 00:29:53,440
be here um

00:29:52,159 --> 00:29:55,279
as you mentioned i think that what we've

00:29:53,440 --> 00:29:56,080
heard so far there are all really great

00:29:55,279 --> 00:29:58,799
examples

00:29:56,080 --> 00:30:00,880
of the types of things that that this

00:29:58,799 --> 00:30:02,320
course is trying to respond to i think

00:30:00,880 --> 00:30:04,080
uh bullying and harassment online

00:30:02,320 --> 00:30:05,600
bullying harassment in the tech industry

00:30:04,080 --> 00:30:08,080
or in higher education

00:30:05,600 --> 00:30:09,279
misinformation uh in various different

00:30:08,080 --> 00:30:11,840
domains

00:30:09,279 --> 00:30:13,200
as well as ai but ai bias which i

00:30:11,840 --> 00:30:15,440
suppose we'll talk about more

00:30:13,200 --> 00:30:16,640
more in a little bit uh and these are

00:30:15,440 --> 00:30:18,480
all examples of how

00:30:16,640 --> 00:30:20,080
power is organized embedded and

00:30:18,480 --> 00:30:20,640
perpetuated power relations are

00:30:20,080 --> 00:30:23,120
organized

00:30:20,640 --> 00:30:24,640
and perpetuated information technologies

00:30:23,120 --> 00:30:27,279
and the degree is really about

00:30:24,640 --> 00:30:28,080
what can we do and uh what kind of

00:30:27,279 --> 00:30:30,320
creative

00:30:28,080 --> 00:30:32,320
or critical or activist practice can can

00:30:30,320 --> 00:30:34,080
address these things

00:30:32,320 --> 00:30:35,520
it's a really unique opportunity and one

00:30:34,080 --> 00:30:37,279
that i'm really excited about because

00:30:35,520 --> 00:30:40,000
for the last few years i've increasingly

00:30:37,279 --> 00:30:41,039
been been participating in tech activism

00:30:40,000 --> 00:30:44,000
and just trying to make

00:30:41,039 --> 00:30:45,039
my day job as much about that as i can

00:30:44,000 --> 00:30:46,960
and so

00:30:45,039 --> 00:30:48,399
the the creative computing institute at

00:30:46,960 --> 00:30:49,919
the university of the arts london

00:30:48,399 --> 00:30:52,080
is a really unique place to be to be

00:30:49,919 --> 00:30:54,640
developing developing a program because

00:30:52,080 --> 00:30:56,640
as an art school that that has a lot of

00:30:54,640 --> 00:30:58,399
activist faculty members and a lot of

00:30:56,640 --> 00:31:00,640
kind of activist orientation

00:30:58,399 --> 00:31:03,360
in in the institution it's a real

00:31:00,640 --> 00:31:05,679
opportunity to to do more than just

00:31:03,360 --> 00:31:07,039
teaching or learning or learning

00:31:05,679 --> 00:31:09,840
together but also

00:31:07,039 --> 00:31:10,720
really trying to learn by doing and and

00:31:09,840 --> 00:31:12,399
orient

00:31:10,720 --> 00:31:13,679
the entire program as much as possible

00:31:12,399 --> 00:31:14,480
towards towards actually some of the

00:31:13,679 --> 00:31:16,080
social change

00:31:14,480 --> 00:31:18,080
that that i hope we talked more about

00:31:16,080 --> 00:31:21,600
today um

00:31:18,080 --> 00:31:22,640
and yeah i think that there's there's a

00:31:21,600 --> 00:31:25,279
wide range of folks

00:31:22,640 --> 00:31:26,080
who are engaged in this in this kind of

00:31:25,279 --> 00:31:30,080
work

00:31:26,080 --> 00:31:31,840
in in tech activism uh i hope that

00:31:30,080 --> 00:31:34,000
folks designers interested in design

00:31:31,840 --> 00:31:35,360
justice programmers interested in

00:31:34,000 --> 00:31:37,519
surveillance capitalism

00:31:35,360 --> 00:31:38,880
uh social scientists artists activists

00:31:37,519 --> 00:31:39,840
who are concerned about tech or tech

00:31:38,880 --> 00:31:41,200
reporters

00:31:39,840 --> 00:31:43,519
folks are working in ai ethics

00:31:41,200 --> 00:31:45,840
responsible innovation tech policy

00:31:43,519 --> 00:31:47,840
labor organizing i think a lot of these

00:31:45,840 --> 00:31:49,200
sectors are represented here today and

00:31:47,840 --> 00:31:51,120
and all these different

00:31:49,200 --> 00:31:52,080
areas and folks have have different

00:31:51,120 --> 00:31:53,120
pieces to contribute to this

00:31:52,080 --> 00:31:54,559
conversation

00:31:53,120 --> 00:31:56,559
and so the types of changes that i'm

00:31:54,559 --> 00:31:58,320
hoping to see and

00:31:56,559 --> 00:32:00,640
also i mean i think the type of changes

00:31:58,320 --> 00:32:02,000
i'm hoping to see and the issues i think

00:32:00,640 --> 00:32:03,120
that are emerging are actually quite

00:32:02,000 --> 00:32:05,840
closely related

00:32:03,120 --> 00:32:06,640
and so uh to address both of those i

00:32:05,840 --> 00:32:08,480
would say

00:32:06,640 --> 00:32:10,399
i'm i'm looking for broader

00:32:08,480 --> 00:32:12,000
understanding of social issues

00:32:10,399 --> 00:32:14,159
and the logics of collective action

00:32:12,000 --> 00:32:17,519
protest resistance and refusal in

00:32:14,159 --> 00:32:19,279
in the tech industry um taking more or

00:32:17,519 --> 00:32:20,320
having a culture of taking more personal

00:32:19,279 --> 00:32:21,760
responsibility

00:32:20,320 --> 00:32:24,000
as well as developing networks of

00:32:21,760 --> 00:32:26,480
accountability to to help

00:32:24,000 --> 00:32:28,159
myself and help help us all to to take

00:32:26,480 --> 00:32:31,440
that personal responsibility

00:32:28,159 --> 00:32:33,039
uh really opening up to pluralistic

00:32:31,440 --> 00:32:34,320
modes of knowledge production and

00:32:33,039 --> 00:32:35,360
thinking about different kinds of

00:32:34,320 --> 00:32:36,880
expertise

00:32:35,360 --> 00:32:39,039
and the kinds of expertise that come

00:32:36,880 --> 00:32:42,320
from all of our lived experiences

00:32:39,039 --> 00:32:45,360
and doing work whether it's research or

00:32:42,320 --> 00:32:46,080
design or engineering that begins not

00:32:45,360 --> 00:32:47,279
with

00:32:46,080 --> 00:32:49,440
a research question or problem

00:32:47,279 --> 00:32:52,559
formulation but that begins with with a

00:32:49,440 --> 00:32:52,559
theory of social change

00:32:55,519 --> 00:32:59,200
thank you so much i really admire how

00:32:58,399 --> 00:33:00,960
thoughtfully

00:32:59,200 --> 00:33:03,279
you're thinking about these issues and

00:33:00,960 --> 00:33:04,320
also solutions to these to these issues

00:33:03,279 --> 00:33:07,360
and i think that

00:33:04,320 --> 00:33:09,440
um each one of you actually

00:33:07,360 --> 00:33:10,559
has kind of a different approach to

00:33:09,440 --> 00:33:13,279
thinking about

00:33:10,559 --> 00:33:15,440
um i suppose the angle at which you're

00:33:13,279 --> 00:33:16,559
working on on these issues from which is

00:33:15,440 --> 00:33:19,760
fascinating

00:33:16,559 --> 00:33:22,559
uh to dive into more i wanted to ask

00:33:19,760 --> 00:33:24,000
rasha for um for you and your team at

00:33:22,559 --> 00:33:26,320
amnesty you've done a lot of

00:33:24,000 --> 00:33:27,440
great work to explore how the business

00:33:26,320 --> 00:33:29,840
models um

00:33:27,440 --> 00:33:30,960
of of corporate giants like google or

00:33:29,840 --> 00:33:32,799
facebook are

00:33:30,960 --> 00:33:34,320
are actually at odds with human rights

00:33:32,799 --> 00:33:37,840
and i was wondering if you could

00:33:34,320 --> 00:33:39,039
unpack that a bit explain it to us um

00:33:37,840 --> 00:33:42,240
and in particular

00:33:39,039 --> 00:33:43,919
as a campaigner i'm really interested in

00:33:42,240 --> 00:33:46,559
you know whether you think that there is

00:33:43,919 --> 00:33:48,480
a different approach that we should take

00:33:46,559 --> 00:33:50,320
um when we're thinking about pressuring

00:33:48,480 --> 00:33:53,039
private companies to implement

00:33:50,320 --> 00:33:53,519
uh you know more more trustworthy ai or

00:33:53,039 --> 00:33:55,679
um

00:33:53,519 --> 00:33:59,120
versus you know governments or public

00:33:55,679 --> 00:33:59,120
institutions what do you think

00:33:59,919 --> 00:34:03,440
thank you so much firstly thank you so

00:34:01,360 --> 00:34:06,320
much for having me um at this event

00:34:03,440 --> 00:34:07,039
and uh with such amazing fellow speakers

00:34:06,320 --> 00:34:08,560
um

00:34:07,039 --> 00:34:10,560
and i apologize if there's any

00:34:08,560 --> 00:34:12,240
background now noise the neighbors have

00:34:10,560 --> 00:34:12,879
decided to start doing a whole ton of

00:34:12,240 --> 00:34:15,119
work

00:34:12,879 --> 00:34:16,000
just now just as i'm doing this event

00:34:15,119 --> 00:34:18,560
but hopefully

00:34:16,000 --> 00:34:20,480
it's not too noisy um and thank you for

00:34:18,560 --> 00:34:23,200
your question brandi i think

00:34:20,480 --> 00:34:25,040
i mean there's so many things that i can

00:34:23,200 --> 00:34:25,599
say on on this topic but i'll start off

00:34:25,040 --> 00:34:28,879
by

00:34:25,599 --> 00:34:31,200
talking about um how

00:34:28,879 --> 00:34:32,879
we analyze the business model of google

00:34:31,200 --> 00:34:34,560
and facebook from a human rights

00:34:32,879 --> 00:34:36,639
perspective and and what

00:34:34,560 --> 00:34:37,599
what conclusions we reached and and then

00:34:36,639 --> 00:34:39,760
i'll move on

00:34:37,599 --> 00:34:41,200
a little bit to talk about what needs to

00:34:39,760 --> 00:34:41,919
be done because i think that's that's

00:34:41,200 --> 00:34:43,839
really the

00:34:41,919 --> 00:34:45,200
the crux of the of the issue here i

00:34:43,839 --> 00:34:46,079
think we all know what the problem is

00:34:45,200 --> 00:34:47,919
right but

00:34:46,079 --> 00:34:50,480
what do we do about it and and i'm going

00:34:47,919 --> 00:34:53,440
to suggest a a bit of a

00:34:50,480 --> 00:34:54,879
perhaps a radical um radical response to

00:34:53,440 --> 00:34:58,240
what needs to be done

00:34:54,879 --> 00:35:00,320
so in 2019 um we published a report

00:34:58,240 --> 00:35:02,720
called surveillance giants which

00:35:00,320 --> 00:35:04,400
um basically looked at the how the

00:35:02,720 --> 00:35:05,440
business model of google and facebook

00:35:04,400 --> 00:35:08,800
works

00:35:05,440 --> 00:35:10,720
and what the uh human rights risks um

00:35:08,800 --> 00:35:12,560
are in relation to the business model

00:35:10,720 --> 00:35:14,320
and what we

00:35:12,560 --> 00:35:16,240
analyzed was basically that these

00:35:14,320 --> 00:35:17,920
platforms are optimized

00:35:16,240 --> 00:35:20,240
they're designed to optimize three

00:35:17,920 --> 00:35:22,560
things how long they can keep us

00:35:20,240 --> 00:35:23,599
engaged um keep our eyes on on the

00:35:22,560 --> 00:35:26,800
screens

00:35:23,599 --> 00:35:28,400
how many users they can get and how much

00:35:26,800 --> 00:35:30,400
ad revenue they can generate

00:35:28,400 --> 00:35:32,160
from serving ads to people whose eyes

00:35:30,400 --> 00:35:34,480
are on those screens

00:35:32,160 --> 00:35:35,839
and this basically feeds the the

00:35:34,480 --> 00:35:38,079
underlying business model which is

00:35:35,839 --> 00:35:42,079
essentially to harvest and monetize

00:35:38,079 --> 00:35:44,480
our very intimate personal information

00:35:42,079 --> 00:35:45,599
and the fundamental characteristic is to

00:35:44,480 --> 00:35:47,200
aggregate vast

00:35:45,599 --> 00:35:49,440
amounts of data on people to keep them

00:35:47,200 --> 00:35:51,359
on the platform for as long as possible

00:35:49,440 --> 00:35:52,560
and use that data to infer incredibly

00:35:51,359 --> 00:35:55,040
detailed profile

00:35:52,560 --> 00:35:56,560
people's lives and behavior and then

00:35:55,040 --> 00:35:58,320
monetize this by selling those

00:35:56,560 --> 00:36:00,240
predictions those profiles to whoever

00:35:58,320 --> 00:36:03,680
wants to influence us

00:36:00,240 --> 00:36:06,079
and we warned in our report that the

00:36:03,680 --> 00:36:07,119
business model of facebook and google is

00:36:06,079 --> 00:36:08,720
not only

00:36:07,119 --> 00:36:10,240
inherently compatible with the right to

00:36:08,720 --> 00:36:12,079
privacy because

00:36:10,240 --> 00:36:13,599
in order to access those platforms you

00:36:12,079 --> 00:36:16,240
basically have to sign away

00:36:13,599 --> 00:36:17,119
that information these these platforms

00:36:16,240 --> 00:36:19,440
are not free

00:36:17,119 --> 00:36:21,040
um we pay with our very intimate

00:36:19,440 --> 00:36:23,680
personal data

00:36:21,040 --> 00:36:24,480
and our behavior online but it's not

00:36:23,680 --> 00:36:27,839
just about

00:36:24,480 --> 00:36:29,920
the right to privacy that initial um

00:36:27,839 --> 00:36:31,760
abuse of the right to privacy allows

00:36:29,920 --> 00:36:34,480
other abuses to happen

00:36:31,760 --> 00:36:37,119
specifically um in relation to our

00:36:34,480 --> 00:36:38,839
freedom of opinion freedom of thought

00:36:37,119 --> 00:36:40,720
the right to equality and

00:36:38,839 --> 00:36:42,160
non-discrimination and so when we're

00:36:40,720 --> 00:36:43,359
talking about misinformation

00:36:42,160 --> 00:36:44,800
disinformation

00:36:43,359 --> 00:36:47,359
it's really important to remember that

00:36:44,800 --> 00:36:49,119
the content that we see on social media

00:36:47,359 --> 00:36:51,040
is underpinned by this business model

00:36:49,119 --> 00:36:52,000
which is designed to attract and keep

00:36:51,040 --> 00:36:54,240
people's attention

00:36:52,000 --> 00:36:55,599
for as long as possible in order to get

00:36:54,240 --> 00:36:58,640
the most engagement

00:36:55,599 --> 00:37:00,400
clicks views shares etc um and

00:36:58,640 --> 00:37:01,040
algorithms on facebook determine what we

00:37:00,400 --> 00:37:04,000
see

00:37:01,040 --> 00:37:06,079
um on youtube algorithms on youtube

00:37:04,000 --> 00:37:09,119
determine what videos we see

00:37:06,079 --> 00:37:11,440
um and facebook and twitter algorithms

00:37:09,119 --> 00:37:12,880
decide what what we see on on our feeds

00:37:11,440 --> 00:37:15,839
um as well and very

00:37:12,880 --> 00:37:17,760
often these these algorithms amplify

00:37:15,839 --> 00:37:20,960
misinformation disinformation define

00:37:17,760 --> 00:37:24,640
divisive content they fuel racism um

00:37:20,960 --> 00:37:28,160
they they um uh they spread

00:37:24,640 --> 00:37:31,280
um very sort of uh sensationalist

00:37:28,160 --> 00:37:35,200
material and so

00:37:31,280 --> 00:37:38,400
tackling misinformation disinformation

00:37:35,200 --> 00:37:39,839
is is going to require a mix of of

00:37:38,400 --> 00:37:41,680
things and some of the things that

00:37:39,839 --> 00:37:44,000
jacqueline um mentioned

00:37:41,680 --> 00:37:45,920
are super important so digital and

00:37:44,000 --> 00:37:48,640
meteor media literacy

00:37:45,920 --> 00:37:50,240
taking down content but also being

00:37:48,640 --> 00:37:50,640
mindful that there is a risk there that

00:37:50,240 --> 00:37:52,320
if you

00:37:50,640 --> 00:37:54,000
if you take down too much content then

00:37:52,320 --> 00:37:55,200
you then infringe on people's freedom of

00:37:54,000 --> 00:37:58,400
expression

00:37:55,200 --> 00:38:01,280
um you know labeling tweets um

00:37:58,400 --> 00:38:02,880
and and um you know building the

00:38:01,280 --> 00:38:03,599
capacity of people to be able to be very

00:38:02,880 --> 00:38:05,760
critical

00:38:03,599 --> 00:38:07,359
about the the information that that you

00:38:05,760 --> 00:38:08,160
see i think that's really really

00:38:07,359 --> 00:38:10,400
important

00:38:08,160 --> 00:38:12,079
but that only goes so far right and i

00:38:10,400 --> 00:38:14,079
think we'll continue to see this

00:38:12,079 --> 00:38:16,720
this material spreading we'll continue

00:38:14,079 --> 00:38:20,000
to see voters being dissuaded from

00:38:16,720 --> 00:38:23,040
from from voting will continue to see

00:38:20,000 --> 00:38:26,480
um you know lies about

00:38:23,040 --> 00:38:29,359
vaccines etc uh and abusive

00:38:26,480 --> 00:38:30,160
and and and hateful content spread very

00:38:29,359 --> 00:38:33,440
fast

00:38:30,160 --> 00:38:36,079
and until we tackle

00:38:33,440 --> 00:38:37,680
the root causes of of that content and

00:38:36,079 --> 00:38:39,599
the root causes of how that content

00:38:37,680 --> 00:38:41,839
spreads which is essentially

00:38:39,599 --> 00:38:43,520
the business model and so i think when

00:38:41,839 --> 00:38:45,280
it comes to

00:38:43,520 --> 00:38:46,720
figuring out okay so what do we do about

00:38:45,280 --> 00:38:48,240
it um

00:38:46,720 --> 00:38:50,160
we first have to recognize that

00:38:48,240 --> 00:38:52,480
companies like facebook and google

00:38:50,160 --> 00:38:53,359
have grown in such size and power and

00:38:52,480 --> 00:38:56,880
have such

00:38:53,359 --> 00:38:58,880
market dominance that um

00:38:56,880 --> 00:39:00,960
they've been been allowed to sort of

00:38:58,880 --> 00:39:02,560
avoid any kind of state regulation

00:39:00,960 --> 00:39:06,400
over that business model for a very very

00:39:02,560 --> 00:39:08,640
long time and it's not until we tackle

00:39:06,400 --> 00:39:10,079
that that business model that we'll be

00:39:08,640 --> 00:39:11,599
able to then deal with these

00:39:10,079 --> 00:39:13,359
sort of systemic and root causes of hate

00:39:11,599 --> 00:39:14,800
speechless information and manipulation

00:39:13,359 --> 00:39:17,280
that we see flourish online

00:39:14,800 --> 00:39:18,320
so what do we need to do um i can very

00:39:17,280 --> 00:39:21,359
clearly tell you

00:39:18,320 --> 00:39:22,079
what we should absolutely not do um

00:39:21,359 --> 00:39:23,920
which is

00:39:22,079 --> 00:39:25,200
we should not rely on the companies to

00:39:23,920 --> 00:39:26,839
self-regulate because i

00:39:25,200 --> 00:39:29,200
time after time we've seen that

00:39:26,839 --> 00:39:30,320
self-regulation doesn't work and so we

00:39:29,200 --> 00:39:33,359
absolutely need

00:39:30,320 --> 00:39:33,839
um government regulation to step in and

00:39:33,359 --> 00:39:36,400
and

00:39:33,839 --> 00:39:37,599
and check the the the power of these big

00:39:36,400 --> 00:39:40,079
tech companies

00:39:37,599 --> 00:39:41,520
and so i i propose three three main

00:39:40,079 --> 00:39:44,480
things um

00:39:41,520 --> 00:39:45,920
and no one of these things is enough in

00:39:44,480 --> 00:39:47,839
and of itself it has to i think it's

00:39:45,920 --> 00:39:48,880
it's about having a smart mix of

00:39:47,839 --> 00:39:50,560
structural solutions

00:39:48,880 --> 00:39:52,160
that are really going to get to the to

00:39:50,560 --> 00:39:55,280
the bottom of this

00:39:52,160 --> 00:39:58,480
so first is is that the ability of of

00:39:55,280 --> 00:40:00,240
to to gather so much data and use that

00:39:58,480 --> 00:40:02,000
information to target ads of people i

00:40:00,240 --> 00:40:05,119
think has to be outlawed

00:40:02,000 --> 00:40:06,560
and this could look like banning their

00:40:05,119 --> 00:40:10,000
ability to collect people's

00:40:06,560 --> 00:40:11,760
data um banning the ability for

00:40:10,000 --> 00:40:14,160
for them to be able to target them with

00:40:11,760 --> 00:40:14,880
ads and there are several discussions at

00:40:14,160 --> 00:40:17,920
the moment

00:40:14,880 --> 00:40:19,839
um around banning targeted behavioral

00:40:17,920 --> 00:40:22,079
advertising particularly in the eu

00:40:19,839 --> 00:40:24,240
there's a coalition of meps

00:40:22,079 --> 00:40:25,280
um that's that's called essentially

00:40:24,240 --> 00:40:28,319
calling for this

00:40:25,280 --> 00:40:30,400
so so one of the one of the things um to

00:40:28,319 --> 00:40:32,960
tackle this issue could be

00:40:30,400 --> 00:40:34,720
banning um targeted ads but that's not

00:40:32,960 --> 00:40:36,160
enough right it needs

00:40:34,720 --> 00:40:38,240
that that's not going to solve the whole

00:40:36,160 --> 00:40:40,720
the whole problem the second

00:40:38,240 --> 00:40:41,440
um the second thing that needs to happen

00:40:40,720 --> 00:40:43,440
is

00:40:41,440 --> 00:40:45,200
modifying these recommendation

00:40:43,440 --> 00:40:47,680
algorithms which are designed

00:40:45,200 --> 00:40:48,960
for automated amplification and audience

00:40:47,680 --> 00:40:52,079
targeting

00:40:48,960 --> 00:40:54,720
and ensuring that these algorithms one

00:40:52,079 --> 00:40:55,520
are more transparent that we know what

00:40:54,720 --> 00:40:57,920
they're

00:40:55,520 --> 00:41:00,400
um what they're dialing up the the the

00:40:57,920 --> 00:41:01,920
content on and what they're dialing down

00:41:00,400 --> 00:41:03,920
ensuring that those algorithms

00:41:01,920 --> 00:41:04,880
de-prioritize disinformation and

00:41:03,920 --> 00:41:06,720
incendiary

00:41:04,880 --> 00:41:08,160
content just like facebook did ahead of

00:41:06,720 --> 00:41:10,800
the us elections

00:41:08,160 --> 00:41:12,800
um and and also giving users

00:41:10,800 --> 00:41:15,040
transparency about why they're seeing

00:41:12,800 --> 00:41:16,560
what they're seeing and giving them more

00:41:15,040 --> 00:41:18,400
control of what

00:41:16,560 --> 00:41:20,079
they can see and what they don't want to

00:41:18,400 --> 00:41:23,280
see and then finally

00:41:20,079 --> 00:41:24,160
it's also about data protection now in

00:41:23,280 --> 00:41:26,000
the eu we have

00:41:24,160 --> 00:41:27,200
very good very strong data protection

00:41:26,000 --> 00:41:29,760
regulations

00:41:27,200 --> 00:41:31,760
and the gdpr but we need to ensure that

00:41:29,760 --> 00:41:33,280
those are actually effectively enforced

00:41:31,760 --> 00:41:35,599
it's all good and well having very

00:41:33,280 --> 00:41:37,680
robust data protection frameworks

00:41:35,599 --> 00:41:39,200
but really the the developers in the

00:41:37,680 --> 00:41:42,000
detail and and what's needed

00:41:39,200 --> 00:41:43,280
is is to ensure that they are actually

00:41:42,000 --> 00:41:45,520
effectively

00:41:43,280 --> 00:41:47,119
um implemented and that people are able

00:41:45,520 --> 00:41:48,240
to get accountability for the way in

00:41:47,119 --> 00:41:50,880
which the data is used

00:41:48,240 --> 00:41:52,000
and misused and where there isn't strong

00:41:50,880 --> 00:41:54,720
data protection

00:41:52,000 --> 00:41:55,680
um frameworks we absolutely need to put

00:41:54,720 --> 00:41:57,599
those in place

00:41:55,680 --> 00:41:59,280
um because that that will be a step

00:41:57,599 --> 00:42:02,880
towards safeguarding people's

00:41:59,280 --> 00:42:07,440
um data and and personal information

00:42:02,880 --> 00:42:07,440
and um with that i will i will stop

00:42:08,640 --> 00:42:12,000
thank you so much rasha i stopped

00:42:10,560 --> 00:42:14,400
furious nodding when you said

00:42:12,000 --> 00:42:16,319
self-regulation is not enough

00:42:14,400 --> 00:42:17,599
so that's what i want to dive in on

00:42:16,319 --> 00:42:20,560
right now um

00:42:17,599 --> 00:42:21,119
and i think you raised so many really

00:42:20,560 --> 00:42:23,440
great

00:42:21,119 --> 00:42:25,040
practical um interventions that can be

00:42:23,440 --> 00:42:27,359
made i think there's also

00:42:25,040 --> 00:42:28,480
a discussion that we could have as well

00:42:27,359 --> 00:42:31,359
about

00:42:28,480 --> 00:42:32,800
governance at large so a lot of

00:42:31,359 --> 00:42:34,400
regulation that's being proposed you

00:42:32,800 --> 00:42:36,319
know aims to get it sort of

00:42:34,400 --> 00:42:37,760
one type of feature or way of working

00:42:36,319 --> 00:42:39,359
and you raise a really good question

00:42:37,760 --> 00:42:41,359
about is this whole business model

00:42:39,359 --> 00:42:42,960
flawed and problematic do we need

00:42:41,359 --> 00:42:44,560
an overhaul and how we're thinking about

00:42:42,960 --> 00:42:46,160
how uh the

00:42:44,560 --> 00:42:48,319
governance of our online spaces is

00:42:46,160 --> 00:42:50,480
structured so i i would invite

00:42:48,319 --> 00:42:51,599
the panelists now we move into the more

00:42:50,480 --> 00:42:54,640
conversational

00:42:51,599 --> 00:42:57,200
uh part of this to to respond to uh to

00:42:54,640 --> 00:42:57,200
any of that

00:42:57,599 --> 00:43:01,200
i mean i would i would definitely agree

00:42:59,920 --> 00:43:04,960
it's certainly

00:43:01,200 --> 00:43:07,119
um i've i've had i'm like a cat i've had

00:43:04,960 --> 00:43:09,599
many professional lives but

00:43:07,119 --> 00:43:11,119
you know in one of the iterations of me

00:43:09,599 --> 00:43:14,319
trying to look for a paycheck

00:43:11,119 --> 00:43:17,280
i worked on ai uh regulation

00:43:14,319 --> 00:43:18,800
um in the in the u.s so at the federal

00:43:17,280 --> 00:43:20,960
level and when we were looking at the

00:43:18,800 --> 00:43:23,040
deep fakes accountability act we really

00:43:20,960 --> 00:43:26,319
were getting to this

00:43:23,040 --> 00:43:28,079
question of how can we find how can we

00:43:26,319 --> 00:43:29,200
write statute that is going to regulate

00:43:28,079 --> 00:43:32,240
the internet and

00:43:29,200 --> 00:43:35,280
came to what russia described as um

00:43:32,240 --> 00:43:36,960
in terms of labeling and

00:43:35,280 --> 00:43:39,359
the reason that we thought labeling

00:43:36,960 --> 00:43:42,240
would be a powerful first step

00:43:39,359 --> 00:43:44,640
was really raising the idea that there

00:43:42,240 --> 00:43:47,480
is misleading content online

00:43:44,640 --> 00:43:48,720
because we're speaking about kind of

00:43:47,480 --> 00:43:51,599
00:43:48,720 --> 00:43:53,119
through to 2019 and it seems really

00:43:51,599 --> 00:43:55,839
ridiculous

00:43:53,119 --> 00:43:57,920
now but then you know pre-cambridge

00:43:55,839 --> 00:43:58,720
analytica the idea that the internet

00:43:57,920 --> 00:44:01,839
could be used

00:43:58,720 --> 00:44:02,240
to not tell us the truth was was felt to

00:44:01,839 --> 00:44:05,599
be

00:44:02,240 --> 00:44:07,520
um somewhat of a heresy

00:44:05,599 --> 00:44:09,200
and then this recent article that's come

00:44:07,520 --> 00:44:12,560
out here in the us

00:44:09,200 --> 00:44:14,880
by um karen hoe at tech review which was

00:44:12,560 --> 00:44:15,920
the culmination of what she says are

00:44:14,880 --> 00:44:20,000
over two

00:44:15,920 --> 00:44:22,560
dozen interviews with facebook um

00:44:20,000 --> 00:44:23,280
within facebook to find that mark

00:44:22,560 --> 00:44:25,920
zuckerberg

00:44:23,280 --> 00:44:26,319
according to that incredible reporting

00:44:25,920 --> 00:44:30,160
was

00:44:26,319 --> 00:44:32,079
always going to prioritize engagement

00:44:30,160 --> 00:44:34,000
and so that one of the case studies she

00:44:32,079 --> 00:44:34,480
looks at are the decisions made up into

00:44:34,000 --> 00:44:36,560
the

00:44:34,480 --> 00:44:38,079
the lead of myanmar and the decision

00:44:36,560 --> 00:44:40,000
that was made well there were two

00:44:38,079 --> 00:44:40,640
decisions made within the company number

00:44:40,000 --> 00:44:43,599
one

00:44:40,640 --> 00:44:44,240
that this ai ethics framing which i rail

00:44:43,599 --> 00:44:46,160
against

00:44:44,240 --> 00:44:48,079
because i think that you know we have

00:44:46,160 --> 00:44:48,960
all of these frameworks and all of this

00:44:48,079 --> 00:44:50,480
racism

00:44:48,960 --> 00:44:53,520
and let's get rid of the racism and the

00:44:50,480 --> 00:44:54,480
sexism and framework yourselves to death

00:44:53,520 --> 00:44:56,880
but not

00:44:54,480 --> 00:44:59,040
at the expense of my humanity and my

00:44:56,880 --> 00:45:00,079
safety online we need actual enforcement

00:44:59,040 --> 00:45:02,000
structures

00:45:00,079 --> 00:45:04,079
but there was this decision to create an

00:45:02,000 --> 00:45:06,560
ai responsible ai

00:45:04,079 --> 00:45:07,839
framing within the company but not to

00:45:06,560 --> 00:45:09,839
have a team

00:45:07,839 --> 00:45:10,960
that specifically went after hate speech

00:45:09,839 --> 00:45:13,440
so as the

00:45:10,960 --> 00:45:15,520
situation got worse in myanmar there was

00:45:13,440 --> 00:45:17,680
literally nobody in the company

00:45:15,520 --> 00:45:19,040
to stop that and that was by design

00:45:17,680 --> 00:45:19,359
because what they're really interested

00:45:19,040 --> 00:45:22,400
in

00:45:19,359 --> 00:45:25,200
growth so at that point we have to

00:45:22,400 --> 00:45:26,319
really go to the business model is this

00:45:25,200 --> 00:45:28,800
a business model

00:45:26,319 --> 00:45:29,440
in which we can have not just human

00:45:28,800 --> 00:45:31,920
rights but

00:45:29,440 --> 00:45:33,200
justice and if you're coming from a

00:45:31,920 --> 00:45:36,000
justice framework

00:45:33,200 --> 00:45:38,640
and we certainly um ai for the people

00:45:36,000 --> 00:45:40,800
are a racial justice organization

00:45:38,640 --> 00:45:42,480
we want to rip things down and start

00:45:40,800 --> 00:45:45,359
anew we want to look

00:45:42,480 --> 00:45:47,599
at laws policies and practices that do

00:45:45,359 --> 00:45:48,240
not serve the weakest whoever those we

00:45:47,599 --> 00:45:51,280
could

00:45:48,240 --> 00:45:53,119
whomever they are so we don't have to

00:45:51,280 --> 00:45:55,520
experience an oppression

00:45:53,119 --> 00:45:56,960
to be in deep solidarity and to build

00:45:55,520 --> 00:46:00,000
power against that

00:45:56,960 --> 00:46:03,520
to resist right that's our position

00:46:00,000 --> 00:46:06,160
we we are definitely in a space where

00:46:03,520 --> 00:46:09,040
we're thinking well is it antitrust

00:46:06,160 --> 00:46:10,960
is it not allowing transnational

00:46:09,040 --> 00:46:13,280
companies to exist i mean

00:46:10,960 --> 00:46:14,240
what what what lever what levers can we

00:46:13,280 --> 00:46:15,680
pull

00:46:14,240 --> 00:46:17,440
to make sure that the information that

00:46:15,680 --> 00:46:20,560
we receive does

00:46:17,440 --> 00:46:22,079
in fact serve us and um how much more

00:46:20,560 --> 00:46:24,480
evidence do we need

00:46:22,079 --> 00:46:25,359
and do we even need to be held hostage i

00:46:24,480 --> 00:46:27,920
mean these are

00:46:25,359 --> 00:46:29,440
these are very privileged thoughts right

00:46:27,920 --> 00:46:32,079
i'm zambian

00:46:29,440 --> 00:46:33,920
some of my zambian family have never

00:46:32,079 --> 00:46:36,560
forgiven me for stop using

00:46:33,920 --> 00:46:38,400
because i stopped using facebook because

00:46:36,560 --> 00:46:40,800
that is their

00:46:38,400 --> 00:46:41,680
that is their way into the internet and

00:46:40,800 --> 00:46:43,680
so we're now

00:46:41,680 --> 00:46:45,359
having to think about well do you have

00:46:43,680 --> 00:46:48,079
signal and do you have these other

00:46:45,359 --> 00:46:50,000
platforms by which by which way i can

00:46:48,079 --> 00:46:52,560
communicate with you for free

00:46:50,000 --> 00:46:54,480
um that i'm definitely in deep support

00:46:52,560 --> 00:46:57,839
and i definitely wouldn't believe

00:46:54,480 --> 00:47:01,200
anything that you know jack or sergey or

00:46:57,839 --> 00:47:02,880
uh mark were telling me i i i want

00:47:01,200 --> 00:47:03,200
justice and i don't believe that they

00:47:02,880 --> 00:47:07,200
had

00:47:03,200 --> 00:47:07,200
they're not incentivized to to provide

00:47:08,839 --> 00:47:11,839
them

00:47:13,359 --> 00:47:18,560
um yeah i totally totally agree that uh

00:47:16,880 --> 00:47:20,400
if we if we accept that these companies

00:47:18,560 --> 00:47:22,160
are doing harm

00:47:20,400 --> 00:47:23,599
and we look at what they've tried to do

00:47:22,160 --> 00:47:26,720
already in terms of

00:47:23,599 --> 00:47:28,079
you know improving privacy or some form

00:47:26,720 --> 00:47:30,720
of content moderation

00:47:28,079 --> 00:47:32,000
or some kind of ai ethics we're still

00:47:30,720 --> 00:47:32,640
seeing the harm so clearly what they're

00:47:32,000 --> 00:47:35,599
doing

00:47:32,640 --> 00:47:37,040
hasn't gone far enough and what what is

00:47:35,599 --> 00:47:38,640
the thing that they haven't been willing

00:47:37,040 --> 00:47:41,520
to touch it's the thing that's

00:47:38,640 --> 00:47:42,480
part of their business this exploitation

00:47:41,520 --> 00:47:45,680
of data

00:47:42,480 --> 00:47:47,440
and targeted advertising and

00:47:45,680 --> 00:47:50,319
essentially these other forms of

00:47:47,440 --> 00:47:52,880
intervention don't seem different from

00:47:50,319 --> 00:47:54,000
like lobbying really where if these

00:47:52,880 --> 00:47:55,200
companies are saying

00:47:54,000 --> 00:47:57,280
you know we can take care of it

00:47:55,200 --> 00:47:58,960
ourselves don't worry and they have the

00:47:57,280 --> 00:48:01,280
ear or the policy makers

00:47:58,960 --> 00:48:03,200
uh they're they're really just trying to

00:48:01,280 --> 00:48:05,599
kind of

00:48:03,200 --> 00:48:08,079
distract from what what is the real

00:48:05,599 --> 00:48:08,079
issue here

00:48:10,880 --> 00:48:14,720
yeah i think i agree with the rest um i

00:48:13,440 --> 00:48:16,400
think there's enough evidence and

00:48:14,720 --> 00:48:18,400
analysis that's been done to confirm

00:48:16,400 --> 00:48:19,440
that you know safeguards and regulation

00:48:18,400 --> 00:48:21,520
won't be enough

00:48:19,440 --> 00:48:22,800
um unless we can imagine what equity

00:48:21,520 --> 00:48:25,440
actually looks like

00:48:22,800 --> 00:48:27,680
in these evolving digital spaces i think

00:48:25,440 --> 00:48:29,040
a lot about parlor for a lot of reasons

00:48:27,680 --> 00:48:32,000
leading up to

00:48:29,040 --> 00:48:33,760
the election and and the insurrection

00:48:32,000 --> 00:48:35,680
and how things went down there right um

00:48:33,760 --> 00:48:37,040
and how that freedom of speech space

00:48:35,680 --> 00:48:39,599
that whether knowingly or not

00:48:37,040 --> 00:48:40,880
i believe knowingly allowed hateful uh

00:48:39,599 --> 00:48:42,400
racist um

00:48:40,880 --> 00:48:44,720
content to spread and so you could uh

00:48:42,400 --> 00:48:45,200
attribute that to moderation guidelines

00:48:44,720 --> 00:48:46,880
or

00:48:45,200 --> 00:48:48,559
you could blame section 230 for more

00:48:46,880 --> 00:48:49,680
removing you know uh platform

00:48:48,559 --> 00:48:52,160
accountability

00:48:49,680 --> 00:48:53,760
but uh for-profit organizations i think

00:48:52,160 --> 00:48:54,960
will always have the dollar in mind

00:48:53,760 --> 00:48:56,000
because that's the nature of the

00:48:54,960 --> 00:48:58,880
operation

00:48:56,000 --> 00:49:00,480
um so if you could ever put a price tag

00:48:58,880 --> 00:49:02,240
on public interest would that change the

00:49:00,480 --> 00:49:05,839
way you regulate and moderate

00:49:02,240 --> 00:49:05,839
um so that's how i think

00:49:06,559 --> 00:49:10,800
yeah absolutely and i think a lot about

00:49:08,400 --> 00:49:13,839
this the the question of

00:49:10,800 --> 00:49:15,040
enforcement of policies um because i

00:49:13,839 --> 00:49:17,839
think that when we

00:49:15,040 --> 00:49:19,839
when we look at just the policies of

00:49:17,839 --> 00:49:21,200
many platforms alone you might might

00:49:19,839 --> 00:49:23,200
read it and think at least

00:49:21,200 --> 00:49:24,880
first for some of these issues you might

00:49:23,200 --> 00:49:26,960
read it and think yeah that sounds

00:49:24,880 --> 00:49:28,000
good and then you all are researchers

00:49:26,960 --> 00:49:30,079
who are you know

00:49:28,000 --> 00:49:31,920
actually in looking at what's happening

00:49:30,079 --> 00:49:32,720
and you're like but this is not at all

00:49:31,920 --> 00:49:34,960
reflective

00:49:32,720 --> 00:49:36,559
of you know there's a huge problem with

00:49:34,960 --> 00:49:37,359
enforcement huge problem with

00:49:36,559 --> 00:49:40,480
enforcement

00:49:37,359 --> 00:49:42,160
um especially for non-english content

00:49:40,480 --> 00:49:44,640
for example and so i'm wondering

00:49:42,160 --> 00:49:45,680
um do you all think i suppose it's never

00:49:44,640 --> 00:49:47,200
an either or

00:49:45,680 --> 00:49:49,359
but how do you think about the the

00:49:47,200 --> 00:49:50,319
question of of enforcement of policies

00:49:49,359 --> 00:49:53,680
versus you know

00:49:50,319 --> 00:49:57,040
uh regulation so i'm

00:49:53,680 --> 00:49:59,440
actually going through this right now um

00:49:57,040 --> 00:50:00,640
in two ways personally as somebody who's

00:49:59,440 --> 00:50:02,640
being attacked

00:50:00,640 --> 00:50:04,880
and and having the privilege to access

00:50:02,640 --> 00:50:08,240
the platform right most people

00:50:04,880 --> 00:50:09,680
have to make an in-platform complaint i

00:50:08,240 --> 00:50:11,680
actually have the privilege of

00:50:09,680 --> 00:50:13,599
emailing a person who has enough power

00:50:11,680 --> 00:50:16,319
within the company who can

00:50:13,599 --> 00:50:17,200
look at look at my issue and in initial

00:50:16,319 --> 00:50:20,160
conversations

00:50:17,200 --> 00:50:21,280
even with twitter i found two things uh

00:50:20,160 --> 00:50:24,400
legislative

00:50:21,280 --> 00:50:24,960
capture so the person that i am speaking

00:50:24,400 --> 00:50:26,800
to

00:50:24,960 --> 00:50:28,559
is also somebody that had worked at the

00:50:26,800 --> 00:50:29,440
congressional black caucus at the same

00:50:28,559 --> 00:50:32,480
time

00:50:29,440 --> 00:50:35,520
that i was uh doing work on the hill

00:50:32,480 --> 00:50:37,280
and was on my side at that point in

00:50:35,520 --> 00:50:40,240
terms of regulation and is now

00:50:37,280 --> 00:50:41,119
in-house in the company protecting the

00:50:40,240 --> 00:50:43,119
very

00:50:41,119 --> 00:50:44,240
um you know the very system that we were

00:50:43,119 --> 00:50:47,119
looking to overhaul

00:50:44,240 --> 00:50:48,880
only years later with and have taken

00:50:47,119 --> 00:50:52,079
those relationships

00:50:48,880 --> 00:50:54,160
um into that space so the

00:50:52,079 --> 00:50:55,520
number one number two when we looked at

00:50:54,160 --> 00:50:56,720
content moderation and when we were

00:50:55,520 --> 00:50:59,520
looking at takedown

00:50:56,720 --> 00:51:00,319
because um ai for the people is a really

00:50:59,520 --> 00:51:03,280
really small

00:51:00,319 --> 00:51:04,880
organization like we roar loud on the

00:51:03,280 --> 00:51:05,760
internet but there's actually not very

00:51:04,880 --> 00:51:08,079
many of us

00:51:05,760 --> 00:51:08,800
but we work with larger partners and our

00:51:08,079 --> 00:51:12,400
partner

00:51:08,800 --> 00:51:13,599
in our information integrity work

00:51:12,400 --> 00:51:17,119
which is where our distant

00:51:13,599 --> 00:51:19,920
misinformation work really lies was

00:51:17,119 --> 00:51:21,520
moveon.org in the last election cycle

00:51:19,920 --> 00:51:22,000
and so we were working with them and

00:51:21,520 --> 00:51:24,319
using

00:51:22,000 --> 00:51:25,680
their might really to force this

00:51:24,319 --> 00:51:28,559
conversation

00:51:25,680 --> 00:51:29,040
with platforms and we found that not

00:51:28,559 --> 00:51:31,839
only

00:51:29,040 --> 00:51:33,520
are the content moderation was so

00:51:31,839 --> 00:51:36,000
focused on harm

00:51:33,520 --> 00:51:37,119
like somebody actually had to be doxxed

00:51:36,000 --> 00:51:40,960
or somebody

00:51:37,119 --> 00:51:43,440
had to there had to be this bodily harm

00:51:40,960 --> 00:51:44,079
that there wasn't this prevention this

00:51:43,440 --> 00:51:47,599
harm

00:51:44,079 --> 00:51:50,880
reduction lens and kind of

00:51:47,599 --> 00:51:52,400
so we were legislating language that

00:51:50,880 --> 00:51:55,119
they had written

00:51:52,400 --> 00:51:57,040
to to maintain engagement which was

00:51:55,119 --> 00:51:58,400
incredibly frustrating and then the

00:51:57,040 --> 00:52:01,440
third thing i would say

00:51:58,400 --> 00:52:02,480
is that racism is not seen as a harm and

00:52:01,440 --> 00:52:04,160
that is

00:52:02,480 --> 00:52:05,599
something that when you are talking to

00:52:04,160 --> 00:52:09,359
people in

00:52:05,599 --> 00:52:11,760
pla on platform you really have to

00:52:09,359 --> 00:52:13,520
justify and this is and this isn't just

00:52:11,760 --> 00:52:15,119
hire more black people because it was

00:52:13,520 --> 00:52:16,000
black people telling me that this wasn't

00:52:15,119 --> 00:52:19,440
a harm

00:52:16,000 --> 00:52:22,319
and so it's like what is a harm

00:52:19,440 --> 00:52:23,280
who has the right to be harmed um we

00:52:22,319 --> 00:52:26,240
were finding

00:52:23,280 --> 00:52:27,040
similar reports with um you know our

00:52:26,240 --> 00:52:30,160
comrades

00:52:27,040 --> 00:52:32,079
um who are uh non-binary and trans

00:52:30,160 --> 00:52:33,200
our comrades who are people living with

00:52:32,079 --> 00:52:35,359
disabilities

00:52:33,200 --> 00:52:36,559
you know the most marginalized of the

00:52:35,359 --> 00:52:38,880
marginalized

00:52:36,559 --> 00:52:40,880
we were told that this wasn't really a

00:52:38,880 --> 00:52:43,280
thing which is just

00:52:40,880 --> 00:52:44,079
gaslighting by these huge powerful

00:52:43,280 --> 00:52:46,160
companies

00:52:44,079 --> 00:52:48,800
and you're dealing with these very very

00:52:46,160 --> 00:52:51,280
small uh grassroots groups

00:52:48,800 --> 00:52:53,760
and you know certainly in my research

00:52:51,280 --> 00:52:56,640
work i have looked to these questions

00:52:53,760 --> 00:52:57,599
of what does it mean to have a racially

00:52:56,640 --> 00:52:59,920
literate

00:52:57,599 --> 00:53:00,880
tech sector where they understand that

00:52:59,920 --> 00:53:03,280
racism

00:53:00,880 --> 00:53:04,800
is has a history and it's and it's

00:53:03,280 --> 00:53:09,119
really

00:53:04,800 --> 00:53:11,520
steeped in uh capitalism capitalistic

00:53:09,119 --> 00:53:12,640
gain and so if you're dealing with

00:53:11,520 --> 00:53:14,880
capitalistic

00:53:12,640 --> 00:53:15,920
uh companies like facebook twitter

00:53:14,880 --> 00:53:17,680
whatever

00:53:15,920 --> 00:53:19,760
and you're telling them the harm is

00:53:17,680 --> 00:53:21,920
rooted in in this thing that i'm telling

00:53:19,760 --> 00:53:24,880
you and they don't want to believe it

00:53:21,920 --> 00:53:25,200
then there's a fragility that comes with

00:53:24,880 --> 00:53:26,960
me

00:53:25,200 --> 00:53:28,640
i'm not racist how could i possibly be

00:53:26,960 --> 00:53:31,040
racist i mean

00:53:28,640 --> 00:53:32,640
i have a whole report where people were

00:53:31,040 --> 00:53:34,559
coming back and telling us that they had

00:53:32,640 --> 00:53:37,200
voted for barack obama which was really

00:53:34,559 --> 00:53:39,119
cool but it wasn't on the survey

00:53:37,200 --> 00:53:41,200
it wasn't on the survey it was being

00:53:39,119 --> 00:53:43,520
used as a way to kind of deflect

00:53:41,200 --> 00:53:45,119
um this idea that we were calling them

00:53:43,520 --> 00:53:48,160
racist and then not having

00:53:45,119 --> 00:53:50,160
effective action planning and allowing

00:53:48,160 --> 00:53:53,359
that one of the actions on the table

00:53:50,160 --> 00:53:56,240
should be burn this down and

00:53:53,359 --> 00:53:56,720
you know just uh i could talk forever

00:53:56,240 --> 00:53:59,200
i'm not

00:53:56,720 --> 00:54:01,440
i'm i'm gonna yield i'm getting mad but

00:53:59,200 --> 00:54:04,000
you know there has to be there has to be

00:54:01,440 --> 00:54:07,760
a better way we have to be able to have

00:54:04,000 --> 00:54:10,319
technology and have safe

00:54:07,760 --> 00:54:11,520
online spaces as well i just refuse to

00:54:10,319 --> 00:54:15,040
yield that that's not

00:54:11,520 --> 00:54:15,040
that that cannot be the case

00:54:16,079 --> 00:54:19,760
completely agree i'll just jump in

00:54:17,359 --> 00:54:21,200
quickly to say that i i it's a false

00:54:19,760 --> 00:54:25,839
choice right to say that

00:54:21,200 --> 00:54:27,839
um either we submit ourselves to

00:54:25,839 --> 00:54:29,200
pervasive surveillance online or we

00:54:27,839 --> 00:54:30,880
can't access um

00:54:29,200 --> 00:54:33,359
platforms like like google and facebook

00:54:30,880 --> 00:54:35,920
that's that's not a legitimate choice

00:54:33,359 --> 00:54:36,799
and um it's based on a very deliberate

00:54:35,920 --> 00:54:39,359
business decision

00:54:36,799 --> 00:54:40,480
by the companies themselves to maximize

00:54:39,359 --> 00:54:43,680
their profits

00:54:40,480 --> 00:54:45,359
and i'm i'm a firm believer in in people

00:54:43,680 --> 00:54:47,200
power and i think

00:54:45,359 --> 00:54:48,400
when it comes to enforcement of policies

00:54:47,200 --> 00:54:50,720
and when it comes to

00:54:48,400 --> 00:54:52,480
this radical rethink of the business

00:54:50,720 --> 00:54:55,839
model of the internet

00:54:52,480 --> 00:54:57,359
um rethinking and reimagining is only

00:54:55,839 --> 00:55:00,319
part of the work right the

00:54:57,359 --> 00:55:00,799
the remaking the actual doing is is

00:55:00,319 --> 00:55:02,160
what's

00:55:00,799 --> 00:55:04,160
what's gonna make the difference and

00:55:02,160 --> 00:55:06,079
that's gonna take time it's gonna take

00:55:04,160 --> 00:55:08,559
time and it's gonna take

00:55:06,079 --> 00:55:09,599
um people power honestly um you know

00:55:08,559 --> 00:55:12,160
amnesty

00:55:09,599 --> 00:55:13,280
we did we did a poll in 10 countries in

00:55:12,160 --> 00:55:14,799
00:55:13,280 --> 00:55:16,400
um just just after we launched our

00:55:14,799 --> 00:55:19,440
report to

00:55:16,400 --> 00:55:20,319
um look into how people feel about the

00:55:19,440 --> 00:55:23,359
corporate power

00:55:20,319 --> 00:55:24,480
that the power of big tech and and how

00:55:23,359 --> 00:55:26,000
their data is how much their data is

00:55:24,480 --> 00:55:27,520
collected and how it's how it's used and

00:55:26,000 --> 00:55:28,720
whether that concerns them and seven out

00:55:27,520 --> 00:55:30,480
of ten people

00:55:28,720 --> 00:55:32,319
from countries from all regions of the

00:55:30,480 --> 00:55:34,000
world said that they felt

00:55:32,319 --> 00:55:35,599
concerned and uncomfortable about how

00:55:34,000 --> 00:55:37,839
much data is being collected on

00:55:35,599 --> 00:55:39,520
about them and how that data is used and

00:55:37,839 --> 00:55:43,920
a recent poll by

00:55:39,520 --> 00:55:46,160
accountable tech um 81

00:55:43,920 --> 00:55:47,520
of of americans support banning

00:55:46,160 --> 00:55:49,280
companies from collecting

00:55:47,520 --> 00:55:50,640
their personal data and being used to

00:55:49,280 --> 00:55:52,960
target being used

00:55:50,640 --> 00:55:54,400
to use that data to target them with ads

00:55:52,960 --> 00:55:55,920
and they'd rather keep their personal

00:55:54,400 --> 00:55:58,319
data private even if it means

00:55:55,920 --> 00:55:59,839
seeing less less relevant ads and so i

00:55:58,319 --> 00:56:00,319
think there is this momentum there is

00:55:59,839 --> 00:56:03,359
this

00:56:00,319 --> 00:56:06,400
this sort of awakening this awareness

00:56:03,359 --> 00:56:06,720
building about the power of big tech and

00:56:06,400 --> 00:56:08,640
and

00:56:06,720 --> 00:56:10,319
and people don't don't want the the

00:56:08,640 --> 00:56:11,119
current model they they want a different

00:56:10,319 --> 00:56:12,880
model

00:56:11,119 --> 00:56:14,640
um and in order to create a different

00:56:12,880 --> 00:56:16,160
model that truly serves the people

00:56:14,640 --> 00:56:18,720
rather than the pockets of

00:56:16,160 --> 00:56:20,400
a few companies in silicon valley we

00:56:18,720 --> 00:56:21,200
need to make sure that that internet is

00:56:20,400 --> 00:56:24,240
designed

00:56:21,200 --> 00:56:27,839
by the people with meaningful um

00:56:24,240 --> 00:56:29,920
meaningful uh collaboration and and

00:56:27,839 --> 00:56:31,839
cooperation and you know matala made a

00:56:29,920 --> 00:56:33,839
really good point earlier

00:56:31,839 --> 00:56:35,119
about the fact that in in some parts of

00:56:33,839 --> 00:56:37,440
the world

00:56:35,119 --> 00:56:39,520
facebook is the internet you know that's

00:56:37,440 --> 00:56:42,079
that that is the inter that's how people

00:56:39,520 --> 00:56:42,799
connect on online and where are those

00:56:42,079 --> 00:56:44,559
people where are

00:56:42,799 --> 00:56:47,760
those people in the debates about what a

00:56:44,559 --> 00:56:49,680
future rights respecting data ecosystem

00:56:47,760 --> 00:56:51,599
should look like they're not present

00:56:49,680 --> 00:56:54,640
anywhere because the the the

00:56:51,599 --> 00:56:58,000
the the the

00:56:54,640 --> 00:57:01,920
the debate is being set by

00:56:58,000 --> 00:57:03,520
the uh the powerful few the powerful few

00:57:01,920 --> 00:57:05,040
silicon valley companies who

00:57:03,520 --> 00:57:07,520
who set the limits on what is and what

00:57:05,040 --> 00:57:10,160
is what isn't acceptable in terms of

00:57:07,520 --> 00:57:11,520
privacy in terms of interaction in terms

00:57:10,160 --> 00:57:12,400
of content that we're allowed to see

00:57:11,520 --> 00:57:14,079
online

00:57:12,400 --> 00:57:15,599
and and that really needs a radical

00:57:14,079 --> 00:57:18,960
overhaul and to rethink

00:57:15,599 --> 00:57:21,440
um because you know we then

00:57:18,960 --> 00:57:22,400
leave people behind and and there is a

00:57:21,440 --> 00:57:25,359
disproportionate

00:57:22,400 --> 00:57:27,280
impact of um you know the business model

00:57:25,359 --> 00:57:28,559
data colonialism data harvesting by

00:57:27,280 --> 00:57:32,799
people in the global south

00:57:28,559 --> 00:57:32,799
and and that has to change

00:57:34,960 --> 00:57:38,559
thanks and i think something that um i

00:57:37,440 --> 00:57:40,079
picked up on that

00:57:38,559 --> 00:57:41,839
you talked about a little bit russia and

00:57:40,079 --> 00:57:44,079
also mutually you mentioned

00:57:41,839 --> 00:57:45,040
was this idea that uh of you know we

00:57:44,079 --> 00:57:47,200
need to do something

00:57:45,040 --> 00:57:48,400
and also that there are people within

00:57:47,200 --> 00:57:51,359
the tech industry

00:57:48,400 --> 00:57:52,559
uh who are you know trying maybe trying

00:57:51,359 --> 00:57:54,559
to make change for the better

00:57:52,559 --> 00:57:56,559
or whose jobs are positioned as you know

00:57:54,559 --> 00:57:58,400
that they are uh

00:57:56,559 --> 00:58:00,559
you know attempting to intervene in some

00:57:58,400 --> 00:58:02,799
sense so and obviously this is

00:58:00,559 --> 00:58:04,319
in discussion a lot right now and i know

00:58:02,799 --> 00:58:05,280
that a few of you are involved in the

00:58:04,319 --> 00:58:07,200
campaign

00:58:05,280 --> 00:58:08,559
launch of the no tech for tyrants um

00:58:07,200 --> 00:58:10,559
recruit me not campaign

00:58:08,559 --> 00:58:12,319
so i was hoping that um that you could

00:58:10,559 --> 00:58:14,799
talk a little bit about that movement

00:58:12,319 --> 00:58:17,040
like what it means for ethics research

00:58:14,799 --> 00:58:20,160
within the tech industry itself um

00:58:17,040 --> 00:58:21,680
and really like i wanna dig in on what

00:58:20,160 --> 00:58:23,760
what should be the role of that this

00:58:21,680 --> 00:58:26,000
kind of work and in particular

00:58:23,760 --> 00:58:28,160
what conditions are required for there

00:58:26,000 --> 00:58:31,200
to be genuine accountability coming

00:58:28,160 --> 00:58:31,200
out of the industry itself

00:58:32,000 --> 00:58:35,599
oh gosh i'm so glad that you gave a

00:58:33,760 --> 00:58:36,319
shout out to the recruitment campaign i

00:58:35,599 --> 00:58:37,839
think it's

00:58:36,319 --> 00:58:40,000
such an important thing that's happening

00:58:37,839 --> 00:58:42,079
right now um

00:58:40,000 --> 00:58:43,839
i i guess that there are there are two

00:58:42,079 --> 00:58:45,920
parts of this that that

00:58:43,839 --> 00:58:47,119
mutually and russia both touched on one

00:58:45,920 --> 00:58:48,720
is the burn down

00:58:47,119 --> 00:58:50,400
like how do we actually burn it down and

00:58:48,720 --> 00:58:51,040
then then what happens afterwards like

00:58:50,400 --> 00:58:52,799
this

00:58:51,040 --> 00:58:55,200
envisioning of alternative techno

00:58:52,799 --> 00:58:58,480
futures and

00:58:55,200 --> 00:58:59,520
i think that you know the revolution is

00:58:58,480 --> 00:59:03,040
not going to happen

00:58:59,520 --> 00:59:04,319
from ai ethicists in companies and

00:59:03,040 --> 00:59:06,960
the revolution is not going to happen

00:59:04,319 --> 00:59:09,440
from ai regulators

00:59:06,960 --> 00:59:11,200
but as russia points out there's a lot

00:59:09,440 --> 00:59:12,000
of potential here for collective action

00:59:11,200 --> 00:59:13,200
like 81

00:59:12,000 --> 00:59:14,960
i hadn't heard that statistic but that's

00:59:13,200 --> 00:59:15,599
incredible 81 of people in the united

00:59:14,960 --> 00:59:17,680
states

00:59:15,599 --> 00:59:18,799
thinking that data collection should

00:59:17,680 --> 00:59:21,359
should be banned

00:59:18,799 --> 00:59:22,880
uh this isn't like a republican versus

00:59:21,359 --> 00:59:23,760
democrat issue i mean i think there's a

00:59:22,880 --> 00:59:26,400
lot of

00:59:23,760 --> 00:59:26,960
opportunity here for unlikely coalitions

00:59:26,400 --> 00:59:29,040
uh

00:59:26,960 --> 00:59:30,480
across party lines in the in the united

00:59:29,040 --> 00:59:32,640
states and

00:59:30,480 --> 00:59:34,480
to to come back a little bit to the

00:59:32,640 --> 00:59:37,119
question you asked about enforcement

00:59:34,480 --> 00:59:38,960
like i think that regulatory enforcement

00:59:37,119 --> 00:59:41,440
is it's not gonna be enough because

00:59:38,960 --> 00:59:43,119
these these movements like the the no

00:59:41,440 --> 00:59:43,920
tech for tyrants and their recruitment

00:59:43,119 --> 00:59:45,359
campaign

00:59:43,920 --> 00:59:47,680
they're an example of the type of

00:59:45,359 --> 00:59:50,079
collective action that that both

00:59:47,680 --> 00:59:51,520
helps hold governments accountable when

00:59:50,079 --> 00:59:53,200
when they're when they're not

00:59:51,520 --> 00:59:55,119
you know doing the enforcement part and

00:59:53,200 --> 00:59:57,920
holds companies accountable

00:59:55,119 --> 00:59:58,960
when they're just doing what they do and

00:59:57,920 --> 01:00:01,520
we've seen these type of

00:59:58,960 --> 01:00:02,480
campaigns succeed in the past with

01:00:01,520 --> 01:00:05,119
project maven

01:00:02,480 --> 01:00:06,880
at google there was a huge tech worker

01:00:05,119 --> 01:00:09,040
uprising around that

01:00:06,880 --> 01:00:10,400
that was also coupled with the public

01:00:09,040 --> 01:00:12,640
campaign also

01:00:10,400 --> 01:00:13,520
the sexual harassment issues at google

01:00:12,640 --> 01:00:16,240
that were happening

01:00:13,520 --> 01:00:17,119
from the tech executives not being held

01:00:16,240 --> 01:00:20,000
accountable

01:00:17,119 --> 01:00:20,720
again the google walkout for change uh

01:00:20,000 --> 01:00:23,920
led to

01:00:20,720 --> 01:00:25,920
you know a big conversation there

01:00:23,920 --> 01:00:27,599
and so uh what what this current

01:00:25,920 --> 01:00:29,359
campaign the recruitment campaign is

01:00:27,599 --> 01:00:31,520
doing is

01:00:29,359 --> 01:00:32,640
focus on this issue of uh to meet

01:00:31,520 --> 01:00:35,440
cabrio's

01:00:32,640 --> 01:00:36,319
dismissal on you know completely bogus

01:00:35,440 --> 01:00:38,720
grounds

01:00:36,319 --> 01:00:40,079
uh of she she was basically publishing a

01:00:38,720 --> 01:00:42,960
paper that

01:00:40,079 --> 01:00:44,799
said ai bias still exists wow and

01:00:42,960 --> 01:00:46,480
machine learning has climate impacts

01:00:44,799 --> 01:00:48,079
and you know i guess google said you

01:00:46,480 --> 01:00:48,480
can't keep can't keep publishing this

01:00:48,079 --> 01:00:52,160
stuff

01:00:48,480 --> 01:00:53,599
so you got to go uh as as

01:00:52,160 --> 01:00:55,440
you know which is another brilliant

01:00:53,599 --> 01:00:58,000
example of of uh

01:00:55,440 --> 01:00:58,559
why this ai ethics within companies is

01:00:58,000 --> 01:01:02,079
just like

01:00:58,559 --> 01:01:04,880
it's just policy lobbying um

01:01:02,079 --> 01:01:05,599
and so what the recruitment campaign is

01:01:04,880 --> 01:01:07,520
is doing

01:01:05,599 --> 01:01:08,880
is saying you know google you're not

01:01:07,520 --> 01:01:12,240
gonna get away with this

01:01:08,880 --> 01:01:13,760
and one one way that anyone who's

01:01:12,240 --> 01:01:16,160
a tech worker or a student or a

01:01:13,760 --> 01:01:17,839
researcher can help put pressure on them

01:01:16,160 --> 01:01:19,839
is by saying we're not going to work at

01:01:17,839 --> 01:01:22,160
google until you listen to

01:01:19,839 --> 01:01:23,599
the google walk out for change folks who

01:01:22,160 --> 01:01:25,200
aren't making unreasonable demands you

01:01:23,599 --> 01:01:28,160
know they're they're asking for

01:01:25,200 --> 01:01:29,680
more transparency in the review process

01:01:28,160 --> 01:01:32,319
at google they're asking

01:01:29,680 --> 01:01:32,720
to you know reveal what exactly happened

01:01:32,319 --> 01:01:35,040
uh

01:01:32,720 --> 01:01:36,799
and if if google can mute these demands

01:01:35,040 --> 01:01:38,000
which by all by all means that you know

01:01:36,799 --> 01:01:40,559
it should be able to

01:01:38,000 --> 01:01:41,920
uh then then you know you can go work

01:01:40,559 --> 01:01:42,960
for google again even if you're taking

01:01:41,920 --> 01:01:52,240
the pledge

01:01:42,960 --> 01:01:54,880
but yeah check it out recruitment

01:01:52,240 --> 01:01:56,319
the other thing i would also say um

01:01:54,880 --> 01:01:59,839
about the recruit me

01:01:56,319 --> 01:02:03,039
not campaign is that at least

01:01:59,839 --> 01:02:04,880
from the people that i've seen it tends

01:02:03,039 --> 01:02:08,079
it's tending to be women

01:02:04,880 --> 01:02:11,200
it's tending to be women of color

01:02:08,079 --> 01:02:13,680
and they are in um

01:02:11,200 --> 01:02:14,880
the launch was between cambridge

01:02:13,680 --> 01:02:17,359
university

01:02:14,880 --> 01:02:18,000
and edinburgh university which are two

01:02:17,359 --> 01:02:21,200
um

01:02:18,000 --> 01:02:22,000
elite academic spaces so they're also

01:02:21,200 --> 01:02:25,359
within

01:02:22,000 --> 01:02:28,000
google like facebook like

01:02:25,359 --> 01:02:30,160
environments already right and these

01:02:28,000 --> 01:02:31,280
people are doing this at great personal

01:02:30,160 --> 01:02:34,319
risk

01:02:31,280 --> 01:02:37,760
and i think the reason this idea of

01:02:34,319 --> 01:02:41,200
cutting tech off at the sauce was really

01:02:37,760 --> 01:02:43,760
brilliant to me because um i was

01:02:41,200 --> 01:02:45,359
i wrote um i wrote an article for

01:02:43,760 --> 01:02:46,960
harvard business review

01:02:45,359 --> 01:02:48,559
probably i don't know maybe two years

01:02:46,960 --> 01:02:50,319
who knows with covet it was

01:02:48,559 --> 01:02:52,000
when we could go outside and before

01:02:50,319 --> 01:02:54,400
times and

01:02:52,000 --> 01:02:56,480
one of the things i talked about were

01:02:54,400 --> 01:02:57,440
how companies have to really think about

01:02:56,480 --> 01:03:00,480
social

01:02:57,440 --> 01:03:03,599
corporate responsibility within

01:03:00,480 --> 01:03:05,599
ai ethics about really changing

01:03:03,599 --> 01:03:07,359
society really thinking about well which

01:03:05,599 --> 01:03:09,680
politicians are you

01:03:07,359 --> 01:03:10,480
which which packs are you recruiting to

01:03:09,680 --> 01:03:13,520
right and

01:03:10,480 --> 01:03:16,000
what harm are those packs doing um

01:03:13,520 --> 01:03:17,760
what other which non-profits are you

01:03:16,000 --> 01:03:19,200
recruiting to and are you creating a

01:03:17,760 --> 01:03:22,240
better society or

01:03:19,200 --> 01:03:24,000
are you just you know instead of

01:03:22,240 --> 01:03:26,000
overhauling the education system you're

01:03:24,000 --> 01:03:28,160
just giving more books that you publish

01:03:26,000 --> 01:03:30,640
right so let's let's really think about

01:03:28,160 --> 01:03:32,640
this reimagining and so much of

01:03:30,640 --> 01:03:34,000
ai for the people's work is in the

01:03:32,640 --> 01:03:37,039
imagination space

01:03:34,000 --> 01:03:39,039
using art and culture to create new

01:03:37,039 --> 01:03:41,039
possibilities through story

01:03:39,039 --> 01:03:42,720
for people who are new to the issues or

01:03:41,039 --> 01:03:43,839
don't care about the issues

01:03:42,720 --> 01:03:45,760
and that's certainly something that we

01:03:43,839 --> 01:03:47,599
did with our disinformation work we were

01:03:45,760 --> 01:03:48,480
really passionate about not we did

01:03:47,599 --> 01:03:50,559
publish a paper

01:03:48,480 --> 01:03:52,400
because we were researchers but we also

01:03:50,559 --> 01:03:53,440
created a series of videos and then

01:03:52,400 --> 01:03:56,480
measured

01:03:53,440 --> 01:04:00,240
video engagement with pro voting pro

01:03:56,480 --> 01:04:02,240
social messages against engagement with

01:04:00,240 --> 01:04:03,520
the dope vote hashtag and guess what

01:04:02,240 --> 01:04:05,920
blackjoy1

01:04:03,520 --> 01:04:06,880
people actually wanted to have a reason

01:04:05,920 --> 01:04:09,359
to do something

01:04:06,880 --> 01:04:11,039
the disinformation messages were really

01:04:09,359 --> 01:04:13,359
crazy and then when we went back to

01:04:11,039 --> 01:04:14,960
local news ecosystems and we're able to

01:04:13,359 --> 01:04:16,880
do headline analysis

01:04:14,960 --> 01:04:18,400
we're showing how they the way that they

01:04:16,880 --> 01:04:19,119
were covering disinformation was an

01:04:18,400 --> 01:04:21,760
issue

01:04:19,119 --> 01:04:24,480
but that aside with no tech for tyrants

01:04:21,760 --> 01:04:28,400
what they're doing is really changing

01:04:24,480 --> 01:04:29,520
this story about why people want to get

01:04:28,400 --> 01:04:31,920
into tech

01:04:29,520 --> 01:04:35,039
what they want to do when they get there

01:04:31,920 --> 01:04:38,000
and for all the diversity and inclusion

01:04:35,039 --> 01:04:39,680
you know nonsense like theater that

01:04:38,000 --> 01:04:41,680
these companies engage with

01:04:39,680 --> 01:04:42,799
they wouldn't even really have that

01:04:41,680 --> 01:04:44,400
supply

01:04:42,799 --> 01:04:46,480
and i thought it was incredibly

01:04:44,400 --> 01:04:48,480
important to use tim nick gabriel's

01:04:46,480 --> 01:04:51,680
story michelle mitchell's story

01:04:48,480 --> 01:04:53,839
and alex hannah who isn't who is a trans

01:04:51,680 --> 01:04:55,839
um researcher who is still in the belly

01:04:53,839 --> 01:04:58,960
of the beast doing the great work

01:04:55,839 --> 01:05:01,359
of resisting from within right now

01:04:58,960 --> 01:05:02,240
as we speak i'm sure at under intense

01:05:01,359 --> 01:05:05,039
pressure

01:05:02,240 --> 01:05:05,920
and and giving these students another

01:05:05,039 --> 01:05:09,119
way

01:05:05,920 --> 01:05:11,440
um of being and so i

01:05:09,119 --> 01:05:12,880
much of the time i was at that launch

01:05:11,440 --> 01:05:15,520
tried to engage

01:05:12,880 --> 01:05:17,760
in an opportunity in a conversation

01:05:15,520 --> 01:05:20,960
about alternative tech futures

01:05:17,760 --> 01:05:24,000
so what is public interest technology

01:05:20,960 --> 01:05:27,280
who can be in it how can we be

01:05:24,000 --> 01:05:30,480
in it one of the things uh russia um

01:05:27,280 --> 01:05:32,720
russia and i are presently collaborating

01:05:30,480 --> 01:05:34,480
um on a project but our teams worked

01:05:32,720 --> 01:05:36,960
really closely together

01:05:34,480 --> 01:05:38,480
um in the launch of a different body of

01:05:36,960 --> 01:05:39,760
work that we're involved in

01:05:38,480 --> 01:05:42,480
but one of the things that really

01:05:39,760 --> 01:05:46,160
impressed me about that were

01:05:42,480 --> 01:05:49,359
here was this racial justice female-led

01:05:46,160 --> 01:05:51,280
um not very well-funded so foundation

01:05:49,359 --> 01:05:53,520
foundation shout out you could be doing

01:05:51,280 --> 01:05:55,760
more just saying holler at your girl

01:05:53,520 --> 01:05:57,359
but you know you and but but one of the

01:05:55,760 --> 01:06:00,319
things that she was able to do

01:05:57,359 --> 01:06:00,720
was use her institut institutional power

01:06:00,319 --> 01:06:04,079
to

01:06:00,720 --> 01:06:07,200
listen to critiques that we had about

01:06:04,079 --> 01:06:07,920
the way amnesty and rights organizations

01:06:07,200 --> 01:06:09,839
work

01:06:07,920 --> 01:06:11,200
and not only listen to critiques but

01:06:09,839 --> 01:06:13,359
create space for

01:06:11,200 --> 01:06:14,559
leadership and then i was able to take

01:06:13,359 --> 01:06:17,280
that back to the no

01:06:14,559 --> 01:06:19,200
tech that recruit me nuts and say there

01:06:17,280 --> 01:06:22,000
are alternative places

01:06:19,200 --> 01:06:22,960
um that you can work and we have to keep

01:06:22,000 --> 01:06:26,160
creating we have to keep

01:06:22,960 --> 01:06:29,119
building this uh this area

01:06:26,160 --> 01:06:31,119
of uh public interest tech so that there

01:06:29,119 --> 01:06:32,319
are pl there are places that people can

01:06:31,119 --> 01:06:35,039
go and certainly

01:06:32,319 --> 01:06:37,119
for the people that work with me it

01:06:35,039 --> 01:06:39,440
would be my honor and pleasure if

01:06:37,119 --> 01:06:42,079
somebody were to apply to an amnesty

01:06:39,440 --> 01:06:43,920
and then be looked at and recruited

01:06:42,079 --> 01:06:45,520
based on the work that we had already

01:06:43,920 --> 01:06:48,720
done with that company

01:06:45,520 --> 01:06:51,280
and really be a trusted recruitment

01:06:48,720 --> 01:06:52,799
pipeline um i can't imagine that people

01:06:51,280 --> 01:06:54,160
would want to work with me would run to

01:06:52,799 --> 01:06:56,559
google but who knows

01:06:54,160 --> 01:06:57,520
you know i'm i'm not in the business of

01:06:56,559 --> 01:06:59,200
training

01:06:57,520 --> 01:07:01,039
people to become cogs i'm in the

01:06:59,200 --> 01:07:02,079
business of training people to make

01:07:01,039 --> 01:07:04,720
meaningful

01:07:02,079 --> 01:07:06,000
um and radical change and go to pray you

01:07:04,720 --> 01:07:08,000
know apply to programs

01:07:06,000 --> 01:07:10,000
and like yours profess you know

01:07:08,000 --> 01:07:12,000
professor craft we would love to take

01:07:10,000 --> 01:07:14,079
undergraduates that would then be

01:07:12,000 --> 01:07:15,599
um have that theoretical background as

01:07:14,079 --> 01:07:16,799
well because we're not going to have a

01:07:15,599 --> 01:07:18,319
new

01:07:16,799 --> 01:07:20,640
sector we're not going to have a public

01:07:18,319 --> 01:07:21,599
interest sector if we don't have people

01:07:20,640 --> 01:07:24,880
who are trained

01:07:21,599 --> 01:07:26,880
and held and and say safely held

01:07:24,880 --> 01:07:28,880
and then paid as if they were at a tech

01:07:26,880 --> 01:07:29,920
company as well because i had a huge

01:07:28,880 --> 01:07:32,160
conversation

01:07:29,920 --> 01:07:33,920
around how you should still be able to

01:07:32,160 --> 01:07:35,599
be paid in the way that you were in

01:07:33,920 --> 01:07:42,720
these other spaces

01:07:35,599 --> 01:07:44,640
um for doing this work

01:07:42,720 --> 01:07:46,000
thank you i want to raise a question

01:07:44,640 --> 01:07:49,119
that we got um

01:07:46,000 --> 01:07:50,240
from the from the audience uh on twitter

01:07:49,119 --> 01:07:53,520
which i think is

01:07:50,240 --> 01:07:54,880
is related um i think

01:07:53,520 --> 01:07:56,559
one of the things i loved that you were

01:07:54,880 --> 01:07:56,880
talking about mitchell was this idea

01:07:56,559 --> 01:07:58,880
that

01:07:56,880 --> 01:08:00,000
in order to create public interest tech

01:07:58,880 --> 01:08:02,400
we're going to need more

01:08:00,000 --> 01:08:04,559
more people from more spaces and and

01:08:02,400 --> 01:08:06,400
maybe even outside of just

01:08:04,559 --> 01:08:08,400
technologists or people who consider

01:08:06,400 --> 01:08:10,720
themselves to be technologists so

01:08:08,400 --> 01:08:12,079
um this question that we got speaks to

01:08:10,720 --> 01:08:14,960
that um

01:08:12,079 --> 01:08:16,040
so someone on twitter asked what actions

01:08:14,960 --> 01:08:19,199
can

01:08:16,040 --> 01:08:22,400
non-technologists take to counteract

01:08:19,199 --> 01:08:23,600
harmful ai and i think related to this i

01:08:22,400 --> 01:08:25,759
wanted to

01:08:23,600 --> 01:08:27,359
sneak in another question because i'm

01:08:25,759 --> 01:08:29,679
like looking at the time

01:08:27,359 --> 01:08:32,319
um about one of the things i want to ask

01:08:29,679 --> 01:08:34,880
was around also kind of community-based

01:08:32,319 --> 01:08:36,480
solutions to misinformation i think

01:08:34,880 --> 01:08:40,080
possibly there's a link here but you all

01:08:36,480 --> 01:08:40,080
can tell me if i'm right there

01:08:40,400 --> 01:08:43,839
great so um you know much of today's

01:08:42,480 --> 01:08:45,520
information disorder

01:08:43,839 --> 01:08:46,880
has been designed to undermine like a

01:08:45,520 --> 01:08:48,640
lot of our basic values

01:08:46,880 --> 01:08:51,199
our systemic values um in trusted

01:08:48,640 --> 01:08:53,040
institutions uh trust in the sciences

01:08:51,199 --> 01:08:54,799
and misinformation has built this kind

01:08:53,040 --> 01:08:57,040
of identity-based narratives

01:08:54,799 --> 01:08:58,880
um that can pick communities uh and

01:08:57,040 --> 01:09:00,400
community members against one another

01:08:58,880 --> 01:09:02,640
uh so i was just thinking for instance

01:09:00,400 --> 01:09:04,239
uh the recent rise of asian-american

01:09:02,640 --> 01:09:06,480
violence across the u.s

01:09:04,239 --> 01:09:07,679
has been used to create rifts between

01:09:06,480 --> 01:09:09,920
the asian american community and the

01:09:07,679 --> 01:09:12,719
black community in some spaces online

01:09:09,920 --> 01:09:14,799
um and so most of the content

01:09:12,719 --> 01:09:17,199
proliferating about incidents like these

01:09:14,799 --> 01:09:18,799
might not be picked up by an algorithm

01:09:17,199 --> 01:09:20,799
or possibly even flagged as missed or

01:09:18,799 --> 01:09:22,799
disinformation as much of it falls into

01:09:20,799 --> 01:09:23,279
this kind of gray area that algorithms

01:09:22,799 --> 01:09:25,120
might

01:09:23,279 --> 01:09:27,120
not might currently have issues with

01:09:25,120 --> 01:09:28,960
tracking um

01:09:27,120 --> 01:09:31,359
so it's important that those who work

01:09:28,960 --> 01:09:32,799
with and identify as community leaders

01:09:31,359 --> 01:09:34,080
understand how certain types of

01:09:32,799 --> 01:09:36,239
disinformation is

01:09:34,080 --> 01:09:38,239
is being weaponized against them daily

01:09:36,239 --> 01:09:40,080
uh so basically at first draft uh

01:09:38,239 --> 01:09:42,000
where i work which is a non-profit

01:09:40,080 --> 01:09:42,799
dedicated to tackling disinformation

01:09:42,000 --> 01:09:44,319
globally

01:09:42,799 --> 01:09:46,000
we've been working closely with the

01:09:44,319 --> 01:09:47,839
disinformation defense league

01:09:46,000 --> 01:09:49,279
which is a collective of over 200

01:09:47,839 --> 01:09:50,719
grassroots organizations

01:09:49,279 --> 01:09:52,480
fighting miss and disinformation in

01:09:50,719 --> 01:09:54,320
black and latinx communities

01:09:52,480 --> 01:09:56,239
um and knowing that newsrooms in tech

01:09:54,320 --> 01:09:57,600
lack the diversity and often do not

01:09:56,239 --> 01:09:58,719
represent the communities that they are

01:09:57,600 --> 01:10:01,120
meant to serve

01:09:58,719 --> 01:10:02,400
this community intervention is key and

01:10:01,120 --> 01:10:03,199
we have been working with community

01:10:02,400 --> 01:10:05,040
leaders

01:10:03,199 --> 01:10:06,640
and training the trainer to be able to

01:10:05,040 --> 01:10:09,120
identify propaganda

01:10:06,640 --> 01:10:10,400
misinformation and disinformation and so

01:10:09,120 --> 01:10:12,800
they in turn

01:10:10,400 --> 01:10:14,560
develop organizational strategies uh to

01:10:12,800 --> 01:10:16,000
tackle this information that is then

01:10:14,560 --> 01:10:17,679
shared with their network

01:10:16,000 --> 01:10:19,360
so i just think as far as systemic

01:10:17,679 --> 01:10:20,400
change a big problem in the creation of

01:10:19,360 --> 01:10:21,760
these algorithms

01:10:20,400 --> 01:10:23,840
is that people from bipod communities

01:10:21,760 --> 01:10:25,120
voices are not included um

01:10:23,840 --> 01:10:27,040
appreciating the nuances and

01:10:25,120 --> 01:10:28,640
characteristics of these communities

01:10:27,040 --> 01:10:29,360
like seeing through people that we work

01:10:28,640 --> 01:10:32,000
with

01:10:29,360 --> 01:10:34,080
is critical for anyone who studies funds

01:10:32,000 --> 01:10:34,800
or works on technology um trying to

01:10:34,080 --> 01:10:36,640
temper

01:10:34,800 --> 01:10:38,000
information disorder online and so

01:10:36,640 --> 01:10:39,840
that's why i think working with people

01:10:38,000 --> 01:10:40,800
on the ground and having them in turn

01:10:39,840 --> 01:10:42,560
work with people within their

01:10:40,800 --> 01:10:46,719
communities is the best way in my

01:10:42,560 --> 01:10:46,719
opinion to uh solve our current crisis

01:10:48,960 --> 01:10:54,480
um ai for the people i'm a journalist

01:10:52,239 --> 01:10:56,000
um 20 years in the game blacked out

01:10:54,480 --> 01:10:58,080
crack and

01:10:56,000 --> 01:10:59,679
and i've been a member of the national

01:10:58,080 --> 01:11:00,960
association of black journalists for all

01:10:59,679 --> 01:11:04,159
of that time

01:11:00,960 --> 01:11:05,199
so um we started out with just old

01:11:04,159 --> 01:11:08,640
friends

01:11:05,199 --> 01:11:10,560
and have been uh working with nabj

01:11:08,640 --> 01:11:12,080
last year through their conference

01:11:10,560 --> 01:11:15,120
looking at disinformation

01:11:12,080 --> 01:11:17,440
and presenting and have lots of

01:11:15,120 --> 01:11:18,480
exciting projects and once we get them

01:11:17,440 --> 01:11:21,440
off the ground

01:11:18,480 --> 01:11:22,560
that we'll be looking into those um

01:11:21,440 --> 01:11:25,600
those marketplaces

01:11:22,560 --> 01:11:27,520
as well as looking at opinion um as well

01:11:25,600 --> 01:11:30,719
because one of the things that we found

01:11:27,520 --> 01:11:33,280
and certainly with our report where

01:11:30,719 --> 01:11:35,120
the network were able to then use

01:11:33,280 --> 01:11:38,400
the opinion pages

01:11:35,120 --> 01:11:41,280
as a as kind of an entryway in so

01:11:38,400 --> 01:11:41,920
um we've been speaking to some funders

01:11:41,280 --> 01:11:45,440
around

01:11:41,920 --> 01:11:48,719
uh looking at that and then really

01:11:45,440 --> 01:11:50,640
we've been um we piloted last year micro

01:11:48,719 --> 01:11:53,120
influencer program

01:11:50,640 --> 01:11:55,520
because just in internal conversations

01:11:53,120 --> 01:11:58,960
we were really excited that

01:11:55,520 --> 01:12:01,120
kendall jenner could sell lipgloss um

01:11:58,960 --> 01:12:02,400
using her platform using her influence

01:12:01,120 --> 01:12:04,159
and we piloted

01:12:02,400 --> 01:12:05,520
in philadelphia in the run-up to the

01:12:04,159 --> 01:12:08,000
election a

01:12:05,520 --> 01:12:10,239
political micro-influencer program which

01:12:08,000 --> 01:12:13,360
was really really successful

01:12:10,239 --> 01:12:16,480
so we're beginning to think about

01:12:13,360 --> 01:12:19,040
going into cities um

01:12:16,480 --> 01:12:20,320
and looking at ethnic local media

01:12:19,040 --> 01:12:23,120
ecosystems

01:12:20,320 --> 01:12:23,600
that we're already connected with and

01:12:23,120 --> 01:12:26,560
getting

01:12:23,600 --> 01:12:28,400
over this idea that journalists have to

01:12:26,560 --> 01:12:31,440
be objective by bringing in

01:12:28,400 --> 01:12:33,840
non non-journalistic actors who

01:12:31,440 --> 01:12:35,120
have who are trusted voices within

01:12:33,840 --> 01:12:38,560
communities

01:12:35,120 --> 01:12:39,199
who perhaps wouldn't see that would see

01:12:38,560 --> 01:12:42,960
themselves

01:12:39,199 --> 01:12:45,120
outside of the disinformation league or

01:12:42,960 --> 01:12:46,719
those types of big coalitions where it's

01:12:45,120 --> 01:12:47,760
like hundreds and and they want

01:12:46,719 --> 01:12:50,480
something a little bit

01:12:47,760 --> 01:12:51,040
more intimate smaller um and working

01:12:50,480 --> 01:12:54,480
with them

01:12:51,040 --> 01:12:57,760
so kind of piloting but i i do agree

01:12:54,480 --> 01:12:58,560
that all solutions are local and i don't

01:12:57,760 --> 01:13:00,719
even know

01:12:58,560 --> 01:13:02,640
even how even being a policy maker

01:13:00,719 --> 01:13:06,239
coming from that world

01:13:02,640 --> 01:13:08,719
um policy isn't going to change

01:13:06,239 --> 01:13:09,840
the conditions of people movements are

01:13:08,719 --> 01:13:12,560
going to contain

01:13:09,840 --> 01:13:15,440
change the conditions of people and you

01:13:12,560 --> 01:13:18,080
know this past year we had started to

01:13:15,440 --> 01:13:18,880
really work with the new georgia project

01:13:18,080 --> 01:13:20,880
on like

01:13:18,880 --> 01:13:22,719
questions of disinformation and one of

01:13:20,880 --> 01:13:25,199
the things that um

01:13:22,719 --> 01:13:26,560
insane foot who's the ceo there and a

01:13:25,199 --> 01:13:28,640
close friend

01:13:26,560 --> 01:13:30,880
always said to me throughout the 2020

01:13:28,640 --> 01:13:32,480
election she was like natalie

01:13:30,880 --> 01:13:35,040
you know women didn't get the right to

01:13:32,480 --> 01:13:37,360
vote because men decided it was time

01:13:35,040 --> 01:13:38,400
it was because of the suffrage suffrage

01:13:37,360 --> 01:13:40,560
movement

01:13:38,400 --> 01:13:41,600
and black women were part of that so

01:13:40,560 --> 01:13:44,400
always

01:13:41,600 --> 01:13:45,920
lean into culture lean into movement and

01:13:44,400 --> 01:13:47,600
when we're speaking in black and brown

01:13:45,920 --> 01:13:51,040
places that also means

01:13:47,600 --> 01:13:54,880
leaning um into film into television

01:13:51,040 --> 01:13:58,480
into um other ways of

01:13:54,880 --> 01:14:00,560
pricking the cultural conversation um

01:13:58,480 --> 01:14:02,719
in a in in in a way that's going to

01:14:00,560 --> 01:14:04,800
resonate and we're seeing that

01:14:02,719 --> 01:14:06,719
in the work that we did with amnesty

01:14:04,800 --> 01:14:08,239
wasn't on disinformation it was looking

01:14:06,719 --> 01:14:11,360
at biometrics

01:14:08,239 --> 01:14:12,239
but the creation of the we create a film

01:14:11,360 --> 01:14:14,640
together

01:14:12,239 --> 01:14:15,440
which we are we're traveling all over

01:14:14,640 --> 01:14:18,400
the country

01:14:15,440 --> 01:14:20,000
and every time we do policy briefings

01:14:18,400 --> 01:14:21,600
we're doing hundreds and hundreds of

01:14:20,000 --> 01:14:23,280
policy briefings

01:14:21,600 --> 01:14:25,199
the policy makers come back to us and

01:14:23,280 --> 01:14:28,239
they ask us about the story

01:14:25,199 --> 01:14:30,320
of the people in the video and

01:14:28,239 --> 01:14:32,320
the fact that amnesty put their stamp on

01:14:30,320 --> 01:14:34,640
it gives it that other led

01:14:32,320 --> 01:14:36,080
level of credibility but it's through

01:14:34,640 --> 01:14:38,480
those stories

01:14:36,080 --> 01:14:39,679
that we're able to either support

01:14:38,480 --> 01:14:41,600
legislation

01:14:39,679 --> 01:14:43,440
and in some cases say you know this

01:14:41,600 --> 01:14:44,560
legislation really needs to be changed

01:14:43,440 --> 01:14:46,480
because

01:14:44,560 --> 01:14:48,000
this person in the video would not have

01:14:46,480 --> 01:14:50,239
been protected

01:14:48,000 --> 01:14:51,679
and it just really lowers the bar and i

01:14:50,239 --> 01:14:53,920
think really

01:14:51,679 --> 01:14:56,719
lowers the temperature because in the

01:14:53,920 --> 01:14:58,800
communities in which we um

01:14:56,719 --> 01:15:00,159
in which we're well networked and we're

01:14:58,800 --> 01:15:02,960
drawing on like

01:15:00,159 --> 01:15:04,719
decades of relationships just saying the

01:15:02,960 --> 01:15:10,080
word algorithm completely switch off

01:15:04,719 --> 01:15:12,239
nobody wants to talk to you

01:15:10,080 --> 01:15:14,159
um if i can just quickly say a few

01:15:12,239 --> 01:15:16,960
concrete actions that i that

01:15:14,159 --> 01:15:18,159
um as mutely mentioned you know

01:15:16,960 --> 01:15:20,080
everything's very local

01:15:18,159 --> 01:15:22,480
and so it really depends where you are

01:15:20,080 --> 01:15:24,560
um if you're in europe or the uk

01:15:22,480 --> 01:15:26,400
learn about your data rights and make

01:15:24,560 --> 01:15:28,080
requests for your for your data

01:15:26,400 --> 01:15:29,600
and file complaints to the information

01:15:28,080 --> 01:15:30,159
commissioner when when you see like

01:15:29,600 --> 01:15:32,640
cookies

01:15:30,159 --> 01:15:33,920
that are bad um if you're in seattle you

01:15:32,640 --> 01:15:35,199
can make public comments on their

01:15:33,920 --> 01:15:36,800
surveillance technologies there's just

01:15:35,199 --> 01:15:38,719
an article out about a cyclica one of

01:15:36,800 --> 01:15:41,760
the surveillance technologies last week

01:15:38,719 --> 01:15:42,960
um use linux and free software if you're

01:15:41,760 --> 01:15:44,320
not already

01:15:42,960 --> 01:15:46,159
and what i would say if there's a single

01:15:44,320 --> 01:15:48,080
thing i would recommend to everyone

01:15:46,159 --> 01:15:49,360
as vitelli says this is about collective

01:15:48,080 --> 01:15:50,960
action so

01:15:49,360 --> 01:15:52,480
join a trade union but don't just join

01:15:50,960 --> 01:15:54,159
get involved in case work

01:15:52,480 --> 01:15:56,000
uh get involved in organizing we need to

01:15:54,159 --> 01:15:58,400
we need to build counter power and i'll

01:15:56,000 --> 01:15:58,400
be ready

01:15:58,640 --> 01:16:02,320
and one final thing from me just very

01:16:01,120 --> 01:16:04,960
much related to that

01:16:02,320 --> 01:16:05,600
um he is going to be launching a

01:16:04,960 --> 01:16:07,440
campaign

01:16:05,600 --> 01:16:09,840
on on the issue of the business model of

01:16:07,440 --> 01:16:12,239
big tech um hopefully in the summer

01:16:09,840 --> 01:16:14,320
we'll have some something on our website

01:16:12,239 --> 01:16:17,760
and that people can get involved in

01:16:14,320 --> 01:16:18,960
but um please talk to your talk to your

01:16:17,760 --> 01:16:20,159
family about it talk to your friends

01:16:18,960 --> 01:16:23,360
about it um

01:16:20,159 --> 01:16:23,920
and um get involved with with amnesty um

01:16:23,360 --> 01:16:25,920
if

01:16:23,920 --> 01:16:27,360
in your country if you have an amnesty

01:16:25,920 --> 01:16:29,840
section get involved

01:16:27,360 --> 01:16:30,640
and um come come on board this this

01:16:29,840 --> 01:16:33,520
mission

01:16:30,640 --> 01:16:34,400
um with us to to make her rights

01:16:33,520 --> 01:16:37,600
respecting

01:16:34,400 --> 01:16:39,440
internet um that that is is made for

01:16:37,600 --> 01:16:42,880
for society rather than for the pockets

01:16:39,440 --> 01:16:42,880
of um silicon valley

01:16:43,679 --> 01:16:49,440
great thank you so much and with that um

01:16:47,199 --> 01:16:50,880
that is our dialogues and debates for

01:16:49,440 --> 01:16:53,920
today so thank you all

01:16:50,880 --> 01:17:03,840
um so so much for being here and to

01:16:53,920 --> 01:17:03,840
everyone online thank you for tuning in

01:17:08,560 --> 01:17:10,640

YouTube URL: https://www.youtube.com/watch?v=7yb4ZDeljtE


