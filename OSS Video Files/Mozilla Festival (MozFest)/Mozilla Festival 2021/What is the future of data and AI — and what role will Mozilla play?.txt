Title: What is the future of data and AI — and what role will Mozilla play?
Publication date: 2021-03-11
Playlist: Mozilla Festival 2021
Description: 
	In this one-hour plenary, Mozilla community members highlight the need for more trustworthy AI. And, also explore what the MozFest community can do to make it a reality. You’ll hear from engineers, activists, and researchers about new (and better) ways of using, sharing, and thinking about data for the public good.
Captions: 
	00:00:00,000 --> 00:00:03,040
[Music]

00:00:00,320 --> 00:00:03,040
good and live

00:00:03,280 --> 00:00:09,920
all right we are live thank you uh hi

00:00:06,879 --> 00:00:12,559
everyone and welcome i hope you've had a

00:00:09,920 --> 00:00:13,599
wonderful start to moss fest uh my name

00:00:12,559 --> 00:00:15,759
is sarah awun

00:00:13,599 --> 00:00:17,440
i'm a human rights technologist and i'll

00:00:15,759 --> 00:00:18,960
be one of your hosts for the next hour

00:00:17,440 --> 00:00:20,720
for this really really special session

00:00:18,960 --> 00:00:22,640
that i hope you'll all enjoy

00:00:20,720 --> 00:00:25,599
for those who don't know me i used to be

00:00:22,640 --> 00:00:27,039
actually an open web fellow back in 2017

00:00:25,599 --> 00:00:28,720
and i'm now the chief technologist at

00:00:27,039 --> 00:00:30,320
the open tech fund

00:00:28,720 --> 00:00:32,320
and let me tell you a bit about what

00:00:30,320 --> 00:00:32,960
we're doing here today uh most of us

00:00:32,320 --> 00:00:35,200
watching

00:00:32,960 --> 00:00:36,399
uh including myself like i said are

00:00:35,200 --> 00:00:38,160
members of

00:00:36,399 --> 00:00:39,680
this greater mozilla community and this

00:00:38,160 --> 00:00:41,840
greater missoula family

00:00:39,680 --> 00:00:43,360
and we're also all citizens of the

00:00:41,840 --> 00:00:45,920
global internet

00:00:43,360 --> 00:00:47,520
and with that i think we're all at least

00:00:45,920 --> 00:00:50,079
a little bit aware

00:00:47,520 --> 00:00:51,760
um that there's a very specific and

00:00:50,079 --> 00:00:54,399
complex relationship between

00:00:51,760 --> 00:00:54,879
data and power and this is exactly why

00:00:54,399 --> 00:00:56,320
we're

00:00:54,879 --> 00:00:58,719
here today and what we're going to be

00:00:56,320 --> 00:01:01,600
talking about um

00:00:58,719 --> 00:01:02,160
to talk about the role that mozilla

00:01:01,600 --> 00:01:04,720
plays

00:01:02,160 --> 00:01:05,760
in internet in data health and the

00:01:04,720 --> 00:01:07,920
impact that

00:01:05,760 --> 00:01:08,960
that has on digital and human rights

00:01:07,920 --> 00:01:11,760
overall

00:01:08,960 --> 00:01:13,600
and um with that in mind i would like to

00:01:11,760 --> 00:01:15,520
introduce you to one of my uh to my

00:01:13,600 --> 00:01:18,240
co-host actually for this session

00:01:15,520 --> 00:01:20,159
and he is of course uh one of the hosts

00:01:18,240 --> 00:01:21,040
behind the scenes for this entire event

00:01:20,159 --> 00:01:24,159
this week

00:01:21,040 --> 00:01:25,280
please welcome me in uh and please join

00:01:24,159 --> 00:01:26,799
me in welcoming

00:01:25,280 --> 00:01:28,479
missoula's executive director mark

00:01:26,799 --> 00:01:31,280
sermon

00:01:28,479 --> 00:01:32,320
thank you sarah it's funny to have me in

00:01:31,280 --> 00:01:35,200
toronto and you

00:01:32,320 --> 00:01:36,159
in brooklyn doing this uh you know doing

00:01:35,200 --> 00:01:37,520
this session

00:01:36,159 --> 00:01:39,360
i remember last time we actually saw

00:01:37,520 --> 00:01:40,159
each other in person we were able to get

00:01:39,360 --> 00:01:43,280
on airplanes

00:01:40,159 --> 00:01:45,439
was um also talking about the politics

00:01:43,280 --> 00:01:47,520
of the internet but in a bar in tunisia

00:01:45,439 --> 00:01:48,560
at wright's con 2019 so things are

00:01:47,520 --> 00:01:51,040
different but

00:01:48,560 --> 00:01:52,240
but good it's good to have you here um

00:01:51,040 --> 00:01:55,920
and you know when j

00:01:52,240 --> 00:01:58,640
bob and i were brainstorming storming um

00:01:55,920 --> 00:02:00,399
about who should co-host this plenary uh

00:01:58,640 --> 00:02:02,799
she said well what about sarah ian

00:02:00,399 --> 00:02:04,640
and i thought that is a perfect idea uh

00:02:02,799 --> 00:02:05,360
you're such an amazing technologist and

00:02:04,640 --> 00:02:07,280
activist and

00:02:05,360 --> 00:02:09,520
you've been a great collaborator for us

00:02:07,280 --> 00:02:11,280
so i appreciate you being here today

00:02:09,520 --> 00:02:12,560
well thank you mark um thank you so much

00:02:11,280 --> 00:02:14,560
for having me and it's really good to be

00:02:12,560 --> 00:02:16,000
here and to be back again in the mozilla

00:02:14,560 --> 00:02:17,200
in the missile family to host the

00:02:16,000 --> 00:02:19,440
session with you

00:02:17,200 --> 00:02:21,360
um and as you know back to that bar in

00:02:19,440 --> 00:02:23,040
tunisia we've always i mean i've always

00:02:21,360 --> 00:02:24,160
been at this intersection of technology

00:02:23,040 --> 00:02:25,840
and human rights

00:02:24,160 --> 00:02:27,520
uh which is very similar to a lot of the

00:02:25,840 --> 00:02:29,040
work that mozilla is doing and has been

00:02:27,520 --> 00:02:31,040
doing for a long time

00:02:29,040 --> 00:02:32,560
um so for everyone else who's here with

00:02:31,040 --> 00:02:33,120
us today let me tell you a bit about

00:02:32,560 --> 00:02:35,840
what we're

00:02:33,120 --> 00:02:37,840
what to expect for for the next session

00:02:35,840 --> 00:02:40,720
uh we're going to be exploring

00:02:37,840 --> 00:02:41,360
the these topics around uh data and

00:02:40,720 --> 00:02:42,959
power

00:02:41,360 --> 00:02:44,959
and for that we have three really

00:02:42,959 --> 00:02:46,400
amazing speakers coming on to talk about

00:02:44,959 --> 00:02:47,200
that and present some of the work on

00:02:46,400 --> 00:02:49,360
that

00:02:47,200 --> 00:02:50,720
um but before we get to them mark can

00:02:49,360 --> 00:02:52,720
you tell us a little bit

00:02:50,720 --> 00:02:54,640
more about why you picked that topic and

00:02:52,720 --> 00:02:57,200
why we're specifically thinking about

00:02:54,640 --> 00:03:00,879
data and power and that dynamic today

00:02:57,200 --> 00:03:03,120
yeah totally happy to um we've always

00:03:00,879 --> 00:03:04,560
used the monsters plenaries as a time to

00:03:03,120 --> 00:03:06,319
look at a theme that

00:03:04,560 --> 00:03:08,319
is especially resonant with the mozilla

00:03:06,319 --> 00:03:10,239
community at a particular moment in

00:03:08,319 --> 00:03:11,760
time and when i think about this journey

00:03:10,239 --> 00:03:14,800
actually i think about this slide

00:03:11,760 --> 00:03:16,560
maybe cue the slide um you know it's a

00:03:14,800 --> 00:03:18,239
slide about how strategy

00:03:16,560 --> 00:03:19,760
isn't something that you know just goes

00:03:18,239 --> 00:03:20,879
in a straight line or the evolution of

00:03:19,760 --> 00:03:23,040
ideas doesn't go

00:03:20,879 --> 00:03:24,560
in a straight line it actually kind of

00:03:23,040 --> 00:03:25,599
wanders around if you go to the next

00:03:24,560 --> 00:03:27,120
slide

00:03:25,599 --> 00:03:29,760
that's actually kind of how i think

00:03:27,120 --> 00:03:30,560
about the 10 years of of topics we've

00:03:29,760 --> 00:03:33,120
been talking about

00:03:30,560 --> 00:03:34,080
at mozfast so we've gone from open

00:03:33,120 --> 00:03:36,480
source and

00:03:34,080 --> 00:03:38,319
the open web in in the beginning all

00:03:36,480 --> 00:03:40,560
through lots of different paths

00:03:38,319 --> 00:03:41,599
uh to the the focus on trustworthy ai

00:03:40,560 --> 00:03:43,840
today

00:03:41,599 --> 00:03:45,440
but through all of this the core ideas

00:03:43,840 --> 00:03:48,480
and the conversation have been the same

00:03:45,440 --> 00:03:50,959
championing openness privacy security

00:03:48,480 --> 00:03:52,720
inclusion and creating an internet that

00:03:50,959 --> 00:03:54,319
is open accessible to all

00:03:52,720 --> 00:03:56,000
so we've been in the moment always but

00:03:54,319 --> 00:03:58,720
on the same path

00:03:56,000 --> 00:04:00,480
even if we kind of wound a little bit so

00:03:58,720 --> 00:04:02,640
this year we decided uh

00:04:00,480 --> 00:04:04,000
you know in the same spirit to focus

00:04:02,640 --> 00:04:05,760
specifically on

00:04:04,000 --> 00:04:07,200
shifting power dynamics around data

00:04:05,760 --> 00:04:10,560
because it's so central

00:04:07,200 --> 00:04:13,519
to mozilla's trustworthy ai agenda and

00:04:10,560 --> 00:04:14,879
we all know that things are broken one

00:04:13,519 --> 00:04:15,760
of the things that is broken about the

00:04:14,879 --> 00:04:18,639
internet is

00:04:15,760 --> 00:04:20,160
who controls data and with this planner

00:04:18,639 --> 00:04:21,440
we wanted to go

00:04:20,160 --> 00:04:23,199
and bring in some speakers who aren't

00:04:21,440 --> 00:04:24,560
just talking about what is broken into

00:04:23,199 --> 00:04:27,120
the world of data

00:04:24,560 --> 00:04:28,080
but who are doing something to fix it so

00:04:27,120 --> 00:04:30,320
that's what you're going to hear

00:04:28,080 --> 00:04:32,000
in a moment but before we do that um

00:04:30,320 --> 00:04:33,840
sarah as a technologist i wanted to ask

00:04:32,000 --> 00:04:35,600
you how do you think about issues around

00:04:33,840 --> 00:04:37,759
data and power

00:04:35,600 --> 00:04:39,199
yeah um thanks for that question mark i

00:04:37,759 --> 00:04:42,240
mean

00:04:39,199 --> 00:04:43,040
ai is such a such a big topic and a big

00:04:42,240 --> 00:04:44,880
field

00:04:43,040 --> 00:04:46,400
but when i think about it i mean ai is

00:04:44,880 --> 00:04:48,560
definitely going to

00:04:46,400 --> 00:04:50,240
to shape and define the way that we move

00:04:48,560 --> 00:04:52,880
forward as a society

00:04:50,240 --> 00:04:53,360
uh and the and the world of ai that i

00:04:52,880 --> 00:04:56,400
live in

00:04:53,360 --> 00:04:57,520
in my day-to-day work feels really like

00:04:56,400 --> 00:05:01,199
a dystopian future

00:04:57,520 --> 00:05:03,199
like i spend most of my days um really

00:05:01,199 --> 00:05:05,600
thinking about the type of ai that

00:05:03,199 --> 00:05:08,880
reinforces his bias the type of ai that

00:05:05,600 --> 00:05:10,479
helps automate surveillance for example

00:05:08,880 --> 00:05:11,919
so in thinking about that for instance

00:05:10,479 --> 00:05:14,160
like repressive regimes

00:05:11,919 --> 00:05:14,960
uh in the world are using ai today to

00:05:14,160 --> 00:05:16,720
automate

00:05:14,960 --> 00:05:17,919
censorship and surveillance efforts and

00:05:16,720 --> 00:05:18,720
that's something that i'm working on all

00:05:17,919 --> 00:05:20,479
the time

00:05:18,720 --> 00:05:22,000
uh which makes censorship and

00:05:20,479 --> 00:05:24,240
surveillance easier

00:05:22,000 --> 00:05:26,160
faster and cheaper than ever if you take

00:05:24,240 --> 00:05:29,199
the chinese government for example

00:05:26,160 --> 00:05:31,440
they're using ai to rapidly identify

00:05:29,199 --> 00:05:32,960
and block circumvention tools and

00:05:31,440 --> 00:05:34,160
circumvention techniques that people are

00:05:32,960 --> 00:05:36,320
using in china

00:05:34,160 --> 00:05:37,280
so this in the past used to take months

00:05:36,320 --> 00:05:39,120
to implement

00:05:37,280 --> 00:05:41,759
and right now it's literally just a

00:05:39,120 --> 00:05:44,080
matter of days or hours

00:05:41,759 --> 00:05:45,199
but in if you want to bring this closer

00:05:44,080 --> 00:05:48,240
to home and think about

00:05:45,199 --> 00:05:50,479
another example a bit closer to home

00:05:48,240 --> 00:05:52,000
and we can queue up the slide for that

00:05:50,479 --> 00:05:54,560
robo dogs just a few

00:05:52,000 --> 00:05:56,319
weeks ago were deployed in new york city

00:05:54,560 --> 00:05:57,199
and are being tested on low-income

00:05:56,319 --> 00:06:01,199
communities

00:05:57,199 --> 00:06:02,800
such as the bronx in new york and

00:06:01,199 --> 00:06:04,639
i mean that's obviously not the first

00:06:02,800 --> 00:06:05,440
time that technology is being used to

00:06:04,639 --> 00:06:08,319
reinforce

00:06:05,440 --> 00:06:08,720
bias and racism and a lot of us know

00:06:08,319 --> 00:06:10,080
that

00:06:08,720 --> 00:06:11,120
like we've been talking about this and

00:06:10,080 --> 00:06:11,919
we've been thinking about this for a

00:06:11,120 --> 00:06:14,960
long time

00:06:11,919 --> 00:06:17,919
and the idea is that the idea of ai

00:06:14,960 --> 00:06:19,440
is that by removing the human factor ai

00:06:17,919 --> 00:06:22,240
is supposed to be a sort of

00:06:19,440 --> 00:06:23,280
equalizer but there's really an

00:06:22,240 --> 00:06:25,520
asymmetry of

00:06:23,280 --> 00:06:27,039
of um of power between the people who

00:06:25,520 --> 00:06:28,960
own the codes

00:06:27,039 --> 00:06:31,360
and the folks who are on the receiving

00:06:28,960 --> 00:06:34,800
end of this algorithmic harm

00:06:31,360 --> 00:06:36,720
um and we've seen like the the impacts

00:06:34,800 --> 00:06:40,400
basically of that and we know

00:06:36,720 --> 00:06:42,800
that everyone has unconscious bias

00:06:40,400 --> 00:06:44,720
that's undeniable and people embed

00:06:42,800 --> 00:06:47,039
their own biases in technology when

00:06:44,720 --> 00:06:48,319
they're developing technology

00:06:47,039 --> 00:06:50,080
because the reality at the end of the

00:06:48,319 --> 00:06:52,800
day is like technology

00:06:50,080 --> 00:06:53,520
is um these systems are designed by

00:06:52,800 --> 00:06:55,840
people

00:06:53,520 --> 00:06:57,360
like us uh like you and i sitting here

00:06:55,840 --> 00:06:58,319
today like a lot of people listening to

00:06:57,360 --> 00:07:00,880
the sock right now

00:06:58,319 --> 00:07:03,280
from fairly privileged backgrounds um

00:07:00,880 --> 00:07:06,080
and we're designing the systems based on

00:07:03,280 --> 00:07:08,000
our own understanding of the world but

00:07:06,080 --> 00:07:09,919
that does not always end up being the

00:07:08,000 --> 00:07:12,240
best use case for the people

00:07:09,919 --> 00:07:13,360
that these systems serve in the end and

00:07:12,240 --> 00:07:15,840
if we're not thinking

00:07:13,360 --> 00:07:17,440
about those folks in advance when we're

00:07:15,840 --> 00:07:19,039
making these design decisions

00:07:17,440 --> 00:07:21,199
this is how we essentially end up

00:07:19,039 --> 00:07:23,199
failing in that process

00:07:21,199 --> 00:07:25,199
yeah no i i totally agree and we see

00:07:23,199 --> 00:07:28,160
that all the time now whether it's like

00:07:25,199 --> 00:07:28,479
not considering different communities uh

00:07:28,160 --> 00:07:30,000
and

00:07:28,479 --> 00:07:31,599
the impact on people in the design or

00:07:30,000 --> 00:07:33,919
like considering society

00:07:31,599 --> 00:07:35,440
and we've seen a lot of dark and scary

00:07:33,919 --> 00:07:37,840
and confusing things happen

00:07:35,440 --> 00:07:38,880
as we lean more on data and ai to make

00:07:37,840 --> 00:07:41,280
decisions

00:07:38,880 --> 00:07:42,880
for us on the flip side like

00:07:41,280 --> 00:07:44,160
increasingly i meet people who are

00:07:42,880 --> 00:07:45,039
trying to push things in the opposite

00:07:44,160 --> 00:07:46,639
direction

00:07:45,039 --> 00:07:48,479
people are actually building technology

00:07:46,639 --> 00:07:51,919
today that has the

00:07:48,479 --> 00:07:52,639
potential to by design take us closer to

00:07:51,919 --> 00:07:55,039
a more just

00:07:52,639 --> 00:07:56,879
and humane digital world and so our

00:07:55,039 --> 00:07:59,280
three speakers today kathleen

00:07:56,879 --> 00:08:01,199
james brandy they're all examples of

00:07:59,280 --> 00:08:02,800
these kind of people and i'm hoping

00:08:01,199 --> 00:08:04,560
they can aspire more of us to do this

00:08:02,800 --> 00:08:06,879
kind of work

00:08:04,560 --> 00:08:08,160
so i think we should start uh i want to

00:08:06,879 --> 00:08:11,840
start with

00:08:08,160 --> 00:08:13,440
kathleen siminu she is a language and

00:08:11,840 --> 00:08:16,080
speech researcher and the regional

00:08:13,440 --> 00:08:19,120
coordinator of ai for development

00:08:16,080 --> 00:08:21,120
and um she's also a part of masakani

00:08:19,120 --> 00:08:22,240
a grassroots organization whose mission

00:08:21,120 --> 00:08:24,400
is to strengthen

00:08:22,240 --> 00:08:25,759
natural language processing research in

00:08:24,400 --> 00:08:27,919
african languages

00:08:25,759 --> 00:08:30,639
for africans by africans we shall talk a

00:08:27,919 --> 00:08:33,039
little bit uh in a moment

00:08:30,639 --> 00:08:34,399
awesome thank you mark uh kathleen are

00:08:33,039 --> 00:08:37,919
you ready to come

00:08:34,399 --> 00:08:39,440
on um and if you are everyone else

00:08:37,919 --> 00:08:41,519
mozilla community please let's give a

00:08:39,440 --> 00:08:42,800
warm warm welcome uh to our first

00:08:41,519 --> 00:08:44,959
speaker today

00:08:42,800 --> 00:08:46,480
um who's coming from nairobi kenya so

00:08:44,959 --> 00:08:48,240
kathleen thank you so much for joining

00:08:46,480 --> 00:08:50,640
us

00:08:48,240 --> 00:08:52,080
thank you uh mcintyre for that

00:08:50,640 --> 00:08:55,200
introduction

00:08:52,080 --> 00:08:56,240
um the title of my talk is a series of

00:08:55,200 --> 00:08:58,640
wise

00:08:56,240 --> 00:09:00,480
and as you will see that is exactly what

00:08:58,640 --> 00:09:02,880
my talk is composed of

00:09:00,480 --> 00:09:03,839
as i walk you through my journey as an

00:09:02,880 --> 00:09:06,160
engineer

00:09:03,839 --> 00:09:08,000
my involvement with some ai communities

00:09:06,160 --> 00:09:12,080
and subsequently venturing into

00:09:08,000 --> 00:09:12,080
research next slide please

00:09:13,120 --> 00:09:16,720
when i was much younger i wanted to

00:09:15,200 --> 00:09:20,800
become a doctor

00:09:16,720 --> 00:09:23,279
why perhaps because both my parents are

00:09:20,800 --> 00:09:24,240
perhaps because at some point it was one

00:09:23,279 --> 00:09:26,399
of few

00:09:24,240 --> 00:09:28,320
professions considered prestigious in

00:09:26,399 --> 00:09:31,600
the african context

00:09:28,320 --> 00:09:32,959
whatever the case when i was 16 i spent

00:09:31,600 --> 00:09:35,279
a couple of months doing

00:09:32,959 --> 00:09:36,880
community service at a hospital and

00:09:35,279 --> 00:09:38,399
learned that i did not have the stomach

00:09:36,880 --> 00:09:40,720
to study medicine

00:09:38,399 --> 00:09:43,360
watching people suffer day in day out

00:09:40,720 --> 00:09:46,160
literally sucked the energy out of me

00:09:43,360 --> 00:09:46,160
next slide please

00:09:47,040 --> 00:09:53,200
this is my first laptop which my mom

00:09:50,480 --> 00:09:54,320
got me when i started uni i named it

00:09:53,200 --> 00:09:57,279
faith

00:09:54,320 --> 00:09:58,399
why so that i could say to myself when

00:09:57,279 --> 00:10:00,959
appropriate

00:09:58,399 --> 00:10:02,240
have a little faith kathleen and then

00:10:00,959 --> 00:10:04,800
respond to myself

00:10:02,240 --> 00:10:05,680
but i do referring to my little laptop

00:10:04,800 --> 00:10:08,240
faith

00:10:05,680 --> 00:10:10,160
which weighed about 12 kgs by the way so

00:10:08,240 --> 00:10:11,839
it wasn't that little

00:10:10,160 --> 00:10:14,240
after realizing i couldn't become a

00:10:11,839 --> 00:10:15,839
doctor i made the decision to study math

00:10:14,240 --> 00:10:18,240
and computer science

00:10:15,839 --> 00:10:19,120
why because i really enjoyed math in

00:10:18,240 --> 00:10:22,240
high school

00:10:19,120 --> 00:10:24,800
and math and physics were two of my

00:10:22,240 --> 00:10:27,120
strongest subjects

00:10:24,800 --> 00:10:29,040
during my undergrad years i discovered

00:10:27,120 --> 00:10:31,519
data science and decided that it was a

00:10:29,040 --> 00:10:34,160
career path i wanted to go down

00:10:31,519 --> 00:10:35,600
why i found data science more

00:10:34,160 --> 00:10:37,680
interesting and exciting than

00:10:35,600 --> 00:10:41,200
web and mobile development which is a

00:10:37,680 --> 00:10:43,200
path i had started on

00:10:41,200 --> 00:10:44,720
i started doing some online courses as

00:10:43,200 --> 00:10:46,000
well as some projects in the final year

00:10:44,720 --> 00:10:48,079
of my undergrad

00:10:46,000 --> 00:10:50,160
and when i was done with school i was

00:10:48,079 --> 00:10:52,000
intent on getting someone to hire me as

00:10:50,160 --> 00:10:54,800
a data scientist

00:10:52,000 --> 00:10:54,800
next slide please

00:10:55,519 --> 00:10:59,360
i landed my first job as a data

00:10:57,680 --> 00:11:01,600
scientist as a company at

00:10:59,360 --> 00:11:03,040
a company working in the telco space in

00:11:01,600 --> 00:11:05,600
africa

00:11:03,040 --> 00:11:07,680
uh i think it was the best first job

00:11:05,600 --> 00:11:10,800
anyone could ever ask for

00:11:07,680 --> 00:11:12,640
why because they trusted me with a job

00:11:10,800 --> 00:11:14,560
that had not been done

00:11:12,640 --> 00:11:16,959
a job that no one in the company knew

00:11:14,560 --> 00:11:18,800
how to do i was their first hire for the

00:11:16,959 --> 00:11:20,959
data department

00:11:18,800 --> 00:11:22,320
they also recognized that i myself was

00:11:20,959 --> 00:11:24,079
still very green

00:11:22,320 --> 00:11:25,680
having not been professionally trained

00:11:24,079 --> 00:11:28,079
as a data scientist

00:11:25,680 --> 00:11:29,680
and they supported my technical growth

00:11:28,079 --> 00:11:32,240
by paying for me to attend summer

00:11:29,680 --> 00:11:35,040
schools and conferences

00:11:32,240 --> 00:11:35,040
next slide please

00:11:35,600 --> 00:11:38,959
this job also allowed me to get a taste

00:11:38,000 --> 00:11:40,800
of working

00:11:38,959 --> 00:11:42,720
particularly in languages and the

00:11:40,800 --> 00:11:44,399
challenges of trying to innovate for

00:11:42,720 --> 00:11:46,320
african languages

00:11:44,399 --> 00:11:47,760
as i've mentioned it was a company in

00:11:46,320 --> 00:11:50,880
the telco space

00:11:47,760 --> 00:11:52,000
and one of our products was sms it

00:11:50,880 --> 00:11:54,480
happened to be

00:11:52,000 --> 00:11:55,279
an election year one year and what

00:11:54,480 --> 00:11:57,519
typically

00:11:55,279 --> 00:11:58,880
unfortunately happens in my country is

00:11:57,519 --> 00:12:02,000
that the country will

00:11:58,880 --> 00:12:03,360
be divided along tribal lines for

00:12:02,000 --> 00:12:06,399
political purposes

00:12:03,360 --> 00:12:07,360
every election year the spread of hate

00:12:06,399 --> 00:12:09,120
speech

00:12:07,360 --> 00:12:11,440
had been a problem in previous election

00:12:09,120 --> 00:12:12,560
years so the communication authority was

00:12:11,440 --> 00:12:15,200
being very strict

00:12:12,560 --> 00:12:16,720
about ensuring operators did not let any

00:12:15,200 --> 00:12:19,519
hate speech be sent

00:12:16,720 --> 00:12:20,399
using their infrastructure what that

00:12:19,519 --> 00:12:23,519
looked like

00:12:20,399 --> 00:12:25,920
for us internally was thinking through

00:12:23,519 --> 00:12:27,760
a technical solution to identify and

00:12:25,920 --> 00:12:29,680
therefore stop hate speech

00:12:27,760 --> 00:12:31,519
and the most elegant thing we could come

00:12:29,680 --> 00:12:35,440
up with which was extremely

00:12:31,519 --> 00:12:38,720
inelegant in my opinion was first

00:12:35,440 --> 00:12:40,240
to pass with a language identifier

00:12:38,720 --> 00:12:41,760
to pass the messages with a language

00:12:40,240 --> 00:12:44,320
identifier

00:12:41,760 --> 00:12:46,160
if the message was in english we could

00:12:44,320 --> 00:12:48,560
pass the text for keywords

00:12:46,160 --> 00:12:51,440
that might signal hate speech and block

00:12:48,560 --> 00:12:53,519
those messages for manual inspection

00:12:51,440 --> 00:12:55,120
messages in all other languages were

00:12:53,519 --> 00:12:58,320
automatically stopped

00:12:55,120 --> 00:12:59,519
and needed manual inspection to put this

00:12:58,320 --> 00:13:02,160
in context

00:12:59,519 --> 00:13:03,600
we have over 44 different tribes and

00:13:02,160 --> 00:13:05,200
therefore 44 plus

00:13:03,600 --> 00:13:07,360
different languages are spoken in the

00:13:05,200 --> 00:13:09,680
country

00:13:07,360 --> 00:13:11,519
messages in kiswahili which is also a

00:13:09,680 --> 00:13:13,360
national language in the country

00:13:11,519 --> 00:13:15,600
could be verified by anyone in the

00:13:13,360 --> 00:13:17,440
office and for good measure

00:13:15,600 --> 00:13:20,079
because of the tribal dynamics of

00:13:17,440 --> 00:13:20,880
elections we simply barred all messages

00:13:20,079 --> 00:13:24,480
that were not

00:13:20,880 --> 00:13:26,000
in either english or kiswahili

00:13:24,480 --> 00:13:28,079
the fact that there were no tools to

00:13:26,000 --> 00:13:30,240
support better analysis

00:13:28,079 --> 00:13:32,880
of text in other languages spoken in the

00:13:30,240 --> 00:13:37,200
country is probably what got me

00:13:32,880 --> 00:13:40,000
on intent on working particularly in nlp

00:13:37,200 --> 00:13:40,000
next slide please

00:13:40,320 --> 00:13:44,160
i have also spent a lot of my time

00:13:42,720 --> 00:13:46,560
building and supporting

00:13:44,160 --> 00:13:48,639
data science machine learning and

00:13:46,560 --> 00:13:50,160
generally ai communities on the african

00:13:48,639 --> 00:13:53,199
continent

00:13:50,160 --> 00:13:54,959
why um because

00:13:53,199 --> 00:13:57,120
as many of them are focused first and

00:13:54,959 --> 00:13:58,720
foremost on capacity building

00:13:57,120 --> 00:14:01,040
it has always been a great way to ensure

00:13:58,720 --> 00:14:01,760
that i have avenues to grow my technical

00:14:01,040 --> 00:14:03,440
skill

00:14:01,760 --> 00:14:05,680
as well as interact with others in the

00:14:03,440 --> 00:14:08,399
same professional field

00:14:05,680 --> 00:14:10,560
beyond that they helped me to situate

00:14:08,399 --> 00:14:11,040
myself within tech and particularly

00:14:10,560 --> 00:14:14,160
within

00:14:11,040 --> 00:14:14,720
ai women in machine learning and data

00:14:14,160 --> 00:14:16,880
science

00:14:14,720 --> 00:14:19,760
creates a platform by which women can

00:14:16,880 --> 00:14:22,000
explore and venture into ai careers

00:14:19,760 --> 00:14:24,000
through my journey venturing into tech

00:14:22,000 --> 00:14:25,839
i'd become aware of the fact that there

00:14:24,000 --> 00:14:27,440
were less and less women the further

00:14:25,839 --> 00:14:30,000
along i got

00:14:27,440 --> 00:14:32,480
in my undergrad class i was one of four

00:14:30,000 --> 00:14:34,880
women in a class of 55

00:14:32,480 --> 00:14:35,920
at my first job i was one of two women

00:14:34,880 --> 00:14:39,199
on the tech team

00:14:35,920 --> 00:14:41,360
out of about 10 to start with so it was

00:14:39,199 --> 00:14:43,519
important for me to create pathways for

00:14:41,360 --> 00:14:45,600
other women to enter the fields

00:14:43,519 --> 00:14:46,560
i co-organized the nairobi chapter for

00:14:45,600 --> 00:14:48,000
three years

00:14:46,560 --> 00:14:50,079
and we went on to hire from that

00:14:48,000 --> 00:14:50,959
community when they cam when the time

00:14:50,079 --> 00:14:53,519
came to

00:14:50,959 --> 00:14:54,959
to grow the data department at my place

00:14:53,519 --> 00:14:58,079
of work

00:14:54,959 --> 00:15:00,560
next slide please

00:14:58,079 --> 00:15:02,480
which brings me back to my desire to

00:15:00,560 --> 00:15:03,920
work on language tools for african

00:15:02,480 --> 00:15:07,279
languages

00:15:03,920 --> 00:15:09,519
why because it really is a problem

00:15:07,279 --> 00:15:10,639
for which the tooling to solve already

00:15:09,519 --> 00:15:13,680
exists

00:15:10,639 --> 00:15:15,360
it just hasn't been a research focus

00:15:13,680 --> 00:15:17,279
because there aren't nearly enough

00:15:15,360 --> 00:15:19,760
researchers that look like me

00:15:17,279 --> 00:15:20,720
that can first identify that this is a

00:15:19,760 --> 00:15:23,279
problem

00:15:20,720 --> 00:15:24,560
and then proceed to work towards solving

00:15:23,279 --> 00:15:27,360
it

00:15:24,560 --> 00:15:28,959
also because language and culture are

00:15:27,360 --> 00:15:31,759
intricately linked

00:15:28,959 --> 00:15:34,079
beyond the surface meaning of words and

00:15:31,759 --> 00:15:35,440
phrases that constitute a language

00:15:34,079 --> 00:15:37,440
there is a lot of nuance to

00:15:35,440 --> 00:15:40,240
communication and that

00:15:37,440 --> 00:15:42,399
is heavily reliant on context culture

00:15:40,240 --> 00:15:45,279
and shared experiences

00:15:42,399 --> 00:15:46,959
so i made the decision to leave industry

00:15:45,279 --> 00:15:50,240
and go back to school to get

00:15:46,959 --> 00:15:53,279
my masters why

00:15:50,240 --> 00:15:55,279
because while my job had been able to

00:15:53,279 --> 00:15:57,680
grant me the gift of breath

00:15:55,279 --> 00:16:00,480
which was great for exploring the field

00:15:57,680 --> 00:16:03,279
i now knew what i wanted to go deep into

00:16:00,480 --> 00:16:05,120
and unfortunately my job could not

00:16:03,279 --> 00:16:07,279
afford me the depth as well

00:16:05,120 --> 00:16:09,120
i wanted to build nlp tools for african

00:16:07,279 --> 00:16:11,279
languages

00:16:09,120 --> 00:16:13,199
it is also important to me as a leader

00:16:11,279 --> 00:16:16,399
in the african ai space

00:16:13,199 --> 00:16:19,040
to have that level of credibility we

00:16:16,399 --> 00:16:20,959
are after all doing science it's not

00:16:19,040 --> 00:16:22,720
just about tinkering with tools to see

00:16:20,959 --> 00:16:25,519
what happens

00:16:22,720 --> 00:16:25,519
next slide please

00:16:26,240 --> 00:16:30,480
which then brings me to masakane yet

00:16:29,199 --> 00:16:34,000
another community i got

00:16:30,480 --> 00:16:36,320
involved in why well

00:16:34,000 --> 00:16:37,759
masakane is focused on specialized

00:16:36,320 --> 00:16:41,440
capacity building for

00:16:37,759 --> 00:16:43,759
african nlp research the reason i knew

00:16:41,440 --> 00:16:45,120
masakani was the right fit for me

00:16:43,759 --> 00:16:47,199
is because of the values that are

00:16:45,120 --> 00:16:50,000
central to the work

00:16:47,199 --> 00:16:51,600
masakane is a zulu word that means we

00:16:50,000 --> 00:16:54,880
build together

00:16:51,600 --> 00:16:56,000
this ethos plays hope pays homage to the

00:16:54,880 --> 00:16:58,720
fact that

00:16:56,000 --> 00:16:59,519
fair responsible and localized ai

00:16:58,720 --> 00:17:01,920
systems

00:16:59,519 --> 00:17:03,040
can only be built with the invite with

00:17:01,920 --> 00:17:05,919
the involvement

00:17:03,040 --> 00:17:06,799
of locals one cannot just enter a

00:17:05,919 --> 00:17:10,000
community

00:17:06,799 --> 00:17:12,559
or an ecosystem and imagine that

00:17:10,000 --> 00:17:14,480
because they have technical skill they

00:17:12,559 --> 00:17:16,400
are best suited to build technical

00:17:14,480 --> 00:17:18,720
solutions

00:17:16,400 --> 00:17:19,520
the outcome when we are not involved in

00:17:18,720 --> 00:17:22,640
the process

00:17:19,520 --> 00:17:24,959
is already apparent we are all now

00:17:22,640 --> 00:17:28,000
reliant on a digital space that does not

00:17:24,959 --> 00:17:28,960
understand our names our cultures our

00:17:28,000 --> 00:17:33,840
places

00:17:28,960 --> 00:17:33,840
or our history next slide please

00:17:34,480 --> 00:17:39,679
now working in low resource nlp is no

00:17:38,640 --> 00:17:43,039
easy task

00:17:39,679 --> 00:17:46,160
there are many challenges most glaring

00:17:43,039 --> 00:17:48,880
is a lack of availability of data

00:17:46,160 --> 00:17:49,919
creating data sets is time consuming and

00:17:48,880 --> 00:17:51,679
expensive

00:17:49,919 --> 00:17:53,440
so many researchers will choose what

00:17:51,679 --> 00:17:55,039
they work on based on whether the data

00:17:53,440 --> 00:17:57,840
exists

00:17:55,039 --> 00:17:59,520
where the data is available data quality

00:17:57,840 --> 00:18:01,520
is another issue

00:17:59,520 --> 00:18:03,200
because of the evangelical work that

00:18:01,520 --> 00:18:04,799
missionaries did in africa during

00:18:03,200 --> 00:18:07,200
colonial times

00:18:04,799 --> 00:18:09,360
most of the digitized resources of

00:18:07,200 --> 00:18:12,840
african language data in existence

00:18:09,360 --> 00:18:14,160
are religious texts the use of this data

00:18:12,840 --> 00:18:16,480
exclusively

00:18:14,160 --> 00:18:18,720
leads to a phenomenon we have dubbed the

00:18:16,480 --> 00:18:20,799
biblification of systems

00:18:18,720 --> 00:18:22,799
we noted during evaluation of the

00:18:20,799 --> 00:18:24,240
machine translation models trained

00:18:22,799 --> 00:18:26,960
within masakane

00:18:24,240 --> 00:18:28,000
that canada for example kept being

00:18:26,960 --> 00:18:31,679
translated into

00:18:28,000 --> 00:18:33,120
canon there is also the matter of their

00:18:31,679 --> 00:18:35,039
critics

00:18:33,120 --> 00:18:36,720
many african languages have their

00:18:35,039 --> 00:18:39,760
critics which are the

00:18:36,720 --> 00:18:42,880
various accents you would see on letters

00:18:39,760 --> 00:18:43,840
signaling perhaps a variation in tone a

00:18:42,880 --> 00:18:46,320
word could mean

00:18:43,840 --> 00:18:48,320
two or three completely different things

00:18:46,320 --> 00:18:49,039
based on the diacritics that accompany

00:18:48,320 --> 00:18:51,120
it

00:18:49,039 --> 00:18:53,120
and this level of correctness is not

00:18:51,120 --> 00:18:56,080
always respected

00:18:53,120 --> 00:18:58,160
bbc for example has a news website on

00:18:56,080 --> 00:19:01,039
which they publish news in yoruba

00:18:58,160 --> 00:19:02,400
a language spoken widely in west africa

00:19:01,039 --> 00:19:05,840
but they do not bother to

00:19:02,400 --> 00:19:08,240
properly diacritize the text

00:19:05,840 --> 00:19:10,799
then comes the bigger challenge where

00:19:08,240 --> 00:19:12,320
the data simply does not exist

00:19:10,799 --> 00:19:15,440
in the cases where languages are more

00:19:12,320 --> 00:19:17,280
spoken than anything

00:19:15,440 --> 00:19:18,640
instances where the people that speak

00:19:17,280 --> 00:19:21,679
these languages

00:19:18,640 --> 00:19:23,120
are literally its custodians there are

00:19:21,679 --> 00:19:25,760
actually many languages that

00:19:23,120 --> 00:19:27,280
could become extinct extinct in coming

00:19:25,760 --> 00:19:29,200
decades

00:19:27,280 --> 00:19:30,559
my parents belong to a tribe known as

00:19:29,200 --> 00:19:33,760
the luya

00:19:30,559 --> 00:19:35,600
the luya are a group of 16 to 18

00:19:33,760 --> 00:19:38,240
communities who have a shared

00:19:35,600 --> 00:19:39,679
socio-cultural history and whose

00:19:38,240 --> 00:19:41,840
languages in some cases

00:19:39,679 --> 00:19:43,679
are very similar and mutually

00:19:41,840 --> 00:19:45,600
intelligible

00:19:43,679 --> 00:19:47,919
it is possible for some members of

00:19:45,600 --> 00:19:51,039
different communities to communicate

00:19:47,919 --> 00:19:52,960
each speaking their native tongues

00:19:51,039 --> 00:19:54,960
now because these languages have always

00:19:52,960 --> 00:19:57,120
been relegated to the home

00:19:54,960 --> 00:19:58,080
never used in education or other formal

00:19:57,120 --> 00:19:59,679
settings

00:19:58,080 --> 00:20:01,600
they do not have standard writing

00:19:59,679 --> 00:20:04,320
systems which means

00:20:01,600 --> 00:20:06,159
two individuals from one community both

00:20:04,320 --> 00:20:08,159
fluent in their mother tongue can

00:20:06,159 --> 00:20:09,440
disagree on the spelling of various

00:20:08,159 --> 00:20:11,120
words

00:20:09,440 --> 00:20:13,360
because these languages are more oral

00:20:11,120 --> 00:20:15,440
than written speech systems would be

00:20:13,360 --> 00:20:17,360
valuable for these communities

00:20:15,440 --> 00:20:19,120
but how does one begin to build these

00:20:17,360 --> 00:20:22,400
speech systems given that

00:20:19,120 --> 00:20:24,559
the transcribed data does not exist

00:20:22,400 --> 00:20:27,600
given that standard agreed upon spelling

00:20:24,559 --> 00:20:30,080
of words also do not exist

00:20:27,600 --> 00:20:32,000
it's the hardest most exciting problem

00:20:30,080 --> 00:20:34,480
i've come across thus far

00:20:32,000 --> 00:20:35,520
which is why i'm choosing to work on it

00:20:34,480 --> 00:20:37,919
i spent

00:20:35,520 --> 00:20:40,480
the better part of last year working on

00:20:37,919 --> 00:20:43,760
transcription systems that map sounds to

00:20:40,480 --> 00:20:44,960
ipa that's the international phonetic

00:20:43,760 --> 00:20:47,200
alphabets

00:20:44,960 --> 00:20:48,559
it is a standardized representation of

00:20:47,200 --> 00:20:50,880
speech sounds

00:20:48,559 --> 00:20:52,559
in written form and should help us

00:20:50,880 --> 00:20:55,360
bypass the fact that

00:20:52,559 --> 00:20:57,840
many languages in africa do not have

00:20:55,360 --> 00:21:03,120
formal return systems

00:20:57,840 --> 00:21:06,320
next slide please which brings me to

00:21:03,120 --> 00:21:09,440
my parting shots i firmly believe

00:21:06,320 --> 00:21:11,520
that the responsible thing to do

00:21:09,440 --> 00:21:13,440
when building ai technologies for

00:21:11,520 --> 00:21:15,440
marginalized communities

00:21:13,440 --> 00:21:16,559
is to natively involve them in the

00:21:15,440 --> 00:21:18,880
process

00:21:16,559 --> 00:21:20,720
equipping them with a technical know-how

00:21:18,880 --> 00:21:24,159
and centering their experiences

00:21:20,720 --> 00:21:27,280
in development of any solutions so

00:21:24,159 --> 00:21:30,480
i leave you with this quote what is done

00:21:27,280 --> 00:21:33,280
for me without me is done against me

00:21:30,480 --> 00:21:33,280
thank you very much

00:21:34,240 --> 00:21:37,679
kathleen thank you so so much for this

00:21:37,120 --> 00:21:40,080
talk

00:21:37,679 --> 00:21:41,360
um really inspiring i mean especially

00:21:40,080 --> 00:21:42,320
the last quote that you have i think

00:21:41,360 --> 00:21:44,320
about it a lot

00:21:42,320 --> 00:21:46,159
um especially being in the field of

00:21:44,320 --> 00:21:46,960
technology and human rights this idea of

00:21:46,159 --> 00:21:49,760
parachuting

00:21:46,960 --> 00:21:51,679
in and coming into certain contexts and

00:21:49,760 --> 00:21:53,360
building tools for people but without

00:21:51,679 --> 00:21:55,120
including people in the development of

00:21:53,360 --> 00:21:56,240
this tool so this idea that like you

00:21:55,120 --> 00:21:58,320
have to be inclusive

00:21:56,240 --> 00:21:59,760
in building ai and in thinking about the

00:21:58,320 --> 00:22:01,039
context that you're in is so so

00:21:59,760 --> 00:22:03,520
important and thank you for driving that

00:22:01,039 --> 00:22:03,520
point home

00:22:03,600 --> 00:22:06,799
yeah and i totally agree thank you

00:22:05,600 --> 00:22:09,919
kathleen and

00:22:06,799 --> 00:22:11,360
um this is actually at the center of the

00:22:09,919 --> 00:22:12,000
conversation we're having here around

00:22:11,360 --> 00:22:14,400
data and

00:22:12,000 --> 00:22:15,760
powers who's involved who who gets to

00:22:14,400 --> 00:22:17,760
have a say

00:22:15,760 --> 00:22:20,080
and you know kathleen's story really

00:22:17,760 --> 00:22:20,880
shows people taking control of data that

00:22:20,080 --> 00:22:22,720
matters

00:22:20,880 --> 00:22:24,159
most of them and it's not even data it's

00:22:22,720 --> 00:22:26,320
like their lives their

00:22:24,159 --> 00:22:27,520
culture their language and that's what

00:22:26,320 --> 00:22:29,520
really what we're talking about that

00:22:27,520 --> 00:22:32,159
that can change the overall

00:22:29,520 --> 00:22:33,600
power dynamics and it's you know i'll

00:22:32,159 --> 00:22:34,799
say as a side piece

00:22:33,600 --> 00:22:36,960
in line with what we're trying to do

00:22:34,799 --> 00:22:38,960
with mozilla common voice which relates

00:22:36,960 --> 00:22:40,640
a little bit to some of what

00:22:38,960 --> 00:22:42,080
kathleen talked about and what masakana

00:22:40,640 --> 00:22:43,760
is doing so hopefully we can build some

00:22:42,080 --> 00:22:45,360
connections there between calm the voice

00:22:43,760 --> 00:22:48,400
and masakani coming

00:22:45,360 --> 00:22:51,280
out of mazdas um

00:22:48,400 --> 00:22:53,200
so another person who is doing concrete

00:22:51,280 --> 00:22:55,039
things to shift power dynamics around

00:22:53,200 --> 00:22:57,120
data is james rar

00:22:55,039 --> 00:22:58,080
james co-founded the worker info

00:22:57,120 --> 00:22:59,679
exchange

00:22:58,080 --> 00:23:02,559
and is one of the first grantees to be

00:22:59,679 --> 00:23:04,640
supported by mozilla's data futures lab

00:23:02,559 --> 00:23:06,400
james and his compatriots have shown

00:23:04,640 --> 00:23:08,880
that gig economy

00:23:06,400 --> 00:23:10,640
companies often use their control over

00:23:08,880 --> 00:23:13,360
data to block workers

00:23:10,640 --> 00:23:14,960
from accessing their most basic rights

00:23:13,360 --> 00:23:16,880
and they've also shown how workers can

00:23:14,960 --> 00:23:17,919
get those rights back by taking control

00:23:16,880 --> 00:23:21,760
of their own

00:23:17,919 --> 00:23:23,280
data oh awesome mark thank you so much

00:23:21,760 --> 00:23:25,760
i'm really excited for this one uh

00:23:23,280 --> 00:23:26,880
everyone please uh join me in welcoming

00:23:25,760 --> 00:23:30,000
uh our next speaker

00:23:26,880 --> 00:23:31,679
james farrar oh well thank you very much

00:23:30,000 --> 00:23:32,720
for the introduction and um thanks for

00:23:31,679 --> 00:23:35,600
having me here

00:23:32,720 --> 00:23:37,280
um i was thinking about data and power

00:23:35,600 --> 00:23:39,200
but i was also thinking about the word

00:23:37,280 --> 00:23:40,559
purpose and it's really been an

00:23:39,200 --> 00:23:42,000
important theme in the work that i've

00:23:40,559 --> 00:23:44,559
been doing over the past

00:23:42,000 --> 00:23:45,360
few years as i've moved from a career in

00:23:44,559 --> 00:23:46,960
software

00:23:45,360 --> 00:23:48,480
to the other side of the disruption

00:23:46,960 --> 00:23:51,039
chasm and then further on

00:23:48,480 --> 00:23:51,919
still but the thing about purpose is

00:23:51,039 --> 00:23:53,600
that it means

00:23:51,919 --> 00:23:55,120
it's a multi-purpose word it's a

00:23:53,600 --> 00:23:57,679
multi-purposeful word

00:23:55,120 --> 00:23:59,440
it can mean values as in your value set

00:23:57,679 --> 00:24:00,080
what is your purpose what's your purpose

00:23:59,440 --> 00:24:02,320
of

00:24:00,080 --> 00:24:03,760
of what you do it can be a function

00:24:02,320 --> 00:24:04,400
what's the purpose of this thing we're

00:24:03,760 --> 00:24:06,559
building

00:24:04,400 --> 00:24:08,000
but it can also be a design feature what

00:24:06,559 --> 00:24:09,760
is the purpose of what we're designing

00:24:08,000 --> 00:24:12,320
here what are we designing it for

00:24:09,760 --> 00:24:14,240
and these are important aspects of

00:24:12,320 --> 00:24:15,039
software and systems design but they're

00:24:14,240 --> 00:24:18,799
also

00:24:15,039 --> 00:24:21,600
important aspects of achieving justice

00:24:18,799 --> 00:24:24,240
of understanding what your relationship

00:24:21,600 --> 00:24:27,919
is with an employer in society and so on

00:24:24,240 --> 00:24:30,960
and next slide please so i

00:24:27,919 --> 00:24:33,840
um my purpose of work i i moved

00:24:30,960 --> 00:24:34,880
not a typical um career path i might add

00:24:33,840 --> 00:24:37,440
but i moved from

00:24:34,880 --> 00:24:39,120
a career in software at sap and to

00:24:37,440 --> 00:24:40,799
become an uber driver the other side of

00:24:39,120 --> 00:24:42,320
the the software

00:24:40,799 --> 00:24:44,880
of the disruption chasm and when i was

00:24:42,320 --> 00:24:47,200
at sap i was working in an incubator

00:24:44,880 --> 00:24:48,480
we were craving the idea that if we

00:24:47,200 --> 00:24:49,679
could only get our hands on some

00:24:48,480 --> 00:24:51,279
disruption i mean we were in an

00:24:49,679 --> 00:24:53,600
enterprise software company

00:24:51,279 --> 00:24:54,640
we dreamt of bringing social um

00:24:53,600 --> 00:24:57,360
disruption

00:24:54,640 --> 00:24:58,080
to that world and what it could unleash

00:24:57,360 --> 00:24:59,840
um

00:24:58,080 --> 00:25:01,679
but you know we had middle in results

00:24:59,840 --> 00:25:02,720
but then i went over to uber to see what

00:25:01,679 --> 00:25:06,080
that was like

00:25:02,720 --> 00:25:07,679
uh as a driver um and it was interesting

00:25:06,080 --> 00:25:08,720
you know the purpose the purpose was the

00:25:07,679 --> 00:25:10,960
same you know

00:25:08,720 --> 00:25:13,520
um sap wanted to make the world run

00:25:10,960 --> 00:25:16,320
better uber wanted to reinvent transport

00:25:13,520 --> 00:25:18,799
all great potential purposes for society

00:25:16,320 --> 00:25:21,679
next slide please

00:25:18,799 --> 00:25:23,440
and so i signed up at the uber um office

00:25:21,679 --> 00:25:25,039
in london and this is what i found

00:25:23,440 --> 00:25:26,000
behind the podium sorry about the poor

00:25:25,039 --> 00:25:28,080
quality

00:25:26,000 --> 00:25:30,400
of the of the picture but this is the

00:25:28,080 --> 00:25:31,440
promise to drivers to precarious workers

00:25:30,400 --> 00:25:33,679
maybe coming from

00:25:31,440 --> 00:25:34,720
let's face it even worse employment

00:25:33,679 --> 00:25:36,559
situations than

00:25:34,720 --> 00:25:37,760
uber let's be honest about it but what

00:25:36,559 --> 00:25:39,919
he had to say as if

00:25:37,760 --> 00:25:41,600
is that i make three times more money

00:25:39,919 --> 00:25:42,080
compared to my previous career driver

00:25:41,600 --> 00:25:44,240
job

00:25:42,080 --> 00:25:45,279
with uber i don't need to worry about

00:25:44,240 --> 00:25:47,120
bills

00:25:45,279 --> 00:25:48,880
who doesn't need to worry about bills if

00:25:47,120 --> 00:25:49,760
my child wants a new jacket i tell them

00:25:48,880 --> 00:25:51,919
you know what buy

00:25:49,760 --> 00:25:53,520
two of them if you want this is life at

00:25:51,919 --> 00:25:55,520
uber as an uber driver

00:25:53,520 --> 00:25:58,000
and of course that's not the reality the

00:25:55,520 --> 00:25:59,840
reality is it takes you about 30 hours a

00:25:58,000 --> 00:26:01,360
week to break even because you've got to

00:25:59,840 --> 00:26:02,799
cover your running costs before you make

00:26:01,360 --> 00:26:05,360
something for yourself

00:26:02,799 --> 00:26:07,360
it's it's very dangerously long hours

00:26:05,360 --> 00:26:08,559
it's very tough work it's very lonely

00:26:07,360 --> 00:26:10,400
work

00:26:08,559 --> 00:26:11,840
but this is the dream that's been sold

00:26:10,400 --> 00:26:14,159
to precarious workers that you would

00:26:11,840 --> 00:26:16,080
have this entrepreneurial freedom

00:26:14,159 --> 00:26:18,159
uh and you would have this unlimited

00:26:16,080 --> 00:26:19,840
opportunity now the freedom there is

00:26:18,159 --> 00:26:21,520
something to it because when i went in

00:26:19,840 --> 00:26:23,360
through the doors and i sat down through

00:26:21,520 --> 00:26:24,000
the session a lot of these guys were

00:26:23,360 --> 00:26:26,240
coming from

00:26:24,000 --> 00:26:28,080
and women actually only two percent of

00:26:26,240 --> 00:26:30,880
drivers in london are women

00:26:28,080 --> 00:26:32,799
um that's a whole other topic but a lot

00:26:30,880 --> 00:26:34,159
of these uh people were

00:26:32,799 --> 00:26:36,640
in the session were coming from more

00:26:34,159 --> 00:26:37,120
traditional firms uh where you had you

00:26:36,640 --> 00:26:39,600
know

00:26:37,120 --> 00:26:41,120
a cab office and a dispatcher who would

00:26:39,600 --> 00:26:42,480
sort of shout at the drivers and have

00:26:41,120 --> 00:26:44,559
favorites and it was all

00:26:42,480 --> 00:26:46,159
pretty rough and tumble not very nice

00:26:44,559 --> 00:26:47,840
but when they came in here uber said you

00:26:46,159 --> 00:26:48,400
know what the best thing here about uber

00:26:47,840 --> 00:26:51,120
is

00:26:48,400 --> 00:26:53,679
the algorithm decides who gets the work

00:26:51,120 --> 00:26:56,240
not the not this bad boss in the office

00:26:53,679 --> 00:26:56,880
who you have to um humor the algorithm

00:26:56,240 --> 00:26:59,120
decides

00:26:56,880 --> 00:27:01,279
and there was a huge emotional cheer of

00:26:59,120 --> 00:27:03,520
100 people on their feet cheering and i

00:27:01,279 --> 00:27:05,520
you know for me coming from sap i never

00:27:03,520 --> 00:27:07,760
heard anybody cheer an algorithm before

00:27:05,520 --> 00:27:09,279
but it tells you something about about

00:27:07,760 --> 00:27:10,559
this the final thing that i remember

00:27:09,279 --> 00:27:12,880
from that first day

00:27:10,559 --> 00:27:15,279
is that coming from sap i was interested

00:27:12,880 --> 00:27:17,039
in real-time information

00:27:15,279 --> 00:27:18,399
really good quality analytics and i

00:27:17,039 --> 00:27:20,000
thought this must be really interesting

00:27:18,399 --> 00:27:22,080
because i bet you you can show me where

00:27:20,000 --> 00:27:24,080
supply and demand is in real time

00:27:22,080 --> 00:27:25,919
on a map in london on a heat map and so

00:27:24,080 --> 00:27:27,200
on and they kind of just looked at me

00:27:25,919 --> 00:27:28,960
stunningly and said

00:27:27,200 --> 00:27:30,640
yeah that's proprietary information

00:27:28,960 --> 00:27:32,320
you'll never see that we can't share

00:27:30,640 --> 00:27:33,520
that with you but i'm thinking but i'm

00:27:32,320 --> 00:27:36,159
i'm working for you and

00:27:33,520 --> 00:27:39,360
i'm on the team now uh no it wasn't to

00:27:36,159 --> 00:27:39,360
be next slide please

00:27:39,520 --> 00:27:43,279
and so this is the picture of all the

00:27:41,200 --> 00:27:45,360
work i did at london in london working

00:27:43,279 --> 00:27:46,640
for uber this is the data when i finally

00:27:45,360 --> 00:27:48,480
got my hands on it

00:27:46,640 --> 00:27:50,240
after many years and lots of lawyer

00:27:48,480 --> 00:27:52,559
letters i got my data and i was able to

00:27:50,240 --> 00:27:55,279
map it and this is everywhere i ever was

00:27:52,559 --> 00:27:56,480
the red is slow speed the lighter colors

00:27:55,279 --> 00:27:59,760
are higher speeds

00:27:56,480 --> 00:28:00,640
heathrow airport on the left and i even

00:27:59,760 --> 00:28:02,960
got to map

00:28:00,640 --> 00:28:04,559
and find on through the data and through

00:28:02,960 --> 00:28:07,360
the gps plotting

00:28:04,559 --> 00:28:08,799
an incident that changed my life and uh

00:28:07,360 --> 00:28:10,000
changed the maybe the course of

00:28:08,799 --> 00:28:12,720
employment law

00:28:10,000 --> 00:28:14,480
if you uh if you to read the academics i

00:28:12,720 --> 00:28:16,559
was assaulted one night and uber

00:28:14,480 --> 00:28:18,240
would not identify to the police or to

00:28:16,559 --> 00:28:19,440
me who the passenger was and they

00:28:18,240 --> 00:28:20,559
wouldn't cooperate with the police for

00:28:19,440 --> 00:28:24,000
10 weeks

00:28:20,559 --> 00:28:25,600
next slide please and so i began to

00:28:24,000 --> 00:28:27,039
understand why that was when i started

00:28:25,600 --> 00:28:28,880
looking at the contract the way this is

00:28:27,039 --> 00:28:30,080
set up is that the drivers contracted

00:28:28,880 --> 00:28:33,039
with the passenger the passenger

00:28:30,080 --> 00:28:34,880
is contracted with the driver um and

00:28:33,039 --> 00:28:36,960
uber is acting only as an intermediary

00:28:34,880 --> 00:28:39,679
and so it doesn't want to really know

00:28:36,960 --> 00:28:41,440
um about my little problems and the and

00:28:39,679 --> 00:28:43,520
it because it doesn't want to trigger

00:28:41,440 --> 00:28:45,039
public liability with passengers and it

00:28:43,520 --> 00:28:47,440
doesn't want to trigger

00:28:45,039 --> 00:28:48,320
employment liability with me and so what

00:28:47,440 --> 00:28:50,159
you've got is

00:28:48,320 --> 00:28:52,320
set up here is this very artificial

00:28:50,159 --> 00:28:54,159
contractual relationship

00:28:52,320 --> 00:28:55,600
um that says i'm the entrepreneur i'm

00:28:54,159 --> 00:28:57,360
the transportation provider they're

00:28:55,600 --> 00:28:59,279
simply a technology company

00:28:57,360 --> 00:29:00,640
and you have to agree these things that

00:28:59,279 --> 00:29:02,240
was the interesting thing that we found

00:29:00,640 --> 00:29:03,760
out in court that the judges really

00:29:02,240 --> 00:29:05,679
attacked hard was that

00:29:03,760 --> 00:29:07,200
we were not only agreeing obligations

00:29:05,679 --> 00:29:08,159
but we were agreeing things that just

00:29:07,200 --> 00:29:09,679
weren't true

00:29:08,159 --> 00:29:11,600
that is that uber is not a

00:29:09,679 --> 00:29:12,080
transportation company it's a technology

00:29:11,600 --> 00:29:13,919
company

00:29:12,080 --> 00:29:15,840
but you know we had no bargaining power

00:29:13,919 --> 00:29:17,360
so if uber asks us to agree that that

00:29:15,840 --> 00:29:18,320
the you know the sky is green and the

00:29:17,360 --> 00:29:20,399
grass is blue

00:29:18,320 --> 00:29:21,360
we would have said sure we'll sign that

00:29:20,399 --> 00:29:23,200
as well

00:29:21,360 --> 00:29:24,640
and and that brings you to the problem

00:29:23,200 --> 00:29:26,799
of these types of contracts but the

00:29:24,640 --> 00:29:28,080
other element of control undoubtedly and

00:29:26,799 --> 00:29:30,159
has intensified

00:29:28,080 --> 00:29:32,080
is surveillance and algorithmic control

00:29:30,159 --> 00:29:33,399
algorithmic management control

00:29:32,080 --> 00:29:36,000
because this is all part of the

00:29:33,399 --> 00:29:37,440
misclassification game to conceal the

00:29:36,000 --> 00:29:38,880
true relationship we're in

00:29:37,440 --> 00:29:41,679
and make no mistake we're in a

00:29:38,880 --> 00:29:43,440
relationship and part of our struggle

00:29:41,679 --> 00:29:45,279
is to prove that we're in an employment

00:29:43,440 --> 00:29:47,120
relationship and that means we have to

00:29:45,279 --> 00:29:49,440
evidence the management control and

00:29:47,120 --> 00:29:51,039
increasingly that's hidden in algorithm

00:29:49,440 --> 00:29:52,480
you know have you ever been in a

00:29:51,039 --> 00:29:54,240
situation where you're trying to

00:29:52,480 --> 00:29:55,600
convince the other person that you're in

00:29:54,240 --> 00:29:57,279
a relationship

00:29:55,600 --> 00:29:59,440
with them and they deny that they are i

00:29:57,279 --> 00:30:01,279
mean before uber the last time that

00:29:59,440 --> 00:30:05,279
happened to me was in high school but

00:30:01,279 --> 00:30:07,679
that's another talk um next slide please

00:30:05,279 --> 00:30:09,120
and so we get into this spinning game so

00:30:07,679 --> 00:30:11,600
we spin the data

00:30:09,120 --> 00:30:12,880
so one week i worked 91 hours i earned

00:30:11,600 --> 00:30:16,640
503 an hour

00:30:12,880 --> 00:30:18,799
which is about uh 70 um about 75 percent

00:30:16,640 --> 00:30:21,520
of the national minimum wage at the time

00:30:18,799 --> 00:30:22,399
and i only accepted about 40 of the work

00:30:21,520 --> 00:30:24,399
offered

00:30:22,399 --> 00:30:25,840
now uber went to court and it also went

00:30:24,399 --> 00:30:28,640
to parliament with this exact

00:30:25,840 --> 00:30:30,080
example and said well look at the look

00:30:28,640 --> 00:30:33,679
at the sovereignty he had

00:30:30,080 --> 00:30:35,360
he was able to decline all that work but

00:30:33,679 --> 00:30:36,960
when i looked at the data when i finally

00:30:35,360 --> 00:30:38,720
got it i found well actually my

00:30:36,960 --> 00:30:40,799
productivity rate

00:30:38,720 --> 00:30:43,440
was higher than what uber was expecting

00:30:40,799 --> 00:30:44,960
at 40 to 6 hours i was at 91 hours

00:30:43,440 --> 00:30:46,720
it didn't really matter the work i was

00:30:44,960 --> 00:30:48,000
declining what mattered was the work i

00:30:46,720 --> 00:30:50,320
was doing

00:30:48,000 --> 00:30:51,440
um and it's just it made me at that

00:30:50,320 --> 00:30:53,039
moment realized that

00:30:51,440 --> 00:30:54,320
workers needed to control their data

00:30:53,039 --> 00:30:55,840
they needed to be able to tell their own

00:30:54,320 --> 00:30:57,039
story they needed to be able to defend

00:30:55,840 --> 00:31:00,399
themselves and bargain

00:30:57,039 --> 00:31:02,399
next slide please so we also spin the

00:31:00,399 --> 00:31:04,320
communication so what uber says is that

00:31:02,399 --> 00:31:06,720
you're not the worker you're

00:31:04,320 --> 00:31:08,399
the customer uh get your head around

00:31:06,720 --> 00:31:09,840
that you don't have a job you have an

00:31:08,399 --> 00:31:11,120
opportunity to earn money we can't call

00:31:09,840 --> 00:31:13,120
it a job that might trigger

00:31:11,120 --> 00:31:14,799
a legal responsibility in employment

00:31:13,120 --> 00:31:16,399
we're not a transportation company we're

00:31:14,799 --> 00:31:17,679
definitely a tech company

00:31:16,399 --> 00:31:19,360
except the courts have all said that

00:31:17,679 --> 00:31:20,640
that's not true and you're regulated as

00:31:19,360 --> 00:31:22,080
a transportation company

00:31:20,640 --> 00:31:24,559
and you're definitely not fired or

00:31:22,080 --> 00:31:28,720
dismissed you're deactivated are

00:31:24,559 --> 00:31:28,720
disaffiliated next slide please

00:31:28,960 --> 00:31:32,720
and so your performance managed by

00:31:30,880 --> 00:31:34,559
ratings but these ratings are quite

00:31:32,720 --> 00:31:35,760
discriminatory and arbitrary i don't

00:31:34,559 --> 00:31:38,480
think anybody in

00:31:35,760 --> 00:31:39,120
in real life beyond uber driving would

00:31:38,480 --> 00:31:41,279
want to be

00:31:39,120 --> 00:31:43,200
performance managed this way but one

00:31:41,279 --> 00:31:45,120
star is a terrible service

00:31:43,200 --> 00:31:47,039
this is verbatim i'm giving you the

00:31:45,120 --> 00:31:49,279
explanation two star

00:31:47,039 --> 00:31:50,880
is a bad trip and i don't think that's a

00:31:49,279 --> 00:31:53,039
narcotics euphemism

00:31:50,880 --> 00:31:54,640
three stars is pretty disappointing four

00:31:53,039 --> 00:31:55,279
stars is okay but there was definitely a

00:31:54,640 --> 00:31:57,039
problem

00:31:55,279 --> 00:31:58,799
five stars it's excellent you get the

00:31:57,039 --> 00:32:00,320
you get the drift here there's only one

00:31:58,799 --> 00:32:03,519
you know it's a binary system

00:32:00,320 --> 00:32:05,279
good or bad and a 4.4 average rating on

00:32:03,519 --> 00:32:08,000
your past 500 trips

00:32:05,279 --> 00:32:08,559
you are fired or disaffiliated as i

00:32:08,000 --> 00:32:12,080
might say

00:32:08,559 --> 00:32:14,799
next next slide please and so

00:32:12,080 --> 00:32:15,360
we went to court and we won and lord

00:32:14,799 --> 00:32:18,159
leggett

00:32:15,360 --> 00:32:19,840
and the uh the unanimous ruling of the

00:32:18,159 --> 00:32:21,200
supreme court in the united kingdom

00:32:19,840 --> 00:32:23,360
recognized they saw through these

00:32:21,200 --> 00:32:25,600
contracts they saw through the business

00:32:23,360 --> 00:32:27,440
processes that were encoded an algorithm

00:32:25,600 --> 00:32:29,440
and they recognized that uber drivers

00:32:27,440 --> 00:32:31,120
truly have little or no ability to

00:32:29,440 --> 00:32:33,440
improve their economic position

00:32:31,120 --> 00:32:35,039
through skill our profession

00:32:33,440 --> 00:32:37,200
professional entrepreneurial skill

00:32:35,039 --> 00:32:38,960
and that the only way that we could earn

00:32:37,200 --> 00:32:41,600
more money was to work

00:32:38,960 --> 00:32:43,360
more hours while constantly meeting

00:32:41,600 --> 00:32:44,880
uber's measures of performance

00:32:43,360 --> 00:32:47,760
and this simply isn't fair either it's

00:32:44,880 --> 00:32:50,080
an open market and we are free

00:32:47,760 --> 00:32:51,440
um or it is a closed market where

00:32:50,080 --> 00:32:52,080
performance managed and we have an

00:32:51,440 --> 00:32:53,919
employment

00:32:52,080 --> 00:32:57,039
relationship and the rights that go with

00:32:53,919 --> 00:32:58,720
it next slide please

00:32:57,039 --> 00:33:00,399
and so while i'm we could pause here for

00:32:58,720 --> 00:33:02,240
five minutes and and gaze at this i

00:33:00,399 --> 00:33:03,600
often do myself this is our victory pose

00:33:02,240 --> 00:33:07,279
outside the supreme court

00:33:03,600 --> 00:33:07,279
uh last month next slide please

00:33:07,679 --> 00:33:11,120
and so how is uber reacted to this well

00:33:10,240 --> 00:33:12,720
actually

00:33:11,120 --> 00:33:15,120
uber has doubled down what uber is

00:33:12,720 --> 00:33:18,559
trying to say now is well

00:33:15,120 --> 00:33:21,840
this judgment relates to how uber was

00:33:18,559 --> 00:33:24,000
then not how uber is now so uber wants

00:33:21,840 --> 00:33:26,399
to make cosmetic changes to the business

00:33:24,000 --> 00:33:27,919
model to the way the app works maybe let

00:33:26,399 --> 00:33:30,799
you see the destination

00:33:27,919 --> 00:33:32,320
uh where you couldn't see it before and

00:33:30,799 --> 00:33:33,519
then say well look there's plausible

00:33:32,320 --> 00:33:35,120
deniability here

00:33:33,519 --> 00:33:36,640
you might have been free then but you're

00:33:35,120 --> 00:33:38,159
definitely free now

00:33:36,640 --> 00:33:40,080
and what we're seeing is a doubling down

00:33:38,159 --> 00:33:41,120
on algorithmic control so we're less

00:33:40,080 --> 00:33:43,519
communication

00:33:41,120 --> 00:33:44,320
more algorithmic control more um

00:33:43,519 --> 00:33:45,840
surveillance

00:33:44,320 --> 00:33:48,000
uh there's a bit of a surveillance arms

00:33:45,840 --> 00:33:51,200
race in the gig economy at the moment

00:33:48,000 --> 00:33:51,519
in using um geolocation identity checks

00:33:51,200 --> 00:33:54,240
and

00:33:51,519 --> 00:33:55,760
and uh and facial recognition but also

00:33:54,240 --> 00:33:57,120
algorithmic performance management and

00:33:55,760 --> 00:33:58,799
the way this works is that

00:33:57,120 --> 00:34:00,399
the only thing worse than being fired is

00:33:58,799 --> 00:34:01,600
not being fired you find that you're

00:34:00,399 --> 00:34:03,039
getting less work

00:34:01,600 --> 00:34:04,640
and you're you know you have this

00:34:03,039 --> 00:34:06,320
boiling frog syndrome where you can't

00:34:04,640 --> 00:34:07,679
make it you're not earning enough and

00:34:06,320 --> 00:34:08,800
you fall away eventually that's the

00:34:07,679 --> 00:34:12,079
brutal reality

00:34:08,800 --> 00:34:13,359
and next slide please and so worker info

00:34:12,079 --> 00:34:14,960
exchange was set up to tackle that

00:34:13,359 --> 00:34:16,399
problem we're taking advantage of data

00:34:14,960 --> 00:34:18,000
access rights

00:34:16,399 --> 00:34:20,079
in the european union on the under the

00:34:18,000 --> 00:34:21,599
eu gdpr the general data protection

00:34:20,079 --> 00:34:21,839
regulation it gives us important rights

00:34:21,599 --> 00:34:24,159
of

00:34:21,839 --> 00:34:25,919
access to our data not only that but

00:34:24,159 --> 00:34:26,800
also we're entitled to algorithmic

00:34:25,919 --> 00:34:28,639
transparency

00:34:26,800 --> 00:34:30,240
so that we understand how we have been

00:34:28,639 --> 00:34:32,320
managed by a platform like

00:34:30,240 --> 00:34:34,560
uber and then we also have rights to

00:34:32,320 --> 00:34:35,359
challenge automated decision making that

00:34:34,560 --> 00:34:36,879
haven't had

00:34:35,359 --> 00:34:38,560
proper human intervention or you've been

00:34:36,879 --> 00:34:40,079
denied an appeal and this is important

00:34:38,560 --> 00:34:40,960
for decisions relating to work

00:34:40,079 --> 00:34:42,240
allocation

00:34:40,960 --> 00:34:44,399
deciding whether you're going to work or

00:34:42,240 --> 00:34:46,320
not but also performance management and

00:34:44,399 --> 00:34:49,919
ultimately dismissals as well

00:34:46,320 --> 00:34:51,760
next slide please so what does the

00:34:49,919 --> 00:34:52,240
battleground for this data access look

00:34:51,760 --> 00:34:54,000
like

00:34:52,240 --> 00:34:55,440
well the green box is the data we

00:34:54,000 --> 00:34:57,119
entered into the machine ourselves i

00:34:55,440 --> 00:34:58,880
mean this can be no dispute that

00:34:57,119 --> 00:35:00,160
that's our data we gave it to you can we

00:34:58,880 --> 00:35:02,800
have it back please

00:35:00,160 --> 00:35:04,560
the the amber box is the data that the

00:35:02,800 --> 00:35:05,680
machine observes about us at work the

00:35:04,560 --> 00:35:08,800
geolocation

00:35:05,680 --> 00:35:10,320
um information the gps traces uh the

00:35:08,800 --> 00:35:12,400
customer ratings the breaking

00:35:10,320 --> 00:35:14,240
acceleration telematics information

00:35:12,400 --> 00:35:16,400
we know uber has all that they don't

00:35:14,240 --> 00:35:18,160
like giving it to us and they don't like

00:35:16,400 --> 00:35:19,680
giving it to us in an orderly way and we

00:35:18,160 --> 00:35:21,760
have to fight for that

00:35:19,680 --> 00:35:23,440
and but but it's there and then finally

00:35:21,760 --> 00:35:24,240
the real battleground is the inference

00:35:23,440 --> 00:35:26,079
data

00:35:24,240 --> 00:35:27,920
that is the decision that the machine is

00:35:26,079 --> 00:35:28,640
making about us having observed us at

00:35:27,920 --> 00:35:30,480
work

00:35:28,640 --> 00:35:32,160
our profiling our classification

00:35:30,480 --> 00:35:34,240
according to performance i won't get

00:35:32,160 --> 00:35:36,720
into it but we know it exists

00:35:34,240 --> 00:35:37,440
we have clear evidence of that type of

00:35:36,720 --> 00:35:40,000
profiling

00:35:37,440 --> 00:35:40,880
of workers and that that type of profile

00:35:40,000 --> 00:35:42,320
is put into the

00:35:40,880 --> 00:35:44,240
the algorithmic decision-making

00:35:42,320 --> 00:35:45,440
automated decision making around things

00:35:44,240 --> 00:35:47,760
like work allocation

00:35:45,440 --> 00:35:49,440
so it's really really important that we

00:35:47,760 --> 00:35:50,480
have full transparency of that and we do

00:35:49,440 --> 00:35:54,079
not yet

00:35:50,480 --> 00:35:55,839
next slide please and so the mission of

00:35:54,079 --> 00:35:56,480
what we're trying to do is access the

00:35:55,839 --> 00:35:58,400
data

00:35:56,480 --> 00:36:00,079
algorithmic transparency those go hand

00:35:58,400 --> 00:36:02,240
in glove without

00:36:00,079 --> 00:36:03,680
just having your personal data alone is

00:36:02,240 --> 00:36:05,119
not enough you need to understand how

00:36:03,680 --> 00:36:07,599
that was being processed

00:36:05,119 --> 00:36:09,359
through the body that is uber it's a bit

00:36:07,599 --> 00:36:10,720
like you know you get that injection of

00:36:09,359 --> 00:36:13,680
dye into your arm and you go

00:36:10,720 --> 00:36:15,440
to have an mri so so you know you you'll

00:36:13,680 --> 00:36:17,200
just have your blood you see how it's

00:36:15,440 --> 00:36:18,960
performing through your major organs in

00:36:17,200 --> 00:36:21,200
your body it's the same way with the

00:36:18,960 --> 00:36:23,599
platform we want to see how that data

00:36:21,200 --> 00:36:25,200
our personal data was processed through

00:36:23,599 --> 00:36:26,800
uber systems and that would give us a

00:36:25,200 --> 00:36:27,520
really clear indication of how we were

00:36:26,800 --> 00:36:29,760
managed

00:36:27,520 --> 00:36:31,520
how decisions were made relating to us

00:36:29,760 --> 00:36:32,640
and then when we identify these unfair

00:36:31,520 --> 00:36:35,359
decisions we can

00:36:32,640 --> 00:36:37,119
challenge them uh illegally next

00:36:35,359 --> 00:36:38,720
question please

00:36:37,119 --> 00:36:40,720
legally and collectively i might add

00:36:38,720 --> 00:36:42,000
this is about building collective power

00:36:40,720 --> 00:36:43,520
because what we

00:36:42,000 --> 00:36:45,839
even though we've had a great supreme

00:36:43,520 --> 00:36:46,960
court victory the only long-term

00:36:45,839 --> 00:36:48,160
sustainable solution

00:36:46,960 --> 00:36:50,079
is collective power and collective

00:36:48,160 --> 00:36:51,119
intelligence and that means a data trust

00:36:50,079 --> 00:36:53,040
for the masses

00:36:51,119 --> 00:36:55,119
but we are taking legal challenges

00:36:53,040 --> 00:36:57,040
against uber enola in amsterdam where

00:36:55,119 --> 00:36:58,960
the data has been held

00:36:57,040 --> 00:37:00,800
on grounds of access we haven't been

00:36:58,960 --> 00:37:02,480
given proper access to our data

00:37:00,800 --> 00:37:04,000
we have not been giving algor given

00:37:02,480 --> 00:37:07,200
algorithmic transparency

00:37:04,000 --> 00:37:09,119
and we have not um we've also brought

00:37:07,200 --> 00:37:10,880
a bunch of cases where drivers have been

00:37:09,119 --> 00:37:12,720
deactivated unfairly and we say that

00:37:10,880 --> 00:37:13,200
they've been deactivated in an automated

00:37:12,720 --> 00:37:15,040
way

00:37:13,200 --> 00:37:16,640
without proper human intervention or

00:37:15,040 --> 00:37:17,599
without any right of appeal and we say

00:37:16,640 --> 00:37:19,599
that's illegal

00:37:17,599 --> 00:37:21,520
those cases are before the courts and

00:37:19,599 --> 00:37:25,119
we're waiting for decisions shortly

00:37:21,520 --> 00:37:25,680
next slide please so this is my final

00:37:25,119 --> 00:37:27,359
slide

00:37:25,680 --> 00:37:29,119
and what i wanted to do was return to

00:37:27,359 --> 00:37:31,200
the topic of purpose

00:37:29,119 --> 00:37:33,200
you know i i don't mind telling you i

00:37:31,200 --> 00:37:35,760
shed tears when i saw that supreme court

00:37:33,200 --> 00:37:36,560
ruling because it was a unanimous ruling

00:37:35,760 --> 00:37:38,240
and

00:37:36,560 --> 00:37:39,839
which was unexpected these were black

00:37:38,240 --> 00:37:41,359
letter lawyers they were not known to be

00:37:39,839 --> 00:37:42,079
left-wing lawyers there were commercial

00:37:41,359 --> 00:37:44,960
lawyers that

00:37:42,079 --> 00:37:46,160
went on to become judges bankrupt back

00:37:44,960 --> 00:37:47,680
with backgrounds and banking and

00:37:46,160 --> 00:37:49,440
insurance and such

00:37:47,680 --> 00:37:51,599
and i wasn't sure how they would react

00:37:49,440 --> 00:37:52,480
to this topic but they were pretty

00:37:51,599 --> 00:37:55,440
brutal on

00:37:52,480 --> 00:37:57,520
uber and what they said was so simple

00:37:55,440 --> 00:37:58,880
after all the talk after all the talk

00:37:57,520 --> 00:38:00,560
about contracts

00:37:58,880 --> 00:38:02,640
after all the talk about how the app

00:38:00,560 --> 00:38:05,200
works they simply said

00:38:02,640 --> 00:38:06,560
that the purpose of the law was to

00:38:05,200 --> 00:38:07,920
protect workers

00:38:06,560 --> 00:38:10,000
who are precarious in precarious

00:38:07,920 --> 00:38:10,640
circumstances to protect vulnerable

00:38:10,000 --> 00:38:12,880
workers

00:38:10,640 --> 00:38:13,760
that is the purpose of the law and so

00:38:12,880 --> 00:38:16,560
when uber

00:38:13,760 --> 00:38:18,640
constructs fake contracts like this for

00:38:16,560 --> 00:38:19,200
the purpose of cheating us out of those

00:38:18,640 --> 00:38:21,359
rights

00:38:19,200 --> 00:38:22,880
those right those contracts cannot stand

00:38:21,359 --> 00:38:25,119
in law and they have to be discarded and

00:38:22,880 --> 00:38:26,880
disrespected and discarded

00:38:25,119 --> 00:38:29,040
and it's the same thing with algorithms

00:38:26,880 --> 00:38:30,720
if you build algorithms as uber has done

00:38:29,040 --> 00:38:32,320
if you build these processes that are

00:38:30,720 --> 00:38:34,320
automated and hidden

00:38:32,320 --> 00:38:36,000
for the purpose of concealing the true

00:38:34,320 --> 00:38:37,280
relationship that there is an employment

00:38:36,000 --> 00:38:39,200
relationship here

00:38:37,280 --> 00:38:40,320
those cannot stand either they have to

00:38:39,200 --> 00:38:42,480
be exposed

00:38:40,320 --> 00:38:44,240
and we have to evidence them as

00:38:42,480 --> 00:38:46,400
management in action and therefore

00:38:44,240 --> 00:38:48,960
there's an employment relationship

00:38:46,400 --> 00:38:49,839
so this idea of purpose is really really

00:38:48,960 --> 00:38:52,320
important

00:38:49,839 --> 00:38:53,839
in your values in the function of the

00:38:52,320 --> 00:38:56,240
software you're building

00:38:53,839 --> 00:38:57,760
in the design decisions that you make

00:38:56,240 --> 00:39:00,640
and most importantly

00:38:57,760 --> 00:39:01,680
in your values so i just invite you also

00:39:00,640 --> 00:39:03,599
to think about

00:39:01,680 --> 00:39:06,000
in the work that you do every day what

00:39:03,599 --> 00:39:07,839
is your purpose and how can you help

00:39:06,000 --> 00:39:09,440
people like me in future not have to

00:39:07,839 --> 00:39:10,079
spend six years in court against the

00:39:09,440 --> 00:39:14,000
company

00:39:10,079 --> 00:39:16,560
like uber thank you very much

00:39:14,000 --> 00:39:18,560
you so much james i really uh i love

00:39:16,560 --> 00:39:22,079
that talk and the work you're doing

00:39:18,560 --> 00:39:24,160
and just the ending with that idea

00:39:22,079 --> 00:39:25,839
of purpose and the purpose of like labor

00:39:24,160 --> 00:39:28,240
law or human rights law is

00:39:25,839 --> 00:39:29,040
to protect us so we take those rights

00:39:28,240 --> 00:39:31,680
for granted

00:39:29,040 --> 00:39:32,320
in many societies and then this

00:39:31,680 --> 00:39:34,560
connection

00:39:32,320 --> 00:39:35,839
to the idea of a data trust for gig

00:39:34,560 --> 00:39:37,680
workers data like

00:39:35,839 --> 00:39:39,680
unless we actually can use our data

00:39:37,680 --> 00:39:41,680
rights in a practical way we can't

00:39:39,680 --> 00:39:44,240
actually take advantage of the the

00:39:41,680 --> 00:39:46,160
rights that we we take for granted so

00:39:44,240 --> 00:39:48,240
really that connection is very clear

00:39:46,160 --> 00:39:49,920
here and it's also

00:39:48,240 --> 00:39:51,599
what the data futures lab that mozilla

00:39:49,920 --> 00:39:52,400
illuminate and siegel and others are

00:39:51,599 --> 00:39:55,119
setting up

00:39:52,400 --> 00:39:56,000
is and mozilla's work on trustworthy ai

00:39:55,119 --> 00:39:58,880
really need to focus

00:39:56,000 --> 00:40:00,400
on that connection between real human

00:39:58,880 --> 00:40:02,160
needs and life and rights

00:40:00,400 --> 00:40:05,440
and and how we give people power over

00:40:02,160 --> 00:40:07,280
the data to to tap into that

00:40:05,440 --> 00:40:10,000
i i honestly i couldn't agree more i

00:40:07,280 --> 00:40:11,920
mean thinking about

00:40:10,000 --> 00:40:13,680
data and power and the fact that like

00:40:11,920 --> 00:40:16,400
everyone who uses

00:40:13,680 --> 00:40:17,680
um the internet who uses technology like

00:40:16,400 --> 00:40:20,160
should have

00:40:17,680 --> 00:40:22,480
a say and a role to play around like

00:40:20,160 --> 00:40:24,160
shifting the power dynamics around data

00:40:22,480 --> 00:40:25,599
uh such an important point and love

00:40:24,160 --> 00:40:27,440
james talk for that

00:40:25,599 --> 00:40:28,960
um and that gets me thinking actually

00:40:27,440 --> 00:40:31,440
going back to the example that i shared

00:40:28,960 --> 00:40:34,400
earlier mark about china using ai

00:40:31,440 --> 00:40:34,720
to enhance the censorship efforts uh so

00:40:34,400 --> 00:40:37,200
when

00:40:34,720 --> 00:40:39,200
we're thinking about like the using ai

00:40:37,200 --> 00:40:40,400
to shift the power dynamic one of the

00:40:39,200 --> 00:40:43,119
initiatives that

00:40:40,400 --> 00:40:44,960
uh is supported by otf the open

00:40:43,119 --> 00:40:47,280
technology fund where i currently work

00:40:44,960 --> 00:40:49,440
uh is called the geneva project and it's

00:40:47,280 --> 00:40:50,319
led by researchers at the university of

00:40:49,440 --> 00:40:52,000
maryland

00:40:50,319 --> 00:40:53,839
and what they're trying to do is is that

00:40:52,000 --> 00:40:55,599
they're trying to shift this balance

00:40:53,839 --> 00:40:58,960
this balance of power

00:40:55,599 --> 00:41:01,119
in the in the case of the censorship uh

00:40:58,960 --> 00:41:02,400
race by using ai as well so they

00:41:01,119 --> 00:41:04,640
developed a tool

00:41:02,400 --> 00:41:06,079
that uses ai and that uses machine

00:41:04,640 --> 00:41:09,040
learning to automatically

00:41:06,079 --> 00:41:10,000
learn about the new blocking techniques

00:41:09,040 --> 00:41:12,240
and analyze

00:41:10,000 --> 00:41:13,839
how to circumvent censorship by the

00:41:12,240 --> 00:41:16,640
great firewall of china

00:41:13,839 --> 00:41:17,280
um just imagine what that could mean for

00:41:16,640 --> 00:41:19,280
residents

00:41:17,280 --> 00:41:21,280
of china who want to access a free and

00:41:19,280 --> 00:41:21,920
open internet to then have access to

00:41:21,280 --> 00:41:23,520
this data

00:41:21,920 --> 00:41:25,359
and to have access to this research and

00:41:23,520 --> 00:41:27,440
understand you know how how to

00:41:25,359 --> 00:41:28,880
how to be able to access an open and

00:41:27,440 --> 00:41:31,920
free internet that way

00:41:28,880 --> 00:41:34,480
um and the idea of like you know the

00:41:31,920 --> 00:41:35,680
what this means for ends what this means

00:41:34,480 --> 00:41:38,720
for end users

00:41:35,680 --> 00:41:39,920
uh and how important it is to shift the

00:41:38,720 --> 00:41:41,839
power dynamic

00:41:39,920 --> 00:41:44,000
i think connects very well to our next

00:41:41,839 --> 00:41:45,280
speaker uh and what she's about to

00:41:44,000 --> 00:41:47,200
talk about so would you like to

00:41:45,280 --> 00:41:50,079
introduce her mark yeah

00:41:47,200 --> 00:41:51,599
absolutely i think it does i'm happy to

00:41:50,079 --> 00:41:54,000
introduce my colleague

00:41:51,599 --> 00:41:55,920
randy garcink one of our senior

00:41:54,000 --> 00:41:57,920
campaigners here at mozilla

00:41:55,920 --> 00:41:59,599
brandy is a part of a growing collection

00:41:57,920 --> 00:42:02,400
of people who are focused on

00:41:59,599 --> 00:42:04,400
the idea that data donations people

00:42:02,400 --> 00:42:05,040
sharing their own data with researchers

00:42:04,400 --> 00:42:07,359
with

00:42:05,040 --> 00:42:08,800
uh you know non-profits with people who

00:42:07,359 --> 00:42:11,520
are trying to

00:42:08,800 --> 00:42:12,319
tackle these issues that citizens can

00:42:11,520 --> 00:42:15,200
get involved

00:42:12,319 --> 00:42:17,040
in shifting power around data so brandy

00:42:15,200 --> 00:42:18,880
are you here you want to join us on this

00:42:17,040 --> 00:42:22,720
virtual stage there you are

00:42:18,880 --> 00:42:25,599
welcome thank you so much mark

00:42:22,720 --> 00:42:27,359
hi i'm brandi i want to start off today

00:42:25,599 --> 00:42:30,319
by telling you a few stories

00:42:27,359 --> 00:42:32,000
about the impact of data and power on

00:42:30,319 --> 00:42:34,480
human lives

00:42:32,000 --> 00:42:35,119
these stories aren't mine but i can see

00:42:34,480 --> 00:42:38,319
myself

00:42:35,119 --> 00:42:40,319
and my own story in each of them

00:42:38,319 --> 00:42:42,480
growing up i wanted more than anything

00:42:40,319 --> 00:42:44,319
else to be a dancer

00:42:42,480 --> 00:42:45,760
that didn't work out because i'm a

00:42:44,319 --> 00:42:48,079
terrible dancer

00:42:45,760 --> 00:42:49,440
but i remember all of the time that i

00:42:48,079 --> 00:42:51,040
spent at the studio

00:42:49,440 --> 00:42:53,040
and all of the friends that i made along

00:42:51,040 --> 00:42:54,960
the way and i think back on that time in

00:42:53,040 --> 00:42:57,200
my life really fondly

00:42:54,960 --> 00:42:59,760
so it hit extra hard when i read this

00:42:57,200 --> 00:43:03,359
one mother story about her little girl

00:42:59,760 --> 00:43:04,480
next slide please she wrote my 10 year

00:43:03,359 --> 00:43:06,560
old sweet daughter

00:43:04,480 --> 00:43:07,839
innocently searched youtube for tap

00:43:06,560 --> 00:43:10,079
dance videos

00:43:07,839 --> 00:43:12,960
and now is in this spiral of horrible

00:43:10,079 --> 00:43:15,280
extreme dance and contortionist videos

00:43:12,960 --> 00:43:17,680
that give her unsafe body harming and

00:43:15,280 --> 00:43:19,920
body image damaging advice

00:43:17,680 --> 00:43:21,599
i've tried to go in and manually delete

00:43:19,920 --> 00:43:24,720
all recommended videos

00:43:21,599 --> 00:43:26,640
put in parental controls everything but

00:43:24,720 --> 00:43:28,960
she's finding ways to log on using

00:43:26,640 --> 00:43:30,960
browsers and school computers

00:43:28,960 --> 00:43:32,640
these terrible videos just keep being

00:43:30,960 --> 00:43:34,960
recommended to her

00:43:32,640 --> 00:43:37,119
i heard her downstairs saying work to

00:43:34,960 --> 00:43:39,440
eat work to drink

00:43:37,119 --> 00:43:41,359
i don't know how i can undo the damage

00:43:39,440 --> 00:43:43,760
that's been done to her impressionable

00:43:41,359 --> 00:43:43,760
mind

00:43:44,480 --> 00:43:48,400
instead of becoming a dancer i went on

00:43:46,720 --> 00:43:50,240
to study political science at a

00:43:48,400 --> 00:43:50,960
university that was affectionately

00:43:50,240 --> 00:43:53,680
referred to

00:43:50,960 --> 00:43:54,000
as the most liberal square footage in

00:43:53,680 --> 00:43:57,119
south

00:43:54,000 --> 00:43:57,680
carolina i was lucky that for me and my

00:43:57,119 --> 00:44:00,000
friends

00:43:57,680 --> 00:44:01,520
it was a really welcoming and inclusive

00:44:00,000 --> 00:44:03,119
and safe space

00:44:01,520 --> 00:44:04,720
to learn more about ourselves and our

00:44:03,119 --> 00:44:06,560
identities

00:44:04,720 --> 00:44:08,000
i heard a story the other day that

00:44:06,560 --> 00:44:10,880
reminded me how our online

00:44:08,000 --> 00:44:13,440
spaces can be anything but that next

00:44:10,880 --> 00:44:13,440
slide please

00:44:13,520 --> 00:44:17,359
in coming out to myself and close

00:44:15,680 --> 00:44:20,079
friends as transgender

00:44:17,359 --> 00:44:22,079
my biggest regret was turning to youtube

00:44:20,079 --> 00:44:25,359
to hear the stories of other queer

00:44:22,079 --> 00:44:26,720
and trans people simply typing in the

00:44:25,359 --> 00:44:28,960
word transgender

00:44:26,720 --> 00:44:31,040
brought up countless videos that were

00:44:28,960 --> 00:44:33,359
essentially describing my struggle

00:44:31,040 --> 00:44:35,280
as a mental illness and as something

00:44:33,359 --> 00:44:37,760
that shouldn't exist

00:44:35,280 --> 00:44:41,280
youtube reminded me why i hid in the

00:44:37,760 --> 00:44:41,280
closet for so many years

00:44:42,319 --> 00:44:45,440
so after i graduated i was really fresh

00:44:45,119 --> 00:44:47,359
with

00:44:45,440 --> 00:44:49,359
idealism about all of the ways that

00:44:47,359 --> 00:44:51,680
through my work i could change the world

00:44:49,359 --> 00:44:54,160
and make the world a better place

00:44:51,680 --> 00:44:56,640
i had a really specific world view and

00:44:54,160 --> 00:44:57,040
my politics had definitely been shaped

00:44:56,640 --> 00:44:59,839
by

00:44:57,040 --> 00:45:01,680
the past few years spent at university

00:44:59,839 --> 00:45:04,560
and the process of really learning how

00:45:01,680 --> 00:45:06,000
to think critically it became

00:45:04,560 --> 00:45:08,160
more and more difficult for me to

00:45:06,000 --> 00:45:10,000
connect with members of my family who at

00:45:08,160 --> 00:45:13,280
the time were embracing like

00:45:10,000 --> 00:45:15,359
alternative facts and i didn't always

00:45:13,280 --> 00:45:17,680
respond the way that i wish i would have

00:45:15,359 --> 00:45:19,920
like with empathy and with understanding

00:45:17,680 --> 00:45:21,280
which is why i respect and value so much

00:45:19,920 --> 00:45:24,160
this story

00:45:21,280 --> 00:45:24,160
next slide please

00:45:24,400 --> 00:45:28,800
my stepfather is an 80 year old retired

00:45:26,960 --> 00:45:31,440
scientist from ecuador

00:45:28,800 --> 00:45:32,480
he was always curious about all sources

00:45:31,440 --> 00:45:34,640
of wisdom

00:45:32,480 --> 00:45:36,400
theories and knowledge and is very

00:45:34,640 --> 00:45:38,480
literate and educated

00:45:36,400 --> 00:45:40,720
but for a few years he has become quite

00:45:38,480 --> 00:45:41,920
lonely and spends a large amount of time

00:45:40,720 --> 00:45:44,960
on the internet

00:45:41,920 --> 00:45:45,440
and on youtube in particular his curious

00:45:44,960 --> 00:45:47,440
mind

00:45:45,440 --> 00:45:49,760
quickly brought him towards multiple

00:45:47,440 --> 00:45:52,640
conspiracy theories illuminati

00:45:49,760 --> 00:45:54,720
other alien-based stories despite the

00:45:52,640 --> 00:45:56,800
fact that we tried to erase his youtube

00:45:54,720 --> 00:45:58,560
history and clean his browser

00:45:56,800 --> 00:46:00,640
his recommendations are completely

00:45:58,560 --> 00:46:03,520
filled with these kinds of videos

00:46:00,640 --> 00:46:05,520
he sometimes falls asleep while watching

00:46:03,520 --> 00:46:07,440
his days are filled by such content

00:46:05,520 --> 00:46:08,319
which has quite strongly affected his

00:46:07,440 --> 00:46:09,760
world view

00:46:08,319 --> 00:46:11,440
towards a much grimmer and more

00:46:09,760 --> 00:46:14,560
pessimistic turn

00:46:11,440 --> 00:46:16,400
it seems impossible for us his family

00:46:14,560 --> 00:46:20,160
to fight against the recommendation

00:46:16,400 --> 00:46:23,680
algorithms that steer his life

00:46:20,160 --> 00:46:24,800
in order to put his story in context as

00:46:23,680 --> 00:46:26,800
well as the others

00:46:24,800 --> 00:46:28,160
it's really important that we all better

00:46:26,800 --> 00:46:30,480
understand the platform

00:46:28,160 --> 00:46:32,880
that all of this is happening on next

00:46:30,480 --> 00:46:32,880
slide

00:46:33,280 --> 00:46:36,400
youtube is one of the most powerful

00:46:35,440 --> 00:46:39,760
recommendation

00:46:36,400 --> 00:46:42,400
engines on our planet it's the world's

00:46:39,760 --> 00:46:44,720
second most visited website

00:46:42,400 --> 00:46:47,520
adults on average spend nearly five

00:46:44,720 --> 00:46:50,599
hours per week on the platform

00:46:47,520 --> 00:46:52,400
and youtube's recommendation ai drives

00:46:50,599 --> 00:46:54,880
700 million

00:46:52,400 --> 00:46:56,480
hours of watch time on the platform

00:46:54,880 --> 00:47:00,160
every day

00:46:56,480 --> 00:47:01,839
next slide the 10 year old girl in the

00:47:00,160 --> 00:47:04,240
story that i shared

00:47:01,839 --> 00:47:06,079
she didn't actively seek out videos

00:47:04,240 --> 00:47:09,119
about eating disorders

00:47:06,079 --> 00:47:12,560
she was steered towards them

00:47:09,119 --> 00:47:14,000
why how many other 10 year old girls

00:47:12,560 --> 00:47:17,280
were suggested to watch

00:47:14,000 --> 00:47:19,040
similar videos how many trans and queer

00:47:17,280 --> 00:47:21,040
people were subjected to hate

00:47:19,040 --> 00:47:23,760
and abuse on the platform when they were

00:47:21,040 --> 00:47:28,240
looking for community

00:47:23,760 --> 00:47:28,880
slide a billion hours of youtube are

00:47:28,240 --> 00:47:31,599
watched

00:47:28,880 --> 00:47:32,160
every single day but it's impossible to

00:47:31,599 --> 00:47:34,559
know

00:47:32,160 --> 00:47:36,319
about the impact that this platform and

00:47:34,559 --> 00:47:38,800
especially the recommendation engine

00:47:36,319 --> 00:47:41,040
is having on our societies because

00:47:38,800 --> 00:47:43,760
youtube makes it really really hard

00:47:41,040 --> 00:47:45,520
to study their platform without

00:47:43,760 --> 00:47:46,880
information about what youtube is

00:47:45,520 --> 00:47:49,119
recommending to us

00:47:46,880 --> 00:47:50,000
or to our parents or to our children and

00:47:49,119 --> 00:47:52,079
why

00:47:50,000 --> 00:47:53,119
we have no way to scrutinize those

00:47:52,079 --> 00:47:56,319
decisions

00:47:53,119 --> 00:47:58,800
or contextualize the stories

00:47:56,319 --> 00:48:01,119
we are made powerless without the

00:47:58,800 --> 00:48:04,079
information that we need to understand

00:48:01,119 --> 00:48:06,880
if and how this system might be working

00:48:04,079 --> 00:48:10,240
against our best interests

00:48:06,880 --> 00:48:11,359
next slide but it doesn't have to be

00:48:10,240 --> 00:48:14,079
that way

00:48:11,359 --> 00:48:16,480
because you and me and mark and sarah

00:48:14,079 --> 00:48:17,440
and james and kathleen we all have a lot

00:48:16,480 --> 00:48:20,640
more power

00:48:17,440 --> 00:48:22,240
than we might think if you're a person

00:48:20,640 --> 00:48:24,079
who uses social media

00:48:22,240 --> 00:48:25,920
or more broadly a person who uses the

00:48:24,079 --> 00:48:26,880
web and since this is a virtual

00:48:25,920 --> 00:48:29,680
conference

00:48:26,880 --> 00:48:30,400
that means you you hold a tremendously

00:48:29,680 --> 00:48:32,559
valuable

00:48:30,400 --> 00:48:34,079
piece of the puzzle which is what you

00:48:32,559 --> 00:48:37,040
are seeing online

00:48:34,079 --> 00:48:39,040
your own personalized social media feed

00:48:37,040 --> 00:48:41,839
your recommendations

00:48:39,040 --> 00:48:43,760
your stories you can use this power to

00:48:41,839 --> 00:48:47,599
drive change

00:48:43,760 --> 00:48:49,839
next slide so in 2019

00:48:47,599 --> 00:48:51,440
after years of seeing news reports like

00:48:49,839 --> 00:48:53,520
these about harmful youtube

00:48:51,440 --> 00:48:56,000
recommendations that were driving people

00:48:53,520 --> 00:48:58,240
to the internet's darkest corners we

00:48:56,000 --> 00:48:59,680
decided that the only way to really

00:48:58,240 --> 00:49:02,000
understand this problem

00:48:59,680 --> 00:49:03,599
at scale was for youtube to make their

00:49:02,000 --> 00:49:06,480
recommendation algorithm

00:49:03,599 --> 00:49:07,680
more transparent i sent an email to the

00:49:06,480 --> 00:49:09,520
mozilla community

00:49:07,680 --> 00:49:11,599
asking people whether they had a story

00:49:09,520 --> 00:49:15,280
to tell us about getting

00:49:11,599 --> 00:49:18,720
served harmful youtube recommendations

00:49:15,280 --> 00:49:20,720
next slide before the week was up

00:49:18,720 --> 00:49:22,720
thousands of people had come forward to

00:49:20,720 --> 00:49:24,640
tell us their stories

00:49:22,720 --> 00:49:27,359
three of those were people whose stories

00:49:24,640 --> 00:49:29,280
i told you a few minutes ago

00:49:27,359 --> 00:49:31,440
the mother whose ten-year-old daughter

00:49:29,280 --> 00:49:33,200
began starving herself

00:49:31,440 --> 00:49:35,440
the trans person who was made to feel

00:49:33,200 --> 00:49:37,280
ashamed of who they are

00:49:35,440 --> 00:49:39,680
the person whose final years with their

00:49:37,280 --> 00:49:42,240
stepfather were derailed by paranoia and

00:49:39,680 --> 00:49:44,880
by conspiracy theories

00:49:42,240 --> 00:49:46,480
these stories were so powerful i thought

00:49:44,880 --> 00:49:48,720
if we can tell these to youtube

00:49:46,480 --> 00:49:50,800
and we can show them the real harm that

00:49:48,720 --> 00:49:52,559
this is having in people's lives

00:49:50,800 --> 00:49:54,400
then we can convince them that they have

00:49:52,559 --> 00:49:55,839
to act

00:49:54,400 --> 00:49:58,319
so we brought them into a meeting with

00:49:55,839 --> 00:50:00,640
youtube we read them out loud

00:49:58,319 --> 00:50:01,680
and we tried to urge youtube to release

00:50:00,640 --> 00:50:03,440
information

00:50:01,680 --> 00:50:05,920
that would allow the public to dig

00:50:03,440 --> 00:50:07,760
deeper on some of these examples

00:50:05,920 --> 00:50:10,319
to understand where things were going

00:50:07,760 --> 00:50:13,599
wrong and what could be done about it

00:50:10,319 --> 00:50:16,160
next slide but months passed

00:50:13,599 --> 00:50:17,839
with no action from youtube they weren't

00:50:16,160 --> 00:50:19,920
releasing any of the data that we had

00:50:17,839 --> 00:50:21,520
been asking them to

00:50:19,920 --> 00:50:23,359
but those stories stuck with me

00:50:21,520 --> 00:50:25,200
throughout all of those months

00:50:23,359 --> 00:50:27,200
our campaign had raised awareness of

00:50:25,200 --> 00:50:29,599
this problem but as each day

00:50:27,200 --> 00:50:31,680
passed another billion hours were spent

00:50:29,599 --> 00:50:32,559
on youtube feeding their ai with more

00:50:31,680 --> 00:50:35,839
and more data

00:50:32,559 --> 00:50:37,440
and they were refusing to act so then

00:50:35,839 --> 00:50:39,200
one day i thought

00:50:37,440 --> 00:50:41,200
why are we waiting around for this

00:50:39,200 --> 00:50:44,319
company to do the right thing

00:50:41,200 --> 00:50:45,920
could there be another way if thousands

00:50:44,319 --> 00:50:48,240
of people had responded to my

00:50:45,920 --> 00:50:51,040
email sharing their story with us to

00:50:48,240 --> 00:50:53,119
pressure youtube to do the right thing

00:50:51,040 --> 00:50:54,480
might those same people be willing to go

00:50:53,119 --> 00:50:57,359
a little bit further

00:50:54,480 --> 00:50:58,960
to actually share data and hard evidence

00:50:57,359 --> 00:51:00,800
of the rabbit holes that they were stuck

00:50:58,960 --> 00:51:02,559
in on youtube

00:51:00,800 --> 00:51:03,839
what would putting thousands of these

00:51:02,559 --> 00:51:05,839
examples together

00:51:03,839 --> 00:51:07,599
reveal about the way that youtube were

00:51:05,839 --> 00:51:10,079
manipulating our behavior

00:51:07,599 --> 00:51:12,160
and shaping our view of the world would

00:51:10,079 --> 00:51:14,079
people trust us with their data

00:51:12,160 --> 00:51:16,079
knowing that we would use it in an

00:51:14,079 --> 00:51:17,359
effort to improve the experience for

00:51:16,079 --> 00:51:19,040
others

00:51:17,359 --> 00:51:20,960
could we build this in a way that

00:51:19,040 --> 00:51:23,200
protected people's privacy and was

00:51:20,960 --> 00:51:24,319
wasn't exploitative or extractive

00:51:23,200 --> 00:51:26,160
[Music]

00:51:24,319 --> 00:51:28,800
if youtube wouldn't tell us what was

00:51:26,160 --> 00:51:30,960
going on could we start to understand it

00:51:28,800 --> 00:51:34,480
ourselves

00:51:30,960 --> 00:51:35,920
next side so i started talking with some

00:51:34,480 --> 00:51:37,119
of my colleagues at the mozilla

00:51:35,920 --> 00:51:39,359
corporation

00:51:37,119 --> 00:51:41,040
and together actually a couple of years

00:51:39,359 --> 00:51:43,119
ago at mozfest

00:51:41,040 --> 00:51:44,160
the idea for the regrets reporter was

00:51:43,119 --> 00:51:46,160
born

00:51:44,160 --> 00:51:48,480
we decided that we would build a browser

00:51:46,160 --> 00:51:49,400
extension to enable people to send us

00:51:48,480 --> 00:51:51,119
the harmful

00:51:49,400 --> 00:51:54,079
recommendations that they were getting

00:51:51,119 --> 00:51:57,760
on youtube right as it happened

00:51:54,079 --> 00:51:59,599
next slide the regrets reporter lets you

00:51:57,760 --> 00:52:02,720
take action in real time

00:51:59,599 --> 00:52:03,440
with just three easy steps two thousand

00:52:02,720 --> 00:52:05,119
people

00:52:03,440 --> 00:52:08,160
had shared their stories in our first

00:52:05,119 --> 00:52:10,160
campaign but when we released this tool

00:52:08,160 --> 00:52:12,000
more than ten times that number of

00:52:10,160 --> 00:52:12,800
people came forward wanting to

00:52:12,000 --> 00:52:14,880
participate

00:52:12,800 --> 00:52:16,720
in just the two months the first two

00:52:14,880 --> 00:52:18,240
months that it was live

00:52:16,720 --> 00:52:20,160
through the regrets reporter we're

00:52:18,240 --> 00:52:22,400
already starting to see patterns

00:52:20,160 --> 00:52:24,000
in those people's experiences we're

00:52:22,400 --> 00:52:25,920
starting to understand where

00:52:24,000 --> 00:52:28,079
harm is occurring and hold youtube

00:52:25,920 --> 00:52:30,880
accountable despite the fact that our

00:52:28,079 --> 00:52:33,920
user base is just a teeny tiny fraction

00:52:30,880 --> 00:52:36,559
of all of the people who watch youtube

00:52:33,920 --> 00:52:37,359
i've used youtube as an example here but

00:52:36,559 --> 00:52:40,000
the story

00:52:37,359 --> 00:52:40,800
is bigger than youtube it's a story

00:52:40,000 --> 00:52:43,839
about data

00:52:40,800 --> 00:52:45,680
and power regrets reporter is part of a

00:52:43,839 --> 00:52:46,480
growing movement of data donation

00:52:45,680 --> 00:52:48,160
projects

00:52:46,480 --> 00:52:50,480
that are powered by people like you and

00:52:48,160 --> 00:52:54,160
me people who understand

00:52:50,480 --> 00:52:56,240
that our own data and our experiences

00:52:54,160 --> 00:52:57,359
are what give these algorithmic systems

00:52:56,240 --> 00:52:59,920
their power

00:52:57,359 --> 00:53:00,559
but we have a choice in how we use that

00:52:59,920 --> 00:53:02,319
power

00:53:00,559 --> 00:53:03,599
and we have to take a more active role

00:53:02,319 --> 00:53:05,440
in using that and

00:53:03,599 --> 00:53:06,960
understanding that we have the ability

00:53:05,440 --> 00:53:08,800
to do so

00:53:06,960 --> 00:53:11,119
so beyond regrets reporter which works

00:53:08,800 --> 00:53:12,079
with youtube we there are also so many

00:53:11,119 --> 00:53:14,160
other examples

00:53:12,079 --> 00:53:15,359
of people taking action to create

00:53:14,160 --> 00:53:19,119
transparency

00:53:15,359 --> 00:53:21,280
where there isn't any next slide

00:53:19,119 --> 00:53:23,200
there's ad observatory which is run by a

00:53:21,280 --> 00:53:24,240
group of researchers out of new york

00:53:23,200 --> 00:53:26,640
university

00:53:24,240 --> 00:53:28,880
and what's crap on whatsapp which is run

00:53:26,640 --> 00:53:31,040
by africa check and volume

00:53:28,880 --> 00:53:33,119
their citizen browser a project by

00:53:31,040 --> 00:53:35,040
investigative newsroom the markup

00:53:33,119 --> 00:53:36,559
and tracking exposed which is run by a

00:53:35,040 --> 00:53:39,119
small team of people who have been doing

00:53:36,559 --> 00:53:42,240
this work for many many years now

00:53:39,119 --> 00:53:44,400
next slide there's also really great

00:53:42,240 --> 00:53:46,880
work being done on this by my colleagues

00:53:44,400 --> 00:53:49,200
over on the mozilla corporation side

00:53:46,880 --> 00:53:50,480
they've repurposed the data platform

00:53:49,200 --> 00:53:53,359
that they built to support

00:53:50,480 --> 00:53:55,440
firefox product development at scale

00:53:53,359 --> 00:53:57,720
into a data donation platform

00:53:55,440 --> 00:53:59,200
that can support research and advocacy

00:53:57,720 --> 00:54:01,359
organizations

00:53:59,200 --> 00:54:02,240
mozilla rally is a data donation

00:54:01,359 --> 00:54:04,480
platform

00:54:02,240 --> 00:54:06,960
that enables people to donate their data

00:54:04,480 --> 00:54:09,839
to causes that they care about

00:54:06,960 --> 00:54:11,680
we hold two beliefs close that people

00:54:09,839 --> 00:54:12,880
have the right to choose who they share

00:54:11,680 --> 00:54:14,559
their data with

00:54:12,880 --> 00:54:17,040
and that there are communities who have

00:54:14,559 --> 00:54:19,280
actionable ideas about how to use that

00:54:17,040 --> 00:54:19,920
data to increase transparency on the

00:54:19,280 --> 00:54:22,640
internet

00:54:19,920 --> 00:54:24,000
and to build better technology over the

00:54:22,640 --> 00:54:26,640
past year we've been focusing

00:54:24,000 --> 00:54:28,480
on building open source tools to enable

00:54:26,640 --> 00:54:30,559
communities to develop their

00:54:28,480 --> 00:54:31,680
own data donation projects and we're

00:54:30,559 --> 00:54:34,319
excited to launch

00:54:31,680 --> 00:54:36,559
with our first partners discorder next

00:54:34,319 --> 00:54:38,880
slide

00:54:36,559 --> 00:54:40,400
and this is really just the start

00:54:38,880 --> 00:54:43,920
because transparency

00:54:40,400 --> 00:54:45,280
isn't an end goal in itself transparency

00:54:43,920 --> 00:54:48,000
is the starting point

00:54:45,280 --> 00:54:48,799
from which we can understand what is

00:54:48,000 --> 00:54:51,040
happening

00:54:48,799 --> 00:54:52,000
enough to be able to challenge it to

00:54:51,040 --> 00:54:55,200
contest it

00:54:52,000 --> 00:54:57,119
to try to change it eventually

00:54:55,200 --> 00:54:58,880
if we can understand these systems well

00:54:57,119 --> 00:55:01,040
enough and know what is happening

00:54:58,880 --> 00:55:02,160
on them we can imagine how they can be

00:55:01,040 --> 00:55:04,559
better

00:55:02,160 --> 00:55:06,079
we can imagine how to build algorithms

00:55:04,559 --> 00:55:08,160
that are optimized

00:55:06,079 --> 00:55:10,079
for the well-being of our communities

00:55:08,160 --> 00:55:12,240
and not the profit motivations of a

00:55:10,079 --> 00:55:14,079
handful of companies

00:55:12,240 --> 00:55:15,280
we can start building a future so that

00:55:14,079 --> 00:55:17,200
decades from now

00:55:15,280 --> 00:55:19,280
when that 10 year old little girl in the

00:55:17,200 --> 00:55:20,000
story is all grown up and maybe she has

00:55:19,280 --> 00:55:23,760
a daughter of her

00:55:20,000 --> 00:55:25,680
own that child will be safe online

00:55:23,760 --> 00:55:27,280
the internet will be a place where she

00:55:25,680 --> 00:55:29,119
can discover who she is

00:55:27,280 --> 00:55:32,079
and connect with content that allows her

00:55:29,119 --> 00:55:34,400
to explore and to grow in her talents

00:55:32,079 --> 00:55:35,839
we can build online spaces that value

00:55:34,400 --> 00:55:37,599
inclusion and freedom

00:55:35,839 --> 00:55:39,119
for those who are historically and

00:55:37,599 --> 00:55:41,359
presently marginalized

00:55:39,119 --> 00:55:43,599
so not one more young queer or trans

00:55:41,359 --> 00:55:45,520
person is targeted with shaming and

00:55:43,599 --> 00:55:47,359
harassing content just because of who

00:55:45,520 --> 00:55:50,319
they are

00:55:47,359 --> 00:55:52,400
our online spaces can be places that

00:55:50,319 --> 00:55:53,760
help our aging parents and loved ones

00:55:52,400 --> 00:55:59,119
feel more connected

00:55:53,760 --> 00:56:01,599
and less alone in the world next slide

00:55:59,119 --> 00:56:02,319
shifting power starts from a handful of

00:56:01,599 --> 00:56:04,960
people

00:56:02,319 --> 00:56:06,160
who believe that they can join us in

00:56:04,960 --> 00:56:08,559
doing so

00:56:06,160 --> 00:56:08,559
thank you

00:56:10,240 --> 00:56:16,319
brittany thank you so much for this talk

00:56:13,760 --> 00:56:18,400
um i love the idea of not having i mean

00:56:16,319 --> 00:56:21,440
unfortunately not having to wait

00:56:18,400 --> 00:56:24,960
for corporations to to

00:56:21,440 --> 00:56:26,799
respond to our uh our request for more

00:56:24,960 --> 00:56:28,160
accountability and more transparency and

00:56:26,799 --> 00:56:29,599
i love this this story that you shared

00:56:28,160 --> 00:56:30,720
about basically how you just flipped the

00:56:29,599 --> 00:56:32,319
table there

00:56:30,720 --> 00:56:33,680
and kind of took matters into your own

00:56:32,319 --> 00:56:35,920
hands uh so thank you so much for

00:56:33,680 --> 00:56:37,920
sharing that

00:56:35,920 --> 00:56:39,680
yeah really amazing brandi thanks so

00:56:37,920 --> 00:56:42,000
much and i would say your talk

00:56:39,680 --> 00:56:43,599
and all of these talks remind me of why

00:56:42,000 --> 00:56:44,000
i joined brazil in the first place and

00:56:43,599 --> 00:56:46,079
and

00:56:44,000 --> 00:56:47,040
frankly why we started moss fest 10

00:56:46,079 --> 00:56:49,359
years ago

00:56:47,040 --> 00:56:51,040
this kind of ethos of not just talking

00:56:49,359 --> 00:56:52,960
about what's broken on the internet

00:56:51,040 --> 00:56:54,240
but rolling up your sleeves and doing

00:56:52,960 --> 00:56:56,160
something to fix it

00:56:54,240 --> 00:56:58,079
and that's what firefox was about

00:56:56,160 --> 00:56:58,880
originally with open source and open

00:56:58,079 --> 00:57:00,880
standards

00:56:58,880 --> 00:57:02,559
and it's you know why we're focused on

00:57:00,880 --> 00:57:05,200
trustworthy ai today it just

00:57:02,559 --> 00:57:06,799
all ties back to those values uh in the

00:57:05,200 --> 00:57:08,400
mozilla manifesto

00:57:06,799 --> 00:57:09,839
and i just i'm so grateful for the work

00:57:08,400 --> 00:57:12,240
that all three of our speakers

00:57:09,839 --> 00:57:13,119
and frankly everybody here at mozfast is

00:57:12,240 --> 00:57:15,200
doing

00:57:13,119 --> 00:57:17,599
kind of in pursuit of that role up your

00:57:15,200 --> 00:57:19,680
sleeves and advance those values

00:57:17,599 --> 00:57:21,680
uh kind of approach and i'm committed to

00:57:19,680 --> 00:57:23,920
doing everything i can to help these

00:57:21,680 --> 00:57:27,440
people succeed and putting mozilla's

00:57:23,920 --> 00:57:30,160
resources and muscle behind that

00:57:27,440 --> 00:57:32,400
um thank you mark and that's that makes

00:57:30,160 --> 00:57:34,559
me very happy to hear as well um

00:57:32,400 --> 00:57:35,680
mostly because i think uh truly mozilla

00:57:34,559 --> 00:57:38,559
has such a

00:57:35,680 --> 00:57:39,599
unique chance to carve itself a spot in

00:57:38,559 --> 00:57:41,839
this conversation

00:57:39,599 --> 00:57:42,960
about ai and to distinguish itself from

00:57:41,839 --> 00:57:44,559
other companies

00:57:42,960 --> 00:57:46,160
uh because you've already done this in

00:57:44,559 --> 00:57:47,520
so many ways like um

00:57:46,160 --> 00:57:50,640
for example just like you talked about

00:57:47,520 --> 00:57:53,200
firefox firefox really cemented itself

00:57:50,640 --> 00:57:54,799
um and cemented its place as a privacy

00:57:53,200 --> 00:57:56,640
first browser

00:57:54,799 --> 00:57:58,240
and now mozilla has a chance to kind of

00:57:56,640 --> 00:58:01,359
do the same thing but with

00:57:58,240 --> 00:58:03,520
ai principles and ai development and i

00:58:01,359 --> 00:58:05,599
would say really more than just a chance

00:58:03,520 --> 00:58:07,119
i think mozilla has a responsibility to

00:58:05,599 --> 00:58:09,520
do it because it's so

00:58:07,119 --> 00:58:10,559
tied to your core values uh you've

00:58:09,520 --> 00:58:13,839
already all funded

00:58:10,559 --> 00:58:15,440
work and people like myself um whether

00:58:13,839 --> 00:58:18,799
it was like the open web fellowship

00:58:15,440 --> 00:58:20,319
or the policy fellowships um towards

00:58:18,799 --> 00:58:21,440
like building towards that future

00:58:20,319 --> 00:58:23,520
essentially and there is

00:58:21,440 --> 00:58:25,280
there is truly a responsibility not just

00:58:23,520 --> 00:58:27,040
a chance but a responsibility to keep

00:58:25,280 --> 00:58:29,520
going in that direction

00:58:27,040 --> 00:58:31,119
and um we all know this like we know

00:58:29,520 --> 00:58:32,480
technology is not neutral the way that

00:58:31,119 --> 00:58:33,520
we make technology the way that we

00:58:32,480 --> 00:58:35,119
conceptualize it

00:58:33,520 --> 00:58:37,440
how it fits into our culture the

00:58:35,119 --> 00:58:38,799
problems it solves those are all design

00:58:37,440 --> 00:58:41,280
decisions that can have

00:58:38,799 --> 00:58:43,040
a really deep impact on society and you

00:58:41,280 --> 00:58:43,599
always have to be asking like what's the

00:58:43,040 --> 00:58:45,839
worst

00:58:43,599 --> 00:58:46,720
case scenario uh what's the worst thing

00:58:45,839 --> 00:58:48,799
that can happen

00:58:46,720 --> 00:58:50,079
and you have to plan for those use cases

00:58:48,799 --> 00:58:53,680
even if they represent

00:58:50,079 --> 00:58:55,359
maybe 0.5 of the use cases that you

00:58:53,680 --> 00:58:58,000
might have

00:58:55,359 --> 00:58:59,680
and um like i said i mean we all know

00:58:58,000 --> 00:59:00,720
this on some level just a conversation

00:58:59,680 --> 00:59:04,559
about ai and the

00:59:00,720 --> 00:59:06,079
impact that it can have on society um

00:59:04,559 --> 00:59:07,520
and what's important to remember is like

00:59:06,079 --> 00:59:08,079
the the people who are the most

00:59:07,520 --> 00:59:09,680
vulnerable

00:59:08,079 --> 00:59:11,839
in society the most likely to be

00:59:09,680 --> 00:59:13,359
harassed uh the most likely to be

00:59:11,839 --> 00:59:14,319
surveilled all the stories that were

00:59:13,359 --> 00:59:16,000
shared here today

00:59:14,319 --> 00:59:18,160
those are also the people who are the

00:59:16,000 --> 00:59:19,599
most vulnerable to technology

00:59:18,160 --> 00:59:21,599
and to certain developments into

00:59:19,599 --> 00:59:23,920
technology and when we put

00:59:21,599 --> 00:59:25,119
energy and protecting these people

00:59:23,920 --> 00:59:26,799
specifically

00:59:25,119 --> 00:59:28,720
those who are maybe living in that

00:59:26,799 --> 00:59:30,480
dystopian future that we want to avoid

00:59:28,720 --> 00:59:32,319
we end up protecting everyone and we

00:59:30,480 --> 00:59:33,119
make sure that we as a society don't end

00:59:32,319 --> 00:59:36,240
up there

00:59:33,119 --> 00:59:38,559
um in that dystopian future so

00:59:36,240 --> 00:59:40,160
in working on aia development and in

00:59:38,559 --> 00:59:42,400
mozilla working on ai

00:59:40,160 --> 00:59:43,200
development and principle in being

00:59:42,400 --> 00:59:46,079
intentional

00:59:43,200 --> 00:59:48,160
about slower and more thoughtful

00:59:46,079 --> 00:59:48,960
development and thinking about ai at

00:59:48,160 --> 00:59:51,440
every step

00:59:48,960 --> 00:59:52,880
of that process when we make sure that

00:59:51,440 --> 00:59:54,640
that 0.5

00:59:52,880 --> 00:59:56,640
of people are safe we end up building

00:59:54,640 --> 01:00:00,799
tools for everyone

00:59:56,640 --> 01:00:01,599
and i want to mostly urge everyone to

01:00:00,799 --> 01:00:05,119
resist

01:00:01,599 --> 01:00:07,040
the feeling of inevitability this idea

01:00:05,119 --> 01:00:08,880
that like we can't change the future

01:00:07,040 --> 01:00:10,640
ai is just what it is it's going to

01:00:08,880 --> 01:00:12,240
evolve this way it's going to have an

01:00:10,640 --> 01:00:13,760
impact on people this way

01:00:12,240 --> 01:00:15,920
and some people are going to be harmed

01:00:13,760 --> 01:00:18,640
by it

01:00:15,920 --> 01:00:19,760
like all these things of computing and

01:00:18,640 --> 01:00:21,599
all the

01:00:19,760 --> 01:00:23,440
technology development is almost

01:00:21,599 --> 01:00:25,200
impossible to rethink

01:00:23,440 --> 01:00:27,520
and impossible to redesign and to

01:00:25,200 --> 01:00:30,240
implement but that's not true

01:00:27,520 --> 01:00:31,680
even though ai today maybe might be

01:00:30,240 --> 01:00:33,599
becoming more opaque and more

01:00:31,680 --> 01:00:34,079
unaccountable and more divisive in many

01:00:33,599 --> 01:00:36,079
ways

01:00:34,079 --> 01:00:37,920
there are very few companies that are

01:00:36,079 --> 01:00:38,960
that are stepping in and offering a

01:00:37,920 --> 01:00:41,520
different

01:00:38,960 --> 01:00:42,160
uh and healthy vision of computing and

01:00:41,520 --> 01:00:44,319
of ai

01:00:42,160 --> 01:00:45,839
development and that are building on top

01:00:44,319 --> 01:00:48,240
of that and mozilla could totally play

01:00:45,839 --> 01:00:50,640
that role essentially and really shape

01:00:48,240 --> 01:00:52,000
that future which obviously is going to

01:00:50,640 --> 01:00:54,559
completely steer the direction which

01:00:52,000 --> 01:00:56,640
we're going to

01:00:54,559 --> 01:00:58,640
thank you sarah and i could not agree

01:00:56,640 --> 01:01:00,880
more and i really appreciate the push

01:00:58,640 --> 01:01:02,720
i mean that is where we need to to show

01:01:00,880 --> 01:01:04,720
up and i would say

01:01:02,720 --> 01:01:06,799
you know i think back over the 10 years

01:01:04,720 --> 01:01:08,880
of mosfets and and look at the people

01:01:06,799 --> 01:01:11,280
who i've already seen show up at mozfest

01:01:08,880 --> 01:01:12,079
this year this community is a group of

01:01:11,280 --> 01:01:14,720
people who

01:01:12,079 --> 01:01:16,640
do not take the direction of technology

01:01:14,720 --> 01:01:19,760
and the digital world is inevitable

01:01:16,640 --> 01:01:21,599
um and certainly i am committed to

01:01:19,760 --> 01:01:23,760
making sure that mozfest stays this way

01:01:21,599 --> 01:01:24,640
and then mozilla continues to back

01:01:23,760 --> 01:01:26,480
people

01:01:24,640 --> 01:01:28,079
who are trying to take technology

01:01:26,480 --> 01:01:30,240
practically concretely

01:01:28,079 --> 01:01:32,720
in a different direction so really

01:01:30,240 --> 01:01:35,119
excited and grateful to everybody who

01:01:32,720 --> 01:01:36,880
was on this panel and and then this

01:01:35,119 --> 01:01:38,400
plenary for

01:01:36,880 --> 01:01:41,040
for being here and showing that in

01:01:38,400 --> 01:01:42,319
action and sarah um for co-hosting with

01:01:41,040 --> 01:01:45,520
me thank you

01:01:42,319 --> 01:01:46,880
thank you so much and um thank you to

01:01:45,520 --> 01:01:48,400
everybody who's here at moz fest

01:01:46,880 --> 01:01:49,119
hopefully you've got some inspiration

01:01:48,400 --> 01:01:50,799
but i know

01:01:49,119 --> 01:01:52,480
everything you're doing is sharing

01:01:50,799 --> 01:01:53,680
inspiration as we bounce off of each

01:01:52,480 --> 01:01:55,839
other so enjoy

01:01:53,680 --> 01:01:58,160
the rest of mod's nest and enjoy each

01:01:55,839 --> 01:02:07,839
other thanks very much

01:01:58,160 --> 01:02:07,839
thank you

01:02:24,240 --> 01:02:26,319

YouTube URL: https://www.youtube.com/watch?v=VdxPi3yEf_w


