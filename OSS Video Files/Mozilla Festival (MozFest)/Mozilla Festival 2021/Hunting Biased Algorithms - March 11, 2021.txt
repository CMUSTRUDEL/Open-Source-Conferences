Title: Hunting Biased Algorithms - March 11, 2021
Publication date: 2021-02-05
Playlist: Mozilla Festival 2021
Description: 
	
Captions: 
	00:00:01,490 --> 00:00:27,399
[Music]

00:00:43,270 --> 00:01:06,129
[Music]

00:01:22,000 --> 00:01:46,879
[Music]

00:01:50,620 --> 00:02:43,300
[Music]

00:02:59,170 --> 00:03:24,060
[Music]

00:03:27,800 --> 00:03:43,860
[Music]

00:03:44,840 --> 00:03:47,840
do

00:03:54,570 --> 00:04:20,479
[Music]

00:04:36,350 --> 00:05:01,259
[Music]

00:05:04,980 --> 00:05:57,660
[Music]

00:06:13,530 --> 00:06:38,410
[Music]

00:06:42,150 --> 00:07:34,820
[Music]

00:07:50,700 --> 00:08:15,589
[Music]

00:08:19,330 --> 00:09:12,009
[Music]

00:09:27,880 --> 00:09:52,770
[Music]

00:09:56,510 --> 00:10:13,869
[Music]

00:10:17,360 --> 00:10:22,800
hi my name is deb raji um and i

00:10:20,480 --> 00:10:24,399
am a mozilla fellow studying algorithmic

00:10:22,800 --> 00:10:25,680
auditing and evaluation

00:10:24,399 --> 00:10:27,600
um i work very closely with the

00:10:25,680 --> 00:10:29,519
algorithmic justice league initiative

00:10:27,600 --> 00:10:31,040
which was founded by joy bulenwini at

00:10:29,519 --> 00:10:33,760
the mit media lab

00:10:31,040 --> 00:10:34,640
um and i've worked with google's ethical

00:10:33,760 --> 00:10:37,279
ai team

00:10:34,640 --> 00:10:38,480
um thinking about research engineering

00:10:37,279 --> 00:10:40,640
practice

00:10:38,480 --> 00:10:42,240
for responsible machine learning

00:10:40,640 --> 00:10:44,240
development as well as the partnership

00:10:42,240 --> 00:10:48,640
on ai and the ai now institute at the

00:10:44,240 --> 00:10:51,040
new york university i'm glad to be here

00:10:48,640 --> 00:10:53,120
and hi my name is camille francois i'm

00:10:51,040 --> 00:10:54,959
the chief innovation officer at graphica

00:10:53,120 --> 00:10:56,160
an affiliate at the harvard burke and

00:10:54,959 --> 00:10:58,320
science center

00:10:56,160 --> 00:11:00,079
for internet in society and the mozilla

00:10:58,320 --> 00:11:03,440
fellow in this capacity

00:11:00,079 --> 00:11:06,560
i have the uh pleasure and

00:11:03,440 --> 00:11:08,320
uh honor to work with deb on a project

00:11:06,560 --> 00:11:09,920
at the algorithmic justice league where

00:11:08,320 --> 00:11:13,120
we are trying to figure out

00:11:09,920 --> 00:11:16,079
what is the best way to um

00:11:13,120 --> 00:11:16,640
report and address algorithmic hearts

00:11:16,079 --> 00:11:19,600
and

00:11:16,640 --> 00:11:21,279
uh devin very excited for people to hear

00:11:19,600 --> 00:11:21,839
more about the work that you've been

00:11:21,279 --> 00:11:24,959
doing

00:11:21,839 --> 00:11:26,320
in this area and so if i'm allowed to

00:11:24,959 --> 00:11:29,920
ask the first question

00:11:26,320 --> 00:11:32,560
uh i will ask leading one that says them

00:11:29,920 --> 00:11:33,440
tell us about your work um looking into

00:11:32,560 --> 00:11:36,240
what are the current

00:11:33,440 --> 00:11:37,279
ways in which algorithmic harms are

00:11:36,240 --> 00:11:40,399
discovered

00:11:37,279 --> 00:11:41,120
uh are um investigated and then

00:11:40,399 --> 00:11:43,440
ultimately

00:11:41,120 --> 00:11:45,279
are addressed uh what are the types of

00:11:43,440 --> 00:11:47,120
processes that exist to do that

00:11:45,279 --> 00:11:48,560
um and the real question i want to ask

00:11:47,120 --> 00:11:50,240
is does it

00:11:48,560 --> 00:11:52,000
does it really is it really like those

00:11:50,240 --> 00:11:54,000
people complaining on twitter

00:11:52,000 --> 00:11:56,240
and then this makes the headlines and

00:11:54,000 --> 00:11:58,399
then corporations finally take it

00:11:56,240 --> 00:12:00,720
seriously i know you've worked a lot on

00:11:58,399 --> 00:12:03,040
uh surveying the existing practices and

00:12:00,720 --> 00:12:05,360
i think that's a great place to start

00:12:03,040 --> 00:12:06,079
yeah so for for one thing you know the

00:12:05,360 --> 00:12:09,040
twitter

00:12:06,079 --> 00:12:11,200
outrage leading to you know uh you know

00:12:09,040 --> 00:12:14,000
press attention and public outrage

00:12:11,200 --> 00:12:15,839
is definitely a strategy and we we we

00:12:14,000 --> 00:12:18,480
are we describe the strategy as sort of

00:12:15,839 --> 00:12:20,959
like a participatory approach towards

00:12:18,480 --> 00:12:22,800
um identifying harms where you have like

00:12:20,959 --> 00:12:24,639
the public being able to or member of

00:12:22,800 --> 00:12:26,240
the public being able to identify

00:12:24,639 --> 00:12:28,160
that they've been negatively affected by

00:12:26,240 --> 00:12:31,279
an algorithm they post about it

00:12:28,160 --> 00:12:32,160
um and you know it garners attention and

00:12:31,279 --> 00:12:34,160
that causes

00:12:32,160 --> 00:12:35,200
people to pay attention to the harm and

00:12:34,160 --> 00:12:36,800
attempt to address it

00:12:35,200 --> 00:12:38,639
a common example of this would be with

00:12:36,800 --> 00:12:40,560
the apple card um

00:12:38,639 --> 00:12:42,639
credit assignment situation where apple

00:12:40,560 --> 00:12:45,360
was releasing credit cards and

00:12:42,639 --> 00:12:47,040
um different levels of um credit were

00:12:45,360 --> 00:12:48,720
given to men and women and

00:12:47,040 --> 00:12:50,480
you know very prominent technologists

00:12:48,720 --> 00:12:51,360
had posted about it and escalated the

00:12:50,480 --> 00:12:53,120
issue

00:12:51,360 --> 00:12:54,880
um and that's sort of more of a

00:12:53,120 --> 00:12:56,320
participatory approach and we do see

00:12:54,880 --> 00:12:58,079
that a lot that is a lot of

00:12:56,320 --> 00:12:59,680
a lot of cases of algorithmic harm or

00:12:58,079 --> 00:13:01,920
just people getting you know

00:12:59,680 --> 00:13:03,839
upset and and escalating it over social

00:13:01,920 --> 00:13:05,440
media but there's a lot of other really

00:13:03,839 --> 00:13:07,120
interesting strategies that people have

00:13:05,440 --> 00:13:08,399
as well in terms of reporting these

00:13:07,120 --> 00:13:10,800
harms

00:13:08,399 --> 00:13:12,480
an interesting strategy that's um been

00:13:10,800 --> 00:13:15,279
championed

00:13:12,480 --> 00:13:16,399
at northeastern's sort of data

00:13:15,279 --> 00:13:18,560
journalism

00:13:16,399 --> 00:13:20,000
uh program and lab has been this

00:13:18,560 --> 00:13:23,839
approach of transparency for

00:13:20,000 --> 00:13:26,320
the sake of uh discovery so

00:13:23,839 --> 00:13:27,760
they will sort of through foia requests

00:13:26,320 --> 00:13:29,600
um attempt to

00:13:27,760 --> 00:13:31,440
list out and articulate all the

00:13:29,600 --> 00:13:33,680
different types of algorithms being used

00:13:31,440 --> 00:13:35,440
by a very critical agency let's say

00:13:33,680 --> 00:13:37,360
ice or the department of homeland

00:13:35,440 --> 00:13:38,880
security and then just by virtue of

00:13:37,360 --> 00:13:41,920
having a complete or

00:13:38,880 --> 00:13:43,760
or uh you know a visible list of the

00:13:41,920 --> 00:13:44,880
kind of algorithms being used having

00:13:43,760 --> 00:13:46,399
that disclosure

00:13:44,880 --> 00:13:48,000
um they'll they'll you'll be able to

00:13:46,399 --> 00:13:49,279
sort of get a sense of which algorithms

00:13:48,000 --> 00:13:51,440
are impactful and be able to

00:13:49,279 --> 00:13:53,040
investigate further which algorithms to

00:13:51,440 --> 00:13:54,800
pay attention to this is also an

00:13:53,040 --> 00:13:58,240
approach that was um

00:13:54,800 --> 00:13:59,199
uh sort of the the logic behind the post

00:13:58,240 --> 00:14:02,000
act in new york

00:13:59,199 --> 00:14:03,680
um to sort of enforce a little bit of

00:14:02,000 --> 00:14:05,519
visibility and disclosure around what

00:14:03,680 --> 00:14:06,240
kind of surveillance tools nypd was

00:14:05,519 --> 00:14:09,199
using

00:14:06,240 --> 00:14:10,399
and that helps people be able to sort of

00:14:09,199 --> 00:14:12,399
take a look at that list and

00:14:10,399 --> 00:14:14,639
identify you know which are likely to

00:14:12,399 --> 00:14:15,440
cause harm so the id the strategy of

00:14:14,639 --> 00:14:17,519
disclosure

00:14:15,440 --> 00:14:18,959
is also really a really sort of popular

00:14:17,519 --> 00:14:20,480
technique um

00:14:18,959 --> 00:14:23,040
and then there's this sort of third

00:14:20,480 --> 00:14:24,320
strategy of like internal inspection or

00:14:23,040 --> 00:14:26,240
internal auditing

00:14:24,320 --> 00:14:27,760
um and this is where the tech companies

00:14:26,240 --> 00:14:29,199
will either have some level of like

00:14:27,760 --> 00:14:31,199
customer feedback

00:14:29,199 --> 00:14:33,120
or like built into their product or

00:14:31,199 --> 00:14:35,279
built into their their process or

00:14:33,120 --> 00:14:37,839
they'll have like an internal audit team

00:14:35,279 --> 00:14:39,519
whose job it is to really actively think

00:14:37,839 --> 00:14:41,360
about what can go wrong

00:14:39,519 --> 00:14:42,720
um and try to explore all these

00:14:41,360 --> 00:14:44,160
different strategies and attempt to

00:14:42,720 --> 00:14:46,880
discover harms through that

00:14:44,160 --> 00:14:48,800
sort of speculative process um so yeah

00:14:46,880 --> 00:14:50,160
lots of interesting strategies from

00:14:48,800 --> 00:14:52,720
different individuals

00:14:50,160 --> 00:14:54,000
um in the harms discovery stage which is

00:14:52,720 --> 00:14:55,600
very much

00:14:54,000 --> 00:14:57,120
you know less of a focus i think the

00:14:55,600 --> 00:14:59,680
machine learning community is

00:14:57,120 --> 00:15:00,560
really into this sort of like reactive

00:14:59,680 --> 00:15:02,240
phase where

00:15:00,560 --> 00:15:03,760
um people are just realizing that things

00:15:02,240 --> 00:15:05,920
can go wrong in this way

00:15:03,760 --> 00:15:07,360
and they're jumping right away to like

00:15:05,920 --> 00:15:09,279
how can we actually

00:15:07,360 --> 00:15:10,880
you know analyze these harms how can we

00:15:09,279 --> 00:15:12,639
fix them um

00:15:10,880 --> 00:15:14,320
so it's been really fun to actually

00:15:12,639 --> 00:15:15,680
explore how we discover these harms and

00:15:14,320 --> 00:15:18,000
how we find out that things are wrong in

00:15:15,680 --> 00:15:20,480
the first place

00:15:18,000 --> 00:15:22,240
for sure and i think that for our

00:15:20,480 --> 00:15:23,760
project this was a point of departure

00:15:22,240 --> 00:15:25,040
right like as you said

00:15:23,760 --> 00:15:26,880
the fact that there's a little bit of

00:15:25,040 --> 00:15:27,760
that just happening on social media in a

00:15:26,880 --> 00:15:30,079
crowd source way

00:15:27,760 --> 00:15:32,320
perhaps is okay perhaps that's some form

00:15:30,079 --> 00:15:32,959
of like participatory harms discovery

00:15:32,320 --> 00:15:34,800
and

00:15:32,959 --> 00:15:36,480
as we were thinking through like what

00:15:34,800 --> 00:15:38,720
other models could we

00:15:36,480 --> 00:15:41,199
learn from in order to create these

00:15:38,720 --> 00:15:43,600
maybe more viable maybe more structured

00:15:41,199 --> 00:15:46,079
participatory harms discovery we ended

00:15:43,600 --> 00:15:48,320
up thinking about the bug bounties in

00:15:46,079 --> 00:15:49,519
zach and sort of studying that bug

00:15:48,320 --> 00:15:52,720
bounty model

00:15:49,519 --> 00:15:54,160
to learn really what could be brought

00:15:52,720 --> 00:15:56,560
from these practices

00:15:54,160 --> 00:15:57,920
into the algorithmic harm space um and

00:15:56,560 --> 00:16:00,000
to also learn

00:15:57,920 --> 00:16:01,680
uh what are you know what are the things

00:16:00,000 --> 00:16:02,560
that can go wrong when you're setting up

00:16:01,680 --> 00:16:05,120
these backbone

00:16:02,560 --> 00:16:06,000
platforms yeah yeah i think that was

00:16:05,120 --> 00:16:08,720
like a very

00:16:06,000 --> 00:16:09,759
important uh like step in terms of

00:16:08,720 --> 00:16:13,040
insight where

00:16:09,759 --> 00:16:14,160
um uh the way that participatory homes

00:16:13,040 --> 00:16:15,519
discovery happens right now for

00:16:14,160 --> 00:16:18,240
algorithmic harm

00:16:15,519 --> 00:16:20,240
is so haphazard everyone has their own

00:16:18,240 --> 00:16:21,920
understanding of how they're aggrieved

00:16:20,240 --> 00:16:23,920
um you know something that's been really

00:16:21,920 --> 00:16:25,440
great about like exploring bug bounty

00:16:23,920 --> 00:16:27,199
literature is that they're

00:16:25,440 --> 00:16:28,720
super thoughtful around ideas around

00:16:27,199 --> 00:16:30,160
like verification like if

00:16:28,720 --> 00:16:31,440
just because someone feels like they've

00:16:30,160 --> 00:16:32,079
been harmed you know how do you actually

00:16:31,440 --> 00:16:34,480
verify

00:16:32,079 --> 00:16:36,560
that the the the security vulnerability

00:16:34,480 --> 00:16:38,000
is there or in this case the algorithmic

00:16:36,560 --> 00:16:39,600
harm is legitimate

00:16:38,000 --> 00:16:41,040
um you know how do you actually

00:16:39,600 --> 00:16:42,480
prioritize different harms that have

00:16:41,040 --> 00:16:44,399
been reported and i think that like

00:16:42,480 --> 00:16:46,560
these are all things that are very

00:16:44,399 --> 00:16:48,160
kind of they're they've been reflected

00:16:46,560 --> 00:16:50,800
on in the security space

00:16:48,160 --> 00:16:51,680
and in the algorithmic arm space um we

00:16:50,800 --> 00:16:53,199
haven't really

00:16:51,680 --> 00:16:54,800
really thought about that yet and and

00:16:53,199 --> 00:16:56,399
this was something um

00:16:54,800 --> 00:16:58,800
that i think camille had mentioned as

00:16:56,399 --> 00:17:00,079
well in terms of what is that connection

00:16:58,800 --> 00:17:02,639
between

00:17:00,079 --> 00:17:04,079
security vulnerabilities and algorithmic

00:17:02,639 --> 00:17:05,919
harms and what do you see as

00:17:04,079 --> 00:17:07,600
you know the thing that links those two

00:17:05,919 --> 00:17:08,720
um in terms of how we approach both of

00:17:07,600 --> 00:17:10,880
them but also

00:17:08,720 --> 00:17:12,959
uh just theoretically how do you see

00:17:10,880 --> 00:17:14,720
them being linked or connected

00:17:12,959 --> 00:17:16,480
yeah it's really interesting because i

00:17:14,720 --> 00:17:18,319
think that we've spent a bit of time

00:17:16,480 --> 00:17:21,199
thinking about what is uh

00:17:18,319 --> 00:17:21,600
similar between uh algorithmic harms and

00:17:21,199 --> 00:17:24,079
sort of

00:17:21,600 --> 00:17:24,640
cyber security vulnerability and what is

00:17:24,079 --> 00:17:26,000
different

00:17:24,640 --> 00:17:27,280
and i think more importantly we spend

00:17:26,000 --> 00:17:29,360
quite a bit of time thinking about like

00:17:27,280 --> 00:17:30,320
what is similar with the state of the

00:17:29,360 --> 00:17:33,280
field

00:17:30,320 --> 00:17:35,039
of cyber security when bug bounties

00:17:33,280 --> 00:17:36,320
became a thing and what is similar

00:17:35,039 --> 00:17:37,600
between the state of the field and

00:17:36,320 --> 00:17:40,240
algorithmic harm

00:17:37,600 --> 00:17:42,720
at that moment where we question uh what

00:17:40,240 --> 00:17:45,679
does a participatory and responsible

00:17:42,720 --> 00:17:47,360
uh harms discovery process look like um

00:17:45,679 --> 00:17:50,080
yeah i want to share the sort of like

00:17:47,360 --> 00:17:51,120
main questions that we've had when we

00:17:50,080 --> 00:17:52,799
started looking into

00:17:51,120 --> 00:17:55,120
boundary literature and talking to

00:17:52,799 --> 00:17:56,799
people in the face because i think

00:17:55,120 --> 00:17:58,880
they sort of helped get a precise idea

00:17:56,799 --> 00:18:02,720
forward after um

00:17:58,880 --> 00:18:05,440
the first one is really understanding

00:18:02,720 --> 00:18:07,280
how big bounties help structure

00:18:05,440 --> 00:18:09,440
communities of practice

00:18:07,280 --> 00:18:11,280
and we looked at bug bounties programs

00:18:09,440 --> 00:18:12,960
and platforms to think about like our

00:18:11,280 --> 00:18:15,120
own community of practice around people

00:18:12,960 --> 00:18:16,320
who can do the discovery of algorithmic

00:18:15,120 --> 00:18:19,200
harms and thinking

00:18:16,320 --> 00:18:20,240
do book bounties and open competitions

00:18:19,200 --> 00:18:22,559
play a role

00:18:20,240 --> 00:18:24,400
in creating this community of practice

00:18:22,559 --> 00:18:25,120
it was really interesting and talking to

00:18:24,400 --> 00:18:27,120
like the

00:18:25,120 --> 00:18:28,640
platforms to hear that they really see

00:18:27,120 --> 00:18:31,679
their own role as

00:18:28,640 --> 00:18:33,840
talent platforms right like going to see

00:18:31,679 --> 00:18:34,720
and like recruiting new hackers and

00:18:33,840 --> 00:18:36,720
telling them

00:18:34,720 --> 00:18:38,480
come and tinker and play with this and i

00:18:36,720 --> 00:18:38,960
think like that that we kind of want to

00:18:38,480 --> 00:18:42,000
bring

00:18:38,960 --> 00:18:43,679
from the space into yeah

00:18:42,000 --> 00:18:45,600
do you feel like there's a cohesive

00:18:43,679 --> 00:18:46,799
community in the algorithmic auditing

00:18:45,600 --> 00:18:48,720
space or do you feel like

00:18:46,799 --> 00:18:50,000
um that's still sort of this very

00:18:48,720 --> 00:18:52,160
disparate thing and there's different

00:18:50,000 --> 00:18:55,120
communities out there right now

00:18:52,160 --> 00:18:56,000
um i don't know what what what do you

00:18:55,120 --> 00:18:57,919
think i think that there's

00:18:56,000 --> 00:18:59,440
quite a bit that we can do to help

00:18:57,919 --> 00:19:01,120
create like uh

00:18:59,440 --> 00:19:02,640
a more cohesive yeah i don't want to

00:19:01,120 --> 00:19:04,559
interrupt you either

00:19:02,640 --> 00:19:06,320
um i was just yeah i think i think like

00:19:04,559 --> 00:19:07,919
in the algorithmic auditing space

00:19:06,320 --> 00:19:09,600
um it's really interesting because you

00:19:07,919 --> 00:19:10,320
have very different groups doing this

00:19:09,600 --> 00:19:12,080
work

00:19:10,320 --> 00:19:14,799
in terms of just practical algorithmic

00:19:12,080 --> 00:19:17,039
auditing work you have like journalists

00:19:14,799 --> 00:19:18,799
like at the markup for example doing

00:19:17,039 --> 00:19:19,919
like doing an algorithmic audit but you

00:19:18,799 --> 00:19:23,039
also have

00:19:19,919 --> 00:19:26,080
um you know civil society orgs

00:19:23,039 --> 00:19:28,320
or advocacy orgs like the aclu and ajl

00:19:26,080 --> 00:19:29,440
but also like regulatory bodies like

00:19:28,320 --> 00:19:31,039
mist

00:19:29,440 --> 00:19:32,880
and then like i mentioned these internal

00:19:31,039 --> 00:19:34,320
auditing teams so it's a really

00:19:32,880 --> 00:19:36,799
interesting

00:19:34,320 --> 00:19:38,480
ecosystem of like people that would see

00:19:36,799 --> 00:19:39,600
themselves as being part of a community

00:19:38,480 --> 00:19:40,720
doing this work

00:19:39,600 --> 00:19:43,120
and i've always found that an

00:19:40,720 --> 00:19:45,039
interesting difference between um

00:19:43,120 --> 00:19:46,160
sort of security where it seems like

00:19:45,039 --> 00:19:48,320
there's a little bit more cohesion

00:19:46,160 --> 00:19:50,640
around this identity of being a hacker

00:19:48,320 --> 00:19:52,080
um that we still like haven't quite

00:19:50,640 --> 00:19:52,799
pinned down in the algorithmic auditing

00:19:52,080 --> 00:19:54,559
space

00:19:52,799 --> 00:19:56,000
that being said like the hacker

00:19:54,559 --> 00:19:56,240
literature really tells you that there

00:19:56,000 --> 00:19:58,720
are

00:19:56,240 --> 00:20:00,400
different communities of hacker and that

00:19:58,720 --> 00:20:02,240
by and large they dear friend

00:20:00,400 --> 00:20:03,440
the hacker community with sort of that

00:20:02,240 --> 00:20:05,840
one big c

00:20:03,440 --> 00:20:08,000
is is a bit of a of a misnomer really

00:20:05,840 --> 00:20:09,919
but i think that leads us well to like

00:20:08,000 --> 00:20:11,600
our second question when we looked at

00:20:09,919 --> 00:20:13,919
these bug bounties which is

00:20:11,600 --> 00:20:15,360
what do they contribute in terms of

00:20:13,919 --> 00:20:17,760
tools and methods

00:20:15,360 --> 00:20:19,520
to empower this community of practice

00:20:17,760 --> 00:20:22,080
and really to help people

00:20:19,520 --> 00:20:24,159
consider it as a viable path right like

00:20:22,080 --> 00:20:24,799
what type of templates do they put on

00:20:24,159 --> 00:20:26,640
the table

00:20:24,799 --> 00:20:28,080
what type of tools and tutorials like

00:20:26,640 --> 00:20:31,360
what do we need

00:20:28,080 --> 00:20:32,799
for the bar to be lowered for people

00:20:31,360 --> 00:20:34,720
during this community practice right for

00:20:32,799 --> 00:20:37,840
people to say like well i might do

00:20:34,720 --> 00:20:39,760
like casual hunting for six hours and

00:20:37,840 --> 00:20:42,640
we'll learn something and perhaps

00:20:39,760 --> 00:20:44,000
to do a contribution yeah and i think

00:20:42,640 --> 00:20:47,440
like we learned quite a bit

00:20:44,000 --> 00:20:50,720
in terms of like how much there is still

00:20:47,440 --> 00:20:52,320
to structure to help to help empower the

00:20:50,720 --> 00:20:54,880
community that way

00:20:52,320 --> 00:20:56,080
and the third sort of question that we

00:20:54,880 --> 00:20:58,320
had that i think

00:20:56,080 --> 00:20:59,120
um was perhaps the most difficult for us

00:20:58,320 --> 00:21:01,440
to to

00:20:59,120 --> 00:21:02,480
to wrestle with is um what do book

00:21:01,440 --> 00:21:05,280
bounties

00:21:02,480 --> 00:21:06,480
contribute in terms of increasing

00:21:05,280 --> 00:21:08,880
transparency

00:21:06,480 --> 00:21:10,880
and and accountability around these

00:21:08,880 --> 00:21:14,320
issues at the industry level

00:21:10,880 --> 00:21:15,679
um and like that let me know if you

00:21:14,320 --> 00:21:16,080
agree but i think like this is where

00:21:15,679 --> 00:21:19,039
we've

00:21:16,080 --> 00:21:20,320
seen most of the sort of counter

00:21:19,039 --> 00:21:23,039
examples right like

00:21:20,320 --> 00:21:24,640
talking to hackers and platforms who are

00:21:23,039 --> 00:21:25,280
saying like yes there's this great sort

00:21:24,640 --> 00:21:27,280
of like

00:21:25,280 --> 00:21:28,720
magic bounty ideas right you have a

00:21:27,280 --> 00:21:32,080
problem just put a bug

00:21:28,720 --> 00:21:33,679
on it and the

00:21:32,080 --> 00:21:36,080
entities will be had accountable but in

00:21:33,679 --> 00:21:38,240
reality because of the

00:21:36,080 --> 00:21:40,400
dynamics around the nda and some of the

00:21:38,240 --> 00:21:43,039
dynamics around the disclosure

00:21:40,400 --> 00:21:44,480
it isn't evident at all that more that

00:21:43,039 --> 00:21:45,760
this process creates

00:21:44,480 --> 00:21:47,679
large-scale transparency and

00:21:45,760 --> 00:21:49,440
accountability which i think gave us

00:21:47,679 --> 00:21:51,440
a little bit to think about and the sort

00:21:49,440 --> 00:21:52,960
of entire purpose of this exercise for

00:21:51,440 --> 00:21:55,200
us is really to understand

00:21:52,960 --> 00:21:56,400
what are the design considerations what

00:21:55,200 --> 00:21:59,039
are the different

00:21:56,400 --> 00:21:59,440
um levers we can play things we can

00:21:59,039 --> 00:22:02,240
learn

00:21:59,440 --> 00:22:04,960
so that we can shape participatory uh

00:22:02,240 --> 00:22:07,600
harms discovery space that makes sense

00:22:04,960 --> 00:22:08,480
um yeah i was gonna say yeah like your

00:22:07,600 --> 00:22:10,960
last point

00:22:08,480 --> 00:22:12,799
definitely uh in my mind i'm thinking of

00:22:10,960 --> 00:22:14,480
like the anti-examples in the

00:22:12,799 --> 00:22:16,480
algorithmic auditing space

00:22:14,480 --> 00:22:18,480
that we've talked about in terms of just

00:22:16,480 --> 00:22:20,000
um you know with algorithmic auditing

00:22:18,480 --> 00:22:21,760
especially as an external auditor is

00:22:20,000 --> 00:22:22,480
someone that's not an employee or not

00:22:21,760 --> 00:22:25,200
even a

00:22:22,480 --> 00:22:26,480
hired consultant by the company that

00:22:25,200 --> 00:22:29,280
that relationship can turn

00:22:26,480 --> 00:22:29,840
adversarial in a very important way i i

00:22:29,280 --> 00:22:31,280
thought something

00:22:29,840 --> 00:22:33,039
interesting too about the work you guys

00:22:31,280 --> 00:22:36,080
have done on bug bounties is

00:22:33,039 --> 00:22:38,480
that you know the um the

00:22:36,080 --> 00:22:39,360
at the beginning of you know the

00:22:38,480 --> 00:22:42,000
security

00:22:39,360 --> 00:22:42,400
like if you look historically um you

00:22:42,000 --> 00:22:44,320
know

00:22:42,400 --> 00:22:46,000
uh there was mention of this idea that

00:22:44,320 --> 00:22:48,080
you know security wasn't always

00:22:46,000 --> 00:22:49,440
embraced by industry their companies

00:22:48,080 --> 00:22:52,799
were not always

00:22:49,440 --> 00:22:53,679
you know as um cooperative in the setup

00:22:52,799 --> 00:22:55,520
of bounties

00:22:53,679 --> 00:22:57,600
in the infrastructure and in the support

00:22:55,520 --> 00:22:59,760
of like security professionals

00:22:57,600 --> 00:23:00,880
um so it's possible that maybe we're

00:22:59,760 --> 00:23:02,799
just in that early phase with

00:23:00,880 --> 00:23:04,799
algorithmic auditing but i do know that

00:23:02,799 --> 00:23:06,240
you know one of the distinct distinctive

00:23:04,799 --> 00:23:08,000
factors of

00:23:06,240 --> 00:23:09,919
algorithmic systems and in particular

00:23:08,000 --> 00:23:12,080
machine learning systems is that data

00:23:09,919 --> 00:23:14,159
requirement the fact that

00:23:12,080 --> 00:23:15,200
you know you you need to look at the

00:23:14,159 --> 00:23:17,039
data and access

00:23:15,200 --> 00:23:18,640
certain details about that you know that

00:23:17,039 --> 00:23:19,760
the information around the data or

00:23:18,640 --> 00:23:22,799
access to the model

00:23:19,760 --> 00:23:24,720
um in order to be able to audit it

00:23:22,799 --> 00:23:26,240
and that's something that i think in

00:23:24,720 --> 00:23:28,000
security sometimes with just like a

00:23:26,240 --> 00:23:31,039
consumer level

00:23:28,000 --> 00:23:32,159
um of access to you know the product or

00:23:31,039 --> 00:23:35,039
the the software

00:23:32,159 --> 00:23:36,480
um you can be able to discover a lot of

00:23:35,039 --> 00:23:39,120
different vulnerabilities

00:23:36,480 --> 00:23:40,640
whereas with uh algorithmic auditing

00:23:39,120 --> 00:23:42,159
it's this challenge of you actually

00:23:40,640 --> 00:23:44,000
having to have a certain amount of you

00:23:42,159 --> 00:23:46,159
know in-depth

00:23:44,000 --> 00:23:48,080
access to the model before you can

00:23:46,159 --> 00:23:49,360
actually be able to assess it or

00:23:48,080 --> 00:23:52,240
understand some of these

00:23:49,360 --> 00:23:53,679
issues such as bias for example so yeah

00:23:52,240 --> 00:23:56,240
it's this weird thing where you kind of

00:23:53,679 --> 00:23:58,559
need the cooperation of companies

00:23:56,240 --> 00:24:00,159
and they're super reluctant and also

00:23:58,559 --> 00:24:00,640
we've talked about this other idea as

00:24:00,159 --> 00:24:04,240
well of

00:24:00,640 --> 00:24:05,440
um you know people are really open to

00:24:04,240 --> 00:24:08,559
getting critique

00:24:05,440 --> 00:24:11,120
about their product or about uh

00:24:08,559 --> 00:24:11,840
you know uh you know a model they're

00:24:11,120 --> 00:24:13,600
developing or

00:24:11,840 --> 00:24:15,919
a piece of software they're developing

00:24:13,600 --> 00:24:19,120
as long as it doesn't like directly

00:24:15,919 --> 00:24:19,919
contradict the profit motive and there's

00:24:19,120 --> 00:24:22,000
moments

00:24:19,919 --> 00:24:23,279
i noticed in algorithmic auditing where

00:24:22,000 --> 00:24:27,120
um you know

00:24:23,279 --> 00:24:29,120
some of the uh the the challenges

00:24:27,120 --> 00:24:31,120
brought forth in terms of you know this

00:24:29,120 --> 00:24:32,720
algorithm is unethical or ineffective in

00:24:31,120 --> 00:24:36,480
this particular way

00:24:32,720 --> 00:24:39,679
uh um sort of interferes

00:24:36,480 --> 00:24:41,679
with you know the product timeline and

00:24:39,679 --> 00:24:43,360
uh companies become hostile as a result

00:24:41,679 --> 00:24:44,480
of that and and part of it too is that

00:24:43,360 --> 00:24:45,600
sometimes someone will build an

00:24:44,480 --> 00:24:47,760
algorithm

00:24:45,600 --> 00:24:49,600
you know to be applied on an affected

00:24:47,760 --> 00:24:50,159
population that isn't the user and this

00:24:49,600 --> 00:24:51,840
is something i

00:24:50,159 --> 00:24:53,600
like feel like i'm repeating myself a

00:24:51,840 --> 00:24:56,159
lot but you know they'll build the

00:24:53,600 --> 00:24:58,400
facial recognition tool for the police

00:24:56,159 --> 00:24:59,840
uh for police use and it'll affect you

00:24:58,400 --> 00:25:01,279
know a different community and the

00:24:59,840 --> 00:25:02,480
advocacy group might be representing

00:25:01,279 --> 00:25:04,799
that community

00:25:02,480 --> 00:25:05,840
um but the the company doesn't actually

00:25:04,799 --> 00:25:07,520
care because they're

00:25:05,840 --> 00:25:09,039
they're optimizing for you know what's

00:25:07,520 --> 00:25:11,840
best for the police officer

00:25:09,039 --> 00:25:12,400
so that that dynamic i think um uh is

00:25:11,840 --> 00:25:15,760
less

00:25:12,400 --> 00:25:18,080
common uh in sort of like traditional

00:25:15,760 --> 00:25:19,760
security software dynamics um and that

00:25:18,080 --> 00:25:21,520
makes it very challenging

00:25:19,760 --> 00:25:23,919
and i think that's something that we

00:25:21,520 --> 00:25:24,880
sort of like quickly understood right

00:25:23,919 --> 00:25:28,080
like that

00:25:24,880 --> 00:25:29,440
um you know to the extent that people

00:25:28,080 --> 00:25:31,520
think they can solve everything with

00:25:29,440 --> 00:25:33,039
magic machine learning right like oh

00:25:31,520 --> 00:25:34,880
you have a difficult society problem

00:25:33,039 --> 00:25:38,000
magic machine learning was all bad

00:25:34,880 --> 00:25:40,000
uh magic bug bounties isn't the way to

00:25:38,000 --> 00:25:40,320
solve magic machine learning problems

00:25:40,000 --> 00:25:43,120
yeah

00:25:40,320 --> 00:25:44,799
exactly one of our interviewees has this

00:25:43,120 --> 00:25:46,799
really fun way to put it at katie

00:25:44,799 --> 00:25:49,039
missouri's who's absolutely like

00:25:46,799 --> 00:25:50,559
extraordinarily smart and in in this

00:25:49,039 --> 00:25:52,240
area and has been very

00:25:50,559 --> 00:25:54,880
uh great and help us think through that

00:25:52,240 --> 00:25:57,360
she talks about the digestive systems

00:25:54,880 --> 00:25:58,720
that go beyond the bounties right so you

00:25:57,360 --> 00:26:01,600
have a buck bounty

00:25:58,720 --> 00:26:03,039
program great but then are you as an

00:26:01,600 --> 00:26:04,960
institution

00:26:03,039 --> 00:26:06,240
able to actually digest what's going to

00:26:04,960 --> 00:26:09,840
come out of it do you

00:26:06,240 --> 00:26:10,320
have sound uh you know processes in

00:26:09,840 --> 00:26:13,279
place

00:26:10,320 --> 00:26:15,279
that can take that into account at the

00:26:13,279 --> 00:26:17,679
you know at the level of the full life

00:26:15,279 --> 00:26:20,320
cycle of the product do you have

00:26:17,679 --> 00:26:22,080
a mature way to think about this issue

00:26:20,320 --> 00:26:24,720
that enables you to actually

00:26:22,080 --> 00:26:26,480
ingest what the bug bounty may produce

00:26:24,720 --> 00:26:28,640
and i think unfortunately when we look

00:26:26,480 --> 00:26:30,720
at the algorithmic harm space

00:26:28,640 --> 00:26:32,640
a lot of the answer to that question is

00:26:30,720 --> 00:26:34,400
not yet yeah and i wonder if we're going

00:26:32,640 --> 00:26:36,000
to need to help build the digestive

00:26:34,400 --> 00:26:38,799
tracts before

00:26:36,000 --> 00:26:40,240
before we can put bounties on it oh yeah

00:26:38,799 --> 00:26:41,600
yeah and i think this is like this goes

00:26:40,240 --> 00:26:44,000
back to your point around

00:26:41,600 --> 00:26:44,960
you know is it actually okay that right

00:26:44,000 --> 00:26:48,159
now

00:26:44,960 --> 00:26:51,679
we have this you know um twitter uh

00:26:48,159 --> 00:26:53,760
justice um uh sort of informal

00:26:51,679 --> 00:26:54,880
mechanism in place and i think it's okay

00:26:53,760 --> 00:26:57,440
to a certain extent

00:26:54,880 --> 00:26:58,799
but um you know the the things that will

00:26:57,440 --> 00:27:02,559
be promoted on twitter

00:26:58,799 --> 00:27:05,440
are are you know topics that um

00:27:02,559 --> 00:27:06,080
are of concern to you know a very

00:27:05,440 --> 00:27:07,360
specific

00:27:06,080 --> 00:27:09,679
population and it might not even

00:27:07,360 --> 00:27:10,799
represent the situations that cause the

00:27:09,679 --> 00:27:12,640
most harm so even

00:27:10,799 --> 00:27:14,320
you know an idea of how to prioritize

00:27:12,640 --> 00:27:16,320
and verify like those

00:27:14,320 --> 00:27:17,840
ideas that really provide that structure

00:27:16,320 --> 00:27:20,240
of like how do we

00:27:17,840 --> 00:27:21,200
go from you know someone being upset

00:27:20,240 --> 00:27:24,080
about something

00:27:21,200 --> 00:27:26,240
to actual accountability uh like that

00:27:24,080 --> 00:27:27,840
requires structure that requires like

00:27:26,240 --> 00:27:29,679
infrastructure that doesn't currently

00:27:27,840 --> 00:27:31,360
exist in order to make that pipeline

00:27:29,679 --> 00:27:33,919
clear and like that

00:27:31,360 --> 00:27:34,640
the digestive like we need intestines to

00:27:33,919 --> 00:27:37,600
digest

00:27:34,640 --> 00:27:38,640
use your analogy like we actually need

00:27:37,600 --> 00:27:40,640
like you know

00:27:38,640 --> 00:27:41,919
a structure in order for that to happen

00:27:40,640 --> 00:27:42,799
so i think that like that's definitely

00:27:41,919 --> 00:27:45,360
something that's

00:27:42,799 --> 00:27:46,799
truly lacking in the algorithmic space

00:27:45,360 --> 00:27:48,159
um is that that

00:27:46,799 --> 00:27:49,919
that lack of structure leads to

00:27:48,159 --> 00:27:50,880
situations where someone might raise an

00:27:49,919 --> 00:27:53,440
alarm

00:27:50,880 --> 00:27:54,399
and um just due to lack of present dress

00:27:53,440 --> 00:27:57,760
for example

00:27:54,399 --> 00:28:00,000
um nothing happens um or they might

00:27:57,760 --> 00:28:00,320
raise the alarm and it's this huge story

00:28:00,000 --> 00:28:02,159
but

00:28:00,320 --> 00:28:03,840
um because there's no actual solar like

00:28:02,159 --> 00:28:05,760
formal mechanisms of accountability the

00:28:03,840 --> 00:28:09,120
company can just ignore the story

00:28:05,760 --> 00:28:11,120
or spin the story and like let it go so

00:28:09,120 --> 00:28:12,320
yeah connecting you know going from you

00:28:11,120 --> 00:28:15,679
know i

00:28:12,320 --> 00:28:17,440
we know that this harm is there um uh

00:28:15,679 --> 00:28:18,799
and it's a valid and it's a valid harm

00:28:17,440 --> 00:28:21,760
and it's a prioritized harm

00:28:18,799 --> 00:28:23,279
um going from that to like actually you

00:28:21,760 --> 00:28:25,520
know compensating a victim

00:28:23,279 --> 00:28:28,159
or pulling something off of the market

00:28:25,520 --> 00:28:30,640
recalling a faulty a faulty product

00:28:28,159 --> 00:28:32,399
uh it's like a very mysterious process

00:28:30,640 --> 00:28:33,679
right now and i feel like more structure

00:28:32,399 --> 00:28:35,840
would actually lead to like a more

00:28:33,679 --> 00:28:36,799
reliable outcome of like accountability

00:28:35,840 --> 00:28:38,640
like you're mentioning

00:28:36,799 --> 00:28:40,320
yeah and i'm excited that we're looking

00:28:38,640 --> 00:28:41,840
into sort of the history of the book

00:28:40,320 --> 00:28:45,120
family space too because

00:28:41,840 --> 00:28:48,159
you know it it looks like but

00:28:45,120 --> 00:28:50,159
but there's you know the one of the

00:28:48,159 --> 00:28:51,360
a few of the things that we learned as

00:28:50,159 --> 00:28:53,279
we started looking into the

00:28:51,360 --> 00:28:54,960
bounty is like hey there is no such

00:28:53,279 --> 00:28:56,799
thing as a big bounty model

00:28:54,960 --> 00:28:58,080
right there's actually a lot of

00:28:56,799 --> 00:29:00,640
different ways

00:28:58,080 --> 00:29:01,679
to do this uh ways to host the book

00:29:00,640 --> 00:29:03,360
bounty program

00:29:01,679 --> 00:29:05,520
the platforms themselves are very

00:29:03,360 --> 00:29:06,720
different and so i think this exposed us

00:29:05,520 --> 00:29:08,320
to like

00:29:06,720 --> 00:29:10,799
a little bit of the choose your own

00:29:08,320 --> 00:29:10,799
adventure

00:29:10,840 --> 00:29:16,000
yeah yeah

00:29:13,279 --> 00:29:17,279
but we were surprised um you know

00:29:16,000 --> 00:29:19,919
honestly that there were not

00:29:17,279 --> 00:29:20,720
more adversarial models that had

00:29:19,919 --> 00:29:22,960
succeeded

00:29:20,720 --> 00:29:25,200
right like by and large it really is a

00:29:22,960 --> 00:29:26,000
space that as you said requires the

00:29:25,200 --> 00:29:28,399
institution

00:29:26,000 --> 00:29:32,080
to be bought in requires them to have

00:29:28,399 --> 00:29:33,600
the mature digestive tracts for it

00:29:32,080 --> 00:29:35,360
and i think it's fair to say that we're

00:29:33,600 --> 00:29:38,799
not there yet

00:29:35,360 --> 00:29:41,120
and yeah some of the companies honestly

00:29:38,799 --> 00:29:42,080
reactions in the face of these issues

00:29:41,120 --> 00:29:44,000
are concerned

00:29:42,080 --> 00:29:45,600
and doesn't suggest that they're emitted

00:29:44,000 --> 00:29:49,200
away from getting

00:29:45,600 --> 00:29:50,799
good digestive tracts on yeah yeah yeah

00:29:49,200 --> 00:29:52,320
i was going to say i think that that

00:29:50,799 --> 00:29:54,159
also says something about the nature of

00:29:52,320 --> 00:29:55,919
security problems right like security

00:29:54,159 --> 00:29:57,600
problems there's an alignment in terms

00:29:55,919 --> 00:30:00,080
of incentive there where

00:29:57,600 --> 00:30:02,559
a company wants you to find its

00:30:00,080 --> 00:30:04,559
vulnerability so that it can patch them

00:30:02,559 --> 00:30:06,080
um i don't well we don't we don't know

00:30:04,559 --> 00:30:07,919
this for sure but there's you know

00:30:06,080 --> 00:30:09,760
it's very rare that you know patching of

00:30:07,919 --> 00:30:12,080
vulnerability wouldn't be like

00:30:09,760 --> 00:30:14,000
an economically beneficial move for the

00:30:12,080 --> 00:30:16,240
company to make so that you know future

00:30:14,000 --> 00:30:18,159
hackers can access that information

00:30:16,240 --> 00:30:20,399
whereas um sometimes with algorithmic

00:30:18,159 --> 00:30:23,520
auditing or automatic accountability

00:30:20,399 --> 00:30:25,679
um uh you know addressing the issue

00:30:23,520 --> 00:30:26,960
requires uh overhaul of your data

00:30:25,679 --> 00:30:28,799
practice or

00:30:26,960 --> 00:30:30,640
it requires you know consultation with

00:30:28,799 --> 00:30:34,159
an affected community member

00:30:30,640 --> 00:30:36,720
or you know the the the ask

00:30:34,159 --> 00:30:37,840
some or it or you know it might require

00:30:36,720 --> 00:30:40,320
getting rid of the product

00:30:37,840 --> 00:30:41,600
completely to protect certain people so

00:30:40,320 --> 00:30:43,840
that the ask

00:30:41,600 --> 00:30:45,360
sometimes is something that people

00:30:43,840 --> 00:30:47,360
resist even if it's a

00:30:45,360 --> 00:30:48,720
like a documentation ask of like can you

00:30:47,360 --> 00:30:51,600
please write this down

00:30:48,720 --> 00:30:53,279
i've had like my early experiences in

00:30:51,600 --> 00:30:56,000
this field was just me

00:30:53,279 --> 00:30:57,840
you know um thinking that it wasn't that

00:30:56,000 --> 00:30:58,480
big a deal to just do this thing and

00:30:57,840 --> 00:31:00,480
then

00:30:58,480 --> 00:31:02,080
realizing the like amount of resistance

00:31:00,480 --> 00:31:02,399
coming from certain companies asked to

00:31:02,080 --> 00:31:04,799
do

00:31:02,399 --> 00:31:06,559
to do that thing so i think i think the

00:31:04,799 --> 00:31:08,399
the type of interventions that people

00:31:06,559 --> 00:31:11,440
are asking of companies

00:31:08,399 --> 00:31:13,279
in the security space um really

00:31:11,440 --> 00:31:14,480
changes in the algorithmic sort of

00:31:13,279 --> 00:31:17,600
auditing space

00:31:14,480 --> 00:31:18,159
where the dynamic shifts and uh uh

00:31:17,600 --> 00:31:19,760
there's

00:31:18,159 --> 00:31:21,519
so many other reasons there's so many

00:31:19,760 --> 00:31:23,279
reasons in my mind why they're hostile

00:31:21,519 --> 00:31:24,159
it's like a rational reaction from the

00:31:23,279 --> 00:31:26,080
company

00:31:24,159 --> 00:31:27,519
um which kind of like pushes these

00:31:26,080 --> 00:31:28,799
algorithmic auditors into this more like

00:31:27,519 --> 00:31:31,039
adversarial

00:31:28,799 --> 00:31:32,000
uh dynamic and position so i feel like

00:31:31,039 --> 00:31:34,240
yeah that is like a huge

00:31:32,000 --> 00:31:36,159
difference there yeah and to your point

00:31:34,240 --> 00:31:38,399
i think like one of the key

00:31:36,159 --> 00:31:41,039
makers of the difference is there is a

00:31:38,399 --> 00:31:45,360
mature regulatory framework i mean

00:31:41,039 --> 00:31:48,480
right like there is like

00:31:45,360 --> 00:31:49,200
there's a sense of regulation in that

00:31:48,480 --> 00:31:51,279
space

00:31:49,200 --> 00:31:53,679
yeah it's interesting because you know

00:31:51,279 --> 00:31:57,120
in in the work that we've done we also

00:31:53,679 --> 00:32:00,240
see cyber security struggle

00:31:57,120 --> 00:32:02,480
with expanding its banneries to

00:32:00,240 --> 00:32:04,159
better include socio-technical harms

00:32:02,480 --> 00:32:07,360
right and i think there's definitely

00:32:04,159 --> 00:32:10,080
a moment of reckoning around more

00:32:07,360 --> 00:32:11,279
privacy oriented cybersecurity harms and

00:32:10,080 --> 00:32:13,120
so for instance

00:32:11,279 --> 00:32:15,039
in our work we also saw some of the bug

00:32:13,120 --> 00:32:17,039
bounties adapt to that right like

00:32:15,039 --> 00:32:18,640
we're going to create a new type of

00:32:17,039 --> 00:32:19,360
submission that's focused on for

00:32:18,640 --> 00:32:22,000
instance

00:32:19,360 --> 00:32:23,360
api abuse which is a little bit of a

00:32:22,000 --> 00:32:24,320
shift in perspective but there's

00:32:23,360 --> 00:32:27,120
definitely

00:32:24,320 --> 00:32:28,240
sort of a struggle to expand at

00:32:27,120 --> 00:32:31,039
banneries some

00:32:28,240 --> 00:32:32,240
say that extension of battery is needed

00:32:31,039 --> 00:32:34,559
because some of these

00:32:32,240 --> 00:32:36,399
closer technical issues are cyber

00:32:34,559 --> 00:32:39,120
security harms and some say

00:32:36,399 --> 00:32:41,120
no they're too different from that space

00:32:39,120 --> 00:32:42,640
the tools and methods are too different

00:32:41,120 --> 00:32:43,919
yeah it's actually not helpful to

00:32:42,640 --> 00:32:45,679
stretch your boundaries so that's

00:32:43,919 --> 00:32:48,320
definitely something we see

00:32:45,679 --> 00:32:49,279
yeah yeah and again like you know

00:32:48,320 --> 00:32:50,960
thinking about

00:32:49,279 --> 00:32:52,559
you know who in the space is making

00:32:50,960 --> 00:32:54,640
these um

00:32:52,559 --> 00:32:56,159
is prioritizing these directions i think

00:32:54,640 --> 00:32:58,240
is really important where

00:32:56,159 --> 00:32:59,760
um like in the algorithmic auditing

00:32:58,240 --> 00:33:01,600
space you know there are

00:32:59,760 --> 00:33:03,919
there's like an engineering incentive to

00:33:01,600 --> 00:33:05,840
care about you know or to advocate for

00:33:03,919 --> 00:33:08,840
like oh if i can just like de-bias

00:33:05,840 --> 00:33:11,840
my model and that's the intervention

00:33:08,840 --> 00:33:11,840
required

00:33:14,159 --> 00:33:18,799
uh unbiased or deep biased data set that

00:33:16,960 --> 00:33:19,840
we have yet to find you know after years

00:33:18,799 --> 00:33:22,840
of searching

00:33:19,840 --> 00:33:25,440
uh a nice certification like a certified

00:33:22,840 --> 00:33:27,120
device

00:33:25,440 --> 00:33:28,480
but it's kind of like i think people are

00:33:27,120 --> 00:33:30,399
like engineers i find

00:33:28,480 --> 00:33:32,080
um and i'll include myself in this

00:33:30,399 --> 00:33:33,279
especially when i was first sort of in

00:33:32,080 --> 00:33:36,240
this space like

00:33:33,279 --> 00:33:37,919
you're eager to seek problems that have

00:33:36,240 --> 00:33:39,360
easy solutions and it's much more

00:33:37,919 --> 00:33:41,840
uncomfortable to

00:33:39,360 --> 00:33:43,919
realize the fact that oh you know this

00:33:41,840 --> 00:33:45,440
is not just a technical problem

00:33:43,919 --> 00:33:46,960
and i i really love that journey was

00:33:45,440 --> 00:33:48,480
going through this phase too of

00:33:46,960 --> 00:33:49,760
recognizing that this is not just a

00:33:48,480 --> 00:33:51,760
technical problem and there's other

00:33:49,760 --> 00:33:52,000
dynamics at play because the reality is

00:33:51,760 --> 00:33:55,120
of

00:33:52,000 --> 00:33:56,799
it is a lot of technology and

00:33:55,120 --> 00:33:58,399
especially how technology sort of

00:33:56,799 --> 00:34:01,519
intersects with like harms and

00:33:58,399 --> 00:34:03,840
harms discovery like it's tech hurts

00:34:01,519 --> 00:34:05,440
people because of factors outside of

00:34:03,840 --> 00:34:06,080
just the properties of the technology

00:34:05,440 --> 00:34:08,320
itself

00:34:06,080 --> 00:34:10,399
sometimes the technology fails and that

00:34:08,320 --> 00:34:13,200
causes a lot of

00:34:10,399 --> 00:34:13,760
of the hurt that people experience but

00:34:13,200 --> 00:34:16,960
there's

00:34:13,760 --> 00:34:19,679
so many other factors involved in

00:34:16,960 --> 00:34:21,200
that result outside of just the design

00:34:19,679 --> 00:34:22,240
of the technology or the properties of

00:34:21,200 --> 00:34:24,800
the technology

00:34:22,240 --> 00:34:25,760
and i find that you know um this is

00:34:24,800 --> 00:34:27,440
something that both

00:34:25,760 --> 00:34:29,119
both communities are going through right

00:34:27,440 --> 00:34:30,320
now i find it really cool that cyber

00:34:29,119 --> 00:34:32,480
security is going through it because

00:34:30,320 --> 00:34:34,240
there's a long phase of just

00:34:32,480 --> 00:34:35,760
we're looking for technical glitches and

00:34:34,240 --> 00:34:36,879
we're trying to find technical patches

00:34:35,760 --> 00:34:38,639
so the fact that

00:34:36,879 --> 00:34:40,720
you know that community has expanded its

00:34:38,639 --> 00:34:42,960
definition is like okay like algorithmic

00:34:40,720 --> 00:34:43,919
auditing definitely in a situation where

00:34:42,960 --> 00:34:45,440
i think the interesting thing about

00:34:43,919 --> 00:34:46,000
machine learning is like it involves

00:34:45,440 --> 00:34:48,960
data

00:34:46,000 --> 00:34:49,599
and data is just such a you know it

00:34:48,960 --> 00:34:51,200
requires

00:34:49,599 --> 00:34:53,040
it requires this inherent sort of like

00:34:51,200 --> 00:34:53,679
socio-technical consideration just

00:34:53,040 --> 00:34:57,200
because it's

00:34:53,679 --> 00:35:00,560
generated from humans and it's embodying

00:34:57,200 --> 00:35:03,760
and such a specific type of like

00:35:00,560 --> 00:35:05,200
you know culture and um very specific

00:35:03,760 --> 00:35:08,079
sort of like

00:35:05,200 --> 00:35:09,680
uh ambiguities so i feel like the fact

00:35:08,079 --> 00:35:12,320
that data is

00:35:09,680 --> 00:35:13,280
a reality of a lot of algorithmic

00:35:12,320 --> 00:35:15,119
systems

00:35:13,280 --> 00:35:17,200
uh makes that socio-technical

00:35:15,119 --> 00:35:21,119
consideration even more

00:35:17,200 --> 00:35:21,839
like necessary um so yeah i'm hoping

00:35:21,119 --> 00:35:23,440
like

00:35:21,839 --> 00:35:25,359
the algorithmic auditing space also sort

00:35:23,440 --> 00:35:26,720
of expands even its view beyond bias and

00:35:25,359 --> 00:35:28,480
really starts asking some of these

00:35:26,720 --> 00:35:31,119
questions that we're seeing

00:35:28,480 --> 00:35:32,400
um happen in the cyber security space

00:35:31,119 --> 00:35:32,960
where it's just like this is not just

00:35:32,400 --> 00:35:34,320
like a

00:35:32,960 --> 00:35:36,640
question of me asking you to do this

00:35:34,320 --> 00:35:37,839
technical patch like i actually need you

00:35:36,640 --> 00:35:39,680
to change your

00:35:37,839 --> 00:35:41,119
engineering process change your

00:35:39,680 --> 00:35:43,440
documentation like that's

00:35:41,119 --> 00:35:44,480
i don't know we'll see if they do it but

00:35:43,440 --> 00:35:46,880
like i do think that's

00:35:44,480 --> 00:35:48,160
that's the hope right a conversation

00:35:46,880 --> 00:35:49,280
that's happening and it's interesting

00:35:48,160 --> 00:35:51,200
because it's absolutely happening on

00:35:49,280 --> 00:35:52,560
privacy it's also happening on friends

00:35:51,200 --> 00:35:55,440
when i think about information

00:35:52,560 --> 00:35:56,240
operations definitely like add the super

00:35:55,440 --> 00:35:58,240
fascinating

00:35:56,240 --> 00:36:00,079
boundary of the space right something

00:35:58,240 --> 00:36:03,119
this is a cyber security issue

00:36:00,079 --> 00:36:05,920
some of this is not yeah yeah yeah

00:36:03,119 --> 00:36:07,920
they struggle with its own boundary and

00:36:05,920 --> 00:36:09,280
forced to reflect upon like okay how do

00:36:07,920 --> 00:36:09,920
we define the problem that we're

00:36:09,280 --> 00:36:12,880
tackling

00:36:09,920 --> 00:36:14,480
and what part of our tool set applies in

00:36:12,880 --> 00:36:15,920
reverse is like what needs a total

00:36:14,480 --> 00:36:18,400
reinvention it's great yeah

00:36:15,920 --> 00:36:18,960
and one of the most important like

00:36:18,400 --> 00:36:21,040
pieces

00:36:18,960 --> 00:36:22,400
that that we should also like highlight

00:36:21,040 --> 00:36:24,480
is how you've

00:36:22,400 --> 00:36:26,160
you know helped reframe this and like no

00:36:24,480 --> 00:36:28,880
we're not looking at

00:36:26,160 --> 00:36:29,359
bugs or biases or whatnot we're looking

00:36:28,880 --> 00:36:32,160
at

00:36:29,359 --> 00:36:32,400
harms right like the framing is a harm

00:36:32,160 --> 00:36:36,400
and

00:36:32,400 --> 00:36:39,760
this will help you oh yeah really

00:36:36,400 --> 00:36:41,119
yeah yeah yeah no and i actually i went

00:36:39,760 --> 00:36:42,960
through this whole phase

00:36:41,119 --> 00:36:44,079
um i don't even know if i told you the

00:36:42,960 --> 00:36:46,160
story but i went through this whole

00:36:44,079 --> 00:36:48,960
phase of like when i first

00:36:46,160 --> 00:36:49,520
like first worked on my first paper even

00:36:48,960 --> 00:36:53,040
um

00:36:49,520 --> 00:36:55,520
uh i really thought of bias

00:36:53,040 --> 00:36:58,000
as a bug like i thought of like oh this

00:36:55,520 --> 00:37:00,720
is like a technical glitch like this is

00:36:58,000 --> 00:37:02,079
a situation where and i do think like

00:37:00,720 --> 00:37:04,800
there are a lot of bugs

00:37:02,079 --> 00:37:06,400
in modern algorithmic tools like um you

00:37:04,800 --> 00:37:08,240
know there are a lot of ways in which we

00:37:06,400 --> 00:37:10,720
construct these systems

00:37:08,240 --> 00:37:11,920
and they fail because they're not well

00:37:10,720 --> 00:37:13,599
constructed

00:37:11,920 --> 00:37:16,720
and you know there's problems in how

00:37:13,599 --> 00:37:18,960
it's constructed um and i'm like very

00:37:16,720 --> 00:37:19,839
honest about the reality of that but i

00:37:18,960 --> 00:37:23,599
do think that

00:37:19,839 --> 00:37:27,040
um you know sometimes uh

00:37:23,599 --> 00:37:27,599
it you know it really is like to address

00:37:27,040 --> 00:37:29,920
bias

00:37:27,599 --> 00:37:32,320
requires like a redefinition of like

00:37:29,920 --> 00:37:34,320
what it means for the system to work

00:37:32,320 --> 00:37:35,440
um or it's a definition or it's like a

00:37:34,320 --> 00:37:37,760
you know byproduct

00:37:35,440 --> 00:37:39,839
of a situation where people were not

00:37:37,760 --> 00:37:42,160
reflecting on like the history of

00:37:39,839 --> 00:37:43,839
you know a particular context or how

00:37:42,160 --> 00:37:44,640
it's going to be used or how it can be

00:37:43,839 --> 00:37:46,960
weaponized

00:37:44,640 --> 00:37:47,680
and those kind of considerations of like

00:37:46,960 --> 00:37:50,240
how like

00:37:47,680 --> 00:37:50,960
injustice can result from an algorithm

00:37:50,240 --> 00:37:52,880
that has

00:37:50,960 --> 00:37:54,800
you know that's separate a separate

00:37:52,880 --> 00:37:56,720
conversation from like

00:37:54,800 --> 00:37:57,839
the quality of the construction of the

00:37:56,720 --> 00:38:00,640
algorithm

00:37:57,839 --> 00:38:02,320
like that really yeah that was a really

00:38:00,640 --> 00:38:03,680
big sort of like shift in my mindset

00:38:02,320 --> 00:38:05,040
where i was like oh like there's a lot

00:38:03,680 --> 00:38:06,960
like

00:38:05,040 --> 00:38:08,880
modern day algorithms are very buggy

00:38:06,960 --> 00:38:10,320
systems i will be the first person to

00:38:08,880 --> 00:38:12,400
know like there's a lot of things that

00:38:10,320 --> 00:38:15,599
are poorly constructed about them

00:38:12,400 --> 00:38:17,760
um uh but they also um

00:38:15,599 --> 00:38:20,079
you know there's also a lot about you

00:38:17,760 --> 00:38:21,119
know how they're used and how they're

00:38:20,079 --> 00:38:22,800
built

00:38:21,119 --> 00:38:23,680
that's almost like the separate question

00:38:22,800 --> 00:38:25,520
like you mentioned this more

00:38:23,680 --> 00:38:27,599
socio-technical question

00:38:25,520 --> 00:38:29,680
um so yeah i i definitely think that

00:38:27,599 --> 00:38:31,520
that's something that the

00:38:29,680 --> 00:38:33,760
the machine learning community kind of

00:38:31,520 --> 00:38:35,280
has gone through this like

00:38:33,760 --> 00:38:36,960
phase of like discovering this and

00:38:35,280 --> 00:38:38,079
realizing this and i'll put myself in

00:38:36,960 --> 00:38:39,280
there because i definitely went through

00:38:38,079 --> 00:38:42,079
that phase of like

00:38:39,280 --> 00:38:44,320
realizing that um you know there is a

00:38:42,079 --> 00:38:47,599
sense of debugging required but it's not

00:38:44,320 --> 00:38:50,800
just that there's a lot more to

00:38:47,599 --> 00:38:51,359
like communicate with a harms discovery

00:38:50,800 --> 00:38:53,119
report

00:38:51,359 --> 00:38:54,720
or there's a lot more to communicate or

00:38:53,119 --> 00:38:56,160
to address when you're trying to

00:38:54,720 --> 00:38:59,280
mitigate harms

00:38:56,160 --> 00:39:00,000
um so yeah and i'm you know from what

00:38:59,280 --> 00:39:02,320
i've read of like

00:39:00,000 --> 00:39:04,000
the the shift in terms of cyber security

00:39:02,320 --> 00:39:05,520
um you know part of that like

00:39:04,000 --> 00:39:06,480
socio-technical like it's part of an

00:39:05,520 --> 00:39:09,119
openness to

00:39:06,480 --> 00:39:10,800
the socio-technical approach comes from

00:39:09,119 --> 00:39:12,320
the realization that

00:39:10,800 --> 00:39:14,160
you know you have harms coming from all

00:39:12,320 --> 00:39:17,359
kinds of sources

00:39:14,160 --> 00:39:19,520
not just the faulty tech source

00:39:17,359 --> 00:39:21,359
i will say it's a slow slow process

00:39:19,520 --> 00:39:23,440
right

00:39:21,359 --> 00:39:24,640
oh yeah and like for sure i don't think

00:39:23,440 --> 00:39:26,800
it's that fast

00:39:24,640 --> 00:39:28,079
on this side either i think something

00:39:26,800 --> 00:39:29,599
that i appreciate in

00:39:28,079 --> 00:39:32,400
the work that we're doing together with

00:39:29,599 --> 00:39:33,520
hl is also sort of the interdisciplinary

00:39:32,400 --> 00:39:36,880
lens and yeah

00:39:33,520 --> 00:39:39,119
you know um sasha who's also part of

00:39:36,880 --> 00:39:40,560
of this project like in their book

00:39:39,119 --> 00:39:42,640
design justice

00:39:40,560 --> 00:39:44,320
i think also puts on the table a lot of

00:39:42,640 --> 00:39:46,880
things that should be

00:39:44,320 --> 00:39:49,359
integrated in the way we do adversarial

00:39:46,880 --> 00:39:51,280
security research right because

00:39:49,359 --> 00:39:53,680
when we think about oh how is the system

00:39:51,280 --> 00:39:56,400
going to be abused how will

00:39:53,680 --> 00:39:57,760
how will it create harm there is also

00:39:56,400 --> 00:40:00,800
some form of like

00:39:57,760 --> 00:40:02,640
secret design thinking that goes into

00:40:00,800 --> 00:40:03,680
how we test for these adversarial

00:40:02,640 --> 00:40:07,760
hypotheses

00:40:03,680 --> 00:40:10,880
that often is um self-centered

00:40:07,760 --> 00:40:13,440
ignores community harms and leads to

00:40:10,880 --> 00:40:14,800
bad security so i'm also excited to

00:40:13,440 --> 00:40:18,240
bring sasha's

00:40:14,800 --> 00:40:21,520
design justice ideas into rethinking

00:40:18,240 --> 00:40:23,839
uh how really we do adversarial testing

00:40:21,520 --> 00:40:26,400
because i think that too uh tends to

00:40:23,839 --> 00:40:28,800
often be very misguided

00:40:26,400 --> 00:40:29,680
yeah and that's like another thing i

00:40:28,800 --> 00:40:32,160
love about

00:40:29,680 --> 00:40:33,680
um sort of our agl collaboration as well

00:40:32,160 --> 00:40:34,960
is that interdisciplinary

00:40:33,680 --> 00:40:36,880
element but also this very sort of

00:40:34,960 --> 00:40:40,160
community oriented community focused

00:40:36,880 --> 00:40:40,720
element i've sort of been in spaces

00:40:40,160 --> 00:40:43,520
where

00:40:40,720 --> 00:40:44,079
um you know corporate actors are the

00:40:43,520 --> 00:40:46,319
more

00:40:44,079 --> 00:40:47,359
the loudest voices in the room and

00:40:46,319 --> 00:40:49,359
really define

00:40:47,359 --> 00:40:50,960
a lot of things about the process that

00:40:49,359 --> 00:40:53,200
we end up um

00:40:50,960 --> 00:40:55,359
going along with and the harms are sort

00:40:53,200 --> 00:40:57,040
of described in this like defensive way

00:40:55,359 --> 00:40:58,560
like they'll talk about like the three

00:40:57,040 --> 00:41:00,720
lines of defense and

00:40:58,560 --> 00:41:02,640
protecting the company from liability

00:41:00,720 --> 00:41:04,560
and quality control of their products so

00:41:02,640 --> 00:41:08,000
that it gets widely adopted

00:41:04,560 --> 00:41:09,680
and and those are like legitimate

00:41:08,000 --> 00:41:11,119
priorities for a company or for a

00:41:09,680 --> 00:41:13,200
company representative to have

00:41:11,119 --> 00:41:14,640
but when you start thinking about harms

00:41:13,200 --> 00:41:16,240
from the perspective of those that are

00:41:14,640 --> 00:41:17,760
impacted from the perspective of

00:41:16,240 --> 00:41:19,920
community members and you start

00:41:17,760 --> 00:41:21,520
really trying to figure out like what

00:41:19,920 --> 00:41:22,960
are the tools that community members can

00:41:21,520 --> 00:41:24,960
use to better understand

00:41:22,960 --> 00:41:26,720
the harms that impact their lives and

00:41:24,960 --> 00:41:29,599
you know work to

00:41:26,720 --> 00:41:31,359
report those harms um the conversation

00:41:29,599 --> 00:41:34,000
and the question kind of just like

00:41:31,359 --> 00:41:35,920
shifts completely and you know a whole

00:41:34,000 --> 00:41:38,400
set of strategies that

00:41:35,920 --> 00:41:40,319
um would feel incompatible with this

00:41:38,400 --> 00:41:41,760
like three lines of defense model that's

00:41:40,319 --> 00:41:44,960
really popular in

00:41:41,760 --> 00:41:46,480
the corporate world um uh

00:41:44,960 --> 00:41:48,240
there's a lot more options or there's a

00:41:46,480 --> 00:41:49,520
lot more interesting sort of like

00:41:48,240 --> 00:41:51,760
directions to play

00:41:49,520 --> 00:41:52,880
to to consider so another like for

00:41:51,760 --> 00:41:54,400
example like

00:41:52,880 --> 00:41:56,000
you know looking at things from a

00:41:54,400 --> 00:41:57,280
community lens me and sasha are

00:41:56,000 --> 00:41:58,000
realizing there's much more of an

00:41:57,280 --> 00:42:00,640
imperative

00:41:58,000 --> 00:42:02,079
with respect to disclosure and education

00:42:00,640 --> 00:42:03,680
whereas like

00:42:02,079 --> 00:42:05,359
companies when they're thinking about

00:42:03,680 --> 00:42:06,720
identifying harms you know for their

00:42:05,359 --> 00:42:08,480
purposes

00:42:06,720 --> 00:42:10,240
they're they don't want to be in a

00:42:08,480 --> 00:42:12,079
position where they're opening access of

00:42:10,240 --> 00:42:13,280
their model for other people

00:42:12,079 --> 00:42:15,520
or they're just they're over there

00:42:13,280 --> 00:42:18,000
disclosing details about you know

00:42:15,520 --> 00:42:19,680
what tools they're using or how you know

00:42:18,000 --> 00:42:21,440
the the specific engineering details of

00:42:19,680 --> 00:42:24,640
how something is constructed

00:42:21,440 --> 00:42:27,760
um and they're not also investing in

00:42:24,640 --> 00:42:30,240
you know educating uh users uh

00:42:27,760 --> 00:42:31,280
to the you know to a degree beyond what

00:42:30,240 --> 00:42:33,040
would be like

00:42:31,280 --> 00:42:35,359
profitable for them yeah whereas you

00:42:33,040 --> 00:42:37,200
know if you look at harm's discovery or

00:42:35,359 --> 00:42:37,920
even this conversation of algorithmic

00:42:37,200 --> 00:42:39,520
harm

00:42:37,920 --> 00:42:41,359
from like a community lens or like

00:42:39,520 --> 00:42:42,480
really anchoring or prioritizing that

00:42:41,359 --> 00:42:44,720
community lens

00:42:42,480 --> 00:42:45,680
then it's like education is like super

00:42:44,720 --> 00:42:47,440
important

00:42:45,680 --> 00:42:49,359
it's incredibly important to make sure

00:42:47,440 --> 00:42:52,079
that um the public

00:42:49,359 --> 00:42:54,240
understands you know what potential

00:42:52,079 --> 00:42:55,839
harms could exist or what potential

00:42:54,240 --> 00:42:58,560
applications could

00:42:55,839 --> 00:42:59,359
um could could affect them so yeah it

00:42:58,560 --> 00:43:01,200
was this whole

00:42:59,359 --> 00:43:02,720
interesting experience too and that's

00:43:01,200 --> 00:43:03,200
something that her her book is really

00:43:02,720 --> 00:43:05,440
great

00:43:03,200 --> 00:43:06,960
about sort of outlining this need to

00:43:05,440 --> 00:43:09,040
really anchor and prioritize the

00:43:06,960 --> 00:43:11,119
affected population's perspective

00:43:09,040 --> 00:43:12,880
yeah something that's also quite

00:43:11,119 --> 00:43:15,359
fascinating to see

00:43:12,880 --> 00:43:16,880
is how some of the cyber security

00:43:15,359 --> 00:43:19,359
practices are getting

00:43:16,880 --> 00:43:20,560
important imported and sort of like

00:43:19,359 --> 00:43:22,319
reapplied

00:43:20,560 --> 00:43:24,000
and perhaps they work but for other

00:43:22,319 --> 00:43:25,119
reasons so like i was really interested

00:43:24,000 --> 00:43:27,200
when you shared

00:43:25,119 --> 00:43:29,200
that in releasing gender shades the

00:43:27,200 --> 00:43:30,640
paper you also thought about

00:43:29,200 --> 00:43:32,319
the you know sort of responsible

00:43:30,640 --> 00:43:35,440
disclosure model which

00:43:32,319 --> 00:43:37,280
i think here applies but for other

00:43:35,440 --> 00:43:38,880
different reasons that it does in

00:43:37,280 --> 00:43:40,079
infosec so if you want to share that

00:43:38,880 --> 00:43:42,880
story i think it's

00:43:40,079 --> 00:43:44,319
oh yeah pretty sure um yeah and that was

00:43:42,880 --> 00:43:45,680
something we outlined in our actionable

00:43:44,319 --> 00:43:48,000
auditing paper where

00:43:45,680 --> 00:43:48,960
um there's sort of like you know

00:43:48,000 --> 00:43:51,520
coordinated

00:43:48,960 --> 00:43:52,880
vulnerability disclosure um that which

00:43:51,520 --> 00:43:55,119
is sort of the procedure in cyber

00:43:52,880 --> 00:43:55,680
security to make sure that when you

00:43:55,119 --> 00:43:58,319
release

00:43:55,680 --> 00:43:59,599
information about you know software

00:43:58,319 --> 00:44:01,760
vulnerability

00:43:59,599 --> 00:44:03,359
um you're doing so responsibly so you

00:44:01,760 --> 00:44:06,560
inform the company

00:44:03,359 --> 00:44:08,160
um and then you know you uh give the

00:44:06,560 --> 00:44:10,079
company a deadline to respond

00:44:08,160 --> 00:44:11,760
and then you include the company's

00:44:10,079 --> 00:44:12,720
response and the communications with the

00:44:11,760 --> 00:44:14,480
public

00:44:12,720 --> 00:44:16,480
you know there's there's so many

00:44:14,480 --> 00:44:17,839
interesting reflections on like how do

00:44:16,480 --> 00:44:20,240
we actually

00:44:17,839 --> 00:44:21,920
uh you know release this information to

00:44:20,240 --> 00:44:22,240
the company and to the public in a way

00:44:21,920 --> 00:44:24,960
that

00:44:22,240 --> 00:44:25,839
um ensures safety and also you know

00:44:24,960 --> 00:44:29,200
protects

00:44:25,839 --> 00:44:31,200
the public and um we literally you know

00:44:29,200 --> 00:44:33,200
adapted this process and called it the

00:44:31,200 --> 00:44:34,640
coordinated bias disclosure process

00:44:33,200 --> 00:44:36,480
where we really were like

00:44:34,640 --> 00:44:38,240
this is great this is exactly what we

00:44:36,480 --> 00:44:40,000
were looking for one of the challenges

00:44:38,240 --> 00:44:40,400
with a lot of past algorithmic audits

00:44:40,000 --> 00:44:43,599
where

00:44:40,400 --> 00:44:45,599
researchers were really worried to uh

00:44:43,599 --> 00:44:47,040
name companies that they were auditing

00:44:45,599 --> 00:44:50,319
um a lot of the

00:44:47,040 --> 00:44:53,440
hci audit papers that were sort of

00:44:50,319 --> 00:44:55,680
uh released before uh gender shades

00:44:53,440 --> 00:44:57,040
um were reluctant to name companies

00:44:55,680 --> 00:44:58,480
because they were worried of a hostile

00:44:57,040 --> 00:45:01,200
corporate reaction

00:44:58,480 --> 00:45:02,560
um so uh gender shape it's right to be

00:45:01,200 --> 00:45:04,560
worried

00:45:02,560 --> 00:45:06,400
yeah figures they were right they were

00:45:04,560 --> 00:45:09,599
you know as as we know

00:45:06,400 --> 00:45:11,119
they were fully wrecked to be working um

00:45:09,599 --> 00:45:12,640
uh but it was just i was just really

00:45:11,119 --> 00:45:14,319
interesting because with with um

00:45:12,640 --> 00:45:16,240
with yeah with gender shades that was

00:45:14,319 --> 00:45:18,319
something that um joy had really thought

00:45:16,240 --> 00:45:20,240
about and

00:45:18,319 --> 00:45:21,680
with the coordinated bias disclosure we

00:45:20,240 --> 00:45:23,200
were able to really

00:45:21,680 --> 00:45:24,800
kind of do our due diligence where the

00:45:23,200 --> 00:45:26,880
companies knew it was coming

00:45:24,800 --> 00:45:28,240
and we could include their response and

00:45:26,880 --> 00:45:29,040
it was just it was just a much more

00:45:28,240 --> 00:45:31,040
organized

00:45:29,040 --> 00:45:32,640
process around doing it and i think

00:45:31,040 --> 00:45:35,599
that's most of what

00:45:32,640 --> 00:45:36,800
um uh we hope to poach from cyber

00:45:35,599 --> 00:45:38,800
security

00:45:36,800 --> 00:45:40,400
is like there's there's a maturity to

00:45:38,800 --> 00:45:42,079
the field of cyber security where

00:45:40,400 --> 00:45:43,599
there's more structure and it's just a

00:45:42,079 --> 00:45:44,079
little bit more organized around some of

00:45:43,599 --> 00:45:46,880
these

00:45:44,079 --> 00:45:47,920
um these processes that kind of happen

00:45:46,880 --> 00:45:49,839
haphazardly

00:45:47,920 --> 00:45:52,160
in the algorithmic auditing space right

00:45:49,839 --> 00:45:53,680
now we've been wondering a lot about the

00:45:52,160 --> 00:45:55,359
coordinated vulnerability disclosure

00:45:53,680 --> 00:45:57,280
process because i think in cyber

00:45:55,359 --> 00:45:58,240
security one of the main reason it makes

00:45:57,280 --> 00:46:00,880
sense is

00:45:58,240 --> 00:46:02,720
you're gonna you're gonna uh disclose a

00:46:00,880 --> 00:46:05,200
bug that others can use

00:46:02,720 --> 00:46:05,920
and so you do need to protect the users

00:46:05,200 --> 00:46:09,040
yeah

00:46:05,920 --> 00:46:11,520
and it's not necessarily the case in

00:46:09,040 --> 00:46:14,079
um algorithmic harm's discovery oh yeah

00:46:11,520 --> 00:46:16,640
yeah say sometimes i wonder if

00:46:14,079 --> 00:46:17,599
importing those practices from cyber

00:46:16,640 --> 00:46:20,720
security

00:46:17,599 --> 00:46:22,480
doesn't give too much leeway frankly to

00:46:20,720 --> 00:46:24,480
the corporation and industries that are

00:46:22,480 --> 00:46:26,319
being studied right because they don't

00:46:24,480 --> 00:46:28,720
there are different imperatives and some

00:46:26,319 --> 00:46:30,560
of these like oh yeah

00:46:28,720 --> 00:46:32,480
so some of the differences that we

00:46:30,560 --> 00:46:34,720
talked about between like algorithmic

00:46:32,480 --> 00:46:36,480
harms and cyber security bug including

00:46:34,720 --> 00:46:36,960
the fact that a cyber security bug can

00:46:36,480 --> 00:46:40,079
be

00:46:36,960 --> 00:46:41,040
exploited by others and can be sold on a

00:46:40,079 --> 00:46:43,119
market for

00:46:41,040 --> 00:46:44,400
cyber security bugs don't really apply

00:46:43,119 --> 00:46:46,240
to that space and so

00:46:44,400 --> 00:46:48,560
yeah um it's been really interesting to

00:46:46,240 --> 00:46:50,400
keep these differences in mind and to be

00:46:48,560 --> 00:46:51,760
thoughtful about like what do we what do

00:46:50,400 --> 00:46:53,359
we import and what

00:46:51,760 --> 00:46:55,040
what are we important with what we're

00:46:53,359 --> 00:46:57,839
importing yeah

00:46:55,040 --> 00:46:58,880
yeah and i think like so like i think

00:46:57,839 --> 00:47:00,880
that's incredibly

00:46:58,880 --> 00:47:02,800
true in the sense of like the rationale

00:47:00,880 --> 00:47:03,920
behind why it makes sense in cyber

00:47:02,800 --> 00:47:06,400
security

00:47:03,920 --> 00:47:08,400
um differs from the rationale of you

00:47:06,400 --> 00:47:09,680
know what elements are taken into the

00:47:08,400 --> 00:47:11,520
bias disclosure

00:47:09,680 --> 00:47:13,200
sort of process i think i think there's

00:47:11,520 --> 00:47:14,960
an interesting

00:47:13,200 --> 00:47:16,560
uh there's like interesting points of

00:47:14,960 --> 00:47:18,720
alignment in the sense of

00:47:16,560 --> 00:47:20,079
part of why cyber security also doesn't

00:47:18,720 --> 00:47:21,599
want to release it

00:47:20,079 --> 00:47:23,119
or first of the public is to give the

00:47:21,599 --> 00:47:25,280
companies a chance to respond so that

00:47:23,119 --> 00:47:27,680
the companies don't you know sue them or

00:47:25,280 --> 00:47:28,960
accuse them of being hackers or legally

00:47:27,680 --> 00:47:31,760
pursue them

00:47:28,960 --> 00:47:33,520
um um you know for releasing that uh

00:47:31,760 --> 00:47:34,880
vulnerability publicly without

00:47:33,520 --> 00:47:36,559
communicating it to them first so the

00:47:34,880 --> 00:47:37,599
stakes are quite different as well in

00:47:36,559 --> 00:47:40,240
the sense of like that

00:47:37,599 --> 00:47:41,520
those are quite high stakes um so yeah i

00:47:40,240 --> 00:47:42,880
think i think there's there's moments

00:47:41,520 --> 00:47:44,000
where that difference becomes like

00:47:42,880 --> 00:47:46,240
incredibly clear

00:47:44,000 --> 00:47:48,240
and um it becomes very tricky to figure

00:47:46,240 --> 00:47:50,880
out what you take and what you leave

00:47:48,240 --> 00:47:52,000
another thing too is like a lot of um

00:47:50,880 --> 00:47:55,920
and we don't talk a lot about

00:47:52,000 --> 00:47:57,920
this but you know cfa and other sort of

00:47:55,920 --> 00:47:59,680
legal structures put in place

00:47:57,920 --> 00:48:02,400
anti-hacking laws

00:47:59,680 --> 00:48:03,599
um uh interfere with algorithmic

00:48:02,400 --> 00:48:06,240
auditing in the sense that

00:48:03,599 --> 00:48:06,640
um it you know makes it difficult to you

00:48:06,240 --> 00:48:08,319
know

00:48:06,640 --> 00:48:09,839
scrape the data that you need in order

00:48:08,319 --> 00:48:12,400
to analyze a system

00:48:09,839 --> 00:48:13,760
to to conduct a particular audit and

00:48:12,400 --> 00:48:17,280
that's like a tension that

00:48:13,760 --> 00:48:19,920
um is becoming increasingly uh

00:48:17,280 --> 00:48:21,280
clear where sometimes there's alignment

00:48:19,920 --> 00:48:21,920
in the sense of you know what makes it

00:48:21,280 --> 00:48:24,960
easier

00:48:21,920 --> 00:48:25,359
to sort of like hack and figure out you

00:48:24,960 --> 00:48:27,680
know

00:48:25,359 --> 00:48:28,800
security vulnerabilities also is

00:48:27,680 --> 00:48:31,200
required for

00:48:28,800 --> 00:48:32,319
an algorithmic audit to even happen um

00:48:31,200 --> 00:48:33,760
so there's just so many

00:48:32,319 --> 00:48:35,839
moments of intersection where it's just

00:48:33,760 --> 00:48:38,400
like oh these are very different

00:48:35,839 --> 00:48:39,119
problems but clearly points of alignment

00:48:38,400 --> 00:48:41,280
and

00:48:39,119 --> 00:48:42,880
specific causes that i think both

00:48:41,280 --> 00:48:45,599
communities would probably

00:48:42,880 --> 00:48:46,480
advocate for they're different and yet

00:48:45,599 --> 00:48:48,480
they're similar

00:48:46,480 --> 00:48:50,240
in such striking ways because i think

00:48:48,480 --> 00:48:53,280
that adopting this language

00:48:50,240 --> 00:48:55,359
to companies who are you know familiar

00:48:53,280 --> 00:48:55,680
with these processes is also a way to

00:48:55,359 --> 00:48:58,160
say

00:48:55,680 --> 00:49:00,480
hey we expect you to be responsible in

00:48:58,160 --> 00:49:02,640
the face of that disclosure right

00:49:00,480 --> 00:49:02,640
like

00:49:09,839 --> 00:49:13,599
how do we create the safe harbors for

00:49:11,760 --> 00:49:14,800
researchers who do that is a really

00:49:13,599 --> 00:49:17,119
important one like

00:49:14,800 --> 00:49:19,280
what models do we have to create these

00:49:17,119 --> 00:49:21,680
safe harbors for researchers

00:49:19,280 --> 00:49:23,760
yeah can we find them in the cyber

00:49:21,680 --> 00:49:25,040
security space perhaps can we find them

00:49:23,760 --> 00:49:26,720
elsewhere perhaps but

00:49:25,040 --> 00:49:28,160
i think that the things we're looking

00:49:26,720 --> 00:49:30,720
for are the same right like

00:49:28,160 --> 00:49:32,880
let's harbor the researchers and let's

00:49:30,720 --> 00:49:35,200
create the expectations that

00:49:32,880 --> 00:49:36,640
uh industry needs to behave in the face

00:49:35,200 --> 00:49:39,680
of these disclosure

00:49:36,640 --> 00:49:41,839
and to be um cooperative

00:49:39,680 --> 00:49:43,599
and transparent and accountable in the

00:49:41,839 --> 00:49:45,440
face of these disclosures

00:49:43,599 --> 00:49:46,960
yeah and we've we've talked about this

00:49:45,440 --> 00:49:48,640
but you know another unfortunate

00:49:46,960 --> 00:49:50,240
similarity between you know cyber

00:49:48,640 --> 00:49:52,480
security and sort of the early

00:49:50,240 --> 00:49:54,640
um you know fairness across auditing

00:49:52,480 --> 00:49:56,079
space is that like lack of diversity and

00:49:54,640 --> 00:49:58,640
representation

00:49:56,079 --> 00:49:59,359
it makes it it makes it very difficult

00:49:58,640 --> 00:50:01,839
for

00:49:59,359 --> 00:50:03,839
um you know members of the affected or

00:50:01,839 --> 00:50:06,319
vulnerable populations to really

00:50:03,839 --> 00:50:07,760
be able to participate actively and i

00:50:06,319 --> 00:50:09,119
think that's like one of the things that

00:50:07,760 --> 00:50:10,480
we're trying to

00:50:09,119 --> 00:50:12,720
you know in the case of algorithmic

00:50:10,480 --> 00:50:15,280
auditing because of the data

00:50:12,720 --> 00:50:17,119
element of things uh you know the

00:50:15,280 --> 00:50:19,440
connection to that affected population

00:50:17,119 --> 00:50:21,280
is even more direct in the sense of

00:50:19,440 --> 00:50:22,480
those that are affected by the system

00:50:21,280 --> 00:50:24,640
often you know

00:50:22,480 --> 00:50:26,640
have artifacts or representations of

00:50:24,640 --> 00:50:28,960
themselves embedded in that system

00:50:26,640 --> 00:50:30,079
uh in a way that you know increases the

00:50:28,960 --> 00:50:31,839
stakes of

00:50:30,079 --> 00:50:33,520
of how they respond or how they react to

00:50:31,839 --> 00:50:35,119
the situation so

00:50:33,520 --> 00:50:36,240
yeah that's making it very interesting

00:50:35,119 --> 00:50:37,680
in the sense of how do you actually

00:50:36,240 --> 00:50:41,520
create a space

00:50:37,680 --> 00:50:42,640
um that is you know able to sort of

00:50:41,520 --> 00:50:46,079
accommodate

00:50:42,640 --> 00:50:47,520
um or even like uh allow for the

00:50:46,079 --> 00:50:48,400
participation of that affected

00:50:47,520 --> 00:50:49,680
population

00:50:48,400 --> 00:50:51,520
so that when you're looking at the

00:50:49,680 --> 00:50:53,440
problem you actually have this

00:50:51,520 --> 00:50:54,960
incredibly community informed view or

00:50:53,440 --> 00:50:56,240
community-informed ones

00:50:54,960 --> 00:50:58,720
um and i think that's something that

00:50:56,240 --> 00:51:01,440
will be very very tricky to figure out

00:50:58,720 --> 00:51:03,119
in both communities and you know there

00:51:01,440 --> 00:51:05,119
will be some resistance there'll be some

00:51:03,119 --> 00:51:06,960
cultural resistance to that shift as

00:51:05,119 --> 00:51:08,880
well and that's something on which

00:51:06,960 --> 00:51:10,160
the cyber security field and closet

00:51:08,880 --> 00:51:13,280
field in general has been

00:51:10,160 --> 00:51:14,960
bad at right like excluding entire

00:51:13,280 --> 00:51:17,119
categories of users from their

00:51:14,960 --> 00:51:20,160
conception of harm that explains like

00:51:17,119 --> 00:51:20,960
why a stalker ware for instance is

00:51:20,160 --> 00:51:23,280
something that

00:51:20,960 --> 00:51:24,400
the community took so long to look at

00:51:23,280 --> 00:51:26,720
right i think that

00:51:24,400 --> 00:51:28,160
this recognition that the way in which

00:51:26,720 --> 00:51:30,720
the community has defined

00:51:28,160 --> 00:51:32,880
harms or even user right this term that

00:51:30,720 --> 00:51:36,960
we should be talking much more about

00:51:32,880 --> 00:51:39,520
yeah is is also creating blind spots

00:51:36,960 --> 00:51:40,240
that that makes the community less

00:51:39,520 --> 00:51:42,160
effective

00:51:40,240 --> 00:51:45,040
at its stated purpose which is to

00:51:42,160 --> 00:51:47,440
protect individuals from digital harms

00:51:45,040 --> 00:51:49,119
yeah yeah and we see that exactly with

00:51:47,440 --> 00:51:50,880
algorithmic auditing as well when you

00:51:49,119 --> 00:51:52,079
don't have the participation of a wide

00:51:50,880 --> 00:51:54,079
range of people

00:51:52,079 --> 00:51:56,160
um it's very easy to neglect certain

00:51:54,079 --> 00:51:57,599
issues um you know something that was

00:51:56,160 --> 00:51:59,920
recently reported on was sort of the

00:51:57,599 --> 00:52:02,559
neglect of you know isis

00:51:59,920 --> 00:52:03,760
risk assessment tool and how um you know

00:52:02,559 --> 00:52:06,079
immigration

00:52:03,760 --> 00:52:07,359
especially immigration with respect to

00:52:06,079 --> 00:52:09,920
um you know

00:52:07,359 --> 00:52:11,760
uh the deportation of undocumented

00:52:09,920 --> 00:52:13,680
individuals is something that

00:52:11,760 --> 00:52:16,319
you know the typical machine learning

00:52:13,680 --> 00:52:17,599
researcher might not necessarily connect

00:52:16,319 --> 00:52:19,119
with as a topic

00:52:17,599 --> 00:52:20,880
and as a result of that it becomes

00:52:19,119 --> 00:52:21,359
difficult to incentivize you know the

00:52:20,880 --> 00:52:23,520
group that

00:52:21,359 --> 00:52:25,680
is meant to already represent the space

00:52:23,520 --> 00:52:28,079
to care about some of these problems

00:52:25,680 --> 00:52:28,880
that feel unfamiliar or uncomfortable

00:52:28,079 --> 00:52:30,880
for them

00:52:28,880 --> 00:52:32,319
um and you know this is just even more

00:52:30,880 --> 00:52:33,920
of an incentive to

00:52:32,319 --> 00:52:35,440
you know increase the range of

00:52:33,920 --> 00:52:37,200
perspective that we include

00:52:35,440 --> 00:52:39,280
in terms of defining and prioritizing

00:52:37,200 --> 00:52:41,599
what important problems are

00:52:39,280 --> 00:52:43,040
so i know we need to rap i love having

00:52:41,599 --> 00:52:47,680
these conversations together

00:52:43,040 --> 00:52:50,800
so i think we owe it to everybody to

00:52:47,680 --> 00:52:51,520
to leave it on the on a note of optimism

00:52:50,800 --> 00:52:55,040
which is i

00:52:51,520 --> 00:52:56,880
i am very excited by the um uh practices

00:52:55,040 --> 00:52:58,720
that we're interrogating in trying to

00:52:56,880 --> 00:53:01,359
figure out what does it mean to build

00:52:58,720 --> 00:53:02,240
a reliable participatory arms discovery

00:53:01,359 --> 00:53:05,680
system

00:53:02,240 --> 00:53:07,520
uh and what else do we need around it

00:53:05,680 --> 00:53:09,200
uh for it to work what else do we need

00:53:07,520 --> 00:53:11,280
in terms of regulation what else do we

00:53:09,200 --> 00:53:12,400
need in terms of digestive system in the

00:53:11,280 --> 00:53:14,079
private sector

00:53:12,400 --> 00:53:16,720
what are the set of practices that

00:53:14,079 --> 00:53:19,040
really we want to see around us

00:53:16,720 --> 00:53:20,720
uh for this too so this to work so i'm

00:53:19,040 --> 00:53:23,599
i'm super grateful to be

00:53:20,720 --> 00:53:24,640
working with you on this and to have had

00:53:23,599 --> 00:53:26,839
this conversation

00:53:24,640 --> 00:53:28,400
and i will leave the final word to you

00:53:26,839 --> 00:53:30,000
though

00:53:28,400 --> 00:53:30,710
yeah i was like i don't know what that

00:53:30,000 --> 00:53:36,830
last word

00:53:30,710 --> 00:53:36,830
[Laughter]

00:53:37,440 --> 00:53:42,240
yeah i think i think this was yeah this

00:53:39,359 --> 00:53:43,200
was a great moment of like reflecting on

00:53:42,240 --> 00:53:45,280
you know what we've learned in the

00:53:43,200 --> 00:53:46,400
project so far i think i think the the

00:53:45,280 --> 00:53:48,000
conclusion or my

00:53:46,400 --> 00:53:50,400
my understanding of the situation is

00:53:48,000 --> 00:53:50,720
that you know there's so much to learn

00:53:50,400 --> 00:53:53,040
from

00:53:50,720 --> 00:53:54,319
other groups that have gone through this

00:53:53,040 --> 00:53:57,119
process before

00:53:54,319 --> 00:53:58,480
this idea of you know identifying harms

00:53:57,119 --> 00:54:00,720
and technical systems and

00:53:58,480 --> 00:54:01,520
specifically like information security

00:54:00,720 --> 00:54:04,000
um you know

00:54:01,520 --> 00:54:05,839
that a whole uh journey that the field

00:54:04,000 --> 00:54:08,000
has gone through is something that

00:54:05,839 --> 00:54:09,680
we can learn from just because the form

00:54:08,000 --> 00:54:12,000
of the technical artifact has kind of

00:54:09,680 --> 00:54:13,760
evolved from software programs to

00:54:12,000 --> 00:54:16,000
you know machine learning models doesn't

00:54:13,760 --> 00:54:17,359
mean that um there's not a lot of sort

00:54:16,000 --> 00:54:20,160
of parallel

00:54:17,359 --> 00:54:21,920
uh problems that exist but also parallel

00:54:20,160 --> 00:54:22,880
processes that we can learn from even if

00:54:21,920 --> 00:54:25,119
the problems are not

00:54:22,880 --> 00:54:26,720
quite the exact same there's still a lot

00:54:25,119 --> 00:54:28,800
to to take in and i

00:54:26,720 --> 00:54:30,559
i've personally benefited so much from

00:54:28,800 --> 00:54:33,280
even hearing your stories of like

00:54:30,559 --> 00:54:34,880
how passionate people are about you know

00:54:33,280 --> 00:54:36,000
bug bounties and everything i'm hoping

00:54:34,880 --> 00:54:38,079
there will be a day where

00:54:36,000 --> 00:54:39,680
people will have that passion for you

00:54:38,079 --> 00:54:40,960
know algorithmic bias bounties or

00:54:39,680 --> 00:54:43,680
whatever happens

00:54:40,960 --> 00:54:44,079
um where people can really um you know

00:54:43,680 --> 00:54:48,319
think

00:54:44,079 --> 00:54:52,000
incredibly uh uh intentionally about

00:54:48,319 --> 00:54:54,160
about how we you know think how we um

00:54:52,000 --> 00:54:55,599
characterize harms how we identify harms

00:54:54,160 --> 00:54:57,680
and how we address them

00:54:55,599 --> 00:54:58,640
um that's my hope at least that we move

00:54:57,680 --> 00:55:00,720
beyond the twitter

00:54:58,640 --> 00:55:03,359
outrage phase and we have a more

00:55:00,720 --> 00:55:05,280
structured response to all of us

00:55:03,359 --> 00:55:07,440
definitely more than a copy paste of the

00:55:05,280 --> 00:55:09,040
bug bounty structure what we're doing is

00:55:07,440 --> 00:55:11,839
a re-imagining

00:55:09,040 --> 00:55:12,960
what bounty principles could look like

00:55:11,839 --> 00:55:15,040
and would suggest

00:55:12,960 --> 00:55:16,559
in the space of algorithmic harms and

00:55:15,040 --> 00:55:19,359
that's quite an exciting

00:55:16,559 --> 00:55:20,000
yeah for sure for sure yeah yeah i think

00:55:19,359 --> 00:55:22,799
even that

00:55:20,000 --> 00:55:23,920
process of like the redesign or the re

00:55:22,799 --> 00:55:25,760
the rebrand

00:55:23,920 --> 00:55:27,760
um not even the rebrand but just really

00:55:25,760 --> 00:55:28,799
like um like i mentioned earlier sort of

00:55:27,760 --> 00:55:30,319
the borrowing of

00:55:28,799 --> 00:55:32,160
the things that work and that are

00:55:30,319 --> 00:55:33,359
applicable and then you know letting go

00:55:32,160 --> 00:55:36,480
of the things that are not

00:55:33,359 --> 00:55:39,040
as as compatible right yeah i i

00:55:36,480 --> 00:55:40,240
mean that whole process is just and and

00:55:39,040 --> 00:55:43,280
ultimately it's such an

00:55:40,240 --> 00:55:44,960
engaging uh design challenge um so i'm

00:55:43,280 --> 00:55:46,880
excited to sort of see

00:55:44,960 --> 00:55:48,799
um what comes as a result of sort of

00:55:46,880 --> 00:55:51,359
this very

00:55:48,799 --> 00:55:53,200
long and interesting like design process

00:55:51,359 --> 00:55:55,040
we're going through right now

00:55:53,200 --> 00:55:57,040
um it's been really fun and i think at

00:55:55,040 --> 00:56:00,160
minimum we'll like learn a lot

00:55:57,040 --> 00:56:02,640
um about this very related community

00:56:00,160 --> 00:56:04,240
um and hopefully you know pick up some

00:56:02,640 --> 00:56:07,760
lessons and make sure that we

00:56:04,240 --> 00:56:11,040
don't make the same mistakes and uh

00:56:07,760 --> 00:56:11,839
from the successes yeah that's a good

00:56:11,040 --> 00:56:16,880
goal

00:56:11,839 --> 00:56:18,640
i can't believe that yeah

00:56:16,880 --> 00:56:21,200
so i don't know if that was uh not

00:56:18,640 --> 00:56:31,839
optimistic enough but we'll find out

00:56:21,200 --> 00:56:31,839
i think that works

00:57:05,119 --> 00:57:07,200

YouTube URL: https://www.youtube.com/watch?v=1iT0eSSNlC4


