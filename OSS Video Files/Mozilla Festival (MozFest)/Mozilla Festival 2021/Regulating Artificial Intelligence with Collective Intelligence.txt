Title: Regulating Artificial Intelligence with Collective Intelligence
Publication date: 2021-03-25
Playlist: Mozilla Festival 2021
Description: 
	To conceptualize the best manifestation of AI, we must offer the best of human governance by elevating the best of our collective intelligence. Imagine Habermasian Discourse Ethics guiding the most difficult questions of AI Governance and decision-making? This session will explore the vision, tactics and results of the recently-concluded Global Citizenâ€™s Dialogue on the Future of the Internet, We, the Internet (WTI). Since its inception in 2018, WTI has organized national deliberations in 77 countries, activating the collective insights of over 5000 citizens. Our overarching goal is to test, improve and institutionalize AI governance with and for citizens. We will open the session with reflections on the results of the 2020 global dialogue with the country leads for Argentina, Malaysia, India, Philippines, Rwanda and Timor Leste followed by an invitation to co-imagine the future of AI Governance. More details can be realised here : https://wetheinternet.org/
Captions: 
	00:00:00,000 --> 00:00:15,640
[Music]

00:00:22,080 --> 00:01:31,010
[Music]

00:01:33,640 --> 00:01:52,759
[Music]

00:01:59,190 --> 00:03:08,129
[Music]

00:03:10,750 --> 00:03:29,869
[Music]

00:03:36,310 --> 00:04:45,249
[Music]

00:04:47,860 --> 00:05:07,000
[Music]

00:05:13,400 --> 00:06:22,420
[Music]

00:06:25,050 --> 00:06:44,180
[Music]

00:06:50,620 --> 00:07:59,600
[Music]

00:08:02,230 --> 00:08:21,360
[Music]

00:08:27,750 --> 00:09:36,780
[Music]

00:09:39,400 --> 00:09:58,530
[Music]

00:10:04,980 --> 00:11:13,950
[Music]

00:11:16,580 --> 00:11:35,710
[Music]

00:11:42,150 --> 00:12:51,129
[Music]

00:12:53,760 --> 00:13:12,890
[Music]

00:13:19,330 --> 00:14:28,310
[Music]

00:14:30,930 --> 00:14:46,629
[Music]

00:14:53,279 --> 00:14:59,839
okay

00:14:55,199 --> 00:14:59,839
so hello everyone welcome

00:15:08,720 --> 00:15:13,839
recording in progress

00:15:14,560 --> 00:15:19,360
hi everyone uh a very good morning good

00:15:18,160 --> 00:15:23,440
afternoon good evening

00:15:19,360 --> 00:15:25,440
uh we are still getting people in

00:15:23,440 --> 00:15:26,480
so we're just gonna wait for a few

00:15:25,440 --> 00:15:28,880
minutes but

00:15:26,480 --> 00:15:30,560
in the meantime let me introduce myself

00:15:28,880 --> 00:15:34,320
uh i am rasheed saksena

00:15:30,560 --> 00:15:36,320
i am the scientific committee member and

00:15:34,320 --> 00:15:38,000
national strategic partner for we the

00:15:36,320 --> 00:15:40,800
internet in india

00:15:38,000 --> 00:15:42,160
a very well welcome uh just a few

00:15:40,800 --> 00:15:44,240
housekeeping rules we

00:15:42,160 --> 00:15:45,360
would be recording this particular

00:15:44,240 --> 00:15:48,079
session

00:15:45,360 --> 00:15:49,839
and if you can use a pseudonym if you

00:15:48,079 --> 00:15:52,480
want you can also

00:15:49,839 --> 00:15:54,160
uh switch off your cameras and we're

00:15:52,480 --> 00:15:56,480
also trying to do this

00:15:54,160 --> 00:15:58,320
dialogue in three different languages so

00:15:56,480 --> 00:15:59,440
we encourage you to introduce yourselves

00:15:58,320 --> 00:16:02,320
in the chat

00:15:59,440 --> 00:16:03,360
and also tell us your preferred mode of

00:16:02,320 --> 00:16:09,839
language

00:16:03,360 --> 00:16:09,839
for the activity session

00:16:11,759 --> 00:16:19,120
thank you rashi i see that um

00:16:14,959 --> 00:16:19,120
people are coming in my um

00:16:19,279 --> 00:16:24,240
yeah maybe wait two to three more

00:16:21,279 --> 00:16:27,519
minutes for people to join

00:16:24,240 --> 00:16:29,519
and then we will um we'll start

00:16:27,519 --> 00:16:30,800
but in the meantime what we can do is

00:16:29,519 --> 00:16:34,800
each of us

00:16:30,800 --> 00:16:37,440
write a sentence to present oneself

00:16:34,800 --> 00:16:38,959
and put it in the chat so we can have

00:16:37,440 --> 00:16:42,240
that kind of

00:16:38,959 --> 00:16:45,279
dynamic presentation and

00:16:42,240 --> 00:16:51,839
have everyone saying hello

00:16:45,279 --> 00:16:51,839
so let's go let's start that exercise

00:16:53,920 --> 00:16:57,839
lawrence if you could also give your

00:16:56,079 --> 00:16:59,199
preference of which language you would

00:16:57,839 --> 00:17:00,000
want your activity and that would be

00:16:59,199 --> 00:17:03,440
great that way

00:17:00,000 --> 00:17:04,959
you could just keep uh you know record

00:17:03,440 --> 00:17:06,640
or what you want because we do plan

00:17:04,959 --> 00:17:08,079
we plan to carry out this in french

00:17:06,640 --> 00:17:10,400
spanish and english those are the three

00:17:08,079 --> 00:17:10,400
modes

00:17:13,919 --> 00:17:19,839
hi clara welcome french is good english

00:17:16,319 --> 00:17:19,839
is good okay

00:17:28,480 --> 00:17:47,840
right sure they actually inform me that

00:17:32,160 --> 00:17:47,840
there's no available promotions

00:17:52,240 --> 00:17:55,600
which i think we should wait for another

00:17:53,840 --> 00:18:01,120
two minutes and then have

00:17:55,600 --> 00:18:03,360
everyone else done yes we do have

00:18:01,120 --> 00:18:04,320
um those extra two minutes are located

00:18:03,360 --> 00:18:06,400
for people to

00:18:04,320 --> 00:18:07,440
to come in to introduce themselves and

00:18:06,400 --> 00:18:14,690
to be able to

00:18:07,440 --> 00:18:18,239
feel comfortable in the room

00:18:14,690 --> 00:18:18,239
[Music]

00:18:23,039 --> 00:18:29,039
okay so as a waiting exercise

00:18:26,080 --> 00:18:31,200
could you write in the chat how much uh

00:18:29,039 --> 00:18:32,160
degrees in celsius you have at the place

00:18:31,200 --> 00:18:34,160
you are

00:18:32,160 --> 00:18:36,480
i see john with a t-shirt i see

00:18:34,160 --> 00:18:38,799
jean-francois in a room but i don't know

00:18:36,480 --> 00:18:40,160
how many degrees i see your ventilator

00:18:38,799 --> 00:18:43,600
is not on so

00:18:40,160 --> 00:18:45,039
maybe it's still not too warm we we have

00:18:43,600 --> 00:18:47,840
a dream i know an eye but

00:18:45,039 --> 00:18:48,559
i'm not gonna tell you yes i'm going to

00:18:47,840 --> 00:18:51,679
write you

00:18:48,559 --> 00:18:51,679
how much degrees we have

00:18:53,039 --> 00:18:58,240
i have to look i'm also inside okay now

00:18:55,919 --> 00:18:58,240
i know

00:18:59,679 --> 00:19:07,360
so it's eight degrees celsius in berlin

00:19:03,360 --> 00:19:10,230
malaysia 28 23 in lisbon

00:19:07,360 --> 00:19:13,359
paris is rainy

00:19:10,230 --> 00:19:13,359
[Music]

00:19:14,880 --> 00:19:19,200
for those who just joining us we are

00:19:17,360 --> 00:19:21,679
asking everyone to

00:19:19,200 --> 00:19:23,600
introduce yourself on the chat and to

00:19:21,679 --> 00:19:24,000
also give us your preference in terms of

00:19:23,600 --> 00:19:26,400
language

00:19:24,000 --> 00:19:39,840
so we can accommodate for the breakout

00:19:26,400 --> 00:19:39,840
rooms thank you

00:19:54,000 --> 00:19:57,520
and we also have alien joining elin is

00:19:56,480 --> 00:20:07,840
also part of the

00:19:57,520 --> 00:20:07,840
preparation team

00:20:10,400 --> 00:20:13,039
akt

00:20:18,320 --> 00:20:21,520
we do have a few other people joining in

00:20:20,640 --> 00:20:24,159
maybe yes

00:20:21,520 --> 00:20:24,159
i see that

00:20:26,000 --> 00:20:29,919
it's the people that were catching a

00:20:28,480 --> 00:20:34,799
coffee between two sessions

00:20:29,919 --> 00:20:37,200
as you normally do in a conference

00:20:34,799 --> 00:20:39,520
after those who just joined in uh if you

00:20:37,200 --> 00:20:41,120
could introduce yourselves on the chat

00:20:39,520 --> 00:20:42,960
are we planning to do a breakout

00:20:41,120 --> 00:20:45,280
activity in three different languages

00:20:42,960 --> 00:20:46,159
that's spanish french english so if you

00:20:45,280 --> 00:20:47,760
could

00:20:46,159 --> 00:21:03,840
let us know what your preference is and

00:20:47,760 --> 00:21:03,840
we will update you

00:21:12,320 --> 00:21:18,400
antoine i think you should go ahead i

00:21:15,440 --> 00:21:19,120
wanted to say exactly that um so thank

00:21:18,400 --> 00:21:22,960
you everyone

00:21:19,120 --> 00:21:24,960
for being there and thank you um

00:21:22,960 --> 00:21:26,000
for the time that we're going to bring

00:21:24,960 --> 00:21:28,480
together and for that

00:21:26,000 --> 00:21:30,400
weather report uh i'm sure if we could

00:21:28,480 --> 00:21:31,120
connect that to artificial intelligence

00:21:30,400 --> 00:21:34,720
we could

00:21:31,120 --> 00:21:36,480
beat the weather channel and artificial

00:21:34,720 --> 00:21:38,240
intelligence is going to be our topic

00:21:36,480 --> 00:21:41,120
today but not

00:21:38,240 --> 00:21:42,400
only it is going to be connected to the

00:21:41,120 --> 00:21:45,600
question of collective

00:21:42,400 --> 00:21:46,159
intelligence um i will now share my

00:21:45,600 --> 00:21:48,159
screen

00:21:46,159 --> 00:21:49,600
and we have some elements of

00:21:48,159 --> 00:21:52,480
presentation

00:21:49,600 --> 00:21:54,240
and so the topic for today is regulating

00:21:52,480 --> 00:21:55,520
artificial intelligence with collective

00:21:54,240 --> 00:21:58,080
intelligence

00:21:55,520 --> 00:21:59,919
um and this session has been prepared by

00:21:58,080 --> 00:22:01,440
the innovation for policy foundation the

00:21:59,919 --> 00:22:04,720
io foundation

00:22:01,440 --> 00:22:05,039
into the internet society um the chapter

00:22:04,720 --> 00:22:08,080
in

00:22:05,039 --> 00:22:10,320
argentina and the youth so that's 18

00:22:08,080 --> 00:22:12,000
from the youth observatory and with the

00:22:10,320 --> 00:22:15,360
support of mission public

00:22:12,000 --> 00:22:19,200
um our agenda for today is

00:22:15,360 --> 00:22:21,039
um that we are going to present the team

00:22:19,200 --> 00:22:23,120
that is going to be quick and the

00:22:21,039 --> 00:22:25,120
participants who already started that

00:22:23,120 --> 00:22:26,799
we then present some key results on

00:22:25,120 --> 00:22:28,080
artificial intelligence from the global

00:22:26,799 --> 00:22:30,720
citizens dialogue

00:22:28,080 --> 00:22:32,320
on the future of internet and then we go

00:22:30,720 --> 00:22:36,000
in a discussion

00:22:32,320 --> 00:22:38,960
um and that is um then we convene back

00:22:36,000 --> 00:22:40,080
in plenary for the participants and

00:22:38,960 --> 00:22:41,600
teams of participants we

00:22:40,080 --> 00:22:43,120
have had that in in the chat for the

00:22:41,600 --> 00:22:44,880
team i am antoine

00:22:43,120 --> 00:22:46,320
i will be uh taking over the main

00:22:44,880 --> 00:22:49,600
moderation today

00:22:46,320 --> 00:22:51,679
um and i'd like um to pass

00:22:49,600 --> 00:22:53,120
rapidly the microphone too so rashi you

00:22:51,679 --> 00:22:54,640
already presented yourself but maybe you

00:22:53,120 --> 00:22:56,880
start again

00:22:54,640 --> 00:22:57,760
as you are also from the core team and

00:22:56,880 --> 00:23:00,320
then pass over to

00:22:57,760 --> 00:23:01,679
the other from the team i'm gonna keep

00:23:00,320 --> 00:23:04,799
it short uh

00:23:01,679 --> 00:23:07,120
hi everyone uh my name is dashi i'm the

00:23:04,799 --> 00:23:09,760
national strategic partner from india

00:23:07,120 --> 00:23:12,880
and also the scientific committee member

00:23:09,760 --> 00:23:17,919
of the internet dialogue and i

00:23:12,880 --> 00:23:17,919
pass it over to john from diy

00:23:18,720 --> 00:23:24,000
hi everyone my name is jean-pastore i go

00:23:20,799 --> 00:23:25,600
by john makes everybody's life simpler

00:23:24,000 --> 00:23:27,919
i manage an organization called the io

00:23:25,600 --> 00:23:30,159
foundation whose advocacy is data

00:23:27,919 --> 00:23:32,000
centric digital rights and we are the

00:23:30,159 --> 00:23:33,760
national implementing partner

00:23:32,000 --> 00:23:36,240
for malaysia philippines and

00:23:33,760 --> 00:23:39,840
immortalistic for the we the internet

00:23:36,240 --> 00:23:42,480
and i will pass it to my cortana's john

00:23:39,840 --> 00:23:42,480
from rwanda

00:23:48,080 --> 00:23:52,960
john up to you

00:23:51,120 --> 00:23:54,400
hey everyone good afternoon i'm very

00:23:52,960 --> 00:23:56,320
delighted to be here with you all my

00:23:54,400 --> 00:23:58,720
name is john stever i'm with

00:23:56,320 --> 00:24:01,360
the innovation for policy foundation and

00:23:58,720 --> 00:24:04,640
we are the national partners for

00:24:01,360 --> 00:24:05,039
rwanda working together with rashi on

00:24:04,640 --> 00:24:08,559
the

00:24:05,039 --> 00:24:12,400
scientific committee and i guess then

00:24:08,559 --> 00:24:12,400
i'll pass over to eileen

00:24:13,679 --> 00:24:17,039
thank you very much hi everyone my name

00:24:16,720 --> 00:24:21,039
is

00:24:17,039 --> 00:24:24,400
aileen my parents are she here

00:24:21,039 --> 00:24:27,520
i'm from argentina so i have been

00:24:24,400 --> 00:24:28,480
organizing the global citizens dialogue

00:24:27,520 --> 00:24:31,600
in my country

00:24:28,480 --> 00:24:35,200
i'm also part of the youth coalition

00:24:31,600 --> 00:24:37,520
and the utc which is the youth special

00:24:35,200 --> 00:24:39,600
interest group for internal society and

00:24:37,520 --> 00:24:46,000
now i'm passing the

00:24:39,600 --> 00:24:47,279
floor to russia

00:24:46,000 --> 00:24:49,279
we'll take over here because rashi

00:24:47,279 --> 00:24:52,480
already presented herself and i go too

00:24:49,279 --> 00:24:55,360
rapidly to mano and maria

00:24:52,480 --> 00:24:55,360
and then we can start

00:24:55,600 --> 00:24:59,440
hi everyone good afternoon or good

00:24:57,360 --> 00:25:00,960
morning depending on

00:24:59,440 --> 00:25:02,960
which side of the world you are who you

00:25:00,960 --> 00:25:06,080
are uh so a manual was part of the

00:25:02,960 --> 00:25:08,640
global coordination team for his intent

00:25:06,080 --> 00:25:11,039
so working also with antoine and maria

00:25:08,640 --> 00:25:13,840
that i'm going to give the floor to

00:25:11,039 --> 00:25:15,279
hi everyone i'm maria from from paris

00:25:13,840 --> 00:25:18,000
here and i'm working

00:25:15,279 --> 00:25:19,200
also with antoine and manuel at mission

00:25:18,000 --> 00:25:23,360
public

00:25:19,200 --> 00:25:23,360
and and i'm very happy to be here

00:25:23,760 --> 00:25:27,039
so thank you very much thank you

00:25:25,279 --> 00:25:31,039
everyone and i know

00:25:27,039 --> 00:25:34,080
go over um ah that's the team

00:25:31,039 --> 00:25:37,360
so no you you know us um and

00:25:34,080 --> 00:25:39,520
a word on on the vision um so the vision

00:25:37,360 --> 00:25:41,440
is and how do we test improve and

00:25:39,520 --> 00:25:42,960
institutionalize internet governance

00:25:41,440 --> 00:25:45,840
with and for the citizens

00:25:42,960 --> 00:25:46,559
we have seen um at mission public that

00:25:45,840 --> 00:25:48,799
um

00:25:46,559 --> 00:25:50,559
there is a gap a gap between a

00:25:48,799 --> 00:25:52,320
multi-stakeholder model of governance

00:25:50,559 --> 00:25:53,120
for internet the global dimension of

00:25:52,320 --> 00:25:54,960
internet

00:25:53,120 --> 00:25:57,520
and the way it is governed and decisions

00:25:54,960 --> 00:25:58,960
are taken on the future of the internet

00:25:57,520 --> 00:26:01,440
and we're thinking okay we need to

00:25:58,960 --> 00:26:03,760
bridge that gap and how do we do that

00:26:01,440 --> 00:26:06,559
we do that with um deliberative

00:26:03,760 --> 00:26:09,520
democracy and deliberative methods

00:26:06,559 --> 00:26:10,400
of citizens engagement and that's why we

00:26:09,520 --> 00:26:13,760
wanted to

00:26:10,400 --> 00:26:14,480
work on the future of intent how to do

00:26:13,760 --> 00:26:17,520
that

00:26:14,480 --> 00:26:19,840
how to do that um we launched in 2017

00:26:17,520 --> 00:26:21,200
a project that we could we the internet

00:26:19,840 --> 00:26:24,320
and with the internet has

00:26:21,200 --> 00:26:26,880
three pinas the first one is um

00:26:24,320 --> 00:26:27,600
that we have deliberations meaning um

00:26:26,880 --> 00:26:31,039
groups of

00:26:27,600 --> 00:26:35,279
citizens ordinary citizens that gather

00:26:31,039 --> 00:26:36,960
and meet and discuss on key questions on

00:26:35,279 --> 00:26:38,799
the future of the internet but they

00:26:36,960 --> 00:26:41,360
don't do that

00:26:38,799 --> 00:26:43,279
purely randomly they do that because

00:26:41,360 --> 00:26:45,360
they go through a process

00:26:43,279 --> 00:26:46,720
of structured discussion which is based

00:26:45,360 --> 00:26:49,600
on information

00:26:46,720 --> 00:26:51,679
and a discussion and then they formulate

00:26:49,600 --> 00:26:54,720
their preferences the prior

00:26:51,679 --> 00:26:57,120
prioritize their views and

00:26:54,720 --> 00:26:57,760
these reasons are what we call the

00:26:57,120 --> 00:27:00,320
informed

00:26:57,760 --> 00:27:02,559
views of citizens and then the third

00:27:00,320 --> 00:27:06,960
pillar is once we have those results

00:27:02,559 --> 00:27:08,880
we take them to decision makers and we

00:27:06,960 --> 00:27:10,000
try to make sure that they have an

00:27:08,880 --> 00:27:13,200
impact on the discussion

00:27:10,000 --> 00:27:15,120
so that's how we imagine uh the kind of

00:27:13,200 --> 00:27:17,200
process that could improve internet

00:27:15,120 --> 00:27:21,760
governance to make it more inclusive

00:27:17,200 --> 00:27:24,799
with citizens of the world and in 2020

00:27:21,760 --> 00:27:27,840
and we managed to um deploy such

00:27:24,799 --> 00:27:31,520
citizens and stakeholders dialogue so a

00:27:27,840 --> 00:27:32,720
two leg system in over

00:27:31,520 --> 00:27:34,960
80 countries of the world for the

00:27:32,720 --> 00:27:36,880
citizens dialogue it was 70

00:27:34,960 --> 00:27:38,159
plus countries for the stakeholder style

00:27:36,880 --> 00:27:41,440
we had people from

00:27:38,159 --> 00:27:43,039
around 80 countries and we had over 5

00:27:41,440 --> 00:27:46,320
000 participants and as

00:27:43,039 --> 00:27:49,440
um francois john and eden

00:27:46,320 --> 00:27:51,039
and rashi have said we had in all

00:27:49,440 --> 00:27:52,720
those countries a national partner

00:27:51,039 --> 00:27:54,880
strategic partner in the country

00:27:52,720 --> 00:27:57,600
in charge of implementing the dialogue

00:27:54,880 --> 00:28:00,799
at the national level

00:27:57,600 --> 00:28:03,200
we had different topics um to

00:28:00,799 --> 00:28:04,080
do that we tackled with this dialogue

00:28:03,200 --> 00:28:07,039
the first one

00:28:04,080 --> 00:28:08,480
was on the future of was the vision of

00:28:07,039 --> 00:28:10,399
internet for citizens

00:28:08,480 --> 00:28:12,320
the second topic was on the question of

00:28:10,399 --> 00:28:15,039
data and data governance

00:28:12,320 --> 00:28:16,080
the third topic was about disinformation

00:28:15,039 --> 00:28:18,320
the fourth topic

00:28:16,080 --> 00:28:20,080
was about artificial intelligence and

00:28:18,320 --> 00:28:21,360
the fifth topic was about the future of

00:28:20,080 --> 00:28:23,679
internet governance

00:28:21,360 --> 00:28:26,480
today we are going to present you some

00:28:23,679 --> 00:28:29,600
key results on artificial intelligence

00:28:26,480 --> 00:28:32,720
um so as we um

00:28:29,600 --> 00:28:35,440
do such processes um citizens

00:28:32,720 --> 00:28:37,360
give a feedback in terms of qualitative

00:28:35,440 --> 00:28:40,399
elements and quantitative elements

00:28:37,360 --> 00:28:43,039
so and and they also give vision as

00:28:40,399 --> 00:28:44,320
individuals but also as groups of people

00:28:43,039 --> 00:28:47,760
because they discuss

00:28:44,320 --> 00:28:50,480
in groups at tables or this last year

00:28:47,760 --> 00:28:51,279
also online but the idea is that they

00:28:50,480 --> 00:28:54,320
are

00:28:51,279 --> 00:28:56,000
building a view um between themselves

00:28:54,320 --> 00:28:59,440
it's not completely individual

00:28:56,000 --> 00:29:00,399
it's a mix of what people think and what

00:28:59,440 --> 00:29:03,840
people discuss

00:29:00,399 --> 00:29:06,559
and manage to get as a common vision and

00:29:03,840 --> 00:29:07,760
so as part of the results what we see is

00:29:06,559 --> 00:29:09,760
that for each of those

00:29:07,760 --> 00:29:11,200
large topic i mentioned there is a

00:29:09,760 --> 00:29:12,880
vision which is

00:29:11,200 --> 00:29:14,399
emerging from the results from all over

00:29:12,880 --> 00:29:16,960
the world for the

00:29:14,399 --> 00:29:18,640
question of artificial intelligence um

00:29:16,960 --> 00:29:21,120
the interesting

00:29:18,640 --> 00:29:23,360
thing about that vision is that um

00:29:21,120 --> 00:29:24,799
citizens considered that their knowledge

00:29:23,360 --> 00:29:25,679
and understanding of artificial

00:29:24,799 --> 00:29:28,399
intelligence

00:29:25,679 --> 00:29:29,279
was not enough to have a really informed

00:29:28,399 --> 00:29:31,520
discussion

00:29:29,279 --> 00:29:33,039
and to really be able to take um

00:29:31,520 --> 00:29:35,120
decisions to

00:29:33,039 --> 00:29:36,799
take recommendations or to formulate

00:29:35,120 --> 00:29:39,120
strong recommendations

00:29:36,799 --> 00:29:40,640
at the same time they also thought and

00:29:39,120 --> 00:29:42,960
articulated the fact

00:29:40,640 --> 00:29:44,320
that for them other stakeholders

00:29:42,960 --> 00:29:46,880
companies governments

00:29:44,320 --> 00:29:47,440
have the same limitations so one of the

00:29:46,880 --> 00:29:50,240
main

00:29:47,440 --> 00:29:51,520
uh results of the dialogue in terms of

00:29:50,240 --> 00:29:54,000
qualitative results

00:29:51,520 --> 00:29:55,039
is that what they saw is the necessity

00:29:54,000 --> 00:29:57,120
to discuss

00:29:55,039 --> 00:29:59,279
more that topic and to discuss the

00:29:57,120 --> 00:30:02,320
question of governance of intern

00:29:59,279 --> 00:30:05,120
of artificial intelligence

00:30:02,320 --> 00:30:05,679
that's for the vision now i will show

00:30:05,120 --> 00:30:07,840
you that

00:30:05,679 --> 00:30:09,039
i will commend it of course so during

00:30:07,840 --> 00:30:12,240
the dialogue we asked

00:30:09,039 --> 00:30:14,240
them to rank different scenarios

00:30:12,240 --> 00:30:16,480
uh for different use cases of artificial

00:30:14,240 --> 00:30:18,080
intelligence and we ask them to position

00:30:16,480 --> 00:30:21,440
themselves on one side

00:30:18,080 --> 00:30:22,000
so this is the left side of what you see

00:30:21,440 --> 00:30:25,039
on that

00:30:22,000 --> 00:30:28,640
picture is to we ask them okay is

00:30:25,039 --> 00:30:31,760
this scenario use case more a threat

00:30:28,640 --> 00:30:34,159
or more an opportunity so let's take

00:30:31,760 --> 00:30:35,120
the first one on one side they had the

00:30:34,159 --> 00:30:37,520
possibility to go

00:30:35,120 --> 00:30:38,559
on the side of artificial intelligence

00:30:37,520 --> 00:30:41,200
will bring

00:30:38,559 --> 00:30:43,679
advances in science and research um that

00:30:41,200 --> 00:30:45,600
are not worth the huge investment needed

00:30:43,679 --> 00:30:46,799
so we should invest the money elsewhere

00:30:45,600 --> 00:30:49,840
so that's

00:30:46,799 --> 00:30:51,360
a scenario the on the other side ai

00:30:49,840 --> 00:30:53,440
brings a lot of breakthrough in science

00:30:51,360 --> 00:30:56,159
and research that benefits humanity

00:30:53,440 --> 00:30:57,200
and citizens had to position themselves

00:30:56,159 --> 00:31:00,080
on what they thought

00:30:57,200 --> 00:31:01,360
would be the balance as you see the the

00:31:00,080 --> 00:31:04,799
main reasons of that

00:31:01,360 --> 00:31:08,000
is that the gray part in the middle is

00:31:04,799 --> 00:31:09,519
more or less a quarter of the

00:31:08,000 --> 00:31:11,840
percentage of the positioning of the

00:31:09,519 --> 00:31:14,640
people so what people tell us with that

00:31:11,840 --> 00:31:15,679
is that for them there is no inherent

00:31:14,640 --> 00:31:17,919
beneficial

00:31:15,679 --> 00:31:18,880
or harmful effect of artificial

00:31:17,919 --> 00:31:21,760
intelligence

00:31:18,880 --> 00:31:22,320
in the different use cases they really

00:31:21,760 --> 00:31:24,399
see

00:31:22,320 --> 00:31:25,440
artificial intelligence as being a

00:31:24,399 --> 00:31:28,640
technology

00:31:25,440 --> 00:31:29,039
that can be steered and governed and can

00:31:28,640 --> 00:31:31,440
go

00:31:29,039 --> 00:31:32,640
either way on the positive or the

00:31:31,440 --> 00:31:34,720
negative side

00:31:32,640 --> 00:31:36,640
so that's a very important message for

00:31:34,720 --> 00:31:37,760
us today and the discussion we are going

00:31:36,640 --> 00:31:38,240
to have because we are going to talk

00:31:37,760 --> 00:31:40,559
about

00:31:38,240 --> 00:31:41,760
governance of artificial intelligence

00:31:40,559 --> 00:31:43,919
and the message is

00:31:41,760 --> 00:31:44,960
yes people see that there is a need for

00:31:43,919 --> 00:31:47,039
governance and

00:31:44,960 --> 00:31:49,279
that it can be steered and that there is

00:31:47,039 --> 00:31:52,880
no ignorant

00:31:49,279 --> 00:31:54,880
harmful or very positive scenario out

00:31:52,880 --> 00:31:56,480
there

00:31:54,880 --> 00:31:58,399
another thing that is maybe interesting

00:31:56,480 --> 00:32:01,679
is that citizens strongly

00:31:58,399 --> 00:32:04,559
um showed that for them

00:32:01,679 --> 00:32:05,919
artificial intelligence is one of the

00:32:04,559 --> 00:32:08,559
topic where they see

00:32:05,919 --> 00:32:09,279
the strongest need for a global

00:32:08,559 --> 00:32:12,320
governance

00:32:09,279 --> 00:32:13,279
structure much more than on data or on

00:32:12,320 --> 00:32:14,720
this information

00:32:13,279 --> 00:32:16,399
that's interesting because they see

00:32:14,720 --> 00:32:18,720
artificial intelligence as a global

00:32:16,399 --> 00:32:20,640
challenge

00:32:18,720 --> 00:32:22,399
now let's focus some key results on the

00:32:20,640 --> 00:32:25,279
countries that we have

00:32:22,399 --> 00:32:26,880
today here as national partners um so

00:32:25,279 --> 00:32:27,760
i'm sorry for the quality of the

00:32:26,880 --> 00:32:30,960
extraction of the

00:32:27,760 --> 00:32:33,440
images we we can

00:32:30,960 --> 00:32:35,120
rework them later but if we take

00:32:33,440 --> 00:32:35,919
argentina and we are going to take one

00:32:35,120 --> 00:32:38,640
question

00:32:35,919 --> 00:32:40,240
and what is interesting is that we ask

00:32:38,640 --> 00:32:42,000
citizens to

00:32:40,240 --> 00:32:44,320
tell us if they see artificial

00:32:42,000 --> 00:32:47,039
intelligence more as an opportunity

00:32:44,320 --> 00:32:48,799
than a threat more threat than an

00:32:47,039 --> 00:32:52,799
opportunity or equally

00:32:48,799 --> 00:32:54,640
or if they had no opinion on that and a

00:32:52,799 --> 00:32:57,039
huge majority of citizens in all

00:32:54,640 --> 00:32:59,039
countries say that they see that as

00:32:57,039 --> 00:33:00,480
equally an opportunity and a threat so

00:32:59,039 --> 00:33:03,760
that's for argentina

00:33:00,480 --> 00:33:06,240
but the same goes for philippines um

00:33:03,760 --> 00:33:06,960
for rwanda where you see um that here

00:33:06,240 --> 00:33:09,600
maybe

00:33:06,960 --> 00:33:10,960
the opportunity is taking over so maybe

00:33:09,600 --> 00:33:13,120
john you'll have to comment

00:33:10,960 --> 00:33:14,559
uh why citizens in rwanda were more

00:33:13,120 --> 00:33:15,279
positive than in other places in the

00:33:14,559 --> 00:33:18,559
world

00:33:15,279 --> 00:33:21,760
um and in india it's um equally

00:33:18,559 --> 00:33:24,320
an opportunity and a threat and um

00:33:21,760 --> 00:33:26,480
and more an opportunity so maybe here

00:33:24,320 --> 00:33:28,640
you want to comment that result

00:33:26,480 --> 00:33:29,519
in timor-leste we have also that kind of

00:33:28,640 --> 00:33:32,720
presence

00:33:29,519 --> 00:33:34,159
and shared between the two and and

00:33:32,720 --> 00:33:36,080
i think that's it for that short

00:33:34,159 --> 00:33:39,039
presentation um

00:33:36,080 --> 00:33:39,600
so now um i'd like to um have some

00:33:39,039 --> 00:33:41,760
comments

00:33:39,600 --> 00:33:44,000
from the coming from the countries uh

00:33:41,760 --> 00:33:44,480
before we go in the breakout rooms to

00:33:44,000 --> 00:33:48,399
work

00:33:44,480 --> 00:33:48,399
on the next step of our discussion

00:33:49,679 --> 00:33:53,440
i'm not sure if you have time for the

00:33:51,679 --> 00:33:55,600
comments i think we should go because

00:33:53,440 --> 00:33:57,039
it's going to be a 40-minute activity

00:33:55,600 --> 00:33:59,440
and we also have two people who just

00:33:57,039 --> 00:33:59,440
like you

00:34:00,559 --> 00:34:07,840
yeah so i will start to i can start to

00:34:02,480 --> 00:34:07,840
break out just a hint

00:34:10,879 --> 00:34:15,440
could you all introduce yourself and let

00:34:12,399 --> 00:34:21,280
us know any preferred language

00:34:15,440 --> 00:34:23,679
if possible if there's anyone

00:34:21,280 --> 00:34:25,440
you can write on the chat as well one

00:34:23,679 --> 00:34:26,800
there was just one person who did

00:34:25,440 --> 00:34:35,599
mention to us that

00:34:26,800 --> 00:34:37,760
they would prefer it in french

00:34:35,599 --> 00:34:38,879
they were if i'm not wrong there would

00:34:37,760 --> 00:34:42,240
be

00:34:38,879 --> 00:34:45,599
um two people who expressed that they

00:34:42,240 --> 00:34:47,839
could um um choose french although they

00:34:45,599 --> 00:34:51,040
would also be able to

00:34:47,839 --> 00:34:54,560
to participate in in english and

00:34:51,040 --> 00:34:56,000
from analyzing the the poll i would say

00:34:54,560 --> 00:34:59,359
that

00:34:56,000 --> 00:35:01,119
we can safely move into english as we

00:34:59,359 --> 00:35:03,200
have been

00:35:01,119 --> 00:35:05,040
uh discussing internally in the past

00:35:03,200 --> 00:35:08,000
minutes and that should be

00:35:05,040 --> 00:35:09,040
uh serving for for everyone and if

00:35:08,000 --> 00:35:10,880
there's any

00:35:09,040 --> 00:35:12,079
need to translate any of the terms

00:35:10,880 --> 00:35:15,040
during the sessions of course we're

00:35:12,079 --> 00:35:16,720
going to be available for that

00:35:15,040 --> 00:35:19,040
sure so let's have free groups all in

00:35:16,720 --> 00:35:19,040
english

00:35:19,280 --> 00:35:23,680
okay yes and if there's anyone who still

00:35:21,520 --> 00:35:26,079
wants it in a different language

00:35:23,680 --> 00:35:28,079
please raise your hand right now we will

00:35:26,079 --> 00:35:31,599
see if we can accommodate you

00:35:28,079 --> 00:35:35,119
i do know one person who had texted me

00:35:31,599 --> 00:35:35,119
and reached out to me

00:35:36,079 --> 00:35:40,400
okay so i will start the subgroup um and

00:35:38,720 --> 00:35:42,000
antoine will stay also in plenary so if

00:35:40,400 --> 00:35:43,599
you have any issue

00:35:42,000 --> 00:35:45,280
or if you you are disconnected you can

00:35:43,599 --> 00:35:47,200
always come back to this link and

00:35:45,280 --> 00:35:48,560
he will be there to put you in the right

00:35:47,200 --> 00:35:59,839
group let's start

00:35:48,560 --> 00:35:59,839
and see you in 40 minutes

00:36:22,000 --> 00:36:27,839
sure i can affect them or

00:36:28,240 --> 00:36:35,280
yeah i would affect them hi

00:36:31,359 --> 00:36:35,280
thomas yeah we can read thomas just

00:36:38,320 --> 00:36:42,640
so they have to click on on the

00:36:39,839 --> 00:36:45,119
subgroups yeah

00:36:42,640 --> 00:36:47,359
and try and i put your main moderator

00:36:45,119 --> 00:36:47,359
okay

00:36:52,839 --> 00:36:55,839
now

00:37:20,839 --> 00:37:23,839
foreign

00:37:25,320 --> 00:38:24,079
[Music]

00:38:22,000 --> 00:38:26,160
university and i'm a convenient phd at

00:38:24,079 --> 00:38:29,520
the university of toronto so i'm

00:38:26,160 --> 00:38:32,000
um based in in the greater toronto area

00:38:29,520 --> 00:38:34,000
in canada

00:38:32,000 --> 00:38:35,440
for joining us must be early in the

00:38:34,000 --> 00:38:38,720
morning right since

00:38:35,440 --> 00:38:41,280
the ones who are from toronto 11 a.m

00:38:38,720 --> 00:38:44,079
it's a good time it's a good time okay

00:38:41,280 --> 00:38:44,079
then we have kai

00:38:44,320 --> 00:38:47,680
kai can you hear that yes hello hope

00:38:46,960 --> 00:38:49,440
it's working

00:38:47,680 --> 00:38:50,880
yeah my name's kay i'm joining from

00:38:49,440 --> 00:38:51,760
germany i'm an experienced designer

00:38:50,880 --> 00:38:54,160
working in the

00:38:51,760 --> 00:38:55,680
tech sector sorry for joining a little

00:38:54,160 --> 00:38:58,640
bit late i think i um

00:38:55,680 --> 00:39:00,560
just hopped in right before you uh broke

00:38:58,640 --> 00:39:03,440
us out into different rooms so

00:39:00,560 --> 00:39:06,000
uh i think i would slowly take off from

00:39:03,440 --> 00:39:06,000
the conversation

00:39:06,880 --> 00:39:10,000
and then john do you want to go and

00:39:08,800 --> 00:39:15,280
introduce yourself

00:39:10,000 --> 00:39:17,440
maybe you can try with the camera

00:39:15,280 --> 00:39:19,200
okay let's see if this works hi everyone

00:39:17,440 --> 00:39:20,240
my name is john i i introduce myself in

00:39:19,200 --> 00:39:22,000
the main room i'm one of the

00:39:20,240 --> 00:39:22,640
coordinators of the global citizen

00:39:22,000 --> 00:39:26,000
dialogue

00:39:22,640 --> 00:39:28,240
for the last two years in rwanda um

00:39:26,000 --> 00:39:29,839
and we actually organized a sort of

00:39:28,240 --> 00:39:31,119
longer session on artificial

00:39:29,839 --> 00:39:32,800
intelligence in rwanda because it was

00:39:31,119 --> 00:39:33,760
feeding into the national

00:39:32,800 --> 00:39:36,000
the development of the national

00:39:33,760 --> 00:39:38,720
artificial intelligence policy in rwanda

00:39:36,000 --> 00:39:41,520
um and yeah super happy to be here i'm

00:39:38,720 --> 00:39:43,280
joining you guys all from south sinai

00:39:41,520 --> 00:39:45,440
actually a lot of the materials that we

00:39:43,280 --> 00:39:45,839
are using for the activity is inspired

00:39:45,440 --> 00:39:48,640
by

00:39:45,839 --> 00:39:50,079
what i policy that's john's team along

00:39:48,640 --> 00:39:53,359
with future society has

00:39:50,079 --> 00:39:55,040
implemented uh just um let me

00:39:53,359 --> 00:39:56,960
tell you a little bit about that i'm

00:39:55,040 --> 00:39:58,160
gonna go jump in right into the session

00:39:56,960 --> 00:40:00,400
because we don't have time

00:39:58,160 --> 00:40:01,440
uh i'm actually gonna paste the mirror

00:40:00,400 --> 00:40:03,119
board i'm just gonna

00:40:01,440 --> 00:40:05,200
check where we are it's over there with

00:40:03,119 --> 00:40:08,839
that and are you all

00:40:05,200 --> 00:40:11,839
is everyone on a is everyone using a

00:40:08,839 --> 00:40:11,839
desktop

00:40:12,960 --> 00:40:16,720
and he's actually from the tag team

00:40:17,839 --> 00:40:22,800
for mozilla so andy andy is always here

00:40:22,839 --> 00:40:27,599
he's

00:40:25,359 --> 00:40:30,960
okay uh every everyone's through a

00:40:27,599 --> 00:40:30,960
desktop right including kai

00:40:31,520 --> 00:40:35,119
yeah yeah because we're going to be

00:40:33,280 --> 00:40:37,040
using miro just just wanted to check i'm

00:40:35,119 --> 00:40:39,000
just going to okay

00:40:37,040 --> 00:40:40,720
great okay

00:40:39,000 --> 00:40:42,960
[Music]

00:40:40,720 --> 00:40:44,000
i just want to see who's there who

00:40:42,960 --> 00:40:45,440
wasn't there

00:40:44,000 --> 00:40:47,200
because then otherwise what i could also

00:40:45,440 --> 00:40:52,720
do is i could also share my screen

00:40:47,200 --> 00:40:55,599
that's also something that i could do

00:40:52,720 --> 00:40:58,480
if you want me to it would be great if

00:40:55,599 --> 00:41:07,839
you could share the link anyway

00:40:58,480 --> 00:41:07,839
yeah i can do that thank you great

00:41:15,280 --> 00:41:19,040
i'm also going to share my screen

00:41:17,260 --> 00:41:21,760
[Music]

00:41:19,040 --> 00:41:23,760
you can you see my screen are you just

00:41:21,760 --> 00:41:27,040
giving me a headphone

00:41:23,760 --> 00:41:28,800
yeah okay so we have three

00:41:27,040 --> 00:41:30,480
actually you know john i'm gonna you're

00:41:28,800 --> 00:41:33,040
gonna explain this now since this is

00:41:30,480 --> 00:41:34,800
yours go ahead

00:41:33,040 --> 00:41:36,640
i mean if you want like just give as to

00:41:34,800 --> 00:41:38,400
what was the thought process behind this

00:41:36,640 --> 00:41:40,160
we we haven't taken all aspects i think

00:41:38,400 --> 00:41:40,640
perhaps maybe the banking scenario is

00:41:40,160 --> 00:41:42,400
the one

00:41:40,640 --> 00:41:44,800
that's very similar to what you did but

00:41:42,400 --> 00:41:46,839
everything else is a little different

00:41:44,800 --> 00:41:49,839
but go ahead you can go behind the

00:41:46,839 --> 00:41:49,839
methodology

00:41:50,400 --> 00:41:54,880
okay i mean like really the the future

00:41:52,560 --> 00:41:56,319
society was largely responsible for

00:41:54,880 --> 00:41:58,640
putting this scenario

00:41:56,319 --> 00:42:00,720
idea together and it was again it was it

00:41:58,640 --> 00:42:04,079
was developed to really try and

00:42:00,720 --> 00:42:06,000
to gather thoughts from um the

00:42:04,079 --> 00:42:07,520
sort of very representative sample of

00:42:06,000 --> 00:42:07,920
the population that we brought together

00:42:07,520 --> 00:42:10,640
for the

00:42:07,920 --> 00:42:11,280
for the national dialogue in rwanda um

00:42:10,640 --> 00:42:14,079
and we

00:42:11,280 --> 00:42:16,640
organized um more than two hours of

00:42:14,079 --> 00:42:19,040
discussion on artificial intelligence

00:42:16,640 --> 00:42:22,240
to be able to go through these scenarios

00:42:19,040 --> 00:42:26,000
as well as the unesco rome principles

00:42:22,240 --> 00:42:29,040
and yeah so perhaps russia you could

00:42:26,000 --> 00:42:29,040
take us through the um

00:42:31,760 --> 00:42:36,319
we y'all have a chance to look to the

00:42:35,200 --> 00:42:38,880
scenarios we could

00:42:36,319 --> 00:42:40,720
we could have a lively discussion around

00:42:38,880 --> 00:42:41,599
it or y'all could just work on it in

00:42:40,720 --> 00:42:43,520
science

00:42:41,599 --> 00:42:45,280
i could read out the scenario and then

00:42:43,520 --> 00:42:47,280
we have two guiding questions for each

00:42:45,280 --> 00:42:49,200
scenario so we have one scenario

00:42:47,280 --> 00:42:51,040
with deep fix we have one scenario with

00:42:49,200 --> 00:42:52,560
mental health and we want to have one

00:42:51,040 --> 00:42:54,880
banking scenario

00:42:52,560 --> 00:42:56,720
and they have two primary questions

00:42:54,880 --> 00:42:58,560
which i'm just expanding on as to

00:42:56,720 --> 00:43:00,400
what are the key ai ethical issues that

00:42:58,560 --> 00:43:00,960
are raised by this particular scenario

00:43:00,400 --> 00:43:02,480
and then

00:43:00,960 --> 00:43:05,119
what are some of the solutions or

00:43:02,480 --> 00:43:06,480
recommendations to the ai ethical issues

00:43:05,119 --> 00:43:10,640
identified

00:43:06,480 --> 00:43:12,480
uh so yeah we're i could go either way

00:43:10,640 --> 00:43:14,400
and we could have like just a

00:43:12,480 --> 00:43:15,920
round-robin discussion here and

00:43:14,400 --> 00:43:19,359
oh you could also use the sticky notes

00:43:15,920 --> 00:43:22,480
and just paste your suggestions

00:43:19,359 --> 00:43:23,520
let me know what way you want to go i

00:43:22,480 --> 00:43:25,839
guess i would

00:43:23,520 --> 00:43:27,119
love to hear what people's thoughts are

00:43:25,839 --> 00:43:29,280
i the

00:43:27,119 --> 00:43:30,880
and maybe because the mirror gets a lot

00:43:29,280 --> 00:43:32,240
it's a lot of content and i'd like to

00:43:30,880 --> 00:43:35,599
hear what people have to say

00:43:32,240 --> 00:43:37,680
for first if that would be okay

00:43:35,599 --> 00:43:39,520
so then what i could do is that i could

00:43:37,680 --> 00:43:42,560
also wait one second

00:43:39,520 --> 00:43:44,240
one thing that i forgot i need to also

00:43:42,560 --> 00:43:46,240
i need to start the recording for this

00:43:44,240 --> 00:43:48,319
and that's okay it's already recorded

00:43:46,240 --> 00:43:51,200
andy is this already recorded we have to

00:43:48,319 --> 00:43:51,200
enable recording

00:43:51,680 --> 00:43:55,040
breakout rooms are not usually recorded

00:43:53,359 --> 00:43:58,000
but if it's going to be recorded then

00:43:55,040 --> 00:43:58,640
i'm going to go off video okay no no no

00:43:58,000 --> 00:44:01,040
that's fine

00:43:58,640 --> 00:44:01,920
i think they are i'm not too sure about

00:44:01,040 --> 00:44:04,800
it but then but

00:44:01,920 --> 00:44:05,200
i can i'm i can i'm ready to take notes

00:44:04,800 --> 00:44:07,680
and

00:44:05,200 --> 00:44:08,880
if everyone is okay with brenna then i

00:44:07,680 --> 00:44:10,480
could just go ahead and read the

00:44:08,880 --> 00:44:11,839
scenarios and then listen everyone's

00:44:10,480 --> 00:44:14,079
discussions

00:44:11,839 --> 00:44:16,400
i'm happy to do that is everyone okay

00:44:14,079 --> 00:44:19,839
with that ushnira

00:44:16,400 --> 00:44:23,359
hi okay

00:44:19,839 --> 00:44:24,640
great let me go ahead so we start with

00:44:23,359 --> 00:44:27,680
the banking scenario

00:44:24,640 --> 00:44:30,079
where we have which is similar to what

00:44:27,680 --> 00:44:30,720
uh the rwanda team went ahead with so we

00:44:30,079 --> 00:44:33,280
go with

00:44:30,720 --> 00:44:34,319
yolanda is a university student majoring

00:44:33,280 --> 00:44:36,880
on data science

00:44:34,319 --> 00:44:39,040
with excellent grades uh to pay her

00:44:36,880 --> 00:44:40,640
tuition she works as a waitress at a

00:44:39,040 --> 00:44:43,359
restaurant on weekends

00:44:40,640 --> 00:44:44,400
and due to the endemic and the covet 19

00:44:43,359 --> 00:44:46,160
lockdown

00:44:44,400 --> 00:44:48,160
her earnings have been reduced and she's

00:44:46,160 --> 00:44:50,480
concerned about running out of money her

00:44:48,160 --> 00:44:52,720
only option seems to be

00:44:50,480 --> 00:44:54,880
seeking for a loan coincidentally she

00:44:52,720 --> 00:44:56,160
receives an sms from her bank offering a

00:44:54,880 --> 00:44:57,599
loan facility

00:44:56,160 --> 00:44:59,359
she's amazed to find that this

00:44:57,599 --> 00:45:01,440
pre-approved loan is exactly what she's

00:44:59,359 --> 00:45:03,280
looking for

00:45:01,440 --> 00:45:04,960
and she clicks on the link accepts the

00:45:03,280 --> 00:45:06,000
loan through an app and finds the money

00:45:04,960 --> 00:45:08,800
in her bank account

00:45:06,000 --> 00:45:09,599
in order later her bank has developed a

00:45:08,800 --> 00:45:12,240
credit

00:45:09,599 --> 00:45:13,920
scoring model that utilizes ai

00:45:12,240 --> 00:45:16,400
harnessing data about

00:45:13,920 --> 00:45:17,359
account activity degree course and

00:45:16,400 --> 00:45:19,599
grades

00:45:17,359 --> 00:45:21,680
enables the bank to automatically select

00:45:19,599 --> 00:45:23,839
students for pre-approved loans

00:45:21,680 --> 00:45:25,359
the speed and efficiency of processing

00:45:23,839 --> 00:45:26,880
these loans has increased the bank's

00:45:25,359 --> 00:45:28,480
profit and improved customer

00:45:26,880 --> 00:45:30,640
satisfaction

00:45:28,480 --> 00:45:32,480
meanwhile yolanda's friend john has been

00:45:30,640 --> 00:45:33,599
waiting for over a month to hear back

00:45:32,480 --> 00:45:36,000
from his bank about

00:45:33,599 --> 00:45:37,119
a paper based loan application that took

00:45:36,000 --> 00:45:39,040
hours to complete

00:45:37,119 --> 00:45:41,119
and on hearing about sandra's positive

00:45:39,040 --> 00:45:43,920
response experience

00:45:41,119 --> 00:45:46,000
uh john decides to switch backs so

00:45:43,920 --> 00:45:47,920
that's the banking scenario you also we

00:45:46,000 --> 00:45:50,000
also have that in french and spanish

00:45:47,920 --> 00:45:51,200
and was interested in reading but i'd

00:45:50,000 --> 00:45:53,920
like to go ahead

00:45:51,200 --> 00:45:54,720
and you know keep these questions here

00:45:53,920 --> 00:45:57,500
and

00:45:54,720 --> 00:45:58,560
yeah feel free you can to

00:45:57,500 --> 00:46:01,359
[Music]

00:45:58,560 --> 00:46:03,280
populate and yeah to speak up i'm i'm

00:46:01,359 --> 00:46:05,440
i'm ready

00:46:03,280 --> 00:46:06,640
what do you think are the key ai killer

00:46:05,440 --> 00:46:09,839
issues raised by

00:46:06,640 --> 00:46:09,839
this particular scenario

00:46:12,880 --> 00:46:19,280
so i can maybe start us off so

00:46:16,000 --> 00:46:22,720
when i read that um the credit scoring

00:46:19,280 --> 00:46:26,400
algorithm is based on grades

00:46:22,720 --> 00:46:29,440
that my alarm bells go off um so

00:46:26,400 --> 00:46:32,560
it you know to in in

00:46:29,440 --> 00:46:33,359
in one sense um i think credit scoring

00:46:32,560 --> 00:46:36,000
information should

00:46:33,359 --> 00:46:36,880
purely be based on credit information

00:46:36,000 --> 00:46:38,720
but

00:46:36,880 --> 00:46:40,400
you know i'm sure some people will argue

00:46:38,720 --> 00:46:41,920
that you're

00:46:40,400 --> 00:46:43,920
you know people with good grades get

00:46:41,920 --> 00:46:45,680
good jobs and are you know lower credit

00:46:43,920 --> 00:46:49,599
risks and so on and so on right

00:46:45,680 --> 00:46:52,079
um but that that great information

00:46:49,599 --> 00:46:53,599
is housed at a different institution at

00:46:52,079 --> 00:46:57,040
an educational institution

00:46:53,599 --> 00:46:57,599
and um you can argue that um it should

00:46:57,040 --> 00:47:00,319
be kept

00:46:57,599 --> 00:47:00,720
completely independent so i think that

00:47:00,319 --> 00:47:04,160
use

00:47:00,720 --> 00:47:11,839
of grade and academic information

00:47:04,160 --> 00:47:11,839
for loans is a major ethical issue

00:47:12,800 --> 00:47:20,000
okay yeah i guess in relation to that

00:47:15,839 --> 00:47:23,040
just the very notion that a lot of data

00:47:20,000 --> 00:47:25,280
uh that she's probably um

00:47:23,040 --> 00:47:26,240
data about air activities in the

00:47:25,280 --> 00:47:28,960
internet

00:47:26,240 --> 00:47:30,559
on the internet more broadly it's been

00:47:28,960 --> 00:47:34,079
harvested to

00:47:30,559 --> 00:47:38,079
you know create this uh model and

00:47:34,079 --> 00:47:40,640
she's clearly not aware um

00:47:38,079 --> 00:47:41,760
of it she's not aware that this is

00:47:40,640 --> 00:47:44,720
happening at all

00:47:41,760 --> 00:47:45,599
considering that she's really surprised

00:47:44,720 --> 00:47:48,480
um

00:47:45,599 --> 00:47:50,000
that the bank knew so much that you know

00:47:48,480 --> 00:47:52,000
it's almost like a surprise that she

00:47:50,000 --> 00:47:54,240
receives the offer for the

00:47:52,000 --> 00:47:54,240
loan

00:47:55,119 --> 00:48:09,880
okay anyone wants to go next

00:48:06,750 --> 00:48:09,880
[Music]

00:48:19,440 --> 00:48:24,000
uh race is i think he and then also kind

00:48:22,559 --> 00:48:26,160
of the tracking that's

00:48:24,000 --> 00:48:27,359
probably i mean the word coincidentally

00:48:26,160 --> 00:48:29,839
is what sticks out

00:48:27,359 --> 00:48:31,599
to me and so i'm always like are there

00:48:29,839 --> 00:48:33,040
coincidences when ai is involved

00:48:31,599 --> 00:48:34,800
if there are then that would be kind of

00:48:33,040 --> 00:48:38,240
funny but

00:48:34,800 --> 00:48:41,680
uh so yeah um and then i guess

00:48:38,240 --> 00:48:45,599
the um yeah i think that

00:48:41,680 --> 00:48:47,119
there's two the two bane areas uh

00:48:45,599 --> 00:48:48,960
i'd be curious to know what happens to

00:48:47,119 --> 00:48:50,960
john if he gets like uh

00:48:48,960 --> 00:48:52,640
upped in the queue because uh he says

00:48:50,960 --> 00:48:54,319
sandra told him about it and if that has

00:48:52,640 --> 00:48:56,000
anything to do so like if you're

00:48:54,319 --> 00:48:58,800
then it's sort of like a pyramid scheme

00:48:56,000 --> 00:48:58,800
of loans

00:48:59,520 --> 00:49:03,280
do you think it's promoting a pyramid

00:49:01,200 --> 00:49:05,839
scheme of loans

00:49:03,280 --> 00:49:07,680
no like the end of the scenario kind of

00:49:05,839 --> 00:49:08,240
leads me like what happens in the next

00:49:07,680 --> 00:49:09,760
chapter

00:49:08,240 --> 00:49:10,960
so like maybe john is like oh i'm

00:49:09,760 --> 00:49:12,319
switching banks and then they're like

00:49:10,960 --> 00:49:14,000
how did you learn about us and he says

00:49:12,319 --> 00:49:16,800
his friend and then that kind of

00:49:14,000 --> 00:49:17,280
gives that friend more cachet or clout

00:49:16,800 --> 00:49:19,359
you know

00:49:17,280 --> 00:49:22,079
i don't know if that would happen but um

00:49:19,359 --> 00:49:23,760
that seems to be like another area that

00:49:22,079 --> 00:49:27,119
but that's the next scenario

00:49:23,760 --> 00:49:27,119
chapter two of the banking

00:49:27,200 --> 00:49:30,800
okay yeah just

00:49:32,240 --> 00:49:35,760
it doesn't explicitly say this in the

00:49:34,079 --> 00:49:36,240
scenario but there's a little bit of a

00:49:35,760 --> 00:49:39,119
hint

00:49:36,240 --> 00:49:40,559
that that those referrals matter in

00:49:39,119 --> 00:49:42,240
terms of if you make

00:49:40,559 --> 00:49:44,160
not just a verbal referral but a

00:49:42,240 --> 00:49:46,559
referral through social media

00:49:44,160 --> 00:49:48,240
now that's trackable right and maybe the

00:49:46,559 --> 00:49:50,319
banks can track

00:49:48,240 --> 00:49:51,760
you know successful loan applicants and

00:49:50,319 --> 00:49:54,480
who they refer

00:49:51,760 --> 00:49:55,920
the information to um and that becomes

00:49:54,480 --> 00:49:57,920
part of the scoring algorithm it's not

00:49:55,920 --> 00:49:59,920
an explicit private scenario but that

00:49:57,920 --> 00:50:02,000
you can see it going in that direction

00:49:59,920 --> 00:50:03,440
in terms of capturing social media

00:50:02,000 --> 00:50:04,720
referral data

00:50:03,440 --> 00:50:07,599
and using that as part of the credit

00:50:04,720 --> 00:50:07,599
scoring algorithm

00:50:09,359 --> 00:50:13,839
great points

00:50:14,880 --> 00:50:19,040
in the last sentence is meant to be on

00:50:17,040 --> 00:50:20,240
hearing about yolanda positive

00:50:19,040 --> 00:50:23,920
experience i suppose

00:50:20,240 --> 00:50:27,040
rather than sandra yeah yeah

00:50:23,920 --> 00:50:30,079
yeah so i guess the ethical

00:50:27,040 --> 00:50:33,200
maybe broader question is that obviously

00:50:30,079 --> 00:50:36,400
it's also the fact that um

00:50:33,200 --> 00:50:38,000
with yolanda not really understanding

00:50:36,400 --> 00:50:40,720
what's going on

00:50:38,000 --> 00:50:42,480
with the recommended system and the

00:50:40,720 --> 00:50:44,559
credit scoring model

00:50:42,480 --> 00:50:45,760
uh she portrays the whole thing as a

00:50:44,559 --> 00:50:49,200
really good thing

00:50:45,760 --> 00:50:51,119
and no wonder why uh john is up for it

00:50:49,200 --> 00:50:53,680
and i think that's that's exactly what

00:50:51,119 --> 00:50:54,079
is happening anyway on a massive scale

00:50:53,680 --> 00:50:57,119
uh

00:50:54,079 --> 00:50:58,160
today in terms of the lack of a sort of

00:50:57,119 --> 00:51:00,800
critical approach

00:50:58,160 --> 00:51:02,720
to the technologies that we are using

00:51:00,800 --> 00:51:05,359
every day

00:51:02,720 --> 00:51:06,880
yeah but i think also like the ethical

00:51:05,359 --> 00:51:08,000
thing is like why is she even needing a

00:51:06,880 --> 00:51:11,280
loan for her education

00:51:08,000 --> 00:51:13,119
in the first place like in like i mean

00:51:11,280 --> 00:51:16,160
in some ways like that

00:51:13,119 --> 00:51:17,839
starts it off and so i don't know

00:51:16,160 --> 00:51:19,680
because like i get what clara's saying

00:51:17,839 --> 00:51:20,800
but also like the reality of many

00:51:19,680 --> 00:51:22,559
people's experiences

00:51:20,800 --> 00:51:24,079
is that they may be she could very well

00:51:22,559 --> 00:51:25,359
be aware of all the stuff she's having

00:51:24,079 --> 00:51:27,280
to give over but

00:51:25,359 --> 00:51:28,960
it's like if you're going on food stamps

00:51:27,280 --> 00:51:30,480
you know people know that

00:51:28,960 --> 00:51:32,480
they're given access to a lot of their

00:51:30,480 --> 00:51:35,920
life but they need the food stamps so

00:51:32,480 --> 00:51:37,040
that's the institution is the ethical um

00:51:35,920 --> 00:51:38,720
at fault

00:51:37,040 --> 00:51:40,880
i'm not disagreeing with clara but i'm

00:51:38,720 --> 00:51:41,520
just like i mean why does she need a

00:51:40,880 --> 00:51:44,079
loan

00:51:41,520 --> 00:51:45,839
um you know and so like then it's more

00:51:44,079 --> 00:51:46,800
like what's the relationship of the bank

00:51:45,839 --> 00:51:49,599
and the

00:51:46,800 --> 00:51:50,240
the uh institution you know like maybe

00:51:49,599 --> 00:51:52,160
there's

00:51:50,240 --> 00:51:54,079
we don't know but like in the united

00:51:52,160 --> 00:51:56,079
states many i mean i went to a public

00:51:54,079 --> 00:51:58,000
university and it was nine percent

00:51:56,079 --> 00:51:59,520
like funded by the state it was known as

00:51:58,000 --> 00:52:02,640
the university of nike

00:51:59,520 --> 00:52:04,000
right so like there's a lot of capital

00:52:02,640 --> 00:52:06,720
relationships um

00:52:04,000 --> 00:52:07,280
that universities have um that i think

00:52:06,720 --> 00:52:09,359
are

00:52:07,280 --> 00:52:10,559
there's ethical questions not just on

00:52:09,359 --> 00:52:12,160
the ai side

00:52:10,559 --> 00:52:13,599
and i don't know if that's how that's

00:52:12,160 --> 00:52:15,119
why i'm interested in the conversation

00:52:13,599 --> 00:52:17,200
it's like where do we line up

00:52:15,119 --> 00:52:18,960
like this is the ethics of ai and the

00:52:17,200 --> 00:52:20,160
internet versus this is the ethics of

00:52:18,960 --> 00:52:25,839
capitalism

00:52:20,160 --> 00:52:25,839
um and such absolutely thanks

00:52:26,800 --> 00:52:33,839
maybe kai has something to say

00:52:30,720 --> 00:52:36,640
maybe if we pronounced his name as k um

00:52:33,839 --> 00:52:36,640
sorry she did

00:52:36,960 --> 00:52:45,280
are you with us yeah i'm just listening

00:52:40,720 --> 00:52:48,880
um i'm just thinking um from perspective

00:52:45,280 --> 00:52:50,880
why um a friend john

00:52:48,880 --> 00:52:52,720
has been put in the situation that he

00:52:50,880 --> 00:52:55,119
has to wait for so long

00:52:52,720 --> 00:52:56,240
and still not uh hearing anything back

00:52:55,119 --> 00:52:59,680
from the bank

00:52:56,240 --> 00:53:02,720
and is it because um that uh

00:52:59,680 --> 00:53:05,119
it's been deprioritized in a way so

00:53:02,720 --> 00:53:06,480
we're kind of excluding uh the group of

00:53:05,119 --> 00:53:10,720
people that might not

00:53:06,480 --> 00:53:13,040
um have the most um convenient access to

00:53:10,720 --> 00:53:15,040
uh i don't know the data that they have

00:53:13,040 --> 00:53:17,680
been uh generating all the data

00:53:15,040 --> 00:53:19,440
digital banks services that they have uh

00:53:17,680 --> 00:53:21,280
have been put out in the market

00:53:19,440 --> 00:53:23,280
or yeah i'm just thinking in this

00:53:21,280 --> 00:53:26,640
direction i don't really have a

00:53:23,280 --> 00:53:29,839
a an answer yet just maybe bring

00:53:26,640 --> 00:53:29,839
questions to the group

00:53:31,119 --> 00:53:34,960
that's okay and maybe we could move on

00:53:33,520 --> 00:53:37,520
to the next one and we still have two

00:53:34,960 --> 00:53:39,920
more scenarios and not too much time so

00:53:37,520 --> 00:53:42,000
what do you think saw are some solutions

00:53:39,920 --> 00:53:44,559
or recommendations to the

00:53:42,000 --> 00:53:47,040
ai ethical issues that you all have been

00:53:44,559 --> 00:53:49,839
able to identify about

00:53:47,040 --> 00:53:51,040
and we could go with bennett whoever

00:53:49,839 --> 00:53:52,880
wants to go

00:53:51,040 --> 00:53:54,160
clara i'm also i also recognize that you

00:53:52,880 --> 00:53:55,760
sounded like you were trying to say

00:53:54,160 --> 00:53:57,200
something just before we switched over

00:53:55,760 --> 00:53:58,319
to k so i'm wondering if you had another

00:53:57,200 --> 00:54:00,960
thought you wanna

00:53:58,319 --> 00:54:01,839
you wanted to throw out no no i was i

00:54:00,960 --> 00:54:04,400
was just

00:54:01,839 --> 00:54:05,119
i wanted to just reply to brenna to say

00:54:04,400 --> 00:54:08,000
that uh

00:54:05,119 --> 00:54:08,960
yeah i was in agreement in terms of you

00:54:08,000 --> 00:54:11,760
know the root

00:54:08,960 --> 00:54:12,319
causes of you know what we are talking

00:54:11,760 --> 00:54:17,520
about

00:54:12,319 --> 00:54:20,880
and you know questions around

00:54:17,520 --> 00:54:21,680
um the way in which uh our global

00:54:20,880 --> 00:54:25,839
capital

00:54:21,680 --> 00:54:29,520
capitalist system fails to take care of

00:54:25,839 --> 00:54:32,720
people and therefore how ai is really

00:54:29,520 --> 00:54:33,440
simply the sort of replication of that

00:54:32,720 --> 00:54:36,319
model

00:54:33,440 --> 00:54:36,319
in many ways

00:54:41,680 --> 00:54:44,719
[Music]

00:54:47,599 --> 00:54:53,040
the next question what do you think

00:54:50,640 --> 00:54:53,760
are some solutions or recommendations to

00:54:53,040 --> 00:54:57,280
the

00:54:53,760 --> 00:54:57,280
ai ethical issues identify

00:54:57,359 --> 00:55:00,880
are we doing all three scenarios or we i

00:54:59,599 --> 00:55:02,319
thought we're just doing one but if are

00:55:00,880 --> 00:55:05,280
we doing all three is that what

00:55:02,319 --> 00:55:07,200
you're wanting us to do with the time we

00:55:05,280 --> 00:55:09,359
could do we could i think we have time

00:55:07,200 --> 00:55:10,720
for one more but if you want you could i

00:55:09,359 --> 00:55:13,040
could read out the other two and then

00:55:10,720 --> 00:55:15,040
you want to do it as a collective

00:55:13,040 --> 00:55:17,440
is that what you want to do i mean i'm

00:55:15,040 --> 00:55:19,119
fine with whatever the group decides

00:55:17,440 --> 00:55:20,880
i don't know i was just thinking like

00:55:19,119 --> 00:55:21,280
maybe the solutions are going to be a

00:55:20,880 --> 00:55:22,880
little

00:55:21,280 --> 00:55:24,160
overlapping but maybe not i don't know

00:55:22,880 --> 00:55:26,799
if you all are familiar with the

00:55:24,160 --> 00:55:26,799
scenarios

00:55:27,280 --> 00:55:31,599
i could go ahead and read out the

00:55:28,640 --> 00:55:33,359
scenarios we could go with deep fake

00:55:31,599 --> 00:55:34,720
and we could i could read out mental

00:55:33,359 --> 00:55:36,160
health and then

00:55:34,720 --> 00:55:38,240
we could go with the questions because

00:55:36,160 --> 00:55:43,440
we only have the questions are the same

00:55:38,240 --> 00:55:47,280
so we have an elected politician

00:55:43,440 --> 00:55:48,480
whose primary he asked that strategy is

00:55:47,280 --> 00:55:51,280
to use

00:55:48,480 --> 00:55:52,240
social media to post information during

00:55:51,280 --> 00:55:55,280
the pandemic

00:55:52,240 --> 00:55:57,440
and a hacker next

00:55:55,280 --> 00:55:59,760
and machine learn and in ai machine

00:55:57,440 --> 00:56:01,760
learning and former developer for a big

00:55:59,760 --> 00:56:03,440
tech company decides to hack

00:56:01,760 --> 00:56:06,240
the politician's twitter account and

00:56:03,440 --> 00:56:08,559
post the video impersonating him

00:56:06,240 --> 00:56:10,319
the video simulates a secret recording

00:56:08,559 --> 00:56:11,520
and showcases the politician making

00:56:10,319 --> 00:56:13,760
statements that

00:56:11,520 --> 00:56:17,839
immigrants are harming their country and

00:56:13,760 --> 00:56:17,839
should be expelled

00:56:18,160 --> 00:56:22,640
uh the quality of this software used

00:56:21,359 --> 00:56:24,720
reproduces

00:56:22,640 --> 00:56:25,920
the night of march 3rd and by morning

00:56:24,720 --> 00:56:27,760
over 5000

00:56:25,920 --> 00:56:29,680
residents gathered in the outskirts of a

00:56:27,760 --> 00:56:30,960
neighborhood inhabited predominantly by

00:56:29,680 --> 00:56:32,960
immigrants

00:56:30,960 --> 00:56:36,319
so that's one scenario and then the

00:56:32,960 --> 00:56:36,319
second scenario is

00:56:39,680 --> 00:56:44,880
but he's a 17 year old originally from

00:56:42,400 --> 00:56:46,319
grand federic

00:56:44,880 --> 00:56:47,839
famous for their textile rooms and

00:56:46,319 --> 00:56:49,520
frequent internet pockets he's a full

00:56:47,839 --> 00:56:51,280
scholarship to study computer science

00:56:49,520 --> 00:56:52,720
at the university of albania uh this

00:56:51,280 --> 00:56:54,240
would require him to move five thousand

00:56:52,720 --> 00:56:56,799
miles away from the town to the

00:56:54,240 --> 00:56:59,040
cosmopolitan city of elbonia

00:56:56,799 --> 00:57:00,640
uh due to the poet 19 pandemic and the

00:56:59,040 --> 00:57:02,160
strict lockdown restrictions his

00:57:00,640 --> 00:57:03,440
university has decided to make all

00:57:02,160 --> 00:57:05,839
lectures online

00:57:03,440 --> 00:57:07,839
but unlike his peers decides to stay as

00:57:05,839 --> 00:57:10,240
the unreliable internet in his hometown

00:57:07,839 --> 00:57:12,960
significantly have his regular

00:57:10,240 --> 00:57:14,559
university schedule the rigorous online

00:57:12,960 --> 00:57:16,400
engagements and lack of social

00:57:14,559 --> 00:57:17,520
gatherings have taken a massive toll on

00:57:16,400 --> 00:57:19,280
embarks mental

00:57:17,520 --> 00:57:21,599
well-being and causes has grades to

00:57:19,280 --> 00:57:23,680
decline one day a friend of baths

00:57:21,599 --> 00:57:24,319
introduces him to an online community of

00:57:23,680 --> 00:57:26,000
safe

00:57:24,319 --> 00:57:28,079
trusted resources for giving and

00:57:26,000 --> 00:57:29,920
receiving emotional support

00:57:28,079 --> 00:57:31,520
the interface is governed by a chatbot

00:57:29,920 --> 00:57:33,520
part by ai and proposes

00:57:31,520 --> 00:57:36,240
a sophisticated and mature set of safety

00:57:33,520 --> 00:57:37,760
measures informed by the experience of

00:57:36,240 --> 00:57:39,599
mental health professionals

00:57:37,760 --> 00:57:41,119
and aligned with online best practices

00:57:39,599 --> 00:57:42,240
for building safety into social

00:57:41,119 --> 00:57:44,400
environments

00:57:42,240 --> 00:57:45,599
that address the needs of vulnerable

00:57:44,400 --> 00:57:48,000
user populations

00:57:45,599 --> 00:57:49,839
at a fraction of the cost of any other

00:57:48,000 --> 00:57:52,480
available solution

00:57:49,839 --> 00:57:54,799
uh so these are the three scenarios and

00:57:52,480 --> 00:57:54,799
yes

00:58:04,400 --> 00:58:08,720
um just personally i i see the

00:58:07,920 --> 00:58:11,760
politician

00:58:08,720 --> 00:58:14,000
deep fake scenario bringing up

00:58:11,760 --> 00:58:14,799
more ethics issues than the mental

00:58:14,000 --> 00:58:18,480
health one

00:58:14,799 --> 00:58:19,040
but but um it's also the harder one to

00:58:18,480 --> 00:58:22,000
solve

00:58:19,040 --> 00:58:25,200
i think there's no easy solutions but in

00:58:22,000 --> 00:58:25,200
the deep fake scenario

00:58:33,200 --> 00:58:38,319
jumping in uh yeah i think it's really

00:58:37,040 --> 00:58:41,359
interesting and quite

00:58:38,319 --> 00:58:44,480
quite uh different from

00:58:41,359 --> 00:58:46,559
the other two scenarios um in some

00:58:44,480 --> 00:58:49,839
respects i wonder

00:58:46,559 --> 00:58:53,200
i mean um in terms of

00:58:49,839 --> 00:58:54,799
yeah i tend not to like uh to talk about

00:58:53,200 --> 00:58:58,160
solutions

00:58:54,799 --> 00:59:01,200
um to very

00:58:58,160 --> 00:59:04,480
complex issues and i think all of these

00:59:01,200 --> 00:59:06,720
are in a sense that

00:59:04,480 --> 00:59:08,000
it's not just about the potential and

00:59:06,720 --> 00:59:11,200
what deep fake

00:59:08,000 --> 00:59:14,240
does in and of itself but also

00:59:11,200 --> 00:59:15,040
the complexities around people feeling

00:59:14,240 --> 00:59:18,319
like they

00:59:15,040 --> 00:59:21,760
can and you know um

00:59:18,319 --> 00:59:23,200
you know play about with with news in

00:59:21,760 --> 00:59:26,400
this way that have

00:59:23,200 --> 00:59:29,119
clearly significant uh

00:59:26,400 --> 00:59:30,799
real stake consequences for people and

00:59:29,119 --> 00:59:33,040
people's lives

00:59:30,799 --> 00:59:34,799
um but i think in some respects some of

00:59:33,040 --> 00:59:38,960
the solutions should be

00:59:34,799 --> 00:59:42,480
uh also around helping people

00:59:38,960 --> 00:59:44,000
to somewhat identify

00:59:42,480 --> 00:59:46,160
i mean i know that one of the things

00:59:44,000 --> 00:59:46,960
about deep fakes is that it's really

00:59:46,160 --> 00:59:50,160
difficult

00:59:46,960 --> 00:59:52,720
to actually they are so well made that

00:59:50,160 --> 00:59:55,520
they constitute a real problem from a

00:59:52,720 --> 00:59:58,559
low innovation perspective as well uh in

00:59:55,520 --> 01:00:01,760
terms of how you even get people done

00:59:58,559 --> 01:00:02,880
for that uh but but i guess there is

01:00:01,760 --> 01:00:04,880
always

01:00:02,880 --> 01:00:05,920
you know there are always skills that we

01:00:04,880 --> 01:00:09,200
can develop

01:00:05,920 --> 01:00:09,839
as humans in terms of assessing and

01:00:09,200 --> 01:00:12,960
analyze

01:00:09,839 --> 01:00:15,760
the news that we are being given

01:00:12,960 --> 01:00:17,599
in context in different ways and i think

01:00:15,760 --> 01:00:18,079
one of the responses to some of these

01:00:17,599 --> 01:00:22,240
issues

01:00:18,079 --> 01:00:25,280
should involve uh helping

01:00:22,240 --> 01:00:26,640
uh people you know uh developing

01:00:25,280 --> 01:00:29,359
capabilities

01:00:26,640 --> 01:00:31,040
to really critically look at the range

01:00:29,359 --> 01:00:31,920
of news that they are presenting the

01:00:31,040 --> 01:00:35,119
context

01:00:31,920 --> 01:00:35,680
how this is done and in relation to you

01:00:35,119 --> 01:00:43,200
know

01:00:35,680 --> 01:00:46,559
an ecosystem of news items and media

01:00:43,200 --> 01:00:48,720
i think that i agree with all y'all

01:00:46,559 --> 01:00:49,599
effort and then uh if we're talking

01:00:48,720 --> 01:00:52,720
about the pr

01:00:49,599 --> 01:00:54,480
one the i mean the first thing is like

01:00:52,720 --> 01:00:56,160
why was he even hacked like that's an

01:00:54,480 --> 01:00:58,960
ethical failure of this entire

01:00:56,160 --> 01:01:00,720
campaign and team um so like there's

01:00:58,960 --> 01:01:02,400
like the human failure of like

01:01:00,720 --> 01:01:04,480
and like how did that happen and then

01:01:02,400 --> 01:01:05,760
there's twitter's ethical responsibility

01:01:04,480 --> 01:01:07,839
and i feel like like

01:01:05,760 --> 01:01:09,119
um that's probably where like i don't i

01:01:07,839 --> 01:01:11,280
don't know enough of the technical

01:01:09,119 --> 01:01:13,119
how it's going to work but i feel like

01:01:11,280 --> 01:01:14,640
that there's also going to be

01:01:13,119 --> 01:01:15,920
the companies themselves if they're

01:01:14,640 --> 01:01:17,599
going to allow if they they're going to

01:01:15,920 --> 01:01:18,559
need to harness the machine learning and

01:01:17,599 --> 01:01:20,319
i don't know if that's going to be

01:01:18,559 --> 01:01:21,680
possible but like why can't the machines

01:01:20,319 --> 01:01:25,040
identify their own

01:01:21,680 --> 01:01:28,559
their own work so like that somehow uh

01:01:25,040 --> 01:01:30,880
that their machines the deep faking

01:01:28,559 --> 01:01:32,480
would be harder to actually go through

01:01:30,880 --> 01:01:33,200
the machines that are running twitter i

01:01:32,480 --> 01:01:34,160
don't know how that would work

01:01:33,200 --> 01:01:36,319
technically

01:01:34,160 --> 01:01:37,520
and then um i think that the other thing

01:01:36,319 --> 01:01:39,520
is like why

01:01:37,520 --> 01:01:41,200
you know it's again going back to the

01:01:39,520 --> 01:01:43,599
hack like as soon as the

01:01:41,200 --> 01:01:44,400
accounts are compromised why did it take

01:01:43,599 --> 01:01:46,559
you know

01:01:44,400 --> 01:01:48,079
so long it took 12 hours it just says

01:01:46,559 --> 01:01:49,839
the night right so

01:01:48,079 --> 01:01:51,680
in 12 hours there should have been

01:01:49,839 --> 01:01:52,000
immediate shutdown just as if there was

01:01:51,680 --> 01:01:55,039
a

01:01:52,000 --> 01:01:58,160
fire or a toxic waste dump or a nuclear

01:01:55,039 --> 01:01:58,640
accident um so somehow like the ethicals

01:01:58,160 --> 01:02:00,960
are

01:01:58,640 --> 01:02:02,240
not only just that somebody and then i

01:02:00,960 --> 01:02:02,799
would say the other thing is a failure

01:02:02,240 --> 01:02:06,079
of like

01:02:02,799 --> 01:02:06,720
computer and programming education and i

01:02:06,079 --> 01:02:08,160
was like

01:02:06,720 --> 01:02:10,160
i mean i know this is going to happen

01:02:08,160 --> 01:02:13,839
but like you shouldn't be

01:02:10,160 --> 01:02:15,680
there should be like community like

01:02:13,839 --> 01:02:17,119
support so that ideally won't have

01:02:15,680 --> 01:02:17,839
people like deciding that they're going

01:02:17,119 --> 01:02:19,839
to like

01:02:17,839 --> 01:02:21,839
just hack into people's accounts i know

01:02:19,839 --> 01:02:24,000
that's a little bit naive but

01:02:21,839 --> 01:02:25,440
so through i'd say that i guess i don't

01:02:24,000 --> 01:02:27,359
have enough technical information about

01:02:25,440 --> 01:02:28,319
how ai works right because these feel

01:02:27,359 --> 01:02:31,839
like more

01:02:28,319 --> 01:02:33,440
political um and regulatory things and

01:02:31,839 --> 01:02:35,200
especially because it's explicit about

01:02:33,440 --> 01:02:37,359
twitter like it just says social media

01:02:35,200 --> 01:02:40,160
but then it says it happened on twitter

01:02:37,359 --> 01:02:40,720
um so i would say like there's certain

01:02:40,160 --> 01:02:42,559
things that

01:02:40,720 --> 01:02:43,839
in terms of the regulation of twitter

01:02:42,559 --> 01:02:48,400
that might be

01:02:43,839 --> 01:02:49,760
more code so i hope that makes sense

01:02:48,400 --> 01:02:51,119
so are you saying there should be

01:02:49,760 --> 01:02:52,799
regulation of the time did you say that

01:02:51,119 --> 01:02:55,280
there should be a media blocker is that

01:02:52,799 --> 01:02:55,280
what you said

01:02:56,160 --> 01:03:00,000
say that the what i was saying was that

01:02:58,720 --> 01:03:04,960
like

01:03:00,000 --> 01:03:04,960
you mentioned media again you mentioned

01:03:05,359 --> 01:03:10,240
no i didn't say media blockade i said

01:03:07,760 --> 01:03:12,559
the regulation of twitter so for example

01:03:10,240 --> 01:03:14,000
like if again it doesn't say which

01:03:12,559 --> 01:03:17,359
country this is in

01:03:14,000 --> 01:03:19,359
um but like there is

01:03:17,359 --> 01:03:21,599
ideally enough we would be able to have

01:03:19,359 --> 01:03:24,000
more democratic control over these

01:03:21,599 --> 01:03:25,200
uh large corporations that have you know

01:03:24,000 --> 01:03:28,480
taken over the

01:03:25,200 --> 01:03:30,079
public sphere um you know but that again

01:03:28,480 --> 01:03:31,839
is very contextual to the different

01:03:30,079 --> 01:03:32,720
countries you're talking about right so

01:03:31,839 --> 01:03:35,760
like it's

01:03:32,720 --> 01:03:37,359
i don't know what the um solution is but

01:03:35,760 --> 01:03:39,760
in terms of like taking down

01:03:37,359 --> 01:03:41,119
fake like fake posts like that is

01:03:39,760 --> 01:03:41,920
something that can be part of the

01:03:41,119 --> 01:03:43,760
corporate

01:03:41,920 --> 01:03:44,960
uh community guidelines or pr or

01:03:43,760 --> 01:03:46,319
participation

01:03:44,960 --> 01:03:48,160
you don't you know and i think we're

01:03:46,319 --> 01:03:49,039
seeing that right you don't get to

01:03:48,160 --> 01:03:50,640
participate

01:03:49,039 --> 01:03:52,960
in this way if you violate this code of

01:03:50,640 --> 01:03:53,520
conduct and in this scenario these are

01:03:52,960 --> 01:03:56,319
failures

01:03:53,520 --> 01:03:57,760
on in first on you know of the fact that

01:03:56,319 --> 01:03:59,039
he's even allowed to be hacked and i

01:03:57,760 --> 01:04:00,559
know that we're going to continue to be

01:03:59,039 --> 01:04:00,880
hacked but it's not talking about a data

01:04:00,559 --> 01:04:02,799
leak

01:04:00,880 --> 01:04:04,400
it's not talking it's like his account

01:04:02,799 --> 01:04:07,680
was hacked and like

01:04:04,400 --> 01:04:08,880
that is a design failure um and also a

01:04:07,680 --> 01:04:10,799
social failure and then

01:04:08,880 --> 01:04:12,319
in terms of taking down something that's

01:04:10,799 --> 01:04:14,319
something that

01:04:12,319 --> 01:04:16,319
humans have control over um and then

01:04:14,319 --> 01:04:17,839
what i was trying to say was like that

01:04:16,319 --> 01:04:20,000
there's not going to be the ability of

01:04:17,839 --> 01:04:20,400
humans to monitor all the deep fakes so

01:04:20,000 --> 01:04:22,240
like

01:04:20,400 --> 01:04:24,079
i don't know where the ethical line is

01:04:22,240 --> 01:04:25,920
going to be but it seems like

01:04:24,079 --> 01:04:27,680
if we're able to have the machines make

01:04:25,920 --> 01:04:29,599
the deep bakes i would hope we could get

01:04:27,680 --> 01:04:31,039
the machines to identify the deep fakes

01:04:29,599 --> 01:04:32,880
down the road i don't think it's there

01:04:31,039 --> 01:04:34,400
yet does that answer your question i

01:04:32,880 --> 01:04:34,799
wasn't saying media blockade i don't

01:04:34,400 --> 01:04:36,160
know if

01:04:34,799 --> 01:04:37,920
i'm talking really fast because that's

01:04:36,160 --> 01:04:39,520
happening

01:04:37,920 --> 01:04:41,440
that what about i also have patchy

01:04:39,520 --> 01:04:42,880
internet but actually to your question

01:04:41,440 --> 01:04:45,920
there are a lot of countries

01:04:42,880 --> 01:04:47,440
that are taking uh legislation in their

01:04:45,920 --> 01:04:48,559
own hands when it comes to regulation

01:04:47,440 --> 01:04:52,079
india is one of them

01:04:48,559 --> 01:04:53,119
we recently came out with an i.t revised

01:04:52,079 --> 01:04:55,039
id rules

01:04:53,119 --> 01:04:56,480
australia recently did so there are a

01:04:55,039 --> 01:04:58,079
lot of people who are actually taking

01:04:56,480 --> 01:05:00,640
regulation based on their

01:04:58,079 --> 01:05:01,680
social and cultural context thank you so

01:05:00,640 --> 01:05:04,480
much for

01:05:01,680 --> 01:05:06,160
your comments uh yeah and i'll leave it

01:05:04,480 --> 01:05:08,319
open everyone we have around 10 more

01:05:06,160 --> 01:05:10,720
minutes i'm just thinking

01:05:08,319 --> 01:05:13,039
if do we have john how long do we have

01:05:10,720 --> 01:05:14,559
we we started at what time exactly

01:05:13,039 --> 01:05:16,400
we do have some time so anyone else who

01:05:14,559 --> 01:05:19,039
has any comments please go ahead

01:05:16,400 --> 01:05:21,760
i'm i'm not i'm not sure about how much

01:05:19,039 --> 01:05:21,760
time we have left

01:05:25,760 --> 01:05:29,359
can we dig into the mental health

01:05:28,240 --> 01:05:32,799
scenario

01:05:29,359 --> 01:05:34,240
um like i've got that sure i was having

01:05:32,799 --> 01:05:35,920
a hard time

01:05:34,240 --> 01:05:37,599
identify the ethics issues because it's

01:05:35,920 --> 01:05:40,960
very the case

01:05:37,599 --> 01:05:45,039
is very positively framed like it says

01:05:40,960 --> 01:05:47,680
you know um trusted resources

01:05:45,039 --> 01:05:49,119
um you know sophisticated mature set of

01:05:47,680 --> 01:05:50,000
safety measures informed by the

01:05:49,119 --> 01:05:53,039
experience of mental health

01:05:50,000 --> 01:05:54,799
professionals so on the

01:05:53,039 --> 01:05:56,640
first glance i read it as a very

01:05:54,799 --> 01:05:59,200
positive example

01:05:56,640 --> 01:06:00,079
with very few ethical questions but now

01:05:59,200 --> 01:06:01,839
i had a

01:06:00,079 --> 01:06:03,200
chance to think about it so in mental

01:06:01,839 --> 01:06:05,119
health there's a difference between like

01:06:03,200 --> 01:06:07,599
the um

01:06:05,119 --> 01:06:08,400
clinicians who kind of treat the issues

01:06:07,599 --> 01:06:11,680
as medical

01:06:08,400 --> 01:06:12,880
problem and the actual um

01:06:11,680 --> 01:06:14,720
people suffering mental health and

01:06:12,880 --> 01:06:17,680
there's kind of all kinds of

01:06:14,720 --> 01:06:18,000
survivor groups you know decades ago at

01:06:17,680 --> 01:06:20,240
least

01:06:18,000 --> 01:06:21,280
in canada the people used to be put in

01:06:20,240 --> 01:06:22,559
institutions

01:06:21,280 --> 01:06:24,480
and that was dragged by the medical

01:06:22,559 --> 01:06:28,240
professionals so if

01:06:24,480 --> 01:06:31,119
the ai system is constructed

01:06:28,240 --> 01:06:32,400
by the professionals that sometimes

01:06:31,119 --> 01:06:34,240
becomes a problem

01:06:32,400 --> 01:06:37,039
for the individuals with mental health

01:06:34,240 --> 01:06:38,720
issues it's like um in the disability

01:06:37,039 --> 01:06:41,359
community they say nothing about us

01:06:38,720 --> 01:06:42,720
without us and i'm not seeing that piece

01:06:41,359 --> 01:06:46,079
reflected here

01:06:42,720 --> 01:06:48,240
where it's it would say hey this was

01:06:46,079 --> 01:06:50,559
built on you know consulting with the

01:06:48,240 --> 01:06:51,359
community of people who are experiencing

01:06:50,559 --> 01:06:55,039
mental health

01:06:51,359 --> 01:06:56,720
issues it was more it's framed more as

01:06:55,039 --> 01:06:58,640
coming from the professionals so that

01:06:56,720 --> 01:07:01,280
can be an ethical issue

01:06:58,640 --> 01:07:02,160
that when the professionals construct a

01:07:01,280 --> 01:07:04,799
system

01:07:02,160 --> 01:07:06,240
and this applies to like technology

01:07:04,799 --> 01:07:07,440
professionals as well as mental health

01:07:06,240 --> 01:07:10,400
professionals

01:07:07,440 --> 01:07:11,200
and you're not it's not coming you know

01:07:10,400 --> 01:07:12,960
um

01:07:11,200 --> 01:07:14,319
bottom up from the user the needs are

01:07:12,960 --> 01:07:16,640
coming button for the user

01:07:14,319 --> 01:07:18,319
you develop a system that is driven top

01:07:16,640 --> 01:07:21,200
down on a particular

01:07:18,319 --> 01:07:22,079
set of interests always right um so

01:07:21,200 --> 01:07:24,240
that's maybe

01:07:22,079 --> 01:07:25,440
the issue i'm identifying in the mental

01:07:24,240 --> 01:07:29,039
health

01:07:25,440 --> 01:07:32,319
um kind of that the ai system is based

01:07:29,039 --> 01:07:36,079
more on professional opinion than

01:07:32,319 --> 01:07:36,079
end user community

01:07:38,240 --> 01:07:44,400
okay that makes sense is that someone

01:07:41,119 --> 01:07:44,400
who said something on the chat

01:07:47,920 --> 01:07:51,440
i was just appreciating what rich nish

01:07:49,680 --> 01:07:53,359
was saying about um

01:07:51,440 --> 01:07:54,880
you know this this expression of nothing

01:07:53,359 --> 01:07:57,599
about us without us and

01:07:54,880 --> 01:07:58,319
and i was actually wondering um rena i

01:07:57,599 --> 01:08:00,400
felt like

01:07:58,319 --> 01:08:02,319
the the point that is making is

01:08:00,400 --> 01:08:03,280
ultimately fundamentally a question of

01:08:02,319 --> 01:08:05,039
power and i feel that

01:08:03,280 --> 01:08:06,640
that relates to what you you and clara

01:08:05,039 --> 01:08:08,400
were saying around

01:08:06,640 --> 01:08:10,559
in a way i was trying to summarize them

01:08:08,400 --> 01:08:13,440
with these two sort of like off-yellow

01:08:10,559 --> 01:08:14,960
post-it notes um but kind of thinking

01:08:13,440 --> 01:08:17,279
about how

01:08:14,960 --> 01:08:18,799
essentially the sort of artificial

01:08:17,279 --> 01:08:19,679
intelligence ethical questions are kind

01:08:18,799 --> 01:08:21,839
of compounding

01:08:19,679 --> 01:08:23,040
traditional ethical questions and that

01:08:21,839 --> 01:08:24,799
it's fundamentally

01:08:23,040 --> 01:08:26,560
sort of amplifying traditional questions

01:08:24,799 --> 01:08:27,040
of power structures and relationships

01:08:26,560 --> 01:08:28,560
between

01:08:27,040 --> 01:08:30,719
ultimately people governments and

01:08:28,560 --> 01:08:32,960
corporations and and how

01:08:30,719 --> 01:08:33,759
how decisions are ultimately mediated

01:08:32,960 --> 01:08:36,239
and i i

01:08:33,759 --> 01:08:36,880
was wondering yeah how you all felt

01:08:36,239 --> 01:08:39,839
about

01:08:36,880 --> 01:08:39,839
how you felt about that

01:08:43,120 --> 01:08:47,359
yeah i mean personally i totally agree

01:08:46,719 --> 01:08:50,640
with

01:08:47,359 --> 01:08:52,880
you know what you've just said uh

01:08:50,640 --> 01:08:54,640
in terms of you know what what was the

01:08:52,880 --> 01:08:57,759
process of making this

01:08:54,640 --> 01:09:01,279
ai and who participated but i still

01:08:57,759 --> 01:09:04,719
think that ai is not

01:09:01,279 --> 01:09:06,560
as cool as we are making it to be so i

01:09:04,719 --> 01:09:09,600
would be very sad

01:09:06,560 --> 01:09:13,120
to be living in a world where mental

01:09:09,600 --> 01:09:13,120
health issue is dealt

01:09:13,440 --> 01:09:20,480
purely mediated by a technology

01:09:17,600 --> 01:09:22,480
which will inevitably always have a

01:09:20,480 --> 01:09:25,839
number of shortcomings

01:09:22,480 --> 01:09:26,880
um i believe i believe you can get rid

01:09:25,839 --> 01:09:30,319
of issues

01:09:26,880 --> 01:09:33,600
such as bias and and even though

01:09:30,319 --> 01:09:34,560
perhaps you know in the pandemic a sort

01:09:33,600 --> 01:09:37,920
of chatbot

01:09:34,560 --> 01:09:39,040
um could go some way in providing some

01:09:37,920 --> 01:09:42,480
supports

01:09:39,040 --> 01:09:45,040
um i i would be worried that

01:09:42,480 --> 01:09:47,440
you know that would sort of sanction for

01:09:45,040 --> 01:09:49,679
the ai to replace all

01:09:47,440 --> 01:09:50,960
forms of social interactions that are

01:09:49,679 --> 01:09:53,040
essential

01:09:50,960 --> 01:09:54,320
not just for mental health i think for

01:09:53,040 --> 01:09:58,880
all sorts of

01:09:54,320 --> 01:10:02,000
um social issues but also for social joy

01:09:58,880 --> 01:10:04,640
i think for the good things of life the

01:10:02,000 --> 01:10:08,159
the things that we want to share and you

01:10:04,640 --> 01:10:10,150
know the things that make life rich

01:10:08,159 --> 01:10:11,920
because it's unpredictable

01:10:10,150 --> 01:10:14,960
[Music]

01:10:11,920 --> 01:10:17,040
i like that i think echoing that uh just

01:10:14,960 --> 01:10:18,239
like the other things that i read within

01:10:17,040 --> 01:10:20,719
the ethical things

01:10:18,239 --> 01:10:21,360
i i see also is like social not just the

01:10:20,719 --> 01:10:24,000
ai

01:10:21,360 --> 01:10:25,840
like you know the fact that like the

01:10:24,000 --> 01:10:26,640
term rigorous online engagements on

01:10:25,840 --> 01:10:28,320
university

01:10:26,640 --> 01:10:30,000
like i feel like that also is

01:10:28,320 --> 01:10:31,360
questionable like what is the you know

01:10:30,000 --> 01:10:33,440
who's deciding what

01:10:31,360 --> 01:10:34,400
what is determining rigor and what is

01:10:33,440 --> 01:10:37,520
determining like

01:10:34,400 --> 01:10:39,920
you know like effective learning and um

01:10:37,520 --> 01:10:41,040
you know that unto itself we were having

01:10:39,920 --> 01:10:43,760
huge problems in

01:10:41,040 --> 01:10:45,520
this academia with student mental health

01:10:43,760 --> 01:10:46,960
before covid it was just really not

01:10:45,520 --> 01:10:48,719
discussed as much but

01:10:46,960 --> 01:10:50,640
frontline workers of mental health are

01:10:48,719 --> 01:10:53,120
often adjunct professors

01:10:50,640 --> 01:10:54,719
and university people right um so like

01:10:53,120 --> 01:10:56,320
there's like that kind of component of

01:10:54,719 --> 01:10:58,400
like why is it so rigorous

01:10:56,320 --> 01:10:59,520
that it's taking this massive toll even

01:10:58,400 --> 01:11:01,600
regardless of the

01:10:59,520 --> 01:11:03,120
isolation and then yeah i think that the

01:11:01,600 --> 01:11:05,040
one part at the end

01:11:03,120 --> 01:11:06,719
where it's like the addressing the needs

01:11:05,040 --> 01:11:08,480
of vulnerable user populations at the

01:11:06,719 --> 01:11:09,600
fraction of cost of any other solution

01:11:08,480 --> 01:11:12,320
it goes back to like

01:11:09,600 --> 01:11:13,280
why do when why is the cost part of the

01:11:12,320 --> 01:11:17,120
equation like

01:11:13,280 --> 01:11:20,000
that who is um running this ai

01:11:17,120 --> 01:11:22,000
or this chatbot is unclear to me is it a

01:11:20,000 --> 01:11:22,800
public service is it part of a national

01:11:22,000 --> 01:11:27,040
health program

01:11:22,800 --> 01:11:31,199
is it a for-profit um like therapies

01:11:27,040 --> 01:11:33,440
but it's under is those are kind of

01:11:31,199 --> 01:11:34,320
variables that are unclear but also have

01:11:33,440 --> 01:11:41,840
a whole host of

01:11:34,320 --> 01:11:41,840
ethical problems

01:11:41,920 --> 01:11:45,199
it's so hard to like think and watch all

01:11:43,679 --> 01:11:46,159
the little things move on the mirror

01:11:45,199 --> 01:11:55,840
like

01:11:46,159 --> 01:11:55,840
it's like

01:11:58,800 --> 01:12:05,600
i'm just switching back to the deep fake

01:12:02,159 --> 01:12:09,360
politician scenario um

01:12:05,600 --> 01:12:12,640
anybody know if people are regulating

01:12:09,360 --> 01:12:15,440
deep fakes in any way like i know um uh

01:12:12,640 --> 01:12:16,640
um as you mentioned can regulation of

01:12:15,440 --> 01:12:19,920
social media which i

01:12:16,640 --> 01:12:22,800
don't think of as the same thing like

01:12:19,920 --> 01:12:24,800
i would think deep fakes are really

01:12:22,800 --> 01:12:29,440
really hard to

01:12:24,800 --> 01:12:32,719
regulate um in terms of

01:12:29,440 --> 01:12:34,080
uh um yeah the only things i've heard is

01:12:32,719 --> 01:12:36,480
um because

01:12:34,080 --> 01:12:37,760
a lot of the fake technology was used in

01:12:36,480 --> 01:12:41,280
is is is used

01:12:37,760 --> 01:12:43,679
in pornography like they put the

01:12:41,280 --> 01:12:45,679
face of a famous actress on a different

01:12:43,679 --> 01:12:49,280
body and and so on and so on

01:12:45,679 --> 01:12:50,880
um i'm a little bit on people trying to

01:12:49,280 --> 01:12:53,120
regulate that piece

01:12:50,880 --> 01:12:54,480
but i have not come across any examples

01:12:53,120 --> 01:12:56,960
where

01:12:54,480 --> 01:12:58,239
the people are jurisdictions are trying

01:12:56,960 --> 01:13:04,560
to regulate deep fakes

01:12:58,239 --> 01:13:08,560
in terms of politics um

01:13:04,560 --> 01:13:11,600
i just put in the chat um on this one

01:13:08,560 --> 01:13:15,199
um i want to see this uh lecture well

01:13:11,600 --> 01:13:18,000
uh lillian is in my in my university

01:13:15,199 --> 01:13:20,400
so she's a researcher and she's exactly

01:13:18,000 --> 01:13:23,920
looking into the challenges of

01:13:20,400 --> 01:13:27,600
regulating uh fakes and that's a

01:13:23,920 --> 01:13:31,440
that's a link to um a lecture

01:13:27,600 --> 01:13:34,719
uh which i i really enjoyed

01:13:31,440 --> 01:13:36,480
okay we have less than a minute actually

01:13:34,719 --> 01:13:38,080
at the moment but this has been great

01:13:36,480 --> 01:13:41,120
thank you so much uh

01:13:38,080 --> 01:13:43,040
one of the one of our inspirations from

01:13:41,120 --> 01:13:44,480
going ahead with the defect scenario

01:13:43,040 --> 01:13:47,760
was the one that flew around with tom

01:13:44,480 --> 01:13:50,719
cruise playing golf and the missing coin

01:13:47,760 --> 01:13:52,719
uh so we really thought that was very

01:13:50,719 --> 01:13:54,880
edgy and yeah we were able to get

01:13:52,719 --> 01:13:56,719
conversation up there so

01:13:54,880 --> 01:13:58,400
thank you so much and with regards to

01:13:56,719 --> 01:13:59,040
the mental health aspect we kept it

01:13:58,400 --> 01:14:02,320
ambiguous

01:13:59,040 --> 01:14:04,560
and the the end point was basically

01:14:02,320 --> 01:14:06,000
that mental health is so expensive

01:14:04,560 --> 01:14:06,400
especially in countries like global

01:14:06,000 --> 01:14:09,280
south

01:14:06,400 --> 01:14:10,960
where they don't accommodate the cost so

01:14:09,280 --> 01:14:11,520
the whole idea we were coming from is

01:14:10,960 --> 01:14:13,840
that

01:14:11,520 --> 01:14:14,800
it's it's cheaper than talking to a

01:14:13,840 --> 01:14:17,120
person

01:14:14,800 --> 01:14:18,000
that was just i hope if that makes it

01:14:17,120 --> 01:14:19,600
clear

01:14:18,000 --> 01:14:22,080
but thank you so much everyone this has

01:14:19,600 --> 01:14:22,080
been great

01:14:22,560 --> 01:14:26,239
and the banking scenario probably exists

01:14:25,120 --> 01:14:29,840
in real life somewhere

01:14:26,239 --> 01:14:29,840
i'm sure

01:14:33,440 --> 01:14:38,400
recording in progress hello everyone

01:14:39,679 --> 01:14:43,840
you're back welcome back

01:14:45,280 --> 01:14:49,360
so i think we miss one group's team

01:14:50,320 --> 01:14:53,840
nice yeah

01:14:54,719 --> 01:15:00,400
we honestly john maybe john left and

01:14:56,480 --> 01:15:00,400
he's gonna jump back no john is here

01:15:01,840 --> 01:15:06,400
so good um thank you very much i hope

01:15:04,560 --> 01:15:07,120
the discussion i'm sure the description

01:15:06,400 --> 01:15:09,679
was uh

01:15:07,120 --> 01:15:10,719
good in the subgroup there is no reason

01:15:09,679 --> 01:15:13,840
why not

01:15:10,719 --> 01:15:15,600
um it's a quarter to five normally we

01:15:13,840 --> 01:15:18,159
should close the session

01:15:15,600 --> 01:15:19,120
now uh also considering that the room

01:15:18,159 --> 01:15:21,679
may be used by

01:15:19,120 --> 01:15:23,920
the next session but i'd like a very

01:15:21,679 --> 01:15:27,280
short feedback from your groups maybe

01:15:23,920 --> 01:15:27,760
if you could give us a short insight in

01:15:27,280 --> 01:15:29,280
what to

01:15:27,760 --> 01:15:31,280
discuss that would be fantastic before

01:15:29,280 --> 01:15:34,400
we close um

01:15:31,280 --> 01:15:37,440
so i would um ask

01:15:34,400 --> 01:15:39,040
in the first group who could um give us

01:15:37,440 --> 01:15:42,640
um

01:15:39,040 --> 01:15:42,640
yeah snapshot

01:15:46,800 --> 01:15:51,280
so who is the first group okay i had

01:15:50,000 --> 01:15:54,239
room number one

01:15:51,280 --> 01:15:54,960
for me was this room number one two

01:15:54,239 --> 01:15:58,320
three so man

01:15:54,960 --> 01:16:00,960
you spoke first so you are you are okay

01:15:58,320 --> 01:16:02,480
so okay to have a short feedback thank

01:16:00,960 --> 01:16:03,920
you for those who were in my room for

01:16:02,480 --> 01:16:06,640
the great input

01:16:03,920 --> 01:16:07,040
um so we had a first round actually of

01:16:06,640 --> 01:16:09,840
uh

01:16:07,040 --> 01:16:10,239
uh questions and and reactions on uh on

01:16:09,840 --> 01:16:12,320
the

01:16:10,239 --> 01:16:13,760
whole process of his internet because uh

01:16:12,320 --> 01:16:15,679
we didn't have a lot of details in

01:16:13,760 --> 01:16:16,560
plenary so it was also a good occasion

01:16:15,679 --> 01:16:18,480
for that

01:16:16,560 --> 01:16:20,000
uh and then going to the scenario so we

01:16:18,480 --> 01:16:22,159
didn't have to have time to discuss

01:16:20,000 --> 01:16:23,360
in-depth first scenario the banking

01:16:22,159 --> 01:16:25,920
scenarios

01:16:23,360 --> 01:16:26,480
um but on the first scenario so we the

01:16:25,920 --> 01:16:28,560
issue

01:16:26,480 --> 01:16:29,520
ethical issue we raise on the deep facts

01:16:28,560 --> 01:16:31,920
scenarios

01:16:29,520 --> 01:16:34,400
uh we're mostly related to to lack of

01:16:31,920 --> 01:16:37,840
regulation and the need for a regulated

01:16:34,400 --> 01:16:39,920
regulation framework um also that uh

01:16:37,840 --> 01:16:41,040
this kind of situation the lack of

01:16:39,920 --> 01:16:43,520
actually exist

01:16:41,040 --> 01:16:45,040
the lack of regulation framework creates

01:16:43,520 --> 01:16:48,000
a lack of trust

01:16:45,040 --> 01:16:48,800
so it's a political issue and also we

01:16:48,000 --> 01:16:52,480
have a

01:16:48,800 --> 01:16:55,040
big transparency issue there and

01:16:52,480 --> 01:16:56,800
another one was the education issues and

01:16:55,040 --> 01:16:58,000
the fact that not everyone has a right

01:16:56,800 --> 01:17:01,120
filter

01:16:58,000 --> 01:17:02,960
in a way to to to have a comprehensive

01:17:01,120 --> 01:17:06,000
understanding and full of understanding

01:17:02,960 --> 01:17:08,719
of what they read online

01:17:06,000 --> 01:17:10,480
so i hope that sum up well the what we

01:17:08,719 --> 01:17:14,000
said on the first scenarios

01:17:10,480 --> 01:17:16,000
uh sorry for the shortened sum up

01:17:14,000 --> 01:17:17,760
and on the mental health issue it was

01:17:16,000 --> 01:17:19,120
also interesting because it was more as

01:17:17,760 --> 01:17:22,159
a quality positive

01:17:19,120 --> 01:17:25,600
in a way ethical issues that were raised

01:17:22,159 --> 01:17:26,560
so in this uh situation uh we really see

01:17:25,600 --> 01:17:30,080
that uh

01:17:26,560 --> 01:17:33,120
there is ai a

01:17:30,080 --> 01:17:35,040
also create a positive effect on mental

01:17:33,120 --> 01:17:37,760
health and a community uh

01:17:35,040 --> 01:17:38,480
belonging its effect in a way so that

01:17:37,760 --> 01:17:41,520
was a

01:17:38,480 --> 01:17:43,440
let's say the most quick sum up of the

01:17:41,520 --> 01:17:47,520
discussion

01:17:43,440 --> 01:17:49,920
i hope it's um thank you manon

01:17:47,520 --> 01:17:51,679
thank you man um i would go over to the

01:17:49,920 --> 01:17:54,159
second group but here um

01:17:51,679 --> 01:17:55,440
i have to ask for your help uh who

01:17:54,159 --> 01:17:58,480
moderated

01:17:55,440 --> 01:18:01,040
uh and and could give us um

01:17:58,480 --> 01:18:01,040
a wrap up

01:18:02,960 --> 01:18:06,800
which which was the number i didn't

01:18:04,560 --> 01:18:08,880
realize my own record number

01:18:06,800 --> 01:18:10,239
was it was okay so it's for you so let's

01:18:08,880 --> 01:18:14,239
go join your group

01:18:10,239 --> 01:18:17,600
and then that's a proceeding

01:18:14,239 --> 01:18:19,040
island would you like to to work on it

01:18:17,600 --> 01:18:21,199
because i haven't had you haven't had

01:18:19,040 --> 01:18:25,440
the chance to participate either

01:18:21,199 --> 01:18:28,640
yeah i can i can add some notes and

01:18:25,440 --> 01:18:28,960
then you continue okay uh so we decided

01:18:28,640 --> 01:18:31,520
to

01:18:28,960 --> 01:18:32,719
change a little bit the approach and we

01:18:31,520 --> 01:18:35,600
started to discuss

01:18:32,719 --> 01:18:36,000
uh about the topics that are within

01:18:35,600 --> 01:18:38,960
these

01:18:36,000 --> 01:18:40,080
scenarios so we started to speak about

01:18:38,960 --> 01:18:42,800
the fake

01:18:40,080 --> 01:18:44,640
and we have a very interesting

01:18:42,800 --> 01:18:48,719
conversation with participants

01:18:44,640 --> 01:18:52,000
on how we can approach this topic to

01:18:48,719 --> 01:18:53,760
to citizens and it was suggested for

01:18:52,000 --> 01:18:56,080
example to explain

01:18:53,760 --> 01:18:57,360
more flexible models in a way that

01:18:56,080 --> 01:18:59,520
people can

01:18:57,360 --> 01:19:02,080
feel that they own their data and they

01:18:59,520 --> 01:19:05,679
know what is going to happen with that

01:19:02,080 --> 01:19:08,239
uh it was a remark about uh to take into

01:19:05,679 --> 01:19:11,679
consideration diversity and the context

01:19:08,239 --> 01:19:15,840
and the different cultures and also well

01:19:11,679 --> 01:19:19,360
we have a very strong um input about

01:19:15,840 --> 01:19:22,960
misinformation on how we can uh approach

01:19:19,360 --> 01:19:26,159
uh the propaganda issue in this

01:19:22,960 --> 01:19:30,000
in this topic and so

01:19:26,159 --> 01:19:32,400
gene you can continue

01:19:30,000 --> 01:19:32,400
thank you

01:19:33,040 --> 01:19:36,239
yes essentially just to to pile up on

01:19:34,960 --> 01:19:38,320
that um we we

01:19:36,239 --> 01:19:40,159
had some quite very interesting uh

01:19:38,320 --> 01:19:42,159
conversation that lena much more

01:19:40,159 --> 01:19:43,199
uh much more interested on the technical

01:19:42,159 --> 01:19:44,960
side i think

01:19:43,199 --> 01:19:46,320
if there was uh technical people in the

01:19:44,960 --> 01:19:48,400
room all of them fell

01:19:46,320 --> 01:19:49,440
in into into my groom and so the dynamic

01:19:48,400 --> 01:19:51,679
was a little bit

01:19:49,440 --> 01:19:53,840
different was very very interesting and

01:19:51,679 --> 01:19:56,400
we just couldn't really go as deep as we

01:19:53,840 --> 01:19:57,520
would have wanted because of the

01:19:56,400 --> 01:19:59,199
constraints of the

01:19:57,520 --> 01:20:01,199
of the scenarios but definitely was a

01:19:59,199 --> 01:20:03,199
very interesting and we were able to

01:20:01,199 --> 01:20:04,000
take some notes about recommendations on

01:20:03,199 --> 01:20:06,320
how to improve

01:20:04,000 --> 01:20:07,040
the um the consultation for for next

01:20:06,320 --> 01:20:09,440
session so

01:20:07,040 --> 01:20:10,719
there was some some good outputs out of

01:20:09,440 --> 01:20:12,800
it

01:20:10,719 --> 01:20:14,560
thank you very much jean now i'll turn

01:20:12,800 --> 01:20:19,840
to the third and last group

01:20:14,560 --> 01:20:19,840
um who wants to volunteer report

01:20:22,400 --> 01:20:26,000
i'm sure so i mean i think anything that

01:20:24,639 --> 01:20:26,639
rashi and i are going to say is really

01:20:26,000 --> 01:20:28,239
just

01:20:26,639 --> 01:20:30,000
building on the beautiful ideas we got

01:20:28,239 --> 01:20:31,840
from tara and brenna

01:20:30,000 --> 01:20:33,679
k and ushnish from the group so thank

01:20:31,840 --> 01:20:36,639
you all for the really

01:20:33,679 --> 01:20:37,840
active participation um so the idea is

01:20:36,639 --> 01:20:39,760
really i mean i think

01:20:37,840 --> 01:20:41,040
at least what i what i picked up on was

01:20:39,760 --> 01:20:43,199
was this notion that

01:20:41,040 --> 01:20:44,400
um that artificial intelligence is

01:20:43,199 --> 01:20:46,400
really magnifying

01:20:44,400 --> 01:20:47,840
traditional ethical issues that already

01:20:46,400 --> 01:20:49,920
exist in society and

01:20:47,840 --> 01:20:52,000
traditional power relationships and

01:20:49,920 --> 01:20:53,840
society between people government and

01:20:52,000 --> 01:20:55,920
corporations for example

01:20:53,840 --> 01:20:57,040
there was elevated in several of the

01:20:55,920 --> 01:20:58,880
scenarios

01:20:57,040 --> 01:21:00,719
and and this brought up a really

01:20:58,880 --> 01:21:03,440
beautiful point from mushneesh

01:21:00,719 --> 01:21:04,960
from the disability community of nothing

01:21:03,440 --> 01:21:08,400
for us without us

01:21:04,960 --> 01:21:11,440
and um nothing about us without us um

01:21:08,400 --> 01:21:13,199
rather and uh and the idea really that

01:21:11,440 --> 01:21:14,800
you know collective intelligence and

01:21:13,199 --> 01:21:15,840
other forms of collective decision

01:21:14,800 --> 01:21:17,920
making

01:21:15,840 --> 01:21:19,440
could be really usefully applied to sort

01:21:17,920 --> 01:21:20,960
of unpacking some of these

01:21:19,440 --> 01:21:22,560
ethical issues related to artificial

01:21:20,960 --> 01:21:24,080
intelligence but also

01:21:22,560 --> 01:21:25,679
the associated ethical issues that are

01:21:24,080 --> 01:21:26,880
related to for example the relationship

01:21:25,679 --> 01:21:29,120
between corporates and

01:21:26,880 --> 01:21:30,239
educational institutions for example on

01:21:29,120 --> 01:21:33,199
the banking example

01:21:30,239 --> 01:21:33,440
and otherwise so thank you all again for

01:21:33,199 --> 01:21:37,600
the

01:21:33,440 --> 01:21:37,600
for the great conversation with rashi

01:21:38,480 --> 01:21:44,080
i think good animated conversations

01:21:41,679 --> 01:21:45,520
from i think we spent the most time in

01:21:44,080 --> 01:21:47,760
the banking sector and

01:21:45,520 --> 01:21:49,760
a lot of the a lot of the comments were

01:21:47,760 --> 01:21:50,880
around how oblivious the actual person

01:21:49,760 --> 01:21:53,360
is when it comes to

01:21:50,880 --> 01:21:55,040
where the information is going uh there

01:21:53,360 --> 01:21:56,560
were there were some positive comments

01:21:55,040 --> 01:21:58,000
around mental health

01:21:56,560 --> 01:22:00,239
but there was also the question of why

01:21:58,000 --> 01:22:02,239
mental health is so expensive

01:22:00,239 --> 01:22:04,480
uh with and yeah there was some good

01:22:02,239 --> 01:22:06,159
animated conversations around how

01:22:04,480 --> 01:22:07,679
it's difficult to get rid of something

01:22:06,159 --> 01:22:10,480
like a bias

01:22:07,679 --> 01:22:11,600
and how to actually get rid of these fix

01:22:10,480 --> 01:22:14,159
from a technical

01:22:11,600 --> 01:22:14,159
perspective

01:22:15,360 --> 01:22:19,600
thank you very much thank you everyone

01:22:17,840 --> 01:22:22,880
and maybe one um

01:22:19,600 --> 01:22:26,000
well before we we close um it is

01:22:22,880 --> 01:22:27,679
for us um um on the plan

01:22:26,000 --> 01:22:29,600
to go on working with artificial

01:22:27,679 --> 01:22:31,360
intelligence and deliberative democracy

01:22:29,600 --> 01:22:34,000
so it's at the beginning and we

01:22:31,360 --> 01:22:34,560
got the message from participants um

01:22:34,000 --> 01:22:36,320
that

01:22:34,560 --> 01:22:38,080
we need much more discussion on that so

01:22:36,320 --> 01:22:41,120
we have done just a piece of that

01:22:38,080 --> 01:22:43,920
in the past hour but it's not over

01:22:41,120 --> 01:22:45,679
of course and we it's uh on our agenda

01:22:43,920 --> 01:22:47,840
and one of our priority to go on

01:22:45,679 --> 01:22:49,840
working with that and so thank you very

01:22:47,840 --> 01:22:53,600
much for your contributions

01:22:49,840 --> 01:22:56,239
i wish you a very good um further

01:22:53,600 --> 01:22:56,880
further participation at the most fast

01:22:56,239 --> 01:23:00,400
um

01:22:56,880 --> 01:23:04,159
and see you soon maybe for some of us

01:23:00,400 --> 01:23:06,800
in real at some point i hope so

01:23:04,159 --> 01:23:07,920
and otherwise again online thank you

01:23:06,800 --> 01:23:11,440
very much

01:23:07,920 --> 01:23:14,320
um yes um from the last comment

01:23:11,440 --> 01:23:15,520
and as output of that session uh we will

01:23:14,320 --> 01:23:18,800
produce a blog

01:23:15,520 --> 01:23:21,679
um documentation and a blog text

01:23:18,800 --> 01:23:22,639
um so you can also share the results of

01:23:21,679 --> 01:23:25,600
that session

01:23:22,639 --> 01:23:28,480
thank you very much all of you and i

01:23:25,600 --> 01:23:35,840
hope to see you soon

01:23:28,480 --> 01:23:35,840
john if you could also go ahead

01:24:39,440 --> 01:24:41,520

YouTube URL: https://www.youtube.com/watch?v=2nlPmkGymaU


