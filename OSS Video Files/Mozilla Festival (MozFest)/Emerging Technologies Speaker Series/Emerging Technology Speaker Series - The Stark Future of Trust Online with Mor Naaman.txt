Title: Emerging Technology Speaker Series - The Stark Future of Trust Online with Mor Naaman
Publication date: 2020-05-12
Playlist: Emerging Technologies Speaker Series
Description: 
	Trust is what enables our society to function, from supporting interpersonal transactions to providing the very foundation of our democracy. How trust is established online is therefore a key question for HCI to understand and address, especially as the landscape is rapidly changing with AI and algorithms increasingly mediating our online experiences. This talk will cover two different and critical aspects of online trust. In the first part of the talk, I will present work on trust in the news media. Most people consume online news on platforms where algorithms present content from a mixed set of sources, for example news aggregators and news feeds. It is important, then, to evaluate the factors that contribute to how people evaluate the veracity of content in these environments. In this work, we follow on a body of research showing higher reported trust in politically aligned news sources. We investigate the determinants of the increased trust to distinguish between source evaluation and confirmation bias effects.

It is more than recommendations algorithms that mediate our online experiences. In the second part of the talk, I will outline a near-future where our personal communications are mediated by AI agents, or as we termed it: AI Mediated Communication (AI-MC). I will lay out the various ways in which AI-MC might impact our interpersonal communications, and describe a study where we examined the potential impact of AI-generated profile text on the perceived trustworthiness of Airbnb hosts. Taken together, the findings point to a bleak future of trust in our society, and offer a set of significant challenges for the future of HCI.

The studies I will present were led by PhD student Maurice Jakesch at Cornell, and are joint work with Jeff Hancock, Xiao Ma, and Karen Levy among others.


Mor Naaman is an associate professor of Information Science at the Jacobs Institute at Cornell Tech. Mor leads a research group focused on topics related to the intersection of technology, media and democracy. The group applies multidisciplinary techniques — from machine learning to qualitative social science — to study our information ecosystem and its challenges. Previously, Mor was on the faculty at the Rutgers School of Communication and Information, led a research team at Yahoo! Research Berkeley, received a Ph.D. in Computer Science from the Stanford University InfoLab, and played professional basketball for Hapoel Tel Aviv. He is also a former startup co-founder, and advises startup companies in social computing and related areas. His research is widely recognized, including with an NSF Early Faculty CAREER Award, research awards and grants from numerous corporations, and multiple best paper awards
Captions: 
	00:03:11,850 --> 00:03:13,910
you

00:06:03,550 --> 00:06:05,610
you

00:08:56,200 --> 00:09:01,900
and I'm gonna be filling filterable

00:08:58,060 --> 00:09:04,000
filter questions through me good morning

00:09:01,900 --> 00:09:05,770
my name is Joe fish K and principal

00:09:04,000 --> 00:09:08,260
research scientist here at Mozilla in

00:09:05,770 --> 00:09:11,050
Mountain View and today my pleasure is

00:09:08,260 --> 00:09:12,670
to that's not even a sentence today it

00:09:11,050 --> 00:09:14,920
is my pleasure to introduce to you mourn

00:09:12,670 --> 00:09:19,420
among more is a professor at Cornell

00:09:14,920 --> 00:09:20,950
Tech and has been a longtime researcher

00:09:19,420 --> 00:09:22,390
in the field of human-computer

00:09:20,950 --> 00:09:25,630
interaction and computer-supported

00:09:22,390 --> 00:09:27,250
cooperative work and I think his current

00:09:25,630 --> 00:09:30,610
work in which he's looking into what it

00:09:27,250 --> 00:09:32,410
means to explore Trust online both in

00:09:30,610 --> 00:09:35,560
terms of trusted in the sources that you

00:09:32,410 --> 00:09:37,990
read but also trust in shall we say the

00:09:35,560 --> 00:09:40,180
trusted personal agent which I'll remind

00:09:37,990 --> 00:09:42,670
you is part of Mozilla's true north

00:09:40,180 --> 00:09:44,470
right now that we're exploring so I

00:09:42,670 --> 00:09:46,510
think this will be very relevant I think

00:09:44,470 --> 00:09:47,500
that ties to the work that Moore is

00:09:46,510 --> 00:09:49,990
talking about right now

00:09:47,500 --> 00:09:51,430
to some of the presentations we've seen

00:09:49,990 --> 00:09:53,020
previously like the work that Jenna

00:09:51,430 --> 00:09:55,630
Borel presented all the work that Kate

00:09:53,020 --> 00:09:57,700
Starbird presented really point to a

00:09:55,630 --> 00:09:59,080
theme here and it's very exciting to see

00:09:57,700 --> 00:10:01,120
that as part of the discussion so

00:09:59,080 --> 00:10:06,550
Bethany further ado I introduce you to

00:10:01,120 --> 00:10:08,500
morning thank you great to be here at

00:10:06,550 --> 00:10:12,220
newsela thank Jawfish for the invitation

00:10:08,500 --> 00:10:15,520
and if people there they help do have

00:10:12,220 --> 00:10:17,500
remote questions at any time or so

00:10:15,520 --> 00:10:19,030
somebody's gonna flag I'm gonna talk to

00:10:17,500 --> 00:10:23,110
you somebody gonna fight me because they

00:10:19,030 --> 00:10:25,260
don't see you guys out there so this is

00:10:23,110 --> 00:10:27,340
a relatively brand-new talk with some

00:10:25,260 --> 00:10:29,590
results that we're still working on

00:10:27,340 --> 00:10:32,830
interpreting and understanding so bear

00:10:29,590 --> 00:10:36,010
with me and just a few credits before I

00:10:32,830 --> 00:10:38,980
start this is mostly work led by my PhD

00:10:36,010 --> 00:10:41,100
student Morris at Cornell Tech with

00:10:38,980 --> 00:10:45,490
other collaborators species udents

00:10:41,100 --> 00:10:47,140
faculty and others and also a word of

00:10:45,490 --> 00:10:51,250
non-credit I'm currently on sabbatical

00:10:47,140 --> 00:10:54,700
at Google here in Mountain View but this

00:10:51,250 --> 00:10:57,610
is a hundred percent Cornell talk I do

00:10:54,700 --> 00:11:01,150
not the work is not from Google and I'm

00:10:57,610 --> 00:11:07,480
not presenting Google in any way in the

00:11:01,150 --> 00:11:09,980
talk so trust

00:11:07,480 --> 00:11:17,990
actually let me do it in a John Oliver

00:11:09,980 --> 00:11:21,020
way Trust has my British accent Trust is

00:11:17,990 --> 00:11:23,420
what why we go 5,000 miles and stay with

00:11:21,020 --> 00:11:26,060
a stranger that we've never met before

00:11:23,420 --> 00:11:27,980
Trust is also why we can live food in

00:11:26,060 --> 00:11:30,170
the Mozilla fridge knowing that they

00:11:27,980 --> 00:11:32,090
will not be eaten by others

00:11:30,170 --> 00:11:33,800
Trust is why we can book speakers

00:11:32,090 --> 00:11:35,450
inviting people on social media and

00:11:33,800 --> 00:11:38,660
remote locations to see them knowing

00:11:35,450 --> 00:11:42,080
that they'll show up and Trust is also

00:11:38,660 --> 00:11:43,940
why we can eat out at restaurants Joe

00:11:42,080 --> 00:11:46,550
fish might recognize this scene I don't

00:11:43,940 --> 00:11:51,890
know if everybody young people do these

00:11:46,550 --> 00:11:53,900
days so Trust is often defined as the

00:11:51,890 --> 00:11:56,300
willingness to take risk or to make

00:11:53,900 --> 00:11:58,430
oneself vulnerable to other parties in

00:11:56,300 --> 00:12:00,890
hope for some future expectation of some

00:11:58,430 --> 00:12:03,350
future reward trust is fundamental for

00:12:00,890 --> 00:12:05,290
all our interpersonal interactions for

00:12:03,350 --> 00:12:08,510
our transactions for our relationships

00:12:05,290 --> 00:12:10,820
but not only those for our society to

00:12:08,510 --> 00:12:12,740
function for an economy to function we

00:12:10,820 --> 00:12:15,320
also need to trust institutions various

00:12:12,740 --> 00:12:18,050
institutions and organizations we do

00:12:15,320 --> 00:12:20,270
that especially in Western societies in

00:12:18,050 --> 00:12:22,310
multiple in daily ways for example we

00:12:20,270 --> 00:12:24,500
just in government to mostly act on our

00:12:22,310 --> 00:12:27,170
behalf which was corporations like

00:12:24,500 --> 00:12:31,100
Mozilla with our data and we trust the

00:12:27,170 --> 00:12:32,540
media to reflect the world to us and

00:12:31,100 --> 00:12:34,040
society in how its functioning we'll

00:12:32,540 --> 00:12:37,250
come back to media in a second

00:12:34,040 --> 00:12:39,440
altogether trust I don't need to tell

00:12:37,250 --> 00:12:43,630
this audience is fundamental to the

00:12:39,440 --> 00:12:46,610
stability of our democratic societies

00:12:43,630 --> 00:12:50,510
but how we trust each other has also

00:12:46,610 --> 00:12:52,490
shifted our social structures our social

00:12:50,510 --> 00:12:56,360
interactions have shifted from face to

00:12:52,490 --> 00:12:58,339
face to social and online media where

00:12:56,360 --> 00:13:00,980
our interactions consumption our

00:12:58,339 --> 00:13:02,270
institutions who are interacting

00:13:00,980 --> 00:13:04,280
interactions and consumption of

00:13:02,270 --> 00:13:06,440
information with institutions and others

00:13:04,280 --> 00:13:09,890
are increasingly governed and sometimes

00:13:06,440 --> 00:13:11,470
generated by algorithms by AI by

00:13:09,890 --> 00:13:14,750
recommender systems and so forth

00:13:11,470 --> 00:13:17,120
decisions to trust has moved to and are

00:13:14,750 --> 00:13:19,649
informed by technology platforms and

00:13:17,120 --> 00:13:21,779
algorithms

00:13:19,649 --> 00:13:23,430
what happens to trust in these settings

00:13:21,779 --> 00:13:24,959
has been a topic my research has been

00:13:23,430 --> 00:13:28,499
looking at the last over the last few

00:13:24,959 --> 00:13:29,879
years so we want to trust understand

00:13:28,499 --> 00:13:31,920
trust in this new information ecosystem

00:13:29,879 --> 00:13:33,199
supported by technology notice that I'm

00:13:31,920 --> 00:13:35,399
not talking about trust in technology

00:13:33,199 --> 00:13:39,869
necessarily but how we trust each other

00:13:35,399 --> 00:13:43,379
through technology in various ways so

00:13:39,869 --> 00:13:45,540
here's the outline of this talk I'm

00:13:43,379 --> 00:13:47,490
gonna start with one thread that talks

00:13:45,540 --> 00:13:49,050
about trust in media and I'll describe

00:13:47,490 --> 00:13:52,110
an on an experiment where we asked

00:13:49,050 --> 00:13:54,569
people to evaluate headline claims news

00:13:52,110 --> 00:13:56,069
reports in partisan settings they're

00:13:54,569 --> 00:13:59,610
much like the ones that we have online

00:13:56,069 --> 00:14:02,699
these days the other thread I'm gonna

00:13:59,610 --> 00:14:04,829
introduce a new idea a new concept AI

00:14:02,699 --> 00:14:07,980
immediate communication and show how the

00:14:04,829 --> 00:14:11,550
what the potential effect of that is on

00:14:07,980 --> 00:14:13,110
how we trust each other online these are

00:14:11,550 --> 00:14:16,679
two online experiment but there are

00:14:13,110 --> 00:14:19,139
other work in my lab of different types

00:14:16,679 --> 00:14:21,360
of working my lab including things that

00:14:19,139 --> 00:14:23,550
are more system buildings data science

00:14:21,360 --> 00:14:29,069
machine learning anything from creating

00:14:23,550 --> 00:14:32,790
tools for journalists to support to help

00:14:29,069 --> 00:14:34,290
them track visual misinformation to

00:14:32,790 --> 00:14:36,839
understanding misinformation targeted

00:14:34,290 --> 00:14:40,050
political candidates online and social

00:14:36,839 --> 00:14:41,370
media and understanding trust for

00:14:40,050 --> 00:14:43,649
example in social group and how that

00:14:41,370 --> 00:14:46,170
relates to misinformation but I'm not

00:14:43,649 --> 00:14:50,970
talking about those today will stay in

00:14:46,170 --> 00:14:56,100
the online experiment world all right

00:14:50,970 --> 00:14:58,620
trust in media again I don't think this

00:14:56,100 --> 00:15:05,819
crowd needs a lot of motivation but as

00:14:58,620 --> 00:15:08,819
you know when we have in we have social

00:15:05,819 --> 00:15:12,540
filter aggregator we change the way we

00:15:08,819 --> 00:15:14,999
think about we trust the sources of that

00:15:12,540 --> 00:15:18,299
information in particular media and

00:15:14,999 --> 00:15:20,309
journalism is increasingly very

00:15:18,299 --> 00:15:24,089
important for us to understand how

00:15:20,309 --> 00:15:26,160
people evaluate as we know the quality

00:15:24,089 --> 00:15:27,629
of democratic life depends on the public

00:15:26,160 --> 00:15:29,910
having the facts and being able to make

00:15:27,629 --> 00:15:31,559
sense of them and it's also the purpose

00:15:29,910 --> 00:15:32,810
of journalism to provide us with this

00:15:31,559 --> 00:15:34,910
information with this

00:15:32,810 --> 00:15:37,040
fact so we can be free and

00:15:34,910 --> 00:15:40,880
self-governing as the Democratic idea

00:15:37,040 --> 00:15:42,530
goes at the same time media is also

00:15:40,880 --> 00:15:45,710
increasingly under attack increasingly

00:15:42,530 --> 00:15:49,430
is or seen as partisan increasingly

00:15:45,710 --> 00:15:52,010
manipulated with consequences of how we

00:15:49,430 --> 00:15:55,750
perceived it for example we know that

00:15:52,010 --> 00:15:58,220
trust in media it has been very low

00:15:55,750 --> 00:16:00,380
using here from the mattress report

00:15:58,220 --> 00:16:02,240
trusted media and government our lowest

00:16:00,380 --> 00:16:06,350
from all the other type of institutions

00:16:02,240 --> 00:16:08,720
they study it's also very partisan trust

00:16:06,350 --> 00:16:10,370
in media organizations is split along

00:16:08,720 --> 00:16:12,740
partisan dimensions and this

00:16:10,370 --> 00:16:17,870
partisanship is what we focus on in this

00:16:12,740 --> 00:16:20,320
study that I will present in particular

00:16:17,870 --> 00:16:23,330
in this specific study we were

00:16:20,320 --> 00:16:24,800
interested in how people when they

00:16:23,330 --> 00:16:25,750
encounter news online let's say in a

00:16:24,800 --> 00:16:28,160
feed

00:16:25,750 --> 00:16:31,540
how will the evaluation of the news that

00:16:28,160 --> 00:16:34,880
they see will change as a response to

00:16:31,540 --> 00:16:36,860
political politically congruent and

00:16:34,880 --> 00:16:39,410
incongruent headline claims or media

00:16:36,860 --> 00:16:41,660
sources so when they agree with the

00:16:39,410 --> 00:16:44,270
headline or where they align with the

00:16:41,660 --> 00:16:47,180
source where how would they change their

00:16:44,270 --> 00:16:49,190
evaluation so for example if your

00:16:47,180 --> 00:16:52,040
conservatives say in the US and you have

00:16:49,190 --> 00:16:54,290
a preference for Fox News what would you

00:16:52,040 --> 00:16:56,150
make of and fox news headline that says

00:16:54,290 --> 00:16:59,420
that the majority of Americans support

00:16:56,150 --> 00:17:02,210
the impeachment process are you gonna

00:16:59,420 --> 00:17:03,440
say yes I trust this is from Fox News

00:17:02,210 --> 00:17:05,240
are you going to reject it because it

00:17:03,440 --> 00:17:08,300
doesn't lie with your worldview

00:17:05,240 --> 00:17:10,459
regardless of your trusted source and

00:17:08,300 --> 00:17:15,350
what will happen if that same headline

00:17:10,459 --> 00:17:16,910
was shown say by the New York Times so I

00:17:15,350 --> 00:17:20,240
already hinted that a few things that

00:17:16,910 --> 00:17:23,600
might impact the evaluation when you

00:17:20,240 --> 00:17:26,360
Encarta such information so for example

00:17:23,600 --> 00:17:27,880
one thing if it will impact it this is

00:17:26,360 --> 00:17:30,940
how you evaluate the source credibility

00:17:27,880 --> 00:17:33,800
so the way you evaluate the credibility

00:17:30,940 --> 00:17:36,500
has to do with expertise that you are

00:17:33,800 --> 00:17:38,720
assigned to the source but also those

00:17:36,500 --> 00:17:41,330
evaluation of those expertise is often

00:17:38,720 --> 00:17:44,750
impacted by your perception of how you

00:17:41,330 --> 00:17:46,990
share goals and values with that source

00:17:44,750 --> 00:17:50,120
of that organization

00:17:46,990 --> 00:17:51,799
the other thing that may impact your

00:17:50,120 --> 00:17:53,960
evaluation is what's called motivated

00:17:51,799 --> 00:17:56,210
reasoning sometimes directionally

00:17:53,960 --> 00:17:58,280
motivated reasoning which leads people

00:17:56,210 --> 00:18:00,500
to seek out information reinforces their

00:17:58,280 --> 00:18:02,150
view sometimes related or happens

00:18:00,500 --> 00:18:04,850
through a mechanism we call confirmation

00:18:02,150 --> 00:18:06,470
bias right so this generally means that

00:18:04,850 --> 00:18:08,900
we view information that supports our

00:18:06,470 --> 00:18:11,840
worldview as more convincing than

00:18:08,900 --> 00:18:13,640
information that goes against it so

00:18:11,840 --> 00:18:18,500
that's another thing that might impact

00:18:13,640 --> 00:18:19,909
our evaluation the last thing we're

00:18:18,500 --> 00:18:22,340
going to look at is what's called

00:18:19,909 --> 00:18:24,650
expressive responding somebody called

00:18:22,340 --> 00:18:25,850
motivated responding and this is when

00:18:24,650 --> 00:18:28,190
you're presented with information that

00:18:25,850 --> 00:18:29,840
is opposed to the view you know what the

00:18:28,190 --> 00:18:32,390
right answer is but you will respond in

00:18:29,840 --> 00:18:35,870
a manner that supports your answer so

00:18:32,390 --> 00:18:37,400
famously an example when there was a

00:18:35,870 --> 00:18:39,650
study that shows when Republicans were

00:18:37,400 --> 00:18:43,809
shown side by side Obama and Trump

00:18:39,650 --> 00:18:46,909
inauguration photos they still said that

00:18:43,809 --> 00:18:48,559
Trump's was better attended right so

00:18:46,909 --> 00:18:51,110
this is where they probably know the

00:18:48,559 --> 00:18:53,090
right answer but still perform an answer

00:18:51,110 --> 00:18:55,179
that aligns with the views so when we

00:18:53,090 --> 00:18:58,429
ask people to evaluate content online

00:18:55,179 --> 00:19:05,090
all these these are three of the factors

00:18:58,429 --> 00:19:06,500
that may be part of their calculation so

00:19:05,090 --> 00:19:08,990
we can ask the most specific research

00:19:06,500 --> 00:19:11,299
question now so how will the relation

00:19:08,990 --> 00:19:13,820
change in particular what is the

00:19:11,299 --> 00:19:15,830
relative impact of your alignment with

00:19:13,820 --> 00:19:17,750
the source versus your alignment with

00:19:15,830 --> 00:19:20,480
the claim and whether there is any

00:19:17,750 --> 00:19:22,610
evidence of expressive responding where

00:19:20,480 --> 00:19:24,679
you know the right answer but provide a

00:19:22,610 --> 00:19:26,270
different one if I'm not gonna talk

00:19:24,679 --> 00:19:28,970
about this latter one you can ask me

00:19:26,270 --> 00:19:30,679
about that later

00:19:28,970 --> 00:19:35,780
or maybe I'll briefly mention that if we

00:19:30,679 --> 00:19:39,169
have time so to answer that questions

00:19:35,780 --> 00:19:41,750
here is what we did we had an online

00:19:39,169 --> 00:19:43,490
experiment where we showed headlines

00:19:41,750 --> 00:19:44,570
that were randomly assigned to sources

00:19:43,490 --> 00:19:46,100
so the headlines were either right or

00:19:44,570 --> 00:19:48,620
left leaning and the sources were either

00:19:46,100 --> 00:19:51,919
right or left leaning so this was the

00:19:48,620 --> 00:19:53,539
entire manipulation and here's the the

00:19:51,919 --> 00:19:55,640
one method slide if you want to pay

00:19:53,539 --> 00:19:57,799
attention this is the one method

00:19:55,640 --> 00:19:59,150
slightly you need to understand like I

00:19:57,799 --> 00:20:01,180
just

00:19:59,150 --> 00:20:02,780
we took a left-leaning headline and

00:20:01,180 --> 00:20:04,190
sometimes we showed it with a

00:20:02,780 --> 00:20:08,390
left-leaning source and sometimes we

00:20:04,190 --> 00:20:10,070
showed participants the same headline

00:20:08,390 --> 00:20:11,750
with aligned with the right leaning

00:20:10,070 --> 00:20:15,590
source and then we did the same with the

00:20:11,750 --> 00:20:19,250
right cleaning headline and mixed up the

00:20:15,590 --> 00:20:22,040
sources this way okay I lied there is

00:20:19,250 --> 00:20:23,420
one more slide to pay attention to this

00:20:22,040 --> 00:20:25,430
is what we asked them we asked them when

00:20:23,420 --> 00:20:28,010
we showed those headlines and I'll show

00:20:25,430 --> 00:20:29,870
you in a second the more detailed setup

00:20:28,010 --> 00:20:32,600
of the experiment we basically asked

00:20:29,870 --> 00:20:39,559
them to review whether the claim in the

00:20:32,600 --> 00:20:42,650
headline is true or false so those

00:20:39,559 --> 00:20:46,700
claims that we use an experiment we had

00:20:42,650 --> 00:20:49,160
to be quite selective in how in which

00:20:46,700 --> 00:20:51,200
ones which headlines which claims which

00:20:49,160 --> 00:20:53,090
onto the participants so we wanted

00:20:51,200 --> 00:20:54,800
things that are represented a

00:20:53,090 --> 00:20:56,510
representative of what people might

00:20:54,800 --> 00:20:59,660
encounter in their online news

00:20:56,510 --> 00:21:01,700
consumption we wanted things that could

00:20:59,660 --> 00:21:05,900
be true or false of the content factual

00:21:01,700 --> 00:21:08,510
claims we wanted claims that are clearly

00:21:05,900 --> 00:21:11,750
either liberal leaning or conservative

00:21:08,510 --> 00:21:13,880
leaning and we wanted also claims that

00:21:11,750 --> 00:21:15,380
are difficult to identify it's true

00:21:13,880 --> 00:21:20,000
that's not everybody knows so everybody

00:21:15,380 --> 00:21:23,120
agrees that they're true or false I'm

00:21:20,000 --> 00:21:26,420
not going to go into the details but

00:21:23,120 --> 00:21:27,890
stuff is to say that Maurice the grad

00:21:26,420 --> 00:21:29,870
students case went through a detailed

00:21:27,890 --> 00:21:34,929
process of data collection and

00:21:29,870 --> 00:21:36,830
validation at the end of which we had

00:21:34,929 --> 00:21:41,360
ten right-leaning

00:21:36,830 --> 00:21:44,690
and ten left-leaning headlines that were

00:21:41,360 --> 00:21:47,800
evaluated as such in a pretest with

00:21:44,690 --> 00:21:50,600
claims specific claims that could be

00:21:47,800 --> 00:21:53,600
perceived as through a force but that we

00:21:50,600 --> 00:21:56,690
at least with our biases evaluated is

00:21:53,600 --> 00:21:59,179
true and they were not importantly not

00:21:56,690 --> 00:22:01,730
obviously true to participants in the

00:21:59,179 --> 00:22:05,840
pretest okay so we had twenty such

00:22:01,730 --> 00:22:07,090
headlines to use with two examples that

00:22:05,840 --> 00:22:10,480
we've seen before

00:22:07,090 --> 00:22:10,480
were part of it

00:22:11,149 --> 00:22:17,820
okay so to choose our sources so

00:22:15,629 --> 00:22:20,220
remember we mixed the headlines with

00:22:17,820 --> 00:22:22,529
sources sources we wanted to have widely

00:22:20,220 --> 00:22:25,259
recognized media organizations that most

00:22:22,529 --> 00:22:27,210
participants will recognize but also

00:22:25,259 --> 00:22:30,149
those that are perceived as biases

00:22:27,210 --> 00:22:37,370
partisan either on the right or on the

00:22:30,149 --> 00:22:39,870
left so we ended up using six sources

00:22:37,370 --> 00:22:41,330
CNN half Post and The New York Times is

00:22:39,870 --> 00:22:44,820
the one that are seen as left-leaning

00:22:41,330 --> 00:22:49,320
Fox News Drudge and Breitbart as those

00:22:44,820 --> 00:22:51,659
that seem right leaning here's what we

00:22:49,320 --> 00:22:56,940
did in the experiment setup remember we

00:22:51,659 --> 00:22:59,159
had our ten claims 1020 claims ten

00:22:56,940 --> 00:23:03,090
Republican leaning and ten Democratic

00:22:59,159 --> 00:23:05,070
leaning in blue and six sources so when

00:23:03,090 --> 00:23:09,659
we had a new participant coming to our

00:23:05,070 --> 00:23:12,119
experiment we matched one at random

00:23:09,659 --> 00:23:16,529
shows one liberal claimed and matched it

00:23:12,119 --> 00:23:19,049
to one of the liberal sources we took

00:23:16,529 --> 00:23:20,279
one conservative right leaning claim and

00:23:19,049 --> 00:23:22,679
match it with one of the right leaning

00:23:20,279 --> 00:23:25,559
sources and then we did the opposite we

00:23:22,679 --> 00:23:27,929
mixed up left and right and right

00:23:25,559 --> 00:23:32,220
claimed with a left-leaning headline so

00:23:27,929 --> 00:23:33,929
we had for such headlines in each

00:23:32,220 --> 00:23:36,299
version of the experiment this is what

00:23:33,929 --> 00:23:38,100
we use this is the data that we got from

00:23:36,299 --> 00:23:41,429
every participant right so we got so

00:23:38,100 --> 00:23:46,830
every participant four data points that

00:23:41,429 --> 00:23:49,470
mixed up headlines and sources this

00:23:46,830 --> 00:23:51,419
would have been probably obvious the

00:23:49,470 --> 00:23:53,610
participants what we're trying to do so

00:23:51,419 --> 00:23:56,610
we also mixed it up with a bunch of

00:23:53,610 --> 00:23:58,139
decoys I'm not gonna give you the

00:23:56,610 --> 00:24:01,830
details of how we generated those but

00:23:58,139 --> 00:24:04,619
among the decoys we had which only 10

00:24:01,830 --> 00:24:07,019
other headlines associated with multiple

00:24:04,619 --> 00:24:09,139
other sources including two headlines

00:24:07,019 --> 00:24:12,269
that you see here in black there were

00:24:09,139 --> 00:24:14,279
fake news very clearly fake news

00:24:12,269 --> 00:24:16,080
headlines so when we ask them about

00:24:14,279 --> 00:24:19,980
things that are true or false they could

00:24:16,080 --> 00:24:21,520
have always had in the in the study to

00:24:19,980 --> 00:24:24,010
had nice that

00:24:21,520 --> 00:24:29,650
very clearly fossil they can choose that

00:24:24,010 --> 00:24:31,840
option we also asked participants about

00:24:29,650 --> 00:24:33,790
their political leaning using various

00:24:31,840 --> 00:24:35,020
measures you can ask me about later but

00:24:33,790 --> 00:24:36,490
eventually we had a label of

00:24:35,020 --> 00:24:39,370
left-leaning or right leaning for each

00:24:36,490 --> 00:24:40,720
of them we asked them generally about

00:24:39,370 --> 00:24:43,780
trust recognition of the media sources

00:24:40,720 --> 00:24:49,690
and obviously demographics that I will

00:24:43,780 --> 00:24:53,680
not talk about in the analysis today we

00:24:49,690 --> 00:24:55,570
ran it on Amazon Mechanical Turk so we

00:24:53,680 --> 00:24:58,570
had a hundred 60 participants of meaning

00:24:55,570 --> 00:25:01,660
that we got four data points on the four

00:24:58,570 --> 00:25:07,630
items experimental items from each of

00:25:01,660 --> 00:25:10,360
these people and now we can start

00:25:07,630 --> 00:25:12,730
answering the question of whether people

00:25:10,360 --> 00:25:14,830
would swayed by a source or whether they

00:25:12,730 --> 00:25:19,060
swayed by the headline that aligns with

00:25:14,830 --> 00:25:21,700
our view now usually there may be I'll

00:25:19,060 --> 00:25:23,980
walk around and it will be hard for us

00:25:21,700 --> 00:25:26,470
to count I usually ask before what they

00:25:23,980 --> 00:25:28,530
think so if you spend one second now as

00:25:26,470 --> 00:25:31,360
Joe Fisher put your fish on the spot

00:25:28,530 --> 00:25:32,890
what they think had more impact whether

00:25:31,360 --> 00:25:34,470
you align yourself with the source or

00:25:32,890 --> 00:25:37,000
whether you aligned with the headline

00:25:34,470 --> 00:25:40,410
regardless of source what had more

00:25:37,000 --> 00:25:42,940
impact on your evaluation of contents

00:25:40,410 --> 00:25:45,100
yeah so Joe fish goes with the source I

00:25:42,940 --> 00:25:51,240
think the majority of crowds that I

00:25:45,100 --> 00:25:51,240
present is to go with the source solid

00:25:51,390 --> 00:25:59,320
solid guests and one and that we'll see

00:25:54,730 --> 00:26:03,010
in a second if was true or not so let me

00:25:59,320 --> 00:26:06,490
walk you through the high-level results

00:26:03,010 --> 00:26:10,150
so this is let's do it very quickly so

00:26:06,490 --> 00:26:13,540
here we see the response of the

00:26:10,150 --> 00:26:15,720
participants to all there every time

00:26:13,540 --> 00:26:18,940
that they saw a headline that was

00:26:15,720 --> 00:26:20,200
aligned with a left-leaning source right

00:26:18,940 --> 00:26:21,690
so that's matter all the headlines

00:26:20,200 --> 00:26:24,070
right-leaning left-leaning

00:26:21,690 --> 00:26:27,580
but associated with left-leaning source

00:26:24,070 --> 00:26:32,040
and the participants in blue you can see

00:26:27,580 --> 00:26:34,900
the 56% of the time thought that was

00:26:32,040 --> 00:26:38,490
those headlines were true

00:26:34,900 --> 00:26:41,230
and when we had the red participant

00:26:38,490 --> 00:26:44,650
right-leaning participants 53 so a

00:26:41,230 --> 00:26:47,350
little less 53 percent of the time they

00:26:44,650 --> 00:26:49,450
thought those left-leaning sources those

00:26:47,350 --> 00:26:52,630
claims were true okay

00:26:49,450 --> 00:26:54,370
now two things one thing to remember all

00:26:52,630 --> 00:26:56,850
the claims at least we've added all the

00:26:54,370 --> 00:27:00,310
claims to be true so that we expect

00:26:56,850 --> 00:27:02,860
participants to agree a hundred percent

00:27:00,310 --> 00:27:05,620
right if they actually knew the state of

00:27:02,860 --> 00:27:07,180
the world to agree with those sources

00:27:05,620 --> 00:27:11,110
100 percent of the time this is true for

00:27:07,180 --> 00:27:14,140
all the results that I'll show you in a

00:27:11,110 --> 00:27:16,030
second so let's see what happens when we

00:27:14,140 --> 00:27:17,980
have a right-leaning source right so if

00:27:16,030 --> 00:27:20,260
we think that the source and your lamb

00:27:17,980 --> 00:27:22,360
who the sources impact we now expect the

00:27:20,260 --> 00:27:25,420
Republicans to believe the right-leaning

00:27:22,360 --> 00:27:30,430
sources a lot more and the Liberals to

00:27:25,420 --> 00:27:34,150
believe them less and indeed this is the

00:27:30,430 --> 00:27:35,950
case so when we show the information in

00:27:34,150 --> 00:27:38,620
the same claims fran randomly assigned

00:27:35,950 --> 00:27:41,080
to run right-leaning source there is a

00:27:38,620 --> 00:27:43,870
significant drop in how the left-leaning

00:27:41,080 --> 00:27:46,300
crowd evaluates them versus the right

00:27:43,870 --> 00:27:49,380
leaning notice and will maybe come back

00:27:46,300 --> 00:27:51,550
to it the shift is mostly with

00:27:49,380 --> 00:27:53,890
left-leaning the liberal participants

00:27:51,550 --> 00:27:57,810
there's very little change in how the

00:27:53,890 --> 00:28:00,130
right leaning participants evaluated

00:27:57,810 --> 00:28:03,460
okay so this was the source clearly has

00:28:00,130 --> 00:28:06,430
some effect let's see the effect of

00:28:03,460 --> 00:28:10,330
whether or not you align with the

00:28:06,430 --> 00:28:14,200
headline and you can see here that this

00:28:10,330 --> 00:28:15,970
effect was much larger so regardless of

00:28:14,200 --> 00:28:18,040
the source now we are getting by the

00:28:15,970 --> 00:28:20,050
claims so every time you see a

00:28:18,040 --> 00:28:22,720
left-leaning claim we can see that

00:28:20,050 --> 00:28:27,880
left-leaning participants tended to rate

00:28:22,720 --> 00:28:29,290
it as much as true in much higher

00:28:27,880 --> 00:28:31,450
numbers than the brightening

00:28:29,290 --> 00:28:32,590
participants and that flipped when we're

00:28:31,450 --> 00:28:38,980
showing what we're saying a

00:28:32,590 --> 00:28:42,460
right-leaning claim again one thing to

00:28:38,980 --> 00:28:44,770
notice is that the shift is mainly

00:28:42,460 --> 00:28:47,470
happening with the left-leaning

00:28:44,770 --> 00:28:49,180
participants and almost

00:28:47,470 --> 00:28:51,850
no movement with the right leaning

00:28:49,180 --> 00:28:53,380
person when they evaluate those claims

00:28:51,850 --> 00:28:58,840
and again I'll come back to that in a

00:28:53,380 --> 00:29:02,680
second the warning this is the initial

00:28:58,840 --> 00:29:04,540
analysis but one can you do this look at

00:29:02,680 --> 00:29:07,210
this data and run a logistic regression

00:29:04,540 --> 00:29:10,000
predicting whether you will see a true

00:29:07,210 --> 00:29:13,660
force response based on the different

00:29:10,000 --> 00:29:15,310
alignment criteria so one thing that our

00:29:13,660 --> 00:29:16,900
aggression shows that you're if you have

00:29:15,310 --> 00:29:20,320
a line source used seven percent more

00:29:16,900 --> 00:29:22,330
likely to believe it but yes if you have

00:29:20,320 --> 00:29:26,620
an align claim you're 15 percent more

00:29:22,330 --> 00:29:29,230
likely to believe that where we have in

00:29:26,620 --> 00:29:34,120
general the right leaning claims was

00:29:29,230 --> 00:29:37,750
about twelve percent less credible and

00:29:34,120 --> 00:29:40,990
that that is related of course to the

00:29:37,750 --> 00:29:45,970
shift that we saw but we still analyze

00:29:40,990 --> 00:29:48,490
him to see exactly why yes

00:29:45,970 --> 00:29:54,430
AIII should the fare a I we we had an AI

00:29:48,490 --> 00:29:57,340
predict Thea okay so I'm using a

00:29:54,430 --> 00:29:59,230
stronger language here than what we

00:29:57,340 --> 00:30:02,650
showed an experiment but it's more the

00:29:59,230 --> 00:30:04,540
claim that the motivated reasoning has a

00:30:02,650 --> 00:30:08,050
stronger effect than the source right so

00:30:04,540 --> 00:30:09,400
even if Fox News reports something that

00:30:08,050 --> 00:30:11,110
the A's

00:30:09,400 --> 00:30:13,890
doesn't allow with your view you might

00:30:11,110 --> 00:30:18,370
reject the source even if you'd trust it

00:30:13,890 --> 00:30:19,660
to choose your view over it the other

00:30:18,370 --> 00:30:20,890
thing that we find is surprising

00:30:19,660 --> 00:30:25,900
asymmetry that the left-leaning

00:30:20,890 --> 00:30:27,550
participants exhibit wider gaps with and

00:30:25,900 --> 00:30:31,180
in general the right leaning headlines

00:30:27,550 --> 00:30:33,460
were less believed this could be a

00:30:31,180 --> 00:30:34,990
function of the claims of specific

00:30:33,460 --> 00:30:38,440
claims or the specific media

00:30:34,990 --> 00:30:39,840
organization that we chose the claims

00:30:38,440 --> 00:30:44,230
obviously have more direct impact

00:30:39,840 --> 00:30:46,810
however we replicated the same phenomena

00:30:44,230 --> 00:30:48,220
in with two other experiments one of

00:30:46,810 --> 00:30:51,190
them used completely different set of

00:30:48,220 --> 00:30:53,950
claims so I think that merits more

00:30:51,190 --> 00:30:57,310
investigation and I don't know how to do

00:30:53,950 --> 00:30:58,840
it yet but we're thinking about it I

00:30:57,310 --> 00:31:00,160
didn't talk about the expressive

00:30:58,840 --> 00:31:01,090
responding remember this is when people

00:31:00,160 --> 00:31:03,879
know the right answer

00:31:01,090 --> 00:31:06,249
but provide an a so that aligns with the

00:31:03,879 --> 00:31:17,409
view we have a mix evidence that that's

00:31:06,249 --> 00:31:19,389
going on from from our studies the first

00:31:17,409 --> 00:31:22,570
the question basically seems to be about

00:31:19,389 --> 00:31:24,759
sources so both how did you determine

00:31:22,570 --> 00:31:27,399
what sources were left or right leaning

00:31:24,759 --> 00:31:29,440
and whether the sources that you chose

00:31:27,399 --> 00:31:32,139
unnecessary so comparing the Drudge

00:31:29,440 --> 00:31:34,269
Report to the New York Times those seem

00:31:32,139 --> 00:31:37,389
like there are some other factors other

00:31:34,269 --> 00:31:41,259
than merely left and right yeah oh my

00:31:37,389 --> 00:31:43,090
god I showed I talked to a version of

00:31:41,259 --> 00:31:44,619
this work to crowd of journalists and

00:31:43,090 --> 00:31:46,809
they were very offended may forget the

00:31:44,619 --> 00:31:50,529
Drudge Report even by putting the time

00:31:46,809 --> 00:31:53,950
since Fox News on the same right yeah so

00:31:50,529 --> 00:31:55,360
obviously problematic and we are we have

00:31:53,950 --> 00:31:57,070
the data of how people responded to each

00:31:55,360 --> 00:31:58,539
one of these organizations and I don't

00:31:57,070 --> 00:32:02,820
have those results yet but they're

00:31:58,539 --> 00:32:05,619
certainly differences even within

00:32:02,820 --> 00:32:07,559
participants on how they but the chance

00:32:05,619 --> 00:32:11,559
I think were roughly the same

00:32:07,559 --> 00:32:14,230
but absolutely not idea the reason I

00:32:11,559 --> 00:32:16,080
think we were almost kind of stuck with

00:32:14,230 --> 00:32:22,059
these is that because literally only

00:32:16,080 --> 00:32:23,950
once it of why widely recognizable so

00:32:22,059 --> 00:32:27,580
people know them and people know their

00:32:23,950 --> 00:32:29,230
biases well yeah Drudge Report I would

00:32:27,580 --> 00:32:32,980
maybe remove from the next iteration of

00:32:29,230 --> 00:32:38,139
this just because of recognized ability

00:32:32,980 --> 00:32:39,190
but yes I also I had a little notice

00:32:38,139 --> 00:32:40,960
here right we're talking about media

00:32:39,190 --> 00:32:42,999
organization right yeah necessarily

00:32:40,960 --> 00:32:44,679
journalistic organization so I mean

00:32:42,999 --> 00:32:47,440
there is a point that it's it's not

00:32:44,679 --> 00:32:50,409
quite clear what a how far left you have

00:32:47,440 --> 00:32:52,690
to go from Breitbart in Britain we have

00:32:50,409 --> 00:32:55,179
a paper The Daily Worker and which my

00:32:52,690 --> 00:32:57,389
grandfather used to get yeah I'm willing

00:32:55,179 --> 00:33:01,090
to believe that one but I don't know of

00:32:57,389 --> 00:33:05,649
yeah you have to go pretty far left

00:33:01,090 --> 00:33:07,360
right well it turns out less than you'd

00:33:05,649 --> 00:33:10,059
expect based on the perception so we use

00:33:07,360 --> 00:33:12,880
there are a couple of studies that that

00:33:10,059 --> 00:33:15,490
measure the perceived biases of

00:33:12,880 --> 00:33:16,900
these are actually roughly balanced on

00:33:15,490 --> 00:33:18,370
the right than the left I mean not

00:33:16,900 --> 00:33:20,800
exactly of course because we can't

00:33:18,370 --> 00:33:23,860
choose it but it's not doesn't feel like

00:33:20,800 --> 00:33:25,870
in our public perception show in the

00:33:23,860 --> 00:33:27,280
u.s. it doesn't feel like it's that bad

00:33:25,870 --> 00:33:30,490
well there's also a little bit of a

00:33:27,280 --> 00:33:31,930
concern that your polaroid by the

00:33:30,490 --> 00:33:33,850
definition by the choices you're making

00:33:31,930 --> 00:33:36,790
your polarizing the the discussion right

00:33:33,850 --> 00:33:40,090
as in defining say the CH CN n as

00:33:36,790 --> 00:33:42,460
left-leaning when I I know that the

00:33:40,090 --> 00:33:45,700
right defines CNN as left-leaning

00:33:42,460 --> 00:33:47,980
I feel like a lot of people left to find

00:33:45,700 --> 00:33:50,710
CN n is pretty right leaning and when

00:33:47,980 --> 00:33:52,270
you look at the graphs of sort of

00:33:50,710 --> 00:33:53,920
correlation you know it's it's somewhere

00:33:52,270 --> 00:33:57,420
in the middle of things well absolutely

00:33:53,920 --> 00:33:59,590
I'm also obviously I'm also collapsing

00:33:57,420 --> 00:34:01,270
left and right leaning into a single

00:33:59,590 --> 00:34:04,900
dimension which is absolutely not the

00:34:01,270 --> 00:34:08,230
case and in reality although I would say

00:34:04,900 --> 00:34:10,420
that despite uh again I mean I think the

00:34:08,230 --> 00:34:12,910
perception within the general population

00:34:10,420 --> 00:34:15,520
is that the CNN is left-leaning and is

00:34:12,910 --> 00:34:18,190
seen almost equivalently as left-leaning

00:34:15,520 --> 00:34:20,470
as bright parties right leaning so so

00:34:18,190 --> 00:34:23,980
yeah we can reason about that and talk

00:34:20,470 --> 00:34:26,320
about that we have one more quick I'm

00:34:23,980 --> 00:34:28,030
gonna let you go on soon yeah um one

00:34:26,320 --> 00:34:31,990
more question that we weren't people

00:34:28,030 --> 00:34:34,630
weren't sure about the headlines that

00:34:31,990 --> 00:34:36,880
you chose were they all 100% were they

00:34:34,630 --> 00:34:40,660
all true a hundred percent true they

00:34:36,880 --> 00:34:44,080
were 100 percent true okay we evaluated

00:34:40,660 --> 00:34:46,180
them it's true so what you saw for

00:34:44,080 --> 00:34:50,260
example Trump mar-a-lago Street cost

00:34:46,180 --> 00:34:52,930
thirty million dollars we manually

00:34:50,260 --> 00:34:57,040
evaluated each one of those headlines as

00:34:52,930 --> 00:35:00,420
true anyway but there was a recent study

00:34:57,040 --> 00:35:02,970
and I forget just now blanking on the

00:35:00,420 --> 00:35:06,370
word came out was just two weeks ago

00:35:02,970 --> 00:35:10,090
that showed also an evaluation of claims

00:35:06,370 --> 00:35:13,390
that were all true and agreement rates

00:35:10,090 --> 00:35:16,870
in their study were also low as low as

00:35:13,390 --> 00:35:23,230
in our study which is surprising it was

00:35:16,870 --> 00:35:24,910
surprising to us and one more question

00:35:23,230 --> 00:35:26,170
yeah I mean I said I think this question

00:35:24,910 --> 00:35:28,170
of like how do people

00:35:26,170 --> 00:35:30,700
independently rate these these news

00:35:28,170 --> 00:35:32,020
things I know I've seen some work on

00:35:30,700 --> 00:35:35,380
that and that'll be interesting to dig

00:35:32,020 --> 00:35:36,610
in more with more than a binary left or

00:35:35,380 --> 00:35:39,010
right right it would be interesting to

00:35:36,610 --> 00:35:41,710
sort of see whether there's a scale I

00:35:39,010 --> 00:35:43,600
think I would be feel pretty strong

00:35:41,710 --> 00:35:46,090
about making the case that Breitbart is

00:35:43,600 --> 00:35:46,420
further to the right than CNN is to the

00:35:46,090 --> 00:35:49,660
left

00:35:46,420 --> 00:35:52,540
yeah but then again my understand

00:35:49,660 --> 00:35:53,830
immigrant my my sense of politics in the

00:35:52,540 --> 00:35:55,450
United States is always a little bit off

00:35:53,830 --> 00:35:57,700
like it's always slightly more right

00:35:55,450 --> 00:35:59,490
than I expect it to be yeah and I don't

00:35:57,700 --> 00:36:01,780
remember how those studies but I can

00:35:59,490 --> 00:36:03,400
point to I was interested to some of the

00:36:01,780 --> 00:36:05,430
studies that we looked at and yeah but I

00:36:03,400 --> 00:36:08,530
I agree it's not you know it's it's

00:36:05,430 --> 00:36:10,240
perception how you ask but in general

00:36:08,530 --> 00:36:12,610
with the number of studies that show

00:36:10,240 --> 00:36:14,770
that one more question on this and then

00:36:12,610 --> 00:36:16,540
let you go which is did you have a sense

00:36:14,770 --> 00:36:18,340
of the average veracity of the media

00:36:16,540 --> 00:36:21,670
outlets that is to say just given the

00:36:18,340 --> 00:36:25,180
remedial s without any without any

00:36:21,670 --> 00:36:26,470
Associated information to test whether

00:36:25,180 --> 00:36:28,750
the response of conditioned by the

00:36:26,470 --> 00:36:30,820
sources per se tend to disbelieve or

00:36:28,750 --> 00:36:33,220
like a priori prior to giving them any

00:36:30,820 --> 00:36:35,110
yeah we have the data I don't have that

00:36:33,220 --> 00:36:36,880
in an analysis right now but we have for

00:36:35,110 --> 00:36:39,700
each participant how they evaluated

00:36:36,880 --> 00:36:41,440
generally how much I trusted each one of

00:36:39,700 --> 00:36:44,920
those organizations and we asked about

00:36:41,440 --> 00:36:50,650
and a couple of others in initial work

00:36:44,920 --> 00:36:57,340
there was obviously a lot of correlation

00:36:50,650 --> 00:36:59,530
but it's kind of kids it was in a way

00:36:57,340 --> 00:37:00,880
expected by we expect right leaning left

00:36:59,530 --> 00:37:02,380
any people we know that from the

00:37:00,880 --> 00:37:04,630
previous studies to trust the right and

00:37:02,380 --> 00:37:07,270
left knee resources differently but we

00:37:04,630 --> 00:37:09,970
wanted to see if if that's actually a

00:37:07,270 --> 00:37:12,910
stronger predictor than their alignment

00:37:09,970 --> 00:37:16,270
with the claim itself it turns out I

00:37:12,910 --> 00:37:18,550
mean one so you know the double

00:37:16,270 --> 00:37:21,910
challenge here is that we have I'm gonna

00:37:18,550 --> 00:37:25,480
I'm gonna take it forward then that that

00:37:21,910 --> 00:37:29,920
even when you have a source that you

00:37:25,480 --> 00:37:32,650
trust report some information in that

00:37:29,920 --> 00:37:35,860
rejects or goes against your worldview

00:37:32,650 --> 00:37:37,290
you are still gonna reject that well

00:37:35,860 --> 00:37:40,260
view right so

00:37:37,290 --> 00:37:43,020
which is perhaps a little bit troubling

00:37:40,260 --> 00:37:44,850
all right so here what I'm asking for

00:37:43,020 --> 00:37:49,800
next steps are you know what kind of

00:37:44,850 --> 00:37:52,020
other questions we can ask that might

00:37:49,800 --> 00:37:53,430
impact how people trust in use and how

00:37:52,020 --> 00:37:55,260
we can improve that this is the big

00:37:53,430 --> 00:37:59,040
question something that we're thinking

00:37:55,260 --> 00:38:00,450
about another question is you know it's

00:37:59,040 --> 00:38:02,700
not just a filter bubble right it's not

00:38:00,450 --> 00:38:05,820
this study and others clearly show that

00:38:02,700 --> 00:38:07,470
it's not about showing content from Fox

00:38:05,820 --> 00:38:12,540
News in your feed if you're liberal or

00:38:07,470 --> 00:38:14,280
the other way around because to to learn

00:38:12,540 --> 00:38:18,270
from people to learn from those views

00:38:14,280 --> 00:38:21,270
that we see we also must trust them

00:38:18,270 --> 00:38:22,920
enough to believe what they share and

00:38:21,270 --> 00:38:24,630
this is not even you know even if you

00:38:22,920 --> 00:38:26,040
see Fox News doesn't lie with the view

00:38:24,630 --> 00:38:30,030
it's not even just about the source it's

00:38:26,040 --> 00:38:34,010
we're really a problematic set up that

00:38:30,030 --> 00:38:37,230
goes beyond the question filter bubble

00:38:34,010 --> 00:38:39,000
so we'll come back to that at the end a

00:38:37,230 --> 00:38:40,590
little bit let's come back to that and

00:38:39,000 --> 00:38:41,820
thank you for letting me derail you in

00:38:40,590 --> 00:38:43,230
the middle yeah but I didn't want you to

00:38:41,820 --> 00:38:44,820
go on to the second bit which is sort of

00:38:43,230 --> 00:38:46,260
a different topic without yeah no these

00:38:44,820 --> 00:38:50,640
are all great that's a great question

00:38:46,260 --> 00:38:53,850
and thank you let's talk about

00:38:50,640 --> 00:38:57,420
interpersonal trust there's a little bit

00:38:53,850 --> 00:39:03,030
of a big switch to a slightly different

00:38:57,420 --> 00:39:05,910
domain and here we asked about how we

00:39:03,030 --> 00:39:09,090
trust directly trust each other or other

00:39:05,910 --> 00:39:10,140
people so interpersonal trust thing

00:39:09,090 --> 00:39:13,410
about the sharing economy for example

00:39:10,140 --> 00:39:17,240
uber Airbnb despite the decline in

00:39:13,410 --> 00:39:20,790
general trusts or trust in institutions

00:39:17,240 --> 00:39:22,350
we seem to want to trust each other a

00:39:20,790 --> 00:39:25,770
little bit more like we jump into a

00:39:22,350 --> 00:39:30,330
stranger's car to stay in Airbnb they're

00:39:25,770 --> 00:39:30,750
mainly hosts and so forth but not for

00:39:30,330 --> 00:39:33,750
long

00:39:30,750 --> 00:39:35,490
III is coming in and maybe well

00:39:33,750 --> 00:39:37,710
undermine our trust in each other and

00:39:35,490 --> 00:39:41,280
let's see how and first let me talk

00:39:37,710 --> 00:39:42,750
about what we think is a new kind of a

00:39:41,280 --> 00:39:45,120
new concept in how we communicate with

00:39:42,750 --> 00:39:47,100
each other so traditionally we had

00:39:45,120 --> 00:39:50,959
computer mediated communication rights

00:39:47,100 --> 00:39:52,579
the idea that we have

00:39:50,959 --> 00:39:55,549
we communicate with each other through

00:39:52,579 --> 00:39:58,279
technology but we also assume that the

00:39:55,549 --> 00:40:00,799
sender adapts to the channel whatever

00:39:58,279 --> 00:40:02,839
they're using text messaging or video

00:40:00,799 --> 00:40:04,099
conferencing or whatever they'll adapt

00:40:02,839 --> 00:40:06,920
to the channel to craft their message

00:40:04,099 --> 00:40:10,369
but the message as they created it will

00:40:06,920 --> 00:40:13,670
be transmitted to the recipients so

00:40:10,369 --> 00:40:16,819
right now you are seeing me and this is

00:40:13,670 --> 00:40:21,640
pretty much a neutral evaluation of the

00:40:16,819 --> 00:40:23,509
pixels that are captured by the camera

00:40:21,640 --> 00:40:25,489
increasingly though we have

00:40:23,509 --> 00:40:27,170
communication channels where an agent in

00:40:25,489 --> 00:40:30,829
the middle can participate in the

00:40:27,170 --> 00:40:32,719
communication act suggest edits do

00:40:30,829 --> 00:40:36,140
modification or generate the entire

00:40:32,719 --> 00:40:40,429
communication on our behalf on a paper

00:40:36,140 --> 00:40:44,900
that will appear at JCM see Karen levy

00:40:40,429 --> 00:40:47,089
Jeff thank you and I call that a IMC or

00:40:44,900 --> 00:40:48,739
AI immediate communication so these are

00:40:47,089 --> 00:40:50,329
interpersonal communication or optimized

00:40:48,739 --> 00:40:52,429
augmented or even generated by

00:40:50,329 --> 00:40:55,939
algorithms to achieve specific

00:40:52,429 --> 00:41:01,579
communicative or relational outcome let

00:40:55,939 --> 00:41:06,140
me give you some examples many of you

00:41:01,579 --> 00:41:07,729
probably use Gmail or other kinds that

00:41:06,140 --> 00:41:11,359
allow you to generate other responses

00:41:07,729 --> 00:41:13,880
now and here for example we have my

00:41:11,359 --> 00:41:16,699
student Maurice emailing me with the

00:41:13,880 --> 00:41:22,239
dissertation proposal and these are some

00:41:16,699 --> 00:41:26,859
of the suggested responses from Google

00:41:22,239 --> 00:41:33,949
so this is where AI comes in and and

00:41:26,859 --> 00:41:36,799
notice that it's providing not only some

00:41:33,949 --> 00:41:39,079
suggestions but also a tone that even if

00:41:36,799 --> 00:41:44,179
I don't take the the suggestions I may

00:41:39,079 --> 00:41:46,699
be impacted by there are other places in

00:41:44,179 --> 00:41:50,539
more different contexts for example

00:41:46,699 --> 00:41:53,179
LinkedIn can auto generate your profile

00:41:50,539 --> 00:41:57,830
description from your the details of

00:41:53,179 --> 00:42:00,470
your CV more

00:41:57,830 --> 00:42:02,480
involved is the Google duplex that can

00:42:00,470 --> 00:42:04,220
have entire conversation on your behalf

00:42:02,480 --> 00:42:06,650
reflecting on your relationship with

00:42:04,220 --> 00:42:08,810
others by for example calling a business

00:42:06,650 --> 00:42:10,270
and this is just a current day and

00:42:08,810 --> 00:42:13,100
mostly text-based

00:42:10,270 --> 00:42:13,610
examples of such interactions in the

00:42:13,100 --> 00:42:16,070
future

00:42:13,610 --> 00:42:19,960
AI can have a greater even greater

00:42:16,070 --> 00:42:24,320
impact on our communication for example

00:42:19,960 --> 00:42:27,470
AI can make your text or maybe even your

00:42:24,320 --> 00:42:29,500
face seem more trustworthy even in live

00:42:27,470 --> 00:42:33,170
video conversation right so we can take

00:42:29,500 --> 00:42:35,690
me and maybe change my face in a way

00:42:33,170 --> 00:42:38,150
that is that will make me make you trust

00:42:35,690 --> 00:42:38,570
me more it will make me look at the

00:42:38,150 --> 00:42:41,300
camera

00:42:38,570 --> 00:42:44,930
in fact some phones already do that so

00:42:41,300 --> 00:42:49,430
with the impact on our own your

00:42:44,930 --> 00:42:51,619
valuations of who you're speaking to new

00:42:49,430 --> 00:42:54,110
AI can depict you dancing like a

00:42:51,619 --> 00:42:56,330
professional or lifting heavy weights on

00:42:54,110 --> 00:42:58,730
your online profiles for example without

00:42:56,330 --> 00:43:00,650
you having to actually do that

00:42:58,730 --> 00:43:02,420
or we can for example optimize your

00:43:00,650 --> 00:43:04,970
message or appearance for maximal

00:43:02,420 --> 00:43:07,070
persuasion for each specific recipient

00:43:04,970 --> 00:43:08,990
for example it can give me an accent

00:43:07,070 --> 00:43:11,020
that is localized to any one of these

00:43:08,990 --> 00:43:18,470
locations that were currently

00:43:11,020 --> 00:43:20,170
broadcasting to so to start looking into

00:43:18,470 --> 00:43:24,619
this world of AI immediate communication

00:43:20,170 --> 00:43:26,900
we had one very simple risk question

00:43:24,619 --> 00:43:29,300
which is will people evaluate others

00:43:26,900 --> 00:43:31,550
differently if they think that a I was

00:43:29,300 --> 00:43:34,280
involved in uttering their the other

00:43:31,550 --> 00:43:36,800
person's profile right so this is very

00:43:34,280 --> 00:43:41,080
much restricted to profile settings and

00:43:36,800 --> 00:43:44,119
in fact as I've shown a second to Airbnb

00:43:41,080 --> 00:43:46,790
so why would they evaluate AI profiles a

00:43:44,119 --> 00:43:48,619
written profess differently one

00:43:46,790 --> 00:43:50,990
assumption said that they want there

00:43:48,619 --> 00:43:52,520
will be no change will be indifferent

00:43:50,990 --> 00:43:54,650
there's some studies that show that in

00:43:52,520 --> 00:43:58,190
other context people have not changed

00:43:54,650 --> 00:44:00,230
their valuation they could see this as

00:43:58,190 --> 00:44:01,490
more trustworthy AI generated profiles

00:44:00,230 --> 00:44:03,080
because there is a machinery stick

00:44:01,490 --> 00:44:04,250
algorithmic Authority and they said the

00:44:03,080 --> 00:44:07,760
computer world tastes the computers

00:44:04,250 --> 00:44:10,250
don't lie they'll believe it more they

00:44:07,760 --> 00:44:11,120
can also believe it less because adding

00:44:10,250 --> 00:44:13,700
a

00:44:11,120 --> 00:44:18,710
I will maybe add more uncertainty about

00:44:13,700 --> 00:44:22,870
AI or about their the the person who

00:44:18,710 --> 00:44:25,100
wrote the profile and believe it less

00:44:22,870 --> 00:44:27,050
I'm gonna ask you again your fish which

00:44:25,100 --> 00:44:30,500
one you think happened so you better

00:44:27,050 --> 00:44:33,230
memorize this let's make it concrete

00:44:30,500 --> 00:44:36,020
first we use Airbnb as a research

00:44:33,230 --> 00:44:38,060
context I showed you Rick before the

00:44:36,020 --> 00:44:41,150
beginning of the talk this is a very

00:44:38,060 --> 00:44:42,710
high trust situation not only it's

00:44:41,150 --> 00:44:46,090
interesting for that reason we also had

00:44:42,710 --> 00:44:49,460
a lot of data and reliable measures of

00:44:46,090 --> 00:44:54,290
trustworthiness from a previous studies

00:44:49,460 --> 00:44:57,650
on Airbnb I was not in AI MC world so it

00:44:54,290 --> 00:45:00,290
was a great place for us to start and

00:44:57,650 --> 00:45:01,730
specifically we asked about Airbnb what

00:45:00,290 --> 00:45:03,290
potential guests evaluate the host

00:45:01,730 --> 00:45:07,270
differently if they believe a I was

00:45:03,290 --> 00:45:09,500
involved in uttering the host profile so

00:45:07,270 --> 00:45:12,110
again we did an online experiment like I

00:45:09,500 --> 00:45:13,700
mentioned and we led people to believe

00:45:12,110 --> 00:45:16,970
that certain profiles that they

00:45:13,700 --> 00:45:20,990
evaluated were written by AI we had a

00:45:16,970 --> 00:45:22,490
label on profiles that said general

00:45:20,990 --> 00:45:26,510
profiles not not always but that was the

00:45:22,490 --> 00:45:28,100
manipulation you'll see in a second in

00:45:26,510 --> 00:45:30,800
fact when we told them this is happening

00:45:28,100 --> 00:45:34,730
this is how we showed them this was

00:45:30,800 --> 00:45:38,090
happening we basically showed them a gif

00:45:34,730 --> 00:45:40,040
of profile getting automatically

00:45:38,090 --> 00:45:43,880
generated there are two important things

00:45:40,040 --> 00:45:45,620
to note here one is that we didn't use a

00:45:43,880 --> 00:45:47,630
word that official intelligence we just

00:45:45,620 --> 00:45:48,700
use we said generated by computers or

00:45:47,630 --> 00:45:50,870
something like that

00:45:48,700 --> 00:45:54,560
optimized profile would have something

00:45:50,870 --> 00:45:56,930
like that that did not use AI as we

00:45:54,560 --> 00:45:58,820
thought people might react negatively

00:45:56,930 --> 00:46:02,840
overly some people migrated negatively

00:45:58,820 --> 00:46:04,310
to that phrase and the second that all

00:46:02,840 --> 00:46:05,960
the profits that we use just like all

00:46:04,310 --> 00:46:08,180
the headlines were true in the previous

00:46:05,960 --> 00:46:10,910
study here all the profiles were

00:46:08,180 --> 00:46:12,440
actually written by hosts it was no AI

00:46:10,910 --> 00:46:16,280
was harmed in the making of this

00:46:12,440 --> 00:46:20,360
research everything was written by

00:46:16,280 --> 00:46:23,170
actual Airbnb hosts so here's the first

00:46:20,360 --> 00:46:25,010
experiment it was a very simple setup

00:46:23,170 --> 00:46:27,050
here we had

00:46:25,010 --> 00:46:30,220
ten prophets or participants rated the

00:46:27,050 --> 00:46:32,900
same ten profiles for trustworthiness

00:46:30,220 --> 00:46:34,369
some witch selected five that we knew

00:46:32,900 --> 00:46:35,869
from previous studies are usually

00:46:34,369 --> 00:46:38,420
getting high trustworthiness course

00:46:35,869 --> 00:46:40,730
inspired are getting usually we get low

00:46:38,420 --> 00:46:43,910
trustworthiness course and in one

00:46:40,730 --> 00:46:46,040
condition we just showed it to our

00:46:43,910 --> 00:46:48,050
participant and said these were these

00:46:46,040 --> 00:46:51,050
are Airbnb profiles rate them for

00:46:48,050 --> 00:46:53,119
trustworthiness and another condition we

00:46:51,050 --> 00:46:54,770
showed the same profiles to other

00:46:53,119 --> 00:46:56,270
participants and we showed them be we

00:46:54,770 --> 00:46:59,599
told them these were written by AI

00:46:56,270 --> 00:47:07,339
please rate them for trustworthiness of

00:46:59,599 --> 00:47:12,260
the hosts so that's that was the setup

00:47:07,339 --> 00:47:14,630
of the experiment again we ran this on

00:47:12,260 --> 00:47:18,130
Amazon Mechanical Turk and now let me

00:47:14,630 --> 00:47:20,089
just show you the results for the

00:47:18,130 --> 00:47:21,980
written by human rights is the

00:47:20,089 --> 00:47:25,250
participants that were told that this

00:47:21,980 --> 00:47:28,369
was written by Airbnb hosts as you can

00:47:25,250 --> 00:47:29,750
see here we have ten participants and we

00:47:28,369 --> 00:47:34,640
have the trustworthiness course on the

00:47:29,750 --> 00:47:38,390
y-axis for each of those participant

00:47:34,640 --> 00:47:40,220
aggregated averaged over all the Raiders

00:47:38,390 --> 00:47:42,050
everything everybody leverage then one

00:47:40,220 --> 00:47:44,599
thing to look at this the reliability

00:47:42,050 --> 00:47:46,670
the we have still have five high

00:47:44,599 --> 00:47:49,700
trustworthy and five lower trustworthy

00:47:46,670 --> 00:47:55,430
profiles so this measure of trust is the

00:47:49,700 --> 00:48:01,099
wheels is pretty reliable so now let's

00:47:55,430 --> 00:48:03,109
see what happened when the same profiles

00:48:01,099 --> 00:48:07,190
were rated by people that thought that

00:48:03,109 --> 00:48:09,109
the purpose were written by AI Jawfish

00:48:07,190 --> 00:48:11,230
higher trustworthiness largest

00:48:09,109 --> 00:48:11,230
wilderness

00:48:14,819 --> 00:48:21,430
let's dress woody but I don't whether

00:48:17,589 --> 00:48:26,740
I'm exposing my own biases here yeah

00:48:21,430 --> 00:48:30,400
let's write who doesn't love a guy well

00:48:26,740 --> 00:48:31,450
it turns out it was a trick question but

00:48:30,400 --> 00:48:34,720
there was because there was no

00:48:31,450 --> 00:48:37,240
difference at all in fact spot on every

00:48:34,720 --> 00:48:43,000
single profile people have had in the

00:48:37,240 --> 00:48:45,250
same way and but you know that this if

00:48:43,000 --> 00:48:47,440
this was our final results I would not

00:48:45,250 --> 00:48:50,980
have shown you this work today so I will

00:48:47,440 --> 00:48:54,490
continue and show you when we think

00:48:50,980 --> 00:48:58,750
there is actually a difference okay so

00:48:54,490 --> 00:49:00,430
that was the first study but what we did

00:48:58,750 --> 00:49:03,660
in this study it was very separate right

00:49:00,430 --> 00:49:06,700
the inner evaluated AI or the evaluated

00:49:03,660 --> 00:49:08,410
human profiles but we asked what happens

00:49:06,700 --> 00:49:09,789
if those profiles are mixed so there's

00:49:08,410 --> 00:49:11,920
sometimes seeing a I've written

00:49:09,789 --> 00:49:13,510
professor sometimes were seeing human

00:49:11,920 --> 00:49:16,359
written professed in the same system

00:49:13,510 --> 00:49:19,029
right so we had experiment two we had

00:49:16,359 --> 00:49:21,849
the same ten profile and now we didn't

00:49:19,029 --> 00:49:23,950
tell them we told them that they were

00:49:21,849 --> 00:49:25,569
either written by a host or by a I I'm

00:49:23,950 --> 00:49:26,710
not gonna tell you about this experiment

00:49:25,569 --> 00:49:29,609
I'm gonna tell you about another

00:49:26,710 --> 00:49:33,910
experiment that we even more elaborate

00:49:29,609 --> 00:49:35,589
that asked the same question and this

00:49:33,910 --> 00:49:37,900
one was more elaborate in that which

00:49:35,589 --> 00:49:39,460
shows the profiles differently instead

00:49:37,900 --> 00:49:41,799
of choosing high low trustworthiness

00:49:39,460 --> 00:49:46,260
profiles we chose prefers that we

00:49:41,799 --> 00:49:50,279
evaluated with participants in advance

00:49:46,260 --> 00:49:52,960
that look like they were written by AI

00:49:50,279 --> 00:49:57,760
or did not look like they're written by

00:49:52,960 --> 00:49:59,650
I call it AI nests right so the

00:49:57,760 --> 00:50:02,380
assumption that some profiles people

00:49:59,650 --> 00:50:04,720
will think are more likely to have been

00:50:02,380 --> 00:50:07,720
written by a sample some are less let me

00:50:04,720 --> 00:50:10,089
give you an example here this is from a

00:50:07,720 --> 00:50:13,119
pretest right so and it was fairly as I

00:50:10,089 --> 00:50:16,779
say very reliable here is a proffer that

00:50:13,119 --> 00:50:19,119
was rated high on the AI scale hi hello

00:50:16,779 --> 00:50:19,839
I am Sebastian originally from Berlin

00:50:19,119 --> 00:50:22,839
Germany

00:50:19,839 --> 00:50:25,690
this was a very got a high a honest

00:50:22,839 --> 00:50:26,560
score and this one let me open with a

00:50:25,690 --> 00:50:30,100
huge

00:50:26,560 --> 00:50:33,820
to everyone reading this was rated as

00:50:30,100 --> 00:50:40,810
low likelihood to be have been written

00:50:33,820 --> 00:50:45,070
by AI so we had 30 such profiles for

00:50:40,810 --> 00:50:48,280
each participant we chose five random a

00:50:45,070 --> 00:50:52,540
high or in five random low AI profiles

00:50:48,280 --> 00:50:54,340
and we had a number of conditions I'm

00:50:52,540 --> 00:50:57,160
going to show only three of them we had

00:50:54,340 --> 00:50:59,410
a few more in one we did the same thing

00:50:57,160 --> 00:51:02,500
we told them these are all were all

00:50:59,410 --> 00:51:04,900
written by the host in the second we

00:51:02,500 --> 00:51:06,580
told them this may have been written by

00:51:04,900 --> 00:51:10,270
the host or may have been written by AI

00:51:06,580 --> 00:51:12,070
and in the third we labeled them

00:51:10,270 --> 00:51:13,720
everything that was it looked like a I

00:51:12,070 --> 00:51:17,740
we actually labeled the I and everything

00:51:13,720 --> 00:51:22,980
that was not we labeled as we didn't we

00:51:17,740 --> 00:51:27,790
label this assuming that was a person

00:51:22,980 --> 00:51:29,530
okay let's see what happened now so here

00:51:27,790 --> 00:51:32,470
we're comparing the overall scores for

00:51:29,530 --> 00:51:34,330
all the professors that were in the AI

00:51:32,470 --> 00:51:37,210
that looked like AI vs. didn't look like

00:51:34,330 --> 00:51:39,160
AI so we actually see when when we told

00:51:37,210 --> 00:51:40,690
this is the first control condition so

00:51:39,160 --> 00:51:41,950
we didn't we didn't say anything about

00:51:40,690 --> 00:51:44,680
AI

00:51:41,950 --> 00:51:46,900
so the participant thought that all

00:51:44,680 --> 00:51:50,160
these prophets were written by the host

00:51:46,900 --> 00:51:53,910
and they rated the AI profiles the

00:51:50,160 --> 00:52:00,340
higher they'll hire trustworthiness

00:51:53,910 --> 00:52:02,500
scores in this control settings however

00:52:00,340 --> 00:52:07,110
when we told them that these are mixed

00:52:02,500 --> 00:52:10,390
AI and human profiles there was a

00:52:07,110 --> 00:52:12,640
decrease in trust for everything that

00:52:10,390 --> 00:52:15,370
looked like AI and an increase for

00:52:12,640 --> 00:52:17,440
everything that looked like human so

00:52:15,370 --> 00:52:18,790
remember we they were not labeled like

00:52:17,440 --> 00:52:21,190
this is just the assumption or they look

00:52:18,790 --> 00:52:24,550
they look at the profile they kind of

00:52:21,190 --> 00:52:27,880
think it was AI they trust worthiness

00:52:24,550 --> 00:52:30,550
settings grow when the same profiles

00:52:27,880 --> 00:52:36,160
were labeled as AI it was even a more

00:52:30,550 --> 00:52:38,740
significant drop where people lowered

00:52:36,160 --> 00:52:40,150
even more the trustworthiness course for

00:52:38,740 --> 00:52:44,920
those simple

00:52:40,150 --> 00:52:46,900
was compared to the only profiles in

00:52:44,920 --> 00:52:48,640
fact with another condition where we

00:52:46,900 --> 00:52:51,250
collected some data

00:52:48,640 --> 00:52:52,750
how much participant starts something

00:52:51,250 --> 00:52:54,880
was a I when they gave it a

00:52:52,750 --> 00:52:57,279
trustworthiness score and we can see

00:52:54,880 --> 00:52:59,079
that the even when there was a slight

00:52:57,279 --> 00:53:01,539
suspicion that something was a I

00:52:59,079 --> 00:53:09,190
generated the trustworthiness go over

00:53:01,539 --> 00:53:13,599
the plummeted so there's no effect only

00:53:09,190 --> 00:53:20,010
our rating except where the content is

00:53:13,599 --> 00:53:25,990
mixed so when we have this world of

00:53:20,010 --> 00:53:28,119
mixed information we have the AI labeled

00:53:25,990 --> 00:53:30,369
or a I suspected profiles receiving

00:53:28,119 --> 00:53:33,549
lower trustworthiness rating for this

00:53:30,369 --> 00:53:37,210
course we used in the paper this was

00:53:33,549 --> 00:53:38,289
published in KY 2009 we talked about how

00:53:37,210 --> 00:53:42,910
this might be explained by the

00:53:38,289 --> 00:53:46,480
elaboration likelihood model so 2019

00:53:42,910 --> 00:53:50,529
thank you did I say 9 yeah no we only

00:53:46,480 --> 00:53:52,299
thought about this last year so we

00:53:50,529 --> 00:53:54,099
talked about how this might explain that

00:53:52,299 --> 00:53:56,730
elaboration likelihood model all right

00:53:54,099 --> 00:53:58,869
so we have a mixed source environment

00:53:56,730 --> 00:54:00,430
maybe that makes the source of the

00:53:58,869 --> 00:54:02,680
profile or thinking about the source

00:54:00,430 --> 00:54:05,740
more salient and leading participants to

00:54:02,680 --> 00:54:09,279
engage in more careful processing of the

00:54:05,740 --> 00:54:11,170
profile the the theory calls it central

00:54:09,279 --> 00:54:16,029
route and therefore causing them to

00:54:11,170 --> 00:54:17,529
reduce their score regardless of what

00:54:16,029 --> 00:54:19,869
theory explaining we did give it a new

00:54:17,529 --> 00:54:23,380
name we called it the replicant effect

00:54:19,869 --> 00:54:26,470
with apologies to philip k dick and

00:54:23,380 --> 00:54:28,270
ridley scott because in our experimental

00:54:26,470 --> 00:54:32,260
world populated by both humans and

00:54:28,270 --> 00:54:33,369
non-human agents the replicants in such

00:54:32,260 --> 00:54:36,849
environment the knowledge or even

00:54:33,369 --> 00:54:41,460
suspicion that something is a replicant

00:54:36,849 --> 00:54:41,460
reportage replicant results in distrust

00:54:41,819 --> 00:54:48,789
finally one interesting finding was the

00:54:45,220 --> 00:54:51,520
idea of AI NS people had strong

00:54:48,789 --> 00:54:52,869
suspicions and reliable suspicions but

00:54:51,520 --> 00:54:56,230
what makes

00:54:52,869 --> 00:54:59,140
a profile AI what makes it likely to be

00:54:56,230 --> 00:55:02,019
written by AI and and when it is more

00:54:59,140 --> 00:55:03,640
likely written by a human we talked a

00:55:02,019 --> 00:55:06,819
little bit more about that in a paper

00:55:03,640 --> 00:55:09,819
are we trying to do much larger studies

00:55:06,819 --> 00:55:14,200
that understand these perceptions better

00:55:09,819 --> 00:55:15,430
in multiple settings in general I think

00:55:14,200 --> 00:55:20,140
there are a lot of interesting questions

00:55:15,430 --> 00:55:24,249
to be asked about AI MC including what

00:55:20,140 --> 00:55:26,640
we started is how it might impact what

00:55:24,249 --> 00:55:33,640
kind of lasting effect we will have on

00:55:26,640 --> 00:55:36,579
trust the so in our interactions or

00:55:33,640 --> 00:55:37,990
relationships going forward as well the

00:55:36,579 --> 00:55:39,819
second more even more immediate

00:55:37,990 --> 00:55:41,529
questions you can ask is why how we

00:55:39,819 --> 00:55:43,799
might impact language use and

00:55:41,529 --> 00:55:47,170
communication effectiveness for example

00:55:43,799 --> 00:55:50,589
my colleague just Hornstein and Martha

00:55:47,170 --> 00:55:53,170
young at Cornell showed that first of

00:55:50,589 --> 00:55:55,869
all the language that is used by the

00:55:53,170 --> 00:56:01,690
smart replied is very positive it my

00:55:55,869 --> 00:56:03,460
cause or might help Prime you to use

00:56:01,690 --> 00:56:04,539
more positive language since

00:56:03,460 --> 00:56:07,779
experimentally we're working on

00:56:04,539 --> 00:56:10,710
expanding but also in a new paper that

00:56:07,779 --> 00:56:13,749
blamed for communication outcome is that

00:56:10,710 --> 00:56:17,589
associated differently if AI was used in

00:56:13,749 --> 00:56:20,640
the communication so this is people

00:56:17,589 --> 00:56:23,650
doing a task using AI or not using AI

00:56:20,640 --> 00:56:26,319
but you asking them about blame where

00:56:23,650 --> 00:56:29,140
the tasks fail and that turns out to be

00:56:26,319 --> 00:56:30,999
different when AI was involved and

00:56:29,140 --> 00:56:33,999
finally their important policy questions

00:56:30,999 --> 00:56:36,069
of how we should build and design these

00:56:33,999 --> 00:56:38,650
technologies into product should the use

00:56:36,069 --> 00:56:40,450
of AI be revealed for example how do we

00:56:38,650 --> 00:56:42,759
feel about the maternity effect of

00:56:40,450 --> 00:56:46,920
language and maybe even appearance that

00:56:42,759 --> 00:56:50,349
can result and so for so many different

00:56:46,920 --> 00:56:56,170
questions that we should address as we

00:56:50,349 --> 00:56:58,239
plunge into this world of AI MC all of

00:56:56,170 --> 00:57:00,970
these questions that I mentioned and

00:56:58,239 --> 00:57:05,079
some of the studies you can get from

00:57:00,970 --> 00:57:06,210
this working not quite the final version

00:57:05,079 --> 00:57:09,480
of the app

00:57:06,210 --> 00:57:13,500
humming JCM see paper so this is a bit

00:57:09,480 --> 00:57:16,920
leaf slash a IMC - paper case-sensitive

00:57:13,500 --> 00:57:23,910
I think go back and talk about trust

00:57:16,920 --> 00:57:25,349
before we close as I make the case in

00:57:23,910 --> 00:57:27,470
the beginning without trust our

00:57:25,349 --> 00:57:30,420
democracy in our society cannot function

00:57:27,470 --> 00:57:32,070
in this talk I showed how the partisan

00:57:30,420 --> 00:57:35,160
tendencies impact our evaluation of

00:57:32,070 --> 00:57:37,170
content in our information ecosystem and

00:57:35,160 --> 00:57:39,780
therefore impact our shirred fact-based

00:57:37,170 --> 00:57:41,430
I also showed our AI involvement in

00:57:39,780 --> 00:57:44,330
counted generation may challenge our

00:57:41,430 --> 00:57:48,109
evaluation of trustworthiness of others

00:57:44,330 --> 00:57:50,880
these trends will obviously continue

00:57:48,109 --> 00:57:54,270
increasingly partisan content content

00:57:50,880 --> 00:57:57,570
fake or real will continue to push us to

00:57:54,270 --> 00:58:00,150
silos of anger and distrust online that

00:57:57,570 --> 00:58:03,060
hard to see from this research how we

00:58:00,150 --> 00:58:05,609
can overcome this more exposure to this

00:58:03,060 --> 00:58:07,710
increasingly troubled or some trends all

00:58:05,609 --> 00:58:10,050
the way to deep fake which are maybe an

00:58:07,710 --> 00:58:11,700
extreme form of a IMC would undermine

00:58:10,050 --> 00:58:14,730
our trust in any Content that we see

00:58:11,700 --> 00:58:18,420
online text video of image interpersonal

00:58:14,730 --> 00:58:20,339
otherwise AI generated or not when this

00:58:18,420 --> 00:58:24,030
happens our assumptions about how our

00:58:20,339 --> 00:58:26,790
institutions or system operating can be

00:58:24,030 --> 00:58:29,190
trusted and how we can trust each other

00:58:26,790 --> 00:58:32,730
may not allow us to trust in the same

00:58:29,190 --> 00:58:35,490
ease and regularity that we do today in

00:58:32,730 --> 00:58:39,119
online and maybe even offline settings

00:58:35,490 --> 00:58:44,300
and this might have dire consequences if

00:58:39,119 --> 00:58:44,300
you read the Joffe to be read fall yet

00:58:47,690 --> 00:58:52,080
so let me give you a little spoiler it's

00:58:50,700 --> 00:58:55,609
not really spoiler it's 900-page book

00:58:52,080 --> 00:58:58,800
and I'm gonna talk about page 200 or so

00:58:55,609 --> 00:59:01,589
in for Neal Stephenson imagine is

00:58:58,800 --> 00:59:03,119
dystopian but not far-fetched future

00:59:01,589 --> 00:59:05,970
that is dominated by this information

00:59:03,119 --> 00:59:08,040
and in this book a group for the massive

00:59:05,970 --> 00:59:11,070
hoax that aims to convince the world

00:59:08,040 --> 00:59:13,859
that mob in Utah was destroyed by a

00:59:11,070 --> 00:59:16,619
nuclear explosion this group the

00:59:13,859 --> 00:59:19,369
perpetrators were meant to cause enough

00:59:16,619 --> 00:59:20,839
panic for society to reconsider the

00:59:19,369 --> 00:59:22,910
basic elements of our information

00:59:20,839 --> 00:59:26,539
ecosystem the Internet of the miasma

00:59:22,910 --> 00:59:29,289
that's called in the book that those are

00:59:26,539 --> 00:59:32,029
the ones that allowed the hoax to happen

00:59:29,289 --> 00:59:34,700
it may be time for us to consider a

00:59:32,029 --> 00:59:39,019
similar move I don't mean that we need

00:59:34,700 --> 00:59:41,269
to orchestrate a host of nuked mob but I

00:59:39,019 --> 00:59:44,569
do think that we need to rethink the

00:59:41,269 --> 00:59:47,509
infrastructure of how we trust each

00:59:44,569 --> 00:59:49,279
other online trust of course is the

00:59:47,509 --> 00:59:51,289
wicked problem there is no obvious

00:59:49,279 --> 00:59:53,779
solution to it there is no stopping

00:59:51,289 --> 00:59:55,670
condition to know that you're done it

00:59:53,779 --> 00:59:58,930
requires multiple stakeholders and many

00:59:55,670 --> 01:00:01,880
disciplines to tackle technology makers

00:59:58,930 --> 01:00:04,869
policy people education media and

01:00:01,880 --> 01:00:09,499
journalism and yes organizations and

01:00:04,869 --> 01:00:13,450
such as Mozilla some of the questions we

01:00:09,499 --> 01:00:16,039
can ask how to solve it include

01:00:13,450 --> 01:00:18,529
continuing to understand and research

01:00:16,039 --> 01:00:20,390
how people trust each other and what can

01:00:18,529 --> 01:00:21,829
impact that trust and there's a lot of

01:00:20,390 --> 01:00:24,980
focus on misinformation research in the

01:00:21,829 --> 01:00:27,049
last few years but how human human trust

01:00:24,980 --> 01:00:29,739
is established undermined and evaluated

01:00:27,049 --> 01:00:31,910
online system could and should be

01:00:29,739 --> 01:00:34,309
continued

01:00:31,910 --> 01:00:37,460
topic of research we also need to

01:00:34,309 --> 01:00:39,769
understand how to design develop and

01:00:37,460 --> 01:00:41,569
evaluate systems and infrastructure that

01:00:39,769 --> 01:00:43,700
help promote and maintain trust for the

01:00:41,569 --> 01:00:45,739
incredible content which is not the case

01:00:43,700 --> 01:00:49,130
on the web today what we have system

01:00:45,739 --> 01:00:51,739
that perhaps do the opposite and finally

01:00:49,130 --> 01:00:53,239
we need to understand what policy

01:00:51,739 --> 01:00:57,049
ethical and regulatory frameworks can

01:00:53,239 --> 01:01:01,339
support these systems and we need to do

01:00:57,049 --> 01:01:03,259
it probably today in the meantime I

01:01:01,339 --> 01:01:05,180
would leave you with my twitter handle a

01:01:03,259 --> 01:01:08,630
lovely photo of the Cornell tech campus

01:01:05,180 --> 01:01:12,640
at night and I'm happy to answer any

01:01:08,630 --> 01:01:12,640
questions thank you

01:01:12,809 --> 01:01:17,530
thank you very much here I'm gonna

01:01:14,619 --> 01:01:22,359
single-handedly climb for you there's a

01:01:17,530 --> 01:01:24,160
couple of questions so I'm gonna ask one

01:01:22,359 --> 01:01:26,650
but then I first I'm gonna give you one

01:01:24,160 --> 01:01:29,140
from Jared I don't know where he's

01:01:26,650 --> 01:01:31,809
sitting somewhere in the world I have no

01:01:29,140 --> 01:01:35,920
idea his local time is 153 so what's

01:01:31,809 --> 01:01:37,540
that Chicago something anyway are their

01:01:35,920 --> 01:01:39,700
communication mechanics that can modify

01:01:37,540 --> 01:01:41,770
or increase trust such as encryption

01:01:39,700 --> 01:01:45,430
verified identities digital signing

01:01:41,770 --> 01:01:46,510
anything like that yeah so it's

01:01:45,430 --> 01:01:49,349
interesting because all this is a

01:01:46,510 --> 01:01:51,549
contract big mechanisms are not always

01:01:49,349 --> 01:01:54,910
increasing trust right so in something

01:01:51,549 --> 01:01:57,400
some cases they replace trust and and

01:01:54,910 --> 01:02:00,790
make us not need to basically you're not

01:01:57,400 --> 01:02:03,730
taking risk anymore right so it they may

01:02:00,790 --> 01:02:05,890
work in some limited settings and may

01:02:03,730 --> 01:02:07,500
help to that but they're not gonna be

01:02:05,890 --> 01:02:11,380
helped more generally what trust is

01:02:07,500 --> 01:02:13,680
really a system where you make yourself

01:02:11,380 --> 01:02:17,260
vulnerable right so I think there are

01:02:13,680 --> 01:02:21,970
things that you can do anything from

01:02:17,260 --> 01:02:24,280
content verification to exchange support

01:02:21,970 --> 01:02:26,799
in exchange but it's not gonna be our

01:02:24,280 --> 01:02:29,319
only solution um I have a more

01:02:26,799 --> 01:02:31,119
fundamental question which is so I'm not

01:02:29,319 --> 01:02:33,640
convinced the word trust means anything

01:02:31,119 --> 01:02:34,780
I think I've lost any track of what it

01:02:33,640 --> 01:02:37,869
means yeah

01:02:34,780 --> 01:02:40,119
and in particular I'm interested in

01:02:37,869 --> 01:02:42,069
trying to understand let me tell you a

01:02:40,119 --> 01:02:43,900
particular case studies so we're one of

01:02:42,069 --> 01:02:45,609
the efforts that we're doing is around

01:02:43,900 --> 01:02:46,480
web of things right which is trying to

01:02:45,609 --> 01:02:50,440
say well that we should have an

01:02:46,480 --> 01:02:52,780
open-source set of interactions with

01:02:50,440 --> 01:02:54,520
Internet of Things right we have this

01:02:52,780 --> 01:02:56,290
problem that of course we're up against

01:02:54,520 --> 01:02:59,079
some of the biggest companies in the

01:02:56,290 --> 01:03:01,150
world as is I want right we seem to do

01:02:59,079 --> 01:03:03,220
that a lot but if you look at Amazon and

01:03:01,150 --> 01:03:06,160
you say well do people trust Amazon

01:03:03,220 --> 01:03:07,420
right people say yes and then you say

01:03:06,160 --> 01:03:08,440
well what tell me what you mean

01:03:07,420 --> 01:03:10,599
you know when was the last time you

01:03:08,440 --> 01:03:13,930
trusted Amazon and what they mean is

01:03:10,599 --> 01:03:15,430
when I click on USB cable and it says

01:03:13,930 --> 01:03:17,559
you will do this will be delivered in

01:03:15,430 --> 01:03:20,349
two days people believe that the USB

01:03:17,559 --> 01:03:21,940
cable will be delivered in two days the

01:03:20,349 --> 01:03:24,609
question is the degree to which that

01:03:21,940 --> 01:03:25,550
bleeds over to I think this company

01:03:24,609 --> 01:03:29,810
should have an open my

01:03:25,550 --> 01:03:31,910
crow phone in my house or you know these

01:03:29,810 --> 01:03:33,260
questions of trust that go cross domains

01:03:31,910 --> 01:03:35,540
because all of the both of the cases you

01:03:33,260 --> 01:03:38,390
looks at it's sort of one thing right

01:03:35,540 --> 01:03:41,510
like you're not asking do I trust

01:03:38,390 --> 01:03:43,070
Breitbart as a consumer no I don't right

01:03:41,510 --> 01:03:44,990
like they haven't I haven't to disagree

01:03:43,070 --> 01:03:46,790
with with you know the information they

01:03:44,990 --> 01:03:48,860
present do I think they're going to

01:03:46,790 --> 01:03:50,780
steal my credit card information well

01:03:48,860 --> 01:03:52,400
they're a big company they're probably

01:03:50,780 --> 01:03:54,740
not gonna steal my credit card

01:03:52,400 --> 01:03:56,510
information so those questions about

01:03:54,740 --> 01:03:58,520
what is this trust and what how does

01:03:56,510 --> 01:04:04,010
that manifest have you seen any work

01:03:58,520 --> 01:04:05,540
that talks to that question yes and

01:04:04,010 --> 01:04:08,510
let's separate the two things right

01:04:05,540 --> 01:04:10,280
there's a and I think you're correct to

01:04:08,510 --> 01:04:12,890
point out that trust in informations

01:04:10,280 --> 01:04:14,720
it'll be different than the Trust's

01:04:12,890 --> 01:04:16,550
where I make myself vulnerable right so

01:04:14,720 --> 01:04:21,220
it's that it's not quite the same thing

01:04:16,550 --> 01:04:25,070
in our work on trustworthiness we asked

01:04:21,220 --> 01:04:27,830
specifically about different factors of

01:04:25,070 --> 01:04:32,350
trust so we asked about this is for

01:04:27,830 --> 01:04:34,310
Meyer in 1975 paper and it talks about

01:04:32,350 --> 01:04:37,760
benevolence it talks about integrity it

01:04:34,310 --> 01:04:40,340
talks about ability and that's also true

01:04:37,760 --> 01:04:43,250
perhaps for organizations as well and

01:04:40,340 --> 01:04:45,260
making yourself making those decisions

01:04:43,250 --> 01:04:50,420
on how for example you make it available

01:04:45,260 --> 01:04:55,520
to the Amazon microphone versus you know

01:04:50,420 --> 01:04:57,440
whether or not there's a lower-risk

01:04:55,520 --> 01:05:00,290
situation where they send you a USB

01:04:57,440 --> 01:05:03,010
cable could be impacted by your

01:05:00,290 --> 01:05:05,510
evaluation of the company under in those

01:05:03,010 --> 01:05:07,520
dimensions yeah so I definitely think

01:05:05,510 --> 01:05:08,810
there is a relationship there there is

01:05:07,520 --> 01:05:12,170
an affiliation that's being made and

01:05:08,810 --> 01:05:14,150
there is impact on how people evaluate

01:05:12,170 --> 01:05:16,730
these corporations and how these

01:05:14,150 --> 01:05:19,340
products yeah no I think it's insert

01:05:16,730 --> 01:05:22,790
further research here right yeah I have

01:05:19,340 --> 01:05:26,630
one more question this is from Benjamin

01:05:22,790 --> 01:05:29,330
Alaska I was in Toronto maybe Ben is in

01:05:26,630 --> 01:05:31,310
Toronto - I don't know do explicit

01:05:29,330 --> 01:05:33,950
labels on machine generated content

01:05:31,310 --> 01:05:39,380
improved perceptions or trust of human

01:05:33,950 --> 01:05:41,720
generated content we have in our data

01:05:39,380 --> 01:05:43,370
it's sort of the missing column isn't it

01:05:41,720 --> 01:05:45,980
it's third yeah yeah no we don't have

01:05:43,370 --> 01:05:49,280
the human rating seems to go up a little

01:05:45,980 --> 01:05:51,320
bit but the results we I cannot say with

01:05:49,280 --> 01:05:53,480
certainty I think I think something is

01:05:51,320 --> 01:05:56,630
happening where people trust the human

01:05:53,480 --> 01:05:59,270
content more but I cannot say that for

01:05:56,630 --> 01:06:02,450
sure definitely the difference had

01:05:59,270 --> 01:06:04,010
changed so I think there is there's

01:06:02,450 --> 01:06:06,350
multiple effects you're right so the

01:06:04,010 --> 01:06:08,000
idea that when you have this mixed

01:06:06,350 --> 01:06:11,420
content that people will trust a human

01:06:08,000 --> 01:06:12,880
labeled one more or I don't know if you

01:06:11,420 --> 01:06:16,880
go up with definitely you trust it more

01:06:12,880 --> 01:06:21,440
what will happen if some people start

01:06:16,880 --> 01:06:24,500
using this content but suddenly if it's

01:06:21,440 --> 01:06:26,360
not exposed for example your human

01:06:24,500 --> 01:06:27,800
written profiles they could be suspected

01:06:26,360 --> 01:06:30,440
as they are as we've shown and therefore

01:06:27,800 --> 01:06:31,880
suffer as well without any action on

01:06:30,440 --> 01:06:34,100
your part I think they were fascinating

01:06:31,880 --> 01:06:36,110
questions to be asking and measured in

01:06:34,100 --> 01:06:37,430
those settings yeah well and I think

01:06:36,110 --> 01:06:38,840
again there's some intra in that

01:06:37,430 --> 01:06:41,360
particular case there's this interesting

01:06:38,840 --> 01:06:44,990
question how much do you trust air B&B

01:06:41,360 --> 01:06:47,900
right as the underlying platform that is

01:06:44,990 --> 01:06:49,790
able that because again the failure mode

01:06:47,900 --> 01:06:52,100
there is sort of becoming something

01:06:49,790 --> 01:06:54,170
people are aware of right yeah and

01:06:52,100 --> 01:06:54,860
that's both the failure mode and then

01:06:54,170 --> 01:06:58,190
there's sort of these ethical

01:06:54,860 --> 01:06:59,630
considerations about how much do you you

01:06:58,190 --> 01:07:02,480
know if you are you concerned that that

01:06:59,630 --> 01:07:05,780
air B&B is like taking away housing from

01:07:02,480 --> 01:07:07,730
from regular people yeah to trust trust

01:07:05,780 --> 01:07:11,390
but not only right because we think I

01:07:07,730 --> 01:07:13,520
think the the matter of urban the only

01:07:11,390 --> 01:07:15,110
works because people trust the platform

01:07:13,520 --> 01:07:17,210
and mostly trust the other individuals

01:07:15,110 --> 01:07:19,850
on the other side but we see an impact

01:07:17,210 --> 01:07:22,010
of such technologies is perhaps in the

01:07:19,850 --> 01:07:23,630
fairness of how they trust the other so

01:07:22,010 --> 01:07:26,030
people will end up with Airbnb hosts but

01:07:23,630 --> 01:07:27,410
they may be consistently selecting one

01:07:26,030 --> 01:07:30,740
host over the other because of the

01:07:27,410 --> 01:07:33,230
perception of language that their host

01:07:30,740 --> 01:07:35,690
uses so I think or maybe it's already

01:07:33,230 --> 01:07:39,760
you know luckily for them that's a

01:07:35,690 --> 01:07:42,230
profit it's a company for now you know

01:07:39,760 --> 01:07:44,960
putting aside their impact on local

01:07:42,230 --> 01:07:47,570
economies people are willing to trust

01:07:44,960 --> 01:07:49,100
but they're making those decisions in

01:07:47,570 --> 01:07:51,920
uniform way let me just give another

01:07:49,100 --> 01:07:52,849
example right this you have this the

01:07:51,920 --> 01:07:55,450
same

01:07:52,849 --> 01:07:58,039
similar gays fixing that the Apple I

01:07:55,450 --> 01:08:01,400
think Apple devices do now right so they

01:07:58,039 --> 01:08:03,200
can make you look at the other speaker

01:08:01,400 --> 01:08:05,420
warning having a video conversation

01:08:03,200 --> 01:08:07,729
again this is the simple use of AI

01:08:05,420 --> 01:08:09,410
may impact your evaluation of trust may

01:08:07,729 --> 01:08:11,089
also have a direct impact on people who

01:08:09,410 --> 01:08:12,680
are not using it we don't know which way

01:08:11,089 --> 01:08:14,440
right the people who view not using it

01:08:12,680 --> 01:08:16,489
you may be seen as less trustworthy

01:08:14,440 --> 01:08:18,500
because you're not looking at the camera

01:08:16,489 --> 01:08:20,960
but you also may seem as more

01:08:18,500 --> 01:08:23,210
trustworthy because you know that

01:08:20,960 --> 01:08:26,119
looking at the camera part is fake yeah

01:08:23,210 --> 01:08:31,759
so yeah and I wonder how long these cues

01:08:26,119 --> 01:08:33,889
that we sort of currently treat as as

01:08:31,759 --> 01:08:36,020
signals right I mean people are pretty

01:08:33,889 --> 01:08:37,849
good at learning noise and signal and as

01:08:36,020 --> 01:08:38,900
signal changes into noise people are

01:08:37,849 --> 01:08:40,460
pretty good at picking that up as well

01:08:38,900 --> 01:08:42,020
that's right so there will be long-term

01:08:40,460 --> 01:08:44,599
consequences I think you're the same

01:08:42,020 --> 01:08:47,690
again another of my favorite examples is

01:08:44,599 --> 01:08:50,179
Gmail right so if Gmail basically trains

01:08:47,690 --> 01:08:54,279
us to respond in an overly positive

01:08:50,179 --> 01:08:57,589
manner to mundane day-to-day questions

01:08:54,279 --> 01:09:00,020
what would it mean if I pick well first

01:08:57,589 --> 01:09:01,369
of all would I be primed to use this

01:09:00,020 --> 01:09:03,170
kind of language and what does it mean

01:09:01,369 --> 01:09:05,389
for people not using it and we'll just

01:09:03,170 --> 01:09:07,279
now yes I do want to join you for the

01:09:05,389 --> 01:09:10,580
movies today yeah got an exclamation

01:09:07,279 --> 01:09:11,980
point and you'll be seen as having a

01:09:10,580 --> 01:09:15,619
different kind of interaction suddenly

01:09:11,980 --> 01:09:19,159
with nothing but nothing else changes

01:09:15,619 --> 01:09:20,630
exactly other people using AI yeah okay

01:09:19,159 --> 01:09:22,159
we're gonna wrap up because that's all

01:09:20,630 --> 01:09:24,619
we have time for I'm gonna just

01:09:22,159 --> 01:09:25,880
single-handedly here thank you very much

01:09:24,619 --> 01:09:27,469
but I want to show you there's a whole

01:09:25,880 --> 01:09:29,060
bunch of other be a thank you to all the

01:09:27,469 --> 01:09:34,869
remote people and their questions

01:09:29,060 --> 01:09:34,869

YouTube URL: https://www.youtube.com/watch?v=ZUyXmOGBuCM


