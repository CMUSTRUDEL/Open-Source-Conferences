Title: Speaker Series: Revealing Data - Creepy or Curious with Yvonne Rogers
Publication date: 2019-08-26
Playlist: Emerging Technologies Speaker Series
Description: 
	Revealing Data: Creepy or Curious? with Yvonne Rogers
Captions: 
	00:00:02,029 --> 00:01:58,730
[Music]

00:02:04,330 --> 00:03:07,439
[Music]

00:03:12,580 --> 00:03:17,010
Oh Chuck

00:03:18,670 --> 00:05:17,829
[Music]

00:05:20,950 --> 00:06:24,029
[Music]

00:06:29,200 --> 00:06:33,660
Oh jump

00:06:35,300 --> 00:08:32,040
[Music]

00:08:37,590 --> 00:09:23,549
[Music]

00:09:44,790 --> 00:09:49,389
everyone welcome to the Mozilla London

00:09:47,410 --> 00:09:51,220
office my name is John Lloyd I'm the

00:09:49,389 --> 00:09:54,250
head of European campaigns here at the

00:09:51,220 --> 00:09:56,110
Mozilla Foundation the Foundation's

00:09:54,250 --> 00:09:57,639
theme this year is your data and you and

00:09:56,110 --> 00:09:59,350
we've been exploring all the ways that

00:09:57,639 --> 00:10:01,870
data is collected about us

00:09:59,350 --> 00:10:04,389
where it's stored and how it's used and

00:10:01,870 --> 00:10:06,279
shared since Cambridge general Medicare

00:10:04,389 --> 00:10:08,350
there's been a marked increase and

00:10:06,279 --> 00:10:09,730
concern from the public about the way

00:10:08,350 --> 00:10:11,829
that their data is being collected and

00:10:09,730 --> 00:10:14,050
how it shapes the world so we've been

00:10:11,829 --> 00:10:16,300
seeking out conversations to our theme

00:10:14,050 --> 00:10:18,610
of your data anew and looking for

00:10:16,300 --> 00:10:20,680
different and unique points of view so

00:10:18,610 --> 00:10:23,130
that's why I'm absolutely delighted that

00:10:20,680 --> 00:10:25,600
we were able to invite Yvonne Rodgers

00:10:23,130 --> 00:10:27,639
she works as professor and director of

00:10:25,600 --> 00:10:28,930
University College London and Direction

00:10:27,639 --> 00:10:30,910
center

00:10:28,930 --> 00:10:33,220
Yvonne's work resonated with us in

00:10:30,910 --> 00:10:34,899
particular because her approach is

00:10:33,220 --> 00:10:38,290
centered around the experiences of

00:10:34,899 --> 00:10:39,940
regular people so today she'll be

00:10:38,290 --> 00:10:42,430
speaking about a more open approach to

00:10:39,940 --> 00:10:44,440
data collection and use an approach that

00:10:42,430 --> 00:10:47,410
enables everyday people to be more aware

00:10:44,440 --> 00:10:50,110
of understand shift and act upon data

00:10:47,410 --> 00:10:51,880
collected about them after your Vaughn's

00:10:50,110 --> 00:10:55,660
finish speaking we're gonna have a brief

00:10:51,880 --> 00:10:58,839
Q&A so get your questions in on Twitter

00:10:55,660 --> 00:11:01,990
that's hashtag Mozilla speakers and on

00:10:58,839 --> 00:11:04,630
the slack Channel that's hashtag speaker

00:11:01,990 --> 00:11:07,149
- series and we'll also take questions

00:11:04,630 --> 00:11:10,720
here in London and the other offices so

00:11:07,149 --> 00:11:13,110
um please welcome me in joining Yvonne

00:11:10,720 --> 00:11:15,760
[Applause]

00:11:13,110 --> 00:11:18,010
thank you very much John it's a great

00:11:15,760 --> 00:11:19,690
pleasure to be in Mozilla London early

00:11:18,010 --> 00:11:21,760
in the year I was in Mozilla Mountain

00:11:19,690 --> 00:11:24,400
View so I'm just ticking them off I'll

00:11:21,760 --> 00:11:26,020
be off to Berlin's and the other ones

00:11:24,400 --> 00:11:29,290
which I know there is far too many to

00:11:26,020 --> 00:11:31,570
mention anyway so my talk today is going

00:11:29,290 --> 00:11:34,300
to be as you can see from the title in

00:11:31,570 --> 00:11:36,760
two halves the first half is going to be

00:11:34,300 --> 00:11:38,589
talking about creepy data and what that

00:11:36,760 --> 00:11:40,960
is and why everyone's suddenly got

00:11:38,589 --> 00:11:42,160
interested in it and then the second

00:11:40,960 --> 00:11:44,320
part is gonna be talking about my own

00:11:42,160 --> 00:11:46,839
research which is let's not just let

00:11:44,320 --> 00:11:48,670
this take over the debate but let's

00:11:46,839 --> 00:11:51,220
think about how we can make data open

00:11:48,670 --> 00:11:53,140
and accessible and to make people

00:11:51,220 --> 00:11:57,010
curious about that data and want to know

00:11:53,140 --> 00:11:59,050
more and ultimately empower them um

00:11:57,010 --> 00:12:02,080
every week there are stories about

00:11:59,050 --> 00:12:04,480
creepy dates or creepy technology in the

00:12:02,080 --> 00:12:06,010
news and I'm sure you've seen many of

00:12:04,480 --> 00:12:07,390
these these the ones I just took off

00:12:06,010 --> 00:12:09,880
from the last two or three weeks

00:12:07,390 --> 00:12:11,890
the first one is Spotify and tinder need

00:12:09,880 --> 00:12:14,470
to stop being creepy with customer data

00:12:11,890 --> 00:12:16,570
and this is not just them but also

00:12:14,470 --> 00:12:19,089
Netflix and it's saying that they've

00:12:16,570 --> 00:12:21,670
been revealing personal details of

00:12:19,089 --> 00:12:23,920
customers in an attempt to create viral

00:12:21,670 --> 00:12:26,950
marketing campaigns and the one in

00:12:23,920 --> 00:12:29,710
particular they refer to as last year

00:12:26,950 --> 00:12:31,420
Netflix publicly announced that 53

00:12:29,710 --> 00:12:34,390
people had watched the film and

00:12:31,420 --> 00:12:37,000
Christmas prints for 18 days in a row

00:12:34,390 --> 00:12:38,800
now that is that just making fun of

00:12:37,000 --> 00:12:41,620
someone why are they doing that as they

00:12:38,800 --> 00:12:44,170
got such a boring life but anyway this

00:12:41,620 --> 00:12:46,270
this got to the attention of the of the

00:12:44,170 --> 00:12:47,110
news and they think that's creepy then

00:12:46,270 --> 00:12:49,560
last week

00:12:47,110 --> 00:12:52,810
Facebook's announcement of its new

00:12:49,560 --> 00:12:55,330
portal video conferencing system that's

00:12:52,810 --> 00:12:57,430
going to come out soon that got a lot of

00:12:55,330 --> 00:12:59,350
attention in the media saying Facebook's

00:12:57,430 --> 00:13:01,720
creepy new speakers are freaking people

00:12:59,350 --> 00:13:05,080
out and I'll talk a bit more about this

00:13:01,720 --> 00:13:06,970
but the the the opinion of of these

00:13:05,080 --> 00:13:09,400
reporters is that because these have got

00:13:06,970 --> 00:13:11,709
cameras in them as well as microphones

00:13:09,400 --> 00:13:13,510
they can see clear accord what people

00:13:11,709 --> 00:13:16,180
are doing in their homes and then use

00:13:13,510 --> 00:13:18,670
that and then this last one was from a

00:13:16,180 --> 00:13:20,770
week ago I'm sure some of you seen this

00:13:18,670 --> 00:13:23,400
which was about Netflix is deceiving

00:13:20,770 --> 00:13:26,010
black users with creepy posters and I

00:13:23,400 --> 00:13:27,810
you know Netflix likes to customize what

00:13:26,010 --> 00:13:31,830
you might want to view next based on

00:13:27,810 --> 00:13:36,870
what you viewed before and for the some

00:13:31,830 --> 00:13:38,430
of their black viewers they was noticing

00:13:36,870 --> 00:13:41,130
that they like to watch films of black

00:13:38,430 --> 00:13:44,460
actors in and so they were suggesting

00:13:41,130 --> 00:13:47,190
these films or movies except these

00:13:44,460 --> 00:13:51,990
weren't main actors they were you know

00:13:47,190 --> 00:13:53,550
only bit part actors and this was felt

00:13:51,990 --> 00:13:56,240
to be creepy they were deceiving that

00:13:53,550 --> 00:13:59,790
the viewers so there's lots of these

00:13:56,240 --> 00:14:01,560
appearing each week and the media isn't

00:13:59,790 --> 00:14:04,170
really interested in this notion of

00:14:01,560 --> 00:14:07,470
creepiness but I'll in what way are they

00:14:04,170 --> 00:14:09,000
creepy and I guess the the bottom line

00:14:07,470 --> 00:14:11,100
is that it makes people feel

00:14:09,000 --> 00:14:13,380
uncomfortable and then that they are

00:14:11,100 --> 00:14:14,880
unaware that their digital habits what

00:14:13,380 --> 00:14:16,740
they're watching what they're touching

00:14:14,880 --> 00:14:19,620
and clicking and so on are being

00:14:16,740 --> 00:14:21,630
collected and being created being

00:14:19,620 --> 00:14:24,090
compared or even being sold to third

00:14:21,630 --> 00:14:27,270
parties and as a result of this they can

00:14:24,090 --> 00:14:30,360
personalize new content or ads to you

00:14:27,270 --> 00:14:32,610
they can target propaganda or fake news

00:14:30,360 --> 00:14:35,280
and in the Netflix case they can just

00:14:32,610 --> 00:14:37,860
simply make fun of you but what happens

00:14:35,280 --> 00:14:40,050
if users were more if they knew more

00:14:37,860 --> 00:14:42,570
about the process would they mind as

00:14:40,050 --> 00:14:46,290
much so how many of you here have a

00:14:42,570 --> 00:14:49,440
loyalty card to white rose or Tesco's so

00:14:46,290 --> 00:14:52,980
that you get and you're comfortable with

00:14:49,440 --> 00:14:55,800
that yes because you get free coffee you

00:14:52,980 --> 00:14:58,110
get discounts you get pouches a lot of

00:14:55,800 --> 00:14:59,880
people know what the loyalty card is

00:14:58,110 --> 00:15:04,770
about it helps them to do their

00:14:59,880 --> 00:15:06,720
marketing and analysis so what if you

00:15:04,770 --> 00:15:08,340
just knew about the processes that the

00:15:06,720 --> 00:15:11,160
Netflix and these other companies are

00:15:08,340 --> 00:15:13,170
using would they find it less disturbing

00:15:11,160 --> 00:15:15,950
would they be more accepting I think

00:15:13,170 --> 00:15:18,360
this is the question we need to ask

00:15:15,950 --> 00:15:19,860
another thing is do you know this is

00:15:18,360 --> 00:15:22,440
very much coming from a media

00:15:19,860 --> 00:15:24,540
perspective but do the general public

00:15:22,440 --> 00:15:26,950
themselves mind their personal data

00:15:24,540 --> 00:15:29,410
being collected so I've just had a hand

00:15:26,950 --> 00:15:31,180
you know getting you to show your hands

00:15:29,410 --> 00:15:34,210
and then shake your heads there was a

00:15:31,180 --> 00:15:36,820
study done last year by YouGov in the UK

00:15:34,210 --> 00:15:39,040
and they've found seven in ten consumers

00:15:36,820 --> 00:15:40,780
don't mind sharing their data if they

00:15:39,040 --> 00:15:42,550
get something back and that something

00:15:40,780 --> 00:15:45,610
could be saving money it could be a

00:15:42,550 --> 00:15:47,830
bargain or improve customer service and

00:15:45,610 --> 00:15:49,660
that's quite a high figure so long as

00:15:47,830 --> 00:15:52,540
you know you get something back if you

00:15:49,660 --> 00:15:54,100
give something but on the other hand we

00:15:52,540 --> 00:15:56,100
know that as people learn more about

00:15:54,100 --> 00:15:58,900
what these companies are getting up to

00:15:56,100 --> 00:16:01,330
and if they feel it's an invasion of

00:15:58,900 --> 00:16:03,430
their digital privacy and some will go

00:16:01,330 --> 00:16:06,970
and you know will take action to stop it

00:16:03,430 --> 00:16:08,860
and we've seen an increase in people

00:16:06,970 --> 00:16:11,410
particularly young people quitting

00:16:08,860 --> 00:16:13,990
social media for example Facebook how

00:16:11,410 --> 00:16:16,480
many of you have quit Facebook quite a

00:16:13,990 --> 00:16:18,550
few of you here put your hands up how

00:16:16,480 --> 00:16:21,730
many of you cover up your laptop cameras

00:16:18,550 --> 00:16:24,190
with plasters nearly over half of you

00:16:21,730 --> 00:16:25,930
how many of you have removed your store

00:16:24,190 --> 00:16:28,300
bank details from online sites like

00:16:25,930 --> 00:16:30,220
Amazon as a result of all the breaches a

00:16:28,300 --> 00:16:33,580
number of you so I think you're probably

00:16:30,220 --> 00:16:35,800
you know in the audience well aware of

00:16:33,580 --> 00:16:37,900
the measures you can take to prevent

00:16:35,800 --> 00:16:40,270
these things from happening so you can

00:16:37,900 --> 00:16:42,490
see you know that some people don't

00:16:40,270 --> 00:16:45,310
minds and others will take precautionary

00:16:42,490 --> 00:16:47,500
measures and I think it's not just a

00:16:45,310 --> 00:16:49,390
clear-cut matter as to whether it's one

00:16:47,500 --> 00:16:52,000
thing black or white but there are many

00:16:49,390 --> 00:16:54,610
shades to viewing personal data

00:16:52,000 --> 00:16:56,170
collection and its use and I think

00:16:54,610 --> 00:16:58,600
sometimes it can be beneficial to

00:16:56,170 --> 00:17:01,240
society to learn about the health of a

00:16:58,600 --> 00:17:03,040
nation and economic trends by collecting

00:17:01,240 --> 00:17:04,540
data you know and this is what's been

00:17:03,040 --> 00:17:06,970
done with the sensors for hundreds of

00:17:04,540 --> 00:17:09,010
years I think sometimes targeting

00:17:06,970 --> 00:17:10,780
advertising is what people want they

00:17:09,010 --> 00:17:12,430
want to have that advert that shows on a

00:17:10,780 --> 00:17:14,680
pair of slippers that they've always

00:17:12,430 --> 00:17:18,760
wanted at Christmas or you know

00:17:14,680 --> 00:17:21,760
particular kinds of goods and it can

00:17:18,760 --> 00:17:23,410
work well but sometimes as I've just

00:17:21,760 --> 00:17:25,449
said people don't mind giving up their

00:17:23,410 --> 00:17:27,220
data if they get something back but

00:17:25,449 --> 00:17:29,380
sometimes it's downright evil or

00:17:27,220 --> 00:17:32,470
insidious to collect it and it just

00:17:29,380 --> 00:17:34,810
depends on the user group in the context

00:17:32,470 --> 00:17:36,610
but what I also want to mention is that

00:17:34,810 --> 00:17:39,070
I think sometimes it can raise new

00:17:36,610 --> 00:17:40,130
ethical issues for us as society and

00:17:39,070 --> 00:17:41,480
researchers

00:17:40,130 --> 00:17:44,090
start thinking about which we didn't

00:17:41,480 --> 00:17:47,030
have to think about before and this is

00:17:44,090 --> 00:17:50,360
one I think which appeared last year

00:17:47,030 --> 00:17:52,430
which is the headline here is new

00:17:50,360 --> 00:17:54,200
technology is forcing us to confront the

00:17:52,430 --> 00:17:56,180
ethics of bringing people back from the

00:17:54,200 --> 00:18:00,830
dead I don't know how many of you saw

00:17:56,180 --> 00:18:03,110
this but Eugenia Kuja one of her close

00:18:00,830 --> 00:18:04,910
friends died in a car accident and he

00:18:03,110 --> 00:18:08,090
was only in his 20s and she was you know

00:18:04,910 --> 00:18:11,540
devastated and it really cut her up and

00:18:08,090 --> 00:18:13,340
she was an AI expert and she didn't want

00:18:11,540 --> 00:18:15,920
to lose the memory of her friend Roman

00:18:13,340 --> 00:18:18,770
and so what she did was she go at all

00:18:15,920 --> 00:18:20,720
the texts that Roman had written and

00:18:18,770 --> 00:18:23,780
combined those together to make a

00:18:20,720 --> 00:18:25,580
chatbot and this allowed her to then

00:18:23,780 --> 00:18:28,460
communicate with her dead friend as if

00:18:25,580 --> 00:18:30,410
he was alive so she could type in I miss

00:18:28,460 --> 00:18:32,750
you and the dead person wouldn't say I

00:18:30,410 --> 00:18:35,900
miss you too and so have a conversation

00:18:32,750 --> 00:18:38,420
as if with that person so here is an

00:18:35,900 --> 00:18:40,340
example if you can see it with Roman and

00:18:38,420 --> 00:18:41,300
he says what do you want to know what

00:18:40,340 --> 00:18:43,520
are you working on

00:18:41,300 --> 00:18:46,310
that's the the real person who's alive

00:18:43,520 --> 00:18:48,170
and Roman saying working on disrupting

00:18:46,310 --> 00:18:50,960
death and stampy at the same time I'll

00:18:48,170 --> 00:18:53,450
go get an internship at a funeral house

00:18:50,960 --> 00:18:56,240
for my research and so on so this chat

00:18:53,450 --> 00:18:58,510
bot is expressing Ronan's personality

00:18:56,240 --> 00:19:03,050
his poetic perspective and also his

00:18:58,510 --> 00:19:04,880
self-deprecating sense of humor is this

00:19:03,050 --> 00:19:06,970
creepy or is it something that just

00:19:04,880 --> 00:19:09,320
seems okay and I think it's very

00:19:06,970 --> 00:19:10,940
comforting for the grieving friend to

00:19:09,320 --> 00:19:13,880
have the chat

00:19:10,940 --> 00:19:16,250
it makes her you know feel closer so

00:19:13,880 --> 00:19:17,890
Roman but we have to remember that Roman

00:19:16,250 --> 00:19:21,170
didn't give permission for his data

00:19:17,890 --> 00:19:22,430
his text to be used in this way and when

00:19:21,170 --> 00:19:25,310
I was talking to one of my colleagues

00:19:22,430 --> 00:19:27,620
about this she was immediately repost

00:19:25,310 --> 00:19:30,080
and she said it's this bit disrespectful

00:19:27,620 --> 00:19:31,730
of the dead and some communities and

00:19:30,080 --> 00:19:33,950
they're there you know what they think

00:19:31,730 --> 00:19:35,330
of the dead but then I said well how

00:19:33,950 --> 00:19:36,950
different is it from looking at all the

00:19:35,330 --> 00:19:39,350
photos and the videos that you might

00:19:36,950 --> 00:19:40,850
have collected from that person you're

00:19:39,350 --> 00:19:44,660
you know you're wanting to remember that

00:19:40,850 --> 00:19:46,220
person and connect with them and you

00:19:44,660 --> 00:19:48,320
know what if Roman had actually agreed

00:19:46,220 --> 00:19:49,910
to having his tech smashed up in this

00:19:48,320 --> 00:19:52,490
way in a pre death digital

00:19:49,910 --> 00:19:54,380
agreement so I think here is a here's an

00:19:52,490 --> 00:19:56,090
example where we didn't have this before

00:19:54,380 --> 00:19:58,940
I think it's different from looking at

00:19:56,090 --> 00:20:00,950
photos or videos because it's you don't

00:19:58,940 --> 00:20:02,900
know you know what's there you you're

00:20:00,950 --> 00:20:06,290
not asking questions I'm pretending it's

00:20:02,900 --> 00:20:10,280
someone but is it is it creepy and if so

00:20:06,290 --> 00:20:12,470
for what for what reasons I want to move

00:20:10,280 --> 00:20:15,620
on now to think about another field new

00:20:12,470 --> 00:20:17,480
feel that's emerging and how that raises

00:20:15,620 --> 00:20:20,690
new issues for us to think about which

00:20:17,480 --> 00:20:22,640
is the field of emotional AI and

00:20:20,690 --> 00:20:26,030
emotional AI is using machine learning

00:20:22,640 --> 00:20:28,430
techniques to recognize people's face

00:20:26,030 --> 00:20:30,800
and detect their emotions so it'll take

00:20:28,430 --> 00:20:33,620
shots of your face it'll use some

00:20:30,800 --> 00:20:35,360
sophisticated 2d 3d modeling and then it

00:20:33,620 --> 00:20:37,520
will say whether or not you're angry or

00:20:35,360 --> 00:20:41,710
sad and it can also recognize who you

00:20:37,520 --> 00:20:46,880
are originally this technology was

00:20:41,710 --> 00:20:51,770
developed for store for to be used in in

00:20:46,880 --> 00:20:54,500
in stores to match up with known

00:20:51,770 --> 00:20:57,050
shoplifters and so the cameras would be

00:20:54,500 --> 00:20:58,730
placed around and they would take

00:20:57,050 --> 00:21:01,570
pictures of people in the store and

00:20:58,730 --> 00:21:04,550
converted into these biometric templates

00:21:01,570 --> 00:21:06,760
but once these are in place these

00:21:04,550 --> 00:21:09,320
cameras they can also be used to measure

00:21:06,760 --> 00:21:10,790
in-store dwell time so by that I mean

00:21:09,320 --> 00:21:13,460
the amount of time that you're looking

00:21:10,790 --> 00:21:17,240
at that can of dr. pepper or looking at

00:21:13,460 --> 00:21:18,920
you and also the responses so if I'm

00:21:17,240 --> 00:21:20,600
looking at that and I might be

00:21:18,920 --> 00:21:22,940
salivating that might be really thirsty

00:21:20,600 --> 00:21:25,490
or looking at that food they can then

00:21:22,940 --> 00:21:27,650
measure my interest in that whereas over

00:21:25,490 --> 00:21:30,920
here I might not be looking at that at

00:21:27,650 --> 00:21:33,800
all and so this can be used for

00:21:30,920 --> 00:21:35,810
marketing purposes but also machine

00:21:33,800 --> 00:21:38,150
learning can classify the lookers in

00:21:35,810 --> 00:21:41,390
milliseconds so it could classify with

00:21:38,150 --> 00:21:43,310
me by my gender by my age and my assumed

00:21:41,390 --> 00:21:46,280
emotional state am i nervous am i

00:21:43,310 --> 00:21:48,500
anxious am I really hungry and so on and

00:21:46,280 --> 00:21:51,710
so the question here I think for us to

00:21:48,500 --> 00:21:53,480
consider is retailers have argued that

00:21:51,710 --> 00:21:55,730
this kind of data analysis has been

00:21:53,480 --> 00:21:57,960
happening in online shopping they've

00:21:55,730 --> 00:22:00,480
been looking at how long people

00:21:57,960 --> 00:22:03,179
click whether they stay and so on for

00:22:00,480 --> 00:22:05,880
many years so they already have good

00:22:03,179 --> 00:22:08,399
models of online shoppers so is this any

00:22:05,880 --> 00:22:10,289
different for physical shopping where

00:22:08,399 --> 00:22:11,549
you go in you've got cameras there why

00:22:10,289 --> 00:22:13,980
is that different it's because it's

00:22:11,549 --> 00:22:16,380
getting them the same information and I

00:22:13,980 --> 00:22:18,809
think it is different and the reason is

00:22:16,380 --> 00:22:22,320
it's getting under your skin in ways in

00:22:18,809 --> 00:22:23,789
which online isn't so imagine yourself

00:22:22,320 --> 00:22:26,279
as a shopper would you be comfortable

00:22:23,789 --> 00:22:28,260
with this kind of face tracking getting

00:22:26,279 --> 00:22:29,880
the biometrics if you knew it was

00:22:28,260 --> 00:22:31,950
happening if you knew that these cameras

00:22:29,880 --> 00:22:35,220
around here we're just looking were

00:22:31,950 --> 00:22:38,159
looking at you and detecting different

00:22:35,220 --> 00:22:40,020
changes in in your facial expression and

00:22:38,159 --> 00:22:43,440
being able to decide whether you are

00:22:40,020 --> 00:22:45,090
interested or not it's not just in

00:22:43,440 --> 00:22:48,510
stores that this has been used but it's

00:22:45,090 --> 00:22:50,580
now being used by companies for job

00:22:48,510 --> 00:22:55,649
interview ease so imagine you're going

00:22:50,580 --> 00:22:56,909
for a job interview at Mozilla and that

00:22:55,649 --> 00:23:00,659
Mozilla have just heard about this

00:22:56,909 --> 00:23:03,710
amazing new technology which can detect

00:23:00,659 --> 00:23:07,320
you know can help get the right person

00:23:03,710 --> 00:23:10,020
so a startup company called human which

00:23:07,320 --> 00:23:12,630
is based in London claims to be able to

00:23:10,020 --> 00:23:14,820
detect subliminal facial expressions by

00:23:12,630 --> 00:23:17,700
that is meant those that you cannot

00:23:14,820 --> 00:23:19,169
detect yourself in others so looking at

00:23:17,700 --> 00:23:21,630
you I can see that you're very

00:23:19,169 --> 00:23:23,190
interested but if I had this spin this

00:23:21,630 --> 00:23:24,990
technology it might be able to tell me

00:23:23,190 --> 00:23:27,840
much more about what's going on what

00:23:24,990 --> 00:23:30,600
you're thinking about and it can

00:23:27,840 --> 00:23:32,580
calculate our scores for how honest you

00:23:30,600 --> 00:23:35,640
are for example how nervous you are how

00:23:32,580 --> 00:23:37,919
passionate you are and in particular to

00:23:35,640 --> 00:23:40,320
answering questions so you know if I'm

00:23:37,919 --> 00:23:42,659
asked a question like tell me about a

00:23:40,320 --> 00:23:45,000
difficult time you had at work and how

00:23:42,659 --> 00:23:49,440
you dealt with it it could see how I

00:23:45,000 --> 00:23:51,630
responded to that so in this case human

00:23:49,440 --> 00:23:54,659
is just providing a service it will do

00:23:51,630 --> 00:23:56,429
this profiling it'll capture data so it

00:23:54,659 --> 00:24:00,630
will have cameras in the interview room

00:23:56,429 --> 00:24:02,789
and you can do the analysis and then it

00:24:00,630 --> 00:24:05,610
will compile a report the hiring company

00:24:02,789 --> 00:24:08,100
like Mozilla will just pay for this

00:24:05,610 --> 00:24:09,770
service and they will use the results

00:24:08,100 --> 00:24:13,770
from this

00:24:09,770 --> 00:24:16,140
this type of analysis with other HR data

00:24:13,770 --> 00:24:17,910
so no one needs to be responsible for

00:24:16,140 --> 00:24:21,410
the ethics behind this whether it is

00:24:17,910 --> 00:24:24,299
ethical so the moral question here is

00:24:21,410 --> 00:24:26,460
recruiting companies already profile

00:24:24,299 --> 00:24:30,450
applicants from their tweets their blogs

00:24:26,460 --> 00:24:32,850
and posts so how is facial profiling any

00:24:30,450 --> 00:24:34,410
different they want to get the best

00:24:32,850 --> 00:24:37,309
person or they want to get the person

00:24:34,410 --> 00:24:39,750
that matches the job or her description

00:24:37,309 --> 00:24:42,030
well I will put this to you to think

00:24:39,750 --> 00:24:43,679
about which is if you are an interviewee

00:24:42,030 --> 00:24:45,860
would you be comfortable if you knew

00:24:43,679 --> 00:24:48,780
that your every smile your every twitch

00:24:45,860 --> 00:24:50,280
your every laugh was being analyzed as

00:24:48,780 --> 00:24:54,330
you sat there in the interview in a

00:24:50,280 --> 00:24:56,580
stressful situation maybe you wouldn't

00:24:54,330 --> 00:24:58,590
mind maybe you could learn how to make

00:24:56,580 --> 00:25:00,120
your face appear that you're very

00:24:58,590 --> 00:25:03,480
interested and passionate so you could

00:25:00,120 --> 00:25:05,700
be ahead of the game but it's still very

00:25:03,480 --> 00:25:09,360
much something that you know it gets

00:25:05,700 --> 00:25:10,530
under your skin and this raises you know

00:25:09,360 --> 00:25:13,770
more general question is whether

00:25:10,530 --> 00:25:16,770
emotional AI is an acceptable way of

00:25:13,770 --> 00:25:19,049
collecting data and using it and I think

00:25:16,770 --> 00:25:21,150
our understanding of people's intent and

00:25:19,049 --> 00:25:23,220
their emotions through their facial

00:25:21,150 --> 00:25:25,890
expressions is still very much in its

00:25:23,220 --> 00:25:27,179
infancy so we don't really know whether

00:25:25,890 --> 00:25:30,270
we're getting what we're getting is

00:25:27,179 --> 00:25:32,840
accurate but there's already a variety

00:25:30,270 --> 00:25:36,450
of applications that are being developed

00:25:32,840 --> 00:25:39,179
for the tailoring of ads as I said the

00:25:36,450 --> 00:25:41,549
screening of job interviewees monitoring

00:25:39,179 --> 00:25:44,309
of employees and they can do that from

00:25:41,549 --> 00:25:45,929
their laptop seeing how you know how to

00:25:44,309 --> 00:25:48,210
structure they are how hard they're

00:25:45,929 --> 00:25:50,070
working and so on and more recently

00:25:48,210 --> 00:25:52,500
there's been the move towards using this

00:25:50,070 --> 00:25:55,860
technique for identifying mental health

00:25:52,500 --> 00:25:59,190
symptoms if someone we're about to you

00:25:55,860 --> 00:26:01,590
know become depressed are they looking

00:25:59,190 --> 00:26:04,679
anxious and the moral question I think

00:26:01,590 --> 00:26:08,010
that this approach raises is would it be

00:26:04,679 --> 00:26:09,750
okay if people were aware of their face

00:26:08,010 --> 00:26:12,419
how their facial expressions were being

00:26:09,750 --> 00:26:17,760
analyzed and and what's being inferred

00:26:12,419 --> 00:26:19,200
from it and I think again it's not a

00:26:17,760 --> 00:26:21,539
question of it being black or white but

00:26:19,200 --> 00:26:25,649
it's something we need to consider

00:26:21,539 --> 00:26:27,929
different contexts so that's just giving

00:26:25,649 --> 00:26:32,489
you an overview I think of the creepy

00:26:27,929 --> 00:26:34,320
data but there's sort of the you know

00:26:32,489 --> 00:26:36,059
how it's being reported in the press but

00:26:34,320 --> 00:26:38,639
also these new technologies that are

00:26:36,059 --> 00:26:40,739
being developed and how we need to think

00:26:38,639 --> 00:26:43,919
about what the consequences and

00:26:40,739 --> 00:26:46,109
implications are and when I want to move

00:26:43,919 --> 00:26:48,210
on to think about what can be done to

00:26:46,109 --> 00:26:50,720
protect people from invasion of their

00:26:48,210 --> 00:26:53,100
personal privacy and clearly there are

00:26:50,720 --> 00:26:54,539
examples as we heard Cambridge analytic

00:26:53,100 --> 00:26:57,659
I don't need to go into any more details

00:26:54,539 --> 00:27:00,440
and I think there are four main areas

00:26:57,659 --> 00:27:02,989
that we should be thinking about firstly

00:27:00,440 --> 00:27:05,249
government's introducing stricter laws

00:27:02,989 --> 00:27:08,279
increasing awareness of what's going on

00:27:05,249 --> 00:27:09,690
and providing explicit privacy warnings

00:27:08,279 --> 00:27:12,179
like health warnings we have on

00:27:09,690 --> 00:27:14,249
cigarette packages and we see this in

00:27:12,179 --> 00:27:16,889
the UK in Europe with the GDP are this

00:27:14,249 --> 00:27:18,389
year that they are trying to you know

00:27:16,889 --> 00:27:20,460
make it more difficult

00:27:18,389 --> 00:27:23,580
or should I say make it stricter and

00:27:20,460 --> 00:27:25,669
think about the person and how to

00:27:23,580 --> 00:27:28,499
protect them from this type of invasion

00:27:25,669 --> 00:27:30,570
secondly we can think about developing a

00:27:28,499 --> 00:27:32,879
completely new platforms for the

00:27:30,570 --> 00:27:36,090
internet that can ring-fence

00:27:32,879 --> 00:27:39,479
personal data and Tim berners-lee who I

00:27:36,090 --> 00:27:41,879
understand was at Mozza Fez s sorry last

00:27:39,479 --> 00:27:45,690
week he's got a startup company called

00:27:41,879 --> 00:27:47,489
interrupts and the idea is that you as

00:27:45,690 --> 00:27:50,009
an individual can create manage and

00:27:47,489 --> 00:27:52,440
secure your own personal online data in

00:27:50,009 --> 00:27:55,649
your on your laptop and you can then

00:27:52,440 --> 00:28:00,239
decide who accesses it and you know who

00:27:55,649 --> 00:28:02,669
you can share it with another team in

00:28:00,239 --> 00:28:05,669
the UK is also coming you know in

00:28:02,669 --> 00:28:07,320
Cambridge and in London thinking about

00:28:05,669 --> 00:28:09,090
this in terms of data in a box so

00:28:07,320 --> 00:28:13,379
they're coming up with a new framework

00:28:09,090 --> 00:28:15,899
for this so this research is going on we

00:28:13,379 --> 00:28:18,239
can also expect companies particularly

00:28:15,899 --> 00:28:21,090
tech companies to do more to protect

00:28:18,239 --> 00:28:22,859
their users and be transparent about

00:28:21,090 --> 00:28:26,159
their practices and have better controls

00:28:22,859 --> 00:28:28,349
and checkups and then we can you know

00:28:26,159 --> 00:28:30,299
ask researchers or expect researchers to

00:28:28,349 --> 00:28:32,849
do much more into investigating what's

00:28:30,299 --> 00:28:34,910
acceptable and find out what people

00:28:32,849 --> 00:28:39,110
think about others using their dates

00:28:34,910 --> 00:28:45,350
so for the second part of my talk I just

00:28:39,110 --> 00:28:47,780
need a bit some water Thanks I'm going

00:28:45,350 --> 00:28:50,150
to just cover number three and four

00:28:47,780 --> 00:28:51,650
which is what can companies do what are

00:28:50,150 --> 00:28:55,790
they doing and then what are we as

00:28:51,650 --> 00:28:57,860
researchers doing so I don't know how

00:28:55,790 --> 00:28:59,330
many of you have looked at what Google

00:28:57,860 --> 00:29:00,830
and Facebook have been up to but they've

00:28:59,330 --> 00:29:04,700
been very active behind the scenes

00:29:00,830 --> 00:29:07,100
trying to introduce their privacy check

00:29:04,700 --> 00:29:09,200
ups and what they're doing - and making

00:29:07,100 --> 00:29:11,720
it much more accessible it's no longer

00:29:09,200 --> 00:29:13,460
like the legalese that used to be for

00:29:11,720 --> 00:29:15,110
terms and conditions they're actually

00:29:13,460 --> 00:29:17,090
writing it so that people can understand

00:29:15,110 --> 00:29:18,980
they're using images they're showing you

00:29:17,090 --> 00:29:21,320
what you can do to turn off and turn off

00:29:18,980 --> 00:29:23,060
and also the explanation why they're

00:29:21,320 --> 00:29:24,680
collecting data about you so this is one

00:29:23,060 --> 00:29:27,940
there are many on Google's privacy

00:29:24,680 --> 00:29:30,830
checkup if you want to have a look and

00:29:27,940 --> 00:29:34,100
you know and they they are trying to

00:29:30,830 --> 00:29:36,080
both give an explanation and also easy

00:29:34,100 --> 00:29:37,490
way for you to turn on and off if you're

00:29:36,080 --> 00:29:39,830
not comfortable in the way they're

00:29:37,490 --> 00:29:41,570
collecting data from you for example

00:29:39,830 --> 00:29:44,590
your chrome history on which sites

00:29:41,570 --> 00:29:47,750
you've clicked visited on the web I

00:29:44,590 --> 00:29:50,870
mentioned earlier about Facebook's new

00:29:47,750 --> 00:29:54,080
home video conferencing device and this

00:29:50,870 --> 00:29:56,210
was very much in the news this is a it's

00:29:54,080 --> 00:29:57,950
not out yet but this is an image of how

00:29:56,210 --> 00:30:00,110
it might appear in someone's kitchen and

00:29:57,950 --> 00:30:03,560
of course what you can see is there is a

00:30:00,110 --> 00:30:05,930
camera and a microphone and but this may

00:30:03,560 --> 00:30:08,180
I here so the camera can follow you as

00:30:05,930 --> 00:30:10,910
you move around the room and that can be

00:30:08,180 --> 00:30:12,590
quite a you know a good technology if

00:30:10,910 --> 00:30:16,010
you want to show what's happening in

00:30:12,590 --> 00:30:17,990
your kitchen and so this ability to

00:30:16,010 --> 00:30:19,670
automatically zoom in and out I think is

00:30:17,990 --> 00:30:22,490
you know for us as interaction designers

00:30:19,670 --> 00:30:25,010
is really exciting but as we heard that

00:30:22,490 --> 00:30:27,830
there's the potential or the problem

00:30:25,010 --> 00:30:31,340
that this device could just start

00:30:27,830 --> 00:30:33,530
recording video of you or audio without

00:30:31,340 --> 00:30:35,930
you realizing this and then use it in

00:30:33,530 --> 00:30:38,960
some sort of way now Facebook isn't that

00:30:35,930 --> 00:30:42,530
stupid and they have been thinking a lot

00:30:38,960 --> 00:30:44,690
about how to address this issue and have

00:30:42,530 --> 00:30:47,149
their own practice called private by

00:30:44,690 --> 00:30:49,789
design and again similar to Google

00:30:47,149 --> 00:30:53,210
they have made it easy for you to

00:30:49,789 --> 00:30:55,460
disable the cameras and the audio and

00:30:53,210 --> 00:30:57,169
also to explain how it works so here you

00:30:55,460 --> 00:31:00,019
can completely disable the camera and

00:30:57,169 --> 00:31:02,330
microphone with a single tap will block

00:31:00,019 --> 00:31:04,070
the camera lens Facebook doesn't listen

00:31:02,330 --> 00:31:06,289
to view or keep the contents of your

00:31:04,070 --> 00:31:08,389
portal video cause your portal

00:31:06,289 --> 00:31:10,639
conversation stay behind between you and

00:31:08,389 --> 00:31:13,099
the people you're calling and more and

00:31:10,639 --> 00:31:15,769
more and so it's really trying to be

00:31:13,099 --> 00:31:18,729
clear about its policy and also the

00:31:15,769 --> 00:31:22,519
ability for you as the user to feel

00:31:18,729 --> 00:31:24,889
secure but also comfortable with you

00:31:22,519 --> 00:31:26,450
know how much you want to reveal when

00:31:24,889 --> 00:31:29,210
you want to turn your camera on and off

00:31:26,450 --> 00:31:32,299
and making it easy and so in a way it's

00:31:29,210 --> 00:31:34,909
it's providing the user with the ability

00:31:32,299 --> 00:31:37,129
to enjoy that functionality that new

00:31:34,909 --> 00:31:39,229
functionality that this new system can

00:31:37,129 --> 00:31:41,450
provide by the way these portals can

00:31:39,229 --> 00:31:44,149
connect to four others so you could have

00:31:41,450 --> 00:31:45,979
five of them in different locations and

00:31:44,149 --> 00:31:49,849
you can imagine how that might be used

00:31:45,979 --> 00:31:52,429
by families and friends so it'll be

00:31:49,849 --> 00:31:55,460
interesting to see whether or not people

00:31:52,429 --> 00:31:57,679
read these new descriptions and they act

00:31:55,460 --> 00:31:58,549
upon it or whether the press do their

00:31:57,679 --> 00:32:01,519
scaremongering

00:31:58,549 --> 00:32:05,450
and say look this is you know outrageous

00:32:01,519 --> 00:32:09,229
that you can they may be recording so

00:32:05,450 --> 00:32:11,779
it's not for me to set to condone or to

00:32:09,229 --> 00:32:13,429
celebrate what these tech companies are

00:32:11,779 --> 00:32:15,349
doing but I just think we should be made

00:32:13,429 --> 00:32:18,320
aware that they are being quite serious

00:32:15,349 --> 00:32:23,779
about coming up with ways to counteract

00:32:18,320 --> 00:32:27,379
and to you know consider the invasion of

00:32:23,779 --> 00:32:29,269
personal data what I want to do for the

00:32:27,379 --> 00:32:30,529
last part of my talk is to talk about

00:32:29,269 --> 00:32:33,619
some of the research that we've been

00:32:30,529 --> 00:32:35,809
doing which is is the second part which

00:32:33,619 --> 00:32:38,119
is to think about if you open up data

00:32:35,809 --> 00:32:40,940
what could you do with it in particular

00:32:38,119 --> 00:32:42,889
can you empower communities and the

00:32:40,940 --> 00:32:46,219
general public to be interested to be

00:32:42,889 --> 00:32:49,159
curious about it and if so how do we do

00:32:46,219 --> 00:32:51,589
this and so I'm going to talk about

00:32:49,159 --> 00:32:55,389
three studies that I've done over the

00:32:51,589 --> 00:32:57,380
years which is to think about you know

00:32:55,389 --> 00:32:59,360
understanding our people comfortable

00:32:57,380 --> 00:33:02,600
about revealing data about themselves

00:32:59,360 --> 00:33:05,059
but also being able to interact with the

00:33:02,600 --> 00:33:08,210
first one is a study we did a few years

00:33:05,059 --> 00:33:09,919
ago which was to confront people about

00:33:08,210 --> 00:33:12,200
what they really think by setting up a

00:33:09,919 --> 00:33:14,720
hoax situation and this is called the

00:33:12,200 --> 00:33:17,049
quantified toilet study and I'll come

00:33:14,720 --> 00:33:19,159
into that in a minute the second one is

00:33:17,049 --> 00:33:22,039
about showing people different

00:33:19,159 --> 00:33:24,260
perspectives about the impact of a new

00:33:22,039 --> 00:33:26,299
technology that's that can deal with

00:33:24,260 --> 00:33:28,519
sensitive issues before it's been

00:33:26,299 --> 00:33:31,159
developed and to try and get people's

00:33:28,519 --> 00:33:33,230
different opinions using what we call

00:33:31,159 --> 00:33:37,460
the contra vision method and then the

00:33:33,230 --> 00:33:39,320
third one is if we open up data which is

00:33:37,460 --> 00:33:41,690
normally considered to be slightly

00:33:39,320 --> 00:33:43,789
creepy in this case tracking people

00:33:41,690 --> 00:33:45,830
where they go and let the public see

00:33:43,789 --> 00:33:47,690
where they go and what they think of

00:33:45,830 --> 00:33:49,340
that will it become more acceptable will

00:33:47,690 --> 00:33:51,350
it will they be interested in it and

00:33:49,340 --> 00:33:53,210
could they contribute to you know

00:33:51,350 --> 00:33:54,740
helping you know use that data and

00:33:53,210 --> 00:33:58,690
interesting ways and that's going to be

00:33:54,740 --> 00:34:02,480
the Madeira tourism project so the first

00:33:58,690 --> 00:34:04,700
study we did involve doing mild

00:34:02,480 --> 00:34:06,559
deception though any of you have filled

00:34:04,700 --> 00:34:10,339
in your ethics forms will know that it's

00:34:06,559 --> 00:34:14,079
possible to do mild deception whereby

00:34:10,339 --> 00:34:17,210
you set up a situation that isn't true

00:34:14,079 --> 00:34:19,490
to question people as to imagining what

00:34:17,210 --> 00:34:23,899
it would be like and you're disrupting

00:34:19,490 --> 00:34:27,020
an accepted state of affairs by setting

00:34:23,899 --> 00:34:30,260
up this hoax and I'll describe in a

00:34:27,020 --> 00:34:31,639
minute how my researchers did that but

00:34:30,260 --> 00:34:33,379
the purpose of this is to observe

00:34:31,639 --> 00:34:35,960
people's reactions when they come across

00:34:33,379 --> 00:34:39,020
it do they become upset surprised

00:34:35,960 --> 00:34:41,569
outraged or do they not mind so they

00:34:39,020 --> 00:34:42,859
question the reality of the situation so

00:34:41,569 --> 00:34:45,050
they tell others about it

00:34:42,859 --> 00:34:46,490
and what else do they do and the reason

00:34:45,050 --> 00:34:48,379
for doing this rather than just you know

00:34:46,490 --> 00:34:51,020
asking them in an interview or a

00:34:48,379 --> 00:34:52,520
questionnaire is it sits there in the

00:34:51,020 --> 00:34:55,639
moment so you get their immediate

00:34:52,520 --> 00:34:58,339
reaction and response rather than one

00:34:55,639 --> 00:35:03,079
that's measured and and they've thought

00:34:58,339 --> 00:35:05,359
about so what did my researchers do well

00:35:03,079 --> 00:35:06,660
a few years ago they were at a workshop

00:35:05,359 --> 00:35:08,489
on internet

00:35:06,660 --> 00:35:11,190
and they really interested in how

00:35:08,489 --> 00:35:13,470
community would react to having their

00:35:11,190 --> 00:35:16,049
personal data analyzed and used in a

00:35:13,470 --> 00:35:19,460
public place and they wants to spark a

00:35:16,049 --> 00:35:23,190
public debate on future of surveillance

00:35:19,460 --> 00:35:26,339
technologies and so what they did was

00:35:23,190 --> 00:35:28,339
they set up this fake service which was

00:35:26,339 --> 00:35:30,960
they were a fictitious company called

00:35:28,339 --> 00:35:33,660
quantified toilets and they had

00:35:30,960 --> 00:35:35,700
installed a urine analysis technology in

00:35:33,660 --> 00:35:38,039
the public toilets in this conference

00:35:35,700 --> 00:35:42,029
center to improve the public health of

00:35:38,039 --> 00:35:43,680
that community and they want to know how

00:35:42,029 --> 00:35:46,640
would people react knowing that their

00:35:43,680 --> 00:35:49,829
urine was being analyzed and made public

00:35:46,640 --> 00:35:52,170
so what did they do as I said this was a

00:35:49,829 --> 00:35:53,849
mild deception they placed these signs

00:35:52,170 --> 00:35:57,059
in every toilets at the convention

00:35:53,849 --> 00:36:00,180
center in Toronto and these signs say

00:35:57,059 --> 00:36:01,650
this facility is proud about participate

00:36:00,180 --> 00:36:03,720
in the healthy building initiative

00:36:01,650 --> 00:36:06,660
behavior at these toilets is being

00:36:03,720 --> 00:36:10,259
recorded for analysis and it's to help

00:36:06,660 --> 00:36:13,499
monitor public health so these were put

00:36:10,259 --> 00:36:16,109
on the mirrors and in the toilets both

00:36:13,499 --> 00:36:19,289
all of the toilets on on all of the

00:36:16,109 --> 00:36:22,650
floors and then they created a fake

00:36:19,289 --> 00:36:25,109
website which showed the results of this

00:36:22,650 --> 00:36:28,710
analysis and this website still lair you

00:36:25,109 --> 00:36:31,349
want to have a look and it shows the the

00:36:28,710 --> 00:36:34,229
time the the ID of the toilet the sex of

00:36:31,349 --> 00:36:36,450
the person who was in that toilet what

00:36:34,229 --> 00:36:38,279
they deposited the odor the

00:36:36,450 --> 00:36:40,559
blood-alcohol level whether there were

00:36:38,279 --> 00:36:42,119
drugs detected whether they were

00:36:40,559 --> 00:36:46,829
pregnant or whether there are any

00:36:42,119 --> 00:36:49,259
infections to add authenticity they

00:36:46,829 --> 00:36:51,299
provide this table on the website and it

00:36:49,259 --> 00:36:52,920
showed you know what were the benefits

00:36:51,299 --> 00:36:55,680
are having this quantified toilet

00:36:52,920 --> 00:36:58,400
approach versus traditional approach and

00:36:55,680 --> 00:37:01,680
you can see that it's unobtrusive it

00:36:58,400 --> 00:37:07,440
provides constant analysis and community

00:37:01,680 --> 00:37:10,859
health statistics so what happened well

00:37:07,440 --> 00:37:14,670
within an hour it went viral there were

00:37:10,859 --> 00:37:17,309
many many tweets retweets and blogs the

00:37:14,670 --> 00:37:19,319
people working in the Convention Center

00:37:17,309 --> 00:37:19,810
were asked to come along and check what

00:37:19,319 --> 00:37:22,240
was going

00:37:19,810 --> 00:37:24,030
and take down the stickers was much

00:37:22,240 --> 00:37:26,140
discussion about all of the people there

00:37:24,030 --> 00:37:28,450
but there was also much discussion

00:37:26,140 --> 00:37:31,480
online so it created it did exactly what

00:37:28,450 --> 00:37:34,210
was hoped have a debate about beta

00:37:31,480 --> 00:37:36,400
tracking some people felt duped they

00:37:34,210 --> 00:37:39,100
really thought it was true the press

00:37:36,400 --> 00:37:42,340
gained enormous coverage so for example

00:37:39,100 --> 00:37:44,650
of Atlantic rotor had a big article

00:37:42,340 --> 00:37:46,960
about what a toilet hopes can tell us

00:37:44,650 --> 00:37:49,300
about the future of surveillance so it

00:37:46,960 --> 00:37:52,240
got people really in you know talking

00:37:49,300 --> 00:37:55,210
but not only that we got a very wide set

00:37:52,240 --> 00:37:58,690
of responses and this is just analysis

00:37:55,210 --> 00:38:00,730
of some of these from passersby on those

00:37:58,690 --> 00:38:03,160
who are online there was as you might

00:38:00,730 --> 00:38:05,680
expect this approval health advice it

00:38:03,160 --> 00:38:08,710
does not get any creepier then there was

00:38:05,680 --> 00:38:11,970
concern imagine your employer could find

00:38:08,710 --> 00:38:14,410
out how hard you'd party last night

00:38:11,970 --> 00:38:16,480
then there was resignation

00:38:14,410 --> 00:38:20,170
I'm sure the government's been doing

00:38:16,480 --> 00:38:22,990
this for years then there was voyeurism

00:38:20,170 --> 00:38:25,840
I just spent the last 10 minutes

00:38:22,990 --> 00:38:30,610
watching the PP logs can't stop watching

00:38:25,840 --> 00:38:32,740
them then there was humor

00:38:30,610 --> 00:38:35,380
some people just stood outside the

00:38:32,740 --> 00:38:37,690
toilets and they met try too much people

00:38:35,380 --> 00:38:39,280
entering and exiting the toilets with

00:38:37,690 --> 00:38:42,550
some of the data that was peering on the

00:38:39,280 --> 00:38:45,210
website so you can see that it's not you

00:38:42,550 --> 00:38:48,220
know the reaction might be for many

00:38:45,210 --> 00:38:50,200
disgust or outrage but actually there's

00:38:48,220 --> 00:38:52,180
a lot more responses that you might get

00:38:50,200 --> 00:38:53,980
from something like this and remember

00:38:52,180 --> 00:38:59,530
this was all anonymous it wasn't you

00:38:53,980 --> 00:39:02,710
know naming someone so I think as an

00:38:59,530 --> 00:39:04,360
early attempt at trying to get people's

00:39:02,710 --> 00:39:06,220
reactions it was very effective at

00:39:04,360 --> 00:39:07,960
opening up the debate about surveillance

00:39:06,220 --> 00:39:10,150
and I think that's what we need to do is

00:39:07,960 --> 00:39:11,800
not to always think it's black or white

00:39:10,150 --> 00:39:13,570
it's yeah we should definitely stop this

00:39:11,800 --> 00:39:15,880
we should never have this this is

00:39:13,570 --> 00:39:17,350
something that can happen and it might

00:39:15,880 --> 00:39:20,290
be something that's already happening in

00:39:17,350 --> 00:39:24,220
Japan in certain places and the question

00:39:20,290 --> 00:39:27,530
is are people prepared to accept this if

00:39:24,220 --> 00:39:30,970
it has something for the common good

00:39:27,530 --> 00:39:33,920
the second approach or study I want to

00:39:30,970 --> 00:39:36,980
describe is what is called the

00:39:33,920 --> 00:39:38,690
controversion and this was a project we

00:39:36,980 --> 00:39:42,710
did when we were looking at mobile

00:39:38,690 --> 00:39:44,810
privacy a few years ago now and we are

00:39:42,710 --> 00:39:47,120
very interested again about future

00:39:44,810 --> 00:39:49,670
technology and and how that might affect

00:39:47,120 --> 00:39:50,780
people's privacy and we again we didn't

00:39:49,670 --> 00:39:53,030
want to just ask people what they

00:39:50,780 --> 00:39:56,090
thought we wanted to elicit a range of

00:39:53,030 --> 00:39:58,850
reactions their values and attitudes for

00:39:56,090 --> 00:40:01,040
a hypothetical invasive healthcare

00:39:58,850 --> 00:40:03,530
system and again this healthcare system

00:40:01,040 --> 00:40:05,180
was one that we conjured up that if you

00:40:03,530 --> 00:40:08,080
could collect all sorts of personal data

00:40:05,180 --> 00:40:10,940
about you and send it to your doctor and

00:40:08,080 --> 00:40:13,640
the the particular area that we want to

00:40:10,940 --> 00:40:15,320
talk tackle was the difficult challenge

00:40:13,640 --> 00:40:16,640
of food addiction for people who just

00:40:15,320 --> 00:40:18,770
can't stop eating and they've tried

00:40:16,640 --> 00:40:21,050
every diet but they just carry on eating

00:40:18,770 --> 00:40:24,470
how might our new system be able to help

00:40:21,050 --> 00:40:26,480
them and the method uses both positive

00:40:24,470 --> 00:40:29,030
and negative videos of the same scenario

00:40:26,480 --> 00:40:32,060
to enable people to see different points

00:40:29,030 --> 00:40:34,550
of view so this was the fictitious

00:40:32,060 --> 00:40:36,650
technology that we came up with called

00:40:34,550 --> 00:40:38,690
diet modern and it was meant to be a

00:40:36,650 --> 00:40:41,480
wearable diet monitoring system that you

00:40:38,690 --> 00:40:43,010
could use 24/7 and as I said it was

00:40:41,480 --> 00:40:44,900
meant for people who've got serious

00:40:43,010 --> 00:40:47,380
problems and who've tried everything

00:40:44,900 --> 00:40:49,700
else but that has failed and the

00:40:47,380 --> 00:40:52,220
components of this fictitious system

00:40:49,700 --> 00:40:55,880
were a pair of glasses that had a camera

00:40:52,220 --> 00:40:57,500
in this was before Google glass and it

00:40:55,880 --> 00:41:00,680
could take if you are stared at that for

00:40:57,500 --> 00:41:03,140
three seconds it would be able to take a

00:41:00,680 --> 00:41:05,030
picture this would then be analyzed and

00:41:03,140 --> 00:41:06,620
then it would send you your phone had

00:41:05,030 --> 00:41:12,380
the calorific how many calories there

00:41:06,620 --> 00:41:15,200
are in that not only that the system has

00:41:12,380 --> 00:41:17,810
a chip that can be embedded into a user

00:41:15,200 --> 00:41:19,730
and their physiological responses to

00:41:17,810 --> 00:41:21,530
eating could then be sent back to their

00:41:19,730 --> 00:41:24,110
doctor so they could see any changes in

00:41:21,530 --> 00:41:26,750
this sugar level sugar level and so on

00:41:24,110 --> 00:41:30,590
so this the idea here was to have a

00:41:26,750 --> 00:41:33,110
system that could help people by giving

00:41:30,590 --> 00:41:34,550
them constant feedback about what was

00:41:33,110 --> 00:41:35,650
happening and whether they were over

00:41:34,550 --> 00:41:38,150
their targets

00:41:35,650 --> 00:41:40,610
so I'm going to let you watch a video

00:41:38,150 --> 00:41:41,990
now and then to explain how it works

00:41:40,610 --> 00:41:46,180
we'll look at a plate of food for three

00:41:41,990 --> 00:41:46,180
seconds and it will take a photograph

00:41:48,670 --> 00:41:54,410
analyze the coloring context and then

00:41:51,320 --> 00:41:57,560
send you narrative media in particular

00:41:54,410 --> 00:41:59,780
video has been a powerful for triggering

00:41:57,560 --> 00:42:02,750
mental reactions few interfaces and

00:41:59,780 --> 00:42:04,520
technologies future concept videos are

00:42:02,750 --> 00:42:06,910
one way as immersing the viewer in the

00:42:04,520 --> 00:42:10,250
technology the design that exists

00:42:06,910 --> 00:42:13,790
Apple's knowledge navigator from 1987 is

00:42:10,250 --> 00:42:15,680
an early example Microsoft's future

00:42:13,790 --> 00:42:18,050
healthcare videos are more recent

00:42:15,680 --> 00:42:20,360
examples productionvalue day in the life

00:42:18,050 --> 00:42:22,190
style videos designed to give the viewer

00:42:20,360 --> 00:42:24,440
a sense of how the technology will

00:42:22,190 --> 00:42:26,360
affect their life these concept videos

00:42:24,440 --> 00:42:28,760
all show a very positive view of

00:42:26,360 --> 00:42:31,160
technology and we're concerned that our

00:42:28,760 --> 00:42:33,830
utopian only perspective limits user

00:42:31,160 --> 00:42:37,660
reactions particularly with respect to

00:42:33,830 --> 00:42:40,550
concerns such as privacy our methodology

00:42:37,660 --> 00:42:42,320
contra vision involves creating both a

00:42:40,550 --> 00:42:45,290
positive and negative version of each

00:42:42,320 --> 00:42:48,370
scene in the story here is one scene

00:42:45,290 --> 00:42:48,370
from our example video

00:43:00,280 --> 00:43:06,410
note the protagonists enthusiasm

00:43:02,960 --> 00:43:12,740
regarding the technology yep that'll do

00:43:06,410 --> 00:43:14,420
that's great I've got this gadget that

00:43:12,740 --> 00:43:19,510
my doctor's given me cuz I'm Tyler's

00:43:14,420 --> 00:43:19,510
weight and these glasses have got camera

00:43:20,680 --> 00:43:34,580
have a look at that cake there right

00:43:23,570 --> 00:43:37,100
just look at the three seconds that's so

00:43:34,580 --> 00:43:39,170
that the cameras record the image and

00:43:37,100 --> 00:43:42,320
then cross-reference with a database and

00:43:39,170 --> 00:43:43,940
they do an approximate estimation of the

00:43:42,320 --> 00:43:45,500
calorific content plus I've got this

00:43:43,940 --> 00:43:48,590
other thing which is a little microchip

00:43:45,500 --> 00:43:50,420
in my wrist and it model here is the

00:43:48,590 --> 00:43:53,870
same Christine from the negative version

00:43:50,420 --> 00:43:56,060
of the story yes I will

00:43:53,870 --> 00:43:59,650
oh sorry I'm just going to take this

00:43:56,060 --> 00:43:59,650
I'll see you in a sec

00:44:03,780 --> 00:44:09,310
notice how the protagonist hides his use

00:44:06,790 --> 00:44:12,420
of the technology the other guillotines

00:44:09,310 --> 00:44:12,420
shows similar

00:44:18,350 --> 00:44:25,770
he's gone down quite quickly what'd you

00:44:20,970 --> 00:44:27,330
just get this cake please using a

00:44:25,770 --> 00:44:29,490
controversion approach may help

00:44:27,330 --> 00:44:33,170
researchers gather a wider spectrum of

00:44:29,490 --> 00:44:35,940
these responses to future technologies

00:44:33,170 --> 00:44:37,620
ok sorry a bit about the quality of that

00:44:35,940 --> 00:44:40,080
video but I hope you got the message

00:44:37,620 --> 00:44:42,060
that what we've trying to do was to

00:44:40,080 --> 00:44:44,430
present the same scenario but from a

00:44:42,060 --> 00:44:47,340
positive and a negative perspective in

00:44:44,430 --> 00:44:50,340
order to get people to have a wider set

00:44:47,340 --> 00:44:53,520
of possibilities to think about this

00:44:50,340 --> 00:44:58,590
technology so that was just one we've

00:44:53,520 --> 00:45:01,800
made six different videos short videos

00:44:58,590 --> 00:45:04,410
for different scenes one was meeting the

00:45:01,800 --> 00:45:06,840
doctor another was having breakfast with

00:45:04,410 --> 00:45:09,180
the family another one was at the work

00:45:06,840 --> 00:45:11,940
for this birthday party and other was

00:45:09,180 --> 00:45:16,260
going past a cake shop and then having

00:45:11,940 --> 00:45:18,000
dinner with colleagues and an aperitif

00:45:16,260 --> 00:45:21,510
at a bar where there's lots of peanuts

00:45:18,000 --> 00:45:24,060
and crisps and what we found well just

00:45:21,510 --> 00:45:26,430
to reiterate the main difference between

00:45:24,060 --> 00:45:28,530
the videos is that in the positive the

00:45:26,430 --> 00:45:30,600
protagonist embraces the technology

00:45:28,530 --> 00:45:33,180
they're very open enthusiastic they're

00:45:30,600 --> 00:45:34,950
proactive and see how effective it is

00:45:33,180 --> 00:45:37,140
and they negotiate obstacles they're

00:45:34,950 --> 00:45:40,920
getting away and ultimately succeeds

00:45:37,140 --> 00:45:42,900
with this in the negative video the

00:45:40,920 --> 00:45:45,480
protagonist is reluctant to try it

00:45:42,900 --> 00:45:47,610
they're very deceptive and secretive

00:45:45,480 --> 00:45:52,050
about what they're doing they're passive

00:45:47,610 --> 00:45:54,570
in ineffectual and defeated by obstacles

00:45:52,050 --> 00:45:56,100
and then ultimately fail so even though

00:45:54,570 --> 00:45:58,950
that the same scenario you get very

00:45:56,100 --> 00:46:03,330
different ways in which the protagonist

00:45:58,950 --> 00:46:06,230
and the people around them respond so we

00:46:03,330 --> 00:46:09,000
carried out a number of focus groups and

00:46:06,230 --> 00:46:10,920
interviews for people who watch these

00:46:09,000 --> 00:46:13,650
videos both the positive and negative

00:46:10,920 --> 00:46:16,020
and what we found was that there was a

00:46:13,650 --> 00:46:18,440
wide range of responses in both

00:46:16,020 --> 00:46:21,690
conditions and here's some examples of

00:46:18,440 --> 00:46:23,220
awesome quotes but for the positive

00:46:21,690 --> 00:46:25,290
someone said it's not natural

00:46:23,220 --> 00:46:27,420
he's too open not normal I wouldn't

00:46:25,290 --> 00:46:27,990
expose myself too much I would make me

00:46:27,420 --> 00:46:30,330
look bad

00:46:27,990 --> 00:46:32,280
but it was from the negative was in

00:46:30,330 --> 00:46:34,500
someone was saying encourages deception

00:46:32,280 --> 00:46:36,840
it's better to be open to lower the

00:46:34,500 --> 00:46:40,320
level of stress with his deception he

00:46:36,840 --> 00:46:42,510
alienates others and then more reactions

00:46:40,320 --> 00:46:45,450
that it's too interfering it's rewarding

00:46:42,510 --> 00:46:47,490
too much hello

00:46:45,450 --> 00:46:51,660
I don't know where that comes from

00:46:47,490 --> 00:46:54,350
there's someone yes it does work it's

00:46:51,660 --> 00:46:58,560
too distracting or it's invading privacy

00:46:54,350 --> 00:47:01,440
and there you know so you get a range of

00:46:58,560 --> 00:47:03,180
different reactions so for the invading

00:47:01,440 --> 00:47:04,830
prison privacy I wouldn't want others to

00:47:03,180 --> 00:47:09,750
see what I eat if I had a problem with

00:47:04,830 --> 00:47:11,369
weight and so on so this approach which

00:47:09,750 --> 00:47:13,500
we've called contra vision I think

00:47:11,369 --> 00:47:15,990
provides an alternative way of examining

00:47:13,500 --> 00:47:17,880
attitudes values and concerns for a

00:47:15,990 --> 00:47:20,040
future technology that could be very

00:47:17,880 --> 00:47:22,500
sensitive but ultimately could have many

00:47:20,040 --> 00:47:24,630
benefits for people in society but

00:47:22,500 --> 00:47:25,940
before we you know launch this and

00:47:24,630 --> 00:47:28,140
perhaps Google might have benefited

00:47:25,940 --> 00:47:30,600
doing something like this with Google

00:47:28,140 --> 00:47:33,030
glass where you know getting seen

00:47:30,600 --> 00:47:35,970
whether people are accepting or they're

00:47:33,030 --> 00:47:38,010
horrified or they find it creepy and I

00:47:35,970 --> 00:47:39,780
think it what it does is it exposes

00:47:38,010 --> 00:47:42,420
people to different perspectives and

00:47:39,780 --> 00:47:44,609
that can lead to more informed opinions

00:47:42,420 --> 00:47:47,310
about sensitive issues particularly on

00:47:44,609 --> 00:47:49,920
the personal use of new technology also

00:47:47,310 --> 00:47:51,770
whether external support is there to

00:47:49,920 --> 00:47:54,270
solve an uncomfortable problem and

00:47:51,770 --> 00:47:57,800
whether there's a level of openness or

00:47:54,270 --> 00:48:00,600
deception that will be you know

00:47:57,800 --> 00:48:03,859
facilitated or happen and then what's

00:48:00,600 --> 00:48:07,530
the impact on others reactions to this

00:48:03,859 --> 00:48:09,840
okay so the the last part of my talk is

00:48:07,530 --> 00:48:12,450
Ben is talking more about why don't we

00:48:09,840 --> 00:48:13,950
try and open up more data to the public

00:48:12,450 --> 00:48:15,660
and empower them so very much a

00:48:13,950 --> 00:48:18,630
bottom-up approach there's been so much

00:48:15,660 --> 00:48:20,730
discussion about big data from top-down

00:48:18,630 --> 00:48:23,250
about what companies are doing with this

00:48:20,730 --> 00:48:25,020
and the creepy data agenda but I was

00:48:23,250 --> 00:48:27,420
thinking well wouldn't it be great if

00:48:25,020 --> 00:48:30,119
some of us had access to more data about

00:48:27,420 --> 00:48:32,910
our personal data our health data the

00:48:30,119 --> 00:48:34,650
environmental data so that we know more

00:48:32,910 --> 00:48:37,500
about what's going on in the world and

00:48:34,650 --> 00:48:39,750
in our communities but how do we do this

00:48:37,500 --> 00:48:43,530
and what might be the benefits

00:48:39,750 --> 00:48:45,090
and one approach that the open inch data

00:48:43,530 --> 00:48:48,060
Institute which is based in London

00:48:45,090 --> 00:48:49,080
opened a few years ago was to think we

00:48:48,060 --> 00:48:51,869
need to increase people's awareness

00:48:49,080 --> 00:48:53,609
first of all of how to be ethical when

00:48:51,869 --> 00:48:57,840
doing this so it's one thing just to say

00:48:53,609 --> 00:49:01,290
here here's all the data about your your

00:48:57,840 --> 00:49:03,119
health for your your heart rates for the

00:49:01,290 --> 00:49:04,950
last 10 years but what would you do with

00:49:03,119 --> 00:49:07,890
it would it make you anxious or neurotic

00:49:04,950 --> 00:49:09,750
how do we make sure that when we give

00:49:07,890 --> 00:49:12,000
data to people it's done in the

00:49:09,750 --> 00:49:14,700
appropriate way and so what they've done

00:49:12,000 --> 00:49:16,830
at the open data Institute is to provide

00:49:14,700 --> 00:49:19,230
tools and guidelines to companies to

00:49:16,830 --> 00:49:21,090
organizations to think about when

00:49:19,230 --> 00:49:24,540
they're making data open how best to do

00:49:21,090 --> 00:49:26,970
this and they provided this data ethics

00:49:24,540 --> 00:49:29,280
canvas and you probably can't read here

00:49:26,970 --> 00:49:31,410
but there are lots of questions that you

00:49:29,280 --> 00:49:33,390
should ask when when making data

00:49:31,410 --> 00:49:36,960
available and you can if you can just

00:49:33,390 --> 00:49:39,540
see here I've blown up a couple of them

00:49:36,960 --> 00:49:42,599
here which is engaging with people

00:49:39,540 --> 00:49:44,460
how can people engage with you can

00:49:42,599 --> 00:49:46,770
people affected appeal or request

00:49:44,460 --> 00:49:48,000
changes the service to what extent so

00:49:46,770 --> 00:49:49,650
there are lots and lots of questions

00:49:48,000 --> 00:49:52,260
that they're proposing that you go

00:49:49,650 --> 00:49:53,730
through and I don't know how many people

00:49:52,260 --> 00:49:55,740
are using this but I think it's going in

00:49:53,730 --> 00:49:58,349
the right direction to give companies

00:49:55,740 --> 00:50:00,540
who want to make their data open a set

00:49:58,349 --> 00:50:02,730
of questions to go through to feel

00:50:00,540 --> 00:50:05,520
reassured that they're least you know

00:50:02,730 --> 00:50:08,790
making an effort to think of what people

00:50:05,520 --> 00:50:12,000
themselves might think of but what else

00:50:08,790 --> 00:50:14,660
can researchers do and this is the last

00:50:12,000 --> 00:50:17,400
case study I'm gonna talk about which is

00:50:14,660 --> 00:50:20,490
how do we explore how and when to reveal

00:50:17,400 --> 00:50:21,780
data to the general public what's been

00:50:20,490 --> 00:50:24,599
collected about them their local

00:50:21,780 --> 00:50:28,890
environment and where they go does it is

00:50:24,599 --> 00:50:32,790
it does it trigger curiosity or does it

00:50:28,890 --> 00:50:34,800
you know trigger MIT mistrust and also

00:50:32,790 --> 00:50:36,720
why where and what is its data being

00:50:34,800 --> 00:50:39,510
used for why are you making open why are

00:50:36,720 --> 00:50:40,770
you collecting this data and lastly I

00:50:39,510 --> 00:50:44,790
want to talk about some research that's

00:50:40,770 --> 00:50:46,080
currently doing which is can we make can

00:50:44,790 --> 00:50:49,200
we investigate where the data can

00:50:46,080 --> 00:50:51,390
democratize society can society benefit

00:50:49,200 --> 00:50:54,109
and learn more about how to put it to

00:50:51,390 --> 00:50:57,000
good use and that's what I'll finish

00:50:54,109 --> 00:51:00,200
so this is a project that we conducted a

00:50:57,000 --> 00:51:03,180
couple of years ago which was helping

00:51:00,200 --> 00:51:06,230
the tourist board on Madeira to

00:51:03,180 --> 00:51:08,670
understand better the impact of tourism

00:51:06,230 --> 00:51:11,039
it was particularly economic and

00:51:08,670 --> 00:51:12,510
ecological on the small island of

00:51:11,039 --> 00:51:15,059
Madeira how many of you been to Madeira

00:51:12,510 --> 00:51:17,579
a few of you so you know how beautiful

00:51:15,059 --> 00:51:20,039
it is and you know also know that the

00:51:17,579 --> 00:51:22,369
number of cruise ships that come in

00:51:20,039 --> 00:51:24,930
there are about three or four a day and

00:51:22,369 --> 00:51:26,910
thousands of tourists just come off the

00:51:24,930 --> 00:51:28,289
cruise ship and they go off to onto the

00:51:26,910 --> 00:51:30,690
Isle and do a few things and then come

00:51:28,289 --> 00:51:32,640
back and effectively there's a well over

00:51:30,690 --> 00:51:36,809
1 million tourists that visit this

00:51:32,640 --> 00:51:40,680
island per annum and there's a

00:51:36,809 --> 00:51:44,180
population of only 250,000 people and so

00:51:40,680 --> 00:51:46,380
be obviously the tourist board and

00:51:44,180 --> 00:51:48,150
people on the island are worried about

00:51:46,380 --> 00:51:50,640
how it's affecting the resources and the

00:51:48,150 --> 00:51:53,490
wildlife so they wanted to have a better

00:51:50,640 --> 00:51:58,140
picture of where the tourists go and how

00:51:53,490 --> 00:52:00,150
many and what they installed was a Wi-Fi

00:51:58,140 --> 00:52:01,829
sensing infrastructure that can measure

00:52:00,150 --> 00:52:05,009
the tourists coming from the airport

00:52:01,829 --> 00:52:07,799
and off the cruise ships to the port and

00:52:05,009 --> 00:52:09,869
seeing where they went and how many and

00:52:07,799 --> 00:52:12,900
what times and the way in which they did

00:52:09,869 --> 00:52:15,539
this was to count the number of Wi-Fi

00:52:12,900 --> 00:52:17,250
enabled devices in a given location so

00:52:15,539 --> 00:52:19,829
your smartphone if it's got Wi-Fi

00:52:17,250 --> 00:52:22,200
capability it could detect the unique

00:52:19,829 --> 00:52:24,329
identifier from those phones that have

00:52:22,200 --> 00:52:26,490
the Wi-Fi switched on and then they

00:52:24,329 --> 00:52:28,890
could a given time estimate how many

00:52:26,490 --> 00:52:32,160
people are there that beta was collected

00:52:28,890 --> 00:52:36,059
was anonymous they could of course you

00:52:32,160 --> 00:52:37,829
know using various forms of analysis of

00:52:36,059 --> 00:52:40,289
who's come off the ship for what planes

00:52:37,829 --> 00:52:41,579
have detected who they were but they

00:52:40,289 --> 00:52:43,950
didn't they just wanted to get the

00:52:41,579 --> 00:52:46,859
numbers of course and then the numbers

00:52:43,950 --> 00:52:48,720
were converted into tourist flows so

00:52:46,859 --> 00:52:52,049
here's an example of a tourist flow that

00:52:48,720 --> 00:52:54,509
was that made from this data collection

00:52:52,049 --> 00:52:57,750
and you can see peaks and troughs at

00:52:54,509 --> 00:53:00,329
different times and what it's showing us

00:52:57,750 --> 00:53:02,970
is exactly that peaks and troughs of

00:53:00,329 --> 00:53:05,519
people that are at this place on the

00:53:02,970 --> 00:53:06,530
island but why are there Peaks what's

00:53:05,519 --> 00:53:07,940
behind them

00:53:06,530 --> 00:53:09,380
why suddenly there are a lot of people

00:53:07,940 --> 00:53:11,420
here and that's what's really

00:53:09,380 --> 00:53:14,120
interesting is one thing to have numbers

00:53:11,420 --> 00:53:15,650
and graphs and it's another to be able

00:53:14,120 --> 00:53:17,510
to understand what's behind it and

00:53:15,650 --> 00:53:19,880
that's where I think the tourist board

00:53:17,510 --> 00:53:22,430
they were fine getting this data but it

00:53:19,880 --> 00:53:24,050
didn't really help them to and so that's

00:53:22,430 --> 00:53:26,090
where we were asked to come in and help

00:53:24,050 --> 00:53:29,900
think about how we could enrich this

00:53:26,090 --> 00:53:32,090
data and so we started asking questions

00:53:29,900 --> 00:53:34,520
and talking to the tourist board would

00:53:32,090 --> 00:53:38,000
people the general public passes by be

00:53:34,520 --> 00:53:40,610
interested in understanding how tourism

00:53:38,000 --> 00:53:42,500
is impacting Madeira would they provide

00:53:40,610 --> 00:53:44,090
more information than just numbers about

00:53:42,500 --> 00:53:46,580
themselves and their surroundings for

00:53:44,090 --> 00:53:48,890
example what can they see around are

00:53:46,580 --> 00:53:50,540
their families are there young people

00:53:48,890 --> 00:53:52,070
whether people with mustaches how many

00:53:50,540 --> 00:53:54,620
people have got glasses whatever

00:53:52,070 --> 00:53:57,500
questions you know just to get them to I

00:53:54,620 --> 00:53:59,060
was being a bit silly there but I'm just

00:53:57,500 --> 00:54:02,270
saying that you can ask many questions

00:53:59,060 --> 00:54:04,100
for them to look around but more

00:54:02,270 --> 00:54:06,080
seriously could they provide

00:54:04,100 --> 00:54:10,480
explanations of what is behind the data

00:54:06,080 --> 00:54:12,860
the peaks and troughs so what we did

00:54:10,480 --> 00:54:15,260
we've been involved in designing a

00:54:12,860 --> 00:54:19,040
series of physical installations in my

00:54:15,260 --> 00:54:21,110
lab to attract people passers-by to

00:54:19,040 --> 00:54:23,210
answer questions and this one we

00:54:21,110 --> 00:54:25,670
developed at which we call Romeo it's a

00:54:23,210 --> 00:54:27,560
physical public kiosk the idea was that

00:54:25,670 --> 00:54:30,110
it would move around but we didn't have

00:54:27,560 --> 00:54:31,850
this technology to let it move around it

00:54:30,110 --> 00:54:34,190
kept falling over so it was more

00:54:31,850 --> 00:54:37,430
stationary but it was to attract

00:54:34,190 --> 00:54:38,810
passers-by to answer questions so it's

00:54:37,430 --> 00:54:41,030
got a very friendly interface it had a

00:54:38,810 --> 00:54:43,340
bow tie and some eyes just to make it

00:54:41,030 --> 00:54:46,250
appear friendly it wasn't meant to be

00:54:43,340 --> 00:54:49,030
anthropomorphize and popo Morpheus it

00:54:46,250 --> 00:54:51,380
was meant to just be attractive and

00:54:49,030 --> 00:54:53,240
passers-by are asked questions about

00:54:51,380 --> 00:54:54,980
themselves that collected data flow

00:54:53,240 --> 00:54:56,870
visualizations you saw and the

00:54:54,980 --> 00:54:59,270
surroundings and they can answer simply

00:54:56,870 --> 00:55:01,900
by using yes/no buttons you can see the

00:54:59,270 --> 00:55:04,630
white buttons all there's a keypad for

00:55:01,900 --> 00:55:06,890
open-ended answers

00:55:04,630 --> 00:55:08,300
so the questions asked within a lot of

00:55:06,890 --> 00:55:09,650
time with the tourist world thinking

00:55:08,300 --> 00:55:12,950
about the questions that we might ask

00:55:09,650 --> 00:55:14,870
how many so we asked questions about as

00:55:12,950 --> 00:55:17,840
I said context describe the people

00:55:14,870 --> 00:55:18,200
around you and then what's the problem

00:55:17,840 --> 00:55:20,270
mood

00:55:18,200 --> 00:55:22,580
you think people are happy is there a

00:55:20,270 --> 00:55:26,030
good vibe what's the average age around

00:55:22,580 --> 00:55:27,590
here does this place feel busy to you so

00:55:26,030 --> 00:55:29,780
things that you can never get from

00:55:27,590 --> 00:55:31,940
numbers but might be really interesting

00:55:29,780 --> 00:55:35,240
and important to know about what's

00:55:31,940 --> 00:55:36,740
happening in a place contextual

00:55:35,240 --> 00:55:39,560
questions do you come here often

00:55:36,740 --> 00:55:41,510
what nationality are you how often have

00:55:39,560 --> 00:55:42,950
you been into Madeira why are you here

00:55:41,510 --> 00:55:45,620
so many questions that are quite

00:55:42,950 --> 00:55:48,260
personal and then the ones about data

00:55:45,620 --> 00:55:50,210
why does this Airport get busier from

00:55:48,260 --> 00:55:53,270
May onwards why they're more people at

00:55:50,210 --> 00:55:55,460
the port why is it busier at lunchtime

00:55:53,270 --> 00:55:57,170
and so on and so forth and then some

00:55:55,460 --> 00:55:59,030
factual questions general knowledge

00:55:57,170 --> 00:56:01,490
which people seem to love trying to

00:55:59,030 --> 00:56:03,950
answer how many people use the airport

00:56:01,490 --> 00:56:06,350
on an average day from what country do

00:56:03,950 --> 00:56:08,510
most tourists come from which is the

00:56:06,350 --> 00:56:10,640
wettest month and so on so what we did

00:56:08,510 --> 00:56:12,380
was we mix these up people could come up

00:56:10,640 --> 00:56:14,360
just aren't any and any number of

00:56:12,380 --> 00:56:17,300
questions and walk away it would then

00:56:14,360 --> 00:56:21,680
reset itself and so someone else could

00:56:17,300 --> 00:56:24,400
come up what happened we placed this in

00:56:21,680 --> 00:56:27,740
different locations on the island and

00:56:24,400 --> 00:56:30,260
there was much intrigue and use by all

00:56:27,740 --> 00:56:32,150
manner of people we had individuals we

00:56:30,260 --> 00:56:34,730
had pairs we had groups and families and

00:56:32,150 --> 00:56:37,130
then people came back with others to

00:56:34,730 --> 00:56:39,860
show them this and during the first

00:56:37,130 --> 00:56:42,020
deployment over 500 people answered the

00:56:39,860 --> 00:56:44,120
questions over a thousand questions are

00:56:42,020 --> 00:56:45,590
answered because it's in Portugal we had

00:56:44,120 --> 00:56:50,360
the option during Portuguese or English

00:56:45,590 --> 00:56:53,000
and so there some a good percentage were

00:56:50,360 --> 00:56:56,570
Portuguese most of them were yes/no

00:56:53,000 --> 00:56:57,950
answers but some people gave freeform

00:56:56,570 --> 00:57:01,430
answers and there was much discussion

00:56:57,950 --> 00:57:02,840
from the groups around about what why

00:57:01,430 --> 00:57:05,690
were these questions being asked what

00:57:02,840 --> 00:57:08,480
was you know what was the data that they

00:57:05,690 --> 00:57:11,260
were being shown but they were highly

00:57:08,480 --> 00:57:13,700
engaged so the majority of people spent

00:57:11,260 --> 00:57:15,980
between one minute and five minutes

00:57:13,700 --> 00:57:18,590
interacting with it others would be

00:57:15,980 --> 00:57:21,050
waiting for their go we only had one of

00:57:18,590 --> 00:57:23,570
these built at the time and then they

00:57:21,050 --> 00:57:24,950
would come along and use it and so and

00:57:23,570 --> 00:57:26,780
then there were quite a few people used

00:57:24,950 --> 00:57:28,580
it for over five minutes and we didn't

00:57:26,780 --> 00:57:29,930
coerce them we weren't there to say hey

00:57:28,580 --> 00:57:31,730
come and try our amazing

00:57:29,930 --> 00:57:33,859
technology give us answers it just stood

00:57:31,730 --> 00:57:37,130
there at the airport and in a tourist

00:57:33,859 --> 00:57:39,020
board and at the port the data that we

00:57:37,130 --> 00:57:40,339
presented was fairly simple we just want

00:57:39,020 --> 00:57:42,980
to see whether people would be prepared

00:57:40,339 --> 00:57:46,130
to see if they could answer the question

00:57:42,980 --> 00:57:48,230
so why is a central part of Funchal so

00:57:46,130 --> 00:57:50,210
busy at 10 o'clock and the answer is

00:57:48,230 --> 00:57:52,160
bolo do CACO any of you have been there

00:57:50,210 --> 00:57:54,500
there's most amazing Portuguese bread

00:57:52,160 --> 00:57:57,740
it's delicious and people queue up for

00:57:54,500 --> 00:57:59,000
that so that was the explanation why are

00:57:57,740 --> 00:58:01,579
there more people at the airport on

00:57:59,000 --> 00:58:03,530
Mondays a lot of people selected more

00:58:01,579 --> 00:58:05,900
incoming tourists or business flights

00:58:03,530 --> 00:58:08,329
but 5% suggest other reasons which were

00:58:05,900 --> 00:58:10,190
really quite interesting as was why does

00:58:08,329 --> 00:58:12,619
it get busier in this area from May

00:58:10,190 --> 00:58:13,940
onwards so most people said because

00:58:12,619 --> 00:58:15,440
there were more events or they're more

00:58:13,940 --> 00:58:20,000
tourists but there were others nearly

00:58:15,440 --> 00:58:22,880
20% suggested other reasons so what did

00:58:20,000 --> 00:58:25,640
I study reveal I think firstly that this

00:58:22,880 --> 00:58:26,720
kind of device rather than walking up to

00:58:25,640 --> 00:58:28,520
someone saying can you answer a few

00:58:26,720 --> 00:58:31,309
questions or can you go online to a

00:58:28,520 --> 00:58:33,380
website it attracted many people they

00:58:31,309 --> 00:58:34,880
were very curious about it and then once

00:58:33,380 --> 00:58:37,069
they started answering the questions

00:58:34,880 --> 00:58:39,619
they didn't stop they really enjoyed

00:58:37,069 --> 00:58:41,690
answering those questions and it was a

00:58:39,619 --> 00:58:43,339
diversity people so it was tourists it

00:58:41,690 --> 00:58:46,700
was local it was people from all over

00:58:43,339 --> 00:58:51,380
the world and it was families and young

00:58:46,700 --> 00:58:52,880
kids and a lot of them had to go at

00:58:51,380 --> 00:58:55,010
explaining those people flow

00:58:52,880 --> 00:58:56,780
visualizations and we were interested to

00:58:55,010 --> 00:58:59,000
see whether if we gave them the data

00:58:56,780 --> 00:59:00,680
what did they think of it but the data

00:58:59,000 --> 00:59:02,420
was a week old because we could only get

00:59:00,680 --> 00:59:05,150
that data we couldn't have the real data

00:59:02,420 --> 00:59:07,339
that there and so it's limited in what

00:59:05,150 --> 00:59:09,319
they knew as to how they could interpret

00:59:07,339 --> 00:59:11,750
it but it was promising for us to think

00:59:09,319 --> 00:59:13,819
you can give data to people and not just

00:59:11,750 --> 00:59:17,390
ask the experts the tourists or the

00:59:13,819 --> 00:59:19,040
scientists analyze it but let others and

00:59:17,390 --> 00:59:20,960
I think for the tourist board they

00:59:19,040 --> 00:59:23,630
obtained a richer picture what was going

00:59:20,960 --> 00:59:25,720
on in the island and more importantly

00:59:23,630 --> 00:59:29,180
they started to ask different questions

00:59:25,720 --> 00:59:30,790
about what tourists do rather than you

00:59:29,180 --> 00:59:35,450
know which countries do they come from

00:59:30,790 --> 00:59:38,390
and so I think for us it was an attempt

00:59:35,450 --> 00:59:41,809
to combine automatic data tracking or

00:59:38,390 --> 00:59:43,310
monitoring which was this Wi-Fi hotspot'

00:59:41,809 --> 00:59:45,440
analysis with

00:59:43,310 --> 00:59:49,790
more acceptable approach to collecting

00:59:45,440 --> 00:59:53,000
data from the public about the public so

00:59:49,790 --> 00:59:54,350
I think we're just coming to the end so

00:59:53,000 --> 00:59:55,730
I've got five minutes I just want to

00:59:54,350 --> 00:59:57,920
finish off with some current research

00:59:55,730 --> 00:59:59,930
that we're doing which is working with

00:59:57,920 --> 01:00:02,270
Great Ormond Street Hospitals New Living

00:59:59,930 --> 01:00:04,970
Lab which is called Drive for exploring

01:00:02,270 --> 01:00:07,160
health data and this was launched a

01:00:04,970 --> 01:00:08,510
couple of weeks ago and there's a lot of

01:00:07,160 --> 01:00:10,640
excitement those of you who don't know

01:00:08,510 --> 01:00:12,800
Great Ormond Street Hospital is for

01:00:10,640 --> 01:00:15,860
children who are very sick it's a very

01:00:12,800 --> 01:00:17,720
well-known Ospital in London and they're

01:00:15,860 --> 01:00:20,030
very much at the forefront of new

01:00:17,720 --> 01:00:22,550
technology trying to harness this new

01:00:20,030 --> 01:00:25,640
technology from VR to artificial

01:00:22,550 --> 01:00:27,710
intelligence to data science to somehow

01:00:25,640 --> 01:00:30,590
think of how can they transform clinical

01:00:27,710 --> 01:00:32,900
practice and enhance patient experience

01:00:30,590 --> 01:00:35,600
and this Living Lab is a very large

01:00:32,900 --> 01:00:37,340
space opposite Russell Square and it's

01:00:35,600 --> 01:00:40,010
been designed to be a safe place to try

01:00:37,340 --> 01:00:42,380
out new technologies and ways of looking

01:00:40,010 --> 01:00:45,320
at the data so that members of the

01:00:42,380 --> 01:00:47,540
public clinicians and students can come

01:00:45,320 --> 01:00:50,570
along and try it out before trying to

01:00:47,540 --> 01:00:53,120
put it into the hospital in you know and

01:00:50,570 --> 01:00:54,950
and see whether it's effective rather

01:00:53,120 --> 01:00:57,770
than it being too late and spending lots

01:00:54,950 --> 01:01:02,270
of money and it not being doing much or

01:00:57,770 --> 01:01:03,560
doing detrimental work so part of what

01:01:02,270 --> 01:01:05,030
we're doing there are many exciting

01:01:03,560 --> 01:01:07,670
projects that are going on there

01:01:05,030 --> 01:01:09,380
but within you click we're looking at

01:01:07,670 --> 01:01:12,590
how we can visualize the clinical data

01:01:09,380 --> 01:01:14,780
to make it usable useful and actionable

01:01:12,590 --> 01:01:17,560
and the hospitals can now collect lots

01:01:14,780 --> 01:01:20,630
of data about patients for example

01:01:17,560 --> 01:01:22,400
cardiovascular data for different kinds

01:01:20,630 --> 01:01:25,360
of operations how much blood is being

01:01:22,400 --> 01:01:28,070
used for not for a particular operation

01:01:25,360 --> 01:01:31,070
how many patients have been through the

01:01:28,070 --> 01:01:32,750
difference between the success rates of

01:01:31,070 --> 01:01:34,820
different kinds of operations and so on

01:01:32,750 --> 01:01:37,160
and this data used to be ephemeral but

01:01:34,820 --> 01:01:39,320
now they can store it and they can do

01:01:37,160 --> 01:01:41,000
things with it and we're interested in

01:01:39,320 --> 01:01:44,270
what you might do with all of this data

01:01:41,000 --> 01:01:46,160
for clinicians who are very busy and in

01:01:44,270 --> 01:01:47,750
particular we're interested in how we

01:01:46,160 --> 01:01:51,350
could you know use this data to

01:01:47,750 --> 01:01:52,580
democratize existing practices and we're

01:01:51,350 --> 01:01:55,040
looking at how we can design new

01:01:52,580 --> 01:01:57,680
interfaces to support more

01:01:55,040 --> 01:01:59,690
to participation so typically you'll get

01:01:57,680 --> 01:02:02,150
a data scientist they'll have the data

01:01:59,690 --> 01:02:04,760
they'll be poring over it and then they

01:02:02,150 --> 01:02:06,740
might report back on that data to the

01:02:04,760 --> 01:02:08,840
clinicians or there might be just one

01:02:06,740 --> 01:02:13,580
clinician you spent some time learning

01:02:08,840 --> 01:02:16,190
how to use these data science tools and

01:02:13,580 --> 01:02:18,350
so what it tends to do is it biases

01:02:16,190 --> 01:02:21,980
towards supporting one person usually an

01:02:18,350 --> 01:02:24,650
expert to control in control to steer

01:02:21,980 --> 01:02:26,450
the analysis and interpret the data what

01:02:24,650 --> 01:02:29,090
we're trying to do is to move away from

01:02:26,450 --> 01:02:30,820
that model to one where it's more level

01:02:29,090 --> 01:02:34,780
playing field where there are a number

01:02:30,820 --> 01:02:36,890
tuna tease for who's around that data to

01:02:34,780 --> 01:02:38,510
interact and understand it so we're

01:02:36,890 --> 01:02:42,460
designing a new multimodal interface

01:02:38,510 --> 01:02:48,260
we're combining speech input we're using

01:02:42,460 --> 01:02:54,560
the Alaia liza elixir little Freudian

01:02:48,260 --> 01:02:57,020
sit there the Alexa analytics API and

01:02:54,560 --> 01:02:59,210
we're combining that with visual data so

01:02:57,020 --> 01:03:01,910
that you can talk to the visualization

01:02:59,210 --> 01:03:03,440
and break it down and add to it and so

01:03:01,910 --> 01:03:07,130
that anyone can ask questions of the

01:03:03,440 --> 01:03:08,840
data and thinking about by doing that by

01:03:07,130 --> 01:03:10,580
talking rather than just simply pressing

01:03:08,840 --> 01:03:12,440
buttons on and off or filters on and off

01:03:10,580 --> 01:03:13,670
does that encourage a new way does that

01:03:12,440 --> 01:03:15,590
mean people encourage people to

01:03:13,670 --> 01:03:18,530
externalize and think through new

01:03:15,590 --> 01:03:20,570
questions and also to get people to in a

01:03:18,530 --> 01:03:21,950
group to talk more about the data we

01:03:20,570 --> 01:03:24,740
don't know we're just starting to work

01:03:21,950 --> 01:03:26,030
on this and we really want to facilitate

01:03:24,740 --> 01:03:27,920
new ways of thinking about clinical

01:03:26,030 --> 01:03:30,500
practices typically in a hospital it's

01:03:27,920 --> 01:03:34,580
very hierarchical as to who gets to say

01:03:30,500 --> 01:03:36,410
what if we could change that from if you

01:03:34,580 --> 01:03:39,650
look on the left that's typically how

01:03:36,410 --> 01:03:42,200
you might get your data science experts

01:03:39,650 --> 01:03:44,540
working on some data towards something

01:03:42,200 --> 01:03:47,600
like this this is Apple design studio I

01:03:44,540 --> 01:03:49,490
couldn't find an image by just borrowed

01:03:47,600 --> 01:03:50,870
that because I think it captures what

01:03:49,490 --> 01:03:53,360
we're trying to do which is to allow

01:03:50,870 --> 01:03:55,790
everyone to be able to have a say and to

01:03:53,360 --> 01:03:57,800
collaborate we think that if you can

01:03:55,790 --> 01:04:01,760
design new interfaces to support this

01:03:57,800 --> 01:04:03,860
kind of collaboration you'll get you'll

01:04:01,760 --> 01:04:05,600
be able to open up the data and allow

01:04:03,860 --> 01:04:07,220
more people to have a say about what

01:04:05,600 --> 01:04:08,400
they think about this data what they can

01:04:07,220 --> 01:04:10,690
do with it

01:04:08,400 --> 01:04:13,000
so on that note I'm just going to

01:04:10,690 --> 01:04:14,890
summarize and then open the floor to

01:04:13,000 --> 01:04:17,080
some questions which is at the beginning

01:04:14,890 --> 01:04:19,150
of my talk I said that creepy data

01:04:17,080 --> 01:04:21,520
collections on the rise and there were

01:04:19,150 --> 01:04:24,400
some examples and I think it raises some

01:04:21,520 --> 01:04:26,470
important privacy issues but also some

01:04:24,400 --> 01:04:29,040
questions moral questions that we as a

01:04:26,470 --> 01:04:30,940
community need to address more which is

01:04:29,040 --> 01:04:32,890
should we just be thinking about

01:04:30,940 --> 01:04:37,180
restricting monitoring and requiring

01:04:32,890 --> 01:04:38,800
people's consent for any data or if the

01:04:37,180 --> 01:04:40,420
reasons for collecting personal data are

01:04:38,800 --> 01:04:42,250
made more public will people be more

01:04:40,420 --> 01:04:45,700
accepting and they'll think that's okay

01:04:42,250 --> 01:04:48,520
and then lastly I talked about how we

01:04:45,700 --> 01:04:50,200
can you know carry out research that can

01:04:48,520 --> 01:04:52,210
help discover what's acceptable to

01:04:50,200 --> 01:04:54,760
people what their opinions are when

01:04:52,210 --> 01:04:55,990
they're put on the spot and lastly this

01:04:54,760 --> 01:04:58,030
is something that's very close to my

01:04:55,990 --> 01:05:00,340
heart which is how can we open up data

01:04:58,030 --> 01:05:02,440
so that it could be democratizing and

01:05:00,340 --> 01:05:04,390
empowering to communities and society at

01:05:02,440 --> 01:05:07,570
large rather than always providing

01:05:04,390 --> 01:05:09,520
experts with new tools to analyze that

01:05:07,570 --> 01:05:16,810
data now on that note I'd like to say

01:05:09,520 --> 01:05:19,180
thank you very much thank you so much

01:05:16,810 --> 01:05:21,910
you gave us lots and lots of food for

01:05:19,180 --> 01:05:23,530
thought and now we're just gonna open it

01:05:21,910 --> 01:05:25,270
up for questions there's still a chance

01:05:23,530 --> 01:05:27,400
to get your questions in on Twitter

01:05:25,270 --> 01:05:29,470
that's hashtag Mozilla speakers and on

01:05:27,400 --> 01:05:33,130
slack which is our speaker series

01:05:29,470 --> 01:05:34,600
Channel and Diane I think we'll be

01:05:33,130 --> 01:05:38,290
taking questions from some of the other

01:05:34,600 --> 01:05:41,230
officers we're going to start with a

01:05:38,290 --> 01:05:43,000
question that's come in and that's it's

01:05:41,230 --> 01:05:45,130
a bit of a meaty one but should privacy

01:05:43,000 --> 01:05:47,710
advocates target the point of collection

01:05:45,130 --> 01:05:50,470
the point of data processing or the

01:05:47,710 --> 01:05:54,790
point of actions stemming from that that

01:05:50,470 --> 01:05:57,520
processing I think the easy answer to

01:05:54,790 --> 01:06:00,100
say all of them but it depends on the

01:05:57,520 --> 01:06:02,620
context I think we need to you know what

01:06:00,100 --> 01:06:05,320
does it mean to be a data activist and

01:06:02,620 --> 01:06:08,380
you know for what context is it about

01:06:05,320 --> 01:06:10,450
health data is it about data that's been

01:06:08,380 --> 01:06:11,890
collected about you know the emotional a

01:06:10,450 --> 01:06:13,900
I'd later and I think each of these

01:06:11,890 --> 01:06:15,130
contexts it could be different as to

01:06:13,900 --> 01:06:17,130
whether you should be focusing on the

01:06:15,130 --> 01:06:21,029
data collection or how it's being used

01:06:17,130 --> 01:06:23,579
so I don't think it again I think is

01:06:21,029 --> 01:06:25,259
isn't black or white answer I think some

01:06:23,579 --> 01:06:27,689
activists may be thinking we should do

01:06:25,259 --> 01:06:29,669
it for all but it could be that at

01:06:27,689 --> 01:06:49,439
certain points it's more important than

01:06:29,669 --> 01:06:52,919
others great thank you very much and

01:06:49,439 --> 01:06:57,150
just speak right in so you talked a bit

01:06:52,919 --> 01:06:58,559
about giving users more information on

01:06:57,150 --> 01:07:01,739
what their data is going to be used for

01:06:58,559 --> 01:07:03,959
and do you think that most users

01:07:01,739 --> 01:07:06,269
understand the implications of the data

01:07:03,959 --> 01:07:07,890
that they share to the extent that they

01:07:06,269 --> 01:07:10,890
can make informed choices about whether

01:07:07,890 --> 01:07:16,249
or not to share those data I think

01:07:10,890 --> 01:07:19,499
that's a really important point and it I

01:07:16,249 --> 01:07:20,699
think in certain context yes I'm fudging

01:07:19,499 --> 01:07:23,249
it here because I don't think it's an

01:07:20,699 --> 01:07:25,489
easy yes no I think people if if

01:07:23,249 --> 01:07:28,529
awareness has been increased if they

01:07:25,489 --> 01:07:31,229
receive or they understand or they know

01:07:28,529 --> 01:07:34,919
what the data is and where it's going I

01:07:31,229 --> 01:07:37,049
think yes I think we shouldn't you know

01:07:34,919 --> 01:07:38,819
consider that people may not understand

01:07:37,049 --> 01:07:40,949
I think it's a matter of education and

01:07:38,819 --> 01:07:42,449
increasing awareness I think which is

01:07:40,949 --> 01:07:45,259
what we're trying to do is to give

01:07:42,449 --> 01:07:47,579
people the opportunity themselves to

01:07:45,259 --> 01:07:49,380
interrogating explore data which they

01:07:47,579 --> 01:07:51,929
previously haven't had the opportunity

01:07:49,380 --> 01:07:55,819
to and we're trying to design interfaces

01:07:51,929 --> 01:08:00,059
that are natural to them for to do that

01:07:55,819 --> 01:08:02,429
so I would err on saying that if you

01:08:00,059 --> 01:08:05,009
provide people with you know good enough

01:08:02,429 --> 01:08:27,119
explanations and you're able to explain

01:08:05,009 --> 01:08:30,059
it to them they should understand it I

01:08:27,119 --> 01:08:32,159
go every day they through Camden to go

01:08:30,059 --> 01:08:33,840
to the University and as you know it's a

01:08:32,159 --> 01:08:36,030
very busy tourist area

01:08:33,840 --> 01:08:38,010
and every day I see people taking

01:08:36,030 --> 01:08:39,960
pictures for Instagram and I know I

01:08:38,010 --> 01:08:42,330
sometimes end up on some of those

01:08:39,960 --> 01:08:44,640
pictures so I was thinking like is this

01:08:42,330 --> 01:08:46,860
kind of a privacy issue is it like on

01:08:44,640 --> 01:08:51,180
the personal should be dealt on a

01:08:46,860 --> 01:08:53,280
personal basis does it make you feel

01:08:51,180 --> 01:08:55,350
uncomfortable or do you not mind I don't

01:08:53,280 --> 01:08:58,650
mind I'm just like interested in knowing

01:08:55,350 --> 01:09:00,480
like I don't mind being on a picture I'm

01:08:58,650 --> 01:09:02,240
just interested in knowing how to deal

01:09:00,480 --> 01:09:04,560
with that what do you think about that I

01:09:02,240 --> 01:09:06,480
think it's in a public place and I don't

01:09:04,560 --> 01:09:10,530
mind that you know people are taking

01:09:06,480 --> 01:09:12,120
pictures if I'm sitting on a train or

01:09:10,530 --> 01:09:13,530
something and someone points a camera at

01:09:12,120 --> 01:09:14,910
me I feel uncomfortable while they're

01:09:13,530 --> 01:09:16,590
doing that because it's been directed at

01:09:14,910 --> 01:09:18,120
me if you are just happened to be

01:09:16,590 --> 01:09:20,040
walking past when someone's taking a

01:09:18,120 --> 01:09:22,440
picture of someone else

01:09:20,040 --> 01:09:24,390
I think that's incidental and I think

01:09:22,440 --> 01:09:25,890
you know sometimes we maybe get a little

01:09:24,390 --> 01:09:27,960
Precious about our faces

01:09:25,890 --> 01:09:30,270
it depends what it's being used for so I

01:09:27,960 --> 01:09:33,780
you know if I find that I want someone's

01:09:30,270 --> 01:09:35,220
Instagram feed and I'm looking okay

01:09:33,780 --> 01:09:38,480
that's fine when I'm looking terrible

01:09:35,220 --> 01:09:38,480
there's a different matter

01:09:41,910 --> 01:09:49,260
[Music]

01:09:44,360 --> 01:09:50,640
hi Yvonne it's Joe Fisher my dad first

01:09:49,260 --> 01:09:53,580
off thank you very much for coming it's

01:09:50,640 --> 01:09:56,370
lovely to see this I wanted to ask a

01:09:53,580 --> 01:09:58,080
very specific question about some of the

01:09:56,370 --> 01:10:00,960
cross-cultural implications of the stuff

01:09:58,080 --> 01:10:06,510
that you've been talking about on one

01:10:00,960 --> 01:10:09,590
hand in the u.s. we from the point of

01:10:06,510 --> 01:10:12,360
view of Europe and UK there is no gdpr

01:10:09,590 --> 01:10:13,830
right there is no data protection on the

01:10:12,360 --> 01:10:16,170
other hand I mean and the last question

01:10:13,830 --> 01:10:18,600
I think nicely pulled us out the number

01:10:16,170 --> 01:10:20,760
of security cameras and CCTV cameras

01:10:18,600 --> 01:10:22,650
that you walk across in London is far

01:10:20,760 --> 01:10:24,900
far greater I mean I I think I may

01:10:22,650 --> 01:10:27,390
actually come from home to work without

01:10:24,900 --> 01:10:29,190
passing a single camera right like and

01:10:27,390 --> 01:10:32,790
that's probably not possible anywhere in

01:10:29,190 --> 01:10:34,470
the UK I use that right I've been

01:10:32,790 --> 01:10:37,470
wanting to write a paper for years about

01:10:34,470 --> 01:10:39,540
this winter paper which doesn't actually

01:10:37,470 --> 01:10:42,090
work in print in that I want to call it

01:10:39,540 --> 01:10:44,280
privacy or privacy across cultural

01:10:42,090 --> 01:10:46,200
analysis and of course it doesn't work

01:10:44,280 --> 01:10:47,140
when you write it down but I wanted to

01:10:46,200 --> 01:10:48,670
ask about that but

01:10:47,140 --> 01:10:51,040
kela question right what are the what

01:10:48,670 --> 01:10:52,780
are the cultural associations with these

01:10:51,040 --> 01:10:54,880
notions of data privacy and how do those

01:10:52,780 --> 01:10:56,700
change and how do you address that in

01:10:54,880 --> 01:10:59,560
the work that you're proposing to do

01:10:56,700 --> 01:11:01,480
that's a really good point and good to

01:10:59,560 --> 01:11:04,210
see you up so early in the morning I

01:11:01,480 --> 01:11:06,970
think it's there is difference I think

01:11:04,210 --> 01:11:09,100
you know when CCTV cameras arrived in

01:11:06,970 --> 01:11:14,140
the UK people were up in arms and now

01:11:09,100 --> 01:11:18,550
ten years on they just accept it whereas

01:11:14,140 --> 01:11:24,700
in Germany for example a store

01:11:18,550 --> 01:11:29,140
introduced cameras in in one of the just

01:11:24,700 --> 01:11:32,100
above one of the screens that was used

01:11:29,140 --> 01:11:35,620
to advertise a particular product and

01:11:32,100 --> 01:11:37,540
when an could then detect what someone

01:11:35,620 --> 01:11:39,640
near the dwell time and how long they

01:11:37,540 --> 01:11:42,520
were looking at particular products they

01:11:39,640 --> 01:11:45,070
were up in arms the the many of the

01:11:42,520 --> 01:11:48,670
people in Germany and so as a result of

01:11:45,070 --> 01:11:49,360
that the the Germans grocery store had

01:11:48,670 --> 01:11:52,240
to take them down

01:11:49,360 --> 01:11:54,460
I would never probably never happen in

01:11:52,240 --> 01:11:56,260
the UK so I think there are big

01:11:54,460 --> 01:11:57,460
differences and it depends on the the

01:11:56,260 --> 01:12:00,310
culture you're right as to what's

01:11:57,460 --> 01:12:02,890
considered acceptable Germany people are

01:12:00,310 --> 01:12:05,350
very concerned about where their data is

01:12:02,890 --> 01:12:07,330
going and I suspect it would be good to

01:12:05,350 --> 01:12:09,480
see you know across different cultures

01:12:07,330 --> 01:12:12,370
and why that's the case is it changing

01:12:09,480 --> 01:12:17,410
you know depending on how old you are

01:12:12,370 --> 01:12:19,330
and and why that is so I think you'll be

01:12:17,410 --> 01:12:24,940
great for you to write that paper maybe

01:12:19,330 --> 01:12:29,190
it's one that could speak but thanks for

01:12:24,940 --> 01:12:34,450
that thank you great we're gonna have to

01:12:29,190 --> 01:12:35,710
shut down for the broadcast so we'll say

01:12:34,450 --> 01:12:37,570
goodbye to our colleagues who are

01:12:35,710 --> 01:12:39,130
joining us over in Mozilla but we're

01:12:37,570 --> 01:12:42,060
going to continue taking questions here

01:12:39,130 --> 01:12:47,280
in the office so thanks for coming and

01:12:42,060 --> 01:12:47,280
yeah any more questions from the room

01:12:55,320 --> 01:13:02,440
so you talked about said I think there's

01:13:00,190 --> 01:13:05,040
some scenario in which the idea of like

01:13:02,440 --> 01:13:08,050
individual consent is insufficient so

01:13:05,040 --> 01:13:10,180
like obvious examples would be like if I

01:13:08,050 --> 01:13:11,800
decide to upload all my like contacts to

01:13:10,180 --> 01:13:15,250
Facebook there may be that affects those

01:13:11,800 --> 01:13:17,350
people and we like often see this for

01:13:15,250 --> 01:13:19,300
like genetic data where maybe you know

01:13:17,350 --> 01:13:23,110
people related to your uploading data it

01:13:19,300 --> 01:13:25,810
implies they for about use so do you

01:13:23,110 --> 01:13:29,260
have any sort of suggestions about how

01:13:25,810 --> 01:13:30,400
we can think about you know that the

01:13:29,260 --> 01:13:32,080
things you've been talking about in

01:13:30,400 --> 01:13:33,940
these scenarios where it's not really

01:13:32,080 --> 01:13:35,350
about one individual understand the

01:13:33,940 --> 01:13:39,160
impact that actually about the impact

01:13:35,350 --> 01:13:41,770
thing on like a white group and that's

01:13:39,160 --> 01:13:43,780
really hard question as to how you know

01:13:41,770 --> 01:13:46,080
I wish I could answer it on the top of

01:13:43,780 --> 01:13:48,970
my head like that but it you know so

01:13:46,080 --> 01:13:51,700
uploading I mean I think it's the

01:13:48,970 --> 01:13:55,240
question asked earlier about walking in

01:13:51,700 --> 01:13:56,650
Camden and taking photos uploading them

01:13:55,240 --> 01:13:59,520
all these other people might be involved

01:13:56,650 --> 01:14:02,140
but then someone might be doing it too

01:13:59,520 --> 01:14:04,390
specifically about a data set which is

01:14:02,140 --> 01:14:06,640
about themselves but others so for

01:14:04,390 --> 01:14:08,800
example of things we're doing in Great

01:14:06,640 --> 01:14:11,950
Ormond Street Hospital we might be able

01:14:08,800 --> 01:14:14,230
to have all the data about people who've

01:14:11,950 --> 01:14:16,030
got a particular disease and the

01:14:14,230 --> 01:14:18,340
operations that were conducted on them

01:14:16,030 --> 01:14:20,380
and it might be that some people object

01:14:18,340 --> 01:14:21,670
to that even though it's anonymize they

01:14:20,380 --> 01:14:24,430
might think well that's my personal data

01:14:21,670 --> 01:14:25,900
my son or daughter had that operation

01:14:24,430 --> 01:14:28,630
and died and I don't really want to be

01:14:25,900 --> 01:14:30,880
participating in this and so I think you

01:14:28,630 --> 01:14:33,070
know we don't know and I think this

01:14:30,880 --> 01:14:35,350
Living Lab is an opportunity to get a

01:14:33,070 --> 01:14:37,570
much wider range of people's views and

01:14:35,350 --> 01:14:41,050
opinions because all too often you're

01:14:37,570 --> 01:14:43,360
just do a survey or and there's only a

01:14:41,050 --> 01:14:45,760
small number of people who who will

01:14:43,360 --> 01:14:47,740
respond so that's part of what we're

01:14:45,760 --> 01:14:49,510
trying to do is to see whether you know

01:14:47,740 --> 01:14:51,340
you you give the negative or the

01:14:49,510 --> 01:14:53,710
positive or you give the whole range of

01:14:51,340 --> 01:14:56,230
things that might happen and see whether

01:14:53,710 --> 01:14:58,000
people think that's not acceptable that

01:14:56,230 --> 01:15:00,520
is acceptable so I think I'm methodology

01:14:58,000 --> 01:15:03,160
could be used where you know you can

01:15:00,520 --> 01:15:04,670
start to aggregate data or it's a

01:15:03,160 --> 01:15:06,680
collective data and it

01:15:04,670 --> 01:15:09,730
could affect certain people so that's a

01:15:06,680 --> 01:15:09,730

YouTube URL: https://www.youtube.com/watch?v=vEDfnn60EQk


