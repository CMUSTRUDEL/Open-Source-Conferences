Title: Emerging Technology Speaker Series - Beyond Bots and Trolls - Understanding Disinformation
Publication date: 2019-10-11
Playlist: Emerging Technologies Speaker Series
Description: 
	Beyond “Bots and Trolls” — Understanding Disinformation as Collaborative Work
Kate Starbird, University of Washington

Strategic information operations (e.g. disinformation, political propaganda, and other forms of online manipulation) are a critical concern for democratic societies—as they destabilize the “common ground” that we need to stand upon to govern ourselves. In this talk, I argue that defending against strategic information operations will require a more nuanced understanding of the problem. In particular, we will need to move beyond focusing on “bots” and “trolls” to looking at the collaborative nature of disinformation campaigns that target, infiltrate, shape, and leverage online communities. Drawing from three distinct case studies, I describe how orchestrated campaigns can become deeply entangled within “organic” online crowds and I highlight a persistent challenge for researchers, platform designers, and policy makers—distinguishing between orchestrated, explicitly-coordinated information operations and the emergent, organic behaviors of an online crowd.

Kate Starbird is an Associate Professor at the Department of Human Centered Design & Engineering (HCDE) at the University of Washington (UW). Starbird’s research is situated within human-computer interaction (HCI) and the emerging field of crisis informatics—the study of the how information-communication technologies (ICTs) are used during crisis events. One aspect of her research focuses on how online rumors spread during natural disasters and man-made crisis events. More recently, she has begun to focus on disinformation and other forms of strategic information operations online. She is a co-founder and executive council member of the UW Center for an Informed Public. Starbird earned her PhD from the University of Colorado at Boulder in Technology, Media and Society and holds a BS in Computer Science from Stanford University.
Captions: 
	00:00:02,419 --> 00:00:07,710
good morning good evening wherever you

00:00:05,250 --> 00:00:09,990
are good day my name is Joe Fresh K I'm

00:00:07,710 --> 00:00:11,790
principal research scientist in the

00:00:09,990 --> 00:00:13,679
emerging technologies team and it is my

00:00:11,790 --> 00:00:16,199
distinct pleasure today to introduce you

00:00:13,679 --> 00:00:19,650
to Kate starboard Kate is gonna be

00:00:16,199 --> 00:00:21,359
talking about bots and trolls right and

00:00:19,650 --> 00:00:22,410
going beyond these designations to

00:00:21,359 --> 00:00:24,269
really understand what it means to

00:00:22,410 --> 00:00:26,580
address these as questions and this is

00:00:24,269 --> 00:00:28,830
this topic which hits most of us

00:00:26,580 --> 00:00:31,040
interest in so many different ways

00:00:28,830 --> 00:00:33,300
we obviously have this interest around

00:00:31,040 --> 00:00:34,920
disinformation right people are using

00:00:33,300 --> 00:00:37,079
the internet and we're seeing problems

00:00:34,920 --> 00:00:39,629
with that but we've also got our own

00:00:37,079 --> 00:00:40,950
problems with BOTS and trolls and I

00:00:39,629 --> 00:00:43,440
wanted to take this opportunity to

00:00:40,950 --> 00:00:46,350
mention that for Mozilla employees and

00:00:43,440 --> 00:00:47,610
contributors we have systems in place so

00:00:46,350 --> 00:00:50,449
that if you find that you are being

00:00:47,610 --> 00:00:53,430
targeted or harassed talk to a manager

00:00:50,449 --> 00:00:54,539
ping me and we can put you in touch with

00:00:53,430 --> 00:00:57,539
people who can help you with those

00:00:54,539 --> 00:01:00,300
processes I don't think that's a total

00:00:57,539 --> 00:01:01,829
solution and as Kate will explain it's

00:01:00,300 --> 00:01:03,600
only a very small part of what a

00:01:01,829 --> 00:01:04,799
solution might look like but it does

00:01:03,600 --> 00:01:08,250
point to a way in which we take this

00:01:04,799 --> 00:01:10,500
really seriously for Mozilla ins so I'm

00:01:08,250 --> 00:01:11,970
really excited by this talk I'm really

00:01:10,500 --> 00:01:13,760
excited to introduce you to Kate whose

00:01:11,970 --> 00:01:16,200
work I've admired for a long time

00:01:13,760 --> 00:01:21,299
without any further ado Kate's darling

00:01:16,200 --> 00:01:23,100
all right thank you Joe fish yes so as

00:01:21,299 --> 00:01:24,689
you said I'm gonna talk a little bit

00:01:23,100 --> 00:01:26,520
about spots and trolls but really what I

00:01:24,689 --> 00:01:29,189
want to do is actually push our our

00:01:26,520 --> 00:01:30,710
conversation beyond BOTS and trolls and

00:01:29,189 --> 00:01:33,150
kind of think more holistically about

00:01:30,710 --> 00:01:36,810
the problems of disinformation and

00:01:33,150 --> 00:01:39,720
others in this space so fake news

00:01:36,810 --> 00:01:41,130
political propaganda information war in

00:01:39,720 --> 00:01:42,720
the last couple of years we've been

00:01:41,130 --> 00:01:44,820
introduced to a lot of new terms that

00:01:42,720 --> 00:01:46,530
attempt to describe some of the

00:01:44,820 --> 00:01:48,420
toxicities that we're perceiving in the

00:01:46,530 --> 00:01:51,180
information space and I'm not going to

00:01:48,420 --> 00:01:53,490
take all of these on today I really want

00:01:51,180 --> 00:01:54,810
to focus on on two of these terms mostly

00:01:53,490 --> 00:01:56,430
disinformation a little bit of

00:01:54,810 --> 00:01:58,590
information operations I might use that

00:01:56,430 --> 00:02:00,090
term but I'm gonna try to focus on on

00:01:58,590 --> 00:02:03,329
this term disinformation and talk a

00:02:00,090 --> 00:02:05,520
little bit about what it means but we

00:02:03,329 --> 00:02:06,899
well our lab has been studying

00:02:05,520 --> 00:02:08,610
disinformation for a few years now but

00:02:06,899 --> 00:02:11,760
we actually didn't start out to study

00:02:08,610 --> 00:02:13,700
disinformation like many online users

00:02:11,760 --> 00:02:16,610
out there we sort of stumbled down

00:02:13,700 --> 00:02:18,590
the rabbit-hole we were initially

00:02:16,610 --> 00:02:20,569
looking at actually my first line of

00:02:18,590 --> 00:02:22,190
research that kind of intersects here

00:02:20,569 --> 00:02:24,890
was we were looking at online

00:02:22,190 --> 00:02:27,410
volunteerism and online political

00:02:24,890 --> 00:02:29,540
activism but around different kinds of

00:02:27,410 --> 00:02:30,709
crisis events and and political

00:02:29,540 --> 00:02:33,080
movements and we were looking at how

00:02:30,709 --> 00:02:34,819
volunteers use online tools to organize

00:02:33,080 --> 00:02:36,860
and this is actually going to come back

00:02:34,819 --> 00:02:39,200
a little bit to how I'm gonna talk about

00:02:36,860 --> 00:02:41,330
this information later but that was my

00:02:39,200 --> 00:02:44,239
first area of research and then about

00:02:41,330 --> 00:02:47,030
2013 my collaborators and I began to

00:02:44,239 --> 00:02:49,069
study online rumors during crisis events

00:02:47,030 --> 00:02:51,140
and looked at how people were trying to

00:02:49,069 --> 00:02:52,459
make sense of crisis events and figure

00:02:51,140 --> 00:02:54,500
out what was going on and often that

00:02:52,459 --> 00:02:58,130
turned out to be rumors and in

00:02:54,500 --> 00:03:00,799
misinformation but in around 2015 and

00:02:58,130 --> 00:03:03,200
2016 we began to realize that we weren't

00:03:00,799 --> 00:03:04,670
just looking at accidental rumors and

00:03:03,200 --> 00:03:06,200
misinformation but we were actually

00:03:04,670 --> 00:03:08,510
beginning to see pervasive

00:03:06,200 --> 00:03:11,269
disinformation and not just pervasive

00:03:08,510 --> 00:03:12,470
disinformation at the margins which is

00:03:11,269 --> 00:03:13,549
where we thought it was but we've

00:03:12,470 --> 00:03:17,090
started to see that disinformation

00:03:13,549 --> 00:03:19,040
percolate up into the Twitter accounts

00:03:17,090 --> 00:03:21,920
of influencers and eventually political

00:03:19,040 --> 00:03:26,390
leaders and because we saw that kind of

00:03:21,920 --> 00:03:30,049
revelant that new revelant relevance

00:03:26,390 --> 00:03:31,190
excuse me to to disinformation we began

00:03:30,049 --> 00:03:34,940
to make that the focus of our research

00:03:31,190 --> 00:03:37,489
and so I'm gonna talk today a little bit

00:03:34,940 --> 00:03:38,989
about mostly disinformation I want to

00:03:37,489 --> 00:03:41,540
help kind of define and contextualize

00:03:38,989 --> 00:03:43,430
that term hope you help you understand

00:03:41,540 --> 00:03:45,859
how it how it works where it comes from

00:03:43,430 --> 00:03:50,329
and then give you a few examples of how

00:03:45,859 --> 00:03:51,560
online disinformation works and as I do

00:03:50,329 --> 00:03:53,630
that I want to correct some

00:03:51,560 --> 00:03:55,430
misperceptions and add some nuance to

00:03:53,630 --> 00:03:56,359
our definition of disinformation because

00:03:55,430 --> 00:03:58,459
I think that's going to be really

00:03:56,359 --> 00:04:00,530
important I'm also going to discuss some

00:03:58,459 --> 00:04:02,569
technology and policy implications and I

00:04:00,530 --> 00:04:04,069
hope this number five is a hope and I'm

00:04:02,569 --> 00:04:06,620
not sure I'm going to achieve it is to

00:04:04,069 --> 00:04:09,590
end on a little less hopeless note I

00:04:06,620 --> 00:04:10,579
have been giving talks for a couple

00:04:09,590 --> 00:04:13,609
years now that have been really

00:04:10,579 --> 00:04:15,530
depressing and I am on sabbatical right

00:04:13,609 --> 00:04:17,479
now is telling Joe fish that I'm not

00:04:15,530 --> 00:04:19,970
feeling as depressed myself perhaps

00:04:17,479 --> 00:04:21,799
because I'm well slept but I'm also

00:04:19,970 --> 00:04:25,130
thinking that there is sort of a turning

00:04:21,799 --> 00:04:26,620
point of us kind of starting to think

00:04:25,130 --> 00:04:28,090
about

00:04:26,620 --> 00:04:30,490
potential solutions or not just

00:04:28,090 --> 00:04:31,870
solutions but kind of a more hopeful

00:04:30,490 --> 00:04:33,639
future even though we know that there's

00:04:31,870 --> 00:04:35,830
there's a problem here with online

00:04:33,639 --> 00:04:38,139
misinformation disinformation and other

00:04:35,830 --> 00:04:39,550
things so the first thing I want to do

00:04:38,139 --> 00:04:41,380
is distinguish between these two terms

00:04:39,550 --> 00:04:42,220
and I think this is an important place

00:04:41,380 --> 00:04:43,449
to start

00:04:42,220 --> 00:04:44,949
between misinformation and

00:04:43,449 --> 00:04:46,690
disinformation the differences between

00:04:44,949 --> 00:04:48,070
the two are actually really important

00:04:46,690 --> 00:04:50,050
when we start to talk about solutions

00:04:48,070 --> 00:04:50,560
and so I think it's a good place to

00:04:50,050 --> 00:04:52,180
start

00:04:50,560 --> 00:04:54,430
so one high-level way to characterize a

00:04:52,180 --> 00:04:57,190
distinction between the two is one of

00:04:54,430 --> 00:04:58,870
intent simply intent misinformation is

00:04:57,190 --> 00:05:00,729
information that is false but not

00:04:58,870 --> 00:05:03,130
necessarily deliberately so and

00:05:00,729 --> 00:05:05,560
disinformation is intentionally false

00:05:03,130 --> 00:05:06,880
information and this is a pretty working

00:05:05,560 --> 00:05:08,199
definition this is the one I give to my

00:05:06,880 --> 00:05:10,449
students that's the one I start out with

00:05:08,199 --> 00:05:12,880
but actually in another view

00:05:10,449 --> 00:05:15,340
disinformation isn't simply about intent

00:05:12,880 --> 00:05:17,199
the term disinformation has other more

00:05:15,340 --> 00:05:20,350
specific meanings at a historical

00:05:17,199 --> 00:05:24,490
context it's been connected to this term

00:05:20,350 --> 00:05:27,729
des informat SIA which is which

00:05:24,490 --> 00:05:29,380
originated in Soviet intelligence and it

00:05:27,729 --> 00:05:31,240
has kind of a history of a certain kind

00:05:29,380 --> 00:05:32,830
of political propaganda our

00:05:31,240 --> 00:05:34,570
understanding of disinformation in our

00:05:32,830 --> 00:05:36,220
lab and actually in the community that's

00:05:34,570 --> 00:05:38,650
building around these studies is

00:05:36,220 --> 00:05:40,750
informed by the work of Ladislav Bittman

00:05:38,650 --> 00:05:42,849
who was a practitioner of Soviet of

00:05:40,750 --> 00:05:44,650
Soviet disinformation he was a member of

00:05:42,849 --> 00:05:47,200
the Czech intelligence and he defected

00:05:44,650 --> 00:05:49,630
the United States in 1968 and became a

00:05:47,200 --> 00:05:52,570
professor of disinformation and then he

00:05:49,630 --> 00:05:53,950
wrote a lot about his experiences as a

00:05:52,570 --> 00:05:56,139
in his understanding of disinformation

00:05:53,950 --> 00:05:58,180
as a practitioner and we've been

00:05:56,139 --> 00:06:00,039
borrowing from bitman to define

00:05:58,180 --> 00:06:02,410
disinformation as a suite of tactics

00:06:00,039 --> 00:06:04,810
applied by influence agents to

00:06:02,410 --> 00:06:08,380
manipulate information spaces to achieve

00:06:04,810 --> 00:06:09,940
strategic political objectives it's

00:06:08,380 --> 00:06:11,590
important to note and I'm going to

00:06:09,940 --> 00:06:13,510
stress this that not all this

00:06:11,590 --> 00:06:15,940
information historical or modern is

00:06:13,510 --> 00:06:18,130
connected to Russia in fact right now in

00:06:15,940 --> 00:06:19,780
2020 election I think we're gonna really

00:06:18,130 --> 00:06:21,870
be looking at domestic actors and really

00:06:19,780 --> 00:06:24,639
and that's gonna be really important but

00:06:21,870 --> 00:06:26,320
Russian intelligence has been or Soviet

00:06:24,639 --> 00:06:27,580
intelligence and now Russia have been

00:06:26,320 --> 00:06:29,139
innovators in the space of

00:06:27,580 --> 00:06:31,090
disinformation I think it's important to

00:06:29,139 --> 00:06:32,620
appreciate how they work and what

00:06:31,090 --> 00:06:34,750
they've done and so I'm gonna present

00:06:32,620 --> 00:06:36,729
three case studies of recent online

00:06:34,750 --> 00:06:38,249
disinformation two with very strong

00:06:36,729 --> 00:06:40,289
connections to Russia's

00:06:38,249 --> 00:06:41,609
apparatus and the other with more

00:06:40,289 --> 00:06:43,619
tenuous connections but they do

00:06:41,609 --> 00:06:44,879
intersect in interesting ways not just

00:06:43,619 --> 00:06:46,919
with Russian disinformation but

00:06:44,879 --> 00:06:49,139
disinformation from other nation-states

00:06:46,919 --> 00:06:50,729
these are long-term studies that took

00:06:49,139 --> 00:06:54,389
place over the course of multiple years

00:06:50,729 --> 00:06:56,699
within our lab before I go too far in I

00:06:54,389 --> 00:06:58,589
want to do a little methods interlude I

00:06:56,699 --> 00:07:02,669
think the methods that we use are

00:06:58,589 --> 00:07:04,109
probably the most important piece of our

00:07:02,669 --> 00:07:06,479
research contribution to this growing

00:07:04,109 --> 00:07:07,859
field in this study I don't think that

00:07:06,479 --> 00:07:09,449
I'd be standing here today giving a talk

00:07:07,859 --> 00:07:10,949
like this if it wasn't for the methods

00:07:09,449 --> 00:07:14,309
that we've been using for several years

00:07:10,949 --> 00:07:16,589
we are our methods are profoundly mixed

00:07:14,309 --> 00:07:18,659
method we conduct iterative qualitative

00:07:16,589 --> 00:07:20,849
and quantitative interpretive analysis

00:07:18,659 --> 00:07:24,029
of big social data and other online

00:07:20,849 --> 00:07:25,799
content we dive into data at different

00:07:24,029 --> 00:07:27,749
levels we're not afraid to read

00:07:25,799 --> 00:07:30,089
thousands and thousands of tweets and

00:07:27,749 --> 00:07:31,649
look at accounts and then we also step

00:07:30,089 --> 00:07:33,779
back and visualize data in different

00:07:31,649 --> 00:07:36,569
ways to look at patterns identify

00:07:33,779 --> 00:07:37,949
anomalies develop hypotheses what's

00:07:36,569 --> 00:07:39,869
going on there I think it might be this

00:07:37,949 --> 00:07:41,279
more often than not we go in and find if

00:07:39,869 --> 00:07:43,409
nothing's going on in there it's not

00:07:41,279 --> 00:07:45,059
what we thought but the times we go in

00:07:43,409 --> 00:07:47,369
and find really interesting things and

00:07:45,059 --> 00:07:48,629
so it's our very close engagement with

00:07:47,369 --> 00:07:50,819
the data and our movement across

00:07:48,629 --> 00:07:52,829
different kind of scales that I think

00:07:50,819 --> 00:07:55,019
has been really important for us in

00:07:52,829 --> 00:08:00,809
terms of understanding what what what

00:07:55,019 --> 00:08:02,069
this phenomena is to some extent the the

00:08:00,809 --> 00:08:03,659
cases I'm going to prove it's it is that

00:08:02,069 --> 00:08:05,249
here are all cross platform

00:08:03,659 --> 00:08:08,969
disinformation is profoundly like

00:08:05,249 --> 00:08:10,199
multi-channel but we often start with

00:08:08,969 --> 00:08:12,299
Twitter data because that's what we have

00:08:10,199 --> 00:08:14,159
and that's what we've had this is this

00:08:12,299 --> 00:08:15,029
is changing a little bit currently we're

00:08:14,159 --> 00:08:17,219
starting to get access to other

00:08:15,029 --> 00:08:19,499
platforms but all of our case studies

00:08:17,219 --> 00:08:22,319
begin with Twitter data not I can talk

00:08:19,499 --> 00:08:23,579
about that in the questions I'm also

00:08:22,319 --> 00:08:27,299
going to apply sort of a socio-technical

00:08:23,579 --> 00:08:29,489
lens to this which is kind of comes from

00:08:27,299 --> 00:08:30,919
my experience and commit in my field of

00:08:29,489 --> 00:08:33,899
computer support of cooperative work

00:08:30,919 --> 00:08:35,459
where it's also kind of intersecting

00:08:33,899 --> 00:08:39,389
with the methods that we use is we're

00:08:35,459 --> 00:08:41,879
kind of looking at how social actions

00:08:39,389 --> 00:08:43,949
taken in the world are shaped by norms

00:08:41,879 --> 00:08:46,920
and they're shaped by two technologies

00:08:43,949 --> 00:08:48,209
and also those actions shape those norms

00:08:46,920 --> 00:08:51,520
and in some ways shaped those

00:08:48,209 --> 00:08:54,460
technological or socio technological

00:08:51,520 --> 00:08:56,020
and I think this kind of perspective on

00:08:54,460 --> 00:08:59,050
how these things work together gets us

00:08:56,020 --> 00:09:00,760
away from simplistic ideas of like oh we

00:08:59,050 --> 00:09:02,650
have this disinformation campaign and

00:09:00,760 --> 00:09:04,630
we're going to measure the outcomes and

00:09:02,650 --> 00:09:06,160
you know they heard this message and

00:09:04,630 --> 00:09:07,060
they voted this way or didn't vote this

00:09:06,160 --> 00:09:08,320
way and that's how we're going to

00:09:07,060 --> 00:09:10,480
measure things I think it's a much more

00:09:08,320 --> 00:09:12,940
complex system than that and and that's

00:09:10,480 --> 00:09:14,890
how how we see it and that's how I hope

00:09:12,940 --> 00:09:17,770
to be kind of communicating some of that

00:09:14,890 --> 00:09:20,530
nuance in this talk so the first case

00:09:17,770 --> 00:09:22,840
study I want to focus on is the case of

00:09:20,530 --> 00:09:26,920
Russian interference in the 2016 US

00:09:22,840 --> 00:09:28,600
election and this in this research study

00:09:26,920 --> 00:09:30,160
we've done several different papers on

00:09:28,600 --> 00:09:31,420
this in this research study we actually

00:09:30,160 --> 00:09:33,430
didn't start out looking at

00:09:31,420 --> 00:09:35,590
disinformation we actually started out

00:09:33,430 --> 00:09:38,380
studying framing contests in black lives

00:09:35,590 --> 00:09:40,210
matter discourse I had a student who was

00:09:38,380 --> 00:09:44,230
really interested in how the black lives

00:09:40,210 --> 00:09:46,540
matter movement was organizing and he

00:09:44,230 --> 00:09:48,370
wanted to kind of under understand how

00:09:46,540 --> 00:09:50,050
they were kind of bringing resources to

00:09:48,370 --> 00:09:51,340
bear and so we we were looking at that

00:09:50,050 --> 00:09:53,640
conversation and then we started

00:09:51,340 --> 00:09:57,130
noticing sort of a counter conversation

00:09:53,640 --> 00:09:58,900
and this is a bit sort of a very highly

00:09:57,130 --> 00:10:00,730
polarized conversation and we began to

00:09:58,900 --> 00:10:02,650
study study that kind of polarization

00:10:00,730 --> 00:10:05,950
and the conversations there so this is

00:10:02,650 --> 00:10:08,230
from data related to shooting events and

00:10:05,950 --> 00:10:09,550
in 2016 we generated a data set of

00:10:08,230 --> 00:10:12,160
tweets that have you there black lives

00:10:09,550 --> 00:10:13,990
matter or blue lives matter in it and we

00:10:12,160 --> 00:10:18,520
created a structural graph of that data

00:10:13,990 --> 00:10:21,640
and so this graph has in this graph

00:10:18,520 --> 00:10:23,800
every little node or circle there is a

00:10:21,640 --> 00:10:24,970
is an account and those accounts are

00:10:23,800 --> 00:10:26,740
clustered together it's a retweet

00:10:24,970 --> 00:10:28,030
retweet Network graph the accounts are

00:10:26,740 --> 00:10:29,650
clustered together their closer together

00:10:28,030 --> 00:10:31,600
if they retweeted each other and they're

00:10:29,650 --> 00:10:33,100
further apart if they didn't the edges

00:10:31,600 --> 00:10:35,020
are missing we remove the edges here

00:10:33,100 --> 00:10:37,300
because it gets really ugly but I'll

00:10:35,020 --> 00:10:38,530
show you some edges later but so just

00:10:37,300 --> 00:10:40,150
the edges are invisible but they're

00:10:38,530 --> 00:10:42,100
they're pulling accounts closer together

00:10:40,150 --> 00:10:44,170
if they retweeted each other and what

00:10:42,100 --> 00:10:46,540
you can see here is two different

00:10:44,170 --> 00:10:47,770
separate communities or echo chambers we

00:10:46,540 --> 00:10:49,600
can talk about in a different way as

00:10:47,770 --> 00:10:50,890
polarization but two different

00:10:49,600 --> 00:10:53,080
communities on either side of the

00:10:50,890 --> 00:10:55,320
conversation we have probe black lives

00:10:53,080 --> 00:10:55,320
matter

00:10:55,600 --> 00:11:00,710
accounts on the left and it was also

00:10:58,040 --> 00:11:02,330
politically left and then anti black

00:11:00,710 --> 00:11:04,640
lives matter on the right which was also

00:11:02,330 --> 00:11:06,260
sort of politically right and the two

00:11:04,640 --> 00:11:08,470
the two sides created and spread very

00:11:06,260 --> 00:11:11,690
different frames about police shootings

00:11:08,470 --> 00:11:13,550
of african-american citizens and so we

00:11:11,690 --> 00:11:14,990
were looking at this we studied it we

00:11:13,550 --> 00:11:20,030
wrote this paper we published that paper

00:11:14,990 --> 00:11:23,180
in October 2017 and a couple weeks after

00:11:20,030 --> 00:11:25,070
we published it in November 2017

00:11:23,180 --> 00:11:26,900
Twitter released a list of accounts

00:11:25,070 --> 00:11:28,940
associated with the Russia with Russia's

00:11:26,900 --> 00:11:31,070
Internet research agency which had been

00:11:28,940 --> 00:11:32,930
running disinformation operations online

00:11:31,070 --> 00:11:35,120
during that time targeting us

00:11:32,930 --> 00:11:36,500
populations just had another US Senate

00:11:35,120 --> 00:11:39,290
report about this came out this week

00:11:36,500 --> 00:11:43,430
this is sort of an established and

00:11:39,290 --> 00:11:44,900
established phenomenon so when Twitter

00:11:43,430 --> 00:11:46,730
releases this list is the first time we

00:11:44,900 --> 00:11:49,010
have a list of these are known I owe a

00:11:46,730 --> 00:11:50,240
accounts we actually I've looked over

00:11:49,010 --> 00:11:51,830
the list I was like holy-moly I

00:11:50,240 --> 00:11:53,930
recognize some of these accounts in the

00:11:51,830 --> 00:11:55,370
list we'd featured some of the accounts

00:11:53,930 --> 00:11:57,860
in our earlier paper they were in

00:11:55,370 --> 00:11:59,090
excerpts they were in tables we they

00:11:57,860 --> 00:12:02,030
have been some of the most influential

00:11:59,090 --> 00:12:04,280
accounts in the conversation so I had my

00:12:02,030 --> 00:12:06,890
students I said can you go can you go

00:12:04,280 --> 00:12:08,810
run those accounts over this graph to

00:12:06,890 --> 00:12:13,340
see where the Russians the Russians

00:12:08,810 --> 00:12:14,780
trolls were in this conversation and so

00:12:13,340 --> 00:12:16,190
the troll accounts here are mapped in

00:12:14,780 --> 00:12:18,230
orange and here we show sort of the

00:12:16,190 --> 00:12:21,140
edges are retweets of their accounts in

00:12:18,230 --> 00:12:22,970
orange as well and so you can see that

00:12:21,140 --> 00:12:24,800
the Russian information operations or

00:12:22,970 --> 00:12:26,420
disinformation agents were active on

00:12:24,800 --> 00:12:28,610
both sides of the black lives matter

00:12:26,420 --> 00:12:30,560
conversation a few were among the most

00:12:28,610 --> 00:12:32,900
influential accounts in the conversation

00:12:30,560 --> 00:12:38,360
one of the accounts on they left was

00:12:32,900 --> 00:12:40,070
retweeted by Jack and me so several

00:12:38,360 --> 00:12:41,810
accounts on the right had integrated

00:12:40,070 --> 00:12:43,400
into other grassroots or online

00:12:41,810 --> 00:12:45,830
organizing efforts on the conservative

00:12:43,400 --> 00:12:46,940
side they were following those accounts

00:12:45,830 --> 00:12:49,300
those accounts were following them

00:12:46,940 --> 00:12:52,370
they'd become part of these sort of

00:12:49,300 --> 00:12:54,530
follow back groups on the right that are

00:12:52,370 --> 00:12:56,840
very good at organizing I'd love to talk

00:12:54,530 --> 00:12:59,930
about that another time as well so that

00:12:56,840 --> 00:13:03,230
they were in both sides so my PhD

00:12:59,930 --> 00:13:04,850
student Emmer Arief conducted in-depth

00:13:03,230 --> 00:13:06,500
qualitative research on the accounts in

00:13:04,850 --> 00:13:07,230
Orange he found that the IRA accounts

00:13:06,500 --> 00:13:09,900
enacted

00:13:07,230 --> 00:13:12,210
multi-dimensional online personas across

00:13:09,900 --> 00:13:14,580
platforms that played on stereotypes of

00:13:12,210 --> 00:13:17,070
african-americans on one side and white

00:13:14,580 --> 00:13:19,080
can us conservatives on the other they

00:13:17,070 --> 00:13:21,690
were impersonating activists and also

00:13:19,080 --> 00:13:23,340
modeling for others what online activism

00:13:21,690 --> 00:13:25,380
looked like they're shaping the

00:13:23,340 --> 00:13:27,870
information space they were reflecting

00:13:25,380 --> 00:13:30,330
norms but also to some extent shaping

00:13:27,870 --> 00:13:32,430
them often their content wasn't

00:13:30,330 --> 00:13:34,020
superficially problematic or any

00:13:32,430 --> 00:13:35,670
different from what others others in the

00:13:34,020 --> 00:13:38,760
space were doing they were tweeting

00:13:35,670 --> 00:13:40,560
about strong black voices on the left or

00:13:38,760 --> 00:13:42,480
supportive us veterans on the right

00:13:40,560 --> 00:13:44,880
they were cultivating audiences for

00:13:42,480 --> 00:13:46,770
strategic future strategic messaging in

00:13:44,880 --> 00:13:48,510
other places they were sowing an

00:13:46,770 --> 00:13:49,980
amplifying division some of their

00:13:48,510 --> 00:13:52,260
content was among the most vitriolic

00:13:49,980 --> 00:13:53,970
content in the space advocating for

00:13:52,260 --> 00:13:57,030
violence against police on the left and

00:13:53,970 --> 00:13:58,350
using racial epithets on the right in a

00:13:57,030 --> 00:14:00,330
few places we can see them holding

00:13:58,350 --> 00:14:02,130
arguments with themselves like a

00:14:00,330 --> 00:14:03,930
puppeteer having one of their accounts

00:14:02,130 --> 00:14:08,130
on the right fight with one of their

00:14:03,930 --> 00:14:11,220
accounts on the left we now know a lot

00:14:08,130 --> 00:14:13,320
more about this activity a year after

00:14:11,220 --> 00:14:14,760
Twitter released the first list of

00:14:13,320 --> 00:14:16,410
accounts they actually released all of

00:14:14,760 --> 00:14:17,580
the data from all of the accounts that

00:14:16,410 --> 00:14:20,310
were that they had found to be

00:14:17,580 --> 00:14:22,650
associated with the IRA and they're sort

00:14:20,310 --> 00:14:24,510
of in their project sunlight and we were

00:14:22,650 --> 00:14:26,580
able to take that data and analyze it

00:14:24,510 --> 00:14:28,260
and this is actually what was

00:14:26,580 --> 00:14:29,910
interesting for us is that we figured

00:14:28,260 --> 00:14:31,650
out that what we had found in that two

00:14:29,910 --> 00:14:33,630
sides of that conversation was a

00:14:31,650 --> 00:14:35,340
significant part of their operations in

00:14:33,630 --> 00:14:38,790
fact it was Moke we had found evidence

00:14:35,340 --> 00:14:41,850
of most of their operation a window into

00:14:38,790 --> 00:14:44,640
a bigger a bigger set but this was a

00:14:41,850 --> 00:14:46,770
core part of their operation so this is

00:14:44,640 --> 00:14:49,770
a retweet Network graph kind of a

00:14:46,770 --> 00:14:53,220
similar property here of the IRA

00:14:49,770 --> 00:14:54,990
accounts that and that retweeted each

00:14:53,220 --> 00:14:56,760
other so these are just a array accounts

00:14:54,990 --> 00:14:58,680
to show where they were amplifying each

00:14:56,760 --> 00:15:00,510
other and it actually reveals four

00:14:58,680 --> 00:15:02,880
clusters of different kinds of accounts

00:15:00,510 --> 00:15:03,810
that they used so they had these you

00:15:02,880 --> 00:15:06,720
might have heard about this they

00:15:03,810 --> 00:15:09,060
impersonated news accounts in the US

00:15:06,720 --> 00:15:13,650
including one that was in Seattle that

00:15:09,060 --> 00:15:16,620
was looked very much like a real a real

00:15:13,650 --> 00:15:18,540
group there so they had a bunch of like

00:15:16,620 --> 00:15:19,329
fake news accounts literally they were

00:15:18,540 --> 00:15:22,600
fake

00:15:19,329 --> 00:15:23,799
accounts a group of hashtag warriors the

00:15:22,600 --> 00:15:25,600
way they play these little games to try

00:15:23,799 --> 00:15:29,110
to get people to engage with them and

00:15:25,600 --> 00:15:31,119
then to politicized persona groups one

00:15:29,110 --> 00:15:32,799
on the US right and one in sort of the

00:15:31,119 --> 00:15:34,779
african-american community or black

00:15:32,799 --> 00:15:36,670
community or african-american activists

00:15:34,779 --> 00:15:40,629
up there in the blue so these are the

00:15:36,670 --> 00:15:43,029
core part of I mean this this is most of

00:15:40,629 --> 00:15:44,559
their their successful operations and

00:15:43,029 --> 00:15:45,879
I'm gonna kind of talk to you a little

00:15:44,559 --> 00:15:47,199
bit about that this is new stuff I

00:15:45,879 --> 00:15:50,049
haven't published on it but I have

00:15:47,199 --> 00:15:51,699
started to give a few talks on it so if

00:15:50,049 --> 00:15:55,569
we look at the basic stats around these

00:15:51,699 --> 00:15:57,850
clusters so the cluster in blue these

00:15:55,569 --> 00:15:59,139
are the accounts we're talking about as

00:15:57,850 --> 00:16:00,639
being associated with the African

00:15:59,139 --> 00:16:02,980
American community they actually use

00:16:00,639 --> 00:16:05,739
terms like black community black to live

00:16:02,980 --> 00:16:07,629
black to vist and some other terms there

00:16:05,739 --> 00:16:10,540
these are hashtags that were in their

00:16:07,629 --> 00:16:12,610
profiles and the US right you can see

00:16:10,540 --> 00:16:15,309
things like conservative Tea Co T which

00:16:12,610 --> 00:16:18,309
is an old hashtag means top conservative

00:16:15,309 --> 00:16:20,079
on Twitter wake up America PJ net which

00:16:18,309 --> 00:16:22,360
is a fascinating thing about how

00:16:20,079 --> 00:16:23,949
conservatives organize online so these

00:16:22,360 --> 00:16:26,679
are accounts these are the terms that

00:16:23,949 --> 00:16:28,989
were prominent in the IRA troll accounts

00:16:26,679 --> 00:16:30,610
that were in these two clusters and so

00:16:28,989 --> 00:16:32,259
you can kind of get a sense of who they

00:16:30,610 --> 00:16:34,419
were are and what they were trying to do

00:16:32,259 --> 00:16:36,489
similar to those images I showed you

00:16:34,419 --> 00:16:38,980
earlier so what was interesting is that

00:16:36,489 --> 00:16:40,419
we now know the IRA been using troll

00:16:38,980 --> 00:16:42,429
accounts to participate in and try to

00:16:40,419 --> 00:16:45,220
manipulate us discourse in the US and

00:16:42,429 --> 00:16:47,139
elsewhere since maybe 2010 but most of

00:16:45,220 --> 00:16:49,509
those efforts had very little impact

00:16:47,139 --> 00:16:52,569
this is actually a graph of their tweets

00:16:49,509 --> 00:16:54,279
over time and their you know they'd

00:16:52,569 --> 00:16:55,959
spike at different times most of this is

00:16:54,279 --> 00:16:58,629
really spammy and a lot of it was

00:16:55,959 --> 00:17:00,309
automated especially early but if you

00:16:58,629 --> 00:17:01,809
look at this I'm gonna that's been

00:17:00,309 --> 00:17:04,149
struck and I'm gonna overlay the graph

00:17:01,809 --> 00:17:05,829
of their reach what we count per day you

00:17:04,149 --> 00:17:07,449
can see that they actually you know

00:17:05,829 --> 00:17:08,889
initially don't get any retweets no

00:17:07,449 --> 00:17:09,669
one's following them no one's paying

00:17:08,889 --> 00:17:15,250
attention to them

00:17:09,669 --> 00:17:18,429
it's how about July 2015 and the July

00:17:15,250 --> 00:17:20,439
2015 what they did was they started to

00:17:18,429 --> 00:17:22,779
establish those personas that we talked

00:17:20,439 --> 00:17:24,850
about those us right-leaning personas

00:17:22,779 --> 00:17:26,980
and those African American black lives

00:17:24,850 --> 00:17:29,500
matter activists personas on the left

00:17:26,980 --> 00:17:32,110
and it was when they started to develop

00:17:29,500 --> 00:17:34,170
those personas that there that there

00:17:32,110 --> 00:17:36,429
content started to take off in the US

00:17:34,170 --> 00:17:39,130
that's when they started to get retweets

00:17:36,429 --> 00:17:40,870
and in fact when you look at this this

00:17:39,130 --> 00:17:43,270
graph here most of their retweets

00:17:40,870 --> 00:17:47,080
are from a very small number of accounts

00:17:43,270 --> 00:17:49,299
it's about 300 150 on one side and 150

00:17:47,080 --> 00:17:50,679
on the other side and that's almost all

00:17:49,299 --> 00:17:55,830
of their retweets of all of their

00:17:50,679 --> 00:17:58,540
operations to give you a little better

00:17:55,830 --> 00:18:02,380
different view of this these are kind of

00:17:58,540 --> 00:18:05,620
the count clusters that we had they had

00:18:02,380 --> 00:18:06,640
about like so 134 accounts that we

00:18:05,620 --> 00:18:09,760
clustered with the black lives matter

00:18:06,640 --> 00:18:11,830
activists 154 we clicked we clustered

00:18:09,760 --> 00:18:13,150
with the right side and they sent almost

00:18:11,830 --> 00:18:15,370
the same a number of tweets so they was

00:18:13,150 --> 00:18:16,840
like very parallel operations they were

00:18:15,370 --> 00:18:21,520
a little more successful on the right

00:18:16,840 --> 00:18:23,740
but that's not that's not here there and

00:18:21,520 --> 00:18:25,210
interestingly though they were they were

00:18:23,740 --> 00:18:29,380
yelling at each other they were often

00:18:25,210 --> 00:18:31,030
you know sowing and amplifying division

00:18:29,380 --> 00:18:32,350
these kinds of things they also they

00:18:31,030 --> 00:18:34,720
converge in a couple of different ways

00:18:32,350 --> 00:18:36,429
that converged to attack media and they

00:18:34,720 --> 00:18:38,380
converged in support of Donald Trump's

00:18:36,429 --> 00:18:40,059
presidency on the right they were very

00:18:38,380 --> 00:18:41,650
active they said this go vote for Trump

00:18:40,059 --> 00:18:44,370
and on the left they tried to encourage

00:18:41,650 --> 00:18:48,190
encourage people not to vote at all

00:18:44,370 --> 00:18:49,900
people that were Democrat so stepping

00:18:48,190 --> 00:18:51,640
back we can look at this as a multi-year

00:18:49,900 --> 00:18:53,169
campaign to infiltrate in shape

00:18:51,640 --> 00:18:55,179
political discourse in the United States

00:18:53,169 --> 00:18:56,980
it was orchestrated by a group with ties

00:18:55,179 --> 00:18:58,780
to the Russian government and we can see

00:18:56,980 --> 00:19:02,620
them burrowing into and working with in

00:18:58,780 --> 00:19:04,000
organic online communities we can talk

00:19:02,620 --> 00:19:05,410
about this also as highly orchestrated

00:19:04,000 --> 00:19:06,580
and I'm going to come back to this

00:19:05,410 --> 00:19:07,929
because the other examples that I'm

00:19:06,580 --> 00:19:09,549
gonna give are not this is highly

00:19:07,929 --> 00:19:10,780
orchestrated behavior this is a group

00:19:09,549 --> 00:19:11,650
they're organized they're inside a

00:19:10,780 --> 00:19:13,750
building they're working together

00:19:11,650 --> 00:19:14,980
they're given some kind of directions

00:19:13,750 --> 00:19:16,809
everyday of what they're supposed to be

00:19:14,980 --> 00:19:18,820
tweeting about and they're and they're

00:19:16,809 --> 00:19:22,150
trying to infiltrate and shape these

00:19:18,820 --> 00:19:24,160
conversations so the second case that

00:19:22,150 --> 00:19:25,870
some of shift now the second case study

00:19:24,160 --> 00:19:28,600
looks at disinformation during armed

00:19:25,870 --> 00:19:29,590
conflict this is a really hard case

00:19:28,600 --> 00:19:33,220
study and I'm just going to acknowledge

00:19:29,590 --> 00:19:35,350
that right right off since 2017 we've

00:19:33,220 --> 00:19:38,710
been looking at online discourse related

00:19:35,350 --> 00:19:40,030
to the civil war in Syria in particular

00:19:38,710 --> 00:19:41,980
we've been focusing on discourse

00:19:40,030 --> 00:19:44,020
surrounding the work of a serious civil

00:19:41,980 --> 00:19:45,850
defense also known as the white helmets

00:19:44,020 --> 00:19:47,530
it's a volunteer rescue group

00:19:45,850 --> 00:19:49,419
that works in rebel-held areas of Syria

00:19:47,530 --> 00:19:50,950
they do search-and-rescue they provide

00:19:49,419 --> 00:19:52,750
medical aid to people that are impacted

00:19:50,950 --> 00:19:53,860
by the Civil War there again it's very

00:19:52,750 --> 00:19:55,539
important to note they work in the

00:19:53,860 --> 00:19:56,830
rebel-held area so they're not working

00:19:55,539 --> 00:20:00,610
in the areas that are controlled by the

00:19:56,830 --> 00:20:03,880
Syrian regime the white helmets also

00:20:00,610 --> 00:20:06,520
document these impacts in in this case

00:20:03,880 --> 00:20:08,230
of this tweet here specifically calling

00:20:06,520 --> 00:20:09,429
out the Syrian government's Russian

00:20:08,230 --> 00:20:12,100
allies and claiming that they

00:20:09,429 --> 00:20:16,270
specifically target aid workers we also

00:20:12,100 --> 00:20:19,090
have theirs they also have videos of the

00:20:16,270 --> 00:20:22,510
impacts of air attacks from from Russian

00:20:19,090 --> 00:20:25,690
planes in similar kinds of things and so

00:20:22,510 --> 00:20:27,070
this content that they've shared has

00:20:25,690 --> 00:20:28,690
served to some extent to garner

00:20:27,070 --> 00:20:30,669
solidarity with and sympathy for the

00:20:28,690 --> 00:20:33,520
Syrian people from Western audiences in

00:20:30,669 --> 00:20:35,049
particular in about 2016 there was a

00:20:33,520 --> 00:20:37,690
documentary about the white helmets and

00:20:35,049 --> 00:20:39,730
their content they won an Oscar for the

00:20:37,690 --> 00:20:42,429
documentary and their content was

00:20:39,730 --> 00:20:44,860
actually beginning to change some

00:20:42,429 --> 00:20:47,110
conversations in the West and and this

00:20:44,860 --> 00:20:48,400
is about when they became targeted very

00:20:47,110 --> 00:20:53,350
specifically by a disinformation

00:20:48,400 --> 00:20:55,270
campaign in 2017 2018 when we started

00:20:53,350 --> 00:20:57,159
studying this conversation if you search

00:20:55,270 --> 00:20:58,720
Twitter or Google for white helmets you

00:20:57,159 --> 00:21:01,179
may have found some tweets supporting

00:20:58,720 --> 00:21:03,549
them but most likely you would have

00:21:01,179 --> 00:21:05,919
encountered content like this content

00:21:03,549 --> 00:21:07,299
that challenges the white helmets plenty

00:21:05,919 --> 00:21:09,220
that they're propagating a construct of

00:21:07,299 --> 00:21:14,020
the West equating them with terrorists

00:21:09,220 --> 00:21:16,900
calling them crisis actors in our

00:21:14,020 --> 00:21:19,030
research on this conversation said we've

00:21:16,900 --> 00:21:20,919
been we've been doing it for years and

00:21:19,030 --> 00:21:23,230
we still have open studies here

00:21:20,919 --> 00:21:25,630
we began with a collection of tweets

00:21:23,230 --> 00:21:27,760
around the white helmets this is one

00:21:25,630 --> 00:21:29,110
representation of that data in the

00:21:27,760 --> 00:21:31,360
center of this graph is a retweet

00:21:29,110 --> 00:21:32,470
network graph again this is the last

00:21:31,360 --> 00:21:33,909
time I'll show you one of those I'll

00:21:32,470 --> 00:21:36,490
show you other kinds of Grasse little

00:21:33,909 --> 00:21:38,020
graphs later but so again this is a

00:21:36,490 --> 00:21:41,230
retweet net Network graph in this case

00:21:38,020 --> 00:21:43,480
the nodes are sized by the number of

00:21:41,230 --> 00:21:45,190
retweets they receive and they are

00:21:43,480 --> 00:21:47,320
clustered together and they're attached

00:21:45,190 --> 00:21:49,299
by edges when someone retweet when one

00:21:47,320 --> 00:21:51,970
the count retweets another again you can

00:21:49,299 --> 00:21:55,630
see a polarized conversation and in this

00:21:51,970 --> 00:21:58,210
case we have pro-white health's I'm a

00:21:55,630 --> 00:22:00,040
pro white helmets content on the left

00:21:58,210 --> 00:22:01,420
in effect serious civil defense is the

00:22:00,040 --> 00:22:03,850
official account of the white helmets

00:22:01,420 --> 00:22:05,680
and then on the right you have anti

00:22:03,850 --> 00:22:08,410
white helmets content it's persistently

00:22:05,680 --> 00:22:10,510
consistently the kind of content that I

00:22:08,410 --> 00:22:14,260
showed you above typically I would not

00:22:10,510 --> 00:22:22,480
show a lot of the names of certain

00:22:14,260 --> 00:22:24,580
accounts there if people are thinking

00:22:22,480 --> 00:22:27,220
that they're anonymous and but these

00:22:24,580 --> 00:22:28,960
accounts that you can see here are it if

00:22:27,220 --> 00:22:31,270
you can see their name here they are

00:22:28,960 --> 00:22:32,950
accounts that have hundreds of thousands

00:22:31,270 --> 00:22:36,430
of followers and that have been

00:22:32,950 --> 00:22:37,900
consistently active and and in many

00:22:36,430 --> 00:22:39,310
cases are journalists and we could talk

00:22:37,900 --> 00:22:41,410
more about that because I've made a

00:22:39,310 --> 00:22:45,730
conscious decision to not anonymizing

00:22:41,410 --> 00:22:47,350
this case so as you can see in this in

00:22:45,730 --> 00:22:49,270
this conversation the red cluster is

00:22:47,350 --> 00:22:51,700
dominating this conversation their

00:22:49,270 --> 00:22:53,170
activity is persistent over time many of

00:22:51,700 --> 00:22:54,760
the same accounts repeatedly tweeting

00:22:53,170 --> 00:22:56,380
and retweeting content that specifically

00:22:54,760 --> 00:22:58,060
attacks the white helmets they're

00:22:56,380 --> 00:23:02,500
literally drowning out the voices of

00:22:58,060 --> 00:23:03,790
accounts on the other side but who are

00:23:02,500 --> 00:23:05,620
these accounts dominating this

00:23:03,790 --> 00:23:07,030
conversation the top influencers include

00:23:05,620 --> 00:23:08,920
self-described journalists that can

00:23:07,030 --> 00:23:10,300
consistently produce content aligned

00:23:08,920 --> 00:23:10,900
with Syrian and Russian government

00:23:10,300 --> 00:23:13,240
interests

00:23:10,900 --> 00:23:14,590
there are also officials from the Syrian

00:23:13,240 --> 00:23:16,870
government which you would expect and

00:23:14,590 --> 00:23:18,730
other governments as well and we can see

00:23:16,870 --> 00:23:21,130
that a few of what seemed to be

00:23:18,730 --> 00:23:22,870
inauthentic and likely paid or

00:23:21,130 --> 00:23:24,940
organizationally controlled troll

00:23:22,870 --> 00:23:26,410
accounts but inauthentic troll accounts

00:23:24,940 --> 00:23:28,480
make up only a tiny percentage of the

00:23:26,410 --> 00:23:30,160
content in this conversation and we did

00:23:28,480 --> 00:23:33,010
not find BOTS that were contributed in

00:23:30,160 --> 00:23:34,630
any significant way many of the accounts

00:23:33,010 --> 00:23:36,340
in red most of the smaller ones that you

00:23:34,630 --> 00:23:38,410
can't see the names of our unaffiliated

00:23:36,340 --> 00:23:39,520
accounts who identify as activists

00:23:38,410 --> 00:23:41,380
they're the ones that are actually doing

00:23:39,520 --> 00:23:43,840
the retweet tweeting and keeping this

00:23:41,380 --> 00:23:46,240
going they're not paid they're not paid

00:23:43,840 --> 00:23:48,520
and they're not they're not controlled

00:23:46,240 --> 00:23:50,320
they're sincere activists in this space

00:23:48,520 --> 00:23:52,740
many have been active in other political

00:23:50,320 --> 00:23:55,540
conversations a large chunk have tweeted

00:23:52,740 --> 00:23:58,060
pro-palestinian content many identify as

00:23:55,540 --> 00:24:02,650
anti Imperials anti imperialist and

00:23:58,060 --> 00:24:05,380
anti-war a lot of these accounts look a

00:24:02,650 --> 00:24:08,320
lot like my research group who was

00:24:05,380 --> 00:24:08,800
studying this conversation we share you

00:24:08,320 --> 00:24:10,210
know you

00:24:08,800 --> 00:24:12,340
you feel like you share a lot of their

00:24:10,210 --> 00:24:13,660
values if you look at their if you're a

00:24:12,340 --> 00:24:16,870
left-leaning person and you look at

00:24:13,660 --> 00:24:19,090
their account profiles and yet we're

00:24:16,870 --> 00:24:21,100
seeing them in this case be very

00:24:19,090 --> 00:24:22,360
specifically attack attacking this group

00:24:21,100 --> 00:24:24,760
and I'm going to talk about how that

00:24:22,360 --> 00:24:26,770
affected us later so what are they doing

00:24:24,760 --> 00:24:28,360
they're participating in what very much

00:24:26,770 --> 00:24:30,190
looks like online activism they're

00:24:28,360 --> 00:24:31,990
sharing articles and video videos that

00:24:30,190 --> 00:24:33,040
support their narratives they mention

00:24:31,990 --> 00:24:36,700
each other a lot

00:24:33,040 --> 00:24:38,890
they often whistle and then dogpile

00:24:36,700 --> 00:24:40,210
people so when they see content that's

00:24:38,890 --> 00:24:42,250
check that's supporting the white

00:24:40,210 --> 00:24:43,630
helmets the folks in red will call it

00:24:42,250 --> 00:24:51,880
out and all of a sudden they will

00:24:43,630 --> 00:24:53,800
converge I sorry I'm expecting something

00:24:51,880 --> 00:24:56,290
here that's right so I'm going to show

00:24:53,800 --> 00:24:58,480
you a little example this was after I

00:24:56,290 --> 00:25:00,090
wrote the paper about this I put out a

00:24:58,480 --> 00:25:02,170
tweet to say hey I just wrote the paper

00:25:00,090 --> 00:25:04,300
about all these case that he said and

00:25:02,170 --> 00:25:07,090
I'm doing this and in soon max

00:25:04,300 --> 00:25:09,960
Blumenthal found me and he claimed that

00:25:07,090 --> 00:25:13,000
I well I was citing new knowledge

00:25:09,960 --> 00:25:15,490
integrity issue anyways he tried to use

00:25:13,000 --> 00:25:18,570
a guilt by association to submit claims

00:25:15,490 --> 00:25:23,830
of CIA NSA and FBI affiliations for me

00:25:18,570 --> 00:25:25,300
and then then more not a miss Act 'va

00:25:23,830 --> 00:25:26,440
section this is one that people have

00:25:25,300 --> 00:25:31,450
seen a lot but I'm gonna agree about

00:25:26,440 --> 00:25:32,950
anyways call maintain Orwellian

00:25:31,450 --> 00:25:35,680
authoritarian control of the pro2

00:25:32,950 --> 00:25:39,540
anyways you can see where this is going

00:25:35,680 --> 00:25:41,860
I'm served NATO interests apparently and

00:25:39,540 --> 00:25:46,240
been at a meeting with James clapper

00:25:41,860 --> 00:25:49,180
once I get spoon-fed the script and then

00:25:46,240 --> 00:25:53,050
this guy created a count a impersonating

00:25:49,180 --> 00:25:55,450
an actual academic and he'd just created

00:25:53,050 --> 00:25:57,550
it that day and said I'm glad I'll never

00:25:55,450 --> 00:25:59,250
get to meet you I actually replied back

00:25:57,550 --> 00:26:03,370
and I told in the feeling was mutual

00:25:59,250 --> 00:26:05,830
and so this their so they they mobilized

00:26:03,370 --> 00:26:10,990
very quickly to kind of to kind of

00:26:05,830 --> 00:26:12,550
attack and challenge this yeah

00:26:10,990 --> 00:26:15,100
PhD she needs some of the contact our

00:26:12,550 --> 00:26:16,900
lawyers to pursue a refund from my PhD

00:26:15,100 --> 00:26:18,670
and then my favorite this is my favorite

00:26:16,900 --> 00:26:20,770
she's just frustrated all the American

00:26:18,670 --> 00:26:22,810
spinster that could not get an American

00:26:20,770 --> 00:26:25,000
football player to marry her Balan just

00:26:22,810 --> 00:26:26,440
I like I had to trade that one that was

00:26:25,000 --> 00:26:28,510
my favorite one but anyways what you get

00:26:26,440 --> 00:26:29,530
is in this it doesn't this is just just

00:26:28,510 --> 00:26:32,140
happened to me I mean this is what

00:26:29,530 --> 00:26:34,600
happens anytime anyone in that blue says

00:26:32,140 --> 00:26:37,150
anything they just get they just get

00:26:34,600 --> 00:26:38,320
dogpile by this by this group because

00:26:37,150 --> 00:26:40,000
they're very active so they're

00:26:38,320 --> 00:26:42,400
persistent they tweet about the white

00:26:40,000 --> 00:26:44,500
helmets for months years they activated

00:26:42,400 --> 00:26:45,700
opportune times to capitalize on new

00:26:44,500 --> 00:26:47,440
information that can help them make

00:26:45,700 --> 00:26:48,880
their case and distract from breaking

00:26:47,440 --> 00:26:51,130
news events that are not aligned with

00:26:48,880 --> 00:26:53,830
their narratives and they organized in

00:26:51,130 --> 00:26:55,480
many ways they look a lot like the

00:26:53,830 --> 00:26:57,250
disaster volunteers that I used to study

00:26:55,480 --> 00:26:58,990
using Twitter and other platforms to

00:26:57,250 --> 00:27:01,210
coordinate and collaborate to recruit

00:26:58,990 --> 00:27:04,060
members to spread their messages this is

00:27:01,210 --> 00:27:05,500
very much sort of collaborative work it

00:27:04,060 --> 00:27:08,890
looks very much like the collaborative

00:27:05,500 --> 00:27:10,150
work I used to study around the outside

00:27:08,890 --> 00:27:12,100
of this graph are a different kind of

00:27:10,150 --> 00:27:14,920
node they're web domains or web sites

00:27:12,100 --> 00:27:18,370
that are linked to from tweets shared by

00:27:14,920 --> 00:27:19,990
the accounts so these are in this case

00:27:18,370 --> 00:27:23,560
around the outside circle are domains

00:27:19,990 --> 00:27:26,590
that that we had links to from inside

00:27:23,560 --> 00:27:27,940
the tweets and the and so they're

00:27:26,590 --> 00:27:29,830
connected they're in their size by the

00:27:27,940 --> 00:27:33,580
the relative number tweets that went to

00:27:29,830 --> 00:27:35,440
each domain you can see you know YouTube

00:27:33,580 --> 00:27:36,550
up there is is prominent I'm not going

00:27:35,440 --> 00:27:37,930
to talk about that today we're studying

00:27:36,550 --> 00:27:40,420
that now I don't have a lot of results

00:27:37,930 --> 00:27:43,690
on that but I want to focus in on these

00:27:40,420 --> 00:27:45,940
two to the top domains the top domains

00:27:43,690 --> 00:27:47,700
supporting this conversation are RT

00:27:45,940 --> 00:27:50,370
formerly Russia today and Sputnik news

00:27:47,700 --> 00:27:52,720
both domains are part of the Russia

00:27:50,370 --> 00:27:54,520
Russia's government median influence

00:27:52,720 --> 00:27:55,840
apparatus they're controlled partially

00:27:54,520 --> 00:27:58,870
by the Russian government and they're

00:27:55,840 --> 00:28:00,700
there purposefully to as a propaganda

00:27:58,870 --> 00:28:02,260
kind of apparatus for the for the

00:28:00,700 --> 00:28:04,060
government in the white helmets

00:28:02,260 --> 00:28:06,760
conversation these news domains provide

00:28:04,060 --> 00:28:08,380
content including video content of the

00:28:06,760 --> 00:28:09,910
conflict and interviews with voices that

00:28:08,380 --> 00:28:12,280
share their objectives they shape

00:28:09,910 --> 00:28:13,870
narratives they amplify voices of anti

00:28:12,280 --> 00:28:15,820
white helmets influencers they

00:28:13,870 --> 00:28:17,080
repeatedly interview the journalists who

00:28:15,820 --> 00:28:19,420
are among the most retweeted in this

00:28:17,080 --> 00:28:21,370
conversation and often those journalists

00:28:19,420 --> 00:28:22,960
link back to their interviews on RT

00:28:21,370 --> 00:28:25,270
demonstrating this kind of cross

00:28:22,960 --> 00:28:27,760
promotion and so these websites are a

00:28:25,270 --> 00:28:30,910
resource for the activity that's

00:28:27,760 --> 00:28:32,289
happening there in the anti whites the

00:28:30,910 --> 00:28:34,779
helmets come

00:28:32,289 --> 00:28:37,210
there are other domains in pink it's

00:28:34,779 --> 00:28:39,669
kind of around the bottom in others that

00:28:37,210 --> 00:28:41,799
function is a kind of gray propaganda we

00:28:39,669 --> 00:28:42,909
actually one of the things we noticed

00:28:41,799 --> 00:28:45,820
when we first started setting this

00:28:42,909 --> 00:28:49,299
conversation was first of all it's

00:28:45,820 --> 00:28:52,899
extremely disorienting our researchers

00:28:49,299 --> 00:28:55,120
many of us are left leaning identified

00:28:52,899 --> 00:28:56,679
with sort of anti-imperialist a

00:28:55,120 --> 00:29:01,029
left-leaning is sorry left-leaning and

00:28:56,679 --> 00:29:03,700
international the the research team was

00:29:01,029 --> 00:29:05,500
was very international and so a lot of

00:29:03,700 --> 00:29:06,820
the the anti white helmets narratives

00:29:05,500 --> 00:29:08,820
initially resonated with my with my

00:29:06,820 --> 00:29:10,870
students that were studying this and

00:29:08,820 --> 00:29:13,149
they would go online and they would say

00:29:10,870 --> 00:29:15,370
you know I saw this one article yeah one

00:29:13,149 --> 00:29:17,080
articles on RTI rejected that but I saw

00:29:15,370 --> 00:29:19,000
the same kind of content and I was seen

00:29:17,080 --> 00:29:20,620
you know you know in in this one

00:29:19,000 --> 00:29:22,809
placement press news and I really liked

00:29:20,620 --> 00:29:24,340
that site it really speaks to me and it

00:29:22,809 --> 00:29:26,529
wasn't until we were able to step back

00:29:24,340 --> 00:29:27,730
and map what was going on that they

00:29:26,529 --> 00:29:30,100
began to understand that the same

00:29:27,730 --> 00:29:32,019
content was being shared across all of

00:29:30,100 --> 00:29:34,629
these different sites and it gave him

00:29:32,019 --> 00:29:36,820
this kind of small sense of try of

00:29:34,629 --> 00:29:39,070
triangulation so they they were getting

00:29:36,820 --> 00:29:41,889
the same ideas so this map actually is

00:29:39,070 --> 00:29:43,870
in this case we took all the articles

00:29:41,889 --> 00:29:45,970
and if the same article appeared in two

00:29:43,870 --> 00:29:48,220
different web sites we created a link

00:29:45,970 --> 00:29:50,350
and the link is stronger for the number

00:29:48,220 --> 00:29:52,840
of articles that appear on both of them

00:29:50,350 --> 00:29:55,720
so mint press news in 21st century wire

00:29:52,840 --> 00:29:57,850
have a pretty strong link they had like

00:29:55,720 --> 00:29:58,899
six of the same articles and in the

00:29:57,850 --> 00:30:02,139
period of time we were looking at

00:29:58,899 --> 00:30:05,080
appeared on both of both sites and so

00:30:02,139 --> 00:30:07,809
what we had here is this kind of a

00:30:05,080 --> 00:30:09,730
network where they're content sharing of

00:30:07,809 --> 00:30:12,789
the articles for a lot of these places

00:30:09,730 --> 00:30:14,409
for a lot of the little ones in there

00:30:12,789 --> 00:30:17,049
there's some terrible terrible little

00:30:14,409 --> 00:30:21,519
sites in there there are things called

00:30:17,049 --> 00:30:25,389
jus World Order there's also little the

00:30:21,519 --> 00:30:27,039
truth seeker the free free thought

00:30:25,389 --> 00:30:28,809
projects and different kinds of things

00:30:27,039 --> 00:30:31,059
things that are like conspiracy they

00:30:28,809 --> 00:30:32,409
just put out whatever in terms of

00:30:31,059 --> 00:30:34,629
conspiracy theories but then there's

00:30:32,409 --> 00:30:36,639
other sites that really have you know

00:30:34,629 --> 00:30:38,190
they have an ideology and they're saying

00:30:36,639 --> 00:30:41,350
these things and what's happening is

00:30:38,190 --> 00:30:42,519
there's this content sharing where some

00:30:41,350 --> 00:30:43,370
of these sites just like the free

00:30:42,519 --> 00:30:44,630
content

00:30:43,370 --> 00:30:46,670
and they're putting it up there when

00:30:44,630 --> 00:30:48,590
they can because they getting access to

00:30:46,670 --> 00:30:50,270
this content and others have sort of

00:30:48,590 --> 00:30:51,740
this political propaganda mission where

00:30:50,270 --> 00:30:54,200
they're putting certain kinds of content

00:30:51,740 --> 00:30:58,300
out there that aligns with their aligns

00:30:54,200 --> 00:31:00,230
with their perspectives and we've got

00:30:58,300 --> 00:31:01,520
what's interesting here is that you can

00:31:00,230 --> 00:31:02,630
see the same articles dress up in

00:31:01,520 --> 00:31:03,680
different wrappers designed for

00:31:02,630 --> 00:31:06,200
different audiences

00:31:03,680 --> 00:31:08,570
we have disaffected US veterans uprooted

00:31:06,200 --> 00:31:11,180
Palestinians anti-imperialist think

00:31:08,570 --> 00:31:13,370
tanks anti-war activists all right

00:31:11,180 --> 00:31:15,110
Patriots sites and a multitude of sites

00:31:13,370 --> 00:31:17,690
dedicated to conspiracy theorizing on

00:31:15,110 --> 00:31:19,580
various topics the structure functions

00:31:17,690 --> 00:31:21,320
to bring ideologically distinct and in

00:31:19,580 --> 00:31:23,210
some cases seemingly oppositional

00:31:21,320 --> 00:31:25,040
domains together around common

00:31:23,210 --> 00:31:26,450
narratives and perspectives this gives

00:31:25,040 --> 00:31:28,250
the appearance of a diverse set of

00:31:26,450 --> 00:31:30,679
people converging around the same ideas

00:31:28,250 --> 00:31:32,990
a sense of triangulation but false

00:31:30,679 --> 00:31:34,309
triangulation it's also possible and

00:31:32,990 --> 00:31:36,440
even likely considering what we know

00:31:34,309 --> 00:31:38,120
about historical strategies and what

00:31:36,440 --> 00:31:39,440
we're seeing in other cases is that

00:31:38,120 --> 00:31:41,120
these content sharing practices are

00:31:39,440 --> 00:31:42,890
micro targeting specific communities

00:31:41,120 --> 00:31:44,570
with strategic narratives through

00:31:42,890 --> 00:31:48,770
websites designed to speak with two

00:31:44,570 --> 00:31:49,970
different audiences so stepping back we

00:31:48,770 --> 00:31:51,950
see a couple different things happening

00:31:49,970 --> 00:31:53,540
here but we are looking at this as a

00:31:51,950 --> 00:31:55,429
persistent campaign of at least

00:31:53,540 --> 00:31:57,980
partially organic political activism

00:31:55,429 --> 00:31:59,840
supported by Russia's government by

00:31:57,980 --> 00:32:02,990
their media apparatus and in line it

00:31:59,840 --> 00:32:04,550
with their political objectives this is

00:32:02,990 --> 00:32:06,440
not orchestrated like what we saw before

00:32:04,550 --> 00:32:11,420
there aren't just a set of trolls

00:32:06,440 --> 00:32:13,550
somewhere that that are that are the

00:32:11,420 --> 00:32:16,520
whole disinformation activity isn't just

00:32:13,550 --> 00:32:19,160
organized in some in some room in st.

00:32:16,520 --> 00:32:22,490
Petersburg but it's cultivated and it's

00:32:19,160 --> 00:32:24,170
it's the Russian information apparatus

00:32:22,490 --> 00:32:25,730
amplifies friendly voices provides a

00:32:24,170 --> 00:32:27,260
platform for journalists whose stories

00:32:25,730 --> 00:32:29,750
aligned with their narratives lifts

00:32:27,260 --> 00:32:31,400
their profile hosts content on their

00:32:29,750 --> 00:32:32,960
flagship sites but also allows their

00:32:31,400 --> 00:32:34,490
content to flow through various grey

00:32:32,960 --> 00:32:36,710
propaganda sites they make it available

00:32:34,490 --> 00:32:38,600
and and freely shared across that

00:32:36,710 --> 00:32:40,580
network we showed they encourage viewers

00:32:38,600 --> 00:32:42,850
to question more while spreading dozens

00:32:40,580 --> 00:32:45,650
of narratives that attempt to sow doubt

00:32:42,850 --> 00:32:50,059
about who they white helmets are and

00:32:45,650 --> 00:32:53,960
what they do all right the third case

00:32:50,059 --> 00:32:55,460
study actually you know what I think I'm

00:32:53,960 --> 00:32:56,280
gonna skip the third case study because

00:32:55,460 --> 00:33:01,350
I'm gonna

00:32:56,280 --> 00:33:04,290
I'm going to for timing I want to get to

00:33:01,350 --> 00:33:05,970
the other part third case study is

00:33:04,290 --> 00:33:10,200
basically about conspiracy theories in

00:33:05,970 --> 00:33:11,670
online and it talks about how we start

00:33:10,200 --> 00:33:13,050
about with conspiracy theories of crisis

00:33:11,670 --> 00:33:15,360
events people that claim that shooting

00:33:13,050 --> 00:33:16,980
events in the u.s. don't happen but we

00:33:15,360 --> 00:33:19,260
talked about how conspiracy theorizing

00:33:16,980 --> 00:33:22,140
serves as a vector for disinformation

00:33:19,260 --> 00:33:23,640
and how we can find communities that are

00:33:22,140 --> 00:33:26,880
almost organic and the fact that they

00:33:23,640 --> 00:33:28,230
don't need any outside group to help

00:33:26,880 --> 00:33:31,050
them start their conspiracy theorizing

00:33:28,230 --> 00:33:33,300
but the they've become sort of a soft

00:33:31,050 --> 00:33:34,860
space where disinformation agents can

00:33:33,300 --> 00:33:36,870
easily target them to spread their own

00:33:34,860 --> 00:33:39,480
conspiracy theories and so they act as

00:33:36,870 --> 00:33:42,690
sort of a vector of this information and

00:33:39,480 --> 00:33:46,410
we can see that in cases like what was

00:33:42,690 --> 00:33:48,000
happening after El Paso we can see that

00:33:46,410 --> 00:33:50,300
in cases around South rich and other

00:33:48,000 --> 00:33:52,500
kinds of things we can see this kind of

00:33:50,300 --> 00:33:55,020
conspiracy theorizing community where

00:33:52,500 --> 00:33:57,630
they don't believe the media they don't

00:33:55,020 --> 00:33:59,970
trust you they don't trust Western

00:33:57,630 --> 00:34:01,500
governments they're very kind of

00:33:59,970 --> 00:34:04,050
susceptible to ideas of like a deep

00:34:01,500 --> 00:34:05,760
state that that narrative landed very

00:34:04,050 --> 00:34:07,890
well in sort of a community that already

00:34:05,760 --> 00:34:09,629
already had this kind of conspiracy

00:34:07,890 --> 00:34:11,460
theorizing about how the world works and

00:34:09,629 --> 00:34:15,179
then they could become this sort of

00:34:11,460 --> 00:34:17,100
organic and emergent behavior but it can

00:34:15,179 --> 00:34:18,750
actively spread sort of distant from

00:34:17,100 --> 00:34:21,090
there disinformation narratives and

00:34:18,750 --> 00:34:22,620
we're seeing these communities start

00:34:21,090 --> 00:34:24,659
conspiracy theories that are then

00:34:22,620 --> 00:34:26,340
spreading up to influencers and

00:34:24,659 --> 00:34:27,929
spreading up to political leaders not

00:34:26,340 --> 00:34:30,929
just here in the US but elsewhere in the

00:34:27,929 --> 00:34:32,070
world as well so this is a the other

00:34:30,929 --> 00:34:33,870
kind of case lay but I wanted to go

00:34:32,070 --> 00:34:35,429
through it fast because I really want to

00:34:33,870 --> 00:34:37,169
get to sort of adding nuance to our

00:34:35,429 --> 00:34:39,540
evolving desta understanding of online

00:34:37,169 --> 00:34:41,429
disinformation so I want to use some of

00:34:39,540 --> 00:34:43,320
the material from the studies I just

00:34:41,429 --> 00:34:45,419
about and want to revisit our

00:34:43,320 --> 00:34:47,310
understanding of disinformation to share

00:34:45,419 --> 00:34:49,320
a couple of things we've learned about

00:34:47,310 --> 00:34:50,790
what it is how it works why we're

00:34:49,320 --> 00:34:54,120
vulnerable and why solutions aren't so

00:34:50,790 --> 00:34:55,740
simple so the first thing is the

00:34:54,120 --> 00:34:58,230
disinformation is not simply false

00:34:55,740 --> 00:35:00,600
information in fact sometimes it isn't

00:34:58,230 --> 00:35:02,640
false at all to be effective

00:35:00,600 --> 00:35:04,770
disinformation layers truth with lies

00:35:02,640 --> 00:35:07,530
often building on a true or plausible

00:35:04,770 --> 00:35:09,360
core but then adding new details and

00:35:07,530 --> 00:35:12,090
omitting others to shape a specific

00:35:09,360 --> 00:35:14,880
towards a specific strategic objective

00:35:12,090 --> 00:35:16,440
in the IRA troll case the accounts the

00:35:14,880 --> 00:35:18,090
troll accounts mislead their audiences

00:35:16,440 --> 00:35:20,070
about who they were and what their

00:35:18,090 --> 00:35:21,510
intentions or motivations were they

00:35:20,070 --> 00:35:23,520
attempted to shape political discourse

00:35:21,510 --> 00:35:25,290
and political action in that case voting

00:35:23,520 --> 00:35:26,340
of the communities they infiltrated

00:35:25,290 --> 00:35:28,500
towards objectives that those

00:35:26,340 --> 00:35:31,950
communities were not aware of and likely

00:35:28,500 --> 00:35:33,330
did not share in the white helmets case

00:35:31,950 --> 00:35:35,130
we can see narratives like this one

00:35:33,330 --> 00:35:37,410
built from factual information that the

00:35:35,130 --> 00:35:38,700
white helmets and others provide about

00:35:37,410 --> 00:35:40,680
the funding they receive from Western

00:35:38,700 --> 00:35:42,180
governments and evidence such as the

00:35:40,680 --> 00:35:44,210
videos of the white helmets themselves

00:35:42,180 --> 00:35:46,710
produced documenting regime atrocities

00:35:44,210 --> 00:35:48,090
and they use that evidence in

00:35:46,710 --> 00:35:49,860
combination to claim that they're there

00:35:48,090 --> 00:35:52,110
for a propaganda construct up the West

00:35:49,860 --> 00:35:53,910
according to the activists in pink in

00:35:52,110 --> 00:35:55,620
that web site I showed so you know in

00:35:53,910 --> 00:35:57,510
that graph I showed and the media that

00:35:55,620 --> 00:35:58,800
support them the white helmets are paid

00:35:57,510 --> 00:36:00,690
by Western governments to create

00:35:58,800 --> 00:36:03,030
propaganda meant to change how people

00:36:00,690 --> 00:36:04,560
see the war it's a compelling narrative

00:36:03,030 --> 00:36:06,330
that's hard to problematize that's

00:36:04,560 --> 00:36:10,440
simply true or false

00:36:06,330 --> 00:36:11,700
its strategic another insight from this

00:36:10,440 --> 00:36:13,650
work is that we need to consider or

00:36:11,700 --> 00:36:14,610
reconsider our unit of analysis it's not

00:36:13,650 --> 00:36:15,930
always effective to think of

00:36:14,610 --> 00:36:18,000
disinformation is a single piece of

00:36:15,930 --> 00:36:21,720
information but instead to think of it

00:36:18,000 --> 00:36:23,130
as a campaign Bittman uses that analysis

00:36:21,720 --> 00:36:24,750
that unit of analysis talking about

00:36:23,130 --> 00:36:26,670
disinformation efforts as part of an

00:36:24,750 --> 00:36:28,800
information operation or campaign and

00:36:26,670 --> 00:36:30,660
borrowing from his characterization and

00:36:28,800 --> 00:36:32,610
our analyses of modern disinformation we

00:36:30,660 --> 00:36:34,170
agree that it's useful to think of this

00:36:32,610 --> 00:36:36,360
information not as a quality of a

00:36:34,170 --> 00:36:38,790
specific piece of information but as a

00:36:36,360 --> 00:36:41,880
collection of information actions or a

00:36:38,790 --> 00:36:43,440
campaign this is going to have impact on

00:36:41,880 --> 00:36:44,580
how we think about fact-checking or how

00:36:43,440 --> 00:36:47,190
we think about solutions in the

00:36:44,580 --> 00:36:49,110
technical space because when we're

00:36:47,190 --> 00:36:52,170
looking at conspiracy theories of crisis

00:36:49,110 --> 00:36:53,520
a crisis events in part they exist in a

00:36:52,170 --> 00:36:55,170
messy space where the echo of old

00:36:53,520 --> 00:36:57,180
campaigns can be heard and the community

00:36:55,170 --> 00:36:59,040
is softened for the messaging and

00:36:57,180 --> 00:37:01,170
narrative sensitives anyway so but

00:36:59,040 --> 00:37:02,850
looking at the 2016 election

00:37:01,170 --> 00:37:05,250
interference in the u.s. we can see a

00:37:02,850 --> 00:37:07,470
multi-channel multi-dimensional campaign

00:37:05,250 --> 00:37:09,030
on the IRA side we talked about Twitter

00:37:07,470 --> 00:37:10,860
but the troll activity was

00:37:09,030 --> 00:37:12,900
multi-platform it was on Facebook it was

00:37:10,860 --> 00:37:15,450
on Instagram they were commenting on

00:37:12,900 --> 00:37:16,890
websites and forums they were making

00:37:15,450 --> 00:37:18,020
SoundCloud

00:37:16,890 --> 00:37:20,780
[Music]

00:37:18,020 --> 00:37:23,960
playlists and distributing things so

00:37:20,780 --> 00:37:25,920
it's so it was multi it was

00:37:23,960 --> 00:37:28,260
multi-platform but it wasn't just social

00:37:25,920 --> 00:37:29,940
media campaign and it or just the IRA

00:37:28,260 --> 00:37:31,440
and its trolls it included the release

00:37:29,940 --> 00:37:32,700
of stolen emails that have been

00:37:31,440 --> 00:37:34,349
repeatedly associated with Russian

00:37:32,700 --> 00:37:36,150
hackers the use of great propaganda

00:37:34,349 --> 00:37:37,829
sites to push their narratives the

00:37:36,150 --> 00:37:39,690
manipulation of mainstream media spread

00:37:37,829 --> 00:37:41,160
their stolen content and the

00:37:39,690 --> 00:37:43,290
infiltration of other social and

00:37:41,160 --> 00:37:46,140
political organizations it's not just

00:37:43,290 --> 00:37:48,089
one troll it's not just one one set of

00:37:46,140 --> 00:37:50,520
activities it's a much larger thing in

00:37:48,089 --> 00:37:52,619
the white helmets example it's not just

00:37:50,520 --> 00:37:54,660
a single piece of information or even a

00:37:52,619 --> 00:37:56,030
single narrative but an on site of

00:37:54,660 --> 00:37:58,799
stories that built into several

00:37:56,030 --> 00:38:00,839
connected narratives using Twitter and

00:37:58,799 --> 00:38:03,000
YouTube and in combination with

00:38:00,839 --> 00:38:05,369
connected media strategies across dozens

00:38:03,000 --> 00:38:06,869
of great propaganda sites this effort

00:38:05,369 --> 00:38:08,280
sought to undermine the white helmets

00:38:06,869 --> 00:38:10,859
especially their efforts to garner

00:38:08,280 --> 00:38:12,569
sympathy from global audiences but also

00:38:10,859 --> 00:38:14,520
to connect them to terrorists on one

00:38:12,569 --> 00:38:16,079
hand and questions and narratives

00:38:14,520 --> 00:38:18,380
questioning Western intervention and

00:38:16,079 --> 00:38:20,309
challenging Western media on the other I

00:38:18,380 --> 00:38:23,640
end with one of their most recent

00:38:20,309 --> 00:38:25,170
narratives a tweet featuring an RT

00:38:23,640 --> 00:38:26,760
interview with a prominent journalist in

00:38:25,170 --> 00:38:28,170
this space Vanessa B Lee where she

00:38:26,760 --> 00:38:29,940
claims that the white helmets are

00:38:28,170 --> 00:38:37,140
trafficking in the organs of children

00:38:29,940 --> 00:38:39,150
and I'm gonna leave that there so

00:38:37,140 --> 00:38:41,160
considering these two together the unit

00:38:39,150 --> 00:38:42,809
of analysis for for disinformation is a

00:38:41,160 --> 00:38:45,450
campaign and the disinformation is not

00:38:42,809 --> 00:38:46,740
simply false information as we turn to

00:38:45,450 --> 00:38:48,240
questions of detection which is an

00:38:46,740 --> 00:38:50,220
important area for research especially

00:38:48,240 --> 00:38:51,990
in online environments this means that

00:38:50,220 --> 00:38:53,339
identifying this information isn't about

00:38:51,990 --> 00:38:55,770
determining the truth value of a

00:38:53,339 --> 00:38:57,569
specific post or the authenticity of a

00:38:55,770 --> 00:38:59,609
single account but to think about how

00:38:57,569 --> 00:39:01,799
that post and account fit into a larger

00:38:59,609 --> 00:39:03,359
campaign where the underlying intent is

00:39:01,799 --> 00:39:05,880
to mislead for strategic political

00:39:03,359 --> 00:39:08,069
purpose in this view disinformation

00:39:05,880 --> 00:39:10,619
remains disinformation regardless of who

00:39:08,069 --> 00:39:11,790
the intermediary is a person does not

00:39:10,619 --> 00:39:15,240
need to be aware of their role in the

00:39:11,790 --> 00:39:16,290
campaign to be a participant this gets

00:39:15,240 --> 00:39:17,970
to the point that we highlighted in the

00:39:16,290 --> 00:39:19,440
talk title of a talk a lot of this

00:39:17,970 --> 00:39:21,270
research has been focused on detected

00:39:19,440 --> 00:39:23,700
automated activity or BOTS or

00:39:21,270 --> 00:39:25,349
inauthentic accounts trolls but this

00:39:23,700 --> 00:39:28,079
information isn't just about BOTS and

00:39:25,349 --> 00:39:30,720
trolls as we see in our studies it

00:39:28,079 --> 00:39:32,520
targets infiltrates cultivates shapes at

00:39:30,720 --> 00:39:34,349
ultimately leverages online crowds of

00:39:32,520 --> 00:39:36,210
communities Pittman talked about the

00:39:34,349 --> 00:39:38,760
role that unwitting agents play and

00:39:36,210 --> 00:39:40,560
historical disinformation campaigns but

00:39:38,760 --> 00:39:43,980
here we can see agents working with

00:39:40,560 --> 00:39:46,380
collaborating with online crowds or

00:39:43,980 --> 00:39:48,990
unwitting crowds to achieve their goals

00:39:46,380 --> 00:39:50,460
so I encourage researchers and platform

00:39:48,990 --> 00:39:51,810
designers not just to focus on the

00:39:50,460 --> 00:39:54,060
orange parts of the graph on the left

00:39:51,810 --> 00:39:56,040
but to look at the blue and the pink and

00:39:54,060 --> 00:39:57,540
the graph on the right to study how

00:39:56,040 --> 00:39:59,760
online communities are targeted and

00:39:57,540 --> 00:40:01,410
affected by disinformation how some

00:39:59,760 --> 00:40:02,760
members of those unwitting crowds take

00:40:01,410 --> 00:40:03,840
it up and make it their own

00:40:02,760 --> 00:40:06,570
eventually generating their own

00:40:03,840 --> 00:40:07,910
propaganda like messages that reflect

00:40:06,570 --> 00:40:10,920
the narratives of the disinformation

00:40:07,910 --> 00:40:12,750
operatives and this is really important

00:40:10,920 --> 00:40:14,790
we're all vulnerable to disinformation

00:40:12,750 --> 00:40:17,280
when we went to study the white helmets

00:40:14,790 --> 00:40:19,650
conversation we were completely

00:40:17,280 --> 00:40:20,940
disoriented at first it took us we kept

00:40:19,650 --> 00:40:23,460
asking the people we were working with

00:40:20,940 --> 00:40:25,950
is this really propaganda is this really

00:40:23,460 --> 00:40:28,320
wrong some of this looks very it looks

00:40:25,950 --> 00:40:30,630
true to me and and and we got very

00:40:28,320 --> 00:40:32,670
confused because of the way that these

00:40:30,630 --> 00:40:33,570
messages were were were set up and

00:40:32,670 --> 00:40:36,630
different they were all on these

00:40:33,570 --> 00:40:39,119
different websites and in the messaging

00:40:36,630 --> 00:40:40,980
in many cases those web sites were were

00:40:39,119 --> 00:40:44,430
designed and taught to talk to people

00:40:40,980 --> 00:40:45,960
like us and so the experience is that

00:40:44,430 --> 00:40:49,260
one of my students will describe it is

00:40:45,960 --> 00:40:51,330
it completely disorienting as if like

00:40:49,260 --> 00:40:55,020
there's just static on all the time and

00:40:51,330 --> 00:40:56,910
he he's unable to kind of find the

00:40:55,020 --> 00:40:59,730
signal with the noise and in some cases

00:40:56,910 --> 00:41:01,589
in some cases it was never about well

00:40:59,730 --> 00:41:03,119
I'll get to that in two slides so

00:41:01,589 --> 00:41:05,310
looking beyond a single campaign the

00:41:03,119 --> 00:41:07,320
pervasive use of disinformation as

00:41:05,310 --> 00:41:09,420
consequences for society's experiencing

00:41:07,320 --> 00:41:10,589
it one of those is diminished trust and

00:41:09,420 --> 00:41:13,080
information we've talked a lot about

00:41:10,589 --> 00:41:14,970
that here disinformation reduces our

00:41:13,080 --> 00:41:16,680
ability to know who and what to trust

00:41:14,970 --> 00:41:17,910
with the idea that when people don't

00:41:16,680 --> 00:41:19,650
know where to go for information they

00:41:17,910 --> 00:41:21,720
can trust they lose their agency their

00:41:19,650 --> 00:41:24,660
ability to take decisions based on

00:41:21,720 --> 00:41:26,369
knowledge in the world you know there's

00:41:24,660 --> 00:41:28,380
also we've see direct attacks on the

00:41:26,369 --> 00:41:32,099
media one of the prominent things in the

00:41:28,380 --> 00:41:35,070
IRA in the IRA data set where attacks on

00:41:32,099 --> 00:41:36,180
the media in this case they say CNN is

00:41:35,070 --> 00:41:38,109
Isis

00:41:36,180 --> 00:41:40,930
we have tweets on the in the Siri

00:41:38,109 --> 00:41:43,559
data set we have same thing CNN as Isis

00:41:40,930 --> 00:41:46,210
it's like the same the same the same

00:41:43,559 --> 00:41:48,910
catchphrase was used in both but you can

00:41:46,210 --> 00:41:52,420
see this sort of direct attack on media

00:41:48,910 --> 00:41:54,190
happening in these spaces you can also

00:41:52,420 --> 00:41:56,259
it's also not just fake news or

00:41:54,190 --> 00:41:57,970
mainstream media that we can trust it's

00:41:56,259 --> 00:41:59,829
also about anything you encounter online

00:41:57,970 --> 00:42:02,289
that that account that you're arguing

00:41:59,829 --> 00:42:03,999
with just label them a Russian troll and

00:42:02,289 --> 00:42:05,769
block them right so now that we don't

00:42:03,999 --> 00:42:07,599
know that if we can actually trust who's

00:42:05,769 --> 00:42:09,579
there we've got trolls in the space we

00:42:07,599 --> 00:42:11,799
begin to lose our faith that the

00:42:09,579 --> 00:42:14,349
information spaces are as they seem and

00:42:11,799 --> 00:42:20,789
in this you know this is just an example

00:42:14,349 --> 00:42:23,349
of kind of our erosion of being able to

00:42:20,789 --> 00:42:25,859
trust that things are as they seem and

00:42:23,349 --> 00:42:28,420
their insightful essay on the Menace of

00:42:25,859 --> 00:42:30,519
unreality pomerantsev and wise described

00:42:28,420 --> 00:42:32,049
how the purpose of disinformation is not

00:42:30,519 --> 00:42:34,089
to convince the purpose of

00:42:32,049 --> 00:42:36,430
disinformation is to confuse to create

00:42:34,089 --> 00:42:37,989
muddled thinking across society with

00:42:36,430 --> 00:42:39,609
this idea that a society that doesn't

00:42:37,989 --> 00:42:41,200
know who can where can go for trusted

00:42:39,609 --> 00:42:45,489
information is a society that's easily

00:42:41,200 --> 00:42:47,019
controlled and this loss of trust feeds

00:42:45,489 --> 00:42:48,369
into this sort of more troubling point

00:42:47,019 --> 00:42:50,529
that disinformation undermines

00:42:48,369 --> 00:42:52,749
democratic societies it destabilizes the

00:42:50,529 --> 00:42:54,880
common ground that we need to stand upon

00:42:52,749 --> 00:42:56,559
to govern ourselves it undermines our

00:42:54,880 --> 00:42:58,480
shared reality we lose trust and

00:42:56,559 --> 00:42:59,980
information in the confidence that we

00:42:58,480 --> 00:43:02,289
know what we know that things are as

00:42:59,980 --> 00:43:04,420
they seem if you notice the positioning

00:43:02,289 --> 00:43:07,480
in these arms clusters on the outside of

00:43:04,420 --> 00:43:09,789
these communities helping you know this

00:43:07,480 --> 00:43:11,440
is helping to pull these these

00:43:09,789 --> 00:43:13,299
communities further apart this is

00:43:11,440 --> 00:43:15,579
empirical but it's also metaphorical

00:43:13,299 --> 00:43:18,609
disinformation tears at the fabric of

00:43:15,579 --> 00:43:20,680
the social fabric it amplifies existing

00:43:18,609 --> 00:43:22,660
divisions and if the divisions become

00:43:20,680 --> 00:43:24,670
too extreme we can't come together to

00:43:22,660 --> 00:43:27,940
govern ourselves and this is one of the

00:43:24,670 --> 00:43:30,539
things that a lot of folks in including

00:43:27,940 --> 00:43:38,109
this great article by Henry Pharrell

00:43:30,539 --> 00:43:42,430
talk about how it's not just harassment

00:43:38,109 --> 00:43:44,079
or that we don't that we are exposed to

00:43:42,430 --> 00:43:48,089
fake news it's actually undermining

00:43:44,079 --> 00:43:48,089
democratic societies more profoundly

00:43:48,500 --> 00:43:50,870
tactics and the effects of

00:43:49,580 --> 00:43:52,370
disinformation are resonating with

00:43:50,870 --> 00:43:54,890
features of our socio technical

00:43:52,370 --> 00:43:57,080
environments we know this affordance is

00:43:54,890 --> 00:43:59,630
like anonymity phenomenon like echo

00:43:57,080 --> 00:44:01,580
chambers political polarization the fact

00:43:59,630 --> 00:44:02,930
that we can access these platforms from

00:44:01,580 --> 00:44:04,730
anywhere in the world and they can

00:44:02,930 --> 00:44:06,950
access into political activist

00:44:04,730 --> 00:44:09,770
communities social media did not start

00:44:06,950 --> 00:44:12,350
this fire but they have become fuel for

00:44:09,770 --> 00:44:14,060
it and it online tools more broadly

00:44:12,350 --> 00:44:15,410
they're appropriated by individuals

00:44:14,060 --> 00:44:18,500
organizations and governments all over

00:44:15,410 --> 00:44:20,090
the world and and they're in there kind

00:44:18,500 --> 00:44:22,460
of under undermining what we care about

00:44:20,090 --> 00:44:23,870
and so as folks that are part of these

00:44:22,460 --> 00:44:25,490
socio-technical systems are build the

00:44:23,870 --> 00:44:27,440
systems that shape these behaviors I

00:44:25,490 --> 00:44:29,810
think that it's important to understand

00:44:27,440 --> 00:44:34,100
that that were part of they were part of

00:44:29,810 --> 00:44:36,620
the system addressing the problem is

00:44:34,100 --> 00:44:38,360
gonna it's not gonna be easy so I was on

00:44:36,620 --> 00:44:41,330
a panel this is where I got a little

00:44:38,360 --> 00:44:43,190
depressing I was on a panel a year ago

00:44:41,330 --> 00:44:44,540
right now and we were all talking about

00:44:43,190 --> 00:44:46,700
the problem like oh we're so vulnerable

00:44:44,540 --> 00:44:48,950
and now we can't trust anything and and

00:44:46,700 --> 00:44:50,690
it's just everything's resonating and we

00:44:48,950 --> 00:44:52,490
don't know where it is and at the end of

00:44:50,690 --> 00:44:57,020
this panel this guy got up and this guy

00:44:52,490 --> 00:44:58,490
was old and I don't he was and everyone

00:44:57,020 --> 00:45:00,080
knew him there and I didn't because it

00:44:58,490 --> 00:45:01,550
was in Washington DC and he'd been an

00:45:00,080 --> 00:45:04,130
ambassador and I don't know where but

00:45:01,550 --> 00:45:06,350
he'd been around a long time and he

00:45:04,130 --> 00:45:08,420
scolded us and he got up there and he

00:45:06,350 --> 00:45:11,120
was he's like you know what we've had

00:45:08,420 --> 00:45:12,440
hard problems in the past and and we

00:45:11,120 --> 00:45:13,880
threw our hands up you know we didn't

00:45:12,440 --> 00:45:15,770
throw our hands up in the air

00:45:13,880 --> 00:45:17,270
we had the Cold War and people said oh

00:45:15,770 --> 00:45:18,770
it's never gonna end and eventually we

00:45:17,270 --> 00:45:20,030
we chipped away at it we chipped away of

00:45:18,770 --> 00:45:22,430
it from all these different things and

00:45:20,030 --> 00:45:25,070
eventually that problem is gone and we

00:45:22,430 --> 00:45:26,540
have new problems but you're all up here

00:45:25,070 --> 00:45:28,490
and you're you're acting like we can't

00:45:26,540 --> 00:45:30,260
solve it's like it's unsolvable but but

00:45:28,490 --> 00:45:31,520
it's but it's not gonna be one little

00:45:30,260 --> 00:45:33,740
thing you're not gonna take one little

00:45:31,520 --> 00:45:35,330
you know one little patch on a

00:45:33,740 --> 00:45:37,160
technology platform or something else

00:45:35,330 --> 00:45:38,960
we're gonna solve this by chipping away

00:45:37,160 --> 00:45:40,850
at it from all these different sides and

00:45:38,960 --> 00:45:42,680
I think the technology platforms have a

00:45:40,850 --> 00:45:43,220
role to play I think education has a

00:45:42,680 --> 00:45:45,050
role to play

00:45:43,220 --> 00:45:48,320
I think journalism has a lot to do to

00:45:45,050 --> 00:45:50,120
make itself a stronger institution and I

00:45:48,320 --> 00:45:52,550
think there's there are policy things

00:45:50,120 --> 00:45:56,630
that we should do to talk specifically

00:45:52,550 --> 00:45:59,600
about some platform policies I think my

00:45:56,630 --> 00:46:01,780
last two slides I think it's important

00:45:59,600 --> 00:46:01,780
to

00:46:02,609 --> 00:46:06,280
important kind of look at what

00:46:04,210 --> 00:46:07,960
innovations happening I think Facebook's

00:46:06,280 --> 00:46:11,200
doing a good job they're trying to do a

00:46:07,960 --> 00:46:12,970
good job to think about coordinated

00:46:11,200 --> 00:46:15,190
inauthentic behavior and target maybe

00:46:12,970 --> 00:46:18,220
the troll activity but as we've seen

00:46:15,190 --> 00:46:20,559
like once once a disinformation campaign

00:46:18,220 --> 00:46:22,089
takes root we actually have a much more

00:46:20,559 --> 00:46:24,099
complex problem because we've got people

00:46:22,089 --> 00:46:25,660
who are sincere believers who are part

00:46:24,099 --> 00:46:29,020
of that disinformation campaign and that

00:46:25,660 --> 00:46:30,760
in our freedom of speech values mean

00:46:29,020 --> 00:46:32,079
that we're not we don't want the

00:46:30,760 --> 00:46:33,910
platforms to take the same kind of

00:46:32,079 --> 00:46:35,859
action against people that are sincere

00:46:33,910 --> 00:46:37,210
believers as they might take against an

00:46:35,859 --> 00:46:38,710
online troll and so we have to kind of

00:46:37,210 --> 00:46:40,809
figure out what those vulnerabilities

00:46:38,710 --> 00:46:42,400
are how do we help how do we stop

00:46:40,809 --> 00:46:44,319
campaigns quickly enough so people don't

00:46:42,400 --> 00:46:46,420
become sincere believers of the of the

00:46:44,319 --> 00:46:49,359
propaganda but also how do we start

00:46:46,420 --> 00:46:50,950
dealing with with these with these more

00:46:49,359 --> 00:46:52,809
kind of complex arrangements of

00:46:50,950 --> 00:46:54,970
disinformation that aren't just BOTS and

00:46:52,809 --> 00:46:56,559
trolls we've also got the problem of

00:46:54,970 --> 00:46:58,420
working the refs where people in power

00:46:56,559 --> 00:47:01,599
right now have benefited from the

00:46:58,420 --> 00:47:04,089
manipulation of these platforms and that

00:47:01,599 --> 00:47:05,950
any attempt to kind of stop that

00:47:04,089 --> 00:47:09,190
manipulation from happening is going to

00:47:05,950 --> 00:47:11,890
be grounds for some political folks to

00:47:09,190 --> 00:47:15,690
say that platform is biased and so I

00:47:11,890 --> 00:47:18,760
think it's really important that that

00:47:15,690 --> 00:47:21,430
technology companies understand that

00:47:18,760 --> 00:47:23,530
this is a disingenuous attack and that

00:47:21,430 --> 00:47:24,849
technology is not they'll say that

00:47:23,530 --> 00:47:26,290
you're not neutral well technology has

00:47:24,849 --> 00:47:28,299
never been neutral those of us that are

00:47:26,290 --> 00:47:30,069
in HCI have always talked about this

00:47:28,299 --> 00:47:31,720
there's no neutrality there's there's

00:47:30,069 --> 00:47:33,460
values it's not about right versus left

00:47:31,720 --> 00:47:35,290
but it's about a commitment a certain

00:47:33,460 --> 00:47:36,640
kind of values and I think one of the

00:47:35,290 --> 00:47:38,260
commitments that we need to make is to

00:47:36,640 --> 00:47:39,970
supporting democratic discourse a

00:47:38,260 --> 00:47:41,950
commitment to supporting democratic

00:47:39,970 --> 00:47:44,650
discourse and that tech platforms need

00:47:41,950 --> 00:47:46,540
to identify their own and own their own

00:47:44,650 --> 00:47:49,450
values to anchor themselves and their

00:47:46,540 --> 00:47:51,010
values to grow roots or they risk kind

00:47:49,450 --> 00:47:53,020
of getting bowled over by these

00:47:51,010 --> 00:47:56,109
political influence campaigns that are

00:47:53,020 --> 00:47:58,450
targeted at them and finally I think

00:47:56,109 --> 00:48:00,579
this is probably most this is the slide

00:47:58,450 --> 00:48:03,099
I added for you all I'm kind of most

00:48:00,579 --> 00:48:05,079
relevant for Mozilla it's like how do we

00:48:03,099 --> 00:48:07,780
help people navigate these spaces to

00:48:05,079 --> 00:48:09,700
help restore and support trust and

00:48:07,780 --> 00:48:11,319
information systems if this information

00:48:09,700 --> 00:48:12,610
is that it is sort of an attack on our

00:48:11,319 --> 00:48:16,840
trust and information

00:48:12,610 --> 00:48:19,030
how do we help better understand so how

00:48:16,840 --> 00:48:20,560
do we help users be more resilient in

00:48:19,030 --> 00:48:21,790
these spaces sort of the better

00:48:20,560 --> 00:48:24,480
understand the provenance of the

00:48:21,790 --> 00:48:27,700
information they see where it comes from

00:48:24,480 --> 00:48:29,500
for that that graph we have of the

00:48:27,700 --> 00:48:31,330
disinformation campaign that gets copied

00:48:29,500 --> 00:48:33,640
and pasted all over the information

00:48:31,330 --> 00:48:36,100
system how do we help someone say hey

00:48:33,640 --> 00:48:38,500
you know what this same article 90%

00:48:36,100 --> 00:48:40,690
appears in 90% of the same form in these

00:48:38,500 --> 00:48:44,290
seven other places and to give people

00:48:40,690 --> 00:48:45,940
those kinds of cues that to say you know

00:48:44,290 --> 00:48:47,470
no you're not actually triangulating

00:48:45,940 --> 00:48:49,150
you're seeing the same thing in four

00:48:47,470 --> 00:48:53,290
different places I think that's really

00:48:49,150 --> 00:48:54,730
important I think there's other other

00:48:53,290 --> 00:48:56,080
kinds of innovations I'd love to have

00:48:54,730 --> 00:49:01,510
conversations about what we think might

00:48:56,080 --> 00:49:03,580
work to to help to help information

00:49:01,510 --> 00:49:08,230
consumers and information participants

00:49:03,580 --> 00:49:11,110
which we all are make become a resilient

00:49:08,230 --> 00:49:12,730
in this spaces and I am hopeful but I

00:49:11,110 --> 00:49:15,490
think it's going to take a lot of change

00:49:12,730 --> 00:49:17,050
and a lot of different from a lot of

00:49:15,490 --> 00:49:19,030
different sides and I just want to chip

00:49:17,050 --> 00:49:20,920
away and not throw our hands up in there

00:49:19,030 --> 00:49:22,240
as I was told last year I've taken that

00:49:20,920 --> 00:49:24,130
to heart I think we should chip away at

00:49:22,240 --> 00:49:26,460
this problem from all sides all right

00:49:24,130 --> 00:49:26,460
thank you

00:49:28,740 --> 00:49:34,420
I'd also like to thank a host of

00:49:30,910 --> 00:49:36,850
students and collaborators very much

00:49:34,420 --> 00:49:38,830
including Emmer Arief Tom Wilson Leo

00:49:36,850 --> 00:49:40,420
student Tom and John Robinson have been

00:49:38,830 --> 00:49:43,950
sort of integral in different ways on

00:49:40,420 --> 00:49:43,950

YouTube URL: https://www.youtube.com/watch?v=498-c_SjArg


