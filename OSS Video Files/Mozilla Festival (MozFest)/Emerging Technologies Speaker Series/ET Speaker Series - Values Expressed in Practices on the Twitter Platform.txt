Title: ET Speaker Series - Values Expressed in Practices on the Twitter Platform
Publication date: 2019-10-11
Playlist: Emerging Technologies Speaker Series
Description: 
	When Users Control the Algorithms: Values Expressed in Practices on the Twitter Platform

Jenna Burrell, UC Berkeley

Recent interest in ethical AI has brought a slew of values, including fairness, into conversations about technology design. Research in the area of algorithmic fairness tends to be rooted in questions of distribution that can be subject to precise formalism and technical implementation. We seek to expand this conversation to include the experiences of people subject to algorithmic classification and decision-making. By examining tweets about the â€œTwitter algorithmâ€ we consider the wide range of concerns and desires Twitter users express. We find a concern with fairness (narrowly construed) is present, particularly in the ways users complain that the platform enacts a political bias against conservatives. However, we find another important category of concern, evident in attempts to exert control over the algorithm. Twitter users who seek control do so for a variety of reasons and their strategies often produce considerable social utility. Beyond the experiences on any single platform, we argue for better and clearer definitions of what constitutes legitimate and illegitimate control over algorithmic processes.


Jenna Burrell is an Associate Professor in the School of Information at UC Berkeley. Her research focuses on how marginalized communities adapt digital technologies to meet their needs and to pursue their goals and ideals. She is the co-director of the Algorithmic Fairness and Opacity Working Group. She is currently working on a book about rural communities that host critical Internet infrastructure such as fiber optic cables and data centers. Her first book Invisible Users: Youth in the Internet Cafes of Urban Ghana (The MIT Press) came out in May 2012. She earned a PhD in Sociology at the London School of Economics.

https://wiki.mozilla.org/Speaker_Series#Thursday.2C_October_17.2C_2019:_When_Users_Control_the_Algorithms:_Values_Expressed_in_Practices_on_the_Twitter_Platform
Captions: 
	00:00:01,550 --> 00:00:05,879
good morning good afternoon good evening

00:00:03,780 --> 00:00:08,040
my name is Joe fish kay I am principal

00:00:05,879 --> 00:00:09,570
research scientist in the emerging

00:00:08,040 --> 00:00:11,639
technologies team and today I have the

00:00:09,570 --> 00:00:13,290
pleasure of presenting to you Jenna

00:00:11,639 --> 00:00:15,089
Burrell Jenna Burrell associate

00:00:13,290 --> 00:00:17,609
professor at the School of Information

00:00:15,089 --> 00:00:21,869
at UC Berkeley and she's someone whose

00:00:17,609 --> 00:00:26,580
work I've been following for 20 years

00:00:21,869 --> 00:00:27,960
which is which is sort of terrifying one

00:00:26,580 --> 00:00:29,939
of the what I particularly like about

00:00:27,960 --> 00:00:32,399
Jenna's work and why I asked her to come

00:00:29,939 --> 00:00:34,050
and give a talk here is that she both

00:00:32,399 --> 00:00:37,260
has a strong understanding of the

00:00:34,050 --> 00:00:40,230
technology but also the social situation

00:00:37,260 --> 00:00:41,760
within it works with with with in let's

00:00:40,230 --> 00:00:43,530
try that again she both has a strong

00:00:41,760 --> 00:00:45,360
understanding of the technology and the

00:00:43,530 --> 00:00:47,789
social situation within which it works

00:00:45,360 --> 00:00:49,530
and I think that's really crucial in

00:00:47,789 --> 00:00:52,620
particular her work right now on

00:00:49,530 --> 00:00:54,300
questions like algorithmic fairness are

00:00:52,620 --> 00:00:56,039
really in line with the work that we're

00:00:54,300 --> 00:00:58,859
doing at Mozilla and the work we're

00:00:56,039 --> 00:01:01,260
doing at the foundation in particular

00:00:58,859 --> 00:01:03,239
and I think it's really interesting to

00:01:01,260 --> 00:01:04,799
see how those have come together and how

00:01:03,239 --> 00:01:06,960
that has gone from being a sort of edge

00:01:04,799 --> 00:01:08,760
case topic to something that is like

00:01:06,960 --> 00:01:10,049
right there that is sort of part of the

00:01:08,760 --> 00:01:12,299
discourse in culture in a really

00:01:10,049 --> 00:01:19,040
interesting way so without any further

00:01:12,299 --> 00:01:23,880
ado thank you thank you Joe fish yeah so

00:01:19,040 --> 00:01:26,280
my talk today concerns the way users of

00:01:23,880 --> 00:01:28,170
online platforms think about and engage

00:01:26,280 --> 00:01:31,799
with algorithms in this very explicit

00:01:28,170 --> 00:01:35,009
way talking about attending to these

00:01:31,799 --> 00:01:36,869
things they they call algorithms and I'm

00:01:35,009 --> 00:01:39,950
especially looking at algorithms that

00:01:36,869 --> 00:01:42,240
shape content the content users see so

00:01:39,950 --> 00:01:44,310
these are sometimes called content

00:01:42,240 --> 00:01:45,869
curation algorithms I think that there

00:01:44,310 --> 00:01:47,939
are lots of examples of that so search

00:01:45,869 --> 00:01:50,600
engine search engines produce they

00:01:47,939 --> 00:01:52,049
curate content they order and rank it

00:01:50,600 --> 00:01:53,850
timeline

00:01:52,049 --> 00:01:55,619
Twitter's timeline I'll be talking about

00:01:53,850 --> 00:01:58,680
that today the Facebook newsfeed

00:01:55,619 --> 00:02:01,710
lots and lots of examples of content

00:01:58,680 --> 00:02:03,960
curation algorithms but also content

00:02:01,710 --> 00:02:05,540
moderation algorithms that's a little

00:02:03,960 --> 00:02:07,350
more tricky to talk about because

00:02:05,540 --> 00:02:09,000
content moderation is not always

00:02:07,350 --> 00:02:09,390
algorithmically handled it's handled by

00:02:09,000 --> 00:02:11,610
human

00:02:09,390 --> 00:02:13,260
but there seems to be an algorithmic

00:02:11,610 --> 00:02:16,650
component to that as well

00:02:13,260 --> 00:02:19,170
there is certainly a rising awareness

00:02:16,650 --> 00:02:22,140
among the general public among Internet

00:02:19,170 --> 00:02:24,720
users of algorithms some of that has to

00:02:22,140 --> 00:02:27,000
do with just an onslaught of mass media

00:02:24,720 --> 00:02:28,950
coverage that's using that term

00:02:27,000 --> 00:02:33,540
there's scholarship there's advocacy

00:02:28,950 --> 00:02:35,850
efforts more and more than internet

00:02:33,540 --> 00:02:37,920
using public I think has some notion of

00:02:35,850 --> 00:02:41,400
what an algorithm is and may even use

00:02:37,920 --> 00:02:43,080
that word it's like six months ago I was

00:02:41,400 --> 00:02:45,959
getting a haircut and I was talking to

00:02:43,080 --> 00:02:47,790
my hairdresser about this talk I was

00:02:45,959 --> 00:02:51,170
gonna go give it Virginia Tech and I was

00:02:47,790 --> 00:02:53,400
explaining it and kind of it's you know

00:02:51,170 --> 00:02:55,730
non-technical language and she said to

00:02:53,400 --> 00:02:58,200
me oh you're talking about algorithms

00:02:55,730 --> 00:03:00,269
that that was a real moment of clarity

00:02:58,200 --> 00:03:01,860
for me some of it had to do with she's

00:03:00,269 --> 00:03:03,780
very social media savvy she was

00:03:01,860 --> 00:03:05,070
advertising herself on social media

00:03:03,780 --> 00:03:07,590
she'd taken one of these marketing

00:03:05,070 --> 00:03:10,190
classes so that's one example of how

00:03:07,590 --> 00:03:12,690
that that word is being mainstreamed

00:03:10,190 --> 00:03:15,480
around this more public conversation

00:03:12,690 --> 00:03:17,070
about algorithms there is a sense that

00:03:15,480 --> 00:03:18,840
there's some kind of problem with

00:03:17,070 --> 00:03:21,780
algorithms particularly machine learning

00:03:18,840 --> 00:03:23,700
and AI algorithms there's concern about

00:03:21,780 --> 00:03:26,640
the data that's being fed into those

00:03:23,700 --> 00:03:28,620
algorithms being skewed there's concern

00:03:26,640 --> 00:03:30,540
that they're biased or unfair in some

00:03:28,620 --> 00:03:33,930
way the fairness with make fairness

00:03:30,540 --> 00:03:36,690
discussion is related to that in some

00:03:33,930 --> 00:03:40,380
really high stakes domains like criminal

00:03:36,690 --> 00:03:42,450
justice there's discussion about racist

00:03:40,380 --> 00:03:44,480
algorithms there in particular there's

00:03:42,450 --> 00:03:47,910
been a lot of conversation about

00:03:44,480 --> 00:03:49,350
recidivism risk algorithms these you

00:03:47,910 --> 00:03:51,570
know functions that basically say

00:03:49,350 --> 00:03:53,610
whether a an inmate or someone who's

00:03:51,570 --> 00:03:57,000
about to get parole is high risk for

00:03:53,610 --> 00:03:59,579
reoffending and some research that has

00:03:57,000 --> 00:04:04,140
shown that for example black parolees

00:03:59,579 --> 00:04:06,600
are more likely to be mistakenly labeled

00:04:04,140 --> 00:04:09,030
as high risk than white parolees for

00:04:06,600 --> 00:04:11,880
example but also lots of other high

00:04:09,030 --> 00:04:15,359
stakes domains vetting job applicants

00:04:11,880 --> 00:04:17,100
college admissions lending credit and

00:04:15,359 --> 00:04:19,230
lending those those are all domains

00:04:17,100 --> 00:04:22,130
where these conversations are happening

00:04:19,230 --> 00:04:25,350
and I think the scholarship broadly is

00:04:22,130 --> 00:04:28,160
AI FX is that is the term that's kind of

00:04:25,350 --> 00:04:30,240
encompassing a lot of those issues I

00:04:28,160 --> 00:04:33,030
have this hunch I've been participating

00:04:30,240 --> 00:04:34,980
in the fairness debate for a while for

00:04:33,030 --> 00:04:36,930
at least a few years now and I had a

00:04:34,980 --> 00:04:39,240
hunch that some of the alarm was really

00:04:36,930 --> 00:04:41,040
not about fairness which suggestively

00:04:39,240 --> 00:04:42,750
redistribute things it'll be everything

00:04:41,040 --> 00:04:45,120
will be hunky-dory and it'll all be fine

00:04:42,750 --> 00:04:46,800
I had a hunch that it was more

00:04:45,120 --> 00:04:49,410
fundamentally about what it means for

00:04:46,800 --> 00:04:52,770
humans to be subject to automation just

00:04:49,410 --> 00:04:54,750
that fundamental fact which suggested

00:04:52,770 --> 00:04:57,240
that even if we kind of validate and

00:04:54,750 --> 00:04:59,520
prove a fair algorithm it won't entirely

00:04:57,240 --> 00:05:02,760
erase some of the anxieties and problems

00:04:59,520 --> 00:05:05,490
that people attach to algorithmic

00:05:02,760 --> 00:05:08,100
decision-making and I was really

00:05:05,490 --> 00:05:10,640
interested in exploring instead matters

00:05:08,100 --> 00:05:12,480
of human autonomy control

00:05:10,640 --> 00:05:15,380
self-determination that seemed to be

00:05:12,480 --> 00:05:18,000
somewhat less explored in the AI ethics

00:05:15,380 --> 00:05:21,630
discussion and so that's what I said

00:05:18,000 --> 00:05:23,340
about with several of our fantastic PhD

00:05:21,630 --> 00:05:26,520
students at the high school to

00:05:23,340 --> 00:05:28,680
investigate so there are many many

00:05:26,520 --> 00:05:30,440
acronyms in the space here's ours

00:05:28,680 --> 00:05:33,870
there's fat there's faith there's fact

00:05:30,440 --> 00:05:36,720
we're a fog the algorithmic fairness and

00:05:33,870 --> 00:05:38,760
opacity group a little sense of location

00:05:36,720 --> 00:05:40,290
there's fog there's there's the berkeley

00:05:38,760 --> 00:05:42,720
campus in the foreground and the fog

00:05:40,290 --> 00:05:44,250
rolling in in the background if you're

00:05:42,720 --> 00:05:47,580
interested in what we're doing in this

00:05:44,250 --> 00:05:49,710
space there's a website we're the

00:05:47,580 --> 00:05:52,860
algorithmic fairness and opacity working

00:05:49,710 --> 00:05:56,250
group which if you like welsh it's kind

00:05:52,860 --> 00:05:58,470
of sounds like a welsh word a fog but we

00:05:56,250 --> 00:06:00,960
just kind of dropped the w and it's and

00:05:58,470 --> 00:06:04,350
it's a silent w so we call it that we

00:06:00,960 --> 00:06:06,660
call it eight five we've been running a

00:06:04,350 --> 00:06:08,480
fog for hmmmm I think we're in our third

00:06:06,660 --> 00:06:13,500
year we're funded by Google

00:06:08,480 --> 00:06:14,820
thanks Google we are kind of a just a

00:06:13,500 --> 00:06:17,880
small working group at this point with

00:06:14,820 --> 00:06:19,710
aspirations to grow larger you can see

00:06:17,880 --> 00:06:22,110
in the the acronym we've selected we've

00:06:19,710 --> 00:06:24,030
highlighted two concerns controls not

00:06:22,110 --> 00:06:26,190
one of them autonomy self-determination

00:06:24,030 --> 00:06:29,130
we picked like everyone else fairness

00:06:26,190 --> 00:06:30,960
and opacity or transparency those are

00:06:29,130 --> 00:06:32,950
some of the most common kind of values

00:06:30,960 --> 00:06:38,220
or concerns that are raised in this

00:06:32,950 --> 00:06:40,810
space of AIX I think I want to just say

00:06:38,220 --> 00:06:42,820
you know take a moment to kind of

00:06:40,810 --> 00:06:44,410
contemplate why it is that fairness has

00:06:42,820 --> 00:06:47,080
gotten so much attention why that's

00:06:44,410 --> 00:06:48,880
really gotten traction above the many

00:06:47,080 --> 00:06:52,870
values and concerns we might be

00:06:48,880 --> 00:06:54,400
discussing in this space I think it it

00:06:52,870 --> 00:06:56,290
you know in part it has to do with the

00:06:54,400 --> 00:06:59,470
fact that fairness is really amenable to

00:06:56,290 --> 00:07:02,170
formalism and so you see the same people

00:06:59,470 --> 00:07:04,240
kind of designing the biased algorithms

00:07:02,170 --> 00:07:07,300
are now able to kind of weigh in and say

00:07:04,240 --> 00:07:09,730
well this is how we could correct it so

00:07:07,300 --> 00:07:11,710
the kind of deep how are the makers have

00:07:09,730 --> 00:07:15,190
a role to play they can we just

00:07:11,710 --> 00:07:17,860
precisely define fairness then we can

00:07:15,190 --> 00:07:20,320
kind of analyze to what extent an

00:07:17,860 --> 00:07:22,180
algorithm is or is not fair we can maybe

00:07:20,320 --> 00:07:24,700
find ways to correct it and then we have

00:07:22,180 --> 00:07:26,590
good new new algorithms that are more

00:07:24,700 --> 00:07:32,980
fair so the problem of algorithms is

00:07:26,590 --> 00:07:35,020
solved with algorithms I think also the

00:07:32,980 --> 00:07:36,490
fairness discussion has often focused on

00:07:35,020 --> 00:07:39,760
things like error rates like the

00:07:36,490 --> 00:07:41,650
distribution of error rates and that is

00:07:39,760 --> 00:07:43,560
that ties into accuracy that's sort of

00:07:41,650 --> 00:07:46,810
in the world of machine learning and AI

00:07:43,560 --> 00:07:49,350
accuracy is sort of the core concern of

00:07:46,810 --> 00:07:52,330
kind of properly functioning algorithms

00:07:49,350 --> 00:07:55,020
so it just fits in well into the space

00:07:52,330 --> 00:07:56,950
that's already inhabited by people

00:07:55,020 --> 00:07:58,930
working in machine learning and AI

00:07:56,950 --> 00:08:00,670
that's not a good reason to kind of

00:07:58,930 --> 00:08:02,350
leave it there I think we should you

00:08:00,670 --> 00:08:06,220
know as I'm arguing today broaden beyond

00:08:02,350 --> 00:08:08,410
that beyond that kind of narrow focus on

00:08:06,220 --> 00:08:10,750
particular values that are easy to kind

00:08:08,410 --> 00:08:12,730
of grapple with and control is the one

00:08:10,750 --> 00:08:15,630
I'm advocating for today it's not one of

00:08:12,730 --> 00:08:19,330
these marquee values in the AI debate

00:08:15,630 --> 00:08:21,100
and so with these PhD students of mine

00:08:19,330 --> 00:08:22,510
sort of wait and just think how can we

00:08:21,100 --> 00:08:24,460
how can we start to grapple with

00:08:22,510 --> 00:08:27,550
questions of control how can we find

00:08:24,460 --> 00:08:29,200
some kind of evidentiary basis to think

00:08:27,550 --> 00:08:33,040
about what it is what it means to people

00:08:29,200 --> 00:08:36,669
using online platforms and what value it

00:08:33,040 --> 00:08:39,300
might offer okay so the outline I've

00:08:36,669 --> 00:08:42,190
already sort of tackled one which is

00:08:39,300 --> 00:08:44,660
suggesting that control as has been sort

00:08:42,190 --> 00:08:46,220
of a deep prioritized value in AI FX

00:08:44,660 --> 00:08:49,639
apps and it's just it's not one of those

00:08:46,220 --> 00:08:53,680
marquee values I'm gonna now go on to

00:08:49,639 --> 00:08:56,839
talk about with a particular taste study

00:08:53,680 --> 00:08:58,730
some evidence that users certainly do

00:08:56,839 --> 00:09:01,459
care about control and in fact they

00:08:58,730 --> 00:09:04,639
attempt to exert control in these really

00:09:01,459 --> 00:09:06,439
creative ways over algorithms we used a

00:09:04,639 --> 00:09:08,149
data set of Twitter data which I'll get

00:09:06,439 --> 00:09:10,220
into in another slide and then I'm gonna

00:09:08,149 --> 00:09:13,850
after I talk about that study and what

00:09:10,220 --> 00:09:16,040
we found I'm gonna talk about um apart

00:09:13,850 --> 00:09:18,980
from just control being in and of itself

00:09:16,040 --> 00:09:21,439
an important thing the kind of broader

00:09:18,980 --> 00:09:23,870
social utility that is created when

00:09:21,439 --> 00:09:27,230
users seize control of the algorithms

00:09:23,870 --> 00:09:29,509
and I wanted to acknowledge my great

00:09:27,230 --> 00:09:31,670
collaborators on this this project of

00:09:29,509 --> 00:09:34,579
co-authors of a CSE w paper that I'm

00:09:31,670 --> 00:09:38,860
presenting in November three PhD student

00:09:34,579 --> 00:09:38,860
Zoe Kahn and Jonas and Daniel Griffin

00:09:40,420 --> 00:09:49,160
okay so there is some literature about

00:09:46,670 --> 00:09:51,860
control it's not necessarily in the AI

00:09:49,160 --> 00:09:54,800
ethics space but I wanted to just kind

00:09:51,860 --> 00:09:56,389
of point to it to show where I'm coming

00:09:54,800 --> 00:09:58,130
from and how I'm doing something

00:09:56,389 --> 00:10:00,769
distinct from these prior literature's I

00:09:58,130 --> 00:10:03,589
think the most compatible work is tends

00:10:00,769 --> 00:10:05,899
to be in the law scholarship it's often

00:10:03,589 --> 00:10:09,259
sort of buried in papers about AI ethics

00:10:05,899 --> 00:10:11,329
so there's discussions about human

00:10:09,259 --> 00:10:14,449
autonomy issues of self-determination

00:10:11,329 --> 00:10:16,610
the right for decisions that are made to

00:10:14,449 --> 00:10:18,610
involve a human in the loop so if you

00:10:16,610 --> 00:10:21,079
feel you've been mistreated by a

00:10:18,610 --> 00:10:23,149
automated decision there's some form of

00:10:21,079 --> 00:10:27,709
recourse there's a human you can appeal

00:10:23,149 --> 00:10:30,019
to again that's that's certainly in the

00:10:27,709 --> 00:10:35,089
law scholarship it tends not to be as

00:10:30,019 --> 00:10:36,290
central in the AI ethics literature but

00:10:35,089 --> 00:10:38,899
it's there and that is really where I'm

00:10:36,290 --> 00:10:41,269
aligning my work in this space is to

00:10:38,899 --> 00:10:44,870
think about control as a as a value as

00:10:41,269 --> 00:10:47,209
that kind of dignitary concern there's a

00:10:44,870 --> 00:10:48,949
literature that goes back to the 90s

00:10:47,209 --> 00:10:51,649
there's sort of a recent revival around

00:10:48,949 --> 00:10:54,339
this idea of intelligence augmentation I

00:10:51,649 --> 00:10:57,290
love that stuff is that it's a great

00:10:54,339 --> 00:10:57,890
space to be working in it the idea is

00:10:57,290 --> 00:11:00,290
that

00:10:57,890 --> 00:11:04,279
machine learning and AI can produce all

00:11:00,290 --> 00:11:06,740
kinds of great new possibilities but

00:11:04,279 --> 00:11:08,269
human what human ways of thinking human

00:11:06,740 --> 00:11:10,310
cognition is a distinctly different

00:11:08,269 --> 00:11:12,890
thing and it has its own strengths and

00:11:10,310 --> 00:11:15,170
if you can combine machine intelligence

00:11:12,890 --> 00:11:16,880
and human intelligence you have

00:11:15,170 --> 00:11:20,480
something even better right you have

00:11:16,880 --> 00:11:23,390
some kind of combination of talents and

00:11:20,480 --> 00:11:25,040
abilities I think what I would how I

00:11:23,390 --> 00:11:26,930
would distinguish my work in the space

00:11:25,040 --> 00:11:29,570
is that that work tends to focus on

00:11:26,930 --> 00:11:31,880
cognition and kind of the efficiency

00:11:29,570 --> 00:11:34,370
gains that can be realized by bringing

00:11:31,880 --> 00:11:36,050
together machines in humans and that's

00:11:34,370 --> 00:11:38,750
just a different kind of argument than

00:11:36,050 --> 00:11:42,190
one that's really about control and

00:11:38,750 --> 00:11:46,490
human self-determination as a as a right

00:11:42,190 --> 00:11:48,050
or as a value there is also a I was

00:11:46,490 --> 00:11:51,160
realizing this last night that I had

00:11:48,050 --> 00:11:54,470
completely omitted the AI safety people

00:11:51,160 --> 00:11:55,970
which is you know some argue that's a

00:11:54,470 --> 00:11:57,620
little bit fringe but it's this idea

00:11:55,970 --> 00:11:59,360
that we will have super intelligent

00:11:57,620 --> 00:12:01,550
machines and we need to have some kind

00:11:59,360 --> 00:12:04,310
of human oversight a way for humans to

00:12:01,550 --> 00:12:06,620
kind of pull the plug and make sure that

00:12:04,310 --> 00:12:08,149
the machines don't you know develop an

00:12:06,620 --> 00:12:10,760
intelligence that leads to them killing

00:12:08,149 --> 00:12:13,250
us very famous proponents of this

00:12:10,760 --> 00:12:16,640
perspective Elon Musk's you know Hawking

00:12:13,250 --> 00:12:18,470
at my university Stewart Russell is

00:12:16,640 --> 00:12:21,290
talking a lot about this there's the

00:12:18,470 --> 00:12:23,720
future of humanity Institute at Oxford

00:12:21,290 --> 00:12:26,890
so that's there's a control theme in

00:12:23,720 --> 00:12:29,410
there for sure that's also not where I'm

00:12:26,890 --> 00:12:32,810
kind of aligning my work in the space

00:12:29,410 --> 00:12:37,510
and then there's a really interesting

00:12:32,810 --> 00:12:39,790
and and ever richer literature on

00:12:37,510 --> 00:12:41,600
algorithmic gaming or strategic

00:12:39,790 --> 00:12:45,110
classification I've just been reading

00:12:41,600 --> 00:12:47,329
more and more in that space professor at

00:12:45,110 --> 00:12:49,250
UC Berkeley in this computer science

00:12:47,329 --> 00:12:50,810
department more it's hard who I've been

00:12:49,250 --> 00:12:53,449
talking to is doing a lot of work in

00:12:50,810 --> 00:12:56,260
this space and that's you know a

00:12:53,449 --> 00:12:58,339
literature that acknowledges humans as

00:12:56,260 --> 00:13:00,160
attempting to exert control over

00:12:58,339 --> 00:13:02,890
algorithms and tries to kind of

00:13:00,160 --> 00:13:04,850
understand in what ways that control is

00:13:02,890 --> 00:13:07,130
legitimate or illegitimate

00:13:04,850 --> 00:13:10,190
I mean gaming generally by definition

00:13:07,130 --> 00:13:11,690
means kind of illegitimate control over

00:13:10,190 --> 00:13:14,680
some sort of algorithmic decision-making

00:13:11,690 --> 00:13:16,520
process and I think it's worth just

00:13:14,680 --> 00:13:19,340
fleshing out a couple of definitions

00:13:16,520 --> 00:13:22,570
because those seem the most kind of

00:13:19,340 --> 00:13:25,070
clear and easy to grapple with of

00:13:22,570 --> 00:13:26,870
gaining control as gaming so there's

00:13:25,070 --> 00:13:29,540
this piece by Moore it's hard who I just

00:13:26,870 --> 00:13:33,860
mentioned from a few years back where he

00:13:29,540 --> 00:13:36,260
defines gaming as harm to it the harm is

00:13:33,860 --> 00:13:40,310
to the prediction accuracy of an of a

00:13:36,260 --> 00:13:43,130
classifier its gaming is something that

00:13:40,310 --> 00:13:46,370
where users seize control in a way that

00:13:43,130 --> 00:13:50,450
gives them an unintended advantage and

00:13:46,370 --> 00:13:52,250
you know the harm is to the accuracy of

00:13:50,450 --> 00:13:54,500
the classifier the accuracy of the

00:13:52,250 --> 00:13:55,970
algorithm to me I think these

00:13:54,500 --> 00:13:57,350
definitions are useful because if you

00:13:55,970 --> 00:13:58,960
have a nice clear definition like that

00:13:57,350 --> 00:14:01,370
you can start to think about ways that

00:13:58,960 --> 00:14:04,100
control over algorithms might not be

00:14:01,370 --> 00:14:07,730
illegitimate might be valid so for

00:14:04,100 --> 00:14:09,710
example in that place that definition if

00:14:07,730 --> 00:14:11,590
you're manipulating an algorithm in such

00:14:09,710 --> 00:14:14,720
a way that it doesn't harm the accuracy

00:14:11,590 --> 00:14:15,740
or give you some unintended advantage

00:14:14,720 --> 00:14:17,360
whatever that might mean

00:14:15,740 --> 00:14:19,490
then that's controlled but it's not

00:14:17,360 --> 00:14:21,940
necessarily gaming and I think just kind

00:14:19,490 --> 00:14:27,260
of keeping space open to talk about

00:14:21,940 --> 00:14:29,090
control as a legitimate practice is

00:14:27,260 --> 00:14:31,040
really important and that's I'm going to

00:14:29,090 --> 00:14:32,270
talk about that today

00:14:31,040 --> 00:14:34,130
the other piece this is a law

00:14:32,270 --> 00:14:36,560
scholarship art this is probably the the

00:14:34,130 --> 00:14:38,630
most relevant piece I was able to find

00:14:36,560 --> 00:14:40,160
that I could kind of link into it's a

00:14:38,630 --> 00:14:42,590
piece by BAM bar and it's our ski it's

00:14:40,160 --> 00:14:44,060
only a year old it's called the

00:14:42,590 --> 00:14:45,980
algorithm game I would definitely

00:14:44,060 --> 00:14:49,190
recommend it it's very comprehensive as

00:14:45,980 --> 00:14:50,840
Law Review articles often are their

00:14:49,190 --> 00:14:53,839
definition really rests on the idea that

00:14:50,840 --> 00:14:55,820
often what's being measured is not the

00:14:53,839 --> 00:14:57,640
thing you're interested in but a proxy

00:14:55,820 --> 00:15:01,339
right so if you're trying to measure

00:14:57,640 --> 00:15:03,470
college readiness you do like an s8 like

00:15:01,339 --> 00:15:05,630
a standardized test right and we all

00:15:03,470 --> 00:15:08,360
kind of can understand that yeah how

00:15:05,630 --> 00:15:09,830
well you do on the SAT is kind of

00:15:08,360 --> 00:15:12,980
related to how ready you are for college

00:15:09,830 --> 00:15:16,160
but not exactly and gaming is where you

00:15:12,980 --> 00:15:18,860
exploit the proxy so if you get this is

00:15:16,160 --> 00:15:20,600
recently in the news if you get extra

00:15:18,860 --> 00:15:21,139
time to take the test if you have

00:15:20,600 --> 00:15:23,329
someone

00:15:21,139 --> 00:15:25,759
take it for you you're manipulating the

00:15:23,329 --> 00:15:27,739
proxy the SAT score and it's not

00:15:25,759 --> 00:15:29,299
actually reflective of the thing that's

00:15:27,739 --> 00:15:33,290
you're trying to measure which is which

00:15:29,299 --> 00:15:36,230
is college readiness yes so then of

00:15:33,290 --> 00:15:38,179
course if that's what gaming is if

00:15:36,230 --> 00:15:40,579
you're you are actually changing your

00:15:38,179 --> 00:15:42,499
college readiness and you're getting a

00:15:40,579 --> 00:15:45,169
better score as a result of that that's

00:15:42,499 --> 00:15:49,519
a way of exerting control over the

00:15:45,169 --> 00:15:54,339
scoring mechanism without gaming it okay

00:15:49,519 --> 00:15:54,339
so moving on from some definition work

00:15:54,790 --> 00:15:58,970
we were interested in control and from

00:15:57,470 --> 00:16:01,220
the beginning self-determination how

00:15:58,970 --> 00:16:04,249
people relate explicitly to algorithms

00:16:01,220 --> 00:16:06,139
and then we thought well how do we how

00:16:04,249 --> 00:16:08,809
do we find out more about that how do we

00:16:06,139 --> 00:16:10,639
get some kind of evidence what we ended

00:16:08,809 --> 00:16:12,230
up doing I will I will say right up

00:16:10,639 --> 00:16:14,619
front that I was really interested in

00:16:12,230 --> 00:16:17,749
these kind of high-stakes domains like

00:16:14,619 --> 00:16:19,279
criminal justice so we went and we

00:16:17,749 --> 00:16:21,739
looked at like Twitter data that's not

00:16:19,279 --> 00:16:23,299
quite the same domain but it was a good

00:16:21,739 --> 00:16:26,029
place to start I think it was a nice

00:16:23,299 --> 00:16:32,839
modest way to kind of get started on

00:16:26,029 --> 00:16:35,839
some of these questions we collected

00:16:32,839 --> 00:16:38,269
data over a 12 week period this was a

00:16:35,839 --> 00:16:41,329
mmm about it actually almost exactly a

00:16:38,269 --> 00:16:43,879
year ago and what we did was to collect

00:16:41,329 --> 00:16:45,949
all the tweets that referenced the

00:16:43,879 --> 00:16:47,660
Twitter algorithm and variance so

00:16:45,949 --> 00:16:50,350
Twitter's algorithm Twitter algorithms

00:16:47,660 --> 00:16:55,759
right but those two words in sequence

00:16:50,350 --> 00:16:57,879
and we ended up with eight thousand 875

00:16:55,759 --> 00:16:59,989
tweets

00:16:57,879 --> 00:17:02,239
they're pretty evenly I mean there's

00:16:59,989 --> 00:17:03,679
there wasn't a real concentration in

00:17:02,239 --> 00:17:06,769
particular account so you can see that

00:17:03,679 --> 00:17:08,269
there are 8,000 41 accounts user

00:17:06,769 --> 00:17:11,839
accounts that produced those tweets

00:17:08,269 --> 00:17:14,329
which was I think was a good sign you

00:17:11,839 --> 00:17:16,429
know half of them were about half where

00:17:14,329 --> 00:17:20,600
original tweets about half were reply

00:17:16,429 --> 00:17:23,089
tweets and I wanted to actually

00:17:20,600 --> 00:17:25,250
reference Kate star birds talk last week

00:17:23,089 --> 00:17:27,260
because I one thing I noticed and felt

00:17:25,250 --> 00:17:28,909
affirmed by with she was like we spent a

00:17:27,260 --> 00:17:30,769
lot of time actually reading like

00:17:28,909 --> 00:17:32,419
thousands of tweets like reading them by

00:17:30,769 --> 00:17:35,260
hand and that's really what this project

00:17:32,419 --> 00:17:38,900
was we didn't read all eight

00:17:35,260 --> 00:17:42,560
875 we read a sample we read in the end

00:17:38,900 --> 00:17:45,620
we read and tried to code 911 tweets

00:17:42,560 --> 00:17:49,910
between the four researchers on the

00:17:45,620 --> 00:17:51,470
project so that I'm glad to see that

00:17:49,910 --> 00:17:52,910
Cait agrees that's a valid way of doing

00:17:51,470 --> 00:17:55,220
this research and and I found it really

00:17:52,910 --> 00:17:57,320
illuminating to just sit and read tweet

00:17:55,220 --> 00:17:59,150
after tweet after tweet and see what why

00:17:57,320 --> 00:18:00,680
are people invoking the Twitter

00:17:59,150 --> 00:18:02,780
algorithm what are they trying to say

00:18:00,680 --> 00:18:04,160
what are they trying to do and that was

00:18:02,780 --> 00:18:06,110
really the question we started with what

00:18:04,160 --> 00:18:09,260
are Twitter users doing when they invoke

00:18:06,110 --> 00:18:12,890
the algorithm so to start what we did

00:18:09,260 --> 00:18:14,510
with our you know 911 well some small

00:18:12,890 --> 00:18:16,430
set of that maybe a hundred we were just

00:18:14,510 --> 00:18:18,200
trying to develop a set of codes

00:18:16,430 --> 00:18:21,530
inductively that would kind of thematic

00:18:18,200 --> 00:18:23,150
ly categorize those tweets answering the

00:18:21,530 --> 00:18:24,950
question what are Twitter users doing

00:18:23,150 --> 00:18:26,960
when they invoke the algorithm are they

00:18:24,950 --> 00:18:31,130
complaining about it are they trying to

00:18:26,960 --> 00:18:33,470
understand it better and what we got out

00:18:31,130 --> 00:18:35,780
of that was a sense that can there was a

00:18:33,470 --> 00:18:38,930
control theme for sure we definitely

00:18:35,780 --> 00:18:40,490
found a you know a subset of tweets

00:18:38,930 --> 00:18:42,680
within our sample that were about

00:18:40,490 --> 00:18:44,270
feeling like people people feeling like

00:18:42,680 --> 00:18:46,580
they didn't have control over the

00:18:44,270 --> 00:18:49,610
algorithms or trying to exert control in

00:18:46,580 --> 00:18:51,260
the way they wrote tweets that's what

00:18:49,610 --> 00:18:54,880
I'm gonna spend most of my time kind of

00:18:51,260 --> 00:18:59,300
delving into but this was our method

00:18:54,880 --> 00:19:03,200
there were so our process we downloaded

00:18:59,300 --> 00:19:07,640
we took the subset of tweets I said 911

00:19:03,200 --> 00:19:09,860
tweets the end data set was 718 because

00:19:07,640 --> 00:19:11,540
some of those tweets ended up being dead

00:19:09,860 --> 00:19:13,100
links we actually went and followed the

00:19:11,540 --> 00:19:16,760
links for each tweet because we wanted

00:19:13,100 --> 00:19:18,230
to see the tweet in context thinking

00:19:16,760 --> 00:19:21,080
about kind of the ethics of it if a

00:19:18,230 --> 00:19:23,300
tweet was deleted if it was now in a

00:19:21,080 --> 00:19:25,040
protected account if they're just ways

00:19:23,300 --> 00:19:27,680
that the author signal that they didn't

00:19:25,040 --> 00:19:30,080
want it to be public we removed it from

00:19:27,680 --> 00:19:31,700
our data set and then somewhere just

00:19:30,080 --> 00:19:33,170
uninterpretable like we just couldn't

00:19:31,700 --> 00:19:34,130
really understand what the person was

00:19:33,170 --> 00:19:36,560
saying I mean the nice thing about

00:19:34,130 --> 00:19:39,580
Twitter data is that people are

00:19:36,560 --> 00:19:42,620
generally writing for an imagined human

00:19:39,580 --> 00:19:44,420
mass public audience so for the most

00:19:42,620 --> 00:19:45,500
part we felt we could understand what

00:19:44,420 --> 00:19:46,910
people were trying to say in those

00:19:45,500 --> 00:19:48,510
tweets but in some cases we really

00:19:46,910 --> 00:19:51,670
couldn't

00:19:48,510 --> 00:19:55,990
okay so this is a subset of the ten

00:19:51,670 --> 00:19:59,350
codes that we developed there were four

00:19:55,990 --> 00:20:01,150
meta categories the first is kind of an

00:19:59,350 --> 00:20:02,740
explanation category education

00:20:01,150 --> 00:20:05,800
explanation so where people are trying

00:20:02,740 --> 00:20:07,900
to they're talking about the Twitter

00:20:05,800 --> 00:20:09,670
algorithm in an effort to either find

00:20:07,900 --> 00:20:11,050
information about how it works or to

00:20:09,670 --> 00:20:14,440
share information about how they think

00:20:11,050 --> 00:20:16,870
it works there were a set of complaints

00:20:14,440 --> 00:20:18,910
for codes under the complaints category

00:20:16,870 --> 00:20:20,770
that's not surprising people talking

00:20:18,910 --> 00:20:21,850
about the Twitter algorithm to say it's

00:20:20,770 --> 00:20:22,990
doing something weird it's doing

00:20:21,850 --> 00:20:24,400
something wrong they don't they don't

00:20:22,990 --> 00:20:26,830
like it

00:20:24,400 --> 00:20:30,250
there was a meta category we called

00:20:26,830 --> 00:20:32,140
relationship management which that sort

00:20:30,250 --> 00:20:33,550
of was the seed for an idea we developed

00:20:32,140 --> 00:20:36,160
further through the course of the

00:20:33,550 --> 00:20:38,590
project the two tweets I think are good

00:20:36,160 --> 00:20:40,330
examples of relationship management so

00:20:38,590 --> 00:20:43,300
people sometimes felt like they had to

00:20:40,330 --> 00:20:45,310
intervene they saw Twitter as a platform

00:20:43,300 --> 00:20:48,070
for social interaction for community

00:20:45,310 --> 00:20:49,960
building sometimes they felt they were

00:20:48,070 --> 00:20:52,540
undermined by the algorithm it was

00:20:49,960 --> 00:20:54,190
connecting them to people that they you

00:20:52,540 --> 00:20:55,870
know they're trying to explain why they

00:20:54,190 --> 00:20:57,550
aren't creepy why they're responding to

00:20:55,870 --> 00:21:00,190
someone's tweet who they don't know or

00:20:57,550 --> 00:21:01,420
if they had been ignoring someone

00:21:00,190 --> 00:21:03,430
because they hadn't been seeing their

00:21:01,420 --> 00:21:05,830
tweets they would invoke the algorithm

00:21:03,430 --> 00:21:08,980
to explain why they hadn't been a better

00:21:05,830 --> 00:21:10,660
you know friend on Twitter so that's the

00:21:08,980 --> 00:21:12,910
first tweet reflects that kind of

00:21:10,660 --> 00:21:14,650
relationship management someone says I

00:21:12,910 --> 00:21:16,690
this person says I haven't seen I

00:21:14,650 --> 00:21:18,460
realized I haven't seen a Twitter friend

00:21:16,690 --> 00:21:19,990
tweet in awhile but I realized that

00:21:18,460 --> 00:21:22,030
might be because Twitter's algorithm

00:21:19,990 --> 00:21:23,980
hasn't been surfacing their tweets so I

00:21:22,030 --> 00:21:25,450
do a search for their account and sure

00:21:23,980 --> 00:21:27,850
enough they've been tweeting good tweets

00:21:25,450 --> 00:21:30,580
I haven't been supporting so you can see

00:21:27,850 --> 00:21:32,740
the sense of obligation to be a good

00:21:30,580 --> 00:21:35,410
Twitter friend I mean this tweet even

00:21:32,740 --> 00:21:37,300
uses the phrase Twitter friend and the

00:21:35,410 --> 00:21:39,400
sense that there's some kind of act of

00:21:37,300 --> 00:21:42,910
support they should be carrying out they

00:21:39,400 --> 00:21:46,270
haven't been doing and then the second

00:21:42,910 --> 00:21:48,580
one you know I deeply apologize person

00:21:46,270 --> 00:21:50,680
says to anybody who's seen gay porn as a

00:21:48,580 --> 00:21:52,210
result of me liking it Twitter

00:21:50,680 --> 00:21:54,520
algorithms are hard at work once again

00:21:52,210 --> 00:21:58,180
so make end of making a joke to friends

00:21:54,520 --> 00:21:59,710
about you know the things that you you

00:21:58,180 --> 00:22:00,260
like that you support on Twitter and

00:21:59,710 --> 00:22:02,150
then get it

00:22:00,260 --> 00:22:03,410
exposed to your Twitter friends and

00:22:02,150 --> 00:22:05,390
maybe you didn't intend for that to

00:22:03,410 --> 00:22:10,340
happen so explaining and kind of trying

00:22:05,390 --> 00:22:12,170
to mollify any any you know less than

00:22:10,340 --> 00:22:15,530
positive reactions your Twitter friends

00:22:12,170 --> 00:22:16,910
after that and that's an example of one

00:22:15,530 --> 00:22:18,770
I didn't put all the codes but one of

00:22:16,910 --> 00:22:21,110
the code codes we called shifting blame

00:22:18,770 --> 00:22:22,520
from person to algorithm that's an

00:22:21,110 --> 00:22:23,840
example it's not my fault

00:22:22,520 --> 00:22:26,780
it's the Twitter algorithm that's doing

00:22:23,840 --> 00:22:29,660
algorithm that's doing this and then the

00:22:26,780 --> 00:22:31,490
final category exerting control over

00:22:29,660 --> 00:22:34,760
algorithms that's really where we spend

00:22:31,490 --> 00:22:37,160
a lot of time focusing our attention the

00:22:34,760 --> 00:22:41,180
two color-coded ones our control themed

00:22:37,160 --> 00:22:43,850
tweets we did actually so a few things

00:22:41,180 --> 00:22:46,340
to say it was kind of a quasi content

00:22:43,850 --> 00:22:49,580
analysis we did actually look at the

00:22:46,340 --> 00:22:52,430
frequency of each of these codes I am

00:22:49,580 --> 00:22:54,560
reluctant to kind of broadcast that I

00:22:52,430 --> 00:22:57,890
don't think it it's easy to misinterpret

00:22:54,560 --> 00:22:59,870
what that means I think what we found

00:22:57,890 --> 00:23:02,600
just reading the tweets themselves was

00:22:59,870 --> 00:23:04,700
that people generally are writing about

00:23:02,600 --> 00:23:07,670
the algorithms in a moment where

00:23:04,700 --> 00:23:09,470
something is happening so it's very much

00:23:07,670 --> 00:23:11,570
skewed towards the parts of the Twitter

00:23:09,470 --> 00:23:13,670
platform that are very visible right so

00:23:11,570 --> 00:23:15,380
the time line people complaining about

00:23:13,670 --> 00:23:19,040
the time line people commenting on the

00:23:15,380 --> 00:23:20,810
time line the frequency of these codes I

00:23:19,040 --> 00:23:23,090
think should not be interpreted to mean

00:23:20,810 --> 00:23:24,740
that these are kind of rankings of how

00:23:23,090 --> 00:23:26,990
much people care about each of these

00:23:24,740 --> 00:23:29,330
things they're really more kind of what

00:23:26,990 --> 00:23:33,710
what happens to people the most on the

00:23:29,330 --> 00:23:35,510
platform and then I think so I have an

00:23:33,710 --> 00:23:37,370
appendix slide if people really want to

00:23:35,510 --> 00:23:39,440
get into that I it is included but I'm

00:23:37,370 --> 00:23:39,860
not gonna go over this the frequency of

00:23:39,440 --> 00:23:41,690
the codes

00:23:39,860 --> 00:23:44,120
I think the control code was like

00:23:41,690 --> 00:23:46,760
between ten and twenty percent of the

00:23:44,120 --> 00:23:47,960
tweets we from our sample had a control

00:23:46,760 --> 00:23:50,570
theme so that tells you there's

00:23:47,960 --> 00:23:52,670
something you can find in the data it's

00:23:50,570 --> 00:23:55,670
not hard to find people talking about

00:23:52,670 --> 00:23:57,800
control among complaints people

00:23:55,670 --> 00:23:59,990
sometimes complain that they felt the

00:23:57,800 --> 00:24:01,730
algorithm was dictating behavior to them

00:23:59,990 --> 00:24:03,770
often it was tied to complaints about

00:24:01,730 --> 00:24:05,480
censorship like I'm being censored I

00:24:03,770 --> 00:24:08,030
can't say things in the way I want you

00:24:05,480 --> 00:24:09,470
because the algorithms and usually they

00:24:08,030 --> 00:24:11,360
were referring to what they thought were

00:24:09,470 --> 00:24:13,710
content moderation algorithms

00:24:11,360 --> 00:24:15,750
prevent them from being free on the

00:24:13,710 --> 00:24:25,470
platform and the way they express

00:24:15,750 --> 00:24:27,660
themselves okay so let's move on to talk

00:24:25,470 --> 00:24:30,150
about I think what I wanted to start

00:24:27,660 --> 00:24:32,730
with before I dig into the control

00:24:30,150 --> 00:24:34,380
themed tweets I wanted to just kind of

00:24:32,730 --> 00:24:36,480
give you a sense of the gist out of what

00:24:34,380 --> 00:24:37,800
if you look at all those tweets together

00:24:36,480 --> 00:24:40,440
when people are talking about the

00:24:37,800 --> 00:24:42,210
Twitter algorithm we sort of started to

00:24:40,440 --> 00:24:43,620
get this emerging sense of what it what

00:24:42,210 --> 00:24:48,900
it is that people are doing on Twitter

00:24:43,620 --> 00:24:51,690
what they seem to be doing and I think

00:24:48,900 --> 00:24:54,000
in us in a nutshell it seems like there

00:24:51,690 --> 00:24:55,980
was a real emphasis especially in the

00:24:54,000 --> 00:24:58,530
tweets we were looking at in community

00:24:55,980 --> 00:25:00,180
building I'm calling it assembly work

00:24:58,530 --> 00:25:01,860
which I'll get to you later in the

00:25:00,180 --> 00:25:03,060
presentation but community building is

00:25:01,860 --> 00:25:04,320
maybe a more accessible way to talk

00:25:03,060 --> 00:25:05,970
about it

00:25:04,320 --> 00:25:09,000
and what was really interesting about

00:25:05,970 --> 00:25:12,570
that was it seemed to be contradicted by

00:25:09,000 --> 00:25:14,340
the way Twitter as a platform describes

00:25:12,570 --> 00:25:17,460
itself if you look in there kind of help

00:25:14,340 --> 00:25:19,920
files this is not a place where people

00:25:17,460 --> 00:25:24,090
were just going to consume engaging

00:25:19,920 --> 00:25:27,080
content right they were trying to build

00:25:24,090 --> 00:25:29,610
communities and what they were doing

00:25:27,080 --> 00:25:32,460
algorithmically or in attending to and

00:25:29,610 --> 00:25:34,830
controlling the algorithms was pretty

00:25:32,460 --> 00:25:36,390
nuanced it wasn't trying to manipulate

00:25:34,830 --> 00:25:38,790
the algorithms to get more attention

00:25:36,390 --> 00:25:40,920
sometimes it was but it wasn't just that

00:25:38,790 --> 00:25:43,110
sometimes it was manipulating the

00:25:40,920 --> 00:25:45,330
algorithms to get less attention or to

00:25:43,110 --> 00:25:47,340
draw kind of boundaries around their

00:25:45,330 --> 00:25:49,740
communities kind of self-protective

00:25:47,340 --> 00:25:51,450
boundaries and I will talk about that in

00:25:49,740 --> 00:25:54,660
more depth but I think that's the gist

00:25:51,450 --> 00:25:56,760
of what on the whole we saw with these

00:25:54,660 --> 00:25:59,820
control strategies community building

00:25:56,760 --> 00:26:03,120
boundary drawing some amount of kind of

00:25:59,820 --> 00:26:05,340
attention seeking but not it wasn't just

00:26:03,120 --> 00:26:09,030
gaming the algorithm to get more

00:26:05,340 --> 00:26:12,300
attention on the platform I want to talk

00:26:09,030 --> 00:26:13,680
about so I've talked about people

00:26:12,300 --> 00:26:15,630
talking about Twitter algorithms I

00:26:13,680 --> 00:26:18,600
haven't mentioned like what are those

00:26:15,630 --> 00:26:21,720
Twitter algorithms what did they mean by

00:26:18,600 --> 00:26:24,570
what Oh what do I mean by it there's

00:26:21,720 --> 00:26:25,030
some pretty clear-cut examples I mean

00:26:24,570 --> 00:26:27,940
the

00:26:25,030 --> 00:26:29,410
line is a very straightforward one the

00:26:27,940 --> 00:26:31,510
timeline doesn't show you a

00:26:29,410 --> 00:26:35,470
chronological list of everyone you

00:26:31,510 --> 00:26:39,010
follow its curated by Twitter in some

00:26:35,470 --> 00:26:44,320
way that is not totally clear to users

00:26:39,010 --> 00:26:46,390
so that that many of the users we whose

00:26:44,320 --> 00:26:48,280
tweets we looked at would call the time

00:26:46,390 --> 00:26:50,140
line algorithm I talked to some Twitter

00:26:48,280 --> 00:26:52,780
people the other day there's not like

00:26:50,140 --> 00:26:55,840
one algorithm so it's a bunch of

00:26:52,780 --> 00:26:57,610
algorithms but as a concept I think it

00:26:55,840 --> 00:26:59,590
makes sense more or less to think of it

00:26:57,610 --> 00:27:01,690
as the time line algorithm there's also

00:26:59,590 --> 00:27:03,520
you know these recommendation algorithms

00:27:01,690 --> 00:27:05,500
there's the who to follow box that

00:27:03,520 --> 00:27:07,930
suggests people for you to follow people

00:27:05,500 --> 00:27:09,280
would hone in on that as a Twitter

00:27:07,930 --> 00:27:11,350
algorithm

00:27:09,280 --> 00:27:13,300
there's the ads that it serves you

00:27:11,350 --> 00:27:16,390
people would point to that as a Twitter

00:27:13,300 --> 00:27:18,010
algorithm and I think the content

00:27:16,390 --> 00:27:20,260
moderation side of things is where I'm

00:27:18,010 --> 00:27:22,150
really unclear about to what extent

00:27:20,260 --> 00:27:24,880
that's automated or to what extent it

00:27:22,150 --> 00:27:27,070
makes sense to call that an algorithm or

00:27:24,880 --> 00:27:29,050
something that is done algorithmically

00:27:27,070 --> 00:27:30,640
but certainly Twitter users thought that

00:27:29,050 --> 00:27:33,190
part of the platform was done

00:27:30,640 --> 00:27:36,160
algorithmically so we're kind of working

00:27:33,190 --> 00:27:37,390
between perceptions and reality mostly

00:27:36,160 --> 00:27:38,710
staying on the side of perceptions

00:27:37,390 --> 00:27:42,520
because I'm just looking at what users

00:27:38,710 --> 00:27:49,800
identify as algorithms and what they you

00:27:42,520 --> 00:27:49,800
know what they're seeking to control a

00:27:50,970 --> 00:27:54,430
couple other things they want to say

00:27:52,900 --> 00:27:56,230
about Twitter just to kind of

00:27:54,430 --> 00:27:58,750
distinguish it from other platforms I

00:27:56,230 --> 00:28:00,700
mean certainly I think this project it

00:27:58,750 --> 00:28:03,100
became very Twitter specific because of

00:28:00,700 --> 00:28:05,680
the data we had but I would I would like

00:28:03,100 --> 00:28:09,250
to kind of think more broadly to other

00:28:05,680 --> 00:28:10,540
online platforms and then go offline and

00:28:09,250 --> 00:28:12,430
think about other domains like I

00:28:10,540 --> 00:28:14,710
mentioned criminal justice and you know

00:28:12,430 --> 00:28:16,570
similar terms in terms of control a

00:28:14,710 --> 00:28:18,070
couple of things that I think is just

00:28:16,570 --> 00:28:22,030
helpful to remind ourselves about with

00:28:18,070 --> 00:28:24,190
Twitter its ad-supported yes of course

00:28:22,030 --> 00:28:27,340
it's very public compared to say

00:28:24,190 --> 00:28:29,430
Facebook Twitter is much more public the

00:28:27,340 --> 00:28:33,730
assumption is that you tweet for a

00:28:29,430 --> 00:28:35,840
unknown invisible audience it's driven

00:28:33,730 --> 00:28:38,210
by user-generated content

00:28:35,840 --> 00:28:40,700
as are many other platforms there's lots

00:28:38,210 --> 00:28:41,600
of encounters between strangers on that

00:28:40,700 --> 00:28:43,909
platform

00:28:41,600 --> 00:28:46,460
you're followed continually by people

00:28:43,909 --> 00:28:48,220
you've never met it's not like Facebook

00:28:46,460 --> 00:28:50,150
that way there's no emphasis on

00:28:48,220 --> 00:28:52,549
following or being followed by people

00:28:50,150 --> 00:28:54,140
you know in real life there's no real

00:28:52,549 --> 00:28:56,690
names policy so there are lots of

00:28:54,140 --> 00:28:57,740
pseudonymous accounts I mean you can

00:28:56,690 --> 00:28:59,330
probably see where I'm going with that

00:28:57,740 --> 00:29:01,549
that's generated lots of problems on

00:28:59,330 --> 00:29:04,460
that platform in part related to you've

00:29:01,549 --> 00:29:06,110
got a mob harassment but those are some

00:29:04,460 --> 00:29:08,659
of the qualities of the platform to keep

00:29:06,110 --> 00:29:10,640
in mind I think that I would draw a

00:29:08,659 --> 00:29:12,020
distinction certainly into YouTube so

00:29:10,640 --> 00:29:15,740
there's not people aren't on Twitter

00:29:12,020 --> 00:29:18,500
very explicitly monetizing themselves

00:29:15,740 --> 00:29:20,870
and the way you see on YouTube so I

00:29:18,500 --> 00:29:21,980
think on YouTube you probably see a lot

00:29:20,870 --> 00:29:24,950
but if I was going to guess I would

00:29:21,980 --> 00:29:27,950
expect a lot more strategies around

00:29:24,950 --> 00:29:29,750
maximizing attention on YouTube and

00:29:27,950 --> 00:29:31,490
maybe less so on on Twitter because

00:29:29,750 --> 00:29:34,250
there's you know money on the line on

00:29:31,490 --> 00:29:38,600
YouTube whereas there's not direct kind

00:29:34,250 --> 00:29:40,309
of monetization on Twitter okay so a

00:29:38,600 --> 00:29:42,110
little bets a little bit just to kind of

00:29:40,309 --> 00:29:44,390
keep in mind what's specific and unique

00:29:42,110 --> 00:29:47,690
and interesting and characteristic of

00:29:44,390 --> 00:29:49,669
Twitter let's talk about control

00:29:47,690 --> 00:29:52,760
strategies so I'm going to talk about

00:29:49,669 --> 00:29:56,049
three kind of major strategies of

00:29:52,760 --> 00:29:59,179
exerting control that's that last code

00:29:56,049 --> 00:30:00,679
in my list from I'll just kind of take

00:29:59,179 --> 00:30:02,659
us back there just just looking at what

00:30:00,679 --> 00:30:05,270
we found in this category where people

00:30:02,659 --> 00:30:07,789
were not just talking about control but

00:30:05,270 --> 00:30:11,330
trying to use the platform to exert

00:30:07,789 --> 00:30:12,740
control over Twitter algorithms whatever

00:30:11,330 --> 00:30:15,200
they thought those algorithms might be

00:30:12,740 --> 00:30:17,299
and that I think was a surprise we

00:30:15,200 --> 00:30:19,399
expected to see people talking about it

00:30:17,299 --> 00:30:22,250
I don't think we realized in advance

00:30:19,399 --> 00:30:23,960
that there'd be so much work to invoke

00:30:22,250 --> 00:30:26,029
the algorithms in the course of trying

00:30:23,960 --> 00:30:29,539
to do something to them trying to effect

00:30:26,029 --> 00:30:33,860
some outcome through them so one

00:30:29,539 --> 00:30:35,690
category of commentary about controlling

00:30:33,860 --> 00:30:42,200
the algorithms was about personalization

00:30:35,690 --> 00:30:44,809
so a couple of ways that surfaced people

00:30:42,200 --> 00:30:48,440
trying to I'm gonna go with the second

00:30:44,809 --> 00:30:49,220
bullet point first I described it as

00:30:48,440 --> 00:30:52,280
disrupt the

00:30:49,220 --> 00:30:55,850
construction of a data double so trying

00:30:52,280 --> 00:30:57,799
to undermine the platform's efforts to

00:30:55,850 --> 00:31:00,169
collect data and to know who they are

00:30:57,799 --> 00:31:03,500
and to kind of develop a coherent

00:31:00,169 --> 00:31:06,380
accurate data profile there are people

00:31:03,500 --> 00:31:11,600
who talked about ways to game the

00:31:06,380 --> 00:31:16,580
Twitter algorithm to to disrupt that too

00:31:11,600 --> 00:31:19,370
to not be known by the algorithm the

00:31:16,580 --> 00:31:22,370
second tweet is more to that point than

00:31:19,370 --> 00:31:24,289
the first so this person says I decided

00:31:22,370 --> 00:31:26,090
to investigate what Twitter algorithms

00:31:24,289 --> 00:31:28,370
think of me I haven't checked before on

00:31:26,090 --> 00:31:30,320
this account damn whatever I'm doing I'm

00:31:28,370 --> 00:31:32,330
doing write it as it's all over the

00:31:30,320 --> 00:31:34,130
place so that I have to give a little

00:31:32,330 --> 00:31:36,260
context that's in reference to there's

00:31:34,130 --> 00:31:38,059
this kind of a configuration screen on

00:31:36,260 --> 00:31:40,190
Twitter that shows you your interests

00:31:38,059 --> 00:31:42,320
it's this list of things that Twitter

00:31:40,190 --> 00:31:43,789
thinks it knows about you this person

00:31:42,320 --> 00:31:46,010
apparently checked that list and was

00:31:43,789 --> 00:31:47,720
like wow that's so inaccurate that just

00:31:46,010 --> 00:31:49,400
covers so many different interests or

00:31:47,720 --> 00:31:51,740
things that Twitter thinks it knows

00:31:49,400 --> 00:31:52,100
about me which are just wrong and I like

00:31:51,740 --> 00:31:55,659
it

00:31:52,100 --> 00:31:58,610
you know I like that I like that I've

00:31:55,659 --> 00:32:01,130
evaded Twitter as a platform their

00:31:58,610 --> 00:32:04,220
attempts to kind of develop a data

00:32:01,130 --> 00:32:06,200
profile about me and there's a there's a

00:32:04,220 --> 00:32:07,909
whole book about this obfuscation by

00:32:06,200 --> 00:32:10,070
Helen Eason bomb and fin Brunton if

00:32:07,909 --> 00:32:11,960
you're interested in that topic

00:32:10,070 --> 00:32:13,789
lots more to read about that but I see

00:32:11,960 --> 00:32:15,679
that that this isn't the only example of

00:32:13,789 --> 00:32:17,510
that I saw that in a certain pattern

00:32:15,679 --> 00:32:19,880
among the tweets that talked about

00:32:17,510 --> 00:32:22,580
control as having to do with things like

00:32:19,880 --> 00:32:25,340
privacy or working against

00:32:22,580 --> 00:32:29,299
personalization or trying not to be

00:32:25,340 --> 00:32:31,760
profiled the other example of that is

00:32:29,299 --> 00:32:33,980
where people I mean on the whole people

00:32:31,760 --> 00:32:36,380
seem to be very invested in being

00:32:33,980 --> 00:32:39,049
understood by the algorithm so having

00:32:36,380 --> 00:32:41,570
their content personalized and being

00:32:39,049 --> 00:32:43,520
shown ads that speak to them and even

00:32:41,570 --> 00:32:45,200
among the complaints people offend it

00:32:43,520 --> 00:32:47,570
because Twitter seems to think they're

00:32:45,200 --> 00:32:50,150
like a person who'd be interested in the

00:32:47,570 --> 00:32:52,700
Rolex watch and that's just so not true

00:32:50,150 --> 00:32:54,200
about them right so more often we saw

00:32:52,700 --> 00:32:57,380
complaints that the platform wasn't

00:32:54,200 --> 00:32:59,000
personalized enough or didn't fully

00:32:57,380 --> 00:33:00,920
understand them but there were some

00:32:59,000 --> 00:33:03,290
other

00:33:00,920 --> 00:33:06,380
tweets about the algorithm that had to

00:33:03,290 --> 00:33:08,929
do with trying to get outside of that

00:33:06,380 --> 00:33:11,240
filter bubble trying to get content that

00:33:08,929 --> 00:33:12,950
wasn't exactly reflective of who they

00:33:11,240 --> 00:33:15,380
were or who their interests were or who

00:33:12,950 --> 00:33:20,420
their opinions were so I've labeled that

00:33:15,380 --> 00:33:22,460
escaping the filter bubble yes so this

00:33:20,420 --> 00:33:24,740
is very kind of individualistic so this

00:33:22,460 --> 00:33:27,530
is things that people are doing alone

00:33:24,740 --> 00:33:30,679
to try and configure their account in a

00:33:27,530 --> 00:33:32,929
way that they consider preferable and I

00:33:30,679 --> 00:33:34,520
think what this reflects is that there's

00:33:32,929 --> 00:33:36,559
not one way in which there's kind of

00:33:34,520 --> 00:33:38,480
dominant themes in what Twitter users

00:33:36,559 --> 00:33:40,429
want generally they want to have

00:33:38,480 --> 00:33:43,030
personalized content but there are of

00:33:40,429 --> 00:33:45,500
course you know human variety is vast

00:33:43,030 --> 00:33:46,970
people who don't want that kind of

00:33:45,500 --> 00:33:49,070
personalization who don't want to be

00:33:46,970 --> 00:33:51,320
seen or understood by the platform and

00:33:49,070 --> 00:33:53,630
who don't want to be fed content that's

00:33:51,320 --> 00:33:58,040
kind of catering to their existing

00:33:53,630 --> 00:34:01,250
interests and identity okay

00:33:58,040 --> 00:34:04,400
another category of control strategies

00:34:01,250 --> 00:34:07,790
I'm describing us overcoming the

00:34:04,400 --> 00:34:09,800
platform's errors those could be things

00:34:07,790 --> 00:34:11,240
like content moderation errors I mean

00:34:09,800 --> 00:34:13,010
these are probably things that won't be

00:34:11,240 --> 00:34:16,250
surprising to you at all often it's

00:34:13,010 --> 00:34:21,139
about miss reading sentiment missing the

00:34:16,250 --> 00:34:23,119
nuance in in tweet contents

00:34:21,139 --> 00:34:27,139
I think the second no the first one

00:34:23,119 --> 00:34:29,119
tweet is one of my favorites so pro tips

00:34:27,139 --> 00:34:31,639
since Twitter's algorithm is being hyper

00:34:29,119 --> 00:34:35,149
aggressive with suspending accounts that

00:34:31,639 --> 00:34:38,020
use the word kill sarcastically I

00:34:35,149 --> 00:34:40,010
suggest we switch to the word kale

00:34:38,020 --> 00:34:41,450
there's and then there's an example I

00:34:40,010 --> 00:34:44,780
swear to god Janna if you don't shut up

00:34:41,450 --> 00:34:47,570
I will kale kale me now honestly in my

00:34:44,780 --> 00:34:49,460
mind it's more terrifying so a kind of

00:34:47,570 --> 00:34:51,320
strategy of swapping in one word for

00:34:49,460 --> 00:34:53,119
another it's agreed upon by the people

00:34:51,320 --> 00:34:55,580
in that community that we use the word

00:34:53,119 --> 00:34:59,030
kale instead of kill and then that way

00:34:55,580 --> 00:35:01,310
we avoid this mistake that you you know

00:34:59,030 --> 00:35:03,470
whatever content moderation process is

00:35:01,310 --> 00:35:06,310
undertaken makes and seeing that word

00:35:03,470 --> 00:35:08,810
kill and not understanding that it's not

00:35:06,310 --> 00:35:12,710
explicitly part of an attempt to

00:35:08,810 --> 00:35:14,690
threaten someone and I think you I mean

00:35:12,710 --> 00:35:17,079
you can see that on many online platform

00:35:14,690 --> 00:35:21,680
this is a real challenge human

00:35:17,079 --> 00:35:25,310
expressive behavior is so nuanced so

00:35:21,680 --> 00:35:28,069
vast so varied that to the extent it's

00:35:25,310 --> 00:35:30,050
being tackled through automated systems

00:35:28,069 --> 00:35:32,869
it makes mistakes all the times all the

00:35:30,050 --> 00:35:35,780
time problems that misunderstanding like

00:35:32,869 --> 00:35:39,109
common sense problems but problems that

00:35:35,780 --> 00:35:44,450
certainly are true of automated kind of

00:35:39,109 --> 00:35:46,220
detection systems and I just will put it

00:35:44,450 --> 00:35:47,720
put it out there maybe you know more

00:35:46,220 --> 00:35:50,780
than I do but it seems to me that

00:35:47,720 --> 00:35:53,329
automated systems that can decode the

00:35:50,780 --> 00:35:55,490
nuance within the universe of human

00:35:53,329 --> 00:35:58,460
creative expression are a pretty long

00:35:55,490 --> 00:36:01,250
way off right so that's gonna continue

00:35:58,460 --> 00:36:03,829
to be a challenge on platforms that try

00:36:01,250 --> 00:36:06,470
to either curate content or moderate

00:36:03,829 --> 00:36:08,420
content and so the workaround which i

00:36:06,470 --> 00:36:09,710
think is you know probably an

00:36:08,420 --> 00:36:12,109
encouraging thing to see is people

00:36:09,710 --> 00:36:13,730
picking up on ways in which the platform

00:36:12,109 --> 00:36:15,380
is misunderstanding what they're saying

00:36:13,730 --> 00:36:17,150
and trying to develop ways to avoid

00:36:15,380 --> 00:36:22,160
those problems themselves kind of

00:36:17,150 --> 00:36:26,630
correct for the errors another problem

00:36:22,160 --> 00:36:28,160
that was often brought up which I think

00:36:26,630 --> 00:36:30,440
Twitter has a new policy that maybe is

00:36:28,160 --> 00:36:34,130
addressing this and people would engage

00:36:30,440 --> 00:36:36,770
with a politician or a political figure

00:36:34,130 --> 00:36:39,050
whose views they were very much opposed

00:36:36,770 --> 00:36:41,599
to and they would engage with those

00:36:39,050 --> 00:36:43,910
views to criticize them or reject them

00:36:41,599 --> 00:36:46,130
they would do it through a retweet or a

00:36:43,910 --> 00:36:48,530
reply and then later on they would see

00:36:46,130 --> 00:36:51,140
that same figure recommended to them as

00:36:48,530 --> 00:36:52,970
someone to follow and that certainly

00:36:51,140 --> 00:36:55,760
picked up on people's sense of being

00:36:52,970 --> 00:36:57,980
misunderstood by the platform like I

00:36:55,760 --> 00:36:59,690
don't know if Twitter is using sentiment

00:36:57,980 --> 00:37:01,970
analysis or if this is just beyond

00:36:59,690 --> 00:37:03,710
sentiment analysis generally people who

00:37:01,970 --> 00:37:05,660
despise certain politicians don't also

00:37:03,710 --> 00:37:08,240
want to follow them sometimes maybe they

00:37:05,660 --> 00:37:10,010
do but generally they don't so that was

00:37:08,240 --> 00:37:12,290
often read by users as an error like

00:37:10,010 --> 00:37:15,339
clearly the platform doesn't understand

00:37:12,290 --> 00:37:17,990
that I am critiquing this person not

00:37:15,339 --> 00:37:19,880
following and celebrating them and it

00:37:17,990 --> 00:37:21,740
also tapped into concerns people had

00:37:19,880 --> 00:37:24,069
about engagement the kind of logic of

00:37:21,740 --> 00:37:26,180
engagement on the platform that

00:37:24,069 --> 00:37:27,450
engagement even when it was negative

00:37:26,180 --> 00:37:31,200
engagement was

00:37:27,450 --> 00:37:32,700
warded on the platform and amplified and

00:37:31,200 --> 00:37:34,110
so there were workarounds that had to do

00:37:32,700 --> 00:37:35,520
with people trying to which I will talk

00:37:34,110 --> 00:37:39,090
about in the next slide people's trying

00:37:35,520 --> 00:37:42,390
to avoid rewarding people who they

00:37:39,090 --> 00:37:45,240
didn't support with engagement and using

00:37:42,390 --> 00:37:55,130
the platform to avoid indicating

00:37:45,240 --> 00:37:59,760
engagement okay so this is the kind of

00:37:55,130 --> 00:38:02,010
summative strategy of control that I

00:37:59,760 --> 00:38:03,930
think captures most what we're trying to

00:38:02,010 --> 00:38:08,070
contribute to the discussion about

00:38:03,930 --> 00:38:09,360
control so I said in a few slides back

00:38:08,070 --> 00:38:11,070
when I was talking about what Twitter

00:38:09,360 --> 00:38:13,110
users are doing on Twitter I talked

00:38:11,070 --> 00:38:15,570
about community building

00:38:13,110 --> 00:38:18,650
I am also calling this assembly work

00:38:15,570 --> 00:38:21,330
there's a piece in a Law Review Journal

00:38:18,650 --> 00:38:25,890
Cornell Law Review from a few years back

00:38:21,330 --> 00:38:30,810
titled virtual assembly which references

00:38:25,890 --> 00:38:33,030
the not freedom of speech but freedom of

00:38:30,810 --> 00:38:35,130
assembly in the US Constitution kind of

00:38:33,030 --> 00:38:38,370
a clause that's lesser recognized and is

00:38:35,130 --> 00:38:40,080
not about the freedom to express ideas

00:38:38,370 --> 00:38:43,050
of any sort but about the freedom to

00:38:40,080 --> 00:38:44,250
come together with other people and I

00:38:43,050 --> 00:38:46,890
think what's really interesting about

00:38:44,250 --> 00:38:48,000
what we saw in the control strategies of

00:38:46,890 --> 00:38:48,480
people on Twitter was that they were

00:38:48,000 --> 00:38:49,860
trying

00:38:48,480 --> 00:38:52,620
they were basically trying to assemble

00:38:49,860 --> 00:38:54,120
they're trying to curate communities

00:38:52,620 --> 00:38:56,010
trying to come together with other

00:38:54,120 --> 00:38:58,140
people's you know social media it was a

00:38:56,010 --> 00:39:01,200
very social practice it wasn't just

00:38:58,140 --> 00:39:03,630
about consuming or producing content it

00:39:01,200 --> 00:39:05,160
was about relationship management it was

00:39:03,630 --> 00:39:09,870
about community building it was about

00:39:05,160 --> 00:39:12,870
doing assembly work and often the way

00:39:09,870 --> 00:39:15,030
the platform worked undermined those the

00:39:12,870 --> 00:39:17,250
algorithms like the timeline algorithm

00:39:15,030 --> 00:39:18,930
undermine those efforts and people had

00:39:17,250 --> 00:39:20,970
to develop some very creative ways to be

00:39:18,930 --> 00:39:23,340
able to do that assembly work to say who

00:39:20,970 --> 00:39:25,740
was in their community to set boundaries

00:39:23,340 --> 00:39:28,140
often that was about avoiding relentless

00:39:25,740 --> 00:39:30,810
harassment not to just be

00:39:28,140 --> 00:39:33,270
algorithmically recognizable not to tap

00:39:30,810 --> 00:39:35,010
into engagement not to be more and more

00:39:33,270 --> 00:39:38,300
gain more and more attention on the

00:39:35,010 --> 00:39:39,890
platform but to avoid certain kinds of

00:39:38,300 --> 00:39:44,000
unwanted attention

00:39:39,890 --> 00:39:45,650
from certain groups and that you know I

00:39:44,000 --> 00:39:47,720
talked about on the slide enacting

00:39:45,650 --> 00:39:50,150
community boundaries so let me give some

00:39:47,720 --> 00:39:54,799
examples of what that means like what

00:39:50,150 --> 00:39:57,470
are some examples of that there is a

00:39:54,799 --> 00:40:00,470
piece by Emily Vander Nagle about she

00:39:57,470 --> 00:40:02,450
talks about she uses the term Voldemort

00:40:00,470 --> 00:40:03,740
een I mean there's so that's a good one

00:40:02,450 --> 00:40:05,450
there's also people talk about

00:40:03,740 --> 00:40:07,010
subtweeting that's probably one that

00:40:05,450 --> 00:40:08,750
more people know about so subtweeting

00:40:07,010 --> 00:40:10,910
you're talking about something you're

00:40:08,750 --> 00:40:14,150
talking about someone but you very

00:40:10,910 --> 00:40:18,260
purposefully don't act them you don't

00:40:14,150 --> 00:40:19,700
use their account name in talking about

00:40:18,260 --> 00:40:21,349
them because you don't want to draw

00:40:19,700 --> 00:40:22,880
their attention and maybe more

00:40:21,349 --> 00:40:26,769
importantly you don't want to draw the

00:40:22,880 --> 00:40:33,559
attention of their followers so the

00:40:26,769 --> 00:40:36,049
gamergate thing the issue controversy of

00:40:33,559 --> 00:40:37,640
a few years ago was an example where a

00:40:36,049 --> 00:40:40,160
kind of code words were used by people

00:40:37,640 --> 00:40:42,470
who were trying to avoid being targeted

00:40:40,160 --> 00:40:45,440
by the gamergate community they would

00:40:42,470 --> 00:40:47,059
use the word goober gate or gamergate

00:40:45,440 --> 00:40:49,849
with an @ symbol they'd used these kind

00:40:47,059 --> 00:40:52,430
of subtweet like code words as a way to

00:40:49,849 --> 00:40:54,980
communicate within that their sub

00:40:52,430 --> 00:40:57,549
community and avoid attention drop

00:40:54,980 --> 00:41:00,319
boundaries and avoid attention from

00:40:57,549 --> 00:41:03,650
groups they felt were likely to harass

00:41:00,319 --> 00:41:06,170
them so that's an example of how that's

00:41:03,650 --> 00:41:10,640
enacted as control on the platform

00:41:06,170 --> 00:41:13,759
itself the first tweet is super

00:41:10,640 --> 00:41:16,190
interesting so there was a set of tweets

00:41:13,759 --> 00:41:18,710
that were in that in our data set there

00:41:16,190 --> 00:41:20,240
was one person who tweeted 25 times so

00:41:18,710 --> 00:41:21,859
the most of anyone any individual

00:41:20,240 --> 00:41:24,680
account and tweeted almost the same

00:41:21,859 --> 00:41:27,049
exact thing I don't think this was the

00:41:24,680 --> 00:41:29,119
exact tweet but it was very similar so

00:41:27,049 --> 00:41:32,809
this is someone replying to a journalist

00:41:29,119 --> 00:41:34,880
and saying cheers Adam it's the

00:41:32,809 --> 00:41:37,400
advantages they get from the Twitter

00:41:34,880 --> 00:41:39,589
algorithm that most concerns us in the

00:41:37,400 --> 00:41:41,029
disinformation combat community so

00:41:39,589 --> 00:41:44,180
there's a community that's been labeled

00:41:41,029 --> 00:41:47,029
the disinformation combat community this

00:41:44,180 --> 00:41:50,089
reply is so basically this journalist

00:41:47,029 --> 00:41:53,160
tweeted about something retweeted a

00:41:50,089 --> 00:41:55,349
politician added their there

00:41:53,160 --> 00:41:57,390
their account name in the tweet and this

00:41:55,349 --> 00:41:59,400
person's trying to kind of correct their

00:41:57,390 --> 00:42:02,190
behavior and say don't do that if you

00:41:59,400 --> 00:42:03,660
want to talk about a politician if you

00:42:02,190 --> 00:42:07,259
want to talk about a political idea

00:42:03,660 --> 00:42:11,279
that's very that you oppose don't

00:42:07,259 --> 00:42:13,529
retweet don't like don't reply use a

00:42:11,279 --> 00:42:16,230
screenshot and actually this became a

00:42:13,529 --> 00:42:18,150
hashtag hashtag use screenshots so you

00:42:16,230 --> 00:42:21,089
can engage with ideas without feeding

00:42:18,150 --> 00:42:24,960
into that engagement mechanism by you

00:42:21,089 --> 00:42:27,000
know copying it using that screenshot in

00:42:24,960 --> 00:42:28,589
your tweet and avoiding all of the kind

00:42:27,000 --> 00:42:30,390
of engagement the way it's had that taps

00:42:28,589 --> 00:42:33,420
into the way Twitter the Twitter

00:42:30,390 --> 00:42:35,130
platform rewards engagement I this tweet

00:42:33,420 --> 00:42:38,190
is so interesting is so on so many

00:42:35,130 --> 00:42:39,990
levels I think one is that this was

00:42:38,190 --> 00:42:42,660
clearly a kind of effort by this person

00:42:39,990 --> 00:42:46,079
to spread at best practice I mean he

00:42:42,660 --> 00:42:49,829
kept tweeting at people encouraging them

00:42:46,079 --> 00:42:51,720
to use screenshots and this hashtag if

00:42:49,829 --> 00:42:54,569
you search on the use screenshots hash

00:42:51,720 --> 00:42:56,130
tag there's lots of examples still and

00:42:54,569 --> 00:42:59,549
this is shows you kind of collaborative

00:42:56,130 --> 00:43:01,650
efforts within a community this this

00:42:59,549 --> 00:43:04,910
person identifies himself as the part of

00:43:01,650 --> 00:43:07,470
the disinformation combat community and

00:43:04,910 --> 00:43:11,579
you know advocating for a certain best

00:43:07,470 --> 00:43:15,839
practice around making Twitter a better

00:43:11,579 --> 00:43:17,730
place with less misinformation so

00:43:15,839 --> 00:43:21,089
certainly some examples in this space

00:43:17,730 --> 00:43:23,579
were a tenth attention seeking so I've

00:43:21,089 --> 00:43:25,589
talked a bit about people trying to be

00:43:23,579 --> 00:43:27,569
less recognizable or invisible on the

00:43:25,589 --> 00:43:29,460
platform as a way to kind of defend and

00:43:27,569 --> 00:43:33,569
create boundaries around a community

00:43:29,460 --> 00:43:35,630
they're building Alma on Twitter XO

00:43:33,569 --> 00:43:39,150
global does that mean anything to anyone

00:43:35,630 --> 00:43:42,180
it's one of the very popular Korean boy

00:43:39,150 --> 00:43:46,079
bands so this is part of fan Twitter

00:43:42,180 --> 00:43:48,210
there was a whole set of tweets that

00:43:46,079 --> 00:43:50,250
talked about the algorithm the Twitter

00:43:48,210 --> 00:43:52,559
algorithm related to I call it a

00:43:50,250 --> 00:43:56,700
campaign a campaign among fans of this

00:43:52,559 --> 00:43:58,829
boy band to get their their bands to

00:43:56,700 --> 00:44:01,200
trend on the Twitter trends so that's

00:43:58,829 --> 00:44:03,660
certainly kind of a classic example of

00:44:01,200 --> 00:44:04,869
trying to game the algorithm to maximize

00:44:03,660 --> 00:44:07,569
its resource is it

00:44:04,869 --> 00:44:09,220
tensional resources right to kind of get

00:44:07,569 --> 00:44:11,559
as much attention for something you're

00:44:09,220 --> 00:44:14,049
in support of in this case a Korean boy

00:44:11,559 --> 00:44:15,910
band and I think this is also a really

00:44:14,049 --> 00:44:17,710
good example of community building or

00:44:15,910 --> 00:44:21,160
assembly work so this is a group that

00:44:17,710 --> 00:44:23,890
identifies itself as a fan group fans of

00:44:21,160 --> 00:44:27,549
EXO global coming together for a very

00:44:23,890 --> 00:44:29,740
kind of short term campaign to get

00:44:27,549 --> 00:44:31,089
attention for this this and support for

00:44:29,740 --> 00:44:39,220
this band that they are really

00:44:31,089 --> 00:44:45,299
passionate about okay all right am i

00:44:39,220 --> 00:44:49,259
aiming for like 45 minutes or so okay

00:44:45,299 --> 00:44:54,069
okay all right I'm closing in on the end

00:44:49,259 --> 00:44:55,599
okay so I said I would talk about as the

00:44:54,069 --> 00:44:57,970
kind of third bullet point my outline

00:44:55,599 --> 00:45:00,039
this idea that algorithmic control

00:44:57,970 --> 00:45:01,990
strategies have social utility and I

00:45:00,039 --> 00:45:03,309
hope that sort of been clear from what

00:45:01,990 --> 00:45:05,410
I've described so far this kind of

00:45:03,309 --> 00:45:09,640
community building work working around

00:45:05,410 --> 00:45:12,249
platform errors what I would say kind of

00:45:09,640 --> 00:45:14,200
just to summarize that I mean the point

00:45:12,249 --> 00:45:16,660
here is that we can talk about control

00:45:14,200 --> 00:45:18,339
inherently as a value or a right like

00:45:16,660 --> 00:45:21,099
it's just a matter of self determination

00:45:18,339 --> 00:45:23,739
and as much as possible it makes sense

00:45:21,099 --> 00:45:26,589
to allow users control on the platforms

00:45:23,739 --> 00:45:27,970
that they use that matter to them but I

00:45:26,589 --> 00:45:31,329
think what's interesting here is you can

00:45:27,970 --> 00:45:33,759
see how that that control strategy turns

00:45:31,329 --> 00:45:35,170
over into supporting other kinds of

00:45:33,759 --> 00:45:37,720
values on the platform including

00:45:35,170 --> 00:45:39,630
community building and assembly work

00:45:37,720 --> 00:45:42,339
right so there's another value this this

00:45:39,630 --> 00:45:45,549
Law Review article talks about the value

00:45:42,339 --> 00:45:48,400
of assembling online so control as a

00:45:45,549 --> 00:45:51,009
value feeds into other supporting other

00:45:48,400 --> 00:45:52,660
kinds of values on the platform so in

00:45:51,009 --> 00:45:54,339
some I would say about what Twitter

00:45:52,660 --> 00:45:56,799
users are doing when they exert control

00:45:54,339 --> 00:45:58,119
is they're drawing boundaries around the

00:45:56,799 --> 00:45:59,499
communities that they're building to

00:45:58,119 --> 00:46:03,279
protect them from abuse and harassment

00:45:59,499 --> 00:46:05,680
there's a real social utility there they

00:46:03,279 --> 00:46:07,900
are seeking to resist the platform's

00:46:05,680 --> 00:46:11,200
relentless push towards engagement when

00:46:07,900 --> 00:46:12,970
they don't want to fuel that they're

00:46:11,200 --> 00:46:14,980
using it to work around algorithmic

00:46:12,970 --> 00:46:17,350
errors such as problems in reading

00:46:14,980 --> 00:46:20,380
sentiment

00:46:17,350 --> 00:46:22,750
other users are resisting being known by

00:46:20,380 --> 00:46:25,450
the algorithm in a principled stance of

00:46:22,750 --> 00:46:27,760
resistance to the platform's logic of

00:46:25,450 --> 00:46:32,110
surveillance and they're doing that by

00:46:27,760 --> 00:46:33,220
engaging in personal data obfuscation so

00:46:32,110 --> 00:46:35,350
those are some of the reasonable

00:46:33,220 --> 00:46:38,440
strategies of users who you know they

00:46:35,350 --> 00:46:40,240
may be diverging from the goals of the

00:46:38,440 --> 00:46:42,610
platform itself they're customizing it

00:46:40,240 --> 00:46:44,650
to their own needs and interests and

00:46:42,610 --> 00:46:46,480
they're expressing in a way personal

00:46:44,650 --> 00:46:49,300
views that diverge from the platforms

00:46:46,480 --> 00:46:51,640
goals so I think those are some pretty

00:46:49,300 --> 00:46:54,520
clear examples of social social utility

00:46:51,640 --> 00:46:58,210
and I think to kind of summarize that

00:46:54,520 --> 00:47:00,610
point it appears that people really need

00:46:58,210 --> 00:47:03,490
to know how the algorithms function in

00:47:00,610 --> 00:47:06,370
order to do this work of managing how

00:47:03,490 --> 00:47:09,480
and with whom they assemble on platforms

00:47:06,370 --> 00:47:12,700
like Twitter like it's pretty necessary

00:47:09,480 --> 00:47:14,830
and because those algorithms operate as

00:47:12,700 --> 00:47:16,390
background processes the ways users

00:47:14,830 --> 00:47:20,590
interact with them are really pretty

00:47:16,390 --> 00:47:21,940
indirect right and you know you have to

00:47:20,590 --> 00:47:23,560
talk I'd have to talk some more to the

00:47:21,940 --> 00:47:25,510
people at Twitter to understand whether

00:47:23,560 --> 00:47:27,130
it seems like they're being discouraged

00:47:25,510 --> 00:47:30,730
from interacting with those algorithms

00:47:27,130 --> 00:47:32,530
right they're expected to just behave in

00:47:30,730 --> 00:47:34,900
these kind of natural uncoordinated ways

00:47:32,530 --> 00:47:38,230
and that's that's how the algorithms are

00:47:34,900 --> 00:47:39,640
designed to kind of read behavior they

00:47:38,230 --> 00:47:41,890
actually was thinking if we're talking

00:47:39,640 --> 00:47:44,910
about assembly work or people assembling

00:47:41,890 --> 00:47:48,190
online I was trying to think of some

00:47:44,910 --> 00:47:50,950
offline examples that might sort of

00:47:48,190 --> 00:47:51,970
reflect the struggles people have with

00:47:50,950 --> 00:47:53,590
trying to understand how these

00:47:51,970 --> 00:47:57,520
algorithms work and turn them to their

00:47:53,590 --> 00:48:02,050
own purposes so if you could think of a

00:47:57,520 --> 00:48:04,810
public place with a gate and the

00:48:02,050 --> 00:48:07,330
schedule isn't posted but there's kind

00:48:04,810 --> 00:48:10,690
of an underlying pattern about when you

00:48:07,330 --> 00:48:12,970
can enter the space but it's

00:48:10,690 --> 00:48:14,200
purposefully obfuscated right that's

00:48:12,970 --> 00:48:15,790
sort of an example of how people

00:48:14,200 --> 00:48:17,980
experience these algorithms it's like

00:48:15,790 --> 00:48:19,630
sometimes you can come in sometimes you

00:48:17,980 --> 00:48:21,570
can get attention sometimes you can have

00:48:19,630 --> 00:48:23,920
a platform to announce something

00:48:21,570 --> 00:48:25,840
sometimes you can't if you really work

00:48:23,920 --> 00:48:29,170
hard you can figure out maybe what the

00:48:25,840 --> 00:48:31,119
pattern is there or another example

00:48:29,170 --> 00:48:35,109
might be if you sent out a

00:48:31,119 --> 00:48:37,210
an invitation to an event too you know a

00:48:35,109 --> 00:48:39,940
list of a hundred people and some random

00:48:37,210 --> 00:48:42,220
subset of maybe twenty actually got your

00:48:39,940 --> 00:48:43,930
invitation I think that's another

00:48:42,220 --> 00:48:45,160
example that's pretty approximate it's

00:48:43,930 --> 00:48:47,349
sort of like when you post something on

00:48:45,160 --> 00:48:48,849
your timeline some people see it and

00:48:47,349 --> 00:48:50,319
some people don't it depends on whether

00:48:48,849 --> 00:48:52,150
it's placed on their timeline whether

00:48:50,319 --> 00:48:54,039
they're on Twitter right so there's a

00:48:52,150 --> 00:48:55,660
certain kind of difficult randomness

00:48:54,039 --> 00:48:57,519
that people really struggle with and

00:48:55,660 --> 00:49:01,630
trying to use this platform and many

00:48:57,519 --> 00:49:04,269
others as a place to assemble when it's

00:49:01,630 --> 00:49:06,039
so obvious kated okay so what do we do

00:49:04,269 --> 00:49:07,329
about it I'm just gonna go through this

00:49:06,039 --> 00:49:09,999
briefly I really like this piece by

00:49:07,329 --> 00:49:12,400
Gruden jonathan Gruden from it's a

00:49:09,999 --> 00:49:15,069
little old 2009 he sort of tracks this

00:49:12,400 --> 00:49:19,359
pattern between like the rise of AI like

00:49:15,069 --> 00:49:22,150
as AI Rises HCI falls and as HCI Rises

00:49:19,359 --> 00:49:23,650
you know AI is falling I think he's

00:49:22,150 --> 00:49:25,420
arguing in this paper that maybe it

00:49:23,650 --> 00:49:29,410
won't continue this way but I do think

00:49:25,420 --> 00:49:31,420
in this case around the debate on AI

00:49:29,410 --> 00:49:34,180
ethics the field of HCI human-computer

00:49:31,420 --> 00:49:36,039
interaction really has a lot to

00:49:34,180 --> 00:49:38,140
contribute and I think we're gradually

00:49:36,039 --> 00:49:39,309
starting to see that as these criticisms

00:49:38,140 --> 00:49:40,869
of AI emerge

00:49:39,309 --> 00:49:42,190
there's a really important role to play

00:49:40,869 --> 00:49:45,029
certainly around things like user

00:49:42,190 --> 00:49:47,650
control HCI has a really important place

00:49:45,029 --> 00:49:50,529
so things that might support user

00:49:47,650 --> 00:49:52,749
control it's not just about design

00:49:50,529 --> 00:49:55,029
technology design their existing

00:49:52,749 --> 00:49:57,309
examples where control seems to be part

00:49:55,029 --> 00:50:01,150
of the thinking so the gdpr has this

00:49:57,309 --> 00:50:04,239
right to explanation clause which partly

00:50:01,150 --> 00:50:06,249
is about people not being just subject

00:50:04,239 --> 00:50:11,049
to automated decisions that they have no

00:50:06,249 --> 00:50:12,549
way of understanding appealing right so

00:50:11,049 --> 00:50:15,999
that there's a bit of a control element

00:50:12,549 --> 00:50:18,130
an autonomy goal underlying the right to

00:50:15,999 --> 00:50:19,980
explanation and this isn't just a new

00:50:18,130 --> 00:50:22,690
thing so if you look at credit scoring

00:50:19,980 --> 00:50:25,900
the Equal Credit Opportunity Act has

00:50:22,690 --> 00:50:28,779
this this is from this 1970s GDP are

00:50:25,900 --> 00:50:30,579
obviously is EU ECOA as US but there's

00:50:28,779 --> 00:50:32,559
this adverse action notice if you if you

00:50:30,579 --> 00:50:35,619
apply for a loan and you're denied the

00:50:32,559 --> 00:50:38,279
loan by law you have to be told why and

00:50:35,619 --> 00:50:41,200
there's sort of a list of 20 reasons

00:50:38,279 --> 00:50:42,880
that that's not it doesn't have to be

00:50:41,200 --> 00:50:44,160
one of those but generally lenders use

00:50:42,880 --> 00:50:45,569
one of those twenty reasons

00:50:44,160 --> 00:50:47,730
to explain to you why it didn't get

00:50:45,569 --> 00:50:50,099
ballon and the idea there of course in

00:50:47,730 --> 00:50:52,140
part is that now that you know you can

00:50:50,099 --> 00:50:54,420
correct for that you can do something to

00:50:52,140 --> 00:50:56,970
make yourself more creditworthy in the

00:50:54,420 --> 00:51:00,180
future which obviously is a you know

00:50:56,970 --> 00:51:04,339
it's trying to support control site

00:51:00,180 --> 00:51:06,839
policies so in 2018 Facebook kind of

00:51:04,339 --> 00:51:09,839
increase the transparency of their

00:51:06,839 --> 00:51:12,270
content moderation process they added an

00:51:09,839 --> 00:51:14,400
appeals process and promised that if you

00:51:12,270 --> 00:51:16,230
appeal a content moderation decision a

00:51:14,400 --> 00:51:18,510
human will look at it which I thought

00:51:16,230 --> 00:51:20,280
was really interesting and then there's

00:51:18,510 --> 00:51:22,470
a lot of work kind of emerging in the

00:51:20,280 --> 00:51:24,150
interaction design space I cited three

00:51:22,470 --> 00:51:25,799
things that if you're looking for more

00:51:24,150 --> 00:51:28,799
these are three good ones to start with

00:51:25,799 --> 00:51:30,740
there's Wendy G's piece from 2008 on

00:51:28,799 --> 00:51:32,970
implicit interaction which I really like

00:51:30,740 --> 00:51:34,770
that's not super new but if we're

00:51:32,970 --> 00:51:37,079
talking about background processes that

00:51:34,770 --> 00:51:38,480
don't present themselves to users how do

00:51:37,079 --> 00:51:42,569
you think about design when that's

00:51:38,480 --> 00:51:44,190
operating in your systems paper and I

00:51:42,569 --> 00:51:46,020
can't remember which ACM conference but

00:51:44,190 --> 00:51:48,809
a paper called guidelines for human AI

00:51:46,020 --> 00:51:52,170
interaction that had lots of good tips

00:51:48,809 --> 00:51:53,760
and suggestions and then uh I think

00:51:52,170 --> 00:51:56,700
there's a medium article yeah medium

00:51:53,760 --> 00:51:58,380
article by Josh Lovejoy called human

00:51:56,700 --> 00:52:00,240
centered AI cheat sheet so there if you

00:51:58,380 --> 00:52:01,770
if this is interesting to you and you're

00:52:00,240 --> 00:52:04,619
thinking about design those are those

00:52:01,770 --> 00:52:06,690
are places you could go read more so

00:52:04,619 --> 00:52:10,589
conclusion reasons to cede control to

00:52:06,690 --> 00:52:12,000
platform users they use it in these not

00:52:10,589 --> 00:52:13,619
always but they often use it in these

00:52:12,000 --> 00:52:17,309
productive ways so working around

00:52:13,619 --> 00:52:19,980
algorithmic errors users on massively

00:52:17,309 --> 00:52:22,819
scaled platforms like like Twitter users

00:52:19,980 --> 00:52:26,369
see things that the platform can't right

00:52:22,819 --> 00:52:29,520
so empowering people to address the

00:52:26,369 --> 00:52:31,440
problems or issues they encounter is it

00:52:29,520 --> 00:52:33,690
seems an obvious like obviously a good

00:52:31,440 --> 00:52:34,680
idea and the Law Review article I

00:52:33,690 --> 00:52:37,589
mentioned at the beginning on

00:52:34,680 --> 00:52:39,990
algorithmic gaming they talked about the

00:52:37,589 --> 00:52:41,520
creativity of control and gain even

00:52:39,990 --> 00:52:44,069
gaming even the things that might be

00:52:41,520 --> 00:52:46,289
labeled illegitimate they can spur

00:52:44,069 --> 00:52:48,119
innovations that allow the designer and

00:52:46,289 --> 00:52:52,349
its users to find novel uses for

00:52:48,119 --> 00:52:55,289
existing platforms I also want to just

00:52:52,349 --> 00:52:57,990
put in a push here to say if we're not

00:52:55,289 --> 00:53:00,510
thinking long and hard about granting

00:52:57,990 --> 00:53:03,240
control are we expecting platforms like

00:53:00,510 --> 00:53:05,850
Twitter to handle all these issues and

00:53:03,240 --> 00:53:07,800
do we really want platforms run by

00:53:05,850 --> 00:53:10,710
private companies to be more autocratic

00:53:07,800 --> 00:53:13,590
as they continue to function as a de

00:53:10,710 --> 00:53:15,930
facto public sphere right that's an

00:53:13,590 --> 00:53:19,860
important question there is a debate

00:53:15,930 --> 00:53:23,160
about control that centers on Burt not

00:53:19,860 --> 00:53:26,430
burdening people with you know having to

00:53:23,160 --> 00:53:28,290
take on the duty of managing platforms

00:53:26,430 --> 00:53:30,000
and their experience of platforms on

00:53:28,290 --> 00:53:31,560
their own without a lot of knowledge or

00:53:30,000 --> 00:53:32,970
understanding about how they work and

00:53:31,560 --> 00:53:35,280
that's a great argument the

00:53:32,970 --> 00:53:38,220
counter-argument is if we don't invest

00:53:35,280 --> 00:53:40,740
that kind of control and users empower

00:53:38,220 --> 00:53:42,510
them in certain ways are we then saying

00:53:40,740 --> 00:53:44,940
well private companies are gonna just

00:53:42,510 --> 00:53:47,820
handle it it's obviously hard to handle

00:53:44,940 --> 00:53:50,130
as we've seen and then just the

00:53:47,820 --> 00:53:53,610
fundamental argument is that control

00:53:50,130 --> 00:53:56,130
supports is it's a dignitary concern it

00:53:53,610 --> 00:53:57,540
supports human dignity and that in and

00:53:56,130 --> 00:54:01,530
of itself maybe doesn't require further

00:53:57,540 --> 00:54:03,330
justification the piece is available

00:54:01,530 --> 00:54:05,040
online right now there's a shortened

00:54:03,330 --> 00:54:07,320
link if you want to go take a look at it

00:54:05,040 --> 00:54:09,150
it's the papers titled when users

00:54:07,320 --> 00:54:13,320
control the algorithms I'm presenting it

00:54:09,150 --> 00:54:15,450
at the CSC W conference in November and

00:54:13,320 --> 00:54:23,400
I think that's it I'm happy to take any

00:54:15,450 --> 00:54:28,680
questions and I am gonna go present this

00:54:23,400 --> 00:54:29,609
at Twitter oh that'll be fun yeah um so

00:54:28,680 --> 00:54:31,920
Twitter is sort of this interesting

00:54:29,609 --> 00:54:33,900
player here because in some ways I mean

00:54:31,920 --> 00:54:37,160
I love the framing that you've done and

00:54:33,900 --> 00:54:39,300
I love your organizational work here and

00:54:37,160 --> 00:54:42,869
framing what people are doing this

00:54:39,300 --> 00:54:46,080
virtual assembly work is is a very

00:54:42,869 --> 00:54:49,260
positive take yeah on what you have

00:54:46,080 --> 00:54:52,650
otherwise characterized as resisting

00:54:49,260 --> 00:54:54,420
relentless harassment yeah all right so

00:54:52,650 --> 00:54:56,730
there her Astor's are doing assembly

00:54:54,420 --> 00:54:58,350
work right yes yeah cuz they doing

00:54:56,730 --> 00:55:00,390
assembly work too I mean and in a world

00:54:58,350 --> 00:55:02,310
in which Twitter and I'm going to come

00:55:00,390 --> 00:55:05,190
out and say this had responsible

00:55:02,310 --> 00:55:06,960
policies to actually address harassment

00:55:05,190 --> 00:55:09,330
in the way that one might expect a

00:55:06,960 --> 00:55:11,260
company with increasing private private

00:55:09,330 --> 00:55:13,110
public commitment to actually

00:55:11,260 --> 00:55:15,610
- yeah if they did their job

00:55:13,110 --> 00:55:17,410
right would this still be necessary

00:55:15,610 --> 00:55:20,080
right what do we see the same person

00:55:17,410 --> 00:55:22,060
yeah so that Law Review article I mean

00:55:20,080 --> 00:55:23,730
that's the essential question and I'm

00:55:22,060 --> 00:55:28,390
glad you brought it up so we can kind of

00:55:23,730 --> 00:55:31,210
tangle with the it's not it's a very not

00:55:28,390 --> 00:55:33,460
black and white issue and I am on that

00:55:31,210 --> 00:55:35,080
level I'm a little uncomfortable in

00:55:33,460 --> 00:55:36,790
making this kind of one-sided argument

00:55:35,080 --> 00:55:41,200
that control is great and people need

00:55:36,790 --> 00:55:43,000
more control so the Law Review article I

00:55:41,200 --> 00:55:45,880
referenced the virtual assembly article

00:55:43,000 --> 00:55:47,920
talks about these I think he calls an

00:55:45,880 --> 00:55:49,780
expressive acts these for expressive

00:55:47,920 --> 00:55:52,120
acts where people basically say here's

00:55:49,780 --> 00:55:54,220
this community here's who gets to join

00:55:52,120 --> 00:55:56,890
this community here are the things we do

00:55:54,220 --> 00:55:59,860
as a community people I mean expulsion

00:55:56,890 --> 00:56:02,730
is one of those expressive acts people

00:55:59,860 --> 00:56:05,560
can be expelled from the community if

00:56:02,730 --> 00:56:07,810
harassment was in check I think they're

00:56:05,560 --> 00:56:10,570
still absolutely a value and people

00:56:07,810 --> 00:56:12,370
being able to say this community is

00:56:10,570 --> 00:56:15,130
about this topic and we're talking about

00:56:12,370 --> 00:56:16,690
for example feminism in this way and we

00:56:15,130 --> 00:56:19,660
don't want a lot of newbies who are

00:56:16,690 --> 00:56:21,670
gonna ask the same tedious questions and

00:56:19,660 --> 00:56:25,660
burden our time it's not necessarily

00:56:21,670 --> 00:56:27,810
harassment right it's just about the the

00:56:25,660 --> 00:56:30,550
way the community identifies itself so

00:56:27,810 --> 00:56:32,470
it is absolutely great for Twitter to do

00:56:30,550 --> 00:56:35,770
much more than its doing to tackle

00:56:32,470 --> 00:56:38,320
harassment but I think this need to

00:56:35,770 --> 00:56:42,360
support control remains even when that's

00:56:38,320 --> 00:56:44,440
been checked and and when that the

00:56:42,360 --> 00:56:49,570
wonderful day when the online harassment

00:56:44,440 --> 00:56:54,010
problem is in check cool one plug by the

00:56:49,570 --> 00:56:55,480
way we have a we writ large have an

00:56:54,010 --> 00:56:56,890
exhibition on Market Street right now

00:56:55,480 --> 00:56:59,290
called the glass room which is all about

00:56:56,890 --> 00:57:00,850
issues of surveillance and things like

00:56:59,290 --> 00:57:01,930
this and oh my god you should totally go

00:57:00,850 --> 00:57:02,620
and check that that looks like it'll

00:57:01,930 --> 00:57:05,610
make you so happy

00:57:02,620 --> 00:57:09,810
well or deeply distressed or both of us

00:57:05,610 --> 00:57:09,810
do I have questions in the room

00:57:10,710 --> 00:57:14,680
Thanks thanks to the talk yeah I found

00:57:13,210 --> 00:57:17,550
it really interesting especially the

00:57:14,680 --> 00:57:21,810
takeaways on the social utility of

00:57:17,550 --> 00:57:26,610
algorithmic control but

00:57:21,810 --> 00:57:30,090
to poke at it a little bit do do people

00:57:26,610 --> 00:57:32,010
feel that they have more control through

00:57:30,090 --> 00:57:33,720
these strategies do they feel that the

00:57:32,010 --> 00:57:37,260
virtual assembly work has been

00:57:33,720 --> 00:57:39,060
successful and if they don't does that

00:57:37,260 --> 00:57:42,060
undermine a little bit the argument that

00:57:39,060 --> 00:57:43,890
they have control that they have exerted

00:57:42,060 --> 00:57:46,500
some control over the algorithm yeah I

00:57:43,890 --> 00:57:49,470
mean I probably haven't been totally

00:57:46,500 --> 00:57:52,080
clear in how I'm taking these examples

00:57:49,470 --> 00:57:53,730
of exerting control because I don't have

00:57:52,080 --> 00:57:56,160
a lot of evidence that any of those

00:57:53,730 --> 00:57:57,450
strategies are working very well and I

00:57:56,160 --> 00:57:59,580
don't have a lot of evidence that the

00:57:57,450 --> 00:58:02,400
people using those strategies feel that

00:57:59,580 --> 00:58:04,710
they're working very well and that would

00:58:02,400 --> 00:58:07,530
be a great kind of next step for this

00:58:04,710 --> 00:58:09,900
this project what I've shown is that

00:58:07,530 --> 00:58:11,640
there's a desire for control and that

00:58:09,900 --> 00:58:16,440
people have these strategies that they

00:58:11,640 --> 00:58:18,410
think do or could work and I think some

00:58:16,440 --> 00:58:20,520
of those strategies don't work at all

00:58:18,410 --> 00:58:22,110
but that's the question and I think

00:58:20,520 --> 00:58:23,880
what's missing is because these

00:58:22,110 --> 00:58:25,740
algorithmic processes are treated as

00:58:23,880 --> 00:58:27,240
this it's just this background thing

00:58:25,740 --> 00:58:28,860
that Twitter handles and you just behave

00:58:27,240 --> 00:58:30,720
the way you normally would and Twitter

00:58:28,860 --> 00:58:31,980
will handle it they don't tell you how

00:58:30,720 --> 00:58:33,990
it works I mean I actually have this

00:58:31,980 --> 00:58:37,020
great slide I'm glad I have an excuse to

00:58:33,990 --> 00:58:40,440
show it this is how Twitter describes

00:58:37,020 --> 00:58:42,150
their timeline it's super vague top

00:58:40,440 --> 00:58:44,730
tweets are the ones you're likely to

00:58:42,150 --> 00:58:46,560
care about most they're based on

00:58:44,730 --> 00:58:49,590
accounts you interact with tweets you

00:58:46,560 --> 00:58:51,060
engage with and much more okay but so I

00:58:49,590 --> 00:58:52,230
mean the tweets the accounts you

00:58:51,060 --> 00:58:54,080
interact with tweets you engage with

00:58:52,230 --> 00:58:57,180
that tells you a little bit of something

00:58:54,080 --> 00:58:59,070
and our goal is to show you content that

00:58:57,180 --> 00:59:01,050
you're interested in that contributes to

00:58:59,070 --> 00:59:03,210
the conversation that's relevant

00:59:01,050 --> 00:59:05,220
credible and safe right so that's a

00:59:03,210 --> 00:59:07,740
general general information about how it

00:59:05,220 --> 00:59:10,500
works which really tells you very little

00:59:07,740 --> 00:59:15,900
about what you might do to change it or

00:59:10,500 --> 00:59:17,340
to bend it to your interests there's

00:59:15,900 --> 00:59:20,610
much more that could be done in this

00:59:17,340 --> 00:59:22,860
space I think if you're trying and users

00:59:20,610 --> 00:59:24,540
engaging with your algorithms if you're

00:59:22,860 --> 00:59:26,250
not trying to hide them if you're not

00:59:24,540 --> 00:59:28,140
trying to obfuscate how they work

00:59:26,250 --> 00:59:29,760
there's a lot more you could tell people

00:59:28,140 --> 00:59:31,860
so that they're empowered to do

00:59:29,760 --> 00:59:33,579
something with their with their

00:59:31,860 --> 00:59:35,869
community building on the planet

00:59:33,579 --> 00:59:37,460
so that's really where I'm pointing to

00:59:35,869 --> 00:59:41,390
is like what could what would it mean to

00:59:37,460 --> 00:59:42,920
be more transparent to to not treat

00:59:41,390 --> 00:59:45,109
algorithms is something that people

00:59:42,920 --> 00:59:47,089
shouldn't pay attention to but as

00:59:45,109 --> 00:59:48,829
something they will pay attention to and

00:59:47,089 --> 00:59:50,569
will try to control and how can you help

00:59:48,829 --> 00:59:54,619
them do it in a way that that will serve

00:59:50,569 --> 00:59:58,520
their interests I know I have a bunch

00:59:54,619 --> 01:00:01,040
more questions for you but we've got

00:59:58,520 --> 01:00:03,880
some time to chat please join me in

01:00:01,040 --> 01:00:09,149
thanking Jenna very much at all

01:00:03,880 --> 01:00:09,149
[Applause]

01:00:22,890 --> 01:00:24,950

YouTube URL: https://www.youtube.com/watch?v=MSZSJqDXQLE


