Title: Emerging Tech Speaker Series - Libby Hemphill
Publication date: 2019-08-26
Playlist: Emerging Technologies Speaker Series
Description: 
	Designing Targeted, Automated De-escalation Strategies for Social Media
Libby Hemphill, Associate Professor, School of Information, University of Michigan

Harassment, insults, trolling, threats, and many other anti-social behaviors are toxic to conversations online. Reporting and addressing these behaviors requires a lot of time, labor, and emotional strain, and they're often ineffective. To move toward more effective methods to curb problematic behaviors such as harassment and hostility, I propose that we think about the problem differently in two ways. First, we must be more specific and explicit about the behaviors and content that are unacceptable in particular contexts so that we can design targeted mechanisms for addressing them and recognize the potential unintended impacts of our interventions. Second, we must treat problematic behavior as a social problem, not just an individual one, which demands that we address the contexts in which behavior occurs. To ground this discussion, I'll provide an example of a targeted mechanism for addressing personal insults and an experiment designed to reduce their prevalence in existing online communities.
Captions: 
	00:00:03,700 --> 00:00:08,450
good morning everybody my name is Joe

00:00:06,440 --> 00:00:09,980
fish K I'm principal research scientist

00:00:08,450 --> 00:00:13,190
here in the emerging technologies team

00:00:09,980 --> 00:00:16,460
and it is my pleasure to present Libby

00:00:13,190 --> 00:00:18,020
Hemphill Libby is a professor at the

00:00:16,460 --> 00:00:19,250
University of Michigan hadn't the

00:00:18,020 --> 00:00:23,210
information school there sorry the

00:00:19,250 --> 00:00:27,140
School of Information and I've known

00:00:23,210 --> 00:00:32,029
Louise work for a while and we gave her

00:00:27,140 --> 00:00:34,039
a Mozilla research grant last year and I

00:00:32,029 --> 00:00:35,269
thought when she mentioned she was gonna

00:00:34,039 --> 00:00:37,429
be in the Bay Area rifle this was an

00:00:35,269 --> 00:00:38,929
excellent chance to have her come over

00:00:37,429 --> 00:00:42,559
and talk about some the work that she's

00:00:38,929 --> 00:00:49,579
been doing yeah so I'm really looking

00:00:42,559 --> 00:00:53,149
for to it I should say I'm sorry we are

00:00:49,579 --> 00:00:54,469
taking questions on hashtag et Libby's

00:00:53,149 --> 00:00:58,760
mentioned that she's totally happy to

00:00:54,469 --> 00:01:00,499
have people interrupt and so kind of

00:00:58,760 --> 00:01:02,749
take me then I'll like you know tap on

00:01:00,499 --> 00:01:04,970
the shoulder or something if people are

00:01:02,749 --> 00:01:09,680
watching on YouTube you can also send me

00:01:04,970 --> 00:01:11,840
tweets to at Joe fish and again I'll

00:01:09,680 --> 00:01:13,190
pass the messages on and let me know if

00:01:11,840 --> 00:01:14,660
you particularly want to either sort of

00:01:13,190 --> 00:01:16,460
ask a question right now or whether

00:01:14,660 --> 00:01:19,340
you're happy for its way to the end so

00:01:16,460 --> 00:01:23,120
again that's on after I eat on slack and

00:01:19,340 --> 00:01:29,660
at Joe fish on Twitter thank you thanks

00:01:23,120 --> 00:01:31,400
good morning I'm so glad that I was able

00:01:29,660 --> 00:01:33,380
to come and that you all had time for me

00:01:31,400 --> 00:01:35,060
today I'm hoping that we'll have a

00:01:33,380 --> 00:01:37,850
conversation about what it is that my

00:01:35,060 --> 00:01:41,260
team and I are working thank you so much

00:01:37,850 --> 00:01:41,260
for making our work possible

00:01:41,630 --> 00:01:44,840
and I'm hoping that we can have kind of

00:01:43,820 --> 00:01:46,540
conversation I'll tell you what we're

00:01:44,840 --> 00:01:48,740
working on if you have ideas or

00:01:46,540 --> 00:01:50,990
clarifications I would love your

00:01:48,740 --> 00:01:55,720
feedback so please do interrupt if you

00:01:50,990 --> 00:01:55,720
have a question um okay so

00:01:58,439 --> 00:02:06,030
this one there we go okay I thought I'd

00:02:01,049 --> 00:02:08,039
start talking about with why do this

00:02:06,030 --> 00:02:10,049
work so the title of my talk was deep

00:02:08,039 --> 00:02:12,930
breath folks using box2d escalate

00:02:10,049 --> 00:02:15,150
conflict in social media and often the

00:02:12,930 --> 00:02:17,040
next slide from a talk that starts that

00:02:15,150 --> 00:02:19,799
way is gonna go directly into what does

00:02:17,040 --> 00:02:20,819
the escalation mean and waterboxx and

00:02:19,799 --> 00:02:21,810
what are we trying to do in social media

00:02:20,819 --> 00:02:24,150
and I thought it would be better to

00:02:21,810 --> 00:02:27,510
start with why do this work in the first

00:02:24,150 --> 00:02:29,610
place because I think that often when we

00:02:27,510 --> 00:02:31,650
have these conversations about how to

00:02:29,610 --> 00:02:33,360
make social media less awful that's

00:02:31,650 --> 00:02:34,799
where we start is what if we make it

00:02:33,360 --> 00:02:36,209
less awful and I would rather think

00:02:34,799 --> 00:02:38,970
about how do you make social media

00:02:36,209 --> 00:02:41,069
better so instead of thinking about what

00:02:38,970 --> 00:02:43,410
we want in absence of let's think about

00:02:41,069 --> 00:02:45,739
what do we want the presence of and what

00:02:43,410 --> 00:02:49,170
I would like to see in social media is

00:02:45,739 --> 00:02:52,019
effective use of that kind of tool for

00:02:49,170 --> 00:02:54,090
justice as a value what do I be honest

00:02:52,019 --> 00:02:55,410
to do I want us to use social media as

00:02:54,090 --> 00:02:58,859
an effective way for people to have

00:02:55,410 --> 00:03:01,829
conversations that lead to justice and

00:02:58,859 --> 00:03:04,019
so my work is not about removing stuff

00:03:01,829 --> 00:03:06,359
from online it's about making space for

00:03:04,019 --> 00:03:08,639
things that are possible so even though

00:03:06,359 --> 00:03:10,590
I will talk about de-escalation which is

00:03:08,639 --> 00:03:12,209
sort of removal of escalation and my

00:03:10,590 --> 00:03:15,780
goal is to make it possible for

00:03:12,209 --> 00:03:21,299
conversations to happen not simply to

00:03:15,780 --> 00:03:23,269
remove nasty stuff okay so what does

00:03:21,299 --> 00:03:25,889
that mean for my approach it means that

00:03:23,269 --> 00:03:27,900
I find that it's really important for us

00:03:25,889 --> 00:03:29,519
to be specific about what it is that

00:03:27,900 --> 00:03:31,319
we're trying to do so instead of saying

00:03:29,519 --> 00:03:33,540
something like problematic behaviors

00:03:31,319 --> 00:03:36,150
which I put there at the top of my slide

00:03:33,540 --> 00:03:37,410
like well what is a problematic behavior

00:03:36,150 --> 00:03:39,389
I can think of lots of different things

00:03:37,410 --> 00:03:42,859
that are problematic harassment trolling

00:03:39,389 --> 00:03:45,419
dachshund insulting aggression profanity

00:03:42,859 --> 00:03:48,480
some other kind of rule violation if you

00:03:45,419 --> 00:03:50,370
have one even an off-topic direct or a

00:03:48,480 --> 00:03:52,200
divergent direction could be problematic

00:03:50,370 --> 00:03:54,480
so I think it's important that we first

00:03:52,200 --> 00:03:56,819
be very specific and explicit about what

00:03:54,480 --> 00:03:58,410
is the thing that we are attending to in

00:03:56,819 --> 00:04:01,379
this process whether it's a moderation

00:03:58,410 --> 00:04:03,569
process or a policy implementation or in

00:04:01,379 --> 00:04:05,700
my case a machine learning approach to

00:04:03,569 --> 00:04:07,410
understanding something and then the

00:04:05,700 --> 00:04:09,509
second is that we need to address the

00:04:07,410 --> 00:04:10,980
social context in which these behaviors

00:04:09,509 --> 00:04:12,900
occur so not a number

00:04:10,980 --> 00:04:14,640
turning in a vacuum and people have been

00:04:12,900 --> 00:04:16,739
terrible to each other long before we

00:04:14,640 --> 00:04:18,660
could do so online and there we can

00:04:16,739 --> 00:04:21,090
learn from the context of history and

00:04:18,660 --> 00:04:23,220
interaction and the existing struggles

00:04:21,090 --> 00:04:25,560
for justice elsewhere and informing what

00:04:23,220 --> 00:04:28,650
we're doing and what does it mean to

00:04:25,560 --> 00:04:29,850
learn from a struggle for justice so as

00:04:28,650 --> 00:04:32,310
I mentioned up front and when I think

00:04:29,850 --> 00:04:35,030
about what is present and this is in

00:04:32,310 --> 00:04:37,530
part because I'm informed by work on

00:04:35,030 --> 00:04:40,410
feminism and anti racism where we're

00:04:37,530 --> 00:04:42,360
looking for equality as a positive thing

00:04:40,410 --> 00:04:46,830
not the absence of people from a space

00:04:42,360 --> 00:04:49,620
so there's a really powerful passage in

00:04:46,830 --> 00:04:51,960
one of their King Junior's letters from

00:04:49,620 --> 00:04:54,840
the Birmingham jail that when the end of

00:04:51,960 --> 00:04:57,090
the quote is essentially when the

00:04:54,840 --> 00:04:58,680
biggest obstacle towards freedom is the

00:04:57,090 --> 00:05:00,389
white matter it was more devoted to

00:04:58,680 --> 00:05:02,639
order than to justice who prefers a

00:05:00,389 --> 00:05:04,470
negative peace which is the absence of

00:05:02,639 --> 00:05:06,449
tension to a positive peace which is the

00:05:04,470 --> 00:05:07,979
presence of justice I think that's a

00:05:06,449 --> 00:05:10,199
helpful quote for us to thinking about

00:05:07,979 --> 00:05:12,120
what can we learn we want to learn from

00:05:10,199 --> 00:05:15,780
existing struggles that happened

00:05:12,120 --> 00:05:21,650
elsewhere and are ongoing and how can we

00:05:15,780 --> 00:05:21,650
use that push online as well Oh

00:05:24,570 --> 00:05:29,669
so we're seeking justice over order this

00:05:26,970 --> 00:05:32,370
means that I want to avoid tone policing

00:05:29,669 --> 00:05:35,010
and I'll come back to what that is that

00:05:32,370 --> 00:05:37,139
reason is not greater than emotionality

00:05:35,010 --> 00:05:38,760
and so this is when we're having

00:05:37,139 --> 00:05:42,330
discussions online sometimes those

00:05:38,760 --> 00:05:43,440
discussions should be heated and you'll

00:05:42,330 --> 00:05:45,539
notice there that I said that they

00:05:43,440 --> 00:05:47,250
should get heated not all discussions

00:05:45,539 --> 00:05:48,900
shouldn't but some should it those in

00:05:47,250 --> 00:05:51,180
which the sides are not equal are the

00:05:48,900 --> 00:05:53,250
ones that are likely to do so and our

00:05:51,180 --> 00:05:55,410
tendency especially under a free speech

00:05:53,250 --> 00:05:58,860
program is to think that all speech is

00:05:55,410 --> 00:06:01,289
as deserving of a platform as all other

00:05:58,860 --> 00:06:03,750
speech and I just don't think that

00:06:01,289 --> 00:06:07,740
that's true so that we'll figure in here

00:06:03,750 --> 00:06:09,570
as well so what are ways that we try to

00:06:07,740 --> 00:06:11,970
do this already when we're trying to

00:06:09,570 --> 00:06:14,370
figure out how to stamp out

00:06:11,970 --> 00:06:16,889
problematic behaviors or even create

00:06:14,370 --> 00:06:18,539
more open spaces the most common way to

00:06:16,889 --> 00:06:20,039
me do it is through flagging and some

00:06:18,539 --> 00:06:21,990
combination of flagging and human

00:06:20,039 --> 00:06:22,540
moderation so this is where you see

00:06:21,990 --> 00:06:25,510
something

00:06:22,540 --> 00:06:27,910
nasty and you say through whatever

00:06:25,510 --> 00:06:29,530
button that platform has provided for

00:06:27,910 --> 00:06:31,890
you this is nasty and then they send it

00:06:29,530 --> 00:06:34,810
off to an underpaid overworked

00:06:31,890 --> 00:06:37,270
traumatized team of people to examine it

00:06:34,810 --> 00:06:39,940
and then usually nothing happens

00:06:37,270 --> 00:06:40,900
sometimes the content gets taken down or

00:06:39,940 --> 00:06:42,990
there's some sort of retributive

00:06:40,900 --> 00:06:45,220
punishment for the person who did it

00:06:42,990 --> 00:06:47,230
that can result in something like a

00:06:45,220 --> 00:06:49,660
blanket ban so here it might be at the

00:06:47,230 --> 00:06:53,890
end of an individual or it might be a

00:06:49,660 --> 00:06:54,730
ban of a set of accounts like the

00:06:53,890 --> 00:06:56,800
Infowars

00:06:54,730 --> 00:06:58,630
set of accounts or it might be like

00:06:56,800 --> 00:07:01,960
instagram has tried to do with the pro

00:06:58,630 --> 00:07:04,120
eating disorder community where they ban

00:07:01,960 --> 00:07:07,120
particular hashtags from appearing in

00:07:04,120 --> 00:07:08,770
search and then i'm in the artificial

00:07:07,120 --> 00:07:10,330
intelligence and machine learning space

00:07:08,770 --> 00:07:12,580
and this is where we try to be a little

00:07:10,330 --> 00:07:15,160
smarter about the content and these

00:07:12,580 --> 00:07:16,780
things can all happen in combination and

00:07:15,160 --> 00:07:18,520
they usually do that platforms are

00:07:16,780 --> 00:07:20,800
usually trying one or more of these

00:07:18,520 --> 00:07:24,370
things together to see if they can

00:07:20,800 --> 00:07:26,830
reduce the presence of problematic

00:07:24,370 --> 00:07:28,960
behaviors on their platform yeah

00:07:26,830 --> 00:07:32,860
question there I mean like you say that

00:07:28,960 --> 00:07:35,470
I would when I get spam on my phone I

00:07:32,860 --> 00:07:36,700
get annoyed i press to to be taken off

00:07:35,470 --> 00:07:38,740
the call this and they keep calling

00:07:36,700 --> 00:07:39,160
calling and it just like Hilton rewards

00:07:38,740 --> 00:07:41,140
ánotá

00:07:39,160 --> 00:07:43,300
and I just feel like even though and

00:07:41,140 --> 00:07:45,460
then I went for a while I went to do not

00:07:43,300 --> 00:07:46,540
call calm and entered every phone number

00:07:45,460 --> 00:07:48,520
and a date in the time and all that

00:07:46,540 --> 00:07:49,960
information and if they were stuck up

00:07:48,520 --> 00:07:53,380
coming it's like there's a disconnect

00:07:49,960 --> 00:07:55,690
between flagging and getting them

00:07:53,380 --> 00:07:59,620
stopped right and I'm wondering if

00:07:55,690 --> 00:08:03,900
you've figured out technology we're

00:07:59,620 --> 00:08:06,640
flagging plus the actual ability to like

00:08:03,900 --> 00:08:11,440
share that phone call while it's

00:08:06,640 --> 00:08:14,980
happening like how do I like say detour

00:08:11,440 --> 00:08:18,430
copy over here into this AI machine

00:08:14,980 --> 00:08:21,760
learning thing that detects what's going

00:08:18,430 --> 00:08:25,240
on and then as others report it's like a

00:08:21,760 --> 00:08:27,010
much more immediate no human processing

00:08:25,240 --> 00:08:29,050
like machine learning can actually

00:08:27,010 --> 00:08:32,110
identify those as spam and then that

00:08:29,050 --> 00:08:34,690
phone number is like blocked

00:08:32,110 --> 00:08:36,790
sure is there something so two things

00:08:34,690 --> 00:08:38,890
from that so one is that I think much

00:08:36,790 --> 00:08:43,000
like the Do Not Call list right now

00:08:38,890 --> 00:08:45,640
there is basically no cost for platforms

00:08:43,000 --> 00:08:48,130
for getting this wrong that we don't

00:08:45,640 --> 00:08:50,110
leave and advertisers don't leave and

00:08:48,130 --> 00:08:52,960
the money keeps flowing and the users

00:08:50,110 --> 00:08:55,660
keep coming so the platforms haven't

00:08:52,960 --> 00:08:57,040
experienced any costs to that which is

00:08:55,660 --> 00:08:58,660
similar to they do not call list that

00:08:57,040 --> 00:09:00,250
it's very cheap to just get a different

00:08:58,660 --> 00:09:01,840
number and start calling you again there

00:09:00,250 --> 00:09:04,480
isn't really any cost to them into

00:09:01,840 --> 00:09:06,880
violating that rule so that's the first

00:09:04,480 --> 00:09:09,340
thing is that without the incentives for

00:09:06,880 --> 00:09:11,200
platforms to change we have to work

00:09:09,340 --> 00:09:13,420
outside the system so one way we could

00:09:11,200 --> 00:09:15,370
work outside the system is as you

00:09:13,420 --> 00:09:17,680
mentioned some sort of crowd-sourced if

00:09:15,370 --> 00:09:20,050
we document this object and we put it in

00:09:17,680 --> 00:09:22,300
a space how can we do that are you

00:09:20,050 --> 00:09:26,350
familiar with heart mob this is one that

00:09:22,300 --> 00:09:28,780
I would recommend so if you google heart

00:09:26,350 --> 00:09:37,630
mob I feel like I shouldn't have the URL

00:09:28,780 --> 00:09:40,960
for that so that's a positive case the

00:09:37,630 --> 00:09:45,610
challenge is that when you crowdsource

00:09:40,960 --> 00:09:48,550
something like the reporting you can end

00:09:45,610 --> 00:09:50,500
up making it possible for the people who

00:09:48,550 --> 00:09:54,580
you were originally trying to block to

00:09:50,500 --> 00:09:58,290
rally to overwhelm the system so

00:09:54,580 --> 00:10:03,490
crowdsourcing is susceptible to crowds

00:09:58,290 --> 00:10:05,950
and if especially distributed daxing

00:10:03,490 --> 00:10:08,860
attempts have they just overwhelmed this

00:10:05,950 --> 00:10:10,660
system so that there just aren't enough

00:10:08,860 --> 00:10:11,950
of the marginalized voices doing the

00:10:10,660 --> 00:10:14,620
reporting instead there are the

00:10:11,950 --> 00:10:17,200
aggressors who make it look like yeah

00:10:14,620 --> 00:10:18,730
they are the victim yeah in some cases

00:10:17,200 --> 00:10:22,360
that might be the case but in other

00:10:18,730 --> 00:10:24,490
cases like you say that just end runs

00:10:22,360 --> 00:10:26,020
you know if they're they're just trying

00:10:24,490 --> 00:10:31,480
to keep skirting around the boundaries

00:10:26,020 --> 00:10:32,920
you just boundaries faster well those

00:10:31,480 --> 00:10:35,590
boggers faster is that nicer where I

00:10:32,920 --> 00:10:37,510
think I think of it as whack-a-mole it's

00:10:35,590 --> 00:10:39,040
just not it doesn't scale like it just

00:10:37,510 --> 00:10:40,690
it really doesn't scale so how do we

00:10:39,040 --> 00:10:42,580
deal with it so some of these are the

00:10:40,690 --> 00:10:43,510
one of the great things about hard bob

00:10:42,580 --> 00:10:46,600
is a

00:10:43,510 --> 00:10:49,900
victims to social resources so not just

00:10:46,600 --> 00:10:51,430
that it doesn't just do reporting but

00:10:49,900 --> 00:10:53,470
reporting is one piece of it but then

00:10:51,430 --> 00:10:55,330
the provision of social support is

00:10:53,470 --> 00:10:58,570
coupled in there so it's very easy to

00:10:55,330 --> 00:11:00,490
say then if you're a heart robbery you

00:10:58,570 --> 00:11:02,260
send another tweet or a message says

00:11:00,490 --> 00:11:04,470
you're not alone I have your back these

00:11:02,260 --> 00:11:07,210
sorts of support so that while the

00:11:04,470 --> 00:11:08,320
minimally effective reporting happens at

00:11:07,210 --> 00:11:12,190
least you're getting some positive

00:11:08,320 --> 00:11:14,770
social support in the meantime and that

00:11:12,190 --> 00:11:17,470
I encourage you to use and support that

00:11:14,770 --> 00:11:27,670
group as one of the positive examples of

00:11:17,470 --> 00:11:30,780
how crowdsourcing can help oh okay in

00:11:27,670 --> 00:11:34,680
the next few slides there is profanity

00:11:30,780 --> 00:11:37,180
I've tried to keep there these are

00:11:34,680 --> 00:11:40,990
medium level of comments for oh sorry so

00:11:37,180 --> 00:11:43,090
this is why is this so hard so we all

00:11:40,990 --> 00:11:44,350
have the sense that there's this nasty

00:11:43,090 --> 00:11:45,730
stuff out there why can't we just take

00:11:44,350 --> 00:11:49,420
care of it what makes this problem

00:11:45,730 --> 00:11:50,890
difficult so for the next few slides the

00:11:49,420 --> 00:11:54,370
great the dark gray text there is a

00:11:50,890 --> 00:11:56,020
comment from a platform that we have

00:11:54,370 --> 00:11:58,930
studied so in this one it says and at

00:11:56,020 --> 00:12:01,270
what point you agreed that you as an ass

00:11:58,930 --> 00:12:03,550
at that moment so if we're using a

00:12:01,270 --> 00:12:05,800
language filter we would tag this post

00:12:03,550 --> 00:12:09,010
and take it down because it has

00:12:05,800 --> 00:12:10,780
profanity in it but I think for the most

00:12:09,010 --> 00:12:13,710
part we can look at this comment and see

00:12:10,780 --> 00:12:16,390
that it wasn't meant to be destructive

00:12:13,710 --> 00:12:19,150
it was meant to get someone else to take

00:12:16,390 --> 00:12:21,670
accountability for a prior action and

00:12:19,150 --> 00:12:22,990
maybe we can imagine a different way

00:12:21,670 --> 00:12:24,640
that we would like someone to ask

00:12:22,990 --> 00:12:26,290
someone to take accountability for a

00:12:24,640 --> 00:12:29,200
prior action but in many communities

00:12:26,290 --> 00:12:31,330
saying just recognize that you were a

00:12:29,200 --> 00:12:33,580
jerk is actually what we should do and

00:12:31,330 --> 00:12:35,610
in this community letting them say you

00:12:33,580 --> 00:12:38,140
as an ass man you have an own that is

00:12:35,610 --> 00:12:40,510
more effective so instead of saying

00:12:38,140 --> 00:12:41,950
let's do it in this nice polite way like

00:12:40,510 --> 00:12:42,580
how about we let them do it in an

00:12:41,950 --> 00:12:44,530
effective way

00:12:42,580 --> 00:12:46,420
and for some communities this would be

00:12:44,530 --> 00:12:49,990
effective so that in orange there it's

00:12:46,420 --> 00:12:52,330
like what makes this hard that detection

00:12:49,990 --> 00:12:53,860
using language alone isn't enough and at

00:12:52,330 --> 00:12:55,769
the norms of the community this may be

00:12:53,860 --> 00:12:59,489
an acceptable way for people to

00:12:55,769 --> 00:13:01,139
but the next one

00:12:59,489 --> 00:13:02,550
you don't really well do you he's not

00:13:01,139 --> 00:13:04,019
defending cops he's saying they're

00:13:02,550 --> 00:13:05,939
learning that they can get away with

00:13:04,019 --> 00:13:07,679
doing illegal things so why are they

00:13:05,939 --> 00:13:09,569
stopped they're not stupid they just

00:13:07,679 --> 00:13:12,119
know they can get away with it so again

00:13:09,569 --> 00:13:14,939
here the language is not enough so often

00:13:12,119 --> 00:13:16,439
you just say things like stupid and

00:13:14,939 --> 00:13:19,920
illegal will show up if we're using

00:13:16,439 --> 00:13:22,230
existing dictionaries that try to detect

00:13:19,920 --> 00:13:23,549
problematic behavior but here we can

00:13:22,230 --> 00:13:25,410
tell that the conversation isn't

00:13:23,549 --> 00:13:28,529
actually about illegal things

00:13:25,410 --> 00:13:30,869
it's about policing and the stupid

00:13:28,529 --> 00:13:32,429
doesn't refer to another individual it

00:13:30,869 --> 00:13:34,589
refers to a class of people who aren't

00:13:32,429 --> 00:13:36,269
part of the conversation but that first

00:13:34,589 --> 00:13:38,610
one that says you don't read well do you

00:13:36,269 --> 00:13:42,059
that's gay it's a little bit of a push

00:13:38,610 --> 00:13:43,860
right it's a and we can imagine more

00:13:42,059 --> 00:13:45,540
polite ways that you can say that so we

00:13:43,860 --> 00:13:50,100
get that first question and then it's

00:13:45,540 --> 00:13:52,410
fine right but maybe yes so we can sort

00:13:50,100 --> 00:13:56,519
of train users to not escalate in the

00:13:52,410 --> 00:14:00,059
first place that's one so those are two

00:13:56,519 --> 00:14:01,439
that I think are they're kind of medium

00:14:00,059 --> 00:14:03,059
level right we'd say there are nicer

00:14:01,439 --> 00:14:13,920
ways to say that but they're not so bad

00:14:03,059 --> 00:14:16,499
but they would be flag but then we

00:14:13,920 --> 00:14:20,369
wouldn't get like it would still flag

00:14:16,499 --> 00:14:23,009
was stupid and legal so here's another

00:14:20,369 --> 00:14:24,509
one about profanity so it's this

00:14:23,009 --> 00:14:26,369
guy should stick your foot in his face

00:14:24,509 --> 00:14:28,790
for those slides into second let me know

00:14:26,369 --> 00:14:31,740
what they're actually talking about here

00:14:28,790 --> 00:14:44,730
they're talking about baseball yeah so

00:14:31,740 --> 00:14:47,339
it's not yeah so this is actually the

00:14:44,730 --> 00:14:49,309
conversation was about baseball and the

00:14:47,339 --> 00:14:51,809
person may be saying this guy he's

00:14:49,309 --> 00:14:53,040
about the baserunner there's not

00:14:51,809 --> 00:14:54,540
actually talking about somebody who's in

00:14:53,040 --> 00:14:56,610
the conversation but talking about

00:14:54,540 --> 00:14:58,049
someone who they're having a

00:14:56,610 --> 00:15:00,449
conversation about a play at second

00:14:58,049 --> 00:15:03,179
telling the second base person to stick

00:15:00,449 --> 00:15:05,100
his foot out of this slide but this one

00:15:03,179 --> 00:15:07,319
gets flagged because it has this

00:15:05,100 --> 00:15:09,329
guy and your foot in his face

00:15:07,319 --> 00:15:12,660
these are they set they sound

00:15:09,329 --> 00:15:15,119
physical threats so for sort of our

00:15:12,660 --> 00:15:16,799
baselines they get caught and so again

00:15:15,119 --> 00:15:20,790
if we knew something about the context

00:15:16,799 --> 00:15:22,350
even if we knew the name of the this was

00:15:20,790 --> 00:15:24,029
this example is from reddit and it

00:15:22,350 --> 00:15:25,439
occurred in our baseball so if we knew

00:15:24,029 --> 00:15:27,509
that it was from our baseball maybe we

00:15:25,439 --> 00:15:30,779
say don't worry about profanity or

00:15:27,509 --> 00:15:33,329
slides something but then again we're

00:15:30,779 --> 00:15:34,920
just building these dictionaries again

00:15:33,329 --> 00:15:37,410
with more and more dictionaries on

00:15:34,920 --> 00:15:39,029
whack-a-mole I'm not going to read these

00:15:37,410 --> 00:15:41,129
next two but I will talk over and you

00:15:39,029 --> 00:15:43,649
can read them there as I start to

00:15:41,129 --> 00:15:45,629
introduce the idea of in-group and

00:15:43,649 --> 00:15:48,749
out-group slurs and when is it okay to

00:15:45,629 --> 00:15:53,179
be angry or profane so these two come

00:15:48,749 --> 00:15:56,420
from a discussion in our news about

00:15:53,179 --> 00:16:01,230
marriage equality in Australia and

00:15:56,420 --> 00:16:08,189
whether or not the equal marriage rights

00:16:01,230 --> 00:16:09,569
afford extra rights to gay people and

00:16:08,189 --> 00:16:12,929
these two comments come from the same

00:16:09,569 --> 00:16:14,730
person so the one here your little

00:16:12,929 --> 00:16:16,470
inbred brain can't comprehend this it's

00:16:14,730 --> 00:16:18,629
getting a little like it starts out are

00:16:16,470 --> 00:16:19,860
you really that stupid and then not

00:16:18,629 --> 00:16:21,869
surprised you're a little inbred brain

00:16:19,860 --> 00:16:25,529
can't comprehend and then there's a

00:16:21,869 --> 00:16:31,279
reaction and then we get longer

00:16:25,529 --> 00:16:31,279
I don't pause for you

00:16:44,459 --> 00:16:51,129
so what would what should we do here if

00:16:47,649 --> 00:16:52,839
we are you know say that we don't

00:16:51,129 --> 00:16:54,430
operate them at the scale of Reddit and

00:16:52,839 --> 00:16:57,430
said we have just a few comments what

00:16:54,430 --> 00:17:00,339
should we do with this one can we leave

00:16:57,430 --> 00:17:01,689
it up on the site should we have a heart

00:17:00,339 --> 00:17:08,530
to hurt but the person who posted it

00:17:01,689 --> 00:17:12,069
should we take it down edit it I feel

00:17:08,530 --> 00:17:15,640
like we should maybe not have a heart to

00:17:12,069 --> 00:17:20,380
her but just be like all right violation

00:17:15,640 --> 00:17:23,500
of our code of conduct was this one yeah

00:17:20,380 --> 00:17:25,959
I think that one was okay I think I

00:17:23,500 --> 00:17:30,220
think all of this sort of just like you

00:17:25,959 --> 00:17:32,770
say this making putting up a wall with

00:17:30,220 --> 00:17:36,250
criticism it's not gonna have the other

00:17:32,770 --> 00:17:37,990
person care at all about what you're

00:17:36,250 --> 00:17:40,750
trying to about the point you're trying

00:17:37,990 --> 00:17:43,320
to get across like totally defeats the

00:17:40,750 --> 00:17:49,900
purpose of trying to raise awareness

00:17:43,320 --> 00:17:52,600
criticize someone first what is the goal

00:17:49,900 --> 00:17:54,880
here is not to change the mind of the

00:17:52,600 --> 00:17:57,970
little inbred brain but to signal to

00:17:54,880 --> 00:18:00,909
other people who realest thread who also

00:17:57,970 --> 00:18:06,220
want to enjoy equal rights that someone

00:18:00,909 --> 00:18:09,940
is on their side so what if we are

00:18:06,220 --> 00:18:14,440
leaving what if we got readers to know

00:18:09,940 --> 00:18:17,320
that conversations have unequal emotion

00:18:14,440 --> 00:18:20,230
and that that unequal emotion has some

00:18:17,320 --> 00:18:23,409
route somewhere which makes it different

00:18:20,230 --> 00:18:25,990
from this one where there's a threat of

00:18:23,409 --> 00:18:28,570
violence and profanity which I read is

00:18:25,990 --> 00:18:32,950
different from this one and from this

00:18:28,570 --> 00:18:35,140
one where I would agree that these are

00:18:32,950 --> 00:18:38,470
not the most productive ways that this

00:18:35,140 --> 00:18:42,190
person could have responded but they do

00:18:38,470 --> 00:18:46,720
have value in that they signal to us

00:18:42,190 --> 00:18:49,500
what is a where is a moment of conflict

00:18:46,720 --> 00:18:53,440
that is so great that it leads to

00:18:49,500 --> 00:18:56,380
emotionality and to readers to hear

00:18:53,440 --> 00:18:57,130
somebody is so supportive of your

00:18:56,380 --> 00:19:00,190
position

00:18:57,130 --> 00:19:02,530
that they go apeshit about it doesn't

00:19:00,190 --> 00:19:04,510
mean that going that way is appropriate

00:19:02,530 --> 00:19:06,429
but does it mean that we want to take it

00:19:04,510 --> 00:19:11,320
down or do we want to sanction in a

00:19:06,429 --> 00:19:19,390
different way that is supportive of the

00:19:11,320 --> 00:19:21,039
position but not the expression yeah I

00:19:19,390 --> 00:19:22,570
like the pervert so I may call you an

00:19:21,039 --> 00:19:26,650
inbred piece of but that doesn't

00:19:22,570 --> 00:19:29,409
take away your rights that's like

00:19:26,650 --> 00:19:31,990
something that like I would want to keep

00:19:29,409 --> 00:19:33,100
but then like for me it's the personal

00:19:31,990 --> 00:19:36,520
attack which is you are a disgusting

00:19:33,100 --> 00:19:40,270
excuse for a human being which is kind

00:19:36,520 --> 00:19:43,659
of like not like your previous statement

00:19:40,270 --> 00:19:45,130
I don't love it but I feel like you're

00:19:43,659 --> 00:19:47,230
not trying to personally attack the

00:19:45,130 --> 00:19:51,640
other person whereas you're discussing

00:19:47,230 --> 00:19:55,179
excuse so that's too far when we go

00:19:51,640 --> 00:19:56,950
personally going too far yeah okay yeah

00:19:55,179 --> 00:20:00,100
I just feel like people who write like

00:19:56,950 --> 00:20:05,789
that oughta have a filtering system on

00:20:00,100 --> 00:20:07,780
their own we made up for it it's sent I

00:20:05,789 --> 00:20:10,000
mean if you can go back to your thing

00:20:07,780 --> 00:20:13,210
about context right like what's the

00:20:10,000 --> 00:20:14,890
social context of this or either of

00:20:13,210 --> 00:20:16,750
these people gonna walk away feeling

00:20:14,890 --> 00:20:18,130
like emotionally damaged and be like I

00:20:16,750 --> 00:20:20,679
can't believe they call me an inbred

00:20:18,130 --> 00:20:22,210
piece of or it is right I'm not

00:20:20,679 --> 00:20:26,710
necessarily convinced that they are I

00:20:22,210 --> 00:20:28,720
mean I like I like oh this is just a

00:20:26,710 --> 00:20:31,990
show that we have polarized attitudes we

00:20:28,720 --> 00:20:36,220
already know that and so I guess showing

00:20:31,990 --> 00:20:40,230
this what's the point in allowing sort

00:20:36,220 --> 00:20:43,090
of crass ways of communicating to exist

00:20:40,230 --> 00:20:46,390
in some in some channels it's allowed

00:20:43,090 --> 00:20:48,850
because the readers expect that but like

00:20:46,390 --> 00:20:51,220
and I think for me and some channels

00:20:48,850 --> 00:20:55,200
it's allowed because the emotion is

00:20:51,220 --> 00:20:59,610
helpful for understanding the issue that

00:20:55,200 --> 00:21:03,010
you say craft I say emotive cool and

00:20:59,610 --> 00:21:04,720
while I still agree there are more there

00:21:03,010 --> 00:21:05,919
are less hurtful ways I'm probably more

00:21:04,720 --> 00:21:10,840
proactive ways to have this conversation

00:21:05,919 --> 00:21:12,430
or this individual but the when someone

00:21:10,840 --> 00:21:14,500
talking about whether or not you should

00:21:12,430 --> 00:21:17,200
enjoy the same rights as other humans

00:21:14,500 --> 00:21:19,930
you get heated and that you get heated

00:21:17,200 --> 00:21:21,940
as a useful thing for us to know because

00:21:19,930 --> 00:21:24,700
it signals that something matters here

00:21:21,940 --> 00:21:26,200
and that often our argument is that

00:21:24,700 --> 00:21:27,760
while if you would just be more

00:21:26,200 --> 00:21:29,440
reasonable that I could talk to you or

00:21:27,760 --> 00:21:31,240
if you would just take a different tone

00:21:29,440 --> 00:21:33,790
than I could talk to you and when we do

00:21:31,240 --> 00:21:35,860
that we set the rules for engagement in

00:21:33,790 --> 00:21:37,630
a way that silences the voices that are

00:21:35,860 --> 00:21:40,300
already been silenced so instead of

00:21:37,630 --> 00:21:42,280
saying I'll listen to you if you use

00:21:40,300 --> 00:21:43,900
nice words like that's what I say to my

00:21:42,280 --> 00:21:46,150
five-year-old but to his grown-up I say

00:21:43,900 --> 00:21:48,610
wow I see that you're really angry can

00:21:46,150 --> 00:21:50,770
you help me understand and that those

00:21:48,610 --> 00:21:53,200
reactions should be different instead of

00:21:50,770 --> 00:21:55,510
silencing we should have any how can we

00:21:53,200 --> 00:21:57,070
turn this productive instead of when you

00:21:55,510 --> 00:21:59,350
can calm down that I can talk to you

00:21:57,070 --> 00:22:02,740
it's a very paternalistic very white way

00:21:59,350 --> 00:22:06,940
for us to respond so if we instead say I

00:22:02,740 --> 00:22:08,950
hear that you are angry let's find a way

00:22:06,940 --> 00:22:10,780
for that anger to be productive instead

00:22:08,950 --> 00:22:13,750
of saying I'll talk to you when you're

00:22:10,780 --> 00:22:16,540
done being angry or maybe you flag this

00:22:13,750 --> 00:22:19,750
with this icon of this person's whose

00:22:16,540 --> 00:22:22,930
head on fire like angry like an angry

00:22:19,750 --> 00:22:24,370
emoji of sorts and you started dim the

00:22:22,930 --> 00:22:25,900
content and someone has to actually

00:22:24,370 --> 00:22:29,500
click on it they want to read this

00:22:25,900 --> 00:22:33,010
heated like basically yeah I've even two

00:22:29,500 --> 00:22:35,110
users to like you know kind of an anger

00:22:33,010 --> 00:22:37,600
Turner warning yeah it's it's like it's

00:22:35,110 --> 00:22:40,050
like when NPR's is neo explicit content

00:22:37,600 --> 00:22:42,520
or something follows whatever it's like

00:22:40,050 --> 00:22:44,260
beware reader but then you have to be

00:22:42,520 --> 00:22:45,490
careful about what well and we'll come

00:22:44,260 --> 00:22:48,310
back to this but what do we mark as

00:22:45,490 --> 00:22:50,770
explicit content and NPR is a good

00:22:48,310 --> 00:22:52,510
example because the a lot of times IPR

00:22:50,770 --> 00:22:55,390
will flag something like profanity but

00:22:52,510 --> 00:22:58,210
they won't flag discussions of violence

00:22:55,390 --> 00:22:59,110
so if you're commuting in the car and

00:22:58,210 --> 00:23:00,670
they're about to talk to you about

00:22:59,110 --> 00:23:08,740
really horrible deaths

00:23:00,670 --> 00:23:09,970
they don't yeah okay so I want you to

00:23:08,740 --> 00:23:11,590
remember the two ways that you talked

00:23:09,970 --> 00:23:12,760
about how we might want to do with this

00:23:11,590 --> 00:23:15,220
because that's gonna come up in my next

00:23:12,760 --> 00:23:18,490
life so here then I'm saying okay what

00:23:15,220 --> 00:23:20,530
if we could try to do some de-escalation

00:23:18,490 --> 00:23:21,640
so instead of just silencing the

00:23:20,530 --> 00:23:22,780
escalation we try to bring the

00:23:21,640 --> 00:23:24,480
conversation back down

00:23:22,780 --> 00:23:26,790
so the first

00:23:24,480 --> 00:23:29,400
caris can we learn des coding behaviors

00:23:26,790 --> 00:23:30,660
so can we figure out what already works

00:23:29,400 --> 00:23:33,540
because some of these conversations

00:23:30,660 --> 00:23:35,309
recover and then the second thing is can

00:23:33,540 --> 00:23:36,720
we teach them to a bot so that it can

00:23:35,309 --> 00:23:39,299
act as an effective third party

00:23:36,720 --> 00:23:41,120
bystander to intervene so some

00:23:39,299 --> 00:23:47,040
conversations do go back to productive

00:23:41,120 --> 00:23:49,260
and what is already out there so some

00:23:47,040 --> 00:23:51,360
ways that I have been inspired to work

00:23:49,260 --> 00:23:52,650
on this because one on the left is a

00:23:51,360 --> 00:23:55,559
reference to something called rethink

00:23:52,650 --> 00:23:57,900
and it was developed by a Naperville

00:23:55,559 --> 00:24:01,620
teenager and she went on Shark Tank

00:23:57,900 --> 00:24:03,390
she's got an app that you can install so

00:24:01,620 --> 00:24:05,130
that before you post something if it's

00:24:03,390 --> 00:24:08,850
nasty it says are you sure you want to

00:24:05,130 --> 00:24:10,559
post that and often people then don't

00:24:08,850 --> 00:24:12,299
post the terrible thing that they were

00:24:10,559 --> 00:24:12,840
gonna put so this is where the deep

00:24:12,299 --> 00:24:15,090
breath

00:24:12,840 --> 00:24:17,309
folks comes from that if there were a

00:24:15,090 --> 00:24:21,179
way for us to just interrupt the heated

00:24:17,309 --> 00:24:23,970
miss that a lot of people she found 93%

00:24:21,179 --> 00:24:27,090
of teenagers or teenagers 93% of the

00:24:23,970 --> 00:24:29,040
time we're willing to not post that in

00:24:27,090 --> 00:24:30,809
her instance she's looking at cyber

00:24:29,040 --> 00:24:32,400
bullying not post that bullying comment

00:24:30,809 --> 00:24:34,890
to post something else instead so if we

00:24:32,400 --> 00:24:38,700
can just interrupt that feedback to our

00:24:34,890 --> 00:24:41,340
lizard brains that that can be effective

00:24:38,700 --> 00:24:42,809
and then on their right this number of

00:24:41,340 --> 00:24:46,980
paper I think is in political behavior

00:24:42,809 --> 00:24:50,400
he's a PhD student I think anyway you

00:24:46,980 --> 00:24:52,710
and he used BOTS to respond to people

00:24:50,400 --> 00:24:56,030
who had posted to white men who had

00:24:52,710 --> 00:24:59,730
posted the Edward on Twitter and he had

00:24:56,030 --> 00:25:02,340
lots of different types so he had box

00:24:59,730 --> 00:25:03,750
that were pretending and they're not

00:25:02,340 --> 00:25:06,440
really back because it was he did it by

00:25:03,750 --> 00:25:09,299
hands they're more friendly sock puppets

00:25:06,440 --> 00:25:11,490
where they were impersonating white or

00:25:09,299 --> 00:25:13,890
black men with varying levels of social

00:25:11,490 --> 00:25:16,290
power as indicated by their follower

00:25:13,890 --> 00:25:17,910
numbers and so here when it says

00:25:16,290 --> 00:25:19,740
in-group hi that's the one that had the

00:25:17,910 --> 00:25:22,530
most effect so this is when and a

00:25:19,740 --> 00:25:24,660
Twitter account that looks like a white

00:25:22,530 --> 00:25:27,330
man with high social power says to

00:25:24,660 --> 00:25:28,650
another white man hey there's a real

00:25:27,330 --> 00:25:31,260
human being at the end of that quit

00:25:28,650 --> 00:25:33,870
harassing them that the instance of

00:25:31,260 --> 00:25:37,530
their and were used over time went down

00:25:33,870 --> 00:25:37,890
for the first week panels B and C in

00:25:37,530 --> 00:25:41,850
this

00:25:37,890 --> 00:25:43,200
same an image in his paper showed that

00:25:41,850 --> 00:25:45,720
there's a decay function though so he

00:25:43,200 --> 00:25:49,440
has a one-week panel two week and at a

00:25:45,720 --> 00:25:51,510
month and then so there's a it's not a

00:25:49,440 --> 00:25:52,920
super fast DK function I think a month

00:25:51,510 --> 00:25:57,180
on social media is actually a pretty

00:25:52,920 --> 00:25:59,850
long time but the rate of n renews goes

00:25:57,180 --> 00:26:03,000
back up but over a month it's still

00:25:59,850 --> 00:26:04,920
lower than it was before but all of

00:26:03,000 --> 00:26:06,690
those error bars are pretty wide - so

00:26:04,920 --> 00:26:08,250
this was a relatively small sorry but

00:26:06,690 --> 00:26:11,040
it's a promising like if you focus on

00:26:08,250 --> 00:26:12,630
just one thing and you can automate a

00:26:11,040 --> 00:26:16,700
response you might be able to make a

00:26:12,630 --> 00:26:18,630
difference so your instincts aren't good

00:26:16,700 --> 00:26:23,250
that there are ways that we might

00:26:18,630 --> 00:26:26,640
building - so my approach is to focus as

00:26:23,250 --> 00:26:28,830
you said katelynn on personal insults so

00:26:26,640 --> 00:26:31,380
instead of focusing on profanity or on

00:26:28,830 --> 00:26:33,270
emotional language we focus on when you

00:26:31,380 --> 00:26:36,090
say something really mean about the

00:26:33,270 --> 00:26:37,590
person you're talking to that's where we

00:26:36,090 --> 00:26:40,170
want to interrupt so instead of trying

00:26:37,590 --> 00:26:42,690
to solve all bad things we say just when

00:26:40,170 --> 00:26:45,930
you are attacking someone else in this

00:26:42,690 --> 00:26:48,750
conversation and then sex second pick a

00:26:45,930 --> 00:26:50,640
reply that's designed to de-escalate and

00:26:48,750 --> 00:26:52,620
then we'll measure the rate of insults

00:26:50,640 --> 00:26:56,310
much like monger measured the rate of

00:26:52,620 --> 00:26:58,560
anger use so on this detection thing it

00:26:56,310 --> 00:27:02,210
turns out that is also really hard so

00:26:58,560 --> 00:27:05,220
there we had a few models the first one

00:27:02,210 --> 00:27:08,280
we have I built a model with colleagues

00:27:05,220 --> 00:27:11,430
at Illinois Tech on Instagram that is

00:27:08,280 --> 00:27:13,110
designed to the goal there was to help

00:27:11,430 --> 00:27:14,730
parents know whether or not there was

00:27:13,110 --> 00:27:18,360
something brewing on their child's in

00:27:14,730 --> 00:27:20,310
shrimpie and it's a pretty good model it

00:27:18,360 --> 00:27:24,060
can predict you know it's in the point

00:27:20,310 --> 00:27:26,790
seven point eight for accuracy and that

00:27:24,060 --> 00:27:28,470
was good enough for parents and we

00:27:26,790 --> 00:27:30,270
thought okay let's first try to see

00:27:28,470 --> 00:27:33,030
whether the Instagram model ports to

00:27:30,270 --> 00:27:34,500
Reddit and no it does not

00:27:33,030 --> 00:27:36,690
there are two main reasons that it

00:27:34,500 --> 00:27:38,550
doesn't the profanity threshold on

00:27:36,690 --> 00:27:40,320
reddit is very high you're expected to

00:27:38,550 --> 00:27:42,000
swear a lot and run it so if it's over

00:27:40,320 --> 00:27:44,370
tuned for profanity it doesn't work

00:27:42,000 --> 00:27:46,260
there and the second is that the

00:27:44,370 --> 00:27:48,870
precision recall trade-off of the model

00:27:46,260 --> 00:27:50,679
is very different for bots than it is

00:27:48,870 --> 00:27:55,220
for a human in

00:27:50,679 --> 00:27:58,250
so that thank you yeah I will

00:27:55,220 --> 00:28:03,169
so um here that this is what we show

00:27:58,250 --> 00:28:05,330
parents so precision is how many of when

00:28:03,169 --> 00:28:08,149
we say that something is bad how often

00:28:05,330 --> 00:28:10,690
are we right so a false positive is if

00:28:08,149 --> 00:28:14,480
we say that it's bad and it's not really

00:28:10,690 --> 00:28:16,070
and the and that impact precision and

00:28:14,480 --> 00:28:20,480
then recall is how many of the things do

00:28:16,070 --> 00:28:22,789
we catch so precision are we right when

00:28:20,480 --> 00:28:25,789
we flag it recall did we catch all the

00:28:22,789 --> 00:28:27,559
stuff okay and in most cases for

00:28:25,789 --> 00:28:30,980
modeling you're making trade-offs there

00:28:27,559 --> 00:28:33,110
that as you get better at being right

00:28:30,980 --> 00:28:34,669
you will miss something so your

00:28:33,110 --> 00:28:36,590
precision goes up with but you'll recall

00:28:34,669 --> 00:28:38,179
those down and as you get better at

00:28:36,590 --> 00:28:40,250
catching a lot of stuff you're gonna

00:28:38,179 --> 00:28:42,230
catch more stuff that isn't really bad

00:28:40,250 --> 00:28:46,730
and so that where your precision goes

00:28:42,230 --> 00:28:48,740
down but your recall goes up because the

00:28:46,730 --> 00:28:50,659
Instagram model was designed for parents

00:28:48,740 --> 00:28:52,190
and they didn't want us to do any

00:28:50,659 --> 00:28:54,500
intervention they just want parents

00:28:52,190 --> 00:28:56,029
wanted us to warn them so to see here

00:28:54,500 --> 00:28:58,220
there's hostilities predicted and

00:28:56,029 --> 00:29:01,580
hostilities present so this is saying in

00:28:58,220 --> 00:29:03,740
the Instagram posts for it's always me

00:29:01,580 --> 00:29:05,750
we think that something is gonna happen

00:29:03,740 --> 00:29:07,519
here student that's what the yellow

00:29:05,750 --> 00:29:08,809
means and in Reds baby we say there's

00:29:07,519 --> 00:29:11,509
actually something has already happened

00:29:08,809 --> 00:29:13,490
and then what parents wanted to do was

00:29:11,509 --> 00:29:15,379
to look at those posts themselves and

00:29:13,490 --> 00:29:16,789
then talk to their kids about them they

00:29:15,379 --> 00:29:20,450
didn't want the system to do any of that

00:29:16,789 --> 00:29:22,519
and so it was okay that we had moderate

00:29:20,450 --> 00:29:24,230
precision and good recall because

00:29:22,519 --> 00:29:27,649
parents wanted to be warned if it might

00:29:24,230 --> 00:29:29,330
be there but they could tolerate low

00:29:27,649 --> 00:29:33,500
precision because they didn't really

00:29:29,330 --> 00:29:35,600
mind reading kanakam spreads but if

00:29:33,500 --> 00:29:37,070
you're gonna have a bot do something you

00:29:35,600 --> 00:29:38,929
want really high precision because

00:29:37,070 --> 00:29:42,320
people get really angry if the bot

00:29:38,929 --> 00:29:44,389
interrupts when it shouldn't and so then

00:29:42,320 --> 00:29:46,460
we tried a rule based model so this is

00:29:44,389 --> 00:29:47,570
where we tried like we did earlier to

00:29:46,460 --> 00:29:49,580
come up with all the different ways that

00:29:47,570 --> 00:29:51,980
we would make rules about what you

00:29:49,580 --> 00:29:54,289
should be able to do so we tried to use

00:29:51,980 --> 00:29:57,009
policy documents and to implement them

00:29:54,289 --> 00:29:59,179
programmatically and that model

00:29:57,009 --> 00:30:01,340
completely failed it was even worse than

00:29:59,179 --> 00:30:03,590
the instrument model ported to reddit

00:30:01,340 --> 00:30:06,440
that we just because

00:30:03,590 --> 00:30:10,399
of the language and our access to prior

00:30:06,440 --> 00:30:11,779
histories for users and their previous

00:30:10,399 --> 00:30:13,009
comments there just wasn't a way for us

00:30:11,779 --> 00:30:15,320
to get the whole base model to work

00:30:13,009 --> 00:30:16,820
because the list of rules and then the

00:30:15,320 --> 00:30:17,990
adaptations for breaking the rules just

00:30:16,820 --> 00:30:19,960
kept getting longer and longer and

00:30:17,990 --> 00:30:23,629
longer and so this model needed constant

00:30:19,960 --> 00:30:25,340
tuning so the labor overall didn't

00:30:23,629 --> 00:30:27,049
actually go down and if we're trying to

00:30:25,340 --> 00:30:29,809
help people not have to moderate as much

00:30:27,049 --> 00:30:33,190
that model didn't work so let me try a

00:30:29,809 --> 00:30:37,659
tuesday's model with a dictionary and

00:30:33,190 --> 00:30:40,639
spacy is a Python module that does the

00:30:37,659 --> 00:30:42,049
linguistic structure of sentences so

00:30:40,639 --> 00:30:43,850
Spacey is a really great tool for

00:30:42,049 --> 00:30:46,759
helping us figure out when is an insult

00:30:43,850 --> 00:30:48,230
directed at a human being and Noddy not

00:30:46,759 --> 00:30:49,639
just at a human but somebody who's in

00:30:48,230 --> 00:30:54,850
this conversation so these are

00:30:49,639 --> 00:30:58,009
essentially you are constructions or

00:30:54,850 --> 00:30:59,929
incentives they are like the this

00:30:58,009 --> 00:31:02,600
guy would not be caught by Spacey but if

00:30:59,929 --> 00:31:06,799
it were a few then it would be um so

00:31:02,600 --> 00:31:08,929
we're able to exploit the syntax and the

00:31:06,799 --> 00:31:10,789
structure of the sentence plus a

00:31:08,929 --> 00:31:12,679
dictionary component so we have the

00:31:10,789 --> 00:31:14,419
model happens in two stages says does

00:31:12,679 --> 00:31:16,639
this look like an insult based on this

00:31:14,419 --> 00:31:19,580
list of ways that you can insult people

00:31:16,639 --> 00:31:21,799
which is really long and is it directed

00:31:19,580 --> 00:31:23,119
at somebody and I say not quite failed

00:31:21,799 --> 00:31:26,059
there because we actually ran a pilot

00:31:23,119 --> 00:31:28,399
that I'll report on to see how well it

00:31:26,059 --> 00:31:30,049
did and we decided that we needed even

00:31:28,399 --> 00:31:31,970
higher precision because people do get

00:31:30,049 --> 00:31:34,970
really mad if you interrupt when you

00:31:31,970 --> 00:31:36,980
shouldn't so we tried another model

00:31:34,970 --> 00:31:39,639
where we replaced the dictionary part

00:31:36,980 --> 00:31:44,029
with a deep learning model trained on

00:31:39,639 --> 00:31:46,639
and labeled insults Pavlov so we

00:31:44,029 --> 00:31:49,639
basically just use oh I can link to it

00:31:46,639 --> 00:31:51,289
in github so this one we take basically

00:31:49,639 --> 00:31:53,059
an out-of-the-box deep learning model

00:31:51,289 --> 00:31:57,049
that has been trained on somebody else's

00:31:53,059 --> 00:31:58,759
insult list and then we add the is it

00:31:57,049 --> 00:32:01,789
directed any human being component and

00:31:58,759 --> 00:32:03,590
the precision here gets much better so

00:32:01,789 --> 00:32:07,909
here are my precision recall and we're

00:32:03,590 --> 00:32:10,070
using f1 scores here because the samples

00:32:07,909 --> 00:32:11,389
are really unbalanced so we think that

00:32:10,070 --> 00:32:12,860
there's a lot of really terrible stuff

00:32:11,389 --> 00:32:14,299
online but compared to all the stuff

00:32:12,860 --> 00:32:16,130
online terrible stuff it's really very

00:32:14,299 --> 00:32:18,560
small

00:32:16,130 --> 00:32:20,750
katelyn the precision again is like

00:32:18,560 --> 00:32:22,460
though when I'm how often my my right

00:32:20,750 --> 00:32:24,500
recall how much of the stuff do a catch

00:32:22,460 --> 00:32:27,740
and then f1 is a balance between those

00:32:24,500 --> 00:32:30,710
two and so here even though the

00:32:27,740 --> 00:32:32,420
Instagram model has a higher f1 score we

00:32:30,710 --> 00:32:35,060
prefer that the learning model because

00:32:32,420 --> 00:32:36,830
it has a higher precision but if we were

00:32:35,060 --> 00:32:39,020
just writing a paper for a triple-a high

00:32:36,830 --> 00:32:40,040
or something you would use Instagram

00:32:39,020 --> 00:32:41,660
because it has better overall

00:32:40,040 --> 00:32:43,820
performance so it's important that we

00:32:41,660 --> 00:32:45,620
think about what are we gonna do with

00:32:43,820 --> 00:32:48,620
the results of the model when choosing a

00:32:45,620 --> 00:32:52,550
model and then the baseline SVM so that

00:32:48,620 --> 00:32:54,530
was the rule based model it just I mean

00:32:52,550 --> 00:32:57,590
precision at 2007 we'd be better off

00:32:54,530 --> 00:33:01,190
randomly selecting things so that was

00:32:57,590 --> 00:33:01,880
really bad and adding your data I did

00:33:01,190 --> 00:33:04,130
not make it better

00:33:01,880 --> 00:33:05,840
so often artists thing it's like oh

00:33:04,130 --> 00:33:07,610
we'll just add more data as though

00:33:05,840 --> 00:33:10,250
reading and tilting comments all day for

00:33:07,610 --> 00:33:16,040
8 cents a tag it's not a terrible way to

00:33:10,250 --> 00:33:17,930
spend time I okay so now we're using

00:33:16,040 --> 00:33:20,660
this deep learning two-stage model we

00:33:17,930 --> 00:33:22,670
have a pretty high precision we're still

00:33:20,660 --> 00:33:25,430
working on ways that we can increase the

00:33:22,670 --> 00:33:27,350
precision even higher and then we start

00:33:25,430 --> 00:33:29,810
to wonder well is there actually gonna

00:33:27,350 --> 00:33:31,880
be enough stuff if we make our precision

00:33:29,810 --> 00:33:33,740
95 percent is there gonna be enough

00:33:31,880 --> 00:33:38,210
stuff for us to respond to didn't get a

00:33:33,740 --> 00:33:40,130
higher high enough incident rate that we

00:33:38,210 --> 00:33:42,140
can have control groups and let our

00:33:40,130 --> 00:33:44,330
experiment well we're thinking about

00:33:42,140 --> 00:33:46,970
experimenting on reddit where there are

00:33:44,330 --> 00:33:50,300
33 insults per minute so yeah there's

00:33:46,970 --> 00:33:53,270
plenty of data for us to respond to and

00:33:50,300 --> 00:33:55,160
this is excluding the places so when we

00:33:53,270 --> 00:33:58,040
look at reddit we are excluding places

00:33:55,160 --> 00:34:00,560
where you have to be 18 and anything

00:33:58,040 --> 00:34:01,490
that's been in quarantined and places

00:34:00,560 --> 00:34:04,550
where you have to have a certain level

00:34:01,490 --> 00:34:05,540
of karma to post but otherwise you're

00:34:04,550 --> 00:34:08,060
tweaking already

00:34:05,540 --> 00:34:10,820
substance the same already it serves as

00:34:08,060 --> 00:34:13,750
possible points of intervention okay but

00:34:10,820 --> 00:34:16,159
not the same so we're actually I think

00:34:13,750 --> 00:34:19,460
like do you have local models of what an

00:34:16,159 --> 00:34:24,260
insult is for any given one so I know

00:34:19,460 --> 00:34:28,100
but the when you take out well let's see

00:34:24,260 --> 00:34:29,750
what is my next slide we don't you

00:34:28,100 --> 00:34:32,510
mentioned Eric Gilbert earlier

00:34:29,750 --> 00:34:35,480
hoping to use Eric and each wires model

00:34:32,510 --> 00:34:39,440
of local reddit rules to improve the

00:34:35,480 --> 00:34:44,659
precision of reddit reddit a subreddit

00:34:39,440 --> 00:34:46,940
sure so and I think that the places

00:34:44,659 --> 00:34:49,850
where we really struggle are in gaming

00:34:46,940 --> 00:34:53,060
yes and in places that are sort of aggro

00:34:49,850 --> 00:34:56,090
anyway so some of them that get flagged

00:34:53,060 --> 00:34:58,400
or they get quarantined then pop up as

00:34:56,090 --> 00:35:02,810
another one so the red pill or in cells

00:34:58,400 --> 00:35:06,920
and that whole group of basically angry

00:35:02,810 --> 00:35:08,870
angry man subreddits we those have

00:35:06,920 --> 00:35:11,930
really high incident rates and then in

00:35:08,870 --> 00:35:15,080
the general discussion places like our

00:35:11,930 --> 00:35:17,150
politics and our news that the

00:35:15,080 --> 00:35:20,420
thresholds are not zero but they're also

00:35:17,150 --> 00:35:24,650
not very high that the moderators in

00:35:20,420 --> 00:35:26,480
those places remove those comments

00:35:24,650 --> 00:35:29,300
pretty frequently but I just recently I

00:35:26,480 --> 00:35:30,470
was like ask science or ask historians

00:35:29,300 --> 00:35:32,690
which is ready

00:35:30,470 --> 00:35:35,440
lockdown yeah because that's been

00:35:32,690 --> 00:35:39,530
between our science that's the same as

00:35:35,440 --> 00:35:42,140
you know Australia it's it's sort of for

00:35:39,530 --> 00:35:44,980
people who find our Australia to be too

00:35:42,140 --> 00:35:47,630
too like constrained and the ban polite

00:35:44,980 --> 00:35:48,700
so it's entirely Australians being rude

00:35:47,630 --> 00:35:53,860
to each other all the time

00:35:48,700 --> 00:35:57,790
and then just any model that blithely to

00:35:53,860 --> 00:36:02,720
Australia and and took say I don't know

00:35:57,790 --> 00:36:05,030
ask a historian and treated those as the

00:36:02,720 --> 00:36:07,490
same data source yeah would be shooting

00:36:05,030 --> 00:36:09,110
himself in the foot so in one way do we

00:36:07,490 --> 00:36:11,540
try to deal with that is that we look at

00:36:09,110 --> 00:36:14,870
what is the overall insult incident rate

00:36:11,540 --> 00:36:17,540
in this place hmm and above some certain

00:36:14,870 --> 00:36:19,100
threshold we don't interrupt okay so

00:36:17,540 --> 00:36:20,570
another place for this happens and I

00:36:19,100 --> 00:36:22,490
forget what this somewhere is called but

00:36:20,570 --> 00:36:24,440
it's where you post your own picture so

00:36:22,490 --> 00:36:29,180
that people will mock you yeah right the

00:36:24,440 --> 00:36:31,640
point is to be mean to interrupt in that

00:36:29,180 --> 00:36:33,650
place so yeah we look at sort of what

00:36:31,640 --> 00:36:35,000
are the overall incidents we started

00:36:33,650 --> 00:36:36,200
with what if we tried to do this on all

00:36:35,000 --> 00:36:38,750
of Reddit and then we took out

00:36:36,200 --> 00:36:40,400
quarantine and then we took out over 18

00:36:38,750 --> 00:36:42,480
this turns out people swear a lot while

00:36:40,400 --> 00:36:45,750
talking about porn

00:36:42,480 --> 00:36:47,820
and then we're working on where is that

00:36:45,750 --> 00:36:50,579
right threshold then you know if the

00:36:47,820 --> 00:36:53,310
regular incident rate for comments that

00:36:50,579 --> 00:36:55,050
survived on the site is you know ten

00:36:53,310 --> 00:36:57,780
percent or something then that's a good

00:36:55,050 --> 00:36:59,070
place for us to intervene where if most

00:36:57,780 --> 00:37:00,060
of the comments in that place have

00:36:59,070 --> 00:37:01,950
insults anyway

00:37:00,060 --> 00:37:05,910
that's probably it can be anyone that we

00:37:01,950 --> 00:37:13,410
shouldn't invite yeah so figuring out

00:37:05,910 --> 00:37:16,740
where should we respond is so now we

00:37:13,410 --> 00:37:18,450
have a reasonable model of a our

00:37:16,740 --> 00:37:20,880
personal insult and then we select a

00:37:18,450 --> 00:37:24,450
reply I guess the sub here is select a

00:37:20,880 --> 00:37:25,589
place where we should reply so there are

00:37:24,450 --> 00:37:27,000
three different ways that we look for

00:37:25,589 --> 00:37:28,890
replies so the first one is that you

00:37:27,000 --> 00:37:31,680
learn from the platform and then here's

00:37:28,890 --> 00:37:33,450
an example so one way that to deescalate

00:37:31,680 --> 00:37:35,430
or that was affected in the escalating

00:37:33,450 --> 00:37:36,660
was to say your openness to views that

00:37:35,430 --> 00:37:37,920
may not agree with your own is very

00:37:36,660 --> 00:37:40,470
refreshing and I want to let you know I

00:37:37,920 --> 00:37:42,829
appreciate your willingness to hear this

00:37:40,470 --> 00:37:45,480
really did occur on reddit I know that

00:37:42,829 --> 00:37:47,160
it was surprising to me to read this and

00:37:45,480 --> 00:37:48,960
I asked really asked the student I was

00:37:47,160 --> 00:37:52,020
like that's the that's your reply right

00:37:48,960 --> 00:37:53,790
just like no that actually happened so

00:37:52,020 --> 00:37:57,150
that wasn't effective we need to get

00:37:53,790 --> 00:37:58,319
people to stop being so we're looking

00:37:57,150 --> 00:38:01,079
for other ways that we can learn from

00:37:58,319 --> 00:38:03,420
the platform so we look for winded and

00:38:01,079 --> 00:38:05,819
insults occur and then in the next few

00:38:03,420 --> 00:38:08,190
comments what happened and were there

00:38:05,819 --> 00:38:10,079
more insults after to look for the

00:38:08,190 --> 00:38:13,619
candidates for de-escalation are those

00:38:10,079 --> 00:38:15,540
that appear after insults happen and

00:38:13,619 --> 00:38:18,119
they usually start with something like I

00:38:15,540 --> 00:38:20,819
appreciate or I apologize I understand

00:38:18,119 --> 00:38:23,040
you I misunderstood one of the sad

00:38:20,819 --> 00:38:25,079
things for us here is that most of those

00:38:23,040 --> 00:38:27,660
are I statements which means that BOTS

00:38:25,079 --> 00:38:30,869
can't do them because a bot can't

00:38:27,660 --> 00:38:34,339
apologize on behalf of someone else so

00:38:30,869 --> 00:38:37,230
it limits the space that we can learn

00:38:34,339 --> 00:38:39,270
but then we see some things like tone it

00:38:37,230 --> 00:38:41,310
down and stop posting stop trolling

00:38:39,270 --> 00:38:42,720
you're not wrong you sound reasonable

00:38:41,310 --> 00:38:47,970
those are things that third parties can

00:38:42,720 --> 00:38:49,650
do so and I appreciate we can our box

00:38:47,970 --> 00:38:51,030
can appreciate things it's just maybe

00:38:49,650 --> 00:38:52,800
not going to be as effective as an

00:38:51,030 --> 00:38:55,790
individual who's part of the conflict to

00:38:52,800 --> 00:38:58,280
say I appreciate or at all

00:38:55,790 --> 00:38:59,630
and then the second is that we look at

00:38:58,280 --> 00:39:01,430
the literature about conflicts and

00:38:59,630 --> 00:39:03,710
organizations so what can we learn from

00:39:01,430 --> 00:39:05,569
what we know about how we defuse

00:39:03,710 --> 00:39:10,220
conflict at work most of this is from

00:39:05,569 --> 00:39:11,630
organizational psychology okay so

00:39:10,220 --> 00:39:13,700
examples here are I can't believe how

00:39:11,630 --> 00:39:16,520
you guys are arguing back and forth I'm

00:39:13,700 --> 00:39:18,380
out or I understand your overall point

00:39:16,520 --> 00:39:19,010
but this is a not right way to discuss

00:39:18,380 --> 00:39:21,260
in here

00:39:19,010 --> 00:39:22,760
please stop insulting others what's with

00:39:21,260 --> 00:39:25,460
the insults there's no need for that in

00:39:22,760 --> 00:39:27,349
here they map to these four main ways

00:39:25,460 --> 00:39:30,470
from organizational psychology and then

00:39:27,349 --> 00:39:32,059
the fifth from interpersonal conflict

00:39:30,470 --> 00:39:34,160
escalation so are there ways that we can

00:39:32,059 --> 00:39:37,339
indicate compromise or avoidance

00:39:34,160 --> 00:39:40,510
smoothing or confronting and we're

00:39:37,339 --> 00:39:42,829
experimenting with what are the relative

00:39:40,510 --> 00:39:45,109
returns for each of these potential

00:39:42,829 --> 00:39:46,670
types of the escalation and so that's

00:39:45,109 --> 00:39:50,270
why we need a lot of escalation to

00:39:46,670 --> 00:39:53,299
respond to because we have you know 8 by

00:39:50,270 --> 00:39:56,059
35 experiment zone so it's gonna take a

00:39:53,299 --> 00:39:57,920
while for us to get enough data by humor

00:39:56,059 --> 00:39:59,210
strategy is using memes or jokes so if

00:39:57,920 --> 00:40:01,579
you've ever been involved in a reddit

00:39:59,210 --> 00:40:03,760
thread that went haywire sometimes it's

00:40:01,579 --> 00:40:06,380
because somebody inserts a non-sequitur

00:40:03,760 --> 00:40:10,400
and then we're wondering well what if we

00:40:06,380 --> 00:40:11,960
could be positively disruptive so if

00:40:10,400 --> 00:40:15,230
something is really nasty if we can't

00:40:11,960 --> 00:40:17,329
control at the platform like rethink

00:40:15,230 --> 00:40:19,579
does so we can't get people to pause by

00:40:17,329 --> 00:40:21,079
limiting how frequently they can post or

00:40:19,579 --> 00:40:21,349
asking them to take a deep breath or

00:40:21,079 --> 00:40:23,720
something

00:40:21,349 --> 00:40:27,530
can we trick them into taking a deep

00:40:23,720 --> 00:40:31,400
breath by being funny so what if we put

00:40:27,530 --> 00:40:32,809
up a cat meme like can you just what if

00:40:31,400 --> 00:40:35,210
you just laughed for a second and we

00:40:32,809 --> 00:40:36,619
trick your psychology and we turn off

00:40:35,210 --> 00:40:39,559
your lizard brain for a minute maybe you

00:40:36,619 --> 00:40:40,130
don't respond or we give you a Michael

00:40:39,559 --> 00:40:45,140
Jordan gift

00:40:40,130 --> 00:40:47,059
who doesn't want one one of my favorites

00:40:45,140 --> 00:40:52,130
is Taylor Swift saying why are you gonna

00:40:47,059 --> 00:40:54,970
be so mean that so those are some of the

00:40:52,130 --> 00:40:57,349
replies that we use and then the last

00:40:54,970 --> 00:41:01,849
it's from conflict and intimate partner

00:40:57,349 --> 00:41:03,079
relationships so here an example is I

00:41:01,849 --> 00:41:04,339
understand that you're upset but could

00:41:03,079 --> 00:41:06,230
you please remember to respect other

00:41:04,339 --> 00:41:08,369
people's experiences I'd appreciate it

00:41:06,230 --> 00:41:10,859
and these come from

00:41:08,369 --> 00:41:12,630
the four horsemen so the Gottman

00:41:10,859 --> 00:41:15,749
Institute is about how to keep people

00:41:12,630 --> 00:41:17,519
married and so on the rights or sorry on

00:41:15,749 --> 00:41:20,460
the left are the four horsemen these are

00:41:17,519 --> 00:41:22,529
the big threats to your communication

00:41:20,460 --> 00:41:25,140
style so criticism contempt

00:41:22,529 --> 00:41:26,849
defensiveness and stonewalling and then

00:41:25,140 --> 00:41:30,239
on the right are antidotes to those

00:41:26,849 --> 00:41:32,640
types of conversations and we're in that

00:41:30,239 --> 00:41:34,559
contempt layer which says attacking a

00:41:32,640 --> 00:41:38,700
sense of self with an intent to insult

00:41:34,559 --> 00:41:40,890
or abuse so they found that marriage is

00:41:38,700 --> 00:41:43,469
where people have a communication style

00:41:40,890 --> 00:41:44,999
of contempt don't last very long and

00:41:43,469 --> 00:41:47,969
that one way that you can try to

00:41:44,999 --> 00:41:50,700
inoculate the relationship is to build a

00:41:47,969 --> 00:41:53,249
culture of appreciation so and you can

00:41:50,700 --> 00:41:55,529
reduce the incident of insult and sort

00:41:53,249 --> 00:41:59,339
of attacking your humanity which is what

00:41:55,529 --> 00:42:02,160
makes a racial slur or an ethnic slur or

00:41:59,339 --> 00:42:04,950
a sexual orientation sore so powerful is

00:42:02,160 --> 00:42:06,839
that they go directly to the humaneness

00:42:04,950 --> 00:42:09,029
of the person at whom Whistler is

00:42:06,839 --> 00:42:11,489
directed so they are indicators of

00:42:09,029 --> 00:42:14,969
contempt so we're looking for ways that

00:42:11,489 --> 00:42:17,369
our bot could remind each other about

00:42:14,969 --> 00:42:21,960
positive qualities organize to find

00:42:17,369 --> 00:42:25,640
gratitude or appreciation and then we

00:42:21,960 --> 00:42:28,859
have those we have 40 different

00:42:25,640 --> 00:42:32,910
responses that the bot might post and

00:42:28,859 --> 00:42:34,529
then we have the candidate reddit or

00:42:32,910 --> 00:42:38,039
subreddit like is this a good place for

00:42:34,529 --> 00:42:39,719
us to try to interrupt and then we test

00:42:38,039 --> 00:42:41,420
whether or not the person who made the

00:42:39,719 --> 00:42:43,589
insult is already in our population

00:42:41,420 --> 00:42:45,739
excuse me um in soldiers we try to

00:42:43,589 --> 00:42:49,710
interact with each person only one time

00:42:45,739 --> 00:42:51,869
and so we sort them into so we have an

00:42:49,710 --> 00:42:53,400
insult that we've detected it's in a

00:42:51,869 --> 00:42:56,219
subreddit where we feel like we might be

00:42:53,400 --> 00:42:57,989
able to do something good and then we

00:42:56,219 --> 00:42:59,549
assign that person either to treatment

00:42:57,989 --> 00:43:03,630
or control and if they you assigned to

00:42:59,549 --> 00:43:04,920
control 40 different ways to respond and

00:43:03,630 --> 00:43:07,170
then how do we know as we did anything

00:43:04,920 --> 00:43:09,089
right so we measure the rate of insults

00:43:07,170 --> 00:43:12,869
and the life of the conversation so this

00:43:09,089 --> 00:43:16,920
is did we reduce the contempt and was

00:43:12,869 --> 00:43:20,759
conversation able to continue and we did

00:43:16,920 --> 00:43:21,630
this for a day and what we found is our

00:43:20,759 --> 00:43:24,210
best for

00:43:21,630 --> 00:43:26,549
reply was why you gotta be so mean can't

00:43:24,210 --> 00:43:29,369
we all just get along it got us a lot of

00:43:26,549 --> 00:43:31,500
positive karma it got some up votes and

00:43:29,369 --> 00:43:33,150
people made some more funny comments

00:43:31,500 --> 00:43:36,359
afterwards and so the thread turned

00:43:33,150 --> 00:43:39,059
towards humor and away from anger and

00:43:36,359 --> 00:43:40,529
insult our worst-performing reply was I

00:43:39,059 --> 00:43:43,500
can't believe how you guys are going are

00:43:40,529 --> 00:43:46,380
we going back and forth I'm out which

00:43:43,500 --> 00:43:48,299
the threat to leave is very common on

00:43:46,380 --> 00:43:52,079
Reddit and we found that it was

00:43:48,299 --> 00:43:54,450
sometimes useful for that when real

00:43:52,079 --> 00:43:56,549
users made threats to leave the

00:43:54,450 --> 00:44:00,089
conversation often ended which is one

00:43:56,549 --> 00:44:01,710
way to get infiltrate to go down but we

00:44:00,089 --> 00:44:03,660
want productive conversations to

00:44:01,710 --> 00:44:06,230
continue so the conversation ending was

00:44:03,660 --> 00:44:08,579
not great and then privacy to apply

00:44:06,230 --> 00:44:10,230
meaning that it got a little bit of

00:44:08,579 --> 00:44:11,430
positive feedback not as much as the

00:44:10,230 --> 00:44:13,529
best-performing one but this

00:44:11,430 --> 00:44:15,029
conversation is very irritating you guys

00:44:13,529 --> 00:44:17,700
really need to tone it down and stick to

00:44:15,029 --> 00:44:19,619
civil discussion so here we're

00:44:17,700 --> 00:44:21,720
explicitly referencing tone and civility

00:44:19,619 --> 00:44:23,789
which I try to avoid but it was

00:44:21,720 --> 00:44:25,740
effective here and so I note that I'm

00:44:23,789 --> 00:44:28,650
not always right here's the thing that

00:44:25,740 --> 00:44:29,819
seems like it might work and to your

00:44:28,650 --> 00:44:32,099
question Joe Fisher about where does

00:44:29,819 --> 00:44:33,480
this work and where does it not or where

00:44:32,099 --> 00:44:35,730
should we be and where should we not so

00:44:33,480 --> 00:44:41,369
the bot did better in general interest

00:44:35,730 --> 00:44:42,569
and Q&A sites ask women news and then

00:44:41,369 --> 00:44:46,619
conversation where we screwed up

00:44:42,569 --> 00:44:48,140
overwatch for sure and any of the like a

00:44:46,619 --> 00:44:50,250
gross so it might just be that we don't

00:44:48,140 --> 00:44:53,369
interrupt in gaming

00:44:50,250 --> 00:44:56,039
we had mixed results in sports so

00:44:53,369 --> 00:44:57,569
there's maybe some there might be some

00:44:56,039 --> 00:45:02,700
way for the bot to be useful in sports

00:44:57,569 --> 00:45:06,500
but out of the box not as much and then

00:45:02,700 --> 00:45:08,759
here's our on reddit you get karma we

00:45:06,500 --> 00:45:11,789
increased our overall karma so the bot

00:45:08,759 --> 00:45:13,740
was a net benefit but people when they

00:45:11,789 --> 00:45:17,430
got mad they got really mad so we'd like

00:45:13,740 --> 00:45:20,549
to and then I have a please stand by

00:45:17,430 --> 00:45:24,420
because we this experiment may or may

00:45:20,549 --> 00:45:26,549
not be running now and in the future so

00:45:24,420 --> 00:45:29,609
I'll update you and heard some results

00:45:26,549 --> 00:45:32,160
so spring 29th where did where are we

00:45:29,609 --> 00:45:34,799
headed next so 2019 this anti insult

00:45:32,160 --> 00:45:36,420
bought experiment and then

00:45:34,799 --> 00:45:38,459
summer we're gonna focus on

00:45:36,420 --> 00:45:41,039
conversational resilience so this is

00:45:38,459 --> 00:45:41,999
when conversations recover how do they

00:45:41,039 --> 00:45:44,249
recover

00:45:41,999 --> 00:45:46,769
so these graphs are from the Instagram

00:45:44,249 --> 00:45:48,719
study so on the middle one this is the

00:45:46,769 --> 00:45:50,609
position of the first hostile comment so

00:45:48,719 --> 00:45:53,009
one of the reasons that we do this work

00:45:50,609 --> 00:45:55,829
is to figure out how do we build a

00:45:53,009 --> 00:45:57,179
priority stack for intervention even if

00:45:55,829 --> 00:45:59,609
the intervention is gonna be automated

00:45:57,179 --> 00:46:00,269
because we can't actually intervene all

00:45:59,609 --> 00:46:03,599
the time

00:46:00,269 --> 00:46:06,689
so here this shows us that there are

00:46:03,599 --> 00:46:09,569
conversations where you have time to

00:46:06,689 --> 00:46:11,279
intervene before things get really nasty

00:46:09,569 --> 00:46:13,559
but there are some conversations that

00:46:11,279 --> 00:46:16,319
they will flame so fast that our

00:46:13,559 --> 00:46:17,699
intervention may not be effective if we

00:46:16,319 --> 00:46:19,679
rely on humans and so they're good

00:46:17,699 --> 00:46:21,420
candidates for automated intervention if

00:46:19,679 --> 00:46:25,079
it's gonna happen really fast and then

00:46:21,420 --> 00:46:28,019
on the right here each dot is a comment

00:46:25,079 --> 00:46:31,319
each row or each position on the y-axis

00:46:28,019 --> 00:46:34,679
is a post and contact right on Instagram

00:46:31,319 --> 00:46:37,529
and the color of the comment is great

00:46:34,679 --> 00:46:39,709
for innocuous blue for hostile and red

00:46:37,529 --> 00:46:44,239
for a threat of physical violence and

00:46:39,709 --> 00:46:48,299
then the space is a measure of time

00:46:44,239 --> 00:46:52,160
where the x-axis is like seconds but

00:46:48,299 --> 00:46:54,420
it's in it's on a log scale so time ish

00:46:52,160 --> 00:46:56,249
and you can see that there are many

00:46:54,420 --> 00:46:58,469
conversations that do recover so they

00:46:56,249 --> 00:47:00,420
have a lot of blue or red on the left

00:46:58,469 --> 00:47:02,039
and then there's grey on the right and

00:47:00,420 --> 00:47:05,699
then the conversation continues and so

00:47:02,039 --> 00:47:07,469
we're trying to build a big data set of

00:47:05,699 --> 00:47:08,999
those conversations that have blue or

00:47:07,469 --> 00:47:11,130
red at the beginning and gray towards

00:47:08,999 --> 00:47:12,869
the end so that we can figure out what

00:47:11,130 --> 00:47:15,059
do they have in common

00:47:12,869 --> 00:47:16,709
which will involve reading a lot of them

00:47:15,059 --> 00:47:20,489
by hand we're also probably gonna do

00:47:16,709 --> 00:47:21,900
topic and syntactical models there to

00:47:20,489 --> 00:47:27,509
see if there are particular ways that

00:47:21,900 --> 00:47:31,859
they recover sadly one of the four the

00:47:27,509 --> 00:47:33,390
ones on Instagram they go blue all the

00:47:31,859 --> 00:47:38,160
way across or red all the way across

00:47:33,390 --> 00:47:41,249
they're usually talking about women so

00:47:38,160 --> 00:47:44,770
when we look at our feature space for

00:47:41,249 --> 00:47:47,650
which features are most related tasks

00:47:44,770 --> 00:47:49,330
she and her and other feminine pronoun

00:47:47,650 --> 00:47:51,340
or references are the highest-performing

00:47:49,330 --> 00:47:54,580
features which tells us that it's do

00:47:51,340 --> 00:47:58,390
something about the poster themselves

00:47:54,580 --> 00:48:01,690
and not the content which absolutely

00:47:58,390 --> 00:48:03,460
yeah that's a social issue that we're

00:48:01,690 --> 00:48:12,790
gonna have to address but a bot is not

00:48:03,460 --> 00:48:14,260
gonna fix misogyny and then in the fall

00:48:12,790 --> 00:48:15,960
we're gonna see it now we'll have these

00:48:14,260 --> 00:48:18,670
two different battles one on Instagram

00:48:15,960 --> 00:48:21,160
and one on reddit and potentially one on

00:48:18,670 --> 00:48:24,280
Twitter that how can we build a cross

00:48:21,160 --> 00:48:28,690
domain adaptation so how can we sort of

00:48:24,280 --> 00:48:30,250
add training from the new platform to

00:48:28,690 --> 00:48:31,720
automatically retrain the models that

00:48:30,250 --> 00:48:34,360
platform so you don't have to do this

00:48:31,720 --> 00:48:39,340
labor for each of them so domain

00:48:34,360 --> 00:48:42,010
adaptation is a small project there we

00:48:39,340 --> 00:48:43,810
go thank you so much so yeah my approach

00:48:42,010 --> 00:48:45,070
let's be really specific to try to solve

00:48:43,810 --> 00:48:47,410
one thing at a time we can build

00:48:45,070 --> 00:48:49,900
combinatorial models but only when we

00:48:47,410 --> 00:48:51,550
have the components right we need to

00:48:49,900 --> 00:48:54,070
think about the social context in which

00:48:51,550 --> 00:48:56,200
these things occur and then we can learn

00:48:54,070 --> 00:48:58,240
from servos for justice especially to

00:48:56,200 --> 00:49:00,760
stop prioritizing order and instead to

00:48:58,240 --> 00:49:02,560
prioritize some higher value and my

00:49:00,760 --> 00:49:04,090
current project plan is to focus really

00:49:02,560 --> 00:49:07,840
specifically on these personal insults

00:49:04,090 --> 00:49:09,670
that are markers of contempt that we

00:49:07,840 --> 00:49:11,260
recognize as problematic and then we try

00:49:09,670 --> 00:49:14,770
to be smart about replying and measure

00:49:11,260 --> 00:49:17,290
their impact I work with a lot of great

00:49:14,770 --> 00:49:18,730
people like some at Michigan and summit

00:49:17,290 --> 00:49:24,460
annoy tech and you can find their names

00:49:18,730 --> 00:49:26,050
their soup thank you thank you very much

00:49:24,460 --> 00:49:30,370
yeah thank you for making this possible

00:49:26,050 --> 00:49:31,630
you pay for the students and the data we

00:49:30,370 --> 00:49:34,230
are happy to make this happen and hope

00:49:31,630 --> 00:49:36,970
this is the first of many collaborations

00:49:34,230 --> 00:49:41,940
are there questions that people want

00:49:36,970 --> 00:49:44,230
other questions in the room became yeah

00:49:41,940 --> 00:49:47,140
are there questions on put them on the

00:49:44,230 --> 00:49:50,170
ETA channel or tell me and then start

00:49:47,140 --> 00:49:52,390
talking or something like that actually

00:49:50,170 --> 00:49:54,820
because this is all like official and

00:49:52,390 --> 00:49:55,550
Moscati like like there may even be one

00:49:54,820 --> 00:50:00,740
of those box

00:49:55,550 --> 00:50:03,730
you can talk into mr. Hoyer you are you

00:50:00,740 --> 00:50:03,730
enthusiastically

00:50:04,000 --> 00:50:16,460
but for something yeah why not um what

00:50:12,290 --> 00:50:18,590
does a like what does an ideal API for

00:50:16,460 --> 00:50:22,810
this look like like if we had to start

00:50:18,590 --> 00:50:24,950
with if we have to start instead with

00:50:22,810 --> 00:50:26,240
knowing the outcomes we wanted and the

00:50:24,950 --> 00:50:29,990
tools that we wanted to build them

00:50:26,240 --> 00:50:31,340
knowing what was effective and I

00:50:29,990 --> 00:50:32,720
apologize that I was only sorta I was

00:50:31,340 --> 00:50:33,950
only here for part of the meeting so you

00:50:32,720 --> 00:50:36,590
may have covered this already but what

00:50:33,950 --> 00:50:39,410
does an ideal API for this look like if

00:50:36,590 --> 00:50:40,940
we started if we started instead from if

00:50:39,410 --> 00:50:42,890
we if we didn't start from we have a

00:50:40,940 --> 00:50:44,510
social network we want to do a thing if

00:50:42,890 --> 00:50:45,650
we started instead with we want to do a

00:50:44,510 --> 00:50:47,210
thing and we're gonna build the social

00:50:45,650 --> 00:50:52,850
network around it what does that look

00:50:47,210 --> 00:50:54,560
like okay I guess I can I can try to

00:50:52,850 --> 00:50:55,850
enter the API question a little bit but

00:50:54,560 --> 00:51:01,610
if we're trying to build a social

00:50:55,850 --> 00:51:04,850
network around it so I think that there

00:51:01,610 --> 00:51:09,050
is if yeah in order to be effective the

00:51:04,850 --> 00:51:10,850
body requires that we're trying to build

00:51:09,050 --> 00:51:13,250
it on just the comment itself in part

00:51:10,850 --> 00:51:15,170
because that's an easy API to build so

00:51:13,250 --> 00:51:17,060
that you can pass a comment instead of

00:51:15,170 --> 00:51:20,030
having to pass a comment and all the

00:51:17,060 --> 00:51:21,980
context but if you were gonna build a

00:51:20,030 --> 00:51:25,430
social network you it would be more

00:51:21,980 --> 00:51:28,640
effective if you were able to indicate

00:51:25,430 --> 00:51:31,540
to the model both the context and the

00:51:28,640 --> 00:51:34,490
content so like I I gave an example

00:51:31,540 --> 00:51:36,500
earlier of some profanity and physical

00:51:34,490 --> 00:51:40,940
threat in a baseball discussion where it

00:51:36,500 --> 00:51:46,730
was reasonable but we wouldn't want that

00:51:40,940 --> 00:51:49,460
in our news or instance but we can

00:51:46,730 --> 00:51:51,890
capture most of the problematic stuff

00:51:49,460 --> 00:51:54,050
with just the content and so it could

00:51:51,890 --> 00:51:56,990
work something like a less racist API

00:51:54,050 --> 00:51:58,790
than Google's perspective where you pass

00:51:56,990 --> 00:52:00,650
it a comment and it gives you a

00:51:58,790 --> 00:52:03,860
probability score that that's

00:52:00,650 --> 00:52:06,040
problematic and then depending on your

00:52:03,860 --> 00:52:08,200
tool you can pass it to a second stage

00:52:06,040 --> 00:52:10,599
but the

00:52:08,200 --> 00:52:12,670
I think an API is submitted comments get

00:52:10,599 --> 00:52:14,369
probability score back in part because

00:52:12,670 --> 00:52:16,240
then you can tune your thresholds for

00:52:14,369 --> 00:52:24,940
different places that have different

00:52:16,240 --> 00:52:30,310
thresholds but I'm generally bad by

00:52:24,940 --> 00:52:34,930
thinking on scale so the question here

00:52:30,310 --> 00:52:36,970
from Tamra who's up in Seattle which is

00:52:34,930 --> 00:52:40,570
whether there's what there's a need but

00:52:36,970 --> 00:52:43,690
what the correlation is between form

00:52:40,570 --> 00:52:46,240
factors and the effectiveness right like

00:52:43,690 --> 00:52:48,550
are there is the medium the message

00:52:46,240 --> 00:52:50,950
right so you're doing text interaction

00:52:48,550 --> 00:52:53,020
in a text format do you think that voice

00:52:50,950 --> 00:52:55,480
stuff would be more impactful than

00:52:53,020 --> 00:52:57,910
anything around this I don't have

00:52:55,480 --> 00:53:00,010
anything on voice that's a good question

00:52:57,910 --> 00:53:01,960
we have talked about ways that we can

00:53:00,010 --> 00:53:03,670
look at images especially because we

00:53:01,960 --> 00:53:05,910
started looking on Instagram like are

00:53:03,670 --> 00:53:09,040
there particular ways that people are

00:53:05,910 --> 00:53:13,750
aggressive or inappropriate in their

00:53:09,040 --> 00:53:16,000
image response so if you think about I'm

00:53:13,750 --> 00:53:17,650
Jezebel you're familiar with Jezebel one

00:53:16,000 --> 00:53:19,119
way that they quickly derail

00:53:17,650 --> 00:53:22,599
conversation is they put up really

00:53:19,119 --> 00:53:24,609
violent porn gifts as a way to derail

00:53:22,599 --> 00:53:27,960
conversation so are there ways that we

00:53:24,609 --> 00:53:30,910
can detect sort of aggression through

00:53:27,960 --> 00:53:32,650
imagery but voice I haven't really

00:53:30,910 --> 00:53:34,300
thought about I do think that it would

00:53:32,650 --> 00:53:39,520
be useful though because we can do some

00:53:34,300 --> 00:53:43,260
things with pitch and speech rate that

00:53:39,520 --> 00:53:45,220
would be really helpful for the

00:53:43,260 --> 00:53:47,890
emotionality that's useful and

00:53:45,220 --> 00:53:49,180
emotionality that's harmful hmm I saw a

00:53:47,890 --> 00:53:50,680
really good talk at the Midwest

00:53:49,180 --> 00:53:53,560
Political Science Association meeting

00:53:50,680 --> 00:53:56,530
last year about using pitch to study

00:53:53,560 --> 00:53:58,270
political speech that I think there's

00:53:56,530 --> 00:54:00,640
some stuff there to learn about pitch

00:53:58,270 --> 00:54:02,290
and speech rate in voice no but that's

00:54:00,640 --> 00:54:04,119
beyond my expertise but I think is a

00:54:02,290 --> 00:54:07,359
really good question well this is really

00:54:04,119 --> 00:54:10,630
weird we've actually been doing Genesis

00:54:07,359 --> 00:54:14,710
I up in Seattle and just man :

00:54:10,630 --> 00:54:16,690
this is a student at CMU was an intern

00:54:14,710 --> 00:54:19,450
last summer and they started looking at

00:54:16,690 --> 00:54:20,260
how you evaluate the quality of voices

00:54:19,450 --> 00:54:22,930
and one of the things we

00:54:20,260 --> 00:54:25,210
so different so we have our own voices

00:54:22,930 --> 00:54:27,190
that our team in Berlin have made deep

00:54:25,210 --> 00:54:28,660
speech and do speech Nancy remember

00:54:27,190 --> 00:54:32,500
comparing those to the Amazon poly

00:54:28,660 --> 00:54:34,330
voices - yeah the Google Wave net voices

00:54:32,500 --> 00:54:37,390
and the differences between men and

00:54:34,330 --> 00:54:40,000
women are really significant right

00:54:37,390 --> 00:54:41,109
people really have very different gender

00:54:40,000 --> 00:54:43,180
turns out to be very good predictor of

00:54:41,109 --> 00:54:44,470
which voices you will like a particular

00:54:43,180 --> 00:54:47,859
women don't like listening to women

00:54:44,470 --> 00:54:50,650
voices right - sort of summarized yeah

00:54:47,859 --> 00:54:54,310
to summarize quite a lot of data a lot

00:54:50,650 --> 00:54:55,390
of research right which ties in I mean

00:54:54,310 --> 00:54:57,310
we know that there are gender affects

00:54:55,390 --> 00:55:00,609
reading there some anecdotal stuff about

00:54:57,310 --> 00:55:02,710
Germans when like the first automated

00:55:00,609 --> 00:55:04,960
voice has happened in cars Germans

00:55:02,710 --> 00:55:06,490
didn't want women German men didn't want

00:55:04,960 --> 00:55:08,580
women telling them what to do right with

00:55:06,490 --> 00:55:10,540
some voice right

00:55:08,580 --> 00:55:11,740
but it's wrecked me yet there's some

00:55:10,540 --> 00:55:13,960
interesting stuff there around

00:55:11,740 --> 00:55:18,820
interventions that could be that could

00:55:13,960 --> 00:55:23,250
leverage those those assumptions yeah I

00:55:18,820 --> 00:55:23,250
like wood Bob Ross's voice coming people

00:55:23,730 --> 00:55:28,450
I think that if I worked on a platform

00:55:26,859 --> 00:55:31,540
side

00:55:28,450 --> 00:55:32,890
I think the quickest way that you can

00:55:31,540 --> 00:55:37,630
address this is some kind of

00:55:32,890 --> 00:55:39,990
interruption that there we just know so

00:55:37,630 --> 00:55:42,910
much about the psychology of aggression

00:55:39,990 --> 00:55:44,859
that if you can just slow down or if

00:55:42,910 --> 00:55:46,690
you've ever parented a toddler if you

00:55:44,859 --> 00:55:49,240
just count it for like we put in a

00:55:46,690 --> 00:55:52,090
little Daniel Tiger and make people do

00:55:49,240 --> 00:55:54,460
math block them or make them click

00:55:52,090 --> 00:55:56,440
refresh or so that we can that is a

00:55:54,460 --> 00:55:59,710
really effective way to deal with this

00:55:56,440 --> 00:56:01,750
stuff and it's preventative but it does

00:55:59,710 --> 00:56:03,190
mean that people get frustrated with the

00:56:01,750 --> 00:56:06,010
platform itself for a moment to

00:56:03,190 --> 00:56:08,350
understand the incentive is off a little

00:56:06,010 --> 00:56:12,119
bit but it would make a lot of sense to

00:56:08,350 --> 00:56:12,119
rate limit aggro

00:56:13,470 --> 00:56:17,800
in part because we all do it I think one

00:56:16,600 --> 00:56:19,810
of the things I didn't mention up front

00:56:17,800 --> 00:56:21,730
is that often what it means is they're

00:56:19,810 --> 00:56:23,470
the it would pay attention to social

00:56:21,730 --> 00:56:25,320
context is that there's a context in

00:56:23,470 --> 00:56:28,240
which we will all behave inappropriately

00:56:25,320 --> 00:56:30,790
so this is not that there are bad users

00:56:28,240 --> 00:56:33,910
and good users but there are moments

00:56:30,790 --> 00:56:35,789
where people make mistakes

00:56:33,910 --> 00:56:42,849
that we ought to be able to address them

00:56:35,789 --> 00:56:44,710
justly I was trying to figure out how to

00:56:42,849 --> 00:56:49,990
frame this question but I was wondering

00:56:44,710 --> 00:56:51,760
like if the norms of the reddit faith or

00:56:49,990 --> 00:56:55,030
the Instagram places that you were

00:56:51,760 --> 00:56:57,309
testing on like if you were able to feel

00:56:55,030 --> 00:57:00,910
like okay well this particular community

00:56:57,309 --> 00:57:04,000
has like kind of a low tolerance for

00:57:00,910 --> 00:57:06,970
like personal insult behavior and how

00:57:04,000 --> 00:57:09,160
that impacted the bots effectiveness

00:57:06,970 --> 00:57:10,530
like I'm just curious cause like we know

00:57:09,160 --> 00:57:14,049
that there's like the really toxic

00:57:10,530 --> 00:57:16,270
subreddits where we should just put up a

00:57:14,049 --> 00:57:18,579
sign that says no one should enter this

00:57:16,270 --> 00:57:20,049
place dragons yeah exactly like here be

00:57:18,579 --> 00:57:23,079
dragons like don't even go there

00:57:20,049 --> 00:57:24,940
like did you I mean was that part of the

00:57:23,079 --> 00:57:26,589
study at all yeah so the instrumental

00:57:24,940 --> 00:57:29,619
one isn't actually a good case for this

00:57:26,589 --> 00:57:31,450
because we the network that we were

00:57:29,619 --> 00:57:33,099
studying on Instagram was around a high

00:57:31,450 --> 00:57:35,289
school on the south side of Chicago and

00:57:33,099 --> 00:57:37,390
the demographics of this neighborhood

00:57:35,289 --> 00:57:42,160
are majority minority and actually

00:57:37,390 --> 00:57:43,720
majority black so there were somewhere

00:57:42,160 --> 00:57:46,900
between 8 and 12 percent of the

00:57:43,720 --> 00:57:50,470
population of that school is white and

00:57:46,900 --> 00:57:52,680
then about 60 or 70 percent is after

00:57:50,470 --> 00:57:55,630
American and then there's some

00:57:52,680 --> 00:57:59,650
multiracial or some other non-white race

00:57:55,630 --> 00:58:02,650
in the middle and what that meant for

00:57:59,650 --> 00:58:06,460
our detection and why I bring up

00:58:02,650 --> 00:58:08,440
perspective as a racist API that a lot

00:58:06,460 --> 00:58:10,569
of the conversation that those teenagers

00:58:08,440 --> 00:58:12,819
had with each other would be completely

00:58:10,569 --> 00:58:15,400
inappropriate for us in this room to

00:58:12,819 --> 00:58:17,380
have but it is completely reasonable and

00:58:15,400 --> 00:58:21,369
appropriate for them to have and so we

00:58:17,380 --> 00:58:22,900
had to remove roughly a third of the

00:58:21,369 --> 00:58:26,109
things that constitute racial slur

00:58:22,900 --> 00:58:29,650
because it's in a group conversation and

00:58:26,109 --> 00:58:32,710
so if we have these really aggressive

00:58:29,650 --> 00:58:34,990
like you may not use slur in any way

00:58:32,710 --> 00:58:38,589
rules than me actually silence really

00:58:34,990 --> 00:58:40,690
regular socially beneficial speech and

00:58:38,589 --> 00:58:42,640
then it's not about what is the thing

00:58:40,690 --> 00:58:44,559
being said but who is saying the thing

00:58:42,640 --> 00:58:46,450
and to cool and that's why we need

00:58:44,559 --> 00:58:47,270
something about the structure as well in

00:58:46,450 --> 00:58:50,670
that

00:58:47,270 --> 00:58:52,290
moment but the Instagram model was also

00:58:50,670 --> 00:58:54,059
different because we weren't even about

00:58:52,290 --> 00:58:56,460
to respond we were just flagging stuff

00:58:54,059 --> 00:58:58,920
for parents who said they wanted to have

00:58:56,460 --> 00:59:00,780
conversations with their kids in part

00:58:58,920 --> 00:59:03,630
cuz that project started with us talking

00:59:00,780 --> 00:59:05,220
to high schools about what their biggest

00:59:03,630 --> 00:59:06,690
problems were because we think oh cyber

00:59:05,220 --> 00:59:08,640
bullying is in the news let's add our

00:59:06,690 --> 00:59:10,079
cyber bullying and they said I don't

00:59:08,640 --> 00:59:12,599
care about that what I care about is

00:59:10,079 --> 00:59:14,549
these two girls get into it offline they

00:59:12,599 --> 00:59:15,690
escalate at school during the day online

00:59:14,549 --> 00:59:17,760
and then they have a physical

00:59:15,690 --> 00:59:19,799
altercation in the hallway as they're

00:59:17,760 --> 00:59:22,619
leaving and that happens every single

00:59:19,799 --> 00:59:25,260
day cyber bullying happens and we deal

00:59:22,619 --> 00:59:28,500
with it when it does but I really I want

00:59:25,260 --> 00:59:30,359
to keep these girls out of juvie so then

00:59:28,500 --> 00:59:31,500
like okay we had the problem wrong

00:59:30,359 --> 00:59:34,190
because we need to go back and think

00:59:31,500 --> 00:59:37,190
about what is the problem and they have

00:59:34,190 --> 00:59:39,329
they also had the school day to respond

00:59:37,190 --> 00:59:40,680
so we didn't need to be able to respond

00:59:39,329 --> 00:59:42,890
in a minute we needed to be able to

00:59:40,680 --> 00:59:45,359
respond in the four-hour block between

00:59:42,890 --> 00:59:48,329
when they enter the building and when

00:59:45,359 --> 00:59:50,309
their first outdoor break happens and so

00:59:48,329 --> 00:59:53,369
we were able to do some tuning in there

00:59:50,309 --> 00:59:54,960
as well yeah and parents were really in

00:59:53,369 --> 00:59:56,660
fact that they didn't want the system to

00:59:54,960 --> 00:59:59,609
do anything they thought that was creepy

00:59:56,660 --> 01:00:01,349
they just wanted some any kind of

00:59:59,609 --> 01:00:03,990
passive alert that they could check on

01:00:01,349 --> 01:00:05,760
and then if something was there just

01:00:03,990 --> 01:00:07,920
show them what happened so that's why we

01:00:05,760 --> 01:00:11,190
link back to the Szwed and then they

01:00:07,920 --> 01:00:13,829
will talk to their kids about it yes

01:00:11,190 --> 01:00:15,390
cool yeah and it was really great to

01:00:13,829 --> 01:00:19,170
send students out on those interviews

01:00:15,390 --> 01:00:20,369
too because we assumed like we know what

01:00:19,170 --> 01:00:21,780
the problem is and we know what people

01:00:20,369 --> 01:00:24,420
might want and we just have to build the

01:00:21,780 --> 01:00:26,309
model and at every moment that the

01:00:24,420 --> 01:00:29,160
population we were working with was like

01:00:26,309 --> 01:00:31,740
no that's not my issue or no that's not

01:00:29,160 --> 01:00:34,290
what I need you to do so that was a good

01:00:31,740 --> 01:00:34,500
learning experience for us cool all

01:00:34,290 --> 01:00:35,970
right

01:00:34,500 --> 01:00:37,950
that's what you've got time for so I

01:00:35,970 --> 01:00:40,049
ain't going to say thank you very much

01:00:37,950 --> 01:00:41,760
to Libby I'm gonna remind everybody that

01:00:40,049 --> 01:00:44,569
we've got another talk in this speaker

01:00:41,760 --> 01:00:48,059
series next Thursday at the same time

01:00:44,569 --> 01:00:50,520
and that's Jane McGonigal we'll be

01:00:48,059 --> 01:00:52,830
talking about the ethical os toolkit so

01:00:50,520 --> 01:00:55,200
I'm pretty excited too

01:00:52,830 --> 01:00:57,540
to hear that and very morally hosting

01:00:55,200 --> 01:00:59,520
that so look forward to seeing you there

01:00:57,540 --> 01:01:02,870
thank you all for coming and have a very

01:00:59,520 --> 01:01:02,870

YouTube URL: https://www.youtube.com/watch?v=U4vfFyQuT9I


