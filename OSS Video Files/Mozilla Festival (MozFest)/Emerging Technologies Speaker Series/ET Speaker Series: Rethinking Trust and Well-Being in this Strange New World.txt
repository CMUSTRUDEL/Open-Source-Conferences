Title: ET Speaker Series: Rethinking Trust and Well-Being in this Strange New World
Publication date: 2020-05-12
Playlist: Emerging Technologies Speaker Series
Description: 
	ET Speaker Series: Jeff Hancock, Stanford University
Rethinking Trust and Well-Being in this Strange New World

A new trust framework is emerging – fueled by social, economic and technological forces that will profoundly alter how we trust, not only what we see and read online, but also one another. At the same time, technology is now crucial for staying distant socializing while we must keep our social distance. These changes have profound implications for our psychological well-being. In this talk we will discuss how principles from psychology and communication intersect deception, trust and well-being with technology. We will discuss several  studies that reveal key principles to guide how we think about truth and trust on the internet, and I will report on a new meta-analysis examining every study examining social media and well-being. 

Bio
Jeff Hancock is the Harry and Norman Chandler Professor of Communication at Stanford University and the Founding Director of the Stanford Social Media Lab. A leading expert in social media behavior and the psychology of online interaction, Professor Hancock studies the impact of social media and technology on well-being, relationships, deception and trust, identity, and more.

His research has been published in over 100 journal articles and conference proceedings and has been supported by funding from the U.S. National Science Foundation and the U.S. Department of Defense. Professor Hancock’s TED Talk on deception has been seen over 1 million times and his research has been frequently featured in the popular press, including the New York Times, CNN, NPR, CBS and the BBC.

Professor Hancock worked for Canada Customs before earning his PhD in Psychology at Dalhousie University, Canada. He was a Professor of Information Science (and co-Chair) and Communication at Cornell University prior to joining Stanford in 2015. He currently lives in Palo Alto with his wife and daughter, and he regularly gets shot at on the ice as a hockey goalie.
Captions: 
	00:00:00,880 --> 00:00:05,379
good morning my name is joe fish kay i'm

00:00:03,639 --> 00:00:07,689
principal research scientist at mozilla

00:00:05,379 --> 00:00:12,310
and it is my pleasure today to introduce

00:00:07,689 --> 00:00:15,209
you to Jeff Hancock I just checked this

00:00:12,310 --> 00:00:18,550
morning I have known Jeff for 17 years I

00:00:15,209 --> 00:00:23,080
first met Jeff in 2003 at the KY

00:00:18,550 --> 00:00:25,180
conference in Fort Lauderdale and at the

00:00:23,080 --> 00:00:26,320
time I was a master's I had finished a

00:00:25,180 --> 00:00:27,460
master's degree and I was looking

00:00:26,320 --> 00:00:30,880
forward to becoming a PhD student

00:00:27,460 --> 00:00:33,609
somewhere and hello

00:00:30,880 --> 00:00:35,739
and Jeff was a huge part of that

00:00:33,609 --> 00:00:37,300
decision for me to go to Cornell and I

00:00:35,739 --> 00:00:40,479
was actually lucky enough to have Jeff

00:00:37,300 --> 00:00:42,370
as a member of my committee today Jeff

00:00:40,479 --> 00:00:45,249
is gonna be talking about trust and

00:00:42,370 --> 00:00:46,300
well-being in this world of covert 19

00:00:45,249 --> 00:00:49,839
and I think that's particularly relevant

00:00:46,300 --> 00:00:52,479
and important to Mozilla it's relevant

00:00:49,839 --> 00:00:54,129
to the whole world because of how our

00:00:52,479 --> 00:00:56,319
practices and how our every day has

00:00:54,129 --> 00:00:58,989
changed like having children walking

00:00:56,319 --> 00:01:02,319
through when you're giving introductions

00:00:58,989 --> 00:01:04,780
to people's talks but also because Trust

00:01:02,319 --> 00:01:07,360
is so important to Mozilla Mozilla

00:01:04,780 --> 00:01:09,789
relies on trust our brand is about trust

00:01:07,360 --> 00:01:11,500
and our identity is about trust and we

00:01:09,789 --> 00:01:13,299
run into the problems that we think it

00:01:11,500 --> 00:01:15,729
is very important that people trust us

00:01:13,299 --> 00:01:20,109
and yet we don't always have a strong

00:01:15,729 --> 00:01:23,140
sense of what the competition looks like

00:01:20,109 --> 00:01:25,840
for there right I saw a poll about six

00:01:23,140 --> 00:01:27,579
months ago and it said would you trust

00:01:25,840 --> 00:01:30,990
this company with your medical data and

00:01:27,579 --> 00:01:35,170
Amazon came in at something like 71%

00:01:30,990 --> 00:01:38,109
Google came in at 68% and Mozilla came

00:01:35,170 --> 00:01:40,000
in at 65% and I thought that was

00:01:38,109 --> 00:01:42,100
fascinating because we believe that we

00:01:40,000 --> 00:01:46,509
are this this bastion of truth this

00:01:42,100 --> 00:01:48,070
bastian of trusted engagement and the

00:01:46,509 --> 00:01:49,450
more we can understand that and

00:01:48,070 --> 00:01:51,579
understand how people make these

00:01:49,450 --> 00:01:54,100
decisions about what to trust and who to

00:01:51,579 --> 00:01:56,109
trust and when to trust I think that's

00:01:54,100 --> 00:01:57,789
crucial for the company and really

00:01:56,109 --> 00:02:00,159
crucial for the survival of the internet

00:01:57,789 --> 00:02:01,719
which is a medium that is really driven

00:02:00,159 --> 00:02:03,670
by trust from the very beginning and

00:02:01,719 --> 00:02:05,920
that's one quite so excited to have Jeff

00:02:03,670 --> 00:02:08,370
here so I'm gonna stop pontificating

00:02:05,920 --> 00:02:10,900
you'll be seeing more from me later on

00:02:08,370 --> 00:02:14,319
one thing I would like to encourage

00:02:10,900 --> 00:02:18,459
Mazzilli ins please join me

00:02:14,319 --> 00:02:20,890
the speaker - series slack channel where

00:02:18,459 --> 00:02:22,329
will I'll take questions for folks

00:02:20,890 --> 00:02:25,209
outside the company I will take

00:02:22,329 --> 00:02:27,670
questions at twitter.com slash Joe fish

00:02:25,209 --> 00:02:30,159
and I look forward to hearing from you

00:02:27,670 --> 00:02:31,959
over to you Jeff thank you thanks Joe

00:02:30,159 --> 00:02:33,849
fish really appreciate it and thank you

00:02:31,959 --> 00:02:36,129
for the invitation I was telling Joe a

00:02:33,849 --> 00:02:39,970
fish earlier that this is my first talk

00:02:36,129 --> 00:02:43,510
in the cove in nineteen ara so first

00:02:39,970 --> 00:02:46,540
time doing this virtually and I'm lucky

00:02:43,510 --> 00:02:48,310
enough not to be teaching right now most

00:02:46,540 --> 00:02:50,890
of my colleagues are doing this kind of

00:02:48,310 --> 00:02:53,440
class every day so bear with me if

00:02:50,890 --> 00:02:54,459
there's any issues as Joe fish said

00:02:53,440 --> 00:02:58,299
today we're going to talk a lot about

00:02:54,459 --> 00:03:01,180
trust I will kind of do a deep dive

00:02:58,299 --> 00:03:03,609
underneath the hood of what Trust is and

00:03:01,180 --> 00:03:06,069
we're gonna take a detour over into

00:03:03,609 --> 00:03:08,159
deception detection to try and learn a

00:03:06,069 --> 00:03:10,480
few things about trust and Trust - there

00:03:08,159 --> 00:03:12,099
and then because this is something

00:03:10,480 --> 00:03:14,079
that's been on I think everybody's mind

00:03:12,099 --> 00:03:17,250
now that we're using technology all the

00:03:14,079 --> 00:03:20,079
time to engage in distant socializing

00:03:17,250 --> 00:03:21,669
I'm gonna talk about well-being as well

00:03:20,079 --> 00:03:25,329
which actually think is an important and

00:03:21,669 --> 00:03:27,519
related construct to trust and basically

00:03:25,329 --> 00:03:29,620
we're over the largest meta-analysis

00:03:27,519 --> 00:03:32,379
that's been done to date looking at how

00:03:29,620 --> 00:03:35,889
social media is related to well-being

00:03:32,379 --> 00:03:39,190
and big questions does using social

00:03:35,889 --> 00:03:42,639
media affect our well-being and I can

00:03:39,190 --> 00:03:45,340
tell you before the pandemic almost all

00:03:42,639 --> 00:03:47,470
of the you know public narrative was

00:03:45,340 --> 00:03:48,910
that yeah it was really bad for us and I

00:03:47,470 --> 00:03:50,620
think that's dramatically changing we'll

00:03:48,910 --> 00:03:54,819
see whether that's consistent with what

00:03:50,620 --> 00:03:56,379
we're finding in our research we'll have

00:03:54,819 --> 00:03:59,169
a point maybe halfway through where

00:03:56,379 --> 00:04:02,799
we'll do some polls and and and Joe fish

00:03:59,169 --> 00:04:05,139
will organize that and also after we

00:04:02,799 --> 00:04:06,370
about halfway through we can talk about

00:04:05,139 --> 00:04:09,069
any questions that people have about

00:04:06,370 --> 00:04:13,359
trust before we make that shift into

00:04:09,069 --> 00:04:16,180
well-being so this question of how Trust

00:04:13,359 --> 00:04:19,449
works and the way in which maybe trust

00:04:16,180 --> 00:04:23,080
is declining is feels like an important

00:04:19,449 --> 00:04:26,080
question right now it feels like trust

00:04:23,080 --> 00:04:27,820
is on the decline it feels like there's

00:04:26,080 --> 00:04:29,620
less and less of it the

00:04:27,820 --> 00:04:32,440
maybe prior generations were more

00:04:29,620 --> 00:04:34,270
trustworthy than ours but this is not a

00:04:32,440 --> 00:04:37,920
new phenomenon actually this is really

00:04:34,270 --> 00:04:40,600
old it looks like every generation

00:04:37,920 --> 00:04:42,340
thinks that the previous generations

00:04:40,600 --> 00:04:45,070
were were more honourable and more

00:04:42,340 --> 00:04:47,920
trustworthy and live less it goes back

00:04:45,070 --> 00:04:50,620
to our very beginnings really some of

00:04:47,920 --> 00:04:53,950
you out there that had some classical

00:04:50,620 --> 00:04:56,350
training may recognize this fellow this

00:04:53,950 --> 00:04:58,720
is Diogenes and he's holding up a

00:04:56,350 --> 00:05:01,150
lantern in which he was searching for a

00:04:58,720 --> 00:05:03,070
single honest man and of course the

00:05:01,150 --> 00:05:04,870
story goes he he never found one and

00:05:03,070 --> 00:05:06,430
died without finding one a really

00:05:04,870 --> 00:05:09,070
uplifting story for those of you looking

00:05:06,430 --> 00:05:12,550
for some light reading so the Greeks

00:05:09,070 --> 00:05:14,530
were were deeply concerned that people

00:05:12,550 --> 00:05:16,900
were becoming less honest less

00:05:14,530 --> 00:05:19,780
trustworthy and this wasn't just a

00:05:16,900 --> 00:05:20,680
Western society phenomena also in the

00:05:19,780 --> 00:05:22,560
East

00:05:20,680 --> 00:05:26,110
Confucius around the same time

00:05:22,560 --> 00:05:29,020
developing his sort of philosophy and a

00:05:26,110 --> 00:05:31,870
core part of it was authenticity being

00:05:29,020 --> 00:05:34,500
real not only talking to talk but

00:05:31,870 --> 00:05:37,240
walking the walk which is maybe the

00:05:34,500 --> 00:05:40,270
worst summary of what Confucianism is

00:05:37,240 --> 00:05:42,750
but that's roughly the idea here so

00:05:40,270 --> 00:05:46,360
we've had a throughout human history

00:05:42,750 --> 00:05:49,060
deep concerns that trusts on the decline

00:05:46,360 --> 00:05:51,490
that people are less good I think that

00:05:49,060 --> 00:05:56,860
we we are feeling that now and we look

00:05:51,490 --> 00:05:58,810
about to to cast blame as is we want to

00:05:56,860 --> 00:06:01,030
do typically and I think technology and

00:05:58,810 --> 00:06:03,610
the Internet is attracting much of that

00:06:01,030 --> 00:06:05,380
and as I'll argue throughout this I

00:06:03,610 --> 00:06:07,840
think technology is is definitely

00:06:05,380 --> 00:06:09,070
changing the way we trust but I don't

00:06:07,840 --> 00:06:11,710
believe it's leading to necessarily

00:06:09,070 --> 00:06:13,810
decline although whether there is a real

00:06:11,710 --> 00:06:16,120
decline or not is is a real question

00:06:13,810 --> 00:06:20,440
I'll show you some evidence that perhaps

00:06:16,120 --> 00:06:23,830
there is my own personal take on trust

00:06:20,440 --> 00:06:26,290
professionally started about 25 years

00:06:23,830 --> 00:06:30,990
ago this is 1996 I was a customs officer

00:06:26,290 --> 00:06:34,240
in Canada working on the maritime border

00:06:30,990 --> 00:06:36,760
so checking lots of both some things I

00:06:34,240 --> 00:06:38,140
not you can see that I have a little

00:06:36,760 --> 00:06:40,570
bulge there you might think that's a

00:06:38,140 --> 00:06:41,320
weapon a gun but actually that's just a

00:06:40,570 --> 00:06:42,760
stamp

00:06:41,320 --> 00:06:44,470
welcoming you into Canada because that's

00:06:42,760 --> 00:06:47,950
how we can yinz were all a little bit

00:06:44,470 --> 00:06:49,660
more welcoming but it was my very first

00:06:47,950 --> 00:06:52,120
experience of thinking about okay is

00:06:49,660 --> 00:06:53,920
this person lying to me like they they

00:06:52,120 --> 00:06:56,470
might be they might have a weapon they

00:06:53,920 --> 00:06:58,810
might have more alcohol than I say they

00:06:56,470 --> 00:07:01,540
might have drugs and so it was my first

00:06:58,810 --> 00:07:04,000
time of trying to professionally think

00:07:01,540 --> 00:07:07,270
about this issue of trust and Trust is

00:07:04,000 --> 00:07:09,940
one of these strange things where it's

00:07:07,270 --> 00:07:11,590
it's it's sort of like you know freedom

00:07:09,940 --> 00:07:12,970
to move about and go to the office it's

00:07:11,590 --> 00:07:15,310
not something that you miss until it's

00:07:12,970 --> 00:07:17,290
been it still it's been taken away if

00:07:15,310 --> 00:07:20,020
it's that really well Trust is in many

00:07:17,290 --> 00:07:23,110
ways the the glue that holds the side

00:07:20,020 --> 00:07:27,250
together it's what makes living possible

00:07:23,110 --> 00:07:32,410
it allows the use of of money contracts

00:07:27,250 --> 00:07:34,720
law pretty much under grids almost all

00:07:32,410 --> 00:07:38,020
of the major sort of functioning of

00:07:34,720 --> 00:07:40,420
human activity and it's one of those

00:07:38,020 --> 00:07:42,820
things that's invisible until it's it's

00:07:40,420 --> 00:07:47,320
gone and then we realized how much and

00:07:42,820 --> 00:07:50,350
how important it is because it's so big

00:07:47,320 --> 00:07:52,480
it's also really ambiguous and and

00:07:50,350 --> 00:07:54,580
difficult to define when we ask people

00:07:52,480 --> 00:07:56,740
what they think of trust these are kind

00:07:54,580 --> 00:07:58,720
of the topics that will come up the

00:07:56,740 --> 00:08:00,790
ideas it's related to risk and

00:07:58,720 --> 00:08:03,370
uncertainty this idea that like I'm not

00:08:00,790 --> 00:08:05,260
sure that Joe fish will you know be

00:08:03,370 --> 00:08:08,070
there when I when he says he's going to

00:08:05,260 --> 00:08:11,800
be there so there's some risk where if I

00:08:08,070 --> 00:08:13,600
am buying something from him online how

00:08:11,800 --> 00:08:16,660
do I know that when I send the money

00:08:13,600 --> 00:08:19,570
that the product will arrive and when

00:08:16,660 --> 00:08:22,330
you look through the main large

00:08:19,570 --> 00:08:25,390
successful online companies that was one

00:08:22,330 --> 00:08:27,220
of the main trust points that they had

00:08:25,390 --> 00:08:31,690
to solve was how do you make it so that

00:08:27,220 --> 00:08:34,630
the money moves between between partners

00:08:31,690 --> 00:08:38,800
and so that was you know from Eric

00:08:34,630 --> 00:08:42,340
Alibaba to Airbnb to blah blah car in

00:08:38,800 --> 00:08:44,860
Europe a key advance for those companies

00:08:42,340 --> 00:08:46,210
was figuring out how payments worked and

00:08:44,860 --> 00:08:49,710
there was because of this risk and

00:08:46,210 --> 00:08:50,880
uncertainty part another

00:08:49,710 --> 00:08:53,640
really important notion and we're gonna

00:08:50,880 --> 00:08:55,880
focus on this a lot today is that trust

00:08:53,640 --> 00:08:58,440
is the keeping of promises and the

00:08:55,880 --> 00:09:00,420
understanding of one's expectations and

00:08:58,440 --> 00:09:03,110
I think that's a really core sort of

00:09:00,420 --> 00:09:05,760
idea and the last is reliability and

00:09:03,110 --> 00:09:09,830
confidence typically these are more time

00:09:05,760 --> 00:09:11,700
related so as we'll talk throughout the

00:09:09,830 --> 00:09:14,520
discussion today there's really three

00:09:11,700 --> 00:09:16,110
things that can signal trust one of them

00:09:14,520 --> 00:09:18,600
is reliability now there is the

00:09:16,110 --> 00:09:20,640
confidence that somebody or something

00:09:18,600 --> 00:09:22,890
seems confident and the others that they

00:09:20,640 --> 00:09:25,980
come across as as honest or or

00:09:22,890 --> 00:09:29,430
trustworthy and those are the three

00:09:25,980 --> 00:09:31,230
simple you know cues to trustworthiness

00:09:29,430 --> 00:09:33,270
of course it's hard to implement them

00:09:31,230 --> 00:09:36,000
but we'll see those come up again and

00:09:33,270 --> 00:09:40,160
again throughout our discussion today my

00:09:36,000 --> 00:09:42,500
favorite single definition of trust is

00:09:40,160 --> 00:09:45,570
this one from a German sociologist

00:09:42,500 --> 00:09:48,150
Niklas Luhmann Trust is the confidence

00:09:45,570 --> 00:09:49,890
in one's expectations and here it's

00:09:48,150 --> 00:09:52,950
usually about what other humans will do

00:09:49,890 --> 00:09:55,740
we'll Joe fish be there will he send me

00:09:52,950 --> 00:09:58,620
the thing that I bought from him will

00:09:55,740 --> 00:10:00,300
that girl I'm gonna go meet after

00:09:58,620 --> 00:10:03,690
checking out her online dating profile

00:10:00,300 --> 00:10:05,760
look like that person these are all

00:10:03,690 --> 00:10:08,820
about confidence in one's expectations

00:10:05,760 --> 00:10:12,930
and it has this sort of promissory peace

00:10:08,820 --> 00:10:15,450
to it where when people say that they're

00:10:12,930 --> 00:10:17,760
going to be or do or look like something

00:10:15,450 --> 00:10:19,920
that's a promise and then the question

00:10:17,760 --> 00:10:21,930
is to what degree can you trust that

00:10:19,920 --> 00:10:25,200
promise and those are what set up your

00:10:21,930 --> 00:10:28,500
expectations so let's start with a

00:10:25,200 --> 00:10:31,260
little bit of a history of trust over

00:10:28,500 --> 00:10:36,030
time trust appeared to sort of start in

00:10:31,260 --> 00:10:38,820
a dyadic way so when it was very small

00:10:36,030 --> 00:10:40,770
groups of people we would trust one

00:10:38,820 --> 00:10:42,930
another because of our relationship so I

00:10:40,770 --> 00:10:46,950
would trust Joe fish and that would be

00:10:42,930 --> 00:10:51,090
based entirely on our relationship as we

00:10:46,950 --> 00:10:53,160
got into larger groups of humans we can

00:10:51,090 --> 00:10:55,620
no longer have a one-on-one trust

00:10:53,160 --> 00:10:57,750
relationship with everybody that we had

00:10:55,620 --> 00:10:59,880
in our our social group and so then

00:10:57,750 --> 00:11:03,300
there was the rise of distributed trust

00:10:59,880 --> 00:11:06,420
so I might trust Joe fish not because

00:11:03,300 --> 00:11:09,540
I know him as perhaps I don't but my

00:11:06,420 --> 00:11:12,779
other friend Amy who knows Joe fish

00:11:09,540 --> 00:11:17,310
trust Joe fish and since I trust Amy

00:11:12,779 --> 00:11:20,220
then I can also trust Joe fish so the

00:11:17,310 --> 00:11:22,230
trust becomes embedded in in my social

00:11:20,220 --> 00:11:24,089
network and that's really powerful

00:11:22,230 --> 00:11:27,870
because now I can trust a whole lot more

00:11:24,089 --> 00:11:29,550
people and those people are incentivized

00:11:27,870 --> 00:11:32,310
to act trustworthy because if they don't

00:11:29,550 --> 00:11:34,589
if they betray me in some way or deceive

00:11:32,310 --> 00:11:36,990
me then they've also harmed their

00:11:34,589 --> 00:11:38,730
relationship with somebody else not just

00:11:36,990 --> 00:11:41,339
with me but the other people in my

00:11:38,730 --> 00:11:44,250
network there that we share so this is

00:11:41,339 --> 00:11:47,070
where a reputational type trust comes in

00:11:44,250 --> 00:11:48,980
and this was a development as we move

00:11:47,070 --> 00:11:51,570
into larger social groups

00:11:48,980 --> 00:11:54,660
then as we move into even larger so

00:11:51,570 --> 00:11:57,209
beyond villages but into cities it no

00:11:54,660 --> 00:11:59,880
longer efficient really to rely just on

00:11:57,209 --> 00:12:02,550
distributive trust here we start to rely

00:11:59,880 --> 00:12:04,529
on institutions so now instead of having

00:12:02,550 --> 00:12:08,610
to trust that Joe official will do

00:12:04,529 --> 00:12:12,000
something I can rely on the church or I

00:12:08,610 --> 00:12:15,450
can rely on government or journalism to

00:12:12,000 --> 00:12:17,250
keep our trust in check so if I were

00:12:15,450 --> 00:12:20,610
buying something from Joe fish now and

00:12:17,250 --> 00:12:24,000
and he scammed me or he he failed to pay

00:12:20,610 --> 00:12:26,850
I could actually bring law in I could

00:12:24,000 --> 00:12:29,370
bring the police in these larger

00:12:26,850 --> 00:12:32,220
institutions earlier in our history the

00:12:29,370 --> 00:12:35,100
church was perhaps the the longest and

00:12:32,220 --> 00:12:39,089
most powerful institution the last

00:12:35,100 --> 00:12:42,390
century was a very strong institutional

00:12:39,089 --> 00:12:46,320
trust century trust in government trust

00:12:42,390 --> 00:12:47,760
in media trust in church all of these

00:12:46,320 --> 00:12:50,370
truck and trusting corporations were

00:12:47,760 --> 00:12:53,010
very high and as we'll see here in a

00:12:50,370 --> 00:12:56,130
minute these are starting to decline one

00:12:53,010 --> 00:12:58,440
reason for that is it's not like there's

00:12:56,130 --> 00:12:59,970
you know the dyadic form of trust or

00:12:58,440 --> 00:13:01,649
there's only the distributed or there's

00:12:59,970 --> 00:13:04,050
only the institutional they're

00:13:01,649 --> 00:13:07,560
constantly working I have a dyadic

00:13:04,050 --> 00:13:10,470
relationship with Joe fish we also share

00:13:07,560 --> 00:13:12,660
a social network and we each have

00:13:10,470 --> 00:13:15,089
institutions that we engage with so I'm

00:13:12,660 --> 00:13:16,470
at Stanford visit Mozilla and there's

00:13:15,089 --> 00:13:20,220
all these other ones that I've mentioned

00:13:16,470 --> 00:13:22,860
like the law and police etc so they're

00:13:20,220 --> 00:13:25,350
constantly now interacting with one

00:13:22,860 --> 00:13:28,650
another and we're seeing in some ways

00:13:25,350 --> 00:13:31,320
distributed trust having an influence on

00:13:28,650 --> 00:13:33,630
institutional trust and here's a tape my

00:13:31,320 --> 00:13:35,460
tape on how that's going I think that

00:13:33,630 --> 00:13:38,990
with distributed trust we're seeing a

00:13:35,460 --> 00:13:41,550
rise in a way more and more emphasis on

00:13:38,990 --> 00:13:43,980
distributed trust partly because of

00:13:41,550 --> 00:13:46,890
digitization over the last twenty to

00:13:43,980 --> 00:13:48,450
thirty years everything moving digital

00:13:46,890 --> 00:13:52,880
it's much more easy to share information

00:13:48,450 --> 00:13:56,550
to move information around the rise of

00:13:52,880 --> 00:14:00,120
social networking sites the ability to

00:13:56,550 --> 00:14:03,660
share with large amounts of people the

00:14:00,120 --> 00:14:06,180
sort of rise of social has made social

00:14:03,660 --> 00:14:09,060
trust trust in social networks more

00:14:06,180 --> 00:14:11,580
powerful and then big data and AI are

00:14:09,060 --> 00:14:14,400
changing the ways we can know things so

00:14:11,580 --> 00:14:17,040
we have the ability to assess you know

00:14:14,400 --> 00:14:18,600
millions of documents and and summarize

00:14:17,040 --> 00:14:22,500
and analyze them and we have new

00:14:18,600 --> 00:14:25,020
techniques for doing that these

00:14:22,500 --> 00:14:28,800
phenomena are related to distributed

00:14:25,020 --> 00:14:32,480
trust and an empowering distribute trust

00:14:28,800 --> 00:14:35,880
I think I'm leading to a decline in

00:14:32,480 --> 00:14:38,940
institutional trust it's affecting the

00:14:35,880 --> 00:14:40,950
way institutions can operate and this is

00:14:38,940 --> 00:14:44,490
leading to a lot of scandals in fact if

00:14:40,950 --> 00:14:47,580
you think back to any major scandals

00:14:44,490 --> 00:14:49,880
over the last ten to fifteen years they

00:14:47,580 --> 00:14:52,880
tend to involve some form of these

00:14:49,880 --> 00:14:56,370
digitization social big data data an

00:14:52,880 --> 00:14:59,700
example are the Panama papers this was a

00:14:56,370 --> 00:15:01,680
leak of papers with millions of

00:14:59,700 --> 00:15:04,140
documents that were then analyzed by a

00:15:01,680 --> 00:15:07,970
group of journalists using you know

00:15:04,140 --> 00:15:12,090
pretty advanced NLP and AI to then

00:15:07,970 --> 00:15:17,270
unmask the financial malfeasance of a

00:15:12,090 --> 00:15:21,090
number of politicians world leaders

00:15:17,270 --> 00:15:25,320
variety of rich people and these kinds

00:15:21,090 --> 00:15:28,500
of scandals sort of show how the sausage

00:15:25,320 --> 00:15:30,089
is made in institutions and that I don't

00:15:28,500 --> 00:15:32,879
think institutions necessarily

00:15:30,089 --> 00:15:35,430
more corrupt or more deceptive than they

00:15:32,879 --> 00:15:36,990
were in the past but the rise of

00:15:35,430 --> 00:15:39,870
networked trust and all the tools

00:15:36,990 --> 00:15:42,839
associated with it is giving us some

00:15:39,870 --> 00:15:44,220
insight into how they've operated so

00:15:42,839 --> 00:15:46,050
we're seeing more and more scandals

00:15:44,220 --> 00:15:48,839
about politicians more and more scandals

00:15:46,050 --> 00:15:51,809
about banks about the media all of these

00:15:48,839 --> 00:15:55,170
large institutions we've relied on and

00:15:51,809 --> 00:15:58,319
trusted are becoming a little more

00:15:55,170 --> 00:15:59,959
transparent not of their own accord but

00:15:58,319 --> 00:16:02,459
because of all these new tools we have

00:15:59,959 --> 00:16:04,019
so one possibility is it's not that

00:16:02,459 --> 00:16:06,120
they're becoming more deceptive or

00:16:04,019 --> 00:16:07,889
corrupt but rather we're seeing more of

00:16:06,120 --> 00:16:11,100
what's going on and in part because of

00:16:07,889 --> 00:16:13,170
all these these tools so we're seeing

00:16:11,100 --> 00:16:16,110
these changes in these shifts in the way

00:16:13,170 --> 00:16:19,889
that Trust works and I think thriving

00:16:16,110 --> 00:16:22,139
companies these days are are adjusting

00:16:19,889 --> 00:16:24,149
to those so if you think of a company

00:16:22,139 --> 00:16:26,930
for example like Airbnb certainly before

00:16:24,149 --> 00:16:30,480
the pandemic it was taking all of the

00:16:26,930 --> 00:16:32,850
networked trust dynamics by allowing

00:16:30,480 --> 00:16:36,959
people to see who would be renting their

00:16:32,850 --> 00:16:38,910
place or staying in their home but they

00:16:36,959 --> 00:16:42,050
were embedding that sort of networked

00:16:38,910 --> 00:16:44,009
trust inside the institutional trust so

00:16:42,050 --> 00:16:46,199
Airbnb would say you know what don't

00:16:44,009 --> 00:16:48,240
worry you're you have insurance it's a

00:16:46,199 --> 00:16:51,209
million dollars insurance so they're

00:16:48,240 --> 00:16:53,069
using this large institutional trust

00:16:51,209 --> 00:16:55,350
approach they're also saying if you run

00:16:53,069 --> 00:16:56,970
into trouble we'll bring in the police

00:16:55,350 --> 00:17:00,809
we'll bring in the law we'll have our

00:16:56,970 --> 00:17:02,370
own safety and security people payments

00:17:00,809 --> 00:17:04,529
they'd figured out payments you'll never

00:17:02,370 --> 00:17:05,970
see money from another person we'll take

00:17:04,529 --> 00:17:07,380
care of that for you they're gonna pay

00:17:05,970 --> 00:17:10,980
us and we'll pay you and your that's

00:17:07,380 --> 00:17:13,949
guaranteed so I think a lot of companies

00:17:10,980 --> 00:17:17,250
that are thriving right now are

00:17:13,949 --> 00:17:19,140
adjusting by recognizing that Network

00:17:17,250 --> 00:17:21,299
Trust is really important and then

00:17:19,140 --> 00:17:24,480
building that on top of these kind of

00:17:21,299 --> 00:17:26,370
legacy institution forms of trust I

00:17:24,480 --> 00:17:29,240
think that's gonna be an important way

00:17:26,370 --> 00:17:31,860
forward as as companies to adjust to how

00:17:29,240 --> 00:17:37,710
technology is shifting these these trust

00:17:31,860 --> 00:17:40,470
dynamics obviously there was you know a

00:17:37,710 --> 00:17:42,809
huge shock to the sort of idea around

00:17:40,470 --> 00:17:45,659
trusted technology with Fay

00:17:42,809 --> 00:17:51,299
news that became a huge topic after the

00:17:45,659 --> 00:17:53,639
2016 election and this led to the trust

00:17:51,299 --> 00:17:56,009
crisis this is the cover of the Harvard

00:17:53,639 --> 00:17:57,960
Business Review last summer this has

00:17:56,009 --> 00:17:59,879
been an ongoing sort of concerns last

00:17:57,960 --> 00:18:02,070
two years when I get requests by

00:17:59,879 --> 00:18:03,629
companies to give talks it's almost

00:18:02,070 --> 00:18:06,269
always about trust

00:18:03,629 --> 00:18:08,220
how Trust is changing how to recover

00:18:06,269 --> 00:18:10,860
trust when there's been a violation and

00:18:08,220 --> 00:18:13,740
and what is going on with trust for the

00:18:10,860 --> 00:18:16,889
future part of that I think really was

00:18:13,740 --> 00:18:19,049
driven by the change in in in fake news

00:18:16,889 --> 00:18:22,950
and we're seeing that even today so this

00:18:19,049 --> 00:18:25,769
week my colleague Joan Donovan had a

00:18:22,950 --> 00:18:28,080
piece in nature talking about how social

00:18:25,769 --> 00:18:29,789
media companies need to do more around

00:18:28,080 --> 00:18:33,539
misinformation especially with the co

00:18:29,789 --> 00:18:35,220
good 19 phenomenon what we'll look at

00:18:33,539 --> 00:18:37,950
when we look into deception is that

00:18:35,220 --> 00:18:41,340
people tend to live for a reason

00:18:37,950 --> 00:18:44,009
and with the pandemic it's provided a

00:18:41,340 --> 00:18:46,259
way for people that want to try and scam

00:18:44,009 --> 00:18:49,470
for either money persuasion prestige

00:18:46,259 --> 00:18:51,179
trolling it's given them an outlet

00:18:49,470 --> 00:18:54,210
there's so much attention on coab in

00:18:51,179 --> 00:18:56,639
nineteen that it's easy to implement

00:18:54,210 --> 00:19:00,419
these kind of scams and indeed these

00:18:56,639 --> 00:19:03,200
scammers and and mal info operators are

00:19:00,419 --> 00:19:06,749
coming out of the woodwork we're seeing

00:19:03,200 --> 00:19:08,700
tech companies starting to take more

00:19:06,749 --> 00:19:12,240
serious steps so this is from the New

00:19:08,700 --> 00:19:16,100
York Times this morning where Facebook's

00:19:12,240 --> 00:19:19,470
not only working on trying to reduce the

00:19:16,100 --> 00:19:21,330
visibility of misinformation rights to

00:19:19,470 --> 00:19:22,740
the corona virus but is now actually

00:19:21,330 --> 00:19:24,119
going to be reaching out to people and

00:19:22,740 --> 00:19:25,559
letting them know that they've come into

00:19:24,119 --> 00:19:28,320
contact with misinformation

00:19:25,559 --> 00:19:31,379
I think a fascinating and potentially

00:19:28,320 --> 00:19:33,360
hopeful new approach that remains to be

00:19:31,379 --> 00:19:34,649
seen whether it's going to be useful or

00:19:33,360 --> 00:19:36,629
not and whether people will believe what

00:19:34,649 --> 00:19:41,490
what Facebook has to say about their

00:19:36,629 --> 00:19:45,029
their encounters with these fake news or

00:19:41,490 --> 00:19:47,249
more broadly misinformation as with any

00:19:45,029 --> 00:19:48,690
kind of phenomenon that academics get

00:19:47,249 --> 00:19:50,700
involved in it takes a little while

00:19:48,690 --> 00:19:54,809
though to get really good research on it

00:19:50,700 --> 00:19:56,640
and I'll admit that in 2016 when the

00:19:54,809 --> 00:19:59,190
fake news crisis sort of

00:19:56,640 --> 00:20:02,720
and it became clear that there there was

00:19:59,190 --> 00:20:05,460
a lot and that it was a real problem I

00:20:02,720 --> 00:20:10,500
was just starting a sabbatical and and

00:20:05,460 --> 00:20:13,350
was sort of shook in my faith in myself

00:20:10,500 --> 00:20:16,370
anyways here I was a researcher on

00:20:13,350 --> 00:20:19,620
deception and Trust and technology and I

00:20:16,370 --> 00:20:20,190
did not see the fake news thing coming

00:20:19,620 --> 00:20:22,470
at all

00:20:20,190 --> 00:20:24,840
and I wondered you know what what what

00:20:22,470 --> 00:20:28,020
happened how did I how did I miss this

00:20:24,840 --> 00:20:30,570
how did I not see this coming and I

00:20:28,020 --> 00:20:32,880
spent a lot of my sabbatical doing some

00:20:30,570 --> 00:20:34,830
sort of soul-searching and also trying

00:20:32,880 --> 00:20:37,590
to understand what happened and I I

00:20:34,830 --> 00:20:40,770
think that there a couple of key things

00:20:37,590 --> 00:20:42,600
that have now come out that have helped

00:20:40,770 --> 00:20:44,190
me understand what I missed

00:20:42,600 --> 00:20:48,840
and I want to share a couple of those

00:20:44,190 --> 00:20:53,490
with you the first is that we I think

00:20:48,840 --> 00:20:56,520
over overestimated how broadly fake news

00:20:53,490 --> 00:20:58,500
happened so this is a finding from a

00:20:56,520 --> 00:21:02,430
really important piece that came out in

00:20:58,500 --> 00:21:03,780
science in 2019 again it takes you know

00:21:02,430 --> 00:21:05,280
two years or sort of get really great

00:21:03,780 --> 00:21:07,830
science out there typically for

00:21:05,280 --> 00:21:09,450
something as complex as fake news and

00:21:07,830 --> 00:21:13,830
that is that uh there was a lot of fake

00:21:09,450 --> 00:21:15,930
news but it was very focused as it says

00:21:13,830 --> 00:21:19,710
here 1% of individuals account for 80%

00:21:15,930 --> 00:21:22,440
of fake news exposures so most people

00:21:19,710 --> 00:21:25,610
weren't exposed to any fake news at all

00:21:22,440 --> 00:21:29,040
very small number of people were were

00:21:25,610 --> 00:21:30,840
received most of the exposures and I

00:21:29,040 --> 00:21:32,700
started to suspect this when I was

00:21:30,840 --> 00:21:34,500
asking students in my class we would do

00:21:32,700 --> 00:21:38,280
these exercises where I'd asked them to

00:21:34,500 --> 00:21:41,550
go through whatever feed they were most

00:21:38,280 --> 00:21:45,000
often using and to look for fake news

00:21:41,550 --> 00:21:46,740
and in a class of 70 to 80 people we

00:21:45,000 --> 00:21:48,960
would go back weeks and none could find

00:21:46,740 --> 00:21:50,610
anything that would even be close to

00:21:48,960 --> 00:21:52,530
fake news so here we are these they're a

00:21:50,610 --> 00:21:55,290
bunch of 20 year olds that just couldn't

00:21:52,530 --> 00:21:58,320
find any fake news and that turns out to

00:21:55,290 --> 00:22:00,120
be an important piece of the puzzle the

00:21:58,320 --> 00:22:02,880
other is the second part of this which

00:22:00,120 --> 00:22:04,650
is that very small 0.1% of people who

00:22:02,880 --> 00:22:07,320
kind of for almost all of their sharing

00:22:04,650 --> 00:22:10,070
so there was a small small number of us

00:22:07,320 --> 00:22:11,870
that were sort of super shares they were

00:22:10,070 --> 00:22:14,750
that's that shared almost all of the

00:22:11,870 --> 00:22:18,890
fake news so that most people received

00:22:14,750 --> 00:22:21,440
not very little if at all any and that

00:22:18,890 --> 00:22:24,110
various very small fraction did any of

00:22:21,440 --> 00:22:26,270
the sharing and it wasn't random who was

00:22:24,110 --> 00:22:28,070
sharing so this is from another really

00:22:26,270 --> 00:22:28,630
excellent paper by Andy Gaston

00:22:28,070 --> 00:22:34,820
colleagues

00:22:28,630 --> 00:22:37,340
Josh Tucker and NYU conservatives were

00:22:34,820 --> 00:22:40,820
much more likely to share fake news

00:22:37,340 --> 00:22:43,700
stories and people that were over 65 in

00:22:40,820 --> 00:22:46,460
that graph on the right they were much

00:22:43,700 --> 00:22:48,020
more likely to be to share fake news

00:22:46,460 --> 00:22:50,390
there were seven times more likely to

00:22:48,020 --> 00:22:53,210
share fake news then somebody that was

00:22:50,390 --> 00:22:56,030
in my class now is this because

00:22:53,210 --> 00:22:58,070
conservatives are worse no a hundred

00:22:56,030 --> 00:23:02,120
percent not is this because older people

00:22:58,070 --> 00:23:05,090
are you know somehow dumber or less you

00:23:02,120 --> 00:23:06,470
know more gullible no I don't think

00:23:05,090 --> 00:23:09,590
that's the case at all in fact these

00:23:06,470 --> 00:23:13,100
people were targeted they were targeted

00:23:09,590 --> 00:23:16,490
with almost all of the fake news part of

00:23:13,100 --> 00:23:19,640
it was because Trump as a conservative

00:23:16,490 --> 00:23:21,500
candidate was very attention grabbing it

00:23:19,640 --> 00:23:24,530
so when people were producing fake news

00:23:21,500 --> 00:23:28,100
typically it was for financial gain they

00:23:24,530 --> 00:23:31,360
were targeting conservatives and for

00:23:28,100 --> 00:23:34,190
older people there is a whole

00:23:31,360 --> 00:23:36,470
long-standing industry of elder fraud

00:23:34,190 --> 00:23:39,940
multi-billion dollar industry the FBI

00:23:36,470 --> 00:23:44,450
has its own division focused on this

00:23:39,940 --> 00:23:48,860
older folks are targeted because they

00:23:44,450 --> 00:23:50,630
are some of the wealthiest wealthiest

00:23:48,860 --> 00:23:52,250
generation currently alive so they have

00:23:50,630 --> 00:23:53,810
a lot of resources they're very

00:23:52,250 --> 00:23:55,460
civically engaged so you can actually

00:23:53,810 --> 00:23:58,220
get them to do things and to believe

00:23:55,460 --> 00:24:00,280
things and relative to these other

00:23:58,220 --> 00:24:03,380
generations tend to be a little bit less

00:24:00,280 --> 00:24:05,390
tech-savvy so it's not that they're less

00:24:03,380 --> 00:24:07,580
intelligence but they're they have less

00:24:05,390 --> 00:24:12,710
experience than other generations and

00:24:07,580 --> 00:24:15,230
using online technologies so they're

00:24:12,710 --> 00:24:18,320
also just being targeted and that's why

00:24:15,230 --> 00:24:20,660
they end up sharing fake news so this

00:24:18,320 --> 00:24:23,430
was one piece of the puzzle of like how

00:24:20,660 --> 00:24:26,310
I screwed up so badly

00:24:23,430 --> 00:24:28,680
this is I tend to focus on general

00:24:26,310 --> 00:24:31,620
population I'm often interested in how

00:24:28,680 --> 00:24:34,350
people interpersonal ways trust and

00:24:31,620 --> 00:24:35,580
deceive each other and this was a huge

00:24:34,350 --> 00:24:38,610
change in the way that they were

00:24:35,580 --> 00:24:40,680
targeting very specific parts of the of

00:24:38,610 --> 00:24:43,380
the population and it also suggests that

00:24:40,680 --> 00:24:45,410
we need to provide a lot of support to

00:24:43,380 --> 00:24:49,020
these folks so we have a whole new

00:24:45,410 --> 00:24:52,620
research program aimed at helping older

00:24:49,020 --> 00:24:55,200
people older adults and happy to talk

00:24:52,620 --> 00:24:56,880
about that if people are interested the

00:24:55,200 --> 00:25:01,020
other piece of that puzzle that I

00:24:56,880 --> 00:25:03,390
realized is a lot of my work tends to

00:25:01,020 --> 00:25:05,190
focus on people that we know so we can

00:25:03,390 --> 00:25:07,050
think of this is our social tie Network

00:25:05,190 --> 00:25:09,270
these are people that I either have a

00:25:07,050 --> 00:25:11,220
reputational tie with so I know Joe fish

00:25:09,270 --> 00:25:12,990
and you have a history or it could be

00:25:11,220 --> 00:25:16,800
that girl I want to go dating I don't

00:25:12,990 --> 00:25:18,630
have a I don't know her yet but I want

00:25:16,800 --> 00:25:21,120
to so I anticipate some future

00:25:18,630 --> 00:25:23,430
interaction I want her to be a social

00:25:21,120 --> 00:25:25,140
tie it could be a client so I don't have

00:25:23,430 --> 00:25:26,820
really shape with this company or this

00:25:25,140 --> 00:25:29,010
other person yet but I want to do

00:25:26,820 --> 00:25:30,750
business with them so my reputation is

00:25:29,010 --> 00:25:32,640
involved and that's what social ties

00:25:30,750 --> 00:25:34,920
really are about these connections with

00:25:32,640 --> 00:25:38,790
reputations and that's where most of my

00:25:34,920 --> 00:25:41,220
research had focused and there is of

00:25:38,790 --> 00:25:43,010
course online this whole other world

00:25:41,220 --> 00:25:45,510
that will call the zero time Network

00:25:43,010 --> 00:25:47,700
these are people that have no

00:25:45,510 --> 00:25:49,740
relationship with me no anticipated

00:25:47,700 --> 00:25:52,080
relationship with me but they're really

00:25:49,740 --> 00:25:56,370
what social media and online

00:25:52,080 --> 00:26:00,750
technologies make possible so that

00:25:56,370 --> 00:26:03,630
Russian spy the Nigerian prince as well

00:26:00,750 --> 00:26:06,170
as legitimate appetizers and people that

00:26:03,630 --> 00:26:08,490
I want me to get out and vote these

00:26:06,170 --> 00:26:10,920
people and organizations can actually

00:26:08,490 --> 00:26:13,050
access me now because of online

00:26:10,920 --> 00:26:16,080
technologies through a variety of ways

00:26:13,050 --> 00:26:20,460
from advertising to you know just

00:26:16,080 --> 00:26:23,730
reaching out on Twitter so one problem

00:26:20,460 --> 00:26:27,180
here that I realized in my work was for

00:26:23,730 --> 00:26:30,480
a lot of people these two things have

00:26:27,180 --> 00:26:31,590
merged online and especially for the

00:26:30,480 --> 00:26:34,230
older people that I was just mentioning

00:26:31,590 --> 00:26:37,049
people over 65 in particular people that

00:26:34,230 --> 00:26:41,190
didn't have email as part of their job

00:26:37,049 --> 00:26:43,769
before they retired for these folks the

00:26:41,190 --> 00:26:46,679
online the social time network and the

00:26:43,769 --> 00:26:48,840
zero time network overlap and this could

00:26:46,679 --> 00:26:50,220
be in your Facebook newsfeed it could be

00:26:48,840 --> 00:26:54,749
in your Twitter feed it could be in your

00:26:50,220 --> 00:26:56,279
inbox or your email inbox and because

00:26:54,749 --> 00:26:58,169
those things now are just sitting on top

00:26:56,279 --> 00:27:01,320
of each other without a lot of

00:26:58,169 --> 00:27:03,989
experience it's difficult for people to

00:27:01,320 --> 00:27:04,980
tease those two networks apart and to

00:27:03,989 --> 00:27:07,710
understand that yeah

00:27:04,980 --> 00:27:09,210
when Joe fish emails me and has an

00:27:07,710 --> 00:27:11,279
important request he needs me to do

00:27:09,210 --> 00:27:14,009
something I need to do that but what he

00:27:11,279 --> 00:27:17,639
says actually matters whereas Starman

00:27:14,009 --> 00:27:19,919
six six four two what he does what he

00:27:17,639 --> 00:27:23,700
says what he wants should have zero

00:27:19,919 --> 00:27:26,009
impact on me my actions my emotions but

00:27:23,700 --> 00:27:28,499
when they're sitting on top of each

00:27:26,009 --> 00:27:30,899
other it's really difficult for people

00:27:28,499 --> 00:27:33,480
that have her lower and experience to

00:27:30,899 --> 00:27:35,279
tease those apart and so their emotions

00:27:33,480 --> 00:27:37,529
and their actions and even their

00:27:35,279 --> 00:27:40,139
thinking can be influenced by Starman

00:27:37,529 --> 00:27:43,080
six six four two and that I think is

00:27:40,139 --> 00:27:45,960
another major problem for fake news and

00:27:43,080 --> 00:27:47,940
another reason why I missed that because

00:27:45,960 --> 00:27:51,259
you know we can look at those two worlds

00:27:47,940 --> 00:27:56,850
separately but online they're obviously

00:27:51,259 --> 00:27:59,700
both involved okay that sort of would

00:27:56,850 --> 00:28:01,350
take on trust over time we're gonna look

00:27:59,700 --> 00:28:03,869
now at what we can learn about trust

00:28:01,350 --> 00:28:06,779
from deception detection research it's

00:28:03,869 --> 00:28:09,139
an old field now about 60 to 70 years of

00:28:06,779 --> 00:28:13,679
work and psychology and communication

00:28:09,139 --> 00:28:15,899
and there's been over 300 studies done

00:28:13,679 --> 00:28:17,340
so there's some real key findings here

00:28:15,899 --> 00:28:18,960
that I'm going to share with you now

00:28:17,340 --> 00:28:21,720
sort of think of it as a primer on

00:28:18,960 --> 00:28:24,389
deception detection and out of it we can

00:28:21,720 --> 00:28:27,419
pull some really really core things

00:28:24,389 --> 00:28:29,070
about trust one thing in particular that

00:28:27,419 --> 00:28:32,159
I'll share that I think is the most

00:28:29,070 --> 00:28:33,749
important thing going forward to do that

00:28:32,159 --> 00:28:36,509
we're going to do our first little

00:28:33,749 --> 00:28:39,359
experiment and so with Joe fish's help

00:28:36,509 --> 00:28:43,830
I'm going to ask all of you to fill out

00:28:39,359 --> 00:28:47,009
a poll I'll explain it as you go into

00:28:43,830 --> 00:28:50,559
either slack or on Twitter calm /jo fish

00:28:47,009 --> 00:28:55,090
to participate these are

00:28:50,559 --> 00:28:57,490
to hotel reviews of the James hotel in

00:28:55,090 --> 00:28:59,769
Chicago one of them is real we took from

00:28:57,490 --> 00:29:01,929
hotels calm which is a verified review

00:28:59,769 --> 00:29:05,499
say you have to pay and stay to review

00:29:01,929 --> 00:29:07,749
and the other we purchased so we tend to

00:29:05,499 --> 00:29:10,350
we buy a lot of fake reviews

00:29:07,749 --> 00:29:12,999
we never post them online but we'll get

00:29:10,350 --> 00:29:15,610
Turkish for example to look at the

00:29:12,999 --> 00:29:17,080
website so we can ask them to write a

00:29:15,610 --> 00:29:21,009
fake review in this case we asked them

00:29:17,080 --> 00:29:23,379
to write a positive fake review now what

00:29:21,009 --> 00:29:26,369
your job is is to decide which of these

00:29:23,379 --> 00:29:30,249
reviews about the James hotel is fake is

00:29:26,369 --> 00:29:33,009
a fake or be fake and in the polls you

00:29:30,249 --> 00:29:36,159
can indicate whether you think a is fake

00:29:33,009 --> 00:29:39,009
or B is fake and I'll give you a 20 to

00:29:36,159 --> 00:29:41,529
30 seconds to read through the two and

00:29:39,009 --> 00:29:42,399
make your choice and Joe fish let me

00:29:41,529 --> 00:29:44,350
know if there's anything other

00:29:42,399 --> 00:29:46,410
instructions

00:29:44,350 --> 00:29:46,410
you

00:29:46,790 --> 00:30:04,350
so good votes are flooding in flooding

00:29:52,290 --> 00:30:07,860
in as a strong way to put it now before

00:30:04,350 --> 00:30:11,070
Joe fish shares the results one of the

00:30:07,860 --> 00:30:12,870
main findings in meta analyses where all

00:30:11,070 --> 00:30:15,090
the studies on deception detection we

00:30:12,870 --> 00:30:20,100
put together is that when you have a

00:30:15,090 --> 00:30:22,890
50/50 chance people perform basically at

00:30:20,100 --> 00:30:25,260
5050 when we've analyzed these 300

00:30:22,890 --> 00:30:27,390
studies and they're thousands and

00:30:25,260 --> 00:30:30,260
thousands of trials people on average

00:30:27,390 --> 00:30:34,010
perform at 54% under these circumstances

00:30:30,260 --> 00:30:36,430
so let's see if Moe zillions can beat

00:30:34,010 --> 00:30:40,860
54%

00:30:36,430 --> 00:30:40,860
if we can get that kind of fidelity

00:30:45,660 --> 00:30:49,330
right we're just giving a little bit of

00:30:47,560 --> 00:30:51,670
time I just checked and we just got up

00:30:49,330 --> 00:30:54,040
to the bit in our folks who are watching

00:30:51,670 --> 00:30:55,980
in hubs they just saw you like ask the

00:30:54,040 --> 00:31:00,870
question

00:30:55,980 --> 00:31:02,980
we've got 15 votes so far on slack and

00:31:00,870 --> 00:31:08,620
we've got other four votes over on

00:31:02,980 --> 00:31:14,290
Twitter I know a couple of minutes a

00:31:08,620 --> 00:31:22,840
couple of seconds and we'll see so all

00:31:14,290 --> 00:31:24,280
right seven votes on Twitter right I'm

00:31:22,840 --> 00:31:40,030
gonna give it 15 more seconds and then

00:31:24,280 --> 00:31:41,500
I'm gonna call it okay and we're getting

00:31:40,030 --> 00:31:43,060
comments in this channel people like

00:31:41,500 --> 00:31:46,900
well it's hard to say both of them seem

00:31:43,060 --> 00:31:50,020
plausible good which is good right that

00:31:46,900 --> 00:31:53,680
means you invested your money of buying

00:31:50,020 --> 00:31:59,680
fake reviews well okay I'm gonna call it

00:31:53,680 --> 00:32:02,140
so we have 13 on on Twitter we have

00:31:59,680 --> 00:32:04,210
seven votes six of them think that the

00:32:02,140 --> 00:32:06,850
review a is fake and one of them think

00:32:04,210 --> 00:32:10,540
thinks that review B is fake on slack

00:32:06,850 --> 00:32:13,660
it's a lot closer on slack 13 people

00:32:10,540 --> 00:32:19,480
think review a is fake and 14 people

00:32:13,660 --> 00:32:22,390
think review B is Thanks so pretty much

00:32:19,480 --> 00:32:24,940
exactly 50 exactly 50 50

00:32:22,390 --> 00:32:29,410
all right back to you great well it's

00:32:24,940 --> 00:32:31,480
actually 19 for a and 21 for B which

00:32:29,410 --> 00:32:35,110
will give us pretty much exactly 54

00:32:31,480 --> 00:32:38,230
percent because B is the fake one so

00:32:35,110 --> 00:32:40,290
Mazzilli ins have fittin exactly what

00:32:38,230 --> 00:32:43,990
the meta-analysis finds which is that

00:32:40,290 --> 00:32:46,240
when is fifty-fifty we do terribly we

00:32:43,990 --> 00:32:49,540
basically perform right at chance

00:32:46,240 --> 00:32:50,860
and for those of you that were looking

00:32:49,540 --> 00:32:52,390
and trying to make decisions this is

00:32:50,860 --> 00:32:54,370
actually a really important kind of

00:32:52,390 --> 00:32:55,480
decision because we all use reviews to

00:32:54,370 --> 00:32:56,950
decide where we

00:32:55,480 --> 00:33:00,700
you know once we're able to go places

00:32:56,950 --> 00:33:03,180
again and all of you would have

00:33:00,700 --> 00:33:06,240
different kinds of cues that you rely on

00:33:03,180 --> 00:33:08,530
in this case we were able to use a

00:33:06,240 --> 00:33:10,990
computer algorithm to detect what

00:33:08,530 --> 00:33:12,870
Turkish do differently than real

00:33:10,990 --> 00:33:16,809
reviewers some of you may have noticed

00:33:12,870 --> 00:33:18,820
the overly positive adjectives that is

00:33:16,809 --> 00:33:20,680
indeed a problem Turkish tended to be

00:33:18,820 --> 00:33:22,510
overly positive but they did other

00:33:20,680 --> 00:33:25,540
things too like they failed to talk

00:33:22,510 --> 00:33:28,870
about the rooms they didn't use spatial

00:33:25,540 --> 00:33:30,460
terms as much they tended to talk about

00:33:28,870 --> 00:33:31,990
themselves and who they were with and

00:33:30,460 --> 00:33:34,000
what they were doing more this is like a

00:33:31,990 --> 00:33:36,520
narrative mode and Liars often switch

00:33:34,000 --> 00:33:39,160
into storytelling for those of you who

00:33:36,520 --> 00:33:41,440
thought perhaps it was spelling mistakes

00:33:39,160 --> 00:33:44,170
no those are just bad spellers that not

00:33:41,440 --> 00:33:45,640
seem to be related well that's great so

00:33:44,170 --> 00:33:47,470
that's one of the most important

00:33:45,640 --> 00:33:49,480
findings and something to be really

00:33:47,470 --> 00:33:51,070
thoughtful is that when we don't have

00:33:49,480 --> 00:33:53,050
any other information we can't

00:33:51,070 --> 00:33:55,690
interrogate anything we can't look at

00:33:53,050 --> 00:33:59,260
other corresponding information we do

00:33:55,690 --> 00:34:01,150
really poorly when it's 5050 and in fact

00:33:59,260 --> 00:34:03,250
you guys were perfectly on just slightly

00:34:01,150 --> 00:34:05,110
above chance just like everybody else

00:34:03,250 --> 00:34:07,120
another really important thing is that

00:34:05,110 --> 00:34:09,280
we tend to think that there's cues that

00:34:07,120 --> 00:34:10,960
relate to it so when we pull people

00:34:09,280 --> 00:34:13,149
around the world there's you know the

00:34:10,960 --> 00:34:15,610
eyes and whether the eyes will tell you

00:34:13,149 --> 00:34:17,500
they're shifty or not in fact there's

00:34:15,610 --> 00:34:19,870
sort of these three channels verbal what

00:34:17,500 --> 00:34:21,639
people say nonverbal and physiological

00:34:19,870 --> 00:34:23,860
and this is gonna be hard for you to

00:34:21,639 --> 00:34:26,080
believe it took me years to come to

00:34:23,860 --> 00:34:28,210
grips with this but in fact oops right

00:34:26,080 --> 00:34:30,940
there is no Pinocchio's nose

00:34:28,210 --> 00:34:33,480
there is no single cue that will tell

00:34:30,940 --> 00:34:36,070
you when people are lying and even here

00:34:33,480 --> 00:34:37,840
with this lie detector that's not

00:34:36,070 --> 00:34:40,000
actually detecting lies it's measuring

00:34:37,840 --> 00:34:42,100
arousal which is only assumed to be

00:34:40,000 --> 00:34:43,870
correlated with lying and they're not

00:34:42,100 --> 00:34:46,510
even allowed in a court of law in the US

00:34:43,870 --> 00:34:47,800
and it took me a long time to come to

00:34:46,510 --> 00:34:50,169
grips with this the fact that there's no

00:34:47,800 --> 00:34:52,659
Pinocchio's nose but I can I can confirm

00:34:50,169 --> 00:34:55,270
there is no cue that will tell you every

00:34:52,659 --> 00:34:59,770
time when people are alive

00:34:55,270 --> 00:35:02,770
in contrast we rely on what we call

00:34:59,770 --> 00:35:04,720
believability cues so these we think are

00:35:02,770 --> 00:35:07,390
related to deception but in fact they're

00:35:04,720 --> 00:35:08,950
not now here we'll do another poll this

00:35:07,390 --> 00:35:11,170
will be much easier I'm guess

00:35:08,950 --> 00:35:12,730
that will perform you know very high

00:35:11,170 --> 00:35:15,010
with consensus that almost everybody

00:35:12,730 --> 00:35:17,829
will agree on one of these what if you

00:35:15,010 --> 00:35:19,329
go back to the poll you can choose which

00:35:17,829 --> 00:35:22,450
one of these do you think is the most

00:35:19,329 --> 00:35:25,060
trustworthy do you think a is a person

00:35:22,450 --> 00:35:27,849
that's the most trustworthy or is B the

00:35:25,060 --> 00:35:29,380
one that's most trustworthy so Josh

00:35:27,849 --> 00:35:33,640
maybe we can throw it to the poll again

00:35:29,380 --> 00:35:38,710
for this and a is a trustworthy

00:35:33,640 --> 00:35:43,470
whereas be trustworthy all right we've

00:35:38,710 --> 00:35:43,470
got was starting to get things coming in

00:36:00,500 --> 00:36:02,560
you

00:36:04,440 --> 00:36:24,130
all right we're getting things coming in

00:36:07,740 --> 00:36:28,990
I think that was me checking that hubs

00:36:24,130 --> 00:36:34,120
is working and it's come through so

00:36:28,990 --> 00:36:36,430
finally the raw numbers we're seeing be

00:36:34,120 --> 00:36:38,920
as most trustworthy when I combine all

00:36:36,430 --> 00:36:43,420
combined both poles we're seeing 20

00:36:38,920 --> 00:36:45,250
votes for B and 4 votes for a but we

00:36:43,420 --> 00:36:48,820
also have comments like both look fake

00:36:45,250 --> 00:36:50,680
I'm not sure I want to trust any and the

00:36:48,820 --> 00:36:53,350
point that Trust is contextual I would

00:36:50,680 --> 00:36:55,570
trust security threat analysis more from

00:36:53,350 --> 00:36:58,750
a I would trust an offer of help more

00:36:55,570 --> 00:37:01,330
from B I reject the implied framing of

00:36:58,750 --> 00:37:06,940
universal trustworthiness I don't trust

00:37:01,330 --> 00:37:09,550
your poll I love my colleagues fantastic

00:37:06,940 --> 00:37:12,970
well great points

00:37:09,550 --> 00:37:15,090
in fact people universally will trust B

00:37:12,970 --> 00:37:18,430
they consider B to be more trustworthy

00:37:15,090 --> 00:37:21,460
and this has been psychological work the

00:37:18,430 --> 00:37:23,140
point about trusting contextual is

00:37:21,460 --> 00:37:25,330
hugely important that I love that

00:37:23,140 --> 00:37:26,620
absolutely right but when you ask

00:37:25,330 --> 00:37:28,240
somebody you know who would you trust

00:37:26,620 --> 00:37:31,210
and you had to pick it one of these two

00:37:28,240 --> 00:37:33,730
people it's pretty much a universal

00:37:31,210 --> 00:37:35,290
finding across cultures that B is the

00:37:33,730 --> 00:37:37,510
one that appears my trusting a

00:37:35,290 --> 00:37:39,070
fascinating thing which one of your

00:37:37,510 --> 00:37:41,340
colleagues pointed out is that there is

00:37:39,070 --> 00:37:46,570
no human here this is these are totally

00:37:41,340 --> 00:37:48,940
computer-generated and despite that we

00:37:46,570 --> 00:37:51,910
can make inferences and so right the

00:37:48,940 --> 00:37:53,650
twenty people agreed that B is triple

00:37:51,910 --> 00:37:55,990
more trustworthy even though there is no

00:37:53,650 --> 00:37:57,160
real human here it's because of those

00:37:55,990 --> 00:37:59,260
signals in the face

00:37:57,160 --> 00:38:01,720
same thing operates with deception when

00:37:59,260 --> 00:38:05,170
people act friendly and nice and smile

00:38:01,720 --> 00:38:07,930
and seem calm their demeanor is relaxed

00:38:05,170 --> 00:38:10,030
they seem more believable and it has

00:38:07,930 --> 00:38:12,700
actually nothing to do it's completely

00:38:10,030 --> 00:38:15,910
independent with deception and so the

00:38:12,700 --> 00:38:17,170
real key point here is we tend to assess

00:38:15,910 --> 00:38:20,140
believability

00:38:17,170 --> 00:38:22,380
not deception and that's where trust

00:38:20,140 --> 00:38:24,220
comes in Trust is basically this initial

00:38:22,380 --> 00:38:26,230
framing of does this person seem

00:38:24,220 --> 00:38:28,690
trustworthy but then as I pointed

00:38:26,230 --> 00:38:31,690
earlier reliability and competence are

00:38:28,690 --> 00:38:34,329
the other two main ingredients of trust

00:38:31,690 --> 00:38:36,250
over time so one is about appearing

00:38:34,329 --> 00:38:37,869
these sort of believability cues how

00:38:36,250 --> 00:38:40,390
does a company come across how does a

00:38:37,869 --> 00:38:43,180
leader come across these impressions

00:38:40,390 --> 00:38:45,369
matter they drive how we then interact

00:38:43,180 --> 00:38:48,849
with that person or that entity later

00:38:45,369 --> 00:38:50,940
but over time what matters is competence

00:38:48,849 --> 00:38:53,859
can they do what they say they do and

00:38:50,940 --> 00:38:56,799
reliability do they do it when they say

00:38:53,859 --> 00:39:00,720
they're going to do it and those are the

00:38:56,799 --> 00:39:03,849
three key ingredients of trustworthiness

00:39:00,720 --> 00:39:06,280
the other things that we can learn here

00:39:03,849 --> 00:39:08,079
is these are the points that I've made

00:39:06,280 --> 00:39:09,849
so we're tend to perform a chance

00:39:08,079 --> 00:39:13,450
there's no reliable cue and we rely on

00:39:09,849 --> 00:39:15,579
believability the last thing is the most

00:39:13,450 --> 00:39:18,400
important and only finding that

00:39:15,579 --> 00:39:22,030
replicates in every single to section

00:39:18,400 --> 00:39:23,380
detection study the only replicated

00:39:22,030 --> 00:39:26,260
finding is something called the truth

00:39:23,380 --> 00:39:29,380
bias that is that we believe other

00:39:26,260 --> 00:39:32,890
people automatically when we first hear

00:39:29,380 --> 00:39:34,780
them it's not because we're nice it's

00:39:32,890 --> 00:39:37,390
because of the way we designed language

00:39:34,780 --> 00:39:39,730
we evolved language so that we have to

00:39:37,390 --> 00:39:42,819
have what we believe our cooperative

00:39:39,730 --> 00:39:45,790
partners so our default is a trust state

00:39:42,819 --> 00:39:47,319
and my favorite work right now and a

00:39:45,790 --> 00:39:50,530
book I'd highly recommend for anybody

00:39:47,319 --> 00:39:53,559
interesting this further is called Deut

00:39:50,530 --> 00:39:56,260
by Tim Levine and in that he lays out

00:39:53,559 --> 00:39:59,670
his theory called the trust default

00:39:56,260 --> 00:40:04,359
theory so we begin any interaction with

00:39:59,670 --> 00:40:06,760
any human or entity in a trust state and

00:40:04,359 --> 00:40:08,440
then we move out of that trust state

00:40:06,760 --> 00:40:10,089
only when we become suspicious

00:40:08,440 --> 00:40:12,309
perhaps because this something doesn't

00:40:10,089 --> 00:40:14,319
seem right you've learned some of their

00:40:12,309 --> 00:40:16,900
information from someone else it

00:40:14,319 --> 00:40:20,319
contrasts something you already know and

00:40:16,900 --> 00:40:23,500
this truth bias is is sort of the core

00:40:20,319 --> 00:40:26,890
of human trust it's what allows it to to

00:40:23,500 --> 00:40:29,470
operate and as I mentioned it's so

00:40:26,890 --> 00:40:30,220
powerful that it's the only only finding

00:40:29,470 --> 00:40:32,980
we replicate

00:40:30,220 --> 00:40:36,040
every single deception detection study

00:40:32,980 --> 00:40:38,290
until very recently and so I want to

00:40:36,040 --> 00:40:41,380
share with you a study that we've done

00:40:38,290 --> 00:40:43,090
that to me is kind of worrisome and I'm

00:40:41,380 --> 00:40:45,369
an optimist I'm a Canadian and I'm an

00:40:43,090 --> 00:40:47,710
optimist so it makes me feel pretty good

00:40:45,369 --> 00:40:49,359
about the future and this study has me a

00:40:47,710 --> 00:40:51,580
little bit worried and I'll explain why

00:40:49,359 --> 00:40:55,330
the first thing I have to explain is

00:40:51,580 --> 00:40:57,099
when we have a truth bias it leads to

00:40:55,330 --> 00:41:00,190
something called the veracity effect and

00:40:57,099 --> 00:41:02,560
it turns out that that 54 percent number

00:41:00,190 --> 00:41:06,700
is pretty much just a mathematical

00:41:02,560 --> 00:41:10,660
outcome of the truth bias so if it's

00:41:06,700 --> 00:41:12,640
5050 like your hotel review and I have a

00:41:10,660 --> 00:41:14,859
truth bias most of the time I'm just

00:41:12,640 --> 00:41:17,640
gonna say yeah it's true yeah it's true

00:41:14,859 --> 00:41:21,070
yeah it's true so when I'm shown

00:41:17,640 --> 00:41:24,070
truthful things real messages or real

00:41:21,070 --> 00:41:25,630
reviews I'm gonna be really accurate so

00:41:24,070 --> 00:41:27,040
I might like on the right hand side here

00:41:25,630 --> 00:41:29,080
I might perform at ninety percent

00:41:27,040 --> 00:41:31,000
because I'm just saying true most of the

00:41:29,080 --> 00:41:32,980
time and everyone saw I'll say well

00:41:31,000 --> 00:41:35,710
maybe that's fake and I'm wrong but most

00:41:32,980 --> 00:41:37,359
of the time I'm staying true and so just

00:41:35,710 --> 00:41:39,580
by chance by saying true all the time

00:41:37,359 --> 00:41:42,790
I'm gonna be really accurate at truthful

00:41:39,580 --> 00:41:45,730
things on the flipside when I mean line

00:41:42,790 --> 00:41:46,810
two I'm gonna do really poorly so

00:41:45,730 --> 00:41:49,990
because I'm going to keep saying true

00:41:46,810 --> 00:41:51,609
true true and I'll miss most of the lies

00:41:49,990 --> 00:41:54,339
I might get the odd one and so I'll have

00:41:51,609 --> 00:41:56,890
a little bit accuracy and that 54

00:41:54,339 --> 00:41:59,920
percent number is actually the averaging

00:41:56,890 --> 00:42:01,990
of two different accuracies truthful

00:41:59,920 --> 00:42:04,390
ability my ability to detect true

00:42:01,990 --> 00:42:07,060
actions which is high and my ability to

00:42:04,390 --> 00:42:09,339
detect lies which is low put them

00:42:07,060 --> 00:42:12,820
together and they come down to this sort

00:42:09,339 --> 00:42:16,690
of fifty percent number and so when you

00:42:12,820 --> 00:42:19,510
combine the truth bias with the

00:42:16,690 --> 00:42:21,460
underlying deception rate you get this

00:42:19,510 --> 00:42:23,290
kind of effect the veracity effect where

00:42:21,460 --> 00:42:26,220
the truth bias is I do really well with

00:42:23,290 --> 00:42:28,839
truth and I do really poorly with lies

00:42:26,220 --> 00:42:32,380
this works in communication and

00:42:28,839 --> 00:42:35,619
information systems in which there's low

00:42:32,380 --> 00:42:38,109
rates of line some rate depends on the

00:42:35,619 --> 00:42:40,330
system but it should be relatively

00:42:38,109 --> 00:42:43,099
infrequent when it's relatively

00:42:40,330 --> 00:42:45,349
infrequent and the truth bias is in play

00:42:43,099 --> 00:42:47,119
I'll just be naturally acted all the

00:42:45,349 --> 00:42:49,220
time most of the time Joe fish when we

00:42:47,119 --> 00:42:50,960
talked is telling you the truth we've

00:42:49,220 --> 00:42:53,930
looked at text messaging for example

00:42:50,960 --> 00:42:56,329
college students and adults text

00:42:53,930 --> 00:42:58,220
messages tend to be about 95 to 97

00:42:56,329 --> 00:43:00,710
percent of time honest

00:42:58,220 --> 00:43:02,059
so that means when I see text messages

00:43:00,710 --> 00:43:04,279
from joe fish and I just do my truth

00:43:02,059 --> 00:43:06,140
bias my accuracy is gonna be really high

00:43:04,279 --> 00:43:09,650
I'm gonna perform it whatever that his

00:43:06,140 --> 00:43:14,029
truthful rate is I'll miss those two to

00:43:09,650 --> 00:43:16,130
five percent Blair he says he's on his

00:43:14,029 --> 00:43:18,859
way but in fact he's just getting out of

00:43:16,130 --> 00:43:20,720
the shower but in the overall system

00:43:18,859 --> 00:43:21,979
it's working well I don't have to think

00:43:20,720 --> 00:43:23,450
do I trust him on this one

00:43:21,979 --> 00:43:25,460
do I trust him on this one it's

00:43:23,450 --> 00:43:27,739
automatic and that leads to this

00:43:25,460 --> 00:43:30,670
veracity effect which is we're good with

00:43:27,739 --> 00:43:34,210
real or truthful and we're bad with lie

00:43:30,670 --> 00:43:34,210
the problem

00:43:34,280 --> 00:43:39,350
um that is that comes out of the fake

00:43:36,620 --> 00:43:42,050
news phenomena is we're now really

00:43:39,350 --> 00:43:45,980
worried about the news and about media

00:43:42,050 --> 00:43:47,270
and who studies with my PT a former

00:43:45,980 --> 00:43:51,820
student David Markowitz

00:43:47,270 --> 00:43:55,580
he's now Oregon we showed people

00:43:51,820 --> 00:43:58,220
headlines from Facebook actual headlines

00:43:55,580 --> 00:44:00,290
and they were either in politics science

00:43:58,220 --> 00:44:03,710
or health and they were either real

00:44:00,290 --> 00:44:06,620
headlines or they were fake that we

00:44:03,710 --> 00:44:09,910
pulled from a fake news database and we

00:44:06,620 --> 00:44:15,069
asked them to detect whether they

00:44:09,910 --> 00:44:16,359
and here's how they performed the three

00:44:15,069 --> 00:44:18,309
different domains you can see health

00:44:16,359 --> 00:44:21,339
politics science so we see there's these

00:44:18,309 --> 00:44:24,339
three kinds how well they did fake and

00:44:21,339 --> 00:44:28,140
real so the dark bars are fake news and

00:44:24,339 --> 00:44:31,780
you can see that they kind of did poorly

00:44:28,140 --> 00:44:34,059
they were performing around 58% people

00:44:31,780 --> 00:44:35,680
did pretty well in politics one reason

00:44:34,059 --> 00:44:37,690
for that is they seem to be better

00:44:35,680 --> 00:44:40,390
versed in politics than in health and

00:44:37,690 --> 00:44:44,049
science news and the detection accuracy

00:44:40,390 --> 00:44:45,760
was less than you know 60% but in some

00:44:44,049 --> 00:44:48,819
cases we can see they did better in fake

00:44:45,760 --> 00:44:51,910
news and in real news you can see that

00:44:48,819 --> 00:44:55,809
with health and with politics and so

00:44:51,910 --> 00:44:59,980
this is evidence of a no drip lacks that

00:44:55,809 --> 00:45:02,440
fact there's a little bit here of a lie

00:44:59,980 --> 00:45:03,760
bias and we don't get that veracity

00:45:02,440 --> 00:45:07,000
effect we should see really good

00:45:03,760 --> 00:45:09,280
accuracy for real news and really poor

00:45:07,000 --> 00:45:10,960
accuracy for fake news but in fact

00:45:09,280 --> 00:45:13,349
people are really really suspicious of

00:45:10,960 --> 00:45:15,809
any news now that they see and obviously

00:45:13,349 --> 00:45:17,980
even more suspicious when they're in a

00:45:15,809 --> 00:45:22,210
experiment looking at this kind of thing

00:45:17,980 --> 00:45:24,220
we just study where we also included

00:45:22,210 --> 00:45:26,680
things like the legs were coming from

00:45:24,220 --> 00:45:28,270
friends or there were lots of

00:45:26,680 --> 00:45:30,369
endorsements like a lot of people shared

00:45:28,270 --> 00:45:31,599
it or a lot of people liked it and we

00:45:30,369 --> 00:45:35,920
see the same kind of thing that people

00:45:31,599 --> 00:45:38,529
are actually doing pretty well with the

00:45:35,920 --> 00:45:40,960
fake news and you know roughly the same

00:45:38,529 --> 00:45:43,599
for real news pointed here do we don't

00:45:40,960 --> 00:45:47,020
see that veracity effect instead what we

00:45:43,599 --> 00:45:49,779
see is a reduced ability to detect real

00:45:47,020 --> 00:45:53,049
news and an increased ability to detect

00:45:49,779 --> 00:45:54,609
fake news on the surface that might seem

00:45:53,049 --> 00:45:57,789
good like okay good we're going to get

00:45:54,609 --> 00:46:00,400
duped like faking is less often it's

00:45:57,789 --> 00:46:02,829
gonna come at a huge cost and to put it

00:46:00,400 --> 00:46:04,990
in in perspective all the concern in the

00:46:02,829 --> 00:46:06,910
media around fake news and in part by

00:46:04,990 --> 00:46:08,440
you know academics like myself talking

00:46:06,910 --> 00:46:11,289
about fake news and them and the worries

00:46:08,440 --> 00:46:13,210
we have is its enhancing the public's

00:46:11,289 --> 00:46:13,839
ability to detect fake news that's a

00:46:13,210 --> 00:46:16,329
good thing

00:46:13,839 --> 00:46:19,770
the problem is is most of the news that

00:46:16,329 --> 00:46:22,750
we encounter even in social media is

00:46:19,770 --> 00:46:23,620
real news fake news is a very small

00:46:22,750 --> 00:46:25,570
overall

00:46:23,620 --> 00:46:27,040
percentage it's difficult to judge and

00:46:25,570 --> 00:46:29,200
it varies by platform but it's usually

00:46:27,040 --> 00:46:32,070
in the single digits of the amount of

00:46:29,200 --> 00:46:34,870
news that a person will consume in a day

00:46:32,070 --> 00:46:38,890
so while we're improving our ability to

00:46:34,870 --> 00:46:41,650
detect fake news that's that's a benefit

00:46:38,890 --> 00:46:44,380
that's relatively small compared to the

00:46:41,650 --> 00:46:46,330
hit we're taking on being able to

00:46:44,380 --> 00:46:48,850
understand what's real news which is

00:46:46,330 --> 00:46:54,070
most of our our news consumption and

00:46:48,850 --> 00:46:55,510
that's a major worry that I have alright

00:46:54,070 --> 00:46:58,420
I think I'm going to pause there we're

00:46:55,510 --> 00:46:59,650
45 minutes in I do have a bunch of stuff

00:46:58,420 --> 00:47:03,010
I want to share with you about wellness

00:46:59,650 --> 00:47:04,930
and the up side of it and the short

00:47:03,010 --> 00:47:10,480
answer is it turns out when we look at

00:47:04,930 --> 00:47:13,150
over 220 articles over 12 years then the

00:47:10,480 --> 00:47:16,270
overall effect size of social media and

00:47:13,150 --> 00:47:17,380
its correlation with well-being and lots

00:47:16,270 --> 00:47:20,410
of different kinds of well-being is

00:47:17,380 --> 00:47:22,210
effectively zero and by effectively I

00:47:20,410 --> 00:47:24,030
mean we have a really precise estimate

00:47:22,210 --> 00:47:29,230
of zero the confidence intervals are

00:47:24,030 --> 00:47:32,140
negative 0.04 to 0.02 so I think a lot

00:47:29,230 --> 00:47:33,820
of the hysteria around social media and

00:47:32,140 --> 00:47:40,390
its effect on well-being before the

00:47:33,820 --> 00:47:42,580
pandemic was overdone and one thing the

00:47:40,390 --> 00:47:44,590
pandemic has done is has led people to

00:47:42,580 --> 00:47:46,530
be thinking more about like what you're

00:47:44,590 --> 00:47:48,730
doing with these technologies is

00:47:46,530 --> 00:47:51,040
important rather than how much time

00:47:48,730 --> 00:47:53,050
you're spending on so I'll just leave it

00:47:51,040 --> 00:47:54,550
there and perhaps I'll come back again

00:47:53,050 --> 00:47:56,710
some other time we can go into those

00:47:54,550 --> 00:47:58,690
wellbeing data with a little more depth

00:47:56,710 --> 00:48:01,870
but thank you all for your attention and

00:47:58,690 --> 00:48:04,570
happy to take questions super we have a

00:48:01,870 --> 00:48:06,970
stack of them that have piled up and I

00:48:04,570 --> 00:48:08,500
think they there's a nice sort of like

00:48:06,970 --> 00:48:11,170
digging in on the theme that I really

00:48:08,500 --> 00:48:12,850
liked my first question I'm going to

00:48:11,170 --> 00:48:14,470
take complete advantage of being with

00:48:12,850 --> 00:48:18,160
the person who's on zoom' who's doing

00:48:14,470 --> 00:48:20,280
this and one of things that was

00:48:18,160 --> 00:48:25,120
interesting was looking at those results

00:48:20,280 --> 00:48:27,940
like Grinberg a town unlike the the for

00:48:25,120 --> 00:48:29,860
work paper is the implication that we're

00:48:27,940 --> 00:48:31,930
going to see the impact of fake news age

00:48:29,860 --> 00:48:33,310
out of the population I mean the analogy

00:48:31,930 --> 00:48:35,770
that sort of in the back of my head is

00:48:33,310 --> 00:48:36,680
attitudes to homosexuality right where

00:48:35,770 --> 00:48:38,240
if you

00:48:36,680 --> 00:48:39,349
sort of the people who are against gay

00:48:38,240 --> 00:48:41,240
marriage and you look at as a

00:48:39,349 --> 00:48:45,079
statistical property of the population

00:48:41,240 --> 00:48:46,670
the and you look at the US Western

00:48:45,079 --> 00:48:48,680
democracies things like that you're

00:48:46,670 --> 00:48:50,300
generally seeing that that that's the

00:48:48,680 --> 00:48:53,030
thing that's been a generational change

00:48:50,300 --> 00:48:55,730
right are we going to see the impact of

00:48:53,030 --> 00:48:57,140
fake news H out of the population yeah

00:48:55,730 --> 00:49:02,869
it's a really good question being a

00:48:57,140 --> 00:49:05,240
short answer is yes we I mean we we have

00:49:02,869 --> 00:49:06,849
a very strong present ISM bias as humans

00:49:05,240 --> 00:49:09,290
it's very difficult for us to imagine

00:49:06,849 --> 00:49:11,329
what things were like in the past and we

00:49:09,290 --> 00:49:13,280
have a really hard time speculating

00:49:11,329 --> 00:49:14,809
imagine the future we tend to think that

00:49:13,280 --> 00:49:17,809
the way it is now is the way it always

00:49:14,809 --> 00:49:20,690
will be and so fake news feels like this

00:49:17,809 --> 00:49:22,910
like massive problem that we'll never be

00:49:20,690 --> 00:49:24,980
able to deal with now it's not to

00:49:22,910 --> 00:49:29,980
minimize the risks and the problems but

00:49:24,980 --> 00:49:32,359
yes over time we as humans adapt and we

00:49:29,980 --> 00:49:34,849
gain experience over time so those

00:49:32,359 --> 00:49:38,089
people that don't have experience will

00:49:34,849 --> 00:49:40,760
end up passing away those are that are

00:49:38,089 --> 00:49:42,680
you know in their 60s and 70s are about

00:49:40,760 --> 00:49:45,290
to get a whole bunch of help there are a

00:49:42,680 --> 00:49:47,559
number of organizations that are now

00:49:45,290 --> 00:49:51,140
trying to work with adult children and

00:49:47,559 --> 00:49:53,329
directly with with seniors on improving

00:49:51,140 --> 00:49:56,839
their their abilities and we're already

00:49:53,329 --> 00:49:58,910
starting to see improvement so yeah I'm

00:49:56,839 --> 00:50:00,410
quite optimistic we've this is not the

00:49:58,910 --> 00:50:04,430
first time societies had to deal with

00:50:00,410 --> 00:50:06,799
fake news and it won't be the last

00:50:04,430 --> 00:50:08,450
but I think that this way in which fake

00:50:06,799 --> 00:50:12,079
news has been operating and I'm thinking

00:50:08,450 --> 00:50:14,599
of the Russian PlayBook we will become

00:50:12,079 --> 00:50:16,579
savvy to and be able to through

00:50:14,599 --> 00:50:18,619
education and this sort of generational

00:50:16,579 --> 00:50:20,569
change deal with that's that's a

00:50:18,619 --> 00:50:23,750
delightfully optimistic note I'm very

00:50:20,569 --> 00:50:25,250
I'm kind of glad to hear that um one of

00:50:23,750 --> 00:50:28,940
the questions is from Jennifer Davidson

00:50:25,250 --> 00:50:30,319
who I think I was being in Portland but

00:50:28,940 --> 00:50:32,299
maybe she's not now I'm like like have

00:50:30,319 --> 00:50:35,089
that sudden moment of weights is my own

00:50:32,299 --> 00:50:36,950
fake news there are three ingredients

00:50:35,089 --> 00:50:37,790
that you mentioned which I I want to

00:50:36,950 --> 00:50:39,380
make sure I've got all three it's

00:50:37,790 --> 00:50:43,240
believability reliability and there's a

00:50:39,380 --> 00:50:46,790
third clubheads competence confidence

00:50:43,240 --> 00:50:49,730
okay so does that exchange extent of

00:50:46,790 --> 00:50:51,109
organizations or is that a person yeah

00:50:49,730 --> 00:50:53,300
and so for people that are interested in

00:50:51,109 --> 00:50:56,720
learning more about this I'd recommend

00:50:53,300 --> 00:51:00,500
Rachel Botsman book Lots when is B OTS

00:50:56,720 --> 00:51:03,560
MA and who can we trust and she does a

00:51:00,500 --> 00:51:06,020
really nice job of of summarizing an

00:51:03,560 --> 00:51:08,240
academic literature typically it's

00:51:06,020 --> 00:51:10,340
really complicated what you know signals

00:51:08,240 --> 00:51:12,170
something and academia and we can never

00:51:10,340 --> 00:51:15,350
agree but in this case there are these

00:51:12,170 --> 00:51:17,240
three signals competence honest or

00:51:15,350 --> 00:51:20,750
trustworthy appearing honest trustworthy

00:51:17,240 --> 00:51:23,300
warm and reliability the challenge of

00:51:20,750 --> 00:51:24,590
course is you know as your when your

00:51:23,300 --> 00:51:27,530
people before said it's highly

00:51:24,590 --> 00:51:29,270
contextual so I hope that I am you know

00:51:27,530 --> 00:51:30,619
trustworthy and appear confident and

00:51:29,270 --> 00:51:32,450
reliable when talking about this

00:51:30,619 --> 00:51:35,600
research but you wouldn't want me to be

00:51:32,450 --> 00:51:38,660
your dentist or to sing you a song both

00:51:35,600 --> 00:51:40,609
things I'd be terrible at so I think

00:51:38,660 --> 00:51:42,470
that's a key thing that has a lot of

00:51:40,609 --> 00:51:44,680
companies transitioning to this new era

00:51:42,470 --> 00:51:49,150
they need to learn how to signal

00:51:44,680 --> 00:51:51,380
confidence and honesty and reliability

00:51:49,150 --> 00:51:54,770
yeah I mean I think there is a problem

00:51:51,380 --> 00:51:56,710
that we run into in the a lot of

00:51:54,770 --> 00:51:59,359
questions about trust end up being

00:51:56,710 --> 00:52:01,300
uniaccess right do you trust Amazon and

00:51:59,359 --> 00:52:04,100
the problem is you get crossover between

00:52:01,300 --> 00:52:06,920
do my trust Amazon to deliver my USB

00:52:04,100 --> 00:52:08,570
cable when they say they will and do my

00:52:06,920 --> 00:52:10,490
trust them not to listen to me through

00:52:08,570 --> 00:52:14,630
the Alexa device when I don't want them

00:52:10,490 --> 00:52:16,180
to and the sense I get is that that

00:52:14,630 --> 00:52:18,080
humans not very good about

00:52:16,180 --> 00:52:20,150
disambiguating these sort of roles of

00:52:18,080 --> 00:52:23,619
trusting that if I trust Jeff and Jeff

00:52:20,150 --> 00:52:25,730
says hey looks like you're in your

00:52:23,619 --> 00:52:27,440
bicuspid there needs a bit of work so

00:52:25,730 --> 00:52:28,810
I'm like well I guess I trust him to do

00:52:27,440 --> 00:52:31,700
dental work as well is that the case

00:52:28,810 --> 00:52:34,460
yeah so there's a number of different

00:52:31,700 --> 00:52:35,630
kind of models about trust and you're

00:52:34,460 --> 00:52:37,340
right so one is this uni-dimensional

00:52:35,630 --> 00:52:40,190
model it's the simplest one I either

00:52:37,340 --> 00:52:41,869
trust or distrust you some level another

00:52:40,190 --> 00:52:45,290
one is by dimensional model so I can

00:52:41,869 --> 00:52:46,970
trust an entity like Amazon high on some

00:52:45,290 --> 00:52:49,190
dimensions and I can distrust them on

00:52:46,970 --> 00:52:50,990
others and those are independent so

00:52:49,190 --> 00:52:53,180
exactly like you said I trust them to

00:52:50,990 --> 00:52:55,550
deliver that on this time I may be

00:52:53,180 --> 00:52:59,300
distrust them about like the operation

00:52:55,550 --> 00:53:02,030
of taxi there are operations on my tax

00:52:59,300 --> 00:53:03,380
base in my county or on whether they're

00:53:02,030 --> 00:53:06,140
going to listen to me so we

00:53:03,380 --> 00:53:08,480
to have a multi-dimensional take on

00:53:06,140 --> 00:53:11,839
individuals and then there's this third

00:53:08,480 --> 00:53:14,630
model which is relational and you see

00:53:11,839 --> 00:53:17,390
the sort of heavy reliance on kind of

00:53:14,630 --> 00:53:19,369
cognitive trust so I'm assessing them

00:53:17,390 --> 00:53:20,420
and does this seem right can I trust

00:53:19,369 --> 00:53:22,400
what they say and then there's

00:53:20,420 --> 00:53:24,049
behavioral and they do what they said

00:53:22,400 --> 00:53:26,150
they were going to do and then there's

00:53:24,049 --> 00:53:27,920
this last one that kicks in over time

00:53:26,150 --> 00:53:30,680
which is relational that is I don't have

00:53:27,920 --> 00:53:32,720
to think or observe joe fish's behavior

00:53:30,680 --> 00:53:35,509
anymore I've known him for a long time

00:53:32,720 --> 00:53:38,839
we have established relational trust and

00:53:35,509 --> 00:53:40,640
this happens with brands as well and and

00:53:38,839 --> 00:53:41,690
we all have that feeling of like okay I

00:53:40,640 --> 00:53:42,859
don't have to worry about it it's this

00:53:41,690 --> 00:53:46,910
brand that I've been using for a long

00:53:42,859 --> 00:53:48,980
time violations occur they can transform

00:53:46,910 --> 00:53:50,720
you know in the same way that a

00:53:48,980 --> 00:53:54,740
violation of marriage transforms that

00:53:50,720 --> 00:53:56,059
marriage it can transform relationships

00:53:54,740 --> 00:53:57,619
with brands as well it doesn't mean that

00:53:56,059 --> 00:54:00,920
it's over I think people tend to

00:53:57,619 --> 00:54:04,279
overestimate how damaging violations are

00:54:00,920 --> 00:54:06,049
they they obviously are really harmful

00:54:04,279 --> 00:54:07,130
but they they change the nature of the

00:54:06,049 --> 00:54:08,720
relationship rather than necessarily

00:54:07,130 --> 00:54:11,690
ending it and so that's another

00:54:08,720 --> 00:54:13,250
important thing to think about um so

00:54:11,690 --> 00:54:15,500
this is a question which is a lovely

00:54:13,250 --> 00:54:19,069
follow-up to that from Alice 3 who is in

00:54:15,500 --> 00:54:20,599
South Carolina do you have examples of

00:54:19,069 --> 00:54:21,829
other companies particularly tech

00:54:20,599 --> 00:54:24,470
companies who are doing particularly

00:54:21,829 --> 00:54:25,900
well around signaling trust and why do

00:54:24,470 --> 00:54:29,359
you think they're doing such a good job

00:54:25,900 --> 00:54:32,180
yeah I'll think - one American and one

00:54:29,359 --> 00:54:37,269
European and you know there's a third

00:54:32,180 --> 00:54:40,940
Alibaba it goes back to that embedded

00:54:37,269 --> 00:54:43,759
trust so taking all of the advantages of

00:54:40,940 --> 00:54:47,059
network trust and then embedding that

00:54:43,759 --> 00:54:51,319
inside what I'll call legacy trust

00:54:47,059 --> 00:54:54,740
systems like contract law like policing

00:54:51,319 --> 00:54:57,019
like banking and payments like insurance

00:54:54,740 --> 00:55:00,410
and I think what they do is they use

00:54:57,019 --> 00:55:03,380
their brand to say you're gonna have

00:55:00,410 --> 00:55:05,900
this interaction with another person and

00:55:03,380 --> 00:55:07,460
you're gonna really operate that you and

00:55:05,900 --> 00:55:11,539
these other people so you have this sort

00:55:07,460 --> 00:55:14,269
of network form of trust but we got you

00:55:11,539 --> 00:55:16,730
so if anything goes wrong in there we're

00:55:14,269 --> 00:55:17,090
going to this brand it's either err

00:55:16,730 --> 00:55:20,690
being

00:55:17,090 --> 00:55:23,090
be blahblah corner or Alibaba they're

00:55:20,690 --> 00:55:26,750
using their brand to say we're going to

00:55:23,090 --> 00:55:30,890
bring to bear these legacy forms of

00:55:26,750 --> 00:55:33,080
trust so in a way when I get in a car to

00:55:30,890 --> 00:55:36,590
do a long distance ride from Paris to

00:55:33,080 --> 00:55:39,160
nice in blah blah car I don't have to

00:55:36,590 --> 00:55:41,420
worry about is how do I pay this guy

00:55:39,160 --> 00:55:43,730
what if this guy turns out to be really

00:55:41,420 --> 00:55:47,330
like you know he speeds a lot of Acts

00:55:43,730 --> 00:55:50,180
criminally like I've got blah blah car

00:55:47,330 --> 00:55:52,240
the brand to trust because they're

00:55:50,180 --> 00:55:55,040
bringing all these other forms of

00:55:52,240 --> 00:55:56,510
institutional trust to bear I think

00:55:55,040 --> 00:56:00,230
that's what a lot of these companies

00:55:56,510 --> 00:56:01,730
that are successful are doing well when

00:56:00,230 --> 00:56:03,920
you have things like Facebook which is

00:56:01,730 --> 00:56:05,900
struggle with trust it's it's because

00:56:03,920 --> 00:56:07,370
they're managing you know interpersonal

00:56:05,900 --> 00:56:10,640
relationships as well and when people

00:56:07,370 --> 00:56:12,380
act poorly they're doing it on the

00:56:10,640 --> 00:56:14,900
Facebook platform and then Facebook

00:56:12,380 --> 00:56:17,900
takes in that absorbs some of the blame

00:56:14,900 --> 00:56:19,580
for the way those people behave where I

00:56:17,900 --> 00:56:21,350
think these other ones that are engaging

00:56:19,580 --> 00:56:23,920
in sort of transactional things between

00:56:21,350 --> 00:56:29,900
people can draw more clearly on these

00:56:23,920 --> 00:56:31,610
institutional forms of trust um one

00:56:29,900 --> 00:56:35,180
question that we have and this is from

00:56:31,610 --> 00:56:36,590
Emma Humphreys who is works out of the

00:56:35,180 --> 00:56:38,090
Mountain View office when she works out

00:56:36,590 --> 00:56:41,560
of the office but of course that's not

00:56:38,090 --> 00:56:45,290
now I believe maybe in Campbell but or

00:56:41,560 --> 00:56:47,240
Fremont I have you know anyway local and

00:56:45,290 --> 00:56:49,100
she asks I'm curious about communities

00:56:47,240 --> 00:56:51,320
who are used to deception by groups with

00:56:49,100 --> 00:56:53,810
power over them is there an opposite to

00:56:51,320 --> 00:56:55,310
the veracity effect no I like that when

00:56:53,810 --> 00:56:57,260
I was thinking about the work that Susan

00:56:55,310 --> 00:56:59,660
Silby did looking at how people think

00:56:57,260 --> 00:57:01,400
about the commonplace of law I think is

00:56:59,660 --> 00:57:02,420
the title of the book in which she

00:57:01,400 --> 00:57:05,060
talked about people have been

00:57:02,420 --> 00:57:07,310
disenfranchised and how their approach

00:57:05,060 --> 00:57:09,620
to understanding law was very very

00:57:07,310 --> 00:57:11,480
different to people who were sort of in

00:57:09,620 --> 00:57:15,500
a mainstream majority kind of assumption

00:57:11,480 --> 00:57:17,240
so differences in those populations yeah

00:57:15,500 --> 00:57:19,460
so I think we can think about this in

00:57:17,240 --> 00:57:23,720
two very different ways one is the sort

00:57:19,460 --> 00:57:27,260
of corruption approach Brazil being a

00:57:23,720 --> 00:57:29,630
really important example where you know

00:57:27,260 --> 00:57:30,950
corruption has run rampant in Brazil for

00:57:29,630 --> 00:57:33,530
four decades and it's

00:57:30,950 --> 00:57:36,619
really undermined people's trust in

00:57:33,530 --> 00:57:38,570
government and in the economy in all the

00:57:36,619 --> 00:57:39,950
infrastructure that's there and they're

00:57:38,570 --> 00:57:41,780
working really hard on rooting it out

00:57:39,950 --> 00:57:44,480
but it comes at this massive cost

00:57:41,780 --> 00:57:46,849
because people will not listen to the

00:57:44,480 --> 00:57:49,970
government they won't take direction

00:57:46,849 --> 00:57:52,040
from them they end up engaging behaviors

00:57:49,970 --> 00:57:55,849
that have to prioritize themselves and

00:57:52,040 --> 00:57:57,740
their families over larger goods and so

00:57:55,849 --> 00:58:00,530
right they have to protect themselves

00:57:57,740 --> 00:58:04,460
from corrupt and powerful institutions

00:58:00,530 --> 00:58:07,820
so that's one sort of like massive

00:58:04,460 --> 00:58:09,470
blinking warning sign about corruption I

00:58:07,820 --> 00:58:12,440
think one reason that even when

00:58:09,470 --> 00:58:15,890
everything's going Alda pot that the

00:58:12,440 --> 00:58:18,170
u.s. is you know people purchase US

00:58:15,890 --> 00:58:22,250
dollars is that there's low corruption

00:58:18,170 --> 00:58:25,160
in the US another way to tackle that is

00:58:22,250 --> 00:58:29,410
thinking about totalitarianism and and

00:58:25,160 --> 00:58:33,790
here I think another book I recommend is

00:58:29,410 --> 00:58:36,970
Hannah Arendt book lying in politics

00:58:33,790 --> 00:58:41,900
she's German and thinking about

00:58:36,970 --> 00:58:43,369
totalitarianism in the 40s and 50s which

00:58:41,900 --> 00:58:46,040
he observes is something even more

00:58:43,369 --> 00:58:50,410
worrisome in some ways for a person

00:58:46,040 --> 00:58:53,390
deception person like me she finds that

00:58:50,410 --> 00:58:56,089
totalitarian leaders will will lie to

00:58:53,390 --> 00:59:00,740
their population and the problem is that

00:58:56,089 --> 00:59:02,900
their supporters will then be okay with

00:59:00,740 --> 00:59:06,050
being lied to because they see it as

00:59:02,900 --> 00:59:08,900
their leaders using it as a weapon

00:59:06,050 --> 00:59:10,579
attacked it to fight the enemy and the

00:59:08,900 --> 00:59:12,280
enemy is whether whatever other

00:59:10,579 --> 00:59:16,849
political party there is in the country

00:59:12,280 --> 00:59:20,569
so you know I don't want to make this

00:59:16,849 --> 00:59:23,060
political at all but when you have when

00:59:20,569 --> 00:59:26,060
you have leaders that are willing to

00:59:23,060 --> 00:59:27,800
live frequently and then a part of a

00:59:26,060 --> 00:59:29,630
population that's willing to forgive

00:59:27,800 --> 00:59:33,710
those lies because they're viewed as

00:59:29,630 --> 00:59:37,940
part of a war then you have really hard

00:59:33,710 --> 00:59:40,790
time holding the leaders accountable and

00:59:37,940 --> 00:59:42,589
I think Hannah is penetrance analysis of

00:59:40,790 --> 00:59:44,690
this is the most cogent and the most

00:59:42,589 --> 00:59:47,900
sort of upsetting and disturbing

00:59:44,690 --> 00:59:50,329
and why I think we have to hold leaders

00:59:47,900 --> 00:59:52,700
accountable when they engage in line and

00:59:50,329 --> 00:59:54,589
we can you can't like let that go I

00:59:52,700 --> 00:59:57,319
don't think and that's that's a huge

00:59:54,589 --> 01:00:03,260
that's a challenge for us that's a huge

00:59:57,319 --> 01:00:04,490
challenge yes and yes I have one more

01:00:03,260 --> 01:00:05,839
than I'm gonna squeeze and I think we're

01:00:04,490 --> 01:00:07,579
over time but it's such an interesting

01:00:05,839 --> 01:00:10,970
thread of questions that I want to I

01:00:07,579 --> 01:00:14,300
want to continue and and it's all

01:00:10,970 --> 01:00:16,460
virtual anyway and that question is from

01:00:14,300 --> 01:00:16,910
Miriam Avery and she's in San Jose as

01:00:16,460 --> 01:00:20,300
well

01:00:16,910 --> 01:00:22,069
how does skepticism that overestimation

01:00:20,300 --> 01:00:24,050
of falsehood interact with well being

01:00:22,069 --> 01:00:25,910
and I think that goes to the sort of the

01:00:24,050 --> 01:00:27,079
mystery portion of your of your talk

01:00:25,910 --> 01:00:29,660
that we didn't have a chance to get to

01:00:27,079 --> 01:00:33,109
yeah I love that question and I have a

01:00:29,660 --> 01:00:35,000
good analogy I have a really dear friend

01:00:33,109 --> 01:00:38,030
Jeanie Zakaria he's a world-class

01:00:35,000 --> 01:00:40,730
journalist amazing teachers here at

01:00:38,030 --> 01:00:42,950
Stanford and she is just constantly

01:00:40,730 --> 01:00:45,109
skeptical if everything and as a result

01:00:42,950 --> 01:00:47,690
quite worried about everything and she

01:00:45,109 --> 01:00:48,319
and I debate a lot because as an

01:00:47,690 --> 01:00:52,520
optimist

01:00:48,319 --> 01:00:55,790
I don't and I think what happens is most

01:00:52,520 --> 01:00:57,890
of us are not that skeptical we have the

01:00:55,790 --> 01:01:01,210
truth bias in place and most of the time

01:00:57,890 --> 01:01:04,010
it works great and what I do with

01:01:01,210 --> 01:01:07,750
respect to Jeanine as an example as I'm

01:01:04,010 --> 01:01:10,700
a free rider so I free ride on Janine

01:01:07,750 --> 01:01:12,940
skepticism and she pays a heavy price

01:01:10,700 --> 01:01:16,099
which is being worried and anxious

01:01:12,940 --> 01:01:17,660
because she's not gonna trust that you

01:01:16,099 --> 01:01:20,599
are gonna do what you say she's gonna

01:01:17,660 --> 01:01:22,970
like investigate that using you know the

01:01:20,599 --> 01:01:27,170
best practices and tools of journalism

01:01:22,970 --> 01:01:29,180
and so I think for most people we have a

01:01:27,170 --> 01:01:30,950
good truth bias we're reasonably

01:01:29,180 --> 01:01:32,690
suspicious when we should be you know

01:01:30,950 --> 01:01:34,160
you know you're in that dark alley a guy

01:01:32,690 --> 01:01:37,369
tries to sell you a Rolex you're gonna

01:01:34,160 --> 01:01:39,500
be suspicious but there's other people a

01:01:37,369 --> 01:01:42,079
smaller percentage of people amongst us

01:01:39,500 --> 01:01:45,050
that are much more skeptical and and

01:01:42,079 --> 01:01:48,740
help they save us by by ferreting out

01:01:45,050 --> 01:01:50,960
the lies and the scams achieve and they

01:01:48,740 --> 01:01:53,960
do happen and they pay a price they are

01:01:50,960 --> 01:01:56,450
typically more anxious and and worried

01:01:53,960 --> 01:01:58,099
and that people that have the strong

01:01:56,450 --> 01:01:59,180
truth bias so that's how I see it

01:01:58,099 --> 01:02:02,910
operate

01:01:59,180 --> 01:02:05,579
most people can can turn on that

01:02:02,910 --> 01:02:07,349
suspicion when when it's needed and then

01:02:05,579 --> 01:02:09,900
turn it off when they're interacting now

01:02:07,349 --> 01:02:13,380
with family and friends but some amongst

01:02:09,900 --> 01:02:16,319
us you know they pay that price and if

01:02:13,380 --> 01:02:18,630
the truth bias is reduced overall as I'm

01:02:16,319 --> 01:02:21,269
a little bit worried is happening right

01:02:18,630 --> 01:02:26,660
now then more of us will be feeling the

01:02:21,269 --> 01:02:29,670
anxiety that comes with that there is a

01:02:26,660 --> 01:02:31,769
series of threads of people questioning

01:02:29,670 --> 01:02:33,599
the degree to which like this

01:02:31,769 --> 01:02:36,089
particularly impacts news right in the

01:02:33,599 --> 01:02:38,190
degree to which this is the tension

01:02:36,089 --> 01:02:41,819
between sort of objective fact and spin

01:02:38,190 --> 01:02:44,809
yeah and I think I definitely see this

01:02:41,819 --> 01:02:47,069
when I look at the studies of

01:02:44,809 --> 01:02:50,839
particularly miss and disinformation and

01:02:47,069 --> 01:02:53,700
this question of whether there are

01:02:50,839 --> 01:02:54,719
biases there I'm always a little

01:02:53,700 --> 01:02:56,700
suspicious because I'm not sure I

01:02:54,719 --> 01:02:59,329
believe in that objective truth and yet

01:02:56,700 --> 01:03:06,440
you have that tension around that um I

01:02:59,329 --> 01:03:08,569
think I think we

01:03:06,440 --> 01:03:10,760
I think we're going to close this up now

01:03:08,569 --> 01:03:12,109
because if we did reach the hour and we

01:03:10,760 --> 01:03:15,470
went over because it was getting really

01:03:12,109 --> 01:03:17,089
interesting but Jeff on behalf of

01:03:15,470 --> 01:03:18,980
Mozilla and the people watching around

01:03:17,089 --> 01:03:20,930
world thank you very much your time I

01:03:18,980 --> 01:03:22,430
really appreciate it it's been superb

01:03:20,930 --> 01:03:24,020
may you go first you appreciate

01:03:22,430 --> 01:03:26,210
everybody's attention and all the great

01:03:24,020 --> 01:03:29,560
questions thank you have a good day

01:03:26,210 --> 01:03:29,560

YouTube URL: https://www.youtube.com/watch?v=KLWCZNuopco


