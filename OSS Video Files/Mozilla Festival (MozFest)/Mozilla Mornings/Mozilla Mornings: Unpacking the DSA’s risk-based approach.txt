Title: Mozilla Mornings: Unpacking the DSA’s risk-based approach
Publication date: 2021-05-24
Playlist: Mozilla Mornings
Description: 
	This installment of Mozilla Mornings focused on the DSA’s risk-based approach, specifically the draft law’s provisions on risk assessment, risk mitigation, and auditing for very large online platforms. We’ll be looking at what these provisions seek to solve for; how they’re likely to work in practice; and what we can learn from related proposals in other jurisdictions.

Speakers

Carly Kind
Director, Ada Lovelace Institute

Ben Scott
Executive Director, Reset

Owen Bennett
Senior Policy Manager, Mozilla Corporation

Moderated by Brian Maguire
EU journalist and broadcaster
Captions: 
	00:00:00,400 --> 00:00:02,890
[Music]

00:00:04,160 --> 00:00:06,799
good morning and welcome to the latest

00:00:06,000 --> 00:00:08,240
edition of

00:00:06,799 --> 00:00:10,320
mozilla mornings this is mozilla

00:00:08,240 --> 00:00:12,160
morning's light it's every bit as good

00:00:10,320 --> 00:00:14,000
as the other content just no pastries

00:00:12,160 --> 00:00:15,599
and no coffee that we're providing so

00:00:14,000 --> 00:00:17,119
thanks for joining us we appreciate

00:00:15,599 --> 00:00:18,880
the time you've taken i'm brian mcguire

00:00:17,119 --> 00:00:21,039
i'm a journalist based here in brussels

00:00:18,880 --> 00:00:22,480
i'll lead the session this morning and

00:00:21,039 --> 00:00:23,439
today our focus is on the digital

00:00:22,480 --> 00:00:25,840
services act

00:00:23,439 --> 00:00:27,359
a risk-based approach uh specifically

00:00:25,840 --> 00:00:28,240
the draft laws provision on risk

00:00:27,359 --> 00:00:30,640
assessment

00:00:28,240 --> 00:00:32,880
risk mitigation and auditing for very

00:00:30,640 --> 00:00:34,960
large online platforms we'll be looking

00:00:32,880 --> 00:00:35,920
at what these provisions seek to do how

00:00:34,960 --> 00:00:38,320
they might work

00:00:35,920 --> 00:00:40,640
in real life and what we can learn from

00:00:38,320 --> 00:00:41,200
related proposals in other jurisdictions

00:00:40,640 --> 00:00:42,640
as well

00:00:41,200 --> 00:00:44,399
and just to say to our audience before i

00:00:42,640 --> 00:00:46,320
introduce the panel is that we won't

00:00:44,399 --> 00:00:48,399
really want your participation today

00:00:46,320 --> 00:00:50,559
and you can send questions uh and

00:00:48,399 --> 00:00:53,360
comments as well uh for our panel

00:00:50,559 --> 00:00:54,800
and the q a uh section there our team

00:00:53,360 --> 00:00:55,840
will highlight some of those for me and

00:00:54,800 --> 00:00:57,039
to bring through we'll try and get

00:00:55,840 --> 00:00:58,960
through as many of those

00:00:57,039 --> 00:01:00,559
as possible please tell us who you are

00:00:58,960 --> 00:01:02,640
and where you're from if you don't mind

00:01:00,559 --> 00:01:03,760
and and if you have a specific question

00:01:02,640 --> 00:01:05,760
for any of the panelists

00:01:03,760 --> 00:01:06,799
uh please identify that in your comment

00:01:05,760 --> 00:01:09,119
or question

00:01:06,799 --> 00:01:11,119
as well so our speakers today uh carly

00:01:09,119 --> 00:01:12,640
kind she's the director at a lovelace

00:01:11,119 --> 00:01:14,479
institute good morning carly

00:01:12,640 --> 00:01:16,479
and carly is a human rights lawyer and

00:01:14,479 --> 00:01:17,759
leading authority on the intersection of

00:01:16,479 --> 00:01:19,680
technology policy

00:01:17,759 --> 00:01:20,960
and human rights we also have ben scott

00:01:19,680 --> 00:01:23,280
he's the executive director

00:01:20,960 --> 00:01:25,119
at reset an initiative focused on

00:01:23,280 --> 00:01:26,240
tackling digital threats to democracy

00:01:25,119 --> 00:01:28,560
growth industry

00:01:26,240 --> 00:01:30,560
and uh owen bennett he's a senior policy

00:01:28,560 --> 00:01:32,720
manager at the mozilla corporation

00:01:30,560 --> 00:01:34,640
an expert in policy and laws surrounding

00:01:32,720 --> 00:01:37,119
european content responsibility

00:01:34,640 --> 00:01:37,920
and platform regulations so good morning

00:01:37,119 --> 00:01:40,560
and welcome

00:01:37,920 --> 00:01:42,640
to all of you and just uh before we get

00:01:40,560 --> 00:01:44,560
into the discussion i'll ask our panel

00:01:42,640 --> 00:01:46,479
just to give us a 60-second

00:01:44,560 --> 00:01:48,560
elevator pitch their key message or

00:01:46,479 --> 00:01:50,560
their headline for today carly can we

00:01:48,560 --> 00:01:53,840
kick off with you

00:01:50,560 --> 00:01:55,280
great question um i my first headline to

00:01:53,840 --> 00:01:56,799
the audience is that i'm no expert in

00:01:55,280 --> 00:01:58,079
the detail of the dsa so please don't

00:01:56,799 --> 00:01:59,600
ask me any difficult questions

00:01:58,079 --> 00:02:01,280
i think the work that ada lovelace

00:01:59,600 --> 00:02:01,920
institute has been doing is really more

00:02:01,280 --> 00:02:04,560
around

00:02:01,920 --> 00:02:06,000
audit as a mechanism for ensuring ethics

00:02:04,560 --> 00:02:08,479
and accountability in the space

00:02:06,000 --> 00:02:10,319
of content moderation and algorithms

00:02:08,479 --> 00:02:12,080
more broadly and so we're looking at

00:02:10,319 --> 00:02:14,160
what does the emerging field of auditing

00:02:12,080 --> 00:02:16,080
look like what types of technical

00:02:14,160 --> 00:02:17,920
capabilities do we need to make it

00:02:16,080 --> 00:02:19,680
possible uh what type of legal and

00:02:17,920 --> 00:02:20,400
normative rules do we need to make it

00:02:19,680 --> 00:02:22,480
effective

00:02:20,400 --> 00:02:23,599
so i suppose my major message will be

00:02:22,480 --> 00:02:24,239
we're in the midst of a really

00:02:23,599 --> 00:02:26,000
interesting

00:02:24,239 --> 00:02:27,760
development around audit there's a field

00:02:26,000 --> 00:02:30,160
emerging a set of practices

00:02:27,760 --> 00:02:32,560
emerging and and uh the dsa will be

00:02:30,160 --> 00:02:37,040
really key in shaping that

00:02:32,560 --> 00:02:40,239
thank you ben good morning good morning

00:02:37,040 --> 00:02:43,680
um i'm going to step up one level

00:02:40,239 --> 00:02:46,640
and say you know my headline message is

00:02:43,680 --> 00:02:48,000
that i've spent the last couple of years

00:02:46,640 --> 00:02:48,800
running around the world talking to

00:02:48,000 --> 00:02:52,319
governments

00:02:48,800 --> 00:02:54,400
uh that are focused on different aspects

00:02:52,319 --> 00:02:55,920
of digital threats to democracy

00:02:54,400 --> 00:02:57,920
who are developing different kinds of

00:02:55,920 --> 00:03:00,720
policy solutions to

00:02:57,920 --> 00:03:01,120
illegal content to recommender systems

00:03:00,720 --> 00:03:03,200
that

00:03:01,120 --> 00:03:05,760
lend themselves to polarization

00:03:03,200 --> 00:03:09,360
radicalization and extreme views

00:03:05,760 --> 00:03:12,000
that are looking to uncover ways to

00:03:09,360 --> 00:03:12,959
mitigate these harms the dsa is the

00:03:12,000 --> 00:03:16,720
first

00:03:12,959 --> 00:03:20,640
major statute that understood audit as

00:03:16,720 --> 00:03:23,599
a an essential component that without

00:03:20,640 --> 00:03:24,400
audit capability without the the

00:03:23,599 --> 00:03:26,879
capacity

00:03:24,400 --> 00:03:27,760
in the regulatory authority to open the

00:03:26,879 --> 00:03:31,440
black box

00:03:27,760 --> 00:03:34,560
of how automated systems

00:03:31,440 --> 00:03:35,840
curate organic and paid content on these

00:03:34,560 --> 00:03:37,680
platforms

00:03:35,840 --> 00:03:40,400
you cannot really regulate them

00:03:37,680 --> 00:03:41,920
effectively nor can you verify

00:03:40,400 --> 00:03:43,760
whether your regulations are being

00:03:41,920 --> 00:03:46,720
complied with so

00:03:43,760 --> 00:03:48,000
to me the dsa is groundbreaking for that

00:03:46,720 --> 00:03:49,440
reason alone

00:03:48,000 --> 00:03:51,040
although there are many other reasons

00:03:49,440 --> 00:03:53,120
why i think the dsa

00:03:51,040 --> 00:03:54,720
is an important piece of policy this

00:03:53,120 --> 00:03:57,280
century centre

00:03:54,720 --> 00:03:59,200
the centrality of the audit provision in

00:03:57,280 --> 00:04:01,280
the dsa in its clear

00:03:59,200 --> 00:04:03,280
linkage to the other regulatory

00:04:01,280 --> 00:04:04,560
provisions that are established in the

00:04:03,280 --> 00:04:06,640
in the proposed law

00:04:04,560 --> 00:04:08,159
really signals to me that its drafters

00:04:06,640 --> 00:04:10,319
have understood

00:04:08,159 --> 00:04:12,080
an essential problem with platform

00:04:10,319 --> 00:04:13,120
regulation in general and hopefully that

00:04:12,080 --> 00:04:16,160
will become

00:04:13,120 --> 00:04:17,440
a a pattern that others will follow

00:04:16,160 --> 00:04:19,199
excellent thank you good morning

00:04:17,440 --> 00:04:21,600
everyone

00:04:19,199 --> 00:04:22,320
good morning um so i'm going to go one

00:04:21,600 --> 00:04:25,120
level

00:04:22,320 --> 00:04:26,160
above again in terms of higher order um

00:04:25,120 --> 00:04:28,400
and i think that

00:04:26,160 --> 00:04:29,919
as an opening gambit i think that we can

00:04:28,400 --> 00:04:32,160
we can be quite happy with where we are

00:04:29,919 --> 00:04:33,040
in terms of the dsa draft law like i

00:04:32,160 --> 00:04:34,720
recall

00:04:33,040 --> 00:04:36,320
being on one of these panels around this

00:04:34,720 --> 00:04:39,040
time last year and then

00:04:36,320 --> 00:04:40,960
um saying that the dsa was going to be

00:04:39,040 --> 00:04:43,120
the commission's last chance saloon it

00:04:40,960 --> 00:04:45,600
was like the last chance that they had

00:04:43,120 --> 00:04:46,880
to really get content regulation right

00:04:45,600 --> 00:04:48,720
because there wasn't going to be another

00:04:46,880 --> 00:04:49,360
opportunity for probably several more

00:04:48,720 --> 00:04:51,360
years

00:04:49,360 --> 00:04:52,720
and if you were told me then that fast

00:04:51,360 --> 00:04:53,919
forward a year we'll be having this

00:04:52,720 --> 00:04:55,680
conversation about

00:04:53,919 --> 00:04:57,680
the merits and the opportunities of the

00:04:55,680 --> 00:04:59,680
dsa's risk-based approach

00:04:57,680 --> 00:05:01,120
i would have been quite happy um and so

00:04:59,680 --> 00:05:02,800
i think that what as a high-level

00:05:01,120 --> 00:05:03,360
message i think from a mozilla point of

00:05:02,800 --> 00:05:04,800
view

00:05:03,360 --> 00:05:06,800
we're really happy with the kind of the

00:05:04,800 --> 00:05:08,000
policy contours of this proposal like

00:05:06,800 --> 00:05:09,120
where it seeks to go

00:05:08,000 --> 00:05:10,639
and obviously there's a lot of things

00:05:09,120 --> 00:05:11,680
that need to be ironed out need to be

00:05:10,639 --> 00:05:13,680
clarified and

00:05:11,680 --> 00:05:15,680
hopefully conversations like this are a

00:05:13,680 --> 00:05:17,120
way of kind of moving in that direction

00:05:15,680 --> 00:05:18,160
getting towards a really tight and

00:05:17,120 --> 00:05:21,600
workable

00:05:18,160 --> 00:05:22,880
legal regime excellent thank you uh it's

00:05:21,600 --> 00:05:24,639
unusual i have to say

00:05:22,880 --> 00:05:26,000
as a journalist and moderate to hear

00:05:24,639 --> 00:05:27,520
such high praise for the commission on a

00:05:26,000 --> 00:05:30,639
policy document so early

00:05:27,520 --> 00:05:32,800
in its uh life uh but uh let's give ben

00:05:30,639 --> 00:05:34,000
i just to set some of the the territory

00:05:32,800 --> 00:05:35,199
here and you've already touched on some

00:05:34,000 --> 00:05:37,199
of this already you know what kind of

00:05:35,199 --> 00:05:39,600
risks are we trying to address

00:05:37,199 --> 00:05:42,320
with the dsa and and do the definitions

00:05:39,600 --> 00:05:42,320
go far enough

00:05:42,720 --> 00:05:47,360
i think that's that's the question

00:05:46,080 --> 00:05:50,960
everyone's wrestling with

00:05:47,360 --> 00:05:51,520
is you can establish all the powers in

00:05:50,960 --> 00:05:54,080
the world

00:05:51,520 --> 00:05:56,000
and if those powers only apply to

00:05:54,080 --> 00:05:57,199
specific aspects of the product or

00:05:56,000 --> 00:05:58,160
service you're trying to regulate and

00:05:57,199 --> 00:06:00,479
you miss the target with your

00:05:58,160 --> 00:06:02,960
definitions it's all for nothing

00:06:00,479 --> 00:06:05,120
and so i i think there's a lot of

00:06:02,960 --> 00:06:07,120
scrutiny of

00:06:05,120 --> 00:06:09,600
spots in the proposed law particularly

00:06:07,120 --> 00:06:12,720
article 26 which lays out

00:06:09,600 --> 00:06:14,720
the kinds of harms that um the law

00:06:12,720 --> 00:06:16,080
contemplates being subject to this risk

00:06:14,720 --> 00:06:17,360
mitigation approach

00:06:16,080 --> 00:06:19,440
and to the audits that we all

00:06:17,360 --> 00:06:22,639
highlighted at the top of the call

00:06:19,440 --> 00:06:24,000
um can i just interrupt just a second

00:06:22,639 --> 00:06:25,520
because i know half our audience is

00:06:24,000 --> 00:06:26,000
probably familiar with this and someone

00:06:25,520 --> 00:06:28,240
so just

00:06:26,000 --> 00:06:29,840
article 26 it deals with the

00:06:28,240 --> 00:06:31,280
dissemination of illegal content

00:06:29,840 --> 00:06:32,400
negative effects on the exercises

00:06:31,280 --> 00:06:35,440
fundamental rights

00:06:32,400 --> 00:06:37,280
and intentional manipulation um of the

00:06:35,440 --> 00:06:40,080
the platform services as well

00:06:37,280 --> 00:06:41,520
and uh the responses those platforms and

00:06:40,080 --> 00:06:43,360
services will have to

00:06:41,520 --> 00:06:45,039
to make uh to deal with dissemination

00:06:43,360 --> 00:06:48,800
with legal content just

00:06:45,039 --> 00:06:52,400
in a quick scope

00:06:48,800 --> 00:06:54,000
uh remedial i i i will confess that i

00:06:52,400 --> 00:06:54,750
brought it up on my screen and reread it

00:06:54,000 --> 00:06:56,960
before we started

00:06:54,750 --> 00:07:00,000
[Laughter]

00:06:56,960 --> 00:07:03,120
um but that to me is sort of the fulcrum

00:07:00,000 --> 00:07:05,039
of this proposed law and

00:07:03,120 --> 00:07:06,560
and the reason why it draws my attention

00:07:05,039 --> 00:07:08,160
is because

00:07:06,560 --> 00:07:09,919
it is the way that the european

00:07:08,160 --> 00:07:11,680
commission has attempted to address a

00:07:09,919 --> 00:07:12,479
problem that is vexing regulators all

00:07:11,680 --> 00:07:15,280
over the world

00:07:12,479 --> 00:07:16,800
particularly in democratic societies who

00:07:15,280 --> 00:07:17,440
want to get at the problem of the

00:07:16,800 --> 00:07:19,440
platform

00:07:17,440 --> 00:07:20,880
without regulating speech that is not

00:07:19,440 --> 00:07:23,039
clearly illegal

00:07:20,880 --> 00:07:24,880
that's that's the real problem with the

00:07:23,039 --> 00:07:26,880
platforms is that

00:07:24,880 --> 00:07:28,400
there's one class of content captured in

00:07:26,880 --> 00:07:28,720
piece one of the definition you just

00:07:28,400 --> 00:07:31,199
read

00:07:28,720 --> 00:07:32,880
illegal content in in every country

00:07:31,199 --> 00:07:35,280
there are

00:07:32,880 --> 00:07:36,560
definitions in law of what constitutes

00:07:35,280 --> 00:07:38,080
illegal speech

00:07:36,560 --> 00:07:39,520
and there's no reason why the illegal

00:07:38,080 --> 00:07:41,840
speech online should be treated

00:07:39,520 --> 00:07:44,080
differently than illegal speech offline

00:07:41,840 --> 00:07:45,520
there's not a lot of controversy about

00:07:44,080 --> 00:07:46,560
whether that should be taken down from

00:07:45,520 --> 00:07:49,360
the platforms

00:07:46,560 --> 00:07:52,960
the controversy is over the how and the

00:07:49,360 --> 00:07:52,960
transparency with which it is done

00:07:53,039 --> 00:07:58,160
however that illegal problem in my view

00:07:55,919 --> 00:07:59,520
gets way more attention than it deserves

00:07:58,160 --> 00:08:01,759
because the vast majority of the

00:07:59,520 --> 00:08:03,360
problems that we all think about when we

00:08:01,759 --> 00:08:04,319
think about the problem of platforms and

00:08:03,360 --> 00:08:07,360
democracy

00:08:04,319 --> 00:08:09,120
is with not clearly illegal content

00:08:07,360 --> 00:08:10,960
manifestly illegal content i think is

00:08:09,120 --> 00:08:12,879
the term that the commission uses

00:08:10,960 --> 00:08:14,240
but with content that is not manifestly

00:08:12,879 --> 00:08:17,280
illegal

00:08:14,240 --> 00:08:19,759
but nonetheless when amplified by

00:08:17,280 --> 00:08:20,960
algorithmic promotion distorts the

00:08:19,759 --> 00:08:25,039
public sphere

00:08:20,960 --> 00:08:27,440
and creates a harm for the public that

00:08:25,039 --> 00:08:29,120
is nonetheless a risk how do we deal

00:08:27,440 --> 00:08:31,039
with that risk and how do we define the

00:08:29,120 --> 00:08:33,519
type of risk that we're dealing with

00:08:31,039 --> 00:08:35,120
so as a regulator that does not want to

00:08:33,519 --> 00:08:36,479
infringe on freedom of expression you

00:08:35,120 --> 00:08:38,000
can't say

00:08:36,479 --> 00:08:40,000
here are the things that we're worried

00:08:38,000 --> 00:08:41,120
about that are not clearly illegal but

00:08:40,000 --> 00:08:43,680
if amplified

00:08:41,120 --> 00:08:45,600
in unspecified ways by platform

00:08:43,680 --> 00:08:47,920
recommender systems that

00:08:45,600 --> 00:08:49,440
that rep that increase the frequency of

00:08:47,920 --> 00:08:51,200
this kind of content

00:08:49,440 --> 00:08:52,560
in people's news feeds or in their

00:08:51,200 --> 00:08:53,360
twitter streams or in their search

00:08:52,560 --> 00:08:56,320
results

00:08:53,360 --> 00:08:56,720
in ways that are disproportionate from

00:08:56,320 --> 00:08:58,000
the

00:08:56,720 --> 00:09:00,880
way those views are actually held in the

00:08:58,000 --> 00:09:02,640
population that that constitutes a harm

00:09:00,880 --> 00:09:04,959
okay let me put this to carly and

00:09:02,640 --> 00:09:04,959
slightly different

00:09:06,959 --> 00:09:10,000
you too much with the foreign of the dsa

00:09:08,560 --> 00:09:11,440
but in general principles

00:09:10,000 --> 00:09:13,200
this isn't the level playing field

00:09:11,440 --> 00:09:15,760
because as as ben says

00:09:13,200 --> 00:09:17,360
you know what's what constitutes uh

00:09:15,760 --> 00:09:19,040
illegal content in one country is not

00:09:17,360 --> 00:09:19,680
the same another so there's no level

00:09:19,040 --> 00:09:21,440
playing

00:09:19,680 --> 00:09:23,680
field here even within europe and if

00:09:21,440 --> 00:09:24,160
we're talking about a single market as

00:09:23,680 --> 00:09:26,240
well

00:09:24,160 --> 00:09:27,760
you know there is no open playing field

00:09:26,240 --> 00:09:29,200
as well you in spain

00:09:27,760 --> 00:09:31,120
if you happen to be a rapper who who

00:09:29,200 --> 00:09:33,839
strongly disapproves of of

00:09:31,120 --> 00:09:34,480
uh the the monarchy you can end up in

00:09:33,839 --> 00:09:36,720
prison

00:09:34,480 --> 00:09:37,600
uh in a hurry uh hungary for uh

00:09:36,720 --> 00:09:39,600
disliking

00:09:37,600 --> 00:09:40,720
uh other elements of the government you

00:09:39,600 --> 00:09:43,760
so how

00:09:40,720 --> 00:09:45,120
how do we balance uh what the the the

00:09:43,760 --> 00:09:48,080
commission is trying to do

00:09:45,120 --> 00:09:49,440
here with free speech and and can you

00:09:48,080 --> 00:09:52,959
really deal with this in a risk

00:09:49,440 --> 00:09:52,959
a risk assessment way

00:09:53,680 --> 00:09:58,000
uh i think it's very very challenging i

00:09:55,920 --> 00:09:59,279
think that we've seen even from the

00:09:58,000 --> 00:10:01,360
recent attempts of the facebook

00:09:59,279 --> 00:10:02,720
oversight board to apply some kind of

00:10:01,360 --> 00:10:04,640
international set of human rights

00:10:02,720 --> 00:10:06,240
standards to content moderation

00:10:04,640 --> 00:10:07,519
decisions there are

00:10:06,240 --> 00:10:09,440
you know there are real cultural

00:10:07,519 --> 00:10:10,640
specificities to some of the questions

00:10:09,440 --> 00:10:12,320
around freedom of expression

00:10:10,640 --> 00:10:15,440
i think what the dsa here is trying to

00:10:12,320 --> 00:10:18,000
do is to establish as far as as possible

00:10:15,440 --> 00:10:18,720
a standardized framework and in order to

00:10:18,000 --> 00:10:20,560
make that

00:10:18,720 --> 00:10:22,160
real i think we could separate out some

00:10:20,560 --> 00:10:23,120
buckets of challenges some is really

00:10:22,160 --> 00:10:24,800
around

00:10:23,120 --> 00:10:26,560
specifying and understanding and having

00:10:24,800 --> 00:10:27,680
regulators understand the content of the

00:10:26,560 --> 00:10:29,839
harms that they're

00:10:27,680 --> 00:10:31,760
trying to address but and ben spoken to

00:10:29,839 --> 00:10:33,519
that i think a second is really around

00:10:31,760 --> 00:10:35,600
standardizing the approaches

00:10:33,519 --> 00:10:37,200
for addressing those harms so how do we

00:10:35,600 --> 00:10:38,640
make sure that there are there's kind of

00:10:37,200 --> 00:10:40,720
a quality of arms when it comes to

00:10:38,640 --> 00:10:43,839
regulatory capacity to inspect these

00:10:40,720 --> 00:10:45,200
uh platforms and you know it may be that

00:10:43,839 --> 00:10:47,040
some european countries have

00:10:45,200 --> 00:10:48,480
regulators that have more technical

00:10:47,040 --> 00:10:50,880
capacity than others

00:10:48,480 --> 00:10:52,560
um how do we start to kind of build up

00:10:50,880 --> 00:10:54,800
and standardize across

00:10:52,560 --> 00:10:57,279
member states and the commission the

00:10:54,800 --> 00:10:59,279
ability to kind of address these issues

00:10:57,279 --> 00:11:00,959
equally and with the same level of

00:10:59,279 --> 00:11:02,320
competency i suppose will actually be

00:11:00,959 --> 00:11:04,480
just as important as

00:11:02,320 --> 00:11:05,839
uh standardizing the content of the

00:11:04,480 --> 00:11:07,120
harms that they're trying to address i

00:11:05,839 --> 00:11:07,760
think that's one of the big challenges

00:11:07,120 --> 00:11:09,360
that

00:11:07,760 --> 00:11:11,200
that we're really focusing on at the

00:11:09,360 --> 00:11:12,959
moment thank you on

00:11:11,200 --> 00:11:14,800
you know is this really a change in how

00:11:12,959 --> 00:11:16,240
we deal with tech policy or

00:11:14,800 --> 00:11:18,800
are we dealing simply with buzzwords

00:11:16,240 --> 00:11:20,079
here or is this the structure coming

00:11:18,800 --> 00:11:21,839
into place of something that will have

00:11:20,079 --> 00:11:24,160
real teeth

00:11:21,839 --> 00:11:24,959
um i think it is quite a paradigm shift

00:11:24,160 --> 00:11:26,240
and i think

00:11:24,959 --> 00:11:28,079
when when you look at the way the

00:11:26,240 --> 00:11:29,440
european commission communicated around

00:11:28,079 --> 00:11:30,959
this in the last year

00:11:29,440 --> 00:11:33,040
it was a very much a lot of rhetoric

00:11:30,959 --> 00:11:34,720
that the dsa was going to be a next

00:11:33,040 --> 00:11:35,680
generation approach to content

00:11:34,720 --> 00:11:37,519
regulation

00:11:35,680 --> 00:11:39,519
and i think it largely is that and i

00:11:37,519 --> 00:11:40,640
think the the reason why is because it

00:11:39,519 --> 00:11:42,959
responds

00:11:40,640 --> 00:11:44,640
quite clearly to the kind of problems

00:11:42,959 --> 00:11:47,200
that ben has outlined

00:11:44,640 --> 00:11:48,399
um just a few moments ago because today

00:11:47,200 --> 00:11:50,240
i i agree

00:11:48,399 --> 00:11:51,519
if you ask people what they're concerned

00:11:50,240 --> 00:11:53,680
about online

00:11:51,519 --> 00:11:54,720
it's not very often illegal content like

00:11:53,680 --> 00:11:57,440
i think that we can

00:11:54,720 --> 00:11:59,600
we can be quite confident that the the

00:11:57,440 --> 00:12:01,040
very large mainstream platforms and the

00:11:59,600 --> 00:12:03,839
tech industry in general

00:12:01,040 --> 00:12:05,519
today has been doing a relatively decent

00:12:03,839 --> 00:12:07,760
job with addressing illegal and

00:12:05,519 --> 00:12:09,600
particularly manifest illegal content

00:12:07,760 --> 00:12:11,440
because it has been something which has

00:12:09,600 --> 00:12:12,399
been around since the very early days of

00:12:11,440 --> 00:12:13,600
the internet

00:12:12,399 --> 00:12:15,519
but obviously in the last couple of

00:12:13,600 --> 00:12:17,440
years you've seen the emergence of new

00:12:15,519 --> 00:12:18,079
types of products new types of business

00:12:17,440 --> 00:12:19,680
models

00:12:18,079 --> 00:12:21,279
and this kind of endless stream of

00:12:19,680 --> 00:12:22,959
disruptive innovation

00:12:21,279 --> 00:12:24,800
and so some of the harms that we see

00:12:22,959 --> 00:12:27,120
today are

00:12:24,800 --> 00:12:28,959
largely about how companies are

00:12:27,120 --> 00:12:29,920
interacting with the content on their

00:12:28,959 --> 00:12:32,160
services

00:12:29,920 --> 00:12:33,120
and how new and interesting and novel

00:12:32,160 --> 00:12:35,920
product features

00:12:33,120 --> 00:12:37,200
can give rise to unforeseen or

00:12:35,920 --> 00:12:40,000
unmitigated risks

00:12:37,200 --> 00:12:41,680
and so i think by looking looking

00:12:40,000 --> 00:12:43,040
through that lens we gave ourselves a

00:12:41,680 --> 00:12:45,040
much better schema for

00:12:43,040 --> 00:12:46,560
being able to respond to those problems

00:12:45,040 --> 00:12:49,040
without breaking the internet

00:12:46,560 --> 00:12:50,480
without treating everybody like they're

00:12:49,040 --> 00:12:52,639
a very large platform

00:12:50,480 --> 00:12:53,920
and without undermining undermining

00:12:52,639 --> 00:12:55,440
fundamental rights

00:12:53,920 --> 00:12:57,279
and i think this is a this is a natural

00:12:55,440 --> 00:12:59,120
progression like we had the e-commerce

00:12:57,279 --> 00:13:00,639
framework for the last 20 years

00:12:59,120 --> 00:13:02,800
and that was made at the time when the

00:13:00,639 --> 00:13:04,560
internet was very different the types of

00:13:02,800 --> 00:13:06,079
systems we have today like recommender

00:13:04,560 --> 00:13:08,160
systems and news feeds

00:13:06,079 --> 00:13:10,000
they were simply unfathomable back then

00:13:08,160 --> 00:13:11,760
and so the ecommerce director was all

00:13:10,000 --> 00:13:14,079
about trying to protect the fledging

00:13:11,760 --> 00:13:16,880
industry and trying to help it grow in a

00:13:14,079 --> 00:13:19,120
in a kind of a regulatory sandbox and i

00:13:16,880 --> 00:13:19,120
think

00:13:19,600 --> 00:13:23,279
yeah i think and kind of sheltering a

00:13:21,440 --> 00:13:25,519
niche industry and i think it's

00:13:23,279 --> 00:13:26,639
the very fact of the ecd being so

00:13:25,519 --> 00:13:28,160
successful

00:13:26,639 --> 00:13:29,920
is now that we're starting to see these

00:13:28,160 --> 00:13:31,120
these new problems arise today and so i

00:13:29,920 --> 00:13:32,639
think that we should be comfortable with

00:13:31,120 --> 00:13:34,240
the fact that regulation has to evolve

00:13:32,639 --> 00:13:36,800
with those problems too

00:13:34,240 --> 00:13:37,839
okay and when that regulation evolves uh

00:13:36,800 --> 00:13:40,240
ben you know the

00:13:37,839 --> 00:13:41,839
dsa puts a lot of emphasis on national

00:13:40,240 --> 00:13:44,240
regulators as well this is

00:13:41,839 --> 00:13:46,240
where the teeth will bite and that

00:13:44,240 --> 00:13:48,079
requires resources it requires training

00:13:46,240 --> 00:13:49,279
it requires a clarity of legal

00:13:48,079 --> 00:13:51,120
interpretation

00:13:49,279 --> 00:13:53,360
as well are you comfortable with how the

00:13:51,120 --> 00:13:55,199
dsa sets this out in terms of what the

00:13:53,360 --> 00:13:58,240
national regulators are required to do

00:13:55,199 --> 00:13:58,240
will it work in practice

00:13:59,680 --> 00:14:04,399
um i think like all things regulatory in

00:14:02,560 --> 00:14:07,839
the european union

00:14:04,399 --> 00:14:08,800
it'll be hit and miss and it will take a

00:14:07,839 --> 00:14:12,320
while

00:14:08,800 --> 00:14:14,320
for there to be commonly understood

00:14:12,320 --> 00:14:17,600
standards about how these rules are

00:14:14,320 --> 00:14:19,920
applied i think the commission

00:14:17,600 --> 00:14:21,760
has no choice but to devolve the powers

00:14:19,920 --> 00:14:23,279
down to the member states and to leave

00:14:21,760 --> 00:14:25,760
up to the member states

00:14:23,279 --> 00:14:27,440
how the digital services coordinator

00:14:25,760 --> 00:14:29,760
will be designated whether it will be

00:14:27,440 --> 00:14:31,920
a new regulator a standalone institution

00:14:29,760 --> 00:14:32,959
embedded in another existing regulatory

00:14:31,920 --> 00:14:34,560
institution

00:14:32,959 --> 00:14:36,079
is that a data protection authority is

00:14:34,560 --> 00:14:37,680
that a broadcast authority is a

00:14:36,079 --> 00:14:38,959
telecommunications authority

00:14:37,680 --> 00:14:40,959
those institutions are going to look

00:14:38,959 --> 00:14:44,639
different in every member state

00:14:40,959 --> 00:14:48,160
and then you add on the complexity of

00:14:44,639 --> 00:14:50,000
well do i as a regulator in any given

00:14:48,160 --> 00:14:51,680
member state handle all the problems

00:14:50,000 --> 00:14:54,880
affecting my citizens or

00:14:51,680 --> 00:14:56,880
do those get transferred over to let's

00:14:54,880 --> 00:14:59,199
be honest to the irish

00:14:56,880 --> 00:15:00,240
in a lot of cases where these companies

00:14:59,199 --> 00:15:04,800
are incorporated

00:15:00,240 --> 00:15:06,720
does that put on an unbearable burden

00:15:04,800 --> 00:15:08,240
on the irish regulator to handle

00:15:06,720 --> 00:15:09,440
everything to do with google and

00:15:08,240 --> 00:15:11,040
facebook

00:15:09,440 --> 00:15:14,000
i think those are all fair questions to

00:15:11,040 --> 00:15:15,920
ask and i i read the dsa as

00:15:14,000 --> 00:15:17,120
hedging against that to some degree by

00:15:15,920 --> 00:15:19,440
creating the board

00:15:17,120 --> 00:15:21,120
at the commission by creating uh some

00:15:19,440 --> 00:15:22,880
additional capacity to support the

00:15:21,120 --> 00:15:24,959
digital services coordinators

00:15:22,880 --> 00:15:26,720
how exactly that will look in practice

00:15:24,959 --> 00:15:28,079
and how it will work and what the level

00:15:26,720 --> 00:15:29,839
of coordination will be

00:15:28,079 --> 00:15:30,959
between brussels and any given member

00:15:29,839 --> 00:15:32,000
state and whether the member states

00:15:30,959 --> 00:15:34,320
would be willing

00:15:32,000 --> 00:15:36,000
to give up those big cases and hand them

00:15:34,320 --> 00:15:38,320
over to the irish

00:15:36,000 --> 00:15:40,079
those are pretty big open questions in

00:15:38,320 --> 00:15:42,079
my mind

00:15:40,079 --> 00:15:44,160
thank you i i want to come to carling a

00:15:42,079 --> 00:15:46,480
moment just to talk about how you audit

00:15:44,160 --> 00:15:47,759
big platforms as well but uh first owen

00:15:46,480 --> 00:15:49,440
you know what are the harms that we're

00:15:47,759 --> 00:15:50,880
trying to prevent here what is the dsa

00:15:49,440 --> 00:15:53,120
trying to do and you

00:15:50,880 --> 00:15:54,000
you say some harms we can't know yet

00:15:53,120 --> 00:15:58,320
because technology

00:15:54,000 --> 00:16:00,320
is evolving so how do we define this

00:15:58,320 --> 00:16:01,440
um well i think this this kind of goes

00:16:00,320 --> 00:16:02,399
back a little bit to the earlier

00:16:01,440 --> 00:16:04,880
question over what

00:16:02,399 --> 00:16:06,399
um are the definitions sufficient in the

00:16:04,880 --> 00:16:08,320
in the dsa and i think

00:16:06,399 --> 00:16:10,079
when when i've been looking at um this

00:16:08,320 --> 00:16:12,720
kind of risk-based approach

00:16:10,079 --> 00:16:14,079
i think it all comes back to what is

00:16:12,720 --> 00:16:15,199
what is risk management what are we

00:16:14,079 --> 00:16:16,880
trying to solve for there

00:16:15,199 --> 00:16:18,720
and typically if you look at other

00:16:16,880 --> 00:16:19,519
sectors where where these these methods

00:16:18,720 --> 00:16:22,399
are deployed

00:16:19,519 --> 00:16:23,199
people think in terms of the probability

00:16:22,399 --> 00:16:25,600
of a

00:16:23,199 --> 00:16:26,959
bad outcome and the impact of that

00:16:25,600 --> 00:16:28,160
outcome when it rises

00:16:26,959 --> 00:16:30,639
and so you're trying to you're trying to

00:16:28,160 --> 00:16:32,000
minimize the risk of things going wrong

00:16:30,639 --> 00:16:34,399
and you're trying to ensure that when

00:16:32,000 --> 00:16:36,399
those risks do invariably materialize

00:16:34,399 --> 00:16:38,160
that the impact is not as bad as it

00:16:36,399 --> 00:16:39,040
would otherwise be if you had done

00:16:38,160 --> 00:16:41,680
nothing

00:16:39,040 --> 00:16:43,199
and when i read the dsa particularly the

00:16:41,680 --> 00:16:46,079
articles themselves

00:16:43,199 --> 00:16:46,720
around like risk and stuff it seems very

00:16:46,079 --> 00:16:49,519
much that the

00:16:46,720 --> 00:16:51,839
the commission's focus is is largely on

00:16:49,519 --> 00:16:52,800
trying to minimize the damage when a bad

00:16:51,839 --> 00:16:54,959
thing goes wrong

00:16:52,800 --> 00:16:57,120
so for instance if your service is

00:16:54,959 --> 00:16:59,199
weaponized for disinformation or for the

00:16:57,120 --> 00:17:00,639
dissemination of a legal content

00:16:59,199 --> 00:17:02,079
you need to make sure you have measures

00:17:00,639 --> 00:17:02,800
in place to address that when it goes

00:17:02,079 --> 00:17:04,160
wrong

00:17:02,800 --> 00:17:05,760
but i think that's that's good it's a

00:17:04,160 --> 00:17:06,240
positive step forward to think in those

00:17:05,760 --> 00:17:07,760
terms

00:17:06,240 --> 00:17:09,199
but i don't think it's really sufficient

00:17:07,760 --> 00:17:11,280
because if you look for instance to

00:17:09,199 --> 00:17:13,280
think of an analogy of a theme park

00:17:11,280 --> 00:17:15,039
theme park is a risky place that's what

00:17:13,280 --> 00:17:16,240
makes it fun because you're going there

00:17:15,039 --> 00:17:18,400
for the thrills

00:17:16,240 --> 00:17:20,000
if you went on a roller coaster and the

00:17:18,400 --> 00:17:20,640
the manager of the theme park simply

00:17:20,000 --> 00:17:22,640
told you

00:17:20,640 --> 00:17:24,400
we take your safety very seriously and

00:17:22,640 --> 00:17:25,039
we ensure that if the roller coaster

00:17:24,400 --> 00:17:26,559
crashes

00:17:25,039 --> 00:17:28,640
that we can get you to a hospital

00:17:26,559 --> 00:17:29,600
straight away to to minimize the damage

00:17:28,640 --> 00:17:31,840
it causes you

00:17:29,600 --> 00:17:33,120
you think well that's great but also at

00:17:31,840 --> 00:17:35,520
the same time you'd want to know that

00:17:33,120 --> 00:17:37,039
the theme park is taking steps to ensure

00:17:35,520 --> 00:17:38,720
the roller coasters are safe in the

00:17:37,039 --> 00:17:40,320
first place that they're regularly

00:17:38,720 --> 00:17:42,160
checked that they were conformed to all

00:17:40,320 --> 00:17:44,799
the necessary safety standards

00:17:42,160 --> 00:17:46,640
and so i think in the dsa given that a

00:17:44,799 --> 00:17:47,840
lot of the kind of harms and the risks

00:17:46,640 --> 00:17:49,919
we're trying to solve for

00:17:47,840 --> 00:17:51,520
are risks that are related to how

00:17:49,919 --> 00:17:53,520
companies are acting to

00:17:51,520 --> 00:17:55,360
the markets they roll out in the systems

00:17:53,520 --> 00:17:57,200
they use and the various

00:17:55,360 --> 00:17:58,400
other kind of contextual factors that

00:17:57,200 --> 00:18:01,200
comes with running

00:17:58,400 --> 00:18:01,919
very large very influential businesses i

00:18:01,200 --> 00:18:04,640
think that the

00:18:01,919 --> 00:18:07,360
the risk-based approach needs to focus

00:18:04,640 --> 00:18:09,440
more on those choices it needs to really

00:18:07,360 --> 00:18:11,280
invest in the bloodstream of a company

00:18:09,440 --> 00:18:12,160
and to make sure that when companies are

00:18:11,280 --> 00:18:14,240
operating

00:18:12,160 --> 00:18:16,960
thinking prudently and thinking wisely

00:18:14,240 --> 00:18:18,880
and like moving slow and fixing things

00:18:16,960 --> 00:18:20,799
is is very much part of the of the

00:18:18,880 --> 00:18:22,480
process and not just an afterthought

00:18:20,799 --> 00:18:23,679
that you clean up

00:18:22,480 --> 00:18:25,360
thank you the only good thing about

00:18:23,679 --> 00:18:27,120
attraction parts of the food oh and i

00:18:25,360 --> 00:18:29,919
think we can all agree on that

00:18:27,120 --> 00:18:31,840
and uh carly the platforms how do we

00:18:29,919 --> 00:18:34,000
audit platforms you know what's the

00:18:31,840 --> 00:18:36,000
what's the model that we need to pursue

00:18:34,000 --> 00:18:38,400
here and can we do this in-house

00:18:36,000 --> 00:18:39,200
do we need to bring in a whole bunch of

00:18:38,400 --> 00:18:41,520
consultants

00:18:39,200 --> 00:18:43,360
uh to to who can be expert in this and

00:18:41,520 --> 00:18:43,679
and perhaps more commercially minded as

00:18:43,360 --> 00:18:46,799
well

00:18:43,679 --> 00:18:48,320
how's this done so one thing that's come

00:18:46,799 --> 00:18:50,240
with the dsa and also

00:18:48,320 --> 00:18:52,000
um and i'm not sure how familiar the

00:18:50,240 --> 00:18:53,520
audience is with this but a similar and

00:18:52,000 --> 00:18:55,440
parallel piece of work that's happening

00:18:53,520 --> 00:18:57,840
in the uk around the online safety bill

00:18:55,440 --> 00:18:59,919
is a real maturing of the conversation

00:18:57,840 --> 00:19:02,240
around audit i would say two years ago

00:18:59,919 --> 00:19:03,120
people used audit to refer to any and

00:19:02,240 --> 00:19:04,799
all um

00:19:03,120 --> 00:19:07,120
type of accountability mechanisms that

00:19:04,799 --> 00:19:09,120
might be executed visit the v

00:19:07,120 --> 00:19:10,880
large platforms and social media

00:19:09,120 --> 00:19:12,240
companies and we're starting to see some

00:19:10,880 --> 00:19:14,240
real granular detail

00:19:12,240 --> 00:19:15,520
come out in the dsa in the online safety

00:19:14,240 --> 00:19:17,360
bill and elsewhere

00:19:15,520 --> 00:19:18,559
i think from the dsa's perspective

00:19:17,360 --> 00:19:19,840
there's kind of three

00:19:18,559 --> 00:19:24,720
types of audits or three kind of

00:19:19,840 --> 00:19:26,799
components of order there's a kind of um

00:19:24,720 --> 00:19:28,480
a kind of monitoring function which is

00:19:26,799 --> 00:19:30,320
about ongoing

00:19:28,480 --> 00:19:32,880
oversight by regulators to surface

00:19:30,320 --> 00:19:35,280
concerns or non-compliance with the dsa

00:19:32,880 --> 00:19:37,280
then there's specific investigations

00:19:35,280 --> 00:19:38,480
with on specific instances of

00:19:37,280 --> 00:19:40,160
non-compliance

00:19:38,480 --> 00:19:42,480
and then there's an annual compliance

00:19:40,160 --> 00:19:43,039
function which would be a kind of annual

00:19:42,480 --> 00:19:45,679
audit

00:19:43,039 --> 00:19:46,880
now each types of those functions

00:19:45,679 --> 00:19:49,520
require

00:19:46,880 --> 00:19:50,080
different technical and qualitative

00:19:49,520 --> 00:19:52,880
measures

00:19:50,080 --> 00:19:53,360
in order to really ensure compliance and

00:19:52,880 --> 00:19:55,760
and

00:19:53,360 --> 00:19:56,559
in terms of the detail of that we're

00:19:55,760 --> 00:19:58,960
really seeing

00:19:56,559 --> 00:20:00,960
an evolving uh field and evolving

00:19:58,960 --> 00:20:01,840
discourse about what would require for

00:20:00,960 --> 00:20:04,559
example

00:20:01,840 --> 00:20:07,039
to do ongoing monitoring is it about

00:20:04,559 --> 00:20:07,679
creation of documentation on the part of

00:20:07,039 --> 00:20:10,240
the

00:20:07,679 --> 00:20:12,480
platform that can later be inspected is

00:20:10,240 --> 00:20:15,600
it about permitting access or permitting

00:20:12,480 --> 00:20:18,640
interviews um permitting scrutiny of

00:20:15,600 --> 00:20:20,320
technical systems um and can which parts

00:20:18,640 --> 00:20:21,280
of that can be outsourced to an external

00:20:20,320 --> 00:20:24,159
auditor versus

00:20:21,280 --> 00:20:24,960
conducted by a regulator itself at a top

00:20:24,159 --> 00:20:26,480
level i would say

00:20:24,960 --> 00:20:28,400
there's kind of three areas that would

00:20:26,480 --> 00:20:30,559
need to be looked at from an audit one

00:20:28,400 --> 00:20:31,440
would be around policy so what policies

00:20:30,559 --> 00:20:33,600
the platforms

00:20:31,440 --> 00:20:34,559
have with respect to the specific types

00:20:33,600 --> 00:20:36,720
of content

00:20:34,559 --> 00:20:38,640
under scrutiny how the platform says

00:20:36,720 --> 00:20:40,960
it's going to act with respect to that

00:20:38,640 --> 00:20:42,159
what criteria for intervention there are

00:20:40,960 --> 00:20:44,159
and then the second one is around

00:20:42,159 --> 00:20:45,679
process so what processes are actually

00:20:44,159 --> 00:20:48,000
followed by the platform

00:20:45,679 --> 00:20:49,840
and that breaks out into two sections

00:20:48,000 --> 00:20:51,039
one is around technical implementation

00:20:49,840 --> 00:20:53,760
and one is around

00:20:51,039 --> 00:20:54,159
um kind of personnel implementation what

00:20:53,760 --> 00:20:55,600
you know

00:20:54,159 --> 00:20:57,679
how is the governance structure being

00:20:55,600 --> 00:20:59,360
executed and then the third is around

00:20:57,679 --> 00:21:00,960
outcomes what does the platform know

00:20:59,360 --> 00:21:02,080
about the outcomes of its products what

00:21:00,960 --> 00:21:03,679
data do they collect

00:21:02,080 --> 00:21:05,520
what and to what extent can those

00:21:03,679 --> 00:21:06,880
outcomes be scrutinized and i would say

00:21:05,520 --> 00:21:07,919
there's a range of different technical

00:21:06,880 --> 00:21:10,720
methods that can be

00:21:07,919 --> 00:21:11,200
that can be pursued to your point there

00:21:10,720 --> 00:21:13,840
about

00:21:11,200 --> 00:21:14,960
should there be um an external audit

00:21:13,840 --> 00:21:16,320
industry brian

00:21:14,960 --> 00:21:18,480
one of the ways you know if we look at

00:21:16,320 --> 00:21:20,480
an analogous you know

00:21:18,480 --> 00:21:22,640
by extension analogy field the financial

00:21:20,480 --> 00:21:25,919
sector you know huge complicated

00:21:22,640 --> 00:21:26,240
uh sector that has um really struggled

00:21:25,919 --> 00:21:28,400
with

00:21:26,240 --> 00:21:30,400
ethical normative issues particularly

00:21:28,400 --> 00:21:30,799
since 2008 and tried to embed those

00:21:30,400 --> 00:21:33,520
through

00:21:30,799 --> 00:21:34,320
a stronger audit regime we have seen an

00:21:33,520 --> 00:21:36,480
industry

00:21:34,320 --> 00:21:38,000
pop up around external compliance that

00:21:36,480 --> 00:21:40,000
helps companies adhere

00:21:38,000 --> 00:21:41,280
to this i suspect we will see the same

00:21:40,000 --> 00:21:42,880
here but

00:21:41,280 --> 00:21:44,320
it should be acknowledged that that

00:21:42,880 --> 00:21:45,760
industry is in its infancy

00:21:44,320 --> 00:21:47,520
and there was really interesting article

00:21:45,760 --> 00:21:49,679
in the markup last week

00:21:47,520 --> 00:21:51,200
just about how much turmoil the order

00:21:49,679 --> 00:21:52,720
industry is because there's no

00:21:51,200 --> 00:21:54,080
standardization there's no common

00:21:52,720 --> 00:21:55,600
understanding about what audit is and

00:21:54,080 --> 00:21:58,960
how it has to be done so

00:21:55,600 --> 00:22:01,360
i'm i i think the dsa will really um

00:21:58,960 --> 00:22:02,240
force the crystallization of methods

00:22:01,360 --> 00:22:03,919
approaches

00:22:02,240 --> 00:22:06,400
you know best practice when it comes to

00:22:03,919 --> 00:22:08,000
audit and but it's going to be a little

00:22:06,400 --> 00:22:10,240
bit trial and error i would say

00:22:08,000 --> 00:22:12,000
until um and and we'll see some kind of

00:22:10,240 --> 00:22:14,480
competing regimes emerging i would say

00:22:12,000 --> 00:22:16,000
in european countries in the uk and then

00:22:14,480 --> 00:22:17,760
in the u.s where there are a number of

00:22:16,000 --> 00:22:18,960
pieces of legislation also calling for

00:22:17,760 --> 00:22:20,640
algorithm audits

00:22:18,960 --> 00:22:22,400
and i suppose hopefully we'll see a

00:22:20,640 --> 00:22:24,080
coalescence around the common

00:22:22,400 --> 00:22:26,400
highest common denominator when it comes

00:22:24,080 --> 00:22:26,799
to audit in the coming years and do you

00:22:26,400 --> 00:22:29,600
see

00:22:26,799 --> 00:22:30,400
certification of the this other process

00:22:29,600 --> 00:22:32,400
or is this

00:22:30,400 --> 00:22:34,240
just an oversight which would have some

00:22:32,400 --> 00:22:37,280
recommendations or would this

00:22:34,240 --> 00:22:38,960
uh be punishable as well how how should

00:22:37,280 --> 00:22:40,799
this go

00:22:38,960 --> 00:22:42,559
again i would say there are layers i

00:22:40,799 --> 00:22:44,799
suspect we'll see a certification

00:22:42,559 --> 00:22:46,400
industry pop up the same way we've seen

00:22:44,799 --> 00:22:48,240
in data protection

00:22:46,400 --> 00:22:49,600
and in from a data protection

00:22:48,240 --> 00:22:52,320
perspective that's been quite

00:22:49,600 --> 00:22:54,640
um problematic because data protection

00:22:52,320 --> 00:22:56,960
certification is is something that

00:22:54,640 --> 00:22:57,840
has you know um come up from a

00:22:56,960 --> 00:23:00,080
grassroots level

00:22:57,840 --> 00:23:01,440
isn't standardized isn't necessarily

00:23:00,080 --> 00:23:02,400
quality checked by some central

00:23:01,440 --> 00:23:04,480
authority and i think

00:23:02,400 --> 00:23:06,880
we'll probably see a phase of audit

00:23:04,480 --> 00:23:09,919
where there's you know audit approved by

00:23:06,880 --> 00:23:11,520
us xyz external auditor

00:23:09,919 --> 00:23:13,760
that doesn't necessarily correspond to

00:23:11,520 --> 00:23:17,360
the requirements of the dsa

00:23:13,760 --> 00:23:18,240
i think um given the immensity of the

00:23:17,360 --> 00:23:21,120
task

00:23:18,240 --> 00:23:22,960
and this uncertainty about regulatory

00:23:21,120 --> 00:23:25,200
capacity and who's going to implement

00:23:22,960 --> 00:23:27,679
i think we'll see a mix of regulators

00:23:25,200 --> 00:23:28,799
doing specific compliance related audits

00:23:27,679 --> 00:23:31,360
specific compliance-related

00:23:28,799 --> 00:23:31,760
investigations and then companies trying

00:23:31,360 --> 00:23:34,000
to

00:23:31,760 --> 00:23:35,760
hedge their bets or preempt any critique

00:23:34,000 --> 00:23:36,720
through preemptory audits that will

00:23:35,760 --> 00:23:39,600
probably

00:23:36,720 --> 00:23:40,080
um be serviced by an external industry

00:23:39,600 --> 00:23:41,679
um

00:23:40,080 --> 00:23:43,679
and we're seeing that in a little bit in

00:23:41,679 --> 00:23:45,520
the us at the moment um with

00:23:43,679 --> 00:23:47,440
companies voluntarily undertaking audits

00:23:45,520 --> 00:23:49,279
in order to kind of preempt any

00:23:47,440 --> 00:23:50,960
uh regulatory scrutiny i'd say it'll be

00:23:49,279 --> 00:23:53,760
a mixed bag um

00:23:50,960 --> 00:23:55,760
unless and until we get proper coverage

00:23:53,760 --> 00:23:57,120
of regulatory capacity across

00:23:55,760 --> 00:23:58,880
member states and i think that that's

00:23:57,120 --> 00:24:00,159
probably a long way out

00:23:58,880 --> 00:24:01,600
yeah i was just about to say that this

00:24:00,159 --> 00:24:02,720
doesn't sound like it's going to happen

00:24:01,600 --> 00:24:06,159
any time soon

00:24:02,720 --> 00:24:07,760
in real terms thank you ben you

00:24:06,159 --> 00:24:09,840
auditing requires that you see inside

00:24:07,760 --> 00:24:11,600
the black box does the dsa

00:24:09,840 --> 00:24:14,159
do anything to help us see inside the

00:24:11,600 --> 00:24:14,159
black box

00:24:14,880 --> 00:24:19,600
um i think that the powers established

00:24:18,000 --> 00:24:22,320
in the dsa

00:24:19,600 --> 00:24:23,840
are sufficient to open the black box

00:24:22,320 --> 00:24:25,360
what remains to be seen is if any

00:24:23,840 --> 00:24:27,760
regulator will choose to use those

00:24:25,360 --> 00:24:27,760
powers

00:24:28,799 --> 00:24:35,120
i can make a case that the gdpr

00:24:32,080 --> 00:24:36,480
is a great idea and and a wonderful

00:24:35,120 --> 00:24:38,159
piece of statute and someone should

00:24:36,480 --> 00:24:39,919
really try it

00:24:38,159 --> 00:24:42,559
you know there are many pieces of the

00:24:39,919 --> 00:24:43,919
gdpr that have never been fully enforced

00:24:42,559 --> 00:24:45,919
and the reason isn't because it isn't

00:24:43,919 --> 00:24:47,679
written in plain language in the law

00:24:45,919 --> 00:24:50,159
it's because no regulator has

00:24:47,679 --> 00:24:51,360
picked up its tools and gone in and

00:24:50,159 --> 00:24:54,080
tried to enforce those

00:24:51,360 --> 00:24:54,480
those aspects so how to use the tools

00:24:54,080 --> 00:24:55,600
though

00:24:54,480 --> 00:24:57,600
do you think this is a problem of

00:24:55,600 --> 00:24:58,559
education of training what's the issue

00:24:57,600 --> 00:25:01,440
here i think it's

00:24:58,559 --> 00:25:02,400
resources and political will which is

00:25:01,440 --> 00:25:05,039
why

00:25:02,400 --> 00:25:07,039
does the dsa provide any way in which

00:25:05,039 --> 00:25:09,279
the european union will

00:25:07,039 --> 00:25:11,760
fund training fund the resources needed

00:25:09,279 --> 00:25:11,760
to do this

00:25:12,320 --> 00:25:18,559
uh not that i see directly

00:25:15,679 --> 00:25:19,440
yeah so um you end up with ireland as

00:25:18,559 --> 00:25:22,960
carly said you

00:25:19,440 --> 00:25:24,880
yeah having to deal with a lot of this

00:25:22,960 --> 00:25:26,799
perhaps and other countries who simply

00:25:24,880 --> 00:25:30,240
don't have the resources to do it become

00:25:26,799 --> 00:25:31,760
an easy option were i a member of the

00:25:30,240 --> 00:25:33,200
european parliament these are the kinds

00:25:31,760 --> 00:25:35,840
of questions that i would be raising

00:25:33,200 --> 00:25:37,360
and seeking to solve this law makes its

00:25:35,840 --> 00:25:39,760
way through the process

00:25:37,360 --> 00:25:41,440
that it's not enough to have the right

00:25:39,760 --> 00:25:45,279
language written on paper

00:25:41,440 --> 00:25:48,640
in the law it requires having

00:25:45,279 --> 00:25:50,799
the ins the instrumentation correct and

00:25:48,640 --> 00:25:53,919
that means having regulators with both

00:25:50,799 --> 00:25:55,039
the education and the resources and the

00:25:53,919 --> 00:25:57,039
political will

00:25:55,039 --> 00:25:58,799
and the motivation to go ahead and

00:25:57,039 --> 00:26:00,799
enforce those laws

00:25:58,799 --> 00:26:02,320
but you know we've been for the last

00:26:00,799 --> 00:26:03,520
half hour talking about

00:26:02,320 --> 00:26:05,679
all the ways in which this is going to

00:26:03,520 --> 00:26:08,880
be very hard and complicated and messy

00:26:05,679 --> 00:26:10,320
and i i want to put a little bit of a

00:26:08,880 --> 00:26:13,440
silver lining here which is

00:26:10,320 --> 00:26:15,120
in my experience now watching technology

00:26:13,440 --> 00:26:18,799
and telecommunications and media

00:26:15,120 --> 00:26:18,799
regulation for the last 20 years

00:26:19,520 --> 00:26:23,120
it isn't necessary for the law to be

00:26:21,520 --> 00:26:25,360
perfect or to be perfectly

00:26:23,120 --> 00:26:27,360
implemented in order to see change in

00:26:25,360 --> 00:26:31,120
product and practice in the pocket

00:26:27,360 --> 00:26:34,240
because companies will begin to

00:26:31,120 --> 00:26:36,320
assess regulatory overhang

00:26:34,240 --> 00:26:38,159
and to begin internalizing some of the

00:26:36,320 --> 00:26:38,960
costs that they have been externalizing

00:26:38,159 --> 00:26:41,200
up to now

00:26:38,960 --> 00:26:42,159
because they're the lawyers in their

00:26:41,200 --> 00:26:43,840
offices will say

00:26:42,159 --> 00:26:45,760
look there's a non-zero chance that you

00:26:43,840 --> 00:26:48,400
could be held in non-compliance

00:26:45,760 --> 00:26:49,600
and the the penalties for non-compliance

00:26:48,400 --> 00:26:51,679
are extraordinary

00:26:49,600 --> 00:26:53,360
and the potential brand damage for being

00:26:51,679 --> 00:26:54,320
held in non-compliance with such an

00:26:53,360 --> 00:26:56,240
important law

00:26:54,320 --> 00:26:57,440
especially if the politics and public

00:26:56,240 --> 00:26:59,840
opinion about

00:26:57,440 --> 00:27:01,520
these issues are are aligned against you

00:26:59,840 --> 00:27:02,960
that we better go ahead and start

00:27:01,520 --> 00:27:04,240
changing product and practice you saw

00:27:02,960 --> 00:27:06,320
that with the gdpr

00:27:04,240 --> 00:27:07,760
despite the fact that the gdpr in my

00:27:06,320 --> 00:27:08,559
opinion has not been effectively

00:27:07,760 --> 00:27:10,640
enforced

00:27:08,559 --> 00:27:12,000
it has changed industry practice very

00:27:10,640 --> 00:27:13,840
substantially

00:27:12,000 --> 00:27:15,760
and i think the same will be true for

00:27:13,840 --> 00:27:18,880
the dsa even in advance

00:27:15,760 --> 00:27:22,240
of any adjudications being brought so

00:27:18,880 --> 00:27:23,760
the the the an imperfect law is

00:27:22,240 --> 00:27:26,720
therefore better than a perfect one

00:27:23,760 --> 00:27:29,120
in any case because it begins to change

00:27:26,720 --> 00:27:29,840
the assumptions and the expectations in

00:27:29,120 --> 00:27:32,080
industry

00:27:29,840 --> 00:27:34,240
and and in that way i think we'll begin

00:27:32,080 --> 00:27:36,080
to see movement particularly amongst the

00:27:34,240 --> 00:27:37,039
very large platforms who are obviously

00:27:36,080 --> 00:27:40,559
at the center

00:27:37,039 --> 00:27:43,039
of this proposal okay can i just

00:27:40,559 --> 00:27:43,039
reinforce

00:27:44,159 --> 00:27:49,039
sorry sorry yes i was just gonna say to

00:27:46,240 --> 00:27:50,720
reinforce what ben said i think even if

00:27:49,039 --> 00:27:52,159
even if regulators didn't get to the

00:27:50,720 --> 00:27:54,720
stage of of um

00:27:52,159 --> 00:27:55,520
executing audits and things stayed at a

00:27:54,720 --> 00:27:57,120
um

00:27:55,520 --> 00:27:59,200
self-commissioned audit stage or even

00:27:57,120 --> 00:28:00,799
just at a documentation of due diligence

00:27:59,200 --> 00:28:01,679
process we would be miles ahead of where

00:28:00,799 --> 00:28:03,840
we are now so

00:28:01,679 --> 00:28:05,520
i think putting that regime in place

00:28:03,840 --> 00:28:06,960
even to the extent that it incentivizes

00:28:05,520 --> 00:28:09,200
documentation

00:28:06,960 --> 00:28:10,240
which not only creates evidence but also

00:28:09,200 --> 00:28:12,000
changes practice

00:28:10,240 --> 00:28:13,520
we we can see you know things like model

00:28:12,000 --> 00:28:15,679
cards being used

00:28:13,520 --> 00:28:17,360
does actually change the practice of

00:28:15,679 --> 00:28:19,840
developing technology so i think

00:28:17,360 --> 00:28:21,360
like i i certainly don't want to give

00:28:19,840 --> 00:28:23,520
the impression that it's all critique

00:28:21,360 --> 00:28:25,520
either i think that ben's point is right

00:28:23,520 --> 00:28:27,360
it's an immense step forward just to

00:28:25,520 --> 00:28:27,760
require platforms to undertake this type

00:28:27,360 --> 00:28:30,960
of

00:28:27,760 --> 00:28:33,360
due diligence thank you i'm just going

00:28:30,960 --> 00:28:34,880
to say alina popescu uh sends a question

00:28:33,360 --> 00:28:36,240
i'm asking about who should all of the

00:28:34,880 --> 00:28:38,159
platforms i think that's been answered

00:28:36,240 --> 00:28:39,600
if we answer that correctly for you and

00:28:38,159 --> 00:28:41,279
let me know and we also have another

00:28:39,600 --> 00:28:42,159
question just following up on as from

00:28:41,279 --> 00:28:44,799
vesna

00:28:42,159 --> 00:28:46,799
following up on ben's gdpr case uh

00:28:44,799 --> 00:28:50,000
what's with the right to explanation

00:28:46,799 --> 00:28:50,399
do we know if and how that law has ever

00:28:50,000 --> 00:28:54,320
been

00:28:50,399 --> 00:28:54,320
exercised ben

00:28:55,440 --> 00:28:58,480
you sorry i i don't think i quite

00:28:57,039 --> 00:28:59,200
understood the questions can you reset

00:28:58,480 --> 00:29:00,960
it

00:28:59,200 --> 00:29:02,960
uh i think you can see it in the q a uh

00:29:00,960 --> 00:29:05,279
panel as well there as picking up on

00:29:02,960 --> 00:29:07,120
your gdpr case reference uh what's up

00:29:05,279 --> 00:29:08,559
with the right to explanation do we know

00:29:07,120 --> 00:29:11,840
if and how the law

00:29:08,559 --> 00:29:13,120
has ever been exercised

00:29:11,840 --> 00:29:16,559
i think you answered this actually

00:29:13,120 --> 00:29:16,559
pretty broad um

00:29:17,600 --> 00:29:22,880
good question i don't know that i have a

00:29:20,480 --> 00:29:25,840
great answer that question

00:29:22,880 --> 00:29:27,600
owen is much more of a gdpr hand than i

00:29:25,840 --> 00:29:30,240
am owen what do you think about that

00:29:27,600 --> 00:29:30,640
one um i actually was going to say i'm

00:29:30,240 --> 00:29:32,240
not

00:29:30,640 --> 00:29:34,080
i think that maybe we'd all need to

00:29:32,240 --> 00:29:35,360
consult with our with our

00:29:34,080 --> 00:29:37,600
our friends in the data protection

00:29:35,360 --> 00:29:39,600
community to answer that question

00:29:37,600 --> 00:29:42,559
but i do just to pick up on the other

00:29:39,600 --> 00:29:45,760
question um that was raised by

00:29:42,559 --> 00:29:47,520
um by alina regarding auditing i think

00:29:45,760 --> 00:29:48,960
uh building on what what carly says the

00:29:47,520 --> 00:29:51,120
idea that we're going to have a

00:29:48,960 --> 00:29:52,559
kind of cottage industry for auditing a

00:29:51,120 --> 00:29:54,559
peer and other

00:29:52,559 --> 00:29:56,159
other forms i think that's going to be

00:29:54,559 --> 00:29:58,000
crucial it's going to be crucial to

00:29:56,159 --> 00:29:58,880
solve the the problem that ben has

00:29:58,000 --> 00:30:00,960
identified

00:29:58,880 --> 00:30:03,200
that this might be great in in theory

00:30:00,960 --> 00:30:06,399
but kind of useless in practice

00:30:03,200 --> 00:30:09,520
and the reason being is that um

00:30:06,399 --> 00:30:12,240
the more kind of polycentric regulation

00:30:09,520 --> 00:30:12,559
is the least the lesser the chance you

00:30:12,240 --> 00:30:14,880
have

00:30:12,559 --> 00:30:15,600
of kind of choke points or failures

00:30:14,880 --> 00:30:17,840
because

00:30:15,600 --> 00:30:19,840
like for instance we see in gdpr there's

00:30:17,840 --> 00:30:22,799
so much pressure is put on one member

00:30:19,840 --> 00:30:24,159
state to actually enforce and

00:30:22,799 --> 00:30:25,919
mean that if there's a problem in that

00:30:24,159 --> 00:30:28,159
member state for whatever reason

00:30:25,919 --> 00:30:29,840
then the law can can effectively become

00:30:28,159 --> 00:30:31,039
com communists for many people to

00:30:29,840 --> 00:30:32,720
enforce their rights

00:30:31,039 --> 00:30:34,880
so i think in addition to having the

00:30:32,720 --> 00:30:35,520
idea of these third-party external

00:30:34,880 --> 00:30:37,360
audits

00:30:35,520 --> 00:30:39,200
assuming we can get that standardized to

00:30:37,360 --> 00:30:39,840
a degree and in assuming that we can

00:30:39,200 --> 00:30:41,840
ensure

00:30:39,840 --> 00:30:43,279
that there's a structural misalignment

00:30:41,840 --> 00:30:45,520
of incentives between

00:30:43,279 --> 00:30:47,200
the auditor and the audited that's

00:30:45,520 --> 00:30:48,960
obviously essential

00:30:47,200 --> 00:30:50,559
that misalignment should be there i

00:30:48,960 --> 00:30:51,200
think it will be a really big step

00:30:50,559 --> 00:30:53,120
forward

00:30:51,200 --> 00:30:54,240
but i also think that kind of a third

00:30:53,120 --> 00:30:56,880
pillar of this

00:30:54,240 --> 00:30:57,760
is there needs to be systemic

00:30:56,880 --> 00:31:00,480
transparency

00:30:57,760 --> 00:31:01,279
that's systemic in a public-facing way

00:31:00,480 --> 00:31:02,880
into how

00:31:01,279 --> 00:31:04,880
platforms are working and how some of

00:31:02,880 --> 00:31:06,240
these risk mitigation efforts are being

00:31:04,880 --> 00:31:09,039
are being made

00:31:06,240 --> 00:31:10,880
because even though risk management is a

00:31:09,039 --> 00:31:12,559
very kind of technical subject and it's

00:31:10,880 --> 00:31:15,200
considered to be a science

00:31:12,559 --> 00:31:16,480
i think it's fair to say all science is

00:31:15,200 --> 00:31:19,120
value laden to an

00:31:16,480 --> 00:31:21,120
extent and we are making value judgments

00:31:19,120 --> 00:31:23,519
in what we choose to assess

00:31:21,120 --> 00:31:25,039
how we assess it and how ultimately we

00:31:23,519 --> 00:31:27,760
choose to act upon it

00:31:25,039 --> 00:31:29,519
and personally i don't want that that

00:31:27,760 --> 00:31:30,240
value judgment to be made just within

00:31:29,519 --> 00:31:32,320
companies

00:31:30,240 --> 00:31:33,760
and just within engineering communities

00:31:32,320 --> 00:31:36,000
based in silicon valley

00:31:33,760 --> 00:31:36,960
i want to make sure that everybody has a

00:31:36,000 --> 00:31:39,039
chance to

00:31:36,960 --> 00:31:40,799
be part of that process see what

00:31:39,039 --> 00:31:42,480
decisions are being made and ultimately

00:31:40,799 --> 00:31:43,279
correct for those decisions when when

00:31:42,480 --> 00:31:45,760
they're wrong

00:31:43,279 --> 00:31:47,360
so the dsa does make some useful

00:31:45,760 --> 00:31:49,600
ventures in this regard by

00:31:47,360 --> 00:31:50,640
for instance facilitating access to data

00:31:49,600 --> 00:31:52,000
for researchers

00:31:50,640 --> 00:31:54,080
i think those are provisions we need to

00:31:52,000 --> 00:31:56,159
spend a lot more time working on because

00:31:54,080 --> 00:31:57,760
they need to be as broad as possible and

00:31:56,159 --> 00:31:58,480
they need to be ensured that it's not

00:31:57,760 --> 00:32:01,679
just

00:31:58,480 --> 00:32:03,840
um trusted researchers or vetted civil

00:32:01,679 --> 00:32:05,200
society orgs but really as many

00:32:03,840 --> 00:32:06,960
stakeholders as possible

00:32:05,200 --> 00:32:08,559
have a chance to be involved in this

00:32:06,960 --> 00:32:10,240
this kind of auditing this risk

00:32:08,559 --> 00:32:11,840
mitigation process to ensure that it

00:32:10,240 --> 00:32:13,360
works for everybody

00:32:11,840 --> 00:32:15,120
thank you i wrote a piece on sunday

00:32:13,360 --> 00:32:15,919
about this in the context of ai and

00:32:15,120 --> 00:32:18,720
ethics as well

00:32:15,919 --> 00:32:20,240
the commission's iai ethics guidelines

00:32:18,720 --> 00:32:22,720
you know the the guidelines are

00:32:20,240 --> 00:32:23,840
are good they're sensible um but one of

00:32:22,720 --> 00:32:26,000
the points i made was that

00:32:23,840 --> 00:32:28,080
basically we should start with values we

00:32:26,000 --> 00:32:30,000
should we should encode the values in

00:32:28,080 --> 00:32:31,120
a constitutional way and i wonder if the

00:32:30,000 --> 00:32:33,440
same doesn't apply

00:32:31,120 --> 00:32:34,559
here as well what is it like you say

00:32:33,440 --> 00:32:36,880
what are we trying to do

00:32:34,559 --> 00:32:37,600
and we can't leave the value judgments

00:32:36,880 --> 00:32:39,039
to

00:32:37,600 --> 00:32:40,640
individual companies or other

00:32:39,039 --> 00:32:42,080
organizations or national governments

00:32:40,640 --> 00:32:44,080
to do this if we want a harmonized

00:32:42,080 --> 00:32:47,200
approach carly question for you

00:32:44,080 --> 00:32:49,120
this is from aliska percova uh thanks

00:32:47,200 --> 00:32:50,559
atleast she's from accessnow

00:32:49,120 --> 00:32:53,200
i would like to ask about the role of

00:32:50,559 --> 00:32:56,880
mandatory human rights impact assessment

00:32:53,200 --> 00:32:59,279
do you think the hria should be the

00:32:56,880 --> 00:33:01,039
primary mechanism for assessing uh

00:32:59,279 --> 00:33:03,279
system at systemic risks

00:33:01,039 --> 00:33:06,399
followed by a risk-based approach as a

00:33:03,279 --> 00:33:06,399
secondary mechanism

00:33:06,480 --> 00:33:10,240
that's a great question yeah i think

00:33:08,480 --> 00:33:11,760
that i think that um

00:33:10,240 --> 00:33:13,600
uh human rights impact assessment

00:33:11,760 --> 00:33:16,960
provide absolutely the

00:33:13,600 --> 00:33:17,840
the platform the the um the kind of

00:33:16,960 --> 00:33:19,519
template

00:33:17,840 --> 00:33:21,120
starting point for risk assessment in

00:33:19,519 --> 00:33:23,840
this space um

00:33:21,120 --> 00:33:26,000
as to whether it's uh i would say it's

00:33:23,840 --> 00:33:27,519
necessary perhaps not sufficient

00:33:26,000 --> 00:33:29,200
uh to capture all of the values

00:33:27,519 --> 00:33:32,399
questions that you just alluded to

00:33:29,200 --> 00:33:33,679
brian so i think um absolutely as a

00:33:32,399 --> 00:33:35,120
floor we should be

00:33:33,679 --> 00:33:37,519
requiring mandatory human rights

00:33:35,120 --> 00:33:40,399
assessments whether or not it will

00:33:37,519 --> 00:33:41,679
enable us to capture some of the larger

00:33:40,399 --> 00:33:44,559
ethical

00:33:41,679 --> 00:33:46,320
questions that um aren't captured by the

00:33:44,559 --> 00:33:48,000
detail of human rights impact assessment

00:33:46,320 --> 00:33:49,440
i'm not entirely sure i think one of the

00:33:48,000 --> 00:33:51,440
things we

00:33:49,440 --> 00:33:52,799
we've struggled with in the ai ethics

00:33:51,440 --> 00:33:55,360
space is

00:33:52,799 --> 00:33:57,039
where i articu articulating very high

00:33:55,360 --> 00:33:58,720
level principles without

00:33:57,039 --> 00:34:00,399
really getting into the details of how

00:33:58,720 --> 00:34:01,039
they apply in practice and how they

00:34:00,399 --> 00:34:02,880
involve

00:34:01,039 --> 00:34:04,799
uh values judgments which needed to be

00:34:02,880 --> 00:34:08,240
traded off against each other

00:34:04,799 --> 00:34:09,599
and um applying something

00:34:08,240 --> 00:34:11,919
like a principle like justice for

00:34:09,599 --> 00:34:13,919
example of fairness um

00:34:11,919 --> 00:34:14,960
is very difficult to judge and i think

00:34:13,919 --> 00:34:16,399
including

00:34:14,960 --> 00:34:18,079
that level of analysis in a risk

00:34:16,399 --> 00:34:19,359
assessment is more or less um

00:34:18,079 --> 00:34:21,760
ineffectual

00:34:19,359 --> 00:34:22,720
um equally i think human rights impact

00:34:21,760 --> 00:34:24,879
assessments

00:34:22,720 --> 00:34:26,480
um provide a flaw but perhaps don't

00:34:24,879 --> 00:34:27,040
allow us to get to that level of detail

00:34:26,480 --> 00:34:29,040
around

00:34:27,040 --> 00:34:31,359
trade-offs around justice fairness um

00:34:29,040 --> 00:34:33,280
structural questions for example so

00:34:31,359 --> 00:34:35,200
um certainly the model yes whether or

00:34:33,280 --> 00:34:36,320
not we need some type of additional

00:34:35,200 --> 00:34:38,639
layer of consideration

00:34:36,320 --> 00:34:39,440
i would say we also need to think about

00:34:38,639 --> 00:34:42,560
form

00:34:39,440 --> 00:34:43,919
and function so i think i think owen was

00:34:42,560 --> 00:34:45,919
getting at this point when

00:34:43,919 --> 00:34:47,440
risk assessment is done by a company

00:34:45,919 --> 00:34:48,960
we're going to get one particular

00:34:47,440 --> 00:34:50,480
perspective on the world

00:34:48,960 --> 00:34:52,000
when it's done for example in a

00:34:50,480 --> 00:34:53,839
participatory way which involves

00:34:52,000 --> 00:34:55,200
affected users we might get a different

00:34:53,839 --> 00:34:57,680
set of insights so

00:34:55,200 --> 00:34:59,599
um to take go back to owen's example of

00:34:57,680 --> 00:35:01,359
the roller coaster in a theme park

00:34:59,599 --> 00:35:03,280
you know you could do a whole set of

00:35:01,359 --> 00:35:04,960
risk assessments but not necessarily

00:35:03,280 --> 00:35:05,839
think about people who are above seven

00:35:04,960 --> 00:35:07,520
feet tall and

00:35:05,839 --> 00:35:08,880
when they go on the roller coaster they

00:35:07,520 --> 00:35:11,119
have a whole different experience

00:35:08,880 --> 00:35:12,320
so do we know that that all affected

00:35:11,119 --> 00:35:14,960
communities are involved

00:35:12,320 --> 00:35:16,000
in weighing in to risks and how they

00:35:14,960 --> 00:35:17,599
perceive them and

00:35:16,000 --> 00:35:19,040
i think that involves rethinking a

00:35:17,599 --> 00:35:20,880
little bit the

00:35:19,040 --> 00:35:22,400
form and function of impact assessment

00:35:20,880 --> 00:35:24,480
to try to move towards something like a

00:35:22,400 --> 00:35:26,480
participatory or community based impact

00:35:24,480 --> 00:35:28,160
assessment process as well

00:35:26,480 --> 00:35:29,760
and else just in terms of how we sorry

00:35:28,160 --> 00:35:32,720
go ahead go on i just want to

00:35:29,760 --> 00:35:33,359
quickly uh um i think it's it's super

00:35:32,720 --> 00:35:35,760
relevant

00:35:33,359 --> 00:35:37,200
in our context if you look at other

00:35:35,760 --> 00:35:38,720
sectors where risk is a key

00:35:37,200 --> 00:35:40,400
focus take for instance the general

00:35:38,720 --> 00:35:42,880
insurance sector for say

00:35:40,400 --> 00:35:44,000
car insurance we have like almost 100

00:35:42,880 --> 00:35:47,119
years of data

00:35:44,000 --> 00:35:49,680
about how cars get involved in accidents

00:35:47,119 --> 00:35:51,440
and we know pretty well the various kind

00:35:49,680 --> 00:35:52,960
of contours of what those risks might

00:35:51,440 --> 00:35:55,760
look like

00:35:52,960 --> 00:35:57,280
with technology and with content sharing

00:35:55,760 --> 00:35:58,800
platforms and other services

00:35:57,280 --> 00:36:00,640
we are dealing with innovation that's

00:35:58,800 --> 00:36:02,000
moving so quickly with new product

00:36:00,640 --> 00:36:03,599
features that have been rolled out in

00:36:02,000 --> 00:36:05,440
different ways all the time

00:36:03,599 --> 00:36:07,839
that it's effectively impossible to make

00:36:05,440 --> 00:36:10,240
a long list of all the risks that might

00:36:07,839 --> 00:36:11,599
might be faced and so while as carly

00:36:10,240 --> 00:36:13,599
says we should set up kind of

00:36:11,599 --> 00:36:14,880
procedural standards and principles by

00:36:13,599 --> 00:36:17,040
which this should be

00:36:14,880 --> 00:36:18,880
um this should be assessed it's never

00:36:17,040 --> 00:36:21,520
going to be possible to identify

00:36:18,880 --> 00:36:23,119
all of the risks in a generalized format

00:36:21,520 --> 00:36:25,200
and it's never going to be possible for

00:36:23,119 --> 00:36:27,440
one entity in one place in time

00:36:25,200 --> 00:36:28,400
to adequately define them all and so

00:36:27,440 --> 00:36:30,480
that's why

00:36:28,400 --> 00:36:31,839
in our section more than any other is

00:36:30,480 --> 00:36:32,320
going to be incredibly important that

00:36:31,839 --> 00:36:35,359
this is

00:36:32,320 --> 00:36:37,200
transparent public facing and need

00:36:35,359 --> 00:36:38,560
participatory for for affected

00:36:37,200 --> 00:36:40,560
stakeholders

00:36:38,560 --> 00:36:42,240
thank you uh let's talk about

00:36:40,560 --> 00:36:43,040
advertising transparency provisions in

00:36:42,240 --> 00:36:45,599
the dsa

00:36:43,040 --> 00:36:48,320
ben what's your perception you know is

00:36:45,599 --> 00:36:48,320
the detail there

00:36:49,040 --> 00:36:51,599
i think it's

00:36:53,440 --> 00:36:59,280
simple and sweeping in the way that it

00:36:56,880 --> 00:37:02,320
describes the transparency requirements

00:36:59,280 --> 00:37:04,640
i think it will have to be customized on

00:37:02,320 --> 00:37:06,640
a platform by platform basis because

00:37:04,640 --> 00:37:09,359
the way the ad platforms work is

00:37:06,640 --> 00:37:13,119
different for different services

00:37:09,359 --> 00:37:15,119
i think provided there's flexibility to

00:37:13,119 --> 00:37:16,880
evolve the rules over time to match the

00:37:15,119 --> 00:37:18,480
development of advertising

00:37:16,880 --> 00:37:20,480
in this industry i think the

00:37:18,480 --> 00:37:21,440
transparency provisions are in principle

00:37:20,480 --> 00:37:24,240
correct

00:37:21,440 --> 00:37:26,079
so i'm i'm pretty happy with the way

00:37:24,240 --> 00:37:28,880
that they've addressed that issue

00:37:26,079 --> 00:37:29,680
um but i think you know it's something

00:37:28,880 --> 00:37:30,960
that

00:37:29,680 --> 00:37:32,400
is going to constantly change and

00:37:30,960 --> 00:37:32,960
regulators going to have to keep on top

00:37:32,400 --> 00:37:37,520
of it

00:37:32,960 --> 00:37:39,920
particularly if we find uh

00:37:37,520 --> 00:37:41,920
systematic evasion of rules in order to

00:37:39,920 --> 00:37:42,960
perpetrate more deceptive advertising

00:37:41,920 --> 00:37:44,320
practices through

00:37:42,960 --> 00:37:46,079
loopholes we've got to be able to

00:37:44,320 --> 00:37:46,960
rapidly address those what are the

00:37:46,079 --> 00:37:50,160
chances of that

00:37:46,960 --> 00:37:53,280
really who could imagine

00:37:50,160 --> 00:37:53,280
oh and what you taking this

00:37:54,480 --> 00:37:59,839
well so we we worked very hard on the

00:37:58,000 --> 00:38:01,680
ad transparency piece like along with

00:37:59,839 --> 00:38:03,040
many allies and in the community

00:38:01,680 --> 00:38:05,040
and i think that we were we were pretty

00:38:03,040 --> 00:38:06,720
happy like we kind of shared ben's spend

00:38:05,040 --> 00:38:07,200
sentiment it's kind of it's a clear and

00:38:06,720 --> 00:38:08,880
it's a

00:38:07,200 --> 00:38:10,560
it's a crisp framework for our

00:38:08,880 --> 00:38:11,920
transparency we actually dedicated quite

00:38:10,560 --> 00:38:13,599
a few as other mornings

00:38:11,920 --> 00:38:15,200
to kind of teasing out what that might

00:38:13,599 --> 00:38:16,240
look like so i think that's one of the

00:38:15,200 --> 00:38:18,960
areas where

00:38:16,240 --> 00:38:19,839
now we're thinking this is a pretty good

00:38:18,960 --> 00:38:22,079
piece of the

00:38:19,839 --> 00:38:23,599
of the puzzle let's see how we can how

00:38:22,079 --> 00:38:24,400
we can maintain it and possibly

00:38:23,599 --> 00:38:26,400
strengthen it so

00:38:24,400 --> 00:38:28,320
like as ben says it is more future proof

00:38:26,400 --> 00:38:29,280
going forward but i think the ad

00:38:28,320 --> 00:38:31,599
transparency

00:38:29,280 --> 00:38:33,200
piece is a really good example of where

00:38:31,599 --> 00:38:34,960
transparency is not going to be an end

00:38:33,200 --> 00:38:37,599
in itself like this is not going to

00:38:34,960 --> 00:38:39,760
solve the problem of disinformation or

00:38:37,599 --> 00:38:41,920
amplification of harmful content

00:38:39,760 --> 00:38:43,200
on certain platforms but it is a

00:38:41,920 --> 00:38:45,440
necessary prerequisite

00:38:43,200 --> 00:38:47,119
in the way that auditing in the more

00:38:45,440 --> 00:38:48,160
broader sense of the risk-based approach

00:38:47,119 --> 00:38:49,520
is also

00:38:48,160 --> 00:38:51,359
probably not going to be an end in

00:38:49,520 --> 00:38:53,119
itself but it is going to give us the

00:38:51,359 --> 00:38:54,880
tools and the insight by which we can

00:38:53,119 --> 00:38:55,520
address the kind of much more trickier

00:38:54,880 --> 00:38:57,920
problem

00:38:55,520 --> 00:38:59,760
of how do we how do we correct for

00:38:57,920 --> 00:39:01,359
certain problems in the tech sector and

00:38:59,760 --> 00:39:02,720
how do we sanction certain problems if

00:39:01,359 --> 00:39:05,040
they are they are systemic

00:39:02,720 --> 00:39:06,720
so yeah a good start but definitely not

00:39:05,040 --> 00:39:09,680
to be confused with uh with the

00:39:06,720 --> 00:39:11,359
the end of the line thank you carly a

00:39:09,680 --> 00:39:14,240
question from a very number gone

00:39:11,359 --> 00:39:15,680
uh verna good to see you online uh i'm

00:39:14,240 --> 00:39:17,200
going to knock her in your whole

00:39:15,680 --> 00:39:19,280
dissertation here just the the top

00:39:17,200 --> 00:39:21,200
question the same human rights offline

00:39:19,280 --> 00:39:22,640
need to be enforceable online

00:39:21,200 --> 00:39:24,560
what do the panelists think about the

00:39:22,640 --> 00:39:29,839
implications for the rule of law

00:39:24,560 --> 00:39:29,839
in the dsa carly first

00:39:30,320 --> 00:39:34,960
big question um um

00:39:33,359 --> 00:39:36,480
i don't know i don't know if i have a

00:39:34,960 --> 00:39:37,599
clear answer to that other than to

00:39:36,480 --> 00:39:40,160
underscore that

00:39:37,599 --> 00:39:42,000
this the dsa takes a long way takes us a

00:39:40,160 --> 00:39:44,400
long way forward in recognizing

00:39:42,000 --> 00:39:45,680
that we need um human rights aren't

00:39:44,400 --> 00:39:47,520
going to be protected online

00:39:45,680 --> 00:39:49,200
inherently but actually require

00:39:47,520 --> 00:39:49,920
enforcement structures regulatory

00:39:49,200 --> 00:39:51,440
structures and

00:39:49,920 --> 00:39:53,520
i think for a long time there's been

00:39:51,440 --> 00:39:54,800
some reticence around regulating the

00:39:53,520 --> 00:39:57,280
online space and that

00:39:54,800 --> 00:39:58,400
in part links to the the origins of the

00:39:57,280 --> 00:40:01,359
internet as a regular

00:39:58,400 --> 00:40:02,800
unregulated and fully democratized space

00:40:01,359 --> 00:40:03,520
but not under the control of any

00:40:02,800 --> 00:40:05,920
government

00:40:03,520 --> 00:40:07,359
i think the dsa takes us a long way away

00:40:05,920 --> 00:40:10,160
from that original vision but

00:40:07,359 --> 00:40:11,839
by necessity and i think closer to

00:40:10,160 --> 00:40:12,800
providing some sense of the rule of law

00:40:11,839 --> 00:40:14,240
online so

00:40:12,800 --> 00:40:15,920
you know from that perspective i think

00:40:14,240 --> 00:40:19,119
the dsa is a huge step

00:40:15,920 --> 00:40:22,000
forward um even with all of the caveats

00:40:19,119 --> 00:40:26,400
that we've expressed here today

00:40:22,000 --> 00:40:30,000
thank you ben well

00:40:26,400 --> 00:40:30,000
i think as the question implies

00:40:30,160 --> 00:40:32,880
in practice

00:40:33,280 --> 00:40:36,960
this law like any law you might write to

00:40:35,760 --> 00:40:39,599
govern

00:40:36,960 --> 00:40:40,400
the flow of information and content on

00:40:39,599 --> 00:40:44,079
the platform

00:40:40,400 --> 00:40:45,200
the size of these people will take

00:40:44,079 --> 00:40:47,920
advantage of the laws

00:40:45,200 --> 00:40:48,400
to seek their own ends as as government

00:40:47,920 --> 00:40:49,440
actors

00:40:48,400 --> 00:40:50,640
and that's going to be true in the

00:40:49,440 --> 00:40:53,200
european union it's going to be true

00:40:50,640 --> 00:40:55,040
around the world

00:40:53,200 --> 00:40:56,720
i think to some degree that is

00:40:55,040 --> 00:40:58,720
inevitable we can hedge against it as

00:40:56,720 --> 00:41:01,359
best we can we can write

00:40:58,720 --> 00:41:02,240
qualifications you can add language into

00:41:01,359 --> 00:41:04,960
the

00:41:02,240 --> 00:41:07,839
into the addenda to the law explaining

00:41:04,960 --> 00:41:07,839
what you mean

00:41:08,319 --> 00:41:10,640
areas

00:41:15,359 --> 00:41:19,680
be exploited there's one aspect of it

00:41:18,319 --> 00:41:21,040
however which i want to highlight which

00:41:19,680 --> 00:41:23,119
i think we haven't gotten to which i

00:41:21,040 --> 00:41:27,520
think is very important

00:41:23,119 --> 00:41:29,760
and that is the degree to which the dsa

00:41:27,520 --> 00:41:31,920
will be presidential

00:41:29,760 --> 00:41:33,040
i can tell you working in many capitals

00:41:31,920 --> 00:41:35,599
around the world

00:41:33,040 --> 00:41:37,520
how closely this document has been

00:41:35,599 --> 00:41:39,040
scrutinized by other governments by

00:41:37,520 --> 00:41:41,119
civil servants and ministries and

00:41:39,040 --> 00:41:43,200
regulatory agencies around the world

00:41:41,119 --> 00:41:45,280
because it is the first of its kind to

00:41:43,200 --> 00:41:47,359
take on these big questions like audit

00:41:45,280 --> 00:41:49,680
and risk mitigation to deal with

00:41:47,359 --> 00:41:50,560
not manifestly illegal but clearly

00:41:49,680 --> 00:41:53,760
harmful

00:41:50,560 --> 00:41:53,760
behavior on the platforms

00:41:54,000 --> 00:41:57,760
what the the the downside of dealing

00:41:56,400 --> 00:41:59,200
with companies that are

00:41:57,760 --> 00:42:00,480
the largest and wealthiest in the

00:41:59,200 --> 00:42:02,079
history of the world they're very

00:42:00,480 --> 00:42:05,119
difficult to regulate

00:42:02,079 --> 00:42:07,440
the upside is that they're everywhere

00:42:05,119 --> 00:42:09,599
and that means any country that

00:42:07,440 --> 00:42:11,680
implements any kind of rule

00:42:09,599 --> 00:42:14,079
that looks a little bit like the dsa

00:42:11,680 --> 00:42:17,040
after the dsa is in place

00:42:14,079 --> 00:42:18,960
any regulatory oversight of audit any

00:42:17,040 --> 00:42:21,119
independent third-party assessment of

00:42:18,960 --> 00:42:22,240
data that's put in a repository in a

00:42:21,119 --> 00:42:24,800
data sharing agreement

00:42:22,240 --> 00:42:25,599
similar to what the dsa proposes those

00:42:24,800 --> 00:42:29,680
all have the

00:42:25,599 --> 00:42:32,319
opportunity to turn up areas of

00:42:29,680 --> 00:42:32,880
problematic behavior of a technology

00:42:32,319 --> 00:42:35,119
that's come

00:42:32,880 --> 00:42:36,480
off the rails and produced a harm that

00:42:35,119 --> 00:42:38,480
government needs to address

00:42:36,480 --> 00:42:39,680
and when we see it anywhere we ought to

00:42:38,480 --> 00:42:42,000
be able to address it

00:42:39,680 --> 00:42:43,680
everywhere and in that way there's a

00:42:42,000 --> 00:42:45,920
kind of

00:42:43,680 --> 00:42:47,599
crowd government crowd sourcing that can

00:42:45,920 --> 00:42:49,200
occur when we're dealing with the same

00:42:47,599 --> 00:42:51,359
products and services that are applied

00:42:49,200 --> 00:42:52,319
in very similar ways in countries all

00:42:51,359 --> 00:42:54,400
over the world

00:42:52,319 --> 00:42:56,160
every reg no regulator will be able to

00:42:54,400 --> 00:42:57,520
address all the problems with an audit

00:42:56,160 --> 00:42:59,040
regime there simply won't be the

00:42:57,520 --> 00:43:00,160
competence in the scale of

00:42:59,040 --> 00:43:02,160
implementation

00:43:00,160 --> 00:43:04,480
but we don't necessarily need that if we

00:43:02,160 --> 00:43:06,000
can all be in a community of practice

00:43:04,480 --> 00:43:07,280
where we're sharing information about

00:43:06,000 --> 00:43:08,960
what was found

00:43:07,280 --> 00:43:10,480
how it was investigated how it was

00:43:08,960 --> 00:43:11,599
adjudicated and what the response from

00:43:10,480 --> 00:43:14,160
the company was

00:43:11,599 --> 00:43:14,880
because if you get a positive outcome

00:43:14,160 --> 00:43:16,720
one time

00:43:14,880 --> 00:43:18,319
and the company sinks the cost to change

00:43:16,720 --> 00:43:19,839
that product or service in order for

00:43:18,319 --> 00:43:20,800
that country to be satisfied with

00:43:19,839 --> 00:43:22,720
compliance

00:43:20,800 --> 00:43:24,720
they've already spent the money in order

00:43:22,720 --> 00:43:26,000
to fix that problem and the additional

00:43:24,720 --> 00:43:27,680
cost to deploy the same

00:43:26,000 --> 00:43:30,000
solution in other countries ought to be

00:43:27,680 --> 00:43:31,839
marginal that should be an advantage for

00:43:30,000 --> 00:43:32,400
us going forward if we can figure out a

00:43:31,839 --> 00:43:34,319
way

00:43:32,400 --> 00:43:39,760
to make this an international community

00:43:34,319 --> 00:43:41,520
of of regulatory dialogue

00:43:39,760 --> 00:43:43,599
um if i could just come in brian on um

00:43:41,520 --> 00:43:45,040
going back a bit to ivarina's question

00:43:43,599 --> 00:43:47,520
about the the kind of

00:43:45,040 --> 00:43:49,040
the the fundamental rights risks for

00:43:47,520 --> 00:43:49,920
free expression particularly from the

00:43:49,040 --> 00:43:52,000
dsa

00:43:49,920 --> 00:43:54,000
and i share her sentiment i think there

00:43:52,000 --> 00:43:54,640
is there is a lot of risks there in any

00:43:54,000 --> 00:43:56,319
regulation

00:43:54,640 --> 00:43:58,240
that that's that's governing online

00:43:56,319 --> 00:44:00,480
speech however one

00:43:58,240 --> 00:44:02,240
one kind of comfort i draw from reading

00:44:00,480 --> 00:44:04,079
the draft text is that

00:44:02,240 --> 00:44:05,920
there is an i think there's a pretty

00:44:04,079 --> 00:44:08,079
strong way and there's a pretty clear

00:44:05,920 --> 00:44:10,240
way of implementing the dsa

00:44:08,079 --> 00:44:11,119
which doesn't fall foul of some of those

00:44:10,240 --> 00:44:12,880
problems

00:44:11,119 --> 00:44:15,839
and this is in contrast with say for

00:44:12,880 --> 00:44:18,079
instance in the last eu public mandate

00:44:15,839 --> 00:44:19,359
where we had the the copyright directive

00:44:18,079 --> 00:44:20,240
and we had the terrorist content

00:44:19,359 --> 00:44:22,640
regulation

00:44:20,240 --> 00:44:24,079
where both of those files were really

00:44:22,640 --> 00:44:26,880
aimed at trying to

00:44:24,079 --> 00:44:28,240
get as much content off the web as

00:44:26,880 --> 00:44:30,000
quickly as possible

00:44:28,240 --> 00:44:31,839
and leaving the adjudication as to

00:44:30,000 --> 00:44:33,599
whether that content was illegal or

00:44:31,839 --> 00:44:36,079
harmful or public interest

00:44:33,599 --> 00:44:36,880
to after the fact and if it was even

00:44:36,079 --> 00:44:39,280
going to be made

00:44:36,880 --> 00:44:41,280
at all whereas i think with the dsa by

00:44:39,280 --> 00:44:43,920
shifting the focus away from

00:44:41,280 --> 00:44:44,560
content per se and really on to the

00:44:43,920 --> 00:44:46,160
question of

00:44:44,560 --> 00:44:48,640
what does kind of higher order

00:44:46,160 --> 00:44:50,319
responsibility look like from platforms

00:44:48,640 --> 00:44:51,920
what are the kind of process that they

00:44:50,319 --> 00:44:53,760
should have in place what are the kind

00:44:51,920 --> 00:44:55,280
of procedures they should have in place

00:44:53,760 --> 00:44:56,800
then there's a way of doing this which

00:44:55,280 --> 00:44:58,560
is a little bit less

00:44:56,800 --> 00:45:00,400
rights interfering because it shifts the

00:44:58,560 --> 00:45:01,280
incentive it shifts the incentives more

00:45:00,400 --> 00:45:03,280
towards

00:45:01,280 --> 00:45:05,359
the companies themselves and less onto

00:45:03,280 --> 00:45:06,079
regulating the speech and regulating the

00:45:05,359 --> 00:45:09,760
behavior

00:45:06,079 --> 00:45:09,760
of those who are using their services

00:45:12,319 --> 00:45:18,560
i think we lost brian for a second we

00:45:13,920 --> 00:45:21,359
lost brian maybe we should uh

00:45:18,560 --> 00:45:22,319
guys sorry big online platform and we're

00:45:21,359 --> 00:45:25,520
back

00:45:22,319 --> 00:45:27,119
so thank you for that ben um carly

00:45:25,520 --> 00:45:28,720
on this as well i don't picture too much

00:45:27,119 --> 00:45:30,720
on on on the

00:45:28,720 --> 00:45:32,079
the advertising side but and on this

00:45:30,720 --> 00:45:34,319
information in terms of how

00:45:32,079 --> 00:45:35,920
the the dsa would be perceived in the uk

00:45:34,319 --> 00:45:40,240
or how the uk perceives it what

00:45:35,920 --> 00:45:40,240
lessons can be translated here as well

00:45:41,040 --> 00:45:46,240
um i think that the uk's regime the uk

00:45:44,720 --> 00:45:49,040
is pursuing its own regime

00:45:46,240 --> 00:45:51,200
around online safety there's a lot of

00:45:49,040 --> 00:45:51,839
similarities in terms of the mechanisms

00:45:51,200 --> 00:45:54,960
for

00:45:51,839 --> 00:45:57,440
um audit and accountability

00:45:54,960 --> 00:45:58,079
i think there'll be quite a lot of skill

00:45:57,440 --> 00:46:00,240
sharing

00:45:58,079 --> 00:46:01,920
knowledge sharing i would hope to see in

00:46:00,240 --> 00:46:03,280
the coming days the uk is taking a

00:46:01,920 --> 00:46:06,720
slightly different framing

00:46:03,280 --> 00:46:09,760
around the duty of care um versus the

00:46:06,720 --> 00:46:10,079
dsa's approach but i think you know at a

00:46:09,760 --> 00:46:12,800
top

00:46:10,079 --> 00:46:14,560
level i would say the the two

00:46:12,800 --> 00:46:15,280
jurisdictions are moving in a similar

00:46:14,560 --> 00:46:18,240
direction

00:46:15,280 --> 00:46:19,599
and you know i'm sure we could be um

00:46:18,240 --> 00:46:20,400
nitpicking the differences between the

00:46:19,599 --> 00:46:22,319
two but i'm not

00:46:20,400 --> 00:46:23,839
i'm not sure that that's particularly

00:46:22,319 --> 00:46:25,040
useful because i think that overriding

00:46:23,839 --> 00:46:27,520
messages

00:46:25,040 --> 00:46:28,240
um to ben's point we're moving in a in a

00:46:27,520 --> 00:46:30,240
direction

00:46:28,240 --> 00:46:31,359
that's going to have the effect of

00:46:30,240 --> 00:46:32,480
having much more accountability on

00:46:31,359 --> 00:46:34,160
platforms and

00:46:32,480 --> 00:46:35,760
preventing the kind of whack-a-mole

00:46:34,160 --> 00:46:37,359
problem where um

00:46:35,760 --> 00:46:38,800
platforms could uh seek other

00:46:37,359 --> 00:46:41,680
jurisdictions to kind of take

00:46:38,800 --> 00:46:42,640
safe harbor in um so i think broadly

00:46:41,680 --> 00:46:44,720
speaking

00:46:42,640 --> 00:46:46,160
similar messages that we've discussed

00:46:44,720 --> 00:46:47,520
today and

00:46:46,160 --> 00:46:49,599
uh a lot of it will be around

00:46:47,520 --> 00:46:51,359
implementation i would say the uk is

00:46:49,599 --> 00:46:52,880
perhaps in a slightly better position

00:46:51,359 --> 00:46:54,839
when it comes to

00:46:52,880 --> 00:46:56,720
uh implementation through regulators and

00:46:54,839 --> 00:46:58,160
regulatory capacity that

00:46:56,720 --> 00:47:00,160
the information commissioner's office

00:46:58,160 --> 00:47:02,079
has always been one of the most active

00:47:00,160 --> 00:47:04,160
regulators and well-resourced and i mean

00:47:02,079 --> 00:47:05,839
the two are absolutely

00:47:04,160 --> 00:47:08,400
yes why is it well resourced though why

00:47:05,839 --> 00:47:10,319
did they choose to to resource this uh

00:47:08,400 --> 00:47:12,640
from from the start oh that's a really

00:47:10,319 --> 00:47:14,000
good question i i'm i'm not sure i can

00:47:12,640 --> 00:47:15,839
speak to that i think there's probably

00:47:14,000 --> 00:47:19,280
some historical um

00:47:15,839 --> 00:47:20,559
and cultural uh investment in freedom of

00:47:19,280 --> 00:47:22,480
information in the uk

00:47:20,559 --> 00:47:25,040
and kind of open access to information

00:47:22,480 --> 00:47:27,440
that the uk really does hold itself out

00:47:25,040 --> 00:47:28,960
as a as a leader in open government and

00:47:27,440 --> 00:47:29,760
access to information for a really long

00:47:28,960 --> 00:47:32,480
time

00:47:29,760 --> 00:47:34,800
um and the ico has you know is one of

00:47:32,480 --> 00:47:37,599
the largest in number and well-resourced

00:47:34,800 --> 00:47:39,040
regulators in the uk but it should also

00:47:37,599 --> 00:47:40,400
be pointed out that there are other

00:47:39,040 --> 00:47:42,240
regulators here that have

00:47:40,400 --> 00:47:43,920
capacity that they're really bringing to

00:47:42,240 --> 00:47:45,680
the table and expertise not least as the

00:47:43,920 --> 00:47:48,000
competition markets authority which

00:47:45,680 --> 00:47:49,359
is arguably even the head of the ico in

00:47:48,000 --> 00:47:50,720
the uk when it comes to thinking about

00:47:49,359 --> 00:47:52,079
audited platforms

00:47:50,720 --> 00:47:54,400
because they're thinking about it in the

00:47:52,079 --> 00:47:55,599
context of pricing algorithms

00:47:54,400 --> 00:47:58,960
competition

00:47:55,599 --> 00:48:00,640
um and uh and of course in the uk

00:47:58,960 --> 00:48:02,240
context the regulator

00:48:00,640 --> 00:48:04,160
that will be charged with implementation

00:48:02,240 --> 00:48:05,200
of online safety is neither of those two

00:48:04,160 --> 00:48:06,800
regulators but is

00:48:05,200 --> 00:48:08,240
um ofcom which is the media and

00:48:06,800 --> 00:48:09,520
communications regulator

00:48:08,240 --> 00:48:11,359
but what we're seeing is really

00:48:09,520 --> 00:48:13,040
interesting collaboration between the

00:48:11,359 --> 00:48:14,559
three of those regulators

00:48:13,040 --> 00:48:16,559
under the auspices of something called

00:48:14,559 --> 00:48:18,640
the digital market unit which has been

00:48:16,559 --> 00:48:19,760
established to to encourage

00:48:18,640 --> 00:48:21,359
collaboration

00:48:19,760 --> 00:48:23,359
i also think that there's a lot to learn

00:48:21,359 --> 00:48:25,440
from the financial sector's regulation

00:48:23,359 --> 00:48:27,440
in the uk and of course the uk being the

00:48:25,440 --> 00:48:29,680
biggest financial services market

00:48:27,440 --> 00:48:31,440
and having to have having done a lot in

00:48:29,680 --> 00:48:34,400
the way of financial services

00:48:31,440 --> 00:48:35,760
um regulatory reform since 2008 i think

00:48:34,400 --> 00:48:37,200
there's probably some lessons to be

00:48:35,760 --> 00:48:39,680
learned there as well so

00:48:37,200 --> 00:48:41,440
it's i think from a kind of

00:48:39,680 --> 00:48:41,920
implementation perspective there's a lot

00:48:41,440 --> 00:48:43,839
of

00:48:41,920 --> 00:48:45,119
real opportunity here to develop some

00:48:43,839 --> 00:48:46,640
best practices

00:48:45,119 --> 00:48:48,160
um and hopefully there'll be some

00:48:46,640 --> 00:48:49,680
connections in with um

00:48:48,160 --> 00:48:52,160
regulators on the continent as that

00:48:49,680 --> 00:48:54,559
happens i think it's interesting

00:48:52,160 --> 00:48:55,440
that uh collaboration between different

00:48:54,559 --> 00:48:58,079
regulators

00:48:55,440 --> 00:48:58,480
is seen as necessary or possible now and

00:48:58,079 --> 00:48:59,920
that

00:48:58,480 --> 00:49:01,839
this information is not separate from

00:48:59,920 --> 00:49:02,640
competition law to a large degree not

00:49:01,839 --> 00:49:03,920
anymore either

00:49:02,640 --> 00:49:05,680
we have two more questions thank you

00:49:03,920 --> 00:49:06,240
carly two more questions here we're

00:49:05,680 --> 00:49:08,800
coming

00:49:06,240 --> 00:49:10,559
uh nine minutes left so if you have

00:49:08,800 --> 00:49:13,520
questions please send them right now

00:49:10,559 --> 00:49:15,680
claudia pretner and she is the legal

00:49:13,520 --> 00:49:17,599
policy advisor at amnesty international

00:49:15,680 --> 00:49:19,200
claudia asks coming back to the topic of

00:49:17,599 --> 00:49:19,680
advertising only you want to take this

00:49:19,200 --> 00:49:21,200
one

00:49:19,680 --> 00:49:23,040
do the speakers think targeted

00:49:21,200 --> 00:49:24,720
advertising needs to be regulated more

00:49:23,040 --> 00:49:26,640
strictly as called for by the european

00:49:24,720 --> 00:49:28,800
parliament and the edps

00:49:26,640 --> 00:49:31,119
and could this help mitigate some of the

00:49:28,800 --> 00:49:33,680
risks on

00:49:31,119 --> 00:49:35,520
yeah on this one i think it's so this is

00:49:33,680 --> 00:49:37,200
like definitely a topic which has

00:49:35,520 --> 00:49:39,119
grown in a lot of attention the last

00:49:37,200 --> 00:49:41,440
couple of couple of months and

00:49:39,119 --> 00:49:43,119
year and i think it's it's interesting

00:49:41,440 --> 00:49:45,200
because it speaks to a lot of concern

00:49:43,119 --> 00:49:46,800
you see in the european parliament that

00:49:45,200 --> 00:49:48,319
it's some of the problems we're seeing

00:49:46,800 --> 00:49:51,119
in the online space

00:49:48,319 --> 00:49:52,079
are a result of of ad driven business

00:49:51,119 --> 00:49:54,960
models

00:49:52,079 --> 00:49:56,480
um and so the idea is that for instance

00:49:54,960 --> 00:49:58,079
the business models have a commercial

00:49:56,480 --> 00:50:00,079
incentive to promote content

00:49:58,079 --> 00:50:01,920
which can be which can be inflammatory

00:50:00,079 --> 00:50:02,480
which can be polar polarizing and which

00:50:01,920 --> 00:50:04,800
can be

00:50:02,480 --> 00:50:05,839
otherwise objectionable and so i think

00:50:04,800 --> 00:50:07,760
the dsa

00:50:05,839 --> 00:50:09,440
as it's currently drafted is going to

00:50:07,760 --> 00:50:11,920
address quite a few of those things

00:50:09,440 --> 00:50:13,839
if it's implemented properly in terms of

00:50:11,920 --> 00:50:15,599
there is a lot of um a lot of language

00:50:13,839 --> 00:50:17,920
in there about looking at how

00:50:15,599 --> 00:50:19,280
content moderation content recommender

00:50:17,920 --> 00:50:21,359
systems are defined

00:50:19,280 --> 00:50:23,040
and looking at how advertising is served

00:50:21,359 --> 00:50:26,079
so i think that the dsa is already

00:50:23,040 --> 00:50:27,520
looking closely at this um there's

00:50:26,079 --> 00:50:29,920
probably ways it could it could

00:50:27,520 --> 00:50:31,040
go in other directions i think what will

00:50:29,920 --> 00:50:34,079
be interesting is

00:50:31,040 --> 00:50:35,680
seeing to what extent um many of the

00:50:34,079 --> 00:50:37,920
problems which people are talking about

00:50:35,680 --> 00:50:39,359
in the online advertising space

00:50:37,920 --> 00:50:41,839
are going to be solved or would be

00:50:39,359 --> 00:50:44,079
solved by just a better enforcement and

00:50:41,839 --> 00:50:46,000
implementation of things like the gdpr

00:50:44,079 --> 00:50:48,000
because at the moment going back to what

00:50:46,000 --> 00:50:49,599
was said earlier there's a lot of pieces

00:50:48,000 --> 00:50:50,480
of that law which haven't been tried out

00:50:49,599 --> 00:50:53,119
very well

00:50:50,480 --> 00:50:54,079
um but i think that like i expect this

00:50:53,119 --> 00:50:56,079
this conversation

00:50:54,079 --> 00:50:57,839
around the role of advertising and

00:50:56,079 --> 00:50:59,680
whether and to what extent it can and

00:50:57,839 --> 00:51:00,240
should be banned or mitigated will will

00:50:59,680 --> 00:51:01,920
probably

00:51:00,240 --> 00:51:03,440
get even more more prominent in the

00:51:01,920 --> 00:51:07,359
debate so yeah

00:51:03,440 --> 00:51:10,480
thank you ben um

00:51:07,359 --> 00:51:12,160
this uh one of the parties

00:51:10,480 --> 00:51:14,400
groups in the parliament just introduced

00:51:12,160 --> 00:51:14,800
a new campaign to ban personalized ads

00:51:14,400 --> 00:51:16,960
and

00:51:14,800 --> 00:51:18,800
that's an idea kicking around in amongst

00:51:16,960 --> 00:51:21,599
different organizations in washington

00:51:18,800 --> 00:51:23,200
as well and i think the the spirit of

00:51:21,599 --> 00:51:26,960
the idea

00:51:23,200 --> 00:51:29,599
is an appealing one that if we prohibit

00:51:26,960 --> 00:51:30,599
the targeting of ads that this might

00:51:29,599 --> 00:51:33,760
lead to

00:51:30,599 --> 00:51:37,040
disincentivizing profiling

00:51:33,760 --> 00:51:38,800
and the curation of content in ways that

00:51:37,040 --> 00:51:40,720
separates one group from another or the

00:51:38,800 --> 00:51:44,000
public from facts or

00:51:40,720 --> 00:51:47,599
many other negative externalities of

00:51:44,000 --> 00:51:49,359
uh customized content curation

00:51:47,599 --> 00:51:51,280
but i think it is a misunderstanding of

00:51:49,359 --> 00:51:52,160
how the ad industry works and i think it

00:51:51,280 --> 00:51:54,800
is

00:51:52,160 --> 00:51:55,440
it's a simplification that you can

00:51:54,800 --> 00:51:57,440
define

00:51:55,440 --> 00:51:59,280
a class of advertising practices that

00:51:57,440 --> 00:51:59,920
are prohibited and they will be easily

00:51:59,280 --> 00:52:02,079
evaded

00:51:59,920 --> 00:52:04,480
there are 100 different ways to

00:52:02,079 --> 00:52:06,480
customize and target an ad

00:52:04,480 --> 00:52:07,520
and i think what we ought to be talking

00:52:06,480 --> 00:52:09,200
about

00:52:07,520 --> 00:52:11,040
is the root of the problem which is the

00:52:09,200 --> 00:52:14,319
profiling in the first place

00:52:11,040 --> 00:52:16,480
if you want to get at the problem of

00:52:14,319 --> 00:52:18,000
targeted curation either in paid or

00:52:16,480 --> 00:52:20,240
organic content why not talk

00:52:18,000 --> 00:52:21,680
about the collection and use of data in

00:52:20,240 --> 00:52:23,040
the ways that permits this

00:52:21,680 --> 00:52:25,040
rather than trying to get at it through

00:52:23,040 --> 00:52:27,520
the practice of targeted ads

00:52:25,040 --> 00:52:29,520
which brings me back to the gdpr being

00:52:27,520 --> 00:52:32,720
something someone should really try

00:52:29,520 --> 00:52:34,559
because in my view this is already

00:52:32,720 --> 00:52:35,599
prohibited under the law we just haven't

00:52:34,559 --> 00:52:39,119
addressed this

00:52:35,599 --> 00:52:39,440
directly through the gdpr and well i

00:52:39,119 --> 00:52:41,760
think

00:52:39,440 --> 00:52:42,559
it is reasonable to use a discussion

00:52:41,760 --> 00:52:44,839
about

00:52:42,559 --> 00:52:46,400
about targeted advertising behavioral

00:52:44,839 --> 00:52:48,800
advertising

00:52:46,400 --> 00:52:50,559
to open this debate i don't think we

00:52:48,800 --> 00:52:54,160
should fool ourselves that we can

00:52:50,559 --> 00:52:56,480
fix it by strengthening regulations on a

00:52:54,160 --> 00:52:58,240
particular type of ad

00:52:56,480 --> 00:52:59,920
thank you carly we've talked a lot about

00:52:58,240 --> 00:53:01,520
government about regulators

00:52:59,920 --> 00:53:03,440
what role does the public have in terms

00:53:01,520 --> 00:53:06,480
of regulating platforms

00:53:03,440 --> 00:53:10,319
in this way that's a

00:53:06,480 --> 00:53:12,079
great question i think um we're

00:53:10,319 --> 00:53:13,839
starting to understand the need for

00:53:12,079 --> 00:53:16,960
public consultation and public

00:53:13,839 --> 00:53:18,800
involvement in the development of

00:53:16,960 --> 00:53:20,240
real accountability mechanisms around

00:53:18,800 --> 00:53:22,480
large platforms

00:53:20,240 --> 00:53:24,480
um what i would like to see more of is

00:53:22,480 --> 00:53:26,319
more participatory approaches where we

00:53:24,480 --> 00:53:28,960
really bring in members of the public

00:53:26,319 --> 00:53:30,480
to first do that kind of values

00:53:28,960 --> 00:53:33,119
definition piece that you spoke about

00:53:30,480 --> 00:53:35,200
brian what exactly are the values how as

00:53:33,119 --> 00:53:36,319
as societies do we want to balance and

00:53:35,200 --> 00:53:39,839
trade them off

00:53:36,319 --> 00:53:40,720
and secondly to involve the public in

00:53:39,839 --> 00:53:42,400
these types of

00:53:40,720 --> 00:53:43,920
undeveloping interventions in a

00:53:42,400 --> 00:53:46,079
necessary space

00:53:43,920 --> 00:53:47,200
um you know there's a lot of lip service

00:53:46,079 --> 00:53:48,880
paid to

00:53:47,200 --> 00:53:51,280
for example people don't care about

00:53:48,880 --> 00:53:53,680
privacy because they still use facebook

00:53:51,280 --> 00:53:55,359
and really digging into what you know to

00:53:53,680 --> 00:53:57,119
what extent do people feel agency and

00:53:55,359 --> 00:53:59,040
choice in the online domain

00:53:57,119 --> 00:54:00,880
and what would how would they design a

00:53:59,040 --> 00:54:03,440
system differently i think could really

00:54:00,880 --> 00:54:03,920
be quite fruitful in trying to develop

00:54:03,440 --> 00:54:06,400
better

00:54:03,920 --> 00:54:07,599
you know approaches to to for example

00:54:06,400 --> 00:54:08,160
the use of personal data and the way

00:54:07,599 --> 00:54:11,359
that ben

00:54:08,160 --> 00:54:12,800
described um and to rebalancing the

00:54:11,359 --> 00:54:14,319
power i think most people feel so

00:54:12,800 --> 00:54:17,119
powerless in the face of

00:54:14,319 --> 00:54:18,240
um uh the online platforms and they're

00:54:17,119 --> 00:54:20,559
they can't imagine

00:54:18,240 --> 00:54:22,160
different futures at the moment um

00:54:20,559 --> 00:54:23,760
beyond ones in which they have to use

00:54:22,160 --> 00:54:24,480
google and facebook but as the recent

00:54:23,760 --> 00:54:26,640
developments in

00:54:24,480 --> 00:54:27,839
australia have shown us you know it may

00:54:26,640 --> 00:54:29,359
it may be something that we can

00:54:27,839 --> 00:54:31,040
need to confront sooner rather than

00:54:29,359 --> 00:54:33,200
later so trying to

00:54:31,040 --> 00:54:34,960
bring as wide array of people into that

00:54:33,200 --> 00:54:37,200
conversation as possible i think is

00:54:34,960 --> 00:54:39,119
really important and not least because

00:54:37,200 --> 00:54:40,559
you know the silicon valley mindset has

00:54:39,119 --> 00:54:41,040
dictated the way these platforms have

00:54:40,559 --> 00:54:44,240
been

00:54:41,040 --> 00:54:46,559
um uh built and now as to ben's point

00:54:44,240 --> 00:54:48,400
earlier they are everywhere and they are

00:54:46,559 --> 00:54:50,160
being embedded in different cultural

00:54:48,400 --> 00:54:52,079
ways in different societies so

00:54:50,160 --> 00:54:53,200
starting to to really understand what

00:54:52,079 --> 00:54:55,680
that means i think

00:54:53,200 --> 00:54:56,960
is should be a priority for us actually

00:54:55,680 --> 00:54:58,559
thank you oh and you want to follow on

00:54:56,960 --> 00:55:00,640
that

00:54:58,559 --> 00:55:02,640
um i just i i saw there's a there's a

00:55:00,640 --> 00:55:04,319
question in the in the q a from

00:55:02,640 --> 00:55:05,920
gabrielle from article 19 which i think

00:55:04,319 --> 00:55:07,280
it's really interesting i think that

00:55:05,920 --> 00:55:10,319
it's it's something that hasn't

00:55:07,280 --> 00:55:12,640
come up yet is the idea of

00:55:10,319 --> 00:55:14,640
the interference with fundamental rights

00:55:12,640 --> 00:55:15,920
of of the risk assessment process and

00:55:14,640 --> 00:55:17,839
how governments in

00:55:15,920 --> 00:55:19,359
potentially undemocratic countries might

00:55:17,839 --> 00:55:22,960
lean on um

00:55:19,359 --> 00:55:24,720
on actors to to suppress undesirable or

00:55:22,960 --> 00:55:26,559
politically sensitive speech

00:55:24,720 --> 00:55:27,920
and i think this is this is going to be

00:55:26,559 --> 00:55:30,160
a challenge that we

00:55:27,920 --> 00:55:31,839
that we we can address to some extent in

00:55:30,160 --> 00:55:33,119
the law but it's going to require that

00:55:31,839 --> 00:55:33,760
constant monitoring and it's going to

00:55:33,119 --> 00:55:36,319
require

00:55:33,760 --> 00:55:37,359
a lot of again this kind of polycentric

00:55:36,319 --> 00:55:39,839
oversight of the

00:55:37,359 --> 00:55:40,480
of the regulatory system i think in any

00:55:39,839 --> 00:55:43,520
case

00:55:40,480 --> 00:55:45,119
we shouldn't be thinking purely about

00:55:43,520 --> 00:55:46,960
content so we shouldn't be thinking

00:55:45,119 --> 00:55:48,880
about what is the risk of

00:55:46,960 --> 00:55:50,720
disinformation what is the risk of hate

00:55:48,880 --> 00:55:53,280
speech on your on your platform

00:55:50,720 --> 00:55:54,160
but rather thinking more in the sense of

00:55:53,280 --> 00:55:56,319
operations

00:55:54,160 --> 00:55:58,000
so really embedding the risk assessment

00:55:56,319 --> 00:55:59,760
piece into

00:55:58,000 --> 00:56:01,440
when a company is thinking how it's

00:55:59,760 --> 00:56:03,359
going to roll at a product

00:56:01,440 --> 00:56:06,000
what markets is going to deploy that

00:56:03,359 --> 00:56:07,359
product in what kind of trust and safety

00:56:06,000 --> 00:56:09,520
functions it's going to have

00:56:07,359 --> 00:56:12,240
embedded into the product rollout to

00:56:09,520 --> 00:56:12,720
mitigate the money and manifold risks

00:56:12,240 --> 00:56:15,119
that may

00:56:12,720 --> 00:56:16,079
that may pop up so i think that in the

00:56:15,119 --> 00:56:18,480
first instance

00:56:16,079 --> 00:56:20,480
we shouldn't limit ourselves to like

00:56:18,480 --> 00:56:21,839
categorizing specific risks rather we

00:56:20,480 --> 00:56:22,880
should make sure it's a fluid and it's a

00:56:21,839 --> 00:56:24,880
constantly

00:56:22,880 --> 00:56:26,960
iterative process and i think in that

00:56:24,880 --> 00:56:28,559
regard and the tech sector probably has

00:56:26,960 --> 00:56:29,760
a bit of an advantage because one of the

00:56:28,559 --> 00:56:32,640
reasons that's made

00:56:29,760 --> 00:56:35,200
tech so successful is a philosophy of

00:56:32,640 --> 00:56:37,280
constant user testing constant mode

00:56:35,200 --> 00:56:38,240
monitoring and iterative feedback in

00:56:37,280 --> 00:56:39,680
product design

00:56:38,240 --> 00:56:41,280
and that is at the core of risk

00:56:39,680 --> 00:56:42,720
management too so i think that if we

00:56:41,280 --> 00:56:44,640
take that philosophy of how

00:56:42,720 --> 00:56:46,559
we build and implement our products

00:56:44,640 --> 00:56:47,119
towards how we assess and mitigate the

00:56:46,559 --> 00:56:49,200
risks

00:56:47,119 --> 00:56:50,720
then i think the dsa will probably be a

00:56:49,200 --> 00:56:51,359
good step forward without some of the

00:56:50,720 --> 00:56:53,599
negative

00:56:51,359 --> 00:56:54,960
fundamental rights potentials i was

00:56:53,599 --> 00:56:56,240
going to ask you for a final comment but

00:56:54,960 --> 00:56:58,000
i think that will serve as your final

00:56:56,240 --> 00:56:58,559
comment and we're one minute to go thank

00:56:58,000 --> 00:57:01,280
you

00:56:58,559 --> 00:57:02,880
and carly and ben just a 30 second wrap

00:57:01,280 --> 00:57:03,680
up and just before you do we have a

00:57:02,880 --> 00:57:05,760
comment from

00:57:03,680 --> 00:57:06,960
colin machill thank you uh thank you for

00:57:05,760 --> 00:57:07,760
this fruitful discussion excellent

00:57:06,960 --> 00:57:09,680
insight

00:57:07,760 --> 00:57:11,599
and key regulatory policy issues raised

00:57:09,680 --> 00:57:13,200
you're very welcome uh adeline conboy

00:57:11,599 --> 00:57:14,000
sorry we can get your question on the

00:57:13,200 --> 00:57:15,680
irish

00:57:14,000 --> 00:57:17,280
safety and media bill we'll follow up on

00:57:15,680 --> 00:57:17,680
that afterwards and to laura bartley

00:57:17,280 --> 00:57:19,359
also

00:57:17,680 --> 00:57:20,720
i will try and get your response and

00:57:19,359 --> 00:57:22,240
just after as well thank you to all

00:57:20,720 --> 00:57:26,960
those who sent in their questions

00:57:22,240 --> 00:57:29,599
uh wrap up remark carly ben carly first

00:57:26,960 --> 00:57:30,960
um i don't think i have anything

00:57:29,599 --> 00:57:32,319
particularly to say other than i

00:57:30,960 --> 00:57:34,079
appreciate this conversation being

00:57:32,319 --> 00:57:36,160
convened and it's great to see

00:57:34,079 --> 00:57:37,440
uh the opportunity to kind of delve into

00:57:36,160 --> 00:57:38,640
the details of this and i hope there'll

00:57:37,440 --> 00:57:42,079
be more of them

00:57:38,640 --> 00:57:44,720
thank you ben last word i want to

00:57:42,079 --> 00:57:47,119
end on a positive note and say as this

00:57:44,720 --> 00:57:48,720
conversation has made clear

00:57:47,119 --> 00:57:50,480
this is going to be a messy debate over

00:57:48,720 --> 00:57:51,280
the months ahead even though i think

00:57:50,480 --> 00:57:53,599
there's

00:57:51,280 --> 00:57:55,520
a strong support for the dsa in the

00:57:53,599 --> 00:57:56,480
parliament strong support for the dsa in

00:57:55,520 --> 00:57:59,359
the council

00:57:56,480 --> 00:58:00,400
there's an overriding momentum for these

00:57:59,359 --> 00:58:01,839
kinds of rules

00:58:00,400 --> 00:58:04,079
not just in the eu but in other

00:58:01,839 --> 00:58:06,240
democratic countries

00:58:04,079 --> 00:58:08,400
how to do it and in order to be

00:58:06,240 --> 00:58:10,480
maximally effective will continue to be

00:58:08,400 --> 00:58:11,440
a challenge but i think we have to keep

00:58:10,480 --> 00:58:13,760
our eye on the prize

00:58:11,440 --> 00:58:15,119
like just the fact that this law has

00:58:13,760 --> 00:58:16,640
been proposed that we're having these

00:58:15,119 --> 00:58:17,920
debates that we're trying to figure out

00:58:16,640 --> 00:58:19,280
how to get these problems through

00:58:17,920 --> 00:58:22,160
different mechanisms

00:58:19,280 --> 00:58:23,119
will begin to change the industry and

00:58:22,160 --> 00:58:24,640
that's a

00:58:23,119 --> 00:58:26,880
that's a really important step in the

00:58:24,640 --> 00:58:30,079
right direction and we need to

00:58:26,880 --> 00:58:30,079
carry on down this path

00:58:30,240 --> 00:58:32,480
okay

00:58:34,400 --> 00:58:38,559
thank you ben it's uh froze there for a

00:58:36,799 --> 00:58:40,160
second again as well carly kind

00:58:38,559 --> 00:58:41,680
ben scott bennett thank you so much for

00:58:40,160 --> 00:58:42,960
participation today thanks to all our

00:58:41,680 --> 00:58:45,280
audience as well for

00:58:42,960 --> 00:58:46,559
uh your questions and your attention and

00:58:45,280 --> 00:58:48,559
uh join us for the next

00:58:46,559 --> 00:58:51,839
mozilla morning coming soon i wish you

00:58:48,559 --> 00:58:51,839

YouTube URL: https://www.youtube.com/watch?v=tEDJ3nx88MM


