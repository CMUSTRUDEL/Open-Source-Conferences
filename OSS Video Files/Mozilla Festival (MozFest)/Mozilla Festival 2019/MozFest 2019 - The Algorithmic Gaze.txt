Title: MozFest 2019 - The Algorithmic Gaze
Publication date: 2019-11-04
Playlist: Mozilla Festival 2019
Description: 
	Algorithms spread bias at gigabit speeds. What does this mean for society? And who can fix it?

Guillaume Chaslot - @gchaslot
Founder of AlgoTransparency, former software engineer at Google/YouTube, and Mozilla Fellow

Cansu Canca - @ccansu
Founder and Director, AI Ethics Lab
Captions: 
	00:00:00,030 --> 00:00:06,629
so up next we have two panelists talking

00:00:03,720 --> 00:00:08,550
with me about the algorithmic gaze so

00:00:06,629 --> 00:00:10,800
obviously people make algorithms and

00:00:08,550 --> 00:00:12,870
people are really crappy so they're

00:00:10,800 --> 00:00:14,639
crapping this enters those algorithms so

00:00:12,870 --> 00:00:16,890
we're going to talk about how bias can

00:00:14,639 --> 00:00:20,220
enter algorithms so I'd like to invite

00:00:16,890 --> 00:00:22,920
up on stage Geum and kanjou are they

00:00:20,220 --> 00:00:31,560
back there oh here they are here give

00:00:22,920 --> 00:00:38,820
him around applause guys done pretty

00:00:31,560 --> 00:00:41,579
good so Johnson already so John sue John

00:00:38,820 --> 00:00:43,500
Josh pardon me sorry about that so

00:00:41,579 --> 00:00:46,230
Johnson is a founding director of the AI

00:00:43,500 --> 00:00:49,710
ethics lab she also has a PhD in

00:00:46,230 --> 00:00:51,090
philosophy and ethics and she's formerly

00:00:49,710 --> 00:00:53,039
a lecturer at the university of hong

00:00:51,090 --> 00:00:55,110
kong which is pretty cool also

00:00:53,039 --> 00:00:57,120
researcher to Harvard Law Harvard

00:00:55,110 --> 00:01:00,780
Medical School is a lot to name there

00:00:57,120 --> 00:01:02,910
and then young fellow its founder algo

00:01:00,780 --> 00:01:05,280
transparency org a foundation dedicated

00:01:02,910 --> 00:01:08,400
to showing the effects of algorithms

00:01:05,280 --> 00:01:10,170
he's also Mozilla fellow funnily enough

00:01:08,400 --> 00:01:12,119
and he also worked at Google back in the

00:01:10,170 --> 00:01:15,560
day on YouTube and you did a lot of

00:01:12,119 --> 00:01:18,420
research on YouTube after you left there

00:01:15,560 --> 00:01:20,310
so I want to kind of start with a kind

00:01:18,420 --> 00:01:23,400
of softball question but it'll be kind

00:01:20,310 --> 00:01:25,860
of a good level setting question what

00:01:23,400 --> 00:01:28,380
exactly is an algorithm and what would

00:01:25,860 --> 00:01:28,850
you say is not an algorithm does that

00:01:28,380 --> 00:01:32,579
make sense

00:01:28,850 --> 00:01:35,640
yes who basically an algorithm is just a

00:01:32,579 --> 00:01:38,850
piece of software that take inputs and

00:01:35,640 --> 00:01:42,810
gets output so right now the algorithms

00:01:38,850 --> 00:01:44,930
that control the internet whether it's a

00:01:42,810 --> 00:01:49,530
YouTube Facebook Twitter

00:01:44,930 --> 00:01:51,540
ya know everything about us every single

00:01:49,530 --> 00:01:56,219
interaction we add with the system and

00:01:51,540 --> 00:01:59,490
they use this past to try to predict the

00:01:56,219 --> 00:02:01,710
best interaction and then they have this

00:01:59,490 --> 00:02:04,590
definition of best interaction what

00:02:01,710 --> 00:02:07,129
what's the goal of the way I it has a

00:02:04,590 --> 00:02:09,660
goal only tries to achieve this goal

00:02:07,129 --> 00:02:11,459
nice and then justice so what would you

00:02:09,660 --> 00:02:13,050
say is the difference between an

00:02:11,459 --> 00:02:16,080
algorithm and

00:02:13,050 --> 00:02:18,270
full-on AI I don't think I should assets

00:02:16,080 --> 00:02:23,550
I think it's so like diems question out

00:02:18,270 --> 00:02:24,990
you want to take that one too and so the

00:02:23,550 --> 00:02:26,610
difference between them say just like

00:02:24,990 --> 00:02:29,400
artificial intelligence artificial

00:02:26,610 --> 00:02:30,840
intelligence and just an algorithm yeah

00:02:29,400 --> 00:02:33,360
I think artificial intelligence is just

00:02:30,840 --> 00:02:35,670
a fancy way to say algorithm it's like a

00:02:33,360 --> 00:02:39,590
complex algorithm and we use artificial

00:02:35,670 --> 00:02:43,140
intelligence now for neural networks

00:02:39,590 --> 00:02:46,230
because it's a bit more complex but it's

00:02:43,140 --> 00:02:49,290
basically an algorithm that does tasks

00:02:46,230 --> 00:02:51,090
that we're usually done by humans nice

00:02:49,290 --> 00:02:55,050
and then one thing I like to think about

00:02:51,090 --> 00:02:57,060
you know what exactly do we mean when it

00:02:55,050 --> 00:02:58,320
comes to ethics and AI because we have

00:02:57,060 --> 00:03:00,660
technology we build I don't think of the

00:02:58,320 --> 00:03:02,430
ethics of a microwave but you know

00:03:00,660 --> 00:03:04,020
artificial intelligence official

00:03:02,430 --> 00:03:05,520
intelligence there's ethics there so

00:03:04,020 --> 00:03:07,440
when we're talking about ethics and AI

00:03:05,520 --> 00:03:08,580
what do we mean what are we talking

00:03:07,440 --> 00:03:10,980
about

00:03:08,580 --> 00:03:14,370
so basically when we say ethics what we

00:03:10,980 --> 00:03:16,530
are talking about is what is the right

00:03:14,370 --> 00:03:18,330
action to take in any given

00:03:16,530 --> 00:03:21,630
circumstances and also what is the right

00:03:18,330 --> 00:03:23,700
policy to implement so when you talk

00:03:21,630 --> 00:03:25,560
about microwaves for example the ethics

00:03:23,700 --> 00:03:27,240
question would come in as like what but

00:03:25,560 --> 00:03:28,770
what are the right things to do with the

00:03:27,240 --> 00:03:31,860
microwave like you shouldn't microwave

00:03:28,770 --> 00:03:33,870
your cat it's wrong there will be an

00:03:31,860 --> 00:03:36,780
ethical question there but you don't

00:03:33,870 --> 00:03:38,010
implement anything ethics related into

00:03:36,780 --> 00:03:39,690
the microwave because microwave doesn't

00:03:38,010 --> 00:03:43,530
make decisions whereas with the AI

00:03:39,690 --> 00:03:46,440
systems because they are taking and

00:03:43,530 --> 00:03:49,350
taking into value considerations into

00:03:46,440 --> 00:03:51,240
account there is a place where the

00:03:49,350 --> 00:03:53,959
ethical decision-making goes into the AI

00:03:51,240 --> 00:03:56,010
as well as into the user or into the

00:03:53,959 --> 00:04:00,020
platforms that are using these AI

00:03:56,010 --> 00:04:02,190
systems nice that's yeah that's a good

00:04:00,020 --> 00:04:05,580
point I think on the other thing it's

00:04:02,190 --> 00:04:07,080
kind of hard about working with AI is

00:04:05,580 --> 00:04:08,190
that you know mice in the microwave I

00:04:07,080 --> 00:04:10,200
can see a microwave right there and

00:04:08,190 --> 00:04:13,380
there it is but AI feels a little bit

00:04:10,200 --> 00:04:16,130
more amorphous like you can't really

00:04:13,380 --> 00:04:18,419
touch AI like is it kind of hard to

00:04:16,130 --> 00:04:20,549
regulate it in a way because we can't

00:04:18,419 --> 00:04:22,660
really see it it's all around us like

00:04:20,549 --> 00:04:27,280
you know the force like Star Wars

00:04:22,660 --> 00:04:28,510
I'm sort of disagree with the this time

00:04:27,280 --> 00:04:30,690
with the microwave and knowledge are

00:04:28,510 --> 00:04:34,570
there in the sense that I mean you do

00:04:30,690 --> 00:04:37,330
see AI the systems that use AI right

00:04:34,570 --> 00:04:39,010
so I mean your phone is a good very good

00:04:37,330 --> 00:04:40,540
example of a whole bunch of AI systems

00:04:39,010 --> 00:04:42,160
yes you don't see exactly the code

00:04:40,540 --> 00:04:45,520
inside but in a similar way you don't

00:04:42,160 --> 00:04:47,470
see how the you know molecules work in

00:04:45,520 --> 00:04:49,000
the microwave so it's not that you have

00:04:47,470 --> 00:04:51,280
to you are looking at the product if

00:04:49,000 --> 00:04:53,680
you're looking at the product you can

00:04:51,280 --> 00:04:56,200
talk a lot about how this product should

00:04:53,680 --> 00:04:58,630
be ethically developed designed research

00:04:56,200 --> 00:05:01,690
and all those things again when you go

00:04:58,630 --> 00:05:03,910
one layer in as you are coding it you

00:05:01,690 --> 00:05:06,130
will still have these value trade-offs

00:05:03,910 --> 00:05:08,080
that you are gonna implement by which I

00:05:06,130 --> 00:05:12,850
mean by the way let me try to explain

00:05:08,080 --> 00:05:14,980
this a bit more concretely like okay

00:05:12,850 --> 00:05:17,590
that I hate that example but the

00:05:14,980 --> 00:05:21,100
self-driving car example you know yes if

00:05:17,590 --> 00:05:23,230
you are factoring in which under which

00:05:21,100 --> 00:05:26,800
conditions the car should react which

00:05:23,230 --> 00:05:28,120
way that is a value decision I don't

00:05:26,800 --> 00:05:29,800
think self-driving cars is a good

00:05:28,120 --> 00:05:32,440
example in the sense that it's it takes

00:05:29,800 --> 00:05:36,430
too much of our conversation with too

00:05:32,440 --> 00:05:38,580
little substance but in the sense that

00:05:36,430 --> 00:05:40,840
showing how value comes into the

00:05:38,580 --> 00:05:42,820
decision-making of the system I think

00:05:40,840 --> 00:05:43,840
it's a good example yeah yeah I think

00:05:42,820 --> 00:05:46,000
it's interesting having you both on

00:05:43,840 --> 00:05:49,450
stage because one of you is an engineer

00:05:46,000 --> 00:05:53,020
and one of you is a philosophy master no

00:05:49,450 --> 00:05:56,860
all why are both important when it comes

00:05:53,020 --> 00:06:00,490
to talking about AI why are both sides

00:05:56,860 --> 00:06:03,880
of that important well I think when

00:06:00,490 --> 00:06:06,580
you're saying AI should be good or AI

00:06:03,880 --> 00:06:09,160
should be ethical really that sentence

00:06:06,580 --> 00:06:11,170
has two aspects you know the AI which

00:06:09,160 --> 00:06:13,810
you know people like the Omar expert on

00:06:11,170 --> 00:06:15,970
and then when you say it should be good

00:06:13,810 --> 00:06:18,280
it should be fair well then there you

00:06:15,970 --> 00:06:19,720
are immediately entering entering into

00:06:18,280 --> 00:06:21,550
the flows of a discussion you know we've

00:06:19,720 --> 00:06:23,950
been dealing with this all in many other

00:06:21,550 --> 00:06:26,350
fields it's not like this suddenly can't

00:06:23,950 --> 00:06:28,690
face with these question also how are we

00:06:26,350 --> 00:06:30,130
going to create ethical products or how

00:06:28,690 --> 00:06:31,600
are we going to create ethical systems

00:06:30,130 --> 00:06:32,830
you know our public policy has been

00:06:31,600 --> 00:06:33,729
dealing with these questions public

00:06:32,830 --> 00:06:36,250
health has been dealing with these

00:06:33,729 --> 00:06:36,490
questions environmental considerations

00:06:36,250 --> 00:06:38,289
all

00:06:36,490 --> 00:06:40,330
of these are questions about what is

00:06:38,289 --> 00:06:42,340
good what is fair we have a huge

00:06:40,330 --> 00:06:45,340
literature both theoretically and in

00:06:42,340 --> 00:06:47,289
applied in philosophy and I think you

00:06:45,340 --> 00:06:49,330
know if we don't make use of that

00:06:47,289 --> 00:06:51,460
literature and try to rediscover what

00:06:49,330 --> 00:06:54,039
ethics means we are really losing time

00:06:51,460 --> 00:06:55,840
so also like in the second Valley the

00:06:54,039 --> 00:06:57,580
mentality has been like okay we are

00:06:55,840 --> 00:07:01,330
reinventing everything so we are just

00:06:57,580 --> 00:07:03,370
going to make everything new and we

00:07:01,330 --> 00:07:05,139
forget about the past and we don't like

00:07:03,370 --> 00:07:08,380
take into account like lessons of

00:07:05,139 --> 00:07:10,990
history lessons of philosophy all that

00:07:08,380 --> 00:07:14,530
like people were too busy like focusing

00:07:10,990 --> 00:07:16,419
on optimizing algorithm to to care about

00:07:14,530 --> 00:07:18,250
these things and we see a lot of the

00:07:16,419 --> 00:07:21,729
problems we encounter on the web right

00:07:18,250 --> 00:07:24,520
now they have been solved in the past so

00:07:21,729 --> 00:07:26,139
we need to learn those lessons to solve

00:07:24,520 --> 00:07:28,000
them again even if the tools are

00:07:26,139 --> 00:07:29,259
different yeah I want to talk about what

00:07:28,000 --> 00:07:31,900
you just said a little bit because you

00:07:29,259 --> 00:07:34,900
know you have worked out on the YouTube

00:07:31,900 --> 00:07:37,330
products you have studied YouTube in the

00:07:34,900 --> 00:07:38,680
form of that in that it's and now and

00:07:37,330 --> 00:07:41,229
you mentioned that we can look to

00:07:38,680 --> 00:07:43,389
history to see similar problems but

00:07:41,229 --> 00:07:45,610
we've never seen a point history where a

00:07:43,389 --> 00:07:47,560
video player suddenly is a pending

00:07:45,610 --> 00:07:50,259
democracy or and controlling what people

00:07:47,560 --> 00:07:52,930
think so when you studied YouTube what

00:07:50,259 --> 00:07:54,310
aspects there kind of rang true

00:07:52,930 --> 00:07:56,259
throughout all of history what are some

00:07:54,310 --> 00:08:00,039
new aspects that you discovered does

00:07:56,259 --> 00:08:03,159
that make sense what is what is big yeah

00:08:00,039 --> 00:08:05,080
what aspects were problems you saw in

00:08:03,159 --> 00:08:07,180
history in the past over and over again

00:08:05,080 --> 00:08:09,969
and then what things were new problems

00:08:07,180 --> 00:08:12,340
that came about with this specific so I

00:08:09,969 --> 00:08:14,320
think most key problems that we saw over

00:08:12,340 --> 00:08:17,290
and over that appear the

00:08:14,320 --> 00:08:21,340
allegro transparency like conspiracy

00:08:17,290 --> 00:08:24,070
theories a trade radicalization

00:08:21,340 --> 00:08:27,640
so you see that yeah radicalization is

00:08:24,070 --> 00:08:29,590
really good for engagement we saw that

00:08:27,640 --> 00:08:32,080
that's a lesson of the 20th century if

00:08:29,590 --> 00:08:33,340
we if there is one lesson is that when

00:08:32,080 --> 00:08:35,440
you radicalize people that get very

00:08:33,340 --> 00:08:37,930
engaged and then if you have an

00:08:35,440 --> 00:08:40,680
algorithm that maximizes engagement then

00:08:37,930 --> 00:08:43,980
of course you maximize radicalization

00:08:40,680 --> 00:08:48,040
yeah and I want to talk a little bit oh

00:08:43,980 --> 00:08:50,130
I think I mean just following up when

00:08:48,040 --> 00:08:51,940
you think about the the problems that

00:08:50,130 --> 00:08:54,940
that you've been dealing with with

00:08:51,940 --> 00:08:57,250
YouTube and the radicalization it again

00:08:54,940 --> 00:08:59,710
they are really really like old school

00:08:57,250 --> 00:09:01,120
problems we always had to prep again the

00:08:59,710 --> 00:09:03,160
issues we always had the hate speech

00:09:01,120 --> 00:09:04,900
issues we always had the engagement

00:09:03,160 --> 00:09:06,970
radicalization and engagement issue in

00:09:04,900 --> 00:09:09,490
the sense that well why do you think

00:09:06,970 --> 00:09:11,560
that the TV shows are sort of catering

00:09:09,490 --> 00:09:13,030
to this this feeling because they know

00:09:11,560 --> 00:09:14,890
next time you're gonna turn on that TV

00:09:13,030 --> 00:09:19,480
for that at that time because you

00:09:14,890 --> 00:09:21,820
enjoyed feeling this common enemy and

00:09:19,480 --> 00:09:24,250
going against it right the only thing

00:09:21,820 --> 00:09:28,060
that is significantly different is the

00:09:24,250 --> 00:09:30,370
scale and how everywhere it happens like

00:09:28,060 --> 00:09:32,740
your you could be somewhat prepared when

00:09:30,370 --> 00:09:35,530
you turn on a political TV show or a

00:09:32,740 --> 00:09:39,220
newspaper but you might be really in a

00:09:35,530 --> 00:09:40,240
very passive and vulnerable mindset when

00:09:39,220 --> 00:09:42,940
you are just scrolling through a

00:09:40,240 --> 00:09:44,530
facebook before sleep and that is this

00:09:42,940 --> 00:09:46,750
this a little bit of a sinister aspect

00:09:44,530 --> 00:09:47,980
that that creeps in I think it's a

00:09:46,750 --> 00:09:50,410
really good point I mean one thing as

00:09:47,980 --> 00:09:52,360
you talked about that when I think of in

00:09:50,410 --> 00:09:54,310
this kind of a curve ball but it seems

00:09:52,360 --> 00:09:56,650
like technology we're really good at

00:09:54,310 --> 00:09:59,110
solving problems if we want to solve a

00:09:56,650 --> 00:10:03,340
certain task but problems that we've

00:09:59,110 --> 00:10:04,990
already had technology seems to make it

00:10:03,340 --> 00:10:06,580
even more hyperbolic and kind of really

00:10:04,990 --> 00:10:08,860
bring it out it seems like we're really

00:10:06,580 --> 00:10:10,180
good at solving tasks but not so good at

00:10:08,860 --> 00:10:13,270
solving the problems that we've always

00:10:10,180 --> 00:10:19,120
had your philosopher your smartest

00:10:13,270 --> 00:10:22,720
person on stage why is why why why do we

00:10:19,120 --> 00:10:24,460
see why is this keep on happening I mean

00:10:22,720 --> 00:10:26,980
one answer is because with the

00:10:24,460 --> 00:10:28,120
technology a lot of the times we are not

00:10:26,980 --> 00:10:29,350
trying to solve

00:10:28,120 --> 00:10:31,900
these problems we are trying to build

00:10:29,350 --> 00:10:34,420
another product and I mean correct me if

00:10:31,900 --> 00:10:36,700
I'm wrong but as we are building another

00:10:34,420 --> 00:10:39,010
product we know these problems are there

00:10:36,700 --> 00:10:41,200
and we sort of don't care in the

00:10:39,010 --> 00:10:42,550
meantime they get bigger so yes we are

00:10:41,200 --> 00:10:46,210
trying to make sure that you spend more

00:10:42,550 --> 00:10:48,040
time on YouTube or you engage more on

00:10:46,210 --> 00:10:50,080
Twitter and we know you know there are

00:10:48,040 --> 00:10:51,040
issues about the hate speech and

00:10:50,080 --> 00:10:53,050
conspiracy and all of those

00:10:51,040 --> 00:10:54,730
radicalization but the goal is not to

00:10:53,050 --> 00:10:56,560
solve them like if you had a product

00:10:54,730 --> 00:10:57,940
that actually tries to solve them that's

00:10:56,560 --> 00:11:00,580
a different question then we can talk

00:10:57,940 --> 00:11:02,890
about well is it possible in which ways

00:11:00,580 --> 00:11:05,500
can we go around it and in a lot of the

00:11:02,890 --> 00:11:07,600
times engineers says when we talk about

00:11:05,500 --> 00:11:09,880
these things they say well look your job

00:11:07,600 --> 00:11:11,620
is to solve the social problems in the

00:11:09,880 --> 00:11:12,880
social realm and once you solve them we

00:11:11,620 --> 00:11:14,890
will not have any problems anyway

00:11:12,880 --> 00:11:18,790
because you know things will be cleared

00:11:14,890 --> 00:11:21,430
from and yes in some sense but I mean I

00:11:18,790 --> 00:11:22,990
think 2,000 years more than 2,000 years

00:11:21,430 --> 00:11:23,890
of history trolls told us that that's

00:11:22,990 --> 00:11:26,380
not gonna happen

00:11:23,890 --> 00:11:28,240
so we might as well like join forces and

00:11:26,380 --> 00:11:30,339
try to solve it in both ways and there

00:11:28,240 --> 00:11:32,709
is a feedback loop right so that that

00:11:30,339 --> 00:11:34,570
also is very important yeah yeah I'm you

00:11:32,709 --> 00:11:39,310
are an engineer so like you know is it

00:11:34,570 --> 00:11:40,660
the case you know so is it true that you

00:11:39,310 --> 00:11:43,209
know folks on the inside are only

00:11:40,660 --> 00:11:45,640
prioritizing for more views more views

00:11:43,209 --> 00:11:47,620
more views is there anyone on the inside

00:11:45,640 --> 00:11:51,490
like guys can we need not like screw

00:11:47,620 --> 00:11:57,400
everything up yes so well no short

00:11:51,490 --> 00:12:00,310
answer no absolutely terrifying so when

00:11:57,400 --> 00:12:01,810
I've been there and I tried to talk

00:12:00,310 --> 00:12:03,580
about some issues like about filter

00:12:01,810 --> 00:12:06,430
bubbles on how we can solve these issues

00:12:03,580 --> 00:12:09,120
a real life that a lot of engineers were

00:12:06,430 --> 00:12:11,380
really interested in it was a social

00:12:09,120 --> 00:12:13,720
aspect of it on that oh we can improve

00:12:11,380 --> 00:12:15,400
that but then when you go to the

00:12:13,720 --> 00:12:18,940
management and you have like five level

00:12:15,400 --> 00:12:21,670
five layers of hierarchy on each time at

00:12:18,940 --> 00:12:24,850
each layer that try they have their KPI

00:12:21,670 --> 00:12:28,180
they try to optimize a watch time on

00:12:24,850 --> 00:12:29,950
number of users etc and you talk to them

00:12:28,180 --> 00:12:33,790
about something else it's like you're

00:12:29,950 --> 00:12:35,920
talking a different language and so so

00:12:33,790 --> 00:12:38,170
it's not like they don't try to solve it

00:12:35,920 --> 00:12:41,279
it's not that they fail at solving these

00:12:38,170 --> 00:12:43,860
problems that they don't even try yeah

00:12:41,279 --> 00:12:46,290
I mean there are always so I have this

00:12:43,860 --> 00:12:48,180
self selected group that I engage but

00:12:46,290 --> 00:12:50,220
right because I I talk to people who

00:12:48,180 --> 00:12:53,249
already contact me and they contact me

00:12:50,220 --> 00:12:55,290
for a reason so I know that there are a

00:12:53,249 --> 00:12:56,910
lot of people in big companies and in

00:12:55,290 --> 00:13:00,269
startups that are really concerned about

00:12:56,910 --> 00:13:02,309
ethical issues in the big companies I

00:13:00,269 --> 00:13:03,689
think yesterday the main problem is they

00:13:02,309 --> 00:13:05,819
might be concerned in this one

00:13:03,689 --> 00:13:07,620
particular team but if they whatever

00:13:05,819 --> 00:13:09,809
they do doesn't go through all the way

00:13:07,620 --> 00:13:12,389
then it's sort of like a wasted effort

00:13:09,809 --> 00:13:17,550
of theirs and with the startups yes you

00:13:12,389 --> 00:13:19,920
can actually really help them get on the

00:13:17,550 --> 00:13:22,170
right track but the problem is well are

00:13:19,920 --> 00:13:24,689
they gonna be strong enough to compete

00:13:22,170 --> 00:13:26,339
in the end in the market and of course

00:13:24,689 --> 00:13:28,439
there is this other question well if the

00:13:26,339 --> 00:13:30,839
market doesn't sort of doesn't have the

00:13:28,439 --> 00:13:32,370
fair competition in the sense that some

00:13:30,839 --> 00:13:35,550
don't care about ethics and some try

00:13:32,370 --> 00:13:37,949
well you are sort of pushing those who

00:13:35,550 --> 00:13:39,990
care to be in a in a disadvantaged

00:13:37,949 --> 00:13:41,189
position from the start yeah I mean

00:13:39,990 --> 00:13:42,389
competition is such a good point I mean

00:13:41,189 --> 00:13:45,420
where the other video players are

00:13:42,389 --> 00:13:47,490
competing with YouTube I mean Facebook

00:13:45,420 --> 00:13:53,699
on TV basically they're they see their

00:13:47,490 --> 00:13:55,110
competition as waste so I want to talk

00:13:53,699 --> 00:13:58,769
we've talked a lot about YouTube but I

00:13:55,110 --> 00:14:00,480
want to talk about AI and its other

00:13:58,769 --> 00:14:02,040
forms because AI seems to be so

00:14:00,480 --> 00:14:04,290
pervasive nowadays it's not just

00:14:02,040 --> 00:14:06,629
Google's video player like you know AI

00:14:04,290 --> 00:14:10,199
effects law enforcement and how they do

00:14:06,629 --> 00:14:11,699
their job it affects government and

00:14:10,199 --> 00:14:14,100
things are going on there I even think

00:14:11,699 --> 00:14:16,350
of like banking nowadays like you know

00:14:14,100 --> 00:14:20,370
401ks that are kind of run by Robo

00:14:16,350 --> 00:14:22,740
advisors do you want to speak to how AI

00:14:20,370 --> 00:14:24,389
effects everything not just the phones

00:14:22,740 --> 00:14:26,879
are live in our apps I mean absolutely

00:14:24,389 --> 00:14:28,379
our phone well I mean yeah although I do

00:14:26,879 --> 00:14:30,929
assume that everyone here is sort of

00:14:28,379 --> 00:14:33,569
aware of these things but I can tell you

00:14:30,929 --> 00:14:36,959
a couple of the ways that they effect so

00:14:33,569 --> 00:14:40,129
for example in in law enforcement well

00:14:36,959 --> 00:14:43,230
first of all whenever you have the

00:14:40,129 --> 00:14:44,459
collaborations between big companies and

00:14:43,230 --> 00:14:45,990
the government there is there's

00:14:44,459 --> 00:14:48,720
something to be careful about because

00:14:45,990 --> 00:14:51,809
you are really joining two types of very

00:14:48,720 --> 00:14:53,879
strong forces one has all the data about

00:14:51,809 --> 00:14:55,170
you and knows a lot and the other one

00:14:53,879 --> 00:14:58,620
has

00:14:55,170 --> 00:15:01,200
basically the only legitimate power for

00:14:58,620 --> 00:15:03,210
coercion and force so combining them is

00:15:01,200 --> 00:15:05,160
very scary and when you go to these

00:15:03,210 --> 00:15:06,810
domains you realize that the people who

00:15:05,160 --> 00:15:08,580
are going to employ the AI systems

00:15:06,810 --> 00:15:10,740
actually have no idea what they are

00:15:08,580 --> 00:15:13,410
supposed to be doing they are as lost so

00:15:10,740 --> 00:15:15,840
they would choose algal a AI systems

00:15:13,410 --> 00:15:17,700
algorithms basically because the next

00:15:15,840 --> 00:15:19,860
guy chose that one and they would have

00:15:17,700 --> 00:15:21,420
absolutely no idea how to evaluate it

00:15:19,860 --> 00:15:23,730
and that's terrifying because if you

00:15:21,420 --> 00:15:25,500
have a law enforcement agent really

00:15:23,730 --> 00:15:27,360
trying to do the right thing but have

00:15:25,500 --> 00:15:29,850
absolutely no idea how to go about it

00:15:27,360 --> 00:15:33,390
there's a problem so what we try to sort

00:15:29,850 --> 00:15:36,450
of do is find ways both procedurally and

00:15:33,390 --> 00:15:38,250
in terms of adding the expertise for

00:15:36,450 --> 00:15:41,070
them to be able to a little bit have a

00:15:38,250 --> 00:15:43,050
little bit more insight about what these

00:15:41,070 --> 00:15:49,680
systems offer and what they don't offer

00:15:43,050 --> 00:15:53,940
where they actually are really weak yeah

00:15:49,680 --> 00:15:55,350
just what I want to add is that you have

00:15:53,940 --> 00:15:56,970
all the system and sometimes they are

00:15:55,350 --> 00:15:58,770
there to help you on sometimes they are

00:15:56,970 --> 00:16:01,710
not there to help you they are there to

00:15:58,770 --> 00:16:04,770
try to trick you so an example of that

00:16:01,710 --> 00:16:07,770
that everybody uses is uber so there is

00:16:04,770 --> 00:16:09,420
a search algorithm so depending is a

00:16:07,770 --> 00:16:12,840
search algorithm trying to help you to

00:16:09,420 --> 00:16:14,790
match the offer on demand and to get

00:16:12,840 --> 00:16:16,470
more drivers on the street or is a

00:16:14,790 --> 00:16:20,250
search algorithm just trying to make you

00:16:16,470 --> 00:16:22,740
pay more everybody thinks oh it's trying

00:16:20,250 --> 00:16:24,750
to help you trying to just get more

00:16:22,740 --> 00:16:26,790
driver on the street but when you

00:16:24,750 --> 00:16:28,920
compare your price that you pay and you

00:16:26,790 --> 00:16:31,320
compare the price that the driver

00:16:28,920 --> 00:16:35,250
receives you see that often the search

00:16:31,320 --> 00:16:36,870
goes entirely to uber so a lot of times

00:16:35,250 --> 00:16:39,300
an algorithm that you think is trying to

00:16:36,870 --> 00:16:40,460
help you is actually trying to steal

00:16:39,300 --> 00:16:43,770
your money

00:16:40,460 --> 00:16:45,540
that's very depressing so I with that in

00:16:43,770 --> 00:16:46,620
mind like you know what are some things

00:16:45,540 --> 00:16:47,760
and this I think will be the last

00:16:46,620 --> 00:16:50,210
question before we open it up to the

00:16:47,760 --> 00:16:52,980
audience Q&A but what are some things

00:16:50,210 --> 00:16:54,120
companies can do right now I mean

00:16:52,980 --> 00:16:56,580
unfortunately there are always going to

00:16:54,120 --> 00:16:59,070
prioritize profits but if they were able

00:16:56,580 --> 00:17:00,630
to put billion making billions aside

00:16:59,070 --> 00:17:07,280
what are some things they can do right

00:17:00,630 --> 00:17:08,510
now to be to make AI more fair yes who

00:17:07,280 --> 00:17:10,850
what I've been advocating is

00:17:08,510 --> 00:17:12,949
transparency so if we look at this

00:17:10,850 --> 00:17:14,780
uber example we see that if we would

00:17:12,949 --> 00:17:17,420
know exactly what the driver make on

00:17:14,780 --> 00:17:20,780
what the user make then people will see

00:17:17,420 --> 00:17:23,780
where the money goes so you work will

00:17:20,780 --> 00:17:25,310
instill money at all with YouTube it's

00:17:23,780 --> 00:17:27,110
the same if they are transparent but

00:17:25,310 --> 00:17:28,459
what I recommend if they start

00:17:27,110 --> 00:17:31,100
recommending something that ethically

00:17:28,459 --> 00:17:34,400
like philosophers on others like just

00:17:31,100 --> 00:17:37,280
user I think is wrong then will detect

00:17:34,400 --> 00:17:38,870
it really fast when we stop that but

00:17:37,280 --> 00:17:41,000
what happened is that YouTube

00:17:38,870 --> 00:17:43,640
recommended like pedophile videos on all

00:17:41,000 --> 00:17:45,170
kind of really scary things for years on

00:17:43,640 --> 00:17:46,520
nobody noticed because there was no

00:17:45,170 --> 00:17:50,230
transparency so we need much more

00:17:46,520 --> 00:17:52,640
transparency and I would say that what I

00:17:50,230 --> 00:17:55,970
advocate and try to push for is to

00:17:52,640 --> 00:17:58,490
really add this ethical problem solving

00:17:55,970 --> 00:18:00,830
from the very beginning of the research

00:17:58,490 --> 00:18:03,020
phase all the way through the designing

00:18:00,830 --> 00:18:06,830
and development and deployment of AI

00:18:03,020 --> 00:18:10,610
systems and it's sort of like what but

00:18:06,830 --> 00:18:12,890
iam is advocating helps me in some ways

00:18:10,610 --> 00:18:15,550
because the incentive is the problem

00:18:12,890 --> 00:18:17,750
with my approach like how do you if

00:18:15,550 --> 00:18:19,460
companies are already concerned that's

00:18:17,750 --> 00:18:21,620
great then you have an entry point but

00:18:19,460 --> 00:18:23,810
if they are not concerned then they just

00:18:21,620 --> 00:18:26,480
shrug and say well we'll we'll deal with

00:18:23,810 --> 00:18:29,090
it when you know when people ask us why

00:18:26,480 --> 00:18:30,500
did you screw up we'll just say oh sorry

00:18:29,090 --> 00:18:31,310
we didn't think about that we'll do

00:18:30,500 --> 00:18:34,070
better next time

00:18:31,310 --> 00:18:35,900
whereas if they had if they are forced

00:18:34,070 --> 00:18:37,970
to have some more transparency then

00:18:35,900 --> 00:18:39,230
there is incentive for them to say well

00:18:37,970 --> 00:18:40,880
let's just do it from the beginning

00:18:39,230 --> 00:18:43,220
rightly so that we don't have things

00:18:40,880 --> 00:18:45,080
blowing up there later on it's a good

00:18:43,220 --> 00:18:46,880
point hopefully more companies do it

00:18:45,080 --> 00:18:48,230
right from the start so if we want to do

00:18:46,880 --> 00:18:48,710
any questions from the audience we can

00:18:48,230 --> 00:18:51,220
do that right now

00:18:48,710 --> 00:18:55,190
and you might have any questions for

00:18:51,220 --> 00:18:59,890
John soo young yeah you have a

00:18:55,190 --> 00:18:59,890
microphone that's going around yeah

00:19:01,339 --> 00:19:05,999
as a gun yeah good man and question for

00:19:04,469 --> 00:19:08,369
again big fan of your work I've been

00:19:05,999 --> 00:19:10,139
following on YouTube from a policy

00:19:08,369 --> 00:19:11,759
perspective what are you advocating for

00:19:10,139 --> 00:19:14,519
what should our lawmakers be thinking

00:19:11,759 --> 00:19:15,779
about particularly regard to dealing

00:19:14,519 --> 00:19:18,749
with user-generated user-generated

00:19:15,779 --> 00:19:24,089
content at that kind of scale yeah so

00:19:18,749 --> 00:19:26,429
you know in order not to do to impact

00:19:24,089 --> 00:19:27,859
free speech I think transparency is

00:19:26,429 --> 00:19:29,909
really the best so for instance

00:19:27,859 --> 00:19:32,190
transparency of outcomes not

00:19:29,909 --> 00:19:34,109
transparency of what the card is so we

00:19:32,190 --> 00:19:37,009
don't need to know what what it does but

00:19:34,109 --> 00:19:39,690
we need to know we know we need to know

00:19:37,009 --> 00:19:41,549
what the impact of the algorithm so for

00:19:39,690 --> 00:19:44,699
instance how many times did the

00:19:41,549 --> 00:19:46,109
algorithm recommend this or this person

00:19:44,699 --> 00:19:47,909
how many times did the algorithm

00:19:46,109 --> 00:19:50,579
recommend Alex Jones how many times did

00:19:47,909 --> 00:19:53,309
they recommend a CNN and then we can see

00:19:50,579 --> 00:19:55,009
if the algorithm is biased conservatives

00:19:53,309 --> 00:19:57,749
say that the algorithms are biased

00:19:55,009 --> 00:19:59,399
against them if we would have this

00:19:57,749 --> 00:20:01,849
transparency we would know if it's the

00:19:59,399 --> 00:20:01,849
case or not

00:20:02,579 --> 00:20:07,009
we have a question up here microphone

00:20:08,930 --> 00:20:15,200
and the one in the back after hi so

00:20:12,410 --> 00:20:18,590
we've had now for a bit more than a year

00:20:15,200 --> 00:20:22,370
something this explosion of ethical

00:20:18,590 --> 00:20:25,780
principles for a 80 or more of those

00:20:22,370 --> 00:20:28,910
exists government corporate etc

00:20:25,780 --> 00:20:31,310
obviously these are being criticized for

00:20:28,910 --> 00:20:34,760
being nice but how to actually implement

00:20:31,310 --> 00:20:37,610
them where the teeth now there is a move

00:20:34,760 --> 00:20:40,070
from standard-setting organizations and

00:20:37,610 --> 00:20:42,800
from other groups to try to translate

00:20:40,070 --> 00:20:45,040
some of these into technology standards

00:20:42,800 --> 00:20:48,110
or into certification kinds of things

00:20:45,040 --> 00:20:50,270
what do you think will be necessary in

00:20:48,110 --> 00:20:52,040
order to make those kinds of efforts

00:20:50,270 --> 00:20:55,790
into something that actually gets picked

00:20:52,040 --> 00:20:57,740
up and used to have real impact well I

00:20:55,790 --> 00:21:00,290
think the the main question to answer

00:20:57,740 --> 00:21:02,180
for for being able to do that is what's

00:21:00,290 --> 00:21:04,220
the goal I mean yes we know the goal is

00:21:02,180 --> 00:21:06,770
to get the ethical AI or AI systems to

00:21:04,220 --> 00:21:09,860
be ethical but like like what is the the

00:21:06,770 --> 00:21:12,230
one level below goal so if we are trying

00:21:09,860 --> 00:21:14,390
to make like right regulations yes it

00:21:12,230 --> 00:21:17,000
helps to by the way like yes we have

00:21:14,390 --> 00:21:18,590
over 80 sets of principles which

00:21:17,000 --> 00:21:20,780
actually can be summarized into four

00:21:18,590 --> 00:21:22,820
principles and those principles we

00:21:20,780 --> 00:21:26,660
dislike we've have been using them since

00:21:22,820 --> 00:21:29,030
1979 when it was developed for research

00:21:26,660 --> 00:21:30,800
ethics so nothing new has been

00:21:29,030 --> 00:21:32,540
discovered I don't know why companies

00:21:30,800 --> 00:21:34,160
keep writing new ones like with

00:21:32,540 --> 00:21:35,240
different wording it makes absolutely no

00:21:34,160 --> 00:21:38,720
sense

00:21:35,240 --> 00:21:42,170
but there is one like two ways that you

00:21:38,720 --> 00:21:44,810
can operationalize them one is within

00:21:42,170 --> 00:21:47,300
the company it they sort of I mean okay

00:21:44,810 --> 00:21:51,500
here are the principles respect

00:21:47,300 --> 00:21:54,680
individual agency do good for the people

00:21:51,500 --> 00:21:58,220
and for the society reduce harm and have

00:21:54,680 --> 00:21:59,960
like social justice super broad right

00:21:58,220 --> 00:22:02,120
like there is nothing that is clearly

00:21:59,960 --> 00:22:03,980
action guiding yes they are good in

00:22:02,120 --> 00:22:05,600
terms of thinking about these are the

00:22:03,980 --> 00:22:09,800
areas that you should have ways to

00:22:05,600 --> 00:22:12,140
secure these results so you should think

00:22:09,800 --> 00:22:14,090
about those but for regulations you can

00:22:12,140 --> 00:22:16,430
think about them and have set the

00:22:14,090 --> 00:22:18,260
regulations and for within the company I

00:22:16,430 --> 00:22:20,000
think what you need to think about these

00:22:18,260 --> 00:22:22,580
principles is they are going to clash

00:22:20,000 --> 00:22:25,700
like you know privacy versus

00:22:22,580 --> 00:22:28,519
accuracy is gonna clash so what are you

00:22:25,700 --> 00:22:30,890
gonna do if within the acceptable

00:22:28,519 --> 00:22:33,049
trade-offs so nothing going completely

00:22:30,890 --> 00:22:35,019
off immoral but within the acceptable

00:22:33,049 --> 00:22:37,190
trade-offs are you going to prioritize

00:22:35,019 --> 00:22:39,140
complete privacy or are you going to

00:22:37,190 --> 00:22:40,700
prioritize well this is gonna help like

00:22:39,140 --> 00:22:42,559
what are you prioritizing within the

00:22:40,700 --> 00:22:44,870
company so in terms of like they will

00:22:42,559 --> 00:22:47,299
help for strategy within the company and

00:22:44,870 --> 00:22:48,620
they would help for regulations but we

00:22:47,299 --> 00:22:51,230
really didn't have to spend the last

00:22:48,620 --> 00:22:53,720
year writing 80 versions of the same

00:22:51,230 --> 00:22:54,950
thing I think we have one last question

00:22:53,720 --> 00:22:56,450
in the back

00:22:54,950 --> 00:23:05,059
right there and then that's all the time

00:22:56,450 --> 00:23:07,429
we have for today I think here at most

00:23:05,059 --> 00:23:09,590
fest we probably find on the stage in

00:23:07,429 --> 00:23:11,870
the audience there's a fairly common

00:23:09,590 --> 00:23:16,220
mindset we're all gathered here to think

00:23:11,870 --> 00:23:19,399
about similar sorts of issues how do we

00:23:16,220 --> 00:23:22,669
all collectively reach out to the people

00:23:19,399 --> 00:23:29,919
that are shall we say most responsive to

00:23:22,669 --> 00:23:29,919
populist politics at the moment it's a

00:23:34,990 --> 00:23:43,000
good philosophers move right there like

00:23:37,220 --> 00:23:46,309
that is the first step is to understand

00:23:43,000 --> 00:23:49,179
yet their mindset on what they are

00:23:46,309 --> 00:23:52,610
scared of scary but I mean populist they

00:23:49,179 --> 00:23:55,490
they use fears to make people act

00:23:52,610 --> 00:23:59,029
basically so if you understand what what

00:23:55,490 --> 00:24:01,309
people are scared of then then you can

00:23:59,029 --> 00:24:03,820
understand why they get tricked tricked

00:24:01,309 --> 00:24:06,769
by AI so for instance I noticed that

00:24:03,820 --> 00:24:08,720
there were some scary videos that we are

00:24:06,769 --> 00:24:12,080
like tricking people into going into

00:24:08,720 --> 00:24:14,659
some type of populist thinking and when

00:24:12,080 --> 00:24:15,769
you understand this pathway is if again

00:24:14,659 --> 00:24:17,149
when you have more transparency you

00:24:15,769 --> 00:24:20,090
understand the best ways you look at the

00:24:17,149 --> 00:24:23,360
pathways and then you use that to

00:24:20,090 --> 00:24:26,240
counter the narrative that was in the

00:24:23,360 --> 00:24:27,590
best way I think that's all the time we

00:24:26,240 --> 00:24:30,260
have thank you guys so much

00:24:27,590 --> 00:24:32,450
John soo-jung thank you round of

00:24:30,260 --> 00:24:40,190
applause for these folks

00:24:32,450 --> 00:24:42,820
today I stand next so next up panels

00:24:40,190 --> 00:24:44,930
great code great responsibility

00:24:42,820 --> 00:24:48,940
that's a 12:45 that's not happening

00:24:44,930 --> 00:24:48,940

YouTube URL: https://www.youtube.com/watch?v=3t_KfScir00


