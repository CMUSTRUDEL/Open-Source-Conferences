Title: MozFest - Sunday Speakers (morning)
Publication date: 2019-11-04
Playlist: Mozilla Festival 2019
Description: 
	
Captions: 
	00:00:00,000 --> 00:00:04,589
the difference between small tasks and

00:00:02,280 --> 00:00:08,069
if they can promote tasks or whether

00:00:04,589 --> 00:00:09,870
they will out-compete the humankind take

00:00:08,069 --> 00:00:12,150
over and that makes us feel powerless it

00:00:09,870 --> 00:00:15,120
makes us feel like we can't control we

00:00:12,150 --> 00:00:18,180
can't govern we we don't know what to

00:00:15,120 --> 00:00:20,250
protect citizens and makes make it I

00:00:18,180 --> 00:00:22,710
would think that you know if we have to

00:00:20,250 --> 00:00:25,410
talk about how to define AI for example

00:00:22,710 --> 00:00:27,570
let's think of it as for example from my

00:00:25,410 --> 00:00:29,910
perspective from data ethics perspective

00:00:27,570 --> 00:00:33,200
it's it's very complex data processing

00:00:29,910 --> 00:00:35,520
and it's very very large systems of

00:00:33,200 --> 00:00:37,380
complex data processing and what are the

00:00:35,520 --> 00:00:39,629
ethical implications of that we have

00:00:37,380 --> 00:00:41,700
things in terms of mass citizens we have

00:00:39,629 --> 00:00:45,120
to control and think of change even

00:00:41,700 --> 00:00:47,520
mozilla applied in the thinking from the

00:00:45,120 --> 00:00:50,850
better decision-making as opposed to the

00:00:47,520 --> 00:00:55,590
trustworthy AI which is the term coined

00:00:50,850 --> 00:00:58,800
by the european union's ethics guideline

00:00:55,590 --> 00:01:01,079
and what that means and it has three

00:00:58,800 --> 00:01:04,170
pillars of being legal ethical and

00:01:01,079 --> 00:01:06,299
robust but at the moment is more what we

00:01:04,170 --> 00:01:09,119
see is that it's a marketing term and

00:01:06,299 --> 00:01:11,970
I'm I hope that this community can turn

00:01:09,119 --> 00:01:15,840
it into something more than more than

00:01:11,970 --> 00:01:17,729
that and just on that narrative setting

00:01:15,840 --> 00:01:20,189
for me what's very interesting I looked

00:01:17,729 --> 00:01:22,020
at my emails when was the first time in

00:01:20,189 --> 00:01:24,060
the work setting we started talking

00:01:22,020 --> 00:01:25,700
about artificial intelligence internally

00:01:24,060 --> 00:01:31,710
on the team beyond just sharing some

00:01:25,700 --> 00:01:35,040
newsletters and it was mid-2016 talking

00:01:31,710 --> 00:01:38,430
about the formation in the fall of the

00:01:35,040 --> 00:01:40,770
partnership on AI by Google Facebook IBM

00:01:38,430 --> 00:01:43,500
Amazon and Microsoft and I think it

00:01:40,770 --> 00:01:45,899
still is the reality that already in

00:01:43,500 --> 00:01:48,649
2016 these companies form the

00:01:45,899 --> 00:01:51,570
partnership that now we are a member of

00:01:48,649 --> 00:01:53,520
about correlation based decision support

00:01:51,570 --> 00:01:56,310
systems right you know how does that

00:01:53,520 --> 00:01:59,219
work with a very large audience it's not

00:01:56,310 --> 00:02:00,509
going to work but but it's important

00:01:59,219 --> 00:02:03,030
that we keep in mind that this is what

00:02:00,509 --> 00:02:04,619
we're talking about or I think that's at

00:02:03,030 --> 00:02:05,969
the core of what we are talking about

00:02:04,619 --> 00:02:08,700
when we're trying to identify the

00:02:05,969 --> 00:02:10,170
challenges that we have because we are

00:02:08,700 --> 00:02:12,390
looking more and more at these

00:02:10,170 --> 00:02:13,470
correlation based decisions instead of

00:02:12,390 --> 00:02:15,630
causation right

00:02:13,470 --> 00:02:17,370
and this is this is a fundamental change

00:02:15,630 --> 00:02:19,770
because we just assume that because

00:02:17,370 --> 00:02:21,960
there's massive amounts of data and we

00:02:19,770 --> 00:02:23,970
can detect patterns in it then that

00:02:21,960 --> 00:02:25,830
needs to guide our decisions and this is

00:02:23,970 --> 00:02:28,530
what's problematic and this is why what

00:02:25,830 --> 00:02:30,720
we need to understand better and then

00:02:28,530 --> 00:02:33,180
also take precautions because we are not

00:02:30,720 --> 00:02:35,580
going to get away from that anymore and

00:02:33,180 --> 00:02:38,040
it's helpful you know would like that if

00:02:35,580 --> 00:02:40,200
it's all you know people think for

00:02:38,040 --> 00:02:44,490
myself to the police officer but we

00:02:40,200 --> 00:02:47,100
expect tech companies to yeah I think at

00:02:44,490 --> 00:02:48,720
a company and and this is I think a

00:02:47,100 --> 00:02:50,150
typical perspective but one that I think

00:02:48,720 --> 00:02:52,800
is change is not enough it's not enough

00:02:50,150 --> 00:02:54,540
expectation for corporate accountability

00:02:52,800 --> 00:02:56,880
when it comes to the deployment of

00:02:54,540 --> 00:02:57,930
artificial intelligence systems so and

00:02:56,880 --> 00:03:01,410
one of the things that we're doing on

00:02:57,930 --> 00:03:03,510
our team is is we're in frequent

00:03:01,410 --> 00:03:05,670
conversation with regulators about how

00:03:03,510 --> 00:03:08,010
we can kind of find an in-between space

00:03:05,670 --> 00:03:09,420
or apply existing their existing

00:03:08,010 --> 00:03:12,030
regulatory framework how it should be

00:03:09,420 --> 00:03:13,530
better applied to AI and managing its

00:03:12,030 --> 00:03:16,050
risks particularly the human rights

00:03:13,530 --> 00:03:17,670
framework and then finding like ways

00:03:16,050 --> 00:03:20,280
that we can through kind of a light

00:03:17,670 --> 00:03:21,720
touch approach to regulation introduce

00:03:20,280 --> 00:03:23,730
greater accountability either through

00:03:21,720 --> 00:03:26,280
different reporting requirements for

00:03:23,730 --> 00:03:28,980
impact assessments and things like that

00:03:26,280 --> 00:03:30,690
so I think the conversation is gradually

00:03:28,980 --> 00:03:32,580
shifting well well didn't I think

00:03:30,690 --> 00:03:34,140
there's a consensus growing consensus

00:03:32,580 --> 00:03:35,400
and so far not enough and the new

00:03:34,140 --> 00:03:37,170
reality the companies have to deal with

00:03:35,400 --> 00:03:40,590
is what is that regulatory landscape

00:03:37,170 --> 00:03:45,120
going to look like how can it how can it

00:03:40,590 --> 00:03:45,959
pretty but allow technology to to to be

00:03:45,120 --> 00:03:48,900
developed in a way that is beneficial

00:03:45,959 --> 00:03:51,120
yeah agree should Facebook moderate

00:03:48,900 --> 00:03:54,060
Facebook assured government regulate

00:03:51,120 --> 00:03:56,190
Facebook so I said Myron right away

00:03:54,060 --> 00:03:58,110
should Facebook regulate Facebook

00:03:56,190 --> 00:04:00,540
assured and dust by itself by creating

00:03:58,110 --> 00:04:02,850
it some new law that then by by

00:04:00,540 --> 00:04:04,860
definition fixes everything or in a

00:04:02,850 --> 00:04:06,630
technological space we think of it as a

00:04:04,860 --> 00:04:08,430
technological fix and we create this

00:04:06,630 --> 00:04:10,230
system for example in terms of content

00:04:08,430 --> 00:04:11,940
moderation we created system that can

00:04:10,230 --> 00:04:14,190
they can moderate everything and that's

00:04:11,940 --> 00:04:16,049
it that it doesn't work like that

00:04:14,190 --> 00:04:18,359
anymore governance governance is law

00:04:16,049 --> 00:04:20,910
it's culture education

00:04:18,359 --> 00:04:24,090
it's where the AI we actually make it

00:04:20,910 --> 00:04:26,200
yeah yeah yeah it's second that at the

00:04:24,090 --> 00:04:28,690
same time I would also warn against

00:04:26,200 --> 00:04:30,700
this narrative of do we need regulation

00:04:28,690 --> 00:04:34,420
of course we need regulation and all

00:04:30,700 --> 00:04:36,460
these companies and processes are

00:04:34,420 --> 00:04:38,470
already heavily regulated there's no

00:04:36,460 --> 00:04:41,470
such thing as an unregulated Facebook

00:04:38,470 --> 00:04:43,930
you know there are privileges on

00:04:41,470 --> 00:04:46,000
liability that are hugely important when

00:04:43,930 --> 00:04:48,280
it comes to Facebook this is an active

00:04:46,000 --> 00:04:50,290
regulation there is copyright regulation

00:04:48,280 --> 00:04:52,870
that is highly complex and that has been

00:04:50,290 --> 00:04:55,240
around for many many years so it's more

00:04:52,870 --> 00:04:58,300
like what is good regulation and what is

00:04:55,240 --> 00:05:02,200
bad regulation apply to to these actors

00:04:58,300 --> 00:05:04,660
and and we also should probably I mean

00:05:02,200 --> 00:05:06,550
there has to be someone here on the

00:05:04,660 --> 00:05:09,100
stage who says that Facebook is just an

00:05:06,550 --> 00:05:11,560
 or company right and and and

00:05:09,100 --> 00:05:14,410
they are they sort of dug their own dig

00:05:11,560 --> 00:05:16,870
dug their own grave I mean it's not a

00:05:14,410 --> 00:05:20,800
grave yet but the pit you know that

00:05:16,870 --> 00:05:25,810
they'll fall into because they were just

00:05:20,800 --> 00:05:27,820
so ignorant about all the problems that

00:05:25,810 --> 00:05:30,190
they created in theirs they still are

00:05:27,820 --> 00:05:32,710
that everyone is is really aggressively

00:05:30,190 --> 00:05:35,410
attacking them now although in many

00:05:32,710 --> 00:05:37,080
countries they are still the basically

00:05:35,410 --> 00:05:39,460
the place for people to exchange

00:05:37,080 --> 00:05:41,890
information freely you know in

00:05:39,460 --> 00:05:43,660
authoritarian countries it's still great

00:05:41,890 --> 00:05:46,090
to have Facebook because there's no one

00:05:43,660 --> 00:05:48,550
else there to provide these services so

00:05:46,090 --> 00:05:51,550
it seems a little unfair but it's not

00:05:48,550 --> 00:05:56,020
because you know it always also depends

00:05:51,550 --> 00:05:58,960
on how these companies but also state

00:05:56,020 --> 00:06:01,210
institutions behave visa vie the demands

00:05:58,960 --> 00:06:03,460
that the public's have and this is what

00:06:01,210 --> 00:06:05,800
we need to find the balance you know to

00:06:03,460 --> 00:06:08,800
come up with good regulation that

00:06:05,800 --> 00:06:10,660
addresses set without harming for

00:06:08,800 --> 00:06:12,430
example free speech or other kinds of

00:06:10,660 --> 00:06:17,740
freedoms and this is really terribly

00:06:12,430 --> 00:06:20,800
hard yeah so but for me this is a power

00:06:17,740 --> 00:06:22,120
like a power question we can talk more

00:06:20,800 --> 00:06:23,680
details about the whole quantum

00:06:22,120 --> 00:06:25,030
moderation question just to bring it

00:06:23,680 --> 00:06:26,770
back a little bit closer

00:06:25,030 --> 00:06:30,790
you mentioned the autocratic countries

00:06:26,770 --> 00:06:33,730
as well for me the the specificities of

00:06:30,790 --> 00:06:36,640
the specific decision-making processes

00:06:33,730 --> 00:06:39,580
make it interesting to see that these

00:06:36,640 --> 00:06:41,170
governments implement decisions with

00:06:39,580 --> 00:06:43,150
different technologies and is there

00:06:41,170 --> 00:06:45,940
anything around the eye that makes it

00:06:43,150 --> 00:06:48,250
specifically dangerous or risky that

00:06:45,940 --> 00:06:50,620
creates different different thresholds

00:06:48,250 --> 00:06:52,450
that people need to put in place and for

00:06:50,620 --> 00:06:54,640
instance just to go back to the Facebook

00:06:52,450 --> 00:06:57,310
example they just announced that they

00:06:54,640 --> 00:07:01,090
would still allow government propaganda

00:06:57,310 --> 00:07:03,820
as they have for the past decade or so

00:07:01,090 --> 00:07:06,750
so it's not just around the technology

00:07:03,820 --> 00:07:09,040
they use but absolutely the policies

00:07:06,750 --> 00:07:10,930
yeah yeah here's a question I have I

00:07:09,040 --> 00:07:14,020
mean I've seen you know we've all seen

00:07:10,930 --> 00:07:18,040
the European Union fine Google and find

00:07:14,020 --> 00:07:19,570
Facebook billions of dollars but you

00:07:18,040 --> 00:07:22,030
know these companies make billions of

00:07:19,570 --> 00:07:24,460
dollars in just a few months how do you

00:07:22,030 --> 00:07:25,960
go about you know actually punishing

00:07:24,460 --> 00:07:36,880
these companies in the way they actually

00:07:25,960 --> 00:07:39,000
feel it not only the punishment but what

00:07:36,880 --> 00:07:43,720
we see is that probably Facebook will

00:07:39,000 --> 00:07:44,410
perform better this quarter than ever

00:07:43,720 --> 00:07:46,180
before

00:07:44,410 --> 00:07:49,390
even though I don't think it has been

00:07:46,180 --> 00:07:51,820
demonstrated more how how terrible

00:07:49,390 --> 00:07:54,880
they've they did to individuals and

00:07:51,820 --> 00:07:58,240
societies this year so the question is

00:07:54,880 --> 00:08:00,820
why are we still buying into this model

00:07:58,240 --> 00:08:02,830
but I also think I'd like these fines

00:08:00,820 --> 00:08:05,290
maximun the new laws we're creating in

00:08:02,830 --> 00:08:07,180
these groups where I think we could kind

00:08:05,290 --> 00:08:08,950
of choose to look at them as as symptoms

00:08:07,180 --> 00:08:11,560
of change and change it doesn't happen

00:08:08,950 --> 00:08:12,850
very fast all the time but it consists

00:08:11,560 --> 00:08:16,780
of different things and we are having

00:08:12,850 --> 00:08:18,850
like I've at least seen with in working

00:08:16,780 --> 00:08:21,190
in this field within the last even just

00:08:18,850 --> 00:08:26,260
the last five years an incredible change

00:08:21,190 --> 00:08:28,090
in in how we are addressing the risks

00:08:26,260 --> 00:08:29,920
and the issues of Facebook and Google

00:08:28,090 --> 00:08:32,830
and the way they deal with things in the

00:08:29,920 --> 00:08:34,510
press in in normal speech I remember

00:08:32,830 --> 00:08:37,000
going into a room and having to fight

00:08:34,510 --> 00:08:39,219
for these things and now when I go in we

00:08:37,000 --> 00:08:40,900
have people nodding and I'm sure anyone

00:08:39,219 --> 00:08:43,719
sitting here has the same that it's it's

00:08:40,900 --> 00:08:45,760
getting so easy to talk about but that

00:08:43,719 --> 00:08:47,860
that of course doesn't mean that that

00:08:45,760 --> 00:08:49,010
that it's happening that it that the

00:08:47,860 --> 00:08:51,199
change are happening

00:08:49,010 --> 00:08:54,709
but it means that we have a support for

00:08:51,199 --> 00:08:56,089
four and if I made it it's not it's not

00:08:54,709 --> 00:08:57,589
so much because sometimes you get the

00:08:56,089 --> 00:08:59,449
impression it's about you know having

00:08:57,589 --> 00:09:02,360
your revenge on Facebook because they're

00:08:59,449 --> 00:09:04,730
making so much money you know and they

00:09:02,360 --> 00:09:06,769
behave so badly so we need to punish

00:09:04,730 --> 00:09:08,750
them for that but of course what we are

00:09:06,769 --> 00:09:12,740
trying to and this is where your

00:09:08,750 --> 00:09:15,560
question is is going how do we how can

00:09:12,740 --> 00:09:18,829
we change their behavior and I think

00:09:15,560 --> 00:09:20,420
it's not going to work with fines of

00:09:18,829 --> 00:09:22,760
course they can hurt and they should be

00:09:20,420 --> 00:09:28,100
high enough to hurt but they also need

00:09:22,760 --> 00:09:30,260
to be really well justified but I see as

00:09:28,100 --> 00:09:32,149
the main problem with the large

00:09:30,260 --> 00:09:33,800
companies and we shouldn't just talk

00:09:32,149 --> 00:09:35,720
about the large companies because they

00:09:33,800 --> 00:09:37,670
are specific but of course they pose one

00:09:35,720 --> 00:09:39,740
of the biggest problems with the large

00:09:37,670 --> 00:09:42,500
companies it is the lack of

00:09:39,740 --> 00:09:45,889
accountability right because they just

00:09:42,500 --> 00:09:48,139
don't give a damn about what we think

00:09:45,889 --> 00:09:50,690
what our governments think you know what

00:09:48,139 --> 00:09:52,279
society is think and this is what we

00:09:50,690 --> 00:09:53,990
need to address we have to come up with

00:09:52,279 --> 00:09:55,819
better ideas for holding them

00:09:53,990 --> 00:09:58,790
accountable so that they can't just say

00:09:55,819 --> 00:10:00,920
oh yeah we'll throw some AI on it or

00:09:58,790 --> 00:10:03,230
we'll hire a couple of thousand people

00:10:00,920 --> 00:10:05,110
in Indonesia to moderate our content and

00:10:03,230 --> 00:10:07,940
then this problem will probably go away

00:10:05,110 --> 00:10:09,920
this this is not going to work we need

00:10:07,940 --> 00:10:12,529
to make them accountable to individuals

00:10:09,920 --> 00:10:14,060
and to other stakeholders so that they

00:10:12,529 --> 00:10:16,970
feel the pressure that they're really

00:10:14,060 --> 00:10:20,540
doing some good stuff right you know I

00:10:16,970 --> 00:10:22,940
always say that at the same time but

00:10:20,540 --> 00:10:25,370
where they fail you know they that needs

00:10:22,940 --> 00:10:30,380
to be addressed and we need to be able

00:10:25,370 --> 00:10:32,510
as citizens to address that me I on it

00:10:30,380 --> 00:10:35,300
so fine to me I think one thing I would

00:10:32,510 --> 00:10:36,860
add a green tie early with Matias was

00:10:35,300 --> 00:10:39,139
saying but I think one lesson that we

00:10:36,860 --> 00:10:40,490
should draw from this is that the and

00:10:39,139 --> 00:10:42,670
it's an obvious one but we should remind

00:10:40,490 --> 00:10:46,100
ourselves that the market does not value

00:10:42,670 --> 00:10:48,170
responsible AI something that Fanny just

00:10:46,100 --> 00:10:49,790
pointed to like Facebook shares keep key

00:10:48,170 --> 00:10:51,889
the share price keep going up so that

00:10:49,790 --> 00:10:53,899
the market does not value this so how do

00:10:51,889 --> 00:10:56,260
we create a market through appropriate

00:10:53,899 --> 00:10:58,760
regulatory interventions that makes

00:10:56,260 --> 00:11:00,280
responsible AI and investments and

00:10:58,760 --> 00:11:02,500
explained ability

00:11:00,280 --> 00:11:07,150
and you know accountability and AI

00:11:02,500 --> 00:11:08,890
systems and and like a prerequisite to

00:11:07,150 --> 00:11:10,690
developing this technology and not just

00:11:08,890 --> 00:11:12,730
something that like the company will do

00:11:10,690 --> 00:11:14,860
out of its own goodwill and if they can

00:11:12,730 --> 00:11:16,540
convince investors that this is a

00:11:14,860 --> 00:11:20,500
worthwhile cost just give you guys an

00:11:16,540 --> 00:11:22,000
example at our company there's 30 to 40%

00:11:20,500 --> 00:11:24,520
of product development is explained

00:11:22,000 --> 00:11:26,410
ability you know that gets difficult to

00:11:24,520 --> 00:11:28,000
justify when you're getting to the end

00:11:26,410 --> 00:11:29,410
of a financing round well when you're

00:11:28,000 --> 00:11:31,000
when you're trying to raise the next one

00:11:29,410 --> 00:11:32,620
and you're running out of cash and you

00:11:31,000 --> 00:11:33,850
have all that all all the goodwill in

00:11:32,620 --> 00:11:37,510
the world you know when the rubber

00:11:33,850 --> 00:11:39,550
rubber hits the road and you're burning

00:11:37,510 --> 00:11:42,370
you know X amount of money per month it

00:11:39,550 --> 00:11:43,660
gets tough to justify because you you

00:11:42,370 --> 00:11:46,600
have to you have to keep the company

00:11:43,660 --> 00:11:48,460
going right so you know there we need to

00:11:46,600 --> 00:11:50,230
we need to think I think even more about

00:11:48,460 --> 00:11:51,970
instead of regulatory protections but

00:11:50,230 --> 00:11:53,890
the type of incentive structures that

00:11:51,970 --> 00:11:56,440
could be built into the market to

00:11:53,890 --> 00:11:57,820
privilege you know the investing in

00:11:56,440 --> 00:11:59,920
those texts and technologies it could be

00:11:57,820 --> 00:12:01,630
through procurement strategic

00:11:59,920 --> 00:12:03,550
procurement efforts and pilot programs

00:12:01,630 --> 00:12:07,050
that are showing up that technology as

00:12:03,550 --> 00:12:10,000
like the fund the fundamental base layer

00:12:07,050 --> 00:12:13,480
you know and then we might we might get

00:12:10,000 --> 00:12:16,120
get get somewhere get somewhere new is

00:12:13,480 --> 00:12:18,339
that like when you say that the market

00:12:16,120 --> 00:12:21,010
is not valuing responsible yeah like

00:12:18,339 --> 00:12:23,110
that means that just let's lose all

00:12:21,010 --> 00:12:24,760
these adjectives for a moment malavika

00:12:23,110 --> 00:12:26,620
arm has been talking about this for

00:12:24,760 --> 00:12:29,170
years where are all the conferences and

00:12:26,620 --> 00:12:32,170
companies developing irresponsible AI or

00:12:29,170 --> 00:12:37,390
AI for social bed or untrustworthy AI

00:12:32,170 --> 00:12:39,910
and so for me this means that already

00:12:37,390 --> 00:12:42,040
there is a market failure and there are

00:12:39,910 --> 00:12:44,410
existing like regulations that we need

00:12:42,040 --> 00:12:45,850
to apply first and to go back to your

00:12:44,410 --> 00:12:50,650
point about the complex information

00:12:45,850 --> 00:12:52,480
processing if if in 15 1955 they would

00:12:50,650 --> 00:12:55,210
have chosen this complex information

00:12:52,480 --> 00:12:57,130
processing to describe this method

00:12:55,210 --> 00:12:59,280
instead of artificial intelligence to

00:12:57,130 --> 00:13:02,589
get more funding would it be more

00:12:59,280 --> 00:13:04,990
appealing to lawmakers to sit in front

00:13:02,589 --> 00:13:07,600
of a blanket paper and write although on

00:13:04,990 --> 00:13:09,790
it I I doubt it I think we should look

00:13:07,600 --> 00:13:12,810
at the impacts on individuals and

00:13:09,790 --> 00:13:15,270
societies and first and force and also

00:13:12,810 --> 00:13:17,340
the gaps and regulate that area not the

00:13:15,270 --> 00:13:18,630
technology yeah yeah you're talking for

00:13:17,340 --> 00:13:20,610
about companies and it reminded me of

00:13:18,630 --> 00:13:21,840
something that Matthias you were talking

00:13:20,610 --> 00:13:23,520
with the panelists about I was

00:13:21,840 --> 00:13:24,870
eavesdropping where you're saying you

00:13:23,520 --> 00:13:26,790
know you need a license to practice

00:13:24,870 --> 00:13:28,050
medicine you need a license to drive a

00:13:26,790 --> 00:13:31,260
car but anyone could just make I I

00:13:28,050 --> 00:13:33,600
should we have licenses to AI do you

00:13:31,260 --> 00:13:36,660
think that would help you know I'm

00:13:33,600 --> 00:13:38,630
skeptical about that but it was just you

00:13:36,660 --> 00:13:40,650
know we started discussing this with

00:13:38,630 --> 00:13:42,300
mentioning that there's apparently a

00:13:40,650 --> 00:13:43,980
proposal in Canada it you can probably

00:13:42,300 --> 00:13:47,280
talk about where that is suggested I'm

00:13:43,980 --> 00:13:50,340
skeptical about this because I I'm all

00:13:47,280 --> 00:13:52,260
for having changes in curricula

00:13:50,340 --> 00:13:54,030
you know for example computer engineers

00:13:52,260 --> 00:13:58,110
and computer scientists learning more

00:13:54,030 --> 00:14:01,800
about responsible work with what they're

00:13:58,110 --> 00:14:03,690
doing that would apply to biologists and

00:14:01,800 --> 00:14:05,490
it does apply to biologists you know to

00:14:03,690 --> 00:14:07,410
people who work with a genetic

00:14:05,490 --> 00:14:08,970
engineering and so on and so forth it's

00:14:07,410 --> 00:14:11,700
not working there either by the way you

00:14:08,970 --> 00:14:13,170
know my wife studied biology she didn't

00:14:11,700 --> 00:14:17,190
have one ethics class in her entire

00:14:13,170 --> 00:14:19,770
program so that that is hard to address

00:14:17,190 --> 00:14:21,600
I don't know whether we should use this

00:14:19,770 --> 00:14:22,770
licensing idea to probably increase the

00:14:21,600 --> 00:14:24,750
pressure or something it's the first

00:14:22,770 --> 00:14:27,830
time I start thinking about this I would

00:14:24,750 --> 00:14:30,390
talk about a little about more about

00:14:27,830 --> 00:14:32,700
standardization but probably you know we

00:14:30,390 --> 00:14:34,590
finished our licensing idea first yeah

00:14:32,700 --> 00:14:36,750
well I know that you guys mentioned you

00:14:34,590 --> 00:14:39,360
want to get more time than usual yeah

00:14:36,750 --> 00:14:41,460
Q&A so think of questions now but the

00:14:39,360 --> 00:14:43,230
last question I'll ask you agree and it

00:14:41,460 --> 00:14:45,000
anyone can answer you're talking about

00:14:43,230 --> 00:14:46,560
for about all the the kind of headlines

00:14:45,000 --> 00:14:48,330
were seeing about all the problems being

00:14:46,560 --> 00:14:50,490
addressed of social media companies do

00:14:48,330 --> 00:14:52,320
you feel like there might be a placebo

00:14:50,490 --> 00:14:54,180
effect where if you're not Mozilla

00:14:52,320 --> 00:14:55,980
mosfets audience member just a regular

00:14:54,180 --> 00:14:57,390
person seeing the headlines of Facebook

00:14:55,980 --> 00:14:58,770
did this and this is what the government

00:14:57,390 --> 00:15:00,540
like you know Zuckerberg there's a

00:14:58,770 --> 00:15:01,590
congressional hearing do you think there

00:15:00,540 --> 00:15:02,790
might be a placebo effect or it's like

00:15:01,590 --> 00:15:04,380
oh the problems are being solved let me

00:15:02,790 --> 00:15:05,820
go back to using Facebook do you thin

00:15:04,380 --> 00:15:07,800
like kind of explains like the

00:15:05,820 --> 00:15:10,160
increasing stock price like do you think

00:15:07,800 --> 00:15:13,800
it'll see back there

00:15:10,160 --> 00:15:14,880
see I I don't know I think I think I was

00:15:13,800 --> 00:15:17,490
sitting and thinking here I was

00:15:14,880 --> 00:15:19,080
listening to also what element hey I the

00:15:17,490 --> 00:15:22,470
you as a company and I was thinking I

00:15:19,080 --> 00:15:25,380
wanted to ask you like how what because

00:15:22,470 --> 00:15:26,310
element AI for example is known as a

00:15:25,380 --> 00:15:28,110
kind of sustained

00:15:26,310 --> 00:15:30,720
company and thinking about these

00:15:28,110 --> 00:15:32,699
questions and then I hear that you get I

00:15:30,720 --> 00:15:35,959
got possibly surprised that you got a

00:15:32,699 --> 00:15:38,910
big round of funding and I was one of

00:15:35,959 --> 00:15:41,339
their kind of profile was part of it and

00:15:38,910 --> 00:15:43,199
and maybe that's where we have to start

00:15:41,339 --> 00:15:44,819
in the what we talk about it not so much

00:15:43,199 --> 00:15:46,439
because I always thought that it was

00:15:44,819 --> 00:15:48,990
problematic that we put everything on

00:15:46,439 --> 00:15:51,209
the shoulders of the consumers and in

00:15:48,990 --> 00:15:53,309
consumer action and these things in this

00:15:51,209 --> 00:15:55,319
has to be solved by the companies that

00:15:53,309 --> 00:15:57,600
are building things to investors that

00:15:55,319 --> 00:16:00,509
are investing in specific solutions and

00:15:57,600 --> 00:16:04,319
asking for specific results and it has

00:16:00,509 --> 00:16:06,449
to be salt in law so I think that's more

00:16:04,319 --> 00:16:08,819
you know maybe maybe there's a placebo

00:16:06,449 --> 00:16:11,490
effect maybe there's not but in the end

00:16:08,819 --> 00:16:14,670
it is something that has to be sold by

00:16:11,490 --> 00:16:16,379
by some of the actual responsible actors

00:16:14,670 --> 00:16:18,389
that are building these infrastructures

00:16:16,379 --> 00:16:21,389
so I don't know if that's answering your

00:16:18,389 --> 00:16:23,040
question but but that's more changing

00:16:21,389 --> 00:16:25,139
the direction of the question yeah yeah

00:16:23,040 --> 00:16:26,790
all right so we'll end it there because

00:16:25,139 --> 00:16:28,170
they wanted to hear what you guys had to

00:16:26,790 --> 00:16:30,029
say so with any questions in the

00:16:28,170 --> 00:16:34,050
audience the one up here we went over

00:16:30,029 --> 00:16:37,649
there I let Kevin decide who we hear

00:16:34,050 --> 00:16:40,350
from first right there hi so I agree

00:16:37,649 --> 00:16:42,660
that self regulation for companies doing

00:16:40,350 --> 00:16:44,910
AI is not a good idea but what options

00:16:42,660 --> 00:16:47,040
are there for you know small startups

00:16:44,910 --> 00:16:51,449
like what can we do to move away from

00:16:47,040 --> 00:16:55,949
that model of things I guess that's

00:16:51,449 --> 00:16:58,649
directed to me cool I mean anybody could

00:16:55,949 --> 00:17:00,660
answer that to you but I'll start you

00:16:58,649 --> 00:17:02,040
know I think it's it's important one

00:17:00,660 --> 00:17:03,540
thing that we've learned and just give

00:17:02,040 --> 00:17:05,069
you the background we're a company that

00:17:03,540 --> 00:17:08,699
was founded three years ago and we now

00:17:05,069 --> 00:17:11,280
have 500 employees so it was the scaling

00:17:08,699 --> 00:17:13,140
of you know happened very quickly there

00:17:11,280 --> 00:17:14,939
was there's the phases when I was hired

00:17:13,140 --> 00:17:17,039
where we were hiring 20 employees every

00:17:14,939 --> 00:17:18,959
two weeks and so when you're doing when

00:17:17,039 --> 00:17:22,319
you're doing that type of growth it's

00:17:18,959 --> 00:17:23,970
it's very difficult I think to to ensure

00:17:22,319 --> 00:17:25,649
that the ethical framework you've

00:17:23,970 --> 00:17:28,470
developed gets operationalized

00:17:25,649 --> 00:17:30,600
socialized and passed down to all the

00:17:28,470 --> 00:17:33,000
new hires but you have to do that from

00:17:30,600 --> 00:17:35,610
the beginning otherwise you it's like

00:17:33,000 --> 00:17:36,990
really hard to catch up so so I would

00:17:35,610 --> 00:17:40,260
say like you know that's that's one

00:17:36,990 --> 00:17:42,600
thing to like invest in

00:17:40,260 --> 00:17:46,430
invest early on in an explained ability

00:17:42,600 --> 00:17:46,430
team this is like a core component of

00:17:52,010 --> 00:17:57,390
looking looking for a word but of AI

00:17:54,750 --> 00:17:59,640
that you know it can be that that can

00:17:57,390 --> 00:18:04,260
that can be that is lawful and that is

00:17:59,640 --> 00:18:07,100
that can that can be you know contested

00:18:04,260 --> 00:18:09,480
and that and that will be more just so

00:18:07,100 --> 00:18:12,300
those are like those are two things and

00:18:09,480 --> 00:18:13,860
then something that we're working on now

00:18:12,300 --> 00:18:15,840
is to to look at different types of

00:18:13,860 --> 00:18:17,910
impact assessments on on products that

00:18:15,840 --> 00:18:19,470
are a little bit more mature because you

00:18:17,910 --> 00:18:21,030
know the you probably know if you're

00:18:19,470 --> 00:18:22,440
working in a start-up the product

00:18:21,030 --> 00:18:24,330
roadmap is it can also be a bit of a

00:18:22,440 --> 00:18:26,490
moving target so like do you do you

00:18:24,330 --> 00:18:28,460
invest heavily into like a kind of a new

00:18:26,490 --> 00:18:30,450
process for this type for this design

00:18:28,460 --> 00:18:31,440
when you're not even sure that it's

00:18:30,450 --> 00:18:33,720
going to be something that you roll out

00:18:31,440 --> 00:18:37,800
right so I think it's a it's like a

00:18:33,720 --> 00:18:40,380
tough game Duff negotiation to do but

00:18:37,800 --> 00:18:42,330
like also start early with the type of

00:18:40,380 --> 00:18:44,550
if you know that you're the main risk of

00:18:42,330 --> 00:18:45,810
your system might be you know bias and

00:18:44,550 --> 00:18:47,700
discrimination which is often the case

00:18:45,810 --> 00:18:49,710
and it isn't some of our products then

00:18:47,700 --> 00:18:52,260
just start early you know so I think

00:18:49,710 --> 00:18:54,150
that's what I would say and then start

00:18:52,260 --> 00:18:56,550
speaking to regulators companies don't

00:18:54,150 --> 00:18:58,920
always have to only always have the the

00:18:56,550 --> 00:19:01,910
budget early on for like a policy team

00:18:58,920 --> 00:19:04,560
but you know executives engineers

00:19:01,910 --> 00:19:06,900
anybody can do this have regular

00:19:04,560 --> 00:19:09,120
conversations with your regulators and

00:19:06,900 --> 00:19:10,980
and to get to get to know the regulatory

00:19:09,120 --> 00:19:12,450
framework that in which you're you're

00:19:10,980 --> 00:19:14,100
you're situated and help them get to

00:19:12,450 --> 00:19:16,110
know what type of technology you're

00:19:14,100 --> 00:19:17,670
building and what some of those so some

00:19:16,110 --> 00:19:19,710
of those risks are and and then you can

00:19:17,670 --> 00:19:22,320
come to like an appropriate you know set

00:19:19,710 --> 00:19:23,730
of policy options that can that can

00:19:22,320 --> 00:19:26,130
allow your tech now your company to grow

00:19:23,730 --> 00:19:29,190
and and hopefully in the right way that

00:19:26,130 --> 00:19:32,670
protects people I also have one

00:19:29,190 --> 00:19:36,630
suggestion I think for startups it's

00:19:32,670 --> 00:19:38,400
probably quite upsetting to see how low

00:19:36,630 --> 00:19:40,380
and all the policy conversations are

00:19:38,400 --> 00:19:43,980
targeted at these main actors especially

00:19:40,380 --> 00:19:46,890
if you're come with a like a genuine

00:19:43,980 --> 00:19:48,810
purpose of wire company exists and so

00:19:46,890 --> 00:19:50,640
what I would recommend we are trying to

00:19:48,810 --> 00:19:53,470
create a common language and narrative

00:19:50,640 --> 00:19:55,600
around automated decision making

00:19:53,470 --> 00:19:59,139
and we call it this myth-busting

00:19:55,600 --> 00:20:01,960
exercise that mozilla and daniel do for

00:19:59,139 --> 00:20:04,360
a fellow is working on hosted access now

00:20:01,960 --> 00:20:07,659
and we are looking for contributors who

00:20:04,360 --> 00:20:10,240
helped us create truth and demystifying

00:20:07,659 --> 00:20:13,299
all this around it and so

00:20:10,240 --> 00:20:15,940
because it matters to you what the tech

00:20:13,299 --> 00:20:18,009
the tech can can achieve but also the

00:20:15,940 --> 00:20:20,230
limitations like both the skills and the

00:20:18,009 --> 00:20:28,240
limitation so if you're interested and

00:20:20,230 --> 00:20:30,190
talk to us next question okay hi I have

00:20:28,240 --> 00:20:33,129
two remarks and one questions so first

00:20:30,190 --> 00:20:36,549
remark the first transatlantic boats or

00:20:33,129 --> 00:20:39,159
ships didn't have live boats on them the

00:20:36,549 --> 00:20:42,370
first cars came without seatbelts and

00:20:39,159 --> 00:20:44,440
people were flying out of windows when

00:20:42,370 --> 00:20:46,870
they were in an accident what I'm saying

00:20:44,440 --> 00:20:50,470
is that industry was throughout history

00:20:46,870 --> 00:20:52,570
very bad at effective self regulating

00:20:50,470 --> 00:20:57,129
something that was obvious from the

00:20:52,570 --> 00:21:01,149
beginning so here's my question how do

00:20:57,129 --> 00:21:04,059
we make this whole process of regulation

00:21:01,149 --> 00:21:07,769
and self-regulation and whatever lies in

00:21:04,059 --> 00:21:10,750
between move away from this spectacle of

00:21:07,769 --> 00:21:12,460
here's five people on stage we're going

00:21:10,750 --> 00:21:15,370
to save the world in one hour and then

00:21:12,460 --> 00:21:17,889
we're gonna go for a beer to the place

00:21:15,370 --> 00:21:20,230
where we go okay you do this I'll do

00:21:17,889 --> 00:21:22,690
this I'll do this and you do this and in

00:21:20,230 --> 00:21:24,639
the end hopefully every boat will every

00:21:22,690 --> 00:21:26,259
ship will have a lifeboat and every car

00:21:24,639 --> 00:21:30,850
will have a seatbelt we're actually

00:21:26,259 --> 00:21:33,429
gonna save 140 minutes but that's really

00:21:30,850 --> 00:21:38,799
unfair I think I I thought we'd come up

00:21:33,429 --> 00:21:41,500
with a solution but seriously yeah of

00:21:38,799 --> 00:21:44,590
course you're right and and this is also

00:21:41,500 --> 00:21:48,309
I think the four of us agree on that

00:21:44,590 --> 00:21:51,850
that everyone has to work on different

00:21:48,309 --> 00:21:54,730
parts of this problem and even saying

00:21:51,850 --> 00:21:56,379
this problem is a problem because we are

00:21:54,730 --> 00:21:58,029
not looking at one problem with one

00:21:56,379 --> 00:22:00,639
solution but we're looking at a ton of

00:21:58,029 --> 00:22:04,090
different challenges that that need to

00:22:00,639 --> 00:22:07,240
be solved and you know one of the

00:22:04,090 --> 00:22:09,180
ideas is that for example like in the

00:22:07,240 --> 00:22:11,950
car industry and in other industries

00:22:09,180 --> 00:22:14,800
there can be better standardization that

00:22:11,950 --> 00:22:17,500
then at the same time means that people

00:22:14,800 --> 00:22:20,170
in companies know better what to do how

00:22:17,500 --> 00:22:22,600
to develop their stuff for it to be for

00:22:20,170 --> 00:22:24,160
example more transparent and allow for

00:22:22,600 --> 00:22:26,440
better accountability and explain

00:22:24,160 --> 00:22:28,240
ability and then the regulator can for

00:22:26,440 --> 00:22:30,240
example decide to give some liability

00:22:28,240 --> 00:22:33,100
privileges because if you apply these

00:22:30,240 --> 00:22:35,380
standards then if something goes wrong

00:22:33,100 --> 00:22:38,770
you can at least prove that you've used

00:22:35,380 --> 00:22:42,040
the state-of-the-art testing and in

00:22:38,770 --> 00:22:45,190
whatever it is I now also think that it

00:22:42,040 --> 00:22:48,970
will take a long time to come up with

00:22:45,190 --> 00:22:50,440
all of this and again it's never done in

00:22:48,970 --> 00:22:52,000
Germany I don't know the numbers in

00:22:50,440 --> 00:22:55,180
other countries but in Germany we still

00:22:52,000 --> 00:22:58,960
have 3,000 fatalities each year on the

00:22:55,180 --> 00:23:01,660
roads right that is a very high price we

00:22:58,960 --> 00:23:04,300
accept for mobility right but it has

00:23:01,660 --> 00:23:06,220
been much higher before and you know

00:23:04,300 --> 00:23:08,740
everyone is still working on this it's

00:23:06,220 --> 00:23:11,470
in the end it's about a society coming

00:23:08,740 --> 00:23:13,840
to sort of a conclusion on what we are

00:23:11,470 --> 00:23:15,280
willing to accept agree you want to jump

00:23:13,840 --> 00:23:17,020
in yeah you

00:23:15,280 --> 00:23:19,030
I mean I was just thinking about you

00:23:17,020 --> 00:23:21,070
know mentioning the cars and everything

00:23:19,030 --> 00:23:23,110
and if we think of the history of cars

00:23:21,070 --> 00:23:25,360
and and pollution and all these things

00:23:23,110 --> 00:23:27,820
then we can think back in the 1950s

00:23:25,360 --> 00:23:29,620
where cars were we're polluting a lot

00:23:27,820 --> 00:23:31,630
and we had we were building this there

00:23:29,620 --> 00:23:33,220
were this gigantic American cars going

00:23:31,630 --> 00:23:36,160
around for example which was the most

00:23:33,220 --> 00:23:39,700
popular cars and we get the 1970 there's

00:23:36,160 --> 00:23:41,500
a clean act regulatory approach then you

00:23:39,700 --> 00:23:43,830
get scientists that are tracing

00:23:41,500 --> 00:23:47,250
pollution back to to cars and then

00:23:43,830 --> 00:23:49,930
everything evolves with different every

00:23:47,250 --> 00:23:52,060
group has their own expertise that they

00:23:49,930 --> 00:23:53,440
work together with on one circle the

00:23:52,060 --> 00:23:56,500
next goal is of course we could if we

00:23:53,440 --> 00:23:58,630
could get electric cars but it's more to

00:23:56,500 --> 00:24:01,390
say that you know every every stage and

00:23:58,630 --> 00:24:03,100
every actor has their own expertise that

00:24:01,390 --> 00:24:04,630
we somehow have to that's what I'm

00:24:03,100 --> 00:24:05,980
hoping that we can kind of do in these

00:24:04,630 --> 00:24:09,120
kind of forms is this is an

00:24:05,980 --> 00:24:13,000
awareness-raising exercise and hopefully

00:24:09,120 --> 00:24:15,450
whatever this exercise gets into it will

00:24:13,000 --> 00:24:16,700
feed into these processes of regulatory

00:24:15,450 --> 00:24:22,130
frameworks

00:24:16,700 --> 00:24:24,050
of engineering of scientists that are

00:24:22,130 --> 00:24:26,390
looking into the problems and the risks

00:24:24,050 --> 00:24:28,280
of AI and then in the end will probably

00:24:26,390 --> 00:24:30,860
be somewhere where for example we have

00:24:28,280 --> 00:24:33,260
and a scandal like the Volkswagen

00:24:30,860 --> 00:24:35,060
scandal where you have engineers that

00:24:33,260 --> 00:24:37,580
are actually prosecuting for building

00:24:35,060 --> 00:24:39,950
cheap devices for for cars that are

00:24:37,580 --> 00:24:41,930
polluting too much so so I think this is

00:24:39,950 --> 00:24:47,060
something we can learn from history yeah

00:24:41,930 --> 00:24:49,730
and yes I have a few concrete

00:24:47,060 --> 00:24:51,710
suggestions how to solve not how to save

00:24:49,730 --> 00:24:53,630
the world how to solve some of these

00:24:51,710 --> 00:24:55,780
problems so first of all I am a human

00:24:53,630 --> 00:24:59,030
rights lawyer so if I don't believe in

00:24:55,780 --> 00:25:01,460
saving the world and who will and I hope

00:24:59,030 --> 00:25:04,340
you join me in this but so the concrete

00:25:01,460 --> 00:25:06,980
the concrete topic there are very basic

00:25:04,340 --> 00:25:10,460
areas where even though we still believe

00:25:06,980 --> 00:25:12,470
in the European values and fundamental

00:25:10,460 --> 00:25:14,480
rights frameworks and the enforcement of

00:25:12,470 --> 00:25:16,700
it but still there are failures already

00:25:14,480 --> 00:25:18,950
so there are two areas we can talk about

00:25:16,700 --> 00:25:21,310
AI as much as we want but as long as we

00:25:18,950 --> 00:25:23,600
don't have proper basic online

00:25:21,310 --> 00:25:26,350
protections for privacy for instance

00:25:23,600 --> 00:25:29,540
then what are we even talking about here

00:25:26,350 --> 00:25:31,280
not let's not let the European Union and

00:25:29,540 --> 00:25:33,770
get away with not concluding the e

00:25:31,280 --> 00:25:37,160
privacy reform that's one and the other

00:25:33,770 --> 00:25:39,440
one the EU there are you based companies

00:25:37,160 --> 00:25:41,870
that are manufacturing technology that

00:25:39,440 --> 00:25:45,100
is being sold outside the European Union

00:25:41,870 --> 00:25:47,690
as and it's used to kill to surveil

00:25:45,100 --> 00:25:50,240
journalists human rights defenders

00:25:47,690 --> 00:25:52,250
political political opponents so

00:25:50,240 --> 00:25:56,030
surveillance export control is another

00:25:52,250 --> 00:25:59,150
very concrete topic where we can all get

00:25:56,030 --> 00:26:01,760
better results in order for any a I

00:25:59,150 --> 00:26:03,310
related regulation to be effective and

00:26:01,760 --> 00:26:06,590
meaningful

00:26:03,310 --> 00:26:08,900
okay thanks want to make two points I

00:26:06,590 --> 00:26:12,590
think in response to the question which

00:26:08,900 --> 00:26:14,570
you know may not be super satisfying but

00:26:12,590 --> 00:26:16,670
I think like Matias said it's an ongoing

00:26:14,570 --> 00:26:19,400
effort it's gonna and it's going to be

00:26:16,670 --> 00:26:21,650
you chip away at it progressively with a

00:26:19,400 --> 00:26:23,540
group of people maybe even you know

00:26:21,650 --> 00:26:24,890
people here today but last week Fanny

00:26:23,540 --> 00:26:27,980
and I were at a conference that element

00:26:24,890 --> 00:26:29,870
a I organized with Mozilla to do kind of

00:26:27,980 --> 00:26:30,470
just that we got together 15 15

00:26:29,870 --> 00:26:33,530
different people

00:26:30,470 --> 00:26:35,390
from 15 different organizations across

00:26:33,530 --> 00:26:36,470
stakeholder groups and we've pre spent

00:26:35,390 --> 00:26:37,789
two and a half days together trying to

00:26:36,470 --> 00:26:39,890
hash out what a regulatory framework

00:26:37,789 --> 00:26:41,270
could it for AI could look like or a

00:26:39,890 --> 00:26:43,700
governance framework whatever you want

00:26:41,270 --> 00:26:46,010
to call it from shoring up expertise on

00:26:43,700 --> 00:26:47,210
AI and its risks within you know

00:26:46,010 --> 00:26:48,950
different government departments so that

00:26:47,210 --> 00:26:51,890
they're able to more meaningfully apply

00:26:48,950 --> 00:26:54,340
their legislation give it with a you

00:26:51,890 --> 00:26:57,200
know with giving regard to a as risks or

00:26:54,340 --> 00:26:59,600
you know to have different reporting

00:26:57,200 --> 00:27:00,590
mechanisms so I think I think you know

00:26:59,600 --> 00:27:02,150
it's gonna it's gonna happen

00:27:00,590 --> 00:27:03,770
progressively I think not we've seen

00:27:02,150 --> 00:27:05,870
that not a lot across the world not a

00:27:03,770 --> 00:27:07,159
lot of attention has been placed on this

00:27:05,870 --> 00:27:09,020
and the at least the national AI

00:27:07,159 --> 00:27:10,610
strategies but you know that's something

00:27:09,020 --> 00:27:12,230
that we have to have to do more work on

00:27:10,610 --> 00:27:14,000
it's going to happen progressively all

00:27:12,230 --> 00:27:16,220
that no I like to just like highlight in

00:27:14,000 --> 00:27:18,440
we talked about cars but I often think

00:27:16,220 --> 00:27:20,960
about the aviation system as like as an

00:27:18,440 --> 00:27:23,539
example of a space that has been highly

00:27:20,960 --> 00:27:26,720
innovative has become progressively more

00:27:23,539 --> 00:27:29,210
safe over time there's still a lot of

00:27:26,720 --> 00:27:31,100
you know issues related to to pollution

00:27:29,210 --> 00:27:33,320
but even even that I think will be

00:27:31,100 --> 00:27:35,390
changing in the next generation but it's

00:27:33,320 --> 00:27:37,070
it's an extremely sophisticated and

00:27:35,390 --> 00:27:39,159
onerous regulatory infrastructure from

00:27:37,070 --> 00:27:42,309
standardization to certification schemes

00:27:39,159 --> 00:27:44,840
to to oversight continuous monitoring

00:27:42,309 --> 00:27:48,200
and it didn't happen overnight and it

00:27:44,840 --> 00:27:50,480
took about 80 years and and then and

00:27:48,200 --> 00:27:53,630
then it but it served as a good model

00:27:50,480 --> 00:27:56,419
for something like complex drone

00:27:53,630 --> 00:27:58,000
operations and they borrowed bits and

00:27:56,419 --> 00:28:00,590
pieces from it in kind of a weak

00:27:58,000 --> 00:28:04,220
licensing process to provide some

00:28:00,590 --> 00:28:05,870
oversights and and and also you know

00:28:04,220 --> 00:28:07,850
bring those operations along to a place

00:28:05,870 --> 00:28:09,620
where they'll be subject to the more

00:28:07,850 --> 00:28:11,330
rigorous ambit of the regulatory

00:28:09,620 --> 00:28:12,710
framework so I think we've done this

00:28:11,330 --> 00:28:13,789
before in other industries we can do it

00:28:12,710 --> 00:28:15,799
with AI we're starting at the very

00:28:13,789 --> 00:28:17,360
beginning and then another thing I'll

00:28:15,799 --> 00:28:19,220
say and I realize we're almost running

00:28:17,360 --> 00:28:21,559
out of time but no we're not we're out

00:28:19,220 --> 00:28:24,530
to enter the regular Q&A so yeah okay

00:28:21,559 --> 00:28:26,840
cool is the is we have to think about

00:28:24,530 --> 00:28:29,780
new ways of doing data governance and

00:28:26,840 --> 00:28:32,240
there's been a few sessions at Moz fest

00:28:29,780 --> 00:28:34,820
on data governance and and specifically

00:28:32,240 --> 00:28:37,250
on the opportunity that something like

00:28:34,820 --> 00:28:38,659
data Trust's could present I also saw a

00:28:37,250 --> 00:28:41,310
session I wasn't able to attend on data

00:28:38,659 --> 00:28:43,740
unions but we need to think of

00:28:41,310 --> 00:28:45,690
of more ways from the very beginning

00:28:43,740 --> 00:28:48,000
governing how we use data and having

00:28:45,690 --> 00:28:50,280
more democratic representation and how

00:28:48,000 --> 00:28:53,820
we make choices about how data should be

00:28:50,280 --> 00:28:56,250
used greater accountability I think I

00:28:53,820 --> 00:28:57,960
think it starts there so I think a lot

00:28:56,250 --> 00:28:59,790
of the AI governance questions may be

00:28:57,960 --> 00:29:01,260
solved through different models and

00:28:59,790 --> 00:29:03,240
approaches to data governance right

00:29:01,260 --> 00:29:09,150
there's sometimes for some actually one

00:29:03,240 --> 00:29:12,750
more question I like Kevin aside i

00:29:09,150 --> 00:29:14,970
always wasn't expecting that just to

00:29:12,750 --> 00:29:18,030
build on the the reference to the

00:29:14,970 --> 00:29:20,100
aviation industry and the increased

00:29:18,030 --> 00:29:23,100
regulation incremental regulation over

00:29:20,100 --> 00:29:25,800
time that seems to have been happening

00:29:23,100 --> 00:29:30,300
in parallel or maybe actually directly

00:29:25,800 --> 00:29:32,970
against greater rights of workers in

00:29:30,300 --> 00:29:36,540
that industry and i'm really curious as

00:29:32,970 --> 00:29:39,230
to how you see the governance growing

00:29:36,540 --> 00:29:42,740
governments of AI without the loss of

00:29:39,230 --> 00:29:44,880
kind of worker power so that this

00:29:42,740 --> 00:29:48,510
emergency kana me serves people

00:29:44,880 --> 00:29:50,940
economically but also without the loss

00:29:48,510 --> 00:29:53,130
of accountability that comes from taking

00:29:50,940 --> 00:29:58,440
human beings out of more and more

00:29:53,130 --> 00:30:01,860
aspects of that governance to the second

00:29:58,440 --> 00:30:05,130
part and then so on the the

00:30:01,860 --> 00:30:07,890
accountability piece for instance one

00:30:05,130 --> 00:30:10,230
one key recommendation from the Council

00:30:07,890 --> 00:30:14,670
of Europe Human Rights Commissioners

00:30:10,230 --> 00:30:16,770
paper on this 10 steps to unbox AI one

00:30:14,670 --> 00:30:20,270
of the key recommendations is to always

00:30:16,770 --> 00:30:22,680
have either a natural or a legal person

00:30:20,270 --> 00:30:26,730
identified as accountable for the

00:30:22,680 --> 00:30:29,070
decision so it I think that's the answer

00:30:26,730 --> 00:30:34,260
like we cannot move away from from that

00:30:29,070 --> 00:30:37,020
governance structure this is a question

00:30:34,260 --> 00:30:39,450
related to workers rights more about the

00:30:37,020 --> 00:30:41,940
use of automation and autonomous systems

00:30:39,450 --> 00:30:50,040
in aviation or about conditions for

00:30:41,940 --> 00:30:51,290
workers in more generally both I think

00:30:50,040 --> 00:30:53,910
there's something that happens just

00:30:51,290 --> 00:30:54,840
psychologically once more and more

00:30:53,910 --> 00:30:58,620
governance is

00:30:54,840 --> 00:31:00,240
around automated systems that has the

00:30:58,620 --> 00:31:03,120
psychological effect of removing the

00:31:00,240 --> 00:31:06,420
person from the center that also makes

00:31:03,120 --> 00:31:09,360
accountability harder and then the issue

00:31:06,420 --> 00:31:12,660
of economic prosperity for human beings

00:31:09,360 --> 00:31:14,700
within that field as well I think yeah

00:31:12,660 --> 00:31:17,040
they feed each other the minute we start

00:31:14,700 --> 00:31:18,930
focusing so much on automated systems

00:31:17,040 --> 00:31:21,480
the less importance the human being

00:31:18,930 --> 00:31:23,850
becomes the easier it is to justify not

00:31:21,480 --> 00:31:25,980
paying them living a living wage or they

00:31:23,850 --> 00:31:28,440
are less integral to the system the

00:31:25,980 --> 00:31:31,080
harder it is to manage accountability

00:31:28,440 --> 00:31:32,160
and so on and so forth okay I'll try and

00:31:31,080 --> 00:31:34,920
answer some of those questions I think

00:31:32,160 --> 00:31:36,300
there are few sub sub questions but it

00:31:34,920 --> 00:31:37,710
reminds you a conversation with TAS and

00:31:36,300 --> 00:31:40,470
I were having before beforehand about

00:31:37,710 --> 00:31:45,390
the the blowing max max eighths and the

00:31:40,470 --> 00:31:46,880
use of autonomous systems and yeah well

00:31:45,390 --> 00:31:49,890
for the first thing I would say is that

00:31:46,880 --> 00:31:52,680
as in aviation the use of autonomous

00:31:49,890 --> 00:31:54,690
systems or just in the AI a eco system

00:31:52,680 --> 00:31:56,880
more broadly I think we should I think

00:31:54,690 --> 00:31:59,250
we should be very careful not to

00:31:56,880 --> 00:32:02,130
attribute accountability to anything

00:31:59,250 --> 00:32:03,600
other than a human and in some case that

00:32:02,130 --> 00:32:04,980
might be there might be a legal person

00:32:03,600 --> 00:32:07,470
or might be a physical person but I

00:32:04,980 --> 00:32:09,150
think that's that we should not be

00:32:07,470 --> 00:32:10,860
confused about that is the the the

00:32:09,150 --> 00:32:14,370
accountability and liability lies with

00:32:10,860 --> 00:32:19,650
with with a person and then with regards

00:32:14,370 --> 00:32:21,420
to workers yeah I mean there's the

00:32:19,650 --> 00:32:24,000
displacement of workers and the

00:32:21,420 --> 00:32:25,290
devaluing of workers this is a problem

00:32:24,000 --> 00:32:26,880
and this is something that the companies

00:32:25,290 --> 00:32:29,100
and governments also I mean civil

00:32:26,880 --> 00:32:31,020
society more broadly has to has to

00:32:29,100 --> 00:32:32,670
address and there needs to be there

00:32:31,020 --> 00:32:34,650
should be policies in place possibly

00:32:32,670 --> 00:32:36,590
with you know AI industries that are

00:32:34,650 --> 00:32:38,820
that are contributing to this

00:32:36,590 --> 00:32:42,420
participating in these in programs to

00:32:38,820 --> 00:32:44,250
upskill retrain workers or if it should

00:32:42,420 --> 00:32:46,410
they so desire or have some type of

00:32:44,250 --> 00:32:47,430
other compensation scheme you know these

00:32:46,410 --> 00:32:48,780
are some of the options that could be

00:32:47,430 --> 00:32:50,430
considered to alleviate some of the

00:32:48,780 --> 00:32:53,400
burden of the dislocation that workers

00:32:50,430 --> 00:32:55,640
will feel and I know I hope that answers

00:32:53,400 --> 00:32:58,620
in part at least some of your question

00:32:55,640 --> 00:33:00,240
chime in on that okay

00:32:58,620 --> 00:33:01,650
well I guess we can end the panel there

00:33:00,240 --> 00:33:03,130
give a big round of applause for our

00:33:01,650 --> 00:33:09,100
panelists

00:33:03,130 --> 00:33:10,870
Paige coming up next we have a long

00:33:09,100 --> 00:33:12,250
break would be back at 3:15 where we

00:33:10,870 --> 00:33:13,350
have one more talking one more panel

00:33:12,250 --> 00:33:16,350
I'll see you guys soon

00:33:13,350 --> 00:33:16,350
Thanks

02:03:52,030 --> 02:03:59,100
we will draw for the home the sermon

02:03:54,970 --> 02:03:59,100
okay so no speaker yeah

02:04:16,180 --> 02:04:20,120
[Music]

02:04:17,719 --> 02:04:21,800
all right it's three o'clock it is time

02:04:20,120 --> 02:04:24,860
for the all-important raffle drawing

02:04:21,800 --> 02:04:28,340
who's here for the raffle drawing all

02:04:24,860 --> 02:04:30,020
right who's gonna win okay good good all

02:04:28,340 --> 02:04:34,130
right a first thing we're gonna draw for

02:04:30,020 --> 02:04:37,640
is for the sono speaker so Sam B is our

02:04:34,130 --> 02:04:51,739
guest guest drower today so Sam do you

02:04:37,640 --> 02:04:55,850
want to do the honors okay and the

02:04:51,739 --> 02:04:58,370
winner is it's blue tickets the top

02:04:55,850 --> 02:05:01,570
letters and they matter today I won't

02:04:58,370 --> 02:05:03,469
mess this up well I might but the ad

02:05:01,570 --> 02:05:07,790
everybody's like oh I'm doing good so

02:05:03,469 --> 02:05:11,630
far right seven four eight four five

02:05:07,790 --> 02:05:15,160
five and the three letters that matter

02:05:11,630 --> 02:05:20,590
are I mean numbers that matter are five

02:05:15,160 --> 02:05:23,930
four four do we have a winner

02:05:20,590 --> 02:05:30,530
anybody have a D seven four eight four

02:05:23,930 --> 02:05:31,940
five five five four four no winners all

02:05:30,530 --> 02:05:35,090
right we're gonna put this up on the

02:05:31,940 --> 02:05:37,190
Mozilla Twitter account and and if we

02:05:35,090 --> 02:05:40,489
don't have a winner by 3:45 we will

02:05:37,190 --> 02:05:42,410
redraw for a new winner all right and

02:05:40,489 --> 02:05:48,770
now for the Apple watch

02:05:42,410 --> 02:05:49,340
ooh anticipation Rises Sam tries to open

02:05:48,770 --> 02:06:02,449
a box

02:05:49,340 --> 02:06:04,489
oh all right here we go stir it up good

02:06:02,449 --> 02:06:10,670
close your eyes eyes are closed I can

02:06:04,489 --> 02:06:12,739
verify that and we have a winner all

02:06:10,670 --> 02:06:15,949
right this is an orange ticket you have

02:06:12,739 --> 02:06:22,150
an orange ticket the number at the top

02:06:15,949 --> 02:06:24,800
is ad three nine four six five oh and

02:06:22,150 --> 02:06:30,350
the three the three numbers that matter

02:06:24,800 --> 02:06:32,540
are nine three one aid

02:06:30,350 --> 02:06:35,330
it's only slight now it's not rigged I

02:06:32,540 --> 02:06:35,870
promise the winner are you we have a

02:06:35,330 --> 02:06:38,270
winner

02:06:35,870 --> 02:06:42,770
come here Gump come up on stage show me

02:06:38,270 --> 02:06:44,950
your ticket ad we have a winner

02:06:42,770 --> 02:06:48,410
[Applause]

02:06:44,950 --> 02:06:50,240
alright alright so the apple watch is

02:06:48,410 --> 02:06:52,640
taken but if you're still hoping for the

02:06:50,240 --> 02:06:54,500
Sonos um don't go anywhere until 3:45

02:06:52,640 --> 02:06:55,760
maybe the winner will check on Twitter

02:06:54,500 --> 02:07:00,280
and find it but maybe they won't and

02:06:55,760 --> 02:07:00,280
will redraw 345 so thank you guys

02:08:16,960 --> 02:08:23,320
yeah yeah it's great it's fine it's my

02:08:19,340 --> 02:08:29,300
first moss fast oh really

02:08:23,320 --> 02:08:30,590
you texted me sweetly oh thanks oh yeah

02:08:29,300 --> 02:08:31,670
you're also doing two days in a row I

02:08:30,590 --> 02:08:33,940
don't think a lot of people are doing

02:08:31,670 --> 02:08:33,940
that

02:08:53,610 --> 02:08:59,250
she you should tell her that she might

02:08:55,780 --> 02:08:59,250
be your friend but like in this guy's

02:09:27,170 --> 02:09:30,800
yeah I would not want my

02:10:28,880 --> 02:10:31,880
yes

02:10:34,079 --> 02:10:38,900
what do you ask why do you ask

02:10:42,530 --> 02:10:47,570
yeah I forget this is like kind of more

02:10:44,579 --> 02:10:47,570
of a European focus

02:11:22,240 --> 02:11:26,140
and then you're done

02:12:10,070 --> 02:12:13,070
here

02:12:28,000 --> 02:12:31,000
okay

02:13:00,389 --> 02:13:50,889
evil I think I've heard people love

02:13:02,619 --> 02:13:55,649
turkey doesn't get rave reviews that's

02:13:50,889 --> 02:13:55,649
more than twice in New York that's a big

02:14:48,580 --> 02:15:11,810
yeah that's how it works

02:14:51,820 --> 02:15:14,590
customs works I live here now I'm

02:15:11,810 --> 02:15:14,590
stretching them

02:16:38,059 --> 02:16:42,200
how many companies afraid of me

02:17:02,420 --> 02:17:05,420
yeah

02:17:50,750 --> 02:17:53,929
follow me

02:18:38,450 --> 02:18:43,350
hey folks welcome back to MOSFETs 2019

02:18:41,910 --> 02:18:46,470
can get a round of applause for all the

02:18:43,350 --> 02:18:48,150
folks who have been around applause it's

02:18:46,470 --> 02:18:53,040
like you guys are asleep right now

02:18:48,150 --> 02:18:54,540
thank you so next up we have a talk from

02:18:53,040 --> 02:18:57,060
John sue janja she's one of the

02:18:54,540 --> 02:18:59,220
panelists yesterday she is the founder

02:18:57,060 --> 02:19:01,500
and director of the AI ethics lab and

02:18:59,220 --> 02:19:02,580
she also has a PhD in philosophy so I

02:19:01,500 --> 02:19:12,750
like to bring out John's to do right now

02:19:02,580 --> 02:19:15,180
you know 100 plus ok thanks so I will be

02:19:12,750 --> 02:19:18,240
talking about ethical AI which too

02:19:15,180 --> 02:19:20,010
quickly which is something that we are

02:19:18,240 --> 02:19:21,480
talking about so much recently that it

02:19:20,010 --> 02:19:24,060
seems like we've been talking about it

02:19:21,480 --> 02:19:27,750
for ages that's not really the case for

02:19:24,060 --> 02:19:30,180
the longest time ethics of AI or AI

02:19:27,750 --> 02:19:32,820
attics has been only something that is

02:19:30,180 --> 02:19:34,710
of interest of a handful of philosophers

02:19:32,820 --> 02:19:37,950
computer scientists and sci-fi authors

02:19:34,710 --> 02:19:39,930
basically only recently the public

02:19:37,950 --> 02:19:42,620
started caring about it and I can be

02:19:39,930 --> 02:19:45,841
even more specific so this is the

02:19:42,620 --> 02:19:48,631
worldwide Google search trends for the

02:19:45,841 --> 02:19:50,790
last past five years of the term AI

02:19:48,631 --> 02:19:54,210
ethics you can try different versions

02:19:50,790 --> 02:19:55,580
you know ethical AI ethics in AI and you

02:19:54,210 --> 02:20:00,061
will get more or less the same trend

02:19:55,580 --> 02:20:02,070
basically until 2016 when you set AI

02:20:00,061 --> 02:20:03,990
attics you would just get like plain

02:20:02,070 --> 02:20:06,420
blank stares people wouldn't know what

02:20:03,990 --> 02:20:11,040
you're talking about then things started

02:20:06,420 --> 02:20:14,551
changing in late 2017 and that is when

02:20:11,040 --> 02:20:16,681
we started having scandals and these are

02:20:14,551 --> 02:20:19,591
just a few of the major scandals there

02:20:16,681 --> 02:20:22,351
is a whole larger map out there if you

02:20:19,591 --> 02:20:24,960
search for it and the thing is these

02:20:22,351 --> 02:20:27,631
scandals started bringing home the

02:20:24,960 --> 02:20:31,110
message that technology and AI is not

02:20:27,631 --> 02:20:34,381
free from ethics it's not it's not

02:20:31,110 --> 02:20:36,601
ethics neutral on the contrary all of

02:20:34,381 --> 02:20:39,931
these ethical problems are really

02:20:36,601 --> 02:20:41,880
evolving into the phases of a building

02:20:39,931 --> 02:20:44,000
an AI system so from the research

02:20:41,880 --> 02:20:46,739
to design development deployment

02:20:44,000 --> 02:20:49,319
updating marketing all of them have

02:20:46,739 --> 02:20:53,040
different ethical questions and some of

02:20:49,319 --> 02:20:54,600
these ethical scandals are about social

02:20:53,040 --> 02:20:56,449
media and its effects on politics

02:20:54,600 --> 02:20:58,640
democracy some of them are about

02:20:56,449 --> 02:21:01,020
premature collaborations between

02:20:58,640 --> 02:21:03,359
companies tech companies and militaries

02:21:01,020 --> 02:21:06,720
and law enforcement and some about

02:21:03,359 --> 02:21:11,069
safety and security and privacy and how

02:21:06,720 --> 02:21:12,510
AI systems categorize us into you know

02:21:11,069 --> 02:21:15,540
who gets to get the job

02:21:12,510 --> 02:21:19,500
who gets the loan financial loan and who

02:21:15,540 --> 02:21:21,540
gets to the jail and I mean if you

02:21:19,500 --> 02:21:23,880
realize this is a problem for all of us

02:21:21,540 --> 02:21:26,220
so even if you are a developer or you

02:21:23,880 --> 02:21:28,199
are a lead at the tech company it's

02:21:26,220 --> 02:21:30,510
still you're not just the creator of AI

02:21:28,199 --> 02:21:32,850
systems you're always also subject to AI

02:21:30,510 --> 02:21:36,270
systems other AI subsystems that you are

02:21:32,850 --> 02:21:41,460
not creating so even if you are building

02:21:36,270 --> 02:21:43,529
them you are always being categorized by

02:21:41,460 --> 02:21:45,330
the systems that others are building so

02:21:43,529 --> 02:21:47,819
we have to we have to have some sort of

02:21:45,330 --> 02:21:51,390
solution to these problems and the

02:21:47,819 --> 02:21:54,029
solution has been coming in the form of

02:21:51,390 --> 02:21:56,930
so this is another timeline that sort of

02:21:54,029 --> 02:21:59,699
matches the last one that I showed

02:21:56,930 --> 02:22:01,949
basically three types of solutions have

02:21:59,699 --> 02:22:04,710
been proposed one is early stage

02:22:01,949 --> 02:22:07,680
regulations in AI ethics

02:22:04,710 --> 02:22:09,540
the other one is AI boards so many

02:22:07,680 --> 02:22:12,029
companies are now building AI epic

02:22:09,540 --> 02:22:13,710
sports and the other one is AI

02:22:12,029 --> 02:22:16,020
principles a whole bunch of AI

02:22:13,710 --> 02:22:18,840
principles so we have right now a little

02:22:16,020 --> 02:22:22,220
over 80 sets of AI principles coming

02:22:18,840 --> 02:22:24,569
from private companies institutions

02:22:22,220 --> 02:22:28,710
international organizations governmental

02:22:24,569 --> 02:22:33,119
agencies and these all of these efforts

02:22:28,710 --> 02:22:37,229
we can sort of categorize them into two

02:22:33,119 --> 02:22:39,899
two approaches so one is enforcing

02:22:37,229 --> 02:22:42,659
ethics how do we enforce ethics and the

02:22:39,899 --> 02:22:44,729
other one is integrating ethics to

02:22:42,659 --> 02:22:48,060
enforce ethics we need external

02:22:44,729 --> 02:22:50,069
regulations so there is a very important

02:22:48,060 --> 02:22:52,080
function of the regulations and laws

02:22:50,069 --> 02:22:54,351
which is to make sure that we at least

02:22:52,080 --> 02:22:56,240
have the

02:22:54,351 --> 02:22:59,690
the boundaries of what is ethically

02:22:56,240 --> 02:23:01,551
acceptable and not acceptable so this is

02:22:59,690 --> 02:23:03,610
also very important because if you want

02:23:01,551 --> 02:23:06,891
certain companies or certain developers

02:23:03,610 --> 02:23:08,330
to care about ethics you also want to

02:23:06,891 --> 02:23:10,460
make sure that you don't force them to

02:23:08,330 --> 02:23:12,410
compete with those who don't care you

02:23:10,460 --> 02:23:16,250
want to make sure at least there is some

02:23:12,410 --> 02:23:19,011
sort of fair playing field but the

02:23:16,250 --> 02:23:21,110
problem with the regulations is that

02:23:19,011 --> 02:23:22,971
they are they can never be very detailed

02:23:21,110 --> 02:23:24,860
so they are not going to answer your

02:23:22,971 --> 02:23:28,761
everyday ethics questions they are just

02:23:24,860 --> 02:23:31,580
gonna be general guidelines and when you

02:23:28,761 --> 02:23:33,710
think about if you take an example of

02:23:31,580 --> 02:23:35,540
how the ethical questions arise while

02:23:33,710 --> 02:23:37,011
you are developing an AI system you

02:23:35,540 --> 02:23:38,960
really realize that yes there is a

02:23:37,011 --> 02:23:41,330
there's a very important function of

02:23:38,960 --> 02:23:44,000
regulations but it's just not not enough

02:23:41,330 --> 02:23:47,271
so think about for example developing a

02:23:44,000 --> 02:23:51,170
personalized AI or personalized system

02:23:47,271 --> 02:23:54,380
for advice in healthy living so it's

02:23:51,170 --> 02:23:56,030
sort of like an AI health coach there

02:23:54,380 --> 02:23:58,280
will be as you are developing something

02:23:56,030 --> 02:23:59,780
like this and this is an example by the

02:23:58,280 --> 02:24:02,301
way that was brought to us so that we

02:23:59,780 --> 02:24:03,980
work on the ethical issues of this so it

02:24:02,301 --> 02:24:06,740
was it's not a hypothetical completely

02:24:03,980 --> 02:24:09,650
speaking so there will be many ethically

02:24:06,740 --> 02:24:11,420
loaded questions so what data are you

02:24:09,650 --> 02:24:14,330
gonna so think about it like a variable

02:24:11,420 --> 02:24:17,210
it's gonna basically understand your

02:24:14,330 --> 02:24:21,230
health and we'll give you the right

02:24:17,210 --> 02:24:22,730
health advice healthy living advice so

02:24:21,230 --> 02:24:23,870
there will be many questions that you

02:24:22,730 --> 02:24:25,340
have to answer as you're building

02:24:23,870 --> 02:24:27,200
there's many ethically loaded questions

02:24:25,340 --> 02:24:30,551
what data are you gonna collect are you

02:24:27,200 --> 02:24:34,301
gonna only track the user's physical

02:24:30,551 --> 02:24:37,040
data like like a Fitbit or are you gonna

02:24:34,301 --> 02:24:39,170
log into their Instagram are you gonna

02:24:37,040 --> 02:24:42,021
have access to their Instagram to check

02:24:39,170 --> 02:24:43,940
their mood how much do they party are

02:24:42,021 --> 02:24:45,290
you going to log into their Alexa to see

02:24:43,940 --> 02:24:47,240
whether they have a peaceful home

02:24:45,290 --> 02:24:49,160
environment or not they would be very

02:24:47,240 --> 02:24:51,891
useful information for healthy living

02:24:49,160 --> 02:24:54,290
for your advice but is it are these

02:24:51,891 --> 02:24:56,360
ethical options who do you share this

02:24:54,290 --> 02:24:58,761
data with whatever you are collecting

02:24:56,360 --> 02:25:00,681
and how do you design a consent form for

02:24:58,761 --> 02:25:02,811
the user to understand what they are

02:25:00,681 --> 02:25:04,700
sharing with whom they are sharing and

02:25:02,811 --> 02:25:07,480
what are they getting out of it what are

02:25:04,700 --> 02:25:07,480
they receiving in return

02:25:07,610 --> 02:25:12,170
oh and and and of course other connected

02:25:10,190 --> 02:25:14,300
questions to these right like are there

02:25:12,170 --> 02:25:18,650
safeguards for insurance companies not

02:25:14,300 --> 02:25:20,750
to later punish the user for not taking

02:25:18,650 --> 02:25:22,910
the advice from the AI health coach so

02:25:20,750 --> 02:25:24,710
there are also connected questions

02:25:22,910 --> 02:25:26,450
within the domain you're operating in if

02:25:24,710 --> 02:25:28,070
it is healthcare you will have all the

02:25:26,450 --> 02:25:31,070
health care related questions that's

02:25:28,070 --> 02:25:33,110
there so these are complex questions and

02:25:31,070 --> 02:25:35,600
these are not questions that just simple

02:25:33,110 --> 02:25:38,510
regulations were simple just regulations

02:25:35,600 --> 02:25:42,561
general regulations even if they are not

02:25:38,510 --> 02:25:45,440
simple can address and they have to they

02:25:42,561 --> 02:25:48,080
are internal questions so like nothing

02:25:45,440 --> 02:25:50,300
it shouldn't be answered by external

02:25:48,080 --> 02:25:54,110
bodies but they should be answered

02:25:50,300 --> 02:25:56,420
internally and we do seem to have going

02:25:54,110 --> 02:25:59,540
to that direction of this internal

02:25:56,420 --> 02:26:04,100
integration of ethics into the process

02:25:59,540 --> 02:26:06,710
for building AI systems by these AI

02:26:04,100 --> 02:26:09,200
principles that are proposed by the

02:26:06,710 --> 02:26:13,820
companies and the AI ethics ports within

02:26:09,200 --> 02:26:15,650
the companies so this is actually an

02:26:13,820 --> 02:26:17,720
oversight and compliance model the

02:26:15,650 --> 02:26:21,680
principles and the boards and we have

02:26:17,720 --> 02:26:23,330
this model for the last 40 years from

02:26:21,680 --> 02:26:25,010
research ethics so this is not a new

02:26:23,330 --> 02:26:27,710
model at all you have the research

02:26:25,010 --> 02:26:29,120
ethics boards those of you who are doing

02:26:27,710 --> 02:26:30,950
research in academia you are familiar

02:26:29,120 --> 02:26:31,550
with that institutional review boards

02:26:30,950 --> 02:26:33,170
irps

02:26:31,550 --> 02:26:36,730
or research ethics committees in the

02:26:33,170 --> 02:26:40,940
Europe recs and they use principles to

02:26:36,730 --> 02:26:42,470
evaluate research and they are still

02:26:40,940 --> 02:26:44,180
trying to apply these general principles

02:26:42,470 --> 02:26:47,150
there are just four principles actually

02:26:44,180 --> 02:26:50,660
that they are trying to apply and the

02:26:47,150 --> 02:26:53,240
the problem with this approach is that

02:26:50,660 --> 02:26:55,760
if you realize it's still not really

02:26:53,240 --> 02:26:57,890
integrating its within the company but

02:26:55,760 --> 02:26:59,600
it is still a board that is sitting

02:26:57,890 --> 02:27:02,420
distant from the developers or the

02:26:59,600 --> 02:27:04,940
project teams and that has this also add

02:27:02,420 --> 02:27:07,220
like a hierarchical level that approves

02:27:04,940 --> 02:27:11,110
your research or does not approve your

02:27:07,220 --> 02:27:14,470
research so it's not exactly

02:27:11,110 --> 02:27:19,280
integration we are talking about and

02:27:14,470 --> 02:27:21,440
also most of the time these boards

02:27:19,280 --> 02:27:24,710
either don't have the right expertise

02:27:21,440 --> 02:27:26,660
both for the science of the whatever you

02:27:24,710 --> 02:27:28,280
are doing and the ethics of whatever you

02:27:26,660 --> 02:27:29,510
are doing and they certainly don't have

02:27:28,280 --> 02:27:31,040
enough time even if they have the

02:27:29,510 --> 02:27:33,140
expertise because you imagine all these

02:27:31,040 --> 02:27:34,820
questions questions being posed by every

02:27:33,140 --> 02:27:40,640
single project team we are talking about

02:27:34,820 --> 02:27:41,990
a lot of questions um so we are in the

02:27:40,640 --> 02:27:44,061
in one sense we are still talking about

02:27:41,990 --> 02:27:46,100
enforcement of ethics with these boards

02:27:44,061 --> 02:27:47,750
and principals it's just a different

02:27:46,100 --> 02:27:49,400
level of enforcement it doesn't come

02:27:47,750 --> 02:27:51,410
from the regulatory level like the

02:27:49,400 --> 02:27:54,260
external level it's within the company

02:27:51,410 --> 02:27:56,540
but it's still enforcing ethics and it

02:27:54,260 --> 02:28:01,550
seems to this whole enforcing ethics

02:27:56,540 --> 02:28:03,650
while it's important seems to well it's

02:28:01,550 --> 02:28:06,521
important if it's crafted well let's put

02:28:03,650 --> 02:28:09,050
it that way seems to miss the main

02:28:06,521 --> 02:28:10,850
question which is how do you solve

02:28:09,050 --> 02:28:16,070
ethical problems as you are developing

02:28:10,850 --> 02:28:19,010
AI systems so in other words how should

02:28:16,070 --> 02:28:22,010
we really integrate ethics into building

02:28:19,010 --> 02:28:24,620
and developing AI systems not enforcing

02:28:22,010 --> 02:28:27,500
but integrating it so before I get to

02:28:24,620 --> 02:28:30,740
that though I think I should talk about

02:28:27,500 --> 02:28:33,440
why should we care about this because um

02:28:30,740 --> 02:28:36,860
so I'm a philosopher it's happy I said

02:28:33,440 --> 02:28:38,271
and we have a very large literature we

02:28:36,860 --> 02:28:40,070
are not fooling ourselves we have a very

02:28:38,271 --> 02:28:42,050
large literature about how fickle

02:28:40,070 --> 02:28:44,450
reasoning is great but it doesn't

02:28:42,050 --> 02:28:46,460
motivate people to do the right thing so

02:28:44,450 --> 02:28:47,990
I'm gonna try to give you a couple more

02:28:46,460 --> 02:28:48,980
motivations if you're already motivated

02:28:47,990 --> 02:28:52,640
because it's the right thing that's

02:28:48,980 --> 02:28:55,311
great but if you're not then you don't

02:28:52,640 --> 02:28:57,440
met as a company or as a developer as

02:28:55,311 --> 02:28:59,300
practitioners basically when you are not

02:28:57,440 --> 02:29:00,980
endorsing ethics basically you are

02:28:59,300 --> 02:29:04,061
taking a risk and it's quite a high

02:29:00,980 --> 02:29:06,680
stakes because what you're doing is that

02:29:04,061 --> 02:29:08,750
every time something goes wrong every

02:29:06,680 --> 02:29:10,430
time there's a scandal basically you are

02:29:08,750 --> 02:29:12,230
giving the impression that you are the

02:29:10,430 --> 02:29:14,811
wild ones that the society has to

02:29:12,230 --> 02:29:16,521
control which means heavy regulations

02:29:14,811 --> 02:29:18,620
are going to come to your way and which

02:29:16,521 --> 02:29:20,811
also means if you if there if the

02:29:18,620 --> 02:29:22,670
scandals keep coming they're going to

02:29:20,811 --> 02:29:24,110
come really fast and so these

02:29:22,670 --> 02:29:26,150
regulations will not even be good

02:29:24,110 --> 02:29:28,400
regulations which is terrible for

02:29:26,150 --> 02:29:29,670
everyone because it's bad for the

02:29:28,400 --> 02:29:31,680
businesses there won't be

02:29:29,670 --> 02:29:33,360
enough space for innovation it's bad for

02:29:31,680 --> 02:29:35,370
the society because innovation helps

02:29:33,360 --> 02:29:37,380
things as long as the innovation is

02:29:35,370 --> 02:29:40,290
responsible it's a good thing we want to

02:29:37,380 --> 02:29:42,210
have the innovation going on so this is

02:29:40,290 --> 02:29:45,260
at least one more reason why you should

02:29:42,210 --> 02:29:47,939
care so then how do we integrate ethics

02:29:45,260 --> 02:29:50,550
so if you want to solve ethical

02:29:47,939 --> 02:29:53,160
questions then we have to think of them

02:29:50,550 --> 02:29:54,689
not just like policing ethics but we

02:29:53,160 --> 02:29:58,020
think we need to think of them as like

02:29:54,689 --> 02:29:59,580
puzzle solving of course ethical

02:29:58,020 --> 02:30:01,890
questions are not this simple so they

02:29:59,580 --> 02:30:04,470
are like this and there's like very

02:30:01,890 --> 02:30:06,420
complex moving parts at all times that

02:30:04,470 --> 02:30:07,590
you have to deal with about you know

02:30:06,420 --> 02:30:09,870
individual well-being individual

02:30:07,590 --> 02:30:12,960
autonomy societal well-being social

02:30:09,870 --> 02:30:15,510
justice harm but you have to consider

02:30:12,960 --> 02:30:17,340
them and you have to consider them at

02:30:15,510 --> 02:30:19,860
each stage as I said you know from the

02:30:17,340 --> 02:30:22,770
beginning research development design

02:30:19,860 --> 02:30:25,979
deployment updating all of these have

02:30:22,770 --> 02:30:31,140
unique ethical questions so what's the

02:30:25,979 --> 02:30:34,080
model um one thing that we we need to do

02:30:31,140 --> 02:30:36,000
is to catch these ethical problems from

02:30:34,080 --> 02:30:38,160
the very beginning as soon as we can

02:30:36,000 --> 02:30:41,280
before they become bigger problems and

02:30:38,160 --> 02:30:43,350
if possible just eliminate them and for

02:30:41,280 --> 02:30:45,570
that we need the people who are actually

02:30:43,350 --> 02:30:47,580
building AI systems the developers and

02:30:45,570 --> 02:30:49,890
the researchers to be aware of the

02:30:47,580 --> 02:30:52,830
relevant ethical issues and to have some

02:30:49,890 --> 02:30:54,900
sort of tool to deal with these ethical

02:30:52,830 --> 02:30:57,710
issues so if these questions are not

02:30:54,900 --> 02:31:00,240
very complex then we can do that by

02:30:57,710 --> 02:31:02,490
training the developers and the

02:31:00,240 --> 02:31:06,330
researchers basically add another tool

02:31:02,490 --> 02:31:09,000
for their toolbox to deal with these

02:31:06,330 --> 02:31:11,280
ethical issues by deal with them I mean

02:31:09,000 --> 02:31:14,130
either if it is as I said simple solve

02:31:11,280 --> 02:31:17,150
them if not flag them just flag them and

02:31:14,130 --> 02:31:20,040
demand solutions for them the second

02:31:17,150 --> 02:31:24,600
aspect is when the question is really

02:31:20,040 --> 02:31:28,550
complex obviously we don't want we don't

02:31:24,600 --> 02:31:34,110
want to spend all the energy of people

02:31:28,550 --> 02:31:36,830
whose task is to build these tools to

02:31:34,110 --> 02:31:39,330
dive into the deep flow so fickle

02:31:36,830 --> 02:31:40,770
literature but there is a deep

02:31:39,330 --> 02:31:42,601
philosophical literature and applied

02:31:40,770 --> 02:31:45,150
ethics has been around for very

02:31:42,601 --> 02:31:50,061
time so we have a lot of knowledge

02:31:45,150 --> 02:31:52,230
around applied ethics and using ethical

02:31:50,061 --> 02:31:54,511
problem-solving in practice we've been

02:31:52,230 --> 02:31:56,910
using it in medicine in public health in

02:31:54,511 --> 02:31:58,501
environmental questions in businesses

02:31:56,910 --> 02:31:59,730
we've been using applied ethics so it's

02:31:58,501 --> 02:32:02,251
not like we are not reinventing the

02:31:59,730 --> 02:32:04,440
wheel here so what you can do is to

02:32:02,251 --> 02:32:06,301
analyze the project if it is a complex

02:32:04,440 --> 02:32:08,791
project then you analyze the the depth

02:32:06,301 --> 02:32:10,921
domain if it is in healthcare again

02:32:08,791 --> 02:32:12,660
think about the AI health coaches that I

02:32:10,921 --> 02:32:14,551
just mentioned well you have to

02:32:12,660 --> 02:32:16,950
understand the ethical questions around

02:32:14,551 --> 02:32:18,360
healthcare system and you have to

02:32:16,950 --> 02:32:21,391
understand the questions about around

02:32:18,360 --> 02:32:23,311
variables the technology itself and you

02:32:21,391 --> 02:32:27,240
need to figure out a set of right

02:32:23,311 --> 02:32:29,551
answers yes a lot of the times the there

02:32:27,240 --> 02:32:31,650
is no single right answer then the

02:32:29,551 --> 02:32:33,660
question is complex enough we won't have

02:32:31,650 --> 02:32:35,700
a single right answer but we can

02:32:33,660 --> 02:32:37,831
eliminate it to a couple of right

02:32:35,700 --> 02:32:40,771
answers and that if you could just

02:32:37,831 --> 02:32:42,931
choose from that set of right answers

02:32:40,771 --> 02:32:46,620
you're really really good we are so much

02:32:42,931 --> 02:32:49,230
better off and finally certain complex

02:32:46,620 --> 02:32:51,811
questions will keep appearing over and

02:32:49,230 --> 02:32:53,341
over again and for that we don't want to

02:32:51,811 --> 02:32:55,711
rediscover the whole thing again we

02:32:53,341 --> 02:32:57,181
don't want to do analysis again from the

02:32:55,711 --> 02:32:59,581
very beginning what we want to do is

02:32:57,181 --> 02:33:02,820
slowly set a strategy for the company or

02:32:59,581 --> 02:33:04,891
the institution so basically figure out

02:33:02,820 --> 02:33:06,900
you know those ad sets of principles

02:33:04,891 --> 02:33:09,091
they'll figure out what they really want

02:33:06,900 --> 02:33:11,910
them to say but what do these companies

02:33:09,091 --> 02:33:13,291
want to say with those principles a lot

02:33:11,910 --> 02:33:17,160
of the times principals will say things

02:33:13,291 --> 02:33:19,950
that are very charming like be socially

02:33:17,160 --> 02:33:22,501
beneficial and protect individual

02:33:19,950 --> 02:33:24,780
autonomy and privacy and you will

02:33:22,501 --> 02:33:26,400
realize that well they clash a lot of

02:33:24,780 --> 02:33:29,280
the times that's what a complex ethical

02:33:26,400 --> 02:33:31,171
questions they clash so how are you

02:33:29,280 --> 02:33:34,011
gonna deal with them as a company what

02:33:31,171 --> 02:33:38,131
is your agenda what is your strategy

02:33:34,011 --> 02:33:40,200
ethical strategy and so operationalizing

02:33:38,131 --> 02:33:43,200
these principles putting processes

02:33:40,200 --> 02:33:44,730
around them and setting precedents what

02:33:43,200 --> 02:33:46,711
did you decide last time you had a

02:33:44,730 --> 02:33:49,021
similar question and how are you going

02:33:46,711 --> 02:33:53,160
to use that knowledge now with this next

02:33:49,021 --> 02:33:54,160
question on the table so these three

02:33:53,160 --> 02:33:56,900
steps

02:33:54,160 --> 02:33:59,240
how are we where are we actually doing

02:33:56,900 --> 02:34:01,370
this where can we do this these well

02:33:59,240 --> 02:34:04,540
obviously in the companies so any

02:34:01,370 --> 02:34:10,280
company that has some resources to

02:34:04,540 --> 02:34:13,820
devote to ethical analysis can look into

02:34:10,280 --> 02:34:16,820
these integrating these into their a

02:34:13,820 --> 02:34:20,150
build into the processes in building AI

02:34:16,820 --> 02:34:24,620
systems the other one is incubators and

02:34:20,150 --> 02:34:26,301
accelerators startups are usually quite

02:34:24,620 --> 02:34:28,131
concerned about the ethical aspects of

02:34:26,301 --> 02:34:29,450
the products that they are building but

02:34:28,131 --> 02:34:32,120
they don't have the resources and that's

02:34:29,450 --> 02:34:33,410
very understandable so the incubators

02:34:32,120 --> 02:34:34,881
that they are located in and the

02:34:33,410 --> 02:34:36,830
accelerators that they are located in

02:34:34,881 --> 02:34:39,561
all the right places where these

02:34:36,830 --> 02:34:43,551
services these information can be

02:34:39,561 --> 02:34:46,101
offered them and finally the research

02:34:43,551 --> 02:34:50,650
centers so academic institutions are as

02:34:46,101 --> 02:34:55,030
lost as the private companies so you

02:34:50,650 --> 02:34:57,801
still have need these steps to be

02:34:55,030 --> 02:34:59,751
integrated into the research and

02:34:57,801 --> 02:35:01,671
development process within the academic

02:34:59,751 --> 02:35:04,751
centers within the research institutions

02:35:01,671 --> 02:35:07,490
and the demand for this should come from

02:35:04,751 --> 02:35:11,330
companies and startups who understand

02:35:07,490 --> 02:35:14,751
that well hopefully who understand that

02:35:11,330 --> 02:35:18,440
ethics matters but if not that business

02:35:14,751 --> 02:35:21,351
risk matters developers who care about

02:35:18,440 --> 02:35:22,940
really just developing good technology

02:35:21,351 --> 02:35:25,311
because when your technology is making

02:35:22,940 --> 02:35:28,131
huge ethical mistakes check usually they

02:35:25,311 --> 02:35:31,551
are also wrong if you are trying to have

02:35:28,131 --> 02:35:34,641
the best algorithm for hiring decisions

02:35:31,551 --> 02:35:37,011
and you because of biases you discard

02:35:34,641 --> 02:35:38,450
whole bunch of women you your algorithm

02:35:37,011 --> 02:35:41,030
is actually not getting the right people

02:35:38,450 --> 02:35:43,280
so it is an enhancement of the

02:35:41,030 --> 02:35:47,391
technology if you get the ethics right

02:35:43,280 --> 02:35:51,110
and as developers I my senses we care

02:35:47,391 --> 02:35:54,021
about this and investors who are

02:35:51,110 --> 02:35:55,820
investing into these companies because

02:35:54,021 --> 02:35:59,150
they are also taking a risk business

02:35:55,820 --> 02:36:03,980
risk so basically what we need to be

02:35:59,150 --> 02:36:07,641
doing is moving from this this model of

02:36:03,980 --> 02:36:11,240
ethics as policing which puts off

02:36:07,641 --> 02:36:12,681
of the practitioners as well as trust me

02:36:11,240 --> 02:36:15,980
a lot of the philosophers

02:36:12,681 --> 02:36:18,620
but really go into puzzle solving and to

02:36:15,980 --> 02:36:21,351
do that using the knowledge that comes

02:36:18,620 --> 02:36:23,450
from ethics together with the knowledge

02:36:21,351 --> 02:36:33,830
that comes from AI and hence the AI

02:36:23,450 --> 02:36:35,210
ethics that's all thank you all right

02:36:33,830 --> 02:36:36,740
now we'll take three questions from the

02:36:35,210 --> 02:36:40,000
audience if there's anyone who wants to

02:36:36,740 --> 02:36:46,240
ask a question John sue now is your time

02:36:40,000 --> 02:36:46,240
we have one over here sorry one second

02:36:54,130 --> 02:37:00,351
hi I'm so you talked about startups and

02:36:58,460 --> 02:37:02,150
how they can you know the incubators are

02:37:00,351 --> 02:37:03,530
a great place to for them to access

02:37:02,150 --> 02:37:07,250
information about ethics and how to

02:37:03,530 --> 02:37:08,841
integrate it and I mean yeah I

02:37:07,250 --> 02:37:10,610
completely I've worked with quite a lot

02:37:08,841 --> 02:37:12,170
of startup founders who want to you know

02:37:10,610 --> 02:37:14,420
kind of I've seen they've spotted a gap

02:37:12,170 --> 02:37:15,980
or they've seen what some some thing

02:37:14,420 --> 02:37:17,750
that they can that can be fixed and

02:37:15,980 --> 02:37:20,360
often there's an ethical reason for that

02:37:17,750 --> 02:37:23,210
but what happens when investment starts

02:37:20,360 --> 02:37:25,490
coming in so I think in you know kind of

02:37:23,210 --> 02:37:27,440
in my experience it often becomes more

02:37:25,490 --> 02:37:28,730
difficult for start and startup founders

02:37:27,440 --> 02:37:31,190
to kind of keep hold of that original

02:37:28,730 --> 02:37:34,190
kind of those original values once

02:37:31,190 --> 02:37:36,320
investment comes in yeah I mean

02:37:34,190 --> 02:37:38,750
basically you're right and what you're

02:37:36,320 --> 02:37:41,150
pointing to is that as soon as there are

02:37:38,750 --> 02:37:42,980
parties involved that don't care about

02:37:41,150 --> 02:37:45,980
the ethics of whatever you're doing

02:37:42,980 --> 02:37:49,040
there is tension and then you have to

02:37:45,980 --> 02:37:50,750
make the decision well I really want to

02:37:49,040 --> 02:37:54,471
get this stored up going should I just

02:37:50,750 --> 02:37:56,601
just this once not do this the good

02:37:54,471 --> 02:37:58,250
thing is that the investors care more

02:37:56,601 --> 02:38:00,591
and more in my experience I mean you

02:37:58,250 --> 02:38:02,301
know this was again and absolutely not

02:38:00,591 --> 02:38:03,710
talked about topic just two years ago

02:38:02,301 --> 02:38:06,021
when you talk to investors and now they

02:38:03,710 --> 02:38:07,880
are they do care a lot of the times

02:38:06,021 --> 02:38:10,820
especially when investing into the

02:38:07,880 --> 02:38:12,500
startups because they don't yet have the

02:38:10,820 --> 02:38:16,820
commitment right like they can now think

02:38:12,500 --> 02:38:19,460
about it and hopefully getting them care

02:38:16,820 --> 02:38:21,590
is how you can and if it is not I mean

02:38:19,460 --> 02:38:24,290
you're really you are really looking

02:38:21,590 --> 02:38:28,609
get the ethics work versus the market

02:38:24,290 --> 02:38:31,340
pull which is the problem but it is also

02:38:28,609 --> 02:38:33,200
as the culture is slowly changing it is

02:38:31,340 --> 02:38:37,160
also a problem that there is hope that

02:38:33,200 --> 02:38:40,580
it can align somehow I have a question

02:38:37,160 --> 02:38:41,859
back there am i coming around right now

02:38:40,580 --> 02:38:44,689
[Applause]

02:38:41,859 --> 02:38:47,420
hi my name is funny thank you very much

02:38:44,689 --> 02:38:50,479
for the presentation I would like to ask

02:38:47,420 --> 02:38:52,580
you to give me one example to apply all

02:38:50,479 --> 02:38:54,740
that model and and your presentation to

02:38:52,580 --> 02:38:56,600
an ethical problem and if you could tell

02:38:54,740 --> 02:38:59,510
me a little bit about how to distinguish

02:38:56,600 --> 02:39:01,700
your solution with ethics as opposed to

02:38:59,510 --> 02:39:04,550
a legal solution to that problem thank

02:39:01,700 --> 02:39:06,320
you sure so I can give you the example

02:39:04,550 --> 02:39:06,950
that I just gave in the so to keep it

02:39:06,320 --> 02:39:10,280
short

02:39:06,950 --> 02:39:14,590
the that I explained that I gave in the

02:39:10,280 --> 02:39:18,040
in the talk building AI health coaches

02:39:14,590 --> 02:39:20,990
the questions are so legally speaking

02:39:18,040 --> 02:39:27,170
consent form for a something like that

02:39:20,990 --> 02:39:28,880
is very non functional dysfunctional you

02:39:27,170 --> 02:39:30,170
basically have terms of services you

02:39:28,880 --> 02:39:31,820
scroll through you accept no one

02:39:30,170 --> 02:39:35,210
everyone knows you're not reading it and

02:39:31,820 --> 02:39:37,250
it's not a big legal issue we there is a

02:39:35,210 --> 02:39:39,590
debate but there is no enforcement of it

02:39:37,250 --> 02:39:41,930
it is an ethical issue and we are all

02:39:39,590 --> 02:39:45,200
aware of that so one way that it

02:39:41,930 --> 02:39:47,479
diverges is like ethically speaking

02:39:45,200 --> 02:39:49,100
terms and conditions' mean absolutely

02:39:47,479 --> 02:39:51,560
nothing if we know that you're not

02:39:49,100 --> 02:39:54,590
reading it because what it should mean

02:39:51,560 --> 02:39:56,390
is informed consent you know there is a

02:39:54,590 --> 02:39:58,100
reason why we call it informed consent

02:39:56,390 --> 02:39:59,660
and there is no information provided the

02:39:58,100 --> 02:40:02,600
information is not provided in the way

02:39:59,660 --> 02:40:04,520
that a person will have the time to read

02:40:02,600 --> 02:40:06,439
it will have the energy to read and it's

02:40:04,520 --> 02:40:08,780
updated all the time so everything

02:40:06,439 --> 02:40:10,910
around that for example is a major

02:40:08,780 --> 02:40:12,979
ethical problem it's not yet a legal

02:40:10,910 --> 02:40:17,090
problem the good thing about ethics and

02:40:12,979 --> 02:40:18,410
legal too often aligning is that when

02:40:17,090 --> 02:40:21,800
the conversation starts somewhere

02:40:18,410 --> 02:40:25,910
usually in ethics that informs the rules

02:40:21,800 --> 02:40:28,220
regulations luckily so that they start

02:40:25,910 --> 02:40:30,710
reinforced there is a good feedback loop

02:40:28,220 --> 02:40:33,290
between the two but if something that

02:40:30,710 --> 02:40:35,221
you kept saying as a again going back to

02:40:33,290 --> 02:40:37,681
another field

02:40:35,221 --> 02:40:40,681
just medical healthcare something that

02:40:37,681 --> 02:40:42,990
was a problem the informed consent just

02:40:40,681 --> 02:40:45,751
ethically speaking became a legal rule

02:40:42,990 --> 02:40:48,480
you have to have this implemented and

02:40:45,751 --> 02:40:51,570
implemented properly at this point so I

02:40:48,480 --> 02:40:54,391
would say this is one really simple

02:40:51,570 --> 02:40:56,931
example where it changes and how it

02:40:54,391 --> 02:40:59,251
would happen well you know the the whole

02:40:56,931 --> 02:41:01,681
consent structure around this particular

02:40:59,251 --> 02:41:04,711
product has to be designed while you are

02:41:01,681 --> 02:41:08,041
designing the before you basically pull

02:41:04,711 --> 02:41:09,480
it out push it out it's not for one more

02:41:08,041 --> 02:41:12,331
question in case there was somebody who

02:41:09,480 --> 02:41:16,280
wants to we have one question right here

02:41:12,331 --> 02:41:16,280
this is Mike right behind you

02:41:16,940 --> 02:41:23,701
hi thank you so much for your talk I

02:41:20,161 --> 02:41:27,121
think the integration point is a really

02:41:23,701 --> 02:41:30,860
really powerful one and I'm curious if

02:41:27,121 --> 02:41:33,440
you've seen examples of this especially

02:41:30,860 --> 02:41:36,501
and ones that are really functional

02:41:33,440 --> 02:41:38,701
because from what I've observed

02:41:36,501 --> 02:41:40,141
engineers by training don't have

02:41:38,701 --> 02:41:42,690
necessarily all of the domain knowledge

02:41:40,141 --> 02:41:44,070
necessary to do that puzzle solving oh

02:41:42,690 --> 02:41:45,511
yeah absolutely

02:41:44,070 --> 02:41:47,521
well and so how do you bring the other

02:41:45,511 --> 02:41:49,771
relevant experts to the table and have

02:41:47,521 --> 02:41:51,570
you seen that working and how did that

02:41:49,771 --> 02:41:53,280
come to be because it doesn't seem to me

02:41:51,570 --> 02:41:54,900
to be the way that most tech companies

02:41:53,280 --> 02:41:58,681
are structured today you're absolutely

02:41:54,900 --> 02:42:00,751
right basically everything right now is

02:41:58,681 --> 02:42:03,421
still in the experimental phase so we

02:42:00,751 --> 02:42:06,061
went from like zero concern to real

02:42:03,421 --> 02:42:08,461
concern and now there is experiment like

02:42:06,061 --> 02:42:11,131
all the companies are experimenting with

02:42:08,461 --> 02:42:12,601
different models of integration and as I

02:42:11,131 --> 02:42:15,961
said over that compliance and oversight

02:42:12,601 --> 02:42:18,841
model is sort of taking hold of the

02:42:15,961 --> 02:42:22,081
situation but companies are also doing

02:42:18,841 --> 02:42:25,681
seminars and trainings they are having

02:42:22,081 --> 02:42:27,601
boards that are more functional the way

02:42:25,681 --> 02:42:29,551
by the way I think yes very important

02:42:27,601 --> 02:42:31,831
point you make is that no we I don't

02:42:29,551 --> 02:42:33,811
expect engineers to now ditch

02:42:31,831 --> 02:42:35,521
engineering going to philosophy solved

02:42:33,811 --> 02:42:37,860
all the ethical problems because I mean

02:42:35,521 --> 02:42:39,541
there's a division of labor right so

02:42:37,860 --> 02:42:42,150
philosophers are the ones who deal with

02:42:39,541 --> 02:42:45,271
the ethical questions and if you need

02:42:42,150 --> 02:42:46,650
like a field information well sociology

02:42:45,271 --> 02:42:48,960
people should be the ones that you

02:42:46,650 --> 02:42:52,650
should go for so we have

02:42:48,960 --> 02:42:54,720
the right training already and we have

02:42:52,650 --> 02:42:56,460
these trainings come to make together in

02:42:54,720 --> 02:43:00,420
different fields where we decided to

02:42:56,460 --> 02:43:02,130
care about Essex already so again I talk

02:43:00,420 --> 02:43:04,229
about the healthcare example a lot

02:43:02,130 --> 02:43:07,040
because healthcare is an area where we

02:43:04,229 --> 02:43:09,689
managed to get ethics really integrated

02:43:07,040 --> 02:43:12,240
not perfect but the best that we have

02:43:09,689 --> 02:43:15,120
and there you do have philosophers

02:43:12,240 --> 02:43:17,340
collaborate with public health people to

02:43:15,120 --> 02:43:18,990
decide you know how to set the

02:43:17,340 --> 02:43:20,670
guidelines how to set their agendas and

02:43:18,990 --> 02:43:21,300
in particular cases you know in a

02:43:20,670 --> 02:43:22,800
hospital

02:43:21,300 --> 02:43:25,140
what are you going to do about this

02:43:22,800 --> 02:43:27,290
particular case you don't tell the

02:43:25,140 --> 02:43:29,640
surgeon okay now take three days off

02:43:27,290 --> 02:43:31,920
figure everything out yourself no

02:43:29,640 --> 02:43:34,800
because that would be a waste of the

02:43:31,920 --> 02:43:36,479
surgeons time in this particular

02:43:34,800 --> 02:43:39,210
question whereas you have people who are

02:43:36,479 --> 02:43:41,490
actually skilled in this nice alright

02:43:39,210 --> 02:43:47,189
well that's it thank you so much a great

02:43:41,490 --> 02:43:50,010
talk thank you look for all right next

02:43:47,189 --> 02:43:53,280
up is the future of digital private put

02:43:50,010 --> 02:43:59,370
digital policy in Europe panel I'll be

02:43:53,280 --> 02:44:01,610
joined by three panelists soon the

02:43:59,370 --> 02:44:04,439
panelists here well here they are

02:44:01,610 --> 02:44:06,140
arguably hand for a casa Jana gear mo

02:44:04,439 --> 02:44:10,060
and prop hot

02:44:06,140 --> 02:44:10,060
[Applause]

02:44:10,430 --> 02:44:18,380
there they are do you guys see help okay

02:44:19,640 --> 02:44:22,640
Hey

02:44:25,670 --> 02:44:31,109
alright so a little bit of introductions

02:44:28,619 --> 02:44:33,569
I'll start ladies first Kaka Zana Shem

02:44:31,109 --> 02:44:35,970
yella which co-founder and president of

02:44:33,569 --> 02:44:37,470
the panopticon foundation lawyer and

02:44:35,970 --> 02:44:39,300
activist specializing in human rights

02:44:37,470 --> 02:44:40,619
and technology and you might have

02:44:39,300 --> 02:44:43,290
recognized her from the top she was part

02:44:40,619 --> 02:44:44,689
of yesterday and kasha for short right

02:44:43,290 --> 02:44:48,420
okay that's cool

02:44:44,689 --> 02:44:49,829
Rob hot Aggarwal prophet's been with the

02:44:48,420 --> 02:44:52,770
european commission for the last twelve

02:44:49,829 --> 02:44:55,260
years now doing online platforms and

02:44:52,770 --> 02:44:57,329
e-commerce he's a policy maker the EU

02:44:55,260 --> 02:44:59,760
Commission dealing with regulating

02:44:57,329 --> 02:45:02,699
platforms and you have a tech background

02:44:59,760 --> 02:45:07,319
he studies physics computer science etc

02:45:02,699 --> 02:45:09,630
etc it's like a long resume and then we

02:45:07,319 --> 02:45:11,600
have Guillermo Beltran Guillermo is a

02:45:09,630 --> 02:45:13,979
global policy director at access now

02:45:11,600 --> 02:45:16,829
global NGO working to defend human

02:45:13,979 --> 02:45:18,930
rights for users at risk so thank you

02:45:16,829 --> 02:45:21,840
for joining me today I want to talk

02:45:18,930 --> 02:45:25,020
about digital policy in Europe but just

02:45:21,840 --> 02:45:27,029
the kind of level set and start GDP are

02:45:25,020 --> 02:45:29,220
went into effect about a year and a half

02:45:27,029 --> 02:45:31,680
ago what do you think we've learned

02:45:29,220 --> 02:45:35,630
since which parts are working in which

02:45:31,680 --> 02:45:41,460
parts are not working anyone can start

02:45:35,630 --> 02:45:43,350
yeah you wanted to so I think like I

02:45:41,460 --> 02:45:45,390
don't want to go into the details of

02:45:43,350 --> 02:45:47,399
kind of which part because there's a

02:45:45,390 --> 02:45:48,869
huge piece of law and I didn't let me

02:45:47,399 --> 02:45:50,760
just zoom out on kind of a couple of

02:45:48,869 --> 02:45:52,710
things what what work and what don't

02:45:50,760 --> 02:45:54,720
work you know of course I get annoyed

02:45:52,710 --> 02:45:57,659
like many people clicking away my cookie

02:45:54,720 --> 02:45:59,040
consent forms but I think that that kind

02:45:57,659 --> 02:46:02,040
of distracts from the real issue I think

02:45:59,040 --> 02:46:04,649
that the what's really impressive is

02:46:02,040 --> 02:46:08,340
that I think gdpr has really been a game

02:46:04,649 --> 02:46:10,710
changer in kind of how we see tech

02:46:08,340 --> 02:46:12,600
policy and tech regulation you know

02:46:10,710 --> 02:46:14,699
taking an issue which is a fundamental

02:46:12,600 --> 02:46:18,029
rights issue you know and taking putting

02:46:14,699 --> 02:46:20,579
a frame around it and what I think has

02:46:18,029 --> 02:46:22,529
been really surprising is that I've been

02:46:20,579 --> 02:46:23,310
kind of part of this from the beginning

02:46:22,529 --> 02:46:25,470
but from us

02:46:23,310 --> 02:46:27,270
i distance because this is a data

02:46:25,470 --> 02:46:29,520
protection issue dealt with by another

02:46:27,270 --> 02:46:30,869
team but the way that we've turned had

02:46:29,520 --> 02:46:32,760
at 80 degrees from something that was

02:46:30,869 --> 02:46:34,680
completely crazy completely impossible

02:46:32,760 --> 02:46:37,199
it would never work we destroy the

02:46:34,680 --> 02:46:39,359
Internet to something where a lot of

02:46:37,199 --> 02:46:42,000
people say wow you know this is like a

02:46:39,359 --> 02:46:44,159
really powerful framework for protecting

02:46:42,000 --> 02:46:46,050
user right and and you know lots of

02:46:44,159 --> 02:46:48,659
people are saying we want to we want to

02:46:46,050 --> 02:46:50,850
now copy this at least in part you know

02:46:48,659 --> 02:46:53,760
around the world and and I think that's

02:46:50,850 --> 02:46:56,279
been really a learning experience where

02:46:53,760 --> 02:46:59,670
that has informed my some of my thinking

02:46:56,279 --> 02:47:01,319
okay so you know some of the some of the

02:46:59,670 --> 02:47:03,449
things that are still kind of settling

02:47:01,319 --> 02:47:05,100
in right now you know because it's it's

02:47:03,449 --> 02:47:07,199
relatively recent so it's a lot of

02:47:05,100 --> 02:47:08,729
emphasis now is shifting away from the

02:47:07,199 --> 02:47:10,590
actual meaning of the rules and how they

02:47:08,729 --> 02:47:13,109
apply not to enforcing those you know

02:47:10,590 --> 02:47:14,970
like so we we see a cases being geared

02:47:13,109 --> 02:47:16,800
up by data protection authorities in in

02:47:14,970 --> 02:47:19,500
Member States we see the global

02:47:16,800 --> 02:47:21,899
conversation around data protection and

02:47:19,500 --> 02:47:24,449
privacy changing so those are really the

02:47:21,899 --> 02:47:26,579
the game-changing positive elements I

02:47:24,449 --> 02:47:28,619
think then some of the negative elements

02:47:26,579 --> 02:47:30,600
that I think people are annoyed about is

02:47:28,619 --> 02:47:31,829
the kind of banner clicking but let's

02:47:30,600 --> 02:47:34,020
not forget that there's also you

02:47:31,829 --> 02:47:35,609
previously regulation that was still

02:47:34,020 --> 02:47:37,739
being negotiated and I'm still hoping

02:47:35,609 --> 02:47:39,540
that it'll be adopted soon that at least

02:47:37,739 --> 02:47:42,329
in parts of some of the issues you know

02:47:39,540 --> 02:47:44,460
but more by in general people are much

02:47:42,329 --> 02:47:46,260
more aware of privacy issues around the

02:47:44,460 --> 02:47:48,300
internet than just even a couple of

02:47:46,260 --> 02:47:50,010
years ago people are taking much more

02:47:48,300 --> 02:47:52,050
seriously it's conversation in schools

02:47:50,010 --> 02:47:53,520
it's conversation not only in some

02:47:52,050 --> 02:47:55,500
offices in Brussels but it's a

02:47:53,520 --> 02:47:57,689
conversations worldwide I think that's

02:47:55,500 --> 02:47:59,939
really my take away from what has

02:47:57,689 --> 02:48:02,279
happening right yeah you know I think I

02:47:59,939 --> 02:48:04,140
agree politically it's a success in the

02:48:02,279 --> 02:48:05,460
sense that it has sparked this global

02:48:04,140 --> 02:48:07,109
conversation about the importance of

02:48:05,460 --> 02:48:09,119
privacy in data protection we've seen a

02:48:07,109 --> 02:48:10,890
10% increase in countries that have

02:48:09,119 --> 02:48:12,770
actually adopted new data protection

02:48:10,890 --> 02:48:15,479
frameworks that are completed and

02:48:12,770 --> 02:48:17,880
there's like 30 other countries around

02:48:15,479 --> 02:48:21,720
the world that are now working on their

02:48:17,880 --> 02:48:24,090
new data protection frameworks so I was

02:48:21,720 --> 02:48:28,800
thinking so I agree the main objective

02:48:24,090 --> 02:48:30,270
of the gdpr is to put some legal

02:48:28,800 --> 02:48:32,430
structure on the protection of a

02:48:30,270 --> 02:48:33,540
fundamental right in Europe and it's

02:48:32,430 --> 02:48:35,670
important to remember that it's

02:48:33,540 --> 02:48:37,051
everything that's in GDP our is not new

02:48:35,670 --> 02:48:38,881
actually not

02:48:37,051 --> 02:48:41,341
many things are new it's just bringing

02:48:38,881 --> 02:48:43,021
an old beta protection law into kind of

02:48:41,341 --> 02:48:44,400
more the modern area and putting some

02:48:43,021 --> 02:48:45,360
obligations and some enforcement

02:48:44,400 --> 02:48:47,820
measures on top of it

02:48:45,360 --> 02:48:50,641
but I was thinking earlier in the panel

02:48:47,820 --> 02:48:52,711
you guys had on governing AI about how

02:48:50,641 --> 02:48:54,391
do you regulate the market like how do

02:48:52,711 --> 02:48:56,101
you tell the market where a market in

02:48:54,391 --> 02:48:58,621
that case it was AI but here is like

02:48:56,101 --> 02:49:00,451
more the digital space where do you want

02:48:58,621 --> 02:49:02,431
it to go and that I think is that the GD

02:49:00,451 --> 02:49:03,961
P R is a good public policy tool because

02:49:02,431 --> 02:49:05,521
in addition to protecting a fundamental

02:49:03,961 --> 02:49:06,811
right you're telling the market that's

02:49:05,521 --> 02:49:11,161
the direction I want you guys to

02:49:06,811 --> 02:49:13,141
innovate towards and to sink towards and

02:49:11,161 --> 02:49:15,421
in that sense that is it is being a

02:49:13,141 --> 02:49:17,671
success all that said like private jet

02:49:15,421 --> 02:49:19,201
said it's way too soon it was it only

02:49:17,671 --> 02:49:20,940
entered into force about a year and a

02:49:19,201 --> 02:49:22,891
half ago they're still Slovenian Greece

02:49:20,940 --> 02:49:26,070
have not developed their own

02:49:22,891 --> 02:49:27,991
implementing laws nationally data

02:49:26,070 --> 02:49:30,511
protection authorities don't have our

02:49:27,991 --> 02:49:32,581
way far from having sufficient resources

02:49:30,511 --> 02:49:35,190
to go police everything that they have

02:49:32,581 --> 02:49:36,541
to go police so we're happy where things

02:49:35,190 --> 02:49:39,601
are going but there's a lot of work to

02:49:36,541 --> 02:49:40,771
be done still yeah I fully agree it's a

02:49:39,601 --> 02:49:42,541
bit too early I think we need at least

02:49:40,771 --> 02:49:45,511
three more years to see the enforcement

02:49:42,541 --> 02:49:47,641
measures at full speed what was the

02:49:45,511 --> 02:49:51,421
biggest victory in this beer is that the

02:49:47,641 --> 02:49:53,461
old rules existing for two decades a got

02:49:51,421 --> 02:49:55,980
extended globally because they finally

02:49:53,461 --> 02:49:58,621
are used successfully against troubled

02:49:55,980 --> 02:50:00,061
companies who target European citizens

02:49:58,621 --> 02:50:03,241
with their services and that was in the

02:50:00,061 --> 02:50:05,161
case before and B we have money is in

02:50:03,241 --> 02:50:06,781
place financial huge financial fight

02:50:05,161 --> 02:50:08,851
fines are in place and that's the

02:50:06,781 --> 02:50:11,371
conversation starter or dough door

02:50:08,851 --> 02:50:13,291
opener for the EU to start negotiating

02:50:11,371 --> 02:50:15,121
the standards really but where it gets

02:50:13,291 --> 02:50:17,701
difficult is this moment where we

02:50:15,121 --> 02:50:20,311
realize the gdp r is proposing very

02:50:17,701 --> 02:50:22,411
sound rules for old business models that

02:50:20,311 --> 02:50:25,051
are in fundamental conflict with these

02:50:22,411 --> 02:50:29,461
rules so if you think about cookies or

02:50:25,051 --> 02:50:31,980
or pop-ups i mean no no we cannot

02:50:29,461 --> 02:50:35,101
imagine the amount of popups necessary

02:50:31,980 --> 02:50:36,301
to to to fulfill the they need to inform

02:50:35,101 --> 02:50:38,311
people about what's going on with your

02:50:36,301 --> 02:50:40,860
data behind the screen that's not the

02:50:38,311 --> 02:50:42,991
solution we have to remove the dentists

02:50:40,860 --> 02:50:44,791
we have to end the data transfers

02:50:42,991 --> 02:50:48,091
happening behind the screen rather than

02:50:44,791 --> 02:50:50,460
try to clean them or legalize them with

02:50:48,091 --> 02:50:51,660
pop-ups or any measures of that sort and

02:50:50,460 --> 02:50:54,030
this is where it gets difficult because

02:50:51,660 --> 02:50:56,040
at some at some point and enforcement of

02:50:54,030 --> 02:50:58,229
GDP our would have to mean that we

02:50:56,040 --> 02:51:00,030
counsel some of the business models will

02:50:58,229 --> 02:51:02,220
simply delete them like the whole

02:51:00,030 --> 02:51:04,170
architect business model the advertising

02:51:02,220 --> 02:51:06,300
technology developed in a way that can

02:51:04,170 --> 02:51:09,390
not be rocked and reconciled with GDP

02:51:06,300 --> 02:51:11,160
are no pop up will clean it so we have

02:51:09,390 --> 02:51:13,050
to face it and that's the difficult part

02:51:11,160 --> 02:51:15,000
because even for the deputation

02:51:13,050 --> 02:51:17,340
authorities today it is a bit hard to

02:51:15,000 --> 02:51:19,350
say okay that branch of the industry has

02:51:17,340 --> 02:51:21,780
to disappear because it's not in line

02:51:19,350 --> 02:51:23,520
with our principles that's the heart the

02:51:21,780 --> 02:51:25,229
heartbeat and once you guys you and

02:51:23,520 --> 02:51:26,820
profit brought the cookie populist is a

02:51:25,229 --> 02:51:28,740
hot-button issue backstage when we were

02:51:26,820 --> 02:51:31,110
talking about when you go to website hey

02:51:28,740 --> 02:51:32,880
we are asking to save cookies in your

02:51:31,110 --> 02:51:35,010
computer is that okay look I get that

02:51:32,880 --> 02:51:37,170
pop up still 50 times a day

02:51:35,010 --> 02:51:40,470
but Guillermo when I mentioned that you

02:51:37,170 --> 02:51:42,720
said don't blame the site's for that pop

02:51:40,470 --> 02:51:45,360
of like who's that blame for that

02:51:42,720 --> 02:51:47,100
annoyance or that no bad thing or is it

02:51:45,360 --> 02:51:49,290
all like what's important to recognize

02:51:47,100 --> 02:51:51,650
is that nobody is forcing those

02:51:49,290 --> 02:51:54,960
advertising networks to do those complex

02:51:51,650 --> 02:51:59,130
cookie-cookie pop-ups or consent notices

02:51:54,960 --> 02:52:00,990
via email that way at all and to Cash's

02:51:59,130 --> 02:52:03,180
point nobody's forcing them to have that

02:52:00,990 --> 02:52:05,580
business model where it's all based on

02:52:03,180 --> 02:52:09,030
tracking every single one of us every

02:52:05,580 --> 02:52:12,780
single click we do around the web so the

02:52:09,030 --> 02:52:15,000
whole the whole point is a how do we not

02:52:12,780 --> 02:52:17,040
the market out of that business model

02:52:15,000 --> 02:52:21,060
fully agreed into one that is much more

02:52:17,040 --> 02:52:23,310
privacy friendly but also remember that

02:52:21,060 --> 02:52:24,840
the GD P R is not the only legislative

02:52:23,310 --> 02:52:26,370
tool that Europe has in any question

02:52:24,840 --> 02:52:28,020
provide has just mentioned the privacy

02:52:26,370 --> 02:52:29,820
regulation this goes hand in hand with

02:52:28,020 --> 02:52:33,210
the GD P R and it's not complete yet and

02:52:29,820 --> 02:52:35,610
it's there where the EU is trying to

02:52:33,210 --> 02:52:37,680
give some rules out there to users and

02:52:35,610 --> 02:52:39,870
to and to market actors about how to

02:52:37,680 --> 02:52:42,510
deal with for example browser settings

02:52:39,870 --> 02:52:45,360
and kind of the settings on the devices

02:52:42,510 --> 02:52:47,640
on what on what companies can and cannot

02:52:45,360 --> 02:52:49,500
do with our own personal data and with

02:52:47,640 --> 02:52:54,330
our devices and unfortunately that

02:52:49,500 --> 02:52:56,790
legislative file is stuck yeah I just

02:52:54,330 --> 02:52:58,860
maybe just a side comment I think that

02:52:56,790 --> 02:52:59,970
you know one thing is kind of to focus

02:52:58,860 --> 02:53:01,020
on the kind of friction and the

02:52:59,970 --> 02:53:03,790
annoyance you know and the other thing

02:53:01,020 --> 02:53:05,530
is to kind of see it turn around

02:53:03,790 --> 02:53:08,830
a little bit and say that actually you

02:53:05,530 --> 02:53:10,630
know that bit of friction um is making a

02:53:08,830 --> 02:53:13,810
parent it's surfacing something that has

02:53:10,630 --> 02:53:16,420
been kind of been hiding you know like a

02:53:13,810 --> 02:53:19,450
and and so you know it feels a little

02:53:16,420 --> 02:53:21,850
bit like kind of you know there's a kind

02:53:19,450 --> 02:53:23,470
of bump under the carpet and before it

02:53:21,850 --> 02:53:25,360
you couldn't really feel the bump right

02:53:23,470 --> 02:53:26,740
and the and now you kind of say hi

02:53:25,360 --> 02:53:28,120
there's a funny bump under the carpet

02:53:26,740 --> 02:53:29,590
what's going on and you kind of look

02:53:28,120 --> 02:53:31,540
under the carpet and you see all these

02:53:29,590 --> 02:53:33,970
this whole tracking business that's

02:53:31,540 --> 02:53:35,980
going on right and and and so of course

02:53:33,970 --> 02:53:37,750
the bump is uncomfortable and you know

02:53:35,980 --> 02:53:38,920
I'm be much nicer if the bumper went

02:53:37,750 --> 02:53:40,300
there you know but actually it's worth

02:53:38,920 --> 02:53:41,590
looking under the carpet and to see

02:53:40,300 --> 02:53:44,740
what's going on there you know and I

02:53:41,590 --> 02:53:46,840
think that so we would you know we have

02:53:44,740 --> 02:53:48,640
regulated and made proposals for making

02:53:46,840 --> 02:53:50,620
the bump a bit more smoother it's just

02:53:48,640 --> 02:53:52,840
what we mo said and let's see what the

02:53:50,620 --> 02:53:55,300
future of that file a holes but I think

02:53:52,840 --> 02:53:56,771
besides ironing out the bump

02:53:55,300 --> 02:53:57,970
let's take a look under the carpet you

02:53:56,771 --> 02:53:59,800
know and let's do let's have a look

02:53:57,970 --> 02:54:01,390
what's actually going on there and and

02:53:59,800 --> 02:54:03,430
there's something that we'll be we'll be

02:54:01,390 --> 02:54:05,710
focusing on in the next in the next

02:54:03,430 --> 02:54:07,900
couple of months and years to come you

02:54:05,710 --> 02:54:10,900
know to take a to shine a light a little

02:54:07,900 --> 02:54:13,750
bit on on on exactly a very complex kind

02:54:10,900 --> 02:54:17,050
of industry and a kind of complex

02:54:13,750 --> 02:54:20,500
service which is you know I think I you

02:54:17,050 --> 02:54:21,640
know it's I'm not really I'm agnostic at

02:54:20,500 --> 02:54:23,290
this particular point where there's

02:54:21,640 --> 02:54:25,750
something that like Kasia said we know

02:54:23,290 --> 02:54:27,130
we need to kind of be tough on on that

02:54:25,750 --> 02:54:28,960
you know I'm also conscious that

02:54:27,130 --> 02:54:30,940
actually some of this is cross financing

02:54:28,960 --> 02:54:33,130
free services and there's a huge

02:54:30,940 --> 02:54:35,080
expectation of cons consumers to kind of

02:54:33,130 --> 02:54:37,690
consume content for free on on the web

02:54:35,080 --> 02:54:39,970
so my offer step is gonna be just to

02:54:37,690 --> 02:54:42,250
take a look you know actually and and

02:54:39,970 --> 02:54:45,130
and take it take a look with open mind

02:54:42,250 --> 02:54:46,930
at a very complex in area what we do

02:54:45,130 --> 02:54:48,160
know though from some behavioral

02:54:46,930 --> 02:54:50,620
research that we've been doing and

02:54:48,160 --> 02:54:52,300
commissioning is that when you talk to

02:54:50,620 --> 02:54:53,650
users the first time you talk to users

02:54:52,300 --> 02:54:54,820
and you say hey what do you think about

02:54:53,650 --> 02:54:56,710
targeting what do you think about

02:54:54,820 --> 02:54:58,271
recommender systems and so on you know

02:54:56,710 --> 02:55:00,040
first time they say yeah this is really

02:54:58,271 --> 02:55:02,080
good stuff you know like the I get what

02:55:00,040 --> 02:55:03,610
I want you know and then you iterate in

02:55:02,080 --> 02:55:06,190
you explain hey do you know how they get

02:55:03,610 --> 02:55:07,810
to know how to target you know I said I

02:55:06,190 --> 02:55:10,120
didn't know and then the second a second

02:55:07,810 --> 02:55:11,680
iteration they said maybe I'm it's not

02:55:10,120 --> 02:55:12,940
such a great idea you know if you to

02:55:11,680 --> 02:55:15,400
iterate once more you know when people

02:55:12,940 --> 02:55:16,660
can swing their views on on on whether

02:55:15,400 --> 02:55:17,850
they like what's going on

02:55:16,660 --> 02:55:19,920
quite

02:55:17,850 --> 02:55:21,270
I think that process of discussion is

02:55:19,920 --> 02:55:23,760
really something that we need to have

02:55:21,270 --> 02:55:25,680
now again writing system is that people

02:55:23,760 --> 02:55:27,540
are on one hand observe constantly

02:55:25,680 --> 02:55:29,790
beyond they awareness on the other they

02:55:27,540 --> 02:55:32,550
are constantly targeted so that's a

02:55:29,790 --> 02:55:34,140
deeply unfair setting anyway we have to

02:55:32,550 --> 02:55:36,000
rebuild it and that's why I'm saying

02:55:34,140 --> 02:55:38,689
that some business models will have to

02:55:36,000 --> 02:55:41,220
go away because they're fundamentally

02:55:38,689 --> 02:55:43,590
they cannot be aligned with the

02:55:41,220 --> 02:55:46,890
principles of trust transparency choice

02:55:43,590 --> 02:55:48,359
that we built in the European law it's

02:55:46,890 --> 02:55:50,699
like with ecology you know we can

02:55:48,359 --> 02:55:52,319
discuss how to deal with waste and how

02:55:50,699 --> 02:55:54,420
to recycle it but wouldn't be better to

02:55:52,319 --> 02:55:55,949
produce less waste or you know remove

02:55:54,420 --> 02:55:57,569
just two quick thoughts actually on all

02:55:55,949 --> 02:55:59,069
of this one is first of all it's very

02:55:57,569 --> 02:56:00,390
encouraging to hear that an important

02:55:59,069 --> 02:56:02,040
regulator that like the European

02:56:00,390 --> 02:56:03,899
Commission is not willing to lift up the

02:56:02,040 --> 02:56:05,520
carpet and look underneath it because

02:56:03,899 --> 02:56:08,220
many groups like ours we've been kind of

02:56:05,520 --> 02:56:10,819
something that alarm bell for 10 15

02:56:08,220 --> 02:56:13,949
years there's a carbon metal forest

02:56:10,819 --> 02:56:15,899
there's that there's been or there was

02:56:13,949 --> 02:56:17,489
an ongoing global conversation about

02:56:15,899 --> 02:56:19,140
this whole do-not-track standard and

02:56:17,489 --> 02:56:20,970
when that failed we at least in Europe

02:56:19,140 --> 02:56:23,220
were saying well then we gotta find at

02:56:20,970 --> 02:56:25,260
least a European solution because try

02:56:23,220 --> 02:56:27,630
tracking online tracking cannot become

02:56:25,260 --> 02:56:29,069
the just the main business model

02:56:27,630 --> 02:56:31,020
everywhere and it has and it's bigger

02:56:29,069 --> 02:56:33,270
and bigger so how to keep that under

02:56:31,020 --> 02:56:35,010
control so it's great that the European

02:56:33,270 --> 02:56:40,140
Commission it now has a better approach

02:56:35,010 --> 02:56:41,699
but also the gdpr has important

02:56:40,140 --> 02:56:43,649
principles so I'm coming back to the

02:56:41,699 --> 02:56:45,479
question on enforcement as well we have

02:56:43,649 --> 02:56:48,479
a lot of rules but not a lot of

02:56:45,479 --> 02:56:50,310
resources and time yet to actually go

02:56:48,479 --> 02:56:52,290
enforce them so for example privacy by

02:56:50,310 --> 02:56:55,020
design and by default are two very

02:56:52,290 --> 02:56:56,760
important principles that I bet you

02:56:55,020 --> 02:56:59,369
anything all these ad networks are not

02:56:56,760 --> 02:57:01,619
complying with if it came to to a court

02:56:59,369 --> 02:57:02,939
to have to rule on that so there are

02:57:01,619 --> 02:57:06,060
already tools that we have to start

02:57:02,939 --> 02:57:07,859
using to iron out that carpet and get

02:57:06,060 --> 02:57:09,869
all those business models that are not

02:57:07,859 --> 02:57:12,300
compatible with fundamental rights out

02:57:09,869 --> 02:57:15,330
the door yeah I this may be wrong but

02:57:12,300 --> 02:57:17,250
sometimes it feels like I mean I take

02:57:15,330 --> 02:57:20,159
your PIN perspective I'm looking at what

02:57:17,250 --> 02:57:21,840
FTC does and what European what us-based

02:57:20,159 --> 02:57:23,399
politicians like I said before and say

02:57:21,840 --> 02:57:26,279
from European perspective so I actually

02:57:23,399 --> 02:57:28,109
see quite radical headlines coming from

02:57:26,279 --> 02:57:30,960
the US I also remember a from two years

02:57:28,109 --> 02:57:33,410
ago a us-based researchers that

02:57:30,960 --> 02:57:36,870
okay it's coming to CP TP our annual

02:57:33,410 --> 02:57:39,600
brussels-based conference on privacy

02:57:36,870 --> 02:57:40,680
with this break them up banner and I

02:57:39,600 --> 02:57:42,300
follow Wow

02:57:40,680 --> 02:57:45,060
welcome guys like finally somebody's

02:57:42,300 --> 02:57:46,890
saying this really forceful way rules

02:57:45,060 --> 02:57:47,670
for measures like financial science for

02:57:46,890 --> 02:57:49,260
quite a while

02:57:47,670 --> 02:57:51,210
against these companies and I believe

02:57:49,260 --> 02:57:53,520
they already got to the wall to the

02:57:51,210 --> 02:57:55,440
point where they see that structural

02:57:53,520 --> 02:57:57,450
measures like breaking them up are

02:57:55,440 --> 02:57:59,460
necessary because financial fines

02:57:57,450 --> 02:58:01,350
clearly don't work so maybe they haven't

02:57:59,460 --> 02:58:03,300
came up with privacy protective

02:58:01,350 --> 02:58:06,360
regulation yet it's only now happening

02:58:03,300 --> 02:58:08,640
but before us they started using anti

02:58:06,360 --> 02:58:10,260
Pro competition measures against these

02:58:08,640 --> 02:58:12,300
big platforms because they had them at

02:58:10,260 --> 02:58:15,090
home and maybe they were the first to

02:58:12,300 --> 02:58:17,040
see targeting risks or manipulation of

02:58:15,090 --> 02:58:19,530
public debate risks because they had a

02:58:17,040 --> 02:58:21,990
lot of this business going on before we

02:58:19,530 --> 02:58:24,360
saw it at full scale in Europe with will

02:58:21,990 --> 02:58:27,000
say brexit campaign so I wouldn't be

02:58:24,360 --> 02:58:29,940
that critical I think a lot of ideas are

02:58:27,000 --> 02:58:32,310
coming from the other side of Oh first

02:58:29,940 --> 02:58:33,479
of all this is what I meant earlier you

02:58:32,310 --> 02:58:35,370
know in the conversation the global

02:58:33,479 --> 02:58:36,479
conversation around regulating tech has

02:58:35,370 --> 02:58:39,420
of course changed you know and that's

02:58:36,479 --> 02:58:41,160
part so part of it is that can general

02:58:39,420 --> 02:58:43,979
tech lash kind of phenomenon and the

02:58:41,160 --> 02:58:45,479
other one is that that that you know

02:58:43,979 --> 02:58:47,550
people are looking at all over the world

02:58:45,479 --> 02:58:49,770
about you know kind of similar issues

02:58:47,550 --> 02:58:52,860
you know whether it's data protection or

02:58:49,770 --> 02:58:54,720
content moderation or you know like what

02:58:52,860 --> 02:58:56,520
happened after the Christchurch incident

02:58:54,720 --> 02:58:57,630
was a kind of example where kind of

02:58:56,520 --> 02:58:59,070
governments saying hey what are you

02:58:57,630 --> 02:59:01,940
going to do about it kind of just watch

02:58:59,070 --> 02:59:04,440
going watch this happening I just one

02:59:01,940 --> 02:59:06,180
wanted a warning and maybe that's a bit

02:59:04,440 --> 02:59:08,400
weird from a European Commission to say

02:59:06,180 --> 02:59:09,930
that but you know the quantity of of

02:59:08,400 --> 02:59:11,940
regulation is not necessarily a good

02:59:09,930 --> 02:59:14,280
indicator you know of success you know

02:59:11,940 --> 02:59:15,930
it is it's you know we see the Russian

02:59:14,280 --> 02:59:17,340
internet law is about to come in force

02:59:15,930 --> 02:59:19,170
on the 1st of November you know and we

02:59:17,340 --> 02:59:21,000
really have quite serious concerns about

02:59:19,170 --> 02:59:23,760
that you know so it's not just whether

02:59:21,000 --> 02:59:25,770
you are a like a regulating the net and

02:59:23,760 --> 02:59:28,320
how many laws you've got and got there

02:59:25,770 --> 02:59:32,160
you know it's really about finding you

02:59:28,320 --> 02:59:33,810
know a good operable framework that

02:59:32,160 --> 02:59:36,240
gives market passage participants

02:59:33,810 --> 02:59:38,160
clarity that protects fundamental rights

02:59:36,240 --> 02:59:40,050
and users rights you know and that that

02:59:38,160 --> 02:59:42,540
meets kind of legitimate well specified

02:59:40,050 --> 02:59:44,521
policy aims and and that's that's really

02:59:42,540 --> 02:59:46,891
kind of what we should be looking at you

02:59:44,521 --> 02:59:49,440
not really the amount of regulation

02:59:46,891 --> 02:59:52,440
there's no shortage of appetite

02:59:49,440 --> 02:59:54,301
particularly in in less free countries

02:59:52,440 --> 02:59:55,891
to regulate the Internet you know let's

02:59:54,301 --> 02:59:57,950
not forget that you know like this

02:59:55,891 --> 03:00:00,811
should not be the only yardstick that we

02:59:57,950 --> 03:00:02,490
that we apply you know whether we well

03:00:00,811 --> 03:00:04,171
the Arctic we should be applying is are

03:00:02,490 --> 03:00:07,051
we solving a real problem to have

03:00:04,171 --> 03:00:09,601
evidence of the nature the scale and and

03:00:07,051 --> 03:00:12,301
and and the depth of the problem do we

03:00:09,601 --> 03:00:14,910
are we taking the least invasive option

03:00:12,301 --> 03:00:17,400
which is still effective you know to

03:00:14,910 --> 03:00:18,841
address these problems this is the this

03:00:17,400 --> 03:00:22,801
is the standard to which the Commission

03:00:18,841 --> 03:00:25,051
should hold itself and we do and then

03:00:22,801 --> 03:00:28,410
the democratic process should should

03:00:25,051 --> 03:00:31,290
challenge that right and the so but it's

03:00:28,410 --> 03:00:33,631
really important not to just say our is

03:00:31,290 --> 03:00:35,251
there enough regulation or not because

03:00:33,631 --> 03:00:37,141
that puts us in a binary of whether we

03:00:35,251 --> 03:00:38,971
regulate or not it's not that's the

03:00:37,141 --> 03:00:40,171
wrong that's the wrong framing the wrong

03:00:38,971 --> 03:00:44,330
framing is what's the problem exactly

03:00:40,171 --> 03:00:47,910
and on how we're gonna solve it yeah and

03:00:44,330 --> 03:00:50,761
well first of all also to say like I

03:00:47,910 --> 03:00:52,200
agree just regulation for the sake of

03:00:50,761 --> 03:00:54,601
regulation is not what we're talking

03:00:52,200 --> 03:00:56,521
about and if I'm a Europe doesn't do

03:00:54,601 --> 03:00:58,080
everything right either so not we're not

03:00:56,521 --> 03:01:00,030
like a civil society I think we're not

03:00:58,080 --> 03:01:01,440
happy with every Internet related

03:01:00,030 --> 03:01:03,150
regulation that has come out today you

03:01:01,440 --> 03:01:04,351
look at the copyright directive or what

03:01:03,150 --> 03:01:08,851
might happen with the terrorist content

03:01:04,351 --> 03:01:11,101
directive so on the but from either

03:01:08,851 --> 03:01:13,681
point is that the there are many

03:01:11,101 --> 03:01:16,681
conversations around the world in Latin

03:01:13,681 --> 03:01:18,360
America in the u.s. in India in so many

03:01:16,681 --> 03:01:19,471
different for as well about how do we

03:01:18,360 --> 03:01:21,150
deal with these complex

03:01:19,471 --> 03:01:23,580
multi-dimensional Internet problems and

03:01:21,150 --> 03:01:26,671
the solution will not always be through

03:01:23,580 --> 03:01:29,160
EU style regulation and that may be fine

03:01:26,671 --> 03:01:31,801
as long as we are solving problems that

03:01:29,160 --> 03:01:33,601
are that allows work as long as for

03:01:31,801 --> 03:01:35,820
finding saloons or is that the GDP are

03:01:33,601 --> 03:01:38,511
is not just now that we're singling in

03:01:35,820 --> 03:01:42,181
like zooming in to GDP are is not just a

03:01:38,511 --> 03:01:44,070
protective sphere for Europeans or for

03:01:42,181 --> 03:01:45,780
people in Europe it's actually if you

03:01:44,070 --> 03:01:48,811
read the title of the GDP are the second

03:01:45,780 --> 03:01:50,881
part of it is until data flows around

03:01:48,811 --> 03:01:53,131
the world or something like this and and

03:01:50,881 --> 03:01:54,900
that's something that that we need the

03:01:53,131 --> 03:01:58,260
European Commission as well to through

03:01:54,900 --> 03:02:00,359
those negotiations it has with other

03:01:58,260 --> 03:02:02,910
country's so that the data can personal

03:02:00,359 --> 03:02:05,279
data can flow in both directions uphold

03:02:02,910 --> 03:02:07,080
kind of help elevate the level of data

03:02:05,279 --> 03:02:08,520
protection around the world yeah yeah I

03:02:07,080 --> 03:02:10,260
want to go that's a really good point

03:02:08,520 --> 03:02:12,240
but I want to go back to what prophet

03:02:10,260 --> 03:02:14,970
was saying about but I got from that was

03:02:12,240 --> 03:02:16,350
essentially quantity is not important

03:02:14,970 --> 03:02:18,450
like the quality of the laws being

03:02:16,350 --> 03:02:19,979
passed is more important and this is a

03:02:18,450 --> 03:02:22,290
question I asked in the previous panel

03:02:19,979 --> 03:02:24,569
but whenever I see Facebook and Google

03:02:22,290 --> 03:02:26,640
being fined billions and looking at how

03:02:24,569 --> 03:02:28,439
much they make it's just in the quarter

03:02:26,640 --> 03:02:31,620
which is multiple billions like if we're

03:02:28,439 --> 03:02:33,740
talking about future of policy for these

03:02:31,620 --> 03:02:38,310
Internet companies how do you go about

03:02:33,740 --> 03:02:40,350
reprimanding or putting ineffective laws

03:02:38,310 --> 03:02:46,200
when money is no object how does that

03:02:40,350 --> 03:02:48,090
how do you do that well it's a it's a

03:02:46,200 --> 03:02:50,819
good question so the the treaty says

03:02:48,090 --> 03:02:52,410
that the sanction should be this way

03:02:50,819 --> 03:02:55,470
stiff proportionate and effective you

03:02:52,410 --> 03:02:57,660
know in it and and and yeah and we're

03:02:55,470 --> 03:02:59,220
asking ourselves what does that mean now

03:02:57,660 --> 03:03:01,109
you know if the stock price goes up

03:02:59,220 --> 03:03:03,630
after you hit a company with a five

03:03:01,109 --> 03:03:05,220
billion dollar fine you know what is

03:03:03,630 --> 03:03:07,290
that this weighs if you know is that

03:03:05,220 --> 03:03:10,500
effective and is it proportionate you

03:03:07,290 --> 03:03:11,760
know so that's that's that's a really

03:03:10,500 --> 03:03:15,060
that's a really good question you know

03:03:11,760 --> 03:03:17,189
actually ultimately though the fine is a

03:03:15,060 --> 03:03:21,330
is a means to an end and is the behavior

03:03:17,189 --> 03:03:24,029
change you know that you end to the to

03:03:21,330 --> 03:03:25,319
the rules you know anthea so it's

03:03:24,029 --> 03:03:28,050
certainly something we're going to take

03:03:25,319 --> 03:03:30,990
into account and and you know Commission

03:03:28,050 --> 03:03:33,470
on antitrust has fine Google on numerous

03:03:30,990 --> 03:03:36,630
occasions you know idea in some cases

03:03:33,470 --> 03:03:38,819
you know they have been commitments

03:03:36,630 --> 03:03:40,350
which you were monitoring and and to see

03:03:38,819 --> 03:03:42,240
whether they're really living up so it's

03:03:40,350 --> 03:03:45,210
not just about the define so there's

03:03:42,240 --> 03:03:47,010
also a follow up on that but you all

03:03:45,210 --> 03:03:49,740
right I mean I think we are we are

03:03:47,010 --> 03:03:51,270
entering into into a space where we're

03:03:49,740 --> 03:03:52,529
asking ourselves you know what what does

03:03:51,270 --> 03:03:54,630
this place have really looked like you

03:03:52,529 --> 03:03:56,160
know and and and and that's a really

03:03:54,630 --> 03:03:58,800
important question you know and a lot of

03:03:56,160 --> 03:04:00,479
people are asking that question in terms

03:03:58,800 --> 03:04:02,250
of kind of and they're framing it I'm

03:04:00,479 --> 03:04:03,090
not necessarily framing like this but it

03:04:02,250 --> 03:04:05,609
is being framed

03:04:03,090 --> 03:04:07,830
as a matter of sovereignty rule of law

03:04:05,609 --> 03:04:09,420
kind of you know who who's the boss yeah

03:04:07,830 --> 03:04:11,939
you know actually it's not necessarily

03:04:09,420 --> 03:04:14,010
how I view it but this is a calm

03:04:11,939 --> 03:04:16,289
as part of the conversation out there

03:04:14,010 --> 03:04:19,619
you know and so will I don't have a good

03:04:16,289 --> 03:04:20,789
answer of what the alternative is but

03:04:19,619 --> 03:04:22,709
it's certainly going something we're

03:04:20,789 --> 03:04:24,390
going to look at I have a suggestion I

03:04:22,709 --> 03:04:26,189
think there are these three approaches

03:04:24,390 --> 03:04:28,079
and two of them we already tried first

03:04:26,189 --> 03:04:30,029
is taxing them because that's

03:04:28,079 --> 03:04:31,470
effectively what the fines are it's just

03:04:30,029 --> 03:04:33,090
another tax imposed on these companies

03:04:31,470 --> 03:04:35,010
and they're clearly ready to pay that

03:04:33,090 --> 03:04:36,600
tax because they became wealthy enough

03:04:35,010 --> 03:04:38,520
to do it and that's not going to be

03:04:36,600 --> 03:04:40,619
effective but well I mean transferring

03:04:38,520 --> 03:04:42,270
money from their budgets to public

03:04:40,619 --> 03:04:44,249
budgets make sense I'm not saying stop

03:04:42,270 --> 03:04:45,720
finding them yes please continue but

03:04:44,249 --> 03:04:47,729
remember that there are two other

03:04:45,720 --> 03:04:49,649
approaches possible second is what I

03:04:47,729 --> 03:04:51,959
call Constitution for digital Emperor's

03:04:49,649 --> 03:04:53,430
we can approach these digital Emperor's

03:04:51,959 --> 03:04:56,340
and kindly ask them to be more

03:04:53,430 --> 03:04:59,279
protective of our principles our rights

03:04:56,340 --> 03:05:01,170
so and ask Facebook to observe due

03:04:59,279 --> 03:05:03,659
process when it's removing content or

03:05:01,170 --> 03:05:06,659
maybe reveal and more information about

03:05:03,659 --> 03:05:07,739
targeting political ads all other types

03:05:06,659 --> 03:05:09,720
of content you know we've been

03:05:07,739 --> 03:05:12,359
discussing this for a decade I believe

03:05:09,720 --> 03:05:13,560
we had some progress gdpr also can be

03:05:12,359 --> 03:05:15,959
seen as this constitution for the

03:05:13,560 --> 03:05:18,479
emperor to some extent I believe digital

03:05:15,959 --> 03:05:20,159
services Act another initiative coming

03:05:18,479 --> 03:05:21,989
from Brussels which I have welcome very

03:05:20,159 --> 03:05:26,220
much could be such a constitution but

03:05:21,989 --> 03:05:28,739
that's still not taking the power away

03:05:26,220 --> 03:05:30,659
from these Emperor's and that we

03:05:28,739 --> 03:05:33,989
deferred approach which is structural

03:05:30,659 --> 03:05:35,489
measures but not case-by-case having FCC

03:05:33,989 --> 03:05:38,340
or European Commission busy

03:05:35,489 --> 03:05:40,199
investigating you know multiple markets

03:05:38,340 --> 03:05:42,630
different markets different companies

03:05:40,199 --> 03:05:44,939
but maybe having some new data I

03:05:42,630 --> 03:05:46,619
advocate for also in my previous talk in

03:05:44,939 --> 03:05:47,939
this space and I think that's the

03:05:46,619 --> 03:05:50,220
biggest challenge of course we know we

03:05:47,939 --> 03:05:52,170
won't come up with it next month but

03:05:50,220 --> 03:05:54,630
maybe next five years is the timeframe

03:05:52,170 --> 03:05:56,340
in which we could redevelop our thinking

03:05:54,630 --> 03:05:58,680
about this data ecosystem and propose

03:05:56,340 --> 03:06:02,369
simply new structure move the power a

03:05:58,680 --> 03:06:04,529
bit around the field yeah I the Emperor

03:06:02,369 --> 03:06:07,039
Empire metaphor is so strong I want to

03:06:04,529 --> 03:06:10,050
go back to character it just add

03:06:07,039 --> 03:06:12,140
learning from past mistakes is also very

03:06:10,050 --> 03:06:14,310
important so a very recent one is

03:06:12,140 --> 03:06:16,800
allowing the merger between Facebook and

03:06:14,310 --> 03:06:18,930
whatsapp I think that it's now evident

03:06:16,800 --> 03:06:21,060
that we've just allowed that giant to

03:06:18,930 --> 03:06:23,100
become so big that it has created the

03:06:21,060 --> 03:06:24,600
that it now controls effectively the

03:06:23,100 --> 03:06:24,820
largest communication platform in the

03:06:24,600 --> 03:06:27,551
world

03:06:24,820 --> 03:06:29,110
world and regulating it and keeping

03:06:27,551 --> 03:06:30,910
under control all the problems that stem

03:06:29,110 --> 03:06:33,851
from that has become so complex because

03:06:30,910 --> 03:06:35,590
of that and in hindsight at least in

03:06:33,851 --> 03:06:36,790
from a European context the European

03:06:35,590 --> 03:06:39,820
Commission was in a good place

03:06:36,790 --> 03:06:42,431
wait a second this is probably gonna

03:06:39,820 --> 03:06:45,271
create two wave more problems than

03:06:42,431 --> 03:06:47,771
benefits the the question is whether

03:06:45,271 --> 03:06:49,570
competition rules and the policies that

03:06:47,771 --> 03:06:52,181
in that helped the Commission make those

03:06:49,570 --> 03:06:54,190
decisions are up and you know kind of

03:06:52,181 --> 03:06:55,330
ready for the digital age and that's

03:06:54,190 --> 03:06:58,030
another question that we need to be

03:06:55,330 --> 03:07:00,070
asking how can we rethink all those

03:06:58,030 --> 03:07:02,440
frameworks and all those enforcer

03:07:00,070 --> 03:07:05,710
authorities to be able to deal with such

03:07:02,440 --> 03:07:07,630
new complex phenomenon yeah yeah we

03:07:05,710 --> 03:07:08,860
don't well so the panelist told me that

03:07:07,630 --> 03:07:10,840
they want to forfeit some of their time

03:07:08,860 --> 03:07:12,460
to hear some of your questions so I'll

03:07:10,840 --> 03:07:14,230
only keep it at two more questions and

03:07:12,460 --> 03:07:16,181
then you guys can hear from them the

03:07:14,230 --> 03:07:18,910
audience but going back to what you said

03:07:16,181 --> 03:07:20,891
before about how GDP are a big part of

03:07:18,910 --> 03:07:22,900
it is like data flow freely around the

03:07:20,891 --> 03:07:26,040
world and global is the first word in

03:07:22,900 --> 03:07:30,011
GDP are I want to talk to you about

03:07:26,040 --> 03:07:31,660
global not just regional but also being

03:07:30,011 --> 03:07:34,380
all-inclusive like you know it's only

03:07:31,660 --> 03:07:37,931
lawmakers generally who kind of have a

03:07:34,380 --> 03:07:39,070
influence over the laws get passed yeah

03:07:37,931 --> 03:07:40,570
that's a good an interesting

03:07:39,070 --> 03:07:43,811
conversation we were having backstage as

03:07:40,570 --> 03:07:45,750
well in the context of so well first of

03:07:43,811 --> 03:07:49,150
all and we had a very interesting

03:07:45,750 --> 03:07:50,801
conversation about what is our

03:07:49,150 --> 03:07:52,950
understanding of the open Internet

03:07:50,801 --> 03:08:01,480
globally like what does open mean and

03:07:52,950 --> 03:08:03,431
and there is such a multiple fora both

03:08:01,480 --> 03:08:05,170
regulatory and self regulatory and just

03:08:03,431 --> 03:08:07,181
conversations around the world that it's

03:08:05,170 --> 03:08:09,130
not very easy to just pinpoint one place

03:08:07,181 --> 03:08:11,110
where you can say okay there is where

03:08:09,130 --> 03:08:13,960
you have you really need that kind of a

03:08:11,110 --> 03:08:16,240
multi-stakeholder approach because on

03:08:13,960 --> 03:08:18,761
some you have it on some you don't so

03:08:16,240 --> 03:08:20,200
right now the just the complexity of the

03:08:18,761 --> 03:08:22,721
global conversation about all these

03:08:20,200 --> 03:08:25,990
issues is pretty and manageable what we

03:08:22,721 --> 03:08:27,820
were talking about just earlier is that

03:08:25,990 --> 03:08:30,761
when you have conversations about human

03:08:27,820 --> 03:08:32,681
right it is for us it is very important

03:08:30,761 --> 03:08:35,380
and that's a lot of what we do globally

03:08:32,681 --> 03:08:37,330
to bringing groups that are the most

03:08:35,380 --> 03:08:38,090
affected to the conversation the most

03:08:37,330 --> 03:08:40,399
March

03:08:38,090 --> 03:08:43,100
vulnerable groups that see themselves

03:08:40,399 --> 03:08:44,960
most affected when there is content

03:08:43,100 --> 03:08:47,780
about issues that they care of being

03:08:44,960 --> 03:08:52,880
taken down or when they are suffering a

03:08:47,780 --> 03:08:54,710
lot of just on tax to the to who they

03:08:52,880 --> 03:08:56,870
are or to what they're doing and so on

03:08:54,710 --> 03:08:58,880
and having those voices in those

03:08:56,870 --> 03:09:01,609
conversations is just key at any level

03:08:58,880 --> 03:09:04,250
yeah and then the last question I want

03:09:01,609 --> 03:09:07,220
to ask everybody on stage before I open

03:09:04,250 --> 03:09:08,510
up to the audience is just thought you

03:09:07,220 --> 03:09:14,060
know what does the wind look like like

03:09:08,510 --> 03:09:18,590
what should we for here don't all rush

03:09:14,060 --> 03:09:20,479
it on us it's a hard one right i I don't

03:09:18,590 --> 03:09:21,710
think we can have final answer today

03:09:20,479 --> 03:09:23,330
because that's a process that's

03:09:21,710 --> 03:09:25,550
something that I believe should be the

03:09:23,330 --> 03:09:27,979
task for the whole movement of people in

03:09:25,550 --> 03:09:29,930
mosfets and Beyond mosfets because it is

03:09:27,979 --> 03:09:32,210
a real task that we have to you know

03:09:29,930 --> 03:09:33,950
work on together nobody should have the

03:09:32,210 --> 03:09:36,500
arrogance aside that's the win for

03:09:33,950 --> 03:09:38,689
everybody but definitely what's not the

03:09:36,500 --> 03:09:40,670
win is the current situation when we

03:09:38,689 --> 03:09:43,340
have big dominant players in the middle

03:09:40,670 --> 03:09:45,080
of the ecosystem controlling data flows

03:09:43,340 --> 03:09:47,090
controlling what is allowed what is not

03:09:45,080 --> 03:09:49,550
allowed guessing the needs of all these

03:09:47,090 --> 03:09:52,070
vulnerable people ruling them basically

03:09:49,550 --> 03:09:54,470
in quite outer Italian way that's what

03:09:52,070 --> 03:09:56,090
we want to depart from some on the other

03:09:54,470 --> 03:09:58,910
on the other side of that spectrum I can

03:09:56,090 --> 03:10:01,010
see open ecosystem in which for example

03:09:58,910 --> 03:10:03,950
vernal group groups can organize

03:10:01,010 --> 03:10:05,720
themselves bottom up and define what it

03:10:03,950 --> 03:10:08,060
means for them to be safe or what it

03:10:05,720 --> 03:10:11,359
means for them to express themselves and

03:10:08,060 --> 03:10:13,910
then the ecosystem basically including

03:10:11,359 --> 03:10:17,090
this allowing for that diversity so a

03:10:13,910 --> 03:10:19,729
diverse federated open ecosystem which

03:10:17,090 --> 03:10:22,910
is not ruled by any benevolent Emperor

03:10:19,729 --> 03:10:25,820
but basically much more bottom-up and

03:10:22,910 --> 03:10:29,120
preserved with rules like GP are like

03:10:25,820 --> 03:10:30,790
hopefully new regulations providing for

03:10:29,120 --> 03:10:34,280
privacy and security

03:10:30,790 --> 03:10:35,720
however she goes yeah I mean I think I

03:10:34,280 --> 03:10:37,340
could have said it better it's that's

03:10:35,720 --> 03:10:39,800
the whole concept we have to protect the

03:10:37,340 --> 03:10:44,300
Internet as the public resource that it

03:10:39,800 --> 03:10:46,640
is to enable communities individuals and

03:10:44,300 --> 03:10:49,760
also companies to thrive in a way in

03:10:46,640 --> 03:10:51,729
which it empowers and expands

03:10:49,760 --> 03:10:54,010
fundamental rights and

03:10:51,729 --> 03:10:55,390
and innovation and so on and not the

03:10:54,010 --> 03:10:57,520
contrary and we've been having this

03:10:55,390 --> 03:10:59,170
common type in like multiple circles

03:10:57,520 --> 03:11:01,449
having similar conversations throughout

03:10:59,170 --> 03:11:03,489
these past days and I've heard some ask

03:11:01,449 --> 03:11:05,920
where are we having a nostalgic vision

03:11:03,489 --> 03:11:07,420
of the internet was it is it that what

03:11:05,920 --> 03:11:11,560
it used to be and we just have to change

03:11:07,420 --> 03:11:13,720
mindset I would say no that still that

03:11:11,560 --> 03:11:15,130
for me still the idea is just the

03:11:13,720 --> 03:11:16,689
dynamics might be different the

03:11:15,130 --> 03:11:18,909
technology is definitely different now

03:11:16,689 --> 03:11:20,739
the you know things like for example the

03:11:18,909 --> 03:11:22,359
advent of AI and so on complicate the

03:11:20,739 --> 03:11:24,789
whole conversation even much more and so

03:11:22,359 --> 03:11:30,010
on but the idea of having this public

03:11:24,789 --> 03:11:31,359
resource that helps society work better

03:11:30,010 --> 03:11:33,760
and integrate better amongst themselves

03:11:31,359 --> 03:11:36,670
and be more open be more open it's

03:11:33,760 --> 03:11:39,880
definitely what we should try for profit

03:11:36,670 --> 03:11:41,979
I think from my perspective I think that

03:11:39,880 --> 03:11:45,760
kind of three elements would offer when

03:11:41,979 --> 03:11:49,510
you know one is users feel kind of safe

03:11:45,760 --> 03:11:51,340
and empowered online but also free you

03:11:49,510 --> 03:11:52,630
know and where that balance lies is

03:11:51,340 --> 03:11:55,029
going to be our challenge in the next

03:11:52,630 --> 03:11:58,510
kind of couple of of months but I think

03:11:55,029 --> 03:12:00,880
that on those two aspects you know users

03:11:58,510 --> 03:12:02,409
being safe empowered and free I think

03:12:00,880 --> 03:12:04,569
the balance isn't quite right at the

03:12:02,409 --> 03:12:06,340
moment you know it's part of the abuse a

03:12:04,569 --> 03:12:07,930
problem that that we mentioned it's

03:12:06,340 --> 03:12:09,819
maybe the ad tracking problem the cache

03:12:07,930 --> 03:12:11,109
I mentioned you know that's one thing

03:12:09,819 --> 03:12:13,149
there's a there's a third element which

03:12:11,109 --> 03:12:15,340
we didn't really discuss which for the

03:12:13,149 --> 03:12:17,260
four from the EU perspective is super

03:12:15,340 --> 03:12:18,520
important is that to continue to make

03:12:17,260 --> 03:12:20,260
sure that the EU is a great place to

03:12:18,520 --> 03:12:21,909
start kind of innovative service in your

03:12:20,260 --> 03:12:23,560
interest maybe I'll end on this anecdote

03:12:21,909 --> 03:12:26,560
a couple of weeks ago I was in Riga and

03:12:23,560 --> 03:12:28,869
it's a great startup student setting up

03:12:26,560 --> 03:12:31,060
a kind of house sharing platform when

03:12:28,869 --> 03:12:33,789
they move when they move into university

03:12:31,060 --> 03:12:35,710
you know it's great great stuff and they

03:12:33,789 --> 03:12:37,420
wanted to expand to Amsterdam and Berlin

03:12:35,710 --> 03:12:39,760
and like even the guys just look at me

03:12:37,420 --> 03:12:41,859
are there any EU EU laws that I should

03:12:39,760 --> 03:12:44,979
be aware of you know I mean I just said

03:12:41,859 --> 03:12:47,800
no I don't worry you know but so it was

03:12:44,979 --> 03:12:50,439
a it was but you know it needs to be and

03:12:47,800 --> 03:12:51,999
we need to make sure that we also keep

03:12:50,439 --> 03:12:54,069
these people in mind we focus on

03:12:51,999 --> 03:12:55,510
Facebook and Google and the big ones you

03:12:54,069 --> 03:12:57,100
know but they're like 10,000 small

03:12:55,510 --> 03:12:59,229
startups in Europe you know all trying

03:12:57,100 --> 03:13:01,149
to kind of grow here at home you know

03:12:59,229 --> 03:13:04,239
and and and they need to make sure we

03:13:01,149 --> 03:13:05,350
need to also make rules that that work

03:13:04,239 --> 03:13:08,319
for these people you know

03:13:05,350 --> 03:13:10,840
so I win for me is a situation in which

03:13:08,319 --> 03:13:13,300
users are safe empowered and free online

03:13:10,840 --> 03:13:14,710
and where small startups feel that

03:13:13,300 --> 03:13:17,170
Europe is a good place to start a

03:13:14,710 --> 03:13:19,510
business I just want to add one more on

03:13:17,170 --> 03:13:21,880
the big ones one for me would also be

03:13:19,510 --> 03:13:24,220
even know that generated about humankind

03:13:21,880 --> 03:13:26,080
all of us by the big ones can serve the

03:13:24,220 --> 03:13:27,430
smaller ones and can serve the public

03:13:26,080 --> 03:13:29,920
purpose let's not forget that there is

03:13:27,430 --> 03:13:31,930
also a lot of knowledge of value

03:13:29,920 --> 03:13:34,390
generated by the existing ecosystem the

03:13:31,930 --> 03:13:35,710
clothes exist exist echo system that is

03:13:34,390 --> 03:13:37,840
now controlled for commercial reasons

03:13:35,710 --> 03:13:40,420
but we don't want to destroy it right we

03:13:37,840 --> 03:13:43,060
want to I hope reuse it for a better so

03:13:40,420 --> 03:13:45,790
that's also target yes all right now

03:13:43,060 --> 03:13:47,949
audience question answer anybody have

03:13:45,790 --> 03:13:50,260
any questions I've got a few in the

03:13:47,949 --> 03:13:51,850
middle one in the back

03:13:50,260 --> 03:13:57,159
Kevin's come around the microphone right

03:13:51,850 --> 03:13:59,920
now I thinks for the panel is really

03:13:57,159 --> 03:14:01,510
interesting um I just wanted to pick up

03:13:59,920 --> 03:14:03,790
on the point about competition law which

03:14:01,510 --> 03:14:06,040
I think Guillermo mentioned and how does

03:14:03,790 --> 03:14:09,250
that need to evolve in the EU and Beyond

03:14:06,040 --> 03:14:11,170
because consumers are giving their data

03:14:09,250 --> 03:14:13,930
away for free they're not exchanging

03:14:11,170 --> 03:14:16,750
money with these companies so how do you

03:14:13,930 --> 03:14:18,640
measure that detriment when it's not

03:14:16,750 --> 03:14:20,350
someone losing money and there might not

03:14:18,640 --> 03:14:24,909
be a choice to switch to different

03:14:20,350 --> 03:14:27,040
service provider if you take this charge

03:14:24,909 --> 03:14:31,119
you can take it so that's a really good

03:14:27,040 --> 03:14:32,920
question so in my team and also a digit

03:14:31,119 --> 03:14:36,060
competition of the European Commission

03:14:32,920 --> 03:14:38,649
like the antitrust team we are in a

03:14:36,060 --> 03:14:41,619
ongoing conversation about exactly that

03:14:38,649 --> 03:14:42,699
what you've just said and I think you

03:14:41,619 --> 03:14:45,100
might have seen that there are a couple

03:14:42,699 --> 03:14:46,689
of reports that happened you know just

03:14:45,100 --> 03:14:50,350
have been published this year around the

03:14:46,689 --> 03:14:53,439
future of competition law and and for in

03:14:50,350 --> 03:14:54,790
this space and and I think that what

03:14:53,439 --> 03:14:56,290
we're doing right now and it's going to

03:14:54,790 --> 03:14:58,000
take a little bit of time but digesting

03:14:56,290 --> 03:15:00,729
all the recommendations that have been

03:14:58,000 --> 03:15:03,159
made in at EU level in Germany in the UK

03:15:00,729 --> 03:15:04,689
in the u.s. also in Australia and we're

03:15:03,159 --> 03:15:07,830
kind of processing this at the moment to

03:15:04,689 --> 03:15:09,909
see it still which elements of

03:15:07,830 --> 03:15:12,939
recommendations will go through a kind

03:15:09,909 --> 03:15:14,470
of review of competition rules and and

03:15:12,939 --> 03:15:16,869
to what extent are they necessary but

03:15:14,470 --> 03:15:18,939
we're also looking at in this area if

03:15:16,869 --> 03:15:21,430
there is any any need for additional ex

03:15:18,939 --> 03:15:22,689
under regulations you know to help that

03:15:21,430 --> 03:15:24,729
but the the point that you're making

03:15:22,689 --> 03:15:26,289
that the kind of consumer standard

03:15:24,729 --> 03:15:28,090
consumer welfare standard is something

03:15:26,289 --> 03:15:29,890
to kind of double check is that really

03:15:28,090 --> 03:15:31,239
the right standard that's certainly

03:15:29,890 --> 03:15:33,579
something we're looking at very closely

03:15:31,239 --> 03:15:35,949
so we have a pretty good team of people

03:15:33,579 --> 03:15:37,149
looking at it is the same team that was

03:15:35,949 --> 03:15:40,060
responsible for the antitrust

03:15:37,149 --> 03:15:41,709
enforcement for Google and I think

03:15:40,060 --> 03:15:43,869
there's a willingness to kind of take a

03:15:41,709 --> 03:15:45,459
fresh look at this and and executive

03:15:43,869 --> 03:15:47,949
vice-president Vista has also announced

03:15:45,459 --> 03:15:50,800
that we would be looking at that and as

03:15:47,949 --> 03:15:52,810
a follow up how exactly it would look

03:15:50,800 --> 03:15:54,159
like I think there are some ideas in the

03:15:52,810 --> 03:15:57,249
table we're just kind of going through

03:15:54,159 --> 03:15:59,680
exactly what it's been proposed and it's

03:15:57,249 --> 03:16:01,180
not going to be a quick quick win but

03:15:59,680 --> 03:16:03,909
it's certainly something we're taking

03:16:01,180 --> 03:16:06,039
very seriously so point fully taken and

03:16:03,909 --> 03:16:07,959
there's a at least two full working

03:16:06,039 --> 03:16:12,130
teams on this to follow it up in the

03:16:07,959 --> 03:16:13,749
Commission okay any other questions out

03:16:12,130 --> 03:16:18,999
in the audience we had a couple over

03:16:13,749 --> 03:16:20,770
here actually a few I'm just McCroskey a

03:16:18,999 --> 03:16:22,060
data scientist with Mozilla and thanks

03:16:20,770 --> 03:16:24,430
to the thoughtful discussion

03:16:22,060 --> 03:16:27,550
I was curious that given that many

03:16:24,430 --> 03:16:29,140
problematic uses of data online use the

03:16:27,550 --> 03:16:31,300
example of the ad tech industry which i

03:16:29,140 --> 03:16:33,729
think is great can really constitute

03:16:31,300 --> 03:16:35,979
human experimentation do you think that

03:16:33,729 --> 03:16:37,840
the existing frameworks around ethical

03:16:35,979 --> 03:16:43,180
approval for human experimentation could

03:16:37,840 --> 03:16:45,520
be a useful tool here I believe you

03:16:43,180 --> 03:16:49,060
refer to an framework for research right

03:16:45,520 --> 03:16:51,279
yeah that's that's a brilliant idea and

03:16:49,060 --> 03:16:53,709
I think also coming back we currently in

03:16:51,279 --> 03:16:56,020
terms of horizon 2020 and these types of

03:16:53,709 --> 03:16:58,390
publicly funded programs that absolutely

03:16:56,020 --> 03:17:00,729
should include this thinking I mean I'm

03:16:58,390 --> 03:17:02,859
saying as external observer I'm not part

03:17:00,729 --> 03:17:03,819
of I'm either researcher here nor a

03:17:02,859 --> 03:17:05,399
regulator

03:17:03,819 --> 03:17:07,659
I wonder what Prabhas has to say but

03:17:05,399 --> 03:17:10,479
definitely it's it's it's the it's

03:17:07,659 --> 03:17:11,979
something that funders of public

03:17:10,479 --> 03:17:14,619
research should have have in mind that

03:17:11,979 --> 03:17:15,909
an oldy to do but I think that your

03:17:14,619 --> 03:17:18,279
question was probably not so much of our

03:17:15,909 --> 03:17:19,539
publicly funded research in that's going

03:17:18,279 --> 03:17:21,670
on but actually about the tech in

03:17:19,539 --> 03:17:22,949
industry practice itself whether that

03:17:21,670 --> 03:17:25,499
should be subject to prior authorization

03:17:22,949 --> 03:17:28,239
due to kind of in the same way that

03:17:25,499 --> 03:17:30,069
biotech experiments by private companies

03:17:28,239 --> 03:17:30,931
have to be pre-approved is that what you

03:17:30,069 --> 03:17:33,750
want it

03:17:30,931 --> 03:17:35,700
I think so we're not there yet but it's

03:17:33,750 --> 03:17:37,740
definitely an interesting and

03:17:35,700 --> 03:17:39,330
interesting Avenue of thought for for

03:17:37,740 --> 03:17:41,641
publicly funded research even if it's

03:17:39,330 --> 03:17:43,620
carried out by private companies there

03:17:41,641 --> 03:17:45,841
is an ethical screening and it data

03:17:43,620 --> 03:17:48,420
protection aspects are a mandatory part

03:17:45,841 --> 03:17:50,280
of that debt screening so if you run if

03:17:48,420 --> 03:17:52,080
you want to get R&D money from the

03:17:50,280 --> 03:17:54,061
European Commission even as a company as

03:17:52,080 --> 03:17:56,551
Facebook or some other company and that

03:17:54,061 --> 03:17:58,591
involves data protection relevant issues

03:17:56,551 --> 03:18:00,450
you have to do a screening otherwise you

03:17:58,591 --> 03:18:02,730
don't get the money but whether the

03:18:00,450 --> 03:18:05,190
industry itself should be subject and

03:18:02,730 --> 03:18:06,540
subject to such such screening is a good

03:18:05,190 --> 03:18:09,740
question I don't have a good answer but

03:18:06,540 --> 03:18:09,740
it's something that's worth examining

03:18:10,431 --> 03:18:18,811
okay next question let's see what kevin

03:18:13,170 --> 03:18:21,450
has the microphone there and I was

03:18:18,811 --> 03:18:23,250
wondering caches that the this new

03:18:21,450 --> 03:18:26,910
ecosystem part of it should come

03:18:23,250 --> 03:18:30,181
bottom-up so I was wondering as citizens

03:18:26,910 --> 03:18:31,710
what can we do to try and help push

03:18:30,181 --> 03:18:34,290
things in the right direction should we

03:18:31,710 --> 03:18:38,221
be weaning ourselves off Google and

03:18:34,290 --> 03:18:40,110
Facebook and all these things and in the

03:18:38,221 --> 03:18:41,641
ecosystem I imagine I do not assume

03:18:40,110 --> 03:18:42,931
movement of people outside of these

03:18:41,641 --> 03:18:45,570
networks I think that could be a

03:18:42,931 --> 03:18:47,700
consequence of a new version a federal

03:18:45,570 --> 03:18:50,040
diversion so once you have a federated

03:18:47,700 --> 03:18:52,681
one then there are places where we can

03:18:50,040 --> 03:18:54,960
move but before we create it it's it's

03:18:52,681 --> 03:18:58,290
it's it's rather forcing these companies

03:18:54,960 --> 03:19:00,960
to open their api's to their level where

03:18:58,290 --> 03:19:04,891
we can experiment and add new layers

03:19:00,960 --> 03:19:06,150
new alternative services on them to to

03:19:04,891 --> 03:19:07,440
start with something like for example

03:19:06,150 --> 03:19:11,210
there is a lot of discussion around

03:19:07,440 --> 03:19:14,160
newsfeed how messy how uncomfortable how

03:19:11,210 --> 03:19:16,801
human experimentation line of thinking I

03:19:14,160 --> 03:19:18,360
will use it you know evil the the

03:19:16,801 --> 03:19:20,551
newsfeed for example is for a different

03:19:18,360 --> 03:19:23,181
one can I achieve it well technically

03:19:20,551 --> 03:19:25,710
yes right I can imagine API that this

03:19:23,181 --> 03:19:27,240
possibility and then eventually we might

03:19:25,710 --> 03:19:28,950
imagine this as a federal structure

03:19:27,240 --> 03:19:30,601
where yes people could also move

03:19:28,950 --> 03:19:32,341
somewhere else but right now they own

03:19:30,601 --> 03:19:37,170
they have nowhere to move and we have to

03:19:32,341 --> 03:19:38,670
acknowledge this I think it would the

03:19:37,170 --> 03:19:40,261
point you raise I have a slightly

03:19:38,670 --> 03:19:41,670
different perspective but you know I

03:19:40,261 --> 03:19:43,620
don't want to confuse you Kasia because

03:19:41,670 --> 03:19:44,279
she's got great ideas but I think that

03:19:43,620 --> 03:19:46,739
the

03:19:44,279 --> 03:19:49,079
power of users to take their business

03:19:46,739 --> 03:19:50,579
elsewhere is really enormous and you

03:19:49,079 --> 03:19:52,979
shouldn't underestimate that you know

03:19:50,579 --> 03:19:55,709
actually it's a it's a really powerful

03:19:52,979 --> 03:19:56,999
factor in in changing company's behavior

03:19:55,709 --> 03:19:59,760
you know much more I think than

03:19:56,999 --> 03:20:02,850
regulators I think you know and and we

03:19:59,760 --> 03:20:04,979
know this from from documents about you

03:20:02,850 --> 03:20:06,930
know strategy documents about how to

03:20:04,979 --> 03:20:09,119
counter WeChat or how to counter

03:20:06,930 --> 03:20:11,129
snapchat and so on you know this is a

03:20:09,119 --> 03:20:14,999
really important thing so countries

03:20:11,129 --> 03:20:17,249
consuming you know it's really a hugely

03:20:14,999 --> 03:20:19,499
important factor in you you do have

03:20:17,249 --> 03:20:22,049
choice you know likely an agency and one

03:20:19,499 --> 03:20:24,779
of the one of the things that that that

03:20:22,049 --> 03:20:27,239
that we want to also say is that you

03:20:24,779 --> 03:20:28,319
know you're not sometimes what I get a

03:20:27,239 --> 03:20:29,850
little bit frustrated with in

03:20:28,319 --> 03:20:32,819
conversations in Brussels in my bubble

03:20:29,850 --> 03:20:34,589
is that that that users are represented

03:20:32,819 --> 03:20:36,419
as kind of purely passive you know like

03:20:34,589 --> 03:20:37,829
that that they are just kind of victims

03:20:36,419 --> 03:20:40,409
of what is thrown at them and they have

03:20:37,829 --> 03:20:43,319
like zero agencies and I think maybe

03:20:40,409 --> 03:20:45,029
we're simultaneously over and under

03:20:43,319 --> 03:20:47,430
estimating the agency of the users you

03:20:45,029 --> 03:20:49,049
know like the sometimes in some aspects

03:20:47,430 --> 03:20:50,879
we think that they can do more than they

03:20:49,049 --> 03:20:53,640
can reasonably do an attention economy

03:20:50,879 --> 03:20:55,079
but in other cases I think consumers do

03:20:53,640 --> 03:20:58,260
have a choice you know and they can make

03:20:55,079 --> 03:20:59,760
a they can make a conscious a choice

03:20:58,260 --> 03:21:01,079
when you know when you pick your food

03:20:59,760 --> 03:21:02,789
and when you pick your clothes you know

03:21:01,079 --> 03:21:05,310
you take conscious choices you know you

03:21:02,789 --> 03:21:06,749
can also do some of that online and

03:21:05,310 --> 03:21:08,310
educate yourself and this is something

03:21:06,749 --> 03:21:09,720
that is really something as part of the

03:21:08,310 --> 03:21:11,399
conversation it's not only about a

03:21:09,720 --> 03:21:14,220
regulation it's also about empowering

03:21:11,399 --> 03:21:16,470
people and educating users you know and

03:21:14,220 --> 03:21:18,149
and I'm actually very optimistic I see

03:21:16,470 --> 03:21:19,379
it when we were in a little bit of work

03:21:18,149 --> 03:21:21,539
we do with kids and we should be doing

03:21:19,379 --> 03:21:23,100
much more is that we actually aware of a

03:21:21,539 --> 03:21:25,560
much higher level of awareness than we

03:21:23,100 --> 03:21:27,149
sometimes assume about about the dangers

03:21:25,560 --> 03:21:29,699
and so on and and their usage patterns

03:21:27,149 --> 03:21:32,159
are different from from from what the

03:21:29,699 --> 03:21:33,869
kind of you know lambda user in the

03:21:32,159 --> 03:21:35,159
policy makers head is and that's

03:21:33,869 --> 03:21:38,189
something that's an important part so

03:21:35,159 --> 03:21:39,629
maybe I'm I'm over emphasizing I'm over

03:21:38,189 --> 03:21:41,299
optimistic but I do think this is

03:21:39,629 --> 03:21:44,600
important part of the conversation I

03:21:41,299 --> 03:21:48,449
would emphasize Roberts last point about

03:21:44,600 --> 03:21:50,399
it read about this like talk to your

03:21:48,449 --> 03:21:53,609
friends about these issues get more

03:21:50,399 --> 03:21:55,109
interested look into how these platforms

03:21:53,609 --> 03:21:58,320
are designed to capture your attention

03:21:55,109 --> 03:22:01,380
and keep you there because

03:21:58,320 --> 03:22:02,700
i well i deleted my facebook account a

03:22:01,380 --> 03:22:07,080
year and a half ago haven't missed it

03:22:02,700 --> 03:22:09,360
for a day we might like there there

03:22:07,080 --> 03:22:10,650
might be some examples where we don't we

03:22:09,360 --> 03:22:11,910
don't really have alternatives and

03:22:10,650 --> 03:22:14,100
you'll ask yourself yeah but then where

03:22:11,910 --> 03:22:14,820
do I go but ask yourself also do I

03:22:14,100 --> 03:22:17,160
really need it

03:22:14,820 --> 03:22:19,230
could I be spending my time somewhere

03:22:17,160 --> 03:22:21,980
else on the internet that is more

03:22:19,230 --> 03:22:26,730
creative for me more participatory more

03:22:21,980 --> 03:22:28,700
and yet just don't like let's have this

03:22:26,730 --> 03:22:32,640
conversation be more more mainstream

03:22:28,700 --> 03:22:34,350
than it actually is just purify my point

03:22:32,640 --> 03:22:37,110
that nowhere to go does not refer to

03:22:34,350 --> 03:22:39,120
lack of other brands or services I mean

03:22:37,110 --> 03:22:40,890
the business model if the business model

03:22:39,120 --> 03:22:41,640
everywhere is more as the same we really

03:22:40,890 --> 03:22:42,960
have nowhere to go

03:22:41,640 --> 03:22:45,090
so that's what I mean it that's why I

03:22:42,960 --> 03:22:47,280
think we need to rethink ecosystem so it

03:22:45,090 --> 03:22:51,420
becomes not service or even but value

03:22:47,280 --> 03:22:52,800
driven no time for that's all time we

03:22:51,420 --> 03:22:56,550
have okay well that's all the time we

03:22:52,800 --> 03:23:00,390
have for this panel but I was late yeah

03:22:56,550 --> 03:23:04,620
do one more will do one more hand up

03:23:00,390 --> 03:23:06,630
back there yeah that's good call my

03:23:04,620 --> 03:23:09,390
question for the panel is should be you

03:23:06,630 --> 03:23:12,930
policy support the concept of digital

03:23:09,390 --> 03:23:14,880
generativity and by that I mean the idea

03:23:12,930 --> 03:23:17,610
if you do things like intro for this in

03:23:14,880 --> 03:23:20,310
decentralization you can enable

03:23:17,610 --> 03:23:24,750
uncoordinated audiences to collaborate

03:23:20,310 --> 03:23:26,850
and generate unplanned innovation the

03:23:24,750 --> 03:23:30,210
classic example is the payment services

03:23:26,850 --> 03:23:33,060
directive to where by mandating open API

03:23:30,210 --> 03:23:35,610
is for banking and also allowing users

03:23:33,060 --> 03:23:36,900
to take their data that at least in

03:23:35,610 --> 03:23:38,910
theories allows you know new third

03:23:36,900 --> 03:23:41,280
parties to come up with new services and

03:23:38,910 --> 03:23:42,900
makes the e much more competitive to

03:23:41,280 --> 03:23:48,271
convince the US where banking's

03:23:42,900 --> 03:23:50,790
in the Stone Age I'm afraid simple

03:23:48,271 --> 03:23:53,760
answer yes absolutely and it's great

03:23:50,790 --> 03:23:55,230
that you raise the PSD to because that's

03:23:53,760 --> 03:23:57,690
one example but there's another example

03:23:55,230 --> 03:23:59,340
for an EU telecom slow now there is the

03:23:57,690 --> 03:24:03,300
power for regulatory authorities to

03:23:59,340 --> 03:24:05,310
force interoperability between chat apps

03:24:03,300 --> 03:24:07,110
between say whatsapp and signal and

03:24:05,310 --> 03:24:08,240
telegram and so on is that something

03:24:07,110 --> 03:24:10,771
that they're going to be using or not

03:24:08,240 --> 03:24:12,000
but definitely having that as a policy

03:24:10,771 --> 03:24:13,560
goal to

03:24:12,000 --> 03:24:15,660
helped create that inter up and that

03:24:13,560 --> 03:24:19,140
desynchronization berlin favorite for

03:24:15,660 --> 03:24:22,320
sure no yeah I mean so and we're pretty

03:24:19,140 --> 03:24:24,480
proud of those the the innovations in

03:24:22,320 --> 03:24:26,310
the interoperability space that that

03:24:24,480 --> 03:24:28,380
that you both mentioned and and I think

03:24:26,310 --> 03:24:30,810
that's that's something that we want to

03:24:28,380 --> 03:24:32,160
build on I just have a small word of

03:24:30,810 --> 03:24:33,960
caution in there is that

03:24:32,160 --> 03:24:35,460
interoperability really works well when

03:24:33,960 --> 03:24:36,630
you have kind of two equivalent things

03:24:35,460 --> 03:24:38,670
that you can really connect to each

03:24:36,630 --> 03:24:41,430
other you know like the and in the

03:24:38,670 --> 03:24:43,380
banking system and or in net neutrality

03:24:41,430 --> 03:24:45,480
regulations you know you did deal with

03:24:43,380 --> 03:24:46,620
you you deal with things that you can

03:24:45,480 --> 03:24:49,320
easily interconnect because they're

03:24:46,620 --> 03:24:53,460
basically the same you know it's not

03:24:49,320 --> 03:24:55,050
super trivial to to take to take that

03:24:53,460 --> 03:24:57,170
concept and to take it to kind of very

03:24:55,050 --> 03:24:59,460
heterogeneous systems you know like

03:24:57,170 --> 03:25:01,470
mastodons and Facebook are kind of very

03:24:59,460 --> 03:25:03,420
different things you know and and so

03:25:01,470 --> 03:25:05,910
while the principle of interoperability

03:25:03,420 --> 03:25:07,800
and decentralized kind of thing is

03:25:05,910 --> 03:25:09,030
really good how to make it work in

03:25:07,800 --> 03:25:10,580
practice I think this is something that

03:25:09,030 --> 03:25:12,360
that requires a little bit more

03:25:10,580 --> 03:25:14,700
conversation I think there's an

03:25:12,360 --> 03:25:16,830
interesting standard which was proposed

03:25:14,700 --> 03:25:17,640
a couple of years ago by w3c about

03:25:16,830 --> 03:25:19,950
social media

03:25:17,640 --> 03:25:21,960
you know news feeds and so on in other

03:25:19,950 --> 03:25:23,820
words that was designed to enable this

03:25:21,960 --> 03:25:25,470
kind of decentralized Federation I think

03:25:23,820 --> 03:25:27,930
we're gonna take a fresh look at that

03:25:25,470 --> 03:25:29,490
and and have a have a you know and to

03:25:27,930 --> 03:25:31,470
have a better understanding of what was

03:25:29,490 --> 03:25:35,130
you know what what what happened to that

03:25:31,470 --> 03:25:36,510
and so on so yes in principle slight

03:25:35,130 --> 03:25:38,790
word of caution on how would this work

03:25:36,510 --> 03:25:40,500
in print in practice you know one of the

03:25:38,790 --> 03:25:42,270
takeaways from discussions here in this

03:25:40,500 --> 03:25:44,910
space is was let's let's take step by

03:25:42,270 --> 03:25:47,940
step approach maybe start with the the

03:25:44,910 --> 03:25:49,950
lower hanging fruit like federating the

03:25:47,940 --> 03:25:52,020
the public posts without discussing

03:25:49,950 --> 03:25:54,150
private groups or private messages that

03:25:52,020 --> 03:25:56,670
are more difficult so also that could be

03:25:54,150 --> 03:25:58,320
like that the banking direct if it

03:25:56,670 --> 03:26:00,270
doesn't really open the whole ecosystem

03:25:58,320 --> 03:26:02,310
it opens just one layer but that's

03:26:00,270 --> 03:26:06,000
already a big breakthrough right so we

03:26:02,310 --> 03:26:07,680
could take the similar approach all

03:26:06,000 --> 03:26:09,360
right I think that's all the time we

03:26:07,680 --> 03:26:12,390
have so big round of applause for our

03:26:09,360 --> 03:26:16,860
panelists today on favor now and all day

03:26:12,390 --> 03:26:20,340
today that is our show this is the end

03:26:16,860 --> 03:26:23,790
of Mozilla Moss Fest 2019 Xavier Harding

03:26:20,340 --> 03:26:24,320
and see you guys soon yeah

03:26:23,790 --> 03:26:30,549
thanks everybody

03:26:24,320 --> 03:26:30,549

YouTube URL: https://www.youtube.com/watch?v=2FOru6UfuqE


