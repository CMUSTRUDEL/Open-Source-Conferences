Title: MozFest 2019 - On AI and Ethics
Publication date: 2019-11-04
Playlist: Mozilla Festival 2019
Description: 
	Cansu Canca is a philosopher and the founder/director of the AI Ethics Lab, where she leads teams of computer scientists, philosophers, and legal scholars to provide ethics analysis and guidance to researchers and practitioners.
Captions: 
	00:00:00,110 --> 00:00:05,339
so next up we have a talk from John Hsu

00:00:03,990 --> 00:00:07,529
John Joshi was one of the panelists

00:00:05,339 --> 00:00:10,050
yesterday she is the founder and

00:00:07,529 --> 00:00:12,030
director of the AI ethics lab and she

00:00:10,050 --> 00:00:13,340
also has a PhD in philosophy so I like

00:00:12,030 --> 00:00:23,160
to bring up John's to me right now you

00:00:13,340 --> 00:00:25,590
know 100 plus ok thanks so I will be

00:00:23,160 --> 00:00:28,710
talking about ethical AI which too

00:00:25,590 --> 00:00:30,420
quickly which is something that we are

00:00:28,710 --> 00:00:31,920
talking about so much recently that it

00:00:30,420 --> 00:00:32,610
seems like we've been talking about it

00:00:31,920 --> 00:00:34,620
for ages

00:00:32,610 --> 00:00:38,670
that's not really the case for the

00:00:34,620 --> 00:00:40,920
longest time ethics of AI or AI Attucks

00:00:38,670 --> 00:00:43,260
has been only something that is of

00:00:40,920 --> 00:00:45,149
interest of a handful of philosophers

00:00:43,260 --> 00:00:48,390
computer scientists and sci-fi authors

00:00:45,149 --> 00:00:50,340
basically only recently the public

00:00:48,390 --> 00:00:53,030
started caring about it and I can be

00:00:50,340 --> 00:00:56,250
even more specific so this is the

00:00:53,030 --> 00:00:59,039
worldwide Google search trends for the

00:00:56,250 --> 00:01:01,230
last past five years of the term AI

00:00:59,039 --> 00:01:04,650
ethics you can try different versions

00:01:01,230 --> 00:01:05,990
you know ethical AI ethics in AI and you

00:01:04,650 --> 00:01:10,500
will get more or less the same trend

00:01:05,990 --> 00:01:12,479
basically until 2016 when you set AI

00:01:10,500 --> 00:01:14,400
attics you would just get like plain

00:01:12,479 --> 00:01:16,830
blank stares people wouldn't know what

00:01:14,400 --> 00:01:21,450
you're talking about then things started

00:01:16,830 --> 00:01:24,960
changing in late 2017 and that is when

00:01:21,450 --> 00:01:27,090
we started having scandals and these are

00:01:24,960 --> 00:01:30,000
just a few of the major scandals there

00:01:27,090 --> 00:01:32,759
is a whole larger map out there if you

00:01:30,000 --> 00:01:35,369
search for it and the thing is these

00:01:32,759 --> 00:01:38,070
scandals started bringing home the

00:01:35,369 --> 00:01:41,520
message that technology an AI is not

00:01:38,070 --> 00:01:44,820
free from ethics it's not it's not

00:01:41,520 --> 00:01:46,890
ethics neutral on the contrary all of

00:01:44,820 --> 00:01:50,340
these ethical problems are really

00:01:46,890 --> 00:01:52,159
evolving into the phases of a building

00:01:50,340 --> 00:01:55,409
an AI systems are from their research to

00:01:52,159 --> 00:01:57,390
design development deployment updating

00:01:55,409 --> 00:01:59,880
marketing all of them have different

00:01:57,390 --> 00:02:03,600
ethical questions and some of these

00:01:59,880 --> 00:02:05,880
ethical scandals are about social media

00:02:03,600 --> 00:02:07,860
and its effects on politics democracy

00:02:05,880 --> 00:02:10,319
some of them are about premature

00:02:07,860 --> 00:02:11,730
collaborations between companies tech

00:02:10,319 --> 00:02:12,990
companies and militaries and law

00:02:11,730 --> 00:02:15,450
enforcement

00:02:12,990 --> 00:02:19,910
some about safety and security and

00:02:15,450 --> 00:02:22,920
privacy and how AI systems categorize us

00:02:19,910 --> 00:02:25,950
into you know who gets to get the job

00:02:22,920 --> 00:02:29,910
who gets the loan financial loan and who

00:02:25,950 --> 00:02:31,950
gets to the jail and I mean if you

00:02:29,910 --> 00:02:34,320
realize this is a problem for all of us

00:02:31,950 --> 00:02:36,630
so even if you are a developer or you

00:02:34,320 --> 00:02:38,610
are a lead at the tech company it's

00:02:36,630 --> 00:02:40,920
still you're not just the creator of AI

00:02:38,610 --> 00:02:43,230
systems you're always also subject to AI

00:02:40,920 --> 00:02:46,500
systems other AI systems that you are

00:02:43,230 --> 00:02:51,870
not creating so even if you are building

00:02:46,500 --> 00:02:53,970
them you are always being categorized by

00:02:51,870 --> 00:02:55,740
the systems that others are building so

00:02:53,970 --> 00:02:58,230
we have to we have to have some sort of

00:02:55,740 --> 00:03:01,800
solution to these problems and the

00:02:58,230 --> 00:03:04,410
solution has been coming in the form of

00:03:01,800 --> 00:03:07,340
so this is another timeline that sort of

00:03:04,410 --> 00:03:10,110
matches the last one that I showed

00:03:07,340 --> 00:03:12,150
basically three types of solutions have

00:03:10,110 --> 00:03:15,150
been proposed one is early-stage

00:03:12,150 --> 00:03:18,090
regulations in AI ethics

00:03:15,150 --> 00:03:19,950
the other one is AI boards so many

00:03:18,090 --> 00:03:22,440
companies are now building AI epic

00:03:19,950 --> 00:03:24,120
sports and the other one is AI

00:03:22,440 --> 00:03:26,430
principles a whole bunch of AI

00:03:24,120 --> 00:03:29,250
principles so we have right now a little

00:03:26,430 --> 00:03:32,930
over 80 sets of AI principles coming

00:03:29,250 --> 00:03:34,980
from private companies institutions

00:03:32,930 --> 00:03:39,120
international organizations governmental

00:03:34,980 --> 00:03:43,530
agencies and these all of these efforts

00:03:39,120 --> 00:03:47,640
we can sort of categorize them into two

00:03:43,530 --> 00:03:50,310
two approaches so one is enforcing

00:03:47,640 --> 00:03:53,070
ethics how do we enforce ethics and the

00:03:50,310 --> 00:03:55,140
other one is integrating ethics to

00:03:53,070 --> 00:03:58,470
enforce ethics we need external

00:03:55,140 --> 00:04:00,480
regulations so there is a very important

00:03:58,470 --> 00:04:02,490
function of the regulations and laws

00:04:00,480 --> 00:04:06,120
which is to make sure that we at least

00:04:02,490 --> 00:04:08,040
have the the boundaries of what is

00:04:06,120 --> 00:04:11,070
ethically acceptable and not acceptable

00:04:08,040 --> 00:04:13,440
so this is also very important because

00:04:11,070 --> 00:04:16,920
if you want certain companies or certain

00:04:13,440 --> 00:04:18,510
developers to care about ethics you also

00:04:16,920 --> 00:04:20,010
want to make sure that you don't force

00:04:18,510 --> 00:04:22,020
them to compete with those who don't

00:04:20,010 --> 00:04:26,590
care you want to make sure at least

00:04:22,020 --> 00:04:29,260
there is some sort of fair playing field

00:04:26,590 --> 00:04:31,540
the problem with the regulations is that

00:04:29,260 --> 00:04:33,340
they are they can never be very detailed

00:04:31,540 --> 00:04:35,260
so they are not going to answer your

00:04:33,340 --> 00:04:39,010
everyday ethics questions they are just

00:04:35,260 --> 00:04:41,710
going to be general guidelines and when

00:04:39,010 --> 00:04:44,139
you think about if you take an example

00:04:41,710 --> 00:04:45,940
of how the ethical questions arise while

00:04:44,139 --> 00:04:47,410
you're developing an AI system you

00:04:45,940 --> 00:04:49,330
really realize that yes there is a

00:04:47,410 --> 00:04:51,729
there's a very important function of

00:04:49,330 --> 00:04:54,400
regulations but it's just not not enough

00:04:51,729 --> 00:04:57,880
so think about for example developing a

00:04:54,400 --> 00:05:01,600
personalized AI for personalized system

00:04:57,880 --> 00:05:04,810
for advice in healthy living so it's

00:05:01,600 --> 00:05:06,310
sort of like an AI health coach there

00:05:04,810 --> 00:05:08,680
will be as you are developing something

00:05:06,310 --> 00:05:10,360
like this and this is an example by the

00:05:08,680 --> 00:05:12,729
way that was brought to us so that we

00:05:10,360 --> 00:05:14,410
work on the ethical issues of this so it

00:05:12,729 --> 00:05:17,139
was it's not a hypothetical completely

00:05:14,410 --> 00:05:20,080
speaking so there will be many ethically

00:05:17,139 --> 00:05:21,669
loaded questions so what data are you

00:05:20,080 --> 00:05:24,760
gonna so think about it like a variable

00:05:21,669 --> 00:05:27,639
it's gonna basically understand your

00:05:24,760 --> 00:05:31,660
health and we'll give you the right

00:05:27,639 --> 00:05:33,160
health advice healthy living advice so

00:05:31,660 --> 00:05:34,150
there will be many questions that you

00:05:33,160 --> 00:05:35,740
have to answer as you're building

00:05:34,150 --> 00:05:37,600
there's many ethically loaded questions

00:05:35,740 --> 00:05:40,950
what data are you gonna collect are you

00:05:37,600 --> 00:05:44,700
going to only track the user's physical

00:05:40,950 --> 00:05:47,440
data like like a Fitbit or are you gonna

00:05:44,700 --> 00:05:49,570
log into their Instagram are you gonna

00:05:47,440 --> 00:05:52,450
have access to their Instagram to check

00:05:49,570 --> 00:05:54,340
their mood or how much do they party are

00:05:52,450 --> 00:05:55,660
you going to log into their Alexa to see

00:05:54,340 --> 00:05:57,639
whether they have a peaceful home

00:05:55,660 --> 00:05:59,620
environment or not they would be very

00:05:57,639 --> 00:06:02,320
useful information for healthy living

00:05:59,620 --> 00:06:04,720
for your advice but is it are these

00:06:02,320 --> 00:06:06,729
ethical options who do you share this

00:06:04,720 --> 00:06:09,160
data with whatever you are collecting

00:06:06,729 --> 00:06:11,050
and how do you design a consent form for

00:06:09,160 --> 00:06:13,240
the user to understand what they are

00:06:11,050 --> 00:06:15,099
sharing with whom they are sharing and

00:06:13,240 --> 00:06:19,150
what are they getting out of it what are

00:06:15,099 --> 00:06:21,370
they receiving in return oh and and and

00:06:19,150 --> 00:06:23,260
of course other connected questions to

00:06:21,370 --> 00:06:26,979
these right like are there safeguards

00:06:23,260 --> 00:06:29,169
for insurance companies not to later

00:06:26,979 --> 00:06:31,389
punish the user for not taking the

00:06:29,169 --> 00:06:33,669
advice from the AI health coach so there

00:06:31,389 --> 00:06:35,380
are also connected questions within the

00:06:33,669 --> 00:06:36,849
domain you're operating in if it is

00:06:35,380 --> 00:06:38,500
healthcare you will have all the

00:06:36,849 --> 00:06:41,469
healthcare related questions that's

00:06:38,500 --> 00:06:43,510
there so these are complex questions and

00:06:41,469 --> 00:06:46,000
these are not questions that just simple

00:06:43,510 --> 00:06:48,820
regulations were simple just regulations

00:06:46,000 --> 00:06:52,960
general regulations even if they are not

00:06:48,820 --> 00:06:55,840
simple can address and they have to they

00:06:52,960 --> 00:06:58,479
are internal questions so like nothing

00:06:55,840 --> 00:07:00,729
it shouldn't be answered by external

00:06:58,479 --> 00:07:04,419
buddies but they should be answered

00:07:00,729 --> 00:07:06,820
internally and we do seem to have going

00:07:04,419 --> 00:07:09,940
to that direction of this internal

00:07:06,820 --> 00:07:14,530
integration of ethics into the process

00:07:09,940 --> 00:07:17,140
for building AI systems by these AI

00:07:14,530 --> 00:07:19,419
principles that are proposed by the

00:07:17,140 --> 00:07:24,130
companies and the AI ethics boards

00:07:19,419 --> 00:07:26,050
within the companies so this is actually

00:07:24,130 --> 00:07:28,120
an oversight and compliance model the

00:07:26,050 --> 00:07:32,169
principals and the boards and we have

00:07:28,120 --> 00:07:33,729
this model for the last 40 years from

00:07:32,169 --> 00:07:35,469
research ethics so this is not a new

00:07:33,729 --> 00:07:38,110
model at all you have the research

00:07:35,469 --> 00:07:39,340
ethics boards those of you who are doing

00:07:38,110 --> 00:07:41,349
research in academia you are familiar

00:07:39,340 --> 00:07:43,479
with that institutional review boards

00:07:41,349 --> 00:07:46,450
IRPs or research ethics committees in

00:07:43,479 --> 00:07:51,340
the Europe regs and they use principles

00:07:46,450 --> 00:07:52,870
to evaluate research and they are still

00:07:51,340 --> 00:07:54,580
trying to apply these general principles

00:07:52,870 --> 00:07:57,550
there are just four principles actually

00:07:54,580 --> 00:08:01,900
that they are trying to apply and the

00:07:57,550 --> 00:08:03,639
problem with this approach is that if

00:08:01,900 --> 00:08:06,159
you realize it's still not really

00:08:03,639 --> 00:08:08,289
integrating its within the company but

00:08:06,159 --> 00:08:10,090
it is still a board that is sitting

00:08:08,289 --> 00:08:12,820
distant from the developers or the

00:08:10,090 --> 00:08:15,340
project teams and that has this also add

00:08:12,820 --> 00:08:17,620
like a hierarchical level that approves

00:08:15,340 --> 00:08:22,690
your research or does not approve your

00:08:17,620 --> 00:08:28,719
research so it's not exactly integration

00:08:22,690 --> 00:08:30,729
we are talking about and also most of

00:08:28,719 --> 00:08:33,760
the time these boards either don't have

00:08:30,729 --> 00:08:36,130
the right expertise both for the science

00:08:33,760 --> 00:08:37,839
of the whatever you are doing and the

00:08:36,130 --> 00:08:39,490
ethics of whatever you're doing and they

00:08:37,839 --> 00:08:40,719
certainly don't have enough time even if

00:08:39,490 --> 00:08:42,039
they have the expertise because we

00:08:40,719 --> 00:08:43,839
imagine all these question being

00:08:42,039 --> 00:08:45,760
questions being posed by every single

00:08:43,839 --> 00:08:49,580
project team we are talking about a lot

00:08:45,760 --> 00:08:52,070
of questions so

00:08:49,580 --> 00:08:53,930
we are in the nonsense we are still

00:08:52,070 --> 00:08:56,089
talking about enforcement of ethics with

00:08:53,930 --> 00:08:57,620
these boards and principals it's just a

00:08:56,089 --> 00:08:59,480
different level of enforcement it

00:08:57,620 --> 00:09:01,430
doesn't come from the regulatory level

00:08:59,480 --> 00:09:03,410
like the external level it's within the

00:09:01,430 --> 00:09:06,620
company but it's still enforcing ethics

00:09:03,410 --> 00:09:10,750
and it seems to this whole enforcing

00:09:06,620 --> 00:09:13,519
ethics while it's important seems to

00:09:10,750 --> 00:09:16,550
well it's important if it's crafted well

00:09:13,519 --> 00:09:19,430
let's put it that way seems to miss the

00:09:16,550 --> 00:09:21,260
main question which is how do you solve

00:09:19,430 --> 00:09:26,480
ethical problems as you are developing

00:09:21,260 --> 00:09:29,420
AI systems so in other words how should

00:09:26,480 --> 00:09:32,390
we really integrate ethics into building

00:09:29,420 --> 00:09:35,029
and developing AI systems not enforcing

00:09:32,390 --> 00:09:37,790
but integrating it so before I get to

00:09:35,029 --> 00:09:40,640
that though I think I should talk about

00:09:37,790 --> 00:09:43,640
why should we care about this because

00:09:40,640 --> 00:09:46,640
some are Philosopher's it's a view I

00:09:43,640 --> 00:09:48,440
said and we have a very large literature

00:09:46,640 --> 00:09:50,510
we are not fooling ourselves we have a

00:09:48,440 --> 00:09:52,339
very large literature about how epical

00:09:50,510 --> 00:09:54,860
reasoning is great but it doesn't

00:09:52,339 --> 00:09:56,870
motivate people to do the right thing so

00:09:54,860 --> 00:09:58,070
I'm gonna try to give you a couple more

00:09:56,870 --> 00:09:59,390
motivations if you're already motivated

00:09:58,070 --> 00:10:03,020
because it's the right thing that's

00:09:59,390 --> 00:10:05,420
great but if you're not then you don't

00:10:03,020 --> 00:10:07,850
where as a company or as a developer as

00:10:05,420 --> 00:10:09,709
practitioners basically when you're not

00:10:07,850 --> 00:10:11,390
endorsing ethics basically you are

00:10:09,709 --> 00:10:13,579
taking a risk and it's quite a high

00:10:11,390 --> 00:10:16,670
stake it's because what you're doing is

00:10:13,579 --> 00:10:18,890
that every time something goes wrong

00:10:16,670 --> 00:10:20,360
every time there's a scandal basically

00:10:18,890 --> 00:10:22,550
you are giving the impression that you

00:10:20,360 --> 00:10:25,190
are the wild ones that the society has

00:10:22,550 --> 00:10:26,930
to control which means heavy regulations

00:10:25,190 --> 00:10:29,029
are going to come to your way and which

00:10:26,930 --> 00:10:31,220
also means if you if there if the

00:10:29,029 --> 00:10:32,180
scandals keep coming they're going to

00:10:31,220 --> 00:10:34,279
come really fast

00:10:32,180 --> 00:10:36,529
also these regulations will not even be

00:10:34,279 --> 00:10:38,779
good regulations which is terrible for

00:10:36,529 --> 00:10:40,339
everyone because it's bad for the

00:10:38,779 --> 00:10:42,560
businesses there won't be enough space

00:10:40,339 --> 00:10:44,510
for innovation it's bad for the society

00:10:42,560 --> 00:10:46,850
because innovation helps things as long

00:10:44,510 --> 00:10:48,079
as the innovation is responsible it's a

00:10:46,850 --> 00:10:51,140
good thing we want to have the

00:10:48,079 --> 00:10:53,269
innovation going on so this is at least

00:10:51,140 --> 00:10:56,779
one more reason why you should care so

00:10:53,269 --> 00:10:59,570
then how do we integrate ethics so if

00:10:56,779 --> 00:11:01,760
you want to solve ethical questions then

00:10:59,570 --> 00:11:03,800
we have to think of them now

00:11:01,760 --> 00:11:05,480
just like policing ethics but we think

00:11:03,800 --> 00:11:08,839
we need to think of them as like puzzle

00:11:05,480 --> 00:11:10,370
solving of course ethical questions are

00:11:08,839 --> 00:11:13,250
not this simple so they are like this

00:11:10,370 --> 00:11:15,949
and there's like very complex moving

00:11:13,250 --> 00:11:17,300
parts at all times that you have to deal

00:11:15,949 --> 00:11:19,430
with about you know individual

00:11:17,300 --> 00:11:22,579
well-being individual autonomy societal

00:11:19,430 --> 00:11:25,339
well-being social justice harm but you

00:11:22,579 --> 00:11:26,959
have to consider them and you have to

00:11:25,339 --> 00:11:28,250
consider them at each stage as I said

00:11:26,959 --> 00:11:31,329
you know from the beginning

00:11:28,250 --> 00:11:33,560
research development design deployment

00:11:31,329 --> 00:11:38,139
updating all of these have unique

00:11:33,560 --> 00:11:42,680
ethical questions so what's the model um

00:11:38,139 --> 00:11:44,630
one thing that we we need to do is to

00:11:42,680 --> 00:11:46,910
catch these ethical problems from the

00:11:44,630 --> 00:11:48,949
very beginning as soon as we can before

00:11:46,910 --> 00:11:51,680
they become bigger problems and if

00:11:48,949 --> 00:11:53,750
possible just eliminate them and for

00:11:51,680 --> 00:11:55,970
that we need the people who are actually

00:11:53,750 --> 00:11:57,980
building AI systems the developers and

00:11:55,970 --> 00:12:00,290
the researchers to be aware of the

00:11:57,980 --> 00:12:03,260
relevant ethical issues and to have some

00:12:00,290 --> 00:12:05,300
sort of tool to deal with these ethical

00:12:03,260 --> 00:12:08,079
issues so if these questions are not

00:12:05,300 --> 00:12:10,639
very complex then we can do that by

00:12:08,079 --> 00:12:12,889
training the developers and the

00:12:10,639 --> 00:12:16,730
researchers basically add another tool

00:12:12,889 --> 00:12:19,370
for their toolbox to deal with these

00:12:16,730 --> 00:12:21,680
ethical issues by deal with them I mean

00:12:19,370 --> 00:12:24,500
either if it is as I said simple solve

00:12:21,680 --> 00:12:27,550
them if not flag them just flag them and

00:12:24,500 --> 00:12:30,440
demand solutions for them the second

00:12:27,550 --> 00:12:34,790
aspect is when the question is really

00:12:30,440 --> 00:12:38,920
complex obviously we don't want we don't

00:12:34,790 --> 00:12:44,510
want to spend all the energy of people

00:12:38,920 --> 00:12:47,360
whose task is to build these tools to

00:12:44,510 --> 00:12:49,730
dive into the deep flow so fickle

00:12:47,360 --> 00:12:51,170
literature but there is a deep

00:12:49,730 --> 00:12:52,790
philosophical literature and applied

00:12:51,170 --> 00:12:55,760
ethics has been around for a very long

00:12:52,790 --> 00:13:00,459
time so we have a lot of knowledge

00:12:55,760 --> 00:13:02,630
around applied ethics and using ethical

00:13:00,459 --> 00:13:04,910
problem-solving in practice we've been

00:13:02,630 --> 00:13:07,310
using it in medicine in public health in

00:13:04,910 --> 00:13:08,899
environmental questions in businesses

00:13:07,310 --> 00:13:10,130
we've been using applied ethics so it's

00:13:08,899 --> 00:13:12,649
not like we are not reinventing the

00:13:10,130 --> 00:13:14,839
wheel here so what you can do is to

00:13:12,649 --> 00:13:15,100
analyze the project if it is a complex

00:13:14,839 --> 00:13:17,890
four

00:13:15,100 --> 00:13:19,930
Danya analyzed it the domain if it is in

00:13:17,890 --> 00:13:21,640
healthcare again think about the AI

00:13:19,930 --> 00:13:24,160
health coaches that I just mentioned

00:13:21,640 --> 00:13:26,470
well you have to understand the ethical

00:13:24,160 --> 00:13:28,180
questions around health care system and

00:13:26,470 --> 00:13:30,010
you have to understand the questions

00:13:28,180 --> 00:13:33,010
about around variables the technology

00:13:30,010 --> 00:13:36,040
itself and you need to figure out a set

00:13:33,010 --> 00:13:39,730
of right answers yes a lot of the times

00:13:36,040 --> 00:13:41,830
the there is no single right answer when

00:13:39,730 --> 00:13:43,030
the question is complex enough we won't

00:13:41,830 --> 00:13:45,850
have a single right answer

00:13:43,030 --> 00:13:48,220
but we can eliminate it to a couple of

00:13:45,850 --> 00:13:50,920
right answers and that if you could just

00:13:48,220 --> 00:13:53,290
choose from that set of right answers

00:13:50,920 --> 00:13:57,010
you're really really good we are so much

00:13:53,290 --> 00:13:59,140
better off and finally certain complex

00:13:57,010 --> 00:14:01,930
quest questions will keep appearing over

00:13:59,140 --> 00:14:03,490
and over again and for that we don't

00:14:01,930 --> 00:14:06,010
want to rediscover the whole thing again

00:14:03,490 --> 00:14:07,570
we don't want to do analysis again from

00:14:06,010 --> 00:14:10,090
the very beginning what we want to do is

00:14:07,570 --> 00:14:13,210
slowly set a strategy for the company or

00:14:10,090 --> 00:14:15,280
the institution so basically figure out

00:14:13,210 --> 00:14:17,290
you know those ad sets of principles

00:14:15,280 --> 00:14:19,480
they'll figure out what they really want

00:14:17,290 --> 00:14:22,300
them to say but what do these companies

00:14:19,480 --> 00:14:23,920
want to say with those principles a lot

00:14:22,300 --> 00:14:26,740
of the times principals will say things

00:14:23,920 --> 00:14:29,340
that are very charming like being

00:14:26,740 --> 00:14:32,740
socially beneficial and protect

00:14:29,340 --> 00:14:34,990
individual autonomy and privacy and you

00:14:32,740 --> 00:14:36,550
will realize that well they clash a lot

00:14:34,990 --> 00:14:39,550
of the times that's what a complex

00:14:36,550 --> 00:14:41,260
ethical questions they clash so how are

00:14:39,550 --> 00:14:43,450
you gonna deal with them as a company

00:14:41,260 --> 00:14:47,520
what is your agenda what is your

00:14:43,450 --> 00:14:49,270
strategy ethical strategy and so

00:14:47,520 --> 00:14:51,310
operationalizing these principles

00:14:49,270 --> 00:14:54,250
putting processes around them and

00:14:51,310 --> 00:14:56,110
setting precedents what did you decide

00:14:54,250 --> 00:14:58,060
last time you had a similar question and

00:14:56,110 --> 00:15:01,740
how are you going to use that knowledge

00:14:58,060 --> 00:15:06,130
now with this next question on the table

00:15:01,740 --> 00:15:08,170
so these three steps how are we where

00:15:06,130 --> 00:15:10,210
are we actually doing this where can we

00:15:08,170 --> 00:15:13,110
do this these well obviously in the

00:15:10,210 --> 00:15:17,410
companies so any company that has some

00:15:13,110 --> 00:15:22,870
resources to devote to ethical analysis

00:15:17,410 --> 00:15:26,650
can look into these integrating these

00:15:22,870 --> 00:15:28,139
into their build into the processes in

00:15:26,650 --> 00:15:30,550
building AI systems

00:15:28,139 --> 00:15:35,019
the other one is incubators and

00:15:30,550 --> 00:15:36,670
accelerators startups are usually quite

00:15:35,019 --> 00:15:38,529
concerned about the ethical aspects of

00:15:36,670 --> 00:15:39,790
the products that they are building but

00:15:38,529 --> 00:15:42,519
they don't have the resources and that's

00:15:39,790 --> 00:15:43,810
very understandable so the incubators

00:15:42,519 --> 00:15:45,250
that they are located in and the

00:15:43,810 --> 00:15:47,259
accelerators that they are located in

00:15:45,250 --> 00:15:49,930
are the right places where these

00:15:47,259 --> 00:15:53,920
services these information can be

00:15:49,930 --> 00:15:56,470
offered them and finally the research

00:15:53,920 --> 00:16:01,600
centers so academic institutions are as

00:15:56,470 --> 00:16:05,399
lost as the private companies so you

00:16:01,600 --> 00:16:08,259
still have need these steps to be

00:16:05,399 --> 00:16:10,060
integrated into the research and

00:16:08,259 --> 00:16:12,040
development process within the academic

00:16:10,060 --> 00:16:15,189
centers within the research institutions

00:16:12,040 --> 00:16:17,889
and the demand for this should come from

00:16:15,189 --> 00:16:21,699
companies and startups who understand

00:16:17,889 --> 00:16:25,120
that well hopefully you understand that

00:16:21,699 --> 00:16:28,990
ethics matters but if not that business

00:16:25,120 --> 00:16:31,720
risk matters developers who care about

00:16:28,990 --> 00:16:33,490
really just developing good technology

00:16:31,720 --> 00:16:35,589
because when your technology is making

00:16:33,490 --> 00:16:38,259
huge ethical mistakes check usually

00:16:35,589 --> 00:16:41,259
they're also wrong if you are trying to

00:16:38,259 --> 00:16:44,529
have the best algorithm for hiring

00:16:41,259 --> 00:16:46,959
decisions and you because of biases you

00:16:44,529 --> 00:16:48,459
discard a whole bunch of women you your

00:16:46,959 --> 00:16:51,009
algorithm is actually not getting the

00:16:48,459 --> 00:16:53,439
right people so it is an enhancement of

00:16:51,009 --> 00:16:57,189
the technology if you get the ethics

00:16:53,439 --> 00:17:01,480
right and as developers I my senses we

00:16:57,189 --> 00:17:04,390
care about this and investors who are

00:17:01,480 --> 00:17:06,189
investing into these companies because

00:17:04,390 --> 00:17:09,549
they are also taking a risk business

00:17:06,189 --> 00:17:14,350
risk so basically what we need to be

00:17:09,549 --> 00:17:18,069
doing is moving from this this model of

00:17:14,350 --> 00:17:21,640
ethics as policing which puts off a lot

00:17:18,069 --> 00:17:23,709
of the practitioners as well as trust me

00:17:21,640 --> 00:17:26,679
a lot of the philosophers but really

00:17:23,709 --> 00:17:29,320
going to puzzle solving and to do that

00:17:26,679 --> 00:17:31,960
using the knowledge that comes from

00:17:29,320 --> 00:17:34,360
ethics together with the knowledge that

00:17:31,960 --> 00:17:37,799
comes from AI and hence the AI ethics

00:17:34,360 --> 00:17:37,799
that's all thank you

00:17:41,789 --> 00:17:46,269
now all right all right now we'll take

00:17:44,649 --> 00:17:47,440
three questions from the audience if

00:17:46,269 --> 00:17:51,399
there's anyone who wants to ask a

00:17:47,440 --> 00:17:56,370
question John Hsu now is your time we

00:17:51,399 --> 00:17:56,370
have one over here sorry once again

00:18:04,500 --> 00:18:10,629
hi I'm so you talked about startups and

00:18:08,830 --> 00:18:12,549
how they can you know the incubators are

00:18:10,629 --> 00:18:13,899
a great place to for them to access

00:18:12,549 --> 00:18:17,620
information about ethics and how to

00:18:13,899 --> 00:18:19,210
integrate it and I mean yeah I

00:18:17,620 --> 00:18:20,980
completely I've worked with quite a lot

00:18:19,210 --> 00:18:22,389
of startup founders who want to you know

00:18:20,980 --> 00:18:24,820
kind of they've seen they've spotted a

00:18:22,389 --> 00:18:26,379
gap or they've seen what some some thing

00:18:24,820 --> 00:18:28,120
that they can that can be fixed and

00:18:26,379 --> 00:18:30,759
often there's an ethical reason for that

00:18:28,120 --> 00:18:33,580
but what happens when investment starts

00:18:30,759 --> 00:18:35,860
coming in so I think in you know kind of

00:18:33,580 --> 00:18:37,899
in my experience it often becomes more

00:18:35,860 --> 00:18:39,460
difficult for start startup founders to

00:18:37,899 --> 00:18:41,710
kind of keep hold of that original kind

00:18:39,460 --> 00:18:44,769
of those original values once

00:18:41,710 --> 00:18:46,690
investment comes in yeah I mean

00:18:44,769 --> 00:18:49,149
basically you're right and what you're

00:18:46,690 --> 00:18:51,549
pointing to is that as soon as there are

00:18:49,149 --> 00:18:53,379
parties involved that don't care about

00:18:51,549 --> 00:18:56,350
the ethics of whatever you're doing

00:18:53,379 --> 00:18:59,440
there is tension and then you have to

00:18:56,350 --> 00:19:01,120
make the decision well I really want to

00:18:59,440 --> 00:19:04,870
get this stored up going should I just

00:19:01,120 --> 00:19:07,000
just this once not do this the good

00:19:04,870 --> 00:19:08,620
thing is that the investors care more

00:19:07,000 --> 00:19:10,960
and more in my experience I mean you

00:19:08,620 --> 00:19:12,549
know this was again and absolutely not

00:19:10,960 --> 00:19:14,080
talked about topic just two years ago

00:19:12,549 --> 00:19:16,419
when you talk to investors and now they

00:19:14,080 --> 00:19:18,279
are they do care a lot of the times

00:19:16,419 --> 00:19:20,950
especially when investing into into the

00:19:18,279 --> 00:19:22,899
startups because they don't yet have the

00:19:20,950 --> 00:19:27,190
commitment right like they can now think

00:19:22,899 --> 00:19:29,919
about it and hopefully getting damn care

00:19:27,190 --> 00:19:32,049
is how you can and if it is not I mean

00:19:29,919 --> 00:19:34,929
you're really you are really looking at

00:19:32,049 --> 00:19:39,519
the ethics work versus the market pull

00:19:34,929 --> 00:19:41,710
which is the problem but it is also as

00:19:39,519 --> 00:19:43,570
the culture is slowly changing it is

00:19:41,710 --> 00:19:47,559
also a problem that there is hope that

00:19:43,570 --> 00:19:48,929
it can align somehow I have a question

00:19:47,559 --> 00:19:50,020
back there

00:19:48,929 --> 00:19:53,710
my

00:19:50,020 --> 00:19:55,570
coming around right now hi my name is

00:19:53,710 --> 00:19:58,180
funny thank you very much for the

00:19:55,570 --> 00:20:00,910
presentation I would like to ask you to

00:19:58,180 --> 00:20:03,040
give me one example to apply all that

00:20:00,910 --> 00:20:05,320
model and and your presentation to an

00:20:03,040 --> 00:20:07,020
ethical problem and if you could tell me

00:20:05,320 --> 00:20:09,910
a little bit about how to distinguish

00:20:07,020 --> 00:20:12,070
your solution with ethics as opposed to

00:20:09,910 --> 00:20:12,430
a legal solution to that problem thank

00:20:12,070 --> 00:20:15,160
you

00:20:12,430 --> 00:20:17,280
sure so I can give you the example that

00:20:15,160 --> 00:20:20,650
I just gave in the so to keep it short

00:20:17,280 --> 00:20:24,960
the that I explained that I gave in the

00:20:20,650 --> 00:20:28,720
in the talk building AI health coaches

00:20:24,960 --> 00:20:31,390
the questions are so legally speaking a

00:20:28,720 --> 00:20:37,570
consent form for AI something like that

00:20:31,390 --> 00:20:39,250
is very non functional dysfunctional you

00:20:37,570 --> 00:20:40,570
basically have terms of services you

00:20:39,250 --> 00:20:42,220
scroll through you accept no one

00:20:40,570 --> 00:20:45,580
everyone knows you're not reading it and

00:20:42,220 --> 00:20:47,620
it's not a big legal issue we there is a

00:20:45,580 --> 00:20:49,990
debate but there is no enforcement of it

00:20:47,620 --> 00:20:52,330
it is an ethical issue and we are all

00:20:49,990 --> 00:20:55,570
aware of that so one way that it

00:20:52,330 --> 00:20:57,790
diverges is like ethically speaking

00:20:55,570 --> 00:20:59,560
terms and conditions' mean absolutely

00:20:57,790 --> 00:21:02,140
nothing if we know that you're not

00:20:59,560 --> 00:21:04,960
reading it because what it should mean

00:21:02,140 --> 00:21:06,760
is informed consent you know there is a

00:21:04,960 --> 00:21:08,470
reason why we call it informed consent

00:21:06,760 --> 00:21:10,030
and there is no information provided the

00:21:08,470 --> 00:21:12,970
information is not provided in the way

00:21:10,030 --> 00:21:14,890
that a person will have the time to read

00:21:12,970 --> 00:21:16,960
it will have the energy to read and it's

00:21:14,890 --> 00:21:19,150
updated all the time so everything

00:21:16,960 --> 00:21:21,280
around that for example is a major

00:21:19,150 --> 00:21:23,290
ethical problem it's not yet a legal

00:21:21,280 --> 00:21:27,460
problem the good thing about ethics and

00:21:23,290 --> 00:21:28,780
legal too often aligning is that when

00:21:27,460 --> 00:21:32,170
the conversation starts somewhere

00:21:28,780 --> 00:21:36,280
usually in ethics that informs the rules

00:21:32,170 --> 00:21:38,590
regulations likely so that they start

00:21:36,280 --> 00:21:41,080
reinforced there is a good feedback loop

00:21:38,590 --> 00:21:43,660
between the two but if something that

00:21:41,080 --> 00:21:46,570
you kept saying as a again going back to

00:21:43,660 --> 00:21:49,660
another field just just medical health

00:21:46,570 --> 00:21:51,760
care something that was a problem the

00:21:49,660 --> 00:21:54,250
informed consent just ethically speaking

00:21:51,760 --> 00:21:56,440
became a legal rule you have to have

00:21:54,250 --> 00:21:59,170
this implemented and implemented

00:21:56,440 --> 00:22:02,200
properly at this point so I would say

00:21:59,170 --> 00:22:03,430
this is one like really simple example

00:22:02,200 --> 00:22:06,370
where

00:22:03,430 --> 00:22:08,740
Changez and how it would happen well you

00:22:06,370 --> 00:22:11,050
know the the whole constant structure

00:22:08,740 --> 00:22:12,790
around this particular product has to be

00:22:11,050 --> 00:22:15,640
designed while you are designing the

00:22:12,790 --> 00:22:18,940
before you basically pull it up push it

00:22:15,640 --> 00:22:22,000
out it's time for one more question in

00:22:18,940 --> 00:22:23,770
case there was somebody who wants to we

00:22:22,000 --> 00:22:28,870
have one question right here this is

00:22:23,770 --> 00:22:32,070
Mike right behind you hi thank you so

00:22:28,870 --> 00:22:34,330
much for your talk I think the

00:22:32,070 --> 00:22:37,600
integration point is a really really

00:22:34,330 --> 00:22:42,250
powerful one and I'm curious if you've

00:22:37,600 --> 00:22:44,650
seen examples of this especially and and

00:22:42,250 --> 00:22:48,070
ones that are really functional because

00:22:44,650 --> 00:22:49,990
from what I've observed engineers by

00:22:48,070 --> 00:22:51,820
training don't have necessarily all of

00:22:49,990 --> 00:22:54,010
the domain knowledge necessary to do

00:22:51,820 --> 00:22:56,020
that puzzle solving oh yeah absolutely

00:22:54,010 --> 00:22:57,910
well and so how do you bring the other

00:22:56,020 --> 00:23:00,130
relevant experts to the table and have

00:22:57,910 --> 00:23:01,930
you seen that working and how did that

00:23:00,130 --> 00:23:03,670
come to be because it doesn't seem to me

00:23:01,930 --> 00:23:05,260
to be the way that most tech companies

00:23:03,670 --> 00:23:09,190
are structured today you're absolutely

00:23:05,260 --> 00:23:11,140
right basically everything right now is

00:23:09,190 --> 00:23:13,780
still in the experimental phase so we

00:23:11,140 --> 00:23:16,450
went from like zero concern to real

00:23:13,780 --> 00:23:19,180
concern and now there is experiment like

00:23:16,450 --> 00:23:21,490
all the companies are experimenting with

00:23:19,180 --> 00:23:22,960
different models of integration and as I

00:23:21,490 --> 00:23:26,500
said over that compliance and oversight

00:23:22,960 --> 00:23:29,260
model is sort of taking hold of the

00:23:26,500 --> 00:23:32,470
situation but companies are also doing

00:23:29,260 --> 00:23:36,040
seminars and trainings they are having

00:23:32,470 --> 00:23:37,990
boards that are more functional the way

00:23:36,040 --> 00:23:39,940
by the way I think yes very important

00:23:37,990 --> 00:23:42,220
point you make is that no we I don't

00:23:39,940 --> 00:23:44,170
expect engineers to now ditch

00:23:42,220 --> 00:23:45,880
engineering go into philosophy solved

00:23:44,170 --> 00:23:48,220
all the ethical problems because I mean

00:23:45,880 --> 00:23:49,900
there's a division of labor right so

00:23:48,220 --> 00:23:52,510
philosophers are the ones who deal with

00:23:49,900 --> 00:23:55,660
the ethical questions and if you need

00:23:52,510 --> 00:23:56,920
like a field information well sociology

00:23:55,660 --> 00:24:00,210
people which would be the ones that you

00:23:56,920 --> 00:24:03,250
should go for so we have the the right

00:24:00,210 --> 00:24:05,080
training already and we have these

00:24:03,250 --> 00:24:06,850
trainings come to make together in

00:24:05,080 --> 00:24:09,910
different fields where we decided to

00:24:06,850 --> 00:24:12,340
care about ethics already so again I

00:24:09,910 --> 00:24:14,440
talked about the healthcare example a

00:24:12,340 --> 00:24:16,270
lot because health care is an area where

00:24:14,440 --> 00:24:17,869
we managed to get ethics really

00:24:16,270 --> 00:24:20,059
integrated

00:24:17,869 --> 00:24:22,609
not perfect but the best that we have

00:24:20,059 --> 00:24:25,489
and there you do have philosophers

00:24:22,609 --> 00:24:27,589
collaborate with public health people to

00:24:25,489 --> 00:24:29,299
decide you know how to set the

00:24:27,589 --> 00:24:31,039
guidelines how to set their agendas and

00:24:29,299 --> 00:24:32,749
in particular cases you know in a

00:24:31,039 --> 00:24:35,509
hospital what are you going to do about

00:24:32,749 --> 00:24:37,659
this particular case you don't tell the

00:24:35,509 --> 00:24:40,009
surgeon okay now take three days off

00:24:37,659 --> 00:24:42,289
figure everything out yourself no

00:24:40,009 --> 00:24:45,079
because that would be a waste of the

00:24:42,289 --> 00:24:46,789
surgeons time in this particular

00:24:45,079 --> 00:24:49,609
question whereas you have people who are

00:24:46,789 --> 00:24:51,859
actually skilled in this yes alright

00:24:49,609 --> 00:24:52,110
well that's it thank you so much a great

00:24:51,859 --> 00:24:55,860
talk

00:24:52,110 --> 00:24:55,860

YouTube URL: https://www.youtube.com/watch?v=tulYK6szTBs


