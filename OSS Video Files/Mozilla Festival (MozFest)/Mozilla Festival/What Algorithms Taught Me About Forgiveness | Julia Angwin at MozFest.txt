Title: What Algorithms Taught Me About Forgiveness | Julia Angwin at MozFest
Publication date: 2020-09-29
Playlist: Mozilla Festival
Description: 
	“Our society hands out forgiveness and punishment unequally.”

ProPublica journalist Julia Angwin (https://twitter.com/juliaangwin) talks about algorithmic bias in the criminal justice system.

MozFest is the world's leading festival for the open internet movement. Our speakers address topics like privacy, online harassment, and digital inclusion.

https://mozillafestival.org | https://twitter.com/mozillafestival
Captions: 
	00:00:00,030 --> 00:00:04,920
okay so last up in this segment is a

00:00:02,580 --> 00:00:06,029
journalist here with ProPublica she was

00:00:04,920 --> 00:00:08,910
at the Wall Street Journal for several

00:00:06,029 --> 00:00:12,660
years for 13 years I think Julia Angwin

00:00:08,910 --> 00:00:20,970
over to you Julia her title 25 forgive

00:00:12,660 --> 00:00:23,550
me hi there who I'm blinded so sorry if

00:00:20,970 --> 00:00:26,099
I squint - the whole time I am going to

00:00:23,550 --> 00:00:27,510
talk about something that doesn't sound

00:00:26,099 --> 00:00:31,830
like it should be at a tech conference

00:00:27,510 --> 00:00:33,870
but but it's so I want to tell you about

00:00:31,830 --> 00:00:36,300
what I've learned about forgiveness in

00:00:33,870 --> 00:00:38,280
my studies of algorithmic accountability

00:00:36,300 --> 00:00:41,640
but I'm gonna start with a little bit of

00:00:38,280 --> 00:00:43,140
just a small history of me and my own

00:00:41,640 --> 00:00:49,020
experience that I think will be relevant

00:00:43,140 --> 00:00:53,370
so this is me I grew up in the Silicon

00:00:49,020 --> 00:00:55,289
Valley Palo Alto in a time when the

00:00:53,370 --> 00:00:58,859
personal computer was really exciting

00:00:55,289 --> 00:01:00,690
this is my first computer and I thought

00:00:58,859 --> 00:01:02,250
I would be a programmer when I grew up

00:01:00,690 --> 00:01:03,690
actually didn't know there were any

00:01:02,250 --> 00:01:05,430
choices other than hardware or software

00:01:03,690 --> 00:01:07,920
I thought like you pick one and then

00:01:05,430 --> 00:01:10,650
you're cool and I was like software seem

00:01:07,920 --> 00:01:15,750
more interesting so I was studying to do

00:01:10,650 --> 00:01:17,400
that and I took a wrong turn somehow and

00:01:15,750 --> 00:01:19,920
I fell in love with journals in my

00:01:17,400 --> 00:01:21,270
college newspaper and thought well I'll

00:01:19,920 --> 00:01:24,869
just do this for a little while it seems

00:01:21,270 --> 00:01:27,720
fun so I ended up at The Wall Street

00:01:24,869 --> 00:01:29,970
Journal in 2000 they hired me to cover

00:01:27,720 --> 00:01:33,299
the Internet and I was like anything in

00:01:29,970 --> 00:01:35,159
particular about their like mad just

00:01:33,299 --> 00:01:38,310
everything you seem like you know

00:01:35,159 --> 00:01:41,939
computers so so I was like okay that's

00:01:38,310 --> 00:01:44,009
also a great assignment so I spent 13

00:01:41,939 --> 00:01:45,479
years they're covering technology and I

00:01:44,009 --> 00:01:50,399
was in the New York office I had the

00:01:45,479 --> 00:01:53,790
weird angle of mostly covering the AOL

00:01:50,399 --> 00:01:55,770
Time Warner merger so I want to talk

00:01:53,790 --> 00:01:59,369
about what I learned about forgiveness

00:01:55,770 --> 00:02:01,200
as my time as a reporter so for 14 years

00:01:59,369 --> 00:02:02,549
I was covering technology for The Wall

00:02:01,200 --> 00:02:04,500
Street Journal I read a lot of stories

00:02:02,549 --> 00:02:06,299
right most of them weren't that

00:02:04,500 --> 00:02:08,369
interesting but some of them were one of

00:02:06,299 --> 00:02:10,200
the biggest stories I worked on was the

00:02:08,369 --> 00:02:12,220
AOL Time Warner merger because that was

00:02:10,200 --> 00:02:15,130
a WA was a tech company of its time

00:02:12,220 --> 00:02:19,180
and that murderer consumed 10 years of

00:02:15,130 --> 00:02:21,130
my life and really as you probably all

00:02:19,180 --> 00:02:23,560
may remember it was based on accounting

00:02:21,130 --> 00:02:26,140
fraud right like they they were actually

00:02:23,560 --> 00:02:28,210
doing this crazy thing and well they had

00:02:26,140 --> 00:02:30,550
like a cafeteria they would ask the guys

00:02:28,210 --> 00:02:32,740
to like you know provide the cafeteria

00:02:30,550 --> 00:02:35,560
services at AOL and instead of paying

00:02:32,740 --> 00:02:38,710
them they would say we're gonna pay you

00:02:35,560 --> 00:02:40,510
double and then you buy ads and that was

00:02:38,710 --> 00:02:42,250
the game that's nothing at all this ad

00:02:40,510 --> 00:02:45,480
revenue because Walter it was only

00:02:42,250 --> 00:02:48,760
valuing on ad revenue not on profits so

00:02:45,480 --> 00:02:50,140
so those fun times and in the end I

00:02:48,760 --> 00:02:53,560
think they paid a 300 million dollar

00:02:50,140 --> 00:02:56,170
fine it was a big story I got to be part

00:02:53,560 --> 00:02:58,600
of a Pulitzer Prize winning team super

00:02:56,170 --> 00:03:01,210
good but what's weird is that of my

00:02:58,600 --> 00:03:03,090
reporting there's only two times where

00:03:01,210 --> 00:03:07,840
people that I wrote about went to jail

00:03:03,090 --> 00:03:09,730
so one was a spammer so this guy was a

00:03:07,840 --> 00:03:12,130
spammer he was called the Buffalo

00:03:09,730 --> 00:03:15,400
spammer he was it was early days of spam

00:03:12,130 --> 00:03:17,080
so it was exciting I went to his house

00:03:15,400 --> 00:03:19,720
knocked on his door doctors mother you

00:03:17,080 --> 00:03:22,330
know et cetera and as part of my

00:03:19,720 --> 00:03:23,530
reporting and then additional you know

00:03:22,330 --> 00:03:25,060
the New York Attorney General charged

00:03:23,530 --> 00:03:26,650
him and he actually went to jail he got

00:03:25,060 --> 00:03:29,650
the maximum sentence of three and a half

00:03:26,650 --> 00:03:32,290
years the other guy who I wrote about

00:03:29,650 --> 00:03:34,300
who went to jail was a nail executive

00:03:32,290 --> 00:03:35,890
actually who did an embezzlement scheme

00:03:34,300 --> 00:03:37,180
I don't have his photo he seems to

00:03:35,890 --> 00:03:41,019
remove it from the internet but he was

00:03:37,180 --> 00:03:42,900
also a black man and he also got a

00:03:41,019 --> 00:03:44,830
prison sentence for doing some

00:03:42,900 --> 00:03:49,510
small-time embezzlement he was the head

00:03:44,830 --> 00:03:51,130
of HR so and then we think about where

00:03:49,510 --> 00:03:52,330
the former AOL executives I wrote about

00:03:51,130 --> 00:03:55,120
who did all those round trip deals

00:03:52,330 --> 00:03:56,440
they're doing cool Steve Case funding a

00:03:55,120 --> 00:03:58,810
lot of things Bob Bateman running a

00:03:56,440 --> 00:04:01,630
giant radio network Dave Colburn the guy

00:03:58,810 --> 00:04:02,860
who actually did all the deals who that

00:04:01,630 --> 00:04:04,330
were around trips he settled with the

00:04:02,860 --> 00:04:05,890
SEC for four million and now he's

00:04:04,330 --> 00:04:09,100
investing in tech companies and in

00:04:05,890 --> 00:04:10,959
Israel so you know look this isn't a

00:04:09,100 --> 00:04:12,850
particularly unique story but let's just

00:04:10,959 --> 00:04:14,830
say this is a very American story about

00:04:12,850 --> 00:04:17,109
forgiveness who was forgiven in this

00:04:14,830 --> 00:04:19,450
case what was the unique factor about

00:04:17,109 --> 00:04:21,220
these people I mean I don't know they

00:04:19,450 --> 00:04:23,580
were white men I don't know they were

00:04:21,220 --> 00:04:25,710
really powerful right and the two

00:04:23,580 --> 00:04:28,259
like I find it really depressing and sad

00:04:25,710 --> 00:04:30,539
that the two people I heard about as a

00:04:28,259 --> 00:04:32,039
tech reporter who went to prison we're

00:04:30,539 --> 00:04:33,150
both black men because as you know they

00:04:32,039 --> 00:04:34,590
were probably the only two black men

00:04:33,150 --> 00:04:38,009
ever encountered in my whole time

00:04:34,590 --> 00:04:41,340
covering technology right so we already

00:04:38,009 --> 00:04:43,669
know that our society hands out

00:04:41,340 --> 00:04:44,970
forgiveness and punishment on equally

00:04:43,669 --> 00:04:46,229
right

00:04:44,970 --> 00:04:48,090
I'm not telling anything you don't know

00:04:46,229 --> 00:04:51,620
I'm just telling it to you in a form of

00:04:48,090 --> 00:04:53,879
a personal story so flash forward to I

00:04:51,620 --> 00:04:55,770
decided I'm gonna write a series about

00:04:53,879 --> 00:04:58,770
algorithmic accountability at my new

00:04:55,770 --> 00:05:00,180
employer Pro Publica as we all know

00:04:58,770 --> 00:05:02,879
algorithms are very important in our

00:05:00,180 --> 00:05:04,530
lives right this is the if you haven't

00:05:02,879 --> 00:05:06,750
seen it the Wall Street Journal blue

00:05:04,530 --> 00:05:08,190
feed red feed there's a delightful app

00:05:06,750 --> 00:05:11,099
that you can visit every day this is

00:05:08,190 --> 00:05:12,719
last night and it shows you sort of the

00:05:11,099 --> 00:05:14,340
top stories that would be trending in a

00:05:12,719 --> 00:05:15,990
conservative newsfeed and in a liberal

00:05:14,340 --> 00:05:18,419
news feed so of course yesterday was

00:05:15,990 --> 00:05:23,039
spectacular the conservative news feed

00:05:18,419 --> 00:05:23,669
Trump economy growing 3% and the liberal

00:05:23,039 --> 00:05:26,129
news feed

00:05:23,669 --> 00:05:27,419
Mueller's charges right so it's like

00:05:26,129 --> 00:05:30,449
very different stories are being

00:05:27,419 --> 00:05:33,270
presented so I decided I wanted to do

00:05:30,449 --> 00:05:36,360
some accountability studies about

00:05:33,270 --> 00:05:38,099
algorithms in our lives and it's hard to

00:05:36,360 --> 00:05:39,569
study the news feed in a quantitative

00:05:38,099 --> 00:05:41,750
way and I also wanted something with

00:05:39,569 --> 00:05:44,610
higher stakes so I started with an

00:05:41,750 --> 00:05:47,130
algorithm that is used in the criminal

00:05:44,610 --> 00:05:49,020
justice system to predict whether a

00:05:47,130 --> 00:05:52,199
person is likely to commit a future

00:05:49,020 --> 00:05:54,060
crime this is literally Minority Report

00:05:52,199 --> 00:05:56,250
software basically that is used

00:05:54,060 --> 00:05:58,770
throughout United States first

00:05:56,250 --> 00:06:00,060
sentencing parole pretrial release a lot

00:05:58,770 --> 00:06:03,300
of different stages of the criminal

00:06:00,060 --> 00:06:06,120
justice system so how many people were

00:06:03,300 --> 00:06:07,800
aware that that's even happening okay

00:06:06,120 --> 00:06:09,240
good yay

00:06:07,800 --> 00:06:11,669
when I started looking into it two years

00:06:09,240 --> 00:06:13,680
ago it wasn't as well know that this was

00:06:11,669 --> 00:06:15,060
even being done and so I thought well

00:06:13,680 --> 00:06:17,219
I'm gonna look into this and see if I

00:06:15,060 --> 00:06:20,789
can actually figure out if this software

00:06:17,219 --> 00:06:22,879
is biased so it went and did a Freedom

00:06:20,789 --> 00:06:25,259
of Information request in Florida and

00:06:22,879 --> 00:06:27,569
took five months and some legal

00:06:25,259 --> 00:06:28,919
wrangling but we did get all of the

00:06:27,569 --> 00:06:30,509
scores that they had assigned to

00:06:28,919 --> 00:06:32,880
everyone who was arrested during a two

00:06:30,509 --> 00:06:35,810
year period so the first thing we did

00:06:32,880 --> 00:06:38,360
was put those scores up in a histogram

00:06:35,810 --> 00:06:41,240
well what you see is that know if you

00:06:38,360 --> 00:06:43,220
can see but on the left is black

00:06:41,240 --> 00:06:45,770
defendants and on the right is white

00:06:43,220 --> 00:06:47,690
defendants and so the distribution of

00:06:45,770 --> 00:06:51,340
scores one through ten which is

00:06:47,690 --> 00:06:53,810
basically one least risky ten most risky

00:06:51,340 --> 00:06:55,880
it's very even for the black defendants

00:06:53,810 --> 00:06:58,910
but for the white defendants it's

00:06:55,880 --> 00:07:03,530
strangely skewed towards low-risk right

00:06:58,910 --> 00:07:05,600
so my first thought was huh that's weird

00:07:03,530 --> 00:07:09,080
but you can't really say it's a hundred

00:07:05,600 --> 00:07:11,570
biased until you test whether it's

00:07:09,080 --> 00:07:13,580
accurate right what if every one of the

00:07:11,570 --> 00:07:15,740
white defendants was Mother Teresa right

00:07:13,580 --> 00:07:17,720
and they never did anything wrong it was

00:07:15,740 --> 00:07:21,890
just some weird like jaywalking ticket

00:07:17,720 --> 00:07:23,870
or something so we went and did six

00:07:21,890 --> 00:07:25,250
months of scraping the criminal records

00:07:23,870 --> 00:07:27,500
every one of those defendants that's

00:07:25,250 --> 00:07:30,380
18,000 people it was a complete

00:07:27,500 --> 00:07:32,990
nightmare and and joining those data

00:07:30,380 --> 00:07:34,460
sets to make sure we had the match of a

00:07:32,990 --> 00:07:35,960
person's score with their true

00:07:34,460 --> 00:07:37,850
recidivism did they actually go on to

00:07:35,960 --> 00:07:39,770
commit a crime in the next two years and

00:07:37,850 --> 00:07:42,530
also what was their prior record like

00:07:39,770 --> 00:07:44,750
and what we found was that there was a

00:07:42,530 --> 00:07:46,460
disparity right when you did a logistic

00:07:44,750 --> 00:07:47,930
regression which is just a statistical

00:07:46,460 --> 00:07:50,180
technique which allows you to control

00:07:47,930 --> 00:07:52,070
for all other factors when you control

00:07:50,180 --> 00:07:54,590
for all other factors than race you saw

00:07:52,070 --> 00:07:56,720
that black defendants were 45% more

00:07:54,590 --> 00:07:58,760
likely to be given a high risk score and

00:07:56,720 --> 00:08:00,740
that's controlling for the outcome too

00:07:58,760 --> 00:08:02,000
right which is like not committing a

00:08:00,740 --> 00:08:03,530
crime or committing a crime in the

00:08:02,000 --> 00:08:05,780
future in the next two years so that

00:08:03,530 --> 00:08:08,780
meant there was some disparity here and

00:08:05,780 --> 00:08:10,370
when you looked at it in a in a chart

00:08:08,780 --> 00:08:12,740
basically when you look at false

00:08:10,370 --> 00:08:15,590
positives and false negatives you see

00:08:12,740 --> 00:08:18,700
that the difference is really stark

00:08:15,590 --> 00:08:21,110
right the false positive rate for

00:08:18,700 --> 00:08:22,520
african-american defendants was twice as

00:08:21,110 --> 00:08:24,950
high right they're twice as likely to be

00:08:22,520 --> 00:08:26,450
given a high risk score but not actually

00:08:24,950 --> 00:08:29,690
go on to commit a future crime so like

00:08:26,450 --> 00:08:30,979
falsely be given a high risk score then

00:08:29,690 --> 00:08:33,260
a white defend it and similarly the

00:08:30,979 --> 00:08:36,589
white defendant is twice as likely to

00:08:33,260 --> 00:08:37,849
get an unjustifiable low-risk score

00:08:36,589 --> 00:08:40,849
despite the fact that they turned out to

00:08:37,849 --> 00:08:44,780
have been far more risky and so that

00:08:40,849 --> 00:08:46,459
disparity is really a question of

00:08:44,780 --> 00:08:48,920
forgiveness right we have decided that

00:08:46,459 --> 00:08:51,020
some people are just like more forgiving

00:08:48,920 --> 00:08:52,910
upfront right despite the fact that the

00:08:51,020 --> 00:08:56,540
facts on the ground were exactly the

00:08:52,910 --> 00:08:58,520
same that was a surprising outcome for

00:08:56,540 --> 00:09:02,180
me because I think also we also think of

00:08:58,520 --> 00:09:04,280
bias as against right the bias against

00:09:02,180 --> 00:09:07,810
black defendants but really what this

00:09:04,280 --> 00:09:09,920
was was a bias for white defendants and

00:09:07,810 --> 00:09:11,270
it's sort of a distinction without a

00:09:09,920 --> 00:09:12,800
difference but it's interesting to think

00:09:11,270 --> 00:09:14,630
about and that's why I like to frame

00:09:12,800 --> 00:09:16,340
these conversations around forgiveness

00:09:14,630 --> 00:09:18,530
but you know you could say this is a

00:09:16,340 --> 00:09:20,630
one-off so anyway if we did another

00:09:18,530 --> 00:09:23,330
analysis oh I forgot to show you what

00:09:20,630 --> 00:09:25,190
sorry what it looks like in practice so

00:09:23,330 --> 00:09:27,130
here's a black defendant white defendant

00:09:25,190 --> 00:09:31,010
same crime petty theft

00:09:27,130 --> 00:09:34,550
Boresha high-risk Verna and low-risk

00:09:31,010 --> 00:09:38,360
so Brisa 18 years old walking down the

00:09:34,550 --> 00:09:40,730
street grabbed a bicycle kids bicycle

00:09:38,360 --> 00:09:43,940
from a yard trying to get on it right it

00:09:40,730 --> 00:09:45,290
got a few yards down the mother came out

00:09:43,940 --> 00:09:47,120
said that's my kids bike she gave it

00:09:45,290 --> 00:09:48,110
back but in the meantime the

00:09:47,120 --> 00:09:52,010
neighborhood called the police so she

00:09:48,110 --> 00:09:54,080
was arrested and Vernon stole about 80

00:09:52,010 --> 00:09:57,980
dollars worth of stuff from the CVS so

00:09:54,080 --> 00:09:59,180
they get these high risk scores they get

00:09:57,980 --> 00:10:02,870
their wrist swords when they're arrested

00:09:59,180 --> 00:10:05,510
and basically when you look at it it was

00:10:02,870 --> 00:10:06,950
completely the opposite right so Verna

00:10:05,510 --> 00:10:09,170
got a low-risk score despite the fact

00:10:06,950 --> 00:10:10,760
that he had been already committed two

00:10:09,170 --> 00:10:12,020
armed robberies and one attempted armed

00:10:10,760 --> 00:10:15,200
robbery Nene already served a five-year

00:10:12,020 --> 00:10:16,520
prison sentence and he went on to commit

00:10:15,200 --> 00:10:17,720
grand theft

00:10:16,520 --> 00:10:19,100
she stole thousands of dollars of

00:10:17,720 --> 00:10:21,070
electronics from a warehouse and he's

00:10:19,100 --> 00:10:24,920
now serving a 10-year prison term

00:10:21,070 --> 00:10:27,260
Boresha had some prior arrests but they

00:10:24,920 --> 00:10:28,730
were juvenile misdemeanors and so

00:10:27,260 --> 00:10:30,500
records are sealed but I can tell you

00:10:28,730 --> 00:10:32,120
that misdemeanors are not usually armed

00:10:30,500 --> 00:10:34,400
robberies so let's just say it's a

00:10:32,120 --> 00:10:35,780
smaller crime and she doesn't go on to

00:10:34,400 --> 00:10:37,460
commit any crimes in the next two years

00:10:35,780 --> 00:10:39,080
so this is what a false positive and a

00:10:37,460 --> 00:10:40,940
false negative look like in real life

00:10:39,080 --> 00:10:42,230
that's what forgiveness unfair

00:10:40,940 --> 00:10:44,270
forgiveness really in that one case

00:10:42,230 --> 00:10:45,860
looks like well though you could argue

00:10:44,270 --> 00:10:48,530
me for we should forgive everybody but

00:10:45,860 --> 00:10:50,240
that's a separate issue so so anyways

00:10:48,530 --> 00:10:51,380
this is what we found for this one thing

00:10:50,240 --> 00:10:53,390
so then I was like okay I want to try it

00:10:51,380 --> 00:10:55,460
now this was fun so we did another

00:10:53,390 --> 00:10:57,080
analysis I was like what's another

00:10:55,460 --> 00:10:59,510
algorithm that predicts an outcome well

00:10:57,080 --> 00:11:01,190
weirdly car insurance so your car

00:10:59,510 --> 00:11:02,510
insurance premium that you pay is

00:11:01,190 --> 00:11:03,360
actually meant to predict your

00:11:02,510 --> 00:11:05,610
likelihood

00:11:03,360 --> 00:11:07,140
of getting in an accident right so I was

00:11:05,610 --> 00:11:09,600
like I'm gonna compare that to true risk

00:11:07,140 --> 00:11:11,010
right that's my new game predicted risk

00:11:09,600 --> 00:11:14,970
true risk that's what I do

00:11:11,010 --> 00:11:16,680
so once again it was an enormous amount

00:11:14,970 --> 00:11:19,399
of work to get all the data

00:11:16,680 --> 00:11:22,709
I got Consumer Reports actually bought

00:11:19,399 --> 00:11:26,880
proprietary data set that I analyzed

00:11:22,709 --> 00:11:29,160
with them and we found a similar issue

00:11:26,880 --> 00:11:32,070
which was there was a difference in the

00:11:29,160 --> 00:11:35,250
way risk was allocated so we looked an

00:11:32,070 --> 00:11:38,579
example is this is a guy Oda smash he

00:11:35,250 --> 00:11:41,130
lives in East Garfield Park in Chicago

00:11:38,579 --> 00:11:41,970
which is there's really no way to

00:11:41,130 --> 00:11:44,550
describe it it's pretty much a

00:11:41,970 --> 00:11:47,089
bombed-out bad neighborhood in Chicago

00:11:44,550 --> 00:11:52,170
on the west side and it's dangerous and

00:11:47,089 --> 00:11:53,820
almost entirely minority and he pays 190

00:11:52,170 --> 00:11:55,380
dollars a month for car insurance he's

00:11:53,820 --> 00:12:00,360
never had any accidents he's a great

00:11:55,380 --> 00:12:03,360
driver and he has Geico and he's but

00:12:00,360 --> 00:12:04,680
he's struggling he's 190 months for

00:12:03,360 --> 00:12:08,040
somebody who's the worst as a security

00:12:04,680 --> 00:12:09,720
guard is is no joke he works six days a

00:12:08,040 --> 00:12:12,570
week and he can barely afford it

00:12:09,720 --> 00:12:13,519
so then there's this guy Ryan across the

00:12:12,570 --> 00:12:16,199
treant town

00:12:13,519 --> 00:12:19,350
he pays 55 dollars a month for the same

00:12:16,199 --> 00:12:21,000
plan from Geico right and he has

00:12:19,350 --> 00:12:25,680
actually just recently gotten in an

00:12:21,000 --> 00:12:27,959
accident and has the same coverage and

00:12:25,680 --> 00:12:30,360
you know the real difference between

00:12:27,959 --> 00:12:32,519
these two is their zip code so insurance

00:12:30,360 --> 00:12:35,160
companies actually have one factor that

00:12:32,519 --> 00:12:36,540
they use to price your insurance that is

00:12:35,160 --> 00:12:38,880
separate from your driving record and

00:12:36,540 --> 00:12:40,829
it's called the zip code factor and they

00:12:38,880 --> 00:12:43,500
basically assign a risk score to each

00:12:40,829 --> 00:12:46,019
zip code that is independent of how you

00:12:43,500 --> 00:12:48,360
drive and when you look at it now Ryan

00:12:46,019 --> 00:12:49,440
and Otis are never gonna be exactly the

00:12:48,360 --> 00:12:50,699
same they're not the same age so they

00:12:49,440 --> 00:12:52,529
don't have exactly the same risk factors

00:12:50,699 --> 00:12:54,690
but when you control for all the risk

00:12:52,529 --> 00:12:57,209
factors every single one of our charts

00:12:54,690 --> 00:13:00,180
looks like this so the chart is

00:12:57,209 --> 00:13:02,910
basically predicted risk to true risk

00:13:00,180 --> 00:13:04,350
and if you think of predictive risk as

00:13:02,910 --> 00:13:07,199
essentially your premium and the red

00:13:04,350 --> 00:13:08,880
straight line is for minority

00:13:07,199 --> 00:13:10,800
neighborhoods so for minority

00:13:08,880 --> 00:13:12,630
neighborhoods at the prices track risk

00:13:10,800 --> 00:13:14,730
just keep going straight like a nice

00:13:12,630 --> 00:13:16,080
linear relationship and the blue line

00:13:14,730 --> 00:13:17,730
that goes down

00:13:16,080 --> 00:13:19,680
that's where the white neighborhoods are

00:13:17,730 --> 00:13:21,930
and that's basically they go up and then

00:13:19,680 --> 00:13:24,750
all the sudden as they get riskier the

00:13:21,930 --> 00:13:27,480
price goes down unexplainably right and

00:13:24,750 --> 00:13:29,850
so once again we have the strange mother

00:13:27,480 --> 00:13:32,460
measure of discount applying to white

00:13:29,850 --> 00:13:34,500
pen to white neighborhoods that is not

00:13:32,460 --> 00:13:36,360
explainable by risk and the insurance

00:13:34,500 --> 00:13:38,400
industry to this day we published this

00:13:36,360 --> 00:13:40,110
earlier this year has yet to respond

00:13:38,400 --> 00:13:42,210
they said they would come out with a big

00:13:40,110 --> 00:13:44,430
paper explaining the risk why this was

00:13:42,210 --> 00:13:45,600
true in there as of yet they am have not

00:13:44,430 --> 00:13:48,630
responded I'm speaking at their

00:13:45,600 --> 00:13:51,150
convention next week and I'm anxiously

00:13:48,630 --> 00:13:55,440
awaiting their rebuttal maybe they want

00:13:51,150 --> 00:13:58,370
to present as me on stage anyways but

00:13:55,440 --> 00:14:01,290
you know this is again this weird

00:13:58,370 --> 00:14:04,080
unexplained forgiveness for one set of

00:14:01,290 --> 00:14:05,880
people baked into an algorithm right and

00:14:04,080 --> 00:14:07,680
so I guess what I want to say is like

00:14:05,880 --> 00:14:09,300
all of you guys might be in the position

00:14:07,680 --> 00:14:10,380
to build algorithms maybe that's what

00:14:09,300 --> 00:14:12,810
you're gonna do next or maybe you're

00:14:10,380 --> 00:14:14,490
gonna be auditing them we're all in a

00:14:12,810 --> 00:14:15,900
world of automated decision-making there

00:14:14,490 --> 00:14:17,700
will be more and more decisions that are

00:14:15,900 --> 00:14:18,810
going to be automated and so I guess I

00:14:17,700 --> 00:14:20,880
would just like to leave you with this

00:14:18,810 --> 00:14:22,590
thought which is we talked about bias

00:14:20,880 --> 00:14:25,470
and bias is important to think about but

00:14:22,590 --> 00:14:28,260
think about forgiveness too because in

00:14:25,470 --> 00:14:30,630
some ways what we have done is at least

00:14:28,260 --> 00:14:32,550
in the things I've studied is that we've

00:14:30,630 --> 00:14:36,360
just meted out forgiveness some people

00:14:32,550 --> 00:14:38,670
get impunity they're not held to the

00:14:36,360 --> 00:14:41,220
same standard the theoretical standard

00:14:38,670 --> 00:14:44,280
that we apply to everyone else and so

00:14:41,220 --> 00:14:46,640
take that with you and I'd be happy to

00:14:44,280 --> 00:14:46,640
take any questions

00:14:52,929 --> 00:14:58,819
questions for Julia our people get

00:14:57,319 --> 00:15:00,439
getting their confidence I'm really keen

00:14:58,819 --> 00:15:01,790
to never lie the makeup of your team who

00:15:00,439 --> 00:15:04,579
you're working with to kind of helps in

00:15:01,790 --> 00:15:07,670
to scrape xviii oh yeah of course right

00:15:04,579 --> 00:15:10,100
yeah yeah I almost did knock on the

00:15:07,670 --> 00:15:12,800
future of journalism which was about I

00:15:10,100 --> 00:15:13,879
was I couldn't decide because I do feel

00:15:12,800 --> 00:15:15,829
like I'm building a new kind of

00:15:13,879 --> 00:15:18,230
journalism here I have two programmers

00:15:15,829 --> 00:15:19,550
working for me and a researcher so we

00:15:18,230 --> 00:15:24,079
have a real team and each one of these

00:15:19,550 --> 00:15:27,350
projects takes a year and I think that

00:15:24,079 --> 00:15:29,269
as we go towards you know us up the talk

00:15:27,350 --> 00:15:31,970
on the earthquake was so important

00:15:29,269 --> 00:15:33,759
because journalists are going to need to

00:15:31,970 --> 00:15:36,679
do much more validation verification

00:15:33,759 --> 00:15:40,040
forensics analysis right and so we do

00:15:36,679 --> 00:15:42,050
need to spill more of basically

00:15:40,040 --> 00:15:44,199
quantified teams and so I'm trying to

00:15:42,050 --> 00:15:46,970
pioneer that a little bit in my way

00:15:44,199 --> 00:15:50,029
there's a question right at the front

00:15:46,970 --> 00:15:54,740
here from yeah from castelo first and

00:15:50,029 --> 00:15:56,149
then we'll go to the back sorry days

00:15:54,740 --> 00:15:58,999
they're just making you run round thank

00:15:56,149 --> 00:16:00,980
you very much actually my question was

00:15:58,999 --> 00:16:05,209
delving into that a little bit deeper

00:16:00,980 --> 00:16:08,689
how do you process all the data and how

00:16:05,209 --> 00:16:11,269
do you like actually merge the different

00:16:08,689 --> 00:16:13,490
databases like if you could just explain

00:16:11,269 --> 00:16:14,779
more in more a little bit more detail

00:16:13,490 --> 00:16:17,269
I'm not sure it's a very complicated

00:16:14,779 --> 00:16:22,119
process but a little bit more in detail

00:16:17,269 --> 00:16:25,819
how like the analogy of it goes right so

00:16:22,119 --> 00:16:27,679
one reason that you don't see so much

00:16:25,819 --> 00:16:31,339
work like this including from academics

00:16:27,679 --> 00:16:34,339
is because it's a nightmare so for

00:16:31,339 --> 00:16:35,990
instance in both cases you know what

00:16:34,339 --> 00:16:37,910
what the special sauce that we brought

00:16:35,990 --> 00:16:39,559
was to match the predicted risk to the

00:16:37,910 --> 00:16:43,040
true risk and what that means really is

00:16:39,559 --> 00:16:46,699
a giant database join right and those

00:16:43,040 --> 00:16:48,379
are super messy and in both cases you

00:16:46,699 --> 00:16:51,230
know one took six months one to nine

00:16:48,379 --> 00:16:52,490
months and there's really no getting

00:16:51,230 --> 00:16:53,360
around the fact that you have to do a

00:16:52,490 --> 00:16:55,819
lot of it by hand

00:16:53,360 --> 00:16:58,160
like there's we tried to automate and we

00:16:55,819 --> 00:17:01,590
try to do probabilistic matching and all

00:16:58,160 --> 00:17:04,209
that stuff but truthfully

00:17:01,590 --> 00:17:06,060
the standards that were held to as

00:17:04,209 --> 00:17:08,350
journalists is it can't be just

00:17:06,060 --> 00:17:10,570
probabilistic match it can't be like 80

00:17:08,350 --> 00:17:13,780
percent right it has to be right and so

00:17:10,570 --> 00:17:16,540
in the end we end up doing a lot of hand

00:17:13,780 --> 00:17:18,820
matching of Records which was horrible

00:17:16,540 --> 00:17:20,560
and one thing I've been thinking about a

00:17:18,820 --> 00:17:22,240
lot is how to build more capacity for

00:17:20,560 --> 00:17:23,860
that because I don't think most

00:17:22,240 --> 00:17:26,020
newsrooms can't do this right ProPublica

00:17:23,860 --> 00:17:28,600
is like this you know utopian universe

00:17:26,020 --> 00:17:31,240
of journalism nonprofit funded really

00:17:28,600 --> 00:17:32,530
doing well and invested in this type of

00:17:31,240 --> 00:17:34,720
work but that's not true of most

00:17:32,530 --> 00:17:36,130
newsrooms and so I've been thinking

00:17:34,720 --> 00:17:37,780
about the fact that like you know this

00:17:36,130 --> 00:17:40,900
is something maybe Mechanical Turk could

00:17:37,780 --> 00:17:43,210
be brought to bear I'm actually trying

00:17:40,900 --> 00:17:48,220
to work with this amazing coding group

00:17:43,210 --> 00:17:49,810
at San Quentin prison in California they

00:17:48,220 --> 00:17:52,150
actually have a coding Academy and they

00:17:49,810 --> 00:17:54,130
meet and they so I'm trying to work with

00:17:52,150 --> 00:17:55,600
them to teach the inmates maybe to help

00:17:54,130 --> 00:17:57,670
with this type of matching I think that

00:17:55,600 --> 00:17:59,020
there's a lot of untapped opportunities

00:17:57,670 --> 00:18:00,400
for this type of work that I've been

00:17:59,020 --> 00:18:01,870
trying to explore because I do think

00:18:00,400 --> 00:18:06,970
this is the gating factor for this type

00:18:01,870 --> 00:18:08,530
of work hi I work for the New York City

00:18:06,970 --> 00:18:10,060
Department of Education and we've

00:18:08,530 --> 00:18:12,130
actually been using your articles to

00:18:10,060 --> 00:18:14,470
teach students about algorithmic bias so

00:18:12,130 --> 00:18:16,990
thank you writing such an important

00:18:14,470 --> 00:18:20,050
journalism that our students can use but

00:18:16,990 --> 00:18:22,960
I was curious how I how might we think

00:18:20,050 --> 00:18:25,300
about empowering the next generation of

00:18:22,960 --> 00:18:26,920
students to make ethical decisions on

00:18:25,300 --> 00:18:29,410
some of this can feel a little bit

00:18:26,920 --> 00:18:30,940
hopeless when you see some of it and

00:18:29,410 --> 00:18:33,990
right how do we make them feel empowered

00:18:30,940 --> 00:18:36,430
oh I love that question because I am

00:18:33,990 --> 00:18:38,770
strangely hopeful person despite my

00:18:36,430 --> 00:18:43,510
weird job of doing only unhelpful things

00:18:38,770 --> 00:18:45,760
and so I do believe you know like the

00:18:43,510 --> 00:18:50,740
criminal risk score algorithm is a good

00:18:45,760 --> 00:18:53,100
example if they adjust if they fixed so

00:18:50,740 --> 00:18:55,090
after our story came out a bunch of

00:18:53,100 --> 00:18:57,640
mathematicians the commuter science came

00:18:55,090 --> 00:18:58,990
with all these papers studying our data

00:18:57,640 --> 00:19:00,940
set and coming up with some theoretical

00:18:58,990 --> 00:19:02,740
conclusions and essentially they all

00:19:00,940 --> 00:19:04,990
said you know you could fix this

00:19:02,740 --> 00:19:06,370
algorithm if you were to balance the

00:19:04,990 --> 00:19:08,110
error rates like if you were to choose

00:19:06,370 --> 00:19:09,460
to optimize your algorithm to balance

00:19:08,110 --> 00:19:11,890
the error rates they've chosen to after

00:19:09,460 --> 00:19:14,570
optimize it another way which is for

00:19:11,890 --> 00:19:16,640
predictive accuracy so meaning it's

00:19:14,570 --> 00:19:18,290
correct in its prediction 60% of the

00:19:16,640 --> 00:19:19,940
time for both black and white defendants

00:19:18,290 --> 00:19:21,680
but when it's wrong it's wrong in this

00:19:19,940 --> 00:19:24,350
completely disparate way right so you

00:19:21,680 --> 00:19:25,970
could actually fix it that way and all

00:19:24,350 --> 00:19:27,650
that would happen the only bad outcome

00:19:25,970 --> 00:19:29,090
was what the algorithm would be more

00:19:27,650 --> 00:19:30,290
accurate for black defendants than white

00:19:29,090 --> 00:19:32,000
which makes sense because there's

00:19:30,290 --> 00:19:34,460
actually more black defendants in our

00:19:32,000 --> 00:19:36,080
criminal justice system so what's weird

00:19:34,460 --> 00:19:37,880
is there is a hopeful outcome right

00:19:36,080 --> 00:19:39,470
there's like something you could do now

00:19:37,880 --> 00:19:41,810
I would also though like to step back

00:19:39,470 --> 00:19:44,360
and say I'm not entirely sure we're

00:19:41,810 --> 00:19:46,220
predicting anyone's criminality in the

00:19:44,360 --> 00:19:47,180
future I can't even predict my husband

00:19:46,220 --> 00:19:48,920
and I have been married to him for a

00:19:47,180 --> 00:19:50,780
long time it's like predicting human

00:19:48,920 --> 00:19:52,760
behavior like we can't even get our maps

00:19:50,780 --> 00:19:54,680
to get us to the right place most of the

00:19:52,760 --> 00:19:56,510
time so is is this really where we want

00:19:54,680 --> 00:19:57,920
to bring computers to bear is predicting

00:19:56,510 --> 00:19:59,600
human behavior I feel like this is maybe

00:19:57,920 --> 00:20:02,110
like a future thing that we're not gonna

00:19:59,600 --> 00:20:04,700
be so good at yet but I do think that

00:20:02,110 --> 00:20:07,940
algorithms are going to be better than

00:20:04,700 --> 00:20:09,950
people right we are in a lot of ways but

00:20:07,940 --> 00:20:11,360
we have to learn how to hold them

00:20:09,950 --> 00:20:13,760
accountable and we have to have build

00:20:11,360 --> 00:20:15,470
systems around that I'm I'm perfectly

00:20:13,760 --> 00:20:17,540
sure that a car is gonna drive better

00:20:15,470 --> 00:20:19,430
than me I'm pretty bad driver right like

00:20:17,540 --> 00:20:20,870
so another self-driving car so I feel

00:20:19,430 --> 00:20:22,790
like there is I don't want to be a

00:20:20,870 --> 00:20:23,930
Luddite about it I just want to say we

00:20:22,790 --> 00:20:25,130
need systems of oversight and

00:20:23,930 --> 00:20:27,050
accountability before we can move

00:20:25,130 --> 00:20:29,720
forward because otherwise it really will

00:20:27,050 --> 00:20:34,990
be the Wild West there's a question just

00:20:29,720 --> 00:20:39,980
in front as well oh I am right okay

00:20:34,990 --> 00:20:41,930
any other questions burning questions

00:20:39,980 --> 00:20:42,910
okay well lesson let's thank Julia for

00:20:41,930 --> 00:20:45,990
her

00:20:42,910 --> 00:20:46,490
[Music]

00:20:45,990 --> 00:20:46,610
you

00:20:46,490 --> 00:20:50,180
[Music]

00:20:46,610 --> 00:20:50,180

YouTube URL: https://www.youtube.com/watch?v=Yei7SS8nEqw


