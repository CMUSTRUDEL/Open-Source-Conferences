Title: AI's Collateral Damage | MozFest 2018
Publication date: 2019-06-06
Playlist: Mozilla Festival
Description: 
	Silicon Valley, Washington, DC, Beijing and other power centers are in an AI arms race. What does this mean for human rights and the centralization of power? 

This panel from MozFest 2018 features Mozilla Fellow Camille Francois; AlgoTransparency founder Guillaume Chaslot; Social Science Research Council president Alondra Nelson; and author and former FBI special agent Clinton Watts.

MozFest is Mozilla's seven-day celebration for, by, and about people who love the internet. Learn more: https://mozillafestival.org/
Captions: 
	00:00:00,420 --> 00:00:03,830
I hope some of you went to the party

00:00:02,580 --> 00:00:06,830
last night

00:00:03,830 --> 00:00:06,830
today's

00:00:07,049 --> 00:00:10,320
we'll just get started really quickly

00:00:08,490 --> 00:00:12,480
there's been a couple of changes in

00:00:10,320 --> 00:00:15,360
today's programming the next panel is

00:00:12,480 --> 00:00:17,680
actually no tow would you say for that

00:00:15,360 --> 00:00:19,060
anyway

00:00:17,680 --> 00:00:21,670
but let's just pick this one officer

00:00:19,060 --> 00:00:24,520
today's first panel biggest pattern is

00:00:21,670 --> 00:00:27,490
called a is collateral damage and we

00:00:24,520 --> 00:00:29,439
have with us me francois whose research

00:00:27,490 --> 00:00:31,869
directed grafica which analyzes social

00:00:29,439 --> 00:00:33,550
networks and affiliates at Harvard's

00:00:31,869 --> 00:00:38,949
Berkman Center for Internet and Society

00:00:33,550 --> 00:00:40,989
and a Mozilla fellow which he inshallah

00:00:38,949 --> 00:00:42,010
is the founder of algo transparency

00:00:40,989 --> 00:00:44,769
which some of you may have come across

00:00:42,010 --> 00:00:47,379
it's really cool but have shed a little

00:00:44,769 --> 00:00:50,080
bit of light on the YouTube algorithms

00:00:47,379 --> 00:00:53,530
and also the Center for Humane

00:00:50,080 --> 00:00:55,269
technology Alondra Nelson is president

00:00:53,530 --> 00:00:58,080
of the social science research council

00:00:55,269 --> 00:00:58,080
in New York

00:00:58,780 --> 00:01:04,180
cheap ass at Columbia New York and

00:01:00,670 --> 00:01:06,580
indeed a data and society Research

00:01:04,180 --> 00:01:08,860
Institute in New York and finally Clint

00:01:06,580 --> 00:01:11,860
what is the author of a book on

00:01:08,860 --> 00:01:14,470
disinformation and extremism not messin

00:01:11,860 --> 00:01:16,270
with the enemy and a foreign sorry

00:01:14,470 --> 00:01:21,869
fellow at the Foreign Policy Research

00:01:16,270 --> 00:01:23,820
Institute in morning everyone

00:01:21,869 --> 00:01:26,310
I'm so that's the biggest parent we have

00:01:23,820 --> 00:01:27,869
poor people I'm going to turn my better

00:01:26,310 --> 00:01:29,090
shows as possible to everyone that knows

00:01:27,869 --> 00:01:32,270
to speak

00:01:29,090 --> 00:01:34,450
I'm just gonna kick off with you Gil

00:01:32,270 --> 00:01:37,700
so over the past year there's a lot of

00:01:34,450 --> 00:01:40,130
pieces and I have read certainly about

00:01:37,700 --> 00:01:41,899
YouTube and home you know you start with

00:01:40,130 --> 00:01:42,890
a nice Peppa Pig video and before you

00:01:41,899 --> 00:01:45,369
know it you're being brainwashed by

00:01:42,890 --> 00:01:45,369
Nazis

00:01:47,890 --> 00:01:52,530
just to bring

00:01:49,570 --> 00:01:56,230
well some transparency to the algorithm

00:01:52,530 --> 00:01:58,710
and also at the Center for human

00:01:56,230 --> 00:02:01,000
technology you have the assertion that

00:01:58,710 --> 00:02:03,400
technology is hijacking our minds in

00:02:01,000 --> 00:02:05,890
society how much of that hijacking do

00:02:03,400 --> 00:02:07,540
you think is just human beings being

00:02:05,890 --> 00:02:09,100
human beings being drawn to spectacle

00:02:07,540 --> 00:02:12,340
and how much do you actually attribute

00:02:09,100 --> 00:02:16,210
to the machines themselves creating

00:02:12,340 --> 00:02:19,930
these misconception algorithm our mirror

00:02:16,210 --> 00:02:22,920
of society that it just amplify

00:02:19,930 --> 00:02:25,840
everything same but they don't like they

00:02:22,920 --> 00:02:28,930
especially if I'm or some parts of the

00:02:25,840 --> 00:02:32,620
political spectrum on less another part

00:02:28,930 --> 00:02:36,070
that are better for what they are made

00:02:32,620 --> 00:02:38,640
for so for instance most social media

00:02:36,070 --> 00:02:41,110
now is made for engagement over time and

00:02:38,640 --> 00:02:43,450
on that Russia is in a certain direction

00:02:41,110 --> 00:02:47,500
it would be made for something else for

00:02:43,450 --> 00:02:49,870
legs or a versus evil things like that

00:02:47,500 --> 00:02:50,320
it would recommend something completely

00:02:49,870 --> 00:02:53,140
different

00:02:50,320 --> 00:02:55,959
but they would still be similarities but

00:02:53,140 --> 00:02:57,489
they would be a little tease

00:02:55,959 --> 00:02:59,200
but I know what I'm trying to get at is

00:02:57,489 --> 00:03:01,150
that there's still human beings making

00:02:59,200 --> 00:03:04,239
these decisions right the agency is with

00:03:01,150 --> 00:03:07,090
people but if it's not it's not me as a

00:03:04,239 --> 00:03:08,530
user who's driving it it the creators of

00:03:07,090 --> 00:03:11,319
the algorithms that are driving my

00:03:08,530 --> 00:03:14,260
behavior it's a little tough to

00:03:11,319 --> 00:03:17,469
understand because if you make the type

00:03:14,260 --> 00:03:20,109
of content that one gets Raimondi or 40

00:03:17,469 --> 00:03:21,790
times more than the other then at the

00:03:20,109 --> 00:03:23,920
end of the day you will feel like oh

00:03:21,790 --> 00:03:25,870
this conference is more popular so we in

00:03:23,920 --> 00:03:29,049
turn it will push content creators

00:03:25,870 --> 00:03:32,620
really is that so it's actually know

00:03:29,049 --> 00:03:35,620
we're obeying the algorithm in a way we

00:03:32,620 --> 00:03:37,419
don't even realize using the behaviors

00:03:35,620 --> 00:03:40,959
are embedded in it or the incentives are

00:03:37,419 --> 00:03:43,030
embedded in it so the instant it's a

00:03:40,959 --> 00:03:45,819
very complex system it's like a big

00:03:43,030 --> 00:03:50,010
machine learning a big black box we have

00:03:45,819 --> 00:03:52,540
no idea and we operate and even even the

00:03:50,010 --> 00:03:57,099
programmers even me when I was a I

00:03:52,540 --> 00:03:59,230
didn't understand why they specifically

00:03:57,099 --> 00:04:00,969
right and under I see noting over there

00:03:59,230 --> 00:04:04,209
yeah I mean I want to go back to your

00:04:00,969 --> 00:04:06,249
point Leo about choice because they are

00:04:04,209 --> 00:04:07,959
black boxes they're very complicated but

00:04:06,249 --> 00:04:10,480
after foundation the cornerstone or

00:04:07,959 --> 00:04:12,129
choices that we've made a society about

00:04:10,480 --> 00:04:14,889
what incentive structures we want to

00:04:12,129 --> 00:04:17,979
embed into technology and so I think

00:04:14,889 --> 00:04:19,750
that our conversation about AI um too

00:04:17,979 --> 00:04:21,549
often is a kind of technological

00:04:19,750 --> 00:04:22,990
determinist framing that the technology

00:04:21,549 --> 00:04:24,669
is sort of doing the things and we are

00:04:22,990 --> 00:04:26,560
caught in the system and that is true I

00:04:24,669 --> 00:04:29,620
mean guillaumes work shows that some

00:04:26,560 --> 00:04:31,150
regard but you know I love spaces like

00:04:29,620 --> 00:04:33,099
this because I think it allows us an

00:04:31,150 --> 00:04:34,720
opportunity to have a conversation that

00:04:33,099 --> 00:04:37,750
resets and reframes the

00:04:34,720 --> 00:04:39,400
sort of terms of conversation and I

00:04:37,750 --> 00:04:43,210
think that we need to think increasingly

00:04:39,400 --> 00:04:45,430
about choices that we're making around

00:04:43,210 --> 00:04:47,110
AI and we also any need to leave open

00:04:45,430 --> 00:04:48,759
the space of possibility that we don't

00:04:47,110 --> 00:04:51,490
have to choose it but there are other

00:04:48,759 --> 00:04:54,090
kinds of imaginaries that we can have

00:04:51,490 --> 00:04:56,710
for our entertainment for our politics

00:04:54,090 --> 00:05:00,820
for national security and the like and

00:04:56,710 --> 00:05:03,060
that we should while we can be caught up

00:05:00,820 --> 00:05:05,979
in almost an addictive sort of

00:05:03,060 --> 00:05:08,590
relationship to particularly sort of

00:05:05,979 --> 00:05:11,050
YouTube's algorithms that are feeding us

00:05:08,590 --> 00:05:14,229
things that give us pleasure we also I

00:05:11,050 --> 00:05:16,060
think have the choice to make other of

00:05:14,229 --> 00:05:18,850
other options well

00:05:16,060 --> 00:05:21,040
and you're talking about lizards it's

00:05:18,850 --> 00:05:24,640
not even pleasure also YouTube recently

00:05:21,040 --> 00:05:28,060
note is that the current algorithm that

00:05:24,640 --> 00:05:31,000
maximizes watch time sometimes is very

00:05:28,060 --> 00:05:35,230
hurtful so they did surveys recently on

00:05:31,000 --> 00:05:37,780
the noticed people were often like very

00:05:35,230 --> 00:05:39,220
upset about the recommendation so they

00:05:37,780 --> 00:05:42,370
use this survey to improve things but

00:05:39,220 --> 00:05:44,560
it's like very recently I've done that

00:05:42,370 --> 00:05:46,360
in years ago right whether it's it I

00:05:44,560 --> 00:05:48,160
mean I guess pleasure is one of a range

00:05:46,360 --> 00:05:51,190
of emotions right in the ideas emotion

00:05:48,160 --> 00:05:52,950
rather than strictly pleasure okay which

00:05:51,190 --> 00:05:54,900
one do you choose

00:05:52,950 --> 00:05:57,120
I mean you look like you were about to

00:05:54,900 --> 00:05:58,770
say something now I'm wondering upon

00:05:57,120 --> 00:06:01,800
your first question which is where does

00:05:58,770 --> 00:06:05,300
agency moment and I think it's it's

00:06:01,800 --> 00:06:05,300
difficult on both ends of the spectrum

00:06:05,390 --> 00:06:11,430
of a rhythm I think our role is to

00:06:08,610 --> 00:06:13,980
recognize that as you said they are

00:06:11,430 --> 00:06:17,250
designed by humans who make decision

00:06:13,980 --> 00:06:19,500
maybe it's watch time maybe it's on

00:06:17,250 --> 00:06:22,980
instead is more likely to yield

00:06:19,500 --> 00:06:25,950
advertising revenue may be played as Yun

00:06:22,980 --> 00:06:29,790
said the universe of parameters that

00:06:25,950 --> 00:06:32,130
lead to a decision that this content in

00:06:29,790 --> 00:06:34,290
turn of people is huge at a typical

00:06:32,130 --> 00:06:36,390
impact and so you have on that spectrum

00:06:34,290 --> 00:06:38,610
you have like business or making design

00:06:36,390 --> 00:06:40,200
choices they're influenced by your

00:06:38,610 --> 00:06:42,030
business models by their values with

00:06:40,200 --> 00:06:44,090
their choices on that it and on the

00:06:42,030 --> 00:06:46,470
other end I think it's you know we feel

00:06:44,090 --> 00:06:48,360
but isn't as you said like isn't this

00:06:46,470 --> 00:06:50,640
the users problem after all they are

00:06:48,360 --> 00:06:53,340
clicking on it right they're watching

00:06:50,640 --> 00:06:55,320
the video no one's forcing them

00:06:53,340 --> 00:06:57,630
and I'm trying to think through you know

00:06:55,320 --> 00:06:59,070
what's the right frame and something

00:06:57,630 --> 00:07:01,620
that comes to mind and really like you

00:06:59,070 --> 00:07:03,570
know as you're speaking is it's almost

00:07:01,620 --> 00:07:05,430
like if you were saying this movie

00:07:03,570 --> 00:07:07,800
didn't really perform well in the

00:07:05,430 --> 00:07:10,050
theaters and someone was telling you but

00:07:07,800 --> 00:07:11,670
no one was showing it and yet everyone

00:07:10,050 --> 00:07:13,530
was showing Star Wars and there were ads

00:07:11,670 --> 00:07:15,660
everywhere right so like of course

00:07:13,530 --> 00:07:18,360
there's agency on the user side but of

00:07:15,660 --> 00:07:21,870
course user is evolving in a universe in

00:07:18,360 --> 00:07:23,960
which suggestions incontinence through

00:07:21,870 --> 00:07:26,910
their device in a very personal way are

00:07:23,960 --> 00:07:29,220
absolutely constant and I think you

00:07:26,910 --> 00:07:31,220
should definitely read about what I

00:07:29,220 --> 00:07:35,210
think is fascinating Indians work is

00:07:31,220 --> 00:07:38,550
he's able now to quantify this right and

00:07:35,210 --> 00:07:41,040
it's difficult to measure because data

00:07:38,550 --> 00:07:43,970
is in straight boredom combined

00:07:41,040 --> 00:07:48,690
frequency I would quantify the degree

00:07:43,970 --> 00:07:51,600
which the user is being pushed into a

00:07:48,690 --> 00:07:54,240
rabbit hole of depression and quantify

00:07:51,600 --> 00:07:56,850
at scale which part of what the user is

00:07:54,240 --> 00:07:59,160
receiving is actually an algorithm and

00:07:56,850 --> 00:08:01,110
so it's interesting is when your content

00:07:59,160 --> 00:08:04,080
rate on YouTube for instance and you

00:08:01,110 --> 00:08:07,500
have a video and you monetize your video

00:08:04,080 --> 00:08:10,760
you tells you as a content creator which

00:08:07,500 --> 00:08:12,450
part of your view time comes through

00:08:10,760 --> 00:08:14,790
recommendations that were made by the

00:08:12,450 --> 00:08:16,290
algorithm so you know if you're a

00:08:14,790 --> 00:08:18,240
content creator you know this for your

00:08:16,290 --> 00:08:20,610
own little video I think what's

00:08:18,240 --> 00:08:22,530
fascinating is now we're able to see

00:08:20,610 --> 00:08:24,480
what this what this looks like at the

00:08:22,530 --> 00:08:27,140
degree of an entire conversation what

00:08:24,480 --> 00:08:27,140
does it look like you

00:08:27,670 --> 00:08:31,980
so we

00:08:30,060 --> 00:08:35,130
precise examples from a website everyday

00:08:31,980 --> 00:08:37,560
scraps are thousands of panels on see

00:08:35,130 --> 00:08:41,310
where is a recommendation leads on for

00:08:37,560 --> 00:08:45,120
instance as the day before the synagogue

00:08:41,310 --> 00:08:49,110
tank and exhibit user was recommended by

00:08:45,120 --> 00:08:53,250
the most panel was a Devi day I kidding

00:08:49,110 --> 00:08:56,040
but Rosaura supporting her as the

00:08:53,250 --> 00:08:59,550
world's money than that was recommended

00:08:56,040 --> 00:09:03,000
for more than 40 channels whereas he had

00:08:59,550 --> 00:09:05,370
only 800 views please like various and

00:09:03,000 --> 00:09:08,370
why did he get recommendation I have

00:09:05,370 --> 00:09:09,510
from the start and I'll know but nobody

00:09:08,370 --> 00:09:12,360
can explain that because the other

00:09:09,510 --> 00:09:14,010
person is okay okay so I'm sorry clean I

00:09:12,360 --> 00:09:17,220
just want to come back to one thing

00:09:14,010 --> 00:09:19,290
which is my very first point which is I

00:09:17,220 --> 00:09:22,630
feel like it's easy for us to say oh a

00:09:19,290 --> 00:09:24,550
i/o algorithms

00:09:22,630 --> 00:09:26,590
in the end its business its market

00:09:24,550 --> 00:09:28,630
incentives that is driving this right if

00:09:26,590 --> 00:09:29,980
not on the part of the user if not on

00:09:28,630 --> 00:09:33,330
the part of the content creator it's

00:09:29,980 --> 00:09:36,640
certainly in the pot business right

00:09:33,330 --> 00:09:38,950
ultimately if we look back this is one

00:09:36,640 --> 00:09:40,600
part algorithm one part sir and within

00:09:38,950 --> 00:09:42,700
the user we're always letting them off

00:09:40,600 --> 00:09:43,900
and they cry about it so I always use

00:09:42,700 --> 00:09:47,110
the example of food

00:09:43,900 --> 00:09:49,600
I love Krispy Kreme donut if I ate 12

00:09:47,110 --> 00:09:51,520
per speaking Donuts every day I'm going

00:09:49,600 --> 00:09:52,590
to be enormous and feel terrible right

00:09:51,520 --> 00:09:53,970
and

00:09:52,590 --> 00:09:55,860
got to a point where we explained that

00:09:53,970 --> 00:09:58,260
with food you know warnings on

00:09:55,860 --> 00:10:01,590
cigarettes things like that so we're at

00:09:58,260 --> 00:10:03,630
that social media has a problem their

00:10:01,590 --> 00:10:05,490
product is that the liver is designed to

00:10:03,630 --> 00:10:08,670
deliver people things that they want to

00:10:05,490 --> 00:10:10,740
see this is confirmation bias this is

00:10:08,670 --> 00:10:12,540
implicit bias I want things that come

00:10:10,740 --> 00:10:14,940
from people that look like me and talk

00:10:12,540 --> 00:10:16,290
like me this is the nationalist dynamic

00:10:14,940 --> 00:10:19,230
we're seeing playing out right now and

00:10:16,290 --> 00:10:21,100
so this is going to be very hard for

00:10:19,230 --> 00:10:23,410
them to get around

00:10:21,100 --> 00:10:25,780
I want to rewind a little bit his point

00:10:23,410 --> 00:10:28,150
about the algorithm we were tracking

00:10:25,780 --> 00:10:30,310
terrorists on Twitter

00:10:28,150 --> 00:10:31,510
there was one terrorist that we knew was

00:10:30,310 --> 00:10:33,910
an actual terrorist he's on the

00:10:31,510 --> 00:10:35,950
most-wanted list if you followed him on

00:10:33,910 --> 00:10:38,980
twitter twitter would recommend other

00:10:35,950 --> 00:10:40,810
terrorists for you to follow then we

00:10:38,980 --> 00:10:42,760
would take the list and we would watch

00:10:40,810 --> 00:10:44,530
it and then we would go back to the

00:10:42,760 --> 00:10:45,610
social media companies go there's a

00:10:44,530 --> 00:10:47,350
whole bunch of terrorists on your

00:10:45,610 --> 00:10:49,570
platform never like you don't know that

00:10:47,350 --> 00:10:51,670
I'm like actually your algorithm told me

00:10:49,570 --> 00:10:53,800
they were on your platform like that's

00:10:51,670 --> 00:10:55,750
how we figured it out because they know

00:10:53,800 --> 00:10:57,670
the location and they know behind the

00:10:55,750 --> 00:10:59,350
scenes in that algorithm right that

00:10:57,670 --> 00:11:02,990
you're communicating with people that

00:10:59,350 --> 00:11:05,240
look like you and like so

00:11:02,990 --> 00:11:06,589
ultimately I think part of it is and we

00:11:05,240 --> 00:11:08,810
came to this in the government

00:11:06,589 --> 00:11:11,360
intelligence space is you have to merge

00:11:08,810 --> 00:11:13,580
technicians with people who think like

00:11:11,360 --> 00:11:16,160
bad so intelligence analyst and

00:11:13,580 --> 00:11:17,779
understand okay if I come up with this

00:11:16,160 --> 00:11:20,540
really cool thing it's gonna be really

00:11:17,779 --> 00:11:22,670
cool or 99% of only users right it's

00:11:20,540 --> 00:11:24,440
gonna be great but for one percent of

00:11:22,670 --> 00:11:27,620
users they're gonna come up with some

00:11:24,440 --> 00:11:28,370
awful thing that they can do and what

00:11:27,620 --> 00:11:30,800
might that be

00:11:28,370 --> 00:11:33,200
you know from the outset I think that's

00:11:30,800 --> 00:11:35,120
the ad you know if the recommendations

00:11:33,200 --> 00:11:37,730
how do we get clickbait arms you know

00:11:35,120 --> 00:11:39,740
that if I make a fake news story that's

00:11:37,730 --> 00:11:41,300
crazy at least to lots of advertising

00:11:39,740 --> 00:11:43,670
revenues there's always this sort of

00:11:41,300 --> 00:11:45,200
thing and so can we combine the human

00:11:43,670 --> 00:11:47,000
element with the great technological

00:11:45,200 --> 00:11:48,050
advances because we want people to be

00:11:47,000 --> 00:11:49,910
innovative we don't want to be

00:11:48,050 --> 00:11:52,370
constantly be looking over the shoulder

00:11:49,910 --> 00:11:56,060
going all crazy things can emerge but

00:11:52,370 --> 00:11:58,250
can we work with them that way I could I

00:11:56,060 --> 00:12:00,050
mean I don't I'm not trying to let a

00:11:58,250 --> 00:12:01,160
user off the hook but I think that we

00:12:00,050 --> 00:12:03,290
need to understand I mean I've been

00:12:01,160 --> 00:12:05,450
talking about YouTube at AI has and it's

00:12:03,290 --> 00:12:08,149
broad a spectrum is a field or

00:12:05,450 --> 00:12:11,600
surveillance and so the intention there

00:12:08,149 --> 00:12:15,649
is not to provide agency and is to sort

00:12:11,600 --> 00:12:18,200
of capture users either be they active

00:12:15,649 --> 00:12:20,480
or inactive users into a sort of web

00:12:18,200 --> 00:12:24,220
that sort of watches tries to anticipate

00:12:20,480 --> 00:12:27,110
in some cases tries to predict behavior

00:12:24,220 --> 00:12:28,550
so this is not the you know I'm not

00:12:27,110 --> 00:12:29,839
trying to make the case that the you

00:12:28,550 --> 00:12:31,820
know that actions no matter and if

00:12:29,839 --> 00:12:33,140
you're following a terrorist that that

00:12:31,820 --> 00:12:36,490
you might be interested in following

00:12:33,140 --> 00:12:39,560
other terrorists but these are you know

00:12:36,490 --> 00:12:41,300
systems that are meant to encourage that

00:12:39,560 --> 00:12:44,120
they're not meant to encourage other

00:12:41,300 --> 00:12:46,910
things necessarily and to capture

00:12:44,120 --> 00:12:48,980
moreover I think talking across platform

00:12:46,910 --> 00:12:51,860
technology platforms talking across

00:12:48,980 --> 00:12:55,160
social sites the aim here is a kind of

00:12:51,860 --> 00:12:56,990
surveillance and so in that case you

00:12:55,160 --> 00:12:58,279
know it's less about the volition of the

00:12:56,990 --> 00:13:00,640
user and I think that we should be

00:12:58,279 --> 00:13:04,820
careful and and sort of you know

00:13:00,640 --> 00:13:07,250
suggesting that the user has here is

00:13:04,820 --> 00:13:09,079
very much you know in them you know in

00:13:07,250 --> 00:13:10,610
social research or the social sciences

00:13:09,079 --> 00:13:12,260
you talk about the structure and agency

00:13:10,610 --> 00:13:14,930
problem right and so it's always the

00:13:12,260 --> 00:13:15,990
sense that the agent has you know a

00:13:14,930 --> 00:13:18,180
limited

00:13:15,990 --> 00:13:20,610
change of agency certainly but it's

00:13:18,180 --> 00:13:23,459
increasingly constrained by what AI

00:13:20,610 --> 00:13:24,899
systems create as the field of play look

00:13:23,459 --> 00:13:27,420
what the guys were letting off the hook

00:13:24,899 --> 00:13:29,730
here the guys who are making the

00:13:27,420 --> 00:13:31,760
algorithms I mean you mentioned you

00:13:29,730 --> 00:13:34,110
mentioned Donuts you mentioned tobacco

00:13:31,760 --> 00:13:38,730
for the food industry and the tobacco

00:13:34,110 --> 00:13:40,499
industry are heavily regulated and until

00:13:38,730 --> 00:13:42,269
now at least

00:13:40,499 --> 00:13:43,739
companies have you know at least in

00:13:42,269 --> 00:13:44,879
America they've gotten away with quite a

00:13:43,739 --> 00:13:46,649
bit maybe

00:13:44,879 --> 00:13:50,389
is that something we need to stop

00:13:46,649 --> 00:13:50,389
thinking about I don't think

00:13:50,430 --> 00:13:53,760
from the regulator's perspective they

00:13:52,410 --> 00:13:56,430
understand tech as well as they

00:13:53,760 --> 00:13:58,290
understand doughnuts in fact you know I

00:13:56,430 --> 00:14:01,200
mean we saw that when you know Mark

00:13:58,290 --> 00:14:03,870
Zuckerberg lied to the Senate it's Dave

00:14:01,200 --> 00:14:05,820
he turns out he knows Facebook better

00:14:03,870 --> 00:14:09,690
than anyone else in Facebook right and

00:14:05,820 --> 00:14:11,400
and there were very few this speaks to

00:14:09,690 --> 00:14:13,140
all sorts of political dynamics about

00:14:11,400 --> 00:14:15,420
incumbency and things like that and

00:14:13,140 --> 00:14:17,370
donors but there are very few that

00:14:15,420 --> 00:14:19,710
really understood how to ask questions I

00:14:17,370 --> 00:14:22,440
think from my conversations in the u.s.

00:14:19,710 --> 00:14:26,370
they really don't know what to do not

00:14:22,440 --> 00:14:28,050
build the industry and and also there's

00:14:26,370 --> 00:14:30,120
a undercurrent at least the United

00:14:28,050 --> 00:14:31,740
States that certain parties want the

00:14:30,120 --> 00:14:34,290
industry build so they can move their

00:14:31,740 --> 00:14:36,300
audiences apps so they can use

00:14:34,290 --> 00:14:38,460
artificial intelligence better than all

00:14:36,300 --> 00:14:40,830
the minds of their supporters you know

00:14:38,460 --> 00:14:43,339
there's this also for a dynamic of the

00:14:40,830 --> 00:14:46,140
balkanization Internet and social media

00:14:43,339 --> 00:14:48,630
so I think it's going to be tough a much

00:14:46,140 --> 00:14:51,480
tougher conundrum then things that are

00:14:48,630 --> 00:14:53,940
like safety public safety issue

00:14:51,480 --> 00:14:56,670
so I mentioned well those of you who

00:14:53,940 --> 00:14:58,020
were here yesterday this will be vaguely

00:14:56,670 --> 00:14:59,310
familiar I mentioned yesterday is

00:14:58,020 --> 00:15:01,740
something I've been thinking about quite

00:14:59,310 --> 00:15:04,620
a bit recently and looking into which is

00:15:01,740 --> 00:15:06,810
um credit scoring and edit regulation in

00:15:04,620 --> 00:15:08,940
the United States and I've been looking

00:15:06,810 --> 00:15:13,830
at through this late 60 from the late

00:15:08,940 --> 00:15:17,390
60s late seventies there was low

00:15:13,830 --> 00:15:20,460
Senate hearings house hearings and those

00:15:17,390 --> 00:15:21,330
there were two landmark bills that

00:15:20,460 --> 00:15:23,370
proposed there was a Fair Credit

00:15:21,330 --> 00:15:25,800
Reporting Act which is a sort of

00:15:23,370 --> 00:15:28,890
protonated Protection Act when there was

00:15:25,800 --> 00:15:30,720
the Equal Credit Opportunity Act and I

00:15:28,890 --> 00:15:32,580
think there are many parallels the

00:15:30,720 --> 00:15:35,520
credit scoring industry was founded on

00:15:32,580 --> 00:15:37,740
the basis of ingesting data using

00:15:35,520 --> 00:15:40,440
algorithms to predict human behavior and

00:15:37,740 --> 00:15:41,850
indeed in its present form to model

00:15:40,440 --> 00:15:46,940
human behavior to make you a better

00:15:41,850 --> 00:15:46,940
better borrower better debt of

00:15:47,130 --> 00:15:51,740
perhaps I mean to the idea that

00:15:50,060 --> 00:15:54,200
doesn't understand the idea that this

00:15:51,740 --> 00:15:56,600
stuff is too complicated if they could

00:15:54,200 --> 00:15:59,350
do it 50 or 40 odd years ago I don't see

00:15:56,600 --> 00:15:59,350
why they can't do it today

00:15:59,470 --> 00:16:02,920
I think

00:16:00,970 --> 00:16:05,170
the difficult question in that

00:16:02,920 --> 00:16:06,820
conversation and I think Alondra your

00:16:05,170 --> 00:16:08,800
point was really leading to this it's

00:16:06,820 --> 00:16:10,960
like what is AI what is machine learning

00:16:08,800 --> 00:16:13,870
you said in the millet surveillance it's

00:16:10,960 --> 00:16:15,670
automated decision-making and I think we

00:16:13,870 --> 00:16:18,300
also have to ask the question what

00:16:15,670 --> 00:16:21,130
Warren's automated decision-making and

00:16:18,300 --> 00:16:22,630
you know Rocco's gonna make backhoe

00:16:21,130 --> 00:16:24,520
that's their industry but the tech

00:16:22,630 --> 00:16:27,130
industry does not have to deploy machine

00:16:24,520 --> 00:16:30,370
learning systems everywhere and actually

00:16:27,130 --> 00:16:33,010
some of these applications are poorly

00:16:30,370 --> 00:16:35,170
suited for machine learning in the first

00:16:33,010 --> 00:16:38,290
place and I think a question we have to

00:16:35,170 --> 00:16:40,780
ask is when you see a new AI tool a new

00:16:38,290 --> 00:16:42,670
machine learning phase system questions

00:16:40,780 --> 00:16:43,420
like is this a good idea in the first

00:16:42,670 --> 00:16:46,480
place

00:16:43,420 --> 00:16:48,520
ai doesn't have to be embedded in all

00:16:46,480 --> 00:16:52,510
the technology we use and so when we

00:16:48,520 --> 00:16:55,570
look at the Amazon machine learning

00:16:52,510 --> 00:16:58,180
resume program in which they trained a

00:16:55,570 --> 00:17:00,040
machine learning program to see what

00:16:58,180 --> 00:17:02,110
types of resumes would make good

00:17:00,040 --> 00:17:04,090
candidates for Amazon and basically

00:17:02,110 --> 00:17:06,100
their recommendations was if the

00:17:04,090 --> 00:17:07,960
person's name Jared and if they play

00:17:06,100 --> 00:17:10,750
lacrosse they're probably gonna be great

00:17:07,960 --> 00:17:13,180
candidate that's stupid but it was also

00:17:10,750 --> 00:17:15,340
a bad decision like you actually

00:17:13,180 --> 00:17:17,860
probably don't need to have a machine

00:17:15,340 --> 00:17:20,170
learning system given the state of the

00:17:17,860 --> 00:17:22,570
technology and given the problem and I

00:17:20,170 --> 00:17:24,100
think on top of educating policy makers

00:17:22,570 --> 00:17:27,790
you also need to take a step back and

00:17:24,100 --> 00:17:30,460
say okay AI is great but is this a good

00:17:27,790 --> 00:17:33,310
application given a the state of the

00:17:30,460 --> 00:17:35,230
field the state of our technology and a

00:17:33,310 --> 00:17:38,270
the question you're trying to tackle not

00:17:35,230 --> 00:17:41,660
everything needs to be an AI problem

00:17:38,270 --> 00:17:44,150
so you yeah I wanted to talk about the

00:17:41,660 --> 00:17:46,490
make a comment on I think Clint's really

00:17:44,150 --> 00:17:49,040
good point about the expert lack of

00:17:46,490 --> 00:17:50,450
expertise I mean we are I think we need

00:17:49,040 --> 00:17:52,280
to accept that we're in a moment where

00:17:50,450 --> 00:17:54,230
the level of expertise needed to be able

00:17:52,280 --> 00:17:55,730
to regulate these things is like nothing

00:17:54,230 --> 00:17:57,290
we've ever seen before and so in that

00:17:55,730 --> 00:17:59,720
regard it's totally different from the

00:17:57,290 --> 00:18:01,970
historical examples from the late 20th

00:17:59,720 --> 00:18:03,740
century that you raise but we do have

00:18:01,970 --> 00:18:05,810
and this is a u.s. context you know

00:18:03,740 --> 00:18:07,370
other paradigms to think about so in the

00:18:05,810 --> 00:18:09,140
u.s. we still actually have maritime

00:18:07,370 --> 00:18:11,480
courts there's a sort of sense that

00:18:09,140 --> 00:18:12,800
maritime law and that agrees is so

00:18:11,480 --> 00:18:14,900
distinctive that it's a completely

00:18:12,800 --> 00:18:17,930
different court system but we can't

00:18:14,900 --> 00:18:19,850
imagine having a court system for you

00:18:17,930 --> 00:18:22,310
know for DNA expertise or for data

00:18:19,850 --> 00:18:25,700
expertise so that's I think one thing to

00:18:22,310 --> 00:18:28,310
talk about or think about related to the

00:18:25,700 --> 00:18:31,040
point about the the prior legislation I

00:18:28,310 --> 00:18:32,270
would want and while I take your point

00:18:31,040 --> 00:18:34,130
that there were moments when we were

00:18:32,270 --> 00:18:35,900
able in the u.s. to sort of move the

00:18:34,130 --> 00:18:38,480
needle on protective protections for

00:18:35,900 --> 00:18:40,310
consumers um there was still significant

00:18:38,480 --> 00:18:42,050
discrimination in the United States with

00:18:40,310 --> 00:18:43,870
regards to credit reporting and access

00:18:42,050 --> 00:18:46,550
to credit right particularly for

00:18:43,870 --> 00:18:49,520
minority communities a United States and

00:18:46,550 --> 00:18:51,560
indeed even with these in place the 2008

00:18:49,520 --> 00:18:53,090
you know financial crisis actually

00:18:51,560 --> 00:18:55,460
really is an outcome of the failure of

00:18:53,090 --> 00:18:57,230
these to actually protect people and

00:18:55,460 --> 00:18:59,320
these regulations to be as nimble in

00:18:57,230 --> 00:19:02,360
depth as deaf as they needed to be as

00:18:59,320 --> 00:19:04,580
the market got more technologized in

00:19:02,360 --> 00:19:07,520
sort of data centric so you know I think

00:19:04,580 --> 00:19:09,860
that we want to think about and this is

00:19:07,520 --> 00:19:11,890
a little bit to Camille's point you know

00:19:09,860 --> 00:19:14,750
the ways and you know what's the word

00:19:11,890 --> 00:19:18,230
what's warranted for AI decision-making

00:19:14,750 --> 00:19:22,430
and in what places is it only going to

00:19:18,230 --> 00:19:25,610
exacerbate discrimination and and what

00:19:22,430 --> 00:19:29,150
places it will even regulation in the

00:19:25,610 --> 00:19:31,610
case of these smart that you may not do

00:19:29,150 --> 00:19:35,600
the work of the sort of politics and

00:19:31,610 --> 00:19:37,490
social justice piece you know that

00:19:35,600 --> 00:19:38,780
regulation alone can't accomplish that

00:19:37,490 --> 00:19:40,310
and so I think that all of these

00:19:38,780 --> 00:19:42,770
conversations need to be had about

00:19:40,310 --> 00:19:45,050
expertise and technology but always I

00:19:42,770 --> 00:19:46,970
think in therefore with a kind of

00:19:45,050 --> 00:19:49,900
political vision of what we want the

00:19:46,970 --> 00:19:51,400
world to look like as part of that

00:19:49,900 --> 00:19:54,720
here in here from both of you is correct

00:19:51,400 --> 00:19:58,290
me if I'm wrong but Hooper

00:19:54,720 --> 00:20:00,540
is is an argument that lets everybody

00:19:58,290 --> 00:20:01,830
stop for a couple of minutes note let's

00:20:00,540 --> 00:20:03,720
slow down let's think about this little

00:20:01,830 --> 00:20:05,970
no let's stop think about it and then

00:20:03,720 --> 00:20:08,610
carry on once we could figure things out

00:20:05,970 --> 00:20:10,950
do you think that's a version of your

00:20:08,610 --> 00:20:13,050
argument

00:20:10,950 --> 00:20:14,970
I would say let's imagine stopping I

00:20:13,050 --> 00:20:16,560
mean I think we are freaked out by the

00:20:14,970 --> 00:20:17,790
idea of stopping like people probably

00:20:16,560 --> 00:20:20,460
like you know like we we couldn't

00:20:17,790 --> 00:20:22,530
possibly spot and so even they take that

00:20:20,460 --> 00:20:24,450
as a not experiment like why don't we

00:20:22,530 --> 00:20:26,190
take that as a political thought

00:20:24,450 --> 00:20:28,200
experiment and sort of imagine what when

00:20:26,190 --> 00:20:30,870
would or would not happen if you know as

00:20:28,200 --> 00:20:34,910
Mila suggested we took AI out of some of

00:20:30,870 --> 00:20:34,910
the places it doesn't seralini

00:20:35,150 --> 00:20:41,160
but we did this in cybersecurity I mean

00:20:38,640 --> 00:20:42,840
we started making products that had tons

00:20:41,160 --> 00:20:45,420
of vulnerabilities in it and you know

00:20:42,840 --> 00:20:47,070
industries financial manufacturing

00:20:45,420 --> 00:20:48,660
whatever might be they lost tons of

00:20:47,070 --> 00:20:50,040
money due to hacking and then they

00:20:48,660 --> 00:20:52,020
started building policies already said

00:20:50,040 --> 00:20:53,880
okay we're gonna build products and then

00:20:52,020 --> 00:20:55,590
we're gonna do a QA QC and we're gonna

00:20:53,880 --> 00:20:56,490
do Boehner ability testing those sort of

00:20:55,590 --> 00:21:00,360
things

00:20:56,490 --> 00:21:02,730
quality control X um before we deploy

00:21:00,360 --> 00:21:05,250
that out to market right and the

00:21:02,730 --> 00:21:06,990
argument of technologists was always we

00:21:05,250 --> 00:21:08,580
will be behind the market as we don't

00:21:06,990 --> 00:21:10,230
want its product fast enough but they

00:21:08,580 --> 00:21:12,000
started to realize that if you launch a

00:21:10,230 --> 00:21:13,800
product and then you lose eight hundred

00:21:12,000 --> 00:21:15,120
million dollars to hackers and

00:21:13,800 --> 00:21:18,750
experience that you're not really making

00:21:15,120 --> 00:21:20,100
a profit you know in the long run we can

00:21:18,750 --> 00:21:22,920
do this though with artificial

00:21:20,100 --> 00:21:25,110
intelligence too but we having at the

00:21:22,920 --> 00:21:26,520
market you know the industry has not

00:21:25,110 --> 00:21:28,080
gotten around it they're just like well

00:21:26,520 --> 00:21:30,570
we don't do it somebody else will and we

00:21:28,080 --> 00:21:31,800
don't get their person home and well if

00:21:30,570 --> 00:21:35,220
you get there first you might actually

00:21:31,800 --> 00:21:37,289
be ruined you know if you're like

00:21:35,220 --> 00:21:38,880
what was Facebook's bono move fast and

00:21:37,289 --> 00:21:40,350
break Bank and they did they broke the

00:21:38,880 --> 00:21:41,460
most important thing which was press

00:21:40,350 --> 00:21:43,309
right

00:21:41,460 --> 00:21:45,929
nothing else mattered really but trust

00:21:43,309 --> 00:21:48,000
so if you move so fast that you just

00:21:45,929 --> 00:21:51,360
break us then you don't have an industry

00:21:48,000 --> 00:21:52,409
oh you don't have sharing you have

00:21:51,360 --> 00:21:53,940
people saying maybe we need to

00:21:52,409 --> 00:21:57,990
nationalize this to break it into

00:21:53,940 --> 00:21:59,940
absolute so if any sort of in

00:21:57,990 --> 00:22:02,040
that you see take off like a rocket I'm

00:21:59,940 --> 00:22:04,380
always waiting for it to come right down

00:22:02,040 --> 00:22:05,550
like a rocket as well because you're

00:22:04,380 --> 00:22:08,040
you're likely building those

00:22:05,550 --> 00:22:10,020
vulnerabilities in over I think we can

00:22:08,040 --> 00:22:12,480
come to you know a process where you're

00:22:10,020 --> 00:22:14,670
saying okay what's the downside of this

00:22:12,480 --> 00:22:16,830
you know AI sword application how do we

00:22:14,670 --> 00:22:19,559
filled it in a way to kind understand I

00:22:16,830 --> 00:22:22,260
think the wrong incorrect so one of the

00:22:19,559 --> 00:22:24,809
big one of the big worry worries about

00:22:22,260 --> 00:22:26,460
here has been its effects on society the

00:22:24,809 --> 00:22:30,360
other one that we haven't touched on yet

00:22:26,460 --> 00:22:31,980
is jobs that's another big bogeyman is

00:22:30,360 --> 00:22:33,900
that another reason do you think the

00:22:31,980 --> 00:22:35,550
slowdown or is that an or do we need to

00:22:33,900 --> 00:22:37,700
think about jobs as well in a different

00:22:35,550 --> 00:22:37,700
way

00:22:38,080 --> 00:22:43,660
right so I don't know how I would say

00:22:40,630 --> 00:22:45,430
that he wants to speak lowest I did hi

00:22:43,660 --> 00:22:47,470
so I came from the Army and I was an

00:22:45,430 --> 00:22:49,660
infantry guy which you should ask why

00:22:47,470 --> 00:22:51,580
I'm on the state on that dumb where I

00:22:49,660 --> 00:22:54,370
mostly carried heavy things around in

00:22:51,580 --> 00:22:57,520
the woods but I I go to my infantry

00:22:54,370 --> 00:22:59,170
friends and it's about AI and robots of

00:22:57,520 --> 00:23:00,430
robot you know unmanned systems and I'm

00:22:59,170 --> 00:23:02,560
like you guys are gonna be the best

00:23:00,430 --> 00:23:04,510
security guards at your own base some

00:23:02,560 --> 00:23:06,430
day and they look at me they're like

00:23:04,510 --> 00:23:09,160
wait I am and I'm like man this is all

00:23:06,430 --> 00:23:11,790
going to become a I you know robots

00:23:09,160 --> 00:23:14,320
you're essentially going to rely on

00:23:11,790 --> 00:23:17,080
increasingly to technology you know to a

00:23:14,320 --> 00:23:22,149
certain extent and the adoption of that

00:23:17,080 --> 00:23:24,279
over the last when in 19 2000

00:23:22,149 --> 00:23:25,569
who doesn't want i was the infantry

00:23:24,279 --> 00:23:28,149
company commander and they made us go

00:23:25,569 --> 00:23:31,419
down to an airfield and watch a grown

00:23:28,149 --> 00:23:34,329
fly around in that airfield and we were

00:23:31,419 --> 00:23:37,269
like this is his bro

00:23:34,329 --> 00:23:39,009
let me tell you what that has overtaken

00:23:37,269 --> 00:23:40,839
where they're like you don't need

00:23:39,009 --> 00:23:42,369
soldiers so much anymore you know we

00:23:40,839 --> 00:23:45,429
have these automated systems that didn't

00:23:42,369 --> 00:23:47,289
do a lot of this stuff and we like

00:23:45,429 --> 00:23:49,539
couldn't spend five minutes to watch

00:23:47,289 --> 00:23:50,919
this thing it changed the entire global

00:23:49,539 --> 00:23:53,739
war on terror but I think it speaks more

00:23:50,919 --> 00:23:56,709
to that is uh just one of many

00:23:53,739 --> 00:23:58,749
industries where the human you know

00:23:56,709 --> 00:24:00,579
dimension of it the identity of being a

00:23:58,749 --> 00:24:03,009
worker is a big part of a lot of things

00:24:00,579 --> 00:24:06,189
see that is gonna be just changed

00:24:03,009 --> 00:24:07,719
dramatically with this AI so we have a

00:24:06,189 --> 00:24:10,619
p.m. at the Social Science Research

00:24:07,719 --> 00:24:13,059
Council many projects some of which are

00:24:10,619 --> 00:24:14,649
international and one of them is looking

00:24:13,059 --> 00:24:16,179
at geopolitics and technology are

00:24:14,649 --> 00:24:17,769
beginning to look at it and one of the

00:24:16,179 --> 00:24:21,009
things about the drone nearly in the

00:24:17,769 --> 00:24:22,989
Middle East is that it's not you know

00:24:21,009 --> 00:24:24,879
they're being used to also drop food and

00:24:22,989 --> 00:24:26,979
because you don't have humans there I

00:24:24,879 --> 00:24:28,959
think communities are you know freaked

00:24:26,979 --> 00:24:31,659
out is it are these the drones that kill

00:24:28,959 --> 00:24:33,729
or the drones that feed and when you

00:24:31,659 --> 00:24:36,159
don't have soldiers or you don't I mean

00:24:33,729 --> 00:24:38,919
as complicated as that is during times

00:24:36,159 --> 00:24:40,569
of war I know whose job is it to explain

00:24:38,919 --> 00:24:42,459
that or when you're dropping pamphlet

00:24:40,569 --> 00:24:44,049
that to a community that might not be

00:24:42,459 --> 00:24:46,299
fully literate like how do they even

00:24:44,049 --> 00:24:48,890
understand again is it the drone that

00:24:46,299 --> 00:24:50,809
kills or their own that feeds

00:24:48,890 --> 00:24:53,270
I think to your question about like

00:24:50,809 --> 00:24:54,890
should we stop I'm having a heart I'm

00:24:53,270 --> 00:24:58,910
thinking through what it would mean da

00:24:54,890 --> 00:25:01,790
um I was recently at Google where I was

00:24:58,910 --> 00:25:03,980
working not only on on AI and I when I

00:25:01,790 --> 00:25:07,580
was there when it will made this big

00:25:03,980 --> 00:25:10,549
ship to becoming an AI first company and

00:25:07,580 --> 00:25:13,990
when such a hemas move store saying

00:25:10,549 --> 00:25:16,549
everything we do now going you've got AI

00:25:13,990 --> 00:25:18,200
you don't wonder when that's gonna stop

00:25:16,549 --> 00:25:21,320
you're like the industry is heading

00:25:18,200 --> 00:25:23,960
there full speed the industry the VC and

00:25:21,320 --> 00:25:26,840
it's it's more like what are we gonna

00:25:23,960 --> 00:25:28,820
ask in the face of that and it's

00:25:26,840 --> 00:25:31,250
questions like well have you been

00:25:28,820 --> 00:25:33,559
thoughtful have you done quality control

00:25:31,250 --> 00:25:35,150
do you know what is the social impact of

00:25:33,559 --> 00:25:36,890
your product you know if different

00:25:35,150 --> 00:25:39,530
categories of people are being affected

00:25:36,890 --> 00:25:41,990
differently by the machine learning

00:25:39,530 --> 00:25:44,090
models that do you develop does that

00:25:41,990 --> 00:25:46,520
need it to be an ml based decision and

00:25:44,090 --> 00:25:48,700
so I think it's more how are we gonna

00:25:46,520 --> 00:25:51,040
add it

00:25:48,700 --> 00:25:55,570
ranking at this stage I think stopping

00:25:51,040 --> 00:25:59,350
it whatever it could mean I'm not seeing

00:25:55,570 --> 00:26:01,360
that but so the questions you all know

00:25:59,350 --> 00:26:02,800
are they being us and are they being

00:26:01,360 --> 00:26:06,190
answered

00:26:02,800 --> 00:26:08,680
so some of them are being asked and I

00:26:06,190 --> 00:26:09,760
think there's a big question on what

00:26:08,680 --> 00:26:12,460
does it mean to do this

00:26:09,760 --> 00:26:15,610
proactively and so what we've seen a lot

00:26:12,460 --> 00:26:17,380
off these recent years is a product is

00:26:15,610 --> 00:26:19,450
being developed and then suddenly

00:26:17,380 --> 00:26:22,240
everyone looks at it and being like wow

00:26:19,450 --> 00:26:22,870
this is dumb like while the impact on

00:26:22,240 --> 00:26:24,820
society

00:26:22,870 --> 00:26:27,070
this is discriminating for certain

00:26:24,820 --> 00:26:29,080
categories of people how the did

00:26:27,070 --> 00:26:30,820
you put this on the market also this was

00:26:29,080 --> 00:26:32,670
a terrible idea in the first place so

00:26:30,820 --> 00:26:34,840
that has happened a few times

00:26:32,670 --> 00:26:37,480
every time it's kind of fun to watch

00:26:34,840 --> 00:26:39,640
also a bit disheartening stop those down

00:26:37,480 --> 00:26:41,890
here and there people you know like fun

00:26:39,640 --> 00:26:43,510
series of articles and we're gonna have

00:26:41,890 --> 00:26:45,520
like a little bit more fun and then I

00:26:43,510 --> 00:26:47,530
think the question that both young and I

00:26:45,520 --> 00:26:49,270
were tackling internally in the industry

00:26:47,530 --> 00:26:51,550
is what does it look like to do this

00:26:49,270 --> 00:26:54,780
correctively when you develop a product

00:26:51,550 --> 00:26:57,430
what does a proper testing of the

00:26:54,780 --> 00:26:59,260
potential impact on society of your

00:26:57,430 --> 00:27:01,570
rowlett look like what does it look like

00:26:59,260 --> 00:27:04,150
to audit your training data what does it

00:27:01,570 --> 00:27:06,430
look like to audit any annotation system

00:27:04,150 --> 00:27:08,020
around your training data what does it

00:27:06,430 --> 00:27:10,540
look like to audit or discriminate

00:27:08,020 --> 00:27:13,390
impact and those practices are being

00:27:10,540 --> 00:27:16,270
slowly developed what I regret is that

00:27:13,390 --> 00:27:19,150
they're currently rooted in computer

00:27:16,270 --> 00:27:21,340
science and so the field it's tackling

00:27:19,150 --> 00:27:23,320
what does it look like to proactively

00:27:21,340 --> 00:27:25,690
think about it is really computer

00:27:23,320 --> 00:27:28,150
science and that's great but we also

00:27:25,690 --> 00:27:31,050
need design thinking about proactive

00:27:28,150 --> 00:27:33,370
tackling of AI impact we also need

00:27:31,050 --> 00:27:36,340
sociological thinking about it and we

00:27:33,370 --> 00:27:38,260
need practices that go beyond just we're

00:27:36,340 --> 00:27:40,660
gonna have new system to test out

00:27:38,260 --> 00:27:43,510
different computer science definitions

00:27:40,660 --> 00:27:45,790
of fairness we need a more

00:27:43,510 --> 00:27:48,250
interdisciplinary approach to what it

00:27:45,790 --> 00:27:50,170
can mean to do corrected fairness

00:27:48,250 --> 00:27:51,860
testing for the products that are going

00:27:50,170 --> 00:27:53,780
to be put out there you

00:27:51,860 --> 00:27:55,490
is kind of what you guys working on at

00:27:53,780 --> 00:27:57,770
the center of human technology room yes

00:27:55,490 --> 00:28:00,559
exactly so inside the companies or

00:27:57,770 --> 00:28:03,380
incentives are really bad through that

00:28:00,559 --> 00:28:06,770
because you do too much testing it's

00:28:03,380 --> 00:28:10,670
being slowed down you release an exit

00:28:06,770 --> 00:28:12,770
person is very badly aligned in it so

00:28:10,670 --> 00:28:17,480
you need people from the outside who

00:28:12,770 --> 00:28:20,210
actually are measuring that pressure

00:28:17,480 --> 00:28:23,390
instantly so it means like was like

00:28:20,210 --> 00:28:27,169
governments in his books like normal

00:28:23,390 --> 00:28:31,640
users on nonprofit and actually we know

00:28:27,169 --> 00:28:34,850
but in Russia like we can measure social

00:28:31,640 --> 00:28:35,929
impact on of even a a minute tell us for

00:28:34,850 --> 00:28:37,760
the people in this room were not

00:28:35,929 --> 00:28:39,470
familiar with the Center for human

00:28:37,760 --> 00:28:41,720
technology do you want to tell us what

00:28:39,470 --> 00:28:46,160
it was set up to do and how it work so

00:28:41,720 --> 00:28:47,960
here's a synthetic human dignity was was

00:28:46,160 --> 00:28:50,900
wrote a McNamee on Christian

00:28:47,960 --> 00:28:52,880
missionaries and so the idea that our

00:28:50,900 --> 00:28:55,010
brain

00:28:52,880 --> 00:28:57,740
haven't evolved too much in the last

00:28:55,010 --> 00:29:01,580
10,000 years if also really slowly

00:28:57,740 --> 00:29:04,580
didn't get a security and our tools are

00:29:01,580 --> 00:29:07,090
evolving at later on we're having this

00:29:04,580 --> 00:29:07,090
great light

00:29:08,639 --> 00:29:15,170
knock yourself in right to exploit

00:29:12,970 --> 00:29:19,850
vulnerabilities on a look

00:29:15,170 --> 00:29:22,060
those people a tourism using this been

00:29:19,850 --> 00:29:22,060
nervous

00:29:22,549 --> 00:29:29,790
like that so how we design technology

00:29:27,750 --> 00:29:32,750
that protects these vulnerabilities

00:29:29,790 --> 00:29:32,750
instead of

00:29:33,270 --> 00:29:38,490
how do you as in do work with the

00:29:35,460 --> 00:29:40,680
companies you write white papers also so

00:29:38,490 --> 00:29:44,520
when I'm working on right now is like

00:29:40,680 --> 00:29:47,070
how to measure measures impacting on

00:29:44,520 --> 00:29:50,700
Caesar filner abilities I don't say them

00:29:47,070 --> 00:29:55,800
one by one and working either with

00:29:50,700 --> 00:29:58,980
companies or if looking alternative

00:29:55,800 --> 00:30:01,670
produced for the Deaf lesbian ever this

00:29:58,980 --> 00:30:01,670
exploit

00:30:01,799 --> 00:30:06,899
under Camille mentioned sociologists

00:30:04,139 --> 00:30:09,029
being a sociology being a part of this

00:30:06,899 --> 00:30:11,489
thinking from your point of view how

00:30:09,029 --> 00:30:13,529
would that work I think the true

00:30:11,489 --> 00:30:15,739
beginning to be built out and I think

00:30:13,529 --> 00:30:18,659
there's there's lots of I think

00:30:15,739 --> 00:30:21,629
perspectives that a social science even

00:30:18,659 --> 00:30:23,759
more general perspective brings so one

00:30:21,629 --> 00:30:26,070
would be around social movements and

00:30:23,759 --> 00:30:28,649
social behavior a social collective

00:30:26,070 --> 00:30:30,929
behavior another around issues of

00:30:28,649 --> 00:30:32,429
inequality and discrimination for

00:30:30,929 --> 00:30:35,039
certain so there are particular

00:30:32,429 --> 00:30:36,629
questions that a social scientist asks

00:30:35,039 --> 00:30:39,720
about the world about the human impacts

00:30:36,629 --> 00:30:42,109
of things I think that need to be asked

00:30:39,720 --> 00:30:44,129
in there and the AI space and need to be

00:30:42,109 --> 00:30:45,809
anticipated and I think that we need to

00:30:44,129 --> 00:30:49,289
think about a sort of division of labor

00:30:45,809 --> 00:30:51,389
to go to meals point that you know we

00:30:49,289 --> 00:30:53,220
certainly need people with you know iam

00:30:51,389 --> 00:30:55,519
and meals expertise to sort of be

00:30:53,220 --> 00:30:57,659
thinking about systems and trying to

00:30:55,519 --> 00:31:00,389
understand the black box of the

00:30:57,659 --> 00:31:02,100
algorithms but I think for our part you

00:31:00,389 --> 00:31:05,340
know social scientists need to be trying

00:31:02,100 --> 00:31:06,809
to anticipate you know what the changes

00:31:05,340 --> 00:31:08,429
might be and to sort of get ahead of

00:31:06,809 --> 00:31:11,460
that so to the extent that social

00:31:08,429 --> 00:31:13,470
science policy pieces how do you how do

00:31:11,460 --> 00:31:16,970
we think about sort of reimagining a

00:31:13,470 --> 00:31:20,009
regulatory or policy regime that would

00:31:16,970 --> 00:31:22,649
you know help to anticipate and put some

00:31:20,009 --> 00:31:24,960
guardrails on some of this so I think in

00:31:22,649 --> 00:31:26,249
some ways it's reimagining I think the

00:31:24,960 --> 00:31:28,999
things that the social sciences have

00:31:26,249 --> 00:31:31,409
already done but or a moment in which

00:31:28,999 --> 00:31:32,759
product production and technological

00:31:31,409 --> 00:31:35,460
development is happening much more

00:31:32,759 --> 00:31:37,499
quickly and so the work that we do needs

00:31:35,460 --> 00:31:40,230
to be both deeply empirical but also I

00:31:37,499 --> 00:31:41,940
think lightly speculative hmm relative

00:31:40,230 --> 00:31:43,410
sociology

00:31:41,940 --> 00:31:45,390
yeah

00:31:43,410 --> 00:31:47,210
I'd like to come back to jobs for a

00:31:45,390 --> 00:31:49,740
second just make the argument that I

00:31:47,210 --> 00:31:52,910
mean you're a Clinton Lord those guys

00:31:49,740 --> 00:31:52,910
are going to lose their jobs

00:31:53,260 --> 00:31:58,950
ideally we'll be redeployed to more

00:31:55,300 --> 00:31:58,950
productive work the Schaefer work

00:31:59,760 --> 00:32:04,970
warfare in your example also becomes

00:32:03,270 --> 00:32:07,920
more efficient

00:32:04,970 --> 00:32:09,990
ideally less bloody and depends on the

00:32:07,920 --> 00:32:12,570
outcomes you're going for but in any in

00:32:09,990 --> 00:32:15,180
any industry where jobs are replaced by

00:32:12,570 --> 00:32:19,020
automation and this is going on for

00:32:15,180 --> 00:32:21,030
hundreds of decades anyway what tends to

00:32:19,020 --> 00:32:22,530
happen is that the industry gets bigger

00:32:21,030 --> 00:32:25,890
there's more people who have access or

00:32:22,530 --> 00:32:27,930
whatever that is and employees are

00:32:25,890 --> 00:32:29,940
redirected into some other form of work

00:32:27,930 --> 00:32:34,160
um so how much should we really be

00:32:29,940 --> 00:32:35,960
worrying about coming a AI job popular

00:32:34,160 --> 00:32:38,150
I love that question because I don't

00:32:35,960 --> 00:32:41,630
know how we make America great again if

00:32:38,150 --> 00:32:42,980
we are doing that I mean we're we're

00:32:41,630 --> 00:32:45,410
looking at major technological

00:32:42,980 --> 00:32:47,660
advancements that are going across ways

00:32:45,410 --> 00:32:50,440
against some of the currents that we

00:32:47,660 --> 00:32:54,290
have in terms of our slide right now and

00:32:50,440 --> 00:32:57,040
I know in the u.s. context and you see

00:32:54,290 --> 00:32:57,040
it a little bit here

00:32:57,669 --> 00:33:02,589
into people they want to go back the

00:33:00,129 --> 00:33:04,659
wire mills of making our own truck and

00:33:02,589 --> 00:33:06,190
things like that and that's a fantasy

00:33:04,659 --> 00:33:07,779
you know it's not really gonna happen

00:33:06,190 --> 00:33:09,999
not only are they not still to do those

00:33:07,779 --> 00:33:13,749
older jobs you know we can't move

00:33:09,999 --> 00:33:15,070
backward right we've also been talking

00:33:13,749 --> 00:33:17,409
as if we have a decision

00:33:15,070 --> 00:33:19,499
as liberal democracies to say we're not

00:33:17,409 --> 00:33:22,450
going to do this or maybe we have voices

00:33:19,499 --> 00:33:25,450
the truth is liberal democracy is not

00:33:22,450 --> 00:33:27,399
the one that decides it

00:33:25,450 --> 00:33:29,109
the presidents of both China and Russia

00:33:27,399 --> 00:33:31,829
have said whoever gets artificial

00:33:29,109 --> 00:33:34,299
intelligence when and in the ratio

00:33:31,829 --> 00:33:36,339
quantum computing is something that they

00:33:34,299 --> 00:33:37,779
are seeking in a very dependent way and

00:33:36,339 --> 00:33:39,849
we've we were talking about scoring

00:33:37,779 --> 00:33:42,070
systems I mean China's already

00:33:39,849 --> 00:33:44,099
instituting the most advanced scoring

00:33:42,070 --> 00:33:47,079
system in the history of the world so I

00:33:44,099 --> 00:33:49,239
don't if we can make those choices like

00:33:47,079 --> 00:33:50,589
as liberal democracies there will be

00:33:49,239 --> 00:33:54,369
there is no choice choice has already

00:33:50,589 --> 00:33:56,049
been made a far and in in terms of how

00:33:54,369 --> 00:33:57,789
it plays out with jobs in warfare those

00:33:56,049 --> 00:34:00,309
sorts of things it'll have those cross

00:33:57,789 --> 00:34:02,619
currents well in places like China but

00:34:00,309 --> 00:34:05,109
they are moving forward now

00:34:02,619 --> 00:34:07,690
I mean aggressively that they are

00:34:05,109 --> 00:34:11,049
recruiting and if you I tell you as an

00:34:07,690 --> 00:34:12,669
industry if is an industry in liberal

00:34:11,049 --> 00:34:15,039
democracies you turn and say we're gonna

00:34:12,669 --> 00:34:17,500
slow down with AI and we're gonna really

00:34:15,039 --> 00:34:19,029
reinvest or whatever there are gonna be

00:34:17,500 --> 00:34:22,179
a couple of countries that show up I

00:34:19,029 --> 00:34:26,710
have a job my country make really

00:34:22,179 --> 00:34:27,700
advanced AI do you want to it so you

00:34:26,710 --> 00:34:29,829
know in the

00:34:27,700 --> 00:34:31,030
my Chrissy's is always charged for embed

00:34:29,829 --> 00:34:32,770
out ahead on these we invent the

00:34:31,030 --> 00:34:34,560
Internet and social media and all these

00:34:32,770 --> 00:34:37,990
things I don't know that that's the case

00:34:34,560 --> 00:34:39,849
a lot of countries are pursuing these

00:34:37,990 --> 00:34:42,010
technological advances and do not err

00:34:39,849 --> 00:34:44,130
and will never there I've bet some of

00:34:42,010 --> 00:34:47,820
the social impact

00:34:44,130 --> 00:34:50,859
you moon is my favorite and it's very

00:34:47,820 --> 00:34:53,809
public before and house in

00:34:50,859 --> 00:34:58,849
awesomely so what do we do about that

00:34:53,809 --> 00:35:03,880
right and I think what what was very

00:34:58,849 --> 00:35:07,299
striking me as I left Google to do this

00:35:03,880 --> 00:35:09,250
but also from the outside is how much we

00:35:07,299 --> 00:35:11,529
talk about it as like these are black

00:35:09,250 --> 00:35:14,170
boxes are so complicated they're

00:35:11,529 --> 00:35:16,660
impossible to audit it's Silicon Valley

00:35:14,170 --> 00:35:18,750
they're so hard to regulate but at the

00:35:16,660 --> 00:35:23,160
end of the day those are also

00:35:18,750 --> 00:35:25,260
acknowledgeable systems that users

00:35:23,160 --> 00:35:27,000
I still say users because like invalid

00:35:25,260 --> 00:35:30,050
thing but it actually means both I'm

00:35:27,000 --> 00:35:33,540
gonna be good and say people not users

00:35:30,050 --> 00:35:35,580
people are totally able to do wonderful

00:35:33,540 --> 00:35:37,230
innovation around it and I think what

00:35:35,580 --> 00:35:40,110
matters and we've seen that right so for

00:35:37,230 --> 00:35:44,010
instance the first ProPublica work that

00:35:40,110 --> 00:35:46,380
audited at the a I systems for

00:35:44,010 --> 00:35:49,860
predictive policing managed to make a

00:35:46,380 --> 00:35:51,840
wonderful in-depth inquiry without zero

00:35:49,860 --> 00:35:53,970
access to the system news by poking at

00:35:51,840 --> 00:35:57,060
it from the outside Guillaume is able to

00:35:53,970 --> 00:35:59,010
do really fantastic work data-driven and

00:35:57,060 --> 00:36:00,840
put on a date of evidence on the table

00:35:59,010 --> 00:36:02,490
without having magic access from the

00:36:00,840 --> 00:36:04,470
inside and I think we need to keep that

00:36:02,490 --> 00:36:07,510
in mind right it's about demystify this

00:36:04,470 --> 00:36:09,400
system empower the user

00:36:07,510 --> 00:36:11,500
we don't have to fight against back

00:36:09,400 --> 00:36:14,230
boxes and the big slick advising it's

00:36:11,500 --> 00:36:17,230
also you know back to our roots and I

00:36:14,230 --> 00:36:18,820
think it is interesting here at Mozilla

00:36:17,230 --> 00:36:21,160
where it's really the ethos of Mozilla a

00:36:18,820 --> 00:36:23,650
back to the roots of empowering people

00:36:21,160 --> 00:36:25,420
to be able to tinker with the technology

00:36:23,650 --> 00:36:27,280
making sure they have access to the

00:36:25,420 --> 00:36:29,320
open-source system and enable them to

00:36:27,280 --> 00:36:30,910
book around making sure that people

00:36:29,320 --> 00:36:34,090
understand what machine learning it but

00:36:30,910 --> 00:36:35,470
it's not you know mystified black box

00:36:34,090 --> 00:36:37,540
that they're able to train their own

00:36:35,470 --> 00:36:39,490
persistence and is the same thing with

00:36:37,540 --> 00:36:41,320
debates right making sure that it is

00:36:39,490 --> 00:36:42,820
know what a deep thing is but they can

00:36:41,320 --> 00:36:44,800
make one they understand how it works

00:36:42,820 --> 00:36:47,110
they can look at it and I think we need

00:36:44,800 --> 00:36:49,150
to make sure we keep that in mind as we

00:36:47,110 --> 00:36:51,070
speak about these issues it's also

00:36:49,150 --> 00:36:53,350
something that people have access to

00:36:51,070 --> 00:36:56,740
should have access to and that we should

00:36:53,350 --> 00:36:58,510
demystify and empower around in no case

00:36:56,740 --> 00:37:03,150
we should slow down on a a we should

00:36:58,510 --> 00:37:05,280
weigh in a way that makes sense

00:37:03,150 --> 00:37:09,210
on YouTube for instance the incentive is

00:37:05,280 --> 00:37:15,090
to maximize which name on the AIA earns

00:37:09,210 --> 00:37:19,200
but once an hour of use with very little

00:37:15,090 --> 00:37:22,080
from a global point of view and YouTube

00:37:19,200 --> 00:37:25,020
is maximized the best day of the world

00:37:22,080 --> 00:37:27,870
that's nice make waste your time it

00:37:25,020 --> 00:37:31,160
doesn't make any sense from from the

00:37:27,870 --> 00:37:33,049
society just make sense from Google

00:37:31,160 --> 00:37:34,670
and so you can sort of a common theme in

00:37:33,049 --> 00:37:38,329
these although it's only different

00:37:34,670 --> 00:37:40,880
things okay so we can't stop we should

00:37:38,329 --> 00:37:43,339
think about it we should have an idea

00:37:40,880 --> 00:37:45,440
and I had the console probably shouldn't

00:37:43,339 --> 00:37:48,220
slow down if we do there are other

00:37:45,440 --> 00:37:49,869
people who are going to do this anyway

00:37:48,220 --> 00:37:52,000
what this is making me think open it's

00:37:49,869 --> 00:37:54,520
an imperfect analogy but it's making me

00:37:52,000 --> 00:37:58,270
think of the softer part of the cold

00:37:54,520 --> 00:38:01,800
wool where I mean apart from the proxy

00:37:58,270 --> 00:38:05,560
was in sub-saharan African countries um

00:38:01,800 --> 00:38:07,030
the idea was to show the rest of the

00:38:05,560 --> 00:38:09,369
world that our system is a better one

00:38:07,030 --> 00:38:10,910
ours works better ours is more desirable

00:38:09,369 --> 00:38:13,160
and

00:38:10,910 --> 00:38:15,230
what happened people got on planes and

00:38:13,160 --> 00:38:15,890
they moved to America they they moved to

00:38:15,230 --> 00:38:18,380
Western Europe

00:38:15,890 --> 00:38:20,569
nobody was crossing the jumping over the

00:38:18,380 --> 00:38:22,640
Berlin world at East Berlin right so is

00:38:20,569 --> 00:38:26,260
the idea then who is one of the answers

00:38:22,640 --> 00:38:30,030
then to simply make better

00:38:26,260 --> 00:38:30,030
mr. fide battery

00:38:31,420 --> 00:38:34,230
well I mean you know people were

00:38:32,950 --> 00:38:36,330
migrating because

00:38:34,230 --> 00:38:39,270
is also a place serve these replaces

00:38:36,330 --> 00:38:42,270
right in the West mean to be more humane

00:38:39,270 --> 00:38:44,850
and so they weren't only you know

00:38:42,270 --> 00:38:47,340
rushing into the arms of technology and

00:38:44,850 --> 00:38:49,350
so I think if I would just only put that

00:38:47,340 --> 00:38:51,780
on the table as part of what you're

00:38:49,350 --> 00:38:54,660
saying possesses a more humane

00:38:51,780 --> 00:38:57,540
technology in the ovens formulation yeah

00:38:54,660 --> 00:39:01,329
I think there's an aspect to it but

00:38:57,540 --> 00:39:03,039
we're at a weird time in history

00:39:01,329 --> 00:39:06,819
you're actually seeing people try and

00:39:03,039 --> 00:39:10,979
reinvent a more flawed ass rather than

00:39:06,819 --> 00:39:13,329
charging you know based on oftentimes

00:39:10,979 --> 00:39:15,700
alternative realities in actual reality

00:39:13,329 --> 00:39:18,549
the two dimensions that I'm most

00:39:15,700 --> 00:39:22,359
interested in now having come out of all

00:39:18,549 --> 00:39:24,190
of this is how much do people identify

00:39:22,359 --> 00:39:26,469
and share an allegiance online as

00:39:24,190 --> 00:39:28,420
opposed to in person meaning that

00:39:26,469 --> 00:39:31,329
digital tribes people will stay

00:39:28,420 --> 00:39:34,029
committed to for an enduring period to

00:39:31,329 --> 00:39:35,709
keep that digital identity to the point

00:39:34,029 --> 00:39:37,930
where they will turn on their national

00:39:35,709 --> 00:39:40,180
identity community identity in the real

00:39:37,930 --> 00:39:42,359
world whatever that'll be the other part

00:39:40,180 --> 00:39:45,099
I think that is very indicative

00:39:42,359 --> 00:39:47,680
particularly looking at cell phone usage

00:39:45,099 --> 00:39:50,109
times what apps are on is that you won't

00:39:47,680 --> 00:39:52,089
be taken over by the matrix you land

00:39:50,109 --> 00:39:54,009
list in it particularly if you're thirty

00:39:52,089 --> 00:39:57,459
years younger from the time you're born

00:39:54,009 --> 00:39:59,769
now you've had in the Western countries

00:39:57,459 --> 00:40:03,279
at least a cell phone and you have been

00:39:59,769 --> 00:40:04,509
tied to wife by five hours or more a day

00:40:03,279 --> 00:40:06,459
this is going to reshape your

00:40:04,509 --> 00:40:08,440
understanding of what the world is and

00:40:06,459 --> 00:40:11,739
where you're where you want to live and

00:40:08,440 --> 00:40:13,599
so to keep a Wi-Fi connection to keep

00:40:11,739 --> 00:40:16,660
that steady pace of information coming

00:40:13,599 --> 00:40:18,640
in to stay at 72 degrees indoors you'll

00:40:16,660 --> 00:40:20,529
give up on everything in the real world

00:40:18,640 --> 00:40:22,630
in terms of governance and you'll turn

00:40:20,529 --> 00:40:24,579
back on a lot of principles in human

00:40:22,630 --> 00:40:26,799
rights like that that you wouldn't

00:40:24,579 --> 00:40:30,250
before that's what I actually am sort of

00:40:26,799 --> 00:40:32,260
like more focused on which is this

00:40:30,250 --> 00:40:37,120
turn or it's a matrix rather than away

00:40:32,260 --> 00:40:38,680
and I'm shocked from watching this over

00:40:37,120 --> 00:40:41,110
the past few years I mean in the United

00:40:38,680 --> 00:40:43,450
States if you took a 16-year old phone

00:40:41,110 --> 00:40:46,800
away from them and didn't let him get on

00:40:43,450 --> 00:40:49,180
a laptop they might have panic because

00:40:46,800 --> 00:40:51,130
and this is not a joke I mean this is a

00:40:49,180 --> 00:40:53,950
serious sort of thing people have gotten

00:40:51,130 --> 00:40:57,160
used to having streaming Wi-Fi

00:40:53,950 --> 00:40:59,200
connections and information to them is

00:40:57,160 --> 00:41:02,380
as valuable or more valuable than water

00:40:59,200 --> 00:41:04,210
and food and I didn't grow up and that's

00:41:02,380 --> 00:41:05,620
just there was no internet you know when

00:41:04,210 --> 00:41:08,380
I was young I think this will

00:41:05,620 --> 00:41:10,060
fundamentally change society allegiances

00:41:08,380 --> 00:41:12,160
I think the other thing that's coming

00:41:10,060 --> 00:41:14,320
online that we need to look at is the

00:41:12,160 --> 00:41:16,450
biotech conversion sort of part it's not

00:41:14,320 --> 00:41:18,670
my expertise per se but you will have

00:41:16,450 --> 00:41:21,100
humans that are technologically superior

00:41:18,670 --> 00:41:23,200
eventually in the next 20 years to other

00:41:21,100 --> 00:41:25,780
humans and how they will treat each

00:41:23,200 --> 00:41:27,430
other when they have you know when

00:41:25,780 --> 00:41:29,790
certain societies have those augment

00:41:27,430 --> 00:41:31,930
abilities cognitive pants and

00:41:29,790 --> 00:41:34,030
automatically regenerate replace over

00:41:31,930 --> 00:41:37,080
organs those are the things combined

00:41:34,030 --> 00:41:40,000
with AI that I think we don't entirely

00:41:37,080 --> 00:41:41,380
understand either play out and that's

00:41:40,000 --> 00:41:43,540
why I think you see with young people in

00:41:41,380 --> 00:41:45,310
America for the first time have said I'm

00:41:43,540 --> 00:41:47,170
not sure democracy is the best form of

00:41:45,310 --> 00:41:49,180
governance because with the raise they

00:41:47,170 --> 00:41:51,970
saying is I better have my Wi-Fi and you

00:41:49,180 --> 00:41:54,580
better give me we might have to leave

00:41:51,970 --> 00:41:55,600
the biotech for another panel yeah I

00:41:54,580 --> 00:41:56,800
don't know anything about I'm just

00:41:55,600 --> 00:41:59,350
saying I think a lot of these friends

00:41:56,800 --> 00:42:01,540
play into AI and how it will change us

00:41:59,350 --> 00:42:05,440
people begin you wanted to say something

00:42:01,540 --> 00:42:08,770
on young people in technology I think of

00:42:05,440 --> 00:42:12,010
the matrix metaphor is is actually very

00:42:08,770 --> 00:42:13,400
good on we should or but that is we

00:42:12,010 --> 00:42:16,040
creating a world

00:42:13,400 --> 00:42:18,200
that looks like really easy bills like

00:42:16,040 --> 00:42:20,900
bills even better than really he

00:42:18,200 --> 00:42:24,470
sometimes but that's actually a

00:42:20,900 --> 00:42:27,230
distortion early and that is a maze for

00:42:24,470 --> 00:42:32,530
one goal on in the matrix like humans

00:42:27,230 --> 00:42:35,330
are there who earn a few cents per hour

00:42:32,530 --> 00:42:37,370
creating energy and only it's exactly

00:42:35,330 --> 00:42:40,400
the same on YouTube you you basically

00:42:37,370 --> 00:42:44,990
create a few cents per hour for you

00:42:40,400 --> 00:42:47,270
right I won't I know that time is short

00:42:44,990 --> 00:42:50,240
I want to ask you guys one last question

00:42:47,270 --> 00:42:51,800
and then open it up the audience a lot

00:42:50,240 --> 00:42:55,800
of what we've talked about has been

00:42:51,800 --> 00:42:57,410
related to information to society

00:42:55,800 --> 00:43:01,370
I'm Clint you've touched a little bit on

00:42:57,410 --> 00:43:04,410
traditional war in terms of drones um

00:43:01,370 --> 00:43:07,020
what I'd like answered is at what point

00:43:04,410 --> 00:43:09,600
is a is a properly transition or has it

00:43:07,020 --> 00:43:11,280
already from being something in an

00:43:09,600 --> 00:43:13,680
informational subject is something that

00:43:11,280 --> 00:43:16,320
is used much more broadly in traditional

00:43:13,680 --> 00:43:19,060
room I think it's already half to a

00:43:16,320 --> 00:43:22,480
large tree

00:43:19,060 --> 00:43:23,890
just in the last 10 years terrorism and

00:43:22,480 --> 00:43:26,140
counterterrorism has brought a lot of

00:43:23,890 --> 00:43:30,040
this in much much faster than what would

00:43:26,140 --> 00:43:32,920
normally have natural evolution I also

00:43:30,040 --> 00:43:34,300
think it's not just the information

00:43:32,920 --> 00:43:36,880
warfare but the political warfare

00:43:34,300 --> 00:43:38,620
aspects of it so when I was tracking

00:43:36,880 --> 00:43:40,060
disinformation campaigns either from

00:43:38,620 --> 00:43:43,570
extremists or nation-states

00:43:40,060 --> 00:43:46,900
we're still doing select all ace excel

00:43:43,570 --> 00:43:49,060
you know fine you know that's not you

00:43:46,900 --> 00:43:52,120
know if you have this capability as a

00:43:49,060 --> 00:43:55,420
nation-state imagine the social media

00:43:52,120 --> 00:43:58,810
that we saw 10 years ago al-qaida in

00:43:55,420 --> 00:44:01,440
Iraq is per thing but now you can pull

00:43:58,810 --> 00:44:03,550
all of that in and within minutes

00:44:01,440 --> 00:44:06,250
determine not only locations but

00:44:03,550 --> 00:44:08,890
usernames of this or thing location this

00:44:06,250 --> 00:44:11,920
is going to dominate in warfare that

00:44:08,890 --> 00:44:15,550
sort of information panel space the

00:44:11,920 --> 00:44:18,160
other part is in terms of how things

00:44:15,550 --> 00:44:20,560
will be deployed or created visually

00:44:18,160 --> 00:44:23,200
you're already seeing this if you watch

00:44:20,560 --> 00:44:27,630
the battlefield Ukraine and there's an

00:44:23,200 --> 00:44:29,819
amazing hits Radiolab know

00:44:27,630 --> 00:44:31,559
didn't something rather I somebody will

00:44:29,819 --> 00:44:32,729
remember but it's a great episode where

00:44:31,559 --> 00:44:35,449
they talking about when the reporters

00:44:32,729 --> 00:44:37,739
come the war starts which means that

00:44:35,449 --> 00:44:39,660
using technology knowing in the

00:44:37,739 --> 00:44:41,190
information space you don't conduct

00:44:39,660 --> 00:44:42,599
warfare unless there's someone there to

00:44:41,190 --> 00:44:44,729
report on or otherwise it's a waste of

00:44:42,599 --> 00:44:46,619
time it's not even really about killing

00:44:44,729 --> 00:44:49,049
people Ray's just that the perception of

00:44:46,619 --> 00:44:52,109
violence or the perception of moving or

00:44:49,049 --> 00:44:54,359
perception of a conflict so I think

00:44:52,109 --> 00:44:57,989
that's remarkable how quickly that has

00:44:54,359 --> 00:44:59,969
changed and how warfare isn't always so

00:44:57,989 --> 00:45:01,799
much kinetic now like I'm I'm waiting

00:44:59,969 --> 00:45:03,869
for someone to lose a war without a shot

00:45:01,799 --> 00:45:06,869
being fired anything on the perception

00:45:03,869 --> 00:45:08,400
that were you're having this this is the

00:45:06,869 --> 00:45:11,009
virtual dislike he was talking about

00:45:08,400 --> 00:45:13,259
this is the virtual and real world sort

00:45:11,009 --> 00:45:17,640
of colliding in in ways that is hard to

00:45:13,259 --> 00:45:19,499
understand the financial space as well I

00:45:17,640 --> 00:45:20,249
think you'll see much more finance

00:45:19,499 --> 00:45:22,280
warfare

00:45:20,249 --> 00:45:25,910
I mean Oh

00:45:22,280 --> 00:45:27,890
from non stadion data using a I wonder

00:45:25,910 --> 00:45:30,680
do you have any losing remote before we

00:45:27,890 --> 00:45:32,750
open it up I want to I would hope we

00:45:30,680 --> 00:45:35,600
hope we would get here but I wanted to

00:45:32,750 --> 00:45:38,020
talk a little bit about issues of I mean

00:45:35,600 --> 00:45:42,260
Neil touched on a little bit about

00:45:38,020 --> 00:45:44,600
accountability and also you know issues

00:45:42,260 --> 00:45:48,040
of inclusion and diversity and data set

00:45:44,600 --> 00:45:50,660
has been an obsession of mine and I am

00:45:48,040 --> 00:45:53,780
certainly in the u.s. there's been a lot

00:45:50,660 --> 00:45:55,610
of conversation about having training

00:45:53,780 --> 00:45:57,350
data sets that are for facial

00:45:55,610 --> 00:46:00,260
recognition that are broadly kind of

00:45:57,350 --> 00:46:03,470
inclusive and I certainly worry that we

00:46:00,260 --> 00:46:05,230
confuse our political aspirations and

00:46:03,470 --> 00:46:07,460
political conversation around equality

00:46:05,230 --> 00:46:10,550
and issues of equality and social

00:46:07,460 --> 00:46:13,190
justice with what it means with with a

00:46:10,550 --> 00:46:16,400
technical bias or technical imprecision

00:46:13,190 --> 00:46:18,940
and so I would invite us to eat those

00:46:16,400 --> 00:46:21,940
things distinct even as they need and

00:46:18,940 --> 00:46:21,940
fine

00:46:22,540 --> 00:46:28,490
it's really important like I mean we

00:46:25,220 --> 00:46:35,090
have no idea or bias AI systems already

00:46:28,490 --> 00:46:37,250
are like I was telling someone like I'm

00:46:35,090 --> 00:46:39,380
noticing that your your systems are

00:46:37,250 --> 00:46:42,280
racist only is telling me over the user

00:46:39,380 --> 00:46:42,280
were racist

00:46:42,650 --> 00:46:49,430
you have like a bay sdai will amplify

00:46:44,690 --> 00:46:51,710
and so and at a certain point it becomes

00:46:49,430 --> 00:46:53,000
very unhealthy right and then a certain

00:46:51,710 --> 00:46:54,350
point becomes obvious but not

00:46:53,000 --> 00:46:56,090
necessarily before that exactly

00:46:54,350 --> 00:46:59,360
especially like the people whose he is a

00:46:56,090 --> 00:47:00,890
racist person or after the racist so if

00:46:59,360 --> 00:47:06,640
you're not racist feel not being see

00:47:00,890 --> 00:47:09,080
that the a is racist other people let's

00:47:06,640 --> 00:47:11,360
yeah to your first question and

00:47:09,080 --> 00:47:13,340
difference point I think machine

00:47:11,360 --> 00:47:15,680
learning has definitely made it way into

00:47:13,340 --> 00:47:18,800
how we work they're also made its way

00:47:15,680 --> 00:47:21,170
and how we Ramona Livity and Tamara

00:47:18,800 --> 00:47:22,670
learned about demystifying I think we

00:47:21,170 --> 00:47:25,550
have this idea that one day is gonna be

00:47:22,670 --> 00:47:27,170
Wow hey I wore but this isn't how I work

00:47:25,550 --> 00:47:29,810
and I think this is why I strive to say

00:47:27,170 --> 00:47:31,610
machine learning instead of AI where you

00:47:29,810 --> 00:47:34,070
have is really complex systems and

00:47:31,610 --> 00:47:36,740
military operations and increasingly

00:47:34,070 --> 00:47:37,820
most of those smalls decisions become

00:47:36,740 --> 00:47:39,470
automated right

00:47:37,820 --> 00:47:40,900
maybe it's the targeting that's being

00:47:39,470 --> 00:47:43,700
helped by a machine learning system

00:47:40,900 --> 00:47:46,130
maybe it's you know deployment and

00:47:43,700 --> 00:47:47,780
analysis maybe satellite imagery so it's

00:47:46,130 --> 00:47:50,240
not gonna be like a switch one day it's

00:47:47,780 --> 00:47:53,230
a war it's going to be a slowing breeze

00:47:50,240 --> 00:47:55,340
of how much of these complex system

00:47:53,230 --> 00:47:57,440
become reliant on machine learning

00:47:55,340 --> 00:48:00,620
processes and we're already there so

00:47:57,440 --> 00:48:03,500
it's almost like what percentage of our

00:48:00,620 --> 00:48:05,690
decisions when we conduct war fair are

00:48:03,500 --> 00:48:07,670
currently being supported by machine

00:48:05,690 --> 00:48:09,350
learning and therefore what are the

00:48:07,670 --> 00:48:10,940
impact on humanitarian law and human

00:48:09,350 --> 00:48:12,350
rights and to the point

00:48:10,940 --> 00:48:16,660
alone is making I think another question

00:48:12,350 --> 00:48:19,700
we have to ask is why do we miss in our

00:48:16,660 --> 00:48:22,370
ecosystem and business model in order to

00:48:19,700 --> 00:48:26,870
be able to have these better datasets

00:48:22,370 --> 00:48:29,600
and better foundational processes if

00:48:26,870 --> 00:48:33,160
there's less bias AI something that I'm

00:48:29,600 --> 00:48:36,280
particularly worried about is um

00:48:33,160 --> 00:48:38,890
data that's being used for AI is being

00:48:36,280 --> 00:48:41,710
annotated it's a very important part

00:48:38,890 --> 00:48:43,750
before the industry relies on add the

00:48:41,710 --> 00:48:45,760
annotation st. so for instance you get a

00:48:43,750 --> 00:48:47,920
piece of tag or you get an image and

00:48:45,760 --> 00:48:51,430
people can say yes this is the image of

00:48:47,920 --> 00:48:54,099
a dog or people can say this is tech the

00:48:51,430 --> 00:48:56,680
way we build these annotation is very

00:48:54,099 --> 00:48:59,230
obscure we have a very hard time telling

00:48:56,680 --> 00:49:01,420
who's annotating the data where type of

00:48:59,230 --> 00:49:04,539
bias are they bringing in and right now

00:49:01,420 --> 00:49:07,030
we don't really have a good business or

00:49:04,539 --> 00:49:09,130
providing unbiased an auditable

00:49:07,030 --> 00:49:10,660
annotation it's also going to be about

00:49:09,130 --> 00:49:13,539
looking at these entire system and say

00:49:10,660 --> 00:49:15,880
hey here at this very specific little

00:49:13,539 --> 00:49:18,990
step that no one cares about right we're

00:49:15,880 --> 00:49:22,359
missing a business offering that enables

00:49:18,990 --> 00:49:24,460
businesses and you know like anyone to

00:49:22,359 --> 00:49:26,039
do this properly and be able to answer

00:49:24,460 --> 00:49:31,299
those questions

00:49:26,039 --> 00:49:36,490
terrifying note to end on I'm hoping the

00:49:31,299 --> 00:49:38,829
questions from the audience is someone

00:49:36,490 --> 00:49:40,720
would be with a mic who will wander

00:49:38,829 --> 00:49:43,029
around to other issues if you want to

00:49:40,720 --> 00:49:45,359
raise your hands someone will come to

00:49:43,029 --> 00:49:45,359
you

00:49:45,900 --> 00:49:57,339
oh yeah what is the current status of

00:49:52,559 --> 00:50:00,839
understanding and what's that of Western

00:49:57,339 --> 00:50:00,839
governments have paid and so far

00:50:03,470 --> 00:50:10,039
if whom do you want to just listen to

00:50:06,200 --> 00:50:11,780
everybody so western states in terms of

00:50:10,039 --> 00:50:12,289
understanding AI and then what to do

00:50:11,780 --> 00:50:14,690
about it

00:50:12,289 --> 00:50:16,640
yeah they're still trying to figure out

00:50:14,690 --> 00:50:20,619
the internet I mean if I have to be

00:50:16,640 --> 00:50:20,619
honest I I mean it is

00:50:20,700 --> 00:50:24,900
yeah it's just not it's not even an

00:50:22,950 --> 00:50:26,369
issue I were still at least in the u.s.

00:50:24,900 --> 00:50:27,750
were crying about what happened on

00:50:26,369 --> 00:50:30,420
social media two and a half years ago

00:50:27,750 --> 00:50:33,990
you know and trying to figure that part

00:50:30,420 --> 00:50:36,359
out in terms of technology I'm not

00:50:33,990 --> 00:50:38,160
seeing anything significant in terms of

00:50:36,359 --> 00:50:39,569
artificial intelligence they've they may

00:50:38,160 --> 00:50:41,780
know that better because I don't act

00:50:39,569 --> 00:50:41,780
like that

00:50:42,960 --> 00:50:48,270
III mean in the US political spectrum

00:50:45,630 --> 00:50:50,730
certainly you know the Obama

00:50:48,270 --> 00:50:52,740
administration had some you know

00:50:50,730 --> 00:50:54,750
meetings on AI and I think was beginning

00:50:52,740 --> 00:50:57,960
a conversation about forms of regulation

00:50:54,750 --> 00:51:00,360
about it but that steam has sort of run

00:50:57,960 --> 00:51:02,220
out on that and you know the Trump

00:51:00,360 --> 00:51:05,190
administration recently had a meeting on

00:51:02,220 --> 00:51:07,500
AI at which I understand correctly only

00:51:05,190 --> 00:51:10,110
CEOs of AI companies were invited right

00:51:07,500 --> 00:51:11,970
so it's a very different in the Obama

00:51:10,110 --> 00:51:14,490
administration it was you know

00:51:11,970 --> 00:51:16,440
policymakers and community groups as

00:51:14,490 --> 00:51:19,380
well as experts and as well as business

00:51:16,440 --> 00:51:22,020
leaders so there's been a sort of change

00:51:19,380 --> 00:51:23,880
both in what's happened and and and you

00:51:22,020 --> 00:51:25,500
know reading the tea leaves what might

00:51:23,880 --> 00:51:28,900
be able to happen given how we're

00:51:25,500 --> 00:51:30,960
talking about it just nation state level

00:51:28,900 --> 00:51:30,960
you

00:51:31,540 --> 00:51:36,820
I'm gonna try to provide an optimistic I

00:51:34,600 --> 00:51:38,860
think the work that the Obama

00:51:36,820 --> 00:51:41,410
administration conducted around AI has

00:51:38,860 --> 00:51:42,760
been noted and although it indeed

00:51:41,410 --> 00:51:44,680
doesn't seem like the current

00:51:42,760 --> 00:51:46,300
administration had picked that up I have

00:51:44,680 --> 00:51:48,160
seen all their nation's becoming more

00:51:46,300 --> 00:51:49,990
involved around it one interesting

00:51:48,160 --> 00:51:52,720
example is the franco canadian working

00:51:49,990 --> 00:51:54,790
group on AI burns has a Fields Medal

00:51:52,720 --> 00:51:57,190
synthetic villainy was a basically a

00:51:54,790 --> 00:51:59,800
good equivalent of the Nobel Prize for

00:51:57,190 --> 00:52:04,090
math and he has been tasked by his own

00:51:59,800 --> 00:52:06,400
government to us to evaluate policy

00:52:04,090 --> 00:52:08,020
solutions to tackle this problem I think

00:52:06,400 --> 00:52:10,120
that's a great development it's being

00:52:08,020 --> 00:52:12,310
done in an international way both France

00:52:10,120 --> 00:52:13,990
and Canada together it's being put in

00:52:12,310 --> 00:52:16,330
the hands of someone who on his hand a

00:52:13,990 --> 00:52:18,580
sense map and this is making his science

00:52:16,330 --> 00:52:20,320
first approach to it it's not gonna

00:52:18,580 --> 00:52:22,000
solve the problem in itself but I think

00:52:20,320 --> 00:52:23,920
you know if we look at the art there's

00:52:22,000 --> 00:52:25,510
been new developments in the u.s. this

00:52:23,920 --> 00:52:27,730
is being picked up a little bit see

00:52:25,510 --> 00:52:29,710
things happen yes it's not exactly the

00:52:27,730 --> 00:52:31,470
whole solution we need but you know

00:52:29,710 --> 00:52:33,550
they're they're positive sign said

00:52:31,470 --> 00:52:35,870
governments are spreading tackle this

00:52:33,550 --> 00:52:38,480
live in or three

00:52:35,870 --> 00:52:41,450
being pessimistic enough because if

00:52:38,480 --> 00:52:44,300
talks a lot about training data sets not

00:52:41,450 --> 00:52:47,300
being being biased and in producing not

00:52:44,300 --> 00:52:49,160
a landscape but then you've got what the

00:52:47,300 --> 00:52:51,590
car industry did try and hide its

00:52:49,160 --> 00:52:54,260
emissions by essentially erupting the

00:52:51,590 --> 00:52:57,200
algorithm Utley to defeat regulators and

00:52:54,260 --> 00:52:59,180
then - that's industry playing a game

00:52:57,200 --> 00:53:01,480
but then you've also got a bench full of

00:52:59,180 --> 00:53:04,010
individuals backing the algorithms

00:53:01,480 --> 00:53:06,680
infiltrating companies you know the

00:53:04,010 --> 00:53:08,960
thesaurus example that was given you

00:53:06,680 --> 00:53:10,640
know how come how maybe that's somebody

00:53:08,960 --> 00:53:12,350
yelling out for the right you know

00:53:10,640 --> 00:53:14,960
deliberately targeting a right-wing

00:53:12,350 --> 00:53:16,960
agenda inside the algorithm from

00:53:14,960 --> 00:53:20,770
everybody else

00:53:16,960 --> 00:53:24,370
humans there is so many so many factors

00:53:20,770 --> 00:53:26,410
now we can't like so that's why what we

00:53:24,370 --> 00:53:30,520
did this more transparency whether it's

00:53:26,410 --> 00:53:34,170
from nonprofits or from a government

00:53:30,520 --> 00:53:37,700
that rates regulation that tastes so

00:53:34,170 --> 00:53:41,210
green activate not sure

00:53:37,700 --> 00:53:42,789
works with their doing inside of plenty

00:53:41,210 --> 00:53:45,249
but

00:53:42,789 --> 00:53:47,650
as in the world matters only if we can't

00:53:45,249 --> 00:53:51,239
be sure that and it can can go wrong

00:53:47,650 --> 00:53:51,239
only we have a way of doing

00:53:56,580 --> 00:53:58,640
you

00:54:05,210 --> 00:54:11,260
20 things were being asthma

00:54:08,350 --> 00:54:15,940
do we think we're being pissed I'm like

00:54:11,260 --> 00:54:17,880
oh I think everybody knows that but look

00:54:15,940 --> 00:54:19,410
go away

00:54:17,880 --> 00:54:21,989
look at what's happened in the last few

00:54:19,410 --> 00:54:24,059
years I heard two years I was trying to

00:54:21,989 --> 00:54:26,819
talk about this information nobody here

00:54:24,059 --> 00:54:28,400
at all then for two years everybody

00:54:26,819 --> 00:54:32,039
wants to talk about this information and

00:54:28,400 --> 00:54:34,650
so people do tend to pivot or spin and

00:54:32,039 --> 00:54:36,390
we've got more activism right now like

00:54:34,650 --> 00:54:38,729
physical activism groups like this

00:54:36,390 --> 00:54:41,579
people are coming together socially

00:54:38,729 --> 00:54:43,619
conscious and worried about things and I

00:54:41,579 --> 00:54:45,479
feel like that's an optimism you know

00:54:43,619 --> 00:54:47,519
like we actually have people that are

00:54:45,479 --> 00:54:49,529
thinking about these things now that two

00:54:47,519 --> 00:54:51,959
to three years ago I'll go back to the

00:54:49,529 --> 00:54:54,119
Arab Spring when Facebook Twitter saved

00:54:51,959 --> 00:54:56,699
the world as we all know traveling great

00:54:54,119 --> 00:54:58,499
right wait we were super optimistic in

00:54:56,699 --> 00:55:02,999
those times and they hit there has been

00:54:58,499 --> 00:55:05,400
great things so I'm not I am going to

00:55:02,999 --> 00:55:07,289
throw the pessimistic out there you know

00:55:05,400 --> 00:55:09,589
and that's sort of how I oriented think

00:55:07,289 --> 00:55:11,549
because I came from a warfare

00:55:09,589 --> 00:55:13,499
intelligence background you know that's

00:55:11,549 --> 00:55:15,989
how everything was negative you know it

00:55:13,499 --> 00:55:19,349
worked but I am optimistic in the sense

00:55:15,989 --> 00:55:22,529
that these shots in the arm sometimes or

00:55:19,349 --> 00:55:25,019
what makes liberal democracies great we

00:55:22,529 --> 00:55:26,969
have to you know we have two digits the

00:55:25,019 --> 00:55:28,769
trajectory isn't a straight line through

00:55:26,969 --> 00:55:31,589
the stratosphere right we have to have

00:55:28,769 --> 00:55:33,239
pickups so that we can improve and we

00:55:31,589 --> 00:55:36,040
can figure out where the weaknesses are

00:55:33,239 --> 00:55:37,990
you know in our society and so

00:55:36,040 --> 00:55:40,290
at the same point I can tell you at

00:55:37,990 --> 00:55:42,580
least in the u.s. context we have more

00:55:40,290 --> 00:55:43,960
activism being brought together by

00:55:42,580 --> 00:55:45,820
digital connections that are now

00:55:43,960 --> 00:55:48,010
physically bringing people together of

00:55:45,820 --> 00:55:49,660
all age ranges that I like I've never

00:55:48,010 --> 00:55:52,510
seen before at least since I've been

00:55:49,660 --> 00:55:54,490
alive that are truly like bringing about

00:55:52,510 --> 00:55:56,140
change now we'll find out in the u.s. in

00:55:54,490 --> 00:55:58,270
the next few weeks whether any event is

00:55:56,140 --> 00:56:00,910
true right we don't actually know if it

00:55:58,270 --> 00:56:03,640
will sort of make those games but I'm

00:56:00,910 --> 00:56:06,280
not all besa mystic but thanks and I do

00:56:03,640 --> 00:56:09,190
I think there's a great Pew study that

00:56:06,280 --> 00:56:10,720
said young people half of them that

00:56:09,190 --> 00:56:13,180
downloaded their Facebook data

00:56:10,720 --> 00:56:16,510
immediately deleted the app after they

00:56:13,180 --> 00:56:18,580
realized what was there more than old

00:56:16,510 --> 00:56:20,590
people old people download it they just

00:56:18,580 --> 00:56:21,910
sort of went back to watching TV or

00:56:20,590 --> 00:56:24,760
whatever they were doing you know and

00:56:21,910 --> 00:56:26,170
they weren't is so there's societal

00:56:24,760 --> 00:56:27,640
changes that going on I think you're

00:56:26,170 --> 00:56:29,830
positive and people are starting to take

00:56:27,640 --> 00:56:33,160
some some agency in it I don't want to

00:56:29,830 --> 00:56:35,080
be passive mystic all the way in saying

00:56:33,160 --> 00:56:36,730
that things about young people because I

00:56:35,080 --> 00:56:38,410
was worried after your remark about how

00:56:36,730 --> 00:56:40,900
they're all gonna find oh no no I pad

00:56:38,410 --> 00:56:42,370
here's what we know just two points

00:56:40,900 --> 00:56:44,080
about the younger generation that I

00:56:42,370 --> 00:56:45,700
think are remarkable they're better at

00:56:44,080 --> 00:56:48,250
spotting false information than their

00:56:45,700 --> 00:56:50,350
parent they have grown up in the system

00:56:48,250 --> 00:56:51,850
and so they understand you know how like

00:56:50,350 --> 00:56:52,780
this information comes through I think

00:56:51,850 --> 00:56:55,420
that's great

00:56:52,780 --> 00:56:57,130
they've also started to take on privacy

00:56:55,420 --> 00:56:59,500
in a much bigger way because they aren't

00:56:57,130 --> 00:57:01,780
want their stupid Facebook posts to toss

00:56:59,500 --> 00:57:02,980
them a job later on in life and you know

00:57:01,780 --> 00:57:05,620
and they're they're wanting different

00:57:02,980 --> 00:57:06,970
features in their social media so I'm

00:57:05,620 --> 00:57:08,560
not at least don't think it's all

00:57:06,970 --> 00:57:09,820
negative I think there's a lot of

00:57:08,560 --> 00:57:11,470
positive things that we can learn from

00:57:09,820 --> 00:57:13,360
the younger generation I'm being cheeky

00:57:11,470 --> 00:57:15,070
I think we sometimes have a tendency to

00:57:13,360 --> 00:57:16,600
throw victims at the bus saying like oh

00:57:15,070 --> 00:57:18,310
they're a bunch of outright Nazis who

00:57:16,600 --> 00:57:20,500
are gonna cry if you remove the iPad I

00:57:18,310 --> 00:57:22,090
think it's are wonderful so much smarter

00:57:20,500 --> 00:57:23,890
than sometimes we give them credit or

00:57:22,090 --> 00:57:25,750
and as you said there are many signs

00:57:23,890 --> 00:57:27,130
that younger generations when they're

00:57:25,750 --> 00:57:29,530
empowered and when they're informed

00:57:27,130 --> 00:57:31,540
actually here about the chip system it

00:57:29,530 --> 00:57:33,490
will have the ability changes just

00:57:31,540 --> 00:57:36,130
anecdotally I mostly talked to old

00:57:33,490 --> 00:57:38,350
people in the US and I will do these

00:57:36,130 --> 00:57:40,000
briefings and their immediate reactions

00:57:38,350 --> 00:57:42,220
we've gotta tell our kids about this I

00:57:40,000 --> 00:57:43,960
said no no your kids need to tell you

00:57:42,220 --> 00:57:45,330
about this like they've got to figure it

00:57:43,960 --> 00:57:46,980
out like

00:57:45,330 --> 00:57:48,480
you know we should rely on them they've

00:57:46,980 --> 00:57:51,060
spent more time online than their

00:57:48,480 --> 00:57:53,970
parents have already you know so it yeah

00:57:51,060 --> 00:57:55,910
it is a great dynamic going on all right

00:57:53,970 --> 00:57:58,170
we have time for one last question

00:57:55,910 --> 00:58:00,090
yeah thank you for the great

00:57:58,170 --> 00:58:02,160
conversation and I came this morning

00:58:00,090 --> 00:58:04,830
very joyful and thinking we can change

00:58:02,160 --> 00:58:08,760
the world at 32 she writes and now I'm

00:58:04,830 --> 00:58:11,880
on my first I'm I'm joking around um I

00:58:08,760 --> 00:58:16,530
was wondering like as today it's all

00:58:11,880 --> 00:58:19,110
about AI and the technology and I feel

00:58:16,530 --> 00:58:21,900
we are almost being colonized by your

00:58:19,110 --> 00:58:24,720
technology technology life for the sake

00:58:21,900 --> 00:58:29,370
of technology and not as a mean burger

00:58:24,720 --> 00:58:32,730
and and how can we keep being critical

00:58:29,370 --> 00:58:35,190
of data and of what we are presented as

00:58:32,730 --> 00:58:38,100
the solutions of power of features and

00:58:35,190 --> 00:58:40,470
and thinking of all people like my

00:58:38,100 --> 00:58:43,290
mother who can't use the internet and

00:58:40,470 --> 00:58:46,470
people who are not only computer

00:58:43,290 --> 00:58:49,920
literate what will happen to them as

00:58:46,470 --> 00:58:51,360
everything is moving online and people

00:58:49,920 --> 00:58:53,850
who don't have the education and back

00:58:51,360 --> 00:58:57,600
and the background where will they go to

00:58:53,850 --> 00:59:00,840
get their services today although online

00:58:57,600 --> 00:59:04,650
and the run-through would I've got

00:59:00,840 --> 00:59:07,800
written very few are understand and how

00:59:04,650 --> 00:59:11,270
can we challenge our algorithms that I

00:59:07,800 --> 00:59:14,640
used by companies like recently yeah

00:59:11,270 --> 00:59:18,060
like anecdote I have nightfall well we

00:59:14,640 --> 00:59:19,770
let them onto the earth yeah okay no do

00:59:18,060 --> 00:59:22,380
you wanna you wanna take yeah I mean I

00:59:19,770 --> 00:59:24,210
would refer you to a wonderful book live

00:59:22,380 --> 00:59:26,160
Virginia Eubanks called automating

00:59:24,210 --> 00:59:28,830
inequality which shows something we've

00:59:26,160 --> 00:59:30,780
talked about users here she really talks

00:59:28,830 --> 00:59:34,440
about the communities who are become the

00:59:30,780 --> 00:59:37,700
objects of a is surveillance and who

00:59:34,440 --> 00:59:41,040
don't have agency but who are who are

00:59:37,700 --> 00:59:43,820
subject to the use to the increasing use

00:59:41,040 --> 00:59:47,340
of AI and social welfare systems

00:59:43,820 --> 00:59:50,130
predictive policing social housing and

00:59:47,340 --> 00:59:52,650
these sorts of things and and how those

00:59:50,130 --> 00:59:55,620
these systems are disproportionately

00:59:52,650 --> 00:59:57,120
impacting these communities and ways in

00:59:55,620 --> 00:59:57,600
which they don't feel like they have say

00:59:57,120 --> 01:00:00,390
or

00:59:57,600 --> 01:00:03,330
that it's just really a kind of almost

01:00:00,390 --> 01:00:06,240
carceral net over people's activities

01:00:03,330 --> 01:00:08,250
and so you know it gets I think more

01:00:06,240 --> 01:00:10,440
depressive than even the note that we

01:00:08,250 --> 01:00:13,800
were planning to end on I think if one

01:00:10,440 --> 01:00:17,210
you know elaborates this out to um you

01:00:13,800 --> 01:00:20,040
know not a sort of Liberal Democratic

01:00:17,210 --> 01:00:22,110
sort of you know middle class subject

01:00:20,040 --> 01:00:24,120
mind set that all of us sort of come to

01:00:22,110 --> 01:00:27,180
this conversation with that I think if

01:00:24,120 --> 01:00:28,590
we switch that time mindset changes how

01:00:27,180 --> 01:00:30,150
we think about the future of work and a

01:00:28,590 --> 01:00:31,650
I mean what are we going to do in a

01:00:30,150 --> 01:00:33,960
place like the United States if we're

01:00:31,650 --> 01:00:36,660
successful with Dakar serration two

01:00:33,960 --> 01:00:38,400
million people you know are no longer in

01:00:36,660 --> 01:00:40,140
jail like what is the future of work for

01:00:38,400 --> 01:00:42,990
them our conversations about the future

01:00:40,140 --> 01:00:44,820
were always imply a middle class agent

01:00:42,990 --> 01:00:47,430
and so I think there are lots of

01:00:44,820 --> 01:00:50,010
conversations just have about AI um that

01:00:47,430 --> 01:00:52,200
really take seriously society as a whole

01:00:50,010 --> 01:00:55,140
and the sort of stratification of what

01:00:52,200 --> 01:00:57,590
the incursion of AI in life means for

01:00:55,140 --> 01:00:57,590
lots of

01:00:57,840 --> 01:01:02,520
you know one of a positive note is like

01:01:00,540 --> 01:01:05,550
once you know that hey a strength

01:01:02,520 --> 01:01:08,700
paycheck like Facebook whispering make

01:01:05,550 --> 01:01:11,550
you brainless Lee's all on you watch

01:01:08,700 --> 01:01:13,560
whatever conspiracy theories once you

01:01:11,550 --> 01:01:15,570
know that you ingest as a user you have

01:01:13,560 --> 01:01:18,450
your own goals on using you know that

01:01:15,570 --> 01:01:20,310
it's been great to do so you can say is

01:01:18,450 --> 01:01:22,770
it really my goal to stay as long as

01:01:20,310 --> 01:01:25,560
possible and facing and then you can

01:01:22,770 --> 01:01:27,870
disprove you can get out doesn't matter

01:01:25,560 --> 01:01:30,690
so if you know how it works over Tory

01:01:27,870 --> 01:01:34,160
like talking about that is I'm Justin

01:01:30,690 --> 01:01:37,020
it's not very useful on out user and

01:01:34,160 --> 01:01:39,300
achieves a bowl like this let's end with

01:01:37,020 --> 01:01:41,490
this pay happy positive note of user

01:01:39,300 --> 01:01:44,490
agency before somebody else has

01:01:41,490 --> 01:01:46,200
something terrible thank you very much

01:01:44,490 --> 01:01:48,590
and we're gonna have another girl

01:01:46,200 --> 01:01:48,590
puffiness

01:01:58,400 --> 01:02:18,520

YouTube URL: https://www.youtube.com/watch?v=503Yup0wpIk


