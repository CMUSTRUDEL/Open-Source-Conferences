Title: MozFest 2019 - Governing AI Panel
Publication date: 2020-09-29
Playlist: Mozilla Festival
Description: 
	How do we create policies that keep companies accountable and enshrine human rights in automated systems?

Gry Hasselbalch
Co-Founder of the think-do tank DataEthics.eu. Gry is a member of the European Commission’s High Level Expert Group on AI and was a member of the data ethics expert group appointed to provide the Danish government with recommendations in 2018. Gry is also co-chair of the IEEE P7006 standard on personal data AI agents

Fanny Hidvegi
Access Now’s Europe Policy Manager based in Brussels. Fanny is appointed to the European Commission’s High Level Expert Group on Artificial Intelligence, serves on the board of the Hungarian Civil Liberties Union (HCLU), and is a Marshall Memorial Fellow.

Matthias Spielkamp
Co-Founder and Executive Director of AlgorithmWatch, and an award-winning journalist

Philip Dawson
Public Policy Lead at Element AI

PLEASE NOTE: Due to encoder equipment over-heating, the beginning of this presentation recording has video/audio dropout. The issue was resolved ~3 min. into the presentation.
Captions: 
	00:00:00,030 --> 00:00:04,659
most I want to bring up

00:00:01,790 --> 00:00:04,659
panelists

00:00:07,850 --> 00:00:12,290
[Applause]

00:00:11,970 --> 00:00:18,059
you

00:00:12,290 --> 00:00:18,059
[Applause]

00:00:20,710 --> 00:00:23,730
yes I see

00:00:23,949 --> 00:00:29,160
decorative director algorithm watch he's

00:00:27,340 --> 00:00:32,650
also the founder editorial director

00:00:29,160 --> 00:00:34,600
about that sorry foundation that sheds

00:00:32,650 --> 00:00:38,680
light on how algorithms make decisions

00:00:34,600 --> 00:00:43,800
that affect society and then we have

00:00:38,680 --> 00:00:43,800
fanny hit McGee European policy manager

00:00:44,570 --> 00:00:47,140
access now

00:00:47,290 --> 00:00:54,239
on the EU Commission and in the past

00:00:51,239 --> 00:00:54,239
that's

00:01:00,620 --> 00:01:07,000
Austin filled Dawson's

00:01:03,740 --> 00:01:07,000
leads public

00:01:07,700 --> 00:01:10,330
at the

00:01:11,270 --> 00:01:17,070
your paint tank

00:01:14,530 --> 00:01:17,070
data

00:01:19,259 --> 00:01:24,000
dottie you and you are a member of the

00:01:21,780 --> 00:01:28,560
European Commission's high lead expert

00:01:24,000 --> 00:01:30,810
group on AI same as Vanna yeah so let's

00:01:28,560 --> 00:01:36,619
talk off by asking you guys you know you

00:01:30,810 --> 00:01:36,619
know generally makes things better

00:01:42,180 --> 00:01:44,240
you

00:01:50,189 --> 00:01:56,159
and of frameworks that we use them in

00:01:53,939 --> 00:01:59,640
and develop them in and design them in

00:01:56,159 --> 00:02:03,840
so in in general it all depends on on

00:01:59,640 --> 00:02:05,609
how it's built and how it's adopted and

00:02:03,840 --> 00:02:08,519
that's how we have to think of a as well

00:02:05,609 --> 00:02:10,200
not as this new emergent thing that that

00:02:08,519 --> 00:02:11,819
changes everything and disrupts

00:02:10,200 --> 00:02:15,230
everything but it has to build into

00:02:11,819 --> 00:02:15,230
existing legal framework

00:03:14,940 --> 00:03:18,660
last couple of years then it's really

00:03:16,950 --> 00:03:20,250
hard to come up with some good example

00:03:18,660 --> 00:03:24,390
it's it's easier to come up with bad

00:03:20,250 --> 00:03:26,700
examples of automation not working for

00:03:24,390 --> 00:03:30,840
the people who are affected by it but

00:03:26,700 --> 00:03:33,570
rather than rather using it to you know

00:03:30,840 --> 00:03:36,150
to surveil them and to curtail them and

00:03:33,570 --> 00:03:38,460
there was even a very critical report I

00:03:36,150 --> 00:03:41,970
don't agree with to a hundred percent

00:03:38,460 --> 00:03:44,460
but by the UN Special Rapporteur on

00:03:41,970 --> 00:03:46,140
human rights and extreme poverty that

00:03:44,460 --> 00:03:49,860
was looking at the digitized welfare

00:03:46,140 --> 00:03:53,070
state and yeah it was basically blasting

00:03:49,860 --> 00:03:55,440
most of the of the attempts to use these

00:03:53,070 --> 00:03:57,930
kinds of systems there and it's a shame

00:03:55,440 --> 00:04:02,060
because you know as algorithm water and

00:03:57,930 --> 00:04:02,060
I think many here in the room

00:04:07,650 --> 00:04:09,709
you

00:04:22,820 --> 00:04:27,830
this is what why we need to keep a very

00:04:25,340 --> 00:04:30,440
very keen eye on how they let you simply

00:04:27,830 --> 00:04:32,570
call human rights violations they are

00:04:30,440 --> 00:04:35,360
not just simply challenges and they are

00:04:32,570 --> 00:04:38,330
already existing while the benefits is

00:04:35,360 --> 00:04:40,250
still a bit more learning so I think we

00:04:38,330 --> 00:04:43,370
already have to recognize the failures

00:04:40,250 --> 00:04:48,280
and try to try to correct that and not

00:04:43,370 --> 00:04:48,280
just think about future options

00:04:55,210 --> 00:04:57,270
you

00:06:08,530 --> 00:06:11,689
versus something that's artificial

00:06:10,120 --> 00:06:13,360
intelligence

00:06:11,689 --> 00:06:17,269
there were some common misconceptions

00:06:13,360 --> 00:06:19,399
when we talk about AI I can say a few

00:06:17,269 --> 00:06:22,249
things about I think it's all like in

00:06:19,399 --> 00:06:24,049
general I think that there are some

00:06:22,249 --> 00:06:26,419
problems in terms of the we talk about

00:06:24,049 --> 00:06:29,029
AI and AI I don't think it's about this

00:06:26,419 --> 00:06:31,610
granular difference between small tasks

00:06:29,029 --> 00:06:34,279
in the end if they can perform small

00:06:31,610 --> 00:06:37,610
tasks or whether they will out-compete

00:06:34,279 --> 00:06:38,779
the the human kind and take over and so

00:06:37,610 --> 00:06:41,529
forth I think they're two very different

00:06:38,779 --> 00:06:41,529
discourse

00:06:47,960 --> 00:06:50,020
you

00:06:56,780 --> 00:07:02,150
that we can't control we can't govern we

00:07:00,620 --> 00:07:05,240
we don't know what to do with these

00:07:02,150 --> 00:07:09,370
systems because either their scientific

00:07:05,240 --> 00:07:09,370
business adventure so please put out

00:07:11,580 --> 00:07:17,669
don't don't don't start hindering

00:07:15,270 --> 00:07:19,289
innovation or their wait love we can't

00:07:17,669 --> 00:07:22,080
control because it's already too late I

00:07:19,289 --> 00:07:24,569
would think that you know if we're to

00:07:22,080 --> 00:07:27,270
talk about how to define AI for example

00:07:24,569 --> 00:07:29,430
let's think of it as for example from my

00:07:27,270 --> 00:07:31,770
perspective from data ethics perspective

00:07:29,430 --> 00:07:35,060
it's it's very complex data processing

00:07:31,770 --> 00:07:37,379
and it's very very large systems of

00:07:35,060 --> 00:07:39,240
complex data processing and what are the

00:07:37,379 --> 00:07:41,759
ethical implications of that we have

00:07:39,240 --> 00:07:43,889
things in terms of mass citizen scoring

00:07:41,759 --> 00:07:46,610
that we see in some context we see

00:07:43,889 --> 00:07:46,610
predict

00:07:46,639 --> 00:07:49,090
tipsy

00:07:53,080 --> 00:07:57,520
yes

00:07:55,060 --> 00:07:59,440
for the narrative I think it's already

00:07:57,520 --> 00:08:03,300
quite interesting to look at the

00:07:59,440 --> 00:08:03,300
narrative change even Mozilla

00:08:03,460 --> 00:08:08,069
I applied in the thinking from the

00:08:05,860 --> 00:08:11,699
better decision of posted a trustworthy

00:08:08,069 --> 00:08:15,849
AI which is the term coined by the

00:08:11,699 --> 00:08:19,000
European Union's ethics guideline and

00:08:15,849 --> 00:08:22,690
what that means and it has three pillars

00:08:19,000 --> 00:08:24,610
of being legal ethical and robust but at

00:08:22,690 --> 00:08:27,610
the moment is more what we see is that

00:08:24,610 --> 00:08:29,470
it's a marketing term and I'm I hope

00:08:27,610 --> 00:08:33,450
that this community can turn it into

00:08:29,470 --> 00:08:36,190
something more than more than that and

00:08:33,450 --> 00:08:37,930
just on that narrative setting for me

00:08:36,190 --> 00:08:40,270
what's very interesting I looked at my

00:08:37,930 --> 00:08:42,039
emails when was the first time in the

00:08:40,270 --> 00:08:44,110
work setting we started talking about

00:08:42,039 --> 00:08:45,630
artificial intelligence internally on

00:08:44,110 --> 00:08:51,670
the team beyond just sharing some

00:08:45,630 --> 00:08:55,000
newsletters and it was mid-2016 talking

00:08:51,670 --> 00:08:58,390
about the formation in the fall of the

00:08:55,000 --> 00:09:01,029
partnership on AI by Google Facebook IBM

00:08:58,390 --> 00:09:03,810
Amazon and Microsoft and I think it it

00:09:01,029 --> 00:09:03,810
still is

00:09:09,760 --> 00:09:11,820
you

00:09:29,600 --> 00:09:34,340
I mean how do you stop that right we

00:09:32,390 --> 00:09:37,220
want to live in liberal societies that

00:09:34,340 --> 00:09:40,760
means that you know companies should

00:09:37,220 --> 00:09:42,950
have the freedom to make the decision on

00:09:40,760 --> 00:09:45,530
what they'd like to work on in certain

00:09:42,950 --> 00:09:46,760
boundaries right but that is of course

00:09:45,530 --> 00:09:50,060
what we are struggling with right now

00:09:46,760 --> 00:09:51,530
where they are what they are and just

00:09:50,060 --> 00:09:53,930
really coming back to that narrative

00:09:51,530 --> 00:09:55,910
question we think it's important but of

00:09:53,930 --> 00:09:58,310
course we also know that our influence

00:09:55,910 --> 00:09:59,660
is limited on that you know actually I

00:09:58,310 --> 00:10:01,760
think we should be talking about

00:09:59,660 --> 00:10:03,680
correlation based decision support

00:10:01,760 --> 00:10:06,500
systems right you know how does that

00:10:03,680 --> 00:10:09,410
work with a larger audience it's not

00:10:06,500 --> 00:10:10,700
going to work but but it's important

00:10:09,410 --> 00:10:13,220
that we keep in mind that this is what

00:10:10,700 --> 00:10:14,720
we're talking about or I think that's at

00:10:13,220 --> 00:10:16,130
the core of what we're talking about

00:10:14,720 --> 00:10:18,860
when we're trying to identify the

00:10:16,130 --> 00:10:20,360
challenges that we have because we are

00:10:18,860 --> 00:10:22,550
looking more and more at these

00:10:20,360 --> 00:10:25,100
correlation based decisions instead of

00:10:22,550 --> 00:10:26,870
causation right and this is this is a

00:10:25,100 --> 00:10:28,760
fundamental change because we just

00:10:26,870 --> 00:10:30,470
assume that because there's massive

00:10:28,760 --> 00:10:32,780
amounts of data and we can detect

00:10:30,470 --> 00:10:34,340
patterns in it then that needs to guide

00:10:32,780 --> 00:10:36,350
our decisions and this is what's

00:10:34,340 --> 00:10:39,380
problematic and this is why what we need

00:10:36,350 --> 00:10:41,300
to understand better and then also take

00:10:39,380 --> 00:10:43,790
precautions because we are not going to

00:10:41,300 --> 00:10:47,680
get away from that any more and it's

00:10:43,790 --> 00:10:47,680
helpful you know we wouldn't have

00:10:53,560 --> 00:10:55,620
you

00:11:13,510 --> 00:11:20,420
that if we were and all you know people

00:11:16,200 --> 00:11:20,420
it's then we need to understand

00:11:25,279 --> 00:11:27,339
you

00:11:53,360 --> 00:11:57,769
Soloff because none of these questions

00:11:55,699 --> 00:11:59,989
are ever saw if we just like the meat

00:11:57,769 --> 00:12:02,889
and potatoes of what that company maybe

00:11:59,989 --> 00:12:02,889
I'll start off on this one

00:12:08,800 --> 00:12:10,860
you

00:12:15,589 --> 00:12:22,699
won't be able to do to govern AI on

00:12:19,399 --> 00:12:24,620
their own and and people have a much

00:12:22,699 --> 00:12:26,870
higher society as much higher

00:12:24,620 --> 00:12:28,610
expectation for corporate accountability

00:12:26,870 --> 00:12:31,100
when it comes to the deployment of

00:12:28,610 --> 00:12:32,059
artificial intelligence systems so one

00:12:31,100 --> 00:12:35,480
of the things that we're doing in our

00:12:32,059 --> 00:12:37,579
team is it's free and we're in frequent

00:12:35,480 --> 00:12:39,769
conversation with regulators about how

00:12:37,579 --> 00:12:42,110
we can kind of find an in-between space

00:12:39,769 --> 00:12:43,490
or apply existing their existing

00:12:42,110 --> 00:12:46,100
regulatory framework how it should be

00:12:43,490 --> 00:12:47,600
better applied to AI and managing its

00:12:46,100 --> 00:12:50,120
risks particularly the human rights

00:12:47,600 --> 00:12:51,769
framework and then finding like ways

00:12:50,120 --> 00:12:54,350
that we can through kind of a light

00:12:51,769 --> 00:12:55,790
touch approach to regulation introduce

00:12:54,350 --> 00:12:57,800
greater accountability either through

00:12:55,790 --> 00:13:00,379
different reporting requirements for

00:12:57,800 --> 00:13:03,050
impact assessments and things like that

00:13:00,379 --> 00:13:04,759
so I think the conversation is gradually

00:13:03,050 --> 00:13:06,649
shifting well well didn't I think

00:13:04,759 --> 00:13:09,309
there's a consensus growing consensus

00:13:06,649 --> 00:13:09,309
that self-regulation

00:13:10,690 --> 00:13:14,139
it's not enough and the new reality the

00:13:12,519 --> 00:13:15,579
companies have to deal with is what is

00:13:14,139 --> 00:13:17,379
that regulatory landscape going to look

00:13:15,579 --> 00:13:20,259
like an it protects us so that

00:13:17,379 --> 00:13:21,579
technology to to to be developed in a

00:13:20,259 --> 00:13:23,949
way that is beneficial yeah

00:13:21,579 --> 00:13:26,259
Greece should Facebook moderate Facebook

00:13:23,949 --> 00:13:30,189
assured its government regulate Facebook

00:13:26,259 --> 00:13:31,269
so I admire in regular should Facebook

00:13:30,189 --> 00:13:32,949
regulate Facebook where should the

00:13:31,269 --> 00:13:34,540
government regulate Facebook I think

00:13:32,949 --> 00:13:35,740
it's much more complex than just a

00:13:34,540 --> 00:13:36,910
question between should it be

00:13:35,740 --> 00:13:40,569
self-regulation or

00:13:36,910 --> 00:13:42,610
or law when I when I think of governance

00:13:40,569 --> 00:13:45,129
for example we did the policy and

00:13:42,610 --> 00:13:47,319
investment recommendations in the EU

00:13:45,129 --> 00:13:49,000
group for the EU and how to deal with

00:13:47,319 --> 00:13:51,850
this there was a set of different

00:13:49,000 --> 00:13:54,310
approaches on how to govern this area so

00:13:51,850 --> 00:13:57,189
there's things like you can invest in

00:13:54,310 --> 00:13:59,410
education of Engineers you can create

00:13:57,189 --> 00:14:02,350
laws but you can also map existing laws

00:13:59,410 --> 00:14:04,930
you can look at specific areas like if

00:14:02,350 --> 00:14:06,939
anyone was talking about before one

00:14:04,930 --> 00:14:08,740
discussion that we can have which is

00:14:06,939 --> 00:14:11,800
also part of a governance framework is

00:14:08,740 --> 00:14:15,810
to exactly discuss which areas and what

00:14:11,800 --> 00:14:18,339
kind of measures we can make to to to

00:14:15,810 --> 00:14:19,930
stop the innovation of AI and specific

00:14:18,339 --> 00:14:22,930
areas that shouldn't go in in certain

00:14:19,930 --> 00:14:24,699
ways so so governance is is is you know

00:14:22,930 --> 00:14:26,949
we tend to think of it as something that

00:14:24,699 --> 00:14:29,829
either the government does by itself by

00:14:26,949 --> 00:14:32,170
creating it some new law that then by by

00:14:29,829 --> 00:14:34,180
definition fixes everything or in a

00:14:32,170 --> 00:14:35,949
technological space we think of it as a

00:14:34,180 --> 00:14:37,779
technological fix and we create this

00:14:35,949 --> 00:14:39,519
system for example in terms of constant

00:14:37,779 --> 00:14:41,319
moderation we created a system that can

00:14:39,519 --> 00:14:43,480
they can moderate everything and that's

00:14:41,319 --> 00:14:45,370
it that that it doesn't work like that

00:14:43,480 --> 00:14:49,240
anymore governance governance is law

00:14:45,370 --> 00:14:51,490
it's culture education it's is companies

00:14:49,240 --> 00:14:53,949
doing things and we have to think of

00:14:51,490 --> 00:14:56,709
smart contextual based ways of governing

00:14:53,949 --> 00:14:59,230
these systems so we create what we call

00:14:56,709 --> 00:15:02,610
trustworthy AI we don't just dream about

00:14:59,230 --> 00:15:05,350
trustworthy AI we actually make it yeah

00:15:02,610 --> 00:15:07,750
yeah it's second that at the same time I

00:15:05,350 --> 00:15:10,089
would also warn against this narrative

00:15:07,750 --> 00:15:12,459
of do we need regulation of course we

00:15:10,089 --> 00:15:15,670
need regulation and all these companies

00:15:12,459 --> 00:15:17,769
and processes are already heavily

00:15:15,670 --> 00:15:20,759
regulated there's no such thing as an

00:15:17,769 --> 00:15:23,410
unregulated Facebook you know there are

00:15:20,759 --> 00:15:24,940
privileges on liability that I

00:15:23,410 --> 00:15:27,220
hugely important when it comes to

00:15:24,940 --> 00:15:29,350
Facebook this is an active regulation

00:15:27,220 --> 00:15:31,120
there is copyright regulation that is

00:15:29,350 --> 00:15:33,639
highly complex and that has been around

00:15:31,120 --> 00:15:35,980
for many many years so it's more like

00:15:33,639 --> 00:15:38,829
what is good regulation and what is bad

00:15:35,980 --> 00:15:42,730
regulation applied to to these actors

00:15:38,829 --> 00:15:45,189
and and we also should probably I mean

00:15:42,730 --> 00:15:47,079
there has to be someone here on the

00:15:45,189 --> 00:15:49,689
stage who says that Facebook is just an

00:15:47,079 --> 00:15:52,089
 or company right and and and

00:15:49,689 --> 00:15:54,939
they are they sort of dug their own dig

00:15:52,089 --> 00:15:57,399
dug their own grave I mean it's not a

00:15:54,939 --> 00:16:01,269
grave yet but the pit you know that

00:15:57,399 --> 00:16:06,339
they'll fall into because they were just

00:16:01,269 --> 00:16:08,379
so ignorant about all the problems that

00:16:06,339 --> 00:16:10,720
they created and there's they still are

00:16:08,379 --> 00:16:13,240
that everyone is is really aggressively

00:16:10,720 --> 00:16:16,209
attacking them now although in many

00:16:13,240 --> 00:16:18,610
countries they are still basically the

00:16:16,209 --> 00:16:20,529
place for people to exchange information

00:16:18,610 --> 00:16:22,810
freely you know in authoritarian

00:16:20,529 --> 00:16:24,250
countries it's still great to have

00:16:22,810 --> 00:16:27,250
Facebook because there's no one else

00:16:24,250 --> 00:16:29,079
there to provide these services so it

00:16:27,250 --> 00:16:32,079
seems a little unfair but it's not

00:16:29,079 --> 00:16:36,550
because you know it always also depends

00:16:32,079 --> 00:16:39,490
on how these companies but also state

00:16:36,550 --> 00:16:41,709
institutions behave visa vie the demands

00:16:39,490 --> 00:16:43,990
that the public's have and this is what

00:16:41,709 --> 00:16:46,300
we need to find a balance you know to

00:16:43,990 --> 00:16:49,329
come up with good regulation that

00:16:46,300 --> 00:16:51,189
addresses that without harming for

00:16:49,329 --> 00:16:52,899
example free speech or other kinds of

00:16:51,189 --> 00:16:58,269
freedoms and this is really terribly

00:16:52,899 --> 00:17:01,329
hard yeah so but for me this is a power

00:16:58,269 --> 00:17:02,649
like a power question we can talk more

00:17:01,329 --> 00:17:04,209
details about the whole quantum

00:17:02,649 --> 00:17:05,559
moderation question just to bring it

00:17:04,209 --> 00:17:07,299
back a little bit closer

00:17:05,559 --> 00:17:11,169
you mentioned the autocratic countries

00:17:07,299 --> 00:17:14,260
as well for me the the specificities of

00:17:11,169 --> 00:17:17,169
this specific decision-making processes

00:17:14,260 --> 00:17:20,110
make it interesting to see that these

00:17:17,169 --> 00:17:21,699
governments implement decisions with

00:17:20,110 --> 00:17:23,650
different technologies and is there

00:17:21,699 --> 00:17:26,470
anything around the eye that makes it

00:17:23,650 --> 00:17:28,780
specifically dangerous or risky that

00:17:26,470 --> 00:17:31,150
creates different different thresholds

00:17:28,780 --> 00:17:32,980
that people need to put in place and for

00:17:31,150 --> 00:17:35,169
instance just to go back to the Facebook

00:17:32,980 --> 00:17:36,740
example they just announced that they

00:17:35,169 --> 00:17:40,220
would still allow

00:17:36,740 --> 00:17:43,010
government propaganda as they have for

00:17:40,220 --> 00:17:45,470
the past decade or so so it's not just

00:17:43,010 --> 00:17:47,960
around the technology they use but

00:17:45,470 --> 00:17:49,580
absolutely the policies that worries

00:17:47,960 --> 00:17:51,440
yeah yeah here's a question I have I

00:17:49,580 --> 00:17:54,530
mean I've seen you know we've all seen

00:17:51,440 --> 00:17:58,550
the European Union fine Google and find

00:17:54,530 --> 00:18:00,080
Facebook billions of dollars but you

00:17:58,550 --> 00:18:02,540
know these companies make billions of

00:18:00,080 --> 00:18:04,970
dollars in just a few months how do you

00:18:02,540 --> 00:18:06,320
go about you know actually punishing

00:18:04,970 --> 00:18:17,030
these companies in a way they actually

00:18:06,320 --> 00:18:18,470
feel it well and not only the punishment

00:18:17,030 --> 00:18:22,040
but what we see is that probably

00:18:18,470 --> 00:18:23,480
Facebook will perform better this

00:18:22,040 --> 00:18:25,760
quarter

00:18:23,480 --> 00:18:28,490
than ever before even though I don't

00:18:25,760 --> 00:18:31,179
think it has been demonstrated more how

00:18:28,490 --> 00:18:34,070
how terrible they've they did to

00:18:31,179 --> 00:18:36,470
individuals and societies this year so

00:18:34,070 --> 00:18:40,429
the question is why are we still buying

00:18:36,470 --> 00:18:42,230
into this model but I also think I'd

00:18:40,429 --> 00:18:43,730
like these fines maximum and the new

00:18:42,230 --> 00:18:46,460
laws we're creating in these groups

00:18:43,730 --> 00:18:48,170
where I think we could kind of choose to

00:18:46,460 --> 00:18:50,030
look at them as as symptoms of change

00:18:48,170 --> 00:18:52,100
and change it doesn't happen very fast

00:18:50,030 --> 00:18:54,050
all the time but it consists of

00:18:52,100 --> 00:18:57,470
different things and we're having like

00:18:54,050 --> 00:18:59,540
I've at least seen within working in

00:18:57,470 --> 00:19:02,030
this field within the last even just the

00:18:59,540 --> 00:19:06,980
last five years an incredible change in

00:19:02,030 --> 00:19:08,750
in how we are addressing the risks and

00:19:06,980 --> 00:19:10,700
the issues of Facebook and Google and

00:19:08,750 --> 00:19:13,370
the way they deal with things in the

00:19:10,700 --> 00:19:15,020
press in in normal speech I remember

00:19:13,370 --> 00:19:17,540
going into a room and having to fight

00:19:15,020 --> 00:19:19,820
for these things and now when I go in we

00:19:17,540 --> 00:19:21,440
have people nodding and I'm sure anyone

00:19:19,820 --> 00:19:24,410
sitting here has the same that it's it's

00:19:21,440 --> 00:19:26,900
getting so easy to talk about but that

00:19:24,410 --> 00:19:28,640
of course doesn't mean that that that

00:19:26,900 --> 00:19:30,710
it's happening that it that the change

00:19:28,640 --> 00:19:34,340
are happening but but it means that we

00:19:30,710 --> 00:19:36,260
have a support for and if I made it it's

00:19:34,340 --> 00:19:37,790
not it's not so much because sometimes

00:19:36,260 --> 00:19:39,380
you get the impression it's about you

00:19:37,790 --> 00:19:41,660
know having your revenge on Facebook

00:19:39,380 --> 00:19:44,929
because they're making so much money you

00:19:41,660 --> 00:19:46,730
know and they behave so badly so we need

00:19:44,929 --> 00:19:49,100
to punish them for that but of course

00:19:46,730 --> 00:19:49,950
what we are trying to and this is where

00:19:49,100 --> 00:19:53,610
your question

00:19:49,950 --> 00:19:56,970
is going how do we how can we change

00:19:53,610 --> 00:19:59,820
their behavior and I think it's not

00:19:56,970 --> 00:20:01,440
going to work with fines of course they

00:19:59,820 --> 00:20:03,929
can hurt and they should be high enough

00:20:01,440 --> 00:20:09,179
to hurt but they also need to be really

00:20:03,929 --> 00:20:11,940
well justified but I see as the main

00:20:09,179 --> 00:20:13,350
problem with the large companies and we

00:20:11,940 --> 00:20:15,179
shouldn't just talk about the large

00:20:13,350 --> 00:20:16,950
companies because they are specific but

00:20:15,179 --> 00:20:19,529
of course they pose one of the biggest

00:20:16,950 --> 00:20:22,049
problems with the large companies it is

00:20:19,529 --> 00:20:25,289
the lack of accountability right because

00:20:22,049 --> 00:20:28,110
they just don't give a damn about what

00:20:25,289 --> 00:20:30,779
we think what our governments think you

00:20:28,110 --> 00:20:32,610
know what society is think and this is

00:20:30,779 --> 00:20:34,529
what we need to address we have to come

00:20:32,610 --> 00:20:36,360
up with better ideas for holding them

00:20:34,529 --> 00:20:39,750
accountable so that they can't just say

00:20:36,360 --> 00:20:41,519
oh yeah we saw some AI on it or we'll

00:20:39,750 --> 00:20:43,769
hire a couple of thousand people in

00:20:41,519 --> 00:20:45,649
Indonesia to moderate our content and

00:20:43,769 --> 00:20:48,450
then this problem will probably go away

00:20:45,649 --> 00:20:50,429
this this is not going to work we need

00:20:48,450 --> 00:20:53,039
to make them accountable to individuals

00:20:50,429 --> 00:20:54,690
and to other stakeholders so that they

00:20:53,039 --> 00:20:57,480
feel the pressure that they're really

00:20:54,690 --> 00:21:01,049
doing some good stuff right you know I

00:20:57,480 --> 00:21:03,480
always say that at the same time but

00:21:01,049 --> 00:21:05,880
where they fail you know they that needs

00:21:03,480 --> 00:21:10,970
to be addressed and we need to be able

00:21:05,880 --> 00:21:12,990
as citizens to address that me I on it

00:21:10,970 --> 00:21:15,090
justifying to me I think one thing I

00:21:12,990 --> 00:21:16,889
would add a green tie earlier that

00:21:15,090 --> 00:21:18,210
Matthias was saying but I think one

00:21:16,889 --> 00:21:20,669
lesson that we should draw from this is

00:21:18,210 --> 00:21:22,230
that the and it's an obvious one but we

00:21:20,669 --> 00:21:25,049
should remind ourselves that the market

00:21:22,230 --> 00:21:27,840
does not value responsible AI something

00:21:25,049 --> 00:21:29,639
that Fanny just pointed to like Facebook

00:21:27,840 --> 00:21:30,990
shares keep key the share price keep

00:21:29,639 --> 00:21:33,269
going up so then the market does not

00:21:30,990 --> 00:21:34,740
value this so how do we create a market

00:21:33,269 --> 00:21:37,889
through appropriate regulatory

00:21:34,740 --> 00:21:41,149
interventions that makes responsible AI

00:21:37,889 --> 00:21:43,529
and investments and explain ability and

00:21:41,149 --> 00:21:48,330
you know accountability and AI systems

00:21:43,529 --> 00:21:49,440
and and like a prerequisite to develop

00:21:48,330 --> 00:21:51,210
in this technology and not just

00:21:49,440 --> 00:21:53,250
something that like the company will do

00:21:51,210 --> 00:21:55,380
out of its own goodwill and if they can

00:21:53,250 --> 00:21:57,059
convince investors that this is a

00:21:55,380 --> 00:22:01,019
worthwhile cost just give you guys an

00:21:57,059 --> 00:22:02,490
example at our company there's 30 to 40%

00:22:01,019 --> 00:22:03,419
of product development is explained

00:22:02,490 --> 00:22:05,849
ability you know

00:22:03,419 --> 00:22:07,679
that gets difficult to justify when

00:22:05,849 --> 00:22:08,969
you're getting to the end of a financing

00:22:07,679 --> 00:22:10,529
round well when you're when you're

00:22:08,969 --> 00:22:11,700
trying to raise the next one and you're

00:22:10,529 --> 00:22:13,379
running out of cash and you have all

00:22:11,700 --> 00:22:15,989
that all all the goodwill in the world

00:22:13,379 --> 00:22:18,599
you know when the rubber rubber hits the

00:22:15,989 --> 00:22:20,579
road and you're burning you know X

00:22:18,599 --> 00:22:23,519
amount of money per month it gets tough

00:22:20,579 --> 00:22:25,109
to justify because you you have to you

00:22:23,519 --> 00:22:27,719
have to keep the company going right so

00:22:25,109 --> 00:22:29,579
you know there we need to we need to

00:22:27,719 --> 00:22:31,229
think I think even more about instead of

00:22:29,579 --> 00:22:32,879
regulatory protections but the type of

00:22:31,229 --> 00:22:35,909
incentive structures that could be built

00:22:32,879 --> 00:22:38,039
into the market to privilege you know

00:22:35,909 --> 00:22:39,440
the investing in those text technologies

00:22:38,039 --> 00:22:41,759
it could be through procurement

00:22:39,440 --> 00:22:43,169
strategic procurement efforts and pilot

00:22:41,759 --> 00:22:46,229
programs that are showing up that

00:22:43,169 --> 00:22:49,139
technology as like the fund the

00:22:46,229 --> 00:22:51,509
fundamental base layer you know and then

00:22:49,139 --> 00:22:56,159
we might we might get get get somewhere

00:22:51,509 --> 00:22:58,229
get somewhere new like when you say that

00:22:56,159 --> 00:23:00,989
the market is not valuing responsible

00:22:58,229 --> 00:23:03,149
yeah like that means that just let's

00:23:00,989 --> 00:23:05,039
lose all these adjectives for a moment

00:23:03,149 --> 00:23:06,959
malavika arm has been talking about this

00:23:05,039 --> 00:23:09,089
for years where are all the conferences

00:23:06,959 --> 00:23:09,450
and companies developing irresponsible

00:23:09,089 --> 00:23:12,690
AI

00:23:09,450 --> 00:23:17,909
or AI for social bed or untrustworthy AI

00:23:12,690 --> 00:23:20,429
and so for me this means that already

00:23:17,909 --> 00:23:22,559
there is a market failure and there are

00:23:20,429 --> 00:23:24,899
existing like regulations that we need

00:23:22,559 --> 00:23:26,369
to apply first and to go back to your

00:23:24,899 --> 00:23:31,169
point about the complex information

00:23:26,369 --> 00:23:32,999
processing if if in 15 1955 they would

00:23:31,169 --> 00:23:35,729
have chosen this complex information

00:23:32,999 --> 00:23:37,649
processing to describe this method

00:23:35,729 --> 00:23:39,799
instead of artificial intelligence to

00:23:37,649 --> 00:23:43,109
get more funding would it be more

00:23:39,799 --> 00:23:45,509
appealing to lawmakers to sit in front

00:23:43,109 --> 00:23:48,119
of a blanket paper and write although on

00:23:45,509 --> 00:23:50,309
it I I doubt it I think we should look

00:23:48,119 --> 00:23:52,889
at the impacts on individuals and

00:23:50,309 --> 00:23:55,379
societies and first and force and also

00:23:52,889 --> 00:23:57,509
find the gaps and and regulate that area

00:23:55,379 --> 00:23:58,619
not the technology yeah yeah you're

00:23:57,509 --> 00:23:59,969
talking for about companies and it

00:23:58,619 --> 00:24:01,919
reminded me of something that mattias

00:23:59,969 --> 00:24:03,899
you were talking with the panelists

00:24:01,919 --> 00:24:05,369
about house dropping where you're saying

00:24:03,899 --> 00:24:07,289
you know you need a license to practice

00:24:05,369 --> 00:24:08,579
medicine you need a license to drive a

00:24:07,289 --> 00:24:11,789
car but anyone could just make a I

00:24:08,579 --> 00:24:14,099
should we have licenses to AI do you

00:24:11,789 --> 00:24:16,830
think that would help you know I'm

00:24:14,099 --> 00:24:18,720
skeptical about that but it was

00:24:16,830 --> 00:24:20,880
just you know we started discussing this

00:24:18,720 --> 00:24:22,799
with mentioning that there's apparently

00:24:20,880 --> 00:24:24,510
a proposal in Canada you can probably

00:24:22,799 --> 00:24:27,809
talk about where that is suggested I'm

00:24:24,510 --> 00:24:30,840
skeptical about this because I I'm all

00:24:27,809 --> 00:24:32,760
for having changes in curricula

00:24:30,840 --> 00:24:34,529
you know for example computer engineers

00:24:32,760 --> 00:24:38,640
and computer scientists learning more

00:24:34,529 --> 00:24:42,330
about responsible work with what they're

00:24:38,640 --> 00:24:44,190
doing that would apply to biologists and

00:24:42,330 --> 00:24:45,990
it does apply to biologists you know to

00:24:44,190 --> 00:24:47,909
people who work with a genetic

00:24:45,990 --> 00:24:49,500
engineering and so on and so forth it's

00:24:47,909 --> 00:24:52,200
not working there either by the way you

00:24:49,500 --> 00:24:53,669
know my wife studied biology she didn't

00:24:52,200 --> 00:24:57,690
have one ethics class in her entire

00:24:53,669 --> 00:25:00,269
program so that that is hard to address

00:24:57,690 --> 00:25:02,100
I don't know whether we should use this

00:25:00,269 --> 00:25:03,299
licensing idea to probably increase the

00:25:02,100 --> 00:25:05,250
pressure or something it's the first

00:25:03,299 --> 00:25:08,330
time I start thinking about this I would

00:25:05,250 --> 00:25:10,889
talk about a little about more about

00:25:08,330 --> 00:25:13,230
standardization but probably you know we

00:25:10,889 --> 00:25:15,090
finish that licensing idea first yeah

00:25:13,230 --> 00:25:17,100
well I know that you guys mentioned you

00:25:15,090 --> 00:25:19,769
want to get more time than usual

00:25:17,100 --> 00:25:21,720
yeah Q&A so think of questions now but

00:25:19,769 --> 00:25:23,730
the last question I'll ask you agree and

00:25:21,720 --> 00:25:25,500
anyone can answer you're talking about

00:25:23,730 --> 00:25:26,789
for about all the the kind of headlines

00:25:25,500 --> 00:25:27,809
we're seeing about all the problems

00:25:26,789 --> 00:25:30,360
being addressed with social media

00:25:27,809 --> 00:25:31,889
companies do you feel like there might

00:25:30,360 --> 00:25:34,409
be a placebo effect where if you're not

00:25:31,889 --> 00:25:36,149
Mozilla mosfets audience member just a

00:25:34,409 --> 00:25:37,440
regular person seeing the headlines of

00:25:36,149 --> 00:25:38,820
Facebook did this and this is what the

00:25:37,440 --> 00:25:40,769
government like you know Zuckerberg

00:25:38,820 --> 00:25:41,909
there's a congressional hearing do you

00:25:40,769 --> 00:25:42,840
think there might be a placebo effect or

00:25:41,909 --> 00:25:44,220
it's like oh the problems are being

00:25:42,840 --> 00:25:46,080
solved let me go back to using Facebook

00:25:44,220 --> 00:25:48,179
do you think like kind of explains like

00:25:46,080 --> 00:25:52,860
the increasing stock price like do you

00:25:48,179 --> 00:25:54,600
think we'll see you back there see I I I

00:25:52,860 --> 00:25:55,980
don't know I think I think I was sitting

00:25:54,600 --> 00:25:58,320
and thinking here I was listening to

00:25:55,980 --> 00:26:00,149
also what element hey I the you as a

00:25:58,320 --> 00:26:04,139
company and I was thinking I wanted to

00:26:00,149 --> 00:26:06,299
ask you like how because element AI for

00:26:04,139 --> 00:26:08,340
example is known as a kind of a

00:26:06,299 --> 00:26:10,830
sustainable company and thinking about

00:26:08,340 --> 00:26:13,049
these questions and then I hear that you

00:26:10,830 --> 00:26:16,230
get I got possibly surprised that you

00:26:13,049 --> 00:26:19,230
got a big round of funding and I was one

00:26:16,230 --> 00:26:21,510
of their kind of profile was part of it

00:26:19,230 --> 00:26:23,460
and and maybe that's where we have to

00:26:21,510 --> 00:26:25,200
start in the what we talked about it not

00:26:23,460 --> 00:26:26,700
so much because I always thought that it

00:26:25,200 --> 00:26:28,920
was problematic that we put everything

00:26:26,700 --> 00:26:31,080
on the shoulders of the consumers

00:26:28,920 --> 00:26:33,120
and in consumer action and these things

00:26:31,080 --> 00:26:34,950
in this has to be solved by the

00:26:33,120 --> 00:26:37,380
companies that are building things to

00:26:34,950 --> 00:26:39,720
investors that are investing in specific

00:26:37,380 --> 00:26:42,900
solutions and asking for specific

00:26:39,720 --> 00:26:45,810
results and it has to be solved in law

00:26:42,900 --> 00:26:47,820
so I think that's more you know maybe

00:26:45,810 --> 00:26:50,070
maybe there's a placebo effect maybe

00:26:47,820 --> 00:26:53,160
there's not but in the end it is

00:26:50,070 --> 00:26:55,380
something that has to be sold by by some

00:26:53,160 --> 00:26:57,600
of the actual responsible actors that

00:26:55,380 --> 00:26:58,890
are building these infrastructures so I

00:26:57,600 --> 00:27:02,010
don't know if that's answering your

00:26:58,890 --> 00:27:03,750
question but but it's more changing the

00:27:02,010 --> 00:27:05,730
direction of the question yeah yeah

00:27:03,750 --> 00:27:07,230
all right so we'll end it there because

00:27:05,730 --> 00:27:08,580
they wanted to hear what you guys had to

00:27:07,230 --> 00:27:10,530
say so there's any questions in the

00:27:08,580 --> 00:27:14,550
audience the one up here we went over

00:27:10,530 --> 00:27:18,150
there I look Kevin decide who we hear

00:27:14,550 --> 00:27:20,850
from first right there hi so I agree

00:27:18,150 --> 00:27:23,160
that self regulation for companies doing

00:27:20,850 --> 00:27:25,410
AI is not a good idea but what options

00:27:23,160 --> 00:27:27,570
are there for you know small startups

00:27:25,410 --> 00:27:31,950
like what can we do to move away from

00:27:27,570 --> 00:27:36,450
that model of things I guess that's

00:27:31,950 --> 00:27:39,270
directed to me cool I mean anybody could

00:27:36,450 --> 00:27:41,280
answer that too but I'll start you know

00:27:39,270 --> 00:27:42,660
I think it's it's important one thing

00:27:41,280 --> 00:27:44,040
that we've learned and just give you a

00:27:42,660 --> 00:27:45,570
bit of background we're a company that

00:27:44,040 --> 00:27:49,200
was founded three years ago and we now

00:27:45,570 --> 00:27:51,780
have 500 employees so it was the scaling

00:27:49,200 --> 00:27:53,460
of you know happened very quickly there

00:27:51,780 --> 00:27:55,080
was there's their phases what I was

00:27:53,460 --> 00:27:57,300
hired where we were hiring 20 employees

00:27:55,080 --> 00:27:58,590
every two weeks and so when you're doing

00:27:57,300 --> 00:28:02,640
when you're doing that type of growth

00:27:58,590 --> 00:28:04,500
it's it's very difficult I think to to

00:28:02,640 --> 00:28:06,180
ensure that the ethical framework you've

00:28:04,500 --> 00:28:08,970
developed gets operationalized

00:28:06,180 --> 00:28:11,160
socialized and passed down to all the

00:28:08,970 --> 00:28:13,500
new hires but you have to do that from

00:28:11,160 --> 00:28:16,110
the beginning otherwise you it's like

00:28:13,500 --> 00:28:17,490
really hard to catch up so so I would

00:28:16,110 --> 00:28:21,690
say like you know that's that's one

00:28:17,490 --> 00:28:23,820
thing to like invest in invest early on

00:28:21,690 --> 00:28:26,810
in an explain ability team this is like

00:28:23,820 --> 00:28:26,810
a core component of

00:28:27,340 --> 00:28:38,559
of looking looking for a word but of AI

00:28:35,259 --> 00:28:40,929
that you know can be that that can that

00:28:38,559 --> 00:28:44,980
can be that is lawful and that is that

00:28:40,929 --> 00:28:48,580
can that can be you know contested and

00:28:44,980 --> 00:28:50,970
that and that will be more just so those

00:28:48,580 --> 00:28:53,080
are like those are two things and then

00:28:50,970 --> 00:28:54,639
something that we're working on now is

00:28:53,080 --> 00:28:56,470
to to look at different types of impact

00:28:54,639 --> 00:28:58,509
assessments on on products that are a

00:28:56,470 --> 00:29:00,070
little bit more mature because you know

00:28:58,509 --> 00:29:02,139
the you probably know if you're working

00:29:00,070 --> 00:29:03,700
in a start-up the product roadmap is it

00:29:02,139 --> 00:29:06,070
can also be a bit of a moving target so

00:29:03,700 --> 00:29:07,869
like do you do you invest heavily into

00:29:06,070 --> 00:29:10,149
like a kind of a new process for this

00:29:07,869 --> 00:29:11,289
type for this design when you're not

00:29:10,149 --> 00:29:12,820
even sure that it's going to be

00:29:11,289 --> 00:29:17,230
something that you roll out right so I

00:29:12,820 --> 00:29:19,869
think it's a it's like a tough game Duff

00:29:17,230 --> 00:29:21,669
negotiation to do but like also start

00:29:19,869 --> 00:29:23,590
early with the type of if you know that

00:29:21,669 --> 00:29:25,059
you're the main risk of your system

00:29:23,590 --> 00:29:26,320
might be you know bias and

00:29:25,059 --> 00:29:28,210
discrimination which is often the case

00:29:26,320 --> 00:29:30,220
and it is in some of our products then

00:29:28,210 --> 00:29:32,769
you just start early you know so I think

00:29:30,220 --> 00:29:34,659
that's what I would say and then start

00:29:32,769 --> 00:29:37,029
speaking to regulators companies don't

00:29:34,659 --> 00:29:39,429
always have to only always have the the

00:29:37,029 --> 00:29:42,419
budget early on for like a policy team

00:29:39,429 --> 00:29:45,070
but you know executives engineers

00:29:42,419 --> 00:29:47,379
anybody can do this have regular

00:29:45,070 --> 00:29:49,629
conversations with your regulators and

00:29:47,379 --> 00:29:51,490
and to get to get to know the regulatory

00:29:49,629 --> 00:29:52,960
framework that in which you're you're

00:29:51,490 --> 00:29:54,580
you're situated and help them get to

00:29:52,960 --> 00:29:56,590
know what type of technology you're

00:29:54,580 --> 00:29:58,330
building and what some of those so some

00:29:56,590 --> 00:30:00,340
of those risks are and then you can come

00:29:58,330 --> 00:30:03,039
to like an appropriate you know set of

00:30:00,340 --> 00:30:05,049
policy options that can that can allow

00:30:03,039 --> 00:30:06,909
your tech your company to grow and and

00:30:05,049 --> 00:30:10,980
hopefully in the right way that protects

00:30:06,909 --> 00:30:14,519
people I also have one suggestion I

00:30:10,980 --> 00:30:17,649
think for startups it's probably quite

00:30:14,519 --> 00:30:19,509
upsetting to see how low and all the

00:30:17,649 --> 00:30:22,179
policy conversations are targeted at

00:30:19,509 --> 00:30:25,570
these main actors especially if you come

00:30:22,179 --> 00:30:27,940
with a like a genuine purpose of our

00:30:25,570 --> 00:30:29,740
company exists and so what I would

00:30:27,940 --> 00:30:32,009
recommend we are trying to create a

00:30:29,740 --> 00:30:35,049
common language and narrative around

00:30:32,009 --> 00:30:37,659
automated decision making and we call it

00:30:35,049 --> 00:30:38,890
this myth busting exercise that Mozilla

00:30:37,659 --> 00:30:40,809
and

00:30:38,890 --> 00:30:43,330
Daniel ooh frere fellow is working on

00:30:40,809 --> 00:30:45,580
mostly that access now and we are

00:30:43,330 --> 00:30:48,670
looking for contributors who helped us

00:30:45,580 --> 00:30:51,940
create truth and demystifying all this

00:30:48,670 --> 00:30:54,309
 around it and so because you it

00:30:51,940 --> 00:30:57,040
matters to you what the tech the tech

00:30:54,309 --> 00:30:59,080
can can achieve but also the limitations

00:30:57,040 --> 00:31:03,690
like both the skills and the limitation

00:30:59,080 --> 00:31:03,690
so if you're interested and talk to us

00:31:04,500 --> 00:31:10,330
next question okay

00:31:07,080 --> 00:31:12,549
hi I have two remarks and one questions

00:31:10,330 --> 00:31:15,820
so first remark the first transatlantic

00:31:12,549 --> 00:31:18,280
boats or ships didn't have live boats on

00:31:15,820 --> 00:31:21,429
them the first cars came without

00:31:18,280 --> 00:31:23,710
seatbelts and people were flying out of

00:31:21,429 --> 00:31:26,500
windows when they were in an accident

00:31:23,710 --> 00:31:29,970
what I'm saying is that industry was

00:31:26,500 --> 00:31:32,110
throughout history very bad at effective

00:31:29,970 --> 00:31:35,140
self-regulating something that was

00:31:32,110 --> 00:31:38,830
obvious from the beginning so here's my

00:31:35,140 --> 00:31:41,919
question how do we make this whole

00:31:38,830 --> 00:31:44,559
process of regulation and

00:31:41,919 --> 00:31:48,270
self-regulation and whatever lies in

00:31:44,559 --> 00:31:51,220
between move away from this spectacle of

00:31:48,270 --> 00:31:52,960
here's five people on stage we're gonna

00:31:51,220 --> 00:31:55,870
save the world in one hour and then

00:31:52,960 --> 00:31:58,390
we're gonna go for a beer to the place

00:31:55,870 --> 00:32:00,730
where we go okay you do this I'll do

00:31:58,390 --> 00:32:03,190
this I'll do this and you do this and in

00:32:00,730 --> 00:32:05,140
the end hopefully every boat will every

00:32:03,190 --> 00:32:06,760
ship will have a lifeboat and every car

00:32:05,140 --> 00:32:11,799
will have a seatbelt we're actually

00:32:06,760 --> 00:32:14,140
gonna save 140 minutes but that's unfair

00:32:11,799 --> 00:32:19,540
I think I I thought we'd come up with a

00:32:14,140 --> 00:32:22,240
solution but seriously yeah of course

00:32:19,540 --> 00:32:25,510
you're right and and this is also I

00:32:22,240 --> 00:32:29,230
think the four of us agree on that that

00:32:25,510 --> 00:32:32,650
everyone has to work on different parts

00:32:29,230 --> 00:32:35,320
of this problem and even saying this

00:32:32,650 --> 00:32:37,450
problem is a problem because we're not

00:32:35,320 --> 00:32:39,160
looking at one problem with one solution

00:32:37,450 --> 00:32:42,220
what we're looking at a ton of different

00:32:39,160 --> 00:32:45,640
challenges that that need to be solved

00:32:42,220 --> 00:32:48,490
and you know one of the ideas is that

00:32:45,640 --> 00:32:51,270
for example like in the car industry and

00:32:48,490 --> 00:32:53,050
in other industries there can be better

00:32:51,270 --> 00:32:55,300
standardization

00:32:53,050 --> 00:32:58,000
then at the same time means that people

00:32:55,300 --> 00:33:00,670
in companies know better what to do how

00:32:58,000 --> 00:33:03,100
to develop their stuff for it to be for

00:33:00,670 --> 00:33:04,660
example more transparent and allow for

00:33:03,100 --> 00:33:06,940
better accountability and explain

00:33:04,660 --> 00:33:08,740
ability and then the regulator can for

00:33:06,940 --> 00:33:10,740
example decide to give some liability

00:33:08,740 --> 00:33:13,480
privileges because if you apply these

00:33:10,740 --> 00:33:15,880
standards then if something goes wrong

00:33:13,480 --> 00:33:19,270
you can at least prove that you've used

00:33:15,880 --> 00:33:22,540
the state-of-the-art testing and and in

00:33:19,270 --> 00:33:25,900
whatever it is I now also think that it

00:33:22,540 --> 00:33:29,470
will take a long time to come up with

00:33:25,900 --> 00:33:30,910
all of this and again it's never done in

00:33:29,470 --> 00:33:32,500
Germany I don't know the numbers in

00:33:30,910 --> 00:33:35,500
other countries but in Germany we still

00:33:32,500 --> 00:33:39,460
have 3000 fatalities each year on the

00:33:35,500 --> 00:33:42,160
rolls right that is a very high price we

00:33:39,460 --> 00:33:44,770
accept for mobility right but it has

00:33:42,160 --> 00:33:46,720
been much higher before and you know

00:33:44,770 --> 00:33:49,330
everyone is still working on this it's

00:33:46,720 --> 00:33:52,030
in the end it's about a society coming

00:33:49,330 --> 00:33:54,340
to sort of a conclusion on what we are

00:33:52,030 --> 00:33:55,780
willing to accept agree you want to jump

00:33:54,340 --> 00:33:57,490
in yeah you

00:33:55,780 --> 00:33:59,680
I mean I was just thinking about you

00:33:57,490 --> 00:34:01,540
know mentioning the cars and everything

00:33:59,680 --> 00:34:03,310
and if we think of the history of cars

00:34:01,540 --> 00:34:05,020
and and and pollution and all these

00:34:03,310 --> 00:34:08,140
things then we can think back in the

00:34:05,020 --> 00:34:09,820
1950s where cars were we're polluting a

00:34:08,140 --> 00:34:11,830
lot and we had we were building this

00:34:09,820 --> 00:34:13,600
there were this gigantic American cars

00:34:11,830 --> 00:34:16,390
going around for example which was the

00:34:13,600 --> 00:34:17,970
most popular cars and we get the 1970

00:34:16,390 --> 00:34:20,500
there's a clean act

00:34:17,970 --> 00:34:22,600
regulatory approach then you get

00:34:20,500 --> 00:34:25,330
scientists that are tracing pollution

00:34:22,600 --> 00:34:28,780
back to to cars and then everything

00:34:25,330 --> 00:34:30,580
evolves with different every group has

00:34:28,780 --> 00:34:32,710
their own expertise that they work

00:34:30,580 --> 00:34:34,179
together with unwanted goal the next

00:34:32,710 --> 00:34:37,149
goal is of course we could if we could

00:34:34,179 --> 00:34:39,130
get electric cars but it's more to say

00:34:37,149 --> 00:34:41,890
that you know every every stage and

00:34:39,130 --> 00:34:43,570
every actor has their own expertise that

00:34:41,890 --> 00:34:45,130
we somehow have to that's what I'm

00:34:43,570 --> 00:34:46,480
hoping that we can kind of do in these

00:34:45,130 --> 00:34:49,470
kind of forms is this is an

00:34:46,480 --> 00:34:53,470
awareness-raising exercise and hopefully

00:34:49,470 --> 00:34:56,169
whatever this exercise gets into it will

00:34:53,470 --> 00:35:02,320
feed into these processes of regulatory

00:34:56,169 --> 00:35:04,060
frameworks of engineering of scientists

00:35:02,320 --> 00:35:05,200
that are looking into the problems and

00:35:04,060 --> 00:35:07,000
the risks of

00:35:05,200 --> 00:35:09,070
and then in the end we'll probably be

00:35:07,000 --> 00:35:12,040
somewhere where for example we have and

00:35:09,070 --> 00:35:13,900
a scandal like the Volkswagen scandal

00:35:12,040 --> 00:35:16,030
where you have engineers that are

00:35:13,900 --> 00:35:18,490
actually prosecuting for building cheat

00:35:16,030 --> 00:35:20,829
devices for for cars that are polluting

00:35:18,490 --> 00:35:25,510
too much so so I think this is something

00:35:20,829 --> 00:35:28,810
we can learn from history yeah and yes I

00:35:25,510 --> 00:35:31,180
have a few concrete suggestions how to

00:35:28,810 --> 00:35:33,040
solve not how to save the world how to

00:35:31,180 --> 00:35:35,320
solve some of these problems so first of

00:35:33,040 --> 00:35:38,230
all I am a human rights lawyer so if I

00:35:35,320 --> 00:35:40,690
don't believe in saving the world and

00:35:38,230 --> 00:35:43,089
who will and I hope you join me in this

00:35:40,690 --> 00:35:45,820
but so the concrete the concrete topic

00:35:43,089 --> 00:35:48,869
there are very basic areas where even

00:35:45,820 --> 00:35:51,700
though we still believe in the European

00:35:48,869 --> 00:35:53,560
values and fundamental rights frameworks

00:35:51,700 --> 00:35:55,960
and the enforcement of it but still

00:35:53,560 --> 00:35:57,880
there are failures already so there are

00:35:55,960 --> 00:35:59,770
two areas we can talk about AI as much

00:35:57,880 --> 00:36:02,950
as we want but as long as we don't have

00:35:59,770 --> 00:36:05,770
proper basic online protections for

00:36:02,950 --> 00:36:08,440
privacy for instance then what are we

00:36:05,770 --> 00:36:10,750
even talking about here not let's not

00:36:08,440 --> 00:36:12,760
let the European Union and get away with

00:36:10,750 --> 00:36:15,520
not concluding the e privacy reform

00:36:12,760 --> 00:36:18,010
that's one and the other one the EU

00:36:15,520 --> 00:36:20,290
there are you based companies that are

00:36:18,010 --> 00:36:22,780
manufacturing technology that is being

00:36:20,290 --> 00:36:26,589
sold outside the European Union as and

00:36:22,780 --> 00:36:28,930
it's used to kill to surveil journalists

00:36:26,589 --> 00:36:31,390
human rights defenders political

00:36:28,930 --> 00:36:33,670
political opponents so surveillance

00:36:31,390 --> 00:36:37,000
export control is another very concrete

00:36:33,670 --> 00:36:40,300
topic where we can all get better

00:36:37,000 --> 00:36:42,220
results in order for any a I related

00:36:40,300 --> 00:36:43,800
regulation to be effective and

00:36:42,220 --> 00:36:47,079
meaningful

00:36:43,800 --> 00:36:49,390
yeah thanks I want to make two points I

00:36:47,079 --> 00:36:53,079
think in response to the question which

00:36:49,390 --> 00:36:55,060
you know may not be super satisfying but

00:36:53,079 --> 00:36:57,730
I think like Matias said it's an ongoing

00:36:55,060 --> 00:36:59,890
effort it's gonna and it's gonna be you

00:36:57,730 --> 00:37:02,200
chip away at it progressively with a

00:36:59,890 --> 00:37:04,030
group of people maybe even you know

00:37:02,200 --> 00:37:05,380
people here today but last week Fanny

00:37:04,030 --> 00:37:08,470
and I were at a conference that element

00:37:05,380 --> 00:37:10,359
a I organized with Mozilla to do kind of

00:37:08,470 --> 00:37:12,690
just that we got together of 15 15

00:37:10,359 --> 00:37:14,980
different people from 15 different

00:37:12,690 --> 00:37:16,329
organizations across stakeholder groups

00:37:14,980 --> 00:37:17,619
and we've pre spent two and a half days

00:37:16,329 --> 00:37:18,860
together trying to hash out what a

00:37:17,619 --> 00:37:20,840
regulatory framework could

00:37:18,860 --> 00:37:22,100
for a I could look like or governance

00:37:20,840 --> 00:37:24,740
framework whatever you want to call it

00:37:22,100 --> 00:37:26,780
from showing up expertise on AI and its

00:37:24,740 --> 00:37:27,890
risks within you know different

00:37:26,780 --> 00:37:29,870
government departments so that they're

00:37:27,890 --> 00:37:32,720
able to more meaningfully apply their

00:37:29,870 --> 00:37:35,930
legislation give it with a you know with

00:37:32,720 --> 00:37:38,120
giving regard to a as risks or you know

00:37:35,930 --> 00:37:40,340
to have different reporting mechanisms

00:37:38,120 --> 00:37:41,960
so I think I think you know it's gonna

00:37:40,340 --> 00:37:43,820
it's gonna happen progressively I think

00:37:41,960 --> 00:37:45,110
not we've seen that not a lot across the

00:37:43,820 --> 00:37:47,180
world not a lot of attention has been

00:37:45,110 --> 00:37:49,040
placed on this and the at least the

00:37:47,180 --> 00:37:50,540
national AI strategies but you know

00:37:49,040 --> 00:37:51,620
that's something that we have to have to

00:37:50,540 --> 00:37:53,750
do more work on it's going to happen

00:37:51,620 --> 00:37:55,580
progressively all that no I like to just

00:37:53,750 --> 00:37:57,770
like highlight in we talk about cars but

00:37:55,580 --> 00:38:00,740
I often think about the aviation system

00:37:57,770 --> 00:38:03,200
as like as an example of a space that

00:38:00,740 --> 00:38:05,530
has been highly innovative has become

00:38:03,200 --> 00:38:08,420
progressively more safe over time

00:38:05,530 --> 00:38:10,820
there's still a lot of you know issues

00:38:08,420 --> 00:38:12,020
related to to pollution but even even

00:38:10,820 --> 00:38:14,240
that I think will be changing in the

00:38:12,020 --> 00:38:16,250
next generation but it's it's an

00:38:14,240 --> 00:38:17,540
extremely sophisticated and onerous

00:38:16,250 --> 00:38:19,660
regulatory infrastructure from

00:38:17,540 --> 00:38:22,670
standardization to certification schemes

00:38:19,660 --> 00:38:25,010
to to oversight continuous monitoring

00:38:22,670 --> 00:38:28,670
and it didn't happen overnight

00:38:25,010 --> 00:38:31,010
it took about 80 years and and then and

00:38:28,670 --> 00:38:34,130
then it but it served as a good model

00:38:31,010 --> 00:38:36,890
for something like complex joint

00:38:34,130 --> 00:38:38,500
operations and they borrowed bits and

00:38:36,890 --> 00:38:41,060
pieces from it in kind of a weak

00:38:38,500 --> 00:38:44,690
licensing process to provide some

00:38:41,060 --> 00:38:46,370
oversight and and and also you know

00:38:44,690 --> 00:38:48,320
bring those operations along to a place

00:38:46,370 --> 00:38:50,120
where they'll be subject to the more

00:38:48,320 --> 00:38:51,800
rigorous ambit of the regulatory

00:38:50,120 --> 00:38:53,180
framework so I think we've done this

00:38:51,800 --> 00:38:54,290
before in other industries we can do it

00:38:53,180 --> 00:38:56,270
with AI we're starting at the very

00:38:54,290 --> 00:38:57,830
beginning and then another thing I'll

00:38:56,270 --> 00:38:59,690
say and I realize we're almost running

00:38:57,830 --> 00:39:02,060
out of time but no we're not we're not

00:38:59,690 --> 00:39:05,030
to enter the regular Q&A so yeah okay

00:39:02,060 --> 00:39:07,310
cool is the is we have to think about

00:39:05,030 --> 00:39:10,250
new ways of doing data governance and

00:39:07,310 --> 00:39:12,680
there's been a few sessions at Moz fest

00:39:10,250 --> 00:39:15,350
on data governance and and specifically

00:39:12,680 --> 00:39:17,750
on the opportunity that something like

00:39:15,350 --> 00:39:19,160
data Trust's could present I also saw a

00:39:17,750 --> 00:39:22,550
session I wasn't able to attend on data

00:39:19,160 --> 00:39:24,920
unions but we need to think of more ways

00:39:22,550 --> 00:39:27,440
of from the very beginning governing how

00:39:24,920 --> 00:39:29,570
we use data and having more democratic

00:39:27,440 --> 00:39:32,130
representation and how we how we make

00:39:29,570 --> 00:39:34,589
choices about how data should be used

00:39:32,130 --> 00:39:36,930
greater accountability I think I think

00:39:34,589 --> 00:39:38,880
it starts there so I think a lot of the

00:39:36,930 --> 00:39:40,890
AI governance questions may be solved

00:39:38,880 --> 00:39:42,420
through different models and approaches

00:39:40,890 --> 00:39:44,900
to data governance right there's some

00:39:42,420 --> 00:39:54,809
time for some actually one more question

00:39:44,900 --> 00:39:57,809
Kevin just to build on the the reference

00:39:54,809 --> 00:39:59,999
to the aviation industry and the

00:39:57,809 --> 00:40:03,059
increased regulation incremental

00:39:59,999 --> 00:40:05,190
regulation over time that seems to have

00:40:03,059 --> 00:40:09,809
been happening in parallel or maybe

00:40:05,190 --> 00:40:12,690
actually directly against greater rights

00:40:09,809 --> 00:40:15,089
of workers in that industry and I'm

00:40:12,690 --> 00:40:17,869
really curious as to how you see the

00:40:15,089 --> 00:40:21,589
governance growing governance of AI

00:40:17,869 --> 00:40:25,079
without the loss of kind of worker power

00:40:21,589 --> 00:40:28,739
so that this emergency khana me serves

00:40:25,079 --> 00:40:31,170
people economically but also without the

00:40:28,739 --> 00:40:33,599
loss of accountability that comes from

00:40:31,170 --> 00:40:38,910
taking human beings out of more and more

00:40:33,599 --> 00:40:42,329
aspects of that governance to the second

00:40:38,910 --> 00:40:45,630
part and then so on the the

00:40:42,329 --> 00:40:48,359
accountability piece for instance one

00:40:45,630 --> 00:40:50,699
one key recommendation from the Council

00:40:48,359 --> 00:40:55,140
of Europe Human Rights Commissioners

00:40:50,699 --> 00:40:57,239
paper on these ten steps to unbox AI one

00:40:55,140 --> 00:41:00,739
of the key recommendations is to always

00:40:57,239 --> 00:41:03,180
have either a natural or a legal person

00:41:00,739 --> 00:41:07,199
identified as accountable for the

00:41:03,180 --> 00:41:09,719
decision so it I think that's the answer

00:41:07,199 --> 00:41:14,729
like we cannot move away from from that

00:41:09,719 --> 00:41:17,489
governance structure this is a question

00:41:14,729 --> 00:41:19,949
related to workers rights more about the

00:41:17,489 --> 00:41:22,410
use of automation and autonomous systems

00:41:19,949 --> 00:41:30,239
in aviation or about conditions for

00:41:22,410 --> 00:41:31,949
workers in more generally both I think

00:41:30,239 --> 00:41:34,410
there's something that happens just

00:41:31,949 --> 00:41:37,319
psychologically once more and more

00:41:34,410 --> 00:41:40,140
governance is around automated systems

00:41:37,319 --> 00:41:43,079
that has the psychological effect of

00:41:40,140 --> 00:41:45,220
removing the person from the center that

00:41:43,079 --> 00:41:48,310
also makes accountability harder

00:41:45,220 --> 00:41:50,710
and then the issue of economic

00:41:48,310 --> 00:41:53,619
prosperity for human beings within that

00:41:50,710 --> 00:41:55,300
field as well I think yeah they feed

00:41:53,619 --> 00:41:58,150
each other the minute we start focusing

00:41:55,300 --> 00:42:00,400
so much on automated systems the less

00:41:58,150 --> 00:42:02,680
importance the human being becomes the

00:42:00,400 --> 00:42:04,630
easier it is to justify not paying them

00:42:02,680 --> 00:42:07,000
living a living wage or they are less

00:42:04,630 --> 00:42:09,490
integral to the system the harder it is

00:42:07,000 --> 00:42:11,920
to manage accountability and so on and

00:42:09,490 --> 00:42:13,060
so forth okay I'll try and answer some

00:42:11,920 --> 00:42:15,849
of those questions I think there a few

00:42:13,060 --> 00:42:17,050
sub sub questions but it reminds you a

00:42:15,849 --> 00:42:19,390
conversation Mateus and I were having

00:42:17,050 --> 00:42:21,070
before beforehand about the the blowing

00:42:19,390 --> 00:42:26,140
max max eighths and the use of

00:42:21,070 --> 00:42:28,720
autonomous systems and yeah well for the

00:42:26,140 --> 00:42:30,700
first thing I would say is that as in

00:42:28,720 --> 00:42:33,490
aviation the use of autonomous systems

00:42:30,700 --> 00:42:35,770
or just in the AI ecosystem more broadly

00:42:33,490 --> 00:42:37,800
I think we should I think we should be

00:42:35,770 --> 00:42:40,119
very careful not to attribute

00:42:37,800 --> 00:42:42,880
accountability to anything other than a

00:42:40,119 --> 00:42:44,470
human and in some case that might be

00:42:42,880 --> 00:42:45,849
there might be a legal person or might

00:42:44,470 --> 00:42:47,020
be a physical person but I think that's

00:42:45,849 --> 00:42:49,030
that

00:42:47,020 --> 00:42:50,859
we should not be confused about that is

00:42:49,030 --> 00:42:54,010
the the the accountability and liability

00:42:50,859 --> 00:42:59,320
lies with with with with a person and

00:42:54,010 --> 00:43:01,450
then with regards to workers yeah I mean

00:42:59,320 --> 00:43:04,030
there's the displacement of workers and

00:43:01,450 --> 00:43:05,170
the devaluing of workers this is a

00:43:04,030 --> 00:43:07,030
problem and this is something that's

00:43:05,170 --> 00:43:09,369
companies and governments also I mean

00:43:07,030 --> 00:43:11,500
civil society more broadly has to has to

00:43:09,369 --> 00:43:13,210
address and there needs to be there

00:43:11,500 --> 00:43:15,130
should be policies in place possibly

00:43:13,210 --> 00:43:17,070
with you know AI industries that are

00:43:15,130 --> 00:43:19,300
that are contributing to this

00:43:17,070 --> 00:43:22,900
participating in these in programs to

00:43:19,300 --> 00:43:24,700
upskill retrain workers or if it should

00:43:22,900 --> 00:43:26,890
they so desire or have some type of

00:43:24,700 --> 00:43:27,880
other compensation scheme you know these

00:43:26,890 --> 00:43:29,230
are some of the options that could be

00:43:27,880 --> 00:43:30,910
considered to alleviate some of the

00:43:29,230 --> 00:43:33,849
burden of the dislocation that workers

00:43:30,910 --> 00:43:36,119
will feel and I know I hope that answers

00:43:33,849 --> 00:43:39,460
in part of these some of your question

00:43:36,119 --> 00:43:41,440
chime in on that okay well I guess we

00:43:39,460 --> 00:43:45,750
can end the panel there give a big round

00:43:41,440 --> 00:43:45,750
of applause for our panelists stage

00:43:47,589 --> 00:43:51,730
coming up next we have a long break

00:43:49,900 --> 00:43:53,349
we'll be back at 3:15 where we have one

00:43:51,730 --> 00:43:56,819
more talk and one more panel I'll see

00:43:53,349 --> 00:43:56,819

YouTube URL: https://www.youtube.com/watch?v=oXEOXDS8nJ8


