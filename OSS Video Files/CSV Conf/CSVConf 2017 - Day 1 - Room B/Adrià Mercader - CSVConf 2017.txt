Title: AdriaÌ€ Mercader - CSVConf 2017
Publication date: 2017-05-28
Playlist: CSVConf 2017 - Day 1 - Room B
Description: 
	
Captions: 
	00:00:01,840 --> 00:00:07,000
hi everybody thanks for being here after

00:00:04,450 --> 00:00:08,740
a long day of great talks my name is

00:00:07,000 --> 00:00:11,170
adria america ray and i'd like to talk

00:00:08,740 --> 00:00:13,889
to you about continuous data validation

00:00:11,170 --> 00:00:18,820
and hopefully get you excited about it

00:00:13,889 --> 00:00:21,430
and I'm a developer I work for up nas

00:00:18,820 --> 00:00:23,800
international I spent a long time on

00:00:21,430 --> 00:00:25,390
github so if you ever interacted with

00:00:23,800 --> 00:00:27,850
this parrot in an issue or a pull

00:00:25,390 --> 00:00:30,580
request that with me I'm also on Twitter

00:00:27,850 --> 00:00:32,800
a ton or Caray right sometimes I tweet

00:00:30,580 --> 00:00:36,370
about open later and I work at open

00:00:32,800 --> 00:00:39,579
knowledge and open knowledge is a

00:00:36,370 --> 00:00:42,130
nonprofit that was founded on 2004 with

00:00:39,579 --> 00:00:44,399
the mission of opening up all public

00:00:42,130 --> 00:00:46,660
interest information and making and

00:00:44,399 --> 00:00:49,590
deriving insight and knowledge from it

00:00:46,660 --> 00:00:52,629
to have an impact on people's lives and

00:00:49,590 --> 00:00:55,360
to do so we build tools and communities

00:00:52,629 --> 00:00:58,809
around things like data literacy or data

00:00:55,360 --> 00:01:01,390
publication standards or somatic areas

00:00:58,809 --> 00:01:03,789
and just to give you a sense of the

00:01:01,390 --> 00:01:06,700
range of projects that Open Knowledge is

00:01:03,789 --> 00:01:09,130
involved with on the data literacy fron

00:01:06,700 --> 00:01:12,789
we have the school of data fellowship

00:01:09,130 --> 00:01:14,709
program this has had helped teach data

00:01:12,789 --> 00:01:18,310
skills to hundreds of people around the

00:01:14,709 --> 00:01:21,099
world we have open trials which is an

00:01:18,310 --> 00:01:23,109
open and linked database of all data and

00:01:21,099 --> 00:01:25,829
documents related to clinical trials

00:01:23,109 --> 00:01:29,740
that is accessible to researchers

00:01:25,829 --> 00:01:32,109
doctors and patients the global open

00:01:29,740 --> 00:01:34,329
data index is a crowdsource survey of

00:01:32,109 --> 00:01:37,840
the state of open data around the world

00:01:34,329 --> 00:01:41,499
and it's performed yearly and actually

00:01:37,840 --> 00:01:45,969
the 2016 edition has an old life today

00:01:41,499 --> 00:01:48,249
and open spending is really mature and

00:01:45,969 --> 00:01:49,989
well stablished project for and

00:01:48,249 --> 00:01:51,759
community and a question of resources

00:01:49,989 --> 00:01:54,239
from the publication of financial data

00:01:51,759 --> 00:01:56,469
like budgets and spending data I

00:01:54,239 --> 00:01:57,669
encourage you to check the our website

00:01:56,469 --> 00:02:01,359
to learn more about this and other

00:01:57,669 --> 00:02:04,149
projects that Open Knowledge is involved

00:02:01,359 --> 00:02:05,859
with but when I will talk about a bit

00:02:04,149 --> 00:02:07,599
more because I'm the one I'm more

00:02:05,859 --> 00:02:10,689
involved so we get its own slide its

00:02:07,599 --> 00:02:12,939
second and second it's an open source

00:02:10,689 --> 00:02:14,090
software for publishing open data at

00:02:12,939 --> 00:02:17,810
powers

00:02:14,090 --> 00:02:20,360
many open data portals around the world

00:02:17,810 --> 00:02:23,360
from national instances like little goth

00:02:20,360 --> 00:02:26,060
in the States or Canada to more regional

00:02:23,360 --> 00:02:29,900
and local ones but also it's used from

00:02:26,060 --> 00:02:32,650
by universities or research institutions

00:02:29,900 --> 00:02:37,220
and museums to publish their open data

00:02:32,650 --> 00:02:40,069
and if the velopment started quite a few

00:02:37,220 --> 00:02:42,260
years ago and during all these years of

00:02:40,069 --> 00:02:45,190
knowledge has helped deploy and

00:02:42,260 --> 00:02:48,380
customized a lot of these instances and

00:02:45,190 --> 00:02:51,110
from our work with a lots of

00:02:48,380 --> 00:02:52,489
organizations around the world and our

00:02:51,110 --> 00:02:54,950
exposure to different little

00:02:52,489 --> 00:02:57,470
publications flows and all this has

00:02:54,950 --> 00:03:01,610
highlighted what we call the friction

00:02:57,470 --> 00:03:04,280
involved in working with data and right

00:03:01,610 --> 00:03:06,739
from one end of the process were later

00:03:04,280 --> 00:03:08,720
publishers are trying to make data

00:03:06,739 --> 00:03:10,940
available online to as many people as

00:03:08,720 --> 00:03:14,120
possible to the other end where people

00:03:10,940 --> 00:03:16,130
is trying to use data it's a pain

00:03:14,120 --> 00:03:18,610
towards open data with having

00:03:16,130 --> 00:03:21,910
highlighted on many occasions during and

00:03:18,610 --> 00:03:25,100
today's talks there are issues about

00:03:21,910 --> 00:03:28,310
later being difficult to discover or to

00:03:25,100 --> 00:03:31,040
find and data is difficult to export

00:03:28,310 --> 00:03:32,329
from one system to another or there's

00:03:31,040 --> 00:03:35,170
there's a lack of schemas and

00:03:32,329 --> 00:03:37,730
conventions that allow you to compare or

00:03:35,170 --> 00:03:40,730
employ your dataset directly into your

00:03:37,730 --> 00:03:42,890
own tool and perhaps the most pressing

00:03:40,730 --> 00:03:45,739
issues again has been highlighted many

00:03:42,890 --> 00:03:48,280
times is that data is in many cases

00:03:45,739 --> 00:03:50,540
poorly structure and such a low quality

00:03:48,280 --> 00:03:53,000
so even when you find it you have to

00:03:50,540 --> 00:03:56,269
spend quite a lot of time cleaning up

00:03:53,000 --> 00:03:58,579
and tweaking it so it can be imported

00:03:56,269 --> 00:04:03,079
into your own tool and use it for your

00:03:58,579 --> 00:04:05,420
needs and this is this is a process that

00:04:03,079 --> 00:04:08,810
has no which is problem that has no

00:04:05,420 --> 00:04:11,660
immediate solution and has to be tackle

00:04:08,810 --> 00:04:13,850
from different fronts but to help

00:04:11,660 --> 00:04:15,410
address some of these issues that Open

00:04:13,850 --> 00:04:17,229
Knowledge we started a couple of years

00:04:15,410 --> 00:04:20,419
ago a project called frictionless data

00:04:17,229 --> 00:04:22,970
which as the name implies it's all about

00:04:20,419 --> 00:04:26,400
removing at least some of the friction

00:04:22,970 --> 00:04:29,430
in working with data

00:04:26,400 --> 00:04:31,560
two main goals is to make improve the

00:04:29,430 --> 00:04:34,169
way in which data share consume and

00:04:31,560 --> 00:04:36,210
analyze and at the same time to make

00:04:34,169 --> 00:04:40,050
easier for publishers to publish good

00:04:36,210 --> 00:04:42,560
quality data and to do so we're acting

00:04:40,050 --> 00:04:46,560
on two fronts we're working on a set of

00:04:42,560 --> 00:04:48,630
specifications and standards with strong

00:04:46,560 --> 00:04:51,900
emphasis on them being lightweight and

00:04:48,630 --> 00:04:54,509
easy to adapt to existing data and that

00:04:51,900 --> 00:04:56,639
build on top of existing best practices

00:04:54,509 --> 00:04:58,580
and on the other hand we'll building

00:04:56,639 --> 00:05:01,800
tools on top of the standards that

00:04:58,580 --> 00:05:07,470
actually have the impact that we want to

00:05:01,800 --> 00:05:11,280
see on the data publication process at

00:05:07,470 --> 00:05:14,400
the core of the the fictional data

00:05:11,280 --> 00:05:16,760
project is the data package which is

00:05:14,400 --> 00:05:19,800
which builds on the concept of

00:05:16,760 --> 00:05:23,849
containerization of data and essentially

00:05:19,800 --> 00:05:27,419
is simple packaging or wrapping format

00:05:23,849 --> 00:05:30,300
for packaging data in in order to make

00:05:27,419 --> 00:05:33,720
it interchangeable between people and

00:05:30,300 --> 00:05:36,539
tools and we believe that this

00:05:33,720 --> 00:05:37,800
abstraction between the actual contents

00:05:36,539 --> 00:05:40,710
of the data or the format of the data

00:05:37,800 --> 00:05:43,710
and the tools that it's using it they're

00:05:40,710 --> 00:05:45,990
using it can have a similar impact to

00:05:43,710 --> 00:05:48,449
the one that technologies like docker

00:05:45,990 --> 00:05:51,000
has had on application deployment and

00:05:48,449 --> 00:05:53,970
development essentially by decoupling

00:05:51,000 --> 00:05:57,210
the data producers from the data

00:05:53,970 --> 00:06:01,919
consumers by providing this simple

00:05:57,210 --> 00:06:05,880
common interchange and format how do

00:06:01,919 --> 00:06:09,000
these data packages look like and so

00:06:05,880 --> 00:06:12,650
here's an example of tabular data packet

00:06:09,000 --> 00:06:16,470
a specific profiles for packaging

00:06:12,650 --> 00:06:19,650
tabular data which is store on one or

00:06:16,470 --> 00:06:23,010
more CSV files and on one hand you have

00:06:19,650 --> 00:06:26,400
the CSV or CSV from multiple files as

00:06:23,010 --> 00:06:30,210
well and next to them we have the data

00:06:26,400 --> 00:06:33,419
package the structure which is a data

00:06:30,210 --> 00:06:35,219
packet or JSON file which contains basic

00:06:33,419 --> 00:06:39,660
metadata or the data like the title

00:06:35,219 --> 00:06:41,790
keywords and license contact info

00:06:39,660 --> 00:06:46,410
and of course the pointers to the actual

00:06:41,790 --> 00:06:48,330
data files or resources and also

00:06:46,410 --> 00:06:50,720
embedded on these data resources you can

00:06:48,330 --> 00:06:53,040
make use of another really powerful

00:06:50,720 --> 00:06:56,400
specification which is a table schema

00:06:53,040 --> 00:06:58,560
which is used to describe the actual

00:06:56,400 --> 00:07:01,710
contents of the data so how this data is

00:06:58,560 --> 00:07:03,480
expected to look like it allows you to

00:07:01,710 --> 00:07:06,810
define things like the name of the field

00:07:03,480 --> 00:07:10,320
or what type these fields are like

00:07:06,810 --> 00:07:12,180
numbers date strings etc but also

00:07:10,320 --> 00:07:14,970
constraints on the fields like things

00:07:12,180 --> 00:07:18,780
like this field is the primary key for

00:07:14,970 --> 00:07:20,940
this file or data set or these numeric

00:07:18,780 --> 00:07:24,300
fields and also row from can only go

00:07:20,940 --> 00:07:27,060
from 0 to 100 or this field can only

00:07:24,300 --> 00:07:29,130
have a value which is one of these

00:07:27,060 --> 00:07:32,700
predefined set of values and things like

00:07:29,130 --> 00:07:34,950
that and that's pretty much it so the

00:07:32,700 --> 00:07:36,540
combination of data one hand and this

00:07:34,950 --> 00:07:39,420
data packaged edition on the other is

00:07:36,540 --> 00:07:42,630
what forms the power data package and

00:07:39,420 --> 00:07:45,660
this descriptors are really easy to do

00:07:42,630 --> 00:07:48,120
to create you can make them by hand or

00:07:45,660 --> 00:07:51,270
using an an online helper that we

00:07:48,120 --> 00:07:53,850
created or more likely some tool or

00:07:51,270 --> 00:07:56,310
integration will create them for you but

00:07:53,850 --> 00:08:00,330
what's exciting of course is what once

00:07:56,310 --> 00:08:01,860
we have this common layer that that

00:08:00,330 --> 00:08:05,010
wraps different data into the same

00:08:01,860 --> 00:08:07,440
interface what exciting is the tools

00:08:05,010 --> 00:08:08,730
that we can plug into that into it so I

00:08:07,440 --> 00:08:09,810
don't have time to go through all the

00:08:08,730 --> 00:08:11,340
tooling that we're working on a

00:08:09,810 --> 00:08:14,910
frictionless data but just to mention

00:08:11,340 --> 00:08:16,560
that and there are libraries to explore

00:08:14,910 --> 00:08:20,760
an import data packages for instance

00:08:16,560 --> 00:08:24,450
from two common to R or SQL databases or

00:08:20,760 --> 00:08:28,170
a bigquery there's the data packages

00:08:24,450 --> 00:08:32,160
pipelines framework the that can be used

00:08:28,170 --> 00:08:35,280
to transform streams of tabular data and

00:08:32,160 --> 00:08:39,229
of course there are validation tools as

00:08:35,280 --> 00:08:42,900
well as I said later quality and

00:08:39,229 --> 00:08:45,120
validation is a huge topic that will

00:08:42,900 --> 00:08:48,060
gain more and more importance as we go

00:08:45,120 --> 00:08:50,040
forward so one of the most important

00:08:48,060 --> 00:08:53,660
pieces of the fictional data toolset is

00:08:50,040 --> 00:08:56,460
the good tables validation library and

00:08:53,660 --> 00:09:00,080
it's a library to perform checks and

00:08:56,460 --> 00:09:02,040
validate tabular data and get

00:09:00,080 --> 00:09:04,770
implementations on Python and JavaScript

00:09:02,040 --> 00:09:07,980
and essentially it can run two types of

00:09:04,770 --> 00:09:10,860
checks at first the ones we call like

00:09:07,980 --> 00:09:13,710
structural checks or look for structural

00:09:10,860 --> 00:09:16,230
errors things like extra columns or

00:09:13,710 --> 00:09:18,570
missing headers or duplicate rows or

00:09:16,230 --> 00:09:22,530
blank rows things that affect the actual

00:09:18,570 --> 00:09:25,860
structure of the CSV but also if there's

00:09:22,530 --> 00:09:28,620
one provided it can validate the beta

00:09:25,860 --> 00:09:30,300
file against the schema so it will look

00:09:28,620 --> 00:09:33,060
for things like the ones I mention about

00:09:30,300 --> 00:09:37,170
the field types etc so it will graze

00:09:33,060 --> 00:09:39,570
errors like online 200 300 and 21 this

00:09:37,170 --> 00:09:43,260
field was supposed to be a number but

00:09:39,570 --> 00:09:45,660
someone wrote a comment on it or this

00:09:43,260 --> 00:09:50,000
date does not adhere to the format that

00:09:45,660 --> 00:09:53,280
was pretty sign of things like that and

00:09:50,000 --> 00:09:55,140
this works really well it's used on

00:09:53,280 --> 00:09:58,110
different projects and in different

00:09:55,140 --> 00:09:58,890
contexts and you can import it on your

00:09:58,110 --> 00:10:00,810
own scripts

00:09:58,890 --> 00:10:03,240
there's a common line interface to

00:10:00,810 --> 00:10:06,240
perform these checks that's even an

00:10:03,240 --> 00:10:09,240
online tool built on top of feed that

00:10:06,240 --> 00:10:12,630
where you can upload the CSV is and it

00:10:09,240 --> 00:10:16,980
returns you the the report with the

00:10:12,630 --> 00:10:20,130
errors but all this is still a manual

00:10:16,980 --> 00:10:22,589
project a manual process so if we really

00:10:20,130 --> 00:10:25,350
want to bring that evaluation to the

00:10:22,589 --> 00:10:28,020
foreground and making and making it a

00:10:25,350 --> 00:10:31,770
default on the data publication process

00:10:28,020 --> 00:10:35,660
manual checking and monetization is not

00:10:31,770 --> 00:10:39,950
going to be enough what we need is

00:10:35,660 --> 00:10:42,630
automatic continuous data validation and

00:10:39,950 --> 00:10:44,580
we believe that data publication can

00:10:42,630 --> 00:10:46,530
benefit much in the same way as the

00:10:44,580 --> 00:10:48,870
software development as the well has

00:10:46,530 --> 00:10:52,890
benefited from services like Travis CI

00:10:48,870 --> 00:10:54,720
or circle CI and for those who are not

00:10:52,890 --> 00:10:58,620
familiar with them these services work

00:10:54,720 --> 00:11:01,829
by running a predefined set of tests

00:10:58,620 --> 00:11:04,920
against code ways so every time that

00:11:01,829 --> 00:11:06,870
code this push to a repository with

00:11:04,920 --> 00:11:08,790
services run the test and provide

00:11:06,870 --> 00:11:12,110
feedback to the developers about whether

00:11:08,790 --> 00:11:14,040
the DES pass or not so these gives

00:11:12,110 --> 00:11:16,800
difficulty well oppress continuously

00:11:14,040 --> 00:11:19,889
talk about whether the code is working

00:11:16,800 --> 00:11:22,110
as they expected and the same principles

00:11:19,889 --> 00:11:25,940
can apply to the later publication

00:11:22,110 --> 00:11:30,480
process so whenever data is updated

00:11:25,940 --> 00:11:32,160
checks to run and if they fail like the

00:11:30,480 --> 00:11:34,949
publishers should get this feedback as

00:11:32,160 --> 00:11:37,829
soon as possible and to identify the

00:11:34,949 --> 00:11:42,029
data issues early in the process and and

00:11:37,829 --> 00:11:44,730
fix them as they as they appear this

00:11:42,029 --> 00:11:47,339
regular feedback will lead to better

00:11:44,730 --> 00:11:50,850
data quality by ensuring that the data

00:11:47,339 --> 00:11:54,000
at least what it's expected to be and

00:11:50,850 --> 00:11:55,980
also crucially it will ensure that this

00:11:54,000 --> 00:11:58,440
quality is maintained over time here in

00:11:55,980 --> 00:12:00,350
its different tools or person are

00:11:58,440 --> 00:12:03,600
involved in the data publication process

00:12:00,350 --> 00:12:06,199
and at the other end of course for the

00:12:03,600 --> 00:12:09,779
data consumers for users it gives a

00:12:06,199 --> 00:12:12,329
reassurance that the data is well

00:12:09,779 --> 00:12:16,230
structured and that can be relied on for

00:12:12,329 --> 00:12:21,720
analysis or for for for using it for

00:12:16,230 --> 00:12:25,350
your needs so we are really excited

00:12:21,720 --> 00:12:27,860
about this concept and to bring it

00:12:25,350 --> 00:12:31,050
forward and make it possible within

00:12:27,860 --> 00:12:33,870
working on on a service that does

00:12:31,050 --> 00:12:36,570
exactly that and we hope that we'll win

00:12:33,870 --> 00:12:39,899
and continuous data validation - to

00:12:36,570 --> 00:12:43,949
everybody it is called whatever that IO

00:12:39,899 --> 00:12:46,019
and it builds on top of well-established

00:12:43,949 --> 00:12:49,769
and material frictionless data tooling

00:12:46,019 --> 00:12:52,019
and is basically designed to connect to

00:12:49,769 --> 00:12:54,199
different data sources and whenever data

00:12:52,019 --> 00:12:59,339
is updated on these sources and

00:12:54,199 --> 00:13:02,370
validation jobs will run and the data

00:12:59,339 --> 00:13:06,420
publishers will get notified if the if

00:13:02,370 --> 00:13:10,350
the data is incorrect in some regard

00:13:06,420 --> 00:13:12,810
regarding to this - this checks for the

00:13:10,350 --> 00:13:14,910
first initial beta version we are

00:13:12,810 --> 00:13:17,680
focusing on data that it's store on

00:13:14,910 --> 00:13:20,260
github and on

00:13:17,680 --> 00:13:23,529
not honestly but we definitely want to

00:13:20,260 --> 00:13:26,050
explore new integrations and seeing how

00:13:23,529 --> 00:13:30,820
this can be extended that can be useful

00:13:26,050 --> 00:13:35,110
to more publishers and more users and so

00:13:30,820 --> 00:13:38,410
let me give you a quick tour and once

00:13:35,110 --> 00:13:41,470
you sign up and log in on the system you

00:13:38,410 --> 00:13:45,070
get a simple dashboard with your data

00:13:41,470 --> 00:13:48,339
sources that shows the result of the

00:13:45,070 --> 00:13:52,180
last validation job run on the on each

00:13:48,339 --> 00:13:54,880
of these sources and there's a link to a

00:13:52,180 --> 00:13:57,390
page to manage these sources so adding

00:13:54,880 --> 00:14:01,600
new ones and removing them and for

00:13:57,390 --> 00:14:03,250
Amazon s3 adding any source implies like

00:14:01,600 --> 00:14:06,430
providing an access key and a lot an

00:14:03,250 --> 00:14:08,610
access key under the bucket name and for

00:14:06,430 --> 00:14:12,670
github it's a really similar process to

00:14:08,610 --> 00:14:15,310
and similar services like when you out

00:14:12,670 --> 00:14:16,959
you also write the application and you

00:14:15,310 --> 00:14:19,360
synchronize your organization's in the

00:14:16,959 --> 00:14:20,770
ten repositories and once you get the

00:14:19,360 --> 00:14:22,620
list of repositories you flip the switch

00:14:20,770 --> 00:14:26,260
and the ones that you want to validate

00:14:22,620 --> 00:14:28,480
and then under the hood and the service

00:14:26,260 --> 00:14:31,830
calls the necessary API census that

00:14:28,480 --> 00:14:34,839
alone hooks so whenever a file is

00:14:31,830 --> 00:14:37,029
updated on the Amazon s3 bucket or a

00:14:34,839 --> 00:14:40,180
committees push to the github repository

00:14:37,029 --> 00:14:44,620
or a pull request is created validation

00:14:40,180 --> 00:14:48,190
job is run and feedback is provided on

00:14:44,620 --> 00:14:53,080
the github case and good Calvo Sotelo

00:14:48,190 --> 00:14:56,589
becomes just another github check and

00:14:53,080 --> 00:15:00,400
the results are displayed on the user

00:14:56,589 --> 00:15:03,339
interface and ate bacon block in merging

00:15:00,400 --> 00:15:05,560
put request if the repository is

00:15:03,339 --> 00:15:07,990
configured like that and of course you

00:15:05,560 --> 00:15:09,850
can add your the cute little badge on

00:15:07,990 --> 00:15:16,230
the readme to show that your data is

00:15:09,850 --> 00:15:20,110
valid or not and this is an example of a

00:15:16,230 --> 00:15:22,270
report a failing report apart from

00:15:20,110 --> 00:15:25,329
general information about the job like

00:15:22,270 --> 00:15:28,470
who lead the community or who push to

00:15:25,329 --> 00:15:30,960
what branch or the commit message etc

00:15:28,470 --> 00:15:33,690
there's a report

00:15:30,960 --> 00:15:37,680
below with the eros group by error typed

00:15:33,690 --> 00:15:40,260
and a preview of the growth are failing

00:15:37,680 --> 00:15:44,460
to give a really immediate feedback to

00:15:40,260 --> 00:15:47,550
the data publisher and we definitely

00:15:44,460 --> 00:15:51,270
want to improve this by having better

00:15:47,550 --> 00:15:53,730
descriptions and Pro even if possible

00:15:51,270 --> 00:15:57,900
give suggestions on how to fix this

00:15:53,730 --> 00:16:03,020
problem so we can listen and be easily

00:15:57,900 --> 00:16:03,020
fix on the on the data set itself and

00:16:04,010 --> 00:16:10,950
this the validation will will run Glee

00:16:07,260 --> 00:16:13,920
two types of projects that I mentioned

00:16:10,950 --> 00:16:17,700
before so structural checks and if this

00:16:13,920 --> 00:16:19,770
schema provided either a separate file

00:16:17,700 --> 00:16:22,620
or embedded in a data packet it will

00:16:19,770 --> 00:16:25,950
also validate the inter contents and of

00:16:22,620 --> 00:16:28,290
the file just to be clear and you can

00:16:25,950 --> 00:16:31,410
run the validation against any

00:16:28,290 --> 00:16:35,550
repository that has power data like CSV

00:16:31,410 --> 00:16:37,770
or Excel files but of course if s schema

00:16:35,550 --> 00:16:41,150
provided either separately or embedded

00:16:37,770 --> 00:16:44,400
the test will be more thorough and

00:16:41,150 --> 00:16:50,730
consistent with what the data set is is

00:16:44,400 --> 00:16:53,460
expected to be to further control

00:16:50,730 --> 00:16:55,800
evaluation process thus an optional put

00:16:53,460 --> 00:16:58,050
tables a general file where you can

00:16:55,800 --> 00:17:00,800
define things like where the data is

00:16:58,050 --> 00:17:03,390
located where the scheme is located and

00:17:00,800 --> 00:17:07,740
things like what the delimiter is for

00:17:03,390 --> 00:17:10,320
CSV is or what rows to skip etc if

00:17:07,740 --> 00:17:12,510
there's no general file define the

00:17:10,320 --> 00:17:15,450
service will look for a data package the

00:17:12,510 --> 00:17:17,250
JSON file and it can find one and again

00:17:15,450 --> 00:17:19,200
it will look for all the tables on the

00:17:17,250 --> 00:17:21,680
repository enters one the validation

00:17:19,200 --> 00:17:21,680
against them

00:17:23,520 --> 00:17:29,830
that really might see that this stage

00:17:26,039 --> 00:17:32,580
just give you an idea of what we been

00:17:29,830 --> 00:17:35,230
thinking about introducing in the future

00:17:32,580 --> 00:17:38,140
maybe just paid plans to make this very

00:17:35,230 --> 00:17:42,000
sustainable that enable extra features

00:17:38,140 --> 00:17:44,919
like private repos or things like that

00:17:42,000 --> 00:17:48,750
new integrations with other backends I

00:17:44,919 --> 00:17:51,520
think like Google Drive Dropbox and

00:17:48,750 --> 00:17:54,130
doubles I think is one particularly

00:17:51,520 --> 00:17:56,049
worth exploring because it could target

00:17:54,130 --> 00:17:57,760
a whole new set of users or

00:17:56,049 --> 00:18:01,179
organizations where they're more

00:17:57,760 --> 00:18:05,350
familiar with and desktop tools that

00:18:01,179 --> 00:18:08,260
this cloud-based systems and also could

00:18:05,350 --> 00:18:10,740
lower the like the technical barrier

00:18:08,260 --> 00:18:15,279
entry barrier to this kind of tools but

00:18:10,740 --> 00:18:16,690
also we definitely want to enable API

00:18:15,279 --> 00:18:20,110
access to the service we can be

00:18:16,690 --> 00:18:25,120
integrated to any custom workflow or or

00:18:20,110 --> 00:18:28,059
tool essentially by sending file or list

00:18:25,120 --> 00:18:30,580
of files to the service and getting back

00:18:28,059 --> 00:18:34,090
a report of the valuation before and

00:18:30,580 --> 00:18:36,159
that will hopefully enable people to

00:18:34,090 --> 00:18:38,620
integrate it on their own workflows on

00:18:36,159 --> 00:18:42,240
their own tools and/or on tools like

00:18:38,620 --> 00:18:45,190
catalogs like sitting or others and make

00:18:42,240 --> 00:18:47,020
later publication or just another normal

00:18:45,190 --> 00:18:52,570
part of the this later publication

00:18:47,020 --> 00:18:54,700
process and think that's pretty much it

00:18:52,570 --> 00:18:58,409
at this point I really encourage you to

00:18:54,700 --> 00:19:00,640
give it a go who'd they would allow and

00:18:58,409 --> 00:19:03,640
its early stages there are a lot of

00:19:00,640 --> 00:19:06,490
rough edges to polish so bear with us

00:19:03,640 --> 00:19:07,990
and there's a link or the header of the

00:19:06,490 --> 00:19:12,039
application to give us feedback that

00:19:07,990 --> 00:19:14,610
link to this thread in our forum so

00:19:12,039 --> 00:19:18,429
please add any comments er that you

00:19:14,610 --> 00:19:20,200
purchasing they're worth facing we are

00:19:18,429 --> 00:19:22,630
also on bidder if you want to have a

00:19:20,200 --> 00:19:24,250
chat and of course myself and my

00:19:22,630 --> 00:19:27,340
colleagues will be around for the rest

00:19:24,250 --> 00:19:29,020
of the conference so gravis with you if

00:19:27,340 --> 00:19:38,679
you want to have a chat

00:19:29,020 --> 00:19:52,830
and that was it thank you very much any

00:19:38,679 --> 00:19:55,120
questions no there's JSON objects

00:19:52,830 --> 00:19:58,450
sorry turn my computer so I'm not sure

00:19:55,120 --> 00:20:00,850
how to show you one but anyway it's a

00:19:58,450 --> 00:20:03,700
JSON file with one of the keys being

00:20:00,850 --> 00:20:06,370
schema one it has a schema laser like a

00:20:03,700 --> 00:20:09,610
an array of fields and it's field is an

00:20:06,370 --> 00:20:12,880
object that's a type date and format

00:20:09,610 --> 00:20:15,100
this I can definitely point you to the

00:20:12,880 --> 00:20:32,380
ER schema now and show you examples of

00:20:15,100 --> 00:20:39,820
it any other question and give me an

00:20:32,380 --> 00:20:42,970
example freeze like and not building on

00:20:39,820 --> 00:20:45,429
the on the library that under life is

00:20:42,970 --> 00:20:47,289
good tables but this library is

00:20:45,429 --> 00:20:50,110
extensible so you can provide your own

00:20:47,289 --> 00:20:53,169
checks so we can definitely look at the

00:20:50,110 --> 00:20:55,179
way of integrating this or making this

00:20:53,169 --> 00:20:56,980
customization expose this customization

00:20:55,179 --> 00:21:00,100
and allowing users to provide their own

00:20:56,980 --> 00:21:03,370
checks and that could be actually really

00:21:00,100 --> 00:21:05,470
really powerful especially for taking

00:21:03,370 --> 00:21:08,529
form for spelling as you said but also

00:21:05,470 --> 00:21:10,690
on recess on research data to make sure

00:21:08,529 --> 00:21:13,210
that no standard deviation doesn't go

00:21:10,690 --> 00:21:14,559
from these margins on a particular

00:21:13,210 --> 00:21:15,880
column or things like that

00:21:14,559 --> 00:21:18,330
so that could be also really powerful

00:21:15,880 --> 00:21:18,330
yeah

00:21:19,130 --> 00:21:33,560
is there ways you'll accomplish little

00:21:21,610 --> 00:21:36,260
tracks like comedies yes yes I don't

00:21:33,560 --> 00:21:43,010
think that's possible now but it's a

00:21:36,260 --> 00:21:45,550
great idea to race on on this set any

00:21:43,010 --> 00:21:45,550
other question

00:22:01,490 --> 00:22:13,810
and I just I organization that sounds

00:22:11,480 --> 00:22:18,340
like a bug

00:22:13,810 --> 00:22:18,340
thank you for pointing to everybody

00:22:23,480 --> 00:22:29,470
it's really quite handsome Jacob played

00:22:25,490 --> 00:22:29,470
like pulling my belly

00:22:31,360 --> 00:22:36,080
Shakespeare check multiple columns or

00:22:33,530 --> 00:22:41,630
suggests clove it is the difference

00:22:36,080 --> 00:22:47,000
between the lower advisor be able to use

00:22:41,630 --> 00:22:49,670
like my extension of the library no dead

00:22:47,000 --> 00:22:52,070
if you can solve the problem of how to

00:22:49,670 --> 00:22:55,300
do it yes it's not an easy problem with

00:22:52,070 --> 00:22:55,300
yeah yeah

00:22:58,420 --> 00:23:04,910
yes definitely I mean the so whatever is

00:23:02,780 --> 00:23:06,860
the service but the library with tables

00:23:04,910 --> 00:23:13,370
is we can write locally and there's a

00:23:06,860 --> 00:23:21,400
common line to grandis checks the

00:23:13,370 --> 00:23:21,400
customs checks yes yes yep exactly

00:23:23,700 --> 00:23:30,720
who else can we lock you thank you very

00:23:28,450 --> 00:23:30,720
much

00:23:33,340 --> 00:23:36,980

YouTube URL: https://www.youtube.com/watch?v=Gk2F4hncAgY


