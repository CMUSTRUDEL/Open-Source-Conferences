Title: Melissa Santos - Time-to-Event Analysis for Non-Medical Applications
Publication date: 2020-05-28
Playlist: CSVConf 2020
Description: 
	How do you estimate the time until an event, especially if the event might never happen? The statistical methods for this come from studying time from disease diagnosis to death, but we can use these methods for much more cheerful data. For example, how long does a subscription customer continue to pay you? How long does it take from someone commenting on your open source code to becoming a contributor? How long does it take from the user being seen the first time to them becoming a paid customer?  Kaplan-Meier survival curves are a non-parametric estimates of the time to an event. They make no assumptions about the distribution of the time to the event, and they handle samples of various ages that may or may not have made it to the event. As well as the theory of these, we'll dive into how to calculate them directly in SQL. To finish, I'll share some ways we've been using Kaplan-Meier curves to make decisions at a Software as a Service company. 

--
csv,conf,v5 is a community conference for data makers everywhere, featuring stories about data sharing and data analysis from science, journalism, government, and open source.  Held May 13-14, 2020, Online. https://csvconf.com/
Captions: 
	00:00:00,480 --> 00:00:04,319
all right so the rest of the the

00:00:03,199 --> 00:00:06,080
talks in this session are about

00:00:04,319 --> 00:00:07,359
healthcare data but this is specifically

00:00:06,080 --> 00:00:09,519
about a healthcare

00:00:07,359 --> 00:00:10,639
analysis tool and how to use it for

00:00:09,519 --> 00:00:12,559
things that are not

00:00:10,639 --> 00:00:13,920
involved with healthcare so hopefully

00:00:12,559 --> 00:00:14,880
this will be interesting and useful to

00:00:13,920 --> 00:00:16,080
some of you

00:00:14,880 --> 00:00:19,359
um it's actually been one of the biggest

00:00:16,080 --> 00:00:20,560
projects i did last year at my work

00:00:19,359 --> 00:00:22,880
we're going to talk a little bit about

00:00:20,560 --> 00:00:23,439
what right sensor data is and then about

00:00:22,880 --> 00:00:25,599
what a

00:00:23,439 --> 00:00:27,519
survival function is and how to estimate

00:00:25,599 --> 00:00:29,199
it i have a couple of real world

00:00:27,519 --> 00:00:30,800
examples i'll show you but they're kind

00:00:29,199 --> 00:00:33,440
of

00:00:30,800 --> 00:00:34,800
boring industry paying money customer

00:00:33,440 --> 00:00:36,239
examples

00:00:34,800 --> 00:00:39,920
and then we'll take a little detour into

00:00:36,239 --> 00:00:39,920
the sql to actually compute these curves

00:00:40,079 --> 00:00:43,440
so right censored data is on a left to

00:00:42,320 --> 00:00:45,600
right timeline

00:00:43,440 --> 00:00:46,800
anytime you have data where at some

00:00:45,600 --> 00:00:48,320
point you stop

00:00:46,800 --> 00:00:49,920
you stop being able to observe it and

00:00:48,320 --> 00:00:51,680
you don't know how far it goes on from

00:00:49,920 --> 00:00:53,760
there

00:00:51,680 --> 00:00:56,160
so if we wanted to for example know the

00:00:53,760 --> 00:00:59,120
average length of the lines in this

00:00:56,160 --> 00:01:01,199
in this figure we would know that it was

00:00:59,120 --> 00:01:02,559
more than what we can see

00:01:01,199 --> 00:01:04,239
because we know some of them go on

00:01:02,559 --> 00:01:09,280
longer than than what is

00:01:04,239 --> 00:01:09,280
visible here but we don't know exactly

00:01:09,520 --> 00:01:13,119
how how much how much further they do go

00:01:11,520 --> 00:01:14,640
and so it's a little hard to answer some

00:01:13,119 --> 00:01:15,520
questions about like how long these

00:01:14,640 --> 00:01:18,640
things go and

00:01:15,520 --> 00:01:20,240
how information like that

00:01:18,640 --> 00:01:21,759
and not all right sensor data is all

00:01:20,240 --> 00:01:22,960
beautifully lined up at the start of

00:01:21,759 --> 00:01:25,200
your experiment

00:01:22,960 --> 00:01:27,280
some of it is is about is from real

00:01:25,200 --> 00:01:29,360
world data and so it's things like

00:01:27,280 --> 00:01:32,000
when did the customer start paying you

00:01:29,360 --> 00:01:35,360
or when did somebody graduate from

00:01:32,000 --> 00:01:37,040
their mlis um and so

00:01:35,360 --> 00:01:39,680
there's all this information about that

00:01:37,040 --> 00:01:40,479
just it's based on how long you've seen

00:01:39,680 --> 00:01:42,479
it for

00:01:40,479 --> 00:01:44,479
and you can be censored at any time

00:01:42,479 --> 00:01:46,240
because they drop out of the study

00:01:44,479 --> 00:01:48,000
are because you hit the centering time

00:01:46,240 --> 00:01:49,600
where there's no longer

00:01:48,000 --> 00:01:50,720
it's now today and so we don't know if

00:01:49,600 --> 00:01:54,399
people have hit the event that we're

00:01:50,720 --> 00:01:56,000
interested in

00:01:54,399 --> 00:01:57,439
so how do you find a midpoint when so

00:01:56,000 --> 00:01:59,600
many points are unknown

00:01:57,439 --> 00:02:00,719
well they're not unknown we do have some

00:01:59,600 --> 00:02:02,960
data about them

00:02:00,719 --> 00:02:03,920
so there is a method we can try and do

00:02:02,960 --> 00:02:06,240
this

00:02:03,920 --> 00:02:08,080
uh but i'm actually also going to try

00:02:06,240 --> 00:02:11,520
and say that maybe this is not the best

00:02:08,080 --> 00:02:12,959
question a lot of the time i get

00:02:11,520 --> 00:02:14,959
questions like how long does it take

00:02:12,959 --> 00:02:16,480
someone to use such and such a feature

00:02:14,959 --> 00:02:18,480
how long does it take someone to send an

00:02:16,480 --> 00:02:21,200
invite and they

00:02:18,480 --> 00:02:22,480
they really want to know what happens in

00:02:21,200 --> 00:02:24,480
the first few days

00:02:22,480 --> 00:02:25,840
are in the first little while of us

00:02:24,480 --> 00:02:28,080
getting something so it's

00:02:25,840 --> 00:02:29,680
just as good or better to answer how

00:02:28,080 --> 00:02:30,560
many people send one during the first

00:02:29,680 --> 00:02:32,800
day

00:02:30,560 --> 00:02:34,720
um how what proportion of people do

00:02:32,800 --> 00:02:38,239
something during the first seven days

00:02:34,720 --> 00:02:40,879
it is just as useful or more useful than

00:02:38,239 --> 00:02:43,760
what's the average time the average time

00:02:40,879 --> 00:02:43,760
could be much longer

00:02:44,720 --> 00:02:49,280
but we can still use survival analysis

00:02:47,440 --> 00:02:51,599
to get that average time and some other

00:02:49,280 --> 00:02:52,640
information about that process

00:02:51,599 --> 00:02:54,720
we're going to look at survival

00:02:52,640 --> 00:02:56,080
functions and there are these downward

00:02:54,720 --> 00:02:58,480
sloping curves

00:02:56,080 --> 00:02:59,920
that have time on the x-axis and the

00:02:58,480 --> 00:03:01,680
proportion that have not yet hit the

00:02:59,920 --> 00:03:04,000
event on the y-axis

00:03:01,680 --> 00:03:05,920
the event is canonically death because

00:03:04,000 --> 00:03:07,440
of survival analysis

00:03:05,920 --> 00:03:09,120
so it's often called the proportion

00:03:07,440 --> 00:03:11,200
surviving and that's why it's called the

00:03:09,120 --> 00:03:12,239
survivor function

00:03:11,200 --> 00:03:13,920
we're going to estimate them with

00:03:12,239 --> 00:03:16,800
kaplan-meier curves and they're a

00:03:13,920 --> 00:03:16,800
non-parametric

00:03:16,840 --> 00:03:21,280
uh estimator

00:03:19,680 --> 00:03:23,680
that um that will approach the true

00:03:21,280 --> 00:03:25,360
survivor function as we add more data

00:03:23,680 --> 00:03:27,120
and it uses all the data available for

00:03:25,360 --> 00:03:30,239
every time point so

00:03:27,120 --> 00:03:32,720
it it treats the data that is like this

00:03:30,239 --> 00:03:34,640
as data that is like this by saying i've

00:03:32,720 --> 00:03:37,920
seen it for the first few days

00:03:34,640 --> 00:03:40,080
so i can count it for day one

00:03:37,920 --> 00:03:42,480
regardless of when day one happened in

00:03:40,080 --> 00:03:42,480
the time

00:03:44,840 --> 00:03:50,080
um then we can actually estimate the

00:03:48,720 --> 00:03:51,360
actual estimator that we have is

00:03:50,080 --> 00:03:53,680
actually pretty simple

00:03:51,360 --> 00:03:55,200
and we it's a stair set function over

00:03:53,680 --> 00:03:58,319
each period of time

00:03:55,200 --> 00:04:00,000
and we take the product of one minus the

00:03:58,319 --> 00:04:01,920
number of people who hit the number of

00:04:00,000 --> 00:04:03,840
observations that saw the event

00:04:01,920 --> 00:04:06,640
at time t over the number of

00:04:03,840 --> 00:04:09,360
observations we know about at time t

00:04:06,640 --> 00:04:10,319
um and as the censoring goes on you're

00:04:09,360 --> 00:04:11,920
going to have more and more

00:04:10,319 --> 00:04:14,080
observations drop out of that bottom

00:04:11,920 --> 00:04:15,599
number you're going to have

00:04:14,080 --> 00:04:17,199
you're just going to not know about

00:04:15,599 --> 00:04:19,759
people for

00:04:17,199 --> 00:04:21,600
i some of my paying customers i know

00:04:19,759 --> 00:04:22,639
about for four years or three weeks or

00:04:21,600 --> 00:04:24,720
whatever

00:04:22,639 --> 00:04:26,240
but as i get as i get further out i

00:04:24,720 --> 00:04:27,680
start looking at four years i have fewer

00:04:26,240 --> 00:04:28,720
and fewer customers that i know about

00:04:27,680 --> 00:04:30,000
for that long

00:04:28,720 --> 00:04:33,199
and so there's that number in the

00:04:30,000 --> 00:04:35,919
denominator gets smaller and smaller

00:04:33,199 --> 00:04:37,360
um and one other wrinkle of the survival

00:04:35,919 --> 00:04:40,560
function itself

00:04:37,360 --> 00:04:42,639
is that it um this estimator of it will

00:04:40,560 --> 00:04:44,800
often not go all the way to zero

00:04:42,639 --> 00:04:46,639
because you would have to have every

00:04:44,800 --> 00:04:47,040
single you would have to have one minus

00:04:46,639 --> 00:04:49,520
one

00:04:47,040 --> 00:04:51,919
in the product and that would mean that

00:04:49,520 --> 00:04:53,759
every single one of the

00:04:51,919 --> 00:04:55,120
the observations that you knew about at

00:04:53,759 --> 00:04:57,600
a certain time

00:04:55,120 --> 00:04:59,199
um saw the event and but we know we have

00:04:57,600 --> 00:05:01,600
censored data so that's not likely to

00:04:59,199 --> 00:05:01,600
happen

00:05:02,639 --> 00:05:06,240
but if you do do that and you have your

00:05:04,720 --> 00:05:08,880
you have your estimator

00:05:06,240 --> 00:05:10,400
it's really easy to find the median and

00:05:08,880 --> 00:05:11,440
it's not the mean that people are

00:05:10,400 --> 00:05:13,199
actually asking for

00:05:11,440 --> 00:05:15,199
um but the median is probably close

00:05:13,199 --> 00:05:17,280
enough for what people really want

00:05:15,199 --> 00:05:19,600
um and so you can just find out what's

00:05:17,280 --> 00:05:21,919
the proportion that survive

00:05:19,600 --> 00:05:23,600
what you you want to find out when 50 of

00:05:21,919 --> 00:05:25,919
the proportion have

00:05:23,600 --> 00:05:27,440
have met the event and so that's just

00:05:25,919 --> 00:05:30,479
something you can find on the curve and

00:05:27,440 --> 00:05:30,479
go down to the timeline

00:05:30,639 --> 00:05:33,759
but that's not all you get you get this

00:05:32,639 --> 00:05:36,400
entire curve

00:05:33,759 --> 00:05:38,080
and so this is a real data set from one

00:05:36,400 --> 00:05:40,080
from my real

00:05:38,080 --> 00:05:42,080
work where we want a time that a

00:05:40,080 --> 00:05:45,120
customer stays a subscribing customer

00:05:42,080 --> 00:05:46,800
and is paying us uh and so i can answer

00:05:45,120 --> 00:05:48,639
questions about like

00:05:46,800 --> 00:05:50,240
um what's the probability that somebody

00:05:48,639 --> 00:05:51,199
will stay for the first seven days after

00:05:50,240 --> 00:05:53,039
they sign up

00:05:51,199 --> 00:05:55,039
or what's the proportion that stayed for

00:05:53,039 --> 00:05:57,600
an entire year

00:05:55,039 --> 00:06:01,199
and this is a lot more rich of a data

00:05:57,600 --> 00:06:01,199
set than just what's the average

00:06:01,360 --> 00:06:05,919
but that's not all you can have you can

00:06:04,400 --> 00:06:07,919
look at different groups of data and

00:06:05,919 --> 00:06:09,680
compare their curves on the same chart

00:06:07,919 --> 00:06:13,039
and get a feel for which ones are more

00:06:09,680 --> 00:06:15,680
likely to hit that event sooner or later

00:06:13,039 --> 00:06:17,120
um this is our favorite example because

00:06:15,680 --> 00:06:19,759
it gave us some really good business

00:06:17,120 --> 00:06:22,479
objectives to change things around

00:06:19,759 --> 00:06:22,479
when we

00:06:22,800 --> 00:06:28,400
when we bill people annually they're

00:06:25,840 --> 00:06:30,080
more likely to drop out in the first 30

00:06:28,400 --> 00:06:32,080
days

00:06:30,080 --> 00:06:33,680
than when we build them monthly but

00:06:32,080 --> 00:06:35,039
after the first 30 days

00:06:33,680 --> 00:06:36,800
they're more likely to stick around for

00:06:35,039 --> 00:06:38,319
one year than they are

00:06:36,800 --> 00:06:40,400
than monthly customers are to stick

00:06:38,319 --> 00:06:42,160
around for just two months so we really

00:06:40,400 --> 00:06:43,199
want people to stick around and keep

00:06:42,160 --> 00:06:44,720
paying us

00:06:43,199 --> 00:06:46,800
so this incentivizes us to give

00:06:44,720 --> 00:06:51,360
discounts for annual subscriptions

00:06:46,800 --> 00:06:51,360
and try and sell them more for people

00:06:54,080 --> 00:06:57,120
i'm going to talk a little bit about how

00:06:55,599 --> 00:06:59,280
to do this in sql

00:06:57,120 --> 00:07:00,639
and you probably don't need to do this

00:06:59,280 --> 00:07:03,599
in sql for what you're doing

00:07:00,639 --> 00:07:05,440
you might it's cool but you can

00:07:03,599 --> 00:07:07,280
definitely do this in r or python and

00:07:05,440 --> 00:07:09,919
then it's a lot simpler

00:07:07,280 --> 00:07:11,520
it's a little more straightforward but i

00:07:09,919 --> 00:07:14,960
work in dashboard land

00:07:11,520 --> 00:07:17,039
and i need to show this data

00:07:14,960 --> 00:07:18,639
update it periodically with all the real

00:07:17,039 --> 00:07:20,800
data behind it and i don't want to have

00:07:18,639 --> 00:07:22,960
to do some kind of crazy job that runs

00:07:20,800 --> 00:07:26,319
python every night and uploads a file

00:07:22,960 --> 00:07:28,720
with the image in it every time so i

00:07:26,319 --> 00:07:30,240
just have i just do it in sql and i'm

00:07:28,720 --> 00:07:34,160
able to actually get that data

00:07:30,240 --> 00:07:36,319
right away in the in the data for

00:07:34,160 --> 00:07:37,520
in the in the dashboard for people to

00:07:36,319 --> 00:07:38,639
see it

00:07:37,520 --> 00:07:41,120
so there's a couple of tricks i'm going

00:07:38,639 --> 00:07:42,400
to play i'm going to generate a list of

00:07:41,120 --> 00:07:44,000
numbers

00:07:42,400 --> 00:07:45,520
i'm going to sum the results of a case

00:07:44,000 --> 00:07:48,879
statement

00:07:45,520 --> 00:07:50,319
and then my most mathematical trick here

00:07:48,879 --> 00:07:51,919
believe me on this one we might need to

00:07:50,319 --> 00:07:54,000
look it up but

00:07:51,919 --> 00:07:55,759
this the log of a product is equal to

00:07:54,000 --> 00:07:58,560
the sum of the logs

00:07:55,759 --> 00:07:59,199
so we can rewrite that product function

00:07:58,560 --> 00:08:01,199
so that

00:07:59,199 --> 00:08:02,720
we take the log of it we change that to

00:08:01,199 --> 00:08:04,400
the sum of the logs then we take the

00:08:02,720 --> 00:08:06,000
exponential of the whole thing

00:08:04,400 --> 00:08:08,319
to get it back out into the the right

00:08:06,000 --> 00:08:10,800
frame um and that

00:08:08,319 --> 00:08:12,960
gives us that lets us use tools that are

00:08:10,800 --> 00:08:13,520
all built into sql the sql does not know

00:08:12,960 --> 00:08:15,360
this

00:08:13,520 --> 00:08:17,599
notion of taking a product over many

00:08:15,360 --> 00:08:20,479
rows it's pretty cool with taking a sum

00:08:17,599 --> 00:08:20,479
over many rows

00:08:21,199 --> 00:08:24,319
so all you have to do all you have to

00:08:22,800 --> 00:08:26,800
have in your main table

00:08:24,319 --> 00:08:28,400
is the event time and the time scene and

00:08:26,800 --> 00:08:30,479
the event time is going to be null for

00:08:28,400 --> 00:08:33,279
all those censored observations

00:08:30,479 --> 00:08:34,640
um it's going to you're going to have um

00:08:33,279 --> 00:08:35,760
a lot of those censored observations

00:08:34,640 --> 00:08:37,120
where you just don't

00:08:35,760 --> 00:08:38,800
they don't ever hit the event you they

00:08:37,120 --> 00:08:40,399
might hit eventually later but you don't

00:08:38,800 --> 00:08:42,080
have that data yet

00:08:40,399 --> 00:08:44,640
but you always know how long you've seen

00:08:42,080 --> 00:08:46,480
them for whether that's one week or four

00:08:44,640 --> 00:08:47,920
years or whatever it is

00:08:46,480 --> 00:08:51,440
and so you just get that into the same

00:08:47,920 --> 00:08:54,399
frame and put it all together

00:08:51,440 --> 00:08:55,120
and that is like a framework for our

00:08:54,399 --> 00:08:56,399
query

00:08:55,120 --> 00:08:58,800
for our next part we're going to

00:08:56,399 --> 00:09:00,959
generate a list of the the time periods

00:08:58,800 --> 00:09:02,080
that we want to go over

00:09:00,959 --> 00:09:04,720
and we're going to do this with row

00:09:02,080 --> 00:09:07,279
number so we're going to some create

00:09:04,720 --> 00:09:09,440
this table called day shift by selecting

00:09:07,279 --> 00:09:13,279
the row number

00:09:09,440 --> 00:09:15,200
of over over anything

00:09:13,279 --> 00:09:17,279
from a big table and limiting to three

00:09:15,200 --> 00:09:19,040
thousand

00:09:17,279 --> 00:09:20,480
so i only want three thousand days so

00:09:19,040 --> 00:09:22,480
this is going to give me a list of one

00:09:20,480 --> 00:09:24,080
through three thousand and i can play

00:09:22,480 --> 00:09:26,560
with that and you just use it as a set

00:09:24,080 --> 00:09:26,560
of numbers

00:09:26,959 --> 00:09:30,399
i apologize for there being a slide that

00:09:28,800 --> 00:09:32,080
is 100 sql

00:09:30,399 --> 00:09:34,480
but i promise you this is just

00:09:32,080 --> 00:09:36,160
calculating one minus di over ni for

00:09:34,480 --> 00:09:38,560
each of those time periods

00:09:36,160 --> 00:09:40,320
but let's look at it so for each of

00:09:38,560 --> 00:09:41,680
those time periods so we're from day

00:09:40,320 --> 00:09:43,040
shift our list of one through three

00:09:41,680 --> 00:09:47,360
thousand

00:09:43,040 --> 00:09:50,480
um i get the day number i get one minus

00:09:47,360 --> 00:09:53,120
um i take the sum of the time

00:09:50,480 --> 00:09:54,560
when they remember this is the the the

00:09:53,120 --> 00:09:56,080
numerator is the people who've actually

00:09:54,560 --> 00:09:57,600
seen the event at that time

00:09:56,080 --> 00:09:59,760
so we've seen it we've seen them for

00:09:57,600 --> 00:10:02,399
that amount of time and the event time

00:09:59,760 --> 00:10:02,399
is not null

00:10:02,640 --> 00:10:06,079
and then the denominator is how many we

00:10:04,480 --> 00:10:08,240
joined from our

00:10:06,079 --> 00:10:09,680
our table that has our data in it and so

00:10:08,240 --> 00:10:11,040
that's all the ones that we've seen

00:10:09,680 --> 00:10:18,720
where the time scene is greater than or

00:10:11,040 --> 00:10:20,800
equal to this time number

00:10:18,720 --> 00:10:21,839
but so now i've got a table that's just

00:10:20,800 --> 00:10:24,880
00:10:21,839 --> 00:10:26,720
comma 1 minus d i over ni and all i want

00:10:24,880 --> 00:10:27,600
to pull it together and get that product

00:10:26,720 --> 00:10:30,959
function

00:10:27,600 --> 00:10:33,200
so i get to use a summation as a window

00:10:30,959 --> 00:10:36,320
function

00:10:33,200 --> 00:10:37,360
so um so i take the exponential of the

00:10:36,320 --> 00:10:40,480
sum of the log

00:10:37,360 --> 00:10:42,800
of the inside the one minus d i over ni

00:10:40,480 --> 00:10:44,959
um and i take it over unbounded

00:10:42,800 --> 00:10:47,519
proceeding in current row

00:10:44,959 --> 00:10:50,000
so that takes all of the previous rows

00:10:47,519 --> 00:10:51,839
and take then adds them up

00:10:50,000 --> 00:10:53,279
for up to the current row so that for

00:10:51,839 --> 00:10:56,320
every ti you have t

00:10:53,279 --> 00:10:58,720
at t1 by itself and then t1 minus t2 and

00:10:56,320 --> 00:11:01,600
then t1 minus t2 minus t3

00:10:58,720 --> 00:11:03,519
and so on and then this gives you a

00:11:01,600 --> 00:11:05,839
table

00:11:03,519 --> 00:11:07,519
that um just has the date number and

00:11:05,839 --> 00:11:10,000
then the proportion that survived for

00:11:07,519 --> 00:11:10,000
each time

00:11:10,480 --> 00:11:14,079
but i told you you could have more than

00:11:11,839 --> 00:11:15,680
one curve and you can have them in your

00:11:14,079 --> 00:11:17,440
sql without having to compute each of

00:11:15,680 --> 00:11:19,920
these separately

00:11:17,440 --> 00:11:21,360
you just need some categorical variable

00:11:19,920 --> 00:11:23,279
and to include that in your original

00:11:21,360 --> 00:11:26,320
data set

00:11:23,279 --> 00:11:30,160
and then when you create calculate

00:11:26,320 --> 00:11:32,079
the di over ni for each of the the days

00:11:30,160 --> 00:11:33,600
you also ca you also group by your

00:11:32,079 --> 00:11:35,360
categorical variable

00:11:33,600 --> 00:11:37,279
so i would have a dana comma billing

00:11:35,360 --> 00:11:40,399
period here and i would group by dnm

00:11:37,279 --> 00:11:40,399
common ability period

00:11:40,959 --> 00:11:45,440
and then when i get to the end i just

00:11:43,040 --> 00:11:47,279
tell it to partition by billing period

00:11:45,440 --> 00:11:48,800
so that it is so it calculates these

00:11:47,279 --> 00:11:49,360
separately for the two different billing

00:11:48,800 --> 00:11:52,320
periods

00:11:49,360 --> 00:11:53,839
or however many cases that you have in

00:11:52,320 --> 00:11:55,279
your thing

00:11:53,839 --> 00:11:56,959
um and then you still have your

00:11:55,279 --> 00:11:58,399
unbounded proceeding and current road to

00:11:56,959 --> 00:12:01,680
get your multiplication over

00:11:58,399 --> 00:12:03,040
all the past values of ti um and then

00:12:01,680 --> 00:12:05,120
you'll get something like this where you

00:12:03,040 --> 00:12:08,320
have the day number

00:12:05,120 --> 00:12:09,760
and then also the the different values

00:12:08,320 --> 00:12:12,880
for the billing period or whatever your

00:12:09,760 --> 00:12:12,880
categorical variable is

00:12:14,639 --> 00:12:18,000
so kind of an analysis is pretty cool

00:12:17,200 --> 00:12:20,480
you can use it

00:12:18,000 --> 00:12:21,680
it can help you out um you can find the

00:12:20,480 --> 00:12:24,240
average time to an event

00:12:21,680 --> 00:12:25,760
but that's not all you can actually

00:12:24,240 --> 00:12:27,279
understand the process and get some

00:12:25,760 --> 00:12:29,120
difference get some data about

00:12:27,279 --> 00:12:30,399
like how long different proportions of

00:12:29,120 --> 00:12:32,480
time last

00:12:30,399 --> 00:12:34,399
and perhaps most excitingly you can

00:12:32,480 --> 00:12:36,399
compare different groups

00:12:34,399 --> 00:12:37,519
and make business decisions based on

00:12:36,399 --> 00:12:40,000
which of them are more likely to hit

00:12:37,519 --> 00:12:43,920
those events and how frequently

00:12:40,000 --> 00:12:45,519
so thanks for listening to me

00:12:43,920 --> 00:12:47,839
um you can find me on the survival

00:12:45,519 --> 00:12:49,440
analysis channel on the csv conf slack

00:12:47,839 --> 00:12:51,200
um all right so i'll still be in the q a

00:12:49,440 --> 00:12:53,279
channel like everybody else

00:12:51,200 --> 00:12:56,639
i'm ann stayed on twitter and i'm nancy

00:12:53,279 --> 00:12:56,639
weirder earth on mastodon

00:12:58,160 --> 00:13:01,760
awesome thank you so much melissa

00:13:02,480 --> 00:13:09,040
my cat is determined to be part of this

00:13:06,399 --> 00:13:09,040
hello kitty

00:13:11,200 --> 00:13:14,639
it's fine okay

00:13:15,040 --> 00:13:18,639
uh we have a question if you don't mind

00:13:16,639 --> 00:13:21,360
answering if that's okay

00:13:18,639 --> 00:13:22,399
how much slash what kind of dialogue did

00:13:21,360 --> 00:13:24,560
it take to switch

00:13:22,399 --> 00:13:25,760
to this method versus using average

00:13:24,560 --> 00:13:29,040
slash rates

00:13:25,760 --> 00:13:29,680
wow what a good question um it was

00:13:29,040 --> 00:13:32,560
actually

00:13:29,680 --> 00:13:34,160
an idea proposed by my ceo so that meant

00:13:32,560 --> 00:13:37,360
we had plenty of buy-in

00:13:34,160 --> 00:13:40,000
right away um

00:13:37,360 --> 00:13:41,120
uh they had heard about survival

00:13:40,000 --> 00:13:43,600
functions and and

00:13:41,120 --> 00:13:45,279
knew that they could have something that

00:13:43,600 --> 00:13:46,639
could give them that comparison over

00:13:45,279 --> 00:13:49,040
groups and that was really what they

00:13:46,639 --> 00:13:50,959
were going for and but

00:13:49,040 --> 00:13:52,639
um but they still ask me questions in

00:13:50,959 --> 00:13:53,360
that midpoint way and i just don't

00:13:52,639 --> 00:13:55,600
answer them

00:13:53,360 --> 00:14:02,000
i just answer them there for seven days

00:13:55,600 --> 00:14:04,160
or the first day or something

00:14:02,000 --> 00:14:06,079
cool we still have a bit of time so if

00:14:04,160 --> 00:14:06,399
anyone has any other questions uh i'm

00:14:06,079 --> 00:14:08,639
sure

00:14:06,399 --> 00:14:10,560
melissa will gladly try to answer it as

00:14:08,639 --> 00:14:14,839
much as she can

00:14:10,560 --> 00:14:17,839
oh she totally will answer it i'll do my

00:14:14,839 --> 00:14:17,839
best

00:14:25,360 --> 00:14:31,839
let me just trim my camera back on okay

00:14:29,040 --> 00:14:31,839
i'm not alone

00:14:32,160 --> 00:14:36,320
if everyone's uh happy i'm melissa will

00:14:34,880 --> 00:14:38,000
be around she does have a slack channel

00:14:36,320 --> 00:14:40,399
on csv conf which

00:14:38,000 --> 00:14:43,199
hopefully she'll visit after the

00:14:40,399 --> 00:14:43,199
conference as well

00:14:43,519 --> 00:14:49,519
and oh that's a good question any

00:14:45,279 --> 00:14:49,519
suggestions for background reading

00:14:50,000 --> 00:14:54,399
the slides of the sequel are on the noto

00:14:52,160 --> 00:14:56,399
um the the book i've been using

00:14:54,399 --> 00:15:00,320
i hope this is not flipped for you is

00:14:56,399 --> 00:15:02,720
survival analysis by climate and klein

00:15:00,320 --> 00:15:04,079
um and it this is just chapter one

00:15:02,720 --> 00:15:06,399
there's so

00:15:04,079 --> 00:15:07,839
much more there's all these statistical

00:15:06,399 --> 00:15:09,680
tests to tell you whether those

00:15:07,839 --> 00:15:11,360
curves are different or ways to put

00:15:09,680 --> 00:15:12,079
covariates in there and add them into

00:15:11,360 --> 00:15:13,680
the thing

00:15:12,079 --> 00:15:15,839
like there's there's so much more to

00:15:13,680 --> 00:15:17,760
survival analysis this is just like the

00:15:15,839 --> 00:15:20,480
very tiniest bit you could do to

00:15:17,760 --> 00:15:20,480
do something useful

00:15:21,519 --> 00:15:26,880
great wow all these great questions us

00:15:24,959 --> 00:15:30,480
someone has asked can you tell us a

00:15:26,880 --> 00:15:33,680
little more about your own background

00:15:30,480 --> 00:15:35,360
um i i'm a data analyst

00:15:33,680 --> 00:15:37,680
data engineer i've been doing this for a

00:15:35,360 --> 00:15:39,600
long time i work at a startup and i'm

00:15:37,680 --> 00:15:41,680
just the one data person there

00:15:39,600 --> 00:15:44,160
so i do all the product analytics and

00:15:41,680 --> 00:15:46,000
financial analytics and things like that

00:15:44,160 --> 00:15:47,839
but i'm very interested in government

00:15:46,000 --> 00:15:49,279
and open data and things like that and

00:15:47,839 --> 00:15:51,839
so i'm always really happy to be here at

00:15:49,279 --> 00:15:54,399
csv com

00:15:51,839 --> 00:15:56,240
amazing and another question is how

00:15:54,399 --> 00:15:57,920
sorry have you used survival analysis

00:15:56,240 --> 00:16:00,480
for forecasting

00:15:57,920 --> 00:16:02,959
i have not i think that that would be

00:16:00,480 --> 00:16:02,959
interesting

00:16:04,079 --> 00:16:08,800
and this has me thinking of looking at

00:16:06,399 --> 00:16:09,680
outcomes for program evaluations any

00:16:08,800 --> 00:16:12,639
warnings for people

00:16:09,680 --> 00:16:13,680
trying to apply it i think you should

00:16:12,639 --> 00:16:17,120
totally try it

00:16:13,680 --> 00:16:18,399
um and that would be an easy thing to

00:16:17,120 --> 00:16:21,440
have separate curves for

00:16:18,399 --> 00:16:22,959
so different programs and what are the

00:16:21,440 --> 00:16:25,199
the outcomes and when did people reach

00:16:22,959 --> 00:16:26,079
them and do people reach them from

00:16:25,199 --> 00:16:28,880
certain programs

00:16:26,079 --> 00:16:28,880
faster than others

00:16:30,480 --> 00:16:34,000
cool well then if there are no other

00:16:32,399 --> 00:16:35,440
questions which i feel like

00:16:34,000 --> 00:16:38,160
when i said does anyone have questions

00:16:35,440 --> 00:16:38,160
everyone came out

00:16:39,120 --> 00:16:43,120
but uh but a big thank you from all of

00:16:41,199 --> 00:16:45,040
us at csv cough because mel you've been

00:16:43,120 --> 00:16:48,240
one of our biggest supporters for

00:16:45,040 --> 00:16:50,720
many years now so thank you you keep

00:16:48,240 --> 00:16:53,120
accepting my talks i keep coming

00:16:50,720 --> 00:16:54,240
and they're always so much fun so thanks

00:16:53,120 --> 00:16:58,000
so much

00:16:54,240 --> 00:16:58,000

YouTube URL: https://www.youtube.com/watch?v=SOy8lH2yQzo


