Title: Tempest van Schaik, PhD - Building successful collaborations around healthcare data
Publication date: 2020-05-28
Playlist: CSVConf 2020
Description: 
	Besides a few high-profile machine learning for health breakthroughs, what is the state of real-word data science for health, and why do so few algorithms make it into production? We’ll explore the rich variety of health data that exists, how to avoid common pitfalls with it, ways to make the most positive impact with health data, and the culture of different stakeholders who work with data. We’ll consider this topic with reference to Project Fizzyo, which aims to improve the lives of children with cystic fibrosis, using data from custom respiratory devices which are used during physiotherapy.

--
csv,conf,v5 is a community conference for data makers everywhere, featuring stories about data sharing and data analysis from science, journalism, government, and open source.  Held May 13-14, 2020, Online. https://csvconf.com/
Captions: 
	00:00:00,000 --> 00:00:06,810
but building successful collaborations

00:00:02,610 --> 00:00:09,269
with health data okay thank you

00:00:06,810 --> 00:00:12,830
so my name is Tempest Van Schaick I am a

00:00:09,269 --> 00:00:16,609
machine learning engineer at Microsoft

00:00:12,830 --> 00:00:20,189
working with with healthcare data

00:00:16,609 --> 00:00:22,140
just the usual disclaimer I'm going to

00:00:20,189 --> 00:00:25,350
be talking about my personal views not

00:00:22,140 --> 00:00:27,240
necessarily the views of my employer so

00:00:25,350 --> 00:00:29,010
I'm going to talk about building

00:00:27,240 --> 00:00:31,949
successful collaborations with health

00:00:29,010 --> 00:00:35,250
data because I have witnessed an

00:00:31,949 --> 00:00:37,469
increasing growing interest in working

00:00:35,250 --> 00:00:39,140
with health data and building machine

00:00:37,469 --> 00:00:43,350
learning projects with healthcare data

00:00:39,140 --> 00:00:46,649
so I'm hoping to give some some kind of

00:00:43,350 --> 00:00:49,440
advice that insights that you can use in

00:00:46,649 --> 00:00:54,230
your projects if anyone is interested in

00:00:49,440 --> 00:00:56,430
health data here so we'll take a look at

00:00:54,230 --> 00:00:58,969
first understanding what makes

00:00:56,430 --> 00:01:02,280
healthcare data quite so tricky and

00:00:58,969 --> 00:01:06,119
unique and interesting then we'll look

00:01:02,280 --> 00:01:11,150
at this important point of data people

00:01:06,119 --> 00:01:13,320
like us relying on medical experts and

00:01:11,150 --> 00:01:15,390
largely we'll look at some of the

00:01:13,320 --> 00:01:18,390
cultural differences between sort of

00:01:15,390 --> 00:01:20,610
data take people and and healthcare

00:01:18,390 --> 00:01:23,580
people and how we can better understand

00:01:20,610 --> 00:01:27,630
those differences and and play nicely

00:01:23,580 --> 00:01:29,610
together in our collaborations so first

00:01:27,630 --> 00:01:34,829
of all let's take a look at what makes

00:01:29,610 --> 00:01:37,310
health data quite so tricky so the most

00:01:34,829 --> 00:01:41,790
obvious thing that comes to mind is the

00:01:37,310 --> 00:01:43,920
sensitivity of health care data and when

00:01:41,790 --> 00:01:47,640
we're working with a set of security is

00:01:43,920 --> 00:01:51,090
really important as we saw in the talk

00:01:47,640 --> 00:01:53,880
prior prior prior to this one it's

00:01:51,090 --> 00:01:58,320
really important to work securely with

00:01:53,880 --> 00:02:01,590
that data maintain the privacy of people

00:01:58,320 --> 00:02:03,770
in your data set and often healthcare

00:02:01,590 --> 00:02:07,619
data can be tied to consent

00:02:03,770 --> 00:02:09,629
so I've experienced working with genomic

00:02:07,619 --> 00:02:11,640
data where someone had consented to have

00:02:09,629 --> 00:02:12,870
their data used for cardiovascular

00:02:11,640 --> 00:02:15,269
research

00:02:12,870 --> 00:02:18,060
and we couldn't use their data for

00:02:15,269 --> 00:02:20,640
cancer research because they consented

00:02:18,060 --> 00:02:22,769
specifically outlined only one type of

00:02:20,640 --> 00:02:27,180
research so that's another interesting

00:02:22,769 --> 00:02:28,739
aspect obviously the consequences of the

00:02:27,180 --> 00:02:30,090
decisions we make with healthcare dates

00:02:28,739 --> 00:02:32,430
are extremely serious

00:02:30,090 --> 00:02:35,459
so if we use healthcare data to train a

00:02:32,430 --> 00:02:37,620
machine learning model to determine how

00:02:35,459 --> 00:02:42,030
long somebody should wait until they get

00:02:37,620 --> 00:02:44,099
their treatment or how much how much we

00:02:42,030 --> 00:02:46,079
should spend on somebody's treatment or

00:02:44,099 --> 00:02:49,910
which treatment they should get those

00:02:46,079 --> 00:02:53,250
are obviously very serious decisions

00:02:49,910 --> 00:02:55,260
data is not the new oil there was this

00:02:53,250 --> 00:02:56,640
saying a couple of years ago that it is

00:02:55,260 --> 00:02:59,609
the new oil and I think it's very

00:02:56,640 --> 00:03:03,120
regrettable health data in particular is

00:02:59,609 --> 00:03:05,819
very very precious and we're extremely

00:03:03,120 --> 00:03:07,849
privileged if we are if we get the

00:03:05,819 --> 00:03:12,540
opportunity to work with healthcare data

00:03:07,849 --> 00:03:14,819
and I've noticed this very um I'm

00:03:12,540 --> 00:03:17,130
acutely when working with clinical trial

00:03:14,819 --> 00:03:19,650
data where you feel very grateful for

00:03:17,130 --> 00:03:22,200
each participant in your trial and

00:03:19,650 --> 00:03:28,500
folder and you really treat that data as

00:03:22,200 --> 00:03:31,079
a precious precious gift I suppose and

00:03:28,500 --> 00:03:33,209
in fact for the average clinical trial

00:03:31,079 --> 00:03:35,849
phase one registered on clinical trials

00:03:33,209 --> 00:03:38,030
that God that includes around 75

00:03:35,849 --> 00:03:40,919
patients where and each one costs

00:03:38,030 --> 00:03:44,699
$15,000 to recruit and to look after on

00:03:40,919 --> 00:03:47,940
the trial continued more things that

00:03:44,699 --> 00:03:51,269
make data tricky it exists in a non

00:03:47,940 --> 00:03:53,430
digital form very often and so we can't

00:03:51,269 --> 00:03:57,060
do exciting machine learning unless it

00:03:53,430 --> 00:03:59,879
has been digitized and that goes on to

00:03:57,060 --> 00:04:02,090
the quality of medical data and when it

00:03:59,879 --> 00:04:06,889
is digitized it's often missing or

00:04:02,090 --> 00:04:09,540
poorly captured but if we think about

00:04:06,889 --> 00:04:10,319
why this is the case we may be a bit

00:04:09,540 --> 00:04:12,780
more sympathetic

00:04:10,319 --> 00:04:15,540
so as data scientist is frustrating to

00:04:12,780 --> 00:04:19,229
have missing values but imagine a nurse

00:04:15,540 --> 00:04:22,710
and a busy er their their priority is

00:04:19,229 --> 00:04:25,070
not entering data correctly it's to save

00:04:22,710 --> 00:04:25,070
lives

00:04:25,199 --> 00:04:29,939
now labeling health data for the points

00:04:27,810 --> 00:04:33,539
purpose of training is particularly

00:04:29,939 --> 00:04:36,270
difficult when we have images of it's

00:04:33,539 --> 00:04:38,159
easy to label them as cat or dog but

00:04:36,270 --> 00:04:41,939
when we have a panel of healthcare data

00:04:38,159 --> 00:04:44,280
it's very difficult to define what makes

00:04:41,939 --> 00:04:47,729
someone healthy and someone not healthy

00:04:44,280 --> 00:04:49,439
so that labeling is often one of the

00:04:47,729 --> 00:04:53,879
most difficult parts of a health it's a

00:04:49,439 --> 00:04:55,530
project and one of the only health

00:04:53,879 --> 00:04:59,210
outcomes that everyone can agree on is

00:04:55,530 --> 00:05:02,550
whether someone is alive or not

00:04:59,210 --> 00:05:04,650
EXR requires a huge amount of expertise

00:05:02,550 --> 00:05:07,139
health data requires a huge amount of

00:05:04,650 --> 00:05:08,810
expertise to understand and we can't

00:05:07,139 --> 00:05:13,979
just kind of walk into it and get going

00:05:08,810 --> 00:05:16,139
and so here I'm showing that there are a

00:05:13,979 --> 00:05:18,349
huge amount of published machine

00:05:16,139 --> 00:05:21,090
learning algorithms in existence and

00:05:18,349 --> 00:05:23,520
there's a smaller set of those that are

00:05:21,090 --> 00:05:26,520
actually useful algorithms that solve a

00:05:23,520 --> 00:05:28,889
useful problem and then an even smaller

00:05:26,520 --> 00:05:32,550
set of those are algorithms that make it

00:05:28,889 --> 00:05:34,650
into medical clinical practice because

00:05:32,550 --> 00:05:39,569
there's a lot of barriers along the way

00:05:34,650 --> 00:05:43,050
a lot of regulation for example and then

00:05:39,569 --> 00:05:44,879
even smaller set is algorithms that are

00:05:43,050 --> 00:05:47,339
available in clinical practice and are

00:05:44,879 --> 00:05:49,169
actually used so the last stumbling

00:05:47,339 --> 00:05:52,169
block can often be usability and user

00:05:49,169 --> 00:05:55,529
experience I've had an experience where

00:05:52,169 --> 00:05:58,289
I was in a surgery wearing scrubs and I

00:05:55,529 --> 00:06:01,650
handed a device medical device to a

00:05:58,289 --> 00:06:04,250
surgeon and they kind of tried it out

00:06:01,650 --> 00:06:07,199
didn't like it gave it back to me

00:06:04,250 --> 00:06:09,509
because it wasted a couple of seconds

00:06:07,199 --> 00:06:12,810
and they just don't have seconds to

00:06:09,509 --> 00:06:15,120
waste and when you're designing apps for

00:06:12,810 --> 00:06:17,129
permissions if they're used to clicking

00:06:15,120 --> 00:06:19,439
once and you make them click twice they

00:06:17,129 --> 00:06:23,460
may not actually use it because their

00:06:19,439 --> 00:06:26,550
time is very very precious so we've had

00:06:23,460 --> 00:06:30,689
a look at what makes medical data and

00:06:26,550 --> 00:06:32,430
medical algorithm so tricky so now I

00:06:30,689 --> 00:06:35,430
want to talk about how important it is

00:06:32,430 --> 00:06:38,020
for data people I guess to rely on

00:06:35,430 --> 00:06:41,150
medical experts

00:06:38,020 --> 00:06:43,460
so I would say that we should be aware

00:06:41,150 --> 00:06:45,950
of health dates of publications that

00:06:43,460 --> 00:06:48,080
have no health experts on them and I've

00:06:45,950 --> 00:06:50,540
seen this even in in journals like

00:06:48,080 --> 00:06:53,390
Nature where computer scientists will

00:06:50,540 --> 00:06:55,550
get hold of some real medical data do an

00:06:53,390 --> 00:06:58,070
analysis and publish their conclusions

00:06:55,550 --> 00:07:02,180
without ever consulting with relevant

00:06:58,070 --> 00:07:04,280
healthcare experts and and some of the

00:07:02,180 --> 00:07:07,700
healthcare experts in that I've spoken

00:07:04,280 --> 00:07:09,200
to have been really really shocked and

00:07:07,700 --> 00:07:13,010
disappointed because the results are not

00:07:09,200 --> 00:07:14,390
right the computer science sisters just

00:07:13,010 --> 00:07:16,520
did not have the context and it's

00:07:14,390 --> 00:07:20,000
embarrassing so let's all avoid doing

00:07:16,520 --> 00:07:22,750
that however it doesn't mean that you

00:07:20,000 --> 00:07:27,560
should just get a doctor on your

00:07:22,750 --> 00:07:30,110
publication for the sake of it so this

00:07:27,560 --> 00:07:32,330
is a kind of a common question that gets

00:07:30,110 --> 00:07:36,520
supposed to be by people who are

00:07:32,330 --> 00:07:38,900
starting their healthcare dated project

00:07:36,520 --> 00:07:40,760
so they said how do you recommend

00:07:38,900 --> 00:07:42,860
collaborating with doctors when the time

00:07:40,760 --> 00:07:44,840
comes to write the actual paper I made a

00:07:42,860 --> 00:07:46,190
network that it takes tumors but I'm not

00:07:44,840 --> 00:07:48,830
sure how to split the paper with the

00:07:46,190 --> 00:07:50,360
doctor okay so this is this is the wrong

00:07:48,830 --> 00:07:52,220
approach you don't just include the

00:07:50,360 --> 00:07:54,680
expert at the end it's really important

00:07:52,220 --> 00:08:00,140
to have that domain expertise at the

00:07:54,680 --> 00:08:01,820
beginning and another thing I would say

00:08:00,140 --> 00:08:06,070
is to make sure you're solving a real

00:08:01,820 --> 00:08:09,380
problem and if you had collaborated with

00:08:06,070 --> 00:08:10,160
a medical expert from the beginning you

00:08:09,380 --> 00:08:12,530
could have checked

00:08:10,160 --> 00:08:14,900
do they need a network that detects

00:08:12,530 --> 00:08:16,940
tumors is that or is that a real problem

00:08:14,900 --> 00:08:22,340
or one that you kind of just assumed or

00:08:16,940 --> 00:08:25,010
made up so it's important for us I stay

00:08:22,340 --> 00:08:27,170
two people to collaborate on a

00:08:25,010 --> 00:08:31,520
multidisciplinary team it's not just us

00:08:27,170 --> 00:08:33,229
data people there's also so there's data

00:08:31,520 --> 00:08:34,880
scientists there's software engineers

00:08:33,229 --> 00:08:35,390
that actually puts models into

00:08:34,880 --> 00:08:37,310
production

00:08:35,390 --> 00:08:39,289
maybe user experience designers and

00:08:37,310 --> 00:08:41,750
product developers and these are the

00:08:39,289 --> 00:08:44,360
kind of tech people on the lift then we

00:08:41,750 --> 00:08:46,460
have clinical researchers they might be

00:08:44,360 --> 00:08:48,110
in a university or maybe in an R&D

00:08:46,460 --> 00:08:50,510
department of a pharmaceutical company

00:08:48,110 --> 00:08:53,120
and people who play a date

00:08:50,510 --> 00:08:55,490
government's role may be regulators like

00:08:53,120 --> 00:08:57,530
FDA or the institutional review board of

00:08:55,490 --> 00:08:59,150
the University and then on the other

00:08:57,530 --> 00:09:00,920
side of the wall the healthcare

00:08:59,150 --> 00:09:03,260
professionals and patients who are all

00:09:00,920 --> 00:09:05,120
stakeholders and I've drawn them on the

00:09:03,260 --> 00:09:08,660
other side of the wall because often as

00:09:05,120 --> 00:09:11,720
tech people don't have direct contact

00:09:08,660 --> 00:09:13,220
with patients or even doctors we often

00:09:11,720 --> 00:09:16,150
go through some sort of clinical

00:09:13,220 --> 00:09:19,010
researcher I'm going to just highlight

00:09:16,150 --> 00:09:21,650
these these stakeholders so I'm going to

00:09:19,010 --> 00:09:26,110
lump them as sort of take people and the

00:09:21,650 --> 00:09:28,070
medical experts so when entering

00:09:26,110 --> 00:09:29,650
collaboration it's important that data

00:09:28,070 --> 00:09:33,050
scientists and software engineers are

00:09:29,650 --> 00:09:37,280
humble and respectful of domain

00:09:33,050 --> 00:09:38,830
expertise and this has been it's been

00:09:37,280 --> 00:09:42,110
quite interesting during this

00:09:38,830 --> 00:09:44,000
coronavirus pandemic to see that

00:09:42,110 --> 00:09:46,130
suddenly everyone is an epidemiologist

00:09:44,000 --> 00:09:48,320
so software engineers and data

00:09:46,130 --> 00:09:50,420
scientists get all some code runs and

00:09:48,320 --> 00:09:53,540
models and have strong opinions about

00:09:50,420 --> 00:09:55,670
epidemiology there sometimes go against

00:09:53,540 --> 00:10:01,280
the experts that have been in studying

00:09:55,670 --> 00:10:03,740
this field for four decades and I it's

00:10:01,280 --> 00:10:05,840
important for for for tech people not to

00:10:03,740 --> 00:10:08,030
judge a fish by its ability to climb a

00:10:05,840 --> 00:10:11,120
tree so when we enter into a

00:10:08,030 --> 00:10:13,190
collaboration if we judge and our

00:10:11,120 --> 00:10:17,750
collaborators the medical experts on

00:10:13,190 --> 00:10:19,310
their use of technology and fail to see

00:10:17,750 --> 00:10:22,550
the expertise that they bring to the

00:10:19,310 --> 00:10:23,290
collaboration that is a that is a big

00:10:22,550 --> 00:10:28,100
failure

00:10:23,290 --> 00:10:30,650
so I've noticed also during

00:10:28,100 --> 00:10:33,590
coronaviruses of being a very world

00:10:30,650 --> 00:10:39,110
leading epidemiologist who has had his

00:10:33,590 --> 00:10:40,670
code criticized very publicly and by

00:10:39,110 --> 00:10:42,860
software engineers who are saying that

00:10:40,670 --> 00:10:44,120
his whole if it semiological model is

00:10:42,860 --> 00:10:47,090
wrong because they don't like the look

00:10:44,120 --> 00:10:49,040
of his code so here's a screenshot of a

00:10:47,090 --> 00:10:50,990
tweet saying that he should have used

00:10:49,040 --> 00:10:53,930
expressive class names they don't like

00:10:50,990 --> 00:10:56,060
the way his arguments are passed he has

00:10:53,930 --> 00:10:59,120
amateur dev traits the code as a lack of

00:10:56,060 --> 00:11:00,800
modularity and so on but that doesn't

00:10:59,120 --> 00:11:02,610
mean that he's not an expert in his

00:11:00,800 --> 00:11:04,980
field and some

00:11:02,610 --> 00:11:06,150
that we can learn from so that's not the

00:11:04,980 --> 00:11:09,630
right attitude to go into a

00:11:06,150 --> 00:11:11,490
collaboration with so I'm going to

00:11:09,630 --> 00:11:14,220
mention when a project I've worked on

00:11:11,490 --> 00:11:17,610
recently with health data and this is

00:11:14,220 --> 00:11:20,670
called project physio physio is an

00:11:17,610 --> 00:11:23,730
ongoing study of children with cystic

00:11:20,670 --> 00:11:26,580
fibrosis cystic fibrosis is a very

00:11:23,730 --> 00:11:30,290
serious genetic disease mainly affecting

00:11:26,580 --> 00:11:32,820
the lungs and children that are affected

00:11:30,290 --> 00:11:38,220
the main way that the disease is treated

00:11:32,820 --> 00:11:40,920
is to do these coughing and exhalation

00:11:38,220 --> 00:11:42,750
exercises to clear their lines of mucus

00:11:40,920 --> 00:11:45,870
and to actually cough up the mucus every

00:11:42,750 --> 00:11:48,960
day and it's a very demanding kind of

00:11:45,870 --> 00:11:52,440
physiotherapy to do so we have the

00:11:48,960 --> 00:11:54,420
collaboration between Microsoft and UCL

00:11:52,440 --> 00:11:56,970
Institute of Child Health and Great

00:11:54,420 --> 00:12:00,180
Ormond Street Hospital where a team at

00:11:56,970 --> 00:12:02,910
Microsoft developed and custom sensors

00:12:00,180 --> 00:12:05,400
custom patients pressure sensors to go

00:12:02,910 --> 00:12:08,180
into this device that the children do

00:12:05,400 --> 00:12:11,820
their exhalation physiotherapy into and

00:12:08,180 --> 00:12:13,500
custom video games as well and the

00:12:11,820 --> 00:12:16,530
pressure since actually controls the

00:12:13,500 --> 00:12:18,060
characters in the video games and our

00:12:16,530 --> 00:12:20,220
hope is that gamifying this

00:12:18,060 --> 00:12:23,100
physiotherapy will make it less of a

00:12:20,220 --> 00:12:24,660
burden on the children and because some

00:12:23,100 --> 00:12:26,340
of the children actually dislike their

00:12:24,660 --> 00:12:30,360
physiotherapy more than they dislike

00:12:26,340 --> 00:12:34,650
having this very serious disease and so

00:12:30,360 --> 00:12:37,440
my team did the kind of data analysis of

00:12:34,650 --> 00:12:40,050
this study and set up the pipeline for

00:12:37,440 --> 00:12:41,460
processing this huge amount of data huge

00:12:40,050 --> 00:12:44,760
amounts of time series data that's

00:12:41,460 --> 00:12:47,250
coming out and what I've shown here is

00:12:44,760 --> 00:12:49,080
sort of clusters of bricks with the

00:12:47,250 --> 00:12:50,850
child as they're breathing and then

00:12:49,080 --> 00:12:53,940
pauses to cough and then does their

00:12:50,850 --> 00:12:57,840
breathing and so on and I think this was

00:12:53,940 --> 00:13:00,030
a particularly successful collaboration

00:12:57,840 --> 00:13:02,390
because the clinicians we worked with

00:13:00,030 --> 00:13:06,000
were highly engaged in the data science

00:13:02,390 --> 00:13:07,770
so as long as well as myself as the data

00:13:06,000 --> 00:13:09,750
scientists we also had software

00:13:07,770 --> 00:13:11,340
engineers that helped build the

00:13:09,750 --> 00:13:15,690
pipelines and put this into production

00:13:11,340 --> 00:13:18,090
and experts being experts

00:13:15,690 --> 00:13:21,300
child health in cystic fibrosis and in

00:13:18,090 --> 00:13:24,090
physiotherapy and they actually attended

00:13:21,300 --> 00:13:27,720
all of our engineering daily stand-ups

00:13:24,090 --> 00:13:30,270
and all of our weekly meetings too which

00:13:27,720 --> 00:13:34,160
is incredible I've never had such a huge

00:13:30,270 --> 00:13:36,390
amount of engagement from clinicians and

00:13:34,160 --> 00:13:38,220
that was great because we were able to

00:13:36,390 --> 00:13:40,710
involve them in every decision about

00:13:38,220 --> 00:13:42,600
every data point that gets excluded or

00:13:40,710 --> 00:13:47,250
every feature that it's used or not used

00:13:42,600 --> 00:13:49,500
in analysis so now I'm going to talk

00:13:47,250 --> 00:13:52,680
about some of the cultural differences

00:13:49,500 --> 00:13:56,220
I've observed between take people and

00:13:52,680 --> 00:14:01,950
medical experts and advice about how we

00:13:56,220 --> 00:14:04,440
can all work well together so one major

00:14:01,950 --> 00:14:08,070
cultural difference is how we view

00:14:04,440 --> 00:14:11,040
technology and risk and progress I think

00:14:08,070 --> 00:14:13,950
as technologists we think of new

00:14:11,040 --> 00:14:17,720
technology as something that will

00:14:13,950 --> 00:14:20,880
improve our lives is something exciting

00:14:17,720 --> 00:14:24,080
something to work towards as a sign of

00:14:20,880 --> 00:14:27,930
progress but I did a project with a

00:14:24,080 --> 00:14:30,420
medical regulator that his concern is

00:14:27,930 --> 00:14:31,650
patient safety and they had a very

00:14:30,420 --> 00:14:34,770
different worldview which was

00:14:31,650 --> 00:14:41,150
fascinating they when they approve a new

00:14:34,770 --> 00:14:44,640
medical device to be used they just see

00:14:41,150 --> 00:14:46,680
new accidents that will happen new ways

00:14:44,640 --> 00:14:49,950
that the device could malfunction or be

00:14:46,680 --> 00:14:53,310
used in unexpected ways and they just

00:14:49,950 --> 00:14:55,170
see a kind of a flood of new new

00:14:53,310 --> 00:14:58,800
accidents and new harms

00:14:55,170 --> 00:15:01,440
happening so they want they want to keep

00:14:58,800 --> 00:15:03,270
patients safe and so that's a very

00:15:01,440 --> 00:15:04,560
different worldview which I thought it

00:15:03,270 --> 00:15:06,540
was quite gives quite an interesting

00:15:04,560 --> 00:15:10,170
insight into the way that this community

00:15:06,540 --> 00:15:12,650
thinks move fast and break things is a

00:15:10,170 --> 00:15:16,380
phrase that was popular in tech and

00:15:12,650 --> 00:15:19,140
startups however you can see it's so so

00:15:16,380 --> 00:15:21,720
different to the medical community where

00:15:19,140 --> 00:15:23,490
which moves very slowly very cautiously

00:15:21,720 --> 00:15:26,980
and does not want to break anything

00:15:23,490 --> 00:15:32,180
especially I'm healthcare

00:15:26,980 --> 00:15:35,060
so how can we be good collaborators and

00:15:32,180 --> 00:15:38,270
so I think medical researchers should be

00:15:35,060 --> 00:15:40,580
engaged and curious and open to learning

00:15:38,270 --> 00:15:43,090
trying to do things in a different way

00:15:40,580 --> 00:15:45,920
trying out new technology

00:15:43,090 --> 00:15:47,840
ideally able to commit time and it's

00:15:45,920 --> 00:15:51,110
very precious but the more time they can

00:15:47,840 --> 00:15:53,800
put in the better and I'd like to see

00:15:51,110 --> 00:15:56,600
more medical organizations investing in

00:15:53,800 --> 00:16:00,160
in-house data skills so I don't mean

00:15:56,600 --> 00:16:02,390
doctors learning Python I mean medical

00:16:00,160 --> 00:16:05,620
organizations are indeed departments

00:16:02,390 --> 00:16:08,960
hiring data scientists and data analysts

00:16:05,620 --> 00:16:11,870
so that they can take ownership of the

00:16:08,960 --> 00:16:13,670
analysis rather than collecting data and

00:16:11,870 --> 00:16:19,400
giving it to another organization to

00:16:13,670 --> 00:16:21,620
analyze and then finally I can say this

00:16:19,400 --> 00:16:23,000
having spent time in academia not

00:16:21,620 --> 00:16:27,260
letting perfect get in the way of

00:16:23,000 --> 00:16:29,990
progress so often researchers are kind

00:16:27,260 --> 00:16:32,750
of overwhelmed by the complexity of the

00:16:29,990 --> 00:16:34,790
disease area that they work in and I

00:16:32,750 --> 00:16:36,920
feel like engineers are quite good at

00:16:34,790 --> 00:16:39,370
coming in breaking down a problem and

00:16:36,920 --> 00:16:43,000
starting to build something simple first

00:16:39,370 --> 00:16:48,020
just to get going

00:16:43,000 --> 00:16:50,630
for the tech people I would say that we

00:16:48,020 --> 00:16:55,280
are responsible for using data in an

00:16:50,630 --> 00:16:59,810
ethical way so it's extremely valuable

00:16:55,280 --> 00:17:02,960
to have lawyers sociologists

00:16:59,810 --> 00:17:06,530
philosophers who who are experts in

00:17:02,960 --> 00:17:08,810
ethics involved in this conversation but

00:17:06,530 --> 00:17:10,820
it's also crucial that data scientists

00:17:08,810 --> 00:17:14,000
have a seat at the table because we

00:17:10,820 --> 00:17:16,490
understand data and the algorithms and

00:17:14,000 --> 00:17:18,589
the limitations of them and what they

00:17:16,490 --> 00:17:20,030
can do better than anyone else so it's

00:17:18,589 --> 00:17:24,370
really important that we're actively

00:17:20,030 --> 00:17:28,760
involved in this discussion and that we

00:17:24,370 --> 00:17:30,830
slag any anything in our work which does

00:17:28,760 --> 00:17:33,470
not seem ethical that's really is our

00:17:30,830 --> 00:17:35,630
responsibility we should be humble

00:17:33,470 --> 00:17:36,920
respectful and relying on germain

00:17:35,630 --> 00:17:39,290
expertise

00:17:36,920 --> 00:17:41,120
Engineers in particular should not be

00:17:39,290 --> 00:17:44,210
judging a fish by its ability to climb a

00:17:41,120 --> 00:17:46,430
tree we should be inviting researchers

00:17:44,210 --> 00:17:48,320
to meetings and planning documents and

00:17:46,430 --> 00:17:50,840
write into our code repo so that they

00:17:48,320 --> 00:17:53,900
see what we're doing and are involved in

00:17:50,840 --> 00:17:55,700
our decisions they should be patient in

00:17:53,900 --> 00:17:57,470
a tech phobic environment because we

00:17:55,700 --> 00:18:00,080
understand why the environment is

00:17:57,470 --> 00:18:02,150
conservative and cautious data

00:18:00,080 --> 00:18:04,640
scientists should be prepared for messy

00:18:02,150 --> 00:18:08,390
real-world data and they should be

00:18:04,640 --> 00:18:10,520
prepared to balance statistical machine

00:18:08,390 --> 00:18:11,990
learning matrix with what the clinician

00:18:10,520 --> 00:18:14,900
wants and what makes sense to the

00:18:11,990 --> 00:18:18,670
clinician and data scientists should be

00:18:14,900 --> 00:18:18,670
obsessed with building something useful

00:18:19,340 --> 00:18:24,230
so hopefully that overview was was was

00:18:22,130 --> 00:18:28,280
useful and hopefully what you take away

00:18:24,230 --> 00:18:31,580
from this talk is that medical health

00:18:28,280 --> 00:18:36,580
data is is tricky for a number of

00:18:31,580 --> 00:18:39,860
reasons that data people must include

00:18:36,580 --> 00:18:44,210
medical experts in their projects from

00:18:39,860 --> 00:18:45,470
the beginning and that you can expect

00:18:44,210 --> 00:18:48,560
some cultural differences between

00:18:45,470 --> 00:18:50,480
collaborators on a project and if you

00:18:48,560 --> 00:18:53,540
recognize them hopefully we can all play

00:18:50,480 --> 00:18:56,900
our role in in collaborating nicely

00:18:53,540 --> 00:18:59,030
together thank you and you can find me

00:18:56,900 --> 00:19:03,440
on Twitter and you can find out about my

00:18:59,030 --> 00:19:07,900
team at this your own lift and about our

00:19:03,440 --> 00:19:07,900

YouTube URL: https://www.youtube.com/watch?v=Gdi8T2XKLss


