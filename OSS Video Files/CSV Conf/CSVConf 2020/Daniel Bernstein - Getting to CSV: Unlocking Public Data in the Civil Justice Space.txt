Title: Daniel Bernstein - Getting to CSV: Unlocking Public Data in the Civil Justice Space
Publication date: 2020-05-30
Playlist: CSVConf 2020
Description: 
	Many Americans are forced to navigate the civil justice system without an attorney because they cannot afford representation. To better support self-represented individuals, we need detailed information about these individualsâ€™ experiences. Data documenting the parties, events, and outcomes in court cases could help legal aid organizations better address their local needs and court systems better manage public resources. However, state and local courts operate their own data management systems and do not make this information readily available in a format suitable for analytical purposes. Courts also vary drastically in the granularity of their public records, causing inconsistencies when attempting to compare data across jurisdictions. This talk will discuss our work to web scrape, manage, and analyze millions of civil court records to better understand how individuals without attorneys fare in the civil justice system, especially in important areas such as eviction, debt, and domestic violence. This talk will include lessons learned in creating massive datasets and promoting data sharing while maintaining data privacy.

--
csv,conf,v5 is a community conference for data makers everywhere, featuring stories about data sharing and data analysis from science, journalism, government, and open source.  Held May 13-14, 2020, Online. https://csvconf.com/
Captions: 
	00:00:00,030 --> 00:00:04,740
well hello everyone I am dan Bernstein

00:00:02,760 --> 00:00:06,569
and I'm a research analyst at the Legal

00:00:04,740 --> 00:00:08,730
Services Corporation and I'll be

00:00:06,569 --> 00:00:11,219
speaking today about my office's work to

00:00:08,730 --> 00:00:13,530
collect and analyze very messy and very

00:00:11,219 --> 00:00:16,080
diverse data from across the u.s. civil

00:00:13,530 --> 00:00:17,279
justice system so I hope that you come

00:00:16,080 --> 00:00:19,949
away from this talk with a better

00:00:17,279 --> 00:00:22,380
understanding of how civil court data

00:00:19,949 --> 00:00:24,539
can support more informed policymaking

00:00:22,380 --> 00:00:25,890
and help organizations promote equality

00:00:24,539 --> 00:00:28,230
in the justice system

00:00:25,890 --> 00:00:29,689
my topic is focused around a few

00:00:28,230 --> 00:00:31,980
questions that are listed on the slide

00:00:29,689 --> 00:00:33,630
and while I'll be talking about a lot of

00:00:31,980 --> 00:00:35,460
the lessons learned from that this past

00:00:33,630 --> 00:00:37,350
year of work I will mention that this

00:00:35,460 --> 00:00:39,660
project is ongoing and we're eager to

00:00:37,350 --> 00:00:41,760
engage with others in the data space or

00:00:39,660 --> 00:00:43,590
the legal space to inform our approach

00:00:41,760 --> 00:00:47,730
to data gathering data sharing and data

00:00:43,590 --> 00:00:49,500
privacy so very briefly I work for the

00:00:47,730 --> 00:00:51,870
Legal Services Corporation which is a

00:00:49,500 --> 00:00:54,360
nonprofit that was established by the US

00:00:51,870 --> 00:00:56,370
Congress to fill the Justice gap the

00:00:54,360 --> 00:00:58,590
Justice gap is a difference between

00:00:56,370 --> 00:01:00,420
criminal and civil court in u.s.

00:00:58,590 --> 00:01:02,870
criminal court if you cannot afford an

00:01:00,420 --> 00:01:05,309
attorney you can have a public defender

00:01:02,870 --> 00:01:07,560
appointed to represent you this

00:01:05,309 --> 00:01:09,900
protection does not exist in the u.s.

00:01:07,560 --> 00:01:12,180
civil justice system if you are facing a

00:01:09,900 --> 00:01:13,650
fiction or debt collection or attempting

00:01:12,180 --> 00:01:15,600
to address domestic violence in your

00:01:13,650 --> 00:01:17,759
life if you cannot afford an attorney

00:01:15,600 --> 00:01:20,159
you are forced to represent yourself in

00:01:17,759 --> 00:01:22,110
court many studies have shown that your

00:01:20,159 --> 00:01:24,420
odds of winning a case are significantly

00:01:22,110 --> 00:01:25,770
better when you have an attorney so this

00:01:24,420 --> 00:01:28,170
is a large issue for low-income

00:01:25,770 --> 00:01:30,090
individuals who face many civil legal

00:01:28,170 --> 00:01:32,850
issues but cannot afford legal

00:01:30,090 --> 00:01:35,070
representation LSC the Legal Services

00:01:32,850 --> 00:01:36,990
Corporation receives almost half a

00:01:35,070 --> 00:01:39,210
billion dollars a year to fund civil

00:01:36,990 --> 00:01:41,880
legal aid we provide those money as

00:01:39,210 --> 00:01:43,229
grants to local legal aid providers

00:01:41,880 --> 00:01:45,420
throughout the United States and the

00:01:43,229 --> 00:01:47,430
territories they use the money to help

00:01:45,420 --> 00:01:50,670
low-income individuals with their legal

00:01:47,430 --> 00:01:52,710
issues for free the grantees mostly

00:01:50,670 --> 00:01:54,659
represent people in court but the data

00:01:52,710 --> 00:01:57,119
about this larger civil justice system

00:01:54,659 --> 00:02:00,659
is not usually accessible to the people

00:01:57,119 --> 00:02:02,549
in the courtroom within LSC I work in

00:02:00,659 --> 00:02:04,020
office of data governance and analysis

00:02:02,549 --> 00:02:07,079
and this is a new office that was

00:02:04,020 --> 00:02:08,970
created in 2015 that was supposed to

00:02:07,079 --> 00:02:11,340
help the organization and our grantees

00:02:08,970 --> 00:02:12,720
around the country better use data in

00:02:11,340 --> 00:02:16,290
their work in operation

00:02:12,720 --> 00:02:18,240
in outreach and so in the past year or

00:02:16,290 --> 00:02:20,400
so our office has realized that there's

00:02:18,240 --> 00:02:22,560
a need to better understand how we GLE

00:02:20,400 --> 00:02:24,750
issues affecting low-income Americans

00:02:22,560 --> 00:02:27,150
actually play out in the court system

00:02:24,750 --> 00:02:28,800
and so that has led to a project that

00:02:27,150 --> 00:02:31,020
I've spent the past year working on I

00:02:28,800 --> 00:02:33,990
call it you know locally the civil court

00:02:31,020 --> 00:02:36,210
data project and our goal is to use web

00:02:33,990 --> 00:02:38,280
scraping to gather court records from

00:02:36,210 --> 00:02:40,950
state and county courts across the

00:02:38,280 --> 00:02:43,260
United States for a few reasons first

00:02:40,950 --> 00:02:44,900
you want to answer very basic questions

00:02:43,260 --> 00:02:47,550
about how low-income individuals

00:02:44,900 --> 00:02:49,560
experience the civil justice system this

00:02:47,550 --> 00:02:51,570
includes questions like how often are

00:02:49,560 --> 00:02:53,820
low-income individuals represented in

00:02:51,570 --> 00:02:56,250
eviction or debt collection cases and

00:02:53,820 --> 00:02:58,440
how does legal representation correlate

00:02:56,250 --> 00:03:00,180
with outcomes these questions seem very

00:02:58,440 --> 00:03:01,920
basic but there has been a little

00:03:00,180 --> 00:03:04,170
research that uses a large amounts of

00:03:01,920 --> 00:03:06,450
data to describe the landscape of civil

00:03:04,170 --> 00:03:08,430
justice and poverty we also want to

00:03:06,450 --> 00:03:10,560
build tools such as dashboards that help

00:03:08,430 --> 00:03:12,750
our grantees better align their work

00:03:10,560 --> 00:03:14,760
with their local needs if we can

00:03:12,750 --> 00:03:16,709
determine parts of a state or city that

00:03:14,760 --> 00:03:19,290
are experiencing a high amount of legal

00:03:16,709 --> 00:03:21,540
issues our grantees can adjust their

00:03:19,290 --> 00:03:23,220
outreach to target high need areas all

00:03:21,540 --> 00:03:25,380
of these questions revolve around

00:03:23,220 --> 00:03:27,090
helping legal aid providers which are

00:03:25,380 --> 00:03:29,190
organizations that traditionally have

00:03:27,090 --> 00:03:32,250
not had access to large amounts of data

00:03:29,190 --> 00:03:33,720
or data analytic expertise to help them

00:03:32,250 --> 00:03:35,880
better serve their communities and

00:03:33,720 --> 00:03:37,739
communicate to funders that their work

00:03:35,880 --> 00:03:40,920
is important in changing the lives of

00:03:37,739 --> 00:03:43,080
low-income Americans to give you an

00:03:40,920 --> 00:03:45,959
example of how you we can use court data

00:03:43,080 --> 00:03:47,970
about two weeks ago my office received a

00:03:45,959 --> 00:03:49,890
request for data about garnishment in

00:03:47,970 --> 00:03:52,410
the state of Tennessee if you lose a

00:03:49,890 --> 00:03:55,080
court case and Oh someone money the

00:03:52,410 --> 00:03:56,670
winner can file to garnish your wages or

00:03:55,080 --> 00:03:58,650
bank account which means that they can

00:03:56,670 --> 00:04:01,290
directly take money out of your paycheck

00:03:58,650 --> 00:04:03,420
before it is deposited we were asked by

00:04:01,290 --> 00:04:05,700
a group of advocacy organizations in

00:04:03,420 --> 00:04:07,799
Tennessee to give them any data that

00:04:05,700 --> 00:04:09,930
showed that private debt collectors were

00:04:07,799 --> 00:04:12,360
still trying to collect outstanding debt

00:04:09,930 --> 00:04:15,120
even as millions of Americans lost their

00:04:12,360 --> 00:04:16,709
jobs to the Cova 19 pandemic the

00:04:15,120 --> 00:04:18,930
advocates wanted to convince the

00:04:16,709 --> 00:04:20,789
Tennessee Supreme Court that the court

00:04:18,930 --> 00:04:22,680
needed to put in place an order that

00:04:20,789 --> 00:04:24,900
would forbid private debt collectors

00:04:22,680 --> 00:04:26,520
from garnishing the $1,200

00:04:24,900 --> 00:04:29,370
stimulus payments

00:04:26,520 --> 00:04:30,930
Americans received in April I got to

00:04:29,370 --> 00:04:33,090
work assessing the availability of

00:04:30,930 --> 00:04:35,550
corrugated in Tennessee and in less than

00:04:33,090 --> 00:04:37,860
36 hours we were able to gather and

00:04:35,550 --> 00:04:39,449
analyze five years worth of court data

00:04:37,860 --> 00:04:41,879
which included over three hundred

00:04:39,449 --> 00:04:44,550
thousand civil cases and we clearly

00:04:41,879 --> 00:04:47,220
showed that almost 2500 people adjust in

00:04:44,550 --> 00:04:49,289
the city of Memphis had experienced some

00:04:47,220 --> 00:04:51,360
kind of court activity related to you a

00:04:49,289 --> 00:04:53,849
debt collector trying to garnish their

00:04:51,360 --> 00:04:55,500
wages in the month of April we were also

00:04:53,849 --> 00:04:57,960
able to highlight many cases where

00:04:55,500 --> 00:05:00,030
employers specifically identify the

00:04:57,960 --> 00:05:02,310
pandemic as a reason that someone had

00:05:00,030 --> 00:05:04,169
lost their job so the employer would not

00:05:02,310 --> 00:05:06,750
be able to withhold any wages from these

00:05:04,169 --> 00:05:08,699
individuals the advocates deliver this

00:05:06,750 --> 00:05:10,199
analysis to the Tennessee Supreme Court

00:05:08,699 --> 00:05:13,800
and it will be used as they make a

00:05:10,199 --> 00:05:15,090
determination about their next steps to

00:05:13,800 --> 00:05:17,099
give you a sense of what the data

00:05:15,090 --> 00:05:19,259
generally looks like we can take a look

00:05:17,099 --> 00:05:21,569
at this example here we have an example

00:05:19,259 --> 00:05:23,310
of a web page from Tarrant County Texas

00:05:21,569 --> 00:05:26,009
which is the county including the city

00:05:23,310 --> 00:05:28,050
of Fort Worth this web page comes from

00:05:26,009 --> 00:05:29,639
the public access wet web portal where

00:05:28,050 --> 00:05:32,159
you can look up any case that has been

00:05:29,639 --> 00:05:33,719
filed and entered into the system in

00:05:32,159 --> 00:05:35,340
this example we can see that there are

00:05:33,719 --> 00:05:37,530
many useful elements from a data

00:05:35,340 --> 00:05:39,690
analysis perspective we have a case

00:05:37,530 --> 00:05:41,849
number that uniquely identifies this

00:05:39,690 --> 00:05:44,069
case from all other cases in the system

00:05:41,849 --> 00:05:46,319
we also have information about when the

00:05:44,069 --> 00:05:48,509
case was filed what the case type is

00:05:46,319 --> 00:05:51,090
here it's an eviction case from the year

00:05:48,509 --> 00:05:53,009
2000 and we also get party names and

00:05:51,090 --> 00:05:55,500
street addresses which enable geospatial

00:05:53,009 --> 00:05:57,509
analysis we also get the names of any

00:05:55,500 --> 00:06:00,150
attorneys associated with the two

00:05:57,509 --> 00:06:01,830
parties in this case neither party is

00:06:00,150 --> 00:06:04,800
represented so the lead attorney for

00:06:01,830 --> 00:06:07,949
both parties is identified as pro se

00:06:04,800 --> 00:06:09,719
which means on behalf of themselves we

00:06:07,949 --> 00:06:11,490
also have information about who won the

00:06:09,719 --> 00:06:13,169
case and how much money the loser owes

00:06:11,490 --> 00:06:14,880
the winner all these pieces of

00:06:13,169 --> 00:06:16,919
information could help us answer the

00:06:14,880 --> 00:06:19,469
questions we are after but they're stuck

00:06:16,919 --> 00:06:21,990
on this HTML web page rather than in a

00:06:19,469 --> 00:06:22,740
clean tabular format that's ready for

00:06:21,990 --> 00:06:25,139
analysis

00:06:22,740 --> 00:06:29,300
my work is to get all these case files

00:06:25,139 --> 00:06:32,190
and extract information for analysis but

00:06:29,300 --> 00:06:34,680
as we'll see it's not as easy as just

00:06:32,190 --> 00:06:36,539
writing one script for scraping and

00:06:34,680 --> 00:06:38,639
another one for parsing there are many

00:06:36,539 --> 00:06:40,050
barriers to gathering this data that we

00:06:38,639 --> 00:06:42,090
have had to overcome

00:06:40,050 --> 00:06:44,819
that we continue to address with every

00:06:42,090 --> 00:06:47,909
location that we attempt to stay first

00:06:44,819 --> 00:06:49,830
not all states provide access to civil

00:06:47,909 --> 00:06:51,810
court records this map shows the

00:06:49,830 --> 00:06:54,569
availability of court records at the

00:06:51,810 --> 00:06:56,759
state level only nine states make data

00:06:54,569 --> 00:06:59,460
available in a manner that we consider

00:06:56,759 --> 00:07:01,530
available this means that a single

00:06:59,460 --> 00:07:03,629
website contains multiple years of

00:07:01,530 --> 00:07:06,440
historical case records like we saw

00:07:03,629 --> 00:07:09,360
before for all counties within the state

00:07:06,440 --> 00:07:11,220
this also means that a state website has

00:07:09,360 --> 00:07:13,590
not imposed any limitations that

00:07:11,220 --> 00:07:16,409
prevents scraping such as a CAPTCHA a

00:07:13,590 --> 00:07:19,259
paywall a login or Terms of Service that

00:07:16,409 --> 00:07:20,639
explicitly prohibit scraping the nine

00:07:19,259 --> 00:07:22,979
states that are available provide a

00:07:20,639 --> 00:07:25,199
decent diversity of geographies but

00:07:22,979 --> 00:07:26,940
we're missing data from the Western and

00:07:25,199 --> 00:07:30,270
South East and New England parts of the

00:07:26,940 --> 00:07:31,949
United States so we've also investigated

00:07:30,270 --> 00:07:32,250
the availability of data at the county

00:07:31,949 --> 00:07:33,960
level

00:07:32,250 --> 00:07:35,250
there are many counties in the United

00:07:33,960 --> 00:07:37,530
States that have more residents than

00:07:35,250 --> 00:07:39,419
entire states we went through a process

00:07:37,530 --> 00:07:41,280
of assessing the 100 largest

00:07:39,419 --> 00:07:43,319
metropolitan areas for the availability

00:07:41,280 --> 00:07:45,630
of court data the same way we did with

00:07:43,319 --> 00:07:47,880
the states and we've also leveraged data

00:07:45,630 --> 00:07:49,860
on the prevalence of eviction and debt

00:07:47,880 --> 00:07:52,229
that others have published to prioritize

00:07:49,860 --> 00:07:54,090
where we use our resources in the past

00:07:52,229 --> 00:07:56,400
year we have conducted data collection

00:07:54,090 --> 00:07:59,250
in the locations on this map for the

00:07:56,400 --> 00:08:02,370
period of 2000 through the end of 2019

00:07:59,250 --> 00:08:04,199
today this data includes over 25 million

00:08:02,370 --> 00:08:06,229
civil case records in the states and

00:08:04,199 --> 00:08:09,659
counties for for the states and counties

00:08:06,229 --> 00:08:11,250
that are home to over 40 million

00:08:09,659 --> 00:08:13,319
Americans we're still missing

00:08:11,250 --> 00:08:14,879
geographies in the Pacific Northwest and

00:08:13,319 --> 00:08:16,379
New England but we have filled in many

00:08:14,879 --> 00:08:19,199
gaps and will continue to pursue

00:08:16,379 --> 00:08:20,819
counties as our time allows we're also

00:08:19,199 --> 00:08:22,770
currently building an architecture to

00:08:20,819 --> 00:08:24,930
allow us to scrape and parse data in

00:08:22,770 --> 00:08:26,729
near real-time so that as the United

00:08:24,930 --> 00:08:29,550
States begins to come out of the

00:08:26,729 --> 00:08:31,710
pandemic we can monitor the patterns in

00:08:29,550 --> 00:08:33,959
case violence and outcomes we'll want to

00:08:31,710 --> 00:08:35,820
see how landlords respond to the

00:08:33,959 --> 00:08:37,680
reopening will courts be inundated with

00:08:35,820 --> 00:08:39,449
eviction cases and how does that vary

00:08:37,680 --> 00:08:41,219
across states in relation to the

00:08:39,449 --> 00:08:43,919
policies and laws that they have in

00:08:41,219 --> 00:08:46,260
place and overall what is the geographic

00:08:43,919 --> 00:08:48,230
distribution will be another key element

00:08:46,260 --> 00:08:50,699
that we'll be studying

00:08:48,230 --> 00:08:53,250
another major issue is the structure of

00:08:50,699 --> 00:08:55,800
the data there's no standard format for

00:08:53,250 --> 00:08:58,080
data here we see five examples five

00:08:55,800 --> 00:08:59,940
different formats they all display

00:08:58,080 --> 00:09:02,340
similar information about court cases

00:08:59,940 --> 00:09:04,680
but the HTML structure and the variable

00:09:02,340 --> 00:09:06,360
names are all different in some areas we

00:09:04,680 --> 00:09:08,070
can use the same scripts for scraping

00:09:06,360 --> 00:09:11,370
and parsing because counties in the same

00:09:08,070 --> 00:09:13,680
geographic area seem to cluster and use

00:09:11,370 --> 00:09:15,720
the same website technology providers

00:09:13,680 --> 00:09:17,610
but for the most part we have to develop

00:09:15,720 --> 00:09:20,100
custom scripts for every jurisdiction

00:09:17,610 --> 00:09:22,140
that we approach having similar data in

00:09:20,100 --> 00:09:23,640
different formats also forces us to

00:09:22,140 --> 00:09:26,520
address the question of standardization

00:09:23,640 --> 00:09:28,470
we either have to preserve the format

00:09:26,520 --> 00:09:30,690
and naming conventions of the individual

00:09:28,470 --> 00:09:33,420
of data sources and our analyses would

00:09:30,690 --> 00:09:34,830
be more complex and time-consuming or we

00:09:33,420 --> 00:09:36,540
have to make assumptions and impose a

00:09:34,830 --> 00:09:38,370
data model on the datasets

00:09:36,540 --> 00:09:41,580
now it may cross eye analysis either

00:09:38,370 --> 00:09:44,100
easier but we lose the nuance of the

00:09:41,580 --> 00:09:46,590
individual sources we have chosen to not

00:09:44,100 --> 00:09:48,510
standardize data up to this point our

00:09:46,590 --> 00:09:50,190
intention is to extract the data in

00:09:48,510 --> 00:09:52,500
these web pages and use the original

00:09:50,190 --> 00:09:54,240
naming conventions and structure so that

00:09:52,500 --> 00:09:56,340
an analyst has an easier time

00:09:54,240 --> 00:09:59,280
understanding how the clean data relates

00:09:56,340 --> 00:10:01,680
to the raw HTML data and but as we

00:09:59,280 --> 00:10:03,570
pursue analyses across jurisdictions we

00:10:01,680 --> 00:10:05,640
will have to make some assumptions and

00:10:03,570 --> 00:10:09,540
do some standardizations for specific

00:10:05,640 --> 00:10:11,400
analyses to further describe our

00:10:09,540 --> 00:10:13,170
workload here's a basic diagram of how I

00:10:11,400 --> 00:10:15,060
imagine my workflow would be when we

00:10:13,170 --> 00:10:16,560
started this project I thought that we

00:10:15,060 --> 00:10:18,660
would evaluate the data on a given

00:10:16,560 --> 00:10:20,640
website if the site was suitable we

00:10:18,660 --> 00:10:22,350
would scrape all the case numbers and

00:10:20,640 --> 00:10:24,510
then look up each case number to get the

00:10:22,350 --> 00:10:26,460
Associated case record and we have then

00:10:24,510 --> 00:10:29,070
have all the case records then we would

00:10:26,460 --> 00:10:31,230
simply parse the HTML clean the parsed

00:10:29,070 --> 00:10:33,360
data and produce beautiful tabular data

00:10:31,230 --> 00:10:35,490
and documentation ready for analysis but

00:10:33,360 --> 00:10:37,170
as you might guess it didn't work out

00:10:35,490 --> 00:10:39,990
that way we found that there were issues

00:10:37,170 --> 00:10:42,150
at nearly every step in the process the

00:10:39,990 --> 00:10:44,640
forced us to go back and iterate on

00:10:42,150 --> 00:10:46,170
scripts and strategies when working with

00:10:44,640 --> 00:10:47,960
data on the web you might have issues

00:10:46,170 --> 00:10:50,730
related to connecting to a server or

00:10:47,960 --> 00:10:52,260
issues retrieving data the server might

00:10:50,730 --> 00:10:54,060
send back errors if you query the

00:10:52,260 --> 00:10:56,130
website too quickly or if the server

00:10:54,060 --> 00:10:57,330
might just be inaccessible during

00:10:56,130 --> 00:11:00,060
certain hours of the day due to

00:10:57,330 --> 00:11:01,950
maintenance sometimes we get all the way

00:11:00,060 --> 00:11:03,839
to clean data before realizing that a

00:11:01,950 --> 00:11:05,110
substantial portion of the data contain

00:11:03,839 --> 00:11:06,730
errors that we could have

00:11:05,110 --> 00:11:09,370
tected if we had built in certain

00:11:06,730 --> 00:11:11,890
explicit verification steps earlier in

00:11:09,370 --> 00:11:13,779
the process after overcoming all the

00:11:11,890 --> 00:11:16,329
issues in web scraping there are other

00:11:13,779 --> 00:11:18,820
issues in extracting data some web pages

00:11:16,329 --> 00:11:20,950
might look like they contain a table but

00:11:18,820 --> 00:11:22,959
due to errors in the underlying HTML

00:11:20,950 --> 00:11:25,750
there might actually be missing HTML

00:11:22,959 --> 00:11:27,910
tags that ruin what would otherwise be a

00:11:25,750 --> 00:11:30,220
pretty predictable extraction process

00:11:27,910 --> 00:11:31,839
and after parsing we've had to make

00:11:30,220 --> 00:11:34,089
assumptions about the data and type

00:11:31,839 --> 00:11:35,500
conversions and feature engineering for

00:11:34,089 --> 00:11:38,170
example there might be an element on

00:11:35,500 --> 00:11:40,420
page that is called money owed which you

00:11:38,170 --> 00:11:42,459
assume is a numeric value but after

00:11:40,420 --> 00:11:44,529
cleaning the data you find all kinds of

00:11:42,459 --> 00:11:46,390
wonky values that you never would assume

00:11:44,529 --> 00:11:48,339
would be under that variable which

00:11:46,390 --> 00:11:50,459
forces you to go back and iterate on how

00:11:48,339 --> 00:11:52,720
you clean information from that section

00:11:50,459 --> 00:11:54,970
so when dealing with thousands or

00:11:52,720 --> 00:11:56,709
millions of items like we are we needed

00:11:54,970 --> 00:11:59,589
to really understand that the diversity

00:11:56,709 --> 00:12:02,440
of potential values by studying dozens

00:11:59,589 --> 00:12:04,060
of HTML files from each site to really

00:12:02,440 --> 00:12:08,050
understand them before we were able to

00:12:04,060 --> 00:12:09,970
clean the data so as I mentioned we have

00:12:08,050 --> 00:12:11,949
recently begun developing the cloud

00:12:09,970 --> 00:12:13,930
architecture needed to do the scraping

00:12:11,949 --> 00:12:15,370
and parsing on a regular basis when

00:12:13,930 --> 00:12:17,620
you're doing this data collection just

00:12:15,370 --> 00:12:19,390
once for historical data there is room

00:12:17,620 --> 00:12:21,760
to iterate and repeat steps when they

00:12:19,390 --> 00:12:23,320
fail but in an automated workflow we

00:12:21,760 --> 00:12:25,510
really need to fully understand the ways

00:12:23,320 --> 00:12:27,610
a website operates and represents data

00:12:25,510 --> 00:12:30,579
so that we don't have to manually go in

00:12:27,610 --> 00:12:32,110
and fix code and data frequently so in

00:12:30,579 --> 00:12:33,610
developing these new workflows I've

00:12:32,110 --> 00:12:35,709
reflected on the lessons I have learned

00:12:33,610 --> 00:12:37,930
from developing dozens of scrapers and

00:12:35,709 --> 00:12:39,850
parses over the past year the lessons

00:12:37,930 --> 00:12:41,980
really break down to three areas be

00:12:39,850 --> 00:12:45,250
respectful be resilient and be

00:12:41,980 --> 00:12:47,890
resourceful first we need to understand

00:12:45,250 --> 00:12:49,660
that the scrapers do impose a burden on

00:12:47,890 --> 00:12:51,670
the websites they are accessing if you

00:12:49,660 --> 00:12:53,529
query a website too quickly you could

00:12:51,670 --> 00:12:55,690
have your IP address blocked or take the

00:12:53,529 --> 00:12:57,370
server down momentarily it's important

00:12:55,690 --> 00:12:59,140
to monitor how long it takes the server

00:12:57,370 --> 00:13:00,579
to respond to your requests and to add

00:12:59,140 --> 00:13:02,709
delays in your code to ensure you are

00:13:00,579 --> 00:13:04,839
not overburdening the website servers

00:13:02,709 --> 00:13:07,000
the Urban Institute has developed a

00:13:04,839 --> 00:13:09,130
simple tool called site monitor that is

00:13:07,000 --> 00:13:10,750
available on github and monitors your

00:13:09,130 --> 00:13:12,760
requests to a server and will

00:13:10,750 --> 00:13:14,589
dynamically adjust the time between your

00:13:12,760 --> 00:13:17,050
requests so that you don't overburden

00:13:14,589 --> 00:13:18,279
servers I strongly recommend this tool

00:13:17,050 --> 00:13:19,810
to anyone developing

00:13:18,279 --> 00:13:22,300
papers that will access websites for

00:13:19,810 --> 00:13:24,490
thousands or millions of queries the

00:13:22,300 --> 00:13:25,959
next lesson is to be resilient many of

00:13:24,490 --> 00:13:28,060
the errors and our workflow came from

00:13:25,959 --> 00:13:29,829
not fully understanding the websites we

00:13:28,060 --> 00:13:32,139
were scraping there are predictable

00:13:29,829 --> 00:13:34,360
errors such as incorrect inputs and also

00:13:32,139 --> 00:13:36,939
unpredictable ones such as server errors

00:13:34,360 --> 00:13:39,339
downtime and IP blocking you want to

00:13:36,939 --> 00:13:41,319
manage as many of the known issues up

00:13:39,339 --> 00:13:43,540
front and also building mechanisms to

00:13:41,319 --> 00:13:46,180
learn about unknown issues as they arise

00:13:43,540 --> 00:13:48,819
I spend a good amount of time upfront

00:13:46,180 --> 00:13:50,829
just manually investigating a website to

00:13:48,819 --> 00:13:53,709
understand what query patterns will

00:13:50,829 --> 00:13:55,689
trigger errors and what are the HTML CSS

00:13:53,709 --> 00:13:57,730
and JavaScript patterns that occurred

00:13:55,689 --> 00:14:00,249
during the errors to build mechanisms to

00:13:57,730 --> 00:14:02,499
overcome them we have also built in

00:14:00,249 --> 00:14:03,970
verification steps to our workflow after

00:14:02,499 --> 00:14:06,339
scraping all the case records and

00:14:03,970 --> 00:14:08,199
parsing them we explicitly compare the

00:14:06,339 --> 00:14:09,970
number of raw Records to the number of

00:14:08,199 --> 00:14:12,459
parsed records and thoroughly

00:14:09,970 --> 00:14:14,220
investigate any discrepancies we do the

00:14:12,459 --> 00:14:17,350
same at every stage of the process

00:14:14,220 --> 00:14:19,449
finally be resourceful when you're

00:14:17,350 --> 00:14:21,309
automating web queries each decision you

00:14:19,449 --> 00:14:23,110
make has ripple effects if you could if

00:14:21,309 --> 00:14:24,910
you include a code snippet that takes

00:14:23,110 --> 00:14:26,800
twice as long as alternative that

00:14:24,910 --> 00:14:29,199
doubling propagates in every code

00:14:26,800 --> 00:14:30,939
iteration this can drastically lengthen

00:14:29,199 --> 00:14:32,980
the time it takes to scrape and parse

00:14:30,939 --> 00:14:34,569
court records when dealing with web

00:14:32,980 --> 00:14:36,819
scraping you often have to choose

00:14:34,569 --> 00:14:39,040
between either using simple requests to

00:14:36,819 --> 00:14:40,660
a server or using a slower headless

00:14:39,040 --> 00:14:42,610
browser approach that actually renders

00:14:40,660 --> 00:14:44,620
every webpage that it encounters as you

00:14:42,610 --> 00:14:46,209
navigate through a website if you

00:14:44,620 --> 00:14:48,910
thoroughly investigate the network

00:14:46,209 --> 00:14:50,230
activity required to access web data you

00:14:48,910 --> 00:14:52,689
might find that you might not actually

00:14:50,230 --> 00:14:55,180
need a slower headless browser approach

00:14:52,689 --> 00:14:57,910
in a series of HTTP requests might

00:14:55,180 --> 00:15:00,360
actually suffice this will greatly speed

00:14:57,910 --> 00:15:03,429
up your workflow and most browsers

00:15:00,360 --> 00:15:05,439
include great network analysis tools

00:15:03,429 --> 00:15:08,649
that make this process very easy to

00:15:05,439 --> 00:15:10,089
understand and to mimic we've also

00:15:08,649 --> 00:15:12,730
learned lessons about creating large

00:15:10,089 --> 00:15:14,170
data sets when I have used public data

00:15:12,730 --> 00:15:16,809
sets in the past I never really thought

00:15:14,170 --> 00:15:18,790
about how difficult the decisions must

00:15:16,809 --> 00:15:20,800
have been for the creators to take

00:15:18,790 --> 00:15:22,689
disparate data sets and put them into a

00:15:20,800 --> 00:15:24,910
format and make those assumptions and

00:15:22,689 --> 00:15:26,559
document all those things so the first

00:15:24,910 --> 00:15:29,050
lesson is to be clear about the

00:15:26,559 --> 00:15:30,699
intentions of your data set we continue

00:15:29,050 --> 00:15:31,900
to debate the relative merits of

00:15:30,699 --> 00:15:34,540
preserving a structure

00:15:31,900 --> 00:15:37,270
of raw data or standardizing across

00:15:34,540 --> 00:15:39,040
different websites the ultimate decision

00:15:37,270 --> 00:15:40,900
really comes down to how people will

00:15:39,040 --> 00:15:42,490
interact with the information if you're

00:15:40,900 --> 00:15:44,800
building a small number of products that

00:15:42,490 --> 00:15:46,420
are specific to individual websites you

00:15:44,800 --> 00:15:48,730
could build custom tools that use the

00:15:46,420 --> 00:15:50,260
structure of the raw data or if you need

00:15:48,730 --> 00:15:51,970
to provide interpretability of the data

00:15:50,260 --> 00:15:53,920
of preserving the raw naming conventions

00:15:51,970 --> 00:15:55,300
will definitely help analysts compare

00:15:53,920 --> 00:15:57,640
the broad data sets to what you

00:15:55,300 --> 00:15:58,750
alternately produce and we've also

00:15:57,640 --> 00:16:01,540
learned a lot about building data

00:15:58,750 --> 00:16:03,339
products for users we often need to make

00:16:01,540 --> 00:16:05,650
decisions about what data is suitable

00:16:03,339 --> 00:16:07,510
for different use cases if the data will

00:16:05,650 --> 00:16:09,370
be used for operational decision-making

00:16:07,510 --> 00:16:11,920
such as monitoring the civil legal

00:16:09,370 --> 00:16:13,800
fallout after the Kovach pandemic then

00:16:11,920 --> 00:16:16,480
having updated and accurate information

00:16:13,800 --> 00:16:18,580
readily available is important and the

00:16:16,480 --> 00:16:21,060
structure of your architecture and the

00:16:18,580 --> 00:16:23,470
data model that you produce will follow

00:16:21,060 --> 00:16:25,150
so I want to thank you all for your

00:16:23,470 --> 00:16:26,830
attention and I want to thank the CSB

00:16:25,150 --> 00:16:28,480
kampf for putting on a brilliant

00:16:26,830 --> 00:16:31,060
conference during these difficult times

00:16:28,480 --> 00:16:32,830
and as I mentioned we are very open to

00:16:31,060 --> 00:16:35,110
learning more about how this data could

00:16:32,830 --> 00:16:36,430
be used in other contexts so please get

00:16:35,110 --> 00:16:38,920
in touch with me if you're interested in

00:16:36,430 --> 00:16:42,459
collaboration I can be reached at on my

00:16:38,920 --> 00:16:45,310
email or on Twitter thank you all yeah

00:16:42,459 --> 00:16:47,110
thank you it's really great um I was

00:16:45,310 --> 00:16:50,200
just slacking with other organizers that

00:16:47,110 --> 00:16:53,650
I I get teary-eyed when I see these kind

00:16:50,200 --> 00:16:55,600
of like grabbing all the these this

00:16:53,650 --> 00:16:57,400
information Civic information off of all

00:16:55,600 --> 00:16:59,560
these websites and then normalizing it

00:16:57,400 --> 00:17:01,959
and thinking it through it's just like

00:16:59,560 --> 00:17:03,940
amazing how much powers about is as as

00:17:01,959 --> 00:17:05,170
as out there that is not being leveraged

00:17:03,940 --> 00:17:06,760
so I mean the work that all these

00:17:05,170 --> 00:17:08,980
different projects including yours is

00:17:06,760 --> 00:17:12,579
doing they just unlock this is amazing

00:17:08,980 --> 00:17:15,730
so we are running rate on time so I'm

00:17:12,579 --> 00:17:20,079
going to say that the questions can be

00:17:15,730 --> 00:17:23,939
moved over to slack and thank you very

00:17:20,079 --> 00:17:23,939

YouTube URL: https://www.youtube.com/watch?v=yfRR7YGRQ1c


