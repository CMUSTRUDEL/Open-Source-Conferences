Title: Hao Ye - Accessibility and reproducibility in ecological time series analysis
Publication date: 2020-05-28
Playlist: CSVConf 2020
Description: 
	Although many scientific datasets are now shared openly and there are numerous tools for reproducible research, many barriers remain. Datasets are diversely structured and dispersedly situated; most domain scientists lack formal training in data management, software development, or reproducible research; and learning these skills or assembling a skilled team requires a large investment of time and/or resources. To resolve these challenges, we built MATSS (Macroecological Analysis of Time Series Structure), an R package that provides access to over 80,000 ecological time series in a standardized format and promotes best practices in computational research through reproducible & shareable templates of full workflows bundled as research compendia ~ Marwick et al. 2018.

--
csv,conf,v5 is a community conference for data makers everywhere, featuring stories about data sharing and data analysis from science, journalism, government, and open source.  Held May 13-14, 2020, Online. https://csvconf.com/
Captions: 
	00:00:00,030 --> 00:00:05,819
Thanks so hi I'm Calle my partner's

00:00:03,389 --> 00:00:07,620
Archy him and I'm currently at the

00:00:05,819 --> 00:00:10,469
reproducibility librarian at the

00:00:07,620 --> 00:00:13,320
University of Florida I'm here today to

00:00:10,469 --> 00:00:17,340
talk to you about a project that I

00:00:13,320 --> 00:00:20,250
worked on in my former job as a

00:00:17,340 --> 00:00:22,439
postdoctoral researcher on improving

00:00:20,250 --> 00:00:25,890
accessibility and reproducibility in

00:00:22,439 --> 00:00:28,920
ecological time-series analysis so

00:00:25,890 --> 00:00:31,800
although this conference is virtual I

00:00:28,920 --> 00:00:34,050
still want to acknowledge that the

00:00:31,800 --> 00:00:35,489
University of Florida occupies land that

00:00:34,050 --> 00:00:37,410
is the territory of many nations

00:00:35,489 --> 00:00:37,920
including the Seminole and make you a

00:00:37,410 --> 00:00:40,980
peoples

00:00:37,920 --> 00:00:42,950
and furthermore the the project I'm

00:00:40,980 --> 00:00:46,770
going to talk to you about involves

00:00:42,950 --> 00:00:49,500
collections of ecological data and we

00:00:46,770 --> 00:00:52,170
haven't gone through and looked at where

00:00:49,500 --> 00:00:53,640
all this data was sourced from but and

00:00:52,170 --> 00:00:57,449
most likely includes at least a few

00:00:53,640 --> 00:01:00,809
examples of what has now been termed

00:00:57,449 --> 00:01:02,550
parachute research which describes the

00:01:00,809 --> 00:01:04,619
scientists who basically go into a

00:01:02,550 --> 00:01:06,689
community and collect data about or

00:01:04,619 --> 00:01:08,549
within the community in a way that

00:01:06,689 --> 00:01:12,150
benefits the academic in their career

00:01:08,549 --> 00:01:16,380
and not always involving the community

00:01:12,150 --> 00:01:19,670
or trying to understand how there are

00:01:16,380 --> 00:01:22,560
interests Mike along with the research

00:01:19,670 --> 00:01:25,890
so to give you some context about this

00:01:22,560 --> 00:01:28,680
talk so like I said in my previous life

00:01:25,890 --> 00:01:31,259
less than two weeks ago I was a

00:01:28,680 --> 00:01:34,770
computational ecologist and my training

00:01:31,259 --> 00:01:36,509
is in time series analysis developing

00:01:34,770 --> 00:01:39,450
packages software packages and the

00:01:36,509 --> 00:01:41,369
programming language are and working on

00:01:39,450 --> 00:01:44,790
research and dynamic systems and chaos

00:01:41,369 --> 00:01:47,549
theory and so the projects I'm going to

00:01:44,790 --> 00:01:50,009
talk to you about we are calling mats

00:01:47,549 --> 00:01:53,579
which stands for the macro ecological

00:01:50,009 --> 00:01:55,950
analysis of time-series structure it's a

00:01:53,579 --> 00:01:59,310
software package that currently exists

00:01:55,950 --> 00:02:05,280
on github and has a nicely render

00:01:59,310 --> 00:02:08,810
website using package down and mats

00:02:05,280 --> 00:02:11,910
the backs team involves several

00:02:08,810 --> 00:02:13,180
different members of the lab including

00:02:11,910 --> 00:02:16,269
both of the

00:02:13,180 --> 00:02:19,599
ethan white and morgan earnest as well

00:02:16,269 --> 00:02:22,659
as grad students for non ideas and ellen

00:02:19,599 --> 00:02:26,799
blood so our lab manager Glenda Yanni

00:02:22,659 --> 00:02:29,950
and statistician juniper Simonis and so

00:02:26,799 --> 00:02:31,840
we have a lot of different people in the

00:02:29,950 --> 00:02:34,269
in this lab but we called you lab at the

00:02:31,840 --> 00:02:36,310
University of Florida with lots of

00:02:34,269 --> 00:02:38,760
expertise across different domains so

00:02:36,310 --> 00:02:41,530
that includes software development

00:02:38,760 --> 00:02:44,439
statistics field ecology and population

00:02:41,530 --> 00:02:47,590
and community ecology and so in general

00:02:44,439 --> 00:02:48,849
our lab is interested in lots of big

00:02:47,590 --> 00:02:52,030
ecological questions

00:02:48,849 --> 00:02:54,040
some of them include things like how

00:02:52,030 --> 00:02:56,260
accurately can we predict population

00:02:54,040 --> 00:02:59,829
changes how far into the future can we

00:02:56,260 --> 00:03:03,099
make those predictions what ecosystem

00:02:59,829 --> 00:03:05,470
properties are associated with those

00:03:03,099 --> 00:03:07,060
changes in abundance or maybe our

00:03:05,470 --> 00:03:09,730
drivers of those changes in abundance

00:03:07,060 --> 00:03:11,980
what kinds of changes occur in the whole

00:03:09,730 --> 00:03:14,590
ecosystem communities and are those

00:03:11,980 --> 00:03:17,049
changes in the community merely the sum

00:03:14,590 --> 00:03:18,459
of individual population dynamics or our

00:03:17,049 --> 00:03:20,980
interactions between different

00:03:18,459 --> 00:03:24,129
populations in an ecosystem important

00:03:20,980 --> 00:03:27,519
for directing that community level

00:03:24,129 --> 00:03:29,799
change and so I feel like I should

00:03:27,519 --> 00:03:32,799
probably also mention right now at the

00:03:29,799 --> 00:03:34,150
exact same time there is a meeting a

00:03:32,799 --> 00:03:36,970
virtual meeting of the ecological

00:03:34,150 --> 00:03:38,220
forecasting initiative and so we

00:03:36,970 --> 00:03:40,989
actually have team members that are

00:03:38,220 --> 00:03:42,489
split some of them I think might be in

00:03:40,989 --> 00:03:43,930
the audience for this talk and some of

00:03:42,489 --> 00:03:47,620
them might be in that virtual meeting as

00:03:43,930 --> 00:03:50,500
well so to kind of kind of summarize our

00:03:47,620 --> 00:03:52,030
like perspective on research and kind of

00:03:50,500 --> 00:03:54,790
what we're interested in doing we

00:03:52,030 --> 00:03:58,720
basically want to do all the analyses on

00:03:54,790 --> 00:04:01,629
all the time series and so in order to

00:03:58,720 --> 00:04:05,139
really try and approach that kind of

00:04:01,629 --> 00:04:08,199
work we really have to focus on data

00:04:05,139 --> 00:04:10,690
analysis in a systemic way and so we do

00:04:08,199 --> 00:04:13,510
this by gathering open ecological

00:04:10,690 --> 00:04:16,120
datasets tidying them and ensuring that

00:04:13,510 --> 00:04:20,409
there's consistent metadata about things

00:04:16,120 --> 00:04:24,460
like location and the taxonomic ID of

00:04:20,409 --> 00:04:26,270
species and populations we use diverse

00:04:24,460 --> 00:04:28,069
modeling approaches

00:04:26,270 --> 00:04:29,830
population dynamics models as well as

00:04:28,069 --> 00:04:32,120
machine learning and statistical models

00:04:29,830 --> 00:04:36,229
and then we also write software for

00:04:32,120 --> 00:04:38,300
reproducible analyses and again this is

00:04:36,229 --> 00:04:40,970
something that is achievable because in

00:04:38,300 --> 00:04:43,069
the we ecology lab we have a lot of

00:04:40,970 --> 00:04:46,370
different folks with diverse backgrounds

00:04:43,069 --> 00:04:49,310
and a depth of skill and experience

00:04:46,370 --> 00:04:51,979
so among the like seven members of this

00:04:49,310 --> 00:04:54,919
project including myself five of us have

00:04:51,979 --> 00:04:57,949
PhDs and I have to admit that this is

00:04:54,919 --> 00:05:01,419
not actually the typical situation for

00:04:57,949 --> 00:05:04,340
an ecologist or an ecological researcher

00:05:01,419 --> 00:05:06,979
so for example suppose you are an

00:05:04,340 --> 00:05:09,400
ecologist and you are interested in

00:05:06,979 --> 00:05:11,720
testing some of your hypotheses on

00:05:09,400 --> 00:05:13,460
ecological populations and how they

00:05:11,720 --> 00:05:15,560
might be changing in time and you know

00:05:13,460 --> 00:05:17,720
there's a lot of open data out there

00:05:15,560 --> 00:05:19,520
that might be used and certainly this is

00:05:17,720 --> 00:05:22,490
definitely the experience that many are

00:05:19,520 --> 00:05:24,259
going through now given that kovat is

00:05:22,490 --> 00:05:26,300
restricting field seasons and other

00:05:24,259 --> 00:05:27,530
kinds of research activities a lot of

00:05:26,300 --> 00:05:30,680
folks are more interested in doing

00:05:27,530 --> 00:05:33,740
computational research so you have an

00:05:30,680 --> 00:05:36,349
idea and then you like browse Twitter

00:05:33,740 --> 00:05:38,479
and based on the recommendation of the

00:05:36,349 --> 00:05:40,490
our stats people you decide that there's

00:05:38,479 --> 00:05:42,919
this nice book or for data science that

00:05:40,490 --> 00:05:44,990
is going to get you started and right in

00:05:42,919 --> 00:05:47,060
the middle of chapter one you see this

00:05:44,990 --> 00:05:49,130
nice workflow diagram that describes how

00:05:47,060 --> 00:05:51,500
you're going to accomplish your analysis

00:05:49,130 --> 00:05:54,349
right and so that seems pretty

00:05:51,500 --> 00:05:56,389
straightforward of course it's a it is a

00:05:54,349 --> 00:05:58,039
workflow diagram is this kind of like

00:05:56,389 --> 00:06:00,409
flow chart that is intentionally a

00:05:58,039 --> 00:06:02,419
simplistic model of how to approach data

00:06:00,409 --> 00:06:04,849
analysis and so once you start getting

00:06:02,419 --> 00:06:06,199
into the weeds of doing the research you

00:06:04,849 --> 00:06:08,479
realize that there's actually a lot of

00:06:06,199 --> 00:06:10,219
skills that are involved in trying to do

00:06:08,479 --> 00:06:13,460
this research right you have to figure

00:06:10,219 --> 00:06:14,599
out how to obtain and import the data if

00:06:13,460 --> 00:06:17,060
you're getting data from different

00:06:14,599 --> 00:06:20,509
sources you often have to figure out how

00:06:17,060 --> 00:06:22,550
to get them to you know be consistently

00:06:20,509 --> 00:06:24,590
formatted you might have to change how

00:06:22,550 --> 00:06:28,159
dates are structured in the data there

00:06:24,590 --> 00:06:31,400
might be errors and the identification

00:06:28,159 --> 00:06:32,900
of species other you need to fix then

00:06:31,400 --> 00:06:35,449
you'll have to you know come up with

00:06:32,900 --> 00:06:36,949
your statistical model that addresses

00:06:35,449 --> 00:06:39,470
your research question you have to learn

00:06:36,949 --> 00:06:41,240
programming to code it up

00:06:39,470 --> 00:06:43,610
and then you know kind of after you've

00:06:41,240 --> 00:06:45,710
gone through like all of these steps you

00:06:43,610 --> 00:06:48,410
might then you know learn about this

00:06:45,710 --> 00:06:50,150
notion called reproducibility and all of

00:06:48,410 --> 00:06:51,500
this literature about how research is

00:06:50,150 --> 00:06:53,030
not reproducible and there are these

00:06:51,500 --> 00:06:55,670
best practices out there that you have

00:06:53,030 --> 00:06:59,000
to follow right and so this is all kind

00:06:55,670 --> 00:07:02,570
of like large daunting challenges if you

00:06:59,000 --> 00:07:04,730
are trained as an ecologist but not as a

00:07:02,570 --> 00:07:07,820
statistician or a data scientist or a

00:07:04,730 --> 00:07:10,940
software developer or a philosopher on

00:07:07,820 --> 00:07:14,300
you know how scientific knowledge gets

00:07:10,940 --> 00:07:17,060
generated and so as a team we thought

00:07:14,300 --> 00:07:18,770
about how we can build a project to

00:07:17,060 --> 00:07:21,080
reduce some of these barriers for a

00:07:18,770 --> 00:07:22,910
reproducible data analysis and focusing

00:07:21,080 --> 00:07:25,040
specifically in the areas that we have

00:07:22,910 --> 00:07:28,400
expertise which is working on ecological

00:07:25,040 --> 00:07:31,130
time-series and so what we came up with

00:07:28,400 --> 00:07:33,740
with max is set up an infrastructure

00:07:31,130 --> 00:07:35,780
that basically has functionality to

00:07:33,740 --> 00:07:37,790
accomplish all those steps in the data

00:07:35,780 --> 00:07:41,990
analysis and make them as easy as

00:07:37,790 --> 00:07:43,970
possible for researchers to do and so it

00:07:41,990 --> 00:07:45,710
has some technology for example to allow

00:07:43,970 --> 00:07:47,990
you to obtain ecological time series

00:07:45,710 --> 00:07:49,760
data we have written code that

00:07:47,990 --> 00:07:55,100
transforms all that data into a common

00:07:49,760 --> 00:07:58,160
format if template workflows so that you

00:07:55,100 --> 00:08:00,680
can take an analysis on one data set and

00:07:58,160 --> 00:08:02,930
basically repeat it all the data sets

00:08:00,680 --> 00:08:05,030
that we make available to you so you can

00:08:02,930 --> 00:08:07,700
conduct that large-scale comparison and

00:08:05,030 --> 00:08:09,830
then finally we have functionality for

00:08:07,700 --> 00:08:12,590
generating reproducible reports and

00:08:09,830 --> 00:08:15,860
sharing your code with other researchers

00:08:12,590 --> 00:08:19,850
and so the way that that kind of works

00:08:15,860 --> 00:08:24,250
in like a software level we build off of

00:08:19,850 --> 00:08:26,210
an existing software or existing data

00:08:24,250 --> 00:08:28,340
collections manager called the data

00:08:26,210 --> 00:08:31,040
retriever that obtains datasets from

00:08:28,340 --> 00:08:33,320
different places we build off of the

00:08:31,040 --> 00:08:34,909
Thai diverse set of our packages for

00:08:33,320 --> 00:08:37,550
transforming data sets into a common

00:08:34,909 --> 00:08:41,240
format and deal with irregular or

00:08:37,550 --> 00:08:43,880
missing samples in the data we use the

00:08:41,240 --> 00:08:46,250
Drake workflow package to help organize

00:08:43,880 --> 00:08:49,370
all the analyses so that when you are

00:08:46,250 --> 00:08:52,800
applying your statistical model to a lot

00:08:49,370 --> 00:08:55,110
of time series I it does it in a way

00:08:52,800 --> 00:08:57,540
you know really organizes all those

00:08:55,110 --> 00:09:01,470
different sets of results in a coherent

00:08:57,540 --> 00:09:04,050
way and then we build off of templates

00:09:01,470 --> 00:09:07,350
like those in the use of this package to

00:09:04,050 --> 00:09:09,149
provide research compendium template to

00:09:07,350 --> 00:09:14,009
enhance reproducibility and make it

00:09:09,149 --> 00:09:16,079
really easy to share your work so some

00:09:14,009 --> 00:09:18,180
of the data sets that we have linked to

00:09:16,079 --> 00:09:20,519
with our project include the North

00:09:18,180 --> 00:09:22,740
American breeding bird survey the global

00:09:20,519 --> 00:09:24,810
population dynamics database the bio

00:09:22,740 --> 00:09:27,329
time database as well as ten

00:09:24,810 --> 00:09:29,250
individually curated data sets and

00:09:27,329 --> 00:09:32,209
totaling over three hundred thousand

00:09:29,250 --> 00:09:35,639
time series I don't actually know the

00:09:32,209 --> 00:09:37,920
number of data points given that each

00:09:35,639 --> 00:09:39,839
time series has different number of data

00:09:37,920 --> 00:09:42,360
points depending on the length of the

00:09:39,839 --> 00:09:44,160
sampling and we provide this own in this

00:09:42,360 --> 00:09:47,250
like standardized data and metadata

00:09:44,160 --> 00:09:49,079
format so that when you come when you

00:09:47,250 --> 00:09:51,750
decide on you know how you're going to

00:09:49,079 --> 00:09:53,879
do your analysis you only have to make

00:09:51,750 --> 00:09:55,860
it work for one data set and then it can

00:09:53,879 --> 00:10:01,410
be applied to all the data sets that we

00:09:55,860 --> 00:10:04,230
give you access to and furthermore the

00:10:01,410 --> 00:10:06,259
most exciting part for me is trying to

00:10:04,230 --> 00:10:09,750
come up with a way to promote

00:10:06,259 --> 00:10:12,689
reproducible workflows and so having had

00:10:09,750 --> 00:10:15,269
many years of experience teaching in our

00:10:12,689 --> 00:10:17,779
users groups and workshops for the

00:10:15,269 --> 00:10:20,730
carbon trees there's a lot of

00:10:17,779 --> 00:10:22,800
information about you know being an

00:10:20,730 --> 00:10:25,019
effective teacher and employing good

00:10:22,800 --> 00:10:27,660
pedagogy to teach programming and data

00:10:25,019 --> 00:10:30,209
science skills but one of the principles

00:10:27,660 --> 00:10:32,180
that you know I've kind of synthesized

00:10:30,209 --> 00:10:34,970
through all that experience is that

00:10:32,180 --> 00:10:37,319
providing good user defaults is

00:10:34,970 --> 00:10:40,380
essential for getting people started and

00:10:37,319 --> 00:10:43,170
so you can provide a lot of teaching and

00:10:40,380 --> 00:10:45,569
workshops and readings and resources for

00:10:43,170 --> 00:10:49,110
learners but if they don't have a

00:10:45,569 --> 00:10:51,029
default computational workflow then when

00:10:49,110 --> 00:10:52,920
the students try to actually do the

00:10:51,029 --> 00:10:54,930
research they end up coming back and

00:10:52,920 --> 00:10:57,779
meeting or helped to get started and so

00:10:54,930 --> 00:11:00,059
we try to do this by using Drake which

00:10:57,779 --> 00:11:02,160
is a workflow package to organize code

00:11:00,059 --> 00:11:04,470
and then we provide a research

00:11:02,160 --> 00:11:06,600
compendium to package all that up in a

00:11:04,470 --> 00:11:08,740
research

00:11:06,600 --> 00:11:10,330
so I know a lot of you may not be

00:11:08,740 --> 00:11:14,440
familiar with what a research compendium

00:11:10,330 --> 00:11:16,870
is but it's basically a bundle of code

00:11:14,440 --> 00:11:19,089
and data together in a way that makes it

00:11:16,870 --> 00:11:21,399
easy to share and reproduce the results

00:11:19,089 --> 00:11:23,890
and the way that we have constructed

00:11:21,399 --> 00:11:26,080
this for our project is to actually

00:11:23,890 --> 00:11:28,209
extend the functionality of our packages

00:11:26,080 --> 00:11:30,040
so if you've ever worked in the art

00:11:28,209 --> 00:11:32,440
programming language you know that there

00:11:30,040 --> 00:11:34,029
are all these are packages that are out

00:11:32,440 --> 00:11:36,010
there that provide additional

00:11:34,029 --> 00:11:39,089
functionality and you can install them

00:11:36,010 --> 00:11:41,170
very simply and so the idea behind the

00:11:39,089 --> 00:11:44,200
building or research compendium in the

00:11:41,170 --> 00:11:47,860
same way is that if there is a

00:11:44,200 --> 00:11:49,270
compendium that defines a particular

00:11:47,860 --> 00:11:51,279
analysis of a particular research

00:11:49,270 --> 00:11:54,339
project you should be able to install

00:11:51,279 --> 00:11:56,470
back as easily as our package get all

00:11:54,339 --> 00:12:01,360
the data and the code and be able to

00:11:56,470 --> 00:12:03,670
reproduce the results so we try to make

00:12:01,360 --> 00:12:06,190
this as simple as possible so I'm going

00:12:03,670 --> 00:12:08,860
to kind of guide you through how we have

00:12:06,190 --> 00:12:11,380
thought about this so this is the only

00:12:08,860 --> 00:12:13,779
graph I have in the talk and so I have

00:12:11,380 --> 00:12:16,060
along the x-axis the ease of use and the

00:12:13,779 --> 00:12:21,279
y-axis capabilities of different kinds

00:12:16,060 --> 00:12:23,410
of tools and so one extreme you will

00:12:21,279 --> 00:12:25,480
have something like the Keurig one

00:12:23,410 --> 00:12:27,550
push-button coffee machine or you push

00:12:25,480 --> 00:12:29,589
the button and it makes coffee it's

00:12:27,550 --> 00:12:32,529
really easy to use it does one simple

00:12:29,589 --> 00:12:33,940
task really easily at the other extreme

00:12:32,529 --> 00:12:35,650
you have these programming languages

00:12:33,940 --> 00:12:37,600
which can do a lot of different things

00:12:35,650 --> 00:12:39,220
but there's a steep learning curve to

00:12:37,600 --> 00:12:39,820
learning how to use those tools and get

00:12:39,220 --> 00:12:43,690
started

00:12:39,820 --> 00:12:45,100
I put Microsoft Excel over here it has a

00:12:43,690 --> 00:12:46,900
little bit more functionality than the

00:12:45,100 --> 00:12:48,339
coffee machine but in many cases if

00:12:46,900 --> 00:12:50,170
you're doing the statistical analysis

00:12:48,339 --> 00:12:52,150
you're still pushing like a few

00:12:50,170 --> 00:12:56,200
keystrokes or a few buttons to run a

00:12:52,150 --> 00:12:58,750
very specific statistical test and then

00:12:56,200 --> 00:13:02,829
of course I have to put the conference

00:12:58,750 --> 00:13:05,800
travel reimbursement as very difficult

00:13:02,829 --> 00:13:08,950
to use and only performs one very simple

00:13:05,800 --> 00:13:11,140
function and so we have designed Matz

00:13:08,950 --> 00:13:13,270
with the intention that trying to

00:13:11,140 --> 00:13:15,459
produce these research compendium that

00:13:13,270 --> 00:13:17,740
do these large-scale data analyses as

00:13:15,459 --> 00:13:20,410
following somewhere in this

00:13:17,740 --> 00:13:23,440
area of the space right - so that it has

00:13:20,410 --> 00:13:25,660
as much capability as those programming

00:13:23,440 --> 00:13:28,540
languages but also as easy to use as

00:13:25,660 --> 00:13:30,490
possible so that basically with one push

00:13:28,540 --> 00:13:33,490
of a button you have accomplished as you

00:13:30,490 --> 00:13:37,720
know as much as the analysis as we can

00:13:33,490 --> 00:13:39,160
reliably automate for you and so we do

00:13:37,720 --> 00:13:40,720
that by providing you with the

00:13:39,160 --> 00:13:42,490
functionality where with one line of

00:13:40,720 --> 00:13:47,320
code you can create a research

00:13:42,490 --> 00:13:49,150
compendium and in fact it's so simple

00:13:47,320 --> 00:13:52,750
that we have actually built in scripts

00:13:49,150 --> 00:13:54,790
to automate the example of it so I'm

00:13:52,750 --> 00:13:59,080
going to go through and click on this to

00:13:54,790 --> 00:14:01,000
show you now so we structured the master

00:13:59,080 --> 00:14:06,280
project as the software package on

00:14:01,000 --> 00:14:09,220
github and we have automated the testing

00:14:06,280 --> 00:14:11,470
of it so that every time we make changes

00:14:09,220 --> 00:14:14,290
to the software package we automatically

00:14:11,470 --> 00:14:16,570
generate a new example of the research

00:14:14,290 --> 00:14:18,940
compendium that by default gets created

00:14:16,570 --> 00:14:21,390
when you run that line of code and so

00:14:18,940 --> 00:14:23,920
what you're saying here is that

00:14:21,390 --> 00:14:26,470
automated example that exists on github

00:14:23,920 --> 00:14:28,870
of what that sample research companion

00:14:26,470 --> 00:14:31,840
looks like so it in fact is an our

00:14:28,870 --> 00:14:34,870
package that you can download from

00:14:31,840 --> 00:14:39,400
github directly and contained within it

00:14:34,870 --> 00:14:43,480
you'll see an analysis folder that has a

00:14:39,400 --> 00:14:45,400
templated report that shows you example

00:14:43,480 --> 00:14:48,040
analysis apply to some of the sample

00:14:45,400 --> 00:14:50,410
data sets that we have included and so

00:14:48,040 --> 00:14:52,840
this is done in our markdown which will

00:14:50,410 --> 00:14:55,990
be the topic of a later talk I think in

00:14:52,840 --> 00:14:57,160
this conference but you can see it you

00:14:55,990 --> 00:15:00,120
know it goes through the instructions of

00:14:57,160 --> 00:15:02,770
exactly how to read in the results and

00:15:00,120 --> 00:15:05,530
process the results and then generate

00:15:02,770 --> 00:15:08,980
plots of some of the outputs for this

00:15:05,530 --> 00:15:10,410
particular example analysis and then of

00:15:08,980 --> 00:15:13,600
course because we care a lot about

00:15:10,410 --> 00:15:15,790
assigning credit appropriately we also

00:15:13,600 --> 00:15:17,590
make sure that all the data sets that

00:15:15,790 --> 00:15:19,450
get used in this analysis report

00:15:17,590 --> 00:15:21,640
automatically get added in a citations

00:15:19,450 --> 00:15:24,930
to this report um as well as references

00:15:21,640 --> 00:15:30,760
to this the math software package that

00:15:24,930 --> 00:15:34,240
generated this example analysis

00:15:30,760 --> 00:15:37,570
back to my talk ok so we have that

00:15:34,240 --> 00:15:39,510
automated example and you can go look

00:15:37,570 --> 00:15:43,090
through and see what that looks like and

00:15:39,510 --> 00:15:45,040
so we have we are actually using maps

00:15:43,090 --> 00:15:47,200
currently in several different kinds of

00:15:45,040 --> 00:15:50,830
projects so both internal and external

00:15:47,200 --> 00:15:54,490
collaborations in the lab one example is

00:15:50,830 --> 00:15:55,900
to explicitly study forecasting and what

00:15:54,490 --> 00:15:58,060
influences the forecast skill of

00:15:55,900 --> 00:16:00,970
populations on testing different

00:15:58,060 --> 00:16:02,410
hypotheses about weather like history

00:16:00,970 --> 00:16:04,800
characteristics or time series

00:16:02,410 --> 00:16:07,240
properties or environmental covariance

00:16:04,800 --> 00:16:09,310
influence the forecast skill with the

00:16:07,240 --> 00:16:10,720
idea that we want to produce general

00:16:09,310 --> 00:16:14,980
guidance on how to choose a good

00:16:10,720 --> 00:16:17,980
forecasting method a second project is

00:16:14,980 --> 00:16:20,740
what we call mass L bats which applies

00:16:17,980 --> 00:16:23,500
this particular statistical model the

00:16:20,740 --> 00:16:25,690
latent dirichlet allocation and basing

00:16:23,500 --> 00:16:26,950
time series analysis to identifying

00:16:25,690 --> 00:16:30,550
quantified different currents of

00:16:26,950 --> 00:16:32,410
community change to test weather across

00:16:30,550 --> 00:16:34,600
all the different ecological data sets

00:16:32,410 --> 00:16:36,960
that we have what kind of patterns of

00:16:34,600 --> 00:16:41,020
change we see in those whole communities

00:16:36,960 --> 00:16:42,880
and send this sum up I've talked a lot

00:16:41,020 --> 00:16:44,710
about how there are these barriers for

00:16:42,880 --> 00:16:46,380
large-scale computational research they

00:16:44,710 --> 00:16:50,170
aren't all going to go away immediately

00:16:46,380 --> 00:16:52,990
but we think that you know we our

00:16:50,170 --> 00:16:54,910
project here is a prototype example of

00:16:52,990 --> 00:16:56,770
how they can be addressed at least

00:16:54,910 --> 00:16:58,720
partially through the use of tools and

00:16:56,770 --> 00:17:01,000
technology doing this work of course

00:16:58,720 --> 00:17:03,280
does have upfront costs because while

00:17:01,000 --> 00:17:05,230
you are creating these tools you are

00:17:03,280 --> 00:17:07,510
taking time away from doing the research

00:17:05,230 --> 00:17:09,990
yourself and so it's important that we

00:17:07,510 --> 00:17:12,100
get funding and support for this from

00:17:09,990 --> 00:17:13,900
funding agencies who are thankful that

00:17:12,100 --> 00:17:16,600
like the NSF and the Moore Foundation

00:17:13,900 --> 00:17:18,190
has helped to fund this work but we hope

00:17:16,600 --> 00:17:19,750
that the you know the results of this

00:17:18,190 --> 00:17:23,410
project has been end up being a force

00:17:19,750 --> 00:17:25,180
multiplier for research and help lots of

00:17:23,410 --> 00:17:27,430
other researchers out there do really

00:17:25,180 --> 00:17:30,700
cool things with these datasets thank

00:17:27,430 --> 00:17:33,580
you great thank you yeah we got several

00:17:30,700 --> 00:17:35,200
questions and I guess I would just jump

00:17:33,580 --> 00:17:37,150
in and say thank you to the Moore

00:17:35,200 --> 00:17:40,270
Foundation for supporting CSV cough as

00:17:37,150 --> 00:17:44,440
well it's always nice to thank our

00:17:40,270 --> 00:17:44,660
funders I had the question I think we

00:17:44,440 --> 00:17:50,930
have

00:17:44,660 --> 00:17:52,550
time for one is can how can this type of

00:17:50,930 --> 00:17:56,090
work be better supported and sent

00:17:52,550 --> 00:17:57,620
advised and scaled up do you have any

00:17:56,090 --> 00:18:01,970
kind of thoughts about that that you

00:17:57,620 --> 00:18:03,860
would that is a great question I would

00:18:01,970 --> 00:18:07,310
say that is something that I am thinking

00:18:03,860 --> 00:18:09,650
about a lot more and my new position as

00:18:07,310 --> 00:18:11,660
the reproducibility librarian I'm really

00:18:09,650 --> 00:18:13,580
interested in how we can make start full

00:18:11,660 --> 00:18:17,030
stomach change in academia to support

00:18:13,580 --> 00:18:18,110
these kinds of efforts I think it might

00:18:17,030 --> 00:18:20,660
have been Vicki

00:18:18,110 --> 00:18:23,360
Steve's actually who mentioned earlier

00:18:20,660 --> 00:18:25,430
that a lot of these kinds of work this

00:18:23,360 --> 00:18:28,370
like support work is all you know

00:18:25,430 --> 00:18:31,820
gendered and valued less than for for

00:18:28,370 --> 00:18:34,580
example like the you know Sloane genius

00:18:31,820 --> 00:18:37,390
or the innovator and so I think it's you

00:18:34,580 --> 00:18:39,800
know we really have to be talking with

00:18:37,390 --> 00:18:41,480
you know like funding agencies and

00:18:39,800 --> 00:18:44,120
publishers and making sure that there

00:18:41,480 --> 00:18:46,880
are incentives and acknowledgement and

00:18:44,120 --> 00:18:50,960
recognition of these efforts in order to

00:18:46,880 --> 00:18:53,350
really promote them yeah okay well thank

00:18:50,960 --> 00:18:53,350

YouTube URL: https://www.youtube.com/watch?v=FP2qIG0tXkU


