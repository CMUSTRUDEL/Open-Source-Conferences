Title: Monica Granados & Lily Zhao - How Frictionless Data can help you grease your data
Publication date: 2020-05-28
Playlist: CSVConf 2020
Description: 
	I think we have all been subject to other peopleâ€™s data - the frustration and the disappointment that follows when we determine that the data is unusable. The Frictionless Data initiative at Open Knowledge Foundation aims to reduce friction in working with data, with a goal to make it effortless to transport data among different tools and platforms for further analysis, and with an emphasis on reproducible research and open data. As inaugural Reproducible Research Fellows of the program, we will demonstrate how we have applied the principles and tools of Frictionless Data to our own research  data on the octopus trade and open access bibliometric data to make our data more reusable by others. This talk is aimed at all data wranglers, and along the way we will talk about our experience in the fellowship, what were some difficulties, and about our triumphs. 

--
csv,conf,v5 is a community conference for data makers everywhere, featuring stories about data sharing and data analysis from science, journalism, government, and open source.  Held May 13-14, 2020, Online. https://csvconf.com/
Captions: 
	00:00:00,030 --> 00:00:05,549
okay hi everyone my name is Monika

00:00:03,600 --> 00:00:09,170
granado's I am one of the inaugural

00:00:05,549 --> 00:00:12,500
fellows of the reproducible research

00:00:09,170 --> 00:00:15,750
fellowship from the frictionless data

00:00:12,500 --> 00:00:20,130
program as that is part of the Open

00:00:15,750 --> 00:00:22,260
Knowledge Foundation I'm also a policy

00:00:20,130 --> 00:00:24,810
analyst for the Government of Canada as

00:00:22,260 --> 00:00:27,720
well as serving on the leadership team

00:00:24,810 --> 00:00:30,810
of pre-review Lilly you like to

00:00:27,720 --> 00:00:33,840
introduce yourself hi everyone I'm also

00:00:30,810 --> 00:00:36,329
a fellow with Monika and I'm a marine

00:00:33,840 --> 00:00:40,350
science PhD student at UC Santa Barbara

00:00:36,329 --> 00:00:41,790
in California and I studied how coral

00:00:40,350 --> 00:00:43,649
reefs and coastal communities are

00:00:41,790 --> 00:00:46,190
affected by a global environmental

00:00:43,649 --> 00:00:48,960
change and they see reproduce the

00:00:46,190 --> 00:00:51,690
research reproducibility and open data

00:00:48,960 --> 00:00:53,899
science as a tool for helping us to

00:00:51,690 --> 00:00:56,629
speed up the ability to appliances to

00:00:53,899 --> 00:01:01,559
improve resilience in these systems and

00:00:56,629 --> 00:01:02,010
understand issues faster great thanks

00:01:01,559 --> 00:01:04,470
Lily

00:01:02,010 --> 00:01:07,140
so the two of us are frictionless

00:01:04,470 --> 00:01:09,869
fellows and we wanted to tell you a

00:01:07,140 --> 00:01:12,630
little bit about what our experience was

00:01:09,869 --> 00:01:16,200
like as part of the inaugural cohort and

00:01:12,630 --> 00:01:17,640
tell you a little bit about two really

00:01:16,200 --> 00:01:19,650
important tools that we learned about

00:01:17,640 --> 00:01:24,180
during this fellowship and how we've

00:01:19,650 --> 00:01:28,619
applied it in our own work so when we do

00:01:24,180 --> 00:01:31,770
science like lily and I we do a lot of

00:01:28,619 --> 00:01:34,710
data collection so I'm a trained food

00:01:31,770 --> 00:01:38,009
web ecologist I go out and collect data

00:01:34,710 --> 00:01:42,299
on crayfish Lily husk has collected some

00:01:38,009 --> 00:01:46,409
data on the octopus trade and that data

00:01:42,299 --> 00:01:49,049
gets put into some form of data entry so

00:01:46,409 --> 00:01:52,110
at least in the ecological field a lot

00:01:49,049 --> 00:01:56,939
of us just use spreadsheets like Excel

00:01:52,110 --> 00:01:59,810
that output CSV files when we start to

00:01:56,939 --> 00:02:02,909
talk about our data whether it's in a

00:01:59,810 --> 00:02:05,579
publication or maybe we put out tweet

00:02:02,909 --> 00:02:08,429
about it we might get a lot of interest

00:02:05,579 --> 00:02:11,310
about the data no people will say well

00:02:08,429 --> 00:02:12,750
I'd like to use your data or part of

00:02:11,310 --> 00:02:14,040
your data wouldn't would make a great

00:02:12,750 --> 00:02:17,040
addition to a meta-analysis

00:02:14,040 --> 00:02:18,900
that we're doing okay so we get people

00:02:17,040 --> 00:02:21,900
who are interested in our data how could

00:02:18,900 --> 00:02:25,349
we share that data so there's been many

00:02:21,900 --> 00:02:27,629
ways that we can share the data and ways

00:02:25,349 --> 00:02:30,810
that we share data in sort of the

00:02:27,629 --> 00:02:33,870
history of science and scientific

00:02:30,810 --> 00:02:36,989
publishing we could you know you could

00:02:33,870 --> 00:02:41,269
send it by carrier pigeon or by snail

00:02:36,989 --> 00:02:44,760
mail or by the Pony Express or by

00:02:41,269 --> 00:02:48,409
computer email but regardless of the way

00:02:44,760 --> 00:02:51,930
that you send that information

00:02:48,409 --> 00:02:55,829
oftentimes you end up staring at the

00:02:51,930 --> 00:02:59,549
computer screaming I hate other people's

00:02:55,829 --> 00:03:01,919
data and that's because a lot of the

00:02:59,549 --> 00:03:04,439
data that you have if they're sort of

00:03:01,919 --> 00:03:06,209
like innate parts that you understand

00:03:04,439 --> 00:03:08,159
yourself but that fact may not

00:03:06,209 --> 00:03:10,709
necessarily mean that others are gonna

00:03:08,159 --> 00:03:13,909
understand that data so when they open

00:03:10,709 --> 00:03:16,590
that CSV file or that spreadsheet

00:03:13,909 --> 00:03:19,560
there's going to be columns that you

00:03:16,590 --> 00:03:22,109
don't understand or blanks that you

00:03:19,560 --> 00:03:26,909
don't understand or sometimes the NA

00:03:22,109 --> 00:03:29,579
srna /a or you know hash tag 99 has been

00:03:26,909 --> 00:03:32,569
one of my favorites and it leads to you

00:03:29,579 --> 00:03:35,609
know a lot of confusion and frustration

00:03:32,569 --> 00:03:38,489
so what if we could make it easier to

00:03:35,609 --> 00:03:40,709
share data what if we can make it easier

00:03:38,489 --> 00:03:43,319
to give the information that we've

00:03:40,709 --> 00:03:45,389
collected and instead of just sending in

00:03:43,319 --> 00:03:52,290
s and SV pilot files that we put some

00:03:45,389 --> 00:03:54,810
context into this data so that's the

00:03:52,290 --> 00:03:57,090
reasonable research felt program we did

00:03:54,810 --> 00:03:59,489
a series of journal clubs and seminars

00:03:57,090 --> 00:04:02,040
and blogs and we learned about two

00:03:59,489 --> 00:04:04,979
important tools that are part of the

00:04:02,040 --> 00:04:08,040
frictionless data program data packages

00:04:04,979 --> 00:04:10,379
and data validation so we're going to

00:04:08,040 --> 00:04:12,840
give you a little sneak peek into data

00:04:10,379 --> 00:04:14,129
packages and data validation and then at

00:04:12,840 --> 00:04:16,650
the end of the presentation we'll give

00:04:14,129 --> 00:04:18,930
you a link for you to learn more about

00:04:16,650 --> 00:04:24,029
these tools and how you can implement

00:04:18,930 --> 00:04:26,219
them in your own research workflow so

00:04:24,029 --> 00:04:27,630
starting with data packages okay so what

00:04:26,219 --> 00:04:31,230
are data packages

00:04:27,630 --> 00:04:33,090
I felt that as someone who is you know

00:04:31,230 --> 00:04:34,530
pretty committed to the open science

00:04:33,090 --> 00:04:38,070
movement I was doing a really good job

00:04:34,530 --> 00:04:39,930
of making my data available so my data

00:04:38,070 --> 00:04:44,100
for a particular manuscript that I

00:04:39,930 --> 00:04:46,740
published last year is all available are

00:04:44,100 --> 00:04:49,860
the codes available as well as the CSV

00:04:46,740 --> 00:04:51,450
files where the raw data is in so you

00:04:49,860 --> 00:04:52,950
could go to github and grab that

00:04:51,450 --> 00:04:54,300
information and there's even like a

00:04:52,950 --> 00:04:57,300
readme file that gives you a little bit

00:04:54,300 --> 00:04:58,740
of information about the data but the

00:04:57,300 --> 00:05:00,690
truth of the matter is if you don't

00:04:58,740 --> 00:05:02,910
really have any information about the

00:05:00,690 --> 00:05:04,950
column headings you're not gonna know

00:05:02,910 --> 00:05:06,930
what anything means you don't know about

00:05:04,950 --> 00:05:10,230
units you don't know how I collected

00:05:06,930 --> 00:05:12,840
that data and so through the program we

00:05:10,230 --> 00:05:14,550
learned about data packages and it's I'm

00:05:12,840 --> 00:05:17,630
going to I'm going to tell you a little

00:05:14,550 --> 00:05:21,270
bit about the web tool so if you go to

00:05:17,630 --> 00:05:22,620
the data package creator the URL is at

00:05:21,270 --> 00:05:25,800
the bottom of the screen there on the

00:05:22,620 --> 00:05:29,700
bottom right what you can do is upload

00:05:25,800 --> 00:05:32,370
your raw data so you can upload either

00:05:29,700 --> 00:05:35,640
the CSV file or you can upload the

00:05:32,370 --> 00:05:37,310
resource path so I can actually take the

00:05:35,640 --> 00:05:41,700
data that was already in my github

00:05:37,310 --> 00:05:44,490
upload it and then what's really neat

00:05:41,700 --> 00:05:49,050
about this tool is that it lets me give

00:05:44,490 --> 00:05:50,790
context to all of my columns basically

00:05:49,050 --> 00:05:52,260
so all of that information that I have

00:05:50,790 --> 00:05:55,770
so I've got some column headings that

00:05:52,260 --> 00:05:58,020
are de trophic species miso Kazem that

00:05:55,770 --> 00:06:01,140
mean not mean a lot to you if you just

00:05:58,020 --> 00:06:03,270
open that CSV file but through the data

00:06:01,140 --> 00:06:05,400
package creator I can give you some

00:06:03,270 --> 00:06:07,680
context and then make it easier for you

00:06:05,400 --> 00:06:11,400
to take that data and then apply it in

00:06:07,680 --> 00:06:13,980
ways that you may may find useful so

00:06:11,400 --> 00:06:17,910
here it's just a screencap of once you

00:06:13,980 --> 00:06:23,460
load the path of where your resource is

00:06:17,910 --> 00:06:25,740
it'll find your columns and then you can

00:06:23,460 --> 00:06:27,570
give information in the title descriptor

00:06:25,740 --> 00:06:30,180
and then give information about the data

00:06:27,570 --> 00:06:34,800
type so all of this is using information

00:06:30,180 --> 00:06:36,660
to to generate a schema table schema

00:06:34,800 --> 00:06:40,320
basically some information about how the

00:06:36,660 --> 00:06:41,750
data is is structured and information

00:06:40,320 --> 00:06:45,000
about the dated so

00:06:41,750 --> 00:06:47,520
so I can add information about so what

00:06:45,000 --> 00:06:49,620
did you mean by experimental day or by

00:06:47,520 --> 00:06:51,300
day oh it's the experimental day and

00:06:49,620 --> 00:06:53,250
then I can give information about how

00:06:51,300 --> 00:06:55,200
long that experiment ran for example and

00:06:53,250 --> 00:06:59,760
then I can tell you information about

00:06:55,200 --> 00:07:01,890
like the the data the data itself so

00:06:59,760 --> 00:07:03,990
what this does is like once you've

00:07:01,890 --> 00:07:06,780
inputted all of that information you can

00:07:03,990 --> 00:07:10,710
then download the data package as a JSON

00:07:06,780 --> 00:07:12,810
file and send the JSON file to your

00:07:10,710 --> 00:07:15,750
collaborators or to any interested party

00:07:12,810 --> 00:07:18,660
instead of just a CSV file with no

00:07:15,750 --> 00:07:21,720
context you can then also receive other

00:07:18,660 --> 00:07:23,370
adjacent packages and then upload them

00:07:21,720 --> 00:07:25,130
here on the data package creator and

00:07:23,370 --> 00:07:28,290
it'll you can see all the information

00:07:25,130 --> 00:07:31,470
that your collaborator has provided

00:07:28,290 --> 00:07:34,500
about the about the resource or about

00:07:31,470 --> 00:07:37,680
the data now there are other ways that

00:07:34,500 --> 00:07:41,400
you can use the data like to use data

00:07:37,680 --> 00:07:45,390
packages in a more reproducible workflow

00:07:41,400 --> 00:07:46,830
so you can you can use Python in our

00:07:45,390 --> 00:07:48,540
libraries that have been built around

00:07:46,830 --> 00:07:51,320
this as well but I just wanted to give

00:07:48,540 --> 00:07:54,930
you a little taste of the power of

00:07:51,320 --> 00:07:56,280
sharing your data as a data package I'm

00:07:54,930 --> 00:07:57,780
now going to turn it over to Lily just

00:07:56,280 --> 00:08:02,910
can tell us a little bit about good

00:07:57,780 --> 00:08:05,300
tables hi everyone so good tables is the

00:08:02,910 --> 00:08:08,130
second reproducible data science tool

00:08:05,300 --> 00:08:10,710
offered by the frictionless data program

00:08:08,130 --> 00:08:12,630
the tool was developed specifically to

00:08:10,710 --> 00:08:16,410
help with data validation and it's

00:08:12,630 --> 00:08:18,630
available both as a web web tool and

00:08:16,410 --> 00:08:20,820
then through the command line so let's

00:08:18,630 --> 00:08:24,600
walk through the web tool version and

00:08:20,820 --> 00:08:27,300
try to validate Monica's crayfish algae

00:08:24,600 --> 00:08:32,550
and snail data frame so first we have to

00:08:27,300 --> 00:08:37,320
navigate to the try good tables um web

00:08:32,550 --> 00:08:40,590
browser so the first oh great so um

00:08:37,320 --> 00:08:43,020
let's say you are using just the raw

00:08:40,590 --> 00:08:45,300
data and you can do this step before you

00:08:43,020 --> 00:08:48,240
create your data package or if you don't

00:08:45,300 --> 00:08:50,010
have a schema which is made with the

00:08:48,240 --> 00:08:52,350
data package this is still a great tool

00:08:50,010 --> 00:08:54,760
just to check for structural errors of

00:08:52,350 --> 00:09:00,940
the data frame itself so

00:08:54,760 --> 00:09:03,640
when you do this is you upload your data

00:09:00,940 --> 00:09:06,190
and check to see if a structural errors

00:09:03,640 --> 00:09:07,980
such as missing entries you can either

00:09:06,190 --> 00:09:11,050
upload your file from your local

00:09:07,980 --> 00:09:15,010
directory or you can insert the URL

00:09:11,050 --> 00:09:19,210
where your data is stored for example up

00:09:15,010 --> 00:09:21,700
there we can add Monica's raw version of

00:09:19,210 --> 00:09:23,920
her github data and then you hit that

00:09:21,700 --> 00:09:26,230
button that's in gray right now that

00:09:23,920 --> 00:09:27,910
says validate and then if there are no

00:09:26,230 --> 00:09:29,560
structural errors a new data frame

00:09:27,910 --> 00:09:35,530
you'll get a pop-up that says valid

00:09:29,560 --> 00:09:38,290
table but let's say that oh but here in

00:09:35,530 --> 00:09:40,480
this example we see that there wasn't

00:09:38,290 --> 00:09:44,260
there's actually one error structural

00:09:40,480 --> 00:09:47,680
error that we found content area that we

00:09:44,260 --> 00:09:52,390
found which is that the density column

00:09:47,680 --> 00:09:59,050
variable we had it marked in the schema

00:09:52,390 --> 00:10:01,090
file as a numeric or as an integer

00:09:59,050 --> 00:10:03,940
variable when actually it's a numeric

00:10:01,090 --> 00:10:09,100
variable so Monica kids would go back to

00:10:03,940 --> 00:10:12,580
that schema slide I'm part of the data

00:10:09,100 --> 00:10:15,070
package creator a Jason file that you

00:10:12,580 --> 00:10:20,080
make with that includes the schema so

00:10:15,070 --> 00:10:23,890
you have to make sure that you pull out

00:10:20,080 --> 00:10:26,560
that part of the JSON file and so a

00:10:23,890 --> 00:10:29,470
schema makes it possible to run a more

00:10:26,560 --> 00:10:30,400
precise validation check on your data so

00:10:29,470 --> 00:10:32,740
you're not just looking at the

00:10:30,400 --> 00:10:37,840
structural level but also at the content

00:10:32,740 --> 00:10:41,670
level so um then you just copy this part

00:10:37,840 --> 00:10:43,960
of the JSON file which is the schema and

00:10:41,670 --> 00:10:48,670
you have to make sure that you include

00:10:43,960 --> 00:10:52,180
the curly brackets that's a problem I

00:10:48,670 --> 00:10:53,920
always accidentally run into and so then

00:10:52,180 --> 00:10:55,960
you go back to the good tables web

00:10:53,920 --> 00:10:58,120
browser and you insert the schema and

00:10:55,960 --> 00:11:00,160
then you hit validate and that's where

00:10:58,120 --> 00:11:03,910
you will be able to validate the content

00:11:00,160 --> 00:11:05,620
as we showed before and so in order to

00:11:03,910 --> 00:11:08,500
fix that error that we saw on the

00:11:05,620 --> 00:11:11,290
density variable column you can

00:11:08,500 --> 00:11:14,700
either make a change in the data package

00:11:11,290 --> 00:11:18,820
tool or you can change the JSON file

00:11:14,700 --> 00:11:22,810
directly and then after you update the

00:11:18,820 --> 00:11:24,610
JSON file and yuria upload it on the

00:11:22,810 --> 00:11:26,560
good tables web tool and hit validate

00:11:24,610 --> 00:11:33,100
you should get a notice saying that

00:11:26,560 --> 00:11:35,170
everything is valid and yeah and then so

00:11:33,100 --> 00:11:39,010
I guess we just wanted to end by saying

00:11:35,170 --> 00:11:41,050
um that this is a short introduction for

00:11:39,010 --> 00:11:42,760
these different tools and if you want to

00:11:41,050 --> 00:11:46,210
learn more about them we're hosting in

00:11:42,760 --> 00:11:49,470
90 minute hands-on workshop on May 20th

00:11:46,210 --> 00:11:52,330
and we'd love to see you all there and

00:11:49,470 --> 00:11:55,300
then we're the Open Knowledge Foundation

00:11:52,330 --> 00:11:57,610
is also accepting applications for their

00:11:55,300 --> 00:12:00,250
next cohort of frictionless data

00:11:57,610 --> 00:12:02,140
reproducible research fellows so if

00:12:00,250 --> 00:12:05,200
you're an early career researcher and

00:12:02,140 --> 00:12:06,580
interested in applying we would highly

00:12:05,200 --> 00:12:12,610
recommend it and would love to talk with

00:12:06,580 --> 00:12:14,890
you about that more great great oh go

00:12:12,610 --> 00:12:17,650
ahead sorry didn't want to say no thanks

00:12:14,890 --> 00:12:21,820
so much like so much Lily and yeah and

00:12:17,650 --> 00:12:24,820
you can look at those the top URL is the

00:12:21,820 --> 00:12:27,940
or you can actually follow the syllabus

00:12:24,820 --> 00:12:29,050
of the data package and data and the

00:12:27,940 --> 00:12:31,720
scheming you can learn a little bit more

00:12:29,050 --> 00:12:33,460
about frictionless data yeah and at the

00:12:31,720 --> 00:12:35,290
bottom you can you can learn a little

00:12:33,460 --> 00:12:40,410
bit more about what the Fellowship is

00:12:35,290 --> 00:12:40,410
like and how to apply for the next round

00:12:41,490 --> 00:12:50,290
great yeah I just um there was a lot of

00:12:45,240 --> 00:12:52,860
comments and questions in the chat about

00:12:50,290 --> 00:12:54,640
how this all fits together and it works

00:12:52,860 --> 00:12:56,440
and I know there will be a lot of

00:12:54,640 --> 00:12:59,470
questions over in slack for sure

00:12:56,440 --> 00:13:01,870
I just Viki actually just put a question

00:12:59,470 --> 00:13:03,790
and or comment into this like the chat

00:13:01,870 --> 00:13:06,400
that I was I had in my mind as well

00:13:03,790 --> 00:13:11,940
which is could be really great if we can

00:13:06,400 --> 00:13:14,590
mandate this type of analysis or or

00:13:11,940 --> 00:13:16,270
review of tabular data before it goes

00:13:14,590 --> 00:13:17,980
into repositories a lot of data

00:13:16,270 --> 00:13:20,380
repositories are full of tabular data

00:13:17,980 --> 00:13:21,830
that are missing columns or poorly

00:13:20,380 --> 00:13:22,580
documented

00:13:21,830 --> 00:13:24,440
I didn't know if there were any

00:13:22,580 --> 00:13:26,960
discussions that you've had with data

00:13:24,440 --> 00:13:31,810
repositories around integrations on

00:13:26,960 --> 00:13:35,390
ingest or QA for for what in that sense

00:13:31,810 --> 00:13:36,530
yeah I we haven't um asked the fellows

00:13:35,390 --> 00:13:38,140
but I can actually tell you a little bit

00:13:36,530 --> 00:13:42,620
about some of the work that I've done

00:13:38,140 --> 00:13:45,200
through my day job that is you know we

00:13:42,620 --> 00:13:49,720
deal with a lot of data environment and

00:13:45,200 --> 00:13:51,920
climate change and we actually did a

00:13:49,720 --> 00:13:56,960
hackathon to see if we could have

00:13:51,920 --> 00:13:59,600
someone built in like a checks that as

00:13:56,960 --> 00:14:00,920
like data got fed into a repository they

00:13:59,600 --> 00:14:02,510
would check to see at least for some

00:14:00,920 --> 00:14:06,530
really basic things like you know like

00:14:02,510 --> 00:14:09,710
empty cells or you know like characters

00:14:06,530 --> 00:14:11,990
and so um you know on even new like a

00:14:09,710 --> 00:14:13,760
and like a weekend hackathon they were

00:14:11,990 --> 00:14:14,690
able to the students were working on

00:14:13,760 --> 00:14:16,730
that we're able to come up with

00:14:14,690 --> 00:14:20,630
something so it is certainly something

00:14:16,730 --> 00:14:23,810
that big organizations care about and

00:14:20,630 --> 00:14:25,820
would like to see happen and so that at

00:14:23,810 --> 00:14:27,050
least and that the researcher isn't

00:14:25,820 --> 00:14:28,550
necessarily the one that's doing it but

00:14:27,050 --> 00:14:30,020
at least there's some kind of process

00:14:28,550 --> 00:14:32,930
even sure that that actually does those

00:14:30,020 --> 00:14:34,360
checks you do happen yeah definitely

00:14:32,930 --> 00:14:37,310
yeah I agree

00:14:34,360 --> 00:14:38,570
so another question was what advice do

00:14:37,310 --> 00:14:39,950
you have for scientists that are trying

00:14:38,570 --> 00:14:43,550
to integrate these tools into their

00:14:39,950 --> 00:14:44,960
workflows for the first time sir um some

00:14:43,550 --> 00:14:50,750
advice that you'd have for the science

00:14:44,960 --> 00:14:53,060
scientist community yeah I think

00:14:50,750 --> 00:14:55,310
definitely there's a lot of video

00:14:53,060 --> 00:14:57,890
tutorials on that link that Monica

00:14:55,310 --> 00:14:59,090
shared and then link to the workshop

00:14:57,890 --> 00:15:00,140
where we're actually going to work

00:14:59,090 --> 00:15:03,260
through it all together

00:15:00,140 --> 00:15:07,970
um and I think both of those would be

00:15:03,260 --> 00:15:10,970
really great yeah okay great

00:15:07,970 --> 00:15:12,890
well thank you both we have a lot of the

00:15:10,970 --> 00:15:15,260
questions here we will move over to

00:15:12,890 --> 00:15:17,810
slack people can interact with you there

00:15:15,260 --> 00:15:20,210
and we appreciate you walking us through

00:15:17,810 --> 00:15:22,640
it I had seen a presentation about

00:15:20,210 --> 00:15:25,340
frictionless data when it first was

00:15:22,640 --> 00:15:28,550
getting launched and it's very slick

00:15:25,340 --> 00:15:32,150
that you've come so far seems like it's

00:15:28,550 --> 00:15:35,089
really you know hitting a lot of the

00:15:32,150 --> 00:15:36,799
original promise so congratulations

00:15:35,089 --> 00:15:40,009
yeah it's been a pleasure it's and

00:15:36,799 --> 00:15:43,419
really fun yeah great okay thanks for

00:15:40,009 --> 00:15:43,419

YouTube URL: https://www.youtube.com/watch?v=tZmu5DGPRmA


