Title: Scaling computations and maintaining reproducible projects with signac, an open-source framework
Publication date: 2021-05-20
Playlist: CSVConf 2021
Description: 
	Presenter: Alyssa Travitz  



Data science projects are often messy, with complex workflows and large, heterogeneous parameter spaces. Bash scripting and long file names can only get you so far, and can result in a project that is incomprehensible to collaborators or your future self. The open-source data management framework, signac, (https://signac.io) makes it easy to maintain organized and reproducible projects, and integrates with existing file-based workflows. The serverless data management and lightweight workflow model ensure that projects are easily transferable between laptops and high-performance computing environments, and the data model is well suited for high-dimensional parameter searches or hyperparameter optimization of machine learning models. In this talk, we will demonstrate the signac approach to managing data science projects, emphasizing its use in real-world scenarios. The signac framework not only increases research efficiency, but makes best practices and scalability easy and intuitive.
Captions: 
	00:00:03,120 --> 00:00:05,839
well

00:00:03,439 --> 00:00:08,000
thank you for having me um my name is

00:00:05,839 --> 00:00:09,920
alyssa favitz i'm at the university of

00:00:08,000 --> 00:00:11,840
michigan i'm a phd student

00:00:09,920 --> 00:00:14,160
um but today i'll be talking not about

00:00:11,840 --> 00:00:17,359
my thesis work but about cynic

00:00:14,160 --> 00:00:19,039
and open source software package that um

00:00:17,359 --> 00:00:20,880
originated here at the university of

00:00:19,039 --> 00:00:22,960
michigan um and just

00:00:20,880 --> 00:00:24,160
a quick background on signiak and who

00:00:22,960 --> 00:00:27,279
the team is

00:00:24,160 --> 00:00:28,720
um we are five maintainers and three

00:00:27,279 --> 00:00:32,079
committers of which i'm on

00:00:28,720 --> 00:00:32,719
um but we have over 45 contributors 40

00:00:32,079 --> 00:00:35,200
sighted

00:00:32,719 --> 00:00:36,960
papers um and we are numb focused

00:00:35,200 --> 00:00:38,399
affiliated projects so just to give some

00:00:36,960 --> 00:00:39,680
context

00:00:38,399 --> 00:00:41,760
about half of us are still at the

00:00:39,680 --> 00:00:44,399
university of michigan but we have

00:00:41,760 --> 00:00:45,920
um we have maintainers and computers in

00:00:44,399 --> 00:00:46,960
several different countries so we are

00:00:45,920 --> 00:00:50,000
very distributed

00:00:46,960 --> 00:00:53,360
um and we are an open source project

00:00:50,000 --> 00:00:56,719
so the motivation behind signiac

00:00:53,360 --> 00:00:57,440
um is essentially this which is that

00:00:56,719 --> 00:00:59,520
there are two

00:00:57,440 --> 00:01:00,719
common approaches to file naming and

00:00:59,520 --> 00:01:03,760
organization

00:01:00,719 --> 00:01:06,640
um when you're doing academic research

00:01:03,760 --> 00:01:07,760
and typically people go for very long

00:01:06,640 --> 00:01:11,119
file names

00:01:07,760 --> 00:01:14,720
um so you just start appending uh

00:01:11,119 --> 00:01:17,280
a new variable every time um

00:01:14,720 --> 00:01:18,320
or you have these very deeply nested

00:01:17,280 --> 00:01:20,320
directories

00:01:18,320 --> 00:01:22,240
and this is great if you know exactly

00:01:20,320 --> 00:01:23,840
what your data is going to look like at

00:01:22,240 --> 00:01:26,479
the end of your project

00:01:23,840 --> 00:01:27,840
um but that almost never happens in

00:01:26,479 --> 00:01:30,799
academic research or

00:01:27,840 --> 00:01:31,439
ever and so that raises the question how

00:01:30,799 --> 00:01:34,640
do you

00:01:31,439 --> 00:01:35,759
introduce a new parameter and how can

00:01:34,640 --> 00:01:39,280
you quickly

00:01:35,759 --> 00:01:41,360
check to see what parameter space you've

00:01:39,280 --> 00:01:42,159
already analyzed what has yet to be

00:01:41,360 --> 00:01:44,320
analyzed

00:01:42,159 --> 00:01:45,280
what is the status of your project how

00:01:44,320 --> 00:01:47,840
do you quickly get

00:01:45,280 --> 00:01:48,799
information so those are the two main

00:01:47,840 --> 00:01:52,240
motivations

00:01:48,799 --> 00:01:54,240
for the signiac framework um and

00:01:52,240 --> 00:01:56,320
signia can be broken down into two

00:01:54,240 --> 00:01:59,119
separate packages so it originated

00:01:56,320 --> 00:01:59,520
as signiac the data management framework

00:01:59,119 --> 00:02:01,759
um

00:01:59,520 --> 00:02:04,320
and then evolved to include signiac flow

00:02:01,759 --> 00:02:07,439
which is our workflow management

00:02:04,320 --> 00:02:10,479
framework so um to be more

00:02:07,439 --> 00:02:12,879
specific and give an example um

00:02:10,479 --> 00:02:13,760
signiak instead of having this nested

00:02:12,879 --> 00:02:15,760
file a

00:02:13,760 --> 00:02:17,200
nested directory structure or these very

00:02:15,760 --> 00:02:19,520
long file names

00:02:17,200 --> 00:02:20,480
we instead stratify everything and

00:02:19,520 --> 00:02:24,480
everything

00:02:20,480 --> 00:02:27,599
belongs to a single workspace and every

00:02:24,480 --> 00:02:30,959
what we call job is unique and

00:02:27,599 --> 00:02:33,920
has a specific hash id so while this

00:02:30,959 --> 00:02:34,879
may look like nonsense here what this

00:02:33,920 --> 00:02:38,160
actually

00:02:34,879 --> 00:02:41,360
is is making sure that every single

00:02:38,160 --> 00:02:44,080
parameter that you're running is unique

00:02:41,360 --> 00:02:45,519
so for example if i'm running a study

00:02:44,080 --> 00:02:46,239
and i know i'm varying the number of

00:02:45,519 --> 00:02:48,720
particles

00:02:46,239 --> 00:02:49,280
the pressure of my simulation the volume

00:02:48,720 --> 00:02:50,720
of my

00:02:49,280 --> 00:02:53,680
simulation maybe how long i'm

00:02:50,720 --> 00:02:56,640
equilibrating it for for me as a

00:02:53,680 --> 00:02:57,200
material scientist i would run this over

00:02:56,640 --> 00:03:00,480
a very

00:02:57,200 --> 00:03:01,840
large parameter space and i would want

00:03:00,480 --> 00:03:02,480
to know that each one of those are

00:03:01,840 --> 00:03:05,680
unique

00:03:02,480 --> 00:03:07,840
and so each of these jobs has

00:03:05,680 --> 00:03:08,800
a unique state point which is this

00:03:07,840 --> 00:03:10,879
dictionary

00:03:08,800 --> 00:03:11,920
that describes all the parameters that

00:03:10,879 --> 00:03:14,560
are intrinsic

00:03:11,920 --> 00:03:15,680
to all of the data within that workspace

00:03:14,560 --> 00:03:18,720
or within that

00:03:15,680 --> 00:03:20,879
directory and then in that same

00:03:18,720 --> 00:03:21,680
directory is all of the accompanying

00:03:20,879 --> 00:03:25,200
data

00:03:21,680 --> 00:03:26,959
for um for that state point so

00:03:25,200 --> 00:03:28,720
you can have lightweight metadata and

00:03:26,959 --> 00:03:31,680
something we call the job document

00:03:28,720 --> 00:03:32,879
and then you can have any file type any

00:03:31,680 --> 00:03:35,040
extra data

00:03:32,879 --> 00:03:36,000
um stored within that directory so

00:03:35,040 --> 00:03:38,000
everything is just

00:03:36,000 --> 00:03:39,760
one directory deep all belonging in the

00:03:38,000 --> 00:03:41,360
workspace and each of them are

00:03:39,760 --> 00:03:43,440
guaranteed to be unique

00:03:41,360 --> 00:03:45,040
because they're defined by these

00:03:43,440 --> 00:03:48,080
dictionaries that live in

00:03:45,040 --> 00:03:48,799
a single file called the um signiak

00:03:48,080 --> 00:03:52,319
state point

00:03:48,799 --> 00:03:53,360
json file so that's our approach to

00:03:52,319 --> 00:03:56,560
managing

00:03:53,360 --> 00:03:58,480
um your data space and then when it

00:03:56,560 --> 00:03:59,760
comes to operating on it so i said we

00:03:58,480 --> 00:04:01,920
want to know

00:03:59,760 --> 00:04:04,480
what has what data has been analyzed

00:04:01,920 --> 00:04:07,920
what is the status in our whole

00:04:04,480 --> 00:04:12,000
uh workflow pipeline is where

00:04:07,920 --> 00:04:13,920
zenyac flow comes in so netflow takes

00:04:12,000 --> 00:04:15,439
anything that you can do to your data

00:04:13,920 --> 00:04:19,359
space and

00:04:15,439 --> 00:04:21,359
you define it as a python function so

00:04:19,359 --> 00:04:22,880
let's say i want to have a function

00:04:21,359 --> 00:04:24,320
called calculate that's going to take my

00:04:22,880 --> 00:04:25,280
volume and divide it by the number of

00:04:24,320 --> 00:04:29,600
particles

00:04:25,280 --> 00:04:31,919
i can access the um i can asset success

00:04:29,600 --> 00:04:33,759
the volume and the number of particles

00:04:31,919 --> 00:04:36,880
from that cnx state point

00:04:33,759 --> 00:04:39,919
um for every single job in the workspace

00:04:36,880 --> 00:04:42,560
and what cynic flow allows me to do is

00:04:39,919 --> 00:04:43,600
iterate over any job in the workspace

00:04:42,560 --> 00:04:47,360
and

00:04:43,600 --> 00:04:50,240
operate on it using this function so

00:04:47,360 --> 00:04:50,960
also i know that uh at csv conference

00:04:50,240 --> 00:04:53,440
that there's

00:04:50,960 --> 00:04:54,720
probably a lot of people that are doing

00:04:53,440 --> 00:04:56,479
data

00:04:54,720 --> 00:04:57,919
management type solutions and so i want

00:04:56,479 --> 00:04:59,360
to be really clear about what we're good

00:04:57,919 --> 00:05:01,680
for and what we're not good for

00:04:59,360 --> 00:05:03,280
and sort of what are our who our target

00:05:01,680 --> 00:05:04,800
audience is and what problems we're

00:05:03,280 --> 00:05:07,600
trying to solve

00:05:04,800 --> 00:05:08,320
so we're very good for managing

00:05:07,600 --> 00:05:11,759
file-based

00:05:08,320 --> 00:05:14,720
heterogeneous data so um

00:05:11,759 --> 00:05:16,720
i gave a very brief example just now but

00:05:14,720 --> 00:05:20,000
it's very good for when

00:05:16,720 --> 00:05:23,039
your data is messy and has

00:05:20,000 --> 00:05:25,120
a lot of different um

00:05:23,039 --> 00:05:26,479
yeah if it's very heterogeneous so you

00:05:25,120 --> 00:05:28,880
don't have to have this nice

00:05:26,479 --> 00:05:30,000
uniform um where everything is perfectly

00:05:28,880 --> 00:05:32,000
defined

00:05:30,000 --> 00:05:33,840
and organized it's very good for messy

00:05:32,000 --> 00:05:35,680
data um it's also very good for

00:05:33,840 --> 00:05:36,240
searching and accessing the data within

00:05:35,680 --> 00:05:39,039
python

00:05:36,240 --> 00:05:40,800
or on the command line so um you really

00:05:39,039 --> 00:05:42,400
can use any tool you want

00:05:40,800 --> 00:05:45,360
as long as it can be run on the command

00:05:42,400 --> 00:05:46,639
line but it's very python friendly

00:05:45,360 --> 00:05:51,039
and it's very good for scalable and

00:05:46,639 --> 00:05:53,840
reproducible workflows i say scalable

00:05:51,039 --> 00:05:54,560
to mean that it's very easy to go from

00:05:53,840 --> 00:05:57,759
your first

00:05:54,560 --> 00:05:58,720
prototype of something to working on a

00:05:57,759 --> 00:06:00,560
small project

00:05:58,720 --> 00:06:02,000
this is not scaling in the sense that

00:06:00,560 --> 00:06:05,199
some other people here

00:06:02,000 --> 00:06:06,880
might be referring to scaling um and

00:06:05,199 --> 00:06:09,360
it's very good for

00:06:06,880 --> 00:06:10,960
making reproduction reproducible

00:06:09,360 --> 00:06:13,759
workflows because

00:06:10,960 --> 00:06:15,520
um it really kind of forces you to do

00:06:13,759 --> 00:06:18,400
things in a very modular

00:06:15,520 --> 00:06:20,479
and well-documented way um and with that

00:06:18,400 --> 00:06:22,400
with that it's also good for prototyping

00:06:20,479 --> 00:06:23,440
so what it does is it really encourages

00:06:22,400 --> 00:06:25,520
best practices

00:06:23,440 --> 00:06:27,120
as you're doing the very first like

00:06:25,520 --> 00:06:30,319
nuptial part of your

00:06:27,120 --> 00:06:31,520
um data science workflow um

00:06:30,319 --> 00:06:33,520
as i said it's really good for

00:06:31,520 --> 00:06:34,800
integrating with existing tools so

00:06:33,520 --> 00:06:37,120
anything you can access

00:06:34,800 --> 00:06:38,800
on the command line can be integrated

00:06:37,120 --> 00:06:41,840
into signiak

00:06:38,800 --> 00:06:42,400
and what's in x not good for is as i

00:06:41,840 --> 00:06:44,720
said with

00:06:42,400 --> 00:06:45,840
the scalability once you get over about

00:06:44,720 --> 00:06:47,919
a hundred thousand

00:06:45,840 --> 00:06:49,280
individual jobs so that's like

00:06:47,919 --> 00:06:52,400
individual

00:06:49,280 --> 00:06:53,520
um directories in that workspace

00:06:52,400 --> 00:06:57,280
directory

00:06:53,520 --> 00:07:00,319
that's when we start to sort of see uh

00:06:57,280 --> 00:07:04,319
poorer performance so this is not for

00:07:00,319 --> 00:07:07,919
um enormous data sets um

00:07:04,319 --> 00:07:08,720
this is meant for developing things very

00:07:07,919 --> 00:07:11,759
quickly

00:07:08,720 --> 00:07:12,160
and being very agile it's also not great

00:07:11,759 --> 00:07:14,880
for

00:07:12,160 --> 00:07:16,000
existing databases so if you have a lot

00:07:14,880 --> 00:07:18,639
of distributed data

00:07:16,000 --> 00:07:20,560
this is not for you um and if you have

00:07:18,639 --> 00:07:21,840
purely tabular data it's going to be

00:07:20,560 --> 00:07:23,919
overkill

00:07:21,840 --> 00:07:26,319
so getting that out of the way we're

00:07:23,919 --> 00:07:28,800
going to just go through an example

00:07:26,319 --> 00:07:30,000
and quickly see how some of these

00:07:28,800 --> 00:07:33,199
features work

00:07:30,000 --> 00:07:34,960
this is not uh all of the features by

00:07:33,199 --> 00:07:35,759
far it's very flexible it's very

00:07:34,960 --> 00:07:38,000
powerful

00:07:35,759 --> 00:07:39,680
so i encourage you to um please ask me

00:07:38,000 --> 00:07:40,080
follow-up questions in the slack after

00:07:39,680 --> 00:07:42,960
this

00:07:40,080 --> 00:07:44,479
um and or go to our documentation which

00:07:42,960 --> 00:07:48,080
i'll link to at the end

00:07:44,479 --> 00:07:50,639
so this is a um an example that

00:07:48,080 --> 00:07:52,400
was a course project by one of our

00:07:50,639 --> 00:07:54,720
maintainers bradley dice

00:07:52,400 --> 00:07:56,319
along with two other classmates um and

00:07:54,720 --> 00:08:00,080
they were looking at

00:07:56,319 --> 00:08:00,800
um the network structure of uh u.s air

00:08:00,080 --> 00:08:02,160
traffic

00:08:00,800 --> 00:08:04,080
and we don't really need to get into the

00:08:02,160 --> 00:08:05,039
details but i like to use this just as

00:08:04,080 --> 00:08:07,759
sort of a

00:08:05,039 --> 00:08:09,120
a way for us to conceptualize how

00:08:07,759 --> 00:08:12,400
signiak integrates with

00:08:09,120 --> 00:08:14,080
data science problem so i'll cover the

00:08:12,400 --> 00:08:17,440
data management side as well as

00:08:14,080 --> 00:08:21,360
the workflow uh signiak flow side

00:08:17,440 --> 00:08:24,960
so if we know that we want to analyze

00:08:21,360 --> 00:08:27,440
um u.s air traffic and we need to first

00:08:24,960 --> 00:08:30,479
create a parameter space so we know that

00:08:27,440 --> 00:08:33,680
we want to look at how um

00:08:30,479 --> 00:08:36,959
travel patterns have changed over time

00:08:33,680 --> 00:08:37,760
so let's first initialize a parameter

00:08:36,959 --> 00:08:40,479
space

00:08:37,760 --> 00:08:42,479
with all of the the range of years and

00:08:40,479 --> 00:08:46,160
the quarters within those years

00:08:42,479 --> 00:08:49,279
that um we want to investigate so

00:08:46,160 --> 00:08:51,519
really all this requires is a

00:08:49,279 --> 00:08:53,600
tiny nested for loop where you can see

00:08:51,519 --> 00:08:56,880
that we initialize the project

00:08:53,600 --> 00:08:59,920
and then um with that project

00:08:56,880 --> 00:09:02,399
um we can open the job just by

00:08:59,920 --> 00:09:04,240
defining the dictionary that will

00:09:02,399 --> 00:09:06,320
uniquely identify them

00:09:04,240 --> 00:09:08,560
um and then initialize it so what this

00:09:06,320 --> 00:09:11,120
does is if you then try to go back

00:09:08,560 --> 00:09:11,920
and re-initialize um and make a

00:09:11,120 --> 00:09:14,640
duplicate

00:09:11,920 --> 00:09:15,920
state point parameter or state a

00:09:14,640 --> 00:09:17,680
duplicate state point

00:09:15,920 --> 00:09:19,120
it won't allow you to do that it will

00:09:17,680 --> 00:09:21,360
make sure that every single thing is

00:09:19,120 --> 00:09:22,640
unique and it won't create redundant

00:09:21,360 --> 00:09:26,000
directories

00:09:22,640 --> 00:09:26,640
so what this for loop would do is create

00:09:26,000 --> 00:09:29,440
a

00:09:26,640 --> 00:09:30,959
data structure a workspace that looks

00:09:29,440 --> 00:09:33,920
something like this where

00:09:30,959 --> 00:09:35,519
you have um each these would be each of

00:09:33,920 --> 00:09:36,640
these points would be a directory within

00:09:35,519 --> 00:09:39,360
your workspace

00:09:36,640 --> 00:09:40,800
and they are varying over a year and

00:09:39,360 --> 00:09:43,600
then over a quarter and we

00:09:40,800 --> 00:09:45,600
specifically um separate quarters from

00:09:43,600 --> 00:09:47,839
years because the way that the

00:09:45,600 --> 00:09:48,720
data is analyzed depends on which

00:09:47,839 --> 00:09:52,480
quarter

00:09:48,720 --> 00:09:54,480
the um the data is from

00:09:52,480 --> 00:09:55,920
so for example if we would go into one

00:09:54,480 --> 00:09:58,160
of these directories

00:09:55,920 --> 00:09:58,959
uh let's say this is its hash id so this

00:09:58,160 --> 00:10:01,200
would be the

00:09:58,959 --> 00:10:02,800
name of the directory and then the json

00:10:01,200 --> 00:10:04,800
file within that directory

00:10:02,800 --> 00:10:05,839
would just contain the dictionary year

00:10:04,800 --> 00:10:09,120
00:10:05,839 --> 00:10:12,480
and quarter four so

00:10:09,120 --> 00:10:14,959
then once we have that um

00:10:12,480 --> 00:10:16,720
that workspace let's say we leave and we

00:10:14,959 --> 00:10:17,120
come back a year later and we want to

00:10:16,720 --> 00:10:21,360
know

00:10:17,120 --> 00:10:24,480
what um what this workspace contains

00:10:21,360 --> 00:10:26,000
and what range of parameters it covers

00:10:24,480 --> 00:10:28,320
and so we can do that easily with a

00:10:26,000 --> 00:10:29,680
command signiak schema which we can just

00:10:28,320 --> 00:10:31,040
run on the command line and it will

00:10:29,680 --> 00:10:33,440
quickly tell us

00:10:31,040 --> 00:10:35,600
what um what the range of parameters and

00:10:33,440 --> 00:10:37,120
the data types for them are

00:10:35,600 --> 00:10:38,640
um you can imagine that this is much

00:10:37,120 --> 00:10:42,079
more useful when you have

00:10:38,640 --> 00:10:44,320
larger dimension dimensions of data

00:10:42,079 --> 00:10:48,000
and same thing with querying so we can

00:10:44,320 --> 00:10:51,760
use signiak find which will tell us

00:10:48,000 --> 00:10:52,959
which jobs match a specific query so if

00:10:51,760 --> 00:10:56,560
we want to know

00:10:52,959 --> 00:10:57,760
which jobs are corresponding to the year

00:10:56,560 --> 00:11:00,880
00:10:57,760 --> 00:11:04,480
it will slice our workspace and tell us

00:11:00,880 --> 00:11:06,959
which job um job directories or

00:11:04,480 --> 00:11:08,959
the the hashtags that correspond to them

00:11:06,959 --> 00:11:11,680
um

00:11:08,959 --> 00:11:12,000
corresponds to the year 1994. so we can

00:11:11,680 --> 00:11:13,680
do

00:11:12,000 --> 00:11:15,279
much more complex filtering but this

00:11:13,680 --> 00:11:17,519
gives you an idea of

00:11:15,279 --> 00:11:19,839
how you would um access the data once

00:11:17,519 --> 00:11:23,839
it's been initialized in centiac

00:11:19,839 --> 00:11:26,959
and then when we want to um

00:11:23,839 --> 00:11:29,680
modify a workspace this is where

00:11:26,959 --> 00:11:31,360
zenyak is actually very powerful because

00:11:29,680 --> 00:11:34,320
it allows you to

00:11:31,360 --> 00:11:35,120
um modify it with while maintaining the

00:11:34,320 --> 00:11:37,279
integrity

00:11:35,120 --> 00:11:38,880
of your data so if we want to add

00:11:37,279 --> 00:11:40,959
country as another

00:11:38,880 --> 00:11:42,240
um parameter we can just iterate for all

00:11:40,959 --> 00:11:44,640
of the jobs and add

00:11:42,240 --> 00:11:47,040
another parameter you can also re-exist

00:11:44,640 --> 00:11:48,000
and rename an existing state point if we

00:11:47,040 --> 00:11:50,000
decide that

00:11:48,000 --> 00:11:51,360
now we want to capitalize quarter for

00:11:50,000 --> 00:11:52,720
example um

00:11:51,360 --> 00:11:54,240
but you see how this could be very

00:11:52,720 --> 00:11:56,399
useful if you want to change to a

00:11:54,240 --> 00:11:59,360
different naming convention um

00:11:56,399 --> 00:12:00,959
and not worry about invalidating any of

00:11:59,360 --> 00:12:04,079
your data

00:12:00,959 --> 00:12:05,920
and then with the signiac flow side so

00:12:04,079 --> 00:12:08,399
this is actually how do we go in once

00:12:05,920 --> 00:12:09,279
we've decided what parameters we want to

00:12:08,399 --> 00:12:11,040
investigate

00:12:09,279 --> 00:12:12,320
how do we actually go in and do the data

00:12:11,040 --> 00:12:15,519
science on it

00:12:12,320 --> 00:12:16,160
so if we have an idea of for each of

00:12:15,519 --> 00:12:19,279
these

00:12:16,160 --> 00:12:21,440
um directories we have a corresponding

00:12:19,279 --> 00:12:24,560
state point what year what quarter

00:12:21,440 --> 00:12:26,959
and we know that we can access from

00:12:24,560 --> 00:12:28,959
in this case it was from a government

00:12:26,959 --> 00:12:31,839
website that had traffic data

00:12:28,959 --> 00:12:32,639
corresponding to the years and quarters

00:12:31,839 --> 00:12:36,240
we can have

00:12:32,639 --> 00:12:38,880
effect data operation and then once we

00:12:36,240 --> 00:12:40,079
have acquired that data going to pull

00:12:38,880 --> 00:12:42,079
everything down

00:12:40,079 --> 00:12:43,680
and there's going to be a lot of data

00:12:42,079 --> 00:12:44,959
cleaning that needs to be done so we'll

00:12:43,680 --> 00:12:48,079
say that every single

00:12:44,959 --> 00:12:49,920
directory now has a readme.html file

00:12:48,079 --> 00:12:52,720
that we don't need and is going to get

00:12:49,920 --> 00:12:56,560
in the way of our data analysis

00:12:52,720 --> 00:12:59,120
so let's um so in order to go through

00:12:56,560 --> 00:13:03,040
and clean out these unnecessary files

00:12:59,120 --> 00:13:06,639
um we would set up a flow operation

00:13:03,040 --> 00:13:08,639
so this function is a flow operation um

00:13:06,639 --> 00:13:10,680
the only thing that makes it special is

00:13:08,639 --> 00:13:13,440
that it has this

00:13:10,680 --> 00:13:13,839
project.operation decorator and it takes

00:13:13,440 --> 00:13:16,560
job

00:13:13,839 --> 00:13:18,399
as an argument um and then we also have

00:13:16,560 --> 00:13:19,279
these pre and post conditions which is

00:13:18,399 --> 00:13:22,560
what sets this

00:13:19,279 --> 00:13:24,959
up in a flow diagram

00:13:22,560 --> 00:13:26,079
type structure um and so you can see

00:13:24,959 --> 00:13:29,360
that a prediction

00:13:26,079 --> 00:13:31,600
here is uh underneath my circle

00:13:29,360 --> 00:13:32,720
um fetch data and another condition is

00:13:31,600 --> 00:13:34,560
have readmes

00:13:32,720 --> 00:13:37,120
so this makes sure that this operation

00:13:34,560 --> 00:13:39,120
is only run if a readme exists

00:13:37,120 --> 00:13:40,240
and the way that we define that is now

00:13:39,120 --> 00:13:42,800
as a function called

00:13:40,240 --> 00:13:44,560
as readmes which is just a simple liner

00:13:42,800 --> 00:13:48,160
that checks to see if

00:13:44,560 --> 00:13:51,920
um uh if a read me exists

00:13:48,160 --> 00:13:53,120
in the file um and so and then i do want

00:13:51,920 --> 00:13:54,880
to mention there's also this

00:13:53,120 --> 00:13:56,639
functionality called labeling

00:13:54,880 --> 00:13:58,880
which is very useful when you want to

00:13:56,639 --> 00:14:01,440
see the status of your overall

00:13:58,880 --> 00:14:02,160
project so what this looks like is if

00:14:01,440 --> 00:14:05,440
you run

00:14:02,160 --> 00:14:08,480
python project dot pi status so project

00:14:05,440 --> 00:14:10,240
pi is what contains all of your uh

00:14:08,480 --> 00:14:11,920
functions so everything that i just

00:14:10,240 --> 00:14:15,440
showed in the last slide

00:14:11,920 --> 00:14:16,160
and because we've assigned has me as a

00:14:15,440 --> 00:14:19,199
label

00:14:16,160 --> 00:14:22,320
it will tell you what percent of

00:14:19,199 --> 00:14:26,160
um of operations this is

00:14:22,320 --> 00:14:28,800
valid for um and so you can see that

00:14:26,160 --> 00:14:29,760
we also have the function fetch data and

00:14:28,800 --> 00:14:33,120
it will tell you

00:14:29,760 --> 00:14:35,920
how many jobs are eligible to be run um

00:14:33,120 --> 00:14:36,800
so that if you would run this workflow

00:14:35,920 --> 00:14:40,800
it would

00:14:36,800 --> 00:14:43,920
um run fetch data on 104 eligible jobs

00:14:40,800 --> 00:14:44,560
and run remove reviews on 104 eligible

00:14:43,920 --> 00:14:46,399
jobs

00:14:44,560 --> 00:14:48,160
and then once that has run these would

00:14:46,399 --> 00:14:51,839
say zero

00:14:48,160 --> 00:14:53,199
so finally um that i'm going to go over

00:14:51,839 --> 00:14:55,600
just a few other

00:14:53,199 --> 00:14:56,399
um things that if you're interested you

00:14:55,600 --> 00:14:59,440
can look at

00:14:56,399 --> 00:15:02,320
on our documentation or our home page um

00:14:59,440 --> 00:15:04,480
this was a very high level overview with

00:15:02,320 --> 00:15:05,279
a very simple example but i'm happy to

00:15:04,480 --> 00:15:08,639
answer

00:15:05,279 --> 00:15:09,199
specific use cases um but something that

00:15:08,639 --> 00:15:11,199
we're

00:15:09,199 --> 00:15:12,399
very good at is automated cluster

00:15:11,199 --> 00:15:14,320
solution so this

00:15:12,399 --> 00:15:15,760
is um if you're working on several

00:15:14,320 --> 00:15:18,800
different super computers

00:15:15,760 --> 00:15:19,199
we have custom templating so that you

00:15:18,800 --> 00:15:23,279
can

00:15:19,199 --> 00:15:26,399
simply run uh python project submit

00:15:23,279 --> 00:15:27,519
and uh signiak will take care of all of

00:15:26,399 --> 00:15:30,959
the cueing

00:15:27,519 --> 00:15:34,160
and um uh how many

00:15:30,959 --> 00:15:36,240
cpus you need parallelizing all of that

00:15:34,160 --> 00:15:37,920
um and it does all that was cluster

00:15:36,240 --> 00:15:39,839
specific templating so if you have a

00:15:37,920 --> 00:15:41,279
cluster that we don't already support

00:15:39,839 --> 00:15:44,320
you can easily write your own

00:15:41,279 --> 00:15:47,600
and then have it automate for you it's

00:15:44,320 --> 00:15:49,360
good for exporting for data sharing so

00:15:47,600 --> 00:15:52,079
if you have a collaborator

00:15:49,360 --> 00:15:53,279
that you that doesn't use signia or if

00:15:52,079 --> 00:15:56,160
you

00:15:53,279 --> 00:15:58,320
want to archive something you can export

00:15:56,160 --> 00:16:00,399
to that nested directory structure

00:15:58,320 --> 00:16:02,639
and you can convert back from nested

00:16:00,399 --> 00:16:04,959
directory structure to synthetic

00:16:02,639 --> 00:16:06,480
um and we also have snack dashboard

00:16:04,959 --> 00:16:08,160
which i'm just plugging here but i

00:16:06,480 --> 00:16:11,360
encourage you to look at it on our

00:16:08,160 --> 00:16:14,079
website um which is uh the third

00:16:11,360 --> 00:16:15,600
in the synaxonic flow snake dashboard

00:16:14,079 --> 00:16:18,639
which allows you to view

00:16:15,600 --> 00:16:22,480
graphical output um

00:16:18,639 --> 00:16:25,759
in a very uh interactive way it's

00:16:22,480 --> 00:16:27,680
in we also have some new features like

00:16:25,759 --> 00:16:29,759
uh groups which allows you to group

00:16:27,680 --> 00:16:32,240
those operation functions

00:16:29,759 --> 00:16:32,800
onto more complex workflows and more

00:16:32,240 --> 00:16:35,279
complex

00:16:32,800 --> 00:16:37,600
missions we integrate with pandas in

00:16:35,279 --> 00:16:40,240
hdf5 data stores

00:16:37,600 --> 00:16:41,920
and we have container support with

00:16:40,240 --> 00:16:43,440
docker and singularity

00:16:41,920 --> 00:16:46,320
so that's everything that i wanted to

00:16:43,440 --> 00:16:46,639
cover today um please go to syniac.io if

00:16:46,320 --> 00:16:49,360
you're

00:16:46,639 --> 00:16:50,399
curious about our project follow snack

00:16:49,360 --> 00:16:52,399
data on twitter

00:16:50,399 --> 00:16:54,000
and we're very active on our slack if

00:16:52,399 --> 00:16:55,360
you have any questions

00:16:54,000 --> 00:16:57,839
if you just want to know more about the

00:16:55,360 --> 00:17:00,240
project um come say hi

00:16:57,839 --> 00:17:05,839
our slack is linked to on our home page

00:17:00,240 --> 00:17:05,839
thank you

00:17:10,400 --> 00:17:15,039
thank you so much i am

00:17:15,439 --> 00:17:19,280
we have about uh two minutes for

00:17:17,439 --> 00:17:22,160
questions and so i'm gonna

00:17:19,280 --> 00:17:23,919
check the queue and so uh one question

00:17:22,160 --> 00:17:27,199
that came up from the audience is from

00:17:23,919 --> 00:17:27,919
uh kelsey montgomery is storage on

00:17:27,199 --> 00:17:30,799
signiak

00:17:27,919 --> 00:17:33,520
in the cloud such that compute can be

00:17:30,799 --> 00:17:36,559
performed without moving any data

00:17:33,520 --> 00:17:39,280
so um i suppose i didn't make this clear

00:17:36,559 --> 00:17:40,080
but significant is completely file based

00:17:39,280 --> 00:17:42,880
so

00:17:40,080 --> 00:17:43,679
that is um that is one of the cases that

00:17:42,880 --> 00:17:46,640
snap is

00:17:43,679 --> 00:17:47,919
not built for so um yeah everything is a

00:17:46,640 --> 00:17:51,039
file based system

00:17:47,919 --> 00:17:53,280
in my personal case i um

00:17:51,039 --> 00:17:54,240
will run things on a cluster that you

00:17:53,280 --> 00:17:57,200
can

00:17:54,240 --> 00:17:59,039
um you can have things separately but

00:17:57,200 --> 00:18:02,799
not in the way that you are describing

00:17:59,039 --> 00:18:02,799
not in the typical cloud computing sense

00:18:03,919 --> 00:18:07,840
um all right so we have time for one

00:18:06,799 --> 00:18:10,080
more

00:18:07,840 --> 00:18:12,799
how would siniac work in terms of data

00:18:10,080 --> 00:18:14,559
portability or archived compressed data

00:18:12,799 --> 00:18:17,360
would there be a need for some speed

00:18:14,559 --> 00:18:19,440
pre-processing to get it up to speed

00:18:17,360 --> 00:18:20,640
so i would like to talk to you more at

00:18:19,440 --> 00:18:22,160
this after

00:18:20,640 --> 00:18:24,000
um so i can ask some clarifying

00:18:22,160 --> 00:18:26,480
questions but um

00:18:24,000 --> 00:18:28,000
people have used signiak um for

00:18:26,480 --> 00:18:29,679
archiving data

00:18:28,000 --> 00:18:31,120
uh specifically through the university

00:18:29,679 --> 00:18:33,840
of michigan um

00:18:31,120 --> 00:18:35,120
and that requires you because you can

00:18:33,840 --> 00:18:37,039
export it to

00:18:35,120 --> 00:18:38,880
that nested directory structure where

00:18:37,039 --> 00:18:42,080
everything is nicely labeled for you

00:18:38,880 --> 00:18:44,080
it's best to export it and then condense

00:18:42,080 --> 00:18:46,559
it essentially

00:18:44,080 --> 00:18:47,360
but i'm happy to talk about that offline

00:18:46,559 --> 00:18:50,480
all right

00:18:47,360 --> 00:18:53,600
well we'll move the question over to

00:18:50,480 --> 00:18:55,760
the q a in slack and thank you so much

00:18:53,600 --> 00:18:57,840
for a really great presentation thank

00:18:55,760 --> 00:18:57,840

YouTube URL: https://www.youtube.com/watch?v=V_pRCAmH6gc


