Title: Giulia Santarsieri - (Machine) Learning from Open Data platforms
Publication date: 2021-05-20
Playlist: CSVConf 2021
Description: 
	Today, open data platforms host a wide and heterogeneous catalog of datasets. However, these datasets are often neglected in Machine Learning (ML) and other related tasks. This mainly happens because there are few available open data catalogs specialized in ML applications and because it is often unclear whether Machine Learning algorithms would be adequate and well performing on such datasets. Therefore, several open datasets go unused while they could be leveraged by the ML community to explain, evaluate, and challenge existing methods on real open data. For instance, these real-world data could be used by professors teaching ML courses, by students taking these courses, by researchers testing current and novel ML approaches, and possibly to promote the intersection of open data, ML and public policy. In this talk we will show you how we are tackling this issue working on datasets from data.gouv.fr (DGF), the French open data government platform. We aim to answer the question of what makes a dataset suitable and well performing for Machine Learning tasks by leveraging open source tools. Our goal is to establish a first small empirical assessment of the characteristics of a dataset (size, balance of its categorical variables and so on) that make it a “good fit” for Machine Learning algorithms. Specifically, we first manually select an adequate subset of datasets from DGF. Then we perform a statistic profiling on each of these datasets. Thirdly, we automatically train and validate a set of ML algorithms on them and we cluster the datasets according to their evaluation results. These steps help us to better understand the nature of each dataset and thus determine which ones seem suitable for ML applications. Based on these datasets, and inspired by existing resources, we build the first version of a catalog of open datasets for ML. We hope that this platform will be a first stepping stone towards the reuse of open datasets in Machine Learning contexts.
Captions: 
	00:00:03,600 --> 00:00:12,160
So, hello, everyone. Thank you all for being here  today. And especially thank you to the organizers  

00:00:12,160 --> 00:00:20,240
for organizing this amazing conference. So, I'm  Giulia. I work as a data scientist at Etalab. And  

00:00:20,240 --> 00:00:29,840
today I will be talking about machine learning on  open data sets. Etalab is a French administration  

00:00:29,840 --> 00:00:36,000
that works on many data that relate to open data  and public administration and public policy.  

00:00:37,040 --> 00:00:47,120
Some of the main missions of Etalab are to promote  open data through the open data platform, gouv.fr,  

00:00:47,120 --> 00:00:56,240
the French platform. And to support open data  policy and finally through the AI Lab to exploit  

00:00:56,240 --> 00:01:02,720
open data through data science and AI. I will be mainly focusing on this today.  

00:01:02,720 --> 00:01:06,080
Talking about how to exploit  open data with machine learning.  

00:01:07,520 --> 00:01:14,000
So, first we will quickly see how there is an  actual lack of open data in machine learning.  

00:01:14,800 --> 00:01:20,480
We'll try to understand why this is the case and  why we should actually be doing -- using it more  

00:01:20,480 --> 00:01:29,680
in this area. And so, in order to tackle  this, we created DGML, a data repository  

00:01:29,680 --> 00:01:37,040
form of machine learning using open data from  data Gouv. We will show the methodology we used  

00:01:37,040 --> 00:01:42,720
in order to create this platform. So, how did this journey begin  

00:01:43,680 --> 00:01:50,480
sympathy actually started working as Etalab  while finishing my master's. I was doing a  

00:01:50,480 --> 00:02:01,200
machine learning course and wanted to do some  applications. I went to data.gouv.fr, managed by  

00:02:01,200 --> 00:02:12,560
Etalab to look for machine learning tasks. And I  was actually quite surprised by two things. So,  

00:02:12,560 --> 00:02:20,160
the first thing was that there are thousands of  datasets on the platform. But only a few were  

00:02:20,160 --> 00:02:24,880
actually used for machine learning applications  as the ones that you can see on the screen. 

00:02:25,520 --> 00:02:32,320
And also, I realized that when you have such large  amount of data, sometimes you can get lost and it  

00:02:32,320 --> 00:02:39,440
can be hard to identify datasets that could  really be used for if machine learning tasks. 

00:02:41,040 --> 00:02:46,080
So, we discussed this with my colleague,  and we realized that this was actually a  

00:02:46,080 --> 00:02:52,160
much larger issue. So, we found that in machine  learning research, for instance, when comparing  

00:02:52,160 --> 00:03:00,480
the performance of different algorithms, most  of the time it is always the same set of small  

00:03:00,480 --> 00:03:08,000
data -- always the same small set of datasets  that is used. So, you can find some of these  

00:03:08,000 --> 00:03:15,360
very famous datasets on the table I put here.  And so, you can also see that these datasets  

00:03:16,640 --> 00:03:21,760
actually have some pretty good meta features. So,  for instance, they don't have too many missing  

00:03:21,760 --> 00:03:30,240
values. They have a good size most of the time.  And so, this leads -- led us to wonder if these  

00:03:30,240 --> 00:03:37,360
datasets were actually representing the reality  and the challenges of open data. So, you know,  

00:03:37,360 --> 00:03:43,440
sometimes open data can look like this. So,  this is a table I put as an example from  

00:03:43,440 --> 00:03:49,200
a dataset using data.gouv.fr. You  can see a lot of missing values.  

00:03:50,080 --> 00:03:56,160
You have some weird data types. So you  have code. And also in the second column  

00:03:56,800 --> 00:04:01,760
you have a categorical variable that  actually has a lot of categories. 

00:04:01,760 --> 00:04:09,520
So, for simplicity, I only put a few of them. But  you can sometimes have datasets where you have  

00:04:09,520 --> 00:04:14,960
hundreds or even thousands of categories.  And these can be pretty hard to handle.  

00:04:16,320 --> 00:04:23,680
So, this actually leads us to the first reason  why we believe we should be using more open data  

00:04:23,680 --> 00:04:30,000
in machine learning. So, the challenges  I just showed you could be interesting  

00:04:30,000 --> 00:04:36,960
to challenge machine learning algorithms. And  also to help evaluate and compare performance  

00:04:37,520 --> 00:04:45,280
on a larger number of datasets. Then  since open data covers such a large  

00:04:45,280 --> 00:04:52,880
number of topics, we can imagine some insights  for interesting applications for education,  

00:04:52,880 --> 00:04:58,800
for research in various areas, or for business  and eventually to support public policy. 

00:05:01,040 --> 00:05:08,080
But so, if open data seems so great and we should  definitely use it more in machine learning, then  

00:05:08,080 --> 00:05:14,880
why isn't this the case? We identified -- we  identified three main reasons. Maybe there are  

00:05:14,880 --> 00:05:22,080
more that you can share with us. So, the first one  that is actually important is that there is a lack  

00:05:22,080 --> 00:05:29,840
of data quality. So, you know, in -- in open data  platforms, most of the time anyone can upload a  

00:05:29,840 --> 00:05:38,800
dataset. So, datasets can come in various types  of formats that can be actually challenging to  

00:05:38,800 --> 00:05:45,360
treat. So, we would all like to have nice  CSV files. But sometimes you could have PDF  

00:05:45,360 --> 00:05:51,920
data that are uploaded and many others that  can make the task pretty challenging. Also,  

00:05:51,920 --> 00:05:58,080
you can have issues with data content. So, maybe  you have datasets that are too small to be used  

00:05:59,120 --> 00:06:06,160
to be trained in a machine learning algorithm.  Or that mainly contain too many missing values.  

00:06:06,160 --> 00:06:12,400
So, these two reasons actually lead to a  need for a lot of pre-processing. And this  

00:06:12,400 --> 00:06:17,760
can possibly discourage people from using  open data in machine learning applications. 

00:06:18,960 --> 00:06:22,880
Then we also believe that there  might be a lack of communication  

00:06:22,880 --> 00:06:30,160
about these platforms. So, maybe people are not  always aware of the existence of these platforms.  

00:06:30,160 --> 00:06:36,320
And finally, and this is what we decided  to work on, there is a lack of catalogs,  

00:06:36,320 --> 00:06:41,440
a lack of data repository, there are  specifically specialized in machine learning.  

00:06:43,040 --> 00:06:49,120
What we did was we added the look of the most  famous existing data repositories, mainly UCI  

00:06:49,920 --> 00:06:58,800
repository and Open ML. And we decided to create  our own data repository for machine learning  

00:06:58,800 --> 00:07:05,520
using open data from data on the top. So, here's the challenge. We had  

00:07:05,520 --> 00:07:13,040
thousands of datasets on data.gouv.fr. And  among these thousands, we wanted to identify  

00:07:13,040 --> 00:07:20,640
a set of datasets adequate for machine learning.  Naturally, it was how do we select them? How do we  

00:07:21,600 --> 00:07:28,800
identify a dataset that is okay for machine  learning? So, at first we had a very  

00:07:28,800 --> 00:07:38,160
naive approach, I would say. So, you can see it  on the left of the diagram. We manually selected  

00:07:38,160 --> 00:07:47,600
from data.gouv.fr from datasets that were  machine learning or we knew that content-wise  

00:07:48,160 --> 00:07:53,680
were okay to be used in a machine learning  algorithm. So, we took these datasets  

00:07:54,400 --> 00:07:59,840
with profiling, we generated a statistical  profile and had a look at each one of them.  

00:08:01,200 --> 00:08:08,000
For each one of these statistical profiles, we  then only kept datasets that would actually make  

00:08:08,000 --> 00:08:14,160
sense when trained and tested in our machine  learning algorithms. So, for instance, datasets  

00:08:14,160 --> 00:08:20,960
that are an adequate size or had, for instance, a  variable that could be used as a plugin variable.  

00:08:21,760 --> 00:08:29,280
And then we automatically trained and tested  them using the mljar library. But, of course,  

00:08:29,280 --> 00:08:36,480
this is a very time consuming approach and we  wanted to come up with faster and most -- more  

00:08:36,480 --> 00:08:42,400
efficient way to select these datasets. We  also had an automatic approach that you can  

00:08:42,400 --> 00:08:49,360
see on the right of the diagram. So, we took  a sample of the datasets that are available  

00:08:49,360 --> 00:08:55,920
on data.gouv.fr. And filtered them  to the four conditions that you can  

00:08:55,920 --> 00:09:03,280
see on the screen. Mainly on the size of  the datasets and also we checked that there  

00:09:03,280 --> 00:09:08,400
were both numerical and categorical variables  and there was a low number of missing values. 

00:09:09,040 --> 00:09:14,000
Then again we did profiling. And  thanks to profiling, we found  

00:09:15,200 --> 00:09:23,360
those that were difficult to treat, those with a  lot of categories, highly correlated columns and  

00:09:23,360 --> 00:09:31,600
unsupported columns such as constant columns. And  we trained a set of machine learning algorithms.  

00:09:32,720 --> 00:09:43,200
So, why did we choose to train these algorithms?  Because we actually wanted to provide users with  

00:09:45,360 --> 00:09:51,200
all the information they could need to perform  a machine learning task on their dataset. So,  

00:09:51,200 --> 00:09:59,360
let me just show you how this platform looks like.  So, I'm just changing the screen I'm sharing. 

00:10:01,840 --> 00:10:13,840
This is what the website looks like. You have the  datasets that are available. And you can select  

00:10:13,840 --> 00:10:20,480
the features that you're interested into. So,  let's imagine, for example, you are passionate  

00:10:20,480 --> 00:10:26,560
about data. You want to do some machine learning  on datasets. So, for instance, you want to do a  

00:10:26,560 --> 00:10:34,000
regression. So, these datasets, it's interesting.  It's a dataset about cars pollution in France.  

00:10:35,200 --> 00:10:40,560
And you can click on here. Here is the -- by  clicking here, you will find the datasets on  

00:10:40,560 --> 00:10:48,720
data.gouv.fr. But, you know, you don't really want  to download the dataset and check for each column  

00:10:48,720 --> 00:10:56,160
and see what's in it, what's in each column. So,  by looking at here, you have the data dictionary.  

00:10:56,160 --> 00:11:03,040
So, you can check all the variables that are  in the -- in the dataset. Their description  

00:11:03,040 --> 00:11:09,520
and the type of each variable. So, this can  save you a lot of time. And help you better  

00:11:09,520 --> 00:11:17,840
understand what you could do with this dataset. Then, you know, one of the -- the things that you  

00:11:17,840 --> 00:11:24,160
might be interested in doing when doing machine  learning tasks, but more generally, data science  

00:11:24,160 --> 00:11:32,320
tasks, is to check some of the statistics of your  dataset. So, Pandas profiling automatically does  

00:11:32,320 --> 00:11:37,840
this for you so you don't have to generate it  on your own. You can check the statistics here.  

00:11:38,400 --> 00:11:46,000
The distribution of each one of your variables.  And also, you -- you can be interested about  

00:11:46,000 --> 00:11:52,080
correlations. So, you can see that there are some  interesting ones in these datasets, for instance.  

00:11:52,640 --> 00:11:56,240
And also, you can check the  distribution of missing values. 

00:11:57,360 --> 00:12:07,360
So, you -- so, you -- you are convinced like  this dataset seems interesting to you. And you  

00:12:07,360 --> 00:12:14,400
would like, for instance, to do a regression  using the CO2 variable as a target variable.  

00:12:15,040 --> 00:12:23,840
So, you may wonder, should I do a decision tree?  Should I train on just a linear regression?  

00:12:24,800 --> 00:12:31,600
By clicking on the -- the mljar profile, you  have an overview of what would be the performance  

00:12:31,600 --> 00:12:37,840
of each one of these algorithms if you trained and  tested them on your dataset. And, for instance,  

00:12:38,400 --> 00:12:43,120
you would know that the easier boost  model would be the most performing one.  

00:12:43,760 --> 00:12:50,240
Here you can check the upper parameters  that have been using and the metric values,  

00:12:50,240 --> 00:12:57,440
the learning curves, and some feature importance. And so, this is it. You have a lot of information  

00:12:57,440 --> 00:13:02,880
about your dataset that you collaborate to  build some interesting machine learning models.  

00:13:03,440 --> 00:13:09,200
And finally, you could use some examples that  we put here for you. So, this is an example  

00:13:10,080 --> 00:13:17,440
of an application that someone did on  data.gouv.fr. And this is a very simple code  

00:13:18,320 --> 00:13:26,320
that we did for you. Just to -- to have an  idea of what you can do with this dataset.  

00:13:28,000 --> 00:13:38,960
So, I come back to the few slides I want to  share with you. So, we had a look at the website.  

00:13:39,600 --> 00:13:46,800
And as you can imagine, we would like to improve  our platform. Namely, we would like to have  

00:13:46,800 --> 00:13:53,360
more and more quality datasets. So, this means  that we would like to improve the method that  

00:13:53,360 --> 00:14:00,720
we used in order to select these datasets. So, we  would like to improve our filters, for instance.  

00:14:00,720 --> 00:14:07,440
And in order to do that, we need to  understand the -- which are the meta features.  

00:14:08,400 --> 00:14:14,080
So, here are some of the meta features of  each one of our datasets that you can see. 

00:14:14,880 --> 00:14:21,280
And we would like to understand which one of the  -- which of these -- which of these meta features  

00:14:22,800 --> 00:14:28,720
will influence the algorithm -- would most  influence the algorithm's performance. So,  

00:14:29,760 --> 00:14:35,760
the question we want to answer to is, what makes  a dataset a good dataset for machine learning?  

00:14:35,760 --> 00:14:40,400
So, there are many ways to try to understand  that. Our first experiment that we did  

00:14:40,400 --> 00:14:48,240
was to do a simple linear regression on the meta  features of the datasets that we have until now.  

00:14:49,200 --> 00:14:53,280
Using the metric value of  algorithms as a target variable. 

00:14:53,280 --> 00:15:01,360
So, what we found out was that the size of the  dataset and the number of categories, categorical  

00:15:01,360 --> 00:15:07,440
variables, I'm sorry, were the ones that  influenced more the algorithm's performance. So,  

00:15:08,080 --> 00:15:13,520
maybe in the future, we could use this  information to improve our filters and  

00:15:13,520 --> 00:15:19,360
to better understand when a dataset is a good  dataset -- good dataset for machine learning. 

00:15:20,080 --> 00:15:29,120
So, this leads us to the last -- the ideas that we  have for the future. So, we would like to keep on  

00:15:29,120 --> 00:15:35,760
investigating what makes a dataset a good dataset  for machine learning. This will help us increase  

00:15:35,760 --> 00:15:42,160
the number of datasets we have on the platform.  And also, as I told you, we are very interested  

00:15:42,160 --> 00:15:49,200
about testing machine learning applications  with open data. So, we plan on doing this,  

00:15:49,200 --> 00:15:55,440
for instance, on Scikit-learn examples. And  finally, we feel it's very important to create  

00:15:55,440 --> 00:16:01,680
a stronger link with the data.gouv.fr community  and to encouraging people to use the datasets.  

00:16:02,400 --> 00:16:07,840
And finally, to generalize our  methodology to other platforms. 

00:16:09,120 --> 00:16:15,440
Here are some of the key takeaways for  you. We saw there is a lack of open data  

00:16:15,440 --> 00:16:24,000
in machine learning applications and research. And  to tackle this, we created DGML. So, Data Gouv for  

00:16:24,000 --> 00:16:30,320
Machine Learning. You can see the link here.  You can maybe try some applications yourself.  

00:16:30,880 --> 00:16:36,960
And we show you the methodology that we  would like also to improve that we used to  

00:16:36,960 --> 00:16:43,040
identify datasets that are adequate for machine  learning. So, we hope this is a first step to  

00:16:43,600 --> 00:16:50,080
use more open data in machine learning. And  I thank you very much for your attention. And  

00:16:50,080 --> 00:16:56,640
I welcome any questions you would like to ask me. Serah: Thank you so much, Giulia. This is great.  

00:16:57,360 --> 00:17:03,920
We have one question currently. And everyone is  welcome to add more questions if you have them.  

00:17:03,920 --> 00:17:09,760
So, the question that has been asked is about  one of the slides you had, I think slide 8.  

00:17:09,760 --> 00:17:15,440
Someone says they would love to learn more about  your choice of filters and conditions as presented  

00:17:16,960 --> 00:17:19,520
on this slide. Giulia: Okay. So,  

00:17:20,560 --> 00:17:31,840
we tried to choose filters that allowed us to --  to select datasets that could be used to -- to  

00:17:32,480 --> 00:17:38,960
train and test the meaningful algorithm.  So, this is why, for instance, we removed  

00:17:38,960 --> 00:17:45,840
highly-correlated columns, or a category  called columns with high cardinality. Because  

00:17:46,880 --> 00:17:53,120
it wouldn't really make sense, for instance,  to have columns that are too correlated  

00:17:54,080 --> 00:18:01,840
because it would bias our model. And also,  for the first filter, it was not actually  

00:18:01,840 --> 00:18:09,600
that easy to -- to define what were the features  that make a dataset good for these purposes. 

00:18:09,600 --> 00:18:17,680
So, we had a look at some results from literature.  And some others were mostly, I would say,  

00:18:18,720 --> 00:18:22,960

YouTube URL: https://www.youtube.com/watch?v=BrbdvcVcI1Q


