Title: Keynote: Dr. Kadija Ferryman - Race Matters in Health Data
Publication date: 2021-05-20
Playlist: CSVConf 2021
Description: 
	Dr. Kadija Ferryman (https://www.kadijaferryman.com/) is a cultural anthropologist and bioethicist who studies the social, cultural, and ethical implications of health information technologies. Specifically, her research examines how genomics, digital medical records, artificial intelligence, and other technologies impact racial disparities in health. She is currently Industry Assistant Professor at New York Universityâ€™s Tandon School of Engineering. As a Postdoctoral Scholar at the Data & Society Research Institute in New York, she led the Fairness in Precision Medicine research study, which examines the potential for bias and discrimination in predictive precision medicine. She earned a BA in Anthropology from Yale University, and a PhD in Anthropology from The New School for Social Research. Before completing her PhD, she was a policy researcher at the Urban Institute where she studied how housing and neighborhoods impact well-being, specifically the effects of public housing redevelopment on children, families, and older adults. Ferryman is a member of the Institutional Review Board for the All of Research Program, a Mozilla Open Science Fellow, and an Affiliate at the Center for Critical Race and Digital Studies. Dr. Ferryman has published research in journals such as Paediatric and Perinatal Epidemiology, the Journal of Health Care for the Poor and Underserved, European Journal of Human Genetics, and Genetics in Medicine. Her research has been featured in multiple publications including Nature, STAT, and The Financial Times.
Captions: 
	00:00:03,240 --> 00:00:14,309
>> Good afternoon, and thank you for joining us for our keynote with Dr. Kadija Ferryman,

00:00:14,309 --> 00:00:16,580
Race Matters in Health Data.

00:00:16,580 --> 00:00:26,000
Studying the ethical, social and policy dimensions of digital health.

00:00:26,000 --> 00:00:34,920
Genomics, digital and medical records impact health injustices, such as racial health disparities.

00:00:34,920 --> 00:00:42,090
Dr. Ferrara is an industry assistant professor at the Tandon School of Engineering.

00:00:42,090 --> 00:00:46,870
And made an ethical policy course.

00:00:46,870 --> 00:00:52,110
And was a researcher at the Urban Institute in Washington, D.C.

00:00:52,110 --> 00:00:57,570
She's an affiliate at the Data Society Research Institute and at the Institute for critical

00:00:57,570 --> 00:01:00,830
race and digital studies.

00:01:00,830 --> 00:01:06,700
And at the National Institutes of Health research program.

00:01:06,700 --> 00:01:13,399
Received her BA in anthropology from Yale University and her Ph.D. in anthropology from

00:01:13,399 --> 00:01:15,820
the New School for social research.

00:01:15,820 --> 00:01:22,350
And published in the American informatics association, health care for the poor and

00:01:22,350 --> 00:01:25,990
underserved and ethics in medicine.

00:01:25,990 --> 00:01:30,000
Please join me in welcoming with Dr. Ferrara.

00:01:30,000 --> 00:01:33,590
Dr. Ferryman: Awesome.

00:01:33,590 --> 00:01:36,590
Thank you so much for that lovely invitation.

00:01:36,590 --> 00:01:39,930
And hi to everyone out there virtually.

00:01:39,930 --> 00:01:40,930
Okay.

00:01:40,930 --> 00:01:45,539
I'm going to share my screen with you all.

00:01:45,539 --> 00:01:47,749
Okay.

00:01:47,749 --> 00:01:54,149
So, hopefully everyone can see the screen.

00:01:54,149 --> 00:01:55,380
And I'll just get started.

00:01:55,380 --> 00:01:56,380
Okay.

00:01:56,380 --> 00:02:02,889
Thank you so much to the organizers for invite me to give this talk.

00:02:02,889 --> 00:02:11,120
Although we're not together in person, I'm honored to be in your presence today.

00:02:11,120 --> 00:02:16,470
Even though we are virtual, I would like to give my talk with a land acknowledgment.

00:02:16,470 --> 00:02:22,310
So, I would like to ask you to acknowledging the Lenape community and the exclusions and

00:02:22,310 --> 00:02:28,750
erasures of many Indigenous peoples, not just the Lenape community on whose land I am located

00:02:28,750 --> 00:02:31,220
in what we now refer to as New York City.

00:02:31,220 --> 00:02:36,630
This acknowledgment demonstrates a commitment to beginning the process of working to dismantle

00:02:36,630 --> 00:02:39,880
the ongoing legacies of settler colonialism.

00:02:39,880 --> 00:02:44,790
Acknowledgment such as these invite us to ask and think about: What does it mean to

00:02:44,790 --> 00:02:48,260
live in a post and neocolonial world?

00:02:48,260 --> 00:02:50,010
What did it take for us to get here?

00:02:50,010 --> 00:02:55,920
And how can we accountable to our part in history?

00:02:55,920 --> 00:03:05,120
So, I would like to start my talk today with the title race matters in health data.

00:03:05,120 --> 00:03:11,260
One meaning of the title, Race Matters in Health Data, is that race is important in

00:03:11,260 --> 00:03:12,330
health data.

00:03:12,330 --> 00:03:18,140
But I want to parse out another meaning, another meaning of race matters that brings us closer,

00:03:18,140 --> 00:03:22,770
actually, to the word matter as in substance or material.

00:03:22,770 --> 00:03:29,680
So, when we do this, we can think about the title as referring to how race becomes matter

00:03:29,680 --> 00:03:35,650
and how processes of racialization become material in health data.

00:03:35,650 --> 00:03:40,030
It is this interpretation that I will draw on strongly for this talk.

00:03:40,030 --> 00:03:45,790
I also want us to know that the two meanings of the word "Matter" as in substance, and

00:03:45,790 --> 00:03:50,240
the other meaning, as in importance, are linked together.

00:03:50,240 --> 00:03:55,510
And that's because matter is substance and substance must be contended with.

00:03:55,510 --> 00:03:59,071
So, when we bring these two meanings together, matter and substance -- matter as substance,

00:03:59,071 --> 00:04:04,530
and matter as being important, we can think of this title, Race Matters in Health Data,

00:04:04,530 --> 00:04:10,030
is also saying that race, because it is made material in health data, must be contended

00:04:10,030 --> 00:04:12,620
with and demands attention.

00:04:12,620 --> 00:04:19,190
I would like you to keep these two threads in mind as I go through the talk today.

00:04:19,190 --> 00:04:27,110
So, I wanted to just share with you an outline of the talk today.

00:04:27,110 --> 00:04:33,620
I'll begin with the way that race is often raised as being important or mattering in

00:04:33,620 --> 00:04:34,620
health data.

00:04:34,620 --> 00:04:43,060
And that is through the absence, missingness, or non-representativiveness of health datasets.

00:04:43,060 --> 00:04:48,289
I will then take us to a different view of race matters in health data.

00:04:48,289 --> 00:04:54,680
Not as a lack of representative data over missing data, but how and where we can see

00:04:54,680 --> 00:04:57,960
processes of racialization in health data.

00:04:57,960 --> 00:05:05,270
And by racialization, I mean, the processes by which racial hierarchies are formed, circulated,

00:05:05,270 --> 00:05:07,850
and reinscribed in society.

00:05:07,850 --> 00:05:14,680
I will then conclude with some thoughts and provocations on how we can contend with these

00:05:14,680 --> 00:05:18,460
race matters in health data.

00:05:18,460 --> 00:05:26,870
So, not too long ago, I was at home in front of my laptop watching a conference online.

00:05:26,870 --> 00:05:31,529
As many of us are doing these days, and as you are doing right now.

00:05:31,529 --> 00:05:34,780
The topic was Artificial Intelligence and Health.

00:05:34,780 --> 00:05:39,960
And the speaker was discussing the development of a machine learning tool to detect prostate

00:05:39,960 --> 00:05:40,960
cancer.

00:05:40,960 --> 00:05:46,639
During the question and answer portion of the talk, I asked how health disparities in

00:05:46,639 --> 00:05:52,090
prostate cancer had been considered during the development of this tool since it hadn't

00:05:52,090 --> 00:05:54,110
been mentioned in the talk.

00:05:54,110 --> 00:06:00,879
I knew that race mattered in prostate cancer, since I, myself, have a number of relatives

00:06:00,879 --> 00:06:02,810
who are dealing with this disease.

00:06:02,810 --> 00:06:06,800
And I know that racial disparities exist in this area.

00:06:06,800 --> 00:06:14,789
For example, Black men have higher incidence and mortality rates for prostate cancer.

00:06:14,789 --> 00:06:19,689
And research has also shown that Black men are less likely to receive treatment that

00:06:19,689 --> 00:06:23,460
follows prostate cancer care guidelines.

00:06:23,460 --> 00:06:32,090
They are also less likely to receive diagnostic MRIs, and as this article shows, also experience

00:06:32,090 --> 00:06:38,550
a longer time between diagnosis and the start of treatment than white men.

00:06:38,550 --> 00:06:44,509
And although there is some research on biological contributions to racial differences in prostate

00:06:44,509 --> 00:06:50,689
cancer, there's also evidence showing that Black and white men who receive similar care

00:06:50,689 --> 00:06:51,689
have similar outcomes.

00:06:51,689 --> 00:06:56,569
So, for example, there was a study showing that Black and white men who receive care

00:06:56,569 --> 00:07:02,150
through the Veterans Administration actually, because they have equal access, have very

00:07:02,150 --> 00:07:03,699
similar outcomes.

00:07:03,699 --> 00:07:10,839
So, when I asked this question about health disparities and prostate cancer and how that

00:07:10,839 --> 00:07:18,180
was considered in the development of this AI tool, the response that I received was

00:07:18,180 --> 00:07:21,779
that the team had used a representative dataset.

00:07:21,779 --> 00:07:28,099
And that was how they had attended to issues of racial disparities, health disparities

00:07:28,099 --> 00:07:34,529
in developing the tool to make sure that their dataset that they used to train the tool had

00:07:34,529 --> 00:07:36,839
been representative.

00:07:36,839 --> 00:07:40,880
This response was satisfactory.

00:07:40,880 --> 00:07:44,749
But I also found myself unsettled by this answer.

00:07:44,749 --> 00:07:46,800
Now, don't get me wrong.

00:07:46,800 --> 00:07:50,259
I know that representative datasets matter.

00:07:50,259 --> 00:07:56,569
Especially for developing AI tools for medicine.

00:07:56,569 --> 00:08:00,180
Clinical trials are notorious for being not representative.

00:08:00,180 --> 00:08:06,809
And the emerging field of genomic medicine has a number of examples of how a lack of

00:08:06,809 --> 00:08:11,330
representativeness in data led to harm for racial and ethnic minority groups.

00:08:11,330 --> 00:08:17,830
So, for example, these are just headlines about two cases of genomic medicine.

00:08:17,830 --> 00:08:25,430
One where due to a lack of representation in clinical trials data that Black individuals

00:08:25,430 --> 00:08:33,719
were told that they had a pathogenic genetic variant when in fact if the data had been

00:08:33,719 --> 00:08:38,539
more representative, it would have come out earlier that variant is actually not pathogenic

00:08:38,539 --> 00:08:44,509
and actually found quite often in people of African ancestry.

00:08:44,509 --> 00:08:49,570
There was another case of Plavix, which is a blood thinner.

00:08:49,570 --> 00:08:54,810
And again, due to a lack of representativeness in the study data, it showed that there were

00:08:54,810 --> 00:09:00,620
different outcomes for Plavix for Asian and Pacific Islander individuals.

00:09:00,620 --> 00:09:09,350
So, it's clear that having representative data, having representative health data matters.

00:09:09,350 --> 00:09:15,199
It's clear, also, that having -- that a lack of representation matters for other kinds

00:09:15,199 --> 00:09:17,780
of health data too, not just genomics.

00:09:17,780 --> 00:09:24,000
Such as images used -- images of skin used in dermatology and oncology.

00:09:24,000 --> 00:09:29,680
There is evidence that a lack of diversity of images in skin is leading to bias in AI

00:09:29,680 --> 00:09:35,279
tools that detect skin cancer.

00:09:35,279 --> 00:09:41,680
The lack of diverse and representative images of skin conditions and especially the lack

00:09:41,680 --> 00:09:47,889
of images of skin conditions on dark skin, has prompted the grassroots effort called

00:09:47,889 --> 00:09:55,000
Brown Matters to make these images more available and to make image datasets more representative

00:09:55,000 --> 00:09:57,839
and therefore, useful for more people.

00:09:57,839 --> 00:10:01,880
So, yes, data representation matters in health.

00:10:01,880 --> 00:10:03,730
But is that enough?

00:10:03,730 --> 00:10:05,860
My answer is no.

00:10:05,860 --> 00:10:12,410
And that is because it is not enough if racial and ethnic groups are present in health data.

00:10:12,410 --> 00:10:16,070
But it matters how they are represented in that data.

00:10:16,070 --> 00:10:22,050
It also matters how this data is understood and interpreted.

00:10:22,050 --> 00:10:27,899
As I mentioned before in the previous example of prostate cancer, Black then are less likely

00:10:27,899 --> 00:10:32,050
to receive care that adheres to the official clinical guidelines.

00:10:32,050 --> 00:10:37,680
So, this example shows us that race matters when it comes to the kind of care one receives.

00:10:37,680 --> 00:10:43,960
It's also important to note that clinical guidelines and the care that is given, that

00:10:43,960 --> 00:10:46,750
that is then represented in health data.

00:10:46,750 --> 00:10:53,029
Thus another way to think about it is that guidelines and care shape how people are actually

00:10:53,029 --> 00:10:55,250
represented in clinical data.

00:10:55,250 --> 00:10:59,630
Not just whether they're represented proportionally or not.

00:10:59,630 --> 00:11:07,670
This became clear for me a few years ago when I was conducting research for the Fairness

00:11:07,670 --> 00:11:10,542
in Precision Medicine study along with my colleague Mikaela Pitcan.

00:11:10,542 --> 00:11:18,750
And we were looking at how this emerging field of data-centric medicine, precision medicine,

00:11:18,750 --> 00:11:25,889
might lead to disadvantaged or follow along existing pathways of marginalization and discrimination

00:11:25,889 --> 00:11:29,199
in health care.

00:11:29,199 --> 00:11:34,449
During this study, one of our interviewees explained to us how race matters for clinical

00:11:34,449 --> 00:11:40,110
guidelines and how that in turn shapes clinical data.

00:11:40,110 --> 00:11:44,730
And I'm gonna share an excerpt from our interview with him with you.

00:11:44,730 --> 00:11:49,910
So, he says, here are the guidelines for lung cancer screening.

00:11:49,910 --> 00:11:55,080
You must be 55 to 80 and you must have 30 pack years of smoking.

00:11:55,080 --> 00:12:00,480
This all came from a study that was done previously of 53,000 people.

00:12:00,480 --> 00:12:06,379
Of those 53,000 people, only 4% were African American.

00:12:06,379 --> 00:12:12,000
So, we now have lung cancer screening guidelines based on that 4%.

00:12:12,000 --> 00:12:19,209
And then he went on to say, you don't have to smoke a pack a day if you smoke menthols.

00:12:19,209 --> 00:12:24,850
So, now when I do lung cancer screening in the community, most of the community members

00:12:24,850 --> 00:12:29,580
that we screen don't qualify because they don't have the smoking history.

00:12:29,580 --> 00:12:33,420
Meaning that they don't have the 30 pack years of smoking.

00:12:33,420 --> 00:12:41,180
So, we can use this excerpt to tease out why representative datasets might not be enough.

00:12:41,180 --> 00:12:48,319
His argument here is that because of the way guidelines were developed, some Black people

00:12:48,319 --> 00:12:49,920
were not screened.

00:12:49,920 --> 00:12:55,940
The guidelines did not affect whether or not people got cancer, but instead affected how

00:12:55,940 --> 00:13:00,810
far their cancer progressed before detection or diagnosis.

00:13:00,810 --> 00:13:05,750
Thus the clinical data reflects this lack of early screening.

00:13:05,750 --> 00:13:11,600
Since those who were not able to be screened early would present with cancer at more advanced

00:13:11,600 --> 00:13:12,600
stages.

00:13:12,600 --> 00:13:16,050
And this would, of course, be recorded in the clinical data.

00:13:16,050 --> 00:13:26,290
For so, here we see it's not that Black individuals are not in the data or missing from the data,

00:13:26,290 --> 00:13:32,139
it's a matter of how the clinical data itself represents patterns of racial discrimination.

00:13:32,139 --> 00:13:38,089
A dataset that is representative by population groups is still stamped with this history

00:13:38,089 --> 00:13:40,230
of racial discrimination.

00:13:40,230 --> 00:13:46,070
And without knowing why the data looks the way it does, a number of seemingly reasonable

00:13:46,070 --> 00:13:50,829
patterns could be identified and conclusions could be drawn.

00:13:50,829 --> 00:13:57,950
For example, it might be reasonable to conclude, once looking at this data that has been shaped

00:13:57,950 --> 00:14:03,029
but these lung cancer screening guidelines, it might be reasonable to conclude that there

00:14:03,029 --> 00:14:10,800
is some biological or genetic difference that causes Black people to present clinically

00:14:10,800 --> 00:14:16,670
with lung cancer at more advanced stages with -- although they have fewer pack years of

00:14:16,670 --> 00:14:18,180
smoking.

00:14:18,180 --> 00:14:23,459
But knowledge of the data's history and context would not lead you to those conclusions, but

00:14:23,459 --> 00:14:27,040
would lead you down a different path.

00:14:27,040 --> 00:14:31,709
We were told this history of the screening guidelines and how they disadvantaged Black

00:14:31,709 --> 00:14:34,240
patients almost 3 years ago.

00:14:34,240 --> 00:14:38,339
Our interviewee had known of this history for much longer and this was because he was

00:14:38,339 --> 00:14:43,709
on the ground doing research in Black communities.

00:14:43,709 --> 00:14:50,000
But it wasn't until -- and we conducted that research in 2017/2018.

00:14:50,000 --> 00:14:56,180
But it wasn't until about 2 months ago that this issue was addressed and lung cancer screening

00:14:56,180 --> 00:14:58,110
guidelines were changed.

00:14:58,110 --> 00:15:05,670
The US Preventive Services Task Force recently lowered the number of eligible smoking years

00:15:05,670 --> 00:15:13,730
from 30 to 20 and the age for screening from 55 to 50.

00:15:13,730 --> 00:15:18,350
And this was based on additional research that was done.

00:15:18,350 --> 00:15:20,199
And what did this research show?

00:15:20,199 --> 00:15:24,250
It showed that there were racial disparities in lung cancer screening.

00:15:24,250 --> 00:15:30,079
This was actually a study that was done by Vanderbilt University.

00:15:30,079 --> 00:15:36,540
And this research done, this is the study, the evaluation of the screening guidelines,

00:15:36,540 --> 00:15:38,050
showed a number of differences.

00:15:38,050 --> 00:15:44,399
But one, according to this research, of those diagnosed with lung cancer, 32% of African

00:15:44,399 --> 00:15:50,050
Americans had been eligible for screening versus 56% of whites.

00:15:50,050 --> 00:15:54,750
This screening example shows us that race matters in health data.

00:15:54,750 --> 00:15:59,550
Again, matters in terms of the substance that you can kind of see racialization happening

00:15:59,550 --> 00:16:02,160
and made real in the data.

00:16:02,160 --> 00:16:08,240
And that part -- and that patterns of marginalization literally -- quite literally -- materialize

00:16:08,240 --> 00:16:09,990
in health data.

00:16:09,990 --> 00:16:13,720
It took years for this racial gap in guidelines to be addressed.

00:16:13,720 --> 00:16:19,129
And we can only think about how many people were harmed due to not being able to be screened.

00:16:19,129 --> 00:16:25,839
And also, what kinds of conclusions, again, were drawn from this kind of data without

00:16:25,839 --> 00:16:28,820
a knowledge of the history.

00:16:28,820 --> 00:16:36,220
And this, I argue, is not something that a representative dataset can fix.

00:16:36,220 --> 00:16:44,529
So, there are other examples of how clinical data is racialized.

00:16:44,529 --> 00:16:50,199
Historian Lundy Braun's book, breathing race into the machine: The surprising career of

00:16:50,199 --> 00:16:54,820
spirometer from plantation to genetics.

00:16:54,820 --> 00:17:03,709
And why this includes a racial correction for Black people and Asian people.

00:17:03,709 --> 00:17:10,319
That is, when someone breathes into the spirometer, the measurement is changed depending on whether

00:17:10,319 --> 00:17:15,130
the patient is identified as Black or Asian.

00:17:15,130 --> 00:17:23,360
This correction to spirometer data often happens invisibly and unbeknownst to the patient and

00:17:23,360 --> 00:17:27,250
to the clinicians as well that this is happening.

00:17:27,250 --> 00:17:36,250
Lundy Braun, this came out in 2008. and traces how the beliefs about the supposed

00:17:36,250 --> 00:17:44,380
smaller lungs of Black people helped justify plantation labor as suitable, even helpful

00:17:44,380 --> 00:17:47,880
for the supposed weak lungs of Black people.

00:17:47,880 --> 00:17:54,210
So, this book raised the question of why this kind of racial correction was still being

00:17:54,210 --> 00:17:56,059
used.

00:17:56,059 --> 00:18:10,400
And a decade ago in 2011, clinicians were asking about the EGFR that is a calculator

00:18:10,400 --> 00:18:12,580
of kidney function.

00:18:12,580 --> 00:18:19,290
This calculator uses a race multiplier to adjust for the, again, supposed higher muscle

00:18:19,290 --> 00:18:22,029
mass of African Americans.

00:18:22,029 --> 00:18:29,470
This article here published in 2011 raised questions about the clinical data that informs

00:18:29,470 --> 00:18:37,130
this multiplier and whether this clinical data is scientifically valid.

00:18:37,130 --> 00:18:38,600
This came out in 2011.

00:18:38,600 --> 00:18:43,750
I did my dissertation research a few years later which was an ethnographic study in an

00:18:43,750 --> 00:18:45,000
academic medical center.

00:18:45,000 --> 00:18:53,010
And during that time, one of the trainee positions, a Latino woman, shared with me her perplexities

00:18:53,010 --> 00:18:57,559
and exasperation with the eGFR.

00:18:57,559 --> 00:19:04,090
She told me that she asked her attending physician how to use eGFR for Latinos when she saw there

00:19:04,090 --> 00:19:09,710
were different factors for white individuals and Black individuals.

00:19:09,710 --> 00:19:15,750
He said that what he does is look at how dark the patient is to determine how much African

00:19:15,750 --> 00:19:23,669
ancestry they have and base his use of the race multiplier on that determination.

00:19:23,669 --> 00:19:29,299
She shared with me then her shock and disappointment at how this calculator was being used in what

00:19:29,299 --> 00:19:35,140
she thought was an unprecise and unjust manner.

00:19:35,140 --> 00:19:44,040
So, almost 10 years later, Vyas and colleagues published the first comprehensive examination

00:19:44,040 --> 00:19:48,029
of racial corrections used in medicine.

00:19:48,029 --> 00:19:52,441
This article is called Hidden in plain sight -- reconsidering the use of race correction

00:19:52,441 --> 00:19:55,299
in clinical algorithms.

00:19:55,299 --> 00:20:01,779
And I'll show you here, this is an excerpt of a longer table that they include in this

00:20:01,779 --> 00:20:08,950
article of the multiple racial corrections that are used in multiple specialties in medicine

00:20:08,950 --> 00:20:11,440
from oncology to cardiology.

00:20:11,440 --> 00:20:17,730
So, in this article, the authors question the statistic evidence undergirding these

00:20:17,730 --> 00:20:19,850
corrections.

00:20:19,850 --> 00:20:24,750
And here's an excerpt from the article where they discuss a common risk score that's adjusted

00:20:24,750 --> 00:20:26,940
by race.

00:20:26,940 --> 00:20:33,900
The American Heart Association heart failure risk score predicts the risk of death in patients

00:20:33,900 --> 00:20:36,179
admitted to the hospital.

00:20:36,179 --> 00:20:41,640
It assigns three additional points to any patient identified as nonblack, therefore

00:20:41,640 --> 00:20:46,649
categorizing all black patients as being at a lower risk.

00:20:46,649 --> 00:20:53,580
The AHA -- American Heart Association -- does not provide a rationale for this adjustment.

00:20:53,580 --> 00:20:58,440
Clinicians are advised to use this risk score to guide decisions about referral to cardiology

00:20:58,440 --> 00:21:02,029
and allocation of health care resources.

00:21:02,029 --> 00:21:06,080
Since "black" is quit acquitted with lower risk, following the guidelines could direct

00:21:06,080 --> 00:21:08,850
care away from black patients.

00:21:08,850 --> 00:21:17,120
So, we the authors are arguing that these corrections shape what could be seen as objective

00:21:17,120 --> 00:21:23,960
clinical data and it's important, one, because as they mentioned, there is often not a rationale

00:21:23,960 --> 00:21:30,450
or the scientific evidence undergirding these corrections is outdated or questionable.

00:21:30,450 --> 00:21:32,060
That's one problem.

00:21:32,060 --> 00:21:37,799
But they also highlight that these racial corrections are a matter of justice.

00:21:37,799 --> 00:21:45,230
Because these corrections can further health disparities and health inequities.

00:21:45,230 --> 00:21:52,659
And it wasn't until this past June that major hospitals in Massachusetts decided to stop

00:21:52,659 --> 00:21:56,740
using the eGFR race multiplier.

00:21:56,740 --> 00:22:01,760
And here is a Tweet from med student Lash Nolen announcing the change.

00:22:01,760 --> 00:22:04,770
And she notes here that this is a major win.

00:22:04,770 --> 00:22:09,080
And it's the result of years of hard work, advocacy, and research done by Black students

00:22:09,080 --> 00:22:10,080
and scholars.

00:22:10,080 --> 00:22:17,549
Again, the article, the color of kidneys that I showed earlier was from I believe 2011.

00:22:17,549 --> 00:22:19,950
And here we have the change happening.

00:22:19,950 --> 00:22:25,960
And only happening in some medical centers in 2020.

00:22:25,960 --> 00:22:29,669
So, this is an important step, right?

00:22:29,669 --> 00:22:35,630
To have this major institution drop the use of the eGFR multiplier.

00:22:35,630 --> 00:22:40,590
And you can see some of the reasons given here in -- given by the institution as to

00:22:40,590 --> 00:22:43,020
why they dropped the race multiplier.

00:22:43,020 --> 00:22:47,280
First that race is a social, not a biological construct.

00:22:47,280 --> 00:22:52,470
And that research studies have not provided an acceptable scientific rationale for making

00:22:52,470 --> 00:22:58,210
these decisions based on the social construct of race.

00:22:58,210 --> 00:23:04,760
But although this -- this academic medical center has taken this step, many others still

00:23:04,760 --> 00:23:10,150
use not only the eGFR correction, but some of the many other racial corrections that

00:23:10,150 --> 00:23:15,809
were identified in the Vyas and colleagues article.

00:23:15,809 --> 00:23:23,860
And in addition, there was another commenter on this thread of this Tweet that noted that,

00:23:23,860 --> 00:23:28,690
not only are these corrections still used by many, many hospitals, but they are also

00:23:28,690 --> 00:23:34,020
used by apps that -- that use these risk calculators.

00:23:34,020 --> 00:23:42,460
So, for example, there are apps such as these that use some of these risk corrections -- risk

00:23:42,460 --> 00:23:47,480
calculators to provide people estimates of their risk.

00:23:47,480 --> 00:23:52,230
And as is kind of mentioned in this Tweet, there's a couple of institutions that are

00:23:52,230 --> 00:23:53,560
called out.

00:23:53,560 --> 00:24:04,600
Apps like mdcalc and other apps to say when are these other apps going to remove race-based

00:24:04,600 --> 00:24:07,519
weights from their platforms?

00:24:07,519 --> 00:24:14,990
And it's unclear if these apps -- apps such as these and many others will continue to

00:24:14,990 --> 00:24:19,570
use these racial corrections -- racial corrections.

00:24:19,570 --> 00:24:25,540
And if there is really a way to enforce them or make them kind of not use these kind of

00:24:25,540 --> 00:24:27,130
calculators.

00:24:27,130 --> 00:24:35,289
So, I want to share another example of how race is made material in clinical data.

00:24:35,289 --> 00:24:41,180
And this time in clinician's notes and electronic medical records.

00:24:41,180 --> 00:24:50,320
So, in this example, Chen and colleagues analyzed clinicians notes and found differences in

00:24:50,320 --> 00:24:54,760
how mental health was documented for patients of different races.

00:24:54,760 --> 00:24:59,570
And, again, here is an excerpt from this article.

00:24:59,570 --> 00:25:04,460
And one of the things that the authors note, and they were looking at clinicians' notes,

00:25:04,460 --> 00:25:05,460
right?

00:25:05,460 --> 00:25:09,360
Where clinicians write about the patients in the medical record.

00:25:09,360 --> 00:25:15,110
And found that white patients had a higher topic enrichment values for anxiety and chronic

00:25:15,110 --> 00:25:16,230
pain topics.

00:25:16,230 --> 00:25:21,390
And Black,ing Hispanic and Asian patients had higher topic enrichment values for the

00:25:21,390 --> 00:25:22,580
psychosis topic.

00:25:22,580 --> 00:25:29,309
They found other differences by gender and insurance type.

00:25:29,309 --> 00:25:36,320
This shows that doctors talk about mental health of their patients in different ways

00:25:36,320 --> 00:25:39,510
depending on their race.

00:25:39,510 --> 00:25:47,161
And in another, more recent example, researchers from Johns Hopkins University analyzed clinicians'

00:25:47,161 --> 00:25:52,980
notes and found that Black patients were, again, discussed differently than others.

00:25:52,980 --> 00:25:58,581
So, as this headline shows, an analysis of the medical records, specifically the clinician's

00:25:58,581 --> 00:26:04,090
notes, showed that physicians are more likely to doubt Black patients than white patients.

00:26:04,090 --> 00:26:07,450
And what does that mean, "Doubt patients"?

00:26:07,450 --> 00:26:12,769
And this comes from the study that was done was called Testimonial injustice: Linguistic

00:26:12,769 --> 00:26:16,740
bias in the medical records of Black patients and women.

00:26:16,740 --> 00:26:22,680
And what this article shows is that doctors were more likely to use doubting language,

00:26:22,680 --> 00:26:23,740
such as claims.

00:26:23,740 --> 00:26:25,930
As in a person claims to have pain.

00:26:25,930 --> 00:26:32,049
Or a person claims to have soreness, when referring to Black patients reports of their

00:26:32,049 --> 00:26:33,080
health.

00:26:33,080 --> 00:26:40,770
So, hopefully it's not hard to see racial hierarchies at work here in these examples.

00:26:40,770 --> 00:26:47,730
And we can see that clinical data mirrors and reinforces age-old and harmful stereotypes

00:26:47,730 --> 00:26:53,190
about Black people being more unstable and less trustworthy than others.

00:26:53,190 --> 00:26:59,789
And, again, this is not a problem that respective datasets can fix.

00:26:59,789 --> 00:27:03,820
And I want to share another example of race matters in health data.

00:27:03,820 --> 00:27:06,649
This time, when race is not even present as a variable.

00:27:06,649 --> 00:27:12,740
So, in the previous examples, these were examples of how Black people were talked about, in

00:27:12,740 --> 00:27:14,760
this case, talked about in medical records.

00:27:14,760 --> 00:27:22,690
How they -- their clinical measurements were changed because of explicit racial corrections.

00:27:22,690 --> 00:27:27,990
But we can see this even when race is not used specifically.

00:27:27,990 --> 00:27:37,090
So, we know -- hopefully by now we all know that the COVID-19 pandemic has not affected

00:27:37,090 --> 00:27:42,779
equal -- has not affected everyone equally and has mirrored the previous patterns of

00:27:42,779 --> 00:27:45,220
health disparities in the US.

00:27:45,220 --> 00:27:52,070
In New York where I am, the hardest hit areas during the peak pandemic times here overlapped

00:27:52,070 --> 00:27:58,500
almost exactly with areas that had the lowest life expectancy pre-COVID.

00:27:58,500 --> 00:28:04,940
So, we know that race matters for COVID-19.

00:28:04,940 --> 00:28:12,690
We know that it is racism, not race, that is a risk factor for dying from COVID-19.

00:28:12,690 --> 00:28:18,769
But recently, Schmidt and colleagues explored the ways that racialization affects clinical

00:28:18,769 --> 00:28:24,080
care, even when race is left out of the data.

00:28:24,080 --> 00:28:30,690
Their examination focuses on clinical risk calculators that are used to determine the

00:28:30,690 --> 00:28:32,100
allocation of ventilators.

00:28:32,100 --> 00:28:36,470
A critical issue during the COVID-19 pandemic.

00:28:36,470 --> 00:28:42,279
These calculators used creatinine, which is an indicator of kidney health to compute scores

00:28:42,279 --> 00:28:48,260
to indicate the patient's chances of survival in the near-term as well as their overall

00:28:48,260 --> 00:28:50,570
life expectancy.

00:28:50,570 --> 00:28:57,700
The authors argue that the use of creatinine in these calculators disproportionately and

00:28:57,700 --> 00:29:04,190
unfairly puts Black people at a lower priority for receiving ventilators.

00:29:04,190 --> 00:29:11,300
This is because, as they argue, that creatinine is not just a physiological measure, but it

00:29:11,300 --> 00:29:17,330
also, quote, measures social disadvantages that may cause higher creatinine.

00:29:17,330 --> 00:29:23,880
So, higher levels of creatinine result from chronic conditions such as kidney disease

00:29:23,880 --> 00:29:25,810
and high blood pressure.

00:29:25,810 --> 00:29:31,330
And the prevalence of these conditions in racial and ethnic minority groups can be attributed

00:29:31,330 --> 00:29:34,740
in large part to social determinants of health.

00:29:34,740 --> 00:29:40,490
And can be, as they argue as well, quote, best understood as the consequences of health

00:29:40,490 --> 00:29:43,330
inequities and structural racism.

00:29:43,330 --> 00:29:51,350
So, again, their argument is that even when race is not present, when a measure of creatinine

00:29:51,350 --> 00:29:59,169
is used, that is still bringing in with it histories of social determinants of health,

00:29:59,169 --> 00:30:01,299
histories of health inequities.

00:30:01,299 --> 00:30:08,120
So, again, because this is an example where race matters in health data, even when race

00:30:08,120 --> 00:30:15,840
is absent, a more representative dataset, a more representative dataset by race and

00:30:15,840 --> 00:30:22,700
ethnicity would not do much to address this issue of racialization in this data.

00:30:22,700 --> 00:30:30,019
So, now that I've taken you through these examples of racial -- of racialization in

00:30:30,019 --> 00:30:37,150
health data, from screening guidelines to clinical notes to risk calculators, the question

00:30:37,150 --> 00:30:46,240
might be, well, if data representation is not enough, what else should we be doing?

00:30:46,240 --> 00:30:51,419
And this is where we get to the contending with race matters part of the talk.

00:30:51,419 --> 00:30:56,280
So, I do have a few suggestions in this regard.

00:30:56,280 --> 00:31:07,450
The first is to shift our language.

00:31:07,450 --> 00:31:12,710
To shift our language around data problems because how we talk about problems shapes

00:31:12,710 --> 00:31:16,450
and structures our universe of solutions.

00:31:16,450 --> 00:31:22,200
As my former colleague at the data in society research institute Kinjal Dave wrote, with

00:31:22,200 --> 00:31:27,530
when we stop overusing the word bias, we can begin to use the language that has been designed

00:31:27,530 --> 00:31:31,149
to theorize at the level of structural oppression.

00:31:31,149 --> 00:31:36,570
By using the language of bias, we may end up overly focusing on the individual intents

00:31:36,570 --> 00:31:42,450
of technologists involved -- even on technologies, I would say -- rather than the structural

00:31:42,450 --> 00:31:46,900
power of the institutions they belong to.

00:31:46,900 --> 00:31:53,919
We need to move away from a language of bias, she argues, since it implies local, technical

00:31:53,919 --> 00:31:59,600
fixes for data when instead we should be looking at how institutional and systemic factors

00:31:59,600 --> 00:32:01,700
shape our data.

00:32:01,700 --> 00:32:08,269
Similarly, Stevens and Keys argued that we should not just focus on, quote, biased datasets.

00:32:08,269 --> 00:32:13,090
Because the datasets themselves are not so much biased as they are reflective of their

00:32:13,090 --> 00:32:14,539
sites of use.

00:32:14,539 --> 00:32:20,560
And we must instead focus on the logics and systems of inequality that lead to the datasets

00:32:20,560 --> 00:32:21,560
themselves.

00:32:21,560 --> 00:32:24,780
And lead to the datasets looking the way they do.

00:32:24,780 --> 00:32:32,159
And one common refrain that's often -- that's often mentioned in the kind of growing attention

00:32:32,159 --> 00:32:37,440
to bias, and, again, just like with representative datasets, I'm not saying that they're bad,

00:32:37,440 --> 00:32:39,399
I'm saying they are not enough.

00:32:39,399 --> 00:32:43,620
And in the conversations around bias, there have been some positive things to come out

00:32:43,620 --> 00:32:48,850
of the conversations around data bias and what the impacts of data bias are.

00:32:48,850 --> 00:32:55,500
But they do leave out this issue of structural power of systemic factors that are shaping

00:32:55,500 --> 00:32:57,590
the data the way they do.

00:32:57,590 --> 00:33:02,320
And one common refrain is often, well, garbage in, garbage out.

00:33:02,320 --> 00:33:08,309
And the idea is that if the data are bad or garbage, trash, then the result when those

00:33:08,309 --> 00:33:14,130
data are used, how they're used is gonna lead to a kind of garbage or trash result.

00:33:14,130 --> 00:33:19,919
But I would actually argue that garbage that's going in is not actually garbage, right?

00:33:19,919 --> 00:33:26,720
That garbage data actually is very informative of these social forces, of these patterns

00:33:26,720 --> 00:33:29,019
of marginalization, right?

00:33:29,019 --> 00:33:36,019
Of these histories that if we did more investigating, did more digging of the data, that we could

00:33:36,019 --> 00:33:37,880
actually learn a lot from.

00:33:37,880 --> 00:33:40,270
So, those data aren't garbage.

00:33:40,270 --> 00:33:45,759
They actually are really important political and social artifacts that we should be attending

00:33:45,759 --> 00:33:49,490
to and understanding what they mean.

00:33:49,490 --> 00:33:57,000
So, I suggest, then, that we -- when we shift our language from bias.

00:33:57,000 --> 00:34:04,130
It does get us to this focus on institutional and systemic forces.

00:34:04,130 --> 00:34:12,899
So, moving away from biased data to thinking and talking about our data as racialized data.

00:34:12,899 --> 00:34:20,030
And so, it's my argument that when we approach and kind of think about our data as racialized

00:34:20,030 --> 00:34:23,810
data, then we can begin to ask different questions about the data.

00:34:23,810 --> 00:34:29,100
Then we can begin to see and understand and interpret the data in new ways.

00:34:29,100 --> 00:34:35,050
And so, some questions that could be asked about the data include: What histories does

00:34:35,050 --> 00:34:36,929
this data represent?

00:34:36,929 --> 00:34:43,220
As well as, what current processes of marginalization does this data represent?

00:34:43,220 --> 00:34:48,500
Not necessarily that it's not representative or not biased, but what is it actually showing

00:34:48,500 --> 00:34:49,909
to us?

00:34:49,909 --> 00:34:56,370
And I believe we can learn a lot from that -- from asking these kinds of questions.

00:34:56,370 --> 00:35:01,990
And so, these are questions that individual data analysts can ask themselves and can discuss

00:35:01,990 --> 00:35:03,590
with their teams.

00:35:03,590 --> 00:35:09,060
It may also prompt critical reflections on how some of these same histories and current

00:35:09,060 --> 00:35:15,079
discriminatory and exclusionary practices factor into who is in the room of data analysts

00:35:15,079 --> 00:35:16,710
and who is not there?

00:35:16,710 --> 00:35:23,410
And whose values, world views and experiences, not just technical know how, are shaping the

00:35:23,410 --> 00:35:25,579
analyses being done.

00:35:25,579 --> 00:35:33,589
Going back to the quote from Dr. Kareem Watson, he's a scientific researcher and knew many,

00:35:33,589 --> 00:35:39,960
many years ago because of his experience, because of the type of work that he did that

00:35:39,960 --> 00:35:44,660
lung cancer screening guidelines were disadvantaging the community.

00:35:44,660 --> 00:35:51,650
But it took many years and again, formal research studies to begin that process of changing

00:35:51,650 --> 00:35:52,650
the guidelines.

00:35:52,650 --> 00:35:57,569
And so, what happens when we bring in people who have different experiences and different

00:35:57,569 --> 00:36:03,230
interpretations, different views of the data and what histories they actually represent?

00:36:03,230 --> 00:36:11,740
So, this brings me to this suggestion which follows from the reframing of -- the reframing

00:36:11,740 --> 00:36:18,069
of data from biased or not representative to racialized data.

00:36:18,069 --> 00:36:23,980
That is that technical data analysts can ask themselves these questions.

00:36:23,980 --> 00:36:33,010
But they may also need to expand their network of expertise to include anthropologists, sociologists,

00:36:33,010 --> 00:36:40,310
historians, and others who can provide essential information about the data histories, contexts,

00:36:40,310 --> 00:36:43,920
and social processes that animate the data.

00:36:43,920 --> 00:36:50,990
For those data analysts who are already doing this work, bringing in collaborators from

00:36:50,990 --> 00:36:56,300
other disciplines, I would encourage you to share your stories of how seeing race mattering

00:36:56,300 --> 00:37:05,710
outside of data representativeness and data bias has improved your work.

00:37:05,710 --> 00:37:15,910
So, what are race matters in health data and why does race matter in health data?

00:37:15,910 --> 00:37:21,230
I tried to show that health data are political and social artifacts.

00:37:21,230 --> 00:37:26,890
Meaning that they are marked by, reflect and shape systems of power in society.

00:37:26,890 --> 00:37:32,650
When we shift our language and our frames, we open up new possibilities for understanding

00:37:32,650 --> 00:37:34,540
and for action.

00:37:34,540 --> 00:37:39,820
We open up new paths for the data work that is so needed and so important for it to matter

00:37:39,820 --> 00:37:40,960
for everyone.

00:37:40,960 --> 00:37:42,090
Thank you.

00:37:42,090 --> 00:37:51,170
And here are my references.

00:37:51,170 --> 00:37:52,510
And email.

00:37:52,510 --> 00:37:55,060
And Twitter to reach out.

00:37:55,060 --> 00:38:03,780
Jonathan: Thank you so much for this great presentation.

00:38:03,780 --> 00:38:12,000
It's really -- invoked a lot of conservation in the chat and there's a number of questions.

00:38:12,000 --> 00:38:14,670
Dr. Ferryman: Great.

00:38:14,670 --> 00:38:18,990
Should I keep -- should I keep my sharing screen up?

00:38:18,990 --> 00:38:20,460
Or stop sharing?

00:38:20,460 --> 00:38:23,410
Jonathan: Whatever you prefer to do.

00:38:23,410 --> 00:38:26,849
If you want to keep it up.

00:38:26,849 --> 00:38:29,435
Or if you want to take it down, that's fine as well.

00:38:29,435 --> 00:38:30,435
Dr. Ferryman: Okay.

00:38:30,435 --> 00:38:31,435
Let me -- take it down for now.

00:38:31,435 --> 00:38:34,840
I might put it back up to put my email address and Twitter handle back up there.

00:38:34,840 --> 00:38:37,250
But I'll stop sharing for now.

00:38:37,250 --> 00:38:45,550
Jonathan: So, we have some questions from our audience.

00:38:45,550 --> 00:38:55,690
And the first question is what if we want race-based corrections?

00:38:55,690 --> 00:39:04,510
I am South Asian and new research shows that my community's heart -- community's heart

00:39:04,510 --> 00:39:08,090
health risks are different from those of white patients.

00:39:08,090 --> 00:39:14,140
Most doctors don't follow the research and treat using white norms which puts families

00:39:14,140 --> 00:39:15,470
like mine in danger.

00:39:15,470 --> 00:39:20,410
Why shouldn't we be demanding that our doctors consider research-based racial corrections?

00:39:20,410 --> 00:39:23,350
Dr. Ferryman: That's a great question.

00:39:23,350 --> 00:39:24,730
And thank you for that.

00:39:24,730 --> 00:39:33,190
You know, my -- I think the -- the goal of my discussion of race corrections is -- and

00:39:33,190 --> 00:39:39,230
kind of sharing my story about my doing the -- hearing the comments from the fellow about

00:39:39,230 --> 00:39:45,541
the eGFR and showing that table of some of the racial corrections and the quote from

00:39:45,541 --> 00:39:51,640
the American Heart Association is to begin to question the racial corrections that are

00:39:51,640 --> 00:39:52,920
-- that are being used.

00:39:52,920 --> 00:40:00,119
And so, it's not to say that we should not have racial corrections, or we should not

00:40:00,119 --> 00:40:04,070
have adjustments, risk adjustments that are more tailored, that are more precise.

00:40:04,070 --> 00:40:07,260
This is part of the goal of precision medicine.

00:40:07,260 --> 00:40:14,990
To gather more data about risks for smaller and smaller sub-populations of people to really

00:40:14,990 --> 00:40:17,069
understand health risks.

00:40:17,069 --> 00:40:21,790
Because up until this point, they have been based around the white male as the norm.

00:40:21,790 --> 00:40:26,790
And so -- but with the corrections that I showed, for example, the eGFR, the corrections

00:40:26,790 --> 00:40:28,200
to the spirometer.

00:40:28,200 --> 00:40:40,960
Those are based on, you know, what is really being understood as find of outdated, incorrect,

00:40:40,960 --> 00:40:42,150
clinical research.

00:40:42,150 --> 00:40:43,150
Right?

00:40:43,150 --> 00:40:50,260
And corrections that are really drawing upon the kind of age-old racial stereotypes.

00:40:50,260 --> 00:40:54,619
And even in the space of the spirometer and the racial correction there, correcting.

00:40:54,619 --> 00:41:00,510
And that correction is used for Black -- people who are Black and Asian, right?

00:41:00,510 --> 00:41:05,609
The -- it's -- that correction, again, was sort of drawing on racial stereotypes.

00:41:05,609 --> 00:41:13,089
But it was also quite conveniently as, you know, Lundy Braun describes, used as a way

00:41:13,089 --> 00:41:16,940
to justify existing racial hierarchies, right?

00:41:16,940 --> 00:41:22,470
So, literally these measurements were used as a way to justify why Black people were

00:41:22,470 --> 00:41:25,560
better suited to plantation labor.

00:41:25,560 --> 00:41:27,910
Especially and even as factor labor.

00:41:27,910 --> 00:41:30,130
There was more factory jobs available.

00:41:30,130 --> 00:41:35,190
It was, again, used to say that, no, Black people, they can't kind of handle factory

00:41:35,190 --> 00:41:38,280
labor because they don't have their lung capacity can't handle it.

00:41:38,280 --> 00:41:43,160
It was a way to exclude Black people from factory jobs that, you know, were offering

00:41:43,160 --> 00:41:44,270
higher wages.

00:41:44,270 --> 00:41:50,329
So, I think that is the critical question that we have to ask about when we're thinking

00:41:50,329 --> 00:41:55,960
about different populations and their risks and when to include that is, what kind of

00:41:55,960 --> 00:41:57,900
racial hierarchies are they drawing on?

00:41:57,900 --> 00:42:05,819
Are they working to sort of -- do they fall in line with maintaining existing power hierarchies?

00:42:05,819 --> 00:42:07,280
Or not?

00:42:07,280 --> 00:42:13,220
The other interesting thing about sort of thinking about South Asians and having different

00:42:13,220 --> 00:42:17,900
-- having different risks for heart disease, Let's say.

00:42:17,900 --> 00:42:23,089
Again, even that category, because there's been research showing, even looking at breast

00:42:23,089 --> 00:42:24,300
cancer in Asian women.

00:42:24,300 --> 00:42:29,460
The risk for breast cancer in Asian women is very different for Asian women in the United

00:42:29,460 --> 00:42:32,320
States than it is for Asian women living in Asian.

00:42:32,320 --> 00:42:37,690
So, even when we say something like we need -- there's research showing that South Asian

00:42:37,690 --> 00:42:42,869
people have different risks for heart disease, which South Asian people are we talking about?

00:42:42,869 --> 00:42:47,690
Are we talking about South Asian people in the United States?

00:42:47,690 --> 00:42:48,690
Which countries?

00:42:48,690 --> 00:42:55,460
So, there has to be -- these are the kinds of generalizations that are used that can,

00:42:55,460 --> 00:43:01,260
again, we can kind of question what the evidence there is and sort of think about if these

00:43:01,260 --> 00:43:04,760
-- if these definitions are valid or not?

00:43:04,760 --> 00:43:11,460
And are they being used to kind of improve the health and lives of people with these

00:43:11,460 --> 00:43:12,460
calculators?

00:43:12,460 --> 00:43:13,460
Jonathan: Thank you.

00:43:13,460 --> 00:43:25,390
A second question from Allen asks: What are the mechanisms within institutions like AHA,

00:43:25,390 --> 00:43:27,790
med schools and others to revise race-based corrections?

00:43:27,790 --> 00:43:33,100
Do they have teams or committees that can look at and respond to the research?

00:43:33,100 --> 00:43:38,000
I guess I'm wondering if there are even enough structural institutional starting points for

00:43:38,000 --> 00:43:39,000
advocacy.

00:43:39,000 --> 00:43:40,770
Dr. Ferryman: Yeah.

00:43:40,770 --> 00:43:44,690
That's a great question.

00:43:44,690 --> 00:43:48,420
It's an empirical question, I think.

00:43:48,420 --> 00:43:52,770
I think the evidence showing that this is all very recent, right?

00:43:52,770 --> 00:43:59,829
So, that article identifying that there are all these racial -- race-based calculators

00:43:59,829 --> 00:44:02,480
just, you know, came out not only a year ago.

00:44:02,480 --> 00:44:09,040
And as I mentioned, there are many clinicians who didn't know that, you know, these racial

00:44:09,040 --> 00:44:11,250
calculations were as widespread as they are.

00:44:11,250 --> 00:44:15,140
If you're working in one specialty, you may not know there's a racial correction used

00:44:15,140 --> 00:44:17,450
in other specialties.

00:44:17,450 --> 00:44:22,470
The first step was kind of this awareness raising.

00:44:22,470 --> 00:44:28,880
But as Lash Nolen raised in her Tweet, people have been raising this issue and talking about

00:44:28,880 --> 00:44:33,369
it in different specialties and in different contexts for years.

00:44:33,369 --> 00:44:41,950
And it really did take advocacy from medical students, physicians at institutions to say,

00:44:41,950 --> 00:44:46,770
let's really think about how we're using these calculators and if we should be using these

00:44:46,770 --> 00:44:47,770
calculators, right?

00:44:47,770 --> 00:44:54,050
And if you can imagine, something like a calculator that has a correction that's literally built

00:44:54,050 --> 00:45:01,640
into the health IT infrastructure, it can be very costly and take quite a while to,

00:45:01,640 --> 00:45:04,180
you know, to turn back to undo that kind of decision.

00:45:04,180 --> 00:45:11,740
So, it's not surprising that a very well-funded hospital like I believe was Brigham women's,

00:45:11,740 --> 00:45:12,740
right?

00:45:12,740 --> 00:45:18,270
Was one of the first to step out and say they were able to do this because of the sort of

00:45:18,270 --> 00:45:24,099
time and funding and everything that's involved with changing -- changing that kind of correction.

00:45:24,099 --> 00:45:30,640
So, I think it's -- it depends on, you know, the politics of a particular institution,

00:45:30,640 --> 00:45:31,640
right?

00:45:31,640 --> 00:45:39,270
I learned from doing my dissertation research that health care -- medical centers and hospitals

00:45:39,270 --> 00:45:42,599
have their own politics, you know, there as well.

00:45:42,599 --> 00:45:49,119
So, it will sort of depend institution-by-institution I think in some cases how these kinds of changes

00:45:49,119 --> 00:45:50,119
can be made.

00:45:50,119 --> 00:45:56,411
But I think taking -- having a very well-known, well-regarded hospital take the step, take

00:45:56,411 --> 00:46:00,090
a public step to say that we have found that this is not useful.

00:46:00,090 --> 00:46:03,900
That it's not scientifically valid is really important.

00:46:03,900 --> 00:46:08,900
I think there are these conversations happening at more and more institutions about, do we

00:46:08,900 --> 00:46:11,150
continue to use these corrections?

00:46:11,150 --> 00:46:17,250
And how do we begin to turn back and kind of take these out of our systems?

00:46:17,250 --> 00:46:21,260
Because it literally -- they literally -- quite literally become systemic, right?

00:46:21,260 --> 00:46:23,240
These are things that are programmed in.

00:46:23,240 --> 00:46:27,609
And it can be quite difficult to remove them from systems.

00:46:27,609 --> 00:46:33,369
Jonathan: So, I think we have sort of a related question.

00:46:33,369 --> 00:46:35,819
Or at least one that in my head is related.

00:46:35,819 --> 00:46:46,980
So, Candace was asking -- states: I'm wondering if social determinants of health are habitually

00:46:46,980 --> 00:46:50,760
considered by a doctor during a doctor's visit?

00:46:50,760 --> 00:46:54,270
Dr. Ferryman: Yes, who asked that question?

00:46:54,270 --> 00:47:00,080
Jonathan: Candace: Dr. Ferryman: Thank you for asking that question.

00:47:00,080 --> 00:47:02,210
That is a fantastic question.

00:47:02,210 --> 00:47:09,710
And, again, this is sort of emerging area of clinical medicine.

00:47:09,710 --> 00:47:19,369
And there are a number of health care institutions that, again, after years of advocacy and research

00:47:19,369 --> 00:47:26,140
showing that social determinants of health are real, that they really do translate into

00:47:26,140 --> 00:47:34,160
different health outcomes for people, that one zip code is more determinantive of one's

00:47:34,160 --> 00:47:37,150
health than one's genetic code, for example.

00:47:37,150 --> 00:47:42,500
So, it has become accepted that social determinants of health are real.

00:47:42,500 --> 00:47:44,329
And they should be contended with.

00:47:44,329 --> 00:47:51,530
And so, there is actually a movement afoot to -- and a number of hospitals around the

00:47:51,530 --> 00:47:52,530
country.

00:47:52,530 --> 00:47:55,200
And my knowledge is kind of limited to the US.

00:47:55,200 --> 00:48:02,260
Are trying out essentially kind of social risk screeners as part of routine clinical

00:48:02,260 --> 00:48:03,260
visits.

00:48:03,260 --> 00:48:11,079
So, how do we -- how do clinicians build in, as part of, you know, the usual set of questions

00:48:11,079 --> 00:48:15,480
that you might be asked, as part of your routine visit, you know, your weight, this, whatever,

00:48:15,480 --> 00:48:18,280
to ask about things about housing instability?

00:48:18,280 --> 00:48:21,720
To ask about things like food insecurity?

00:48:21,720 --> 00:48:28,810
Again, because of the wealth of data showing that social determinants of health really

00:48:28,810 --> 00:48:35,250
do kind of determine -- really have a great influence on people's -- on people's health

00:48:35,250 --> 00:48:36,250
outcomes.

00:48:36,250 --> 00:48:39,670
So, there's been a lot of movement around that.

00:48:39,670 --> 00:48:44,060
There are also in terms of data and sort of data infrastructures of architecture which

00:48:44,060 --> 00:48:49,180
I am really interested in as these political artifacts.

00:48:49,180 --> 00:48:55,700
There's also some work happening, again, at a number of institutions around the country.

00:48:55,700 --> 00:49:04,940
Sort of thinking about how to link clinical health data infrastructures with social and

00:49:04,940 --> 00:49:07,710
kind of community health data infrastructures, right?

00:49:07,710 --> 00:49:12,609
So, we have hospitals on the one hand who are providing health services who may not,

00:49:12,609 --> 00:49:18,440
again, have a full picture or an idea of what else is happening in that person's life.

00:49:18,440 --> 00:49:24,640
But then there are actually a network of community institutions or other kind of institutions

00:49:24,640 --> 00:49:32,280
that do have information on this individual and some of these social -- social determinants

00:49:32,280 --> 00:49:33,320
or social information.

00:49:33,320 --> 00:49:41,530
So, how do we begin to literally kind of stitch these data architectures together so that

00:49:41,530 --> 00:49:49,089
in addition to maybe asking some questions that -- that data can kind of be at the fingertips

00:49:49,089 --> 00:49:55,230
of the clinicians on one hand and of, you know, on the -- on the side of the community

00:49:55,230 --> 00:49:56,420
organizations as well.

00:49:56,420 --> 00:49:59,780
Of course, being, you know, privacy protecting and things like that.

00:49:59,780 --> 00:50:06,290
One thing I will say that although it is encouraging to see these efforts to create and I believe

00:50:06,290 --> 00:50:09,520
they're called like Community Care Networks.

00:50:09,520 --> 00:50:15,430
That's the linking of these disparate forms of data and things like training clinicians

00:50:15,430 --> 00:50:21,660
to ask about social issues that might impact people's health outcomes.

00:50:21,660 --> 00:50:27,620
Although I do think those are important, I think we still have to be critical as we are

00:50:27,620 --> 00:50:31,299
moving in this direction, ask critical wees about that.

00:50:31,299 --> 00:50:36,730
Because research has shown that other kinds of screening tools and screening questions

00:50:36,730 --> 00:50:44,710
are -- again, can be used in ways that mirror existing patterns of racial discrimination.

00:50:44,710 --> 00:50:50,900
So, for example, there are screening questions or ways that physicians can screen for if

00:50:50,900 --> 00:50:55,619
a mother is addicted to drugs.

00:50:55,619 --> 00:51:00,819
And research has shown that Black mothers are asked those questions more than white

00:51:00,819 --> 00:51:01,819
mothers are.

00:51:01,819 --> 00:51:08,559
So, if, you know, questions and incorporating social determinants of health are going to

00:51:08,559 --> 00:51:14,910
be done, they should be sort of applied equally and they shouldn't become yet another tool

00:51:14,910 --> 00:51:22,559
to marginalize, discriminate, oppress people who are already facing social and health inequities.

00:51:22,559 --> 00:51:30,720
That would be my only kind of caveat and note that we should still be sort of critical as

00:51:30,720 --> 00:51:37,250
we're seeing movements on these two fronts to incorporate social determinants and social

00:51:37,250 --> 00:51:40,619
issues more into the clinical space.

00:51:40,619 --> 00:51:49,080
Jonathan: So, with your last example, sort of links another question from the audience.

00:51:49,080 --> 00:51:56,010
From William who writes: I'm very interested to know if similar assumptions about data

00:51:56,010 --> 00:52:00,450
have been a cause in other health outcomes such as maternal health?

00:52:00,450 --> 00:52:03,550
Dr. Ferryman: Yeah.

00:52:03,550 --> 00:52:12,750
So, there is a crisis of Black women's maternal health in this country.

00:52:12,750 --> 00:52:14,800
Maternal and infant health in this country.

00:52:14,800 --> 00:52:23,390
So, Black women have, I think, at least three times higher incidences of maternal complications

00:52:23,390 --> 00:52:27,160
and infant mortality than white women do.

00:52:27,160 --> 00:52:36,230
And this is even after adjusting for sociodemographic information like income, like education, et

00:52:36,230 --> 00:52:37,230
cetera.

00:52:37,230 --> 00:52:43,320
So, this is one of those cases where, you know, it's not poverty, it's not access even

00:52:43,320 --> 00:52:44,740
to health care, right?

00:52:44,740 --> 00:52:49,299
That when all those things remain equal, there's still a big disparity.

00:52:49,299 --> 00:52:55,100
And so, there has been -- there's a lot of research sort of looking into how racialization,

00:52:55,100 --> 00:53:02,220
how processes of racialization sort of actually happen in -- during reproductive care for

00:53:02,220 --> 00:53:03,220
Black women, right?

00:53:03,220 --> 00:53:05,799
And during prenatal care for Black women.

00:53:05,799 --> 00:53:12,079
And there's a great ethnography by a cultural anthropologist called "Reproducing race: Pregnancy

00:53:12,079 --> 00:53:19,190
as a site of racialization" by Kiera Bridges and she talks about how racial hierarchies

00:53:19,190 --> 00:53:27,329
-- Black women get racialized as soon as they step into the door to receive pre-natal care.

00:53:27,329 --> 00:53:32,060
She has some really compelling interviews with clinicians who, again, sort of think

00:53:32,060 --> 00:53:36,420
they -- and her ethnography was done in a community hospital.

00:53:36,420 --> 00:53:41,340
These are clinicians who are helping some of the underserved, right?

00:53:41,340 --> 00:53:46,550
Have experience with treating a diverse array of patients.

00:53:46,550 --> 00:53:48,490
But hold these paradoxical views.

00:53:48,490 --> 00:53:55,869
So, for example, one of the kind of paradoxes that she highlighted from her -- from -- through

00:53:55,869 --> 00:54:00,880
her ethnography was that Black women were treated as, on the one hand, as wily patients

00:54:00,880 --> 00:54:08,060
who were trying to game -- kind of game the health care system and get access to benefits

00:54:08,060 --> 00:54:10,089
that they weren't necessarily entitled to.

00:54:10,089 --> 00:54:11,990
But also at the same time uneducated.

00:54:11,990 --> 00:54:16,970
And so, not able to understand health information the same way that other patients do.

00:54:16,970 --> 00:54:21,710
It was the sort of paradoxical thing that, you know, they're saying that they can kind

00:54:21,710 --> 00:54:25,500
of use their smarts in some ways, but are actually not smart in others.

00:54:25,500 --> 00:54:31,770
It's these kind of processes of racialization that I think we really have to attend to when

00:54:31,770 --> 00:54:38,900
thinking about how we see, again, holding other, you know, other variables, you know,

00:54:38,900 --> 00:54:42,440
the same that we see these -- these different out comes.

00:54:42,440 --> 00:54:46,849
There's also some great work being done sort of looking at -- and I'll put a quick plug

00:54:46,849 --> 00:54:53,369
in here for a documentary called "Listen to Me" that's being shot and directed and produced

00:54:53,369 --> 00:54:55,040
by Black women.

00:54:55,040 --> 00:55:01,390
This comes out from the Serena Williams story of not being listened to when she was receiving

00:55:01,390 --> 00:55:02,390
care.

00:55:02,390 --> 00:55:05,799
When she was having her child that she was experiencing, you know, a blood clot.

00:55:05,799 --> 00:55:08,780
And even though she was, again, wealthy.

00:55:08,780 --> 00:55:10,760
It was not an issue of not having access.

00:55:10,760 --> 00:55:14,200
She was there in the best hospital that she could have been, but she literally wasn't

00:55:14,200 --> 00:55:16,599
listened to while she was there.

00:55:16,599 --> 00:55:23,710
That documentary sort of looks at and explores these instances of Black women not being listened

00:55:23,710 --> 00:55:25,360
to about their care.

00:55:25,360 --> 00:55:30,420
And there is also research again, showing, like I showed in the talk where physicians

00:55:30,420 --> 00:55:35,740
don't trust what, you know -- are less likely to trust what Black people are telling them.

00:55:35,740 --> 00:55:40,480
So, I think these are so of the things that we can look to.

00:55:40,480 --> 00:55:45,400
And conduct more research on quite frankly to begin to identify these -- some of these

00:55:45,400 --> 00:55:49,869
processes of racialization that happen in -- in the clinical space.

00:55:49,869 --> 00:55:55,200
Jonathan: So, I think we have time for a couple more questions.

00:55:55,200 --> 00:56:04,880
And Danielle asks: Do you have any insight into how people interested in the quantified

00:56:04,880 --> 00:56:13,460
self movement as was talked about yesterday by some speakers yesterday can interface with

00:56:13,460 --> 00:56:18,080
precision medicine if they choose to sure their personal health data openly?

00:56:18,080 --> 00:56:19,890
Dr. Ferryman: Yes.

00:56:19,890 --> 00:56:24,470
And quantified self is a really interesting -- I'm sorry I didn't get to see some of those

00:56:24,470 --> 00:56:25,470
presentations.

00:56:25,470 --> 00:56:29,740
Quantified self is a really interesting kind of social and cultural movement that a number

00:56:29,740 --> 00:56:34,069
of anthropologists and scientific knowledge scholars have done some really interesting

00:56:34,069 --> 00:56:36,799
work analyzing.

00:56:36,799 --> 00:56:41,520
But I think what's interesting about some of that work is, again, looking at -- thinking

00:56:41,520 --> 00:56:46,810
about where race, racialization, racial hierarchies come to play, right?

00:56:46,810 --> 00:56:53,210
So, as I understand it, some of the -- the people who are sort of -- would call themselves

00:56:53,210 --> 00:57:00,660
part of the quantified self community, you know, are able to gather data, troves of data

00:57:00,660 --> 00:57:06,430
about themselves and sort of use it in -- share the data with their clinicians or also kind

00:57:06,430 --> 00:57:12,079
of use their data to come up with their own kind of hacks or practices to alter their

00:57:12,079 --> 00:57:13,079
health.

00:57:13,079 --> 00:57:19,150
And so, again, sort of thinking about in that quantified self space, how being a -- and

00:57:19,150 --> 00:57:23,390
I don't know if I'm using the right terminology -- but a quantified self, looks different

00:57:23,390 --> 00:57:31,290
and is reacted differently to if one is let's say Black, right?

00:57:31,290 --> 00:57:39,109
And, for example, there's a great book by Eric Topol who has done some great work talking

00:57:39,109 --> 00:57:41,740
about the future, I think it's called "The Doctor Will See You Now."

00:57:41,740 --> 00:57:48,230
It is great because it talks about allow in the future when we have all this data and

00:57:48,230 --> 00:57:54,570
the AI that people will be the managers of their health and the doctors will actually

00:57:54,570 --> 00:57:56,859
take this consultant role.

00:57:56,859 --> 00:57:58,670
But that book doesn't mention race at all.

00:57:58,670 --> 00:58:03,880
And it doesn't mention how that opportunity to sit in and tell your doctor, here.

00:58:03,880 --> 00:58:05,690
This is all the data I have.

00:58:05,690 --> 00:58:11,950
Is not likely, again, based on real evidence that we have, that scholars have collected,

00:58:11,950 --> 00:58:17,660
is not going to be treated the same way if you are Black than if you are white, right?

00:58:17,660 --> 00:58:23,329
So, I think it's really important for people involved in the quantified self movement to

00:58:23,329 --> 00:58:29,059
kind of bring in, to bring the subject of racialization into some of their discussions,

00:58:29,059 --> 00:58:32,030
some of their plans for how to bring in data, how to share data.

00:58:32,030 --> 00:58:33,270
To think, okay.

00:58:33,270 --> 00:58:37,289
Let's not just think about this in terms of the default of a white male who is going to

00:58:37,289 --> 00:58:43,770
be respected, listened to, with their data and, you know, be able to assume these different

00:58:43,770 --> 00:58:50,240
kinds of social positions, change, you know, shifts in relationships between clinicians

00:58:50,240 --> 00:58:51,240
and patients, right?

00:58:51,240 --> 00:58:53,630
That's not going to work the same for everyone.

00:58:53,630 --> 00:58:59,289
So, I think that would be my kind of first suggestion to people interested in those sort

00:58:59,289 --> 00:59:03,330
of questions around the quantified self movement and what people will do with their data and

00:59:03,330 --> 00:59:10,319
this kind of really, I think, important thinking about the future and sort of future-making.

00:59:10,319 --> 00:59:16,650
But imagining that, that future may look different depending on what you look like.

00:59:16,650 --> 00:59:18,450
And how we can actually change that.

00:59:18,450 --> 00:59:24,819
So, that the future can be that -- that we can imagine a future where race won't play

00:59:24,819 --> 00:59:25,819
a role.

00:59:25,819 --> 00:59:28,910
But I think we have to sort of think about that at the outset and we can't just think

00:59:28,910 --> 00:59:35,190
about, you know, sharing data or, you know, different ways to make medicine more precise

00:59:35,190 --> 00:59:39,530
without bringing in these issues of racialization.

00:59:39,530 --> 00:59:45,179

YouTube URL: https://www.youtube.com/watch?v=Pzd2-ZVSUps


