Title: Revealing Room for Improvement in Accessibility within a Social Media Data Visualization Community
Publication date: 2021-05-19
Playlist: CSVConf 2021
Description: 
	Full title: Revealing Room for Improvement in Accessibility within a Social Media Data Visualization Learning Community


Presented by:Silvia Canelón, Elizabeth Hare

We all aim to use data to tell a compelling story, and many of us enjoy sharing how we got there by open-sourcing our code, but we don't always share our story with everyone. Even kind, supportive, and open communities like the #TidyTuesday R learning community on Twitter has a ways to go before the content shared can be accessible to everyone. Lived experiences of blind R users tell us that most data visualizations shared for TidyTuesday are inaccessible to screen reading technology because they lack alternative text (i.e. alt text) descriptions. Our goal was to bring this hidden lack of accessibility to the surface by examining the alternative text accompanying data visualizations shared as part of the TidyTuesday social project. We scraped the alternative text from 6,443 TidyTuesday images posted on Twitter between April 2, 2018 and January 31, 2021. The first image attached to each tweet was considered the primary image and was scraped for alternative text. Manual web inspection revealed the CSS class and HTML element corresponding to the primary image, as well as the attribute containing the alternative text. We used this information and the ROpenSci {RSelenium} package to scrape the alternative text. Our preliminary analysis found that only 2.4% of the images contained a text description entered by the tweet author compared to 84% which were described by default as "Image". This small group of intentional alternative text descriptions had a median word count of 18 (range: 1-170), and a median character count of 83 (range: 8-788). As a reference point, Twitter allows 240 characters in a single tweet and 1,000 characters for image descriptions. This analysis was made possible thanks to a dataset of historical TidyTuesday tweet data collected using the ROpenSci {rtweet} package, and openly available in the TidyTuesday GitHub repository (https://github.com/rfordatascience/tidytuesday). Twitter: @spcanelon; @DogGeneticsLLC GitHub: @spcanelon; @LizHareDogs
Captions: 
	00:00:00,016 --> 00:00:04,240
>> Thanks very much. For anything who would  like to follow along, there's a short link  

00:00:04,240 --> 00:00:15,920
for the slides, it's bitly/TidyTuesday, tidy  with a capital T and does with a capital T. 

00:00:15,920 --> 00:00:22,640
Elizabeth: Thank you for the opportunity to talk  to you today about on online learning community  

00:00:22,640 --> 00:00:29,840
and what we found out about how accessible for  blind people using screen reading software. 

00:00:31,120 --> 00:00:36,800
Silvia: Who are we? I'm Silvia Canelon, a research  scientist at the University of Pennsylvania,  

00:00:37,360 --> 00:00:43,840
I used R programming language to  analyze electronic health record data. 

00:00:43,840 --> 00:00:52,240
Elizabeth: I'm Liz Hare. I have been working  with R for 15 years working on genetics and  

00:00:52,240 --> 00:01:01,440
genomics data analysis. And I met Silvia in MiR,  an organization of people from minority background  

00:01:01,440 --> 00:01:09,920
working together to make R a better space for  everybody. And Silvia had an idea. An interest  

00:01:10,480 --> 00:01:19,440
in data accessible and wanted to rook at the  information from the TidyTuesday project to see  

00:01:21,200 --> 00:01:27,920
how often descriptive text was being added so that  blind people could follow the data visualizations. 

00:01:37,200 --> 00:01:44,480
There's a few things that we wanted to give you to  takeaway from our talk today. There are a lot of  

00:01:44,480 --> 00:01:51,840
online learning communities around  R and other data science software.  

00:01:52,960 --> 00:01:58,240
Mediated in places like Twitter and  Slack that are very supportive and  

00:01:58,800 --> 00:02:07,840
people are really enjoying participating.  TidyTuesday is a specific data visualization  

00:02:09,200 --> 00:02:16,400
project using the Tidy verse packages in the  R open source practice programming language.  

00:02:17,680 --> 00:02:26,960
We found that 3% of the Tweets displaying data  visualizations had alt text which would describe  

00:02:27,680 --> 00:02:32,960
the contents of the data visualization in a way  that could be read by a screenreader. Eighty-four%  

00:02:35,200 --> 00:02:40,400
were described as image which is a default place  holder when the user at no time enter anything. 

00:02:44,320 --> 00:02:47,600
We were still wondering how  blind people can participate in  

00:02:48,320 --> 00:02:54,480
these data science learning communities. You  may be ask why blind people need access to  

00:02:54,480 --> 00:03:00,640
data visualization. I know there are people  that think that it's a form of communication  

00:03:00,640 --> 00:03:04,960
that can't be transmitted in any other way.  But most of the time, it does need to be.  

00:03:06,960 --> 00:03:10,640
There are blind scientists who need to be able to  

00:03:11,920 --> 00:03:17,840
access, read the scientific literature and  contribute to it and present their work.  

00:03:19,120 --> 00:03:24,960
There are more general applications in  current events. Things like election maps  

00:03:26,080 --> 00:03:30,880
and public health. We all would like to  be able to have access to knowing the  

00:03:30,880 --> 00:03:33,760
COVID infection rates in  the communities around us. 

00:03:36,880 --> 00:03:41,680
There are some statistical software  packages like Quorum and SAS  

00:03:42,560 --> 00:03:49,760
that will use vector-oriented graphics to  produce data visualizations that have aspects  

00:03:49,760 --> 00:03:53,840
that can be grabbed by a screenreader, but  R doesn't have that capability at this time.  

00:03:57,440 --> 00:04:02,800
Screen reading software provides voice  or braille access to text that's on the  

00:04:02,800 --> 00:04:11,360
computer screen. It does not describe graphics.  You may be familiar with artificial intelligence  

00:04:11,360 --> 00:04:20,960
image descriptions that are provided newly by some  operating systems and Facebook and LinkedIn. These  

00:04:21,600 --> 00:04:27,840
processes have not been developed enough that they  could meaningfully describe data visualization. 

00:04:30,080 --> 00:04:35,200
So, then we wondered specific about TidyTuesday  and the ability of blind people to participate. 

00:04:35,200 --> 00:04:48,640
Silvia: So, what is it? It's part of  a broader online R learning community  

00:04:48,640 --> 00:04:56,640
and provides participants the opportunity to  look at visualizing data with the tidy packages.  

00:04:58,240 --> 00:05:00,640
On the right-hand side of the  screen is an embedded Tweet  

00:05:01,520 --> 00:05:06,320
which is an invitation for people to  participate on Twitter every week. And so,  

00:05:06,320 --> 00:05:13,520
this Tweet was send out by Tom Mock who leads the  social project. We sent this out last night for  

00:05:13,520 --> 00:05:20,560
today. Tuesday. The Tweet includes information  about the data set, where it comes from,  

00:05:21,120 --> 00:05:25,440
about the project itself. And includes  some relevant links that people can follow. 

00:05:28,800 --> 00:05:33,040
And then on this slide on the left-hand side  is an embedded Tweet showing an example of what  

00:05:33,040 --> 00:05:39,200
one of these -- what we're calling TidyTuesday  submissions or data visualization shares. What  

00:05:39,200 --> 00:05:44,880
they might look like and what nay might include.  So, in the body of the Tweet itself, there might  

00:05:44,880 --> 00:05:50,000
be a verbal description. Maybe the author shares  their and in the data or what they found. Maybe  

00:05:50,000 --> 00:05:54,400
they share something about their learning process.  Maybe they tried a new function or a new package.  

00:05:55,120 --> 00:05:58,880
And then they may also choose to include  some interpretation of the data as well.  

00:05:58,880 --> 00:06:05,360
It can vary. The Tweet can also include the link  to the author's source code, an important aspect  

00:06:05,360 --> 00:06:12,320
of the social project so that they share the code  openly so that others may learn about the process  

00:06:12,320 --> 00:06:19,840
that went into creating the data visualization  and reproduce it or modify it for their own needs. 

00:06:21,040 --> 00:06:25,680
And then, of course, the Tweet will generally have  an image attached of the data visualization that  

00:06:25,680 --> 00:06:30,800
was created. But rarely do these have text  descriptions or alt text attached to them.  

00:06:33,120 --> 00:06:36,640
So, how are the data collected? The Tweets  

00:06:37,840 --> 00:06:43,440
for TidyTuesday have been collected over time  since April of 2018 when the project was started.  

00:06:44,800 --> 00:06:52,880
Tom Mock has collected the data and makes it  actually in the TidyTuesday repository. And uses  

00:06:52,880 --> 00:07:01,840
the R Tweet package from R open side. What I did  was I used that data to identify links for each  

00:07:01,840 --> 00:07:09,680
individual Tweet and did some processing and used  the R Selenium package also from R Open Side to  

00:07:09,680 --> 00:07:16,080
scrape the alt text attribute that corresponded  to the image in each of those Tweets. Now,  

00:07:16,080 --> 00:07:22,080
the right-hand side shows a screenshot of what it  looks like for me if I'm in my browser in Firefox  

00:07:22,080 --> 00:07:30,080
and open up my web inspector. If I open up the  Tweet they shared in the last slide, you can  

00:07:30,080 --> 00:07:34,400
see that on the left-hand side there is some  HTML code that corresponds to that Tweet. 

00:07:35,040 --> 00:07:44,080
And so, the way that RSelenium helped me in this  process was essentially running a browser that I  

00:07:44,080 --> 00:07:50,720
would travel to each of these individual Tweets,  find the picture, find the image that was attached  

00:07:50,720 --> 00:07:58,000
and then take out that -- or scrape that alt text  attribute. And so, in this particular screenshot,  

00:07:58,000 --> 00:08:02,560
we can also see the alt text for the image  underneath the image itself in a block  

00:08:02,560 --> 00:08:10,640
of text. And that's visible to us if because we  -- there are certain extensions you can small in  

00:08:10,640 --> 00:08:14,800
your browser that lets you see the alt text for  different images. That's what is showing up here. 

00:08:18,000 --> 00:08:24,880
So, what did we find after scraping? We found  that over the 3 years of the TidyTuesday project  

00:08:24,880 --> 00:08:31,520
there were over 7,000 data viz Tweets and  only 215, or 3% of them, had alt text.  

00:08:34,240 --> 00:08:40,160
Participation in TidyTuesday has increased over  time. But as we can see from these line graphs,  

00:08:40,160 --> 00:08:45,600
the use of alt text is recent and remains really  low. So, what we see on the right-hand side  

00:08:45,600 --> 00:08:52,000
is a line graph where there's a gray line  corresponding to all the Tweets submitted  

00:08:52,000 --> 00:08:57,520
as part of this project over time. And then  there's another line that is a darker color.  

00:08:58,640 --> 00:09:05,200
And it shows the -- which of those Tweets  also have alt text attached to them. So,  

00:09:05,200 --> 00:09:09,920
we can see that for the most part, there has been  increased participation in the project over time,  

00:09:09,920 --> 00:09:16,560
but there hasn't been much change in how alt  text is used in this context except for at the  

00:09:16,560 --> 00:09:20,800
end where we see this spike. And so, that corresponds to  

00:09:21,920 --> 00:09:26,320
a conversation that was had on Twitter when Liz  and I found out we would be presenting at this  

00:09:26,320 --> 00:09:31,920
talk today and there was some conversation  about our preliminary findings and some  

00:09:31,920 --> 00:09:38,640
changes that were implemented because of that. So, Liz, could you tell us where else alt text  

00:09:38,640 --> 00:09:43,760
is missing in addition to this project? Elizabeth: Yeah. There's a lot of great  

00:09:43,760 --> 00:09:49,680
open source material, books and tutorials  available on the Internet. And they're all  

00:09:50,480 --> 00:09:54,640
wonderfully searchable by Google so that you can  answer the questions you have while you're coding.  

00:09:56,000 --> 00:10:02,240
But many of them are lacking alt text  for their data visualizations. And also,  

00:10:02,240 --> 00:10:09,360
sometimes even display code as PNG images which  can't be read by a screen reader. This problem  

00:10:09,360 --> 00:10:15,360
is also found with Bookshare, which is the  nonprofit that provides access to electronic  

00:10:17,120 --> 00:10:23,760
books for people with print disabilities. They  work with publishers who often also don't provide  

00:10:25,840 --> 00:10:28,880
alt text for images. They  don't provide it themselves.  

00:10:29,680 --> 00:10:37,280
And, again, sometimes the actual code snippets  are missing from those books and manuals. 

00:10:44,080 --> 00:10:49,120
A lot of R package -- not a lot -- but some R  package documentation has data visualizations  

00:10:49,120 --> 00:10:58,560
in to show you how these interesting new Tidy  verse kind of graphs kind of look and come out.  

00:11:00,160 --> 00:11:04,160
And it's really difficult to figure out  as someone who is trying to display data  

00:11:04,160 --> 00:11:11,680
and you can't see those what the code actually  produces. There are also a lot of R blogs that  

00:11:11,680 --> 00:11:17,840
have educational components. But again, are  lacking alt text or lacking access to the code. 

00:11:19,440 --> 00:11:24,000
This happens both at the individual level  as we mentioned with things like blogs,  

00:11:24,000 --> 00:11:25,440
and also at the corporate level  

00:11:26,160 --> 00:11:32,960
where RStudio's documentation on the website  is often missing these accessibility features. 

00:11:38,800 --> 00:11:50,880
So, one of the things, once we had scraped  these alt-texts, we were interested in asking,  

00:11:50,880 --> 00:11:56,800
what about them makes them good? And what  them makes them effective? And based on my  

00:11:56,800 --> 00:12:09,840
experience online and reading media and also going  through each of the alt-texts that Silvia scraped,  

00:12:14,000 --> 00:12:19,040
the first item that I felt was really  important is that something was shared  

00:12:19,040 --> 00:12:28,560
about the -- what the data is showing. What is the  meaning behind conveyed? 34 percent included that.  

00:12:31,840 --> 00:12:39,440
Twenty-eight% included what variables were  on the axes of the data visualization.  

00:12:44,000 --> 00:12:50,560
Twelve percent had some indication of the scale  of the axes or a way to find out what the scale  

00:12:50,560 --> 00:12:59,920
was from the description of the data. And this one  is pretty important too. You need to tell us what  

00:12:59,920 --> 00:13:07,040
kind of graph it is. Is it a line plot? Is it a  scatterplot? Is it a Q2 plot? Or a residual plot?  

00:13:08,640 --> 00:13:16,640
And 56% provided that. I would like to also add  that there are a lot of really cool modern new  

00:13:16,640 --> 00:13:24,000
and also some very specialized kinds of graphics,  especially within TidyVerse that if you're using  

00:13:24,000 --> 00:13:30,400
something that's a little less common, it would be  better to provide some orientation to how the data  

00:13:30,400 --> 00:13:34,840
visualization works and what it's saying. Silvia:  

00:13:37,120 --> 00:13:43,440
So, we saw in the line graph a couple slides ago  that there seems to be something changing now.  

00:13:44,000 --> 00:13:48,240
And so, we wanted to ask, you know, is the time,  you know, has it started shifting? Are we sort  

00:13:48,240 --> 00:13:54,720
of noticing changes outside of this as well? So, we know that conversations are happening  

00:13:54,720 --> 00:14:01,520
within R in different spaces. What we're doing  is using the conference that Liz is a part of,  

00:14:01,520 --> 00:14:06,160
and one of the organizers and volunteers  for. The conference itself has been very  

00:14:06,160 --> 00:14:11,520
intentional about having accessibility practices  in the conference. In the way that's delivered  

00:14:11,520 --> 00:14:18,720
and making it as accessible as possible for  participants attending as speakers or that  

00:14:18,720 --> 00:14:24,480
are just attending to participate in the different  talks and tutorials and as Liz mentioned earlier,  

00:14:24,480 --> 00:14:29,840
the MiR is also where we are having these  conversations like we are having today.  

00:14:30,560 --> 00:14:36,960
There's an accessibility community within that  community that Liz and I are a part of where we  

00:14:37,680 --> 00:14:42,880
regularly meet and discuss these things. And then  within TidyTuesday in particular, as I mentioned,  

00:14:42,880 --> 00:14:47,600
when we shared our preliminary findings on  Twitter, the leader of the social project,  

00:14:47,600 --> 00:14:54,640
Tom Mock, was able to add a whole section to the  TidyTuesday repository about -- or inviting and  

00:14:54,640 --> 00:14:58,800
encouraging participants to include alt text  in the data visualizations that they share  

00:14:58,800 --> 00:15:03,680
on Twitter. It includes resources as well so  participants can have a place to start from. 

00:15:04,560 --> 00:15:08,960
And then more broadly, we also have noticed  conversations happening in other places. So,  

00:15:08,960 --> 00:15:14,800
data visual society had a conference this  year, outlier conf, that had talks about  

00:15:14,800 --> 00:15:21,600
data viz and accessible, and a11y  are having these conversations. 

00:15:23,520 --> 00:15:32,080
There is changes in tooling within R. One good  example is within R mark down, there is now the  

00:15:32,080 --> 00:15:38,800
ability to add alt text to a code chunk -- as an  option to a code chunk. So, whatever the graphical  

00:15:38,800 --> 00:15:45,440
output that is producing, it can come equipped  with the alt extracap it. As we might add  

00:15:45,440 --> 00:15:54,000
a figure caption, now we can add an alt text tag  to that graphical output. But it is important to  

00:15:54,000 --> 00:15:58,080
note that these changes are great, you know?  But they are just starting and they're slow. 

00:15:59,920 --> 00:16:06,400
So, what can you do? You can make your data  visualizations accessible everywhere that you can.  

00:16:06,400 --> 00:16:11,120
So, this includes adding alt-text to  websites, journalism, scientific publishing,  

00:16:11,120 --> 00:16:16,720
social media and other spaces. And in situations  where alt text isn't available in your particular  

00:16:16,720 --> 00:16:21,840
document or platform, find creative ways  to describe your visualizations in words.  

00:16:23,360 --> 00:16:28,800
Liz and I have provided a short list here of  some resources including a great blog post by Amy  

00:16:28,800 --> 00:16:40,960
Cecil, new chartability workbook by Frank -- and  another resource for -- and a few links specific  

00:16:40,960 --> 00:16:48,640
to Twitter. How do I add alt-text to the images  that I attach? How can I see the alt-text if I  

00:16:48,640 --> 00:16:54,080
want to? What kinds of extensions are available?  Are there extensions label is to remind me to  

00:16:54,080 --> 00:16:59,760
attach alt-text? There are resources specific  to Firefox, but others will have them as well.  

00:17:02,080 --> 00:17:08,720
Thanks so much. Resources, Liz's analysis and  also a link to the TidyTuesday alt-text package  

00:17:08,720 --> 00:17:13,600
that we used for this analysis is available  on the link in this slide. Thanks so much,  

00:17:13,600 --> 00:17:19,280
and we're excited to take some questions. Serah: Fantastic. Thank you so much for sharing  

00:17:19,280 --> 00:17:26,000
this amazing and important work that you have been  doing. And tips for us all to do better. There's a  

00:17:26,000 --> 00:17:32,240
question -- there's lots of questions, but I think  we only have time for one. And so, Jess asks,  

00:17:32,240 --> 00:17:38,560
are there any good resources for testing what  the screen reader experience is like for people?  

00:17:38,560 --> 00:17:42,040
Or accessibility in general? Silvia:  

00:17:45,200 --> 00:17:52,720
Yeah. I think a great place to explore would  be some of those a11y resources. A11y stands  

00:17:52,720 --> 00:17:56,720
requester "Accessibility." There's a lot of  conversations there that happen within the  

00:17:56,720 --> 00:18:01,840
context of web development and user testing  and screen reader testing is sort of part  

00:18:01,840 --> 00:18:05,440

YouTube URL: https://www.youtube.com/watch?v=DxLkv2iRdf8


