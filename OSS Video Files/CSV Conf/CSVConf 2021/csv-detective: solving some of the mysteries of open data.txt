Title: csv-detective: solving some of the mysteries of open data
Publication date: 2021-05-19
Playlist: CSVConf 2021
Description: 
	Presenters:  Anthony Auffret, Geoffrey Aldebert and Pavel Soriano-Morales 

Over the last few years, the emphasis on data quality has evolved from being a nice to have to an absolute necessity. As already stated in past editions of this venue, the quality of open data is essential to *truly fulfill its promise as a channel of empowerment*. In practical terms, data quality for tabular data entails, among other tasks, checking the structural integrity and schematic consistency of its contents. In order to satisfy these requirements, we need to look into the files content and first determine whether our files are indeed CSV files, and secondly, and more importantly, we want to discover the type of data we have in order to properly validate it and thus evaluate and then hopefully improve the quality of our datasets. In this talk, we present our work on the automatic detection of data types within the columns of CSV files. Going beyond classic computer science data types (float, integer, date), we are also interested in detecting more specific in-domain data categories. Specifically, given a CSV file, we look into its columns and determine whether it contains postal codes, enterprise identifiers, geographic coordinates, days of the week, and so on. We will show how applied to data from the French open data platform that we maintain (data.gouv.fr.), these specific types allow users to better control data in order to join or link datasets between them. We see our work as a first step and important stepping stone towards data integrity and validation checks while also facilitating the discoverability and contextualization of open datasets. Our approaches are evaluated by annotating and testing over thousands of columns extracted from more than 15 000 CSV files found in data.gouv.fr.
Captions: 
	00:00:03,920 --> 00:00:10,000
My name is Geoffrey. Thanks for inviting us at  this year's csv,conf. Which is a great conference  

00:00:10,000 --> 00:00:16,800
and we are glad to be here. Today we will have  the first talk about the theme solving some of the  

00:00:16,800 --> 00:00:22,880
mysteries of open data. But first of all, we will  talk about ourselves and where we come from. So,  

00:00:24,480 --> 00:00:32,880
we are members, Anthony, Pavel and I of a  department Etalab and is under the authority  

00:00:32,880 --> 00:00:40,000
of the French prime minister. There is many teams  inside of this department. Like an artificial  

00:00:40,000 --> 00:00:48,720
intelligence laboratory, but also a team which is  the name of the French open data platform. And the  

00:00:48,720 --> 00:00:55,280
French open data platform is addressed to every  French Administration that needs to use this data  

00:00:55,280 --> 00:01:04,800
in open data. And as of today, we have more than  36,000 datasets and more than 200,000 resources.  

00:01:06,400 --> 00:01:09,840
And my these different  resources, we have many CSVs.  

00:01:13,840 --> 00:01:21,680
And anybody can deposit data on the platform. The administrations, of course. But also  

00:01:21,680 --> 00:01:28,320
every citizen that needs to share data, it's  possible. So, as you can imagine, the data that is  

00:01:28,320 --> 00:01:33,920
published inside of the open data platform could  be structural, which is very much appreciated.  

00:01:33,920 --> 00:01:42,080
The administration which are fishing through the  data are often a big IT department, which referred  

00:01:42,080 --> 00:01:50,720
to the data format or data standout. But we also  have published many data that is not structured.  

00:01:51,840 --> 00:01:56,560
Why we have that? Because we don't to want  prevent data to be published if we have data  

00:01:56,560 --> 00:02:02,640
even if we are not structured, we can  allow the possibility to publish it. 

00:02:03,680 --> 00:02:11,040
As a result, we have many data quality issues.  So, I will talk about these data quality issues  

00:02:11,040 --> 00:02:18,640
for the presentation. And how can we explore  the unstructured data? So, how can we  

00:02:18,640 --> 00:02:24,480
explore unstructured data? This is important  for different users on open data platforms which  

00:02:24,480 --> 00:02:30,960
want to know which is inside a specific file,  a specific resource. And there is many tools,  

00:02:30,960 --> 00:02:39,040
there are tools existing to retrieve simple  data types. Like if there's an integer, a date,  

00:02:39,040 --> 00:02:47,440
a string, for instance, we have tested recently  a profiling and we will talk a little bit  

00:02:47,440 --> 00:02:52,720
about our experiment around these tools. And this is a great tool to find simple  

00:02:52,720 --> 00:03:00,880
data types. But we also would like to inform users  about complex data types with the business meaning  

00:03:00,880 --> 00:03:08,640
for the different users and for further reuses.  What is complex data types? Complex data types  

00:03:09,440 --> 00:03:15,920
could be information related to a concrete thing.  So, for instance, the identifier of a company,  

00:03:15,920 --> 00:03:23,600
of a society. It's complex data types and we would  like to retrieve and be able to access the data  

00:03:23,600 --> 00:03:30,400
and be sure that a specific document is talking  about information relating to a specific company.  

00:03:30,400 --> 00:03:37,120
There is many complex data types like information  regarding locality like latitude, longitude,  

00:03:38,080 --> 00:03:43,760
for instance, zip code or region name, region  code, department code, et cetera, et cetera. Which  

00:03:43,760 --> 00:03:50,720
is related to French localities, for instance. And more generally, we would like to  

00:03:50,720 --> 00:03:57,360
be able to retrieve some common codes used in  administration -- in French administration. For  

00:03:57,360 --> 00:04:03,760
instance, we would like to be able to retrieve  identifier of French health care establishment  

00:04:04,720 --> 00:04:13,200
which is used to be able to analyze  the data around COVID-19, for instance.  

00:04:13,200 --> 00:04:21,920
And so many, there is also so many different  tools existing. And that is at the beginning  

00:04:21,920 --> 00:04:27,120
of our thought and the development of the  specific tools that we called CSV Detective.  

00:04:28,800 --> 00:04:35,360
We will talk about this tool next and how  can we use it and how can -- how does it  

00:04:35,360 --> 00:04:43,040
work and how can we evaluate it? So, for that,  I would like Anthony to speak on the next slide. 

00:04:44,320 --> 00:04:50,480
Anthony: Thank you, Geoffrey, hi, everyone.  So, yes, I will talk to you a bit more about  

00:04:50,480 --> 00:04:57,600
how -- what is csv-detective and how it works.  First of all, csv-detective is a Python package  

00:04:58,720 --> 00:05:11,280
made by Etalab. Next one. And what does it do  exactly? First step, it's -- it manages to -- to  

00:05:11,280 --> 00:05:18,320
get some classical information that we are used to  have like parsing information. Encoding, headers,  

00:05:18,320 --> 00:05:27,920
separators and others so on, specific to  the CSV file. What's interesting for us  

00:05:28,480 --> 00:05:36,240
here is that for each column, it will detect a  likelihood score for each complex data type that  

00:05:36,800 --> 00:05:42,960
csv-detective can detect. For instance,  if we have a given column in a CSV file,  

00:05:44,240 --> 00:05:53,200
we would have some thing like we have a 70% likely  that this column is a country name, for instance. 

00:05:53,840 --> 00:06:00,880
And on the right, you can see some example of  the types that csv-detective can detect. So,  

00:06:00,880 --> 00:06:07,920
we have very types that we could have in  other countries like latitude, longitude,  

00:06:09,520 --> 00:06:17,280
JSON format, et cetera, et cetera. But we also  have very specific French formats that must  

00:06:17,280 --> 00:06:25,920
respect a very specific format like here we have  the French companies, identifiers. So, we have  

00:06:25,920 --> 00:06:33,280
very specific rules to be able to detect these  type of complex types. And in the end, all this  

00:06:33,280 --> 00:06:42,560
is returned in a JSON-like format. Next slide. So, I have been talking about the likelihood  

00:06:42,560 --> 00:06:48,480
score. But in fact, it's not just one, but three  different scores that we have. Because basically,  

00:06:48,480 --> 00:06:56,000
we tried to -- to leverage every information we  have in the file to -- to be able to identify  

00:06:56,000 --> 00:07:01,840
what type, what complex type, is in each column.  So, the first one in the field scores. So,  

00:07:03,120 --> 00:07:07,440
it rearranges what is in the content  of the column. I won't do it here,  

00:07:07,440 --> 00:07:14,800
I will do it on the next slide. The second one  is what we call the label based on the ed had  

00:07:16,640 --> 00:07:25,120
her. The data type is more or less detected  and we can have the clues in the header,  

00:07:25,120 --> 00:07:33,280
the column title. Here, for instance, we have  a specific list of column titles, examples.  

00:07:33,280 --> 00:07:43,040
And if there is a perfect match with the list that  we have, we give a score of 100%. So, one. And the  

00:07:43,040 --> 00:07:50,240
header of the column only contains the list that  -- one of the words in the list that we have but  

00:07:50,240 --> 00:08:01,920
is not an exact match, we only give a 50% score. The last score is a machine learning score. We  

00:08:01,920 --> 00:08:10,000
designed the machine learning model thanks to  annotated data to be able to detect automatically  

00:08:11,360 --> 00:08:22,400
what -- what complex type is in a CSV file score.  But we must rely on the first two ones because  

00:08:22,400 --> 00:08:28,960
we need to -- it's still a work in progress so we  need to improve it to actually use it afterwards. 

00:08:31,360 --> 00:08:38,400
Next, and just one word about the scores,  sometimes we also combine them to -- to get  

00:08:38,400 --> 00:08:46,880
more -- to get better results in the end. So, now  just an example, a very concrete example, of how  

00:08:46,880 --> 00:08:52,320
csv-detective can work on the field score. It's  a score based on the content of the column. So,  

00:08:52,320 --> 00:08:57,840
the actual values that were in the column. So,  for instance, here we have a column that is  

00:08:57,840 --> 00:09:06,800
called code_commune_insee. We see how we can match  the common data types. It's a code_commune_insee,  

00:09:07,760 --> 00:09:17,440
which is a code in France. You can see,  it analyzes the first rows of the column.  

00:09:18,400 --> 00:09:26,080
And for each element of the column, it will try to  match with some specific rules. So, for instance,  

00:09:26,080 --> 00:09:32,720
in this case, we have the first steps that  is checking thanks to a regX that the content  

00:09:33,520 --> 00:09:43,680
corresponds to a given -- a given format. But we  also have, after the second step, that matches the  

00:09:43,680 --> 00:09:52,160
element with comprehensive list of all possible  zip codes in France. Because not all numbers that  

00:09:52,160 --> 00:10:01,440
matches with the regX can be actual zip codes. So, that we have the most -- most perfect match  

00:10:02,160 --> 00:10:08,720
here, the most specific rule to detect  the code_commune_insee. On the left,  

00:10:08,720 --> 00:10:14,400
you can see the three don't respect the  specific formats and specific rules that we set.  

00:10:15,200 --> 00:10:24,480
So, we have only eight elements that respect  the actual format. Eight over 11. So, the final  

00:10:24,480 --> 00:10:32,400
score would be 73% in this case. And in real  life, of course, we analyze more than 11 rows. 

00:10:33,360 --> 00:10:40,160
So, this is how it works. But the question that  comes after, of course, is does it actually work?  

00:10:41,920 --> 00:10:50,400
And what we do to actually evaluate how it works,  next slide, yeah. Thanks. Is that we compare what  

00:10:50,400 --> 00:10:57,360
we have. We apply csv-detective on the CSV files  and CSV columns that we have in that other.  

00:11:01,280 --> 00:11:07,840
We look at the csv-detective and  compare with manually annotated data  

00:11:08,800 --> 00:11:15,200
that we annotated ourselves, by human beings.  Then thanks to it, we get some improvements  

00:11:16,560 --> 00:11:22,640
in the data we show you afterwards to identify  flows. And thanks to this -- these scores,  

00:11:23,680 --> 00:11:29,200
we can correct the detection method,  the rules that we use to try to improve  

00:11:29,200 --> 00:11:36,400
csv-detective. And we look to see if we had  an actual improvement of the Python package. 

00:11:40,240 --> 00:11:46,320
So, we actually used two -- two kinds of  methods. The first one is a numerical method  

00:11:48,240 --> 00:11:52,080
which is very close to machine learning  method to evaluate classification models.  

00:11:52,880 --> 00:11:58,320
We use precision and recall. So, precision  answers the question, when I predict complex type,  

00:11:59,360 --> 00:12:04,080
a given complex type, what is the likelihood  that my prediction is actually correct? So,  

00:12:04,080 --> 00:12:10,800
it evaluates prediction quality in the recall.  It actually evaluates and answer the question,  

00:12:10,800 --> 00:12:14,240
what is the percentage of actual  columns that were annotated  

00:12:15,600 --> 00:12:23,040
on the given type that were correctly detected  by csv-detective? And it evaluates the prediction  

00:12:23,040 --> 00:12:28,320
comprehensiveness. We don't use accuracy in  this case because it's not relevant here.  

00:12:30,480 --> 00:12:36,480
The main reason is we have unbalanced data in our  case. And you can see on the right that we have  

00:12:39,360 --> 00:12:50,000
mostly good results except for a specific complex  types like address, department code, city name and  

00:12:50,000 --> 00:12:55,120
so on. Just to be using some examples. Serah: You have five minutes. 

00:12:56,080 --> 00:13:06,480
Anthony: The next we use is the confusion matrix.  We can look at complex types, csv-detective  

00:13:06,480 --> 00:13:12,720
confuses with other types. Here we have the region  code that's often confused with the development  

00:13:12,720 --> 00:13:21,440
code. We have trouble differentiating latitude  and longitude. JSON types are not well-detected.  

00:13:22,560 --> 00:13:27,520
Et cetera, et cetera. So now you know how it works. And I will let  

00:13:27,520 --> 00:13:33,840
Pavel talk about the future of csv-detective. Pavel: Yes, thank you very much, Anthony.  

00:13:35,200 --> 00:13:40,480
So, as you heard already, this is ongoing  work. We have some plans to try to improve it  

00:13:41,280 --> 00:13:48,720
in the short-term and in the medium term.  We have these designed in this fashion.  

00:13:48,720 --> 00:13:53,520
The first would be the methodology improvement.  We would like to improve the model and the system,  

00:13:55,520 --> 00:14:02,480
get more information from our columns, from our  CSV files in order to improve the performance?  

00:14:02,480 --> 00:14:09,120
How can we mix the different scores that we use  in order to show to the scores to ourselves why a  

00:14:09,120 --> 00:14:17,280
decision is taken and how can we weight different  scores into the best optimal score. We would like  

00:14:17,280 --> 00:14:23,600
to go forward with the machine learning. But for  now, we still have to get more data to make it  

00:14:23,600 --> 00:14:29,440
more robust. And according to new features,  we would like to -- one, we would like to  

00:14:29,440 --> 00:14:35,200
detect multi-complex columns. That is columns that  contain different types of data. We would like to  

00:14:35,200 --> 00:14:46,480
generalize to more file types. Going -- to Excel  files, for example. Or proprietary files. And we  

00:14:46,480 --> 00:14:53,040
would like to -- maybe we would like to allow for  people to create their own rules and -- and upload  

00:14:53,040 --> 00:14:58,800
their own rules to use csv-detective. Right  now we work with French data and we are  

00:14:58,800 --> 00:15:03,120
working with the French government data. But, of course, this tool could be used for  

00:15:03,760 --> 00:15:08,560
several use cases. Several other  open data platforms. Next slide.  

00:15:10,800 --> 00:15:15,840
And so, finally, to sum it up, very quickly, you  have to keep something in your head before going  

00:15:15,840 --> 00:15:23,680
to sleep. We created a tool that takes CSV and  gives what type of data is within the CSV. We  

00:15:23,680 --> 00:15:31,440
believe that it's an important task to further  downstream data cleaning tasks. We use rules  

00:15:31,440 --> 00:15:38,240
and we use the columns to data -- right now --  to data mine the type of data we are getting. And  

00:15:38,240 --> 00:15:44,000
it's not easy. We have some challenges. We have a  lot of rules and this kind is non-maintainable. We  

00:15:44,000 --> 00:15:51,200
have to find a solution how to manage all these  rules and address this problem. And, of course,  

00:15:51,200 --> 00:16:00,560
data is dirty data. Of course, this is the reason  for the system is to look at data that was not  

00:16:00,560 --> 00:16:06,800
having what it says it has. This is some of our  main challenges. Next slide. So, thank you very  

00:16:06,800 --> 00:16:13,280
much. If you have any questions, don't hesitate,  this is our source code. It's open, of course.  

00:16:13,280 --> 00:16:19,440
And if you wanted to ask a question further on,  there's our emails. Thank you again. We are here  

00:16:19,440 --> 00:16:24,800
and we can answer your questions. Or try to. Serah:  

00:16:24,800 --> 00:16:31,600
Fantastic. Thank you so much Pavel and Anthony  and Geoffrey. We have one question in the  

00:16:31,600 --> 00:16:37,920
ask a question section. So, the question reads:  Can you give an example of how you will leverage  

00:16:37,920 --> 00:16:41,800
csv-detective for detecting in pipelines? Geoffrey:  

00:16:47,440 --> 00:16:55,040
Yes, so, we can use the csv-detective to  -- to be able to analyze automatically  

00:16:55,040 --> 00:17:02,480
every resource that is published inside our  French open data platform. And with this,  

00:17:02,480 --> 00:17:11,920
we can -- we can inform people and users under  the data sheet what is contained inside the CSV  

00:17:11,920 --> 00:17:19,600
file. That is a pipeline that we can imagine.  When someone published data, a CSV file,  

00:17:19,600 --> 00:17:26,880
automatically, it triggers a csv-detective  analysis. And the output JSON format will be  

00:17:26,880 --> 00:17:35,120

YouTube URL: https://www.youtube.com/watch?v=iCjmiya6JMU


