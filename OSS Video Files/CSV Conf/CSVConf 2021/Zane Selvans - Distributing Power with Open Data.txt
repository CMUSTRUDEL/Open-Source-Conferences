Title: Zane Selvans - Distributing Power with Open Data
Publication date: 2021-05-20
Playlist: CSVConf 2021
Description: 
	Some lessons learned from our work getting electricity system data into the hands of activists and researchers who are trying to shift power in US energy policy making. Under what circumstances can data help drive policy changes, and how does that change happen? Is open data always preferable in an advocacy context? What does it really mean for data to be accessible to advocates? US energy regulation is extremely technocratic, and largely happens out of public view. For years we’ve tried to address the information asymmetry that exists between advocates and large utilities by publishing cleaned open datasets detailing the inner workings of the US electricity system. We use Python data science tools to prepare the data and they serve us well, but most advocates -- even those with a lot of domain expertise -- are either hardcore spreadsheet users, or are just starting to dabble in Jupyter Notebooks. Now that we have a decent data pipeline, we are turning our attention to improving the data’s accessibility for our target users, using Datasette, Docker containers that run Jupyter, and Intake data catalogs.
Captions: 
	00:00:03,439 --> 00:00:05,680
okay

00:00:03,840 --> 00:00:07,520
hi everyone i'm zane sullivans and this

00:00:05,680 --> 00:00:09,040
is christina cosnell and we are both

00:00:07,520 --> 00:00:10,960
founders of the catalyst cooperative

00:00:09,040 --> 00:00:12,400
which is a little worker owned data

00:00:10,960 --> 00:00:14,559
wrangling organization

00:00:12,400 --> 00:00:16,800
we mostly work with u.s energy system

00:00:14,559 --> 00:00:17,920
data uh supporting researchers and clean

00:00:16,800 --> 00:00:19,920
energy advocates

00:00:17,920 --> 00:00:21,600
and also just fyi we are currently in

00:00:19,920 --> 00:00:22,960
the process of hiring for two positions

00:00:21,600 --> 00:00:24,400
so if you're interested in the kinds of

00:00:22,960 --> 00:00:26,000
things that we're talking about here

00:00:24,400 --> 00:00:27,680
um and the tools that we're using please

00:00:26,000 --> 00:00:30,240
get in touch with us afterwards

00:00:27,680 --> 00:00:31,119
um and i'll head off to christina to

00:00:30,240 --> 00:00:33,679
kind of set up

00:00:31,119 --> 00:00:33,679
our story

00:00:35,840 --> 00:00:39,600
yeah welcome everyone um thanks for

00:00:37,600 --> 00:00:42,000
having us um

00:00:39,600 --> 00:00:43,120
so this slide looks a little janky right

00:00:42,000 --> 00:00:45,840
now but hopefully it'll

00:00:43,120 --> 00:00:46,559
resolve um so so yeah in this talk we're

00:00:45,840 --> 00:00:48,399
going to go over

00:00:46,559 --> 00:00:49,680
a little bit of context setting and then

00:00:48,399 --> 00:00:51,520
go over when

00:00:49,680 --> 00:00:53,120
when we think data is actually useful in

00:00:51,520 --> 00:00:55,680
a change-making context

00:00:53,120 --> 00:00:57,680
um actually making data work for change

00:00:55,680 --> 00:01:00,719
and getting data into the hands

00:00:57,680 --> 00:01:01,760
getting data into the right hands and

00:01:00,719 --> 00:01:02,640
then we'll leave some time at the end

00:01:01,760 --> 00:01:05,840
for questions so

00:01:02,640 --> 00:01:05,840
uh be prepared

00:01:05,920 --> 00:01:09,200
so yeah a little bit that little

00:01:07,040 --> 00:01:09,600
background catalyst sprung out of an

00:01:09,200 --> 00:01:11,200
advocate

00:01:09,600 --> 00:01:13,040
out of advocacy and our need to have

00:01:11,200 --> 00:01:14,159
better access to data about the energy

00:01:13,040 --> 00:01:16,400
system

00:01:14,159 --> 00:01:17,920
we were involved in a group of advocates

00:01:16,400 --> 00:01:18,960
that were pushing for early retirement

00:01:17,920 --> 00:01:21,680
of coal plants

00:01:18,960 --> 00:01:22,080
and our strategy was to target expensive

00:01:21,680 --> 00:01:24,840
and

00:01:22,080 --> 00:01:26,880
unprofitable coal plants for early

00:01:24,840 --> 00:01:28,799
retirement

00:01:26,880 --> 00:01:30,240
that for us looked like a lot of hand

00:01:28,799 --> 00:01:31,840
scraping of pdfs

00:01:30,240 --> 00:01:33,360
and cobbling together different data

00:01:31,840 --> 00:01:35,680
sets from different government

00:01:33,360 --> 00:01:37,439
sources it was very manual and tedious

00:01:35,680 --> 00:01:38,880
but the resulting advocacy was very

00:01:37,439 --> 00:01:41,119
positive

00:01:38,880 --> 00:01:42,560
and our larger group went to expand and

00:01:41,119 --> 00:01:44,079
look at more than one utility and we

00:01:42,560 --> 00:01:46,159
realized at that point

00:01:44,079 --> 00:01:48,079
we needed to employ a different tactic

00:01:46,159 --> 00:01:49,840
so we created catalyst to tackle this

00:01:48,079 --> 00:01:54,640
larger mission of data access and

00:01:49,840 --> 00:01:57,520
automate a lot of this data curation

00:01:54,640 --> 00:01:59,040
so uh let's start by figuring out when

00:01:57,520 --> 00:02:01,600
something actually is a data problem or

00:01:59,040 --> 00:02:04,079
could be a data problem

00:02:01,600 --> 00:02:04,960
i know that we all love data here myself

00:02:04,079 --> 00:02:07,200
included

00:02:04,960 --> 00:02:09,039
but not every problem is a data or

00:02:07,200 --> 00:02:10,000
technical like has a data or technical

00:02:09,039 --> 00:02:12,560
solution

00:02:10,000 --> 00:02:13,120
often the problem is political uh you

00:02:12,560 --> 00:02:15,840
know

00:02:13,120 --> 00:02:19,280
egalitarian solutions are very often not

00:02:15,840 --> 00:02:22,319
what is best for those wielding power

00:02:19,280 --> 00:02:24,239
and in a similar vein we all have deeply

00:02:22,319 --> 00:02:26,640
integrated ideological frameworks that

00:02:24,239 --> 00:02:29,680
keep us from integrating data lessons

00:02:26,640 --> 00:02:30,239
that go against our worldview even when

00:02:29,680 --> 00:02:33,760
there is

00:02:30,239 --> 00:02:36,160
moving data story to be told

00:02:33,760 --> 00:02:37,519
the work of communication dissemination

00:02:36,160 --> 00:02:39,440
and advocacy

00:02:37,519 --> 00:02:41,280
is you know it doesn't that kind of

00:02:39,440 --> 00:02:43,120
stuff doesn't just happen spontaneously

00:02:41,280 --> 00:02:44,720
when you have good data

00:02:43,120 --> 00:02:46,319
data can be a piece of a broader

00:02:44,720 --> 00:02:49,200
strategy but it doesn't speak

00:02:46,319 --> 00:02:50,080
on its own data also doesn't make

00:02:49,200 --> 00:02:52,319
decisions for

00:02:50,080 --> 00:02:54,000
don't don't make decisions for us we use

00:02:52,319 --> 00:02:54,879
data to be informed about the world

00:02:54,000 --> 00:02:56,720
around us

00:02:54,879 --> 00:02:58,400
but ultimately we make decisions based

00:02:56,720 --> 00:03:00,480
on our values

00:02:58,400 --> 00:03:02,640
and lastly you know we are fully open

00:03:00,480 --> 00:03:04,480
source shop and believe in that strongly

00:03:02,640 --> 00:03:05,760
but open data doesn't also necess

00:03:04,480 --> 00:03:08,000
doesn't necessarily

00:03:05,760 --> 00:03:08,800
produce the best positive societal

00:03:08,000 --> 00:03:11,440
results

00:03:08,800 --> 00:03:13,120
it very much depends on the context um

00:03:11,440 --> 00:03:14,480
you know who has the data currently what

00:03:13,120 --> 00:03:16,800
are their motivations

00:03:14,480 --> 00:03:19,680
uh what is the economic sustainability

00:03:16,800 --> 00:03:23,760
of navigating open data

00:03:19,680 --> 00:03:25,599
but um when when is data actually useful

00:03:23,760 --> 00:03:28,239
in a change-making setting

00:03:25,599 --> 00:03:29,760
if if decisions are happening in highly

00:03:28,239 --> 00:03:31,599
technocratic spaces

00:03:29,760 --> 00:03:33,040
it's often true that arming advocates

00:03:31,599 --> 00:03:36,319
with data can be vital for the

00:03:33,040 --> 00:03:36,319
credibility of their message

00:03:36,400 --> 00:03:39,680
also decision makers often at least

00:03:38,319 --> 00:03:42,159
purport to want

00:03:39,680 --> 00:03:43,120
to make data informed decisions so it

00:03:42,159 --> 00:03:46,159
can be in sport

00:03:43,120 --> 00:03:47,840
important to speak in that language but

00:03:46,159 --> 00:03:50,400
it's always important to keep in mind

00:03:47,840 --> 00:03:53,439
the rdp deep deep preference for

00:03:50,400 --> 00:03:55,599
confirmation bias

00:03:53,439 --> 00:03:56,799
another important indicator is when

00:03:55,599 --> 00:03:59,040
there is a lot of

00:03:56,799 --> 00:04:00,400
information asymmetry when one actor has

00:03:59,040 --> 00:04:04,080
all of the information

00:04:00,400 --> 00:04:05,519
and often um often comes with a lot of

00:04:04,080 --> 00:04:07,120
you know wielding

00:04:05,519 --> 00:04:08,560
wheeling power and decision-making

00:04:07,120 --> 00:04:11,040
ability

00:04:08,560 --> 00:04:13,599
in those instances data can can be very

00:04:11,040 --> 00:04:15,760
useful to level the playing field

00:04:13,599 --> 00:04:17,759
and in our work we found that even a

00:04:15,760 --> 00:04:20,880
little bit of data can go a long way

00:04:17,759 --> 00:04:22,479
into calling questions and calling into

00:04:20,880 --> 00:04:23,759
question the assumptions of incumbent

00:04:22,479 --> 00:04:25,840
actors

00:04:23,759 --> 00:04:28,479
data can also be very empowering for

00:04:25,840 --> 00:04:31,600
movements and bolster public support

00:04:28,479 --> 00:04:32,240
and lastly uh data unless there has to

00:04:31,600 --> 00:04:34,720
be some

00:04:32,240 --> 00:04:35,919
available public data about that's like

00:04:34,720 --> 00:04:37,199
pertinent to the question that you're

00:04:35,919 --> 00:04:39,759
trying to ask

00:04:37,199 --> 00:04:41,440
a availability doesn't necessarily mean

00:04:39,759 --> 00:04:42,400
accessibility which shane will get into

00:04:41,440 --> 00:04:44,240
next

00:04:42,400 --> 00:04:46,320
but if there is no available available

00:04:44,240 --> 00:04:48,400
public data you have a

00:04:46,320 --> 00:04:50,720
data collection problem not a curation

00:04:48,400 --> 00:04:50,720
problem

00:04:52,320 --> 00:04:56,560
so for in our context we saw the use for

00:04:55,840 --> 00:04:59,360
data and

00:04:56,560 --> 00:05:01,919
advocacy efforts but we saw many others

00:04:59,360 --> 00:05:04,080
struggling with the same problem

00:05:01,919 --> 00:05:05,759
advocates researchers and journalists

00:05:04,080 --> 00:05:07,039
alike were all trying to answer

00:05:05,759 --> 00:05:10,160
questions about the

00:05:07,039 --> 00:05:12,240
energy system only after talking with

00:05:10,160 --> 00:05:13,520
lots of folks and exploring other

00:05:12,240 --> 00:05:16,000
related initiatives

00:05:13,520 --> 00:05:16,720
did it become clear to us that there was

00:05:16,000 --> 00:05:19,759
enough

00:05:16,720 --> 00:05:22,160
support and enough need to support

00:05:19,759 --> 00:05:24,880
common infrastructure

00:05:22,160 --> 00:05:26,960
and all and that and that the extra

00:05:24,880 --> 00:05:29,680
effort required to create

00:05:26,960 --> 00:05:31,680
a common resource would be far less than

00:05:29,680 --> 00:05:32,479
the effort of the duplicated toil that

00:05:31,680 --> 00:05:35,759
we were all

00:05:32,479 --> 00:05:39,039
um individually carrying out

00:05:35,759 --> 00:05:40,080
i'll pass it over to zane okay so if

00:05:39,039 --> 00:05:41,280
you've determined that you do really

00:05:40,080 --> 00:05:42,400
have a data problem or

00:05:41,280 --> 00:05:45,039
like an issue that can be addressed by

00:05:42,400 --> 00:05:46,960
data and there's enough of a community

00:05:45,039 --> 00:05:47,280
to justify centralizing that effort what

00:05:46,960 --> 00:05:49,039
are you

00:05:47,280 --> 00:05:51,199
really trying to accomplish by taking on

00:05:49,039 --> 00:05:52,720
that responsibility for the community

00:05:51,199 --> 00:05:55,600
in a lot of cases in our experience it

00:05:52,720 --> 00:05:58,319
boils down to reducing overall user toil

00:05:55,600 --> 00:06:00,319
and what do i mean by toil um it's it's

00:05:58,319 --> 00:06:02,800
just manual repetitive work that can

00:06:00,319 --> 00:06:03,520
often be automated um with the right

00:06:02,800 --> 00:06:05,360
tools

00:06:03,520 --> 00:06:07,360
but frequently it ends up getting done

00:06:05,360 --> 00:06:09,680
on kind of a one-off basis

00:06:07,360 --> 00:06:11,360
um and it says data wrangling is

00:06:09,680 --> 00:06:12,479
notoriously filled with these tasks you

00:06:11,360 --> 00:06:13,840
know like things you can kind of do by

00:06:12,479 --> 00:06:15,919
hand once but

00:06:13,840 --> 00:06:17,600
automating them takes some extra effort

00:06:15,919 --> 00:06:18,160
and when you do it by hand you often end

00:06:17,600 --> 00:06:20,080
up

00:06:18,160 --> 00:06:21,280
generating unreproducible results which

00:06:20,080 --> 00:06:24,080
is not great in a

00:06:21,280 --> 00:06:24,080
legal context

00:06:24,479 --> 00:06:29,680
and when open data isn't analysis ready

00:06:27,759 --> 00:06:31,199
when every user has to do some of the

00:06:29,680 --> 00:06:32,880
same work before they can actually move

00:06:31,199 --> 00:06:34,000
on to solving their own problems the

00:06:32,880 --> 00:06:36,319
data isn't really free

00:06:34,000 --> 00:06:37,680
even if it is openly available or it's

00:06:36,319 --> 00:06:39,039
only free in the sense that the puppy in

00:06:37,680 --> 00:06:41,120
this warning sign is free

00:06:39,039 --> 00:06:42,319
it comes with its own set of obligations

00:06:41,120 --> 00:06:44,160
attached

00:06:42,319 --> 00:06:46,080
and that work that every single person

00:06:44,160 --> 00:06:46,960
has to do after they get the hands on

00:06:46,080 --> 00:06:49,440
the data

00:06:46,960 --> 00:06:50,800
is effectively a paywall it just might

00:06:49,440 --> 00:06:52,720
not be a financial one

00:06:50,800 --> 00:06:54,080
and in a lot of cases it'll prevent some

00:06:52,720 --> 00:06:55,440
people from being able to use the data

00:06:54,080 --> 00:06:56,560
at all either because they don't have

00:06:55,440 --> 00:06:58,240
the time or they don't have the

00:06:56,560 --> 00:06:59,440
specialized skills required to clean it

00:06:58,240 --> 00:07:01,120
up and use it

00:06:59,440 --> 00:07:02,639
so what we're trying to do is centralize

00:07:01,120 --> 00:07:04,720
that work do it once

00:07:02,639 --> 00:07:07,199
do it well and then share the benefits

00:07:04,720 --> 00:07:11,199
with everyone

00:07:07,199 --> 00:07:15,039
that can benefit from it however

00:07:11,199 --> 00:07:16,960
in in providing analysis ready data

00:07:15,039 --> 00:07:19,039
inevitably you end up having to make

00:07:16,960 --> 00:07:21,039
some choices on behalf of your users

00:07:19,039 --> 00:07:22,639
all messy data requires interpretation

00:07:21,039 --> 00:07:25,039
before you can actually work with it

00:07:22,639 --> 00:07:26,400
and cleaning reshaping removing outliers

00:07:25,039 --> 00:07:26,960
filling your missing values all of those

00:07:26,400 --> 00:07:29,120
things

00:07:26,960 --> 00:07:30,160
require judgment calls so there's this

00:07:29,120 --> 00:07:32,800
tension between

00:07:30,160 --> 00:07:34,400
making messy data easily usable and

00:07:32,800 --> 00:07:35,440
preserving the original contents and

00:07:34,400 --> 00:07:37,520
structure

00:07:35,440 --> 00:07:39,199
and that means that archiving the and

00:07:37,520 --> 00:07:40,880
documenting the process

00:07:39,199 --> 00:07:42,800
this collection of choices that you've

00:07:40,880 --> 00:07:44,479
made and applied to the data becomes at

00:07:42,800 --> 00:07:47,360
least as important as preserving the

00:07:44,479 --> 00:07:48,960
data products themselves

00:07:47,360 --> 00:07:50,080
otherwise people can end up losing trust

00:07:48,960 --> 00:07:51,360
in the data when they find some

00:07:50,080 --> 00:07:52,720
unexplained discrepancies

00:07:51,360 --> 00:07:54,000
or they may end up using the data in

00:07:52,720 --> 00:07:56,879
ways that just aren't really appropriate

00:07:54,000 --> 00:07:56,879
given where it came from

00:07:57,039 --> 00:08:01,360
so in our journey treating the data more

00:07:59,840 --> 00:08:02,479
and more like a process and less like a

00:08:01,360 --> 00:08:03,919
static end product

00:08:02,479 --> 00:08:05,120
has pushed us towards using a lot of

00:08:03,919 --> 00:08:06,160
kind of continuous integration and

00:08:05,120 --> 00:08:08,400
deployment tools

00:08:06,160 --> 00:08:09,199
which are often associated with larger

00:08:08,400 --> 00:08:11,919
data sets

00:08:09,199 --> 00:08:12,560
but as a small team we really we need to

00:08:11,919 --> 00:08:14,960
automate

00:08:12,560 --> 00:08:16,080
whatever we can because even a few small

00:08:14,960 --> 00:08:17,680
manual tasks

00:08:16,080 --> 00:08:19,120
will quickly accumulate and end up

00:08:17,680 --> 00:08:20,720
eating a big chunk of our time

00:08:19,120 --> 00:08:22,400
um that's definitely still a work in

00:08:20,720 --> 00:08:23,520
progress we you know have a lot of

00:08:22,400 --> 00:08:24,879
automation still to do

00:08:23,520 --> 00:08:27,280
but that's the general arc that we're

00:08:24,879 --> 00:08:29,280
trying to be on one side effect of this

00:08:27,280 --> 00:08:31,680
is that the tools that we find ourselves

00:08:29,280 --> 00:08:33,680
using day to day to produce the data

00:08:31,680 --> 00:08:35,039
have diverged substantially from the

00:08:33,680 --> 00:08:36,880
tools that most of our users are

00:08:35,039 --> 00:08:39,120
comfortable with in their day to day

00:08:36,880 --> 00:08:41,039
so it's important for us to make sure

00:08:39,120 --> 00:08:43,599
that we are bridging that technical gap

00:08:41,039 --> 00:08:44,399
and not just replacing um you know user

00:08:43,599 --> 00:08:46,720
toil with

00:08:44,399 --> 00:08:48,320
technical barriers that also prevent the

00:08:46,720 --> 00:08:51,120
people that we want to have access to

00:08:48,320 --> 00:08:51,120
the data from using it

00:08:51,760 --> 00:08:58,080
great so how how do we actually uh get

00:08:54,880 --> 00:08:59,519
access get people access to the data

00:08:58,080 --> 00:09:01,279
so we've more and more been thinking

00:08:59,519 --> 00:09:03,839
about uh data access

00:09:01,279 --> 00:09:05,040
as rings that build on one another these

00:09:03,839 --> 00:09:06,480
inner rings here

00:09:05,040 --> 00:09:08,240
contain the work that we've primarily

00:09:06,480 --> 00:09:10,560
been focused on so far

00:09:08,240 --> 00:09:12,880
we pull in disparate data sets into one

00:09:10,560 --> 00:09:16,160
standard database format that is clean

00:09:12,880 --> 00:09:18,560
and tidy and connected to each other

00:09:16,160 --> 00:09:20,720
this next this next set of rings

00:09:18,560 --> 00:09:22,399
encompasses all of the derived values

00:09:20,720 --> 00:09:23,600
that are generated from that processed

00:09:22,399 --> 00:09:26,240
data

00:09:23,600 --> 00:09:28,480
this is where a lot of the tension that

00:09:26,240 --> 00:09:31,839
between usable and pristine data that

00:09:28,480 --> 00:09:34,000
that zane mentioned comes in this

00:09:31,839 --> 00:09:36,080
uh we try to integrate vetted

00:09:34,000 --> 00:09:37,519
methodologies for calculating commonly

00:09:36,080 --> 00:09:40,320
derived values

00:09:37,519 --> 00:09:41,600
uh and impute missing values to fill in

00:09:40,320 --> 00:09:44,399
the gaps

00:09:41,600 --> 00:09:46,240
currently a lot currently all of the

00:09:44,399 --> 00:09:47,440
like more complex analysis and

00:09:46,240 --> 00:09:49,839
imputations are done

00:09:47,440 --> 00:09:51,519
in a software layer that reads data from

00:09:49,839 --> 00:09:53,920
our database

00:09:51,519 --> 00:09:55,120
we did this originally to have a very

00:09:53,920 --> 00:09:57,519
clear separation

00:09:55,120 --> 00:09:58,320
between that pristine data and the drive

00:09:57,519 --> 00:10:00,480
values

00:09:58,320 --> 00:10:02,160
but this does create an additional layer

00:10:00,480 --> 00:10:03,680
of friction for our users

00:10:02,160 --> 00:10:05,440
because they both have to be aware of

00:10:03,680 --> 00:10:08,320
these post etl tools

00:10:05,440 --> 00:10:10,079
and also be able to use them so we're

00:10:08,320 --> 00:10:12,959
currently planning on migrating

00:10:10,079 --> 00:10:14,079
a lot of these derived tables into the

00:10:12,959 --> 00:10:16,800
same database

00:10:14,079 --> 00:10:18,000
with well-labeled table names flags for

00:10:16,800 --> 00:10:20,240
different methodologies

00:10:18,000 --> 00:10:22,000
and confidence levels as well as

00:10:20,240 --> 00:10:23,760
accompanying metadata that explains a

00:10:22,000 --> 00:10:26,560
lot of like the prominence and

00:10:23,760 --> 00:10:28,240
assumptions that go into these but then

00:10:26,560 --> 00:10:29,760
how are users actually interacting with

00:10:28,240 --> 00:10:32,160
the data itself

00:10:29,760 --> 00:10:32,959
our our previous philosophy around this

00:10:32,160 --> 00:10:35,440
led us to

00:10:32,959 --> 00:10:36,800
employ a single access mode using

00:10:35,440 --> 00:10:38,480
frictionless data packages

00:10:36,800 --> 00:10:40,640
that could serve many different types of

00:10:38,480 --> 00:10:42,079
users but we've more and more come to

00:10:40,640 --> 00:10:42,720
come to believe that it is both

00:10:42,079 --> 00:10:45,440
preferable

00:10:42,720 --> 00:10:46,560
and possible to pipe these like standard

00:10:45,440 --> 00:10:48,079
outputs

00:10:46,560 --> 00:10:51,040
from our data processing pipeline into

00:10:48,079 --> 00:10:52,480
many different access modes that serve

00:10:51,040 --> 00:10:54,800
different types of users a little bit

00:10:52,480 --> 00:10:54,800
better

00:10:55,200 --> 00:11:00,079
these are these blue inner rings that

00:10:57,680 --> 00:11:01,519
contain our production pipeline

00:11:00,079 --> 00:11:02,959
so yeah these blue these blue and green

00:11:01,519 --> 00:11:03,760
inner rings contain the production

00:11:02,959 --> 00:11:08,320
pipeline

00:11:03,760 --> 00:11:11,920
which generates a a small number of um

00:11:08,320 --> 00:11:14,240
of outputs and

00:11:11,920 --> 00:11:15,040
those outputs include sqlite parquet

00:11:14,240 --> 00:11:17,200
files

00:11:15,040 --> 00:11:18,640
for our large a smaller and larger data

00:11:17,200 --> 00:11:20,640
sets respectively

00:11:18,640 --> 00:11:22,560
and metadata that's dynamically

00:11:20,640 --> 00:11:25,120
generated as well as docker containers

00:11:22,560 --> 00:11:27,680
which preserve the software environment

00:11:25,120 --> 00:11:29,600
these outputs will be built nightly

00:11:27,680 --> 00:11:31,519
they'll be

00:11:29,600 --> 00:11:33,279
they'll integrate you know lots of tests

00:11:31,519 --> 00:11:34,640
unit tests both unit tests and data

00:11:33,279 --> 00:11:36,240
validation tests

00:11:34,640 --> 00:11:38,399
and will be cloud accessible so they're

00:11:36,240 --> 00:11:40,399
ready for distribution

00:11:38,399 --> 00:11:42,000
and this this is the point at which we

00:11:40,399 --> 00:11:44,240
switch from our means of production

00:11:42,000 --> 00:11:45,519
to means of distribution i see the

00:11:44,240 --> 00:11:48,160
distribution modes as

00:11:45,519 --> 00:11:49,040
tentacles um flowing out from our data

00:11:48,160 --> 00:11:51,680
pipeline

00:11:49,040 --> 00:11:52,720
which which are able to be nimble and

00:11:51,680 --> 00:11:55,279
flexible

00:11:52,720 --> 00:11:57,120
in reaching many different folks which

00:11:55,279 --> 00:11:58,639
have many different use cases

00:11:57,120 --> 00:12:00,399
skills and tools that they're

00:11:58,639 --> 00:12:02,399
comfortable with using

00:12:00,399 --> 00:12:03,519
and the tools for which we kind of use

00:12:02,399 --> 00:12:05,360
this dissemination and see i'm going to

00:12:03,519 --> 00:12:06,800
get into next

00:12:05,360 --> 00:12:09,040
yeah so once we've got these kind of

00:12:06,800 --> 00:12:10,720
standard sqlite parquet files

00:12:09,040 --> 00:12:12,720
docker images that we've generated from

00:12:10,720 --> 00:12:14,079
the nightly builds we wrap them

00:12:12,720 --> 00:12:15,839
in a bunch of different kind of

00:12:14,079 --> 00:12:17,440
accessibility tools so one of them is

00:12:15,839 --> 00:12:18,720
dataset which you may have heard simon

00:12:17,440 --> 00:12:21,040
talk about yesterday

00:12:18,720 --> 00:12:23,120
it lets us kind of embed an sqlite

00:12:21,040 --> 00:12:24,240
database in a friendly web interface for

00:12:23,120 --> 00:12:25,360
folks that just want to kind of browse

00:12:24,240 --> 00:12:27,120
around in it online

00:12:25,360 --> 00:12:29,360
or maybe download smaller parts of it to

00:12:27,120 --> 00:12:31,279
work with in spreadsheets locally

00:12:29,360 --> 00:12:32,639
and one nice thing about having the data

00:12:31,279 --> 00:12:33,680
web accessible like that is that we can

00:12:32,639 --> 00:12:35,839
link to it directly from our

00:12:33,680 --> 00:12:36,800
documentation and data dictionaries

00:12:35,839 --> 00:12:38,480
and that kind of helps bring the

00:12:36,800 --> 00:12:40,160
documentation to life makes it a little

00:12:38,480 --> 00:12:41,920
bit less abstract

00:12:40,160 --> 00:12:43,760
we're also using intake which is a

00:12:41,920 --> 00:12:44,800
library that was initially developed by

00:12:43,760 --> 00:12:46,800
anaconda

00:12:44,800 --> 00:12:48,320
that lets you wrap references to either

00:12:46,800 --> 00:12:50,560
local or remote data sets

00:12:48,320 --> 00:12:52,480
in a conda package creating what they

00:12:50,560 --> 00:12:54,160
call a data catalog so that

00:12:52,480 --> 00:12:55,519
conda then manages the versioning and

00:12:54,160 --> 00:12:57,440
software dependencies and

00:12:55,519 --> 00:12:58,560
intake provides a standard software

00:12:57,440 --> 00:13:00,959
interface for

00:12:58,560 --> 00:13:02,720
accessing the data and metadata it also

00:13:00,959 --> 00:13:04,639
does local caching if the

00:13:02,720 --> 00:13:06,000
the data is being stored remotely and

00:13:04,639 --> 00:13:07,200
then we're using jupiter and docker in a

00:13:06,000 --> 00:13:09,440
couple of different ways

00:13:07,200 --> 00:13:11,040
um first we want to provide

00:13:09,440 --> 00:13:11,600
self-contained archives for kind of

00:13:11,040 --> 00:13:13,279
long-term

00:13:11,600 --> 00:13:14,560
accessibility that include all of the

00:13:13,279 --> 00:13:16,079
data and the software environment

00:13:14,560 --> 00:13:18,240
required to work with it interactively

00:13:16,079 --> 00:13:20,000
so user can download a single 10

00:13:18,240 --> 00:13:21,760
gigabyte tar tarbill

00:13:20,000 --> 00:13:23,200
run docker compose and get dropped into

00:13:21,760 --> 00:13:24,720
a series of tutorial notebooks that will

00:13:23,200 --> 00:13:26,000
walk them through examples of how to use

00:13:24,720 --> 00:13:27,440
the data

00:13:26,000 --> 00:13:29,200
and then we're using exactly the same

00:13:27,440 --> 00:13:30,880
docker image data catalogs and notebooks

00:13:29,200 --> 00:13:33,200
to provide a hosted jupyter hub in

00:13:30,880 --> 00:13:35,839
collaboration with 2i2c

00:13:33,200 --> 00:13:37,360
it runs remotely can provide scalable

00:13:35,839 --> 00:13:39,279
compute resources potentially

00:13:37,360 --> 00:13:41,360
and really requires no setup at all by

00:13:39,279 --> 00:13:42,880
the user um except for getting a login

00:13:41,360 --> 00:13:44,720
from us

00:13:42,880 --> 00:13:46,160
but in working with a lot of these tools

00:13:44,720 --> 00:13:49,120
for data distribution

00:13:46,160 --> 00:13:50,959
we've found that we're kind of in a gap

00:13:49,120 --> 00:13:53,040
uh between

00:13:50,959 --> 00:13:54,399
small and big data so the medium-sized

00:13:53,040 --> 00:13:54,959
data that we're working with is kind of

00:13:54,399 --> 00:13:58,480
in the

00:13:54,959 --> 00:14:00,079
um 10 to 100 gigabyte scale range

00:13:58,480 --> 00:14:02,560
which is too big to host on a github

00:14:00,079 --> 00:14:04,160
repo um and so we've been law

00:14:02,560 --> 00:14:05,680
drawn uh towards a lot of the bigger

00:14:04,160 --> 00:14:06,959
data tools that are out there

00:14:05,680 --> 00:14:08,880
since they make automation and data

00:14:06,959 --> 00:14:10,000
distribution much easier and

00:14:08,880 --> 00:14:12,399
for this amount of data that we're

00:14:10,000 --> 00:14:13,920
trying to work with and distribute the

00:14:12,399 --> 00:14:15,360
you know the storage and the computer

00:14:13,920 --> 00:14:16,880
really quite cheap they're not a big

00:14:15,360 --> 00:14:18,160
expense

00:14:16,880 --> 00:14:20,480
seems like there's this baked in

00:14:18,160 --> 00:14:22,160
assumption that the infrastructure costs

00:14:20,480 --> 00:14:23,360
will end up being large and so it's okay

00:14:22,160 --> 00:14:24,639
if the setup and maintenance are

00:14:23,360 --> 00:14:26,880
difficult or expensive

00:14:24,639 --> 00:14:28,639
which isn't between our case so we're

00:14:26,880 --> 00:14:29,440
also working with 2i2c to try and figure

00:14:28,639 --> 00:14:31,279
out

00:14:29,440 --> 00:14:32,560
what what does an off-the-shelf medium

00:14:31,279 --> 00:14:34,000
data cloud environment look like

00:14:32,560 --> 00:14:34,800
something that's really optimized for

00:14:34,000 --> 00:14:36,639
easy setup

00:14:34,800 --> 00:14:37,839
minimum configuration options um that

00:14:36,639 --> 00:14:39,440
can make automated

00:14:37,839 --> 00:14:41,199
data processing and distribution both

00:14:39,440 --> 00:14:42,720
cheap and easy and

00:14:41,199 --> 00:14:44,079
i suspect that there are other

00:14:42,720 --> 00:14:47,760
communities of data users that would

00:14:44,079 --> 00:14:47,760
benefit from a similar kind of setup

00:14:52,880 --> 00:15:01,360
christina great um

00:14:58,480 --> 00:15:02,720
oh i'm sorry i skipped over our uh so i

00:15:01,360 --> 00:15:04,079
just wanted to quickly acknowledge a lot

00:15:02,720 --> 00:15:06,959
some of our like

00:15:04,079 --> 00:15:08,480
um both client and client work that

00:15:06,959 --> 00:15:10,160
we've been work uh working with that has

00:15:08,480 --> 00:15:12,160
helped us build a lot of a lot of the

00:15:10,160 --> 00:15:13,839
the analysis sit on top of the data as

00:15:12,160 --> 00:15:16,720
well as uh some partners and

00:15:13,839 --> 00:15:17,920
foundational support as well especially

00:15:16,720 --> 00:15:20,320
the sling foundation for

00:15:17,920 --> 00:15:22,639
kind of helping us um build out a lot of

00:15:20,320 --> 00:15:25,760
this foundational infrastructure

00:15:22,639 --> 00:15:27,920
so i'm just gonna put our takeaways on

00:15:25,760 --> 00:15:29,519
on the slides but just open up open up

00:15:27,920 --> 00:15:30,240
the space for questions if anybody has

00:15:29,519 --> 00:15:33,759
any

00:15:30,240 --> 00:15:35,440
um that's great i was just gonna chime

00:15:33,759 --> 00:15:37,680
in and say four minutes left

00:15:35,440 --> 00:15:39,680
because i forgot to say five minutes uh

00:15:37,680 --> 00:15:40,000
we do have a question i believe this is

00:15:39,680 --> 00:15:43,440
from

00:15:40,000 --> 00:15:45,279
jonathan who is our chat moderator

00:15:43,440 --> 00:15:47,040
and jonathan asks well he says this is

00:15:45,279 --> 00:15:48,800
such an interesting process and mission

00:15:47,040 --> 00:15:50,320
have you had any users that you haven't

00:15:48,800 --> 00:15:54,560
expected to

00:15:50,320 --> 00:15:56,880
access or use your product

00:15:54,560 --> 00:15:58,720
i'm sure we have we don't have like the

00:15:56,880 --> 00:16:00,320
greatest kind of communication with our

00:15:58,720 --> 00:16:01,040
users because everything is just totally

00:16:00,320 --> 00:16:02,560
open so

00:16:01,040 --> 00:16:04,079
we will occasionally find out that

00:16:02,560 --> 00:16:06,399
someone we've never heard of

00:16:04,079 --> 00:16:07,759
is using the data and doing research

00:16:06,399 --> 00:16:09,519
with it

00:16:07,759 --> 00:16:11,199
we've had some small kind of renewable

00:16:09,519 --> 00:16:12,639
energy developers work with the data in

00:16:11,199 --> 00:16:14,000
some cases and that's not really a

00:16:12,639 --> 00:16:17,600
market that we've gone after

00:16:14,000 --> 00:16:19,920
directly so much that's interesting

00:16:17,600 --> 00:16:21,440
um i have a follow-up there what are the

00:16:19,920 --> 00:16:22,639
markets that you have gone after

00:16:21,440 --> 00:16:26,240
directly in terms of

00:16:22,639 --> 00:16:28,720
um working with communities or publicity

00:16:26,240 --> 00:16:30,399
yeah so you know we we have a lot of um

00:16:28,720 --> 00:16:32,240
when we first started we

00:16:30,399 --> 00:16:34,240
sort of naturally attracted a fair

00:16:32,240 --> 00:16:34,880
amount of folks who were doing research

00:16:34,240 --> 00:16:36,480
like doing

00:16:34,880 --> 00:16:37,920
doing their phds or you know in a

00:16:36,480 --> 00:16:41,279
postdoc or something like that

00:16:37,920 --> 00:16:42,560
um and we are we're we are we continue

00:16:41,279 --> 00:16:43,519
to be very excited about supporting

00:16:42,560 --> 00:16:45,759
those folks

00:16:43,519 --> 00:16:46,720
um but you know as i said we we really

00:16:45,759 --> 00:16:48,560
started

00:16:46,720 --> 00:16:49,839
coming from the advocacy community so a

00:16:48,560 --> 00:16:52,880
lot of our like client

00:16:49,839 --> 00:16:54,800
work is in kind of hand tailor

00:16:52,880 --> 00:16:56,560
like tailoring data products for them

00:16:54,800 --> 00:16:58,399
for their advocacy use yeah

00:16:56,560 --> 00:16:59,920
or we're integrating a new data set that

00:16:58,399 --> 00:17:01,440
we would like to have available for

00:16:59,920 --> 00:17:02,320
everyone but we haven't like gotten

00:17:01,440 --> 00:17:04,240
around to it yet

00:17:02,320 --> 00:17:06,559
so like a lot of client interactions

00:17:04,240 --> 00:17:08,160
will give us kind of initial money to

00:17:06,559 --> 00:17:09,600
do a rough draft of what the day looks

00:17:08,160 --> 00:17:11,679
like do their analysis and then

00:17:09,600 --> 00:17:13,439
we can use um sometimes the foundation

00:17:11,679 --> 00:17:14,000
grant money to more deeply integrate

00:17:13,439 --> 00:17:16,839
that

00:17:14,000 --> 00:17:18,000
information into the underlying platform

00:17:16,839 --> 00:17:20,000
awesome

00:17:18,000 --> 00:17:21,360
so there's one quite a few journalists

00:17:20,000 --> 00:17:23,679
oh yeah oh of course

00:17:21,360 --> 00:17:25,280
yeah uh we have about two more minutes

00:17:23,679 --> 00:17:25,679
and so i want to get to this question

00:17:25,280 --> 00:17:29,039
from

00:17:25,679 --> 00:17:31,280
uh william lachance and he asks

00:17:29,039 --> 00:17:32,160
did you measure with which methods of

00:17:31,280 --> 00:17:34,080
accessing

00:17:32,160 --> 00:17:35,200
accessing your data were the most used

00:17:34,080 --> 00:17:38,640
and if so

00:17:35,200 --> 00:17:40,960
what was your interpretation of that

00:17:38,640 --> 00:17:41,760
this is a great question um so so right

00:17:40,960 --> 00:17:43,440
right now

00:17:41,760 --> 00:17:46,000
like a lot a lot of the like access

00:17:43,440 --> 00:17:48,080
modes we are a very inactive development

00:17:46,000 --> 00:17:49,840
on so we don't really only have we have

00:17:48,080 --> 00:17:51,600
two or three main access modes right now

00:17:49,840 --> 00:17:53,200
the jupiter hub gives us the most kind

00:17:51,600 --> 00:17:56,240
of data about who

00:17:53,200 --> 00:17:58,799
is using it and how often um but

00:17:56,240 --> 00:17:59,840
you know our our first and kind of main

00:17:58,799 --> 00:18:01,760
access point right now

00:17:59,840 --> 00:18:03,280
really is just like downloading the uh

00:18:01,760 --> 00:18:06,000
downloading the data from

00:18:03,280 --> 00:18:07,440
sonodo and so we have right now we have

00:18:06,000 --> 00:18:08,799
kind of minimal information about who

00:18:07,440 --> 00:18:10,240
uses things but we're really excited

00:18:08,799 --> 00:18:10,559
about a lot of these methods will give

00:18:10,240 --> 00:18:12,799
us

00:18:10,559 --> 00:18:14,320
much better um data about which one is

00:18:12,799 --> 00:18:16,960
going to be most useful

00:18:14,320 --> 00:18:18,480
yeah the data set and the the jupiter

00:18:16,960 --> 00:18:19,520
hub are both kind of in beta right now

00:18:18,480 --> 00:18:21,600
so they haven't been

00:18:19,520 --> 00:18:22,720
really pushed out very much but we're

00:18:21,600 --> 00:18:24,400
looking forward to having much more

00:18:22,720 --> 00:18:25,679
detailed information about who and how

00:18:24,400 --> 00:18:26,320
many people are using each of the

00:18:25,679 --> 00:18:28,400
different

00:18:26,320 --> 00:18:29,760
um the different access modes and

00:18:28,400 --> 00:18:30,559
potentially which data even they're

00:18:29,760 --> 00:18:32,559
using

00:18:30,559 --> 00:18:34,000
great and in the 30 seconds remaining do

00:18:32,559 --> 00:18:38,480
you want to tell everyone what

00:18:34,000 --> 00:18:40,240
are the roles you're hiring for sure

00:18:38,480 --> 00:18:41,679
um we have one that's more like a data

00:18:40,240 --> 00:18:42,000
wrangler and data analyst that will

00:18:41,679 --> 00:18:43,760
probably

00:18:42,000 --> 00:18:45,440
have more client interactions and work

00:18:43,760 --> 00:18:47,520
on prototyping new data sets

00:18:45,440 --> 00:18:49,039
and analyses and then the other one is a

00:18:47,520 --> 00:18:50,640
software engineer

00:18:49,039 --> 00:18:52,960
data engineer that will probably be

00:18:50,640 --> 00:18:55,280
working more on the infrastructure

00:18:52,960 --> 00:18:57,840
and data pipeline and distribution

00:18:55,280 --> 00:18:57,840

YouTube URL: https://www.youtube.com/watch?v=fpQmwQki7bA


