Title: Mike Trizna: Making Smithsonian Open Access Accessible with Python and Dask
Publication date: 2021-05-19
Playlist: CSVConf 2021
Description: 
	In February 2020, the Smithsonian Institution released almost 3 million images and over 12 million collections metadata records under the Creative Commons Zero (CC0) license. The release was made available via a web API, a GitHub repository, and via the Registry of Open Data on Amazon Web Services (AWS). The format of the release on the GitHub and AWS sources made the data well-suited for parallelized analysis, but only with deep knowledge of the complex data structures. In this talk we will discuss how we used the Python Dask library to unlock this parallelization and make the data more accessible, as well as a student intern project that used Python tools to uncover insights specifically into the holdings of the National Museum of American History.
Captions: 
	00:00:03,280 --> 00:00:03,898
All right. Hi, Mike. So, thanks, everyone, for  joining us. We have about 73 people watching live,  

00:00:03,898 --> 00:00:03,999
which is awesome. And probably more on the YouTube  stream. So, Mike, you're here to talk to us about  

00:00:03,999 --> 00:00:04,086
Making Smithsonian Open Access Accessible  with Python and Dask. So, I am going to mute  

00:00:04,086 --> 00:00:04,181
myself and highlight your slides. And then... Mike: Okay. I didn't hear the last part of that. 

00:00:04,181 --> 00:00:05,440
>> Oh, I said, "Take it away!" Mike: All right. I assumed. My name is Mike  

00:00:05,440 --> 00:00:11,920
Trizna. I'm a data science with the Smithsonian  in a group called the Smithsonian O CIO  

00:00:12,720 --> 00:00:19,520
which is the -- our IT office, data science  lab. And I'm gonna be talking today about the  

00:00:19,520 --> 00:00:26,240
Smithsonian Open Access Project that started last  year. And making the data that was released with  

00:00:26,240 --> 00:00:34,240
that more accessible using Python and Dask. And  then show a few examples of that in practice. 

00:00:36,240 --> 00:00:43,600
Okay. So, to start off, the Smithsonian.  A lot of people don't realize that,  

00:00:43,600 --> 00:00:50,640
yes, there are 19 museums. Mostly in Washington,  D.C. that are part of the Smithsonian  

00:00:50,640 --> 00:00:57,520
Institution. We do have one in -- actually,  two in New York. But they're -- they're not  

00:00:57,520 --> 00:01:03,920
necessarily just in Washington, D.C.  We also have 21 libraries and archives.  

00:01:03,920 --> 00:01:11,840
Nine research centers. And a zoo. So, there  are lots of different units at the Smithsonian.  

00:01:12,560 --> 00:01:18,800
Working on, I don't know, the  craziest range of different topics. 

00:01:21,280 --> 00:01:27,400
So, the Smithsonian's mission. I'm not sure  if people know the origin story, if you will,  

00:01:27,400 --> 00:01:37,200
of the Smithsonian. It was founded in 1846 from  an Englishman named James Smithson. He had never  

00:01:39,280 --> 00:01:44,400
visited the US. But decided to leave  his inheritance to the United States  

00:01:44,400 --> 00:01:54,160
Government with this idea that the United States  Government start an organization known as the  

00:01:54,160 --> 00:01:59,360
Smithsonian Institution. So, he thought very  highly of himself. Just grabbing that name. 

00:02:00,320 --> 00:02:06,240
But I think this is so cool. The mission  would be to -- for the increase and  

00:02:06,240 --> 00:02:14,400
diffusion of knowledge. Which is like such  a bold statement. I love it. So, this is a  

00:02:14,960 --> 00:02:22,160
pretty cool comic they came across a few years ago  online. And I've printed it out, stuck it on my  

00:02:22,160 --> 00:02:27,920
desk when I used to work at a desk at an office. But it shows the progression from  

00:02:28,480 --> 00:02:35,840
data, where you're just kind of collecting a lot  of data out in the field or something like that.  

00:02:36,400 --> 00:02:41,600
It becomes information as you connect the  different pieces, it becomes knowledge. And  

00:02:41,600 --> 00:02:52,640
then insight. And then finally, wisdom. I've also  seen this comic kind of extended recently to show  

00:02:52,640 --> 00:02:58,400
connections that aren't there and that's labeled  conspiracy theories. But that's another story. 

00:03:00,080 --> 00:03:05,040
So, the Smithsonian has been increasing  and diffusing knowledge, which is this  

00:03:05,040 --> 00:03:09,920
central panel here. But what about all of  that data? Which is the first panel here. 

00:03:13,120 --> 00:03:19,760
So, all of that data and info, those are the  first two panels, that feed into knowledge,  

00:03:19,760 --> 00:03:25,920
insight and wisdom, they're all being cataloged  and stored in drawers across the Smithsonian.  

00:03:25,920 --> 00:03:33,920
This is a cool shot from the entomology department  at the Natural History Museum. To show you there  

00:03:33,920 --> 00:03:40,160
are just like floors and floors of these racks  of drawers that are closed most of the time. So,  

00:03:40,160 --> 00:03:47,280
you don't get to see this on a daily basis.  But it's kind of cool to see the contents  

00:03:47,280 --> 00:03:51,520
of all these drawers. And it really kind of  starts to hurt your brain a little bit when  

00:03:51,520 --> 00:03:58,320
you try and comprehend all of the different -- the  amount of data that is stored at the Smithsonian  

00:03:58,960 --> 00:04:02,960
both in the drawers and we're  digitizing more and more of that. 

00:04:05,520 --> 00:04:12,560
Okay. So, that was the -- the previous state was  a lot of things in drawers. Then the Smithsonian  

00:04:12,560 --> 00:04:18,560
made the decision in -- throughout 2019 we  were kind of ramping up to this. And then  

00:04:19,520 --> 00:04:30,960
had a launch event in February of 2020 to create  the Open Access policy. To share our data on  

00:04:31,760 --> 00:04:41,280
a CC zero license. This is a photo from the Open  Access launch event. I took it there. You may  

00:04:41,280 --> 00:04:47,840
note that late February 2020, that's known as the  before times now. It was kind of weird. We were  

00:04:47,840 --> 00:04:53,600
all packed into the American History Museum.  But it was a -- it was a pretty neat event. 

00:04:56,320 --> 00:05:03,200
So, yeah. What is in the Open Access release? So,  across the Smithsonian, this is just an estimate  

00:05:03,200 --> 00:05:10,960
because we can't really count it all. There are  155 million objects. Two point 1 million library  

00:05:10,960 --> 00:05:22,880
volumes and archival collections. Just people's  notes in boxes. But within the collection itself,  

00:05:22,880 --> 00:05:32,080
there are 2.8 million mostly 2-D images and we  also have a lot of 3-D scans in there as well.  

00:05:33,360 --> 00:05:41,520
And then kind of most useful to a person like  me, we have -- we've made all of the metadata  

00:05:42,240 --> 00:05:46,080
records for all of the digitized  objects we have in our collections  

00:05:46,880 --> 00:05:51,840
available. And there are over 17  million of those at last count. 

00:05:52,640 --> 00:05:59,760
Okay. So, what does "Open access" mean? I have  been kind of throwing this term around. So, before  

00:05:59,760 --> 00:06:06,800
February 2020 when we had our Open Access launch,  all the Smithsonian Museums and research units,  

00:06:08,480 --> 00:06:12,400
they made their data available and  searching, but it was kind of on  

00:06:13,840 --> 00:06:21,920
their own policies. They set up their own search  engines or data dumps. But it varied by unit.  

00:06:21,920 --> 00:06:33,360
There was no consistency in terms of, I guess,  formatting. Keeping things consistent or anything  

00:06:33,360 --> 00:06:41,040
like that. There were also pretty much every  unit had their own use -- usage agreements. So,  

00:06:41,040 --> 00:06:47,520
I worked at the Natural History Museum then.  And I know the term of use said that you can  

00:06:47,520 --> 00:06:57,440
use any of the data for an educational purpose. However, what the Open Access did was it made  

00:06:57,440 --> 00:07:07,040
all of the media that fell under the Open Access  Policy completely open access. So, on the CC0,  

00:07:08,960 --> 00:07:15,840
that stands for copyright -- something copyright  zero policy -- which means you can take any of the  

00:07:17,280 --> 00:07:22,960
images that are distributed through this and do  whatever you want with them. You can -- you can  

00:07:22,960 --> 00:07:32,240
take scans of paintings. You can reproduce them.  Sell them. Make money off of them. And the idea  

00:07:32,240 --> 00:07:38,400
here is to kind of put all of these things out  there. They're funded by the US taxpayers anyways.  

00:07:38,400 --> 00:07:45,680
So, they kind of belong in the -- in the open  realm there. And see what people do with them. 

00:07:48,320 --> 00:07:56,000
Okay. So, how can all this data be accessed? We  -- I guess -- I said that we released this data,  

00:07:56,000 --> 00:08:02,960
but what does that mean in actuality? I'm going  to cover three different ways that you can access  

00:08:02,960 --> 00:08:10,480
all of the Smithsonian Open Access data. And all  three of them share their -- the metadata records  

00:08:11,440 --> 00:08:16,000
in the exact same consistent JSON structure. And  I know here that it's deeply nested -- note here  

00:08:16,000 --> 00:08:23,360
that it's deeply nested and I'll show you  an example that have later. The first way of  

00:08:25,120 --> 00:08:31,360
accessing the Open Access data -- sorry, there's  a lot of access here -- is through a web API. 

00:08:32,480 --> 00:08:36,400
So, I have a link there.  I'll share the link to these  

00:08:37,440 --> 00:08:43,920
slides afterwards so you can click on all  these links. What this link gives you is the  

00:08:45,280 --> 00:08:52,320
self-contained documentation and kind of like  a playground you can test out the different  

00:08:52,320 --> 00:09:01,680
endpoints in there. You do need to sign up for an  API key to use it. However, it's free. I believe  

00:09:01,680 --> 00:09:08,480
they only -- or ask for your email address.  And it's a quick turn around. It's automated. 

00:09:09,760 --> 00:09:17,120
And I recommend it for getting a feel for how the  data is organized and what the record structure  

00:09:17,120 --> 00:09:25,440
looks like because it's a little peculiar. Okay. So, the API is kind of the  

00:09:26,480 --> 00:09:33,600
most marketed form of getting to the data.  However, if you're interested in doing some  

00:09:33,600 --> 00:09:39,680
big things with the -- the whole dataset, you're  gonna run into some limitations pretty quickly.  

00:09:41,440 --> 00:09:46,960
So, the limitations I list here  are that the records are pretty  

00:09:46,960 --> 00:09:52,320
extensively indexed for searching. So, if  you want to search on the certain term,  

00:09:52,320 --> 00:10:00,000
if you want to search for like a person. All of  the names are indexed. However, you're quickly  

00:10:00,000 --> 00:10:04,400
gonna find examples of things that you know  are in records, but you can't search directly. 

00:10:06,640 --> 00:10:14,000
And another is the limit of -- you're only  limited to a thousand records per API call.  

00:10:14,560 --> 00:10:20,640
If you're looking to do some sort of analysis  across millions of records across multiple units,  

00:10:21,920 --> 00:10:24,960
that's just not gonna be feasible. So,  

00:10:27,280 --> 00:10:33,840
I want to show you a -- an example of where  I use the web API and then the -- our other  

00:10:33,840 --> 00:10:41,280
sources. And I want to set that up here. So, in 2017, the data science group that  

00:10:41,280 --> 00:10:46,960
I'm a part of now, they did this actually before I  joined the group. They did a little pilot project  

00:10:47,600 --> 00:10:52,880
where they went into the holdings of the -- the  botany department at the Natural History Museum.  

00:10:53,760 --> 00:11:01,680
And the botanists traditionally store all of  their plant specimens on a flat sheet. And they  

00:11:01,680 --> 00:11:09,200
have those in -- in drawers at the Natural History  Museum. And they're great for digitizing. They're  

00:11:09,200 --> 00:11:16,480
these flat pieces of paper. And they actually set  up a conveyor belt. I completely forgot to include  

00:11:16,480 --> 00:11:23,280
that in here. But they set up a conveyer belt  that takes pictures as these things come through.  

00:11:24,240 --> 00:11:32,480
We have a ton of digitized herbarium sheets. One  of the things we wanted to try out was to try to  

00:11:32,480 --> 00:11:42,560
detect herbarium sheets that had been stained  with mercury. There was a practice I think in  

00:11:42,560 --> 00:11:52,960
the late 1800s, early 1900s, as they cam in from  the field, they had bugs or mold. They wanted to  

00:11:55,360 --> 00:12:03,280
preserve the plant and kill the bugs. So they  would dip it in mercury. They didn't realize, oh,  

00:12:03,280 --> 00:12:12,240
mercury is pretty dangerous. They did stop  eventually. But we now have these sheets that are  

00:12:13,200 --> 00:12:21,520
soaked in mercury. And people who work with the  sheets on a daily basis are able to spot them. 

00:12:22,240 --> 00:12:26,960
But this was a great chance to try out  machine learning to see if we could,  

00:12:27,520 --> 00:12:36,400
I mean, take a set of expert identified  mercury-stained sheets as well as sheets  

00:12:36,400 --> 00:12:41,920
that we knew were not stained by mercury and build  a machine learning model to differentiate between  

00:12:41,920 --> 00:12:46,640
the two of them across the entire collection.  And there's a link to that paper in there. 

00:12:48,960 --> 00:12:57,760
So, that -- that paper was completed and published  in 2017. If you work in machine learning or are  

00:12:57,760 --> 00:13:05,040
familiar with the field, 2017 is kind of ancient  history from now. A lot has changed since then.  

00:13:05,760 --> 00:13:13,840
And I was -- I was looking to duplicate the  same experiment. And use some of the more  

00:13:13,840 --> 00:13:21,360
modern techniques to build a model. So, I wanted  to get all of those images and do it again. So,  

00:13:21,360 --> 00:13:29,520
all of the images were shared on Figshare as part  of this paper, but I wanted to get the original  

00:13:30,800 --> 00:13:36,160
images from the database because the ones  that were shared to Figshare were re-sized.  

00:13:37,040 --> 00:13:42,560
And I also wanted to get all the metadata  around it. Like when were these specimens  

00:13:42,560 --> 00:13:48,640
collected? By who? To see if we could  use that to build a more accurate model. 

00:13:50,480 --> 00:13:57,360
Okay. So, here is this example  of the -- the record format that  

00:13:58,080 --> 00:14:04,880
we -- we published these records to -- out  in. So, this, believe it or not, is a single  

00:14:08,240 --> 00:14:14,400
botany record. And all of these different pieces  of information are included in here. And you can  

00:14:14,400 --> 00:14:21,440
see they're kind of like variously nested if you  look through here. All I had from the Figshare  

00:14:23,440 --> 00:14:31,120
repository was the barcode. And like I said,  barcode was not one of those index terms so I  

00:14:31,120 --> 00:14:35,440
couldn't search on barcode. So, everyone though  I had a list of several thousand barcodes,  

00:14:35,440 --> 00:14:43,200
I couldn't use the API to grab all of the images  of the herbarium sheets that I needed. I had to  

00:14:44,880 --> 00:14:54,640
download the entire data sheet and use that.  This picture is kind of funny to me. Reminds  

00:14:54,640 --> 00:15:01,600
me of those Mars rover pictures where they  stitch together multiple images into a mosaic.  

00:15:02,160 --> 00:15:06,240
And I kind of had to do the same thing  to capture the entire JSON record. 

00:15:08,480 --> 00:15:14,720
Okay. So, I mentioned the full dataset. And there  are two different sources for that. So, this is  

00:15:15,280 --> 00:15:19,200
option two and three for accessing  the Smithsonian Open Access data.  

00:15:20,000 --> 00:15:29,120
The first is on AWS S3 which stores static  data. And it -- that source has all of the  

00:15:29,120 --> 00:15:34,560
metadata records as well as all of the Open  Access images can be accessed from there.  

00:15:34,560 --> 00:15:41,200
So, it's pretty quick to download. And then  GitHub has all of -- only the metadata. However,  

00:15:41,200 --> 00:15:45,680
it's versioned and I'll show you a  few other advantages to using GitHub. 

00:15:48,320 --> 00:15:54,240
And the way that the files are packaged  are -- as line delimited JSON. A little bit  

00:15:54,240 --> 00:16:01,920
different than the normal JSON you're used  to. And compressed with bzip2 to save space  

00:16:02,640 --> 00:16:10,080
and make it easier to download. And organized  by which units they came from. And the files  

00:16:10,080 --> 00:16:16,480
are split up according to how they are hashed.  And I'll show you an example what that means. 

00:16:17,440 --> 00:16:23,200
So, you probably can't squint and see this.  But I'll describe. This is a screenshot of  

00:16:23,760 --> 00:16:27,920
the holdings from the American History Museum.  

00:16:27,920 --> 00:16:34,320
And it's split into all of these different files.  You may see that the first file here is 00.bzt.02.  

00:16:37,360 --> 00:16:43,280
Following the hashing structure, there are  almost always 256 of these of varying sizes  

00:16:43,280 --> 00:16:50,320
across the are various units. That's a little  bit intimidating. I downloaded all the American  

00:16:50,320 --> 00:16:57,040
history records and you get this. So, bummer. If you need to go through  

00:16:57,040 --> 00:17:01,920
every single file, one at a time  to try and do something with that,  

00:17:01,920 --> 00:17:08,080
that's gonna take a lot of time. There are almost  10,000 files across all the units to process. 

00:17:10,400 --> 00:17:14,880
But believe it or not, there's actually a benefit  to having so many files. I'm not sure if it was  

00:17:14,880 --> 00:17:19,120
thought about before. But my eyes lit up when  I saw that there were so many different files.  

00:17:19,120 --> 00:17:24,480
Because we can use that to multitask. >> I'm gonna just jump in here and  

00:17:24,480 --> 00:17:29,040
say there's 5 minutes left. Mike: Got it. This is where Dask  

00:17:29,040 --> 00:17:38,720
comes in. A Python library built exactly for  multitasking processes. So, here's a little  

00:17:38,720 --> 00:17:47,040
GIF from the Dask website. That shows you can set  up a miniature cluster on your own computer. Most  

00:17:47,040 --> 00:17:52,240
of what I'm going to show you here was done on  my laptop. Or you can actually scale using the  

00:17:52,240 --> 00:17:59,840
exact same code to real compute Clusters either in  the cloud or on high performance compute Clusters.  

00:18:02,000 --> 00:18:07,920
This is a screen grab that I took yesterday,  actually, when I was trying to count the  

00:18:07,920 --> 00:18:11,760
number of images for that -- or the number  of records for that 17 million count here.  

00:18:12,960 --> 00:18:24,480
And this shows how Dask is able to split tasks  across different processers. And it -- it shows  

00:18:24,480 --> 00:18:29,360
you and it's a really cool dashboard which I love  just watching. I could watch this kind of all day. 

00:18:31,600 --> 00:18:38,000
So, Dask may be known a little bit more for  its parallel processing of DataFrames, so,  

00:18:38,000 --> 00:18:48,320
like tabular data. But it's also great at parsing  text data. Like JSON data that we have. So,  

00:18:48,320 --> 00:18:53,840
these are the lines right here that it takes  to process every single one of the records  

00:18:53,840 --> 00:19:00,320
either from S3 or from GitHub. This is the  exact line of code that you would need to run to  

00:19:01,280 --> 00:19:05,680
process all of those at the same time.  And the way that it does it, Dask is  

00:19:05,680 --> 00:19:15,040
delayed in its computing. So, it kind of like  identifies all of the different file locations  

00:19:15,760 --> 00:19:21,040
and then sets it up so that you can  compute against those when you're ready. 

00:19:21,760 --> 00:19:28,800
And they also have a built-in S3 connector. So,  they know that people store a lot of data on S3.  

00:19:28,800 --> 00:19:39,680
And they have a connector ready to connect them. Here's an example of going back to that mercury  

00:19:39,680 --> 00:19:48,480
sheet example. I downloaded the entire Natural  History botany collection which is gigantic  

00:19:48,480 --> 00:19:55,280
in terms of the number of records. But I wrote a  little function to pull out the IDs and these are  

00:19:55,280 --> 00:20:03,600
the lines that it takes to grab a piece -- useful  information from that crazy nested JSON record.  

00:20:04,560 --> 00:20:08,960
And what it looks like when you can pull it out.  And you can actually do some processing on it. 

00:20:10,880 --> 00:20:19,360
Okay. So, I wanted to point out another cool  example. I have been working with an intern from  

00:20:19,360 --> 00:20:27,440
George Mason University, Patrick McManus who may  or may not be in the audience. But he and I worked  

00:20:27,440 --> 00:20:32,320
together. He's -- he focuses on American history.  We looked at the holdings with the American  

00:20:32,320 --> 00:20:38,720
History Museum to look at the different dates  in there. And how different topics, different  

00:20:38,720 --> 00:20:45,520
places show up across different dates. And this is  a screenshot of a Streamlit app we built together. 

00:20:47,360 --> 00:20:53,920
And then another example, there's a link  here, is a tutorial we built as part of our  

00:20:54,640 --> 00:21:02,720
launch with AWS that shows how to pull down all of  the painting images from the American Art Museum.  

00:21:03,600 --> 00:21:09,840
And then run them through a machine learning model  to cluster them by the content. And you can't  

00:21:09,840 --> 00:21:19,280
really tell at closer glance. But you can zoom in  on a larger cluster image we have on the GitHub  

00:21:19,280 --> 00:21:25,600
that's linked here and see why the paintings  showed up where they did in this big cluster. 

00:21:27,440 --> 00:21:31,680
Okay. And that's it. >> All right. Well,  

00:21:31,680 --> 00:21:39,440
we have time for one question. So, let's  go ahead and take Megan O'Donnell from  

00:21:39,440 --> 00:21:47,040
Iowa State University asks: Dare I ask, what  metadata schema or schemas the metadata is in? 

00:21:49,040 --> 00:21:58,240
Mike: I don't actually know. There's no quick  answer to that. There is a documented JSON schema  

00:21:58,240 --> 00:22:05,760
for, I think, all of the different units? And I  believe it's on GitHub. I can -- I can share that  

00:22:06,560 --> 00:22:11,760
in the Slack question and answer afterwards. But  I don't know if there's like a name to the schema  

00:22:11,760 --> 00:22:18,720

YouTube URL: https://www.youtube.com/watch?v=uhIkNMK5zVM


