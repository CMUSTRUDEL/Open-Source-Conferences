Title: Keynote: Simon Willson - Datasette and Dogsheep: Liberating your personal data
Publication date: 2021-05-19
Playlist: CSVConf 2021
Description: 
	Datasette is an open source web application for exploring, analyzing and publishing data. I originally designed it to support data journalists working in newsrooms, but quickly realized that it has applications way beyond journalism. I decided to start digging into my own personal data - the data that sites and services collect about and for me, which thanks to regulations like Europe's GDPR is increasingly available for me to export myself. This led to Dogsheep, a collection of tools for importing personal data from Twitter, GitHub, 23AndMe, Foursquare, Google, HealthKit and more. Being able to export your data isn't much good if you can't easily do interesting things with it. I'll show how the combination of Datasette and Dogsheep can help liberate your personal data, and discuss the lessons I've learned about personal data and open source along the way.
Captions: 
	00:00:03,379 --> 00:00:10,780
Hello, everyone. Welcome back to our second keynote of the day and our speaker is Simon

00:00:10,780 --> 00:00:22,730
Willison. He's most well known about the co-creator of Django framework, but also, the co-creator

00:00:22,730 --> 00:00:29,380
of Dataset. I love Simon's talks and heard other talks about him before. The reason I'm

00:00:29,380 --> 00:00:35,550
super-excited is because his talk is very much in the spirit of what csv,conf is about.

00:00:35,550 --> 00:00:41,460
Which are highlighting tools and practices that make it possible to democratize data.

00:00:41,460 --> 00:00:47,650
So, he'll be telling us today about dataset, how it works. And how it makes it possible

00:00:47,650 --> 00:00:55,890
for people to publish data and help other people explore the data in other ways. Simon,

00:00:55,890 --> 00:01:01,510
take it away. Simon: Good afternoon, csv,conf. It's exciting

00:01:01,510 --> 00:01:08,770
to be here. I was at csv,conf a couple years ago. I have a demo relating to that. I'm going

00:01:08,770 --> 00:01:13,329
to talk about a couple of projects I spent three years working on called Datasette and

00:01:13,329 --> 00:01:23,740
Dogsheep. I'm going to fire up screen sharing. I thought I had this up, so, there's screen

00:01:23,740 --> 00:01:29,390
sharing. So, I'm going to start out with a demo.

00:01:29,390 --> 00:01:36,740
This is a database of all of my swarm checkins. I use swarm on my phone and check into different

00:01:36,740 --> 00:01:41,549
places. And exported that data back out and loaded it into Datasette. This is a relatively

00:01:41,549 --> 00:01:50,229
unexciting-looking table that I've pulled from Swarm. This is checkins. But then I can

00:01:50,229 --> 00:01:57,909
pull out latitude and longitude. And then draw thing on a map. I have an exciting map

00:01:57,909 --> 00:02:05,619
of places I have checked into in the past 5 or 6 years. Or I can turn on the pandemic

00:02:05,619 --> 00:02:12,090
mode and filter for everything that was created since that terrible day last year where Tom

00:02:12,090 --> 00:02:19,070
Hanks was -- was -- was -- came down with coronavirus and the entire world shut down.

00:02:19,070 --> 00:02:23,020
And see that in the last year and a half, I have been a lot more limited in the places

00:02:23,020 --> 00:02:27,450
I have been to. The map here is interesting. This is not a

00:02:27,450 --> 00:02:32,670
feature of Datasette itself. Datasette has a plugin system where plugins can add all

00:02:32,670 --> 00:02:38,220
sorts of additional functionality. I have a plugin which looks at the table on screen,

00:02:38,220 --> 00:02:43,940
tries to find latitude and longitude columns and if they exist, tries to draw them on a

00:02:43,940 --> 00:02:47,530
map. Let's do something really useful with the

00:02:47,530 --> 00:02:53,450
data that we've got here. So, every time I check in with Clio, my dog, I use the wolf

00:02:53,450 --> 00:03:00,480
emoji in my checkin message. I can filter for whether shout message contains that wolf

00:03:00,480 --> 00:03:06,820
emoji. If I do that, I get a map of places Clio likes to go. This is my dog's own personal

00:03:06,820 --> 00:03:14,990
view on the swarm checkin data. I can zoom in and see that. A crucial feature of Datasette

00:03:14,990 --> 00:03:21,960
is something called facetting. The way that works, I can say, facet by this, and I'll

00:03:21,960 --> 00:03:27,210
see a sort of a count of that column across the entire dataset. So this be right now is

00:03:27,210 --> 00:03:33,120
showing my dog's favorite categories of places she likes to go. She loves parks and dog runs,

00:03:33,120 --> 00:03:41,960
but also coffee shops. I can see the 30 checkins that Clio has made at coffee shops and get

00:03:41,960 --> 00:03:48,820
preferences for a San Francisco and now half-moon bay dog. And do a further facet by name to

00:03:48,820 --> 00:03:54,430
see what she loves the most. She loves blue bottle coffee, but has been to Starbucks four

00:03:54,430 --> 00:04:03,620
times as well. You can export the data out, here's a JSON

00:04:03,620 --> 00:04:09,790
feed of coffee shops that my dog likes going to. Or pull it out of CSV that I can download

00:04:09,790 --> 00:04:17,509
into other tools to do a deeper analysis of my dog's coffee habits. And so, this essentially

00:04:17,509 --> 00:04:24,870
is -- this is what Datasette is. Datasette is a web interface on a database. Under the

00:04:24,870 --> 00:04:32,670
hood it's t SQL lite. There are a bunch of reasons I'm using this, but fundamentally,

00:04:32,670 --> 00:04:40,040
SQLite is a great way of packaging data into a binary file. A SQL database is a something.db

00:04:40,040 --> 00:04:49,690
on disk. Because you don't have to run a postgres and MySQL server, they are cheap to send around

00:04:49,690 --> 00:04:56,550
and publish as well. A big part of the Datasette project has been exploring the limits of SQLite

00:04:56,550 --> 00:05:02,340
and figuring out how good is SQLite as a sort of format for publishing and sharing data.

00:05:02,340 --> 00:05:08,190
And honestly, the limits are pretty much endless. I keep on finding new things that SQLite can

00:05:08,190 --> 00:05:14,910
do. And I think the logical limit in size of a SQLite database used to be 1.4 terabytes

00:05:14,910 --> 00:05:20,330
and then doubled to 2.8 terabytes in the most recent release based on feedback from a user.

00:05:20,330 --> 00:05:34,430
There's a lot you can get done on a SQL database. Datasette is the software I just showed you

00:05:34,430 --> 00:05:40,560
that lets you explore through these database tables, visualize them, run plugins against

00:05:40,560 --> 00:05:45,139
them, and export the data back out again. And Datasette is an increasing ecosystem of

00:05:45,139 --> 00:05:51,800
tools around this core idea. I mentioned plugins. I have 59 plugins, there's a few more than

00:05:51,800 --> 00:05:56,510
that, but I need to get into this index that add all sorts of additional features like

00:05:56,510 --> 00:06:05,210
password authentication, APIs. The ability to serve map tiles for JavaScript libraries

00:06:05,210 --> 00:06:11,310
like leaflet. All sorts of different bits and pieces. And then I have these tools. So,

00:06:11,310 --> 00:06:17,430
tools are not -- unlike plugins, you don't install tools into Datasette. But they are

00:06:17,430 --> 00:06:25,090
for building SQL-like databases yourself. It's great if you have it in SQLite, but you

00:06:25,090 --> 00:06:30,050
need to get that data from other sources and get it into that format.

00:06:30,050 --> 00:06:35,780
So, what I'm going to do next is show you a demo of how this overall set of tools works.

00:06:35,780 --> 00:06:44,190
Hang on. Let's do -- here we go. So, the London fire brigade published an amazing CSV file

00:06:44,190 --> 00:06:51,180
called animal rescue incidents attended by LFB. It's 2.6 megabytes, every time they have

00:06:51,180 --> 00:07:00,139
rescued an animal since 2009. I found out about it and downloaded a copy of the CSV

00:07:00,139 --> 00:07:06,570
file. Now, if we look at -- here we go, if we look at the CSV file, it's unsurprisingly,

00:07:06,570 --> 00:07:12,139
a CSV file. It's a bunch of columns like incident number and a type of different incident and

00:07:12,139 --> 00:07:17,150
all of this type of stuff. What I'm going to do now is load this into a SQL-like database.

00:07:17,150 --> 00:07:32,020
I'm going to use a tool I wrote, SQL utils for manipulating SQL databases in 

00:07:32,020 --> 00:07:35,620
a bunch of different ways. Okay. So, I'm going to run SQLite utils, and

00:07:35,620 --> 00:07:43,449
create an instance, incidents.db, a table called incidents. And I'm going to feed it

00:07:43,449 --> 00:07:49,300
that CSV file. Oh, it threw an error here because by default it expects you to get it

00:07:49,300 --> 00:07:56,139
JSON. But you can use the dash, dash CSV function to tell it it's a CSV file. There we go. That's

00:07:56,139 --> 00:08:08,169
now created me a -- let's see. Oop. LS -lah. It's created a 2.8-megabyte

00:08:08,169 --> 00:08:15,710
SQLite file. Now I'm going to run Datasette against it. Get the file name, starts at the

00:08:15,710 --> 00:08:23,320
server. I can click the link or add dash O and have it open the browser for me. Here

00:08:23,320 --> 00:08:31,180
we have this CSV file now running inside of Datasette. What do we got here? We have 7370

00:08:31,180 --> 00:08:36,190
rows. I have the plugin installed. It can show them all on a map, including one that

00:08:36,190 --> 00:08:40,820
has a zero latitude and longitude and shows up in the wrong place. But the interesting

00:08:40,820 --> 00:08:45,700
part is when we facet this. I'm looking through the different columns.

00:08:45,700 --> 00:08:53,990
There was a column here for animal group parent. If I facet by that, I can see that cats get

00:08:53,990 --> 00:08:58,060
in trouble the most often with the London fire brigade. Thirty-five00 instances. And

00:08:58,060 --> 00:09:08,399
surprisingly, birds before dogs, I like ferrets. It turns out there's been eight ferret incidents

00:09:08,399 --> 00:09:13,020
serviced by the London fire bring grade in the last decade. You see the descriptions,

00:09:13,020 --> 00:09:23,149
ferret trapped behind kitchen unit, ferret stuck in grilling area. This is a fantastically

00:09:23,149 --> 00:09:29,490
rich set of data about ferrets and about all other kinds of animals as well.

00:09:29,490 --> 00:09:33,730
But there are a couple of problems with the data we have got here. In particular, we have

00:09:33,730 --> 00:09:41,440
got a column here called incident notional cost. An incident of how much it cost the

00:09:41,440 --> 00:09:49,000
fire brigade to rescue that ferret behind a kitchen unit. But if we sort by that, you

00:09:49,000 --> 00:09:53,880
can't quite see it on this -- actually, for ferrets, if that sort's going to work. I'm

00:09:53,880 --> 00:10:01,430
going to get rid of the ferret search. The problem we have here is because it's a CSV

00:10:01,430 --> 00:10:06,360
file, that -- this field here is being treated as text. So, first you've got the word "Null"

00:10:06,360 --> 00:10:11,060
that shows up first. And then you've got the 999 pounds sorting

00:10:11,060 --> 00:10:16,740
before the 1999 pounds because we're sorting a numeric column alphabetically. The first

00:10:16,740 --> 00:10:23,550
piece of cleanup is convert the types of the columns into numbers. And I've actually got

00:10:23,550 --> 00:10:31,190
a Datasette plugin I can use for that. It looks like I've got it. I haven't got it installed.

00:10:31,190 --> 00:10:39,050
I can do Datasette install, Datasette-edit-schema. That will install that plugin. When I open

00:10:39,050 --> 00:10:47,800
that dataset again, I need to log into the incidents so I can edit. I have dash, dash

00:10:47,800 --> 00:10:54,580
root to log in as a root user. And now I'm signed in and get a new option on this incidents

00:10:54,580 --> 00:11:02,080
table. I get a cog, and an option for edit table schema. I want pump counts, that's the

00:11:02,080 --> 00:11:14,330
number of fire engines, and -- I want to treat those as integer columns. Going back here,

00:11:14,330 --> 00:11:18,310
these are now integers. You can tell their numbers because we're in a slightly different

00:11:18,310 --> 00:11:24,140
shade of color. If I sort by this, let's sort descending. Unfortunately, I've still got

00:11:24,140 --> 00:11:34,970
the null values. I can filter those out, incident, notional cost, does not equal null as a string.

00:11:34,970 --> 00:11:40,320
And now I can see the most expensive of those events. Which was a cat stuck within a wall

00:11:40,320 --> 00:11:46,790
space where the R SPCA, the animal safety. And then there was a foal in the river, a

00:11:46,790 --> 00:11:51,370
horse fell into a swimming pool. These are the more expensive incidents. It's amazing

00:11:51,370 --> 00:11:57,380
that the London fire brigade only consider that a 4,000 pound cost. I think the San Francisco

00:11:57,380 --> 00:12:01,060
fire brigade would charge quite a bit lot more money than that.

00:12:01,060 --> 00:12:10,019
Yeah. So, this illustrates participant of the expanding scope. Initially, it was read-only,

00:12:10,019 --> 00:12:19,320
but now it's writing to the database as well. Like in this case, fixing up schemas and converting

00:12:19,320 --> 00:12:27,519
column types. I'm going to make a couple more changes to this. I'm really frustrated by

00:12:27,519 --> 00:12:35,120
those null strings. So, what I can do there, is I can just run SQL directorially against

00:12:35,120 --> 00:12:40,779
that database file to clean those up. So, I'm going run a SQL query against incidents.db

00:12:40,779 --> 00:12:47,660
to update the sunset when notional cost is the string null. It tells me effective 50

00:12:47,660 --> 00:12:59,220
rows, paste in the other three as well. Now, if I open up Datasette again, those null

00:12:59,220 --> 00:13:05,310
rows don't need to be filtered out anymore. The one last thing I want to do is I'm going

00:13:05,310 --> 00:13:10,310
to set up full text search across this data so that we can actually run search queries

00:13:10,310 --> 00:13:18,269
against it. So, I'm gonna sign in as ROOT again. And then there's another plugin I built

00:13:18,269 --> 00:13:25,040
in call the Datasette configure full text search. I want to be able to search this data

00:13:25,040 --> 00:13:31,410
by the animal group parent and by that final description. So, now configure search across

00:13:31,410 --> 00:13:36,540
these columns. And this is using SQLite's built-in full text search feature which is

00:13:36,540 --> 00:13:44,950
actually quite powerful. It can do streaming, it can do relevant scoring. It's a neat piece

00:13:44,950 --> 00:13:50,690
of the SQLite package. And Datasette is smart enough to see if you configured full text

00:13:50,690 --> 00:13:58,660
search. I'm going to search for ferret again. There's the ferret ones. I'm going to search

00:13:58,660 --> 00:14:04,029
for ditch. Because I happen to know that a lot of creatures end up in trouble when they

00:14:04,029 --> 00:14:08,300
fall into a ditch. If we look at the -- if we facet by animal

00:14:08,300 --> 00:14:13,240
group parent, we can see there have been 29 horses that will fallen into ditches around

00:14:13,240 --> 00:14:18,390
London and needed to be rescued by the fire department. Two deer, one bird. How does a

00:14:18,390 --> 00:14:25,769
bird get stuck in the ditch? A duckling was trapped in the drainage ditch. There you go.

00:14:25,769 --> 00:14:30,269
And this is really where this stuff gets really fun is when you start doing this ad hoc data

00:14:30,269 --> 00:14:36,740
analysis with it. If we go back to the facet by animal group parent and look at just these

00:14:36,740 --> 00:14:42,390
horse incidents, this map right here suddenly starts telling us a story. It shows that the

00:14:42,390 --> 00:14:47,950
most treacherous place to be a horse is in this area of London. It looks like it's around

00:14:47,950 --> 00:14:53,600
lakes. There's a whole bunch of incidents where horses get stuck in the water, rescued

00:14:53,600 --> 00:15:01,050
from water or mud. They thought -- they debate into water ways. There's a bunch of horse

00:15:01,050 --> 00:15:06,390
casualties there. And I presume that's popular riding stable there as well.

00:15:06,390 --> 00:15:12,899
So, of the original ideas for Datasette came from the work I was doing at The Guardian

00:15:12,899 --> 00:15:21,320
Newspaper in London. At The Guardian, we decided we wanted to start publish the data we were

00:15:21,320 --> 00:15:28,360
collecting behind some of our news stories. Newspapers have data journalists and they

00:15:28,360 --> 00:15:32,760
fetch numbers and facts and figures about the world and use them to help reporters tell

00:15:32,760 --> 00:15:37,490
the stories. What happened if you published the data behind the stories as well? This

00:15:37,490 --> 00:15:44,399
was back in 2009, 2010. And the way we did this after different options was to start

00:15:44,399 --> 00:15:50,470
a blog. The Guardian data blog, the idea was to publish the numbers behind the different

00:15:50,470 --> 00:15:57,280
stories in the newspaper. And the way we published that data was Google Sheets. Free and available,

00:15:57,280 --> 00:16:02,130
nice and easy to get started with. We didn't have to think about servers and hosting and

00:16:02,130 --> 00:16:05,320
so forth. We would put up stories and then publish these

00:16:05,320 --> 00:16:10,880
fabulously detailed spreadsheets that helped support the news and helped explain our workings

00:16:10,880 --> 00:16:17,010
and the stories that we were telling. But I always felt frustrated by this. I felt like

00:16:17,010 --> 00:16:21,360
there should be a better way to publish this kind of data. There should be something that's

00:16:21,360 --> 00:16:26,850
more open source, more flexible that lets people -- people do more -- do more interesting

00:16:26,850 --> 00:16:31,430
things with that data. And that was the idea that sort of drove the creation of Datasette

00:16:31,430 --> 00:16:35,851
three years ago. I was looking at this new trend of serverless hosting providers. Things

00:16:35,851 --> 00:16:43,170
like the cell and Google Cloud Run and AWS Lambda functions. And all have a restriction

00:16:43,170 --> 00:16:48,529
that you're not allowed to have a database because databases -- a database is a much

00:16:48,529 --> 00:16:52,480
more expensive thing to run. You have to have backups, you have to handle reads and writes

00:16:52,480 --> 00:16:57,230
and all of those kinds of things. And I realize that in my case, for newspaper

00:16:57,230 --> 00:17:02,200
data, none of it requires any writes at all. It's read-only blogs of data. So, the trick

00:17:02,200 --> 00:17:09,000
there is you can put them in a SQL-like database and deploy it as an asset as part of the application

00:17:09,000 --> 00:17:17,010
online. You can use incredibly cheap hosting providers for that. Back to the fire brigade

00:17:17,010 --> 00:17:23,699
demo. I'm going to demonstrate this by publishing this data to the Internet.

00:17:23,699 --> 00:17:35,820
I'm gonna start by installing a plugin called Datasette-publish-vercel. That's 

00:17:35,820 --> 00:17:40,740
one of the serverless hosting providers. And a file name, instants.db. Let me check my

00:17:40,740 --> 00:17:46,650
notes. What I was going to do with this. Let's do lfb-animal-rescues. Oh. Incidents.db-project.

00:17:46,650 --> 00:17:58,000
This 

00:17:58,000 --> 00:18:06,890
is going to take the database from earlier, bundle it up with the web application as well.

00:18:06,890 --> 00:18:14,000
It's going to push that combination up to Vercel to be deployed. They use Amazon Lambda

00:18:14,000 --> 00:18:17,910
functions under the hood. This is going to be wrapped up, and turned into a Lambda function

00:18:17,910 --> 00:18:28,280
and stick it online and assign a URL to refer to the project. There we go. This is the progress

00:18:28,280 --> 00:18:33,840
report on what it's doing. It's been running for 4 seconds and already installing runtimes

00:18:33,840 --> 00:18:41,460
and various dependencies. Normally this takes about 30 seconds. Of which point I will have

00:18:41,460 --> 00:18:45,880
an online version of the thing I have been just been showing you.

00:18:45,880 --> 00:18:54,340
Oh, it's uploading -- deploying build outputs. And here we go. There we go. And so, this

00:18:54,340 --> 00:18:59,200
is that exact demo that I just showed you, but it's running on the Internet now. Looking

00:18:59,200 --> 00:19:03,559
at the instance page, you'll see that the map is missing. I can run it again, and tell

00:19:03,559 --> 00:19:10,380
it that I want to install the cluster map plugin. And that will churn away and deploy

00:19:10,380 --> 00:19:16,580
a new version of this with that plugin installed. And I'm actually -- if you're in the -- hang

00:19:16,580 --> 00:19:22,360
on. If you're in the Google Doc, I'll drop a link to this in as well. If you want to

00:19:22,360 --> 00:19:27,470
-- if you want to try that out. Where is that Google Doc gone? Here we go.

00:19:27,470 --> 00:19:34,919
There we go. So, that's a link to the -- to the demo that I just deployed. So, this is

00:19:34,919 --> 00:19:39,340
pretty exciting. Because what we've got now is a workflow that lets me take some random

00:19:39,340 --> 00:19:45,390
data off the Internet, analyze it, make some minor tweaks to it, fix it up, do a little

00:19:45,390 --> 00:19:49,200
bit of data cleaning and then publish it online so that other people can then interact with

00:19:49,200 --> 00:19:56,350
it. Oh, there we go. This is the new version they just published that has the map. But

00:19:56,350 --> 00:20:00,290
now that it's online, now it's got a JSON API. If you want to build an application against

00:20:00,290 --> 00:20:07,110
the data, you can do that right now. The JSON API is available. It's ready for you to start

00:20:07,110 --> 00:20:13,900
using. If we go back to the ditch example, not Dutch, ditch. The API is a server API

00:20:13,900 --> 00:20:19,640
as well. We have a JSON API that lets you enter search terms in the URL bar and get

00:20:19,640 --> 00:20:25,730
the information back different ways. If you hit view and edit SQL here, it will show the

00:20:25,730 --> 00:20:38,110
SQL query the dataset used to return the results on the search. And this is editable as well.

00:20:38,110 --> 00:20:43,170
Give me the sunset number, the pump count and the final description. But drop out all

00:20:43,170 --> 00:20:52,560
of these other columns. I'll keep -- so, I've now run a custom SQL query.

00:20:52,560 --> 00:20:58,570
And I can even get that out of CSV. So, this is now a CSV file returning the -- a SQL query

00:20:58,570 --> 00:21:04,250
that's embedded in the URL -- in that URL up at the top of the page. And this is kind

00:21:04,250 --> 00:21:10,650
of fascinating, right? This is an API for returning JSON or CSV by executing SQL queries

00:21:10,650 --> 00:21:15,789
against in this case this arbitrary set of data. So, this also seems like it should be

00:21:15,789 --> 00:21:21,190
a big problem. Like SQL injection is one of the most -- one of the most damaging of security

00:21:21,190 --> 00:21:25,280
vulnerabilities. And I've built something where SQL injection is a documented feature

00:21:25,280 --> 00:21:31,080
of the entire package. I've got a couple of things in place to try and account for that.

00:21:31,080 --> 00:21:34,740
Firstly, this -- all of these queries have run against a read-only database.

00:21:34,740 --> 00:21:41,610
You can try to insert data, but it won't let you. And secondly, I have a 1 second time

00:21:41,610 --> 00:21:45,940
limit on the query. If you have a query and it takes longer than a second, you'll get

00:21:45,940 --> 00:21:51,419
back an error message instead. Combined with the fact that they're running on serverless

00:21:51,419 --> 00:21:56,380
hosting providers that can restart something if it breaks, it means this is a very robust

00:21:56,380 --> 00:22:03,010
way of getting these arbitrary different SQL queries. That's enough of the London fire

00:22:03,010 --> 00:22:08,660
brigade demo. I want to jump back and talk more about Dogsheep.

00:22:08,660 --> 00:22:15,731
This idea that I started to demonstrate earlier by showing you my swarm. And there's a story

00:22:15,731 --> 00:22:27,760
behind the name here. Dogsheep is -- oh, I've lost my window -- here we go. The inspiration

00:22:27,760 --> 00:22:38,750
was an essay written by Stephen Wolfram's, seeking the productive life: Some details

00:22:38,750 --> 00:22:46,470
of my personal infrastructure. It's kind of incredible. This is an essay about his approach

00:22:46,470 --> 00:22:50,919
to productivity. The principle feature here is the length of the scroll bar as you go

00:22:50,919 --> 00:22:58,160
down this. He mapped his heart rate, if the walking desk was good enough, it wasn't. He

00:22:58,160 --> 00:23:08,700
went outside with a custom laptop rig. And scanned every document since the age of 9

00:23:08,700 --> 00:23:15,480
and had OCR against those. Searching everything he's ever done. He's got maps of the places

00:23:15,480 --> 00:23:21,059
he's gone, a green screen room in his basement to help him give talks, give remote talks.

00:23:21,059 --> 00:23:25,650
It's a lot. There is an astounding amount of stuff in here. And I was reading through

00:23:25,650 --> 00:23:30,700
this thinking, this is totally -- this is way too much. This is -- this is not something

00:23:30,700 --> 00:23:34,570
I want in my life. Until I go to this bit. He started talking

00:23:34,570 --> 00:23:39,220
about archive and search. Where he described something called a meta searcher which is

00:23:39,220 --> 00:23:44,520
a search engine he built that search across all of these different sources of data about

00:23:44,520 --> 00:23:50,220
him and these archive things that he created. And that I really liked. I thought, you know,

00:23:50,220 --> 00:23:55,470
I would love to have a single search that covers all of my different personal -- all

00:23:55,470 --> 00:23:59,610
of the different aspects of things that I've built and things that I've saved and so forth.

00:23:59,610 --> 00:24:06,240
And so, I decided to build that. And because Stephen Wolfram had built this

00:24:06,240 --> 00:24:11,690
and I was building something inspired by it, but not really as good, I decided to call

00:24:11,690 --> 00:24:16,629
it Dogsheep. Because she's Stephen Wolfram, my thing was going to be called Dogsheep.

00:24:16,629 --> 00:24:21,440
And the thing that clinched it for me, I have this idea that he built a search engine called

00:24:21,440 --> 00:24:31,090
wolfram alpha. And I wrote Dogsheep beta. That stuck in my head and forced me into 12

00:24:31,090 --> 00:24:37,539
months of on and off development work to develop that. This is pun-driven development. I will

00:24:37,539 --> 00:24:47,990
show you Dogsheep. It's a dataset instance with swarm checkins, which I shows earlier.

00:24:47,990 --> 00:24:54,039
But a whole bunch of different data sources too. One example, I have all of my Tweets.

00:24:54,039 --> 00:24:59,600
But I have all of the Tweets that I've favorited. So, if you have wanted to search your favorited

00:24:59,600 --> 00:25:03,669
Tweets, because I have a database, I can do that. This is a search of CSV Tweets that

00:25:03,669 --> 00:25:14,160
I've favorited, apparently 110 Tweets over the past decade. I can search my followers.

00:25:14,160 --> 00:25:18,140
The interesting thing about people who follow you, is you can send them direct messages.

00:25:18,140 --> 00:25:25,370
So, if I want to talk to an investigative journalist, haven't tried this search before,

00:25:25,370 --> 00:25:29,200
I can have a search and see if there's anyone who follows me who describes themselves as

00:25:29,200 --> 00:25:34,990
a investigative journalist who I can drop a direct message to. My dog, Clio, has a Twitter

00:25:34,990 --> 00:25:44,799
account. Every time she goes to the vet, she Tweets a selfie and how much she weighs. I

00:25:44,799 --> 00:25:50,179
weigh 52.8 pounds. I'm not the only pet to gain weight during COVID, my vet assured me.

00:25:50,179 --> 00:25:56,440
This is fun. I can run a SQL query that uses a regular expression to pull out her weight.

00:25:56,440 --> 00:26:07,360
You can see the weight a. And this is data set Vega. This is Clio's weight over time

00:26:07,360 --> 00:26:12,630
by plotting her self-reported vet selfies which is even more useful than using this

00:26:12,630 --> 00:26:16,659
stuff to figure out what her favorite coffee shop is.

00:26:16,659 --> 00:26:23,840
I have all of my GitHub activity. I built a set of tools called GitHub to SQLite. All

00:26:23,840 --> 00:26:29,240
of my tools are called something to SQLite. So, GitHub to SQLite lets me fetch repositories

00:26:29,240 --> 00:26:36,140
I've starred, commits to), things and organizations I'm a member of. That turns out, that's a

00:26:36,140 --> 00:26:42,760
crazy useful thing to have. Here is a crazy example. This is a plot of the number of commits

00:26:42,760 --> 00:26:47,830
I have done per day since I started tracking. Is this per day? Yeah, this is commits per

00:26:47,830 --> 00:26:52,220
day. I have a chart over time of what I have been up to.

00:26:52,220 --> 00:26:57,170
One of the things this can do is pull in repositories that are dependent on my repositories. Now

00:26:57,170 --> 00:27:02,150
I can run SQL query that shows me recent new GitHub repos that are using software that

00:27:02,150 --> 00:27:06,010
I've written. I haven't looked at this in a while. There's a bunch of interesting stuff

00:27:06,010 --> 00:27:12,140
that I might want to go and take a look at. I use the Apple watch. The fascinating thing

00:27:12,140 --> 00:27:17,690
about the Apple watch is Apple have a really responsible approach to this stuff. This watch

00:27:17,690 --> 00:27:23,650
stores a ridiculous amount of data about me and keeps it on the phone and doesn't upload

00:27:23,650 --> 00:27:30,169
to a creepy cloud server. I found out there's an export option in the Apple health app.

00:27:30,169 --> 00:27:38,700
You can go in and I think I've got a screenshot of this. I can literally click export health

00:27:38,700 --> 00:27:46,030
data. And I can get 145-megabyte zip file of all of my health data that I can then air

00:27:46,030 --> 00:27:52,610
drop over to my laptop. I wrote a talk called healthkit to SQLite that takes it and turns

00:27:52,610 --> 00:27:59,060
it into data tables. I have my body fat percentage over time, the distance swimming I've done,

00:27:59,060 --> 00:28:03,559
my headphone audio exposure showed up recently, the number of mindful sessions I've done.

00:28:03,559 --> 00:28:08,680
I'm not a very mindful person. Sleep analysis. All kinds of crazy stuff.

00:28:08,680 --> 00:28:14,340
But what's really fun is any time you track a workout on the Apple watch, going for a

00:28:14,340 --> 00:28:21,700
run or a walk, it tracks your latitude and longitude every few seconds for the duration

00:28:21,700 --> 00:28:27,360
of the workout. I ran a San Francisco half marathon a few years ago. And this is my exact

00:28:27,360 --> 00:28:33,740
route along the San Francisco marathon. When I realized this was going on, I started deliberately

00:28:33,740 --> 00:28:38,720
starting a walking -- a walking workout any time I was anywhere interesting at all. On

00:28:38,720 --> 00:28:43,470
a hike or going for a walk around time. And over the past few years, this has resulted

00:28:43,470 --> 00:28:48,919
in 2.2 million latitude and longitude points that I've collected through my watch that

00:28:48,919 --> 00:28:54,029
are available just to me. And so, this is a map of all the places where I've recorded

00:28:54,029 --> 00:28:58,590
one of these workout sessions. I have done very little with this data so

00:28:58,590 --> 00:29:02,779
far. But it's kind of incredible that I can -- that richness of data is literally just

00:29:02,779 --> 00:29:07,690
sitting in a little SQLite database, because the Apple watch using SQLite, on my wrist

00:29:07,690 --> 00:29:13,510
waiting for me to pull it out. A couple more demos and then I'll move to questions. I did

00:29:13,510 --> 00:29:18,970
23 and Me. Turns out you can export your genome as a CSV file. I did. That means I can run

00:29:18,970 --> 00:29:26,831
SQL queries that tell me what color my eyes are based on my genome. This tells me that

00:29:26,831 --> 00:29:34,169
my blue 99% of the time. I tried this against my wife's eyes, and they were brown. It works

00:29:34,169 --> 00:29:41,960
against a sample size of two at least. And I use the Apple photos app. Apple photos

00:29:41,960 --> 00:29:47,650
uses a SQLite database under the hood. You can suck your metadata out of the database

00:29:47,650 --> 00:29:53,080
and build your own interface on to it. Here are 46,000 photographs I have taken in Apple

00:29:53,080 --> 00:29:59,299
maps. I can see the recent photographs and run SQL queries against them. A particularly

00:29:59,299 --> 00:30:04,390
cool thing about this is that it turns out Apple run machine learning models on your

00:30:04,390 --> 00:30:09,210
phone and on your laptop to figure out what's in your pictures. And that one sat in SQLite

00:30:09,210 --> 00:30:14,529
as well. This one was labeled bird, pelican, blue sky, animal. It knows what a pelican

00:30:14,529 --> 00:30:24,090
is. And I can run this SQL query of photos we have taken of pelicans. Delightful. They

00:30:24,090 --> 00:30:32,480
have fabulous scoring metrics for overall aesthetic score and harmoniously score. This

00:30:32,480 --> 00:30:37,490
is the most esthetically-pleasing photo I have taken of a pelican according to the Apple

00:30:37,490 --> 00:30:46,570
algorithms. Which is super fun to dig around in.

00:30:46,570 --> 00:30:52,490
This is all from wanting to build Dogsheep beta which I got working a few months ago.

00:30:52,490 --> 00:30:59,370
This is a unified search engine across the different stuff. My Git commits and my Tweets

00:30:59,370 --> 00:31:04,140
and my photographs and things I've quoted on my blog and things I've posted on hacker

00:31:04,140 --> 00:31:11,000
news and all of this stuff. And I can search for llama Portland and a photograph from last

00:31:11,000 --> 00:31:18,730
time I went to csv,conf when I got to hang out with a lovely llama in Portland. Again,

00:31:18,730 --> 00:31:25,270
I'm just scratch the surface. There's so many sources of data you can pull into this. But

00:31:25,270 --> 00:31:30,789
the lesson I've learned and the most important thing I've gotten out of this, if you can

00:31:30,789 --> 00:31:34,250
get stuff into SQLite, you can do basically anything with it.

00:31:34,250 --> 00:31:40,230
The problem with Facebook export and Twitter export, they're all zip files of XML. Be if

00:31:40,230 --> 00:31:44,880
you do the work to convert those, you would you can start combining them and visualizing

00:31:44,880 --> 00:31:51,080
them and viewing them in all kinds of really exciting ways. I've got a couple of extra

00:31:51,080 --> 00:31:55,000
demos which I'm going to leave in the document because I want to leave time for questions.

00:31:55,000 --> 00:32:01,690
But, yeah. So, my next steps for this project. Firstly, I want to get a 1.0 release out.

00:32:01,690 --> 00:32:09,850
Where 1.0 means stability for plugin authors. At the moment, I don't guarantee I'm not going

00:32:09,850 --> 00:32:15,169
to break the release with the next release of the software. Once I hit 1.0, plugins and

00:32:15,169 --> 00:32:23,179
custom templates will be guaranteed to keep working up until Datasette 2.0. Hopefully

00:32:23,179 --> 00:32:29,659
I'll never get as far as that version. And then for Dogsheep, I want this to be easier

00:32:29,659 --> 00:32:35,840
to run. A lot of people asked about a hosted version of this. That's incredibly uncomfortable.

00:32:35,840 --> 00:32:41,520
Because it's extremely private personal data for people. I would much rather encourage

00:32:41,520 --> 00:32:46,909
people to run this stuff on machines they control as opposed to trusting some cloud

00:32:46,909 --> 00:32:51,820
service for it. I just got a Raspberry Pi a few weeks ago and hoping to figure out how

00:32:51,820 --> 00:32:57,260
to get a recipe to run this stuff on a Raspberry Pi so you can have a $30 computer that sits

00:32:57,260 --> 00:33:03,929
on your network and does all this stuff for you.

00:33:03,929 --> 00:33:11,860
If this piqued your interest, Datasette.io ties all of this together. You can also book

00:33:11,860 --> 00:33:19,090
office hours conversations with me. If you want a one-on-one conversation about the projects,

00:33:19,090 --> 00:33:24,500
I'm keen to talk to people who are using this stuff or thinking about using it. With that,

00:33:24,500 --> 00:33:29,040
I'm going to jump over to the Q&A document which is here.

00:33:29,040 --> 00:33:42,480
Here we go. Okay. So, fantastic. How many llamas were rescued in London is now one of

00:33:42,480 --> 00:33:46,350
my burning questions. I didn't see if there were any llamas in there. Let's have a quick

00:33:46,350 --> 00:33:56,620
look. Where did I put that thing? Hold on. Here we do. LFB for animal rescues. I'm just

00:33:56,620 --> 00:34:02,010
going to search llama and see what comes up. I will be disappointed if there's no llamas.

00:34:02,010 --> 00:34:06,260
It's good for the llamas. Fortunately, no llamas have had to be rescued by the London

00:34:06,260 --> 00:34:16,060
fire brigade. Any chance there's an R-friendly version in the works?

00:34:16,060 --> 00:34:23,619
So, my feeling on interacting with other language ecosystems is it's all about the APIs. So,

00:34:23,619 --> 00:34:30,079
Datasette can produce JSON and CSV. Which means if you're using R, you can handle both

00:34:30,079 --> 00:34:34,909
formats. You can load that stuff into your environment. Likewise, I'm really keen on

00:34:34,909 --> 00:34:46,519
observable notebooks. The JavaScript equivalent of Jupyter notebooks. And there was a new

00:34:46,519 --> 00:34:54,739
observable plot out this morning. Pulling in a dataset and uses observable to render

00:34:54,739 --> 00:35:01,609
that. I think there's an enormous amount of value that can be built in admitting -- Python

00:35:01,609 --> 00:35:05,670
is not the language to use for everything. But if you have the clean API boundaries between

00:35:05,670 --> 00:35:13,069
Datasette and other languages. R can do anything that JavaScript is capable of pulling from

00:35:13,069 --> 00:35:19,410
an instance underlying it. How big can SQLite file be before you can

00:35:19,410 --> 00:35:30,089
run into a performance issue? Up to a gigabyte, it's great. Up to 10, make sure you have indexes

00:35:30,089 --> 00:35:37,900
on things you're querying against. Above 10 gigabytes. The theoretical maximum is 1.4

00:35:37,900 --> 00:35:46,749
terabytes. But so far once I've gone above 10 gigs, it's a little bit creeky. There are

00:35:46,749 --> 00:35:52,039
SQL-like cache settings that would make it work better, but I have not done that piece

00:35:52,039 --> 00:35:59,369
of work yet. If you're below a gigabyte, SQLite is flawless.

00:35:59,369 --> 00:36:05,680
This amazing link. If you haven't seen this, check out this link. This is an incredible

00:36:05,680 --> 00:36:13,829
thing -- piece of work. This chap here figured out how to have JavaScript run a -- a version

00:36:13,829 --> 00:36:19,190
of SQLite compiled to WebAssembly. And that's been done before. And so, you can have SQLite

00:36:19,190 --> 00:36:25,859
in your browser. But he figured out to have it use HTTP Range queries to fetch just little

00:36:25,859 --> 00:36:31,390
portions of the overall database file that he needed. he's got a 670-megabyte staticically

00:36:31,390 --> 00:36:41,549
hosted SQLite database. I can run queries against this. I can do, record by rowid desc

00:36:41,549 --> 00:36:52,960
limit 100. Here we go. It ran 49 HTTP requests, fetched 54-kilo bytes of data in chunks from

00:36:52,960 --> 00:36:57,829
the database. And that was enough to answer the query. I was blown away by this. I assumed

00:36:57,829 --> 00:37:01,779
that SQLite in the browser would be limited by the fact that you had to fit the database

00:37:01,779 --> 00:37:08,680
file in the browser's memory. Turns out you don't.

00:37:08,680 --> 00:37:13,069
If you have a table that's a full table scan, it will have to pull down the database. It's

00:37:13,069 --> 00:37:21,259
only for carefully designed queries. But this is the world of SQLite as a unified format

00:37:21,259 --> 00:37:28,410
for publishing data. Do I have examples of people using Datasette for real world applications.

00:37:28,410 --> 00:37:34,869
My target for Datasette is data journalists. I love the idea of supporting journalists

00:37:34,869 --> 00:37:40,180
who are -- actually helping them find stories in the data and secondly, publishing that

00:37:40,180 --> 00:37:48,420
data online. I know a bunch of publications are using Datasette internally. ProPublica

00:37:48,420 --> 00:38:00,749
have used it. But I have the Baltimore Sun who published salary records of public employees

00:38:00,749 --> 00:38:07,809
in the State of Maryland. What's interesting here, this is just Datasette.

00:38:07,809 --> 00:38:11,820
But they're using Datasette's custom templates and theming to add their own color scheme

00:38:11,820 --> 00:38:17,741
and put a custom interface on top of that. That's an important part of Datasette. Custom

00:38:17,741 --> 00:38:27,569
Templates and CSS means you can use it to build datasets. This is actually just Datasette.

00:38:27,569 --> 00:38:39,420
Go to Datasette.io/content. The database is running off. There's a stat -- there's a news

00:38:39,420 --> 00:38:44,239
database which is the news that shows up on home page. All of these have different bits

00:38:44,239 --> 00:38:49,809
and pieces. And I'm increasingly exploring this theme of using Datasette to build sites.

00:38:49,809 --> 00:38:56,700
Another website I built is niche-museums.com. You can dial in location and it shows you

00:38:56,700 --> 00:39:05,779
niche-museums near you. Unfortunately the Burlingame Museum of PEZ closed last year.

00:39:05,779 --> 00:39:14,720
If you add/browse to the URL, you can see the underlying data is the bunch of data in

00:39:14,720 --> 00:39:19,329
the museums table. And the custom templates on the home page load the data and display

00:39:19,329 --> 00:39:26,880
it in different ways. Oh, does anyone know what time the session

00:39:26,880 --> 00:39:31,499
ends? If you could add that to the document at the bottom, that would be useful. I forgot

00:39:31,499 --> 00:39:36,789
to check before I started. >> You've got 15 minutes, Simon.

00:39:36,789 --> 00:39:42,069
Simon: Brilliant, I have additional demo most to throw in. Do you have any information for

00:39:42,069 --> 00:39:54,289
getting data off your Mac. Most of the Dogsheep data comes from web APIs. The swarm API, there's

00:39:54,289 --> 00:39:59,480
the swarm and the Twitter and the GitHub API. I'm actually running a $10 a month Digital

00:39:59,480 --> 00:40:05,720
Ocean serve we are a bunch of crones. Every 10 minutes I have cron script that runs and

00:40:05,720 --> 00:40:14,329
fetches my latest Tweets and things I have on GitHub. And actually, I think I've got

00:40:14,329 --> 00:40:21,579
a copy of the cron tab that I will drop into that document. The harder part is the stuff

00:40:21,579 --> 00:40:28,329
on my Mac. I've shown you the healthkit data goes via my Mac. And so do the photos.

00:40:28,329 --> 00:40:33,619
Right now I've not automated those. Every now and then, I remember to manually export

00:40:33,619 --> 00:40:40,509
my healthkit data, run a script on my Mac and SCP the healthkit.db file back up to my

00:40:40,509 --> 00:40:45,619
server. I've got a script that does the same thing with my photos. I would love to do better

00:40:45,619 --> 00:40:51,140
with that. The biggest problem is I use a laptop. I can't have cron that runs at 3:00

00:40:51,140 --> 00:40:58,219
in the morning because the lid might be closed. But yeah. There's also a really fun trick

00:40:58,219 --> 00:41:10,589
you can do where you can scan your Mac... let's see if this works. Find. There is a

00:41:10,589 --> 00:41:23,759
way to -- now I can't find it. There we go. There's a command you can run on your Mac

00:41:23,759 --> 00:41:29,319
that will find the largest -- find the largest SQLite database files. And it turns out you're

00:41:29,319 --> 00:41:35,599
going to have a lot. So many Mac and iOS applications use SQLite as their format. And this is really

00:41:35,599 --> 00:41:39,190
fun because you can start digging in and saying, okay, what are these apps storing for me?

00:41:39,190 --> 00:41:43,670
What is the signification and how to process it?

00:41:43,670 --> 00:41:49,900
Is there a Dogsheep application to run ravel the Git history with dates. There is a technique

00:41:49,900 --> 00:41:55,420
I have been creating with Git scraping. I have been promoting this idea quite a bit

00:41:55,420 --> 00:42:03,520
over the last few years. The idea is you use GitHub Actions or CircleCI or a scheduled

00:42:03,520 --> 00:42:11,420
cron to run a scrape, store the results in the scraper, commit them to a GitHub repository.

00:42:11,420 --> 00:42:17,690
The reason you do this is that it gives you a free commit history of changes. So, this

00:42:17,690 --> 00:42:24,730
is the -- the California fire department run this website with currently ongoing fires.

00:42:24,730 --> 00:42:31,519
I've got a scraper. Where are we? It runs three time answer an hour and grabs the latest

00:42:31,519 --> 00:42:39,499
copy and commits it. This is a copy of the most recent version of the page. Showing me

00:42:39,499 --> 00:42:45,829
what fires are live right now. But if I go to the history of this file, I've got -- how

00:42:45,829 --> 00:42:52,819
many commits have I got? 445 snapshots of the different incidents. Which is -- so, I've

00:42:52,819 --> 00:42:57,400
got a unique set of data here because they don't publish that history. But I've captured

00:42:57,400 --> 00:43:01,999
that history and I can use that to do -- I mean, theoretically, I can use this to analyze

00:43:01,999 --> 00:43:06,410
fires over time. What I'm actually doing is leaving it sat there in a Git repository.

00:43:06,410 --> 00:43:12,539
But, yeah. So, the question here was, do I have any Dogsheep tools for unraveling that

00:43:12,539 --> 00:43:18,849
Git history with dates? I have not built specific -- any reusable tools for that yet. What I

00:43:18,849 --> 00:43:24,750
have been doing is trying to collect patterns for handling that. Oh, here's the most recent

00:43:24,750 --> 00:43:28,069
one. So, the C DC have a website that tells you

00:43:28,069 --> 00:43:34,309
about -- they've got a website that tells you how people are doing at being -- how different

00:43:34,309 --> 00:43:39,940
counties are doing at being vaccinated. It's their COVID Data Tracker. They have an API

00:43:39,940 --> 00:43:43,949
for it. It's an undocumented API. It's on the pages on their website. What I have been

00:43:43,949 --> 00:43:54,049
doing is I have been grabbing a copy of that API every I think once a day. I have 111 snapshots.

00:43:54,049 --> 00:44:00,329
I wrote a script called bydatabase.py, looping through the history using a Python model and

00:44:00,329 --> 00:44:09,509
turns that into a SQLite database. I'm experimenting with other patterns for doing this.

00:44:09,509 --> 00:44:14,289
But this is a good example of how you can take one of these Git histories and turn that

00:44:14,289 --> 00:44:19,640
into a file. Once I've done that, I publish that online. This is published by the GitHub

00:44:19,640 --> 00:44:28,739
Actions. This is a dataset instance showing me the daily reports of percentage of the

00:44:28,739 --> 00:44:33,089
population who have been vaccinated in different counties around the states. There are 125,000

00:44:33,089 --> 00:44:39,480
resource. It's small, but it's not tiny. And then what I did with that, I built an

00:44:39,480 --> 00:44:47,450
observable notebook which takes that data and uses it to plot visualizations. I think

00:44:47,450 --> 00:44:54,779
it's this one. Here we go. So, this right here is choropleth, like a heatmap of the

00:44:54,779 --> 00:45:03,220
counties in the United States and over time shows you how vaccination is going in the

00:45:03,220 --> 00:45:08,809
counties. What this is doing is hitting that dataset JSON export with a custom SQL query,

00:45:08,809 --> 00:45:13,209
pulling that data back into observable and looping through it.

00:45:13,209 --> 00:45:18,849
But it's a good example how you can use Git scraping to get the data, use SQLite to store

00:45:18,849 --> 00:45:26,609
it, the datasets online and the notebook to turn that into the animation of the -- I think

00:45:26,609 --> 00:45:31,900
it's percentage of over 65s who have been vaccinated in those US counties. So, I will

00:45:31,900 --> 00:45:41,640
drop that into the document too. I have not yet engaged with the Open Humans

00:45:41,640 --> 00:45:47,069
Community directly. There are a whole bunch of -- there are definitely a lot of projects

00:45:47,069 --> 00:45:55,339
in this space. In the space of helping people process their personal data. My philosophy

00:45:55,339 --> 00:45:59,390
on this is, I don't care where the data comes from, I don't care what shape it is. All I

00:45:59,390 --> 00:46:03,349
can care is that I can get it into that SQLite database file. And one of the beautiful things

00:46:03,349 --> 00:46:09,699
about having it in SQL, you import data in different shapes from different sources, you

00:46:09,699 --> 00:46:17,559
can reshape it with a SQL query. Select these columns, rename this, union select this other

00:46:17,559 --> 00:46:23,599
columns from another table, and knock that different sorts of data into shape. I have

00:46:23,599 --> 00:46:29,140
been skipping out on the let's figure out a standard format for the stuff. As long as

00:46:29,140 --> 00:46:35,819
I have it in SQLite, the standard format can come later. But I'm very keen on -- I mean,

00:46:35,819 --> 00:46:40,020
all of this stuff is open source. I'm super-interested in working with other people on taking these

00:46:40,020 --> 00:46:42,749
projects forward. Especially with regards to making this stuff

00:46:42,749 --> 00:46:47,520
more accessible to people. Right now if you have to run Datasette and Dogsheep, you've

00:46:47,520 --> 00:46:51,329
got a lot of work on your hands. Firstly, you have to know how to install Python and

00:46:51,329 --> 00:46:55,890
how to install tools written in Python. You're probably going to want a server somewhere,

00:46:55,890 --> 00:47:04,499
how to secure that, cron jobs and deal with APIs. It's a lot. And while I want to put

00:47:04,499 --> 00:47:09,339
together better documentation and tutorials, if we could make that stuff -- I think if

00:47:09,339 --> 00:47:13,880
we could take the ability to pull in your personal data from different sources and then

00:47:13,880 --> 00:47:18,569
analyze it yourself and turn that into an installable application that you can install

00:47:18,569 --> 00:47:23,170
on your phone or computer, that would open up the flood gates to make this available

00:47:23,170 --> 00:47:29,079
so many more people. I don't want to do all of that work because that's a year or so of

00:47:29,079 --> 00:47:33,940
work just on the -- on the sort of building out that -- those user-facing tools. But I'm

00:47:33,940 --> 00:47:37,739
super-keen on working with anyone who does want to put in that effort to help democratize

00:47:37,739 --> 00:47:41,740
these things. Is there a plugin for modeling tools? I'm

00:47:41,740 --> 00:47:48,069
not sure what you mean by modeling tools. So, probably not. One thing I can show you,

00:47:48,069 --> 00:47:57,499
though, is I've got a observable notebook which -- which does entity relationship diagrams.

00:47:57,499 --> 00:48:04,790
Maybe if I search entity. You know what? I'm going straight to observable for this one.

00:48:04,790 --> 00:48:11,019
Yeah. So, Datasette exposes metadata about your databases as well. Here we go. I can

00:48:11,019 --> 00:48:17,729
give it the URL to a Datasette instance and this tool here pulls in the foreign key relationships

00:48:17,729 --> 00:48:23,709
in the tables and draws you a -- and draws you a diagram of how those tables fit together.

00:48:23,709 --> 00:48:27,009
Which the GitHub one is particularly fun for this because it has a lot of relationships

00:48:27,009 --> 00:48:33,190
going on. And the joy of observable notebooks is once again, this is -- anyone can see this

00:48:33,190 --> 00:48:38,579
and absolute source and fork it and do new things with it. It's why giving something

00:48:38,579 --> 00:48:43,880
a JSON API and putting it online opens up so many opportunities for visualizations and

00:48:43,880 --> 00:48:53,049
collaborations and things like that. So, list of services that you can recommend

00:48:53,049 --> 00:48:59,890
for -- for those of us who want to deploy datasets but not figure out options from scratch.

00:48:59,890 --> 00:49:06,369
I think the cell is the best one at the moment. It's got the best developer experience. You

00:49:06,369 --> 00:49:15,829
can install it. Run Vercel. I think it's Vercel login to create an account with an email address.

00:49:15,829 --> 00:49:22,509
Publish and off you go. At the same time, I'm always interested in new options for hosting

00:49:22,509 --> 00:49:26,779
these things. So, I've got a section in the documentation about publishing data which

00:49:26,779 --> 00:49:34,680
talks about Google Cloud Run, Heroku, Vercel, and the fly, the four I have dedicated support

00:49:34,680 --> 00:49:45,089
for. Google Cloud Run is my favorite for larger sets. Vercel has a 50-megabyte cap. Google

00:49:45,089 --> 00:49:54,510
Cloud Run is unlimited. Over about 1.5 gigabytes of data, you get various errors. But for data

00:49:54,510 --> 00:50:01,970
up to a gigabyte, it's work well. Because they scale to zero, they only charge for when

00:50:01,970 --> 00:50:12,300
they are being used. They mostly cost 50 cents a month. One costs $15 a month because it

00:50:12,300 --> 00:50:18,890
was being hammered by search engine crawlers. This is an inexpensive way of publish data.

00:50:18,890 --> 00:50:27,380
Vercel is effectively free for these datasets. It's great to look at Vercel for this.

00:50:27,380 --> 00:50:39,130
There we go. Do I need help with documentation? I need help with so many things. So, the -- so,

00:50:39,130 --> 00:50:44,479
Datasette. As you can see, there's a lot of it. There are almost a hundred different projects

00:50:44,479 --> 00:50:52,309
under the Datasette project now. That's fine. I'm enjoying it. One of the reasons I'm so

00:50:52,309 --> 00:50:56,670
engaged with this project is that it's had so many different areas that I can explore

00:50:56,670 --> 00:51:00,849
with it. But I think the -- when it comes to contributing to open source, there are

00:51:00,849 --> 00:51:05,140
a whole bunch of ways you can contribute before you even get to the point of like sending

00:51:05,140 --> 00:51:09,789
a pull request with code. The things like particularly interesting people doing, firstly,

00:51:09,789 --> 00:51:14,920
it's using Datasette and showing people what they've built. So, actually putting things

00:51:14,920 --> 00:51:19,630
up there and saying, hey, I used this tool to solve this problem. If you write about

00:51:19,630 --> 00:51:23,789
-- especially if you write a few notes on what works, what didn't. Like, give me a little

00:51:23,789 --> 00:51:29,150
bit of feedback on how the tooling works. That's super-useful. The Datasette ecosystem

00:51:29,150 --> 00:51:36,099
is missing tutorials right now. It's got 150 pages of documentation, great reference documentation,

00:51:36,099 --> 00:51:44,160
I think. But not here's how to get started with Datasette. So, tutorials -- video tutorials

00:51:44,160 --> 00:51:50,579
are super-interesting as well. Writing tutorials is fantastic. And the step up from that, yeah,

00:51:50,579 --> 00:51:54,059
absolutely. If you look through the Datasette documentation and find things that are not

00:51:54,059 --> 00:51:59,109
clear, or miss or could be worded better, really keen on feedback and pull requests

00:51:59,109 --> 00:52:04,910
and contributions on that. And the other thing that's really fun about Datasette is you can

00:52:04,910 --> 00:52:09,369
write plugins for it. And the thing that I love about plugins is

00:52:09,369 --> 00:52:13,829
plugins are features that my features that my software gets overnight without reviewing

00:52:13,829 --> 00:52:17,869
a pull request. I can literally wake up in the morning, if someone has released a new

00:52:17,869 --> 00:52:24,210
plugin, the software has new features. So, I'm like -- the sort of grant vision I have

00:52:24,210 --> 00:52:29,890
for Datasette is sort of inspired by WordPress. Where WordPress is a perfectly decent content

00:52:29,890 --> 00:52:35,029
management system with 7,000-odd plugins that mean that it can solve any publishing problem

00:52:35,029 --> 00:52:39,449
you can think of. So, my sort of grand dream for Datasette, I would love to have Datasette

00:52:39,449 --> 00:52:45,760
as this core utility for publisher data, making available APIs and giving you basic exploration.

00:52:45,760 --> 00:52:52,099
And then plugins that add every kind of visualization you might want. Every kind of data modification

00:52:52,099 --> 00:52:58,619
routine, data cleanup. All sorts of things. If the plugins -- plugins can pick up on those,

00:52:58,619 --> 00:53:05,150
I think that would be an amazingly exciting way for -- for Datasette to grow. I could

00:53:05,150 --> 00:53:09,819
solve -- it could solve so many more problems with that thriving plugin ecosystem.

00:53:09,819 --> 00:53:14,609
And then beyond that, obviously, if people want to work -- hack on Datasette and the

00:53:14,609 --> 00:53:20,559
Datasette projects themselves, it's all open source. I'm very eager to have conversations

00:53:20,559 --> 00:53:25,109
and issues, accept pull requests, review code. I think it's had about 30 contributors so

00:53:25,109 --> 00:53:30,920
far. But I'm super-interested in growing the -- the field of people that are actively contributing

00:53:30,920 --> 00:53:35,489
to the software. So, have you thought about running this in

00:53:35,489 --> 00:53:42,449
the browser? For example, using Pyodide? I've briefly thought about this, but no deeper

00:53:42,449 --> 00:53:47,249
investigations into it. So, Pyodide, if you haven't seen it, it's absolutely phenomenal.

00:53:47,249 --> 00:53:54,650
I think it was a search team at Mozilla. They got Python working with WebAssembly and got

00:53:54,650 --> 00:54:12,029
Jupyter running in that as well. You can run it through WebAssembly. I'm not -- I'm happy

00:54:12,029 --> 00:54:16,920
enough running server -- it's like one of the unique had selling points I thought was

00:54:16,920 --> 00:54:23,440
you can have a gigabyte database and only return the Bits people need. And yesterday,

00:54:23,440 --> 00:54:27,759
this chap demonstrated that's not true. And you can pull ranges of data from SQLite. But

00:54:27,759 --> 00:54:33,549
I think generally I really like the idea of publishing data along with APIs such that

00:54:33,549 --> 00:54:38,961
you can build -- such that you can -- you're not limited by what fits in the browser. You've

00:54:38,961 --> 00:54:43,329
got a server helping you out. The browser itself can do the visualizations and all of

00:54:43,329 --> 00:54:48,269
that other stuff. If somebody gets working at Pyodide, I would be interested to see it.

00:54:48,269 --> 00:54:56,359
But I don't think that's something I'm going to work on myself.

00:54:56,359 --> 00:55:00,209
I'm going to quickly show one more demo. I wanted to get to this earlier. But I thought

00:55:00,209 --> 00:55:11,970
I should cut things off. SQLite has SpatiaLite. It's like postgres. Adds GenomeMetric functions

00:55:11,970 --> 00:55:22,789
to SQLite. You can publish datasets with the SpatiaLite. This is a plugin I wrote called

00:55:22,789 --> 00:55:32,219
Datasette leaflet free draw to feed shapes into a SQL query. What I have here is a SQL

00:55:32,219 --> 00:55:38,699
query I wrote which looks for mini parks in California using this amazing dataset from

00:55:38,699 --> 00:55:44,799
the green info network. And what this lets me do is draw a circle around an area of a

00:55:44,799 --> 00:55:51,949
map. Draw like that, run the query, and this will show me all of the parks within the shape

00:55:51,949 --> 00:55:56,880
that have the word 'mini" in their name. This is a results set of all the mini parks in

00:55:56,880 --> 00:56:03,059
San Francisco. Or draw around Oakland within and run that again. Now getting back mini

00:56:03,059 --> 00:56:09,679
parks in Oakland. Really I built this as a demonstration that you can have plugins that

00:56:09,679 --> 00:56:18,530
provide user input. In this case, draw a circle. It inputs a giant blob of GeoJSON, a SQL query

00:56:18,530 --> 00:56:26,099
and returns the results. But I feel like this model of plugin, this idea of building these

00:56:26,099 --> 00:56:31,900
sort of richer interface elements which just -- in this case -- it magically adds this

00:56:31,900 --> 00:56:35,339
interface if you have a parameter to your SQL called free draw.

00:56:35,339 --> 00:56:44,759
If I change this to FreeDraw 1. And something else, that map goes away. It's -- oh. I think

00:56:44,759 --> 00:56:50,069
I've got FreeDraw in there twice. That's why. But, yeah. So, without -- without code on

00:56:50,069 --> 00:56:56,130
FreeDraw as a parameter, you don't get the widget. With it, you do. Yeah. And so, there's

00:56:56,130 --> 00:57:02,910
a whole scope of dataset around these spatial queries, building up GIS tooling as well.

00:57:02,910 --> 00:57:08,670
Which, again, I've only started investigating. But it feels like a lot of potential too.

00:57:08,670 --> 00:57:15,880
Popping back to the questions. I think we are just about out of time. I think. yes.

00:57:15,880 --> 00:57:21,029
>> Simon, we could keep going forever. I'm having a great time and I think everyone else

00:57:21,029 --> 00:57:27,729
is too. This might be a good place to stop. Thank you for a very exciting keynote. For

00:57:27,729 --> 00:57:32,209
those of you still adding questions to the doc, please do so. Simon will look at it and

00:57:32,209 --> 00:57:33,779
add it. We are done for day one of csv,conf,v6. We look forward to seeing you tomorrow. Thank

00:57:33,779 --> 00:57:34,779
you, all. Simon: Thanks a lot, everyone!

00:57:34,779 --> 00:57:35,279

YouTube URL: https://www.youtube.com/watch?v=UFn82w-97kI


