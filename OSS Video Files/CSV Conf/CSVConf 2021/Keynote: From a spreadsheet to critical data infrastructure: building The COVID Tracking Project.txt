Title: Keynote: From a spreadsheet to critical data infrastructure: building The COVID Tracking Project
Publication date: 2021-05-19
Playlist: CSVConf 2021
Description: 
	Presenters:  Julia Kodysh, Michal Mart, Kevin Miller, Kara Schechtman 

The COVID Tracking Project (https://covidtracking.com/about) began in March 2020 as a stopgap spreadsheet maintained by a handful of journalists, hoping to provide some information on COVID across the US until the federal government stepped in. But that day never came. Powered by over a thousand volunteers collecting data from disparate state and federal systems every day, the project accidentally became an indispensable source of data used by governments, individuals, and institutions to make critical decisions. The decentralized nature of public health infrastructure in the United States, which is mostly managed by overstretched health departments at the local and state level, made it impossible to automate the collection and normalization of data on the pandemic. States, which suddenly found themselves needing to produce and report data out of underfunded and overstretched systems, produced COVID dashboards that were all different from each other, didn't provide APIs and used technologies that are difficult to scrape. Human gleaning of the data from these systems allowed us to identify sudden changes in reporting, keep an eye on data definitions, and develop a deep well of experience and metadata that informed how we reported every state's data. Volunteers not only do the critical work of data collection, but through their experience in working with the data, are empowered to make key decisions in our analysis and reporting. Our tooling has matured as we have tested the edges of the possible with Google Sheets. We still use spreadsheets, but have developed more powerful tools to ensure data quality and improve our publishing process. This infrastructure allows teams working on our website and API to use a reliable dataset that serves millions of users a day. The COVID Tracking Project became the de-facto source of COVID data for so many because of a community built in Slack channels by strangers. The tools and sheets and websites we have built are impressive and useful for others to learn from. But the biggest legacy of the project will be the thousands of people who caught a glimpse of the best of themselves during a terrible time.
Captions: 
	00:00:03,520 --> 00:00:09,920
Okay. Welcome, everybody, to our  very first keynote of csv,conf 2021.  

00:00:11,280 --> 00:00:16,880
I'm really, really excited to have the incredible  team behind the COVID Tracking Project.  

00:00:17,520 --> 00:00:23,440
For those that are not familiar with the  effort, it is a volunteer effort that came  

00:00:23,440 --> 00:00:28,480
together in the early days of the pandemic  when data was really, really hard to come by  

00:00:29,520 --> 00:00:32,160
and there was a lot of missing  information everywhere,  

00:00:32,880 --> 00:00:40,080
and so, a group of technologists and volunteers  came together to fill in the gaps, collect  

00:00:40,080 --> 00:00:46,240
and publish data to help states and other agencies  understand what was happening with the outbreak.  

00:00:47,840 --> 00:00:52,240
The project has done an incredible amount  of effort over the past year, and they  

00:00:53,920 --> 00:00:59,120
have just about wound down at this point, so  this is a really great time to hear from them  

00:00:59,120 --> 00:01:07,920
as the project wraps up. To learn all the  challenges that they've overcome in the past year. 

00:01:08,560 --> 00:01:13,920
I'll do a quick round of introductions.  We have Michal Mart, who is  

00:01:15,280 --> 00:01:20,640
one of the data quality leads for the COVID  Tracking Project. We have Kara Schechtman,  

00:01:20,640 --> 00:01:27,120
who is another data quality lead as well. She'll  be starting a master's at standard very soon.  

00:01:28,640 --> 00:01:33,440
We've got Julia Kodysh, who is a data  infrastructure lead for the project,  

00:01:34,240 --> 00:01:41,200
and last but not least, Kevin Miller, who is the  website lead for the project. Thank you‑all for  

00:01:42,480 --> 00:01:58,000
speaking here today, and I will let you take over. >> Thank you for the introduction. Hello, all. It  

00:01:58,000 --> 00:02:07,440
is nice to meet you. My name is Kara, and I, along  with Michal, Kevin, and Julia, will take you along  

00:02:07,440 --> 00:02:20,240
our journey of the COVID Tracking Project. Just a quick note before we get started.  

00:02:21,440 --> 00:02:25,440
We're going to be touching on a lot of different  things during this presentation. We'll be moving  

00:02:25,440 --> 00:02:31,760
relatively quickly through them. We'll touch on as  many different corners of the project as we can.  

00:02:32,880 --> 00:02:36,000
If you'd like to learn more about  anything that we talk about today,  

00:02:36,000 --> 00:02:42,080
we've compiled this resource sheet that you can  access at this URL that has links to articles that  

00:02:43,360 --> 00:02:47,840
we're going to be talking about and documentation  around our website. You can check that out,  

00:02:47,840 --> 00:02:52,400
if you're interested in learning anything  more about something that we talk about today. 

00:02:54,560 --> 00:02:59,520
So, just to provide a little bit of general  background on what the COVID Tracking Project is,  

00:03:00,720 --> 00:03:06,560
we're a volunteer organization that collected  COVID‑19 data from the websites of all U.S.  

00:03:06,560 --> 00:03:13,120
states, territories, and the District of Columbia.  We started doing that on March 7th of 2020 because  

00:03:13,120 --> 00:03:19,360
it was the only way to get data back then. The  federal government first put out a COVID data  

00:03:19,360 --> 00:03:25,440
tracking in May of 2020. When that data came  out, we did an analysis comparing it to our own  

00:03:25,440 --> 00:03:32,240
data and found there was some discrepancies, so we  continued to track data directly coming from state  

00:03:32,240 --> 00:03:39,120
and territory sources. We recently wrapped up data  collection in March 7th of 2021, so exactly a year  

00:03:39,120 --> 00:03:43,840
after we started, because we felt the federal data  had improved enough over the course of the year.  

00:03:48,400 --> 00:03:54,080
We're currently in an archiving phase. We're doing  that until the end of the month, cleaning all the  

00:03:54,080 --> 00:03:57,840
data, just making sure everything is clean and  tidy, and doing some mapping analysis work. 

00:03:59,360 --> 00:04:09,520
We have three datasets. The first is our test  cases, hospitalizations, and deaths across  

00:04:09,520 --> 00:04:19,680
all 50 states and jurisdictions in the United  States. We've posted on Twitter this information.  

00:04:21,440 --> 00:04:28,400
We have the COVID racial data tracker, which  tracks the previous statistics according  

00:04:28,400 --> 00:04:33,223
to race to address racial disparities in the  pandemic. Then our long‑term care facilities  

00:04:33,223 --> 00:04:43,440
dataset, which tracks deaths and nursing homes  and other long‑term care facilities that were  

00:04:43,440 --> 00:04:59,520
disproportionately impacted by COVID. Whenever we  talk about a difficult dashboard or a difficult  

00:05:00,960 --> 00:05:05,680
dataset to work with, just multiply by 20.  You'll have a sense of the raw data that  

00:05:06,400 --> 00:05:11,040
we managed to make into really amazing datasets. >> KEVIN:  

00:05:14,240 --> 00:05:19,360
The tracking project started because  two staff writers at "The Atlantic"  

00:05:21,840 --> 00:05:28,400
wanted to know basic early on in the pandemic,  like how many tests were states running. There  

00:05:28,400 --> 00:05:35,280
really wasn't any dataset at the federal level  or any clean aggregate dataset they could find,  

00:05:35,280 --> 00:05:38,800
so they started thinking, well, this is simple.  We'll call every state and just ask them.  

00:05:39,600 --> 00:05:47,040
Very quickly, they found a friend of Alexis'  had been doing a similar thing, so they joined  

00:05:47,040 --> 00:05:55,600
forces along with another cofounder to start a  little project, thinking maybe in a few weeks ‑‑  

00:05:56,720 --> 00:05:59,920
this was early March 2020. They  were thinking maybe in a few weeks  

00:06:00,640 --> 00:06:04,640
the federal government would be doing this  and they wouldn't have to do this anymore. 

00:06:08,400 --> 00:06:12,800
In the beginning, our website was  basically a big button that took you  

00:06:12,800 --> 00:06:18,480
to a Google Sheet, but quickly, they  found that the sheet was crashing from  

00:06:18,480 --> 00:06:21,760
just the sheer number of people who  are trying to access it and use it.  

00:06:24,080 --> 00:06:29,600
Over time, we became one of the  most critical pieces of information  

00:06:29,600 --> 00:06:37,760
about the pandemic for the media. Our charts were  oftentimes just copied and pasted into graphics  

00:06:37,760 --> 00:06:45,840
on television and print media. Within a very  short period of time, we went from a Google Sheet  

00:06:45,840 --> 00:06:52,480
managed by a handful of people to a large piece  of data infrastructure about COVID in the U.S. 

00:06:54,880 --> 00:06:59,680
Just a few examples of how we were used. We were  used by two presidential administrations. The  

00:06:59,680 --> 00:07:05,360
Biden administration used our data during their  transition planning for COVID. We were cited in  

00:07:05,360 --> 00:07:15,280
dozens of congressional reports. We were cited  at least 140,000 times in news articles. We have  

00:07:15,280 --> 00:07:21,200
a lot of mentions in academic citations.  Elizabeth Warren once said she must be 10%  

00:07:21,200 --> 00:07:29,520
of all of our website traffic. >> MICHAL:  

00:07:29,520 --> 00:07:35,920
Hi. I'm Michal. We wanted to describe to  you what it was like to build this emergency  

00:07:35,920 --> 00:07:44,640
response organization. To encapsulate what it felt  like, we often said we were building the plane  

00:07:44,640 --> 00:07:53,120
as we were flying it. Here is the itinerary that  we took. We'll talk about the data entry process,  

00:07:53,120 --> 00:07:59,600
which was manual, and why we did it manually.  We'll talk about how we stored the data  

00:07:59,600 --> 00:08:06,320
or the database and the choices about what data  points we stored in it. We'll talk about where the  

00:08:06,320 --> 00:08:13,040
data comes from because we're just an aggregator  and how we created an archive of our data sources.  

00:08:14,000 --> 00:08:18,960
We're going to explain how we presented the  data and made it clear and accessible to all.  

00:08:19,760 --> 00:08:26,160
Finally, we will talk about the community  that came together to do all these things. 

00:08:26,160 --> 00:08:33,520
So, how did we get the data? I talked to a  friend a few weeks ago. He asked me, what did you  

00:08:33,520 --> 00:08:38,560
actually do at the COVID Tracking Project? Were  you calling hospitals and asking them how many  

00:08:38,560 --> 00:08:48,800
patients they had? No. What we did do is look at  state dashboards. We would meet every day on the  

00:08:48,800 --> 00:08:55,120
Slack channel, five or ten of us, and we would  each go to state dashboards and get the numbers  

00:08:55,120 --> 00:09:05,520
and enter them into one gigantic spreadsheet.  Every data point that was entered ‑‑ sorry, the  

00:09:05,520 --> 00:09:10,240
slide controller is a little wonky. Every data point was then double‑checked  

00:09:10,240 --> 00:09:16,880
by another human. Interesting questions were  always popping up, and we would hash them out  

00:09:16,880 --> 00:09:23,600
on a dedicated per state thread until we had a  daily dataset that was ready to be published.  

00:09:26,240 --> 00:09:31,360
Huge disclaimer here. There's going to be  some frustration with the state reporting.  

00:09:31,360 --> 00:09:35,920
We all think that the state and local health  departments are the real data heroes of the  

00:09:35,920 --> 00:09:42,880
pandemic. They had to stand up systems to track  and coordinate a response as well as report them  

00:09:42,880 --> 00:09:49,360
out. Overall, they did an incredible job. But  as the consumers of the data they published,  

00:09:49,360 --> 00:09:51,680
we do reserve the right to vent a little bit. 

00:09:56,400 --> 00:10:03,840
This data was not easy to get. The slides are  also not easy to get. Early on, there were  

00:10:05,520 --> 00:10:12,560
much less dashboards. We relied a lot on  press conferences. We had an amazing team  

00:10:12,560 --> 00:10:21,600
of reporters and outreach people, and they would  watch the press conferences and summarize the  

00:10:21,600 --> 00:10:27,840
relevant COVID numbers for our data entry team.  This was also an amazing two‑way system where  

00:10:27,840 --> 00:10:32,800
we could submit questions to the states  if anything needed further clarification.  

00:10:34,640 --> 00:10:44,480
Another example of how difficult it was to get the  data is the frequent use of hovers. The image that  

00:10:44,480 --> 00:10:52,480
you're about to see shows how hard it was to find  a very narrow strip and hover over it in order to  

00:10:52,480 --> 00:10:59,200
get the daily number. This caused us to react by  creating an emoji to express our feelings about  

00:10:59,200 --> 00:11:06,000
hovering. There were multiple other issues, like  rounding and percentages that really made it very  

00:11:06,000 --> 00:11:10,880
challenging to get this data. All these challenges  

00:11:12,320 --> 00:11:20,080
prompted us to become very efficient in our  data entry process. We came up with a great  

00:11:20,080 --> 00:11:30,160
system of source notes. Along came Brandon  who came in and gave it a nice syntax.  

00:11:31,280 --> 00:11:40,640
This was brilliantly maintained by Hannah Hoffman.  This system consisted of links and steps on where  

00:11:40,640 --> 00:11:47,280
to get each data point. When you hovered over  a data point ‑‑ yes, sometimes hovers can be  

00:11:47,280 --> 00:11:53,840
good ‑‑ you would see very detailed instructions  on where to get the data. You could easily click  

00:11:53,840 --> 00:12:03,600
through and follow them. This system gave us the  ability to glide much more smoothly through data  

00:12:03,600 --> 00:12:08,320
entry sheets. >> JULIA:  

00:12:08,320 --> 00:12:14,160
Hi, I'm Julia. I'll start here by trying to  answer what is probably an obvious question  

00:12:14,160 --> 00:12:19,760
at this juncture. We've talked a lot about  all of our manual processes for this data,  

00:12:19,760 --> 00:12:25,440
so why didn't we just scrape it all? It seems  like this project should lend itself to that,  

00:12:26,400 --> 00:12:34,640
but the data landscape was a pretty large lack of  consistency and standards in COVID data reporting,  

00:12:35,280 --> 00:12:41,360
especially early on. Everything changed really  often, so they were changing dashboards. Many  

00:12:41,360 --> 00:12:46,160
of these dashboards seemed like they're actually  designed to obfuscate or scrape the data from.  

00:12:47,120 --> 00:12:52,160
Generally, there were lots of changes to the  data formats over time, changes to the metrics  

00:12:52,160 --> 00:12:57,200
themselves that were reported. What's possibly  the hardest thing to catch automatically is there  

00:12:57,200 --> 00:13:02,240
could be a change to the meeting of a reported  metric part of the way through. Even if we set  

00:13:02,240 --> 00:13:10,720
up a perfectly automated system on day one, it  is hard or impossible to notice with automated  

00:13:10,720 --> 00:13:23,120
checks. Ultimately, we needed our eyes on the data  context and connotations on the data pipeline. 

00:13:24,080 --> 00:13:29,200
This is just one example. It's one of the better  ones. This is Rhode Island's COVID day from July  

00:13:29,200 --> 00:13:38,800
and September of last year. Disclaimer, Rhode  Island is actually great. They provided a great  

00:13:38,800 --> 00:13:42,160
sheet with all the data available, but  in many other cases that look like this,  

00:13:42,720 --> 00:13:52,960
this kind of formatting change might come with  a change in the data export also. In cases where  

00:13:52,960 --> 00:14:01,440
they didn't support the data export, we did a lot  of begging of data departments for CSV files. As  

00:14:01,440 --> 00:14:08,240
the world started to reach a steady state, we were  able to take much more advantage of automation,  

00:14:08,240 --> 00:14:13,280
but finding that sweet spot is something that  could only be done with the experience of starting  

00:14:13,280 --> 00:14:19,120
out with pretty manual processes first. Even  then, once we did set up some data automation,  

00:14:19,120 --> 00:14:23,840
it still needs a lot of manual care. No  matter the bots, you still need the people. 

00:14:26,160 --> 00:14:30,320
Now we have all this data we collected very  carefully with our eyes and our hands over  

00:14:30,320 --> 00:14:36,080
time. Where does it go? This part of the talk will  involve a lot of our feelings about Google Sheets.  

00:14:41,120 --> 00:14:44,960
During data entry shifts, we had a lot  of volunteers that would scour state  

00:14:44,960 --> 00:14:49,840
dashboards. Everyone writes their findings into  a giant Google Sheet, which we call worksheet  

00:14:49,840 --> 00:14:58,160
2. Nobody talks about worksheet 1. Everything  is ultimately stored in interconnected sheets.  

00:15:00,160 --> 00:15:05,280
Google Sheets is wonderful in many, many ways.  It's a fantastic tool for collaborative data  

00:15:05,280 --> 00:15:10,800
entry, especially for data structure as a table.  It's generic and flexible enough to accommodate  

00:15:11,360 --> 00:15:15,280
lots of folks of different backgrounds  and respond to different circumstances,  

00:15:15,280 --> 00:15:20,880
like different types of data changes, which  came up a lot. Lastly, we probably don't need  

00:15:20,880 --> 00:15:25,840
to convince anyone listening here, but the CSV  format is very important to general data users,  

00:15:26,400 --> 00:15:32,320
so it is baked into our data entry processes and  pipelines pretty much every step of the way. All  

00:15:32,320 --> 00:15:37,920
of that meant for a long time Google Sheets was  our world for anything we needed. We had tooling  

00:15:37,920 --> 00:15:42,640
within Sheets to make all these things possible.  It was a mix of Google Apps Script magic.  

00:15:50,960 --> 00:15:57,760
Using Sheets for data entry was awesome. We  were also using it as a data store for our  

00:15:57,760 --> 00:16:02,880
time series data, which was less awesome because  we started running into a bunch of problems. 

00:16:03,680 --> 00:16:08,960
For starters, our data was too big. We were  adding 56 rows and an increasing number of  

00:16:08,960 --> 00:16:15,520
columns at least once a day. Since the project ran  much longer than the few weeks we thought we would  

00:16:15,520 --> 00:16:22,000
originally do this for, Sheets’ performance  starts to degrade with this many people and  

00:16:22,000 --> 00:16:29,120
people editing all at once during a shift. We  ran into a bunch of different issues, like losing  

00:16:29,120 --> 00:16:37,680
edit history. Rolling back mistakes was hard  because it was coded within the space of Sheets.  

00:16:38,400 --> 00:16:41,920
It was an amazing but somewhat brutal  machine. Problems like that were  

00:16:42,560 --> 00:16:47,840
harder to prevent within Sheets because as our  data got more complex, so did validating it. 

00:16:50,080 --> 00:16:55,440
Along with Zach co‑leading the data infrastructure  team, we added a database layer at a took some of  

00:16:55,440 --> 00:17:00,960
the data store pressure off of Google Sheets.  The main challenge here was adding a critical  

00:17:00,960 --> 00:17:05,040
piece of infrastructure in the middle of  a production data pipeline. It essentially  

00:17:05,040 --> 00:17:09,360
amounted to building a second plane while  flying it and connecting to the first plane  

00:17:09,947 --> 00:17:14,240
mid‑flight very carefully with no interruptions.  Those kind of connections are never easy. 

00:17:20,240 --> 00:17:26,960
Just a brief overview of our data stack. We  set up a Postgres database. That data in that  

00:17:26,960 --> 00:17:32,880
database became our published source of truth.  We had an internal API layer, which was the only  

00:17:32,880 --> 00:17:40,960
programmatic route to the database. This was a gap  in our spreadsheets. It transformed and validated  

00:17:40,960 --> 00:17:46,960
incoming data and transformed the data  feed it back to our website and public API. 

00:17:48,960 --> 00:17:53,120
This is getting a little bit in the weeds, but  just a few words about the way we represented  

00:17:53,120 --> 00:17:59,840
our data. In order to preserve history, we set it  up so data was never modified. It was only added.  

00:18:02,960 --> 00:18:13,840
We can interpret it chronologically.  For example, if we had to go back and  

00:18:14,560 --> 00:18:19,360
revise our data in response to a state  data revision, we know the current batch  

00:18:19,360 --> 00:18:25,360
is the one we should treat as accurately. We were  able to trace these kinds of changes over time.  

00:18:26,960 --> 00:18:33,360
We left the semantics heavy work to the internal  API layer around the database, which meant we  

00:18:33,360 --> 00:18:40,000
could keep our internal data model pretty simple.  In general, keeping the data store as simple as  

00:18:40,000 --> 00:18:45,440
possible was our friend in this work. Having a  sort of working model that was a combination of  

00:18:46,160 --> 00:18:50,880
a simple data representation and a layer  that would transform it as necessary for  

00:18:50,880 --> 00:18:56,240
all the use cases that we had worked pretty  well. As kind of a more, more general point,  

00:18:56,240 --> 00:19:08,000
all of our tech ended up being on one hand generic  solutions. From the engineering standpoint,  

00:19:08,000 --> 00:19:15,360
it was a cool balance to strike. This became an emblem of our database  

00:19:15,360 --> 00:19:21,920
with spreadsheets and data in the style of the  COVID Tracking Project logos. This was a brilliant  

00:19:21,920 --> 00:19:24,480
work on an emoji. >> MICHAL:  

00:19:32,720 --> 00:19:37,840
So, we talked about the systems and the  techniques that we used to enter the data.  

00:19:43,040 --> 00:19:47,680
(Frozen). What data to capture and report. 

00:19:50,080 --> 00:19:57,360
The first data CSV in our data repository  was for March 4th. On that day, we captured  

00:19:57,360 --> 00:20:05,680
up to four metrics in 14 jurisdictions. It was  a total of 45 data points. On the last day we  

00:20:05,680 --> 00:20:14,720
captured data, we covered 56 jurisdictions. We  captured up to 33 metrics and a total of 784  

00:20:14,720 --> 00:20:20,480
data points. You can see we definitely expanded  the scope of what we captured and reported. 

00:20:24,480 --> 00:20:29,280
Early on, the reporting landscape was  extremely varied and there were many  

00:20:29,280 --> 00:20:34,560
nuanced wrinkles in the data that seemed like  it might be meaningful to capture and report.  

00:20:35,840 --> 00:20:43,360
For example, we captured the number of quarantined  people, including cruise ships as their locations,  

00:20:44,320 --> 00:20:49,600
tracking what kind of exposure people had  and the type of labs that conducted testing.  

00:20:50,560 --> 00:20:54,960
Those were some of the things that were very  relevant to understanding the landscape at  

00:20:54,960 --> 00:21:00,240
that point. None of these things ended  up being things we captured and reported.  

00:21:00,800 --> 00:21:09,520
Instead, we found what was relevant was to provide  a national summary on a daily basis of some key  

00:21:09,520 --> 00:21:16,960
metrics. How many test results were reported,  how many cases were found, how many people  

00:21:17,520 --> 00:21:23,600
were currently hospitalized, and how many  have died. And all these numbers come with  

00:21:23,600 --> 00:21:29,840
huge caveats that we talked about often, and  we'll do so more later in today's presentation.  

00:21:31,040 --> 00:21:38,160
They were reported very differently by different  jurisdictions. We did our best to cobble together  

00:21:38,160 --> 00:21:45,440
a cohesive picture. Oftentimes, this meant that  we had to take a metric that we reported early on  

00:21:45,440 --> 00:21:52,480
and expand it into multiple metrics. One such example is our positive field,  

00:21:52,480 --> 00:22:00,080
which represents unique people with a case of  COVID‑19. At first glance, positive seems like a  

00:22:00,080 --> 00:22:06,880
simple enough concept and really a binary thing.  In this dictionary definition, it's defined as  

00:22:06,880 --> 00:22:12,400
a medical test that shows the person has the  disease or condition for which they are being  

00:22:12,400 --> 00:22:19,600
tested. However, on dashboards it wasn't always  clear what kind of a positive test was reported.  

00:22:20,400 --> 00:22:26,720
And it turns out different tests could result in  a different kind of positive and a different kind  

00:22:26,720 --> 00:22:34,480
of case. The COVID‑19 case definition that was  adopted by the CDC is quite long. It's currently  

00:22:34,480 --> 00:22:42,240
scrolling in this animated gif of the left. It  defines a confirmed case, a probable case, and  

00:22:42,240 --> 00:22:49,440
a suspect case. Those are based on the types of  tests, symptoms, and exposure that a person had.  

00:22:52,480 --> 00:22:58,400
When this recommendation was released and then it  was updated, it impacted the reporting on state  

00:22:58,400 --> 00:23:05,680
dashboards of positive cases. As these reporting  recommendations rippled through the dashboards,  

00:23:05,680 --> 00:23:13,360
we adjusted our capture to match. We ended up with  three different fields to reflect cases. We added  

00:23:13,360 --> 00:23:19,840
two columns to provide a more nuanced view for  the jurisdictions that separated their reporting.  

00:23:20,960 --> 00:23:30,800
Our original field became the catchall for some  problem or confirmed cases or just whatever that  

00:23:30,800 --> 00:23:38,720
jurisdiction reported in case it wasn't clear.  Unfortunately, in a few cases it still isn't clear  

00:23:38,720 --> 00:23:42,680
what kind of case a jurisdiction is reporting. >>  

00:23:47,200 --> 00:23:51,840
KARA: So, when we were figuring out the process ‑‑  oh, sorry. I accidentally switched the slide. 

00:23:51,840 --> 00:24:01,520
When we were figuring out what kind of cases  that a state reported, that involved a lot of  

00:24:01,520 --> 00:24:05,840
research because oftentimes when a state  puts up a case number on their dashboard,  

00:24:05,840 --> 00:24:10,400
it will just say something like cases.  You have to go deep into the footnotes,  

00:24:10,400 --> 00:24:18,800
deep into the weeds to kind of understand what  they're actually reporting. And so, we spent a  

00:24:18,800 --> 00:24:25,920
lot of time at the COVID Tracking Project staring  at dashboard footnotes hidden at the very bottom  

00:24:25,920 --> 00:24:31,680
of state dashboards trying to understand what was  going on, what these states were reporting because  

00:24:31,680 --> 00:24:36,960
every state was kind of doing things differently.  And a what we started to learn down in these  

00:24:36,960 --> 00:24:43,920
footnotes was not just the information about what  states were reporting but also the way that it was  

00:24:43,920 --> 00:24:50,080
produced. For the most part, people on the COVID  Tracking Project aren't health informatics people.  

00:24:50,080 --> 00:25:00,880
We realized from these footnotes we had a lot  to learn about the way this data was produced.  

00:25:02,480 --> 00:25:07,760
It was going to be a really uphill battle we soon  figured out. As soon as we started to understand  

00:25:12,880 --> 00:25:17,840
the axiom, if you will, of public health  reporting in the United States, which is  

00:25:19,120 --> 00:25:22,640
a state-by-state thing, it's  a complicated landscape with  

00:25:23,520 --> 00:25:32,160
different state data pipelines at different  levels of health. Some of them are really  

00:25:32,160 --> 00:25:37,760
falling out of date. They've been really  underfunded. In one state, one test might  

00:25:37,760 --> 00:25:42,960
be automatically transmitted to the health  department by the most up‑to‑date protocols.  

00:25:43,600 --> 00:25:49,920
In another state, you might have 40 state  reports coming into the health department by fax.  

00:25:50,480 --> 00:25:57,120
People have to read these paper reports in order  to produce the data that we get on the dashboards. 

00:25:58,320 --> 00:26:02,160
What we started to realize when we began  researching the questions of where the  

00:26:02,160 --> 00:26:08,880
data comes from was that it would really shape  what we saw on the other end of the dashboard.  

00:26:10,160 --> 00:26:15,840
We needed to pay attention to the limits of the  data to understand how it is used responsible  

00:26:15,840 --> 00:26:18,640
because of the way the state  shapes overall reporting.  

00:26:19,440 --> 00:26:27,840
Here's another example, Georgia. This is the  number of tests it conducts versus the number of  

00:26:31,760 --> 00:26:37,120
cases in the state. This one is especially  important because these two numbers coming  

00:26:37,120 --> 00:26:44,560
together form a metric that many people use called  test positivity to understand viral prevalence.  

00:26:44,560 --> 00:26:50,000
You divide the number of cases by the number of  tests. If a state is not doing very much testing,  

00:26:51,040 --> 00:26:55,360
you're not sort of getting an artificially low  case number. You're weighing it by the total  

00:26:55,360 --> 00:27:01,440
number of testing. You think that total tests  and confirmed cases would have the same sources  

00:27:01,440 --> 00:27:08,320
because total tests counts the number of PCR tests  and confirmed cases of people with a PCR tests. It  

00:27:11,040 --> 00:27:17,280
turns out when tests are hard to track  in a state these numbers tend to diverge  

00:27:17,280 --> 00:27:23,040
in their sourcing. That's because what's urgent  for state health department. Cases are really  

00:27:23,040 --> 00:27:31,280
urgent for a state health department to know  about. When you have a case, you need to do a lot  

00:27:31,280 --> 00:27:35,200
as a health department. You need to get in touch  with people who may have been in close contact.  

00:27:35,200 --> 00:27:40,960
You need to follow up and see how that person's  disease is progressing, if they're getting better,  

00:27:41,760 --> 00:27:48,640
and so, states are very thorough when they count  the number of cases. They go through the stacks of  

00:27:48,640 --> 00:27:53,920
faxes and count every single case and make sure  it ends up in their systems. But when it comes  

00:27:53,920 --> 00:27:58,800
to negative tests, there's not really anything  a health department needs to do with that data.  

00:27:59,840 --> 00:28:05,120
You test negative and that's good news. The health  department doesn't need to follow up. Most states,  

00:28:05,120 --> 00:28:11,680
many states, will either leave those faxes in the  corner and just go through them all in one day,  

00:28:11,680 --> 00:28:17,040
which results in a data dump, or they'll  restrict themselves to only counting the tests  

00:28:17,040 --> 00:28:21,440
that are easy to count for negative tests. That's  electronic reporting as opposed to fax reporting.  

00:28:25,680 --> 00:28:30,560
The only source for that number is electronic  laboratory reporting. They're leaving out the  

00:28:30,560 --> 00:28:34,800
faxes. When you compare the number of  positive tests, which you expect to be  

00:28:34,800 --> 00:28:41,840
higher than confirmed cases, it's actually lower  because it is excluding all those fax reports. 

00:28:44,960 --> 00:28:49,360
What you end up with is if you divide the  number of cases by the number of tests in  

00:28:49,360 --> 00:28:58,000
Georgia for case positivity, it's not an accurate  calculation. You need to divide the number of  

00:28:58,000 --> 00:29:03,040
positive tests number, which only includes the  electronic laboratory reporting by the number of  

00:29:03,040 --> 00:29:09,920
total tests to have a responsible indication.  We tried to provide that context to make sure  

00:29:10,480 --> 00:29:14,960
people understood that the way that this  data is shaped, the way it is produced  

00:29:14,960 --> 00:29:20,400
affects how you can actually use it. >> JULIA: One thing that helped us  

00:29:20,400 --> 00:29:24,480
a bit over time with understanding the  evolution of this kind of data context  

00:29:24,480 --> 00:29:29,040
and in general the evolution of the  data over time was having a store of  

00:29:29,040 --> 00:29:32,720
state data screenshots. I'll just say a  few words about our screenshot system,  

00:29:33,440 --> 00:29:37,440
which we built for a bunch of reasons. Most importantly was data prominence.  

00:29:37,440 --> 00:29:42,240
It was really important to us to be able to show  our work and where the data came from. This is  

00:29:42,240 --> 00:29:47,600
very closely connected to data quality. We needed  the ability to go back and check our next numbers  

00:29:47,600 --> 00:29:53,120
against what has been published on specific dates  by the state themselves so we can answer questions  

00:29:53,120 --> 00:29:58,240
like, what were the testing counts that New  Hampshire reported on March 7th, and how do they  

00:29:58,240 --> 00:30:04,560
show that data, which gets us to data definitions.  States would change data definitions very often.  

00:30:05,200 --> 00:30:12,080
This was reflected more of the visual version  of a website as opposed to a data CSV.  

00:30:13,520 --> 00:30:17,200
Screenshots also helped us maintain  history. It was particularly useful for  

00:30:18,000 --> 00:30:21,840
situations where we needed to update our  data in response to state data revisions.  

00:30:24,400 --> 00:30:27,920
Having older screenshots helped us keep  track of what the states themselves put out.  

00:30:29,120 --> 00:30:33,440
Lastly, we did this for archival purposes.  Eventually, we want to be able to present  

00:30:34,000 --> 00:30:38,480
a record of what was reported and when and  have that record be as complete as possible. 

00:30:40,800 --> 00:30:48,720
We built a system to essentially screen grab state  webs. It was a service called phantom JS cloud  

00:30:49,360 --> 00:30:56,720
to render websites and capture screenshots and run  custom JavaScript to navigate around as‑needed.  

00:30:57,600 --> 00:31:05,840
That was encoded in the config. This is an  example of a config. It's structured to our  

00:31:05,840 --> 00:31:12,080
use cases. There's a state URL. There's some  web browser settings, like page size. And then  

00:31:12,080 --> 00:31:18,400
we have a message that we use to remind ourselves  of what this config is doing. The interesting part  

00:31:18,400 --> 00:31:24,160
is the overseer script. This example is far  from the most complicated thing we would do,  

00:31:24,800 --> 00:31:30,880
but very often we would need to click on the  fifth tab of this dashboard, which doesn't  

00:31:30,880 --> 00:31:38,400
have a reliable selector name, so it is like  click of the fifth HTML, wait 49.3 seconds,  

00:31:39,920 --> 00:31:49,840
click our heels, spin around a bunch. One of  our amazing volunteers would take a screenshot  

00:31:49,840 --> 00:32:04,400
manually and put it in our screen store. We  built a brain trust around navigating our JS. 

00:32:04,400 --> 00:32:09,920
So many folks on the project put a ton of work  into maintaining these screenshot configs over  

00:32:09,920 --> 00:32:15,440
time. We actually reached 100% coverage on our  main data points toward the end of the project,  

00:32:15,440 --> 00:32:20,040
which was a big accountability milestone for us. >>  

00:32:24,080 --> 00:32:31,520
KEVIN: As I mentioned before, we started as  a series of spreadsheets, but found that we  

00:32:31,520 --> 00:32:37,360
needed to provide more context and visualization  to this data, especially as news organizations,  

00:32:37,360 --> 00:32:42,800
media organizations, and government institutions  were starting to present this data and  

00:32:42,800 --> 00:32:49,120
visualize it. Sometimes in ways that would be  misleading. We really wanted to set an example  

00:32:50,320 --> 00:32:56,800
of how to put that data into context. We had  a number of principles. First and foremost,  

00:32:56,800 --> 00:33:02,080
everyone should have access to the story and  visualization regardless of their availability  

00:33:02,080 --> 00:33:09,120
and the device they're using. Over 50% of our web  traffic was on mobile devices, so we really wanted  

00:33:09,120 --> 00:33:14,480
to ensure regardless of what community you're  a member of in terms of access to device you  

00:33:14,480 --> 00:33:23,120
could get the information you needed. We wanted  to provide direct charts that people could use  

00:33:23,120 --> 00:33:31,440
as something to look at when comparing one day or  trend the next. We created an entire visualization  

00:33:31,440 --> 00:33:38,160
guide that talked about how to avoid misleading  maps and fancy visualizations that might obscure  

00:33:38,160 --> 00:33:43,120
what's really going on on the ground. In our  all visualizations, we wanted to make sure we  

00:33:43,120 --> 00:33:50,400
called out anomalies. Kara mentioned things like  data dumps throwing off a chart. It could look  

00:33:50,400 --> 00:33:55,920
like a huge problem in the state, but it's  a chunk of data from another period of time.  

00:33:58,720 --> 00:34:03,760
As part of making all our content and website  accessible to people regardless of ability,  

00:34:04,640 --> 00:34:13,840
every chart we had had keyboard navigation. If you were a person who had sight but was  

00:34:13,840 --> 00:34:19,760
not able to use a mouse because of motor  impairment, you could toggle around all  

00:34:19,760 --> 00:34:30,800
our charts and maps using just a keyboard. We  made alt text for all our charts as well as the  

00:34:30,800 --> 00:34:42,720
raw data for every chart. Every day, we produced  this series of charts. We replicated that chart in  

00:34:42,720 --> 00:34:50,080
terms of design and color on our website. People  really told us in our feedback forms that they  

00:34:50,720 --> 00:34:55,040
held onto this chart. This chart was the one  sort of constant in their lives, and I found  

00:34:55,040 --> 00:35:00,800
that feedback really interesting. It was so much  information and everything is very confusing,  

00:35:00,800 --> 00:35:08,160
especially in the early days of the pandemic. Just  having something that was consistent and clear was  

00:35:08,160 --> 00:35:13,840
really helpful. You'll see in the lower right‑hand  side of this slide there's two examples of our  

00:35:13,840 --> 00:35:20,080
annotations. In new deaths here, we have these  big spikes. We would always flag those and provide  

00:35:20,080 --> 00:35:26,000
more information when you clicked on them about  why that spike might be not necessarily accurate. 

00:35:27,840 --> 00:35:33,440
We found early on ‑‑ this is a screenshot  of some of our state interfaces from May ‑‑  

00:35:34,080 --> 00:35:41,920
that the presentation of data in a table layout  sometimes created cognitive frameworks that would  

00:35:41,920 --> 00:35:48,000
never correlation between columns and rows  that wasn't necessarily accurate. For example,  

00:35:48,000 --> 00:35:55,840
in these three states this column is actually  different units. What we ended up doing is in one  

00:35:55,840 --> 00:36:03,920
of our many redesigns of our website is put data  into cards so we could add more context. Every  

00:36:03,920 --> 00:36:10,880
element had a definition. This is on our website  today as well. If you click on a definition,  

00:36:10,880 --> 00:36:16,800
it will pop up a very specific definition of  each field. And we also included per dataset,  

00:36:17,600 --> 00:36:25,200
per state flags if there was something you  should really be aware of. You'll see examples  

00:36:25,200 --> 00:36:31,440
of total test specimens, and other states  it might be different units. As a result,  

00:36:31,440 --> 00:36:41,200
our data page is about as long as a CVS receipt,  but the feedback said it was really helpful. 

00:36:44,080 --> 00:36:48,000
I really want to talk about the great  state of Delaware. So, Delaware.  

00:36:51,600 --> 00:37:00,960
The native Lenni Lenape people lived in Delaware.  Then the Dutch set up a colony that I won't  

00:37:00,960 --> 00:37:11,520
attempt to pronounce in 1624. The Swedes  had a colony in 1654. It was given to  

00:37:11,520 --> 00:37:22,480
Maryland. Pennsylvania said we want it. Charles  Mason and Jeremiah Dixon came to settle the issue,  

00:37:23,120 --> 00:37:27,680
and they mapped out the boundaries of Delaware.  This is where Mason‑Dixon Line gets its name.  

00:37:28,400 --> 00:37:36,320
They were great cartographers, but they  were not very good user interface designers.  

00:37:37,840 --> 00:37:42,640
As a result, this is a classic example  of the kind of interface you would see  

00:37:42,640 --> 00:37:51,200
on newspaper websites or even federal agencies,  which was a map of the U.S. that showed  

00:37:51,200 --> 00:37:57,280
COVID data, but the problem was these maps were  sometimes the only way to navigate to data. It was  

00:37:57,280 --> 00:38:04,640
not only a map. It is also a means of navigation.  Delaware was hard to get information for if you  

00:38:06,080 --> 00:38:10,640
had motor skill issues and couldn't use a  mouse to the fidelity needed to get down to  

00:38:10,640 --> 00:38:17,600
little Delaware. If you were on a mobile device  where your thumb ‑‑ this is an actual size of  

00:38:17,600 --> 00:38:25,840
the average American thumb ‑‑ covers the entire  eastern seaboard. As a result, when we did want  

00:38:25,840 --> 00:38:34,160
to do state‑level maps, we spent a lot of time on  developing text maps that presented every state as  

00:38:34,720 --> 00:38:41,440
a consistent sized target. That also allowed us  to make it more of a grid layout. If you were  

00:38:41,440 --> 00:38:45,840
navigating it with a keyboard, you could use  the arrows to move up and down really easily. 

00:38:47,760 --> 00:38:57,280
Another part of not only presenting but delivering  our data to as many as possible is we had an API  

00:38:57,280 --> 00:39:05,280
from day one in CSV and JSON format. It  was orders of magnitude larger than our  

00:39:05,280 --> 00:39:15,840
website traffic. We were delivering upwards of 9  terabytes of API data a month. Small newspapers  

00:39:15,840 --> 00:39:22,240
could present to their newspapers an automatically  daily updated chart of COVID data for their state.  

00:39:24,640 --> 00:39:28,560
A lot of that was powered by  the internal API and database  

00:39:28,560 --> 00:39:35,840
that Julia mentioned. We then just host it as  a static website, which helps our volunteers  

00:39:35,840 --> 00:39:43,200
not have to focus on things like operations. >> KARA: To conclude, we just wanted to share  

00:39:43,200 --> 00:39:51,120
a little bit about the community that made all of  this, collecting the data, building the websites,  

00:39:51,120 --> 00:39:57,120
that made all that possible. It's a pretty  awesome community. Producing this data was  

00:39:57,120 --> 00:40:02,800
a very large group of dedicated, wonderful humans  who contrary to this photo were not all named Bob.  

00:40:06,400 --> 00:40:09,680
When I say producing this data, I do  mean down in the weeds of this data.  

00:40:12,240 --> 00:40:16,800
Every data point had two eyes on it. This is  what big corners of our Slack looked like.  

00:40:16,800 --> 00:40:22,880
It is just full of threads for each state and  discussing particular numbers in each state.  

00:40:24,080 --> 00:40:28,480
What this amounted to was there was a big  community of people around the United States and,  

00:40:28,480 --> 00:40:34,960
in fact, the entire globe that was recording this  experience as we also looked through it. That  

00:40:34,960 --> 00:40:39,920
can be very intense. For many people, I think  the project was a form of collective mourning,  

00:40:40,800 --> 00:40:46,320
a way to remember each person affected by the  pandemic by making sure they were counted in  

00:40:46,320 --> 00:40:52,240
the numbers. Despite the nature of the task and  the nature of the data that we were dealing with,  

00:40:52,240 --> 00:40:57,440
we didn't want it to be a grim place for  people to log onto each day. In the end,  

00:40:57,440 --> 00:41:02,400
I don't think it was. We wanted people to  find meaning and connection through this work  

00:41:03,040 --> 00:41:06,800
to support each other through the  pandemic and the task of conquering it. 

00:41:08,720 --> 00:41:13,840
We took very seriously creating the virtual place,  the Slack, where we gathered. This is a piece of  

00:41:13,840 --> 00:41:23,520
art that was made. It's called Emojitropolis. I  think it speaks very well to that. Even though  

00:41:23,520 --> 00:41:28,160
it is now quite obvious, it's actually a graph.  It's a graph of custom emojis that were uploaded  

00:41:28,160 --> 00:41:37,200
to our Slack, our virtual home each week that  looks like a city skyline. I want to tell a  

00:41:37,200 --> 00:41:43,600
brief history of how we came to have so many emoji  in our Slack. It's a little bit of a silly yarn,  

00:41:43,600 --> 00:41:49,840
but it's a portal to understand our community and  also the work that came out of that community. 

00:41:51,360 --> 00:41:56,160
Just for context, this is our logo.  It's the circle with the little arc  

00:41:56,160 --> 00:42:01,840
around it. Two fundamental and consequential  questions is we were positive about this logo.  

00:42:07,040 --> 00:42:13,360
The first was posed by our community lead, Amanda  French, who had the good sense to monitor why  

00:42:13,360 --> 00:42:19,360
is there not yet a logo emoji. It's been two  months at this project. That was quickly remedied.  

00:42:20,240 --> 00:42:26,560
Then the lead of the data tracker, Alice, asked  the second pivotal question, which is what if  

00:42:26,560 --> 00:42:35,200
it's fun? What this kind of did is it spawned a  realization that our logo is infinitely livable.  

00:42:40,560 --> 00:42:50,640
A few days after the consequential CTPs were  uploaded, someone asked permission to use our  

00:42:50,640 --> 00:42:58,560
logo as their car dealership's insignia.  We said no, but we tried it out to see  

00:42:58,560 --> 00:43:04,480
what it would look like. This kind of opened the  floodgates. Before you knew it, you would blank  

00:43:04,480 --> 00:43:10,160
and open your eyes and there would be 20 new  CTP emoji in the Slack. You would blink again  

00:43:10,880 --> 00:43:24,160
and there would be 20,000 CTP emoji in the Slack.  You would take a walk around the block. There was  

00:43:24,880 --> 00:43:33,120
a March madness bracket to choose the favorite CTP  emoji. This is the one that won, which I did brag.  

00:43:38,240 --> 00:43:44,080
It's a community of extremely creative people  full of energy that really enjoy building things  

00:43:44,080 --> 00:43:50,800
together, whether that is a critical data  infrastructure or an emoji infrastructure.  

00:43:52,080 --> 00:43:55,920
What you see with the emoji is the way  things felt on this project in general. 

00:44:04,480 --> 00:44:12,240
I have one favorite emoji or ten favorite emoji  with the most variation in our Slack other than  

00:44:12,240 --> 00:44:16,960
the logo itself that I think cuts to  the heart of CTP as an organization,  

00:44:16,960 --> 00:44:24,080
and that's the thank you emoji. There are ten ways  to say thank you on the CTP Slack, including a few  

00:44:24,080 --> 00:44:34,480
that are specifically done by Kevin. CTP has a  real culture of gratitude. There's a channel,  

00:44:34,480 --> 00:44:39,520
gratitude, that's just dedicated to thanking  people. It's a place, I think, where people  

00:44:39,520 --> 00:44:44,720
really wanted to show they cared about each other,  that they cared people were putting in the work to  

00:44:46,720 --> 00:44:52,560
fill this fundamental hole in the U.S.' pandemic  infrastructure. That care was at the heart of our  

00:44:52,560 --> 00:44:58,880
work, I think, the care for each other, the  care for the data as well. We wanted to do  

00:44:58,880 --> 00:45:08,480
everything that we could to get the data right. >> KEVIN: So, we had over 800 volunteers  

00:45:08,480 --> 00:45:15,600
and data entry, data quality outreach  reporting infrastructure communications,  

00:45:16,480 --> 00:45:24,160
visualization, and the website team.  Just in data entry, we calculated over  

00:45:24,160 --> 00:45:27,840
20,000 hours of time were dedicated  to the COVID Tracking Project.  

00:45:31,280 --> 00:45:38,080
Many people volunteered full‑time. Many people  volunteered more than full‑time. They were all  

00:45:38,080 --> 00:45:43,600
doing volunteer work that was really filling gaps  that other institutions should have been doing,  

00:45:45,120 --> 00:45:55,520
and I just want to commend them for that. This has  been a very hard, difficult, tragic year, and the  

00:45:55,520 --> 00:46:00,880
people that you see here, along with hundreds  of others who are not pictured on this slide,  

00:46:00,880 --> 00:46:09,840
have sustained me and each other. I'd like to  quote from Aaron, who is one of our cofounders.  

00:46:11,280 --> 00:46:15,280
For many of us, assembling a daily  count of the sick and the very sick  

00:46:15,280 --> 00:46:19,120
and the dead was an act of service that  kept us going through a difficult year,  

00:46:20,160 --> 00:46:24,720
but it also came at a high cost, especially  when we found ourselves working with numbers  

00:46:24,720 --> 00:46:30,640
that include digits representing our own  friends and family members. For many of us,  

00:46:30,640 --> 00:46:35,280
doing the work together in community  with each other made all the difference. 

00:46:39,360 --> 00:46:46,400
I'm going to hit the next slide before I cry.  Here's the resources again. We have a lot of  

00:46:46,400 --> 00:46:54,080
links to blog posts, especially lately on our  website. You can see a lot of analysis and  

00:46:54,080 --> 00:46:58,640
updates that we've done in a retrospective  format that I think are very powerful  

00:46:58,640 --> 00:47:05,600
and interesting that I would encourage you to  check out. Here is all of our contact information.  

00:47:06,560 --> 00:47:10,840
I think we'll now take it to questions. >>  

00:47:17,440 --> 00:47:27,360
Thank you so much, Michal, Julia, Kara, and  Kevin for being such a part of an incredible  

00:47:27,360 --> 00:47:34,400
effort. We're so glad we got to hear a chance  about it. You have a few questions from the  

00:47:34,400 --> 00:47:43,760
audience. I'll start with a question. Did  you encounter issues with data licenses,  

00:47:45,280 --> 00:47:55,360
ones that were not open, for example? Go ahead. >> MICHAL: I wanted to say something to this.  

00:47:56,640 --> 00:48:02,400
Not so much licenses, per se, but we had a lot of  

00:48:03,600 --> 00:48:10,480
instances where we would write to states and ask  them for specific data or time series data to  

00:48:11,360 --> 00:48:18,240
rule out data dumps. A lot of times what  would happen is they would send it to us  

00:48:18,880 --> 00:48:29,280
in a CSV or Excel. We didn't want to use data  on the testing and outcome side of the project  

00:48:29,840 --> 00:48:37,200
that wasn't publicly available, so we did  a lot of back and forth of just asking them  

00:48:38,160 --> 00:48:43,600
to publish the data themselves on their  website so it would be available to the public.  

00:48:44,400 --> 00:48:48,480
Sometimes it worked and sometimes it didn't  work. Sometimes we had to go back and  

00:48:48,480 --> 00:48:55,680
scrape their power BI for the same data just  so we know we're using publicly available data.  

00:48:59,920 --> 00:49:09,360
Puerto Rico, I think, ended up being the only  state that had an API, and we did want to use that  

00:49:09,360 --> 00:49:16,320
API. There was a data issue with that. We were  chatting with them, and they tried to work on it.  

00:49:16,320 --> 00:49:19,320
As far as I know, it never got resolved. >>  

00:49:23,680 --> 00:49:31,840
Great. We have another question, which is  a little bit depressing. Sadly, this isn't  

00:49:31,840 --> 00:49:37,120
going to be our last pandemic. We're likely  to run into something like this in the future.  

00:49:38,720 --> 00:49:45,280
Does the COVID Tracking Project team have any  plans to write some sort of playbook for future  

00:49:45,280 --> 00:49:53,120
volunteers that might take on a similar effort or  for organizations, like the ones that you've been  

00:49:53,120 --> 00:50:00,000
dealing with, to be able to better provide more  usable data? Because something I've seen often  

00:50:00,000 --> 00:50:06,240
is people want to do the right thing, but they  don't know what the best practices are. Do you  

00:50:06,240 --> 00:50:16,400
have any plans to do something along these lines? >> KEVIN: I would say we've done some of that  

00:50:16,400 --> 00:50:20,560
work already in terms of the posts that we've  made to our website that have been much more  

00:50:22,320 --> 00:50:27,120
descriptive in terms of how we work rather  than just what kind of analysis we're doing.  

00:50:28,080 --> 00:50:37,280
We are working on, as we mentioned before, an  official archive to ensure that not only our  

00:50:37,280 --> 00:50:44,480
data but our organizational structure methods are  preserved. I'm not sure if anyone else wants to  

00:50:44,480 --> 00:50:46,040
speak to that. >>  

00:50:51,680 --> 00:50:57,280
JULIA: Just a quick follow‑up in general on this  part of the project. Even though our public data  

00:50:57,280 --> 00:51:03,200
collections stopped on March 7th, we're in the  phase of trying to think about that question  

00:51:03,200 --> 00:51:08,800
and answer it as best we can. We're very much  still putting out blog posts on covidtracking.com  

00:51:09,360 --> 00:51:14,960
that try to get at the different parts of the  system. And there are components to pretty much  

00:51:14,960 --> 00:51:23,760
all of those posts. There are how‑tos or takeaways  that would hopefully come in useful if something  

00:51:23,760 --> 00:51:30,400
like this ever needs to be done again. >> Great. I think we have time for one  

00:51:30,400 --> 00:51:38,240
more question. How often did you run into  proprietary data formats that were not  

00:51:39,280 --> 00:51:54,240
as simple as CSVs that you could easily process? >> MICHAL: If no one else wants to talk about  

00:51:54,240 --> 00:52:04,080
this, I can. We actually have a recent blog post  that talks about all our automated data scraping.  

00:52:06,240 --> 00:52:18,320
There were PDFs, HTMs, CSV, Excel, and also  just APIs that are powering different dashboards  

00:52:18,320 --> 00:52:29,360
and also hacking dashboards. It's all  there. The link is in our resources.  

00:52:29,920 --> 00:52:37,200
How often? I would say all the time, and  sometimes there just was no data available at all. 

00:52:37,200 --> 00:52:44,000

YouTube URL: https://www.youtube.com/watch?v=TbYAurnUofU


