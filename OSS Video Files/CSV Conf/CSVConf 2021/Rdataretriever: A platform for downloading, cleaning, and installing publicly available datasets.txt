Title: Rdataretriever: A platform for downloading, cleaning, and installing publicly available datasets
Publication date: 2021-05-19
Playlist: CSVConf 2021
Description: 
	Presenter: Henry Senyondo 


The rdataretriever provides an R interface to the Python-based Data Retriever software. The Data Retriever automates the core steps of data preprocessing including downloading, cleaning, standardizing, and importing datasets into a variety of relational databases and flat file formats. The rdataretriever additionally supports provenance tracking for these steps of the analysis workflow by taking snapshots of the datasets to be committed at the time of installation and allowing them to be reinstalled with the same data and processing steps in the future. Finally, the rdataretriever supports the installation of spatial datasets into relational databases with spatial support. The rdataretriever provides an R interface to this functionality and also supports importing of datasets directly into R for immediate analysis. These tools are focused on scientific data applications including several widely used but difficult to work with, datasets in ecology and the environmental sciences. The rdataretriever allows R users to access the Python Data Retriever processing platform through a combination of the reticulate package and custom features developed for working in R. Because many R users, including the domain researchers most strongly supported by this package, are not familiar with Python and its package management systems, a strong emphasis has been placed on simplifying the installation process for this package so that it can be done entirely from R. Installation requires no direct use of Python or the command line. Detailed documentation has been developed to support users in both installation and use of the software. A Docker-based testing system and associated test suite have also been implemented to ensure that the interoperability of the R package and Python package are maintained, which is challenging due to frequent changes in reticulate and complexities in supporting cross-language functionality across multiple operating systems and R programming environments (terminal-based R and RStudio).
Captions: 
	00:00:03,920 --> 00:00:08,880
This is the rdataretriever session. It's  a platform for downloading, cleaning,  

00:00:08,880 --> 00:00:20,000
and installing publicly available datasets.  I'm Henry Senyondo. I work at the Weecology lab  

00:00:22,480 --> 00:00:29,280
at the University of Florida. I do  a lot of data processing. We know  

00:00:29,280 --> 00:00:33,840
data processing workflow usually  starts with acquiring data,  

00:00:33,840 --> 00:00:41,120
cleaning and reformatting the data. We combine the  data and visualize it and analyze it. All these  

00:00:41,120 --> 00:00:48,080
parts take different amounts of time, so there's  quite a bit of time spent in acquiring data.  

00:00:49,200 --> 00:00:56,320
There's even more time spent in the cleaning  of the data. We could utilize that time  

00:00:56,320 --> 00:01:06,320
in other aspects, like analyzing data. The rdataretriever is the tool that will help us  

00:01:07,840 --> 00:01:17,840
spend less time in acquiring and cleaning but  help us analyze the dataset. This is what we  

00:01:17,840 --> 00:01:25,120
actually produce after using the rdataretriever.  We notice less time is used in acquiring the data,  

00:01:25,120 --> 00:01:33,120
cleaning the data, and more time is available  to the scientists to visualize and analyze their  

00:01:33,120 --> 00:01:38,960
datasets and come up with the cool solutions  to all the worldwide problems that we have. 

00:01:39,680 --> 00:01:46,880
Data processing in particular is time consuming.  We know that. Why is that so? There are a large  

00:01:46,880 --> 00:01:57,040
number of formats. We have data packaged in XML  files, CSV files, tabular datasets. We have data  

00:01:57,040 --> 00:02:11,760
that is compressed that are dot zip. Even if  we have agreed‑upon standards, there is lack  

00:02:11,760 --> 00:02:19,600
of that standard of following up on those points  to provide these data packages. When it comes to  

00:02:19,600 --> 00:02:27,360
trying to actually clean it, we, as scientists, we  know everyone can clean data. We always create our  

00:02:27,360 --> 00:02:35,600
own data cleaning code. What happens is every  data package eventually has its data cleaning  

00:02:35,600 --> 00:02:44,240
code. Now we're increasing the amount of time that  we actually use on cleaning or acquiring the data  

00:02:44,240 --> 00:02:51,200
by actually searching for these kind of packages  and debugging because software always breaks. 

00:02:56,000 --> 00:03:03,440
Data processing is also unstable. Everyone writes  their own code, but data updates could change. The  

00:03:03,440 --> 00:03:10,240
change is due to format. The change is maybe  the URL has been moved, for example. Recently,  

00:03:10,240 --> 00:03:17,360
people have been moving their data packages on  GitHub from master to main. The data processing  

00:03:17,360 --> 00:03:23,440
code for regularly updating data frequently  breaks. When that comes to tooling, that is  

00:03:23,440 --> 00:03:29,520
really frustrating because you get your old  pipeline broken down because of these problems.  

00:03:31,040 --> 00:03:39,760
How can the R Data Retriever actually help? It can download your datasets. It can clean  

00:03:39,760 --> 00:03:47,600
them and install all the open datasets  with one line of codon your data storage  

00:03:49,040 --> 00:03:59,520
engines. Datasets in rdataretriever are provided  in form of JSON files. These are specifications.  

00:03:59,520 --> 00:04:05,920
They try to understand attributed properties  of the data. Maybe the data has a version.  

00:04:06,880 --> 00:04:13,920
Maybe it has a license. It has a URL where we can  find the data, and it has the meta specification  

00:04:13,920 --> 00:04:24,640
of which files are maybe a zip folder. Then we  also provide custom scripts for arbitrary complex  

00:04:24,640 --> 00:04:32,320
cleaning or restructuring tasks where the data  is so complicated that we actually have to try  

00:04:32,320 --> 00:04:40,240
to do some tailoring of the data so we can get an  instance that will be ingested into the platform.  

00:04:42,320 --> 00:04:48,160
This is basically similar to a packet manager  for software, but this time it is for your  

00:04:48,160 --> 00:04:57,280
data. How is that? A single person figures out  how to clean the data. He creates a recipe.  

00:04:57,280 --> 00:05:03,920
This recipe is shared to everyone else. What  happens is if something changes in the recipe,  

00:05:03,920 --> 00:05:11,920
it's automatically delivered to all the  users just from one single recipe change. 

00:05:13,280 --> 00:05:21,040
Let's get an overview of what we're actually  dealing with. The rdataretriever collects all  

00:05:21,040 --> 00:05:27,920
these datasets from the public sources. We have  many public sources. Some are domain specific.  

00:05:28,800 --> 00:05:43,600
Some are just data science specific, for example,  Kaggle. There are many, many repositories that  

00:05:46,240 --> 00:05:53,440
are publicly available. The rdataretriever comes  in and cleans these datasets, restructures them  

00:05:53,440 --> 00:05:58,720
in the best way possible for you to analyze,  and puts them in R obviously. If you want to  

00:05:58,720 --> 00:06:04,320
use data frames, you can go ahead. You can  go ahead and store them in some of the data  

00:06:04,320 --> 00:06:16,880
storage engines. We support several data engines,  MySQL, SQL, Postgres, Excel, JSON. There are many  

00:06:17,600 --> 00:06:26,400
supported platforms with this tool. Just to go ahead and show you  

00:06:26,400 --> 00:06:33,840
how the processing is done ‑‑ this may be a little  bit hard to show, but we have several datasets.  

00:06:34,640 --> 00:06:43,120
How do we do that? A single run is loading the  library reticulate. It's one of the underlying  

00:06:44,400 --> 00:07:00,320
libraries we use in the rdataretriever. We load  reticulate and rdataretriever. You can see what  

00:07:00,320 --> 00:07:08,320
kind of functions you want to use. Install CSV or  do a check for updates. Right now, we're running a  

00:07:08,320 --> 00:07:16,240
dataset. We want to see how many datasets we have.  We have our tool approaching 300 unique datasets.  

00:07:19,680 --> 00:07:26,080
Many people would be worried. You have several  datasets, but how do I find the dataset I want to  

00:07:26,080 --> 00:07:32,960
actually use? Datasets are usually categorized  by keywords or something. Retriever the same.  

00:07:32,960 --> 00:07:41,440
You can try a rdataretriever dataset and give  it a keyword. I want datasets with words or with  

00:07:41,440 --> 00:07:50,800
plants. You can go to the website and look at the  citation and provide the description. You can find  

00:07:50,800 --> 00:07:59,360
the dataset that you actually want to use. A simple run on how to install the data  

00:07:59,360 --> 00:08:06,560
would be retriever CSV where we're  installing iris. I think it has 151  

00:08:09,600 --> 00:08:17,360
rows or data rows. In another case, we would want  to actually create a data frame from a dataset  

00:08:17,360 --> 00:08:28,160
called portal. We say retriever fetch portal. It  tells you where we installed the data. We have  

00:08:28,160 --> 00:08:35,680
three files, which is main, plots, and species.  You can go ahead and look at what you have as  

00:08:35,680 --> 00:08:43,760
the data frame. Right now, it would show you a  little bit of what is in portal. It's a list.  

00:08:47,840 --> 00:08:55,600
It will give you basically what you have, right? If we go back to where this rdataretriever  

00:08:55,600 --> 00:09:04,640
comes from, basically we have three main  data processing ‑‑ we provide three major  

00:09:04,640 --> 00:09:12,560
data centers. It's basically written in Python,  which is the core tool. Then we have the R  

00:09:12,560 --> 00:09:30,560
package using the rdataretriever. Then we have  PyCall that uses the reticulate. It uses PyCall. 

00:09:45,120 --> 00:09:49,840
When you're processing data and dealing  with all these data sciency things,  

00:09:50,880 --> 00:10:00,160
we always have a problem of trying to reproduce  an analysis we did in the past, right? Well,  

00:10:00,160 --> 00:10:08,960
the rdataretriever has that functionality. It  tries to give you a snapshot of an analysis to  

00:10:08,960 --> 00:10:17,840
date with the code and recipes. It packages them  all together. It's kind of how versioning works.  

00:10:20,960 --> 00:10:31,760
This time it is for your reproducible pipeline.  You can get data, commit it, get the scripts,  

00:10:31,760 --> 00:10:37,120
commit it. It keeps all the versions together so  that in the future, if you're trying to analyze  

00:10:37,120 --> 00:10:41,920
data and the data has changed so much ‑‑ you're  trying to understand why the results have changed.  

00:10:41,920 --> 00:10:47,040
The data has changed. You can go back to a  day that you actually committed this dataset  

00:10:47,040 --> 00:10:54,800
and try to reproduce the same. This is a very  good part of people who write papers. You provide  

00:10:54,800 --> 00:10:59,840
a paper and somebody does the analysis and is  like, oh, no, I think we have different analyses  

00:11:00,880 --> 00:11:08,880
based on the data. You can go back to that same,  same place and rerun everything. That is cool. 

00:11:09,440 --> 00:11:16,320
This is a sample of how it runs. For example,  retriever is committing a photo. I'm committing  

00:11:16,320 --> 00:11:26,320
it at the CSV conference today. I will try to find  out what I committed that day. A few days back,  

00:11:26,320 --> 00:11:33,840
it gives me ‑‑ after a few runs, it gives me a  commit message and a hash number. This hash number  

00:11:33,840 --> 00:11:39,360
and the message that has been committed and the  date can be used to actually install it in the  

00:11:39,360 --> 00:11:48,240
future. A few days pass, and I want to install  the same analysis using the same code because code  

00:11:48,240 --> 00:11:57,360
always changes. Maybe the package provider added  a new column, and I want to see what happened. You  

00:11:57,360 --> 00:12:03,920
can come into the rdataretriever and say, okay,  install that portal and use that hash number. It  

00:12:03,920 --> 00:12:09,840
will install the same data, and you can go through  the files and see what is actually happening.  

00:12:18,160 --> 00:12:21,120
That is when it is finished  installing the new dataset. 

00:12:22,720 --> 00:12:31,280
Coming to the end, the rdataretriever brings about  robustness in data processing, right? How do we  

00:12:31,280 --> 00:12:41,040
handle that? We know that updates always happen,  and they always break the underlying recipes  

00:12:41,040 --> 00:12:48,160
that we have, but we do a weekly monitoring of  those datasets to check for breaking changes.  

00:12:50,080 --> 00:12:56,880
Because it's a community platform, people can come  in and say, oh, I'm working on this dataset. It's  

00:12:56,880 --> 00:13:02,480
iris, and it's breaking. We can figure out why  it's breaking. Maybe there's a change in version,  

00:13:02,480 --> 00:13:07,440
so we keep on updating the version. We keep on  updating the data so that somebody who installed  

00:13:09,760 --> 00:13:16,960
one point can understand the two point has a  difference in the data because of a change in  

00:13:16,960 --> 00:13:24,320
column. It allows people to quickly fix  these things. When a recipe is fixed,  

00:13:24,320 --> 00:13:30,160
we automatically send the updates to people. You  can go to the rdataretriever tool and say, get  

00:13:30,160 --> 00:13:39,760
updates. It will tell you this has been updated.  It continues to run even with the changes in the  

00:13:39,760 --> 00:13:48,080
data format or location. That's something cool  with this. If everyone doesn't have to devolve  

00:13:48,080 --> 00:13:54,640
their own packages and we have one standard  platform that can clean the datasets for the  

00:13:54,640 --> 00:14:02,720
people maintaining it, this is one of those great  tools in reproducing science and getting the  

00:14:04,000 --> 00:14:11,840
work backflow at an optimal standard. With that being said, if you have any questions,  

00:14:11,840 --> 00:14:17,600
if you want to check out the tool, you can come  up to the home page. I also want to thank all the  

00:14:17,600 --> 00:14:28,560
funding organizations that have helped us get to  this point. I want to thank all the contributors  

00:14:28,560 --> 00:14:35,680
that have actually put in time to actually  get this to evolve from the smallest part  

00:14:35,680 --> 00:14:41,040
that we had ‑‑ right now, we're going to  several datasets and covering most of the  

00:14:41,840 --> 00:14:47,520
open‑source publicly available datasets.  I think that's a great way to go ahead.  

00:14:49,520 --> 00:15:01,680
Thank you very much. If you have any questions... >> Thank you so much. Just want to  

00:15:02,800 --> 00:15:11,840
check really quickly if there's questions  in the audience, for anyone in the audience.  

00:15:14,240 --> 00:15:26,880
If not, can I ask ‑‑ I wasn't sure if I missed  this earlier. Datasets that are maintained  

00:15:27,920 --> 00:15:34,320
for connections and breaks, is there a stable  list or can anyone ‑‑ does it apply to any  

00:15:34,320 --> 00:15:37,920
dataset that somebody is using? >> HENRY: Can you ask again? 

00:15:37,920 --> 00:15:49,600
>> Yeah, I'm sorry. I think I may have  missed this part about the datasets that are  

00:15:51,040 --> 00:15:54,507
tracked by rdataretriever. Is there  like a stable list that people ‑‑ 

00:15:54,507 --> 00:16:00,000
>> HENRY: Oh, yeah. Because data  always changes, right, we're  

00:16:01,360 --> 00:16:06,480
going to provide that platform so people  can actually look up ‑‑ we have a dashboard,  

00:16:06,480 --> 00:16:13,360
which is called the retriever dashboard. What  this is doing is it runs these datasets every  

00:16:13,360 --> 00:16:19,200
day. Retriever installs iris. If we check the  hash numbers with the datasets ‑‑ is the hash  

00:16:19,200 --> 00:16:24,800
number the same or does it change? The change  in the hash number will tell you the recipes.  

00:16:25,600 --> 00:16:33,680
It will be flagged as a dataset that has changed  or the error is unknown. We come in and look.  

00:16:33,680 --> 00:16:43,840
Maybe somebody has changed the URL. Maybe a new  dataset has been introduced to the data. We have  

00:16:43,840 --> 00:16:51,120
a dataset, which is installed, and it's called  "New York Times" COVID‑19. This dataset keeps on  

00:16:51,120 --> 00:16:56,080
changing every day. It has the same URL. That's  the good thing, but data keeps being added on.  

00:16:58,160 --> 00:17:03,120
We know that is a change because of the data, but  there are some datasets ‑‑ for example, iris is  

00:17:03,120 --> 00:17:11,360
a dataset that has been there forever. They're  added new data because maybe their location  

00:17:11,360 --> 00:17:18,880
has changed, right? Those are things we can track  and be able to solve with that instance. At least  

00:17:18,880 --> 00:17:26,320
before people get into problems with their tools. >> All right. Thank you so much. Appreciate it. 

00:17:26,320 --> 00:17:29,280
>> HENRY: Yep. >> I think that's the  

00:17:29,280 --> 00:17:33,520
time that we have for questions for this  presentation, but I definitely wanted to  

00:17:35,440 --> 00:17:40,000
invite people to continue the conversation  in the Slack channels. Would you be  

00:17:40,880 --> 00:17:45,840
willing to answer those questions in Slack? >> HENRY: Yep. I'm going to jump onto Slack.  

00:17:46,560 --> 00:17:52,470
I'll be able to answer most of the questions. >> Thank you very much for the group presentation. 

00:17:52,470 --> 00:17:57,200

YouTube URL: https://www.youtube.com/watch?v=lJr18H-vW7g


