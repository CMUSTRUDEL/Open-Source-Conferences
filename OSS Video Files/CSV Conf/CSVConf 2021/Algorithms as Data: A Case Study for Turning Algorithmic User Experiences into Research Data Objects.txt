Title: Algorithms as Data: A Case Study for Turning Algorithmic User Experiences into Research Data Objects
Publication date: 2021-05-20
Playlist: CSVConf 2021
Description: 
	Presenter: Jason A. Clark

Why does your technology seem to know what you want before you do? Increasingly, our digital experiences are mediated by obscure algorithms. But what are algorithms and how can we audit or quantify them? This session introduces an "algorithmic awareness" research module and a generic code application for auditing algorithmic user experiences and quantifying those experiences as datasets for analysis. What if algorithms weren’t the Ghost in the Machine? Can algorithms be understood as part of the open data continuum? Montana State University Library, with grant funding from the Institute of Museum and Library Services, has conducted research in order to develop software and a curriculum to support the teaching of "Algorithmic Awareness": an understanding around the rules that govern our software and shape our digital experiences. Taking our inspiration from investigative data journalists, like The Markup, we are looking to introduce our research module for algorithm auditing practices using code, web scraping methods, and structured data formats to uncover proprietary algorithms and turn them into research data objects for analysis. (Code is available in our #AlgorithmicAwareness GitHub repository.) Our case study for the module will be the YouTube Video Recommendation Algorithm which has come under criticism for its tactics in drawing parents’ and childrens’ attention to their videos. Our goal will be to show the generic patterns, data points, and scripts one can use to analyze algorithmic user experiences and demonstrate how code can be used to turn algorithms into datasets for analysis. In the end, attendees will be able to realize actionable steps for seeing algorithms as data objects, gain a sense of the first steps one can take to programmatically audit these systems with code, and take away investigative data techniques for their own work.
Captions: 
	00:00:02,659 --> 00:00:07,600
>> JASON: Hi, everyone. I'm Jason Clark.  I'm a professor at Montana State University  

00:00:07,600 --> 00:00:13,520
in the library. I'm going to talk a bit more  about a particular set of research where we're  

00:00:13,520 --> 00:00:18,800
trying to think about how do you turn  algorithmic experiences into forms of data. 

00:00:21,360 --> 00:00:26,000
So, as far as what's ahead, I'm going to provide  a little context for where this research started.  

00:00:29,040 --> 00:00:36,640
The bits of that research that are kind of  unfinished to me. I'm going to show parts  

00:00:36,640 --> 00:00:44,080
of where the next cycle of this research  is going. As we started this research,  

00:00:44,080 --> 00:00:49,760
we were thinking through how do you teach  algorithms and teach to all audiences,  

00:00:50,640 --> 00:00:57,840
from citizens at a land grant university,  Montana citizens, students, researchers.  

00:00:58,880 --> 00:01:06,320
How do you build an audience and teach these  complex ideas to all levels of learning?  

00:01:08,800 --> 00:01:11,840
In this particular session, I'm  going to talk a bit more about  

00:01:12,400 --> 00:01:17,680
algorithmic user experience. Then  I'll pull through a potential auditing  

00:01:17,680 --> 00:01:24,560
technique and talk about research implications. The initial research was founded by the IMLS. For  

00:01:24,560 --> 00:01:30,400
us, that's our primary granting agency. It's  the Institute of Museum and Library Services.  

00:01:32,640 --> 00:01:40,640
The grant was really about looking at  particular forms of algorithmic user experience.  

00:01:45,520 --> 00:01:52,880
A teaching moment with algorithms is where we  started. We found ways to document what are  

00:01:52,880 --> 00:01:58,560
the first principles of an algorithm, how  does a weighted graphic work in Facebook,  

00:01:59,360 --> 00:02:08,880
hypothetically, then we would talk in workshops  or in teaching settings and write pseudocode to  

00:02:08,880 --> 00:02:15,280
just kind of get people thinking in the code  mind‑set, about how when you make decisions  

00:02:15,840 --> 00:02:22,640
with a program, what that means. This work was focused on the pedagogy of  

00:02:22,640 --> 00:02:34,720
the algorithm. This is just introducing a broad  concept, giving people a chance to see how it  

00:02:35,360 --> 00:02:42,880
impact the material we did exercises with,  digital red lining, decisions we make,  

00:02:42,880 --> 00:02:52,240
decisions insurance companies might make. There's  a lot of ways into this teaching space. But  

00:02:52,960 --> 00:03:02,160
there was a component of this research that was  always a little ‑‑ it felt unanswered to me. There  

00:03:02,160 --> 00:03:10,480
are these experiences that we have, whether it's  in TikTok, whether it's in YouTube, on Instagram,  

00:03:11,600 --> 00:03:17,840
primarily in social media is what I was thinking  initially, where we're not really able to  

00:03:19,680 --> 00:03:31,920
understand what exactly ‑‑ how those algorithms  actually might manifest as a data point or as a  

00:03:31,920 --> 00:03:43,840
dataset. What I was struggling with is how  do you start to quantify something like an  

00:03:43,840 --> 00:03:52,560
algorithmic user experience, like an idea of the  timeline in Facebook or even the Twitter timeline.  

00:03:55,680 --> 00:04:00,000
I wanted to start to answer what if these  things were a little more visible or if  

00:04:00,000 --> 00:04:06,960
we came up with a set of pattern recognition  where we could actually say, okay, we captured  

00:04:07,840 --> 00:04:12,560
a form of that user experience and  encoded it as a kind of dataset. 

00:04:16,800 --> 00:04:22,960
In that initial research, we were able to kind of  pull apart all kinds of experiences of algorithms.  

00:04:24,240 --> 00:04:29,040
If you think about the top one here, masking,  this one is a little harder to quantify.  

00:04:30,720 --> 00:04:37,200
This demo I'm going to show doesn't really  talk about this, but the idea of masks,  

00:04:39,680 --> 00:04:46,960
the filters we can put on ourselves inside  of TikTok or Instagram. That's a form of an  

00:04:46,960 --> 00:04:55,840
algorithmic user experience. Lawyer Cat is the  sort of impact of algorithmic user experience. 

00:04:58,480 --> 00:05:04,160
Again, how would you start to capture  that experience as a data point?  

00:05:07,920 --> 00:05:14,320
We don't have to go far. This is last week on  Capitol Hill for those us in the United States,  

00:05:15,120 --> 00:05:21,200
in Washington, D.C. A hearing related  to how algorithms were working  

00:05:22,480 --> 00:05:25,120
in social media. What was interesting about  

00:05:25,120 --> 00:05:32,000
this particular meeting or hearing was  that there were policymakers and policy  

00:05:33,040 --> 00:05:40,640
analysts and academics talking about the role or  the place of algorithms in these environments. 

00:05:44,160 --> 00:05:50,000
For this initial cycle, as the end of that  first cycle of research was coming about,  

00:05:50,880 --> 00:05:55,040
a group of us that were still on the  end of that grant and our partners ‑‑  

00:06:00,960 --> 00:06:07,920
these were undergrads that kind of  worked throughout the project to  

00:06:08,880 --> 00:06:12,960
just answer some of the questions that we  were talking through the first part, about  

00:06:14,720 --> 00:06:19,840
the kinds of ways you might teach  or introduce these concepts.  

00:06:21,440 --> 00:06:28,480
But one of the things we all settled on was an  idea of the YouTube recommendation algorithm.  

00:06:30,320 --> 00:06:40,160
This is a little dated. It's from 2018, but the  principle of it does still stand. A lot of the  

00:06:40,160 --> 00:06:47,840
way people are kept and moved through the YouTube  ecosystem is through this recommendation engine.  

00:06:48,400 --> 00:06:53,040
That seemed like a particular proof  of concept for parts of this project. 

00:06:59,200 --> 00:07:08,640
This is a screenshot of a query for algebra. I'm  the actual profile that it is using right now.  

00:07:10,320 --> 00:07:18,640
If you look on the left, it is quickly moving  me from the query about algebra to various ‑‑  

00:07:22,560 --> 00:07:28,720
what we classified as out‑of‑scope  recommendations. This was a way for  

00:07:28,720 --> 00:07:40,960
us to balance and analyze the algorithm, but it is  instructive because it shows how quickly it moves  

00:07:43,040 --> 00:07:52,480
to personalization. I'm not interested  in lumber prices, but there it was. 

00:07:55,760 --> 00:08:02,080
This case study really was about how  would you begin to audit this experience.  

00:08:04,880 --> 00:08:15,440
I mentioned the undergrads before. Before I  leave this slide, the thing we started with,  

00:08:15,440 --> 00:08:24,240
we wanted to try to use the API to kind of see  how recommendations were happening for particular  

00:08:28,080 --> 00:08:37,600
videos and subject topics. But we realized that  without the simulation ‑‑ I'm going to talk about  

00:08:37,600 --> 00:08:43,040
this in a second as far as the processes you  should follow, if you want to start to do this.  

00:08:50,240 --> 00:08:59,040
The Pew Research YouTube recommendation algorithm  study used only the API. There's a real limitation  

00:08:59,040 --> 00:09:08,720
there because without the personalization and  the browser experience, you don't fall into  

00:09:08,720 --> 00:09:16,080
these rabbit holes that you see, like in this  example. It's trying to pull you out of scope. 

00:09:19,280 --> 00:09:22,000
The first thing we had to do  was think through how do you  

00:09:22,000 --> 00:09:30,800
simulate a user. This was creating a user agent  and particularly moving to web scraping and API  

00:09:31,760 --> 00:09:40,000
data mining. That's the first step. One of the  goals of this session is to kind of introduce  

00:09:41,200 --> 00:09:49,440
how you might start to do this. Again, we're at  the early cycles of this. Step one is move away  

00:09:49,440 --> 00:09:56,960
from ‑‑ even though those APIs are available,  whether it is TikTok or Instagram or YouTube,  

00:09:56,960 --> 00:10:04,960
you're not going to get the ability to  understand actual algorithmic user experience or  

00:10:05,920 --> 00:10:12,320
record it more importantly without using  first simulating a user and a user profile.  

00:10:15,600 --> 00:10:24,800
Step two is actually taking what you see  on that page, identifying the initial  

00:10:26,240 --> 00:10:30,880
video, and then moving to understand  and classify the recommendations on the  

00:10:32,800 --> 00:10:43,200
right side of your screen right now.  There were certain signals or tells  

00:10:43,200 --> 00:10:49,200
in the recommendations that we used to  start to classify good recommendations  

00:10:49,200 --> 00:10:54,960
or out‑of‑scope recommendations or bad  recommendations, so things like all caps.  

00:11:00,000 --> 00:11:06,560
All caps within a title. If a video is screaming  at you, there are chances that the quality wasn't  

00:11:06,560 --> 00:11:13,840
as good. It might be pulling you out of scope  from your original kind of intentional query.  

00:11:19,040 --> 00:11:25,280
We could watch for hyperlinks in the actual  description because those tended to ‑‑ in terms  

00:11:25,280 --> 00:11:33,840
of quality, those tended to be spammier videos and  ways that we could grade or evaluate the actual  

00:11:35,040 --> 00:11:41,040
video itself. And then anytime we moved ‑‑  we were able to have an original query intent  

00:11:41,760 --> 00:11:47,440
to understand it and then note when  the subject was moving out of scope. 

00:11:50,960 --> 00:12:03,360
Finally, regardless of where the analysis ended  up, the last thing that was necessary was a way to  

00:12:03,360 --> 00:12:11,920
capture and record what was there. That's step  three for us. For anybody thinking routinely  

00:12:12,560 --> 00:12:18,960
about this kind of work,  package that dataset for reuse.  

00:12:24,960 --> 00:12:28,960
I know this is a data focus  group. You'll inherently notice  

00:12:29,760 --> 00:12:35,680
or I would imagine you would notice that web  scraping and harvesting can be very brittle.  

00:12:37,600 --> 00:12:44,640
A frontend design can change pretty quickly.  The harvest of the title of the video is now  

00:12:44,640 --> 00:12:52,080
in a different set of tags and your script isn't  working. Another reason to pull that together and  

00:12:52,080 --> 00:12:58,160
make the recording of what the algorithmic user  experience was at that particular date and time. 

00:12:58,160 --> 00:13:08,720
>> Jumping in to give you a five‑minute  warning. We're going to go a couple minutes  

00:13:08,720 --> 00:13:12,400
over since we had a slow start. >> JASON: No worries. Thank you.  

00:13:20,800 --> 00:13:28,480
The one thing that really helped us was this  way to package a source of research data with  

00:13:29,520 --> 00:13:38,960
the metadata. This is the RO‑crate metadata  specification. If you're not aware of it,  

00:13:38,960 --> 00:13:47,200
it's a really useful tool. You can put a manifest  together and embed procedures so it has behavioral  

00:13:47,200 --> 00:13:54,640
metadata. How you would run analysis on a  particular dataset as well as the dataset itself. 

00:13:57,120 --> 00:14:06,400
Stepping back, when you think about this work ‑‑  again, we're at early cycles. I think we're ready  

00:14:06,400 --> 00:14:12,000
to think a little bit more about further  fellowships or different kinds of ways of  

00:14:12,720 --> 00:14:14,880
facilitating this kind of inquiry.  

00:14:17,040 --> 00:14:22,000
It's about finding that user agent and profile,  mapping out your harvesting data points,  

00:14:22,000 --> 00:14:25,200
collecting and analyzing the data,  and then encoding those results. 

00:14:29,680 --> 00:14:33,840
If there's one thing I want us to start  thinking or taking away, is how do we  

00:14:34,880 --> 00:14:47,440
identify and record algorithms as a form of data,  as datasets. The ghost that was left from that  

00:14:47,440 --> 00:14:53,040
last bit ‑‑ the first part of the research was  how do we move behind naming and identifying  

00:14:54,080 --> 00:14:58,160
a user algorithmic experience to quantifying it. 

00:15:01,040 --> 00:15:07,760
A couple of notes. There's a code repository  here. All of the work of the research is there.  

00:15:07,760 --> 00:15:14,480
There's the auditing tool and the various  parts of the script. There's a small app  

00:15:14,480 --> 00:15:19,600
that does some transparency work where you  can actually run a search user experience and  

00:15:22,000 --> 00:15:30,320
look at what's happening behind the scenes. >> Fantastic. Oh, sorry. You go ahead. 

00:15:30,320 --> 00:15:34,640
>> JASON: That's okay. Just wanted  to call out three people. Laurie  

00:15:44,640 --> 00:15:49,520
and Safiya Noble, and this group, The  Markup, which is a new nonprofit newsroom  

00:15:49,520 --> 00:15:56,560
that's doing a lot of this work about how  do you not only report but create data  

00:15:56,560 --> 00:16:03,280
and analyze our technologies. Thanks. >> Awesome. Thank you so much. That was  

00:16:03,280 --> 00:16:09,440
really fascinating. We've got  maybe two minutes for questions,  

00:16:09,440 --> 00:16:14,160
and we have two questions in the chat,  so let's go ahead and chat about them. 

00:16:15,520 --> 00:16:21,760
First, Megan asks, does algorithm  equal user agent? In other words,  

00:16:21,760 --> 00:16:29,120
are you using algorithms to measure an algorithm? >> JASON: (Laughter), that is so meta. I love it. 

00:16:29,120 --> 00:16:34,240
>> It's much like the view  people are getting right now. 

00:16:34,240 --> 00:16:48,480
>> JASON: I guess I wouldn't say user agent  is an algorithm, but certainly the work  

00:16:52,000 --> 00:16:58,320
of creating that profile or of identifying  that profile is something that feels  

00:16:59,200 --> 00:17:03,840
algorithmic, I suppose. It's okay, Megan. >> That's good. That's good. 

00:17:03,840 --> 00:17:09,280
>> JASON: No apologies. >> We have one more question from  

00:17:09,280 --> 00:17:14,320
Nikki, who asks, did you do anything like  survival analysis with the recommendations,  

00:17:14,320 --> 00:17:20,880
i.e., survival time to the rabbit hole? >> JASON: No. That's a great ‑‑ parts of  

00:17:20,880 --> 00:17:28,160
that analysis ‑‑ it was very binary, like is this  a good one, is this a bad one. We just wanted to  

00:17:28,160 --> 00:17:36,720
get a start, but I feel like there's so much  more as far as the metrics and putting it into  

00:17:36,720 --> 00:17:46,000
a pandas data frame. I work in Python. Building  out a couple new fields to kind of qualify the  

00:17:52,640 --> 00:17:58,080
analysis of the algorithm itself ‑‑ I'm  flashing back to the web scraper. We pulled the  

00:17:58,800 --> 00:18:05,360
duration of the video just to see how long  things were, if that could be an indicator.  

00:18:09,120 --> 00:18:13,760

YouTube URL: https://www.youtube.com/watch?v=a6Up8o4Hh_w


