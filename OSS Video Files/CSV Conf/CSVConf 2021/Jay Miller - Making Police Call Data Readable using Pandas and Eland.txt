Title: Jay Miller - Making Police Call Data Readable using Pandas and Eland
Publication date: 2021-05-19
Playlist: CSVConf 2021
Description: 
	Many cities in the United States have public data as a part of the oversight initiatives. These datasets often make the data incredibly hard to analyze. This talk breaks down how I compiled data from the San Diego Police Department into dataframes and make it observable in Elasticsearch Kibana.
Captions: 
	00:00:02,960 --> 00:00:06,880
all of that back story of 3 minutes  in, we're still making good time.  

00:00:06,880 --> 00:00:11,520
If you all look at my notes for this  talk, any future research that I do,  

00:00:12,160 --> 00:00:17,360
there's a link here. Feel free to grab that link.  It goes to a document. That is a live document. It  

00:00:17,360 --> 00:00:24,960
has this presentation and some other stuff that  I'm working on alongside of this included in it.  

00:00:26,400 --> 00:00:31,440
And as I mentioned before, I could add  so many more names to this. But at the  

00:00:31,440 --> 00:00:36,240
time of working on this talk, these were  the names that were very prominent to me. 

00:00:37,680 --> 00:00:42,720
And the idea of this came from a friend of  mine, Wayne Jones, who had done something  

00:00:42,720 --> 00:00:50,080
similar by documenting the shooting incidents  in New York City. Now, New York City is a lot  

00:00:50,080 --> 00:00:57,520
larger of a space, a lot more crowded than San  Diego. But I feel like if he was able to make  

00:00:57,520 --> 00:01:04,160
sense of all this data, I could as well. So, I'm asking, like, what is the purpose  

00:01:04,160 --> 00:01:10,560
of doing all of this? It's not to prove a point.  It's not to do any of those things. It's simple.  

00:01:11,200 --> 00:01:19,520
Make data readable. I mean, it's -- I have seen so  many times, and I will show as we jump into this,  

00:01:20,160 --> 00:01:26,000
that the data that I got was unreadable at first.  There was a lot of stuff. And I mean, I will say  

00:01:26,000 --> 00:01:33,840
that some of it was in CSV times. So, sorry. But it was very, very challenging. That racial  

00:01:33,840 --> 00:01:38,400
identification and profiling act data that  I came up -- that I was trying to discover  

00:01:38,400 --> 00:01:47,440
was separated between 12 different CSV files  that all documented the same 20,000 incidents.  

00:01:49,600 --> 00:01:55,920
So... having to create joins and make sense of all  that data just didn't make sense. So, my goal was  

00:01:55,920 --> 00:02:01,520
to make data readable for people who make real  decisions such as politicians, people who manage  

00:02:02,080 --> 00:02:10,720
city and municipal budgets as well as zoning laws  and other things. And training. But to also think  

00:02:10,720 --> 00:02:17,680
of other use cases for this data. You see, if --  if we make the data readable, it isn't just for  

00:02:18,320 --> 00:02:25,200
those in tech, it isn't just for those in Congress  or at the State Capitol. It could also be for  

00:02:25,200 --> 00:02:30,080
real estate. It could help people make  decisions when they're moving to an area. 

00:02:30,720 --> 00:02:38,080
I'm talking about this specific dataset, the  police call data. How many parties have the  

00:02:38,080 --> 00:02:45,040
cops called on them in a single area where  I'm from? And also allow resource allocation  

00:02:45,040 --> 00:02:51,040
if I know that one particular area of San  Diego has more police calls and more traffic  

00:02:51,040 --> 00:02:56,960
stops than another area. But that area is  grossly underfunded or under-resourced,  

00:02:56,960 --> 00:03:04,160
we can start allocating those funds and resources  to other points. And, of course, for training. 

00:03:04,160 --> 00:03:11,760
So, I say all of this to encourage you because  many cities themselves have open data. They all  

00:03:11,760 --> 00:03:16,000
follow what's called the PDDL or the Public  Domain and Distribution License. Which means  

00:03:16,800 --> 00:03:22,160
the government has to make this available.  And you're allowed to use it for whatever  

00:03:22,160 --> 00:03:28,000
reasons you want. So, I've done enough  talking. Let's look at some data. 

00:03:28,560 --> 00:03:35,920
And at this point, I do need to switch  screens because browser sizes and me  

00:03:35,920 --> 00:03:45,840
looking at cameras will not make sense.  So, let's find -- okay. There we go. And  

00:03:47,920 --> 00:03:55,680
perfect. Awesome. So, I wanted to mention  first -- if I can find my mouse. There we go.  

00:03:56,560 --> 00:04:01,280
As I mentioned before, all of this data is  available on San Diego's Open Data Portal.  

00:04:01,280 --> 00:04:08,800
And I just simply went to police -- to  police calls for service. And grabbed all  

00:04:09,680 --> 00:04:14,240
3 million records that existed in here. And what  you can kind of see here is you have an incident  

00:04:14,240 --> 00:04:23,200
number and some other information here. This  is not readable. This doesn't give us any help. 

00:04:23,840 --> 00:04:30,960
So, we're gonna go to our Jupyter Notebook here.  It's Python. I'm going to walk everybody through  

00:04:30,960 --> 00:04:38,160
it. I'm going to be using a data framing tool  called Pandas. Pandas allow us to re-create that  

00:04:38,880 --> 00:04:46,320
Excel spreadsheet look and feel on  data. I do this read CSV command. And  

00:04:47,040 --> 00:04:50,480
at this point, I just load the  dataset in. And I'm actually gonna... 

00:04:53,280 --> 00:04:59,040
Make sure nothing breaks. All right. Cool.  Perfect. So, I'm doing this live. All of  

00:04:59,040 --> 00:05:05,440
the data is stored on my computer. Or in a  safe cloud place. So, don't try to do this  

00:05:05,440 --> 00:05:10,160
without the data. But as we can see, the first  thing that I did was try to get a definition 

00:05:10,160 --> 00:05:14,880
What these call types are. Because I'm sure if you  have ever watched your favorite buddy cop film,  

00:05:14,880 --> 00:05:20,160
you have seen the comments. We  have a 1016 on Juniper and Ivy.  

00:05:22,080 --> 00:05:27,280
We don't know what that means. But if we have that  description, we can add that. So, that's the first  

00:05:27,280 --> 00:05:33,840
thing I did was add the call types dataset.  Then I had to build some helper scripts. And I  

00:05:33,840 --> 00:05:42,400
had to do this unfortunately because not only  was the data unreadable, but it was also not  

00:05:43,760 --> 00:05:48,320
consistent. Over the years, the process has  changed and improved for the better. Which  

00:05:48,320 --> 00:05:55,120
is great. However, we have to also make sure  that we can read the data as well over time. 

00:05:55,840 --> 00:05:59,680
So, we have built our helper scripts, we  have done that. I'm just gonna tap that  

00:05:59,680 --> 00:06:08,000
really quick to make sure those exist. And then  we start uploading data. I'm going to use just  

00:06:08,000 --> 00:06:13,600
the 2021 data because I'm livestreaming this  and I don't want my computer to explode. So,  

00:06:14,240 --> 00:06:21,200
what we're doing here is we're calling CSV_file,  this dataset from just 2021. Which I don't think  

00:06:21,200 --> 00:06:26,400
is -- I think it's last updated in early April.  And then we're gonna read that data as well. 

00:06:27,760 --> 00:06:32,160
So, we just hit that really quick. And boom!  We now have all of that data. And you can  

00:06:32,160 --> 00:06:38,000
still see those incident numbers, time of  day. We didn't change anything here. So,  

00:06:39,360 --> 00:06:43,680
this is where it gets fun. I'm here  to have fun. I hope y'all are too.  

00:06:44,880 --> 00:06:51,600
What I did here was I created what was called  the -- the DF with call type. So, the DF with  

00:06:51,600 --> 00:06:59,360
call type is simple. And by simple, I mean, it's  rocket science. We're going to do a merge on our  

00:06:59,360 --> 00:07:06,400
data frame. And that call type dataset that we  added, we're gonna merge it on the call type.  

00:07:07,120 --> 00:07:12,320
We're going to remove all the empty values in  that area and we're gonna drop the duplicates. 

00:07:12,320 --> 00:07:17,280
>> Can I make a request? There has been  a request for you to make this bigger. 

00:07:17,280 --> 00:07:21,200
Jay: Absolutely. >> To make it large. Can we do it? 

00:07:21,200 --> 00:07:23,440
Jay: Is that better? >> I tried to do it. But I  

00:07:23,440 --> 00:07:28,080
don't have the power. There we go. Jay: Yes. I hope that's better.  

00:07:29,520 --> 00:07:36,400
Okay. So, I should still be able to read this.  Awesome. All right. I'm gonna break this line up  

00:07:36,400 --> 00:07:44,720
a little bit. The pd.merge takes that glorified  Excel spreadsheet that we made, it's going to  

00:07:44,720 --> 00:07:53,680
combine it with those call types. The fillna is  going to remove the na, nan, not a number fields,  

00:07:53,680 --> 00:08:01,200
and just make it a blank string. Because  sometimes a blank string is just so much easier.  

00:08:01,840 --> 00:08:06,400
And then something weird happened when I did  this. It started creating some duplicate records.  

00:08:07,120 --> 00:08:12,880
Not sure why that happened. I've asked questions  and haven't gotten the right answers yet. But I  

00:08:12,880 --> 00:08:18,640
was told that if you drop duplicates, it  will not affect your data. I tested that  

00:08:18,640 --> 00:08:22,960
and, of course, the numbers did not change.  Which is good. That's what we like to see. 

00:08:24,080 --> 00:08:30,640
So, can we make sense of this data? Well, let's  find out. And if I get an error message, I know  

00:08:30,640 --> 00:08:40,240
why. Because I did not run this message. There we  go. Let's try that one more time. There we are. 

00:08:41,280 --> 00:08:46,720
All right. So, we're gonna create a sorting  function. That's what we did here. The lambda  

00:08:46,720 --> 00:08:55,600
function says, hey, sort it by the length  of the beat. Now, a beat is just a district  

00:08:55,600 --> 00:09:02,560
that -- not even a district -- it's a segment of  the space where police officers will patrol. So,  

00:09:02,560 --> 00:09:07,840
you'll hear me talk about a particular beat here.  For this example, I'm going to use beat 521.  

00:09:09,520 --> 00:09:16,880
So, the -- we're gonna go to the beat  and the call type. And we're gonna sort  

00:09:17,840 --> 00:09:27,280
in those areas. We're gonna sort by the beat  number first. So, if we hit that, what we see is a  

00:09:27,280 --> 00:09:33,040
list of beats and a bunch of calls. Now, remember,  this is just from January to about April. 

00:09:33,040 --> 00:09:41,600
So, beat 521, very busy. Over 9,000 calls.  And we can go through that. Now, I've given  

00:09:41,600 --> 00:09:48,240
this data in a Jupyter notebook because I know  in the data science space that seems to be the  

00:09:49,760 --> 00:09:57,200
default for Python, data science and  presentations. I want you to keep that in  

00:09:57,200 --> 00:10:04,480
mind as we run through the next 5 minutes. So, beat 523, we have that. Look at just  

00:10:04,480 --> 00:10:09,200
those calls. Update here, the records,  the day of the week, the address, the  

00:10:09,200 --> 00:10:15,680
record. We have covered all of this before, and  a description of what those calls actually were.  

00:10:16,880 --> 00:10:22,160
A little bit more readable now. Instead  of 1186, we got special detail. Great. 

00:10:24,240 --> 00:10:28,800
So, what about intersections? I want to see,  what are the busiest intersections, based on  

00:10:28,800 --> 00:10:36,640
our records? And that involves doing a group by  and some more sorting. But it is possible. We can  

00:10:36,640 --> 00:10:41,520
do that. We can get the busiest intersection which  happens to be Fifth and F. That's the busiest for  

00:10:42,640 --> 00:10:48,000
my city. But, of course, nothing  is more readable than a chart. 

00:10:48,560 --> 00:10:54,160
So, if I just -- I'm going to up date this chart  really quick. We can actually see, what are the  

00:10:54,160 --> 00:11:02,000
priority of calls based on this one beat, beat  523. Looks like most of the calls are priority 2.  

00:11:03,200 --> 00:11:08,480
Priority 2 actually means that it  is not of the utmost emergency,  

00:11:08,480 --> 00:11:15,040
but police should respond as fast as possible.  Three, you can take your time. One, you should  

00:11:15,040 --> 00:11:21,200
break laws to get there. And then we have some  other ones that have kind of faded away in time. 

00:11:23,280 --> 00:11:27,840
And, of course, the other  thing that we can check is...  

00:11:29,200 --> 00:11:34,720
this. Our -- what are the most frequent police  calls? Why are people calling the cops in  

00:11:34,720 --> 00:11:40,880
areas in San Diego? For most of them, they're  disturbing the peace. And then someone's reporting  

00:11:40,880 --> 00:11:45,200
a crime that's happening and would like someone  to come and actually do something about it. 

00:11:48,080 --> 00:11:53,120
It's great. It's a little narrow. As I mentioned  before, I'm a developer advocate for Elastic. So,  

00:11:53,120 --> 00:11:57,920
in most cases when I need a lot of data  and I need to understand a lot of data,  

00:11:58,720 --> 00:12:05,360
I choose the Elastic route which is Elasticsearch.  So, what I can do is I can take all of this data,  

00:12:05,360 --> 00:12:10,560
not just 2021, but all of the data,  and store it in an Elasticsearch server  

00:12:11,120 --> 00:12:15,840
which gives me the ability to then use  visualization tools like Kibana Lens  

00:12:15,840 --> 00:12:23,840
and save my computer from exploding. And I can  actually show you that. That's what this is. 

00:12:25,840 --> 00:12:29,680
A little bit more readable. There's a lot  going on. A lot of colors. I'm a colorful  

00:12:29,680 --> 00:12:38,560
person. I like colors. But we can now break  down by the day of the week, or the date,  

00:12:39,920 --> 00:12:47,760
what the calls were. And how they work. And  we can still see the data persists. We can  

00:12:47,760 --> 00:12:52,720
see disturbing the peace. If I want to check on a  beat. I don't have -- oh, I have 521 right here.  

00:12:53,840 --> 00:13:01,600
We can go to 521. And we should see, based on the  geo coordinates, 521's map should have changed. 

00:13:01,600 --> 00:13:10,480
>> Quick interruption to say 4 more minutes. Jay: Perfect. We can see where beat 521 in this  

00:13:10,480 --> 00:13:17,200
area. It's a really nice area. There's also a Pier  there. You should go if you're in the area. Now,  

00:13:18,480 --> 00:13:23,120
I mentioned earlier that Elasticsearch,  as beautiful as it is, is not how  

00:13:23,120 --> 00:13:28,400
data scientists present data to people making  decisions. They do it through Jupyter notebooks.  

00:13:36,320 --> 00:13:39,840
So, we have Eland. I'm going to show you how easy  it is in 3 minutes or less because I will get in  

00:13:39,840 --> 00:13:46,560
trouble if I don't. Traditionally, we would have  to import Elasticsearch, set up a client. We have  

00:13:46,560 --> 00:13:53,920
to do that anyway. Then we have to do a bulk  operator to bulk upload all of the 2021 data.  

00:13:54,640 --> 00:13:59,200
Provide any manipulations on it that we would  want. That's what this actions line is here.  

00:14:01,280 --> 00:14:07,680
But now, what we can do, is we can just  do all that work inside of data frames.  

00:14:08,720 --> 00:14:15,840
We can do things like, results, client search on  the index and match all of the hits that you want. 

00:14:18,320 --> 00:14:21,360
And when we do Eland, it's just one line of code.  

00:14:23,280 --> 00:14:26,800
Eland.dataFrame, the name of the data frame,  and we have to pass in the name of our client  

00:14:27,520 --> 00:14:33,920
and we get this nice little Elasticsearch  ID. But we get all the same information  

00:14:34,960 --> 00:14:38,560
in a slightly different order  that we were looking at before. 

00:14:40,480 --> 00:14:46,400
And as I mentioned before, we can do all of  the same things where we can sort and we can  

00:14:46,400 --> 00:14:50,240
highlight just the particular beat. Like  in this case, where we just get beat 523.  

00:14:54,960 --> 00:15:00,640
They look the same. And as I mentioned, the most  readable way to do this is to give them a nice  

00:15:00,640 --> 00:15:08,400
graph. We can even use the Pandas functionality  by providing graphs and plots using map plot.  

00:15:11,520 --> 00:15:16,880
Nothing changes, our data stays the same.  Ignore really long red errors because I  

00:15:16,880 --> 00:15:25,840
did not re-load that. And that's it. So, my  encouragement for you in the last 20 seconds is:  

00:15:27,040 --> 00:15:30,800
If you want to take action, if you want  to observe, and you keep being told,  

00:15:31,840 --> 00:15:37,040
but the data doesn't say... the data is probably  available and it's probably there. But not many  

00:15:37,040 --> 00:15:42,720
people have read it or seen it because it's not  really readable. But hopefully using tools like  

00:15:42,720 --> 00:15:48,400
Panda, using tools like Eland, maybe even tools  like this to pull up maps and fancy colors,  

00:15:49,200 --> 00:15:52,080
you can make your data more  readable. And you can present it  

00:15:52,720 --> 00:15:57,760

YouTube URL: https://www.youtube.com/watch?v=YM0zQ2OdWT0


