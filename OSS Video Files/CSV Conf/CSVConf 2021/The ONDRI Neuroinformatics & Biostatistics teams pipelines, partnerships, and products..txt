Title: The ONDRI Neuroinformatics & Biostatistics teams pipelines, partnerships, and products.
Publication date: 2021-05-20
Playlist: CSVConf 2021
Description: 
	Presenter:  Derek Beaton 

The Ontario Neurodegenerative Disease Research Initiative (ONDRI) is a large-scale longitudinal project across five disorders spanning 520 participants, each with 3 or more annual assessments, and from 14 sites across the province. ONDRI’s data span lab measures, genetics, ocular imaging, eye-tracking tasks, many cognitive assessments, various brain imaging modalities, and an extensive array of clinical assessments. These data come in many types, shapes, and sizes. Different domain and data experts have different expectations of data standards, how data are curated, and how data are analyzed. However, ONDRI is more than data: it is also 100s of researchers and clinicians coming together to better understand neurodegenerative and cerebrovascular disorders. So, ONDRI’s Neuroinformatics & Biostatistics (NIBS) team built both an infrastructure (data) and a culture (people) around the standardization and integrity of such diverse data. We did this with (1) pipelines: supporting curation from preparation through to analyses and reporting, (2) partnerships: working closely with others on training & collaboration, and (3) products: the tools we build and use; many of whichare publicly available: https://github.com/ondri-nibs. ONDRI’s data standards and outlier analyses are central to the pipelines. Our standards revolve around “data packages” that include data and dictionaries with specific nomenclature and features (e.g., predefined and unambiguous missing data codes). We built the standards around tabular (.csv format) data, then extended standards to “non-tabular” or file-based data, such as neuroimaging. We also perform a battery of (mostly) multivariate outlier analyses. We share standardized reports (written in RMarkdown) for standards and outliers with research and clinical experts to review. These steps identify anomalies and allow us to correct errors. We were able to build a data-focused culture based on shared clinical & research goals, recognizing the difficulty with & importance of data curation, and with an eye toward public release of the data.
Captions: 
	00:00:03,120 --> 00:00:06,480
okay great

00:00:03,760 --> 00:00:07,680
um okay so i'll be um talking about uh

00:00:06,480 --> 00:00:09,440
the anterior nervous gender

00:00:07,680 --> 00:00:11,120
disease research initiative uh

00:00:09,440 --> 00:00:12,799
specifically the neuroinformatics and

00:00:11,120 --> 00:00:14,719
biostatistics

00:00:12,799 --> 00:00:16,480
team and the pipelines partnerships and

00:00:14,719 --> 00:00:19,920
products we have in place

00:00:16,480 --> 00:00:22,080
um i'm derek uh i was a postdoc

00:00:19,920 --> 00:00:24,000
as part of this project for a number of

00:00:22,080 --> 00:00:26,480
years until about a week ago and i'm now

00:00:24,000 --> 00:00:28,800
a director of advanced analytics at the

00:00:26,480 --> 00:00:32,719
data science and advanced analytics

00:00:28,800 --> 00:00:35,840
division at unity health here in toronto

00:00:32,719 --> 00:00:38,239
so um andre for short

00:00:35,840 --> 00:00:40,719
what we call the project is

00:00:38,239 --> 00:00:43,040
fundamentally about people

00:00:40,719 --> 00:00:44,559
we have a lot of participants their

00:00:43,040 --> 00:00:46,239
family members friends and care partners

00:00:44,559 --> 00:00:48,719
that have volunteered their time

00:00:46,239 --> 00:00:49,440
in order for this project to exist we

00:00:48,719 --> 00:00:50,879
also have

00:00:49,440 --> 00:00:52,559
a lot of clinicians clinical

00:00:50,879 --> 00:00:54,480
coordinators researchers scientists

00:00:52,559 --> 00:00:56,239
analysts trainees and scholars

00:00:54,480 --> 00:00:58,559
management administration from all sorts

00:00:56,239 --> 00:01:02,879
of domains and disciplines that really

00:00:58,559 --> 00:01:04,239
form who makes up all of andre

00:01:02,879 --> 00:01:06,720
so we can think of this as a diverse

00:01:04,239 --> 00:01:09,119
group of individuals trying to achieve

00:01:06,720 --> 00:01:10,240
kind of the same same goals missions and

00:01:09,119 --> 00:01:12,560
outcomes

00:01:10,240 --> 00:01:13,360
but really and what is for today is that

00:01:12,560 --> 00:01:16,720
andre

00:01:13,360 --> 00:01:20,000
um is data um

00:01:16,720 --> 00:01:23,600
so when we think about andre as a

00:01:20,000 --> 00:01:26,880
a data problem um we usually present it

00:01:23,600 --> 00:01:28,880
as this hypercube where these colorful

00:01:26,880 --> 00:01:30,799
rows represent the different cohorts

00:01:28,880 --> 00:01:32,000
that individuals can be recruited into

00:01:30,799 --> 00:01:33,040
that are different neurodegenerative

00:01:32,000 --> 00:01:35,759
disorders

00:01:33,040 --> 00:01:37,439
with columns here that are the

00:01:35,759 --> 00:01:38,960
assessment platforms so the types of

00:01:37,439 --> 00:01:41,119
data that we have

00:01:38,960 --> 00:01:42,720
where every individual has a massive

00:01:41,119 --> 00:01:45,600
amount of measurements

00:01:42,720 --> 00:01:46,880
for each one of these um different

00:01:45,600 --> 00:01:49,119
assessment types

00:01:46,880 --> 00:01:50,240
and then going back across this this

00:01:49,119 --> 00:01:51,840
hypercube is

00:01:50,240 --> 00:01:54,479
the number of visits so how often a

00:01:51,840 --> 00:01:56,799
participant will will

00:01:54,479 --> 00:01:58,479
have their data collected in the project

00:01:56,799 --> 00:02:00,560
when we think about all of this

00:01:58,479 --> 00:02:02,399
really kind of the foundation that we

00:02:00,560 --> 00:02:04,960
need to to help make sense of

00:02:02,399 --> 00:02:06,320
the complexity and to make sense of all

00:02:04,960 --> 00:02:09,200
the different data types that exist

00:02:06,320 --> 00:02:11,360
within and across we need a foundation

00:02:09,200 --> 00:02:13,040
of informatics and statistics to kind of

00:02:11,360 --> 00:02:15,599
bind this together make it easy

00:02:13,040 --> 00:02:16,319
and make it more informative so i'm

00:02:15,599 --> 00:02:18,480
going to talk about

00:02:16,319 --> 00:02:20,720
um three key things the pipeline

00:02:18,480 --> 00:02:23,040
partnerships and products but

00:02:20,720 --> 00:02:25,280
there's going to obviously be a very big

00:02:23,040 --> 00:02:28,080
emphasis which is data standards and

00:02:25,280 --> 00:02:30,080
data packages in the project and how we

00:02:28,080 --> 00:02:34,000
we landed on what we landed uh

00:02:30,080 --> 00:02:36,640
for our standards in the laundry project

00:02:34,000 --> 00:02:37,440
so our pipeline briefly is that we have

00:02:36,640 --> 00:02:40,000
um

00:02:37,440 --> 00:02:40,720
uh what's called brain code uh by uh

00:02:40,000 --> 00:02:43,280
indock

00:02:40,720 --> 00:02:44,800
uh which is a system of systems to house

00:02:43,280 --> 00:02:46,720
different types of data so redcap will

00:02:44,800 --> 00:02:49,040
be for surveys and spread will be for

00:02:46,720 --> 00:02:50,239
imaging and other types of data then all

00:02:49,040 --> 00:02:52,480
the data go down to the different

00:02:50,239 --> 00:02:54,080
platforms uh specifically experts within

00:02:52,480 --> 00:02:56,239
those platforms to process

00:02:54,080 --> 00:02:57,599
curate and prepare the data they'll go

00:02:56,239 --> 00:02:59,920
through a standards check

00:02:57,599 --> 00:03:00,879
with the neuroinformatics team and if it

00:02:59,920 --> 00:03:02,560
fails it goes back

00:03:00,879 --> 00:03:05,040
if it passes it goes to an outlier

00:03:02,560 --> 00:03:05,920
analysis and then once passes outwire

00:03:05,040 --> 00:03:09,200
analyses

00:03:05,920 --> 00:03:10,080
it heads out for release however i want

00:03:09,200 --> 00:03:12,720
to go back in

00:03:10,080 --> 00:03:13,280
time a little bit uh to when uh things

00:03:12,720 --> 00:03:15,840
were not

00:03:13,280 --> 00:03:17,200
uh as standardized and we're all

00:03:15,840 --> 00:03:19,599
probably very familiar with

00:03:17,200 --> 00:03:21,120
how different data can be even when it's

00:03:19,599 --> 00:03:23,519
all from the same

00:03:21,120 --> 00:03:25,360
project or same initiatives so this is

00:03:23,519 --> 00:03:25,760
an example of one data set where we can

00:03:25,360 --> 00:03:29,680
see

00:03:25,760 --> 00:03:30,400
some blank cells sex is coded as one and

00:03:29,680 --> 00:03:32,560
two

00:03:30,400 --> 00:03:33,920
so we aren't entirely sure what that is

00:03:32,560 --> 00:03:34,720
we have something here called red cap

00:03:33,920 --> 00:03:37,040
event name

00:03:34,720 --> 00:03:38,000
that seems to indicate visits of some

00:03:37,040 --> 00:03:40,000
sort

00:03:38,000 --> 00:03:41,360
um another data set from a different

00:03:40,000 --> 00:03:44,239
platform had n

00:03:41,360 --> 00:03:46,319
a's and we had now have a visit column

00:03:44,239 --> 00:03:49,360
that says o2s

00:03:46,319 --> 00:03:50,799
and then get another data set where

00:03:49,360 --> 00:03:52,720
if we look at some of the commonalities

00:03:50,799 --> 00:03:54,400
across these we have some some column

00:03:52,720 --> 00:03:56,400
labels that are a little ambiguous if

00:03:54,400 --> 00:03:58,879
you don't know what they are

00:03:56,400 --> 00:04:00,640
and we have these very large numeric

00:03:58,879 --> 00:04:03,040
values that i can tell you

00:04:00,640 --> 00:04:04,400
are supposed to be missing codes so all

00:04:03,040 --> 00:04:08,080
of these things kind of reflect the

00:04:04,400 --> 00:04:10,480
institutional inertia within

00:04:08,080 --> 00:04:12,080
different fields disciplines and domains

00:04:10,480 --> 00:04:14,000
typically this is how

00:04:12,080 --> 00:04:15,760
different teams would deal with their

00:04:14,000 --> 00:04:19,040
data if they were not part of a

00:04:15,760 --> 00:04:22,079
larger scale effort so we need to do

00:04:19,040 --> 00:04:24,000
something to bring it all together

00:04:22,079 --> 00:04:25,759
and this is where our standards really

00:04:24,000 --> 00:04:27,360
developed from the basics that we

00:04:25,759 --> 00:04:29,040
we fundamentally require is that they're

00:04:27,360 --> 00:04:30,560
simple they are comprehensive

00:04:29,040 --> 00:04:32,160
generally fair which we heard a little

00:04:30,560 --> 00:04:36,560
bit about from

00:04:32,160 --> 00:04:39,680
donny and we used very key resources

00:04:36,560 --> 00:04:41,040
to to determine how we're going to build

00:04:39,680 --> 00:04:42,560
up our standards so a lot of these are

00:04:41,040 --> 00:04:43,360
about data sharing crossing between

00:04:42,560 --> 00:04:44,800
disciplines

00:04:43,360 --> 00:04:47,919
data organization which we heard a

00:04:44,800 --> 00:04:49,600
little bit of earlier today from carl

00:04:47,919 --> 00:04:51,520
and then quality curation and fair

00:04:49,600 --> 00:04:53,919
principles

00:04:51,520 --> 00:04:55,440
so i'll be showing you a little bit of

00:04:53,919 --> 00:04:57,040
the details from

00:04:55,440 --> 00:04:59,600
audrey's documentation the standards

00:04:57,040 --> 00:05:01,680
documentation and what we call the

00:04:59,600 --> 00:05:03,440
toy data set which is a synthetic data

00:05:01,680 --> 00:05:06,240
set bundled up to look

00:05:03,440 --> 00:05:07,759
like a real andrei data package so all

00:05:06,240 --> 00:05:09,440
the bells and whistles that are required

00:05:07,759 --> 00:05:11,680
so that people can follow along and

00:05:09,440 --> 00:05:13,280
develop these data packages on their own

00:05:11,680 --> 00:05:15,840
within the project or

00:05:13,280 --> 00:05:17,919
outside if ever they adopt these

00:05:15,840 --> 00:05:20,720
practices

00:05:17,919 --> 00:05:21,520
so an example snapshot of what we call a

00:05:20,720 --> 00:05:25,120
table

00:05:21,520 --> 00:05:26,720
tabular data package uh is here with all

00:05:25,120 --> 00:05:30,160
the different files that are

00:05:26,720 --> 00:05:32,240
essential and required we also require

00:05:30,160 --> 00:05:35,600
in the project a very specific naming

00:05:32,240 --> 00:05:37,520
convention that allows us to quickly

00:05:35,600 --> 00:05:39,280
search for when something is missing or

00:05:37,520 --> 00:05:41,440
something is misnamed

00:05:39,280 --> 00:05:43,759
so something more automated to make sure

00:05:41,440 --> 00:05:46,639
all the pieces are in place

00:05:43,759 --> 00:05:48,560
so the data package is fundamentally

00:05:46,639 --> 00:05:49,520
several files that are required and some

00:05:48,560 --> 00:05:50,880
that are optional

00:05:49,520 --> 00:05:53,440
i'll go through some of these in more

00:05:50,880 --> 00:05:55,680
detail but a quick overview of these is

00:05:53,440 --> 00:05:57,280
uh we have a readme file that is a

00:05:55,680 --> 00:05:59,919
structured

00:05:57,280 --> 00:06:01,280
tabular file in csv format to tell you

00:05:59,919 --> 00:06:02,720
what is in the package

00:06:01,280 --> 00:06:04,560
we have a methods document to give you

00:06:02,720 --> 00:06:06,560
all the nitty gritty details about

00:06:04,560 --> 00:06:08,319
how the data were produced and the ways

00:06:06,560 --> 00:06:10,960
in which it was produced

00:06:08,319 --> 00:06:12,800
and we also have a missing file uh this

00:06:10,960 --> 00:06:13,120
is a key level of detail that will tell

00:06:12,800 --> 00:06:15,199
you

00:06:13,120 --> 00:06:16,560
um when certain observations or

00:06:15,199 --> 00:06:19,039
participants are missing

00:06:16,560 --> 00:06:19,840
on the whole from a data package even

00:06:19,039 --> 00:06:23,520
though

00:06:19,840 --> 00:06:25,199
we may see data from those individuals

00:06:23,520 --> 00:06:26,560
in other data packages or other

00:06:25,199 --> 00:06:30,160
platforms

00:06:26,560 --> 00:06:32,080
so this is a missingness on the whole

00:06:30,160 --> 00:06:34,800
i'll go into more detail specifically

00:06:32,080 --> 00:06:36,479
about our dictionary and data files

00:06:34,800 --> 00:06:38,240
all right so our dictionary file is

00:06:36,479 --> 00:06:39,759
comprised of four columns

00:06:38,240 --> 00:06:41,840
uh something called column label

00:06:39,759 --> 00:06:44,960
description type and values

00:06:41,840 --> 00:06:47,120
and these first two i'll focus us on

00:06:44,960 --> 00:06:48,400
the column label are the variables you

00:06:47,120 --> 00:06:51,360
will find in the

00:06:48,400 --> 00:06:53,759
data file and the description is a short

00:06:51,360 --> 00:06:56,639
approximately 200 character

00:06:53,759 --> 00:06:58,319
description of what these variables mean

00:06:56,639 --> 00:07:00,240
in something that is relatively plain

00:06:58,319 --> 00:07:02,319
language

00:07:00,240 --> 00:07:04,000
so let's take a look at uh now what the

00:07:02,319 --> 00:07:05,759
data set looks like so we can see the

00:07:04,000 --> 00:07:08,000
order of these column labels and it is

00:07:05,759 --> 00:07:10,720
in the same order in the data set

00:07:08,000 --> 00:07:11,199
and underneath all of these um uh

00:07:10,720 --> 00:07:12,720
headers

00:07:11,199 --> 00:07:14,400
the column labels the variables

00:07:12,720 --> 00:07:18,240
themselves are the data

00:07:14,400 --> 00:07:20,479
um for the data file that we have

00:07:18,240 --> 00:07:22,000
so i promise not to take us through 43

00:07:20,479 --> 00:07:23,360
pages of documentation

00:07:22,000 --> 00:07:25,759
but i will take us through some of the

00:07:23,360 --> 00:07:26,960
key value pieces that we established

00:07:25,759 --> 00:07:28,880
as part of our standards that are a

00:07:26,960 --> 00:07:31,919
little bit different from most other

00:07:28,880 --> 00:07:34,080
standards with some reasons why so the

00:07:31,919 --> 00:07:36,000
key indicators are absolutely imperative

00:07:34,080 --> 00:07:37,680
across these packages and they are

00:07:36,000 --> 00:07:40,240
found in every data in every dictionary

00:07:37,680 --> 00:07:41,440
file these are subject visits site and

00:07:40,240 --> 00:07:43,680
date and they must be

00:07:41,440 --> 00:07:47,039
in the first four positions of all

00:07:43,680 --> 00:07:49,919
dictionary and data files

00:07:47,039 --> 00:07:51,759
this helps ensure that we can combine

00:07:49,919 --> 00:07:53,440
and merge data

00:07:51,759 --> 00:07:55,759
almost arbitrarily across the entire

00:07:53,440 --> 00:07:58,400
project

00:07:55,759 --> 00:08:00,000
we also require data types as part of

00:07:58,400 --> 00:08:01,039
the data dictionary and this will help

00:08:00,000 --> 00:08:02,479
data consumers

00:08:01,039 --> 00:08:04,639
uh understand the intent of the

00:08:02,479 --> 00:08:06,080
measurement usually as curators we know

00:08:04,639 --> 00:08:07,680
what they're supposed to be

00:08:06,080 --> 00:08:09,440
and what they're supposed to measure but

00:08:07,680 --> 00:08:11,360
this isn't always familiar to every type

00:08:09,440 --> 00:08:12,560
of data consumer once they get access to

00:08:11,360 --> 00:08:14,720
the data

00:08:12,560 --> 00:08:16,160
we have a small list that we define in

00:08:14,720 --> 00:08:18,080
the project including text and

00:08:16,160 --> 00:08:19,759
categorical ordinal numeric

00:08:18,080 --> 00:08:21,440
even mixed data so when something

00:08:19,759 --> 00:08:23,759
changes from say

00:08:21,440 --> 00:08:26,160
decimals to a greater than or less than

00:08:23,759 --> 00:08:27,919
we needed to accommodate for that

00:08:26,160 --> 00:08:30,479
one in particular that i'd like to point

00:08:27,919 --> 00:08:33,919
out which may be controversial is

00:08:30,479 --> 00:08:35,680
our uh date and in particular our date

00:08:33,919 --> 00:08:38,159
format

00:08:35,680 --> 00:08:39,200
so we've elected to use a non-standard

00:08:38,159 --> 00:08:41,680
date format

00:08:39,200 --> 00:08:43,440
iso 8601 is regarded as standard

00:08:41,680 --> 00:08:44,959
unambiguous however

00:08:43,440 --> 00:08:46,959
through our experience it's only

00:08:44,959 --> 00:08:49,760
unambiguous if everybody knows it and

00:08:46,959 --> 00:08:51,279
everybody uses it which is not the case

00:08:49,760 --> 00:08:53,279
this is especially problematic if you're

00:08:51,279 --> 00:08:56,560
trying to determine if it is

00:08:53,279 --> 00:08:59,040
january 12th or december 1st

00:08:56,560 --> 00:08:59,839
and this is kind of a big deal when it

00:08:59,040 --> 00:09:01,519
comes to

00:08:59,839 --> 00:09:03,920
aging and neurodegenerative research

00:09:01,519 --> 00:09:06,160
projects because data points

00:09:03,920 --> 00:09:07,200
very far apart or very close are are

00:09:06,160 --> 00:09:09,360
important to know

00:09:07,200 --> 00:09:10,880
especially as individuals with

00:09:09,360 --> 00:09:14,080
neurodegenerative disorders

00:09:10,880 --> 00:09:17,680
uh going to decline so so

00:09:14,080 --> 00:09:20,640
having absolutely firm dates is key

00:09:17,680 --> 00:09:22,640
so no ambiguity is really important and

00:09:20,640 --> 00:09:23,360
we elected to go with four digits three

00:09:22,640 --> 00:09:25,600
characters

00:09:23,360 --> 00:09:27,120
two digits to reflect year month and

00:09:25,600 --> 00:09:30,000
date

00:09:27,120 --> 00:09:31,920
uh day uh uh effectively as a contract

00:09:30,000 --> 00:09:33,839
between data curators and data consumers

00:09:31,920 --> 00:09:36,480
we are putting a stamp on this to say

00:09:33,839 --> 00:09:37,200
we have verified unambiguously it is the

00:09:36,480 --> 00:09:39,120
state

00:09:37,200 --> 00:09:40,720
or at least to the best of our ability

00:09:39,120 --> 00:09:41,440
we are telling you this is the date we

00:09:40,720 --> 00:09:43,680
believe

00:09:41,440 --> 00:09:44,800
data collection or some um occurrence

00:09:43,680 --> 00:09:46,399
happened

00:09:44,800 --> 00:09:48,399
we also were motivated by the fact that

00:09:46,399 --> 00:09:51,839
this is easily readable and convertible

00:09:48,399 --> 00:09:51,839
for both humans and machines

00:09:51,920 --> 00:09:55,680
missing codes is another um element that

00:09:54,160 --> 00:09:58,160
we brought into our standards

00:09:55,680 --> 00:10:00,080
and what we did was survey the project

00:09:58,160 --> 00:10:01,440
and lots of individuals across all the

00:10:00,080 --> 00:10:04,240
different platforms

00:10:01,440 --> 00:10:04,959
and we asked what kind of data are going

00:10:04,240 --> 00:10:06,959
to be missing

00:10:04,959 --> 00:10:08,560
in the ontario neurogenic disease

00:10:06,959 --> 00:10:11,360
research initiative now

00:10:08,560 --> 00:10:12,399
and in future projects we came up with

00:10:11,360 --> 00:10:13,680
what we believe is a fairly

00:10:12,399 --> 00:10:16,320
comprehensive list

00:10:13,680 --> 00:10:16,959
of the types of missingness that could

00:10:16,320 --> 00:10:18,240
occur

00:10:16,959 --> 00:10:20,000
so instead of something just being

00:10:18,240 --> 00:10:22,880
missing or instead of having

00:10:20,000 --> 00:10:24,480
numeric codes these are predefined and

00:10:22,880 --> 00:10:27,279
harmonized ways of

00:10:24,480 --> 00:10:29,920
indicating in data this is missing for a

00:10:27,279 --> 00:10:31,440
very specific reason

00:10:29,920 --> 00:10:34,240
and we can even see a little bit of this

00:10:31,440 --> 00:10:35,920
in the toy data set under uh naw percent

00:10:34,240 --> 00:10:38,000
which is normal pairing um

00:10:35,920 --> 00:10:39,440
uh white matter percentage for a

00:10:38,000 --> 00:10:42,399
specific participant

00:10:39,440 --> 00:10:43,839
we have m art which um is a better

00:10:42,399 --> 00:10:45,519
signal of missingness here

00:10:43,839 --> 00:10:47,200
it is due to an artifact in the

00:10:45,519 --> 00:10:48,880
neuroimaging pipeline

00:10:47,200 --> 00:10:51,120
or mirror imaging collection process as

00:10:48,880 --> 00:10:53,040
opposed to say an administrative error

00:10:51,120 --> 00:10:55,120
or the inability

00:10:53,040 --> 00:10:58,839
due to cognitive or behavioral deficits

00:10:55,120 --> 00:11:00,959
uh to actually administer some sort of

00:10:58,839 --> 00:11:02,640
assessment so this brings me to one of

00:11:00,959 --> 00:11:03,200
our next stages i'm going to shift out

00:11:02,640 --> 00:11:05,120
of

00:11:03,200 --> 00:11:06,959
our standards and now into another piece

00:11:05,120 --> 00:11:09,040
of our pipeline which is the outliers

00:11:06,959 --> 00:11:10,959
process

00:11:09,040 --> 00:11:13,440
it is pretty important for us to

00:11:10,959 --> 00:11:14,160
establish whether anomalies are errors

00:11:13,440 --> 00:11:15,839
or not

00:11:14,160 --> 00:11:17,279
we have a lot of data on a lot of

00:11:15,839 --> 00:11:19,839
individuals and

00:11:17,279 --> 00:11:21,279
they're they're very heterogeneous so we

00:11:19,839 --> 00:11:22,240
wanted to know are these very strange

00:11:21,279 --> 00:11:24,720
values real

00:11:22,240 --> 00:11:27,120
reflective of something very specific or

00:11:24,720 --> 00:11:29,440
is it an error in collection or an error

00:11:27,120 --> 00:11:31,120
in processing so the neuroinformatics

00:11:29,440 --> 00:11:33,360
and biostatistics team

00:11:31,120 --> 00:11:34,480
for most of the data releases nearly all

00:11:33,360 --> 00:11:35,839
of them

00:11:34,480 --> 00:11:38,160
perform what we call the outliers

00:11:35,839 --> 00:11:39,600
pipeline when data come in in the

00:11:38,160 --> 00:11:42,320
standardized format

00:11:39,600 --> 00:11:44,320
we go through a series of analyses the

00:11:42,320 --> 00:11:45,600
first is a partially squares regression

00:11:44,320 --> 00:11:46,959
because all of these are effectively

00:11:45,600 --> 00:11:50,000
multivariate data

00:11:46,959 --> 00:11:52,000
multi-response data and we use either

00:11:50,000 --> 00:11:54,000
generalized form or a specific form

00:11:52,000 --> 00:11:55,920
of partially squares regression to

00:11:54,000 --> 00:11:58,800
handle say asian sex uh

00:11:55,920 --> 00:11:59,839
as as covariates for virtually all data

00:11:58,800 --> 00:12:01,760
sets

00:11:59,839 --> 00:12:03,440
the first stage of our outlier analysis

00:12:01,760 --> 00:12:04,959
is to perform a principal components or

00:12:03,440 --> 00:12:06,959
a correspondence analysis

00:12:04,959 --> 00:12:08,560
depending on the data types so pca is

00:12:06,959 --> 00:12:10,079
generally for numeric data

00:12:08,560 --> 00:12:12,000
where correspondence analysis can be

00:12:10,079 --> 00:12:13,519
used for categorical ordinal

00:12:12,000 --> 00:12:16,000
or mixtures of different data types

00:12:13,519 --> 00:12:18,320
including continuous

00:12:16,000 --> 00:12:19,920
the next stage is a minimum covariance

00:12:18,320 --> 00:12:22,079
determinant or a generalized version

00:12:19,920 --> 00:12:24,000
that we developed in the project

00:12:22,079 --> 00:12:26,000
to effectively handle mixtures of

00:12:24,000 --> 00:12:27,680
different data types and identify which

00:12:26,000 --> 00:12:29,200
participants may be outliers in a

00:12:27,680 --> 00:12:32,320
specific data package

00:12:29,200 --> 00:12:34,639
and finally finding out which items are

00:12:32,320 --> 00:12:36,560
are driving these anomaly these

00:12:34,639 --> 00:12:38,320
anomalies these anomalous observations

00:12:36,560 --> 00:12:40,320
there's something called the core max

00:12:38,320 --> 00:12:42,240
procedure where we again

00:12:40,320 --> 00:12:43,680
also developed a new variant of this

00:12:42,240 --> 00:12:46,959
called the generalized core max

00:12:43,680 --> 00:12:48,880
so we can handle different data types

00:12:46,959 --> 00:12:50,079
and once all that is all said and done

00:12:48,880 --> 00:12:51,680
we don't just

00:12:50,079 --> 00:12:52,959
have those pictures and lots of stats

00:12:51,680 --> 00:12:54,480
and multivariate things because that

00:12:52,959 --> 00:12:56,800
makes most people upset

00:12:54,480 --> 00:12:57,760
we bundle them up into uh fairly

00:12:56,800 --> 00:12:59,519
straightforward

00:12:57,760 --> 00:13:01,040
harmonized reports and give them back to

00:12:59,519 --> 00:13:01,920
the different curation teams and

00:13:01,040 --> 00:13:04,240
curators

00:13:01,920 --> 00:13:05,360
so they can review this and then go look

00:13:04,240 --> 00:13:07,279
in more detail

00:13:05,360 --> 00:13:09,120
at specific individuals or groups of

00:13:07,279 --> 00:13:10,800
individuals and say oh yes

00:13:09,120 --> 00:13:12,480
this is an error i need to go fix this

00:13:10,800 --> 00:13:14,399
in our pipeline or that was not

00:13:12,480 --> 00:13:17,040
collected correctly

00:13:14,399 --> 00:13:19,440
or oh yes this is real it's just very

00:13:17,040 --> 00:13:19,440
strange

00:13:19,600 --> 00:13:22,959
and all of this is supported through a

00:13:21,440 --> 00:13:23,760
lot of software that we've we've

00:13:22,959 --> 00:13:25,519
developed

00:13:23,760 --> 00:13:27,839
some of it before we did some of this

00:13:25,519 --> 00:13:29,600
some of it uh as we're actually going

00:13:27,839 --> 00:13:31,600
through the project so these are through

00:13:29,600 --> 00:13:34,720
standards and outliers applications

00:13:31,600 --> 00:13:37,120
as well as an arm markdown template and

00:13:34,720 --> 00:13:39,519
really these are in place so that we can

00:13:37,120 --> 00:13:41,279
effectively guarantee a harmonization of

00:13:39,519 --> 00:13:42,959
the process

00:13:41,279 --> 00:13:44,560
regardless of whatever data are coming

00:13:42,959 --> 00:13:46,800
in

00:13:44,560 --> 00:13:48,000
so this will lead me to partnerships i'm

00:13:46,800 --> 00:13:48,399
gonna go a little faster in some of

00:13:48,000 --> 00:13:50,240
these

00:13:48,399 --> 00:13:52,480
next sections here so there are two

00:13:50,240 --> 00:13:53,120
major components um for the partnerships

00:13:52,480 --> 00:13:54,880
between

00:13:53,120 --> 00:13:56,399
um all of the different platforms in the

00:13:54,880 --> 00:13:58,240
project but more specifically the

00:13:56,399 --> 00:14:00,880
neuroinformatics and biostatistics team

00:13:58,240 --> 00:14:02,720
and the rest of the platforms and one

00:14:00,880 --> 00:14:04,320
way in which we form um

00:14:02,720 --> 00:14:05,839
pretty strong partnerships is by working

00:14:04,320 --> 00:14:07,839
directly with the teams uh

00:14:05,839 --> 00:14:09,760
you're seeing some snapshots of uh some

00:14:07,839 --> 00:14:12,160
gitlab repositories where

00:14:09,760 --> 00:14:13,920
we've worked with uh different platforms

00:14:12,160 --> 00:14:16,079
to help get pipelines set up

00:14:13,920 --> 00:14:17,920
so that their data are formatted uh

00:14:16,079 --> 00:14:21,040
tested we're doing some

00:14:17,920 --> 00:14:23,440
checks and they come out of this code

00:14:21,040 --> 00:14:25,040
in a standard ready format so that

00:14:23,440 --> 00:14:26,720
people don't really have to do anything

00:14:25,040 --> 00:14:28,399
manually they don't have to check things

00:14:26,720 --> 00:14:30,079
they don't have to compute things

00:14:28,399 --> 00:14:31,519
so we're working with a lot of teams to

00:14:30,079 --> 00:14:32,959
do that

00:14:31,519 --> 00:14:34,720
the other side of this is really

00:14:32,959 --> 00:14:35,920
critical because we have a lot of

00:14:34,720 --> 00:14:37,839
technical skill

00:14:35,920 --> 00:14:39,279
uh we need to provide our time and

00:14:37,839 --> 00:14:40,880
training to a lot of the other teams

00:14:39,279 --> 00:14:41,680
which includes phone calls site visits

00:14:40,880 --> 00:14:43,199
one-on-ones

00:14:41,680 --> 00:14:45,040
a variety of different beverages after

00:14:43,199 --> 00:14:46,800
those meetings

00:14:45,040 --> 00:14:49,199
lots of reference material that we make

00:14:46,800 --> 00:14:53,120
available usually to google docs or

00:14:49,199 --> 00:14:54,000
on our github we've run many workshops

00:14:53,120 --> 00:14:57,360
to help

00:14:54,000 --> 00:14:58,240
have closer ties and deeper explanations

00:14:57,360 --> 00:15:00,720
of how to do

00:14:58,240 --> 00:15:02,000
and what to do and why to do um the

00:15:00,720 --> 00:15:04,399
things that we do

00:15:02,000 --> 00:15:06,320
uh and then also a major component of

00:15:04,399 --> 00:15:07,839
this is the formalization of teamwork so

00:15:06,320 --> 00:15:09,519
what does it really mean to do

00:15:07,839 --> 00:15:11,199
the different pieces and one way in

00:15:09,519 --> 00:15:12,560
which we've we've pushed for this is

00:15:11,199 --> 00:15:13,360
through something called the credit

00:15:12,560 --> 00:15:15,839
taxonomy

00:15:13,360 --> 00:15:17,760
um a lot of people do a lot of things

00:15:15,839 --> 00:15:19,519
and a lot of it can't be hidden in this

00:15:17,760 --> 00:15:21,199
process

00:15:19,519 --> 00:15:23,680
so we're trying to uncover a lot of that

00:15:21,199 --> 00:15:24,959
through the formalization of teamwork

00:15:23,680 --> 00:15:27,600
so next i'm going to talk about some of

00:15:24,959 --> 00:15:29,040
the products products are vital because

00:15:27,600 --> 00:15:30,720
they support both the partnerships and

00:15:29,040 --> 00:15:32,560
the pipelines we can't do any of this

00:15:30,720 --> 00:15:35,839
other stuff without the things that

00:15:32,560 --> 00:15:36,800
that make it all run we've seen five of

00:15:35,839 --> 00:15:38,240
these

00:15:36,800 --> 00:15:40,720
out of many that we have so

00:15:38,240 --> 00:15:43,519
documentation toy data packages that are

00:15:40,720 --> 00:15:45,759
very concrete examples some of the tools

00:15:43,519 --> 00:15:47,680
but i want to highlight that we need

00:15:45,759 --> 00:15:48,639
these to make working with our data much

00:15:47,680 --> 00:15:50,320
easier

00:15:48,639 --> 00:15:51,759
and we needed to build these around the

00:15:50,320 --> 00:15:53,120
standards and the goals and missions of

00:15:51,759 --> 00:15:54,959
the project

00:15:53,120 --> 00:15:56,880
i'm going to highlight just two uh that

00:15:54,959 --> 00:15:57,600
are kind of opposite ends that are some

00:15:56,880 --> 00:15:59,519
of the

00:15:57,600 --> 00:16:01,440
the favorites and and i think most

00:15:59,519 --> 00:16:02,800
important ones that are up and coming or

00:16:01,440 --> 00:16:04,480
well established

00:16:02,800 --> 00:16:06,079
one of the products is actually a clone

00:16:04,480 --> 00:16:07,519
of the wes anderson package by one of

00:16:06,079 --> 00:16:10,079
the organizers of

00:16:07,519 --> 00:16:11,279
csv conference karthik where this is

00:16:10,079 --> 00:16:14,079
frankly our most popular

00:16:11,279 --> 00:16:15,040
um our package uh it helps harmonize the

00:16:14,079 --> 00:16:17,279
color palette

00:16:15,040 --> 00:16:19,600
across the project so that whenever

00:16:17,279 --> 00:16:21,519
whenever someone makes a new graph or

00:16:19,600 --> 00:16:23,680
or something we all get the same colors

00:16:21,519 --> 00:16:25,839
across all of our papers

00:16:23,680 --> 00:16:27,199
uh one of the next uh most important

00:16:25,839 --> 00:16:28,800
products right now is something that's

00:16:27,199 --> 00:16:31,199
in a prototype stage

00:16:28,800 --> 00:16:32,959
and this uh ties right back to all of

00:16:31,199 --> 00:16:34,480
the standards um we've developed

00:16:32,959 --> 00:16:36,399
something called andre dataframe or

00:16:34,480 --> 00:16:39,920
android df for short

00:16:36,399 --> 00:16:42,079
where the goal of this is to scoop in

00:16:39,920 --> 00:16:44,160
the data and the dictionary files

00:16:42,079 --> 00:16:45,839
preserve all the missingness while while

00:16:44,160 --> 00:16:47,279
mapping them out and then giving you a

00:16:45,839 --> 00:16:47,920
description of how many things are

00:16:47,279 --> 00:16:49,920
missing

00:16:47,920 --> 00:16:52,079
why they're missing and then give you

00:16:49,920 --> 00:16:54,959
some of that preserved information about

00:16:52,079 --> 00:16:57,199
the variables from the dictionary in a

00:16:54,959 --> 00:16:59,120
data frame

00:16:57,199 --> 00:17:00,399
so this will bring me to the conclusion

00:16:59,120 --> 00:17:03,600
um

00:17:00,399 --> 00:17:04,160
a lot of this this is not possible

00:17:03,600 --> 00:17:06,959
without

00:17:04,160 --> 00:17:07,919
a strong culture seeing the same goals

00:17:06,959 --> 00:17:10,319
and missions

00:17:07,919 --> 00:17:11,919
and trying to achieve the same goals um

00:17:10,319 --> 00:17:13,520
it can be very hard to sell a lot of

00:17:11,919 --> 00:17:16,799
technical things to

00:17:13,520 --> 00:17:18,480
to large-scale projects but with a lot

00:17:16,799 --> 00:17:21,520
of those close bonds with

00:17:18,480 --> 00:17:22,400
time spent with people the value in all

00:17:21,520 --> 00:17:24,000
of these things

00:17:22,400 --> 00:17:25,919
starts becoming very apparent and

00:17:24,000 --> 00:17:28,480
everybody uh will

00:17:25,919 --> 00:17:29,760
get on board eventually um so with that

00:17:28,480 --> 00:17:30,960
i'd like to acknowledge

00:17:29,760 --> 00:17:32,640
a lot of different individuals and

00:17:30,960 --> 00:17:33,600
organizations so this work was primarily

00:17:32,640 --> 00:17:35,520
done at bay crest

00:17:33,600 --> 00:17:37,120
through funding available from the

00:17:35,520 --> 00:17:39,039
ontario brain institute

00:17:37,120 --> 00:17:40,160
in the ontario neurodegenerative disease

00:17:39,039 --> 00:17:43,120
research initiative

00:17:40,160 --> 00:17:44,400
with a variety of individual platforms

00:17:43,120 --> 00:17:46,640
who were subjected to

00:17:44,400 --> 00:17:47,679
many early versions of standards and

00:17:46,640 --> 00:17:50,240
software and

00:17:47,679 --> 00:17:52,320
we apologize and are appreciative of our

00:17:50,240 --> 00:17:54,320
collaborators and friends and colleagues

00:17:52,320 --> 00:17:57,200
who worked with us while we built these

00:17:54,320 --> 00:17:58,559
tools up from the ground

00:17:57,200 --> 00:18:00,640
and those include a number of

00:17:58,559 --> 00:18:04,080
individuals including this list here

00:18:00,640 --> 00:18:06,480
of of the core group to really push

00:18:04,080 --> 00:18:09,440
for a lot of the standardization and

00:18:06,480 --> 00:18:11,919
establishing a lot of these pipelines

00:18:09,440 --> 00:18:13,039
a lot of the students ras and

00:18:11,919 --> 00:18:14,000
researchers that have worked with the

00:18:13,039 --> 00:18:15,840
neuroinformatics team

00:18:14,000 --> 00:18:17,280
over the years that are really running

00:18:15,840 --> 00:18:18,400
all these pipelines and making sure

00:18:17,280 --> 00:18:20,960
everything works

00:18:18,400 --> 00:18:21,840
and a number of uh individuals across

00:18:20,960 --> 00:18:24,640
the entire

00:18:21,840 --> 00:18:25,840
uh project in management leadership and

00:18:24,640 --> 00:18:27,520
different platforms

00:18:25,840 --> 00:18:29,600
that have been vital both in a

00:18:27,520 --> 00:18:32,720
supportive way and in a

00:18:29,600 --> 00:18:34,160
collaborative way so i have a million

00:18:32,720 --> 00:18:35,280
references and i'm just going to kind of

00:18:34,160 --> 00:18:37,520
go through these

00:18:35,280 --> 00:18:38,960
and leave us on some resources and

00:18:37,520 --> 00:18:43,120
finally a

00:18:38,960 --> 00:18:43,120

YouTube URL: https://www.youtube.com/watch?v=2UK6dROXyn8


