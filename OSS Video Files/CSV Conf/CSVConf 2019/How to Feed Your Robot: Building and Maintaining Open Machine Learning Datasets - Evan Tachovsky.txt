Title: How to Feed Your Robot: Building and Maintaining Open Machine Learning Datasets - Evan Tachovsky
Publication date: 2019-06-21
Playlist: CSVConf 2019
Description: 
	While algorithms and computing power get all the press, the special sauce behind many recent machine learning breakthroughs are meticulously labeled training data. Developing and maintaining these data sets as public goods is both an art and a science. In this talk I'll present a new set of best practices gleaned from interview with ~20 data set builders, maintainers, and funders. Topics include: encouraging collaboration between rival data teams; finding and addressing ethical issues with crowd labeling; launching competitions to spur data set use; and revenue generation models for sustainability.


Talk page: https://csvconf.com/speakers/#evan-tachovsky
slides: https://doi.org/10.5281/zenodo.3233117#.XO02fdL52RQ
Captions: 
	00:00:01,530 --> 00:00:05,010
my name is Evan dasky I'm a data

00:00:03,480 --> 00:00:07,830
scientist and program officer at the

00:00:05,010 --> 00:00:10,290
Rockefeller Foundation so in that role I

00:00:07,830 --> 00:00:11,700
do a little bit of data science so I

00:00:10,290 --> 00:00:13,769
work in service of our teams our

00:00:11,700 --> 00:00:15,330
grantees folks Ubu and otherwise be able

00:00:13,769 --> 00:00:17,099
to afford data science services we help

00:00:15,330 --> 00:00:20,130
them out with that and then I also fund

00:00:17,099 --> 00:00:21,660
tools infrastructure help people start

00:00:20,130 --> 00:00:23,039
products to make sure the machine

00:00:21,660 --> 00:00:24,660
learning tools are accessible to people

00:00:23,039 --> 00:00:27,060
who are working on development and

00:00:24,660 --> 00:00:30,060
humanitarian problems mostly across this

00:00:27,060 --> 00:00:32,970
work one of the biggest problems we face

00:00:30,060 --> 00:00:35,070
is building label dating sets for

00:00:32,970 --> 00:00:36,540
machine learning so today I'm going to

00:00:35,070 --> 00:00:37,770
talk a little bit about that we kind of

00:00:36,540 --> 00:00:38,760
took a step back and did some research

00:00:37,770 --> 00:00:40,890
this past couple months

00:00:38,760 --> 00:00:41,760
let me share that research with you just

00:00:40,890 --> 00:00:44,670
to kind of get everyone on the same

00:00:41,760 --> 00:00:46,800
level we do a one minute introduction to

00:00:44,670 --> 00:00:48,570
supervised machine learning for those

00:00:46,800 --> 00:00:51,030
who haven't a certain count this field

00:00:48,570 --> 00:00:52,710
before the basic idea is that we're

00:00:51,030 --> 00:00:54,420
going to show an algorithm or computer a

00:00:52,710 --> 00:00:55,890
set of objects let's say they're

00:00:54,420 --> 00:00:57,570
pictures and we're going to show that a

00:00:55,890 --> 00:00:59,160
set of labels let's say what's in that

00:00:57,570 --> 00:01:01,050
picture to believe maybe but be a

00:00:59,160 --> 00:01:02,309
bounding box around where actually the

00:01:01,050 --> 00:01:04,470
cat is in that picture where the dog is

00:01:02,309 --> 00:01:05,880
we're showing a lot of those so you can

00:01:04,470 --> 00:01:07,650
pick up on features you can pick up on

00:01:05,880 --> 00:01:08,909
but general patterns and then we show it

00:01:07,650 --> 00:01:10,350
something out in the real world might be

00:01:08,909 --> 00:01:12,960
a slightly different cat a winking cat

00:01:10,350 --> 00:01:14,430
or a dog with a different color etc and

00:01:12,960 --> 00:01:16,320
then the algorithm is going to make a

00:01:14,430 --> 00:01:17,759
guess and say I actually think that that

00:01:16,320 --> 00:01:20,700
looks a lot like what I've learned to be

00:01:17,759 --> 00:01:22,500
a cat or a dog there's only one subfield

00:01:20,700 --> 00:01:25,200
of machine learning but it tends to be

00:01:22,500 --> 00:01:27,930
where we spend a lot of our time right

00:01:25,200 --> 00:01:30,060
this is where when we're saying we want

00:01:27,930 --> 00:01:32,070
to automate diagnosis from medical

00:01:30,060 --> 00:01:34,259
imagery this is a big part of what we're

00:01:32,070 --> 00:01:37,710
doing when when I find a team that wants

00:01:34,259 --> 00:01:40,049
to detect crop yields from space in

00:01:37,710 --> 00:01:40,950
Africa across whole continents that's

00:01:40,049 --> 00:01:45,030
what they're doing they're doing a lot

00:01:40,950 --> 00:01:46,649
of supervised machine learning the

00:01:45,030 --> 00:01:49,079
problem is is that in the environments

00:01:46,649 --> 00:01:52,290
where we work the datasets that do exist

00:01:49,079 --> 00:01:54,450
are incredibly biased so incredibly low

00:01:52,290 --> 00:01:56,700
representation if you take image net or

00:01:54,450 --> 00:01:58,979
it's sort of somewhat success or open

00:01:56,700 --> 00:02:00,149
images by the two biggest machine

00:01:58,979 --> 00:02:02,100
learning imagery datasets

00:02:00,149 --> 00:02:04,350
sixty percent of those images are coming

00:02:02,100 --> 00:02:06,899
from just six countries in North America

00:02:04,350 --> 00:02:09,239
and northern Europe right so that means

00:02:06,899 --> 00:02:10,860
when you show a classifier is this a

00:02:09,239 --> 00:02:13,200
wedding and it's a wedding say from

00:02:10,860 --> 00:02:14,400
Pakistan the classroom I won't know that

00:02:13,200 --> 00:02:16,739
that's a wedding

00:02:14,400 --> 00:02:18,390
that's sort of trivial in that case but

00:02:16,739 --> 00:02:20,370
you can imagine that transport into the

00:02:18,390 --> 00:02:22,140
medical field right identifying cancer

00:02:20,370 --> 00:02:23,909
identifying symptoms of a heart disease

00:02:22,140 --> 00:02:25,769
or heart failure things like that and it

00:02:23,909 --> 00:02:27,269
becomes catastrophic it becomes tools

00:02:25,769 --> 00:02:30,569
that we can't even begin to use

00:02:27,269 --> 00:02:32,310
similarly in many places where we're

00:02:30,569 --> 00:02:34,680
getting transactional data say even like

00:02:32,310 --> 00:02:37,079
open data from our cities that encodes a

00:02:34,680 --> 00:02:40,379
lot of bias a lot of racial bias there's

00:02:37,079 --> 00:02:42,420
a great study I'll be linked in my work

00:02:40,379 --> 00:02:44,129
here but from Russia to Robinson

00:02:42,420 --> 00:02:46,170
basically looking at police departments

00:02:44,129 --> 00:02:48,150
that were under consent degrees so they

00:02:46,170 --> 00:02:50,459
were literally being investigated for

00:02:48,150 --> 00:02:52,290
racist policing and then using that data

00:02:50,459 --> 00:02:53,430
directly to build classifiers about

00:02:52,290 --> 00:02:56,370
where they should police in the future

00:02:53,430 --> 00:02:57,959
so even if those police departments fix

00:02:56,370 --> 00:02:59,250
their policies fix their work they

00:02:57,959 --> 00:03:01,109
weren't fixing the algorithms are built

00:02:59,250 --> 00:03:02,670
on those data and these patterns repeat

00:03:01,109 --> 00:03:05,489
over and over again in the places where

00:03:02,670 --> 00:03:07,200
we work so we said let's take a step

00:03:05,489 --> 00:03:08,549
back let's talk to some of the people

00:03:07,200 --> 00:03:09,569
who are building these data sets let's

00:03:08,549 --> 00:03:11,280
talk to some of these people who are

00:03:09,569 --> 00:03:13,200
building them in industry let's talk to

00:03:11,280 --> 00:03:14,400
academic researchers let's talk to

00:03:13,200 --> 00:03:16,620
critical thinkers who are thinking about

00:03:14,400 --> 00:03:18,629
this and you know ask them a set of semi

00:03:16,620 --> 00:03:20,489
structured questions and work through it

00:03:18,629 --> 00:03:22,199
and say how do you build these data sets

00:03:20,489 --> 00:03:23,639
what motivates you what are the

00:03:22,199 --> 00:03:25,290
technical challenges you're running into

00:03:23,639 --> 00:03:26,730
what data sets have you seen have been

00:03:25,290 --> 00:03:30,299
really effective what data sets have

00:03:26,730 --> 00:03:31,530
disappeared quickly about 20 folks I'm

00:03:30,299 --> 00:03:33,000
not going to in tribute anything to

00:03:31,530 --> 00:03:34,949
anybody because a lot of folks actually

00:03:33,000 --> 00:03:38,730
are working on new stuff potentially IP

00:03:34,949 --> 00:03:40,199
related stuff and then I will also note

00:03:38,730 --> 00:03:41,760
that mostly interviewees are focused on

00:03:40,199 --> 00:03:44,459
text applications or imagery

00:03:41,760 --> 00:03:46,409
applications so we don't have anyone in

00:03:44,459 --> 00:03:47,790
here who is say looking at audio or

00:03:46,409 --> 00:03:50,159
things like that and those are unique

00:03:47,790 --> 00:03:51,599
domains don't discount them but most

00:03:50,159 --> 00:03:55,229
these folks are working on text or

00:03:51,599 --> 00:03:57,389
images here's some of those lovely folks

00:03:55,229 --> 00:03:59,639
do you see these people are doing really

00:03:57,389 --> 00:04:02,250
awesome stuff so for example Desmond

00:03:59,639 --> 00:04:05,099
Patton he is building a classifier that

00:04:02,250 --> 00:04:07,079
looks at Instagram posts and tags

00:04:05,099 --> 00:04:09,120
whether there is actual sort of a true

00:04:07,079 --> 00:04:11,250
threat and not for police departments

00:04:09,120 --> 00:04:12,690
but for harm reduction folks who working

00:04:11,250 --> 00:04:14,760
on this stuff in Chicago this is a

00:04:12,690 --> 00:04:16,109
really the idea of you know giving

00:04:14,760 --> 00:04:17,909
communities access to algorithmic

00:04:16,109 --> 00:04:19,680
decision-making so they can help and

00:04:17,909 --> 00:04:22,469
preserve their sort of communities is an

00:04:19,680 --> 00:04:24,870
amazing thing metemma oh sorry boot

00:04:22,469 --> 00:04:28,050
emboss daniel is working on building

00:04:24,870 --> 00:04:30,599
classifiers for essentially we trust

00:04:28,050 --> 00:04:32,009
in cassava and that means going on

00:04:30,599 --> 00:04:33,780
working with farmers collecting imagery

00:04:32,009 --> 00:04:35,729
from them building incentive structures

00:04:33,780 --> 00:04:36,840
to get them to send imagery making sure

00:04:35,729 --> 00:04:38,909
they aren't cheating on those imagery

00:04:36,840 --> 00:04:40,080
etc and so these are just amazing folks

00:04:38,909 --> 00:04:42,870
if you happy to look up any other

00:04:40,080 --> 00:04:44,310
research it's really great stuff today

00:04:42,870 --> 00:04:46,440
I'm going to share essentially five

00:04:44,310 --> 00:04:48,419
lessons from this work and these are

00:04:46,440 --> 00:04:50,190
lessons that are more general so in

00:04:48,419 --> 00:04:51,389
addition to each of these overarching

00:04:50,190 --> 00:04:52,830
lessons we have a fair amount of

00:04:51,389 --> 00:04:55,080
research that goes into like each domain

00:04:52,830 --> 00:04:57,060
I'll be more specific but I want to take

00:04:55,080 --> 00:04:58,620
a step back and say across all of these

00:04:57,060 --> 00:05:00,030
initiatives what are the five things

00:04:58,620 --> 00:05:02,580
that were kind of learning or seeing in

00:05:00,030 --> 00:05:05,610
general patterns the first one is that

00:05:02,580 --> 00:05:07,169
motivations shaped datasets more so than

00:05:05,610 --> 00:05:08,909
any other thing when we dug into it and

00:05:07,169 --> 00:05:10,740
we're asking people why are you doing

00:05:08,909 --> 00:05:12,539
this people are completely

00:05:10,740 --> 00:05:14,490
cross-purposes there's a set of

00:05:12,539 --> 00:05:16,889
motivations I'll describe as commercial

00:05:14,490 --> 00:05:18,569
and those are essential people who want

00:05:16,889 --> 00:05:20,310
to build a new product they're people

00:05:18,569 --> 00:05:21,630
who are saying well I've got an idea

00:05:20,310 --> 00:05:23,819
I probably pitched it to someone

00:05:21,630 --> 00:05:25,830
somewhere and I need enough data to just

00:05:23,819 --> 00:05:27,330
build that Minimum Viable Product that's

00:05:25,830 --> 00:05:28,969
gonna get me the fundraising to go do

00:05:27,330 --> 00:05:30,780
the thing that's one set of folks

00:05:28,969 --> 00:05:32,159
there's another set of folks who are

00:05:30,780 --> 00:05:34,440
saying you know what I need this data

00:05:32,159 --> 00:05:36,030
size of my moat access to these data to

00:05:34,440 --> 00:05:38,099
these records to these labels records is

00:05:36,030 --> 00:05:39,840
what I need to make sure that no one's

00:05:38,099 --> 00:05:41,580
ever going to be able to come from my

00:05:39,840 --> 00:05:42,659
market and there's a bunch of other

00:05:41,580 --> 00:05:44,370
folks who are working in commercial

00:05:42,659 --> 00:05:45,990
settings who know they have a problem

00:05:44,370 --> 00:05:47,580
with their algorithm so these would be

00:05:45,990 --> 00:05:50,279
folks for working on algorithms that

00:05:47,580 --> 00:05:52,710
that don't work to detect hate that

00:05:50,279 --> 00:05:54,180
don't work to detect bias in hiring and

00:05:52,710 --> 00:05:55,680
they know they have a critical problem

00:05:54,180 --> 00:05:57,029
and they're working commercially to try

00:05:55,680 --> 00:05:58,979
to fix it before someone calls the

00:05:57,029 --> 00:06:01,050
moment the second set of motivations are

00:05:58,979 --> 00:06:04,199
what I'll call methodological folks and

00:06:01,050 --> 00:06:05,490
these are folks who frankly care less

00:06:04,199 --> 00:06:07,169
about the substance of the data what

00:06:05,490 --> 00:06:09,509
they care about is that the data set

00:06:07,169 --> 00:06:10,650
will be a benchmark the data set will

00:06:09,509 --> 00:06:12,569
allow them to sort of compete with

00:06:10,650 --> 00:06:15,960
others to prove out a new method to

00:06:12,569 --> 00:06:17,699
publish or to simply pursuit curiosity

00:06:15,960 --> 00:06:19,710
this is where a lot of computer

00:06:17,699 --> 00:06:20,699
scientists find themselves of course

00:06:19,710 --> 00:06:22,680
there are many computer scientist with

00:06:20,699 --> 00:06:25,409
many motivations but generally these are

00:06:22,680 --> 00:06:28,860
the folks who are approaching it with a

00:06:25,409 --> 00:06:30,330
little abstraction and then finally and

00:06:28,860 --> 00:06:32,069
this is the sort of place where I find

00:06:30,330 --> 00:06:33,300
myself as a dataset funder is the

00:06:32,069 --> 00:06:36,779
applied folks and what we're thinking

00:06:33,300 --> 00:06:38,880
about is there's no one in Africa who's

00:06:36,779 --> 00:06:40,409
building a classifier that allows

00:06:38,880 --> 00:06:41,669
community health workers to determine if

00:06:40,409 --> 00:06:44,099
a certain disease exists

00:06:41,669 --> 00:06:46,650
in geography X right I need to solve

00:06:44,099 --> 00:06:48,240
that problem across these motivations

00:06:46,650 --> 00:06:50,759
none of them are you know distinctly bad

00:06:48,240 --> 00:06:52,289
or distinctly good I would say there's

00:06:50,759 --> 00:06:55,259
some that lead to bad outcomes or good

00:06:52,289 --> 00:06:57,030
outcomes but you tend to when you have a

00:06:55,259 --> 00:06:58,620
project see different people across the

00:06:57,030 --> 00:07:02,370
project with different motivations and

00:06:58,620 --> 00:07:04,919
that leads to I would say that the the

00:07:02,370 --> 00:07:06,960
process of managing these motivations is

00:07:04,919 --> 00:07:08,580
what putting together a multi skilled

00:07:06,960 --> 00:07:09,960
team looks like so you will have to have

00:07:08,580 --> 00:07:11,969
some computer scientists who are

00:07:09,960 --> 00:07:14,250
interested sort of in the pure problem

00:07:11,969 --> 00:07:15,990
they're working on this is often the

00:07:14,250 --> 00:07:17,520
problem why it's hard for us to track

00:07:15,990 --> 00:07:18,900
some computer scientists to work on some

00:07:17,520 --> 00:07:20,520
problems that for us in the applied

00:07:18,900 --> 00:07:21,810
setting are super interesting because

00:07:20,520 --> 00:07:24,690
there's nothing new to be done there

00:07:21,810 --> 00:07:26,190
right like it it's totally a discovered

00:07:24,690 --> 00:07:27,599
field and it's an application problem

00:07:26,190 --> 00:07:29,250
and so for them to go through the

00:07:27,599 --> 00:07:31,349
process of working with you it's

00:07:29,250 --> 00:07:33,449
specking out a dataset or doing anything

00:07:31,349 --> 00:07:35,099
like that is that's like a lost quarter

00:07:33,449 --> 00:07:36,930
for them and then similarly the

00:07:35,099 --> 00:07:38,370
commercial folks they're happy to talk

00:07:36,930 --> 00:07:40,409
with you about building maybe this data

00:07:38,370 --> 00:07:42,689
set but at the moment they find out that

00:07:40,409 --> 00:07:44,099
you're gonna have to license this in a

00:07:42,689 --> 00:07:45,539
certain way that say we would make them

00:07:44,099 --> 00:07:47,610
listen to it open instead of like that

00:07:45,539 --> 00:07:50,190
they're gonna walk right away and they

00:07:47,610 --> 00:07:52,650
also have a potentially a lower bar for

00:07:50,190 --> 00:07:53,639
quality so they might be able to just

00:07:52,650 --> 00:07:55,860
get it to a certain point where they

00:07:53,639 --> 00:07:56,849
could pitch it or build it but it's not

00:07:55,860 --> 00:07:58,409
going to be sufficient for either

00:07:56,849 --> 00:08:01,969
scientific or for us to kind of release

00:07:58,409 --> 00:08:04,289
it the second area we found a lot is

00:08:01,969 --> 00:08:05,879
transactional labels are really worse

00:08:04,289 --> 00:08:07,740
than you think so when you're getting

00:08:05,879 --> 00:08:09,270
labels out of electronic health record

00:08:07,740 --> 00:08:11,819
system you're getting labels out of a

00:08:09,270 --> 00:08:14,879
power billing system these labels are

00:08:11,819 --> 00:08:16,589
bad because they were built to do one

00:08:14,879 --> 00:08:18,120
specific test that was not trained a

00:08:16,589 --> 00:08:20,879
classifier to classify something down

00:08:18,120 --> 00:08:22,800
the road they were built so that a

00:08:20,879 --> 00:08:24,810
medical billing office could efficiently

00:08:22,800 --> 00:08:26,099
send out bills to people right they

00:08:24,810 --> 00:08:27,930
weren't built so that you could

00:08:26,099 --> 00:08:29,610
determine within 20 minutes of someone

00:08:27,930 --> 00:08:31,199
hitting an ER whether they're going to

00:08:29,610 --> 00:08:33,599
have a higher or lower risk for for

00:08:31,199 --> 00:08:34,860
heart failure and that is always the

00:08:33,599 --> 00:08:38,490
case of transactional labels and

00:08:34,860 --> 00:08:39,839
transactional labels well I'd say more

00:08:38,490 --> 00:08:41,909
so than any other type of labeling

00:08:39,839 --> 00:08:44,310
carries the sort of systemic bias ease

00:08:41,909 --> 00:08:46,350
of a system within and it can be harder

00:08:44,310 --> 00:08:47,699
to see and get out and so if you're

00:08:46,350 --> 00:08:49,019
going to use these transactional labels

00:08:47,699 --> 00:08:51,930
which of course people do of course

00:08:49,019 --> 00:08:53,610
there's value the basic recommendation

00:08:51,930 --> 00:08:55,050
is you have to embed with that team for

00:08:53,610 --> 00:08:57,660
six months to a year

00:08:55,050 --> 00:08:59,700
and you have to really understand why

00:08:57,660 --> 00:09:01,320
these decisions are being made what

00:08:59,700 --> 00:09:03,180
different labels mean what are the sort

00:09:01,320 --> 00:09:05,790
of embedded incentives behind them and

00:09:03,180 --> 00:09:07,050
and then you can you know consider to

00:09:05,790 --> 00:09:09,510
see if it's useful and we've funded

00:09:07,050 --> 00:09:11,820
teams that in the power setting actually

00:09:09,510 --> 00:09:14,160
someone someone who worked with a large

00:09:11,820 --> 00:09:16,560
power company in Africa for two years

00:09:14,160 --> 00:09:18,180
and knew exactly where all of those data

00:09:16,560 --> 00:09:20,340
sets are broken he's able to build a

00:09:18,180 --> 00:09:21,810
reliable demand prediction algorithm but

00:09:20,340 --> 00:09:23,370
only because he has that sort of depth

00:09:21,810 --> 00:09:26,640
of knowledge not just because he has

00:09:23,370 --> 00:09:29,100
access to the data third thing is

00:09:26,640 --> 00:09:31,320
there's essentially a labeling spectrum

00:09:29,100 --> 00:09:33,240
that we see emerging right so on one end

00:09:31,320 --> 00:09:35,150
it's stuff that anyone can label that's

00:09:33,240 --> 00:09:37,830
finding a school bus in this image

00:09:35,150 --> 00:09:40,350
that's stuff like highlight all the

00:09:37,830 --> 00:09:41,880
universities in this document simple

00:09:40,350 --> 00:09:44,040
stuff you can parse that out - crowd

00:09:41,880 --> 00:09:45,840
workers the issues they're simply making

00:09:44,040 --> 00:09:46,980
sure that you're using well when you're

00:09:45,840 --> 00:09:48,780
treating people fairly you're paying

00:09:46,980 --> 00:09:49,830
them what their time is worth and to

00:09:48,780 --> 00:09:51,780
that you're giving them enough

00:09:49,830 --> 00:09:53,070
guidelines enough rules and sending

00:09:51,780 --> 00:09:54,750
yourself sort of a gold standard data

00:09:53,070 --> 00:09:57,150
set to compare their performance against

00:09:54,750 --> 00:09:58,620
that stuff that's that teams are getting

00:09:57,150 --> 00:10:00,390
really good at that's that's totally

00:09:58,620 --> 00:10:02,220
possible but it only addresses a very

00:10:00,390 --> 00:10:04,200
small set of the problems where we want

00:10:02,220 --> 00:10:05,940
to build data sets on the other end we

00:10:04,200 --> 00:10:09,000
had the experts only category this is

00:10:05,940 --> 00:10:11,160
stuff medical imagery this is stuff in a

00:10:09,000 --> 00:10:13,680
lot of like the hard sciences so this is

00:10:11,160 --> 00:10:16,050
stuff where to even become nano dater is

00:10:13,680 --> 00:10:18,090
at the end of a long process potentially

00:10:16,050 --> 00:10:21,120
a PhD potentially sort of a medical

00:10:18,090 --> 00:10:23,130
degree there's actually less than those

00:10:21,120 --> 00:10:24,420
two there's less interesting stuff in

00:10:23,130 --> 00:10:26,040
either those two fields and you think a

00:10:24,420 --> 00:10:27,930
lot of the most interesting stuff is in

00:10:26,040 --> 00:10:30,390
this sort of messy middle so these are

00:10:27,930 --> 00:10:32,250
things where yeah you could probably get

00:10:30,390 --> 00:10:34,380
some undergrads to do that right you

00:10:32,250 --> 00:10:35,700
could probably contract with some people

00:10:34,380 --> 00:10:37,200
on Mechanical Turk to do that and

00:10:35,700 --> 00:10:40,280
there's this there's this weird sort of

00:10:37,200 --> 00:10:45,060
messy metal and and here it's stuff like

00:10:40,280 --> 00:10:47,340
so we use the the example of imagery of

00:10:45,060 --> 00:10:49,530
plants and diseases implants that's

00:10:47,340 --> 00:10:50,940
stuff that farmers in Africa working in

00:10:49,530 --> 00:10:53,340
agricultural settings but without any

00:10:50,940 --> 00:10:55,230
formal education can be trained to tag

00:10:53,340 --> 00:10:57,360
and identify it can also be trained to

00:10:55,230 --> 00:10:58,470
tag and identify pass the problem is

00:10:57,360 --> 00:11:01,290
it's in this messy middle it's not

00:10:58,470 --> 00:11:03,180
automatic not everyone can do it not

00:11:01,290 --> 00:11:05,730
everyone wants to spend the time doing

00:11:03,180 --> 00:11:08,279
it and and it's not a

00:11:05,730 --> 00:11:10,649
it's an not necessarily good value for

00:11:08,279 --> 00:11:11,910
folks who are in that expert end so the

00:11:10,649 --> 00:11:14,609
kind of the three rules we're thinking

00:11:11,910 --> 00:11:16,980
about there are one first incentives and

00:11:14,609 --> 00:11:18,899
here actually monetary incentives while

00:11:16,980 --> 00:11:20,730
sort of necessary for a lot of this work

00:11:18,899 --> 00:11:23,939
are really not sufficient for improved

00:11:20,730 --> 00:11:27,660
performance going back to that sort of

00:11:23,939 --> 00:11:30,600
labeling we trust example what that team

00:11:27,660 --> 00:11:33,419
found is that simply by giving immediate

00:11:30,600 --> 00:11:35,100
feedback on whether the algorithm at a

00:11:33,419 --> 00:11:37,100
basic level thought there was or wasn't

00:11:35,100 --> 00:11:39,449
the the sort of disease present

00:11:37,100 --> 00:11:40,739
increased farmer compliance with giving

00:11:39,449 --> 00:11:42,809
more samples right so if you're getting

00:11:40,739 --> 00:11:45,119
value in that moment you're shortening

00:11:42,809 --> 00:11:47,040
that feedback loop even if you caveat

00:11:45,119 --> 00:11:48,329
and you say you're not totally sure you

00:11:47,040 --> 00:11:50,129
should also call your farm extension

00:11:48,329 --> 00:11:52,369
worker have them come check it out etc

00:11:50,129 --> 00:11:54,389
if you can shorten that up for people

00:11:52,369 --> 00:11:56,040
that's what gets people coming back

00:11:54,389 --> 00:11:58,079
submitting more samples submitting

00:11:56,040 --> 00:11:59,639
higher-quality samples coming to your

00:11:58,079 --> 00:12:02,279
meetings coming to your trainings

00:11:59,639 --> 00:12:05,819
getting better tagging things the second

00:12:02,279 --> 00:12:07,199
is new tools so this is where simply

00:12:05,819 --> 00:12:09,899
building tooling that makes problems

00:12:07,199 --> 00:12:12,329
easier for people creating workflows for

00:12:09,899 --> 00:12:14,339
people doing a lot of pre-processing so

00:12:12,329 --> 00:12:15,720
images actually look more distinct it's

00:12:14,339 --> 00:12:19,139
not as hard to trace boundaries around

00:12:15,720 --> 00:12:20,249
things that can really increase the

00:12:19,139 --> 00:12:21,809
number of problems that people can

00:12:20,249 --> 00:12:25,259
tackle without much formal training and

00:12:21,809 --> 00:12:27,569
the final one is think kind of rethink

00:12:25,259 --> 00:12:29,069
from scratch whether you can reclassify

00:12:27,569 --> 00:12:31,379
in a different way so instead of asking

00:12:29,069 --> 00:12:32,789
doctors who may disagree about the

00:12:31,379 --> 00:12:34,859
presence of sepsis or something like

00:12:32,789 --> 00:12:37,199
that in an emergency health setting can

00:12:34,859 --> 00:12:39,629
you actually for this problem use 60 day

00:12:37,199 --> 00:12:41,189
readmission can you use you know within

00:12:39,629 --> 00:12:42,720
one year death and can you train a

00:12:41,189 --> 00:12:44,339
classifier that gets you the type of

00:12:42,720 --> 00:12:45,929
answers you want in the moment but with

00:12:44,339 --> 00:12:50,639
the totally different much easier kind

00:12:45,929 --> 00:12:53,399
of classification problem fourth don't

00:12:50,639 --> 00:12:54,869
ignore shelf-life so and this one was

00:12:53,399 --> 00:12:56,639
actually somewhat surprising to me when

00:12:54,869 --> 00:12:58,470
I asked people you know how long is your

00:12:56,639 --> 00:13:00,600
data set can be good for if you you cut

00:12:58,470 --> 00:13:03,569
off today it's getting this sort of

00:13:00,600 --> 00:13:05,669
f-score across the board and you know in

00:13:03,569 --> 00:13:07,949
five years we actually put in new data

00:13:05,669 --> 00:13:09,569
store it and then score it again you

00:13:07,949 --> 00:13:11,399
know how long is this going to be a

00:13:09,569 --> 00:13:13,319
viable data set for classification going

00:13:11,399 --> 00:13:15,209
on and the answer generally is a lot

00:13:13,319 --> 00:13:17,189
shorter than I initially thought

00:13:15,209 --> 00:13:19,080
so there's differences across domains

00:13:17,189 --> 00:13:21,180
but the two basically rules are

00:13:19,080 --> 00:13:22,830
one is is this data set operating in an

00:13:21,180 --> 00:13:24,660
adversarial environment so does someone

00:13:22,830 --> 00:13:27,120
have a motivation to adjust their

00:13:24,660 --> 00:13:29,580
behavior in relation to being scored or

00:13:27,120 --> 00:13:32,430
classified in some way if so that's

00:13:29,580 --> 00:13:34,350
gonna be like a almost a weekly basis

00:13:32,430 --> 00:13:36,360
you need to figure out a system or a way

00:13:34,350 --> 00:13:40,260
to get new labels into your system

00:13:36,360 --> 00:13:42,480
really quickly the second is how stable

00:13:40,260 --> 00:13:44,339
are the biological social processes that

00:13:42,480 --> 00:13:46,350
go into it so an emergency room is

00:13:44,339 --> 00:13:48,510
really unstable things change in an

00:13:46,350 --> 00:13:50,640
emergency room all the time staff come

00:13:48,510 --> 00:13:52,470
and go people come and go and cetera in

00:13:50,640 --> 00:13:54,120
that setting you're almost on that

00:13:52,470 --> 00:13:55,980
similar cycle of like it has a very

00:13:54,120 --> 00:13:57,959
short shelf life you need to be funding

00:13:55,980 --> 00:13:59,610
datasets every sort of year every

00:13:57,959 --> 00:14:02,000
quarter every time a new sort of cohort

00:13:59,610 --> 00:14:05,100
of medical practitioners comes through

00:14:02,000 --> 00:14:07,350
and then finally in fifth the

00:14:05,100 --> 00:14:09,029
overarching theme here right is we have

00:14:07,350 --> 00:14:10,560
to think about machine learning day sets

00:14:09,029 --> 00:14:12,720
is infrastructure not as research

00:14:10,560 --> 00:14:15,480
projects and this is something that I

00:14:12,720 --> 00:14:18,089
think a lot of practitioners will tell

00:14:15,480 --> 00:14:19,380
you and when pushed there's still

00:14:18,089 --> 00:14:21,630
there's some ambiguity about what they

00:14:19,380 --> 00:14:24,060
mean by infrastructure so it was unclear

00:14:21,630 --> 00:14:25,620
you know essentially what they were

00:14:24,060 --> 00:14:27,899
saying is like we need more money for

00:14:25,620 --> 00:14:29,820
longer periods of time to do more of the

00:14:27,899 --> 00:14:32,970
same thing we're doing which doesn't

00:14:29,820 --> 00:14:34,350
quite sort of you know I get why they're

00:14:32,970 --> 00:14:36,420
saying that but it doesn't quite give us

00:14:34,350 --> 00:14:37,980
anything new to work with and kind of

00:14:36,420 --> 00:14:39,990
pushing a little more what we got to

00:14:37,980 --> 00:14:41,790
sort of this definition of of what makes

00:14:39,990 --> 00:14:44,040
a good data set is infrastructure when

00:14:41,790 --> 00:14:47,160
it's ubiquitous it's not bespoke so it's

00:14:44,040 --> 00:14:49,380
not for one sort of one sort of use case

00:14:47,160 --> 00:14:51,480
or things one sort of geographic use

00:14:49,380 --> 00:14:54,000
case it's across all a bunch it's for

00:14:51,480 --> 00:14:55,500
community so thinking about image net

00:14:54,000 --> 00:14:57,360
like image net was for the computer

00:14:55,500 --> 00:14:58,829
vision community and that's why I sort

00:14:57,360 --> 00:15:00,000
of there's a lot of uptake right because

00:14:58,829 --> 00:15:01,980
they they knew their community they

00:15:00,000 --> 00:15:04,440
spoke to it it's not for just one team

00:15:01,980 --> 00:15:06,270
within the computer vision community and

00:15:04,440 --> 00:15:08,010
then other signs are thinking about it

00:15:06,270 --> 00:15:10,410
is a separate budget not a budget line

00:15:08,010 --> 00:15:11,970
and so this is sort of little bit me

00:15:10,410 --> 00:15:13,350
writing myself as a funder right but if

00:15:11,970 --> 00:15:15,779
I see someone with a research project

00:15:13,350 --> 00:15:18,029
come in and curation is just one budget

00:15:15,779 --> 00:15:19,860
line unless they have a lot of prior

00:15:18,029 --> 00:15:21,209
work and that's for maintenance that's

00:15:19,860 --> 00:15:22,260
not going to build an effective data set

00:15:21,209 --> 00:15:24,000
right we need to be thinking about

00:15:22,260 --> 00:15:25,380
separating out that budget and saying we

00:15:24,000 --> 00:15:27,510
really care about this domain an area

00:15:25,380 --> 00:15:29,279
let's fund the data set ahead of time

00:15:27,510 --> 00:15:30,959
and then work with a set of researchers

00:15:29,279 --> 00:15:32,380
downstream to maintain and kind of keep

00:15:30,959 --> 00:15:34,210
that going forward

00:15:32,380 --> 00:15:36,010
and then finally major contributions

00:15:34,210 --> 00:15:37,270
there's one really attractive ass vet

00:15:36,010 --> 00:15:39,700
about machine learning data sets is that

00:15:37,270 --> 00:15:42,040
we can actually see how much new data

00:15:39,700 --> 00:15:43,720
improves our performance and so creating

00:15:42,040 --> 00:15:46,270
economies are out that rewarding people

00:15:43,720 --> 00:15:47,410
tracking usage and making sure that

00:15:46,270 --> 00:15:49,780
people are being compensate on those

00:15:47,410 --> 00:15:52,570
basis is a huge part of what makes this

00:15:49,780 --> 00:15:54,790
infrastructure not a research project so

00:15:52,570 --> 00:15:56,410
with that I'll close up I would love to

00:15:54,790 --> 00:15:58,060
talk to anyone here who is building

00:15:56,410 --> 00:15:59,860
datasets who's thought about this

00:15:58,060 --> 00:16:02,020
specifically anyone who's thought about

00:15:59,860 --> 00:16:03,460
like how do these datasets differ from

00:16:02,020 --> 00:16:05,170
open source software communities how

00:16:03,460 --> 00:16:07,270
they differ from just simply the open

00:16:05,170 --> 00:16:09,730
data community because there isn't

00:16:07,270 --> 00:16:11,470
there's a lot of alignment but there's

00:16:09,730 --> 00:16:12,880
little clarity about what lessons kind

00:16:11,470 --> 00:16:15,740
of cross over so if anyone here

00:16:12,880 --> 00:16:18,369
let's talk but I love to thank you

00:16:15,740 --> 00:16:18,369

YouTube URL: https://www.youtube.com/watch?v=ZbbM5KHl0_s


