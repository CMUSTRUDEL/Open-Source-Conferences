Title: Frictionless Data Processing in the Wild - Amber D. York
Publication date: 2019-06-20
Playlist: CSVConf 2019
Description: 
	Frictionless Data (FD) initiatives out of Open Knowledge International provide attractive informatics and processing capabilities. The BCO-DMO data repository used FD tools on real-world datasets, and we have some lessons learned to share. By building upon existing FD tools, we found ways to reduce the amount of time data managers spend generating metadata, and writing custom scripts. We are also developing ways for data managers with varying levels of scripting ability to make use of Frictionless Data tools.

Talk page: https://csvconf.com/speakers/#amber-d-york
Slides: https://zenodo.org/record/2687557#.XNRWMi-ZNQI
Captions: 
	00:00:01,780 --> 00:00:08,839
thankfully yeah yeah oceanography that's

00:00:07,639 --> 00:00:11,959
right

00:00:08,839 --> 00:00:13,759
welcome CSV conf I'm very excited to be

00:00:11,959 --> 00:00:15,679
here we had some great conversations the

00:00:13,759 --> 00:00:18,500
Lightning talks yesterday and I'd love

00:00:15,679 --> 00:00:20,330
to continue that here today I'm gonna be

00:00:18,500 --> 00:00:23,540
talking about frictionless data

00:00:20,330 --> 00:00:26,540
processing in the wild and to better

00:00:23,540 --> 00:00:28,910
understand where we encounter wild data

00:00:26,540 --> 00:00:32,059
I need to first explain a little bit

00:00:28,910 --> 00:00:34,539
about who we are we are Beco demo which

00:00:32,059 --> 00:00:37,039
stands for the biological and chemical

00:00:34,539 --> 00:00:39,019
oceanography data management office and

00:00:37,039 --> 00:00:40,309
we're part of the Woods Hole

00:00:39,019 --> 00:00:42,109
Oceanographic Institution

00:00:40,309 --> 00:00:44,989
but we're primarily funded by the

00:00:42,109 --> 00:00:47,780
National Science Foundation scientists

00:00:44,989 --> 00:00:50,239
primarily NSF scientists submit data to

00:00:47,780 --> 00:00:53,929
us and we provide data access to the

00:00:50,239 --> 00:00:58,309
public and here is where we first

00:00:53,929 --> 00:00:59,960
encounter the wild data when it moves

00:00:58,309 --> 00:01:03,559
from its native habitat from the

00:00:59,960 --> 00:01:05,239
scientist to us the keen-eyed among you

00:01:03,559 --> 00:01:09,049
may already notice some things about

00:01:05,239 --> 00:01:12,950
these data such as merge cells and date

00:01:09,049 --> 00:01:15,439
ranges in a Cell multiple data tables

00:01:12,950 --> 00:01:18,229
that can compete combined as one should

00:01:15,439 --> 00:01:20,240
be combined as one table some special

00:01:18,229 --> 00:01:24,619
proprietary codes that aren't

00:01:20,240 --> 00:01:26,869
necessarily clear so we work really

00:01:24,619 --> 00:01:30,350
closely with the scientists who collect

00:01:26,869 --> 00:01:34,310
and submit the data to us to get it into

00:01:30,350 --> 00:01:38,590
a format that is conducive to reusing

00:01:34,310 --> 00:01:41,479
the data interoperable formats so

00:01:38,590 --> 00:01:43,759
there's a lot of reasons that we receive

00:01:41,479 --> 00:01:45,979
the data the way we do we ask that it

00:01:43,759 --> 00:01:47,899
all be fully processed in quality

00:01:45,979 --> 00:01:49,729
assessment controlled that doesn't

00:01:47,899 --> 00:01:52,490
always happen or at least to the extent

00:01:49,729 --> 00:01:54,829
that we need so again that's a really

00:01:52,490 --> 00:01:58,539
close conversation and working with the

00:01:54,829 --> 00:01:58,539
scientist to get into good shape

00:01:58,570 --> 00:02:04,369
scientists are really focused on the

00:02:02,090 --> 00:02:06,469
collection and analysis of their data

00:02:04,369 --> 00:02:08,090
and publishing but they don't always

00:02:06,469 --> 00:02:10,159
think about everything that needs to

00:02:08,090 --> 00:02:11,599
happen to it to make it reusable so

00:02:10,159 --> 00:02:12,990
that's where we help them with those

00:02:11,599 --> 00:02:17,520
pain points

00:02:12,990 --> 00:02:20,880
so we have interoperable formats and we

00:02:17,520 --> 00:02:22,590
present the metadata in a way that's

00:02:20,880 --> 00:02:27,240
easily understood on a dataset landing

00:02:22,590 --> 00:02:31,050
page along with the data access and we

00:02:27,240 --> 00:02:39,090
also provide some other formats of the

00:02:31,050 --> 00:02:42,470
metadata like ISO JSON XML so why do we

00:02:39,090 --> 00:02:46,020
process data and it's to be fair it's a

00:02:42,470 --> 00:02:47,880
principles that we really think are very

00:02:46,020 --> 00:02:50,760
important to make sure the data is

00:02:47,880 --> 00:02:52,710
findable accessible interoperable and

00:02:50,760 --> 00:02:54,120
reusable I'm introducing this slide

00:02:52,710 --> 00:02:55,560
early because I'm going to touch on all

00:02:54,120 --> 00:02:57,390
these principles during my presentation

00:02:55,560 --> 00:02:59,340
because they're really the driving

00:02:57,390 --> 00:03:04,680
factors behind why we do a lot of what

00:02:59,340 --> 00:03:07,710
we do so when we're processing what do

00:03:04,680 --> 00:03:09,540
we need to do to be fair so a lot of it

00:03:07,710 --> 00:03:13,170
comes down to making sure the data have

00:03:09,540 --> 00:03:15,840
certain things in them such as spatial

00:03:13,170 --> 00:03:18,300
temporal context so when we get the data

00:03:15,840 --> 00:03:21,390
we make sure that it can correctly be

00:03:18,300 --> 00:03:23,250
placed in space and time and has the

00:03:21,390 --> 00:03:28,230
context of where it fits in on a global

00:03:23,250 --> 00:03:29,820
scale and we also a lot of the things we

00:03:28,230 --> 00:03:32,640
do during data processing or to correct

00:03:29,820 --> 00:03:35,010
quality issues such as inconsistent

00:03:32,640 --> 00:03:38,850
formatting like various date/time

00:03:35,010 --> 00:03:41,700
formats within a column corrupt data

00:03:38,850 --> 00:03:44,750
characters data gaps invalid species

00:03:41,700 --> 00:03:48,780
names and sometimes there's just typos

00:03:44,750 --> 00:03:51,510
and again big emphasis on reformatting

00:03:48,780 --> 00:03:54,240
for usability and also making sure it's

00:03:51,510 --> 00:03:58,590
not in a proprietary format so anyone

00:03:54,240 --> 00:03:59,790
can have access and use the data so back

00:03:58,590 --> 00:04:03,390
to the friction list part what is

00:03:59,790 --> 00:04:04,620
frictionless data and that's an

00:04:03,390 --> 00:04:07,080
initiative out of Open Knowledge

00:04:04,620 --> 00:04:08,340
Foundation open knowledge people raise

00:04:07,080 --> 00:04:12,960
your hands there's a few of them in the

00:04:08,340 --> 00:04:15,420
crowd so round of applause so if you

00:04:12,960 --> 00:04:18,030
need any parking specific questions you

00:04:15,420 --> 00:04:19,170
should talk to that but it's a

00:04:18,030 --> 00:04:21,870
collection of software and

00:04:19,170 --> 00:04:24,639
specifications to allow publication

00:04:21,870 --> 00:04:26,080
transport and consumption of data

00:04:24,639 --> 00:04:29,680
and there's big emphasis on being

00:04:26,080 --> 00:04:33,699
platform-agnostic for interoperability

00:04:29,680 --> 00:04:37,419
and also big emphasis on reducing the

00:04:33,699 --> 00:04:42,129
time to research so time from data to

00:04:37,419 --> 00:04:44,050
insight there's a lot of different

00:04:42,129 --> 00:04:46,479
components to frictionless data I'm just

00:04:44,050 --> 00:04:48,129
gonna be talking about one specific

00:04:46,479 --> 00:04:50,789
component that has to do with data

00:04:48,129 --> 00:04:54,099
processing called data package pipelines

00:04:50,789 --> 00:04:56,409
and why were we so attracted to data

00:04:54,099 --> 00:04:59,229
package pipelines it for a few reasons

00:04:56,409 --> 00:05:02,009
one is it allowed us the ability to

00:04:59,229 --> 00:05:05,319
standardize our data processing steps

00:05:02,009 --> 00:05:08,199
when we do operations like joins modify

00:05:05,319 --> 00:05:10,990
the values in a column columns add

00:05:08,199 --> 00:05:12,669
removes any number of things but it's a

00:05:10,990 --> 00:05:15,580
way to track those changes in a more

00:05:12,669 --> 00:05:17,529
structured way and actually the

00:05:15,580 --> 00:05:19,659
provenance is generated automatically as

00:05:17,529 --> 00:05:22,270
we build these pipelines on the right

00:05:19,659 --> 00:05:26,169
you see an example of the pipeline spec

00:05:22,270 --> 00:05:28,810
ml and that's the actual encoding of the

00:05:26,169 --> 00:05:31,719
processing pipeline so there's different

00:05:28,810 --> 00:05:35,469
steps processing steps that are within a

00:05:31,719 --> 00:05:37,779
processing workflow pipeline and again

00:05:35,469 --> 00:05:40,120
this is extremely attractive to us

00:05:37,779 --> 00:05:42,190
because it also allows very easy

00:05:40,120 --> 00:05:45,330
reproducibility you can run the

00:05:42,190 --> 00:05:49,240
pipeline's as often as you want it it

00:05:45,330 --> 00:05:53,289
includes provenance information to be

00:05:49,240 --> 00:05:55,779
able to fully reproduce and go from the

00:05:53,289 --> 00:05:58,770
input output and you can build on

00:05:55,779 --> 00:06:02,560
pipelines and modify it as you need to

00:05:58,770 --> 00:06:04,569
and one other key thing was de package

00:06:02,560 --> 00:06:07,180
pipelines allowed us the flexibility to

00:06:04,569 --> 00:06:09,250
add some custom processors to do

00:06:07,180 --> 00:06:11,319
specific tasks that we needed to do in

00:06:09,250 --> 00:06:14,339
our data management office and we did

00:06:11,319 --> 00:06:19,120
that writing custom processors in Python

00:06:14,339 --> 00:06:21,430
so we made a web application that is a

00:06:19,120 --> 00:06:25,210
user interface on top of the pipeline's

00:06:21,430 --> 00:06:27,969
so it's a react app that communicates

00:06:25,210 --> 00:06:29,500
with a server running a Python flask app

00:06:27,969 --> 00:06:32,289
which actually executes the pipe

00:06:29,500 --> 00:06:35,050
pipelines that are built and returns the

00:06:32,289 --> 00:06:37,600
data so you'll see here on the left is

00:06:35,050 --> 00:06:39,550
an example of a workflow that's a

00:06:37,600 --> 00:06:42,970
pipeline with the individual processing

00:06:39,550 --> 00:06:46,330
steps and called out is just one of the

00:06:42,970 --> 00:06:50,770
steps and the configuration is shown for

00:06:46,330 --> 00:06:53,200
find and replace and at the bottom you

00:06:50,770 --> 00:06:55,330
see this is how actually that step looks

00:06:53,200 --> 00:07:01,780
in the pipeline specification yeah Mille

00:06:55,330 --> 00:07:03,220
file so why did we feel the need to make

00:07:01,780 --> 00:07:05,770
this user interface and write our own

00:07:03,220 --> 00:07:07,930
custom processors and build upon data

00:07:05,770 --> 00:07:11,200
package pipelines one of the reasons is

00:07:07,930 --> 00:07:14,950
we wanted to give our data managers a

00:07:11,200 --> 00:07:16,390
more immersive experience so when you're

00:07:14,950 --> 00:07:18,190
constructing the pipeline's it's not an

00:07:16,390 --> 00:07:20,470
isolation that you actually are able to

00:07:18,190 --> 00:07:22,150
visualize the pipeline as you're

00:07:20,470 --> 00:07:23,970
constructing it and see how the changes

00:07:22,150 --> 00:07:28,540
you're making affect the final output

00:07:23,970 --> 00:07:29,890
and also to calculate statistics that's

00:07:28,540 --> 00:07:31,330
very important for us for looking at

00:07:29,890 --> 00:07:33,310
some different quality control metrics

00:07:31,330 --> 00:07:34,660
as we're making sure we don't mess

00:07:33,310 --> 00:07:39,100
anything up when we're processing the

00:07:34,660 --> 00:07:41,770
data a key area of why we wanted to make

00:07:39,100 --> 00:07:44,260
the user interface was to reduce the

00:07:41,770 --> 00:07:47,440
time it took to to process the data sets

00:07:44,260 --> 00:07:49,030
so we wanted to avoid handwriting either

00:07:47,440 --> 00:07:52,030
the pipeline specs and running on the

00:07:49,030 --> 00:07:54,160
mung command line or doing custom

00:07:52,030 --> 00:07:57,010
scripts in python using using the

00:07:54,160 --> 00:07:58,960
pipelines so this because we've written

00:07:57,010 --> 00:08:02,290
custom processors we're able to

00:07:58,960 --> 00:08:05,230
integrate those it easier into

00:08:02,290 --> 00:08:07,540
provenance capture in the steps and it

00:08:05,230 --> 00:08:08,620
reduces they set processing time because

00:08:07,540 --> 00:08:12,370
we don't have to handwrite them every

00:08:08,620 --> 00:08:14,410
time also reducing time due to syntax

00:08:12,370 --> 00:08:16,210
errors just little mistakes that can

00:08:14,410 --> 00:08:18,360
happen to anyone but when you have a

00:08:16,210 --> 00:08:25,650
user interface you can avoid those and

00:08:18,360 --> 00:08:29,230
then reducing repetitive tasks so the

00:08:25,650 --> 00:08:32,320
another key area of why we built our

00:08:29,230 --> 00:08:34,390
building this tool is because we wanted

00:08:32,320 --> 00:08:36,880
to improve our provenance capture by

00:08:34,390 --> 00:08:39,490
adding custom metadata and you'll see

00:08:36,880 --> 00:08:42,310
later on that we have a way of capturing

00:08:39,490 --> 00:08:46,040
both the test statistics and specific

00:08:42,310 --> 00:08:49,370
processing notes along with each step

00:08:46,040 --> 00:08:51,560
we wanted to remove one barrier that was

00:08:49,370 --> 00:08:56,080
programming ability so we wanted to open

00:08:51,560 --> 00:08:58,310
up the data package pipeline creation to

00:08:56,080 --> 00:09:05,650
users that didn't necessarily have

00:08:58,310 --> 00:09:09,529
weren't Python whizzes or had as much

00:09:05,650 --> 00:09:11,750
facility with the command line and then

00:09:09,529 --> 00:09:14,839
the last key area of why we wanted to

00:09:11,750 --> 00:09:16,700
build on data package pipelines was as I

00:09:14,839 --> 00:09:19,970
mentioned to include custom processors

00:09:16,700 --> 00:09:22,850
so you'll see in this pipeline here on

00:09:19,970 --> 00:09:24,650
the left there are many steps some of

00:09:22,850 --> 00:09:26,360
them like Find and Replace are right out

00:09:24,650 --> 00:09:31,010
of the box data package pipelines

00:09:26,360 --> 00:09:32,900
processors but we also in our tool added

00:09:31,010 --> 00:09:34,640
a lot of the custom processors to do

00:09:32,900 --> 00:09:37,160
specific things such as convert

00:09:34,640 --> 00:09:39,080
date/time formats to and deal with

00:09:37,160 --> 00:09:44,810
different times on issues that occur

00:09:39,080 --> 00:09:47,029
which is a lot of the time and in things

00:09:44,810 --> 00:09:48,800
like converting to decimal degrees and

00:09:47,029 --> 00:09:51,350
just things we just do commonly though

00:09:48,800 --> 00:09:55,730
so we have our custom processors that we

00:09:51,350 --> 00:09:58,940
have in our pocket for use this is an

00:09:55,730 --> 00:10:00,950
example of building a pipeline so out

00:09:58,940 --> 00:10:04,370
here I'm starting with just a load step

00:10:00,950 --> 00:10:07,190
and I'm looking at a data table this

00:10:04,370 --> 00:10:09,170
data set this is a good example of our

00:10:07,190 --> 00:10:11,180
the level of communication we have with

00:10:09,170 --> 00:10:14,240
the scientists and how that helps our

00:10:11,180 --> 00:10:17,720
final end product so this data set came

00:10:14,240 --> 00:10:20,450
with date and time timezone that was a

00:10:17,720 --> 00:10:25,370
question so we we found out that it is a

00:10:20,450 --> 00:10:29,630
local time and what you do see offset it

00:10:25,370 --> 00:10:32,209
was so we were able to at the end of

00:10:29,630 --> 00:10:38,450
this pipeline also add a date time in

00:10:32,209 --> 00:10:39,890
ISO stamp a t6 l1 in UTC time so here

00:10:38,450 --> 00:10:44,260
I'm adding a second step called round

00:10:39,890 --> 00:10:47,060
fields you see there's the difference

00:10:44,260 --> 00:10:48,440
and again the visualization just helps

00:10:47,060 --> 00:10:52,700
us ensure that we're doing the right

00:10:48,440 --> 00:10:54,350
thing and everything is working properly

00:10:52,700 --> 00:10:57,399
so this is an example of how we can

00:10:54,350 --> 00:10:59,899
figure that step in the time-saving just

00:10:57,399 --> 00:11:01,670
benefits of user interface in general

00:10:59,899 --> 00:11:03,890
where you can select multiple fields and

00:11:01,670 --> 00:11:08,260
apply around into multiple fields at

00:11:03,890 --> 00:11:10,700
once those kind of things and then I

00:11:08,260 --> 00:11:12,200
have a lot of different steps here that

00:11:10,700 --> 00:11:15,410
we had to do to get this data in shape

00:11:12,200 --> 00:11:18,200
but at the end of end of the pipeline so

00:11:15,410 --> 00:11:22,100
here we have our date time stamp in UTC

00:11:18,200 --> 00:11:23,899
we preserve the date and time local in

00:11:22,100 --> 00:11:26,540
in this case because the scientist

00:11:23,899 --> 00:11:29,120
that's how they like to analyze their

00:11:26,540 --> 00:11:31,130
data and again we want to reduce the

00:11:29,120 --> 00:11:32,839
time to research so they don't have to

00:11:31,130 --> 00:11:35,839
back calculate their local time again if

00:11:32,839 --> 00:11:38,089
that's how they like it but we do want

00:11:35,839 --> 00:11:40,550
to still be able to place this in global

00:11:38,089 --> 00:11:42,220
space in time for reusability so we

00:11:40,550 --> 00:11:47,120
compromise by having both in this case

00:11:42,220 --> 00:11:48,769
and you'll see there's also lat LAN here

00:11:47,120 --> 00:11:50,060
now that was in the metadata for this

00:11:48,769 --> 00:11:52,790
submission it wasn't actually in the

00:11:50,060 --> 00:11:58,750
data but for use again we added that as

00:11:52,790 --> 00:12:03,470
columns this is another example of

00:11:58,750 --> 00:12:06,380
configuration of a pipeline so you'll

00:12:03,470 --> 00:12:08,810
see this is to set data types the user

00:12:06,380 --> 00:12:13,360
selects a resource quote/unquote which

00:12:08,810 --> 00:12:16,579
is a tabular data set and the set types

00:12:13,360 --> 00:12:18,680
processor guesses the type it infers it

00:12:16,579 --> 00:12:21,320
and then the user has the ability to

00:12:18,680 --> 00:12:24,320
either override that type or supply

00:12:21,320 --> 00:12:25,910
formats as needed and again on the left

00:12:24,320 --> 00:12:26,959
bottom you'll see this is actually how

00:12:25,910 --> 00:12:29,660
it's encoded in the pipeline

00:12:26,959 --> 00:12:31,040
specification and that's a huge

00:12:29,660 --> 00:12:32,930
time-saver if you don't have to write

00:12:31,040 --> 00:12:37,160
that out and we just click some buttons

00:12:32,930 --> 00:12:39,740
and override the inferred type so back

00:12:37,160 --> 00:12:43,040
to full pipeline at the end when it's

00:12:39,740 --> 00:12:44,930
run and you can actually run it at any

00:12:43,040 --> 00:12:49,640
individual step to see the state at that

00:12:44,930 --> 00:12:53,240
step but at the end you get the ability

00:12:49,640 --> 00:12:55,820
to download the output three different

00:12:53,240 --> 00:12:57,410
critical output files so pipeline spec

00:12:55,820 --> 00:13:01,160
EMA which I've mentioned is the encoding

00:12:57,410 --> 00:13:02,930
of the pipeline data package JSON is the

00:13:01,160 --> 00:13:06,020
full description of the data itself

00:13:02,930 --> 00:13:07,550
including all the types we've specified

00:13:06,020 --> 00:13:11,029
the source and other provenance

00:13:07,550 --> 00:13:12,740
information and finally the CTD which

00:13:11,029 --> 00:13:17,300
stands for conductivity temperature and

00:13:12,740 --> 00:13:20,450
depth in this case CSV file which is the

00:13:17,300 --> 00:13:23,180
result of the whole pipeline so there's

00:13:20,450 --> 00:13:26,060
the yeah mole the description of the

00:13:23,180 --> 00:13:30,560
data the data package JSON and the

00:13:26,060 --> 00:13:32,570
actual data file itself and back going

00:13:30,560 --> 00:13:36,410
back to fair which is our driver for all

00:13:32,570 --> 00:13:38,120
of this really the pipelines is helping

00:13:36,410 --> 00:13:41,709
with all the different components of

00:13:38,120 --> 00:13:41,709
fare them to define durability

00:13:43,000 --> 00:13:49,550
accessibility interoperability and

00:13:45,320 --> 00:13:52,580
reproducibility but specifically I

00:13:49,550 --> 00:13:55,790
wanted to highlight the pipelines in our

00:13:52,580 --> 00:13:57,560
in helping us with reproducibility again

00:13:55,790 --> 00:13:58,730
as I mentioned you can rerun the the

00:13:57,560 --> 00:14:02,320
pipelines and all the province

00:13:58,730 --> 00:14:04,850
provenance is is tracked with that an

00:14:02,320 --> 00:14:08,600
interoperability highlighting the data

00:14:04,850 --> 00:14:13,520
package JSON which provides the

00:14:08,600 --> 00:14:15,380
description of the data and a key

00:14:13,520 --> 00:14:18,830
component that's going to help us with

00:14:15,380 --> 00:14:21,380
the findability and interoperability are

00:14:18,830 --> 00:14:23,450
our statistics calculation so we have a

00:14:21,380 --> 00:14:26,330
package that we've integrated into our

00:14:23,450 --> 00:14:28,339
web application that that allows us to

00:14:26,330 --> 00:14:31,370
calculate basic statistics and also some

00:14:28,339 --> 00:14:33,220
other specific things like you list of

00:14:31,370 --> 00:14:35,900
you can make values and things like that

00:14:33,220 --> 00:14:38,480
and actually add that into the data

00:14:35,900 --> 00:14:43,100
package JSON and we hope to be able to

00:14:38,480 --> 00:14:46,250
use that in data discoverability so you

00:14:43,100 --> 00:14:48,529
know capturing date/time ranges and in

00:14:46,250 --> 00:14:50,589
spatial bounding boxes and things like

00:14:48,529 --> 00:14:50,589
that

00:14:51,620 --> 00:14:55,670
so this is a on the left of the

00:14:53,480 --> 00:14:58,460
screenshot of our current dataset

00:14:55,670 --> 00:15:02,180
landing page and already as I mentioned

00:14:58,460 --> 00:15:07,760
we produce data in metadata in various

00:15:02,180 --> 00:15:09,500
formats that are consumable but if we

00:15:07,760 --> 00:15:13,130
plan to integrate these frictionless

00:15:09,500 --> 00:15:16,820
tools and outputs to better support the

00:15:13,130 --> 00:15:22,520
fare principles specifically the

00:15:16,820 --> 00:15:24,860
reproducibility and interoperability so

00:15:22,520 --> 00:15:27,260
where we're going with all this we plan

00:15:24,860 --> 00:15:29,870
to release an open-source community

00:15:27,260 --> 00:15:32,029
version R of our web application the

00:15:29,870 --> 00:15:36,140
user interface we have now is a

00:15:32,029 --> 00:15:38,180
prototype and we're developing it and a

00:15:36,140 --> 00:15:41,690
lot we plan to also release our custom

00:15:38,180 --> 00:15:43,990
processors and statistics calculator we

00:15:41,690 --> 00:15:47,270
want to be able to integrate the

00:15:43,990 --> 00:15:49,420
pipelines with data so the users can

00:15:47,270 --> 00:15:52,820
access it from our data management

00:15:49,420 --> 00:15:55,250
landing pages and they can either run

00:15:52,820 --> 00:15:58,700
pipelines on their own or build upon

00:15:55,250 --> 00:16:00,290
existing pipelines we've created another

00:15:58,700 --> 00:16:02,959
component of the frictionless universe

00:16:00,290 --> 00:16:05,690
that is very attractive to us that we

00:16:02,959 --> 00:16:08,930
plan to explore further is validation of

00:16:05,690 --> 00:16:11,779
the data and qa/qc using a component of

00:16:08,930 --> 00:16:13,910
frictionless called good tables and we

00:16:11,779 --> 00:16:17,750
have been working very closely with open

00:16:13,910 --> 00:16:20,360
knowledge people and providing feedback

00:16:17,750 --> 00:16:23,150
on how the tools are working for us and

00:16:20,360 --> 00:16:25,550
areas we think that it can be enhanced

00:16:23,150 --> 00:16:27,350
to better serve the needs of science so

00:16:25,550 --> 00:16:29,330
we've had a good communication on github

00:16:27,350 --> 00:16:31,550
and conference calls and it's good to

00:16:29,330 --> 00:16:34,880
see people in paces in person after

00:16:31,550 --> 00:16:37,100
talking with them digitally but yeah so

00:16:34,880 --> 00:16:39,709
we're actually have already started

00:16:37,100 --> 00:16:41,720
doing pull requests back to did package

00:16:39,709 --> 00:16:45,490
pipelines that vinick's merged so that's

00:16:41,720 --> 00:16:48,589
a success for us and yeah it's also

00:16:45,490 --> 00:16:52,370
better support good science so that's

00:16:48,589 --> 00:16:54,030
our primary objective and of course I'm

00:16:52,370 --> 00:16:56,610
going to end

00:16:54,030 --> 00:16:58,350
who the message the practicality of the

00:16:56,610 --> 00:17:04,130
scientists see it's not always the first

00:16:58,350 --> 00:17:08,040
thing they think of but hopefully we can

00:17:04,130 --> 00:17:10,010
educate scientists through this process

00:17:08,040 --> 00:17:13,470
so the next time they go collect data

00:17:10,010 --> 00:17:15,589
they'll think about that next time thank

00:17:13,470 --> 00:17:15,589

YouTube URL: https://www.youtube.com/watch?v=6tUbpxSb_og


