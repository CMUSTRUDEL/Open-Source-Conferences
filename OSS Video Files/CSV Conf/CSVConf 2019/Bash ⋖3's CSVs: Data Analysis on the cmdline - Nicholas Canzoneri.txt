Title: Bash â‹–3's CSVs: Data Analysis on the cmdline - Nicholas Canzoneri
Publication date: 2019-06-21
Playlist: CSVConf 2019
Description: 
	Your bash shell has a _lot_ utilities that can be used to help you analyze your data, often easier and faster than trying to import your data to an external tool. But these utilities can be hard to find and even harder to figure out the right options. I'll walkthrough a data set and show examples of the best utility to use in different situations. I'll go over common commands like `grep` and `cut`, more exotic commands like `comm` and `tr`, and dig up very useful options to a command you might have overlooked, like `sort -k`.

Talk page: https://csvconf.com/speakers/#nicholas-canzoneri

Slides: https://github.com/nickcanz/csvconf2019/blob/master/csvconf_2019.pdf
Captions: 
	00:00:05,470 --> 00:00:09,980
every one yeah thank you for coming yeah

00:00:08,690 --> 00:00:12,889
so yeah we'll be talking about bashing

00:00:09,980 --> 00:00:15,500
sissies and kind of command-line data

00:00:12,889 --> 00:00:18,140
analysis in general yeah so Who am I

00:00:15,500 --> 00:00:21,040
yeah I work at a database infrastructure

00:00:18,140 --> 00:00:24,470
and github here's my social media stuff

00:00:21,040 --> 00:00:26,539
but relevant to this conference my first

00:00:24,470 --> 00:00:28,010
job at a school was at this place the

00:00:26,539 --> 00:00:31,430
Delaware Valley Regional Planning

00:00:28,010 --> 00:00:33,890
Commission they do a lot of urban

00:00:31,430 --> 00:00:36,470
suburban rural planning for the Greater

00:00:33,890 --> 00:00:39,350
Philadelphia region and I think I was

00:00:36,470 --> 00:00:42,350
like one of two developers there at like

00:00:39,350 --> 00:00:44,930
a hundred person organization and I

00:00:42,350 --> 00:00:48,890
think it really sparked my interest in

00:00:44,930 --> 00:00:51,980
open data and GIS info and especially

00:00:48,890 --> 00:00:53,469
you know give you the feeling how having

00:00:51,980 --> 00:00:56,690
like a little bit of programming

00:00:53,469 --> 00:00:59,300
experience can really go a long way into

00:00:56,690 --> 00:01:02,480
helping like reduce kind of manual toil

00:00:59,300 --> 00:01:03,860
and things like that so this talk is

00:01:02,480 --> 00:01:06,109
about using the right tool for the job

00:01:03,860 --> 00:01:10,039
Bosch is not gonna be great for

00:01:06,109 --> 00:01:13,459
everything but in certain cases I think

00:01:10,039 --> 00:01:15,380
you can get a pretty good first pass on

00:01:13,459 --> 00:01:19,789
your data or logs without having to rely

00:01:15,380 --> 00:01:22,639
on external tooling so why do you use

00:01:19,789 --> 00:01:23,929
Bosch in certain cases it can be pretty

00:01:22,639 --> 00:01:26,719
useful this article is a little bit

00:01:23,929 --> 00:01:28,880
tongue-in-cheek how you know command

00:01:26,719 --> 00:01:31,279
line tools are 235 times faster than

00:01:28,880 --> 00:01:32,779
your Hadoop cluster but I think it

00:01:31,279 --> 00:01:33,560
brings up an interesting use case where

00:01:32,779 --> 00:01:36,529
you know when you're in the

00:01:33,560 --> 00:01:39,499
multi-gigabyte range you know opening

00:01:36,529 --> 00:01:43,159
two gigabytes in Excel or Google sheets

00:01:39,499 --> 00:01:44,630
is not going to be a fun time and it's

00:01:43,159 --> 00:01:48,259
really not enough to really make use of

00:01:44,630 --> 00:01:51,950
you know big data tools like Hadoop but

00:01:48,259 --> 00:01:54,109
so what are you left with you can import

00:01:51,950 --> 00:01:55,340
into a database like my sequel or

00:01:54,109 --> 00:01:57,020
something like that but that could be

00:01:55,340 --> 00:01:59,659
you know a bit of a hassle you know I

00:01:57,020 --> 00:02:01,399
have one up and running already or you

00:01:59,659 --> 00:02:04,310
can do a pass with your command-line

00:02:01,399 --> 00:02:07,429
tools so when it's matched a good choice

00:02:04,310 --> 00:02:08,989
to use when your data is line based all

00:02:07,429 --> 00:02:11,300
the core utilities understand lines

00:02:08,989 --> 00:02:13,930
really well if you have multi-line data

00:02:11,300 --> 00:02:15,430
like a stack trace or something the

00:02:13,930 --> 00:02:19,090
probably bash is not going to be a good

00:02:15,430 --> 00:02:21,010
idea next is when you have a consistent

00:02:19,090 --> 00:02:22,870
structure CSV is are a great example of

00:02:21,010 --> 00:02:25,390
that I would also recommend it for

00:02:22,870 --> 00:02:27,069
things like server logs but you have

00:02:25,390 --> 00:02:29,409
something like a hard structure like a

00:02:27,069 --> 00:02:31,659
JSON or an XML that would probably be

00:02:29,409 --> 00:02:35,469
easier to use like a tool specifically

00:02:31,659 --> 00:02:37,299
for that first step in any kind of data

00:02:35,469 --> 00:02:39,819
analysis is to peek at your data so

00:02:37,299 --> 00:02:41,560
you're looking with us so ahead is

00:02:39,819 --> 00:02:45,370
utility that will show you the first n

00:02:41,560 --> 00:02:46,870
lines of the file if you're more

00:02:45,370 --> 00:02:50,500
familiar with sequel it's kind of like a

00:02:46,870 --> 00:02:52,450
select star limit two tailed is the

00:02:50,500 --> 00:02:54,730
opposite of that it'll show you the last

00:02:52,450 --> 00:02:59,590
end lines of your file so it's like a

00:02:54,730 --> 00:03:01,480
order by descending limit to WC stands

00:02:59,590 --> 00:03:06,579
for word count but here we're gonna pass

00:03:01,480 --> 00:03:08,500
the L option to count lines WC can count

00:03:06,579 --> 00:03:11,650
a lot of different things lines

00:03:08,500 --> 00:03:13,450
characters words probably characters and

00:03:11,650 --> 00:03:16,750
words are not the most relevant for our

00:03:13,450 --> 00:03:18,010
CSV file but yeah if you're when I write

00:03:16,750 --> 00:03:19,209
out a tweet ahead of time to make sure

00:03:18,010 --> 00:03:21,159
you're under the limit that can be

00:03:19,209 --> 00:03:24,040
useful

00:03:21,159 --> 00:03:26,379
so to recap from peaking at our

00:03:24,040 --> 00:03:30,220
population CSV file we have a three

00:03:26,379 --> 00:03:32,049
column CSV city state population looks

00:03:30,220 --> 00:03:34,379
like big cities at the top of the file

00:03:32,049 --> 00:03:39,639
smaller cities at the bottom of the file

00:03:34,379 --> 00:03:41,500
761 cities in total so let's say you

00:03:39,639 --> 00:03:42,729
have your file let's say you don't want

00:03:41,500 --> 00:03:44,979
all the lines in it you want you're

00:03:42,729 --> 00:03:46,629
looking at specific lines grep is going

00:03:44,979 --> 00:03:49,540
to be your tool for that

00:03:46,629 --> 00:03:51,459
so that'll search for a string and

00:03:49,540 --> 00:03:56,769
return all the lines that match your

00:03:51,459 --> 00:03:58,359
string you can use the - V flag kind of

00:03:56,769 --> 00:04:00,669
do the opposite to exclude lines that

00:03:58,359 --> 00:04:04,959
you don't want to see sequence E in this

00:04:00,669 --> 00:04:07,180
file Honolulu is it's not the population

00:04:04,959 --> 00:04:08,979
of Honolulu City it's the essence since

00:04:07,180 --> 00:04:11,979
its designated place the same thing with

00:04:08,979 --> 00:04:16,000
Anchorage and Lexington here so that's

00:04:11,979 --> 00:04:17,949
kind of interesting to see so now that

00:04:16,000 --> 00:04:19,150
we have the lines that we want let's say

00:04:17,949 --> 00:04:21,459
we don't want everything within the line

00:04:19,150 --> 00:04:25,419
we want certain things we can use cut to

00:04:21,459 --> 00:04:28,599
chop up the lines in a consistent manner

00:04:25,419 --> 00:04:31,150
so here we're getting the first and the

00:04:28,599 --> 00:04:32,409
third column so city and population

00:04:31,150 --> 00:04:35,800
let's say I don't want to I don't want

00:04:32,409 --> 00:04:38,620
to see state cut gets really interesting

00:04:35,800 --> 00:04:40,960
when you can do multiple passes with it

00:04:38,620 --> 00:04:43,449
so let's say you at the top here I have

00:04:40,960 --> 00:04:45,819
like a server log but I only want the

00:04:43,449 --> 00:04:48,310
time portion of it we can do that with

00:04:45,819 --> 00:04:50,020
two passes of cut so first we're going

00:04:48,310 --> 00:04:52,810
to chop up the line based off of space

00:04:50,020 --> 00:04:55,509
and then we can do another pass that's

00:04:52,810 --> 00:04:57,460
chopping up the line based off of the

00:04:55,509 --> 00:04:59,949
colon character so that way we can

00:04:57,460 --> 00:05:02,860
easily isolate like the exact portion of

00:04:59,949 --> 00:05:07,449
the data that we want but you know it's

00:05:02,860 --> 00:05:10,599
not an easily possible format sort is

00:05:07,449 --> 00:05:14,620
one of those utilities where it does

00:05:10,599 --> 00:05:18,039
what it named by default it works off of

00:05:14,620 --> 00:05:19,900
a whole line alphabetically but in

00:05:18,039 --> 00:05:23,319
generally general we don't want to sort

00:05:19,900 --> 00:05:26,830
on the whole line we can give it a key

00:05:23,319 --> 00:05:31,779
column to sort on so in this example

00:05:26,830 --> 00:05:33,639
we're sorting by state alphabetically it

00:05:31,779 --> 00:05:35,710
gets more interesting when we can do

00:05:33,639 --> 00:05:37,419
multiple key columns so we're able to do

00:05:35,710 --> 00:05:39,969
kind of like sort of advanced stuff on

00:05:37,419 --> 00:05:42,370
the command line here now so here we're

00:05:39,969 --> 00:05:44,949
sorting by state alphabetically and then

00:05:42,370 --> 00:05:47,080
we're sorting by population numerically

00:05:44,949 --> 00:05:49,960
that's what the end there is for so it

00:05:47,080 --> 00:05:52,120
treats it like a number not a word and

00:05:49,960 --> 00:05:54,669
then the r is for reverse so it's going

00:05:52,120 --> 00:05:58,300
to order descending so you have the

00:05:54,669 --> 00:06:00,009
biggest cities at the top now we can

00:05:58,300 --> 00:06:00,550
chain things more even more things

00:06:00,009 --> 00:06:02,289
together

00:06:00,550 --> 00:06:04,719
we can start to answer questions like

00:06:02,289 --> 00:06:06,219
what are the biggest cities by state so

00:06:04,719 --> 00:06:08,800
we take the sort command that we did in

00:06:06,219 --> 00:06:11,860
the previous slide and we can sort it

00:06:08,800 --> 00:06:14,139
again giving it a you flag for unique so

00:06:11,860 --> 00:06:16,990
what that will do and we're giving it

00:06:14,139 --> 00:06:18,759
the state column as well so it'll give

00:06:16,990 --> 00:06:21,039
us the unique output by state

00:06:18,759 --> 00:06:25,210
essentially giving us the the biggest

00:06:21,039 --> 00:06:27,250
cities by state here another question

00:06:25,210 --> 00:06:30,009
that we might have is how many cities

00:06:27,250 --> 00:06:35,169
per state yeah how many super state are

00:06:30,009 --> 00:06:38,529
there so we can get just the state

00:06:35,169 --> 00:06:43,569
column that's the second one and then we

00:06:38,529 --> 00:06:45,879
use unique - and the - see flag - give

00:06:43,569 --> 00:06:47,889
us the the count of how many times that

00:06:45,879 --> 00:06:49,959
state occurred the important thing to

00:06:47,889 --> 00:06:52,509
note here is that unique expect sorted

00:06:49,959 --> 00:06:55,149
input so try to think of you sort and

00:06:52,509 --> 00:06:56,469
unique as a pair together because

00:06:55,149 --> 00:06:58,809
otherwise that you will not get the

00:06:56,469 --> 00:07:02,139
output that you want yeah so we can see

00:06:58,809 --> 00:07:05,289
ya in this file 178 cities or California

00:07:02,139 --> 00:07:07,599
65 or Texas 25 or Michigan which is uh

00:07:05,289 --> 00:07:11,259
yeah not what I was expecting so that's

00:07:07,599 --> 00:07:13,539
cool to see another question that we can

00:07:11,259 --> 00:07:17,529
answer is where does Portland ranked so

00:07:13,539 --> 00:07:19,449
we can sort the file by population and

00:07:17,529 --> 00:07:22,419
we can look for Portland we can patch

00:07:19,449 --> 00:07:26,860
the end flag and so that'll return us

00:07:22,419 --> 00:07:28,779
the line number that this occurred on so

00:07:26,860 --> 00:07:31,239
we can see that Portland Oregon is the

00:07:28,779 --> 00:07:34,119
26 most populous city in our file and

00:07:31,239 --> 00:07:37,419
Portland Maine is the 537 both the

00:07:34,119 --> 00:07:39,129
populous city eventually we want to

00:07:37,419 --> 00:07:42,369
we'll probably have to do something with

00:07:39,129 --> 00:07:45,459
multiple files so let's say I have a

00:07:42,369 --> 00:07:48,399
file that has like a state FIPS code and

00:07:45,459 --> 00:07:50,289
a city name and then I have state zip

00:07:48,399 --> 00:07:52,599
code and the state name so if I have

00:07:50,289 --> 00:07:54,550
this data and I want to see city name

00:07:52,599 --> 00:07:58,449
and state name how can I join them

00:07:54,550 --> 00:07:59,860
together there's a join command that

00:07:58,449 --> 00:08:01,449
works pretty much exactly like how you

00:07:59,860 --> 00:08:03,579
expect the sequel joint to work by it

00:08:01,449 --> 00:08:08,050
work with so it's useful when you have

00:08:03,579 --> 00:08:12,099
like these type of identifier columns

00:08:08,050 --> 00:08:14,949
here another way to work with multiple

00:08:12,099 --> 00:08:18,159
files its paste paste is kind of weird

00:08:14,949 --> 00:08:21,069
in that it's basically like zips two

00:08:18,159 --> 00:08:24,669
files together so if you don't have like

00:08:21,069 --> 00:08:27,669
an ID column or something to join on if

00:08:24,669 --> 00:08:30,939
you pass the - s option it'll transpose

00:08:27,669 --> 00:08:32,289
the rows and columns yeah it's generally

00:08:30,939 --> 00:08:34,750
only useful if you have like a very

00:08:32,289 --> 00:08:39,659
specific output format that you're

00:08:34,750 --> 00:08:39,659
looking for but it's there

00:08:39,950 --> 00:08:47,670
a good example of paste here is let's

00:08:44,970 --> 00:08:49,920
say I wanted to add like a number to

00:08:47,670 --> 00:08:52,170
each line of this file that's like a row

00:08:49,920 --> 00:08:54,450
number or something sequence is a

00:08:52,170 --> 00:08:58,800
command that basically prints out you

00:08:54,450 --> 00:09:00,510
know one item per line so what we can do

00:08:58,800 --> 00:09:03,360
is basically zip out the output of

00:09:00,510 --> 00:09:06,870
sequence with our population CSV file

00:09:03,360 --> 00:09:11,190
so our generated output is we have a

00:09:06,870 --> 00:09:13,530
number for each line now Commons an

00:09:11,190 --> 00:09:16,200
interesting way to compare files comp

00:09:13,530 --> 00:09:18,330
produces three columns of output the

00:09:16,200 --> 00:09:20,310
first column our lines that are only in

00:09:18,330 --> 00:09:21,960
the first file the second column are

00:09:20,310 --> 00:09:24,270
lines that are only in the second file

00:09:21,960 --> 00:09:25,950
and the third column are lines that are

00:09:24,270 --> 00:09:30,450
in both files so if you want to do like

00:09:25,950 --> 00:09:33,450
type of like dipping operations comm is

00:09:30,450 --> 00:09:36,840
also weird in that you tell it you pass

00:09:33,450 --> 00:09:39,000
it a column and then it will not print

00:09:36,840 --> 00:09:41,940
out that output so if you wanted only

00:09:39,000 --> 00:09:46,200
the first column of output you would

00:09:41,940 --> 00:09:48,000
pass it two and three yeah it's weird in

00:09:46,200 --> 00:09:50,670
general I find myself using most of the

00:09:48,000 --> 00:09:54,270
time this last one comm one two so

00:09:50,670 --> 00:09:57,120
that's a will only return the lines that

00:09:54,270 --> 00:09:59,340
are in both files so kind of yeah so

00:09:57,120 --> 00:10:05,400
think of it like a Venn diagram type

00:09:59,340 --> 00:10:06,900
thing we don't always get data in the

00:10:05,400 --> 00:10:09,570
format that we want we'll have to do a

00:10:06,900 --> 00:10:10,800
little bit of cleanup set is a good

00:10:09,570 --> 00:10:14,460
option for that said it's really

00:10:10,800 --> 00:10:15,630
powerful but I find it best to do for

00:10:14,460 --> 00:10:18,450
these type of like simple like search

00:10:15,630 --> 00:10:20,070
and replace type operations so yeah we

00:10:18,450 --> 00:10:21,660
know in our file we have city repeated a

00:10:20,070 --> 00:10:24,630
bunch of times I don't want to see city

00:10:21,660 --> 00:10:27,390
anymore we can search for a space city

00:10:24,630 --> 00:10:32,160
and replace it with nothing and that's

00:10:27,390 --> 00:10:35,100
what it'll do TR is in kind of a niche

00:10:32,160 --> 00:10:37,380
command it stands for translate so

00:10:35,100 --> 00:10:40,590
you're the way to think about it is

00:10:37,380 --> 00:10:43,800
you're translating one character set to

00:10:40,590 --> 00:10:45,900
another so let's say I wanted to the

00:10:43,800 --> 00:10:47,400
first example is translating space to

00:10:45,900 --> 00:10:50,160
new line characters so if you have a big

00:10:47,400 --> 00:10:53,440
long row of things and you can separate

00:10:50,160 --> 00:10:55,930
it out to be a bunch of lines of things

00:10:53,440 --> 00:10:57,190
ter also has the concept of character

00:10:55,930 --> 00:10:59,380
classes so you can even do things like

00:10:57,190 --> 00:11:02,709
upper and lower casing on the command

00:10:59,380 --> 00:11:04,029
line and you can even do things like

00:11:02,709 --> 00:11:06,970
let's say I have a bunch of numbers I

00:11:04,029 --> 00:11:11,019
want to shift by one so you can do that

00:11:06,970 --> 00:11:13,000
and it also works with letters I've done

00:11:11,019 --> 00:11:16,389
a lot of commands out let's go through

00:11:13,000 --> 00:11:19,209
an example so throughout this talk I've

00:11:16,389 --> 00:11:21,490
used yeah this population CSV file it's

00:11:19,209 --> 00:11:24,490
got nice three columns pretty simple too

00:11:21,490 --> 00:11:27,940
easy and to see what's going on where

00:11:24,490 --> 00:11:29,410
did I get it so I did not download it

00:11:27,940 --> 00:11:31,300
directly from the American FactFinder

00:11:29,410 --> 00:11:33,730
website I'm sure you won't be surprised

00:11:31,300 --> 00:11:39,490
to hear that but I did download all the

00:11:33,730 --> 00:11:42,130
raw data from the Census this is what I

00:11:39,490 --> 00:11:45,190
can download directly it's a little bit

00:11:42,130 --> 00:11:47,410
hard to see what's going on there's lots

00:11:45,190 --> 00:11:49,089
of great data in there but let's say I

00:11:47,410 --> 00:11:51,490
don't want all of it

00:11:49,089 --> 00:11:55,240
so first step you'll see there's two

00:11:51,490 --> 00:11:58,630
header rows in this file which is yeah

00:11:55,240 --> 00:11:59,889
which is nice to see I guess so the

00:11:58,630 --> 00:12:03,250
first thing let's get rid of those

00:11:59,889 --> 00:12:05,110
header lines we can use tail for that so

00:12:03,250 --> 00:12:07,569
this version of tail I'm basically

00:12:05,110 --> 00:12:09,610
saying like start at the third line and

00:12:07,569 --> 00:12:12,430
give me everything underneath that so we

00:12:09,610 --> 00:12:14,860
can get rid of that next there's a lot

00:12:12,430 --> 00:12:16,980
of columns of output here let's only get

00:12:14,860 --> 00:12:19,959
the columns that we're looking for and

00:12:16,980 --> 00:12:22,750
through trial and error we can figure

00:12:19,959 --> 00:12:26,079
out that we want the ninth the tenth and

00:12:22,750 --> 00:12:28,750
the nineteenth column of output so this

00:12:26,079 --> 00:12:30,759
is an important one because you'll see

00:12:28,750 --> 00:12:34,420
here the actually city and state are

00:12:30,759 --> 00:12:38,139
quoted field so technically that would

00:12:34,420 --> 00:12:40,540
be one column of CSV input in this case

00:12:38,139 --> 00:12:42,639
bash does not understand that really all

00:12:40,540 --> 00:12:45,069
that well in this case it kind of works

00:12:42,639 --> 00:12:49,259
out in that they're sort of the same but

00:12:45,069 --> 00:12:51,819
yeah if you have more advanced CSV data

00:12:49,259 --> 00:12:53,709
that you might have to do some

00:12:51,819 --> 00:12:56,589
interesting things with bash to get it

00:12:53,709 --> 00:13:00,459
to work so next let's get rid of the

00:12:56,589 --> 00:13:02,470
quotes in our file we can use said to do

00:13:00,459 --> 00:13:03,820
that to search for quote and replace it

00:13:02,470 --> 00:13:05,800
with nothing

00:13:03,820 --> 00:13:07,510
we have the the G at the end there

00:13:05,800 --> 00:13:10,329
because of the fact we have multiple

00:13:07,510 --> 00:13:12,279
occurrences of quotes so by default it

00:13:10,329 --> 00:13:13,930
only look at the first thing and then

00:13:12,279 --> 00:13:17,260
move on to the next line

00:13:13,930 --> 00:13:20,190
so this with the G it'll look or search

00:13:17,260 --> 00:13:25,180
and replace throughout the whole line

00:13:20,190 --> 00:13:29,010
next yeah so we have a leading space in

00:13:25,180 --> 00:13:33,089
the state name now let's clean up that

00:13:29,010 --> 00:13:37,600
so we can do another password said and

00:13:33,089 --> 00:13:41,050
now we have our nice three column CSV

00:13:37,600 --> 00:13:44,320
file we can save that to population CSV

00:13:41,050 --> 00:13:47,579
and we're all done kind of another fun

00:13:44,320 --> 00:13:49,779
example would be like the the

00:13:47,579 --> 00:13:54,100
Shakespeare word count that you see you

00:13:49,779 --> 00:13:57,040
know the Hadoop examples so we can

00:13:54,100 --> 00:13:59,139
download like Shakespeare text which

00:13:57,040 --> 00:14:00,820
clocks it from Project Gutenberg so that

00:13:59,139 --> 00:14:04,600
clocks in about five megabytes which is

00:14:00,820 --> 00:14:07,029
all that much so and we can run a bunch

00:14:04,600 --> 00:14:10,380
of commands and kind of get a word count

00:14:07,029 --> 00:14:15,100
yeah the the interesting things here are

00:14:10,380 --> 00:14:19,300
that we use a TR to delete everything

00:14:15,100 --> 00:14:22,029
that's not a number or a letter and a

00:14:19,300 --> 00:14:25,959
space so this is using the idea of those

00:14:22,029 --> 00:14:28,959
like character classes and then we can

00:14:25,959 --> 00:14:31,300
so that's the first TR the second TR

00:14:28,959 --> 00:14:34,959
will squeeze all the spaces together

00:14:31,300 --> 00:14:36,399
into one space that's what and so we're

00:14:34,959 --> 00:14:38,470
using the space character class because

00:14:36,399 --> 00:14:44,170
it deals with tabs new lines things like

00:14:38,470 --> 00:14:45,760
that and then we turn everything into a

00:14:44,170 --> 00:14:47,589
new line care all the spaces in the new

00:14:45,760 --> 00:14:49,420
line characters that we get one word per

00:14:47,589 --> 00:14:51,910
line remember all the core utilities

00:14:49,420 --> 00:14:54,310
like working with lines then we

00:14:51,910 --> 00:14:57,220
lowercase everything short unique it and

00:14:54,310 --> 00:14:59,889
this is the output we get so we see kind

00:14:57,220 --> 00:15:02,829
of the what you would expect out of the

00:14:59,889 --> 00:15:05,110
the most common occurring words when we

00:15:02,829 --> 00:15:06,850
get down into the the single digits does

00:15:05,110 --> 00:15:08,040
it get interesting you know foeman's

00:15:06,850 --> 00:15:11,620
foeman's

00:15:08,040 --> 00:15:14,769
yes so this is what we're able to do

00:15:11,620 --> 00:15:17,030
here thank you

00:15:14,769 --> 00:15:19,400
yeah all the materials of the

00:15:17,030 --> 00:15:21,020
the cs3 files and stuff and slides are

00:15:19,400 --> 00:15:23,780
at this github repo feel free to ask me

00:15:21,020 --> 00:15:27,430
any questions on Twitter or in person

00:15:23,780 --> 00:15:27,430
here so thank you

00:15:27,750 --> 00:15:32,690

YouTube URL: https://www.youtube.com/watch?v=byEzXt2Ghsc


