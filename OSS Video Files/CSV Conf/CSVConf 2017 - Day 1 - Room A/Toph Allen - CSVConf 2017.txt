Title: Toph Allen - CSVConf 2017
Publication date: 2017-06-04
Playlist: CSVConf 2017 - Day 1 - Room A
Description: 
	
Captions: 
	00:00:01,900 --> 00:00:08,590
now hi I'm toast Alan I'm an

00:00:05,290 --> 00:00:10,450
epidemiologist and I guess director of

00:00:08,590 --> 00:00:14,200
data science on eco health alliance is

00:00:10,450 --> 00:00:15,759
technology and data team and I'm excited

00:00:14,200 --> 00:00:16,750
to hear talk to you all I'm going to try

00:00:15,759 --> 00:00:19,240
and go pretty quickly because I went

00:00:16,750 --> 00:00:20,680
over in one of my rehearsals so eco

00:00:19,240 --> 00:00:23,680
health alliance is a new york-based

00:00:20,680 --> 00:00:25,360
nonprofit we do research a kind of the

00:00:23,680 --> 00:00:27,400
intersection of public health and

00:00:25,360 --> 00:00:29,110
conservation mostly studying emerging

00:00:27,400 --> 00:00:30,640
infectious diseases a more a big

00:00:29,110 --> 00:00:33,010
multidisciplinary organization with

00:00:30,640 --> 00:00:36,339
field ecologists public health

00:00:33,010 --> 00:00:38,679
specialists and modeling and lyrics team

00:00:36,339 --> 00:00:45,370
and then this software team that I kind

00:00:38,679 --> 00:00:47,519
of Ko Ko steer and yeah I'm our team

00:00:45,370 --> 00:00:51,190
works largely on machine learning

00:00:47,519 --> 00:00:53,859
applications for biosurveillance but

00:00:51,190 --> 00:00:56,409
we're student data integration too so

00:00:53,859 --> 00:00:57,969
I'm going to hit on data sharing and

00:00:56,409 --> 00:01:00,219
data integration and kind of what that

00:00:57,969 --> 00:01:02,260
is talk about about tabular metadata

00:01:00,219 --> 00:01:04,089
anthologies and then talk about what

00:01:02,260 --> 00:01:09,180
we're what this proposal that we're

00:01:04,089 --> 00:01:11,230
submitting is about so data sharing is

00:01:09,180 --> 00:01:15,160
it's going way up and this is a good

00:01:11,230 --> 00:01:18,400
thing I went and looked at this 2014

00:01:15,160 --> 00:01:21,220
blog post on the are open side blog and

00:01:18,400 --> 00:01:25,060
so this is the number of packages on the

00:01:21,220 --> 00:01:29,230
Dryad repository and these are packages

00:01:25,060 --> 00:01:33,310
datasets with associated papers and yes

00:01:29,230 --> 00:01:36,850
they're going way up and this blog post

00:01:33,310 --> 00:01:41,170
was in 2014 so yesterday I went back and

00:01:36,850 --> 00:01:43,000
looked and there are a lot more now it's

00:01:41,170 --> 00:01:44,920
increasing becoming the norm to share

00:01:43,000 --> 00:01:46,270
your scientific data poverty and this is

00:01:44,920 --> 00:01:49,750
driven by a bunch of different factors

00:01:46,270 --> 00:01:52,060
including good infrastructure built up

00:01:49,750 --> 00:01:54,280
around hosting data in repositories and

00:01:52,060 --> 00:02:00,010
the growth of data sharing man data's

00:01:54,280 --> 00:02:01,840
from publishers and funders etc and in a

00:02:00,010 --> 00:02:04,000
scientific context you know open data is

00:02:01,840 --> 00:02:06,550
a really good thing a lot of its value

00:02:04,000 --> 00:02:09,099
derives from the ability of you to join

00:02:06,550 --> 00:02:10,229
multiple datasets together meaning that

00:02:09,099 --> 00:02:12,989
you can

00:02:10,229 --> 00:02:14,519
run comparisons or meta-analyses or you

00:02:12,989 --> 00:02:16,830
know studies with greater power cos you

00:02:14,519 --> 00:02:19,200
can have new datasets or out of the

00:02:16,830 --> 00:02:22,260
variables and on a more philosophical

00:02:19,200 --> 00:02:24,300
level I guess it means that it would be

00:02:22,260 --> 00:02:28,680
nice if data is all going into this kind

00:02:24,300 --> 00:02:32,040
of shet big shared effort but that's not

00:02:28,680 --> 00:02:34,349
not really the case as it stands because

00:02:32,040 --> 00:02:38,700
just because data is available doesn't

00:02:34,349 --> 00:02:40,470
mean that it's integrable ice already

00:02:38,700 --> 00:02:45,209
could break down once about the barriers

00:02:40,470 --> 00:02:47,340
to data integration and that define them

00:02:45,209 --> 00:02:49,829
as you need to have stuff in the same

00:02:47,340 --> 00:02:52,110
place and so this is kind of problem

00:02:49,829 --> 00:02:56,970
solved with you know data sharing being

00:02:52,110 --> 00:02:58,739
such a big thing now you need to have

00:02:56,970 --> 00:03:02,040
your information the same structure so

00:02:58,739 --> 00:03:04,410
this is file format this is you know is

00:03:02,040 --> 00:03:06,500
it aggregated to the same level and then

00:03:04,410 --> 00:03:10,319
you need to have the same thing

00:03:06,500 --> 00:03:13,079
described in in the same language so

00:03:10,319 --> 00:03:16,380
semantic incompatibility is a kind of a

00:03:13,079 --> 00:03:18,239
big sticking point and what what

00:03:16,380 --> 00:03:22,709
semantic incompatibility basically

00:03:18,239 --> 00:03:24,810
refers to is you have something say this

00:03:22,709 --> 00:03:26,400
island here and you have a ton of

00:03:24,810 --> 00:03:28,799
different ways you could refer to it you

00:03:26,400 --> 00:03:30,569
might have an official or a short name

00:03:28,799 --> 00:03:32,639
in the same language you might have

00:03:30,569 --> 00:03:35,120
different languages you might have a set

00:03:32,639 --> 00:03:37,950
of one of many types of standard codes

00:03:35,120 --> 00:03:39,540
you could have some proprietary codes or

00:03:37,950 --> 00:03:40,799
a set of coordinates or even just a

00:03:39,540 --> 00:03:45,660
reference to a shapefile and a hard

00:03:40,799 --> 00:03:49,049
drive and this at least some I guess

00:03:45,660 --> 00:03:50,310
anti patterns in data analysis so say

00:03:49,049 --> 00:03:52,200
you have a bunch of country level

00:03:50,310 --> 00:03:53,760
indicators that you need to run analysis

00:03:52,200 --> 00:03:55,920
are and they're from different

00:03:53,760 --> 00:03:58,230
organizations you know they might they

00:03:55,920 --> 00:04:00,900
might look similar but you wind up

00:03:58,230 --> 00:04:03,389
having a fair amount of felling messy

00:04:00,900 --> 00:04:05,910
fairly ugly spaghetti code to get them

00:04:03,389 --> 00:04:07,650
to work together and then you wind up

00:04:05,910 --> 00:04:10,139
writing functions to score how good and

00:04:07,650 --> 00:04:11,040
match different things are and then when

00:04:10,139 --> 00:04:13,019
you have another project you can the

00:04:11,040 --> 00:04:15,630
same indicators you tell your research

00:04:13,019 --> 00:04:17,070
assistants then go download these just

00:04:15,630 --> 00:04:19,709
use the script I wrote to mostly youth

00:04:17,070 --> 00:04:23,100
before because it works

00:04:19,709 --> 00:04:26,580
so thinking about the barriers that

00:04:23,100 --> 00:04:31,310
integration in the solutions you've got

00:04:26,580 --> 00:04:33,479
the data availability problem solved in

00:04:31,310 --> 00:04:35,759
agreements and ways to transport files

00:04:33,479 --> 00:04:38,490
around you have the structure of problem

00:04:35,759 --> 00:04:41,070
solved by the growth of software

00:04:38,490 --> 00:04:42,780
ecosystems and practices for tidy data

00:04:41,070 --> 00:04:45,720
essentially which make that much easier

00:04:42,780 --> 00:04:48,080
reconcile and the way that you get

00:04:45,720 --> 00:04:51,270
around semantic incompatibility is

00:04:48,080 --> 00:04:53,639
annotate your tabular data with links to

00:04:51,270 --> 00:04:58,710
apologies because then you can integrate

00:04:53,639 --> 00:05:00,389
your data set to ontology so just to run

00:04:58,710 --> 00:05:01,500
the same page when I say ontology I'm

00:05:00,389 --> 00:05:04,620
referring to like a structured

00:05:01,500 --> 00:05:06,960
dictionary that documents meaning in an

00:05:04,620 --> 00:05:09,150
innocent domain so say I'm an

00:05:06,960 --> 00:05:12,419
ornithologist and I'm collecting

00:05:09,150 --> 00:05:19,710
sightings of swans in a CSV file and

00:05:12,419 --> 00:05:22,139
that Tidy sightings I guess by day and I

00:05:19,710 --> 00:05:26,159
want to integrate another data set that

00:05:22,139 --> 00:05:28,289
another researcher created and as you

00:05:26,159 --> 00:05:30,870
can see in these two data sets the Swan

00:05:28,289 --> 00:05:32,729
species are referred to differently and

00:05:30,870 --> 00:05:34,020
you don't want to just rename them

00:05:32,729 --> 00:05:37,470
because you know that's not going to

00:05:34,020 --> 00:05:39,840
generalize so if you go and found if you

00:05:37,470 --> 00:05:43,220
hadn't found an ontology of swans you

00:05:39,840 --> 00:05:43,220
might want to call it a swan ecology

00:05:43,710 --> 00:05:51,360
and and you you annotate your data

00:05:48,720 --> 00:05:54,690
linking it to the Swan ontology well

00:05:51,360 --> 00:05:58,080
then after you've done that that's the

00:05:54,690 --> 00:05:59,759
hard part after you have that link there

00:05:58,080 --> 00:06:01,199
conceptually at least it would be

00:05:59,759 --> 00:06:05,250
relatively simple to get your data to

00:06:01,199 --> 00:06:09,150
work together and to actually get this

00:06:05,250 --> 00:06:15,449
to happen a lot of the pieces that you

00:06:09,150 --> 00:06:18,300
would need are in place so you have your

00:06:15,449 --> 00:06:20,610
you know there are standards published

00:06:18,300 --> 00:06:23,580
by the web consortium that about how to

00:06:20,610 --> 00:06:26,699
represent tabular data and metadata and

00:06:23,580 --> 00:06:29,010
in fact this particular standard CSV on

00:06:26,699 --> 00:06:31,350
the web one of the editors in respec

00:06:29,010 --> 00:06:35,940
named Jenny Tennyson gave a presentation

00:06:31,350 --> 00:06:39,320
at last year's CSV conf about that one

00:06:35,940 --> 00:06:41,370
of the ways that this defines to

00:06:39,320 --> 00:06:44,580
encourage your metadata is to have your

00:06:41,370 --> 00:06:47,190
CSV file and to have a sidecar metadata

00:06:44,580 --> 00:06:48,210
file and this degrades nicely because if

00:06:47,190 --> 00:06:50,130
you don't have the software to read

00:06:48,210 --> 00:06:54,300
metadata file you still each have a CSV

00:06:50,130 --> 00:06:56,940
and the metadata file is in Jason for

00:06:54,300 --> 00:06:59,930
Lynch data so we're a plain JSON file

00:06:56,940 --> 00:07:04,740
would just have a bunch of data in it

00:06:59,930 --> 00:07:06,870
json-ld gives that data but also links

00:07:04,740 --> 00:07:09,750
to a schema that says what you're meant

00:07:06,870 --> 00:07:11,220
to expect what the structured bits of

00:07:09,750 --> 00:07:17,220
what the bits are data and what they

00:07:11,220 --> 00:07:19,500
should look like so json-ld and CSV on

00:07:17,220 --> 00:07:20,900
the web a part of the Semantic Web set

00:07:19,500 --> 00:07:24,780
of standards they're a bunch of these

00:07:20,900 --> 00:07:26,340
are the F schema is another one and this

00:07:24,780 --> 00:07:31,409
family of standards also includes a way

00:07:26,340 --> 00:07:33,479
to publish ontology switch our key so

00:07:31,409 --> 00:07:35,970
our web ontology language is already

00:07:33,479 --> 00:07:38,220
used by groups like bio portal which is

00:07:35,970 --> 00:07:41,669
a project of the National Center for

00:07:38,220 --> 00:07:43,740
biology and Stanford University this is

00:07:41,669 --> 00:07:45,210
a centralized ontology I guess

00:07:43,740 --> 00:07:47,370
repository

00:07:45,210 --> 00:07:49,289
it focuses on very broadly defined

00:07:47,370 --> 00:07:52,380
biomedical ontologies so this is

00:07:49,289 --> 00:07:55,830
includes you know cancer terminology but

00:07:52,380 --> 00:07:57,000
also place names and a sister project

00:07:55,830 --> 00:07:58,890
the Center for extend

00:07:57,000 --> 00:08:01,590
data annotation and retrieval they're

00:07:58,890 --> 00:08:04,230
working on tools including some metadata

00:08:01,590 --> 00:08:08,220
recommenders with and then moves we

00:08:04,230 --> 00:08:11,700
focus on laboratory data so data

00:08:08,220 --> 00:08:13,710
repositories like data one are also part

00:08:11,700 --> 00:08:16,050
of the kind of data sharing boom and

00:08:13,710 --> 00:08:17,520
they have support for a number of these

00:08:16,050 --> 00:08:19,710
kind of ontology and linked data

00:08:17,520 --> 00:08:21,530
standards and they also have some

00:08:19,710 --> 00:08:24,660
interfaces that can help create them

00:08:21,530 --> 00:08:27,480
both both sida and data one are

00:08:24,660 --> 00:08:29,460
researching ways to predict structured

00:08:27,480 --> 00:08:31,350
metadata from unstructured metadata so

00:08:29,460 --> 00:08:33,620
say you have a textual description of a

00:08:31,350 --> 00:08:36,810
file or a paper accompanying a file

00:08:33,620 --> 00:08:40,590
they're working on ways to try and

00:08:36,810 --> 00:08:42,780
predict what ontology you might need to

00:08:40,590 --> 00:08:44,099
describe different bits of the file and

00:08:42,780 --> 00:08:46,650
they have some preliminary interface

00:08:44,099 --> 00:08:52,440
that's available on their website and

00:08:46,650 --> 00:08:54,150
I'm k-dub but despite all this despite

00:08:52,440 --> 00:08:56,850
the proliferation of standards and tools

00:08:54,150 --> 00:08:59,610
there's something missing because very

00:08:56,850 --> 00:09:04,230
few datasets have structured metadata

00:08:59,610 --> 00:09:05,370
still so these are two papers that if

00:09:04,230 --> 00:09:09,210
you're interested in this I recommend

00:09:05,370 --> 00:09:12,270
they're both from 2011 and they one is a

00:09:09,210 --> 00:09:15,150
survey of scientists and another is I

00:09:12,270 --> 00:09:17,580
guess an ethnographic study of a bunch

00:09:15,150 --> 00:09:22,230
of research sites which are implementing

00:09:17,580 --> 00:09:24,510
ecological metadata language I guess I

00:09:22,230 --> 00:09:27,300
would sum up their takeaway points by

00:09:24,510 --> 00:09:30,120
saying that many if not most scientists

00:09:27,300 --> 00:09:32,280
at least you know six years ago are

00:09:30,120 --> 00:09:35,880
unaware of metadata and metadata

00:09:32,280 --> 00:09:38,130
standards if there are tools to apply

00:09:35,880 --> 00:09:40,860
them and what the potential benefits of

00:09:38,130 --> 00:09:43,710
metadata would be and even for metadata

00:09:40,860 --> 00:09:46,500
savvy scientists annotation is is time

00:09:43,710 --> 00:09:48,150
consuming at best or impossible at worse

00:09:46,500 --> 00:09:50,010
because there aren't really many tools

00:09:48,150 --> 00:09:52,650
available to perform it and even when

00:09:50,010 --> 00:09:54,300
metadata is properly applied it doesn't

00:09:52,650 --> 00:09:55,890
really provide any immediate payoff to

00:09:54,300 --> 00:09:58,440
that workflow because again you're

00:09:55,890 --> 00:10:01,860
missing the tooling it's most likely

00:09:58,440 --> 00:10:08,810
uses for future projects other people

00:10:01,860 --> 00:10:08,810
and yeah I'll let you read this quote

00:10:11,410 --> 00:10:18,199
and the compounding factors I guess I

00:10:15,889 --> 00:10:20,509
would say a lot of annotation when it

00:10:18,199 --> 00:10:21,829
does happen happens when you're getting

00:10:20,509 --> 00:10:23,449
rid of archive your data so at the end

00:10:21,829 --> 00:10:28,939
of a project you're getting rid to a

00:10:23,449 --> 00:10:31,249
repository in publisher paper and this

00:10:28,939 --> 00:10:33,169
means this means that scientists aren't

00:10:31,249 --> 00:10:37,149
likely to use metadata driven tools

00:10:33,169 --> 00:10:40,939
during the course of their research they

00:10:37,149 --> 00:10:42,319
I guess it also means that decisions

00:10:40,939 --> 00:10:44,600
about how you're going to structure your

00:10:42,319 --> 00:10:46,129
data or errors that you make will

00:10:44,600 --> 00:10:48,619
accumulate over the course of a project

00:10:46,129 --> 00:10:50,089
which means that when you actually go to

00:10:48,619 --> 00:10:52,850
apply metadata using the tools provided

00:10:50,089 --> 00:10:55,220
by repositories it's it's harder and

00:10:52,850 --> 00:10:57,259
also happens as you know you're

00:10:55,220 --> 00:10:59,029
scrambling to get stuff ready or you're

00:10:57,259 --> 00:11:03,649
shifting resources to your next project

00:10:59,029 --> 00:11:06,639
and here just even a mandate not going

00:11:03,649 --> 00:11:06,639
to help at this point

00:11:06,730 --> 00:11:15,249
so that's where we are right now and

00:11:11,769 --> 00:11:17,839
this project this proposal that we've I

00:11:15,249 --> 00:11:21,259
guess with the process of submitting the

00:11:17,839 --> 00:11:24,589
the phone foundation is about a set of

00:11:21,259 --> 00:11:27,259
tools that where we want to develop to

00:11:24,589 --> 00:11:29,539
kind of bring an annotation forward in a

00:11:27,259 --> 00:11:31,489
projects lifespan because we forgive it

00:11:29,539 --> 00:11:33,439
with the right set of tools you can

00:11:31,489 --> 00:11:35,239
reduce the effort and increase the

00:11:33,439 --> 00:11:38,929
incentive for scientists to annotate

00:11:35,239 --> 00:11:41,989
their data and I guess at the same time

00:11:38,929 --> 00:11:46,359
reduce workload and bring a bunch of

00:11:41,989 --> 00:11:46,359
those benefits that open data promises

00:11:46,419 --> 00:11:53,359
so there are two major parts to our plan

00:11:51,709 --> 00:11:55,789
I guess the first is you know we want to

00:11:53,359 --> 00:11:58,999
take our own sonic cream metadata

00:11:55,789 --> 00:12:01,789
recommendation service using machine

00:11:58,999 --> 00:12:03,769
learning and heuristic approaches that

00:12:01,789 --> 00:12:07,069
we've used in other projects that we've

00:12:03,769 --> 00:12:09,199
done and we also want to create

00:12:07,069 --> 00:12:11,779
interfaces graphical and command-line

00:12:09,199 --> 00:12:14,379
interfaces that make it easier to apply

00:12:11,779 --> 00:12:17,329
metadata early in a datasets life cycle

00:12:14,379 --> 00:12:19,300
and make it easier to get some utility

00:12:17,329 --> 00:12:23,360
out of that metadata

00:12:19,300 --> 00:12:26,170
kind of have this virtuous cycle bring

00:12:23,360 --> 00:12:33,709
it forward lower the barrier and

00:12:26,170 --> 00:12:35,810
increase the incentives so with a

00:12:33,709 --> 00:12:38,060
metadata recommender service what we

00:12:35,810 --> 00:12:40,730
want to do is predict the metadata for a

00:12:38,060 --> 00:12:42,980
CSV file and do so in a kind of a

00:12:40,730 --> 00:12:44,660
two-step thing first you want to rate

00:12:42,980 --> 00:12:46,459
the column level ontology and then

00:12:44,660 --> 00:12:49,610
second you want to predict the cell

00:12:46,459 --> 00:12:51,740
level value and we are going to try and

00:12:49,610 --> 00:12:53,360
do so based only on the contents of the

00:12:51,740 --> 00:12:55,490
table itself so we don't really want to

00:12:53,360 --> 00:12:59,269
rely on a paper having been written

00:12:55,490 --> 00:13:01,160
because otherwise it's not gonna be sore

00:12:59,269 --> 00:13:04,250
for a random CSV file sitting around on

00:13:01,160 --> 00:13:08,360
someone's hard drive so essentially we

00:13:04,250 --> 00:13:10,540
want something that good okay yeah we

00:13:08,360 --> 00:13:14,000
want something that can prioritize

00:13:10,540 --> 00:13:17,060
amongst different ontology s and at

00:13:14,000 --> 00:13:19,000
least produce recommendations and you

00:13:17,060 --> 00:13:22,279
know have this package be something with

00:13:19,000 --> 00:13:24,829
you know usable API so that it can be

00:13:22,279 --> 00:13:29,589
useful even if the rest of the project

00:13:24,829 --> 00:13:29,589
sputters because that's good stuff and

00:13:29,709 --> 00:13:35,209
we want to you know train it on there's

00:13:32,600 --> 00:13:37,520
a bunch of existing data out there which

00:13:35,209 --> 00:13:41,779
kind of in the wild which might be

00:13:37,520 --> 00:13:42,950
useful for this so data one is already

00:13:41,779 --> 00:13:43,760
doing research into this sort of thing

00:13:42,950 --> 00:13:45,640
and they're already working on a

00:13:43,760 --> 00:13:49,490
recommender and we were chatting with

00:13:45,640 --> 00:13:52,850
someone from data one who's I forget who

00:13:49,490 --> 00:13:55,850
was but anyway they pointed us to a repo

00:13:52,850 --> 00:13:57,829
that's linked here and they have a

00:13:55,850 --> 00:13:59,750
thousand data set so I think it is which

00:13:57,829 --> 00:14:00,980
I manually annotated specific to the

00:13:59,750 --> 00:14:04,520
purpose of training these sort of

00:14:00,980 --> 00:14:07,279
algorithms now a lot of their approaches

00:14:04,520 --> 00:14:08,959
are getting pretty poor predictive power

00:14:07,279 --> 00:14:10,160
right now and there's no reason to think

00:14:08,959 --> 00:14:12,050
that you know we're going to have a

00:14:10,160 --> 00:14:14,089
silver bullet or do a better job than

00:14:12,050 --> 00:14:15,620
they are which is why I think the

00:14:14,089 --> 00:14:18,079
interface part is really key because

00:14:15,620 --> 00:14:19,850
good interfaces can make a really poorly

00:14:18,079 --> 00:14:21,260
predictive algorithm useful if it

00:14:19,850 --> 00:14:25,850
streamlines the process of going to

00:14:21,260 --> 00:14:28,100
triaging recommendations and also if you

00:14:25,850 --> 00:14:31,180
hook it up right you can set up again a

00:14:28,100 --> 00:14:33,790
kind of a virtuous cycle where your

00:14:31,180 --> 00:14:35,530
creating using unless good algorithms to

00:14:33,790 --> 00:14:46,660
create training gave me better

00:14:35,530 --> 00:14:50,020
algorithms so this is a our tech teams

00:14:46,660 --> 00:14:51,580
main software projects kind of have done

00:14:50,020 --> 00:14:54,160
done this but in a different field so

00:14:51,580 --> 00:14:55,690
we're working on this piece of software

00:14:54,160 --> 00:14:57,430
called either connect which is the

00:14:55,690 --> 00:15:00,040
emerging infectious disease repository

00:14:57,430 --> 00:15:03,040
and it's kind of doing knowledge-based

00:15:00,040 --> 00:15:05,160
population backed by machine learning

00:15:03,040 --> 00:15:08,590
and natural language processing and so

00:15:05,160 --> 00:15:11,680
it'll take text identify case and death

00:15:08,590 --> 00:15:15,220
counts and use those plus identified

00:15:11,680 --> 00:15:18,460
place names and dates to populate this

00:15:15,220 --> 00:15:22,840
schema for a spatial temporal disease

00:15:18,460 --> 00:15:27,490
event but since we don't have training

00:15:22,840 --> 00:15:30,310
data to make it a spot on 100% of the

00:15:27,490 --> 00:15:32,680
time right algorithm we work really hard

00:15:30,310 --> 00:15:36,430
to get this interface where a user can

00:15:32,680 --> 00:15:38,650
triage the suggested annotations and

00:15:36,430 --> 00:15:41,830
then we save that data in a way that it

00:15:38,650 --> 00:15:43,090
can be used by classifiers for kind of

00:15:41,830 --> 00:15:46,480
the next round of updates

00:15:43,090 --> 00:15:47,740
I think the interface is also important

00:15:46,480 --> 00:15:50,380
to give the user especially a

00:15:47,740 --> 00:15:54,490
non-technical user early clear mental

00:15:50,380 --> 00:15:57,760
model of what's going on and here's that

00:15:54,490 --> 00:15:59,260
virtuous cycle I was talking about the

00:15:57,760 --> 00:16:01,990
graphical interface and the art package

00:15:59,260 --> 00:16:04,150
are also a good place for us to surface

00:16:01,990 --> 00:16:06,970
some of the useful functionality which I

00:16:04,150 --> 00:16:09,700
think properly annotated metadata would

00:16:06,970 --> 00:16:11,620
provide so writing out sidecar metadata

00:16:09,700 --> 00:16:13,840
files is a great thing because other

00:16:11,620 --> 00:16:15,250
tools which write which read and write

00:16:13,840 --> 00:16:17,650
data in these standards can then make

00:16:15,250 --> 00:16:19,300
use of that and I think also want to be

00:16:17,650 --> 00:16:22,300
able to write out like a canonicalized

00:16:19,300 --> 00:16:25,090
version of a CSV file it was make it

00:16:22,300 --> 00:16:26,860
much easier to submit your data to a

00:16:25,090 --> 00:16:29,470
repository so there's not that mad - at

00:16:26,860 --> 00:16:34,240
the end of a project and we can also

00:16:29,470 --> 00:16:37,230
provide verbs to operate on tables based

00:16:34,240 --> 00:16:40,600
on the semantic annotations metadata so

00:16:37,230 --> 00:16:43,000
like a left join based on the

00:16:40,600 --> 00:16:43,670
ontological value of your country column

00:16:43,000 --> 00:16:48,740
as a

00:16:43,670 --> 00:16:50,870
to the string value then we have our

00:16:48,740 --> 00:16:54,200
continuous integration setup which gnome

00:16:50,870 --> 00:16:55,670
has been been a big proponent of and I

00:16:54,200 --> 00:17:01,810
think this is the part he's most excited

00:16:55,670 --> 00:17:06,650
about so the the continuous integration

00:17:01,810 --> 00:17:08,690
idea is essentially you set the set the

00:17:06,650 --> 00:17:10,640
software set the service to monitor a

00:17:08,690 --> 00:17:15,110
certain folder maybe in github or

00:17:10,640 --> 00:17:19,040
Dropbox and the web service when it sees

00:17:15,110 --> 00:17:21,860
a change in in the data file there pulls

00:17:19,040 --> 00:17:23,420
up a report and maybe it emailed you or

00:17:21,860 --> 00:17:25,160
could do a number of other things so it

00:17:23,420 --> 00:17:28,010
could automatically right back metadata

00:17:25,160 --> 00:17:30,710
files with recommendations of other

00:17:28,010 --> 00:17:33,890
certain cumference threshold or a user

00:17:30,710 --> 00:17:37,580
or give you nice little badges to put on

00:17:33,890 --> 00:17:40,130
a github repository essentially I think

00:17:37,580 --> 00:17:42,560
where we want to see metadata go in the

00:17:40,130 --> 00:17:45,440
next five years is to take it from the

00:17:42,560 --> 00:17:46,880
mad dash mandated task at the end of a

00:17:45,440 --> 00:17:49,310
project to be part of the

00:17:46,880 --> 00:17:54,220
procrastination yak shaving setup phase

00:17:49,310 --> 00:17:58,400
at the beginning of a project so to

00:17:54,220 --> 00:18:02,030
caveat this this is just a proposal we

00:17:58,400 --> 00:18:04,340
haven't done this yet but I I think this

00:18:02,030 --> 00:18:05,450
is where it the field is going to go and

00:18:04,340 --> 00:18:09,440
there are definitely other people

00:18:05,450 --> 00:18:10,940
working on similar projects and you know

00:18:09,440 --> 00:18:15,140
we're super interested in collaborating

00:18:10,940 --> 00:18:17,450
on this sort of thing another caveat in

00:18:15,140 --> 00:18:19,610
this talk and in our proposal with

00:18:17,450 --> 00:18:21,800
intentionally punting on some of the

00:18:19,610 --> 00:18:23,540
details I'm kind of being glib we're

00:18:21,800 --> 00:18:26,870
really defining stuff is out of scope

00:18:23,540 --> 00:18:29,690
because I get to really technical really

00:18:26,870 --> 00:18:31,940
quickly and a very layered problem and I

00:18:29,690 --> 00:18:37,400
think we're trying to focus to narrow

00:18:31,940 --> 00:18:42,410
our scope in a way to tackle a little

00:18:37,400 --> 00:18:45,290
bit at a time so I guess I want to thank

00:18:42,410 --> 00:18:46,430
Noam who has been a great help kind of

00:18:45,290 --> 00:18:50,390
crystallizing a lot of my thoughts

00:18:46,430 --> 00:18:52,010
around this and it's great to have a

00:18:50,390 --> 00:18:53,480
multi-discipline place to work at the

00:18:52,010 --> 00:18:54,710
sloan Foundation has funded us but

00:18:53,480 --> 00:18:56,690
they've done a bunch of great things and

00:18:54,710 --> 00:18:57,200
it's nice to have an organization to

00:18:56,690 --> 00:19:00,110
pitch this

00:18:57,200 --> 00:19:02,240
thing too and also it's really lovely to

00:19:00,110 --> 00:19:04,190
have a conference like this and an

00:19:02,240 --> 00:19:06,289
audience who's willing to come listen to

00:19:04,190 --> 00:19:08,600
me no doubt about stuff like this so

00:19:06,289 --> 00:19:15,079
thanks everyone

00:19:08,600 --> 00:19:15,079

YouTube URL: https://www.youtube.com/watch?v=z5u4VMHBW78


