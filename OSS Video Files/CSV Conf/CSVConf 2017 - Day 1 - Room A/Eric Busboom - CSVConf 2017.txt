Title: Eric Busboom - CSVConf 2017
Publication date: 2017-06-04
Playlist: CSVConf 2017 - Day 1 - Room A
Description: 
	
Captions: 
	00:00:02,060 --> 00:00:06,140
my name's Eric Bostrom I am the founder

00:00:04,490 --> 00:00:07,520
and director at the San Diego regional

00:00:06,140 --> 00:00:09,049
data library I also run a company called

00:00:07,520 --> 00:00:11,059
Civic knowledge they both do basically

00:00:09,049 --> 00:00:12,800
the same thing depending on whether it's

00:00:11,059 --> 00:00:15,410
a volunteer project or a paid project

00:00:12,800 --> 00:00:17,390
which is health journalists governments

00:00:15,410 --> 00:00:20,539
and nonprofits get answers to

00:00:17,390 --> 00:00:22,880
data-driven questions and obviously data

00:00:20,539 --> 00:00:24,350
has an important component with metadata

00:00:22,880 --> 00:00:26,420
so we're going to thank you for coming

00:00:24,350 --> 00:00:27,740
to talk about one of our plans for

00:00:26,420 --> 00:00:29,449
making it a lot easier to create

00:00:27,740 --> 00:00:31,730
metadata so we're trying to solve the

00:00:29,449 --> 00:00:32,930
questions we had over here about what's

00:00:31,730 --> 00:00:34,820
in it for the person who creates

00:00:32,930 --> 00:00:37,280
metadata well we may not have an answer

00:00:34,820 --> 00:00:40,550
to that but we hopefully can make it

00:00:37,280 --> 00:00:43,700
less expensive and onerous so this

00:00:40,550 --> 00:00:46,640
project started with we were working on

00:00:43,700 --> 00:00:48,410
a grant application called health

00:00:46,640 --> 00:00:49,670
reporter very simple the census reporter

00:00:48,410 --> 00:00:52,309
if you're familiar with that tool except

00:00:49,670 --> 00:00:54,739
this one is designed to help journalists

00:00:52,309 --> 00:00:56,660
find patterns and data using pre

00:00:54,739 --> 00:00:57,890
perceptual visualization techniques it

00:00:56,660 --> 00:01:00,379
worked great everything about was

00:00:57,890 --> 00:01:02,660
fabulous except didn't have much data in

00:01:00,379 --> 00:01:04,699
the repository because wrangling data

00:01:02,660 --> 00:01:07,549
and loading it into a repository like

00:01:04,699 --> 00:01:09,020
this is really expensive and we talked

00:01:07,549 --> 00:01:11,210
to the help the census report of folks

00:01:09,020 --> 00:01:13,330
they gave us numbers that involve many

00:01:11,210 --> 00:01:15,830
hundreds of thousands of dollars I think

00:01:13,330 --> 00:01:17,600
$600,000 for their project we said no

00:01:15,830 --> 00:01:19,400
we're not getting funded for that so how

00:01:17,600 --> 00:01:21,259
do we make it easier to import data well

00:01:19,400 --> 00:01:24,140
one of the answers is to get better

00:01:21,259 --> 00:01:25,729
metadata from the data creators so we

00:01:24,140 --> 00:01:28,130
can just import it programmatically that

00:01:25,729 --> 00:01:30,729
was the idea we found some really

00:01:28,130 --> 00:01:32,780
wonderful ways to do this if you were at

00:01:30,729 --> 00:01:35,990
Adriana's talking you saw a little bit

00:01:32,780 --> 00:01:37,670
about the data packages and that the

00:01:35,990 --> 00:01:39,470
data packages from Open Knowledge

00:01:37,670 --> 00:01:42,860
Foundation are just about the perfect

00:01:39,470 --> 00:01:44,509
solution they're tabular data packages

00:01:42,860 --> 00:01:48,259
they're simple and comprehensive the set

00:01:44,509 --> 00:01:50,090
of terms they have is evidences people

00:01:48,259 --> 00:01:52,970
who really understand how to work with

00:01:50,090 --> 00:01:54,439
public data one problem though I have a

00:01:52,970 --> 00:02:00,500
bunch of epidemiologists who are

00:01:54,439 --> 00:02:03,680
creating data and they do not speak this

00:02:00,500 --> 00:02:05,540
I love JSON you love jason most people

00:02:03,680 --> 00:02:08,959
think jason great because we work with

00:02:05,540 --> 00:02:12,860
xml but epidemiologist like i wrote that

00:02:08,959 --> 00:02:14,970
is so we need to figure out how to make

00:02:12,860 --> 00:02:16,950
this something that normal people

00:02:14,970 --> 00:02:18,420
right and part of the reason is that

00:02:16,950 --> 00:02:19,740
metadata is really important for all

00:02:18,420 --> 00:02:21,990
sorts of stuff here's a good example

00:02:19,740 --> 00:02:23,730
from data gov this has been true for

00:02:21,990 --> 00:02:26,640
years by the way if you do a search for

00:02:23,730 --> 00:02:28,680
diabetes in California you would expect

00:02:26,640 --> 00:02:30,210
to get datasets with diabetes in

00:02:28,680 --> 00:02:32,400
California probably something from the

00:02:30,210 --> 00:02:34,410
CDC no you actually get data B these

00:02:32,400 --> 00:02:36,420
diabetes from Allegheny County and

00:02:34,410 --> 00:02:38,250
Pittsburgh there's a hit in there it's

00:02:36,420 --> 00:02:40,680
all in the first page state of Oklahoma

00:02:38,250 --> 00:02:43,290
and something from the city of Chicago

00:02:40,680 --> 00:02:46,620
that's not the worst of it you also get

00:02:43,290 --> 00:02:49,350
a comic book from the geothermal survey

00:02:46,620 --> 00:02:51,870
of American Geological Survey about

00:02:49,350 --> 00:02:54,060
California geysers and I have absolutely

00:02:51,870 --> 00:02:56,940
no idea what these do about diabetes

00:02:54,060 --> 00:02:58,860
there's clearly a problem here with the

00:02:56,940 --> 00:03:02,580
metadata because you can't search for

00:02:58,860 --> 00:03:04,410
diabetes in California so and the

00:03:02,580 --> 00:03:06,330
solution we decided on was in order to

00:03:04,410 --> 00:03:08,670
work with epidemiologists we've got to

00:03:06,330 --> 00:03:10,590
use a tool that allows them to use

00:03:08,670 --> 00:03:12,209
structured data but there has to be

00:03:10,590 --> 00:03:14,160
something they've already got we can't

00:03:12,209 --> 00:03:15,180
expect them the binding stuff because

00:03:14,160 --> 00:03:16,350
then you go through a government

00:03:15,180 --> 00:03:16,920
procurement program that takes you

00:03:16,350 --> 00:03:20,040
forever

00:03:16,920 --> 00:03:22,500
so the torah' already have is Excel if

00:03:20,040 --> 00:03:24,930
we can get structured metadata all that

00:03:22,500 --> 00:03:26,700
complicated lists and dictionaries into

00:03:24,930 --> 00:03:29,280
Excel we can make the data easier to

00:03:26,700 --> 00:03:32,430
read easier to write and it can be

00:03:29,280 --> 00:03:34,080
included in the Excel spreadsheet that's

00:03:32,430 --> 00:03:36,120
why it's called meta tab your first tab

00:03:34,080 --> 00:03:38,760
in your Excel spreadsheet is the meta

00:03:36,120 --> 00:03:41,220
tab and if we can do that we can improve

00:03:38,760 --> 00:03:43,380
workflow because there's one document

00:03:41,220 --> 00:03:45,660
that goes from reviewer to reviewer

00:03:43,380 --> 00:03:47,190
before it gets published on socratic you

00:03:45,660 --> 00:03:50,970
don't have to separate these things in

00:03:47,190 --> 00:03:52,320
two paths the the folks at various

00:03:50,970 --> 00:03:55,380
California departments really liked that

00:03:52,320 --> 00:03:59,250
one which is what it looks like this is

00:03:55,380 --> 00:04:02,660
in Google Spreadsheets it's actually you

00:03:59,250 --> 00:04:05,280
can see our meta tag out on up there

00:04:02,660 --> 00:04:07,140
it's pretty simple it's pretty close to

00:04:05,280 --> 00:04:13,950
what you would get if you were to write

00:04:07,140 --> 00:04:15,720
this all yourself that's here for me if

00:04:13,950 --> 00:04:18,630
you were to write these if you were to

00:04:15,720 --> 00:04:20,700
give an epidemiologist the the job they

00:04:18,630 --> 00:04:22,260
sit down go into Excel write your meta

00:04:20,700 --> 00:04:27,120
data they would probably do something

00:04:22,260 --> 00:04:28,320
relatively close to that this is like I

00:04:27,120 --> 00:04:28,680
said it's good because it's easier to

00:04:28,320 --> 00:04:30,870
write

00:04:28,680 --> 00:04:32,820
so much easier to read you can give

00:04:30,870 --> 00:04:35,100
somebody metadata in this form and they

00:04:32,820 --> 00:04:36,240
can read it they don't have to parts XML

00:04:35,100 --> 00:04:38,460
in their head they don't have to convert

00:04:36,240 --> 00:04:40,350
it to HTML at a tool they just read the

00:04:38,460 --> 00:04:43,229
document we do this all the time with

00:04:40,350 --> 00:04:46,229
PDF files that have data dictionaries in

00:04:43,229 --> 00:04:47,610
them in a tabular format embedding the

00:04:46,229 --> 00:04:49,710
data in the spreadsheet makes it

00:04:47,610 --> 00:04:51,660
metadata and the data coincident and

00:04:49,710 --> 00:04:53,820
because we're using terms that are

00:04:51,660 --> 00:04:56,370
already in all the other metadata

00:04:53,820 --> 00:04:59,250
schemas or schemes rather we're

00:04:56,370 --> 00:05:03,840
borrowing terms from primarily dublin

00:04:59,250 --> 00:05:05,910
core we've harmonized with tabular data

00:05:03,840 --> 00:05:08,010
packages it allows you to take a meta

00:05:05,910 --> 00:05:10,949
table file write it meta tab and then

00:05:08,010 --> 00:05:12,810
generate your tabular data package so

00:05:10,949 --> 00:05:15,419
now you've got you can use all the open

00:05:12,810 --> 00:05:17,039
knowledge tools we're looking on using

00:05:15,419 --> 00:05:18,750
this for low cost Federation Federation

00:05:17,039 --> 00:05:19,650
is not quite the right word but I think

00:05:18,750 --> 00:05:21,030
you know what I mean when I get a bunch

00:05:19,650 --> 00:05:24,300
of data and bring it together put it in

00:05:21,030 --> 00:05:26,370
a single databases we want to be able to

00:05:24,300 --> 00:05:30,000
improve open nitu workflow particularly

00:05:26,370 --> 00:05:32,430
in the department's health department's

00:05:30,000 --> 00:05:33,660
county departments that publish this and

00:05:32,430 --> 00:05:35,160
one of the features i really like for

00:05:33,660 --> 00:05:37,560
our own work is being able to published

00:05:35,160 --> 00:05:39,840
in multiple places once I've created the

00:05:37,560 --> 00:05:41,940
meta data in meta tab I can use our

00:05:39,840 --> 00:05:45,150
tooling to send it to data dot world and

00:05:41,940 --> 00:05:48,030
to see can and to wherever else I want

00:05:45,150 --> 00:05:51,479
but I manage it locally on my own drive

00:05:48,030 --> 00:05:56,210
in my own github account so here's what

00:05:51,479 --> 00:05:59,190
it looks like let's a little slow here

00:05:56,210 --> 00:06:02,250
it looks like pretty much like you would

00:05:59,190 --> 00:06:05,010
expect it to look like it's got terms

00:06:02,250 --> 00:06:07,979
and values now if you just do this you

00:06:05,010 --> 00:06:10,889
get a property list doesn't give you

00:06:07,979 --> 00:06:12,870
much help you much in managing the full

00:06:10,889 --> 00:06:14,580
complexity of JSON we have to add a few

00:06:12,870 --> 00:06:17,880
things so we can't get too far away from

00:06:14,580 --> 00:06:19,159
it looking like a table so the first

00:06:17,880 --> 00:06:24,479
thing we'll notice that these terms

00:06:19,159 --> 00:06:26,430
declare here declare is it's basically

00:06:24,479 --> 00:06:28,860
your include it tells you a little bit

00:06:26,430 --> 00:06:30,030
about the profile of the data using it

00:06:28,860 --> 00:06:32,099
defines the term it's another meta

00:06:30,030 --> 00:06:34,320
tabbed document but just ignore that for

00:06:32,099 --> 00:06:37,380
an area in there right now let's go to

00:06:34,320 --> 00:06:40,409
the next so we got these columns of

00:06:37,380 --> 00:06:41,550
terms these columns are values and like

00:06:40,409 --> 00:06:42,090
I said this doesn't give you all the

00:06:41,550 --> 00:06:48,210
complex

00:06:42,090 --> 00:06:51,300
of JSON but these terms really are in

00:06:48,210 --> 00:06:54,120
have an implicit structure to them if

00:06:51,300 --> 00:06:55,590
you don't put it in front the assumption

00:06:54,120 --> 00:06:58,680
that the parent of each of the terms is

00:06:55,590 --> 00:07:00,630
root and if you do put it there you can

00:06:58,680 --> 00:07:02,820
describe a different parent for these

00:07:00,630 --> 00:07:04,860
that allows us to generate lists and

00:07:02,820 --> 00:07:08,040
dictionaries by sometimes changing them

00:07:04,860 --> 00:07:10,410
I've added a title turn here the parser

00:07:08,040 --> 00:07:12,990
says well it's got a root title so I've

00:07:10,410 --> 00:07:14,370
created a title for my document has the

00:07:12,990 --> 00:07:16,500
value of California County building

00:07:14,370 --> 00:07:18,900
model when it sees tall robot language

00:07:16,500 --> 00:07:21,389
it makes this a child of the most

00:07:18,900 --> 00:07:24,360
recently created title so now we've got

00:07:21,389 --> 00:07:27,169
root childhood language if I put two

00:07:24,360 --> 00:07:30,570
Thai robot languages in there the JSON

00:07:27,169 --> 00:07:34,020
emitter will emit a list if I put

00:07:30,570 --> 00:07:36,570
another one like I say title dot notes

00:07:34,020 --> 00:07:38,570
or whatever it'll make it into a

00:07:36,570 --> 00:07:40,949
dictionary and depending on how you

00:07:38,570 --> 00:07:42,389
structure what you described in the

00:07:40,949 --> 00:07:43,650
declare document you can force it to

00:07:42,389 --> 00:07:45,930
always be a list or a dictionary

00:07:43,650 --> 00:07:47,729
sometimes it's always a scaler some time

00:07:45,930 --> 00:07:49,680
that's always a dick but you don't need

00:07:47,729 --> 00:07:53,660
to worry about that too much if you just

00:07:49,680 --> 00:07:56,400
omit this you'll get a a data structure

00:07:53,660 --> 00:07:57,840
now the one thing about that is we've

00:07:56,400 --> 00:07:59,550
only used the first two columns there's

00:07:57,840 --> 00:08:01,950
all this information all these columns

00:07:59,550 --> 00:08:04,020
on the other side nothing in them that

00:08:01,950 --> 00:08:06,210
should be useful right we should be able

00:08:04,020 --> 00:08:08,520
to put stuff in there and we can do that

00:08:06,210 --> 00:08:10,590
by specifying in the third column of

00:08:08,520 --> 00:08:13,139
property child so the first thing we do

00:08:10,590 --> 00:08:15,450
is out of section the section there's

00:08:13,139 --> 00:08:17,490
always an implicit one called route this

00:08:15,450 --> 00:08:19,650
case will make it explicit the section

00:08:17,490 --> 00:08:21,479
term declares a section this is called

00:08:19,650 --> 00:08:23,190
the root section that's actually

00:08:21,479 --> 00:08:24,930
different from the root parent a little

00:08:23,190 --> 00:08:28,620
bit of a bug there but it's the root

00:08:24,930 --> 00:08:31,650
section and the everything from the C

00:08:28,620 --> 00:08:34,529
column on is a list of arguments to the

00:08:31,650 --> 00:08:37,409
root that tell the parser what

00:08:34,529 --> 00:08:39,779
the subsequent terms will produce if you

00:08:37,409 --> 00:08:44,430
have a value in this column so in this

00:08:39,779 --> 00:08:47,670
case we create a title a root title term

00:08:44,430 --> 00:08:50,459
give it a value and then create a title

00:08:47,670 --> 00:08:54,410
dot language property with the value of

00:08:50,459 --> 00:08:54,410
en so when we do that

00:08:54,840 --> 00:08:59,520
see sometimes this is not working here's

00:08:57,750 --> 00:09:00,810
what this will turn out this is a small

00:08:59,520 --> 00:09:02,340
example we've got a section for

00:09:00,810 --> 00:09:05,779
resources with a data file they get a

00:09:02,340 --> 00:09:05,779
section for contacts and there we go

00:09:06,029 --> 00:09:10,830
and that's the JSON it turns out here's

00:09:09,090 --> 00:09:12,600
a full example this is actually a

00:09:10,830 --> 00:09:14,339
working example for a data project we're

00:09:12,600 --> 00:09:16,020
running right now we've got a set of

00:09:14,339 --> 00:09:19,710
terms here that are the basic stuff you

00:09:16,020 --> 00:09:21,960
get out of you'd get out of dublin core

00:09:19,710 --> 00:09:23,820
we've got a set of resources these are

00:09:21,960 --> 00:09:25,950
actually modeled on the tabular data

00:09:23,820 --> 00:09:28,110
package open knowledge --is resources

00:09:25,950 --> 00:09:30,180
each one's got our URL but also have a

00:09:28,110 --> 00:09:31,529
name a description and whatever else you

00:09:30,180 --> 00:09:34,770
want to put on there these are property

00:09:31,529 --> 00:09:37,500
children contacts and over in the schema

00:09:34,770 --> 00:09:39,300
section we have both of those ways of

00:09:37,500 --> 00:09:41,870
working here's a table and then the

00:09:39,300 --> 00:09:44,700
children of the table or columns and

00:09:41,870 --> 00:09:47,550
then these columns describe a data type

00:09:44,700 --> 00:09:49,170
on descriptions so this is this is what

00:09:47,550 --> 00:09:52,050
I mean by making it readable we can take

00:09:49,170 --> 00:09:54,600
what would normally be a complicated XML

00:09:52,050 --> 00:09:56,250
JSON or RTF file and you can hand

00:09:54,600 --> 00:09:58,020
somebody this at least the schema

00:09:56,250 --> 00:09:59,880
section it's nice we put the colors in

00:09:58,020 --> 00:10:01,530
there and they can read that without

00:09:59,880 --> 00:10:03,450
having to know anything I can open this

00:10:01,530 --> 00:10:05,190
up and now they've there it's a human

00:10:03,450 --> 00:10:08,370
readable form it's nice to convert it to

00:10:05,190 --> 00:10:09,720
other formats like HTML but if you're

00:10:08,370 --> 00:10:11,220
not doing that if you just hand in a

00:10:09,720 --> 00:10:13,400
file they know what to do with it more

00:10:11,220 --> 00:10:13,400
or less

00:10:13,970 --> 00:10:19,710
so you currently use the system by

00:10:17,490 --> 00:10:22,920
installing a Python module

00:10:19,710 --> 00:10:25,230
it's called meta cab and it introduces a

00:10:22,920 --> 00:10:27,810
meta pack program you can also build

00:10:25,230 --> 00:10:29,640
packages in Google Spreadsheets that's a

00:10:27,810 --> 00:10:32,010
little crusty right now but it's going

00:10:29,640 --> 00:10:33,480
to get some well soon and we're working

00:10:32,010 --> 00:10:36,290
on getting some trying to find some

00:10:33,480 --> 00:10:38,820
funding to build an Excel and Excel

00:10:36,290 --> 00:10:42,270
module or Excel add-on allows you to

00:10:38,820 --> 00:10:46,290
build packages in Excel oops let's play

00:10:42,270 --> 00:10:49,050
this again once you build a package with

00:10:46,290 --> 00:10:51,180
meta pack you can tell meta pack to turn

00:10:49,050 --> 00:10:53,190
that into once you built the meta tab

00:10:51,180 --> 00:10:55,020
file as a CSV file with the metadata in

00:10:53,190 --> 00:10:58,680
it you can build packages you can build

00:10:55,020 --> 00:11:00,839
a zip file that downloads all the URLs

00:10:58,680 --> 00:11:02,880
you referenced in the meta data and

00:11:00,839 --> 00:11:04,980
sticks it in your zip file you can build

00:11:02,880 --> 00:11:08,359
an excel file that has all the data

00:11:04,980 --> 00:11:10,009
embedded as tabs there's a couple other

00:11:08,359 --> 00:11:12,019
you can put it on the web you can use

00:11:10,009 --> 00:11:13,519
packages directly from the web if you

00:11:12,019 --> 00:11:15,259
upload something to date it out world

00:11:13,519 --> 00:11:17,989
you can download those directly but you

00:11:15,259 --> 00:11:22,009
can also use the Python module to get

00:11:17,989 --> 00:11:25,639
access to that package and then get data

00:11:22,009 --> 00:11:27,709
frames in with you can get canvas data

00:11:25,639 --> 00:11:29,539
frames you can get geo pandas dataframes

00:11:27,709 --> 00:11:35,109
a couple other forms because the data

00:11:29,539 --> 00:11:36,859
package understands the schemas so I

00:11:35,109 --> 00:11:39,109
mentioned all this here's a picture of

00:11:36,859 --> 00:11:40,639
data dot world one of the important

00:11:39,109 --> 00:11:42,709
things about this is that when you build

00:11:40,639 --> 00:11:45,109
a package if you build a zip or a file

00:11:42,709 --> 00:11:48,529
system package it has it automatically

00:11:45,109 --> 00:11:51,049
creates a data package JSON file so this

00:11:48,529 --> 00:11:53,539
also works as a tabular data package you

00:11:51,049 --> 00:11:59,179
can use open knowledge tabular data

00:11:53,539 --> 00:12:00,889
package tooling on that zip file here's

00:11:59,179 --> 00:12:02,029
an example of what this looks like when

00:12:00,889 --> 00:12:02,689
you're actually running it this is not

00:12:02,029 --> 00:12:07,399
working out too well

00:12:02,689 --> 00:12:11,269
so you pip install meta tab meta pack -

00:12:07,399 --> 00:12:13,909
C is going to create a new file you open

00:12:11,269 --> 00:12:15,649
it up in your Excel or I'll open it up

00:12:13,909 --> 00:12:18,109
however you open you open up OpenOffice

00:12:15,649 --> 00:12:19,939
or Excel then you can go and add your

00:12:18,109 --> 00:12:23,359
title your descriptions whatever else

00:12:19,939 --> 00:12:25,909
you want meta pack - a will go load in a

00:12:23,359 --> 00:12:28,220
URL if it's just the URL to a CSV file

00:12:25,909 --> 00:12:31,099
it'll stick it in as a resource if it's

00:12:28,220 --> 00:12:33,319
a URL to a zip file it'll open the zip

00:12:31,099 --> 00:12:36,259
file and look for CSV and Excel files

00:12:33,319 --> 00:12:38,119
and stick all those in if it's a URL to

00:12:36,259 --> 00:12:40,819
a web page you'll scrape the web page

00:12:38,119 --> 00:12:43,009
looking for stuff and if it's a URL to a

00:12:40,819 --> 00:12:46,069
web page with zip files that have Excel

00:12:43,009 --> 00:12:48,109
files you'll get hundreds of URLs

00:12:46,069 --> 00:12:51,259
because it will go all the way down to

00:12:48,109 --> 00:12:52,909
the level of the individual tabs and

00:12:51,259 --> 00:12:56,749
extract the data and put them in these

00:12:52,909 --> 00:12:59,539
URLs from there you can generate schemas

00:12:56,749 --> 00:13:01,849
with - s it'll try to intuit all the

00:12:59,539 --> 00:13:03,409
types of data all the column values and

00:13:01,849 --> 00:13:06,819
names they'll produce your data

00:13:03,409 --> 00:13:09,379
dictionary for you you can upload it to

00:13:06,819 --> 00:13:12,289
get up on all the packages s3 with

00:13:09,379 --> 00:13:14,509
medicine then meta can sticks it on to C

00:13:12,289 --> 00:13:17,239
can meta world's fix it on to data dot

00:13:14,509 --> 00:13:19,710
world this about 15 minutes you've

00:13:17,239 --> 00:13:22,440
created multiple data packages

00:13:19,710 --> 00:13:25,260
multiple repositories with complete

00:13:22,440 --> 00:13:27,060
schemas and then can use those in a

00:13:25,260 --> 00:13:29,580
variety of tooling there's a couple of

00:13:27,060 --> 00:13:32,280
neat features on the system one is that

00:13:29,580 --> 00:13:37,050
you can use a program as a source of

00:13:32,280 --> 00:13:40,290
data or a Jupiter notebook so geocoding

00:13:37,050 --> 00:13:42,150
is a bit of a trick sometimes and so we

00:13:40,290 --> 00:13:44,490
write geo coder programs reference them

00:13:42,150 --> 00:13:46,350
from meta tab and I build a package it

00:13:44,490 --> 00:13:49,560
does all the geocoding fix the resulting

00:13:46,350 --> 00:13:51,630
raw data into a CSV file and I upload

00:13:49,560 --> 00:13:53,880
that for my analysts you can also define

00:13:51,630 --> 00:13:55,560
column transforms which are bits of

00:13:53,880 --> 00:13:57,390
Python code attached to an individual

00:13:55,560 --> 00:13:59,580
column which allows you to parse the

00:13:57,390 --> 00:14:02,910
column and and do work as that file is

00:13:59,580 --> 00:14:06,510
building so we use this for if you have

00:14:02,910 --> 00:14:08,580
a geo ID from the Census and you want to

00:14:06,510 --> 00:14:09,630
call on that references the county

00:14:08,580 --> 00:14:12,110
you're going to you know you're going to

00:14:09,630 --> 00:14:15,810
be doing aggregations on County you can

00:14:12,110 --> 00:14:17,580
parse the in the meta tab file you can

00:14:15,810 --> 00:14:20,400
parse you can tell it to parse the geo

00:14:17,580 --> 00:14:23,880
ID and then spit out the county or the

00:14:20,400 --> 00:14:27,930
state this is what it looks like when

00:14:23,880 --> 00:14:29,430
you upload a package to a sea-can

00:14:27,930 --> 00:14:30,930
instance this is one of the projects

00:14:29,430 --> 00:14:33,390
were a volunteer project we're running

00:14:30,930 --> 00:14:34,980
for a group that deals with residential

00:14:33,390 --> 00:14:37,020
elder care facilities and so this is

00:14:34,980 --> 00:14:42,140
actually for a real client once you

00:14:37,020 --> 00:14:44,430
upload you get the all the same thing

00:14:42,140 --> 00:14:47,490
you'd all the same documentation that

00:14:44,430 --> 00:14:49,800
you saw earlier you also get the excel

00:14:47,490 --> 00:14:52,020
file you get a zip file and this is

00:14:49,800 --> 00:14:54,210
called the CSV package is just a raw

00:14:52,020 --> 00:14:59,570
data tab file that references URLs from

00:14:54,210 --> 00:15:04,590
elsewhere if you load that URL to that

00:14:59,570 --> 00:15:07,730
meta tab file into the Python module and

00:15:04,590 --> 00:15:09,750
print out the doc in in this is a

00:15:07,730 --> 00:15:11,340
Jupiter notebook you get all the

00:15:09,750 --> 00:15:16,230
documentation including the names of the

00:15:11,340 --> 00:15:17,640
resources and if you ask for one of

00:15:16,230 --> 00:15:21,690
those resources and print that out

00:15:17,640 --> 00:15:23,790
you'll get the data dictionary and if

00:15:21,690 --> 00:15:25,620
you ask for data frame we get a panda's

00:15:23,790 --> 00:15:27,210
data frame now my analyst will go

00:15:25,620 --> 00:15:29,160
through this process so load in multiple

00:15:27,210 --> 00:15:32,360
packages they'll do all their analysis

00:15:29,160 --> 00:15:35,060
work then we take the

00:15:32,360 --> 00:15:37,070
resulting pandas document refer it to

00:15:35,060 --> 00:15:40,100
and from another minute AB package I

00:15:37,070 --> 00:15:42,320
build that upload that to theta dot

00:15:40,100 --> 00:15:47,779
world and give that to my client so

00:15:42,320 --> 00:15:50,660
that's our workflow process so at this

00:15:47,779 --> 00:15:52,399
point I can either do a demo of it or

00:15:50,660 --> 00:15:56,329
just take questions

00:15:52,399 --> 00:15:59,870
oops you have a preference anybody any

00:15:56,329 --> 00:16:09,019
questions alright we'll do a demo then

00:15:59,870 --> 00:16:11,600
let's see H okay so so I'm going to tack

00:16:09,019 --> 00:16:14,990
- see that will create that meta tab

00:16:11,600 --> 00:16:23,630
file and let's go figure out where I put

00:16:14,990 --> 00:16:29,769
the here this if I go reload this so

00:16:23,630 --> 00:16:34,480
this is the default template it's got a

00:16:29,769 --> 00:16:34,480
UID and a name that we need to change

00:16:40,309 --> 00:16:46,679
we have these items here allow you to

00:16:44,729 --> 00:16:48,869
set the parts of the name our names are

00:16:46,679 --> 00:16:51,509
structured and we'll call this one a

00:16:48,869 --> 00:16:52,769
food data and it has an origin origin is

00:16:51,509 --> 00:16:55,069
where it came from we'll just make an

00:16:52,769 --> 00:16:57,720
example calm but usually we put the

00:16:55,069 --> 00:16:59,639
agency that it came from we've got a

00:16:57,720 --> 00:17:05,720
version this one comes from let's say at

00:16:59,639 --> 00:17:05,720
the California and then when I save that

00:17:09,559 --> 00:17:18,589
that will update and when we go back and

00:17:12,689 --> 00:17:18,589
reload oops let's see we want to go this

00:17:21,260 --> 00:17:27,360
will now have a new name it's updated

00:17:24,269 --> 00:17:30,870
the name and let me get rid of this one

00:17:27,360 --> 00:17:38,760
because that's an example you can make

00:17:30,870 --> 00:17:41,850
this a little bigger okay from there we

00:17:38,760 --> 00:17:44,850
can go amid a pack - a and I think I've

00:17:41,850 --> 00:17:48,059
got yeah so this is an excel file so

00:17:44,850 --> 00:17:50,429
it's got multiple tabs within it it'll

00:17:48,059 --> 00:17:59,070
go through and inspect them all load

00:17:50,429 --> 00:18:03,779
them in and reload again and so now we

00:17:59,070 --> 00:18:05,940
have URLs for all of the files and you

00:18:03,779 --> 00:18:10,470
can see they're fragments the fragments

00:18:05,940 --> 00:18:11,879
name here in the URL is the tab that it

00:18:10,470 --> 00:18:13,919
came from and we've got a fairly

00:18:11,879 --> 00:18:15,480
complicated URL scheme so that just

00:18:13,919 --> 00:18:15,870
about any file that might have data in

00:18:15,480 --> 00:18:19,879
it

00:18:15,870 --> 00:18:22,230
will have a corresponding URL

00:18:19,879 --> 00:18:23,460
they've also it's also given us names

00:18:22,230 --> 00:18:25,909
that are a little too long we're going

00:18:23,460 --> 00:18:25,909
to change those

00:18:28,220 --> 00:18:37,549
oops and you can see a start line and a

00:18:35,870 --> 00:18:38,840
header line it tries to figure out where

00:18:37,549 --> 00:18:40,669
the roads are if you have multiple

00:18:38,840 --> 00:18:41,899
headers it'll skip over there and try to

00:18:40,669 --> 00:18:42,950
figure out where the header line is and

00:18:41,899 --> 00:18:44,419
where the data start is it doesn't

00:18:42,950 --> 00:18:46,370
always get it right particularly if it's

00:18:44,419 --> 00:18:47,870
a lot of text so in this case it doesn't

00:18:46,370 --> 00:18:51,379
no we're just going to have to change

00:18:47,870 --> 00:18:55,129
those but oftentimes it will do a great

00:18:51,379 --> 00:19:05,090
job of skipping the comments and now if

00:18:55,129 --> 00:19:09,230
I save this and rerun it's going to

00:19:05,090 --> 00:19:17,290
write schema ooh what did I do and save

00:19:09,230 --> 00:19:18,610
it huh

00:19:17,290 --> 00:19:22,080
well because I need to get rid of this

00:19:18,610 --> 00:19:22,080
one so it's got some pleat though

00:19:27,520 --> 00:19:38,900
so now it's processing those files and

00:19:31,400 --> 00:19:41,930
it has added in a it's added in the in

00:19:38,900 --> 00:19:44,380
the schema section we now have tables

00:19:41,930 --> 00:19:46,760
and columns you can also see that it has

00:19:44,380 --> 00:19:48,530
detected that these names or crumby

00:19:46,760 --> 00:19:50,330
names for columns so it created new ones

00:19:48,530 --> 00:19:51,800
for me and there's a bunch of other

00:19:50,330 --> 00:19:54,350
stuff this is where you'd also add the

00:19:51,800 --> 00:19:56,960
Python code references to modify things

00:19:54,350 --> 00:19:59,570
as it gets built once you've done that

00:19:56,960 --> 00:20:01,880
this is basically a complete metadata

00:19:59,570 --> 00:20:03,860
you probably want more stuff in there

00:20:01,880 --> 00:20:06,170
references the documentation we probably

00:20:03,860 --> 00:20:07,970
should figure fill in the contacts who

00:20:06,170 --> 00:20:10,940
is the Wrangler and who created and all

00:20:07,970 --> 00:20:14,780
other stuff but then you from there you

00:20:10,940 --> 00:20:15,740
can run your run - after we'll make a

00:20:14,780 --> 00:20:20,900
file system oops

00:20:15,740 --> 00:20:26,120
no corrective still - Z - Z it's going

00:20:20,900 --> 00:20:29,110
to create a zip zip package and an excel

00:20:26,120 --> 00:20:30,800
package so now if we look in the

00:20:29,110 --> 00:20:33,680
packages directory

00:20:30,800 --> 00:20:42,530
we've got a zip package in an excel

00:20:33,680 --> 00:20:45,860
package and if we open up the this is

00:20:42,530 --> 00:20:47,540
what are not terribly nicely formatted

00:20:45,860 --> 00:20:50,450
but you can see it's pretty much the

00:20:47,540 --> 00:20:52,190
same data a couple changes our urls have

00:20:50,450 --> 00:20:54,590
been turned into simple names because

00:20:52,190 --> 00:20:58,400
those are the names of the tabs in there

00:20:54,590 --> 00:21:00,950
and then it's loaded in all of our it's

00:20:58,400 --> 00:21:03,260
loaded in all of our data as tabs using

00:21:00,950 --> 00:21:05,540
those names if we defined any cleanup

00:21:03,260 --> 00:21:07,610
routines in the schema this data will

00:21:05,540 --> 00:21:10,730
not be clean with whatever parsing we

00:21:07,610 --> 00:21:12,590
declared my analysts use this a lot

00:21:10,730 --> 00:21:14,330
because we get lots of crummy data and

00:21:12,590 --> 00:21:16,940
we can programmatically declare what

00:21:14,330 --> 00:21:18,980
we're doing I demand that because I

00:21:16,940 --> 00:21:20,570
don't want my analysts going and mucking

00:21:18,980 --> 00:21:22,940
with CSV files when you're publishing I

00:21:20,570 --> 00:21:26,120
want to know what did you do so I make

00:21:22,940 --> 00:21:30,140
them write Python codes to that from

00:21:26,120 --> 00:21:32,720
here you can upload it to Amazon s3 and

00:21:30,140 --> 00:21:36,860
then deploy it into your various

00:21:32,720 --> 00:21:39,340
repositories so the let's see let's go

00:21:36,860 --> 00:21:39,340
back to

00:21:41,049 --> 00:21:46,489
if you want more information about the

00:21:43,190 --> 00:21:48,589
project meditate org we're at a stage

00:21:46,489 --> 00:21:50,209
where I've been using this in our own

00:21:48,589 --> 00:21:52,820
work and I'm looking forward to

00:21:50,209 --> 00:21:54,409
deploying it we're talking with a couple

00:21:52,820 --> 00:21:56,299
organizations and talking to the babies

00:21:54,409 --> 00:21:58,339
out world folks are awesome and open

00:21:56,299 --> 00:22:01,129
knowledge about making this more or less

00:21:58,339 --> 00:22:02,719
a front-end to data package that JSON

00:22:01,129 --> 00:22:04,039
you like I said you could its harmonized

00:22:02,719 --> 00:22:05,629
with data package saturation it will

00:22:04,039 --> 00:22:10,159
generate those files but it's easier to

00:22:05,629 --> 00:22:12,889
create and I'm looking towards having a

00:22:10,159 --> 00:22:15,709
more reliable expectation that data

00:22:12,889 --> 00:22:18,249
producers particularly ones who produce

00:22:15,709 --> 00:22:20,809
public data and health departments have

00:22:18,249 --> 00:22:23,749
metadata available and that it's

00:22:20,809 --> 00:22:26,179
available in some sort of format that I

00:22:23,749 --> 00:22:27,919
can read if it's JSON great I or if it's

00:22:26,179 --> 00:22:31,339
but if it takes meta tab to get them to

00:22:27,919 --> 00:22:33,709
do that that's why we're promoting this

00:22:31,339 --> 00:22:36,109
we're very happy to have people help

00:22:33,709 --> 00:22:38,389
with the project and if you have any use

00:22:36,109 --> 00:22:39,859
cases for it and would like to deploy

00:22:38,389 --> 00:22:43,489
something like this in your environment

00:22:39,859 --> 00:22:48,019
please let me know particularly if you

00:22:43,489 --> 00:22:50,389
have a need for the Excel version of

00:22:48,019 --> 00:22:54,200
this where I guess I we're looking for a

00:22:50,389 --> 00:22:56,419
core user that we can go justify asking

00:22:54,200 --> 00:22:59,809
for funding to build a tool that can do

00:22:56,419 --> 00:23:03,429
this in Excel thank you very much if we

00:22:59,809 --> 00:23:03,429
get a couple minutes for questions

00:23:06,330 --> 00:23:12,410

YouTube URL: https://www.youtube.com/watch?v=RskLOHepvyc


