Title: Machine Learning with Open Source Infrastructure
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speakers: Anish Asthana, Sherard Griffin, and Daniel Wolf

As data is exponentially growing in organizations, there is an increasing need to consolidate silos of information into a single source of truth, a 'Data Lake' to feed hungry Analytics and Machine Learning Engines that can gather insight at scale. In this talk, we will provide an overview of the Open Data Hub architecture. Then we will highlight a current data science use case leveraging Red Hat OpenShift, Ceph Storage, and analytics with Spark and SKLearn on JupyterHub.
Captions: 
	00:00:02,630 --> 00:00:07,770
hey guys can everyone hear me okay

00:00:05,720 --> 00:00:10,350
awesome

00:00:07,770 --> 00:00:14,309
yeah so I can introduce me um I'm in

00:00:10,350 --> 00:00:16,590
each asana on the data hub team in the a

00:00:14,309 --> 00:00:19,200
I Center of Excellence at red hat and

00:00:16,590 --> 00:00:20,850
I'm Daniel Wolff senior data engineer

00:00:19,200 --> 00:00:21,690
within the products and technologies

00:00:20,850 --> 00:00:24,180
organization

00:00:21,690 --> 00:00:26,340
yeah and we're here to talk to you about

00:00:24,180 --> 00:00:29,070
machine learning with open source

00:00:26,340 --> 00:00:32,930
infrastructures more specifically we'll

00:00:29,070 --> 00:00:35,700
be talking about the open data hub and

00:00:32,930 --> 00:00:40,620
one of our internal users like the

00:00:35,700 --> 00:00:43,860
Crocket team so the open data hub was

00:00:40,620 --> 00:00:45,720
originally started to address some

00:00:43,860 --> 00:00:48,780
internal problems were facing around

00:00:45,720 --> 00:00:50,130
machine learning at Red Hat instead of

00:00:48,780 --> 00:00:52,829
having a number of different teams

00:00:50,130 --> 00:00:56,250
manage their own infrastructure you know

00:00:52,829 --> 00:00:59,190
develop their own support mechanisms and

00:00:56,250 --> 00:01:01,500
security policies we figured having a

00:00:59,190 --> 00:01:04,829
centralized location called the data hub

00:01:01,500 --> 00:01:07,319
where teams could share data and run

00:01:04,829 --> 00:01:10,049
their ml and AI workloads made more

00:01:07,319 --> 00:01:12,179
sense that way we could free our end

00:01:10,049 --> 00:01:14,429
users from having to worry about the

00:01:12,179 --> 00:01:16,380
complexities of managing these systems

00:01:14,429 --> 00:01:17,630
and then focus on what's really

00:01:16,380 --> 00:01:20,670
interesting to them right like

00:01:17,630 --> 00:01:25,079
experimentation getting inside building

00:01:20,670 --> 00:01:28,469
cool stuff the open data hub is a

00:01:25,079 --> 00:01:30,630
natural evolution of that it isn't it is

00:01:28,469 --> 00:01:32,459
a meta open source project that brings

00:01:30,630 --> 00:01:35,369
together a number of open source

00:01:32,459 --> 00:01:37,649
technologies in like data and machine

00:01:35,369 --> 00:01:40,979
learning pipelines all running on

00:01:37,649 --> 00:01:42,450
kubernetes or in our case openshift one

00:01:40,979 --> 00:01:44,939
important thing to note is that it's

00:01:42,450 --> 00:01:47,009
crowded Gnostic so you can run anywhere

00:01:44,939 --> 00:01:51,479
you can run open shift so if you want to

00:01:47,009 --> 00:01:52,319
run on AWS or on pram or on GCE feel

00:01:51,479 --> 00:01:53,639
free to do so

00:01:52,319 --> 00:01:56,819
right or on all three of them at the

00:01:53,639 --> 00:02:01,709
same time you find a community at open

00:01:56,819 --> 00:02:03,299
data hub dot IO so this is a reference

00:02:01,709 --> 00:02:05,369
architecture diagram for the open data

00:02:03,299 --> 00:02:07,610
hub I know it looks like hot but I'll

00:02:05,369 --> 00:02:10,500
walk you through it

00:02:07,610 --> 00:02:14,190
the components you can see in this

00:02:10,500 --> 00:02:16,110
diagram keppo it can be broken into

00:02:14,190 --> 00:02:18,780
roughly three categories

00:02:16,110 --> 00:02:21,750
the first category is technologies of

00:02:18,780 --> 00:02:23,880
products we projects we are looking into

00:02:21,750 --> 00:02:26,700
and betting to see if they would really

00:02:23,880 --> 00:02:29,880
fulfill the needs we have so things like

00:02:26,700 --> 00:02:33,090
red Red Hat data grid or three span

00:02:29,880 --> 00:02:35,640
would sort of fit in there the second

00:02:33,090 --> 00:02:37,740
set of components is technologies we

00:02:35,640 --> 00:02:39,330
have proved work for us internally and

00:02:37,740 --> 00:02:41,490
meet our requirements but we haven't

00:02:39,330 --> 00:02:44,640
integrated yet into the open data hub

00:02:41,490 --> 00:02:45,890
operator and the third set and as like

00:02:44,640 --> 00:02:49,350
technology so that would be like

00:02:45,890 --> 00:02:50,970
elasticsearch for example or Hugh and

00:02:49,350 --> 00:02:53,310
the third set of technologies and

00:02:50,970 --> 00:02:58,110
projects is stuff that we've proved

00:02:53,310 --> 00:02:59,610
works and is a part of the operator so

00:02:58,110 --> 00:03:03,180
if you draw your attention to the bottom

00:02:59,610 --> 00:03:05,840
of the image moves the open shift so

00:03:03,180 --> 00:03:08,910
open shift is Red Hat Enterprise

00:03:05,840 --> 00:03:11,130
kubernetes distribution it's the

00:03:08,910 --> 00:03:12,600
container orchestration engine so and

00:03:11,130 --> 00:03:15,750
that's what we're running open data hub

00:03:12,600 --> 00:03:19,040
on this is what lets us scale up to meet

00:03:15,750 --> 00:03:22,260
any requirements or run on any cloud

00:03:19,040 --> 00:03:25,739
moving up a little bit you can see like

00:03:22,260 --> 00:03:27,959
the data engineer persona on the left so

00:03:25,739 --> 00:03:30,300
data engineers are responsible for

00:03:27,959 --> 00:03:33,300
building out big data infrastructures

00:03:30,300 --> 00:03:35,670
what this really means is that they're

00:03:33,300 --> 00:03:37,290
responsible for developing and they

00:03:35,670 --> 00:03:39,510
planning out systems that can

00:03:37,290 --> 00:03:42,450
incorporate data from different sources

00:03:39,510 --> 00:03:45,840
and store it in one location or multiple

00:03:42,450 --> 00:03:50,340
I guess for you users to like play with

00:03:45,840 --> 00:03:53,940
so to speak the legend you are generally

00:03:50,340 --> 00:03:57,510
dealing with two main types of data data

00:03:53,940 --> 00:03:59,280
data and motion which is like data that

00:03:57,510 --> 00:04:00,810
may be flowing from outside your system

00:03:59,280 --> 00:04:03,269
into your system right left for point A

00:04:00,810 --> 00:04:04,980
to point B and then data in rest which

00:04:03,269 --> 00:04:07,070
is data that's already in your system

00:04:04,980 --> 00:04:12,709
how are you storing it effectively

00:04:07,070 --> 00:04:18,150
so with those like - sort of categories

00:04:12,709 --> 00:04:21,120
defined we are using Kafka log stash and

00:04:18,150 --> 00:04:22,500
swindie primarily internally and as part

00:04:21,120 --> 00:04:24,660
of the open data hub operator you can

00:04:22,500 --> 00:04:26,030
use Kafka and then from a storage

00:04:24,660 --> 00:04:28,400
perspective

00:04:26,030 --> 00:04:31,310
we settled on using staff as our data

00:04:28,400 --> 00:04:33,950
leak for unstructured data we're looking

00:04:31,310 --> 00:04:35,990
into RedHat data grid as an in-memory

00:04:33,950 --> 00:04:38,320
storage option and then for your

00:04:35,990 --> 00:04:42,050
structured data you can really use any

00:04:38,320 --> 00:04:52,610
database you like we use for stress

00:04:42,050 --> 00:04:54,950
equal for the most part yes yeah you can

00:04:52,610 --> 00:04:56,450
have more we have depending on your

00:04:54,950 --> 00:05:00,070
infrastructures you may have multiples

00:04:56,450 --> 00:05:02,330
so some of our internal users they have

00:05:00,070 --> 00:05:04,280
or their systems have integrations

00:05:02,330 --> 00:05:07,280
button with our syslog so it made sense

00:05:04,280 --> 00:05:09,500
for us to incorporate that yeah you can

00:05:07,280 --> 00:05:13,340
use any you really need only one from

00:05:09,500 --> 00:05:16,070
each layer so once you have your data

00:05:13,340 --> 00:05:17,810
and your data like you you know done

00:05:16,070 --> 00:05:19,310
react right you need to actually get

00:05:17,810 --> 00:05:21,800
some insight or value out of that data

00:05:19,310 --> 00:05:23,630
and this is where your data scientists

00:05:21,800 --> 00:05:24,940
and your business analyst come into the

00:05:23,630 --> 00:05:28,370
picture

00:05:24,940 --> 00:05:29,600
if there's just some light visualization

00:05:28,370 --> 00:05:31,700
work they want to do you know just get a

00:05:29,600 --> 00:05:34,190
very rough idea for the data may look

00:05:31,700 --> 00:05:36,890
like and then you can use projects like

00:05:34,190 --> 00:05:39,169
UK banana or super set to look into it

00:05:36,890 --> 00:05:40,729
but if your data scientist wants to do

00:05:39,169 --> 00:05:42,410
something a little more involved right

00:05:40,729 --> 00:05:44,870
like with processing data or

00:05:42,410 --> 00:05:46,700
transforming it or starting to train

00:05:44,870 --> 00:05:49,280
models on it they can use Jupiter hub

00:05:46,700 --> 00:05:51,979
alongside spark with whatever you know

00:05:49,280 --> 00:05:54,530
Python whatever the language of choice

00:05:51,979 --> 00:05:56,800
you like with whatever like machine

00:05:54,530 --> 00:05:59,390
learning libraries you want to use now

00:05:56,800 --> 00:06:02,240
once your data scientist is done with

00:05:59,390 --> 00:06:03,740
like creating a model they probably want

00:06:02,240 --> 00:06:05,780
to deploy it somewheres that you have

00:06:03,740 --> 00:06:08,419
end-users sort of testing it out right

00:06:05,780 --> 00:06:11,300
so for that we are using Selden to serve

00:06:08,419 --> 00:06:14,300
models which data scientists have to

00:06:11,300 --> 00:06:16,310
developed the final bit I want to talk

00:06:14,300 --> 00:06:19,700
about in that portion is actually about

00:06:16,310 --> 00:06:23,180
the open data hub AI library the yeah

00:06:19,700 --> 00:06:25,789
library is a set of pre-built

00:06:23,180 --> 00:06:28,940
statistical and machine learning modules

00:06:25,789 --> 00:06:30,650
that any user can download and can

00:06:28,940 --> 00:06:33,169
quickly get started with for rapid

00:06:30,650 --> 00:06:35,300
prototyping so say if I'm not a data

00:06:33,169 --> 00:06:37,340
scientist but I have a lot of data and I

00:06:35,300 --> 00:06:39,550
want to sort of test out what I can do

00:06:37,340 --> 00:06:41,560
with it you can download the AI library

00:06:39,550 --> 00:06:46,210
and sort of get started with some

00:06:41,560 --> 00:06:47,170
prototyping work very easily now if you

00:06:46,210 --> 00:06:50,890
look over to the other side of the

00:06:47,170 --> 00:06:53,260
diagram you'll see the data strip data

00:06:50,890 --> 00:06:56,200
stored so these folks are responsible

00:06:53,260 --> 00:06:58,090
for restricting access to your data or

00:06:56,200 --> 00:06:59,860
to your platform right they're

00:06:58,090 --> 00:07:02,140
interested in just authentic ation and

00:06:59,860 --> 00:07:11,470
credentials so for them internally we

00:07:02,140 --> 00:07:14,590
have so Jupiter hub is yeah triple app

00:07:11,470 --> 00:07:16,300
yeah yeah so we actually have it running

00:07:14,590 --> 00:07:18,990
and you can deploy it as part of the

00:07:16,300 --> 00:07:18,990
operator

00:07:32,719 --> 00:07:43,549
yes you can sorry where was I yeah so

00:07:41,329 --> 00:07:45,199
your data stewards are concerned with

00:07:43,549 --> 00:07:48,319
restricting access to your data

00:07:45,199 --> 00:07:52,549
so for them we have like currently using

00:07:48,319 --> 00:07:54,589
Red Hat OpenShift OAuth and the staff

00:07:52,549 --> 00:07:57,139
object store for credential management

00:07:54,589 --> 00:07:58,369
and with all in the case of OpenShift

00:07:57,139 --> 00:08:00,589
OAuth you can integrate with the held

00:07:58,369 --> 00:08:03,529
app so it's very easy to restrict access

00:08:00,589 --> 00:08:07,069
to certain components which are

00:08:03,529 --> 00:08:09,379
sensitive finally you have your DevOps

00:08:07,069 --> 00:08:11,029
engineer's right these guys are the ones

00:08:09,379 --> 00:08:13,159
who are responsible for keeping the

00:08:11,029 --> 00:08:15,889
lights on so they're interested in

00:08:13,159 --> 00:08:18,139
monitoring the health of the system

00:08:15,889 --> 00:08:19,610
right seeing if something is down how

00:08:18,139 --> 00:08:22,129
the system is performing with link

00:08:19,610 --> 00:08:23,899
metrics for that use case we have

00:08:22,129 --> 00:08:26,739
settled on using Prometheus ingre fauna

00:08:23,899 --> 00:08:30,619
since they brought up pretty easy way to

00:08:26,739 --> 00:08:32,539
scrape metrics and visualize them the

00:08:30,619 --> 00:08:36,409
last thing I want to talk about is jobs

00:08:32,539 --> 00:08:37,639
right like you you have jobs your users

00:08:36,409 --> 00:08:39,559
may have jobs they need to run on a

00:08:37,639 --> 00:08:42,789
semi-regular basis right and they can

00:08:39,559 --> 00:08:44,750
vary in scope from something simple like

00:08:42,789 --> 00:08:46,790
backups for like you elastic search

00:08:44,750 --> 00:08:48,139
indices to something a lot more

00:08:46,790 --> 00:08:50,509
complicated for like some data

00:08:48,139 --> 00:08:52,250
transformations for like a multi stage

00:08:50,509 --> 00:08:56,689
data transformation for your machine

00:08:52,250 --> 00:08:58,959
learning modules you could use cron jobs

00:08:56,689 --> 00:09:01,189
for that but they're not very robust and

00:08:58,959 --> 00:09:02,959
not very reliable right like if they

00:09:01,189 --> 00:09:06,230
fail you have no idea what happened to

00:09:02,959 --> 00:09:10,519
that and we settled on using Argo and

00:09:06,230 --> 00:09:12,050
Jenkins to manage our jobs this is not

00:09:10,519 --> 00:09:14,750
distribution right we do have some

00:09:12,050 --> 00:09:16,339
components already integrated in and if

00:09:14,750 --> 00:09:19,120
you were to go to the gate lab

00:09:16,339 --> 00:09:21,350
repository and download the operator

00:09:19,120 --> 00:09:25,040
here's what you can deploy out of the

00:09:21,350 --> 00:09:29,269
box so you can deploy premedia Center

00:09:25,040 --> 00:09:32,269
for now for monitoring obviously Seldon

00:09:29,269 --> 00:09:34,129
spark and chapter hub for data

00:09:32,269 --> 00:09:35,839
processing analysis and then model

00:09:34,129 --> 00:09:38,480
surveying and then finally staff and

00:09:35,839 --> 00:09:42,410
Kafka for your data storage data and

00:09:38,480 --> 00:09:44,209
motion needs to answer that generals

00:09:42,410 --> 00:09:45,150
question you don't have to deploy any

00:09:44,209 --> 00:09:47,670
component here

00:09:45,150 --> 00:09:49,110
want to so if you'd like Jupiter lab a

00:09:47,670 --> 00:09:51,510
lot and you don't care about Jupiter hub

00:09:49,110 --> 00:09:53,550
you can just tell the operator not to

00:09:51,510 --> 00:09:55,830
deploy chapter hub and Jupiter lab will

00:09:53,550 --> 00:09:57,870
be at the slot in there perfectly right

00:09:55,830 --> 00:10:04,110
the operator just makes it easy to

00:09:57,870 --> 00:10:05,820
deploy let go get a prototype going next

00:10:04,110 --> 00:10:08,010
I'll be talking a little bit about some

00:10:05,820 --> 00:10:10,710
of our deploy a practical deployments

00:10:08,010 --> 00:10:12,750
right now the first one I'm going to

00:10:10,710 --> 00:10:16,590
talk about is for the Massachusetts open

00:10:12,750 --> 00:10:18,120
cloud or moc for short the MST is a

00:10:16,590 --> 00:10:20,040
collaboration between a number of

00:10:18,120 --> 00:10:23,880
universities in the Greater Boston area

00:10:20,040 --> 00:10:27,900
as well as some industry partners to

00:10:23,880 --> 00:10:30,500
create an open public cloud for you know

00:10:27,900 --> 00:10:32,580
researchers in academia or like

00:10:30,500 --> 00:10:35,400
nonprofits or in an industry to

00:10:32,580 --> 00:10:37,380
collaborate on and innovate in we have

00:10:35,400 --> 00:10:39,120
an open data hub deployment in the MOOC

00:10:37,380 --> 00:10:42,330
right to provide a platform for these

00:10:39,120 --> 00:10:45,210
researchers and nonprofits to develop AI

00:10:42,330 --> 00:10:48,720
services organic again collaborating on

00:10:45,210 --> 00:10:50,850
and driving value for them as a side

00:10:48,720 --> 00:10:53,280
note if you are a researcher or a

00:10:50,850 --> 00:10:55,740
non-profit in the Greater Boston area

00:10:53,280 --> 00:10:58,380
and you're interested in working with

00:10:55,740 --> 00:11:03,300
moc feel free to reach out we can put

00:10:58,380 --> 00:11:04,470
you in touch with some folks there and

00:11:03,300 --> 00:11:05,910
then the second bit of talking about is

00:11:04,470 --> 00:11:09,270
the internal data hub right which is

00:11:05,910 --> 00:11:12,150
where it all started we have three main

00:11:09,270 --> 00:11:13,680
goals internally the first was somewhat

00:11:12,150 --> 00:11:16,530
linked together so I'll talk about them

00:11:13,680 --> 00:11:18,590
together we want to serve as customer

00:11:16,530 --> 00:11:21,840
zero for any new open data hub

00:11:18,590 --> 00:11:24,450
components and we want to prove that the

00:11:21,840 --> 00:11:29,340
open data hub can run in a highly highly

00:11:24,450 --> 00:11:31,530
available man or at scale to expand on

00:11:29,340 --> 00:11:33,930
that I dumped all of these new

00:11:31,530 --> 00:11:35,430
technologies and running all of these

00:11:33,930 --> 00:11:37,620
running and installing all of these new

00:11:35,430 --> 00:11:39,660
technologies and projects on open shift

00:11:37,620 --> 00:11:41,880
isn't always the easiest thing to get

00:11:39,660 --> 00:11:43,440
started with working through all of

00:11:41,880 --> 00:11:45,390
these you know those things that come

00:11:43,440 --> 00:11:47,160
with getting started and like sharing

00:11:45,390 --> 00:11:48,570
that knowledge upstream is gonna make

00:11:47,160 --> 00:11:51,770
life easier for everyone who's in the

00:11:48,570 --> 00:11:54,660
community right going forward from there

00:11:51,770 --> 00:11:56,460
running at scale requires again very

00:11:54,660 --> 00:11:57,240
specific configurations which may not be

00:11:56,460 --> 00:11:57,940
obvious

00:11:57,240 --> 00:12:00,220
initial

00:11:57,940 --> 00:12:06,040
there's specific things you have to do

00:12:00,220 --> 00:12:08,140
so as we have processed large volumes of

00:12:06,040 --> 00:12:09,640
data and stored it for internal

00:12:08,140 --> 00:12:12,310
customers right we've found a lot of

00:12:09,640 --> 00:12:14,460
lessons and we've been a contributing

00:12:12,310 --> 00:12:18,670
them upstream to the open data hub

00:12:14,460 --> 00:12:21,550
community finally we also want to help

00:12:18,670 --> 00:12:23,770
drive teams at Red Hat to be more data

00:12:21,550 --> 00:12:26,800
centric and to that end you know we're

00:12:23,770 --> 00:12:28,720
creating blog posts stocks videos demos

00:12:26,800 --> 00:12:30,370
I'm sure you've seen some talks about

00:12:28,720 --> 00:12:31,660
some of the work people have done you

00:12:30,370 --> 00:12:34,690
probably see more of them over the next

00:12:31,660 --> 00:12:37,630
few days when you saw these use cases

00:12:34,690 --> 00:12:39,250
can be like an O hard drive failure

00:12:37,630 --> 00:12:41,500
prediction and surf for example right

00:12:39,250 --> 00:12:43,150
like these are useful things and there's

00:12:41,500 --> 00:12:44,560
a lot of data but not every er things

00:12:43,150 --> 00:12:48,280
that they can do these sort of

00:12:44,560 --> 00:12:50,140
applications when that node I'd like to

00:12:48,280 --> 00:12:53,290
talk a little bit about some of our

00:12:50,140 --> 00:12:54,910
internal customers the first one I want

00:12:53,290 --> 00:12:59,020
to touch on is the products and

00:12:54,910 --> 00:13:01,660
technology DevOps team they have

00:12:59,020 --> 00:13:03,730
applications in their build and product

00:13:01,660 --> 00:13:06,280
release pipelines that are generating a

00:13:03,730 --> 00:13:09,310
lot of logs all these logs are stored in

00:13:06,280 --> 00:13:11,560
the data hub as I'm sure most of you

00:13:09,310 --> 00:13:13,720
know not all also created equally right

00:13:11,560 --> 00:13:16,570
like some of them actually really matter

00:13:13,720 --> 00:13:19,420
most of them are just silly debug things

00:13:16,570 --> 00:13:28,480
no one cares about right so detecting

00:13:19,420 --> 00:13:30,700
these so detecting like these important

00:13:28,480 --> 00:13:33,460
logs is like anomaly detection is

00:13:30,700 --> 00:13:35,080
something that some of our the PNG dub

00:13:33,460 --> 00:13:37,630
of streams are actually engaging with

00:13:35,080 --> 00:13:38,800
the AI Center of Excellence so on and if

00:13:37,630 --> 00:13:41,170
you're interested in learning more about

00:13:38,800 --> 00:13:43,510
one of these this is a talk later this

00:13:41,170 --> 00:13:46,060
afternoon by Michael Clifford and Zak

00:13:43,510 --> 00:13:47,980
Hassan talking about their experiences

00:13:46,060 --> 00:13:53,800
building in a normally detection model

00:13:47,980 --> 00:13:55,690
for that we also have custom at cluster

00:13:53,800 --> 00:13:57,760
operational metrics for openshift

00:13:55,690 --> 00:14:01,240
clusters flowing into the data hub for

00:13:57,760 --> 00:14:02,890
the telemetry project this is all

00:14:01,240 --> 00:14:05,810
information but you know like how the

00:14:02,890 --> 00:14:07,040
clusters are behaving was they deployed

00:14:05,810 --> 00:14:10,310
what the health of the system looks like

00:14:07,040 --> 00:14:12,440
and it's helping like openshift PM's or

00:14:10,310 --> 00:14:13,970
tech leads make decisions on what future

00:14:12,440 --> 00:14:16,430
work they need to do or what features

00:14:13,970 --> 00:14:19,100
they need to prioritize or what people

00:14:16,430 --> 00:14:21,529
don't actually really care about on a

00:14:19,100 --> 00:14:23,630
similar note somewhat similar we also

00:14:21,529 --> 00:14:26,120
have a lot of customer insights data in

00:14:23,630 --> 00:14:28,400
the data hub this is data from things

00:14:26,120 --> 00:14:31,070
like Red Hat insights or source reports

00:14:28,400 --> 00:14:32,660
right for the customers and we have a

00:14:31,070 --> 00:14:36,650
number of teams internal to Red Hat

00:14:32,660 --> 00:14:38,690
using that data to improve the

00:14:36,650 --> 00:14:40,670
decision-making right like this way they

00:14:38,690 --> 00:14:42,830
can know what work to prioritize what

00:14:40,670 --> 00:14:46,130
maybe what customer partnerships to or

00:14:42,830 --> 00:14:47,390
is V Partnership suppose you and you

00:14:46,130 --> 00:14:49,420
think's in that vein again right it's

00:14:47,390 --> 00:14:52,010
making smarter decisions with that data

00:14:49,420 --> 00:14:53,450
one of these teams one of the teams

00:14:52,010 --> 00:14:56,240
working with the insights data as the

00:14:53,450 --> 00:15:01,730
Crocket team so hand it over to Daniel

00:14:56,240 --> 00:15:05,529
now to talk about that Thanks all right

00:15:01,730 --> 00:15:07,910
now it's great so yeah let's see so

00:15:05,529 --> 00:15:10,460
you're kind of again my name is Daniel

00:15:07,910 --> 00:15:12,490
Wolff senior data engineer and you're

00:15:10,460 --> 00:15:15,080
kind of going to get a two for one talk

00:15:12,490 --> 00:15:17,120
because we heard about the open data of

00:15:15,080 --> 00:15:18,740
infrastructure from the niche and now

00:15:17,120 --> 00:15:21,170
we're going to hear about an actual use

00:15:18,740 --> 00:15:23,630
case that we're building with open data

00:15:21,170 --> 00:15:26,089
hub so I'm gonna cover some slides with

00:15:23,630 --> 00:15:28,040
some background and get into a little

00:15:26,089 --> 00:15:30,050
bit of detail and then certain

00:15:28,040 --> 00:15:33,920
transition to joopa notebook and look at

00:15:30,050 --> 00:15:37,670
some code so the name of our internal

00:15:33,920 --> 00:15:40,730
application is grok it gr okay ket and

00:15:37,670 --> 00:15:45,830
this comes from the word croc which

00:15:40,730 --> 00:15:47,630
means to understand intuitively well I

00:15:45,830 --> 00:15:50,630
wanted to ask if but by quick show of

00:15:47,630 --> 00:15:53,350
hands if who has heard of the term croc

00:15:50,630 --> 00:15:55,610
it is an actual word relatively recent

00:15:53,350 --> 00:15:57,800
okay I'm impressed

00:15:55,610 --> 00:15:59,630
I'm impressed if so so what we are

00:15:57,800 --> 00:16:02,600
interested in understanding better with

00:15:59,630 --> 00:16:05,750
Crockett is data on workload adoption

00:16:02,600 --> 00:16:08,360
across Red Hat products and by work

00:16:05,750 --> 00:16:11,420
Leavitt I'm referring to categories of

00:16:08,360 --> 00:16:14,270
applications like database or software

00:16:11,420 --> 00:16:15,650
development and there's various key

00:16:14,270 --> 00:16:17,780
workloads that were interested in

00:16:15,650 --> 00:16:18,310
tracking and so then we can have a

00:16:17,780 --> 00:16:20,070
bedroom

00:16:18,310 --> 00:16:25,690
standing of how customers are using

00:16:20,070 --> 00:16:27,340
OpenShift and rel so why are we doing

00:16:25,690 --> 00:16:30,520
this well here's an example business

00:16:27,340 --> 00:16:32,890
question of how we are justifying our

00:16:30,520 --> 00:16:34,390
value of this of this effort so for

00:16:32,890 --> 00:16:37,480
example what software vendor

00:16:34,390 --> 00:16:39,940
partnerships should Red Hat create or

00:16:37,480 --> 00:16:43,300
enhance the traditional approach would

00:16:39,940 --> 00:16:45,510
be interviewing customers and asking you

00:16:43,300 --> 00:16:48,670
know what they're running or looking at

00:16:45,510 --> 00:16:51,580
online it's a Gartner research but with

00:16:48,670 --> 00:16:53,860
rocket we have qualitative data that

00:16:51,580 --> 00:16:55,839
helps us answer this question so we know

00:16:53,860 --> 00:16:58,540
what vendor to prioritize for open chef

00:16:55,839 --> 00:17:03,910
certification or when a co marketing

00:16:58,540 --> 00:17:06,579
campaign so how does it work the one

00:17:03,910 --> 00:17:09,790
line response is rocket works by group

00:17:06,579 --> 00:17:11,920
by grouping running processes on a

00:17:09,790 --> 00:17:15,339
system into clusters based on their

00:17:11,920 --> 00:17:17,110
similarity and then ideally each cluster

00:17:15,339 --> 00:17:21,040
corresponds to a running software

00:17:17,110 --> 00:17:24,459
application on that on that system so

00:17:21,040 --> 00:17:26,110
our data is coming in through customers

00:17:24,459 --> 00:17:28,030
that have opted in to Red Hat insights

00:17:26,110 --> 00:17:31,600
which is shifted by default with rel8

00:17:28,030 --> 00:17:34,030
and we get data from thousands of

00:17:31,600 --> 00:17:35,710
systems and any particular process or

00:17:34,030 --> 00:17:37,750
any particular system can have thousands

00:17:35,710 --> 00:17:39,250
of running processes so was that

00:17:37,750 --> 00:17:42,100
multiplier effect this really makes it

00:17:39,250 --> 00:17:47,260
ideal for a machine learning approach as

00:17:42,100 --> 00:17:48,580
opposed to a to a manual review so

00:17:47,260 --> 00:17:50,530
that's what I'm going to get into next

00:17:48,580 --> 00:17:52,320
let's take a step back and give a brief

00:17:50,530 --> 00:17:55,330
overview of the k-means clustering

00:17:52,320 --> 00:17:58,080
algorithm which is the approach that we

00:17:55,330 --> 00:18:00,250
are taking so the k-means clustering

00:17:58,080 --> 00:18:03,220
intelligently groups data into K

00:18:00,250 --> 00:18:05,200
clusters based on the similarity of

00:18:03,220 --> 00:18:08,080
their features and it is a fairly

00:18:05,200 --> 00:18:10,470
popular out algorithm and one of its

00:18:08,080 --> 00:18:13,270
more well-known use cases is with

00:18:10,470 --> 00:18:16,510
recommendation engines so for example

00:18:13,270 --> 00:18:17,679
recommending movies or songs so let's

00:18:16,510 --> 00:18:22,510
say you're a streaming service like

00:18:17,679 --> 00:18:23,950
Netflix and you want and but viewer just

00:18:22,510 --> 00:18:26,740
watched a movie and you want to keep

00:18:23,950 --> 00:18:31,059
them engaged how are you going to figure

00:18:26,740 --> 00:18:32,769
out what to recommend so in this example

00:18:31,059 --> 00:18:35,249
what you can do if you're the streaming

00:18:32,769 --> 00:18:38,679
service is take all of your movies and

00:18:35,249 --> 00:18:41,860
all the attributes about those movies

00:18:38,679 --> 00:18:43,990
feed it into your algorithm and it is

00:18:41,860 --> 00:18:47,169
going to automatically cluster those

00:18:43,990 --> 00:18:50,110
into similar movies so for example if

00:18:47,169 --> 00:18:51,700
you watch Avengers endgame you might

00:18:50,110 --> 00:18:53,830
also like The Dark Knight

00:18:51,700 --> 00:18:56,379
now that's somewhat of a trivial example

00:18:53,830 --> 00:18:59,169
because those are both really popular

00:18:56,379 --> 00:19:01,480
action movies but the algorithm can show

00:18:59,169 --> 00:19:04,509
relationships that are not familiar to

00:19:01,480 --> 00:19:08,590
the to the naked eye and it can do it

00:19:04,509 --> 00:19:10,149
faster and that scale so I'm not going

00:19:08,590 --> 00:19:12,369
to go into too much detail about how the

00:19:10,149 --> 00:19:13,690
algorithm works under the hood or you

00:19:12,369 --> 00:19:16,809
might need to be a mathematician to

00:19:13,690 --> 00:19:19,419
truly do that but it is based on

00:19:16,809 --> 00:19:21,549
calculating the distance between those

00:19:19,419 --> 00:19:24,850
numeric features and based on how close

00:19:21,549 --> 00:19:26,919
or how far apart those features are it

00:19:24,850 --> 00:19:32,499
will group those into into similar

00:19:26,919 --> 00:19:35,440
clusters so that's our use case with

00:19:32,499 --> 00:19:38,139
Brockett we are intelligently clustering

00:19:35,440 --> 00:19:41,830
processes belonging to the same software

00:19:38,139 --> 00:19:43,619
application so here I've got a

00:19:41,830 --> 00:19:46,990
visualization a k-means clustering

00:19:43,619 --> 00:19:49,119
visualization and this has K set to 3

00:19:46,990 --> 00:19:51,789
which means there are three clusters and

00:19:49,119 --> 00:19:55,360
they're color-coded here with a beige of

00:19:51,789 --> 00:19:57,700
blue and a green and so each black dot

00:19:55,360 --> 00:20:01,899
if you can see in the in the

00:19:57,700 --> 00:20:04,059
visualization is a running process on a

00:20:01,899 --> 00:20:06,490
system so again like I mentioned earlier

00:20:04,059 --> 00:20:08,830
we can really feed in millions of

00:20:06,490 --> 00:20:11,309
running processes and now algorithm will

00:20:08,830 --> 00:20:14,559
will group those into similar clusters

00:20:11,309 --> 00:20:16,929
so I'm going to advance here and let's

00:20:14,559 --> 00:20:19,619
take a look at this beige cluster a deep

00:20:16,929 --> 00:20:23,470
dive into it as a simplified example

00:20:19,619 --> 00:20:27,009
I've got four running processes shown

00:20:23,470 --> 00:20:29,740
here and you can see the end result

00:20:27,009 --> 00:20:32,379
based on the clustering is that all of

00:20:29,740 --> 00:20:34,869
the processes have similar words and

00:20:32,379 --> 00:20:37,419
similar terms so you can take a look

00:20:34,869 --> 00:20:43,000
here and see some of the similar terms

00:20:37,419 --> 00:20:44,730
for these processes we've got /bin /lu

00:20:43,000 --> 00:20:47,820
indeed you can see is common across

00:20:44,730 --> 00:20:50,270
all these as well as as well as Ruby

00:20:47,820 --> 00:20:53,100
so the algorithm has determined that

00:20:50,270 --> 00:20:55,530
these processes they have this

00:20:53,100 --> 00:20:58,919
differentiator of having these terms and

00:20:55,530 --> 00:21:09,299
so it puts them into the same process

00:20:58,919 --> 00:21:11,309
question literally just the process

00:21:09,299 --> 00:21:13,530
strengths yep so it's just the process

00:21:11,309 --> 00:21:15,390
strings and when we flip it to the

00:21:13,530 --> 00:21:19,650
Jupiter notebook I'll show a little bit

00:21:15,390 --> 00:21:21,929
more about how we do that and so now we

00:21:19,650 --> 00:21:25,530
have this cluster we can do a little bit

00:21:21,929 --> 00:21:29,040
of manual research and with some domain

00:21:25,530 --> 00:21:32,460
expertise we know that these processes

00:21:29,040 --> 00:21:34,710
all represent fluid the logging

00:21:32,460 --> 00:21:37,590
application fluid be unified logging

00:21:34,710 --> 00:21:40,710
software application so then we know

00:21:37,590 --> 00:21:45,210
that if we are if we scan a system and

00:21:40,710 --> 00:21:49,679
we see a pattern that emerges has Ruby

00:21:45,210 --> 00:21:52,530
and then bin slash fluency we know that

00:21:49,679 --> 00:21:55,440
that the that system and that customer

00:21:52,530 --> 00:21:58,950
is running the fluency application so we

00:21:55,440 --> 00:22:01,799
can label the beige cluster with fluency

00:21:58,950 --> 00:22:05,190
and there are a couple other examples of

00:22:01,799 --> 00:22:07,760
Epirus Splunk and MongoDB there another

00:22:05,190 --> 00:22:07,760
question yes

00:22:25,000 --> 00:22:28,530
well so think about the fact that we

00:22:27,280 --> 00:22:31,030
have but we're getting data from

00:22:28,530 --> 00:22:36,100
thousands of systems on a daily basis

00:22:31,030 --> 00:22:38,230
and so if we were looking at a pure

00:22:36,100 --> 00:22:40,510
frequency count based on these processes

00:22:38,230 --> 00:22:42,460
very there's enough variation in the

00:22:40,510 --> 00:22:45,730
process strings that it would not become

00:22:42,460 --> 00:22:47,860
a parent but the clustering can actually

00:22:45,730 --> 00:22:49,630
group these together so that we know

00:22:47,860 --> 00:22:51,880
that okay this is showing up on a ton of

00:22:49,630 --> 00:22:53,530
systems you don't know what it is but

00:22:51,880 --> 00:22:56,500
it's one of the most important clusters

00:22:53,530 --> 00:22:59,650
and and then tie it back to the fluency

00:22:56,500 --> 00:23:01,360
application and then from there then you

00:22:59,650 --> 00:23:04,419
have a string that you can search all

00:23:01,360 --> 00:23:06,580
future systems on a daily basis for

00:23:04,419 --> 00:23:08,640
fluent date does that answer your

00:23:06,580 --> 00:23:08,640
question

00:23:16,390 --> 00:23:21,730
right right the data is huge and there's

00:23:18,820 --> 00:23:24,790
enough variation in in the process

00:23:21,730 --> 00:23:26,380
strings that would be impossible to do

00:23:24,790 --> 00:23:35,320
it with naked eye or with frequency

00:23:26,380 --> 00:23:38,590
count so so the end goal is to be able

00:23:35,320 --> 00:23:41,290
to model the presence and the prevalence

00:23:38,590 --> 00:23:44,320
of workloads and software applications

00:23:41,290 --> 00:23:46,450
running on customer systems whereas

00:23:44,320 --> 00:23:49,450
before we'd have to just interview a

00:23:46,450 --> 00:23:51,309
customer or do online research with this

00:23:49,450 --> 00:23:53,830
way we have a data-driven approach of

00:23:51,309 --> 00:23:55,750
understanding the software and workloads

00:23:53,830 --> 00:23:58,260
that customers are using for rel and for

00:23:55,750 --> 00:23:58,260
openshift

00:24:40,490 --> 00:24:46,580
but but the the idea that is to be able

00:24:44,090 --> 00:24:49,790
to learn ahead of time potential

00:24:46,580 --> 00:24:54,640
problems potential things based on this

00:24:49,790 --> 00:24:54,640
huge dataset is constantly generating

00:25:00,220 --> 00:25:07,580
thank you very much okay so yeah just

00:25:05,420 --> 00:25:11,360
wrapping up this side yeah all in all we

00:25:07,580 --> 00:25:14,870
have over 200 clusters that we've tied

00:25:11,360 --> 00:25:17,000
back to software applications and and we

00:25:14,870 --> 00:25:19,810
can then do the modeling from there

00:25:17,000 --> 00:25:19,810
another question

00:25:22,330 --> 00:25:26,360
great quit that so the question was I

00:25:24,740 --> 00:25:28,910
was told to repeat the question for the

00:25:26,360 --> 00:25:31,670
video how do you choose the value of K

00:25:28,910 --> 00:25:33,920
and there are several approaches to do

00:25:31,670 --> 00:25:36,500
that and we use one called the elbow

00:25:33,920 --> 00:25:39,080
method and I can show that to you in the

00:25:36,500 --> 00:25:44,900
jupiter notebook which we will get to in

00:25:39,080 --> 00:25:46,730
about just a couple minutes okay so

00:25:44,900 --> 00:25:50,020
before we transition off the slides I

00:25:46,730 --> 00:25:53,930
just wanted to tie it back to anisha's

00:25:50,020 --> 00:25:55,730
open data hub presentation so again

00:25:53,930 --> 00:25:59,480
we're building this with open data hub

00:25:55,730 --> 00:26:02,000
we use set and stuff for our data

00:25:59,480 --> 00:26:03,980
storage and Jupiter hub as our coding

00:26:02,000 --> 00:26:06,050
environment both of these are deployed

00:26:03,980 --> 00:26:08,900
on and orchestrated by Red Hat

00:26:06,050 --> 00:26:10,490
openshift and since his team has set it

00:26:08,900 --> 00:26:14,750
up for us it's behind the scenes to us

00:26:10,490 --> 00:26:19,330
we're just using Jupiter hub and we pull

00:26:14,750 --> 00:26:22,340
the data from Seth into Jupiter hub and

00:26:19,330 --> 00:26:24,830
we do that using the s3 filesystem

00:26:22,340 --> 00:26:27,200
protocol which i think is a key point

00:26:24,830 --> 00:26:31,520
because it reduces the barrier to entry

00:26:27,200 --> 00:26:34,240
for using the Ceph storage because the

00:26:31,520 --> 00:26:37,430
s3 protocol is the same as you have with

00:26:34,240 --> 00:26:40,400
AWS s3 so you don't need to learn any

00:26:37,430 --> 00:26:43,190
new packages you can use the most

00:26:40,400 --> 00:26:47,030
popular packages like bodo 3 or reading

00:26:43,190 --> 00:26:50,690
in through PI spar s3 paths just like

00:26:47,030 --> 00:26:52,830
you would from from AWS and that's what

00:26:50,690 --> 00:26:54,779
we do is we use the pi spark data frames

00:26:52,830 --> 00:26:57,720
to read in the data run some

00:26:54,779 --> 00:27:00,330
pre-processing steps and then we use the

00:26:57,720 --> 00:27:07,140
scikit-learn package which comes with

00:27:00,330 --> 00:27:08,730
k-means to run the ML algorithm so

00:27:07,140 --> 00:27:10,110
that's what I have for the slides so

00:27:08,730 --> 00:27:33,389
without further ado we can switch over

00:27:10,110 --> 00:27:35,639
to the code forgive me while I it's up a

00:27:33,389 --> 00:27:37,350
new pod every time and yeah you can't

00:27:35,639 --> 00:27:39,059
see it through the open shut UI so it

00:27:37,350 --> 00:27:41,730
spins up a new pod every time you start

00:27:39,059 --> 00:27:43,200
your server so how it kind of works is

00:27:41,730 --> 00:27:45,029
like you have your chip to have instance

00:27:43,200 --> 00:27:49,740
running and that's like in the computers

00:27:45,029 --> 00:27:51,450
like your master ok yeah and then every

00:27:49,740 --> 00:27:53,460
time a user tries logging into that

00:27:51,450 --> 00:27:55,769
interface right and then they said I

00:27:53,460 --> 00:27:58,350
start my so board you see like what sort

00:27:55,769 --> 00:27:59,880
of like Jupiter image you want to use so

00:27:58,350 --> 00:28:02,070
you know you want that stuff so

00:27:59,880 --> 00:28:05,070
installed out already you once I get it

00:28:02,070 --> 00:28:10,950
on a spark right and that spins up a new

00:28:05,070 --> 00:28:14,309
pod based on that image ok so I'm just

00:28:10,950 --> 00:28:15,960
going to step through some code here it

00:28:14,309 --> 00:28:18,210
does take a little while to run even

00:28:15,960 --> 00:28:20,159
with a with a fairly small sample size

00:28:18,210 --> 00:28:22,679
so I'm not going to run it real time but

00:28:20,159 --> 00:28:26,250
I will just scroll down so first we

00:28:22,679 --> 00:28:29,669
import our key packages and then here

00:28:26,250 --> 00:28:34,919
comes the spark context setup and this

00:28:29,669 --> 00:28:37,320
is where we can put in the s3 there s3

00:28:34,919 --> 00:28:40,260
like credentials but first F storage and

00:28:37,320 --> 00:28:43,529
point the endpoint URL to to open data

00:28:40,260 --> 00:28:46,710
hub and access key and secret key and

00:28:43,529 --> 00:28:49,230
then we can set our file our key path

00:28:46,710 --> 00:28:51,389
and read in the data and it's

00:28:49,230 --> 00:28:53,159
partitioned by year month and day so we

00:28:51,389 --> 00:28:57,090
can use wildcards

00:28:53,159 --> 00:28:59,789
with the or basic regular expressions

00:28:57,090 --> 00:29:01,889
for capturing a certain range of months

00:28:59,789 --> 00:29:05,250
or wildcards for capturing all months or

00:29:01,889 --> 00:29:06,310
all days and then read it into a data

00:29:05,250 --> 00:29:09,130
frame and then

00:29:06,310 --> 00:29:11,830
this is a sample of what the raw data

00:29:09,130 --> 00:29:13,960
looks like with an attachment ID for the

00:29:11,830 --> 00:29:16,660
insights upload and then the process

00:29:13,960 --> 00:29:18,610
command here so these are all the

00:29:16,660 --> 00:29:19,900
running processes it's just a very small

00:29:18,610 --> 00:29:22,630
sample of what we have out there

00:29:19,900 --> 00:29:25,300
obviously some of them are short like at

00:29:22,630 --> 00:29:27,700
CD and sleep but then most of these

00:29:25,300 --> 00:29:31,090
others that just truncated here are much

00:29:27,700 --> 00:29:35,260
longer so we do some pre-processing

00:29:31,090 --> 00:29:44,320
steps to bring it into a list from data

00:29:35,260 --> 00:29:46,620
frame format and then we question yes

00:29:44,320 --> 00:29:50,230
it's that's a unique it or does the

00:29:46,620 --> 00:29:52,300
attachment ID act as as a as an index

00:29:50,230 --> 00:29:55,630
and yeah that's like a unique identifier

00:29:52,300 --> 00:29:58,330
for that particular upload so yes if we

00:29:55,630 --> 00:30:00,870
did want to tie that back to an account

00:29:58,330 --> 00:30:00,870
yes

00:30:15,040 --> 00:30:22,240
so okay so with the I guess it's it's

00:30:19,420 --> 00:30:23,890
not really acting as a key index we

00:30:22,240 --> 00:30:25,960
don't really have a key index for this

00:30:23,890 --> 00:30:27,970
data frame we're just going to take

00:30:25,960 --> 00:30:31,140
these processes out of the data frame

00:30:27,970 --> 00:30:34,530
and put them into a normal Python list

00:30:31,140 --> 00:30:38,230
so it's so yeah these those those

00:30:34,530 --> 00:30:41,320
attachment IDs are not unique and the

00:30:38,230 --> 00:30:43,930
process list is not unique but yeah we

00:30:41,320 --> 00:30:50,440
don't we don't have a unique key index

00:30:43,930 --> 00:30:52,210
for this data frame so yeah back to the

00:30:50,440 --> 00:30:54,220
pre-processing steps we removed some

00:30:52,210 --> 00:30:58,900
special characters that are not relevant

00:30:54,220 --> 00:31:01,540
to the analysis and we replace all the

00:30:58,900 --> 00:31:05,520
forward slash with spaces so that we can

00:31:01,540 --> 00:31:08,950
have each term as a as a separate word

00:31:05,520 --> 00:31:11,380
and then we removed trailing in leading

00:31:08,950 --> 00:31:12,970
white spaces and for printing out all

00:31:11,380 --> 00:31:16,120
the processes this is how it looks after

00:31:12,970 --> 00:31:18,520
those pre-processing steps so just a lot

00:31:16,120 --> 00:31:20,680
of process strings in there and back to

00:31:18,520 --> 00:31:22,630
the questions karma earlier we aren't

00:31:20,680 --> 00:31:25,120
looking at memory or cache or anything

00:31:22,630 --> 00:31:28,360
like that just purely the the process

00:31:25,120 --> 00:31:32,560
strings so I also mentioned earlier how

00:31:28,360 --> 00:31:34,210
K means has a calculates its distance or

00:31:32,560 --> 00:31:36,850
calculates the cluster is based on the

00:31:34,210 --> 00:31:39,070
distance between numeric features so we

00:31:36,850 --> 00:31:42,100
do need to convert the process strings

00:31:39,070 --> 00:31:44,890
into numeric vectors and for that we are

00:31:42,100 --> 00:31:46,690
using the tf-idf vectorizer

00:31:44,890 --> 00:31:50,050
as you can see here it's just two lines

00:31:46,690 --> 00:31:51,600
of code and there's various approaches

00:31:50,050 --> 00:31:55,060
to doing this as part of the

00:31:51,600 --> 00:31:58,360
scikit-learn package we settled with the

00:31:55,060 --> 00:32:00,310
tf-idf because some of the other ones

00:31:58,360 --> 00:32:03,610
are more geared towards natural language

00:32:00,310 --> 00:32:06,220
and but with this one we're with this

00:32:03,610 --> 00:32:09,400
use case we're using machine language so

00:32:06,220 --> 00:32:12,460
we won't we prefer one that does not

00:32:09,400 --> 00:32:14,200
focus on natural language processing so

00:32:12,460 --> 00:32:16,660
then all of the process strings are then

00:32:14,200 --> 00:32:19,720
converted into numeric vectors at this

00:32:16,660 --> 00:32:21,530
step alright so here's the question

00:32:19,720 --> 00:32:24,410
earlier about

00:32:21,530 --> 00:32:26,540
inque so you do need to have case

00:32:24,410 --> 00:32:28,490
specified when you're training the model

00:32:26,540 --> 00:32:31,190
so this is kind of a brute-force

00:32:28,490 --> 00:32:33,530
approach for optimizing k that's

00:32:31,190 --> 00:32:36,320
referred to as the elbow method and you

00:32:33,530 --> 00:32:39,080
can choose how many clusters you want to

00:32:36,320 --> 00:32:42,920
go up to and arrange for that value of K

00:32:39,080 --> 00:32:45,740
so in this example we go from 1 to 200

00:32:42,920 --> 00:32:47,120
clusters and so we trim so this is the

00:32:45,740 --> 00:32:49,820
step that takes a while because it

00:32:47,120 --> 00:32:53,300
trains the cluster for every value of K

00:32:49,820 --> 00:32:56,690
as you can see here and then it

00:32:53,300 --> 00:32:59,090
calculates the sum of squared distances

00:32:56,690 --> 00:33:04,760
which is a way of calculating the air or

00:32:59,090 --> 00:33:06,970
the impurity of a cluster and so as you

00:33:04,760 --> 00:33:11,360
can see in the chart if your K value

00:33:06,970 --> 00:33:13,300
here on the x-axis is small then you're

00:33:11,360 --> 00:33:16,010
naturally going to have much higher

00:33:13,300 --> 00:33:17,600
impurity of your clusters because you're

00:33:16,010 --> 00:33:20,210
putting a lot of different types of

00:33:17,600 --> 00:33:22,700
processes into a small number of cluster

00:33:20,210 --> 00:33:25,910
and then out to the right as your K

00:33:22,700 --> 00:33:28,210
value increases then that's where you

00:33:25,910 --> 00:33:32,120
can potentially over fit the model and

00:33:28,210 --> 00:33:33,920
and you may have different you may have

00:33:32,120 --> 00:33:36,290
similar processes put in different

00:33:33,920 --> 00:33:38,900
clusters because it's going to get you

00:33:36,290 --> 00:33:41,900
because it's going to return 175 200

00:33:38,900 --> 00:33:43,700
clusters based on the value you give so

00:33:41,900 --> 00:33:46,550
the reason it's called elbow method is

00:33:43,700 --> 00:33:48,650
you may suspect is the optimal value for

00:33:46,550 --> 00:33:51,080
K is going to be somewhere in the elbow

00:33:48,650 --> 00:33:57,490
of this curve which in this example is

00:33:51,080 --> 00:34:00,770
somewhere in that 50 to 75 range so

00:33:57,490 --> 00:34:03,530
after running that visualization we then

00:34:00,770 --> 00:34:06,980
train the model with K set to 50

00:34:03,530 --> 00:34:09,200
clusters and that's the step that's done

00:34:06,980 --> 00:34:10,760
here and once we've done that then we've

00:34:09,200 --> 00:34:13,760
trained the model and we can start

00:34:10,760 --> 00:34:17,450
running some exploratory analysis on the

00:34:13,760 --> 00:34:19,970
model results so for example we can get

00:34:17,450 --> 00:34:21,679
the top terms for each cluster and it's

00:34:19,970 --> 00:34:23,390
not necessarily the most common terms

00:34:21,679 --> 00:34:26,780
for each cluster but it's what's at the

00:34:23,390 --> 00:34:29,960
center of that cluster from a numeric

00:34:26,780 --> 00:34:35,090
standpoint and I've already got it

00:34:29,960 --> 00:34:37,460
scroll down here but the cluster number

00:34:35,090 --> 00:34:38,870
alright labeled here and it's just

00:34:37,460 --> 00:34:42,190
printing the top ten terms for each

00:34:38,870 --> 00:34:44,630
cluster those those labels are not

00:34:42,190 --> 00:34:46,730
necessarily significant it's not like

00:34:44,630 --> 00:34:50,060
cluster zero is more important in

00:34:46,730 --> 00:34:53,690
cluster 40 but you can scroll through

00:34:50,060 --> 00:34:57,590
all of them and then in in this training

00:34:53,690 --> 00:34:59,300
run cluster 31 has some of the terms I

00:34:57,590 --> 00:35:04,760
was referencing earlier with fluent II

00:34:59,300 --> 00:35:07,700
and with Ruby and USR and bin and so you

00:35:04,760 --> 00:35:09,500
can do this for all of your clusters and

00:35:07,700 --> 00:35:11,270
see if there's any key terms that jump

00:35:09,500 --> 00:35:13,610
out at you that you think oh that could

00:35:11,270 --> 00:35:18,410
be a running software application and

00:35:13,610 --> 00:35:20,500
then and then continue on down so the

00:35:18,410 --> 00:35:23,750
number of processes for each cluster

00:35:20,500 --> 00:35:27,290
does not have to stay consistent across

00:35:23,750 --> 00:35:28,880
all clusters you can see the cluster

00:35:27,290 --> 00:35:31,220
labels across the bottom the different

00:35:28,880 --> 00:35:34,940
values for K and the number of processes

00:35:31,220 --> 00:35:36,680
shown on the y-axis so it certainly

00:35:34,940 --> 00:35:38,750
fluctuates and we thought it was

00:35:36,680 --> 00:35:41,780
interesting that there was this big

00:35:38,750 --> 00:35:44,780
spike here for think it's around cluster

00:35:41,780 --> 00:35:47,990
19 or so and when we looked into that

00:35:44,780 --> 00:35:50,720
one it was kind of like we refer to it

00:35:47,990 --> 00:35:52,610
as a junk cluster where it just put a

00:35:50,720 --> 00:35:55,670
bunch of processes in there that didn't

00:35:52,610 --> 00:35:57,410
really fit well anywhere else but that

00:35:55,670 --> 00:36:00,410
can open up additional opportunities to

00:35:57,410 --> 00:36:03,350
set it to then run the clustering

00:36:00,410 --> 00:36:05,900
algorithm on the junk cluster itself and

00:36:03,350 --> 00:36:08,680
see if see if you can get a little bit

00:36:05,900 --> 00:36:08,680
deeper into that

00:36:22,520 --> 00:36:26,630
something like that right when is that

00:36:24,260 --> 00:36:30,080
fit into this Jupiter notebook where

00:36:26,630 --> 00:36:43,400
does this make use of that wider array

00:36:30,080 --> 00:36:45,380
of compute resources right right so so

00:36:43,400 --> 00:36:49,730
for example with what in our scenario

00:36:45,380 --> 00:36:51,380
using PI spark we have the so all this

00:36:49,730 --> 00:36:53,990
is an OD H instance with the Ceph

00:36:51,380 --> 00:36:56,000
storage and with a Jupiter hub and then

00:36:53,990 --> 00:36:59,030
if we really need to scale up our

00:36:56,000 --> 00:37:01,400
compute resource we can from within this

00:36:59,030 --> 00:37:05,480
Jupiter notebook in odh we can connect

00:37:01,400 --> 00:37:07,700
to a remote spark cluster that you know

00:37:05,480 --> 00:37:09,860
it can have can spin up more nodes and

00:37:07,700 --> 00:37:12,140
have as much compute power is needed so

00:37:09,860 --> 00:37:14,470
it's it's more seamless for us and by

00:37:12,140 --> 00:37:18,860
the way I forgot to ask the question but

00:37:14,470 --> 00:37:22,010
the question was about how to how does

00:37:18,860 --> 00:37:24,200
this example demonstrate the use of

00:37:22,010 --> 00:37:31,010
being able to scale up right scale to

00:37:24,200 --> 00:37:33,800
compute power does that answer towards

00:37:31,010 --> 00:37:35,840
the beginning portion of the notebook we

00:37:33,800 --> 00:37:46,190
do specify the spark instance that you

00:37:35,840 --> 00:37:47,630
connect into you have a URL in there so

00:37:46,190 --> 00:37:49,820
there's nothing that really says well

00:37:47,630 --> 00:37:51,920
I'm running this notebooks here and I'm

00:37:49,820 --> 00:37:54,200
using this spark astiz here it's it's

00:37:51,920 --> 00:37:55,520
it's it's almost explicitly going off to

00:37:54,200 --> 00:37:57,800
something else something else just

00:37:55,520 --> 00:38:00,109
happens to be the openshift

00:37:57,800 --> 00:38:05,090
implementation you have that this middle

00:38:00,109 --> 00:38:08,390
book is running on yeah well in this

00:38:05,090 --> 00:38:12,710
example this is the spark master URL

00:38:08,390 --> 00:38:18,260
right here so yeah anis did you want to

00:38:12,710 --> 00:38:20,900
add on there things that we that open

00:38:18,260 --> 00:38:23,450
data hub does behind the scenes one is

00:38:20,900 --> 00:38:26,420
if you select it in Jupiter hub that you

00:38:23,450 --> 00:38:29,060
have a spark enabled you put a notebook

00:38:26,420 --> 00:38:30,740
it actually spins up a spark cluster for

00:38:29,060 --> 00:38:31,810
you behind the scenes automatically and

00:38:30,740 --> 00:38:34,240
it's your own person

00:38:31,810 --> 00:38:35,650
spark cluster you can size that to

00:38:34,240 --> 00:38:37,630
whatever you want so as you're doing

00:38:35,650 --> 00:38:39,400
your data discovery or your data

00:38:37,630 --> 00:38:41,350
exploration of your machine learning you

00:38:39,400 --> 00:38:43,540
can size that independently if you have

00:38:41,350 --> 00:38:46,270
a spark cluster let's say you know it's

00:38:43,540 --> 00:38:48,010
out somewhere else on a Hadoop cluster

00:38:46,270 --> 00:38:51,310
you can also point to it but by default

00:38:48,010 --> 00:38:53,950
we add in some environmental variables

00:38:51,310 --> 00:38:55,630
that make it automatic so that your

00:38:53,950 --> 00:38:57,250
jupiter notebook knows how to use that

00:38:55,630 --> 00:38:59,380
resource the other thing we're not

00:38:57,250 --> 00:39:02,380
showing here that open data hub does is

00:38:59,380 --> 00:39:05,230
if you have a GPU enabled OpenShift then

00:39:02,380 --> 00:39:09,190
we implicitly allow you to run your

00:39:05,230 --> 00:39:11,170
workloads on a GPU by just selecting a

00:39:09,190 --> 00:39:12,520
certain parameter when you spin up your

00:39:11,170 --> 00:39:14,110
jupiter notebook that's not something

00:39:12,520 --> 00:39:15,430
that will show here today but those are

00:39:14,110 --> 00:39:17,050
the types of things that open data hub

00:39:15,430 --> 00:39:19,420
does for you it's automatically

00:39:17,050 --> 00:39:36,490
configured to use those resources for

00:39:19,420 --> 00:39:38,140
you yes you would basically so you would

00:39:36,490 --> 00:39:40,300
click on that control panel and then

00:39:38,140 --> 00:39:42,130
there's a stopped my server you stop the

00:39:40,300 --> 00:39:44,200
server you spin up a new one with

00:39:42,130 --> 00:39:46,000
whatever new parameters you want if you

00:39:44,200 --> 00:39:47,860
wanted to select the GPU you'd select

00:39:46,000 --> 00:39:54,070
the GPU and now your workload would run

00:39:47,860 --> 00:39:54,490
on a GPU enabled container all right

00:39:54,070 --> 00:39:56,290
great

00:39:54,490 --> 00:39:59,950
yes I was getting to the end of the

00:39:56,290 --> 00:40:03,810
notebook here showing a number of

00:39:59,950 --> 00:40:03,810
cluster the number of possible cluster

00:40:18,760 --> 00:40:21,600
yes

00:40:30,599 --> 00:40:34,930
that's true so the jupiter hub digs

00:40:33,819 --> 00:40:37,779
exactly what we're doing

00:40:34,930 --> 00:40:38,470
it's a multi-user environment different

00:40:37,779 --> 00:40:40,749
users

00:40:38,470 --> 00:40:43,660
maybe there's a power user that has to

00:40:40,749 --> 00:40:45,490
access much more data than someone else

00:40:43,660 --> 00:40:47,529
that's just looking at exploratory

00:40:45,490 --> 00:40:49,420
smaller data sets we have the ability to

00:40:47,529 --> 00:40:51,940
control how many resources how much CPU

00:40:49,420 --> 00:40:54,640
how much memory they're allocating per

00:40:51,940 --> 00:40:56,559
user or we can give a default that every

00:40:54,640 --> 00:40:58,720
user gets so that is something that we

00:40:56,559 --> 00:41:00,700
kind of glossed over it is a multi-user

00:40:58,720 --> 00:41:02,920
environment and it's allowing us to

00:41:00,700 --> 00:41:10,690
control the quotas for each data

00:41:02,920 --> 00:41:12,869
scientist yes yeah ok great well that

00:41:10,690 --> 00:41:14,549
about wraps it up biases on this last

00:41:12,869 --> 00:41:17,140
cell

00:41:14,549 --> 00:41:20,619
code cell here is just going to tie it

00:41:17,140 --> 00:41:23,829
back in with the fluency aspect earlier

00:41:20,619 --> 00:41:26,319
highlighted cluster number 31 as being

00:41:23,829 --> 00:41:27,700
fluent D and then here we can see the

00:41:26,319 --> 00:41:30,309
example processes

00:41:27,700 --> 00:41:33,009
it's just ignore that it says 10 there

00:41:30,309 --> 00:41:35,319
because it should be 31 from this latest

00:41:33,009 --> 00:41:37,799
run but based on the pre-processing

00:41:35,319 --> 00:41:39,759
steps it had taken out the slashes and

00:41:37,799 --> 00:41:41,380
forward slashes and replacing with

00:41:39,759 --> 00:41:43,839
spaces but it looks pretty similar

00:41:41,380 --> 00:41:48,999
compared to the example processes from

00:41:43,839 --> 00:41:51,720
the from the slide earlier and that

00:41:48,999 --> 00:41:51,720

YouTube URL: https://www.youtube.com/watch?v=K8G_0z5jbcA


