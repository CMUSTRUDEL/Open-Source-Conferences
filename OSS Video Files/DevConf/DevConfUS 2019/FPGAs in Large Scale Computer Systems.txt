Title: FPGAs in Large Scale Computer Systems
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speaker: Ahmed Sanaullah

As modern data center workloads become increasingly complex, constrained and critical, mainstream "CPU-centric" computing can no longer keep pace. Future data centers are moving towards a more fluid model, with computation and communication no longer localized to commodity CPUs and routers. Next generation "data-centric" data centers will "compute everywhere,” whether data is stationary (in memory) or on the move (in network). Reconfigurable hardware, in the form of Field Programmable Gate Arrays (FPGAs), are transforming ordinary clouds into massive supercomputers. We will highlight many ways to deploy FPGAs in a data center node, such as traditional back-end accelerators, tightly coupled off-load processors, Smart NICs, Bump-in-the-Wire, and even in the router itself. We will also discuss our efforts to make these devices accessible globally accessible, through deeper integration into software stacks, transparent generation of custom hardware stacks, and device management using reconfigurable hardware operating systems.
Captions: 
	00:00:11,240 --> 00:00:15,900
good afternoon everyone my name is

00:00:13,620 --> 00:00:18,210
Emmett Sinhala I'm a fourth year PhD

00:00:15,900 --> 00:00:20,460
student at Boston University and I

00:00:18,210 --> 00:00:22,710
worked with a wonderful set of mentors

00:00:20,460 --> 00:00:24,900
and advisors Martin turbot from bu and

00:00:22,710 --> 00:00:29,339
oral cleaner from bu and bullied Ripper

00:00:24,900 --> 00:00:32,550
from Red Hat sorry about that my talk

00:00:29,339 --> 00:00:35,789
today is on FPGAs we're seeing these pop

00:00:32,550 --> 00:00:38,039
up a lot in data centers and this is not

00:00:35,789 --> 00:00:39,359
some marketing gimmick this is some not

00:00:38,039 --> 00:00:40,800
some novelty people are trying to

00:00:39,359 --> 00:00:43,530
exploit to draw people to the clouds

00:00:40,800 --> 00:00:45,629
they're there for a very good reason and

00:00:43,530 --> 00:00:48,809
that is because we're living in a world

00:00:45,629 --> 00:00:51,660
where our traditional compute systems

00:00:48,809 --> 00:00:53,699
are just not enough to meet the demands

00:00:51,660 --> 00:00:58,589
that are being expected of our data

00:00:53,699 --> 00:00:59,640
centers of our CPUs so let's start from

00:00:58,589 --> 00:01:03,359
the beginning

00:00:59,640 --> 00:01:05,100
well not that far back but Billy there

00:01:03,359 --> 00:01:08,040
was a time not too long ago when a

00:01:05,100 --> 00:01:09,870
single computer was good enough for Zhou

00:01:08,040 --> 00:01:11,520
was going strong then our scaling was

00:01:09,870 --> 00:01:14,430
going strong we could do all

00:01:11,520 --> 00:01:16,500
computations on a single machine but of

00:01:14,430 --> 00:01:19,260
course the complexities of the workload

00:01:16,500 --> 00:01:21,390
that we were trying to do scale faster

00:01:19,260 --> 00:01:23,310
than Hardware could and as a result we

00:01:21,390 --> 00:01:25,190
had to sort of growth we had to take

00:01:23,310 --> 00:01:28,110
multiple machines connect them together

00:01:25,190 --> 00:01:30,750
distribute the workload to meet those

00:01:28,110 --> 00:01:32,730
demands but it didn't stop their

00:01:30,750 --> 00:01:35,520
workloads kept getting larger and larger

00:01:32,730 --> 00:01:37,140
we kept requiring more compute than you

00:01:35,520 --> 00:01:39,060
could get simply by varying the few

00:01:37,140 --> 00:01:40,440
machines together and that's why you

00:01:39,060 --> 00:01:42,900
came up with the idea of data centers

00:01:40,440 --> 00:01:45,180
where you take a whole bunch of compute

00:01:42,900 --> 00:01:46,740
a whole bunch of storage and really

00:01:45,180 --> 00:01:48,780
high-speed nerd connects and connect

00:01:46,740 --> 00:01:50,430
this all together and then you can

00:01:48,780 --> 00:01:52,260
access these remotely and then you can

00:01:50,430 --> 00:01:54,720
have people share the resources

00:01:52,260 --> 00:01:57,000
everybody gets the impression of having

00:01:54,720 --> 00:02:00,440
lots and lots of compute power loss

00:01:57,000 --> 00:02:03,900
storage lots of communication bandwidth

00:02:00,440 --> 00:02:05,850
but did we meet the demands then of

00:02:03,900 --> 00:02:08,160
course not it's going to keep growing

00:02:05,850 --> 00:02:10,530
it's a trend and will continue and it's

00:02:08,160 --> 00:02:12,600
probably going to get worse because we

00:02:10,530 --> 00:02:15,090
live in a data centric world everything

00:02:12,600 --> 00:02:16,230
we do have everything we don't do

00:02:15,090 --> 00:02:18,180
general

00:02:16,230 --> 00:02:20,250
and that needs to get processed and if

00:02:18,180 --> 00:02:21,990
you don't get processed immediately as

00:02:20,250 --> 00:02:23,520
it's getting streamed in there you're

00:02:21,990 --> 00:02:25,080
gonna have this backlog that you just

00:02:23,520 --> 00:02:34,620
have to throw away because you cannot

00:02:25,080 --> 00:02:38,850
get around to prospect bad because so

00:02:34,620 --> 00:02:40,320
how do you solve that well you take data

00:02:38,850 --> 00:02:42,570
from storage you bring it into a

00:02:40,320 --> 00:02:44,700
computer server process it and take it

00:02:42,570 --> 00:02:47,610
back how much time did you spend doing

00:02:44,700 --> 00:02:50,340
actual work the time it was in the CPU

00:02:47,610 --> 00:02:52,530
what about all the other stuff all the

00:02:50,340 --> 00:02:54,840
other cycles that are wasted is bringing

00:02:52,530 --> 00:02:57,150
it to where that had to be computed

00:02:54,840 --> 00:02:59,700
that's a big problem that's a lot of

00:02:57,150 --> 00:03:02,460
cycles wasted and we need to use those

00:02:59,700 --> 00:03:03,870
cycles we need to compute on data when

00:03:02,460 --> 00:03:05,760
it's sitting on storage we need to

00:03:03,870 --> 00:03:07,230
compute on data when it's moving through

00:03:05,760 --> 00:03:09,060
the network hey if you can stop them

00:03:07,230 --> 00:03:09,900
from moving all together that's even

00:03:09,060 --> 00:03:11,840
better

00:03:09,900 --> 00:03:14,280
we can we need to start computing

00:03:11,840 --> 00:03:16,560
everywhere in this data center not just

00:03:14,280 --> 00:03:20,730
this one set of compute servers

00:03:16,560 --> 00:03:23,280
how do we do that well new generation

00:03:20,730 --> 00:03:25,470
will assist sort of happening now we're

00:03:23,280 --> 00:03:27,750
putting FPGA it's where does where does

00:03:25,470 --> 00:03:30,360
one put an SVG in a data center you

00:03:27,750 --> 00:03:31,650
could put them at the entry points of

00:03:30,360 --> 00:03:34,530
notes when data is coming in off

00:03:31,650 --> 00:03:37,260
networks which put an FPGA there let the

00:03:34,530 --> 00:03:41,130
FPGA to the computation instead of the

00:03:37,260 --> 00:03:44,000
CPU of the cat or if it really has to go

00:03:41,130 --> 00:03:47,220
to CPU put an FPGA inside the NIC

00:03:44,000 --> 00:03:49,230
specialized the application-specific the

00:03:47,220 --> 00:03:51,209
use case specific to how you're

00:03:49,230 --> 00:03:53,190
processing those network packets don't

00:03:51,209 --> 00:03:56,040
just rely on a standard NIC architecture

00:03:53,190 --> 00:03:57,750
to do that for you you could take

00:03:56,040 --> 00:03:59,760
Microsoft catechol one approach where

00:03:57,750 --> 00:04:02,040
you connect a bunch of SPG's together

00:03:59,760 --> 00:04:04,110
with these high-speed interconnects in a

00:04:02,040 --> 00:04:06,390
secondary backing network where you can

00:04:04,110 --> 00:04:10,080
go from chip to chip in hundred

00:04:06,390 --> 00:04:11,640
nanoseconds which is incredibly fast you

00:04:10,080 --> 00:04:13,860
could put a features on top of storage

00:04:11,640 --> 00:04:15,930
so you compute a data while it's sitting

00:04:13,860 --> 00:04:18,120
there and destroy servers so for trivial

00:04:15,930 --> 00:04:19,950
operations you're not bringing petabytes

00:04:18,120 --> 00:04:20,669
of data in just to let's say add two

00:04:19,950 --> 00:04:24,150
numbers together

00:04:20,669 --> 00:04:26,400
you can do it there and because SPG's is

00:04:24,150 --> 00:04:28,860
a resource on its own it doesn't need a

00:04:26,400 --> 00:04:29,520
cpu you can actually have a bunch of

00:04:28,860 --> 00:04:31,349
these FPGA

00:04:29,520 --> 00:04:33,000
is sitting connected to the network

00:04:31,349 --> 00:04:34,680
where you're coming in to believe

00:04:33,000 --> 00:04:36,629
programming then programming your stuff

00:04:34,680 --> 00:04:40,680
on it and then using this as part of

00:04:36,629 --> 00:04:42,210
your allocation in the data center all

00:04:40,680 --> 00:04:43,770
wonderful stuff that you can do with it

00:04:42,210 --> 00:04:46,050
for here so there's sort of a summary of

00:04:43,770 --> 00:04:50,580
all the ways in which you can find an

00:04:46,050 --> 00:04:54,569
FPGA in data center today but with life

00:04:50,580 --> 00:04:56,909
as much as we like to believe that FPDs

00:04:54,569 --> 00:04:59,669
were made for data centers they weren't

00:04:56,909 --> 00:05:03,360
the major FPGA market was actually

00:04:59,669 --> 00:05:06,240
routers cisco routers and why is that

00:05:03,360 --> 00:05:08,819
important because for us to use the FPGA

00:05:06,240 --> 00:05:12,060
effectively today we actually have to

00:05:08,819 --> 00:05:14,669
understand why that device came into

00:05:12,060 --> 00:05:17,669
importance and then draw on those

00:05:14,669 --> 00:05:20,250
positive aspects of it as we're gonna

00:05:17,669 --> 00:05:23,099
see in more detail so cisco routers have

00:05:20,250 --> 00:05:26,909
FPGAs that you use for updating

00:05:23,099 --> 00:05:31,409
protocols over time of course if you

00:05:26,909 --> 00:05:33,810
have an FPGA in the router you can do

00:05:31,409 --> 00:05:35,520
more than that so for example high

00:05:33,810 --> 00:05:37,710
frequency trading companies they can

00:05:35,520 --> 00:05:40,949
actually do trades within the router

00:05:37,710 --> 00:05:43,289
which is incredibly fast and something

00:05:40,949 --> 00:05:49,409
you need today to stay competitive in

00:05:43,289 --> 00:05:51,750
this area of course FPGA is as with all

00:05:49,409 --> 00:05:53,729
technology gets bigger gets better and

00:05:51,750 --> 00:05:55,560
so their company comes a point where

00:05:53,729 --> 00:05:59,039
you're thinking well this device has

00:05:55,560 --> 00:06:00,029
lots of DSP units lots of memory maybe I

00:05:59,039 --> 00:06:02,550
could use this as a traditional

00:06:00,029 --> 00:06:05,310
accelerators like GPUs dumps like maybe

00:06:02,550 --> 00:06:08,610
I could take out my GPU and replace that

00:06:05,310 --> 00:06:11,639
with an FPGA sure for some applications

00:06:08,610 --> 00:06:14,159
yeah you can but really that's not what

00:06:11,639 --> 00:06:16,469
the SPG is all about it's not just an

00:06:14,159 --> 00:06:19,169
accelerator for your matrix

00:06:16,469 --> 00:06:21,150
multiplications or vector ads it's more

00:06:19,169 --> 00:06:24,840
than that it can do a whole bunch of

00:06:21,150 --> 00:06:28,800
complicated computations but more

00:06:24,840 --> 00:06:31,650
importantly this you do not have to

00:06:28,800 --> 00:06:34,889
distinguish between communication and

00:06:31,650 --> 00:06:37,229
computation as explicit operations where

00:06:34,889 --> 00:06:38,699
you're to a memory buffer copies the

00:06:37,229 --> 00:06:41,729
gain and the gain and gain to copy data

00:06:38,699 --> 00:06:43,420
from network to kernel space user space

00:06:41,729 --> 00:06:46,360
all that stuff you do in the

00:06:43,420 --> 00:06:47,800
on the CPU you can tightly couple these

00:06:46,360 --> 00:06:50,170
things you can take data from the

00:06:47,800 --> 00:06:53,380
network straight into your adder and

00:06:50,170 --> 00:06:55,780
then straight back out you have any of

00:06:53,380 --> 00:07:00,280
those overhead processing that takes a

00:06:55,780 --> 00:07:02,590
lot of time on the CPU and so for the

00:07:00,280 --> 00:07:05,560
longest time in our lab we were looking

00:07:02,590 --> 00:07:08,080
at this as our main use case refugees

00:07:05,560 --> 00:07:11,710
tightly coupling communication and

00:07:08,080 --> 00:07:13,150
computation one of our classic hard

00:07:11,710 --> 00:07:14,890
mortal applications that we use is

00:07:13,150 --> 00:07:19,930
molecular dynamics will be model in

00:07:14,890 --> 00:07:22,030
n-body system of particles and we were

00:07:19,930 --> 00:07:24,880
able to use this idea of tightly

00:07:22,030 --> 00:07:27,190
coupling communication computation in a

00:07:24,880 --> 00:07:28,810
dedicated FPGA cluster and we got an

00:07:27,190 --> 00:07:31,630
order of magnitude better performance in

00:07:28,810 --> 00:07:33,790
your traditional CPU GPU cluster and

00:07:31,630 --> 00:07:36,580
we're within one order of magnitude off

00:07:33,790 --> 00:07:39,010
your dedicated PC clusters which cost

00:07:36,580 --> 00:07:44,110
millions of dollars to build which is an

00:07:39,010 --> 00:07:46,020
insanely good result of course you can

00:07:44,110 --> 00:07:48,760
have FPGA is not talked about learning

00:07:46,020 --> 00:07:51,520
we actually looked at noodle networks

00:07:48,760 --> 00:07:54,640
specifically convolutional neural net

00:07:51,520 --> 00:07:58,870
training and what we saw was that you

00:07:54,640 --> 00:08:01,510
can take the idea of layers within that

00:07:58,870 --> 00:08:04,450
Marvel and you don't have to be confined

00:08:01,510 --> 00:08:06,850
to that traditional one FPGA one layer

00:08:04,450 --> 00:08:08,050
there does it stop passes it on to the

00:08:06,850 --> 00:08:09,880
next step is you're holding the next

00:08:08,050 --> 00:08:11,920
layer you can be more fluid than that

00:08:09,880 --> 00:08:14,140
you can split your layer amongst

00:08:11,920 --> 00:08:16,360
multiple FPGA and because of this 100

00:08:14,140 --> 00:08:18,450
nanoseconds chip the chip connectivity

00:08:16,360 --> 00:08:22,660
it's as if if everything is on the same

00:08:18,450 --> 00:08:26,020
FPGA and so you can scale up to 80% a

00:08:22,660 --> 00:08:31,090
you get 98 percent utilization off your

00:08:26,020 --> 00:08:34,050
chip by doing so in doing so but of

00:08:31,090 --> 00:08:36,310
course that was half the picture

00:08:34,050 --> 00:08:38,890
Microsoft comes along and this shows you

00:08:36,310 --> 00:08:42,640
know what you guys are doing application

00:08:38,890 --> 00:08:45,630
support on your FPGAs why not split

00:08:42,640 --> 00:08:48,190
these two things apart might not provide

00:08:45,630 --> 00:08:49,810
not just application support for the

00:08:48,190 --> 00:08:51,640
thing that you're doing for provides

00:08:49,810 --> 00:08:54,070
system support for all applications

00:08:51,640 --> 00:08:56,740
they're going to run on the machine and

00:08:54,070 --> 00:08:57,260
so they implement a whole bunch of

00:08:56,740 --> 00:08:59,600
important

00:08:57,260 --> 00:09:01,190
enter system functionality like

00:08:59,600 --> 00:09:03,770
encryption and likes offered by

00:09:01,190 --> 00:09:06,770
networking on their FPGAs and they show

00:09:03,770 --> 00:09:08,780
that you can actually not just save CPU

00:09:06,770 --> 00:09:10,940
cycles which you can then hand over to

00:09:08,780 --> 00:09:13,160
tenants who can and you know make more

00:09:10,940 --> 00:09:14,390
money off it but you can also get quite

00:09:13,160 --> 00:09:16,640
a bit of speed up with this because

00:09:14,390 --> 00:09:19,580
you're doing all these things without

00:09:16,640 --> 00:09:21,410
having to go through that CPU stack

00:09:19,580 --> 00:09:22,760
which is you know if it's in drop base

00:09:21,410 --> 00:09:24,590
then you're spending a whole bunch of

00:09:22,760 --> 00:09:26,030
cycles there and if it's pulling base

00:09:24,590 --> 00:09:27,710
then you're back to the same problem

00:09:26,030 --> 00:09:30,200
where you're spending a CPU thread

00:09:27,710 --> 00:09:31,400
completely on doing the pulling and

00:09:30,200 --> 00:09:35,390
that's really not something you can

00:09:31,400 --> 00:09:37,610
afford to do so we started looking at

00:09:35,390 --> 00:09:40,430
this stuff that system support what can

00:09:37,610 --> 00:09:42,230
we do to improve data movement for an

00:09:40,430 --> 00:09:46,550
application and arbitrary application

00:09:42,230 --> 00:09:48,170
running on the machine well one of the

00:09:46,550 --> 00:09:49,970
things we looked at was lossy

00:09:48,170 --> 00:09:51,860
compression you know data is moving to

00:09:49,970 --> 00:09:53,780
the network you don't want it to be in

00:09:51,860 --> 00:09:55,310
precision flowing point you can actually

00:09:53,780 --> 00:09:56,780
start to compress they throw away a lot

00:09:55,310 --> 00:10:00,020
of information that's not really that

00:09:56,780 --> 00:10:02,870
relevant to the computation and what we

00:10:00,020 --> 00:10:06,460
saw was that tenth FPGAs working

00:10:02,870 --> 00:10:11,420
together can outperform the number one

00:10:06,460 --> 00:10:14,330
supercomputer on the i/o 500 which was

00:10:11,420 --> 00:10:17,450
an insane result we also looked at

00:10:14,330 --> 00:10:19,460
collectives which has you thought where

00:10:17,450 --> 00:10:21,920
you want to get all data together and

00:10:19,460 --> 00:10:23,420
operate on it together except we instead

00:10:21,920 --> 00:10:26,060
of waiting for everything to come

00:10:23,420 --> 00:10:27,680
together to one node you just computer

00:10:26,060 --> 00:10:29,630
on it as it flows to that work you add

00:10:27,680 --> 00:10:31,520
numbers while they're moving through the

00:10:29,630 --> 00:10:35,540
router instead of waiting for it to

00:10:31,520 --> 00:10:37,400
reach CPU again if you try to do this

00:10:35,540 --> 00:10:39,590
and software thousands of instructions

00:10:37,400 --> 00:10:43,130
you're doing Hardware you can do it

00:10:39,590 --> 00:10:45,410
online rates with an offload MPI Miller

00:10:43,130 --> 00:10:49,340
again same as before try to do this

00:10:45,410 --> 00:10:53,960
stuff and software takes too long just

00:10:49,340 --> 00:10:56,290
enough busy a hundred a seconds and by

00:10:53,960 --> 00:10:58,430
the way MPI is core to a lot of

00:10:56,290 --> 00:10:59,410
scientific computation that happened

00:10:58,430 --> 00:11:05,960
today

00:10:59,410 --> 00:11:08,660
so given a note with FPGA both separate

00:11:05,960 --> 00:11:09,930
boards and embedded in Nicks and all

00:11:08,660 --> 00:11:11,550
that stuff well

00:11:09,930 --> 00:11:14,070
can you do with what FPGA you know you

00:11:11,550 --> 00:11:15,660
could put you can take your back PGA

00:11:14,070 --> 00:11:16,830
with a dedicated back in network you can

00:11:15,660 --> 00:11:20,460
do Finance it there you can do

00:11:16,830 --> 00:11:23,010
bioinformatics you can take also that

00:11:20,460 --> 00:11:24,240
VGA and do stuff like like you did I

00:11:23,010 --> 00:11:26,580
make some machine learning that we saw

00:11:24,240 --> 00:11:28,950
before that benefits from that vacuum

00:11:26,580 --> 00:11:31,350
network you can take this fvg that's

00:11:28,950 --> 00:11:33,540
tightly coupled with the memory on the

00:11:31,350 --> 00:11:37,020
CPU board and then you can do some

00:11:33,540 --> 00:11:39,630
protocol handling there some sort of

00:11:37,020 --> 00:11:41,250
compression on the smart Meg and then

00:11:39,630 --> 00:11:44,640
you know a whole bunch of things each

00:11:41,250 --> 00:11:46,830
FPGA can be used for a whole bunch of

00:11:44,640 --> 00:11:52,050
tasks every media that we're putting it

00:11:46,830 --> 00:11:54,000
there has important use cases which

00:11:52,050 --> 00:11:59,279
brings me to my second part of the talk

00:11:54,000 --> 00:12:02,149
and that is we have a few GS in the data

00:11:59,279 --> 00:12:05,610
center but are we using them effectively

00:12:02,149 --> 00:12:07,350
how do we use them effectively because

00:12:05,610 --> 00:12:09,870
there's a difference between providing a

00:12:07,350 --> 00:12:12,360
resource and then providing or

00:12:09,870 --> 00:12:15,300
everything that has to happen for an

00:12:12,360 --> 00:12:17,760
ordinary developer to use that to

00:12:15,300 --> 00:12:19,500
actually be productive with it or rather

00:12:17,760 --> 00:12:21,089
forget productivity just to get on to

00:12:19,500 --> 00:12:23,130
using it that's the first step and

00:12:21,089 --> 00:12:26,700
that's even that's really complicated

00:12:23,130 --> 00:12:28,350
with that PDS today so that's where

00:12:26,700 --> 00:12:29,850
we're working on this framework that you

00:12:28,350 --> 00:12:32,580
call harness hardware as a

00:12:29,850 --> 00:12:34,890
reconfigurable elastic and specialized

00:12:32,580 --> 00:12:38,279
service and there's a meaning cousin am

00:12:34,890 --> 00:12:42,690
beyond the acronym one you don't see the

00:12:38,279 --> 00:12:45,600
word fpga there why is that because fpga

00:12:42,690 --> 00:12:48,690
is a manifestation of this specialized

00:12:45,600 --> 00:12:51,300
hardware technology we don't expect FPDs

00:12:48,690 --> 00:12:53,790
to continue to be the way that you can

00:12:51,300 --> 00:12:55,350
achieve this outcome so we don't want

00:12:53,790 --> 00:12:57,480
that all the effort that we go into

00:12:55,350 --> 00:12:59,400
coming up with this framework be so

00:12:57,480 --> 00:13:01,200
specialized to this particular

00:12:59,400 --> 00:13:03,450
technology or this particular

00:13:01,200 --> 00:13:05,100
formulation of the technology that it

00:13:03,450 --> 00:13:06,329
just gets thrown out the window the

00:13:05,100 --> 00:13:09,860
moment something comes along that

00:13:06,329 --> 00:13:12,630
replaces FPGAs as reconfigurable logic

00:13:09,860 --> 00:13:15,209
ii looking at the three individual terms

00:13:12,630 --> 00:13:17,399
we get reconfiguration right you want to

00:13:15,209 --> 00:13:19,260
be able to be application specific you

00:13:17,399 --> 00:13:21,810
want to be able to change our wear based

00:13:19,260 --> 00:13:22,529
on requirements why specialized as

00:13:21,810 --> 00:13:26,089
separate

00:13:22,529 --> 00:13:27,990
from reconfigurable because being

00:13:26,089 --> 00:13:30,329
reconfiguration built sorry being

00:13:27,990 --> 00:13:31,889
reconsider ball doesn't necessarily mean

00:13:30,329 --> 00:13:34,079
that in a data center you can be

00:13:31,889 --> 00:13:35,939
specialized to your use case and we're

00:13:34,079 --> 00:13:38,310
seeing that a lot and more and more of

00:13:35,939 --> 00:13:40,740
this because take example of let's say

00:13:38,310 --> 00:13:43,199
machine learning you may be provided

00:13:40,740 --> 00:13:45,180
with an FPGA that can implement a

00:13:43,199 --> 00:13:48,029
machine learning model or a neural

00:13:45,180 --> 00:13:49,709
network model but that model may not be

00:13:48,029 --> 00:13:51,240
exactly what you're trying to build this

00:13:49,709 --> 00:13:53,160
could be the closest thing because

00:13:51,240 --> 00:13:55,470
effectively you're not being given to

00:13:53,160 --> 00:13:57,809
control the entire chip you're just

00:13:55,470 --> 00:13:59,430
being given a set of IP cores that you

00:13:57,809 --> 00:14:02,430
can choose from to implement yourself

00:13:59,430 --> 00:14:05,999
you're getting the best case as opposed

00:14:02,430 --> 00:14:07,559
to the best possible and so this is

00:14:05,999 --> 00:14:09,629
different being reconfigurable does not

00:14:07,559 --> 00:14:10,949
guarantee that your specialized we want

00:14:09,629 --> 00:14:12,930
to do both we want to get people

00:14:10,949 --> 00:14:14,610
hardware that they can change and we

00:14:12,930 --> 00:14:17,459
want to give them the capability of

00:14:14,610 --> 00:14:19,499
changing it and making it exactly what

00:14:17,459 --> 00:14:21,629
they need it to be to make the

00:14:19,499 --> 00:14:24,779
application work in the best possible

00:14:21,629 --> 00:14:26,189
way and then elastic previews are big

00:14:24,779 --> 00:14:28,439
devices they're no longer they're tiny

00:14:26,189 --> 00:14:30,480
chips that can do only one small thing

00:14:28,439 --> 00:14:32,639
so you can do multiple things on the

00:14:30,480 --> 00:14:35,220
FPGA so it's important that everyone

00:14:32,639 --> 00:14:39,860
plays nice and shares that FPGA to get

00:14:35,220 --> 00:14:42,540
maximum utilization out of the chip so

00:14:39,860 --> 00:14:44,059
programming an FPGA is hard but it's

00:14:42,540 --> 00:14:48,120
important to identify where that

00:14:44,059 --> 00:14:50,490
difficulty comes in so if you look at

00:14:48,120 --> 00:14:53,009
the traditional way which is assembly

00:14:50,490 --> 00:14:55,410
based programming it takes a lot of time

00:14:53,009 --> 00:15:01,309
to write code it takes a lot of time to

00:14:55,410 --> 00:15:05,579
compile it to optimize it based on sorry

00:15:01,309 --> 00:15:08,209
so HDL is assembly for FPGA is just

00:15:05,579 --> 00:15:08,209
equivalent of

00:15:10,440 --> 00:15:14,560
that's the people of assembly program

00:15:12,400 --> 00:15:16,000
because you're pretty much specifying

00:15:14,560 --> 00:15:17,920
where each and every single wire is

00:15:16,000 --> 00:15:21,850
unless you use something a real so you

00:15:17,920 --> 00:15:23,170
get some abstractions see optimize

00:15:21,850 --> 00:15:25,180
they're based on the feedback that you

00:15:23,170 --> 00:15:27,910
get and then you have to integrate that

00:15:25,180 --> 00:15:30,250
FPGA computation into the overall flow

00:15:27,910 --> 00:15:33,700
of your workload which will involve CPUs

00:15:30,250 --> 00:15:35,760
and even GPUs to do that if you use high

00:15:33,700 --> 00:15:37,840
level synthesis which is your C code or

00:15:35,760 --> 00:15:39,730
Python or whatever high level

00:15:37,840 --> 00:15:42,190
abstraction and that gets created

00:15:39,730 --> 00:15:44,770
turning to hardware you get rid of that

00:15:42,190 --> 00:15:47,760
most of that development upfront time

00:15:44,770 --> 00:15:50,170
but you still have to compile it and

00:15:47,760 --> 00:15:52,060
oftentimes that a little harder to do

00:15:50,170 --> 00:15:54,370
because you're constraint and what

00:15:52,060 --> 00:15:56,590
you're trying to build and optimization

00:15:54,370 --> 00:15:58,510
is even harder here because you do not

00:15:56,590 --> 00:16:00,220
have that low level control you cannot

00:15:58,510 --> 00:16:03,250
go in and change one bit of this

00:16:00,220 --> 00:16:05,560
particular part of the FPGA you can only

00:16:03,250 --> 00:16:08,170
try to do this by specifying high level

00:16:05,560 --> 00:16:10,060
language code and integration is okay

00:16:08,170 --> 00:16:10,390
that's easy because that's how it's

00:16:10,060 --> 00:16:12,220
built

00:16:10,390 --> 00:16:14,170
you have both support packages you have

00:16:12,220 --> 00:16:17,430
all this infrastructure around it to

00:16:14,170 --> 00:16:20,380
help it integrate into your overall flow

00:16:17,430 --> 00:16:21,820
libraries best possible case if you have

00:16:20,380 --> 00:16:24,220
if you need something and it's already

00:16:21,820 --> 00:16:25,810
existing hardware for it just use that I

00:16:24,220 --> 00:16:26,580
mean it's really it's wonderful it's

00:16:25,810 --> 00:16:29,080
beautiful

00:16:26,580 --> 00:16:30,850
well good what is what if it's not an

00:16:29,080 --> 00:16:32,920
exact match that's what happens most of

00:16:30,850 --> 00:16:35,050
the time what you're looking for is not

00:16:32,920 --> 00:16:38,020
exactly what's available because there's

00:16:35,050 --> 00:16:42,100
so many tiny iterations over what you

00:16:38,020 --> 00:16:43,990
want and what people tend to use or at

00:16:42,100 --> 00:16:46,630
least a general case that they can build

00:16:43,990 --> 00:16:48,160
for it and so you're back to square one

00:16:46,630 --> 00:16:50,350
where you're having to build a system

00:16:48,160 --> 00:16:52,000
around this to compensate for the thing

00:16:50,350 --> 00:16:54,700
that are missing from the core and that

00:16:52,000 --> 00:16:56,320
takes you back two months what we're

00:16:54,700 --> 00:16:58,060
looking to do with hardness is to take

00:16:56,320 --> 00:17:00,160
all this down to less than they are you

00:16:58,060 --> 00:17:02,500
going from idea to an actual

00:17:00,160 --> 00:17:06,459
implementation of that hardware in less

00:17:02,500 --> 00:17:09,250
than a day how do we do that we start

00:17:06,459 --> 00:17:11,830
with C code this is simple C code this

00:17:09,250 --> 00:17:14,260
is not hey I'm writing this for an FPGA

00:17:11,830 --> 00:17:16,690
hence I must structure this differently

00:17:14,260 --> 00:17:17,530
this is your regular C code of course

00:17:16,690 --> 00:17:19,300
you're not going to do stuff like

00:17:17,530 --> 00:17:20,540
recursion because you've got his work

00:17:19,300 --> 00:17:24,170
well

00:17:20,540 --> 00:17:25,880
yes but really basic C code where you're

00:17:24,170 --> 00:17:28,130
really naive stuff where all you're

00:17:25,880 --> 00:17:29,630
doing is just maybe optimizing for cash

00:17:28,130 --> 00:17:31,370
because that works well for the videos

00:17:29,630 --> 00:17:33,470
but not more than that and maybe you're

00:17:31,370 --> 00:17:35,600
especially like some things that you

00:17:33,470 --> 00:17:37,490
think will help to come better along but

00:17:35,600 --> 00:17:40,100
really is no requirement for you to be a

00:17:37,490 --> 00:17:42,470
and have ever even picked up an fpga

00:17:40,100 --> 00:17:46,730
board or knowing that an FPGA exists

00:17:42,470 --> 00:17:48,980
before doing this and then the first

00:17:46,730 --> 00:17:51,680
step is we turn that sequel that you're

00:17:48,980 --> 00:17:54,470
giving us into something that will

00:17:51,680 --> 00:17:56,360
actually map well to heart we make an

00:17:54,470 --> 00:17:57,580
assumption here we maybe make the

00:17:56,360 --> 00:18:00,440
assumption that when you're doing this

00:17:57,580 --> 00:18:02,120
you want to generate high performance

00:18:00,440 --> 00:18:05,510
Hardware you want to generate efficient

00:18:02,120 --> 00:18:08,240
hardware that assumption then allows us

00:18:05,510 --> 00:18:10,280
to tune the code to look at design

00:18:08,240 --> 00:18:12,860
patterns that map will set PCA to

00:18:10,280 --> 00:18:15,770
extract those out of this Seco that's

00:18:12,860 --> 00:18:19,670
given and to then replace them with the

00:18:15,770 --> 00:18:21,440
more efficient version and because the

00:18:19,670 --> 00:18:23,890
process is systematic you can implement

00:18:21,440 --> 00:18:28,700
this as a set of pre compiler passes

00:18:23,890 --> 00:18:31,850
then we compile the CA HDL then the HDL

00:18:28,700 --> 00:18:34,730
bit stream then you provide software

00:18:31,850 --> 00:18:36,880
support for taking that bit stream

00:18:34,730 --> 00:18:39,650
packaging packaging that into a job

00:18:36,880 --> 00:18:42,590
having drivers and OS support for

00:18:39,650 --> 00:18:45,050
putting that job on the FPGA interfacing

00:18:42,590 --> 00:18:48,470
it for the duration of its life and then

00:18:45,050 --> 00:18:51,650
moving data between the FPGA board and

00:18:48,470 --> 00:18:53,360
the CPU and then finally just like you

00:18:51,650 --> 00:18:55,400
have an OS on the CPU

00:18:53,360 --> 00:18:57,110
you kind of need one on the FPGA as well

00:18:55,400 --> 00:18:58,820
because you've got all these resources

00:18:57,110 --> 00:19:01,430
and you've got multiple people sharing

00:18:58,820 --> 00:19:03,590
them you need some way of managing that

00:19:01,430 --> 00:19:05,870
sharing process managing all these

00:19:03,590 --> 00:19:07,760
things are going to happen outside of

00:19:05,870 --> 00:19:11,660
that application that you're putting on

00:19:07,760 --> 00:19:13,610
the FPGA and so the last part of the

00:19:11,660 --> 00:19:17,630
presentation I'm just going to go over

00:19:13,610 --> 00:19:19,550
some of these lists I mean parts of the

00:19:17,630 --> 00:19:20,750
compiler that we saw tool team that we

00:19:19,550 --> 00:19:22,610
saw sorry

00:19:20,750 --> 00:19:24,260
starting off with the C to C compiler

00:19:22,610 --> 00:19:26,930
where this is an example we'll be dead

00:19:24,260 --> 00:19:30,410
for I think this was Newman which is a

00:19:26,930 --> 00:19:33,490
DNA sequence mapping application that's

00:19:30,410 --> 00:19:35,710
what we started with the one at the top

00:19:33,490 --> 00:19:37,900
this is what we got after we put it

00:19:35,710 --> 00:19:40,660
through all those code transformations

00:19:37,900 --> 00:19:43,390
that replace that with systematic

00:19:40,660 --> 00:19:46,990
replacing that with what works well or

00:19:43,390 --> 00:19:48,550
what Maps well for the FPGA and we did

00:19:46,990 --> 00:19:51,610
this with the Intel open shield compiler

00:19:48,550 --> 00:19:57,190
and we got over 100 times speed up from

00:19:51,610 --> 00:19:59,020
that going to that seed a steal now that

00:19:57,190 --> 00:20:01,030
sort of thing that hasn't received that

00:19:59,020 --> 00:20:02,860
much attention before but it's a really

00:20:01,030 --> 00:20:04,330
important part of the problem because

00:20:02,860 --> 00:20:06,460
the way that it's currently done is you

00:20:04,330 --> 00:20:09,100
take your source code you break that

00:20:06,460 --> 00:20:12,310
into chunks into sequences and then you

00:20:09,100 --> 00:20:14,800
look in a library of IP blocks what can

00:20:12,310 --> 00:20:17,260
do this functionality seems pretty

00:20:14,800 --> 00:20:20,260
symbolizes to basic look except the

00:20:17,260 --> 00:20:22,990
problem is functionality overlaps so you

00:20:20,260 --> 00:20:24,480
might think hey I've got an adder there

00:20:22,990 --> 00:20:26,800
so let me just pick up an adder do that

00:20:24,480 --> 00:20:29,620
except an error may be done in multiple

00:20:26,800 --> 00:20:31,780
ways how do you choose the best one and

00:20:29,620 --> 00:20:33,760
if you have lots of large library with

00:20:31,780 --> 00:20:35,350
lots of blocks or lapping this problem

00:20:33,760 --> 00:20:37,300
gets harder and harder to optimize how

00:20:35,350 --> 00:20:39,670
do you select the best configuration

00:20:37,300 --> 00:20:41,440
library blocks connected in the best

00:20:39,670 --> 00:20:43,600
possible way to give us the best

00:20:41,440 --> 00:20:46,240
performance that's a really hard problem

00:20:43,600 --> 00:20:48,010
to solve and that's why the back end

00:20:46,240 --> 00:20:51,400
said athiest a steal comparison today

00:20:48,010 --> 00:20:52,600
tends to be inefficient and actually

00:20:51,400 --> 00:20:55,300
prone to failure from personal

00:20:52,600 --> 00:20:58,030
experience this fails a lot of the time

00:20:55,300 --> 00:20:59,650
because it tries to create this huge map

00:20:58,030 --> 00:21:01,960
and it's ended up ends up using about a

00:20:59,650 --> 00:21:04,330
hundred gigabytes of RAM trying to try

00:21:01,960 --> 00:21:06,220
to do a very simple compilation that

00:21:04,330 --> 00:21:09,850
should take 30% of the chip which is

00:21:06,220 --> 00:21:11,950
nothing but it just completely goes out

00:21:09,850 --> 00:21:15,040
of the way and kind of optimized cannot

00:21:11,950 --> 00:21:17,140
converge to a solution what do we want

00:21:15,040 --> 00:21:20,080
to do we want to take smaller code

00:21:17,140 --> 00:21:22,150
sequences we're going to take very basic

00:21:20,080 --> 00:21:25,420
building blocks think gates thinks very

00:21:22,150 --> 00:21:27,790
basic errors and we want to use those to

00:21:25,420 --> 00:21:30,010
create the design so that we're not

00:21:27,790 --> 00:21:32,410
spending time for and find the best

00:21:30,010 --> 00:21:35,320
possible P we're building it from

00:21:32,410 --> 00:21:37,540
scratch that's tuned to the exact

00:21:35,320 --> 00:21:40,390
application that you're putting onto the

00:21:37,540 --> 00:21:44,110
FPGA another thing the compiler is

00:21:40,390 --> 00:21:46,730
supposed to do is to determine whether

00:21:44,110 --> 00:21:49,280
this compilation - hardware

00:21:46,730 --> 00:21:51,460
is it necessary because when we let's

00:21:49,280 --> 00:21:53,780
say do molecular dynamics we have two

00:21:51,460 --> 00:21:56,390
parts of the problem the first part of

00:21:53,780 --> 00:21:57,380
the top that takes almost no time to do

00:21:56,390 --> 00:21:59,260
it's really simple

00:21:57,380 --> 00:22:02,180
Owain problem the one at the bottom

00:21:59,260 --> 00:22:05,030
that's almost like a wind square not

00:22:02,180 --> 00:22:06,440
quite but almost so you can see that

00:22:05,030 --> 00:22:10,820
what's going to take quite a bit of time

00:22:06,440 --> 00:22:12,200
to do if you do that really quickly then

00:22:10,820 --> 00:22:14,300
you're just sitting and waiting for that

00:22:12,200 --> 00:22:16,340
to happen to get done to you pretty much

00:22:14,300 --> 00:22:18,560
wasted your resource instead what you

00:22:16,340 --> 00:22:21,560
can do is you can put this into a soft

00:22:18,560 --> 00:22:24,320
core into a tiny CPU synthesized inside

00:22:21,560 --> 00:22:25,850
the spgs we're right on top of pipe

00:22:24,320 --> 00:22:27,980
lengths so you get the really fast

00:22:25,850 --> 00:22:29,870
communication but you can operate at a

00:22:27,980 --> 00:22:32,090
lower frequency with fewer resources and

00:22:29,870 --> 00:22:34,970
more of temporal mapping of the

00:22:32,090 --> 00:22:36,590
computation and you end up finishing at

00:22:34,970 --> 00:22:38,900
the exact same time as something that's

00:22:36,590 --> 00:22:41,720
mapped directly onto those pipelines

00:22:38,900 --> 00:22:45,230
onto those lots and it's taking longer

00:22:41,720 --> 00:22:46,640
to compute there's a distinction this

00:22:45,230 --> 00:22:48,290
analysis of when thing that's supposed

00:22:46,640 --> 00:22:50,560
to finish that is something that the

00:22:48,290 --> 00:22:53,540
chdo compiler should be able to do

00:22:50,560 --> 00:22:56,690
automatically in terms of software

00:22:53,540 --> 00:22:58,670
support our primary concern is that

00:22:56,690 --> 00:23:00,830
things deprecated really quickly today

00:22:58,670 --> 00:23:02,960
you picked up a driver and provided by a

00:23:00,830 --> 00:23:06,410
vendor a year old will not work anymore

00:23:02,960 --> 00:23:08,420
and it goes without saying it won't work

00:23:06,410 --> 00:23:10,340
for any other vendor so you're stuck

00:23:08,420 --> 00:23:12,320
with this problem where nothing works

00:23:10,340 --> 00:23:14,540
across vendors nothing works across time

00:23:12,320 --> 00:23:18,920
you're just stuck with these old

00:23:14,540 --> 00:23:20,600
redundant useless pieces of software and

00:23:18,920 --> 00:23:21,830
those are becoming bottlenecks because

00:23:20,600 --> 00:23:23,750
the more and more you move towards a

00:23:21,830 --> 00:23:25,340
place where CPUs and if pieces are

00:23:23,750 --> 00:23:26,360
working together there's data transfers

00:23:25,340 --> 00:23:29,720
happening all the time

00:23:26,360 --> 00:23:32,540
you need something that's more specially

00:23:29,720 --> 00:23:33,740
that's better that's faster and you're

00:23:32,540 --> 00:23:36,110
not going to guarantee that you're gonna

00:23:33,740 --> 00:23:38,300
have only one FPGA in your data center

00:23:36,110 --> 00:23:39,770
at all times you might have a sign

00:23:38,300 --> 00:23:42,170
exported you may have an Intel board you

00:23:39,770 --> 00:23:44,090
have a lattice board you should not be

00:23:42,170 --> 00:23:47,210
constrained to using a piece of software

00:23:44,090 --> 00:23:49,850
just because this vendor has its boards

00:23:47,210 --> 00:23:51,200
here and you cannot use the other vendor

00:23:49,850 --> 00:23:53,390
sports so we're looking at building this

00:23:51,200 --> 00:23:55,550
set of software support that's

00:23:53,390 --> 00:23:58,040
independent of the vector that looks at

00:23:55,550 --> 00:24:00,320
the FPGA because really think about it

00:23:58,040 --> 00:24:03,409
the FPGA set across a stand

00:24:00,320 --> 00:24:05,509
bus why should the driver be that's

00:24:03,409 --> 00:24:09,309
dedicated to a vendor when you're just

00:24:05,509 --> 00:24:09,309
talking over at a standard PCI bus

00:24:09,460 --> 00:24:15,559
that's one things we're looking at the

00:24:13,610 --> 00:24:17,149
operating system that talked about we're

00:24:15,559 --> 00:24:20,000
trying to share the FPGA of being

00:24:17,149 --> 00:24:21,620
multiple tenants and even logic from the

00:24:20,000 --> 00:24:23,630
cloud provider and you're providing

00:24:21,620 --> 00:24:26,570
support for reconfiguring it for

00:24:23,630 --> 00:24:30,049
communication for multiplexing the

00:24:26,570 --> 00:24:32,750
resources between these multiple users

00:24:30,049 --> 00:24:36,340
within the same FPGA fabric and recline

00:24:32,750 --> 00:24:39,049
that operating system Morpheus of course

00:24:36,340 --> 00:24:41,960
we cannot have this discussion without

00:24:39,049 --> 00:24:44,600
this light security if we say we're

00:24:41,960 --> 00:24:46,580
sharing the FPGA hey if you're saying

00:24:44,600 --> 00:24:50,450
we're not in shin-bi Pichet even then it

00:24:46,580 --> 00:24:52,639
is hardware that can be configured that

00:24:50,450 --> 00:24:55,070
is a scary stuff if you're if you are

00:24:52,639 --> 00:24:56,179
specializing in security but you know

00:24:55,070 --> 00:24:57,799
it's a question that needs to be

00:24:56,179 --> 00:24:59,990
answered and we're strong enough its

00:24:57,799 --> 00:25:02,210
burned this as well how do you make this

00:24:59,990 --> 00:25:04,129
thing secure how do you tell people that

00:25:02,210 --> 00:25:05,509
you can use this hardware and there's

00:25:04,129 --> 00:25:07,850
their neighbor who's using the same

00:25:05,509 --> 00:25:10,220
hardware as well you're safe how do we

00:25:07,850 --> 00:25:11,899
give appropriate guarantees to actually

00:25:10,220 --> 00:25:12,440
convince people of this thing we'll be

00:25:11,899 --> 00:25:16,159
fine

00:25:12,440 --> 00:25:19,159
you want to lose your item and so with

00:25:16,159 --> 00:25:20,600
that that's my talk thank you for

00:25:19,159 --> 00:25:22,720
listening and I'll be happy to take

00:25:20,600 --> 00:25:22,720
questions

00:25:39,510 --> 00:25:48,070
so the second thing and that's the phone

00:25:45,310 --> 00:25:51,010
so the second thing is so the the once

00:25:48,070 --> 00:25:52,660
light which talked about the soft course

00:25:51,010 --> 00:25:54,430
and so on you might recall that he said

00:25:52,660 --> 00:25:56,110
yeah we can explicitly doing things

00:25:54,430 --> 00:25:58,030
there on the soft core and the rest of

00:25:56,110 --> 00:26:00,550
this working there there's actually a

00:25:58,030 --> 00:26:02,860
little bit more behind that the thing is

00:26:00,550 --> 00:26:05,020
that we have for evening HPC works

00:26:02,860 --> 00:26:07,750
specifically today we are already using

00:26:05,020 --> 00:26:10,150
C C++ code and annotating it's this

00:26:07,750 --> 00:26:12,640
takes on the form currently of openmp

00:26:10,150 --> 00:26:14,830
and similar similar forms in which we

00:26:12,640 --> 00:26:16,480
can just provide the tools with

00:26:14,830 --> 00:26:18,610
additional pieces of information but

00:26:16,480 --> 00:26:21,520
this is exactly what we need to actually

00:26:18,610 --> 00:26:23,890
guide the compiler into deciding which

00:26:21,520 --> 00:26:26,560
part of the code is actually the kernel

00:26:23,890 --> 00:26:28,060
the performance relevant parts and which

00:26:26,560 --> 00:26:31,330
part is just as supporting

00:26:28,060 --> 00:26:33,010
infrastructure the support industry

00:26:31,330 --> 00:26:35,710
infrastructure is not performance

00:26:33,010 --> 00:26:38,470
relevant we can compactly represent this

00:26:35,710 --> 00:26:40,360
in normal execution code so in this case

00:26:38,470 --> 00:26:42,640
we're have selected with five as that

00:26:40,360 --> 00:26:45,180
because we can synthesize an appropriate

00:26:42,640 --> 00:26:49,030
soft course very easily and legally and

00:26:45,180 --> 00:26:51,880
we are done just integrating the rest of

00:26:49,030 --> 00:26:55,720
the code to kernels in what transforming

00:26:51,880 --> 00:26:57,670
them into HDL which by the magic of how

00:26:55,720 --> 00:26:59,350
the instruction set and everything

00:26:57,670 --> 00:27:03,010
around Swiss five is actually structured

00:26:59,350 --> 00:27:04,630
we can in a very efficient way interact

00:27:03,010 --> 00:27:06,040
with each other so if your interest in

00:27:04,630 --> 00:27:07,810
these kind of things we can talk to you

00:27:06,040 --> 00:27:10,090
about this it's a little bit too

00:27:07,810 --> 00:27:13,420
detailed to go into this year right now

00:27:10,090 --> 00:27:16,390
but I think we have found a way together

00:27:13,420 --> 00:27:19,320
with the Morpheus infrastructure of the

00:27:16,390 --> 00:27:23,110
operating system to really make an FPGA

00:27:19,320 --> 00:27:25,380
more looking like a traditional computer

00:27:23,110 --> 00:27:28,300
where parts of it we completely

00:27:25,380 --> 00:27:30,820
reconfigure when we actually need it so

00:27:28,300 --> 00:27:34,020
we have processors running on the FPGA

00:27:30,820 --> 00:27:37,390
which are part of the operating system

00:27:34,020 --> 00:27:40,660
infrastructure and when someone comes up

00:27:37,390 --> 00:27:42,610
with a nice used for actual HDL for IP

00:27:40,660 --> 00:27:45,640
blocks which we can instantiate on the

00:27:42,610 --> 00:27:47,440
FPGA we can accommodate that as well in

00:27:45,640 --> 00:27:49,480
addition to doing this for much of a

00:27:47,440 --> 00:27:51,610
chance at the same time this is by

00:27:49,480 --> 00:27:53,760
itself so Ahmed has dug up a couple of

00:27:51,610 --> 00:27:56,350
first there but people have expressed

00:27:53,760 --> 00:27:58,809
concerned about multi-tenancy so you can

00:27:56,350 --> 00:28:00,250
imagine these are electrical wires and

00:27:58,809 --> 00:28:02,799
they are very close to each other if you

00:28:00,250 --> 00:28:04,450
have a hostile work load somewhere and

00:28:02,799 --> 00:28:07,270
they can actually monitor what the

00:28:04,450 --> 00:28:10,210
electrical signals are between these

00:28:07,270 --> 00:28:13,330
these connecting these IP blocks so

00:28:10,210 --> 00:28:15,520
theoretically just a theoretical promise

00:28:13,330 --> 00:28:17,559
so we're looking and actually getting

00:28:15,520 --> 00:28:20,559
some form of grantees there and this

00:28:17,559 --> 00:28:22,960
will require us to actually have control

00:28:20,559 --> 00:28:24,630
not just over operating system these

00:28:22,960 --> 00:28:26,890
kind of things but also the tool chain

00:28:24,630 --> 00:28:29,850
so that's another thing which we are

00:28:26,890 --> 00:28:32,860
actually have been working on so we are

00:28:29,850 --> 00:28:35,350
going to provide what they are already

00:28:32,860 --> 00:28:38,429
to some extent some two chains out there

00:28:35,350 --> 00:28:40,900
so we are currently paying a company to

00:28:38,429 --> 00:28:42,850
come up with a much more complete

00:28:40,900 --> 00:28:44,950
solution for these kind of things this

00:28:42,850 --> 00:28:46,690
will be for some simple FPGA is at the

00:28:44,950 --> 00:28:50,500
beginning but we hope that this will

00:28:46,690 --> 00:28:53,740
then lead to us having the same kind of

00:28:50,500 --> 00:28:55,419
support for everything on the FPGA not

00:28:53,740 --> 00:28:57,929
just the small ones but also the bigger

00:28:55,419 --> 00:29:00,100
ones and then we have an ecosystem which

00:28:57,929 --> 00:29:02,590
extends the free software world from

00:29:00,100 --> 00:29:04,000
what we know today and the next site to

00:29:02,590 --> 00:29:06,010
even the hardware side where we have

00:29:04,000 --> 00:29:08,230
reconfigure hardware and then comes in

00:29:06,010 --> 00:29:11,169
what Ahmed mentions perhaps in future we

00:29:08,230 --> 00:29:14,100
will not have exquisite FPGAs we might

00:29:11,169 --> 00:29:16,510
have a new types of computer where the

00:29:14,100 --> 00:29:18,730
differentiation between hardware and

00:29:16,510 --> 00:29:21,220
software becomes much more fluid and we

00:29:18,730 --> 00:29:24,990
have something available which is able

00:29:21,220 --> 00:29:24,990
to handle this and programmers

00:29:31,880 --> 00:29:36,830
yeah so just for yeah reiterate sorry so

00:29:34,730 --> 00:29:38,120
when you say softcore if you need a

00:29:36,830 --> 00:29:40,400
softcore you're actually using or risk

00:29:38,120 --> 00:29:44,169
five so so and then you can just

00:29:40,400 --> 00:29:44,169
effectively compile to it

00:29:51,750 --> 00:30:00,150
we don't plan on using parcours that's

00:29:56,750 --> 00:30:02,630
additional cost is their lack of

00:30:00,150 --> 00:30:02,630
transparency

00:30:10,440 --> 00:30:16,380
I think that's always will that's ahmed

00:30:14,310 --> 00:30:19,850
go back to work

00:30:16,380 --> 00:30:19,850
yes it that much

00:30:26,210 --> 00:30:29,410

YouTube URL: https://www.youtube.com/watch?v=wwwtyIWrTZ0


