Title: MLFlow: Experiment Tracking on OpenShift
Publication date: 2019-10-02
Playlist: DevConfUS 2019
Description: 
	Speakers: Hema Veeradhi and Zak Hassan

Machine learning is more complex than traditional software development. We all know that fetching the right parameters for tuning a machine learning model is quintessential in finding the optimal model.

Have multiple machine learning models with different parameters running, but do not have a platform to track each of these models? MLFlow is the new open source platform which allows data scientists, product engineers, developers and large organizations to manage the ML lifecycle, including experimentation, reproducibility and deployment.

Attend this talk to learn how to:
1. Use MLFlow for experiment tracking
2. Perform hyperparameter model tuning with MLFlow
3. Run, compare and visualize the metrics/parameters of multiple runs of your machine learning model with MLFlow
Captions: 
	00:00:03,390 --> 00:00:15,700
thank you so we're gonna spend the next

00:00:12,519 --> 00:00:17,410
30 minutes to talk about our use case

00:00:15,700 --> 00:00:20,020
which is experiment tracking hyper

00:00:17,410 --> 00:00:21,160
parameter twinning and some

00:00:20,020 --> 00:00:23,550
infrastructure we have to build

00:00:21,160 --> 00:00:26,410
internally solve some of these problems

00:00:23,550 --> 00:00:27,640
so first off I just want to give you a

00:00:26,410 --> 00:00:29,560
little overview of what we're gonna be

00:00:27,640 --> 00:00:32,200
talking about today we're gonna spend

00:00:29,560 --> 00:00:34,629
the next 30 minutes just to go over high

00:00:32,200 --> 00:00:37,989
level what is experiment tracking what

00:00:34,629 --> 00:00:39,280
is hyper parameter tuning and some of

00:00:37,989 --> 00:00:42,370
the challenges when building machine

00:00:39,280 --> 00:00:44,019
learning models we'll give some examples

00:00:42,370 --> 00:00:48,929
of unsupervised supervised machine

00:00:44,019 --> 00:00:56,019
learning and well I'll show you a demo

00:00:48,929 --> 00:00:59,109
and yeah let's get into it so when

00:00:56,019 --> 00:01:00,850
you're building machine learning systems

00:00:59,109 --> 00:01:03,129
right your building may be machine

00:01:00,850 --> 00:01:05,890
learning models and you know there are

00:01:03,129 --> 00:01:08,530
some challenges there you might use

00:01:05,890 --> 00:01:11,290
different algorithms to solve your

00:01:08,530 --> 00:01:16,409
problem you might want to use different

00:01:11,290 --> 00:01:16,409
hyper parameters to get the best results

00:01:16,860 --> 00:01:21,280
sometimes experiments are hard to

00:01:19,210 --> 00:01:25,119
reproduce so if you deploy something a

00:01:21,280 --> 00:01:26,049
model into production you want to have

00:01:25,119 --> 00:01:30,189
kind of metadata of

00:01:26,049 --> 00:01:32,500
what Piper parameters were use what all

00:01:30,189 --> 00:01:35,049
these details what data set maybe they

00:01:32,500 --> 00:01:36,850
get hash if it was stored and get so you

00:01:35,049 --> 00:01:40,270
can kind of know exactly what if code

00:01:36,850 --> 00:01:42,280
caused this problem and then you know

00:01:40,270 --> 00:01:45,310
when you're training models there might

00:01:42,280 --> 00:01:46,719
be you know latency in performance like

00:01:45,310 --> 00:01:48,609
for example when you're doing grid

00:01:46,719 --> 00:01:50,490
search it might be you know

00:01:48,609 --> 00:01:54,119
time-consuming to search all these

00:01:50,490 --> 00:01:54,119
different hyper parameters

00:01:56,640 --> 00:02:04,110
so um typical you know software

00:02:00,450 --> 00:02:07,410
engineering involves what like you write

00:02:04,110 --> 00:02:09,720
your code you test your code you deploy

00:02:07,410 --> 00:02:12,150
your code you maybe do some integration

00:02:09,720 --> 00:02:17,610
tests and some CI stuff you monitor your

00:02:12,150 --> 00:02:20,460
code pretty simple but then but then

00:02:17,610 --> 00:02:22,080
like to be successful at software

00:02:20,460 --> 00:02:24,870
engineering and then be successful in

00:02:22,080 --> 00:02:25,950
building machine learning systems they

00:02:24,870 --> 00:02:28,020
have different goals for example

00:02:25,950 --> 00:02:32,400
software engineering your goal is to

00:02:28,020 --> 00:02:34,770
meet a functional specification with XI

00:02:32,400 --> 00:02:37,739
learning your goal is to optimize the

00:02:34,770 --> 00:02:44,010
metric for example accuracy of your

00:02:37,739 --> 00:02:46,650
prediction and quality quality is

00:02:44,010 --> 00:02:49,830
measured maybe in traditional software

00:02:46,650 --> 00:02:52,290
engineering in your code and she

00:02:49,830 --> 00:02:56,459
learning quality is measured with a code

00:02:52,290 --> 00:02:59,640
the data and tuning it and you're gonna

00:02:56,459 --> 00:03:02,610
be regularly you're gonna need regular

00:02:59,640 --> 00:03:06,390
up updated data sets upon continuously

00:03:02,610 --> 00:03:07,769
training your model and then the process

00:03:06,390 --> 00:03:09,900
continues you know you're gonna be

00:03:07,769 --> 00:03:11,730
constantly experimenting with maybe

00:03:09,900 --> 00:03:17,519
different libraries and different models

00:03:11,730 --> 00:03:20,340
and trying to productionize it and we'll

00:03:17,519 --> 00:03:26,910
dig into more details on what all this

00:03:20,340 --> 00:03:31,769
stuff means so what are hyper parameters

00:03:26,910 --> 00:03:35,100
right so when you when you're creating a

00:03:31,769 --> 00:03:36,870
machine learning model you'll you'll be

00:03:35,100 --> 00:03:39,810
presented with like different ways to

00:03:36,870 --> 00:03:42,480
define your model architecture at first

00:03:39,810 --> 00:03:48,720
you don't know what the optimal

00:03:42,480 --> 00:03:51,390
architecture is you sometimes like is

00:03:48,720 --> 00:03:53,400
finding the right hyper parameters kind

00:03:51,390 --> 00:03:57,470
of like a black magic right like you

00:03:53,400 --> 00:04:00,150
don't know exactly how many how many

00:03:57,470 --> 00:04:03,690
clusters should you have been maybe

00:04:00,150 --> 00:04:06,599
k-means for example right you might have

00:04:03,690 --> 00:04:09,300
to try different numbers of clusters to

00:04:06,599 --> 00:04:09,880
find the optimal one will show you more

00:04:09,300 --> 00:04:13,450
if

00:04:09,880 --> 00:04:17,380
how do that but um just remember this

00:04:13,450 --> 00:04:19,570
one thing parameters that which you

00:04:17,380 --> 00:04:22,600
defined in your model architecture are

00:04:19,570 --> 00:04:26,110
hyper parameters and the process of

00:04:22,600 --> 00:04:28,780
searching for the ideal model

00:04:26,110 --> 00:04:36,340
architecture is referred to as hyper

00:04:28,780 --> 00:04:39,070
parameter tuning so helm is gonna tell

00:04:36,340 --> 00:04:42,370
us some some examples of some hyper

00:04:39,070 --> 00:04:46,720
parameters that and give us more deeper

00:04:42,370 --> 00:04:50,050
details yeah so typically hyper

00:04:46,720 --> 00:04:52,660
parameters are numerical in nature so

00:04:50,050 --> 00:04:54,850
I've just given a few of them that we

00:04:52,660 --> 00:04:56,320
commonly sort of use in a machine

00:04:54,850 --> 00:04:59,710
learning model that you're trying to

00:04:56,320 --> 00:05:01,480
train so one such example is the number

00:04:59,710 --> 00:05:04,210
of epochs and the number of epochs

00:05:01,480 --> 00:05:06,310
actually go hand-in-hand with another

00:05:04,210 --> 00:05:09,160
hyper parameter which is the learning

00:05:06,310 --> 00:05:11,020
rate so when you have a machine learning

00:05:09,160 --> 00:05:13,540
model you obviously have a huge amount

00:05:11,020 --> 00:05:15,730
of data so this is what we call as the

00:05:13,540 --> 00:05:18,160
training data that you need to feed to

00:05:15,730 --> 00:05:20,280
your model and with a lot of memory

00:05:18,160 --> 00:05:23,380
constraints you cannot feed the entire

00:05:20,280 --> 00:05:25,600
training data set at once so you would

00:05:23,380 --> 00:05:29,260
need to split these up into smaller

00:05:25,600 --> 00:05:31,990
sizes which we call as batches so and

00:05:29,260 --> 00:05:35,740
what you do is you basically feed each

00:05:31,990 --> 00:05:39,910
batch one at a time to your model for it

00:05:35,740 --> 00:05:42,910
to train so a complete so when every

00:05:39,910 --> 00:05:45,640
batch is basically fed into the model at

00:05:42,910 --> 00:05:48,070
least once is what we say as one

00:05:45,640 --> 00:05:51,280
complete epoch right so it's basically

00:05:48,070 --> 00:05:53,980
equivalent to when the model each of the

00:05:51,280 --> 00:05:56,080
training data set is completely fed into

00:05:53,980 --> 00:05:58,630
your model is what comprises of one

00:05:56,080 --> 00:06:01,540
Deepak and the learning rates are

00:05:58,630 --> 00:06:04,480
basically a small value which ranges

00:06:01,540 --> 00:06:08,440
typically between 0 & 1 we commonly use

00:06:04,480 --> 00:06:10,330
this in deeper neural network models so

00:06:08,440 --> 00:06:13,240
these learning rates basically allow you

00:06:10,330 --> 00:06:16,030
to control the amount of weights that

00:06:13,240 --> 00:06:18,520
you can assign for your input data so

00:06:16,030 --> 00:06:20,090
there is really no fixed learning rate

00:06:18,520 --> 00:06:23,090
sometimes a high

00:06:20,090 --> 00:06:24,410
it can be really bad for your model

00:06:23,090 --> 00:06:26,840
because in the end you're trying to

00:06:24,410 --> 00:06:28,910
reduce the loss of your model as well so

00:06:26,840 --> 00:06:31,280
you're trying to see where is that

00:06:28,910 --> 00:06:33,889
optimal learning rate for which you're

00:06:31,280 --> 00:06:36,440
able to reduce the loss that the model

00:06:33,889 --> 00:06:38,330
is facing but also you are able to pass

00:06:36,440 --> 00:06:40,820
more number of epochs because obviously

00:06:38,330 --> 00:06:43,160
just having one epoch to train your data

00:06:40,820 --> 00:06:45,229
set is not enough you need to constantly

00:06:43,160 --> 00:06:47,419
keep training your data so increasing

00:06:45,229 --> 00:06:50,540
the epochs but at the same time reducing

00:06:47,419 --> 00:06:52,669
the loss function is what we are looking

00:06:50,540 --> 00:06:57,110
at which are considered as too important

00:06:52,669 --> 00:07:00,320
hyper parameters and then again you have

00:06:57,110 --> 00:07:02,000
in case of neural networks we know that

00:07:00,320 --> 00:07:04,610
a neural network has a single input

00:07:02,000 --> 00:07:06,860
layer and then you have a single output

00:07:04,610 --> 00:07:08,510
layer but the intermediate layer is

00:07:06,860 --> 00:07:10,700
called what we call as the hidden layers

00:07:08,510 --> 00:07:12,979
which comprises of different hidden

00:07:10,700 --> 00:07:15,440
units so what happens in a neural

00:07:12,979 --> 00:07:17,630
network is we're trying to create the

00:07:15,440 --> 00:07:19,430
input signal into a form of the output

00:07:17,630 --> 00:07:21,740
signal which is typically like a 0 or a

00:07:19,430 --> 00:07:24,380
1 and what the hidden layers are doing

00:07:21,740 --> 00:07:26,900
inside is how do we sort of fine tune

00:07:24,380 --> 00:07:29,650
this signal to get that output that we

00:07:26,900 --> 00:07:32,660
want again you have a bunch of

00:07:29,650 --> 00:07:34,460
mathematical signal functions that you

00:07:32,660 --> 00:07:36,770
apply which we call as activation

00:07:34,460 --> 00:07:39,260
functions so these are basically when

00:07:36,770 --> 00:07:41,599
you add weights to each data points

00:07:39,260 --> 00:07:43,520
you're summing them up you're feeding it

00:07:41,599 --> 00:07:45,380
to your hidden layer and inside that

00:07:43,520 --> 00:07:47,930
hidden layer is where you can sort of

00:07:45,380 --> 00:07:49,760
play around with different activation

00:07:47,930 --> 00:07:51,470
functions that you can apply to each of

00:07:49,760 --> 00:07:53,630
them so that you can fine tune your

00:07:51,470 --> 00:07:56,570
input to the required output that you

00:07:53,630 --> 00:07:58,460
want so these are all again user-defined

00:07:56,570 --> 00:08:00,919
parameters which is why we call them as

00:07:58,460 --> 00:08:02,930
hyper parameters and you sort of do a

00:08:00,919 --> 00:08:07,520
trial and error each time that you're

00:08:02,930 --> 00:08:10,220
training your model so now that we know

00:08:07,520 --> 00:08:12,530
what exactly our hyper parameters why do

00:08:10,220 --> 00:08:14,660
we need them right so we need them

00:08:12,530 --> 00:08:16,820
because they directly control the

00:08:14,660 --> 00:08:18,620
behavior of your training algorithm and

00:08:16,820 --> 00:08:20,570
any machine learning performance that

00:08:18,620 --> 00:08:23,870
you're trying to do it has a huge impact

00:08:20,570 --> 00:08:25,820
on the performance of this model so a

00:08:23,870 --> 00:08:30,159
good choice of hyper parameters really

00:08:25,820 --> 00:08:30,159
can make your algorithm stand out

00:08:33,140 --> 00:08:38,899
so machine learning in lifecycle you're

00:08:35,899 --> 00:08:40,459
getting raw data sometimes it's not

00:08:38,899 --> 00:08:43,789
clean data you might have to clean your

00:08:40,459 --> 00:08:48,680
data and then you run your data through

00:08:43,789 --> 00:08:51,970
some training in that training box over

00:08:48,680 --> 00:08:55,459
there that's where you'll be doing some

00:08:51,970 --> 00:08:57,769
some some feature engineering maybe some

00:08:55,459 --> 00:09:01,430
hyper parameter tuning and then you'll

00:08:57,769 --> 00:09:03,260
be the point you're the model maybe it

00:09:01,430 --> 00:09:06,740
could be a physical file or it could be

00:09:03,260 --> 00:09:10,540
a function and the process continues as

00:09:06,740 --> 00:09:13,490
you get new data keep on training

00:09:10,540 --> 00:09:16,940
deploying and and also you want to

00:09:13,490 --> 00:09:20,149
monitor the live system of how well and

00:09:16,940 --> 00:09:26,410
how accurate the results are when you're

00:09:20,149 --> 00:09:29,060
running in a production environment and

00:09:26,410 --> 00:09:32,120
when you're deciding you want to decide

00:09:29,060 --> 00:09:36,250
either to do unsupervised or supervised

00:09:32,120 --> 00:09:40,880
machine learning unsupervised means like

00:09:36,250 --> 00:09:43,480
it's unlabeled data and supervised means

00:09:40,880 --> 00:09:47,329
that you have some labels to your data

00:09:43,480 --> 00:09:53,350
we'll talk about some examples of an

00:09:47,329 --> 00:09:53,350
example of each of these in a next slide

00:09:53,740 --> 00:10:00,649
so for example there is k-means k-means

00:09:58,490 --> 00:10:04,870
is unsupervised machine learning

00:10:00,649 --> 00:10:08,390
technique the way k-means works is

00:10:04,870 --> 00:10:11,920
basically you partition your you

00:10:08,390 --> 00:10:20,019
partition n data points into K clusters

00:10:11,920 --> 00:10:23,480
here K is a numeric number suppose K

00:10:20,019 --> 00:10:27,110
basically and so similar data points are

00:10:23,480 --> 00:10:31,040
grouped into under one cluster say K is

00:10:27,110 --> 00:10:34,279
3 and you have 10 data points what

00:10:31,040 --> 00:10:36,769
k-means does is it it takes the features

00:10:34,279 --> 00:10:40,699
of each of these 10 data points and

00:10:36,769 --> 00:10:43,670
assigns each point to either cluster 1 2

00:10:40,699 --> 00:10:45,820
or 3 and the data points which are

00:10:43,670 --> 00:10:49,060
similar are grouped together

00:10:45,820 --> 00:10:53,230
in under one cluster and Hammonds gonna

00:10:49,060 --> 00:10:56,560
tell us more about that yeah so as Zack

00:10:53,230 --> 00:10:58,600
mentioned here were basically plugging

00:10:56,560 --> 00:11:00,820
in different values of K so that's an

00:10:58,600 --> 00:11:03,790
example of a hyper parameter for this

00:11:00,820 --> 00:11:05,620
k-means model because you don't have a

00:11:03,790 --> 00:11:07,690
fixed number of clusters so it really

00:11:05,620 --> 00:11:10,270
depends on how many clusters you want to

00:11:07,690 --> 00:11:13,000
sort of group your randomly distributed

00:11:10,270 --> 00:11:14,830
data points so this is where ml flow

00:11:13,000 --> 00:11:17,110
comes into the picture and we will be

00:11:14,830 --> 00:11:19,540
talking more about ml flow after this so

00:11:17,110 --> 00:11:21,490
ml slow is the tool that we sort of

00:11:19,540 --> 00:11:24,430
think is pretty useful for these hyper

00:11:21,490 --> 00:11:27,070
parameter tuning aspects because you can

00:11:24,430 --> 00:11:28,870
keep track of the different values of K

00:11:27,070 --> 00:11:31,420
that you are playing around with and how

00:11:28,870 --> 00:11:35,490
that helped improve the performance of

00:11:31,420 --> 00:11:38,740
each of your models prospectively

00:11:35,490 --> 00:11:41,200
so now that we know what exactly is

00:11:38,740 --> 00:11:43,860
k-means and a little bit about the

00:11:41,200 --> 00:11:46,840
differences between supervised and

00:11:43,860 --> 00:11:51,300
unsupervised machine learning where

00:11:46,840 --> 00:11:53,980
exactly can we apply J means right so

00:11:51,300 --> 00:11:56,770
typically k-means is useful when your

00:11:53,980 --> 00:11:58,420
data is an American nature when it's

00:11:56,770 --> 00:11:59,800
preferably that it should have small

00:11:58,420 --> 00:12:01,870
number of dimensions but of course

00:11:59,800 --> 00:12:04,240
that's not always the case and then you

00:12:01,870 --> 00:12:07,510
will have to do further dimensionality

00:12:04,240 --> 00:12:09,730
reduction and also it's it's good when

00:12:07,510 --> 00:12:13,210
the data is also sort of having a

00:12:09,730 --> 00:12:15,190
continuous nature to it so that's where

00:12:13,210 --> 00:12:17,200
we would find k-means to be a suitable

00:12:15,190 --> 00:12:19,900
use case when you are trying to group

00:12:17,200 --> 00:12:23,320
things which are similar together so now

00:12:19,900 --> 00:12:26,080
that we know what exactly is k-means and

00:12:23,320 --> 00:12:28,780
sort of where it can be applied how many

00:12:26,080 --> 00:12:30,280
of you think you would have used k mean

00:12:28,780 --> 00:12:33,130
somewhere or have an idea of where

00:12:30,280 --> 00:12:34,810
k-means might be yes or they you've used

00:12:33,130 --> 00:12:40,330
something that uses the k-means

00:12:34,810 --> 00:12:46,540
algorithm don't always your hash it on

00:12:40,330 --> 00:12:47,760
time okay what what did you is that is

00:12:46,540 --> 00:13:00,930
k-means

00:12:47,760 --> 00:13:04,080
internally so liquor yeah Helen's gonna

00:13:00,930 --> 00:13:06,690
give us more details on examples of

00:13:04,080 --> 00:13:12,300
systems or apps or something that you've

00:13:06,690 --> 00:13:15,330
used that rely on keys so I'm sure all

00:13:12,300 --> 00:13:17,430
of us would have used uber or any of the

00:13:15,330 --> 00:13:19,680
other ride-sharing applications right so

00:13:17,430 --> 00:13:21,210
you have over your left arm so all of

00:13:19,680 --> 00:13:23,970
these do have some kind of k-means

00:13:21,210 --> 00:13:27,240
algorithm behind it because when you're

00:13:23,970 --> 00:13:29,460
booking for an uber right you're usually

00:13:27,240 --> 00:13:31,590
paired up with some driver and if you're

00:13:29,460 --> 00:13:33,540
booking like an uber pool you have other

00:13:31,590 --> 00:13:35,790
passengers in the car as well so that's

00:13:33,540 --> 00:13:38,220
sort of how the use key means to

00:13:35,790 --> 00:13:40,620
identify based on your location where

00:13:38,220 --> 00:13:43,800
the nearest possible drivers that we can

00:13:40,620 --> 00:13:45,480
allocate to you to pick up and also they

00:13:43,800 --> 00:13:49,050
use it in their back-end when they do a

00:13:45,480 --> 00:13:50,610
more statistical analysis of rather than

00:13:49,050 --> 00:13:52,830
just using it in real time they actually

00:13:50,610 --> 00:13:56,490
use it for their purpose of identifying

00:13:52,830 --> 00:13:59,460
which areas or which locations had the

00:13:56,490 --> 00:14:01,320
most demand for ubers at this particular

00:13:59,460 --> 00:14:03,210
time what were the number of customers

00:14:01,320 --> 00:14:07,260
that got in and things like that where

00:14:03,210 --> 00:14:09,680
they have used key means another use

00:14:07,260 --> 00:14:13,140
case is like an e-commerce or online

00:14:09,680 --> 00:14:15,360
shopping experience again the D do have

00:14:13,140 --> 00:14:16,560
a lot of integration of some kind of key

00:14:15,360 --> 00:14:18,780
means in them where they're trying to

00:14:16,560 --> 00:14:22,410
predict how they do the delivery

00:14:18,780 --> 00:14:24,030
estimation or locating nearest truck

00:14:22,410 --> 00:14:29,130
drivers and things like that is where

00:14:24,030 --> 00:14:31,290
k-means is used another one is it's sort

00:14:29,130 --> 00:14:34,110
of more complicated use case of it but

00:14:31,290 --> 00:14:35,970
they do when you are do a lot of

00:14:34,110 --> 00:14:37,410
research about a particular location or

00:14:35,970 --> 00:14:39,380
area you're trying to figure out if it's

00:14:37,410 --> 00:14:42,030
safe or not so you have these

00:14:39,380 --> 00:14:44,190
third-party websites which kind of give

00:14:42,030 --> 00:14:46,200
your statistics about the crime rates in

00:14:44,190 --> 00:14:49,800
these localities so they do a lot of

00:14:46,200 --> 00:14:51,660
training on pre historical data over the

00:14:49,800 --> 00:14:53,850
past years and they sort of give you

00:14:51,660 --> 00:14:57,420
that insights into which areas are

00:14:53,850 --> 00:14:59,730
probably safe or not and then another

00:14:57,420 --> 00:15:00,930
use case would be Netflix right so when

00:14:59,730 --> 00:15:02,730
you are streaming on

00:15:00,930 --> 00:15:04,830
Netflix they have a bunch of servers so

00:15:02,730 --> 00:15:06,899
when were you're requesting for a TV

00:15:04,830 --> 00:15:09,570
show or a movie that you want to watch

00:15:06,899 --> 00:15:11,149
they try to see based on your geographic

00:15:09,570 --> 00:15:13,740
location what are the nearest available

00:15:11,149 --> 00:15:15,540
service that you can probably that they

00:15:13,740 --> 00:15:18,330
can assign to you so that you can easily

00:15:15,540 --> 00:15:25,410
get access to that particular video that

00:15:18,330 --> 00:15:30,029
you wanna watch another very popular

00:15:25,410 --> 00:15:32,899
algorithm is K and K nearest neighbors a

00:15:30,029 --> 00:15:37,290
supervised machine learning technique

00:15:32,899 --> 00:15:41,390
with CNN you have some data points that

00:15:37,290 --> 00:15:45,420
you already know what class it is and

00:15:41,390 --> 00:15:48,390
you we use these to figure out the data

00:15:45,420 --> 00:15:52,920
points that that isn't associated to a

00:15:48,390 --> 00:15:57,000
particular class based on proximity and

00:15:52,920 --> 00:16:00,360
who's gonna tell us yeah so there is

00:15:57,000 --> 00:16:02,520
just a simple animation for this to sort

00:16:00,360 --> 00:16:07,140
of understand what key KN is trying to

00:16:02,520 --> 00:16:08,940
do so you already have classified labels

00:16:07,140 --> 00:16:11,550
assigned for some of the data points so

00:16:08,940 --> 00:16:15,270
you have like a Class A and a Class B

00:16:11,550 --> 00:16:17,430
which is the blue and the orange colored

00:16:15,270 --> 00:16:18,839
dots respectively so when you have a new

00:16:17,430 --> 00:16:21,120
data point which is sort of tryna

00:16:18,839 --> 00:16:24,300
identify itself to be classified in

00:16:21,120 --> 00:16:26,700
either Class A or Class B it already has

00:16:24,300 --> 00:16:28,500
a set of labels assigned so that's how

00:16:26,700 --> 00:16:30,660
it's different from an unsupervised

00:16:28,500 --> 00:16:32,550
where there were no labels at all so

00:16:30,660 --> 00:16:34,589
here trying to figure out who do I

00:16:32,550 --> 00:16:37,620
belong to Class A or do I belong to

00:16:34,589 --> 00:16:40,140
class me and here again the key is

00:16:37,620 --> 00:16:43,709
another hyper parameter that you sort of

00:16:40,140 --> 00:16:45,480
tune and you're trying to identify let's

00:16:43,709 --> 00:16:48,270
say in this example it's trying to find

00:16:45,480 --> 00:16:50,430
the nearest three neighbors so the black

00:16:48,270 --> 00:16:52,650
dot is basically saying okay these are

00:16:50,430 --> 00:16:55,079
my three nearest neighbors of which I

00:16:52,650 --> 00:16:56,970
have one which belongs to class a I have

00:16:55,079 --> 00:16:59,070
two neighbors who belong to Class B and

00:16:56,970 --> 00:17:01,260
it also does further distance

00:16:59,070 --> 00:17:03,390
calculations to sort of figure out who

00:17:01,260 --> 00:17:05,640
am I more closer to and then it

00:17:03,390 --> 00:17:08,130
associate self as belonging to I belong

00:17:05,640 --> 00:17:11,720
to probably Class B so that's an example

00:17:08,130 --> 00:17:14,269
of KNN and a useful analogy to remember

00:17:11,720 --> 00:17:21,559
this is birds of a feather flock

00:17:14,269 --> 00:17:23,259
together so again some applications

00:17:21,559 --> 00:17:25,970
where canon might have been used

00:17:23,259 --> 00:17:28,250
typically it's usually in like object

00:17:25,970 --> 00:17:30,889
detection or pattern recognition kind of

00:17:28,250 --> 00:17:32,360
systems so in all of these image

00:17:30,889 --> 00:17:35,450
processing techniques where you're

00:17:32,360 --> 00:17:38,000
trying to associate like pixel belonging

00:17:35,450 --> 00:17:40,639
to which kind of nearest pixels that it

00:17:38,000 --> 00:17:44,179
should be grouped under is where like

00:17:40,639 --> 00:17:46,250
you might have K and then you have like

00:17:44,179 --> 00:17:48,799
video streaming services like YouTube so

00:17:46,250 --> 00:17:51,110
these are all statistically based where

00:17:48,799 --> 00:17:53,929
they use some kind of cannon to identify

00:17:51,110 --> 00:17:56,450
where are my nearest customers and what

00:17:53,929 --> 00:17:58,129
kind of playlists or videos do they have

00:17:56,450 --> 00:18:02,509
and what can we recommend based off of

00:17:58,129 --> 00:18:04,850
that we also have like a popular one is

00:18:02,509 --> 00:18:06,320
like gene sequence matching so when you

00:18:04,850 --> 00:18:09,169
have a lot of pharmaceutical companies

00:18:06,320 --> 00:18:11,809
who do a lot of research on this they're

00:18:09,169 --> 00:18:14,480
trying to identify the gene composition

00:18:11,809 --> 00:18:16,669
and see if they can classify them into

00:18:14,480 --> 00:18:18,110
belonging to a DA a given DNA sequence

00:18:16,669 --> 00:18:21,889
so that they can do it so further

00:18:18,110 --> 00:18:23,799
research analysis aspects of it and then

00:18:21,889 --> 00:18:27,230
you also have the credit card

00:18:23,799 --> 00:18:29,809
application so these are which credit

00:18:27,230 --> 00:18:33,169
card or bank financial sectors sort of

00:18:29,809 --> 00:18:35,509
have a KNN to sort out their customers

00:18:33,169 --> 00:18:38,750
and potentially reach out to like future

00:18:35,509 --> 00:18:41,210
customers what kind of users do

00:18:38,750 --> 00:18:43,490
belonging to which category of customers

00:18:41,210 --> 00:18:46,129
that I can further reach out to for

00:18:43,490 --> 00:18:49,490
further analysis so in all of these

00:18:46,129 --> 00:18:51,440
examples you are doing a lot of trial

00:18:49,490 --> 00:18:53,570
and error methods where you're changing

00:18:51,440 --> 00:18:56,840
different parameters to identify the

00:18:53,570 --> 00:19:01,070
best performance of these models and

00:18:56,840 --> 00:19:02,990
that's exactly where ml flow is sort of

00:19:01,070 --> 00:19:04,220
coming into the picture and Zack is

00:19:02,990 --> 00:19:11,870
going to talk a little bit more about

00:19:04,220 --> 00:19:15,559
that thank you so as you saw you know ml

00:19:11,870 --> 00:19:17,389
flow is just is also another project

00:19:15,559 --> 00:19:19,230
that does high performer tuning there's

00:19:17,389 --> 00:19:21,269
many projects that do I

00:19:19,230 --> 00:19:24,809
I believe ketchup is another project

00:19:21,269 --> 00:19:26,190
that does have a parameter tuning and

00:19:24,809 --> 00:19:29,880
mole flow is great it's an open-source

00:19:26,190 --> 00:19:33,529
project it you can deploy your models

00:19:29,880 --> 00:19:38,700
save your models in different clouds and

00:19:33,529 --> 00:19:40,590
also it's it has a Python library that

00:19:38,700 --> 00:19:46,559
you can embed into your Jupiter notebook

00:19:40,590 --> 00:19:48,590
and and has many great features but just

00:19:46,559 --> 00:19:51,899
to kind of give you a bird's eye view

00:19:48,590 --> 00:19:53,940
there are three sub projects the sub

00:19:51,899 --> 00:19:57,320
project that is of interest to us for

00:19:53,940 --> 00:20:01,409
experiment tracking is just a tracking

00:19:57,320 --> 00:20:03,570
portion the tracking portion we use that

00:20:01,409 --> 00:20:07,830
it's basically a server that runs in a

00:20:03,570 --> 00:20:10,019
container and that server can basically

00:20:07,830 --> 00:20:12,929
you you'd use the Python library to

00:20:10,019 --> 00:20:16,230
connect to that server and store

00:20:12,929 --> 00:20:20,130
metadata and you can also hook up that

00:20:16,230 --> 00:20:24,649
server to maybe deploy these models into

00:20:20,130 --> 00:20:29,639
s3 or other cloud storage systems to

00:20:24,649 --> 00:20:31,289
save your models and also attract the

00:20:29,639 --> 00:20:34,519
give the get hash and all these other

00:20:31,289 --> 00:20:37,710
metadata and you can also store your

00:20:34,519 --> 00:20:40,039
model along with the visualizations that

00:20:37,710 --> 00:20:43,769
you generated through maybe matplotlib

00:20:40,039 --> 00:20:47,460
along with it as part of your experiment

00:20:43,769 --> 00:20:50,700
and and it has some great features like

00:20:47,460 --> 00:20:53,399
the ability to search for the best type

00:20:50,700 --> 00:20:57,110
of parameters it has a Python API I'll

00:20:53,399 --> 00:21:00,539
show an example of it in a later slide

00:20:57,110 --> 00:21:02,700
let's kind of look at just kind of a

00:21:00,539 --> 00:21:04,919
bird's eye view of like let's say I want

00:21:02,700 --> 00:21:07,139
to do hyper parameter tuning I want to

00:21:04,919 --> 00:21:09,679
do k-means and I want to do three

00:21:07,139 --> 00:21:15,570
different I want to try three different

00:21:09,679 --> 00:21:18,000
type of parameters for K I'll try k with

00:21:15,570 --> 00:21:20,549
hyper parameter four five and six and

00:21:18,000 --> 00:21:25,590
let's see which one returns the best

00:21:20,549 --> 00:21:29,610
results right I like to kind of look at

00:21:25,590 --> 00:21:31,890
it in a kind of like a high level each

00:21:29,610 --> 00:21:34,870
blue box is a container

00:21:31,890 --> 00:21:38,830
and they're all connected to the same

00:21:34,870 --> 00:21:41,770
data set and then you have the ml flow

00:21:38,830 --> 00:21:43,240
tracking server which I mentioned this

00:21:41,770 --> 00:21:46,600
was one of the sub projects that we're

00:21:43,240 --> 00:21:50,440
running in the container and we're gonna

00:21:46,600 --> 00:21:52,120
be storing all these metadata and and we

00:21:50,440 --> 00:22:00,460
can also store the model if you want as

00:21:52,120 --> 00:22:02,290
well this is just the code snippet of

00:22:00,460 --> 00:22:04,630
what you would need to add into your

00:22:02,290 --> 00:22:07,660
Jupiter notebook in order to take

00:22:04,630 --> 00:22:10,090
advantage of ml flow you have to import

00:22:07,660 --> 00:22:12,900
the Python library it's just a simple

00:22:10,090 --> 00:22:16,930
pip install and then you start your run

00:22:12,900 --> 00:22:19,390
you log your parameter and then you log

00:22:16,930 --> 00:22:20,860
your your metrics you can have more than

00:22:19,390 --> 00:22:22,930
one metric you can have more than one

00:22:20,860 --> 00:22:26,730
parameter in our case we're just one

00:22:22,930 --> 00:22:26,730
parameter or one metric

00:22:32,110 --> 00:22:39,370
for example if you were to run this on

00:22:35,490 --> 00:22:41,080
many many experiments and say you have

00:22:39,370 --> 00:22:43,059
like hundreds of experiments it's like

00:22:41,080 --> 00:22:45,070
kind of cumbersome to like look through

00:22:43,059 --> 00:22:47,470
click click click click click and search

00:22:45,070 --> 00:22:50,380
through stuff it has a really nice

00:22:47,470 --> 00:22:53,049
Python API what this statement over here

00:22:50,380 --> 00:22:56,320
it does is out of all the runs that I've

00:22:53,049 --> 00:22:59,320
done I want you to search the experiment

00:22:56,320 --> 00:23:00,880
ID zero and use this filter string the

00:22:59,320 --> 00:23:05,710
filter string is right over here

00:23:00,880 --> 00:23:10,809
metric dot R 2 is less than zero point

00:23:05,710 --> 00:23:12,130
zero four six right and only returned to

00:23:10,809 --> 00:23:14,799
me one result so if there's multiple

00:23:12,130 --> 00:23:22,600
with the same criteria just return to me

00:23:14,799 --> 00:23:25,659
one slide this is the mo flow UI I

00:23:22,600 --> 00:23:28,269
showed you in a previous slide you're

00:23:25,659 --> 00:23:31,000
doing everything in Python you could do

00:23:28,269 --> 00:23:32,980
a lot of stuff with the UI as well the

00:23:31,000 --> 00:23:36,190
UI lets you click on three different

00:23:32,980 --> 00:23:38,799
experiments and compare them and in here

00:23:36,190 --> 00:23:41,529
the font is not very big but I'm gonna

00:23:38,799 --> 00:23:44,289
tell you the cake cluster is for here

00:23:41,529 --> 00:23:47,139
it's seven here and it's five here and

00:23:44,289 --> 00:23:52,659
the metric that we we've said we're

00:23:47,139 --> 00:23:55,750
getting back a 96% score on here we're

00:23:52,659 --> 00:23:59,019
getting a 90 we're getting a 94 so

00:23:55,750 --> 00:24:02,380
technically four seems to be the sweet

00:23:59,019 --> 00:24:04,450
spot for our hyper parameter here so

00:24:02,380 --> 00:24:09,669
when we're deploying this will will make

00:24:04,450 --> 00:24:11,260
sure our K is 4 right next slide let's

00:24:09,669 --> 00:24:14,500
say I want to do this at all on Jupiter

00:24:11,260 --> 00:24:17,830
if I like Jupiter a lot I'll just import

00:24:14,500 --> 00:24:19,570
ml flow I'll add ml flow dot and

00:24:17,830 --> 00:24:22,210
whatever library using it so if you use

00:24:19,570 --> 00:24:26,139
in tensor flow you'd use the tensor flow

00:24:22,210 --> 00:24:27,940
package under the Emma flow dot or fuse

00:24:26,139 --> 00:24:30,340
and other libraries they have multiple

00:24:27,940 --> 00:24:33,940
different libraries that you can use and

00:24:30,340 --> 00:24:37,889
then you would do the just track it with

00:24:33,940 --> 00:24:37,889
the log prime log metric

00:24:39,029 --> 00:24:45,999
and when you're doing things like this

00:24:43,210 --> 00:24:47,679
you want to build containers reusable

00:24:45,999 --> 00:24:51,370
containers and openshift is a great

00:24:47,679 --> 00:24:54,220
platform to be able to do built builds

00:24:51,370 --> 00:24:56,200
and have a container built for you if I

00:24:54,220 --> 00:24:59,470
don't care about writing darker files

00:24:56,200 --> 00:25:01,539
anymore I can just have open chef go and

00:24:59,470 --> 00:25:05,679
point to my source code and it'll do a

00:25:01,539 --> 00:25:07,690
source to image and here in the open

00:25:05,679 --> 00:25:10,809
ship template I can pass in which

00:25:07,690 --> 00:25:13,809
parameter the hyper parameter is and

00:25:10,809 --> 00:25:16,990
it'll go and build and run that a job

00:25:13,809 --> 00:25:20,320
with the hybrid parameter that I want to

00:25:16,990 --> 00:25:21,909
try now when I think about this and I

00:25:20,320 --> 00:25:23,919
want to say you know what maybe that's

00:25:21,909 --> 00:25:26,289
not good enough I want to do more

00:25:23,919 --> 00:25:32,190
complex stuff right I want to have a

00:25:26,289 --> 00:25:34,720
visual view of doing this so for example

00:25:32,190 --> 00:25:39,399
let's say I want to have a workflow like

00:25:34,720 --> 00:25:40,899
a pipeline or something this is like Jif

00:25:39,399 --> 00:25:42,759
it's like towards the end of the shift

00:25:40,899 --> 00:25:46,149
but here's the beginning of the jib so I

00:25:42,759 --> 00:25:49,330
have our goal here I've already built my

00:25:46,149 --> 00:25:51,490
openshift build and I have container

00:25:49,330 --> 00:25:53,159
already but I'm passing in an agro

00:25:51,490 --> 00:25:56,889
workflow I'm passing in different

00:25:53,159 --> 00:25:59,200
parameters so when I'm running my my job

00:25:56,889 --> 00:26:01,929
and it's tracking my experiments and mo

00:25:59,200 --> 00:26:07,840
flow you can even track up my models and

00:26:01,929 --> 00:26:14,679
mo flow and my models with mo flow can

00:26:07,840 --> 00:26:17,470
even store my my models in SEF s3 which

00:26:14,679 --> 00:26:22,299
is an open source alternative to the s3

00:26:17,470 --> 00:26:25,779
that's available by AWS and if you're

00:26:22,299 --> 00:26:30,399
interested in looking at this stuff

00:26:25,779 --> 00:26:33,360
I think you'd be good to see this off

00:26:30,399 --> 00:26:33,360
let me

00:26:37,670 --> 00:26:43,140
so if you wanted to try this at home

00:26:39,930 --> 00:26:45,270
later you could follow this gist but

00:26:43,140 --> 00:26:47,010
I'll just like to explain it so you

00:26:45,270 --> 00:26:51,270
understand why what we're doing here

00:26:47,010 --> 00:26:54,300
so in here and this llamo file what I'm

00:26:51,270 --> 00:26:55,560
doing is I'm saying that these are the

00:26:54,300 --> 00:26:57,060
parameters that I'm gonna be passing

00:26:55,560 --> 00:26:59,570
into this container here's the image

00:26:57,060 --> 00:27:03,300
that I'm building that I just built that

00:26:59,570 --> 00:27:04,770
takes in environment variables one

00:27:03,300 --> 00:27:07,530
environment variable is very important

00:27:04,770 --> 00:27:09,450
to know about is the mo for tracking URI

00:27:07,530 --> 00:27:12,390
you have to tell it which mo flow

00:27:09,450 --> 00:27:14,220
instance you you've got deployed that's

00:27:12,390 --> 00:27:18,180
gonna be tracking your metrics and your

00:27:14,220 --> 00:27:20,730
hyper parameters and then I pass in I

00:27:18,180 --> 00:27:22,050
make my script as like a command line

00:27:20,730 --> 00:27:25,200
thing where I can pass in different

00:27:22,050 --> 00:27:28,440
flags or in this case I'm using an

00:27:25,200 --> 00:27:31,200
example that's provided by mo flow you

00:27:28,440 --> 00:27:35,010
pass in two parameters and it'll have

00:27:31,200 --> 00:27:37,610
different results so and here these are

00:27:35,010 --> 00:27:38,730
this is the work flow so for example

00:27:37,610 --> 00:27:41,190
Joana

00:27:38,730 --> 00:27:44,550
just presented earlier on coop flow and

00:27:41,190 --> 00:27:47,730
coop flow pipelines and Argos what about

00:27:44,550 --> 00:27:50,250
pipelines is based on basically it lets

00:27:47,730 --> 00:27:53,400
you decide like I want you to try these

00:27:50,250 --> 00:27:55,230
steps and then when these steps are

00:27:53,400 --> 00:28:00,600
completed then I want you to try these

00:27:55,230 --> 00:28:04,470
particular steps so just to tie this

00:28:00,600 --> 00:28:12,180
back and once and then you can have like

00:28:04,470 --> 00:28:20,340
it do something at the end okay so that

00:28:12,180 --> 00:28:26,330
is our goal and ml flow integration okay

00:28:20,340 --> 00:28:26,330
oh yes

00:28:27,070 --> 00:28:32,680
so we're gonna show you a demo because

00:28:30,180 --> 00:28:35,590
slides are nice but what's better than a

00:28:32,680 --> 00:28:41,740
demo right let's see the real stuff no

00:28:35,590 --> 00:28:45,310
recordings let's let's show them and

00:28:41,740 --> 00:28:49,300
we're gonna be demoing Quran who sits

00:28:45,310 --> 00:28:53,490
over here he works on the the Ceph

00:28:49,300 --> 00:28:57,640
storage predictions for failure tribes

00:28:53,490 --> 00:28:59,860
and he's using hyper parameter tuning to

00:28:57,640 --> 00:29:03,910
do that with mo flow and we're gonna be

00:28:59,860 --> 00:29:09,570
demonstrating his his work all the tough

00:29:03,910 --> 00:29:09,570
questions are gonna go to you now okay

00:29:10,230 --> 00:29:19,470
so maybe we'll get the font a little bit

00:29:12,550 --> 00:29:19,470
bigger can everybody see that No

00:29:33,930 --> 00:29:36,890
whoa-oh

00:29:56,889 --> 00:29:59,889
yes

00:30:05,270 --> 00:30:18,710
more MORE

00:30:11,919 --> 00:30:38,600
stop it all right I'm pretty sure you

00:30:18,710 --> 00:30:42,010
can see this oh yeah

00:30:38,600 --> 00:30:42,010
on a mission

00:30:48,620 --> 00:30:53,770
no more all right

00:30:56,010 --> 00:31:03,130
okay yeah so this was what Karen was

00:31:01,000 --> 00:31:06,670
working on as part of this interim

00:31:03,130 --> 00:31:09,490
project so he was basically trying to do

00:31:06,670 --> 00:31:12,340
like a self Hardware drive failure

00:31:09,490 --> 00:31:15,370
prediction so that was the algorithm

00:31:12,340 --> 00:31:17,850
that he was training on and he had a few

00:31:15,370 --> 00:31:20,290
hyper parameters that he needed to

00:31:17,850 --> 00:31:23,620
fine-tune before feeding it into his

00:31:20,290 --> 00:31:25,600
model so that's the generate hyper

00:31:23,620 --> 00:31:27,540
parameter Python script that he had

00:31:25,600 --> 00:31:34,420
where you just specify a bunch of your

00:31:27,540 --> 00:31:36,370
hyper parameters and once we execute

00:31:34,420 --> 00:31:38,260
that it just tells you what was the

00:31:36,370 --> 00:31:41,950
total number of hyper parameters set and

00:31:38,260 --> 00:31:44,770
it also generates a Yama file which

00:31:41,950 --> 00:31:46,300
basically helps you to create spawn the

00:31:44,770 --> 00:31:50,020
job based on the number of hyper

00:31:46,300 --> 00:31:55,800
parameters that you have set in the code

00:31:50,020 --> 00:31:58,150
and there is an experiment tracking

00:31:55,800 --> 00:32:00,850
repository also which we have where you

00:31:58,150 --> 00:32:04,810
can deploy the ml flow tracking server

00:32:00,850 --> 00:32:07,050
as well so here what the let me just

00:32:04,810 --> 00:32:07,050
quickly

00:32:11,520 --> 00:32:16,450
so there's a small script here which

00:32:14,200 --> 00:32:19,390
basically you're trying to specify where

00:32:16,450 --> 00:32:22,059
exactly is my hyper parameter Yama file

00:32:19,390 --> 00:32:25,390
that I've just generated and then you're

00:32:22,059 --> 00:32:27,190
feeding into where your training data

00:32:25,390 --> 00:32:30,400
set resides currently and where the

00:32:27,190 --> 00:32:34,179
training Python file also exists so

00:32:30,400 --> 00:32:36,100
these are the parameters that you're

00:32:34,179 --> 00:32:38,289
basically passing through and what it

00:32:36,100 --> 00:32:40,990
does is it basically spawns a new jobs

00:32:38,289 --> 00:32:46,840
for every hyper parameter that you have

00:32:40,990 --> 00:32:54,400
set so when we run this particular

00:32:46,840 --> 00:32:56,770
script please don't fail so it basically

00:32:54,400 --> 00:32:58,840
starts spawning those jobs and as you

00:32:56,770 --> 00:33:00,880
can see earlier we saw that it said the

00:32:58,840 --> 00:33:04,179
number of hyper parameters set were 12

00:33:00,880 --> 00:33:07,150
so it should spawn twelve different jobs

00:33:04,179 --> 00:33:09,750
for you and then we can quickly open

00:33:07,150 --> 00:33:09,750
though

00:33:12,929 --> 00:33:25,860
so this is where we have our ml flow

00:33:16,419 --> 00:33:25,860
server set so these are all running

00:33:32,650 --> 00:33:42,220
the

00:33:35,200 --> 00:33:47,530
slide sir yeah so this is the ml slow

00:33:42,220 --> 00:33:49,240
server so this is the tracking UI that

00:33:47,530 --> 00:33:51,310
we were mentioning to you earlier in our

00:33:49,240 --> 00:33:55,180
slides it this is where you basically

00:33:51,310 --> 00:33:58,330
log your parameter so we just ran this

00:33:55,180 --> 00:34:01,000
and it tells you the timestamp that

00:33:58,330 --> 00:34:04,330
gives you the username it gives you what

00:34:01,000 --> 00:34:06,520
was the source code ran for this

00:34:04,330 --> 00:34:08,919
particular model and then you have your

00:34:06,520 --> 00:34:13,090
parameters being passed so you have some

00:34:08,919 --> 00:34:15,909
loss functions l2 values these are just

00:34:13,090 --> 00:34:18,159
parameters tuned for his specific model

00:34:15,909 --> 00:34:23,310
that he was looking into and then you

00:34:18,159 --> 00:34:23,310
also log different metric values as well

00:34:29,109 --> 00:34:37,250
so as I mentioned earlier you know um

00:34:33,769 --> 00:34:40,369
you could technically run this and then

00:34:37,250 --> 00:34:43,220
have your pipe have a Python script that

00:34:40,369 --> 00:34:45,679
will go and query and based on your

00:34:43,220 --> 00:34:48,710
criteria find you the best parameters

00:34:45,679 --> 00:34:53,779
and then deploy up the model that has

00:34:48,710 --> 00:34:56,299
those particular parameters and yeah you

00:34:53,779 --> 00:34:58,670
can pretty much just just search over

00:34:56,299 --> 00:35:13,599
here you type in a particular metric

00:34:58,670 --> 00:35:22,180
let's just say metric metric f2f1

00:35:13,599 --> 00:35:26,559
greater than what this value here yeah

00:35:22,180 --> 00:35:26,559
actually to do it here

00:35:30,760 --> 00:35:34,380
yes right here

00:35:37,349 --> 00:35:40,349
0.98

00:35:51,190 --> 00:35:56,060
[Music]

00:35:53,060 --> 00:35:56,060
0.93

00:35:57,839 --> 00:36:13,769
searching found 53 results they found 18

00:36:05,160 --> 00:36:15,869
results 12 results and so on so forth

00:36:13,769 --> 00:36:18,029
now you you have like two results

00:36:15,869 --> 00:36:21,029
they're fit this criteria and then you

00:36:18,029 --> 00:36:22,949
know you can dig into it more but that's

00:36:21,029 --> 00:36:25,469
just essentially the idea but like I

00:36:22,949 --> 00:36:27,869
said whatever you do with the UI you can

00:36:25,469 --> 00:36:34,199
have it in a Python script and automate

00:36:27,869 --> 00:36:38,249
that and yeah and maybe if you I want to

00:36:34,199 --> 00:36:40,949
maybe further give you a little bit more

00:36:38,249 --> 00:36:43,140
of a tour here is I can just say hey you

00:36:40,949 --> 00:36:44,910
know these two look like the similar

00:36:43,140 --> 00:36:47,849
stuff but I want to kind of compare it

00:36:44,910 --> 00:36:53,880
side-by-side and take a look at it like

00:36:47,849 --> 00:37:02,160
that I can do that yeah yeah

00:36:53,880 --> 00:37:07,170
pretty much that's our presentation we

00:37:02,160 --> 00:37:08,999
did show you the demo and yeah we're

00:37:07,170 --> 00:37:11,939
just gonna give you a little brief

00:37:08,999 --> 00:37:14,369
overview of what you learnt so you've

00:37:11,939 --> 00:37:16,829
learned the difference between regular

00:37:14,369 --> 00:37:18,479
software engineering and machine

00:37:16,829 --> 00:37:23,009
learning engineering you've learned

00:37:18,479 --> 00:37:24,630
about some hyper parameters you've

00:37:23,009 --> 00:37:26,400
learned about unsupervised machine

00:37:24,630 --> 00:37:29,880
learning supervised machine learning and

00:37:26,400 --> 00:37:33,119
some examples of them you learnt about

00:37:29,880 --> 00:37:37,289
how to use mo flow to do experiment

00:37:33,119 --> 00:37:39,269
tracking you've learned how to do put it

00:37:37,289 --> 00:37:42,589
put it tie it all together and run it in

00:37:39,269 --> 00:37:45,739
kubernetes on openshift

00:37:42,589 --> 00:37:45,739
thank you

00:37:48,830 --> 00:37:55,120
and if you're interested in contributing

00:37:52,730 --> 00:37:58,490
this is all this stuff is open-source

00:37:55,120 --> 00:38:00,280
one of the best ways to learn is getting

00:37:58,490 --> 00:38:04,670
involved in open source communities

00:38:00,280 --> 00:38:07,640
contributing the first link that you see

00:38:04,670 --> 00:38:10,280
here is the experiment tracking files

00:38:07,640 --> 00:38:13,730
for deploying the stuff with templates

00:38:10,280 --> 00:38:17,300
on OpenShift that the second link is the

00:38:13,730 --> 00:38:19,870
goal-line operator for mo flow if you're

00:38:17,300 --> 00:38:22,370
interested in contributing in either one

00:38:19,870 --> 00:38:31,760
we're glad to have more contributions

00:38:22,370 --> 00:38:35,440
from the community and I don't know what

00:38:31,760 --> 00:38:35,440
else to say thank you thank you

00:38:38,680 --> 00:38:44,360
also there's just a quick announcement

00:38:41,200 --> 00:38:48,440
so there's going to be a party Def Con

00:38:44,360 --> 00:38:51,080
party tomorrow at 7 p.m. and you can

00:38:48,440 --> 00:38:54,200
collect your tickets for the party today

00:38:51,080 --> 00:38:56,810
at 4:30 at the registration in case you

00:38:54,200 --> 00:38:58,520
can't get it today you can also pick

00:38:56,810 --> 00:39:01,090
them up tomorrow morning at the

00:38:58,520 --> 00:39:01,090
registration desk

00:39:05,949 --> 00:39:09,430
we didn't ask questions

00:39:10,150 --> 00:39:17,539
Oh

00:39:12,319 --> 00:39:19,099
does anybody have any questions if

00:39:17,539 --> 00:39:20,630
anybody has any questions just come on

00:39:19,099 --> 00:39:22,989
up front then we'll will be happy to

00:39:20,630 --> 00:39:22,989

YouTube URL: https://www.youtube.com/watch?v=WgEKfAj7PLc


