Title: How to Build Apache Spark Cloud Services in Python
Publication date: 2019-02-21
Playlist: DevConfUS 2018
Description: 
	In this presentation Michael will demonstrate how to create and deploy Python based Apache Spark applications to cloud native environments. We will explore design patterns to help you integrate your analytics and machine learning algorithms into applications which can take full advantage of cloud native platforms like OpenShift Origin. You will see code samples and live demonstrations of techniques for building and deploying Apache Spark applications written in Python. These samples and techniques will provide a solid basis that you can use to create your own intelligent applications for the cloud.
Captions: 
	00:00:05,860 --> 00:00:10,890
this can pick you up okay so just talk

00:00:08,410 --> 00:00:10,890
real loud

00:00:49,200 --> 00:00:57,020
here we started okay

00:00:53,810 --> 00:00:59,780
so thanks for joining me today my name

00:00:57,020 --> 00:01:01,070
is Michael McEwan and I work for Red Hat

00:00:59,780 --> 00:01:03,710
and I'm going to be talking about

00:01:01,070 --> 00:01:09,050
building apache spark cloud services and

00:01:03,710 --> 00:01:11,180
pipeline so to start with what let's

00:01:09,050 --> 00:01:13,340
talk about what is a cloud service or at

00:01:11,180 --> 00:01:15,020
the very least what I mean by a cloud

00:01:13,340 --> 00:01:18,800
service and what I want to talk about

00:01:15,020 --> 00:01:20,600
today and so I'm kind of relying a lot

00:01:18,800 --> 00:01:22,460
on definitions that have been put out by

00:01:20,600 --> 00:01:24,440
the cloud native computing foundation

00:01:22,460 --> 00:01:27,200
and this is an organization that's like

00:01:24,440 --> 00:01:28,520
a governance body that is kind of

00:01:27,200 --> 00:01:32,090
helping to govern projects like

00:01:28,520 --> 00:01:35,030
kubernetes and Prometheus and what they

00:01:32,090 --> 00:01:36,770
say is that a cloud native application

00:01:35,030 --> 00:01:38,510
should be something that's containerize

00:01:36,770 --> 00:01:39,740
so you know we're talking about building

00:01:38,510 --> 00:01:41,720
for containers we're talking about

00:01:39,740 --> 00:01:44,110
deploying to kubernetes so this is

00:01:41,720 --> 00:01:47,500
obviously a strong part of the story

00:01:44,110 --> 00:01:49,760
they also say that a cloud native

00:01:47,500 --> 00:01:51,710
application or service should be

00:01:49,760 --> 00:01:53,240
dynamically orchestrated and so what

00:01:51,710 --> 00:01:55,940
this means is that you can use the

00:01:53,240 --> 00:01:57,770
container with a container platform to

00:01:55,940 --> 00:01:59,390
you know migrate between instances it

00:01:57,770 --> 00:02:02,000
really should be generically useful in

00:01:59,390 --> 00:02:03,200
those in those situations and then they

00:02:02,000 --> 00:02:05,060
also say it should be micro

00:02:03,200 --> 00:02:07,360
service-oriented and this is a really

00:02:05,060 --> 00:02:10,580
nebulous topic it's a little more

00:02:07,360 --> 00:02:12,740
philosophical or ideological but the way

00:02:10,580 --> 00:02:15,080
I take it is I kind of look back to the

00:02:12,740 --> 00:02:16,820
UNIX philosophy of all that a micro

00:02:15,080 --> 00:02:19,370
service is an application that kind of

00:02:16,820 --> 00:02:20,450
does one thing and does it well and so

00:02:19,370 --> 00:02:23,150
really what you're talking about is

00:02:20,450 --> 00:02:24,380
building purpose-built applications that

00:02:23,150 --> 00:02:27,230
you're going to put into a container and

00:02:24,380 --> 00:02:29,000
deploy to the cloud and here's a link to

00:02:27,230 --> 00:02:30,530
their FAQ they've got some great

00:02:29,000 --> 00:02:31,880
language that kind of talks about what

00:02:30,530 --> 00:02:33,500
these platforms mean what these

00:02:31,880 --> 00:02:37,010
applications are and so I highly

00:02:33,500 --> 00:02:38,180
recommend checking that out now since

00:02:37,010 --> 00:02:40,700
we're talking about the cloud or talk

00:02:38,180 --> 00:02:43,700
about cloud native the platform that I'm

00:02:40,700 --> 00:02:44,840
using is openshift kubernetes and how

00:02:43,700 --> 00:02:48,950
many people here are familiar with

00:02:44,840 --> 00:02:51,290
kubernetes are openshift okay pretty a

00:02:48,950 --> 00:02:54,260
pretty good audience so you know this is

00:02:51,290 --> 00:02:55,800
this is like the generic diagram I'm

00:02:54,260 --> 00:02:57,390
sure many people have seen this

00:02:55,800 --> 00:03:00,090
you've got kind of your container

00:02:57,390 --> 00:03:04,160
Network and everything but what I'm most

00:03:00,090 --> 00:03:06,660
interested in is this part over here

00:03:04,160 --> 00:03:08,700
because I want to use this as a

00:03:06,660 --> 00:03:11,040
developer and I'm kind of interested in

00:03:08,700 --> 00:03:12,720
how does it connect to my source code

00:03:11,040 --> 00:03:14,520
repositories and how can I get kind of

00:03:12,720 --> 00:03:15,960
automated build features in there so

00:03:14,520 --> 00:03:17,820
although all this part is really

00:03:15,960 --> 00:03:19,710
interesting from an infrastructure point

00:03:17,820 --> 00:03:21,210
of view what I'm kind of curious about

00:03:19,710 --> 00:03:25,980
is this part and this is what I want to

00:03:21,210 --> 00:03:29,820
talk about today so no looks like we've

00:03:25,980 --> 00:03:31,910
got that images out here I'm gonna

00:03:29,820 --> 00:03:36,050
switch browsers quickly because this is

00:03:31,910 --> 00:03:36,050
probably a handbook I have an issue here

00:03:36,320 --> 00:03:39,740
sorry about this

00:03:54,830 --> 00:03:59,760
okay so what I want to talk about is

00:03:58,170 --> 00:04:02,640
Apache spark and how many people here

00:03:59,760 --> 00:04:04,770
are familiar with Apache spark okay so

00:04:02,640 --> 00:04:06,630
again a good number of people so this

00:04:04,770 --> 00:04:08,370
diagram might look familiar to you and

00:04:06,630 --> 00:04:11,700
this is basically a general outline of

00:04:08,370 --> 00:04:13,200
what of what an Apache spark application

00:04:11,700 --> 00:04:14,850
might look like you know we have what we

00:04:13,200 --> 00:04:16,799
call the driver process which is where

00:04:14,850 --> 00:04:18,239
all your user code lives and then there

00:04:16,799 --> 00:04:19,860
are a number of executors that help

00:04:18,239 --> 00:04:22,170
perform the distributed work and

00:04:19,860 --> 00:04:23,790
underneath all this is a cluster manager

00:04:22,170 --> 00:04:27,060
that controls how these pieces interact

00:04:23,790 --> 00:04:28,290
with each other and so I look through

00:04:27,060 --> 00:04:29,730
some of this a little quickly because it

00:04:28,290 --> 00:04:31,620
seems like a lot of you guys already

00:04:29,730 --> 00:04:33,960
know this but kind of the fundamental

00:04:31,620 --> 00:04:35,100
abstraction at the base of spark is

00:04:33,960 --> 00:04:36,840
something called the resilient

00:04:35,100 --> 00:04:38,880
distributed data set well you know we

00:04:36,840 --> 00:04:41,820
refer to as the RDD and this is really

00:04:38,880 --> 00:04:43,950
the primary or base level abstraction

00:04:41,820 --> 00:04:48,110
that happens for all the data and what

00:04:43,950 --> 00:04:50,790
it is is a partition lazy and immutable

00:04:48,110 --> 00:04:52,920
homogeneous collection and so what does

00:04:50,790 --> 00:04:54,390
that kind of mean for the resiliency and

00:04:52,920 --> 00:04:56,640
the distributed nature of these things

00:04:54,390 --> 00:04:58,980
well they're partitioned meaning that

00:04:56,640 --> 00:05:01,320
when the data comes in SPARC makes

00:04:58,980 --> 00:05:04,050
partitions in your data set this makes

00:05:01,320 --> 00:05:05,700
it easy to distribute that data second

00:05:04,050 --> 00:05:07,560
they're lazy meaning that any

00:05:05,700 --> 00:05:09,570
calculations that need to be performed

00:05:07,560 --> 00:05:11,550
on those data sets they don't happen

00:05:09,570 --> 00:05:14,070
until they actually absolutely need to

00:05:11,550 --> 00:05:15,930
return a result and finally they're

00:05:14,070 --> 00:05:18,180
immutable meaning that you can't change

00:05:15,930 --> 00:05:20,280
the the data set you can make a new one

00:05:18,180 --> 00:05:22,470
but you can't modify the one that's

00:05:20,280 --> 00:05:24,870
being distributed and these pieces

00:05:22,470 --> 00:05:26,940
together helped to build this resiliency

00:05:24,870 --> 00:05:29,400
into the system so if one piece falls

00:05:26,940 --> 00:05:31,560
out it's very easy to recalculate that

00:05:29,400 --> 00:05:33,990
partition and send it back to do work

00:05:31,560 --> 00:05:36,690
and this really helps spark to become

00:05:33,990 --> 00:05:39,750
kind of a stable platform to do these

00:05:36,690 --> 00:05:41,880
type of calculations so what does this

00:05:39,750 --> 00:05:43,590
look like in action let's say you have

00:05:41,880 --> 00:05:45,210
an array of numbers and you and you want

00:05:43,590 --> 00:05:47,030
to do a calculation to say which one of

00:05:45,210 --> 00:05:49,070
these are even how many of these numbers

00:05:47,030 --> 00:05:50,810
are even so the first thing I would do

00:05:49,070 --> 00:05:53,180
is tell spark two parallel lies this

00:05:50,810 --> 00:05:54,500
this data set and that means it's going

00:05:53,180 --> 00:05:56,750
to distribute these things and it's

00:05:54,500 --> 00:05:58,400
going to create the data set point so

00:05:56,750 --> 00:06:01,250
you see this is our our DD of each

00:05:58,400 --> 00:06:03,110
number in its own partition and then I'm

00:06:01,250 --> 00:06:04,970
going to say perform this filter

00:06:03,110 --> 00:06:07,280
operation on my data set and this this

00:06:04,970 --> 00:06:09,470
will give me a new data set and in this

00:06:07,280 --> 00:06:11,480
case I'm just looking for any number

00:06:09,470 --> 00:06:14,270
that has modulo of two equal to zero

00:06:11,480 --> 00:06:17,000
meaning it's even I want those in my new

00:06:14,270 --> 00:06:20,180
data set and so I end up with two and

00:06:17,000 --> 00:06:21,920
four and I can it now I know how many

00:06:20,180 --> 00:06:24,050
even numbers are in my data set so this

00:06:21,920 --> 00:06:26,450
is a really simplified look at the

00:06:24,050 --> 00:06:28,460
operations I might do but this is how

00:06:26,450 --> 00:06:32,840
these these are DDS are built up and how

00:06:28,460 --> 00:06:35,270
they might be distributed to do work so

00:06:32,840 --> 00:06:38,030
when we take open shift and we take a

00:06:35,270 --> 00:06:40,280
model of SPARC this is what it might

00:06:38,030 --> 00:06:42,500
start to look like at the infrastructure

00:06:40,280 --> 00:06:44,600
layer so we have our nodes that are the

00:06:42,500 --> 00:06:47,120
physical nodes where the couplets live

00:06:44,600 --> 00:06:49,520
and inside those nodes we have our

00:06:47,120 --> 00:06:53,390
container pods and you can see that you

00:06:49,520 --> 00:06:55,400
know over here we have a Python

00:06:53,390 --> 00:06:57,650
application and maybe it's got some

00:06:55,400 --> 00:06:59,450
spark containers you know maybe these

00:06:57,650 --> 00:07:01,880
are the executors and this is our master

00:06:59,450 --> 00:07:04,130
there maybe we have more applications of

00:07:01,880 --> 00:07:06,170
ongo database so you can start to see

00:07:04,130 --> 00:07:08,180
how we can use the platform to

00:07:06,170 --> 00:07:13,640
distribute the different processes that

00:07:08,180 --> 00:07:16,190
are occurring for us so what is a spark

00:07:13,640 --> 00:07:18,920
application and this is kind of the way

00:07:16,190 --> 00:07:21,470
that I generally reduce these things you

00:07:18,920 --> 00:07:23,690
have source data that's going to come in

00:07:21,470 --> 00:07:27,350
you'll perform some sort of processing

00:07:23,690 --> 00:07:29,510
on it and return results now source data

00:07:27,350 --> 00:07:30,710
and results can be very nebulous in many

00:07:29,510 --> 00:07:32,450
ways you know source data might be

00:07:30,710 --> 00:07:34,190
coming from a database it might be

00:07:32,450 --> 00:07:35,930
coming from a file on a file system it

00:07:34,190 --> 00:07:37,449
might be coming from a stream it might

00:07:35,930 --> 00:07:40,360
even be coming from an API

00:07:37,449 --> 00:07:41,889
where data is pushed in likewise results

00:07:40,360 --> 00:07:43,930
could mean the same thing so some of

00:07:41,889 --> 00:07:46,990
these are abstractions that you'll have

00:07:43,930 --> 00:07:49,779
to deal with and what we're looking at

00:07:46,990 --> 00:07:53,439
here is a very simple Python spark

00:07:49,779 --> 00:07:55,930
application and this is going to do kind

00:07:53,439 --> 00:07:57,580
of what we just looked at earlier it's

00:07:55,930 --> 00:07:58,990
going to take in a data set and then

00:07:57,580 --> 00:08:01,749
it's going to parallel eyes that data

00:07:58,990 --> 00:08:03,939
it's going to run our little you know

00:08:01,749 --> 00:08:08,080
even's counting function on it and it'll

00:08:03,939 --> 00:08:10,509
count it and return that and so if I run

00:08:08,080 --> 00:08:12,580
this on my desktop this is maybe what it

00:08:10,509 --> 00:08:14,229
would look like I would have the SPARC

00:08:12,580 --> 00:08:16,270
submit command which is a tool that

00:08:14,229 --> 00:08:17,949
comes with SPARC and I tell it to take

00:08:16,270 --> 00:08:20,020
my application in this case I'm going to

00:08:17,949 --> 00:08:22,810
say for all the numbers zero to a

00:08:20,020 --> 00:08:24,580
thousand tell me how many of those are

00:08:22,810 --> 00:08:27,610
even right so again it's a very simple

00:08:24,580 --> 00:08:29,259
process and it starts to run and the JVM

00:08:27,610 --> 00:08:32,110
goes and it spits out a bunch of logs

00:08:29,259 --> 00:08:33,849
now as an application this is probably

00:08:32,110 --> 00:08:35,139
not that useful to me because what you

00:08:33,849 --> 00:08:37,300
know what just happened what do I have

00:08:35,139 --> 00:08:39,610
is output how many numbers were even

00:08:37,300 --> 00:08:41,529
well you can see you know way up here

00:08:39,610 --> 00:08:44,130
and spit out some little print and said

00:08:41,529 --> 00:08:48,160
all right 500 of those are a bit okay so

00:08:44,130 --> 00:08:49,750
how do I use this in a cloud situation

00:08:48,160 --> 00:08:54,339
how do I go from using this locally to

00:08:49,750 --> 00:08:55,779
taking it to the cloud so again I start

00:08:54,339 --> 00:08:57,730
to think about how am I going to design

00:08:55,779 --> 00:09:00,220
my application in a way word can become

00:08:57,730 --> 00:09:03,880
a microservice so I go back to this

00:09:00,220 --> 00:09:07,690
pattern of ingesting data processing

00:09:03,880 --> 00:09:10,120
data and then publishing it right ingest

00:09:07,690 --> 00:09:11,740
and publish can be very nebulous and

00:09:10,120 --> 00:09:13,600
they'll depend on this the systems that

00:09:11,740 --> 00:09:15,279
you're designing for you may have a

00:09:13,600 --> 00:09:17,829
database that you're reading from to

00:09:15,279 --> 00:09:20,079
ingest data and publishing might mean

00:09:17,829 --> 00:09:22,569
sending a call to an API that another

00:09:20,079 --> 00:09:24,730
service exposes likewise that could be

00:09:22,569 --> 00:09:26,860
inverted you might have a service that

00:09:24,730 --> 00:09:30,040
calls your and calls your micro service

00:09:26,860 --> 00:09:31,880
on an API and publishes data to a

00:09:30,040 --> 00:09:33,410
database the these

00:09:31,880 --> 00:09:35,420
we'll change depending on what you're

00:09:33,410 --> 00:09:40,399
doing but in general you'll be operating

00:09:35,420 --> 00:09:41,899
on this type of model so as you think

00:09:40,399 --> 00:09:43,790
about building these applications you

00:09:41,899 --> 00:09:46,279
need to consider the structural needs of

00:09:43,790 --> 00:09:48,110
what you're building depending on what

00:09:46,279 --> 00:09:49,850
cloud you're using it may be kubernetes

00:09:48,110 --> 00:09:52,160
it may be mezzo so maybe something else

00:09:49,850 --> 00:09:53,269
you have to know how will I deploy my

00:09:52,160 --> 00:09:55,579
application

00:09:53,269 --> 00:09:57,139
how will I command that application from

00:09:55,579 --> 00:09:59,089
the outside and then how will I control

00:09:57,139 --> 00:10:01,910
it what tools are provided to me by the

00:09:59,089 --> 00:10:04,190
platform to do that and likewise where's

00:10:01,910 --> 00:10:06,410
my input and output data going to come

00:10:04,190 --> 00:10:07,880
from and go to and these will be

00:10:06,410 --> 00:10:09,470
dictated by the systems that you're

00:10:07,880 --> 00:10:12,230
working in there's really not a good way

00:10:09,470 --> 00:10:13,220
to generically cover all these except to

00:10:12,230 --> 00:10:16,430
say that you're going to have to

00:10:13,220 --> 00:10:17,990
consider these pieces and so what I'd

00:10:16,430 --> 00:10:19,699
like to talk about is kind of three

00:10:17,990 --> 00:10:21,680
common architectures that I've come

00:10:19,699 --> 00:10:23,209
across and I have a feeling that many of

00:10:21,680 --> 00:10:25,069
you will come across these as well as

00:10:23,209 --> 00:10:28,060
you build applications so we'll talk

00:10:25,069 --> 00:10:30,319
about on-demand batch processing

00:10:28,060 --> 00:10:34,100
continuous batch processing and then

00:10:30,319 --> 00:10:36,500
stream processing so first we'll talk

00:10:34,100 --> 00:10:39,649
about an on-demand batch processing and

00:10:36,500 --> 00:10:42,649
generally what this means is I have some

00:10:39,649 --> 00:10:45,110
event that occurs which kicks off

00:10:42,649 --> 00:10:46,910
processing and then of course I take

00:10:45,110 --> 00:10:49,310
some data in that I want to process and

00:10:46,910 --> 00:10:51,170
I create results so this is something

00:10:49,310 --> 00:10:53,769
that happens every time it's triggered

00:10:51,170 --> 00:10:56,149
and that could be a cron job it could be

00:10:53,769 --> 00:10:59,269
something that comes in and on that API

00:10:56,149 --> 00:11:01,279
call maybe a user visits a website and

00:10:59,269 --> 00:11:05,029
that produces the action but this is one

00:11:01,279 --> 00:11:07,040
pattern you're going to see when might

00:11:05,029 --> 00:11:09,079
you want to use this pattern so in

00:11:07,040 --> 00:11:10,610
general these are kind of some of the

00:11:09,079 --> 00:11:12,709
top things that came up and you know to

00:11:10,610 --> 00:11:15,500
me as I was thinking about this when you

00:11:12,709 --> 00:11:17,509
have non-deterministic requests windows

00:11:15,500 --> 00:11:20,060
so think about a user who visits a

00:11:17,509 --> 00:11:22,250
website like Amazon they're going to be

00:11:20,060 --> 00:11:23,600
clicking through products and just

00:11:22,250 --> 00:11:25,339
looking for something that maybe they'd

00:11:23,600 --> 00:11:27,380
like to buy and every time they click on

00:11:25,339 --> 00:11:29,420
a product there's going to be returned

00:11:27,380 --> 00:11:31,880
some sort of rating some number of stars

00:11:29,420 --> 00:11:32,480
maybe says I suggest this product for

00:11:31,880 --> 00:11:34,730
you or I

00:11:32,480 --> 00:11:36,260
suggest this product for you this is the

00:11:34,730 --> 00:11:38,570
type of area where maybe an on-demand

00:11:36,260 --> 00:11:40,070
call is what you want to use you may not

00:11:38,570 --> 00:11:41,750
have these things pre-calculated

00:11:40,070 --> 00:11:43,880
you may have some model that contains

00:11:41,750 --> 00:11:45,830
this data but you'll need to filter that

00:11:43,880 --> 00:11:47,900
at the time that a user actually hits it

00:11:45,830 --> 00:11:50,290
and you won't necessarily know ahead of

00:11:47,900 --> 00:11:53,900
time that that user is going to do that

00:11:50,290 --> 00:11:55,010
likewise when you have quick results to

00:11:53,900 --> 00:11:57,110
calculate something that could be

00:11:55,010 --> 00:11:59,060
returned very quickly because if if you

00:11:57,110 --> 00:12:00,590
have a situation where you don't know

00:11:59,060 --> 00:12:01,790
when these things are going to happen or

00:12:00,590 --> 00:12:03,560
you have a user visiting a website

00:12:01,790 --> 00:12:05,930
they're not going to want to wait a

00:12:03,560 --> 00:12:07,970
minute for some long processing cycle to

00:12:05,930 --> 00:12:09,260
happen before results come back so this

00:12:07,970 --> 00:12:10,880
is another situation where you might

00:12:09,260 --> 00:12:13,160
think about using this technique and

00:12:10,880 --> 00:12:15,440
then likewise when you have a lot of

00:12:13,160 --> 00:12:17,120
situational dependencies so again think

00:12:15,440 --> 00:12:20,360
of the example of a user visiting a

00:12:17,120 --> 00:12:23,300
website if you're processing depends on

00:12:20,360 --> 00:12:25,670
a user entering information before the

00:12:23,300 --> 00:12:27,590
request can be performed this is a

00:12:25,670 --> 00:12:30,020
situational dependency so this is

00:12:27,590 --> 00:12:31,430
something that you can't pre evaluate or

00:12:30,020 --> 00:12:34,490
maybe it's very difficult to pre

00:12:31,430 --> 00:12:37,760
evaluate this is another scenario where

00:12:34,490 --> 00:12:38,960
on-demand might be what you want so what

00:12:37,760 --> 00:12:40,760
you know what does this kind of look

00:12:38,960 --> 00:12:42,590
like I'm using an example here that I'm

00:12:40,760 --> 00:12:44,630
calling the hello world of spark because

00:12:42,590 --> 00:12:46,580
it's been in the spark codebase I looked

00:12:44,630 --> 00:12:48,530
just the other day and like spark 0.1

00:12:46,580 --> 00:12:50,240
alpha in this example was I think one of

00:12:48,530 --> 00:12:53,150
the only examples that existed in there

00:12:50,240 --> 00:12:55,970
and it's a Monte Carlo method for

00:12:53,150 --> 00:12:57,430
estimating a PI right so you're talking

00:12:55,970 --> 00:12:59,840
about throwing darts at a dartboard

00:12:57,430 --> 00:13:01,850
counting how many land inside a circle

00:12:59,840 --> 00:13:05,540
versus how many land outside and that

00:13:01,850 --> 00:13:07,190
ratio will approximate pi for you so

00:13:05,540 --> 00:13:11,210
this is just a function that does that

00:13:07,190 --> 00:13:13,610
and you can see you know this this part

00:13:11,210 --> 00:13:15,290
here we're doing a similar type of

00:13:13,610 --> 00:13:18,080
operation that we did before you know

00:13:15,290 --> 00:13:20,180
we're paralyzing a range of numbers in

00:13:18,080 --> 00:13:22,960
this case this is the number of random

00:13:20,180 --> 00:13:25,550
points I'd like to use to calculate hi

00:13:22,960 --> 00:13:27,320
then we're mapping a function onto it

00:13:25,550 --> 00:13:28,910
and we're reducing that and this is you

00:13:27,320 --> 00:13:31,190
know this is the technique for returning

00:13:28,910 --> 00:13:32,720
PI so now I've got a function that every

00:13:31,190 --> 00:13:35,150
time I call it can give me that answer

00:13:32,720 --> 00:13:37,760
for pi and what

00:13:35,150 --> 00:13:40,250
I do is I might embed that into an HTTP

00:13:37,760 --> 00:13:42,589
server so if you're familiar with the

00:13:40,250 --> 00:13:45,950
with the framework called flask for

00:13:42,589 --> 00:13:48,050
Python this is an HTTP framework this is

00:13:45,950 --> 00:13:50,830
what it might look like if I embed this

00:13:48,050 --> 00:13:53,660
function into a return so now I've got a

00:13:50,830 --> 00:13:55,670
TTP service on rest based service that

00:13:53,660 --> 00:13:57,770
can basically take a request and give

00:13:55,670 --> 00:13:59,660
back PI whenever I need it and this is

00:13:57,770 --> 00:14:02,000
just one way to look at it you might use

00:13:59,660 --> 00:14:04,310
G RPC you might use some other kind of

00:14:02,000 --> 00:14:08,300
remote procedure call mechanism but this

00:14:04,310 --> 00:14:09,770
is just one way to look at it so let's

00:14:08,300 --> 00:14:12,589
let's move on from that and talk about

00:14:09,770 --> 00:14:14,270
continuous batch processing right and I

00:14:12,589 --> 00:14:16,040
look at this as a this is a type of

00:14:14,270 --> 00:14:18,380
scenario where you have data sitting in

00:14:16,040 --> 00:14:20,900
some sort of data source your processing

00:14:18,380 --> 00:14:22,520
is always occurring there's no need to

00:14:20,900 --> 00:14:24,290
trigger it with an event because you

00:14:22,520 --> 00:14:26,570
want this process to always be running

00:14:24,290 --> 00:14:30,320
and always be producing results to

00:14:26,570 --> 00:14:32,240
wherever your store is and when might be

00:14:30,320 --> 00:14:34,670
used this pattern so if you have data

00:14:32,240 --> 00:14:37,100
that updates very frequently let's say

00:14:34,670 --> 00:14:39,950
you have a database of users who are

00:14:37,100 --> 00:14:41,560
always giving ray days for example you

00:14:39,950 --> 00:14:43,880
want to be creating creating

00:14:41,560 --> 00:14:45,290
recommendations let's say off those user

00:14:43,880 --> 00:14:46,880
ratings and you want to be doing it

00:14:45,290 --> 00:14:48,800
continually because that information is

00:14:46,880 --> 00:14:50,390
always happening so there's probably

00:14:48,800 --> 00:14:52,520
never a time that I don't want that to

00:14:50,390 --> 00:14:54,770
be running and one way you might use

00:14:52,520 --> 00:14:57,170
that is when you're creating machine

00:14:54,770 --> 00:14:58,820
learning models for evaluation right so

00:14:57,170 --> 00:15:01,700
if we think about a recommendation

00:14:58,820 --> 00:15:03,589
engine where users from all your

00:15:01,700 --> 00:15:05,120
products like Amazon for example they're

00:15:03,589 --> 00:15:08,330
always rating what they like and what

00:15:05,120 --> 00:15:10,670
they don't like you want to continuously

00:15:08,330 --> 00:15:13,520
be creating models so that you can

00:15:10,670 --> 00:15:14,750
evaluate how those models are performing

00:15:13,520 --> 00:15:18,589
against what you have in production

00:15:14,750 --> 00:15:20,959
right so you want to say as users add

00:15:18,589 --> 00:15:22,520
more data or my models getting better

00:15:20,959 --> 00:15:24,440
maybe they're getting fresher in the

00:15:22,520 --> 00:15:26,060
case of recommendation system you

00:15:24,440 --> 00:15:27,770
probably always want it to be fresh

00:15:26,060 --> 00:15:30,740
because users are continuing to add

00:15:27,770 --> 00:15:32,750
information and then another way to look

00:15:30,740 --> 00:15:35,300
at this too or a situation you might use

00:15:32,750 --> 00:15:36,250
it in is what I'm calling lifecycle

00:15:35,300 --> 00:15:38,920
management process

00:15:36,250 --> 00:15:41,050
so think about a lot of the work that

00:15:38,920 --> 00:15:44,080
gets done in distributed computing is

00:15:41,050 --> 00:15:46,480
data engineering it's transforming one

00:15:44,080 --> 00:15:48,010
schema into another schema we're taking

00:15:46,480 --> 00:15:50,230
one format and turning it into another

00:15:48,010 --> 00:15:51,640
format this is something where you might

00:15:50,230 --> 00:15:54,070
just want it to be continually running

00:15:51,640 --> 00:15:57,130
say I've got users who are putting in

00:15:54,070 --> 00:15:58,480
data non-stop maybe it's a text data and

00:15:57,130 --> 00:16:00,340
I want to make sure that no one's

00:15:58,480 --> 00:16:02,290
putting in words that are you know on

00:16:00,340 --> 00:16:04,390
our band list or something right so you

00:16:02,290 --> 00:16:05,680
want this to always be running always be

00:16:04,390 --> 00:16:09,370
searching for results that are coming

00:16:05,680 --> 00:16:11,680
out of it now what what might this look

00:16:09,370 --> 00:16:13,990
like so this is a piece of code from a

00:16:11,680 --> 00:16:15,340
recommendation engine that one of my

00:16:13,990 --> 00:16:17,070
colleagues is actually sitting here

00:16:15,340 --> 00:16:20,830
helped to write really in the back there

00:16:17,070 --> 00:16:22,690
and this is part of a model generation

00:16:20,830 --> 00:16:25,000
service and what it does is you can see

00:16:22,690 --> 00:16:26,950
at the top there's this while true loop

00:16:25,000 --> 00:16:29,470
and it does some sort of database

00:16:26,950 --> 00:16:31,600
selection here to pull the ratings out

00:16:29,470 --> 00:16:33,100
of our database and then there's a bunch

00:16:31,600 --> 00:16:35,490
of calculation going on here

00:16:33,100 --> 00:16:39,820
now what I really want to highlight is

00:16:35,490 --> 00:16:41,980
this part here we see where we've taken

00:16:39,820 --> 00:16:44,350
the new ratings from our database source

00:16:41,980 --> 00:16:46,270
since the last time I created a model I

00:16:44,350 --> 00:16:49,360
want to see all the new ratings that

00:16:46,270 --> 00:16:51,580
have been added to the database I create

00:16:49,360 --> 00:16:53,410
an RDD out of that and then I'm going to

00:16:51,580 --> 00:16:56,200
do some processing here where I create a

00:16:53,410 --> 00:16:58,540
model from that and so this is just

00:16:56,200 --> 00:17:00,910
always running and if it sees changes to

00:16:58,540 --> 00:17:02,740
the source data it creates a new model

00:17:00,910 --> 00:17:04,870
and it puts that model into my database

00:17:02,740 --> 00:17:09,610
run I'm kind of storing these models for

00:17:04,870 --> 00:17:11,020
later usage so the last kind of pattern

00:17:09,610 --> 00:17:12,760
I want to talk about is a stream

00:17:11,020 --> 00:17:14,709
processing pattern and this is something

00:17:12,760 --> 00:17:16,270
that I find very intriguing and I think

00:17:14,709 --> 00:17:18,760
it's a really cool way to work with data

00:17:16,270 --> 00:17:20,290
but this is where you have a stream of

00:17:18,760 --> 00:17:24,130
information that's always occurring so

00:17:20,290 --> 00:17:26,949
think about Kafka or MQ or those type of

00:17:24,130 --> 00:17:29,440
message bus type applications there's

00:17:26,949 --> 00:17:31,660
data that's always coming in and my

00:17:29,440 --> 00:17:33,670
processing this is a little different

00:17:31,660 --> 00:17:36,550
than the continuous batch my processing

00:17:33,670 --> 00:17:38,200
is reacting to every time data comes in

00:17:36,550 --> 00:17:39,890
on that bus you know within some sort of

00:17:38,200 --> 00:17:41,720
window and then

00:17:39,890 --> 00:17:45,830
again those results are stored somewhere

00:17:41,720 --> 00:17:47,270
else now you probably would want to use

00:17:45,830 --> 00:17:49,430
this in a situation where you have like

00:17:47,270 --> 00:17:52,640
real time of that processes so think

00:17:49,430 --> 00:17:55,730
about an IOT situation where you have

00:17:52,640 --> 00:17:57,830
sensors maybe public transportation

00:17:55,730 --> 00:17:59,630
right so you want to follow where all

00:17:57,830 --> 00:18:01,640
the buses in your public transportation

00:17:59,630 --> 00:18:03,140
system are going are any of them running

00:18:01,640 --> 00:18:05,210
late those type of things so you have a

00:18:03,140 --> 00:18:07,160
continuous stream of information coming

00:18:05,210 --> 00:18:09,020
in and you always want to be updating

00:18:07,160 --> 00:18:11,210
out what's happening so that may be one

00:18:09,020 --> 00:18:14,030
area another is if you're working with

00:18:11,210 --> 00:18:16,820
systems that are built on a broadcast

00:18:14,030 --> 00:18:18,890
messaging system so I like to think

00:18:16,820 --> 00:18:20,570
about the Fedora message bus

00:18:18,890 --> 00:18:24,920
how many Fedora uses we have in the

00:18:20,570 --> 00:18:26,570
audience here okay a couple Fedora since

00:18:24,920 --> 00:18:28,160
it's large community system they have

00:18:26,570 --> 00:18:30,050
something called the Fedora message bus

00:18:28,160 --> 00:18:33,140
and it's it's a federated message bus

00:18:30,050 --> 00:18:34,940
system where you have messages coming in

00:18:33,140 --> 00:18:36,020
from build systems maybe for all the

00:18:34,940 --> 00:18:38,600
different packages that are being

00:18:36,020 --> 00:18:39,890
created messages coming in from mailing

00:18:38,600 --> 00:18:42,440
lists you know all sorts of information

00:18:39,890 --> 00:18:44,150
being aggregated and this is a situation

00:18:42,440 --> 00:18:45,860
where well I would obviously want to

00:18:44,150 --> 00:18:47,480
build something to that message bus

00:18:45,860 --> 00:18:50,830
because that's the architecture of the

00:18:47,480 --> 00:18:53,420
system I'm working on and then likewise

00:18:50,830 --> 00:18:54,800
another level to this is what what we

00:18:53,420 --> 00:18:57,110
call like you know what people are

00:18:54,800 --> 00:18:58,640
calling campus style architectures and

00:18:57,110 --> 00:19:00,650
and this is a way to look at stream

00:18:58,640 --> 00:19:02,600
processing where you have a stream

00:19:00,650 --> 00:19:04,490
that's your input data you have

00:19:02,600 --> 00:19:06,350
processing happening and then your

00:19:04,490 --> 00:19:08,600
output is always going to be to another

00:19:06,350 --> 00:19:11,630
stream so you're kind of creating these

00:19:08,600 --> 00:19:15,260
message bus scenarios where you know one

00:19:11,630 --> 00:19:17,570
topic might be your your clean data and

00:19:15,260 --> 00:19:19,430
then you have a process that runs and

00:19:17,570 --> 00:19:21,170
maybe changes the schema or pulls out

00:19:19,430 --> 00:19:23,060
some specific information lays it to

00:19:21,170 --> 00:19:25,100
another stream and this allows you to

00:19:23,060 --> 00:19:27,650
build up really complex hierarchies of

00:19:25,100 --> 00:19:28,010
applications that don't necessarily need

00:19:27,650 --> 00:19:29,630
to

00:19:28,010 --> 00:19:32,260
penned on each other all they really

00:19:29,630 --> 00:19:35,990
need to depend on is is the message bus

00:19:32,260 --> 00:19:39,350
so spark has a really cool API called

00:19:35,990 --> 00:19:40,640
the structured streaming API and and

00:19:39,350 --> 00:19:41,990
this is kind of what it looks like

00:19:40,640 --> 00:19:43,790
there's multiple ways to do stream

00:19:41,990 --> 00:19:45,260
processing and spark but I think the

00:19:43,790 --> 00:19:47,480
structured streaming is really

00:19:45,260 --> 00:19:50,240
interesting and what you do is you kind

00:19:47,480 --> 00:19:51,950
of build up this set of instructions you

00:19:50,240 --> 00:19:53,570
tell spark where will my information

00:19:51,950 --> 00:19:57,190
come from and that's kind of up here I'm

00:19:53,570 --> 00:20:00,440
telling it a acosta broker and then I

00:19:57,190 --> 00:20:02,570
tell it what to do so I wanted to select

00:20:00,440 --> 00:20:04,370
you know and this this is real simple

00:20:02,570 --> 00:20:06,920
it's just taking every value that comes

00:20:04,370 --> 00:20:10,100
in on the stream and casting it to a

00:20:06,920 --> 00:20:11,960
string and then rebroadcasting that so

00:20:10,100 --> 00:20:14,720
this is just really kind of looking at a

00:20:11,960 --> 00:20:16,970
simple example and then what happens is

00:20:14,720 --> 00:20:19,280
all those strings that come in get

00:20:16,970 --> 00:20:22,160
grouped by the value that they created

00:20:19,280 --> 00:20:23,960
and counted so what's happening here is

00:20:22,160 --> 00:20:26,450
this application is doing like a word

00:20:23,960 --> 00:20:28,910
count on a stream it looks at every

00:20:26,450 --> 00:20:31,280
string that comes in it groups them by

00:20:28,910 --> 00:20:33,320
similar and then counts them up right so

00:20:31,280 --> 00:20:34,970
you can say imagine you got a stream of

00:20:33,320 --> 00:20:37,460
words coming by this is just going to

00:20:34,970 --> 00:20:39,800
count them and then at the bottom here

00:20:37,460 --> 00:20:41,570
you can see I'm telling you where I

00:20:39,800 --> 00:20:43,010
wanted to put the output of that stream

00:20:41,570 --> 00:20:45,050
and then I just tell it to start and in

00:20:43,010 --> 00:20:47,540
this case I'm just storing it in sparks

00:20:45,050 --> 00:20:49,880
in memory storage and I'm giving it a

00:20:47,540 --> 00:20:51,860
name so that I can query that so at this

00:20:49,880 --> 00:20:55,370
point it's doing a bunch of work but how

00:20:51,860 --> 00:20:57,040
do I get the results out of this so what

00:20:55,370 --> 00:21:00,770
I might have is a function like this

00:20:57,040 --> 00:21:02,840
that allows me to look at sparks

00:21:00,770 --> 00:21:05,480
internal kind of SQL in memory

00:21:02,840 --> 00:21:07,970
representation and then I can I can run

00:21:05,480 --> 00:21:09,050
a query on it so in this case the query

00:21:07,970 --> 00:21:11,330
that I'm running is I just want to know

00:21:09,050 --> 00:21:12,620
the top 10 entries of all the things

00:21:11,330 --> 00:21:14,570
that's counted so I want to know the top

00:21:12,620 --> 00:21:16,700
10 most you know frequent things that

00:21:14,570 --> 00:21:18,330
are happening but this is I could call

00:21:16,700 --> 00:21:22,679
this function on demand when I

00:21:18,330 --> 00:21:23,789
to get information out of it so that

00:21:22,679 --> 00:21:25,080
kind of brings me to the next way you

00:21:23,789 --> 00:21:27,390
could do this right and this is an

00:21:25,080 --> 00:21:28,620
example of what you know the kappa style

00:21:27,390 --> 00:21:30,179
architecture would look like you have a

00:21:28,620 --> 00:21:31,590
stream coming in your processing

00:21:30,179 --> 00:21:35,340
happening then you have a stream that it

00:21:31,590 --> 00:21:36,840
lays it out to and if we look at this

00:21:35,340 --> 00:21:39,000
kind of similar function that we just

00:21:36,840 --> 00:21:42,809
looked at the top part of it's all the

00:21:39,000 --> 00:21:44,279
same it's all doing the same work but if

00:21:42,809 --> 00:21:47,010
we look at the bottom part where it's

00:21:44,279 --> 00:21:48,570
writing output of that stream I've told

00:21:47,010 --> 00:21:51,929
it now just to put it on to another

00:21:48,570 --> 00:21:53,549
Kafka topic right and so what this means

00:21:51,929 --> 00:21:55,649
is that I don't have to worry about

00:21:53,549 --> 00:21:57,990
having a routine that pulls that

00:21:55,649 --> 00:21:59,460
information out using an SQL query I

00:21:57,990 --> 00:22:02,220
could actually have another micro

00:21:59,460 --> 00:22:04,830
service that just listens on that second

00:22:02,220 --> 00:22:06,840
topic and then it would it would be

00:22:04,830 --> 00:22:08,159
automatically giving me all those counts

00:22:06,840 --> 00:22:09,929
and I could do whatever I needed to

00:22:08,159 --> 00:22:11,010
there I could aggregate them or you know

00:22:09,929 --> 00:22:15,960
I could do something different at that

00:22:11,010 --> 00:22:17,340
point okay so you know we've talked

00:22:15,960 --> 00:22:19,830
about some different patterns you might

00:22:17,340 --> 00:22:22,320
use how do I take these from that a

00:22:19,830 --> 00:22:24,389
desktop example and bring them into the

00:22:22,320 --> 00:22:26,669
cloud you know I want to take it from

00:22:24,389 --> 00:22:28,559
source code I need to turn it into a

00:22:26,669 --> 00:22:30,419
container and then I want to push it

00:22:28,559 --> 00:22:32,789
into my orchestration platform and at

00:22:30,419 --> 00:22:34,740
the same time I need a way for the user

00:22:32,789 --> 00:22:39,029
to still get in and out or myself to get

00:22:34,740 --> 00:22:40,409
in and out of it so the group that I

00:22:39,029 --> 00:22:42,059
work with the Red Hat we have a

00:22:40,409 --> 00:22:44,880
community project called rad analytics

00:22:42,059 --> 00:22:47,490
io and we've created some tooling a

00:22:44,880 --> 00:22:49,919
project that we call our Shenko and this

00:22:47,490 --> 00:22:51,690
allows us to use some of the source to

00:22:49,919 --> 00:22:54,120
image type workflows that are in

00:22:51,690 --> 00:22:56,940
OpenShift to say I'm gonna take my

00:22:54,120 --> 00:22:59,190
source code from a git repository I'm

00:22:56,940 --> 00:23:01,620
gonna use the source to image to build

00:22:59,190 --> 00:23:03,899
that into an image that can run and then

00:23:01,620 --> 00:23:06,330
when it gets deployed a spark cluster

00:23:03,899 --> 00:23:09,389
will go with it and be bound to my

00:23:06,330 --> 00:23:11,159
applications lifecycle so in this way I

00:23:09,389 --> 00:23:13,590
don't even have to manage spark anymore

00:23:11,159 --> 00:23:15,299
I can use the workflow that I'm used to

00:23:13,590 --> 00:23:17,890
in open ships going right from my code

00:23:15,299 --> 00:23:20,530
making pushes to my code and that

00:23:17,890 --> 00:23:22,840
through you know a CI testing framework

00:23:20,530 --> 00:23:24,160
and then when it's successful it gets

00:23:22,840 --> 00:23:27,250
deployed on to openshift

00:23:24,160 --> 00:23:31,390
and a spark cluster will appear and be

00:23:27,250 --> 00:23:32,830
found to it so I'm gonna assuming

00:23:31,390 --> 00:23:34,510
nothing else goes wrong I'm gonna try

00:23:32,830 --> 00:23:42,460
and demonstrate for this few you know

00:23:34,510 --> 00:23:44,320
real quickly here so i've got i've got a

00:23:42,460 --> 00:23:46,900
small github repository here and this is

00:23:44,320 --> 00:23:50,050
a tutorial you can find on rad analytics

00:23:46,900 --> 00:23:52,030
and this is a web micro service that's

00:23:50,050 --> 00:23:53,830
going to create that spark pie for us

00:23:52,030 --> 00:23:57,580
right so it's going to create an HTTP

00:23:53,830 --> 00:23:59,800
service that i can on demand query to

00:23:57,580 --> 00:24:02,500
get a spark calculation and you can see

00:23:59,800 --> 00:24:04,240
my republic in this case my repository

00:24:02,500 --> 00:24:06,040
is pretty simple you know I mean I've

00:24:04,240 --> 00:24:08,710
got to read me I've got my app file

00:24:06,040 --> 00:24:10,390
which is not you know not overly long

00:24:08,710 --> 00:24:12,040
it's just a little flask application

00:24:10,390 --> 00:24:15,060
then I've got you know the requirements

00:24:12,040 --> 00:24:18,090
like any Python application might have

00:24:15,060 --> 00:24:19,240
so what we're looking at here is

00:24:18,090 --> 00:24:21,130
OpenShift

00:24:19,240 --> 00:24:22,870
this is my project and I've already

00:24:21,130 --> 00:24:25,300
taken the liberty of loading into this

00:24:22,870 --> 00:24:28,180
the read analytics template so I'm going

00:24:25,300 --> 00:24:30,430
to use and so what I'll do is I'm gonna

00:24:28,180 --> 00:24:32,080
select from my project the template that

00:24:30,430 --> 00:24:35,170
I'd like to use so I'd like to launch a

00:24:32,080 --> 00:24:38,020
Pachi smart Python so you know I select

00:24:35,170 --> 00:24:39,160
the template that I want kind of click

00:24:38,020 --> 00:24:40,930
through the description you can see now

00:24:39,160 --> 00:24:43,540
it's asking me for a bunch of

00:24:40,930 --> 00:24:46,120
information here so maybe I'm gonna call

00:24:43,540 --> 00:24:48,400
my service spark PI and now what it

00:24:46,120 --> 00:24:51,490
wants is the URL for my github

00:24:48,400 --> 00:24:55,750
repository so I'll just copy this from

00:24:51,490 --> 00:24:57,160
here ok now there's a bunch of other

00:24:55,750 --> 00:24:58,810
options I could use here maybe if I

00:24:57,160 --> 00:25:00,400
wanted to build from a branch or there

00:24:58,810 --> 00:25:02,290
was a subdirectory or this building from

00:25:00,400 --> 00:25:04,210
or you know these options help you

00:25:02,290 --> 00:25:06,100
control how the application gets

00:25:04,210 --> 00:25:07,660
deployed but my applications written in

00:25:06,100 --> 00:25:09,550
a very simple manner so I don't need to

00:25:07,660 --> 00:25:11,560
fill in most of these and likewise at

00:25:09,550 --> 00:25:14,200
the bottom I could I could adjust how

00:25:11,560 --> 00:25:17,890
the spark cluster gets deployed I could

00:25:14,200 --> 00:25:20,400
change the options that go to it so I'll

00:25:17,890 --> 00:25:22,270
click create what we see now is

00:25:20,400 --> 00:25:23,950
openshift this is being built on

00:25:22,270 --> 00:25:24,850
openshift and so if i look at the logs

00:25:23,950 --> 00:25:26,670
for this

00:25:24,850 --> 00:25:28,900
you can see this is kind of a standard

00:25:26,670 --> 00:25:31,900
Python build process and then it's

00:25:28,900 --> 00:25:34,150
pushing it to the internal registry now

00:25:31,900 --> 00:25:35,650
you can see at this point my pod is up

00:25:34,150 --> 00:25:37,600
and running but what's happening is the

00:25:35,650 --> 00:25:40,720
spark cluster is being deployed with my

00:25:37,600 --> 00:25:42,100
pod so this is automatically being found

00:25:40,720 --> 00:25:44,650
in my application and it's kind of

00:25:42,100 --> 00:25:45,970
giving it a random name now the last

00:25:44,650 --> 00:25:48,940
thing I need to do to get to this is

00:25:45,970 --> 00:25:52,870
just expose the route to it so that I

00:25:48,940 --> 00:25:55,300
can get a get to it and now if I click

00:25:52,870 --> 00:25:57,760
on this hopefully to work okay so I hit

00:25:55,300 --> 00:25:59,800
the root end point and it tells me you

00:25:57,760 --> 00:26:01,240
know the Python flask spark pie server

00:25:59,800 --> 00:26:04,570
is running I need to add this extra

00:26:01,240 --> 00:26:07,180
thing to get more information so if I

00:26:04,570 --> 00:26:08,620
add this extra route you see now there's

00:26:07,180 --> 00:26:10,330
kind of this weight going on this goes

00:26:08,620 --> 00:26:12,580
back to the quick results side of this

00:26:10,330 --> 00:26:14,140
right what's happening all I can see is

00:26:12,580 --> 00:26:16,390
little spinner going but eventually it

00:26:14,140 --> 00:26:19,360
comes back he gives me a really bad

00:26:16,390 --> 00:26:20,770
estimate of pie okay so like don't you

00:26:19,360 --> 00:26:22,690
know don't go to the moon with this or

00:26:20,770 --> 00:26:26,500
anything but it's it's fun to play with

00:26:22,690 --> 00:26:27,550
right and you know something I want to

00:26:26,500 --> 00:26:31,030
point out here that I talked about

00:26:27,550 --> 00:26:34,810
before which is now that my application

00:26:31,030 --> 00:26:36,160
is linked to this github repository the

00:26:34,810 --> 00:26:38,640
Skip repository you can see I've got

00:26:36,160 --> 00:26:42,280
this thing called a bill dear and I

00:26:38,640 --> 00:26:44,440
don't have my web hooks set up but if I

00:26:42,280 --> 00:26:47,260
if I did what I could do is push a

00:26:44,440 --> 00:26:49,330
change directly to my git repository the

00:26:47,260 --> 00:26:51,190
web hook would hit openshift and this

00:26:49,330 --> 00:26:53,350
would actually rebuild automatically for

00:26:51,190 --> 00:26:55,240
me now in this case you know I could hit

00:26:53,350 --> 00:26:56,920
start build like if I made a change to

00:26:55,240 --> 00:26:58,930
the repository I could hit start build

00:26:56,920 --> 00:27:00,820
and it would run and then deploy it

00:26:58,930 --> 00:27:03,760
again and attach it to the spark cluster

00:27:00,820 --> 00:27:05,530
again so as a developer this is really

00:27:03,760 --> 00:27:07,270
nice because I can really easily test my

00:27:05,530 --> 00:27:09,780
changes out and even in a private

00:27:07,270 --> 00:27:14,490
project I can do this

00:27:09,780 --> 00:27:14,490
so let me switch back here

00:27:16,240 --> 00:27:21,230
so you saw when I made that web request

00:27:19,010 --> 00:27:24,350
to do the work it took a little while to

00:27:21,230 --> 00:27:25,730
come back right and this is where one of

00:27:24,350 --> 00:27:27,830
the problems you're going to run into

00:27:25,730 --> 00:27:29,870
when designing these type of services is

00:27:27,830 --> 00:27:32,660
kind of a synchronicity issue right so I

00:27:29,870 --> 00:27:34,670
make a request for PI and now that

00:27:32,660 --> 00:27:36,620
service is off doing something right and

00:27:34,670 --> 00:27:38,960
if I if I tell it to scan you know to

00:27:36,620 --> 00:27:41,179
use two large data set this could take

00:27:38,960 --> 00:27:42,830
minutes to come back right you don't you

00:27:41,179 --> 00:27:45,050
don't want that result coming back later

00:27:42,830 --> 00:27:47,780
you know the user just walking away from

00:27:45,050 --> 00:27:51,320
the terminal or something so to mitigate

00:27:47,780 --> 00:27:54,050
this in our designs what we like to do

00:27:51,320 --> 00:27:56,870
is start to separate you know the API

00:27:54,050 --> 00:27:59,240
concerns from the actual processing

00:27:56,870 --> 00:28:00,590
concerns and this might be a very common

00:27:59,240 --> 00:28:04,460
way to look at this would be you know

00:28:00,590 --> 00:28:08,179
our main process an API whenever I make

00:28:04,460 --> 00:28:09,650
a request for a new highest of it maybe

00:28:08,179 --> 00:28:11,240
instead of giving me back the pious

00:28:09,650 --> 00:28:13,790
limit maybe what it'll do is right away

00:28:11,240 --> 00:28:15,950
it'll give me back an ID and then I can

00:28:13,790 --> 00:28:19,460
use that ID to query the results of

00:28:15,950 --> 00:28:20,870
what's happening or perhaps you know the

00:28:19,460 --> 00:28:23,720
application on the other end of this it

00:28:20,870 --> 00:28:26,240
uses a WebSocket right so I make the

00:28:23,720 --> 00:28:28,160
request and now the main processing loop

00:28:26,240 --> 00:28:28,670
can push the information back to me when

00:28:28,160 --> 00:28:30,650
it's ready

00:28:28,670 --> 00:28:32,330
and in the meantime my application can

00:28:30,650 --> 00:28:34,400
display some message saying this you

00:28:32,330 --> 00:28:36,520
know work is happening or whatever so

00:28:34,400 --> 00:28:38,330
you know depending on what this API is

00:28:36,520 --> 00:28:40,010
you'll have kind of different ways to

00:28:38,330 --> 00:28:41,660
mitigate this but in general what you

00:28:40,010 --> 00:28:44,720
want to start doing with micro services

00:28:41,660 --> 00:28:46,280
is is pulling apart these concerns to

00:28:44,720 --> 00:28:48,130
make it easy to deal with the other ends

00:28:46,280 --> 00:28:52,070
of them and to address issues like this

00:28:48,130 --> 00:28:54,230
synchronicity issue so what might this

00:28:52,070 --> 00:28:56,270
look like in python well this is like

00:28:54,230 --> 00:28:59,240
the main process right you know got some

00:28:56,270 --> 00:29:00,370
code here we set up you know queues for

00:28:59,240 --> 00:29:02,390
doing the inter process communication

00:29:00,370 --> 00:29:04,940
and then you start off some other

00:29:02,390 --> 00:29:07,190
process and send everything going this

00:29:04,940 --> 00:29:08,300
is pretty compact and I don't want to go

00:29:07,190 --> 00:29:09,980
into every line of it but what I want

00:29:08,300 --> 00:29:12,260
you to take away from this is the top

00:29:09,980 --> 00:29:15,110
line which is import multi-processing

00:29:12,260 --> 00:29:17,720
the Python multi processing package is

00:29:15,110 --> 00:29:18,890
really powerful and I would say that if

00:29:17,720 --> 00:29:19,550
you're going to start doing these type

00:29:18,890 --> 00:29:21,740
of thing

00:29:19,550 --> 00:29:23,510
read the docs on that package because

00:29:21,740 --> 00:29:25,940
the primitives and they are very easy to

00:29:23,510 --> 00:29:29,240
use and so if this is the main process

00:29:25,940 --> 00:29:30,530
side this is maybe what our processing

00:29:29,240 --> 00:29:31,940
loop looks like you know and so we've

00:29:30,530 --> 00:29:34,960
got this big thing here where there's

00:29:31,940 --> 00:29:38,840
you know this is coming from a service

00:29:34,960 --> 00:29:40,820
that actually responds to incoming

00:29:38,840 --> 00:29:42,500
requests for recommendations basically

00:29:40,820 --> 00:29:44,270
right so you have a user who wants to

00:29:42,500 --> 00:29:47,000
get a recommendation just to kind of do

00:29:44,270 --> 00:29:48,770
it for you and you know it's a big piece

00:29:47,000 --> 00:29:50,480
of code whatever I've taken out pieces

00:29:48,770 --> 00:29:53,270
of it but the main thing to look at here

00:29:50,480 --> 00:29:54,860
are these areas that are probably really

00:29:53,270 --> 00:29:56,120
difficult to see because they're in red

00:29:54,860 --> 00:29:58,820
and there's a lot of light washing and

00:29:56,120 --> 00:30:03,170
output you can see these primitives this

00:29:58,820 --> 00:30:06,110
response request response response these

00:30:03,170 --> 00:30:08,120
are the cues that I use to communicate

00:30:06,110 --> 00:30:09,230
back with the main process so whatever

00:30:08,120 --> 00:30:10,670
I'm doing here I'm using these

00:30:09,230 --> 00:30:13,160
primitives and again this is coming from

00:30:10,670 --> 00:30:16,720
the multi processing library and you

00:30:13,160 --> 00:30:16,720
know I really recommend checking it out

00:30:16,870 --> 00:30:20,180
another big thing that you're going to

00:30:18,650 --> 00:30:22,010
run into especially if you're doing

00:30:20,180 --> 00:30:24,830
Python programming for SPARC is

00:30:22,010 --> 00:30:27,380
dependency management so right now

00:30:24,830 --> 00:30:30,410
SPARC has some really good features for

00:30:27,380 --> 00:30:32,870
jvm languages that need to distribute

00:30:30,410 --> 00:30:34,370
dependencies there's a to that SPARC

00:30:32,870 --> 00:30:35,930
submit command there's an option called

00:30:34,370 --> 00:30:39,500
packages and you can give it you know

00:30:35,930 --> 00:30:40,970
maven targets and it will pull all those

00:30:39,500 --> 00:30:42,770
packages in and send them out to the

00:30:40,970 --> 00:30:44,960
entire cluster so your applications can

00:30:42,770 --> 00:30:47,690
use them with Python the tooling is a

00:30:44,960 --> 00:30:50,600
little behind the times so let's say

00:30:47,690 --> 00:30:52,640
we've got this application and you know

00:30:50,600 --> 00:30:53,900
that filter this is maybe like let's say

00:30:52,640 --> 00:30:55,910
we're building another service to tell

00:30:53,900 --> 00:30:57,980
us how many heathens exist in the data

00:30:55,910 --> 00:31:00,920
set you know our classic hard problem

00:30:57,980 --> 00:31:02,210
here and you can see it at the bottom

00:31:00,920 --> 00:31:02,420
here is where I'm actually doing the

00:31:02,210 --> 00:31:04,100
work

00:31:02,420 --> 00:31:06,170
you know I'm telling it to kind of make

00:31:04,100 --> 00:31:08,480
the paralyzed data a filter it and count

00:31:06,170 --> 00:31:10,220
it and this filter even's function is

00:31:08,480 --> 00:31:15,020
doing some sort of database

00:31:10,220 --> 00:31:16,760
right so what this means is that filter

00:31:15,020 --> 00:31:19,280
evens function is going to be

00:31:16,760 --> 00:31:21,440
distributed to the spark cluster and so

00:31:19,280 --> 00:31:24,799
what that means is each executor is

00:31:21,440 --> 00:31:26,120
going to need PI Bongo in place because

00:31:24,799 --> 00:31:26,600
what's going to happen is something like

00:31:26,120 --> 00:31:28,880
this

00:31:26,600 --> 00:31:32,510
my main application wants to talk to

00:31:28,880 --> 00:31:35,450
 but now I've distributed code that

00:31:32,510 --> 00:31:37,159
also talks to Bongo and so that

00:31:35,450 --> 00:31:40,159
library needs to be on every one of the

00:31:37,159 --> 00:31:41,600
executor nodes and this is a situation

00:31:40,159 --> 00:31:43,880
where you might have to manage these

00:31:41,600 --> 00:31:45,440
dependencies yourself until the spark

00:31:43,880 --> 00:31:49,179
community kind of catches up with this

00:31:45,440 --> 00:31:52,549
and gives us better tooling for doing it

00:31:49,179 --> 00:31:55,789
so just let's recap a little bit here

00:31:52,549 --> 00:31:57,440
you know we talked about this design

00:31:55,789 --> 00:32:00,200
pattern that I really like to use the

00:31:57,440 --> 00:32:01,940
ingest publish process this is kind of

00:32:00,200 --> 00:32:05,470
the general pattern I like to get into

00:32:01,940 --> 00:32:07,490
we talked about some different types of

00:32:05,470 --> 00:32:08,929
architecture patterns you might get into

00:32:07,490 --> 00:32:11,750
when you're designing your applications

00:32:08,929 --> 00:32:13,909
we talked about you know the on demand

00:32:11,750 --> 00:32:16,010
batch the continuous batch and the

00:32:13,909 --> 00:32:19,070
stream processing and then we also

00:32:16,010 --> 00:32:22,490
talked about the ocean cows or stim egde

00:32:19,070 --> 00:32:24,860
project so this QR code up here is a

00:32:22,490 --> 00:32:27,470
link to this slide deck you know please

00:32:24,860 --> 00:32:29,000
download it and you're it's it's open

00:32:27,470 --> 00:32:30,919
source it's it's just a reveal.js

00:32:29,000 --> 00:32:34,700
project you're welcome to use what's

00:32:30,919 --> 00:32:37,190
there here's my email and my my blog and

00:32:34,700 --> 00:32:39,230
you know please check out red analytics

00:32:37,190 --> 00:32:40,909
I oh we've got a bunch of tutorials

00:32:39,230 --> 00:32:44,960
there and lots of this material you've

00:32:40,909 --> 00:32:46,640
seen here is on that site so at this

00:32:44,960 --> 00:32:49,929
point you know I'll take any questions

00:32:46,640 --> 00:32:49,929
and you know thank you for your time

00:32:53,670 --> 00:33:04,330
sorry I guess any questions varmint

00:33:00,580 --> 00:33:07,270
don't hit me up too hard here okay yeah

00:33:04,330 --> 00:33:10,170
I'm talkin the pass pattern you the

00:33:07,270 --> 00:33:13,620
client would call rasp service and

00:33:10,170 --> 00:33:16,570
response comes back immediately right so

00:33:13,620 --> 00:33:21,330
there's a code here describing so what

00:33:16,570 --> 00:33:24,250
happens the client continuously Falls

00:33:21,330 --> 00:33:26,620
and the second the function that that

00:33:24,250 --> 00:33:28,990
you showed us right it is the multi

00:33:26,620 --> 00:33:32,470
processing package is the client

00:33:28,990 --> 00:33:35,530
continuously needs to connect to be the

00:33:32,470 --> 00:33:38,620
same invocation of that request right

00:33:35,530 --> 00:33:40,720
for that process to happen yeah how can

00:33:38,620 --> 00:33:42,550
there be like and the status you know

00:33:40,720 --> 00:33:45,550
function independent of the first one

00:33:42,550 --> 00:33:47,890
right that goes back and gets the data

00:33:45,550 --> 00:33:50,950
directly from this spot what is that

00:33:47,890 --> 00:33:53,050
so you know the first example I showed

00:33:50,950 --> 00:33:54,220
is a very simple example right we just

00:33:53,050 --> 00:33:56,350
you're hitting it and it's trying to

00:33:54,220 --> 00:33:57,520
make the request and if you double up on

00:33:56,350 --> 00:33:58,930
requests at that point you're gonna

00:33:57,520 --> 00:34:00,790
break that because that application is

00:33:58,930 --> 00:34:04,150
really simple but once we start to

00:34:00,790 --> 00:34:05,370
separate the API you know the first way

00:34:04,150 --> 00:34:07,900
that I think about it for a rest

00:34:05,370 --> 00:34:09,760
application is I say I'm gonna make a

00:34:07,900 --> 00:34:12,610
request to the rest server to start

00:34:09,760 --> 00:34:15,820
doing work and the rest server gives me

00:34:12,610 --> 00:34:17,740
back an ID and that ID number is the

00:34:15,820 --> 00:34:21,640
work is the idea of the work that's

00:34:17,740 --> 00:34:23,649
being done right so my processing you

00:34:21,640 --> 00:34:26,169
know my processing loop there the second

00:34:23,649 --> 00:34:28,270
process it may have a queue of work

00:34:26,169 --> 00:34:30,190
requests that have come in and it knows

00:34:28,270 --> 00:34:33,220
about the IDs and so when it does each

00:34:30,190 --> 00:34:35,200
bit of work it can take that ID it can

00:34:33,220 --> 00:34:37,360
update the status for what's being done

00:34:35,200 --> 00:34:39,820
and the main process will always be able

00:34:37,360 --> 00:34:43,179
to see what the statuses are with those

00:34:39,820 --> 00:34:45,520
IDs so my client could keep requesting

00:34:43,179 --> 00:34:48,130
me when I make the first request to do

00:34:45,520 --> 00:34:51,130
work I get an ID bag and now I just

00:34:48,130 --> 00:34:52,340
query that ID and the main process can

00:34:51,130 --> 00:34:54,530
tell me you know it's

00:34:52,340 --> 00:34:57,110
no it's not ready yet okay now it's

00:34:54,530 --> 00:34:58,040
ready here's the results right and so

00:34:57,110 --> 00:35:00,530
this is really what I mean about

00:34:58,040 --> 00:35:03,350
separating the concerns your

00:35:00,530 --> 00:35:05,270
architecture will dictate how do I want

00:35:03,350 --> 00:35:06,890
that processing loop to kind of cue up

00:35:05,270 --> 00:35:09,080
information and maybe I have several

00:35:06,890 --> 00:35:11,720
processing loops maybe that work gets

00:35:09,080 --> 00:35:13,820
distributed in a better way but really

00:35:11,720 --> 00:35:16,640
what I want to do is hide those concerns

00:35:13,820 --> 00:35:18,020
from what the user is seeing right so

00:35:16,640 --> 00:35:19,700
that so that they can do that they can

00:35:18,020 --> 00:35:21,860
keep query and say what's happening and

00:35:19,700 --> 00:35:24,110
they're not going to overload the spark

00:35:21,860 --> 00:35:28,660
work that's being done does that does

00:35:24,110 --> 00:35:34,460
that kind of make sense okay cool cool

00:35:28,660 --> 00:35:36,890
any other questions can you tell us more

00:35:34,460 --> 00:35:41,030
about the ocean both source to them

00:35:36,890 --> 00:35:43,310
explain their process sure so what what

00:35:41,030 --> 00:35:45,800
you saw with the demonstration I did was

00:35:43,310 --> 00:35:49,850
me exercising the ocean coast sourced

00:35:45,800 --> 00:35:51,680
image tooling right so the what the

00:35:49,850 --> 00:35:54,440
tooling does is it first pulls the

00:35:51,680 --> 00:35:56,920
source code from your git repository and

00:35:54,440 --> 00:35:58,040
then it creates a container inside of

00:35:56,920 --> 00:36:00,530
OpenShift

00:35:58,040 --> 00:36:02,810
that will do the build process right and

00:36:00,530 --> 00:36:04,610
so that that's that log that I showed it

00:36:02,810 --> 00:36:07,550
was pulling the code and it was starting

00:36:04,610 --> 00:36:10,130
to build it right once it builds that

00:36:07,550 --> 00:36:14,420
code and it's successful it deploys the

00:36:10,130 --> 00:36:16,430
built container to OpenShift and then

00:36:14,420 --> 00:36:18,620
the ocean co tooling is actually inside

00:36:16,430 --> 00:36:22,280
your build container and what it will do

00:36:18,620 --> 00:36:24,320
is it says did you did you request to

00:36:22,280 --> 00:36:26,330
use a spark cluster that already exists

00:36:24,320 --> 00:36:28,730
could you could do that right that's one

00:36:26,330 --> 00:36:30,770
of the options I went by if you didn't

00:36:28,730 --> 00:36:32,990
request that then what it will do is it

00:36:30,770 --> 00:36:35,060
will spawn a spark cluster for you and

00:36:32,990 --> 00:36:36,710
you could specify the configuration for

00:36:35,060 --> 00:36:38,720
what you wanted to do and then your

00:36:36,710 --> 00:36:41,690
application will run and when your

00:36:38,720 --> 00:36:43,610
application exits the ocean Co tooling

00:36:41,690 --> 00:36:45,380
will kind of catch the exit on that and

00:36:43,610 --> 00:36:47,340
it will delete the cluster that goes

00:36:45,380 --> 00:36:49,350
with it right and then your application

00:36:47,340 --> 00:36:51,110
go away or something so that's kind of a

00:36:49,350 --> 00:36:54,420
general look at the steps that happen

00:36:51,110 --> 00:36:56,130
but to do that deployment and it

00:36:54,420 --> 00:36:59,340
associates you know like a service with

00:36:56,130 --> 00:37:01,140
it and exposes ports so it's the same

00:36:59,340 --> 00:37:02,670
behavior you would expect from the other

00:37:01,140 --> 00:37:04,830
source to image tooling whether you were

00:37:02,670 --> 00:37:10,970
using you know JavaScript or Java or

00:37:04,830 --> 00:37:10,970
Python or something like that okay

00:37:15,460 --> 00:37:20,500
so you described a mischenko off a bunch

00:37:18,369 --> 00:37:22,030
without mentioning like CSE B or

00:37:20,500 --> 00:37:24,220
continuous integration there Department

00:37:22,030 --> 00:37:26,710
anything like that can you describe your

00:37:24,220 --> 00:37:28,240
finger without mentioning CS CD or sort

00:37:26,710 --> 00:37:29,770
of but the way you describe it and I

00:37:28,240 --> 00:37:33,400
don't know that much about it is it's a

00:37:29,770 --> 00:37:35,230
very similar workflow if like and we do

00:37:33,400 --> 00:37:37,000
something similar in a Jenkins stack

00:37:35,230 --> 00:37:39,790
that uses help charts to deploy stuff to

00:37:37,000 --> 00:37:41,740
kubernetes cluster is what our like why

00:37:39,790 --> 00:37:46,200
didn't you work that into a more classic

00:37:41,740 --> 00:37:48,849
CD for Ramone well how is it better so

00:37:46,200 --> 00:37:50,109
the reason I didn't work it in is

00:37:48,849 --> 00:37:51,849
because I'm not really diving into the

00:37:50,109 --> 00:37:54,130
pipeline's features that exist with an

00:37:51,849 --> 00:37:55,750
open shift but that would what you've

00:37:54,130 --> 00:37:58,240
seen here is kind of the lowest level of

00:37:55,750 --> 00:37:59,740
application development the next level

00:37:58,240 --> 00:38:00,849
of application that was development that

00:37:59,740 --> 00:38:02,490
would happen if I were going to take

00:38:00,849 --> 00:38:06,220
this into a more production situation

00:38:02,490 --> 00:38:07,930
first I would put testing between you

00:38:06,220 --> 00:38:10,150
know when my my application gets checked

00:38:07,930 --> 00:38:12,460
in to get you know there could be a test

00:38:10,150 --> 00:38:14,140
running there and if it gets rejected

00:38:12,460 --> 00:38:16,030
then you know the commit doesn't get

00:38:14,140 --> 00:38:18,700
merged and it doesn't kick off the new

00:38:16,030 --> 00:38:20,829
build another way to look at this is I

00:38:18,700 --> 00:38:23,410
could use the pipeline functionality

00:38:20,829 --> 00:38:26,260
that exists with an open shift to create

00:38:23,410 --> 00:38:29,049
a pipeline that says first check the

00:38:26,260 --> 00:38:31,420
code out and build it here then run the

00:38:29,049 --> 00:38:33,579
tests here in open shift and then if all

00:38:31,420 --> 00:38:35,140
that works then deploy it and I could

00:38:33,579 --> 00:38:37,210
say deploy it to another project or

00:38:35,140 --> 00:38:38,530
something so part of the reason I didn't

00:38:37,210 --> 00:38:40,839
get into talking about it today is

00:38:38,530 --> 00:38:43,089
because those primitives start to exist

00:38:40,839 --> 00:38:45,040
at a higher level but once you've

00:38:43,089 --> 00:38:47,440
created your applications in this manner

00:38:45,040 --> 00:38:48,760
you know they become very easy to kind

00:38:47,440 --> 00:38:50,589
of mix and match and I think if you're

00:38:48,760 --> 00:38:53,319
using Jenkins with helm and kubernetes

00:38:50,589 --> 00:38:55,540
you're doing something very similar what

00:38:53,319 --> 00:38:59,020
what OpenShift kind of adds to that is

00:38:55,540 --> 00:39:01,299
this you know kind of UX around building

00:38:59,020 --> 00:39:03,040
the pipeline for you right so if you're

00:39:01,299 --> 00:39:05,230
not a Jenkins expert or even if you're

00:39:03,040 --> 00:39:07,869
not familiar with Jenkins the pipeline

00:39:05,230 --> 00:39:09,490
tooling allows you to specify those

00:39:07,869 --> 00:39:11,740
things using a language that's a little

00:39:09,490 --> 00:39:13,960
bit easier than diving into you know a

00:39:11,740 --> 00:39:15,240
Jenkins configuration and our Travis

00:39:13,960 --> 00:39:17,820
configuration or

00:39:15,240 --> 00:39:19,530
like that so really that would depend on

00:39:17,820 --> 00:39:21,330
what you're doing with your application

00:39:19,530 --> 00:39:23,070
design but it starts to exist at a

00:39:21,330 --> 00:39:25,860
higher level than just creating the

00:39:23,070 --> 00:39:27,420
applications and so the ocean cooling

00:39:25,860 --> 00:39:29,070
helps because when we're automating

00:39:27,420 --> 00:39:31,110
these things we don't need to automate

00:39:29,070 --> 00:39:32,490
the spark cluster creation we can just

00:39:31,110 --> 00:39:35,130
let the tooling take care of it for us

00:39:32,490 --> 00:39:37,740
so when it runs the tests it actually

00:39:35,130 --> 00:39:39,780
spawns a cluster dynamically runs the

00:39:37,740 --> 00:39:41,580
full integration tests and then can

00:39:39,780 --> 00:39:43,200
report on whether it's exceeded and then

00:39:41,580 --> 00:39:45,240
you know deletes the spark cluster after

00:39:43,200 --> 00:39:46,830
it's all done without having to kind of

00:39:45,240 --> 00:39:51,119
build it into my Jenkins file or

00:39:46,830 --> 00:39:54,660
something like that I think we're

00:39:51,119 --> 00:39:56,730
running low on time so if you got more

00:39:54,660 --> 00:40:00,680
questions come bug me afterwards yeah

00:39:56,730 --> 00:40:00,680

YouTube URL: https://www.youtube.com/watch?v=Z3Rj8aKqqg4


