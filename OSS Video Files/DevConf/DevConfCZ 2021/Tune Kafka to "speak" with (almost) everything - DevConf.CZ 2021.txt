Title: Tune Kafka to "speak" with (almost) everything - DevConf.CZ 2021
Publication date: 2021-03-14
Playlist: DevConfCZ 2021
Description: 
	Speaker: Hugo Guerrero

Apache Kafka is getting used as an event backbone in new organizations everyday. We would love to send every byte of data through the event bus. However, most of the time, connecting to simple third party applications and services becomes a headache that involves several lines of code and additional applications. As a result, connecting Kafka to services like Google Sheets, communication tools such as Slack or Telegram, or even the omnipresent Salesforce, is a challenge nobody wants to face. Wouldn't you like to have hundreds of connectors readily available out-of-the-box to solve this problem?

Due to these challenges, communities like Apache Camel are working on how to speed up development on key areas of the modern application, like integration. The Camel Kafka Connect project, from the Apache foundation, has enabled their vastly set of connectors to interact with Kafka Connect natively. So, developers can start sending and receiving data from Kafka to and from their preferred services and applications in no time without a single line of code.

In summary, during this session we will:

Introduce you to the Camel Kafka Connector sub-project from Apache Camel
Go over the list of connectors available as part of the project
Showcase a couple examples of integrations using the connectors
Share some guidelines on how to get started with the Camel Kafka Connectors 



Schedule: https://sched.co/gmMe
Captions: 
	00:00:01,599 --> 00:00:08,160
my name is hugo i'm um

00:00:04,080 --> 00:00:10,719
part of the redhead team i'm a mexican

00:00:08,160 --> 00:00:12,799
at massachusetts in the united states

00:00:10,719 --> 00:00:15,839
i'm based in the westford office

00:00:12,799 --> 00:00:16,720
and i'm a specialist in apis and

00:00:15,839 --> 00:00:19,840
messaging

00:00:16,720 --> 00:00:22,480
and also an open source advocate and

00:00:19,840 --> 00:00:23,439
when we used to travel uh a fan of

00:00:22,480 --> 00:00:26,240
traveling

00:00:23,439 --> 00:00:27,359
all around and then also groupman so i

00:00:26,240 --> 00:00:30,240
really

00:00:27,359 --> 00:00:31,119
like to try new food and street food and

00:00:30,240 --> 00:00:33,520
so on so

00:00:31,119 --> 00:00:34,480
i really miss being in a in the czech

00:00:33,520 --> 00:00:37,040
republic

00:00:34,480 --> 00:00:38,640
as sometimes we were able to taste

00:00:37,040 --> 00:00:40,160
really good food there

00:00:38,640 --> 00:00:42,879
so for today we're going to be talking a

00:00:40,160 --> 00:00:45,200
little bit more about kafka

00:00:42,879 --> 00:00:46,399
we will be introducing some challenges

00:00:45,200 --> 00:00:49,680
and and the

00:00:46,399 --> 00:00:51,360
um interconnection of systems and we

00:00:49,680 --> 00:00:52,879
will talk about the apache camera

00:00:51,360 --> 00:00:55,360
project for those that

00:00:52,879 --> 00:00:57,039
haven't really heard about that one for

00:00:55,360 --> 00:00:58,960
the integration it's a

00:00:57,039 --> 00:01:00,719
very interesting project and then we

00:00:58,960 --> 00:01:03,120
will be talking about one of the sub

00:01:00,719 --> 00:01:04,640
projects within the apache camel

00:01:03,120 --> 00:01:06,000
umbrella that is the camel kafka

00:01:04,640 --> 00:01:09,040
connector project

00:01:06,000 --> 00:01:09,840
and what is it offering to us uh at all

00:01:09,040 --> 00:01:12,799
so

00:01:09,840 --> 00:01:13,280
um i hope you liked the session and

00:01:12,799 --> 00:01:16,240
please

00:01:13,280 --> 00:01:17,360
remember you can um put some questions

00:01:16,240 --> 00:01:19,759
comments in the chat

00:01:17,360 --> 00:01:22,240
and we can follow up the conversation uh

00:01:19,759 --> 00:01:26,240
using this course uh as backup event

00:01:22,240 --> 00:01:28,400
uh chat okay so first thing uh let's

00:01:26,240 --> 00:01:30,400
talk about apache kafka so

00:01:28,400 --> 00:01:32,960
apache kafka is the cookie is the center

00:01:30,400 --> 00:01:35,280
of this uh of these chats and

00:01:32,960 --> 00:01:36,159
let's get a review of what is apache

00:01:35,280 --> 00:01:38,000
kafka

00:01:36,159 --> 00:01:39,439
so remember apache kafka was a project

00:01:38,000 --> 00:01:42,960
originally created

00:01:39,439 --> 00:01:45,119
around 2010 within linkedin and it was

00:01:42,960 --> 00:01:46,640
mainly focused on trying to track all

00:01:45,119 --> 00:01:47,600
the different web clicks that you do

00:01:46,640 --> 00:01:50,159
inside

00:01:47,600 --> 00:01:52,240
to give some analytics and and more

00:01:50,159 --> 00:01:54,479
information and insights on how the

00:01:52,240 --> 00:01:56,240
site was being used and for that they

00:01:54,479 --> 00:01:58,079
were trying to um

00:01:56,240 --> 00:02:00,479
get all this information coming from the

00:01:58,079 --> 00:02:03,119
web system and being able to easily

00:02:00,479 --> 00:02:04,719
move that information data around and

00:02:03,119 --> 00:02:06,799
being able to store them

00:02:04,719 --> 00:02:07,759
quickly because the amount of events

00:02:06,799 --> 00:02:10,720
that they have

00:02:07,759 --> 00:02:11,360
in the site was growing so much they

00:02:10,720 --> 00:02:12,959
were

00:02:11,360 --> 00:02:15,599
facing some limitations on the

00:02:12,959 --> 00:02:17,599
traditional messaging infrastructure to

00:02:15,599 --> 00:02:19,520
record all those events so that's why

00:02:17,599 --> 00:02:20,400
they decided to focus their efforts on

00:02:19,520 --> 00:02:22,160
creating these

00:02:20,400 --> 00:02:23,599
pollution subscribe messaging system

00:02:22,160 --> 00:02:26,319
that allows you to

00:02:23,599 --> 00:02:26,720
um implement a very efficient broker

00:02:26,319 --> 00:02:29,920
that

00:02:26,720 --> 00:02:32,720
can handle um thousands of of

00:02:29,920 --> 00:02:33,280
messages and events uh per second and it

00:02:32,720 --> 00:02:35,760
become

00:02:33,280 --> 00:02:37,360
and it has been uh growing into becoming

00:02:35,760 --> 00:02:39,840
a data streaming platform

00:02:37,360 --> 00:02:41,440
but in the core of the system it is a

00:02:39,840 --> 00:02:44,480
distributed commit law

00:02:41,440 --> 00:02:47,519
so it is um a set of uh

00:02:44,480 --> 00:02:49,200
of events coming into a log file that is

00:02:47,519 --> 00:02:52,160
just appending so it's very

00:02:49,200 --> 00:02:53,440
uh focus on on on having this high

00:02:52,160 --> 00:02:56,319
throughput nature

00:02:53,440 --> 00:02:57,920
and as as a result um it has a

00:02:56,319 --> 00:02:58,560
distribution in nature because it's

00:02:57,920 --> 00:03:00,640
doing

00:02:58,560 --> 00:03:01,920
sharding of the information and these

00:03:00,640 --> 00:03:04,480
files across

00:03:01,920 --> 00:03:05,120
a pool of broker nodes that allows you

00:03:04,480 --> 00:03:08,480
to have

00:03:05,120 --> 00:03:11,360
a very good throw put however

00:03:08,480 --> 00:03:13,599
most of the people that are starting to

00:03:11,360 --> 00:03:16,640
talk or getting into the kafka

00:03:13,599 --> 00:03:18,080
world they are focusing on on just

00:03:16,640 --> 00:03:21,200
implementing kafka by

00:03:18,080 --> 00:03:24,000
itself and thinking that perhaps that uh

00:03:21,200 --> 00:03:24,400
implementation it's it's enough to solve

00:03:24,000 --> 00:03:27,680
many

00:03:24,400 --> 00:03:28,799
different um problems and use cases and

00:03:27,680 --> 00:03:30,840
technical patterns

00:03:28,799 --> 00:03:32,000
from microservices or even driven

00:03:30,840 --> 00:03:35,519
architecture

00:03:32,000 --> 00:03:38,480
so let's uh think about this of a

00:03:35,519 --> 00:03:39,840
jedi journey right and one of the things

00:03:38,480 --> 00:03:42,080
that i tell people is when you're

00:03:39,840 --> 00:03:44,480
focusing on kafka it's not like you are

00:03:42,080 --> 00:03:45,440
in the you know the last jedi type of

00:03:44,480 --> 00:03:47,360
movie where

00:03:45,440 --> 00:03:49,680
there's only one jedi and that you only

00:03:47,360 --> 00:03:52,080
need kafka for that and that will solve

00:03:49,680 --> 00:03:53,040
you know all the universe problems you

00:03:52,080 --> 00:03:54,720
are not alone

00:03:53,040 --> 00:03:57,519
when we're talking about paji kafka it's

00:03:54,720 --> 00:03:59,439
more when we are talking about

00:03:57,519 --> 00:04:00,799
the first movie where there's a lot of

00:03:59,439 --> 00:04:04,239
jedi's around so

00:04:00,799 --> 00:04:07,439
don't think about of uh kafka by

00:04:04,239 --> 00:04:08,640
a goal itself but more like kafka as as

00:04:07,439 --> 00:04:11,120
a tool for that

00:04:08,640 --> 00:04:13,200
and this is because kafka it's it's a

00:04:11,120 --> 00:04:15,439
broader ecosystem right beyond

00:04:13,200 --> 00:04:17,280
the broker so you know there's the

00:04:15,439 --> 00:04:19,680
apache kafka project on the

00:04:17,280 --> 00:04:20,959
under the patch umbrella and that that

00:04:19,680 --> 00:04:24,320
does only include

00:04:20,959 --> 00:04:24,639
a certain features like the apis and and

00:04:24,320 --> 00:04:26,479
some

00:04:24,639 --> 00:04:28,080
projects like mirror maker and so on

00:04:26,479 --> 00:04:30,320
however you know

00:04:28,080 --> 00:04:32,000
that we need to be able to connect with

00:04:30,320 --> 00:04:33,440
other systems we need to be able to

00:04:32,000 --> 00:04:36,639
interact with other

00:04:33,440 --> 00:04:38,720
applications and even for

00:04:36,639 --> 00:04:39,919
for the linkedin use case for metrics

00:04:38,720 --> 00:04:44,000
and analytics you know

00:04:39,919 --> 00:04:46,000
you cannot access a kafka ui and being

00:04:44,000 --> 00:04:48,080
able to just uh review the uh

00:04:46,000 --> 00:04:49,680
the topics and so on it's not part of

00:04:48,080 --> 00:04:51,440
the original project you need to

00:04:49,680 --> 00:04:52,960
connect with other different parts and

00:04:51,440 --> 00:04:54,800
systems to be able to explore that

00:04:52,960 --> 00:04:57,199
information to showcase those

00:04:54,800 --> 00:04:57,840
data visualization visualizations or

00:04:57,199 --> 00:04:59,840
even

00:04:57,840 --> 00:05:00,880
able to uh provide more data into your

00:04:59,840 --> 00:05:03,600
system so

00:05:00,880 --> 00:05:04,639
let's talk about how we can um then be

00:05:03,600 --> 00:05:07,680
able to connect with

00:05:04,639 --> 00:05:09,280
with kafka and integrate our systems and

00:05:07,680 --> 00:05:11,440
one of those mechanisms

00:05:09,280 --> 00:05:13,039
within the sim calc ecosystem it's the

00:05:11,440 --> 00:05:15,360
kafka connect api

00:05:13,039 --> 00:05:17,440
so the idea of the cafe connect api is

00:05:15,360 --> 00:05:19,520
to have this wrapper around the consumer

00:05:17,440 --> 00:05:21,680
and the uh producer apis

00:05:19,520 --> 00:05:23,280
and it's this framework that allows you

00:05:21,680 --> 00:05:26,320
to easily

00:05:23,280 --> 00:05:27,680
uh compose uh transferring of of data

00:05:26,320 --> 00:05:30,960
between your kafka

00:05:27,680 --> 00:05:35,360
uh broker cluster and other data systems

00:05:30,960 --> 00:05:37,360
and for this it offers a series of

00:05:35,360 --> 00:05:39,360
mechanisms and components that

00:05:37,360 --> 00:05:41,360
facilitate like data conversion

00:05:39,360 --> 00:05:42,720
scaling and load balancing because you

00:05:41,360 --> 00:05:46,320
can create a

00:05:42,720 --> 00:05:47,759
a connect cluster that is able to handle

00:05:46,320 --> 00:05:50,160
the life cycle of your

00:05:47,759 --> 00:05:50,880
connectors or your connector plug-ins

00:05:50,160 --> 00:05:53,120
that's how

00:05:50,880 --> 00:05:55,440
it's called in the kafka connect world

00:05:53,120 --> 00:05:57,440
for those pieces of software that allows

00:05:55,440 --> 00:05:59,840
you to have specialized um

00:05:57,440 --> 00:06:01,759
connectivity with with systems and it

00:05:59,840 --> 00:06:03,600
defines a framework for having

00:06:01,759 --> 00:06:05,120
sync connectors and source connectors

00:06:03,600 --> 00:06:08,639
depending if you are moving

00:06:05,120 --> 00:06:11,680
your data from or off kafka

00:06:08,639 --> 00:06:13,360
and also remember that the apache

00:06:11,680 --> 00:06:15,360
project only includes

00:06:13,360 --> 00:06:17,360
two connectors the file sync and the

00:06:15,360 --> 00:06:19,520
file source that means that

00:06:17,360 --> 00:06:20,720
the ecosystem around more connectors

00:06:19,520 --> 00:06:22,400
needs to outgrow

00:06:20,720 --> 00:06:24,319
from the apache project and it's been

00:06:22,400 --> 00:06:27,280
provided by additional

00:06:24,319 --> 00:06:28,880
uh providers there's other companies and

00:06:27,280 --> 00:06:31,680
providers that have developed their own

00:06:28,880 --> 00:06:33,440
sort of their own additional plugins and

00:06:31,680 --> 00:06:36,080
you will see that their own

00:06:33,440 --> 00:06:36,479
databases have their own uh plugins or

00:06:36,080 --> 00:06:38,880
other

00:06:36,479 --> 00:06:39,600
capcom providers that have provided

00:06:38,880 --> 00:06:41,600
enterprise

00:06:39,600 --> 00:06:43,120
support they also have their uh their

00:06:41,600 --> 00:06:44,720
connectors so

00:06:43,120 --> 00:06:46,720
this is an ecosystem that's growing and

00:06:44,720 --> 00:06:49,440
growing and growing and let's

00:06:46,720 --> 00:06:51,759
reveal why uh uh kafka connect this one

00:06:49,440 --> 00:06:54,639
as i was mentioning uh before

00:06:51,759 --> 00:06:54,960
uh it's part of a of kafka itself right

00:06:54,639 --> 00:06:58,160
so

00:06:54,960 --> 00:07:00,319
you are talking about um it is that it's

00:06:58,160 --> 00:07:01,840
very well crafted within uh the

00:07:00,319 --> 00:07:03,520
ecosystem to

00:07:01,840 --> 00:07:05,039
match the capabilities use the same

00:07:03,520 --> 00:07:06,639
versions and so on

00:07:05,039 --> 00:07:08,960
and as i will mention if you are using

00:07:06,639 --> 00:07:12,080
the uh cluster nature of the

00:07:08,960 --> 00:07:14,800
connect if you are uh using the uh the

00:07:12,080 --> 00:07:15,280
distributed one uh it's scalable and

00:07:14,800 --> 00:07:18,400
issued

00:07:15,280 --> 00:07:20,880
by default you can also rely on

00:07:18,400 --> 00:07:21,680
storing your um tracking information for

00:07:20,880 --> 00:07:23,520
the offsets

00:07:21,680 --> 00:07:25,840
within kafka itself so it's all

00:07:23,520 --> 00:07:28,400
automatically able to outscale

00:07:25,840 --> 00:07:29,840
and also it offers some things like

00:07:28,400 --> 00:07:30,800
additional transformations so

00:07:29,840 --> 00:07:32,560
transformations

00:07:30,800 --> 00:07:34,160
when you are moving your data from one

00:07:32,560 --> 00:07:36,479
source to another remember

00:07:34,160 --> 00:07:37,840
most of the times you will want to have

00:07:36,479 --> 00:07:40,240
um

00:07:37,840 --> 00:07:41,919
some data management so you can you know

00:07:40,240 --> 00:07:44,960
remove fields or do

00:07:41,919 --> 00:07:45,360
some minor changes so this framework is

00:07:44,960 --> 00:07:48,160
not

00:07:45,360 --> 00:07:49,520
an integration tool but it offers um

00:07:48,160 --> 00:07:51,360
something like the signal message

00:07:49,520 --> 00:07:54,000
transformations that allows you to do

00:07:51,360 --> 00:07:54,800
some kind of these activities uh for

00:07:54,000 --> 00:07:57,520
easy of

00:07:54,800 --> 00:07:58,639
of use right and allows you to have this

00:07:57,520 --> 00:08:00,800
streaming integration

00:07:58,639 --> 00:08:02,319
batch integration so you will have your

00:08:00,800 --> 00:08:05,360
connectivity

00:08:02,319 --> 00:08:08,080
easy and configuration based but and

00:08:05,360 --> 00:08:09,440
you can also write your own um your own

00:08:08,080 --> 00:08:14,080
connectors

00:08:09,440 --> 00:08:16,319
so in this case um

00:08:14,080 --> 00:08:17,520
remember that when you're we're talking

00:08:16,319 --> 00:08:19,759
about this um

00:08:17,520 --> 00:08:21,840
this positioning on the on the first

00:08:19,759 --> 00:08:23,680
movies of the star wars trilogy

00:08:21,840 --> 00:08:25,280
when there's a lot of jedis already

00:08:23,680 --> 00:08:26,879
available

00:08:25,280 --> 00:08:28,960
sometimes you need to you need to

00:08:26,879 --> 00:08:31,039
remember that most of those jedis

00:08:28,960 --> 00:08:32,560
are coming from different uh type of

00:08:31,039 --> 00:08:33,519
species and they talk different

00:08:32,560 --> 00:08:35,519
languages

00:08:33,519 --> 00:08:37,360
and this is very similar to what we have

00:08:35,519 --> 00:08:39,760
with kafka right so

00:08:37,360 --> 00:08:40,399
we have kafka and we need to connect to

00:08:39,760 --> 00:08:42,399
very

00:08:40,399 --> 00:08:44,560
different type of systems from coming

00:08:42,399 --> 00:08:47,839
from slack to mongodb

00:08:44,560 --> 00:08:51,120
to uh passing through telegram or

00:08:47,839 --> 00:08:54,399
exporting those to elasticsearch so

00:08:51,120 --> 00:08:56,560
sometimes those uh systems are uh having

00:08:54,399 --> 00:08:57,519
uh protocols that are not included in in

00:08:56,560 --> 00:08:59,920
some of the uh

00:08:57,519 --> 00:09:01,360
of the connectors providers so you have

00:08:59,920 --> 00:09:03,600
to uh either

00:09:01,360 --> 00:09:04,399
ask the uh the system provider to give

00:09:03,600 --> 00:09:06,560
you an

00:09:04,399 --> 00:09:08,000
easy to consume api or you need to uh

00:09:06,560 --> 00:09:10,399
you know craft and and

00:09:08,000 --> 00:09:12,160
develop your own specific connector and

00:09:10,399 --> 00:09:14,000
sometimes that is uh very complicated

00:09:12,160 --> 00:09:15,600
it's like if you need to understand

00:09:14,000 --> 00:09:18,560
how to connect to each one of those

00:09:15,600 --> 00:09:19,600
services um being just the uh cafeteria

00:09:18,560 --> 00:09:21,600
so

00:09:19,600 --> 00:09:22,800
taking the same approach as uh as we

00:09:21,600 --> 00:09:25,760
were mentioning on the

00:09:22,800 --> 00:09:26,480
star wars trilogy why don't you benefit

00:09:25,760 --> 00:09:28,240
of you know

00:09:26,480 --> 00:09:29,760
some mechanisms that are already

00:09:28,240 --> 00:09:33,040
available for you

00:09:29,760 --> 00:09:35,200
like um in this case c3po who is

00:09:33,040 --> 00:09:37,120
very fluent in over more than six

00:09:35,200 --> 00:09:39,839
millions forms of communication

00:09:37,120 --> 00:09:40,399
and you are able to use and rely on

00:09:39,839 --> 00:09:42,080
these

00:09:40,399 --> 00:09:43,839
different type of technologies to be

00:09:42,080 --> 00:09:46,880
able to help you

00:09:43,839 --> 00:09:47,360
with that an integration between kafka

00:09:46,880 --> 00:09:50,720
and

00:09:47,360 --> 00:09:52,800
other systems so this is the approach to

00:09:50,720 --> 00:09:54,480
you know reuse some of the content that

00:09:52,800 --> 00:09:55,920
is already there that has been proved

00:09:54,480 --> 00:09:58,399
for a lot of time

00:09:55,920 --> 00:09:59,680
because projects that have been dealing

00:09:58,399 --> 00:10:01,839
with integration since

00:09:59,680 --> 00:10:03,120
since a while and then being able to

00:10:01,839 --> 00:10:05,519
tune your kafka

00:10:03,120 --> 00:10:06,800
ecosystem to be able to use this kind of

00:10:05,519 --> 00:10:09,839
components

00:10:06,800 --> 00:10:10,399
so let's uh get a little bit more into

00:10:09,839 --> 00:10:14,640
what it's

00:10:10,399 --> 00:10:16,480
a apache camera so remember apache camel

00:10:14,640 --> 00:10:18,399
it's another project under the apache

00:10:16,480 --> 00:10:21,120
foundation that helps us to

00:10:18,399 --> 00:10:22,880
deal with integrations so in a nutshell

00:10:21,120 --> 00:10:25,200
what is apache camel

00:10:22,880 --> 00:10:26,399
remember is this um open source project

00:10:25,200 --> 00:10:29,839
uh that is

00:10:26,399 --> 00:10:33,279
basically the z knife uh uh the army

00:10:29,839 --> 00:10:34,880
swiss army knife for a framework as for

00:10:33,279 --> 00:10:38,720
integration so

00:10:34,880 --> 00:10:39,600
it has a lot of components more than 350

00:10:38,720 --> 00:10:42,000
of them

00:10:39,600 --> 00:10:43,200
to be able to do data transformation to

00:10:42,000 --> 00:10:45,920
do protocols

00:10:43,200 --> 00:10:47,839
and connectivity as well as the

00:10:45,920 --> 00:10:48,640
implementation of enterprise integration

00:10:47,839 --> 00:10:52,000
patterns

00:10:48,640 --> 00:10:55,040
so you can model your um your

00:10:52,000 --> 00:10:56,399
connection flow or your data flow as a

00:10:55,040 --> 00:10:58,480
route

00:10:56,399 --> 00:11:00,320
and then you can do interesting things

00:10:58,480 --> 00:11:03,839
like being able to do

00:11:00,320 --> 00:11:04,880
um aggregation or splitting or content

00:11:03,839 --> 00:11:07,040
enrichment

00:11:04,880 --> 00:11:09,519
and so on and and this is spread that

00:11:07,040 --> 00:11:11,680
has been around for more than 10 years

00:11:09,519 --> 00:11:13,120
it's a very active project and the

00:11:11,680 --> 00:11:16,079
community is growing

00:11:13,120 --> 00:11:16,959
and and blossoming all the time they're

00:11:16,079 --> 00:11:19,600
um

00:11:16,959 --> 00:11:22,640
moving according to the new technology

00:11:19,600 --> 00:11:24,480
and and they are evolving all the time

00:11:22,640 --> 00:11:25,920
so if you want to take a look closer on

00:11:24,480 --> 00:11:29,360
how camera routes

00:11:25,920 --> 00:11:32,399
look you can see the following

00:11:29,360 --> 00:11:35,440
slide where you define using

00:11:32,399 --> 00:11:36,399
the camel dsls of the domain specific

00:11:35,440 --> 00:11:38,480
languages

00:11:36,399 --> 00:11:40,640
to define how your flow or your

00:11:38,480 --> 00:11:43,920
integration is going to be

00:11:40,640 --> 00:11:45,920
described okay and

00:11:43,920 --> 00:11:47,040
uh sorry about that i i was uh

00:11:45,920 --> 00:11:49,600
mentioning the uh

00:11:47,040 --> 00:11:50,959
how how the camera routes look like so

00:11:49,600 --> 00:11:55,040
it's um

00:11:50,959 --> 00:11:58,320
it's the um the um

00:11:55,040 --> 00:12:01,920
way to check this as um

00:11:58,320 --> 00:12:04,079
as a dsl where you can put like

00:12:01,920 --> 00:12:06,000
from where is your data coming and where

00:12:04,079 --> 00:12:08,240
is the end point that you are uh

00:12:06,000 --> 00:12:10,399
sending your information to so we have

00:12:08,240 --> 00:12:11,360
different ways to express those dsls

00:12:10,399 --> 00:12:14,399
they have the

00:12:11,360 --> 00:12:15,440
java version that it's based on on on

00:12:14,399 --> 00:12:18,639
java language

00:12:15,440 --> 00:12:19,360
where you have a fluent dsl that helps

00:12:18,639 --> 00:12:21,839
you to

00:12:19,360 --> 00:12:24,399
map this kind of approach or you have

00:12:21,839 --> 00:12:27,600
also the xml dsl where you can

00:12:24,399 --> 00:12:30,959
define a spring uh kind of

00:12:27,600 --> 00:12:31,600
of of objects in in xml and then define

00:12:30,959 --> 00:12:34,480
again

00:12:31,600 --> 00:12:36,399
uh using tags uh from where your data is

00:12:34,480 --> 00:12:39,519
coming in this case for example

00:12:36,399 --> 00:12:42,160
it's coming from uh from a file

00:12:39,519 --> 00:12:43,519
that it's uh in that it's stored in the

00:12:42,160 --> 00:12:46,000
data inbox uh

00:12:43,519 --> 00:12:47,120
uh folder and then it's gonna be sent

00:12:46,000 --> 00:12:49,920
into a gms

00:12:47,120 --> 00:12:50,560
queue that it's called order so you can

00:12:49,920 --> 00:12:54,160
see there's

00:12:50,560 --> 00:12:56,720
using this uri pattern where you define

00:12:54,160 --> 00:12:58,959
what is the component that you're gonna

00:12:56,720 --> 00:13:00,000
use and where is the destination for

00:12:58,959 --> 00:13:02,160
that

00:13:00,000 --> 00:13:03,680
and camel the architect the camera

00:13:02,160 --> 00:13:04,880
architecture it's a little bit more

00:13:03,680 --> 00:13:07,680
complex

00:13:04,880 --> 00:13:08,079
so if you want to take a look it's like

00:13:07,680 --> 00:13:10,320
if you

00:13:08,079 --> 00:13:11,600
really want to get into the insides of

00:13:10,320 --> 00:13:15,120
c3po there's

00:13:11,600 --> 00:13:18,160
um a lot of details inside this

00:13:15,120 --> 00:13:19,760
and uh there's the implementation of the

00:13:18,160 --> 00:13:21,519
interracial iteration patterns the

00:13:19,760 --> 00:13:22,000
management of the context between the

00:13:21,519 --> 00:13:24,480
root

00:13:22,000 --> 00:13:25,519
the routes and so on but the interesting

00:13:24,480 --> 00:13:27,279
part here

00:13:25,519 --> 00:13:29,519
is the concept of the components that

00:13:27,279 --> 00:13:30,720
allows you to do this integration with

00:13:29,519 --> 00:13:32,560
other systems

00:13:30,720 --> 00:13:34,240
and then being able to you know transfer

00:13:32,560 --> 00:13:38,079
and pass the information

00:13:34,240 --> 00:13:39,360
along your um your flows so

00:13:38,079 --> 00:13:41,199
this is the really the really

00:13:39,360 --> 00:13:44,000
interesting part that you really want to

00:13:41,199 --> 00:13:45,360
um to focus on and on those components

00:13:44,000 --> 00:13:46,720
that are already available as part of

00:13:45,360 --> 00:13:50,399
the batch camel

00:13:46,720 --> 00:13:52,160
to be able to simplify your integration

00:13:50,399 --> 00:13:54,320
and at the beginning we were talking

00:13:52,160 --> 00:13:56,399
about the apache camel

00:13:54,320 --> 00:13:57,760
umbrella project and the soup projects

00:13:56,399 --> 00:14:00,480
that are available

00:13:57,760 --> 00:14:01,519
so if you already have are familiar with

00:14:00,480 --> 00:14:03,440
with camel

00:14:01,519 --> 00:14:05,760
you will have seen that they have been

00:14:03,440 --> 00:14:07,920
releasing now

00:14:05,760 --> 00:14:09,519
different run times for each one of the

00:14:07,920 --> 00:14:11,920
deployment options so you

00:14:09,519 --> 00:14:13,199
can run in the traditional carafe mode

00:14:11,920 --> 00:14:15,839
using the camelcaraf

00:14:13,199 --> 00:14:16,880
project or you can deploy on top of

00:14:15,839 --> 00:14:19,360
spring boot

00:14:16,880 --> 00:14:20,959
and if you uh check the previous session

00:14:19,360 --> 00:14:24,079
with uh

00:14:20,959 --> 00:14:24,880
with uh with uh quarkus that's another

00:14:24,079 --> 00:14:27,680
option to be

00:14:24,880 --> 00:14:28,240
able to deploy uh camel on top of that

00:14:27,680 --> 00:14:30,880
uh

00:14:28,240 --> 00:14:31,760
container native and cloud native

00:14:30,880 --> 00:14:34,720
runtime

00:14:31,760 --> 00:14:35,680
so you have the also the camel um core

00:14:34,720 --> 00:14:39,040
project where

00:14:35,680 --> 00:14:41,839
all these um um internals of the uh

00:14:39,040 --> 00:14:43,440
of the framework are being developed and

00:14:41,839 --> 00:14:45,920
if you're thinking about

00:14:43,440 --> 00:14:47,360
deploying camel on a server-less uh

00:14:45,920 --> 00:14:48,800
environment running on top of pure

00:14:47,360 --> 00:14:51,120
netize camel k

00:14:48,800 --> 00:14:52,880
is the way to go but in this moment

00:14:51,120 --> 00:14:54,000
we're going to be talking more about the

00:14:52,880 --> 00:14:57,360
camel

00:14:54,000 --> 00:15:00,560
kafka connector sub project where

00:14:57,360 --> 00:15:03,600
we are focusing on dealing with uh camel

00:15:00,560 --> 00:15:06,800
and well as kafka so let's uh

00:15:03,600 --> 00:15:09,040
get a little deep a diver on um on the

00:15:06,800 --> 00:15:12,079
camel capture connector project

00:15:09,040 --> 00:15:14,000
and in this case the idea that the camel

00:15:12,079 --> 00:15:14,880
contributors had with uh with this

00:15:14,000 --> 00:15:18,399
project

00:15:14,880 --> 00:15:19,920
was to tune their components to be able

00:15:18,399 --> 00:15:23,360
to play well together

00:15:19,920 --> 00:15:26,959
with kafka so it's so the idea of having

00:15:23,360 --> 00:15:29,120
this um this part of of the time when

00:15:26,959 --> 00:15:33,440
you need you know c3p to be able to

00:15:29,120 --> 00:15:36,160
easily uh talk and uh and being able to

00:15:33,440 --> 00:15:38,000
you know mention and being able to speak

00:15:36,160 --> 00:15:39,279
what the dark side is saying so you need

00:15:38,000 --> 00:15:42,800
to tune also

00:15:39,279 --> 00:15:45,279
um c3po to be able to um ingest and

00:15:42,800 --> 00:15:47,199
and being able to uh to mention what uh

00:15:45,279 --> 00:15:49,839
what the dark side is saying so

00:15:47,199 --> 00:15:51,360
this is very similar or what the uh the

00:15:49,839 --> 00:15:53,920
the camel team theme

00:15:51,360 --> 00:15:55,360
did for uh making the apache camo

00:15:53,920 --> 00:15:57,920
connectors being able to play

00:15:55,360 --> 00:15:58,800
nicely with kafka and this is why they

00:15:57,920 --> 00:16:01,440
create these

00:15:58,800 --> 00:16:03,040
projects so the camo kafka connector

00:16:01,440 --> 00:16:05,120
project it's a pool of

00:16:03,040 --> 00:16:07,120
kafka connectors that are built on top

00:16:05,120 --> 00:16:10,240
of the apache camel

00:16:07,120 --> 00:16:13,440
components the idea is to

00:16:10,240 --> 00:16:15,120
reuse in a very simple way all the

00:16:13,440 --> 00:16:17,040
different camel components that are

00:16:15,120 --> 00:16:20,720
already available the three more

00:16:17,040 --> 00:16:23,440
three the more than 350 connectors

00:16:20,720 --> 00:16:25,120
that are part of the project and being

00:16:23,440 --> 00:16:28,320
able to just

00:16:25,120 --> 00:16:32,000
add these uh logic and and this uh

00:16:28,320 --> 00:16:35,279
tiny integration to make them

00:16:32,000 --> 00:16:36,480
run as if they were native uh connector

00:16:35,279 --> 00:16:40,320
plugins for

00:16:36,480 --> 00:16:43,279
kafka connect and this uh was

00:16:40,320 --> 00:16:44,240
very successful because now we have uh

00:16:43,279 --> 00:16:46,880
connectors

00:16:44,240 --> 00:16:47,920
that are coming from the camel

00:16:46,880 --> 00:16:50,000
components

00:16:47,920 --> 00:16:51,759
and all most of them are already

00:16:50,000 --> 00:16:52,959
available so there's a connector list

00:16:51,759 --> 00:16:54,639
that you can check with all the

00:16:52,959 --> 00:16:56,720
different connectors available

00:16:54,639 --> 00:16:57,920
and they are being generated every time

00:16:56,720 --> 00:17:00,399
there's a new uh

00:16:57,920 --> 00:17:01,600
a new version of the camel project look

00:17:00,399 --> 00:17:04,880
so you get all those

00:17:01,600 --> 00:17:07,280
connectors available um and easy to use

00:17:04,880 --> 00:17:09,360
and it's uh live as we mentioned

00:17:07,280 --> 00:17:13,199
subproject from the apache camels are

00:17:09,360 --> 00:17:16,400
really uh integrated so

00:17:13,199 --> 00:17:19,600
if we think about why do we use

00:17:16,400 --> 00:17:20,000
the camel kafka connector so most of the

00:17:19,600 --> 00:17:22,240
times

00:17:20,000 --> 00:17:24,559
i was saying you want to have your kafka

00:17:22,240 --> 00:17:26,480
been able to talk to other systems

00:17:24,559 --> 00:17:28,160
either to get information from those

00:17:26,480 --> 00:17:29,360
systems or being able to push that

00:17:28,160 --> 00:17:31,360
information

00:17:29,360 --> 00:17:32,880
and being able to do a little bit more

00:17:31,360 --> 00:17:34,480
transformation so remember there's two

00:17:32,880 --> 00:17:36,480
ways to do the integration

00:17:34,480 --> 00:17:38,240
with your system you can use the

00:17:36,480 --> 00:17:38,960
traditional camera approach where you

00:17:38,240 --> 00:17:42,640
need to

00:17:38,960 --> 00:17:45,200
you know deal with the um with the uh

00:17:42,640 --> 00:17:47,280
with the uh company that receives

00:17:45,200 --> 00:17:48,240
information as it's a consumer or

00:17:47,280 --> 00:17:50,000
producer

00:17:48,240 --> 00:17:51,679
and you can do you know heavyweight

00:17:50,000 --> 00:17:55,039
transformations or you can use

00:17:51,679 --> 00:17:56,480
this new approach to uh have a simple no

00:17:55,039 --> 00:17:58,799
code or low code uh

00:17:56,480 --> 00:18:00,720
type of configuration using the

00:17:58,799 --> 00:18:04,160
internals of kafka connect

00:18:00,720 --> 00:18:06,160
and it it have three like use cases here

00:18:04,160 --> 00:18:08,000
you have consolidated events that you

00:18:06,160 --> 00:18:08,559
are taking from a instance and

00:18:08,000 --> 00:18:10,320
then

00:18:08,559 --> 00:18:12,559
you have several instances and you want

00:18:10,320 --> 00:18:15,200
to consolidate information you can use

00:18:12,559 --> 00:18:16,799
um the scene so you can send

00:18:15,200 --> 00:18:17,919
information there for analytics or

00:18:16,799 --> 00:18:20,640
reporting

00:18:17,919 --> 00:18:23,200
um you can also use the elasticsearch

00:18:20,640 --> 00:18:26,000
sync that it's a component so you can

00:18:23,200 --> 00:18:27,600
use the uh the elasticsearch rest api to

00:18:26,000 --> 00:18:28,640
send information from your kafka

00:18:27,600 --> 00:18:31,200
clusters

00:18:28,640 --> 00:18:33,679
and also for example if you are getting

00:18:31,200 --> 00:18:35,360
uh logs that is a use case that

00:18:33,679 --> 00:18:37,360
people like for example in telefonica

00:18:35,360 --> 00:18:39,679
they are having where

00:18:37,360 --> 00:18:40,720
they are taking information coming from

00:18:39,679 --> 00:18:42,080
their logs

00:18:40,720 --> 00:18:44,320
in the kubernetes environment and

00:18:42,080 --> 00:18:46,320
they're using um the

00:18:44,320 --> 00:18:48,400
the connectors to be able to send

00:18:46,320 --> 00:18:50,640
information as an

00:18:48,400 --> 00:18:52,000
integration lock and then being able to

00:18:50,640 --> 00:18:54,720
then um produce

00:18:52,000 --> 00:18:55,760
a process and do a streaming analytic on

00:18:54,720 --> 00:18:57,840
those locks

00:18:55,760 --> 00:18:59,039
and there's a ton of connectors ready

00:18:57,840 --> 00:19:01,520
available right that

00:18:59,039 --> 00:19:03,120
allows you not only to connect to um

00:19:01,520 --> 00:19:03,679
elasticsearch but also being able to

00:19:03,120 --> 00:19:06,240
connect to

00:19:03,679 --> 00:19:07,600
things like slack so no need to write

00:19:06,240 --> 00:19:10,240
python code to

00:19:07,600 --> 00:19:11,679
connect to slack or any other client you

00:19:10,240 --> 00:19:13,360
can reuse the connector and just

00:19:11,679 --> 00:19:14,960
configure your tokens and then being

00:19:13,360 --> 00:19:17,200
able to send your information

00:19:14,960 --> 00:19:18,880
into slack or for example getting data

00:19:17,200 --> 00:19:21,440
from google sheets so being able to send

00:19:18,880 --> 00:19:24,400
information there

00:19:21,440 --> 00:19:26,880
and a little bit of detail on the code

00:19:24,400 --> 00:19:29,039
configuration so remember this is

00:19:26,880 --> 00:19:30,799
this is pure uh configuration there's no

00:19:29,039 --> 00:19:34,640
code here no need to write

00:19:30,799 --> 00:19:37,200
the java dsl or the xml the xml dsl

00:19:34,640 --> 00:19:38,000
this is using the camera the the kafka

00:19:37,200 --> 00:19:41,039
connect approach

00:19:38,000 --> 00:19:42,000
where you just uh configure uh what is

00:19:41,039 --> 00:19:43,679
your class

00:19:42,000 --> 00:19:46,080
and the different parameters that your

00:19:43,679 --> 00:19:48,720
class needs to for the configuration

00:19:46,080 --> 00:19:49,919
so in the first uh example we have

00:19:48,720 --> 00:19:51,679
amazon s3

00:19:49,919 --> 00:19:53,919
then sending information using the

00:19:51,679 --> 00:19:56,320
configuration you just need to

00:19:53,919 --> 00:19:57,919
pass the information on your secret and

00:19:56,320 --> 00:19:59,840
and your access token

00:19:57,919 --> 00:20:02,720
and and what is the region that you are

00:19:59,840 --> 00:20:04,880
being um acquiring that s3

00:20:02,720 --> 00:20:07,280
bucket and if you are sending from kafka

00:20:04,880 --> 00:20:08,480
to amazon sqs for example is the exact

00:20:07,280 --> 00:20:10,640
same thing you need to

00:20:08,480 --> 00:20:12,480
tell the component where your where is

00:20:10,640 --> 00:20:14,080
the topic that you're listening to

00:20:12,480 --> 00:20:15,679
and then where is the path with the

00:20:14,080 --> 00:20:17,120
information on on how you are going to

00:20:15,679 --> 00:20:21,280
be accessing the

00:20:17,120 --> 00:20:24,080
sqs um uh tears right

00:20:21,280 --> 00:20:24,880
and if you want to get started in your

00:20:24,080 --> 00:20:27,760
journey

00:20:24,880 --> 00:20:28,480
there's uh a very simple steps you can

00:20:27,760 --> 00:20:30,640
follow

00:20:28,480 --> 00:20:32,159
to get uh to get started with this uh

00:20:30,640 --> 00:20:35,039
type of company so

00:20:32,159 --> 00:20:35,520
um the first thing is yeah you can go to

00:20:35,039 --> 00:20:39,200
the uh

00:20:35,520 --> 00:20:41,840
to the website and on camel uh

00:20:39,200 --> 00:20:44,000
apache.org and they've been able to

00:20:41,840 --> 00:20:46,080
navigate into the uh

00:20:44,000 --> 00:20:47,600
list of supported connectors where you

00:20:46,080 --> 00:20:50,400
can find information on

00:20:47,600 --> 00:20:51,360
which of those connectors are are

00:20:50,400 --> 00:20:53,440
available as

00:20:51,360 --> 00:20:55,600
sync connectors or also as source

00:20:53,440 --> 00:20:56,159
connectors so not all the different not

00:20:55,600 --> 00:20:58,080
all the

00:20:56,159 --> 00:20:59,440
all the different components are

00:20:58,080 --> 00:21:02,159
available as both

00:20:59,440 --> 00:21:03,120
sometimes our sums are uh seeing some

00:21:02,159 --> 00:21:05,440
are source

00:21:03,120 --> 00:21:06,159
or sometimes they they do us work as

00:21:05,440 --> 00:21:08,080
both

00:21:06,159 --> 00:21:09,679
and then you can find the download link

00:21:08,080 --> 00:21:11,440
so you can uh download the

00:21:09,679 --> 00:21:13,360
components and then being able to create

00:21:11,440 --> 00:21:16,400
your um

00:21:13,360 --> 00:21:17,200
them as as part of your um connect

00:21:16,400 --> 00:21:18,960
plugins

00:21:17,200 --> 00:21:20,559
folder or you can create your container

00:21:18,960 --> 00:21:21,280
images if you're running for example

00:21:20,559 --> 00:21:24,400
using

00:21:21,280 --> 00:21:27,520
um streamsieve and

00:21:24,400 --> 00:21:28,400
now that you have downloaded the unsaved

00:21:27,520 --> 00:21:31,760
the file

00:21:28,400 --> 00:21:34,559
you can then um take that um that

00:21:31,760 --> 00:21:36,400
set of uh of jars put in your plugins

00:21:34,559 --> 00:21:37,280
directory and then you can configure the

00:21:36,400 --> 00:21:39,120
connector

00:21:37,280 --> 00:21:40,400
so in this example that you're seeing

00:21:39,120 --> 00:21:43,280
here if you are using

00:21:40,400 --> 00:21:43,919
uh just uh traditional deployment on of

00:21:43,280 --> 00:21:46,799
capcom

00:21:43,919 --> 00:21:47,919
on a virtual machine or a server and

00:21:46,799 --> 00:21:50,480
you're using the uh

00:21:47,919 --> 00:21:51,280
standalone mode for kafka connect you

00:21:50,480 --> 00:21:53,840
can just

00:21:51,280 --> 00:21:54,400
add the properties on your properties

00:21:53,840 --> 00:21:56,559
file

00:21:54,400 --> 00:21:57,679
and then start that process with that

00:21:56,559 --> 00:22:00,320
configuration

00:21:57,679 --> 00:22:02,559
however most of us will be using the

00:22:00,320 --> 00:22:04,880
container-based image for example

00:22:02,559 --> 00:22:06,720
and this is a an option that is very

00:22:04,880 --> 00:22:08,960
common for when we're used using

00:22:06,720 --> 00:22:09,919
uh streamsy for example to deploy on

00:22:08,960 --> 00:22:11,760
kubernetes

00:22:09,919 --> 00:22:13,600
or if you're just using the docker

00:22:11,760 --> 00:22:16,559
compose um

00:22:13,600 --> 00:22:18,559
configuration you take a base image and

00:22:16,559 --> 00:22:20,960
then you can add the plugins

00:22:18,559 --> 00:22:23,679
either by creating your own images or

00:22:20,960 --> 00:22:26,960
just configuring your docker compose

00:22:23,679 --> 00:22:29,679
and then you can configure this

00:22:26,960 --> 00:22:30,240
new container and in the case of for

00:22:29,679 --> 00:22:33,679
example

00:22:30,240 --> 00:22:37,440
um stream c you can

00:22:33,679 --> 00:22:40,640
either use the rest endpoint so you can

00:22:37,440 --> 00:22:43,600
send that information directly as the um

00:22:40,640 --> 00:22:45,760
as the endpoint or you can uh also

00:22:43,600 --> 00:22:47,039
define that as a custom resource because

00:22:45,760 --> 00:22:49,440
uh stream c uses

00:22:47,039 --> 00:22:50,240
custom resources to be able to define

00:22:49,440 --> 00:22:52,880
the different

00:22:50,240 --> 00:22:54,000
um configuration of your kafka ecosystem

00:22:52,880 --> 00:22:56,640
so you can define

00:22:54,000 --> 00:22:57,679
a kafka node or you can define a kafka

00:22:56,640 --> 00:22:59,360
connect cluster

00:22:57,679 --> 00:23:01,600
and also the kafka connect connectors

00:22:59,360 --> 00:23:06,000
but if not then you can use the

00:23:01,600 --> 00:23:09,039
the rest api for that and for that

00:23:06,000 --> 00:23:10,000
let's uh let's see if we can do um this

00:23:09,039 --> 00:23:12,880
um

00:23:10,000 --> 00:23:13,440
in a few minutes and show you the power

00:23:12,880 --> 00:23:17,600
of the

00:23:13,440 --> 00:23:17,600
camel connectors so

00:23:18,320 --> 00:23:27,120
please confirm me that you are able to

00:23:22,640 --> 00:23:30,080
see the my screen

00:23:27,120 --> 00:23:30,080
and my demo

00:23:30,480 --> 00:23:37,200
i can see only the

00:23:34,159 --> 00:23:41,679
presentation yep yep all right

00:23:37,200 --> 00:23:41,679
now no it's good

00:23:41,760 --> 00:23:46,640
okay let's increase the font

00:23:48,840 --> 00:23:51,840
okay

00:23:56,720 --> 00:23:59,919
can you increase it a little bit more

00:23:58,799 --> 00:24:04,320
please

00:23:59,919 --> 00:24:08,000
yeah sure let's uh do a couple of more

00:24:04,320 --> 00:24:08,000
that should be better thank you

00:24:09,919 --> 00:24:19,760
all right so first i'm gonna

00:24:14,159 --> 00:24:24,159
clone my ripple

00:24:19,760 --> 00:24:24,159
and oh my god

00:24:30,960 --> 00:24:34,080
so i prepared this example to show you

00:24:33,279 --> 00:24:37,120
how to

00:24:34,080 --> 00:24:40,320
do a simple um

00:24:37,120 --> 00:24:43,200
trigger input um a simple

00:24:40,320 --> 00:24:45,200
um trigger implementation of a timer

00:24:43,200 --> 00:24:46,400
that will send some information into our

00:24:45,200 --> 00:24:50,720
kafka topic

00:24:46,400 --> 00:24:54,559
so for this i have this example

00:24:50,720 --> 00:24:56,960
where i have my camel cafe connectors

00:24:54,559 --> 00:24:58,640
and here i do have a very simple

00:24:56,960 --> 00:24:59,279
configuration first i have a docker

00:24:58,640 --> 00:25:02,880
compose

00:24:59,279 --> 00:25:05,679
file that will start a local

00:25:02,880 --> 00:25:07,440
running version of a kafka cluster so if

00:25:05,679 --> 00:25:08,720
you want to take a look at the docker

00:25:07,440 --> 00:25:11,679
compose

00:25:08,720 --> 00:25:12,400
it's basically defining the zookeeper a

00:25:11,679 --> 00:25:15,679
broker

00:25:12,400 --> 00:25:18,960
using the um examples from

00:25:15,679 --> 00:25:21,919
uh from the um

00:25:18,960 --> 00:25:23,279
platform it's using schema registry and

00:25:21,919 --> 00:25:26,480
i'm defining here

00:25:23,279 --> 00:25:29,919
a connect image that is using the

00:25:26,480 --> 00:25:32,240
graphic connect data gen example from um

00:25:29,919 --> 00:25:33,600
from confluent and the interesting thing

00:25:32,240 --> 00:25:36,640
here is that i'm

00:25:33,600 --> 00:25:39,919
adding here the plugins

00:25:36,640 --> 00:25:42,320
uh directory so i will be um mounting

00:25:39,919 --> 00:25:43,039
this as a file system in the same images

00:25:42,320 --> 00:25:46,000
so

00:25:43,039 --> 00:25:47,279
um when kafka connect starts it will

00:25:46,000 --> 00:25:49,039
query that information

00:25:47,279 --> 00:25:51,120
and then retrieve the information from

00:25:49,039 --> 00:25:53,840
the uh from the plugins

00:25:51,120 --> 00:25:54,880
and will automatically load my uh my

00:25:53,840 --> 00:25:58,240
camel plugins

00:25:54,880 --> 00:25:58,240
from from that place

00:25:58,400 --> 00:26:04,559
so and then the thing we need to do it's

00:26:01,039 --> 00:26:04,559
uh going to the plugins

00:26:06,240 --> 00:26:12,080
directory and currently i don't have

00:26:08,240 --> 00:26:12,080
anything so what we can do is then

00:26:13,200 --> 00:26:21,760
go into the uh into the page

00:26:17,600 --> 00:26:24,080
easily so it's uh camel apache.org

00:26:21,760 --> 00:26:25,120
and as i say was mentioning you you go

00:26:24,080 --> 00:26:27,679
to the uh

00:26:25,120 --> 00:26:29,039
documentation and you can find the

00:26:27,679 --> 00:26:31,520
different projects here

00:26:29,039 --> 00:26:33,520
core camel k what we're interested in

00:26:31,520 --> 00:26:37,120
see the camel kafka connector

00:26:33,520 --> 00:26:37,520
so we go to the documentation and you

00:26:37,120 --> 00:26:40,320
can

00:26:37,520 --> 00:26:41,919
check the connectors list here and well

00:26:40,320 --> 00:26:43,919
a little bit more documentation on how

00:26:41,919 --> 00:26:46,000
to get started

00:26:43,919 --> 00:26:47,200
and here's all the different oh my god

00:26:46,000 --> 00:26:49,840
this is a little bit

00:26:47,200 --> 00:26:49,840
odd so

00:26:50,320 --> 00:26:56,960
well something happened there right

00:26:54,000 --> 00:26:58,240
um but yeah it should be a table with uh

00:26:56,960 --> 00:27:00,080
with the information it looks like

00:26:58,240 --> 00:27:02,720
somehow the ui has been

00:27:00,080 --> 00:27:04,840
messed around let's see if we can find

00:27:02,720 --> 00:27:09,760
our connector

00:27:04,840 --> 00:27:09,760
um we're looking for the timer connector

00:27:13,360 --> 00:27:21,840
oh never mind i i think i can um

00:27:17,039 --> 00:27:25,600
i can download a pre downloaded version

00:27:21,840 --> 00:27:25,600
because the timer is

00:27:28,559 --> 00:27:31,679
okay you know the demo gods are playing

00:27:30,480 --> 00:27:34,960
out against me

00:27:31,679 --> 00:27:49,840
so let's see we'll

00:27:34,960 --> 00:27:49,840
have the camel

00:27:50,640 --> 00:27:52,960
okay

00:27:55,919 --> 00:28:00,320
i sent you a link on the chat if that

00:27:58,480 --> 00:28:03,039
helps

00:28:00,320 --> 00:28:04,320
ah okay i don't worry i have an example

00:28:03,039 --> 00:28:07,679
here so i'm gonna

00:28:04,320 --> 00:28:10,000
just uh copy

00:28:07,679 --> 00:28:11,679
my uh into my plugins directory so i

00:28:10,000 --> 00:28:15,039
have this

00:28:11,679 --> 00:28:19,360
and then i go to my

00:28:15,039 --> 00:28:22,159
project where it's uh samples connectors

00:28:19,360 --> 00:28:23,919
so i'm just gonna paste here my my

00:28:22,159 --> 00:28:26,000
connector and as i once mentioned

00:28:23,919 --> 00:28:28,480
after you download the zip file that uh

00:28:26,000 --> 00:28:31,360
that octavio has shared with the uh

00:28:28,480 --> 00:28:32,159
with with the with the chat there's a

00:28:31,360 --> 00:28:34,559
different

00:28:32,159 --> 00:28:36,960
jars that you can get from uh from that

00:28:34,559 --> 00:28:38,720
jar and those are the implementation

00:28:36,960 --> 00:28:40,240
jars that you need require from from

00:28:38,720 --> 00:28:42,240
camel they are slim down so you don't

00:28:40,240 --> 00:28:45,120
need to download everything

00:28:42,240 --> 00:28:47,919
and they will help us for this so now

00:28:45,120 --> 00:28:51,600
that we have downloaded the um

00:28:47,919 --> 00:28:54,720
the uh file and then we are able to

00:28:51,600 --> 00:28:56,240
uh start our our environment so we're

00:28:54,720 --> 00:28:59,279
gonna do

00:28:56,240 --> 00:29:02,880
get back and then we can do

00:28:59,279 --> 00:29:02,880
docker compose

00:29:03,520 --> 00:29:10,399
close up let's for the recreate

00:29:08,640 --> 00:29:13,600
and this will start the docker compose

00:29:10,399 --> 00:29:17,600
file that i showed you

00:29:13,600 --> 00:29:17,600
it's already in use so okay

00:29:24,840 --> 00:29:27,840
let's

00:29:37,679 --> 00:29:58,159
okay so my content is already running

00:29:55,760 --> 00:29:58,159
okay

00:30:00,240 --> 00:30:05,760
let's try again now it's creating

00:30:04,080 --> 00:30:07,520
as i mentioned it's just running one

00:30:05,760 --> 00:30:08,080
note for zookeeper one node for the

00:30:07,520 --> 00:30:10,080
broker

00:30:08,080 --> 00:30:11,520
uh the schema ratio and the connect uh

00:30:10,080 --> 00:30:13,120
the connect cluster

00:30:11,520 --> 00:30:15,840
so now that it's running we can check

00:30:13,120 --> 00:30:19,279
that everything it's uh it's

00:30:15,840 --> 00:30:19,760
there so let's do a docker ps and if

00:30:19,279 --> 00:30:23,279
everything

00:30:19,760 --> 00:30:27,279
runs correctly there's the um

00:30:23,279 --> 00:30:30,399
the uh different um containers running

00:30:27,279 --> 00:30:33,520
there so now that they're here

00:30:30,399 --> 00:30:36,320
let's do some

00:30:33,520 --> 00:30:38,480
uh integration so as i set up at the

00:30:36,320 --> 00:30:39,279
cafe connect we have available a rest

00:30:38,480 --> 00:30:41,120
api

00:30:39,279 --> 00:30:43,039
that allows us to interact with the

00:30:41,120 --> 00:30:44,640
connect cluster and being able to

00:30:43,039 --> 00:30:46,240
create information on the plugins and

00:30:44,640 --> 00:30:49,520
show the information there

00:30:46,240 --> 00:30:52,720
so let's do a curl command to the

00:30:49,520 --> 00:30:55,679
local host port 8083 that's the uh

00:30:52,720 --> 00:30:56,559
port that is exposed by by the connect

00:30:55,679 --> 00:30:58,320
cluster

00:30:56,559 --> 00:31:00,960
and let's let's check the connector

00:30:58,320 --> 00:31:03,120
plugins available

00:31:00,960 --> 00:31:04,960
and let's make it a little bit clearer

00:31:03,120 --> 00:31:08,640
using jq

00:31:04,960 --> 00:31:11,120
and here we have we have the

00:31:08,640 --> 00:31:13,760
default components available at the

00:31:11,120 --> 00:31:15,679
plugins the connectors for mirror maker

00:31:13,760 --> 00:31:17,279
for the file stream the sync and the

00:31:15,679 --> 00:31:18,000
source that i was mentioning that are

00:31:17,279 --> 00:31:21,039
part of the

00:31:18,000 --> 00:31:23,519
um default uh implementation

00:31:21,039 --> 00:31:24,960
and now we have also the data ginger

00:31:23,519 --> 00:31:26,159
connector that is the one that is using

00:31:24,960 --> 00:31:28,240
as a base image

00:31:26,159 --> 00:31:30,320
as well as the generic camel sink and

00:31:28,240 --> 00:31:31,360
the generic camel source connectors

00:31:30,320 --> 00:31:33,519
but the one that we are really

00:31:31,360 --> 00:31:35,519
interested into is this connector the

00:31:33,519 --> 00:31:36,399
camel timer source connector that is now

00:31:35,519 --> 00:31:39,600
available

00:31:36,399 --> 00:31:40,159
in my kafka connect uh cluster so now

00:31:39,600 --> 00:31:42,559
that we've

00:31:40,159 --> 00:31:44,000
checked that the plugins directory was

00:31:42,559 --> 00:31:46,480
correctly read from

00:31:44,000 --> 00:31:47,519
my environment then i can start to

00:31:46,480 --> 00:31:50,640
interact with

00:31:47,519 --> 00:31:52,880
the configuration so for that i have

00:31:50,640 --> 00:31:54,960
crafted a configuration file for

00:31:52,880 --> 00:31:58,360
configuring my timer

00:31:54,960 --> 00:32:01,440
so if we can see the

00:31:58,360 --> 00:32:03,919
timer.json file we can see that

00:32:01,440 --> 00:32:05,120
we have a component that is going to be

00:32:03,919 --> 00:32:06,880
called timer

00:32:05,120 --> 00:32:08,640
so it's the terminal will be triggering

00:32:06,880 --> 00:32:10,880
every certain amount of time

00:32:08,640 --> 00:32:11,840
i'm going to be using this configuration

00:32:10,880 --> 00:32:14,480
for my connector

00:32:11,840 --> 00:32:16,159
i have the camel timer source connector

00:32:14,480 --> 00:32:21,039
uh we'll be sending messages

00:32:16,159 --> 00:32:24,399
into a topic called camel dot timer.1

00:32:21,039 --> 00:32:26,159
um we just have to put a name on on our

00:32:24,399 --> 00:32:28,559
timer component that's part of the

00:32:26,159 --> 00:32:30,240
camera configuration so it's timer

00:32:28,559 --> 00:32:31,679
and it will be triggering every five

00:32:30,240 --> 00:32:34,080
seconds so 500

00:32:31,679 --> 00:32:35,120
5000 milliseconds we're going to be

00:32:34,080 --> 00:32:37,279
using um

00:32:35,120 --> 00:32:39,279
some more traditional kafka

00:32:37,279 --> 00:32:41,120
configuration like in this cause

00:32:39,279 --> 00:32:42,640
in this case i'm going to use the string

00:32:41,120 --> 00:32:45,679
converter for the key

00:32:42,640 --> 00:32:47,840
a json converter for the value and some

00:32:45,679 --> 00:32:49,760
more configuration for the task of the

00:32:47,840 --> 00:32:52,000
kafka connect in this case i just need

00:32:49,760 --> 00:32:54,480
one trigger to

00:32:52,000 --> 00:32:56,399
one timer to be triggered and these are

00:32:54,480 --> 00:32:57,279
the transformations that kafka connect

00:32:56,399 --> 00:33:00,720
provides

00:32:57,279 --> 00:33:03,679
for you to use when uh using the uh

00:33:00,720 --> 00:33:05,679
the uh kafka connect uh framework in

00:33:03,679 --> 00:33:08,000
this case i'm just gonna use the hoist

00:33:05,679 --> 00:33:11,039
field transformation to be able to

00:33:08,000 --> 00:33:13,679
create a json file that is called timer

00:33:11,039 --> 00:33:15,519
uh so we can have a instead of just

00:33:13,679 --> 00:33:16,399
triggering with the new value i just

00:33:15,519 --> 00:33:18,320
want to

00:33:16,399 --> 00:33:20,240
uh put some information like what is the

00:33:18,320 --> 00:33:23,440
topic that i'm using for the trigger

00:33:20,240 --> 00:33:24,159
as well as uh painting or when putting

00:33:23,440 --> 00:33:27,840
the uh

00:33:24,159 --> 00:33:29,840
in within the pillow the timestamp of my

00:33:27,840 --> 00:33:31,600
trigger so i'm gonna use the hoist value

00:33:29,840 --> 00:33:34,159
and then we're gonna insert

00:33:31,600 --> 00:33:34,960
insert body transformation and i'm gonna

00:33:34,159 --> 00:33:36,880
be adding

00:33:34,960 --> 00:33:38,000
a field that it's called ts for the

00:33:36,880 --> 00:33:40,640
timestamp

00:33:38,000 --> 00:33:42,399
and the information on about the the

00:33:40,640 --> 00:33:44,080
topic so this is traditional kafka

00:33:42,399 --> 00:33:45,919
connect configuration there's

00:33:44,080 --> 00:33:47,600
nothing related to camel here you know

00:33:45,919 --> 00:33:48,880
dsl and and

00:33:47,600 --> 00:33:51,360
nothing like that the only thing you

00:33:48,880 --> 00:33:52,000
need to know is these properties that

00:33:51,360 --> 00:33:55,120
are part of the

00:33:52,000 --> 00:33:58,159
documentation for um for

00:33:55,120 --> 00:33:58,880
the camel connectors so now that i have

00:33:58,159 --> 00:34:02,320
uh

00:33:58,880 --> 00:34:05,200
go over this i can then post to the rest

00:34:02,320 --> 00:34:09,679
api to create this connector

00:34:05,200 --> 00:34:12,800
so let's uh run another curl cupboard

00:34:09,679 --> 00:34:16,879
and we're gonna

00:34:12,800 --> 00:34:17,280
do a post method using uh the connectors

00:34:16,879 --> 00:34:20,480
uh

00:34:17,280 --> 00:34:23,679
endpoint and i'm gonna use this

00:34:20,480 --> 00:34:26,159
json file as the payload for my call

00:34:23,679 --> 00:34:27,760
so if we run this we have a successful

00:34:26,159 --> 00:34:31,119
um

00:34:27,760 --> 00:34:34,000
result and then it just says me that the

00:34:31,119 --> 00:34:36,639
timer was correctly configured and it's

00:34:34,000 --> 00:34:39,040
now running on my cluster

00:34:36,639 --> 00:34:41,280
so let's check if this is uh really

00:34:39,040 --> 00:34:45,119
happening so for that i'm gonna you

00:34:41,280 --> 00:34:46,399
be using a kafka um utility called kafka

00:34:45,119 --> 00:34:48,639
cat it's a

00:34:46,399 --> 00:34:49,760
it's a simple way to interact with your

00:34:48,639 --> 00:34:51,919
kafka cluster without

00:34:49,760 --> 00:34:53,520
using the traditional um console

00:34:51,919 --> 00:34:56,720
consumer console

00:34:53,520 --> 00:34:58,160
producer um uh scripts that are part of

00:34:56,720 --> 00:35:01,280
the project so you can

00:34:58,160 --> 00:35:04,480
uh check this it's uh it's pretty useful

00:35:01,280 --> 00:35:07,680
and for this i'm gonna be reading from

00:35:04,480 --> 00:35:10,320
my local host 1992 that's the uh the

00:35:07,680 --> 00:35:12,079
bootstrap server url that i'm using in

00:35:10,320 --> 00:35:13,680
my local environment and i'm gonna

00:35:12,079 --> 00:35:16,880
select the topic

00:35:13,680 --> 00:35:17,440
camel dot timer one that is the tablet

00:35:16,880 --> 00:35:19,520
that i

00:35:17,440 --> 00:35:21,040
configured to be triggering the

00:35:19,520 --> 00:35:24,480
information

00:35:21,040 --> 00:35:27,920
so now let's take a look and then

00:35:24,480 --> 00:35:30,320
you can see that it's going to be

00:35:27,920 --> 00:35:32,480
publishing uh the information on the

00:35:30,320 --> 00:35:34,640
timer and the topic

00:35:32,480 --> 00:35:36,400
every certain amount of time so every

00:35:34,640 --> 00:35:39,359
five seconds we will get

00:35:36,400 --> 00:35:40,160
a new event a new record coming into the

00:35:39,359 --> 00:35:43,280
topic

00:35:40,160 --> 00:35:46,400
that is being triggered by my um

00:35:43,280 --> 00:35:47,200
camel component so sometimes you will

00:35:46,400 --> 00:35:48,480
need you know

00:35:47,200 --> 00:35:50,240
to trigger you just want to get

00:35:48,480 --> 00:35:50,880
information every certain amount of time

00:35:50,240 --> 00:35:52,720
you can

00:35:50,880 --> 00:35:54,560
craft a little bit on the data that you

00:35:52,720 --> 00:35:57,040
want to uh to receive

00:35:54,560 --> 00:35:58,079
and and so on so it's pretty

00:35:57,040 --> 00:36:01,119
straightforward this is

00:35:58,079 --> 00:36:02,960
a sync connector but you can also use

00:36:01,119 --> 00:36:03,520
other connectors as i was mentioning to

00:36:02,960 --> 00:36:06,720
get

00:36:03,520 --> 00:36:10,880
uh information out of your cluster

00:36:06,720 --> 00:36:12,960
so let's get back to the slides

00:36:10,880 --> 00:36:14,640
yeah i'm ready to interrupt you but i

00:36:12,960 --> 00:36:19,359
have to warn you that you have

00:36:14,640 --> 00:36:22,000
uh three minutes left okay correct

00:36:19,359 --> 00:36:24,320
i'm already done i'm just going to my

00:36:22,000 --> 00:36:26,720
takeaways

00:36:24,320 --> 00:36:27,839
good so i hope you liked the demo uh it

00:36:26,720 --> 00:36:31,119
was pretty interesting

00:36:27,839 --> 00:36:32,960
and then for the takeaways uh that i

00:36:31,119 --> 00:36:36,960
wanted to share with you

00:36:32,960 --> 00:36:39,200
it's that we have um

00:36:36,960 --> 00:36:40,960
the combination of two really

00:36:39,200 --> 00:36:42,240
interesting projects from the apache

00:36:40,960 --> 00:36:44,560
foundation

00:36:42,240 --> 00:36:46,320
for one side we have all the power of

00:36:44,560 --> 00:36:47,440
the data stream platform that kafka

00:36:46,320 --> 00:36:50,400
represents

00:36:47,440 --> 00:36:52,640
and then we have all the knowledge and

00:36:50,400 --> 00:36:56,320
the accumulation of experience that the

00:36:52,640 --> 00:36:56,960
camel uh team has on on the integration

00:36:56,320 --> 00:37:00,160
side

00:36:56,960 --> 00:37:00,400
and playing them together allows you as

00:37:00,160 --> 00:37:02,960
a

00:37:00,400 --> 00:37:04,560
kafka connect user to get a lot of new

00:37:02,960 --> 00:37:07,119
integrations a new

00:37:04,560 --> 00:37:08,800
a lot of options to connect your to your

00:37:07,119 --> 00:37:12,480
system and for

00:37:08,800 --> 00:37:14,240
the camel users allows you to you know

00:37:12,480 --> 00:37:15,520
use reuse all that knowledge that you

00:37:14,240 --> 00:37:17,280
have on the components and the

00:37:15,520 --> 00:37:20,560
connectors to be able to

00:37:17,280 --> 00:37:23,280
easily plug in into the kafka ecosystem

00:37:20,560 --> 00:37:25,040
using the uh very straightforward kafka

00:37:23,280 --> 00:37:27,200
connect approach

00:37:25,040 --> 00:37:28,880
and finally i'm going to show you some

00:37:27,200 --> 00:37:31,599
of the links

00:37:28,880 --> 00:37:34,000
um that i use here so you can go to the

00:37:31,599 --> 00:37:35,599
camel cafe connector github projects

00:37:34,000 --> 00:37:38,480
you can follow the conversation on the

00:37:35,599 --> 00:37:41,680
sleep chat or you can also follow them

00:37:38,480 --> 00:37:43,359
on apache camel for uh more in-depth

00:37:41,680 --> 00:37:45,599
information

00:37:43,359 --> 00:37:47,119
so i hope you like this session i think

00:37:45,599 --> 00:37:49,359
we are running out of time

00:37:47,119 --> 00:37:50,240
i really appreciate the invitation to

00:37:49,359 --> 00:37:52,079
join you

00:37:50,240 --> 00:37:54,400
and if you want to know a little bit

00:37:52,079 --> 00:37:55,760
more on what we are doing on the apis

00:37:54,400 --> 00:37:58,160
and the event driven

00:37:55,760 --> 00:37:58,960
architectural world you can follow me on

00:37:58,160 --> 00:38:02,800
twitter

00:37:58,960 --> 00:38:05,520
on guerrero we double o at the end

00:38:02,800 --> 00:38:07,599
or also i invite you to subscribe to my

00:38:05,520 --> 00:38:11,440
youtube channel where you can find

00:38:07,599 --> 00:38:18,079
some more demos and examples and videos

00:38:11,440 --> 00:38:18,079

YouTube URL: https://www.youtube.com/watch?v=yYTISzU5gIQ


