Title: Have you lost your node...again? - DevConf.CZ 2021
Publication date: 2021-03-19
Playlist: DevConfCZ 2021
Description: 
	Speaker: Petr Horáček


Remote configuration of networking is a risky business. After you hit enter to set up your elaborate network, hopefully you will see the prompt blinking again after a couple of seconds of connectivity loss. But chances are, you won't get to see it ever again.

During this presentation we are going to discuss the resiliency of Kubernetes clusters and the risks of network configuration performed on day-2. We will follow by presenting Kubernetes nmstate, a tool for declarative cluster network configuration, and the mechanisms it uses to prevent and recover from connectivity loss.

The attendees should have at least a basic knowledge about Kubernetes.

The audience will walk away with a better understanding of the risks related to the configuration of the network of a Kubernetes cluster, and their mitigations. 


Schedule: https://sched.co/gmNl
Captions: 
	00:00:02,480 --> 00:00:05,600
hi all

00:00:03,120 --> 00:00:06,480
thanks for joining the very last present

00:00:05,600 --> 00:00:09,440
here

00:00:06,480 --> 00:00:10,559
um so what's hidden behind this daring

00:00:09,440 --> 00:00:12,880
title

00:00:10,559 --> 00:00:14,400
i want to spend the first half of this

00:00:12,880 --> 00:00:16,800
presentation talking about

00:00:14,400 --> 00:00:17,920
the some of the core concepts and

00:00:16,800 --> 00:00:19,279
kubernetes that

00:00:17,920 --> 00:00:21,760
are supposed to help you as an

00:00:19,279 --> 00:00:23,840
administrator or as a user to

00:00:21,760 --> 00:00:26,000
keep your workload up and running no

00:00:23,840 --> 00:00:28,000
matter what happens

00:00:26,000 --> 00:00:31,039
and in the second half of the talk i

00:00:28,000 --> 00:00:32,399
will be talking about

00:00:31,039 --> 00:00:34,239
node network configuration and

00:00:32,399 --> 00:00:36,320
kubernetes and

00:00:34,239 --> 00:00:37,680
i will focus on a safe network

00:00:36,320 --> 00:00:40,800
configuration

00:00:37,680 --> 00:00:43,840
so let's start

00:00:40,800 --> 00:00:44,320
you all know this logo um kumar this is

00:00:43,840 --> 00:00:46,160
an

00:00:44,320 --> 00:00:48,239
orchestration tool that helps you run

00:00:46,160 --> 00:00:50,719
your continuous workloads

00:00:48,239 --> 00:00:52,239
and this workload is usually running on

00:00:50,719 --> 00:00:54,879
multiple nodes

00:00:52,239 --> 00:00:55,920
and even more importantly this workload

00:00:54,879 --> 00:00:59,440
is not running

00:00:55,920 --> 00:01:01,520
on a laptop or a phone of your user

00:00:59,440 --> 00:01:03,760
meaning that if anything goes bad with

00:01:01,520 --> 00:01:05,519
the servers that you run on or within

00:01:03,760 --> 00:01:08,799
networking that's connecting these

00:01:05,519 --> 00:01:11,280
servers the user may end up offline

00:01:08,799 --> 00:01:12,000
so how do we prevent that let's

00:01:11,280 --> 00:01:15,200
illustrate it

00:01:12,000 --> 00:01:15,200
on a couple of examples

00:01:16,159 --> 00:01:21,600
so here we have a node and

00:01:19,360 --> 00:01:23,439
on the node we have a bot which is the

00:01:21,600 --> 00:01:26,799
smallest

00:01:23,439 --> 00:01:29,040
unit of workload in kubernetes and

00:01:26,799 --> 00:01:30,880
we have a user this user is using our

00:01:29,040 --> 00:01:32,640
application running in the pod

00:01:30,880 --> 00:01:33,920
and you can tell that they are quite

00:01:32,640 --> 00:01:36,079
happy

00:01:33,920 --> 00:01:38,400
but then something terrible happens with

00:01:36,079 --> 00:01:40,720
the node and the user loses their

00:01:38,400 --> 00:01:43,759
connectivity the application is not

00:01:40,720 --> 00:01:47,280
reachable anymore and they are upset

00:01:43,759 --> 00:01:50,159
so how do we prevent this

00:01:47,280 --> 00:01:50,960
if the issue was a single point of

00:01:50,159 --> 00:01:53,280
failure

00:01:50,960 --> 00:01:54,880
of our single node let's draw more notes

00:01:53,280 --> 00:01:57,200
in the cluster

00:01:54,880 --> 00:01:59,600
and since we have multiple nodes let's

00:01:57,200 --> 00:02:01,680
include a control plane node that will

00:01:59,600 --> 00:02:04,880
manage them we'll manage all the plots

00:02:01,680 --> 00:02:08,000
and keep the nodes healthy

00:02:04,880 --> 00:02:10,080
and now to leverage our new nodes

00:02:08,000 --> 00:02:11,760
we create something called deployment

00:02:10,080 --> 00:02:13,760
which is a

00:02:11,760 --> 00:02:16,239
simple way in kubernetes how to run your

00:02:13,760 --> 00:02:19,840
application in multiple instances

00:02:16,239 --> 00:02:21,680
uh what it does it deploys your bot uh

00:02:19,840 --> 00:02:23,440
how many times you want if it sees that

00:02:21,680 --> 00:02:27,120
there are more replicas than you

00:02:23,440 --> 00:02:30,720
asked for it will kill some if there is

00:02:27,120 --> 00:02:33,840
less of them it will create a new one

00:02:30,720 --> 00:02:35,440
um so we kind of solved the issue with a

00:02:33,840 --> 00:02:38,480
single point of failure but

00:02:35,440 --> 00:02:39,360
we have a new one and that's which of

00:02:38,480 --> 00:02:43,200
these spots

00:02:39,360 --> 00:02:45,440
should be now accessed by the user um

00:02:43,200 --> 00:02:46,800
for the user they just want to get to

00:02:45,440 --> 00:02:48,720
the application they don't

00:02:46,800 --> 00:02:50,080
care about how many pots you're earning

00:02:48,720 --> 00:02:52,879
in the cluster

00:02:50,080 --> 00:02:54,640
so to solve this issue kubernetes

00:02:52,879 --> 00:02:58,720
introduces something called service

00:02:54,640 --> 00:03:01,760
which is an abstraction of service

00:02:58,720 --> 00:03:03,760
what it does is

00:03:01,760 --> 00:03:04,959
it serves as a single entry bond with a

00:03:03,760 --> 00:03:06,800
virtual ip address

00:03:04,959 --> 00:03:08,560
and a domain name that the user accesses

00:03:06,800 --> 00:03:11,120
to and then

00:03:08,560 --> 00:03:14,560
the request is then forwarded to one of

00:03:11,120 --> 00:03:17,440
the bots implementing the service

00:03:14,560 --> 00:03:19,120
so now the user is happy the traffic is

00:03:17,440 --> 00:03:20,159
getting load balance to our health and

00:03:19,120 --> 00:03:23,920
thoughts

00:03:20,159 --> 00:03:25,360
and say now one of them crashes again

00:03:23,920 --> 00:03:28,000
you see that the user is still happy

00:03:25,360 --> 00:03:28,560
because uh the liveness probe of this

00:03:28,000 --> 00:03:33,200
spot

00:03:28,560 --> 00:03:36,560
uh was started failing and the service

00:03:33,200 --> 00:03:39,760
notice that the one of the posted

00:03:36,560 --> 00:03:43,120
interfaces to is down so it will

00:03:39,760 --> 00:03:45,120
serve on it will dispatch all the

00:03:43,120 --> 00:03:48,720
traffic only to do one health hippo

00:03:45,120 --> 00:03:50,560
from now on um and then

00:03:48,720 --> 00:03:52,560
thanks to our deployment we start a new

00:03:50,560 --> 00:03:54,640
budget do you have two of them

00:03:52,560 --> 00:03:56,159
and the service picks that up and

00:03:54,640 --> 00:03:58,840
forwards the traffic here

00:03:56,159 --> 00:04:00,879
and finally the failed bot is getting

00:03:58,840 --> 00:04:04,319
deleted

00:04:00,879 --> 00:04:06,560
so it looks good so far but you may have

00:04:04,319 --> 00:04:08,239
noticed that by

00:04:06,560 --> 00:04:10,319
while i was solving one issue of a

00:04:08,239 --> 00:04:11,439
single point of failure of our worker i

00:04:10,319 --> 00:04:14,159
introduced a new one

00:04:11,439 --> 00:04:15,200
which was the control plane we have only

00:04:14,159 --> 00:04:18,320
one instance of that

00:04:15,200 --> 00:04:21,359
so what happens if death crashes

00:04:18,320 --> 00:04:23,199
well we will lose our deployment but you

00:04:21,359 --> 00:04:25,520
see from the slide that the user is

00:04:23,199 --> 00:04:28,800
still communicating with our reports

00:04:25,520 --> 00:04:30,320
um that's because kubernetes proves to

00:04:28,800 --> 00:04:32,560
be quite bulletproof here

00:04:30,320 --> 00:04:33,840
um if it loses the control plane it

00:04:32,560 --> 00:04:36,880
enters something

00:04:33,840 --> 00:04:38,560
uh it enters a read-only mode meaning

00:04:36,880 --> 00:04:40,720
you cannot create a new bots the

00:04:38,560 --> 00:04:42,880
deployment cannot do its job but the

00:04:40,720 --> 00:04:45,280
workload running in those spots is still

00:04:42,880 --> 00:04:48,720
running the service is still serving

00:04:45,280 --> 00:04:50,160
and as long as these two plus stay alive

00:04:48,720 --> 00:04:53,360
you're good

00:04:50,160 --> 00:04:55,040
um so

00:04:53,360 --> 00:04:56,960
if you are okay with this little

00:04:55,040 --> 00:04:58,960
downtime until you

00:04:56,960 --> 00:05:01,759
fix this control plane node you should

00:04:58,960 --> 00:05:03,680
be good to go with this kind of setup

00:05:01,759 --> 00:05:05,360
but we won't stop here i want to make

00:05:03,680 --> 00:05:06,960
the control plane highly available as

00:05:05,360 --> 00:05:10,160
well

00:05:06,960 --> 00:05:11,759
so applying this similar process i will

00:05:10,160 --> 00:05:13,840
throw in more nodes for the control

00:05:11,759 --> 00:05:16,800
plane each of them runs

00:05:13,840 --> 00:05:17,759
single instance of the api server and at

00:05:16,800 --> 00:05:20,800
cd

00:05:17,759 --> 00:05:24,000
that serves as a distributed storage

00:05:20,800 --> 00:05:25,440
now how do we handle the high

00:05:24,000 --> 00:05:29,039
availability here

00:05:25,440 --> 00:05:31,360
for the fcd part all of these instances

00:05:29,039 --> 00:05:32,800
are able to surf at any point of time

00:05:31,360 --> 00:05:35,280
you can read from them and you can

00:05:32,800 --> 00:05:38,400
browse to these

00:05:35,280 --> 00:05:40,160
storages of databases um

00:05:38,400 --> 00:05:41,600
for the right though you need to make

00:05:40,160 --> 00:05:43,759
sure that at least

00:05:41,600 --> 00:05:47,520
more than half of these instances are

00:05:43,759 --> 00:05:50,960
alive so they can vote and allow the

00:05:47,520 --> 00:05:53,199
the new data to be stored uh if

00:05:50,960 --> 00:05:55,039
your account of the lcd instances drops

00:05:53,199 --> 00:05:57,840
below the

00:05:55,039 --> 00:06:00,960
half of them then the cluster again

00:05:57,840 --> 00:06:04,080
enters to read on the node

00:06:00,960 --> 00:06:06,639
um for the api server again we have

00:06:04,080 --> 00:06:07,520
several instances of the same service

00:06:06,639 --> 00:06:10,160
here

00:06:07,520 --> 00:06:10,560
um so we have to solve the same issue as

00:06:10,160 --> 00:06:13,280
we

00:06:10,560 --> 00:06:14,400
did with bots and that's we need to

00:06:13,280 --> 00:06:17,360
balance

00:06:14,400 --> 00:06:19,840
the incoming traffic to all of these and

00:06:17,360 --> 00:06:22,560
make sure that if one of them goes down

00:06:19,840 --> 00:06:23,600
the other two can continue serving the

00:06:22,560 --> 00:06:25,360
requests

00:06:23,600 --> 00:06:27,120
and that's done similar to the service

00:06:25,360 --> 00:06:27,840
using virtual ip address and load

00:06:27,120 --> 00:06:30,639
balancers

00:06:27,840 --> 00:06:31,520
i won't get into details of this but

00:06:30,639 --> 00:06:34,720
with all this

00:06:31,520 --> 00:06:38,160
we have kind of a healthy highly

00:06:34,720 --> 00:06:38,160
available in most cluster

00:06:38,880 --> 00:06:42,319
all right now let's change the topic a

00:06:41,840 --> 00:06:44,080
little bit

00:06:42,319 --> 00:06:46,080
i will talk about the network

00:06:44,080 --> 00:06:49,599
configuration if you

00:06:46,080 --> 00:06:53,199
manage to space out in the previous 10

00:06:49,599 --> 00:06:53,199
minutes now it's the time to wake up

00:06:53,759 --> 00:07:02,080
so um let's get to it so

00:06:59,520 --> 00:07:04,160
uh i will try to illustrate why do we

00:07:02,080 --> 00:07:06,080
want to configure the host networking

00:07:04,160 --> 00:07:07,840
and what can go wrong on a simple

00:07:06,080 --> 00:07:10,639
example again

00:07:07,840 --> 00:07:12,560
here we have three nodes each of them

00:07:10,639 --> 00:07:13,759
has a single network interface connected

00:07:12,560 --> 00:07:15,360
to the central switch

00:07:13,759 --> 00:07:16,960
which is then connected to the outside

00:07:15,360 --> 00:07:20,080
network but it

00:07:16,960 --> 00:07:22,639
doesn't matter here too much and

00:07:20,080 --> 00:07:24,319
finally each of these nodes has a

00:07:22,639 --> 00:07:27,039
handful of pots running

00:07:24,319 --> 00:07:29,120
and these spots are communicating sorry

00:07:27,039 --> 00:07:29,360
communicating over the default interface

00:07:29,120 --> 00:07:32,960
and

00:07:29,360 --> 00:07:35,440
everything looks fine let's say we

00:07:32,960 --> 00:07:37,440
start more reports on these nodes smash

00:07:35,440 --> 00:07:39,360
can fit there

00:07:37,440 --> 00:07:41,199
if they generate enough traffic it may

00:07:39,360 --> 00:07:43,280
be

00:07:41,199 --> 00:07:45,039
an issue for our network interface which

00:07:43,280 --> 00:07:48,479
doesn't have enough throughput

00:07:45,039 --> 00:07:48,479
to carry all this traffic

00:07:48,560 --> 00:07:52,720
to applying the same logic as for high

00:07:51,599 --> 00:07:54,800
availability we

00:07:52,720 --> 00:07:55,919
introduce a new network interface to

00:07:54,800 --> 00:07:58,479
these nodes

00:07:55,919 --> 00:08:00,400
and to make sure that they are equally

00:07:58,479 --> 00:08:03,280
utilized we

00:08:00,400 --> 00:08:04,560
aggregate them using a bonding interface

00:08:03,280 --> 00:08:07,199
uh which is a way to

00:08:04,560 --> 00:08:07,680
aggregate multiple network interfaces to

00:08:07,199 --> 00:08:10,080
either

00:08:07,680 --> 00:08:11,840
increase throughput or provide some

00:08:10,080 --> 00:08:14,560
active backup uh

00:08:11,840 --> 00:08:14,879
safety mechanisms and now you can see

00:08:14,560 --> 00:08:17,520
that

00:08:14,879 --> 00:08:17,919
since we increase the throughput uh or

00:08:17,520 --> 00:08:19,919
the

00:08:17,919 --> 00:08:23,360
possibility but all these spots can

00:08:19,919 --> 00:08:27,599
communicate over the network

00:08:23,360 --> 00:08:30,080
so um looks quite good but if we

00:08:27,599 --> 00:08:32,000
made a single mistake we could have end

00:08:30,080 --> 00:08:34,640
up like this

00:08:32,000 --> 00:08:35,839
um and to illustrate what can go wrong

00:08:34,640 --> 00:08:38,479
let me show you a

00:08:35,839 --> 00:08:40,080
very primitive way of configuration of

00:08:38,479 --> 00:08:43,839
these bondings

00:08:40,080 --> 00:08:46,560
and it's just an ssh running a script

00:08:43,839 --> 00:08:48,000
on the cluster if you don't know iptools

00:08:46,560 --> 00:08:51,519
don't worry i will get

00:08:48,000 --> 00:08:52,560
rid of the whole uh setup line by mine

00:08:51,519 --> 00:08:55,120
so

00:08:52,560 --> 00:08:55,920
here we have to we first have to connect

00:08:55,120 --> 00:08:59,440
to the nodes

00:08:55,920 --> 00:09:03,200
or we use ssh to do that um

00:08:59,440 --> 00:09:06,640
and then we run a set of ip

00:09:03,200 --> 00:09:09,120
link commands first we need to bring the

00:09:06,640 --> 00:09:10,800
original management interface et 0 down

00:09:09,120 --> 00:09:14,160
so we can reconfigure it

00:09:10,800 --> 00:09:14,959
and this is the the first problematic

00:09:14,160 --> 00:09:17,360
point

00:09:14,959 --> 00:09:20,560
because as soon as we bring it down we

00:09:17,360 --> 00:09:20,560
lose the management network

00:09:20,640 --> 00:09:24,399
and if any of the following commands

00:09:23,440 --> 00:09:26,000
fail

00:09:24,399 --> 00:09:28,160
then we lose this management network

00:09:26,000 --> 00:09:30,000
forever unless you use console to

00:09:28,160 --> 00:09:32,880
connect back to the host and revive

00:09:30,000 --> 00:09:34,480
the configuration but they may be quite

00:09:32,880 --> 00:09:36,800
troublesome

00:09:34,480 --> 00:09:38,399
um but let's pretend everything is okay

00:09:36,800 --> 00:09:42,160
here and continue to the next

00:09:38,399 --> 00:09:45,440
line so we create this virtual uh

00:09:42,160 --> 00:09:48,080
interface a bone of type bond we

00:09:45,440 --> 00:09:49,440
attach both of our brands interfaces to

00:09:48,080 --> 00:09:51,519
it and we bring

00:09:49,440 --> 00:09:52,640
up all the both interfaces and the

00:09:51,519 --> 00:09:55,360
bonding

00:09:52,640 --> 00:09:55,760
so now we formed this virtual interface

00:09:55,360 --> 00:09:59,519
with

00:09:55,760 --> 00:10:03,120
both of its ports and finally

00:09:59,519 --> 00:10:05,040
um in the original setup the eth0

00:10:03,120 --> 00:10:06,480
as management interface and carried the

00:10:05,040 --> 00:10:09,040
default ip address

00:10:06,480 --> 00:10:10,720
but as soon as we attached it to our

00:10:09,040 --> 00:10:13,200
bond link it lost

00:10:10,720 --> 00:10:14,959
its ip address so now we need to

00:10:13,200 --> 00:10:16,160
configure it again but this time on the

00:10:14,959 --> 00:10:19,600
bond link

00:10:16,160 --> 00:10:23,279
we use a dhcp client to do so

00:10:19,600 --> 00:10:26,720
um and here comes this second issue

00:10:23,279 --> 00:10:29,440
or our pain plan and that's that if we

00:10:26,720 --> 00:10:29,920
don't manage to get an ip address again

00:10:29,440 --> 00:10:33,360
uh

00:10:29,920 --> 00:10:34,800
the host will end up offline and if we

00:10:33,360 --> 00:10:35,920
do get an ip address but it's a

00:10:34,800 --> 00:10:38,320
different one than we

00:10:35,920 --> 00:10:40,959
used to have then the note won't recover

00:10:38,320 --> 00:10:40,959
properly

00:10:41,360 --> 00:10:44,640
and the first issue i guess is if you

00:10:43,519 --> 00:10:48,480
make typo in

00:10:44,640 --> 00:10:48,480
any of this you are again in trouble

00:10:48,560 --> 00:10:56,079
um these are all the issues but

00:10:52,320 --> 00:10:58,000
um what

00:10:56,079 --> 00:10:59,760
another one would be that this is not

00:10:58,000 --> 00:11:00,399
really what we are used to in kubernetes

00:10:59,760 --> 00:11:02,800
right

00:11:00,399 --> 00:11:03,440
what we are used to is something like

00:11:02,800 --> 00:11:06,560
this

00:11:03,440 --> 00:11:09,680
you call cube cuddle apply and some

00:11:06,560 --> 00:11:10,880
gamble file and this is exactly what

00:11:09,680 --> 00:11:14,399
kubernetes nmc

00:11:10,880 --> 00:11:15,200
project which uh logos uh which logo you

00:11:14,399 --> 00:11:18,880
saw in the

00:11:15,200 --> 00:11:21,760
previous slides uh does

00:11:18,880 --> 00:11:23,440
it provides uh coverage native way of

00:11:21,760 --> 00:11:27,839
configuring the networking

00:11:23,440 --> 00:11:30,720
in a kind of a declarative manner

00:11:27,839 --> 00:11:32,959
so let's illustrate how it works on an

00:11:30,720 --> 00:11:35,760
example again

00:11:32,959 --> 00:11:36,480
so here we have our three nodes each of

00:11:35,760 --> 00:11:38,480
them has an

00:11:36,480 --> 00:11:40,240
instance of animstate which is

00:11:38,480 --> 00:11:41,040
communicating with the local network

00:11:40,240 --> 00:11:44,160
manager

00:11:41,040 --> 00:11:44,800
to obtain the network into the network

00:11:44,160 --> 00:11:47,279
status

00:11:44,800 --> 00:11:48,720
and also to write the configuration back

00:11:47,279 --> 00:11:50,720
to the host

00:11:48,720 --> 00:11:52,160
and again each of these nodes has to

00:11:50,720 --> 00:11:56,160
interfaces t0

00:11:52,160 --> 00:11:58,880
and eth1 now the first feature

00:11:56,160 --> 00:12:00,000
of nm state is that it reports the state

00:11:58,880 --> 00:12:03,200
of the network

00:12:00,000 --> 00:12:04,320
as a kubernetes object uh let's call it

00:12:03,200 --> 00:12:06,399
state here

00:12:04,320 --> 00:12:08,800
and the state would then contain the

00:12:06,399 --> 00:12:12,639
list of the interfaces we have available

00:12:08,800 --> 00:12:13,120
here it's eth0 that's up now and adh1

00:12:12,639 --> 00:12:15,519
that's

00:12:13,120 --> 00:12:17,040
currently down it does it for every

00:12:15,519 --> 00:12:19,760
single node in the cluster

00:12:17,040 --> 00:12:22,320
and you can use this information to just

00:12:19,760 --> 00:12:24,480
figure out what interfaces are available

00:12:22,320 --> 00:12:28,240
to integrate with some kind of

00:12:24,480 --> 00:12:28,240
automation or monitoring tools

00:12:28,399 --> 00:12:32,800
um the counterpart of reporting is

00:12:31,200 --> 00:12:35,440
configuration

00:12:32,800 --> 00:12:36,240
this is driven by a policy object where

00:12:35,440 --> 00:12:39,440
we

00:12:36,240 --> 00:12:42,800
declare the desired state of the network

00:12:39,440 --> 00:12:45,680
um on all the hosts that match

00:12:42,800 --> 00:12:47,760
this policy so here we declare the

00:12:45,680 --> 00:12:50,639
policy that says we want a

00:12:47,760 --> 00:12:51,279
interface of type bond uh called bond

00:12:50,639 --> 00:12:54,079
one

00:12:51,279 --> 00:12:56,000
and it should have two parts eta0 and

00:12:54,079 --> 00:12:59,200
eth1

00:12:56,000 --> 00:13:01,920
now when you apply this configuration

00:12:59,200 --> 00:13:02,720
nmstate will create an enactment object

00:13:01,920 --> 00:13:06,000
per each

00:13:02,720 --> 00:13:08,320
node in the cluster and this is then

00:13:06,000 --> 00:13:10,399
used to monitor the progress of the

00:13:08,320 --> 00:13:15,279
configuration and also to develop

00:13:10,399 --> 00:13:17,519
any issues now here you can see that

00:13:15,279 --> 00:13:19,279
we have this central enactment in

00:13:17,519 --> 00:13:20,399
progress while the one on the left and

00:13:19,279 --> 00:13:23,920
on the right are

00:13:20,399 --> 00:13:26,399
pending this is a first safety mechanism

00:13:23,920 --> 00:13:27,760
of nm state where we apply the

00:13:26,399 --> 00:13:30,639
configuration on

00:13:27,760 --> 00:13:31,519
one node at the time and by doing that

00:13:30,639 --> 00:13:33,839
we make sure that

00:13:31,519 --> 00:13:35,519
if the configuration is disruptive for

00:13:33,839 --> 00:13:39,040
the network connectivity it won't

00:13:35,519 --> 00:13:42,320
take down all the nodes at once

00:13:39,040 --> 00:13:44,079
um so let's get into the configuration

00:13:42,320 --> 00:13:46,399
itself now

00:13:44,079 --> 00:13:47,440
the next one is progress and um state

00:13:46,399 --> 00:13:49,199
does its

00:13:47,440 --> 00:13:51,600
thing and it creates the bonding

00:13:49,199 --> 00:13:54,880
interface over these two

00:13:51,600 --> 00:13:58,720
network interfaces et0 and eth1

00:13:54,880 --> 00:14:02,000
now the configuration succeeded

00:13:58,720 --> 00:14:05,120
but we don't stop here we

00:14:02,000 --> 00:14:06,079
we do it now we get to the second safety

00:14:05,120 --> 00:14:08,800
mechanism

00:14:06,079 --> 00:14:09,920
which is the connectivity check we want

00:14:08,800 --> 00:14:11,920
to make sure that

00:14:09,920 --> 00:14:13,440
after the configuration of the host is

00:14:11,920 --> 00:14:15,600
finished we

00:14:13,440 --> 00:14:17,680
still have connectivity to the default

00:14:15,600 --> 00:14:19,519
gateway to the dns server and to the

00:14:17,680 --> 00:14:21,360
kubernetes api server

00:14:19,519 --> 00:14:24,079
to confirm that the node is still

00:14:21,360 --> 00:14:27,680
healthy and member of the cluster

00:14:24,079 --> 00:14:29,760
if we wouldn't get response back

00:14:27,680 --> 00:14:31,199
then the configuration of the node the

00:14:29,760 --> 00:14:33,920
bonding would be

00:14:31,199 --> 00:14:35,040
removed and the default interface would

00:14:33,920 --> 00:14:39,519
be again

00:14:35,040 --> 00:14:43,360
it is 0 but in this case

00:14:39,519 --> 00:14:46,160
our bank get response and we committed

00:14:43,360 --> 00:14:49,040
the configuration on the node

00:14:46,160 --> 00:14:50,240
and then we continue to to the node on

00:14:49,040 --> 00:14:53,519
left we configure it

00:14:50,240 --> 00:14:54,560
and to denote on right it's worth

00:14:53,519 --> 00:14:57,279
mentioning that

00:14:54,560 --> 00:14:58,880
if the configuration on one of these

00:14:57,279 --> 00:15:00,959
nodes failed

00:14:58,880 --> 00:15:02,240
we wouldn't continue configuring all the

00:15:00,959 --> 00:15:05,519
other ones we

00:15:02,240 --> 00:15:08,880
treat every single configuration as a

00:15:05,519 --> 00:15:10,839
canary test and if it fails we just

00:15:08,880 --> 00:15:13,839
abort the whole rollout of the

00:15:10,839 --> 00:15:13,839
configuration

00:15:14,240 --> 00:15:18,639
okay that was kind of the process in

00:15:17,600 --> 00:15:20,160
pictures but

00:15:18,639 --> 00:15:21,920
we are probably more familiar with

00:15:20,160 --> 00:15:24,720
cubectl and manifest

00:15:21,920 --> 00:15:25,600
so let's illustrate the same same

00:15:24,720 --> 00:15:29,279
process

00:15:25,600 --> 00:15:31,360
using these tools so if you want to

00:15:29,279 --> 00:15:33,040
read the current node network state you

00:15:31,360 --> 00:15:35,680
would call cubecut will get

00:15:33,040 --> 00:15:39,120
nns will end the name of node and then

00:15:35,680 --> 00:15:42,320
as being assured for node network state

00:15:39,120 --> 00:15:45,040
and here there is a strip down

00:15:42,320 --> 00:15:45,519
example of a node network state it has a

00:15:45,040 --> 00:15:48,480
name

00:15:45,519 --> 00:15:50,000
matching the name of the node it has a

00:15:48,480 --> 00:15:53,680
list of interfaces

00:15:50,000 --> 00:15:55,120
uh eth0 with its ip address and nth one

00:15:53,680 --> 00:15:57,040
which is down

00:15:55,120 --> 00:15:58,399
if you call this on real cluster you

00:15:57,040 --> 00:16:00,160
would also see

00:15:58,399 --> 00:16:02,160
many other interfaces the dns

00:16:00,160 --> 00:16:04,399
configuration default gateways and much

00:16:02,160 --> 00:16:08,000
more

00:16:04,399 --> 00:16:11,120
so now for the configuration part

00:16:08,000 --> 00:16:13,680
we would call cube ctl apply and then

00:16:11,120 --> 00:16:15,040
we apply an object of kind node network

00:16:13,680 --> 00:16:19,120
configuration policy

00:16:15,040 --> 00:16:22,480
ncp ensure it has an arbitrary name

00:16:19,120 --> 00:16:25,680
and the declaration of the desired state

00:16:22,480 --> 00:16:26,240
here we want the bonding uh we want the

00:16:25,680 --> 00:16:29,839
bond

00:16:26,240 --> 00:16:32,480
interface ctg zero and one and the

00:16:29,839 --> 00:16:33,040
mode of the bonding is uh balance round

00:16:32,480 --> 00:16:36,800
ribbon

00:16:33,040 --> 00:16:41,360
which should balance the traffic evenly

00:16:36,800 --> 00:16:43,440
across both of the ports um

00:16:41,360 --> 00:16:45,519
this api that we use in the kubernetes

00:16:43,440 --> 00:16:46,880
center state is directly taken from the

00:16:45,519 --> 00:16:49,440
nmstate project

00:16:46,880 --> 00:16:50,480
which is not bound to kubernetes if you

00:16:49,440 --> 00:16:53,759
want to learn more about

00:16:50,480 --> 00:16:55,600
it just search for it i guess

00:16:53,759 --> 00:16:57,440
it may be very useful if you are dealing

00:16:55,600 --> 00:16:59,519
with a configuration of

00:16:57,440 --> 00:17:02,720
individual nodes and you want to do it

00:16:59,519 --> 00:17:02,720
in a declarative manner

00:17:03,279 --> 00:17:07,280
uh so we applied the desired

00:17:05,520 --> 00:17:09,199
configuration of the bonding and now we

00:17:07,280 --> 00:17:11,600
want to monitor the progress using node

00:17:09,199 --> 00:17:15,360
network configuration enactment

00:17:11,600 --> 00:17:17,760
or nfc in short um

00:17:15,360 --> 00:17:19,679
as soon as we call it we get this issue

00:17:17,760 --> 00:17:20,480
unable to connect to the server which is

00:17:19,679 --> 00:17:23,520
where you should

00:17:20,480 --> 00:17:26,240
when you should start panicking we

00:17:23,520 --> 00:17:27,839
somehow got disconnected for we are not

00:17:26,240 --> 00:17:28,880
able to connect anymore to the api

00:17:27,839 --> 00:17:31,919
server which

00:17:28,880 --> 00:17:34,240
sounds like treble fortunately after a

00:17:31,919 --> 00:17:37,280
couple of minutes you

00:17:34,240 --> 00:17:40,640
can try it again called get an nce

00:17:37,280 --> 00:17:43,520
and you see that the connection is back

00:17:40,640 --> 00:17:44,559
uh the configuration the first node

00:17:43,520 --> 00:17:46,720
failed

00:17:44,559 --> 00:17:48,000
and the two bedroom nodes just aborted

00:17:46,720 --> 00:17:49,840
the operation and

00:17:48,000 --> 00:17:52,240
never attempted to configure it so this

00:17:49,840 --> 00:17:52,880
is the the rollout and the canary

00:17:52,240 --> 00:17:57,039
testing

00:17:52,880 --> 00:17:59,280
uh practice so now

00:17:57,039 --> 00:18:00,400
let's see why did the configuration

00:17:59,280 --> 00:18:02,559
failed

00:18:00,400 --> 00:18:05,200
so to do that you can get the details of

00:18:02,559 --> 00:18:08,480
the new network configuration enactment

00:18:05,200 --> 00:18:11,440
and in the message it would tell us

00:18:08,480 --> 00:18:11,919
something like this uh basically saying

00:18:11,440 --> 00:18:14,720
that

00:18:11,919 --> 00:18:16,480
we were not able to find the default

00:18:14,720 --> 00:18:18,000
gateway after the configuration was

00:18:16,480 --> 00:18:20,559
finished

00:18:18,000 --> 00:18:21,840
um that sounds like iep configuration

00:18:20,559 --> 00:18:24,960
issue so

00:18:21,840 --> 00:18:26,000
let's review our policy what was wrong

00:18:24,960 --> 00:18:28,240
here

00:18:26,000 --> 00:18:29,600
this is the original one uh i previously

00:18:28,240 --> 00:18:32,240
applied

00:18:29,600 --> 00:18:33,360
and you may remember that on the

00:18:32,240 --> 00:18:35,600
original setup

00:18:33,360 --> 00:18:37,679
the eka0 was the default uh network

00:18:35,600 --> 00:18:39,760
interface keeping the

00:18:37,679 --> 00:18:40,799
default ip address the management ip

00:18:39,760 --> 00:18:44,080
address

00:18:40,799 --> 00:18:45,280
and we forgot to include any ip address

00:18:44,080 --> 00:18:49,120
configuration in this

00:18:45,280 --> 00:18:50,960
uh in this declared state so we can fix

00:18:49,120 --> 00:18:54,000
it by simply saying that this

00:18:50,960 --> 00:18:57,039
bond interface should have ipv4 enabled

00:18:54,000 --> 00:19:00,799
and use dhcp to obtain an ip address

00:18:57,039 --> 00:19:03,600
now when i apply this configuration

00:19:00,799 --> 00:19:04,320
and call get nnce to monitor your

00:19:03,600 --> 00:19:05,520
progress

00:19:04,320 --> 00:19:07,360
you can see that it's successfully

00:19:05,520 --> 00:19:09,120
configured in the first node it's

00:19:07,360 --> 00:19:10,400
currently progressing on the second and

00:19:09,120 --> 00:19:13,600
the third one is just

00:19:10,400 --> 00:19:17,440
waiting in line and eventually

00:19:13,600 --> 00:19:17,440
it should configure all the nodes

00:19:18,880 --> 00:19:24,400
so that was for the current state of

00:19:22,240 --> 00:19:28,080
kubernetes atom state and

00:19:24,400 --> 00:19:29,840
what features are available there now

00:19:28,080 --> 00:19:32,400
kubernetes amstead has many more

00:19:29,840 --> 00:19:35,919
features and supports many more

00:19:32,400 --> 00:19:39,520
types of interfaces to configure but it

00:19:35,919 --> 00:19:40,480
lacks a little bit on the safety side of

00:19:39,520 --> 00:19:42,559
things

00:19:40,480 --> 00:19:44,559
first of all we are currently working on

00:19:42,559 --> 00:19:47,760
a rollout control api

00:19:44,559 --> 00:19:49,120
where you would be able to say that you

00:19:47,760 --> 00:19:51,039
want to

00:19:49,120 --> 00:19:52,960
you don't want to configure in one node

00:19:51,039 --> 00:19:54,960
at the time but you want to

00:19:52,960 --> 00:19:56,240
take groups of them and configure them

00:19:54,960 --> 00:19:59,760
in checks

00:19:56,240 --> 00:20:00,880
now this may be important if you have a

00:19:59,760 --> 00:20:03,440
huge cluster

00:20:00,880 --> 00:20:06,799
where rolling out the configuration node

00:20:03,440 --> 00:20:10,320
by node may take long minutes or hours

00:20:06,799 --> 00:20:14,000
and it's not always necessary to

00:20:10,320 --> 00:20:15,840
be to do it one by one

00:20:14,000 --> 00:20:19,039
in case your desired configuration is

00:20:15,840 --> 00:20:22,159
not really breaking the network

00:20:19,039 --> 00:20:25,039
now the second missing feature for

00:20:22,159 --> 00:20:27,760
safety would be nodes draining

00:20:25,039 --> 00:20:29,679
what we do currently is when you are

00:20:27,760 --> 00:20:31,760
configuring a node we don't

00:20:29,679 --> 00:20:32,960
touch the running pods at all we leave

00:20:31,760 --> 00:20:35,280
them running

00:20:32,960 --> 00:20:37,520
on the note which means that they may

00:20:35,280 --> 00:20:39,679
lose their connectivity for a bit

00:20:37,520 --> 00:20:41,760
all right that shouldn't be an issue if

00:20:39,679 --> 00:20:45,840
you run them in multiple instances but

00:20:41,760 --> 00:20:47,200
it's we want to strive for

00:20:45,840 --> 00:20:49,679
an excellence here and we want to make

00:20:47,200 --> 00:20:52,640
this better by

00:20:49,679 --> 00:20:53,360
draining all the running pots from the

00:20:52,640 --> 00:20:55,200
node

00:20:53,360 --> 00:20:57,440
before we start the configuration there

00:20:55,200 --> 00:20:59,520
so they get time to gracefully shut down

00:20:57,440 --> 00:21:02,240
and start on different nodes and only

00:20:59,520 --> 00:21:04,000
then we attempt to reconfigure the node

00:21:02,240 --> 00:21:06,880
and potentially break the network

00:21:04,000 --> 00:21:06,880
connectively there

00:21:07,840 --> 00:21:12,159
so to wrap up in the first half we

00:21:10,960 --> 00:21:14,799
talked about the

00:21:12,159 --> 00:21:16,640
core concepts of kubernetes that allow

00:21:14,799 --> 00:21:20,960
you to keep your

00:21:16,640 --> 00:21:22,480
containerized workload highly available

00:21:20,960 --> 00:21:25,200
and to keep your clusters highly

00:21:22,480 --> 00:21:27,840
available as well and in the second half

00:21:25,200 --> 00:21:28,400
i talked about the kubernetes nmc

00:21:27,840 --> 00:21:31,760
project

00:21:28,400 --> 00:21:33,039
its api and safety mechanisms that

00:21:31,760 --> 00:21:35,840
should

00:21:33,039 --> 00:21:36,159
help you not to destroy your cluster and

00:21:35,840 --> 00:21:42,960
break

00:21:36,159 --> 00:21:42,960

YouTube URL: https://www.youtube.com/watch?v=xdMeBFcNCqU


