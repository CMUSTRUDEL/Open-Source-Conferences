Title: Data Science Meets DevOps: Gitops with OpenShift - DevConf.CZ 2021
Publication date: 2021-03-19
Playlist: DevConfCZ 2021
Description: 
	Speaker: Hema Veeradhi

OpenShift being a fast-growing open source application platform, makes deploying and managing machine learning models very easy and convenient. Since the development of machine learning models is an iterative procedure, many data scientists prefer to store their model source code like their Jupyter notebooks in Git so that they can perform frequent updates to their models. These models are then deployed via OpenShift in a production environment. To obtain the most optimized model, it is necessary for the models to be continuously retrained and deployed. How can we efficiently manage this periodic retraining and deployment of these machine learning models?

In this talk you will learn how to leverage DevOps for managing machine learning models on OpenShift. With the help of CI/CD tools like Tekton pipelines, we can now extend version control for data science applications as well. You will walk away from this talk knowing how to:
1. Train a machine learning model on an example use case
2. Maintain ML model code in Git
3. Setup CI/CD pipeline for your ML application


Schedule: https://sched.co/gmNQ
Captions: 
	00:00:01,360 --> 00:00:04,160
hello everyone uh thank you all for

00:00:03,439 --> 00:00:07,200
attending

00:00:04,160 --> 00:00:09,280
my talk today i'm hema virab and i'm

00:00:07,200 --> 00:00:12,160
working as a software engineer

00:00:09,280 --> 00:00:13,519
here in the ai ops team and the ai

00:00:12,160 --> 00:00:16,560
center of excellence at

00:00:13,519 --> 00:00:17,680
red hat in today's talk i mainly want to

00:00:16,560 --> 00:00:20,560
go over

00:00:17,680 --> 00:00:22,160
what exactly is devops from a data

00:00:20,560 --> 00:00:23,840
science perspective

00:00:22,160 --> 00:00:26,000
uh what are some of the genops

00:00:23,840 --> 00:00:29,119
principles that could actually be

00:00:26,000 --> 00:00:32,399
applied um for a machine learning model

00:00:29,119 --> 00:00:34,320
development life cycle a quick demo on

00:00:32,399 --> 00:00:35,840
an overview of a machine learning use

00:00:34,320 --> 00:00:38,879
case and how we can

00:00:35,840 --> 00:00:40,559
integrate git ops into it and finally

00:00:38,879 --> 00:00:43,360
leaving some time for any

00:00:40,559 --> 00:00:44,160
q a that you may have with that i also

00:00:43,360 --> 00:00:46,399
want to mention

00:00:44,160 --> 00:00:47,600
um i have a couple of poll questions

00:00:46,399 --> 00:00:50,719
that i've created

00:00:47,600 --> 00:00:51,840
uh and i think um hemanth and um eric

00:00:50,719 --> 00:00:55,039
would probably be

00:00:51,840 --> 00:00:56,879
dropping it in so uh please do uh take

00:00:55,039 --> 00:00:57,280
some time to answer those poll questions

00:00:56,879 --> 00:00:59,920
um

00:00:57,280 --> 00:01:01,359
i would definitely love to hear um what

00:00:59,920 --> 00:01:03,199
kind of audience um

00:01:01,359 --> 00:01:05,199
is sort of interested in this kind of

00:01:03,199 --> 00:01:07,280
topic and yeah

00:01:05,199 --> 00:01:09,200
also drop any other questions that you

00:01:07,280 --> 00:01:12,159
may have and i'll try to

00:01:09,200 --> 00:01:13,600
um get them answered by the end of the

00:01:12,159 --> 00:01:15,840
presentation

00:01:13,600 --> 00:01:16,880
so with that let's go ahead and get

00:01:15,840 --> 00:01:20,479
started

00:01:16,880 --> 00:01:24,720
so what exactly is devops

00:01:20,479 --> 00:01:26,880
devops is part of the agile manifesto

00:01:24,720 --> 00:01:28,560
and one of the principles which i

00:01:26,880 --> 00:01:31,840
believe is highly relevant

00:01:28,560 --> 00:01:32,799
is stated as so our highest priority is

00:01:31,840 --> 00:01:34,640
to satisfy

00:01:32,799 --> 00:01:36,320
the customer through early and

00:01:34,640 --> 00:01:39,680
continuous delivery of

00:01:36,320 --> 00:01:41,439
valuable software now in the case of

00:01:39,680 --> 00:01:44,159
machine learning examples it would

00:01:41,439 --> 00:01:47,600
basically be continuous delivery of

00:01:44,159 --> 00:01:48,960
valuable insights from data so how

00:01:47,600 --> 00:01:52,840
exactly is this

00:01:48,960 --> 00:01:54,320
different from your traditional devops

00:01:52,840 --> 00:01:56,640
approach

00:01:54,320 --> 00:01:58,719
so when you look at your traditional

00:01:56,640 --> 00:02:01,040
software development life cycle

00:01:58,719 --> 00:02:02,000
we know that it comprises of a series of

00:02:01,040 --> 00:02:04,240
steps

00:02:02,000 --> 00:02:06,560
so for example you start off with

00:02:04,240 --> 00:02:09,360
creating your source code for your

00:02:06,560 --> 00:02:11,120
software or application you go ahead and

00:02:09,360 --> 00:02:14,000
start building out this

00:02:11,120 --> 00:02:16,160
software or application you go ahead and

00:02:14,000 --> 00:02:18,959
create your

00:02:16,160 --> 00:02:20,239
integration testing uh to make sure your

00:02:18,959 --> 00:02:21,760
application is meeting all the

00:02:20,239 --> 00:02:23,200
requirements

00:02:21,760 --> 00:02:25,440
finally once you're happy with these

00:02:23,200 --> 00:02:27,200
tests go ahead and deploy it into a

00:02:25,440 --> 00:02:30,480
production environment

00:02:27,200 --> 00:02:32,480
and lastly you have monitoring in place

00:02:30,480 --> 00:02:35,599
to absorb the performance of this

00:02:32,480 --> 00:02:38,720
software in your production environment

00:02:35,599 --> 00:02:40,319
and typically all of this takes place in

00:02:38,720 --> 00:02:43,840
a couple of minutes

00:02:40,319 --> 00:02:46,080
so meaning that this entire end to end

00:02:43,840 --> 00:02:47,440
sort of life cycle is pretty rapid and

00:02:46,080 --> 00:02:49,840
it's pretty fast

00:02:47,440 --> 00:02:50,879
now this is not entirely the case when

00:02:49,840 --> 00:02:53,519
you consider

00:02:50,879 --> 00:02:55,040
a machine learning development lifecycle

00:02:53,519 --> 00:02:57,440
for example

00:02:55,040 --> 00:02:58,720
so machine learning model deployments

00:02:57,440 --> 00:03:01,280
are quite different

00:02:58,720 --> 00:03:02,400
in nature as compared to your software

00:03:01,280 --> 00:03:05,280
deployments

00:03:02,400 --> 00:03:05,760
a recent survey conducted by algorithma

00:03:05,280 --> 00:03:09,200
actually

00:03:05,760 --> 00:03:10,480
found that 55 of companies have not even

00:03:09,200 --> 00:03:13,200
been able to deploy

00:03:10,480 --> 00:03:14,879
their machine learning models and those

00:03:13,200 --> 00:03:16,400
which managed to have these machine

00:03:14,879 --> 00:03:18,879
learning models deployed

00:03:16,400 --> 00:03:20,080
it was seen that very few of them around

00:03:18,879 --> 00:03:22,800
00:03:20,080 --> 00:03:23,680
managed to have it within a week uh some

00:03:22,800 --> 00:03:26,000
of them or

00:03:23,680 --> 00:03:28,159
vastly a majority of them around 50 of

00:03:26,000 --> 00:03:28,640
them managed to have it within a time

00:03:28,159 --> 00:03:31,360
frame

00:03:28,640 --> 00:03:32,080
of one week to three months and then you

00:03:31,360 --> 00:03:35,519
had a small

00:03:32,080 --> 00:03:37,599
portion who were able to do it within uh

00:03:35,519 --> 00:03:39,360
more than three months and some even

00:03:37,599 --> 00:03:41,440
taking a couple of years

00:03:39,360 --> 00:03:43,440
so clearly we can see that there is some

00:03:41,440 --> 00:03:45,840
sort of discrepancy between

00:03:43,440 --> 00:03:46,879
the fast release of software

00:03:45,840 --> 00:03:49,760
applications

00:03:46,879 --> 00:03:50,159
versus your machine learning deployment

00:03:49,760 --> 00:03:53,760
so

00:03:50,159 --> 00:03:56,720
why is this so now when you look at

00:03:53,760 --> 00:03:57,599
a machine learning model deployment

00:03:56,720 --> 00:03:59,680
process

00:03:57,599 --> 00:04:01,360
the core component is basically training

00:03:59,680 --> 00:04:03,120
your machine learning model

00:04:01,360 --> 00:04:04,799
so when you're training the solution

00:04:03,120 --> 00:04:06,000
machine learning model one of the

00:04:04,799 --> 00:04:09,120
challenges we see

00:04:06,000 --> 00:04:11,840
is that the code that you have for your

00:04:09,120 --> 00:04:14,000
model training purposes is independent

00:04:11,840 --> 00:04:16,079
from the data itself

00:04:14,000 --> 00:04:17,199
and also the machine learning model is

00:04:16,079 --> 00:04:19,600
the resulting

00:04:17,199 --> 00:04:22,000
artifact that comes out of this model

00:04:19,600 --> 00:04:24,320
training process

00:04:22,000 --> 00:04:27,120
so in your code of your machine learning

00:04:24,320 --> 00:04:29,520
model it's basically where you define

00:04:27,120 --> 00:04:30,880
all the algorithms that you're looking

00:04:29,520 --> 00:04:33,919
into for training your

00:04:30,880 --> 00:04:36,000
machine learning model so this could be

00:04:33,919 --> 00:04:38,080
deep neural networks for instance it

00:04:36,000 --> 00:04:40,479
could be clustering algorithms

00:04:38,080 --> 00:04:42,240
uh it could be regression based models

00:04:40,479 --> 00:04:43,759
that you're looking into and

00:04:42,240 --> 00:04:45,680
this is where you're actually defining

00:04:43,759 --> 00:04:47,520
the architecture of your model

00:04:45,680 --> 00:04:49,040
along with configuring various

00:04:47,520 --> 00:04:52,240
parameters required

00:04:49,040 --> 00:04:54,720
to train these machine learning models

00:04:52,240 --> 00:04:56,000
the data is of course a very important

00:04:54,720 --> 00:04:58,240
aspect for your machine learning

00:04:56,000 --> 00:04:59,840
model you need to validate your data you

00:04:58,240 --> 00:05:02,240
need to clean it up

00:04:59,840 --> 00:05:03,199
and also ensure that you're able to

00:05:02,240 --> 00:05:05,440
shuffle it

00:05:03,199 --> 00:05:06,800
um accordingly and extract all the

00:05:05,440 --> 00:05:07,919
relevant features

00:05:06,800 --> 00:05:10,320
that are needed for your machine

00:05:07,919 --> 00:05:12,560
learning model training

00:05:10,320 --> 00:05:13,680
and finally you'll have uh the machine

00:05:12,560 --> 00:05:16,880
learning model

00:05:13,680 --> 00:05:19,039
uh created ultimately out of this and we

00:05:16,880 --> 00:05:19,919
basically look at all its outputs we

00:05:19,039 --> 00:05:22,720
look at

00:05:19,919 --> 00:05:24,560
uh the predictions of these models uh we

00:05:22,720 --> 00:05:25,759
try to understand the performance the

00:05:24,560 --> 00:05:28,880
accuracy

00:05:25,759 --> 00:05:30,240
um and we look at the key metrics for

00:05:28,880 --> 00:05:32,400
looking at the

00:05:30,240 --> 00:05:33,840
observing the performance of this model

00:05:32,400 --> 00:05:37,039
and once you're happy with it

00:05:33,840 --> 00:05:39,840
we go ahead and try to get it deployed

00:05:37,039 --> 00:05:42,800
into production

00:05:39,840 --> 00:05:43,440
so as you can see the machine learning

00:05:42,800 --> 00:05:45,520
code

00:05:43,440 --> 00:05:46,479
is only a small part of this entire

00:05:45,520 --> 00:05:48,960
solution

00:05:46,479 --> 00:05:50,080
there are multiple components involved

00:05:48,960 --> 00:05:52,639
and

00:05:50,080 --> 00:05:55,280
acting sort of dependently throughout

00:05:52,639 --> 00:05:58,960
this process

00:05:55,280 --> 00:06:00,160
so often we also see various teams who

00:05:58,960 --> 00:06:02,560
are involved

00:06:00,160 --> 00:06:04,639
um in this entire machine learning uh

00:06:02,560 --> 00:06:07,520
model development process right

00:06:04,639 --> 00:06:09,199
so for instance you have your data

00:06:07,520 --> 00:06:12,560
engineers who are looking

00:06:09,199 --> 00:06:13,520
at um consolidating data from various

00:06:12,560 --> 00:06:15,199
sources

00:06:13,520 --> 00:06:17,680
making it easily available and

00:06:15,199 --> 00:06:20,639
accessible for your data scientists

00:06:17,680 --> 00:06:22,479
you have your data scientists who are

00:06:20,639 --> 00:06:24,479
mainly concerned with

00:06:22,479 --> 00:06:26,240
extracting this data building the

00:06:24,479 --> 00:06:28,240
machine learning model for it

00:06:26,240 --> 00:06:30,639
and in our team we use a tool called

00:06:28,240 --> 00:06:32,880
jupiter hub or jupyter notebooks

00:06:30,639 --> 00:06:34,639
for actually writing up the code for

00:06:32,880 --> 00:06:37,039
this machine learning model

00:06:34,639 --> 00:06:38,080
and once we have this machine learning

00:06:37,039 --> 00:06:40,479
model available

00:06:38,080 --> 00:06:41,520
by the data scientists they typically

00:06:40,479 --> 00:06:43,680
work with

00:06:41,520 --> 00:06:45,360
devops engineers or they work with other

00:06:43,680 --> 00:06:47,120
developers on the team

00:06:45,360 --> 00:06:48,880
to have this machine learning model

00:06:47,120 --> 00:06:51,759
running into your

00:06:48,880 --> 00:06:53,199
production or openshift platforms for

00:06:51,759 --> 00:06:55,680
example

00:06:53,199 --> 00:06:56,880
now uh having worked with data

00:06:55,680 --> 00:06:59,520
scientists myself

00:06:56,880 --> 00:07:01,120
and in fact being involved in machine

00:06:59,520 --> 00:07:03,520
learning projects myself

00:07:01,120 --> 00:07:05,919
i've often seen that data scientists are

00:07:03,520 --> 00:07:08,000
now sort of also being responsible

00:07:05,919 --> 00:07:09,440
for managing the deployments of their

00:07:08,000 --> 00:07:11,680
models as well

00:07:09,440 --> 00:07:13,840
so it's equally important that the data

00:07:11,680 --> 00:07:16,319
scientists are also very familiar

00:07:13,840 --> 00:07:17,520
with the devops principles with the gear

00:07:16,319 --> 00:07:20,160
ops principles

00:07:17,520 --> 00:07:24,080
so that it makes it easier for them to

00:07:20,160 --> 00:07:24,080
understand this end-to-end process

00:07:24,560 --> 00:07:28,400
so with that uh comes the importance of

00:07:27,759 --> 00:07:31,360
europe

00:07:28,400 --> 00:07:33,520
right so like i said in our team we have

00:07:31,360 --> 00:07:35,120
multiple data scientists who are

00:07:33,520 --> 00:07:37,280
uh collaborating who are working

00:07:35,120 --> 00:07:39,520
together to get the code

00:07:37,280 --> 00:07:41,759
um for your machine learning models and

00:07:39,520 --> 00:07:42,400
hence we need a central location to sort

00:07:41,759 --> 00:07:44,639
of

00:07:42,400 --> 00:07:45,440
um consolidate all of these models and

00:07:44,639 --> 00:07:48,720
data

00:07:45,440 --> 00:07:51,919
for which we use github or gitlab

00:07:48,720 --> 00:07:54,879
and this basically allows you to track

00:07:51,919 --> 00:07:55,919
and modify any other changes that you

00:07:54,879 --> 00:07:57,440
might need

00:07:55,919 --> 00:07:59,199
similar to how your software

00:07:57,440 --> 00:08:00,560
applications need to be version

00:07:59,199 --> 00:08:02,240
controlled in git

00:08:00,560 --> 00:08:03,759
we also would want the machine learning

00:08:02,240 --> 00:08:06,479
models to also be

00:08:03,759 --> 00:08:08,879
version control and tracked inside git

00:08:06,479 --> 00:08:11,440
and this also allows us to um

00:08:08,879 --> 00:08:12,800
have external contributors so they can

00:08:11,440 --> 00:08:15,360
submit their prs

00:08:12,800 --> 00:08:18,000
uh somebody who wants to sort of um

00:08:15,360 --> 00:08:19,919
improve the performance of the model

00:08:18,000 --> 00:08:21,280
uh they can just create a pull request

00:08:19,919 --> 00:08:24,080
to the repository

00:08:21,280 --> 00:08:25,919
with their changes and we have our

00:08:24,080 --> 00:08:26,560
continuous integration pipelines in

00:08:25,919 --> 00:08:28,720
place

00:08:26,560 --> 00:08:29,599
so this can be like a jenkins pipeline

00:08:28,720 --> 00:08:32,000
which is

00:08:29,599 --> 00:08:32,800
basically uh checking uh whether your

00:08:32,000 --> 00:08:35,919
code

00:08:32,800 --> 00:08:38,640
successfully goes through and um if so

00:08:35,919 --> 00:08:39,440
it basically containerizes your entire

00:08:38,640 --> 00:08:41,919
application

00:08:39,440 --> 00:08:42,800
creates a container image out of it and

00:08:41,919 --> 00:08:45,519
you can then

00:08:42,800 --> 00:08:47,360
push this into your query repository and

00:08:45,519 --> 00:08:48,320
then comes the aspect of continuous

00:08:47,360 --> 00:08:49,760
deployment

00:08:48,320 --> 00:08:51,680
so you have your application

00:08:49,760 --> 00:08:54,560
manufactures again which are

00:08:51,680 --> 00:08:55,519
residing within git and our team uses

00:08:54,560 --> 00:08:58,640
argo cd

00:08:55,519 --> 00:09:01,040
as the uh deployment tool to

00:08:58,640 --> 00:09:02,320
have these applications uh deployed and

00:09:01,040 --> 00:09:05,120
having them running

00:09:02,320 --> 00:09:06,480
um within openshift platform so

00:09:05,120 --> 00:09:10,080
essentially you see that

00:09:06,480 --> 00:09:13,360
git ops is also highly um

00:09:10,080 --> 00:09:16,240
relevant for a data science perspective

00:09:13,360 --> 00:09:17,600
so now that we know this entire github's

00:09:16,240 --> 00:09:18,640
principles and how it's sort of

00:09:17,600 --> 00:09:22,240
applicable

00:09:18,640 --> 00:09:24,320
let's look at the entire workflow of how

00:09:22,240 --> 00:09:25,519
this looks like from an end-to-end

00:09:24,320 --> 00:09:28,480
machine learning model

00:09:25,519 --> 00:09:28,480
deployment process

00:09:29,360 --> 00:09:32,480
so as you can see first of all we have

00:09:31,760 --> 00:09:34,880
the

00:09:32,480 --> 00:09:36,399
data enthusiasts who are basically

00:09:34,880 --> 00:09:39,279
looking at the data

00:09:36,399 --> 00:09:40,080
um trying to figure out some insights

00:09:39,279 --> 00:09:42,080
from it

00:09:40,080 --> 00:09:44,000
uh performing all of their analysis and

00:09:42,080 --> 00:09:45,839
creating the machine learning model

00:09:44,000 --> 00:09:47,440
and like i said we use uh something

00:09:45,839 --> 00:09:50,000
called as jupiter hub

00:09:47,440 --> 00:09:50,880
um as the primary tool here you'll also

00:09:50,000 --> 00:09:53,279
see this

00:09:50,880 --> 00:09:54,560
in the demo which i have um in next

00:09:53,279 --> 00:09:57,920
couple of minutes

00:09:54,560 --> 00:10:01,040
so once they have this sort of um

00:09:57,920 --> 00:10:02,800
created and developed they basically

00:10:01,040 --> 00:10:05,600
push all of their code into

00:10:02,800 --> 00:10:06,160
git and once you have that into git is

00:10:05,600 --> 00:10:09,360
where

00:10:06,160 --> 00:10:09,680
we want to now have these containerized

00:10:09,360 --> 00:10:12,640
uh

00:10:09,680 --> 00:10:13,120
images pushed into quake for which we

00:10:12,640 --> 00:10:15,839
have

00:10:13,120 --> 00:10:16,880
uh what we call aspect on pipelines so

00:10:15,839 --> 00:10:20,560
this tecton

00:10:16,880 --> 00:10:22,720
pipelines is nothing but an open source

00:10:20,560 --> 00:10:24,240
um end-to-end framework basically which

00:10:22,720 --> 00:10:27,200
allows you to create your

00:10:24,240 --> 00:10:28,000
ci cd systems allowing developers to

00:10:27,200 --> 00:10:31,680
build

00:10:28,000 --> 00:10:32,560
uh test and deploy these um applications

00:10:31,680 --> 00:10:34,880
across

00:10:32,560 --> 00:10:36,079
uh contain across various cloud uh

00:10:34,880 --> 00:10:39,279
platforms

00:10:36,079 --> 00:10:42,480
and we work with our um uh

00:10:39,279 --> 00:10:44,839
thought team which basically is the ai

00:10:42,480 --> 00:10:47,519
uh devsec ops teams within our

00:10:44,839 --> 00:10:48,480
organization and they primarily work a

00:10:47,519 --> 00:10:50,720
lot with

00:10:48,480 --> 00:10:51,920
setting up this kind of continuous

00:10:50,720 --> 00:10:54,399
integration and

00:10:51,920 --> 00:10:55,279
continuous deployment pipelines so we

00:10:54,399 --> 00:10:58,000
were able to

00:10:55,279 --> 00:10:59,440
get their help to have this configured

00:10:58,000 --> 00:11:02,320
for our use case

00:10:59,440 --> 00:11:02,800
so essentially what this tecton pipeline

00:11:02,320 --> 00:11:05,760
does

00:11:02,800 --> 00:11:07,360
is uh for all the changes that we submit

00:11:05,760 --> 00:11:09,680
and we push it into

00:11:07,360 --> 00:11:12,240
your github repository uh you can

00:11:09,680 --> 00:11:14,079
associate your changes with the new tag

00:11:12,240 --> 00:11:16,560
just like how you have new releases and

00:11:14,079 --> 00:11:17,120
new tags uh for software development we

00:11:16,560 --> 00:11:19,440
do this

00:11:17,120 --> 00:11:21,680
follow the same process uh we make all

00:11:19,440 --> 00:11:23,680
our changes associated with the tag

00:11:21,680 --> 00:11:25,360
and when you create this new tag inside

00:11:23,680 --> 00:11:28,240
your github repository

00:11:25,360 --> 00:11:28,959
um it triggers this tecton pipeline

00:11:28,240 --> 00:11:30,959
right

00:11:28,959 --> 00:11:32,720
so what the pipeline does is it triggers

00:11:30,959 --> 00:11:34,160
that there is a new tag being created

00:11:32,720 --> 00:11:37,600
for your repository

00:11:34,160 --> 00:11:40,560
um it creates the container for your

00:11:37,600 --> 00:11:43,040
entire machine learning application code

00:11:40,560 --> 00:11:46,240
and it pushes this container image

00:11:43,040 --> 00:11:49,040
into quake so quay is nothing but your

00:11:46,240 --> 00:11:50,320
container image uh registry which allows

00:11:49,040 --> 00:11:52,480
you to store

00:11:50,320 --> 00:11:53,600
build distribute and deploy your

00:11:52,480 --> 00:11:56,079
containers

00:11:53,600 --> 00:11:58,320
so uh that's the pipeline in basically

00:11:56,079 --> 00:12:00,160
in the configuration we specify

00:11:58,320 --> 00:12:02,320
um which is the query repository you

00:12:00,160 --> 00:12:03,680
want this image to go into

00:12:02,320 --> 00:12:05,760
and uh there are certain other

00:12:03,680 --> 00:12:08,720
configurations that we would need to

00:12:05,760 --> 00:12:11,200
um set up inside the repository and once

00:12:08,720 --> 00:12:13,120
that's done um it sort of

00:12:11,200 --> 00:12:14,560
uh pushes it on its own it takes a

00:12:13,120 --> 00:12:16,560
couple of minutes to run

00:12:14,560 --> 00:12:17,839
through all the checks and then makes

00:12:16,560 --> 00:12:20,639
that image available now

00:12:17,839 --> 00:12:22,079
inside quick and this image can now be

00:12:20,639 --> 00:12:23,600
used inside your

00:12:22,079 --> 00:12:25,839
production which is the third and final

00:12:23,600 --> 00:12:28,000
step so you can use this

00:12:25,839 --> 00:12:29,040
to sort of um expose your machine

00:12:28,000 --> 00:12:31,200
learning model

00:12:29,040 --> 00:12:33,440
let's say you want to expose it as a web

00:12:31,200 --> 00:12:35,680
service um you want to sort of

00:12:33,440 --> 00:12:37,440
uh publish all your predictions of your

00:12:35,680 --> 00:12:40,160
machine learning model

00:12:37,440 --> 00:12:41,920
into some kind of web application so you

00:12:40,160 --> 00:12:45,360
basically can do all of that

00:12:41,920 --> 00:12:48,000
and have that final um deployment

00:12:45,360 --> 00:12:50,959
sort of customized for your use case and

00:12:48,000 --> 00:12:53,920
like i said in our team we use argo cb

00:12:50,959 --> 00:12:54,959
for sort of managing these deployment

00:12:53,920 --> 00:12:57,760
manifests

00:12:54,959 --> 00:12:59,120
and ultimately let's say your goal is to

00:12:57,760 --> 00:13:01,360
have some kind of

00:12:59,120 --> 00:13:02,880
dashboard or some kind of insights that

00:13:01,360 --> 00:13:04,639
you want to share

00:13:02,880 --> 00:13:07,040
about what this model is doing with your

00:13:04,639 --> 00:13:09,440
stakeholders and you could also

00:13:07,040 --> 00:13:10,880
um have that exposed as well as part of

00:13:09,440 --> 00:13:13,680
your entire

00:13:10,880 --> 00:13:15,120
pipeline so this is basically the

00:13:13,680 --> 00:13:17,760
overall workflow

00:13:15,120 --> 00:13:20,639
and in the demo i mainly want to focus

00:13:17,760 --> 00:13:23,360
um particularly on step one and step two

00:13:20,639 --> 00:13:24,639
uh basically seeing for what is it like

00:13:23,360 --> 00:13:26,480
for a data scientist

00:13:24,639 --> 00:13:29,040
to interact with this sort of ci

00:13:26,480 --> 00:13:29,040
pipeline

00:13:29,519 --> 00:13:33,200
so now that we know this entire workflow

00:13:32,320 --> 00:13:36,240
you can sort of

00:13:33,200 --> 00:13:38,399
get an idea that most of the customers

00:13:36,240 --> 00:13:39,920
sort of questions or requirements

00:13:38,399 --> 00:13:41,680
from an end-to-end machine learning

00:13:39,920 --> 00:13:42,560
perspective is sort of being addressed

00:13:41,680 --> 00:13:44,720
over here

00:13:42,560 --> 00:13:46,320
starting with how do we have our code

00:13:44,720 --> 00:13:48,000
and how we have our simulation and

00:13:46,320 --> 00:13:49,519
deployments to production

00:13:48,000 --> 00:13:51,279
and our solution to this is we're

00:13:49,519 --> 00:13:55,440
looking at a container based

00:13:51,279 --> 00:13:57,279
cicd approach how do we want to look at

00:13:55,440 --> 00:13:59,279
maintaining and managing all these

00:13:57,279 --> 00:14:02,000
declarative configurations

00:13:59,279 --> 00:14:02,800
our approach to that is solved with our

00:14:02,000 --> 00:14:04,720
end-to-end

00:14:02,800 --> 00:14:06,720
git ops uh sort of framework that we

00:14:04,720 --> 00:14:08,800
have set up

00:14:06,720 --> 00:14:10,000
and of course how do we do all of our

00:14:08,800 --> 00:14:12,160
data processing

00:14:10,000 --> 00:14:13,839
we're using all the open source aiml

00:14:12,160 --> 00:14:16,240
stacks which are available

00:14:13,839 --> 00:14:17,839
um like i've mentioned jupiter jupiter

00:14:16,240 --> 00:14:20,000
hub or jupiter notebooks

00:14:17,839 --> 00:14:22,399
is one of the primary open source tools

00:14:20,000 --> 00:14:25,040
we use to do all of our data wrangling

00:14:22,399 --> 00:14:27,760
and data processing

00:14:25,040 --> 00:14:29,360
and finally where do we do all your

00:14:27,760 --> 00:14:30,399
machine learning model training and

00:14:29,360 --> 00:14:32,639
deployment

00:14:30,399 --> 00:14:33,760
and pushing it finally into production

00:14:32,639 --> 00:14:36,720
and for this

00:14:33,760 --> 00:14:37,920
is where we actually use the open data

00:14:36,720 --> 00:14:41,440
hub platform

00:14:37,920 --> 00:14:44,000
which has um cicd enabled with it

00:14:41,440 --> 00:14:44,720
and if if you had the chance to sort of

00:14:44,000 --> 00:14:47,360
attend

00:14:44,720 --> 00:14:48,480
uh the keynote today um you would have

00:14:47,360 --> 00:14:50,560
uh heard about

00:14:48,480 --> 00:14:51,600
uh this open data hub and this operate

00:14:50,560 --> 00:14:55,040
first sort of

00:14:51,600 --> 00:14:55,680
initiative um if not we also had a prior

00:14:55,040 --> 00:14:58,560
talk

00:14:55,680 --> 00:15:00,560
i believe by michael and tom who sort of

00:14:58,560 --> 00:15:03,920
gave a walk through of what exactly

00:15:00,560 --> 00:15:07,360
is um the open data hub platform so

00:15:03,920 --> 00:15:09,839
before i sort of um dive into the demo

00:15:07,360 --> 00:15:12,399
aspect of it i will be using uh one

00:15:09,839 --> 00:15:14,959
component of open data hubs so

00:15:12,399 --> 00:15:15,519
for those who may not have heard about

00:15:14,959 --> 00:15:18,320
it

00:15:15,519 --> 00:15:20,000
um open data hub is an end-to-end ai

00:15:18,320 --> 00:15:23,279
machine learning platform

00:15:20,000 --> 00:15:24,880
um available here at red hat and it

00:15:23,279 --> 00:15:27,440
basically integrates

00:15:24,880 --> 00:15:28,320
a collection of multiple open source

00:15:27,440 --> 00:15:30,639
projects

00:15:28,320 --> 00:15:32,399
which can be used as from a data

00:15:30,639 --> 00:15:34,000
scientist perspective which can be used

00:15:32,399 --> 00:15:35,600
as a data engineer

00:15:34,000 --> 00:15:38,000
which can be used from a monitoring

00:15:35,600 --> 00:15:40,959
perspective so basically integrates

00:15:38,000 --> 00:15:43,279
all these bunch of tools and um some of

00:15:40,959 --> 00:15:46,399
the tools like i said are jupiter hub

00:15:43,279 --> 00:15:48,240
arbo cd uh we have monitoring tools like

00:15:46,399 --> 00:15:51,279
prometheus and grafana

00:15:48,240 --> 00:15:54,560
uh we have visualization tools like um

00:15:51,279 --> 00:15:57,040
apache superset and various other

00:15:54,560 --> 00:15:59,279
open source tools available so if you're

00:15:57,040 --> 00:16:01,320
interested to learn more about this um

00:15:59,279 --> 00:16:02,720
i would encourage you to go to

00:16:01,320 --> 00:16:05,120
opendatahub.io

00:16:02,720 --> 00:16:06,240
um and we also have a community over

00:16:05,120 --> 00:16:08,320
there available

00:16:06,240 --> 00:16:10,480
or looking for any contributions as well

00:16:08,320 --> 00:16:12,560
and if you're simply just interested

00:16:10,480 --> 00:16:14,160
um to sort of play around with this i

00:16:12,560 --> 00:16:17,199
would definitely suggest to

00:16:14,160 --> 00:16:20,079
um have a look at that so

00:16:17,199 --> 00:16:20,880
with that i think i'll go ahead um and

00:16:20,079 --> 00:16:25,759
move on

00:16:20,880 --> 00:16:31,680
to the demo so let me go ahead and

00:16:25,759 --> 00:16:31,680
quickly switch screens a bit

00:16:35,120 --> 00:16:41,040
all right so this is um

00:16:38,240 --> 00:16:42,639
the jupiter hub instance we have uh

00:16:41,040 --> 00:16:45,360
publicly available

00:16:42,639 --> 00:16:46,160
um as part of our operate first um

00:16:45,360 --> 00:16:48,880
initiative

00:16:46,160 --> 00:16:50,000
so you don't need to be connected to vpn

00:16:48,880 --> 00:16:52,639
or anything of that so

00:16:50,000 --> 00:16:53,199
it's publicly available um i'll also try

00:16:52,639 --> 00:16:56,320
to

00:16:53,199 --> 00:16:56,320
drop it in chat

00:16:58,560 --> 00:17:02,160
in case you guys are interested uh later

00:17:01,279 --> 00:17:04,480
on to sort of

00:17:02,160 --> 00:17:05,360
see where this is so when you go to this

00:17:04,480 --> 00:17:07,520
url

00:17:05,360 --> 00:17:08,880
um you see this button to sign in with

00:17:07,520 --> 00:17:11,199
openshift

00:17:08,880 --> 00:17:12,000
um so you can go ahead and click over

00:17:11,199 --> 00:17:14,480
here

00:17:12,000 --> 00:17:15,280
uh we have this you you basically this

00:17:14,480 --> 00:17:18,480
entire

00:17:15,280 --> 00:17:19,679
hub instance is hosted on the mass open

00:17:18,480 --> 00:17:21,839
cloud

00:17:19,679 --> 00:17:24,319
cluster so that's where it's currently

00:17:21,839 --> 00:17:26,400
hosted at so this is the login for that

00:17:24,319 --> 00:17:27,600
um you can go ahead and click on google

00:17:26,400 --> 00:17:30,160
account

00:17:27,600 --> 00:17:33,200
and i'll go ahead and log in with my red

00:17:30,160 --> 00:17:35,679
hat email id and once that's done

00:17:33,200 --> 00:17:37,919
uh jupiter hub actually throws this

00:17:35,679 --> 00:17:39,840
spawner ui option for you

00:17:37,919 --> 00:17:41,919
so it has a bunch of notebook images

00:17:39,840 --> 00:17:44,720
that you can actually spawn from

00:17:41,919 --> 00:17:45,520
so for each of our projects we have

00:17:44,720 --> 00:17:48,320
actually

00:17:45,520 --> 00:17:49,200
created a customized notebook image for

00:17:48,320 --> 00:17:52,480
every project

00:17:49,200 --> 00:17:54,080
and we've made them available here and

00:17:52,480 --> 00:17:55,919
apart from that we also have some

00:17:54,080 --> 00:17:57,200
default notebook images that you can

00:17:55,919 --> 00:17:58,960
play around with

00:17:57,200 --> 00:18:00,480
um if you're interested in playing

00:17:58,960 --> 00:18:02,160
around with tensorflow

00:18:00,480 --> 00:18:04,640
if you're interested to play around with

00:18:02,160 --> 00:18:06,799
spark these are some notebook images

00:18:04,640 --> 00:18:09,120
that are supported

00:18:06,799 --> 00:18:10,400
for this demo i'll go ahead and just

00:18:09,120 --> 00:18:14,720
spawn a simple

00:18:10,400 --> 00:18:16,080
default minimal notebook and once that's

00:18:14,720 --> 00:18:19,520
done you go ahead

00:18:16,080 --> 00:18:21,280
and click on start and then it'll take a

00:18:19,520 --> 00:18:24,320
few minutes to just get the

00:18:21,280 --> 00:18:24,880
um pvc attached for your particular

00:18:24,320 --> 00:18:26,720
server

00:18:24,880 --> 00:18:28,880
and uh once that's done you should be

00:18:26,720 --> 00:18:30,559
able to uh be successful into the

00:18:28,880 --> 00:18:34,000
jupiter hub ui

00:18:30,559 --> 00:18:36,960
so while that's doing um taking time to

00:18:34,000 --> 00:18:37,919
spin up i want to go through uh this

00:18:36,960 --> 00:18:39,600
particular

00:18:37,919 --> 00:18:41,360
machine learning use case that i'm

00:18:39,600 --> 00:18:43,760
showing in my demo today

00:18:41,360 --> 00:18:45,039
uh so this is the repository um i've

00:18:43,760 --> 00:18:47,440
created a fork of this

00:18:45,039 --> 00:18:49,200
uh this is the upstream repository

00:18:47,440 --> 00:18:51,520
within our uh

00:18:49,200 --> 00:18:53,919
teams organization i've just taken a

00:18:51,520 --> 00:18:56,080
fork of it for the purpose of this demo

00:18:53,919 --> 00:18:57,360
so what we're essentially doing in this

00:18:56,080 --> 00:19:00,400
project is

00:18:57,360 --> 00:19:01,120
we are trying to identify um if we can

00:19:00,400 --> 00:19:04,080
apply

00:19:01,120 --> 00:19:06,240
ai and machine learning for identifying

00:19:04,080 --> 00:19:09,360
flaky tests or predicting

00:19:06,240 --> 00:19:12,080
any test failures um that we see in our

00:19:09,360 --> 00:19:14,640
continuous integration testing pipelines

00:19:12,080 --> 00:19:15,679
so um the ultimate goal of this is

00:19:14,640 --> 00:19:18,960
basically to

00:19:15,679 --> 00:19:21,919
alleviate uh developers from manual time

00:19:18,960 --> 00:19:22,559
um sort of debugging why exactly did a

00:19:21,919 --> 00:19:24,880
certain

00:19:22,559 --> 00:19:25,760
test field um so we just want to

00:19:24,880 --> 00:19:27,760
understand

00:19:25,760 --> 00:19:29,760
the ci pipeline a little bit more in

00:19:27,760 --> 00:19:32,080
depth and see if we can come up with

00:19:29,760 --> 00:19:34,960
some kind of intelligent solution

00:19:32,080 --> 00:19:35,600
um allowing developers more insights

00:19:34,960 --> 00:19:38,720
into

00:19:35,600 --> 00:19:40,320
what the ci sort of deaths look like

00:19:38,720 --> 00:19:42,080
and the data set that we're actually

00:19:40,320 --> 00:19:44,720
looking at is the test grid

00:19:42,080 --> 00:19:45,760
data set so test grid data set is

00:19:44,720 --> 00:19:49,120
publicly

00:19:45,760 --> 00:19:52,799
um made available by uh google cloud

00:19:49,120 --> 00:19:54,640
platform so they've made this uh tesco

00:19:52,799 --> 00:19:57,120
uh data available that we're actually

00:19:54,640 --> 00:20:00,080
looking at which basically gives us

00:19:57,120 --> 00:20:02,080
um a combination of tests that have run

00:20:00,080 --> 00:20:03,919
on different openshift platforms or on

00:20:02,080 --> 00:20:06,559
different red hat platforms

00:20:03,919 --> 00:20:07,919
um along with the timestamps of when

00:20:06,559 --> 00:20:10,559
these tests were

00:20:07,919 --> 00:20:11,120
run uh whether the tests passed whether

00:20:10,559 --> 00:20:14,159
the tests

00:20:11,120 --> 00:20:15,840
failed or would be flaky in nature so

00:20:14,159 --> 00:20:18,080
that's essentially what this

00:20:15,840 --> 00:20:19,120
uh project is trying to do it's still a

00:20:18,080 --> 00:20:22,159
very early

00:20:19,120 --> 00:20:24,320
um sort of exploratory phase right now

00:20:22,159 --> 00:20:26,000
but um i thought it would be uh

00:20:24,320 --> 00:20:29,039
interesting project to look at

00:20:26,000 --> 00:20:32,159
um from a data science perspective so

00:20:29,039 --> 00:20:35,360
let's see if um which will have it up

00:20:32,159 --> 00:20:36,880
so yes the server came up and uh once

00:20:35,360 --> 00:20:39,600
that's up you see

00:20:36,880 --> 00:20:40,559
um a bunch of folders so this basically

00:20:39,600 --> 00:20:44,080
i've been

00:20:40,559 --> 00:20:46,720
um getting all my forks and repositories

00:20:44,080 --> 00:20:48,720
inside over here so you can see the

00:20:46,720 --> 00:20:52,159
ocpci analysis

00:20:48,720 --> 00:20:53,200
uh repo i've already cloned that over

00:20:52,159 --> 00:20:56,400
here

00:20:53,200 --> 00:21:01,280
and you also have a terminal

00:20:56,400 --> 00:21:03,210
uh which again allows you to easily

00:21:01,280 --> 00:21:04,400
interact with your um

00:21:03,210 --> 00:21:06,159
[Music]

00:21:04,400 --> 00:21:08,720
repositories that you want to base off

00:21:06,159 --> 00:21:11,440
of so you can see i have already set up

00:21:08,720 --> 00:21:14,559
my upstream repo and i have my

00:21:11,440 --> 00:21:17,200
local for people as well

00:21:14,559 --> 00:21:17,760
so let's go ahead and look at some of

00:21:17,200 --> 00:21:20,880
the

00:21:17,760 --> 00:21:23,280
notebooks so as i mentioned um we're

00:21:20,880 --> 00:21:25,280
looking at the test grid data set

00:21:23,280 --> 00:21:26,960
so i have this notebook here called test

00:21:25,280 --> 00:21:30,320
grid clustering dot

00:21:26,960 --> 00:21:32,240
um ipynb so that's the

00:21:30,320 --> 00:21:33,520
extension of your notebooks when you

00:21:32,240 --> 00:21:36,080
create this

00:21:33,520 --> 00:21:36,880
uh code within jupiter hub and uh

00:21:36,080 --> 00:21:40,320
typically

00:21:36,880 --> 00:21:42,799
it's uh solely based on typing

00:21:40,320 --> 00:21:43,520
so most of the packages and everything

00:21:42,799 --> 00:21:46,480
would be

00:21:43,520 --> 00:21:47,840
um dependent on python so in this

00:21:46,480 --> 00:21:51,280
notebook what we're doing

00:21:47,840 --> 00:21:52,240
is um basically going over the discrete

00:21:51,280 --> 00:21:54,240
data set

00:21:52,240 --> 00:21:56,320
and try to see if we could find some

00:21:54,240 --> 00:22:00,000
interesting features out of it

00:21:56,320 --> 00:22:01,600
and um like i said we have a bunch of

00:22:00,000 --> 00:22:03,440
python packages that we are sort of

00:22:01,600 --> 00:22:05,360
using throughout this notebook

00:22:03,440 --> 00:22:06,640
so we start go ahead and start importing

00:22:05,360 --> 00:22:09,840
them and we define these

00:22:06,640 --> 00:22:13,600
in our fifth files as well which

00:22:09,840 --> 00:22:15,520
um reminds me to

00:22:13,600 --> 00:22:17,919
install all of these dependencies so

00:22:15,520 --> 00:22:21,120
that the notebook runs without any

00:22:17,919 --> 00:22:23,440
um issues and uh any dependency issues

00:22:21,120 --> 00:22:24,400
so while that's um installing all of

00:22:23,440 --> 00:22:27,280
these um

00:22:24,400 --> 00:22:29,200
packages um i'll just go over what the

00:22:27,280 --> 00:22:31,039
notebook is doing

00:22:29,200 --> 00:22:32,320
so as you can see here we have the test

00:22:31,039 --> 00:22:35,679
script data set right

00:22:32,320 --> 00:22:38,000
so we have uh tests uh each individual

00:22:35,679 --> 00:22:40,640
tests on this column on the left side

00:22:38,000 --> 00:22:42,159
um and we here here we have all the time

00:22:40,640 --> 00:22:45,200
stamps associated

00:22:42,159 --> 00:22:46,159
for every test and uh all the different

00:22:45,200 --> 00:22:48,720
runs

00:22:46,159 --> 00:22:49,520
executed for each of these tests um if

00:22:48,720 --> 00:22:52,080
the tests

00:22:49,520 --> 00:22:52,720
failed if the tests were successful if

00:22:52,080 --> 00:22:55,600
they were

00:22:52,720 --> 00:22:57,679
having any sort of shaky nature um all

00:22:55,600 --> 00:23:00,960
of these are sort of indicated

00:22:57,679 --> 00:23:02,799
in this test screen data set so we are

00:23:00,960 --> 00:23:05,200
basically trying to look at

00:23:02,799 --> 00:23:06,720
this these different grids that we

00:23:05,200 --> 00:23:08,000
obtain from test grid

00:23:06,720 --> 00:23:10,400
we're trying to see if there are any

00:23:08,000 --> 00:23:12,159
duplicates such grids based on different

00:23:10,400 --> 00:23:14,880
platforms where we're collecting

00:23:12,159 --> 00:23:16,000
this data from um just simple answering

00:23:14,880 --> 00:23:18,480
simple questions

00:23:16,000 --> 00:23:19,600
um like are there any subgroups within

00:23:18,480 --> 00:23:22,799
these tests

00:23:19,600 --> 00:23:25,440
uh which tests do all the sort of groups

00:23:22,799 --> 00:23:28,880
share in common um and just sort of

00:23:25,440 --> 00:23:31,760
looking at this data from a

00:23:28,880 --> 00:23:32,720
analytical point of view and then having

00:23:31,760 --> 00:23:34,799
all of these

00:23:32,720 --> 00:23:37,120
sort of key metrics that we want to

00:23:34,799 --> 00:23:39,840
track like what was the test

00:23:37,120 --> 00:23:41,440
case um test coverage percentage what

00:23:39,840 --> 00:23:44,720
were the total number of tests

00:23:41,440 --> 00:23:47,120
and so on and how can we also compare

00:23:44,720 --> 00:23:49,919
these tests if there was any common

00:23:47,120 --> 00:23:51,279
sort of behavior among them um so that

00:23:49,919 --> 00:23:54,080
that's more of the

00:23:51,279 --> 00:23:55,600
analytical steps that we're doing within

00:23:54,080 --> 00:23:58,480
these notebook

00:23:55,600 --> 00:24:02,480
and we're also trying to sort of plot it

00:23:58,480 --> 00:24:02,480
in a more visualized appealing manner

00:24:02,840 --> 00:24:08,159
and and the main part is basically

00:24:06,159 --> 00:24:09,279
what are the features that we want to

00:24:08,159 --> 00:24:11,520
look at right

00:24:09,279 --> 00:24:12,960
so for example i want to look at what is

00:24:11,520 --> 00:24:15,360
my test pass rate

00:24:12,960 --> 00:24:16,799
i want to look at um how many failures

00:24:15,360 --> 00:24:18,880
we've been having what is the sort of

00:24:16,799 --> 00:24:21,200
trend that you see in your failures

00:24:18,880 --> 00:24:22,559
is there like a failure streak um

00:24:21,200 --> 00:24:24,799
commonly observed

00:24:22,559 --> 00:24:26,799
for your tests so what is the frequency

00:24:24,799 --> 00:24:29,520
of these kind of tests so these are some

00:24:26,799 --> 00:24:29,840
um sort of features that we came up with

00:24:29,520 --> 00:24:31,279
or

00:24:29,840 --> 00:24:32,880
key metrics that we thought would be

00:24:31,279 --> 00:24:35,840
relevant for our

00:24:32,880 --> 00:24:36,880
analysis of these tests and hence we

00:24:35,840 --> 00:24:40,000
consolidate them

00:24:36,880 --> 00:24:41,120
into this uh neat data frame which has

00:24:40,000 --> 00:24:44,400
these six

00:24:41,120 --> 00:24:46,159
um key features that we wanna further

00:24:44,400 --> 00:24:47,440
build our machine learning model on top

00:24:46,159 --> 00:24:50,960
of

00:24:47,440 --> 00:24:53,440
so now that we have these features ready

00:24:50,960 --> 00:24:54,400
and what exactly we want to sort of

00:24:53,440 --> 00:24:56,559
observe

00:24:54,400 --> 00:24:58,320
we can now visualize visualize these

00:24:56,559 --> 00:25:01,600
data points that you have

00:24:58,320 --> 00:25:02,400
so um like i said here we have like six

00:25:01,600 --> 00:25:04,240
dimensions

00:25:02,400 --> 00:25:06,000
but of course if you're looking at a 2d

00:25:04,240 --> 00:25:08,799
plot you need to

00:25:06,000 --> 00:25:11,120
um sort of cut down your dimensions so

00:25:08,799 --> 00:25:13,520
that it's suitable so from a six

00:25:11,120 --> 00:25:15,200
dimensions we go into two dimensions so

00:25:13,520 --> 00:25:18,159
we have this python

00:25:15,200 --> 00:25:20,320
um function or this basically a

00:25:18,159 --> 00:25:21,679
technique called the pca technique

00:25:20,320 --> 00:25:23,840
um don't want to go into too much

00:25:21,679 --> 00:25:26,400
technical details but it's basically

00:25:23,840 --> 00:25:28,159
um a method where you can cut down these

00:25:26,400 --> 00:25:29,760
dimensions so you basically

00:25:28,159 --> 00:25:33,279
say that i want to cut it down to two

00:25:29,760 --> 00:25:36,480
dimensions and then you go ahead and

00:25:33,279 --> 00:25:38,240
plot your data points accordingly so the

00:25:36,480 --> 00:25:41,520
purpose of this plot was

00:25:38,240 --> 00:25:43,919
to actually sort of see if um

00:25:41,520 --> 00:25:44,960
as you can see these kind of points are

00:25:43,919 --> 00:25:47,679
scattered right

00:25:44,960 --> 00:25:49,200
so you can see that some seem to be sort

00:25:47,679 --> 00:25:52,320
of clustered in one group

00:25:49,200 --> 00:25:54,720
some seem to be outliers and so on

00:25:52,320 --> 00:25:56,880
so this sort of was the motivation for

00:25:54,720 --> 00:25:57,679
us to go ahead and train a clustering

00:25:56,880 --> 00:26:00,240
model

00:25:57,679 --> 00:26:01,919
right so it gives us labels on what are

00:26:00,240 --> 00:26:04,320
the different type of clusters

00:26:01,919 --> 00:26:06,720
based on the features we've defined and

00:26:04,320 --> 00:26:10,080
how each of these individual data points

00:26:06,720 --> 00:26:13,200
um get grouped into these clusters so

00:26:10,080 --> 00:26:14,559
for that we have this clustering model

00:26:13,200 --> 00:26:17,840
called db scan

00:26:14,559 --> 00:26:20,880
um so we've basically invoked

00:26:17,840 --> 00:26:22,720
to train this db scan clustering model

00:26:20,880 --> 00:26:24,640
and then finally give us the output of

00:26:22,720 --> 00:26:25,120
this clustering model so it's basically

00:26:24,640 --> 00:26:28,880
just an

00:26:25,120 --> 00:26:31,200
array so for every uh data point it's

00:26:28,880 --> 00:26:32,159
uh telling us which cluster does it

00:26:31,200 --> 00:26:35,679
belong to

00:26:32,159 --> 00:26:38,880
and so on and finally we want to save

00:26:35,679 --> 00:26:40,720
all of this cluster model so you can

00:26:38,880 --> 00:26:42,559
store it in like a pickle file you can

00:26:40,720 --> 00:26:46,240
store it in a job lib file

00:26:42,559 --> 00:26:48,880
uh depending upon your use case and once

00:26:46,240 --> 00:26:49,840
we do that i'll go ahead and run all of

00:26:48,880 --> 00:26:53,750
these

00:26:49,840 --> 00:26:55,919
cells above

00:26:53,750 --> 00:26:59,200
[Music]

00:26:55,919 --> 00:27:03,120
so yeah once you do that um

00:26:59,200 --> 00:27:06,000
you can basically um load this

00:27:03,120 --> 00:27:07,360
stored pickle file or job lib file

00:27:06,000 --> 00:27:09,760
whatever it is

00:27:07,360 --> 00:27:11,279
and we can check to see if it's the same

00:27:09,760 --> 00:27:13,200
yes we see that it's

00:27:11,279 --> 00:27:14,640
uh resulted in the same as what we've

00:27:13,200 --> 00:27:17,039
seen over here

00:27:14,640 --> 00:27:18,399
uh so now let me go ahead and just

00:27:17,039 --> 00:27:20,000
change

00:27:18,399 --> 00:27:23,039
this name just for the purpose of the

00:27:20,000 --> 00:27:26,399
demo so i'm just going to load it again

00:27:23,039 --> 00:27:29,039
um ah sorry

00:27:26,399 --> 00:27:29,440
i need to do it here as well so go ahead

00:27:29,039 --> 00:27:32,559
um

00:27:29,440 --> 00:27:34,320
store it with a different name and again

00:27:32,559 --> 00:27:36,640
load this as a different name

00:27:34,320 --> 00:27:38,399
so now you should be able to see this

00:27:36,640 --> 00:27:42,000
new typical

00:27:38,399 --> 00:27:42,960
uh model file available so now i want to

00:27:42,000 --> 00:27:45,760
actually

00:27:42,960 --> 00:27:46,159
push these changes right so you can see

00:27:45,760 --> 00:27:50,720
that

00:27:46,159 --> 00:27:50,720
there's this new model file available

00:27:59,200 --> 00:28:04,159
i do not want to modify my notebook so

00:28:02,000 --> 00:28:08,399
i'm gonna ignore

00:28:04,159 --> 00:28:12,320
that particular change that happened

00:28:08,399 --> 00:28:16,080
and i just want to add this new

00:28:12,320 --> 00:28:19,120
typical file i'm gonna see

00:28:16,080 --> 00:28:21,520
this is my updated trained

00:28:19,120 --> 00:28:21,520
model

00:28:22,320 --> 00:28:28,320
and now you should see my new latest

00:28:24,960 --> 00:28:32,480
comment over here on top

00:28:28,320 --> 00:28:35,679
and now i'm just gonna push this

00:28:32,480 --> 00:28:35,679
to my branch

00:28:36,480 --> 00:28:46,320
i've got some permissions issues i think

00:28:39,840 --> 00:28:46,320
there's all those okay

00:28:50,320 --> 00:28:53,760
try that again all right so this

00:28:52,960 --> 00:28:56,000
basically

00:28:53,760 --> 00:28:57,840
is pushing all my changes into my

00:28:56,000 --> 00:29:00,399
repository

00:28:57,840 --> 00:29:01,840
and if we go here just do a quick

00:29:00,399 --> 00:29:03,760
refresh

00:29:01,840 --> 00:29:05,600
you should see this new commit being

00:29:03,760 --> 00:29:10,960
pushed we should see

00:29:05,600 --> 00:29:10,960
our new train model

00:29:11,360 --> 00:29:15,360
over here um this testicle updated train

00:29:14,480 --> 00:29:17,440
model

00:29:15,360 --> 00:29:18,640
right so awesome now how do you want to

00:29:17,440 --> 00:29:21,039
trigger your

00:29:18,640 --> 00:29:21,840
tecton pipeline right so like i said

00:29:21,039 --> 00:29:24,559
there are these

00:29:21,840 --> 00:29:26,399
tags associated that can be configured

00:29:24,559 --> 00:29:28,799
in your repository

00:29:26,399 --> 00:29:29,600
if we go to the repository here you

00:29:28,799 --> 00:29:32,080
would see

00:29:29,600 --> 00:29:34,320
a bunch of tags so let's create a new

00:29:32,080 --> 00:29:34,320
tag

00:29:38,640 --> 00:29:48,480
and see version 0.0.5.3

00:29:44,399 --> 00:29:48,480
um and now i'm going to push

00:29:48,720 --> 00:29:55,200
this with this new tag so when you go

00:29:52,399 --> 00:29:57,520
to your repository do a quick refresh um

00:29:55,200 --> 00:29:58,799
i see this new tag has been created in

00:29:57,520 --> 00:30:00,960
my repository

00:29:58,799 --> 00:30:03,039
now the most important thing for setting

00:30:00,960 --> 00:30:06,000
up the ci pipeline is we have

00:30:03,039 --> 00:30:08,320
this um yaml file where you're actually

00:30:06,000 --> 00:30:09,200
setting up this entire ci pipeline which

00:30:08,320 --> 00:30:11,120
the thought team

00:30:09,200 --> 00:30:13,840
were happy enough to tell us how to do

00:30:11,120 --> 00:30:15,760
it um so they have also documentation as

00:30:13,840 --> 00:30:17,440
well on how you can set this up

00:30:15,760 --> 00:30:19,039
so the main important things that you

00:30:17,440 --> 00:30:21,520
need to provide here

00:30:19,039 --> 00:30:23,279
is basically details on where you want

00:30:21,520 --> 00:30:25,760
to push this image right of

00:30:23,279 --> 00:30:26,880
your container image of your um

00:30:25,760 --> 00:30:29,760
application

00:30:26,880 --> 00:30:32,480
so we are using quay in our case i am

00:30:29,760 --> 00:30:35,440
using my own personal account in queen

00:30:32,480 --> 00:30:36,880
i have a project created within queen

00:30:35,440 --> 00:30:40,480
where in which i want to

00:30:36,880 --> 00:30:44,000
sort of push all my images too

00:30:40,480 --> 00:30:45,039
so this is what the queen image registry

00:30:44,000 --> 00:30:48,080
would look like

00:30:45,039 --> 00:30:51,120
i have this ci analysis project created

00:30:48,080 --> 00:30:53,279
for my project and these are the various

00:30:51,120 --> 00:30:55,279
tags associated with it right so for

00:30:53,279 --> 00:30:57,679
every tag release that happens

00:30:55,279 --> 00:30:59,919
it basically gets pushed and the image

00:30:57,679 --> 00:31:02,840
once the pipeline runs successfully

00:30:59,919 --> 00:31:04,080
it gets pushed into this repository over

00:31:02,840 --> 00:31:06,880
here

00:31:04,080 --> 00:31:08,640
and we also have a tecton dashboard so

00:31:06,880 --> 00:31:09,600
like i said all of these are being

00:31:08,640 --> 00:31:12,399
triggered

00:31:09,600 --> 00:31:14,880
um with the tecton pipeline since you

00:31:12,399 --> 00:31:18,559
saw that i pushed it as a new image

00:31:14,880 --> 00:31:20,080
as a new uh tag it got uh triggered and

00:31:18,559 --> 00:31:22,000
this is the latest

00:31:20,080 --> 00:31:23,679
uh pipeline run that's been running with

00:31:22,000 --> 00:31:26,480
the tag release run

00:31:23,679 --> 00:31:28,159
and if we go into uh more details you

00:31:26,480 --> 00:31:29,440
get to see what are all the tasks

00:31:28,159 --> 00:31:32,640
associated

00:31:29,440 --> 00:31:34,880
um what's the status of these um tasks

00:31:32,640 --> 00:31:37,440
and the steps within these tasks

00:31:34,880 --> 00:31:39,279
and it's not necessarily that all the

00:31:37,440 --> 00:31:40,080
tasks would pass some of them would also

00:31:39,279 --> 00:31:42,000
get skipped

00:31:40,080 --> 00:31:43,440
so like you can see here four of them

00:31:42,000 --> 00:31:45,120
were skipped and they were not relevant

00:31:43,440 --> 00:31:49,120
for our particular

00:31:45,120 --> 00:31:51,600
tag release and finally this is the last

00:31:49,120 --> 00:31:52,640
step that's currently running and once

00:31:51,600 --> 00:31:54,399
it's successful

00:31:52,640 --> 00:31:56,000
so basically this is the final part

00:31:54,399 --> 00:31:59,360
where it's actually pushing it

00:31:56,000 --> 00:32:01,120
into quake so once this is completely

00:31:59,360 --> 00:32:02,159
green and once all of these steps have

00:32:01,120 --> 00:32:05,200
completed

00:32:02,159 --> 00:32:07,440
we should eventually see this new

00:32:05,200 --> 00:32:09,039
image tag being created in our

00:32:07,440 --> 00:32:11,360
repository here

00:32:09,039 --> 00:32:12,480
so those are pretty much the steps that

00:32:11,360 --> 00:32:16,000
we need to do

00:32:12,480 --> 00:32:17,600
to configure the ci um pipeline and this

00:32:16,000 --> 00:32:20,640
is the repository the

00:32:17,600 --> 00:32:23,279
uh thought team uses and they have

00:32:20,640 --> 00:32:24,720
a great documentation on how this entire

00:32:23,279 --> 00:32:28,000
architecture is set up

00:32:24,720 --> 00:32:31,360
for them and how you can use it for your

00:32:28,000 --> 00:32:35,120
purposes and that's about it

00:32:31,360 --> 00:32:37,840
i think um in terms of my demo um

00:32:35,120 --> 00:32:39,679
while this is sort of taking a few more

00:32:37,840 --> 00:32:40,320
seconds i think few more minutes to just

00:32:39,679 --> 00:32:44,320
run

00:32:40,320 --> 00:32:47,279
i just want to go back um and conclude

00:32:44,320 --> 00:32:48,640
by sort of saying that um there are many

00:32:47,279 --> 00:32:51,120
other tools out there

00:32:48,640 --> 00:32:51,919
which sort of give the same kind of

00:32:51,120 --> 00:32:53,440
results

00:32:51,919 --> 00:32:55,279
it doesn't have to be tacked on it

00:32:53,440 --> 00:32:57,519
doesn't have to be um

00:32:55,279 --> 00:33:00,080
it doesn't have to be any of these sort

00:32:57,519 --> 00:33:02,399
of tools that we have used in our team

00:33:00,080 --> 00:33:04,080
but uh there are also some great tools

00:33:02,399 --> 00:33:06,720
that we also want to explore

00:33:04,080 --> 00:33:07,760
uh more in the future such as kubeflow

00:33:06,720 --> 00:33:10,000
pipelines or

00:33:07,760 --> 00:33:12,399
elira which are which is basically

00:33:10,000 --> 00:33:14,880
nothing but a jupiter lab extension

00:33:12,399 --> 00:33:15,519
and um like how you saw in the jupiter

00:33:14,880 --> 00:33:18,000
hub

00:33:15,519 --> 00:33:18,640
uh the elira basically allows you to

00:33:18,000 --> 00:33:20,880
create

00:33:18,640 --> 00:33:22,000
like a drag and drop pipeline for your

00:33:20,880 --> 00:33:24,159
entire

00:33:22,000 --> 00:33:25,120
machine learning model development and

00:33:24,159 --> 00:33:28,000
you can allow

00:33:25,120 --> 00:33:29,440
allocate resources for every single step

00:33:28,000 --> 00:33:32,240
in your machine learning

00:33:29,440 --> 00:33:33,360
uh deployment process so this is

00:33:32,240 --> 00:33:35,679
something

00:33:33,360 --> 00:33:37,039
new that we want to definitely try out

00:33:35,679 --> 00:33:39,200
and sort of integrate

00:33:37,039 --> 00:33:40,080
making it easier for data scientists

00:33:39,200 --> 00:33:42,799
also

00:33:40,080 --> 00:33:45,200
um to get hands-on into setting up these

00:33:42,799 --> 00:33:48,240
uh sort of pipelines and workflows

00:33:45,200 --> 00:33:48,240
for their use cases

00:33:48,640 --> 00:33:54,399
so with that um thank you so much um

00:33:51,840 --> 00:33:55,120
if you have any questions um feel free

00:33:54,399 --> 00:33:58,320
to

00:33:55,120 --> 00:34:02,480
um have them answered now i think

00:33:58,320 --> 00:34:07,840
uh i'll have a quick look so that i can

00:34:02,480 --> 00:34:07,840
stop sharing

00:34:08,800 --> 00:34:12,560
okay one question i see is how many ml

00:34:10,800 --> 00:34:13,679
containers are typically generated by

00:34:12,560 --> 00:34:20,079
the pipeline

00:34:13,679 --> 00:34:23,359
one with uh

00:34:20,079 --> 00:34:27,359
oh i think i lost that question

00:34:23,359 --> 00:34:29,599
uh there we go one with all the ml

00:34:27,359 --> 00:34:31,040
or is it broken out into different model

00:34:29,599 --> 00:34:34,399
containers

00:34:31,040 --> 00:34:34,720
um so in in this certain use case it's

00:34:34,399 --> 00:34:36,800
just

00:34:34,720 --> 00:34:38,720
it's just one machine learning uh

00:34:36,800 --> 00:34:39,839
container that's been sort of created by

00:34:38,720 --> 00:34:42,159
the pipeline

00:34:39,839 --> 00:34:42,879
um but you could also definitely break

00:34:42,159 --> 00:34:44,320
it down as

00:34:42,879 --> 00:34:46,720
you know you have a project which has

00:34:44,320 --> 00:34:48,079
multiple um sort of models that you're

00:34:46,720 --> 00:34:50,480
trying to train accordingly

00:34:48,079 --> 00:34:52,079
so i would definitely say it's left to

00:34:50,480 --> 00:34:53,839
sort of your use case and your

00:34:52,079 --> 00:34:56,480
customization that you can do

00:34:53,839 --> 00:34:57,040
in this example it's it's um definitely

00:34:56,480 --> 00:34:59,440
just a

00:34:57,040 --> 00:35:02,560
one simple uh container that that we are

00:34:59,440 --> 00:35:03,920
looking at

00:35:02,560 --> 00:35:05,760
we also have another question from

00:35:03,920 --> 00:35:07,599
william henry how many

00:35:05,760 --> 00:35:09,920
ml containers are typically generated by

00:35:07,599 --> 00:35:10,560
the pipeline one with all the ml or is

00:35:09,920 --> 00:35:13,440
it broken

00:35:10,560 --> 00:35:14,400
out into different model containers oh

00:35:13,440 --> 00:35:17,040
yes yes i just

00:35:14,400 --> 00:35:18,880
i think i just answered that question um

00:35:17,040 --> 00:35:20,079
i think the other question was how do

00:35:18,880 --> 00:35:22,079
you handle

00:35:20,079 --> 00:35:23,200
data versioning and hyper parameters yes

00:35:22,079 --> 00:35:26,160
sorry

00:35:23,200 --> 00:35:26,960
in your architecture are using kubeflow

00:35:26,160 --> 00:35:30,560
as part of it

00:35:26,960 --> 00:35:33,200
in some other way so yes so um hyper

00:35:30,560 --> 00:35:35,839
parameter tuning is definitely something

00:35:33,200 --> 00:35:37,040
um we have right now not really invested

00:35:35,839 --> 00:35:40,079
in a certain tool

00:35:37,040 --> 00:35:43,280
per se um in the past we've used this

00:35:40,079 --> 00:35:46,079
open source tool called ml flow so

00:35:43,280 --> 00:35:47,440
ml flow was pretty useful for us um

00:35:46,079 --> 00:35:50,480
where you can sort of

00:35:47,440 --> 00:35:52,240
it gives you like a ui kind of interface

00:35:50,480 --> 00:35:54,000
where you can actually plug and drop

00:35:52,240 --> 00:35:54,800
your hyper parameters and tune your

00:35:54,000 --> 00:35:56,960
model

00:35:54,800 --> 00:35:58,880
um but like i mentioned we now have

00:35:56,960 --> 00:35:59,440
these cube flow pipelines and we now

00:35:58,880 --> 00:36:01,920
have

00:35:59,440 --> 00:36:03,200
uh the project elira which i think would

00:36:01,920 --> 00:36:05,760
definitely be

00:36:03,200 --> 00:36:07,520
um a good tool to sort of invest for

00:36:05,760 --> 00:36:10,880
this kind of hyper parameter

00:36:07,520 --> 00:36:14,760
tuning use case um but yeah as of now

00:36:10,880 --> 00:36:17,760
we are not really looking too much into

00:36:14,760 --> 00:36:17,760

YouTube URL: https://www.youtube.com/watch?v=Jftqtsdw5x0


