Title: Using Pbench to debug Performance Problems - DevConf.CZ 2020
Publication date: 2020-03-25
Playlist: DevConfCZ 2020
Description: 
	Speakers: Peter Portante

The Performance Engineering team at Red Hat has been developing a tool and infrastructure called Pbench, which helps collect, in a complete and consistent manner, data about the execution of a benchmark, including the configuration data of the systems involved. We'll show you the power of having the configuration data along side arbitrary tool/metric data collected to make it easier to understand complex performance issues of distributed systems.

[ https://sched.co/YOq7 ]

--
Recordings of talks at DevConf are a community effort. Unfortunately not everything works perfectly every time. If you're interested in helping us improve, let us know.
Captions: 
	00:00:04,360 --> 00:00:10,820
so since they're gonna embarrass me this

00:00:07,279 --> 00:00:14,480
is my team from Red Hat over here

00:00:10,820 --> 00:00:16,250
shaker just walked in as well Shack etc

00:00:14,480 --> 00:00:18,859
the team Aneesa who's been a big help on

00:00:16,250 --> 00:00:20,980
this talk so if they start to heckle I'm

00:00:18,859 --> 00:00:25,039
gonna come back and point them out again

00:00:20,980 --> 00:00:28,789
I'm so this is all about P bench which

00:00:25,039 --> 00:00:31,820
is a tool it's a benchmark and

00:00:28,789 --> 00:00:35,050
performance analysis framework it

00:00:31,820 --> 00:00:39,140
provides the ability to capture

00:00:35,050 --> 00:00:40,910
configuration in tool data helping you

00:00:39,140 --> 00:00:45,079
to package it and make it easier to

00:00:40,910 --> 00:00:48,649
display analyze etc it consists of an

00:00:45,079 --> 00:00:51,140
agent and a server and a dashboard the

00:00:48,649 --> 00:00:54,050
agent we kind of wrap up some pricing

00:00:51,140 --> 00:00:56,059
scripts to wrap your benchmark so that

00:00:54,050 --> 00:00:58,129
you get with all the hosts that you have

00:00:56,059 --> 00:00:59,269
to run and a distributed system and all

00:00:58,129 --> 00:01:03,739
the tools that you might want to run

00:00:59,269 --> 00:01:07,490
makes it a little bit easier to go do so

00:01:03,739 --> 00:01:09,530
what problem does this solve we found

00:01:07,490 --> 00:01:11,090
that there's a certain level of instant

00:01:09,530 --> 00:01:15,280
consistency in the data that gets

00:01:11,090 --> 00:01:18,740
collected and in incomplete sets of data

00:01:15,280 --> 00:01:20,990
surrounding a benchmark you know we've

00:01:18,740 --> 00:01:24,860
had a team working on distributed

00:01:20,990 --> 00:01:26,090
systems for a while and in running and

00:01:24,860 --> 00:01:28,310
collecting everybody finds their own way

00:01:26,090 --> 00:01:31,280
to do it you get some inconsistency so

00:01:28,310 --> 00:01:33,800
our goal with this tool is to work

00:01:31,280 --> 00:01:35,540
towards making it easier to capture all

00:01:33,800 --> 00:01:37,730
the configuration data you need the tool

00:01:35,540 --> 00:01:39,500
data you need surrounding and I'll give

00:01:37,730 --> 00:01:42,050
you some examples of that around your

00:01:39,500 --> 00:01:43,910
benchmarks helping you make the

00:01:42,050 --> 00:01:46,340
collection a little more consistent so

00:01:43,910 --> 00:01:48,920
you can see run to run or with someone

00:01:46,340 --> 00:01:51,950
else be able to compare things you want

00:01:48,920 --> 00:01:56,180
to provide a location to archive data to

00:01:51,950 --> 00:01:58,970
visualize that data help analyze it we

00:01:56,180 --> 00:02:00,620
want to be able to reuse what we can for

00:01:58,970 --> 00:02:03,050
visualizations that no one's repeating

00:02:00,620 --> 00:02:05,150
the same things that they do and we want

00:02:03,050 --> 00:02:11,030
to make it easier to compare between

00:02:05,150 --> 00:02:13,250
runs so our team works on mostly

00:02:11,030 --> 00:02:14,000
distributed systems these days so Red

00:02:13,250 --> 00:02:17,360
Hat products

00:02:14,000 --> 00:02:20,540
think about the stores products Gluster

00:02:17,360 --> 00:02:23,090
networking with kubernetes and openshift

00:02:20,540 --> 00:02:25,700
and OpenStack they're all distributed

00:02:23,090 --> 00:02:31,090
systems rarely do we work on one system

00:02:25,700 --> 00:02:34,420
for for all the products so we had

00:02:31,090 --> 00:02:36,860
similar needs across the team for

00:02:34,420 --> 00:02:39,050
exercising the network or a storage

00:02:36,860 --> 00:02:41,780
subsystem or whatever the particular

00:02:39,050 --> 00:02:43,400
thing is and we found that different

00:02:41,780 --> 00:02:45,650
team members we're solving these

00:02:43,400 --> 00:02:47,360
problems their own way which is great

00:02:45,650 --> 00:02:49,190
because you know they needed to get

00:02:47,360 --> 00:02:53,450
their job done but what ends up

00:02:49,190 --> 00:02:55,370
happening is with that unique way of

00:02:53,450 --> 00:02:58,310
doing things sometimes it becomes a

00:02:55,370 --> 00:03:00,230
little hard to have the same data

00:02:58,310 --> 00:03:02,780
collected to know if you're comparing

00:03:00,230 --> 00:03:05,239
from the same environment if you can

00:03:02,780 --> 00:03:08,420
reuse Network results from one for

00:03:05,239 --> 00:03:10,910
another etc so we have this tool with

00:03:08,420 --> 00:03:13,430
some best practices that we came up we

00:03:10,910 --> 00:03:16,190
broke it up into an agent so that

00:03:13,430 --> 00:03:19,459
there's a collector for your benchmark

00:03:16,190 --> 00:03:21,830
we call it the agent it's a it's not an

00:03:19,459 --> 00:03:24,500
agent like a background a it's a more of

00:03:21,830 --> 00:03:25,850
a wrapper that you invoke directly we

00:03:24,500 --> 00:03:28,190
have a server side for helping us

00:03:25,850 --> 00:03:30,709
archive and manage all the data and a

00:03:28,190 --> 00:03:33,980
dashboard to view it so our team has

00:03:30,709 --> 00:03:37,120
since about 2015 we've collected about

00:03:33,980 --> 00:03:40,070
two hundred and forty thousand plus

00:03:37,120 --> 00:03:41,570
tarball datasets about twelve point two

00:03:40,070 --> 00:03:44,120
terabytes of data that we've stored on

00:03:41,570 --> 00:03:45,530
our servers and we don't have a delete

00:03:44,120 --> 00:03:49,640
button so that's why it's so hot right

00:03:45,530 --> 00:03:52,130
now so let me give you a sense of what

00:03:49,640 --> 00:03:54,489
we're trying to solve here so let's say

00:03:52,130 --> 00:03:56,780
you've got a simple program in a script

00:03:54,489 --> 00:03:58,459
you want to find out how long it runs

00:03:56,780 --> 00:04:00,079
what do you do you type time and the

00:03:58,459 --> 00:04:02,480
name of the script you get an output

00:04:00,079 --> 00:04:06,260
tells you Oh us this script happened to

00:04:02,480 --> 00:04:11,660
be calculating I think I did a thousand

00:04:06,260 --> 00:04:14,870
digits of pi using a Python decimal

00:04:11,660 --> 00:04:18,140
system 26 seconds to go do that on my

00:04:14,870 --> 00:04:20,209
little laptop isn't that great okay you

00:04:18,140 --> 00:04:22,849
can get a little more fancy if you don't

00:04:20,209 --> 00:04:25,760
use the built-in bash time you can use

00:04:22,849 --> 00:04:27,830
the user bin time or the the new

00:04:25,760 --> 00:04:29,690
provided time and that gets you a little

00:04:27,830 --> 00:04:32,590
bit more information about your script

00:04:29,690 --> 00:04:35,599
right it tells you not only the time but

00:04:32,590 --> 00:04:37,460
it tells you how much CPU how much

00:04:35,599 --> 00:04:39,470
resident memory it took to go do that

00:04:37,460 --> 00:04:41,750
did you do any i/o in this case it

00:04:39,470 --> 00:04:43,400
didn't did you have any major and minor

00:04:41,750 --> 00:04:45,860
page faults we don't use a lot of memory

00:04:43,400 --> 00:04:48,860
so we have a few minor page faults and

00:04:45,860 --> 00:04:49,640
we didn't swap anything right so this is

00:04:48,860 --> 00:04:52,550
nice

00:04:49,640 --> 00:04:55,340
but it doesn't help you click on a

00:04:52,550 --> 00:04:57,590
simple system with one script it's not

00:04:55,340 --> 00:04:59,390
going to help you understand multiple

00:04:57,590 --> 00:05:01,340
systems and it's not going to help you

00:04:59,390 --> 00:05:03,380
understand how your system was

00:05:01,340 --> 00:05:05,930
configured because the time command well

00:05:03,380 --> 00:05:07,550
it tells you that what happened it

00:05:05,930 --> 00:05:10,520
doesn't tell you how much like for

00:05:07,550 --> 00:05:14,060
instance I used nine thousand nine

00:05:10,520 --> 00:05:16,880
hundred and seventy six pages of memory

00:05:14,060 --> 00:05:20,930
to go do this I think I don't know I

00:05:16,880 --> 00:05:22,970
think that maybe bytes but of what was

00:05:20,930 --> 00:05:25,190
that of a hundred million gigabytes or

00:05:22,970 --> 00:05:27,169
was it a one gigabyte or whatever we

00:05:25,190 --> 00:05:29,750
have no reference point we have no idea

00:05:27,169 --> 00:05:33,500
what the configuration is and it doesn't

00:05:29,750 --> 00:05:35,330
help us with multiple systems so let me

00:05:33,500 --> 00:05:36,830
give you a will just sort of ratchet up

00:05:35,330 --> 00:05:38,570
the complexity here as we go because

00:05:36,830 --> 00:05:40,580
that was a little simple so let's say

00:05:38,570 --> 00:05:42,950
you wanted to test networking between

00:05:40,580 --> 00:05:46,789
two systems right so there's you purpose

00:05:42,950 --> 00:05:48,260
out there you poofed org if you wanted

00:05:46,789 --> 00:05:50,690
to test you perf what do you do you have

00:05:48,260 --> 00:05:53,210
a server you perf - test on one system

00:05:50,690 --> 00:05:55,700
you run you perf client on the other

00:05:53,210 --> 00:05:58,159
system in this case I'm picking the the

00:05:55,700 --> 00:06:00,770
iperf dot xml is the profile that's

00:05:58,159 --> 00:06:02,360
provided in the you perf sources that's

00:06:00,770 --> 00:06:04,820
described a certain way of running a

00:06:02,360 --> 00:06:07,010
network test so I've you know

00:06:04,820 --> 00:06:09,979
modified the XML to talk to you the

00:06:07,010 --> 00:06:12,169
server I time the server just so that I

00:06:09,979 --> 00:06:13,820
get the resource usage as well I'm not

00:06:12,169 --> 00:06:15,470
really going to time it in terms of how

00:06:13,820 --> 00:06:17,300
long the server took because the client

00:06:15,470 --> 00:06:21,169
does that but I time the client as well

00:06:17,300 --> 00:06:23,210
and I pointed in my XML to that other

00:06:21,169 --> 00:06:26,900
server and I run it and I capture the

00:06:23,210 --> 00:06:29,180
output of the report so now I get the

00:06:26,900 --> 00:06:32,560
output I've got nine hundred and forty

00:06:29,180 --> 00:06:34,970
two megabits per second and my output

00:06:32,560 --> 00:06:38,540
okay so now what I do with that is that

00:06:34,970 --> 00:06:41,870
a good number so usually what happened

00:06:38,540 --> 00:06:44,390
is we know quote-unquote know our system

00:06:41,870 --> 00:06:47,060
you know maybe it's my laptop so I know

00:06:44,390 --> 00:06:52,100
that you know this laptop has a one gig

00:06:47,060 --> 00:06:56,570
nic on it but maybe not if it's ten gig

00:06:52,100 --> 00:06:59,030
lik NIC then that's not a really a great

00:06:56,570 --> 00:07:00,560
number right 942 is good for a one gig

00:06:59,030 --> 00:07:02,630
NIC maybe you could do better to eke out

00:07:00,560 --> 00:07:05,420
a little bit more but for a 10 gig gig

00:07:02,630 --> 00:07:07,790
it's not so good if I'm running

00:07:05,420 --> 00:07:10,520
benchmarks and I'm comparing results and

00:07:07,790 --> 00:07:13,700
I'm doing this over time am I always

00:07:10,520 --> 00:07:15,620
gonna know where I ran something so we

00:07:13,700 --> 00:07:18,590
found that getting that configuration

00:07:15,620 --> 00:07:20,840
data is really really important and you

00:07:18,590 --> 00:07:23,150
don't necessarily use it it's not like

00:07:20,840 --> 00:07:25,370
having the configuration data front and

00:07:23,150 --> 00:07:27,560
center is the kind of thing that you're

00:07:25,370 --> 00:07:30,860
gonna look at all the time it's when you

00:07:27,560 --> 00:07:32,840
need it you need to have it right so if

00:07:30,860 --> 00:07:34,760
a result does it makes laterz it makes

00:07:32,840 --> 00:07:36,080
sense some time later you want to go

00:07:34,760 --> 00:07:39,410
back to see that it was configured right

00:07:36,080 --> 00:07:42,290
so the goals of our tools is to try to

00:07:39,410 --> 00:07:45,770
get us a complete set of configuration

00:07:42,290 --> 00:07:50,600
data consistently I may capture as well

00:07:45,770 --> 00:07:55,430
as some tool data as well so excuse me

00:07:50,600 --> 00:07:57,560
I'm skipping ahead here so we were doing

00:07:55,430 --> 00:07:59,150
time and that's really nice that gets

00:07:57,560 --> 00:08:01,010
you a little bit of system configuration

00:07:59,150 --> 00:08:03,290
but there's a lot more information about

00:08:01,010 --> 00:08:06,590
how a system is running then what you do

00:08:03,290 --> 00:08:08,420
get with time right so maybe with a

00:08:06,590 --> 00:08:10,850
networking thing you might want to run

00:08:08,420 --> 00:08:12,890
some kind of e BPF script to figure out

00:08:10,850 --> 00:08:15,320
some interesting thing that's happening

00:08:12,890 --> 00:08:18,260
in the networking stack maybe you want

00:08:15,320 --> 00:08:19,780
to run pit stat in order to grab how all

00:08:18,260 --> 00:08:22,970
the other processes and the systems

00:08:19,780 --> 00:08:25,460
system is running with you either on the

00:08:22,970 --> 00:08:27,740
server or on the client you might want

00:08:25,460 --> 00:08:30,560
to do a perfect cord so that you capture

00:08:27,740 --> 00:08:32,210
you know what is the actual program

00:08:30,560 --> 00:08:34,580
doing maybe what the whole kernel is

00:08:32,210 --> 00:08:36,380
doing where's it spending his time and

00:08:34,580 --> 00:08:38,750
again we'll might want to time the

00:08:36,380 --> 00:08:40,700
system as well and I always you know is

00:08:38,750 --> 00:08:42,320
it should I time the perf record or

00:08:40,700 --> 00:08:44,810
started perfect cord at the time because

00:08:42,320 --> 00:08:46,790
I want the setup the time anyways you

00:08:44,810 --> 00:08:48,560
got to figure that out so you have all

00:08:46,790 --> 00:08:50,060
that done so I would sit here and I got

00:08:48,560 --> 00:08:52,040
over like I capture the pigs of those

00:08:50,060 --> 00:08:54,470
background processes because

00:08:52,040 --> 00:08:56,779
now I wanna I can't run them both the

00:08:54,470 --> 00:08:58,370
BPF trace and pit stat don't take a

00:08:56,779 --> 00:09:00,589
command as an argument so I can't kind

00:08:58,370 --> 00:09:02,480
of chain all these commands together so

00:09:00,589 --> 00:09:04,820
now I gotta save all the pigs and you

00:09:02,480 --> 00:09:06,949
know in a file and do that right and

00:09:04,820 --> 00:09:08,360
then when that's done I got to kill them

00:09:06,949 --> 00:09:13,250
so they don't run forever so that way my

00:09:08,360 --> 00:09:15,259
data's not you know trash but are for

00:09:13,250 --> 00:09:16,970
the for those four tools all I want is

00:09:15,259 --> 00:09:19,519
that all that I need could I you know

00:09:16,970 --> 00:09:20,990
maybe this was a you perf might be

00:09:19,519 --> 00:09:24,980
changed to something else and I need

00:09:20,990 --> 00:09:27,740
some disc information and IO stat and

00:09:24,980 --> 00:09:30,380
then I got to add that so the complexity

00:09:27,740 --> 00:09:33,740
of the tools kind of ramps up on top of

00:09:30,380 --> 00:09:35,630
that that was a simple to client server

00:09:33,740 --> 00:09:37,699
you know I just go between here and

00:09:35,630 --> 00:09:41,000
there what if I want to test the

00:09:37,699 --> 00:09:43,040
networking fabric I've got a big switch

00:09:41,000 --> 00:09:45,050
and I want to saturate the switch and I

00:09:43,040 --> 00:09:47,750
want to have all my clients go through

00:09:45,050 --> 00:09:49,250
and and put that through well I need to

00:09:47,750 --> 00:09:51,440
set up a number of server nodes and a

00:09:49,250 --> 00:09:54,079
number client nodes so I gotta run a for

00:09:51,440 --> 00:09:56,389
loop and got a SSH to all the different

00:09:54,079 --> 00:09:58,040
nodes and start the you perf thing in

00:09:56,389 --> 00:10:00,230
the background and make sure that works

00:09:58,040 --> 00:10:02,630
and I don't know if the syntax will

00:10:00,230 --> 00:10:04,430
actually background the SSH command or

00:10:02,630 --> 00:10:07,399
background the you perf on the server

00:10:04,430 --> 00:10:09,800
I'm not really sure about that and I

00:10:07,399 --> 00:10:11,360
gotta start all the clients however I

00:10:09,800 --> 00:10:14,300
want to run you per if to go do that

00:10:11,360 --> 00:10:16,279
well on top of all the pigs I want to

00:10:14,300 --> 00:10:17,899
collect for the tools and I have to do

00:10:16,279 --> 00:10:21,199
that for all the hosts that I'm dealing

00:10:17,899 --> 00:10:23,540
with and so you start ramping up the

00:10:21,199 --> 00:10:28,399
number of systems and your complexity

00:10:23,540 --> 00:10:30,980
just grows with this so we are trying to

00:10:28,399 --> 00:10:34,160
solve this incomplete inconsistent and

00:10:30,980 --> 00:10:37,069
the Inc completes while also making it

00:10:34,160 --> 00:10:42,139
easier to go about and one on a

00:10:37,069 --> 00:10:45,769
distributed systems so P bench offers

00:10:42,139 --> 00:10:47,600
this simple command register toolset and

00:10:45,769 --> 00:10:50,209
it's another simple command called user

00:10:47,600 --> 00:10:53,060
benchmark to do the same thing we were

00:10:50,209 --> 00:10:55,459
doing by hand but in just simple two

00:10:53,060 --> 00:10:57,500
steps so here what I can do is I can say

00:10:55,459 --> 00:10:59,569
I have a set of tools and so we're gonna

00:10:57,500 --> 00:11:01,130
hand wave on the tool set that we're

00:10:59,569 --> 00:11:03,529
gonna run but I have it defined it's

00:11:01,130 --> 00:11:05,550
just its name somewhere and I have a

00:11:03,529 --> 00:11:07,649
list of hosts that I want to collect my

00:11:05,550 --> 00:11:09,360
data on so I just add that in the

00:11:07,649 --> 00:11:12,570
command line for register tool set and

00:11:09,360 --> 00:11:15,209
then I have my here's my script whatever

00:11:12,570 --> 00:11:16,770
my script is going to be and I'm not

00:11:15,209 --> 00:11:18,720
will get to the yooper in a second

00:11:16,770 --> 00:11:21,000
whatever my script is I just one user

00:11:18,720 --> 00:11:22,980
benchmark I give it a configuration name

00:11:21,000 --> 00:11:23,970
to help me collect some metadata about

00:11:22,980 --> 00:11:29,040
it for later

00:11:23,970 --> 00:11:31,620
and boom I'm able to run my benchmark as

00:11:29,040 --> 00:11:33,390
it is with collecting data on all the

00:11:31,620 --> 00:11:36,779
hosts that I want for all the tools that

00:11:33,390 --> 00:11:39,690
I've registered so let's get into a

00:11:36,779 --> 00:11:42,660
little bit about one more thing so I

00:11:39,690 --> 00:11:44,730
mentioned configuration data that we

00:11:42,660 --> 00:11:47,220
want to capture the configuration of the

00:11:44,730 --> 00:11:50,970
system as well so what we did was in the

00:11:47,220 --> 00:11:52,380
user benchmark command and in all the

00:11:50,970 --> 00:11:55,380
benchmark commands to be offering p

00:11:52,380 --> 00:11:56,940
bench we collect a series of data is

00:11:55,380 --> 00:11:59,040
that too small i

00:11:56,940 --> 00:12:02,520
you don't need glasses or something like

00:11:59,040 --> 00:12:05,520
that Sally okay okay so we have six

00:12:02,520 --> 00:12:09,060
different configuration data sets that

00:12:05,520 --> 00:12:11,279
we collect by default the block tree we

00:12:09,060 --> 00:12:13,320
go find all the discs in the in the tree

00:12:11,279 --> 00:12:15,990
and grab a bunch of files underneath it

00:12:13,320 --> 00:12:18,660
you grab for libvirt if it's available

00:12:15,990 --> 00:12:20,360
we grab data about how libero is

00:12:18,660 --> 00:12:24,089
configured and some of the logs from it

00:12:20,360 --> 00:12:26,279
we could have the kernel confess'd occur

00:12:24,089 --> 00:12:29,520
kn'l is configured on certain cases all

00:12:26,279 --> 00:12:32,459
the security mitigations we grab a sauce

00:12:29,520 --> 00:12:34,620
report a very minimal we have a targeted

00:12:32,459 --> 00:12:36,899
set of modules that we use that don't

00:12:34,620 --> 00:12:39,540
include all big data collections of the

00:12:36,899 --> 00:12:43,260
SAR data or the logs etc and we get a

00:12:39,540 --> 00:12:47,579
topology output from LS topology if all

00:12:43,260 --> 00:12:49,410
these are if they're available we we so

00:12:47,579 --> 00:12:51,930
sauce report collects a lot of that same

00:12:49,410 --> 00:12:54,540
data the reason why I think we get the

00:12:51,930 --> 00:12:55,860
other things as well as a sauce report

00:12:54,540 --> 00:12:56,370
is it makes it's a little bit easier to

00:12:55,860 --> 00:12:58,079
find

00:12:56,370 --> 00:13:01,410
because sauce reports kind of buried in

00:12:58,079 --> 00:13:04,110
the tarball so the sauce report is more

00:13:01,410 --> 00:13:05,670
of a fallback you want that when you

00:13:04,110 --> 00:13:08,760
need it but you don't want to look at it

00:13:05,670 --> 00:13:11,430
every single time we also have three

00:13:08,760 --> 00:13:15,180
other optional data collection tools

00:13:11,430 --> 00:13:18,000
there's a tool that we wrote called

00:13:15,180 --> 00:13:19,350
stockpile which basically is an ansible

00:13:18,000 --> 00:13:23,480
playbook that

00:13:19,350 --> 00:13:27,240
uses the ansible fax feature to go grab

00:13:23,480 --> 00:13:28,590
data about the system this is really

00:13:27,240 --> 00:13:31,080
cool I was gonna point this out to you

00:13:28,590 --> 00:13:33,210
guys and I forgot earlier we this is a

00:13:31,080 --> 00:13:35,430
really neat thing because what we can do

00:13:33,210 --> 00:13:38,220
is all of the upper ones could be

00:13:35,430 --> 00:13:40,080
collapsed down into stockpile eventually

00:13:38,220 --> 00:13:41,970
and you know somehow to have all that

00:13:40,080 --> 00:13:43,890
data in there but it's a really powerful

00:13:41,970 --> 00:13:46,200
tool because we just run in full

00:13:43,890 --> 00:13:47,820
playbook we get a big huge JSON document

00:13:46,200 --> 00:13:50,040
the whole thing can be indexed into an

00:13:47,820 --> 00:13:52,140
elastic search for search and fine it's

00:13:50,040 --> 00:13:55,980
really cool but I'm not going to talk

00:13:52,140 --> 00:13:57,960
about Stockwell insights so insights

00:13:55,980 --> 00:14:01,140
client is a cool little feature that Red

00:13:57,960 --> 00:14:02,490
Hat has we just grabbed the client run

00:14:01,140 --> 00:14:04,530
it grabbed the configuration we can

00:14:02,490 --> 00:14:06,870
store it with the data with the data as

00:14:04,530 --> 00:14:08,520
well and ARRA is neat because it's a

00:14:06,870 --> 00:14:10,920
tool that we use to analyze the

00:14:08,520 --> 00:14:14,100
performance of ansible so in a lot of

00:14:10,920 --> 00:14:17,070
the performance testing we do we have to

00:14:14,100 --> 00:14:19,110
capture how well or how fast ansible

00:14:17,070 --> 00:14:21,330
actually runs when deploying different

00:14:19,110 --> 00:14:25,020
systems and so that's a tool that grabs

00:14:21,330 --> 00:14:29,600
the configuration of the the ansible

00:14:25,020 --> 00:14:31,710
playbook when they ran so well we had

00:14:29,600 --> 00:14:34,170
configuration data we've got the tool

00:14:31,710 --> 00:14:36,090
data we've got a way to do that so real

00:14:34,170 --> 00:14:38,910
quick there's a little simple

00:14:36,090 --> 00:14:41,940
configuration in P bench there's a

00:14:38,910 --> 00:14:45,990
simple agent config file we in that

00:14:41,940 --> 00:14:48,480
config file you can tell PBX ylides

00:14:45,990 --> 00:14:50,580
wrong I got a I missed it that should be

00:14:48,480 --> 00:14:53,540
P bench agent varlet P bench agent our

00:14:50,580 --> 00:14:56,070
default place where we store data and

00:14:53,540 --> 00:14:58,560
you can change that in the in the config

00:14:56,070 --> 00:15:03,030
file and so that way we collect it and

00:14:58,560 --> 00:15:05,370
I'll show you a little bit about that in

00:15:03,030 --> 00:15:07,140
just a second so I want to dive into the

00:15:05,370 --> 00:15:09,240
register tool I'm kind of going quick so

00:15:07,140 --> 00:15:11,520
if it's if you have any questions feel

00:15:09,240 --> 00:15:12,780
free to ask but I figure it's the last

00:15:11,520 --> 00:15:18,390
talk of the day we want to plow through

00:15:12,780 --> 00:15:22,890
this and get on for for dinner so all

00:15:18,390 --> 00:15:24,810
the tool data so you have you know

00:15:22,890 --> 00:15:28,410
earlier we had time and we had perfect

00:15:24,810 --> 00:15:31,680
chord and you know BPF Tracy etc so the

00:15:28,410 --> 00:15:32,880
register tool function records the data

00:15:31,680 --> 00:15:36,030
about the tools that

00:15:32,880 --> 00:15:39,420
you want to run in a local directory in

00:15:36,030 --> 00:15:42,450
the run directory Varla pea bench agent

00:15:39,420 --> 00:15:45,150
under the group name and we give every

00:15:42,450 --> 00:15:46,800
tool set or every set of all the tools

00:15:45,150 --> 00:15:48,540
that you want to run are always in a

00:15:46,800 --> 00:15:50,370
group and that way you can have

00:15:48,540 --> 00:15:52,800
different groups for different purposes

00:15:50,370 --> 00:15:54,270
you may want to run one group of tools

00:15:52,800 --> 00:15:56,150
for doing network analysis and

00:15:54,270 --> 00:15:59,040
electrical tools for doing disc analysis

00:15:56,150 --> 00:16:00,570
and so you just register your tool under

00:15:59,040 --> 00:16:03,480
the group with whatever arguments that

00:16:00,570 --> 00:16:06,270
tool takes we have about 38 tools that

00:16:03,480 --> 00:16:09,840
we've added to P bench or that it

00:16:06,270 --> 00:16:12,450
supports there's a non exhaustive list

00:16:09,840 --> 00:16:15,000
list down there including the ability to

00:16:12,450 --> 00:16:16,710
just do a user tool which basically says

00:16:15,000 --> 00:16:18,960
here's an arbitrary - it'll go run and

00:16:16,710 --> 00:16:21,480
we give you an a very verbose way to

00:16:18,960 --> 00:16:24,390
describe it and an external data source

00:16:21,480 --> 00:16:27,240
so if you've got a running Prometheus or

00:16:24,390 --> 00:16:28,230
running you know PCP environment where

00:16:27,240 --> 00:16:30,840
you're collecting all this stuff

00:16:28,230 --> 00:16:33,060
centrally you can just say hey go look

00:16:30,840 --> 00:16:39,000
at the data or record that I had data

00:16:33,060 --> 00:16:42,210
over there as well so at the very

00:16:39,000 --> 00:16:44,190
beginning I showed you and we'll go

00:16:42,210 --> 00:16:46,410
blindly back real quick I showed you

00:16:44,190 --> 00:16:49,050
that we had this register tool set well

00:16:46,410 --> 00:16:53,090
our register tool set is basically just

00:16:49,050 --> 00:16:56,880
a call to a bunch of register tools

00:16:53,090 --> 00:16:58,620
before and the sets you can define in

00:16:56,880 --> 00:17:02,010
the config file the P bench config file

00:16:58,620 --> 00:17:05,400
so you can say that you know for certain

00:17:02,010 --> 00:17:07,410
tests I want a very light set of tools

00:17:05,400 --> 00:17:08,959
click just a vmstat nothing heavy I

00:17:07,410 --> 00:17:12,120
don't want to interfere with anything

00:17:08,959 --> 00:17:13,350
you might want to have a heavy set of

00:17:12,120 --> 00:17:15,329
tools where you're really getting to

00:17:13,350 --> 00:17:17,339
deep where you got lock stat and you're

00:17:15,329 --> 00:17:19,020
doing a perfect cord and you're looking

00:17:17,339 --> 00:17:22,260
at proc interrupts and which is a ton of

00:17:19,020 --> 00:17:24,810
data Pig stats which get tons of data so

00:17:22,260 --> 00:17:26,939
you can pick and set up the kinds of

00:17:24,810 --> 00:17:29,220
tools you want in the configuration

00:17:26,939 --> 00:17:31,740
there and again you give the list of

00:17:29,220 --> 00:17:38,670
hosts that you want to to use going

00:17:31,740 --> 00:17:41,130
forward so what happens you got this

00:17:38,670 --> 00:17:42,960
register toolset you got your user

00:17:41,130 --> 00:17:44,760
benchmark what's really happening it's

00:17:42,960 --> 00:17:46,390
really it almost actually simple right

00:17:44,760 --> 00:17:47,920
we start

00:17:46,390 --> 00:17:50,220
user benchmark starts running before it

00:17:47,920 --> 00:17:52,480
touches and runs your script it goes off

00:17:50,220 --> 00:17:53,890
figures out all the tools that you asked

00:17:52,480 --> 00:17:56,080
it to go run and just starts them all

00:17:53,890 --> 00:17:58,510
running it handles all the SSH is to

00:17:56,080 --> 00:18:02,049
remote nodes how to start them what to

00:17:58,510 --> 00:18:05,500
go do tracking them the right bid file

00:18:02,049 --> 00:18:06,820
thing cetera it then runs your script

00:18:05,500 --> 00:18:09,340
and we time it

00:18:06,820 --> 00:18:10,750
we don't use time we just use the

00:18:09,340 --> 00:18:12,520
seconds in bash

00:18:10,750 --> 00:18:15,940
we probably should use the time thing

00:18:12,520 --> 00:18:17,140
and then we do is we because we start

00:18:15,940 --> 00:18:19,809
are all the tools when your script runs

00:18:17,140 --> 00:18:22,330
we go and stop all the tools and so

00:18:19,809 --> 00:18:24,309
we've just wrapped and done all what you

00:18:22,330 --> 00:18:26,110
would normally do yourself we've just

00:18:24,309 --> 00:18:28,570
put into the script and if you go look

00:18:26,110 --> 00:18:30,730
at the source the in the slides etc are

00:18:28,570 --> 00:18:32,860
the link to the sources there that's you

00:18:30,730 --> 00:18:34,690
can see it's got a lot of cruft for you

00:18:32,860 --> 00:18:37,000
know support and management but it's

00:18:34,690 --> 00:18:39,130
basically just those steps and then

00:18:37,000 --> 00:18:40,600
after the tools are stopped we give an

00:18:39,130 --> 00:18:43,900
opportunity for each tool to be post

00:18:40,600 --> 00:18:45,910
processed so you know do you need to pit

00:18:43,900 --> 00:18:47,320
stat generates tons and tons of data do

00:18:45,910 --> 00:18:50,080
you need to kind of call that down to a

00:18:47,320 --> 00:18:52,630
smaller set somehow or for instance if

00:18:50,080 --> 00:18:55,510
you do a perfect cord you want to do a

00:18:52,630 --> 00:18:58,900
perf archive so you grab an archive of

00:18:55,510 --> 00:19:00,730
the perf data to use it off host instead

00:18:58,900 --> 00:19:03,640
all those kinds of things happen in the

00:19:00,730 --> 00:19:05,770
post-processing steps and then finally

00:19:03,640 --> 00:19:11,530
for user benchmark we collect all the

00:19:05,770 --> 00:19:12,940
configuration data that's needed so user

00:19:11,530 --> 00:19:15,450
benchmark is simple because it's

00:19:12,940 --> 00:19:18,400
designed to run around one command but

00:19:15,450 --> 00:19:20,140
commands like you you perf and we have

00:19:18,400 --> 00:19:20,620
another one if people are familiar with

00:19:20,140 --> 00:19:24,130
fiyo

00:19:20,620 --> 00:19:28,570
for doing file IO testing and dis

00:19:24,130 --> 00:19:30,790
testing you often want to do multiple

00:19:28,570 --> 00:19:33,820
different types of networking testing am

00:19:30,790 --> 00:19:35,919
i doing a round-robin test where I do an

00:19:33,820 --> 00:19:39,010
echo am i doing some kind of stream test

00:19:35,919 --> 00:19:41,380
etc and I might want to test different

00:19:39,010 --> 00:19:45,190
message sizes and I might want a

00:19:41,380 --> 00:19:47,200
combination of all those things well if

00:19:45,190 --> 00:19:48,370
you used user benchmark for every one of

00:19:47,200 --> 00:19:51,850
those things you're going to get

00:19:48,370 --> 00:19:53,770
separate results for each one but here

00:19:51,850 --> 00:19:55,630
we've offered in people into you

00:19:53,770 --> 00:19:58,270
peripheral Pippins file the way to

00:19:55,630 --> 00:19:59,440
gather all that data in one result

00:19:58,270 --> 00:20:02,590
tarball with

00:19:59,440 --> 00:20:05,620
individual iterations of of data and so

00:20:02,590 --> 00:20:08,080
will you tell us for you perf hey I want

00:20:05,620 --> 00:20:10,330
RR and stream and I want these three

00:20:08,080 --> 00:20:13,720
message sizes so what I'll do is I'll

00:20:10,330 --> 00:20:16,149
run that six times and because I've

00:20:13,720 --> 00:20:18,429
asked for five samples all each time for

00:20:16,149 --> 00:20:20,259
each combination are one at five times

00:20:18,429 --> 00:20:22,360
and calculated a mean and give you the

00:20:20,259 --> 00:20:24,220
closest sample as well so really we'll

00:20:22,360 --> 00:20:28,990
run that thirty times give you a really

00:20:24,220 --> 00:20:31,840
in-depth coverage of those different

00:20:28,990 --> 00:20:34,210
parameters across the six systems you're

00:20:31,840 --> 00:20:37,450
doing so you've got three clients that

00:20:34,210 --> 00:20:39,340
are matched to three servers and then

00:20:37,450 --> 00:20:40,840
we'll also collect the configuration

00:20:39,340 --> 00:20:43,210
data for you and that's the beauty of

00:20:40,840 --> 00:20:44,980
this is that now you can stop thinking

00:20:43,210 --> 00:20:47,559
about when to collect the configuration

00:20:44,980 --> 00:20:50,440
data did I do it here it's all wrapped

00:20:47,559 --> 00:20:52,840
up in the configuration and the run of

00:20:50,440 --> 00:20:54,960
the the P bench commands so we have

00:20:52,840 --> 00:20:58,299
other workloads that we also support

00:20:54,960 --> 00:21:00,190
traffic gen inspect jbb they actually

00:20:58,299 --> 00:21:02,379
use a different method of start and stop

00:21:00,190 --> 00:21:04,179
tools which I don't want to get into

00:21:02,379 --> 00:21:07,090
that right now it's a whole nother can

00:21:04,179 --> 00:21:09,370
of worms and we have a up and coming

00:21:07,090 --> 00:21:12,639
command called P bonsoir benchmark which

00:21:09,370 --> 00:21:14,769
is sort of a big brother to user

00:21:12,639 --> 00:21:17,470
benchmark that will let you describe

00:21:14,769 --> 00:21:23,649
more complex workloads without having

00:21:17,470 --> 00:21:25,509
for us to code them directly so what

00:21:23,649 --> 00:21:27,700
happens on the the tool when it collects

00:21:25,509 --> 00:21:31,860
data well first of all in the run area

00:21:27,700 --> 00:21:34,600
that's varlet P bench agent p bench run

00:21:31,860 --> 00:21:37,269
you there's a directory gets created for

00:21:34,600 --> 00:21:39,669
that run so if you ran five P bench file

00:21:37,269 --> 00:21:42,009
or people in super 4 P bench who's our

00:21:39,669 --> 00:21:44,080
benchmark we have a prefix there

00:21:42,009 --> 00:21:46,539
whatever config parameter you gave on

00:21:44,080 --> 00:21:48,009
the command line gets constructed in the

00:21:46,539 --> 00:21:50,860
time stamp at the time you issue the

00:21:48,009 --> 00:21:53,559
command and so what you get on the

00:21:50,860 --> 00:21:57,129
bottom there is var Lib agent I should

00:21:53,559 --> 00:22:01,179
have a P Vinci agent but if it's our is

00:21:57,129 --> 00:22:03,850
the the one directory so under that run

00:22:01,179 --> 00:22:08,259
directory so here I have an example

00:22:03,850 --> 00:22:12,519
where I've got two test types and two

00:22:08,259 --> 00:22:12,910
message sizes you part P been super is

00:22:12,519 --> 00:22:16,180
going

00:22:12,910 --> 00:22:18,100
to give you one iteration for every

00:22:16,180 --> 00:22:19,870
combination of the parameters that you

00:22:18,100 --> 00:22:24,120
want it to run so there are four

00:22:19,870 --> 00:22:29,020
iterations there 1 2 3 4 I should have a

00:22:24,120 --> 00:22:33,880
pointer I think which I'm not gonna yeah

00:22:29,020 --> 00:22:36,370
there you go 1 2 3 4 and underneath the

00:22:33,880 --> 00:22:37,900
the fourth directory I'm showing the

00:22:36,370 --> 00:22:40,000
fact that there's actually sample

00:22:37,900 --> 00:22:41,980
directories for each sample that was run

00:22:40,000 --> 00:22:43,930
because we had 5 samples up there

00:22:41,980 --> 00:22:46,960
so all 5 samples will be listed there

00:22:43,930 --> 00:22:49,420
and the data for every sample will be

00:22:46,960 --> 00:22:52,060
collected underneath under those

00:22:49,420 --> 00:22:53,830
directories and then we put a reference

00:22:52,060 --> 00:22:56,440
result link because wilcott will

00:22:53,830 --> 00:22:58,690
calculate which sample is closest to the

00:22:56,440 --> 00:23:00,340
mean of the five runs and we'll put a

00:22:58,690 --> 00:23:03,340
link in there so you can easily go find

00:23:00,340 --> 00:23:06,520
which sample actually was closest to

00:23:03,340 --> 00:23:08,280
your mean it's not named ref result but

00:23:06,520 --> 00:23:13,600
it didn't fit on the screen without

00:23:08,280 --> 00:23:16,750
shortening it so underneath excuse me

00:23:13,600 --> 00:23:18,730
each sample all the tool data is put

00:23:16,750 --> 00:23:20,920
because for each sample is what we're

00:23:18,730 --> 00:23:23,950
gonna run and and collect the tools that

00:23:20,920 --> 00:23:26,050
you asked and so let's say I had

00:23:23,950 --> 00:23:30,490
configured MP stat and vmstat

00:23:26,050 --> 00:23:33,850
in there for each host that I asked

00:23:30,490 --> 00:23:37,770
those tools to be run on so host n/a is

00:23:33,850 --> 00:23:40,180
one host host and X is the other host

00:23:37,770 --> 00:23:42,580
echoing back to the previous examples I

00:23:40,180 --> 00:23:45,010
would have an MP stat in a VM stat

00:23:42,580 --> 00:23:46,450
directory for each host with the

00:23:45,010 --> 00:23:50,470
standardout standard error of how those

00:23:46,450 --> 00:23:51,820
tools ran and for MP stat and vmstat but

00:23:50,470 --> 00:23:56,380
I'm not showing it for vmstat

00:23:51,820 --> 00:23:58,290
we calculate a set of graphs and sort of

00:23:56,380 --> 00:24:03,160
call the data a little bit for you

00:23:58,290 --> 00:24:05,350
building an HTML file and we use a tool

00:24:03,160 --> 00:24:07,290
called jst art that was written by one

00:24:05,350 --> 00:24:11,860
of our team members carl rester at IBM

00:24:07,290 --> 00:24:15,130
who is now here at Red Hat and it's it's

00:24:11,860 --> 00:24:17,020
designed to give a graph view of your

00:24:15,130 --> 00:24:20,170
data along with the table next to it so

00:24:17,020 --> 00:24:22,240
you can really introspect zoom in you

00:24:20,170 --> 00:24:24,220
can grab parts of the screen and make it

00:24:22,240 --> 00:24:26,260
bigger and do all kinds of funky d3

00:24:24,220 --> 00:24:26,800
things that you'd like to get at the

00:24:26,260 --> 00:24:30,550
date

00:24:26,800 --> 00:24:34,170
different data sets so when you're all

00:24:30,550 --> 00:24:38,680
done with your data collection and

00:24:34,170 --> 00:24:40,720
you're ready to go we offer the P bench

00:24:38,680 --> 00:24:45,160
move results command or copy results

00:24:40,720 --> 00:24:47,890
command to get your your data off your

00:24:45,160 --> 00:24:49,270
local system on to the main server give

00:24:47,890 --> 00:24:50,920
you either archive or whatever now you

00:24:49,270 --> 00:24:52,750
don't have to do that but if you'd like

00:24:50,920 --> 00:24:56,470
to archive it and you have a P bench

00:24:52,750 --> 00:24:58,690
server that's what you offer the agent

00:24:56,470 --> 00:25:00,150
config file is has all the configuration

00:24:58,690 --> 00:25:03,880
in it for how to talk to that server

00:25:00,150 --> 00:25:07,360
when you do a move results we add

00:25:03,880 --> 00:25:11,470
metadata about the environment that you

00:25:07,360 --> 00:25:14,170
ran the tarball on where your results on

00:25:11,470 --> 00:25:16,450
we compress it we give it an ID I think

00:25:14,170 --> 00:25:18,760
this is one of the cool parts of P bench

00:25:16,450 --> 00:25:21,190
is that the directory at that point in

00:25:18,760 --> 00:25:23,170
time that that directory hierarchy that

00:25:21,190 --> 00:25:26,260
you had for your results when it gets

00:25:23,170 --> 00:25:29,790
hard up and compressed we put the md5

00:25:26,260 --> 00:25:33,910
sum on it and that gives it a unique ID

00:25:29,790 --> 00:25:36,400
as much as um v5 can be unique so that

00:25:33,910 --> 00:25:39,160
now that becomes an object that can be

00:25:36,400 --> 00:25:41,170
reasoned about so we have metadata about

00:25:39,160 --> 00:25:43,570
it I know it's one and we have it's ID

00:25:41,170 --> 00:25:44,800
and now we can look at in the system and

00:25:43,570 --> 00:25:47,200
I'll show you a little bit in a second

00:25:44,800 --> 00:25:49,840
we copied the data over to the P bench

00:25:47,200 --> 00:25:51,520
server the move will remove it locally

00:25:49,840 --> 00:25:55,630
once it's verified on the remote side

00:25:51,520 --> 00:25:58,960
and we give you a URL to where it is our

00:25:55,630 --> 00:26:01,690
P bench server is designed to handle the

00:25:58,960 --> 00:26:04,600
archiving of the data so we make sure

00:26:01,690 --> 00:26:05,830
that it's properly md5 and bits um you

00:26:04,600 --> 00:26:09,400
know bit rot checked all that kind of

00:26:05,830 --> 00:26:14,020
stuff we give a a way to visualize the

00:26:09,400 --> 00:26:16,150
tar balls off of the humanity that MP

00:26:14,020 --> 00:26:18,070
stat HTML file I showed you with J's

00:26:16,150 --> 00:26:20,890
chart we give you a way to see all those

00:26:18,070 --> 00:26:23,620
on the server and we index that data

00:26:20,890 --> 00:26:26,950
into elasticsearch for a dashboard

00:26:23,620 --> 00:26:30,840
features that we have we spend a lot of

00:26:26,950 --> 00:26:33,190
time on indexing data a lot of data so

00:26:30,840 --> 00:26:34,390
that doesn't that's not too visible

00:26:33,190 --> 00:26:37,360
right that's I always thought this slide

00:26:34,390 --> 00:26:40,120
was gonna be a little a little small you

00:26:37,360 --> 00:26:53,140
can't see it right yeah no all right

00:26:40,120 --> 00:26:57,130
so I am prepared so which way do I go it

00:26:53,140 --> 00:26:59,820
won't let me move it hold on I'm not

00:26:57,130 --> 00:26:59,820
totally prepared

00:27:04,530 --> 00:27:15,750
all right I'm not totally prepared hold

00:27:06,790 --> 00:27:15,750
on there we go come back here

00:27:20,299 --> 00:27:24,049
and doesn't work don't ask me why I

00:27:22,399 --> 00:27:27,889
cannot move the window over there

00:27:24,049 --> 00:27:34,399
trust me it's visible and you just I

00:27:27,889 --> 00:27:36,459
need a little song-and-dance here so we

00:27:34,399 --> 00:27:40,909
have a dashboard that shows you

00:27:36,459 --> 00:27:44,149
unfortunately in our team inside Red Hat

00:27:40,909 --> 00:27:46,759
we all kind of own machines if you will

00:27:44,149 --> 00:27:49,789
we do a lot of results off of one run so

00:27:46,759 --> 00:27:51,709
we've organized the data off of the

00:27:49,789 --> 00:27:55,279
control the name of the Hoshi ran the P

00:27:51,709 --> 00:27:58,969
bench command on and so we give you a

00:27:55,279 --> 00:28:01,309
way to list and see all the data from

00:27:58,969 --> 00:28:06,289
your particular host so over here this

00:28:01,309 --> 00:28:11,329
host is DHCP 31 122 and so what you

00:28:06,289 --> 00:28:13,119
would get is if you select that host the

00:28:11,329 --> 00:28:15,950
dashboard shows you all the different

00:28:13,119 --> 00:28:18,529
result tarballs that you had it gives

00:28:15,950 --> 00:28:22,070
you the config name and the starting end

00:28:18,529 --> 00:28:24,919
time for that particular tarball you can

00:28:22,070 --> 00:28:27,379
introspect a particular tarball and this

00:28:24,919 --> 00:28:28,549
protect for for you perf you can see all

00:28:27,379 --> 00:28:30,799
the different rows of data that you

00:28:28,549 --> 00:28:34,339
collected for the different runs in a

00:28:30,799 --> 00:28:37,849
nice table form for the standard

00:28:34,339 --> 00:28:40,820
deviation and mean etc of the of the

00:28:37,849 --> 00:28:44,119
result you can then go through and

00:28:40,820 --> 00:28:45,499
select different results to compare so

00:28:44,119 --> 00:28:48,829
you can figure out if you want to

00:28:45,499 --> 00:28:53,059
compare your streams with a certain size

00:28:48,829 --> 00:28:55,159
or etc and then once you hit the

00:28:53,059 --> 00:28:56,599
comparison will give you the metadata

00:28:55,159 --> 00:28:59,329
that you had compared on the right hand

00:28:56,599 --> 00:29:01,609
side and and the the comparison of your

00:28:59,329 --> 00:29:04,549
you proof results on the set of same

00:29:01,609 --> 00:29:10,459
thing for Phyo and and traffic gen as it

00:29:04,549 --> 00:29:14,239
goes so future directions so we need a

00:29:10,459 --> 00:29:16,549
notion of user into P bench right now as

00:29:14,239 --> 00:29:18,349
I said it's all based on controllers you

00:29:16,549 --> 00:29:21,070
want to get it to the point where each

00:29:18,349 --> 00:29:24,679
user has control over their own data and

00:29:21,070 --> 00:29:26,029
can publish and delete data we don't

00:29:24,679 --> 00:29:29,809
have a delete button right now because

00:29:26,029 --> 00:29:32,749
we don't own data so we're not we don't

00:29:29,809 --> 00:29:34,610
really trust our team over here to not

00:29:32,749 --> 00:29:37,880
delete someone else's data

00:29:34,610 --> 00:29:40,210
so we just don't allow the delete which

00:29:37,880 --> 00:29:43,280
has its ramifications of another sort

00:29:40,210 --> 00:29:45,799
we're working on refactoring on the

00:29:43,280 --> 00:29:49,040
agent side the tooling right now is all

00:29:45,799 --> 00:29:51,410
based on constant ssh --is all over the

00:29:49,040 --> 00:29:55,400
place we are container izing the tools

00:29:51,410 --> 00:29:57,320
and making it so that the the tools are

00:29:55,400 --> 00:29:58,280
described by the container and then

00:29:57,320 --> 00:30:01,660
we're gonna have a thing that will just

00:29:58,280 --> 00:30:03,950
one the containerized tools that you

00:30:01,660 --> 00:30:05,720
that you build so that will making it

00:30:03,950 --> 00:30:07,250
easier for someone who wants to build a

00:30:05,720 --> 00:30:09,320
certain tool they don't have to

00:30:07,250 --> 00:30:11,900
interface into P bench somehow they can

00:30:09,320 --> 00:30:14,799
build a container that has what they

00:30:11,900 --> 00:30:17,150
want behavior in there

00:30:14,799 --> 00:30:20,090
we are going to be changing the yooper

00:30:17,150 --> 00:30:22,549
and Redis it's sort of new perf and file

00:30:20,090 --> 00:30:24,820
workloads to use Redis to help

00:30:22,549 --> 00:30:28,400
coordinate the way that they execute and

00:30:24,820 --> 00:30:32,750
we're desperately moving to Python 3 for

00:30:28,400 --> 00:30:37,070
all the code base the server side we

00:30:32,750 --> 00:30:40,280
have also notion of a user we need to go

00:30:37,070 --> 00:30:42,530
to an s3 for our archiving right now we

00:30:40,280 --> 00:30:43,790
want to finish all of our Python 3 and

00:30:42,530 --> 00:30:47,480
containerize it so we can deploy the

00:30:43,790 --> 00:30:49,790
server a lot easier for the dashboard as

00:30:47,480 --> 00:30:51,200
well as the users for all the other to

00:30:49,790 --> 00:30:55,309
the dashboard needs the notion of the

00:30:51,200 --> 00:30:57,260
user we want to take and right now

00:30:55,309 --> 00:31:01,010
people have access to different data

00:30:57,260 --> 00:31:03,020
sets in Griffin ax or Kabana and we want

00:31:01,010 --> 00:31:05,929
to be able to have the debt the result

00:31:03,020 --> 00:31:09,080
dashboard leap off to more introspective

00:31:05,929 --> 00:31:10,790
data sets in other dashboards that

00:31:09,080 --> 00:31:13,880
people construct so we'd like to add

00:31:10,790 --> 00:31:17,510
support for doing that we also want to

00:31:13,880 --> 00:31:20,179
be able to handle the display of data in

00:31:17,510 --> 00:31:25,370
existing data sets from the dashboard

00:31:20,179 --> 00:31:27,500
like a Prometheus or PCP etc we're

00:31:25,370 --> 00:31:29,210
working on comparing aggregating the

00:31:27,500 --> 00:31:30,530
tool data across the node so if you have

00:31:29,210 --> 00:31:32,450
like you know those 20 nodes you're

00:31:30,530 --> 00:31:34,490
doing you perf on we want to be able to

00:31:32,450 --> 00:31:36,440
take from that tool data and display the

00:31:34,490 --> 00:31:39,230
aggregate set of that data across all

00:31:36,440 --> 00:31:42,470
those nodes and your comparisons between

00:31:39,230 --> 00:31:44,659
runs and we have this feature we call

00:31:42,470 --> 00:31:47,490
table of contents anisha's working on

00:31:44,659 --> 00:31:53,010
this the you know when you

00:31:47,490 --> 00:31:55,470
look at the the you're used to in P

00:31:53,010 --> 00:31:56,550
bench you that you own this data the

00:31:55,470 --> 00:31:59,850
data that you collected in your

00:31:56,550 --> 00:32:02,550
hierarchy is dear to you because you

00:31:59,850 --> 00:32:05,280
spend a lot of time making runs to get

00:32:02,550 --> 00:32:07,020
all that data and so when there's a

00:32:05,280 --> 00:32:09,360
problem when you need to go figure out

00:32:07,020 --> 00:32:10,950
why something's not working you need a

00:32:09,360 --> 00:32:13,260
way to get back and look at that data

00:32:10,950 --> 00:32:16,320
and so one of the things we're offering

00:32:13,260 --> 00:32:18,150
in the dashboard is to get a what they

00:32:16,320 --> 00:32:20,550
call of what we call a table of contents

00:32:18,150 --> 00:32:23,040
so we can see the complete hierarchy of

00:32:20,550 --> 00:32:24,660
what was collected in your tarball and

00:32:23,040 --> 00:32:27,390
then be able to go pick it out and go

00:32:24,660 --> 00:32:29,220
look at it and do that without having to

00:32:27,390 --> 00:32:31,740
unpack the whole table all yourself or

00:32:29,220 --> 00:32:34,410
have us unpack the whole table all and

00:32:31,740 --> 00:32:43,520
look at it with a lot of space savings

00:32:34,410 --> 00:32:43,520
and that's it any questions rich

00:32:56,040 --> 00:33:01,150
that's a good question you do not have

00:32:58,600 --> 00:33:04,090
to have a pee bench server to look at

00:33:01,150 --> 00:33:07,180
that we have a little package that loads

00:33:04,090 --> 00:33:08,080
all the CSS and JavaScript packages for

00:33:07,180 --> 00:33:24,040
you so you can just look it on your

00:33:08,080 --> 00:33:26,590
local machine top like I don't know

00:33:24,040 --> 00:33:28,780
which ones is higher but the notion of a

00:33:26,590 --> 00:33:32,470
user is really critical

00:33:28,780 --> 00:33:36,070
it's it is a killer idea not to have a

00:33:32,470 --> 00:33:37,960
delete button we did it because to add a

00:33:36,070 --> 00:33:39,880
user is a lot of work and we had other

00:33:37,960 --> 00:33:41,380
features we all continue to do and we

00:33:39,880 --> 00:33:44,590
figured we throw hardware at the problem

00:33:41,380 --> 00:33:45,880
and you know just keep adding space but

00:33:44,590 --> 00:33:48,100
you can't keep doing that really and

00:33:45,880 --> 00:33:50,440
we've kind of hit the end of that and so

00:33:48,100 --> 00:33:52,360
we need a notion of curation and you

00:33:50,440 --> 00:33:53,470
can't do curation without a user because

00:33:52,360 --> 00:33:56,320
there's no you have to have

00:33:53,470 --> 00:33:59,440
accountability on who curates what and

00:33:56,320 --> 00:34:00,880
so it all gets back to that that feature

00:33:59,440 --> 00:34:03,040
and it's pervasive it has to go all the

00:34:00,880 --> 00:34:03,490
way through the code base to make that

00:34:03,040 --> 00:34:05,980
happen

00:34:03,490 --> 00:34:09,310
second thing that has to happen is on

00:34:05,980 --> 00:34:11,520
the tool side you know we've got all the

00:34:09,310 --> 00:34:14,260
support for those tools but they're

00:34:11,520 --> 00:34:16,659
somewhat idiosyncratic to pee bench and

00:34:14,260 --> 00:34:18,820
that's really not a good thing we want

00:34:16,659 --> 00:34:21,370
to make it so that the notion of the

00:34:18,820 --> 00:34:24,149
tool that you write is independent of P

00:34:21,370 --> 00:34:27,550
bench and so we want to get it so that

00:34:24,149 --> 00:34:29,230
you can specify and that we think that

00:34:27,550 --> 00:34:31,960
the easiest way to do this is via a

00:34:29,230 --> 00:34:34,080
container you specify what you want to

00:34:31,960 --> 00:34:37,560
go run and then we'll just collect it

00:34:34,080 --> 00:34:37,560
off there

00:34:47,070 --> 00:34:52,830
great question question was are there

00:34:50,169 --> 00:34:56,500
any plans to integrate with opus if for

00:34:52,830 --> 00:34:59,560
so today the team uses it with opens to

00:34:56,500 --> 00:35:02,349
four but it's not a pure integration

00:34:59,560 --> 00:35:05,500
with open shift for we have a tool in

00:35:02,349 --> 00:35:09,070
the team being used called ripsaw which

00:35:05,500 --> 00:35:11,650
is a workload driver and we're we're

00:35:09,070 --> 00:35:14,619
working on this tool meister effort to

00:35:11,650 --> 00:35:18,130
containerize the tools so that that

00:35:14,619 --> 00:35:19,900
driver can leverage the tools and still

00:35:18,130 --> 00:35:22,599
give you ap bench result even though

00:35:19,900 --> 00:35:25,510
it's not a you know nappy bench you per

00:35:22,599 --> 00:35:29,680
for P bench Phyo that drivers can still

00:35:25,510 --> 00:35:31,980
end up providing a P bench tarball when

00:35:29,680 --> 00:35:34,960
it's done and so we'll integrate with

00:35:31,980 --> 00:35:41,339
ripsaw which is a operator written

00:35:34,960 --> 00:35:44,339
kubernetes kubernetes operator sorry

00:35:41,339 --> 00:35:44,339
question

00:35:51,710 --> 00:35:56,690
yeah today today you're saying today we

00:35:54,290 --> 00:35:57,890
do use config maps to run all this and

00:35:56,690 --> 00:36:26,540
get it all set up

00:35:57,890 --> 00:36:29,900
yes historically the lot of this started

00:36:26,540 --> 00:36:32,750
back in 2013-2014 and so we were

00:36:29,900 --> 00:36:37,430
adapting existing workflow processes to

00:36:32,750 --> 00:36:39,740
to this we have PCP in this today I

00:36:37,430 --> 00:36:43,309
didn't show it on the slide because it's

00:36:39,740 --> 00:36:46,390
not a great integration with PCP it

00:36:43,309 --> 00:36:49,790
basically collects it uses a PMC D

00:36:46,390 --> 00:36:54,050
process and PM logger on the same node

00:36:49,790 --> 00:36:56,359
and so it kind of replicates all the

00:36:54,050 --> 00:37:00,799
same behavior on all the nodes which is

00:36:56,359 --> 00:37:01,940
not ideal right the tool what we want to

00:37:00,799 --> 00:37:03,619
do with the tool Meister effort

00:37:01,940 --> 00:37:07,130
container izing that is make it so that

00:37:03,619 --> 00:37:10,940
a PMC D container is just started on all

00:37:07,130 --> 00:37:14,809
the nodes that you want and then use a

00:37:10,940 --> 00:37:18,049
local PM logger to go grab all the data

00:37:14,809 --> 00:37:20,540
from those PM CDs because it lessens the

00:37:18,049 --> 00:37:22,849
overhead of what you're doing on all

00:37:20,540 --> 00:37:25,250
those nodes and gives you a central

00:37:22,849 --> 00:37:26,630
place there's problems you know if you

00:37:25,250 --> 00:37:29,030
if you're running a test and you lose a

00:37:26,630 --> 00:37:32,359
node then you lose all the data that you

00:37:29,030 --> 00:37:34,460
collected and so if you one advantage

00:37:32,359 --> 00:37:35,720
will be with PM logger is that if you

00:37:34,460 --> 00:37:37,819
you know if you lose you're controlling

00:37:35,720 --> 00:37:39,349
node jurist your toast anyways but if

00:37:37,819 --> 00:37:41,329
you lose just one of the nodes then you

00:37:39,349 --> 00:37:42,799
just lost some of that data just for a

00:37:41,329 --> 00:37:45,079
little while and till that container

00:37:42,799 --> 00:37:46,910
comes back PM logger will keep adding it

00:37:45,079 --> 00:37:49,130
to its local thing so all of a sudden

00:37:46,910 --> 00:37:50,690
have small gaps rather than hole losses

00:37:49,130 --> 00:37:52,579
of things we have a team member right

00:37:50,690 --> 00:37:55,819
now that goes to Robbie goes through a

00:37:52,579 --> 00:37:58,160
ton of effort to avoid losing data and

00:37:55,819 --> 00:37:59,990
it's it's a pain so yeah we want to go

00:37:58,160 --> 00:38:02,359
do that same things true with Prometheus

00:37:59,990 --> 00:38:04,910
so we want to be able to put exporters

00:38:02,359 --> 00:38:05,619
in in the containers specialize

00:38:04,910 --> 00:38:08,259
exporters

00:38:05,619 --> 00:38:10,630
if you wanted to and then have a local

00:38:08,259 --> 00:38:12,309
Prometheus that you run with your

00:38:10,630 --> 00:38:14,710
torrent you won but people are under the

00:38:12,309 --> 00:38:16,569
test to go scrape all that data keeping

00:38:14,710 --> 00:38:17,049
it locally if they disappear and come

00:38:16,569 --> 00:38:19,029
back

00:38:17,049 --> 00:38:20,740
you only have small gaps but you have

00:38:19,029 --> 00:38:22,809
all your data locally and then you can

00:38:20,740 --> 00:38:24,249
go save it and visualize it going

00:38:22,809 --> 00:38:26,109
forward and only for that duration of

00:38:24,249 --> 00:38:28,720
the test rather than pointing at some

00:38:26,109 --> 00:38:30,609
monstrous you know Prometheus database

00:38:28,720 --> 00:38:38,109
that's there for in the last three days

00:38:30,609 --> 00:38:46,180
it has tons of data so any other

00:38:38,109 --> 00:38:48,180
questions yeah you're late Dan all right

00:38:46,180 --> 00:38:50,050
if there's nothing else

00:38:48,180 --> 00:38:57,800
thank you very much

00:38:50,050 --> 00:38:57,800

YouTube URL: https://www.youtube.com/watch?v=-p32Nak9Gbg


