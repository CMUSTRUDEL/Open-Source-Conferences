Title: Can OVS-DPDK be further optimised?
Publication date: 2019-03-04
Playlist: DevConfCZ 2019
Description: 
	Presentation name: Can OVS-DPDK be further optimised?
Speaker: Eelco Chaudron
Description:  This session is not about how to get more juice out of an existing OVS-DPDK deployment. But what information engineering needs to further optimize OVS-DPDK for the various business needs. We will present a framework that can be used to share deployment details which will help further optimize the OVS-DPDK datapath.
[ https://sched.co/JceS ]
Captions: 
	00:00:03,990 --> 00:00:10,270
okay so everybody welcome my name is

00:00:07,120 --> 00:00:11,850
Simone I work for the networking

00:00:10,270 --> 00:00:14,710
services team like the last presenter

00:00:11,850 --> 00:00:16,779
and today I would like to talk about

00:00:14,710 --> 00:00:19,779
actually the title is quite nice it says

00:00:16,779 --> 00:00:22,029
scan OBS be further optimized but if you

00:00:19,779 --> 00:00:24,609
read the description it's more a little

00:00:22,029 --> 00:00:28,119
bit more about finding a good baseline

00:00:24,609 --> 00:00:31,479
and then tune the performance so we get

00:00:28,119 --> 00:00:32,980
steady results so what does open be

00:00:31,479 --> 00:00:35,530
sweet actually about so open release

00:00:32,980 --> 00:00:37,989
which is a general-purpose virtual

00:00:35,530 --> 00:00:40,390
switch that use open flow rules to make

00:00:37,989 --> 00:00:42,460
the switching decisions and I think what

00:00:40,390 --> 00:00:45,879
I want to emphasize here is the general

00:00:42,460 --> 00:00:48,129
part because it means you work in all

00:00:45,879 --> 00:00:52,120
scenarios and it's not really optimized

00:00:48,129 --> 00:00:54,100
for a specific scenario to to do the

00:00:52,120 --> 00:00:55,719
best performance so if you would like to

00:00:54,100 --> 00:00:58,379
increase the performance a very easy way

00:00:55,719 --> 00:01:00,399
is just to add more CPU resources to it

00:00:58,379 --> 00:01:02,409
and then you can increase the

00:01:00,399 --> 00:01:05,760
performance for probably all the

00:01:02,409 --> 00:01:08,620
scenarios that it's running running on

00:01:05,760 --> 00:01:11,080
so what is the performance for OBS DVD K

00:01:08,620 --> 00:01:14,590
what is it based on so the data path for

00:01:11,080 --> 00:01:16,420
OBS DP DK is basically it's a dedicated

00:01:14,590 --> 00:01:19,450
threat or a number of dedicated threats

00:01:16,420 --> 00:01:22,090
on a dedicated set of CPUs which are

00:01:19,450 --> 00:01:25,570
only used for packet forwarding and what

00:01:22,090 --> 00:01:28,390
it does it gets at most 32 packets from

00:01:25,570 --> 00:01:31,660
a specific driver reach those 32 packets

00:01:28,390 --> 00:01:34,750
and then tries to classifies them with

00:01:31,660 --> 00:01:37,600
specific flows that are configured and

00:01:34,750 --> 00:01:40,390
then when all the flows are all the 32

00:01:37,600 --> 00:01:43,180
up to 33 packets are classified they

00:01:40,390 --> 00:01:44,680
will be grouped by flow and then all the

00:01:43,180 --> 00:01:47,320
actions are taken so there basically

00:01:44,680 --> 00:01:49,660
they put if you have 32 packets come in

00:01:47,320 --> 00:01:51,310
there for two different set of flows we

00:01:49,660 --> 00:01:52,810
will group them together process all the

00:01:51,310 --> 00:01:55,150
actions for one then process all the

00:01:52,810 --> 00:01:57,310
exits for the other the idea behind is

00:01:55,150 --> 00:01:59,650
that your cache is nicely hot so you

00:01:57,310 --> 00:02:03,430
would do the same packet processing know

00:01:59,650 --> 00:02:05,530
all those packets and then once all the

00:02:03,430 --> 00:02:07,120
actions are done based on the group the

00:02:05,530 --> 00:02:10,110
packets are sent out so that means that

00:02:07,120 --> 00:02:10,110
packets get reordered

00:02:10,709 --> 00:02:16,560
per flow enough airflow like per set of

00:02:14,519 --> 00:02:19,799
flows so you might see packet reordering

00:02:16,560 --> 00:02:21,930
in that scenario so what is really

00:02:19,799 --> 00:02:24,629
influencing defective performance in

00:02:21,930 --> 00:02:26,359
general is one of the main things is the

00:02:24,629 --> 00:02:29,370
flow lookup that takes most of the time

00:02:26,359 --> 00:02:31,139
so there is a different set of actions

00:02:29,370 --> 00:02:33,420
that's taken when the fact it comes in

00:02:31,139 --> 00:02:35,489
to figure out what the flow is on the

00:02:33,420 --> 00:02:37,049
DPD K data path it first tries to do an

00:02:35,489 --> 00:02:39,739
exact match so it would calculate the

00:02:37,049 --> 00:02:41,909
hash over to five couples of the packet

00:02:39,739 --> 00:02:43,290
and once those five couples are known

00:02:41,909 --> 00:02:46,379
and we'll try to see if it's in the

00:02:43,290 --> 00:02:49,620
cache this cache is limited I think

00:02:46,379 --> 00:02:51,780
about 8k or default if it hits the cache

00:02:49,620 --> 00:02:54,090
the packet is forwarded directly and

00:02:51,780 --> 00:02:57,060
we're good to go if it misses there is a

00:02:54,090 --> 00:02:59,340
secondary brothers a new up lookup which

00:02:57,060 --> 00:03:01,680
is the signature match SMC that this

00:02:59,340 --> 00:03:03,450
newly introduced in OBS and it's doing

00:03:01,680 --> 00:03:05,129
experimental phases so it's disabled by

00:03:03,450 --> 00:03:07,950
people but it's adds another layer of

00:03:05,129 --> 00:03:10,349
caching and then you have the data path

00:03:07,950 --> 00:03:12,599
classifier that's the next level if it

00:03:10,349 --> 00:03:15,120
finds the flow there it will go back

00:03:12,599 --> 00:03:16,739
into the tree so it will program the EMC

00:03:15,120 --> 00:03:19,019
cash or SMC case if you have that

00:03:16,739 --> 00:03:22,079
enabled and you will go back then the

00:03:19,019 --> 00:03:24,120
packet gets end up if you even miss the

00:03:22,079 --> 00:03:27,419
data path classifier you go to the

00:03:24,120 --> 00:03:29,579
OpenFlow protocol classifier and I've

00:03:27,419 --> 00:03:31,829
marked it as red because if you look at

00:03:29,579 --> 00:03:33,720
the kernel data path the first three

00:03:31,829 --> 00:03:35,609
items the blue ones are actually the

00:03:33,720 --> 00:03:37,530
ones that happen inside of the kernel so

00:03:35,609 --> 00:03:38,939
that's the data path Bart there's

00:03:37,530 --> 00:03:41,699
different caches but it's the same

00:03:38,939 --> 00:03:43,829
principle and then if you hit the red

00:03:41,699 --> 00:03:45,989
type that basically means you go to user

00:03:43,829 --> 00:03:50,040
space so the kernel modules common user

00:03:45,989 --> 00:03:52,259
space and just look up and this is the

00:03:50,040 --> 00:03:53,609
part the handler tracks are taking care

00:03:52,259 --> 00:03:55,739
of so if you see a lot of CPU

00:03:53,609 --> 00:03:57,509
utilization in your kernel data path on

00:03:55,739 --> 00:03:58,919
the handler threats that's probably

00:03:57,509 --> 00:04:01,319
because you have a lot of packets going

00:03:58,919 --> 00:04:06,000
to slow path because there is no no flow

00:04:01,319 --> 00:04:08,280
in the data path what else

00:04:06,000 --> 00:04:10,829
influences the performance for Obst PDK

00:04:08,280 --> 00:04:12,540
I think one thing is the new Texas and

00:04:10,829 --> 00:04:16,109
the locks that are in decode in several

00:04:12,540 --> 00:04:17,980
places there are very limited places in

00:04:16,109 --> 00:04:20,130
the data path that have locks padartha

00:04:17,980 --> 00:04:22,690
like bonding for example has one known

00:04:20,130 --> 00:04:24,280
and then the other thing that I think

00:04:22,690 --> 00:04:28,240
it's probably one of the biggest one is

00:04:24,280 --> 00:04:30,790
the desist calls so they try to make the

00:04:28,240 --> 00:04:33,040
datapath for DP DK as cisco less as

00:04:30,790 --> 00:04:35,170
possible but there are still Siskel's

00:04:33,040 --> 00:04:37,990
being taken due to the existing

00:04:35,170 --> 00:04:40,210
infrastructure of the legacy more like i

00:04:37,990 --> 00:04:42,880
see the kernel data path of OBS so this

00:04:40,210 --> 00:04:44,980
locale lookups one of them i think that

00:04:42,880 --> 00:04:49,290
takes the most syscalls maybe not the

00:04:44,980 --> 00:04:52,690
most cpu but the most syscalls is the

00:04:49,290 --> 00:04:54,610
the locking and the latches the let's

00:04:52,690 --> 00:04:56,830
check you see here which is the use or

00:04:54,610 --> 00:04:59,260
the are see you a lot if they use their

00:04:56,830 --> 00:05:01,270
own RCU library within an OBS and that

00:04:59,260 --> 00:05:03,430
actually takes care by waking up the

00:05:01,270 --> 00:05:05,380
threat by using a right call and you see

00:05:03,430 --> 00:05:09,000
quite a lot of them i have a nice

00:05:05,380 --> 00:05:11,080
example later on and then the cross Numa

00:05:09,000 --> 00:05:13,570
communications also if you have two

00:05:11,080 --> 00:05:15,910
virtual machines both in a different new

00:05:13,570 --> 00:05:18,190
my note their memory transfer is going

00:05:15,910 --> 00:05:23,080
to be at least has to cross the

00:05:18,190 --> 00:05:26,320
interconnect of the two CPUs so what can

00:05:23,080 --> 00:05:28,330
we do as a developer so there's things

00:05:26,320 --> 00:05:29,890
you can change inside the code that

00:05:28,330 --> 00:05:31,960
might make it faster for for a specific

00:05:29,890 --> 00:05:33,610
use case one of the thing that's

00:05:31,960 --> 00:05:37,000
recently be added is the partial

00:05:33,610 --> 00:05:38,650
hardware upload which basically there's

00:05:37,000 --> 00:05:41,620
a nice block from flavia on that as well

00:05:38,650 --> 00:05:44,320
on our block page but what it does it

00:05:41,620 --> 00:05:47,740
takes the packets that come in it tries

00:05:44,320 --> 00:05:49,480
to match up the the flow and then put

00:05:47,740 --> 00:05:51,910
the marker in the hardware saying if you

00:05:49,480 --> 00:05:53,860
see this flow again let me know what the

00:05:51,910 --> 00:05:55,750
flow ID sort of flow marker flow of

00:05:53,860 --> 00:05:57,580
identifier is so you don't have to

00:05:55,750 --> 00:06:01,090
actually go up and do all the lookups

00:05:57,580 --> 00:06:03,610
and do the cache sorry

00:06:01,090 --> 00:06:08,110
hashing algorithm so you directly know

00:06:03,610 --> 00:06:09,730
which flow to pick out of your tale of

00:06:08,110 --> 00:06:12,070
course for harbor upload it's being

00:06:09,730 --> 00:06:14,320
worked on currently upstream that will

00:06:12,070 --> 00:06:16,330
actually take off all the CPU load or at

00:06:14,320 --> 00:06:19,870
least most of the CPU load load of the

00:06:16,330 --> 00:06:21,790
packets you can disable the AMC cache if

00:06:19,870 --> 00:06:23,830
you have a lot of traffic streams

00:06:21,790 --> 00:06:26,920
flowing at the same time and you will be

00:06:23,830 --> 00:06:28,390
trashing your AMC cache it doesn't

00:06:26,920 --> 00:06:29,199
really make sense to have it enabled

00:06:28,390 --> 00:06:30,999
because

00:06:29,199 --> 00:06:32,949
you still have the overhead of

00:06:30,999 --> 00:06:36,159
calculating the hatch and trying to find

00:06:32,949 --> 00:06:37,240
it so you can rid of that part maybe you

00:06:36,159 --> 00:06:40,689
can come up with better hashing

00:06:37,240 --> 00:06:44,319
algorithms to use SMC is one of the

00:06:40,689 --> 00:06:46,240
examples has recently been added another

00:06:44,319 --> 00:06:48,629
thing is if you have multiple threats

00:06:46,240 --> 00:06:51,400
that actually pull your packet drivers

00:06:48,629 --> 00:06:53,580
some interface might be very busy and a

00:06:51,400 --> 00:06:56,289
couple of other interface might be very

00:06:53,580 --> 00:06:58,210
relaxed a couple of packets coming you

00:06:56,289 --> 00:07:01,599
can do rebalancing of the queues for

00:06:58,210 --> 00:07:03,729
different different course if you have a

00:07:01,599 --> 00:07:05,469
very busy core that's doing three queues

00:07:03,729 --> 00:07:06,879
and you have another board it's only

00:07:05,469 --> 00:07:08,710
doing one queue you might be able to

00:07:06,879 --> 00:07:12,999
shift some of that workload to a

00:07:08,710 --> 00:07:16,990
different court so you have you know

00:07:12,999 --> 00:07:18,460
more balance within your processing you

00:07:16,990 --> 00:07:20,680
can try to see if you can remove locks

00:07:18,460 --> 00:07:22,360
within the system it's gonna be a tough

00:07:20,680 --> 00:07:25,509
thing because a lot of code is shared

00:07:22,360 --> 00:07:27,699
between the kernel data path and the db2

00:07:25,509 --> 00:07:29,499
a data path that may be additional data

00:07:27,699 --> 00:07:31,300
that's coming up in the future

00:07:29,499 --> 00:07:32,919
so it might be hard to change because

00:07:31,300 --> 00:07:36,039
it's you know it's an embedded thing

00:07:32,919 --> 00:07:38,219
within the entire infrastructure and

00:07:36,039 --> 00:07:41,020
then we can see we can remove syscalls

00:07:38,219 --> 00:07:42,879
some idea is that I was looking at them

00:07:41,020 --> 00:07:45,310
it's like maybe we can offload some of

00:07:42,879 --> 00:07:47,800
the SIS calls that take a lot of time to

00:07:45,310 --> 00:07:49,839
a separate threat into the system

00:07:47,800 --> 00:07:52,020
it requires probably in another for

00:07:49,839 --> 00:07:54,009
dedicated core but I think there's other

00:07:52,020 --> 00:07:56,199
features that actually also move into

00:07:54,009 --> 00:07:57,909
having an additional core in the system

00:07:56,199 --> 00:08:00,639
so maybe we can you know move stuff out

00:07:57,909 --> 00:08:04,689
an example is for if you have configured

00:08:00,639 --> 00:08:06,009
a external controller and you have an

00:08:04,689 --> 00:08:08,020
unknown packet that needs to be sent to

00:08:06,009 --> 00:08:10,930
the controller currently what what

00:08:08,020 --> 00:08:13,389
happens is it the UDP transmission in

00:08:10,930 --> 00:08:15,129
the kernel data path happens inside the

00:08:13,389 --> 00:08:18,610
PFD threat so basically you're going to

00:08:15,129 --> 00:08:20,379
call a right inside row P and D track

00:08:18,610 --> 00:08:22,719
see I would like to send out this UDP

00:08:20,379 --> 00:08:24,669
packet which is quite costly if you do

00:08:22,719 --> 00:08:27,759
it on your worker threat because even if

00:08:24,669 --> 00:08:29,529
you a couple of million nanoseconds not

00:08:27,759 --> 00:08:31,890
processing packets on ingress you

00:08:29,529 --> 00:08:36,300
probably gonna drop some

00:08:31,890 --> 00:08:37,890
could be a nice enhancement so even if

00:08:36,300 --> 00:08:40,290
we do all that work I mean the main

00:08:37,890 --> 00:08:44,250
question is will we increase the

00:08:40,290 --> 00:08:46,350
performance and I think the the answer

00:08:44,250 --> 00:08:49,500
is that it all depends on your

00:08:46,350 --> 00:08:51,600
environment like we've like I've seen in

00:08:49,500 --> 00:08:53,610
the past is that everybody has a

00:08:51,600 --> 00:08:55,680
different environment and people test

00:08:53,610 --> 00:08:57,630
like different environments as well so

00:08:55,680 --> 00:09:00,600
if you look at people that are doing

00:08:57,630 --> 00:09:02,910
like upstream development on OBS they

00:09:00,600 --> 00:09:05,520
tend to try to optimize a specific use

00:09:02,910 --> 00:09:07,710
case or their specific use case and they

00:09:05,520 --> 00:09:10,170
don't test with all the other

00:09:07,710 --> 00:09:12,060
possibilities of people whether we're

00:09:10,170 --> 00:09:13,920
using OBS because it'll be as is general

00:09:12,060 --> 00:09:17,040
sometimes you might miss a test case for

00:09:13,920 --> 00:09:18,360
someone else so you don't really get the

00:09:17,040 --> 00:09:19,680
performance increase you might even get

00:09:18,360 --> 00:09:21,180
a decrease in your specific scenario

00:09:19,680 --> 00:09:25,680
while other people will get an increase

00:09:21,180 --> 00:09:27,600
in their scenario so what are the

00:09:25,680 --> 00:09:29,940
dependencies for your environment I

00:09:27,600 --> 00:09:33,360
think the number of P and D traits you

00:09:29,940 --> 00:09:35,880
have might be in an issue why why you

00:09:33,360 --> 00:09:39,360
get a better or worse performance for

00:09:35,880 --> 00:09:41,850
example if you have 15 virtual machines

00:09:39,360 --> 00:09:44,580
we don't want Q you need to pull 15

00:09:41,850 --> 00:09:46,470
virtual machines Q's constantly if you

00:09:44,580 --> 00:09:48,870
have one crapping you pull all those 14

00:09:46,470 --> 00:09:50,790
15 Q's on one threat if you make a

00:09:48,870 --> 00:09:54,690
performance optimization in your your

00:09:50,790 --> 00:09:56,430
driver for that V host user you might

00:09:54,690 --> 00:09:59,400
get the performance increase because

00:09:56,430 --> 00:10:02,670
you're pulling like the 15 ones but if

00:09:59,400 --> 00:10:05,190
you have some of the parts that you have

00:10:02,670 --> 00:10:07,350
a hardware driver pulling you know if

00:10:05,190 --> 00:10:08,640
you make V host optimizations and you

00:10:07,350 --> 00:10:10,080
only have Hardware drivers it doesn't

00:10:08,640 --> 00:10:12,390
really help you in that scenario so it's

00:10:10,080 --> 00:10:14,430
pretty important what kind of driver to

00:10:12,390 --> 00:10:16,140
have how many queues you have assigned

00:10:14,430 --> 00:10:18,210
to it because you might be pulling you

00:10:16,140 --> 00:10:20,910
know 15 queues on one hardware device

00:10:18,210 --> 00:10:22,440
driver so that if you make an

00:10:20,910 --> 00:10:25,620
optimization in the hardware device then

00:10:22,440 --> 00:10:29,310
it might help you they're something

00:10:25,620 --> 00:10:30,660
that's very important is is the open

00:10:29,310 --> 00:10:32,250
flow rules that you program and how

00:10:30,660 --> 00:10:35,470
often you change the open flow rules

00:10:32,250 --> 00:10:37,300
because if you have your data path too

00:10:35,470 --> 00:10:38,680
the rules program your open flow rules

00:10:37,300 --> 00:10:40,930
are there and then you make a decision

00:10:38,680 --> 00:10:42,940
to remove all your open flow rules and

00:10:40,930 --> 00:10:44,650
insert a new set that basically means

00:10:42,940 --> 00:10:46,210
you're gonna flush or caches and every

00:10:44,650 --> 00:10:48,340
packet that comes in from a new flow

00:10:46,210 --> 00:10:50,140
needs to be relearned so if you like

00:10:48,340 --> 00:10:51,910
every second to change your open flow

00:10:50,140 --> 00:10:53,230
rules that means that you might get a

00:10:51,910 --> 00:10:56,530
lot of flushes and you get a lot of

00:10:53,230 --> 00:10:58,390
packets to your to your learning to the

00:10:56,530 --> 00:11:01,950
learning phase going all the way up in

00:10:58,390 --> 00:11:01,950
the cache diagram that I showed earlier

00:11:03,000 --> 00:11:08,710
then of course also the data path rules

00:11:05,320 --> 00:11:10,390
that are our active in your system so

00:11:08,710 --> 00:11:13,300
let's say that you have a million flows

00:11:10,390 --> 00:11:15,070
going into your system in theory you

00:11:13,300 --> 00:11:17,200
could have a hundred data million data

00:11:15,070 --> 00:11:19,210
that flows in your system but what the

00:11:17,200 --> 00:11:22,420
system does is that it times them out so

00:11:19,210 --> 00:11:25,000
after three seconds no traffic your flow

00:11:22,420 --> 00:11:27,220
gets removed if a new someone else opens

00:11:25,000 --> 00:11:29,320
a web browser gets a new website that

00:11:27,220 --> 00:11:31,030
flow needs to get at it so if you have

00:11:29,320 --> 00:11:32,440
short live sessions or a long list

00:11:31,030 --> 00:11:34,840
session that also influences the

00:11:32,440 --> 00:11:37,360
performance of your system because you

00:11:34,840 --> 00:11:40,360
need refresh or your cache or not you

00:11:37,360 --> 00:11:42,640
need to be additional lookups around so

00:11:40,360 --> 00:11:45,070
it's not only the volume of the traffic

00:11:42,640 --> 00:11:47,260
the type of the traffic but it's also

00:11:45,070 --> 00:11:50,110
like how many sessions are you tearing

00:11:47,260 --> 00:11:52,030
down up and down in a second do you have

00:11:50,110 --> 00:11:53,950
long list sessions short late sessions

00:11:52,030 --> 00:11:55,210
stuff like that is it the only tcp

00:11:53,950 --> 00:11:56,620
because you have different you have a

00:11:55,210 --> 00:11:58,210
complete flow set is it like a

00:11:56,620 --> 00:12:00,340
completely different protocol that

00:11:58,210 --> 00:12:06,490
you're using or custom protocol that

00:12:00,340 --> 00:12:08,860
you're using so I think from an upstream

00:12:06,490 --> 00:12:12,220
perspective what we really need is a

00:12:08,860 --> 00:12:14,020
bunch of reference architectures where

00:12:12,220 --> 00:12:17,290
we could say you know in the field

00:12:14,020 --> 00:12:21,010
people using XYZ a set of users are

00:12:17,290 --> 00:12:25,960
using another set of setup system so

00:12:21,010 --> 00:12:28,120
maybe some people use it like maybe only

00:12:25,960 --> 00:12:29,980
a couple of virtual machines and that's

00:12:28,120 --> 00:12:31,690
it other people might be running 100

00:12:29,980 --> 00:12:33,850
virtual machines but with low latency

00:12:31,690 --> 00:12:35,770
traffic and other people might run it

00:12:33,850 --> 00:12:40,150
with high latency traffic or new inflows

00:12:35,770 --> 00:12:42,610
or only 10k flows that way if you can

00:12:40,150 --> 00:12:44,140
test it if someone makes it changes

00:12:42,610 --> 00:12:46,540
upstream you can maybe run their

00:12:44,140 --> 00:12:47,430
baseline tests like maybe two five or

00:12:46,540 --> 00:12:50,010
six or whatever

00:12:47,430 --> 00:12:51,750
my tests are setups you have and then

00:12:50,010 --> 00:12:53,279
you can see for which specific use case

00:12:51,750 --> 00:12:54,660
you have an increase performance and you

00:12:53,279 --> 00:12:55,680
can see for what specific use case you

00:12:54,660 --> 00:12:58,589
have maybe you have a decreased

00:12:55,680 --> 00:13:00,870
performance and then it's it's easier to

00:12:58,589 --> 00:13:02,430
make a decision you know are we going to

00:13:00,870 --> 00:13:04,050
apply to pets or are we going to neck

00:13:02,430 --> 00:13:05,610
the patch and say you know he's a bit

00:13:04,050 --> 00:13:10,500
more optimization for a specific use

00:13:05,610 --> 00:13:12,660
case so how do we get that data and I'm

00:13:10,500 --> 00:13:14,100
looking at you guys that I think if

00:13:12,660 --> 00:13:17,730
there are people that are willing to

00:13:14,100 --> 00:13:19,380
share some of the data then that would

00:13:17,730 --> 00:13:25,050
be at least as good start to create a

00:13:19,380 --> 00:13:27,390
use case what I currently do is I test

00:13:25,050 --> 00:13:29,100
like my scenario normally with what we

00:13:27,390 --> 00:13:31,050
call a PvP test and I have a link later

00:13:29,100 --> 00:13:33,600
on if you want to take a look at it the

00:13:31,050 --> 00:13:35,100
Biscay you run a bunch of traffic in you

00:13:33,600 --> 00:13:37,320
loop it back into virtual machine and

00:13:35,100 --> 00:13:45,330
then use you send it back out and based

00:13:37,320 --> 00:13:48,810
on the number of flows so what I've done

00:13:45,330 --> 00:13:50,550
is what will be nice to get so I put a

00:13:48,810 --> 00:13:53,670
list up here with some stuff that would

00:13:50,550 --> 00:13:55,830
be nice to get the pink some of the

00:13:53,670 --> 00:13:58,740
flows dumping some of the ports so we

00:13:55,830 --> 00:14:00,959
have an ID what actually is used in the

00:13:58,740 --> 00:14:03,330
system and we can probably come up with

00:14:00,959 --> 00:14:05,940
the reference architecture for that when

00:14:03,330 --> 00:14:07,770
we were testing I'll just go quickly go

00:14:05,940 --> 00:14:09,900
over it it's in the slides that are

00:14:07,770 --> 00:14:12,360
online so you can if you are willing to

00:14:09,900 --> 00:14:14,339
share your data you can look it up the

00:14:12,360 --> 00:14:17,190
other thing that I have is the SIS calls

00:14:14,339 --> 00:14:19,860
so I've been I'm able to get whatever

00:14:17,190 --> 00:14:21,870
system I want in my setup and I'm but

00:14:19,860 --> 00:14:24,720
it's not clear how to optimize for what

00:14:21,870 --> 00:14:27,390
Cisco because in some scenarios or at

00:14:24,720 --> 00:14:29,490
least in all my scenarios it's in the

00:14:27,390 --> 00:14:30,959
common scenarios I'm not getting a

00:14:29,490 --> 00:14:33,240
better performance if I get rid of the

00:14:30,959 --> 00:14:35,190
Siskel's but it would be nice to see

00:14:33,240 --> 00:14:37,440
what kind of additional Cisco's are

00:14:35,190 --> 00:14:39,360
actually in customer environments or

00:14:37,440 --> 00:14:41,600
test environments that you have and then

00:14:39,360 --> 00:14:45,180
we can probably optimize for specifics

00:14:41,600 --> 00:14:47,279
what I've done to get an ID is what I do

00:14:45,180 --> 00:14:49,529
is I run a Perl script for a couple of

00:14:47,279 --> 00:14:52,290
minutes capturing all the SIS calls that

00:14:49,529 --> 00:14:53,790
the PMD threads January and then what

00:14:52,290 --> 00:14:57,900
I've done I have a small Python script

00:14:53,790 --> 00:15:00,970
that actually tells me what sis calls

00:14:57,900 --> 00:15:02,740
are called for which PMD thread this

00:15:00,970 --> 00:15:03,970
just a the overview screen so it doesn't

00:15:02,740 --> 00:15:06,970
really tell you much only did you get a

00:15:03,970 --> 00:15:09,250
lot of lot of them but then what it also

00:15:06,970 --> 00:15:11,589
does it analyzes the callbacks for all

00:15:09,250 --> 00:15:15,220
the specific sets and it gives you a

00:15:11,589 --> 00:15:16,930
callback for every Cisco and then

00:15:15,220 --> 00:15:21,459
collapse them so in this case you see

00:15:16,930 --> 00:15:24,579
you see that you have like was it 56k of

00:15:21,459 --> 00:15:26,800
callbacks for this specific code path so

00:15:24,579 --> 00:15:29,319
we can try to see what would be the best

00:15:26,800 --> 00:15:33,639
one to optimize if we ever get time to

00:15:29,319 --> 00:15:34,870
optimize it so what's next I think if

00:15:33,639 --> 00:15:38,529
you're willing to share it you can send

00:15:34,870 --> 00:15:40,930
me an email and then my statement would

00:15:38,529 --> 00:15:42,879
be that I would try to you know collect

00:15:40,930 --> 00:15:45,009
them all try to make some general use

00:15:42,879 --> 00:15:47,800
cases out of them share them upstream

00:15:45,009 --> 00:15:50,050
and then for me the ultimate goal would

00:15:47,800 --> 00:15:52,420
then be to actually add them to the link

00:15:50,050 --> 00:15:56,019
that you see here the bottom link sorry

00:15:52,420 --> 00:15:57,939
good copy that is the script for running

00:15:56,019 --> 00:15:59,980
the pvp test and then if I could

00:15:57,939 --> 00:16:02,350
incorporate some of those use test cases

00:15:59,980 --> 00:16:04,480
it will be easy for me to run in the

00:16:02,350 --> 00:16:08,230
gangsta whatever you know set of data or

00:16:04,480 --> 00:16:11,500
patches that are available and then the

00:16:08,230 --> 00:16:13,290
other link is for the Python script that

00:16:11,500 --> 00:16:17,680
if you would like to do someone else's

00:16:13,290 --> 00:16:19,300
the Cisco so that's the date I have so

00:16:17,680 --> 00:16:21,550
hopefully there are some people that are

00:16:19,300 --> 00:16:24,449
willing to share some of their data can

00:16:21,550 --> 00:16:25,870
maybe have a better testing upstream

00:16:24,449 --> 00:16:28,920
that's it

00:16:25,870 --> 00:16:28,920
any questions

00:16:32,300 --> 00:16:46,700
oh yeah yeah no okay if I have to open n

00:16:44,570 --> 00:16:48,470
of e testing this to be a spur fun right

00:16:46,700 --> 00:16:52,430
yeah yeah have you ever tried to set it

00:16:48,470 --> 00:16:55,370
up okay I think that's the answer so so

00:16:52,430 --> 00:16:57,050
yeah it's possible to use that as well

00:16:55,370 --> 00:16:59,120
it has similar features that's what I

00:16:57,050 --> 00:17:01,370
have but it's really hard to set up

00:16:59,120 --> 00:17:03,530
because it requires you to have a

00:17:01,370 --> 00:17:05,390
defined set up and it will do everything

00:17:03,530 --> 00:17:07,550
for you it will configure everything

00:17:05,390 --> 00:17:09,020
download the virtual machines and in the

00:17:07,550 --> 00:17:10,460
development environment it's not really

00:17:09,020 --> 00:17:11,780
friendly because you have your own

00:17:10,460 --> 00:17:13,700
machine you could you want to run a test

00:17:11,780 --> 00:17:15,590
and with this I just have one command

00:17:13,700 --> 00:17:18,530
line I run I get the graph or whatever I

00:17:15,590 --> 00:17:20,600
want and it's done so yeah but it does

00:17:18,530 --> 00:17:24,520
to be a does the performance testing

00:17:20,600 --> 00:17:24,520
test the same features Esther

00:17:30,540 --> 00:17:36,610
yeah yeah but the thing is with they

00:17:34,210 --> 00:17:39,580
open it the dead test suite that only

00:17:36,610 --> 00:17:42,280
does like we send wire speed traffic see

00:17:39,580 --> 00:17:44,230
what the performance is we do our SC two

00:17:42,280 --> 00:17:47,550
four five five test and that's it and

00:17:44,230 --> 00:17:50,970
there is no environment out there that

00:17:47,550 --> 00:17:54,180
that mimics that environment all right

00:17:50,970 --> 00:17:57,460
tell me like one customer that has the

00:17:54,180 --> 00:17:59,710
RFC to former fire type environment

00:17:57,460 --> 00:18:01,710
right and that's the problem because you

00:17:59,710 --> 00:18:04,540
can opt it the thing is you can optimize

00:18:01,710 --> 00:18:06,190
to your likings for that specific test

00:18:04,540 --> 00:18:10,170
scenario but the performance for a

00:18:06,190 --> 00:18:10,170
real-life scenario might be really bad

00:18:59,510 --> 00:19:03,680
yes

00:19:01,550 --> 00:19:07,040
yeah I think the automatic optimization

00:19:03,680 --> 00:19:10,700
is outside of peace which outside of

00:19:07,040 --> 00:19:13,250
sorry yeah so the question was is there

00:19:10,700 --> 00:19:15,080
any working progress to optimize the

00:19:13,250 --> 00:19:17,240
flow set so if you configure the rules

00:19:15,080 --> 00:19:20,660
to optimize them that's your question

00:19:17,240 --> 00:19:22,790
right so from an OBS open flow

00:19:20,660 --> 00:19:24,890
perspective if fourroux optimization

00:19:22,790 --> 00:19:27,680
really rely on the open flow controller

00:19:24,890 --> 00:19:30,650
to do that to get the most you know the

00:19:27,680 --> 00:19:32,510
best set of open flow rules and then the

00:19:30,650 --> 00:19:34,730
data path will do its own optimization

00:19:32,510 --> 00:19:37,910
so if you look at the slides a couple of

00:19:34,730 --> 00:19:41,060
you see the SMC those will do try to do

00:19:37,910 --> 00:19:42,860
some optimization on the given flow to

00:19:41,060 --> 00:19:45,910
be faster to look it up so there is some

00:19:42,860 --> 00:19:45,910
optimization there

00:19:48,760 --> 00:19:54,309
no more questions all right thank you

00:19:54,480 --> 00:19:59,670

YouTube URL: https://www.youtube.com/watch?v=AOdKFcBb_9w


