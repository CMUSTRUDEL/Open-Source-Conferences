Title: Saving money by reducing power consumption
Publication date: 2020-10-07
Playlist: DevConfUS 2020
Description: 
	Speaker: Han Dong

Monitoring and reducing the energy consumption of servers in data centers is critical. Reducing the power drawn by 10,000 datacenter servers by 10 watts (10 joules/second) would result in energy savings that could power approximately 100 US households with a corresponding financial saving of $100,000/year. The factors that affect a server's energy use arise from the complex interactions between the application workload, software stack, and the hardware characteristics and configuration of the node itself. This work sheds light on the task of tuning hardware parameters to control the time and energy required for IO sensitive, system-centric workloads typical of cloud services. In a memcached server, hardware tuning using both Linux and a library OS and record dramatic savings of 440 joules 42%) on Linux and 635 joules 60% on a library OS while maintaining an 99% tail latency under 500 microsecond. We reveal the significant impact that static tuning can have and the influence of operating systems software on the performance of the aforementioned applications.
Captions: 
	00:00:01,439 --> 00:00:06,640
okay we just have one more minute here

00:00:03,919 --> 00:00:07,040
but i guess i can introduce you and

00:00:06,640 --> 00:00:10,320
you'll

00:00:07,040 --> 00:00:13,440
get started at 205 so

00:00:10,320 --> 00:00:15,040
hello everybody here we have han he is

00:00:13,440 --> 00:00:18,160
going to be talking about

00:00:15,040 --> 00:00:19,359
saving money by reducing power

00:00:18,160 --> 00:00:21,600
consumption

00:00:19,359 --> 00:00:21,600
um

00:00:22,480 --> 00:00:26,320
uh returning on energy efficiency

00:00:24,960 --> 00:00:28,840
operating systems so

00:00:26,320 --> 00:00:31,119
i will hand it over to you you can get

00:00:28,840 --> 00:00:34,239
started

00:00:31,119 --> 00:00:34,640
all right thank you for sending this

00:00:34,239 --> 00:00:38,239
talk

00:00:34,640 --> 00:00:38,239
everyone my name

00:00:43,360 --> 00:00:50,000
this is is uh part of my

00:00:46,480 --> 00:00:53,199
anticipation research that i've been uh

00:00:50,000 --> 00:00:53,600
doing with redhat for about a year so

00:00:53,199 --> 00:00:56,719
far

00:00:53,600 --> 00:00:59,600
and kind of um

00:00:56,719 --> 00:01:01,680
this talk is mainly about uh the latest

00:00:59,600 --> 00:01:05,519
the latest results we've had over the

00:01:01,680 --> 00:01:09,439
summer um right so get started

00:01:05,519 --> 00:01:14,479
um so so the kind of

00:01:09,439 --> 00:01:17,920
the motivation for this research

00:01:14,479 --> 00:01:19,920
such as the fact that for like a modern

00:01:17,920 --> 00:01:21,119
machine you can find in a data center

00:01:19,920 --> 00:01:24,640
there's a bunch of

00:01:21,119 --> 00:01:27,640
uh diverse hardware in them and

00:01:24,640 --> 00:01:28,799
even within the hardware itself the

00:01:27,640 --> 00:01:30,960
manufacturers

00:01:28,799 --> 00:01:32,079
have added like layers of

00:01:30,960 --> 00:01:35,840
programmability

00:01:32,079 --> 00:01:36,799
to them so as a as an application

00:01:35,840 --> 00:01:38,880
developer or

00:01:36,799 --> 00:01:40,000
anyone who wants to optimize your

00:01:38,880 --> 00:01:43,040
application on

00:01:40,000 --> 00:01:46,320
these hardware there are different ways

00:01:43,040 --> 00:01:48,320
of doing it and and for this we've kind

00:01:46,320 --> 00:01:51,439
of focused on a

00:01:48,320 --> 00:01:54,159
set of particular

00:01:51,439 --> 00:01:56,799
hardware settings as we call it and the

00:01:54,159 --> 00:01:58,719
focus is kind of on kind of

00:01:56,799 --> 00:02:00,880
minimizing the time it takes to get your

00:01:58,719 --> 00:02:03,520
work done and also

00:02:00,880 --> 00:02:05,040
minimizing the amount of energy to to do

00:02:03,520 --> 00:02:06,960
it also

00:02:05,040 --> 00:02:08,640
and the kind of question that we the

00:02:06,960 --> 00:02:11,120
main question i want to ask is

00:02:08,640 --> 00:02:14,000
you know if assuming your machine is

00:02:11,120 --> 00:02:15,760
only running a single kind of workload

00:02:14,000 --> 00:02:18,319
uh so you can think of it as like you

00:02:15,760 --> 00:02:18,800
know clusters of like memcached servers

00:02:18,319 --> 00:02:22,160
running

00:02:18,800 --> 00:02:23,280
can you actually uh cater the actual

00:02:22,160 --> 00:02:26,959
hardware itself

00:02:23,280 --> 00:02:30,400
uh towards that single application

00:02:26,959 --> 00:02:32,959
and and to actually uh do it we

00:02:30,400 --> 00:02:34,080
uh in this case we've kind of focused on

00:02:32,959 --> 00:02:37,519
uh

00:02:34,080 --> 00:02:40,879
on on three uh uh

00:02:37,519 --> 00:02:44,239
three hardware settings uh from you know

00:02:40,879 --> 00:02:46,160
from from like looking at the manuals um

00:02:44,239 --> 00:02:47,920
and and and one of these hardware

00:02:46,160 --> 00:02:49,200
settings is what we call the uh the

00:02:47,920 --> 00:02:51,200
interweb delay

00:02:49,200 --> 00:02:53,760
so this is a hardware setting that

00:02:51,200 --> 00:02:56,000
exists on

00:02:53,760 --> 00:02:56,800
on the network card and and what it does

00:02:56,000 --> 00:03:00,239
is

00:02:56,800 --> 00:03:03,760
uh by setting a value there it lets you

00:03:00,239 --> 00:03:06,879
set a set a interrupt delay mechanism

00:03:03,760 --> 00:03:07,920
that lets you actually manually set uh

00:03:06,879 --> 00:03:11,360
how long it'll wait

00:03:07,920 --> 00:03:14,159
before a new a new uh a new

00:03:11,360 --> 00:03:15,920
a new interval fires uh when you when

00:03:14,159 --> 00:03:18,879
you get a packet

00:03:15,920 --> 00:03:19,599
and then the other two are are something

00:03:18,879 --> 00:03:22,319
called the

00:03:19,599 --> 00:03:24,400
this this uh frequency scaling so so

00:03:22,319 --> 00:03:26,400
this is ability that exists on

00:03:24,400 --> 00:03:28,080
uh like an intel processor that lets you

00:03:26,400 --> 00:03:30,560
uh set the actual

00:03:28,080 --> 00:03:31,120
processor frequency on that particular

00:03:30,560 --> 00:03:33,680
core

00:03:31,120 --> 00:03:35,280
and then you know by setting it lower

00:03:33,680 --> 00:03:35,680
you're effectively lowering the clock

00:03:35,280 --> 00:03:38,080
rate

00:03:35,680 --> 00:03:38,959
or the the frequency on that on that

00:03:38,080 --> 00:03:42,000
core

00:03:38,959 --> 00:03:44,239
uh and then the final one is

00:03:42,000 --> 00:03:46,239
is something that they've also added

00:03:44,239 --> 00:03:48,959
which lets you set a power limiting

00:03:46,239 --> 00:03:50,480
on on the actual processor package so

00:03:48,959 --> 00:03:53,680
this is something that

00:03:50,480 --> 00:03:54,799
that you can set on the entire uh cpu

00:03:53,680 --> 00:03:56,480
die effectively

00:03:54,799 --> 00:03:58,319
to kind of limit how much power it can

00:03:56,480 --> 00:04:01,760
draw over some time

00:03:58,319 --> 00:04:04,159
and we're we're kind of curious uh

00:04:01,760 --> 00:04:05,439
for our work here uh in terms of

00:04:04,159 --> 00:04:08,879
investigating

00:04:05,439 --> 00:04:12,799
you know uh for for for for two of these

00:04:08,879 --> 00:04:15,280
uh the interrupt delay and this uh dvfs

00:04:12,799 --> 00:04:17,519
uh these these currently they have

00:04:15,280 --> 00:04:20,239
policies that exist inside linux

00:04:17,519 --> 00:04:21,919
to kind of dynamically modify them to

00:04:20,239 --> 00:04:24,240
adjust to the workload

00:04:21,919 --> 00:04:25,199
uh whereas where we're interested in

00:04:24,240 --> 00:04:27,759
seeing

00:04:25,199 --> 00:04:30,320
uh what if you take out all of this

00:04:27,759 --> 00:04:30,960
dynamic policies and just statically set

00:04:30,320 --> 00:04:34,240
them

00:04:30,960 --> 00:04:36,880
and and by starting selling them

00:04:34,240 --> 00:04:39,600
you to take a simple approach all we're

00:04:36,880 --> 00:04:41,280
doing is basically doing a sweep

00:04:39,600 --> 00:04:42,720
through through each of these parameters

00:04:41,280 --> 00:04:44,080
with different values and in different

00:04:42,720 --> 00:04:46,800
combinations of them

00:04:44,080 --> 00:04:48,080
and try to see which ones give us for a

00:04:46,800 --> 00:04:50,000
particular workload

00:04:48,080 --> 00:04:51,280
the ones that can minimize the amount of

00:04:50,000 --> 00:04:54,800
energy and time

00:04:51,280 --> 00:04:57,919
it takes to accomplish that work

00:04:54,800 --> 00:04:58,880
um and and and here we're we're kind of

00:04:57,919 --> 00:05:00,720
introducing

00:04:58,880 --> 00:05:02,720
the idea of using something called

00:05:00,720 --> 00:05:05,840
energy delay product

00:05:02,720 --> 00:05:08,000
uh so this is a concept that uh

00:05:05,840 --> 00:05:10,000
uh architecture folks have used when

00:05:08,000 --> 00:05:12,240
they're trying to

00:05:10,000 --> 00:05:13,280
measure something with with the

00:05:12,240 --> 00:05:16,000
processor

00:05:13,280 --> 00:05:17,759
in this case the energy delay product is

00:05:16,000 --> 00:05:19,680
is a simple multiplication of the amount

00:05:17,759 --> 00:05:21,680
of energy it takes to do some work along

00:05:19,680 --> 00:05:23,360
with the time it takes

00:05:21,680 --> 00:05:24,720
and the the kind of data we're just

00:05:23,360 --> 00:05:26,160
showing here is kind of like a it's an

00:05:24,720 --> 00:05:28,639
artificial data

00:05:26,160 --> 00:05:30,000
uh so on the x-axis you have the time

00:05:28,639 --> 00:05:31,680
taking in some units

00:05:30,000 --> 00:05:33,280
and then on the y-axis you have your

00:05:31,680 --> 00:05:35,759
joules

00:05:33,280 --> 00:05:36,560
and and and the area under the curve or

00:05:35,759 --> 00:05:38,400
the area

00:05:36,560 --> 00:05:40,080
under the slope here is effectively your

00:05:38,400 --> 00:05:42,880
your edp

00:05:40,080 --> 00:05:45,520
and and i and the interesting about the

00:05:42,880 --> 00:05:48,720
the method of using edp is the fact that

00:05:45,520 --> 00:05:50,960
there is a uh there is a slope attached

00:05:48,720 --> 00:05:53,360
with it which kind of which indicates

00:05:50,960 --> 00:05:56,639
the rate of power energy consumption

00:05:53,360 --> 00:05:59,120
over time so you can both compare

00:05:56,639 --> 00:06:00,800
in absolute terms the the amount of you

00:05:59,120 --> 00:06:01,520
know edp as a measure of energy

00:06:00,800 --> 00:06:04,319
efficiency

00:06:01,520 --> 00:06:05,520
along with the fact that you can you can

00:06:04,319 --> 00:06:08,240
compare and contrast

00:06:05,520 --> 00:06:10,319
how how different methodologies or

00:06:08,240 --> 00:06:13,280
different hardware settings in this case

00:06:10,319 --> 00:06:14,880
uh change the rate of the power energy

00:06:13,280 --> 00:06:18,080
consumption

00:06:14,880 --> 00:06:21,280
and and and we're hoping this will

00:06:18,080 --> 00:06:23,600
you know present the data uh

00:06:21,280 --> 00:06:24,560
that that we collected in a way that's

00:06:23,600 --> 00:06:27,919
more interesting

00:06:24,560 --> 00:06:30,160
uh to look at um

00:06:27,919 --> 00:06:32,840
so so here i kind of talked about our

00:06:30,160 --> 00:06:34,720
experimental setup for how we actually

00:06:32,840 --> 00:06:37,840
uh how we actually

00:06:34,720 --> 00:06:38,800
conducted this study so effective what

00:06:37,840 --> 00:06:42,319
we've done is

00:06:38,800 --> 00:06:45,759
we we run a set of network applications

00:06:42,319 --> 00:06:47,280
and and while they're running uh

00:06:45,759 --> 00:06:48,880
you know we have an application that's

00:06:47,280 --> 00:06:52,160
running in linux

00:06:48,880 --> 00:06:54,080
uh and then uh

00:06:52,160 --> 00:06:55,280
and then we have another one that we

00:06:54,080 --> 00:06:56,720
have which is a

00:06:55,280 --> 00:06:58,639
a library operating system or like a

00:06:56,720 --> 00:06:59,919
unit kernel so we can compare between

00:06:58,639 --> 00:07:02,160
systems

00:06:59,919 --> 00:07:03,520
and and and in the hardware these are

00:07:02,160 --> 00:07:04,240
the three hardware settings that we're

00:07:03,520 --> 00:07:07,280
tuning the

00:07:04,240 --> 00:07:07,680
intro delay uh the power limiting and

00:07:07,280 --> 00:07:10,800
the

00:07:07,680 --> 00:07:12,880
and the no and the frequency and

00:07:10,800 --> 00:07:13,840
effective what we've done is we've

00:07:12,880 --> 00:07:17,360
instrumented

00:07:13,840 --> 00:07:19,120
uh a tracing infrastructure inside the

00:07:17,360 --> 00:07:21,360
network device driver

00:07:19,120 --> 00:07:23,199
uh so inside a network device driver

00:07:21,360 --> 00:07:23,759
there's an interweb handling code that

00:07:23,199 --> 00:07:26,560
fires

00:07:23,759 --> 00:07:27,759
every time the card registers or the

00:07:26,560 --> 00:07:30,560
card wants to tell

00:07:27,759 --> 00:07:32,639
the software that hey i have new packets

00:07:30,560 --> 00:07:35,280
and i want you to process them

00:07:32,639 --> 00:07:36,319
inside there we've instrumented various

00:07:35,280 --> 00:07:39,039
counters

00:07:36,319 --> 00:07:40,319
to such as a measurement of how much

00:07:39,039 --> 00:07:42,560
joule

00:07:40,319 --> 00:07:43,520
like how much energy has been used since

00:07:42,560 --> 00:07:45,919
since the last

00:07:43,520 --> 00:07:46,639
time it was interrupted along with like

00:07:45,919 --> 00:07:49,919
a time sam

00:07:46,639 --> 00:07:52,000
counter uh and then

00:07:49,919 --> 00:07:53,919
and then we have a set of other

00:07:52,000 --> 00:07:56,479
statistics we gathered also

00:07:53,919 --> 00:07:57,360
so so we gather some software statistics

00:07:56,479 --> 00:07:59,520
such as you know

00:07:57,360 --> 00:08:00,800
number of bytes they receive and

00:07:59,520 --> 00:08:02,720
transmitted

00:08:00,800 --> 00:08:05,360
along with how many instructions has

00:08:02,720 --> 00:08:08,479
been used how many cycles

00:08:05,360 --> 00:08:08,720
and etc and so effectively you can think

00:08:08,479 --> 00:08:11,360
of

00:08:08,720 --> 00:08:12,960
think of it as we run the experiment and

00:08:11,360 --> 00:08:14,879
we just collect this trace and at the

00:08:12,960 --> 00:08:15,680
end of it we have a giant text file

00:08:14,879 --> 00:08:18,560
where

00:08:15,680 --> 00:08:20,000
every single line that text file is is

00:08:18,560 --> 00:08:20,879
the time stamp for when when the

00:08:20,000 --> 00:08:22,800
interrupt happens

00:08:20,879 --> 00:08:26,639
along with certain hardware statistics

00:08:22,800 --> 00:08:28,800
we've gathered

00:08:26,639 --> 00:08:30,720
and uh for this talk i'm kind of going

00:08:28,800 --> 00:08:33,839
to talk about one of the workloads

00:08:30,720 --> 00:08:36,800
we ran so it's a very simple workload uh

00:08:33,839 --> 00:08:38,479
it's called netpipe and and and we're

00:08:36,800 --> 00:08:39,279
starting with something very simple here

00:08:38,479 --> 00:08:40,719
because

00:08:39,279 --> 00:08:43,200
because there's so many things in your

00:08:40,719 --> 00:08:46,000
plane such as like the hardware along

00:08:43,200 --> 00:08:49,040
with with the operating system itself

00:08:46,000 --> 00:08:52,959
so so with the netpipe application

00:08:49,040 --> 00:08:55,680
all it does is you have two machines and

00:08:52,959 --> 00:08:56,560
and for the machines we we send a

00:08:55,680 --> 00:08:58,480
message

00:08:56,560 --> 00:09:00,959
of some fixed size between them so it's

00:08:58,480 --> 00:09:03,360
like a ping pong back and forth

00:09:00,959 --> 00:09:05,200
and in this case we we set a static

00:09:03,360 --> 00:09:05,600
number of iterations to do this ping

00:09:05,200 --> 00:09:08,959
pong

00:09:05,600 --> 00:09:11,040
which is n and and for one of the

00:09:08,959 --> 00:09:12,399
machines that we call the the server or

00:09:11,040 --> 00:09:14,800
the victim machine

00:09:12,399 --> 00:09:16,640
uh this is the machine where we reset

00:09:14,800 --> 00:09:17,440
these hardware settings of the interrupt

00:09:16,640 --> 00:09:20,320
delay

00:09:17,440 --> 00:09:22,000
the dvfs and the rapple and you know for

00:09:20,320 --> 00:09:24,399
for each workload we basically

00:09:22,000 --> 00:09:25,600
sweep through these three settings of

00:09:24,399 --> 00:09:27,680
different values

00:09:25,600 --> 00:09:29,200
and while that's running we effectively

00:09:27,680 --> 00:09:32,880
collect a trace

00:09:29,200 --> 00:09:36,000
for for every single run run run of this

00:09:32,880 --> 00:09:39,920
um so given this uh

00:09:36,000 --> 00:09:42,959
the first so in this case uh

00:09:39,920 --> 00:09:43,600
uh this graph that we have here is is is

00:09:42,959 --> 00:09:47,240
net pipe

00:09:43,600 --> 00:09:51,440
we're running with a message size of uh

00:09:47,240 --> 00:09:55,760
8192 bytes and then for this workload

00:09:51,440 --> 00:09:57,600
uh we we ran this 5000 iterations

00:09:55,760 --> 00:09:59,279
so so this is just like a very simple

00:09:57,600 --> 00:10:02,560
edp plot

00:09:59,279 --> 00:10:03,360
uh and and the blue line is is is

00:10:02,560 --> 00:10:05,600
effectively

00:10:03,360 --> 00:10:07,760
kind of just default linux where you

00:10:05,600 --> 00:10:09,680
know it has the dynamic policies

00:10:07,760 --> 00:10:11,519
that that govern the interrupt delay and

00:10:09,680 --> 00:10:13,680
the frequency scaling

00:10:11,519 --> 00:10:15,360
so this is kind of like you know linux

00:10:13,680 --> 00:10:18,240
version five point something

00:10:15,360 --> 00:10:19,279
that comes as is and then and then and

00:10:18,240 --> 00:10:22,560
then we run it

00:10:19,279 --> 00:10:25,040
and then this is the edp we got so

00:10:22,560 --> 00:10:26,640
so effectively you know the this is kind

00:10:25,040 --> 00:10:28,959
of its default behavior

00:10:26,640 --> 00:10:30,000
it took about you know 1.3 seconds and

00:10:28,959 --> 00:10:32,480
the amount of

00:10:30,000 --> 00:10:34,560
joules it used was about 20 something

00:10:32,480 --> 00:10:36,000
and we the green line is what we call

00:10:34,560 --> 00:10:38,880
linux tuned

00:10:36,000 --> 00:10:40,480
uh tune in in this case is where we

00:10:38,880 --> 00:10:42,880
where after doing the sweep

00:10:40,480 --> 00:10:43,920
of multiple uh multiple of these

00:10:42,880 --> 00:10:47,200
hardware settings

00:10:43,920 --> 00:10:48,079
uh we we've effectively graphed uh the

00:10:47,200 --> 00:10:51,519
data

00:10:48,079 --> 00:10:53,360
that has uh that has the minimum of evp

00:10:51,519 --> 00:10:56,480
right so the minimum time and minimum

00:10:53,360 --> 00:10:58,480
energy and that's the green line here

00:10:56,480 --> 00:11:00,720
right so we can see that it it took

00:10:58,480 --> 00:11:02,880
drastically way less time

00:11:00,720 --> 00:11:03,760
uh but the rate of its energy

00:11:02,880 --> 00:11:07,519
consumption

00:11:03,760 --> 00:11:09,440
is higher than this default uh one

00:11:07,519 --> 00:11:10,800
but but the cumulative energy consumed

00:11:09,440 --> 00:11:13,839
is is still

00:11:10,800 --> 00:11:14,560
a coup is still way less and then we

00:11:13,839 --> 00:11:17,519
have our

00:11:14,560 --> 00:11:18,079
library operating system uh so so this

00:11:17,519 --> 00:11:20,079
is

00:11:18,079 --> 00:11:21,920
so all these experiments are run bare

00:11:20,079 --> 00:11:24,800
metal and this library operating system

00:11:21,920 --> 00:11:26,480
is this uh this is c plus plus union

00:11:24,800 --> 00:11:29,760
kernel effectively

00:11:26,480 --> 00:11:32,959
uh it's like a it's like event driven uh

00:11:29,760 --> 00:11:34,800
you know no no scheduling uh

00:11:32,959 --> 00:11:36,160
nothing crazy going on inside the kernel

00:11:34,800 --> 00:11:39,200
so it's so it's very

00:11:36,160 --> 00:11:41,279
very lightweight and very efficient uh

00:11:39,200 --> 00:11:42,959
so you can see that it because it uses

00:11:41,279 --> 00:11:46,160
even less time

00:11:42,959 --> 00:11:48,399
uh and and the rate of its uh

00:11:46,160 --> 00:11:51,680
power consumption and energy consumption

00:11:48,399 --> 00:11:54,399
is slightly higher than the default

00:11:51,680 --> 00:11:56,079
um so so to kind of like so given so

00:11:54,399 --> 00:11:59,839
given this initial edp

00:11:56,079 --> 00:12:00,720
uh data uh and and the fact that we have

00:11:59,839 --> 00:12:02,560
this trace

00:12:00,720 --> 00:12:04,560
of you know every single interrupt that

00:12:02,560 --> 00:12:07,040
happens we have a trace of

00:12:04,560 --> 00:12:07,680
basically the behavior on the server

00:12:07,040 --> 00:12:10,959
side

00:12:07,680 --> 00:12:10,959
how can we start to kind of

00:12:36,839 --> 00:12:39,839
is

00:13:04,839 --> 00:13:07,839
um

00:13:16,880 --> 00:13:21,040
so this number represents kind of

00:13:19,760 --> 00:13:22,800
assuming there is no

00:13:21,040 --> 00:13:24,720
atmospheric effects or any switching

00:13:22,800 --> 00:13:27,519
delays or software device this is

00:13:24,720 --> 00:13:28,639
how fast the network will send uh 8

00:13:27,519 --> 00:13:32,000
kilobytes of data

00:13:28,639 --> 00:13:33,920
which is around 6.5 microseconds so this

00:13:32,000 --> 00:13:35,519
so this number kind of lines up very

00:13:33,920 --> 00:13:37,920
nicely within this range

00:13:35,519 --> 00:13:39,600
between the two linux tuned and library

00:13:37,920 --> 00:13:42,160
os tuned

00:13:39,600 --> 00:13:44,320
so effectively you can think of it as

00:13:42,160 --> 00:13:45,279
we've literally set the interrupt delay

00:13:44,320 --> 00:13:47,440
such that

00:13:45,279 --> 00:13:48,800
it just matches the time it takes for

00:13:47,440 --> 00:13:50,720
the wire to

00:13:48,800 --> 00:13:53,360
have the message to be sent across

00:13:50,720 --> 00:13:54,800
assuming some microsecond of delay

00:13:53,360 --> 00:13:57,040
in terms of you know searching costs

00:13:54,800 --> 00:13:58,639
because we have to go through a switch

00:13:57,040 --> 00:14:01,199
right so in this case we literally

00:13:58,639 --> 00:14:03,199
customize the interrupt delay such that

00:14:01,199 --> 00:14:06,079
it matches perfectly

00:14:03,199 --> 00:14:07,760
with the with the theoretical peak speed

00:14:06,079 --> 00:14:09,920
that you can send that message

00:14:07,760 --> 00:14:10,880
and therefore we've like basically

00:14:09,920 --> 00:14:12,880
minimized

00:14:10,880 --> 00:14:14,160
the amount of the amount of basically

00:14:12,880 --> 00:14:15,519
maximized amount of processing

00:14:14,160 --> 00:14:17,440
efficiency here

00:14:15,519 --> 00:14:19,440
and and that that kind of explains the

00:14:17,440 --> 00:14:20,560
kind of the time it takes the time

00:14:19,440 --> 00:14:23,199
differences

00:14:20,560 --> 00:14:24,720
uh because because inside linux default

00:14:23,199 --> 00:14:26,880
it has a kind of like a dynamic

00:14:24,720 --> 00:14:27,680
algorithm where every time it receives a

00:14:26,880 --> 00:14:29,760
new data

00:14:27,680 --> 00:14:30,800
it will do a computation to kind of

00:14:29,760 --> 00:14:33,199
predict

00:14:30,800 --> 00:14:34,480
what is the value it should set the next

00:14:33,199 --> 00:14:38,079
interrupt delay

00:14:34,480 --> 00:14:40,160
value uh and and we found that

00:14:38,079 --> 00:14:41,360
that algorithm itself is just not very

00:14:40,160 --> 00:14:43,120
efficient because it

00:14:41,360 --> 00:14:45,760
it doesn't really adapt to your

00:14:43,120 --> 00:14:49,920
application specific use cases

00:14:45,760 --> 00:14:50,800
um so so so so that explains so this

00:14:49,920 --> 00:14:54,639
kind of explains

00:14:50,800 --> 00:14:56,240
kind of the uh the the throughput uh

00:14:54,639 --> 00:14:58,560
uh the next thing that we're kind of

00:14:56,240 --> 00:15:00,720
curious is uh

00:14:58,560 --> 00:15:02,160
is how can we explain kind of the energy

00:15:00,720 --> 00:15:05,199
benefits of

00:15:02,160 --> 00:15:05,519
of of of this method right like why is

00:15:05,199 --> 00:15:08,720
it

00:15:05,519 --> 00:15:09,519
that we're able to tune uh these three

00:15:08,720 --> 00:15:11,440
parameters

00:15:09,519 --> 00:15:14,160
and we're able to you know decrease the

00:15:11,440 --> 00:15:17,600
amount of energy consumed

00:15:14,160 --> 00:15:18,000
um so i'm going to skip this so so one

00:15:17,600 --> 00:15:21,120
thing

00:15:18,000 --> 00:15:21,920
that we found uh with with one of the

00:15:21,120 --> 00:15:24,800
parameters

00:15:21,920 --> 00:15:25,519
is is the is the cycles it takes to run

00:15:24,800 --> 00:15:28,639
uh

00:15:25,519 --> 00:15:31,759
this so so the cycle is a measure of

00:15:28,639 --> 00:15:33,040
uh from is a measure of uh how many

00:15:31,759 --> 00:15:35,120
cycles it spends

00:15:33,040 --> 00:15:36,320
uh non-idle or busy when it's busy

00:15:35,120 --> 00:15:38,399
processing

00:15:36,320 --> 00:15:40,079
and and you can do a bit of math to

00:15:38,399 --> 00:15:42,880
convert that into a

00:15:40,079 --> 00:15:43,839
time granularity of like similar to the

00:15:42,880 --> 00:15:45,440
time stamp

00:15:43,839 --> 00:15:47,839
and therefore you you can kind of

00:15:45,440 --> 00:15:50,079
estimate uh basically

00:15:47,839 --> 00:15:51,600
in this case we're showing uh the same

00:15:50,079 --> 00:15:53,519
the same timeline

00:15:51,600 --> 00:15:55,279
uh with the fraction that is spent

00:15:53,519 --> 00:15:58,000
non-idle or busy time

00:15:55,279 --> 00:15:59,199
right so what we see here is the fact

00:15:58,000 --> 00:16:02,160
that uh

00:15:59,199 --> 00:16:03,199
it spent it spent majority of its time

00:16:02,160 --> 00:16:06,560
uh

00:16:03,199 --> 00:16:07,519
uh when you're tuning it uh a busy

00:16:06,560 --> 00:16:10,000
processing

00:16:07,519 --> 00:16:11,040
right and like almost 60 for linux tune

00:16:10,000 --> 00:16:14,240
and 40

00:16:11,040 --> 00:16:16,240
uh for uh for the library os

00:16:14,240 --> 00:16:17,839
and then whereas for the default case

00:16:16,240 --> 00:16:20,160
most of the time you know

00:16:17,839 --> 00:16:22,560
it was only about able to do like 20 of

00:16:20,160 --> 00:16:24,480
the time you know uh are busy in

00:16:22,560 --> 00:16:25,120
processing and and what is and what is

00:16:24,480 --> 00:16:28,240
kind of the

00:16:25,120 --> 00:16:31,920
effect here of of of these three uh

00:16:28,240 --> 00:16:34,000
uh of values um

00:16:31,920 --> 00:16:36,240
so so the next thing we plotted was

00:16:34,000 --> 00:16:39,680
basically the amount of time it takes

00:16:36,240 --> 00:16:42,320
uh and so the graph here

00:16:39,680 --> 00:16:42,800
is effectively every time an interrupt

00:16:42,320 --> 00:16:44,959
happens

00:16:42,800 --> 00:16:46,639
we kind of measure how much joules was

00:16:44,959 --> 00:16:48,880
consumed

00:16:46,639 --> 00:16:50,800
right so the same x axis on the timeline

00:16:48,880 --> 00:16:51,279
and these are just each individual point

00:16:50,800 --> 00:16:54,959
is

00:16:51,279 --> 00:16:56,959
how much energy was was consumed

00:16:54,959 --> 00:16:58,240
right so in terms of linux default you

00:16:56,959 --> 00:17:01,440
see that

00:16:58,240 --> 00:17:02,560
uh most of the time it was in this in

00:17:01,440 --> 00:17:05,120
this range

00:17:02,560 --> 00:17:06,559
uh where it did some work and then

00:17:05,120 --> 00:17:09,039
sometimes it peaked

00:17:06,559 --> 00:17:10,160
uh so so the zero data here you can

00:17:09,039 --> 00:17:13,439
ignore because

00:17:10,160 --> 00:17:15,760
um because because the because uh

00:17:13,439 --> 00:17:17,280
the the counter that we used to to read

00:17:15,760 --> 00:17:19,520
from the energy counter

00:17:17,280 --> 00:17:20,880
you can only read it at a granularity of

00:17:19,520 --> 00:17:22,400
one milliseconds

00:17:20,880 --> 00:17:24,480
but then we have intervals coming in

00:17:22,400 --> 00:17:26,400
faster than that so so some of these we

00:17:24,480 --> 00:17:27,600
just we just put them to zero because we

00:17:26,400 --> 00:17:30,320
cannot actually sample

00:17:27,600 --> 00:17:32,000
uh the energy at that point so so

00:17:30,320 --> 00:17:33,200
literally so just focus on kind of this

00:17:32,000 --> 00:17:35,120
middle part

00:17:33,200 --> 00:17:36,400
and what we can see here is that for

00:17:35,120 --> 00:17:39,520
linux default

00:17:36,400 --> 00:17:42,000
uh most of the time is is spent here and

00:17:39,520 --> 00:17:43,679
and and if you sum up every single dot

00:17:42,000 --> 00:17:45,840
here and you divide it by time

00:17:43,679 --> 00:17:47,360
you get energy over time which is a

00:17:45,840 --> 00:17:49,840
measurement of watts

00:17:47,360 --> 00:17:50,480
so here they spend majority of time you

00:17:49,840 --> 00:17:54,240
know

00:17:50,480 --> 00:17:55,840
at 17 watts effectively and 17 watts

00:17:54,240 --> 00:17:57,679
from some other other measurements that

00:17:55,840 --> 00:18:00,240
we've done is is

00:17:57,679 --> 00:18:01,440
is the time that it takes uh is actually

00:18:00,240 --> 00:18:03,360
the amount of uh

00:18:01,440 --> 00:18:04,960
power that the processor takes just just

00:18:03,360 --> 00:18:06,960
idling

00:18:04,960 --> 00:18:09,120
so so what this is saying effectively

00:18:06,960 --> 00:18:09,600
that you know the default policy most of

00:18:09,120 --> 00:18:12,000
the time

00:18:09,600 --> 00:18:12,880
it's doing work is up here and the rest

00:18:12,000 --> 00:18:15,120
of the time it's just

00:18:12,880 --> 00:18:16,320
idling and it's not doing anything and

00:18:15,120 --> 00:18:19,280
that's why

00:18:16,320 --> 00:18:20,080
and that's why effectively the time the

00:18:19,280 --> 00:18:22,480
time here

00:18:20,080 --> 00:18:23,600
is just wasted spent idling and and

00:18:22,480 --> 00:18:24,720
while you're idling you're still

00:18:23,600 --> 00:18:26,880
consuming power

00:18:24,720 --> 00:18:29,120
so so that's kind of like a waste a

00:18:26,880 --> 00:18:32,559
waste of the uh

00:18:29,120 --> 00:18:34,559
the the the amount

00:18:32,559 --> 00:18:36,080
that you're running uh because you're

00:18:34,559 --> 00:18:39,120
just not doing any work

00:18:36,080 --> 00:18:42,160
whereas here when we actually tune the

00:18:39,120 --> 00:18:43,679
these hardware settings uh we we don't

00:18:42,160 --> 00:18:46,080
spend any time idling

00:18:43,679 --> 00:18:47,200
for linux tune at least because almost

00:18:46,080 --> 00:18:48,640
so that's the green

00:18:47,200 --> 00:18:50,400
data points here most of the time it's

00:18:48,640 --> 00:18:52,320
just it's just it's just always doing

00:18:50,400 --> 00:18:54,880
work

00:18:52,320 --> 00:18:56,000
whereas with the library os uh it's a

00:18:54,880 --> 00:18:58,080
bigger spread

00:18:56,000 --> 00:18:59,600
i think i think because the library os

00:18:58,080 --> 00:19:01,840
is way more efficient

00:18:59,600 --> 00:19:03,120
uh just because of his code base that

00:19:01,840 --> 00:19:04,960
when they finish the work if they're

00:19:03,120 --> 00:19:06,480
still able to take advantage of some

00:19:04,960 --> 00:19:08,880
idling costs

00:19:06,480 --> 00:19:10,000
it still it still goes for idling but

00:19:08,880 --> 00:19:14,000
most of the time is

00:19:10,000 --> 00:19:17,280
uh is you know spent also doing work

00:19:14,000 --> 00:19:20,720
um and oops

00:19:17,280 --> 00:19:22,560
yeah so so so so given

00:19:20,720 --> 00:19:23,760
so given this kind of tracing data

00:19:22,560 --> 00:19:26,640
[Music]

00:19:23,760 --> 00:19:28,160
we've you know we've we've done like

00:19:26,640 --> 00:19:30,960
analysis across

00:19:28,160 --> 00:19:32,640
um all these other applications you know

00:19:30,960 --> 00:19:34,960
other than that pipe

00:19:32,640 --> 00:19:36,400
uh at the eight kilobyte that i've

00:19:34,960 --> 00:19:38,400
talked to you about we have some other

00:19:36,400 --> 00:19:39,440
message sizes and we also have results

00:19:38,400 --> 00:19:41,840
from workloads

00:19:39,440 --> 00:19:43,679
uh where you know we run like a node.js

00:19:41,840 --> 00:19:46,799
uh web server

00:19:43,679 --> 00:19:48,480
uh memcached and then

00:19:46,799 --> 00:19:50,160
and overall we collected a bunch of

00:19:48,480 --> 00:19:52,080
trace data so we have about two

00:19:50,160 --> 00:19:56,320
terabytes of this trace data set

00:19:52,080 --> 00:19:58,320
from this work and and kind of

00:19:56,320 --> 00:19:59,440
and kind of our future work uh or our

00:19:58,320 --> 00:20:02,480
current work right now

00:19:59,440 --> 00:20:04,400
is we're in the process of open sourcing

00:20:02,480 --> 00:20:06,799
this set of trace data that we've

00:20:04,400 --> 00:20:09,200
gathered via website

00:20:06,799 --> 00:20:10,880
um and then and then kind of we're

00:20:09,200 --> 00:20:13,600
interested in seeing

00:20:10,880 --> 00:20:14,799
how we can extend the tracing from the

00:20:13,600 --> 00:20:18,240
network card

00:20:14,799 --> 00:20:19,440
into the application or uh to

00:20:18,240 --> 00:20:21,280
to basically have like a kind of

00:20:19,440 --> 00:20:23,520
end-to-end tracing uh

00:20:21,280 --> 00:20:26,000
methodology that will explain both you

00:20:23,520 --> 00:20:28,159
know how the application is behaving

00:20:26,000 --> 00:20:30,240
how the tcp iv stack is behaving up

00:20:28,159 --> 00:20:32,159
until the device driver itself

00:20:30,240 --> 00:20:33,760
um so there are kind of other

00:20:32,159 --> 00:20:35,760
opportunities here where

00:20:33,760 --> 00:20:36,880
we started to taking look into you know

00:20:35,760 --> 00:20:39,039
can we replace

00:20:36,880 --> 00:20:39,919
the dynamic policy with a something

00:20:39,039 --> 00:20:42,960
that's kind of

00:20:39,919 --> 00:20:45,840
powered by machine learning

00:20:42,960 --> 00:20:47,360
uh and and how we can maybe integrate

00:20:45,840 --> 00:20:50,240
these kind of idling

00:20:47,360 --> 00:20:52,080
uh these like periods of processor idle

00:20:50,240 --> 00:20:54,559
into this policy as well

00:20:52,080 --> 00:20:55,520
uh and always you know there's plenty of

00:20:54,559 --> 00:20:57,520
other hardware

00:20:55,520 --> 00:20:58,799
that you can play around with and like

00:20:57,520 --> 00:21:00,240
they're and

00:20:58,799 --> 00:21:03,600
by reading the manual there might be

00:21:00,240 --> 00:21:06,880
other registers uh to look at also

00:21:03,600 --> 00:21:10,320
um all right i so good so so that's the

00:21:06,880 --> 00:21:10,320
end of my talk uh thank you

00:21:12,240 --> 00:21:18,720
great thank you so much um

00:21:15,440 --> 00:21:22,400
i've just been monitoring the chat and

00:21:18,720 --> 00:21:25,760
i do not see any questions

00:21:22,400 --> 00:21:28,320
but if anybody is watching please

00:21:25,760 --> 00:21:29,280
and you have a question um feel free to

00:21:28,320 --> 00:21:32,400
put it in shot

00:21:29,280 --> 00:21:36,080
we'll give it like a minute or so and if

00:21:32,400 --> 00:21:38,080
um if we don't have any forthcoming

00:21:36,080 --> 00:21:39,520
i guess we can move over to the breakout

00:21:38,080 --> 00:21:43,360
session

00:21:39,520 --> 00:21:45,919
um at that point i'll just put a link to

00:21:43,360 --> 00:21:49,440
the breakout session

00:21:45,919 --> 00:21:49,440
in a little while

00:21:56,840 --> 00:21:59,840
second

00:22:13,600 --> 00:22:18,080
okay i don't see anything coming so

00:22:15,200 --> 00:22:20,320
thank you so much for the time

00:22:18,080 --> 00:22:20,320

YouTube URL: https://www.youtube.com/watch?v=xyfGq4hfkzs


