Title: Kvm Forum 2013: Linux Storage Stack for the Cloud - Yeela Kaplan, Red Hat
Publication date: 2014-10-30
Playlist: KVM Forum 2013
Description: 
	The Linux storage stack is composed of many layers, starting at the device itself (HW), through the kernel modules, user space daemons and the various administration tools. In this session we will review how oVirt utilizes these components in order to implement storage virtualization, using both file and block shared storage, while facing the challenges introduced when working on a distributed system. We will review the usage of the Linux storage stack by oVirt and the architecture decisions at the heart of oVirt storage by introducing the oVirt storage APIs. We shall present how the lessons learned by five years of oVirt storage development can be applied to the openstack storage stack.
Captions: 
	00:00:00,740 --> 00:00:08,429
so everyone again today we're going to

00:00:05,520 --> 00:00:11,010
talk about the Linux story stock for the

00:00:08,429 --> 00:00:14,670
cloud first of all I want to introduce

00:00:11,010 --> 00:00:18,570
myself I mean I'm working gal for red

00:00:14,670 --> 00:00:20,189
hat i'm at the cloud storage team and

00:00:18,570 --> 00:00:24,980
i've been contributing to overage for

00:00:20,189 --> 00:00:28,410
the past 3 i'm living in tel aviv bed

00:00:24,980 --> 00:00:32,610
with my best friend snowy as you can see

00:00:28,410 --> 00:00:36,390
here so what I want to talk to you about

00:00:32,610 --> 00:00:41,850
today is the design principles behind

00:00:36,390 --> 00:00:44,879
the of which George system so what is

00:00:41,850 --> 00:00:47,280
the agenda for today first the wall I

00:00:44,879 --> 00:00:48,750
want to talk about why do we even need

00:00:47,280 --> 00:00:52,140
storage virtualization when talking

00:00:48,750 --> 00:00:55,710
about virtual machines and how we do it

00:00:52,140 --> 00:00:57,510
and challenges when the solutions we're

00:00:55,710 --> 00:01:03,300
talking about the enterprise and the

00:00:57,510 --> 00:01:05,339
overt design and implementation so first

00:01:03,300 --> 00:01:09,240
of all why do we need storage

00:01:05,339 --> 00:01:12,570
virtualization when running a virtual

00:01:09,240 --> 00:01:15,180
machine on a single hypervisor and the

00:01:12,570 --> 00:01:18,090
virtual machine requires at least so the

00:01:15,180 --> 00:01:20,909
next solution would be to just add a

00:01:18,090 --> 00:01:23,640
physical disk into the Machine and but

00:01:20,909 --> 00:01:27,240
what happens when we want to run 100

00:01:23,640 --> 00:01:30,780
grams on this hypervisor and we have a

00:01:27,240 --> 00:01:34,380
link to that physical disk interfaces

00:01:30,780 --> 00:01:38,700
but even if we did under each disk has a

00:01:34,380 --> 00:01:41,040
fixed time and even if you could join

00:01:38,700 --> 00:01:45,570
the disk then we would have a

00:01:41,040 --> 00:01:48,570
performance issues and we have a storage

00:01:45,570 --> 00:01:50,100
area limitations and also we have

00:01:48,570 --> 00:01:55,200
problems with using that multiple

00:01:50,100 --> 00:01:58,649
storage arrays so we use storage

00:01:55,200 --> 00:02:00,930
virtualization which is that what we

00:01:58,649 --> 00:02:05,579
want to do is they create a virtual

00:02:00,930 --> 00:02:07,829
device with this behavior the most basic

00:02:05,579 --> 00:02:10,890
implementation is a partition table as

00:02:07,829 --> 00:02:12,270
you know and we also have a storage

00:02:10,890 --> 00:02:17,340
arrays than that I'll

00:02:12,270 --> 00:02:19,980
vm which we'll talk about later so what

00:02:17,340 --> 00:02:23,490
are the benefits from using a storage

00:02:19,980 --> 00:02:26,460
IRA translation we get that Facebook

00:02:23,490 --> 00:02:29,100
stability we can create devices when we

00:02:26,460 --> 00:02:32,100
need them and we can create a snapshot

00:02:29,100 --> 00:02:38,190
which are shallow copies that provides

00:02:32,100 --> 00:02:41,580
us hope you on right capability so when

00:02:38,190 --> 00:02:45,030
talking about an image in overt we first

00:02:41,580 --> 00:02:49,290
define what is a snapshot a snapshot is

00:02:45,030 --> 00:02:52,380
the volume and each image is a virtual

00:02:49,290 --> 00:02:56,850
disk for a virtual machine and it is

00:02:52,380 --> 00:03:02,070
composed of many volumes each volume is

00:02:56,850 --> 00:03:05,120
just as sequence of blocks so what is

00:03:02,070 --> 00:03:08,160
the actual problem we're talking about

00:03:05,120 --> 00:03:11,790
when we're talking about visualization

00:03:08,160 --> 00:03:15,150
in the enterprise we have different

00:03:11,790 --> 00:03:18,270
needs than just running a virtual

00:03:15,150 --> 00:03:20,430
machine on a single hypervisor we

00:03:18,270 --> 00:03:22,590
actually may have multiple data center

00:03:20,430 --> 00:03:25,200
we need to data center we might have

00:03:22,590 --> 00:03:28,440
hundreds of hosts on each dose hundreds

00:03:25,200 --> 00:03:30,690
of virtual machines with multiple disc

00:03:28,440 --> 00:03:33,300
on each virtual machine and potentially

00:03:30,690 --> 00:03:36,690
thousands of snapshots for each disk

00:03:33,300 --> 00:03:39,209
image so this is a problem on a very

00:03:36,690 --> 00:03:42,750
very large scale and we need that

00:03:39,209 --> 00:03:47,880
dedicated their management solution so

00:03:42,750 --> 00:03:50,220
that's why we have of it so what I want

00:03:47,880 --> 00:03:53,040
to do now is take this a very large

00:03:50,220 --> 00:03:57,150
scale problem and just decoupling into a

00:03:53,040 --> 00:04:00,480
three well-defined challenges the first

00:03:57,150 --> 00:04:04,320
one is that we want the ability to run a

00:04:00,480 --> 00:04:06,900
virtual machines on a chair Austin in

00:04:04,320 --> 00:04:11,940
the data center so we'll be able to

00:04:06,900 --> 00:04:13,470
migrate the virtual machines also we

00:04:11,940 --> 00:04:16,109
need to deal with a very large part of

00:04:13,470 --> 00:04:21,510
your volumes and an enormous size of

00:04:16,109 --> 00:04:24,570
storage so these are a challenge and now

00:04:21,510 --> 00:04:25,840
let's talk about the solution so the

00:04:24,570 --> 00:04:29,139
name resolution

00:04:25,840 --> 00:04:31,360
for dealing with virtual machines that

00:04:29,139 --> 00:04:33,490
are seen bennett is just when migrating

00:04:31,360 --> 00:04:37,000
the virtual machine just copied a tease

00:04:33,490 --> 00:04:40,090
but this is a very large overhead for us

00:04:37,000 --> 00:04:41,949
so what we want to do is use their

00:04:40,090 --> 00:04:47,050
shared storage as you can see in the

00:04:41,949 --> 00:04:49,990
second demonstration so we've talked

00:04:47,050 --> 00:04:53,110
about shared storage for quantity we

00:04:49,990 --> 00:04:56,470
want to create the volumes only when we

00:04:53,110 --> 00:04:58,150
need them and I use templates in order

00:04:56,470 --> 00:05:00,970
to reduce the quantity of volumes and

00:04:58,150 --> 00:05:05,830
use that central database for managed

00:05:00,970 --> 00:05:08,110
fermenting the volumes as for size we

00:05:05,830 --> 00:05:10,330
know that a physical computer only uses

00:05:08,110 --> 00:05:13,180
about fifty percent of itself physical

00:05:10,330 --> 00:05:15,910
storage and for VMS it's even less so

00:05:13,180 --> 00:05:19,030
what we can do is use of a commitment of

00:05:15,910 --> 00:05:21,910
storage and also a thin provisioning

00:05:19,030 --> 00:05:24,370
which i was writing only the data the

00:05:21,910 --> 00:05:27,130
virtual machine actually uses into the

00:05:24,370 --> 00:05:29,580
risk and also here again we use

00:05:27,130 --> 00:05:32,740
templates that abscess in addition to

00:05:29,580 --> 00:05:34,479
reducing the quantity of volumes reduce

00:05:32,740 --> 00:05:39,700
the size of the storage because we have

00:05:34,479 --> 00:05:42,220
share data between virtual machines so

00:05:39,700 --> 00:05:44,169
now let's move on to the average

00:05:42,220 --> 00:05:46,930
implementation generally we have a

00:05:44,169 --> 00:05:49,360
centralized management which is the

00:05:46,930 --> 00:05:51,789
engine that we want a discuss here we'll

00:05:49,360 --> 00:05:54,340
discuss their hypervisor control mate

00:05:51,789 --> 00:05:56,680
which are actually and managing the

00:05:54,340 --> 00:06:01,440
physical storage down the line storage

00:05:56,680 --> 00:06:04,000
so first of all let's talk about the

00:06:01,440 --> 00:06:07,240
Oviatt implementation for snapshots we

00:06:04,000 --> 00:06:11,560
use that the cube how to format for both

00:06:07,240 --> 00:06:14,260
the file and block backing storage it

00:06:11,560 --> 00:06:21,669
provides us with copy-on-write and clean

00:06:14,260 --> 00:06:24,400
volumes now I will move on to the final

00:06:21,669 --> 00:06:27,240
implementation of storage which is the a

00:06:24,400 --> 00:06:30,669
bit more as simple and straightforward

00:06:27,240 --> 00:06:36,880
since we use that file system mechanisms

00:06:30,669 --> 00:06:39,190
to manage it so in overt hm each volume

00:06:36,880 --> 00:06:39,670
when we're talking about fast storage is

00:06:39,190 --> 00:06:43,030
income

00:06:39,670 --> 00:06:44,770
there's a file so as for quantity we can

00:06:43,030 --> 00:06:48,520
create and manage to the files just

00:06:44,770 --> 00:06:51,820
using the file system API and we're

00:06:48,520 --> 00:06:57,880
Richard Kevin a link to the number of

00:06:51,820 --> 00:07:01,420
files as for size we get dynamic sizing

00:06:57,880 --> 00:07:05,890
by the file system and some some file

00:07:01,420 --> 00:07:09,130
system give us a sparse files which also

00:07:05,890 --> 00:07:13,410
helps us reduce the size and at 4shared

00:07:09,130 --> 00:07:17,260
storage we reuse then as usually NFS and

00:07:13,410 --> 00:07:20,260
the file system romantic scared of daksa

00:07:17,260 --> 00:07:24,340
synchronization for us so this is the

00:07:20,260 --> 00:07:27,040
very straightforward now well we move on

00:07:24,340 --> 00:07:32,500
to the local competition which is that a

00:07:27,040 --> 00:07:36,310
bit more a charging so when talking

00:07:32,500 --> 00:07:39,460
about block down we have a few other

00:07:36,310 --> 00:07:40,990
challenges like if you think before

00:07:39,460 --> 00:07:44,620
about fun now we need to ask ourselves

00:07:40,990 --> 00:07:46,990
how do we even create a block device how

00:07:44,620 --> 00:07:51,010
many block devices are supported on

00:07:46,990 --> 00:07:53,620
physical machine as for size how do we

00:07:51,010 --> 00:08:00,430
recite the block volume and these things

00:07:53,620 --> 00:08:03,850
provision even possible for us so the

00:08:00,430 --> 00:08:07,710
most obvious solution that comes to mind

00:08:03,850 --> 00:08:10,300
is just a using remote torrent and it

00:08:07,710 --> 00:08:13,360
surprises the ability to create and

00:08:10,300 --> 00:08:15,490
manage volumes it also has a native and

00:08:13,360 --> 00:08:17,860
the provisioning and naked snapshots

00:08:15,490 --> 00:08:21,670
which are the exact thing that we need

00:08:17,860 --> 00:08:25,600
and that we mean sorry um but to have a

00:08:21,670 --> 00:08:28,000
permanent because we have a in the

00:08:25,600 --> 00:08:31,060
market different storage vendors and

00:08:28,000 --> 00:08:33,220
different models so we don't have a

00:08:31,060 --> 00:08:35,530
standard interface so you probably have

00:08:33,220 --> 00:08:38,260
asked yourselves why do we even want to

00:08:35,530 --> 00:08:39,970
go for all these troubles so so the

00:08:38,260 --> 00:08:42,190
first reason is actually the most

00:08:39,970 --> 00:08:47,140
important one that we have customer

00:08:42,190 --> 00:08:51,520
requirements and also we want to avoid

00:08:47,140 --> 00:08:54,400
that the file system overhead so

00:08:51,520 --> 00:08:56,940
what we chose to do in order to get a

00:08:54,400 --> 00:09:02,380
unified interface is the yourself p.m.

00:08:56,940 --> 00:09:06,730
which gives us volume management and

00:09:02,380 --> 00:09:10,840
we'll talk about it later that's the

00:09:06,730 --> 00:09:13,000
first of all in order to get the shared

00:09:10,840 --> 00:09:16,920
storage we still need to use a remote

00:09:13,000 --> 00:09:19,870
storage which brings us to using a son

00:09:16,920 --> 00:09:23,050
so we're talking about the sound we have

00:09:19,870 --> 00:09:27,280
a different set of terminal terminology

00:09:23,050 --> 00:09:30,010
story initiate or target early on that

00:09:27,280 --> 00:09:35,950
defines a good a globally unique ID of

00:09:30,010 --> 00:09:38,590
loon so the famine transports discussed

00:09:35,950 --> 00:09:43,270
comments over I skazhi and fibre channel

00:09:38,590 --> 00:09:47,380
technologies and the Sun provides for us

00:09:43,270 --> 00:09:50,380
a redundancy which means that each and I

00:09:47,380 --> 00:09:54,130
provider sees the multiple targets for

00:09:50,380 --> 00:09:56,110
the same one so the question that comes

00:09:54,130 --> 00:09:59,280
to mind is how can we tell it it's the

00:09:56,110 --> 00:10:03,190
same LAN so this brings us back to the

00:09:59,280 --> 00:10:06,340
fresh start and a point which is that we

00:10:03,190 --> 00:10:09,250
have the GUI the global indicating so

00:10:06,340 --> 00:10:15,910
what we'll do is use that another day of

00:10:09,250 --> 00:10:18,010
solution which is wholly pal so I now

00:10:15,910 --> 00:10:22,180
we'll move on to our special I use of

00:10:18,010 --> 00:10:24,460
billions compound so what we do when

00:10:22,180 --> 00:10:27,960
using multi path is when we have a

00:10:24,460 --> 00:10:36,310
storage connectivity problem and then

00:10:27,960 --> 00:10:39,280
right then we fell fast on the path and

00:10:36,310 --> 00:10:42,130
post the vm in order to avoid the i/o

00:10:39,280 --> 00:10:45,430
ever reaching the guest operating system

00:10:42,130 --> 00:10:50,110
and disturbing the application once we

00:10:45,430 --> 00:10:54,960
detect the story 2050 is a bath then we

00:10:50,110 --> 00:10:54,960
ought to resume the virtual machine and

00:10:57,600 --> 00:11:04,680
so are multiple I uses a device mother

00:11:02,230 --> 00:11:09,000
in order to map

00:11:04,680 --> 00:11:12,210
the different physical devices into into

00:11:09,000 --> 00:11:16,080
a virtual block device wonderific block

00:11:12,210 --> 00:11:19,110
device so it uses a device matter such

00:11:16,080 --> 00:11:22,500
as other wrap limbs compounds and we do

00:11:19,110 --> 00:11:27,810
the same user also with lvm and for

00:11:22,500 --> 00:11:31,440
device mother so up until now we've

00:11:27,810 --> 00:11:33,930
talked about solving the shirt storage a

00:11:31,440 --> 00:11:36,540
challenge now I will move on to the

00:11:33,930 --> 00:11:42,779
quantity in size that challenges we

00:11:36,540 --> 00:11:45,089
talked about earlier so um as I said we

00:11:42,779 --> 00:11:48,720
use that lvm which gives us that I

00:11:45,089 --> 00:11:52,430
unified interface and we implement just

00:11:48,720 --> 00:11:55,620
like 45 we implemented each volume as a

00:11:52,430 --> 00:11:59,339
farm-out implement each volume as a as

00:11:55,620 --> 00:12:02,040
an lv and we get easy for fishing we're

00:11:59,339 --> 00:12:04,830
using every create another mode and we

00:12:02,040 --> 00:12:10,350
have our own implementation and 14

00:12:04,830 --> 00:12:12,480
provisioning which uses a lv extent so

00:12:10,350 --> 00:12:15,589
up until now it's very standard now i

00:12:12,480 --> 00:12:22,140
will talk about our specialists use of

00:12:15,589 --> 00:12:26,450
LG m and so for what we did for a thin

00:12:22,140 --> 00:12:29,670
provisioning is that we didn't use the

00:12:26,450 --> 00:12:31,680
LPN native thin provisioning because if

00:12:29,670 --> 00:12:36,300
it not scale well for us in a clustered

00:12:31,680 --> 00:12:39,200
environment so we define each volume to

00:12:36,300 --> 00:12:42,180
have an initial size of 1 gigabyte and

00:12:39,200 --> 00:12:46,650
we extended the logical volume when we

00:12:42,180 --> 00:12:50,880
get an inner space we tried to avoid

00:12:46,650 --> 00:12:55,950
that by monitoring qmr and identified

00:12:50,880 --> 00:13:02,459
the high-water mark in order to avoid

00:12:55,950 --> 00:13:05,130
causing the vm so as I said we need--ah

00:13:02,459 --> 00:13:10,140
we are working in a clustered

00:13:05,130 --> 00:13:14,190
environment and we have their vm

00:13:10,140 --> 00:13:18,360
operations debian oestrogen operations

00:13:14,190 --> 00:13:21,540
which are create remove and extend over

00:13:18,360 --> 00:13:25,950
logical volumes which are only VG meta

00:13:21,540 --> 00:13:30,690
data right and simultaneous right I can

00:13:25,950 --> 00:13:35,399
cause a meta data corruption so that's

00:13:30,690 --> 00:13:36,839
why we need a clustering solution so we

00:13:35,399 --> 00:13:40,950
could have used sylvian but it did not

00:13:36,839 --> 00:13:42,589
scale well for us so we need to find a

00:13:40,950 --> 00:13:44,670
solution with no synchronization

00:13:42,589 --> 00:13:51,630
mechanisms in order to avoid that

00:13:44,670 --> 00:13:54,570
contention so we'll get back to our to

00:13:51,630 --> 00:13:56,880
our implementation of the solution but

00:13:54,570 --> 00:14:02,040
first of all let's talk a bit about our

00:13:56,880 --> 00:14:05,670
special configuration of lvm so first of

00:14:02,040 --> 00:14:10,260
all fedora and values uses the lvm by

00:14:05,670 --> 00:14:12,540
default and so what we wanted to do is

00:14:10,260 --> 00:14:16,380
use a separate configuration in order to

00:14:12,540 --> 00:14:19,110
avoid the affecting any other

00:14:16,380 --> 00:14:24,209
applications running on the host so we

00:14:19,110 --> 00:14:29,040
use the runtime configuration and also

00:14:24,209 --> 00:14:31,860
by default lvm scans are all devices on

00:14:29,040 --> 00:14:35,130
the host and we wanted to avoid this in

00:14:31,860 --> 00:14:38,760
order to speed up to speed up our LVN

00:14:35,130 --> 00:14:42,149
operation which are very expensive so in

00:14:38,760 --> 00:14:44,279
terms of performance of course so we do

00:14:42,149 --> 00:14:46,949
is that we use a vm short users because

00:14:44,279 --> 00:14:51,630
we actually know which devices belong to

00:14:46,949 --> 00:14:57,410
us and and which we learned them compose

00:14:51,630 --> 00:14:59,940
hv g on our system so this helps us said

00:14:57,410 --> 00:15:02,100
to compartmentalize program and also

00:14:59,940 --> 00:15:04,019
where we can avoid accessing any other

00:15:02,100 --> 00:15:10,800
devices the dome dome for us on the

00:15:04,019 --> 00:15:14,699
cross also another special thing we do

00:15:10,800 --> 00:15:18,240
is then that usually all the volumes in

00:15:14,699 --> 00:15:20,610
the hosts are activated but since we are

00:15:18,240 --> 00:15:24,000
working the in the enterprise we have a

00:15:20,610 --> 00:15:26,550
very very large one of your volumes so

00:15:24,000 --> 00:15:30,209
what we want to do is keep a the number

00:15:26,550 --> 00:15:32,120
of devices law in order to to speed up

00:15:30,209 --> 00:15:38,300
the operations and

00:15:32,120 --> 00:15:41,240
I had to help her for months so and what

00:15:38,300 --> 00:15:43,100
we do is activate the volumes only when

00:15:41,240 --> 00:15:47,570
we actually need them and we also assume

00:15:43,100 --> 00:15:51,290
that that each host is the story that

00:15:47,570 --> 00:15:54,080
each heavy is the is accessed only by

00:15:51,290 --> 00:15:56,450
one Hospital at each point in time so

00:15:54,080 --> 00:15:59,240
that's why it helps us avoid refreshes

00:15:56,450 --> 00:16:06,170
of the logical volume is mainly when we

00:15:59,240 --> 00:16:11,240
need them also for each PP we have by

00:16:06,170 --> 00:16:14,660
default one method area so for a VG we

00:16:11,240 --> 00:16:18,470
may have multiple not mainly by default

00:16:14,660 --> 00:16:21,520
have multiple other areas this can cause

00:16:18,470 --> 00:16:25,100
a corruption in the cluster environment

00:16:21,520 --> 00:16:29,690
so what we chose to do is use only one

00:16:25,100 --> 00:16:33,260
active method at area at the time we

00:16:29,690 --> 00:16:41,050
also implemented the afib metadata as a

00:16:33,260 --> 00:16:43,610
LG env tetons and in order to avoid an

00:16:41,050 --> 00:16:47,870
unauthorized the storage layer

00:16:43,610 --> 00:16:51,620
separation from like like I said like

00:16:47,870 --> 00:16:53,839
create and expand as i said before then

00:16:51,620 --> 00:17:05,800
we use the lock type foreign such

00:16:53,839 --> 00:17:05,800
operations which is a read-only ok so

00:17:06,100 --> 00:17:16,010
that's a let's move on to our clustering

00:17:09,500 --> 00:17:18,860
solution which is a SPM it's the storage

00:17:16,010 --> 00:17:22,130
pool manager it's a row we find one of

00:17:18,860 --> 00:17:25,670
the hosts in our data center and it can

00:17:22,130 --> 00:17:29,270
be migrated to any one of the horse it

00:17:25,670 --> 00:17:32,210
is responsible for the sarge day

00:17:29,270 --> 00:17:36,110
operation which are creation deletion

00:17:32,210 --> 00:17:39,170
and manipulation volumes and it is the

00:17:36,110 --> 00:17:45,090
single metadata writer in our data

00:17:39,170 --> 00:17:49,020
center so the algorithm used

00:17:45,090 --> 00:17:55,320
if the is based on the chocolate and

00:17:49,020 --> 00:18:00,539
Mulkey algorithm which is a cluster

00:17:55,320 --> 00:18:04,049
solution for for shared storage it gives

00:18:00,539 --> 00:18:07,230
us a single cover balita and the

00:18:04,049 --> 00:18:11,309
primitives of lease renewal it gives us

00:18:07,230 --> 00:18:14,100
a uniform solution which means that it

00:18:11,309 --> 00:18:18,240
scares well and no matter how many costs

00:18:14,100 --> 00:18:21,210
we have in our data center which is why

00:18:18,240 --> 00:18:24,659
we didn't use the Delphian negative

00:18:21,210 --> 00:18:30,679
solution and and it is simple and

00:18:24,659 --> 00:18:42,950
efficient to implement any questions

00:18:30,679 --> 00:18:47,450
okay so for we also a I used on lock

00:18:42,950 --> 00:18:51,120
like a scam it is the it is a based on

00:18:47,450 --> 00:18:53,700
the chalk line rocky algorithm to

00:18:51,120 --> 00:18:57,450
determine that the cluster membership

00:18:53,700 --> 00:19:01,230
for the host and yeah and the thing does

00:18:57,450 --> 00:19:03,690
differently is that it leases a are

00:19:01,230 --> 00:19:06,570
based on that this parts of South orita

00:19:03,690 --> 00:19:09,510
which allows the start to acquire the

00:19:06,570 --> 00:19:16,980
resource saying subseconds which is much

00:19:09,510 --> 00:19:19,289
faster than the previous algorithm so

00:19:16,980 --> 00:19:22,380
the summarize what we talked about and

00:19:19,289 --> 00:19:24,559
so why do we even that mean the storage

00:19:22,380 --> 00:19:28,970
virtualization for a virtual machines

00:19:24,559 --> 00:19:33,000
the operative limitation for snapshots

00:19:28,970 --> 00:19:35,700
profile and local information and how we

00:19:33,000 --> 00:19:37,500
use the multipath device marker lvm an

00:19:35,700 --> 00:19:49,320
hour clustering solution which is the

00:19:37,500 --> 00:19:52,090
SPM so do you have any questions ok so

00:19:49,320 --> 00:19:54,150
thank you very much

00:19:52,090 --> 00:19:54,150

YouTube URL: https://www.youtube.com/watch?v=hH4EIz9yoQg


