Title: [2016] Quo Vadis Virtio? by Michael Tsirkin
Publication date: 2016-09-05
Playlist: KVM Forum 2016
Description: 
	Last version of virtio was published about a year ago: in october 2016. The virtio community has been idle since: implementations are constantly being improved, new features are in the works, or are being discussed. This talk will present an update on current status of the implementations and the challengest they face, and look forward to how we may address these challenges. It will highlight some of the proposed new features and the benefits they might bring. It will also suggest untested ideas for new applications for virtio on which (to presenter's knowledge) no one is currently working - some of these might turn out to be impractical, but most should serve as useful starting points for a discussion.


Michael S. Tsirkin
Red Hat, Senior Principal Software Engineer

Michael S. Tsirkin works on KVM at Red Hat. He is the chair of the virtio technical committee; virtio is the basis of most para-virtualized solutions in use with KVM today. He is the maintainer of virtio and vhost subsystems in Linux as well as PC and PCI subsystems in QEMU. On several occasions he has been recognized as one of the most prolific reviewers and contributors of code to Linux and QEMU. Michael's past speaking experience includes presentations on previous KVM Forum events.

Slides: http://www.linux-kvm.org/images/b/b3/02x06A-Michael_Tsirkin-Quo_Vadis_Virtio.pdf
Captions: 
	00:00:09,769 --> 00:00:15,030
hi I'm Michael circuit from Red Hat I'm

00:00:12,570 --> 00:00:17,730
the chair of the guilty or technical

00:00:15,030 --> 00:00:19,949
committee and part of my job I maintain

00:00:17,730 --> 00:00:23,789
the beauty of family of para virtualized

00:00:19,949 --> 00:00:27,090
interfaces this talk is going to be

00:00:23,789 --> 00:00:29,039
about future directions for VTO so it's

00:00:27,090 --> 00:00:31,769
not going to be a status update I chose

00:00:29,039 --> 00:00:38,640
to focus on where we are heading next

00:00:31,769 --> 00:00:41,250
this year and the next one so the first

00:00:38,640 --> 00:00:43,710
thing that's going to well first thing

00:00:41,250 --> 00:00:46,559
I'm going to mention is that will tie or

00:00:43,710 --> 00:00:49,680
120 is heading towards becoming an oasis

00:00:46,559 --> 00:00:52,260
standard the current status is a

00:00:49,680 --> 00:00:54,539
committee specification this means that

00:00:52,260 --> 00:00:57,600
it went through public review these are

00:00:54,539 --> 00:00:59,489
lots of issues to become an Asia

00:00:57,600 --> 00:01:01,800
standard the requirement is to have

00:00:59,489 --> 00:01:04,350
three statements of use and I'm happy to

00:01:01,800 --> 00:01:06,960
report that it all three levels in the

00:01:04,350 --> 00:01:11,100
stack we have at least three

00:01:06,960 --> 00:01:12,780
implementations you can use linux kernel

00:01:11,100 --> 00:01:15,240
drivers and windows kernel drivers

00:01:12,780 --> 00:01:19,130
within the guest and you can also use d

00:01:15,240 --> 00:01:19,130
pdk userspace drivers with and guest

00:01:19,280 --> 00:01:29,820
device rooms supporting viet I one the 0

00:01:24,049 --> 00:01:36,210
exists in I used by bios of EMF and open

00:01:29,820 --> 00:01:39,860
firmer and on the host side GMU has full

00:01:36,210 --> 00:01:42,960
support for vertigo and 0 0 additionally

00:01:39,860 --> 00:01:46,350
dpd case supports it through the V host

00:01:42,960 --> 00:01:51,240
library and there are be hostnet and

00:01:46,350 --> 00:01:54,840
sketchy drivers in the linux so we are

00:01:51,240 --> 00:01:59,100
all set with that and we are starting to

00:01:54,840 --> 00:02:02,159
plan the next version the next version

00:01:59,100 --> 00:02:05,490
of the vertigo specification which will

00:02:02,159 --> 00:02:08,009
be revision 1 dot one and there I expect

00:02:05,490 --> 00:02:10,940
us first of all to move forward with

00:02:08,009 --> 00:02:13,050
standardization of new interfaces

00:02:10,940 --> 00:02:15,150
implementations for which are

00:02:13,050 --> 00:02:17,790
ready upstream so this includes

00:02:15,150 --> 00:02:23,580
relatively new devices such as vertigo

00:02:17,790 --> 00:02:27,180
input and vertigo GPU you tell a socket

00:02:23,580 --> 00:02:29,730
device and maybe some all the devices

00:02:27,180 --> 00:02:32,640
like the 9p file system if we can find

00:02:29,730 --> 00:02:36,450
someone who can write it up volunteers

00:02:32,640 --> 00:02:39,030
are always welcome so we also going to

00:02:36,450 --> 00:02:41,880
standardize some recent upstream

00:02:39,030 --> 00:02:45,390
features such as virtual I'm mu support

00:02:41,880 --> 00:02:47,910
which is required for example for safe

00:02:45,390 --> 00:02:50,610
usage of guest user space drivers views

00:02:47,910 --> 00:02:54,570
viertel devices and I'm going to discuss

00:02:50,610 --> 00:02:57,800
this a bit late on the stock that's not

00:02:54,570 --> 00:03:01,200
all i expect the slew of new devices

00:02:57,800 --> 00:03:04,970
first wheelchair crypto device which

00:03:01,200 --> 00:03:09,440
offload scripto capabilities to the host

00:03:04,970 --> 00:03:09,440
the persistent storage device a

00:03:10,220 --> 00:03:15,620
signaling device which can send

00:03:12,720 --> 00:03:17,730
interrupt two cpus that's kind of

00:03:15,620 --> 00:03:22,260
interesting and I'm going to discuss

00:03:17,730 --> 00:03:28,590
this a bit later maybe a shared memory

00:03:22,260 --> 00:03:32,550
device unlikely some more existing

00:03:28,590 --> 00:03:35,070
devices will gain new features I'm

00:03:32,550 --> 00:03:38,430
rather confident that balloon device

00:03:35,070 --> 00:03:41,010
will gain ability to report page usage

00:03:38,430 --> 00:03:43,760
hints to the host and this is helpful to

00:03:41,010 --> 00:03:50,190
improve the placement of guest memory

00:03:43,760 --> 00:03:53,459
under hypervisor we might see a new

00:03:50,190 --> 00:03:56,489
transport integrated in the standard we

00:03:53,459 --> 00:03:58,739
host pci is very interesting it allows

00:03:56,489 --> 00:04:02,690
one virtual machine to access the memory

00:03:58,739 --> 00:04:05,430
of another using a vertical device and

00:04:02,690 --> 00:04:07,739
finally we will see a bunch of changes

00:04:05,430 --> 00:04:10,560
that enable your performance

00:04:07,739 --> 00:04:15,420
optimizations in some of these i'm going

00:04:10,560 --> 00:04:19,500
to discuss next so here's starting the

00:04:15,420 --> 00:04:22,620
charts so how much overhead does passing

00:04:19,500 --> 00:04:26,280
messages between host and guest cpus

00:04:22,620 --> 00:04:27,000
using birthday or incur but this chart

00:04:26,280 --> 00:04:30,120
shows fair

00:04:27,000 --> 00:04:31,830
request processing time dependent on the

00:04:30,120 --> 00:04:33,990
batch size and by batch size here I

00:04:31,830 --> 00:04:37,920
simply mean the maximum number of

00:04:33,990 --> 00:04:39,660
outstanding requests in the queue so you

00:04:37,920 --> 00:04:42,420
can see that increasing batching size

00:04:39,660 --> 00:04:45,270
decreases the processing time p requests

00:04:42,420 --> 00:04:47,730
why would that be this is not due to

00:04:45,270 --> 00:04:50,340
signaling because this test actually

00:04:47,730 --> 00:04:53,400
used falling exclusively on both the

00:04:50,340 --> 00:04:55,260
host the Prospero do was the host the

00:04:53,400 --> 00:04:58,650
consumer and guess the producer of

00:04:55,260 --> 00:05:05,790
requests so there are two main reasons

00:04:58,650 --> 00:05:08,940
for batch size being helpful the first

00:05:05,790 --> 00:05:11,460
has to do with pipelining so if we limit

00:05:08,940 --> 00:05:14,400
ourselves to a single outstanding buffer

00:05:11,460 --> 00:05:17,130
then CP used running was the host and a

00:05:14,400 --> 00:05:19,860
guest spend a bunch of time idly waiting

00:05:17,130 --> 00:05:22,380
for a new request to arrive in case of

00:05:19,860 --> 00:05:24,330
the host over the previous request to be

00:05:22,380 --> 00:05:27,630
protest so a new one can be added in

00:05:24,330 --> 00:05:29,370
case of the guests and we have to count

00:05:27,630 --> 00:05:32,790
this time right as part of the

00:05:29,370 --> 00:05:35,970
processing time for the request so if we

00:05:32,790 --> 00:05:38,550
just allow guests to submit multiple

00:05:35,970 --> 00:05:40,550
buffers synchronously suddenly

00:05:38,550 --> 00:05:43,440
processing time per battle decreases

00:05:40,550 --> 00:05:47,130
because the buffers are now processed

00:05:43,440 --> 00:05:51,090
and previously i told time right so

00:05:47,130 --> 00:05:54,479
that's one reason why batching is

00:05:51,090 --> 00:05:58,169
helpful but there is another one and it

00:05:54,479 --> 00:06:01,550
has to do with cpu caching sony two cpus

00:05:58,169 --> 00:06:04,050
are communicating using shared memory

00:06:01,550 --> 00:06:05,729
one of this would typically modify a

00:06:04,050 --> 00:06:09,770
location in memory and then another one

00:06:05,729 --> 00:06:14,610
has to read it and like it proceeds in

00:06:09,770 --> 00:06:16,800
repeatedly and when this happens the

00:06:14,610 --> 00:06:20,010
caches of the two cpu have to be

00:06:16,800 --> 00:06:21,930
synchronized this could involve Cashin

00:06:20,010 --> 00:06:24,930
validations this could involve Odd

00:06:21,930 --> 00:06:28,680
Couple synchronizations cache misses and

00:06:24,930 --> 00:06:35,940
the number of these mrs. directly

00:06:28,680 --> 00:06:37,950
impacts communication latency so I'm

00:06:35,940 --> 00:06:40,350
going to show that as the increase

00:06:37,950 --> 00:06:42,540
batching the number of these mrs.

00:06:40,350 --> 00:06:46,770
decreases right that's why performance

00:06:42,540 --> 00:06:51,660
goes up let's take a look at we retire

00:06:46,770 --> 00:06:55,800
120 this is roughly how a verte lq

00:06:51,660 --> 00:06:58,290
virtual queue looks you have five data

00:06:55,800 --> 00:07:01,320
structures linked together you have to

00:06:58,290 --> 00:07:03,630
access all of these in order to pass a

00:07:01,320 --> 00:07:07,500
message to host and get notification

00:07:03,630 --> 00:07:09,960
that message has been processed you have

00:07:07,500 --> 00:07:12,600
the available index pointing into

00:07:09,960 --> 00:07:15,180
available ring punch into descriptor and

00:07:12,600 --> 00:07:20,220
then you store the process descriptor

00:07:15,180 --> 00:07:24,120
index in the used ring and update the

00:07:20,220 --> 00:07:26,340
used index so we have five parts and

00:07:24,120 --> 00:07:28,470
this buffer moves between host and the

00:07:26,340 --> 00:07:32,340
guests each of these parts is written

00:07:28,470 --> 00:07:35,820
and read at least once this means at

00:07:32,340 --> 00:07:40,890
least five cache misses their buffer if

00:07:35,820 --> 00:07:43,140
no batching is allowed now with batching

00:07:40,890 --> 00:07:46,110
the pear butter overhead is lower

00:07:43,140 --> 00:07:49,470
because multiple buffers fit in a single

00:07:46,110 --> 00:07:51,060
cache line how many buffers would fit in

00:07:49,470 --> 00:07:54,240
a single cache line depends on a batch

00:07:51,060 --> 00:07:57,480
size and depends on a cache line size so

00:07:54,240 --> 00:08:01,280
for example if we have batch sizes 24

00:07:57,480 --> 00:08:05,280
and the cache line happens to be 64 byte

00:08:01,280 --> 00:08:07,560
then each cache line holds four entries

00:08:05,280 --> 00:08:11,340
of each kind read of each of the five

00:08:07,560 --> 00:08:18,990
kinds and this gives us five meters per

00:08:11,340 --> 00:08:22,160
40 quest rate 1 and 2 25 so that's the

00:08:18,990 --> 00:08:24,990
second reason why batching is helpful

00:08:22,160 --> 00:08:28,050
between the two effects which one is the

00:08:24,990 --> 00:08:32,190
more influential is there some way to

00:08:28,050 --> 00:08:36,930
measure how much overhead does each of

00:08:32,190 --> 00:08:44,070
the two effects contribute so I found

00:08:36,930 --> 00:08:46,380
one trick to do this and we could try to

00:08:44,070 --> 00:08:49,350
answer this by running the test and

00:08:46,380 --> 00:08:51,330
hyper threaded system now a hyper thread

00:08:49,350 --> 00:08:55,820
is in fact the partition but within a

00:08:51,330 --> 00:08:59,880
physical CPU and hyper said Sheree cash

00:08:55,820 --> 00:09:04,020
so the cache synchronization affects

00:08:59,880 --> 00:09:06,300
mostly go away most of the performance

00:09:04,020 --> 00:09:08,670
overhead when running consumer and

00:09:06,300 --> 00:09:12,000
producer and to hyper stress within a

00:09:08,670 --> 00:09:15,000
single CPU will be due to the pipelining

00:09:12,000 --> 00:09:16,680
in the processing overhead right so

00:09:15,000 --> 00:09:19,320
unfortunately it's not in clean

00:09:16,680 --> 00:09:21,570
experiment because hyper search actually

00:09:19,320 --> 00:09:23,839
share some more CP resources and they

00:09:21,570 --> 00:09:26,490
might conflict and slow each other down

00:09:23,839 --> 00:09:29,430
but I think it's still interesting it

00:09:26,490 --> 00:09:34,800
kind of gives you an upper you know

00:09:29,430 --> 00:09:37,050
upper bound and the on the overhead of

00:09:34,800 --> 00:09:39,600
the processing in the pipelining and

00:09:37,050 --> 00:09:41,910
hopefully at least to some extent

00:09:39,600 --> 00:09:48,960
exclude the cache synchronization

00:09:41,910 --> 00:09:50,850
overheads so this is the chart and you

00:09:48,960 --> 00:09:52,950
see that most of the time in the

00:09:50,850 --> 00:09:55,740
benchmark in this benchmark is spent on

00:09:52,950 --> 00:09:59,040
cash synchronization especially without

00:09:55,740 --> 00:10:01,110
batching so the issue is these five

00:09:59,040 --> 00:10:04,080
parts of the queue right its most

00:10:01,110 --> 00:10:07,560
noticeably most noticeable without

00:10:04,080 --> 00:10:10,320
patching when these five parts of the

00:10:07,560 --> 00:10:14,089
queue causes five cache misses an ish

00:10:10,320 --> 00:10:17,970
access well can't we do something faster

00:10:14,089 --> 00:10:21,440
for your tail one good one so here's an

00:10:17,970 --> 00:10:24,120
idea and there is actually prototype and

00:10:21,440 --> 00:10:27,600
that you can find this part of the tools

00:10:24,120 --> 00:10:30,720
directory in Linux kernel so instead of

00:10:27,600 --> 00:10:33,770
using five data structures we will try

00:10:30,720 --> 00:10:36,960
to pack everything into the single

00:10:33,770 --> 00:10:41,130
16-byte descriptor that we already have

00:10:36,960 --> 00:10:44,190
anyway right so let's try to see how can

00:10:41,130 --> 00:10:47,630
we do this is this is color coded so the

00:10:44,190 --> 00:10:51,030
these things that are the same color

00:10:47,630 --> 00:10:54,510
that they map from the entire 120 to one

00:10:51,030 --> 00:10:55,649
dot one so that we have the address

00:10:54,510 --> 00:10:58,529
field it

00:10:55,649 --> 00:11:00,839
change we have the lens it also goes and

00:10:58,529 --> 00:11:02,939
changed just copy it in the new

00:11:00,839 --> 00:11:08,939
descriptor format and we have the flags

00:11:02,939 --> 00:11:10,920
a couple of bits that's mostly the

00:11:08,939 --> 00:11:14,699
descriptor I'm going to discuss next a

00:11:10,920 --> 00:11:17,879
bit later the next pointer next we have

00:11:14,699 --> 00:11:21,059
the ID field ID appears in the available

00:11:17,879 --> 00:11:23,490
and the used bring instead of putting

00:11:21,059 --> 00:11:26,550
these in separate rings we will put it

00:11:23,490 --> 00:11:28,740
as part of the descriptor itself and we

00:11:26,550 --> 00:11:32,850
will store the descriptors in the ring

00:11:28,740 --> 00:11:35,550
structure so the idea has to actual to

00:11:32,850 --> 00:11:38,939
it performs two functions one is it

00:11:35,550 --> 00:11:41,670
identifies the descriptor and five and

00:11:38,939 --> 00:11:44,279
second it helps us locate it in the

00:11:41,670 --> 00:11:46,410
descriptor buffer so now that we put

00:11:44,279 --> 00:11:49,079
descriptors in a ring structure they

00:11:46,410 --> 00:11:54,240
just go one after another we don't need

00:11:49,079 --> 00:11:57,649
to use ID find the descriptor right it's

00:11:54,240 --> 00:12:00,779
just an identifier for hosting the guest

00:11:57,649 --> 00:12:02,490
unique identifier and this also means

00:12:00,779 --> 00:12:05,009
that we don't need the next pointer

00:12:02,490 --> 00:12:06,600
there is a next flag in the flags field

00:12:05,009 --> 00:12:09,360
and that's enough to tell you there is a

00:12:06,600 --> 00:12:13,110
descriptor next just go ahead and use it

00:12:09,360 --> 00:12:16,889
you don't need a pointer okay what's

00:12:13,110 --> 00:12:20,850
next right the lens field is also copied

00:12:16,889 --> 00:12:22,769
in the used ring we do not need that we

00:12:20,850 --> 00:12:24,179
can just write directly into the

00:12:22,769 --> 00:12:26,189
descriptor this means that we will make

00:12:24,179 --> 00:12:28,499
the script a writable by the host as

00:12:26,189 --> 00:12:32,399
well and then we can reuse the lens

00:12:28,499 --> 00:12:35,069
field sounds good what else is left

00:12:32,399 --> 00:12:39,540
right there is the available index and

00:12:35,069 --> 00:12:43,980
the used index so this serves two

00:12:39,540 --> 00:12:48,059
purposes one is they just mark a buffer

00:12:43,980 --> 00:12:49,740
valid for host guests to use so a single

00:12:48,059 --> 00:12:52,829
bit in the descriptor will be enough

00:12:49,740 --> 00:12:54,600
with that that's simple the serve

00:12:52,829 --> 00:12:57,049
another purpose and that is to allow

00:12:54,600 --> 00:12:59,579
passes batches of descriptor atomically

00:12:57,049 --> 00:13:01,589
between host and guest for example the

00:12:59,579 --> 00:13:05,939
mirable buffer feature in the networking

00:13:01,589 --> 00:13:08,040
device uses that so we can just label

00:13:05,939 --> 00:13:09,130
descriptors if we have a batch we can

00:13:08,040 --> 00:13:11,470
label the first

00:13:09,130 --> 00:13:15,540
the middle multiple middle descriptors

00:13:11,470 --> 00:13:18,940
on the last and that just needs to bits

00:13:15,540 --> 00:13:21,250
we don't need full indices for this so

00:13:18,940 --> 00:13:24,250
we get total of three extra bits instead

00:13:21,250 --> 00:13:26,530
of 32 bits so like we pack we packed

00:13:24,250 --> 00:13:30,370
everything into one writable descriptor

00:13:26,530 --> 00:13:33,610
now let's take a look it houses

00:13:30,370 --> 00:13:36,640
compactly out is used so to make

00:13:33,610 --> 00:13:38,740
batteries available to host just just

00:13:36,640 --> 00:13:41,640
write them out in descriptors in the

00:13:38,740 --> 00:13:44,710
ring and then set the valid bit to 1 and

00:13:41,640 --> 00:13:48,400
this implies that it's now ok for host

00:13:44,710 --> 00:13:51,330
to use these descriptors what can

00:13:48,400 --> 00:13:55,330
process them in any order we keep this

00:13:51,330 --> 00:13:57,400
feature of your thigh 120 each

00:13:55,330 --> 00:13:59,500
descriptor has an identifier field and

00:13:57,400 --> 00:14:01,890
after processing host will write the

00:13:59,500 --> 00:14:05,320
process descriptor ID in the ring and

00:14:01,890 --> 00:14:08,170
then flip the valid bit to 0 and this

00:14:05,320 --> 00:14:11,230
tells a guest ok this entry now has been

00:14:08,170 --> 00:14:16,090
used the guests can now reuse entry for

00:14:11,230 --> 00:14:19,170
new buffers so you're going to see that

00:14:16,090 --> 00:14:21,970
this is much easier to implement then

00:14:19,170 --> 00:14:26,590
built I 120 the data structure is

00:14:21,970 --> 00:14:29,650
drastically simpler here's just to the

00:14:26,590 --> 00:14:31,510
code for the host and this one kind of

00:14:29,650 --> 00:14:35,710
assumes that hostess processing entries

00:14:31,510 --> 00:14:37,300
in order and as you can see descriptor

00:14:35,710 --> 00:14:40,030
possessing involves first of all

00:14:37,300 --> 00:14:42,610
breathing the descriptor to make sure

00:14:40,030 --> 00:14:44,410
that the valid bit is set and once we

00:14:42,610 --> 00:14:47,440
detect that it's set we process again

00:14:44,410 --> 00:14:51,580
scripter then we clear the valid bit and

00:14:47,440 --> 00:14:53,170
this is right now if you are lucky the

00:14:51,580 --> 00:14:56,230
read will pull the descriptor in our

00:14:53,170 --> 00:14:59,890
cash and then the right we'll just do a

00:14:56,230 --> 00:15:02,440
cash access if we are not likely then

00:14:59,890 --> 00:15:04,810
boss might cause a cache miss or

00:15:02,440 --> 00:15:08,740
invalidation it all architecture

00:15:04,810 --> 00:15:11,470
dependent so this means that on the host

00:15:08,740 --> 00:15:13,690
we get between one and two mrs. to

00:15:11,470 --> 00:15:17,860
process it descriptor still better than

00:15:13,690 --> 00:15:21,529
five or eight and the same thing happens

00:15:17,860 --> 00:15:24,529
on the guest it's kind of symmetrical

00:15:21,529 --> 00:15:30,069
so if we do the math we get between two

00:15:24,529 --> 00:15:30,069
and four mr. buffer busines batching and

00:15:30,100 --> 00:15:36,410
have two one miss when matching up to

00:15:33,709 --> 00:15:39,079
four buffers this is still better than

00:15:36,410 --> 00:15:42,410
we're tie or even in the worst case at

00:15:39,079 --> 00:15:44,300
least in theory but as I said I did

00:15:42,410 --> 00:15:47,600
write a small prototype so let's take a

00:15:44,300 --> 00:15:49,480
look at the numbers and you see that

00:15:47,600 --> 00:15:51,439
yeah they look pretty well

00:15:49,480 --> 00:15:55,819
unsurprisingly otherwise I wouldn't be

00:15:51,439 --> 00:15:58,819
putting this in the presentation rate so

00:15:55,819 --> 00:16:01,850
without patching we managed to reduce

00:15:58,819 --> 00:16:04,990
the overhead of the caching almost by a

00:16:01,850 --> 00:16:07,970
factor of two right if we consider the

00:16:04,990 --> 00:16:10,100
if we consider the caching affect

00:16:07,970 --> 00:16:14,209
everything on top of the hyper serrated

00:16:10,100 --> 00:16:16,579
bar right then we reduce that almost by

00:16:14,209 --> 00:16:19,910
a factor of two and with Hugh's bigger

00:16:16,579 --> 00:16:23,029
batching it's kind of smaller but

00:16:19,910 --> 00:16:24,860
still-still very noticeable I also

00:16:23,029 --> 00:16:28,279
tested the hyper serrated performance

00:16:24,860 --> 00:16:29,930
with this different layout to measure

00:16:28,279 --> 00:16:32,300
the pipelining and processing overhead

00:16:29,930 --> 00:16:36,230
and as you can see it performs more or

00:16:32,300 --> 00:16:38,180
less the same as 120 so better cash

00:16:36,230 --> 00:16:44,660
behaved it probably the main reason that

00:16:38,180 --> 00:16:46,850
this perform as well but just the throw

00:16:44,660 --> 00:16:52,220
performance is not the only benefit of

00:16:46,850 --> 00:16:55,100
this compact layout it actually us to

00:16:52,220 --> 00:16:58,250
address other performance problems that

00:16:55,100 --> 00:17:00,949
we have had for a while so consider a

00:16:58,250 --> 00:17:03,439
networking device some workloads

00:17:00,949 --> 00:17:05,780
exchange predominantly small packets but

00:17:03,439 --> 00:17:09,260
do need to get a large packet once in a

00:17:05,780 --> 00:17:11,449
while the way we solve it now invert I o

00:17:09,260 --> 00:17:14,149
net is using feature called measurable

00:17:11,449 --> 00:17:16,159
batteries so the way it works is that

00:17:14,149 --> 00:17:22,220
the device writes a number called

00:17:16,159 --> 00:17:25,069
numbers in a field and this value will

00:17:22,220 --> 00:17:27,980
be bigger than one for a large packet

00:17:25,069 --> 00:17:30,080
now the guest has to read this value to

00:17:27,980 --> 00:17:35,299
figure out how many batteries does the

00:17:30,080 --> 00:17:37,720
packet contain and unfortunately this

00:17:35,299 --> 00:17:40,279
girls a cache miss overhead per packet

00:17:37,720 --> 00:17:42,110
it was reported yet just by simply

00:17:40,279 --> 00:17:44,960
commenting out the line that reads this

00:17:42,110 --> 00:17:48,320
value improves the throughput by about

00:17:44,960 --> 00:17:50,450
fifteen percent of course at the cost of

00:17:48,320 --> 00:17:53,239
breaking the merch evil buffers feature

00:17:50,450 --> 00:17:57,049
so you can no longer accept large packet

00:17:53,239 --> 00:18:00,799
so we do not want that what we can do

00:17:57,049 --> 00:18:04,129
instead is using the new layout since

00:18:00,799 --> 00:18:08,450
the host now labels descriptors as first

00:18:04,129 --> 00:18:11,690
middle and last in a badge we no longer

00:18:08,450 --> 00:18:13,820
need the number of buffers right to find

00:18:11,690 --> 00:18:17,090
out where does the packet end yet can

00:18:13,820 --> 00:18:18,950
simply scan scan the descriptors until

00:18:17,090 --> 00:18:23,090
it seals the last label in the

00:18:18,950 --> 00:18:27,440
descriptor so I expect to observe Rupert

00:18:23,090 --> 00:18:31,879
improvement once we test this with a new

00:18:27,440 --> 00:18:36,139
ring layout but it didn't yet that's one

00:18:31,879 --> 00:18:38,509
reason this might be a good idea partly

00:18:36,139 --> 00:18:40,940
protesting might become simpler with

00:18:38,509 --> 00:18:43,609
newly out sometimes the port request

00:18:40,940 --> 00:18:46,609
order doesn't matter for example that's

00:18:43,609 --> 00:18:48,859
the case when you are processing used

00:18:46,609 --> 00:18:52,639
transmit buffers in the networking

00:18:48,859 --> 00:18:54,200
device you just free the buffer you

00:18:52,639 --> 00:18:55,549
reuse a memory for something else you

00:18:54,200 --> 00:18:59,509
don't really care and which other this

00:18:55,549 --> 00:19:02,419
happens and so each descriptor is now

00:18:59,509 --> 00:19:04,249
completed independently so with new

00:19:02,419 --> 00:19:07,340
layout it might be possible to partition

00:19:04,249 --> 00:19:09,590
during such that multiple worker thread

00:19:07,340 --> 00:19:10,820
would each process a separate cache line

00:19:09,590 --> 00:19:13,190
they will not conflict with each other

00:19:10,820 --> 00:19:15,489
and maybe we'll get some performance out

00:19:13,190 --> 00:19:15,489
of this

00:19:20,210 --> 00:19:24,650
I Owen interrupts are expensive and so

00:19:23,180 --> 00:19:26,750
we're tiring actually it has two

00:19:24,650 --> 00:19:30,110
mechanisms for iron interrupt mitigation

00:19:26,750 --> 00:19:33,380
one does the event index and one is the

00:19:30,110 --> 00:19:35,120
flags I didn't really examine them for

00:19:33,380 --> 00:19:38,330
cash efficiency both seem to have

00:19:35,120 --> 00:19:41,360
advantages and disadvantages and this

00:19:38,330 --> 00:19:44,570
really has to be done but with one dot

00:19:41,360 --> 00:19:46,730
one compactly out that I outlined here

00:19:44,570 --> 00:19:49,250
there's another option we can use the

00:19:46,730 --> 00:19:51,650
first middle and last labels in the

00:19:49,250 --> 00:19:54,380
descriptor to deliver only one interrupt

00:19:51,650 --> 00:19:56,180
where batch so that's another mechanism

00:19:54,380 --> 00:19:58,070
for interrupt mitigation that might be

00:19:56,180 --> 00:20:01,160
helpful and of course it works for I

00:19:58,070 --> 00:20:03,050
mitigation as well this is possible if

00:20:01,160 --> 00:20:05,390
the requests are supplied in batches

00:20:03,050 --> 00:20:07,850
that is for example the case with modern

00:20:05,390 --> 00:20:10,910
Linux networking which has a black per

00:20:07,850 --> 00:20:13,610
packet that is transmitted at LZ okay

00:20:10,910 --> 00:20:18,590
this is last one so we could say okay

00:20:13,610 --> 00:20:21,140
let's set the last flag now again it's

00:20:18,590 --> 00:20:24,410
not implemented yet so but should help

00:20:21,140 --> 00:20:27,590
right so what else is left to benefit

00:20:24,410 --> 00:20:29,450
from this performance gains and first we

00:20:27,590 --> 00:20:31,610
need to determine whether making

00:20:29,450 --> 00:20:35,650
descriptive writer rules like this has

00:20:31,610 --> 00:20:38,120
any security implications for example I

00:20:35,650 --> 00:20:42,380
seem to remember that the shared memory

00:20:38,120 --> 00:20:45,560
device did want the invariant that EG

00:20:42,380 --> 00:20:54,590
script is a writable or readable by each

00:20:45,560 --> 00:20:56,420
side so okay that's worth considering we

00:20:54,590 --> 00:20:59,930
need to do testing and wasn't just Intel

00:20:56,420 --> 00:21:02,390
processors all different architectures

00:20:59,930 --> 00:21:03,650
really we have significantly differently

00:21:02,390 --> 00:21:07,340
research director okay ash and

00:21:03,650 --> 00:21:09,830
synchronization so the code is out there

00:21:07,340 --> 00:21:12,440
in the linux kernel so you can just run

00:21:09,830 --> 00:21:14,000
the benchmark and you might need to

00:21:12,440 --> 00:21:16,190
implement a couple of lines and assembly

00:21:14,000 --> 00:21:19,700
for for memory synchronization if I

00:21:16,190 --> 00:21:24,350
didn't do put all the virus in there yet

00:21:19,700 --> 00:21:26,450
but it should be easy so M really

00:21:24,350 --> 00:21:29,510
appreciate would appreciate help with

00:21:26,450 --> 00:21:32,210
that and of course yeah if it works well

00:21:29,510 --> 00:21:32,840
we will want to to implement it and

00:21:32,210 --> 00:21:39,760
that's

00:21:32,840 --> 00:21:41,929
a lot of work to do it on all levels um

00:21:39,760 --> 00:21:43,640
okay let's talk about something

00:21:41,929 --> 00:21:46,309
different this is a very interesting

00:21:43,640 --> 00:21:49,190
recent development is support for IMM

00:21:46,309 --> 00:21:51,860
use traditional if your table completely

00:21:49,190 --> 00:21:54,559
ignored the virtual item you even if

00:21:51,860 --> 00:21:57,580
just attempted to configure it and this

00:21:54,559 --> 00:21:59,960
was done for performance but it prevents

00:21:57,580 --> 00:22:03,380
application status' using userspace

00:21:59,960 --> 00:22:06,710
batteries base vfi you're unbeatable

00:22:03,380 --> 00:22:08,750
devices and in one that one there will

00:22:06,710 --> 00:22:12,200
be an option to fix it and actually obey

00:22:08,750 --> 00:22:14,210
the IMU rules lettuce little drivers

00:22:12,200 --> 00:22:16,580
already learned to handle this correctly

00:22:14,210 --> 00:22:18,860
this option correctly if you set this

00:22:16,580 --> 00:22:20,870
option then all the guests that also

00:22:18,860 --> 00:22:24,380
enable the virtual mmm you will just

00:22:20,870 --> 00:22:26,330
break and the kind of cannot and break

00:22:24,380 --> 00:22:29,480
them and be compatible without breaking

00:22:26,330 --> 00:22:32,659
security so I guess that's how it is but

00:22:29,480 --> 00:22:34,100
luckily this is off by default most

00:22:32,659 --> 00:22:36,830
guests just don't configure I'm mu by

00:22:34,100 --> 00:22:42,320
default so that's kind of okay from that

00:22:36,830 --> 00:22:44,029
point of view I'll skip discussion of

00:22:42,320 --> 00:22:47,059
the implementation of that because there

00:22:44,029 --> 00:22:49,549
is another talk that discusses this but

00:22:47,059 --> 00:22:51,799
besides guest user space drivers they

00:22:49,549 --> 00:22:54,919
are also likely to see more uses for

00:22:51,799 --> 00:22:56,750
virtual mm you in particular there's

00:22:54,919 --> 00:22:59,029
going to be talked about the V host pci

00:22:56,750 --> 00:23:02,000
proposal where one virtual machine can

00:22:59,029 --> 00:23:03,980
access the memory of another one and in

00:23:02,000 --> 00:23:07,460
this setup it might be a good idea to

00:23:03,980 --> 00:23:09,200
implement a V host immu such that you

00:23:07,460 --> 00:23:11,419
you'll somehow limit the damage that

00:23:09,200 --> 00:23:15,679
when a bag of is involved Rachel machine

00:23:11,419 --> 00:23:18,789
can cause another one so that's another

00:23:15,679 --> 00:23:21,409
interesting application for this feature

00:23:18,789 --> 00:23:24,620
here's some kind of crazy idea they just

00:23:21,409 --> 00:23:29,029
here not because every practical but

00:23:24,620 --> 00:23:32,000
more like to spur more innovation in the

00:23:29,029 --> 00:23:34,190
field of your tie oh so crazy ideas

00:23:32,000 --> 00:23:37,250
first we have the signal multi

00:23:34,190 --> 00:23:39,830
distribution device that allows you to

00:23:37,250 --> 00:23:41,750
send interrupts the CPUs how about we

00:23:39,830 --> 00:23:46,190
extend this to a full-featured built i

00:23:41,750 --> 00:23:46,559
own a peek would that be useful well

00:23:46,190 --> 00:23:48,120
made

00:23:46,559 --> 00:23:49,740
because at least some networking

00:23:48,120 --> 00:23:51,840
workloads actually spend about twenty

00:23:49,740 --> 00:23:55,799
percent of their time doing exits

00:23:51,840 --> 00:23:59,850
related to epic programming so that's

00:23:55,799 --> 00:24:02,340
one thought and another is that whenever

00:23:59,850 --> 00:24:04,169
a virtual CPU goes idle kvm already

00:24:02,340 --> 00:24:06,929
started doing a bunch of polling and it

00:24:04,169 --> 00:24:09,059
seems to improve performance so with the

00:24:06,929 --> 00:24:12,330
polling via Taio idle driver makes sense

00:24:09,059 --> 00:24:16,559
at some level to maybe not even exit and

00:24:12,330 --> 00:24:21,559
stay within the virtual machine I am the

00:24:16,559 --> 00:24:23,669
true and here's the last wild idea

00:24:21,559 --> 00:24:26,570
general guide technology it's an

00:24:23,669 --> 00:24:28,649
interesting software project that

00:24:26,570 --> 00:24:31,649
implemented the lightweight hypervisor

00:24:28,649 --> 00:24:34,350
with a bunch of hyper calls that led yet

00:24:31,649 --> 00:24:39,330
that allow guests to control the apt

00:24:34,350 --> 00:24:41,669
permissions and so this is useful for

00:24:39,330 --> 00:24:45,869
example if you want to protect your

00:24:41,669 --> 00:24:49,379
kernel text memory so it might be better

00:24:45,869 --> 00:24:53,519
to write protect it also in the apt and

00:24:49,379 --> 00:24:55,289
not just in the guest page tables and as

00:24:53,519 --> 00:24:57,840
we are extending the balloon device with

00:24:55,289 --> 00:25:04,409
guest Paige Kane's right maybe this can

00:24:57,840 --> 00:25:07,830
provide the similar functionality so

00:25:04,409 --> 00:25:09,960
this were ideas related to new

00:25:07,830 --> 00:25:12,059
interfaces between host and guest that

00:25:09,960 --> 00:25:14,340
we might want to introduce the durable

00:25:12,059 --> 00:25:18,360
as a bunch of work that you can do just

00:25:14,340 --> 00:25:21,659
work in an implementation so first of

00:25:18,360 --> 00:25:23,789
all Linux always uses indirect

00:25:21,659 --> 00:25:26,190
descriptors to pack maximum number of

00:25:23,789 --> 00:25:29,100
descriptors per ring and maybe we should

00:25:26,190 --> 00:25:32,899
go easy there and reduce indirection

00:25:29,100 --> 00:25:38,369
levels when the ring is mostly empty

00:25:32,899 --> 00:25:42,509
polling and the horse really uses a lot

00:25:38,369 --> 00:25:44,009
of CPU at the moment and so maybe we can

00:25:42,509 --> 00:25:45,690
get some hints from the scheduler and

00:25:44,009 --> 00:25:48,590
improve performance when the host is

00:25:45,690 --> 00:25:52,679
busy and not sure how to do this exactly

00:25:48,590 --> 00:25:54,960
but it might be a good idea and you need

00:25:52,679 --> 00:25:57,809
to improve our error recovery there are

00:25:54,960 --> 00:25:59,669
two ways to recover from errors if the

00:25:57,809 --> 00:26:00,360
arrow triggers on the host we might be

00:25:59,669 --> 00:26:02,520
able to

00:26:00,360 --> 00:26:05,340
recover transparently just restart the

00:26:02,520 --> 00:26:08,010
back end in some way if there are three

00:26:05,340 --> 00:26:11,220
girls on the guests like a guest bug

00:26:08,010 --> 00:26:13,910
then we probably need guests to notice

00:26:11,220 --> 00:26:18,200
that and do a reset from the outside

00:26:13,910 --> 00:26:21,390
let's just mention quickly how would one

00:26:18,200 --> 00:26:23,040
contribute to your tail so if you're

00:26:21,390 --> 00:26:25,679
just changing patches they're a bunch of

00:26:23,040 --> 00:26:29,510
mailing list it might be confusing just

00:26:25,679 --> 00:26:32,130
copying them all if we are not sure and

00:26:29,510 --> 00:26:34,650
if you are implementing new interfaces

00:26:32,130 --> 00:26:38,549
you want to copy built our dev mailing

00:26:34,650 --> 00:26:40,679
list and no ages and if you want to make

00:26:38,549 --> 00:26:42,690
aspect change notice the bargain spec I

00:26:40,679 --> 00:26:46,400
want to extend it you want to send the

00:26:42,690 --> 00:26:49,110
comment to your tyo comment mailing list

00:26:46,400 --> 00:26:52,110
eventually one of the members of your

00:26:49,110 --> 00:26:54,120
tail will open in a tracking issue in

00:26:52,110 --> 00:26:56,250
our issue tracker and then we'll resolve

00:26:54,120 --> 00:27:00,030
it and put a change in the next revision

00:26:56,250 --> 00:27:03,120
of the spec so just lets me move some

00:27:00,030 --> 00:27:06,809
space for questions it's going to be a

00:27:03,120 --> 00:27:09,840
big release ton of new features and so

00:27:06,809 --> 00:27:12,750
join the fun we really be community lots

00:27:09,840 --> 00:27:16,280
of active contributors just on the spec

00:27:12,750 --> 00:27:18,630
we have nine active contributors and

00:27:16,280 --> 00:27:23,700
another implementation we have tens of

00:27:18,630 --> 00:27:26,510
people contributing patches and that's

00:27:23,700 --> 00:27:26,510
all questions

00:27:28,850 --> 00:27:35,540
okay thanks a lot oh sorry question oh

00:27:32,420 --> 00:27:39,440
yes I'm sorry I hope I assume about

00:27:35,540 --> 00:27:42,950
verte I'll 1.1 right so in the new

00:27:39,440 --> 00:27:47,660
layout there's basically a bit which

00:27:42,950 --> 00:27:51,830
says is this buffer used right you can

00:27:47,660 --> 00:27:56,450
go back to the let's find that one yep

00:27:51,830 --> 00:27:59,420
yep can you go actually this one yeah

00:27:56,450 --> 00:28:01,880
this one so what if what if the what if

00:27:59,420 --> 00:28:04,970
buffers are completed out of order

00:28:01,880 --> 00:28:07,430
alright so if I try to draw this here

00:28:04,970 --> 00:28:10,070
maybe I just be added drawing things so

00:28:07,430 --> 00:28:11,600
does it mean that in the worst case you

00:28:10,070 --> 00:28:13,820
will have to scan the bitmap basically

00:28:11,600 --> 00:28:16,640
like you you wouldn't be able to to tell

00:28:13,820 --> 00:28:19,940
what happened in constant time is it

00:28:16,640 --> 00:28:22,190
true no so what happens right you take a

00:28:19,940 --> 00:28:23,810
look here right you have a buffer we

00:28:22,190 --> 00:28:27,380
have a bunch of buffers here right and

00:28:23,810 --> 00:28:31,070
week the host decided to complete the

00:28:27,380 --> 00:28:34,490
last one first so we just takes this

00:28:31,070 --> 00:28:37,280
descriptor and writes it out first so

00:28:34,490 --> 00:28:39,440
you you when the guest is now looking

00:28:37,280 --> 00:28:41,990
for new entries to store a new stuff in

00:28:39,440 --> 00:28:44,120
there it will just look at the at the

00:28:41,990 --> 00:28:46,850
start of the ring and say okay so here's

00:28:44,120 --> 00:28:48,830
one first this is the idea if I need to

00:28:46,850 --> 00:28:50,660
do some cleanup I can do it based on the

00:28:48,830 --> 00:28:52,930
ID and now I can proceed to the next

00:28:50,660 --> 00:28:56,000
ring so you never scan the whole ring oh

00:28:52,930 --> 00:28:57,380
it copies it copies basically the whole

00:28:56,000 --> 00:28:59,330
thing to the next slot it doesn't just

00:28:57,380 --> 00:29:00,920
clear the bit or should the bid right so

00:28:59,330 --> 00:29:03,140
if you do out of order you have to copy

00:29:00,920 --> 00:29:04,490
the whole thing if you do in order you

00:29:03,140 --> 00:29:11,870
can just look into it okay cool yeah

00:29:04,490 --> 00:29:13,880
thank you just curious your uh how you

00:29:11,870 --> 00:29:17,360
measured twenty percent a pic exits and

00:29:13,880 --> 00:29:18,650
which which fields that was so yeah so

00:29:17,360 --> 00:29:19,880
what were you what was your workload

00:29:18,650 --> 00:29:21,920
when you were saying twenty percent of

00:29:19,880 --> 00:29:24,230
the couturier pics in which which fields

00:29:21,920 --> 00:29:26,450
and they think were being touched um I'm

00:29:24,230 --> 00:29:33,260
sorry that's just hearsay Oh yours is

00:29:26,450 --> 00:29:35,810
the best yeah thanks so with any fee and

00:29:33,260 --> 00:29:39,140
hpc coming becoming more of a reality

00:29:35,810 --> 00:29:41,150
there's a high need for more frames per

00:29:39,140 --> 00:29:42,440
second higher frames per second and it

00:29:41,150 --> 00:29:44,690
looks like the changes made

00:29:42,440 --> 00:29:46,460
for the one that one to have a collected

00:29:44,690 --> 00:29:49,820
fewer reference in cash should also help

00:29:46,460 --> 00:29:52,159
few references for pc i do you think

00:29:49,820 --> 00:29:53,990
it's feasible to have a bird I'll one

00:29:52,159 --> 00:29:56,090
that one device terminated as a hardware

00:29:53,990 --> 00:29:58,460
device instead of in a software on the

00:29:56,090 --> 00:30:02,330
course so I'm kind of hopeful yes

00:29:58,460 --> 00:30:04,399
because for some reason like many people

00:30:02,330 --> 00:30:06,139
discuss doing hardware implementations

00:30:04,399 --> 00:30:08,600
of your thyroid never surfaced and I

00:30:06,139 --> 00:30:10,299
kind of suspect that our layout is just

00:30:08,600 --> 00:30:13,700
too complex and expensive to implement

00:30:10,299 --> 00:30:16,039
so maybe we do a simpler one and it will

00:30:13,700 --> 00:30:18,500
happen but are there any hardware

00:30:16,039 --> 00:30:22,389
contributors lighting opinions on this

00:30:18,500 --> 00:30:25,309
back or you know so well I mean we have

00:30:22,389 --> 00:30:28,759
on the TC we have people who do hardware

00:30:25,309 --> 00:30:33,710
but no one spoke to me about doing this

00:30:28,759 --> 00:30:36,950
so I don't know okay thank you I was

00:30:33,710 --> 00:30:39,409
curious if the epic exits were from the

00:30:36,950 --> 00:30:42,200
message passing benchmarks that I

00:30:39,409 --> 00:30:46,340
published last year let me chase it was

00:30:42,200 --> 00:30:50,419
the apec timer and register that that

00:30:46,340 --> 00:30:53,240
took a lot of the cpu time is that where

00:30:50,419 --> 00:30:55,009
the hearsay was from oh well how do we

00:30:53,240 --> 00:30:58,340
measure these numbers is that the

00:30:55,009 --> 00:31:01,340
question my yeah oh so you can it's a

00:30:58,340 --> 00:31:03,980
benchmark that I put on linux kernel you

00:31:01,340 --> 00:31:06,889
can find another tool slash your tires /

00:31:03,980 --> 00:31:08,720
rims bench it's not actually running

00:31:06,889 --> 00:31:13,370
within the vm it's just two threads that

00:31:08,720 --> 00:31:15,169
communicate using verte on in there is

00:31:13,370 --> 00:31:19,279
an attempt to emulate exits by just

00:31:15,169 --> 00:31:23,179
doing lots of useless stuff okay thanks

00:31:19,279 --> 00:31:26,600
and the other thing was about having a V

00:31:23,179 --> 00:31:29,659
host idle for polling in the guest I

00:31:26,600 --> 00:31:32,419
think you you don't win a lot by moving

00:31:29,659 --> 00:31:36,019
the polling from kvm into the guest you

00:31:32,419 --> 00:31:38,690
save about one micro second so depending

00:31:36,019 --> 00:31:40,129
on you know what your latency goals are

00:31:38,690 --> 00:31:44,240
that's about how much you're going to

00:31:40,129 --> 00:31:47,210
save I think ideally actually we don't

00:31:44,240 --> 00:31:48,769
push halt pulling up into the guest we

00:31:47,210 --> 00:31:53,799
push it down into the scheduler and

00:31:48,769 --> 00:31:57,580
actually take it out of kvm entirely but

00:31:53,799 --> 00:32:02,190
depends on what your goals are yep

00:31:57,580 --> 00:32:02,190
that's my cancer on some level I agree

00:32:03,389 --> 00:32:08,220

YouTube URL: https://www.youtube.com/watch?v=5QIE0F7nU3U


