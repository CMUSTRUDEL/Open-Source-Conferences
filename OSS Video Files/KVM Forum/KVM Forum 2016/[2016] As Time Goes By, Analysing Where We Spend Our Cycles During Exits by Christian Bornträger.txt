Title: [2016] As Time Goes By, Analysing Where We Spend Our Cycles During Exits by Christian BorntrÃ¤ger
Publication date: 2016-09-08
Playlist: KVM Forum 2016
Description: 
	KVM exits spend several hundreds or thousands of cycles running hypervisor code. This talk will should where we spend our time, how to analyze that, what has been done and what can still be done to improve things.

Christian BorntrÃ¤ger
Maintainer KVM on z (s390), IBM Germany

Christian Borntraeger is a software engineer at IBM Germany and is the s390 maintainer for KVM and QEMU/KVM

Slides: http://www.linux-kvm.org/images/0/03/03x09A-Christian_Borntrager-As_Time_Goes_By_Analysing_Where_We_Spend_our_Cycles_During_Exits.pdf
Captions: 
	00:00:09,650 --> 00:00:14,820
I'm welcome to representation I'm

00:00:12,030 --> 00:00:17,400
Christian ma I'm the maintainer 4s

00:00:14,820 --> 00:00:20,850
through 90 and katie m and K mu together

00:00:17,400 --> 00:00:22,680
with Cornelia maybe you have seen the

00:00:20,850 --> 00:00:25,529
extra 90 parts already by cornelia and

00:00:22,680 --> 00:00:28,769
david so I'm here to do something

00:00:25,529 --> 00:00:32,910
different non s390 title is as time goes

00:00:28,769 --> 00:00:35,880
by analyzing where we spend our cycles

00:00:32,910 --> 00:00:40,739
during exits let's talk about motivation

00:00:35,880 --> 00:00:44,370
first it was actually something that did

00:00:40,739 --> 00:00:47,039
not happen did your a particular

00:00:44,370 --> 00:00:49,469
performance issue or some problem that

00:00:47,039 --> 00:00:51,870
Al Capone's team found was more like a

00:00:49,469 --> 00:00:56,690
fun project something that I like to do

00:00:51,870 --> 00:01:00,170
and the declawed balance is actually

00:00:56,690 --> 00:01:07,080
quite good summary of why I did that and

00:01:00,170 --> 00:01:11,490
I I started very early basically three

00:01:07,080 --> 00:01:13,200
of four years ago and micro optimizing

00:01:11,490 --> 00:01:17,000
is certainly something that I did during

00:01:13,200 --> 00:01:19,590
that time and I'm going to give you some

00:01:17,000 --> 00:01:21,390
some parts of the things that I've

00:01:19,590 --> 00:01:24,390
worked on 30 minutes a certain amount

00:01:21,390 --> 00:01:27,330
enough to do all the same so I hope I

00:01:24,390 --> 00:01:31,610
chose the right parts things that are

00:01:27,330 --> 00:01:34,860
interesting for you when I did the

00:01:31,610 --> 00:01:36,659
presentation I also asked myself okay I

00:01:34,860 --> 00:01:40,140
think all this micro optimizations which

00:01:36,659 --> 00:01:43,189
can be seen in micro benchmarks but what

00:01:40,140 --> 00:01:47,060
does it actually give you in a real-life

00:01:43,189 --> 00:01:51,030
benchmark so I started with say you perv

00:01:47,060 --> 00:01:53,759
tcp/ip round-robin workload where you

00:01:51,030 --> 00:01:57,540
send one bite into one direction and

00:01:53,759 --> 00:01:59,700
receive another bite back and you

00:01:57,540 --> 00:02:04,619
basically have two guests running in on

00:01:59,700 --> 00:02:07,710
the same host and then I decided to okay

00:02:04,619 --> 00:02:09,800
let's add and delay after each guest

00:02:07,710 --> 00:02:12,360
exit so whenever we exit the guest I

00:02:09,800 --> 00:02:18,090
added some n delay with the configurable

00:02:12,360 --> 00:02:20,610
delay and I looked what the what kind of

00:02:18,090 --> 00:02:25,710
performance you you get for this

00:02:20,610 --> 00:02:30,210
specific you perv benchmark and the

00:02:25,710 --> 00:02:33,450
interesting thing is that if your

00:02:30,210 --> 00:02:36,240
hardware is really slow let's say you

00:02:33,450 --> 00:02:38,370
your hardware takes thirty thousand men

00:02:36,240 --> 00:02:40,050
0 seconds for an exit it's not even

00:02:38,370 --> 00:02:41,100
bothered to optimize other things

00:02:40,050 --> 00:02:47,700
because you're almost on the same

00:02:41,100 --> 00:02:49,680
performance level but it's what's also

00:02:47,700 --> 00:02:53,880
interesting when you zoom in that you

00:02:49,680 --> 00:02:56,820
actually can get small performance gain

00:02:53,880 --> 00:03:00,870
let's say one or two percent when you

00:02:56,820 --> 00:03:03,330
optimize your extra time by 100 or 200

00:03:00,870 --> 00:03:05,610
nanoseconds so get a slight performance

00:03:03,330 --> 00:03:08,090
benefit for that particular workload so

00:03:05,610 --> 00:03:11,370
this is not completely irrelevant and

00:03:08,090 --> 00:03:18,150
even small things might pile up in the

00:03:11,370 --> 00:03:21,269
end so what did i do basically um I

00:03:18,150 --> 00:03:25,799
wrote the kernel module 4 s through 90

00:03:21,269 --> 00:03:28,170
which basically ran instructions why I

00:03:25,799 --> 00:03:30,720
knew that they will cause an exit from

00:03:28,170 --> 00:03:33,440
the guest and then I measured the time

00:03:30,720 --> 00:03:37,560
before at the time after and I did

00:03:33,440 --> 00:03:39,989
several thousand iterations did the mass

00:03:37,560 --> 00:03:42,480
the average and so on and the

00:03:39,989 --> 00:03:44,070
interferometer time on my system back

00:03:42,480 --> 00:03:46,489
then was the conflict that I had back

00:03:44,070 --> 00:03:50,190
then on the old system to the head was

00:03:46,489 --> 00:03:54,209
actually much much higher but I would

00:03:50,190 --> 00:03:56,010
expected it to be knowing the the time

00:03:54,209 --> 00:04:00,120
that the hardware actually takes doing

00:03:56,010 --> 00:04:02,549
guest accent so um but simply quickly

00:04:00,120 --> 00:04:05,640
quickly looking at some kind of samples

00:04:02,549 --> 00:04:09,360
just doing perf I was able to identify a

00:04:05,640 --> 00:04:13,130
lot of things very easily and brought it

00:04:09,360 --> 00:04:13,130
down to five hundred nanoseconds and

00:04:13,459 --> 00:04:21,060
that is actually something that you can

00:04:15,780 --> 00:04:23,700
learn doing analyzes and always look for

00:04:21,060 --> 00:04:25,670
for the low-hanging fruits for new

00:04:23,700 --> 00:04:27,650
architectures new chips Mike

00:04:25,670 --> 00:04:31,310
give you actually a decent performance

00:04:27,650 --> 00:04:33,800
win pretty quickly and then over time we

00:04:31,310 --> 00:04:36,440
have to find you have to fight against

00:04:33,800 --> 00:04:38,840
new code and introduces with regressions

00:04:36,440 --> 00:04:41,210
makes things more expensive and you

00:04:38,840 --> 00:04:43,760
actually find out that old code that has

00:04:41,210 --> 00:04:46,520
been there forever is actually much more

00:04:43,760 --> 00:04:49,010
expensive than you have sort of now

00:04:46,520 --> 00:04:51,200
today I'm on 30 and 90 seconds on my

00:04:49,010 --> 00:04:54,200
test system with my private Colonel

00:04:51,200 --> 00:04:57,620
config so there's totally uncontrolled

00:04:54,200 --> 00:05:00,470
measurement any colonel contact might

00:04:57,620 --> 00:05:03,590
change that number but really heavily so

00:05:00,470 --> 00:05:06,500
there's no no value in comparing that to

00:05:03,590 --> 00:05:12,740
to whatever I have also some numbers for

00:05:06,500 --> 00:05:14,810
x86 on the next slides okay interesting

00:05:12,740 --> 00:05:17,420
thing is it is still much much higher

00:05:14,810 --> 00:05:21,200
than the hardware overhead that I know

00:05:17,420 --> 00:05:24,590
of so um what else can you do to measure

00:05:21,200 --> 00:05:26,960
um some of you probably know the KD and

00:05:24,590 --> 00:05:31,130
unit test which is available for most

00:05:26,960 --> 00:05:33,560
platforms except s390 and you can

00:05:31,130 --> 00:05:37,850
actually measure the time that typical

00:05:33,560 --> 00:05:39,890
exit take in that case it gives you the

00:05:37,850 --> 00:05:42,680
amount of cycles not the amount of

00:05:39,890 --> 00:05:46,520
nanoseconds and there are basically

00:05:42,680 --> 00:05:49,360
three classes of of things instructions

00:05:46,520 --> 00:05:52,280
that you can see there are instructions

00:05:49,360 --> 00:05:54,590
that are implemented indicator modules

00:05:52,280 --> 00:05:56,000
are handled in the host Colonel there

00:05:54,590 --> 00:05:57,980
are instructions which are handled by

00:05:56,000 --> 00:06:00,350
hardware and there are instructions

00:05:57,980 --> 00:06:03,020
which are handled back here on you these

00:06:00,350 --> 00:06:06,710
are these three classes and what you

00:06:03,020 --> 00:06:08,420
actually want is to have most modes of

00:06:06,710 --> 00:06:09,980
the instructions done by the hardware

00:06:08,420 --> 00:06:12,710
look good that would be the perfect

00:06:09,980 --> 00:06:16,520
solution of course but that's not always

00:06:12,710 --> 00:06:22,130
possible then the next step was to

00:06:16,520 --> 00:06:26,060
actually find out why take a specific

00:06:22,130 --> 00:06:27,620
exit so long so first idea was arm may

00:06:26,060 --> 00:06:30,560
ask estrace and you know you have all

00:06:27,620 --> 00:06:33,980
these kind of cool measurements so let's

00:06:30,560 --> 00:06:36,290
let's use that when I look at the

00:06:33,980 --> 00:06:38,880
function crater and the resolution of

00:06:36,290 --> 00:06:40,680
time of microseconds

00:06:38,880 --> 00:06:44,280
and I talked about some hundred

00:06:40,680 --> 00:06:46,170
nanoseconds though was not the ideal

00:06:44,280 --> 00:06:48,960
solution for Marcus but there's a

00:06:46,170 --> 00:06:51,180
function graph tracer which actually

00:06:48,960 --> 00:06:56,220
gives you nanoseconds for a specific

00:06:51,180 --> 00:06:59,310
function so I started that started my

00:06:56,220 --> 00:07:03,320
measurement and whoop I realized that

00:06:59,310 --> 00:07:07,260
just enabling the function crater um

00:07:03,320 --> 00:07:10,200
makes that particular user bench master

00:07:07,260 --> 00:07:12,360
I had 10 times slower even more than 10

00:07:10,200 --> 00:07:14,970
times for the function graphed razor so

00:07:12,360 --> 00:07:18,660
the half trace has a much higher

00:07:14,970 --> 00:07:20,520
overhead than what we achieve in KDM for

00:07:18,660 --> 00:07:24,000
gas exits which is actually quite

00:07:20,520 --> 00:07:27,330
impressive how how optimized a bit ly

00:07:24,000 --> 00:07:30,840
hardware is in recent generations and B

00:07:27,330 --> 00:07:35,120
how optimized acadia module is I saying

00:07:30,840 --> 00:07:38,430
this is was a big surprise for me how

00:07:35,120 --> 00:07:41,520
inefficient F trace is in contrast to

00:07:38,430 --> 00:07:44,070
that still f'ed race is useful for

00:07:41,520 --> 00:07:47,400
finding interesting places to look

00:07:44,070 --> 00:07:49,530
closer it has some other issues like not

00:07:47,400 --> 00:07:53,730
all assembler files are prepared to do

00:07:49,530 --> 00:07:56,010
that but a what I did in the end was

00:07:53,730 --> 00:07:59,370
actually using perf top and perf

00:07:56,010 --> 00:08:01,920
annotate and staring at disassembly so I

00:07:59,370 --> 00:08:04,920
then look closer at hot sample there are

00:08:01,920 --> 00:08:08,160
some instructions which were very hot in

00:08:04,920 --> 00:08:12,390
these samples and then I looked why um I

00:08:08,160 --> 00:08:15,570
also use handwritten hacks so that I can

00:08:12,390 --> 00:08:18,630
certain then some from code and I

00:08:15,570 --> 00:08:23,370
disabled that's a optimal code and we

00:08:18,630 --> 00:08:27,360
test it so what did I find during that

00:08:23,370 --> 00:08:29,670
the time one example is some early exits

00:08:27,360 --> 00:08:32,280
we have a lot of code that the runs

00:08:29,670 --> 00:08:33,930
during a KDM exit which has to run all

00:08:32,280 --> 00:08:37,349
the time you have to check is the timer

00:08:33,930 --> 00:08:39,870
pending you have to check is a wee cpu

00:08:37,349 --> 00:08:42,570
request pending and these kind of things

00:08:39,870 --> 00:08:45,990
is a signal appending and whenever you

00:08:42,570 --> 00:08:48,690
have a chance you should try to do have

00:08:45,990 --> 00:08:52,050
an early exit for example in that case

00:08:48,690 --> 00:08:52,710
the handle requests function only checks

00:08:52,050 --> 00:08:55,970
for

00:08:52,710 --> 00:08:58,770
I think four or five requests on s390

00:08:55,970 --> 00:09:01,080
but you have a test fit you have a clear

00:08:58,770 --> 00:09:03,180
bridge and you have memory barriers so

00:09:01,080 --> 00:09:06,900
on or the whole function which is

00:09:03,180 --> 00:09:11,820
executed basically on every cycle of kvm

00:09:06,900 --> 00:09:14,460
is quite expensive um and by just doing

00:09:11,820 --> 00:09:16,410
an early exit if no request is pending

00:09:14,460 --> 00:09:19,520
just don't bother about test fit and

00:09:16,410 --> 00:09:22,440
clear it you can actually speed up our

00:09:19,520 --> 00:09:25,290
exit reasonably well there was a

00:09:22,440 --> 00:09:27,450
follow-up idea by paola to pull this

00:09:25,290 --> 00:09:29,970
track into the KTM track request

00:09:27,450 --> 00:09:33,360
function itself so that these visas he

00:09:29,970 --> 00:09:36,120
can optimize first thing I forgot about

00:09:33,360 --> 00:09:39,540
that proposal so it dropped on the floor

00:09:36,120 --> 00:09:41,490
and um I was reminded about that when

00:09:39,540 --> 00:09:44,370
doing the presentation but then i looked

00:09:41,490 --> 00:09:48,530
at x86 and power and they already have a

00:09:44,370 --> 00:09:55,110
similar thing they actually do the same

00:09:48,530 --> 00:09:57,630
same kind of request early exit neglect

00:09:55,110 --> 00:10:00,960
nips they only have one request page

00:09:57,630 --> 00:10:03,720
which is probably not not versi effort

00:10:00,960 --> 00:10:05,160
there and arm sets one bit but i haven't

00:10:03,720 --> 00:10:11,670
have not found the code that actually

00:10:05,160 --> 00:10:15,330
checked that so when i find out that i

00:10:11,670 --> 00:10:18,810
can i can certainly do the the common

00:10:15,330 --> 00:10:21,630
code pet anyway another thing that i've

00:10:18,810 --> 00:10:24,180
found out during that time I a queue

00:10:21,630 --> 00:10:26,010
safe and I a queue restore it's much

00:10:24,180 --> 00:10:29,820
much more expensive than yeah I ask you

00:10:26,010 --> 00:10:33,750
safe and and I could just even enable in

00:10:29,820 --> 00:10:35,580
my case I have 5228 but on other

00:10:33,750 --> 00:10:41,910
Hardware levels I have even seen a

00:10:35,580 --> 00:10:44,460
factor of 20 so um whenever you know in

00:10:41,910 --> 00:10:48,060
which context you are don't use Iook you

00:10:44,460 --> 00:10:52,980
safe Uzziah q disabled and unable it is

00:10:48,060 --> 00:10:55,260
really faster on most CPUs and turns out

00:10:52,980 --> 00:10:57,990
that we we use Iook you disabling and

00:10:55,260 --> 00:11:02,190
labeling quite often them in kvm we used

00:10:57,990 --> 00:11:03,900
around guests enter and exit I'll see

00:11:02,190 --> 00:11:05,250
you code might use it for some specific

00:11:03,900 --> 00:11:08,250
things

00:11:05,250 --> 00:11:11,100
inside some exit handles I found I acute

00:11:08,250 --> 00:11:14,730
disabling and enabling in the scheduler

00:11:11,100 --> 00:11:16,770
code we have that in KDM we now disabled

00:11:14,730 --> 00:11:19,580
and enabled instead of doing a safe and

00:11:16,770 --> 00:11:22,640
restore I did some early patches for

00:11:19,580 --> 00:11:28,430
Ezra 90 mostly and paola basically

00:11:22,640 --> 00:11:28,430
extended that to two other platforms

00:11:29,120 --> 00:11:34,820
okay and then there have been more

00:11:31,950 --> 00:11:37,710
things in the extra 90 code there's a

00:11:34,820 --> 00:11:42,510
tracing feature for most essa nightly

00:11:37,710 --> 00:11:44,250
kernel code so i put then an early

00:11:42,510 --> 00:11:48,110
condition track into an head of fire

00:11:44,250 --> 00:11:50,850
which was then GCC could use to optimize

00:11:48,110 --> 00:11:53,960
very early instead of doing a function

00:11:50,850 --> 00:11:56,880
call then doing a check and returning

00:11:53,960 --> 00:11:59,250
the early exits as i did for the request

00:11:56,880 --> 00:12:02,010
handing I we did similar things for the

00:11:59,250 --> 00:12:05,180
interrupt handling then it turns out

00:12:02,010 --> 00:12:07,800
that Haley em built-in versus module is

00:12:05,180 --> 00:12:12,780
also gives you some nanoseconds of

00:12:07,800 --> 00:12:15,120
performance there are some kind of jumps

00:12:12,780 --> 00:12:17,460
between the the colonel text and the

00:12:15,120 --> 00:12:20,070
Akkadian module another saying is that

00:12:17,460 --> 00:12:22,320
the build and module can be backed by

00:12:20,070 --> 00:12:25,620
large pages because the dacono text

00:12:22,320 --> 00:12:27,300
segment and other small reason it was

00:12:25,620 --> 00:12:32,100
also interesting that this was actually

00:12:27,300 --> 00:12:34,820
measurable the difference and in the end

00:12:32,100 --> 00:12:38,460
I even optimize the iock you code in

00:12:34,820 --> 00:12:41,010
Esther 92 to use instructions which have

00:12:38,460 --> 00:12:44,600
fast passes which made everything faster

00:12:41,010 --> 00:12:47,550
so let's look on where do we stand today

00:12:44,600 --> 00:12:49,560
as I said upstream s90 kernel with my

00:12:47,550 --> 00:12:52,650
kernel corn think i have three nano

00:12:49,560 --> 00:12:54,870
seconds for a simple Hyper call so I

00:12:52,650 --> 00:12:57,630
didn't that measurement with one cpu in

00:12:54,870 --> 00:13:00,630
the house and the guests so i was able

00:12:57,630 --> 00:13:02,880
to remove code that is not strictly

00:13:00,630 --> 00:13:05,160
necessary in that case turns out that

00:13:02,880 --> 00:13:09,030
the biggest trunk is actually time

00:13:05,160 --> 00:13:11,070
accounting you probably know that we

00:13:09,030 --> 00:13:13,680
have the guest time accounting where we

00:13:11,070 --> 00:13:16,920
account if its user time system time and

00:13:13,680 --> 00:13:18,320
guess time and the architecture code is

00:13:16,920 --> 00:13:20,420
a common code

00:13:18,320 --> 00:13:23,750
is actually doing a lot of calculations

00:13:20,420 --> 00:13:30,130
and magic shifting around and this seems

00:13:23,750 --> 00:13:32,900
to be quite a big chunk of code then I

00:13:30,130 --> 00:13:34,730
remove the iock you disable and enable

00:13:32,900 --> 00:13:36,890
around the guest and the exit because I

00:13:34,730 --> 00:13:38,570
know it the worst case that can happen

00:13:36,890 --> 00:13:41,450
is that the numbers are wrong so was

00:13:38,570 --> 00:13:43,520
just interesting interested in how much

00:13:41,450 --> 00:13:46,580
time to be spent there and it gave me

00:13:43,520 --> 00:13:49,460
another 10 nanoseconds and I turn other

00:13:46,580 --> 00:13:52,760
things like removing locking praising

00:13:49,460 --> 00:13:55,670
its these things are really small no big

00:13:52,760 --> 00:13:58,430
win so then I decided to go for the big

00:13:55,670 --> 00:14:03,110
hammer so i added a shortcut in the c

00:13:58,430 --> 00:14:06,040
code if that specific exit comes just

00:14:03,110 --> 00:14:08,840
short circuit and go back and hooray

00:14:06,040 --> 00:14:11,540
another 40 nanoseconds so interrupt

00:14:08,840 --> 00:14:14,630
handling regressed handling all dropped

00:14:11,540 --> 00:14:18,770
and also the handles signal handling

00:14:14,630 --> 00:14:20,750
whatever gives you 40 nanoseconds it's

00:14:18,770 --> 00:14:23,990
still still much more than what the hard

00:14:20,750 --> 00:14:25,910
way actually does so and then went one

00:14:23,990 --> 00:14:29,090
step further do a shortcut in the

00:14:25,910 --> 00:14:32,810
low-level a central routine so I did

00:14:29,090 --> 00:14:34,970
check in there avoided all the address

00:14:32,810 --> 00:14:38,090
space we loading whatever and yeah

00:14:34,970 --> 00:14:39,860
another 25 nanoseconds it's still larger

00:14:38,090 --> 00:14:43,670
than the pure heart rare but we are

00:14:39,860 --> 00:14:51,770
pretty close now there are some things

00:14:43,670 --> 00:14:54,230
that actually explain why the

00:14:51,770 --> 00:14:55,940
measurement might take longer than what

00:14:54,230 --> 00:14:58,340
you see in the table of how long an

00:14:55,940 --> 00:15:01,850
instruction takes because you change

00:14:58,340 --> 00:15:03,950
context so maybe there are some restarts

00:15:01,850 --> 00:15:07,300
in the pipeline necessary or TLB or

00:15:03,950 --> 00:15:11,300
branch prediction has some some hash

00:15:07,300 --> 00:15:14,360
collisions or aliases whatever so um if

00:15:11,300 --> 00:15:17,240
a CPU has a guest exit it will not run

00:15:14,360 --> 00:15:20,810
with full pipeline power but it has some

00:15:17,240 --> 00:15:22,460
fun puddles and of course there are

00:15:20,810 --> 00:15:24,740
still some some code in the hypervisor

00:15:22,460 --> 00:15:27,730
that needs to run the assembler code was

00:15:24,740 --> 00:15:27,730
was not empty

00:15:28,000 --> 00:15:33,220
ok that was the colonel so basically we

00:15:31,300 --> 00:15:35,710
are actually pretty close to the optimal

00:15:33,220 --> 00:15:39,010
solution it's not an order of magnitude

00:15:35,710 --> 00:15:43,690
it's really pretty close next thing is

00:15:39,010 --> 00:15:45,040
um you on Broadway we have about

00:15:43,690 --> 00:15:47,980
additional element of three thousand

00:15:45,040 --> 00:15:51,070
cycles or or am I ivy bridge system here

00:15:47,980 --> 00:15:53,410
six thousand cycles and there are a lot

00:15:51,070 --> 00:15:56,020
of things which are known of course the

00:15:53,410 --> 00:15:59,070
overheads as we have seen just now then

00:15:56,020 --> 00:16:01,240
there is a signal mask handing in kvm

00:15:59,070 --> 00:16:03,010
random had some some interesting

00:16:01,240 --> 00:16:06,760
pictures that we might want to use for

00:16:03,010 --> 00:16:10,480
s390 to get rid of them then of course

00:16:06,760 --> 00:16:12,670
it's a system call return turn KTM run

00:16:10,480 --> 00:16:15,640
is a system call and I'll control and it

00:16:12,670 --> 00:16:17,470
will return to the DFC function there's

00:16:15,640 --> 00:16:21,640
some kind of plumbing code it will

00:16:17,470 --> 00:16:25,900
return to the KTM low-level routine the

00:16:21,640 --> 00:16:28,950
main loop then you do of course II the

00:16:25,900 --> 00:16:32,470
exit handling and go back to G lipsy

00:16:28,950 --> 00:16:36,940
enter the system call and be back in

00:16:32,470 --> 00:16:39,610
guess basically and of course we have

00:16:36,940 --> 00:16:43,000
even more one more context which guest

00:16:39,610 --> 00:16:46,540
host Colonel guest host user space so

00:16:43,000 --> 00:16:48,310
branch prediction TLB catches and of

00:16:46,540 --> 00:16:50,860
course have some things which are

00:16:48,310 --> 00:16:56,320
horribly expensive and co mo which I

00:16:50,860 --> 00:17:01,330
will tell about later first thing is can

00:16:56,320 --> 00:17:05,589
we actually stay in the kernel for most

00:17:01,330 --> 00:17:08,410
exits and it was basically the opposite

00:17:05,589 --> 00:17:12,310
of what Steven Rutherford did for is

00:17:08,410 --> 00:17:14,800
split I akhil work can we do more in the

00:17:12,310 --> 00:17:17,770
current and the question is there are

00:17:14,800 --> 00:17:23,160
some implementations already which which

00:17:17,770 --> 00:17:25,870
do that when you look at the KTM exit

00:17:23,160 --> 00:17:29,920
whether you are in gas context and

00:17:25,870 --> 00:17:31,420
someone you haven't guessed exit if k

00:17:29,920 --> 00:17:33,400
the M can handle that it will just

00:17:31,420 --> 00:17:36,490
re-enter the guest or handle requests

00:17:33,400 --> 00:17:38,800
and reenter if KTM cannot handle

00:17:36,490 --> 00:17:40,630
requests it will exit to the PM your

00:17:38,800 --> 00:17:43,720
thread handled

00:17:40,630 --> 00:17:47,470
go back to the KTM module and then

00:17:43,720 --> 00:17:50,010
re-enter the guest what is done for most

00:17:47,470 --> 00:17:56,620
I or devices is something called

00:17:50,010 --> 00:18:01,030
identity for i/o operations for example

00:17:56,620 --> 00:18:05,790
on the device or whatever you basically

00:18:01,030 --> 00:18:08,550
act with the guests then the kvm module

00:18:05,790 --> 00:18:10,660
not if I the file descriptor and

00:18:08,550 --> 00:18:12,880
re-enters immediately the guests and

00:18:10,660 --> 00:18:15,550
then the operation is handled a

00:18:12,880 --> 00:18:20,650
synchronously and that actually makes

00:18:15,550 --> 00:18:27,970
them the exit handling much more

00:18:20,650 --> 00:18:32,350
efficient when I used event of T versus

00:18:27,970 --> 00:18:34,030
non event of key on Esther 94 diagnosed

00:18:32,350 --> 00:18:37,600
50 knows which is the verdict a cake

00:18:34,030 --> 00:18:41,830
function basically I was able to reduce

00:18:37,600 --> 00:18:43,780
the guests exit time from from 1400 90

00:18:41,830 --> 00:18:45,640
seconds to four in 10 seconds and the

00:18:43,780 --> 00:18:48,850
good thing is I then I tried with

00:18:45,640 --> 00:18:50,650
hundred 200 and 500 disks that this

00:18:48,850 --> 00:18:53,680
actually seems to be quite constant

00:18:50,650 --> 00:18:56,470
though so event of T seems to be scale

00:18:53,680 --> 00:19:00,570
seems to scale very very well which is

00:18:56,470 --> 00:19:04,540
good so then I ask myself okay um that

00:19:00,570 --> 00:19:06,670
side is certainly efficient is that also

00:19:04,540 --> 00:19:10,120
true for the other side so the real

00:19:06,670 --> 00:19:14,520
performance and it turns out as long as

00:19:10,120 --> 00:19:17,380
the disks all have their own eros threat

00:19:14,520 --> 00:19:22,330
then it seems that the performance for a

00:19:17,380 --> 00:19:26,490
fo with some random read on on a new

00:19:22,330 --> 00:19:28,690
block device is basically also

00:19:26,490 --> 00:19:34,720
independent from the amount of disks

00:19:28,690 --> 00:19:39,460
which is good so what can we do when

00:19:34,720 --> 00:19:43,090
when we actually avoid most exits to QMI

00:19:39,460 --> 00:19:46,570
you and stay inside the kernel for the

00:19:43,090 --> 00:19:52,150
most important things there's one thing

00:19:46,570 --> 00:19:54,210
that can be done which is to defer

00:19:52,150 --> 00:19:57,190
certain things

00:19:54,210 --> 00:19:59,080
to a later point in time for example we

00:19:57,190 --> 00:20:02,740
know that the host Colonel does not use

00:19:59,080 --> 00:20:05,950
floating point registers so in KDM we

00:20:02,740 --> 00:20:08,350
can actually avoid storing and restoring

00:20:05,950 --> 00:20:11,860
the floating point registers at guest

00:20:08,350 --> 00:20:15,460
exit I'm and defer that to function V

00:20:11,860 --> 00:20:19,860
cpu load or vcpu put know which is

00:20:15,460 --> 00:20:38,410
called by the preemption modifiers or

00:20:19,860 --> 00:20:40,510
during normal operations so yes that's

00:20:38,410 --> 00:20:42,820
true that that's even more a room for

00:20:40,510 --> 00:20:45,130
improvement but still the low-level

00:20:42,820 --> 00:20:46,930
handling gets gets improved the comment

00:20:45,130 --> 00:20:51,100
was that the Prem community fire

00:20:46,930 --> 00:20:54,970
restores the basically the user context

00:20:51,100 --> 00:20:56,320
which might not be necessary but this is

00:20:54,970 --> 00:20:59,950
certainly something that we can do

00:20:56,320 --> 00:21:07,480
because we have more exits in the kernel

00:20:59,950 --> 00:21:10,540
now okay I said there are some horribly

00:21:07,480 --> 00:21:14,530
expensive things in q mu are there any

00:21:10,540 --> 00:21:19,300
examples this is something that Stephen

00:21:14,530 --> 00:21:22,870
rubber thought mentioned in his call he

00:21:19,300 --> 00:21:25,930
actually tried to get the register

00:21:22,870 --> 00:21:30,460
content or the quantity of registers in

00:21:25,930 --> 00:21:33,400
GM you so you usually do that with cpu

00:21:30,460 --> 00:21:35,830
synchronized state and that causing

00:21:33,400 --> 00:21:38,110
architecture function which then usually

00:21:35,830 --> 00:21:39,460
called bio controls to get the general

00:21:38,110 --> 00:21:42,130
purpose register floating point

00:21:39,460 --> 00:21:45,010
registers and whatnot and that also

00:21:42,130 --> 00:21:50,440
schedules put register before reentering

00:21:45,010 --> 00:21:52,630
kvm so what does it mean x86 you have

00:21:50,440 --> 00:21:55,090
basically to our controls / register

00:21:52,630 --> 00:21:59,710
classes that get and set for each exit

00:21:55,090 --> 00:22:01,360
and you have to know that the system

00:21:59,710 --> 00:22:04,270
call is actually a pretty expensive

00:22:01,360 --> 00:22:07,320
operation it's not that cheap it takes

00:22:04,270 --> 00:22:10,350
hundred or two hundred cycles

00:22:07,320 --> 00:22:16,830
if you have let's how many of these 16

00:22:10,350 --> 00:22:23,460
14 this sums up so I didn't experiment

00:22:16,830 --> 00:22:26,400
on road six year with the community has

00:22:23,460 --> 00:22:29,730
I use that pathetic exit which took nine

00:22:26,400 --> 00:22:32,190
thousands of cycles here and i added cpu

00:22:29,730 --> 00:22:36,800
synchronized state just here in the

00:22:32,190 --> 00:22:40,740
generic generic KTM CD exit function and

00:22:36,800 --> 00:22:44,000
tada instead of nine thousand cycles we

00:22:40,740 --> 00:22:48,870
have now sixty thousand cycles which is

00:22:44,000 --> 00:22:51,240
quite impressive good thing is CB o--

00:22:48,870 --> 00:22:58,560
simple estate is by far not the most

00:22:51,240 --> 00:23:00,840
expensive operating in qm you still we

00:22:58,560 --> 00:23:04,560
want to get rid of that in sm 90 because

00:23:00,840 --> 00:23:07,440
on s through 19 we need the register

00:23:04,560 --> 00:23:09,060
content for almost every exit for

00:23:07,440 --> 00:23:13,080
historic reasons we have basically only

00:23:09,060 --> 00:23:15,300
one exit type and a lot of parameters

00:23:13,080 --> 00:23:17,940
are in registers for specific

00:23:15,300 --> 00:23:20,730
instructions so it's fair to say that

00:23:17,940 --> 00:23:23,300
when you have an ex of the sm 90 you

00:23:20,730 --> 00:23:27,600
very likely call VPO synchronize state

00:23:23,300 --> 00:23:33,450
so what we did most basically suggested

00:23:27,600 --> 00:23:36,240
by Alex as well is to use a fair page

00:23:33,450 --> 00:23:39,290
between the colonel and qm you to

00:23:36,240 --> 00:23:43,230
synchronize the registers over there and

00:23:39,290 --> 00:23:45,810
so we have the simplex structure in the

00:23:43,230 --> 00:23:48,240
kvm run where you basically have all the

00:23:45,810 --> 00:23:50,640
guests registers written on it on a

00:23:48,240 --> 00:23:54,140
guest exit so q mu does not have to call

00:23:50,640 --> 00:23:57,570
any all control any more unnecessary 90

00:23:54,140 --> 00:24:00,720
this is good enough interesting thing is

00:23:57,570 --> 00:24:03,360
with all these optimizations um I can

00:24:00,720 --> 00:24:05,310
still see the put or get register

00:24:03,360 --> 00:24:09,180
function in the profile very low but I

00:24:05,310 --> 00:24:14,250
can see it and in that case we have 32

00:24:09,180 --> 00:24:16,140
vector registers and due to some use the

00:24:14,250 --> 00:24:18,720
architecture parts of the vector

00:24:16,140 --> 00:24:20,190
registers are merged we see floating

00:24:18,720 --> 00:24:25,169
point registers so we

00:24:20,190 --> 00:24:27,090
have to copy them in 64-bit runs and due

00:24:25,169 --> 00:24:29,370
to aliens rules and other things Jesus

00:24:27,090 --> 00:24:31,379
he creates a loop with stores and loads

00:24:29,370 --> 00:24:33,210
instead of one big mem copy and of

00:24:31,379 --> 00:24:35,490
course there are some cash effects we

00:24:33,210 --> 00:24:37,440
had the context which and so on so this

00:24:35,490 --> 00:24:38,909
is not totally Chiefs the long-term

00:24:37,440 --> 00:24:44,879
solution might be just to have access

00:24:38,909 --> 00:24:47,759
functions for registers as I said it's

00:24:44,879 --> 00:24:50,519
not in CPU simple united state is not

00:24:47,759 --> 00:24:53,220
the most expensive thing in qmu another

00:24:50,519 --> 00:24:56,370
thing which is really surprising is how

00:24:53,220 --> 00:25:00,389
long it takes to resolve an object with

00:24:56,370 --> 00:25:03,059
the object model so instead of adding

00:25:00,389 --> 00:25:08,850
CBO synchronized state I just resolved

00:25:03,059 --> 00:25:11,490
some some random value and I have

00:25:08,850 --> 00:25:15,149
basically an overhead of 110,000 cycles

00:25:11,490 --> 00:25:20,490
for just resolving one specific object

00:25:15,149 --> 00:25:23,639
wow I I have not yet time to find out

00:25:20,490 --> 00:25:26,850
what's going on it's it's working g lib

00:25:23,639 --> 00:25:30,299
it eater objects and wat enlist whatever

00:25:26,850 --> 00:25:32,190
so there's probably room for improvement

00:25:30,299 --> 00:25:40,950
in that area and there's something I'd

00:25:32,190 --> 00:25:44,429
want to look in after k TM forum another

00:25:40,950 --> 00:25:48,570
thing where k mu is expensive is the big

00:25:44,429 --> 00:25:52,200
EMU log until Q mu two to four basically

00:25:48,570 --> 00:25:56,309
everything was protected by a big loss

00:25:52,200 --> 00:26:01,710
so whenever you exited the guests from

00:25:56,309 --> 00:26:03,840
KTM you took the big d mu log and with

00:26:01,710 --> 00:26:09,600
two for some kind of push down effort

00:26:03,840 --> 00:26:12,500
started to hold that specific lock less

00:26:09,600 --> 00:26:19,320
often and i have here some an example on

00:26:12,500 --> 00:26:21,269
the blue line is quick to three on an an

00:26:19,320 --> 00:26:23,730
ID bridge with four course and hyper

00:26:21,269 --> 00:26:27,240
threading still ahead aight aight logic

00:26:23,730 --> 00:26:29,549
of course and i started the KTM unit

00:26:27,240 --> 00:26:32,250
tests with the SMP parameter so i had

00:26:29,549 --> 00:26:33,149
some one two three four five six to

00:26:32,250 --> 00:26:37,019
eight

00:26:33,149 --> 00:26:41,399
cpus and you can see that starting with

00:26:37,019 --> 00:26:43,619
4 CPU is basically the exits got slower

00:26:41,399 --> 00:26:50,190
and slower when you compare that with

00:26:43,619 --> 00:26:52,619
the EMU is 26 you see that this push

00:26:50,190 --> 00:26:55,429
down effort really gives you an

00:26:52,619 --> 00:26:59,580
advantage means this is actually

00:26:55,429 --> 00:27:01,919
probably worse than reality because I

00:26:59,580 --> 00:27:04,409
said it's a four core system was hyper

00:27:01,919 --> 00:27:08,639
threading so I would assume that these

00:27:04,409 --> 00:27:13,259
last four elements are actually faster

00:27:08,639 --> 00:27:18,179
than what is printed TN if you had real

00:27:13,259 --> 00:27:22,139
eight cores okay um what else can we do

00:27:18,179 --> 00:27:24,629
um best thing is to avoid excess that's

00:27:22,139 --> 00:27:26,399
what I've said on one of the first light

00:27:24,629 --> 00:27:29,909
so whenever there is a hardware feature

00:27:26,399 --> 00:27:32,759
to avoid exit try to use it or suggest

00:27:29,909 --> 00:27:35,729
hard two features to to to the hardware

00:27:32,759 --> 00:27:38,009
theme if you have the possibility or you

00:27:35,729 --> 00:27:40,799
might improve interfaces for example

00:27:38,009 --> 00:27:45,089
whether o has some kind of interrupt or

00:27:40,799 --> 00:27:46,679
notification avoidance when you're going

00:27:45,089 --> 00:27:49,830
to Colonel there is actually not that

00:27:46,679 --> 00:27:51,210
much room for improvement I will send a

00:27:49,830 --> 00:27:54,089
patch for the request handing to

00:27:51,210 --> 00:27:58,019
optimize that we can certainly optimize

00:27:54,089 --> 00:28:00,719
the signal mask handling brink emu we

00:27:58,019 --> 00:28:03,509
can actually do more we should look

00:28:00,719 --> 00:28:05,489
further for Big Ang lock push down areas

00:28:03,509 --> 00:28:10,649
we have to understand why the object

00:28:05,489 --> 00:28:14,219
model is so slow and we might want to

00:28:10,649 --> 00:28:17,940
optimize or avoid synchronized state and

00:28:14,219 --> 00:28:23,129
we might even extend the event of the

00:28:17,940 --> 00:28:28,349
two other device types coming to my last

00:28:23,129 --> 00:28:32,070
slide um this lab well this laptop here

00:28:28,349 --> 00:28:35,099
is actually much faster as soon as a

00:28:32,070 --> 00:28:39,210
plot in power in terms of cycles not in

00:28:35,099 --> 00:28:41,450
terms of time so it seems that the power

00:28:39,210 --> 00:28:43,919
saving mechanisms of the endless Edu's

00:28:41,450 --> 00:28:46,260
really do have some kind of magic things

00:28:43,919 --> 00:28:48,600
going on where they disable some

00:28:46,260 --> 00:28:51,360
optimizations of part of the trip and I

00:28:48,600 --> 00:28:54,150
found two performance box in the s90

00:28:51,360 --> 00:28:57,000
code in preparing these slides where I

00:28:54,150 --> 00:29:01,380
could actually optimize things and do

00:28:57,000 --> 00:29:04,050
things better so time is up you have

00:29:01,380 --> 00:29:13,620
still some minutes for questions thank

00:29:04,050 --> 00:29:16,920
you very much basic question is what was

00:29:13,620 --> 00:29:18,870
the clock speed of the machine um I had

00:29:16,920 --> 00:29:21,030
it's hard to tell the cause I have

00:29:18,870 --> 00:29:24,570
measurements from x86 and an estradiol

00:29:21,030 --> 00:29:27,180
for ins to 90 sorry I think about 25

00:29:24,570 --> 00:29:30,030
gigahertz ok so just that people can

00:29:27,180 --> 00:29:33,600
also relate the nano seconds yes the

00:29:30,030 --> 00:29:35,480
cycles but anyway for qm you were really

00:29:33,600 --> 00:29:38,880
testing the worst case of fault because

00:29:35,480 --> 00:29:41,280
you are giving no path so basically it's

00:29:38,880 --> 00:29:44,790
going through the whole object to see if

00:29:41,280 --> 00:29:46,230
it's ambiguous and if you have to of the

00:29:44,790 --> 00:29:48,600
sale of the type that is specified then

00:29:46,230 --> 00:29:50,280
it would fail so if you actually test

00:29:48,600 --> 00:29:52,080
the same thing like with a slash machine

00:29:50,280 --> 00:29:57,780
instead of just the empty string should

00:29:52,080 --> 00:30:01,200
be much faster and the other question I

00:29:57,780 --> 00:30:05,730
had can you go back I tell you when to

00:30:01,200 --> 00:30:08,340
stop go back Mac the one way with a lot

00:30:05,730 --> 00:30:17,190
of numbers like the nano seconds in

00:30:08,340 --> 00:30:19,460
honestly 90 yeah this one so the sse you

00:30:17,190 --> 00:30:23,220
locking actually would expect a lot more

00:30:19,460 --> 00:30:25,200
because it has four memory barriers so

00:30:23,220 --> 00:30:29,220
alma mater barriers actually very cheap

00:30:25,200 --> 00:30:31,290
honest 290 yes i mean we are it is a

00:30:29,220 --> 00:30:34,650
platform basically showing the full ones

00:30:31,290 --> 00:30:36,120
the same pnb we have no read and write

00:30:34,650 --> 00:30:39,780
memory bears and the full month is

00:30:36,120 --> 00:30:41,460
basically just fluffing the stalk you so

00:30:39,780 --> 00:30:44,790
how many cycles so roughly do you have

00:30:41,460 --> 00:30:47,670
an idea the instruction itself should be

00:30:44,790 --> 00:30:49,590
single cycle it has a dependency on the

00:30:47,670 --> 00:30:52,560
on the following instruction so it

00:30:49,590 --> 00:30:55,530
really depends on do you have lots of

00:30:52,560 --> 00:30:57,920
storage spending so it can be really

00:30:55,530 --> 00:31:02,040
cheap ok because unless on

00:30:57,920 --> 00:31:04,500
on x86 it's like 50 so we actually see a

00:31:02,040 --> 00:31:08,250
lot less are you locking Michael taking

00:31:04,500 --> 00:31:10,560
introduced the SNP embassy memory

00:31:08,250 --> 00:31:12,450
barrier before SS you unlock thats

00:31:10,560 --> 00:31:16,440
optimize the way because that actually

00:31:12,450 --> 00:31:19,380
said like 50 clock cycles and so this is

00:31:16,440 --> 00:31:22,350
when we have basically a slightly

00:31:19,380 --> 00:31:26,070
stronger memory model than actually 60

00:31:22,350 --> 00:31:28,260
so the thyroid lateness is pretty cheap

00:31:26,070 --> 00:31:36,630
yeah it seemed to be very little so

00:31:28,260 --> 00:31:38,190
that's why i was asking thank you sorry

00:31:36,630 --> 00:31:40,140
i missed your first light so i don't

00:31:38,190 --> 00:31:43,680
know if you cover this or not but one on

00:31:40,140 --> 00:31:45,660
x86 specifically did you have any

00:31:43,680 --> 00:31:48,510
different settings for your bios in

00:31:45,660 --> 00:31:52,080
terms of power profiling or pearcy

00:31:48,510 --> 00:31:56,070
states or things like that yes i have

00:31:52,080 --> 00:31:58,500
more power setting on when and the not

00:31:56,070 --> 00:32:02,190
about your left or in the i don't you

00:31:58,500 --> 00:32:03,390
mean as a vest nor yeah you know on the

00:32:02,190 --> 00:32:05,700
measurements you made specifically

00:32:03,390 --> 00:32:10,200
regard this was with swiss power on and

00:32:05,700 --> 00:32:13,560
and cpu scheduler basically it is a cpu

00:32:10,200 --> 00:32:14,910
frak on the fixed frequency right so you

00:32:13,560 --> 00:32:18,570
didn't have turbo me or anything like

00:32:14,910 --> 00:32:20,370
that going on no okay yeah it's

00:32:18,570 --> 00:32:21,540
something to keep in mind i guess when

00:32:20,370 --> 00:32:23,580
we do these measurements i've come

00:32:21,540 --> 00:32:25,440
across things like if he if you're

00:32:23,580 --> 00:32:27,870
measuring this axial agencies and you

00:32:25,440 --> 00:32:30,270
did that a lot you notice actually

00:32:27,870 --> 00:32:32,820
change the many on run across your type

00:32:30,270 --> 00:32:35,580
I actually try to keep it on on one

00:32:32,820 --> 00:32:38,370
clock speed to actually see differences

00:32:35,580 --> 00:32:39,930
between our that same things yeah that

00:32:38,370 --> 00:32:43,970
that's not what I'll do is disable sleep

00:32:39,930 --> 00:32:43,970
says everything yeah does it

00:32:47,110 --> 00:32:51,340

YouTube URL: https://www.youtube.com/watch?v=ZShoxF6BPpw


