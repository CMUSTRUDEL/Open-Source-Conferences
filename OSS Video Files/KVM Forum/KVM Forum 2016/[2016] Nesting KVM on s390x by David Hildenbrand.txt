Title: [2016] Nesting KVM on s390x by David Hildenbrand
Publication date: 2016-09-08
Playlist: KVM Forum 2016
Description: 
	The way virtualization is built into IBM z Systems hardware allows for a quite neat and clean implementation of nested virtualization. This session gives an introduction to nested virtualization on IBM z Systems, explaining where we stand, how it works and what is missing. Also included is a short discussion of security aspects. Be prepared for some memory management, interrupt handling and virtualization insights.

David Hildenbrand
Software engineer, IBM R&D Germany GmbH

David has been working as software developer at IBM on QEMU/KVM for Linux on z Systems for ~1.5 years. His projects include nested virtualization, hardware support for guest debugging, cpu models and architecture compliance.

Slides: http://www.linux-kvm.org/images/0/0a/03x08A-David_Hildebrand-Nesting_KVM_on_s390x.pdf
Captions: 
	00:00:10,010 --> 00:00:15,299
alright we're a little bit late so we

00:00:12,840 --> 00:00:18,029
better start right ahead my name is

00:00:15,299 --> 00:00:22,289
David Brent I'm a soft engineered IBM i

00:00:18,029 --> 00:00:25,560
work on kvm for IBM c-systems AKA s390x

00:00:22,289 --> 00:00:27,180
aka the good old mainframe and today i

00:00:25,560 --> 00:00:30,090
want to talk about an interesting topic

00:00:27,180 --> 00:00:33,300
called nested virtualization on IBM sea

00:00:30,090 --> 00:00:38,280
systems especially in the context of kvm

00:00:33,300 --> 00:00:41,309
of course so first of all don't be

00:00:38,280 --> 00:00:43,200
scared there is like a little bit

00:00:41,309 --> 00:00:46,129
complicated stuff in there especially

00:00:43,200 --> 00:00:48,329
when it comes to memory management I

00:00:46,129 --> 00:00:50,039
tend to understand it from time two

00:00:48,329 --> 00:00:52,050
times in case you have any questions

00:00:50,039 --> 00:01:00,809
just to feel free to speak up and I'll

00:00:52,050 --> 00:01:05,670
try my best to explain it beautiful more

00:01:00,809 --> 00:01:07,729
time for the trade nuts ok so I'm going

00:01:05,670 --> 00:01:10,200
to go briefly about a basic contact

00:01:07,729 --> 00:01:12,590
concept of nested virtualization and

00:01:10,200 --> 00:01:16,009
then we'll quickly move on to on what

00:01:12,590 --> 00:01:18,570
virtualization looks like on IBM systems

00:01:16,009 --> 00:01:21,240
then we're going to go into the real

00:01:18,570 --> 00:01:25,140
details of how we made it running on the

00:01:21,240 --> 00:01:27,750
IBM systems after then having you like

00:01:25,140 --> 00:01:29,520
completely confused we'll have a look at

00:01:27,750 --> 00:01:31,890
the current status some interesting

00:01:29,520 --> 00:01:34,650
performance numbers short discussion

00:01:31,890 --> 00:01:36,659
about security and in the end I'm going

00:01:34,650 --> 00:01:38,820
to just give a short summary and the

00:01:36,659 --> 00:01:42,930
outlook and maybe we'll have some time

00:01:38,820 --> 00:01:45,570
for questions so um what is nested

00:01:42,930 --> 00:01:47,520
virtualization the ideas actually quite

00:01:45,570 --> 00:01:49,229
simple are you have a virtual machine

00:01:47,520 --> 00:01:51,829
and you want to allow that virtual

00:01:49,229 --> 00:01:55,229
machine to run virtual machines it sells

00:01:51,829 --> 00:01:58,409
in context of K the end and would look

00:01:55,229 --> 00:02:00,450
something like that and you may ask

00:01:58,409 --> 00:02:02,520
yourself then I'm so what is this good

00:02:00,450 --> 00:02:04,590
for why would I won't even want to do

00:02:02,520 --> 00:02:07,439
with something like that I mean one use

00:02:04,590 --> 00:02:09,989
case of course is if you sell somebody a

00:02:07,439 --> 00:02:12,720
virtual machine and you want to allow

00:02:09,989 --> 00:02:13,650
that one to like resell virtual machines

00:02:12,720 --> 00:02:15,360
or to start

00:02:13,650 --> 00:02:17,760
virtual machines for testing purposes

00:02:15,360 --> 00:02:20,879
stuff like that then something like that

00:02:17,760 --> 00:02:23,670
could be useful but until now this is

00:02:20,879 --> 00:02:26,970
really mainly only used as a test and

00:02:23,670 --> 00:02:29,909
debugging mechanism because you have

00:02:26,970 --> 00:02:32,489
like a much more access to a hardware

00:02:29,909 --> 00:02:35,239
virtualization and can like get a more

00:02:32,489 --> 00:02:38,700
detailed look what is actually happening

00:02:35,239 --> 00:02:41,459
once you implement further hardware deep

00:02:38,700 --> 00:02:43,319
have a virtualization support you can

00:02:41,459 --> 00:02:46,590
actually simulate different hardware

00:02:43,319 --> 00:02:49,019
virtualization by Ryan sirs and use this

00:02:46,590 --> 00:02:53,549
as a perfect testing mechanism for like

00:02:49,019 --> 00:02:55,859
new kvm releases or stuff like that so I

00:02:53,549 --> 00:02:58,049
mean actually you just want to run

00:02:55,859 --> 00:02:59,519
virtual machines inside your virtual

00:02:58,049 --> 00:03:02,280
machine so what you could do is you

00:02:59,519 --> 00:03:04,709
could simply go like one of the PowerPC

00:03:02,280 --> 00:03:07,019
way and do a trap and emulate approach

00:03:04,709 --> 00:03:09,590
in your guest it will not perform well

00:03:07,019 --> 00:03:12,450
but like you wouldn't have to implement

00:03:09,590 --> 00:03:15,209
extra nested virtualization supporting

00:03:12,450 --> 00:03:17,220
kvm so what you can do instead is if

00:03:15,209 --> 00:03:19,410
your hardware doesn't like give you

00:03:17,220 --> 00:03:22,319
support for nested virtualization out of

00:03:19,410 --> 00:03:24,389
the box you can actually emulate

00:03:22,319 --> 00:03:27,000
hardware virtualization for your guest

00:03:24,389 --> 00:03:29,370
by reusing hardware virtualization

00:03:27,000 --> 00:03:33,030
yourself which makes that whole process

00:03:29,370 --> 00:03:35,849
quite fast and quite secure so um

00:03:33,030 --> 00:03:38,099
basically it bent looks something like

00:03:35,849 --> 00:03:43,109
that whenever your guest tries to

00:03:38,099 --> 00:03:45,449
execute a virtual guest itself you just

00:03:43,109 --> 00:03:47,909
take control and like emulate the whole

00:03:45,449 --> 00:03:50,010
process by doing some kind of shadow

00:03:47,909 --> 00:03:53,849
execution because you have to work on

00:03:50,010 --> 00:03:55,919
shadow data structures in that case the

00:03:53,849 --> 00:03:57,780
nice thing about that is assuming you

00:03:55,919 --> 00:04:00,840
have it properly running on one level

00:03:57,780 --> 00:04:03,180
it's simply cascade so you can like go

00:04:00,840 --> 00:04:05,519
one level further nested under nested

00:04:03,180 --> 00:04:09,389
and like even further we talk about that

00:04:05,519 --> 00:04:13,040
one later on but interestingly until now

00:04:09,389 --> 00:04:15,780
only x86 was able to like really

00:04:13,040 --> 00:04:18,090
simulate how to virtualization for its

00:04:15,780 --> 00:04:21,930
guests to provide nested virtualization

00:04:18,090 --> 00:04:24,529
support and now also s390x can do that

00:04:21,930 --> 00:04:24,529
with kvm

00:04:25,110 --> 00:04:32,800
so you might wonder why to the first

00:04:29,290 --> 00:04:35,320
lights a nesting nested virtualization

00:04:32,800 --> 00:04:37,120
well basically on I DMC systems we

00:04:35,320 --> 00:04:40,090
always have at least one level of

00:04:37,120 --> 00:04:42,490
virtualization which is provided by the

00:04:40,090 --> 00:04:45,190
prism the alpa hypervisor which just

00:04:42,490 --> 00:04:49,120
cuts like the hardware into logical

00:04:45,190 --> 00:04:52,030
slices this is really like a really fast

00:04:49,120 --> 00:04:54,250
hypervisor and on top of that we are

00:04:52,030 --> 00:04:58,230
able to offer example run under linux

00:04:54,250 --> 00:05:00,640
kvm or the classical hypervisor CDM

00:04:58,230 --> 00:05:02,470
which is then already like level 2

00:05:00,640 --> 00:05:04,810
already some kind of nested

00:05:02,470 --> 00:05:07,570
virtualization but of course what we

00:05:04,810 --> 00:05:10,420
want to do is do we get another level of

00:05:07,570 --> 00:05:14,080
virtualization for example or running

00:05:10,420 --> 00:05:16,480
kvm under kvm or kvm under CDN and then

00:05:14,080 --> 00:05:18,520
we're actually already in level 3 so

00:05:16,480 --> 00:05:20,200
that might might sound like it doesn't

00:05:18,520 --> 00:05:21,760
perform at all but we'll say that let us

00:05:20,200 --> 00:05:26,140
see later on that this is actually quite

00:05:21,760 --> 00:05:28,870
good in order to make use of hardware

00:05:26,140 --> 00:05:31,720
virtualization IBMC systems you have to

00:05:28,870 --> 00:05:34,240
execute a sigh instruction which simply

00:05:31,720 --> 00:05:37,780
says a start interpretive execution and

00:05:34,240 --> 00:05:41,050
then your logic your CPU actually

00:05:37,780 --> 00:05:43,600
execute the vcpu India in the back end

00:05:41,050 --> 00:05:49,540
so something like the DM entry then exit

00:05:43,600 --> 00:05:51,580
on x86 and like usually you want to like

00:05:49,540 --> 00:05:53,920
go further and improve hardware

00:05:51,580 --> 00:05:56,470
virtualization and this is represented

00:05:53,920 --> 00:05:58,840
as so-called side facilities that for

00:05:56,470 --> 00:06:00,790
example provide performance speed ups or

00:05:58,840 --> 00:06:03,850
additional features for your hopper

00:06:00,790 --> 00:06:06,070
virtualization and we'll see later on

00:06:03,850 --> 00:06:13,090
like how far we were able to even

00:06:06,070 --> 00:06:15,760
virtualize these facilities now as I

00:06:13,090 --> 00:06:18,820
said as soon as the CPU execute the ass

00:06:15,760 --> 00:06:21,670
I instruction it executes actually the

00:06:18,820 --> 00:06:24,970
virtual CPU and as soon as there is a

00:06:21,670 --> 00:06:27,370
host interrupt or intercept has to be

00:06:24,970 --> 00:06:29,410
processed so the CPU has to take control

00:06:27,370 --> 00:06:32,110
and like for example emulate an

00:06:29,410 --> 00:06:34,840
instruction we fall back into the CPU

00:06:32,110 --> 00:06:36,200
now of course we have to describe this

00:06:34,840 --> 00:06:38,360
whole guests

00:06:36,200 --> 00:06:40,100
aight somehow and on the one hand we

00:06:38,360 --> 00:06:42,710
have two so-called side control block

00:06:40,100 --> 00:06:45,650
which describes the state of the virtual

00:06:42,710 --> 00:06:47,990
CPU and on the other hand we have to so

00:06:45,650 --> 00:06:50,600
called qi map which is actually the name

00:06:47,990 --> 00:06:53,150
under linux the guest mapping which is

00:06:50,600 --> 00:06:55,610
just an address space that represents

00:06:53,150 --> 00:06:58,040
the guest physical memory so thus i

00:06:55,610 --> 00:07:01,610
instruction is executed in this address

00:06:58,040 --> 00:07:05,930
space given the side control block and

00:07:01,610 --> 00:07:07,490
then the vcp you can run now if we take

00:07:05,930 --> 00:07:10,700
a look at a memory management

00:07:07,490 --> 00:07:14,000
perspective on it all starts out by a q

00:07:10,700 --> 00:07:17,090
mu process which contains some memory

00:07:14,000 --> 00:07:19,550
slots so the guest physical memories

00:07:17,090 --> 00:07:22,100
actually contains some their industry mu

00:07:19,550 --> 00:07:25,820
process and what we have to do in order

00:07:22,100 --> 00:07:29,210
to make this i use it is we have to map

00:07:25,820 --> 00:07:32,270
it all onto the real addresses we want

00:07:29,210 --> 00:07:35,240
it to be so that would mean that for

00:07:32,270 --> 00:07:37,220
example address 0 for the guest has

00:07:35,240 --> 00:07:39,770
really to lie it address 0 and net

00:07:37,220 --> 00:07:42,740
something like in user space at like a

00:07:39,770 --> 00:07:45,980
really high address and we do that by

00:07:42,740 --> 00:07:48,260
sharing certain page tables which is

00:07:45,980 --> 00:07:51,140
quite nice because as soon as we for the

00:07:48,260 --> 00:07:52,640
page into a qmd process and it is mapped

00:07:51,140 --> 00:07:55,000
into our guest mapping it is

00:07:52,640 --> 00:07:57,500
automatically contained in our team app

00:07:55,000 --> 00:08:00,380
which doesn't like require a lot of

00:07:57,500 --> 00:08:02,540
effort if we take a look at the

00:08:00,380 --> 00:08:04,910
controller control blocks we have to

00:08:02,540 --> 00:08:07,550
side control blocks which describes the

00:08:04,910 --> 00:08:09,650
state of the virtual CPU and it contains

00:08:07,550 --> 00:08:12,710
some execution controls how we call it

00:08:09,650 --> 00:08:14,510
so like some mechanism to actually

00:08:12,710 --> 00:08:18,110
control the Bay hardware virtualization

00:08:14,510 --> 00:08:20,120
is done but in addition this control

00:08:18,110 --> 00:08:22,700
block references so called satellite

00:08:20,120 --> 00:08:24,830
control blocks that either contain for

00:08:22,700 --> 00:08:28,120
example additional registers for our

00:08:24,830 --> 00:08:30,980
virtual CPU further control blocks and

00:08:28,120 --> 00:08:33,800
the so-called system control area I'm

00:08:30,980 --> 00:08:37,340
not going to go to too much into detail

00:08:33,800 --> 00:08:40,790
here it is basically a mean to make the

00:08:37,340 --> 00:08:42,530
hardware aware which virtual CPUs belong

00:08:40,790 --> 00:08:44,990
together and then do some like

00:08:42,530 --> 00:08:46,700
additional performance improvements but

00:08:44,990 --> 00:08:49,010
the most important thing here is that in

00:08:46,700 --> 00:08:52,070
this system control Ariel I

00:08:49,010 --> 00:08:55,070
lock that can be used to coordinate

00:08:52,070 --> 00:08:57,620
certain changes done to address spaces

00:08:55,070 --> 00:09:00,020
between the hypervisor and the hardware

00:08:57,620 --> 00:09:07,660
virtualization and that one is important

00:09:00,020 --> 00:09:11,180
later on now this whole process of

00:09:07,660 --> 00:09:13,400
emulating hardware virtualization on for

00:09:11,180 --> 00:09:17,390
our guests so emulating the science

00:09:13,400 --> 00:09:20,510
traction is called a virtual sigh a ka v

00:09:17,390 --> 00:09:22,670
site and it basically works the way that

00:09:20,510 --> 00:09:25,310
we have two shadows certain control

00:09:22,670 --> 00:09:27,230
blocks so we have to copy them in

00:09:25,310 --> 00:09:29,870
addition we have to take care that we

00:09:27,230 --> 00:09:32,660
create a shadow at rest base so we have

00:09:29,870 --> 00:09:36,890
to like mimic whatever the guest cpu

00:09:32,660 --> 00:09:39,620
guess CPU wants us to do and basically

00:09:36,890 --> 00:09:41,270
are we start out by intercepting the

00:09:39,620 --> 00:09:43,940
science traction so the guest wants to

00:09:41,270 --> 00:09:46,280
execute hardware virtualization with

00:09:43,940 --> 00:09:48,920
shadow blocks we perform some black

00:09:46,280 --> 00:09:51,500
magic related to two guests address

00:09:48,920 --> 00:09:53,540
space to shadow address space then we

00:09:51,500 --> 00:09:56,650
can actually execute hardware

00:09:53,540 --> 00:10:00,740
virtualization decide on behalf of our

00:09:56,650 --> 00:10:03,380
virtual CPU of course we then have to

00:10:00,740 --> 00:10:05,600
take care of page faults we get because

00:10:03,380 --> 00:10:08,240
we are page voting on the shadow address

00:10:05,600 --> 00:10:11,300
space so we have to like do some black

00:10:08,240 --> 00:10:13,250
magic to resolve that and eventually we

00:10:11,300 --> 00:10:15,860
are able to rerun that hardware

00:10:13,250 --> 00:10:19,850
virtualization mode all we for example

00:10:15,860 --> 00:10:23,900
have to inject a page fault in to our

00:10:19,850 --> 00:10:27,860
guest CPU so it can we resolve that fold

00:10:23,900 --> 00:10:30,500
for its own guest so this is like the

00:10:27,860 --> 00:10:32,480
big picture and I see some confused

00:10:30,500 --> 00:10:38,900
faces that's why I'm going to go into

00:10:32,480 --> 00:10:41,150
more detail okay so first of all we want

00:10:38,900 --> 00:10:42,770
to know whenever our guest execute Stu

00:10:41,150 --> 00:10:44,990
sigh instruction that is done quite

00:10:42,770 --> 00:10:48,010
easily because it's already done we just

00:10:44,990 --> 00:10:51,980
have to plug in our ambulation code

00:10:48,010 --> 00:10:54,710
second step is um we have to copy this

00:10:51,980 --> 00:10:57,650
side control block out of guest memory

00:10:54,710 --> 00:11:00,450
into our host memory because we want to

00:10:57,650 --> 00:11:03,510
filter it so we really want to have

00:11:00,450 --> 00:11:05,970
control what our guest wants to do and

00:11:03,510 --> 00:11:07,920
we have to take care of all these

00:11:05,970 --> 00:11:10,230
satellite control blocks because these

00:11:07,920 --> 00:11:13,110
pointers are only valid in guest memory

00:11:10,230 --> 00:11:15,720
but not in our hypervisor memory and

00:11:13,110 --> 00:11:18,660
good news are that for most of these

00:11:15,720 --> 00:11:21,150
control blocks we can simply translate

00:11:18,660 --> 00:11:24,180
address pin it in host memory and like

00:11:21,150 --> 00:11:27,840
we rewire it up in our shadow control

00:11:24,180 --> 00:11:30,510
block for some of them unfortunately

00:11:27,840 --> 00:11:32,220
because there are 31 bit addresses we

00:11:30,510 --> 00:11:34,080
have to actually shadow them but these

00:11:32,220 --> 00:11:36,540
are only two and like we have some

00:11:34,080 --> 00:11:38,910
performance improvements done at that

00:11:36,540 --> 00:11:41,970
point to avoid avoided at most of the

00:11:38,910 --> 00:11:43,950
times but interestingly we have this

00:11:41,970 --> 00:11:46,680
system control area as I said previously

00:11:43,950 --> 00:11:50,400
that links all these sigh control blocks

00:11:46,680 --> 00:11:53,880
of one vm together and these pointers

00:11:50,400 --> 00:11:57,240
are not valid in our hypervisor context

00:11:53,880 --> 00:11:59,550
they are valid in guest storage so I'm

00:11:57,240 --> 00:12:01,860
what we want to do here is we want to

00:11:59,550 --> 00:12:05,880
just forward this whole block because

00:12:01,860 --> 00:12:08,880
shadowing a lot is really evil and like

00:12:05,880 --> 00:12:13,020
that could be hardly done so what we do

00:12:08,880 --> 00:12:14,910
here we simply forward the block but

00:12:13,020 --> 00:12:17,400
tell hardware virtualization to never

00:12:14,910 --> 00:12:19,230
ever access one of these linked site

00:12:17,400 --> 00:12:22,380
control blocks because the pointers are

00:12:19,230 --> 00:12:24,840
just broken so that is like the basic

00:12:22,380 --> 00:12:27,690
thing we we have to do to guarantee that

00:12:24,840 --> 00:12:31,620
we have a weathered control block for

00:12:27,690 --> 00:12:33,600
our net set okay the ambulance well

00:12:31,620 --> 00:12:37,010
actually at that point we are already

00:12:33,600 --> 00:12:41,270
able to execute Desai on behalf of our

00:12:37,010 --> 00:12:45,870
kvm gas to execute the nested kvm guess

00:12:41,270 --> 00:12:49,970
except one tiny little detail the shadow

00:12:45,870 --> 00:12:53,310
at rest phase which is like yet another

00:12:49,970 --> 00:12:57,830
another level of complexity let's put it

00:12:53,310 --> 00:13:01,410
that way um so it all starts out that

00:12:57,830 --> 00:13:04,200
when we start running our necessary vm

00:13:01,410 --> 00:13:07,320
guest we have a completely empty shadow

00:13:04,200 --> 00:13:10,590
address base but whenever we get a fault

00:13:07,320 --> 00:13:13,500
while executing are nested kvm gasps we

00:13:10,590 --> 00:13:14,370
try to resolve that fault by shadowing

00:13:13,500 --> 00:13:17,779
it

00:13:14,370 --> 00:13:21,750
our shadow address space and of course

00:13:17,779 --> 00:13:24,779
the only way to make that work is we

00:13:21,750 --> 00:13:28,080
have to manually walk to page tables our

00:13:24,779 --> 00:13:32,820
guest provided us by reading an guest

00:13:28,080 --> 00:13:34,950
memory which is like really ugly we then

00:13:32,820 --> 00:13:37,920
have to like create all these shadow

00:13:34,950 --> 00:13:40,380
hierarchies of page tables and

00:13:37,920 --> 00:13:43,050
eventually will arrive at the lowest

00:13:40,380 --> 00:13:44,940
level where we can actually plug in an

00:13:43,050 --> 00:13:50,100
hour shadow our address space of

00:13:44,940 --> 00:13:54,270
physical hosts hosts page and basically

00:13:50,100 --> 00:13:57,510
then we can rerun the site but of course

00:13:54,270 --> 00:14:01,290
that would be too simple because we have

00:13:57,510 --> 00:14:03,200
to watch out for changes done to the

00:14:01,290 --> 00:14:06,600
original page table we are shadowing

00:14:03,200 --> 00:14:08,850
because for example our kvm gas could

00:14:06,600 --> 00:14:12,750
just go ahead and change the page tables

00:14:08,850 --> 00:14:14,880
in its gmail and we have to like take

00:14:12,750 --> 00:14:18,240
control at that point and make sure that

00:14:14,880 --> 00:14:20,160
our shadow page tables don't contain

00:14:18,240 --> 00:14:23,730
these entries anymore or even the

00:14:20,160 --> 00:14:26,900
updated ones on the other hand for

00:14:23,730 --> 00:14:29,990
example a hypervisor so we could just

00:14:26,900 --> 00:14:33,540
take certain pages out of our original

00:14:29,990 --> 00:14:35,520
kvm gas and page them out but in the end

00:14:33,540 --> 00:14:37,770
as they are also contained in our shadow

00:14:35,520 --> 00:14:39,839
page table somehow we have to take

00:14:37,770 --> 00:14:42,150
control again and make sure that we

00:14:39,839 --> 00:14:44,820
really unsure do anything that could be

00:14:42,150 --> 00:14:47,610
affected this is actually quite

00:14:44,820 --> 00:14:51,750
complicated but we were able to realize

00:14:47,610 --> 00:14:55,980
it by protecting page table entries on

00:14:51,750 --> 00:14:59,640
our original gmaps doing some crazy

00:14:55,980 --> 00:15:03,330
unsure drilling action depending on what

00:14:59,640 --> 00:15:05,910
what is going on in our team app and in

00:15:03,330 --> 00:15:08,820
the end we can now guarantee that we

00:15:05,910 --> 00:15:11,550
only have valid entries in our arm

00:15:08,820 --> 00:15:13,620
shadow address face and whenever

00:15:11,550 --> 00:15:16,350
something is done by okay the end gets

00:15:13,620 --> 00:15:21,540
or by our hypervisor it simply gets

00:15:16,350 --> 00:15:23,810
unshared out now that slide is a little

00:15:21,540 --> 00:15:27,300
bit more easier to understand them

00:15:23,810 --> 00:15:30,690
basically after we have done this crazy

00:15:27,300 --> 00:15:32,940
the management of our shadow gmaps we

00:15:30,690 --> 00:15:35,070
can simply reacts acute how the actual

00:15:32,940 --> 00:15:38,550
hardware virtualization as long as

00:15:35,070 --> 00:15:41,339
possible at one point in time we might

00:15:38,550 --> 00:15:44,490
have to like propagate an intercept to

00:15:41,339 --> 00:15:47,190
our guest itself or while trying to

00:15:44,490 --> 00:15:50,100
resolve a fault on our shadows address

00:15:47,190 --> 00:15:53,100
space we might have to actually forward

00:15:50,100 --> 00:15:58,200
a page fault to our kvm guess so it can

00:15:53,100 --> 00:15:59,880
possess that before we then can leave

00:15:58,200 --> 00:16:02,459
this emulation of hardware

00:15:59,880 --> 00:16:04,200
virtualization we have to unsettle all

00:16:02,459 --> 00:16:06,990
the control blocks we shadowed

00:16:04,200 --> 00:16:09,149
previously and luckily we only have to

00:16:06,990 --> 00:16:11,640
care about the shadow control the side

00:16:09,149 --> 00:16:14,070
control block and we simply have to copy

00:16:11,640 --> 00:16:16,320
selected values back and then we are

00:16:14,070 --> 00:16:18,930
ready to go we can re-execute our real

00:16:16,320 --> 00:16:21,329
KDM gasps and it can then just like do

00:16:18,930 --> 00:16:26,519
whatever it wants to do our resolve page

00:16:21,329 --> 00:16:31,200
faults handle intercepts and so on now

00:16:26,519 --> 00:16:35,520
current state good news are we basically

00:16:31,200 --> 00:16:37,860
allow all a lower message KDM gets to

00:16:35,520 --> 00:16:40,470
have all features our ordinary Canadian

00:16:37,860 --> 00:16:43,290
guests have including vector riches to

00:16:40,470 --> 00:16:47,130
support transaction execution in huge

00:16:43,290 --> 00:16:51,300
pages with up to two gigabyte pages it

00:16:47,130 --> 00:16:54,480
can have up to actually 255 virtual CPUs

00:16:51,300 --> 00:16:57,959
right now with a tiny little patch that

00:16:54,480 --> 00:17:00,149
is still missing except of one

00:16:57,959 --> 00:17:02,870
facilities a one feature that is called

00:17:00,149 --> 00:17:06,089
a collaborative memory management and

00:17:02,870 --> 00:17:08,189
this is then the place where we can

00:17:06,089 --> 00:17:11,069
provide all hardware virtualization

00:17:08,189 --> 00:17:13,829
features for our nested KDM guess

00:17:11,069 --> 00:17:16,470
because it's simply too complicated and

00:17:13,829 --> 00:17:19,110
at that one point for example I'm just

00:17:16,470 --> 00:17:21,059
going to go quickly over that is I

00:17:19,110 --> 00:17:23,189
mentioned a system control area with

00:17:21,059 --> 00:17:25,620
these invalid pointers hanging around

00:17:23,189 --> 00:17:28,100
there we can't enables these and on the

00:17:25,620 --> 00:17:31,290
other hand our shadow address tables

00:17:28,100 --> 00:17:33,390
don't totally mimic the original page

00:17:31,290 --> 00:17:35,220
tables we are shadowing so we have some

00:17:33,390 --> 00:17:38,309
entries that might be empty although

00:17:35,220 --> 00:17:39,630
they are not so that like doesn't really

00:17:38,309 --> 00:17:43,230
allow us to

00:17:39,630 --> 00:17:45,690
all hardware virtualization features for

00:17:43,230 --> 00:17:47,340
now there are no known bugs out there in

00:17:45,690 --> 00:17:51,050
the implementation but of course a lot

00:17:47,340 --> 00:17:55,680
of testing is missing zor that statement

00:17:51,050 --> 00:17:57,870
it might be true might not be true so in

00:17:55,680 --> 00:18:00,270
the end I don't know of any bugs and I

00:17:57,870 --> 00:18:03,210
also don't know of any income abilities

00:18:00,270 --> 00:18:07,470
onto the lion aetna published

00:18:03,210 --> 00:18:10,340
architecture but there is a tiny little

00:18:07,470 --> 00:18:13,890
one but in general we took care that

00:18:10,340 --> 00:18:17,070
everything should be handled still until

00:18:13,890 --> 00:18:20,640
now you manually have to enable nested

00:18:17,070 --> 00:18:23,190
kvm support in the kvm module just like

00:18:20,640 --> 00:18:24,540
on x86 butters like a discussion going

00:18:23,190 --> 00:18:29,040
on right now whether they could

00:18:24,540 --> 00:18:31,410
eventually enable it as default and I

00:18:29,040 --> 00:18:33,480
think once they enabled it we can also

00:18:31,410 --> 00:18:38,820
enable it after like doing some more

00:18:33,480 --> 00:18:41,850
review s390x doesn't have cpu mod

00:18:38,820 --> 00:18:45,600
support yet but we need it in order to

00:18:41,850 --> 00:18:47,130
turn it on to like to show our okay the

00:18:45,600 --> 00:18:50,730
end guess that it has hardware

00:18:47,130 --> 00:18:53,910
virtualization we have a prototype we

00:18:50,730 --> 00:18:55,620
even have like various versions already

00:18:53,910 --> 00:18:58,530
out there on the GMU mailing list so

00:18:55,620 --> 00:19:00,120
we'll see that one soon and we'll see

00:18:58,530 --> 00:19:01,710
you mother support you can actually use

00:19:00,120 --> 00:19:04,380
it once they have turned it on in the

00:19:01,710 --> 00:19:07,260
kvm module now the really cool thing is

00:19:04,380 --> 00:19:09,420
that migration works out of the box so

00:19:07,260 --> 00:19:11,400
you can live my grade I am I was able

00:19:09,420 --> 00:19:15,480
for example to start two levels of

00:19:11,400 --> 00:19:17,610
nested KDM guests and then just save to

00:19:15,480 --> 00:19:20,400
it safe to disk into a managed safe of

00:19:17,610 --> 00:19:23,310
my hypervisor so my first KDM guests in

00:19:20,400 --> 00:19:26,010
the depth case and it basically works

00:19:23,310 --> 00:19:28,110
because all state of the nested KDM

00:19:26,010 --> 00:19:30,660
guest is completely contained in our

00:19:28,110 --> 00:19:33,210
cave end guess so by simply migrating

00:19:30,660 --> 00:19:35,490
the main memory of a of ok DM guess we

00:19:33,210 --> 00:19:37,770
implicitly migrate or completeness that

00:19:35,490 --> 00:19:39,660
kvm guests and all these shadows

00:19:37,770 --> 00:19:42,210
structure we are building up in our

00:19:39,660 --> 00:19:43,320
hypervisor they will simply be silently

00:19:42,210 --> 00:19:45,830
recreated on

00:19:43,320 --> 00:19:48,270
your target so that is pretty cool

00:19:45,830 --> 00:19:53,360
another cool thing when talking about

00:19:48,270 --> 00:19:56,700
security is that we don't emulate any

00:19:53,360 --> 00:19:58,650
instructions for our VSI so far Nestor

00:19:56,700 --> 00:20:02,250
que tengas that is all done by the

00:19:58,650 --> 00:20:04,800
hardware or by ordinary kvm code and we

00:20:02,250 --> 00:20:06,810
don't inject any interrupts into our

00:20:04,800 --> 00:20:08,520
vehicles again this is either done

00:20:06,810 --> 00:20:11,040
completely by our hardware

00:20:08,520 --> 00:20:14,120
virtualization or it is like done

00:20:11,040 --> 00:20:16,740
justice now by okay the anchors of

00:20:14,120 --> 00:20:18,690
course we take care that when we

00:20:16,740 --> 00:20:20,970
actually copy when we shadow this

00:20:18,690 --> 00:20:24,720
control block or all control blocks that

00:20:20,970 --> 00:20:27,690
we only allow selected values and as the

00:20:24,720 --> 00:20:30,600
shadow Chima the shadow address space is

00:20:27,690 --> 00:20:33,270
completely based on the original address

00:20:30,600 --> 00:20:36,090
base of okay the M goes there isn't

00:20:33,270 --> 00:20:38,010
really much they can break as long as we

00:20:36,090 --> 00:20:40,860
take care of probably flushing the t

00:20:38,010 --> 00:20:44,580
obese but as we all know flushing gob

00:20:40,860 --> 00:20:47,250
can be quite hard but we I mean we

00:20:44,580 --> 00:20:49,200
really debug debug that probably and so

00:20:47,250 --> 00:20:53,160
there might be something in there but

00:20:49,200 --> 00:20:58,340
it's pretty stable at that point now

00:20:53,160 --> 00:21:01,290
performance performance is interesting

00:20:58,340 --> 00:21:04,260
you can see like a big spike out there

00:21:01,290 --> 00:21:07,920
but that is actually a nested under

00:21:04,260 --> 00:21:11,430
nested so on what I did here I compile a

00:21:07,920 --> 00:21:14,670
kernel four times on the same machine

00:21:11,430 --> 00:21:17,040
one time on like l-pod the lowest level

00:21:14,670 --> 00:21:20,430
we can go under KTM guest under nested

00:21:17,040 --> 00:21:23,670
kvm and even a level further and what we

00:21:20,430 --> 00:21:26,490
can see is that first memory access is

00:21:23,670 --> 00:21:28,590
expensive but as soon as every memory of

00:21:26,490 --> 00:21:30,540
the guest has been touched there is

00:21:28,590 --> 00:21:33,720
barely overhead I mean even in run

00:21:30,540 --> 00:21:36,120
number three we were like faster in the

00:21:33,720 --> 00:21:40,320
next two kvm gas compared to okay ven

00:21:36,120 --> 00:21:42,330
gasps and of course this is like there

00:21:40,320 --> 00:21:44,280
is some Orion's because the hardware was

00:21:42,330 --> 00:21:47,700
not totally dedicated so these are not

00:21:44,280 --> 00:21:50,220
like absolute numbers here but in

00:21:47,700 --> 00:21:53,070
general like once we touch touched

00:21:50,220 --> 00:21:54,830
memory performance is pretty good so it

00:21:53,070 --> 00:21:58,559
was like roughly

00:21:54,830 --> 00:22:02,100
minus 22 plus five percent after it has

00:21:58,559 --> 00:22:05,100
been touched now the main problem is

00:22:02,100 --> 00:22:07,500
that we run when we run Nestor kvm under

00:22:05,100 --> 00:22:10,200
Nestor kayvyun is that we actually have

00:22:07,500 --> 00:22:13,830
to build shadow page tables on shadow

00:22:10,200 --> 00:22:16,590
page tables which results in constant

00:22:13,830 --> 00:22:20,970
and shadow shadow operations page faults

00:22:16,590 --> 00:22:24,929
going up and down in the stack so while

00:22:20,970 --> 00:22:26,940
that mate might not be like up the

00:22:24,929 --> 00:22:29,850
optimum right now on there is hardly

00:22:26,940 --> 00:22:31,650
some anything we can do about it and in

00:22:29,850 --> 00:22:33,960
the end we don't care because like most

00:22:31,650 --> 00:22:37,200
of the time we will only care about like

00:22:33,960 --> 00:22:39,809
guest level 3 and guest level four is

00:22:37,200 --> 00:22:42,210
just like a test case to see if nested

00:22:39,809 --> 00:22:45,690
virtualization Cascades if we did

00:22:42,210 --> 00:22:47,940
everything correct interestingly also i

00:22:45,690 --> 00:22:50,850
mean i was using multi para virtualized

00:22:47,940 --> 00:22:53,250
disk here and i wanted to see if this is

00:22:50,850 --> 00:22:55,370
like some effect of some caching effect

00:22:53,250 --> 00:22:58,289
because we already compiled the colonel

00:22:55,370 --> 00:23:01,140
but even when i rebooted like that the

00:22:58,289 --> 00:23:03,360
lowest level and not a lowest level for

00:23:01,140 --> 00:23:05,730
example if i rebooted guest level 4 and

00:23:03,360 --> 00:23:08,780
rerun the test the numbers just stay the

00:23:05,730 --> 00:23:14,580
same so that was not some kind of

00:23:08,780 --> 00:23:17,490
cashier that kicked in ok now how deep

00:23:14,580 --> 00:23:20,130
can we go how far can we go and actually

00:23:17,490 --> 00:23:22,530
as I said level 1 is the lowest level we

00:23:20,130 --> 00:23:25,559
can we can have with there's no level 0

00:23:22,530 --> 00:23:28,380
on s390x and actually our Hardware

00:23:25,559 --> 00:23:30,600
limits us to eight levels because then

00:23:28,380 --> 00:23:32,280
some control blocks you have where you

00:23:30,600 --> 00:23:34,169
can actually see it on which level you

00:23:32,280 --> 00:23:37,080
are running our simply full because I

00:23:34,169 --> 00:23:41,789
mean eight levels are more than enough

00:23:37,080 --> 00:23:45,900
and it took like a whole lot of cat

00:23:41,789 --> 00:23:48,720
videos on YouTube to reach level 6 so to

00:23:45,900 --> 00:23:51,360
start o'connell took like really I had

00:23:48,720 --> 00:23:54,030
to go to lunch and go back and then I

00:23:51,360 --> 00:23:56,100
only had booted a colonel so it tends to

00:23:54,030 --> 00:23:58,710
get really really slow the further you

00:23:56,100 --> 00:24:01,770
go down and the question would then be

00:23:58,710 --> 00:24:04,440
can we somehow improve this building

00:24:01,770 --> 00:24:07,220
shadow page tables on shadow page tables

00:24:04,440 --> 00:24:10,159
on shadow page tables and just like

00:24:07,220 --> 00:24:12,640
just like producing steel time so that

00:24:10,159 --> 00:24:15,620
is a really the main question

00:24:12,640 --> 00:24:19,909
interestingly while doing this whole

00:24:15,620 --> 00:24:22,370
whole thing okay the N implementation is

00:24:19,909 --> 00:24:24,980
not able to run with the minimal amount

00:24:22,370 --> 00:24:28,130
of hardware virtualization facilities so

00:24:24,980 --> 00:24:30,440
that was one nice side effect because

00:24:28,130 --> 00:24:33,409
like I try to test everything and just

00:24:30,440 --> 00:24:35,960
implemented that then and we actually

00:24:33,409 --> 00:24:40,690
found one random memory overwrite in the

00:24:35,960 --> 00:24:43,460
in the colonel while running nested kvm

00:24:40,690 --> 00:24:45,440
most mostly because i was starting like

00:24:43,460 --> 00:24:47,570
hundreds and hundreds of guests in a row

00:24:45,440 --> 00:24:52,309
and at one point i got crazy error

00:24:47,570 --> 00:24:55,070
messages the thing is right now because

00:24:52,309 --> 00:24:58,669
we have two shadows certain control

00:24:55,070 --> 00:25:01,460
blocks on DMA pages that we require 1 DM

00:24:58,669 --> 00:25:04,820
a page for each virtual CPU we have and

00:25:01,460 --> 00:25:08,210
as you might be aware you can easily run

00:25:04,820 --> 00:25:10,010
out of dma pages so the question would

00:25:08,210 --> 00:25:13,419
then be can be at one point of time

00:25:10,010 --> 00:25:15,919
reduced amount of DM eight pages and

00:25:13,419 --> 00:25:19,549
make the whole process even faster

00:25:15,919 --> 00:25:24,710
because we maintain a cache of shadow

00:25:19,549 --> 00:25:26,690
control blocks and if we can reduce the

00:25:24,710 --> 00:25:28,549
amount of dma pages we can have more

00:25:26,690 --> 00:25:30,620
shadow side control blocks in our cash

00:25:28,549 --> 00:25:34,549
and therefore speed up for example

00:25:30,620 --> 00:25:37,039
over-commitment scenarios the other

00:25:34,549 --> 00:25:40,820
thing I already talked about is can we

00:25:37,039 --> 00:25:43,309
ever completely dropped on kvm nested

00:25:40,820 --> 00:25:46,549
parameters so can we enable nested kvm

00:25:43,309 --> 00:25:48,860
is default of course we want to have cpu

00:25:46,549 --> 00:25:50,809
model support finally integrated so we

00:25:48,860 --> 00:25:54,080
can enable actually hardware

00:25:50,809 --> 00:25:56,120
virtualization for our kvm gasps and I

00:25:54,080 --> 00:25:59,419
mean the plan in the long run is to

00:25:56,120 --> 00:26:01,669
always support features for nessa kvm

00:25:59,419 --> 00:26:04,580
guests as soon as we saw port them in de

00:26:01,669 --> 00:26:06,919
que tengas so I mean right right now we

00:26:04,580 --> 00:26:08,870
have all the features in place we just

00:26:06,919 --> 00:26:10,880
have to take care when we implement

00:26:08,870 --> 00:26:14,840
further further stuff that we also have

00:26:10,880 --> 00:26:19,659
that at that point all right so that's

00:26:14,840 --> 00:26:19,659
basically it are there any questions

00:26:19,720 --> 00:26:25,639
Lou 9 go back a slide please yes I just

00:26:24,559 --> 00:26:28,129
gave you mind on what I wanted to talk

00:26:25,639 --> 00:26:31,669
to about the reason we have the next

00:26:28,129 --> 00:26:35,360
parameter in well no the reason we're

00:26:31,669 --> 00:26:36,830
using the cpu host model on x86 is not

00:26:35,360 --> 00:26:40,340
because we don't want to have people use

00:26:36,830 --> 00:26:42,110
nesting it's because we bought with two

00:26:40,340 --> 00:26:44,179
reasons the first is that it that was

00:26:42,110 --> 00:26:46,249
unstable but that was a separate

00:26:44,179 --> 00:26:47,960
safeguard really that was more the

00:26:46,249 --> 00:26:50,149
kernel parameter and the special please

00:26:47,960 --> 00:26:53,539
enable it part and the reason it's

00:26:50,149 --> 00:26:56,059
enabled for cpu host is that we just

00:26:53,539 --> 00:26:57,559
didn't solve a live migration part that

00:26:56,059 --> 00:27:00,889
really is the main reason why it's not

00:26:57,559 --> 00:27:03,139
enabled on the default models so if you

00:27:00,889 --> 00:27:05,330
have live migration solve there's no

00:27:03,139 --> 00:27:08,499
reason not to just enabled by default on

00:27:05,330 --> 00:27:12,080
all the systems except a security audit

00:27:08,499 --> 00:27:13,369
sure so you have to you have to be

00:27:12,080 --> 00:27:15,860
certain that it's secure and that it's

00:27:13,369 --> 00:27:17,539
safe to use but once you are there's

00:27:15,860 --> 00:27:19,129
nothing holding you back from from just

00:27:17,539 --> 00:27:21,080
enabling by default okay that's

00:27:19,129 --> 00:27:22,639
perfectly its ballot and it's used for

00:27:21,080 --> 00:27:26,769
its part of the CPU at the end of the

00:27:22,639 --> 00:27:31,249
day the other one is your comment about

00:27:26,769 --> 00:27:34,789
nesting all the way down have you tried

00:27:31,249 --> 00:27:39,259
so that but basically that the issue you

00:27:34,789 --> 00:27:41,809
have is the dynamic faulting of pages

00:27:39,259 --> 00:27:45,350
into a guest right or well you actually

00:27:41,809 --> 00:27:47,539
have two levels you have one time your

00:27:45,350 --> 00:27:48,860
fault because you just need to have the

00:27:47,539 --> 00:27:50,929
users based page populated then you've

00:27:48,860 --> 00:27:54,529
got another time to have your you Nestor

00:27:50,929 --> 00:27:56,419
page table fold it in so basically all

00:27:54,529 --> 00:27:57,769
you need to do is to reduce those two

00:27:56,419 --> 00:28:01,100
the first is easy you can just be

00:27:57,769 --> 00:28:03,190
populate and we could do a mem set on

00:28:01,100 --> 00:28:05,330
our qmu process and that would address

00:28:03,190 --> 00:28:07,249
avoided you have a lot of page from can

00:28:05,330 --> 00:28:11,720
just do an M advice that's good enough

00:28:07,249 --> 00:28:13,789
ok yeah and for your for your NASA page

00:28:11,720 --> 00:28:16,369
table you just want to have some

00:28:13,789 --> 00:28:18,850
whatever it is some mechanism to tell it

00:28:16,369 --> 00:28:21,470
please pre-populate before you start and

00:28:18,850 --> 00:28:24,230
then at that point if you just enable

00:28:21,470 --> 00:28:26,690
those you shouldn't get any false doing

00:28:24,230 --> 00:28:28,550
just basically boot up time should be

00:28:26,690 --> 00:28:31,040
almost the same as in on every layer

00:28:28,550 --> 00:28:34,040
the thing is as soon as you change the

00:28:31,040 --> 00:28:37,970
shadow page table you trigger on shadows

00:28:34,040 --> 00:28:40,070
on another level yes but you never

00:28:37,970 --> 00:28:42,380
change them at that point but you can't

00:28:40,070 --> 00:28:45,860
like fully fully create a shadow up

00:28:42,380 --> 00:28:49,130
front why not if your user space pages

00:28:45,860 --> 00:28:51,560
are fully patched in and you pre

00:28:49,130 --> 00:28:55,210
populate your nesting nested page table

00:28:51,560 --> 00:28:58,130
it won't change over lifetime of your vm

00:28:55,210 --> 00:29:00,230
yeah we could do that the patent would

00:28:58,130 --> 00:29:03,080
then mean that like the first one trying

00:29:00,230 --> 00:29:08,600
to run a virtual CPU would actually like

00:29:03,080 --> 00:29:10,280
trigger a whole bunch of like actions on

00:29:08,600 --> 00:29:12,080
the undershadow I mean you have to

00:29:10,280 --> 00:29:14,540
populate your complete shadow page

00:29:12,080 --> 00:29:17,090
tables and even then you're not sure

00:29:14,540 --> 00:29:20,060
that your host might not simply do our

00:29:17,090 --> 00:29:22,040
case and run or page something out or

00:29:20,060 --> 00:29:24,320
dirty track and then something gets

00:29:22,040 --> 00:29:27,290
thrown out of your shadow page tables

00:29:24,320 --> 00:29:29,960
that's a tuning problem you still can

00:29:27,290 --> 00:29:31,940
use it you can still I mean to give it a

00:29:29,960 --> 00:29:33,590
try see how fast you get if that

00:29:31,940 --> 00:29:35,240
basically gets you into a margin that

00:29:33,590 --> 00:29:36,410
you can go further than eight layers you

00:29:35,240 --> 00:29:38,360
can talk to your architects that you

00:29:36,410 --> 00:29:42,140
need more more pages not more definite

00:29:38,360 --> 00:29:44,150
integrals and I I honestly on the main

00:29:42,140 --> 00:29:47,300
the main problem we had on x86 last time

00:29:44,150 --> 00:29:49,850
I measured it was I all cpu time was

00:29:47,300 --> 00:29:53,060
okayish further down I all went

00:29:49,850 --> 00:29:54,470
completely down anything like you just

00:29:53,060 --> 00:29:56,480
couldn't access any i/o anymore because

00:29:54,470 --> 00:29:58,220
you're constantly bouncing on on air

00:29:56,480 --> 00:29:59,900
miles and PIOs are interrupt on

00:29:58,220 --> 00:30:01,060
everything out there was just bouncing

00:29:59,900 --> 00:30:02,840
back and forth through all the layers

00:30:01,060 --> 00:30:06,650
since you don't have the dishy

00:30:02,840 --> 00:30:07,970
apparently you should be fine really as

00:30:06,650 --> 00:30:10,070
long as you people be like yo your

00:30:07,970 --> 00:30:11,780
memory just give it a try see if it

00:30:10,070 --> 00:30:13,040
works and then you can still disabled

00:30:11,780 --> 00:30:18,470
case and if you really don't want it

00:30:13,040 --> 00:30:21,530
okay thanks do you have like if you use

00:30:18,470 --> 00:30:24,200
huge pages does that make the shadowing

00:30:21,530 --> 00:30:26,600
cheaper like if you use gigabyte pages

00:30:24,200 --> 00:30:28,340
then you only have 30 for don't know

00:30:26,600 --> 00:30:30,590
many levels of page tables you out but

00:30:28,340 --> 00:30:34,490
the thing is that we don't support huge

00:30:30,590 --> 00:30:38,480
pages for kvm backing storage yet okay

00:30:34,490 --> 00:30:41,700
but we supported for nests of cave here

00:30:38,480 --> 00:30:43,889
so what we actually do it so if you

00:30:41,700 --> 00:30:46,559
would like runnin s @ KD n guess that

00:30:43,889 --> 00:30:48,480
has huge pages and creates its own nest

00:30:46,559 --> 00:30:51,749
ok then guess I would be able to create

00:30:48,480 --> 00:30:53,730
fake fake page tables to simulate it so

00:30:51,749 --> 00:30:56,220
that would be possible and later on we

00:30:53,730 --> 00:30:59,309
could even optimized for that case on in

00:30:56,220 --> 00:31:02,279
some scenarios to in that case then

00:30:59,309 --> 00:31:03,960
reduce the amount of like shadow page

00:31:02,279 --> 00:31:06,749
tables we have to create yeah because

00:31:03,960 --> 00:31:09,330
certain certain scenarios because a page

00:31:06,749 --> 00:31:11,460
might be consecutive for our guests but

00:31:09,330 --> 00:31:13,049
it doesn't have to do it have to be a no

00:31:11,460 --> 00:31:15,179
hypervisor where we're creating the

00:31:13,049 --> 00:31:17,850
shadow page tables it could be but it

00:31:15,179 --> 00:31:21,179
doesn't have to yeah so I mean you can

00:31:17,850 --> 00:31:24,600
just configure handed the gigabyte use

00:31:21,179 --> 00:31:28,230
wfs pages in the first 95 in the second

00:31:24,600 --> 00:31:32,249
nine exactly exactly and then really you

00:31:28,230 --> 00:31:34,950
have a thousand entries in the page

00:31:32,249 --> 00:31:37,200
tables for for all the levels of the

00:31:34,950 --> 00:31:39,749
guests so it shouldn't take my much to

00:31:37,200 --> 00:31:41,100
shadow and then shadow toes exactly a as

00:31:39,749 --> 00:31:42,539
soon as we have it further packing

00:31:41,100 --> 00:31:59,580
storage that would absolutely make

00:31:42,539 --> 00:32:02,070
sensor 3 + 2 y 3 week regions segments

00:31:59,580 --> 00:32:05,369
and page 23 but actually I mean it

00:32:02,070 --> 00:32:07,320
depends Yuri you can just like most of

00:32:05,369 --> 00:32:09,539
the time I only get three three levels

00:32:07,320 --> 00:32:13,919
because like you can buy it it doesn't

00:32:09,539 --> 00:32:20,869
have to be five its maximum is 5 ok any

00:32:13,919 --> 00:32:20,869

YouTube URL: https://www.youtube.com/watch?v=eZLxvC7OEOk


