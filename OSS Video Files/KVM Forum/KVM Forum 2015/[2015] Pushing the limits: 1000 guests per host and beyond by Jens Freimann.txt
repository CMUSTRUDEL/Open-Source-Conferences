Title: [2015] Pushing the limits: 1000 guests per host and beyond by Jens Freimann
Publication date: 2015-08-26
Playlist: KVM Forum 2015
Description: 
	What happens when instead of 10 virtual machines per host system you start 100 or even 1000? What happens when you have 1000 disks per guest? On the Mainframe, setups with hundreds or thousands of devices and a huge number of guests are not unheard of. Large scenarios like that tend to find limiting factors, and that is no different on KVM. In this talk, we will present our findings for such setups under KVM. We will discuss our experiences when trying to scale to large numbers of guests and devices on z Systems: which limits we hit, how we got around them, and where we still have unresolved issues.

Jens Freimann
Software Engineer, IBM
Jens Freimann is currently employed at IBM Research & Development Gmbh in Germany. He has been working as a software engineer on KVM and QEMU on System z for the last 3.5 years. Previously he was a Firmware Engineer for I/O Firmware on System z where he was working on a Simulator that used KVM as a tool to do a full system simulation to support in development and test.

Slides: https://drive.google.com/file/d/0B6HTUUWSPdd-QlBta2ZEOWhQRlU/view?usp=sharing
Captions: 
	00:00:14,960 --> 00:00:26,240
okay hi everyone so my name is Jen's I

00:00:20,130 --> 00:00:28,340
work for the kvm on i-290 team at IBM

00:00:26,240 --> 00:00:31,380
we've been trying out some things

00:00:28,340 --> 00:00:35,790
regarding many guests and many devices

00:00:31,380 --> 00:00:37,649
on a kvm hypervisor on s390 and we ran

00:00:35,790 --> 00:00:39,930
into some things that we sought are

00:00:37,649 --> 00:00:45,420
interesting when doing that and that's

00:00:39,930 --> 00:00:49,559
what I want to talk about today so just

00:00:45,420 --> 00:00:52,350
like emus are usually living in small

00:00:49,559 --> 00:00:54,390
groups I would expect that most of the

00:00:52,350 --> 00:00:57,780
time also p.m. use and guests are

00:00:54,390 --> 00:00:59,520
running on a small number of qms and

00:00:57,780 --> 00:01:01,710
guests are running on a host so

00:00:59,520 --> 00:01:04,830
depending on how well it is equipped

00:01:01,710 --> 00:01:07,650
with resources of course so the point of

00:01:04,830 --> 00:01:09,960
this talk kind of us to show what

00:01:07,650 --> 00:01:13,439
happened when we try to make our qm use

00:01:09,960 --> 00:01:17,240
live in large flocks and this picture

00:01:13,439 --> 00:01:20,729
down here is from the wikipedia page of

00:01:17,240 --> 00:01:23,869
australian great war on amuse from 1932

00:01:20,729 --> 00:01:25,799
I won't tell the whole story now but I

00:01:23,869 --> 00:01:28,560
encouraged you to read up on it

00:01:25,799 --> 00:01:31,670
afterwards it's I started was a really

00:01:28,560 --> 00:01:34,290
amusing read it's basically about how

00:01:31,670 --> 00:01:36,119
amuse were hunted by soldiers with

00:01:34,290 --> 00:01:40,409
machine guns mounted to their pickups

00:01:36,119 --> 00:01:43,680
and about how the amuse much as two

00:01:40,409 --> 00:01:48,600
intelligent and two robust and too fast

00:01:43,680 --> 00:01:51,030
for the soldiers to catch them but let's

00:01:48,600 --> 00:01:56,460
go back to the talk so what is this

00:01:51,030 --> 00:01:59,130
about kvm runs from many devices said

00:01:56,460 --> 00:02:01,829
runs on and better devices and runs on

00:01:59,130 --> 00:02:06,210
your laptop it runs on ladder service

00:02:01,829 --> 00:02:10,560
and it runs on the mainframe I meant the

00:02:06,210 --> 00:02:13,740
mainframe and we were curious to find

00:02:10,560 --> 00:02:19,370
out just what the limitations

00:02:13,740 --> 00:02:23,670
our width of the Lewbert qmu kvm stack

00:02:19,370 --> 00:02:25,830
with respect to our systems so the

00:02:23,670 --> 00:02:27,540
question was some how far can we go

00:02:25,830 --> 00:02:30,630
given a sufficient host configuration

00:02:27,540 --> 00:02:33,060
and where and when does it break so how

00:02:30,630 --> 00:02:35,430
many guests could we put on there how

00:02:33,060 --> 00:02:38,310
many block and networking devices could

00:02:35,430 --> 00:02:41,790
we use what what resources are necessary

00:02:38,310 --> 00:02:44,220
in the host system what configuration

00:02:41,790 --> 00:02:47,700
settings do you have to make or do you

00:02:44,220 --> 00:02:52,470
have to adapt when you go move two more

00:02:47,700 --> 00:02:56,190
guests and so I just want to point out

00:02:52,470 --> 00:02:58,770
that this was just testing the extreme

00:02:56,190 --> 00:03:00,780
limits I'm not at saying that you should

00:02:58,770 --> 00:03:03,360
run for thousands guests and one whole

00:03:00,780 --> 00:03:07,940
system just because there's many reasons

00:03:03,360 --> 00:03:10,410
not to I also won't talk about

00:03:07,940 --> 00:03:14,760
performance measurements simply because

00:03:10,410 --> 00:03:16,200
I wasn't involved in them and maybe

00:03:14,760 --> 00:03:18,180
that's not too much sense in doing

00:03:16,200 --> 00:03:24,300
performance measurements with 5,000

00:03:18,180 --> 00:03:27,780
guest was in one single partition so we

00:03:24,300 --> 00:03:31,620
did run workload so it was not just for

00:03:27,780 --> 00:03:34,020
thousands idle processes we did run

00:03:31,620 --> 00:03:36,600
workloads and a couple hundred of those

00:03:34,020 --> 00:03:40,860
guests and I will talk about more we'll

00:03:36,600 --> 00:03:45,180
talk more about that later but let's

00:03:40,860 --> 00:03:49,040
talk about what we tested was so this is

00:03:45,180 --> 00:03:51,360
the newest IBM mainframe mother and

00:03:49,040 --> 00:03:54,420
mainframes you have to know are always

00:03:51,360 --> 00:03:56,580
running a first level hypervisor that

00:03:54,420 --> 00:04:00,810
basically just partitions the hardware

00:03:56,580 --> 00:04:04,560
and we use one of these petitions to run

00:04:00,810 --> 00:04:08,820
a kvm host system and this partition was

00:04:04,560 --> 00:04:12,120
assigned two terabytes of memory it had

00:04:08,820 --> 00:04:15,270
64 CPUs they were shared with other

00:04:12,120 --> 00:04:17,940
partitions on a system but most of the

00:04:15,270 --> 00:04:20,430
time after that we were running just

00:04:17,940 --> 00:04:24,260
this petition especially when running

00:04:20,430 --> 00:04:27,530
the test about on many many guests

00:04:24,260 --> 00:04:32,930
and then f our guests we were running

00:04:27,530 --> 00:04:38,210
Boswell 7.1 and scissors 12 guests had

00:04:32,930 --> 00:04:41,120
one gig of memory to virtual CPUs as if

00:04:38,210 --> 00:04:45,850
the backend boom using mostly image

00:04:41,120 --> 00:04:50,420
files as blood io block devices

00:04:45,850 --> 00:04:57,200
networking was so much but also be

00:04:50,420 --> 00:05:01,370
hostnet devices permanet devices so well

00:04:57,200 --> 00:05:04,340
this is what cpu info looks like now

00:05:01,370 --> 00:05:06,770
what did we test on what kind of tests

00:05:04,340 --> 00:05:10,330
that we run so we test the different

00:05:06,770 --> 00:05:13,340
things not just many guests but also i'm

00:05:10,330 --> 00:05:16,070
attaching many block devices to single

00:05:13,340 --> 00:05:18,590
guest attaching even more of a dial

00:05:16,070 --> 00:05:22,070
block devices to several guests too soon

00:05:18,590 --> 00:05:24,440
one host assigning quite a bit of

00:05:22,070 --> 00:05:28,700
inverter net devices to a single guests

00:05:24,440 --> 00:05:31,340
and then also well as a challenge was

00:05:28,700 --> 00:05:32,930
and assigning large amounts of these

00:05:31,340 --> 00:05:35,510
resources to the host system itself

00:05:32,930 --> 00:05:40,730
because it's not something that has done

00:05:35,510 --> 00:05:45,080
a lot for example in one test we assign

00:05:40,730 --> 00:05:47,330
a terabyte of RAM to the host a talk

00:05:45,080 --> 00:05:50,780
more about that later and we also try to

00:05:47,330 --> 00:05:53,150
migrate many guests I put 65 because it

00:05:50,780 --> 00:05:59,720
was our first test but we could do just

00:05:53,150 --> 00:06:02,780
much more than 64 so as I said we did

00:05:59,720 --> 00:06:09,820
run into a few obstacles and i'll start

00:06:02,780 --> 00:06:09,820
with not very traumatic one so that is

00:06:10,570 --> 00:06:15,230
when we did run into problems so let's

00:06:13,160 --> 00:06:19,610
say we had a kernel crash and we wanted

00:06:15,230 --> 00:06:20,990
to take a memory dump thing is that the

00:06:19,610 --> 00:06:23,840
amount of memory in service has

00:06:20,990 --> 00:06:28,490
increased much faster than the bandwidth

00:06:23,840 --> 00:06:31,190
to memory or to storage so and we have a

00:06:28,490 --> 00:06:34,880
special case here in s 190 so nasty 90

00:06:31,190 --> 00:06:36,529
we can do memory dumps in very early and

00:06:34,880 --> 00:06:40,219
late stages of booting

00:06:36,529 --> 00:06:41,919
and when we do that we're not using the

00:06:40,219 --> 00:06:44,149
typical quedan tool but we have

00:06:41,919 --> 00:06:47,089
standalone tools that we installed two

00:06:44,149 --> 00:06:51,739
separate discs and that we can boot when

00:06:47,089 --> 00:06:54,319
the system has crashed so not Calum

00:06:51,739 --> 00:06:56,659
around tools and the downside of these

00:06:54,319 --> 00:06:59,689
tools is that you don't have all the

00:06:56,659 --> 00:07:03,739
features that k don't have so there's no

00:06:59,689 --> 00:07:06,319
filtering so no make them file you can

00:07:03,739 --> 00:07:09,919
only dumb plus one cpu at a time and so

00:07:06,319 --> 00:07:11,869
on so when you do this dumps it can take

00:07:09,919 --> 00:07:15,699
a very long time depending on how much

00:07:11,869 --> 00:07:18,019
memory you want to dump so hours days

00:07:15,699 --> 00:07:22,759
depending on what how the system is

00:07:18,019 --> 00:07:25,009
configured there's a proof of concept

00:07:22,759 --> 00:07:27,919
that we're working on where we can use

00:07:25,009 --> 00:07:31,459
some both kadem and our standalone tools

00:07:27,919 --> 00:07:35,869
and combination and that works is just

00:07:31,459 --> 00:07:40,099
not widely deployed yet so this is kind

00:07:35,869 --> 00:07:43,249
of work in progress if you are so

00:07:40,099 --> 00:07:46,939
discipline suppose hosts crashes and

00:07:43,249 --> 00:07:50,110
guest crashes but if you are divining I

00:07:46,939 --> 00:07:53,749
want to take a memory dump of a guest

00:07:50,110 --> 00:07:57,679
you could also just do a live debugging

00:07:53,749 --> 00:08:00,110
and that's going to be a talk later this

00:07:57,679 --> 00:08:03,919
afternoon by David children 20 will tell

00:08:00,110 --> 00:08:08,239
you all about how to debug kvm guests

00:08:03,919 --> 00:08:11,389
and you also go into that I believe so

00:08:08,239 --> 00:08:16,069
the next thing we did run into and

00:08:11,389 --> 00:08:20,529
that's s390 specific thing on a string

00:08:16,069 --> 00:08:24,469
90 so we're trying to tell to create

00:08:20,529 --> 00:08:27,999
2,000 networking interfaces in the whole

00:08:24,469 --> 00:08:31,639
system and the problem we found is that

00:08:27,999 --> 00:08:36,439
less in the su 90 networking Trevor code

00:08:31,639 --> 00:08:38,990
so whenever you add a new network

00:08:36,439 --> 00:08:41,209
interface in the nukes the driver will

00:08:38,990 --> 00:08:43,789
rewrite all the current mac addresses

00:08:41,209 --> 00:08:44,990
it's responsible for to the hardware so

00:08:43,789 --> 00:08:47,060
it's toast and a store

00:08:44,990 --> 00:08:48,920
semin hard way again and when you had a

00:08:47,060 --> 00:08:51,680
new one it's not just going to add a new

00:08:48,920 --> 00:08:54,260
one it's going to rewrite every single

00:08:51,680 --> 00:08:58,970
mac address again and then add the new

00:08:54,260 --> 00:09:02,570
one on top and that took quite a long

00:08:58,970 --> 00:09:05,120
time so even with 50 devices and it took

00:09:02,570 --> 00:09:07,820
more than a minute to to add a new

00:09:05,120 --> 00:09:11,060
networking in the face so that's clearly

00:09:07,820 --> 00:09:14,450
a back in an hour driver and were fixing

00:09:11,060 --> 00:09:18,980
this but it was discovered just during

00:09:14,450 --> 00:09:21,589
these tests another thing that's

00:09:18,980 --> 00:09:24,550
probably obvious to many people I said

00:09:21,589 --> 00:09:27,260
Dolan explosion code is kind of limited

00:09:24,550 --> 00:09:30,940
2024 parts so if you need more you would

00:09:27,260 --> 00:09:34,250
add more approaches I guess and then

00:09:30,940 --> 00:09:36,470
also just a system control setting

00:09:34,250 --> 00:09:42,350
basically that we had to increase as the

00:09:36,470 --> 00:09:44,810
number of ipv6 running entries so while

00:09:42,350 --> 00:09:46,459
we added these two thousand devices we

00:09:44,810 --> 00:09:53,329
had to increase the limit because it was

00:09:46,459 --> 00:09:57,950
also i think at 1024 ok let's move from

00:09:53,329 --> 00:09:59,660
network interfaces to disks we hit a few

00:09:57,950 --> 00:10:00,950
system control settings years but i'm

00:09:59,660 --> 00:10:06,470
going to start with them because it's

00:10:00,950 --> 00:10:09,040
the most simple thing I notify watches

00:10:06,470 --> 00:10:11,899
we ran out of them pretty quickly um

00:10:09,040 --> 00:10:14,450
when we attached file hospice to our

00:10:11,899 --> 00:10:16,160
host so that's a setting you would

00:10:14,450 --> 00:10:19,370
increase if you want to use that number

00:10:16,160 --> 00:10:22,370
of disks and then of course file

00:10:19,370 --> 00:10:24,430
descriptors I'm pretty sure every one of

00:10:22,370 --> 00:10:27,170
you had to increase the setting of

00:10:24,430 --> 00:10:30,740
maximum file descriptors at some point

00:10:27,170 --> 00:10:34,000
so did we in this case and so let's

00:10:30,740 --> 00:10:37,310
count file descriptor usage and QM you

00:10:34,000 --> 00:10:39,170
just buy an example I looked at it and

00:10:37,310 --> 00:10:41,870
most of the file descriptors we use our

00:10:39,170 --> 00:10:44,930
event file descriptors we have one

00:10:41,870 --> 00:10:49,760
pervert Q so let's look at an example of

00:10:44,930 --> 00:10:52,730
a veteran at device 12 onward q4 send

00:10:49,760 --> 00:10:55,420
and receive from probably more of you so

00:10:52,730 --> 00:10:59,050
magic you I guess

00:10:55,420 --> 00:11:04,240
one for control lane kind of then for

00:10:59,050 --> 00:11:06,430
death we tap def v hostnet plus 3i r qf

00:11:04,240 --> 00:11:09,340
t so that's some eight file descriptors

00:11:06,430 --> 00:11:13,630
in total do that for a thousand billion

00:11:09,340 --> 00:11:18,360
at devices and it goes boom and that's

00:11:13,630 --> 00:11:22,270
just because our default setting was 20

00:11:18,360 --> 00:11:25,680
but bottom line is that we need many of

00:11:22,270 --> 00:11:29,470
them so just be aware of that if you use

00:11:25,680 --> 00:11:32,200
many devices it's the same for block

00:11:29,470 --> 00:11:39,490
devices of course it's just a day use

00:11:32,200 --> 00:11:41,110
less word cues so let's see what

00:11:39,490 --> 00:11:43,060
happened when we used many video blog

00:11:41,110 --> 00:11:47,500
devices in a single guest we start with

00:11:43,060 --> 00:11:52,480
one added some more still fine added a

00:11:47,500 --> 00:11:56,410
lot more and at that point we discovered

00:11:52,480 --> 00:11:58,030
something interesting so we tweaked all

00:11:56,410 --> 00:12:00,850
the system controller settings now and

00:11:58,030 --> 00:12:03,880
what could still go wrong right and

00:12:00,850 --> 00:12:07,540
before I go on I have to say we tried us

00:12:03,880 --> 00:12:09,250
a while ago a long time ago and back

00:12:07,540 --> 00:12:11,200
then we were still using select with no

00:12:09,250 --> 00:12:14,650
wheels impose so back then the limit was

00:12:11,200 --> 00:12:18,070
at husband for file descriptors so but

00:12:14,650 --> 00:12:21,910
now our target was 4,000 disks and we

00:12:18,070 --> 00:12:25,840
started with 1,000 and what we noticed

00:12:21,910 --> 00:12:28,240
when we did that our guest took 13

00:12:25,840 --> 00:12:32,860
minutes sometimes more sometimes a

00:12:28,240 --> 00:12:36,460
little bit less to start up and so one

00:12:32,860 --> 00:12:38,680
of our developers debugged into it and

00:12:36,460 --> 00:12:40,660
found a problem actually was a

00:12:38,680 --> 00:12:44,770
regression that had sneaked and a while

00:12:40,660 --> 00:12:46,930
ago and so what the problem was that we

00:12:44,770 --> 00:12:49,750
were calling a dysfunction blood drive

00:12:46,930 --> 00:12:53,710
train are for each single video reset

00:12:49,750 --> 00:12:56,530
and for every block driver state that's

00:12:53,710 --> 00:12:57,910
a lot of course we all fall and the

00:12:56,530 --> 00:12:59,230
problem is that actually we don't have

00:12:57,910 --> 00:13:02,500
to call that for every single block

00:12:59,230 --> 00:13:05,920
driver state we can call it for every

00:13:02,500 --> 00:13:08,810
single context and that's just fine so

00:13:05,920 --> 00:13:12,440
with this fix we get down to six

00:13:08,810 --> 00:13:15,710
in seconds for a thousand disks and well

00:13:12,440 --> 00:13:18,710
this is the upstream commit so we could

00:13:15,710 --> 00:13:26,089
even use the 4000 disc after that fixed

00:13:18,710 --> 00:13:28,630
there was no problem really okay um more

00:13:26,089 --> 00:13:32,089
disks but not an was in a single guest

00:13:28,630 --> 00:13:34,790
but was over multiple guests within one

00:13:32,089 --> 00:13:37,010
host system and I are the first thing we

00:13:34,790 --> 00:13:41,660
had to tweak was the setting for the

00:13:37,010 --> 00:13:43,490
maximum number of al context just a

00:13:41,660 --> 00:13:48,589
source control setting and was that

00:13:43,490 --> 00:13:52,120
fixed there was no further problem

00:13:48,589 --> 00:13:55,640
actually you so that worked quite well

00:13:52,120 --> 00:14:01,160
now let's move on to starting many

00:13:55,640 --> 00:14:04,490
guests remember our guests had what I

00:14:01,160 --> 00:14:08,900
one at in the face but I blocked it back

00:14:04,490 --> 00:14:11,589
back by an image and two virtual CPUs so

00:14:08,900 --> 00:14:14,540
down we continued to add more guests

00:14:11,589 --> 00:14:19,120
everything fine I was in fine everything

00:14:14,540 --> 00:14:22,070
fine at some point it was not fine and

00:14:19,120 --> 00:14:26,180
again let me start with some of the

00:14:22,070 --> 00:14:27,710
settings that we had to tweak I know if

00:14:26,180 --> 00:14:29,270
you run into these things at some point

00:14:27,710 --> 00:14:33,110
it's obvious oh of course I have to

00:14:29,270 --> 00:14:36,589
increase that but in some cases it took

00:14:33,110 --> 00:14:39,200
us a little bit to find that out so the

00:14:36,589 --> 00:14:42,770
first thing again was opened files was

00:14:39,200 --> 00:14:45,830
file descriptors and I guess that maybe

00:14:42,770 --> 00:14:48,950
that's too small to read so there's a

00:14:45,830 --> 00:14:51,920
setting in libvirt config file you can

00:14:48,950 --> 00:14:53,209
increase that there and it will increase

00:14:51,920 --> 00:14:58,100
the maximum number of file descriptors

00:14:53,209 --> 00:15:02,110
for your lip read demon process then Max

00:14:58,100 --> 00:15:05,000
PT wise we ran out of them as well on

00:15:02,110 --> 00:15:08,660
but the SSS controlled setting you just

00:15:05,000 --> 00:15:13,160
increase it and it goes on it scales

00:15:08,660 --> 00:15:15,620
quite well when you start many guests

00:15:13,160 --> 00:15:20,030
you also create many processes and

00:15:15,620 --> 00:15:23,290
threats and they can be limited to so in

00:15:20,030 --> 00:15:23,290
the beginning our system

00:15:23,450 --> 00:15:31,050
we just had to increase this number and

00:15:26,580 --> 00:15:34,350
a leopard demonfire end up a different

00:15:31,050 --> 00:15:36,210
service actually that will increase the

00:15:34,350 --> 00:15:38,040
number of maxim poses for your lipid

00:15:36,210 --> 00:15:41,550
demon process and then if you need to

00:15:38,040 --> 00:15:44,010
increase this number for individual qmu

00:15:41,550 --> 00:15:47,700
process that's a different config file

00:15:44,010 --> 00:15:50,640
or different setting in q mu conf so we

00:15:47,700 --> 00:15:52,230
did that as well and then the most

00:15:50,640 --> 00:15:54,810
simple problem actually when we started

00:15:52,230 --> 00:15:58,260
this we just didn't have enough RAM I

00:15:54,810 --> 00:16:03,870
think we only had a terabyte or so too

00:15:58,260 --> 00:16:06,390
bad so we added one more 22 and with

00:16:03,870 --> 00:16:11,300
that you would sink okay now we got it

00:16:06,390 --> 00:16:15,180
all let's start guests but then I

00:16:11,300 --> 00:16:20,130
learned something new that day so it

00:16:15,180 --> 00:16:22,350
turns out and before we were using an

00:16:20,130 --> 00:16:25,170
internal distribution net wasn't using

00:16:22,350 --> 00:16:27,660
system d and I just didn't know but

00:16:25,170 --> 00:16:34,380
there's a service call a static service

00:16:27,660 --> 00:16:37,650
called system new machine d and so

00:16:34,380 --> 00:16:40,290
leopardy will call this function in this

00:16:37,650 --> 00:16:44,670
machine d via an RPC over the bus every

00:16:40,290 --> 00:16:47,160
single time it starts a guest and the

00:16:44,670 --> 00:16:49,920
problem was here like let me go back one

00:16:47,160 --> 00:16:51,450
so at some point we're starting guests

00:16:49,920 --> 00:16:55,230
of the guest of the guests of the guests

00:16:51,450 --> 00:16:58,010
and actually even at something like 20

00:16:55,230 --> 00:17:00,720
guests bus or so and we would see hangs

00:16:58,010 --> 00:17:02,760
so we're starting guests and lube and it

00:17:00,720 --> 00:17:05,910
would start and would just hang do

00:17:02,760 --> 00:17:07,800
nothing anymore until sometimes a couple

00:17:05,910 --> 00:17:12,750
seconds later sometimes a few more

00:17:07,800 --> 00:17:14,280
minutes later it would go on and we just

00:17:12,750 --> 00:17:16,320
didn't know why and we looked into

00:17:14,280 --> 00:17:21,000
levert decode and everything and

00:17:16,320 --> 00:17:23,820
debugged for quite a while but then

00:17:21,000 --> 00:17:25,950
given this error messages i eventually

00:17:23,820 --> 00:17:29,430
tried to find what does create machines

00:17:25,950 --> 00:17:32,090
function was it was caught here and that

00:17:29,430 --> 00:17:35,460
was in the system machine need service

00:17:32,090 --> 00:17:38,549
so turns out that in

00:17:35,460 --> 00:17:40,649
older versions of system d this message

00:17:38,549 --> 00:17:42,510
handling that they were doing and the

00:17:40,649 --> 00:17:45,390
message loop was kind of buggy what

00:17:42,510 --> 00:17:49,890
they're doing what we're on read our

00:17:45,390 --> 00:17:52,590
messages and handle them and then go one

00:17:49,890 --> 00:17:55,260
step further and but in this next step

00:17:52,590 --> 00:17:58,890
there was some but nobody noticed was

00:17:55,260 --> 00:18:00,929
they were read again messages from that

00:17:58,890 --> 00:18:03,720
file descriptor from that Bieber's

00:18:00,929 --> 00:18:06,270
socket or whatever and then the next

00:18:03,720 --> 00:18:09,390
step was okay wait for more messengers

00:18:06,270 --> 00:18:10,919
and so I just pointed with us it was

00:18:09,390 --> 00:18:12,630
just sitting around waiting for my

00:18:10,919 --> 00:18:16,740
messages and it would only continue

00:18:12,630 --> 00:18:19,409
would only continue when a next another

00:18:16,740 --> 00:18:22,320
gas was being started so at that point

00:18:19,409 --> 00:18:24,510
it would go on and process all the

00:18:22,320 --> 00:18:30,690
messages that were read previously into

00:18:24,510 --> 00:18:34,350
the buffer yeah that was a very message

00:18:30,690 --> 00:18:36,539
handling that sticks now so I've first

00:18:34,350 --> 00:18:39,600
thing I found was a patch that someone

00:18:36,539 --> 00:18:42,000
attached to a sandwich bugzilla that

00:18:39,600 --> 00:18:45,840
would fix this message handling and I

00:18:42,000 --> 00:18:49,159
tried it and was that it worked we could

00:18:45,840 --> 00:18:54,360
start guests so this fix went into

00:18:49,159 --> 00:18:56,159
system d version 2 19 i'm not sure i

00:18:54,360 --> 00:18:58,320
don't think they attach to a patch that

00:18:56,159 --> 00:18:59,940
was originally developed for it but they

00:18:58,320 --> 00:19:03,179
rewrote the entire message i'm a goob

00:18:59,940 --> 00:19:08,850
but but does rewrite um we were able to

00:19:03,179 --> 00:19:11,039
start 4096 guests and i can't do a demo

00:19:08,850 --> 00:19:15,390
today but i took a couple of screenshots

00:19:11,039 --> 00:19:16,890
I hope that's okay so at this point when

00:19:15,390 --> 00:19:19,529
I took that screen shield were close to

00:19:16,890 --> 00:19:21,270
running five thousand guests um see

00:19:19,529 --> 00:19:24,090
there's quite a lot of task but only 24

00:19:21,270 --> 00:19:27,690
running so go into that right now a

00:19:24,090 --> 00:19:32,010
little bit later so um you see that we

00:19:27,690 --> 00:19:36,240
defined way more than 4096 guests we had

00:19:32,010 --> 00:19:37,710
to find more than 8,000 inspect and at

00:19:36,240 --> 00:19:42,779
the pot and at this point we're running

00:19:37,710 --> 00:19:45,750
were close to 4,000 actually and so

00:19:42,779 --> 00:19:50,580
given enough resources this was

00:19:45,750 --> 00:19:53,320
absolutely possible and the system was

00:19:50,580 --> 00:19:56,410
absolutely usable stable you can still

00:19:53,320 --> 00:20:01,240
you could still do everything not wasn't

00:19:56,410 --> 00:20:03,370
sluggish or anything at all so yeah

00:20:01,240 --> 00:20:05,860
there's also a screen shot off and I put

00:20:03,370 --> 00:20:10,120
a free so we weren't using all the two

00:20:05,860 --> 00:20:11,920
terabytes and the workload said we're

00:20:10,120 --> 00:20:14,050
running I mentioned the beginning who

00:20:11,920 --> 00:20:15,940
didn't run workload on all the four

00:20:14,050 --> 00:20:21,280
thousand guests think we're running

00:20:15,940 --> 00:20:23,860
workload on close to a thousand it was

00:20:21,280 --> 00:20:27,040
cpu stress tool and naproxen stress tool

00:20:23,860 --> 00:20:34,600
i think there was some instances with a

00:20:27,040 --> 00:20:36,580
block iowa stress yeah so this is a

00:20:34,600 --> 00:20:41,820
screenshot of a guest actually running

00:20:36,580 --> 00:20:41,820
this mantra to let stress in the cpu

00:20:44,010 --> 00:20:49,600
yeah so there were quite a few of those

00:20:47,620 --> 00:20:51,850
well and I think for networking stress

00:20:49,600 --> 00:20:58,900
and so on we used on the stress tool

00:20:51,850 --> 00:21:01,810
itself and others okay um I'm not sure

00:20:58,900 --> 00:21:04,240
if I'd skip over this what ok let's go

00:21:01,810 --> 00:21:07,030
into it um it's a little bit exotic so I

00:21:04,240 --> 00:21:10,690
thought why not sure was wanted to talk

00:21:07,030 --> 00:21:14,620
about this but when kill me use many

00:21:10,690 --> 00:21:17,890
sweats and do that for wisp use words I

00:21:14,620 --> 00:21:20,500
also adds and so on and Shahrukh see has

00:21:17,890 --> 00:21:24,880
had for a while this thing called /

00:21:20,500 --> 00:21:28,000
stretch memory boots or arenas and these

00:21:24,880 --> 00:21:30,520
arenas are created / sweat and so

00:21:28,000 --> 00:21:34,870
they've just allocate some some address

00:21:30,520 --> 00:21:36,820
space basically so for example the

00:21:34,870 --> 00:21:38,950
maximum number of these arenas is

00:21:36,820 --> 00:21:43,090
calculated by us limited by the number

00:21:38,950 --> 00:21:47,740
of course so on a 64-bit system we have

00:21:43,090 --> 00:21:50,620
eight normal foods Pecora maximum x 64

00:21:47,740 --> 00:21:53,770
megabytes of size that each of these

00:21:50,620 --> 00:21:56,400
arenas have so with 64 cars as in our

00:21:53,770 --> 00:21:59,370
case at with 512 arenas

00:21:56,400 --> 00:22:04,320
which is 32 gigabytes of address space

00:21:59,370 --> 00:22:06,710
and that's a lot of address space we're

00:22:04,320 --> 00:22:10,040
just using another problem right but

00:22:06,710 --> 00:22:13,140
some people use trick over commit and

00:22:10,040 --> 00:22:15,780
then there's all of this is a accounted

00:22:13,140 --> 00:22:18,780
a little differently so Malik will fail

00:22:15,780 --> 00:22:22,770
earlier on you and you will get an a bad

00:22:18,780 --> 00:22:24,510
return code and so on this would get

00:22:22,770 --> 00:22:25,950
worse if you increase the number of

00:22:24,510 --> 00:22:30,360
course because then you have kind of

00:22:25,950 --> 00:22:32,130
Mourinho's you can cap the setting by

00:22:30,360 --> 00:22:35,700
setting an environment variable to a

00:22:32,130 --> 00:22:40,080
smaller number so when our case was just

00:22:35,700 --> 00:22:41,880
at 216 at work quite well now I'll admit

00:22:40,080 --> 00:22:43,800
that this is kind of a niche problem

00:22:41,880 --> 00:22:47,010
because many people will say why would

00:22:43,800 --> 00:22:50,040
you use strict overcommit but some

00:22:47,010 --> 00:22:52,650
people do and for them they have to they

00:22:50,040 --> 00:22:54,660
have to be aware of this so even so not

00:22:52,650 --> 00:23:00,390
many people will use that I guess um

00:22:54,660 --> 00:23:03,120
this is a stall thing okay let's talk

00:23:00,390 --> 00:23:04,530
about migration and here I have to say

00:23:03,120 --> 00:23:09,750
that we didn't he really have much

00:23:04,530 --> 00:23:13,620
problems um just two little things what

00:23:09,750 --> 00:23:16,890
we did run out of first was ports so you

00:23:13,620 --> 00:23:19,350
can have a default setting over there is

00:23:16,890 --> 00:23:23,040
a default setting of 64 parts in the

00:23:19,350 --> 00:23:25,410
Liberty config and if you want to do

00:23:23,040 --> 00:23:27,030
more concurrent migrations at a time you

00:23:25,410 --> 00:23:31,190
would have to increase that setting or

00:23:27,030 --> 00:23:36,620
just specify the part directly I think

00:23:31,190 --> 00:23:38,910
so no big trouble so far then another I

00:23:36,620 --> 00:23:42,330
guess strange thing was that one of our

00:23:38,910 --> 00:23:44,880
testers was using scripts that you went

00:23:42,330 --> 00:23:49,050
from another host to trigger on our

00:23:44,880 --> 00:23:50,850
hypervisor here and at some point d

00:23:49,050 --> 00:23:52,920
script will also get stuck and that was

00:23:50,850 --> 00:23:56,100
just because he was running out of ssh

00:23:52,920 --> 00:23:58,130
session so you can only by default you

00:23:56,100 --> 00:24:01,710
could have ten sessions at a time and

00:23:58,130 --> 00:24:06,870
but increasing this number it worked

00:24:01,710 --> 00:24:09,520
also quite well ok so now I'm almost

00:24:06,870 --> 00:24:12,130
nearing the end of my presentation but

00:24:09,520 --> 00:24:15,640
I have a few things where actual code

00:24:12,130 --> 00:24:17,500
was setting limits and we're starting

00:24:15,640 --> 00:24:22,330
with something that only affects s390

00:24:17,500 --> 00:24:24,970
again so nqm you or kvm just is defined

00:24:22,330 --> 00:24:27,280
for the maximum number of pages and that

00:24:24,970 --> 00:24:33,580
means that we can only have an eight

00:24:27,280 --> 00:24:35,920
terabyte per month lot so now 44 s turn

00:24:33,580 --> 00:24:39,660
and you only have one man slot for a

00:24:35,920 --> 00:24:42,010
system ram that means we would probably

00:24:39,660 --> 00:24:44,410
move to something that uses more man

00:24:42,010 --> 00:24:49,090
slots and I think that excited cigs

00:24:44,410 --> 00:24:51,190
already does that i'm not sure but to

00:24:49,090 --> 00:24:53,290
put this into relation um right now the

00:24:51,190 --> 00:24:58,830
newest mainframe model can have ten

00:24:53,290 --> 00:25:01,150
terabyte for the entire system so um I

00:24:58,830 --> 00:25:03,190
admit that it might not be a real

00:25:01,150 --> 00:25:06,550
problem right now but future mainframes

00:25:03,190 --> 00:25:10,810
will have a modded multiple of this so

00:25:06,550 --> 00:25:17,650
who knows so this was not the only I

00:25:10,810 --> 00:25:19,180
tare weight limit we found and this was

00:25:17,650 --> 00:25:22,300
just a simple back in our extra 90

00:25:19,180 --> 00:25:24,520
management code um the problem was that

00:25:22,300 --> 00:25:26,680
the return code of a memory management

00:25:24,520 --> 00:25:28,150
related function was an inn where it

00:25:26,680 --> 00:25:32,290
should have been something bigger and

00:25:28,150 --> 00:25:36,240
anti drama something and was this fixed

00:25:32,290 --> 00:25:40,360
by martine we could successfully assign

00:25:36,240 --> 00:25:45,730
more than a terabyte to our system and

00:25:40,360 --> 00:25:50,820
it would start up and be usable but let

00:25:45,730 --> 00:25:53,950
cut to something a more specific our

00:25:50,820 --> 00:25:58,240
internal irq chip on setting up routes

00:25:53,950 --> 00:26:02,950
so we create an su 90 again we create a

00:25:58,240 --> 00:26:10,720
new irq of the pervert queue and every

00:26:02,950 --> 00:26:12,340
time a new IQ of T is created we really

00:26:10,720 --> 00:26:15,160
so in the kernel what happens then when

00:26:12,340 --> 00:26:16,930
we do this I or control the entire

00:26:15,160 --> 00:26:19,900
routing table is reallocated and

00:26:16,930 --> 00:26:20,290
repopulated and with many devices that

00:26:19,900 --> 00:26:23,800
are

00:26:20,290 --> 00:26:26,530
berceuse this would cause delays because

00:26:23,800 --> 00:26:30,700
the problem here was dead in our a su 90

00:26:26,530 --> 00:26:33,400
code and Q mu for every new route who

00:26:30,700 --> 00:26:36,280
would call this I'll control while but

00:26:33,400 --> 00:26:39,460
the interface actually allow us that you

00:26:36,280 --> 00:26:43,480
can first write all the routing entries

00:26:39,460 --> 00:26:45,220
to global table within q mu and then

00:26:43,480 --> 00:26:48,640
just commit all the routes to the

00:26:45,220 --> 00:26:53,820
colonel by calling the kvm said geez I

00:26:48,640 --> 00:26:53,820
routing code so we weren't doing that

00:26:54,450 --> 00:27:03,970
but fix this and Colonel was the patch %

00:27:00,340 --> 00:27:07,390
upstream I think was already added or

00:27:03,970 --> 00:27:12,070
maybe not but I was talking about this

00:27:07,390 --> 00:27:13,660
like add some only specific to a su 90

00:27:12,070 --> 00:27:18,220
but I think follow one also wanted to

00:27:13,660 --> 00:27:22,350
look at too excited 6 or PCI related

00:27:18,220 --> 00:27:27,760
functions that might do this similarly

00:27:22,350 --> 00:27:30,760
okay then another thing was the IQ chip

00:27:27,760 --> 00:27:32,680
is that it has pins and they are defined

00:27:30,760 --> 00:27:38,650
you're not limited by their defined to

00:27:32,680 --> 00:27:42,490
be 1024 so for su 90 we already extended

00:27:38,650 --> 00:27:45,520
that to 4096 and again soon idea is a

00:27:42,490 --> 00:27:49,960
special case here because we were asked

00:27:45,520 --> 00:27:52,180
or we had to integrate with the already

00:27:49,960 --> 00:27:55,530
existing concept of the IQ chip and

00:27:52,180 --> 00:27:59,830
routing entries which doesn't quite fit

00:27:55,530 --> 00:28:02,380
our adapter interrupt concept we have an

00:27:59,830 --> 00:28:06,280
A through 90 and the problem is that for

00:28:02,380 --> 00:28:08,700
each bird Q we set up an irq of tea and

00:28:06,280 --> 00:28:11,850
we do that to create a mapping between

00:28:08,700 --> 00:28:13,990
the word Q and our adapter interrupt

00:28:11,850 --> 00:28:16,540
indicator bits and some real indicator

00:28:13,990 --> 00:28:19,240
bits so we have a summary indicator

00:28:16,540 --> 00:28:22,180
field here that's a large good field and

00:28:19,240 --> 00:28:26,830
each bit is indicating an adapter that

00:28:22,180 --> 00:28:29,500
we want to inject an end up for notice

00:28:26,830 --> 00:28:32,330
interface requires as to add a routing

00:28:29,500 --> 00:28:34,519
entry for every word q But

00:28:32,330 --> 00:28:36,380
I think oh we're thinking about how we

00:28:34,519 --> 00:28:39,169
could do that differently so of course

00:28:36,380 --> 00:28:43,669
we could just go on and bump up the

00:28:39,169 --> 00:28:48,740
number of pins forever I guess or we

00:28:43,669 --> 00:28:50,240
could like to change the interface or I

00:28:48,740 --> 00:28:54,049
hope we don't have to create a new one

00:28:50,240 --> 00:28:55,940
or something to support a payload where

00:28:54,049 --> 00:28:59,720
we can just and a payload would be just

00:28:55,940 --> 00:29:04,159
an integer basically which is an offset

00:28:59,720 --> 00:29:08,110
to our summary indicators with that

00:29:04,159 --> 00:29:08,110
would not have to set up so many routes

00:29:08,230 --> 00:29:18,019
and I think that was my last slide

00:29:14,620 --> 00:29:20,360
coming to a short conclusion and I would

00:29:18,019 --> 00:29:22,460
like to say that we did not find many

00:29:20,360 --> 00:29:25,370
things that are fundamentally wrong and

00:29:22,460 --> 00:29:28,820
that would prevent the user from running

00:29:25,370 --> 00:29:30,440
many guests simply because I think kvm

00:29:28,820 --> 00:29:34,100
is already in quite a good shape

00:29:30,440 --> 00:29:36,440
regarding scalability I do think it was

00:29:34,100 --> 00:29:39,590
worth as worth doing this effort here

00:29:36,440 --> 00:29:42,740
because one we learned a lot and I'm not

00:29:39,590 --> 00:29:46,130
able to share this here we did fix a few

00:29:42,740 --> 00:29:49,309
things and we have some things to think

00:29:46,130 --> 00:29:52,639
about now so as I said in the beginning

00:29:49,309 --> 00:29:54,139
I won't recommend running four thousand

00:29:52,639 --> 00:29:56,120
guests on your host just because that

00:29:54,139 --> 00:29:57,620
depends on so many things and we have

00:29:56,120 --> 00:30:02,570
more than one petition that you can use

00:29:57,620 --> 00:30:04,100
so but it's nice to know that this

00:30:02,570 --> 00:30:08,929
number is not completely out of this

00:30:04,100 --> 00:30:13,210
world so that being said thank you and

00:30:08,929 --> 00:30:13,210
I'm open for questions asked

00:30:16,310 --> 00:30:23,090
I'm accursed curious that whether you

00:30:19,910 --> 00:30:26,120
have done any comparison between when

00:30:23,090 --> 00:30:28,340
you have many discs in your system and

00:30:26,120 --> 00:30:31,070
the menu carries many dis clin system

00:30:28,340 --> 00:30:33,650
are compared to when you have only one

00:30:31,070 --> 00:30:37,940
disc for example and how does the

00:30:33,650 --> 00:30:41,150
performance look like okay well as I

00:30:37,940 --> 00:30:44,960
said in the beginning we did not I did

00:30:41,150 --> 00:30:48,560
not really look into it didn't we run a

00:30:44,960 --> 00:30:49,970
full performance test I can say what I

00:30:48,560 --> 00:30:52,250
know from my colleagues should learn

00:30:49,970 --> 00:30:55,070
these things they haven't done

00:30:52,250 --> 00:30:57,620
performance tests with 4,000 discs but

00:30:55,070 --> 00:31:02,480
they have done to pharmacists with many

00:30:57,620 --> 00:31:05,030
discs and compared to our other

00:31:02,480 --> 00:31:08,540
solutions with a lot of hype OS and so

00:31:05,030 --> 00:31:10,130
on we look quite good numbers are really

00:31:08,540 --> 00:31:13,540
good especially since we have data plane

00:31:10,130 --> 00:31:16,280
and that really bumped us up quite a bit

00:31:13,540 --> 00:31:19,940
okay thank you I just like to mention

00:31:16,280 --> 00:31:22,610
that because i'm working on improving

00:31:19,940 --> 00:31:25,790
the event loops and so that we don't

00:31:22,610 --> 00:31:28,420
need to create a line i was rad per

00:31:25,790 --> 00:31:33,410
device so we can limit to the number of

00:31:28,420 --> 00:31:35,950
total rat insider one vm which means we

00:31:33,410 --> 00:31:40,640
need to do some some work around the

00:31:35,950 --> 00:31:44,330
pool in cisco and we can use of the ipoh

00:31:40,640 --> 00:31:47,600
interface of the linux are mixed

00:31:44,330 --> 00:31:50,240
e-i-e-i-o in her face more efficient so

00:31:47,600 --> 00:31:53,210
by the way i direct mention that I there

00:31:50,240 --> 00:31:55,100
is also talked about this they are in a

00:31:53,210 --> 00:31:57,470
friday afternoon when if you are

00:31:55,100 --> 00:32:00,920
interested in that you can you're

00:31:57,470 --> 00:32:06,020
welcome to our Comment discuss stuff

00:32:00,920 --> 00:32:07,380
okay sounds great okay okay any other

00:32:06,020 --> 00:32:11,990
questions

00:32:07,380 --> 00:32:11,990

YouTube URL: https://www.youtube.com/watch?v=cj-HLi1q6ZI


