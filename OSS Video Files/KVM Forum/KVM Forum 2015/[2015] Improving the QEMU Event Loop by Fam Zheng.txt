Title: [2015] Improving the QEMU Event Loop by Fam Zheng
Publication date: 2015-09-10
Playlist: KVM Forum 2015
Description: 
	The event loop is the center of QEMU which drives all subsystems in an emulated system, by watching for and dispatching the asynchronous events. Therefore, it affects the programs, capability, performance and efficiency. In this presentation, Fam Zheng will explain how the event based model fits in the program architecture, discuss the challenges of the event loop in the contexts of scalability and data-plane, and look at the relevant work that may address these challenges.

Fam Zheng
Red Hat
Fam Zheng is a developer working in the Red Hat KVM team since 2013. He lives in Beijing, China. As a sub-maintainer (on VMDK and "null" driver) of QEMU, he works in areas including block drivers and block layer, virtio storage devices, data-plane, etc.. Previously, he presented QEMU block subsystem at FOSSASIA 2014 in Phnom Penh, Cambodia.

Slides: https://drive.google.com/open?id=0BzyAwvVlQckeWHplWmdlalE5cU0
Captions: 
	00:00:14,750 --> 00:00:21,600
okay good afternoon my name is fem john

00:00:18,390 --> 00:00:24,300
and i hope you i feel happy happy are

00:00:21,600 --> 00:00:27,510
now looking at the clock I have it's

00:00:24,300 --> 00:00:34,460
actually already saturday hope you had a

00:00:27,510 --> 00:00:38,300
good week so I'm going to talk about

00:00:34,460 --> 00:00:38,300
carinos event loop today

00:00:43,990 --> 00:00:49,480
so this is what I will cover today the

00:00:47,140 --> 00:00:51,880
first half is about the events loops in

00:00:49,480 --> 00:00:53,980
killing your we currently have I will

00:00:51,880 --> 00:00:57,550
give an introduction about how it works

00:00:53,980 --> 00:00:59,680
and what what are there and from there

00:00:57,550 --> 00:01:02,980
we can see of your challenges we are

00:00:59,680 --> 00:01:06,610
facing now including a few aspects such

00:01:02,980 --> 00:01:09,340
as consistency and scalability and the

00:01:06,610 --> 00:01:12,640
correctness and I will show you what the

00:01:09,340 --> 00:01:19,080
real challenges mean and what can we do

00:01:12,640 --> 00:01:22,330
to fix them so carino is the event based

00:01:19,080 --> 00:01:26,229
program that basically means you have a

00:01:22,330 --> 00:01:30,970
loop that dispatches event and for now

00:01:26,229 --> 00:01:35,229
we have several we start started from

00:01:30,970 --> 00:01:38,200
the main loop which are dispatches about

00:01:35,229 --> 00:01:42,060
this create descriptor events and later

00:01:38,200 --> 00:01:46,350
we added more threads into the program

00:01:42,060 --> 00:01:50,320
which EP you were added when kvm is

00:01:46,350 --> 00:01:55,330
enabled and it is responsible for

00:01:50,320 --> 00:01:58,960
running the cast code and this red loss

00:01:55,330 --> 00:02:02,229
of dispatch is what I old events and the

00:01:58,960 --> 00:02:05,020
memory of events the third one iOS rad

00:02:02,229 --> 00:02:07,539
is recently added for the purpose of

00:02:05,020 --> 00:02:10,660
enabling multi-core performance of hosts

00:02:07,539 --> 00:02:13,990
and it it is responsible for running the

00:02:10,660 --> 00:02:17,190
data uplink code so both the main loop

00:02:13,990 --> 00:02:20,260
and the iOS red loop add event loops

00:02:17,190 --> 00:02:23,470
they all work on some D's file

00:02:20,260 --> 00:02:27,990
descriptors and dispatches the event in

00:02:23,470 --> 00:02:30,900
a way that is that has a lot of common

00:02:27,990 --> 00:02:35,319
so let's to cut what is the main loop

00:02:30,900 --> 00:02:38,560
what it does is to dispatch for this

00:02:35,319 --> 00:02:41,440
prevents and some of you done for these

00:02:38,560 --> 00:02:45,100
great services the file descriptors you

00:02:41,440 --> 00:02:50,350
include of four parts one is the aio

00:02:45,100 --> 00:02:52,870
which is the blog layered read write and

00:02:50,350 --> 00:02:57,450
also the i/o event have D which is used

00:02:52,870 --> 00:03:02,340
in the vert vert i/o multi-kill

00:02:57,450 --> 00:03:05,830
this the second 1i o handler this is a

00:03:02,340 --> 00:03:07,989
legacy for these career handlers

00:03:05,830 --> 00:03:12,400
interface we have the insight your email

00:03:07,989 --> 00:03:15,670
and it still works with net subsystem

00:03:12,400 --> 00:03:21,700
and in DD server and all the subsystem

00:03:15,670 --> 00:03:25,450
and div fio and extra extra are the

00:03:21,700 --> 00:03:28,450
first aio and the second io handler has

00:03:25,450 --> 00:03:32,220
quite a few things in common because the

00:03:28,450 --> 00:03:36,519
the all watches for events for certain

00:03:32,220 --> 00:03:39,400
file d square descriptor said with a

00:03:36,519 --> 00:03:42,280
similar interface the third one is a bit

00:03:39,400 --> 00:03:46,930
different because it also belongs to the

00:03:42,280 --> 00:03:51,099
net subsystem but it's particularly used

00:03:46,930 --> 00:03:53,980
in user user space network simulation

00:03:51,099 --> 00:03:57,549
it's called slob it's a library inside

00:03:53,980 --> 00:03:59,739
k'eremu but it it's a very specially

00:03:57,549 --> 00:04:03,250
hooked into the main loop as we will see

00:03:59,739 --> 00:04:09,959
later the first one is a character

00:04:03,250 --> 00:04:19,470
devices it works as very simply as the

00:04:09,959 --> 00:04:19,470
aio one because the apples GD g sources

00:04:19,500 --> 00:04:26,110
that's about how these crimes and we

00:04:23,740 --> 00:04:29,139
have timers and the bottom halves these

00:04:26,110 --> 00:04:34,200
are two specials ap is for internal use

00:04:29,139 --> 00:04:34,200
of the criminal various subsystems

00:04:37,190 --> 00:04:43,970
so with all these elements this is the

00:04:40,070 --> 00:04:47,620
event loop only three steps which is

00:04:43,970 --> 00:04:50,960
simple the first one is a prepare region

00:04:47,620 --> 00:04:54,220
it's also the entry point so in this

00:04:50,960 --> 00:04:57,740
space you do all this last-minute

00:04:54,220 --> 00:05:02,270
preparation of various events and and

00:04:57,740 --> 00:05:05,000
data structure once you are prepared you

00:05:02,270 --> 00:05:07,280
can do a pool the pool basically watches

00:05:05,000 --> 00:05:12,440
for new events from this from the

00:05:07,280 --> 00:05:15,380
colonel so that whenever any i/o happens

00:05:12,440 --> 00:05:19,130
you'll get notified and you go on with

00:05:15,380 --> 00:05:23,090
the event loop into dispatch dispatch

00:05:19,130 --> 00:05:27,050
step in the dispatch tab you will

00:05:23,090 --> 00:05:29,890
dispatch by calling in two different

00:05:27,050 --> 00:05:36,200
callback functions that's registered by

00:05:29,890 --> 00:05:40,250
various subsystems so let's get a little

00:05:36,200 --> 00:05:43,370
bit more details by listing what is

00:05:40,250 --> 00:05:48,200
actually there in each step so the first

00:05:43,370 --> 00:05:51,470
one is preparing to prepare we need to

00:05:48,200 --> 00:05:55,940
fill in the array of how discreet

00:05:51,470 --> 00:05:58,340
descriptors which is useful to our be

00:05:55,940 --> 00:06:01,540
passed into the second one so that is

00:05:58,340 --> 00:06:07,640
what basically slurp and IO handler and

00:06:01,540 --> 00:06:11,419
dilip full toss the other thing they do

00:06:07,640 --> 00:06:15,140
is a timeout timeout is interesting

00:06:11,419 --> 00:06:18,590
because it's also passed into the pool

00:06:15,140 --> 00:06:22,790
function in the second second phase we

00:06:18,590 --> 00:06:25,490
do not do not want to block forever if

00:06:22,790 --> 00:06:27,860
we do not get any new 50 houses critter

00:06:25,490 --> 00:06:31,690
events because we also want to emulate

00:06:27,860 --> 00:06:36,020
the timers so the timeout is basically

00:06:31,690 --> 00:06:39,860
mostly importantly for timers and you

00:06:36,020 --> 00:06:43,010
can see you know first prepare face we

00:06:39,860 --> 00:06:47,350
also have a calculation for time out of

00:06:43,010 --> 00:06:47,350
the main loop timers

00:06:47,759 --> 00:06:56,710
so maybe to depaul we will get a list of

00:06:53,009 --> 00:06:59,439
events from a system each event means

00:06:56,710 --> 00:07:03,189
there are some data activity happening

00:06:59,439 --> 00:07:04,840
here and we should dispatch event so the

00:07:03,189 --> 00:07:07,449
dispatch basically calling to each

00:07:04,840 --> 00:07:12,699
subsystem and each subsystem will take

00:07:07,449 --> 00:07:15,849
care of calling the upper level users by

00:07:12,699 --> 00:07:19,000
us I mean the calling code of each for

00:07:15,849 --> 00:07:21,819
example in instruct called The Dispatch

00:07:19,000 --> 00:07:26,949
would be reading out writing the slurp

00:07:21,819 --> 00:07:29,080
packets so that last step is to dispatch

00:07:26,949 --> 00:07:31,389
all the timers Alzheimer's are

00:07:29,080 --> 00:07:33,310
maintained in your list and this

00:07:31,389 --> 00:07:36,370
function will work through the list of

00:07:33,310 --> 00:07:45,460
timers and the sea witch timers in that

00:07:36,370 --> 00:07:49,240
has expired so let's compare with iOS

00:07:45,460 --> 00:07:54,069
red eye was red is simpler it has a loop

00:07:49,240 --> 00:07:57,610
that looks very short bio poison is an

00:07:54,069 --> 00:08:01,449
interface of the AI old part little bit

00:07:57,610 --> 00:08:05,639
just mentioned so it it does basically

00:08:01,449 --> 00:08:08,199
the same set of actions except for the

00:08:05,639 --> 00:08:14,259
prepare to check dispatch are all

00:08:08,199 --> 00:08:17,500
wrapped in a I'll pool so we very

00:08:14,259 --> 00:08:23,259
quickly realize that the tool to the

00:08:17,500 --> 00:08:26,819
same thing ok so our directly go into

00:08:23,259 --> 00:08:26,819
what challenges are there

00:08:29,370 --> 00:08:38,610
channel one as I mentioned this or

00:08:31,620 --> 00:08:42,240
consistency the I was read thousands you

00:08:38,610 --> 00:08:48,000
know similar fashion as main loop as an

00:08:42,240 --> 00:08:51,900
interface the aio is a basically what is

00:08:48,000 --> 00:08:54,510
only available in your iOS rad but the

00:08:51,900 --> 00:08:58,050
main oppressed a few more scenes that we

00:08:54,510 --> 00:09:03,450
don't care in iOS red but that's the

00:08:58,050 --> 00:09:08,760
something in common and the set of five

00:09:03,450 --> 00:09:14,540
disc roofers we have a we have in iOS

00:09:08,760 --> 00:09:19,020
read a special a builder of array and we

00:09:14,540 --> 00:09:22,529
when we I in May nope we have this G

00:09:19,020 --> 00:09:25,589
leap API from jewelry that are all those

00:09:22,529 --> 00:09:31,700
functions that have all those functions

00:09:25,589 --> 00:09:34,980
of handler that added with me context

00:09:31,700 --> 00:09:38,160
and there are also some synchronization

00:09:34,980 --> 00:09:43,709
semantics that shares basically the same

00:09:38,160 --> 00:09:47,370
idea and the last line is a G source

00:09:43,709 --> 00:09:53,100
support the main approaches are so that

00:09:47,370 --> 00:09:55,709
it can be used by a turtle devices but

00:09:53,100 --> 00:10:03,390
with our thread they are not very

00:09:55,709 --> 00:10:05,640
compatible with that interface so it

00:10:03,390 --> 00:10:08,580
seems that some things are in common the

00:10:05,640 --> 00:10:12,870
some things are not but why should we

00:10:08,580 --> 00:10:15,330
care why should we convert them there

00:10:12,870 --> 00:10:18,120
are mostly through Regents I can think

00:10:15,330 --> 00:10:21,270
of the first one is a because mineral

00:10:18,120 --> 00:10:25,070
pizza hacking mixture of different stuff

00:10:21,270 --> 00:10:28,529
you just say because all those different

00:10:25,070 --> 00:10:32,459
specific functions are hard-coded inside

00:10:28,529 --> 00:10:36,860
the main loop but that's not quite

00:10:32,459 --> 00:10:39,750
important because it is already working

00:10:36,860 --> 00:10:42,450
the second good reason is to reduce the

00:10:39,750 --> 00:10:44,430
code duplication between Johanna and AI

00:10:42,450 --> 00:10:49,470
because they share is actually the same

00:10:44,430 --> 00:10:53,870
interface to support of the different

00:10:49,470 --> 00:10:57,420
subsystems like device emulation and

00:10:53,870 --> 00:11:00,510
things like that to use the use that

00:10:57,420 --> 00:11:04,020
will be different in u.s. us more area

00:11:00,510 --> 00:11:07,860
where I handler has them as a more

00:11:04,020 --> 00:11:11,400
complicated of FD poor staff that is a

00:11:07,860 --> 00:11:14,220
basically a special prepare region but I

00:11:11,400 --> 00:11:21,180
already removed that so what we can do

00:11:14,220 --> 00:11:24,450
is a go further and convert them so the

00:11:21,180 --> 00:11:27,270
last reason is better performance and

00:11:24,450 --> 00:11:31,110
scalability which is what i'd like to

00:11:27,270 --> 00:11:34,560
mention more here so for that we

00:11:31,110 --> 00:11:39,180
strictly strictly go to this second

00:11:34,560 --> 00:11:42,990
challenge scalability because as we know

00:11:39,180 --> 00:11:49,050
the main loop is a hooked up biofuel

00:11:42,990 --> 00:11:53,640
functions as manages each a custom list

00:11:49,050 --> 00:11:58,830
of five dispersed to read it's a

00:11:53,640 --> 00:12:02,030
complicated algorithm compared to what

00:11:58,830 --> 00:12:07,230
they would help when we converge them

00:12:02,030 --> 00:12:12,090
once we have multi q we will have some

00:12:07,230 --> 00:12:17,910
modest prefers like the those used in ir

00:12:12,090 --> 00:12:20,430
qfd and as a I event hefty once we open

00:12:17,910 --> 00:12:23,910
more disks and attached to this guest we

00:12:20,430 --> 00:12:27,840
will have more image of these all these

00:12:23,910 --> 00:12:32,700
add to the slowness of middle because of

00:12:27,840 --> 00:12:35,700
following reasons one is a poor fd's

00:12:32,700 --> 00:12:39,600
fill needs to work through all those

00:12:35,700 --> 00:12:43,310
lists of fds fulfilled in the array that

00:12:39,600 --> 00:12:43,310
we would pass into the people

00:12:44,200 --> 00:12:53,200
and the curial put an s itself which

00:12:50,110 --> 00:12:56,620
basically is a rapper of system called

00:12:53,200 --> 00:13:01,750
people it takes longer because of the

00:12:56,620 --> 00:13:04,660
inherent of the call the reason is that

00:13:01,750 --> 00:13:08,620
we passing your array that answer very

00:13:04,660 --> 00:13:10,330
big lens and the colonel also needs to

00:13:08,620 --> 00:13:16,420
work through the lens works through

00:13:10,330 --> 00:13:19,300
lengthy I read also after we do the

00:13:16,420 --> 00:13:26,860
pulling the dispatch working through

00:13:19,300 --> 00:13:30,010
them them will take more time so as we

00:13:26,860 --> 00:13:32,050
can guess if we add more discs tools to

00:13:30,010 --> 00:13:37,600
virtual machine you can see worse

00:13:32,050 --> 00:13:40,920
performance from the left the proper

00:13:37,600 --> 00:13:45,460
bias read the green bars right and the

00:13:40,920 --> 00:13:48,310
blue one is random read/write if we have

00:13:45,460 --> 00:13:51,820
only a few descriptors it's okay we can

00:13:48,310 --> 00:13:55,930
have a fairly good performance the data

00:13:51,820 --> 00:14:00,970
is about it's very close to 20,000 are

00:13:55,930 --> 00:14:03,310
my testing with a ramdisk and if we go

00:14:00,970 --> 00:14:06,040
to the higher number of skazhi disks

00:14:03,310 --> 00:14:08,590
that attach to the same path which

00:14:06,040 --> 00:14:12,040
basically means we have more about these

00:14:08,590 --> 00:14:16,030
squares inside well the main loop will

00:14:12,040 --> 00:14:23,380
have worse performance the trend is

00:14:16,030 --> 00:14:27,490
mostly linear so it's a problem and now

00:14:23,380 --> 00:14:30,760
we have a iOS read that can move that

00:14:27,490 --> 00:14:33,580
can move some of our disk rest into for

00:14:30,760 --> 00:14:37,300
example if we enable the top lane and

00:14:33,580 --> 00:14:39,670
without blog we can have some we can

00:14:37,300 --> 00:14:42,150
have that set off file disperse move the

00:14:39,670 --> 00:14:46,750
two separate threads so we don't have a

00:14:42,150 --> 00:14:50,140
large set of FAL disperse to jewelry for

00:14:46,750 --> 00:14:55,150
in a single event loop that maybe I'd

00:14:50,140 --> 00:14:56,569
better but it's still not ideal that is

00:14:55,150 --> 00:14:59,459
because

00:14:56,569 --> 00:15:03,870
even though we can have separated threat

00:14:59,459 --> 00:15:06,449
we cannot have infinite wise we want to

00:15:03,870 --> 00:15:10,470
limit the number of threads because we

00:15:06,449 --> 00:15:14,930
don't have so many resources to run all

00:15:10,470 --> 00:15:18,569
this especially with word how scratchy

00:15:14,930 --> 00:15:22,050
virchows Cassidy runs around and as one

00:15:18,569 --> 00:15:26,339
single device model it doesn't have the

00:15:22,050 --> 00:15:29,790
ability to secure across I was right

00:15:26,339 --> 00:15:32,490
even if we now enable the multi q we

00:15:29,790 --> 00:15:40,980
don't have multi-threaded spot for

00:15:32,490 --> 00:15:45,350
molecule so let's do it's a problem so

00:15:40,980 --> 00:15:49,079
that basically means now we have Owen

00:15:45,350 --> 00:15:53,160
performance so the solution is to use

00:15:49,079 --> 00:15:56,339
the Linux native Cisco ippo because it's

00:15:53,160 --> 00:15:59,370
goes well to a large number of watch

00:15:56,339 --> 00:16:06,929
lefties that is what the main page is at

00:15:59,370 --> 00:16:11,519
least the interfaces I like this firstly

00:16:06,929 --> 00:16:15,179
your Creator and PFD with a equal create

00:16:11,519 --> 00:16:18,300
function and then you'll control add of

00:16:15,179 --> 00:16:22,259
more modify or delete file dispersed you

00:16:18,300 --> 00:16:26,069
want to wash into this file in ipoh FD

00:16:22,259 --> 00:16:27,300
and when you are ready you call the

00:16:26,069 --> 00:16:30,569
eeprom weight function which is

00:16:27,300 --> 00:16:34,290
basically the replacement for people and

00:16:30,569 --> 00:16:37,050
this function will wait for the new

00:16:34,290 --> 00:16:41,850
events to arrive for you just as people

00:16:37,050 --> 00:16:46,829
don't cuss but unfortunately the started

00:16:41,850 --> 00:16:51,149
fitting a current mental model because

00:16:46,829 --> 00:16:53,389
the menu pizza have a mixture of my

00:16:51,149 --> 00:16:53,389
other stuff

00:16:55,040 --> 00:17:04,260
but good news is that a i/o interface is

00:16:58,620 --> 00:17:07,800
very similar in aio you create an AI or

00:17:04,260 --> 00:17:11,010
context and your car aio cfd handler to

00:17:07,800 --> 00:17:22,080
add or remove of this purse to this AI

00:17:11,010 --> 00:17:27,000
or context so that's what we can do we

00:17:22,080 --> 00:17:33,110
can convert the aio version api to make

00:17:27,000 --> 00:17:33,110
it use the ecosystem

00:17:37,500 --> 00:17:45,910
okay so here's the idea of the new

00:17:40,540 --> 00:17:48,940
implementation in the aio stat have the

00:17:45,910 --> 00:17:52,630
handler where you insert this an

00:17:48,940 --> 00:17:56,410
interesting FD in we are watched set of

00:17:52,630 --> 00:18:00,700
ftes you can just call the people

00:17:56,410 --> 00:18:03,880
control to update the PFD and enjoy a

00:18:00,700 --> 00:18:08,080
i/o port interface you can just call

00:18:03,880 --> 00:18:11,590
April late the idea is pretty simple and

00:18:08,080 --> 00:18:15,220
it's a low-hanging fruit I've already

00:18:11,590 --> 00:18:18,940
done this and standard patches to carry

00:18:15,220 --> 00:18:23,440
more upstream list which basically shows

00:18:18,940 --> 00:18:27,460
the pride promising results knowledge

00:18:23,440 --> 00:18:34,480
three charts are comparing read write

00:18:27,460 --> 00:18:36,760
and random read/write on iOS thread so

00:18:34,480 --> 00:18:38,530
it has to be advertised Cassie I was red

00:18:36,760 --> 00:18:42,400
because we don't have multiple disks

00:18:38,530 --> 00:18:44,440
time what have a plot point and this is

00:18:42,400 --> 00:18:48,870
easier because I don't need to create

00:18:44,440 --> 00:18:52,000
multiple devices to handle multiple

00:18:48,870 --> 00:18:56,500
disks I only need to attach a grassy

00:18:52,000 --> 00:18:59,890
disk to us to a single pass but the case

00:18:56,500 --> 00:19:01,950
for what how blood data plane would be

00:18:59,890 --> 00:19:07,080
the same you can see that the

00:19:01,950 --> 00:19:07,080
improvements have pretty significant

00:19:09,940 --> 00:19:19,910
okay but there is a another problem

00:19:13,630 --> 00:19:23,720
introduced by this the depot weights

00:19:19,910 --> 00:19:26,720
function is not quite on par about the

00:19:23,720 --> 00:19:29,630
time out of the people because the

00:19:26,720 --> 00:19:33,380
granularity the precision is lower it

00:19:29,630 --> 00:19:35,480
only supports a millisecond and in

00:19:33,380 --> 00:19:40,280
people you can have a times back rich

00:19:35,480 --> 00:19:44,560
has nanosecond that's important we don't

00:19:40,280 --> 00:19:47,510
want to just run during that time out to

00:19:44,560 --> 00:19:50,900
adapt to this people wait because that

00:19:47,510 --> 00:19:59,150
would hurt the timer's the timers needed

00:19:50,900 --> 00:20:02,420
precise time out and dispatch so it can

00:19:59,150 --> 00:20:09,100
be worked around without introducing a

00:20:02,420 --> 00:20:09,100
new brand new equal weight to cisco

00:20:11,080 --> 00:20:19,670
that's a simple approach as well by

00:20:15,020 --> 00:20:22,340
using the time RFD well it's an all

00:20:19,670 --> 00:20:25,910
well-known interface i won't go into

00:20:22,340 --> 00:20:29,510
each explain it but we can add a primary

00:20:25,910 --> 00:20:33,560
FD into the e po ft so that for each

00:20:29,510 --> 00:20:36,500
people wait whenever the timer FD has

00:20:33,560 --> 00:20:38,240
fired will we will see the return so

00:20:36,500 --> 00:20:43,690
that we don't need to do anything about

00:20:38,240 --> 00:20:43,690
the timer FD a time out of equate

00:20:48,980 --> 00:20:55,880
but from previous results where I

00:20:53,120 --> 00:20:59,059
converted the scenes in the data plane I

00:20:55,880 --> 00:21:03,559
was red we don't need this yet because

00:20:59,059 --> 00:21:08,120
we don't have any time timers inside the

00:21:03,559 --> 00:21:12,250
data please read everything is attached

00:21:08,120 --> 00:21:16,640
to the main loop so this is for future

00:21:12,250 --> 00:21:22,190
if we want to move people in two min

00:21:16,640 --> 00:21:24,590
dope as well and we will see it now so

00:21:22,190 --> 00:21:29,410
the main loop is the has more stuff

00:21:24,590 --> 00:21:32,770
there how can we make use of people

00:21:29,410 --> 00:21:36,919
basically we can't reach the right code

00:21:32,770 --> 00:21:41,780
but we can change it we can move the i/o

00:21:36,919 --> 00:21:43,970
of other other things on top of a aio we

00:21:41,780 --> 00:21:48,020
can move the i/o handler we can move the

00:21:43,970 --> 00:21:52,960
slurp and also the Jesus we just need to

00:21:48,020 --> 00:21:57,020
make the i/o interface more powerful so

00:21:52,960 --> 00:21:59,570
now the mission is a sign we need to

00:21:57,020 --> 00:22:02,120
resolve the challenge number one so that

00:21:59,570 --> 00:22:04,130
they are the same and we can use a i/o

00:22:02,120 --> 00:22:10,429
interface envelope and the week at the

00:22:04,130 --> 00:22:14,179
depot performance so these are concrete

00:22:10,429 --> 00:22:16,669
steps one is to make the aisle hundred

00:22:14,179 --> 00:22:19,280
interface consistent with the yellow

00:22:16,669 --> 00:22:21,919
interface as I said they used it would

00:22:19,280 --> 00:22:26,330
be different and it's hard to put a

00:22:21,919 --> 00:22:30,340
handler onto a oh now they are the same

00:22:26,330 --> 00:22:36,190
and we can make the conversion our

00:22:30,340 --> 00:22:36,190
handler we have a slurp and we have a

00:22:37,690 --> 00:22:47,840
slope is the is also a hack we don't

00:22:44,630 --> 00:22:53,270
like that so we can try to change it

00:22:47,840 --> 00:22:56,799
there are patches posted but I felt to

00:22:53,270 --> 00:22:56,799
make it into 2.4

00:22:58,950 --> 00:23:16,090
yeah yeah I was just saying that so

00:23:12,940 --> 00:23:21,550
let's just make this info 2.5 with every

00:23:16,090 --> 00:23:23,590
sin every good things out together and

00:23:21,550 --> 00:23:25,960
after that we can look into the

00:23:23,590 --> 00:23:30,430
character devices they are special

00:23:25,960 --> 00:23:34,090
because it was basically a special

00:23:30,430 --> 00:23:37,660
cheese sauce and actual warranties of TI

00:23:34,090 --> 00:23:40,990
o interface but that's doable with a

00:23:37,660 --> 00:23:42,760
slight change in the API and maybe

00:23:40,990 --> 00:23:46,150
introducing more staff that supports the

00:23:42,760 --> 00:23:51,850
proposition they need to have this is to

00:23:46,150 --> 00:23:56,560
be done so hopefully we can eventually

00:23:51,850 --> 00:24:00,400
put this stuff into this trip so that

00:23:56,560 --> 00:24:05,380
everything else runs right on top of AIO

00:24:00,400 --> 00:24:12,430
and aio is a used to buy my loop so that

00:24:05,380 --> 00:24:13,990
the menu has a better performance this

00:24:12,430 --> 00:24:17,560
also means that the minimum doesn't have

00:24:13,990 --> 00:24:20,850
to do it on steps of the whole event

00:24:17,560 --> 00:24:27,520
loop it just cause they AI hope for like

00:24:20,850 --> 00:24:31,630
like in the iOS ran so the difference in

00:24:27,520 --> 00:24:33,970
the bottom is that if we only convert

00:24:31,630 --> 00:24:37,840
since on top of the i/o we would still

00:24:33,970 --> 00:24:39,790
need people but later we just limit

00:24:37,840 --> 00:24:44,050
everything in my loop you can have the

00:24:39,790 --> 00:24:49,270
people okay that's about it I'll go into

00:24:44,050 --> 00:24:51,250
channel 23 this one is different so I

00:24:49,270 --> 00:24:55,870
I'd like to explain your few more staff

00:24:51,250 --> 00:24:59,940
here the nested event loops this is a

00:24:55,870 --> 00:25:04,240
special structure quite often used in

00:24:59,940 --> 00:25:07,020
the block layer because in the bottom of

00:25:04,240 --> 00:25:09,180
black layer we implement I always

00:25:07,020 --> 00:25:14,940
asynchronous i/o since

00:25:09,180 --> 00:25:17,910
now we do it like providing to set of

00:25:14,940 --> 00:25:20,640
interfaces one is a a synchronous once

00:25:17,910 --> 00:25:23,970
why is a synchronous once we don't want

00:25:20,640 --> 00:25:29,970
to duplicate too much so the nested you

00:25:23,970 --> 00:25:33,390
want to loop segmented this is a glam

00:25:29,970 --> 00:25:36,510
hole that very well demo streets this

00:25:33,390 --> 00:25:38,460
case we have the AI or cancel here which

00:25:36,510 --> 00:25:42,300
is a synchronous version don't be

00:25:38,460 --> 00:25:45,560
confused by the name and we have a AI or

00:25:42,300 --> 00:25:49,650
cancel async there which does the

00:25:45,560 --> 00:25:51,720
trigger in the cancellation but it's not

00:25:49,650 --> 00:25:54,840
finished when this function returns so

00:25:51,720 --> 00:25:58,260
we need to wait with by doing a nested

00:25:54,840 --> 00:26:00,600
event loop so the net the nest eventual

00:25:58,260 --> 00:26:07,590
would listen for the completion or the

00:26:00,600 --> 00:26:10,200
cancellation to succeed other cancel

00:26:07,590 --> 00:26:13,290
there are a bunch of other examples of

00:26:10,200 --> 00:26:18,090
where nested event loops i use basically

00:26:13,290 --> 00:26:23,790
all these interfaces are synchronous but

00:26:18,090 --> 00:26:26,310
that hurts this is an example of varied

00:26:23,790 --> 00:26:30,180
actually used in real life this is a

00:26:26,310 --> 00:26:33,660
monitor command that does drive back up

00:26:30,180 --> 00:26:36,240
its dispatched in the bottom by the main

00:26:33,660 --> 00:26:39,450
loop and litter it went into the monitor

00:26:36,240 --> 00:26:41,010
code to pass their command and the

00:26:39,450 --> 00:26:43,860
monitor code was according to the

00:26:41,010 --> 00:26:50,010
callback of drive back up and drive back

00:26:43,860 --> 00:26:52,020
up would start to create the image where

00:26:50,010 --> 00:26:58,110
is a creation of the images or

00:26:52,020 --> 00:27:03,360
synchronous operation so that leads to a

00:26:58,110 --> 00:27:07,860
cracked knees problem i will explain it

00:27:03,360 --> 00:27:10,470
with an example this is in the ump

00:27:07,860 --> 00:27:13,350
transaction where you want to back up

00:27:10,470 --> 00:27:16,770
two discs at the same time without being

00:27:13,350 --> 00:27:19,340
interrupted by any guests i oh so that

00:27:16,770 --> 00:27:22,390
the two backups would be consistent and

00:27:19,340 --> 00:27:25,810
from exactly the same

00:27:22,390 --> 00:27:29,290
point of time so process goes likely is

00:27:25,810 --> 00:27:32,800
the first one okay even though there is

00:27:29,290 --> 00:27:39,870
our creation there is a event in nested

00:27:32,800 --> 00:27:43,890
event loop it's okay but if this one

00:27:39,870 --> 00:27:48,640
this point after the first one has done

00:27:43,890 --> 00:27:51,130
the guest rights to a to a device the

00:27:48,640 --> 00:27:52,840
hour event healthy will be notified the

00:27:51,130 --> 00:27:54,970
notification will be processed by

00:27:52,840 --> 00:27:58,120
further nested event loops are top-level

00:27:54,970 --> 00:28:00,760
event loops whatever it is so

00:27:58,120 --> 00:28:03,790
unfortunately we still are not done yet

00:28:00,760 --> 00:28:05,920
so we have another nested event loop

00:28:03,790 --> 00:28:08,920
which dispatch is IO event have the

00:28:05,920 --> 00:28:11,350
event which means we get a new request

00:28:08,920 --> 00:28:15,610
that is processed and change the state

00:28:11,350 --> 00:28:20,250
of image true so the solution is to add

00:28:15,610 --> 00:28:20,250
an interface to disable and enable

00:28:21,270 --> 00:28:29,650
around these critical sections of

00:28:25,000 --> 00:28:32,080
operations so we want to assume clued

00:28:29,650 --> 00:28:35,260
the i/o event have these in nested aio

00:28:32,080 --> 00:28:38,290
for the interfaces symbolize that ail

00:28:35,260 --> 00:28:41,500
client disabled and we specify what we

00:28:38,290 --> 00:28:46,390
want to disable and after we're done we

00:28:41,500 --> 00:28:49,000
nibble it so that is a in the middle

00:28:46,390 --> 00:28:51,640
it's a scheme of the acuity transaction

00:28:49,000 --> 00:28:55,600
there are multiple operations to do

00:28:51,640 --> 00:28:59,050
preparation and when every every element

00:28:55,600 --> 00:29:02,110
of the transaction exceeded we were

00:28:59,050 --> 00:29:05,200
committed so what we should do is to

00:29:02,110 --> 00:29:07,330
before we do the actual preparation they

00:29:05,200 --> 00:29:09,880
should disable that I we might have T

00:29:07,330 --> 00:29:12,550
which is Theta print here and only after

00:29:09,880 --> 00:29:17,680
we finish everything we can renewable

00:29:12,550 --> 00:29:20,910
the processing of caster requests so

00:29:17,680 --> 00:29:24,070
there are also patches on the list

00:29:20,910 --> 00:29:26,860
unfortunately we didn't make into 2.4

00:29:24,070 --> 00:29:30,870
but I will make sure that we get this

00:29:26,860 --> 00:29:30,870
quick fix quickly

00:29:31,470 --> 00:29:42,960
so the solution 3 is this and that's

00:29:37,990 --> 00:29:42,960
about it any questions

00:29:50,030 --> 00:30:00,150
so I had one question about the part

00:29:55,620 --> 00:30:02,070
about timers and evil I saw that there

00:30:00,150 --> 00:30:06,030
were some discussion and patches about

00:30:02,070 --> 00:30:08,280
extending the evil system call yeah but

00:30:06,030 --> 00:30:09,570
I don't know what happened can you kind

00:30:08,280 --> 00:30:11,100
of explain that and give us a status

00:30:09,570 --> 00:30:13,760
update whether there will be a new

00:30:11,100 --> 00:30:17,580
ecosystem colin linux that supports

00:30:13,760 --> 00:30:19,770
nanosecond yeah yeah i think that the

00:30:17,580 --> 00:30:24,150
necessary of that actually depends on

00:30:19,770 --> 00:30:26,610
the benchmarking of applications because

00:30:24,150 --> 00:30:29,240
we already have this facility of time r

00:30:26,610 --> 00:30:33,090
FD and the weekend implements the

00:30:29,240 --> 00:30:36,090
nanosecond precision so the maintainer

00:30:33,090 --> 00:30:38,070
would ask for real real benefits of a

00:30:36,090 --> 00:30:40,620
new system called Casa by adding more

00:30:38,070 --> 00:30:44,060
stuff into the colonel actually the

00:30:40,620 --> 00:30:48,330
current status is that I'm trying to

00:30:44,060 --> 00:30:50,210
find a real use of that inside cure in

00:30:48,330 --> 00:30:55,320
your code base so we can't demonstrate

00:30:50,210 --> 00:30:57,740
whether or not it's necessary okay

00:30:55,320 --> 00:30:57,740
thanks

00:31:14,520 --> 00:31:19,800
on one slide you mentioned that you were

00:31:17,400 --> 00:31:23,760
considering or preparing to move more

00:31:19,800 --> 00:31:27,180
code from the main loop into the aio

00:31:23,760 --> 00:31:28,950
code I'm not too familiar with that code

00:31:27,180 --> 00:31:32,730
so one thought that crossed my mind was

00:31:28,950 --> 00:31:33,960
wasn't the aio code always optional so

00:31:32,730 --> 00:31:36,840
you could turn and actually often

00:31:33,960 --> 00:31:39,480
configure also obviously epoll would not

00:31:36,840 --> 00:31:41,610
be available unlike bsd mac OS other

00:31:39,480 --> 00:31:43,230
systems is that still being tested that

00:31:41,610 --> 00:31:46,410
that actually still keeps working when

00:31:43,230 --> 00:31:49,920
you do the Ebola stuff the aio day out

00:31:46,410 --> 00:31:52,650
here is the day I own contacts which is

00:31:49,920 --> 00:31:56,010
basically the event loop structure our

00:31:52,650 --> 00:31:59,220
object of I all I was rad our main hope

00:31:56,010 --> 00:32:04,200
and the aio inside the block layer is

00:31:59,220 --> 00:32:07,110
how we actually process the request and

00:32:04,200 --> 00:32:08,960
stopping to request inside broccoli or

00:32:07,110 --> 00:32:13,010
so there are two different scenes

00:32:08,960 --> 00:32:13,010
peculiar yeah

00:32:13,970 --> 00:32:30,290
yeah my experience says with any sleep

00:32:26,620 --> 00:32:32,660
will not have another seconds precision

00:32:30,290 --> 00:32:35,420
you will stick to milliseconds or even

00:32:32,660 --> 00:32:38,420
10 milliseconds so we have spent a bit

00:32:35,420 --> 00:32:43,550
of time in previous life and even if you

00:32:38,420 --> 00:32:46,940
have to ensure timely interrupt into the

00:32:43,550 --> 00:32:48,950
guest we do this in advance if you have

00:32:46,940 --> 00:32:52,630
a gap for two or three milliseconds so

00:32:48,950 --> 00:32:56,630
maybe listen Eric you adjust to sit

00:32:52,630 --> 00:32:59,060
there going to happen just one

00:32:56,630 --> 00:33:03,040
millisecond before something like wait I

00:32:59,060 --> 00:33:08,420
don't mean we can run run down the

00:33:03,040 --> 00:33:12,410
timeout if you will set them out in

00:33:08,420 --> 00:33:15,370
nanoseconds you will not receive a

00:33:12,410 --> 00:33:18,280
response you will not be woken up in

00:33:15,370 --> 00:33:21,530
nanoseconds Precision's you will be

00:33:18,280 --> 00:33:29,180
working up there's 10 milliseconds or

00:33:21,530 --> 00:33:32,150
something like that if you will set a

00:33:29,180 --> 00:33:37,900
time lock you will not receive event in

00:33:32,150 --> 00:33:42,580
time even if you will have specified

00:33:37,900 --> 00:33:46,430
linux kill this terrible at least him

00:33:42,580 --> 00:33:51,550
it's just maybe my own experience but

00:33:46,430 --> 00:33:51,550
it's my feeling okay thank you

00:33:52,530 --> 00:33:57,430
going slightly beyond the main topic of

00:33:55,450 --> 00:34:00,430
your presentation as far as the event

00:33:57,430 --> 00:34:03,790
loop goes sometimes when running Hugh

00:34:00,430 --> 00:34:06,700
tests or also under certain conditions

00:34:03,790 --> 00:34:09,220
running the decay anger process we've

00:34:06,700 --> 00:34:11,500
been seeing this warning that Anthony

00:34:09,220 --> 00:34:13,540
once introduced about the main loop

00:34:11,500 --> 00:34:16,330
spinner young thousand iterations or

00:34:13,540 --> 00:34:19,120
whatever without whatever at which point

00:34:16,330 --> 00:34:21,460
exactly does that warning come from and

00:34:19,120 --> 00:34:23,740
do you have any idea how we can actually

00:34:21,460 --> 00:34:27,310
get rid of that that one in basically

00:34:23,740 --> 00:34:30,040
means that we are catching a continuous

00:34:27,310 --> 00:34:33,130
events so that we spin inside the event

00:34:30,040 --> 00:34:34,720
loop and we don't yell to the visitors

00:34:33,130 --> 00:34:39,100
ready to take the locker and the

00:34:34,720 --> 00:34:40,720
dispatch it's Pio MMO events so we we

00:34:39,100 --> 00:34:44,410
are concerned that this would hurt the

00:34:40,720 --> 00:34:47,740
responsibility a responsiveness offer of

00:34:44,410 --> 00:34:49,480
the guests so we printed a warning they

00:34:47,740 --> 00:34:53,260
are partying few tests i don't think

00:34:49,480 --> 00:34:56,080
that hurts and if in real applications

00:34:53,260 --> 00:35:00,880
there the one is printed i can say that

00:34:56,080 --> 00:35:03,730
those the responsiveness offer guests at

00:35:00,880 --> 00:35:07,090
that point would be down okay but

00:35:03,730 --> 00:35:09,580
technically does that result from the

00:35:07,090 --> 00:35:11,530
main loop actually processing events or

00:35:09,580 --> 00:35:16,650
is that when the timeout kicks in i

00:35:11,530 --> 00:35:16,650
think that the processing events

00:35:21,920 --> 00:35:25,780

YouTube URL: https://www.youtube.com/watch?v=sX5vAPUDJVU


