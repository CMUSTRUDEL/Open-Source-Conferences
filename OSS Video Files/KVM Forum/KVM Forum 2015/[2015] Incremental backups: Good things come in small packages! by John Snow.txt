Title: [2015] Incremental backups: Good things come in small packages! by John Snow
Publication date: 2015-08-27
Playlist: KVM Forum 2015
Description: 
	Full backups of large storage devices are expensive, slow, and waste a lot of space needlessly by copying sectors that have not changed over and over again. Incremental and differential backups are an oft requested feature in QEMU, and will help eliminate the redundant copying of backup data. This presentation will cover recent developments in related delta-backup technologies, covering incremental and differential backups, image fleecing, and dirty bitmap management. We will highlight how these features are accomplished using modifications to existing QMP primitives such as the 'drive-backup' and 'transaction' commands to unlock rich functionality within our existing APIs. 

Vladimir Sementsov-Ogievskiy

Vladimir Sementsov-Ogievskiy is a postgraduate student of Moscow Institute of Physics and Technology. He have received a master's degree at Applied Mathematics and Physics in 2014 in the same institute, with scientific work about virtual machine testing. Now he deals with virtual machine backup for Qemu and Virtuozzo (Parallels Cloud Server). Vladimir works for two years in Paralles (now Odin) as a Jr. software developer. Vladimir lives in Moscow, Russia. 

John Snow
QEMU Software Engineer, Red Hat

I'm a recent UMass Lowell CS graduate and a recent hire for Red Hat. I work for the virtualization team as a Software Engineer and have been working on AHCI device support for the Q35 chipset emulation, improving our qtest and iotest frameworks, and most recently incremental backup and image fleecing support through the QMP management interface.
Captions: 
	00:00:17,150 --> 00:00:23,369
member

00:00:19,760 --> 00:00:25,529
okay yeah so my name is Jon Snow and

00:00:23,369 --> 00:00:27,150
yeah yeah yeah I know

00:00:25,529 --> 00:00:28,760
I'm software engineer for Red Hat I've

00:00:27,150 --> 00:00:31,920
been working on the incremental backups

00:00:28,760 --> 00:00:34,920
my would-be co-presenter of vladimir

00:00:31,920 --> 00:00:37,500
simonov oginski couldn't be here today

00:00:34,920 --> 00:00:42,390
unfortunately so I'll be covering his

00:00:37,500 --> 00:00:46,320
portion towards the end so before again

00:00:42,390 --> 00:00:48,780
begin just a couple of acknowledgment so

00:00:46,320 --> 00:00:50,820
this feature kind of has been through

00:00:48,780 --> 00:00:53,940
the washing machine for a few cycles and

00:00:50,820 --> 00:00:57,360
it was initially proposed by a jig and

00:00:53,940 --> 00:00:59,160
sundar in 2011 and his work kind of

00:00:57,360 --> 00:01:01,649
informed some of the directions we've

00:00:59,160 --> 00:01:05,369
gone since then so I wanted to put him

00:01:01,649 --> 00:01:08,340
on the slide the features that exist

00:01:05,369 --> 00:01:12,360
today came from fan Jang's patches a

00:01:08,340 --> 00:01:14,820
year or so ago and I'd like to thank

00:01:12,360 --> 00:01:20,580
Stefan and Max for mostly the reviews

00:01:14,820 --> 00:01:22,650
and the patience so for an overview talk

00:01:20,580 --> 00:01:24,869
about what the problem is they approach

00:01:22,650 --> 00:01:27,740
the design goals then I'd like to

00:01:24,869 --> 00:01:31,020
discuss some of the building blocks for

00:01:27,740 --> 00:01:33,360
the feature before getting to the

00:01:31,020 --> 00:01:35,280
lifecycle and example of how to use the

00:01:33,360 --> 00:01:38,820
feature to accomplish incremental

00:01:35,280 --> 00:01:40,860
backups and then towards the end I would

00:01:38,820 --> 00:01:42,420
like to discuss some of the advanced

00:01:40,860 --> 00:01:43,890
features which Vladimir has been working

00:01:42,420 --> 00:01:47,009
on which is the migration the

00:01:43,890 --> 00:01:49,710
persistence and some of the the

00:01:47,009 --> 00:01:54,000
interface for the the functionality and

00:01:49,710 --> 00:01:57,020
at the end some project status questions

00:01:54,000 --> 00:01:57,020
and answers

00:02:00,510 --> 00:02:07,770
so the problem if you make a first

00:02:04,619 --> 00:02:09,630
backup and its 128 gigs and you need to

00:02:07,770 --> 00:02:12,990
keep making backups throughout the week

00:02:09,630 --> 00:02:16,290
and it's 128 gigs again and again and

00:02:12,990 --> 00:02:19,380
again and again regardless of how much

00:02:16,290 --> 00:02:20,120
data has changed on the disk its kind of

00:02:19,380 --> 00:02:22,709
gross

00:02:20,120 --> 00:02:25,910
there's a terrible storage if isn't

00:02:22,709 --> 00:02:28,770
efficient see here it's clunky it's slow

00:02:25,910 --> 00:02:30,660
but it is somewhat simple and convenient

00:02:28,770 --> 00:02:33,930
so it might be the preferred solution

00:02:30,660 --> 00:02:35,610
for people as it stands now because it's

00:02:33,930 --> 00:02:39,690
very easy to roll back to a particular

00:02:35,610 --> 00:02:43,140
day not a lot of complexity so what we'd

00:02:39,690 --> 00:02:44,700
like to do is even though we still have

00:02:43,140 --> 00:02:46,470
to make a full backup for the first day

00:02:44,700 --> 00:02:49,110
of the week for instance it would be

00:02:46,470 --> 00:02:52,260
nice if subsequently we can just copy

00:02:49,110 --> 00:02:55,410
out just the data that has changed much

00:02:52,260 --> 00:02:57,120
much better only copies the modified

00:02:55,410 --> 00:02:59,100
data and it's fast we're only copying

00:02:57,120 --> 00:03:02,160
out just a little bit so you can copy

00:02:59,100 --> 00:03:03,810
more data but maybe it's more

00:03:02,160 --> 00:03:07,260
complicated I would like to convince you

00:03:03,810 --> 00:03:10,739
that that's not the case so the approach

00:03:07,260 --> 00:03:12,930
as mentioned this came from or was

00:03:10,739 --> 00:03:15,630
inspired by Jagan Sundar's live backup

00:03:12,930 --> 00:03:18,570
in 2011 he gave a talk on it at the KVM

00:03:15,630 --> 00:03:21,450
forum back then his approach used

00:03:18,570 --> 00:03:23,730
separate CLI tools to manage the backup

00:03:21,450 --> 00:03:26,579
data so there was a server and a client

00:03:23,730 --> 00:03:29,489
that you had to run to copy the data out

00:03:26,579 --> 00:03:32,910
of key knew it used an entirely new

00:03:29,489 --> 00:03:35,730
network protocol for the feature it ran

00:03:32,910 --> 00:03:37,590
as an independent thread outside of the

00:03:35,730 --> 00:03:40,769
existing key new infrastructure so there

00:03:37,590 --> 00:03:44,820
was some concerns perhaps about the data

00:03:40,769 --> 00:03:47,190
safety it used temporary snapshots to

00:03:44,820 --> 00:03:49,920
manage the the atomicity of the feature

00:03:47,190 --> 00:03:51,269
and it was implemented with in-memory

00:03:49,920 --> 00:03:54,720
dirty block bitmaps

00:03:51,269 --> 00:03:59,010
it was ultimately not merged after he

00:03:54,720 --> 00:04:01,290
moved on to other projects so in 2014 I

00:03:59,010 --> 00:04:04,109
think is when van started working on on

00:04:01,290 --> 00:04:07,180
his version and it is also a dirty

00:04:04,109 --> 00:04:10,090
sector bitmap based solution

00:04:07,180 --> 00:04:11,830
we're using the H fit map and B Drive

00:04:10,090 --> 00:04:14,230
dirty bitmap primitives that already

00:04:11,830 --> 00:04:17,019
existed in Hakeem you we didn't create

00:04:14,230 --> 00:04:19,780
any new external tooling or protocols

00:04:17,019 --> 00:04:24,040
it's managed via the existing qmp

00:04:19,780 --> 00:04:25,540
interface we implemented this as just a

00:04:24,040 --> 00:04:27,580
new backup mode so there's not a

00:04:25,540 --> 00:04:31,960
plethora of new commands or management

00:04:27,580 --> 00:04:33,790
burden here we can use it with any image

00:04:31,960 --> 00:04:36,820
format it's not tied to a particular

00:04:33,790 --> 00:04:38,320
format though some are nicer to use than

00:04:36,820 --> 00:04:40,449
others

00:04:38,320 --> 00:04:42,699
and we are hoping that this maximizes

00:04:40,449 --> 00:04:47,650
the compatibility with existing backup

00:04:42,699 --> 00:04:51,310
tools infrastructure frameworks so so

00:04:47,650 --> 00:04:53,530
design goals we wanted to reuse as much

00:04:51,310 --> 00:04:56,440
as humanly possible as mentioned to

00:04:53,530 --> 00:04:59,830
hopefully streamline this into existing

00:04:56,440 --> 00:05:02,020
customer solutions so we already have

00:04:59,830 --> 00:05:03,910
the block driver dirty bitmap it was

00:05:02,020 --> 00:05:07,120
already tracking dirty sectors for us

00:05:03,910 --> 00:05:09,190
for Drive marrying and migration it

00:05:07,120 --> 00:05:12,010
already had a configurable granularity

00:05:09,190 --> 00:05:15,550
and we can attach as many of these as we

00:05:12,010 --> 00:05:17,200
need to per drive so by using these it

00:05:15,550 --> 00:05:19,270
doesn't interfere with for instance

00:05:17,200 --> 00:05:23,979
mirroring or migration anything like

00:05:19,270 --> 00:05:28,030
that we are reusing the drive back up

00:05:23,979 --> 00:05:30,880
command which is implemented via qmp it

00:05:28,030 --> 00:05:33,970
can already create full backups or point

00:05:30,880 --> 00:05:36,699
in time live backups we can already

00:05:33,970 --> 00:05:38,830
export data via an existing protocol NBD

00:05:36,699 --> 00:05:41,979
we don't have to invent any new tooling

00:05:38,830 --> 00:05:44,650
or protocols there all we have to do is

00:05:41,979 --> 00:05:47,470
add a new incremental sync mode and

00:05:44,650 --> 00:05:50,699
maybe a bitmap named argument so that we

00:05:47,470 --> 00:05:54,580
can instruct the backup framework to

00:05:50,699 --> 00:05:57,160
know what data to copy

00:05:54,580 --> 00:05:59,710
so for coherency as a design goal we

00:05:57,160 --> 00:06:03,400
want it to be able to do multi Drive

00:05:59,710 --> 00:06:07,389
point in time backup you know across

00:06:03,400 --> 00:06:09,400
multiple drives we wanted to use the qmp

00:06:07,389 --> 00:06:12,940
transaction feature to accomplish this

00:06:09,400 --> 00:06:14,400
instead of cooking up a new complicated

00:06:12,940 --> 00:06:17,580
solution

00:06:14,400 --> 00:06:20,010
we want this feature to be persistent

00:06:17,580 --> 00:06:22,680
so the bitmaps have to survive shutdowns

00:06:20,010 --> 00:06:25,830
and reboots and again we didn't want

00:06:22,680 --> 00:06:29,310
this to depend on any particular data

00:06:25,830 --> 00:06:31,020
format or the the target data format so

00:06:29,310 --> 00:06:35,030
we wanted this to remain a flexible

00:06:31,020 --> 00:06:38,550
solution it needs to be migration safe

00:06:35,030 --> 00:06:40,470
so migrating must not lose or reset

00:06:38,550 --> 00:06:43,620
bitmap data because that would make it

00:06:40,470 --> 00:06:45,000
not super useful cloud usage of course

00:06:43,620 --> 00:06:47,520
we want to be able to shuffle these

00:06:45,000 --> 00:06:52,400
machines around so migration needs to

00:06:47,520 --> 00:06:55,770
remain working for us for error handling

00:06:52,400 --> 00:06:57,330
we don't want to lose any of the data on

00:06:55,770 --> 00:07:00,360
backup failure which I know sounds

00:06:57,330 --> 00:07:03,150
obvious but a design goal here is that

00:07:00,360 --> 00:07:05,070
if a incremental backup fails that you

00:07:03,150 --> 00:07:08,220
run out of storage or there's some kind

00:07:05,070 --> 00:07:10,890
of protocol error we lose the ability to

00:07:08,220 --> 00:07:13,440
create the backup as we would in that

00:07:10,890 --> 00:07:15,900
point of time but you can just start a

00:07:13,440 --> 00:07:18,000
new incremental immediately afterwards

00:07:15,900 --> 00:07:20,610
and you can get an incremental from that

00:07:18,000 --> 00:07:22,740
point in time if a backup fails you

00:07:20,610 --> 00:07:26,550
don't have to start all the way back

00:07:22,740 --> 00:07:29,280
with a new full backup and for the

00:07:26,550 --> 00:07:32,130
integrity we want to make sure that we

00:07:29,280 --> 00:07:34,740
can at least detect any desync between

00:07:32,130 --> 00:07:38,310
the persistence data and the bitmap as

00:07:34,740 --> 00:07:40,380
it exists we don't want to be copying

00:07:38,310 --> 00:07:45,150
any incremental backups out that are

00:07:40,380 --> 00:07:47,310
garbage so this was going to come up

00:07:45,150 --> 00:07:50,160
sooner or later it's come up a few times

00:07:47,310 --> 00:07:54,110
on lists people ask why don't you use

00:07:50,160 --> 00:07:56,310
snapshots we already have point in time

00:07:54,110 --> 00:07:58,760
functionality that people are using to

00:07:56,310 --> 00:08:03,120
rollback to specific points in time and

00:07:58,760 --> 00:08:07,440
they look well why not use that well we

00:08:03,120 --> 00:08:09,120
can use the feature as described to free

00:08:07,440 --> 00:08:11,970
ourselves from a lot of format

00:08:09,120 --> 00:08:14,160
dependency so we don't care what format

00:08:11,970 --> 00:08:16,410
the data is in or where it's going we

00:08:14,160 --> 00:08:19,530
don't care what format the you know the

00:08:16,410 --> 00:08:21,720
snapshots are so this gives us a lot of

00:08:19,530 --> 00:08:23,790
flexibility in that sense

00:08:21,720 --> 00:08:25,980
moreover the incremental backups that

00:08:23,790 --> 00:08:27,639
are created they're inert they don't

00:08:25,980 --> 00:08:30,610
they don't change on desk there

00:08:27,639 --> 00:08:32,769
no they're not they're not live they're

00:08:30,610 --> 00:08:34,360
not hooked into anything so we can

00:08:32,769 --> 00:08:36,279
create as many incrementals as we want

00:08:34,360 --> 00:08:40,659
we can delete them there's no overhead

00:08:36,279 --> 00:08:42,969
of shuffling snapshots around so this is

00:08:40,659 --> 00:08:47,410
a little nicer for us and for management

00:08:42,969 --> 00:08:50,860
utilities hopefully and again we can use

00:08:47,410 --> 00:08:54,730
the existing frameworks for backups and

00:08:50,860 --> 00:08:56,620
hopefully extending management utilities

00:08:54,730 --> 00:09:01,209
like libvirt won't be too difficult for

00:08:56,620 --> 00:09:05,589
us so I will get on to talking about the

00:09:01,209 --> 00:09:08,500
the primitives so the block dirty

00:09:05,589 --> 00:09:09,940
bitmaps it was an existing block layer

00:09:08,500 --> 00:09:12,040
structure that we're using to track

00:09:09,940 --> 00:09:14,430
writes for migration and drive mirroring

00:09:12,040 --> 00:09:16,420
it's implemented using a higher

00:09:14,430 --> 00:09:19,149
hierarchical bitmap which was already

00:09:16,420 --> 00:09:21,430
there for us to use any number can be

00:09:19,149 --> 00:09:23,949
attached to a drive which is useful if

00:09:21,430 --> 00:09:25,810
you want to have multiple backup

00:09:23,949 --> 00:09:29,260
schedules for the same data so you can

00:09:25,810 --> 00:09:31,390
attach as many bitmaps as needed and you

00:09:29,260 --> 00:09:33,160
can use those bitmaps to accomplish

00:09:31,390 --> 00:09:35,019
different kinds of backup schedules as

00:09:33,160 --> 00:09:39,100
appropriate it should be a flexible

00:09:35,019 --> 00:09:43,000
solution so the block dirty bitmaps have

00:09:39,100 --> 00:09:44,860
names the existing internal usages are

00:09:43,000 --> 00:09:47,410
anonymous so you can't address them with

00:09:44,860 --> 00:09:50,019
commands they're basically transparent

00:09:47,410 --> 00:09:53,019
to the user but any bitmaps created for

00:09:50,019 --> 00:09:55,420
incremental backups will have names the

00:09:53,019 --> 00:09:58,839
name is unique to the drive but it's not

00:09:55,420 --> 00:10:02,529
a global it's not a global ID so the

00:09:58,839 --> 00:10:08,920
node name pair is the ID you will use to

00:10:02,529 --> 00:10:11,320
address commands to the bitmap so the

00:10:08,920 --> 00:10:15,579
bitmaps as mentioned have a granularity

00:10:11,320 --> 00:10:20,320
so by default they are tracking rights

00:10:15,579 --> 00:10:23,140
at roughly 64 kilobyte increments you

00:10:20,320 --> 00:10:26,589
can make it smaller and hopefully you

00:10:23,140 --> 00:10:28,300
will save yourself some size on the the

00:10:26,589 --> 00:10:31,060
incremental backup but it depends on the

00:10:28,300 --> 00:10:33,190
usage pattern so you'll use more memory

00:10:31,060 --> 00:10:35,110
in exchange for hopefully smaller

00:10:33,190 --> 00:10:38,529
backups or you can increase the

00:10:35,110 --> 00:10:39,240
granularity to maybe have larger bitmaps

00:10:38,529 --> 00:10:43,610
with

00:10:39,240 --> 00:10:48,000
less memory usage but the default is 64

00:10:43,610 --> 00:10:50,940
kilobytes and you don't have to specify

00:10:48,000 --> 00:10:53,279
one it's completely optional but it will

00:10:50,940 --> 00:10:56,100
attempt to match the cluster size of the

00:10:53,279 --> 00:11:00,360
file format you're using which for QQ is

00:10:56,100 --> 00:11:04,350
64 K so more or less the default is 64 K

00:11:00,360 --> 00:11:07,380
but it can't change so the bitmaps

00:11:04,350 --> 00:11:10,080
track writes per sector but you specify

00:11:07,380 --> 00:11:12,570
the granularity in bytes so if you

00:11:10,080 --> 00:11:14,910
specify a granularity of about 64 K

00:11:12,570 --> 00:11:16,649
you'll gets a granularity of about a

00:11:14,910 --> 00:11:18,990
hundred and twenty eight sectors so any

00:11:16,649 --> 00:11:20,220
write to any one byte on your disk is

00:11:18,990 --> 00:11:24,120
gonna cause you to have a backup

00:11:20,220 --> 00:11:27,180
immediately of 128 sectors there but the

00:11:24,120 --> 00:11:29,490
backup engine itself is also copying out

00:11:27,180 --> 00:11:32,580
per cluster and its notion of a cluster

00:11:29,490 --> 00:11:34,830
is also 64 K but then you also have to

00:11:32,580 --> 00:11:37,290
remember that the file format itself has

00:11:34,830 --> 00:11:39,990
a cluster size so depending on the

00:11:37,290 --> 00:11:41,700
tuning for these three levels you may

00:11:39,990 --> 00:11:43,790
find yourself wasting more space than

00:11:41,700 --> 00:11:46,770
you expected for instance right now

00:11:43,790 --> 00:11:48,390
because we're copying out in 64 K

00:11:46,770 --> 00:11:50,670
increments if you set it to any lower

00:11:48,390 --> 00:11:52,920
than that you're not gonna see any space

00:11:50,670 --> 00:11:55,589
saves regardless so you have to be aware

00:11:52,920 --> 00:11:57,720
of the tuning so for now probably stick

00:11:55,589 --> 00:12:01,079
to the default and we can work on tuning

00:11:57,720 --> 00:12:03,209
this and future versions if needed so

00:12:01,079 --> 00:12:05,550
for the management of these things it's

00:12:03,209 --> 00:12:07,050
managed via qmp which is really good

00:12:05,550 --> 00:12:08,640
news if you're a computer because you

00:12:07,050 --> 00:12:11,579
already know how to talk that protocol

00:12:08,640 --> 00:12:13,620
there are four commands add remove and

00:12:11,579 --> 00:12:15,899
clear there were added for this feature

00:12:13,620 --> 00:12:17,820
but query block already existed and I

00:12:15,899 --> 00:12:22,380
will get to explaining why that's there

00:12:17,820 --> 00:12:24,209
so to create oh and I should mention

00:12:22,380 --> 00:12:26,880
before I go too deep down the rabbit

00:12:24,209 --> 00:12:28,829
hole here all of these examples are

00:12:26,880 --> 00:12:30,930
already in the documentation tree so if

00:12:28,829 --> 00:12:33,420
anybody goes oh that's neat I want to

00:12:30,930 --> 00:12:35,430
implement a backup program for this all

00:12:33,420 --> 00:12:37,560
the examples should work as is without

00:12:35,430 --> 00:12:39,899
modification so you can go run and copy

00:12:37,560 --> 00:12:43,829
paste those and make prototypes to your

00:12:39,899 --> 00:12:45,990
heart content so creation they can be

00:12:43,829 --> 00:12:47,699
created at anytime on any node they

00:12:45,990 --> 00:12:50,310
begin recording immediately there's no

00:12:47,699 --> 00:12:52,769
mess no fuss the granularity is optional

00:12:50,310 --> 00:12:55,529
as stated here we used one for one

00:12:52,769 --> 00:12:57,779
28k because smaller as mentioned is

00:12:55,529 --> 00:13:00,059
still a bad idea

00:12:57,779 --> 00:13:01,800
deletion we can delete them in at any

00:13:00,059 --> 00:13:03,600
time they don't affect any of the

00:13:01,800 --> 00:13:05,970
backups we've already created they don't

00:13:03,600 --> 00:13:09,059
affect other bitmaps attached to the

00:13:05,970 --> 00:13:11,189
node the only thing you can't do is

00:13:09,059 --> 00:13:14,879
delete it while you are making a backup

00:13:11,189 --> 00:13:17,970
but hopefully you expected that we can

00:13:14,879 --> 00:13:20,699
also clear or reset a bitmap it's mostly

00:13:17,970 --> 00:13:22,559
for convenience but it does begin

00:13:20,699 --> 00:13:24,720
recording new rights immediately just

00:13:22,559 --> 00:13:30,299
like add you don't have to reactivate it

00:13:24,720 --> 00:13:31,980
or anything like that for querying there

00:13:30,299 --> 00:13:33,929
is the existing query block command

00:13:31,980 --> 00:13:35,369
which returns an awful lot of stuff I've

00:13:33,929 --> 00:13:38,040
trimmed out the bits that aren't

00:13:35,369 --> 00:13:40,920
relevant to our interest right now here

00:13:38,040 --> 00:13:44,309
this shows an excerpt for a device named

00:13:40,920 --> 00:13:48,899
drive 0 you can see the bitmap 0 down

00:13:44,309 --> 00:13:50,579
there and there is status active which

00:13:48,899 --> 00:13:52,199
means that it is currently on and

00:13:50,579 --> 00:13:53,910
recording rights just in case you didn't

00:13:52,199 --> 00:13:56,999
believe me when I said it is recording

00:13:53,910 --> 00:13:58,649
rights as soon as you add it it can show

00:13:56,999 --> 00:14:00,899
frozen there as a string which means

00:13:58,649 --> 00:14:02,490
that it is currently in use for a backup

00:14:00,899 --> 00:14:05,069
and that's when you can't delete it so

00:14:02,490 --> 00:14:06,959
management utilities can check to see if

00:14:05,069 --> 00:14:11,490
it will succeed or not before it tries

00:14:06,959 --> 00:14:13,949
to issue the command and here the count

00:14:11,490 --> 00:14:15,689
is the number of sectors it'll depend on

00:14:13,949 --> 00:14:19,319
the sector size of the block device but

00:14:15,689 --> 00:14:21,029
for 512 bytes and a granularity of 64k

00:14:19,319 --> 00:14:22,980
this is going to be about two thousand

00:14:21,029 --> 00:14:24,869
clusters or so so you can kind of

00:14:22,980 --> 00:14:27,059
predict somewhat the size of the backup

00:14:24,869 --> 00:14:31,769
before you issue it by using the query

00:14:27,059 --> 00:14:34,019
block comment however qmp commands as

00:14:31,769 --> 00:14:36,389
you may be thinking already are not

00:14:34,019 --> 00:14:38,309
super useful by themselves they're not

00:14:36,389 --> 00:14:40,889
atomic we can't issue across multiple

00:14:38,309 --> 00:14:42,959
drives they're only really safe when the

00:14:40,889 --> 00:14:45,540
VM is offline so that's kind of useless

00:14:42,959 --> 00:14:48,980
and we can it get any kind of multi

00:14:45,540 --> 00:14:53,669
Drive coherency guarantee with that so

00:14:48,980 --> 00:14:55,439
we have transactions bitmap management

00:14:53,669 --> 00:14:59,100
transactions will let us do all kinds of

00:14:55,439 --> 00:15:02,339
stuff we can create new full backups

00:14:59,100 --> 00:15:05,639
while adding or clearing bitmaps we can

00:15:02,339 --> 00:15:07,290
set clear create new sync points across

00:15:05,639 --> 00:15:10,050
multiple drives we can issue

00:15:07,290 --> 00:15:12,569
incrementals across multiple drives so

00:15:10,050 --> 00:15:14,970
this gives us the the data safety that

00:15:12,569 --> 00:15:19,139
everybody was you know maybe worried

00:15:14,970 --> 00:15:20,819
about we got to this slide for supported

00:15:19,139 --> 00:15:22,620
actions we have atom clear

00:15:20,819 --> 00:15:25,470
nobody cares about remove because we're

00:15:22,620 --> 00:15:26,939
getting rid of it anyway it will work in

00:15:25,470 --> 00:15:29,249
conjunction with the Drive backup

00:15:26,939 --> 00:15:31,769
command which also has the transaction

00:15:29,249 --> 00:15:34,769
that's already in queue and this will

00:15:31,769 --> 00:15:38,639
let us do incrementals full backups new

00:15:34,769 --> 00:15:40,740
chains and sync points and so on so with

00:15:38,639 --> 00:15:43,259
all that gobbledygook out of the way I

00:15:40,740 --> 00:15:46,470
can show an example of the life cycle

00:15:43,259 --> 00:15:49,160
for how to manage some of this

00:15:46,470 --> 00:15:51,870
so generally you'll create a new bitmap

00:15:49,160 --> 00:15:54,269
you'll attach it to a drive you will

00:15:51,870 --> 00:15:56,579
create a kind of a synchronization point

00:15:54,269 --> 00:15:58,499
between the bitmap and the backup and

00:15:56,579 --> 00:16:01,139
then from there you can create as many

00:15:58,499 --> 00:16:03,059
incrementals as you want and then if in

00:16:01,139 --> 00:16:05,189
the future you want to create a new sync

00:16:03,059 --> 00:16:09,839
point you can roll back or just keep

00:16:05,189 --> 00:16:12,029
making incrementals so for example this

00:16:09,839 --> 00:16:14,069
is how we'd create a new chain you can

00:16:12,029 --> 00:16:16,829
use the add and Drive backup commands

00:16:14,069 --> 00:16:18,689
there along with sync full down there to

00:16:16,829 --> 00:16:20,519
create a full backup and clear the

00:16:18,689 --> 00:16:22,110
bitmap at the same time so you can be

00:16:20,519 --> 00:16:23,970
assured that any new writes that come

00:16:22,110 --> 00:16:26,220
forward will be tracked for your next

00:16:23,970 --> 00:16:28,410
incremental so you start with something

00:16:26,220 --> 00:16:31,199
like this really boring it's just a box

00:16:28,410 --> 00:16:33,749
but we can attach a bitmap and create a

00:16:31,199 --> 00:16:36,360
full backup somewhere on disk perhaps

00:16:33,749 --> 00:16:41,029
and count zero because there's no dirty

00:16:36,360 --> 00:16:43,829
writes now or say we have an existing

00:16:41,029 --> 00:16:45,720
bitmap and you just want to reset it say

00:16:43,829 --> 00:16:49,649
you were testing something or some day

00:16:45,720 --> 00:16:51,420
and I can start a new backup chain you

00:16:49,649 --> 00:16:53,910
would use the clear and Drive backup

00:16:51,420 --> 00:16:56,399
commands in tandem and again sync full

00:16:53,910 --> 00:16:57,720
for the full backup so you'd start with

00:16:56,399 --> 00:16:59,550
something like this where we have a

00:16:57,720 --> 00:17:02,399
bunch of dirty sectors but for whatever

00:16:59,550 --> 00:17:04,589
reason we decided we wanted to start a

00:17:02,399 --> 00:17:07,860
completely new backup so now the full

00:17:04,589 --> 00:17:09,949
queue cow to backup is old we don't need

00:17:07,860 --> 00:17:12,640
it you can delete it if you want but

00:17:09,949 --> 00:17:15,069
we've successfully

00:17:12,640 --> 00:17:16,559
in one transaction created a new backup

00:17:15,069 --> 00:17:19,209
chain

00:17:16,559 --> 00:17:22,750
so first incremental the interesting

00:17:19,209 --> 00:17:25,600
stuff for I know I've been promising

00:17:22,750 --> 00:17:27,490
that it's a format independent but QQ is

00:17:25,600 --> 00:17:29,980
really super useful it has backing files

00:17:27,490 --> 00:17:32,740
and we like those so you can for

00:17:29,980 --> 00:17:35,890
instance create with key new image on

00:17:32,740 --> 00:17:38,080
the command line a new empty q2 file

00:17:35,890 --> 00:17:41,170
with a backing file of the old full

00:17:38,080 --> 00:17:43,600
backup and then you can issue a Drive

00:17:41,170 --> 00:17:46,660
backup command using sync incremental

00:17:43,600 --> 00:17:49,720
which is highlighted down there you can

00:17:46,660 --> 00:17:52,360
use the single qmp command which I have

00:17:49,720 --> 00:17:53,830
illustrated here or like I said and the

00:17:52,360 --> 00:17:56,350
readme already in the docs folder

00:17:53,830 --> 00:17:59,290
there's examples of transactions which

00:17:56,350 --> 00:18:01,210
you can issue a bunch of Drive backups

00:17:59,290 --> 00:18:04,390
across as many drives as you would need

00:18:01,210 --> 00:18:06,160
to so for an example again here we've

00:18:04,390 --> 00:18:08,049
got a bitmap that's accumulated some

00:18:06,160 --> 00:18:10,870
dirty sectors to it and when we issue

00:18:08,049 --> 00:18:12,850
the command poof the bitmaps cleaned out

00:18:10,870 --> 00:18:15,070
and now we have a new incremental then

00:18:12,850 --> 00:18:17,620
it's much smaller than the full backup

00:18:15,070 --> 00:18:19,840
but thanks to the backing file you can

00:18:17,620 --> 00:18:21,520
just point directly to the new

00:18:19,840 --> 00:18:24,490
incremental file and it's a complete

00:18:21,520 --> 00:18:27,130
point in time snapshot there so

00:18:24,490 --> 00:18:28,630
subsequent backups for any substance

00:18:27,130 --> 00:18:30,309
there's nothing special you need to do

00:18:28,630 --> 00:18:32,230
it's basically the same command as last

00:18:30,309 --> 00:18:36,130
time you just point to the last

00:18:32,230 --> 00:18:38,080
incremental you made and you can

00:18:36,130 --> 00:18:39,730
continue forever until your hard drive

00:18:38,080 --> 00:18:44,230
is completely full and you are sad

00:18:39,730 --> 00:18:47,350
inside advanced features these are the

00:18:44,230 --> 00:18:51,820
ones vladimir was working on lots of

00:18:47,350 --> 00:18:53,890
good stuff so bitmap migration the

00:18:51,820 --> 00:18:55,990
mechanism is similar to the live disk

00:18:53,890 --> 00:18:57,400
migration and that we split it into

00:18:55,990 --> 00:18:58,090
chunks and then we send it piece by

00:18:57,400 --> 00:19:00,850
piece

00:18:58,090 --> 00:19:04,840
and while the guest is still writing we

00:19:00,850 --> 00:19:06,429
can catch that and copy extra pieces as

00:19:04,840 --> 00:19:08,799
we need to the chunks are a little bit

00:19:06,429 --> 00:19:13,450
smaller we do one kilobyte hopefully to

00:19:08,799 --> 00:19:15,700
fit in an Ethernet frame but if all of

00:19:13,450 --> 00:19:17,799
your bitmaps together are below one Meg

00:19:15,700 --> 00:19:20,629
we skip the live phase and we just copy

00:19:17,799 --> 00:19:23,539
the data and one chunk

00:19:20,629 --> 00:19:26,239
for an example of kind of how much sizes

00:19:23,539 --> 00:19:29,899
you can expect there a 64 gig disc with

00:19:26,239 --> 00:19:31,369
the standard sector size and the

00:19:29,899 --> 00:19:33,679
standard granularity for the bitmap

00:19:31,369 --> 00:19:36,289
you're only using 128 K for the bitmap

00:19:33,679 --> 00:19:38,389
so it's not too big though of course it

00:19:36,289 --> 00:19:41,299
does have to spend a couple of bytes

00:19:38,389 --> 00:19:44,210
transferring the names and stuff but not

00:19:41,299 --> 00:19:46,399
a big deal they're very small so the

00:19:44,210 --> 00:19:49,009
bitmaps are not transferred alongside

00:19:46,399 --> 00:19:51,859
data yet you don't have to migrate the

00:19:49,009 --> 00:19:54,679
block to migrate the bitmaps so you can

00:19:51,859 --> 00:19:56,509
with or without the data migrate the

00:19:54,679 --> 00:20:01,820
bitmaps over whatever is more convenient

00:19:56,509 --> 00:20:03,320
for your solution we managed I had to

00:20:01,820 --> 00:20:05,719
put this in here because it was funny

00:20:03,320 --> 00:20:08,599
when we were viewing it but we managed

00:20:05,719 --> 00:20:10,249
the the live backup excuse me the live

00:20:08,599 --> 00:20:14,299
migration of the bitmap data with

00:20:10,249 --> 00:20:15,769
bitmaps that track the bitmaps thank

00:20:14,299 --> 00:20:17,960
goodness they're called meta bitmaps now

00:20:15,769 --> 00:20:23,859
instead of yes dirty dirty bitmap

00:20:17,960 --> 00:20:26,989
bitmaps it was it's not too good and

00:20:23,859 --> 00:20:28,759
that uses a very very little memory so

00:20:26,989 --> 00:20:32,239
hopefully those don't you know make

00:20:28,759 --> 00:20:34,159
anybody's day too bad we do have to

00:20:32,239 --> 00:20:36,440
still discuss how this is going to

00:20:34,159 --> 00:20:39,019
interoperate with persistence so if you

00:20:36,440 --> 00:20:41,539
save a bitmap to disk and then you want

00:20:39,019 --> 00:20:42,950
to migrate with or without the disks we

00:20:41,539 --> 00:20:44,809
have to figure out what our policy is

00:20:42,950 --> 00:20:46,700
going to be for where we're gonna stick

00:20:44,809 --> 00:20:48,289
this bitmap during migration because

00:20:46,700 --> 00:20:51,409
there's a lot of different routes that

00:20:48,289 --> 00:20:54,349
can take but for now we do have patches

00:20:51,409 --> 00:20:55,940
on list waiting review and hopefully

00:20:54,349 --> 00:21:02,149
we'll be able to get that in four to

00:20:55,940 --> 00:21:03,849
five so persistence persistence is the

00:21:02,149 --> 00:21:06,499
good one that everybody's curious about

00:21:03,849 --> 00:21:08,599
we can save bitmaps across shutdowns

00:21:06,499 --> 00:21:10,359
because this without that that would be

00:21:08,599 --> 00:21:13,399
kind of not much of a solution at all

00:21:10,359 --> 00:21:16,159
it's a work in progress right now we are

00:21:13,399 --> 00:21:18,889
targeting a cue cow to extension for

00:21:16,159 --> 00:21:20,539
saving the bitmaps we are going to be

00:21:18,889 --> 00:21:23,059
using it hopefully as a generic

00:21:20,539 --> 00:21:25,729
container which is modeled kind of after

00:21:23,059 --> 00:21:28,070
how snapshots are stored but we don't

00:21:25,729 --> 00:21:30,080
require that you use QPR - for the data

00:21:28,070 --> 00:21:32,299
for the backups for the drive we just

00:21:30,080 --> 00:21:32,630
want to use it almost like a zipper at

00:21:32,299 --> 00:21:38,630
our

00:21:32,630 --> 00:21:40,970
for containing our bitmap data so we can

00:21:38,630 --> 00:21:43,550
store them in an empty queue cow - you

00:21:40,970 --> 00:21:45,620
can store multiple bitmaps if you have a

00:21:43,550 --> 00:21:47,390
hundred drives you can store all of

00:21:45,620 --> 00:21:50,990
those in the same thing it doesn't it

00:21:47,390 --> 00:21:55,430
doesn't matter what for convenience of

00:21:50,990 --> 00:21:57,440
course we can store the store of bitmap

00:21:55,430 --> 00:22:00,170
alongside the data it does describe and

00:21:57,440 --> 00:22:04,010
have an auto load when you pass that

00:22:00,170 --> 00:22:07,520
into Kimia so if that doesn't sound good

00:22:04,010 --> 00:22:10,760
for you you should go on the list and

00:22:07,520 --> 00:22:13,250
tell us why but I do it was chosen

00:22:10,760 --> 00:22:15,080
because we didn't want to implement a

00:22:13,250 --> 00:22:17,960
brand new file type and complicate

00:22:15,080 --> 00:22:20,030
complicate the command line so we reused

00:22:17,960 --> 00:22:25,270
again existing infrastructure to kind of

00:22:20,030 --> 00:22:25,270
manage bits of data yeah

00:22:32,180 --> 00:22:34,870
yeah

00:22:37,940 --> 00:22:44,600
so to dues we still need to decide on a

00:22:42,440 --> 00:22:47,510
qmp interface from modifying the

00:22:44,600 --> 00:22:51,140
persistence attributes because we

00:22:47,510 --> 00:22:52,610
haven't quite yet decided how we're

00:22:51,140 --> 00:22:54,920
gonna store the persistence so we

00:22:52,610 --> 00:22:57,170
haven't gone too far on that yet but we

00:22:54,920 --> 00:22:59,990
will need to be able to choose and

00:22:57,170 --> 00:23:02,060
decide you know which bitmap gets stored

00:22:59,990 --> 00:23:04,490
where or if we want to change it

00:23:02,060 --> 00:23:07,310
turn on persistence persistence for a

00:23:04,490 --> 00:23:09,890
bitmap turn off you know we still need

00:23:07,310 --> 00:23:12,500
to create a couple of CLI tools for

00:23:09,890 --> 00:23:14,870
verification and Allisyn analysis and

00:23:12,500 --> 00:23:16,550
managing the bitmaps inside so we don't

00:23:14,870 --> 00:23:19,820
get junk data that we carry around

00:23:16,550 --> 00:23:23,390
forever we still need to add in support

00:23:19,820 --> 00:23:25,970
for checking the validity and repairing

00:23:23,390 --> 00:23:30,620
it maybe in queue image check I don't

00:23:25,970 --> 00:23:33,200
know we still need to discuss the data

00:23:30,620 --> 00:23:37,460
in target oh yeah excuse me the data

00:23:33,200 --> 00:23:39,850
integrity of the persistence so we have

00:23:37,460 --> 00:23:42,470
not yet implemented like a periodic or

00:23:39,850 --> 00:23:44,650
opportunistic flushing back to disk so

00:23:42,470 --> 00:23:48,740
at the moment it's only on load and

00:23:44,650 --> 00:23:51,770
unsaved so we need to work on that just

00:23:48,740 --> 00:23:54,710
a little bit and as dennis luna

00:23:51,770 --> 00:23:57,290
suggested we may convert the migration

00:23:54,710 --> 00:23:59,870
over to using a post copy solution where

00:23:57,290 --> 00:24:01,940
we just migrate the names and some of

00:23:59,870 --> 00:24:04,910
the metadata over and then we can worry

00:24:01,940 --> 00:24:07,520
about copying the bitmaps over after

00:24:04,910 --> 00:24:10,910
we've already migrated because merging

00:24:07,520 --> 00:24:12,890
two bitmaps is trivial so we can just

00:24:10,910 --> 00:24:15,050
put the bitmap into for instance kind of

00:24:12,890 --> 00:24:17,450
like a semi frozen state where it's

00:24:15,050 --> 00:24:18,920
marked is inconsistent until all the

00:24:17,450 --> 00:24:22,820
data has arrived

00:24:18,920 --> 00:24:26,170
but merging that into post copy is

00:24:22,820 --> 00:24:31,000
something that we are looking at doing

00:24:26,170 --> 00:24:31,000
so last bit

00:24:31,050 --> 00:24:39,040
so project status the block dirty bitmap

00:24:35,200 --> 00:24:41,850
qmp interface already in the sync

00:24:39,040 --> 00:24:47,740
incremental mode also already in

00:24:41,850 --> 00:24:50,470
transactions 2.5 the migration is on

00:24:47,740 --> 00:24:52,630
list I expect that to go in shortly and

00:24:50,470 --> 00:24:58,810
the persistence is still an RFC for now

00:24:52,630 --> 00:24:59,200
and so hopefully 2.5 or maybe 2.6 that's

00:24:58,810 --> 00:25:02,130
it

00:24:59,200 --> 00:25:02,130
any questions

00:25:05,840 --> 00:25:11,820
not yet I'm hoping that actually I

00:25:10,500 --> 00:25:13,650
should repeat the question for the thing

00:25:11,820 --> 00:25:18,929
I was asked if there's any lip vert

00:25:13,650 --> 00:25:20,760
tooling support so not yet my hope is

00:25:18,929 --> 00:25:22,650
that by not straying far from how

00:25:20,760 --> 00:25:25,470
backups are done already that it won't

00:25:22,650 --> 00:25:28,410
be too hard to add in but of course the

00:25:25,470 --> 00:25:31,040
delivert people would have more opinions

00:25:28,410 --> 00:25:31,040
on that than I would

00:25:38,270 --> 00:25:41,100
yeah

00:25:39,600 --> 00:25:42,929
so the question is if you can do

00:25:41,100 --> 00:25:45,900
incremental backups across multiple

00:25:42,929 --> 00:25:48,990
drives at the same time yeah just use

00:25:45,900 --> 00:25:52,830
the transaction you can add bitmaps to

00:25:48,990 --> 00:25:56,460
other drives in advance and then if you

00:25:52,830 --> 00:25:59,490
want to create coherent backups across

00:25:56,460 --> 00:26:01,830
as many as you want you can add as many

00:25:59,490 --> 00:26:05,840
commands to the transaction as you want

00:26:01,830 --> 00:26:08,040
so you can do you know multi drive

00:26:05,840 --> 00:26:11,490
packages you'll just have one drive

00:26:08,040 --> 00:26:12,809
backup action per Drive and there's I

00:26:11,490 --> 00:26:14,820
don't think there's a limit to how many

00:26:12,809 --> 00:26:16,800
you can issue so if you want a hundred

00:26:14,820 --> 00:26:19,110
drives and you want them all to have a

00:26:16,800 --> 00:26:29,809
backup at the same instant you can do

00:26:19,110 --> 00:26:29,809
that yes

00:26:40,050 --> 00:26:50,590
yes so the question is if there is some

00:26:48,640 --> 00:26:52,840
kind of an event that's emitted after

00:26:50,590 --> 00:26:55,180
the bitmap is flipped from frozen back

00:26:52,840 --> 00:26:59,500
to active after an incremental backup

00:26:55,180 --> 00:27:01,480
and not a discrete event at the moment

00:26:59,500 --> 00:27:04,770
it's you'll get the standard backup

00:27:01,480 --> 00:27:07,690
completed event and it happens with that

00:27:04,770 --> 00:27:12,220
if there's a use case for an additional

00:27:07,690 --> 00:27:15,580
event we can add one I suppose I should

00:27:12,220 --> 00:27:18,490
take the second to mention that if the

00:27:15,580 --> 00:27:20,710
backup fails as well and you don't get

00:27:18,490 --> 00:27:24,340
the the backup book the block job

00:27:20,710 --> 00:27:26,920
completed event the bitmap will roll

00:27:24,340 --> 00:27:28,450
back from frozen to active but it will

00:27:26,920 --> 00:27:32,950
merge any rights that occurred during

00:27:28,450 --> 00:27:35,050
that window backed into the bitmap so

00:27:32,950 --> 00:27:42,580
you won't lose any of that data in the

00:27:35,050 --> 00:27:51,550
interim nope okay I think we're good

00:27:42,580 --> 00:27:53,740
then thank you just a quick shout out to

00:27:51,550 --> 00:27:55,510
you know if you have any questions if

00:27:53,740 --> 00:27:57,610
the design of some of the later features

00:27:55,510 --> 00:28:00,160
you would like to comment or contribute

00:27:57,610 --> 00:28:01,870
to please send me a mail and see see the

00:28:00,160 --> 00:28:03,280
mailing list so that we can make sure

00:28:01,870 --> 00:28:06,000
that this feature is as good as possible

00:28:03,280 --> 00:28:06,000

YouTube URL: https://www.youtube.com/watch?v=a2BNswm_yf8


