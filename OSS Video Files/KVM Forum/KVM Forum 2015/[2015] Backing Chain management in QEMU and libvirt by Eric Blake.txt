Title: [2015] Backing Chain management in QEMU and libvirt by Eric Blake
Publication date: 2015-09-08
Playlist: KVM Forum 2015
Description: 
	Backing Chains form the backbone of guest storage backups, snapshots, live storage migration, and even some promising new technologies like course-grained lock-step redundancy for better guest failover between hosts. In this presentation, Eric Blake will review how qcow2 backing chains work, and how basic operations such as stream, copy, commit, and dirty bitmaps are combined under management software like libvirt to perform several useful operations on guest storage. Visualizing where guest data lives, and how a given qcow2 file represents all data changes from a set point in time, can be helpful in determining which operation is best for a task at hand.

Eric Blake
Red Hat
I have worked with libvirt and qemu since 2010 as part of the Red Hat virtualization team. My focus is on interface design for ensuring that libvirt can manage qemu efficiently, as well as libvirt management of block storage solutions. Previous conference presentations include Linux Plumber's Conference in 2012, and KVM Forum in 2013.

Slides: https://drive.google.com/open?id=0BzyAwvVlQckecm90M0dMMEpKRXc
Captions: 
	00:00:14,179 --> 00:00:19,590
i'm eric blake worked with redhat i've

00:00:17,460 --> 00:00:22,260
been there for last five years working

00:00:19,590 --> 00:00:25,560
on the liberty and also on its

00:00:22,260 --> 00:00:27,300
integration with the q mu project so I'm

00:00:25,560 --> 00:00:30,769
familiar with a lot of interface design

00:00:27,300 --> 00:00:34,170
and also topic of today's presentation

00:00:30,769 --> 00:00:38,760
backing chain management or how to get

00:00:34,170 --> 00:00:41,879
your data where you want it so today go

00:00:38,760 --> 00:00:44,250
over the q cow to format the building

00:00:41,879 --> 00:00:47,219
blocks in q mu for managing and backing

00:00:44,250 --> 00:00:49,410
chains and then what libert currently

00:00:47,219 --> 00:00:51,960
exposes in some of the future work that

00:00:49,410 --> 00:00:58,920
was coming down the pipeline how to put

00:00:51,960 --> 00:01:02,489
it together for some neat demos on the

00:00:58,920 --> 00:01:05,070
topic of q cow too it's been around for

00:01:02,489 --> 00:01:07,920
a while now the first documentation was

00:01:05,070 --> 00:01:09,990
written in 2006 and we all know how

00:01:07,920 --> 00:01:11,130
documentation lags implementation I

00:01:09,990 --> 00:01:16,590
couldn't find when it was actually

00:01:11,130 --> 00:01:20,299
created but Q copy-on-write gives the

00:01:16,590 --> 00:01:26,060
ability to manage what the guest sees

00:01:20,299 --> 00:01:30,360
without wasting host resources QQ was

00:01:26,060 --> 00:01:33,000
added in 2008 because the Q cow original

00:01:30,360 --> 00:01:35,220
format was severely limited q Caillou

00:01:33,000 --> 00:01:39,210
came along with internal snapshots

00:01:35,220 --> 00:01:42,649
reference counting a bit more structure

00:01:39,210 --> 00:01:47,220
it was no longer 32-bit versus 64-bit

00:01:42,649 --> 00:01:50,159
dependent then in 2009 we figured out

00:01:47,220 --> 00:01:53,700
our headers weren't big enough we have

00:01:50,159 --> 00:01:57,360
to track the format of the backing file

00:01:53,700 --> 00:02:00,119
if we don't want CVEs from probing

00:01:57,360 --> 00:02:04,649
problems and we came up with a great

00:02:00,119 --> 00:02:06,890
hack to inject the backing name into the

00:02:04,649 --> 00:02:06,890
file

00:02:08,030 --> 00:02:14,460
along the side there was the QED format

00:02:11,280 --> 00:02:16,770
that said we're got a lot of baggage can

00:02:14,460 --> 00:02:18,810
we make performance closer to bare metal

00:02:16,770 --> 00:02:20,850
let's experiment with some new features

00:02:18,810 --> 00:02:23,970
and if they pan out will merge them into

00:02:20,850 --> 00:02:26,160
Q cow too well nobody here uses QED

00:02:23,970 --> 00:02:30,080
these days because we did merge them

00:02:26,160 --> 00:02:32,250
into Q cow to version 3 in April 2012

00:02:30,080 --> 00:02:34,670
among the things we added we're feature

00:02:32,250 --> 00:02:38,670
bits so that now we can negotiate on

00:02:34,670 --> 00:02:41,400
what the file contains and gracefully

00:02:38,670 --> 00:02:44,400
degrade with older qm you opening the

00:02:41,400 --> 00:02:46,760
file we added efficient zero cluster

00:02:44,400 --> 00:02:49,770
management which makes it even more

00:02:46,760 --> 00:02:53,840
efficient use of the host resources for

00:02:49,770 --> 00:02:56,790
spite sparse file systems and so forth

00:02:53,840 --> 00:02:58,800
but all this is just talk and bullet

00:02:56,790 --> 00:03:00,450
points if you don't know what the format

00:02:58,800 --> 00:03:03,920
itself is doing so I'm going to look

00:03:00,450 --> 00:03:06,780
under the hood let's create a new file

00:03:03,920 --> 00:03:11,850
100 megabytes will suffice just for the

00:03:06,780 --> 00:03:14,310
demonstration and there is the first

00:03:11,850 --> 00:03:16,709
representation of what the file does I

00:03:14,310 --> 00:03:20,070
looked online and could not find a great

00:03:16,709 --> 00:03:21,810
image for this so obviously can't read

00:03:20,070 --> 00:03:25,050
it but we'll try to point out what's

00:03:21,810 --> 00:03:28,760
important there's always a header the

00:03:25,050 --> 00:03:31,320
first sector Q cow to is cluster-based

00:03:28,760 --> 00:03:33,720
everything we do is in clusters the

00:03:31,320 --> 00:03:36,600
default size is 64 K you can tune that

00:03:33,720 --> 00:03:39,269
down to a small as 512 bytes have your

00:03:36,600 --> 00:03:42,510
clusters mattress sectors great until

00:03:39,269 --> 00:03:45,540
you working on 4k sector disks you can

00:03:42,510 --> 00:03:47,160
tune it up to 2 megabytes there's some

00:03:45,540 --> 00:03:50,760
inefficiencies as you get larger

00:03:47,160 --> 00:03:52,890
clusters for managing copy-on-write but

00:03:50,760 --> 00:03:58,350
there's also some efficiencies economies

00:03:52,890 --> 00:04:00,720
of scale as you do larger I oh so the

00:03:58,350 --> 00:04:05,130
default is good but if you need to tune

00:04:00,720 --> 00:04:07,260
it you're more than welcome to every

00:04:05,130 --> 00:04:09,080
image contains a ref count table and a

00:04:07,260 --> 00:04:12,150
ref count block which is a two layer

00:04:09,080 --> 00:04:16,049
table of here's where every cluster is

00:04:12,150 --> 00:04:18,140
claimed or how many times each cluster

00:04:16,049 --> 00:04:20,060
is claimed from the hosts point of view

00:04:18,140 --> 00:04:22,130
so with

00:04:20,060 --> 00:04:27,260
brand new table we've claimed four

00:04:22,130 --> 00:04:28,790
clusters and of those clusters three of

00:04:27,260 --> 00:04:30,290
them of the header and ref counts that

00:04:28,790 --> 00:04:33,020
we just described and the last one is

00:04:30,290 --> 00:04:39,110
the l1 table and it's not even a cluster

00:04:33,020 --> 00:04:40,580
big so what gives well Q KATU has takes

00:04:39,110 --> 00:04:42,410
advantage the operating system

00:04:40,580 --> 00:04:44,840
guarantees that if you seek past the end

00:04:42,410 --> 00:04:47,840
of the file you'll read zeros so

00:04:44,840 --> 00:04:50,630
depending on the cluster if the file is

00:04:47,840 --> 00:04:55,610
not cluster aligned reads Eros and

00:04:50,630 --> 00:05:00,200
you'll be safe we have an l1 table that

00:04:55,610 --> 00:05:02,450
describes what the guest sees so the ref

00:05:00,200 --> 00:05:04,850
count table is what the host is in using

00:05:02,450 --> 00:05:08,150
the l1 and l2 tables are what the guest

00:05:04,850 --> 00:05:12,110
sees and there's no l2 table because

00:05:08,150 --> 00:05:15,650
right now the guest sees 100 megabytes

00:05:12,110 --> 00:05:17,750
of nothingness so that's boring let's

00:05:15,650 --> 00:05:21,470
add something to it I'm going to use the

00:05:17,750 --> 00:05:25,070
QIO command which is mainly for

00:05:21,470 --> 00:05:27,770
developers it simulates data rights to a

00:05:25,070 --> 00:05:29,390
raw chunk of the file in reality you'd

00:05:27,770 --> 00:05:31,940
probably have a guest file system but

00:05:29,390 --> 00:05:33,890
file systems are finicky the mere act of

00:05:31,940 --> 00:05:35,750
touching a one-bite file probably

00:05:33,890 --> 00:05:37,340
touches two or three sectors and if

00:05:35,750 --> 00:05:40,430
those sectors aren't in the same cluster

00:05:37,340 --> 00:05:44,090
that's two or three clusters it gets a

00:05:40,430 --> 00:05:46,640
lot harder to draw now that I've got

00:05:44,090 --> 00:05:50,690
some data our diagram is a bit more

00:05:46,640 --> 00:05:53,570
complex first thing to notice we added

00:05:50,690 --> 00:05:56,690
some additional ref count blocks to

00:05:53,570 --> 00:05:58,820
track that we added new clusters other

00:05:56,690 --> 00:06:02,270
thing to notice is we added an l2 table

00:05:58,820 --> 00:06:06,020
and those l2 table three clusters of

00:06:02,270 --> 00:06:08,900
data so I stuck some memory in the

00:06:06,020 --> 00:06:13,789
middle of the file near the 99 megabyte

00:06:08,900 --> 00:06:15,889
mark I intentionally wrote 65k of data

00:06:13,789 --> 00:06:20,270
which was not cluster aligned just to

00:06:15,889 --> 00:06:22,520
see what would happen q mu is smart

00:06:20,270 --> 00:06:26,300
enough to allocate the entire cluster 0

00:06:22,520 --> 00:06:28,610
pad what I didn't use then it tracks in

00:06:26,300 --> 00:06:30,979
its l2 table here's the address of every

00:06:28,610 --> 00:06:34,000
cluster from where the guest sees it to

00:06:30,979 --> 00:06:34,000
where the host sees it

00:06:34,970 --> 00:06:40,440
then we get to do the fun features that

00:06:37,650 --> 00:06:46,130
Q katu created which was internal

00:06:40,440 --> 00:06:49,770
snapshots our file has grown even more

00:06:46,130 --> 00:06:52,650
we added an l1 table wait we already

00:06:49,770 --> 00:06:54,420
have an l1 table well the point of a

00:06:52,650 --> 00:06:57,840
snapshot is that you can have multiple

00:06:54,420 --> 00:06:59,910
l1 tables in your file at any given

00:06:57,840 --> 00:07:03,690
point and l1 table describes the state

00:06:59,910 --> 00:07:05,370
the guests would see right now because

00:07:03,690 --> 00:07:08,970
we just created the staff shot it's the

00:07:05,370 --> 00:07:11,250
same state as the file itself sees the

00:07:08,970 --> 00:07:14,250
header points to one of the tables it

00:07:11,250 --> 00:07:16,500
also points to a snapshot structure it

00:07:14,250 --> 00:07:18,720
occupies a partial cluster at the moment

00:07:16,500 --> 00:07:21,420
the snapshot structure points to the

00:07:18,720 --> 00:07:23,790
other l1 table and then through the

00:07:21,420 --> 00:07:30,210
convenience of Q a new image command you

00:07:23,790 --> 00:07:32,220
can pivot between them at will and part

00:07:30,210 --> 00:07:35,910
of creating this was we updated ref

00:07:32,220 --> 00:07:39,990
counts so for clusters 4 5 6 and 7 which

00:07:35,910 --> 00:07:43,500
is our l2 data data data they now have a

00:07:39,990 --> 00:07:46,290
ref count of 2 we also had to tweak in

00:07:43,500 --> 00:07:50,280
the l2 table there's a flag Ref is one

00:07:46,290 --> 00:07:52,740
in the l1 table is a flag Ref is one the

00:07:50,280 --> 00:07:54,930
flag got toggled so that future

00:07:52,740 --> 00:07:56,490
operations don't have to consult the ref

00:07:54,930 --> 00:07:59,400
count every time they can just look at

00:07:56,490 --> 00:08:02,640
the flag and say am I the loan owner of

00:07:59,400 --> 00:08:05,430
this cluster or am I going to have to

00:08:02,640 --> 00:08:08,390
worry about copy on write the data

00:08:05,430 --> 00:08:11,850
specification does point out that the

00:08:08,390 --> 00:08:15,060
ref is one flag is only valid for the

00:08:11,850 --> 00:08:16,890
active layer if you're going if you're

00:08:15,060 --> 00:08:19,110
reading an l1 table through the snapshot

00:08:16,890 --> 00:08:20,820
table you have to compute things every

00:08:19,110 --> 00:08:23,280
single time because you don't know if

00:08:20,820 --> 00:08:24,780
some other code branch has changed

00:08:23,280 --> 00:08:30,180
references behind your back since the

00:08:24,780 --> 00:08:32,550
snapshot was taken as I said snapshot

00:08:30,180 --> 00:08:35,520
with no data is boring so let's add more

00:08:32,550 --> 00:08:39,360
data this time i'm going to write 512

00:08:35,520 --> 00:08:42,710
bytes into an existing cluster and this

00:08:39,360 --> 00:08:45,300
is where copy-on-write takes in place

00:08:42,710 --> 00:08:46,110
queuing Lewis said we've got a cluster

00:08:45,300 --> 00:08:48,930
there

00:08:46,110 --> 00:08:52,260
copy the tail end that didn't change in

00:08:48,930 --> 00:08:55,050
my right so that it's also present in

00:08:52,260 --> 00:08:57,540
the data right my new data and because i

00:08:55,050 --> 00:09:01,680
have a new cluster i need to describe it

00:08:57,540 --> 00:09:05,190
with an l2 table oh dear that means we

00:09:01,680 --> 00:09:07,170
also have to clone the l2 table so it

00:09:05,190 --> 00:09:09,690
changes the ref counts we have fewer

00:09:07,170 --> 00:09:11,399
accounts that are to some of them went

00:09:09,690 --> 00:09:16,170
back to one because there's no

00:09:11,399 --> 00:09:18,810
additional Custer's a lot of the data is

00:09:16,170 --> 00:09:21,390
rearranged and we have our first case of

00:09:18,810 --> 00:09:23,880
a file where the order that the clusters

00:09:21,390 --> 00:09:27,410
appear in the host is different than the

00:09:23,880 --> 00:09:27,410
order the clusters appear in the guest

00:09:27,860 --> 00:09:34,410
makes for some interesting trying to

00:09:32,399 --> 00:09:38,160
figure out where your data lives but

00:09:34,410 --> 00:09:43,110
it's manageable computer does it for you

00:09:38,160 --> 00:09:45,660
right and again as I said there's two l1

00:09:43,110 --> 00:09:47,970
tables depending on which l1 table you

00:09:45,660 --> 00:09:50,579
see determines which other clusters are

00:09:47,970 --> 00:09:53,550
used together to form whether the guests

00:09:50,579 --> 00:09:57,680
each data or study sees the empty bytes

00:09:53,550 --> 00:09:59,760
that were there from before the snapshot

00:09:57,680 --> 00:10:01,980
internal snapshots are fun what about

00:09:59,760 --> 00:10:07,459
external snapshots I'm going to create a

00:10:01,980 --> 00:10:10,800
new file this time call it wrap q cow to

00:10:07,459 --> 00:10:15,630
its back to our original for cluster

00:10:10,800 --> 00:10:17,339
first time created file this time the

00:10:15,630 --> 00:10:19,110
part that makes it interesting is that

00:10:17,339 --> 00:10:24,660
we wrote into the header some details

00:10:19,110 --> 00:10:26,850
about my backing file is based qk to use

00:10:24,660 --> 00:10:31,920
that format header extension to say my

00:10:26,850 --> 00:10:33,899
backing file is also q cow to you can

00:10:31,920 --> 00:10:36,870
mix and match the base file can be of

00:10:33,899 --> 00:10:39,300
any format a very common operation in

00:10:36,870 --> 00:10:42,000
fact in my later slides is having a raw

00:10:39,300 --> 00:10:46,170
file as the base and a cuke how to as

00:10:42,000 --> 00:10:47,910
the wrapper but an external snapshot

00:10:46,170 --> 00:10:50,610
without data is no fun so time to write

00:10:47,910 --> 00:10:53,130
and see what happens here I'm going to

00:10:50,610 --> 00:10:54,959
fill in some of the data that overwrites

00:10:53,130 --> 00:10:57,699
some of what was there and leave in

00:10:54,959 --> 00:11:00,970
place some what was there

00:10:57,699 --> 00:11:05,169
in this case ride it in between where I

00:11:00,970 --> 00:11:07,749
had data last time and just like with

00:11:05,169 --> 00:11:10,540
internal snapshots we have to copy the

00:11:07,749 --> 00:11:12,879
entire cluster to cover the data for the

00:11:10,540 --> 00:11:16,660
part that wasn't mapped by the right

00:11:12,879 --> 00:11:19,149
itself and when we're doing an external

00:11:16,660 --> 00:11:21,100
snapshot we always copy from the backing

00:11:19,149 --> 00:11:23,049
image without paying attention to the

00:11:21,100 --> 00:11:25,720
backing images ref counts ref counts

00:11:23,049 --> 00:11:31,529
only matter for internal snapshot

00:11:25,720 --> 00:11:34,809
purposes and when we're reading the file

00:11:31,529 --> 00:11:36,819
the wake um ooh does it is if the file

00:11:34,809 --> 00:11:38,709
has the data that's the quest of the

00:11:36,819 --> 00:11:41,109
guest sees if the file doesn't have the

00:11:38,709 --> 00:11:42,730
data then we go pull the cluster from

00:11:41,109 --> 00:11:46,929
the backing file at the time of the read

00:11:42,730 --> 00:11:48,399
and it's the right that does the copy so

00:11:46,929 --> 00:11:50,379
that we've have a little bit of a view

00:11:48,399 --> 00:11:52,379
of what a cuke how to file does let's

00:11:50,379 --> 00:11:54,129
look at how we can arrange it I

00:11:52,379 --> 00:11:55,959
mentioned both internal and external

00:11:54,129 --> 00:11:58,899
snapshots let's compare them with

00:11:55,959 --> 00:12:00,669
internal snapshots you have one file

00:11:58,899 --> 00:12:04,179
contains everything related to the guess

00:12:00,669 --> 00:12:06,819
it's you can even include live VN state

00:12:04,179 --> 00:12:09,339
take a snapshot of running guest move

00:12:06,819 --> 00:12:10,989
the file to a different machine reload

00:12:09,339 --> 00:12:12,910
that state and your guests resumes

00:12:10,989 --> 00:12:18,249
execution as if it had never shut down

00:12:12,910 --> 00:12:20,350
except for the clock skew reverting

00:12:18,249 --> 00:12:23,199
between snapshots is easy it was one of

00:12:20,350 --> 00:12:25,869
the first operations for snapshot revert

00:12:23,199 --> 00:12:27,999
that Q mu added and therefore lib bird

00:12:25,869 --> 00:12:30,970
added if you use verte manager that's

00:12:27,999 --> 00:12:32,619
what you get there's no I'll penalties

00:12:30,970 --> 00:12:36,519
to your active state versus your

00:12:32,619 --> 00:12:38,559
inactive state your active image those

00:12:36,519 --> 00:12:43,329
clusters are immediately accessible the

00:12:38,559 --> 00:12:45,160
only time you have to do anything as if

00:12:43,329 --> 00:12:46,629
you write to a cluster you have to copy

00:12:45,160 --> 00:12:49,919
it but that's a one-time operation

00:12:46,629 --> 00:12:53,109
because your copy is now ref count 1

00:12:49,919 --> 00:12:56,559
there's some disadvantages you're

00:12:53,109 --> 00:13:00,480
backing snapshots cannot be read while

00:12:56,559 --> 00:13:03,569
Kumu is running at least not yet there's

00:13:00,480 --> 00:13:06,100
ideas of how we could expose that but

00:13:03,569 --> 00:13:07,569
right now if you have a guest running

00:13:06,100 --> 00:13:09,400
you cannot get at the data of the

00:13:07,569 --> 00:13:11,830
snapshot

00:13:09,400 --> 00:13:14,080
qmp internal snapshot management is

00:13:11,830 --> 00:13:15,760
inefficient we don't have quite as many

00:13:14,080 --> 00:13:18,820
commands available some of the commands

00:13:15,760 --> 00:13:21,310
require you to take live vm state at the

00:13:18,820 --> 00:13:23,410
same time as your disc state some of

00:13:21,310 --> 00:13:28,150
them still use hmp we haven't mapped

00:13:23,410 --> 00:13:31,390
them to qmp and it is what it is q ko

00:13:28,150 --> 00:13:35,290
Sai's if you use a lot of internal

00:13:31,390 --> 00:13:38,260
snapshots your hosts file can exceed the

00:13:35,290 --> 00:13:40,060
size of the guest sees by multiplying

00:13:38,260 --> 00:13:43,270
factor of however many snaps watch you

00:13:40,060 --> 00:13:45,820
take it's provided that or as their

00:13:43,270 --> 00:13:47,890
snapshots start to diverge and your ref

00:13:45,820 --> 00:13:50,910
counts have to be your clusters have to

00:13:47,890 --> 00:13:52,780
be cloned your file can grow quite large

00:13:50,910 --> 00:13:57,820
another disadvantage there's no

00:13:52,780 --> 00:13:59,860
defragmentation q Cal to version 3 added

00:13:57,820 --> 00:14:03,340
the ability to mark image blocks and

00:13:59,860 --> 00:14:05,290
clusters sparse and with newer kernels

00:14:03,340 --> 00:14:07,870
you can punch holes back in your file so

00:14:05,290 --> 00:14:11,290
on file system when you throw away a

00:14:07,870 --> 00:14:13,420
snapshot you can reclaim the space but

00:14:11,290 --> 00:14:14,920
if you're mapping your qk file directly

00:14:13,420 --> 00:14:16,840
on a block device there's no way to

00:14:14,920 --> 00:14:20,050
punch a hole in your block device so it

00:14:16,840 --> 00:14:22,420
can get a bit expensive if you can't

00:14:20,050 --> 00:14:27,220
reclaim some of your space and repack

00:14:22,420 --> 00:14:30,220
the file to be more contiguous on the

00:14:27,220 --> 00:14:32,170
other hand external snapshots we have a

00:14:30,220 --> 00:14:34,180
lot of development on it a lot of new

00:14:32,170 --> 00:14:36,310
features and we're continuing to develop

00:14:34,180 --> 00:14:38,530
new features some of the other talks in

00:14:36,310 --> 00:14:41,590
this presentation have have mentioned

00:14:38,530 --> 00:14:45,670
those as well live backup is easy to do

00:14:41,590 --> 00:14:49,450
storage migration as possible building

00:14:45,670 --> 00:14:51,910
blocks we focused on trying to isolate

00:14:49,450 --> 00:14:53,470
things down the smallest pieces possible

00:14:51,910 --> 00:14:55,570
rather than doing five things in one

00:14:53,470 --> 00:14:57,220
command have five separate commands than

00:14:55,570 --> 00:15:00,730
the application can combine them how it

00:14:57,220 --> 00:15:04,120
sees fit external snapshots are great

00:15:00,730 --> 00:15:07,510
for thin provisioning create a common

00:15:04,120 --> 00:15:10,090
base image and then every other vm and

00:15:07,510 --> 00:15:12,400
your cluster references that base image

00:15:10,090 --> 00:15:14,650
and puts a cuke outer wrapper on top for

00:15:12,400 --> 00:15:18,910
the few clusters that it needs to

00:15:14,650 --> 00:15:21,250
diverge to be its own guest on the other

00:15:18,910 --> 00:15:23,050
hand with external snapshots you're now

00:15:21,250 --> 00:15:25,170
managing multiple files

00:15:23,050 --> 00:15:28,089
deleting snapshots is a bit tricky

00:15:25,170 --> 00:15:29,800
Libert still hasn't implemented the

00:15:28,089 --> 00:15:32,740
commands to do it natively so you have

00:15:29,800 --> 00:15:35,950
to drop into rock you a new image calls

00:15:32,740 --> 00:15:38,260
yourself and then update Libert to have

00:15:35,950 --> 00:15:41,079
a consistent world view and be nice if

00:15:38,260 --> 00:15:44,050
Lewbert can track it all together one of

00:15:41,079 --> 00:15:46,060
the other disadvantages is that the more

00:15:44,050 --> 00:15:49,600
snapshots you take the longer your i/o

00:15:46,060 --> 00:15:52,930
chain is like I said for every cluster

00:15:49,600 --> 00:15:54,880
that q mal humor reads you have to find

00:15:52,930 --> 00:15:56,829
the first file on the backing chain that

00:15:54,880 --> 00:15:58,839
contains that cluster if you have a lot

00:15:56,829 --> 00:16:01,450
of files that's a lot of i/o the first

00:15:58,839 --> 00:16:06,040
time through camula does some caching

00:16:01,450 --> 00:16:08,589
caching helps but there are benefits to

00:16:06,040 --> 00:16:10,690
having a shorter backing chain so

00:16:08,589 --> 00:16:15,399
therefore we need operations on our

00:16:10,690 --> 00:16:17,079
backing chain a lot of times on the

00:16:15,399 --> 00:16:18,970
mailing list and also in my presentation

00:16:17,079 --> 00:16:21,880
you'll see backing chain notation a left

00:16:18,970 --> 00:16:25,450
arrow B means that image a is the

00:16:21,880 --> 00:16:30,070
backing image for image be image B is

00:16:25,450 --> 00:16:31,660
the top or the active layer all use some

00:16:30,070 --> 00:16:33,970
examples based on the diagrams I did

00:16:31,660 --> 00:16:35,920
earlier in the talk we also have a nice

00:16:33,970 --> 00:16:40,480
new command added recently human image

00:16:35,920 --> 00:16:43,060
map tells you about the file so with my

00:16:40,480 --> 00:16:46,510
earlier file we had three sectors

00:16:43,060 --> 00:16:50,500
allocated to sectors that were written

00:16:46,510 --> 00:16:52,930
from rap qko found at this offset and a

00:16:50,500 --> 00:16:56,529
third sector that we read from based q2

00:16:52,930 --> 00:16:58,959
at a different offset all at the offsets

00:16:56,529 --> 00:17:00,670
that the guests would see them and if

00:16:58,959 --> 00:17:05,770
it's not listed in the map and you can

00:17:00,670 --> 00:17:08,470
assume the clusters Rida 0 a good way to

00:17:05,770 --> 00:17:11,559
think of backing chains is points in

00:17:08,470 --> 00:17:13,870
time if we have a backs be back see

00:17:11,559 --> 00:17:16,480
there are two points in time that we

00:17:13,870 --> 00:17:19,419
capture a snapshot and then an active

00:17:16,480 --> 00:17:22,630
layer the first point was when we

00:17:19,419 --> 00:17:27,100
created image B we captured the state

00:17:22,630 --> 00:17:29,770
and that state lives in file a then

00:17:27,100 --> 00:17:31,650
point2 we captured the state at the

00:17:29,770 --> 00:17:34,780
point in time where we created file see

00:17:31,650 --> 00:17:35,920
but that state lives in the combination

00:17:34,780 --> 00:17:38,380
of file a and B

00:17:35,920 --> 00:17:39,970
then our active layers the combination

00:17:38,380 --> 00:17:43,980
of the state through the entire chain of

00:17:39,970 --> 00:17:46,660
a B and C with your naming if you name

00:17:43,980 --> 00:17:49,180
if you create a file and name it after

00:17:46,660 --> 00:17:52,060
the time stamp that gets a little bit

00:17:49,180 --> 00:17:56,410
confusing because the time stamp is of

00:17:52,060 --> 00:17:58,360
the file is not the data at that point

00:17:56,410 --> 00:18:01,480
in time it is the change in the data

00:17:58,360 --> 00:18:04,060
from that point in time the time stamp

00:18:01,480 --> 00:18:09,220
for the snapshot is the data in the

00:18:04,060 --> 00:18:13,690
backing files so being careful how you

00:18:09,220 --> 00:18:16,480
name your files if you use timestamps be

00:18:13,690 --> 00:18:18,910
careful that it's not that the time if I

00:18:16,480 --> 00:18:20,440
name a file Tuesday it doesn't mean that

00:18:18,910 --> 00:18:22,210
it contains the data as it was on

00:18:20,440 --> 00:18:25,240
Tuesday contains the changes since

00:18:22,210 --> 00:18:31,780
Tuesday read the backing chain to see

00:18:25,240 --> 00:18:35,890
what Tuesday had a rule of thumb do not

00:18:31,780 --> 00:18:38,500
change your backing files I know on the

00:18:35,890 --> 00:18:41,200
mailing list I get questions can i

00:18:38,500 --> 00:18:43,180
update my base image and all my thin

00:18:41,200 --> 00:18:47,410
provision guests will magically see my

00:18:43,180 --> 00:18:50,850
new content if you want that use overlay

00:18:47,410 --> 00:18:53,350
FS use multiple devices update this

00:18:50,850 --> 00:18:55,660
update the files in your base file

00:18:53,350 --> 00:18:59,320
system and then your overlay files

00:18:55,660 --> 00:19:02,410
systems will pick up the differences but

00:18:59,320 --> 00:19:04,030
that's not what backing chains do if you

00:19:02,410 --> 00:19:07,770
modify your backing chain that is almost

00:19:04,030 --> 00:19:10,300
a guaranteed recipe for guest corruption

00:19:07,770 --> 00:19:12,460
so let's see what happens I'm going to

00:19:10,300 --> 00:19:14,770
ride into my common image I have guessed

00:19:12,460 --> 00:19:18,400
one I guess to both provisioned off of

00:19:14,770 --> 00:19:22,750
my common image and I've also shown what

00:19:18,400 --> 00:19:25,600
the guest sees and i'm going to write

00:19:22,750 --> 00:19:27,220
four clusters at the beginning of the

00:19:25,600 --> 00:19:29,070
file on the hopes that my guest will

00:19:27,220 --> 00:19:32,020
magically start using this new data

00:19:29,070 --> 00:19:34,810
except that guest one has clusters

00:19:32,020 --> 00:19:36,760
locally those clusters take priority

00:19:34,810 --> 00:19:39,720
over the ones i just wrote so guests

00:19:36,760 --> 00:19:43,330
once these data that did not exist

00:19:39,720 --> 00:19:46,090
guest2 had clusters in they override

00:19:43,330 --> 00:19:48,860
what my common bag vintage guest to now

00:19:46,090 --> 00:19:52,529
sees data that did not exist

00:19:48,860 --> 00:19:54,029
if you get really unlucky your guests

00:19:52,529 --> 00:19:56,370
will continue operating and you won't

00:19:54,029 --> 00:19:58,049
see this corruption for several days or

00:19:56,370 --> 00:19:59,760
weeks and then you're really hosed

00:19:58,049 --> 00:20:02,159
because it's hard to pinpoint why you

00:19:59,760 --> 00:20:04,250
corrupted yourself but the rule of thumb

00:20:02,159 --> 00:20:06,390
is never modify an image vacuum

00:20:04,250 --> 00:20:07,980
externally while cameras running it and

00:20:06,390 --> 00:20:10,049
never modify a backing image well I

00:20:07,980 --> 00:20:12,240
guess might be using it libera does

00:20:10,049 --> 00:20:14,700
provide some lock management demons that

00:20:12,240 --> 00:20:16,679
try to prevent you from shooting

00:20:14,700 --> 00:20:18,179
yourself in the foot they're not always

00:20:16,679 --> 00:20:20,970
perfect but they're better than no

00:20:18,179 --> 00:20:22,470
protection at all Libert manages the

00:20:20,970 --> 00:20:26,100
lock diamond it says I'm opening this

00:20:22,470 --> 00:20:27,980
file read-only therefore nobody else

00:20:26,100 --> 00:20:30,450
should be able to open it read write and

00:20:27,980 --> 00:20:35,210
protects one guest from corrupting

00:20:30,450 --> 00:20:38,159
another our first block string primitive

00:20:35,210 --> 00:20:40,289
our first block operation frame is the

00:20:38,159 --> 00:20:43,440
block stream we're going to start with

00:20:40,289 --> 00:20:46,230
chain ABC and copy or move clusters

00:20:43,440 --> 00:20:47,940
towards the top once we've done that we

00:20:46,230 --> 00:20:51,960
can rewrite the back and chain to drop

00:20:47,940 --> 00:20:54,570
any redundant files with qme 24 you

00:20:51,960 --> 00:20:56,909
could copy from anywhere in the backing

00:20:54,570 --> 00:21:00,779
chain into the active image so i'll

00:20:56,909 --> 00:21:02,539
start by taking a and b take the entire

00:21:00,779 --> 00:21:06,289
chain and pull it into the top layer

00:21:02,539 --> 00:21:09,090
that says i find any cluster that's

00:21:06,289 --> 00:21:12,149
allocated from the guest view but not

00:21:09,090 --> 00:21:14,039
allocated in the host and pull it into

00:21:12,149 --> 00:21:17,490
the host by copying it from its backing

00:21:14,039 --> 00:21:20,070
chain after I've done that I can throw

00:21:17,490 --> 00:21:22,890
away files a and B because file c has

00:21:20,070 --> 00:21:24,419
the same state as it did before but now

00:21:22,890 --> 00:21:28,279
we're not relying on our backing chain

00:21:24,419 --> 00:21:33,360
to provide that state or we can do

00:21:28,279 --> 00:21:36,120
intermediate or a single image pull file

00:21:33,360 --> 00:21:38,429
be into file see here we only have to

00:21:36,120 --> 00:21:40,409
copy two clusters and then we have to

00:21:38,429 --> 00:21:42,809
rewrite our backing chain to point to a

00:21:40,409 --> 00:21:45,779
be used to point to a so now c points to

00:21:42,809 --> 00:21:48,570
a and we've gotten rid of be out of the

00:21:45,779 --> 00:21:50,840
loop the operation is always safe

00:21:48,570 --> 00:21:53,669
because you're pulling data forward the

00:21:50,840 --> 00:21:56,549
files that you bypass or get rid of out

00:21:53,669 --> 00:21:58,169
of your chain never change contents and

00:21:56,549 --> 00:22:00,030
we've never violated our rules so the

00:21:58,169 --> 00:22:00,600
back and files that you can throw them

00:22:00,030 --> 00:22:01,830
away

00:22:00,600 --> 00:22:04,080
because you don't need them anymore bore

00:22:01,830 --> 00:22:11,130
you can keep them around as your backup

00:22:04,080 --> 00:22:13,500
for your point in time to make you doubt

00:22:11,130 --> 00:22:16,700
5 bare toes working on patches for

00:22:13,500 --> 00:22:19,050
intermediate streaming which says

00:22:16,700 --> 00:22:20,520
shorten our chain but keep our active

00:22:19,050 --> 00:22:25,650
layer the same so here we're going to

00:22:20,520 --> 00:22:27,330
copy the clusters from image a into B if

00:22:25,650 --> 00:22:30,690
B doesn't have them so two clusters

00:22:27,330 --> 00:22:34,980
later in bc's the same contents but now

00:22:30,690 --> 00:22:37,590
we can throw away file a the next fun

00:22:34,980 --> 00:22:40,410
primitive is block commit this is the

00:22:37,590 --> 00:22:42,300
opposite direction of stream we're going

00:22:40,410 --> 00:22:44,790
to copy and move clusters away from the

00:22:42,300 --> 00:22:48,930
top again we can rewrite backing data to

00:22:44,790 --> 00:22:50,790
drop redundant files block commit was

00:22:48,930 --> 00:22:53,510
one of the first block commands back in

00:22:50,790 --> 00:22:57,330
king 13 but it could only do

00:22:53,510 --> 00:23:00,420
intermediate files we could move data

00:22:57,330 --> 00:23:04,680
from be any cluster that was written in

00:23:00,420 --> 00:23:08,070
V we could move into file a now the

00:23:04,680 --> 00:23:11,160
contents visible in a have changed we no

00:23:08,070 --> 00:23:14,040
longer have the state in time for where

00:23:11,160 --> 00:23:15,510
a was thin area was marked read only and

00:23:14,040 --> 00:23:17,130
II was created we now have the state in

00:23:15,510 --> 00:23:18,900
time where be was marked read only and

00:23:17,130 --> 00:23:23,850
see created we've thrown away some state

00:23:18,900 --> 00:23:26,460
in time we also rewrite the backing file

00:23:23,850 --> 00:23:28,320
for C to point to a because that's the

00:23:26,460 --> 00:23:30,990
same contents that it was getting from B

00:23:28,320 --> 00:23:37,100
and we can throw be away our chain is

00:23:30,990 --> 00:23:39,930
now shorter Kumu 20 added active commit

00:23:37,100 --> 00:23:44,850
now we can also do the same thing where

00:23:39,930 --> 00:23:47,400
we commit see in to be and now this

00:23:44,850 --> 00:23:48,810
point in time that be tracks is the

00:23:47,400 --> 00:23:51,240
point in time that we did our commit

00:23:48,810 --> 00:23:55,560
operation we thrown away the earlier

00:23:51,240 --> 00:23:57,450
point in time that attract but if we

00:23:55,560 --> 00:24:00,000
want we can throw away a file we can

00:23:57,450 --> 00:24:03,120
make be become the active file the guest

00:24:00,000 --> 00:24:07,050
never sees a change in data or we could

00:24:03,120 --> 00:24:08,880
keep see alive and just track and again

00:24:07,050 --> 00:24:10,980
see is tracking the Delta from the point

00:24:08,880 --> 00:24:11,720
in time so the point in time is now when

00:24:10,980 --> 00:24:13,850
we did the committee

00:24:11,720 --> 00:24:19,250
and we could keep see alive and track a

00:24:13,850 --> 00:24:22,760
new set of Delta or we can do multi file

00:24:19,250 --> 00:24:26,539
commits commits see all the way in the

00:24:22,760 --> 00:24:29,360
image a make a the active image but

00:24:26,539 --> 00:24:32,150
here's my caveat we modified what the

00:24:29,360 --> 00:24:37,220
backing image seized the moment we wrote

00:24:32,150 --> 00:24:39,169
cluster C into backing file a as long as

00:24:37,220 --> 00:24:42,260
we're looking at see the guest data is

00:24:39,169 --> 00:24:44,539
fine as long as we're looking at a the

00:24:42,260 --> 00:24:48,799
guest data is unchanged but if we look

00:24:44,539 --> 00:24:50,960
at file be we have data that never

00:24:48,799 --> 00:24:53,840
appeared in the actual state the guests

00:24:50,960 --> 00:24:56,240
aw so it becomes important that as you

00:24:53,840 --> 00:24:57,890
do multi file steps remember what you

00:24:56,240 --> 00:25:01,460
might be corrupting and be sure to

00:24:57,890 --> 00:25:03,470
delete those files or take other

00:25:01,460 --> 00:25:05,720
preventive measures so that you don't

00:25:03,470 --> 00:25:09,799
try to revert to state that's no longer

00:25:05,720 --> 00:25:13,760
valid in fact there's been talk on the

00:25:09,799 --> 00:25:15,650
mailing list of ideas of when adding a

00:25:13,760 --> 00:25:18,110
flag to the commit operation so that

00:25:15,650 --> 00:25:21,770
when we say we're about to write C into

00:25:18,110 --> 00:25:23,870
a but we also check the overlay oh

00:25:21,770 --> 00:25:27,260
there's nothing there we need to copy

00:25:23,870 --> 00:25:29,690
the previous data from a into B so that

00:25:27,260 --> 00:25:32,360
B stays consistent no matter what we've

00:25:29,690 --> 00:25:35,330
done to a and then we can cleanly detach

00:25:32,360 --> 00:25:38,200
be from the chain and merge it all onto

00:25:35,330 --> 00:25:38,200
one operation

00:25:43,610 --> 00:25:48,930
the question is can I do an intermediate

00:25:46,170 --> 00:25:51,480
stream from be from a and 2 b and then

00:25:48,930 --> 00:25:53,160
do my commits from scene to a yes but

00:25:51,480 --> 00:25:54,810
doing it as two steps is less efficient

00:25:53,160 --> 00:25:57,330
than doing it is a one-step operation

00:25:54,810 --> 00:26:00,510
that does also it's just talk talk of a

00:25:57,330 --> 00:26:07,950
possible optimization another possible

00:26:00,510 --> 00:26:10,410
possible optimization is right now when

00:26:07,950 --> 00:26:12,840
you write your images to your backing

00:26:10,410 --> 00:26:15,510
file the images the clusters are still

00:26:12,840 --> 00:26:17,520
written in the base are now written in

00:26:15,510 --> 00:26:20,940
two places the backing file and the

00:26:17,520 --> 00:26:24,510
active file so the optimization would be

00:26:20,940 --> 00:26:26,610
as we commit files and clusters down

00:26:24,510 --> 00:26:28,470
into the backing image can we also clear

00:26:26,610 --> 00:26:31,890
those clusters from the active image so

00:26:28,470 --> 00:26:33,840
the active image can shrink in disk size

00:26:31,890 --> 00:26:36,420
and we're not duplicating clusters in

00:26:33,840 --> 00:26:38,460
places so those optimizations would be

00:26:36,420 --> 00:26:43,620
adoption of lags and they may be coming

00:26:38,460 --> 00:26:45,600
down the pipeline there's the efficiency

00:26:43,620 --> 00:26:48,300
question if we want to remove the second

00:26:45,600 --> 00:26:50,280
point in time from ABCD remember the

00:26:48,300 --> 00:26:53,430
second point in time is not necessarily

00:26:50,280 --> 00:26:55,950
removing file be but it's remembering

00:26:53,430 --> 00:26:58,950
that the point in time was where we

00:26:55,950 --> 00:27:02,010
created file see and have the contents

00:26:58,950 --> 00:27:06,750
from a plus B so there are two ways to

00:27:02,010 --> 00:27:09,330
do that one is to pull or stream the

00:27:06,750 --> 00:27:11,220
contents of be in to see now B is

00:27:09,330 --> 00:27:16,020
redundant so we have the new chain a to

00:27:11,220 --> 00:27:18,990
c prime daddy or we can commit see in to

00:27:16,020 --> 00:27:22,110
be and now c becomes redundant we have

00:27:18,990 --> 00:27:24,000
the chain a to b prime to be both

00:27:22,110 --> 00:27:25,560
operations have their merit it depends

00:27:24,000 --> 00:27:27,360
on how long you've been running the

00:27:25,560 --> 00:27:30,870
delta and how much data the guest has

00:27:27,360 --> 00:27:32,370
been generating the cumin image map

00:27:30,870 --> 00:27:34,340
command can help you figure out how many

00:27:32,370 --> 00:27:37,340
clusters would i be moving by doing

00:27:34,340 --> 00:27:40,920
operation in one direction to the other

00:27:37,340 --> 00:27:42,630
and again some of it depends on canvas

00:27:40,920 --> 00:27:45,660
stream does not support intermediate

00:27:42,630 --> 00:27:47,760
streams until 25 so it's still a work in

00:27:45,660 --> 00:27:49,200
progress to even expose the right

00:27:47,760 --> 00:27:51,540
interface to the user to quickly

00:27:49,200 --> 00:27:53,010
determine which is my most efficient but

00:27:51,540 --> 00:27:54,160
that is something we hope to be able to

00:27:53,010 --> 00:27:57,010
provide is

00:27:54,160 --> 00:27:58,930
helping a management application choose

00:27:57,010 --> 00:28:01,900
which of the two directions is more

00:27:58,930 --> 00:28:04,500
efficient another point of efficiency

00:28:01,900 --> 00:28:07,180
the question earlier about can you

00:28:04,500 --> 00:28:09,190
stream first and then commit that's two

00:28:07,180 --> 00:28:11,940
operations versus one operation that

00:28:09,190 --> 00:28:14,830
does it all also anytime you're doing

00:28:11,940 --> 00:28:16,840
operation if you do it from a into BB

00:28:14,830 --> 00:28:19,240
into c c and d you get consistent

00:28:16,840 --> 00:28:21,130
results but you may be moving clusters

00:28:19,240 --> 00:28:25,330
multiple times whereas if you go from a

00:28:21,130 --> 00:28:26,830
straight into d you've minimized the

00:28:25,330 --> 00:28:29,910
number of close to the shifting those so

00:28:26,830 --> 00:28:32,950
multi-step operations can add efficiency

00:28:29,910 --> 00:28:34,510
but they also add the risk of leaving an

00:28:32,950 --> 00:28:37,650
image in an inconsistent state if you

00:28:34,510 --> 00:28:37,650
aren't careful where your data is going

00:28:38,280 --> 00:28:46,200
another primitive is the drive mirror

00:28:41,520 --> 00:28:49,540
this is where we take one image and

00:28:46,200 --> 00:28:51,640
while the guest is running copy the

00:28:49,540 --> 00:28:55,690
active layer into a destination of the

00:28:51,640 --> 00:28:59,290
guest choosing we can either do a deep

00:28:55,690 --> 00:29:01,360
copy where the guest where the copy

00:28:59,290 --> 00:29:04,510
created sees everything that the guests

00:29:01,360 --> 00:29:06,400
saw or a shallow copy I'm going to start

00:29:04,510 --> 00:29:10,150
with a deep copy in fact i'm going to

00:29:06,400 --> 00:29:13,030
start this operation in block stream or

00:29:10,150 --> 00:29:16,000
driving error drive mirror is an

00:29:13,030 --> 00:29:17,680
asynchronous operation where it does the

00:29:16,000 --> 00:29:19,570
clusters in the background while the

00:29:17,680 --> 00:29:21,780
guests continues to run but then the

00:29:19,570 --> 00:29:24,580
guests can interrupt things that say i

00:29:21,780 --> 00:29:26,830
want to write clusters myself the guests

00:29:24,580 --> 00:29:28,660
always takes priority so we're in the

00:29:26,830 --> 00:29:31,120
middle of the cluster our copies not yet

00:29:28,660 --> 00:29:34,090
complete but the guest wrote some new

00:29:31,120 --> 00:29:36,280
clusters so part of our straining our

00:29:34,090 --> 00:29:39,250
part of our mirroring operation is the

00:29:36,280 --> 00:29:41,050
clusters go into both files and then

00:29:39,250 --> 00:29:43,990
when we resume the operation because we

00:29:41,050 --> 00:29:47,050
have some down time again we are only

00:29:43,990 --> 00:29:49,570
copying files into the copy if they

00:29:47,050 --> 00:29:54,160
haven't already been written by the

00:29:49,570 --> 00:29:55,780
guest but after the operation settles

00:29:54,160 --> 00:29:58,150
and it may be seconds it may be minutes

00:29:55,780 --> 00:30:02,010
but once the operation settles the two

00:29:58,150 --> 00:30:05,820
files are in sync and we can

00:30:02,010 --> 00:30:08,910
break the operation we can either keep

00:30:05,820 --> 00:30:11,340
the active layer untouched break the

00:30:08,910 --> 00:30:14,430
coffee the copy is now a point in time

00:30:11,340 --> 00:30:16,620
backup or we can pivot and say our

00:30:14,430 --> 00:30:18,570
active and base chain are no longer in

00:30:16,620 --> 00:30:20,910
use there now the backup and our copy

00:30:18,570 --> 00:30:23,220
becomes the new active image so it's

00:30:20,910 --> 00:30:26,250
nice that we can take the the file and

00:30:23,220 --> 00:30:28,140
go either direction that was a deep

00:30:26,250 --> 00:30:31,710
coffee we can also do shallow copies by

00:30:28,140 --> 00:30:34,280
creating pre creating the destination

00:30:31,710 --> 00:30:37,500
file to be a queue calc wrapper around

00:30:34,280 --> 00:30:40,140
the same content as the base file well

00:30:37,500 --> 00:30:42,900
there's the same base file or whether

00:30:40,140 --> 00:30:44,580
it's a copy of the base file you have

00:30:42,900 --> 00:30:47,850
some flexibility as long as it's the

00:30:44,580 --> 00:30:51,090
same content the guests would see and

00:30:47,850 --> 00:30:53,100
there again the operation is

00:30:51,090 --> 00:30:55,770
asynchronous in that we copy any

00:30:53,100 --> 00:30:57,150
clusters from the shallow image we leave

00:30:55,770 --> 00:30:58,830
the clusters from the backing image

00:30:57,150 --> 00:31:01,890
unchanged because we can already see

00:30:58,830 --> 00:31:04,710
those as they are if guest data comes in

00:31:01,890 --> 00:31:08,100
the middle we clone the guest data

00:31:04,710 --> 00:31:10,980
between the two points and then when the

00:31:08,100 --> 00:31:13,260
image is finally synchronized we can

00:31:10,980 --> 00:31:16,170
once again choose whether to keep the

00:31:13,260 --> 00:31:20,250
original or keep the copy as we break

00:31:16,170 --> 00:31:22,590
the synchronization and drive backup is

00:31:20,250 --> 00:31:27,330
similar it's more recent added to the

00:31:22,590 --> 00:31:30,600
tree that says start a point in time so

00:31:27,330 --> 00:31:32,490
drive backup started the point in time

00:31:30,600 --> 00:31:34,980
for drive backup is when you break the

00:31:32,490 --> 00:31:36,750
synchronization the point in time for

00:31:34,980 --> 00:31:41,130
drive backup is when you start the

00:31:36,750 --> 00:31:43,320
operation here we'll start the same

00:31:41,130 --> 00:31:45,780
partial operation we start copying

00:31:43,320 --> 00:31:48,270
clusters from the guests into our copy

00:31:45,780 --> 00:31:50,400
and then the guest comes along and says

00:31:48,270 --> 00:31:52,770
I want to write data but instead of

00:31:50,400 --> 00:31:55,230
copying the data into our active and our

00:31:52,770 --> 00:31:57,300
copy we say anywhere that the data

00:31:55,230 --> 00:32:00,060
touches something that the coffee

00:31:57,300 --> 00:32:03,180
doesn't have we need to write the old

00:32:00,060 --> 00:32:05,430
data into the copy and the new data only

00:32:03,180 --> 00:32:08,490
into the active image so the active

00:32:05,430 --> 00:32:10,710
image is always consistent the old image

00:32:08,490 --> 00:32:14,220
by the end of the operation the old

00:32:10,710 --> 00:32:16,289
image becomes the point in time snapshot

00:32:14,220 --> 00:32:17,909
and that point in time is when we

00:32:16,289 --> 00:32:20,039
started the command it may take several

00:32:17,909 --> 00:32:23,400
seconds or minutes for the backup to

00:32:20,039 --> 00:32:25,980
complete but as long as q mu is running

00:32:23,400 --> 00:32:26,880
that point that data is available and

00:32:25,980 --> 00:32:28,770
because we're running a saying

00:32:26,880 --> 00:32:30,750
asynchronously we will eventually reach

00:32:28,770 --> 00:32:37,799
the point where your copy is flushed and

00:32:30,750 --> 00:32:39,630
you have a safe point in time we also

00:32:37,799 --> 00:32:43,110
have incremental backup coming down the

00:32:39,630 --> 00:32:45,419
pipeline for kme 25 where management can

00:32:43,110 --> 00:32:49,860
track one or more dirty bitmaps

00:32:45,419 --> 00:32:53,159
associated with each image dirty bitmaps

00:32:49,860 --> 00:32:57,090
are the workhorse behind drive mirror

00:32:53,159 --> 00:32:59,150
and drive back up but we are now making

00:32:57,090 --> 00:33:01,679
them a first classes and back to my

00:32:59,150 --> 00:33:03,570
comments earlier about trying to break

00:33:01,679 --> 00:33:05,850
these operations into the smallest piece

00:33:03,570 --> 00:33:08,809
as possible so it management can pull

00:33:05,850 --> 00:33:11,880
them together in fun and unique ways

00:33:08,809 --> 00:33:16,429
dirty bitmaps will allow you to get the

00:33:11,880 --> 00:33:19,409
same benefits of q how to copy on write

00:33:16,429 --> 00:33:20,970
even when running with a raw image or

00:33:19,409 --> 00:33:24,179
something else that does not natively

00:33:20,970 --> 00:33:26,460
support copy-on-write another benefit of

00:33:24,179 --> 00:33:28,679
dirty bitmaps is because you control the

00:33:26,460 --> 00:33:31,440
bitmap directly you're tracking which

00:33:28,679 --> 00:33:34,350
clusters are written without having to

00:33:31,440 --> 00:33:36,030
have a backing chain so I said earlier

00:33:34,350 --> 00:33:38,400
that the longer you're backing chain is

00:33:36,030 --> 00:33:41,130
along the more I owe penalty you have as

00:33:38,400 --> 00:33:43,169
Kuhn has to chase the backing chain with

00:33:41,130 --> 00:33:45,000
dirty bitmaps you don't need the backing

00:33:43,169 --> 00:33:50,010
chain but you can still track the points

00:33:45,000 --> 00:33:51,510
in time finally let's look into what

00:33:50,010 --> 00:33:55,340
some of these operations can do under

00:33:51,510 --> 00:33:59,820
libvirt if you use modern limbert and

00:33:55,340 --> 00:34:02,760
get the guest dump the guest XML you

00:33:59,820 --> 00:34:09,000
will notice that any backing chick man

00:34:02,760 --> 00:34:11,040
wrong button any backing chain in your

00:34:09,000 --> 00:34:14,220
guest description will be listed by a

00:34:11,040 --> 00:34:18,419
nested list of disk as a backing store

00:34:14,220 --> 00:34:22,500
has a backing store so I have a chain of

00:34:18,419 --> 00:34:24,780
my base image is the backing of my rap

00:34:22,500 --> 00:34:27,220
image is the backing of my rap to image

00:34:24,780 --> 00:34:31,570
we can refer to those in

00:34:27,220 --> 00:34:36,190
Edge's by their device name libvirt will

00:34:31,570 --> 00:34:39,669
take the device name vda and the index

00:34:36,190 --> 00:34:43,030
one to refer to whatever backing store

00:34:39,669 --> 00:34:45,159
is at index 1 or you can refer to it by

00:34:43,030 --> 00:34:47,530
file name on the cases where yours using

00:34:45,159 --> 00:34:50,980
file if you're using network like

00:34:47,530 --> 00:34:52,450
gluster or seif it becomes harder to

00:34:50,980 --> 00:34:54,389
come up with a unique file name and

00:34:52,450 --> 00:34:57,790
therefore that's why we came up with the

00:34:54,389 --> 00:34:59,890
reference by index then q a mood to

00:34:57,790 --> 00:35:03,160
liberate aches care of all the magic

00:34:59,890 --> 00:35:05,170
needed to map the name from your xml

00:35:03,160 --> 00:35:12,369
into whatever the underlying string is

00:35:05,170 --> 00:35:14,290
that kumu needs to see creating a

00:35:12,369 --> 00:35:16,060
snapshot so making a chain grow larger

00:35:14,290 --> 00:35:21,640
libvirt has supported this through the

00:35:16,060 --> 00:35:23,800
verdot main snapshot create xml you can

00:35:21,640 --> 00:35:26,640
package all your creation instructions

00:35:23,800 --> 00:35:29,770
into an xml file with snapshot create or

00:35:26,640 --> 00:35:31,839
if it's a simple enough operation we

00:35:29,770 --> 00:35:34,599
created command my sugar snapshot create

00:35:31,839 --> 00:35:37,480
as and spell out the individual options

00:35:34,599 --> 00:35:39,250
and then the command line is smart

00:35:37,480 --> 00:35:42,750
enough to convert that under the hood to

00:35:39,250 --> 00:35:45,520
the xml that libert wants to see and

00:35:42,750 --> 00:35:49,690
maps to the km you block dev snapshot

00:35:45,520 --> 00:35:51,970
sync command common flags that you'll

00:35:49,690 --> 00:35:55,510
see in various scripts for using these

00:35:51,970 --> 00:35:57,339
the no metadata flag says create the

00:35:55,510 --> 00:36:00,339
side effect of changing the backing

00:35:57,339 --> 00:36:03,040
chain and updating the xml to track the

00:36:00,339 --> 00:36:05,440
new file but don't track it in liberty

00:36:03,040 --> 00:36:08,470
as i said earlier Libert is great at

00:36:05,440 --> 00:36:11,320
tracking internal snapshots not so great

00:36:08,470 --> 00:36:12,970
a tracking external snapshots we're

00:36:11,320 --> 00:36:16,270
getting there but it'll take some more

00:36:12,970 --> 00:36:17,859
patches and some more love and effort so

00:36:16,270 --> 00:36:20,140
in the meantime if you create a snapshot

00:36:17,859 --> 00:36:21,160
liberabit tracking this metadata and

00:36:20,140 --> 00:36:22,839
then you go change something behind

00:36:21,160 --> 00:36:25,450
liberals back then you have to clean up

00:36:22,839 --> 00:36:27,750
that metadata so just pass the gnome

00:36:25,450 --> 00:36:31,450
attitude a flag and track it yourself

00:36:27,750 --> 00:36:33,310
another one is the quiesce flag if you

00:36:31,450 --> 00:36:36,760
have the guest station installed on your

00:36:33,310 --> 00:36:39,220
guest then quiesce says please freeze

00:36:36,760 --> 00:36:41,170
all guest io make the guest disc

00:36:39,220 --> 00:36:44,220
consistent before I take my stand

00:36:41,170 --> 00:36:46,690
shot that way if I revert to my snapshot

00:36:44,220 --> 00:36:49,480
the guest was in a consistent state i

00:36:46,690 --> 00:36:52,180
can start running right away and if you

00:36:49,480 --> 00:36:54,220
don't use the quiesce flag the snapshot

00:36:52,180 --> 00:36:55,869
you take is akin to what you would have

00:36:54,220 --> 00:36:58,930
on your disk if you yanked the power

00:36:55,869 --> 00:37:00,430
plug files make be inconsistent and

00:36:58,930 --> 00:37:01,960
hopefully you had journaling installed

00:37:00,430 --> 00:37:07,720
so that you didn't completely mess up

00:37:01,960 --> 00:37:10,569
your data block pole or block stream

00:37:07,720 --> 00:37:14,619
libera uses the term pole kim uses the

00:37:10,569 --> 00:37:16,809
term stream they're synonyms and then

00:37:14,619 --> 00:37:19,690
liberate yets adds yet another name is

00:37:16,809 --> 00:37:22,780
the block rebase api naming is

00:37:19,690 --> 00:37:26,200
historical the fact of history we have

00:37:22,780 --> 00:37:28,089
to live with it right now since q mu

00:37:26,200 --> 00:37:30,160
only supports polling to the active

00:37:28,089 --> 00:37:33,640
layer liberal only supports it too but

00:37:30,160 --> 00:37:36,520
as King Lear 25 ads intermediate

00:37:33,640 --> 00:37:39,579
streaming we plan to reuse the same API

00:37:36,520 --> 00:37:43,619
and then take advantage of our index

00:37:39,579 --> 00:37:46,829
notation so we could merge from vda one

00:37:43,619 --> 00:37:49,119
from its base of vda three to do a

00:37:46,829 --> 00:37:53,859
backing chain commit along the

00:37:49,119 --> 00:37:57,549
intermediate images block commit there

00:37:53,859 --> 00:38:03,359
we at least named the API right some

00:37:57,549 --> 00:38:06,849
example usage we can take dist vda and

00:38:03,359 --> 00:38:11,559
commit the top image of vda one down

00:38:06,849 --> 00:38:12,849
into its base image since q mu has

00:38:11,559 --> 00:38:14,500
slightly different behavior whether

00:38:12,849 --> 00:38:16,119
you're doing a backing chain commit or

00:38:14,500 --> 00:38:18,849
an active commit remember with active

00:38:16,119 --> 00:38:21,970
commit you have to decide whether to

00:38:18,849 --> 00:38:26,319
keep the original chain or pivot over to

00:38:21,970 --> 00:38:30,280
the new chain or pivot to the now

00:38:26,319 --> 00:38:35,619
committed in top image Libert exposes

00:38:30,280 --> 00:38:38,309
that choice a lot of the flags if you

00:38:35,619 --> 00:38:43,150
pass shallow pivot verbose timeout 60

00:38:38,309 --> 00:38:45,520
you get a nice command line progress

00:38:43,150 --> 00:38:48,190
meter of how far the commits gone if it

00:38:45,520 --> 00:38:50,910
happens to finish within 60 seconds you

00:38:48,190 --> 00:38:53,230
then pivot if it hasn't finished then

00:38:50,910 --> 00:38:55,170
you're no worse for the wear your guest

00:38:53,230 --> 00:38:58,060
is still running

00:38:55,170 --> 00:39:00,040
but it's multiple Libert api's under the

00:38:58,060 --> 00:39:01,810
hood through the single command line and

00:39:00,040 --> 00:39:05,650
then those livered api's in turn are

00:39:01,810 --> 00:39:08,349
doing multiple q qm ooh commands so it's

00:39:05,650 --> 00:39:10,270
a nice way to roll up multiple

00:39:08,349 --> 00:39:14,650
operations under a single command line

00:39:10,270 --> 00:39:18,130
and again as q adds features we plan to

00:39:14,650 --> 00:39:22,270
mirror those features into libvirt block

00:39:18,130 --> 00:39:24,460
copy same story we have the block copy

00:39:22,270 --> 00:39:26,619
API and then when you once things are

00:39:24,460 --> 00:39:29,530
synchronized then you create your point

00:39:26,619 --> 00:39:32,349
in time by calling block job abort to

00:39:29,530 --> 00:39:36,430
either abort to the original or abort to

00:39:32,349 --> 00:39:38,349
pivot to the to the new copy right now

00:39:36,430 --> 00:39:40,150
the block copy requires a transient

00:39:38,349 --> 00:39:43,240
domain and that's because we don't have

00:39:40,150 --> 00:39:45,070
persistent bitmaps and Kamui yet but

00:39:43,240 --> 00:39:47,020
that's coming down the road also I

00:39:45,070 --> 00:39:50,140
mentioned earlier we have the drive

00:39:47,020 --> 00:39:52,390
mirror and drive back up commands both

00:39:50,140 --> 00:39:54,580
in kamu right now liberate only targets

00:39:52,390 --> 00:39:56,589
driving mirror so we have work coming

00:39:54,580 --> 00:39:59,170
down the pipeline to support both

00:39:56,589 --> 00:40:00,730
directions whether that remains with

00:39:59,170 --> 00:40:05,740
block copy or whether we create a new

00:40:00,730 --> 00:40:07,839
API remains to be seen so in the last

00:40:05,740 --> 00:40:10,359
five minutes let's see what we can do by

00:40:07,839 --> 00:40:13,000
putting these together I want to create

00:40:10,359 --> 00:40:15,609
a potentially bootable backup of the

00:40:13,000 --> 00:40:17,859
live state of my disk over the state of

00:40:15,609 --> 00:40:21,430
my disc in the guests without any guest

00:40:17,859 --> 00:40:23,230
downtime so right now my guest is

00:40:21,430 --> 00:40:26,200
running with the backing chain as a base

00:40:23,230 --> 00:40:28,750
and an image first thing I'm going to do

00:40:26,200 --> 00:40:34,800
is use snapshot create to create a

00:40:28,750 --> 00:40:38,020
rapper the name I created the name temp

00:40:34,800 --> 00:40:39,250
that name floated onto the file name and

00:40:38,020 --> 00:40:40,750
the only reason I created the name is

00:40:39,250 --> 00:40:43,510
because otherwise deliberate uses a time

00:40:40,750 --> 00:40:46,900
stamp and as i said earlier time stamps

00:40:43,510 --> 00:40:48,940
can get a bit confusing no metadata

00:40:46,900 --> 00:40:53,589
because i'm going to undo things after

00:40:48,940 --> 00:40:55,240
the end if i want to make it bootable

00:40:53,589 --> 00:40:57,250
then I add the quiesce flag that

00:40:55,240 --> 00:40:58,780
requires the guest agent not every guest

00:40:57,250 --> 00:41:02,020
has an agent slippery as flag is

00:40:58,780 --> 00:41:04,420
optional then I'm going to use a copy

00:41:02,020 --> 00:41:07,900
ref link always is awesome if you have

00:41:04,420 --> 00:41:13,390
btrfs then the copy is an in

00:41:07,900 --> 00:41:15,700
didn't operation if you have a sand

00:41:13,390 --> 00:41:18,670
command or whatever your network storage

00:41:15,700 --> 00:41:21,250
or storage solution has if you have an

00:41:18,670 --> 00:41:24,549
efficient copy this is the place to use

00:41:21,250 --> 00:41:27,760
it as soon as I've created my coffee I

00:41:24,549 --> 00:41:33,359
now have my backup image has a base of

00:41:27,760 --> 00:41:36,220
my base and then my snapshot or my

00:41:33,359 --> 00:41:39,279
backup is complete so all I have to do

00:41:36,220 --> 00:41:42,250
is clean up after myself a verse block

00:41:39,279 --> 00:41:45,660
commit take my domain take the VDA disc

00:41:42,250 --> 00:41:48,190
that I expanded and do a shallow commit

00:41:45,660 --> 00:41:50,980
pivoting back in to the original image

00:41:48,190 --> 00:41:53,440
and do it for both so I can watch it

00:41:50,980 --> 00:41:56,770
happen on the command line and then

00:41:53,440 --> 00:41:58,299
remove that temporary file and again i

00:41:56,770 --> 00:42:02,460
created the name so i would know not

00:41:58,299 --> 00:42:02,460
know what filename to delete at the end

00:42:02,730 --> 00:42:07,150
there's no guest time with fast storage

00:42:05,559 --> 00:42:10,359
array you can do this in less than a

00:42:07,150 --> 00:42:12,270
second we need a qk to rapper but only

00:42:10,359 --> 00:42:14,470
for that second that we're doing things

00:42:12,270 --> 00:42:18,130
it's pretty amazing that you can take a

00:42:14,470 --> 00:42:20,980
live backup of your guests now we have

00:42:18,130 --> 00:42:24,579
this back up how do we revert to it so i

00:42:20,980 --> 00:42:26,319
took a snapshot i created a file named

00:42:24,579 --> 00:42:29,109
my experiment which is going to be the

00:42:26,319 --> 00:42:30,849
delta since my snapshot and do something

00:42:29,109 --> 00:42:32,710
to my guest and I didn't like that

00:42:30,849 --> 00:42:36,760
experiment I want to roll back to my

00:42:32,710 --> 00:42:38,049
base image so the first thing since

00:42:36,760 --> 00:42:39,670
we're going to be rolling back we don't

00:42:38,049 --> 00:42:44,460
need the guests running anymore destroy

00:42:39,670 --> 00:42:51,309
it which in verse terminology is stop it

00:42:44,460 --> 00:42:56,250
edit the domain I'm going to take my

00:42:51,309 --> 00:43:00,250
experiment and edit it back to my bass

00:42:56,250 --> 00:43:02,079
remove the now-dead experiment and start

00:43:00,250 --> 00:43:06,720
my domain again and i've reverted to my

00:43:02,079 --> 00:43:06,720
external snapshot and

00:43:06,980 --> 00:43:10,700
if you have to keep your chain

00:43:08,810 --> 00:43:13,930
consistent you may have to throw in some

00:43:10,700 --> 00:43:15,950
cumin image commands or copy commands

00:43:13,930 --> 00:43:17,780
remembering the rule of thumb that if

00:43:15,950 --> 00:43:19,730
you ever write into a backing file that

00:43:17,780 --> 00:43:23,810
somebody else was using you've corrupted

00:43:19,730 --> 00:43:26,930
that somebody else again libert doesn't

00:43:23,810 --> 00:43:30,140
quite manage this we want to enhance

00:43:26,930 --> 00:43:31,910
that and make libvirt make it a little

00:43:30,140 --> 00:43:34,130
easier to do by hand without having to

00:43:31,910 --> 00:43:36,230
resort to doing it by hand a live

00:43:34,130 --> 00:43:38,000
storage migration let's say our storage

00:43:36,230 --> 00:43:39,710
chain is running on network storage and

00:43:38,000 --> 00:43:41,090
for efficiency we wanted on local

00:43:39,710 --> 00:43:47,380
storage but we don't want guests

00:43:41,090 --> 00:43:47,380
downtime I don't know only that profit

00:43:48,340 --> 00:43:53,660
create our snapshot similar to what

00:43:50,510 --> 00:43:58,430
we've done before use our fancy sans

00:43:53,660 --> 00:44:00,230
copy commands to copy things locally now

00:43:58,430 --> 00:44:03,609
we're going to use cumin image to create

00:44:00,230 --> 00:44:05,840
a empty wrapper around our file

00:44:03,609 --> 00:44:10,010
undefined our domain because libvirt

00:44:05,840 --> 00:44:12,830
doesn't have persistence bit mass yet do

00:44:10,010 --> 00:44:14,960
the block copy shallow block copy so

00:44:12,830 --> 00:44:17,540
that says take the data that's in our

00:44:14,960 --> 00:44:22,190
live wrapper and copy it to our

00:44:17,540 --> 00:44:24,230
destination wrapper since we undefined

00:44:22,190 --> 00:44:26,690
our domain we need to redefine it so

00:44:24,230 --> 00:44:32,359
dump the xml from the running xml define

00:44:26,690 --> 00:44:34,430
it block commit so our wrapper image

00:44:32,359 --> 00:44:37,750
gets merged back into local image local

00:44:34,430 --> 00:44:41,510
image can be raw clean up our mess and

00:44:37,750 --> 00:44:44,869
if you have a fast copy you have now

00:44:41,510 --> 00:44:47,060
migrated your storage with minimal time

00:44:44,869 --> 00:44:50,510
for the overall sequence and with no

00:44:47,060 --> 00:44:52,609
guest down time and hopefully in the

00:44:50,510 --> 00:44:54,650
future as libvirt starts to support

00:44:52,609 --> 00:44:56,540
queue and persistent bitmaps we can even

00:44:54,650 --> 00:45:00,500
drop some of those steps about undefined

00:44:56,540 --> 00:45:02,210
and redefine there's offline chain

00:45:00,500 --> 00:45:03,980
management right now libert can only

00:45:02,210 --> 00:45:06,530
show you the backing chain of a running

00:45:03,980 --> 00:45:08,900
guest we need to see what's what's there

00:45:06,530 --> 00:45:11,869
for an offline guest reverting to

00:45:08,900 --> 00:45:14,630
snapshots exposing came there to do got

00:45:11,869 --> 00:45:17,450
five editions there's still a lot of

00:45:14,630 --> 00:45:19,930
fertile ground patches are welcome but I

00:45:17,450 --> 00:45:21,400
hope that the things I've shown today

00:45:19,930 --> 00:45:23,410
can give you a feel for what's going on

00:45:21,400 --> 00:45:33,220
under the hood and how to piece it all

00:45:23,410 --> 00:45:35,260
together any questions yep when you use

00:45:33,220 --> 00:45:38,830
drive back up you have the benefit that

00:45:35,260 --> 00:45:41,140
you can do atomic back up of multiple

00:45:38,830 --> 00:45:44,050
discs where you have the same point of

00:45:41,140 --> 00:45:46,300
time for all the discs in the vm yes I

00:45:44,050 --> 00:45:48,310
didn't cover that my talk but one of the

00:45:46,300 --> 00:45:50,170
point of the qml commands is there is a

00:45:48,310 --> 00:45:52,870
transaction command that can group

00:45:50,170 --> 00:45:54,190
multiple things together so when you're

00:45:52,870 --> 00:45:57,100
doing drive back up you could use a

00:45:54,190 --> 00:45:59,050
transaction to group multiple drive back

00:45:57,100 --> 00:46:03,820
up so all of the drives are at the same

00:45:59,050 --> 00:46:06,310
point in time yeah so is there some

00:46:03,820 --> 00:46:09,960
trick to do atomic back up with dr me go

00:46:06,310 --> 00:46:13,090
I don't remember libvirt can do

00:46:09,960 --> 00:46:16,300
transactions as well for creating

00:46:13,090 --> 00:46:19,360
snapshots I'm not sure if Libert does

00:46:16,300 --> 00:46:21,400
the right now libert only supports the

00:46:19,360 --> 00:46:22,990
drive mirror not the drive back up and

00:46:21,400 --> 00:46:25,270
the drive back up is where it becomes

00:46:22,990 --> 00:46:26,860
more important to specify multiple disks

00:46:25,270 --> 00:46:28,540
in one command so there's still some

00:46:26,860 --> 00:46:31,180
liberabit work to do to expose drive

00:46:28,540 --> 00:46:33,250
back up but yes that you can use the

00:46:31,180 --> 00:46:36,600
dive snapshots and copy them offline

00:46:33,250 --> 00:46:38,290
basically right right and and again

00:46:36,600 --> 00:46:40,210
depending on how you create a snapshot

00:46:38,290 --> 00:46:43,330
as long as you pick the point in time

00:46:40,210 --> 00:46:45,280
and then use the cumulation command so

00:46:43,330 --> 00:46:48,910
the point in time is the same for all of

00:46:45,280 --> 00:46:51,550
your disks then the copying can be done

00:46:48,910 --> 00:46:53,290
offline but the point in time is stable

00:46:51,550 --> 00:47:00,700
and so you have a consistent back up

00:46:53,290 --> 00:47:04,990
across your disks obviously so kind of

00:47:00,700 --> 00:47:07,690
high-level question but so when we want

00:47:04,990 --> 00:47:10,570
to run visible talker in a vm for

00:47:07,690 --> 00:47:13,720
example right and then typical way is

00:47:10,570 --> 00:47:17,950
you pulled out the doc I image then run

00:47:13,720 --> 00:47:21,010
vm and then initially you know you r on

00:47:17,950 --> 00:47:24,580
vm using some for example the QAM you

00:47:21,010 --> 00:47:26,770
image then we need to mount that the

00:47:24,580 --> 00:47:29,500
host file system because local file

00:47:26,770 --> 00:47:32,320
systems are on the cosine and we use it

00:47:29,500 --> 00:47:36,010
for example 90 that's a kind of table

00:47:32,320 --> 00:47:39,670
and then what we want is somehow you

00:47:36,010 --> 00:47:42,370
know we can we want to be praised that

00:47:39,670 --> 00:47:44,770
the host file system is up example the

00:47:42,370 --> 00:47:48,130
QA view image but you know the queue

00:47:44,770 --> 00:47:50,530
image is huge right and I think I

00:47:48,130 --> 00:47:54,400
touched on that it would be nice to have

00:47:50,530 --> 00:47:57,990
overlay FS effects we to get that with a

00:47:54,400 --> 00:48:01,210
guest you need multiple drives have your

00:47:57,990 --> 00:48:03,700
you can't do it through q image because

00:48:01,210 --> 00:48:05,500
Q mu only moves a cluster at a time and

00:48:03,700 --> 00:48:08,140
clusters aren't the same granularity as

00:48:05,500 --> 00:48:10,780
your file system so to play tricks with

00:48:08,140 --> 00:48:13,690
docker and having docker images that

00:48:10,780 --> 00:48:16,720
depend on base images you need tricks

00:48:13,690 --> 00:48:20,110
like overlay FS built on top of multiple

00:48:16,720 --> 00:48:23,230
disks plugged into your guests but there

00:48:20,110 --> 00:48:24,760
I assume that as people figure out and

00:48:23,230 --> 00:48:26,790
play with overlay FS and figure out how

00:48:24,760 --> 00:48:31,030
to plug in multiple disks that you can

00:48:26,790 --> 00:48:33,490
simulate effects where a guest image

00:48:31,030 --> 00:48:35,530
becomes the guest disc that is the

00:48:33,490 --> 00:48:38,980
backing file system for another guest

00:48:35,530 --> 00:48:40,480
discs but it's bigger than this talk

00:48:38,980 --> 00:48:45,820
because this talk is only what you can

00:48:40,480 --> 00:48:50,380
do with the single file okay anything

00:48:45,820 --> 00:48:52,810
else do the persistent bitmaps also

00:48:50,380 --> 00:48:56,080
cover the case of a qmo crash or just

00:48:52,810 --> 00:49:00,250
were you the user shuts down the vm for

00:48:56,080 --> 00:49:02,440
some reason the files the operations

00:49:00,250 --> 00:49:05,290
that liver supports on a persistent

00:49:02,440 --> 00:49:07,360
guests are crash safe if libvirt gets

00:49:05,290 --> 00:49:10,360
restarted or if cameo crashes or the

00:49:07,360 --> 00:49:13,150
guest shuts down the operation itself is

00:49:10,360 --> 00:49:15,220
not lost you can either resume it or

00:49:13,150 --> 00:49:18,400
libert remembers that it was running the

00:49:15,220 --> 00:49:21,070
block copy operation that requires a

00:49:18,400 --> 00:49:24,850
transient guest that's libvirt sway of

00:49:21,070 --> 00:49:27,130
saying if your guest goes away my block

00:49:24,850 --> 00:49:30,850
copy is toast you'll have to start from

00:49:27,130 --> 00:49:32,570
scratch and the hope is that as km ooh

00:49:30,850 --> 00:49:34,430
ads persistent bitmaps and live

00:49:32,570 --> 00:49:38,690
it uses persistent bitmaps than we can

00:49:34,430 --> 00:49:43,970
make that also be robust but the stream

00:49:38,690 --> 00:49:45,410
and pull operations are robust I think

00:49:43,970 --> 00:49:47,500
I've spent my time so thanks for your

00:49:45,410 --> 00:49:47,500

YouTube URL: https://www.youtube.com/watch?v=etIGp12RHRE


