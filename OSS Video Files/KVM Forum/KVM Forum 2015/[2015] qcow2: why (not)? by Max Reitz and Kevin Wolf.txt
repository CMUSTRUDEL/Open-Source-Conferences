Title: [2015] qcow2: why (not)? by Max Reitz and Kevin Wolf
Publication date: 2015-08-27
Playlist: KVM Forum 2015
Description: 
	Full backups of large storage devices are expensive, slow, and waste a lot of space needlessly by copying sectors that have not changed over and over again. Incremental and differential backups are an oft requested feature in QEMU, and will help eliminate the redundant copying of backup data. This presentation will cover recent developments in related delta-backup technologies, covering incremental and differential backups, image fleecing, and dirty bitmap management. We will highlight how these features are accomplished using modifications to existing QMP primitives such as the 'drive-backup' and 'transaction' commands to unlock rich functionality within our existing APIs. 

Max Reitz
Red Hat

Max Reitz is a computer science student at TU Dresden and has been working on QEMU's block layer as a Red Hat intern since 2013. Together with Kevin Wolf, he held a presentation on block device configuration at KVM Forum 2014.

Kevin Wolf
KVM Developer, Red Hat

Kevin Wolf works at Red Hat as a KVM developer, with a focus on block devices. He is the maintainer of QEMU's block subsystem and has contributed many patches to block device emulation and image for drivers. After graduating in Software Engineering at the University of | Stuttgart, Germany in 2008 he worked on Xen's block layer for a year before he started working on KVM for Red Hat in 2009.
Captions: 
	00:00:20,300 --> 00:00:28,289
so yeah our topics are going to be you

00:00:24,810 --> 00:00:29,820
go to why not I Max Evans still sitting

00:00:28,289 --> 00:00:33,120
there but she'll come up on stage later

00:00:29,820 --> 00:00:37,980
and well the question is in the title

00:00:33,120 --> 00:00:39,270
sure to use Keuka to why or why not the

00:00:37,980 --> 00:00:41,820
question comes up every time you're

00:00:39,270 --> 00:00:44,129
creating a new virtual machine you

00:00:41,820 --> 00:00:47,579
basically have the two main choices for

00:00:44,129 --> 00:00:49,289
qmu you have the system default chosen

00:00:47,579 --> 00:00:52,260
by lippard or verte manager which is q

00:00:49,289 --> 00:00:54,090
co2 or you have the raw image and the

00:00:52,260 --> 00:00:55,590
traditional answer is jazz will do you

00:00:54,090 --> 00:00:57,600
need performance or the unique features

00:00:55,590 --> 00:00:59,879
if you need performance you choose wrong

00:00:57,600 --> 00:01:03,059
if you need features you choose Keuka to

00:00:59,879 --> 00:01:07,080
but well what do you do if you need both

00:01:03,059 --> 00:01:08,820
which happens most of the time so you

00:01:07,080 --> 00:01:10,890
can't think really hard about whether

00:01:08,820 --> 00:01:13,080
you really really need the features q

00:01:10,890 --> 00:01:15,810
car to providing well maybe you can get

00:01:13,080 --> 00:01:18,450
away with them so hmm maybe you can go

00:01:15,810 --> 00:01:20,220
for raw in that case but choosing a car

00:01:18,450 --> 00:01:21,810
analogy because we all know car

00:01:20,220 --> 00:01:24,240
analogies are the best way of explaining

00:01:21,810 --> 00:01:26,189
everything it's like throwing out the

00:01:24,240 --> 00:01:28,740
seats in order to gain more acceleration

00:01:26,189 --> 00:01:31,020
so you can like go around in your car

00:01:28,740 --> 00:01:33,540
with more speed but you're standing up

00:01:31,020 --> 00:01:35,520
which sounds like a horrible idea so the

00:01:33,540 --> 00:01:37,650
question is whether it's really worth it

00:01:35,520 --> 00:01:39,479
and the same applies to disk images we

00:01:37,650 --> 00:01:41,640
want you to keep the seats in that's our

00:01:39,479 --> 00:01:43,590
goal you can just you should just be

00:01:41,640 --> 00:01:45,479
able to use to you go to if there is

00:01:43,590 --> 00:01:46,979
even a slight chance that you need any

00:01:45,479 --> 00:01:49,470
of the features you got to provides

00:01:46,979 --> 00:01:51,240
overall then you should just build for

00:01:49,470 --> 00:01:52,890
q2 and don't worry about the performance

00:01:51,240 --> 00:01:56,340
in fact too much because it should be

00:01:52,890 --> 00:01:59,100
minimal if possible and whether that's

00:01:56,340 --> 00:02:02,189
possible that will be the goal the topic

00:01:59,100 --> 00:02:04,170
of our talk the first part what are

00:02:02,189 --> 00:02:07,259
those features Keuka provides

00:02:04,170 --> 00:02:09,840
well we have packing files we have

00:02:07,259 --> 00:02:11,489
internal snapshots we have zero clusters

00:02:09,840 --> 00:02:13,950
and partial applications on all file

00:02:11,489 --> 00:02:15,840
systems so with raw images you can have

00:02:13,950 --> 00:02:18,000
partial allocations on most modern file

00:02:15,840 --> 00:02:19,980
systems too but with Keuka to you get it

00:02:18,000 --> 00:02:22,140
everywhere and also you get compression

00:02:19,980 --> 00:02:24,000
which may or may not be useful to you

00:02:22,140 --> 00:02:25,800
personally but it's a feature q culture

00:02:24,000 --> 00:02:29,550
provides and raw doesn't so it's listed

00:02:25,800 --> 00:02:32,760
here let's talk more about how Keuka to

00:02:29,550 --> 00:02:34,470
is organized every Keuka to file is

00:02:32,760 --> 00:02:37,290
split into clusters which by default

00:02:34,470 --> 00:02:40,050
have a size of 64 kilobytes and each

00:02:37,290 --> 00:02:43,230
cluster can store either data all

00:02:40,050 --> 00:02:45,600
metadata at the bottom you see the empty

00:02:43,230 --> 00:02:47,100
clusters which don't contain any text

00:02:45,600 --> 00:02:48,569
those are data clusters except for the

00:02:47,100 --> 00:02:50,670
one to the far right which is an empty

00:02:48,569 --> 00:02:52,830
cluster and the other clusters they

00:02:50,670 --> 00:02:54,720
contain a today so for instance the

00:02:52,830 --> 00:02:58,440
cluster to the left most of the left

00:02:54,720 --> 00:03:00,600
that one is just the image header one

00:02:58,440 --> 00:03:03,090
kind of meta data we have RL two tables

00:03:00,600 --> 00:03:05,519
which map guests offsets to host offsets

00:03:03,090 --> 00:03:09,660
so every time you are reading or writing

00:03:05,519 --> 00:03:11,549
to a QPR to file data that is you're

00:03:09,660 --> 00:03:14,760
using a guest offset that is the officer

00:03:11,549 --> 00:03:17,310
the guests would use on his disk on its

00:03:14,760 --> 00:03:19,739
disk and the other two tables are used

00:03:17,310 --> 00:03:22,950
to map that offset to an offset in the

00:03:19,739 --> 00:03:24,900
host file and then human knows where to

00:03:22,950 --> 00:03:27,260
get the data from if you're reading

00:03:24,900 --> 00:03:30,239
order to write it to if you're writing

00:03:27,260 --> 00:03:31,980
the ref count blocks are used to store a

00:03:30,239 --> 00:03:34,440
location information that is most

00:03:31,980 --> 00:03:37,440
clusters are used exactly one so the ref

00:03:34,440 --> 00:03:39,090
count will be 1 but some clusters can be

00:03:37,440 --> 00:03:41,340
used more than one so for instance you

00:03:39,090 --> 00:03:44,010
see the two data clusters to the right

00:03:41,340 --> 00:03:46,350
they are on the guest they are actually

00:03:44,010 --> 00:03:48,209
shared they point to the same cluster in

00:03:46,350 --> 00:03:50,760
the host file so they will contain the

00:03:48,209 --> 00:03:52,260
same data when reading them but they

00:03:50,760 --> 00:03:54,900
will contain different data when writing

00:03:52,260 --> 00:03:56,489
to them because in that event a copy on

00:03:54,900 --> 00:03:58,650
write operation will be issued that is

00:03:56,489 --> 00:04:00,780
your writing to one of the clusters that

00:03:58,650 --> 00:04:02,819
cluster content will be copied to a free

00:04:00,780 --> 00:04:04,709
cluster and then the data will be

00:04:02,819 --> 00:04:06,810
written there where is the other cluster

00:04:04,709 --> 00:04:08,880
will remain untouched

00:04:06,810 --> 00:04:11,130
and this information about whether a

00:04:08,880 --> 00:04:13,890
cluster is shared or not is replicated

00:04:11,130 --> 00:04:16,350
in the l2 tables so that when we are

00:04:13,890 --> 00:04:18,959
accessing a cue go to file for a data

00:04:16,350 --> 00:04:22,049
access we generally only need to look at

00:04:18,959 --> 00:04:24,389
the l2 tables because the l2 table will

00:04:22,049 --> 00:04:26,790
tell us whether the cluster is

00:04:24,389 --> 00:04:30,840
referenced exactly once in which event

00:04:26,790 --> 00:04:33,360
we can just write data to the hafsa to

00:04:30,840 --> 00:04:35,370
the offset in the host file or whether

00:04:33,360 --> 00:04:38,010
it is shared in which case we have to do

00:04:35,370 --> 00:04:40,770
a copy on write operation and when doing

00:04:38,010 --> 00:04:43,080
copping a copy on write operation we

00:04:40,770 --> 00:04:45,750
have to look for a new cluster where we

00:04:43,080 --> 00:04:47,729
can write the data to and that's where

00:04:45,750 --> 00:04:49,830
we need the ref count blocks because

00:04:47,729 --> 00:04:52,680
then we need an empty cluster which has

00:04:49,830 --> 00:04:54,270
a ref count of 0 and that's what the ref

00:04:52,680 --> 00:04:57,419
 block is then used for finding such

00:04:54,270 --> 00:05:00,000
a cluster but in the general case we'll

00:04:57,419 --> 00:05:02,400
get away with just using the two tables

00:05:00,000 --> 00:05:04,860
so for most like oh the o2 tables are

00:05:02,400 --> 00:05:07,380
the most important beta data structure

00:05:04,860 --> 00:05:11,370
which will become more important later

00:05:07,380 --> 00:05:14,220
on so let's come to a more interesting

00:05:11,370 --> 00:05:16,710
part performance testing first we'll

00:05:14,220 --> 00:05:20,490
take a look at pre-allocated images that

00:05:16,710 --> 00:05:22,970
is images that have all the data or or

00:05:20,490 --> 00:05:27,630
it is the metadata already allocated and

00:05:22,970 --> 00:05:30,570
pretty much the data to that means that

00:05:27,630 --> 00:05:32,700
every I oh we will execute on that file

00:05:30,570 --> 00:05:34,919
will not need to allocate a new cluster

00:05:32,700 --> 00:05:37,770
in the queue car to file but we can just

00:05:34,919 --> 00:05:40,680
execute the i/o only looking at the l2

00:05:37,770 --> 00:05:43,110
tables and just do the i/o what this

00:05:40,680 --> 00:05:45,510
benchmark motel or these benchmarks will

00:05:43,110 --> 00:05:47,729
tell us is whether there is any

00:05:45,510 --> 00:05:49,889
fundamental problem in queue go to and

00:05:47,729 --> 00:05:52,440
that there is really a systemic problem

00:05:49,889 --> 00:05:55,470
Keuka to that just makes it slower than

00:05:52,440 --> 00:05:58,750
wrong so we look at that and we hope

00:05:55,470 --> 00:06:00,880
that there is no such problem

00:05:58,750 --> 00:06:03,370
the thing about pre-allocated images is

00:06:00,880 --> 00:06:05,620
that you cannot really use all of the

00:06:03,370 --> 00:06:07,270
features q go to provides you with them

00:06:05,620 --> 00:06:10,300
so for instance you cannot have packing

00:06:07,270 --> 00:06:13,180
files which is sad but let's just take a

00:06:10,300 --> 00:06:15,280
look if they're as fast is raw that's a

00:06:13,180 --> 00:06:17,410
good first result and Kevin will take a

00:06:15,280 --> 00:06:20,770
look at non pre-allocated images with

00:06:17,410 --> 00:06:25,360
backing files later on so what is tested

00:06:20,770 --> 00:06:27,910
i had i used a linux gas with fio as the

00:06:25,360 --> 00:06:31,270
benchmark tool i use six gigabyte images

00:06:27,910 --> 00:06:34,780
on an SSD and HDD and i had four

00:06:31,270 --> 00:06:38,650
different access patterns / test case so

00:06:34,780 --> 00:06:41,140
we had have random 4k blocks random 1m

00:06:38,650 --> 00:06:43,300
blocks sequential 4k blogs and bunch of

00:06:41,140 --> 00:06:46,240
one and blocks and then we'll see both

00:06:43,300 --> 00:06:48,730
read and write accesses as I said the

00:06:46,240 --> 00:06:50,830
cue car two images were created with the

00:06:48,730 --> 00:06:54,610
option three allocation equals metadata

00:06:50,830 --> 00:06:56,410
because that makes them faster so it's a

00:06:54,610 --> 00:06:58,600
call it's a better comparison against

00:06:56,410 --> 00:07:00,190
all because well we can see whether it's

00:06:58,600 --> 00:07:04,810
what the Keuka to is fundamentally

00:07:00,190 --> 00:07:08,260
slower so let's take a look this diagram

00:07:04,810 --> 00:07:10,270
shows the comparison of raw performance

00:07:08,260 --> 00:07:13,300
which is on the left always it's the

00:07:10,270 --> 00:07:17,140
blue bars again against QQ which are the

00:07:13,300 --> 00:07:20,710
red bars and each stack is normalized

00:07:17,140 --> 00:07:25,150
against the number of i/o ops of the raw

00:07:20,710 --> 00:07:27,040
image so higher generally is better the

00:07:25,150 --> 00:07:28,780
error bars signified standard deviation

00:07:27,040 --> 00:07:30,820
and I consider everything that's within

00:07:28,780 --> 00:07:33,220
standard deviation to not be significant

00:07:30,820 --> 00:07:36,030
so as you can see standard deviation is

00:07:33,220 --> 00:07:38,229
pretty high but I cannot really

00:07:36,030 --> 00:07:39,550
reasonably make the test any longer than

00:07:38,229 --> 00:07:41,140
hundred twenty seconds because running

00:07:39,550 --> 00:07:45,220
these tests already took like six hours

00:07:41,140 --> 00:07:47,890
so yeah and the standard deviation

00:07:45,220 --> 00:07:49,600
doesn't really get lower so well we have

00:07:47,890 --> 00:07:51,790
to live with that but what we can see is

00:07:49,600 --> 00:07:53,500
that there is no significant difference

00:07:51,790 --> 00:07:57,220
outside of standard deviation between

00:07:53,500 --> 00:07:59,190
raw and q4 write accesses on an SSD

00:07:57,220 --> 00:08:01,800
which is nice

00:07:59,190 --> 00:08:05,490
we can see the same for reads no

00:08:01,800 --> 00:08:11,520
significant difference same for HDD

00:08:05,490 --> 00:08:15,000
rights and the same for HDD rates well

00:08:11,520 --> 00:08:16,890
that's great right so if you use a

00:08:15,000 --> 00:08:19,590
pre-allocated kyouko to image then it's

00:08:16,890 --> 00:08:23,070
just as fast as wrong that's right but

00:08:19,590 --> 00:08:26,070
well we use six gigabyte images that

00:08:23,070 --> 00:08:27,690
seems awfully it's awfully small well is

00:08:26,070 --> 00:08:29,100
there some reason for that maybe we

00:08:27,690 --> 00:08:31,620
should just increase the image size to

00:08:29,100 --> 00:08:33,900
get more realistic approach right okay

00:08:31,620 --> 00:08:38,250
let's use a 16 gigabyte image on an SSD

00:08:33,900 --> 00:08:40,409
oh wait that doesn't seem right suddenly

00:08:38,250 --> 00:08:42,900
pump for the performance for 4k random

00:08:40,409 --> 00:08:45,300
writes drop to like sixty to seventy

00:08:42,900 --> 00:08:47,940
percent and as you can see the standard

00:08:45,300 --> 00:08:50,760
deviation for one megabyte random X is

00:08:47,940 --> 00:08:54,680
pretty high so that seems to indicate a

00:08:50,760 --> 00:08:57,690
problem too it gets even worse for reads

00:08:54,680 --> 00:08:59,880
standard deviation for one am random

00:08:57,690 --> 00:09:02,790
exercises again pretty high but the

00:08:59,880 --> 00:09:04,560
performance for 4k random reads is just

00:09:02,790 --> 00:09:06,980
abysmal it's just above twenty percent

00:09:04,560 --> 00:09:10,680
that's horrible can't work with that so

00:09:06,980 --> 00:09:13,290
hmmm well what about an HDD 32 gigabyte

00:09:10,680 --> 00:09:15,960
image basically the same result only

00:09:13,290 --> 00:09:20,520
this time 1m random excesses are very

00:09:15,960 --> 00:09:24,060
very bad too and it's basically the same

00:09:20,520 --> 00:09:26,250
for reads we actually have a performance

00:09:24,060 --> 00:09:31,500
penalty in this case for 4k sequential

00:09:26,250 --> 00:09:34,380
access is too so ok that doesn't seem

00:09:31,500 --> 00:09:37,230
too nice so what happened cash crushing

00:09:34,380 --> 00:09:40,080
happened keuka to is caching do two

00:09:37,230 --> 00:09:42,089
tables so you don't have to look at the

00:09:40,080 --> 00:09:45,030
disc at the l2 tables on disk every time

00:09:42,089 --> 00:09:46,770
you're doing and I 0 axis and the

00:09:45,030 --> 00:09:48,930
default cache size is one megabyte which

00:09:46,770 --> 00:09:51,660
covers about eight gigabyte of an image

00:09:48,930 --> 00:09:53,460
so once the images got bigger than eight

00:09:51,660 --> 00:09:55,830
gigabytes and we had random accesses all

00:09:53,460 --> 00:09:58,080
over the place the cash was no longer

00:09:55,830 --> 00:10:00,720
effective when you go to had to look at

00:09:58,080 --> 00:10:03,390
GMU the Keuka to drive and hume you had

00:10:00,720 --> 00:10:05,970
to look at the i/o table at the l2

00:10:03,390 --> 00:10:08,339
tables on disk all the time so that's

00:10:05,970 --> 00:10:13,100
why the performance just went

00:10:08,339 --> 00:10:15,899
down how can you fix it first idea is

00:10:13,100 --> 00:10:19,319
don't panic maybe you don't need to fix

00:10:15,899 --> 00:10:21,540
it at all because these were tests where

00:10:19,319 --> 00:10:23,749
you had random excesses of an area of 16

00:10:21,540 --> 00:10:26,550
or 32 gigabyte of an image which is

00:10:23,749 --> 00:10:28,529
probably not what normal applications do

00:10:26,550 --> 00:10:30,930
so as long as you don't have an

00:10:28,529 --> 00:10:33,029
application which actually does these

00:10:30,930 --> 00:10:34,499
excesses which does random accesses all

00:10:33,029 --> 00:10:36,899
over the place over an area which is

00:10:34,499 --> 00:10:38,999
larger than 8 gigabytes then everything

00:10:36,899 --> 00:10:41,579
is fine the actual image size doesn't

00:10:38,999 --> 00:10:43,410
matter as long as random excesses are

00:10:41,579 --> 00:10:44,790
contained within a small area of the

00:10:43,410 --> 00:10:46,620
image everything's fine you don't need

00:10:44,790 --> 00:10:49,019
to fix anything but if you do need to

00:10:46,620 --> 00:10:50,399
fix something you can there is the l2

00:10:49,019 --> 00:10:54,029
cache size runtime option which allows

00:10:50,399 --> 00:10:56,339
you to adjust size of the l2 cache and a

00:10:54,029 --> 00:10:58,920
term to calculate the cache size you

00:10:56,339 --> 00:11:00,720
need is given below you just divide the

00:10:58,920 --> 00:11:02,610
size of the area you want the l2 cache

00:11:00,720 --> 00:11:05,519
to cover by an eighth of the cluster

00:11:02,610 --> 00:11:07,170
size which by default is 64 kilobytes so

00:11:05,519 --> 00:11:09,600
an eighth would be 8 kilobytes and

00:11:07,170 --> 00:11:12,149
that's the result so for instance for

00:11:09,600 --> 00:11:15,509
our 32 gigabyte image on the hard drive

00:11:12,149 --> 00:11:20,339
we would then use 4 megabytes as the l2

00:11:15,509 --> 00:11:22,139
cache size so let's try again for our 16

00:11:20,339 --> 00:11:25,529
gigabyte image we would need at least a

00:11:22,139 --> 00:11:28,800
2 megabyte l2 cache and now everything

00:11:25,529 --> 00:11:31,679
is fine again great for rights and for

00:11:28,800 --> 00:11:34,249
rates trying again with 32 gigabyte hard

00:11:31,679 --> 00:11:37,620
disk image with a 4 mega byte 0 2 cache

00:11:34,249 --> 00:11:41,910
looks good again both for writing and

00:11:37,620 --> 00:11:44,370
for reading so that's nice so what this

00:11:41,910 --> 00:11:47,069
tells us is that there is no significant

00:11:44,370 --> 00:11:49,079
difference between raw and Keuka 243

00:11:47,069 --> 00:11:51,300
allocated images as long as the l2 cache

00:11:49,079 --> 00:11:53,839
is large enough which you or your

00:11:51,300 --> 00:11:56,339
management tools have to make sure it is

00:11:53,839 --> 00:11:59,309
so that means without copy-on-write

00:11:56,339 --> 00:12:01,949
excesses everything is good but well the

00:11:59,309 --> 00:12:04,709
format is named q cow to one reason

00:12:01,949 --> 00:12:07,259
right you probably want packing far as

00:12:04,709 --> 00:12:09,720
most of the time so what happens then

00:12:07,259 --> 00:12:12,319
and that's what Kevin will talk about in

00:12:09,720 --> 00:12:12,319
part 3

00:12:13,960 --> 00:12:21,020
as us max has told us if you don't

00:12:19,280 --> 00:12:25,040
actually use the features that you care

00:12:21,020 --> 00:12:28,100
to brings to everything is fine but at

00:12:25,040 --> 00:12:29,900
least if you take the right options but

00:12:28,100 --> 00:12:33,830
usually use key card to for a reason and

00:12:29,900 --> 00:12:38,480
that reason usually contains backing

00:12:33,830 --> 00:12:41,600
files so you don't use pre allocation in

00:12:38,480 --> 00:12:43,760
the usual case but you start with a

00:12:41,600 --> 00:12:47,510
sparse image that is an overlay of

00:12:43,760 --> 00:12:51,190
backing file and you get classroom

00:12:47,510 --> 00:12:54,140
allocations which change the picture bit

00:12:51,190 --> 00:12:57,110
they don't change the picture a lot 44

00:12:54,140 --> 00:12:59,390
reads that they do for rights and that's

00:12:57,110 --> 00:13:04,420
the old problem that we have been

00:12:59,390 --> 00:13:06,890
talking about many times already so um

00:13:04,420 --> 00:13:10,310
when do we actually allocate new

00:13:06,890 --> 00:13:15,020
classroom new clusters allocated when

00:13:10,310 --> 00:13:18,940
you write to the image to an area of the

00:13:15,020 --> 00:13:18,940
image that is not allocated yet

00:13:20,000 --> 00:13:26,090
if you have backing file at this point

00:13:22,820 --> 00:13:27,500
if the gas was reading from this area it

00:13:26,090 --> 00:13:31,220
would actually see the data that is

00:13:27,500 --> 00:13:34,150
stored in the backing file if you don't

00:13:31,220 --> 00:13:38,420
have a magnified it's just all zeros and

00:13:34,150 --> 00:13:40,580
then there's the other case where you

00:13:38,420 --> 00:13:43,550
get copy-on-write the class already

00:13:40,580 --> 00:13:46,580
exists that it was was referenced more

00:13:43,550 --> 00:13:50,840
than once it's one of the cases that max

00:13:46,580 --> 00:13:52,340
showed actually there are two cases

00:13:50,840 --> 00:13:55,520
where this happens one of them is

00:13:52,340 --> 00:13:58,820
internal snapshots so you have multiple

00:13:55,520 --> 00:14:02,240
snapshots pointing to the same cluster

00:13:58,820 --> 00:14:04,640
in the image file on the host and

00:14:02,240 --> 00:14:06,820
obviously you don't want the snapshot to

00:14:04,640 --> 00:14:11,930
change when your current running gasps

00:14:06,820 --> 00:14:15,950
it's updating some data on harvest the

00:14:11,930 --> 00:14:18,050
other the other case where you get

00:14:15,950 --> 00:14:23,120
copyright is compressed images because

00:14:18,050 --> 00:14:26,060
in that case you have multiple guests

00:14:23,120 --> 00:14:28,950
clusters as the guest system stored in

00:14:26,060 --> 00:14:34,320
the same cluster in a image file

00:14:28,950 --> 00:14:38,130
so the guests laugh I only takes a part

00:14:34,320 --> 00:14:40,830
of the of the clashing image file and

00:14:38,130 --> 00:14:44,240
you can't override it you need to

00:14:40,830 --> 00:14:44,240
allocate a new cluster for that

00:14:45,649 --> 00:14:51,949
so what happens when you do allocation

00:14:47,689 --> 00:14:54,769
is you get a hold gesture the cluster is

00:14:51,949 --> 00:15:00,829
usually 64 kilobytes it's configurable

00:14:54,769 --> 00:15:04,670
but that's a default and the guests may

00:15:00,829 --> 00:15:08,929
write with less than 64 k might just

00:15:04,670 --> 00:15:11,300
write to some part of the cluster but

00:15:08,929 --> 00:15:13,610
when you allocate a cluster all of it

00:15:11,300 --> 00:15:15,559
needs to be valid you can either map a

00:15:13,610 --> 00:15:18,679
whole cluster to cluster an image file

00:15:15,559 --> 00:15:22,279
or you say it's not there so if it's

00:15:18,679 --> 00:15:28,069
there it has to be there when the guests

00:15:22,279 --> 00:15:30,410
rights to it you're right the data the

00:15:28,069 --> 00:15:35,139
guest provided into it but if it doesn't

00:15:30,410 --> 00:15:37,399
fill up all of the data in the cluster

00:15:35,139 --> 00:15:41,089
you need to fill up the rest of the

00:15:37,399 --> 00:15:43,639
cluster by just copying in there what

00:15:41,089 --> 00:15:47,660
was already visible to the guests that

00:15:43,639 --> 00:15:51,439
is either data from the backing file

00:15:47,660 --> 00:15:53,389
zeros field data from the internal

00:15:51,439 --> 00:15:57,410
snapshot the cases that I just mentioned

00:15:53,389 --> 00:15:58,220
and that's a copy and write operation

00:15:57,410 --> 00:16:02,750
and

00:15:58,220 --> 00:16:08,780
that's what makes things slow because it

00:16:02,750 --> 00:16:11,150
involves quite a bit of work so you have

00:16:08,780 --> 00:16:16,520
I have three different problems that i

00:16:11,150 --> 00:16:19,670
want to talk about one results in more

00:16:16,520 --> 00:16:23,240
io requests second one in more bytes

00:16:19,670 --> 00:16:27,140
transferred and the third one curses

00:16:23,240 --> 00:16:29,980
nordisk flashes and all of this makes

00:16:27,140 --> 00:16:29,980
copy-on-write slow

00:16:33,860 --> 00:16:39,980
so the first problem is how do you do

00:16:37,160 --> 00:16:43,430
this copy on write operation the obvious

00:16:39,980 --> 00:16:46,399
naive implementation is we just write

00:16:43,430 --> 00:16:48,440
what the guests gave you then you read

00:16:46,399 --> 00:16:52,640
what's in the backing file for the pub

00:16:48,440 --> 00:16:55,790
before it you're right the park before

00:16:52,640 --> 00:17:00,140
it to the new image you read from the

00:16:55,790 --> 00:17:02,240
backing file and you write it again so

00:17:00,140 --> 00:17:04,699
you have five i/o operations for the

00:17:02,240 --> 00:17:09,189
naive implementation and needless to say

00:17:04,699 --> 00:17:09,189
that this is exactly what Jimmy does

00:17:12,259 --> 00:17:19,499
it's best about thirty percent

00:17:14,489 --> 00:17:20,850
performance compared to rewrite so can

00:17:19,499 --> 00:17:26,009
you leave a write your newly allocated

00:17:20,850 --> 00:17:31,019
test various thirty percent slower then

00:17:26,009 --> 00:17:36,799
if you're right do it again it's easy

00:17:31,019 --> 00:17:36,799
enough to just have a single right

00:17:37,039 --> 00:17:43,559
instead of three different rights so you

00:17:39,629 --> 00:17:46,590
have only one request and that's that

00:17:43,559 --> 00:17:49,019
actually improves the performance if you

00:17:46,590 --> 00:17:53,419
don't have to read from somewhere so if

00:17:49,019 --> 00:17:56,429
you don't have a backhand file you just

00:17:53,419 --> 00:18:00,149
prepared some zeros and you append some

00:17:56,429 --> 00:18:06,739
zeros to the buffer list pass by a guest

00:18:00,149 --> 00:18:11,249
and you have one single write request

00:18:06,739 --> 00:18:13,169
instead of three and almost restore some

00:18:11,249 --> 00:18:14,789
performance it doesn't make a big

00:18:13,169 --> 00:18:19,320
difference if you write just but the gas

00:18:14,789 --> 00:18:22,249
gets requested order the big giant

00:18:19,320 --> 00:18:22,249
include some zeros

00:18:22,290 --> 00:18:27,880
but as soon as you have a backing file

00:18:24,550 --> 00:18:29,290
you need to read and even one read miss

00:18:27,880 --> 00:18:32,970
expensive enough that it doesn't

00:18:29,290 --> 00:18:32,970
actually help much

00:18:36,960 --> 00:18:42,419
the second problem that we have is that

00:18:39,770 --> 00:18:45,330
we're actually doing a lot of copying

00:18:42,419 --> 00:18:49,070
right that's unnecessary because we have

00:18:45,330 --> 00:18:49,070
sequential writes and

00:18:51,770 --> 00:18:57,910
as you can see here we stuck with write

00:18:55,430 --> 00:18:57,910
requests

00:18:58,020 --> 00:19:07,160
you that is actually what's to read and

00:19:01,670 --> 00:19:10,920
the red ones are the areas that the

00:19:07,160 --> 00:19:13,580
copies from the back inside just to fill

00:19:10,920 --> 00:19:13,580
up the classroom

00:19:15,700 --> 00:19:23,710
so in the beginning we actually need to

00:19:17,860 --> 00:19:26,080
do just to be correct but the second one

00:19:23,710 --> 00:19:29,250
is immediately ordered by nitrogenous

00:19:26,080 --> 00:19:32,799
because we're doing a signature right

00:19:29,250 --> 00:19:36,610
again it's doing some company arrived at

00:19:32,799 --> 00:19:38,049
the end which unnecessary again because

00:19:36,610 --> 00:19:40,630
the next right place will override

00:19:38,049 --> 00:19:43,630
immediately so we're doing a lot of

00:19:40,630 --> 00:19:45,760
unnecessary work we don't have to do

00:19:43,630 --> 00:19:51,389
copy all of data at the end of this

00:19:45,760 --> 00:19:55,630
request so what you want to do is if

00:19:51,389 --> 00:19:57,340
this copy on the right area is over we

00:19:55,630 --> 00:20:06,580
just want to avoid the copy in the first

00:19:57,340 --> 00:20:09,720
place and I tried a few things to do

00:20:06,580 --> 00:20:12,250
that it got complicated more complicated

00:20:09,720 --> 00:20:14,320
like dragging all the requests and

00:20:12,250 --> 00:20:16,840
checking what overlaps and stuff like

00:20:14,320 --> 00:20:19,570
that and that's something I remember

00:20:16,840 --> 00:20:23,010
that we had a similar problem with meta

00:20:19,570 --> 00:20:26,490
data performance like five years ago and

00:20:23,010 --> 00:20:28,360
what simplified the solution which was

00:20:26,490 --> 00:20:32,049
when you stop doing all these

00:20:28,360 --> 00:20:36,519
complicated things but actually just

00:20:32,049 --> 00:20:40,419
introduced the cash so I thought let's

00:20:36,519 --> 00:20:42,159
do the same here instead of just having

00:20:40,419 --> 00:20:47,919
a meta data cache we can have a data

00:20:42,159 --> 00:20:50,710
cache as well and the idea is when you

00:20:47,919 --> 00:20:52,870
would have to read from the backing file

00:20:50,710 --> 00:20:56,980
in order to in order to do the copying

00:20:52,870 --> 00:20:58,899
right you instead allocate it a stir in

00:20:56,980 --> 00:21:00,659
the cash you don't actually do any disk

00:20:58,899 --> 00:21:04,990
i/o yet

00:21:00,659 --> 00:21:09,700
put in the data the guest is going to

00:21:04,990 --> 00:21:11,110
write and don't read them yet what's in

00:21:09,700 --> 00:21:14,409
the back and file but just like these

00:21:11,110 --> 00:21:16,330
areas in the caches embedded only

00:21:14,409 --> 00:21:19,809
earlier ate them if you actually need

00:21:16,330 --> 00:21:24,010
them that is when the guests does a

00:21:19,809 --> 00:21:25,870
flash operation or when it reads and you

00:21:24,010 --> 00:21:30,820
have obviously to read in and complete

00:21:25,870 --> 00:21:34,750
the copyright but if the guest rights to

00:21:30,820 --> 00:21:37,600
the same area in the cache that makes

00:21:34,750 --> 00:21:41,610
the cash valid you don't have to

00:21:37,600 --> 00:21:45,130
actually read anything in and you get a

00:21:41,610 --> 00:21:47,289
fully valid cash cluster in memory and

00:21:45,130 --> 00:21:51,940
can write it out at once and you saved

00:21:47,289 --> 00:21:57,279
of the breeds and get down to a single

00:21:51,940 --> 00:22:01,870
right so that actually seems to have a

00:21:57,279 --> 00:22:03,549
lot for me as you can see for rewrite it

00:22:01,870 --> 00:22:06,639
doesn't make a big difference actually

00:22:03,549 --> 00:22:08,380
it shouldn't make any difference that's

00:22:06,639 --> 00:22:15,070
not really significant but what you can

00:22:08,380 --> 00:22:18,840
see there but for small right we can see

00:22:15,070 --> 00:22:22,040
that makes a huge difference so in

00:22:18,840 --> 00:22:24,710
Jeremy a fifth master

00:22:22,040 --> 00:22:28,760
it's this small new thing it's much

00:22:24,710 --> 00:22:32,300
worse than the wrong if we use the data

00:22:28,760 --> 00:22:38,210
cache we get much higher like factor

00:22:32,300 --> 00:22:41,120
four or five even higher than raw the

00:22:38,210 --> 00:22:45,130
reason for that is what happens when you

00:22:41,120 --> 00:22:49,550
catch it these things I'm you have

00:22:45,130 --> 00:22:51,590
really small write requests coming from

00:22:49,550 --> 00:22:56,480
the guest who actually end up merging

00:22:51,590 --> 00:22:59,600
write requests through the caster we do

00:22:56,480 --> 00:23:04,430
even better than raw it's a nice side

00:22:59,600 --> 00:23:06,610
effect was not fun ended but we'll take

00:23:04,430 --> 00:23:06,610
it

00:23:08,880 --> 00:23:15,960
ok and a third problem I want to talk

00:23:13,770 --> 00:23:18,900
about is when you have internal count

00:23:15,960 --> 00:23:20,970
that is a Jacobian right where the

00:23:18,900 --> 00:23:24,990
original cluster is in the same image

00:23:20,970 --> 00:23:26,940
file as we're writing to that are the

00:23:24,990 --> 00:23:31,370
cases that I mentioned before internal

00:23:26,940 --> 00:23:31,370
snapshots and compress images

00:23:32,410 --> 00:23:37,060
then what you have to do is you have to

00:23:35,590 --> 00:23:40,210
allocate a new cluster were to write the

00:23:37,060 --> 00:23:43,420
data to and allocating glasser usually

00:23:40,210 --> 00:23:46,570
means or actually always means you have

00:23:43,420 --> 00:23:49,060
to increase the ref count first and then

00:23:46,570 --> 00:23:54,400
you can hook the cluster up like in in a

00:23:49,060 --> 00:23:56,320
mapping because if he ever crashes you

00:23:54,400 --> 00:24:02,850
have a power failure or anything like

00:23:56,320 --> 00:24:05,650
that if the cluster is used it must be

00:24:02,850 --> 00:24:11,050
referencing a ref cons table so that you

00:24:05,650 --> 00:24:15,160
don't get data corruption so at this

00:24:11,050 --> 00:24:18,540
point we need a flush we do have the

00:24:15,160 --> 00:24:22,330
limited data cache as I said before

00:24:18,540 --> 00:24:25,480
which can batch multiple of these so you

00:24:22,330 --> 00:24:28,180
can just do all ref count increases then

00:24:25,480 --> 00:24:33,700
a flash and then all mapping updates and

00:24:28,180 --> 00:24:36,640
it's safe but what you need for internal

00:24:33,700 --> 00:24:38,800
cow as well as you drop the reference

00:24:36,640 --> 00:24:40,780
for the old classroom and there the

00:24:38,800 --> 00:24:43,720
dependency is exactly the other way

00:24:40,780 --> 00:24:46,990
around you need first to remove the

00:24:43,720 --> 00:24:49,770
mapping and then you can decrease the

00:24:46,990 --> 00:24:51,600
ref console that

00:24:49,770 --> 00:24:53,190
the only thing that can ever happen in

00:24:51,600 --> 00:24:55,620
case of a crash is that your vehicle

00:24:53,190 --> 00:24:57,630
cluster but never you never use the

00:24:55,620 --> 00:25:02,070
cluster that is not referenced in

00:24:57,630 --> 00:25:04,680
earthbound stable so we end up actually

00:25:02,070 --> 00:25:09,780
doing two disc flushes per allocation

00:25:04,680 --> 00:25:12,570
with internal cow which is obviously not

00:25:09,780 --> 00:25:21,800
a quite good idea makes things quite

00:25:12,570 --> 00:25:21,800
slow so there there are

00:25:24,890 --> 00:25:28,330
there is something missing here

00:25:32,260 --> 00:25:39,000
yeah well I'll just tell you what's on

00:25:35,800 --> 00:25:39,000
his like that should be here

00:25:41,639 --> 00:25:46,670
um

00:25:43,500 --> 00:25:46,670
right so

00:25:48,999 --> 00:25:54,669
what are the solutions for this we have

00:25:52,359 --> 00:25:58,329
two different solutions that we can use

00:25:54,669 --> 00:26:00,009
the first one is lazy ref cards which is

00:25:58,329 --> 00:26:05,909
an option that is implemented for quite

00:26:00,009 --> 00:26:05,909
a while now so if you use lazy ref cons

00:26:06,509 --> 00:26:12,609
we set a dirty flag in the image file

00:26:10,229 --> 00:26:15,489
which tells basically the ref cons are

00:26:12,609 --> 00:26:18,819
not accurate if we crash in this state

00:26:15,489 --> 00:26:24,699
you need to run EMU IMG Jack before the

00:26:18,819 --> 00:26:31,389
image can be reused again and in that

00:26:24,699 --> 00:26:33,489
case it's not necessary to to have the

00:26:31,389 --> 00:26:36,669
right order between updating ref guns

00:26:33,489 --> 00:26:40,539
and updating the mappings so we have

00:26:36,669 --> 00:26:44,799
saved two flashes the other option would

00:26:40,539 --> 00:26:47,559
be that we implement a journal like

00:26:44,799 --> 00:26:51,879
journaling file systems which will allow

00:26:47,559 --> 00:26:56,109
to update the mapping and direct on

00:26:51,879 --> 00:26:57,519
table at the same time so what're

00:26:56,109 --> 00:27:00,189
journal gives you is basically

00:26:57,519 --> 00:27:03,159
transactions so you can do things at the

00:27:00,189 --> 00:27:05,819
same time and then we would save the

00:27:03,159 --> 00:27:05,819
flashes as well

00:27:06,660 --> 00:27:14,950
right so what's the current status I

00:27:12,360 --> 00:27:18,789
have some prototype patches for the data

00:27:14,950 --> 00:27:23,679
cache i think i'm going to post them

00:27:18,789 --> 00:27:25,440
during the 25 development cycle not sure

00:27:23,679 --> 00:27:31,679
if they will be ready for the release

00:27:25,440 --> 00:27:31,679
but 25 or 26 eyelids i would guess

00:27:31,980 --> 00:27:37,840
journaling is probably not going to come

00:27:34,600 --> 00:27:41,340
anytime soon but we do already have lazy

00:27:37,840 --> 00:27:43,690
reference and if you have images that

00:27:41,340 --> 00:27:45,700
that use internal Cal that is you have

00:27:43,690 --> 00:27:47,799
compressed images or you have internal

00:27:45,700 --> 00:27:50,970
snapshots then you probably want to use

00:27:47,799 --> 00:27:50,970
lazy ref cons

00:27:53,070 --> 00:27:57,230
and that's it questions

00:28:00,400 --> 00:28:03,400
yep

00:28:07,010 --> 00:28:12,100
yes all of this is or direct and

00:28:16,509 --> 00:28:18,959
if you already

00:28:29,149 --> 00:28:40,339
compared to the other passion not really

00:28:36,289 --> 00:28:42,529
i mean the data cache isn't is not a

00:28:40,339 --> 00:28:45,440
full delicate at a full cache of

00:28:42,529 --> 00:28:47,239
everything we only put things into the

00:28:45,440 --> 00:28:50,839
data cache when you would actually need

00:28:47,239 --> 00:28:53,859
a copy and write operation so it's not

00:28:50,839 --> 00:28:53,859
in general operation

00:29:02,710 --> 00:29:12,669
I only have a bit for marking a cluster

00:29:08,980 --> 00:29:14,860
is always zero yes is it he said that

00:29:12,669 --> 00:29:17,860
the allocation does not mean smell with

00:29:14,860 --> 00:29:19,809
that is there is it worth adding

00:29:17,860 --> 00:29:23,679
extensions thank you how to add another

00:29:19,809 --> 00:29:26,980
did she says we allocate my space lovely

00:29:23,679 --> 00:29:28,620
my vacuum bottle for now so that we

00:29:26,980 --> 00:29:32,409
don't have to allocate the custard

00:29:28,620 --> 00:29:35,500
behind the data sorry space is our

00:29:32,409 --> 00:29:38,140
dessert yeah I thought about the same

00:29:35,500 --> 00:29:42,010
myself it's possible to add something

00:29:38,140 --> 00:29:44,620
like this probably inverted but it would

00:29:42,010 --> 00:29:51,539
mean an incompatible form a change right

00:29:44,620 --> 00:29:51,539
which is well what it is yes

00:29:54,100 --> 00:29:57,630
how much threads

00:30:00,010 --> 00:30:04,950
on the side

00:30:01,950 --> 00:30:04,950
yes

00:30:05,840 --> 00:30:13,770
I ran multiple tests I'm not quite sure

00:30:11,280 --> 00:30:19,230
what this one is but I did run with once

00:30:13,770 --> 00:30:21,630
that was 16 at least so the other tests

00:30:19,230 --> 00:30:24,630
Iran they were with AI o native and I

00:30:21,630 --> 00:30:28,410
think they had a AO depth until up to

00:30:24,630 --> 00:30:31,020
128 threats or aio requests at the same

00:30:28,410 --> 00:30:34,080
time so yeah we were thinking about

00:30:31,020 --> 00:30:36,240
locking maybe making Keuka to slower

00:30:34,080 --> 00:30:38,580
because raw doesn't have any locking but

00:30:36,240 --> 00:30:41,180
Keuka do does but we didn't see any

00:30:38,580 --> 00:30:41,180
effect of that

00:30:46,600 --> 00:30:48,870
move

00:30:49,460 --> 00:30:57,000
131 and

00:30:53,160 --> 00:30:57,000
30 so I was finished

00:30:57,500 --> 00:31:03,920
sorry didn't quite get that you

00:31:07,440 --> 00:31:12,870
I mean increasing the image size okay

00:31:12,990 --> 00:31:24,059
right you reallocate new testers yes

00:31:16,149 --> 00:31:27,300
okay lucky you wait wait and you write

00:31:24,059 --> 00:31:30,220
once fish

00:31:27,300 --> 00:31:33,670
you're right solution to 90 english

00:31:30,220 --> 00:31:37,200
paper provides by 64 kilobytes chance

00:31:33,670 --> 00:31:40,590
and let go Wi-Fi

00:31:37,200 --> 00:31:43,190
wait and you play the song tablet to

00:31:40,590 --> 00:31:43,190
stop you

00:31:43,980 --> 00:31:53,409
now usually not we do have some some

00:31:50,890 --> 00:31:55,299
locking where requests but allocating

00:31:53,409 --> 00:32:02,169
requests have to wait if they attach the

00:31:55,299 --> 00:32:07,390
same cluster until they are hooked up in

00:32:02,169 --> 00:32:09,480
in the mapping but once the first the

00:32:07,390 --> 00:32:12,610
first right to the cluster has completed

00:32:09,480 --> 00:32:14,440
in the case with data cache even if the

00:32:12,610 --> 00:32:19,390
data is still sitting in the cash it's

00:32:14,440 --> 00:32:22,210
not written out yet then multiple

00:32:19,390 --> 00:32:23,830
requests can run in power and as long as

00:32:22,210 --> 00:32:26,730
they touch a different cluster they can

00:32:23,830 --> 00:32:26,730
run in parallel anyway

00:32:32,500 --> 00:32:45,679
status and future

00:32:43,350 --> 00:32:45,679
once

00:32:45,740 --> 00:32:48,370
compressed

00:32:49,859 --> 00:32:57,340
you can can use km gmn to set the option

00:32:54,789 --> 00:33:00,429
or you can actually also set it as a

00:32:57,340 --> 00:33:03,279
runtime option in their strife just

00:33:00,429 --> 00:33:09,299
Wi-Fi equals blah blah blah comma later

00:33:03,279 --> 00:33:09,299
of course includes all so it's a yes

00:33:12,519 --> 00:33:19,089
now it's not a default because it

00:33:15,999 --> 00:33:21,159
requires after a crash that you do the

00:33:19,089 --> 00:33:24,779
chemical energy check so that's not

00:33:21,159 --> 00:33:24,779
something we want to have by default

00:33:32,910 --> 00:33:43,870
yep yes I realize that it's suitable

00:33:39,630 --> 00:33:46,360
yeah well there is a reason the so one

00:33:43,870 --> 00:33:49,270
reason is that once you create a QQ file

00:33:46,360 --> 00:33:51,190
it's basically empty and it's it says

00:33:49,270 --> 00:33:53,050
zero length so if you try to derive like

00:33:51,190 --> 00:33:54,670
that like the catch that from that the

00:33:53,050 --> 00:33:56,050
cash will be empty and we had some

00:33:54,670 --> 00:33:59,230
problems that which should be fixed by

00:33:56,050 --> 00:34:00,580
now but the other reason is that if you

00:33:59,230 --> 00:34:02,650
create like a huge image and you don't

00:34:00,580 --> 00:34:04,180
have any 0.1 you don't have any

00:34:02,650 --> 00:34:06,940
application which actually does random

00:34:04,180 --> 00:34:09,100
access over all your disk then you don't

00:34:06,940 --> 00:34:10,890
need the full cache size Alberto has

00:34:09,100 --> 00:34:13,540
some patches on the list which

00:34:10,890 --> 00:34:17,260
automatically sweep the cash after a

00:34:13,540 --> 00:34:19,630
certain amount of time so after so it

00:34:17,260 --> 00:34:21,190
looks through the cash and looks at all

00:34:19,630 --> 00:34:22,540
the cache entries and if some have not

00:34:21,190 --> 00:34:24,429
been used for a certain amount of time

00:34:22,540 --> 00:34:26,860
tunable by the user then it will

00:34:24,429 --> 00:34:30,340
basically be evicted and once we have

00:34:26,860 --> 00:34:31,750
that we can probably make it default but

00:34:30,340 --> 00:34:34,450
as long as you don't have that we

00:34:31,750 --> 00:34:36,850
probably don't want to have a huge Ram

00:34:34,450 --> 00:34:39,400
users for large images which you don't

00:34:36,850 --> 00:34:40,900
expect but once we have automatic have

00:34:39,400 --> 00:34:43,160
sweeping we can think about that that

00:34:40,900 --> 00:34:46,419
again yeah

00:34:43,160 --> 00:34:50,389
so my suggestion

00:34:46,419 --> 00:34:51,620
gnucash lies maybe get a constant

00:34:50,389 --> 00:34:54,600
parameter

00:34:51,620 --> 00:34:57,360
alt-rock ice-cream hacks and then this

00:34:54,600 --> 00:35:00,400
time oh yeah

00:34:57,360 --> 00:35:03,510
then users on need constant have to

00:35:00,400 --> 00:35:05,980
spare before highlights for immunity

00:35:03,510 --> 00:35:07,010
you're fine then as if you got you to

00:35:05,980 --> 00:35:15,670
Oliver

00:35:07,010 --> 00:35:15,670

YouTube URL: https://www.youtube.com/watch?v=svMpxzl9yI4


