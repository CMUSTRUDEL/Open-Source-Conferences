Title: [2015] KVM Message Passing Performance by David Matlack
Publication date: 2015-09-04
Playlist: KVM Forum 2015
Description: 
	Message Passing (inter-process communication and event-driven) Workloads can be more than 2x slower in KVM guests than native, even when virtual IO devices are not involved. Not only is this bad for performance, it is very counter-intuitive. This talk will cover: * The KVM overheads of Message Passing Workloads (using loopback TCP_RR as an example). * The lifetime and performance of a guest HLT, including the guest's interactions with its local APIC for wakeups. * The effects of a tickless (no-hz) guest kernel on message passing performance. * Plans to improve performance from within KVM. The meat of the talk will be x86 (Intel) specific, but the concepts should apply to most architectures. Note: unlike Rik van Riel's talk from 2013 the workloads discussed in this talk don't depend on the performance of the FPU.

David Matlack
Google
I'm a Software Engineer at Google working on virtualization. I've been working in KVM and the kernel for about a year now. In KVM, I work on performance and efficiency; trying to make our VMs spend less time in the host and more time in the guest.

Slides:  https://drive.google.com/file/d/0BzyAwvVlQckeaGRrM184Q0tUNE0/view?usp=sharing
Captions: 
	00:00:14,030 --> 00:00:19,680
I'm gonna get started my name is David

00:00:16,859 --> 00:00:21,060
Matlack I work at Google and I'm going

00:00:19,680 --> 00:00:25,279
to be talking about message passing

00:00:21,060 --> 00:00:27,570
workloads in kvm and their performance

00:00:25,279 --> 00:00:29,970
so I'll give a brief overview what

00:00:27,570 --> 00:00:31,980
message passing workloads are and then

00:00:29,970 --> 00:00:33,450
I'll use TCP are to demonstrate what the

00:00:31,980 --> 00:00:36,180
performance is and where the upper heads

00:00:33,450 --> 00:00:39,090
are I'll talk a lot about inter

00:00:36,180 --> 00:00:42,210
processor interrupts and halts and how

00:00:39,090 --> 00:00:45,960
those perform in a kvm guest and this

00:00:42,210 --> 00:00:47,039
will be pretty x86 specific so raise

00:00:45,960 --> 00:00:49,710
your hand if you want me to explain

00:00:47,039 --> 00:00:51,000
something more and then at the end i'll

00:00:49,710 --> 00:00:53,460
talk about halt pulling which is a

00:00:51,000 --> 00:00:55,649
feature you can use today to improve the

00:00:53,460 --> 00:01:00,840
important performance of message passing

00:00:55,649 --> 00:01:02,969
workloads so message passing workloads

00:01:00,840 --> 00:01:05,430
is not the first time they've been at

00:01:02,969 --> 00:01:07,979
kvm forum but if you haven't heard of

00:01:05,430 --> 00:01:09,450
them I imagine any workload where you

00:01:07,979 --> 00:01:12,479
have multiple threads passing messages

00:01:09,450 --> 00:01:13,770
in between them and characteristic of

00:01:12,479 --> 00:01:16,619
these workloads that we really care

00:01:13,770 --> 00:01:20,130
about is that they very rapidly switch

00:01:16,619 --> 00:01:21,780
between running and idle so they're

00:01:20,130 --> 00:01:23,610
running when they're receiving a message

00:01:21,780 --> 00:01:25,140
processing the message and then their

00:01:23,610 --> 00:01:27,810
idle when they are we waiting for the

00:01:25,140 --> 00:01:29,729
next message so this is your web servers

00:01:27,810 --> 00:01:33,180
your database servers where they're

00:01:29,729 --> 00:01:34,979
sitting on an open connection waiting

00:01:33,180 --> 00:01:36,240
for a client requests that's when their

00:01:34,979 --> 00:01:37,650
Idol and then once they receive a

00:01:36,240 --> 00:01:41,040
request they service it and send the

00:01:37,650 --> 00:01:43,619
reply that's the running half but also

00:01:41,040 --> 00:01:46,759
multi-threaded applications they use

00:01:43,619 --> 00:01:49,229
condition variables to do wait signals

00:01:46,759 --> 00:01:53,070
these also have the same characteristic

00:01:49,229 --> 00:01:55,140
of quickly switch between running where

00:01:53,070 --> 00:01:56,399
they're doing whatever and then I tell

00:01:55,140 --> 00:02:02,310
where they're waiting on a condition

00:01:56,399 --> 00:02:04,170
variable and a good benchmark of message

00:02:02,310 --> 00:02:07,439
passing workloads is TCP are which is

00:02:04,170 --> 00:02:09,320
part of the net proof benchmarks if you

00:02:07,439 --> 00:02:12,090
haven't heard of it

00:02:09,320 --> 00:02:14,190
so the intuition with a lot of these

00:02:12,090 --> 00:02:16,470
workloads is when you're running a

00:02:14,190 --> 00:02:18,360
virtual machine and you're not doing any

00:02:16,470 --> 00:02:19,860
i/o so maybe you're running your

00:02:18,360 --> 00:02:22,770
multi-threaded workload with these

00:02:19,860 --> 00:02:24,870
condition variables primitives well when

00:02:22,770 --> 00:02:28,530
there is an i/o involved then we should

00:02:24,870 --> 00:02:32,360
run at near native performance but the

00:02:28,530 --> 00:02:36,740
reality is these workloads can often be

00:02:32,360 --> 00:02:40,020
two to three or more times slower so

00:02:36,740 --> 00:02:41,790
loopback memcache meaning you have your

00:02:40,020 --> 00:02:44,100
web server and your mem cache server

00:02:41,790 --> 00:02:46,260
running inside the same virtual machine

00:02:44,100 --> 00:02:48,960
so you're not going across the network

00:02:46,260 --> 00:02:50,760
to hit the mem cache server the latency

00:02:48,960 --> 00:02:52,770
of going to memcache and back is two

00:02:50,760 --> 00:02:56,160
times higher in the KPM virtual machine

00:02:52,770 --> 00:02:59,520
and these windows event objects which is

00:02:56,160 --> 00:03:01,520
what in a windows operating system what

00:02:59,520 --> 00:03:04,020
they call their weight signal primitive

00:03:01,520 --> 00:03:12,410
latency of those is three to four times

00:03:04,020 --> 00:03:16,170
higher so we can use loopback TCP are to

00:03:12,410 --> 00:03:18,650
better understand the virtual overheads

00:03:16,170 --> 00:03:21,210
of these message passing workloads

00:03:18,650 --> 00:03:24,090
because loopback TCP are you have a

00:03:21,210 --> 00:03:27,180
client process and a server process they

00:03:24,090 --> 00:03:29,670
maintain a open tcp connection between

00:03:27,180 --> 00:03:34,080
the two and then they ping pong one byte

00:03:29,670 --> 00:03:35,850
of data back and forth so one

00:03:34,080 --> 00:03:37,320
transaction is starting at the client

00:03:35,850 --> 00:03:39,870
sending a bite to the server and then

00:03:37,320 --> 00:03:42,000
the server sending one bite back and we

00:03:39,870 --> 00:03:45,000
care about what is the latency of that

00:03:42,000 --> 00:03:48,480
one transaction and it's loop back so

00:03:45,000 --> 00:03:51,450
there's no you know networking whether

00:03:48,480 --> 00:03:54,860
it's networking hardware involved or an

00:03:51,450 --> 00:03:54,860
emulated network device

00:03:56,910 --> 00:04:03,570
so this is the performance of Luke back

00:04:00,270 --> 00:04:07,260
T CPR are on an Ivy Bridge machine

00:04:03,570 --> 00:04:11,120
running KBM in a 311 post colonel with

00:04:07,260 --> 00:04:14,880
some slightly modern KPM with backports

00:04:11,120 --> 00:04:17,850
at one cpu the native and virtual

00:04:14,880 --> 00:04:21,060
performance are about the same they both

00:04:17,850 --> 00:04:24,330
run at about 10 microseconds for each of

00:04:21,060 --> 00:04:27,330
these transactions but as you go to 2

00:04:24,330 --> 00:04:30,300
and higher they diverge they both get

00:04:27,330 --> 00:04:31,860
worse but virtual drive diverges a lot a

00:04:30,300 --> 00:04:37,530
lot further than the physical

00:04:31,860 --> 00:04:39,600
performance so at two and higher CPUs 3x

00:04:37,530 --> 00:04:41,310
higher latency in a virtual machine and

00:04:39,600 --> 00:04:48,240
that's about twenty five microseconds

00:04:41,310 --> 00:04:50,280
for this benchmark so if you see

00:04:48,240 --> 00:04:52,140
something slow on one and something slow

00:04:50,280 --> 00:04:53,970
on more than one cpu you could probably

00:04:52,140 --> 00:04:56,220
guess well on one cpu when you're

00:04:53,970 --> 00:04:57,840
passing passing messages back and forth

00:04:56,220 --> 00:04:59,190
between two threads well you're just

00:04:57,840 --> 00:05:01,080
context switching between those two

00:04:59,190 --> 00:05:03,840
threads but if your message passing

00:05:01,080 --> 00:05:05,730
between different CPUs well you're

00:05:03,840 --> 00:05:07,380
probably doing some sort of cross court

00:05:05,730 --> 00:05:10,140
communication which may be as inter

00:05:07,380 --> 00:05:11,850
processor interrupts but what's actually

00:05:10,140 --> 00:05:14,060
going on under the hood and how is this

00:05:11,850 --> 00:05:16,320
being virtualized where's the lien see

00:05:14,060 --> 00:05:18,080
the MX 'it's are a good place to start

00:05:16,320 --> 00:05:20,300
looking when you're looking at a virtual

00:05:18,080 --> 00:05:22,890
virtualization overheads and luckily

00:05:20,300 --> 00:05:25,260
there's a tool proof KPM the most of you

00:05:22,890 --> 00:05:32,310
probably know that lets you count and

00:05:25,260 --> 00:05:35,460
time be images ok so i ran TCP are with

00:05:32,310 --> 00:05:36,870
one and two CPUs and then counted the

00:05:35,460 --> 00:05:39,419
number of the MX's for each and then

00:05:36,870 --> 00:05:43,830
correlated it with how many transactions

00:05:39,419 --> 00:05:47,430
there were so with one virtual CPU there

00:05:43,830 --> 00:05:51,000
is some noise overhead of vm exits but

00:05:47,430 --> 00:05:54,020
they're per transaction of tcp RR is

00:05:51,000 --> 00:05:56,760
essentially zero so there's basically no

00:05:54,020 --> 00:06:00,750
virtualization overhead correlated with

00:05:56,760 --> 00:06:04,050
each transaction but at two CPUs you get

00:06:00,750 --> 00:06:10,940
a little over nine and a half or about

00:06:04,050 --> 00:06:10,940
10 ms are right exits and to halt exits

00:06:13,210 --> 00:06:18,350
so halt is a CPU instruction that you

00:06:16,520 --> 00:06:20,270
execute when you have nothing to do and

00:06:18,350 --> 00:06:21,890
it basically tells the CPU stop

00:06:20,270 --> 00:06:24,470
executing instructions until an

00:06:21,890 --> 00:06:26,150
interrupter eyes so if our virtual CPU

00:06:24,470 --> 00:06:28,850
is executing this instruction that means

00:06:26,150 --> 00:06:30,110
it's decided there's nothing to do and

00:06:28,850 --> 00:06:31,550
we know a message of passing workload

00:06:30,110 --> 00:06:37,390
switch between running and idle so this

00:06:31,550 --> 00:06:39,770
isn't a huge surprise 10 m/s our rights

00:06:37,390 --> 00:06:42,290
MSR is our model specific registers

00:06:39,770 --> 00:06:45,260
they're basically CPU registers that you

00:06:42,290 --> 00:06:49,490
access with a special instruction eight

00:06:45,260 --> 00:06:52,700
of the ten our rights to the a pic timer

00:06:49,490 --> 00:06:57,140
which is a per CPU timer that can be

00:06:52,700 --> 00:06:58,730
used to implement a tick or HR timers so

00:06:57,140 --> 00:07:00,470
you write a value to this register and

00:06:58,730 --> 00:07:01,940
the timer starts counting down and fires

00:07:00,470 --> 00:07:04,700
an interrupt when it gets to zero so

00:07:01,940 --> 00:07:06,260
this is an artifact of the guests being

00:07:04,700 --> 00:07:10,010
a no hearts kernel and i'll talk more

00:07:06,260 --> 00:07:12,290
about that later the other two MSR

00:07:10,010 --> 00:07:13,940
rights are to the interrupt command

00:07:12,290 --> 00:07:17,330
register and this is how you send in

00:07:13,940 --> 00:07:19,640
your processor interrupts so and like we

00:07:17,330 --> 00:07:22,990
said this this is how you would deliver

00:07:19,640 --> 00:07:22,990
messages across CPUs

00:07:25,860 --> 00:07:30,970
so taking those exits and putting them

00:07:29,050 --> 00:07:33,190
on how this is flowing for one

00:07:30,970 --> 00:07:35,200
transaction of T CPR if you start at the

00:07:33,190 --> 00:07:36,460
top left the client sending message to

00:07:35,200 --> 00:07:41,110
the server as an inter processor

00:07:36,460 --> 00:07:42,730
interrupt the server was halted when the

00:07:41,110 --> 00:07:45,460
interrupt was delivered comes out of

00:07:42,730 --> 00:07:47,260
Hall rights to at a pic timer twice and

00:07:45,460 --> 00:07:48,610
then the same thing happens again the

00:07:47,260 --> 00:07:53,350
server sends a message back to the

00:07:48,610 --> 00:07:57,130
client so the important thing here is

00:07:53,350 --> 00:08:00,810
that we had 12 total vm exits but not

00:07:57,130 --> 00:08:03,340
all of them are on the critical path so

00:08:00,810 --> 00:08:04,840
when the client after it sends its inter

00:08:03,340 --> 00:08:07,390
processor interrupt to the server the

00:08:04,840 --> 00:08:09,250
server is now the performance of the

00:08:07,390 --> 00:08:12,580
transaction is now directly up to help

00:08:09,250 --> 00:08:14,260
quickly the server runs right so the two

00:08:12,580 --> 00:08:16,090
timer rights that the client does after

00:08:14,260 --> 00:08:22,600
it sends its interrupt aren't on the

00:08:16,090 --> 00:08:26,980
critical path right so we started with

00:08:22,600 --> 00:08:28,930
eight MSR exits to write to the apec

00:08:26,980 --> 00:08:31,660
timer but only four of them are on the

00:08:28,930 --> 00:08:33,729
critical path and like we said this is

00:08:31,660 --> 00:08:36,490
because of the no hearts guest Colonel

00:08:33,729 --> 00:08:37,960
so when you have a no hearts Colonel

00:08:36,490 --> 00:08:39,610
every time you switch to the idol

00:08:37,960 --> 00:08:41,260
process or any time you switch from the

00:08:39,610 --> 00:08:43,360
idol process it reconfigures the

00:08:41,260 --> 00:08:45,580
scheduler tick so switching to the idol

00:08:43,360 --> 00:08:47,110
process will disable the scheduler tick

00:08:45,580 --> 00:08:50,200
and switching from the idol process will

00:08:47,110 --> 00:08:51,880
enable the scheduler chick again and for

00:08:50,200 --> 00:08:55,150
this case the scheduler tick is

00:08:51,880 --> 00:08:57,040
implemented with the epic timer why are

00:08:55,150 --> 00:08:59,620
there two well the first one is always

00:08:57,040 --> 00:09:05,920
an HR timer canceled and the second is

00:08:59,620 --> 00:09:07,870
always an HR timer start so this is not

00:09:05,920 --> 00:09:10,570
the I'm not completely familiar with how

00:09:07,870 --> 00:09:11,860
the Nohar kernel is implemented but it

00:09:10,570 --> 00:09:15,430
sounds like this is something that could

00:09:11,860 --> 00:09:16,930
be merged into one ms are right but in

00:09:15,430 --> 00:09:18,930
practice this adds about three to five

00:09:16,930 --> 00:09:21,930
microseconds to the round-trip latency

00:09:18,930 --> 00:09:21,930
so

00:09:24,660 --> 00:09:31,000
halt we talked about halt already but

00:09:28,330 --> 00:09:32,589
the interesting thing that we didn't

00:09:31,000 --> 00:09:35,290
mention is that halt is not on the

00:09:32,589 --> 00:09:37,360
critical path meaning executing the halt

00:09:35,290 --> 00:09:41,589
instruction is not on the critical path

00:09:37,360 --> 00:09:43,870
coming out of halt is the way halt works

00:09:41,589 --> 00:09:46,540
in kvm is when a guest execute the halt

00:09:43,870 --> 00:09:49,089
instruction it traps out of guest mode

00:09:46,540 --> 00:09:51,700
and into and to host mode where kvm

00:09:49,089 --> 00:09:54,160
takes over and kvm places the vcpu

00:09:51,700 --> 00:10:02,920
thread on a wake you and then yields the

00:09:54,160 --> 00:10:05,110
host CPU so the IPPY enter processor

00:10:02,920 --> 00:10:07,450
interrupt plus the halted cpu coming out

00:10:05,110 --> 00:10:09,880
of being halted that it that part is on

00:10:07,450 --> 00:10:11,860
the critical path in the way that works

00:10:09,880 --> 00:10:14,170
in kvm is we start at the bottom left

00:10:11,860 --> 00:10:18,370
where the bcp you one is going to send

00:10:14,170 --> 00:10:20,740
in a nippy to be CP 20 writing to the

00:10:18,370 --> 00:10:22,779
apec interrupt command register sends

00:10:20,740 --> 00:10:25,120
the is how you send an inter processor

00:10:22,779 --> 00:10:29,470
interrupt that causes a trap into kvn

00:10:25,120 --> 00:10:32,140
kvm emulates the FP by in kvm vcpu kick

00:10:29,470 --> 00:10:35,920
waking up the target vcpu from its wait

00:10:32,140 --> 00:10:39,070
queue sometime later on the target the

00:10:35,920 --> 00:10:40,900
where vcpu 0 was running it returns from

00:10:39,070 --> 00:10:42,250
schedule where it previously scheduled

00:10:40,900 --> 00:10:47,709
itself out because it was on the wake

00:10:42,250 --> 00:10:52,230
you and then it resumes execution and it

00:10:47,709 --> 00:10:52,230
wakes up in the AP interrupt handler

00:10:54,619 --> 00:11:01,499
so this process of sending a nippy and

00:10:58,709 --> 00:11:03,329
then the halted CPU coming out of it is

00:11:01,499 --> 00:11:06,029
entirely implemented in hardware when

00:11:03,329 --> 00:11:08,040
you're running on bare metal and unlike

00:11:06,029 --> 00:11:09,959
the epic timer the entire length of the

00:11:08,040 --> 00:11:11,670
vm exit is not on the critical path so

00:11:09,959 --> 00:11:12,989
we can't just time how long each p.m.

00:11:11,670 --> 00:11:16,110
ace it takes to figure out what the

00:11:12,989 --> 00:11:17,459
overhead is but we can do the same thing

00:11:16,110 --> 00:11:21,929
on real hardware and compare the

00:11:17,459 --> 00:11:24,959
performance so I wrote a micro benchmark

00:11:21,929 --> 00:11:27,809
using kvm unit tests where you have one

00:11:24,959 --> 00:11:30,540
vcpu execute halt and then some short

00:11:27,809 --> 00:11:32,699
delay later another vcpu reads its

00:11:30,540 --> 00:11:34,889
timestamp counter sends a nippy to the

00:11:32,699 --> 00:11:37,410
target CPU and then the cpu that was

00:11:34,889 --> 00:11:40,230
halted bcp 20 in the first instruction

00:11:37,410 --> 00:11:44,519
of its iffy interrupt handler reads the

00:11:40,230 --> 00:11:46,230
TSE again and then that lets you know

00:11:44,519 --> 00:11:50,220
the late and see from started the IPPY

00:11:46,230 --> 00:11:52,739
to coming out of the halt so back on

00:11:50,220 --> 00:11:55,069
that diagram before we read the tsc

00:11:52,739 --> 00:11:56,850
right before writing to the apec

00:11:55,069 --> 00:11:58,439
interrupt command register and then

00:11:56,850 --> 00:12:02,519
right when we come out of fall on the

00:11:58,439 --> 00:12:06,059
far side rets see if you haven't heard

00:12:02,519 --> 00:12:11,160
of it as a cycle accurate counter in x86

00:12:06,059 --> 00:12:15,899
so you can read so the performance of

00:12:11,160 --> 00:12:18,139
kvm versus sandy bridge it's about 12

00:12:15,899 --> 00:12:20,639
times slower and the median case

00:12:18,139 --> 00:12:24,600
eyewitness one case those 400 times

00:12:20,639 --> 00:12:26,369
slower but in real terms it's about five

00:12:24,600 --> 00:12:28,980
and a half micro seconds to go from

00:12:26,369 --> 00:12:31,529
delivering the sending the apt to coming

00:12:28,980 --> 00:12:35,369
out of halt whereas on real hardware

00:12:31,529 --> 00:12:37,679
it's half of a microsecond and you get

00:12:35,369 --> 00:12:40,739
similar performance on more modern

00:12:37,679 --> 00:12:42,179
hardware I don't have the bare metal

00:12:40,739 --> 00:12:43,429
performance but the kvm phone this is

00:12:42,179 --> 00:12:45,989
about the same five and a half

00:12:43,429 --> 00:12:49,399
microseconds an Ivy Bridge in about 5 90

00:12:45,989 --> 00:12:49,399
seconds on hassle

00:12:51,760 --> 00:12:58,490
so this benchmark is not using the

00:12:54,290 --> 00:13:00,550
floating-point unit so that's usually a

00:12:58,490 --> 00:13:03,560
good candidate for what's slow about

00:13:00,550 --> 00:13:05,990
switching to and from CPUs but it's not

00:13:03,560 --> 00:13:08,600
being used here the host is pretty much

00:13:05,990 --> 00:13:11,090
idle while this benchmark is running so

00:13:08,600 --> 00:13:13,040
the virtual CPU when it's coming out of

00:13:11,090 --> 00:13:16,670
its wait queue is not contending for CPU

00:13:13,040 --> 00:13:18,170
time with other threads and host power

00:13:16,670 --> 00:13:20,270
management is disabled so they're not

00:13:18,170 --> 00:13:22,790
going into extremely deep sleep sleep

00:13:20,270 --> 00:13:28,550
state and then taking many micro seconds

00:13:22,790 --> 00:13:32,080
to come out of it so one of these steps

00:13:28,550 --> 00:13:34,970
is slow but we don't actually know which

00:13:32,080 --> 00:13:39,250
so you can throw a more read TSE at the

00:13:34,970 --> 00:13:39,250
problem and measure each step

00:13:42,560 --> 00:13:47,700
so delivering the EPI is pretty quick

00:13:45,360 --> 00:13:49,920
the hardware transition is just 400

00:13:47,700 --> 00:13:51,600
cycles and then getting to the place

00:13:49,920 --> 00:13:53,220
where you wake up the target vcpu is

00:13:51,600 --> 00:13:56,910
just a couple another couple hundred

00:13:53,220 --> 00:13:59,370
microsite cycles the really slow part is

00:13:56,910 --> 00:14:05,329
waking up the target vcpu from its wait

00:13:59,370 --> 00:14:09,750
queue so that takes about at a minimum

00:14:05,329 --> 00:14:13,680
70 300 cycles or 8500 cycles in median

00:14:09,750 --> 00:14:15,510
case and then getting the CPU to run

00:14:13,680 --> 00:14:19,769
again is pretty quick but also slow at

00:14:15,510 --> 00:14:24,600
around three thousand samples so this

00:14:19,769 --> 00:14:27,209
isn't too surprising scheduling is often

00:14:24,600 --> 00:14:28,680
slow context switching is often slow but

00:14:27,209 --> 00:14:31,140
it's slow even in the completely

00:14:28,680 --> 00:14:33,630
uncontained and completely cash hot case

00:14:31,140 --> 00:14:36,240
so this there's no other threads that

00:14:33,630 --> 00:14:38,130
are scheduling on this virtual CPUs

00:14:36,240 --> 00:14:40,110
physical CPU that's blowing out the

00:14:38,130 --> 00:14:43,829
entire cache and then it has to catch

00:14:40,110 --> 00:14:45,899
everything in so imagine it would be

00:14:43,829 --> 00:14:49,470
even slower if you're running other

00:14:45,899 --> 00:14:51,839
threads on this physical CPU so we can

00:14:49,470 --> 00:14:53,730
run an experiment where instead of when

00:14:51,839 --> 00:14:56,100
we halt instead of putting ourselves on

00:14:53,730 --> 00:14:57,630
a wait queue scheduling ourselves out if

00:14:56,100 --> 00:14:59,130
we just pull for that interrupt inter

00:14:57,630 --> 00:15:03,839
processor interrupt what's the

00:14:59,130 --> 00:15:05,339
performance of that pretty good so we

00:15:03,839 --> 00:15:08,010
started at about five and a half

00:15:05,339 --> 00:15:09,390
microseconds before when we were

00:15:08,010 --> 00:15:12,120
scheduling ourselves out and now it's

00:15:09,390 --> 00:15:15,209
about one and a half 1.7 microseconds

00:15:12,120 --> 00:15:16,980
when we don't schedule still about a

00:15:15,209 --> 00:15:19,649
microsecond slower than bare metal but

00:15:16,980 --> 00:15:23,089
not too bad and similar performance on

00:15:19,649 --> 00:15:23,089
more modern however

00:15:27,390 --> 00:15:33,710
if you look at the per Section cost of

00:15:31,410 --> 00:15:37,170
each after you eliminate scheduling

00:15:33,710 --> 00:15:41,040
pretty obviously the cost of sending the

00:15:37,170 --> 00:15:47,190
kick to the target CPU goes from a lot

00:15:41,040 --> 00:15:49,170
to very little the going from returning

00:15:47,190 --> 00:15:51,390
from schedule to getting the DCP to run

00:15:49,170 --> 00:15:53,070
again also gets faster and then the

00:15:51,390 --> 00:16:00,030
hardware transition also gets a little

00:15:53,070 --> 00:16:03,990
faster so we can eliminate almost all

00:16:00,030 --> 00:16:06,740
the latency by just not scheduling so we

00:16:03,990 --> 00:16:09,720
should just never schedule right but

00:16:06,740 --> 00:16:10,890
scheduling is actually very often the

00:16:09,720 --> 00:16:13,050
right thing to do and it's often very

00:16:10,890 --> 00:16:15,060
the right thing to do for a performance

00:16:13,050 --> 00:16:17,520
from a performance standpoint because

00:16:15,060 --> 00:16:20,610
imagine your virtual CPU is halting

00:16:17,520 --> 00:16:23,220
because it's waiting for a network

00:16:20,610 --> 00:16:25,560
packet and your network io thread is not

00:16:23,220 --> 00:16:27,360
getting being able to run because your

00:16:25,560 --> 00:16:30,360
virtual CPU is polling for the network

00:16:27,360 --> 00:16:34,650
packet to come in so in that case it's

00:16:30,360 --> 00:16:36,180
to your benefit to schedule so it can

00:16:34,650 --> 00:16:37,530
improve performance scheduling can

00:16:36,180 --> 00:16:41,790
improve performance but scheduling can

00:16:37,530 --> 00:16:44,610
harm performance as well as you see so

00:16:41,790 --> 00:16:47,930
we can find a middle ground which is

00:16:44,610 --> 00:16:51,330
halt pulling so imagine when you halt

00:16:47,930 --> 00:16:53,400
for a short period of time you pull for

00:16:51,330 --> 00:16:55,080
the interrupt as long as there's no one

00:16:53,400 --> 00:16:59,430
else that needs to schedule on your

00:16:55,080 --> 00:17:02,370
physical CPU and if you if the interrupt

00:16:59,430 --> 00:17:04,920
is delivered you return and you get your

00:17:02,370 --> 00:17:06,660
quick performance of not scheduling if

00:17:04,920 --> 00:17:08,880
the interrupt doesn't arrive you've

00:17:06,660 --> 00:17:12,030
wasted a small amount of CPU and you

00:17:08,880 --> 00:17:13,620
still scheduled yourself out so the pros

00:17:12,030 --> 00:17:15,959
of this it works on short halts which is

00:17:13,620 --> 00:17:17,579
where the performance actually matters

00:17:15,959 --> 00:17:19,079
if you have a ten millisecond long halt

00:17:17,579 --> 00:17:21,230
you don't really care about a 5 micro

00:17:19,079 --> 00:17:23,280
second game from coming out of that hole

00:17:21,230 --> 00:17:25,079
and you continue to not block the

00:17:23,280 --> 00:17:27,570
progress of your other thread so that

00:17:25,079 --> 00:17:29,820
case where you or virtual CPU is blocked

00:17:27,570 --> 00:17:31,740
on your networks red running this

00:17:29,820 --> 00:17:32,460
approach would still continue to not

00:17:31,740 --> 00:17:35,840
block that now

00:17:32,460 --> 00:17:39,780
third from the downside is you have a

00:17:35,840 --> 00:17:42,810
slight increase in CPU usage so if you

00:17:39,780 --> 00:17:44,580
have an ID lvm that's just halting

00:17:42,810 --> 00:17:46,650
because it has nothing to do meaning

00:17:44,580 --> 00:17:48,360
it's halting for no reason and pulling

00:17:46,650 --> 00:17:52,980
is not the right thing to do you get

00:17:48,360 --> 00:17:54,600
about a one-percent CPU overhead but if

00:17:52,980 --> 00:17:57,150
you care about turbo this doesn't

00:17:54,600 --> 00:18:03,300
actually affect the turbo bins of course

00:17:57,150 --> 00:18:05,610
that are doing real work so how does

00:18:03,300 --> 00:18:09,290
help pulling do it improves your

00:18:05,610 --> 00:18:11,850
memcache round-trip latency via 11.5 x

00:18:09,290 --> 00:18:13,950
when you're the windows event object

00:18:11,850 --> 00:18:17,460
sweet signal primitive about half of

00:18:13,950 --> 00:18:20,520
latency and you can see we eliminated

00:18:17,460 --> 00:18:24,240
about 10 to 15 micro seconds from our

00:18:20,520 --> 00:18:28,140
loopback tcp are our latency the

00:18:24,240 --> 00:18:29,880
remaining performance gap comes from the

00:18:28,140 --> 00:18:32,340
rights still continuing to write to the

00:18:29,880 --> 00:18:34,260
epic timer there's still that one and a

00:18:32,340 --> 00:18:37,530
half microsecond overhead from

00:18:34,260 --> 00:18:41,160
delivering the AP to the halton cpu

00:18:37,530 --> 00:18:43,680
coming up so actually if you if you were

00:18:41,160 --> 00:18:45,570
to run your guests with idle equals pole

00:18:43,680 --> 00:18:49,440
so it just never halts any ran your

00:18:45,570 --> 00:18:51,180
guests with no hurt soft you would

00:18:49,440 --> 00:18:53,280
actually get that blue line there as

00:18:51,180 --> 00:18:56,930
your virtual performance but you don't

00:18:53,280 --> 00:18:59,490
want to do those things for many reasons

00:18:56,930 --> 00:19:01,170
so the best part about Hall pulling is

00:18:59,490 --> 00:19:03,060
you can already play with it Paulo

00:19:01,170 --> 00:19:06,510
already wrote the patch and it's been

00:19:03,060 --> 00:19:09,020
merged into the 40 colonel I believe the

00:19:06,510 --> 00:19:12,200
way you use it is you set the halt pull

00:19:09,020 --> 00:19:14,910
nanoseconds kvm module parameter and

00:19:12,200 --> 00:19:16,560
currently it defaults to off so you have

00:19:14,910 --> 00:19:21,690
to explicitly turn it on if you want to

00:19:16,560 --> 00:19:24,270
use it and there's further improvements

00:19:21,690 --> 00:19:26,250
that we can make one that's very simple

00:19:24,270 --> 00:19:29,940
is to just turn pulling on and off as

00:19:26,250 --> 00:19:32,310
it's needed so if you notice that CPUs

00:19:29,940 --> 00:19:34,170
are halting and pulling is always

00:19:32,310 --> 00:19:35,820
failing just turn pulling off and then

00:19:34,170 --> 00:19:37,410
if you notice you start getting a lot of

00:19:35,820 --> 00:19:39,570
short halts again turn it back on and

00:19:37,410 --> 00:19:40,600
that will easily eliminate the one

00:19:39,570 --> 00:19:44,050
percent overhead that I

00:19:40,600 --> 00:19:45,940
mentioned earlier slightly a harder

00:19:44,050 --> 00:19:48,220
improvement but maybe possible is

00:19:45,940 --> 00:19:51,270
automatically setting how long the pulp

00:19:48,220 --> 00:19:53,830
and burying it based on the workload I

00:19:51,270 --> 00:19:56,230
think this is an open question at the

00:19:53,830 --> 00:19:59,560
moment so people have ideas post them to

00:19:56,230 --> 00:20:01,330
the mailing list and then the last

00:19:59,560 --> 00:20:03,010
improvement is called lazy context

00:20:01,330 --> 00:20:05,350
switching which was introduced a few

00:20:03,010 --> 00:20:09,220
years ago also in a message passing talk

00:20:05,350 --> 00:20:11,110
and this is the halt polling for any

00:20:09,220 --> 00:20:19,810
component of the colonel to use not just

00:20:11,110 --> 00:20:21,430
kbm so yep so that's about it message

00:20:19,810 --> 00:20:22,960
passing is slow it requires

00:20:21,430 --> 00:20:25,330
virtualization because it requires

00:20:22,960 --> 00:20:28,090
virtualizing halts inter processor

00:20:25,330 --> 00:20:31,630
interrupts and for a no hearts Linux

00:20:28,090 --> 00:20:34,480
guest the apec timer we can save about

00:20:31,630 --> 00:20:37,870
10 to 15 micro seconds on round-trip

00:20:34,480 --> 00:20:40,240
latency by using all polling and the

00:20:37,870 --> 00:20:41,950
remaining latency is as I said those

00:20:40,240 --> 00:20:44,620
four rights the APEC timer they add

00:20:41,950 --> 00:20:46,990
about three to five microseconds sending

00:20:44,620 --> 00:20:48,970
the iffy still has to be still causes of

00:20:46,990 --> 00:20:51,400
via vm exit and has to be virtualized

00:20:48,970 --> 00:20:53,620
and then there's still about one and a

00:20:51,400 --> 00:20:55,180
half microseconds from the halt wake up

00:20:53,620 --> 00:20:58,480
and then so for round-trip it's about

00:20:55,180 --> 00:21:00,760
three microseconds okay that's all i

00:20:58,480 --> 00:21:02,790
have for slides does anybody have any

00:21:00,760 --> 00:21:02,790
questions

00:21:13,410 --> 00:21:22,560
just a comments of probably if you use a

00:21:17,970 --> 00:21:24,420
deadline tsc timer we maybe you can

00:21:22,560 --> 00:21:27,960
reduce a number but you know be a

00:21:24,420 --> 00:21:30,150
magazine the tsc deadline timer same

00:21:27,960 --> 00:21:33,920
same story as a big timer still a mess

00:21:30,150 --> 00:21:37,080
are right still customize it ok so you

00:21:33,920 --> 00:21:41,640
I'm wondering why you need to you know

00:21:37,080 --> 00:21:44,990
our right to the Amazon twice yeah yeah

00:21:41,640 --> 00:21:48,510
so yeah I think you know it should be

00:21:44,990 --> 00:21:51,240
dumb just the one right but dumb I agree

00:21:48,510 --> 00:21:55,550
if you use it that when casting yeah so

00:21:51,240 --> 00:22:00,170
the other thing is I p I if you you use

00:21:55,550 --> 00:22:05,190
posted in top then probably you can

00:22:00,170 --> 00:22:07,820
avoid a vm exit the problem with posted

00:22:05,190 --> 00:22:12,300
interrupts in this case is there's no

00:22:07,820 --> 00:22:13,860
virtualized lists halt so the vcp you

00:22:12,300 --> 00:22:15,690
isn't running when you're delivering the

00:22:13,860 --> 00:22:17,220
interrupts so post it interrupts doesn't

00:22:15,690 --> 00:22:25,410
give you any benefit if you to not run

00:22:17,220 --> 00:22:28,200
you okay okay in the back have you have

00:22:25,410 --> 00:22:31,380
you looked at using config no hurts full

00:22:28,200 --> 00:22:33,990
as opposed to just know hurts the no

00:22:31,380 --> 00:22:36,210
hurts just goes tickle asan idle no

00:22:33,990 --> 00:22:38,610
hurts full goes tick list if you have

00:22:36,210 --> 00:22:41,490
only one process to run so if there's

00:22:38,610 --> 00:22:44,040
nothing contending with kvm then it

00:22:41,490 --> 00:22:45,750
should already be in set a timer mode

00:22:44,040 --> 00:22:48,180
and you don't have to reset that when

00:22:45,750 --> 00:22:49,770
you go idle which yeah that sounds like

00:22:48,180 --> 00:22:54,900
that sounds like it would eliminate the

00:22:49,770 --> 00:22:59,780
big timers well thank you um in your

00:22:54,900 --> 00:23:02,760
description of the tcp our our our test

00:22:59,780 --> 00:23:05,700
you just described a single server and

00:23:02,760 --> 00:23:08,760
client but you've got more than 2 v cpus

00:23:05,700 --> 00:23:12,090
i'm not sure how are you actually doing

00:23:08,760 --> 00:23:12,900
a pink pom around multiple processes

00:23:12,090 --> 00:23:17,940
there or

00:23:12,900 --> 00:23:21,180
yeah so the idea with going from more

00:23:17,940 --> 00:23:23,070
cpus there's only two threads running so

00:23:21,180 --> 00:23:25,620
it shouldn't matter right and it doesn't

00:23:23,070 --> 00:23:28,530
matter the sort of variation is just

00:23:25,620 --> 00:23:33,570
noise could have left it at two okay

00:23:28,530 --> 00:23:35,640
yeah but have you done the case where it

00:23:33,570 --> 00:23:37,470
might matter is if you're running if two

00:23:35,640 --> 00:23:40,380
CPUs are running on two hyper threads

00:23:37,470 --> 00:23:42,270
and then if you have more cpus that are

00:23:40,380 --> 00:23:43,970
able to run on now two separate physical

00:23:42,270 --> 00:23:46,970
cores the performance make it better

00:23:43,970 --> 00:23:50,490
right have you done any timings with

00:23:46,970 --> 00:23:52,050
with really more than 22 threads so if

00:23:50,490 --> 00:23:55,590
you pass the token round multiple

00:23:52,050 --> 00:23:59,070
processes for example yeah yeah so he

00:23:55,590 --> 00:24:01,110
now with tcp RR but with the windows of

00:23:59,070 --> 00:24:04,200
n objects Randolph work loaded and

00:24:01,110 --> 00:24:06,710
windows guest and it got forty percent

00:24:04,200 --> 00:24:09,990
improvement with helpful and that was a

00:24:06,710 --> 00:24:11,850
HCP you I don't know how many threads

00:24:09,990 --> 00:24:14,480
but all of CPS were utilized during that

00:24:11,850 --> 00:24:14,480
that just

00:24:17,280 --> 00:24:20,570

YouTube URL: https://www.youtube.com/watch?v=p85FFrloLFg


