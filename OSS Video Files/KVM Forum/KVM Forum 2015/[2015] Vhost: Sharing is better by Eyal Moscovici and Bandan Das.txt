Title: [2015] Vhost: Sharing is better by Eyal Moscovici and Bandan Das
Publication date: 2015-09-06
Playlist: KVM Forum 2015
Description: 
	vhost provides virtio emulation in the kernel and avoids expensive system calls. This talk focuses on various mechanisms that have been proposed to improve vhost scalability and performance. The current stable design uses a dedicated kernel worker thread for every vhost device. There are alternative proposals such as a vhost thread running on each CPU core in lieu of creating a per-device thread, using Concurrency managed system workqueues to handle work, and polling virt queues instead of relying on notifications. We start with an introduction to vhost, descriptions and potential complications with several proposed RFCs and some performance comparisons. We hope that this discussion will help us converge to a stable next generation vhost architecture. Although the design implications are applicable to NET and SCSI virtual devices, we would be primarily focusing on Network performance.

Bandan Das
Software Engineer, Red Hat
Bandan Das is a Software Engineer in the Virtualization group at Red Hat. He likes working on the Linux kernel, he usually picks up work that everyone else has forgotten about or don't want to work on.

Eyal Moscovici
Eyal is a research student at IBM Haifa Research Lab, Israel. He is currently working towards a M.Sc. degree in computer science at the Technion, Israel Institute of Technology under the advisement of Prof. Dan Tsafrir. He is researching I/O virtualization optimization to improve server performance in cloud data centers. He has been working on enhancements for vhost since 2013 as part of his research, focusing on improving the performance of paravirtual I/O in the Linux kernel.

Slides: https://drive.google.com/file/d/0BzyAwvVlQckeQ2xuSm9sM1lmTkU/view?usp=sharing
Captions: 
	00:00:15,379 --> 00:00:22,830
my name is bundan and i'm here with al

00:00:19,850 --> 00:00:30,390
and we'd like to talk a little bit about

00:00:22,830 --> 00:00:32,430
some of the work that disappeared so

00:00:30,390 --> 00:00:37,170
fast forward let's go to one's first

00:00:32,430 --> 00:00:38,579
slide so this is to talk about a V host

00:00:37,170 --> 00:00:43,649
and some of the work that we have been

00:00:38,579 --> 00:00:48,149
trying to do to get a kind of if you

00:00:43,649 --> 00:00:52,140
will next generation design into the

00:00:48,149 --> 00:00:54,750
upstream linux kernel and the scope of

00:00:52,140 --> 00:00:57,719
this talk is not something that we have

00:00:54,750 --> 00:00:59,340
already done but rather something that

00:00:57,719 --> 00:01:02,640
still work in progress we don't even

00:00:59,340 --> 00:01:05,549
know if it might go in we thought that

00:01:02,640 --> 00:01:08,460
it's probably a good idea to discuss

00:01:05,549 --> 00:01:12,210
this with the me no greater community

00:01:08,460 --> 00:01:14,060
and find out where we are so we have

00:01:12,210 --> 00:01:18,299
that this is the layout we just have a

00:01:14,060 --> 00:01:21,530
very basic introduction and then we go

00:01:18,299 --> 00:01:25,380
to some of the ideas that that have been

00:01:21,530 --> 00:01:29,700
you know posted as RFC is mostly in the

00:01:25,380 --> 00:01:31,439
recent few years and then a major

00:01:29,700 --> 00:01:33,750
portion of the target is actually Elvis

00:01:31,439 --> 00:01:37,680
which is I think some considerable work

00:01:33,750 --> 00:01:42,150
that has been done on this in this with

00:01:37,680 --> 00:01:43,439
regards to this topic and and some of

00:01:42,150 --> 00:01:46,470
our up streaming efforts and the

00:01:43,439 --> 00:01:50,759
corresponding results will follow after

00:01:46,470 --> 00:01:53,790
that so this is a slide about farrah

00:01:50,759 --> 00:01:55,890
virtualization so the basic idea if I'm

00:01:53,790 --> 00:01:58,880
pretty sure and everyone knows but just

00:01:55,890 --> 00:02:01,649
in case that we don't have to assume

00:01:58,880 --> 00:02:04,079
that the guest has to be in complete

00:02:01,649 --> 00:02:06,240
darkness as far as where it is running

00:02:04,079 --> 00:02:10,940
rather we can actually use it your

00:02:06,240 --> 00:02:14,010
advantage and this actually

00:02:10,940 --> 00:02:16,200
actually also likes the guest sorry the

00:02:14,010 --> 00:02:20,790
hosts have a little bit of control over

00:02:16,200 --> 00:02:24,780
what the guests can do and as obvious

00:02:20,790 --> 00:02:27,390
you know this is still a software

00:02:24,780 --> 00:02:30,860
emulation so there are scalability

00:02:27,390 --> 00:02:38,090
limitations to that but one of the major

00:02:30,860 --> 00:02:38,090
outcomes from this idea is Verdi oh and

00:02:38,270 --> 00:02:45,780
what the host is in the kernel is that

00:02:42,090 --> 00:02:47,610
it's a V hostnet interface a word i own

00:02:45,780 --> 00:02:50,370
at interface that is actually

00:02:47,610 --> 00:02:53,190
implemented inside the colonel it's not

00:02:50,370 --> 00:02:57,500
a full-fledged what I wanted interface

00:02:53,190 --> 00:03:02,160
but it just implements the data part

00:02:57,500 --> 00:03:04,320
data plane and one of the advantages is

00:03:02,160 --> 00:03:09,420
that this is the much better user space

00:03:04,320 --> 00:03:13,320
and Colonel API which is leveraging on

00:03:09,420 --> 00:03:16,140
what I owe itself and so this is a very

00:03:13,320 --> 00:03:19,470
crappy diagram I'm sure it's not

00:03:16,140 --> 00:03:21,120
beautiful at all but what we are trying

00:03:19,470 --> 00:03:24,840
to show is that this is the colonel

00:03:21,120 --> 00:03:29,400
thread and that's the guest running on

00:03:24,840 --> 00:03:31,620
top of a kvm and anytime the guest needs

00:03:29,400 --> 00:03:35,550
some servicing it notifies the worker

00:03:31,620 --> 00:03:37,140
thread in the kernel and also you know

00:03:35,550 --> 00:03:39,690
writes data to the buffers which are

00:03:37,140 --> 00:03:42,030
shared between the guest and the colonel

00:03:39,690 --> 00:03:43,980
worker thread and then when the worker

00:03:42,030 --> 00:03:45,030
is done with the servicing the worker

00:03:43,980 --> 00:03:47,630
thread is to actually the one that's

00:03:45,030 --> 00:03:50,010
interacting with the network stack and

00:03:47,630 --> 00:03:51,540
it updates the buffers and sends a

00:03:50,010 --> 00:03:54,420
notification again and so this is all

00:03:51,540 --> 00:03:58,380
just you know very basic of i/o event

00:03:54,420 --> 00:04:01,410
FDI TRP is kind of the backbone of no

00:03:58,380 --> 00:04:07,980
gate kbm guest Colonel guests and

00:04:01,410 --> 00:04:09,810
hypervisor interaction so one of the so

00:04:07,980 --> 00:04:14,090
that the design of the current V host

00:04:09,810 --> 00:04:21,079
it's just that we have one kernel worker

00:04:14,090 --> 00:04:24,080
per device or per word q TX Rx pair and

00:04:21,079 --> 00:04:26,240
so and that is basically the like the

00:04:24,080 --> 00:04:30,020
starting point of why we could have a

00:04:26,240 --> 00:04:31,789
better design so it might be the case

00:04:30,020 --> 00:04:36,770
that we don't really need a dedicated

00:04:31,789 --> 00:04:38,659
Colonel thread for one device we could

00:04:36,770 --> 00:04:41,479
probably kind of share responsibilities

00:04:38,659 --> 00:04:43,669
as and we can have one kernel thread

00:04:41,479 --> 00:04:46,189
that actually the services a lot of

00:04:43,669 --> 00:04:50,090
guests and we don't have to actually

00:04:46,189 --> 00:04:54,830
kind of couple tightly couple a you know

00:04:50,090 --> 00:04:56,479
what Q pair with a kernel thread so and

00:04:54,830 --> 00:05:00,550
that is kind of the basis for all these

00:04:56,479 --> 00:05:03,550
scalable you know design RFC's and

00:05:00,550 --> 00:05:09,289
ideally of course we don't want any

00:05:03,550 --> 00:05:11,090
modifications in the user space and so

00:05:09,289 --> 00:05:15,919
the first I'm going to go to is one

00:05:11,090 --> 00:05:18,919
presented in around 2012 this is so

00:05:15,919 --> 00:05:20,719
don't go by the slide titles I just made

00:05:18,919 --> 00:05:22,909
it up just to look look a little bit

00:05:20,719 --> 00:05:26,990
dramatic but it's not a convention so

00:05:22,909 --> 00:05:29,750
what I meant to say is we have like

00:05:26,990 --> 00:05:32,419
dedicated Colonel workers videos to

00:05:29,750 --> 00:05:36,229
workers on each CP course on your system

00:05:32,419 --> 00:05:39,889
and we have some kind of a NUMA aware

00:05:36,229 --> 00:05:43,370
scheduling on top of it so if the guest

00:05:39,889 --> 00:05:45,589
needs servicing you know the word cubes

00:05:43,370 --> 00:05:48,289
interact with the new aware scheduler

00:05:45,589 --> 00:05:51,469
and then you know they're so the key the

00:05:48,289 --> 00:05:53,650
request goes to one of the one of the

00:05:51,469 --> 00:05:57,680
course when one of the workers below

00:05:53,650 --> 00:06:03,129
keeping in mind a new my locality and

00:05:57,680 --> 00:06:09,710
this was also presented at LPC 2012 and

00:06:03,129 --> 00:06:15,169
so this is all good but the question is

00:06:09,710 --> 00:06:18,199
in this in this design we have like if

00:06:15,169 --> 00:06:20,000
you will always on feds each core has a

00:06:18,199 --> 00:06:23,330
thread and the question obvious easy

00:06:20,000 --> 00:06:25,909
question is do we really need threads to

00:06:23,330 --> 00:06:30,050
be running and all go all the course of

00:06:25,909 --> 00:06:31,699
the system and so so using using that as

00:06:30,050 --> 00:06:33,240
a starting point we can say maybe we

00:06:31,699 --> 00:06:35,350
don't have to run

00:06:33,240 --> 00:06:38,770
don't have to utilize all the course

00:06:35,350 --> 00:06:41,890
rather why not have a single thread

00:06:38,770 --> 00:06:45,630
that's running on a core and shared by

00:06:41,890 --> 00:06:49,030
multiple guests and that's what is the

00:06:45,630 --> 00:06:53,910
next design that was proposed and it's

00:06:49,030 --> 00:06:56,440
basically Elvis is actually i think a a

00:06:53,910 --> 00:06:59,530
subset of what elvis proposes is what i

00:06:56,440 --> 00:07:03,610
just said and which is that it proposes

00:06:59,530 --> 00:07:05,770
that we you know divide the system into

00:07:03,610 --> 00:07:07,480
a part which actually takes care of

00:07:05,770 --> 00:07:10,750
executing the guests and thus this

00:07:07,480 --> 00:07:15,670
dedicated I of course that only do the V

00:07:10,750 --> 00:07:17,680
was to work and you know you could

00:07:15,670 --> 00:07:20,350
probably think of this as spend the vos

00:07:17,680 --> 00:07:22,270
worker threads min 21 while specific cpu

00:07:20,350 --> 00:07:23,320
core on your system and that's where the

00:07:22,270 --> 00:07:26,530
video she worker thread is always

00:07:23,320 --> 00:07:29,890
running and I let a I'll talk about this

00:07:26,530 --> 00:07:32,530
a little bit in detail yeah so the Elvis

00:07:29,890 --> 00:07:38,740
work itself was already presented in

00:07:32,530 --> 00:07:41,200
case in forum and 2013 so I'm just doing

00:07:38,740 --> 00:07:45,340
a small recap of what was done in that

00:07:41,200 --> 00:07:47,950
work and and then abandon whilst will

00:07:45,340 --> 00:07:51,610
continue on with the up streaming effort

00:07:47,950 --> 00:07:54,460
that we're doing right now so Elvis and

00:07:51,610 --> 00:07:57,220
in essence is taking the course in the

00:07:54,460 --> 00:08:00,610
system and divided them into two groups

00:07:57,220 --> 00:08:02,350
one running the vm code one running

00:08:00,610 --> 00:08:07,120
running the VMS and the other one

00:08:02,350 --> 00:08:10,450
running the vm I of threads okay so if

00:08:07,120 --> 00:08:13,390
you divide divide the system into two

00:08:10,450 --> 00:08:19,000
groups the groups which runs the i/o

00:08:13,390 --> 00:08:23,560
threats can be unified and we can use a

00:08:19,000 --> 00:08:27,220
single thread per core and this and this

00:08:23,560 --> 00:08:32,409
thread can run multiple layer devices

00:08:27,220 --> 00:08:34,810
and when you have when you have one core

00:08:32,409 --> 00:08:37,539
running multiple devices you can do

00:08:34,810 --> 00:08:40,779
lot of things for example you can do

00:08:37,539 --> 00:08:44,470
we're pulling okay so this is one thing

00:08:40,779 --> 00:08:47,080
that is not currently not currently able

00:08:44,470 --> 00:08:49,240
to do with the original design right

00:08:47,080 --> 00:08:51,040
because when you have a dedicated where

00:08:49,240 --> 00:08:56,640
when you have like a dedicated thread

00:08:51,040 --> 00:09:00,640
for i/o which runs multiple VMs I owe a

00:08:56,640 --> 00:09:04,390
threat I owe devices it makes more sense

00:09:00,640 --> 00:09:07,690
to try and do some polling and what

00:09:04,390 --> 00:09:09,400
polling polling allows us to do is it

00:09:07,690 --> 00:09:12,100
allows us to reduce the amount of

00:09:09,400 --> 00:09:13,779
interrupts in the system okay so

00:09:12,100 --> 00:09:16,390
currently in the traditional part of

00:09:13,779 --> 00:09:19,330
virtual io design when when a guest

00:09:16,390 --> 00:09:22,570
wants to see what to notify and send a

00:09:19,330 --> 00:09:27,310
packet onto the network it has to exit

00:09:22,570 --> 00:09:30,700
in order to in order to notify the view

00:09:27,310 --> 00:09:34,570
thread that this hat it has a packet

00:09:30,700 --> 00:09:37,810
that it wants to send okay and when an

00:09:34,570 --> 00:09:41,080
after viest sent that packet it has to

00:09:37,810 --> 00:09:45,810
exit again for in order for a veal stew

00:09:41,080 --> 00:09:48,610
notified okay so the second part was was

00:09:45,810 --> 00:09:54,520
partially solved already in in Linux but

00:09:48,610 --> 00:09:56,770
but the former part of exiting can be

00:09:54,520 --> 00:09:59,130
solved using polling all right because

00:09:56,770 --> 00:10:03,070
if you use polling you don't need the vm

00:09:59,130 --> 00:10:05,620
to exit and then a notified a io thread

00:10:03,070 --> 00:10:11,040
that has packets daioh thread can pull

00:10:05,620 --> 00:10:11,040
on the vm state and use it without and

00:10:11,339 --> 00:10:20,110
know that just something to send without

00:10:15,000 --> 00:10:24,000
exiting so it reduces exit tremendously

00:10:20,110 --> 00:10:34,089
with which exits are a major performance

00:10:24,000 --> 00:10:37,180
penalty and and another thing that L so

00:10:34,089 --> 00:10:42,640
another thing that we did in Elvis was

00:10:37,180 --> 00:10:47,410
doing with doing exit less interrupts

00:10:42,640 --> 00:10:48,190
right we're doing only running your

00:10:47,410 --> 00:10:51,250
threads without

00:10:48,190 --> 00:10:54,760
causing exits okay and and the way we

00:10:51,250 --> 00:10:59,110
did it use it was using a le exit less

00:10:54,760 --> 00:11:02,620
interrupt but but le is is it's kind of

00:10:59,110 --> 00:11:05,170
hard to upstream so today in view of

00:11:02,620 --> 00:11:11,470
there are several component that can

00:11:05,170 --> 00:11:15,040
change it we can use the kvm p voi which

00:11:11,470 --> 00:11:17,920
reduces the end of interrupt when when a

00:11:15,040 --> 00:11:20,200
guest when I guess sends a packet and

00:11:17,920 --> 00:11:23,500
the packet is already sent then the vo

00:11:20,200 --> 00:11:27,040
says to notify the guest and in their

00:11:23,500 --> 00:11:29,500
own wheels we can do it without exiting

00:11:27,040 --> 00:11:35,680
without causing an exit just notify the

00:11:29,500 --> 00:11:37,810
guest and another thing in Elvis we had

00:11:35,680 --> 00:11:42,490
like software posted interrupts and now

00:11:37,810 --> 00:11:45,160
intel has a VT d technology which allows

00:11:42,490 --> 00:11:47,200
for posted interrupts in hardware so

00:11:45,160 --> 00:11:49,150
there's special hardware do posted

00:11:47,200 --> 00:11:52,690
interrupts and opossum interrupts allow

00:11:49,150 --> 00:11:55,150
us to reduce the exit that that are

00:11:52,690 --> 00:11:56,800
caused when the vm gets a packet right

00:11:55,150 --> 00:11:59,820
when we host wants to run to fight

00:11:56,800 --> 00:12:06,460
against it just a new packet it has to

00:11:59,820 --> 00:12:15,040
cause a vm exit so now bandon will take

00:12:06,460 --> 00:12:19,060
over explained up streaming effort so so

00:12:15,040 --> 00:12:21,790
so Elvis proposes a lot of things and we

00:12:19,060 --> 00:12:25,240
thought that it's probably a good idea

00:12:21,790 --> 00:12:30,940
to kind of break it down and start with

00:12:25,240 --> 00:12:32,320
the most basic which is which is you

00:12:30,940 --> 00:12:34,720
know figuring out if it's possible if

00:12:32,320 --> 00:12:37,480
it's if it's worth it to kind of

00:12:34,720 --> 00:12:38,890
redesign the the P host Colonel

00:12:37,480 --> 00:12:42,730
infrastructure that we have right now

00:12:38,890 --> 00:12:44,470
and so that it's a very small change as

00:12:42,730 --> 00:12:47,410
in terms of change lines of code it's

00:12:44,470 --> 00:12:49,600
around less than three hundred lines and

00:12:47,410 --> 00:12:53,589
what it does is now we have a dedicated

00:12:49,600 --> 00:12:56,680
worker that services up to a certain

00:12:53,589 --> 00:12:58,400
number of guests the default we have

00:12:56,680 --> 00:13:00,350
right now is seven

00:12:58,400 --> 00:13:05,950
don't know how he eats that number but

00:13:00,350 --> 00:13:08,750
that's you could you could change it and

00:13:05,950 --> 00:13:10,610
the idea is that if if you if we can

00:13:08,750 --> 00:13:12,770
stabilize this then we can you know add

00:13:10,610 --> 00:13:19,910
some enhancements like pulling on top of

00:13:12,770 --> 00:13:21,320
it if it makes sense that is and so some

00:13:19,910 --> 00:13:22,580
of the challenges there are some

00:13:21,320 --> 00:13:24,730
challenges with this simple to the

00:13:22,580 --> 00:13:27,440
design as well one of them is see groups

00:13:24,730 --> 00:13:31,390
so the way see groups works right now is

00:13:27,440 --> 00:13:36,050
easy because we have a dedicated

00:13:31,390 --> 00:13:39,770
dedicated Colonel worker per device in

00:13:36,050 --> 00:13:41,840
the user space so we can just attach to

00:13:39,770 --> 00:13:45,050
all the save groups that the quman

00:13:41,840 --> 00:13:47,210
processor has but once we are kind of

00:13:45,050 --> 00:13:48,890
sharing a queue between multiple guests

00:13:47,210 --> 00:13:51,470
now that that becomes complicated either

00:13:48,890 --> 00:13:54,440
we come up with a mechanism where maybe

00:13:51,470 --> 00:13:57,230
we kind of keeps switching cgroups the

00:13:54,440 --> 00:14:00,920
moment we have a you know something to

00:13:57,230 --> 00:14:03,970
service or the easy way and what we have

00:14:00,920 --> 00:14:06,380
done right now as a starting point is

00:14:03,970 --> 00:14:08,600
whenever there's a new device that's

00:14:06,380 --> 00:14:11,690
been created or a new pair will work you

00:14:08,600 --> 00:14:13,580
pair that is being created we we just go

00:14:11,690 --> 00:14:15,500
through all the secrets that it has and

00:14:13,580 --> 00:14:17,660
we look through all the vioce worker

00:14:15,500 --> 00:14:19,550
threads that we have created yet and see

00:14:17,660 --> 00:14:21,380
if you know they match if there is even

00:14:19,550 --> 00:14:24,080
a single mismatch then we just create

00:14:21,380 --> 00:14:28,610
one more sounds pretty dumb but that's

00:14:24,080 --> 00:14:30,980
what we have right now and and it works

00:14:28,610 --> 00:14:34,250
and i think the assumption that i made

00:14:30,980 --> 00:14:36,440
when we did this is virtual machines are

00:14:34,250 --> 00:14:38,420
virtually guests guests are not really

00:14:36,440 --> 00:14:40,700
regular processes that you know get

00:14:38,420 --> 00:14:42,500
killed or and get created more often

00:14:40,700 --> 00:14:45,950
this is going to just happen once when

00:14:42,500 --> 00:14:52,280
the guest is created and this of course

00:14:45,950 --> 00:14:53,870
is a minor kind of you know fix-ups here

00:14:52,280 --> 00:14:56,630
and there was for example if there's a

00:14:53,870 --> 00:15:01,160
if they see groups that the process that

00:14:56,630 --> 00:15:02,840
a user process has is just the the root

00:15:01,160 --> 00:15:04,610
hierarchy in the three groups then we

00:15:02,840 --> 00:15:09,080
just ignore them a check and we just

00:15:04,610 --> 00:15:11,980
know continue using it and one of the

00:15:09,080 --> 00:15:18,920
other problems is

00:15:11,980 --> 00:15:20,870
with polling and so what happens if we

00:15:18,920 --> 00:15:23,839
at at some point in future introduced

00:15:20,870 --> 00:15:25,850
polling and how does it interact with

00:15:23,839 --> 00:15:28,370
see groups do you want to do you want to

00:15:25,850 --> 00:15:31,459
do that yeah so when you do polling

00:15:28,370 --> 00:15:33,949
right the thread that does the polling

00:15:31,459 --> 00:15:37,880
has to decide which vm z wants to

00:15:33,949 --> 00:15:40,990
service so it makes sense to integrate

00:15:37,880 --> 00:15:44,149
see groups into the internet thread

00:15:40,990 --> 00:15:47,360
which which introduces some complexity

00:15:44,149 --> 00:15:48,889
right because if you have like so for

00:15:47,360 --> 00:15:51,800
example if you want to do polling you

00:15:48,889 --> 00:15:54,199
want one thread per core so you can do

00:15:51,800 --> 00:15:58,269
you can always do the pulling and and

00:15:54,199 --> 00:16:01,009
you will you will know you get notified

00:15:58,269 --> 00:16:03,380
when the vm has a packet to send and

00:16:01,009 --> 00:16:05,329
stuff like that and when you when you

00:16:03,380 --> 00:16:10,480
integrate see groups into it and you say

00:16:05,329 --> 00:16:16,009
that we need to create another thread

00:16:10,480 --> 00:16:18,500
for every single mismatch you very very

00:16:16,009 --> 00:16:22,790
fast go and revert to the old state of

00:16:18,500 --> 00:16:27,439
having a thread a dedicated worker

00:16:22,790 --> 00:16:30,680
thread per a vm right because for each

00:16:27,439 --> 00:16:32,810
my shift you've got to create a thread

00:16:30,680 --> 00:16:35,720
and that doesn't work well with polling

00:16:32,810 --> 00:16:40,100
that assumes that your thread is always

00:16:35,720 --> 00:16:42,319
scheduled and always running so we

00:16:40,100 --> 00:16:44,569
probably need to think about deeper

00:16:42,319 --> 00:16:47,269
integration of see groups and we would

00:16:44,569 --> 00:16:51,100
love to hear if you got some you guys

00:16:47,269 --> 00:16:57,139
got some offers and yes ideas of how to

00:16:51,100 --> 00:17:01,399
integrate it ok yet another topic is

00:16:57,139 --> 00:17:04,850
work use so the point is you know is it

00:17:01,399 --> 00:17:07,819
possible that we can somehow leverage on

00:17:04,850 --> 00:17:13,159
the existing work queue mechanism that

00:17:07,819 --> 00:17:17,449
the colonel has so I don't know if this

00:17:13,159 --> 00:17:19,429
may be some background for that is that

00:17:17,449 --> 00:17:21,260
one of the reasons why work use or

00:17:19,429 --> 00:17:24,910
content see manage work is was

00:17:21,260 --> 00:17:27,620
introduced was a lot of colonel

00:17:24,910 --> 00:17:31,790
code or drivers ended up using their own

00:17:27,620 --> 00:17:34,160
work you you know mechanisms and it it

00:17:31,790 --> 00:17:36,860
was thought of that why not have kind of

00:17:34,160 --> 00:17:40,370
a single kind of back-end that everyone

00:17:36,860 --> 00:17:43,790
can use so it makes sense for us to

00:17:40,370 --> 00:17:46,910
think the same as well and one of the

00:17:43,790 --> 00:17:49,880
RFC is actually introduced was using

00:17:46,910 --> 00:17:51,620
work use instead of having to have our

00:17:49,880 --> 00:17:55,970
own kernel threads that was doing the

00:17:51,620 --> 00:17:58,220
vos work but as of now the biggest

00:17:55,970 --> 00:18:03,790
problem with work uses we don't know how

00:17:58,220 --> 00:18:07,760
to do see groups with that and even if

00:18:03,790 --> 00:18:09,230
we do have see groups at some point the

00:18:07,760 --> 00:18:13,490
other problem that's going to be with

00:18:09,230 --> 00:18:16,850
work groups is we don't have much

00:18:13,490 --> 00:18:24,260
control once the work enters the colonel

00:18:16,850 --> 00:18:25,730
back end and so that's going to be the

00:18:24,260 --> 00:18:27,679
disadvantage but the advantage of course

00:18:25,730 --> 00:18:30,140
is that we are going to end up with less

00:18:27,679 --> 00:18:32,510
lesser code and we don't have to worry

00:18:30,140 --> 00:18:34,460
about a lot of things as in work work

00:18:32,510 --> 00:18:37,250
use would be no more aware we don't have

00:18:34,460 --> 00:18:39,050
to worry about that but this is also

00:18:37,250 --> 00:18:41,390
something which is kind of an open

00:18:39,050 --> 00:18:43,970
question we don't know what to do yet

00:18:41,390 --> 00:18:45,710
but as of now we don't use work use we

00:18:43,970 --> 00:18:47,900
just use our own kernel thread and

00:18:45,710 --> 00:18:50,750
another thing about to work use is that

00:18:47,900 --> 00:18:54,620
the work using in wheels tend to be very

00:18:50,750 --> 00:18:57,110
small they're they're always they're

00:18:54,620 --> 00:18:59,470
only kept by the but the number of vm

00:18:57,110 --> 00:19:02,990
serviced by the by the thread because a

00:18:59,470 --> 00:19:05,720
vm only notifies you on the on the first

00:19:02,990 --> 00:19:07,460
packet or the first work and then if it

00:19:05,720 --> 00:19:10,940
has additional work doesn't notify you

00:19:07,460 --> 00:19:13,550
again so the work it doesn't add another

00:19:10,940 --> 00:19:17,480
work to the queue so the queues are

00:19:13,550 --> 00:19:21,610
fairly small so we don't we don't need

00:19:17,480 --> 00:19:21,610
the big mechanism for it

00:19:22,029 --> 00:19:27,639
even go to verizon okay so we wanted to

00:19:25,299 --> 00:19:31,719
show some results and of course Elvis

00:19:27,639 --> 00:19:34,330
was already presented into 13 2013 and

00:19:31,719 --> 00:19:37,179
we didn't want to show again the same

00:19:34,330 --> 00:19:39,789
with the same thing so i'm going to show

00:19:37,179 --> 00:19:43,989
all the results with with the new twist

00:19:39,789 --> 00:19:46,450
right way so we can see here a net

00:19:43,989 --> 00:19:48,779
participe stream which which is a the

00:19:46,450 --> 00:19:51,820
performance measurement we have like the

00:19:48,779 --> 00:19:55,839
network throughput and UDP around for

00:19:51,820 --> 00:19:57,580
our latency ok and and I wanted to add

00:19:55,839 --> 00:20:00,460
something more so either the breakdown

00:19:57,580 --> 00:20:04,479
right so right now we have the blue line

00:20:00,460 --> 00:20:07,539
which is Elvis which is a on par with

00:20:04,479 --> 00:20:09,879
what we presented in the RFC's right

00:20:07,539 --> 00:20:11,499
it's a shared views without polling

00:20:09,879 --> 00:20:14,799
without the posterior up throughout

00:20:11,499 --> 00:20:20,799
nothing just the shirt voiced ok and

00:20:14,799 --> 00:20:24,339
then the orange line is Elvis with the

00:20:20,799 --> 00:20:27,190
shirt wheels and polling ok it falls on

00:20:24,339 --> 00:20:29,259
the guest and the yellow line is with

00:20:27,190 --> 00:20:33,580
polling and post interrupt so the yellow

00:20:29,259 --> 00:20:38,859
line cuz causes no exits and as you can

00:20:33,580 --> 00:20:41,609
see it works the best and I did like so

00:20:38,859 --> 00:20:44,889
I took the former graph and then I I

00:20:41,609 --> 00:20:47,469
wanted to see how much benefit can we

00:20:44,889 --> 00:20:50,379
get from a user using polling and using

00:20:47,469 --> 00:20:53,229
a post interrupt and as you can see for

00:20:50,379 --> 00:20:55,210
for lower number of VMs we can get major

00:20:53,229 --> 00:20:58,269
improvement right you get like thirty

00:20:55,210 --> 00:21:01,330
five percent or something and and when

00:20:58,269 --> 00:21:05,019
the when the I Corps and when the io

00:21:01,330 --> 00:21:08,529
thread becomes becomes more and more

00:21:05,019 --> 00:21:11,229
saturated the shared via say perform was

00:21:08,529 --> 00:21:14,259
better and that could be explained by

00:21:11,229 --> 00:21:16,570
the by the simple fact that if you don't

00:21:14,259 --> 00:21:20,019
if you handle a queue if you under cue

00:21:16,570 --> 00:21:22,570
that produces work yeah and you don't

00:21:20,019 --> 00:21:25,749
and you don't finish handling all the

00:21:22,570 --> 00:21:27,609
other work in the queue you just you

00:21:25,749 --> 00:21:30,669
don't you don't wait to for another

00:21:27,609 --> 00:21:34,210
notification you just take the thread

00:21:30,669 --> 00:21:35,950
and you r eq it and post another work in

00:21:34,210 --> 00:21:40,230
the share thread

00:21:35,950 --> 00:21:45,940
you get something similar to a polling

00:21:40,230 --> 00:21:49,750
so that's why 45 VMs they're a bit on

00:21:45,940 --> 00:21:51,760
par and then four six and seven when

00:21:49,750 --> 00:21:55,960
that when the io thread becomes really

00:21:51,760 --> 00:22:00,010
saturated then we can see that using no

00:21:55,960 --> 00:22:03,460
exits does improve and and 47 and and

00:22:00,010 --> 00:22:06,460
the yellow line from six to seven we

00:22:03,460 --> 00:22:09,519
just got the line rate it just got line

00:22:06,460 --> 00:22:13,210
right so it cannot scale anymore and for

00:22:09,519 --> 00:22:15,490
latency because our VMs are always

00:22:13,210 --> 00:22:20,820
scheduled and the IO core is always

00:22:15,490 --> 00:22:20,820
bowling we get a major performance boost

00:22:23,820 --> 00:22:32,380
and so and finally we also have two

00:22:29,169 --> 00:22:35,519
graphs with the RFC's that we posted

00:22:32,380 --> 00:22:39,370
this was to show the difference between

00:22:35,519 --> 00:22:42,789
you know research and real code they

00:22:39,370 --> 00:22:47,049
always have a difference right but just

00:22:42,789 --> 00:22:49,809
the subset of the host sharing code that

00:22:47,049 --> 00:22:52,570
we pull we posted upstream as RFC and

00:22:49,809 --> 00:22:57,580
just using that let's do some TCP stream

00:22:52,570 --> 00:23:05,190
and DCP request response and the results

00:22:57,580 --> 00:23:07,179
for that or so it seems kind of a I

00:23:05,190 --> 00:23:11,679
would like to point out that the

00:23:07,179 --> 00:23:14,169
difference is not that much here but but

00:23:11,679 --> 00:23:17,679
we don't yet understand why that's

00:23:14,169 --> 00:23:19,870
happening so you could see the the the

00:23:17,679 --> 00:23:23,289
baseline is the the purple line and the

00:23:19,870 --> 00:23:27,070
green is the shared v host performance

00:23:23,289 --> 00:23:29,620
and as we increase the number of guests

00:23:27,070 --> 00:23:31,179
of course that's what we want the

00:23:29,620 --> 00:23:36,519
performance kind of increases but

00:23:31,179 --> 00:23:39,159
doesn't seem kind of a similar to what a

00:23:36,519 --> 00:23:42,250
I'll just showed in the graph no but but

00:23:39,159 --> 00:23:45,200
but you can see that for like 25 VMs you

00:23:42,250 --> 00:23:47,750
get like yeah exactly 35

00:23:45,200 --> 00:23:53,750
every five percent improvement right

00:23:47,750 --> 00:23:55,909
very nice but the the the the lead it

00:23:53,750 --> 00:23:58,820
doesn't seem like that it's actually

00:23:55,909 --> 00:24:02,870
scaling as you know as linear as we

00:23:58,820 --> 00:24:06,019
wanted to write yeah that's true so and

00:24:02,870 --> 00:24:10,760
then we also have a request-response but

00:24:06,019 --> 00:24:14,210
that also if you see over here is the

00:24:10,760 --> 00:24:16,970
same thing again the the performance for

00:24:14,210 --> 00:24:20,350
smaller number of guests somehow shows

00:24:16,970 --> 00:24:22,970
that baseline is actually better and

00:24:20,350 --> 00:24:26,659
even when we increase the number of

00:24:22,970 --> 00:24:29,269
guests the performance of share d host

00:24:26,659 --> 00:24:31,370
is better but it's not as much as we

00:24:29,269 --> 00:24:34,549
would like it to be do you remember your

00:24:31,370 --> 00:24:36,799
graph about the year sure any I've my

00:24:34,549 --> 00:24:41,409
own graphs which were done in and I

00:24:36,799 --> 00:24:45,830
Colonel a 3.9 checking I can show no

00:24:41,409 --> 00:24:50,029
don't have it we don't have it so um so

00:24:45,830 --> 00:24:51,919
that's all we had I guess and so the

00:24:50,029 --> 00:24:54,289
last slide is just you know summarizing

00:24:51,919 --> 00:24:59,059
that there's a lot of interesting scope

00:24:54,289 --> 00:25:01,159
of for work interesting work and you

00:24:59,059 --> 00:25:03,350
know we're going to hopefully slowly

00:25:01,159 --> 00:25:05,210
make progress towards some solution

00:25:03,350 --> 00:25:13,330
that's going to be acceptable upstream

00:25:05,210 --> 00:25:15,860
and we'll see where it goes like I can

00:25:13,330 --> 00:25:19,630
try and show where the graphs that's

00:25:15,860 --> 00:25:19,630
fine we don't have time any questions

00:25:22,570 --> 00:25:29,980
yeah no but we can hear you so go back

00:25:26,769 --> 00:25:32,200
to the last chart you have is that when

00:25:29,980 --> 00:25:35,799
you're using the shared videos is that

00:25:32,200 --> 00:25:37,960
with polling and no the shared videos

00:25:35,799 --> 00:25:40,269
that we posted upstream is not using

00:25:37,960 --> 00:25:41,529
polling at all so this is just just

00:25:40,269 --> 00:25:44,320
sharing the video straight between

00:25:41,529 --> 00:25:46,570
multiple guests yeah and I think that

00:25:44,320 --> 00:25:50,860
the main problem here is that we don't

00:25:46,570 --> 00:25:52,539
have enough I of threads oh yeah yeah so

00:25:50,860 --> 00:25:54,309
a I'll give me an idea that if we

00:25:52,539 --> 00:25:57,100
increase the number of dedicated i/o

00:25:54,309 --> 00:25:59,830
threats for request response and see how

00:25:57,100 --> 00:26:01,149
that you know scales with these results

00:25:59,830 --> 00:26:03,669
that would be a good thing to try but we

00:26:01,149 --> 00:26:07,299
haven't tried that yeah because yeah

00:26:03,669 --> 00:26:09,870
those results are fairly new we need to

00:26:07,299 --> 00:26:11,919
meet probably to increase the number of

00:26:09,870 --> 00:26:15,179
threads and then you'll see a major

00:26:11,919 --> 00:26:17,830
because because the the original vos

00:26:15,179 --> 00:26:20,799
scheme doesn't scale well in terms of

00:26:17,830 --> 00:26:24,250
latency that's something that was shown

00:26:20,799 --> 00:26:27,070
yeah so your baseline numbers for for

00:26:24,250 --> 00:26:29,169
one thread per device is that all those

00:26:27,070 --> 00:26:33,789
thread for or workers are running on it

00:26:29,169 --> 00:26:37,779
given Corp in oh yeah yes let me go back

00:26:33,789 --> 00:26:41,799
to the can you go back to the this one

00:26:37,779 --> 00:26:44,529
yeah so actually up to 10 guests I don't

00:26:41,799 --> 00:26:46,240
do anything I just let it run as your

00:26:44,529 --> 00:26:50,590
system comes both for the shared case

00:26:46,240 --> 00:26:54,730
and the and the single que vos worker

00:26:50,590 --> 00:26:59,049
per device case but starting from X is

00:26:54,730 --> 00:27:01,120
equal to 14 here I start pinning in the

00:26:59,049 --> 00:27:04,179
isolated vhosts thread case I actually

00:27:01,120 --> 00:27:06,370
pin the course pin the Viejo's threads

00:27:04,179 --> 00:27:08,559
once the threads are created of course

00:27:06,370 --> 00:27:12,460
to the cold weather where the guest is

00:27:08,559 --> 00:27:13,659
running and 4shared case I I know the vo

00:27:12,460 --> 00:27:15,399
states are already created when the

00:27:13,659 --> 00:27:17,679
first guess was created so I just pin it

00:27:15,399 --> 00:27:19,269
to specific city of course because when

00:27:17,679 --> 00:27:21,700
we have any serious work all these small

00:27:19,269 --> 00:27:23,529
packets with the original design we have

00:27:21,700 --> 00:27:25,299
the freedom to basically scale by

00:27:23,529 --> 00:27:27,369
basically separating them

00:27:25,299 --> 00:27:31,779
yeah right now Adele you're taking that

00:27:27,369 --> 00:27:34,720
away not really if you look here right

00:27:31,779 --> 00:27:36,309
there's a there's the baseline which is

00:27:34,720 --> 00:27:39,330
the original retail and there is

00:27:36,309 --> 00:27:41,950
baseline with affinity where we just

00:27:39,330 --> 00:27:43,720
instead of a dedicated mic or we take

00:27:41,950 --> 00:27:47,379
all the threads and dedicate them in one

00:27:43,720 --> 00:27:49,989
core okay and and as it and as you can

00:27:47,379 --> 00:27:52,840
see both of them did the both both

00:27:49,989 --> 00:27:55,929
solution don't scale very well you're

00:27:52,840 --> 00:27:59,619
saying you have six cores for we have we

00:27:55,929 --> 00:28:02,200
have eight cores for up to seven VMs so

00:27:59,619 --> 00:28:06,970
we have eight cores and then we have a

00:28:02,200 --> 00:28:09,369
vm running on each core and well in the

00:28:06,970 --> 00:28:11,379
case of baseline with affinity we just

00:28:09,369 --> 00:28:14,350
take all the eye of threats and pin them

00:28:11,379 --> 00:28:18,249
on on the on the eighth score and that's

00:28:14,350 --> 00:28:20,080
how we run system so it is pinned and as

00:28:18,249 --> 00:28:22,480
you can see it doesn't scale very well

00:28:20,080 --> 00:28:25,090
so that actually brings another topic we

00:28:22,480 --> 00:28:26,529
really wanted to have a way of kind of

00:28:25,090 --> 00:28:28,629
jumping back to the previous design if

00:28:26,529 --> 00:28:31,840
possible at all so and you can do that

00:28:28,629 --> 00:28:33,789
if you if you set the number of guests

00:28:31,840 --> 00:28:35,559
for worker to one it's going to

00:28:33,789 --> 00:28:36,940
basically be the you know all design

00:28:35,559 --> 00:28:39,159
because there's an implication as to how

00:28:36,940 --> 00:28:41,259
this end up working stack more than

00:28:39,159 --> 00:28:43,629
likely a tap to ice and hold at nine

00:28:41,259 --> 00:28:46,690
yard after day I agree with you I think

00:28:43,629 --> 00:28:49,600
it does not a case at this point for the

00:28:46,690 --> 00:28:56,159
RFC version does not look like something

00:28:49,600 --> 00:29:00,789
that is going to go for all cases I

00:28:56,159 --> 00:29:03,249
guess so um the girl currently there is

00:29:00,789 --> 00:29:04,580
a big limitation because in that what we

00:29:03,249 --> 00:29:07,650
observe is that

00:29:04,580 --> 00:29:09,470
for each of the where they will accuse

00:29:07,650 --> 00:29:11,040
you have a separate videos friend and

00:29:09,470 --> 00:29:14,100
particularly when you are dealing with

00:29:11,040 --> 00:29:16,530
the network can be vm right which is

00:29:14,100 --> 00:29:18,360
caused a lot of packets and the big O's

00:29:16,530 --> 00:29:21,390
itself wheels friend Colonel threat

00:29:18,360 --> 00:29:22,920
becomes the bottleneck because we are

00:29:21,390 --> 00:29:24,960
passing module of traffic to a

00:29:22,920 --> 00:29:27,630
particular p.m. and the biggest users

00:29:24,960 --> 00:29:30,150
hundred-person see each thread and in a

00:29:27,630 --> 00:29:32,250
typical network device yoga or network

00:29:30,150 --> 00:29:34,920
appliance they would have to be mix at

00:29:32,250 --> 00:29:36,420
least so you have two videos threads so

00:29:34,920 --> 00:29:38,970
as I said you're pinning the Rio stress

00:29:36,420 --> 00:29:41,700
so you essentially need minimum of three

00:29:38,970 --> 00:29:45,390
DCPS just to run one year right because

00:29:41,700 --> 00:29:47,580
you need one vcp for the DM one for each

00:29:45,390 --> 00:29:49,290
of the Vigo's threads so this totally

00:29:47,580 --> 00:29:52,050
limits the number of teams that can

00:29:49,290 --> 00:29:54,480
create in our particular server so it

00:29:52,050 --> 00:29:57,600
will be I mean it will be useful it if

00:29:54,480 --> 00:30:00,990
he was thread can be optimized right

00:29:57,600 --> 00:30:02,760
till I mean maybe polling is one way but

00:30:00,990 --> 00:30:04,170
seriously there s be some from

00:30:02,760 --> 00:30:06,300
processing that because there's a big

00:30:04,170 --> 00:30:10,260
limitation right now with kvm fine which

00:30:06,300 --> 00:30:12,060
we don't see india ritual I oh for what

00:30:10,260 --> 00:30:14,850
I I've seen up we've been working it for

00:30:12,060 --> 00:30:18,810
a few years and I've seen it does take a

00:30:14,850 --> 00:30:21,780
lot of CPU when the yeah especially for

00:30:18,810 --> 00:30:23,610
10 gigabit or 40 gigabits its start

00:30:21,780 --> 00:30:27,240
takes and unfortunately we don't have

00:30:23,610 --> 00:30:29,610
any cpu results with our with our slides

00:30:27,240 --> 00:30:31,650
so yeah that's a good thing to look at

00:30:29,610 --> 00:30:33,570
if you can i've been multiple vertical

00:30:31,650 --> 00:30:35,130
threads listen to cpu because already

00:30:33,570 --> 00:30:36,960
the whatever threading service the body

00:30:35,130 --> 00:30:37,880
it shouldn't want to block this then you

00:30:36,960 --> 00:30:42,180
are going to reduce the performance

00:30:37,880 --> 00:30:43,680
right I income we can you have to pin

00:30:42,180 --> 00:30:49,370
otherwise the performance again goes

00:30:43,680 --> 00:30:49,370
down yeah yeah but

00:30:54,870 --> 00:30:58,520

YouTube URL: https://www.youtube.com/watch?v=odSWXzoMfas


