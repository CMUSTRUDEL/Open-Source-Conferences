Title: [2015] Towards multi-threaded TCG by Alex Bennée and Frederic Konrad
Publication date: 2015-09-05
Playlist: KVM Forum 2015
Description: 
	While QEMU has continued to be optimised for KVM to make use of the growing number of cores on modern systems TCG emulation has been stuck running in a single thread. This year there is another push to get a workable solution merged upstream. We shall present a review of the challenges that need to be addressed: locking, TLB and cache maintenance and generic solution for the various atomic/exclusive operations. We will discuss previous work that has been done in this field before presenting a design that addresses these requirements. Finally we shall look at the current proposed patches and the design decisions they have taken.

Alex Bennée
Senior Software Engineer, Linaro
Alex is a senior software engineer working in Linaro's Virtualization team. | An experienced FLOSS developer with over 20 years of experience in embedded | and systems programming he currently spends most of his time on QEMU's TCG | based emulation. The first piece of assembly he wrote was for the 6809 in his | Dragon 32 followed by excessive pixel flinging on the 68000 before x86 took | over the world.

Frederic Konrad
Fred is a software engineer which is mostly interested in hardware 
simulation projects like QEMU and SystemC. He is one of the contributor to MTTCG as it's a really interesting way to speed up the simulation.

Slides (Alex): https://drive.google.com/file/d/0BzyAwvVlQckeUlBjVlpFdDE4Snc/view?usp=sharing

Slides (Fred): https://drive.google.com/file/d/0BzyAwvVlQckeek1pbGtzeFdPOWc/view?usp=sharing
Captions: 
	00:00:14,389 --> 00:00:23,699
right hello a little introduction first

00:00:19,470 --> 00:00:27,359
my name is Alex been a senior engineer

00:00:23,699 --> 00:00:29,340
in linares virtualization team you can

00:00:27,359 --> 00:00:32,850
find me on the Crimea channelers st

00:00:29,340 --> 00:00:36,059
squad and on the lanai channels as a jb

00:00:32,850 --> 00:00:37,590
llamara I mostly work on arm emulations

00:00:36,059 --> 00:00:41,460
but I've been known to dabble a little

00:00:37,590 --> 00:00:48,059
in kvm as well and for my sins I use

00:00:41,460 --> 00:00:53,820
Emacs oh yeah let's let's start device

00:00:48,059 --> 00:00:57,360
it so let's just start by defining what

00:00:53,820 --> 00:01:01,320
we mean by multi-threaded TCG in case

00:00:57,360 --> 00:01:03,600
anyone's not familiar so TCG simply

00:01:01,320 --> 00:01:05,970
stands for the tiny code generator and

00:01:03,600 --> 00:01:07,740
it's often used as a shorthand for

00:01:05,970 --> 00:01:09,780
whenever you're using chrome user cross

00:01:07,740 --> 00:01:12,210
processor virtualizer so basically

00:01:09,780 --> 00:01:14,580
running code for one instruction set on

00:01:12,210 --> 00:01:16,530
a machine that supports another so

00:01:14,580 --> 00:01:18,270
currently only supports around about 16

00:01:16,530 --> 00:01:24,720
families of processors some of which

00:01:18,270 --> 00:01:26,549
I've put up there so unlike kvm the TCG

00:01:24,720 --> 00:01:30,329
variant of chrome you has only one

00:01:26,549 --> 00:01:33,060
thread for all its emulated V CPUs so

00:01:30,329 --> 00:01:35,340
this means the execution engine runs

00:01:33,060 --> 00:01:38,520
each vcpu in turn and the result looks a

00:01:35,340 --> 00:01:41,850
little something like this so this is my

00:01:38,520 --> 00:01:43,470
eight core desktop and single core is

00:01:41,850 --> 00:01:45,960
frantically spinning around as it

00:01:43,470 --> 00:01:48,450
emulates every vcpu in turn all the

00:01:45,960 --> 00:01:49,770
other courts are looking fairly bored so

00:01:48,450 --> 00:01:51,680
this is obviously a massive waste of

00:01:49,770 --> 00:01:54,840
resources so when we talk about

00:01:51,680 --> 00:01:56,159
multi-threaded TCG we're looking for a

00:01:54,840 --> 00:02:00,689
process model that looks a little bit

00:01:56,159 --> 00:02:02,729
more like this so it simply refers to

00:02:00,689 --> 00:02:06,840
the apparently simple change of making

00:02:02,729 --> 00:02:09,060
hv cpu running its own thread and in

00:02:06,840 --> 00:02:10,979
live eel world each vcpu thread would be

00:02:09,060 --> 00:02:11,970
distributed to run on an individual core

00:02:10,979 --> 00:02:16,440
so we

00:02:11,970 --> 00:02:18,450
mais the performance of the emulation in

00:02:16,440 --> 00:02:26,360
practice the result might be a little

00:02:18,450 --> 00:02:28,740
bit more chaotic so why do we want it

00:02:26,360 --> 00:02:33,780
well part of the answer is that we live

00:02:28,740 --> 00:02:36,570
in a multi-core world this is the humble

00:02:33,780 --> 00:02:40,050
Raspberry Pi sold 5 million units since

00:02:36,570 --> 00:02:44,880
its launch in 2012 its current iteration

00:02:40,050 --> 00:02:47,190
has a 4 core cortex a7 in it or running

00:02:44,880 --> 00:02:51,120
about just under a gigahertz and it's

00:02:47,190 --> 00:02:53,820
available for $25 of course the pie is a

00:02:51,120 --> 00:02:55,640
bit of a toy machine but the dragon

00:02:53,820 --> 00:02:58,560
board here is a commercially available

00:02:55,640 --> 00:03:00,540
64-bit ARM system it's designed for

00:02:58,560 --> 00:03:02,340
prototyping it's designed for

00:03:00,540 --> 00:03:05,850
prototyping and the board provides a

00:03:02,340 --> 00:03:10,830
quad-core Snapdragon and it's available

00:03:05,850 --> 00:03:14,730
for $75 even the phone in my pocket is a

00:03:10,830 --> 00:03:17,790
multi-core device the Nexus has a

00:03:14,730 --> 00:03:19,560
quad-core krait in it and it's setup

00:03:17,790 --> 00:03:21,090
looks faint we're frankly underpowered

00:03:19,560 --> 00:03:23,400
compared to some of the phones coming on

00:03:21,090 --> 00:03:26,340
the market now which have 64-bit octobe

00:03:23,400 --> 00:03:28,920
cause and it's available for three

00:03:26,340 --> 00:03:30,330
hundred dollars of course it's not just

00:03:28,920 --> 00:03:33,750
about the devices that we're trying to

00:03:30,330 --> 00:03:35,519
emulate this is my desktop it's a fairly

00:03:33,750 --> 00:03:38,820
modest I seven which I bought when i

00:03:35,519 --> 00:03:41,220
joined linaro and it has for real cause

00:03:38,820 --> 00:03:44,010
and for fake ones and it cost me around

00:03:41,220 --> 00:03:46,080
about six hundred dollars in the end I

00:03:44,010 --> 00:03:49,799
or I spent more on the RAM than I did on

00:03:46,080 --> 00:03:51,570
the processor of course if you're

00:03:49,799 --> 00:03:53,420
prepared to spend a few thousand dollars

00:03:51,570 --> 00:03:56,010
you can get something like this monster

00:03:53,420 --> 00:03:57,959
its main use cases building Android

00:03:56,010 --> 00:03:59,880
images but it would be nice if it could

00:03:57,959 --> 00:04:04,049
also test those images at a reasonable

00:03:59,880 --> 00:04:06,660
speed as well while we're on the subject

00:04:04,049 --> 00:04:08,720
of Android as some of you may know the

00:04:06,660 --> 00:04:11,970
Android emulator is based on gremio and

00:04:08,720 --> 00:04:17,220
supporting multi-core in the emulator is

00:04:11,970 --> 00:04:19,140
a key use case for linaro so while the

00:04:17,220 --> 00:04:20,940
last few decades have seen quite a

00:04:19,140 --> 00:04:23,340
phenomenal growth in processing as

00:04:20,940 --> 00:04:25,650
Moore's law itself true we're starting

00:04:23,340 --> 00:04:28,860
to see a tail off in

00:04:25,650 --> 00:04:30,449
single core performance so while

00:04:28,860 --> 00:04:32,040
embarrassingly parallel problems can

00:04:30,449 --> 00:04:34,289
quickly take advantage of these cause

00:04:32,040 --> 00:04:38,419
the problem formulation is a little bit

00:04:34,289 --> 00:04:40,590
more complex other than pure performance

00:04:38,419 --> 00:04:45,810
there are some other reasons we should

00:04:40,590 --> 00:04:48,419
care multi-threaded TCG would behave

00:04:45,810 --> 00:04:50,310
more like real systems so while you have

00:04:48,419 --> 00:04:52,500
all your vcpu sat on a single thread

00:04:50,310 --> 00:04:54,570
there are behaviors that you don't see

00:04:52,500 --> 00:04:57,780
an emulation which you will get compared

00:04:54,570 --> 00:04:59,729
to the real system if you've got flawed

00:04:57,780 --> 00:05:01,949
software it's much preferred that any

00:04:59,729 --> 00:05:03,780
issues you have with concurrency show up

00:05:01,949 --> 00:05:05,669
whilst you're doing you'll bring up on

00:05:03,780 --> 00:05:11,370
cremieux then when you actually have to

00:05:05,669 --> 00:05:13,620
debug it on real hardware also as

00:05:11,370 --> 00:05:15,150
cremieux is a dynamic emulation there

00:05:13,620 --> 00:05:18,720
are things you can do which are a lot

00:05:15,150 --> 00:05:20,190
harder to do in real hardware so you can

00:05:18,720 --> 00:05:22,229
do additional instrumentation to

00:05:20,190 --> 00:05:23,699
investigate behavior there's other fancy

00:05:22,229 --> 00:05:25,320
tricks you can do like record and

00:05:23,699 --> 00:05:28,349
playback for debugging and reverse

00:05:25,320 --> 00:05:29,580
debugging so again you want a system

00:05:28,349 --> 00:05:34,889
that behave is more light in the system

00:05:29,580 --> 00:05:37,820
it emulators another often talked about

00:05:34,889 --> 00:05:40,770
use case for Chrome you is cross tooling

00:05:37,820 --> 00:05:42,060
so there's a developer I've spent quite

00:05:40,770 --> 00:05:44,190
a lot of time building things for the

00:05:42,060 --> 00:05:46,530
non x86 world so I'm quite familiar with

00:05:44,190 --> 00:05:48,840
building cross compilers the process has

00:05:46,530 --> 00:05:50,039
got a simpler over the last few years

00:05:48,840 --> 00:05:54,840
but there's still a lot of inherent

00:05:50,039 --> 00:05:56,550
complexity sometimes this can be

00:05:54,840 --> 00:05:58,919
addressed by using Linux user which

00:05:56,550 --> 00:06:01,349
Alexander mentioned but it's not without

00:05:58,919 --> 00:06:03,570
its warts so after you get bin format

00:06:01,349 --> 00:06:05,699
set up and you make sure your to root in

00:06:03,570 --> 00:06:08,250
your multi libs are all right you can

00:06:05,699 --> 00:06:11,270
still get stymied if there's threads or

00:06:08,250 --> 00:06:13,680
signals that occur during your build

00:06:11,270 --> 00:06:15,900
what I really want is I just want to

00:06:13,680 --> 00:06:18,630
boot an armed development board on my

00:06:15,900 --> 00:06:23,760
desktop and have the full distro and all

00:06:18,630 --> 00:06:25,320
the usual tools with it so now we've

00:06:23,760 --> 00:06:27,180
talked about why we want it let's talk

00:06:25,320 --> 00:06:30,150
about some of the things that we need to

00:06:27,180 --> 00:06:33,330
address so the two main ones are global

00:06:30,150 --> 00:06:37,050
stating cremieux and the way that we we

00:06:33,330 --> 00:06:39,190
model the guests memory so when I'm

00:06:37,050 --> 00:06:41,740
talking about global states

00:06:39,190 --> 00:06:45,220
includes numerous Global's that we have

00:06:41,740 --> 00:06:47,080
during TCG cogeneration there's also a

00:06:45,220 --> 00:06:50,070
number of runtime structures that are

00:06:47,080 --> 00:06:52,360
used whilst we're running the TCG code

00:06:50,070 --> 00:06:57,730
and finally there are structures that

00:06:52,360 --> 00:06:59,350
are associated with device emulation the

00:06:57,730 --> 00:07:02,470
other thing that needs attention is

00:06:59,350 --> 00:07:05,200
guest memory behavior well you've got

00:07:02,470 --> 00:07:07,360
vcpu scheduled sequentially you get a

00:07:05,200 --> 00:07:10,030
lot of this behavior for free as soon as

00:07:07,360 --> 00:07:12,040
you've got threads free to run across

00:07:10,030 --> 00:07:14,410
multiple CPUs you actually have to take

00:07:12,040 --> 00:07:18,580
some specific care that your meeting or

00:07:14,410 --> 00:07:21,430
guest memory semantics so memory

00:07:18,580 --> 00:07:24,430
consistency becomes also becomes a

00:07:21,430 --> 00:07:27,490
challenge because load store order is

00:07:24,430 --> 00:07:30,160
often only consistent for the CPU that

00:07:27,490 --> 00:07:32,260
you're running on and currently premier

00:07:30,160 --> 00:07:36,760
ignores any explicit guest memory

00:07:32,260 --> 00:07:40,750
barriers so let's look at some of the

00:07:36,760 --> 00:07:45,669
ways we can tackle this problem there

00:07:40,750 --> 00:07:47,440
are three broad approaches first the

00:07:45,669 --> 00:07:49,300
classic approach is you split into

00:07:47,440 --> 00:07:52,180
threads and you stick new texts around

00:07:49,300 --> 00:07:53,410
anything that's shared however you have

00:07:52,180 --> 00:07:54,700
to take care when you're doing this

00:07:53,410 --> 00:07:57,220
because it's easy to get into situations

00:07:54,700 --> 00:07:58,810
where you get dead locks or you have

00:07:57,220 --> 00:08:00,340
lock contention that's so high that the

00:07:58,810 --> 00:08:05,710
system spends all its time thrashing

00:08:00,340 --> 00:08:07,330
around another approach is to accept the

00:08:05,710 --> 00:08:08,650
limitations of community TCG

00:08:07,330 --> 00:08:11,169
implementation and contain them in

00:08:08,650 --> 00:08:13,720
multiple processes so each worries about

00:08:11,169 --> 00:08:15,669
emulating a single core so this diagram

00:08:13,720 --> 00:08:17,620
comes from a project called core mu

00:08:15,669 --> 00:08:20,669
which is a research project that uses

00:08:17,620 --> 00:08:24,760
crème use code base with an arbitration

00:08:20,669 --> 00:08:26,530
IPC layer and shared memory there's

00:08:24,760 --> 00:08:28,030
variants of this design that you used by

00:08:26,530 --> 00:08:30,070
some people that are doing process

00:08:28,030 --> 00:08:34,050
regulation that talks to things like

00:08:30,070 --> 00:08:34,050
FPGA simulations and that sort of thing

00:08:34,620 --> 00:08:38,469
finally you could accept the crème use

00:08:37,089 --> 00:08:41,349
original designs assumptions are

00:08:38,469 --> 00:08:44,160
incompatible with our SMP and rewrite

00:08:41,349 --> 00:08:47,680
everything and a shiny brand-new

00:08:44,160 --> 00:08:49,240
codebase because you know of course at

00:08:47,680 --> 00:08:50,620
this time you'll get all the pave your

00:08:49,240 --> 00:08:51,100
right and you'll make us what you've

00:08:50,620 --> 00:08:56,110
covered

00:08:51,100 --> 00:08:57,910
the special cases so in summary well the

00:08:56,110 --> 00:08:59,170
classic threads approach should give you

00:08:57,910 --> 00:09:02,339
higher performance you have to be

00:08:59,170 --> 00:09:04,990
careful with your locking strategy

00:09:02,339 --> 00:09:07,720
having separate processes makes it a bit

00:09:04,990 --> 00:09:09,970
easier to get correct behavior but it

00:09:07,720 --> 00:09:12,670
does require a potentially invasive

00:09:09,970 --> 00:09:16,029
remodeling of the code and performance

00:09:12,670 --> 00:09:17,560
can be an issue and while it's often

00:09:16,029 --> 00:09:19,959
tempting to rewrite things remember the

00:09:17,560 --> 00:09:22,509
premium code base is 1.6 million lines

00:09:19,959 --> 00:09:25,420
of code slot count estimates it's around

00:09:22,509 --> 00:09:27,009
260 person years of effort to get to

00:09:25,420 --> 00:09:31,860
where we've got to today so you'd be

00:09:27,009 --> 00:09:34,690
throwing that away so what have we done

00:09:31,860 --> 00:09:37,690
the current work uses the existing code

00:09:34,690 --> 00:09:39,069
base obviously and uses threads but it

00:09:37,690 --> 00:09:40,660
tries to minimize the number of locks

00:09:39,069 --> 00:09:43,930
involved so we don't get exploding

00:09:40,660 --> 00:09:46,209
complexes complexity the run loop is

00:09:43,930 --> 00:09:48,839
mainly mainly serialized but the actual

00:09:46,209 --> 00:09:51,699
translated code is fully multi-threaded

00:09:48,839 --> 00:09:54,220
at the same time we've introduced new

00:09:51,699 --> 00:09:55,569
memory semantics to help with properly

00:09:54,220 --> 00:10:00,370
emulating some of the guests memory

00:09:55,569 --> 00:10:04,240
behavior so let's look at the global

00:10:00,370 --> 00:10:06,610
state so the first major challenge is

00:10:04,240 --> 00:10:08,259
the code generator itself there are a

00:10:06,610 --> 00:10:09,910
large number of per file static

00:10:08,259 --> 00:10:12,850
variables which are used in the code

00:10:09,910 --> 00:10:14,470
generation path if two threads attempt

00:10:12,850 --> 00:10:20,649
to do code generation at the same time

00:10:14,470 --> 00:10:22,720
things can get very confused outside the

00:10:20,649 --> 00:10:24,250
code generator there are structures that

00:10:22,720 --> 00:10:26,980
the wide system uses while the code is

00:10:24,250 --> 00:10:28,959
running so this includes the soft mmu

00:10:26,980 --> 00:10:31,480
TLB which needs to be updated when page

00:10:28,959 --> 00:10:32,889
mappings change there's a jump cash

00:10:31,480 --> 00:10:35,170
that's used for looking up new

00:10:32,889 --> 00:10:38,829
translation buffers and a couple of

00:10:35,170 --> 00:10:41,350
condition and flag variables the

00:10:38,829 --> 00:10:44,199
condition and flag variables are fairly

00:10:41,350 --> 00:10:46,029
simple to make per vcpu so this includes

00:10:44,199 --> 00:10:48,850
a whole condition variable which is used

00:10:46,029 --> 00:10:51,970
to block the vcpu on pending work and an

00:10:48,850 --> 00:10:56,319
exit request which raises a flag to

00:10:51,970 --> 00:10:58,630
bring us out of the translated code so

00:10:56,319 --> 00:11:00,850
now I've mentioned basic blocks let's

00:10:58,630 --> 00:11:02,560
quickly remind ourselves of how TCG

00:11:00,850 --> 00:11:05,570
works

00:11:02,560 --> 00:11:07,760
the process is fairly simple on demand

00:11:05,570 --> 00:11:10,190
we take a block of machine code from the

00:11:07,760 --> 00:11:12,020
target it's converted into an

00:11:10,190 --> 00:11:15,380
intermediate form and then the final

00:11:12,020 --> 00:11:17,480
jittered code is generated so here's a

00:11:15,380 --> 00:11:19,790
simple input machine code it's a very

00:11:17,480 --> 00:11:21,890
simple subroutine which increments a

00:11:19,790 --> 00:11:24,740
value and the branch returns signals the

00:11:21,890 --> 00:11:27,140
end of the basic block in this case it's

00:11:24,740 --> 00:11:30,500
an unconditional return but any control

00:11:27,140 --> 00:11:32,120
flow change basically ends the current

00:11:30,500 --> 00:11:36,080
input block that includes conditional

00:11:32,120 --> 00:11:37,850
branches and computed jumps these are

00:11:36,080 --> 00:11:41,840
broken down into an intermediate

00:11:37,850 --> 00:11:43,010
representation known as TCG ops so

00:11:41,840 --> 00:11:45,770
they're similar to the sort of thing

00:11:43,010 --> 00:11:47,420
that a compiler would generate and then

00:11:45,770 --> 00:11:49,070
it goes through a basic optimization

00:11:47,420 --> 00:11:51,440
phase where we remove things like debt

00:11:49,070 --> 00:11:55,490
assignments and things own constants are

00:11:51,440 --> 00:11:57,620
propagated that sort of thing finally we

00:11:55,490 --> 00:11:59,960
output this binary code for the target

00:11:57,620 --> 00:12:01,940
machine that we're running on I've

00:11:59,960 --> 00:12:03,230
simplified this example somewhat I'm

00:12:01,940 --> 00:12:07,400
leaving the details of the software

00:12:03,230 --> 00:12:09,440
memory for later final code is wrapped

00:12:07,400 --> 00:12:11,990
up into a basic block which consists of

00:12:09,440 --> 00:12:13,700
a short piece of prologue code the

00:12:11,990 --> 00:12:16,460
translated code and then we have up to

00:12:13,700 --> 00:12:20,110
two possible exit pass in this example

00:12:16,460 --> 00:12:20,110
we never use the second exit path

00:12:20,200 --> 00:12:24,140
finally as the code runs these blocks

00:12:22,610 --> 00:12:26,960
are chained together and jump directly

00:12:24,140 --> 00:12:29,660
to each other this only actually occurs

00:12:26,960 --> 00:12:31,520
when we're operating in the same guest

00:12:29,660 --> 00:12:33,320
Paige because jumping out of the page

00:12:31,520 --> 00:12:37,550
may require looking up a new translated

00:12:33,320 --> 00:12:38,780
book because the mappings can change so

00:12:37,550 --> 00:12:40,640
let's just remind ourselves of what

00:12:38,780 --> 00:12:41,810
global state we need to worry about we

00:12:40,640 --> 00:12:43,940
have a bunch of Global's used during

00:12:41,810 --> 00:12:46,070
code generation and we have some global

00:12:43,940 --> 00:12:51,020
runtime structures which any vcpu might

00:12:46,070 --> 00:12:52,790
affect so fundamentally the translated

00:12:51,020 --> 00:12:55,970
code itself is safe when it's doing

00:12:52,790 --> 00:12:58,520
simple computation anything that it

00:12:55,970 --> 00:13:01,760
updates it only updates the internal

00:12:58,520 --> 00:13:03,110
structures for that vcpu so we really

00:13:01,760 --> 00:13:05,350
only need to worry about protecting

00:13:03,110 --> 00:13:07,550
things when we leave the generated code

00:13:05,350 --> 00:13:10,940
so let's look at some of the reasons why

00:13:07,550 --> 00:13:12,440
that might happen so the two cases I'm

00:13:10,940 --> 00:13:13,480
going to look at a returning to the main

00:13:12,440 --> 00:13:17,949
run loop

00:13:13,480 --> 00:13:20,110
or calling a helper function so there

00:13:17,949 --> 00:13:22,510
are two principal exit paths to the run

00:13:20,110 --> 00:13:24,490
loop the first is when we don't have the

00:13:22,510 --> 00:13:27,910
next block patched for the jump and this

00:13:24,490 --> 00:13:29,560
might be because it doesn't exist yet or

00:13:27,910 --> 00:13:31,360
a computer jump or something that needs

00:13:29,560 --> 00:13:33,220
to across a page boundary and the second

00:13:31,360 --> 00:13:36,160
case is where the vcpu has been signaled

00:13:33,220 --> 00:13:37,269
to exit so this is typically done when

00:13:36,160 --> 00:13:40,389
there's an interrupt but other things

00:13:37,269 --> 00:13:42,459
can signal and exit as well it's

00:13:40,389 --> 00:13:44,800
actually also possible to asynchronously

00:13:42,459 --> 00:13:48,639
exit the run loop on a signal and jump

00:13:44,800 --> 00:13:50,320
straight back to the run loop so this

00:13:48,639 --> 00:13:55,600
shows the simplified version of the run

00:13:50,320 --> 00:14:00,360
loop full versions a lot more scary as

00:13:55,600 --> 00:14:03,579
you can see running in a loop round here

00:14:00,360 --> 00:14:06,190
we find a block if we can't find it we

00:14:03,579 --> 00:14:07,690
need to generate it once we've generated

00:14:06,190 --> 00:14:10,089
it we run it and we jump into the

00:14:07,690 --> 00:14:12,790
translated code at some point we'll come

00:14:10,089 --> 00:14:14,920
back so the first thing we've done here

00:14:12,790 --> 00:14:19,420
it's in this code generation phase it's

00:14:14,920 --> 00:14:20,980
all protected by a translation lock so

00:14:19,420 --> 00:14:23,829
this means that we've effectively see

00:14:20,980 --> 00:14:26,019
realized all the code generation so only

00:14:23,829 --> 00:14:28,120
one thread can be generating code at a

00:14:26,019 --> 00:14:30,940
time but it's a relatively quick process

00:14:28,120 --> 00:14:37,180
and once you're running the translated

00:14:30,940 --> 00:14:39,880
code you're fully multi-threaded second

00:14:37,180 --> 00:14:41,319
cases helper functions so helper

00:14:39,880 --> 00:14:43,540
functions are called directly from the

00:14:41,319 --> 00:14:47,079
translated code and you can call them

00:14:43,540 --> 00:14:49,690
for a whole number of reasons often

00:14:47,079 --> 00:14:51,819
they're used to do an operation will be

00:14:49,690 --> 00:14:55,000
very fiddly to do with the TCG ops

00:14:51,819 --> 00:14:56,639
themselves so for example most of the

00:14:55,000 --> 00:15:00,760
simdi in floating point instructions

00:14:56,639 --> 00:15:02,680
implemented by helpers however a number

00:15:00,760 --> 00:15:04,690
of instructions have effect on system

00:15:02,680 --> 00:15:07,470
state so an example would be a system

00:15:04,690 --> 00:15:07,470
TLB flush

00:15:09,340 --> 00:15:14,500
the first case is fairly simple as long

00:15:12,730 --> 00:15:18,670
as the results are private to the vcp

00:15:14,500 --> 00:15:20,230
you know locking is required if the

00:15:18,670 --> 00:15:23,070
results affect the status of other V

00:15:20,230 --> 00:15:25,480
CPUs we need to use some sort of locking

00:15:23,070 --> 00:15:32,050
however there are operations that affect

00:15:25,480 --> 00:15:35,200
all of the vcp use so we could use lots

00:15:32,050 --> 00:15:38,230
to do most of this however you have to

00:15:35,200 --> 00:15:39,490
be careful if you're using a lot to

00:15:38,230 --> 00:15:42,340
protect a structure that's frequently

00:15:39,490 --> 00:15:45,340
read then you're going to start killing

00:15:42,340 --> 00:15:48,490
performance if you're going to use locks

00:15:45,340 --> 00:15:53,310
when you have to mull to modify lots of

00:15:48,490 --> 00:15:56,650
V CPUs that can quickly get very complex

00:15:53,310 --> 00:15:58,450
so it's another approaches we can just

00:15:56,650 --> 00:16:01,540
ensure that the vcp users have come to a

00:15:58,450 --> 00:16:03,280
halt and then do the changes that we

00:16:01,540 --> 00:16:07,200
want to do at that point I say at

00:16:03,280 --> 00:16:07,200
leisure it all happens fairly quickly so

00:16:07,230 --> 00:16:12,850
chrome you already has a deferred work

00:16:09,490 --> 00:16:15,520
system at work can be added to queue and

00:16:12,850 --> 00:16:18,130
then the vcpu a signal to stop once it

00:16:15,520 --> 00:16:20,860
exits the run loop it will process the

00:16:18,130 --> 00:16:23,110
work so for the multi-threading work

00:16:20,860 --> 00:16:26,200
there's been a new queue introduced

00:16:23,110 --> 00:16:27,760
called the queued safe work you this

00:16:26,200 --> 00:16:30,820
works pretty much the same as the other

00:16:27,760 --> 00:16:33,220
one but it waits for all the vcp use to

00:16:30,820 --> 00:16:38,350
stop so this is useful if you want to

00:16:33,220 --> 00:16:40,570
process system-wide events so in summary

00:16:38,350 --> 00:16:43,900
for the TCG we've moved some of the

00:16:40,570 --> 00:16:45,640
variables into per CPU structures the

00:16:43,900 --> 00:16:47,500
translation lock is used to protect all

00:16:45,640 --> 00:16:50,830
the code generation and change this to

00:16:47,500 --> 00:16:53,380
the various runtime state and finally

00:16:50,830 --> 00:16:54,700
there's a new safe work mechanism which

00:16:53,380 --> 00:16:58,650
can be used to work that needs all the

00:16:54,700 --> 00:16:58,650
vcp use to stop while changes are made

00:16:59,100 --> 00:17:06,580
now let's have a look at the way memory

00:17:02,260 --> 00:17:08,920
behaves in emulation so the TCG

00:17:06,580 --> 00:17:11,770
currently doesn't offer any atomic

00:17:08,920 --> 00:17:13,840
primitives so as a result the actual

00:17:11,770 --> 00:17:18,310
code generated will be a load store pair

00:17:13,840 --> 00:17:20,260
inside the basic block with the current

00:17:18,310 --> 00:17:22,240
single threaded behavior basic blocks

00:17:20,260 --> 00:17:23,140
always complete before the next vcpu is

00:17:22,240 --> 00:17:26,079
scheduled so this

00:17:23,140 --> 00:17:28,570
means any load modify store sequence is

00:17:26,079 --> 00:17:31,570
automatically atomic as no other call

00:17:28,570 --> 00:17:33,550
can be running at the same time even on

00:17:31,570 --> 00:17:36,130
weekly ordered systems loads and stores

00:17:33,550 --> 00:17:40,960
still on a program order for the CPU

00:17:36,130 --> 00:17:43,000
that they're running on once multiple

00:17:40,960 --> 00:17:44,770
threads are involved each potentially

00:17:43,000 --> 00:17:47,770
running on a different CPU all these

00:17:44,770 --> 00:17:50,200
assumptions break down code which work

00:17:47,770 --> 00:17:52,720
thanks to your implied atomicity before

00:17:50,200 --> 00:17:55,810
will fail a memory ordering also becomes

00:17:52,720 --> 00:17:57,280
an issue although tend not to notice it

00:17:55,810 --> 00:17:59,860
on the general case of running on x86

00:17:57,280 --> 00:18:02,440
because the backend is strongly ordered

00:17:59,860 --> 00:18:05,440
and this mast a lot of the undesirable

00:18:02,440 --> 00:18:08,700
behavior now of course well-written

00:18:05,440 --> 00:18:11,500
guess we'll use processor features to

00:18:08,700 --> 00:18:14,740
mitigate this so one let's look at one

00:18:11,500 --> 00:18:16,690
of those and load blink store

00:18:14,740 --> 00:18:19,530
conditional are a pair of instructions

00:18:16,690 --> 00:18:21,610
that are common on a lot of risk systems

00:18:19,530 --> 00:18:24,610
anything basically with a load store

00:18:21,610 --> 00:18:25,990
architecture this is in contrast to

00:18:24,610 --> 00:18:27,610
something like compare-and-swap which

00:18:25,990 --> 00:18:31,180
does a load modify store all in what

00:18:27,610 --> 00:18:32,920
instruction so the key thing is the

00:18:31,180 --> 00:18:34,600
store will only exceed if the address

00:18:32,920 --> 00:18:38,470
loaded for earlier hasn't been touched

00:18:34,600 --> 00:18:42,040
by any other cpu even if the value has

00:18:38,470 --> 00:18:44,230
remains the same the nice thing about

00:18:42,040 --> 00:18:46,240
load link store conditionals you can

00:18:44,230 --> 00:18:52,090
emulate all your other atomic primitives

00:18:46,240 --> 00:18:53,980
with it so introducing these proper load

00:18:52,090 --> 00:18:56,740
load link store conditional semantics

00:18:53,980 --> 00:18:58,630
into cremieux is important so we can

00:18:56,740 --> 00:19:00,880
model the load store exclusive in atomic

00:18:58,630 --> 00:19:02,260
instructions but doing this in a way

00:19:00,880 --> 00:19:05,650
that doesn't rely on any intrinsic

00:19:02,260 --> 00:19:06,820
back-end support is also important

00:19:05,650 --> 00:19:09,250
because we want to support all of our

00:19:06,820 --> 00:19:11,770
backends so the first step is we

00:19:09,250 --> 00:19:14,940
introduce a couple of new TCG operations

00:19:11,770 --> 00:19:17,320
for the load link and conditional store

00:19:14,940 --> 00:19:19,420
before we go further it's worth

00:19:17,320 --> 00:19:21,280
reminding yourself of how the soft mme

00:19:19,420 --> 00:19:25,510
works so let's talk a little about the

00:19:21,280 --> 00:19:27,370
implementation the soft mmu is

00:19:25,510 --> 00:19:30,520
responsible for mapping all the memory

00:19:27,370 --> 00:19:32,380
accesses to real memory on the host the

00:19:30,520 --> 00:19:34,660
mapping is actually done by simply

00:19:32,380 --> 00:19:36,360
adding an offset to the address and this

00:19:34,660 --> 00:19:38,780
will point into some real

00:19:36,360 --> 00:19:41,580
real address in the Crimea address space

00:19:38,780 --> 00:19:44,010
the fast path is all implemented in

00:19:41,580 --> 00:19:45,960
generated code and then there's a slow

00:19:44,010 --> 00:19:48,270
path that will fall back into the sea

00:19:45,960 --> 00:19:54,540
code and eventually this ends up walking

00:19:48,270 --> 00:19:57,380
these systems page tables I've drawn

00:19:54,540 --> 00:20:00,570
some nice diagram to show what happens

00:19:57,380 --> 00:20:02,340
each access to the memory contains three

00:20:00,570 --> 00:20:05,580
attributes so there's the address itself

00:20:02,340 --> 00:20:09,900
there's an MMU index and then there's an

00:20:05,580 --> 00:20:12,510
access type the MMU index here isn't

00:20:09,900 --> 00:20:15,059
directly related to the privilege level

00:20:12,510 --> 00:20:16,799
that you're running for example there

00:20:15,059 --> 00:20:18,780
are some LMU modes that allow you to

00:20:16,799 --> 00:20:23,280
access code on user space at the same

00:20:18,780 --> 00:20:25,440
time so the first thing we take mmm new

00:20:23,280 --> 00:20:29,370
index indexes into the table that we're

00:20:25,440 --> 00:20:32,540
going to use given the right mmu table

00:20:29,370 --> 00:20:35,520
we then generate an index into the TLB

00:20:32,540 --> 00:20:37,799
so the size that each entry the TLB

00:20:35,520 --> 00:20:39,840
covers is driven by the number of mmu

00:20:37,799 --> 00:20:42,000
modes we have because the total table

00:20:39,840 --> 00:20:45,540
needs to fit into a reasonable amount of

00:20:42,000 --> 00:20:51,090
space but in general at least be an

00:20:45,540 --> 00:20:53,120
8-bit index table finally the target of

00:20:51,090 --> 00:20:55,770
the guest address is masked out and

00:20:53,120 --> 00:20:59,309
compared to the entry in the TLB table

00:20:55,770 --> 00:21:03,270
if it matches the offset is applied if

00:20:59,309 --> 00:21:04,890
not we fall back to the slow path so in

00:21:03,270 --> 00:21:07,440
the case of doing a bunch of operations

00:21:04,890 --> 00:21:08,730
in the same page we'll do the slow path

00:21:07,440 --> 00:21:10,410
once and then all the following

00:21:08,730 --> 00:21:15,120
operations we'll just keep using the

00:21:10,410 --> 00:21:18,240
entry these flags are quite important

00:21:15,120 --> 00:21:21,570
though you can set flags in the lower

00:21:18,240 --> 00:21:24,270
bits and this will cause any match

00:21:21,570 --> 00:21:29,309
operation to fail and force us into the

00:21:24,270 --> 00:21:31,799
slow path and this is quite useful so

00:21:29,309 --> 00:21:33,150
how does this help with load lincoln

00:21:31,799 --> 00:21:37,890
store conditional so we've already

00:21:33,150 --> 00:21:40,020
introduced that front end TCG ops so

00:21:37,890 --> 00:21:42,509
using the soft mmu slow path we can then

00:21:40,020 --> 00:21:44,190
implement a backend in

00:21:42,509 --> 00:21:49,079
went back in for these operations in a

00:21:44,190 --> 00:21:53,129
fairly generic way we do this by

00:21:49,079 --> 00:21:57,089
introducing a new TLB flag so when a

00:21:53,129 --> 00:21:59,429
load link operation occurs we set a TLB

00:21:57,089 --> 00:22:02,329
exclusive flag for the page that we're

00:21:59,429 --> 00:22:05,519
in and then when the store conditional

00:22:02,329 --> 00:22:09,029
occurs it sees the flag is still set and

00:22:05,519 --> 00:22:11,699
then that succeeds after the load link

00:22:09,029 --> 00:22:13,859
store perras happen normal loads and

00:22:11,699 --> 00:22:15,299
stores can just happen as normal it

00:22:13,859 --> 00:22:18,509
doesn't affect the slope it doesn't

00:22:15,299 --> 00:22:21,509
affect the performance if you do a load

00:22:18,509 --> 00:22:24,239
link and then some other cpu then does

00:22:21,509 --> 00:22:26,699
it right because this flag is set you'll

00:22:24,239 --> 00:22:28,319
force the slow path and then we'll go

00:22:26,699 --> 00:22:30,869
through all the slow path and the MMU

00:22:28,319 --> 00:22:34,229
helpers the MMU helpers will then flip

00:22:30,869 --> 00:22:36,119
the exclusive bit and when the store

00:22:34,229 --> 00:22:38,190
conditional occurs we know that some

00:22:36,119 --> 00:22:39,989
other piece of memory is accessed so

00:22:38,190 --> 00:22:44,459
some other processes access the memory

00:22:39,989 --> 00:22:47,659
and then store conditional would fail so

00:22:44,459 --> 00:22:50,519
in summary introduced a new partner flag

00:22:47,659 --> 00:22:53,039
we can force all accesses down the slow

00:22:50,519 --> 00:22:55,859
path whilst the flag is in process is

00:22:53,039 --> 00:22:58,349
active we trip the exclusive flag and

00:22:55,859 --> 00:23:04,769
then store conditional knows whether or

00:22:58,349 --> 00:23:06,389
not memory has changed so in summary the

00:23:04,769 --> 00:23:08,399
software mu does offer us a fairly

00:23:06,389 --> 00:23:10,979
efficient way to guess memory into the

00:23:08,399 --> 00:23:13,049
host and at the same time it provides a

00:23:10,979 --> 00:23:14,579
fairly efficient way of trapping access

00:23:13,049 --> 00:23:16,949
to certain regions of memory which we

00:23:14,579 --> 00:23:18,959
can use for many things in this case for

00:23:16,949 --> 00:23:21,569
implementing a load link store

00:23:18,959 --> 00:23:25,079
conditional support this doesn't overly

00:23:21,569 --> 00:23:28,609
burden our normal memory access memory

00:23:25,079 --> 00:23:28,609
barriers however are still an issue

00:23:28,789 --> 00:23:36,690
finally let's look at dubai simulation

00:23:33,319 --> 00:23:40,109
good news kvm is already done it all for

00:23:36,690 --> 00:23:42,389
us the kvm model is very much relies on

00:23:40,109 --> 00:23:44,129
having a thread per be CPU so they had

00:23:42,389 --> 00:23:47,579
to add thread safety to a number of

00:23:44,129 --> 00:23:49,949
systems so this included adding a memory

00:23:47,579 --> 00:23:52,349
api so all accesses to the emulated

00:23:49,949 --> 00:23:53,280
device can be properly sterilized with

00:23:52,349 --> 00:23:55,590
locks

00:23:53,280 --> 00:23:57,570
and the other big change was introducing

00:23:55,590 --> 00:23:59,190
an io thread which involves separating

00:23:57,570 --> 00:24:04,080
the handling of io from the rest of the

00:23:59,190 --> 00:24:07,650
system so for TCG access to the device

00:24:04,080 --> 00:24:10,230
memory all the mmio pages are flagged in

00:24:07,650 --> 00:24:12,630
the soft mmu TLB so as you already know

00:24:10,230 --> 00:24:15,240
this will force a slow path this goes

00:24:12,630 --> 00:24:17,400
into our memory API then the memory API

00:24:15,240 --> 00:24:19,410
defines the region of memory as either

00:24:17,400 --> 00:24:21,330
being locked us or locked with the bql

00:24:19,410 --> 00:24:23,610
by default pretty much everything is

00:24:21,330 --> 00:24:25,500
locked with the bql but if you've got a

00:24:23,610 --> 00:24:28,110
device emulation that's going to deal

00:24:25,500 --> 00:24:36,660
with its own locking you can have a lot

00:24:28,110 --> 00:24:38,250
less entry so thanks kvm so let's talk

00:24:36,660 --> 00:24:40,890
now about the current state and for this

00:24:38,250 --> 00:24:48,270
and a little performance and demo slides

00:24:40,890 --> 00:24:52,280
I'm going to hand over to Fred hello

00:24:48,270 --> 00:24:58,200
everybody I'm Connor Frank but should

00:24:52,280 --> 00:25:01,710
call me friday so i want to show you or

00:24:58,200 --> 00:25:04,530
some green socks empty empty TD see

00:25:01,710 --> 00:25:07,220
proof of concepts and some benchmark

00:25:04,530 --> 00:25:07,220
results

00:25:10,029 --> 00:25:15,359
like that oh sorry

00:25:18,770 --> 00:25:25,180
so first I would like to introduce clean

00:25:21,860 --> 00:25:32,330
socks return so it has been created in

00:25:25,180 --> 00:25:35,980
2005 we your mind focused on the open

00:25:32,330 --> 00:25:40,520
source Ottawa simulation we have you

00:25:35,980 --> 00:25:43,250
owned the open source system silvery

00:25:40,520 --> 00:25:46,160
which called the green lib we are the

00:25:43,250 --> 00:25:51,730
with platform and models for customers

00:25:46,160 --> 00:25:58,510
and we contribute to qmu consistency and

00:25:51,730 --> 00:26:03,680
mt TCG so it's a proof of concept I

00:25:58,510 --> 00:26:06,740
wanted to keep it simple so we just hung

00:26:03,680 --> 00:26:11,600
a little will vote system Kristen one on

00:26:06,740 --> 00:26:15,860
our Vic's place a 15 virtual machine so

00:26:11,600 --> 00:26:21,080
the the goal is to check that all vcpu

00:26:15,860 --> 00:26:26,180
of their own thread and check if the

00:26:21,080 --> 00:26:29,710
performance is very better so I will try

00:26:26,180 --> 00:26:29,710
to find back my

00:26:33,450 --> 00:26:36,080
ok

00:26:54,399 --> 00:27:03,370
don't like it so

00:27:00,559 --> 00:27:03,370
here we go

00:27:05,850 --> 00:27:16,390
not easy

00:27:08,230 --> 00:27:20,730
so we run some ash top just to see what

00:27:16,390 --> 00:27:20,730
happened there is a MTG teach each other

00:27:29,570 --> 00:27:42,910
there we go and Charlie we also want

00:27:36,580 --> 00:27:46,910
cream you entities empty TCG running so

00:27:42,910 --> 00:27:52,250
we can just find through twice on at the

00:27:46,910 --> 00:27:54,430
same time and check how many time it

00:27:52,250 --> 00:27:54,430
will take

00:28:03,430 --> 00:28:15,700
here you can see that both vcpu treads

00:28:07,030 --> 00:28:19,300
are fellow did under post Oh cpu was

00:28:15,700 --> 00:28:22,090
followed as well you can see as webs

00:28:19,300 --> 00:28:27,700
that tree take approximately six second

00:28:22,090 --> 00:28:33,760
to achieve this cure on chest so that's

00:28:27,700 --> 00:28:38,650
pretty fast if you want only one twice

00:28:33,760 --> 00:28:42,970
on tests you can see that only one VT

00:28:38,650 --> 00:28:46,950
putrid is fully loaded so in fact that

00:28:42,970 --> 00:28:53,440
means that so twice on one on you on one

00:28:46,950 --> 00:28:58,380
vcpu in the guest and we can compare all

00:28:53,440 --> 00:28:58,380
that with upstream show you

00:29:14,049 --> 00:29:21,019
so we have only Swiss thread because uh

00:29:17,649 --> 00:29:26,480
of course empty tdg waiter another

00:29:21,019 --> 00:29:33,549
sweater and we should do exactly the

00:29:26,480 --> 00:29:33,549
same same commands the same come online

00:29:38,710 --> 00:29:45,130
you can see that you have only one

00:29:40,510 --> 00:29:48,809
thread fully loaded which goes on both

00:29:45,130 --> 00:29:48,809
DCP both cpu

00:29:51,960 --> 00:30:00,270
finally take approximately 30 seconds

00:29:56,330 --> 00:30:09,570
which is approximately twice as empty

00:30:00,270 --> 00:30:16,350
TCG so so is the performance really

00:30:09,570 --> 00:30:22,050
better yes yes I took some number with

00:30:16,350 --> 00:30:27,060
civil test just 22 more precise results

00:30:22,050 --> 00:30:30,810
I run the test with 12 14 v CPUs and

00:30:27,060 --> 00:30:33,860
then I 12 for this lots of dry stone

00:30:30,810 --> 00:30:40,680
plaster check from this table we can

00:30:33,860 --> 00:30:46,890
extract the last line we do the do we do

00:30:40,680 --> 00:30:53,340
one for parallel to ISIL for 124 vcpu

00:30:46,890 --> 00:30:58,410
then we compared with extreme we can see

00:30:53,340 --> 00:31:03,090
that the red line is mt TCG the time is

00:30:58,410 --> 00:31:06,030
a highly reduced compared to upstream we

00:31:03,090 --> 00:31:09,690
are sixty-three percent faster with a 4

00:31:06,030 --> 00:31:15,560
vcpu and forty five percent faster with

00:31:09,690 --> 00:31:15,560
two vcpu which is quite good I guess

00:31:16,110 --> 00:31:26,580
however the test is a medial under the

00:31:20,840 --> 00:31:29,820
benchmark or the part of mt TCG for

00:31:26,580 --> 00:31:33,330
example or the TLB maintenance operation

00:31:29,820 --> 00:31:36,390
which are quite slow if we exceed when

00:31:33,330 --> 00:31:41,030
we do that same for TB regulate and

00:31:36,390 --> 00:31:43,850
validation on TV flush and then the

00:31:41,030 --> 00:31:49,010
widgets were safe GTG atomic instruction

00:31:43,850 --> 00:31:49,010
not been best part with this test

00:31:49,850 --> 00:31:56,429
additionally if you take only one

00:31:52,679 --> 00:32:00,210
drytown instance we can see that we

00:31:56,429 --> 00:32:04,260
tried the TCG is a little roller example

00:32:00,210 --> 00:32:09,330
for for vcpu we have four percent of

00:32:04,260 --> 00:32:11,660
overhead it's might come from blocks

00:32:09,330 --> 00:32:11,660
maybe

00:32:16,000 --> 00:32:25,750
and as a conclusion of this result

00:32:20,940 --> 00:32:29,580
personally I wouldn't I won't make life

00:32:25,750 --> 00:32:34,180
domani more it's those sweating and for

00:32:29,580 --> 00:32:37,930
mt TCG can say your performance a better

00:32:34,180 --> 00:32:42,240
we know why we do that it's a good photo

00:32:37,930 --> 00:32:46,830
increasing parallel tasks in new guests

00:32:42,240 --> 00:32:52,350
but you have more test case to do

00:32:46,830 --> 00:32:55,750
especially for all this TBT nvflash

00:32:52,350 --> 00:32:58,960
external we might want to lock the

00:32:55,750 --> 00:33:02,350
number of occurrence of this this

00:32:58,960 --> 00:33:08,710
operation unmeasured time it takes while

00:33:02,350 --> 00:33:10,950
combine it to a cube swim on over to

00:33:08,710 --> 00:33:10,950
Alex

00:33:11,580 --> 00:33:21,730
ah there we go exit right so let's just

00:33:20,200 --> 00:33:22,780
quickly talk about what's left so I'm

00:33:21,730 --> 00:33:24,700
going to talk about the state of the

00:33:22,780 --> 00:33:28,090
load lincoln store conditional patches

00:33:24,700 --> 00:33:30,700
the multi-threaded TCG patches memory

00:33:28,090 --> 00:33:32,050
barriers what we need to do the front

00:33:30,700 --> 00:33:36,280
and back ends and testing and

00:33:32,050 --> 00:33:38,350
documentation so the majority of the

00:33:36,280 --> 00:33:40,360
load link store conditional patches are

00:33:38,350 --> 00:33:42,970
actually independent from multi-threaded

00:33:40,360 --> 00:33:44,620
TCG there's only two additional patches

00:33:42,970 --> 00:33:47,200
needed on top to enable it for the

00:33:44,620 --> 00:33:50,500
multi-threaded case it's already been

00:33:47,200 --> 00:33:52,570
through a number of review cycles and we

00:33:50,500 --> 00:33:55,630
hope you have to get it moved merged

00:33:52,570 --> 00:33:57,370
soonish now that the tree is open the

00:33:55,630 --> 00:33:59,620
work on that has been done by Alviss

00:33:57,370 --> 00:34:02,790
rigor of virtual open systems and you

00:33:59,620 --> 00:34:08,230
can see is work there's also a MTT cg

00:34:02,790 --> 00:34:11,460
branch as well up there so for the

00:34:08,230 --> 00:34:13,780
fred's multi-threaded TCG patches

00:34:11,460 --> 00:34:15,400
they've gone through a lot of cleanup

00:34:13,780 --> 00:34:17,440
they've been through several rounds of

00:34:15,400 --> 00:34:21,130
review and we're starting to get some of

00:34:17,440 --> 00:34:23,950
the simpler cleanup ones get pulled into

00:34:21,130 --> 00:34:28,390
the maintainer trees so this means the

00:34:23,950 --> 00:34:32,650
Delta between main line and enabling for

00:34:28,390 --> 00:34:34,780
multi-threaded TCG is going to reduce so

00:34:32,650 --> 00:34:36,430
as you see Fred did the work from green

00:34:34,780 --> 00:34:38,080
socks who were very generously sponsored

00:34:36,430 --> 00:34:42,880
in for the last six months to work on

00:34:38,080 --> 00:34:46,330
this memory barriers there is no code

00:34:42,880 --> 00:34:49,960
yet to deal with memory barriers got a

00:34:46,330 --> 00:34:53,190
proposal that introduces 12 maybe more

00:34:49,960 --> 00:34:55,570
TCG ops to represent the memory barriers

00:34:53,190 --> 00:34:57,250
however this is a case that most people

00:34:55,570 --> 00:35:00,040
don't see because if you're running on

00:34:57,250 --> 00:35:02,650
x86 back-end you probably get away with

00:35:00,040 --> 00:35:05,610
it the pathological case of course is

00:35:02,650 --> 00:35:08,260
running x86 on a weekly audit back-end

00:35:05,610 --> 00:35:13,870
and whether or not that's ever going to

00:35:08,260 --> 00:35:15,940
be practical remains to be seen so the

00:35:13,870 --> 00:35:19,990
current testing has mainly been focused

00:35:15,940 --> 00:35:21,960
on I'm 32 on x86 but we want to enable

00:35:19,990 --> 00:35:25,349
it on all from

00:35:21,960 --> 00:35:28,500
i Ken combinations each from ten that

00:35:25,349 --> 00:35:30,510
leads to support that wants to support

00:35:28,500 --> 00:35:33,270
multi-threaded TCG will need to have

00:35:30,510 --> 00:35:38,400
their atomic semantics updated to use

00:35:33,270 --> 00:35:40,170
the new TCG operations and if we go for

00:35:38,400 --> 00:35:41,730
TCG operations memory barriers each

00:35:40,170 --> 00:35:44,369
back-end will also need to support the

00:35:41,730 --> 00:35:46,080
memory barrier operations so this might

00:35:44,369 --> 00:35:48,990
require having an incremental approach

00:35:46,080 --> 00:35:51,240
where only some combinations build for

00:35:48,990 --> 00:35:53,400
multi-threaded TCG at the start and then

00:35:51,240 --> 00:35:58,320
as each but front and back-end is

00:35:53,400 --> 00:36:00,810
converted we can turn it on testing and

00:35:58,320 --> 00:36:03,359
documentation both are important i mean

00:36:00,810 --> 00:36:07,380
this is a potentially invasive change to

00:36:03,359 --> 00:36:09,270
chrome you we want to have a broad array

00:36:07,380 --> 00:36:11,190
of tests we've already got a number of

00:36:09,270 --> 00:36:13,140
hand rolled tests and we've been

00:36:11,190 --> 00:36:15,359
starting to build up a number of kvm

00:36:13,140 --> 00:36:18,119
unit tests which now accepts TCG as a

00:36:15,359 --> 00:36:20,010
first class citizen and mainly torture

00:36:18,119 --> 00:36:22,200
tests are there really to hammer all the

00:36:20,010 --> 00:36:23,520
various flushing cases and make sure

00:36:22,200 --> 00:36:27,390
we've not grow any dead locks and the

00:36:23,520 --> 00:36:29,790
locks and also we want to have a final

00:36:27,390 --> 00:36:31,980
design reference for how multi-threaded

00:36:29,790 --> 00:36:36,390
TCG works in the docs directory and no

00:36:31,980 --> 00:36:40,760
one has to guess about how it works so

00:36:36,390 --> 00:36:40,760
with that are there any more questions

00:36:48,570 --> 00:36:56,250
hi I do you us to use a empty TCG

00:36:53,220 --> 00:37:01,890
we forgiven on the wheel mm you to

00:36:56,250 --> 00:37:05,520
improve performance oh is this lee so

00:37:01,890 --> 00:37:09,600
this is running TCG code but using kvm

00:37:05,520 --> 00:37:13,170
to do the memory mapping I've dreamt

00:37:09,600 --> 00:37:17,760
about it from time to time has been

00:37:13,170 --> 00:37:19,770
nothing more than a dream hi so did you

00:37:17,760 --> 00:37:23,160
already tried self-modifying code on it

00:37:19,770 --> 00:37:26,220
or such things so that one we see be you

00:37:23,160 --> 00:37:28,860
would actually invalidate another's we

00:37:26,220 --> 00:37:30,510
CPU translation book yeah I mean when

00:37:28,860 --> 00:37:34,250
you can trigger that by doing things

00:37:30,510 --> 00:37:37,260
like TLB flushes that will flush all the

00:37:34,250 --> 00:37:39,390
all the coded entries and then the

00:37:37,260 --> 00:37:41,510
toilet a torture test do exactly that ok

00:37:39,390 --> 00:37:45,240
and do they also try to do that with

00:37:41,510 --> 00:37:49,410
incoming exceptions for example given a

00:37:45,240 --> 00:37:55,140
division by zero such a thing no not yet

00:37:49,410 --> 00:38:00,600
but that's a good idea to add so he said

00:37:55,140 --> 00:38:03,720
your lovely sorry Lloyd link store

00:38:00,600 --> 00:38:08,030
conditional is effectively managing the

00:38:03,720 --> 00:38:10,200
reservation at page granularity my

00:38:08,030 --> 00:38:12,690
recollection is that most real CPUs

00:38:10,200 --> 00:38:15,260
manage it at cache line granularity does

00:38:12,690 --> 00:38:21,480
that change mean you could get more

00:38:15,260 --> 00:38:24,630
store conditional failures right so for

00:38:21,480 --> 00:38:27,300
an architectural icon the granularity of

00:38:24,630 --> 00:38:30,960
the protected region is a implementation

00:38:27,300 --> 00:38:34,110
defined number and I suspect from the

00:38:30,960 --> 00:38:38,030
performance basis yo you just have to go

00:38:34,110 --> 00:38:41,160
with what brand new larity the TCG has

00:38:38,030 --> 00:38:43,950
rather than try and do low on so you do

00:38:41,160 --> 00:38:46,260
run a risk of having more more bounces

00:38:43,950 --> 00:38:48,210
on your atomic operations because some

00:38:46,260 --> 00:38:50,600
other unrelated things is touching well

00:38:48,210 --> 00:38:50,600
above it

00:38:57,440 --> 00:39:05,740
any more questions okay well thank you

00:39:03,380 --> 00:39:05,740

YouTube URL: https://www.youtube.com/watch?v=KnSW0WjWHZI


