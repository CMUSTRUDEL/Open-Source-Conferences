Title: [2015] Real-Time KVM by Rik van Riel
Publication date: 2015-08-25
Playlist: KVM Forum 2015
Description: 
	KVM is now suitable for low latency real time workloads. Getting there required several changes to the code, which are now now upstream. Real time KVM also requires very careful system configuration. This presentation will describe some of the issues faced (and fixed) during development, pitfalls in deploying real time KVM (and how to avoid them), as well as some of the automation available to easily deploy real time KVM.

Rik van Riel
Principal Software Engineer, Red Hat
Rik van Riel is a principal software engineer at Red Hat, and a long term contributor to the Linux kernel. He has contributed to the memory management subsystem, the scheduler, and various components related to virtualization. Rik is active in community projects like kernelnewbies.org and likes to hike and rock climb in his spare time.

Slides: http://events.linuxfoundation.org/sites/events/files/slides/kvmforum2015-realtimekvm.pdf

Note: We are appologizing for lower audio quality due to technical issues.
Captions: 
	00:00:14,269 --> 00:00:22,410
okay well turns out both young and I

00:00:18,199 --> 00:00:26,550
submitted talks real-time kvm and we

00:00:22,410 --> 00:00:30,359
coordinated to go through things step by

00:00:26,550 --> 00:00:32,420
step so our talks should sit together

00:00:30,359 --> 00:00:37,559
relatively well and go through the whole

00:00:32,420 --> 00:00:40,860
stack of real-time kvm I'm starting on

00:00:37,559 --> 00:00:43,160
the ground floor explaining what real

00:00:40,860 --> 00:00:47,730
time is why people care about it and

00:00:43,160 --> 00:00:49,530
going from Hardware up to kvm and after

00:00:47,730 --> 00:00:51,140
that young will go through the whole

00:00:49,530 --> 00:00:55,110
management stacked above kayvyun

00:00:51,140 --> 00:00:57,329
explaining the things that are in that

00:00:55,110 --> 00:01:03,180
part of the stack necessary to make

00:00:57,329 --> 00:01:07,080
real-time happen let's first go Oh what

00:01:03,180 --> 00:01:09,780
is real time when a lot of people think

00:01:07,080 --> 00:01:14,460
real time they think it is about doing

00:01:09,780 --> 00:01:17,640
things very very quickly and to a

00:01:14,460 --> 00:01:20,930
certain extent that is true but only to

00:01:17,640 --> 00:01:25,140
certain extent real time is about

00:01:20,930 --> 00:01:28,020
determinism and the things that are

00:01:25,140 --> 00:01:33,450
needed to get to that determinism almost

00:01:28,020 --> 00:01:35,790
always results in lower throughput but

00:01:33,450 --> 00:01:39,030
for certain workloads that's fine

00:01:35,790 --> 00:01:41,070
because determinism and guaranteed

00:01:39,030 --> 00:01:44,520
latency are really the most important

00:01:41,070 --> 00:01:49,979
things telco switching is an obvious

00:01:44,520 --> 00:01:52,200
case where if the software is but it's

00:01:49,979 --> 00:01:54,619
poorly written and packets do not get

00:01:52,200 --> 00:01:59,130
routed on time or converted on something

00:01:54,619 --> 00:02:00,979
then people's voice will break up over

00:01:59,130 --> 00:02:03,090
the phone and if it happens too much

00:02:00,979 --> 00:02:07,290
people switch to a different phone

00:02:03,090 --> 00:02:11,370
company so they really care about always

00:02:07,290 --> 00:02:14,060
having sound and voice and Other

00:02:11,370 --> 00:02:17,610
communication routed on time

00:02:14,060 --> 00:02:22,140
stock trading is a similar thing but

00:02:17,610 --> 00:02:24,420
worse if there is if there are multiple

00:02:22,140 --> 00:02:27,690
client requests coming into the same

00:02:24,420 --> 00:02:32,880
broker and the requests of one of those

00:02:27,690 --> 00:02:38,520
clients are not treated according to the

00:02:32,880 --> 00:02:41,190
rules then the brokerage or the stock

00:02:38,520 --> 00:02:44,570
exchange could get million-dollar fines

00:02:41,190 --> 00:02:48,060
from the government even if the

00:02:44,570 --> 00:02:51,570
maltreatment of some of those requests

00:02:48,060 --> 00:02:54,170
was due to a software bug so they care

00:02:51,570 --> 00:02:56,790
about really deterministic performance

00:02:54,170 --> 00:02:59,910
he talking about Rockets of course or

00:02:56,790 --> 00:03:05,760
other avionics things it is even more

00:02:59,910 --> 00:03:08,220
important if the computer that controls

00:03:05,760 --> 00:03:10,830
your Rockets goes into the garbage

00:03:08,220 --> 00:03:13,890
collection cycle and it doesn't do

00:03:10,830 --> 00:03:16,260
anything for half a second you could

00:03:13,890 --> 00:03:18,000
deal with an exploding rockets or if

00:03:16,260 --> 00:03:19,590
you're the military you could deal with

00:03:18,000 --> 00:03:24,180
the rocket that doesn't explode on time

00:03:19,590 --> 00:03:26,880
which is just as bad so this is

00:03:24,180 --> 00:03:29,850
something where people really care about

00:03:26,880 --> 00:03:35,579
response times happening when they

00:03:29,850 --> 00:03:37,620
expect a response and for different

00:03:35,579 --> 00:03:40,200
workloads you're going to have different

00:03:37,620 --> 00:03:44,190
maximum Layton sees or maximum depth

00:03:40,200 --> 00:03:47,220
response lines and some of the cases for

00:03:44,190 --> 00:03:49,579
example telcos they might be okay with

00:03:47,220 --> 00:03:52,290
occasionally having a response that

00:03:49,579 --> 00:03:55,019
takes a little longer but only very

00:03:52,290 --> 00:03:56,700
rarely because they do not want their

00:03:55,019 --> 00:04:01,500
customers to get upset that their voice

00:03:56,700 --> 00:04:05,959
breaks up all the time go back to the

00:04:01,500 --> 00:04:12,959
same flight that Paolo showed basic rell

00:04:05,959 --> 00:04:17,030
7.2 if the red breath and almost all the

00:04:12,959 --> 00:04:19,890
time the normal kernel has low latencies

00:04:17,030 --> 00:04:23,190
but almost all the time is not good

00:04:19,890 --> 00:04:25,620
enough for some workouts and that is why

00:04:23,190 --> 00:04:26,889
the realtime kernel exists which is the

00:04:25,620 --> 00:04:31,449
green line

00:04:26,889 --> 00:04:34,990
and nowadays also the real time AVM set

00:04:31,449 --> 00:04:39,029
up it's the blue line the average

00:04:34,990 --> 00:04:41,379
latency is on the real time kvm set up

00:04:39,029 --> 00:04:44,469
they are almost certainly going to be

00:04:41,379 --> 00:04:46,319
higher than the average latency on even

00:04:44,469 --> 00:04:50,560
a non real-time Colonel on bare metal

00:04:46,319 --> 00:04:53,349
but again it's the maximum latency that

00:04:50,560 --> 00:04:57,120
is really important because every single

00:04:53,349 --> 00:05:05,469
request comes in needs to be handled

00:04:57,120 --> 00:05:07,240
within a certain deadline as usual the

00:05:05,469 --> 00:05:13,389
biggest pitfalls on a system are the

00:05:07,240 --> 00:05:16,509
bios the firmware and the bios a on x86

00:05:13,389 --> 00:05:19,060
systems in particular system management

00:05:16,509 --> 00:05:24,099
mode and sister management interrupts

00:05:19,060 --> 00:05:26,650
they're always good for trouble system

00:05:24,099 --> 00:05:30,370
management mode is used by hardware

00:05:26,650 --> 00:05:34,210
vendors to implement certain hardware

00:05:30,370 --> 00:05:36,490
features in software for example if I

00:05:34,210 --> 00:05:39,610
were to press the brightness increase or

00:05:36,490 --> 00:05:43,539
decrease on my laptop here chances are

00:05:39,610 --> 00:05:46,120
that it's not going directly to some

00:05:43,539 --> 00:05:48,849
micro controller that controls the

00:05:46,120 --> 00:05:55,089
monitoring it will actually track into

00:05:48,849 --> 00:05:57,669
the CPU and the CPU will it receives the

00:05:55,089 --> 00:06:00,399
system management interrupt and it will

00:05:57,669 --> 00:06:04,300
switch the system management mode it

00:06:00,399 --> 00:06:08,949
will stop execution of all the tasks on

00:06:04,300 --> 00:06:10,509
all two CPUs in system and it is not

00:06:08,949 --> 00:06:13,330
going to return control for the

00:06:10,509 --> 00:06:17,500
operating system until the monitor

00:06:13,330 --> 00:06:18,909
brightness has been adjusted and is used

00:06:17,500 --> 00:06:21,159
for all kinds of things in system

00:06:18,909 --> 00:06:25,839
management and it might be used to

00:06:21,159 --> 00:06:29,050
control fan speed on a server it might

00:06:25,839 --> 00:06:32,729
be used for a nice management console

00:06:29,050 --> 00:06:36,190
that you connect to over the network and

00:06:32,729 --> 00:06:38,560
this runs completely outside the control

00:06:36,190 --> 00:06:41,139
of the operating system

00:06:38,560 --> 00:06:44,250
in some cases it could take milliseconds

00:06:41,139 --> 00:06:47,860
to run and really if your hardware

00:06:44,250 --> 00:06:50,800
prevents your operating system from

00:06:47,860 --> 00:06:54,490
running for several milliseconds there

00:06:50,800 --> 00:06:57,730
is no way you can guarantee micro second

00:06:54,490 --> 00:07:02,230
Layton sees it is going to be unable to

00:06:57,730 --> 00:07:07,570
meet your deadlines if your hardware

00:07:02,230 --> 00:07:09,490
does not let you run so this is an issue

00:07:07,570 --> 00:07:13,810
that really has to be fixed at the

00:07:09,490 --> 00:07:16,660
hardware level for many servers it is

00:07:13,810 --> 00:07:19,720
possible to disable some or all of these

00:07:16,660 --> 00:07:22,600
features in the bios just by going into

00:07:19,720 --> 00:07:24,400
the BIOS configuration and disabling

00:07:22,600 --> 00:07:27,010
thermal management disabling power

00:07:24,400 --> 00:07:28,870
management may be disabling hyper

00:07:27,010 --> 00:07:31,990
shredding all kinds of things that can

00:07:28,870 --> 00:07:34,930
have influenced you on the other hand

00:07:31,990 --> 00:07:37,270
there are also some systems that are not

00:07:34,930 --> 00:07:39,250
fixable here I'm not looking at anybody

00:07:37,270 --> 00:07:42,010
in particular i'm looking at general

00:07:39,250 --> 00:07:48,039
direction and you feel guilty please fix

00:07:42,010 --> 00:07:50,320
your hardware but but the the lesson

00:07:48,039 --> 00:07:53,680
here is that if you want to run real

00:07:50,320 --> 00:07:58,180
time you need to buy hardware that can

00:07:53,680 --> 00:08:00,580
actually do real time and there are some

00:07:58,180 --> 00:08:04,390
tools to help test whether or not your

00:08:00,580 --> 00:08:09,210
hardware is capable there is a hardware

00:08:04,390 --> 00:08:13,660
latency detects module that basically

00:08:09,210 --> 00:08:17,340
runs at very high real-time priority in

00:08:13,660 --> 00:08:20,979
the kernel and it disables the

00:08:17,340 --> 00:08:22,660
interrupts on that CPU and you just

00:08:20,979 --> 00:08:27,460
leave it running out overnight or over

00:08:22,660 --> 00:08:32,320
the weekend and if it notices any large

00:08:27,460 --> 00:08:36,550
latency spikes it will log them and it

00:08:32,320 --> 00:08:40,479
can also check the SM icons in the CPUs

00:08:36,550 --> 00:08:44,849
msrs and see whether such latencies were

00:08:40,479 --> 00:08:44,849
because of an SMI it happened

00:08:47,370 --> 00:08:53,660
linux of course this latency issues that

00:08:51,810 --> 00:08:59,130
are similar to what is found in the bios

00:08:53,660 --> 00:09:02,670
but for different reasons in a normal

00:08:59,130 --> 00:09:06,570
linux kernel we have critical sections

00:09:02,670 --> 00:09:08,880
that cannot be preempted we cannot

00:09:06,570 --> 00:09:11,339
schedule anything else while we are in

00:09:08,880 --> 00:09:15,060
an interrupt while in holding it spin

00:09:11,339 --> 00:09:18,420
lock or even while we are holding the

00:09:15,060 --> 00:09:21,510
arse you read lock you cannot reschedule

00:09:18,420 --> 00:09:27,150
let CPU until the lock has been released

00:09:21,510 --> 00:09:30,029
or the interrupt code is done that means

00:09:27,150 --> 00:09:33,990
that if we have a really high priority

00:09:30,029 --> 00:09:36,210
program that wants to run but currently

00:09:33,990 --> 00:09:39,540
we are running some low priority program

00:09:36,210 --> 00:09:41,550
that is holding a kernel spin lock the

00:09:39,540 --> 00:09:46,100
high priority program is going to have

00:09:41,550 --> 00:09:50,130
to wait this list unacceptable latencies

00:09:46,100 --> 00:09:52,250
and a realtime kernel developers have

00:09:50,130 --> 00:09:57,150
been working on this for many years to

00:09:52,250 --> 00:10:00,600
improve the situation a lot of the real

00:09:57,150 --> 00:10:04,410
time preemption code has already made it

00:10:00,600 --> 00:10:06,900
into the mainline linux kernel some of

00:10:04,410 --> 00:10:10,290
the patches for the config preempt RT

00:10:06,900 --> 00:10:13,350
staff are still in a separate tree which

00:10:10,290 --> 00:10:16,290
stephen rostec maintains and optimist

00:10:13,350 --> 00:10:18,150
like snow and other people and they

00:10:16,290 --> 00:10:20,700
actually have to talk about how to get

00:10:18,150 --> 00:10:24,510
that code upstream at plumbers today I

00:10:20,700 --> 00:10:26,940
think at eleven o'clock and there are

00:10:24,510 --> 00:10:33,630
several URLs where you can find more

00:10:26,940 --> 00:10:35,910
information on real-time Linux in order

00:10:33,630 --> 00:10:38,010
to not run into the issues the spin

00:10:35,910 --> 00:10:43,890
locks and I are accused things like that

00:10:38,010 --> 00:10:45,930
a lot of things have to change some of

00:10:43,890 --> 00:10:48,720
the features that are necessary you may

00:10:45,930 --> 00:10:51,810
already be using without knowing it for

00:10:48,720 --> 00:10:54,839
example of a deterministic real-time

00:10:51,810 --> 00:10:56,730
scheduler that always schedules the

00:10:54,839 --> 00:11:00,000
highest priority programs across the

00:10:56,730 --> 00:11:02,269
whole system plus all the cpus that's

00:11:00,000 --> 00:11:05,119
already up three

00:11:02,269 --> 00:11:08,399
Colonel preemption is already upstream

00:11:05,119 --> 00:11:10,319
priority inherence mutexes you're

00:11:08,399 --> 00:11:13,769
probably not using them but they're

00:11:10,319 --> 00:11:16,439
there the high resolution timer is

00:11:13,769 --> 00:11:20,549
something that is very useful for real

00:11:16,439 --> 00:11:25,259
time because it allows people to set a

00:11:20,549 --> 00:11:27,989
timer for any point in time and a CPU

00:11:25,259 --> 00:11:30,509
will fire the timer wake up your program

00:11:27,989 --> 00:11:34,679
whenever you want essentially an

00:11:30,509 --> 00:11:37,199
arbitrary time on granularity this has

00:11:34,679 --> 00:11:40,049
been upstream for years and everybody

00:11:37,199 --> 00:11:41,189
uses it for all kinds of things but it's

00:11:40,049 --> 00:11:46,589
one of those things that was developed

00:11:41,189 --> 00:11:50,429
for real-time originally pre-emptive

00:11:46,589 --> 00:11:54,209
recopy updates spinlock annotation no

00:11:50,429 --> 00:11:59,459
hurts fool mode things like that are all

00:11:54,209 --> 00:12:01,949
upstream a few people are using it but

00:11:59,459 --> 00:12:05,220
full real-time preemption still has a

00:12:01,949 --> 00:12:07,169
number of things left that really need

00:12:05,220 --> 00:12:13,579
some significant work to get upstream

00:12:07,169 --> 00:12:16,619
the goal of the last pieces of the

00:12:13,579 --> 00:12:19,499
real-time preemption code are to make

00:12:16,619 --> 00:12:23,239
every part to the next colonel either

00:12:19,499 --> 00:12:28,919
pre-emptive of very very short duration

00:12:23,239 --> 00:12:32,669
and this is necessary in order for the

00:12:28,919 --> 00:12:35,249
highest priority task on a cpu to

00:12:32,669 --> 00:12:37,980
preempt everything else not just lower

00:12:35,249 --> 00:12:41,989
priority tasks but also kernel code it

00:12:37,980 --> 00:12:47,569
is holding a spin lock or interrupt goat

00:12:41,989 --> 00:12:52,649
how do you do that let's take a look the

00:12:47,569 --> 00:12:58,139
preempt RT code it turns spin locks into

00:12:52,649 --> 00:13:02,639
priority inherited new texts which means

00:12:58,139 --> 00:13:04,559
that for every lock in the system not

00:13:02,639 --> 00:13:08,549
only can the task it is running the lock

00:13:04,559 --> 00:13:10,619
go to sleep and wake up again later we

00:13:08,549 --> 00:13:13,660
also keep track of who once the log and

00:13:10,619 --> 00:13:16,690
who currently has the lock

00:13:13,660 --> 00:13:20,030
it really increases the overhead of

00:13:16,690 --> 00:13:24,710
these locks which makes throughput of

00:13:20,030 --> 00:13:27,740
everything lower but it allows us to

00:13:24,710 --> 00:13:31,970
always interrupts preempts whatever

00:13:27,740 --> 00:13:33,530
program is running as a little bit of

00:13:31,970 --> 00:13:37,040
code left running with raw spin locks

00:13:33,530 --> 00:13:39,140
but not much at all with your priority

00:13:37,040 --> 00:13:46,160
inheritance I'll get to that again in

00:13:39,140 --> 00:13:50,210
the next slide and threats are used to

00:13:46,160 --> 00:13:52,600
run interrupts seems not not that big a

00:13:50,210 --> 00:13:55,370
deal but it means that if you have

00:13:52,600 --> 00:13:59,110
interrupts on the system it is less

00:13:55,370 --> 00:14:03,380
important than your real time program

00:13:59,110 --> 00:14:05,690
you can stop running the introduc out

00:14:03,380 --> 00:14:07,460
for a little bit and then you can go

00:14:05,690 --> 00:14:12,830
back to running the program if you want

00:14:07,460 --> 00:14:15,260
to run and interrupts can also be given

00:14:12,830 --> 00:14:20,240
priorities just like any other program

00:14:15,260 --> 00:14:22,850
that's running on the system the read

00:14:20,240 --> 00:14:26,780
copy updates a loculus synchronization

00:14:22,850 --> 00:14:29,600
it does not track CPUs in a real-time

00:14:26,780 --> 00:14:33,560
preempts colonel but it tracks tasks

00:14:29,600 --> 00:14:36,290
instead and it does not allow the system

00:14:33,560 --> 00:14:39,860
to move on to a new RC you grace period

00:14:36,290 --> 00:14:43,490
until all the tasks that were using our

00:14:39,860 --> 00:14:46,420
see you but it had an RCA read lock in

00:14:43,490 --> 00:14:49,490
an old grace period have moved on and

00:14:46,420 --> 00:14:51,260
with so many more things to track than

00:14:49,490 --> 00:14:57,940
just two cpus of course you over that

00:14:51,260 --> 00:15:01,160
dinner also increases significantly well

00:14:57,940 --> 00:15:04,490
real-time is pretty hard turns out the

00:15:01,160 --> 00:15:06,710
real time kbm is even harder and we had

00:15:04,490 --> 00:15:13,630
quite a bit of fun trying to make that

00:15:06,710 --> 00:15:16,970
work one of the issues that real-time

00:15:13,630 --> 00:15:19,490
preempts nicely solves is always running

00:15:16,970 --> 00:15:24,060
the highest priority program on the

00:15:19,490 --> 00:15:27,780
system however the host

00:15:24,060 --> 00:15:29,810
has no idea which of the guests is

00:15:27,780 --> 00:15:33,960
running the highest priority program

00:15:29,810 --> 00:15:37,200
it's not visible to the host the only

00:15:33,960 --> 00:15:40,830
thing we can really do is Mark all of

00:15:37,200 --> 00:15:45,510
the vcpu threats with real-time priority

00:15:40,830 --> 00:15:48,320
and we just have to treat them all as

00:15:45,510 --> 00:15:52,230
the highest priority thing in the system

00:15:48,320 --> 00:15:55,980
with maybe the exception of K soft i er

00:15:52,230 --> 00:16:01,170
QD so we can deliver interrupts to the

00:15:55,980 --> 00:16:03,600
CPUs colonel housekeeping tasks that

00:16:01,170 --> 00:16:07,830
might introduce one microsecond of

00:16:03,600 --> 00:16:10,350
latency on a reel on a bare metal

00:16:07,830 --> 00:16:13,730
realtime kernel those might introduce

00:16:10,350 --> 00:16:19,530
three or four microseconds of latency on

00:16:13,730 --> 00:16:22,290
avian the reason for that is that it is

00:16:19,530 --> 00:16:24,420
possible that both the host and the

00:16:22,290 --> 00:16:27,960
guest Colonel want to do this

00:16:24,420 --> 00:16:31,700
housekeeping thing at the same time for

00:16:27,960 --> 00:16:35,040
example a timer tick not only that but

00:16:31,700 --> 00:16:39,480
handling one of these things from a

00:16:35,040 --> 00:16:41,760
guest may require multiple exits from

00:16:39,480 --> 00:16:44,040
the guests into the host switching back

00:16:41,760 --> 00:16:46,770
into the guest do zooming out that goes

00:16:44,040 --> 00:16:51,180
back out into the host so I on the host

00:16:46,770 --> 00:16:53,580
you might deal with one operation having

00:16:51,180 --> 00:16:56,220
the same operation happen on both the

00:16:53,580 --> 00:16:57,360
host and the guest simultaneously you

00:16:56,220 --> 00:17:01,560
might look at half a dozen different

00:16:57,360 --> 00:17:04,500
things that all need to happen but there

00:17:01,560 --> 00:17:07,050
were certain things that did not pose

00:17:04,500 --> 00:17:09,840
any major issues to the bare metal real

00:17:07,050 --> 00:17:13,520
time people while we had to make Colonel

00:17:09,840 --> 00:17:19,460
changes in order for them to not be

00:17:13,520 --> 00:17:21,660
prohibitively latency introducing for us

00:17:19,460 --> 00:17:24,990
another thing that we cannot do is

00:17:21,660 --> 00:17:29,760
priority inheritance because the host

00:17:24,990 --> 00:17:33,870
Colonel cannot see which tasks are

00:17:29,760 --> 00:17:36,510
running inside the guest or what looks

00:17:33,870 --> 00:17:41,760
they might be holding inside against

00:17:36,510 --> 00:17:44,940
and inside the vcpu we have similar

00:17:41,760 --> 00:17:51,840
issues where task that is running may

00:17:44,940 --> 00:17:53,310
not be preempted were a number of

00:17:51,840 --> 00:17:55,220
changes that we had to make to the

00:17:53,310 --> 00:18:01,590
colonel in order to make real-time

00:17:55,220 --> 00:18:05,940
kayvyun work there was something in an

00:18:01,590 --> 00:18:08,940
overcoat that marked CP u SP in

00:18:05,940 --> 00:18:11,070
quiescent for our see you which means

00:18:08,940 --> 00:18:14,400
Sapa would not get folks to run

00:18:11,070 --> 00:18:17,100
callbacks or to move on to two other

00:18:14,400 --> 00:18:22,740
grace periods when running user space

00:18:17,100 --> 00:18:26,390
goat that code was not yet active when

00:18:22,740 --> 00:18:32,250
running in guest mode so we added that

00:18:26,390 --> 00:18:35,820
there is a parameter to disable sinking

00:18:32,250 --> 00:18:39,210
of the kvm clock with ntp changes on the

00:18:35,820 --> 00:18:42,000
host you have a larger guessed it can

00:18:39,210 --> 00:18:43,440
take significant amount of time where a

00:18:42,000 --> 00:18:47,270
significant amount of time is a few

00:18:43,440 --> 00:18:53,610
microseconds to adjust the kvm clock

00:18:47,270 --> 00:18:56,670
data on all three cpus if we disable

00:18:53,610 --> 00:18:59,610
that the guests will no longer get ntp

00:18:56,670 --> 00:19:02,310
corrections you may have to run MTP

00:18:59,610 --> 00:19:06,300
inside the guest this is something you

00:19:02,310 --> 00:19:12,030
care about but it does remove the sort

00:19:06,300 --> 00:19:14,160
of latency if you're running a real time

00:19:12,030 --> 00:19:18,030
task and there is another task runnable

00:19:14,160 --> 00:19:20,160
on the cpu the no hurts full code did

00:19:18,030 --> 00:19:21,660
not disable the timer tick it would only

00:19:20,160 --> 00:19:24,570
disable the timer tick if there were

00:19:21,660 --> 00:19:26,280
just one program running but if we never

00:19:24,570 --> 00:19:30,690
want to run at other program because

00:19:26,280 --> 00:19:33,390
it's really low priority then we should

00:19:30,690 --> 00:19:35,100
disable the scheduler take anyway just

00:19:33,390 --> 00:19:39,720
having that on both the hosts and guests

00:19:35,100 --> 00:19:42,510
introduces to mush Ravens we have a

00:19:39,720 --> 00:19:45,600
parameter to make the timer for a guest

00:19:42,510 --> 00:19:48,880
fired a little earlier to take a wake-up

00:19:45,600 --> 00:19:50,920
latency upper guests into account

00:19:48,880 --> 00:19:53,260
china's various little enhancements to

00:19:50,920 --> 00:19:55,870
the ISIL CPUs code in the work queue

00:19:53,260 --> 00:19:58,480
code just to move more to Colonel

00:19:55,870 --> 00:20:03,840
housekeeping tasks away from the

00:19:58,480 --> 00:20:03,840
real-time CPUs and to the system CPUs a

00:20:04,440 --> 00:20:08,620
priority inversion of priority

00:20:06,550 --> 00:20:13,260
inheritance is another issue that we

00:20:08,620 --> 00:20:17,500
cannot do in a virtualized environment

00:20:13,260 --> 00:20:19,630
because the host does not know what

00:20:17,500 --> 00:20:25,420
looks are being held by tasks in

00:20:19,630 --> 00:20:30,220
different be cpus and vcpu threat

00:20:25,420 --> 00:20:33,100
running a very high priority program it

00:20:30,220 --> 00:20:34,990
may need to take a look that is helped

00:20:33,100 --> 00:20:41,080
by a PCP you running a much lower

00:20:34,990 --> 00:20:48,640
priority program but if the hosts does

00:20:41,080 --> 00:20:52,240
not know that it's just nothing we can

00:20:48,640 --> 00:20:56,220
do to help one program release the lock

00:20:52,240 --> 00:20:59,610
that higher priority program needs and

00:20:56,220 --> 00:21:03,730
the vcp use always have the same

00:20:59,610 --> 00:21:06,460
priority we have no idea whether the CPU

00:21:03,730 --> 00:21:09,340
is running something important or even

00:21:06,460 --> 00:21:16,890
if the PCP you is idle if you're running

00:21:09,340 --> 00:21:19,060
with idle is full and that means if

00:21:16,890 --> 00:21:22,450
you're looking at a particular physical

00:21:19,060 --> 00:21:26,170
CPU on the host that has vcpu thread on

00:21:22,450 --> 00:21:29,350
it having a higher priority thing on the

00:21:26,170 --> 00:21:32,610
host on that same CPU is usually

00:21:29,350 --> 00:21:37,960
unacceptable because it will interfere

00:21:32,610 --> 00:21:40,630
with the latency of our ECP use but

00:21:37,960 --> 00:21:44,350
having a lower priority thing on the

00:21:40,630 --> 00:21:47,170
host the same physical CPUs vcpu could

00:21:44,350 --> 00:21:49,960
lead to a system deadlock because

00:21:47,170 --> 00:21:54,880
something that the system needs to have

00:21:49,960 --> 00:21:57,790
done on that cpu might never run the

00:21:54,880 --> 00:22:00,820
vcpu thread to take a hundred percent of

00:21:57,790 --> 00:22:02,800
the time and it might take days or

00:22:00,820 --> 00:22:11,110
months before something

00:22:02,800 --> 00:22:16,210
priority on that cpu ever gets run so we

00:22:11,110 --> 00:22:19,630
really cannot share a CPU that has a

00:22:16,210 --> 00:22:24,060
real-time vcpu with anything else

00:22:19,630 --> 00:22:26,830
running on a same GPU we have to

00:22:24,060 --> 00:22:29,740
separate out the parts of the system

00:22:26,830 --> 00:22:35,410
that run out that run real-time things

00:22:29,740 --> 00:22:38,800
and I run something else it looks

00:22:35,410 --> 00:22:41,520
something like this we're in this case

00:22:38,800 --> 00:22:46,270
the system has been split into three

00:22:41,520 --> 00:22:50,350
partitions we have system tasks that run

00:22:46,270 --> 00:22:53,430
everything else and we have to real-time

00:22:50,350 --> 00:23:01,350
guests which each get half off the

00:22:53,430 --> 00:23:03,960
real-time CPUs in the system and

00:23:01,350 --> 00:23:07,270
everything else that is not real time

00:23:03,960 --> 00:23:11,230
does not get to run on the real-time

00:23:07,270 --> 00:23:18,250
CPUs it is all wrong with the system

00:23:11,230 --> 00:23:20,620
tasks seems like a good solution but

00:23:18,250 --> 00:23:25,030
it's not enough we need to also do

00:23:20,620 --> 00:23:28,900
partitioning inside the guest because

00:23:25,030 --> 00:23:32,050
inside the guests we may run into a

00:23:28,900 --> 00:23:38,980
program that rights to the emulated

00:23:32,050 --> 00:23:42,520
graphics card at that point qm you will

00:23:38,980 --> 00:23:45,490
trap not just to the host but in the

00:23:42,520 --> 00:23:47,110
host Colonel we will see oh we have a

00:23:45,490 --> 00:23:51,070
program that is trying to do something

00:23:47,110 --> 00:23:55,230
that is not emulated in the colonel we

00:23:51,070 --> 00:23:57,700
have to run the key mu emulator thread

00:23:55,230 --> 00:24:03,070
so we go all the way from the guests

00:23:57,700 --> 00:24:05,680
into the host into qmu inside emu we may

00:24:03,070 --> 00:24:07,990
have to wait for something else to

00:24:05,680 --> 00:24:13,090
finish doing whatever it was doing and

00:24:07,990 --> 00:24:16,690
release a look and they can take it

00:24:13,090 --> 00:24:19,630
really long time at that point if you

00:24:16,690 --> 00:24:22,450
program with very low priority is not

00:24:19,630 --> 00:24:26,920
real time trying to write to the

00:24:22,450 --> 00:24:32,700
graphics card inside against at that

00:24:26,920 --> 00:24:35,680
point that guest vcpu is not running

00:24:32,700 --> 00:24:41,350
it's waiting for the host and it is

00:24:35,680 --> 00:24:43,780
waiting for qmu to do some and we are

00:24:41,350 --> 00:24:46,780
unable to switch from that low priority

00:24:43,780 --> 00:24:52,030
program to a higher priority program

00:24:46,780 --> 00:24:55,630
until Q mu is done so the guests also

00:24:52,030 --> 00:25:00,640
needs to be partitioned it looks very

00:24:55,630 --> 00:25:04,090
similar to populate for the host we blew

00:25:00,640 --> 00:25:08,280
the guests with ISIL CPUs the real time

00:25:04,090 --> 00:25:12,180
tasks run on the real-time CPUs and

00:25:08,280 --> 00:25:15,250
nothing else runs on those CPUs

00:25:12,180 --> 00:25:20,410
everything else is running on the system

00:25:15,250 --> 00:25:24,070
p cpus and this way things that might

00:25:20,410 --> 00:25:26,860
trap to Q mu or that might do something

00:25:24,070 --> 00:25:29,020
that needs annulation in the host or

00:25:26,860 --> 00:25:32,650
does anything else that takes a really

00:25:29,020 --> 00:25:35,770
long time it will not preempt the guests

00:25:32,650 --> 00:25:39,700
or it will not prevent the guests from

00:25:35,770 --> 00:25:41,800
switching to a real time task on the

00:25:39,700 --> 00:25:48,010
CPUs what we want to run in real time

00:25:41,800 --> 00:25:51,690
task dedicated resources are something

00:25:48,010 --> 00:25:58,060
that we can probably get away with today

00:25:51,690 --> 00:25:59,980
that you can buy CPU it is 18 course you

00:25:58,060 --> 00:26:04,770
can buy a system that is multiple of

00:25:59,980 --> 00:26:07,450
those CPUs on one board so it's

00:26:04,770 --> 00:26:12,180
relatively affordable to buy a system

00:26:07,450 --> 00:26:15,490
that has dozens of CPU cores and

00:26:12,180 --> 00:26:18,700
real-time applications are relatively

00:26:15,490 --> 00:26:20,740
specialist so it is something where the

00:26:18,700 --> 00:26:24,310
people who really wants to guarantee

00:26:20,740 --> 00:26:29,140
that their programs always respond

00:26:24,310 --> 00:26:30,550
within say 50 microseconds that they're

00:26:29,140 --> 00:26:35,650
willing to throw a little bit of

00:26:30,550 --> 00:26:38,350
at the problem and by systems that just

00:26:35,650 --> 00:26:42,190
have CPUs dedicated to running real time

00:26:38,350 --> 00:26:44,410
tasks the numbers that we found in the

00:26:42,190 --> 00:26:46,780
latest test by Louise actually look

00:26:44,410 --> 00:26:50,470
better than I have on my slides I think

00:26:46,780 --> 00:26:54,810
in 24 hour tests he is now down to a

00:26:50,470 --> 00:26:57,430
maximum latency of 13 microseconds and

00:26:54,810 --> 00:27:00,700
this is where the test called cyclic

00:26:57,430 --> 00:27:06,190
tests with a stressful running in the

00:27:00,700 --> 00:27:10,630
background this is a slide that I took

00:27:06,190 --> 00:27:16,180
from Aaron and open off you somehow did

00:27:10,630 --> 00:27:19,030
not do the the text on the x-axis

00:27:16,180 --> 00:27:21,670
corrects after I moved it over so the

00:27:19,030 --> 00:27:26,590
middle graph everywhere is the bare

00:27:21,670 --> 00:27:29,710
metal realtime kernel on rel 72 the

00:27:26,590 --> 00:27:32,620
slide on the left has the maximum Layton

00:27:29,710 --> 00:27:34,960
sees in there and we actually have to

00:27:32,620 --> 00:27:42,330
remove the maximum latency from the

00:27:34,960 --> 00:27:47,680
graph to see how bare metal rail 7 and

00:27:42,330 --> 00:27:49,270
real-time rail seven compared to kvm you

00:27:47,680 --> 00:27:52,510
can see that the average and minimum

00:27:49,270 --> 00:27:57,820
Layton sees on kvm are significantly

00:27:52,510 --> 00:28:04,210
higher than on environmental almost a

00:27:57,820 --> 00:28:06,430
factor three but virtualization has some

00:28:04,210 --> 00:28:08,080
advantages for certain people so they're

00:28:06,430 --> 00:28:12,520
willing to live with a slightly higher

00:28:08,080 --> 00:28:14,830
latency and what really matters for most

00:28:12,520 --> 00:28:17,530
real time customers is that the maximum

00:28:14,830 --> 00:28:23,410
latency is really low and we guarantee

00:28:17,530 --> 00:28:25,660
that with a real-time kvm stuff there

00:28:23,410 --> 00:28:29,590
are many many things that he can do to

00:28:25,660 --> 00:28:32,680
break real time and for every single one

00:28:29,590 --> 00:28:38,860
of them the advice boils down to don't

00:28:32,680 --> 00:28:41,470
do that and real time is not a thing

00:28:38,860 --> 00:28:42,890
that you can magically achieve with

00:28:41,470 --> 00:28:46,220
better software

00:28:42,890 --> 00:28:49,400
you need a lot of software changes to

00:28:46,220 --> 00:28:52,190
make real-time happen but you also need

00:28:49,400 --> 00:28:56,570
to set up your system in a very careful

00:28:52,190 --> 00:29:00,790
way and the system configuration has to

00:28:56,570 --> 00:29:03,890
be just right for real time to work

00:29:00,790 --> 00:29:06,530
things like CPU frequency changes and

00:29:03,890 --> 00:29:10,490
hot blog loading and unloading kernel

00:29:06,530 --> 00:29:13,160
modules moving a task a real-time desk

00:29:10,490 --> 00:29:17,390
from an isolated CPU to a non isolated

00:29:13,160 --> 00:29:21,920
CPU or the other way around changing

00:29:17,390 --> 00:29:23,870
your clock source running a system with

00:29:21,920 --> 00:29:25,280
real-time tasks that does not have

00:29:23,870 --> 00:29:28,970
enough memory to keep your real-time

00:29:25,280 --> 00:29:32,660
tasks from swapping basic things like

00:29:28,970 --> 00:29:38,240
that they all ruin your real-time

00:29:32,660 --> 00:29:41,540
performance and you want to be really

00:29:38,240 --> 00:29:45,040
careful when configuring a system to do

00:29:41,540 --> 00:29:49,100
real time you want to make sure that

00:29:45,040 --> 00:29:51,800
your devices are always fast enough to

00:29:49,100 --> 00:29:52,910
do everything that you want to do you

00:29:51,800 --> 00:29:55,190
want to make sure that you have enough

00:29:52,910 --> 00:30:03,020
cpu available to handle all your

00:29:55,190 --> 00:30:07,190
requests if it takes on average 20 micro

00:30:03,020 --> 00:30:12,020
seconds to handle a request and you

00:30:07,190 --> 00:30:15,320
might get 10 requests simultaneously and

00:30:12,020 --> 00:30:17,120
your deadline is 50 microseconds you're

00:30:15,320 --> 00:30:21,470
going to need several cpus to handle

00:30:17,120 --> 00:30:24,590
them because one cpu will take 200

00:30:21,470 --> 00:30:27,920
microseconds to process those 10

00:30:24,590 --> 00:30:31,850
requests and simply by not having

00:30:27,920 --> 00:30:34,130
adequate resources available you might

00:30:31,850 --> 00:30:36,410
not be able to handle a large enough

00:30:34,130 --> 00:30:42,230
number of requests so real-time systems

00:30:36,410 --> 00:30:45,650
always have to be oversized if too many

00:30:42,230 --> 00:30:47,960
requests come in at a time you will not

00:30:45,650 --> 00:30:50,120
meet your deadline so you have to know

00:30:47,960 --> 00:30:53,120
what your workload looks like you have

00:30:50,120 --> 00:30:56,050
to know i can get at most i can get at

00:30:53,120 --> 00:31:02,110
most five requests in at a time

00:30:56,050 --> 00:31:06,640
and my deadline is maybe twice the

00:31:02,110 --> 00:31:09,720
average so I need to have at least three

00:31:06,640 --> 00:31:12,760
or four cpus available to guarantee

00:31:09,720 --> 00:31:15,220
always make my deadline when I have a

00:31:12,760 --> 00:31:20,620
large number of requests coming in the

00:31:15,220 --> 00:31:24,480
same time intel has some new technology

00:31:20,620 --> 00:31:29,490
out there a new cpus to help with this

00:31:24,480 --> 00:31:37,000
we have a system out there that has a

00:31:29,490 --> 00:31:40,020
single chip with 12 or 18 or 30 who

00:31:37,000 --> 00:31:44,710
knows how many cpu cores out there

00:31:40,020 --> 00:31:47,950
currently it is possible that one cpu

00:31:44,710 --> 00:31:50,230
core is running some program that causes

00:31:47,950 --> 00:31:55,660
did they attended all the other programs

00:31:50,230 --> 00:31:59,560
have to be evicted from CPU cache it can

00:31:55,660 --> 00:32:02,080
break real-time response times because

00:31:59,560 --> 00:32:06,990
there really is only so much stuff you

00:32:02,080 --> 00:32:06,990
can access in RAM in a few microseconds

00:32:07,350 --> 00:32:12,610
you can access a number of things

00:32:09,490 --> 00:32:15,010
because drm lathan things are still

00:32:12,610 --> 00:32:20,830
pretty low we're on the order of 60 90

00:32:15,010 --> 00:32:24,460
seconds but if you were to access a few

00:32:20,830 --> 00:32:27,190
hundred different things from drm you

00:32:24,460 --> 00:32:31,150
might no longer be able to make your

00:32:27,190 --> 00:32:34,960
deadline it's easy to add up all those

00:32:31,150 --> 00:32:38,740
Layton sees to something exceeding 20 or

00:32:34,960 --> 00:32:44,140
30 50 even nanoseconds whatever your

00:32:38,740 --> 00:32:48,150
deadline is so we're at the point now

00:32:44,140 --> 00:32:52,060
where low latency real time actually

00:32:48,150 --> 00:32:57,330
depends on proton data being in the CPU

00:32:52,060 --> 00:32:59,650
cache if your data is not any CPU cache

00:32:57,330 --> 00:33:02,740
you might be unable to meet your

00:32:59,650 --> 00:33:06,780
deadline but with cash allocation

00:33:02,740 --> 00:33:08,690
technology you can set quotas for

00:33:06,780 --> 00:33:13,820
different programs

00:33:08,690 --> 00:33:18,050
and the CPU will make sure that programs

00:33:13,820 --> 00:33:22,640
do not have too much data in the level 3

00:33:18,050 --> 00:33:25,610
CPU cache and that in turn make sure

00:33:22,640 --> 00:33:27,530
that the other programs do not have all

00:33:25,610 --> 00:33:29,810
of their debts and evicted from the cash

00:33:27,530 --> 00:33:32,780
just because somebody is running a

00:33:29,810 --> 00:33:35,150
program that that streams through a few

00:33:32,780 --> 00:33:40,220
hundred gigabytes of memory looking for

00:33:35,150 --> 00:33:43,900
something and this really helps improve

00:33:40,220 --> 00:33:47,030
the current e3 of low latencies on

00:33:43,900 --> 00:33:53,420
systems where you have many CPU cores on

00:33:47,030 --> 00:33:55,700
a single chip one of the major

00:33:53,420 --> 00:33:58,040
conclusion of this talk is real-time kvm

00:33:55,700 --> 00:34:00,430
is actually possible this surprises me

00:33:58,040 --> 00:34:04,130
as much as it might surprise some of you

00:34:00,430 --> 00:34:07,280
but we managed to get it to work but it

00:34:04,130 --> 00:34:10,760
is largely achieved through system

00:34:07,280 --> 00:34:13,750
partitioning and overcome it's like you

00:34:10,760 --> 00:34:17,780
will do in many other virtualization

00:34:13,750 --> 00:34:22,130
situations is simply not an option for

00:34:17,780 --> 00:34:25,070
real time for real time you want

00:34:22,130 --> 00:34:29,300
guarantees you want to make sure you can

00:34:25,070 --> 00:34:31,640
always do what you have to do on time so

00:34:29,300 --> 00:34:37,070
your system has to be larger and you

00:34:31,640 --> 00:34:38,990
cannot use all the resources but within

00:34:37,070 --> 00:34:44,840
these constraints we can actually get

00:34:38,990 --> 00:34:46,280
really low latencies and well real-time

00:34:44,840 --> 00:34:48,400
applications need to be written

00:34:46,280 --> 00:34:51,140
carefully because if your real-time

00:34:48,400 --> 00:34:53,750
application does something that is

00:34:51,140 --> 00:34:57,950
really slow it's not going to be a fast

00:34:53,750 --> 00:35:01,640
program and this includes all kinds of

00:34:57,950 --> 00:35:05,050
six goals that may be slow there's no

00:35:01,640 --> 00:35:05,050
different from bare metal real-time

00:35:05,080 --> 00:35:10,460
virtualization I think has a place in

00:35:08,090 --> 00:35:16,700
real time because it really helps with

00:35:10,460 --> 00:35:20,440
system isolation which can help

00:35:16,700 --> 00:35:22,460
customers do things deploy a real-time

00:35:20,440 --> 00:35:25,010
virtual machine

00:35:22,460 --> 00:35:29,480
day and when they upgrade their hardware

00:35:25,010 --> 00:35:33,740
in five years take care continue rolling

00:35:29,480 --> 00:35:37,280
out the same real time guests with the

00:35:33,740 --> 00:35:39,920
same operating system and the same

00:35:37,280 --> 00:35:42,410
virtual hardware running on top of new

00:35:39,920 --> 00:35:44,869
physical hardware and this way they can

00:35:42,410 --> 00:35:47,119
take advantages of improvements in

00:35:44,869 --> 00:35:52,760
hardware without having to re-qualify

00:35:47,119 --> 00:35:54,410
the whole software stack and well it

00:35:52,760 --> 00:35:57,830
takes a lot of very careful

00:35:54,410 --> 00:35:59,540
configuration which Young's presentation

00:35:57,830 --> 00:36:01,520
will go more into because I don't really

00:35:59,540 --> 00:36:03,859
have the time to go through the entire

00:36:01,520 --> 00:36:08,510
stack and we've got a few minutes for

00:36:03,859 --> 00:36:09,619
questions you have any oh and you can

00:36:08,510 --> 00:36:13,510
ask your questions at one of the

00:36:09,619 --> 00:36:13,510
microphones in the middle of the room

00:36:19,670 --> 00:36:29,660
I arm so they prefer you use the iso

00:36:25,430 --> 00:36:33,619
CPUs what you know to have a partitions

00:36:29,660 --> 00:36:35,359
the other way is use you know that has

00:36:33,619 --> 00:36:37,730
some disadvantage right because it's

00:36:35,359 --> 00:36:40,430
starting and that you need to reboot to

00:36:37,730 --> 00:36:43,790
change that the partition the other way

00:36:40,430 --> 00:36:47,030
is to use a secretive cpu set you can do

00:36:43,790 --> 00:36:49,460
that at runtime and I've actually played

00:36:47,030 --> 00:36:53,720
with the skin compared to associate view

00:36:49,460 --> 00:36:56,359
versus you know CPUs nonsense series 60

00:36:53,720 --> 00:36:59,780
you set the only difference we found so

00:36:56,359 --> 00:37:06,309
far as the wrecker HR time Heydrich

00:36:59,780 --> 00:37:10,220
resource of time and the my question is

00:37:06,309 --> 00:37:14,240
do you know use this do you still see

00:37:10,220 --> 00:37:17,180
advantage of all I so CPU or do you

00:37:14,240 --> 00:37:20,869
think we can you know use those see

00:37:17,180 --> 00:37:24,650
groups if you sell in the long run I

00:37:20,869 --> 00:37:27,859
hope we can use cpu sets in see groups

00:37:24,650 --> 00:37:32,240
but right now there are a certain number

00:37:27,859 --> 00:37:37,549
of things in the kernel that are removed

00:37:32,240 --> 00:37:40,099
from ISIL CPUs but if you use a cpu sets

00:37:37,549 --> 00:37:42,710
they still run and it includes things

00:37:40,099 --> 00:37:44,660
like unbound work use and other

00:37:42,710 --> 00:37:48,079
housekeeping tasks in the kernel and

00:37:44,660 --> 00:37:52,819
right now I soul CPUs is the only thing

00:37:48,079 --> 00:37:55,339
that removes those things from from the

00:37:52,819 --> 00:37:57,500
from the real-time CPUs I would like to

00:37:55,339 --> 00:37:58,940
change that in a long run but it's just

00:37:57,500 --> 00:38:01,940
not something that we have gotten around

00:37:58,940 --> 00:38:04,010
to okay one of our honor we found was

00:38:01,940 --> 00:38:10,730
just an offer I'm process of death

00:38:04,010 --> 00:38:14,230
online do you think it works or not it's

00:38:10,730 --> 00:38:14,230
good question okay

00:38:17,320 --> 00:38:30,350
okay get here Eric so you know what I

00:38:28,010 --> 00:38:33,260
just have a few questions or actually

00:38:30,350 --> 00:38:35,810
few comments so the first one is that

00:38:33,260 --> 00:38:39,470
one of the biggest problems we have is

00:38:35,810 --> 00:38:42,080
driving enough timers very carnal so

00:38:39,470 --> 00:38:45,230
like for LTE applications especially

00:38:42,080 --> 00:38:48,340
within the core you have to drive like

00:38:45,230 --> 00:38:52,940
500 600 800 timers into the kernel and

00:38:48,340 --> 00:38:56,150
so when you do that you get about twenty

00:38:52,940 --> 00:38:57,860
percent CPU overhead and you know just

00:38:56,150 --> 00:39:03,350
wondering if you guys been focusing on

00:38:57,860 --> 00:39:05,390
there and then the second issue is you

00:39:03,350 --> 00:39:07,580
know as far as partitioning so we need

00:39:05,390 --> 00:39:09,140
data plane partitioning control plane

00:39:07,580 --> 00:39:17,480
partitioning and also timeshare

00:39:09,140 --> 00:39:23,840
partitioning with and against I think

00:39:17,480 --> 00:39:26,000
for the things that we focus on we're

00:39:23,840 --> 00:39:27,920
not really sure at this point what work

00:39:26,000 --> 00:39:32,119
both people are going to use this for

00:39:27,920 --> 00:39:34,460
and I think the things that we end up

00:39:32,119 --> 00:39:36,170
focusing on next will change when people

00:39:34,460 --> 00:39:38,480
actually start running applications on

00:39:36,170 --> 00:39:42,140
this we will no doubt discover new

00:39:38,480 --> 00:39:44,660
bottlenecks and besides the various

00:39:42,140 --> 00:39:49,220
system things that we have discovered we

00:39:44,660 --> 00:39:54,710
have moved out of the way and your

00:39:49,220 --> 00:39:56,990
partitioning wise you have little choice

00:39:54,710 --> 00:40:00,500
but to make sure that nothing gets in

00:39:56,990 --> 00:40:02,450
the way of your real-time desk but

00:40:00,500 --> 00:40:06,230
inside the guests may be possible to

00:40:02,450 --> 00:40:08,150
squeeze things a little more if you have

00:40:06,230 --> 00:40:11,119
lower priority in higher priority

00:40:08,150 --> 00:40:13,609
programs and you know some of the lower

00:40:11,119 --> 00:40:18,050
priority programs I'm never going to run

00:40:13,609 --> 00:40:20,990
anything that traps to QMI you in the

00:40:18,050 --> 00:40:23,119
host it may be possible to be a little

00:40:20,990 --> 00:40:25,640
less strict with the partitioning inside

00:40:23,119 --> 00:40:27,220
the guest enough if that's what you're

00:40:25,640 --> 00:40:29,450
looking for

00:40:27,220 --> 00:40:31,970
yeah I guess you know what I notice is

00:40:29,450 --> 00:40:34,640
that if you run cichlid test you know

00:40:31,970 --> 00:40:37,609
it'll give you so inside you get kind of

00:40:34,640 --> 00:40:40,339
pretty good you know Layton sees and

00:40:37,609 --> 00:40:43,549
stuff like that but then on the host I'm

00:40:40,339 --> 00:40:45,650
UK like twenty percent overhead yeah

00:40:43,549 --> 00:40:47,240
there is and I think you know that might

00:40:45,650 --> 00:40:48,710
be due to the timer injection or

00:40:47,240 --> 00:40:52,279
something like that could be the timer

00:40:48,710 --> 00:40:54,520
code injection doing that well yeah

00:40:52,279 --> 00:40:57,980
there is significant overhead to running

00:40:54,520 --> 00:41:00,710
real time and you're having it both in

00:40:57,980 --> 00:41:05,450
the host in the guests it definitely

00:41:00,710 --> 00:41:09,140
adds up but that's something where I

00:41:05,450 --> 00:41:10,670
think in the long run we can help we can

00:41:09,140 --> 00:41:14,180
work on improving some of those things

00:41:10,670 --> 00:41:17,299
by making some of the code that I real

00:41:14,180 --> 00:41:18,950
hot pass a loculus for example we can

00:41:17,299 --> 00:41:21,440
get rid of some of the locking overhead

00:41:18,950 --> 00:41:23,210
that the real time code introduced is

00:41:21,440 --> 00:41:27,200
currently but that's really a

00:41:23,210 --> 00:41:29,569
longer-term projects because they really

00:41:27,200 --> 00:41:32,109
want to want to get it to work first all

00:41:29,569 --> 00:41:32,109
right Texas

00:41:50,540 --> 00:41:59,390
I just one quick question regarding the

00:41:56,210 --> 00:42:02,000
lock priority inheritance you said you

00:41:59,390 --> 00:42:04,720
are not able to to get in the hypervisor

00:42:02,000 --> 00:42:07,810
information which written correctly

00:42:04,720 --> 00:42:10,340
characteristics the special thread head

00:42:07,810 --> 00:42:13,760
do you think there is some way of para

00:42:10,340 --> 00:42:16,850
virtualized enters um that's a good

00:42:13,760 --> 00:42:20,560
question in theory it would be possible

00:42:16,850 --> 00:42:23,690
to para virtualized priority inheritance

00:42:20,560 --> 00:42:25,790
by telling the host of the relative

00:42:23,690 --> 00:42:31,490
priorities of everything going on inside

00:42:25,790 --> 00:42:34,520
each guest but I think in practice that

00:42:31,490 --> 00:42:37,010
might introduce so much overhead that we

00:42:34,520 --> 00:42:42,160
will no longer meet the low latency real

00:42:37,010 --> 00:42:42,160
time deadlines first

00:42:49,410 --> 00:42:55,320
hey one last question and then I will

00:42:51,330 --> 00:42:57,480
hand it over to young with regard to the

00:42:55,320 --> 00:43:00,330
CPU isolation do you have any guidance

00:42:57,480 --> 00:43:02,430
on how many CPUs are needed for the host

00:43:00,330 --> 00:43:04,620
and the guest to effectively do their

00:43:02,430 --> 00:43:08,310
work how many can we take away from each

00:43:04,620 --> 00:43:11,840
of them driving on a minimum you will

00:43:08,310 --> 00:43:15,000
want to have one cpu core / sockets

00:43:11,840 --> 00:43:16,620
persistent tasks on the host but it also

00:43:15,000 --> 00:43:19,260
depends on what you are running

00:43:16,620 --> 00:43:21,030
specifics of your workload and if they

00:43:19,260 --> 00:43:25,770
don't let many things going on on the

00:43:21,030 --> 00:43:27,240
host you might need more guests and the

00:43:25,770 --> 00:43:31,610
guest it also depends on what you're

00:43:27,240 --> 00:43:35,130
running one or two CPUs might be enough

00:43:31,610 --> 00:43:46,140
but it yeah it's again very workload

00:43:35,130 --> 00:43:49,790
specific unfortunately yeah we're right

00:43:46,140 --> 00:43:49,790

YouTube URL: https://www.youtube.com/watch?v=cZ5aTHeDLDE


