Title: [2020] Comparing Performance of NVMe HD in KVM, Baremetal and Docker Using Fio and SPDK for VTA
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	Full name: Comparing Performance of NVMe Hard Drives in KVM, Baremetal, and Docker Using Fio and SPDK for Virtual Testbed Applications

As it is known, the highest performance using a NVMe hard drive in a KVM guest is achievable using vfio-pci passthrough. Docker also allows PCI devices to be passed through. A lot of work has been done comparing some combination of drives in some combination of those platforms using some industry standard methods and parameters. But, how do they scale up when we want to add multiple drives per guest/container vs multiple guests/containers with one drive each? How about fine tuning memory/numa/iommu as we might also be passing out other CPI devices? And, and we want to build, test, and collect data in a reproducible way? Some ansible and shell scripting involved.

---

Mauricio Tavares
RENCI, Creator of shiny thingies

Mauricio Tavares (BS Aerospace Engineering) has worked with small and large companies in education, finance, and medical fields building and protecting user data. Currently a researcher at RENCI involved in next generation network research and an instructor with the Chameleon experimental research platform, he has given talks and workshops at ISSA InfoSecCon, Southeast Linux Fest, and IEEE SoutheastCon.
Captions: 
	00:00:00,320 --> 00:00:03,600
hello there today i will be talking

00:00:02,960 --> 00:00:05,680
about

00:00:03,600 --> 00:00:07,040
hard drive performance testing on bare

00:00:05,680 --> 00:00:10,719
metal docker

00:00:07,040 --> 00:00:13,759
and kvm however this talk

00:00:10,719 --> 00:00:14,559
will not be about comparing say two

00:00:13,759 --> 00:00:17,199
drives

00:00:14,559 --> 00:00:19,840
run in some way or form on these

00:00:17,199 --> 00:00:19,840
platforms

00:00:20,880 --> 00:00:25,359
what i am interested on is scaling

00:00:23,519 --> 00:00:27,039
things up

00:00:25,359 --> 00:00:28,480
what happens when you add multiple

00:00:27,039 --> 00:00:31,439
drives per guest

00:00:28,480 --> 00:00:32,559
or container versus multiple guests in

00:00:31,439 --> 00:00:35,680
containers

00:00:32,559 --> 00:00:39,520
with one drive each

00:00:35,680 --> 00:00:43,520
how to investigate which tweaks we need

00:00:39,520 --> 00:00:43,520
to get the best performance

00:00:44,000 --> 00:00:50,719
that will require a lot of testing

00:00:47,760 --> 00:00:51,600
i don't know about you but i don't feel

00:00:50,719 --> 00:00:55,280
like building

00:00:51,600 --> 00:00:56,960
a task test box manually running out the

00:00:55,280 --> 00:01:01,039
tests

00:00:56,960 --> 00:01:03,600
collecting the data just so

00:01:01,039 --> 00:01:04,720
fine just so after all that i find out

00:01:03,600 --> 00:01:07,280
that i forgot

00:01:04,720 --> 00:01:09,520
something and need to start all over

00:01:07,280 --> 00:01:11,680
again

00:01:09,520 --> 00:01:13,200
i have done that and it's not a pretty

00:01:11,680 --> 00:01:17,119
sight

00:01:13,200 --> 00:01:21,280
there has to be a better way so

00:01:17,119 --> 00:01:21,280
how can i be lazy in my testing

00:01:23,680 --> 00:01:28,320
let's begin by defining the problem we

00:01:26,240 --> 00:01:31,520
are trying to solve

00:01:28,320 --> 00:01:34,799
in this case how to do a ton of hard

00:01:31,520 --> 00:01:34,799
drive performance tests

00:01:35,600 --> 00:01:39,520
knowing which tests we want to run is

00:01:38,079 --> 00:01:42,320
important

00:01:39,520 --> 00:01:43,520
there is probably some standardized

00:01:42,320 --> 00:01:46,159
standard

00:01:43,520 --> 00:01:46,159
we can use

00:01:47,759 --> 00:01:54,240
spending the time to find a way not

00:01:51,200 --> 00:01:55,520
to only how to make this testing as much

00:01:54,240 --> 00:01:59,119
as possible

00:01:55,520 --> 00:02:03,280
but also make it become rather

00:01:59,119 --> 00:02:07,360
platform agnostic will go a long way

00:02:03,280 --> 00:02:10,319
in helping us save time

00:02:07,360 --> 00:02:12,239
we should be focusing on the data

00:02:10,319 --> 00:02:15,520
interpretation

00:02:12,239 --> 00:02:19,360
and coming up with questions

00:02:15,520 --> 00:02:23,840
to be answered not on the actual

00:02:19,360 --> 00:02:23,840
running of the experiment

00:02:27,760 --> 00:02:33,280
by then i hope

00:02:30,959 --> 00:02:35,120
we will have found some kind of

00:02:33,280 --> 00:02:38,720
conclusion

00:02:35,120 --> 00:02:42,640
we still have time to make something up

00:02:38,720 --> 00:02:43,840
and finally we will use the remaining

00:02:42,640 --> 00:02:47,840
time

00:02:43,840 --> 00:02:47,840
to go over any questions you may have

00:02:49,120 --> 00:02:55,120
warning my employee has no idea

00:02:52,239 --> 00:02:57,360
that i'm talking here really don't blame

00:02:55,120 --> 00:02:57,360
them

00:02:58,879 --> 00:03:04,080
this presentation is not as technical as

00:03:02,159 --> 00:03:07,599
most of the other talks

00:03:04,080 --> 00:03:11,920
we'll be focusing on the process

00:03:07,599 --> 00:03:15,920
not on the or not on the coding

00:03:11,920 --> 00:03:15,920
but there will be lots of pretty

00:03:16,840 --> 00:03:20,640
pictures

00:03:18,159 --> 00:03:21,280
over here we have researchers who need

00:03:20,640 --> 00:03:24,080
to get

00:03:21,280 --> 00:03:26,959
as close to the wire speed on devices

00:03:24,080 --> 00:03:29,599
they are working with

00:03:26,959 --> 00:03:29,599
gpus

00:03:30,640 --> 00:03:33,840
hard drives

00:03:34,400 --> 00:03:42,080
and network cards as possible

00:03:38,400 --> 00:03:46,319
the as close 2 is

00:03:42,080 --> 00:03:49,200
the name of the game ideally would

00:03:46,319 --> 00:03:50,879
provide each researcher with enough

00:03:49,200 --> 00:03:54,560
better metal servers

00:03:50,879 --> 00:03:57,519
to perform the experiment and insanely

00:03:54,560 --> 00:04:00,159
fast communication between them

00:03:57,519 --> 00:04:02,560
a group that does the enough better

00:04:00,159 --> 00:04:06,799
metal service part

00:04:02,560 --> 00:04:06,799
is the chameleon cloud

00:04:07,200 --> 00:04:14,560
but we do not have infinite resources to

00:04:11,360 --> 00:04:18,000
throw at the problem

00:04:14,560 --> 00:04:21,199
instead we can cram

00:04:18,000 --> 00:04:25,040
a bunch of these devices

00:04:21,199 --> 00:04:27,919
in as few servers as possible

00:04:25,040 --> 00:04:29,520
and then come up with some way for the

00:04:27,919 --> 00:04:33,919
researchers

00:04:29,520 --> 00:04:33,919
to fetch the device they want

00:04:34,960 --> 00:04:40,800
the problem with abstracting hardware

00:04:38,240 --> 00:04:40,800
is

00:04:41,360 --> 00:04:44,479
when using some kind of virtualization

00:04:43,520 --> 00:04:46,639
is that

00:04:44,479 --> 00:04:47,919
something is always lost in the

00:04:46,639 --> 00:04:53,199
transaction

00:04:47,919 --> 00:04:53,199
or translation how much

00:04:53,840 --> 00:05:00,880
that is the question which ties back to

00:04:57,360 --> 00:05:06,000
the as close to comment we had

00:05:00,880 --> 00:05:10,160
before to show what i mean

00:05:06,000 --> 00:05:11,600
let's talk about three different ways to

00:05:10,160 --> 00:05:14,800
pass a network car

00:05:11,600 --> 00:05:18,080
to a vm guest or a

00:05:14,800 --> 00:05:18,080
darker container

00:05:18,960 --> 00:05:25,280
we can pass a virtuous or scale down

00:05:23,440 --> 00:05:27,039
how much scaled down it is really

00:05:25,280 --> 00:05:30,160
depends on the

00:05:27,039 --> 00:05:34,560
driver that's doing the emulation

00:05:30,160 --> 00:05:38,400
card to multiple guests

00:05:34,560 --> 00:05:41,680
out of the same physical card

00:05:38,400 --> 00:05:44,960
this method is called

00:05:41,680 --> 00:05:48,120
para virtualization it uses

00:05:44,960 --> 00:05:50,840
a lot of resources to abstract it at the

00:05:48,120 --> 00:05:53,840
hypervisor

00:05:50,840 --> 00:05:57,680
or container server

00:05:53,840 --> 00:06:02,960
level and is

00:05:57,680 --> 00:06:02,960
as a result the slowest

00:06:03,280 --> 00:06:07,120
and on the top of that it does not

00:06:06,240 --> 00:06:10,479
expose

00:06:07,120 --> 00:06:14,319
much of the original card most

00:06:10,479 --> 00:06:17,840
people are fine with that but

00:06:14,319 --> 00:06:21,840
since we are doing research that is not

00:06:17,840 --> 00:06:21,840
good enough

00:06:21,919 --> 00:06:30,560
other cards have srrov

00:06:26,560 --> 00:06:35,440
which allows us to create

00:06:30,560 --> 00:06:38,880
fake cards at the card itself

00:06:35,440 --> 00:06:42,160
and then hand them off as before

00:06:38,880 --> 00:06:45,360
so in this case the card is really

00:06:42,160 --> 00:06:49,280
a virtual environment

00:06:45,360 --> 00:06:52,880
a mini virtual server

00:06:49,280 --> 00:06:56,240
but it only serves partitions

00:06:52,880 --> 00:06:59,840
of itself a

00:06:56,240 --> 00:07:03,840
classical example of that is a gpu card

00:06:59,840 --> 00:07:03,840
and some network cards

00:07:04,160 --> 00:07:12,720
as such it reveals more about itself

00:07:09,360 --> 00:07:16,880
there are such cards that allow

00:07:12,720 --> 00:07:19,599
each virtual device

00:07:16,880 --> 00:07:22,479
in this mode should be programmed up to

00:07:19,599 --> 00:07:22,479
a certain point

00:07:23,520 --> 00:07:27,599
then we have pci passthrough

00:07:28,160 --> 00:07:33,280
the speed difference between pci

00:07:30,400 --> 00:07:36,880
passthrough and sr rov

00:07:33,280 --> 00:07:40,319
needs to be checked for our context

00:07:36,880 --> 00:07:44,000
but it tends to be

00:07:40,319 --> 00:07:46,720
slightly faster

00:07:44,000 --> 00:07:48,080
however we plan on completely

00:07:46,720 --> 00:07:51,360
reprogramming

00:07:48,080 --> 00:07:55,840
at the very least of network cards

00:07:51,360 --> 00:07:55,840
and that includes changing its firmware

00:07:56,000 --> 00:08:04,400
how much is that how much of that

00:07:59,520 --> 00:08:07,840
is available through srov i don't know

00:08:04,400 --> 00:08:10,840
for now it's safer to just be able to

00:08:07,840 --> 00:08:13,360
hand over the entire card to the

00:08:10,840 --> 00:08:17,919
researcher

00:08:13,360 --> 00:08:17,919
and in such a way that

00:08:18,160 --> 00:08:21,599
the researcher has full control of

00:08:20,879 --> 00:08:26,560
within

00:08:21,599 --> 00:08:29,840
the vm guest

00:08:26,560 --> 00:08:30,319
for our case we are very interested on

00:08:29,840 --> 00:08:33,680
the

00:08:30,319 --> 00:08:38,080
pci passthrough in general

00:08:33,680 --> 00:08:42,640
but we will consider investigating

00:08:38,080 --> 00:08:46,720
other abstractions models

00:08:42,640 --> 00:08:46,720
to see if you can still perform

00:08:46,800 --> 00:08:53,200
we can still achieve the same

00:08:48,640 --> 00:08:53,200
performance and configurability

00:08:53,839 --> 00:08:59,279
that we can get by just handing over the

00:08:56,399 --> 00:08:59,279
entire card

00:09:01,680 --> 00:09:09,839
with that said some of you have realized

00:09:04,720 --> 00:09:09,839
there's something missing here

00:09:10,040 --> 00:09:14,959
pcie version 4.

00:09:13,360 --> 00:09:16,640
i really would like to be talking about

00:09:14,959 --> 00:09:22,080
a pci e5

00:09:16,640 --> 00:09:25,600
but as we know not just here

00:09:22,080 --> 00:09:26,800
anyway pcif4 can provide the same

00:09:25,600 --> 00:09:32,399
bandwidth

00:09:26,800 --> 00:09:35,680
of pci v3 using half of the lanes

00:09:32,399 --> 00:09:38,480
for the sake of compatibility many cards

00:09:35,680 --> 00:09:42,959
scale down their bandwidth usage

00:09:38,480 --> 00:09:46,080
to match they slot they are in

00:09:42,959 --> 00:09:50,000
it may not sound that important but

00:09:46,080 --> 00:09:52,399
when you see a 100 gigabit mega knox

00:09:50,000 --> 00:09:52,399
card

00:09:54,160 --> 00:10:02,880
not uh uh not able to

00:09:59,360 --> 00:10:06,240
get full speed on using one pci

00:10:02,880 --> 00:10:10,000
e3 slot in fact

00:10:06,240 --> 00:10:13,600
having to use two pci 3 slots to

00:10:10,000 --> 00:10:17,040
achieve its full 100 gigabit speed

00:10:13,600 --> 00:10:20,399
when if it was connected to a pcie

00:10:17,040 --> 00:10:24,399
v4 slot we do not

00:10:20,399 --> 00:10:24,399
need this auxiliary card

00:10:24,800 --> 00:10:28,720
you know it really starts to add up

00:10:27,600 --> 00:10:31,760
especially

00:10:28,720 --> 00:10:34,399
if you have a limit on the number of

00:10:31,760 --> 00:10:34,399
slots

00:10:35,519 --> 00:10:44,240
and then there's numa

00:10:38,800 --> 00:10:44,240
which allows us to say textures

00:10:44,839 --> 00:10:50,720
choose cpu server with a few

00:10:48,240 --> 00:10:51,440
some memory and a few pci slots with

00:10:50,720 --> 00:10:54,560
some

00:10:51,440 --> 00:10:54,560
nice orange cards

00:10:56,560 --> 00:11:04,880
and then break it apart

00:11:02,240 --> 00:11:07,680
break the cpus apart to create

00:11:04,880 --> 00:11:10,880
individual computers

00:11:07,680 --> 00:11:16,079
which we really call them pneuma nodes

00:11:10,880 --> 00:11:20,160
with their own pci and memory slots

00:11:16,079 --> 00:11:20,160
and these slots can be accessed

00:11:21,519 --> 00:11:27,519
faster by the cpu you know if

00:11:25,279 --> 00:11:28,480
they if they are inside the same pneuma

00:11:27,519 --> 00:11:31,760
node

00:11:28,480 --> 00:11:34,240
the cpu can access the

00:11:31,760 --> 00:11:35,440
those pci slots and the memory slot

00:11:34,240 --> 00:11:39,200
faster than

00:11:35,440 --> 00:11:42,800
if it tries to access

00:11:39,200 --> 00:11:44,000
a pci slot that is somewhere else in

00:11:42,800 --> 00:11:47,200
some other new

00:11:44,000 --> 00:11:50,720
node the

00:11:47,200 --> 00:11:53,760
bottom line here is that

00:11:50,720 --> 00:11:56,959
this direct access can lead to

00:11:53,760 --> 00:11:59,920
faster vm guests

00:11:56,959 --> 00:12:01,279
and by fast i mean we start getting

00:11:59,920 --> 00:12:03,680
close to bare metal

00:12:01,279 --> 00:12:03,680
speed

00:12:04,639 --> 00:12:08,480
as other talks on this conference will

00:12:07,120 --> 00:12:10,959
show

00:12:08,480 --> 00:12:12,079
there is also a lot of work in the

00:12:10,959 --> 00:12:15,519
numera

00:12:12,079 --> 00:12:20,000
domain being done by amd

00:12:15,519 --> 00:12:20,000
you really should not miss those stocks

00:12:23,440 --> 00:12:30,079
now there are standards for testing

00:12:26,959 --> 00:12:34,240
gpus network cards hard drives

00:12:30,079 --> 00:12:35,360
and so on for this presentation we'll

00:12:34,240 --> 00:12:39,360
focus on

00:12:35,360 --> 00:12:43,279
nvme hard drives but the concepts

00:12:39,360 --> 00:12:43,279
really apply for every for the others

00:12:45,519 --> 00:12:50,720
the standard protocol you use is the

00:12:51,519 --> 00:12:59,279
storage network industry association

00:12:54,720 --> 00:13:02,480
salt network storage performance testing

00:12:59,279 --> 00:13:07,120
specification version 2.01 from

00:13:02,480 --> 00:13:11,680
february 2018. yes it's a more full

00:13:07,120 --> 00:13:14,160
but what does it mean it describes

00:13:11,680 --> 00:13:15,120
the process of testing performance

00:13:14,160 --> 00:13:17,760
testing

00:13:15,120 --> 00:13:18,800
hard drives solid state hard drives

00:13:17,760 --> 00:13:21,680
which includes

00:13:18,800 --> 00:13:22,800
how to pre-condition the hard drive so

00:13:21,680 --> 00:13:25,279
it doesn't behave

00:13:22,800 --> 00:13:26,720
like a hard drive that's fresh out of

00:13:25,279 --> 00:13:30,560
the box

00:13:26,720 --> 00:13:33,600
which should look fast and insane

00:13:30,560 --> 00:13:36,160
but that's inaccurate

00:13:33,600 --> 00:13:38,160
in real life and that would skew the

00:13:36,160 --> 00:13:41,360
results

00:13:38,160 --> 00:13:45,440
it would also deter specify

00:13:41,360 --> 00:13:48,480
which sample range to connect

00:13:45,440 --> 00:13:52,000
if you're doing the testing

00:13:48,480 --> 00:13:53,519
the in the beginning we on the region

00:13:52,000 --> 00:13:57,680
that hard drives might start

00:13:53,519 --> 00:14:00,320
still behaving as new hard drives

00:13:57,680 --> 00:14:00,320
so the

00:14:00,880 --> 00:14:08,160
built-in optimization algorithm

00:14:04,399 --> 00:14:09,519
is working full speed but then we are

00:14:08,160 --> 00:14:14,320
going to start reaching

00:14:09,519 --> 00:14:14,320
a a region where it's

00:14:15,040 --> 00:14:20,240
the it doesn't affect as much because

00:14:18,399 --> 00:14:22,079
there's so much data that had to go

00:14:20,240 --> 00:14:25,199
through it

00:14:22,079 --> 00:14:28,079
that it's now

00:14:25,199 --> 00:14:31,839
finally reaching the same behavior as a

00:14:28,079 --> 00:14:35,279
hard drive has been used

00:14:31,839 --> 00:14:39,040
24 hours every day

00:14:35,279 --> 00:14:39,040
would have

00:14:39,120 --> 00:14:46,639
and after that

00:14:43,600 --> 00:14:49,519
then there is a point that we call the

00:14:46,639 --> 00:14:49,519
steady state

00:14:49,680 --> 00:14:53,920
and that's where we actually want to

00:14:51,680 --> 00:14:56,959
take our readings

00:14:53,920 --> 00:15:01,760
and that is defined when

00:14:56,959 --> 00:15:01,760
the average readings do not change much

00:15:03,040 --> 00:15:12,240
in the experiments we have run here

00:15:07,120 --> 00:15:14,800
that use achieved close to a day of

00:15:12,240 --> 00:15:14,800
testing

00:15:15,440 --> 00:15:22,880
this standard also define which

00:15:19,440 --> 00:15:26,720
tests to perform specifically

00:15:22,880 --> 00:15:27,920
we're talking about bandwidth iops i o

00:15:26,720 --> 00:15:31,279
operations per

00:15:27,920 --> 00:15:33,839
second latency latency

00:15:31,279 --> 00:15:36,639
and each of them has specific read write

00:15:33,839 --> 00:15:40,079
ratios and block sizes

00:15:36,639 --> 00:15:42,240
for that specific test

00:15:40,079 --> 00:15:43,680
for instance iops we're going to be

00:15:42,240 --> 00:15:47,199
doing a

00:15:43,680 --> 00:15:49,519
random read write ratios that

00:15:47,199 --> 00:15:51,279
can be like a hundred to zero 100

00:15:49,519 --> 00:15:55,920
percent reads zero

00:15:51,279 --> 00:15:58,000
right 95 5 five thirty five

00:15:55,920 --> 00:15:59,759
and then and then turns back to thirty

00:15:58,000 --> 00:16:03,279
five sixty five five nine five

00:15:59,759 --> 00:16:06,320
and zero one hundred the block size can

00:16:03,279 --> 00:16:11,680
be from uh i can start from a half a k

00:16:06,320 --> 00:16:11,680
a kilobyte to 1024 kilobytes

00:16:16,880 --> 00:16:23,839
now let's talk about the tools

00:16:20,399 --> 00:16:25,759
that we're going to be using first

00:16:23,839 --> 00:16:27,360
one is going to be the flexible i o

00:16:25,759 --> 00:16:30,000
tester

00:16:27,360 --> 00:16:30,959
which is a multi-threaded io generator

00:16:30,000 --> 00:16:34,639
tool used

00:16:30,959 --> 00:16:39,839
for as its name indicate testing a gif

00:16:34,639 --> 00:16:42,399
and workload it's extremely customizable

00:16:39,839 --> 00:16:44,720
and i personally consider one of the

00:16:42,399 --> 00:16:47,440
best

00:16:44,720 --> 00:16:49,199
testing tools out there then there's

00:16:47,440 --> 00:16:52,560
also the

00:16:49,199 --> 00:16:54,320
storage performance development kit

00:16:52,560 --> 00:16:56,399
which is really a set of tools and

00:16:54,320 --> 00:16:58,959
libraries to write high performance

00:16:56,399 --> 00:17:02,480
scalable

00:16:58,959 --> 00:17:05,360
user mode storage applications

00:17:02,480 --> 00:17:06,559
so what we're really talking about is

00:17:05,360 --> 00:17:09,679
doing a

00:17:06,559 --> 00:17:13,120
zero copy highly parallel access

00:17:09,679 --> 00:17:16,720
directed to a ssd

00:17:13,120 --> 00:17:16,720
at the user space level

00:17:16,839 --> 00:17:23,839
or drivers that we build

00:17:21,039 --> 00:17:25,839
using this they bought you know really

00:17:23,839 --> 00:17:28,960
they should be

00:17:25,839 --> 00:17:32,000
allowed us to get much faster

00:17:28,960 --> 00:17:35,840
storage how fast that is

00:17:32,000 --> 00:17:39,360
compared to other methods that's really

00:17:35,840 --> 00:17:39,360
of course a method of testing

00:17:39,919 --> 00:17:48,320
now spdk can be used in conjunction with

00:17:43,200 --> 00:17:52,559
fio test device using a

00:17:48,320 --> 00:17:56,320
testing device that use spdk driver

00:17:52,559 --> 00:18:00,640
or use its own

00:17:56,320 --> 00:18:04,080
performance testing tool

00:18:00,640 --> 00:18:07,360
and finally we have some custom scripts

00:18:04,080 --> 00:18:10,480
that we created to make our life easier

00:18:07,360 --> 00:18:11,440
one is just to find all the nvme hard

00:18:10,480 --> 00:18:15,440
drives

00:18:11,440 --> 00:18:20,640
that are available in a given computer

00:18:15,440 --> 00:18:24,320
and the order acts as a wrapper

00:18:20,640 --> 00:18:26,960
for fio and spdk

00:18:24,320 --> 00:18:29,360
and deals without that creating the

00:18:26,960 --> 00:18:33,360
laundry list for tests

00:18:29,360 --> 00:18:35,760
to be fed and then running the tests

00:18:33,360 --> 00:18:36,480
saving out the output in some kind of

00:18:35,760 --> 00:18:38,799
format

00:18:36,480 --> 00:18:42,400
that's convenient for us to use in this

00:18:38,799 --> 00:18:45,760
case we chose a comma separated

00:18:42,400 --> 00:18:48,880
files which then

00:18:45,760 --> 00:18:51,919
we can pass to other scripts

00:18:48,880 --> 00:18:57,840
to look for interesting data

00:18:51,919 --> 00:19:01,280
and make pretty graphs

00:18:57,840 --> 00:19:04,880
the host that we've

00:19:01,280 --> 00:19:08,160
been using for testing this is a

00:19:04,880 --> 00:19:09,200
old super micro box which also doubles

00:19:08,160 --> 00:19:13,360
as my

00:19:09,200 --> 00:19:13,360
3d printing filament dryer

00:19:13,520 --> 00:19:20,880
it has 48 pci v3

00:19:16,960 --> 00:19:24,320
lanes and

00:19:20,880 --> 00:19:28,960
i also do intel

00:19:24,320 --> 00:19:32,400
cpus each of those with 18 cores each

00:19:28,960 --> 00:19:36,320
and the wage setup each

00:19:32,400 --> 00:19:40,320
single cpu is an independent pneuma node

00:19:36,320 --> 00:19:44,080
so we really have only two pneuma nodes

00:19:40,320 --> 00:19:47,440
one that takes their course from 0-17

00:19:44,080 --> 00:19:50,880
and the order that takes

00:19:47,440 --> 00:19:50,880
18 to 35.

00:19:51,760 --> 00:20:00,240
the also there

00:19:55,520 --> 00:20:02,320
we have the intel nvme hard drives

00:20:00,240 --> 00:20:03,280
and the ones we are using on this

00:20:02,320 --> 00:20:06,240
machine

00:20:03,280 --> 00:20:08,000
they go on the front that's all those

00:20:06,240 --> 00:20:11,600
drive bays are for

00:20:08,000 --> 00:20:14,159
and they are using the u.2 interface

00:20:11,600 --> 00:20:15,280
which means as the picture shows on the

00:20:14,159 --> 00:20:18,799
bottom

00:20:15,280 --> 00:20:20,080
they look like standard sata connectors

00:20:18,799 --> 00:20:23,280
but they have a few little

00:20:20,080 --> 00:20:23,280
pins in the middle

00:20:24,159 --> 00:20:30,640
the problem here is that all of those

00:20:27,440 --> 00:20:33,440
10 hard drives that we have are attached

00:20:30,640 --> 00:20:37,600
to one single controller

00:20:33,440 --> 00:20:37,600
which is in numa node zero

00:20:38,880 --> 00:20:43,520
so there will be really no performance

00:20:41,600 --> 00:20:46,880
gains to be had

00:20:43,520 --> 00:20:48,640
by running uh the different gas

00:20:46,880 --> 00:20:51,200
containers in separate

00:20:48,640 --> 00:20:55,280
pneuma nodes if you want any performance

00:20:51,200 --> 00:20:55,280
we have to put them on pneuma node zero

00:20:56,240 --> 00:21:03,039
out of curiosity jesus

00:20:59,840 --> 00:21:06,080
uh how we

00:21:03,039 --> 00:21:07,440
looked for the list you know where each

00:21:06,080 --> 00:21:10,640
of those hard drives

00:21:07,440 --> 00:21:13,039
was in which pneuma node it was

00:21:10,640 --> 00:21:15,120
we used a fine drive script that we

00:21:13,039 --> 00:21:19,280
mentioned before

00:21:15,120 --> 00:21:21,760
and a bit of help of

00:21:19,280 --> 00:21:21,760
verse

00:21:26,960 --> 00:21:31,840
now that we know the players let's see

00:21:29,120 --> 00:21:34,000
if we can grasp

00:21:31,840 --> 00:21:36,240
what we really want to accomplish with

00:21:34,000 --> 00:21:40,240
them

00:21:36,240 --> 00:21:40,240
what we are proposing is to compare

00:21:41,520 --> 00:21:49,520
file by itself with with a

00:21:45,200 --> 00:21:54,480
lib io maybe file in spdk

00:21:49,520 --> 00:21:54,480
and maybe even spdk it by itself

00:21:55,280 --> 00:22:00,720
and that's not that's even not counting

00:21:58,720 --> 00:22:04,080
all the optimizations we can do

00:22:00,720 --> 00:22:07,360
by just

00:22:04,080 --> 00:22:09,360
messing with uh spdk

00:22:07,360 --> 00:22:10,640
announce the optimizations we are going

00:22:09,360 --> 00:22:14,960
to be doing by using

00:22:10,640 --> 00:22:18,640
a virtual device a virtual host

00:22:14,960 --> 00:22:22,240
we also have 10 hard drives

00:22:18,640 --> 00:22:23,760
they are exact the same makeup model

00:22:22,240 --> 00:22:26,559
you know it'd be nice to have different

00:22:23,760 --> 00:22:31,679
ones but different models

00:22:26,559 --> 00:22:31,679
but the thing is how similar they are

00:22:32,080 --> 00:22:36,480
if you run the same test on each of

00:22:34,559 --> 00:22:38,840
those hard drives would i get the exact

00:22:36,480 --> 00:22:41,440
same result or it's going to be some

00:22:38,840 --> 00:22:45,440
variation

00:22:41,440 --> 00:22:48,720
and then we have if you're going to do

00:22:45,440 --> 00:22:49,760
the iops test we have eight different

00:22:48,720 --> 00:22:53,039
block size

00:22:49,760 --> 00:22:56,640
1024k 128k

00:22:53,039 --> 00:23:00,159
64k 32 16 8 4

00:22:56,640 --> 00:23:00,159
and then half a k

00:23:01,039 --> 00:23:05,840
and then we have the seven different

00:23:04,080 --> 00:23:06,960
read write ratios that we mentioned

00:23:05,840 --> 00:23:10,640
before

00:23:06,960 --> 00:23:16,000
and that's not even counting sequential

00:23:10,640 --> 00:23:16,000
uh read writes versus random read writes

00:23:17,200 --> 00:23:25,120
and then we have to see the effects

00:23:23,440 --> 00:23:27,919
which we really can't test on this

00:23:25,120 --> 00:23:28,640
machine but they are available they are

00:23:27,919 --> 00:23:32,240
there

00:23:28,640 --> 00:23:34,799
in principle about uh playing

00:23:32,240 --> 00:23:37,120
uh which kind of uh memory we are

00:23:34,799 --> 00:23:40,640
setting up our virtual machines

00:23:37,120 --> 00:23:43,840
the number of cpus no more nodes

00:23:40,640 --> 00:23:43,840
and so on

00:23:43,919 --> 00:23:47,679
i would like to take uh to calculate the

00:23:46,000 --> 00:23:51,679
possible permutations but

00:23:47,679 --> 00:23:54,720
it's a lot like thousands

00:23:51,679 --> 00:23:59,679
you know using the fio test

00:23:54,720 --> 00:24:01,360
we can cut down some of those tests

00:23:59,679 --> 00:24:02,720
but you still have to manually start

00:24:01,360 --> 00:24:04,720
them

00:24:02,720 --> 00:24:08,400
and they still have to be configured and

00:24:04,720 --> 00:24:08,400
you still have to process the data

00:24:08,720 --> 00:24:15,840
i tried to generate a file task

00:24:12,240 --> 00:24:16,240
config files before for a few runs was

00:24:15,840 --> 00:24:19,760
fine

00:24:16,240 --> 00:24:21,279
but it soon became very easy to make

00:24:19,760 --> 00:24:24,720
mistakes

00:24:21,279 --> 00:24:28,000
in fact i made a lot

00:24:24,720 --> 00:24:32,640
and that was bad because

00:24:28,000 --> 00:24:35,679
i had to

00:24:32,640 --> 00:24:38,880
redo them and sometimes find out what

00:24:35,679 --> 00:24:42,080
was wrong and then redo them

00:24:38,880 --> 00:24:43,919
and that means since those tests they

00:24:42,080 --> 00:24:48,559
take hours upon hours that means

00:24:43,919 --> 00:24:54,080
i lost not only ours but days

00:24:48,559 --> 00:24:54,080
in the process i think we can do better

00:24:56,559 --> 00:25:05,039
so as i mentioned before

00:25:01,840 --> 00:25:06,880
we uh wrote a script that takes

00:25:05,039 --> 00:25:08,640
the parameters that are used to create

00:25:06,880 --> 00:25:12,080
the config files for

00:25:08,640 --> 00:25:16,400
fions pdk creates those files

00:25:12,080 --> 00:25:19,440
run the tests and for each of

00:25:16,400 --> 00:25:19,440
those tests

00:25:19,679 --> 00:25:21,919
we

00:25:23,360 --> 00:25:29,760
then process uh we collect the data

00:25:26,559 --> 00:25:33,360
collect the data not only just in a

00:25:29,760 --> 00:25:36,159
csv format but also in a file name

00:25:33,360 --> 00:25:38,880
format that helps us identify which run

00:25:36,159 --> 00:25:38,880
it came from

00:25:39,039 --> 00:25:45,039
and we actually do save

00:25:42,480 --> 00:25:48,000
the config files that were created to

00:25:45,039 --> 00:25:48,000
run those tests

00:25:48,799 --> 00:25:51,279
and

00:25:52,720 --> 00:25:57,520
that means that you know we can go back

00:25:55,919 --> 00:25:59,360
to them

00:25:57,520 --> 00:26:01,679
not only to use for reference for

00:25:59,360 --> 00:26:04,559
documentation but also to reproduce any

00:26:01,679 --> 00:26:08,320
specific tests

00:26:04,559 --> 00:26:12,559
because of this automation

00:26:08,320 --> 00:26:15,520
you know we really you know

00:26:12,559 --> 00:26:17,200
we decide why just running some tests

00:26:15,520 --> 00:26:17,600
for like you know bandwidth some tests

00:26:17,200 --> 00:26:22,640
for

00:26:17,600 --> 00:26:22,640
iops why not just run all of them

00:26:23,360 --> 00:26:28,720
that will cover all the snia test

00:26:26,640 --> 00:26:32,400
requirements

00:26:28,720 --> 00:26:32,400
it's easy to drop the data

00:26:32,480 --> 00:26:36,840
because you know by that i mean it's

00:26:34,880 --> 00:26:38,559
it's always easy to drop data you don't

00:26:36,840 --> 00:26:41,840
need

00:26:38,559 --> 00:26:42,640
but you collect it then it is to to try

00:26:41,840 --> 00:26:45,919
to

00:26:42,640 --> 00:26:48,400
get data that you need but do not

00:26:45,919 --> 00:26:48,400
collect

00:26:48,720 --> 00:26:56,640
you know if this takes like you know

00:26:52,320 --> 00:26:58,960
an extra day or so who cares

00:26:56,640 --> 00:26:59,840
we can automate it we can line them up

00:26:58,960 --> 00:27:02,559
as soon as one

00:26:59,840 --> 00:27:04,480
batch of test starts ends another one

00:27:02,559 --> 00:27:06,720
starts and you keep doing that

00:27:04,480 --> 00:27:07,600
and while that's doing we can we can get

00:27:06,720 --> 00:27:09,679
the data

00:27:07,600 --> 00:27:10,640
we can process it and then plan the next

00:27:09,679 --> 00:27:14,640
batch

00:27:10,640 --> 00:27:14,640
you know automation is good

00:27:15,440 --> 00:27:23,679
now here's an example of uh how that

00:27:20,240 --> 00:27:27,360
torture test script

00:27:23,679 --> 00:27:30,880
creates the config file for children

00:27:27,360 --> 00:27:35,840
with fio in this case

00:27:30,880 --> 00:27:35,840
it is a

00:27:36,720 --> 00:27:42,159
you know it's set up to run to do a to

00:27:39,360 --> 00:27:46,480
run fio with libio

00:27:42,159 --> 00:27:48,480
it's set up to run up to 24 hours

00:27:46,480 --> 00:27:50,960
but if you see that steady state

00:27:48,480 --> 00:27:54,720
statement

00:27:50,960 --> 00:27:57,919
it will uh stop

00:27:54,720 --> 00:28:01,360
running the test when the

00:27:57,919 --> 00:28:03,200
iops slope is less than three percent

00:28:01,360 --> 00:28:06,880
for a period of 1800

00:28:03,200 --> 00:28:09,360
seconds and usually so far

00:28:06,880 --> 00:28:12,080
that has always been before the 24

00:28:09,360 --> 00:28:15,200
window ended

00:28:12,080 --> 00:28:17,840
and for every single run that

00:28:15,200 --> 00:28:17,840
torture

00:28:20,559 --> 00:28:26,480
my little torture drive script will

00:28:23,120 --> 00:28:26,480
create a file like that

00:28:27,360 --> 00:28:32,000
note also the file name that is

00:28:29,760 --> 00:28:35,520
identifies that

00:28:32,000 --> 00:28:39,200
is the the name of the device which test

00:28:35,520 --> 00:28:39,200
we're doing and when we started

00:28:40,960 --> 00:28:46,799
we also use enspo to build

00:28:44,399 --> 00:28:46,799
things

00:28:47,440 --> 00:28:54,320
the playbook we have two playbooks

00:28:50,720 --> 00:28:56,720
actually one is to build the bare metal

00:28:54,320 --> 00:28:56,720
server

00:28:56,799 --> 00:29:01,600
and specifically installs docker and kvm

00:29:02,880 --> 00:29:06,000
and those little arrows that you're

00:29:05,120 --> 00:29:09,760
seeing there

00:29:06,000 --> 00:29:13,120
they actually represent task files

00:29:09,760 --> 00:29:15,360
that are run by the docker uh

00:29:13,120 --> 00:29:19,440
container they do not they're not the

00:29:15,360 --> 00:29:19,440
docker but but the ansible playbook

00:29:20,240 --> 00:29:24,240
i like to keep them separate because

00:29:22,399 --> 00:29:27,679
that allows me to use

00:29:24,240 --> 00:29:31,919
uh them for other tasks

00:29:27,679 --> 00:29:33,919
in fact the install docker

00:29:31,919 --> 00:29:35,440
task i'm going to be using later on

00:29:33,919 --> 00:29:40,960
today

00:29:35,440 --> 00:29:44,240
for another project

00:29:40,960 --> 00:29:48,000
we also have a playbook to install the

00:29:44,240 --> 00:29:48,000
testing software itself

00:29:48,320 --> 00:29:54,640
i really can use an autry

00:29:51,679 --> 00:29:56,480
bare metal kvm darker kinds of text

00:29:54,640 --> 00:29:59,440
boxes

00:29:56,480 --> 00:30:01,200
in fact i have used display book on all

00:29:59,440 --> 00:30:04,799
three before

00:30:01,200 --> 00:30:08,000
the only reason i stopped using it on

00:30:04,799 --> 00:30:12,480
primarily darker and nowadays on

00:30:08,000 --> 00:30:12,480
kvm was

00:30:13,440 --> 00:30:19,440
speed specifically that

00:30:17,360 --> 00:30:20,720
i would have the image already ready to

00:30:19,440 --> 00:30:23,360
go without the

00:30:20,720 --> 00:30:25,919
package installed and i can just start

00:30:23,360 --> 00:30:25,919
it and run

00:30:26,320 --> 00:30:33,039
second by doing that i don't have to

00:30:30,159 --> 00:30:34,159
run ssh inside either the docker

00:30:33,039 --> 00:30:37,360
container

00:30:34,159 --> 00:30:39,760
or the kvm

00:30:37,360 --> 00:30:39,760
guest

00:30:40,640 --> 00:30:47,120
and asphalt needs ssh to

00:30:44,399 --> 00:30:47,120
do its thing

00:30:48,840 --> 00:30:51,840
and

00:30:52,320 --> 00:31:00,320
the that doesn't stop me

00:30:55,840 --> 00:31:02,799
if i want to run the playbook again

00:31:00,320 --> 00:31:04,399
because the playbook will just go

00:31:02,799 --> 00:31:05,840
through it it's going to realize oh you

00:31:04,399 --> 00:31:08,720
already installed this package

00:31:05,840 --> 00:31:10,399
you already download the gs files your

00:31:08,720 --> 00:31:11,200
red compound and build them so you're

00:31:10,399 --> 00:31:14,480
good i don't

00:31:11,200 --> 00:31:17,440
need to up to change unless it has

00:31:14,480 --> 00:31:18,000
a version that's newer than the version

00:31:17,440 --> 00:31:20,480
that's already

00:31:18,000 --> 00:31:20,480
installed

00:31:21,919 --> 00:31:28,960
then we also have to

00:31:25,279 --> 00:31:28,960
talk about our test boxes

00:31:29,519 --> 00:31:31,760
the

00:31:34,960 --> 00:31:41,360
server we really have one that

00:31:38,480 --> 00:31:42,960
right now we only have that super micro

00:31:41,360 --> 00:31:45,360
so

00:31:42,960 --> 00:31:48,000
there is really no point of building a

00:31:45,360 --> 00:31:51,360
separate custom image just for it

00:31:48,000 --> 00:31:54,480
it's much easier just to put the machine

00:31:51,360 --> 00:31:55,840
do a basic install let answer build it

00:31:54,480 --> 00:31:57,919
and off we go

00:31:55,840 --> 00:31:59,440
theoretically i could do it you know

00:31:57,919 --> 00:32:02,720
create a custom machine

00:31:59,440 --> 00:32:07,279
and run it through pixyboot

00:32:02,720 --> 00:32:07,279
but for both dock and kvm

00:32:07,600 --> 00:32:13,600
i have a image

00:32:10,720 --> 00:32:16,960
you know as i mentioned earlier without

00:32:13,600 --> 00:32:16,960
a package that we need

00:32:17,360 --> 00:32:24,080
and once you run this

00:32:20,480 --> 00:32:27,440
uh with this image you start the

00:32:24,080 --> 00:32:30,480
vm guest or the container you give them

00:32:27,440 --> 00:32:33,440
the locations to start to install

00:32:30,480 --> 00:32:34,159
the data that will survive the container

00:32:33,440 --> 00:32:37,760
being gone

00:32:34,159 --> 00:32:40,840
by that i mean it's a storage

00:32:37,760 --> 00:32:43,760
it's a directory that's shared between

00:32:40,840 --> 00:32:47,360
the bare metal server

00:32:43,760 --> 00:32:50,960
and the docker or the kvm

00:32:47,360 --> 00:32:53,200
guest and that's where we're going to be

00:32:50,960 --> 00:32:57,840
storing the

00:32:53,200 --> 00:32:57,840
results from the experiments

00:32:59,039 --> 00:33:02,399
and then with and then we when we start

00:33:01,200 --> 00:33:08,480
the containers

00:33:02,399 --> 00:33:12,399
we we feed the hard drive the pci

00:33:08,480 --> 00:33:13,519
the pci base the not pc well it's on pci

00:33:12,399 --> 00:33:16,240
but the

00:33:13,519 --> 00:33:16,720
using pci passthrough the nvme hard

00:33:16,240 --> 00:33:19,919
drive

00:33:16,720 --> 00:33:22,840
is is fed to either the docker

00:33:19,919 --> 00:33:25,840
container or the vm gas at the start

00:33:22,840 --> 00:33:29,360
time

00:33:25,840 --> 00:33:31,679
if those any of those guys they need to

00:33:29,360 --> 00:33:36,080
download something or access the network

00:33:31,679 --> 00:33:36,080
both on netball networks

00:33:36,840 --> 00:33:41,200
and that means that you know if you

00:33:40,080 --> 00:33:43,760
really need to

00:33:41,200 --> 00:33:47,039
enable ssh we can do that and do some

00:33:43,760 --> 00:33:47,039
port forward and be done

00:33:48,480 --> 00:33:52,000
this is how you use an example how to

00:33:51,039 --> 00:33:55,519
run torture

00:33:52,000 --> 00:33:59,640
disk against one

00:33:55,519 --> 00:34:02,880
against three hard drives

00:33:59,640 --> 00:34:07,120
nvme two

00:34:02,880 --> 00:34:09,440
five and nine and

00:34:07,120 --> 00:34:10,879
running at all those different block

00:34:09,440 --> 00:34:16,159
sizes

00:34:10,879 --> 00:34:19,040
the mixed reads are the read write ratio

00:34:16,159 --> 00:34:20,480
and even though it just shows like 10 to

00:34:19,040 --> 00:34:25,679
zero it's actually

00:34:20,480 --> 00:34:25,679
actually test the all the way around too

00:34:27,200 --> 00:34:32,800
and uh the out uh the out

00:34:30,720 --> 00:34:34,000
the directors which create storylog

00:34:32,800 --> 00:34:37,679
files is based

00:34:34,000 --> 00:34:40,399
on the that lib io name

00:34:37,679 --> 00:34:41,599
plus the name of the hard drive and plus

00:34:40,399 --> 00:34:44,879
the date

00:34:41,599 --> 00:34:48,480
as you saw also as before

00:34:44,879 --> 00:34:50,159
the time here it says that you know

00:34:48,480 --> 00:34:53,599
that's the limit how far it can go

00:34:50,159 --> 00:34:54,320
before it stops instead state it's set

00:34:53,599 --> 00:35:00,320
up to

00:34:54,320 --> 00:35:00,320
do the iop stat state that we mentioned

00:35:00,839 --> 00:35:05,440
before

00:35:02,320 --> 00:35:07,760
now if you uh

00:35:05,440 --> 00:35:08,960
if you're going to run for instance uh

00:35:07,760 --> 00:35:12,720
start uh you know

00:35:08,960 --> 00:35:15,119
creating a bunch of docker containers

00:35:12,720 --> 00:35:18,000
we can do that uh like as in this

00:35:15,119 --> 00:35:18,000
example by

00:35:18,240 --> 00:35:25,200
listing all the

00:35:22,240 --> 00:35:27,200
nvme hard drives that are found on this

00:35:25,200 --> 00:35:30,480
server using find drive

00:35:27,200 --> 00:35:33,839
and then looping over them

00:35:30,480 --> 00:35:35,680
and then we are going to uh create

00:35:33,839 --> 00:35:37,359
the direct on the fly the director is

00:35:35,680 --> 00:35:40,000
going to store the log files

00:35:37,359 --> 00:35:42,160
and then pass this directory to the

00:35:40,000 --> 00:35:44,640
docker container

00:35:42,160 --> 00:35:46,960
and the name of the device and let it do

00:35:44,640 --> 00:35:49,200
its thing

00:35:46,960 --> 00:35:52,320
this is very similar to what we have

00:35:49,200 --> 00:35:53,599
done in kvm

00:35:52,320 --> 00:35:56,160
you know we still have to create an

00:35:53,599 --> 00:35:59,599
image download package here

00:35:56,160 --> 00:36:03,440
and everything else

00:35:59,599 --> 00:36:08,160
it just takes a bit longer oh it takes

00:36:03,440 --> 00:36:11,359
much longer than a darker container

00:36:08,160 --> 00:36:11,680
but it really doesn't matter because

00:36:11,359 --> 00:36:14,480
it's

00:36:11,680 --> 00:36:14,480
automated

00:36:18,160 --> 00:36:24,960
the following graphs show the

00:36:21,760 --> 00:36:30,079
results for running fio tests using

00:36:24,960 --> 00:36:30,079
libio on just one single drive

00:36:30,160 --> 00:36:34,000
remember when i said before that you

00:36:32,240 --> 00:36:36,160
know there's a lot of data and there's a

00:36:34,000 --> 00:36:39,119
lot of tests that we're going to do

00:36:36,160 --> 00:36:40,000
this is just one single drive and jesus

00:36:39,119 --> 00:36:43,520
just as you can

00:36:40,000 --> 00:36:46,320
see here is just doing the fio

00:36:43,520 --> 00:36:47,760
using libio and just not even the entire

00:36:46,320 --> 00:36:50,880
list

00:36:47,760 --> 00:36:54,400
in fact you know i did not

00:36:50,880 --> 00:36:58,079
show the iops

00:36:54,400 --> 00:36:58,480
and bandwidth act note that we have iops

00:36:58,079 --> 00:37:02,320
yes

00:36:58,480 --> 00:37:06,320
it's a bandwidth latency and iops

00:37:02,320 --> 00:37:09,680
but we don't have the half of the tests

00:37:06,320 --> 00:37:11,440
are not here

00:37:09,680 --> 00:37:13,599
but i really like this kind of graph

00:37:11,440 --> 00:37:15,599
because you know they

00:37:13,599 --> 00:37:17,359
really show in this case you know you're

00:37:15,599 --> 00:37:20,640
comparing bare metal

00:37:17,359 --> 00:37:23,760
darker and kvm they

00:37:20,640 --> 00:37:26,240
really show to

00:37:23,760 --> 00:37:26,240
others

00:37:27,040 --> 00:37:34,000
how the the different outcomes that

00:37:30,160 --> 00:37:36,160
can be created using different settings

00:37:34,000 --> 00:37:37,359
yes we still can do like no the f after

00:37:36,160 --> 00:37:39,200
careful considering

00:37:37,359 --> 00:37:40,720
the answers provided by bone reading we

00:37:39,200 --> 00:37:43,200
conclude the best performance is

00:37:40,720 --> 00:37:45,440
achieved by setting the block size to x

00:37:43,200 --> 00:37:46,480
who chooses and why for the rest of the

00:37:45,440 --> 00:37:50,160
week

00:37:46,480 --> 00:37:53,440
you know but

00:37:50,160 --> 00:37:55,119
this is not really a hardcore technical

00:37:53,440 --> 00:37:57,760
talk and i'm not really

00:37:55,119 --> 00:37:58,480
concerned about setting that specific

00:37:57,760 --> 00:38:01,040
drive

00:37:58,480 --> 00:38:02,400
to the best performance and let it go

00:38:01,040 --> 00:38:03,359
i'm going to be testing on the drives

00:38:02,400 --> 00:38:04,800
i'm going to be testing all the

00:38:03,359 --> 00:38:06,160
situations i'm going to be adding other

00:38:04,800 --> 00:38:07,760
things so

00:38:06,160 --> 00:38:11,920
i always want to have more varia

00:38:07,760 --> 00:38:11,920
variants that i need to deal with

00:38:12,720 --> 00:38:17,040
but if you look carefully you'll note

00:38:15,520 --> 00:38:19,359
that you know the

00:38:17,040 --> 00:38:22,000
the charts and the access titles are not

00:38:19,359 --> 00:38:24,079
really consistent

00:38:22,000 --> 00:38:25,040
this happened because you know the way i

00:38:24,079 --> 00:38:28,800
made those

00:38:25,040 --> 00:38:30,880
uh graphs i literally imported the data

00:38:28,800 --> 00:38:32,880
into libreoffice and i tried to create

00:38:30,880 --> 00:38:34,720
the graphs

00:38:32,880 --> 00:38:36,720
so i really had to rely on my lack of

00:38:34,720 --> 00:38:37,520
typing skills instead of having some

00:38:36,720 --> 00:38:40,720
kind of you know

00:38:37,520 --> 00:38:41,520
smart script that take the data do the

00:38:40,720 --> 00:38:45,599
average for each

00:38:41,520 --> 00:38:47,040
run for each test box or drive or do a

00:38:45,599 --> 00:38:49,920
combination whatever you want to do and

00:38:47,040 --> 00:38:49,920
then create the graphs

00:38:50,560 --> 00:38:56,640
next time i'm going to see how to create

00:38:54,400 --> 00:38:59,119
proper nice looking 3d graphs using in

00:38:56,640 --> 00:39:00,960
new plot

00:38:59,119 --> 00:39:02,640
and they look uh i want them to look

00:39:00,960 --> 00:39:05,040
good enough so i can put in a taco

00:39:02,640 --> 00:39:05,040
journal

00:39:05,680 --> 00:39:11,760
and this one shows similar tests but we

00:39:08,880 --> 00:39:15,359
are doing fio with sf

00:39:11,760 --> 00:39:19,359
with spdk

00:39:15,359 --> 00:39:19,839
and as we mentioned before we kind of

00:39:19,359 --> 00:39:23,200
expect

00:39:19,839 --> 00:39:27,520
an increase in iops and bandwidth and

00:39:23,200 --> 00:39:29,280
if you look at the axis

00:39:27,520 --> 00:39:33,200
the scales on the axis you're going to

00:39:29,280 --> 00:39:35,839
see as we had an increase

00:39:33,200 --> 00:39:37,520
i personally i kind of don't like that

00:39:35,839 --> 00:39:39,440
you know it's out scaling but

00:39:37,520 --> 00:39:40,560
i don't know what to do about that maybe

00:39:39,440 --> 00:39:42,320
when i start using

00:39:40,560 --> 00:39:45,920
the plot i can control that so you

00:39:42,320 --> 00:39:45,920
actually can see a difference

00:39:46,480 --> 00:39:49,119
and

00:39:49,520 --> 00:39:55,599
there's by the way an intel uh

00:39:52,640 --> 00:39:57,599
article you know from intel the company

00:39:55,599 --> 00:39:58,880
and i'll put on the end of the slide act

00:39:57,599 --> 00:40:01,839
that states that the

00:39:58,880 --> 00:40:02,640
performance obtained using fio plus

00:40:01,839 --> 00:40:08,000
libiyo

00:40:02,640 --> 00:40:11,680
is less than that with fio plus fp spdk

00:40:08,000 --> 00:40:15,119
and which is less than running the spdk

00:40:11,680 --> 00:40:15,119
performance tool by itself

00:40:15,200 --> 00:40:20,960
the you know i could have shown the

00:40:18,560 --> 00:40:23,680
other drives but i'm not going to

00:40:20,960 --> 00:40:24,079
because you know i have not automated

00:40:23,680 --> 00:40:26,640
the

00:40:24,079 --> 00:40:29,280
plotting part and i'm not really going

00:40:26,640 --> 00:40:32,319
to do this manually

00:40:29,280 --> 00:40:33,839
no thank you and now so you know now

00:40:32,319 --> 00:40:36,839
that jesus stable

00:40:33,839 --> 00:40:38,240
i want you to start focusing on the

00:40:36,839 --> 00:40:40,800
effects

00:40:38,240 --> 00:40:41,599
of using different settings say huge man

00:40:40,800 --> 00:40:44,960
cpu

00:40:41,599 --> 00:40:48,000
pneuma will do

00:40:44,960 --> 00:40:48,000
to the performance

00:40:49,920 --> 00:40:54,240
but remember that you know jesus was not

00:40:52,720 --> 00:40:56,000
really about you know the data showing

00:40:54,240 --> 00:40:57,200
like oh just the results obtained and

00:40:56,000 --> 00:41:00,480
things like that

00:40:57,200 --> 00:41:01,200
this was all about setting up the

00:41:00,480 --> 00:41:04,400
experiment

00:41:01,200 --> 00:41:09,280
and how to automate the experiment

00:41:04,400 --> 00:41:09,280
to add to do some real experimenting

00:41:10,079 --> 00:41:13,839
which what we're talking about here you

00:41:13,119 --> 00:41:18,000
know

00:41:13,839 --> 00:41:21,200
that make your life easier

00:41:18,000 --> 00:41:23,920
make it your experience reproducible

00:41:21,200 --> 00:41:25,520
and be lazy you know create you know

00:41:23,920 --> 00:41:29,520
pre-built images

00:41:25,520 --> 00:41:31,520
for docker and kvm it's nice

00:41:29,520 --> 00:41:34,560
have a script track to do the actual

00:41:31,520 --> 00:41:37,599
health lifting

00:41:34,560 --> 00:41:38,960
and i would like

00:41:37,599 --> 00:41:40,960
you know to take the opportunity to

00:41:38,960 --> 00:41:43,280
thank the linux foundation for the

00:41:40,960 --> 00:41:45,680
opportunity to participate on the kvm

00:41:43,280 --> 00:41:45,680
forum

00:41:46,160 --> 00:41:52,480
and here are some useful links

00:41:49,680 --> 00:41:53,760
you know you can read the 102 pages on

00:41:52,480 --> 00:41:57,440
the

00:41:53,760 --> 00:41:59,920
sni uh how to how to properly test solid

00:41:57,440 --> 00:42:04,079
state drive

00:41:59,920 --> 00:42:06,480
there's the fios pdk links

00:42:04,079 --> 00:42:08,079
the there's the intel article i talked

00:42:06,480 --> 00:42:11,760
about

00:42:08,079 --> 00:42:12,800
the watches fabric is the link to the

00:42:11,760 --> 00:42:17,040
project

00:42:12,800 --> 00:42:20,960
that i'm that i'm building this test

00:42:17,040 --> 00:42:23,520
tools for and finally the

00:42:20,960 --> 00:42:26,240
the last one is the link to my find

00:42:23,520 --> 00:42:26,240
drive script

00:42:27,440 --> 00:42:32,720
and the torture disk is also available

00:42:30,960 --> 00:42:35,119
i'm kind of ashamed of it you know the

00:42:32,720 --> 00:42:36,960
code still needs a lot of cleaning

00:42:35,119 --> 00:42:38,400
so i'm not putting it here even though

00:42:36,960 --> 00:42:40,880
you can go to my github

00:42:38,400 --> 00:42:45,839
thing and find it but just don't tell me

00:42:40,880 --> 00:42:45,839
you did

00:42:52,800 --> 00:42:54,880

YouTube URL: https://www.youtube.com/watch?v=hZUVTCOq8b8


