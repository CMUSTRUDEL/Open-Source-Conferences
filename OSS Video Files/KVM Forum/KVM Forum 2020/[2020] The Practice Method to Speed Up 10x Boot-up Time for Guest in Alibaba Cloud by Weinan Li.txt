Title: [2020] The Practice Method to Speed Up 10x Boot-up Time for Guest in Alibaba Cloud by Weinan Li
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	When the hypervisor assigns memory to one virtual machine, it needs to pin the memory first. As you know, "pin memory" is one time-consuming work which is directly proportional to the amount of memory. If you just assign 8GB RAM to VM, that might be not a case at all, but that must be one big problem if the RAM is 300GB, the only "pin memory" process need more than 60s. 300G is one common configuration in the cloud, and 60s impacts the user experience seriously. This topic will present one simple solution for accelerating the boot process with virtio-balloon, then the hypervisor can pin the memory asynchronously. This whole process runs in the background with little user perception what can bring very good user experience. This solution could reduce around 90 percents boot-time compared with one normal use case.

---

Weinan Li
Alibaba Cloud, Software Engineer
Shanghai, China

Weinan is working on the produce-heterogeneous computing field since 2019 in Alibaba Cloud. Before that, he worked for Intel with Graphics Virtualization since Dec. 2014, was responsible for the enabling work and new features development of several generations of Intel GPU.
Captions: 
	00:00:04,960 --> 00:00:09,120
hello everyone

00:00:06,080 --> 00:00:12,000
this is rena from alibaba cloud goten

00:00:09,120 --> 00:00:15,200
and i complete this practice

00:00:12,000 --> 00:00:16,160
i will present this session today our

00:00:15,200 --> 00:00:18,320
topic is

00:00:16,160 --> 00:00:20,000
speed up the boot time of guest in

00:00:18,320 --> 00:00:22,960
alibaba cloud

00:00:20,000 --> 00:00:24,080
this is the agenda of today's session

00:00:22,960 --> 00:00:26,400
first

00:00:24,080 --> 00:00:28,640
i will introduce the background and why

00:00:26,400 --> 00:00:31,760
do we need to do this

00:00:28,640 --> 00:00:32,320
second i will figure out our solution we

00:00:31,760 --> 00:00:36,079
call it

00:00:32,320 --> 00:00:40,239
async dme map third i will show the

00:00:36,079 --> 00:00:42,559
guest boot process with async dma

00:00:40,239 --> 00:00:43,840
and then i will list several

00:00:42,559 --> 00:00:47,120
optimization design

00:00:43,840 --> 00:00:49,440
for this practice at last

00:00:47,120 --> 00:00:50,800
i will share our achievements with this

00:00:49,440 --> 00:00:54,000
solution

00:00:50,800 --> 00:00:56,960
let's start as a background so

00:00:54,000 --> 00:00:59,359
what's the problem as you know we need

00:00:56,960 --> 00:01:02,320
to dma map all the guest memory

00:00:59,359 --> 00:01:04,559
when there is a pass-through device

00:01:02,320 --> 00:01:05,280
since the device is mapped dma to the

00:01:04,559 --> 00:01:08,880
whole gas

00:01:05,280 --> 00:01:12,000
memory and the memory cannot be swiped

00:01:08,880 --> 00:01:15,600
which is diameter date but we

00:01:12,000 --> 00:01:18,479
don't know what memory is the dma target

00:01:15,600 --> 00:01:19,040
so one simple solution is pin map all

00:01:18,479 --> 00:01:22,960
the guest

00:01:19,040 --> 00:01:25,280
memory it might not be a problem when

00:01:22,960 --> 00:01:28,400
the gas memory is very small

00:01:25,280 --> 00:01:32,320
but as you know the memory is not a

00:01:28,400 --> 00:01:34,159
scarce resource today so a guest

00:01:32,320 --> 00:01:36,320
might have a few hundred or more

00:01:34,159 --> 00:01:39,520
gigabytes system memory

00:01:36,320 --> 00:01:41,119
and the dme map is one time consuming

00:01:39,520 --> 00:01:43,920
process

00:01:41,119 --> 00:01:46,240
since that will be a big problem there

00:01:43,920 --> 00:01:47,439
are two charts to show the impact on the

00:01:46,240 --> 00:01:50,079
guest boot and

00:01:47,439 --> 00:01:52,960
cumule initialization time along with

00:01:50,079 --> 00:01:55,680
the vm's memory size increase

00:01:52,960 --> 00:01:56,799
let's start as the left one the

00:01:55,680 --> 00:02:00,079
horizontal

00:01:56,799 --> 00:02:02,159
coordinate is the vm's memory size

00:02:00,079 --> 00:02:03,200
and the vertical one is the boot time

00:02:02,159 --> 00:02:06,240
with the unit

00:02:03,200 --> 00:02:09,679
second you can see if we

00:02:06,240 --> 00:02:14,720
then 8 gigabytes memory to the vm

00:02:09,679 --> 00:02:16,800
the whole boot time is around 20 seconds

00:02:14,720 --> 00:02:19,280
the boot time will increase along with

00:02:16,800 --> 00:02:21,920
the gas memory going up

00:02:19,280 --> 00:02:22,879
when the gas memory reaches 300

00:02:21,920 --> 00:02:25,599
gigabytes

00:02:22,879 --> 00:02:26,720
the boot time of vm will be upon 2

00:02:25,599 --> 00:02:30,239
minutes

00:02:26,720 --> 00:02:32,640
in this time the users don't know

00:02:30,239 --> 00:02:34,319
what happened in the background and they

00:02:32,640 --> 00:02:37,040
are also not sure if the

00:02:34,319 --> 00:02:39,040
creation is still running just worry by

00:02:37,040 --> 00:02:41,680
the user experience

00:02:39,040 --> 00:02:43,599
we need to figure out the root cause

00:02:41,680 --> 00:02:46,560
then we have the right chart

00:02:43,599 --> 00:02:48,560
it shows the q mu initialization time

00:02:46,560 --> 00:02:50,959
versus memory size

00:02:48,560 --> 00:02:51,599
as you can see most of the boot time is

00:02:50,959 --> 00:02:54,920
in

00:02:51,599 --> 00:02:56,480
qmil initialization and most of the

00:02:54,920 --> 00:02:59,680
initialization time

00:02:56,480 --> 00:03:00,640
is in doing dme map before we reach the

00:02:59,680 --> 00:03:04,080
solution

00:03:00,640 --> 00:03:07,519
we need to know some conditions

00:03:04,080 --> 00:03:10,800
for this problem first it means

00:03:07,519 --> 00:03:14,800
more time cost along with more memory

00:03:10,800 --> 00:03:17,760
second no dma no dma mac

00:03:14,800 --> 00:03:18,000
if there are no devices need to do dma

00:03:17,760 --> 00:03:22,319
of

00:03:18,000 --> 00:03:22,319
course you don't need to do dma map

00:03:22,480 --> 00:03:26,799
but there is one important information

00:03:27,120 --> 00:03:33,760
dma only touch a specific range of

00:03:30,159 --> 00:03:37,200
memory at a certain time it gives

00:03:33,760 --> 00:03:39,519
us a chance to make optimization

00:03:37,200 --> 00:03:40,400
maybe we don't need to pin map all the

00:03:39,519 --> 00:03:43,519
guest memory

00:03:40,400 --> 00:03:46,159
during the creation of vm

00:03:43,519 --> 00:03:48,000
based on the conditions what options can

00:03:46,159 --> 00:03:50,159
we have

00:03:48,000 --> 00:03:51,519
the first thing that comes to mind is

00:03:50,159 --> 00:03:54,799
the virtual iomu

00:03:51,519 --> 00:03:56,640
or pyro virtualization iomu it should be

00:03:54,799 --> 00:04:00,400
a good solution

00:03:56,640 --> 00:04:02,799
but the implementation is very complex

00:04:00,400 --> 00:04:04,720
and it also needs much development

00:04:02,799 --> 00:04:07,280
effort

00:04:04,720 --> 00:04:08,400
so we choose one simple solution we call

00:04:07,280 --> 00:04:12,239
it

00:04:08,400 --> 00:04:15,760
syncdma map there are two key points

00:04:12,239 --> 00:04:17,600
the first one is only map necessary

00:04:15,760 --> 00:04:20,239
memory first

00:04:17,600 --> 00:04:21,759
it ensures the gas operating system boot

00:04:20,239 --> 00:04:24,800
up

00:04:21,759 --> 00:04:27,919
then maps the other memory

00:04:24,800 --> 00:04:29,840
is synchronously in the background it

00:04:27,919 --> 00:04:30,880
might bring a little perception to the

00:04:29,840 --> 00:04:34,000
user

00:04:30,880 --> 00:04:36,479
but it gives better user experience

00:04:34,000 --> 00:04:39,440
this page shows the overview of the

00:04:36,479 --> 00:04:41,600
memory access with a pass-through device

00:04:39,440 --> 00:04:43,919
first let's talk about the current

00:04:41,600 --> 00:04:47,600
status of kvm

00:04:43,919 --> 00:04:50,080
let's use gpu as an example

00:04:47,600 --> 00:04:52,960
the host pin map also gets the memory

00:04:50,080 --> 00:04:56,240
before the guest os boot

00:04:52,960 --> 00:04:58,400
then the gpu driver is loaded and then

00:04:56,240 --> 00:05:00,240
the application generates workloads for

00:04:58,400 --> 00:05:02,080
gpu

00:05:00,240 --> 00:05:03,520
renewable clause comes through gpu

00:05:02,080 --> 00:05:07,520
hardware

00:05:03,520 --> 00:05:10,560
it might trigger dma if the dma address

00:05:07,520 --> 00:05:12,639
hadn't been mapped hardware access arrow

00:05:10,560 --> 00:05:15,280
will have occurred

00:05:12,639 --> 00:05:16,800
our solution is adding the water balloon

00:05:15,280 --> 00:05:19,919
driver

00:05:16,800 --> 00:05:22,960
the water balloon and gpu driver both

00:05:19,919 --> 00:05:24,960
can allocate the system memory

00:05:22,960 --> 00:05:26,240
if the balloon driver is loaded before

00:05:24,960 --> 00:05:29,759
the tpu driver

00:05:26,240 --> 00:05:32,000
it can blow some memory ranges first

00:05:29,759 --> 00:05:32,800
since the gpu driver doesn't have chance

00:05:32,000 --> 00:05:36,240
to allocate

00:05:32,800 --> 00:05:38,880
from these rundays dma won't happen in

00:05:36,240 --> 00:05:42,240
this ranges too

00:05:38,880 --> 00:05:45,440
so it's not necessary to pin map them

00:05:42,240 --> 00:05:49,280
during the creation of guests

00:05:45,440 --> 00:05:51,039
max map them asynchronously is fun

00:05:49,280 --> 00:05:53,680
this page shows the architecture

00:05:51,039 --> 00:05:54,479
overview the solution will touch three

00:05:53,680 --> 00:05:58,960
components

00:05:54,479 --> 00:06:01,759
of kvm motorization the first one is qmu

00:05:58,960 --> 00:06:03,360
it's responsible for triggering dma map

00:06:01,759 --> 00:06:05,199
and the balloon change

00:06:03,360 --> 00:06:08,000
it's also responsible for tracking the

00:06:05,199 --> 00:06:11,039
balloon pages

00:06:08,000 --> 00:06:13,360
the second one is the water open driver

00:06:11,039 --> 00:06:15,680
it's responsible for ballooning pages

00:06:13,360 --> 00:06:18,720
and tiling to the host

00:06:15,680 --> 00:06:23,280
the third one is the vfl driver

00:06:18,720 --> 00:06:23,280
it's responsible for doing pin dma map

00:06:23,360 --> 00:06:26,880
in the qmil initialization the backhand

00:06:26,319 --> 00:06:29,440
driver

00:06:26,880 --> 00:06:32,319
of water balloon in the cumule

00:06:29,440 --> 00:06:34,319
initialize the balloon size of gas

00:06:32,319 --> 00:06:36,880
when the front-end driver of hotel

00:06:34,319 --> 00:06:38,800
balloon in the gas is loaded

00:06:36,880 --> 00:06:40,080
it will query the balloon size

00:06:38,800 --> 00:06:43,120
configuration

00:06:40,080 --> 00:06:46,479
and try to balloon to turn it it needs

00:06:43,120 --> 00:06:48,960
many loops with a little change one time

00:06:46,479 --> 00:06:52,160
every time bloom it will send a

00:06:48,960 --> 00:06:52,160
notification to the host

00:06:52,240 --> 00:06:58,400
the backend driver receives its

00:06:53,919 --> 00:07:00,400
notifications to track the balloon pages

00:06:58,400 --> 00:07:02,639
they are used for generating the balloon

00:07:00,400 --> 00:07:06,080
page table in the host

00:07:02,639 --> 00:07:09,840
the whole page table is generated

00:07:06,080 --> 00:07:12,160
after it is finished the cumule

00:07:09,840 --> 00:07:14,479
called dme map for the memory ranges

00:07:12,160 --> 00:07:16,240
beyond the page table first

00:07:14,479 --> 00:07:17,560
the other memory ranges in the page

00:07:16,240 --> 00:07:21,919
table can be mapped

00:07:17,560 --> 00:07:22,400
economically how about the communication

00:07:21,919 --> 00:07:25,759
channel

00:07:22,400 --> 00:07:27,199
of what io balloon this page shows the

00:07:25,759 --> 00:07:30,560
related functions

00:07:27,199 --> 00:07:31,360
and struct the communication channel is

00:07:30,560 --> 00:07:33,520
ready

00:07:31,360 --> 00:07:37,520
the only thing we need to do is

00:07:33,520 --> 00:07:37,520
recording the bloom pages address

00:07:37,599 --> 00:07:41,199
there are two word queues named inflate

00:07:40,319 --> 00:07:44,479
vq

00:07:41,199 --> 00:07:46,720
and deflate vq the front-end driver used

00:07:44,479 --> 00:07:49,840
the inflate vehicle and deflate vehicle

00:07:46,720 --> 00:07:52,400
to send the notifications to the host

00:07:49,840 --> 00:07:53,280
one handler is attached to these two

00:07:52,400 --> 00:07:56,240
volt queues

00:07:53,280 --> 00:07:57,199
in the backhand driver that is what i

00:07:56,240 --> 00:08:00,400
open

00:07:57,199 --> 00:08:03,440
handle output we can get

00:08:00,400 --> 00:08:04,319
what kill element in this handler it

00:08:03,440 --> 00:08:08,000
contains

00:08:04,319 --> 00:08:11,120
the gas pipe and page number information

00:08:08,000 --> 00:08:12,879
so everything is ready just add a simple

00:08:11,120 --> 00:08:15,520
recording

00:08:12,879 --> 00:08:17,039
logic this page shows the balloon range

00:08:15,520 --> 00:08:20,080
tracking workflow

00:08:17,039 --> 00:08:21,280
it's also very simple in the input

00:08:20,080 --> 00:08:23,360
process

00:08:21,280 --> 00:08:26,240
the front-hand driver sends a input

00:08:23,360 --> 00:08:28,800
notification by inflate vq

00:08:26,240 --> 00:08:31,599
then the background driver receives it

00:08:28,800 --> 00:08:33,519
and dispatches it to the handler

00:08:31,599 --> 00:08:35,039
the backend driver tracks all the

00:08:33,519 --> 00:08:38,399
inflate pages

00:08:35,039 --> 00:08:40,800
and gets the gpa by pfn

00:08:38,399 --> 00:08:43,120
then adds them into the blonde page

00:08:40,800 --> 00:08:46,080
table

00:08:43,120 --> 00:08:47,680
in the deflate process it's similar to

00:08:46,080 --> 00:08:50,399
inflate

00:08:47,680 --> 00:08:52,399
the only difference is the backend

00:08:50,399 --> 00:08:55,120
driver needs to remove

00:08:52,399 --> 00:08:56,959
the released pages out from the blonde

00:08:55,120 --> 00:08:59,200
page table

00:08:56,959 --> 00:09:00,640
this page shows one whole picture of the

00:08:59,200 --> 00:09:03,839
guest boot process

00:09:00,640 --> 00:09:06,880
with async dma map

00:09:03,839 --> 00:09:09,920
there are three phases first one

00:09:06,880 --> 00:09:13,519
is initialization phase two

00:09:09,920 --> 00:09:17,040
is dma map asynchronously

00:09:13,519 --> 00:09:19,839
phase three is completion first one

00:09:17,040 --> 00:09:22,560
begins with the cumule initialization

00:09:19,839 --> 00:09:23,760
the cumule initialize the water balloon

00:09:22,560 --> 00:09:27,600
size

00:09:23,760 --> 00:09:30,560
only the necessary memory for the guest

00:09:27,600 --> 00:09:33,760
then it performs dma map below 4

00:09:30,560 --> 00:09:36,320
gigabytes as euro

00:09:33,760 --> 00:09:37,440
then let's turn to the guest wheel the

00:09:36,320 --> 00:09:39,440
gas os

00:09:37,440 --> 00:09:40,800
will enable the water balloon driver

00:09:39,440 --> 00:09:42,560
first

00:09:40,800 --> 00:09:45,040
then it curious the balloon

00:09:42,560 --> 00:09:45,680
configuration and begins to inflate the

00:09:45,040 --> 00:09:49,839
balloon

00:09:45,680 --> 00:09:52,959
to the target it will call

00:09:49,839 --> 00:09:55,519
fieldbloon to allocate pages and

00:09:52,959 --> 00:09:57,200
tell the host the pi van of the balloon

00:09:55,519 --> 00:09:58,959
pages

00:09:57,200 --> 00:10:01,680
the backend driver receives this

00:09:58,959 --> 00:10:05,440
notification and use the pfn

00:10:01,680 --> 00:10:08,640
to generate the balloon page table

00:10:05,440 --> 00:10:10,399
after the balloon process is finished

00:10:08,640 --> 00:10:12,560
the host will know all the balloon

00:10:10,399 --> 00:10:15,279
memory ranges

00:10:12,560 --> 00:10:17,680
since the balloon memory won't be

00:10:15,279 --> 00:10:20,880
allocated by other devices

00:10:17,680 --> 00:10:23,839
so dma won't happen in this ranges

00:10:20,880 --> 00:10:26,320
so qmio can only map the memory renders

00:10:23,839 --> 00:10:28,079
beyond the page table

00:10:26,320 --> 00:10:31,120
then the password device driver is

00:10:28,079 --> 00:10:31,120
loaded as europe

00:10:31,200 --> 00:10:37,760
phase two cumulative deflate

00:10:34,480 --> 00:10:39,680
balloon step by step the front and

00:10:37,760 --> 00:10:42,880
driver of water balloon

00:10:39,680 --> 00:10:45,279
receives this event will call big

00:10:42,880 --> 00:10:48,640
balloon to deflate

00:10:45,279 --> 00:10:50,720
as same as inflating process cumule can

00:10:48,640 --> 00:10:53,839
receive the different notification

00:10:50,720 --> 00:10:56,720
and get the released pages pfm

00:10:53,839 --> 00:10:58,320
syncomio updates the balloon pitch table

00:10:56,720 --> 00:11:01,360
and trigger dma map

00:10:58,320 --> 00:11:02,640
of the released pages after the balloon

00:11:01,360 --> 00:11:05,920
is empty

00:11:02,640 --> 00:11:06,720
everything is back to normal during the

00:11:05,920 --> 00:11:10,399
practice

00:11:06,720 --> 00:11:10,800
we met several problems then we have

00:11:10,399 --> 00:11:14,240
some

00:11:10,800 --> 00:11:17,440
optimization design for these problems

00:11:14,240 --> 00:11:20,000
the first one is auto combination during

00:11:17,440 --> 00:11:22,959
the inflating process

00:11:20,000 --> 00:11:26,560
the problem is the bloom driver only

00:11:22,959 --> 00:11:28,880
allocate one small page at a time

00:11:26,560 --> 00:11:31,519
and send a notification to the host

00:11:28,880 --> 00:11:34,560
every one megabytes

00:11:31,519 --> 00:11:37,200
the cameo will get huge number pages

00:11:34,560 --> 00:11:38,480
the best method is combining the

00:11:37,200 --> 00:11:40,880
adjacent pages

00:11:38,480 --> 00:11:43,120
and create bigger memory range in the

00:11:40,880 --> 00:11:46,000
brown page table

00:11:43,120 --> 00:11:47,680
actually most of the memory ranges are

00:11:46,000 --> 00:11:49,760
adjacent

00:11:47,680 --> 00:11:50,720
since the balloon driver is loaded out

00:11:49,760 --> 00:11:54,560
very early

00:11:50,720 --> 00:11:54,560
and most of the memory is free

00:11:54,800 --> 00:12:01,120
after the fleeting process finished

00:11:58,959 --> 00:12:04,079
queue will trigger dma map of all the

00:12:01,120 --> 00:12:07,440
memory ranges beyond the pivot table

00:12:04,079 --> 00:12:10,399
it can reduce the dma map times

00:12:07,440 --> 00:12:12,720
second optimization design is increasing

00:12:10,399 --> 00:12:15,920
the balloon page sets

00:12:12,720 --> 00:12:19,120
here is the source code in linux kernel

00:12:15,920 --> 00:12:22,399
you can see bloom page log only locate

00:12:19,120 --> 00:12:23,839
one page at a time 4k page

00:12:22,399 --> 00:12:27,279
is too small for the current

00:12:23,839 --> 00:12:27,279
virtualization environment

00:12:27,600 --> 00:12:31,920
that will import heavy but unnecessary

00:12:30,800 --> 00:12:35,600
communication

00:12:31,920 --> 00:12:37,440
between guest and host if the guest has

00:12:35,600 --> 00:12:41,200
a few hundred or more

00:12:37,440 --> 00:12:42,160
system memory just make a little change

00:12:41,200 --> 00:12:45,600
to use

00:12:42,160 --> 00:12:46,720
lock pages to unlock one large size of

00:12:45,600 --> 00:12:50,160
memory

00:12:46,720 --> 00:12:53,440
for example locating 2 megabytes

00:12:50,160 --> 00:12:54,240
inside of 4 key makes the communication

00:12:53,440 --> 00:12:58,160
much more

00:12:54,240 --> 00:13:02,800
efficient one-time volatile talk

00:12:58,160 --> 00:13:06,160
can inflate or defeat 512

00:13:02,800 --> 00:13:12,000
megabytes memory it can reduce the

00:13:06,160 --> 00:13:12,000
communication frequency significantly

00:13:12,959 --> 00:13:16,399
the third optimization design is

00:13:14,959 --> 00:13:19,279
pre-mapped

00:13:16,399 --> 00:13:20,079
the async dma map can start early

00:13:19,279 --> 00:13:23,600
independent

00:13:20,079 --> 00:13:24,880
of deflating notification qmio triggers

00:13:23,600 --> 00:13:27,440
async dma map

00:13:24,880 --> 00:13:29,760
step by step if there is a new

00:13:27,440 --> 00:13:31,760
notification from deflate vq

00:13:29,760 --> 00:13:33,120
which contains the released pages

00:13:31,760 --> 00:13:36,000
information

00:13:33,120 --> 00:13:36,800
check if they are in the mapped range if

00:13:36,000 --> 00:13:39,839
not

00:13:36,800 --> 00:13:41,519
then map these pages and give ac key to

00:13:39,839 --> 00:13:44,000
the guest

00:13:41,519 --> 00:13:46,560
this optimization design can speed up

00:13:44,000 --> 00:13:49,519
the async dme map process

00:13:46,560 --> 00:13:51,760
last let's see what are the achievements

00:13:49,519 --> 00:13:53,920
of this practice

00:13:51,760 --> 00:13:55,839
this test result is based on the

00:13:53,920 --> 00:13:58,880
initialized balloon size

00:13:55,839 --> 00:14:00,880
it's set as 8 gigabytes

00:13:58,880 --> 00:14:02,959
you can see the cumule initialization

00:14:00,880 --> 00:14:06,079
time is still around

00:14:02,959 --> 00:14:06,480
7 seconds although the guest has more

00:14:06,079 --> 00:14:09,519
than

00:14:06,480 --> 00:14:11,760
300 gigabytes system memory

00:14:09,519 --> 00:14:13,519
the watch click amount can return back

00:14:11,760 --> 00:14:16,000
very quickly

00:14:13,519 --> 00:14:19,040
okay then let's see the guest boot time

00:14:16,000 --> 00:14:21,360
versus memory size

00:14:19,040 --> 00:14:25,120
the boot time of gas hasn't increased

00:14:21,360 --> 00:14:28,000
along with the memory size increase

00:14:25,120 --> 00:14:29,360
you can see the boot time keeps around

00:14:28,000 --> 00:14:32,160
20 seconds

00:14:29,360 --> 00:14:34,800
even though this memory is upon 300

00:14:32,160 --> 00:14:34,800
gigabytes

00:14:35,040 --> 00:14:41,120
so the result shows that this practice

00:14:38,639 --> 00:14:43,199
can speed up the boot time forecast

00:14:41,120 --> 00:14:59,839
significantly

00:14:43,199 --> 00:14:59,839
okay that's it thank you

00:15:00,639 --> 00:15:02,720

YouTube URL: https://www.youtube.com/watch?v=NezDk56-57k


