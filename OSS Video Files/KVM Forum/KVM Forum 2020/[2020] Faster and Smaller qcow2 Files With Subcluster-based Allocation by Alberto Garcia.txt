Title: [2020] Faster and Smaller qcow2 Files With Subcluster-based Allocation by Alberto Garcia
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	qcow2 is QEMU's native format for disk images. qcow2 images are smaller and more flexible than raw files but are also slower. This problem can be partially mitigated by adjusting the cluster size when creating a new qcow2 image. However there is always a trade-off that needs to be considered: smaller cluster sizes result in smaller images and generally faster allocations but also in more metadata and larger memory requirements. Several approaches have been followed in order to improve this situation. In this presentation we introduce subcluster allocation: a new extension for the qcow2 file format that tries to combine the best of both worlds, producing images that are both faster and smaller.


---


Alberto Garcia
Igalia, Software Engineer


Alberto Garcia is a software engineer working at Igalia. He has two decades of professional experience working with Linux-based systems and has been contributing to the QEMU project for more than five. In addition to that he was also involved in the development of the Maemo and MeeGo operating systems, has worked on the GTK port of WebKit and is an active Debian developer.
Captions: 
	00:00:05,040 --> 00:00:08,400
hello

00:00:05,600 --> 00:00:09,519
and thanks for listening my name is

00:00:08,400 --> 00:00:11,599
alberto garcia

00:00:09,519 --> 00:00:13,120
i work for regalia on the qmi project

00:00:11,599 --> 00:00:14,080
and in this presentation i'm going to

00:00:13,120 --> 00:00:16,000
talk about

00:00:14,080 --> 00:00:17,920
the work that i have been doing lately

00:00:16,000 --> 00:00:19,680
and this is related to the q code 2

00:00:17,920 --> 00:00:21,840
file format as you know this is the

00:00:19,680 --> 00:00:24,000
native file format used by qmu and it

00:00:21,840 --> 00:00:28,000
supports many features has encryption

00:00:24,000 --> 00:00:30,240
compression backing files etc

00:00:28,000 --> 00:00:31,439
but the the question that i want to try

00:00:30,240 --> 00:00:33,840
to answer today is

00:00:31,439 --> 00:00:35,360
why is it that sometimes this uh slower

00:00:33,840 --> 00:00:36,800
than a raw file

00:00:35,360 --> 00:00:38,399
there's many reasons for that it can be

00:00:36,800 --> 00:00:39,680
because it hasn't been configured

00:00:38,399 --> 00:00:42,160
correctly we're not using the right

00:00:39,680 --> 00:00:44,399
options for our setup

00:00:42,160 --> 00:00:45,280
of course it can be that the driver can

00:00:44,399 --> 00:00:47,120
be improved and

00:00:45,280 --> 00:00:48,800
there's uh still room for improvement

00:00:47,120 --> 00:00:50,960
there um

00:00:48,800 --> 00:00:52,160
three years ago in the kvm forum 2017 i

00:00:50,960 --> 00:00:54,000
was

00:00:52,160 --> 00:00:55,680
talking about these things so you can

00:00:54,000 --> 00:00:57,760
check the talk it's in youtube it's

00:00:55,680 --> 00:00:59,920
available

00:00:57,760 --> 00:01:01,359
today i want to focus on the problems

00:00:59,920 --> 00:01:05,680
that are a result of the

00:01:01,359 --> 00:01:07,920
very design of the qco2 file format

00:01:05,680 --> 00:01:09,360
so let's start with the format itself

00:01:07,920 --> 00:01:11,280
how how it works

00:01:09,360 --> 00:01:13,360
the basic idea of the the quicker qco2

00:01:11,280 --> 00:01:14,880
file it is it is divided into clusters

00:01:13,360 --> 00:01:18,159
of the same size

00:01:14,880 --> 00:01:20,159
64k by default but it can be changed

00:01:18,159 --> 00:01:23,439
when the file is created

00:01:20,159 --> 00:01:25,040
from 512 bytes to up to 2 megabytes

00:01:23,439 --> 00:01:27,840
there are different cluster size i'm not

00:01:25,040 --> 00:01:29,840
going to go into the details now but

00:01:27,840 --> 00:01:31,280
let's focus on the data cluster which

00:01:29,840 --> 00:01:34,000
contains the

00:01:31,280 --> 00:01:35,600
data that the guest can see so every

00:01:34,000 --> 00:01:38,479
time the guest

00:01:35,600 --> 00:01:38,880
needs to read data it goes to the qcoto

00:01:38,479 --> 00:01:40,960
file

00:01:38,880 --> 00:01:42,320
is the the data the cluster has been

00:01:40,960 --> 00:01:44,960
allocated there then it

00:01:42,320 --> 00:01:46,240
just reads the data for the cluster but

00:01:44,960 --> 00:01:46,799
if the cluster hasn't been allocated

00:01:46,240 --> 00:01:48,720
then

00:01:46,799 --> 00:01:50,159
the it contains zeros or if there's a

00:01:48,720 --> 00:01:52,880
working file it goes to the backup file

00:01:50,159 --> 00:01:55,600
and checks the data is there

00:01:52,880 --> 00:01:57,280
so the cluster is the smallest unit of

00:01:55,600 --> 00:01:58,560
allocation so every time we allocate a

00:01:57,280 --> 00:02:00,399
new cluster

00:01:58,560 --> 00:02:02,159
if the write request is smaller than the

00:02:00,399 --> 00:02:03,439
cluster size then we need to fill the

00:02:02,159 --> 00:02:06,000
rest with data

00:02:03,439 --> 00:02:07,759
and the data means we either go to the

00:02:06,000 --> 00:02:10,160
backing file and get it from there

00:02:07,759 --> 00:02:10,879
so in the case of in this image that you

00:02:10,160 --> 00:02:13,360
see

00:02:10,879 --> 00:02:14,800
we will write the area in pink we will

00:02:13,360 --> 00:02:15,520
need to go to the backing file and read

00:02:14,800 --> 00:02:17,200
the

00:02:15,520 --> 00:02:19,040
the areas in dark blue and copy them to

00:02:17,200 --> 00:02:20,560
the active file

00:02:19,040 --> 00:02:22,080
or if there's no viking file we just

00:02:20,560 --> 00:02:23,840
fill it with zeros but we still need to

00:02:22,080 --> 00:02:25,440
feel it

00:02:23,840 --> 00:02:27,200
so the problem is that of course qmu

00:02:25,440 --> 00:02:28,640
needs to perform additional i o to copy

00:02:27,200 --> 00:02:29,280
the rest of the data so the copy and

00:02:28,640 --> 00:02:32,080
write this and

00:02:29,280 --> 00:02:33,280
can be an expensive operation as you can

00:02:32,080 --> 00:02:34,959
imagine the

00:02:33,280 --> 00:02:36,480
when we increase the cluster size we

00:02:34,959 --> 00:02:37,920
have to do more copy and write so the

00:02:36,480 --> 00:02:39,440
performance goes down you can see that

00:02:37,920 --> 00:02:40,800
in the table

00:02:39,440 --> 00:02:42,640
i had to mention though that if you

00:02:40,800 --> 00:02:45,680
don't have a backing file then

00:02:42,640 --> 00:02:48,400
as i said we should fill the uh

00:02:45,680 --> 00:02:50,160
the cluster with zeros but qmi nowadays

00:02:48,400 --> 00:02:52,720
uses f allocator try to

00:02:50,160 --> 00:02:54,319
to fill it in a more efficient way so if

00:02:52,720 --> 00:02:55,680
that works the file system supports it

00:02:54,319 --> 00:02:57,120
and the operating system supported this

00:02:55,680 --> 00:02:58,319
is very fast and then the cluster size

00:02:57,120 --> 00:02:59,920
doesn't have any effect

00:02:58,319 --> 00:03:02,400
but if that doesn't work then it goes to

00:02:59,920 --> 00:03:03,840
the slow path of writing actual zeros to

00:03:02,400 --> 00:03:06,480
disk and then you can see the numbers

00:03:03,840 --> 00:03:08,159
that i'm showing in the table

00:03:06,480 --> 00:03:10,959
if there's a vacuum file however there's

00:03:08,159 --> 00:03:11,760
no no alternative we need to go to the

00:03:10,959 --> 00:03:14,800
backing file and

00:03:11,760 --> 00:03:16,879
get the data from there so that's

00:03:14,800 --> 00:03:19,280
that's where the the cluster has an

00:03:16,879 --> 00:03:20,800
effect

00:03:19,280 --> 00:03:22,800
the other consequence of this is of

00:03:20,800 --> 00:03:25,360
course the larger the cluster size

00:03:22,800 --> 00:03:27,680
we do more i o we do more copy and write

00:03:25,360 --> 00:03:30,480
and then the image

00:03:27,680 --> 00:03:31,920
is bigger you write the same data but

00:03:30,480 --> 00:03:33,200
you get a bigger image and result

00:03:31,920 --> 00:03:34,799
because you are duplicating data from

00:03:33,200 --> 00:03:36,879
the backing file

00:03:34,799 --> 00:03:39,519
how much well this depends a lot on the

00:03:36,879 --> 00:03:42,560
use case here reports that

00:03:39,519 --> 00:03:44,480
can be 30 larger or 40

00:03:42,560 --> 00:03:46,000
larger but it depends a lot on your use

00:03:44,480 --> 00:03:48,239
case i was just doing a

00:03:46,000 --> 00:03:49,120
couple of tests for this presentation

00:03:48,239 --> 00:03:50,720
and

00:03:49,120 --> 00:03:52,239
you can see that if we have an empty

00:03:50,720 --> 00:03:55,840
image and we write

00:03:52,239 --> 00:03:59,280
100 megabyte worth of

00:03:55,840 --> 00:04:02,640
random 4k requests the the impact of

00:03:59,280 --> 00:04:05,120
having a larger cluster size is very big

00:04:02,640 --> 00:04:08,080
we have a default 64k

00:04:05,120 --> 00:04:09,439
we get a 1.6 gigabyte image which is

00:04:08,080 --> 00:04:11,840
more than 10 times the

00:04:09,439 --> 00:04:12,560
what we were trying to write which is a

00:04:11,840 --> 00:04:14,080
lot

00:04:12,560 --> 00:04:15,599
but if we look at the maximum cluster

00:04:14,080 --> 00:04:18,880
size we get

00:04:15,599 --> 00:04:20,400
29 gigabytes which is 300 times the the

00:04:18,880 --> 00:04:21,919
initial the amount of data that we want

00:04:20,400 --> 00:04:22,639
to write which is very big of course

00:04:21,919 --> 00:04:25,919
this is a

00:04:22,639 --> 00:04:28,479
extreme case normally in a real world

00:04:25,919 --> 00:04:30,160
scenario we don't just write random

00:04:28,479 --> 00:04:31,919
write request

00:04:30,160 --> 00:04:33,600
but it gives an idea of what the problem

00:04:31,919 --> 00:04:36,240
is

00:04:33,600 --> 00:04:37,919
then i did a second test i took an empty

00:04:36,240 --> 00:04:40,479
one terabyte image and i created a

00:04:37,919 --> 00:04:42,320
file system there and you can see the

00:04:40,479 --> 00:04:43,440
file system itself the metadata used by

00:04:42,320 --> 00:04:47,040
the file system which

00:04:43,440 --> 00:04:48,880
is just well is 1.1.1 gigabyte

00:04:47,040 --> 00:04:50,720
but if you increase the cluster size and

00:04:48,880 --> 00:04:52,240
you take it to the maximum then you use

00:04:50,720 --> 00:04:54,160
one more gigabyte

00:04:52,240 --> 00:04:56,880
just for creating a file system an empty

00:04:54,160 --> 00:05:00,320
file system with nothing else in it

00:04:56,880 --> 00:05:01,520
so so in summary if we increase the

00:05:00,320 --> 00:05:03,440
cluster size

00:05:01,520 --> 00:05:06,479
we get less performance because there's

00:05:03,440 --> 00:05:08,479
additional i o that needs to be done

00:05:06,479 --> 00:05:10,479
and we also get larger larger images and

00:05:08,479 --> 00:05:12,400
duplicate data so

00:05:10,479 --> 00:05:15,120
things clear then we don't just just

00:05:12,400 --> 00:05:17,199
reduce the cluster size no

00:05:15,120 --> 00:05:19,520
problems that is not so easy because

00:05:17,199 --> 00:05:21,039
smaller clusters means more metadata or

00:05:19,520 --> 00:05:24,560
more cluster source

00:05:21,039 --> 00:05:26,080
so what does this apart from the guess

00:05:24,560 --> 00:05:28,080
that data itself

00:05:26,080 --> 00:05:29,199
qco2 images also need to store metadata

00:05:28,080 --> 00:05:31,520
about the the

00:05:29,199 --> 00:05:34,240
cluster so important things are the

00:05:31,520 --> 00:05:37,360
cluster mappings which map the guest

00:05:34,240 --> 00:05:40,320
addresses to the host addresses

00:05:37,360 --> 00:05:41,759
and the reference count all clusters in

00:05:40,320 --> 00:05:43,360
github to have reference counts how

00:05:41,759 --> 00:05:45,919
we're going to see later

00:05:43,360 --> 00:05:47,840
so if we're going to have more clusters

00:05:45,919 --> 00:05:51,039
we're going to have more of them so

00:05:47,840 --> 00:05:51,039
it means more metadata

00:05:51,199 --> 00:05:54,240
so the the mapping from the guest

00:05:53,360 --> 00:05:57,199
clusters to the

00:05:54,240 --> 00:05:58,880
the guest offices to the host offices uh

00:05:57,199 --> 00:05:59,919
is done using this structure that we

00:05:58,880 --> 00:06:02,880
call l21

00:05:59,919 --> 00:06:03,840
uh l1 and l2 tables this is a simple

00:06:02,880 --> 00:06:07,280
structure uh

00:06:03,840 --> 00:06:10,000
that maps virtual offsets into

00:06:07,280 --> 00:06:11,759
host offset you can see our example here

00:06:10,000 --> 00:06:14,800
in this graphic

00:06:11,759 --> 00:06:16,880
the l1 table is just one per image

00:06:14,800 --> 00:06:18,720
for snapshot actually because the qgo2

00:06:16,880 --> 00:06:22,080
format can have several snapshots but

00:06:18,720 --> 00:06:24,400
we're not going to go into that now

00:06:22,080 --> 00:06:26,880
but the table itself is very small uh

00:06:24,400 --> 00:06:30,240
it's for a one terabyte image it's just

00:06:26,880 --> 00:06:32,800
16k so it's nothing it's stored

00:06:30,240 --> 00:06:34,639
contiguously in the image file

00:06:32,800 --> 00:06:36,000
and um always keeps it in memory because

00:06:34,639 --> 00:06:37,440
it's very small so it's not there's no

00:06:36,000 --> 00:06:39,360
problem with that

00:06:37,440 --> 00:06:42,160
and basically the table just contains

00:06:39,360 --> 00:06:43,919
pointers to the l2 tables

00:06:42,160 --> 00:06:45,759
digital tables there can be many of them

00:06:43,919 --> 00:06:47,280
and initially there's none but they

00:06:45,759 --> 00:06:49,199
they are allocated in demand as the

00:06:47,280 --> 00:06:51,120
image grows

00:06:49,199 --> 00:06:53,840
delta tables are always one cluster in

00:06:51,120 --> 00:06:55,680
size never more nevertheless

00:06:53,840 --> 00:06:57,360
and they also contain basically good

00:06:55,680 --> 00:06:59,199
pointers to the to the

00:06:57,360 --> 00:07:00,840
data clusters plus some additional

00:06:59,199 --> 00:07:02,080
information that we're going to see

00:07:00,840 --> 00:07:03,759
later

00:07:02,080 --> 00:07:05,599
thing is that of course if we reduce the

00:07:03,759 --> 00:07:07,360
cluster size then we need

00:07:05,599 --> 00:07:08,800
more entries so graphically we have two

00:07:07,360 --> 00:07:11,360
clusters and we

00:07:08,800 --> 00:07:12,800
make the clusters twice as small then

00:07:11,360 --> 00:07:13,120
we're going to have four clusters and we

00:07:12,800 --> 00:07:16,319
need

00:07:13,120 --> 00:07:18,639
four entries this time so

00:07:16,319 --> 00:07:21,599
half the cluster size twice the metadata

00:07:18,639 --> 00:07:21,599
that's the basic idea

00:07:21,759 --> 00:07:25,520
here we see the table what's the the

00:07:23,680 --> 00:07:26,080
maximum metadata for a one terabyte

00:07:25,520 --> 00:07:28,639
image

00:07:26,080 --> 00:07:30,639
as you see if you reduce the cluster by

00:07:28,639 --> 00:07:32,400
size by half you increase metadata by

00:07:30,639 --> 00:07:34,639
two

00:07:32,400 --> 00:07:35,840
which is a very big uh difference of

00:07:34,639 --> 00:07:37,440
course so

00:07:35,840 --> 00:07:38,639
choosing the right cluster size has a

00:07:37,440 --> 00:07:39,599
very big impact on the amount of

00:07:38,639 --> 00:07:42,319
metadata that

00:07:39,599 --> 00:07:42,319
you have in the image

00:07:42,800 --> 00:07:46,160
so what does this mean every time we

00:07:45,039 --> 00:07:50,639
need to

00:07:46,160 --> 00:07:53,520
do a io request in the from the guest

00:07:50,639 --> 00:07:54,240
qrami needs to go to the l2 table and

00:07:53,520 --> 00:07:56,400
get the

00:07:54,240 --> 00:07:58,240
host offset let's transform the guest

00:07:56,400 --> 00:08:00,160
offset into the host offset so that's

00:07:58,240 --> 00:08:00,960
one additional i operation per request

00:08:00,160 --> 00:08:02,639
and this is a

00:08:00,960 --> 00:08:04,160
has a very big import impacting

00:08:02,639 --> 00:08:07,199
performance

00:08:04,160 --> 00:08:09,120
so what qmu does in order to minimize it

00:08:07,199 --> 00:08:11,759
is it keeps the f2 tables in memory

00:08:09,120 --> 00:08:14,639
there's a cubecode to cache

00:08:11,759 --> 00:08:15,440
the l2 cache for that purpose i was

00:08:14,639 --> 00:08:16,960
talking about

00:08:15,440 --> 00:08:19,759
about it in more detail in the previous

00:08:16,960 --> 00:08:21,360
presentation that i mentioned earlier

00:08:19,759 --> 00:08:22,800
and it has a very big impact if we

00:08:21,360 --> 00:08:24,720
increase the cluster size we get

00:08:22,800 --> 00:08:26,319
much more performance so in the case of

00:08:24,720 --> 00:08:28,879
in this example that you see here

00:08:26,319 --> 00:08:30,879
the maximum cache need is five megabytes

00:08:28,879 --> 00:08:33,680
and we get

00:08:30,879 --> 00:08:35,039
40 000 operations per second but we

00:08:33,680 --> 00:08:37,440
reduce the cluster size

00:08:35,039 --> 00:08:38,240
the cache size sorry that the

00:08:37,440 --> 00:08:39,680
performance code

00:08:38,240 --> 00:08:41,760
goes down very quickly because it means

00:08:39,680 --> 00:08:44,159
that there's no

00:08:41,760 --> 00:08:46,720
we need to go to disk to get the l2

00:08:44,159 --> 00:08:50,000
metadata more often

00:08:46,720 --> 00:08:52,720
so reducing the cluster size means

00:08:50,000 --> 00:08:53,600
we have much more metadata and we have

00:08:52,720 --> 00:08:55,600
much more

00:08:53,600 --> 00:08:58,000
ram than we need to keep that metadata

00:08:55,600 --> 00:08:58,000
memory

00:08:58,560 --> 00:09:01,600
then there's the reference counts every

00:09:00,160 --> 00:09:02,800
cluster in a key code image has a

00:09:01,600 --> 00:09:04,240
reference count

00:09:02,800 --> 00:09:05,519
all types of cluster not just data

00:09:04,240 --> 00:09:07,680
cluster these are used for example for

00:09:05,519 --> 00:09:10,080
snapshots because you need to know

00:09:07,680 --> 00:09:11,279
who is using each one of the clusters

00:09:10,080 --> 00:09:12,160
and they are storing a two-level

00:09:11,279 --> 00:09:13,600
structure

00:09:12,160 --> 00:09:15,279
called reference table and reference

00:09:13,600 --> 00:09:17,279
block it's very similar to the

00:09:15,279 --> 00:09:18,880
the one and the two tables that we just

00:09:17,279 --> 00:09:20,880
described

00:09:18,880 --> 00:09:22,560
so of course allocating new cluster has

00:09:20,880 --> 00:09:24,080
additional overhead

00:09:22,560 --> 00:09:26,240
because you need to update the reference

00:09:24,080 --> 00:09:27,920
counts so with smaller clusters we need

00:09:26,240 --> 00:09:29,360
to allocate more of them

00:09:27,920 --> 00:09:31,040
so in general we have a lot of small

00:09:29,360 --> 00:09:32,640
clusters we need to allocate first more

00:09:31,040 --> 00:09:34,320
clusters we need to locate more l2

00:09:32,640 --> 00:09:37,120
tables we need to allocate more

00:09:34,320 --> 00:09:39,680
reference blocks and all that together

00:09:37,120 --> 00:09:41,279
means that the

00:09:39,680 --> 00:09:43,600
although normally reducing the cluster

00:09:41,279 --> 00:09:45,760
size increases the performance because

00:09:43,600 --> 00:09:48,560
there's less copy and write involved

00:09:45,760 --> 00:09:50,720
once we go uh under a certain limit in

00:09:48,560 --> 00:09:53,120
this example is less than 16k

00:09:50,720 --> 00:09:54,480
the performance goes down very quickly

00:09:53,120 --> 00:09:55,200
as you can see the performance when we

00:09:54,480 --> 00:09:58,240
have

00:09:55,200 --> 00:10:00,080
4k clusters is horrible

00:09:58,240 --> 00:10:01,760
even though with 4k clusters there is no

00:10:00,080 --> 00:10:03,360
copy on right but there's we have to

00:10:01,760 --> 00:10:05,519
allocate so many clusters so many

00:10:03,360 --> 00:10:06,399
l2 tables so many reference blocks that

00:10:05,519 --> 00:10:09,360
is the

00:10:06,399 --> 00:10:09,360
performance is very bad

00:10:09,920 --> 00:10:13,760
so the situation so far is that we

00:10:12,240 --> 00:10:17,200
cannot have two big clusters because

00:10:13,760 --> 00:10:18,800
they waste too much space and there's

00:10:17,200 --> 00:10:20,480
additional i o needed for a copy and

00:10:18,800 --> 00:10:21,519
write and we cannot have two small

00:10:20,480 --> 00:10:22,800
clusters because they increase the

00:10:21,519 --> 00:10:25,120
amount of metadata and

00:10:22,800 --> 00:10:26,160
if we decrease it too much then it's

00:10:25,120 --> 00:10:28,560
also very bad

00:10:26,160 --> 00:10:29,760
performance and this is a direct

00:10:28,560 --> 00:10:31,200
consequence of the

00:10:29,760 --> 00:10:33,200
the format itself is not something that

00:10:31,200 --> 00:10:36,480
you can fix in the driver so

00:10:33,200 --> 00:10:38,240
what can we do about it so

00:10:36,480 --> 00:10:39,760
the solution that i'm describing in this

00:10:38,240 --> 00:10:41,600
presentation is

00:10:39,760 --> 00:10:43,600
called sub-cluster allocation and the

00:10:41,600 --> 00:10:44,560
basic idea is that we have big clusters

00:10:43,600 --> 00:10:45,760
in order to reduce the amount of

00:10:44,560 --> 00:10:47,200
metadata

00:10:45,760 --> 00:10:49,279
but each one of them is divided into

00:10:47,200 --> 00:10:50,720
sub-clusters that can be allocated

00:10:49,279 --> 00:10:53,120
separately

00:10:50,720 --> 00:10:54,560
so we have faster allocations and less

00:10:53,120 --> 00:10:58,079
disk usage

00:10:54,560 --> 00:10:59,279
so graphically a normal l2 table

00:10:58,079 --> 00:11:00,410
looks like this we have two data

00:10:59,279 --> 00:11:01,600
clusters as you can see

00:11:00,410 --> 00:11:03,279
[Music]

00:11:01,600 --> 00:11:05,120
with some clusters each one of the data

00:11:03,279 --> 00:11:08,800
clusters divided into

00:11:05,120 --> 00:11:11,040
32 subclusters of the same size

00:11:08,800 --> 00:11:12,800
um they are allocated separately so in

00:11:11,040 --> 00:11:14,480
this case only the rs and blue

00:11:12,800 --> 00:11:16,800
are actually allocated and using space

00:11:14,480 --> 00:11:16,800
and disk

00:11:17,760 --> 00:11:20,959
uh internally the l2 table contains as i

00:11:20,160 --> 00:11:22,880
said earlier

00:11:20,959 --> 00:11:24,720
pointers to the data clusters basically

00:11:22,880 --> 00:11:26,320
it looks like this is the cluster offset

00:11:24,720 --> 00:11:27,680
plus a few more bits that indicate

00:11:26,320 --> 00:11:30,079
whether the cluster is

00:11:27,680 --> 00:11:31,600
allocated or not is compressed or not or

00:11:30,079 --> 00:11:33,760
it contains zeros

00:11:31,600 --> 00:11:35,040
contain zeros is a feature from qco2

00:11:33,760 --> 00:11:36,640
that means that the

00:11:35,040 --> 00:11:38,160
cluster doesn't have any other data

00:11:36,640 --> 00:11:40,160
other than zero so there's no need to go

00:11:38,160 --> 00:11:41,680
to the data cluster and read from there

00:11:40,160 --> 00:11:43,440
we just we just know that it's zeros and

00:11:41,680 --> 00:11:47,519
we can return zeros without

00:11:43,440 --> 00:11:48,880
doing the i o so we have some clusters

00:11:47,519 --> 00:11:51,040
we need to store additional information

00:11:48,880 --> 00:11:54,000
for that and there's no space here

00:11:51,040 --> 00:11:55,120
so we have uh we added this extended l2

00:11:54,000 --> 00:11:56,639
entries which is

00:11:55,120 --> 00:11:58,399
basically very similar to the ones that

00:11:56,639 --> 00:12:00,240
we had before but they contain an

00:11:58,399 --> 00:12:01,760
additional bitmap indicating the status

00:12:00,240 --> 00:12:03,519
of each sub-cluster

00:12:01,760 --> 00:12:05,120
so with this each one of the individuals

00:12:03,519 --> 00:12:09,519
of clusters can be allocated

00:12:05,120 --> 00:12:11,279
unallocated or can be all zeros also

00:12:09,519 --> 00:12:13,600
um compressed clusters don't have this

00:12:11,279 --> 00:12:15,519
however complex clusters

00:12:13,600 --> 00:12:18,560
they cannot be divided into sub-clusters

00:12:15,519 --> 00:12:20,240
and anyway compressed clusters

00:12:18,560 --> 00:12:22,399
there doesn't really make so much sense

00:12:20,240 --> 00:12:24,240
to to use compression with the

00:12:22,399 --> 00:12:27,200
extended l2 entries because there are

00:12:24,240 --> 00:12:27,200
different use cases

00:12:27,440 --> 00:12:31,279
so these cases that i see for the

00:12:28,959 --> 00:12:32,560
subclass relation are two

00:12:31,279 --> 00:12:34,079
one of them is having very large

00:12:32,560 --> 00:12:34,720
clusters because we want to minimize the

00:12:34,079 --> 00:12:36,320
amount of

00:12:34,720 --> 00:12:38,240
metadata and the amount of memory that

00:12:36,320 --> 00:12:41,519
we that we need

00:12:38,240 --> 00:12:45,120
but still want to have good io and

00:12:41,519 --> 00:12:46,160
have smaller images and the other use

00:12:45,120 --> 00:12:47,519
case is that we want to

00:12:46,160 --> 00:12:49,600
maximize the performance so we want to

00:12:47,519 --> 00:12:50,720
have the keep the location unit as close

00:12:49,600 --> 00:12:53,839
as possible to the

00:12:50,720 --> 00:12:53,839
to the

00:12:54,399 --> 00:13:00,000
block uh guest block size

00:12:58,480 --> 00:13:01,360
so we want to minimize the amount of

00:13:00,000 --> 00:13:02,720
copy and write and get the maximum

00:13:01,360 --> 00:13:05,120
performance

00:13:02,720 --> 00:13:06,959
so what does this mean as i said if we

00:13:05,120 --> 00:13:08,079
make the sub cluster size equal to the

00:13:06,959 --> 00:13:10,560
request size

00:13:08,079 --> 00:13:11,920
it means the the files the file system

00:13:10,560 --> 00:13:12,959
block size then we get the there's no

00:13:11,920 --> 00:13:14,160
copy and write at all and we get the

00:13:12,959 --> 00:13:16,160
maximum performance

00:13:14,160 --> 00:13:18,240
we can see here that compared to the

00:13:16,160 --> 00:13:21,040
previous to the default uh

00:13:18,240 --> 00:13:21,680
setup without sub clusters in some cases

00:13:21,040 --> 00:13:25,760
we get

00:13:21,680 --> 00:13:28,880
10 times more io operations per second

00:13:25,760 --> 00:13:31,120
and if we go to the cases where

00:13:28,880 --> 00:13:32,800
the sub-cluster is 4k or less which is

00:13:31,120 --> 00:13:33,680
the the size of the request in this

00:13:32,800 --> 00:13:35,040
example

00:13:33,680 --> 00:13:37,600
then we get the maximum performance

00:13:35,040 --> 00:13:41,120
which is 12 13

00:13:37,600 --> 00:13:41,120
k i operation per second

00:13:42,240 --> 00:13:45,199
without the backing file the relative

00:13:43,760 --> 00:13:46,639
differences are the same of course it is

00:13:45,199 --> 00:13:49,120
faster because we don't need to go to

00:13:46,639 --> 00:13:50,959
the backing file to read the data and

00:13:49,120 --> 00:13:53,040
again i want to mention that if the file

00:13:50,959 --> 00:13:54,880
system supports

00:13:53,040 --> 00:13:56,480
emptying the cluster with f allocate

00:13:54,880 --> 00:13:57,920
then this

00:13:56,480 --> 00:13:59,440
is going to be much faster than this and

00:13:57,920 --> 00:14:00,079
then actually using sub cluster doesn't

00:13:59,440 --> 00:14:01,760
really

00:14:00,079 --> 00:14:04,399
make a difference it's not going to be

00:14:01,760 --> 00:14:07,440
faster with that

00:14:04,399 --> 00:14:10,079
so you have to consider that

00:14:07,440 --> 00:14:12,079
about the the space of course if we have

00:14:10,079 --> 00:14:15,519
now the smaller allocation

00:14:12,079 --> 00:14:18,160
units then the images grow

00:14:15,519 --> 00:14:18,800
much less so we compared the random

00:14:18,160 --> 00:14:20,720
writes that i

00:14:18,800 --> 00:14:21,839
that i mentioned before we write 100

00:14:20,720 --> 00:14:25,199
megabytes

00:14:21,839 --> 00:14:26,959
in 4k write requests the end result is

00:14:25,199 --> 00:14:30,000
much much smaller as you can see in the

00:14:26,959 --> 00:14:33,920
example so so of course

00:14:30,000 --> 00:14:36,959
this is a improves a lot the

00:14:33,920 --> 00:14:39,920
how we use the disk and

00:14:36,959 --> 00:14:43,760
although l2 extent delta entries are

00:14:39,920 --> 00:14:47,040
twice as large as normal l2 entries

00:14:43,760 --> 00:14:48,959
we would uh in principle it would use uh

00:14:47,040 --> 00:14:50,320
more metadata however since each one of

00:14:48,959 --> 00:14:54,720
the l2

00:14:50,320 --> 00:14:57,120
entries now points to 6 32 subclusters

00:14:54,720 --> 00:14:58,720
the end result is that we have 16 times

00:14:57,120 --> 00:15:00,320
less metadata for the same unit of

00:14:58,720 --> 00:15:01,680
allocation so we compared units of

00:15:00,320 --> 00:15:04,720
allocation

00:15:01,680 --> 00:15:08,079
clusters in traditional l2 entries and

00:15:04,720 --> 00:15:11,120
sub clusters in extended l2 entries

00:15:08,079 --> 00:15:13,279
we see that for a 64k cluster size we

00:15:11,120 --> 00:15:16,000
would need 128

00:15:13,279 --> 00:15:17,360
megabytes of cache for a one terabyte

00:15:16,000 --> 00:15:19,279
image

00:15:17,360 --> 00:15:22,079
but with external end to entries if we

00:15:19,279 --> 00:15:23,680
have 64k sub clusters we only need eight

00:15:22,079 --> 00:15:27,839
megabytes which is much less

00:15:23,680 --> 00:15:30,000
so we can have much larger clusters and

00:15:27,839 --> 00:15:33,360
keep good performance and without

00:15:30,000 --> 00:15:35,199
needing so much memory for the cache

00:15:33,360 --> 00:15:36,800
so things that need to be taken into

00:15:35,199 --> 00:15:39,040
account all this looks good but this is

00:15:36,800 --> 00:15:40,720
not magic so

00:15:39,040 --> 00:15:42,639
this feature is useful during allocation

00:15:40,720 --> 00:15:47,040
once the cluster is allocated

00:15:42,639 --> 00:15:48,639
there's no qco2 works just fine

00:15:47,040 --> 00:15:50,560
with or without sub-clusters and you get

00:15:48,639 --> 00:15:52,160
good performance so once the

00:15:50,560 --> 00:15:55,440
image is located this is not going to

00:15:52,160 --> 00:15:57,199
really help so much more

00:15:55,440 --> 00:15:58,560
again with compressed with compressed

00:15:57,199 --> 00:16:00,480
images it doesn't make sense i think

00:15:58,560 --> 00:16:02,240
it's a completely different use case

00:16:00,480 --> 00:16:04,959
so this is not going to give you any

00:16:02,240 --> 00:16:07,279
benefit you're going to have

00:16:04,959 --> 00:16:08,480
twice as much metadata and you're not

00:16:07,279 --> 00:16:11,040
going to see anything

00:16:08,480 --> 00:16:11,040
any benefit

00:16:11,600 --> 00:16:15,519
and if your image doesn't have any

00:16:14,240 --> 00:16:16,720
backing file maybe you don't see any

00:16:15,519 --> 00:16:20,959
speed up as i said

00:16:16,720 --> 00:16:23,600
qemu tries first to use f allocate to

00:16:20,959 --> 00:16:24,880
allocate clusters efficiently so you

00:16:23,600 --> 00:16:26,399
have to try that first to see if it

00:16:24,880 --> 00:16:26,880
helps in your scenario but if you are

00:16:26,399 --> 00:16:28,399
using

00:16:26,880 --> 00:16:30,160
backing files then it will help in any

00:16:28,399 --> 00:16:32,639
case

00:16:30,160 --> 00:16:34,480
and then of course images created with

00:16:32,639 --> 00:16:35,600
this extended l2 entries are not going

00:16:34,480 --> 00:16:37,839
to be

00:16:35,600 --> 00:16:39,920
possible you are not going to be able to

00:16:37,839 --> 00:16:41,680
read them with other versions of qmu

00:16:39,920 --> 00:16:43,839
and i don't expect that this feature can

00:16:41,680 --> 00:16:47,440
be backported easily so

00:16:43,839 --> 00:16:50,000
you will need the latest versions of qmu

00:16:47,440 --> 00:16:51,360
so how do i try this this is not

00:16:50,000 --> 00:16:54,000
available in camera yet

00:16:51,360 --> 00:16:54,720
it's nothing in it release is it will

00:16:54,000 --> 00:16:57,279
probably be

00:16:54,720 --> 00:16:58,639
available in game of 5.2 but the feature

00:16:57,279 --> 00:16:59,839
is complete and it's already in the

00:16:58,639 --> 00:17:01,519
repository and

00:16:59,839 --> 00:17:05,280
you can test it already so you just

00:17:01,519 --> 00:17:07,120
download the latest version from kit

00:17:05,280 --> 00:17:09,520
you compile it and you create an image

00:17:07,120 --> 00:17:12,480
with the option extended l2

00:17:09,520 --> 00:17:13,919
enabled and that's all you also probably

00:17:12,480 --> 00:17:15,600
want to have a

00:17:13,919 --> 00:17:17,280
larger cluster size the default cluster

00:17:15,600 --> 00:17:19,039
size is 64k but

00:17:17,280 --> 00:17:20,319
with this feature it makes sense to use

00:17:19,039 --> 00:17:23,600
larger clusters so

00:17:20,319 --> 00:17:26,400
be sure to try those but that's all that

00:17:23,600 --> 00:17:28,799
you need to do there's no nothing else

00:17:26,400 --> 00:17:30,080
again feedback back reports etc are very

00:17:28,799 --> 00:17:32,720
much appreciated this feature is

00:17:30,080 --> 00:17:33,760
complete but it's new so any testing

00:17:32,720 --> 00:17:36,000
anything that you

00:17:33,760 --> 00:17:37,200
try suggestions etc we will be happy to

00:17:36,000 --> 00:17:38,160
hear about them you can write to the

00:17:37,200 --> 00:17:40,799
mailing list

00:17:38,160 --> 00:17:42,160
or you can contact me directly and

00:17:40,799 --> 00:17:43,600
that's basically it

00:17:42,160 --> 00:17:45,360
i would also like to take the

00:17:43,600 --> 00:17:47,039
opportunity to thank outscale which is

00:17:45,360 --> 00:17:48,960
the company that is sponsoring all my

00:17:47,039 --> 00:17:49,760
work in qmu and in this feature in

00:17:48,960 --> 00:17:51,039
particular so

00:17:49,760 --> 00:17:53,600
everything that you are seeing here

00:17:51,039 --> 00:17:56,320
today is thanks to their sponsoring

00:17:53,600 --> 00:17:57,760
and this is all um i hope that you

00:17:56,320 --> 00:18:01,840
enjoyed the presentation and

00:17:57,760 --> 00:18:01,840

YouTube URL: https://www.youtube.com/watch?v=zJetcfDVFNw


