Title: [2020] Optimizing for NVMe Drives: The 10 Microsecond Challenge by Stefan Hajnoczi
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	Solid-state storage devices with request latencies of less than 10 microseconds pose challenges for virtualization. Even small overheads result in a visible reduction of I/O performance. Solving this requires changes to the I/O stack.

This talk covers recommended tuning and current work on improving I/O performance for QEMU guests with NVMe drives.

The first part to achieving good I/O performance is to ensure that the guest is taking advantage of multicore and NUMA effectively. This involves both manual tuning and recently added optimizations for getting the most out of the hardware.

The second part is efficient I/O request submission and completion. Traditionally this involved vmexits and eventfds, but improvements to QEMU's AioContext polling can eliminate them and achieve much higher performance.

Come find out how close to bare metal performance QEMU gets!

---

Stefan Hajnoczi
Red Hat, Senior Principal Software Engineer
United Kingdom

Stefan has been active in QEMU since 2010 and is a Senior Principal Software Engineer in Red Hat's virtualization team with a focus on storage. He works on virtio drivers in Linux and helps maintain the block layer and tracing in QEMU. He also organizes and mentors in the Google Summer of Code and Outreachy internship programs for QEMU, and participates in the VIRTIO Technical Committee.
Captions: 
	00:00:05,680 --> 00:00:08,160
hi

00:00:06,160 --> 00:00:10,480
my name is stephan hoynitzy and i'm

00:00:08,160 --> 00:00:15,120
going to talk about optimizing kvm

00:00:10,480 --> 00:00:18,240
for nvme drives so what are nvme drives

00:00:15,120 --> 00:00:20,800
nvme is a standard interface for

00:00:18,240 --> 00:00:22,640
solid-state disks so it's a pci

00:00:20,800 --> 00:00:24,880
storage controller interface and there

00:00:22,640 --> 00:00:28,000
is an open specification

00:00:24,880 --> 00:00:31,359
and a standard linux driver that can

00:00:28,000 --> 00:00:34,480
talk to any nvme compliant device

00:00:31,359 --> 00:00:37,600
and devices are made by multiple vendors

00:00:34,480 --> 00:00:38,079
what's interesting about nvme drives is

00:00:37,600 --> 00:00:40,719
that

00:00:38,079 --> 00:00:42,640
some of them have extremely good latency

00:00:40,719 --> 00:00:46,399
much much lower latencies

00:00:42,640 --> 00:00:48,960
than we've had in the past so

00:00:46,399 --> 00:00:50,079
i quoted two figures here one is for an

00:00:48,960 --> 00:00:52,160
enterprise ssd

00:00:50,079 --> 00:00:54,160
so that would be something for for data

00:00:52,160 --> 00:00:56,559
centers and servers and so on

00:00:54,160 --> 00:00:57,440
this is an intel optane drive and it's

00:00:56,559 --> 00:01:00,879
quoted at

00:00:57,440 --> 00:01:04,640
10 microseconds for reads and writes

00:01:00,879 --> 00:01:09,119
but even the consumer ssds can be very

00:01:04,640 --> 00:01:11,680
low latency so the samsung 970 evo plus

00:01:09,119 --> 00:01:13,439
is quoted at around 17 microseconds

00:01:11,680 --> 00:01:15,040
right latency

00:01:13,439 --> 00:01:17,119
now just because the drive says it's

00:01:15,040 --> 00:01:18,880
nvme just because it

00:01:17,119 --> 00:01:20,400
um uses that standard doesn't

00:01:18,880 --> 00:01:21,360
necessarily mean it's one of these low

00:01:20,400 --> 00:01:23,280
latency drives

00:01:21,360 --> 00:01:26,320
there are significant differences so you

00:01:23,280 --> 00:01:29,840
should check the data sheet before

00:01:26,320 --> 00:01:32,560
purchasing drives now in 2012

00:01:29,840 --> 00:01:33,439
jeff dean published a slide that became

00:01:32,560 --> 00:01:35,680
very popular

00:01:33,439 --> 00:01:36,560
called latency numbers every programmer

00:01:35,680 --> 00:01:38,320
should know

00:01:36,560 --> 00:01:40,000
and it's probably became popular because

00:01:38,320 --> 00:01:42,079
it's really interesting to see

00:01:40,000 --> 00:01:44,000
how long different operations in

00:01:42,079 --> 00:01:45,600
computer systems take

00:01:44,000 --> 00:01:46,640
now the reason i'm showing this table

00:01:45,600 --> 00:01:48,320
and the reason it's interesting is

00:01:46,640 --> 00:01:50,960
because if you look at

00:01:48,320 --> 00:01:52,079
other storage if you look at spinning

00:01:50,960 --> 00:01:55,360
disks

00:01:52,079 --> 00:01:57,520
hard disks that have platters

00:01:55,360 --> 00:01:59,119
that store the data magnetically and

00:01:57,520 --> 00:02:02,000
they have drive heads that need to

00:01:59,119 --> 00:02:02,640
move to the correct location in order to

00:02:02,000 --> 00:02:04,240
be able to

00:02:02,640 --> 00:02:06,320
read those blocks of data from the

00:02:04,240 --> 00:02:09,360
platter the

00:02:06,320 --> 00:02:11,200
the our spinning discs have seek times

00:02:09,360 --> 00:02:12,959
in order to perform random accesses if

00:02:11,200 --> 00:02:15,040
we do the same random read

00:02:12,959 --> 00:02:16,480
um there's movement involved and that

00:02:15,040 --> 00:02:19,599
takes time

00:02:16,480 --> 00:02:23,280
so the time for that is quoted as

00:02:19,599 --> 00:02:23,840
two milliseconds so comparing these two

00:02:23,280 --> 00:02:26,959
things

00:02:23,840 --> 00:02:28,080
hard disk versus an nvme solid state

00:02:26,959 --> 00:02:32,800
disk

00:02:28,080 --> 00:02:36,160
we have 2 000 microseconds for accessing

00:02:32,800 --> 00:02:37,840
a random read and

00:02:36,160 --> 00:02:40,000
10 microseconds so there's a huge

00:02:37,840 --> 00:02:43,680
difference here and and and you can

00:02:40,000 --> 00:02:45,200
see why um optimizing for nvme drives is

00:02:43,680 --> 00:02:45,920
very different than from traditional

00:02:45,200 --> 00:02:48,239
disks

00:02:45,920 --> 00:02:49,599
so let's have a look at a relationship

00:02:48,239 --> 00:02:51,680
between iops

00:02:49,599 --> 00:02:52,800
and latency this is an important one in

00:02:51,680 --> 00:02:55,360
fact this is

00:02:52,800 --> 00:02:56,959
the critical thing that drives the rest

00:02:55,360 --> 00:02:58,800
of this presentation

00:02:56,959 --> 00:03:00,879
and the optimization work that we're

00:02:58,800 --> 00:03:02,159
going to look into it's the relationship

00:03:00,879 --> 00:03:04,480
between

00:03:02,159 --> 00:03:06,800
iops and latency iops is the number of

00:03:04,480 --> 00:03:10,080
operations per second

00:03:06,800 --> 00:03:13,760
and here i'm showing a a simple uh

00:03:10,080 --> 00:03:15,519
a model of doing one operation at a time

00:03:13,760 --> 00:03:17,760
and it's and the relationship is just

00:03:15,519 --> 00:03:18,480
the total runtime divided by the latency

00:03:17,760 --> 00:03:20,480
that gives us

00:03:18,480 --> 00:03:22,640
the number of operations we can complete

00:03:20,480 --> 00:03:25,680
in that run time

00:03:22,640 --> 00:03:28,000
so the first thing you notice

00:03:25,680 --> 00:03:28,720
is that this relationship is non-linear

00:03:28,000 --> 00:03:32,720
right it's not

00:03:28,720 --> 00:03:35,840
just a straight line it is a curve and

00:03:32,720 --> 00:03:38,159
to start investigating it imagine that

00:03:35,840 --> 00:03:39,120
we are at the right hand side of this

00:03:38,159 --> 00:03:41,280
curve we're at

00:03:39,120 --> 00:03:43,440
20 microseconds latency that means it

00:03:41,280 --> 00:03:44,879
takes us 20 microseconds to complete our

00:03:43,440 --> 00:03:47,280
operation

00:03:44,879 --> 00:03:49,680
if we identify an optimization we could

00:03:47,280 --> 00:03:52,560
make to make the system faster

00:03:49,680 --> 00:03:53,840
say it shaves off two microseconds then

00:03:52,560 --> 00:03:56,480
we can bring it down to

00:03:53,840 --> 00:03:58,239
18 microseconds latency and the graph

00:03:56,480 --> 00:03:59,920
shows us how many iops

00:03:58,239 --> 00:04:02,799
we are going to get if we move it from

00:03:59,920 --> 00:04:05,599
20 to 18 microseconds latency

00:04:02,799 --> 00:04:06,000
well that slope is pretty flat over

00:04:05,599 --> 00:04:08,000
there

00:04:06,000 --> 00:04:09,680
at 20 microseconds so actually the

00:04:08,000 --> 00:04:12,560
number of iops doesn't

00:04:09,680 --> 00:04:14,480
increase that much but if you imagine

00:04:12,560 --> 00:04:16,639
for a second that we were at four

00:04:14,480 --> 00:04:18,639
microseconds of latency on the left-hand

00:04:16,639 --> 00:04:20,959
side of that graph

00:04:18,639 --> 00:04:22,160
and then we shave off two microseconds

00:04:20,959 --> 00:04:23,759
so again here we're

00:04:22,160 --> 00:04:26,240
just shaving off two microseconds the

00:04:23,759 --> 00:04:27,520
same adjustment in both cases

00:04:26,240 --> 00:04:29,360
but now it brings us down to two

00:04:27,520 --> 00:04:33,360
microseconds and we go from

00:04:29,360 --> 00:04:34,800
250k iops to 500k iops

00:04:33,360 --> 00:04:37,520
and this might be an obvious

00:04:34,800 --> 00:04:39,919
relationship because when we were at 20

00:04:37,520 --> 00:04:41,040
microseconds and we removed two we only

00:04:39,919 --> 00:04:42,479
optimized the way

00:04:41,040 --> 00:04:45,120
percent of the total latency but when we

00:04:42,479 --> 00:04:46,720
were at four and we went down to two

00:04:45,120 --> 00:04:48,479
we optimized away fifty percent of

00:04:46,720 --> 00:04:49,840
latency so it does make sense that

00:04:48,479 --> 00:04:51,360
the jump is going to be bigger here

00:04:49,840 --> 00:04:52,560
because it is a bigger proportional

00:04:51,360 --> 00:04:54,639
improvement

00:04:52,560 --> 00:04:56,080
but still it leads to interesting things

00:04:54,639 --> 00:04:58,479
first of all

00:04:56,080 --> 00:04:59,759
think about having two independent

00:04:58,479 --> 00:05:01,840
optimizations a

00:04:59,759 --> 00:05:03,039
and b that you found and that you want

00:05:01,840 --> 00:05:04,639
to make well

00:05:03,039 --> 00:05:06,080
one of the misleading things here is

00:05:04,639 --> 00:05:09,440
that if you

00:05:06,080 --> 00:05:12,400
apply optimization a

00:05:09,440 --> 00:05:14,240
and then you apply optimization b say

00:05:12,400 --> 00:05:16,240
both of them

00:05:14,240 --> 00:05:17,280
reduce the total latency by two

00:05:16,240 --> 00:05:20,720
microseconds

00:05:17,280 --> 00:05:24,080
then if optimization a gains you 10k

00:05:20,720 --> 00:05:26,400
iops optimization b will gain you

00:05:24,080 --> 00:05:28,160
more than 10k iops because it was

00:05:26,400 --> 00:05:29,680
applied afterwards after we had already

00:05:28,160 --> 00:05:31,120
reduced the total latency

00:05:29,680 --> 00:05:33,199
and because they're independent if we do

00:05:31,120 --> 00:05:36,400
it the other order the other way around

00:05:33,199 --> 00:05:39,520
if we do optimization b first and then

00:05:36,400 --> 00:05:40,240
optimization a then we'll have the

00:05:39,520 --> 00:05:43,199
reverse

00:05:40,240 --> 00:05:43,759
optimization b will give us 10 iops

00:05:43,199 --> 00:05:45,840
improvement

00:05:43,759 --> 00:05:47,120
10k iops improvement and optimization b

00:05:45,840 --> 00:05:48,639
will give us more than that

00:05:47,120 --> 00:05:50,160
so this can be misleading right it can

00:05:48,639 --> 00:05:51,520
be tricky in the order in which we apply

00:05:50,160 --> 00:05:52,880
optimization we can kind of fool

00:05:51,520 --> 00:05:55,520
ourselves thinking that one

00:05:52,880 --> 00:05:56,400
boosts iops by a lot and and the other

00:05:55,520 --> 00:05:59,039
one doesn't

00:05:56,400 --> 00:06:00,639
but in terms of total latency uh it

00:05:59,039 --> 00:06:03,199
doesn't necessarily tell us

00:06:00,639 --> 00:06:04,560
how much time we've reduced and that's

00:06:03,199 --> 00:06:07,440
why saying something like

00:06:04,560 --> 00:06:08,080
iops increased by 10k doesn't really

00:06:07,440 --> 00:06:10,080
convey

00:06:08,080 --> 00:06:11,199
enough information to know what is going

00:06:10,080 --> 00:06:12,880
on

00:06:11,199 --> 00:06:14,400
you need to know at least the basic iops

00:06:12,880 --> 00:06:16,639
level you're at before

00:06:14,400 --> 00:06:19,280
in order to understand how the latency

00:06:16,639 --> 00:06:20,800
came into play

00:06:19,280 --> 00:06:22,319
the other thing that's important about

00:06:20,800 --> 00:06:25,600
this graph

00:06:22,319 --> 00:06:27,440
is that nvme drives being at 10

00:06:25,600 --> 00:06:30,639
microseconds or less

00:06:27,440 --> 00:06:32,400
latency if we focus on on that

00:06:30,639 --> 00:06:34,560
that is the left hand side of this graph

00:06:32,400 --> 00:06:36,720
that's where the graph becomes

00:06:34,560 --> 00:06:38,639
very non-linear that's where the slope

00:06:36,720 --> 00:06:40,960
gets very steep

00:06:38,639 --> 00:06:42,240
so we need to we need to keep that in

00:06:40,960 --> 00:06:45,680
mind that any small

00:06:42,240 --> 00:06:47,199
change to latency increasing it or

00:06:45,680 --> 00:06:48,000
reducing it on the left hand side of the

00:06:47,199 --> 00:06:51,599
graph can have a big

00:06:48,000 --> 00:06:54,800
effect on iops we need to re-examine

00:06:51,599 --> 00:06:56,720
the guest and the host software stack

00:06:54,800 --> 00:06:58,160
because the hardware is faster so now we

00:06:56,720 --> 00:07:00,160
need to re-examine

00:06:58,160 --> 00:07:03,039
the guest and host software stack and we

00:07:00,160 --> 00:07:06,960
need to rethink the architecture

00:07:03,039 --> 00:07:08,800
of qemu of kvm and of the the linux

00:07:06,960 --> 00:07:10,400
drivers and so on

00:07:08,800 --> 00:07:11,840
because the hardware has some gotten so

00:07:10,400 --> 00:07:13,120
much faster and when i say rethink the

00:07:11,840 --> 00:07:14,560
architecture that's that's not a

00:07:13,120 --> 00:07:16,960
buzzword what i mean by that

00:07:14,560 --> 00:07:18,880
is think back to that curve that we saw

00:07:16,960 --> 00:07:21,759
the iop sources latency

00:07:18,880 --> 00:07:22,800
the optimizations that we considered in

00:07:21,759 --> 00:07:26,319
the past

00:07:22,800 --> 00:07:27,840
and that had no measurable effect

00:07:26,319 --> 00:07:30,000
you know they were not significant in

00:07:27,840 --> 00:07:32,319
the past we decided they were too

00:07:30,000 --> 00:07:34,240
complex or for whatever reason we didn't

00:07:32,319 --> 00:07:36,560
implement them in the past

00:07:34,240 --> 00:07:38,400
those optimizations might now be

00:07:36,560 --> 00:07:40,000
relevant again because even shaving off

00:07:38,400 --> 00:07:41,360
a little bit of time on the left hand

00:07:40,000 --> 00:07:43,759
side of that curve

00:07:41,360 --> 00:07:44,879
can boost iops a lot and can allow the

00:07:43,759 --> 00:07:47,120
application to get

00:07:44,879 --> 00:07:48,720
better performance so that's why i mean

00:07:47,120 --> 00:07:51,360
rethinking the architecture because we

00:07:48,720 --> 00:07:53,599
really can reconsider things

00:07:51,360 --> 00:07:55,599
that didn't make sense in the past and

00:07:53,599 --> 00:07:59,120
so that's the 10 microsecond challenge

00:07:55,599 --> 00:08:03,199
that we're going to talk about

00:07:59,120 --> 00:08:03,440
okay so let's begin by looking at the i

00:08:03,199 --> 00:08:06,080
o

00:08:03,440 --> 00:08:06,800
request lifecycle the i o request

00:08:06,080 --> 00:08:08,160
lifecycle

00:08:06,800 --> 00:08:10,960
is the core of what we need to

00:08:08,160 --> 00:08:13,280
understand in order to optimize this

00:08:10,960 --> 00:08:14,560
here is a high level model it doesn't

00:08:13,280 --> 00:08:17,680
show the specifics

00:08:14,560 --> 00:08:20,879
of virtualization right you don't see

00:08:17,680 --> 00:08:23,039
qmu you don't see kvm in here and it

00:08:20,879 --> 00:08:24,560
or any emulated devices or the operating

00:08:23,039 --> 00:08:25,039
system or anything this is just a high

00:08:24,560 --> 00:08:28,240
level

00:08:25,039 --> 00:08:31,360
model what it shows you is that an

00:08:28,240 --> 00:08:33,919
application that is running on the vcpu

00:08:31,360 --> 00:08:35,599
and decides to submit an i o request in

00:08:33,919 --> 00:08:36,159
this case it's a read request that is

00:08:35,599 --> 00:08:38,839
being

00:08:36,159 --> 00:08:40,560
submitted that request needs to be

00:08:38,839 --> 00:08:42,719
prepared

00:08:40,560 --> 00:08:43,919
and then it will be a message will be

00:08:42,719 --> 00:08:47,360
sent to the device

00:08:43,919 --> 00:08:49,839
notifying it that there's a request

00:08:47,360 --> 00:08:51,600
available and then the device can

00:08:49,839 --> 00:08:54,320
process that request

00:08:51,600 --> 00:08:55,920
now when that message has been sent when

00:08:54,320 --> 00:08:59,200
we've submitted our i o

00:08:55,920 --> 00:09:00,560
the vcpu might have no more work to do

00:08:59,200 --> 00:09:02,160
or it might have some other tasks that

00:09:00,560 --> 00:09:04,399
it can run in the meantime while we're

00:09:02,160 --> 00:09:06,160
waiting for the request to finish

00:09:04,399 --> 00:09:08,000
but either way at this point the

00:09:06,160 --> 00:09:10,800
critical path is in the hardware because

00:09:08,000 --> 00:09:12,399
now the device with its latency of say

00:09:10,800 --> 00:09:14,720
10 microseconds

00:09:12,399 --> 00:09:16,080
is going to be processing that request

00:09:14,720 --> 00:09:18,240
and when it finishes

00:09:16,080 --> 00:09:20,000
it sends back a completion notification

00:09:18,240 --> 00:09:23,040
to the vcpu

00:09:20,000 --> 00:09:25,440
the vcpu is going to then

00:09:23,040 --> 00:09:27,920
process that completion and resume the

00:09:25,440 --> 00:09:30,080
application because the io has finished

00:09:27,920 --> 00:09:31,920
and so that's the life cycle of a single

00:09:30,080 --> 00:09:35,120
request of just one request

00:09:31,920 --> 00:09:36,160
in isolation and the important thing to

00:09:35,120 --> 00:09:38,160
understand here

00:09:36,160 --> 00:09:40,560
is that if we're trying to optimize the

00:09:38,160 --> 00:09:43,360
software layers we need to study

00:09:40,560 --> 00:09:44,320
the mechanism through which requests are

00:09:43,360 --> 00:09:46,320
submitted

00:09:44,320 --> 00:09:48,000
the mechanism through which requests are

00:09:46,320 --> 00:09:49,920
completed and of course

00:09:48,000 --> 00:09:51,440
the code path and the layers of code

00:09:49,920 --> 00:09:54,480
that that are

00:09:51,440 --> 00:09:57,279
parsing requests or or or creating them

00:09:54,480 --> 00:09:58,720
in in in memory and so on because that's

00:09:57,279 --> 00:10:01,120
what we can optimize that's under our

00:09:58,720 --> 00:10:01,120
control

00:10:02,079 --> 00:10:06,320
so you might have noticed that i've been

00:10:03,440 --> 00:10:07,920
mentioning latency all the time

00:10:06,320 --> 00:10:09,440
and in this presentation we are going to

00:10:07,920 --> 00:10:11,440
focus on latency

00:10:09,440 --> 00:10:14,480
but latency is just one performance

00:10:11,440 --> 00:10:16,800
factor there are others

00:10:14,480 --> 00:10:18,000
so request parallelism is another thing

00:10:16,800 --> 00:10:20,000
that that really boosts

00:10:18,000 --> 00:10:21,440
performance and batching is one of the

00:10:20,000 --> 00:10:24,240
techniques to to

00:10:21,440 --> 00:10:26,079
to to also benefit from that what they

00:10:24,240 --> 00:10:27,200
can do is they can hide poor latency

00:10:26,079 --> 00:10:28,480
because you're able to do a lot of

00:10:27,200 --> 00:10:30,880
requests at once

00:10:28,480 --> 00:10:32,560
and that way you're able to get a lot of

00:10:30,880 --> 00:10:33,760
work done even if the latency for one

00:10:32,560 --> 00:10:35,920
request is relatively

00:10:33,760 --> 00:10:37,360
long but that's not what we're looking

00:10:35,920 --> 00:10:38,959
at here the reason why we're looking

00:10:37,360 --> 00:10:41,519
purely at latency

00:10:38,959 --> 00:10:43,680
is because it's a fundamental thing and

00:10:41,519 --> 00:10:46,560
if we optimize latency first

00:10:43,680 --> 00:10:47,360
then we can consider those factors later

00:10:46,560 --> 00:10:50,560
now

00:10:47,360 --> 00:10:52,959
there are latency sensitive applications

00:10:50,560 --> 00:10:54,959
and you can't hide poor latency from

00:10:52,959 --> 00:10:58,079
them and the reason why is because

00:10:54,959 --> 00:11:00,480
they need a specific request to complete

00:10:58,079 --> 00:11:03,040
before they can continue even if there's

00:11:00,480 --> 00:11:04,640
a lot of parallelism available to them

00:11:03,040 --> 00:11:07,200
because they need to wait for one

00:11:04,640 --> 00:11:09,760
specific request they are bounded

00:11:07,200 --> 00:11:11,120
by the latency of that one request and

00:11:09,760 --> 00:11:12,480
that's what we're trying to optimize

00:11:11,120 --> 00:11:14,880
here and that's what's most

00:11:12,480 --> 00:11:16,240
obvious those types of applications

00:11:14,880 --> 00:11:19,600
suffer the most

00:11:16,240 --> 00:11:21,120
when they're run on on on virtualization

00:11:19,600 --> 00:11:21,920
on extremely fast hardware because

00:11:21,120 --> 00:11:25,120
that's where

00:11:21,920 --> 00:11:26,560
the overheads become apparent so what

00:11:25,120 --> 00:11:27,200
we're going to do is we're going to look

00:11:26,560 --> 00:11:29,680
at

00:11:27,200 --> 00:11:31,200
cue depth one benchmarks and that means

00:11:29,680 --> 00:11:33,360
only submitting one request

00:11:31,200 --> 00:11:35,600
at a time we're also going to focus

00:11:33,360 --> 00:11:37,519
primarily on small block sizes

00:11:35,600 --> 00:11:38,880
i will be showing graphs that have the

00:11:37,519 --> 00:11:41,040
larger block sizes

00:11:38,880 --> 00:11:42,320
just as a reference but what we'll see

00:11:41,040 --> 00:11:43,920
is that

00:11:42,320 --> 00:11:45,920
other factors come into play there maybe

00:11:43,920 --> 00:11:47,920
the data transfer time and so on

00:11:45,920 --> 00:11:50,160
become more relevant and they dominate

00:11:47,920 --> 00:11:51,920
and we're seeing less of the

00:11:50,160 --> 00:11:53,440
completion and submission latency which

00:11:51,920 --> 00:11:56,000
affects applications

00:11:53,440 --> 00:11:58,000
with small block sizes and if you think

00:11:56,000 --> 00:12:00,399
about networking performance the kind of

00:11:58,000 --> 00:12:02,320
analogy or similar thing there is

00:12:00,399 --> 00:12:04,320
benchmarking small packet sizes that

00:12:02,320 --> 00:12:06,560
tends to show

00:12:04,320 --> 00:12:08,079
the cost the per packet cost and it's

00:12:06,560 --> 00:12:10,639
the same thing here that's what we're

00:12:08,079 --> 00:12:13,440
focusing on

00:12:10,639 --> 00:12:14,880
but you can get other perspectives uh if

00:12:13,440 --> 00:12:15,600
if latency isn't the only thing you're

00:12:14,880 --> 00:12:17,600
interested in

00:12:15,600 --> 00:12:19,120
then then take a look at these talks

00:12:17,600 --> 00:12:22,880
there is another talk this year

00:12:19,120 --> 00:12:24,959
at kvm forum which investigates nvme

00:12:22,880 --> 00:12:27,200
performance and last year there was a

00:12:24,959 --> 00:12:29,120
great talk at kvm forum

00:12:27,200 --> 00:12:30,720
that compared storage performance

00:12:29,120 --> 00:12:34,480
between hypervisors

00:12:30,720 --> 00:12:34,480
nvme and various other things

00:12:36,320 --> 00:12:40,720
okay now i mentioned that the mechanisms

00:12:39,839 --> 00:12:43,200
through which we

00:12:40,720 --> 00:12:43,920
complete and submit requests are

00:12:43,200 --> 00:12:45,279
important

00:12:43,920 --> 00:12:47,519
because they can determine the

00:12:45,279 --> 00:12:48,560
performance or they can determine the

00:12:47,519 --> 00:12:50,240
latency

00:12:48,560 --> 00:12:53,360
so let's have a look at the mechanisms

00:12:50,240 --> 00:12:55,519
that are used in linux and in kvm

00:12:53,360 --> 00:12:57,360
today these aren't the only mechanisms

00:12:55,519 --> 00:12:59,600
but they're the main ones

00:12:57,360 --> 00:13:00,720
first there is eventfd which is a

00:12:59,600 --> 00:13:03,920
counter

00:13:00,720 --> 00:13:06,560
and it's a file descriptor so

00:13:03,920 --> 00:13:08,320
when you read from this file descriptor

00:13:06,560 --> 00:13:11,120
the counter is reset to zero

00:13:08,320 --> 00:13:12,639
if it's already zero then the read would

00:13:11,120 --> 00:13:13,920
block

00:13:12,639 --> 00:13:15,440
now if the counter is incremented

00:13:13,920 --> 00:13:16,320
multiple times you don't need to read it

00:13:15,440 --> 00:13:17,920
multiple times

00:13:16,320 --> 00:13:19,920
because the single read already resets

00:13:17,920 --> 00:13:21,839
it to zero so what this means is that

00:13:19,920 --> 00:13:24,399
multiple notifications will be

00:13:21,839 --> 00:13:27,040
coalesced into a single read which can

00:13:24,399 --> 00:13:29,440
be nice for performance that helps

00:13:27,040 --> 00:13:31,519
now because it's an event because it's a

00:13:29,440 --> 00:13:34,160
file descriptor

00:13:31,519 --> 00:13:35,680
it relies on the kernel scheduler to

00:13:34,160 --> 00:13:38,079
wake threads because if you're trying to

00:13:35,680 --> 00:13:38,880
read from it or you're in a select style

00:13:38,079 --> 00:13:40,399
system call

00:13:38,880 --> 00:13:42,560
waiting for that file descriptor to

00:13:40,399 --> 00:13:45,519
become ready then your

00:13:42,560 --> 00:13:47,680
your thread may be descheduled maybe the

00:13:45,519 --> 00:13:49,680
physical cpu will even be halted and put

00:13:47,680 --> 00:13:53,120
into a low power state

00:13:49,680 --> 00:13:56,639
and waking up again from a halted cpu

00:13:53,120 --> 00:13:59,519
and resuming the the application thread

00:13:56,639 --> 00:14:01,199
that may have been descheduled has some

00:13:59,519 --> 00:14:02,880
latency

00:14:01,199 --> 00:14:06,959
and so this is not necessarily the the

00:14:02,880 --> 00:14:10,560
lowest latency approach the event fd

00:14:06,959 --> 00:14:12,720
but it is widely used it's used by vfio

00:14:10,560 --> 00:14:15,600
interrupts by kvm's i of nfd

00:14:12,720 --> 00:14:16,000
and irq mechanism and by linux aio and

00:14:15,600 --> 00:14:19,199
iou

00:14:16,000 --> 00:14:20,240
rings completion mechanisms so the other

00:14:19,199 --> 00:14:24,639
approach

00:14:20,240 --> 00:14:27,680
polling or busy waiting is also popular

00:14:24,639 --> 00:14:29,600
so this simply means looping and

00:14:27,680 --> 00:14:31,360
continuously checking for the event that

00:14:29,600 --> 00:14:33,839
you're waiting for

00:14:31,360 --> 00:14:36,079
of course if you keep if you do

00:14:33,839 --> 00:14:38,079
something from a from a tight loop

00:14:36,079 --> 00:14:40,240
that consumes cpu cycles and no other

00:14:38,079 --> 00:14:43,120
tasks can run while you're running

00:14:40,240 --> 00:14:45,040
so it's not very power efficient it is

00:14:43,120 --> 00:14:47,360
it does hog the cpu

00:14:45,040 --> 00:14:48,480
but the advantage of this is when i when

00:14:47,360 --> 00:14:51,040
i mentioned how even

00:14:48,480 --> 00:14:52,800
fd relies on the scheduler and the cpu

00:14:51,040 --> 00:14:53,760
could be halted and there's a latency

00:14:52,800 --> 00:14:56,079
associated with that

00:14:53,760 --> 00:14:57,519
well polling does not have that problem

00:14:56,079 --> 00:15:00,240
because when you're polling

00:14:57,519 --> 00:15:01,199
you are running on the cpu you have

00:15:00,240 --> 00:15:03,120
control

00:15:01,199 --> 00:15:06,079
there is no latency you're just going to

00:15:03,120 --> 00:15:09,199
read the completion value and see that

00:15:06,079 --> 00:15:11,600
the request is now ready and so

00:15:09,199 --> 00:15:13,040
it has great latency and that's why it's

00:15:11,600 --> 00:15:16,399
used it's used by

00:15:13,040 --> 00:15:18,959
qmu's aio context by kvm's hotpoll on s

00:15:16,399 --> 00:15:21,279
by cpu idle hall poll and linux io poll

00:15:18,959 --> 00:15:22,880
and dpdk and spdk all these things

00:15:21,279 --> 00:15:24,800
they use it in different ways in various

00:15:22,880 --> 00:15:27,600
flavors but but busy waiting

00:15:24,800 --> 00:15:29,279
is used in order to reduce latency the

00:15:27,600 --> 00:15:30,000
reason i covered these mechanisms is

00:15:29,279 --> 00:15:32,000
because

00:15:30,000 --> 00:15:33,440
later on when we look at the different

00:15:32,000 --> 00:15:34,800
optimizations available

00:15:33,440 --> 00:15:36,720
and the ones that i've tried out and

00:15:34,800 --> 00:15:37,600
that i'm presenting here they are

00:15:36,720 --> 00:15:41,279
related to these

00:15:37,600 --> 00:15:44,079
on how to use them effectively

00:15:41,279 --> 00:15:45,360
okay so the starting point for nvme

00:15:44,079 --> 00:15:47,040
performance for achieving good

00:15:45,360 --> 00:15:50,480
performance in vms

00:15:47,040 --> 00:15:51,920
is device pci device assignment pci

00:15:50,480 --> 00:15:54,399
device assignment is the way

00:15:51,920 --> 00:15:56,240
that achieves the highest performance

00:15:54,399 --> 00:15:57,600
you can get bare metal performance by

00:15:56,240 --> 00:15:59,920
doing this

00:15:57,600 --> 00:16:02,320
and the reason why this is so efficient

00:15:59,920 --> 00:16:04,720
is because when you take a pci device

00:16:02,320 --> 00:16:07,199
and you pass it through to the guest

00:16:04,720 --> 00:16:08,079
then the host is actually not involved

00:16:07,199 --> 00:16:11,680
in the critical

00:16:08,079 --> 00:16:14,480
path of processing i o requests

00:16:11,680 --> 00:16:17,120
this works because the pci device the

00:16:14,480 --> 00:16:20,000
hardware registers of the nvme drive

00:16:17,120 --> 00:16:20,480
will be memory mapped into the guest so

00:16:20,000 --> 00:16:21,920
that

00:16:20,480 --> 00:16:24,240
accessing them doesn't require the

00:16:21,920 --> 00:16:26,480
hypervisor to be involved

00:16:24,240 --> 00:16:28,880
and the irqs the interrupts that are

00:16:26,480 --> 00:16:31,040
raised by the nvme drive

00:16:28,880 --> 00:16:32,800
they can be directly injected into

00:16:31,040 --> 00:16:34,000
running guests if they are currently

00:16:32,800 --> 00:16:37,040
scheduled on a

00:16:34,000 --> 00:16:38,639
vcp on a cpu and a physical cpu and

00:16:37,040 --> 00:16:40,880
that's thanks to posted interrupts the

00:16:38,639 --> 00:16:44,160
cpu feature that's available

00:16:40,880 --> 00:16:45,920
on some cpus so again there

00:16:44,160 --> 00:16:47,199
the hypervisor doesn't need to have an

00:16:45,920 --> 00:16:48,720
interrupt handler doesn't need to

00:16:47,199 --> 00:16:50,160
forward that interrupt in the vm

00:16:48,720 --> 00:16:53,120
instead the hardware is able to do that

00:16:50,160 --> 00:16:56,880
directly which is good for latency

00:16:53,120 --> 00:16:59,600
and then finally for guest ram access

00:16:56,880 --> 00:17:00,880
the i o mmu the i o memory management

00:16:59,600 --> 00:17:04,319
unit

00:17:00,880 --> 00:17:05,120
allows the physical pci device the nvme

00:17:04,319 --> 00:17:07,520
drive

00:17:05,120 --> 00:17:08,959
to directly access guest memory so again

00:17:07,520 --> 00:17:11,120
there you're also

00:17:08,959 --> 00:17:12,640
not involving a software layer in the

00:17:11,120 --> 00:17:15,760
hypervisor in order to

00:17:12,640 --> 00:17:17,360
perform any io so that's why it's fast

00:17:15,760 --> 00:17:20,480
and it's a great approach

00:17:17,360 --> 00:17:23,280
for uh high performance it does have

00:17:20,480 --> 00:17:23,600
some limitations which cause it to not

00:17:23,280 --> 00:17:26,720
be

00:17:23,600 --> 00:17:28,960
as widely deployed um as

00:17:26,720 --> 00:17:30,559
you know as it would would be great for

00:17:28,960 --> 00:17:33,360
performance but

00:17:30,559 --> 00:17:34,640
it does not support uh live migration in

00:17:33,360 --> 00:17:36,799
most cases

00:17:34,640 --> 00:17:38,799
because that device is actually a black

00:17:36,799 --> 00:17:39,360
box to the hypervisor and qmu doesn't

00:17:38,799 --> 00:17:40,880
know

00:17:39,360 --> 00:17:43,039
about what's going on it's unable to

00:17:40,880 --> 00:17:45,919
live migrate it because only the guest

00:17:43,039 --> 00:17:49,360
is using that device and and has the

00:17:45,919 --> 00:17:51,280
driver and the state associated with it

00:17:49,360 --> 00:17:53,520
software features like backup and

00:17:51,280 --> 00:17:55,360
snapshots and so that qmu can offer when

00:17:53,520 --> 00:17:57,120
you're using disk images are also not

00:17:55,360 --> 00:17:59,440
available again because the hypervisor

00:17:57,120 --> 00:18:01,280
is bypassed

00:17:59,440 --> 00:18:04,320
another thing to consider is that

00:18:01,280 --> 00:18:06,799
exposing pci devices to your guests

00:18:04,320 --> 00:18:07,679
may be inconvenient or may have security

00:18:06,799 --> 00:18:09,440
implications

00:18:07,679 --> 00:18:11,360
and this is also why sometimes it cannot

00:18:09,440 --> 00:18:13,440
be used

00:18:11,360 --> 00:18:15,280
the issue here is that of course if you

00:18:13,440 --> 00:18:17,360
have varying hardware

00:18:15,280 --> 00:18:19,280
and you want to live my grass live

00:18:17,360 --> 00:18:21,120
migrate your guests around

00:18:19,280 --> 00:18:23,919
or if you just want to upgrade some of

00:18:21,120 --> 00:18:26,320
the hardware in in your infrastructure

00:18:23,919 --> 00:18:28,080
then the guests will actually see those

00:18:26,320 --> 00:18:28,480
changes they will see the new hardware

00:18:28,080 --> 00:18:30,880
so

00:18:28,480 --> 00:18:32,160
they need driver support and they might

00:18:30,880 --> 00:18:34,880
need reconfiguration

00:18:32,160 --> 00:18:35,840
if it's a different device that requires

00:18:34,880 --> 00:18:37,520
different setup

00:18:35,840 --> 00:18:38,960
and so that can be prohibitive because

00:18:37,520 --> 00:18:40,720
in some environments you don't control

00:18:38,960 --> 00:18:41,760
the guests they may be very old and so

00:18:40,720 --> 00:18:44,400
on and so then you don't have the

00:18:41,760 --> 00:18:46,240
freedom to change the hardware

00:18:44,400 --> 00:18:47,679
you might also be concerned about the

00:18:46,240 --> 00:18:50,000
guest being able to

00:18:47,679 --> 00:18:50,799
say do a firmware update on the device

00:18:50,000 --> 00:18:52,000
and so on so

00:18:50,799 --> 00:18:53,760
there are some issues there with

00:18:52,000 --> 00:18:55,760
exposing those devices so that's another

00:18:53,760 --> 00:18:57,520
thing to watch out for

00:18:55,760 --> 00:18:59,360
and the final thing is simply the cost

00:18:57,520 --> 00:19:02,960
because you do need to

00:18:59,360 --> 00:19:05,039
dedicate one pci device to

00:19:02,960 --> 00:19:06,480
a particular guest so other guests won't

00:19:05,039 --> 00:19:07,679
be able to use that same device that

00:19:06,480 --> 00:19:10,320
cannot share it

00:19:07,679 --> 00:19:11,679
because only one guest can be the the

00:19:10,320 --> 00:19:15,200
one that's running the driver

00:19:11,679 --> 00:19:17,120
at a time now you can use sri ov

00:19:15,200 --> 00:19:18,960
some devices allow you to virtualize

00:19:17,120 --> 00:19:19,919
them and split them up into virtual pci

00:19:18,960 --> 00:19:24,080
devices

00:19:19,919 --> 00:19:25,840
but that also has has its limits

00:19:24,080 --> 00:19:27,280
so here's the configuration i'm not

00:19:25,840 --> 00:19:29,840
going to go into levert

00:19:27,280 --> 00:19:30,640
domain xml details in this presentation

00:19:29,840 --> 00:19:32,799
i just want to

00:19:30,640 --> 00:19:35,120
show you the slides so that if you are

00:19:32,799 --> 00:19:37,039
watching this or reading this later on

00:19:35,120 --> 00:19:39,280
you could have links to the

00:19:37,039 --> 00:19:40,080
documentation and and find the keywords

00:19:39,280 --> 00:19:42,000
and things to

00:19:40,080 --> 00:19:43,679
to to to look up in order to apply this

00:19:42,000 --> 00:19:47,360
configuration

00:19:43,679 --> 00:19:49,919
okay so we've mentioned that

00:19:47,360 --> 00:19:52,720
for starters pci device assignment is a

00:19:49,919 --> 00:19:54,960
way to get good performance

00:19:52,720 --> 00:19:56,960
it you don't necessarily have the best

00:19:54,960 --> 00:19:58,880
performance right away with pci device

00:19:56,960 --> 00:20:02,400
assignment unless you also consider

00:19:58,880 --> 00:20:05,200
the pneuma topology of your host

00:20:02,400 --> 00:20:06,000
and what this means is that on modern

00:20:05,200 --> 00:20:09,120
systems typically

00:20:06,000 --> 00:20:12,400
the system is divided into multiple

00:20:09,120 --> 00:20:13,440
domains that are called pneuma nodes and

00:20:12,400 --> 00:20:15,840
a processor

00:20:13,440 --> 00:20:17,039
and memory and pci devices are

00:20:15,840 --> 00:20:19,919
associated

00:20:17,039 --> 00:20:22,320
with a particular pneuma node accesses

00:20:19,919 --> 00:20:24,720
to the resources within that node

00:20:22,320 --> 00:20:25,520
are local and they are cheaper than

00:20:24,720 --> 00:20:28,640
accesses

00:20:25,520 --> 00:20:30,400
to resources in other nodes and so for

00:20:28,640 --> 00:20:34,159
performance it is important

00:20:30,400 --> 00:20:34,960
that we keep be aware of locality and we

00:20:34,159 --> 00:20:37,360
make sure

00:20:34,960 --> 00:20:39,520
that our operations are within a pneuma

00:20:37,360 --> 00:20:40,880
node where possible

00:20:39,520 --> 00:20:43,280
so the tools that you can use to

00:20:40,880 --> 00:20:44,880
investigate this are pneuma ctl and

00:20:43,280 --> 00:20:46,240
elastopol and then you can get an

00:20:44,880 --> 00:20:48,799
overview of how

00:20:46,240 --> 00:20:50,640
of the topology of of your machine there

00:20:48,799 --> 00:20:51,360
are also performance counters that you

00:20:50,640 --> 00:20:53,039
can use

00:20:51,360 --> 00:20:55,600
if you suspect your application might be

00:20:53,039 --> 00:20:57,440
making cross-pneuma node memory accesses

00:20:55,600 --> 00:21:00,720
and so on

00:20:57,440 --> 00:21:02,880
for more information check out this

00:21:00,720 --> 00:21:04,159
this talk that dario is giving at kvm

00:21:02,880 --> 00:21:08,880
forum this year

00:21:04,159 --> 00:21:11,520
um about topology and pneuma

00:21:08,880 --> 00:21:12,000
okay so in terms of how you set things

00:21:11,520 --> 00:21:15,039
up

00:21:12,000 --> 00:21:18,559
libvert has the uh gives you the ability

00:21:15,039 --> 00:21:21,039
to pin vcpu threads to physical cpus

00:21:18,559 --> 00:21:22,960
you can also control qmu's own threads

00:21:21,039 --> 00:21:26,240
the i o threads and the emulator threat

00:21:22,960 --> 00:21:30,320
they can be pinned to physical cpus

00:21:26,240 --> 00:21:32,320
in addition you can control which host

00:21:30,320 --> 00:21:34,799
pneuma node to allocate

00:21:32,320 --> 00:21:36,000
portions of guest ram for and you can

00:21:34,799 --> 00:21:38,240
also expose

00:21:36,000 --> 00:21:40,400
a virtual numa topology you can describe

00:21:38,240 --> 00:21:41,440
a pneuma topology that the guest will

00:21:40,400 --> 00:21:44,240
see

00:21:41,440 --> 00:21:46,559
and the goal there is we want to align

00:21:44,240 --> 00:21:48,880
the guest's virtual numa topology

00:21:46,559 --> 00:21:49,760
with the hosts numa topology it should

00:21:48,880 --> 00:21:51,919
reflect

00:21:49,760 --> 00:21:54,320
what those resources on the host look

00:21:51,919 --> 00:21:57,200
like and by doing that

00:21:54,320 --> 00:21:58,080
the guest kernel as well as pneuma aware

00:21:57,200 --> 00:22:00,799
applications

00:21:58,080 --> 00:22:01,360
running inside the guest will be able to

00:22:00,799 --> 00:22:03,600
make

00:22:01,360 --> 00:22:05,280
good scheduling and allocation decisions

00:22:03,600 --> 00:22:06,720
because they'll have that locality

00:22:05,280 --> 00:22:08,480
information they'll know

00:22:06,720 --> 00:22:10,559
what is cheap and what is expensive so

00:22:08,480 --> 00:22:13,840
they can choose

00:22:10,559 --> 00:22:13,840
the best configuration

00:22:14,880 --> 00:22:19,039
now here's a little example this is

00:22:17,200 --> 00:22:21,120
trivial but what's interesting is that

00:22:19,039 --> 00:22:23,520
it can also demonstrate how quickly we

00:22:21,120 --> 00:22:24,159
hit limitations and trade-offs here when

00:22:23,520 --> 00:22:26,880
we do

00:22:24,159 --> 00:22:29,440
pneuma tuning so let's say we have a

00:22:26,880 --> 00:22:33,120
1vcpu guest and what we want to do

00:22:29,440 --> 00:22:37,120
is we want this vm to do i o

00:22:33,120 --> 00:22:38,880
so we have an nvme pci adapter

00:22:37,120 --> 00:22:42,559
and you can see the topology here in the

00:22:38,880 --> 00:22:46,480
diagram on the left side of the slide

00:22:42,559 --> 00:22:49,039
now where should we put the vcpu

00:22:46,480 --> 00:22:50,799
on which node should it run since it's

00:22:49,039 --> 00:22:51,520
going to be doing i o and using the nvme

00:22:50,799 --> 00:22:54,000
drive

00:22:51,520 --> 00:22:56,320
let's place it on node 0 because that's

00:22:54,000 --> 00:22:57,919
where the nvme drive is local

00:22:56,320 --> 00:22:59,280
right so instead of placing on node 1

00:22:57,919 --> 00:23:01,039
where it would have to cross the node we

00:22:59,280 --> 00:23:02,880
put it on node 0. so that's

00:23:01,039 --> 00:23:06,159
that's the starting point let's pin the

00:23:02,880 --> 00:23:08,880
vcpu thread onto processor 0.

00:23:06,159 --> 00:23:09,280
now if we're using an i o thread in qmu

00:23:08,880 --> 00:23:12,320
which

00:23:09,280 --> 00:23:13,120
we will also get into later on why using

00:23:12,320 --> 00:23:16,480
i o thread can

00:23:13,120 --> 00:23:19,120
can can be advantageous that is going to

00:23:16,480 --> 00:23:21,200
be doing i o on behalf of the guest

00:23:19,120 --> 00:23:22,799
and so that also needs to be where the

00:23:21,200 --> 00:23:25,919
nvme

00:23:22,799 --> 00:23:27,360
pci adapter is and so we will pin it to

00:23:25,919 --> 00:23:29,679
processor one

00:23:27,360 --> 00:23:32,000
and of course we're on node zero so we

00:23:29,679 --> 00:23:36,080
wanna be using ram zero so hopefully

00:23:32,000 --> 00:23:37,280
guest ram fits into ram zero's range so

00:23:36,080 --> 00:23:39,280
there's enough memory there for our

00:23:37,280 --> 00:23:40,960
entire guest that would be great

00:23:39,280 --> 00:23:42,720
and that's the setup but you can already

00:23:40,960 --> 00:23:44,799
start to see some of the the challenges

00:23:42,720 --> 00:23:46,559
what if we wanted a guess that had more

00:23:44,799 --> 00:23:49,360
ram than was available in ram

00:23:46,559 --> 00:23:51,360
zero then maybe we would have to define

00:23:49,360 --> 00:23:53,600
a virtual pneuma node and use some

00:23:51,360 --> 00:23:55,120
of the memory from ram one as well and

00:23:53,600 --> 00:23:57,600
hopefully the guest will then be able to

00:23:55,120 --> 00:23:59,679
make smart decisions about what to place

00:23:57,600 --> 00:24:00,960
into which of these two virtual pneuma

00:23:59,679 --> 00:24:02,799
nodes

00:24:00,960 --> 00:24:04,640
now if we add more guests to this

00:24:02,799 --> 00:24:06,559
picture it becomes even harder because

00:24:04,640 --> 00:24:08,480
at that point we we need to maybe make

00:24:06,559 --> 00:24:11,919
sacrifices decide whether to

00:24:08,480 --> 00:24:15,679
share resources like processors

00:24:11,919 --> 00:24:18,400
across guests or whether to assign vcpus

00:24:15,679 --> 00:24:20,480
to processors that are on what is going

00:24:18,400 --> 00:24:23,840
to turn out to be the wrong node

00:24:20,480 --> 00:24:26,559
now today pneuma tuning is something

00:24:23,840 --> 00:24:27,520
that pays off for performance critical

00:24:26,559 --> 00:24:30,640
vms

00:24:27,520 --> 00:24:32,480
doing this manually pays off

00:24:30,640 --> 00:24:34,000
and hopefully in the future we'll see

00:24:32,480 --> 00:24:36,159
more automatic

00:24:34,000 --> 00:24:38,320
pneuma tuning support in the management

00:24:36,159 --> 00:24:40,080
tools that use kvm

00:24:38,320 --> 00:24:42,159
so that they can automatically set these

00:24:40,080 --> 00:24:44,159
things up and we don't need to

00:24:42,159 --> 00:24:45,600
manually tune it it gets especially hard

00:24:44,159 --> 00:24:47,279
when we have a lot of vms

00:24:45,600 --> 00:24:49,120
or when we do live migration so the

00:24:47,279 --> 00:24:50,159
situation is dynamic and it's no longer

00:24:49,120 --> 00:24:53,120
so easy to

00:24:50,159 --> 00:24:54,400
come up with a static pinning that makes

00:24:53,120 --> 00:24:58,159
sense

00:24:54,400 --> 00:25:00,559
okay so we covered

00:24:58,159 --> 00:25:02,799
the importance of pneuma and a bit about

00:25:00,559 --> 00:25:06,240
how to tune it and where to look

00:25:02,799 --> 00:25:08,400
so next up is cpu idle halt poll

00:25:06,240 --> 00:25:10,240
this is going back to the i o request

00:25:08,400 --> 00:25:14,080
life cycle that we looked at

00:25:10,240 --> 00:25:16,320
so what we saw was that the two

00:25:14,080 --> 00:25:18,799
important mechanisms that we have are

00:25:16,320 --> 00:25:22,080
submitting requests and completing them

00:25:18,799 --> 00:25:25,679
well if you have passed through an nvme

00:25:22,080 --> 00:25:27,520
pci device there are interrupts

00:25:25,679 --> 00:25:28,720
for the completion when requests

00:25:27,520 --> 00:25:32,480
complete there

00:25:28,720 --> 00:25:36,799
there is an interrupt now

00:25:32,480 --> 00:25:38,400
halting a vcpu involves a vm exit

00:25:36,799 --> 00:25:40,400
and if there's no further work to do

00:25:38,400 --> 00:25:42,320
even on the host then maybe the physical

00:25:40,400 --> 00:25:44,400
cpu will halt two

00:25:42,320 --> 00:25:46,880
it will go into low power state and then

00:25:44,400 --> 00:25:47,840
when the nvme drive completes the

00:25:46,880 --> 00:25:50,880
request

00:25:47,840 --> 00:25:53,120
it will fire the interrupt the cpu

00:25:50,880 --> 00:25:54,799
will come out of that low power state

00:25:53,120 --> 00:25:55,600
and there's a latency cost associated

00:25:54,799 --> 00:25:58,240
with that

00:25:55,600 --> 00:25:58,640
and then we can re-enter that vcpu can

00:25:58,240 --> 00:26:00,799
vm

00:25:58,640 --> 00:26:01,760
enter and you can see that this is this

00:26:00,799 --> 00:26:04,400
becomes a chain

00:26:01,760 --> 00:26:06,880
of several steps and it has a latency

00:26:04,400 --> 00:26:09,520
cost so we want to avoid that

00:26:06,880 --> 00:26:11,440
what the v and what the cpu idle halt

00:26:09,520 --> 00:26:14,480
pole driver does

00:26:11,440 --> 00:26:18,000
is it runs a busy weight loop

00:26:14,480 --> 00:26:19,679
inside the guest so on that vcpu at the

00:26:18,000 --> 00:26:21,279
point where it decides oh i have no more

00:26:19,679 --> 00:26:23,039
work to do

00:26:21,279 --> 00:26:25,039
so it has a timeout and it says well i'm

00:26:23,039 --> 00:26:26,559
going to try running a busy weight loop

00:26:25,039 --> 00:26:29,679
for a little while at least

00:26:26,559 --> 00:26:32,480
to see if something still becomes

00:26:29,679 --> 00:26:32,880
schedulable and becomes ready to run and

00:26:32,480 --> 00:26:35,279
so

00:26:32,880 --> 00:26:36,880
that way the vcpu is actually running is

00:26:35,279 --> 00:26:38,880
actually active when that interrupt

00:26:36,880 --> 00:26:39,760
comes in and so when that interrupt is

00:26:38,880 --> 00:26:42,640
delivered

00:26:39,760 --> 00:26:43,039
we can quickly schedule the application

00:26:42,640 --> 00:26:44,559
again

00:26:43,039 --> 00:26:46,240
after completing the request and we

00:26:44,559 --> 00:26:46,720
don't need to go through this long path

00:26:46,240 --> 00:26:48,480
of

00:26:46,720 --> 00:26:50,400
going down all the way to a halted

00:26:48,480 --> 00:26:53,120
physical cpu and coming back out

00:26:50,400 --> 00:26:54,720
that decreases latency now this

00:26:53,120 --> 00:26:57,440
mechanism makes sense when you are

00:26:54,720 --> 00:26:58,400
pinning vcpus because when you're

00:26:57,440 --> 00:27:00,960
pulling you're

00:26:58,400 --> 00:27:02,880
you're you're wasting those cpu cycles

00:27:00,960 --> 00:27:05,120
until there's some work to do

00:27:02,880 --> 00:27:06,720
and of course if you had lots of vms and

00:27:05,120 --> 00:27:08,000
if they were sharing cpus you wouldn't

00:27:06,720 --> 00:27:08,799
want them all to pull so this is

00:27:08,000 --> 00:27:10,640
something to do

00:27:08,799 --> 00:27:12,159
when you have a high performance or

00:27:10,640 --> 00:27:14,720
performance critical

00:27:12,159 --> 00:27:15,440
vm and you have assigned it a dedicated

00:27:14,720 --> 00:27:17,520
cpu

00:27:15,440 --> 00:27:21,039
okay now here's the tuning this is just

00:27:17,520 --> 00:27:24,320
for the syntax i won't go into detail

00:27:21,039 --> 00:27:26,240
all right so now here's our first graph

00:27:24,320 --> 00:27:28,640
so these are the results for doing

00:27:26,240 --> 00:27:32,320
random reads and random writes

00:27:28,640 --> 00:27:34,880
qdep1 on an nvme drive

00:27:32,320 --> 00:27:36,320
and what we see here is the blue bar the

00:27:34,880 --> 00:27:39,760
leftmost one

00:27:36,320 --> 00:27:43,520
is the bare metal result

00:27:39,760 --> 00:27:44,559
with the nvme drive the red bar the one

00:27:43,520 --> 00:27:48,000
in the middle

00:27:44,559 --> 00:27:51,279
is the vfio results so that

00:27:48,000 --> 00:27:52,640
is a virtual machine with a pci device

00:27:51,279 --> 00:27:54,159
assigned to it

00:27:52,640 --> 00:27:56,080
and we can see that that's typically

00:27:54,159 --> 00:27:58,240
lower not in all cases but but it's

00:27:56,080 --> 00:27:59,600
it's often lower and even by a

00:27:58,240 --> 00:28:01,600
significant amount

00:27:59,600 --> 00:28:03,919
and then the final result is the vfio

00:28:01,600 --> 00:28:06,720
with that cpu idle halt pole

00:28:03,919 --> 00:28:09,440
driver enabled and you can see that that

00:28:06,720 --> 00:28:11,360
one performs very well

00:28:09,440 --> 00:28:13,919
why is it performing better than bare

00:28:11,360 --> 00:28:17,279
metal right how is that possible well

00:28:13,919 --> 00:28:17,919
it's because bare metal isn't doing

00:28:17,279 --> 00:28:20,559
polling

00:28:17,919 --> 00:28:21,760
and so bare metal might halt the cpu it

00:28:20,559 --> 00:28:23,360
will save more power

00:28:21,760 --> 00:28:25,520
but then it will also have higher

00:28:23,360 --> 00:28:28,159
latency and the vm

00:28:25,520 --> 00:28:28,720
is staying active the cpu is still

00:28:28,159 --> 00:28:30,240
running

00:28:28,720 --> 00:28:31,840
and so it's able to take those

00:28:30,240 --> 00:28:33,200
completion interrupts with lower latency

00:28:31,840 --> 00:28:35,679
so actually in this benchmark it

00:28:33,200 --> 00:28:38,640
achieves higher performance

00:28:35,679 --> 00:28:40,480
so that's what we see here and we're

00:28:38,640 --> 00:28:42,159
going to take this a step further

00:28:40,480 --> 00:28:44,080
there's another polling approach we can

00:28:42,159 --> 00:28:45,520
use and it that will actually make the

00:28:44,080 --> 00:28:48,159
bare metal versus

00:28:45,520 --> 00:28:50,080
virtual machines comparison fairer and

00:28:48,159 --> 00:28:54,960
it will show us the final picture

00:28:50,080 --> 00:28:57,200
so here we go the linux nvme driver

00:28:54,960 --> 00:28:58,799
allows you to allocate cues for a

00:28:57,200 --> 00:29:01,440
specific usage

00:28:58,799 --> 00:29:02,720
so it's possible to actually reserve

00:29:01,440 --> 00:29:05,919
polling queues

00:29:02,720 --> 00:29:08,799
and what those poll mode cues do

00:29:05,919 --> 00:29:11,200
is when an application sets the high

00:29:08,799 --> 00:29:14,240
priority request flag

00:29:11,200 --> 00:29:15,279
then the kernel will busy wait for those

00:29:14,240 --> 00:29:18,320
requests to finish

00:29:15,279 --> 00:29:19,440
and it just calls the poll function in

00:29:18,320 --> 00:29:21,760
that driver

00:29:19,440 --> 00:29:22,480
in the nvme driver and that function

00:29:21,760 --> 00:29:23,919
will just check

00:29:22,480 --> 00:29:26,240
it will just check memory and have a

00:29:23,919 --> 00:29:28,000
look to see if the request has completed

00:29:26,240 --> 00:29:29,520
yet

00:29:28,000 --> 00:29:31,840
so the kernel can do polling for us and

00:29:29,520 --> 00:29:34,399
this is called io poll

00:29:31,840 --> 00:29:35,679
and so this improves completion latency

00:29:34,399 --> 00:29:37,679
and actually more than

00:29:35,679 --> 00:29:38,880
cpu idle halt paul because here we're

00:29:37,679 --> 00:29:42,640
guaranteed to

00:29:38,880 --> 00:29:45,279
be spinning and it doesn't give up

00:29:42,640 --> 00:29:46,399
it it keeps running in its default mode

00:29:45,279 --> 00:29:48,000
at least

00:29:46,399 --> 00:29:49,679
and this allows us to make a fair

00:29:48,000 --> 00:29:52,960
comparison because this is

00:29:49,679 --> 00:29:56,240
done by the driver in both bare metal

00:29:52,960 --> 00:29:57,600
and in in the virtual machine case

00:29:56,240 --> 00:29:59,039
so at the bottom of the slide here is

00:29:57,600 --> 00:30:00,880
the syntax in case you want to see how

00:29:59,039 --> 00:30:02,880
to enable it

00:30:00,880 --> 00:30:04,640
and let's look at the performance

00:30:02,880 --> 00:30:06,880
numbers

00:30:04,640 --> 00:30:08,960
so as you can see on this graph at this

00:30:06,880 --> 00:30:12,159
point when we enable that feature

00:30:08,960 --> 00:30:14,080
first of all the absolute number of iops

00:30:12,159 --> 00:30:15,279
the number of i o operations per second

00:30:14,080 --> 00:30:18,559
that we can

00:30:15,279 --> 00:30:19,760
achieve have jumped the max we were at

00:30:18,559 --> 00:30:21,919
was around 80 k

00:30:19,760 --> 00:30:23,520
before we used i o pol now that we're

00:30:21,919 --> 00:30:26,799
polling all the time

00:30:23,520 --> 00:30:29,840
we get up to over 120k iops

00:30:26,799 --> 00:30:32,720
for qdep1 4 kilobyte

00:30:29,840 --> 00:30:34,480
requests so you can see that's a

00:30:32,720 --> 00:30:38,159
significant performance improvement

00:30:34,480 --> 00:30:38,720
enabling iopal the other thing we see

00:30:38,159 --> 00:30:42,159
here

00:30:38,720 --> 00:30:45,600
is that the gap between bare metal

00:30:42,159 --> 00:30:47,520
and vms has closed so at this point

00:30:45,600 --> 00:30:49,760
they're they're very similar

00:30:47,520 --> 00:30:51,120
and you can say that using pci device

00:30:49,760 --> 00:30:53,679
passthrough you can get bare metal

00:30:51,120 --> 00:30:53,679
performance

00:30:54,000 --> 00:30:58,320
so that's great but what about

00:30:56,159 --> 00:30:59,919
situations where you cannot use

00:30:58,320 --> 00:31:01,519
pci device assignment right i had this

00:30:59,919 --> 00:31:03,440
big disclaimer i i

00:31:01,519 --> 00:31:04,880
covered all the cons and why why you

00:31:03,440 --> 00:31:07,519
sometimes cannot use it

00:31:04,880 --> 00:31:08,080
well in that case you can use virtio

00:31:07,519 --> 00:31:10,000
block

00:31:08,080 --> 00:31:11,919
and virtio block is an emulated storage

00:31:10,000 --> 00:31:13,039
controller it's a para virtualized

00:31:11,919 --> 00:31:15,519
device that was designed

00:31:13,039 --> 00:31:17,279
specifically for virtualization so it's

00:31:15,519 --> 00:31:19,600
been optimized over the years

00:31:17,279 --> 00:31:21,440
and has evolved and so it's a good

00:31:19,600 --> 00:31:23,600
storage controller to choose

00:31:21,440 --> 00:31:25,600
if you want to get good performance with

00:31:23,600 --> 00:31:27,120
kvm

00:31:25,600 --> 00:31:28,799
there are two settings though that i

00:31:27,120 --> 00:31:30,399
want to discuss here because they're not

00:31:28,799 --> 00:31:32,000
enabled by default yet

00:31:30,399 --> 00:31:33,679
and they do boost performance so they're

00:31:32,000 --> 00:31:36,559
worth considering

00:31:33,679 --> 00:31:38,159
the first one is multi-q although the

00:31:36,559 --> 00:31:41,440
feature has been there for years

00:31:38,159 --> 00:31:44,399
it hasn't really been used

00:31:41,440 --> 00:31:46,880
and in qmu 5.2 that's going to change in

00:31:44,399 --> 00:31:49,360
qme 5.2 the number of queues is going to

00:31:46,880 --> 00:31:51,679
default to the number of vcpus in other

00:31:49,360 --> 00:31:53,919
words multi-queue will be enabled

00:31:51,679 --> 00:31:56,480
by default on virtio block and also on

00:31:53,919 --> 00:31:58,480
virto scuzzy devices

00:31:56,480 --> 00:32:00,799
the reason why this is a win the reason

00:31:58,480 --> 00:32:04,000
why this improves performance

00:32:00,799 --> 00:32:07,279
is because giving every

00:32:04,000 --> 00:32:11,519
cpu every vcpu a dedicated queue

00:32:07,279 --> 00:32:14,640
means that now the completion interrupts

00:32:11,519 --> 00:32:16,080
they can be directed at the cpu that

00:32:14,640 --> 00:32:17,840
submitted the i o

00:32:16,080 --> 00:32:19,360
and that's the one where the task is

00:32:17,840 --> 00:32:20,000
scheduled and where we want to do our

00:32:19,360 --> 00:32:21,919
completion

00:32:20,000 --> 00:32:24,240
we don't want that interrupt to go to

00:32:21,919 --> 00:32:27,039
some other vcpu that then says oh

00:32:24,240 --> 00:32:28,000
okay i'd better wake up that task that's

00:32:27,039 --> 00:32:29,600
ready to run

00:32:28,000 --> 00:32:31,919
on on another cpu and i'll send a

00:32:29,600 --> 00:32:33,679
message we don't want inter processor

00:32:31,919 --> 00:32:37,600
interrupts

00:32:33,679 --> 00:32:40,399
so by giving every vcpu its own cue

00:32:37,600 --> 00:32:42,159
we can eliminate that and we can improve

00:32:40,399 --> 00:32:44,720
interrupt completion latency

00:32:42,159 --> 00:32:46,960
so that's why multicue helps in addition

00:32:44,720 --> 00:32:49,679
to this the linux block layer also has

00:32:46,960 --> 00:32:51,279
multi-queue support and there are some

00:32:49,679 --> 00:32:52,640
code paths in the linux block there that

00:32:51,279 --> 00:32:55,519
take advantage of it

00:32:52,640 --> 00:32:56,240
when the driver only allocates one queue

00:32:55,519 --> 00:32:59,519
we don't

00:32:56,240 --> 00:33:01,600
take those code paths and the most

00:32:59,519 --> 00:33:03,200
the most obvious user visible effect of

00:33:01,600 --> 00:33:05,120
this for example is that the i o

00:33:03,200 --> 00:33:08,720
scheduler is different

00:33:05,120 --> 00:33:10,480
for devices that have more than one

00:33:08,720 --> 00:33:11,760
queue and that are multi-q block drivers

00:33:10,480 --> 00:33:14,480
so

00:33:11,760 --> 00:33:18,000
that affects latency too and so it's

00:33:14,480 --> 00:33:18,000
it's best to enable multi-cue

00:33:18,159 --> 00:33:23,600
next there's the virtio 1.1 packedvertq

00:33:21,120 --> 00:33:26,320
layout so this is a new memory layout

00:33:23,600 --> 00:33:28,240
for the cues that virtio devices use and

00:33:26,320 --> 00:33:30,880
this layout is more efficient

00:33:28,240 --> 00:33:31,519
in the benchmarks that i've run i've

00:33:30,880 --> 00:33:34,240
seen that

00:33:31,519 --> 00:33:37,919
it improves video block performance so

00:33:34,240 --> 00:33:39,600
this is also worth taking into account

00:33:37,919 --> 00:33:39,919
it's not a huge win but it is a small

00:33:39,600 --> 00:33:42,000
win

00:33:39,919 --> 00:33:43,120
and and and when devices are so low

00:33:42,000 --> 00:33:46,880
latency then every

00:33:43,120 --> 00:33:46,880
small win is worth taking

00:33:47,919 --> 00:33:54,399
okay now here's the syntax we'll move on

00:33:52,320 --> 00:33:56,000
and what we're going to do is i'll walk

00:33:54,399 --> 00:33:56,880
through several more configuration

00:33:56,000 --> 00:33:58,399
topics

00:33:56,880 --> 00:34:00,240
and then i'll walk through some

00:33:58,399 --> 00:34:01,120
optimizations that i've implemented some

00:34:00,240 --> 00:34:03,760
prototypes

00:34:01,120 --> 00:34:05,360
some new stuff and finally at the end

00:34:03,760 --> 00:34:07,519
we'll look at a graph that

00:34:05,360 --> 00:34:08,399
stacks them all up and we can see how

00:34:07,519 --> 00:34:11,599
incrementally

00:34:08,399 --> 00:34:13,679
by combining them we can increase iops

00:34:11,599 --> 00:34:15,760
significantly

00:34:13,679 --> 00:34:17,760
so here we are i o threads is the next

00:34:15,760 --> 00:34:22,159
feature if you're using vertical block

00:34:17,760 --> 00:34:25,280
that is critical i o threads are a way

00:34:22,159 --> 00:34:27,520
for defining threads and assigning

00:34:25,280 --> 00:34:28,720
devices to them so it gives users

00:34:27,520 --> 00:34:32,399
control

00:34:28,720 --> 00:34:33,119
over which physical cpu device emulation

00:34:32,399 --> 00:34:36,320
and i o

00:34:33,119 --> 00:34:38,639
will run on so there's an

00:34:36,320 --> 00:34:40,159
end to one mapping there you can assign

00:34:38,639 --> 00:34:42,240
multiple devices

00:34:40,159 --> 00:34:44,079
to a single i o thread and you can

00:34:42,240 --> 00:34:46,079
define multiple i o threads as well so

00:34:44,079 --> 00:34:47,760
it gives you a lot of flexibility

00:34:46,079 --> 00:34:49,839
and this is great because it allows us

00:34:47,760 --> 00:34:50,879
to reflect the pneuma topology in our

00:34:49,839 --> 00:34:52,800
system

00:34:50,879 --> 00:34:54,480
it is also good for scalability because

00:34:52,800 --> 00:34:57,359
when you have vms with

00:34:54,480 --> 00:34:58,640
many devices that are doing heavy io you

00:34:57,359 --> 00:35:00,640
may want to

00:34:58,640 --> 00:35:02,079
put them into separate io threads and

00:35:00,640 --> 00:35:04,560
run them on separate cpu

00:35:02,079 --> 00:35:05,200
so that there's enough resources for

00:35:04,560 --> 00:35:08,000
both of them

00:35:05,200 --> 00:35:09,680
and no interference between them the

00:35:08,000 --> 00:35:10,720
final thing i want to mention about i o

00:35:09,680 --> 00:35:13,520
threads

00:35:10,720 --> 00:35:14,720
is that the i o threads feature when

00:35:13,520 --> 00:35:16,079
it's enabled

00:35:14,720 --> 00:35:18,480
that device will be able to take

00:35:16,079 --> 00:35:18,880
advantage of an adaptive polling event

00:35:18,480 --> 00:35:21,440
loop

00:35:18,880 --> 00:35:22,800
in qemu it's a different code path from

00:35:21,440 --> 00:35:24,560
cumi's main loop

00:35:22,800 --> 00:35:25,920
and it has lower latency because it's

00:35:24,560 --> 00:35:28,960
able to do polling

00:35:25,920 --> 00:35:31,920
instead of always yielding

00:35:28,960 --> 00:35:34,000
when it waits for file descriptors to

00:35:31,920 --> 00:35:35,920
become ready

00:35:34,000 --> 00:35:37,200
so that's another reason why it's faster

00:35:35,920 --> 00:35:40,560
and we'll see the numbers

00:35:37,200 --> 00:35:40,560
later on when i show the graph

00:35:40,720 --> 00:35:44,720
so here's the configuration the the

00:35:43,440 --> 00:35:46,960
things you can do for

00:35:44,720 --> 00:35:48,320
defining i o threads pinning them on the

00:35:46,960 --> 00:35:51,680
host and then

00:35:48,320 --> 00:35:55,200
assigning devices to i o threads

00:35:51,680 --> 00:35:55,200
in libvard xml syntax

00:35:55,520 --> 00:35:59,760
okay so now we're going to move on to

00:35:57,760 --> 00:36:01,520
things that aren't as standard yet

00:35:59,760 --> 00:36:03,040
things that aren't as widely known and

00:36:01,520 --> 00:36:05,280
as widely used

00:36:03,040 --> 00:36:07,280
there has been a user space nvme driver

00:36:05,280 --> 00:36:08,720
in qmu for some time now

00:36:07,280 --> 00:36:10,400
it's been there for a long time but it

00:36:08,720 --> 00:36:13,440
hasn't been used

00:36:10,400 --> 00:36:16,000
very widely yet what it is

00:36:13,440 --> 00:36:16,560
is it's somewhat similar to pci device

00:36:16,000 --> 00:36:19,680
assignment

00:36:16,560 --> 00:36:22,240
in that that pci device that nvme drive

00:36:19,680 --> 00:36:23,359
can be assigned to a particular vm

00:36:22,240 --> 00:36:25,359
however instead of

00:36:23,359 --> 00:36:27,520
passing the device through into the

00:36:25,359 --> 00:36:28,560
guest and exposing that physical device

00:36:27,520 --> 00:36:30,560
to the guest

00:36:28,560 --> 00:36:33,599
we still have an emulated virtio block

00:36:30,560 --> 00:36:36,400
device that the guest uses and sees

00:36:33,599 --> 00:36:37,680
and in qmu we have the driver so it's in

00:36:36,400 --> 00:36:40,800
qmu user space

00:36:37,680 --> 00:36:41,599
in the host and it's an nvme driver and

00:36:40,800 --> 00:36:44,800
so

00:36:41,599 --> 00:36:48,160
what this means is that we're able to

00:36:44,800 --> 00:36:52,000
get some performance benefits from

00:36:48,160 --> 00:36:53,680
having a user space driver that bypasses

00:36:52,000 --> 00:36:55,760
the kernel that means no system calls

00:36:53,680 --> 00:36:57,200
are necessary and and there is a shorter

00:36:55,760 --> 00:36:59,119
code path that's completely under the

00:36:57,200 --> 00:37:02,880
control of qmu

00:36:59,119 --> 00:37:04,400
um while still offering qmu's block

00:37:02,880 --> 00:37:07,040
layer features things like

00:37:04,400 --> 00:37:08,800
live migration or snapshots and so on

00:37:07,040 --> 00:37:11,440
even image formats can

00:37:08,800 --> 00:37:13,520
work on top of the user space driver so

00:37:11,440 --> 00:37:14,160
this solves some of the limitations of

00:37:13,520 --> 00:37:17,920
pci

00:37:14,160 --> 00:37:19,119
device passthrough and right now

00:37:17,920 --> 00:37:21,359
some improvements are being made

00:37:19,119 --> 00:37:23,040
upstream and activity has

00:37:21,359 --> 00:37:26,000
started up again around around this

00:37:23,040 --> 00:37:28,480
driver non-x86 architecture support

00:37:26,000 --> 00:37:30,320
is being added multi-cue is being added

00:37:28,480 --> 00:37:36,000
and more

00:37:30,320 --> 00:37:38,560
so this is the syntax for configuring it

00:37:36,000 --> 00:37:40,640
one thing that is missing from the nvme

00:37:38,560 --> 00:37:42,640
user space driver in qmu

00:37:40,640 --> 00:37:44,079
that i wanted to add an optimization i

00:37:42,640 --> 00:37:46,480
wanted to try out

00:37:44,079 --> 00:37:47,920
is polled queues because in nvme when

00:37:46,480 --> 00:37:51,040
you create a queue

00:37:47,920 --> 00:37:52,960
you assign to it an interrupt

00:37:51,040 --> 00:37:55,040
the completion queue has an interrupt

00:37:52,960 --> 00:37:58,240
and you can turn that off completely

00:37:55,040 --> 00:37:59,680
while creating the queue and

00:37:58,240 --> 00:38:02,079
when you have a queue that doesn't have

00:37:59,680 --> 00:38:04,240
an interrupt you can just poll

00:38:02,079 --> 00:38:05,920
for completions you can just look at the

00:38:04,240 --> 00:38:08,160
memory and see when those requests

00:38:05,920 --> 00:38:10,560
become ready

00:38:08,160 --> 00:38:11,359
and so doing so is an alternative to

00:38:10,560 --> 00:38:12,880
interrupts

00:38:11,359 --> 00:38:14,560
effectively this is kind of like

00:38:12,880 --> 00:38:18,160
switching from

00:38:14,560 --> 00:38:18,800
the eventfd style mechanism to a polling

00:38:18,160 --> 00:38:20,800
mechanism

00:38:18,800 --> 00:38:22,400
and so we hope that it will reduce

00:38:20,800 --> 00:38:24,079
latency

00:38:22,400 --> 00:38:26,160
one interesting thing about doing this

00:38:24,079 --> 00:38:28,960
though was that it requires

00:38:26,160 --> 00:38:31,040
changes to qmu's event loop itself

00:38:28,960 --> 00:38:33,040
because qmi's event loop is really

00:38:31,040 --> 00:38:34,640
fundamentally designed for file

00:38:33,040 --> 00:38:36,560
descriptor monitoring

00:38:34,640 --> 00:38:38,800
the adaptive polling has been added to

00:38:36,560 --> 00:38:40,880
it but the whole premise is that we only

00:38:38,800 --> 00:38:43,280
poll for short amounts of time

00:38:40,880 --> 00:38:44,560
now when we have a pull mode queue in

00:38:43,280 --> 00:38:47,119
the nvme driver

00:38:44,560 --> 00:38:48,640
we need to pull all the time but if we

00:38:47,119 --> 00:38:50,560
pull all the time we will be starving

00:38:48,640 --> 00:38:53,440
the file descriptors right because

00:38:50,560 --> 00:38:54,480
we're just spinning in our busy loop but

00:38:53,440 --> 00:38:55,599
we're not looking at the file

00:38:54,480 --> 00:38:58,960
descriptors

00:38:55,599 --> 00:39:00,960
so i've i have some patches that i'm

00:38:58,960 --> 00:39:02,960
going to send upstream that extend the

00:39:00,960 --> 00:39:05,040
event loop and in fact some of the iou

00:39:02,960 --> 00:39:08,160
ring work has already

00:39:05,040 --> 00:39:10,560
gone upstream and i found that using iou

00:39:08,160 --> 00:39:12,320
ring we're able to do this efficiently

00:39:10,560 --> 00:39:16,000
and integrate it into the busy weight

00:39:12,320 --> 00:39:18,240
loop without using syscalls

00:39:16,000 --> 00:39:20,079
okay so we'll look at the numbers for

00:39:18,240 --> 00:39:22,640
that in the end

00:39:20,079 --> 00:39:24,560
the next thing i want to share is an

00:39:22,640 --> 00:39:26,720
idea that in one form or another has

00:39:24,560 --> 00:39:29,440
already been around for a long time

00:39:26,720 --> 00:39:31,119
in 2014 when we introduced co-routines

00:39:29,440 --> 00:39:32,400
into the core block layer and started

00:39:31,119 --> 00:39:35,200
using them for request

00:39:32,400 --> 00:39:37,040
processing uh that was very useful

00:39:35,200 --> 00:39:39,760
because we needed them for

00:39:37,040 --> 00:39:40,640
for things like io throttling and and

00:39:39,760 --> 00:39:42,079
some of the

00:39:40,640 --> 00:39:44,480
the operations that were just getting

00:39:42,079 --> 00:39:47,200
really really complex and difficult to

00:39:44,480 --> 00:39:49,440
write in an asynchronous style

00:39:47,200 --> 00:39:52,560
so now there's request cueing and so on

00:39:49,440 --> 00:39:54,560
in in the core block layer in qmu

00:39:52,560 --> 00:39:56,400
but there was concerns even back then

00:39:54,560 --> 00:39:59,040
that maybe this overhead

00:39:56,400 --> 00:40:00,800
will become a problem and so there have

00:39:59,040 --> 00:40:04,240
been discussions in the past about

00:40:00,800 --> 00:40:05,119
can we optimize it away and really the

00:40:04,240 --> 00:40:07,680
thing is

00:40:05,119 --> 00:40:10,240
when you are not using certain qmu

00:40:07,680 --> 00:40:12,240
features like disk image formats or io

00:40:10,240 --> 00:40:14,319
throttling or storage migration

00:40:12,240 --> 00:40:16,160
while those things are inactive you

00:40:14,319 --> 00:40:18,480
don't really need to do the full request

00:40:16,160 --> 00:40:20,160
processing all that machinery that

00:40:18,480 --> 00:40:21,119
infrastructure is only needed to support

00:40:20,160 --> 00:40:22,880
those features

00:40:21,119 --> 00:40:25,760
so wouldn't it be great if there was a

00:40:22,880 --> 00:40:27,520
way to bypass it when it's not needed

00:40:25,760 --> 00:40:29,200
so as a prototype i've tried

00:40:27,520 --> 00:40:31,359
implementing this i've tried

00:40:29,200 --> 00:40:34,000
implementing an aio fast path

00:40:31,359 --> 00:40:35,119
what it does is it introduces an aio

00:40:34,000 --> 00:40:37,440
interface

00:40:35,119 --> 00:40:38,319
to the block drivers in qmu because

00:40:37,440 --> 00:40:40,079
currently

00:40:38,319 --> 00:40:42,000
they have a co-routine interface which

00:40:40,079 --> 00:40:45,920
kind of assumes that you're in this

00:40:42,000 --> 00:40:45,920
full request processing mode

00:40:46,079 --> 00:40:49,520
and that allows the virtio block

00:40:48,480 --> 00:40:52,880
emulation

00:40:49,520 --> 00:40:54,880
to call the nvme user space driver with

00:40:52,880 --> 00:40:56,880
with relatively little overhead and we

00:40:54,880 --> 00:40:58,960
can skip the full request processing

00:40:56,880 --> 00:41:01,839
step

00:40:58,960 --> 00:41:03,119
so we'll see those numbers the next

00:41:01,839 --> 00:41:06,319
thing i want to mention

00:41:03,119 --> 00:41:08,640
is that when we looked at pci device

00:41:06,319 --> 00:41:12,160
assignment we saw how beneficial

00:41:08,640 --> 00:41:15,680
linux i o poll is we saw that

00:41:12,160 --> 00:41:18,800
polling for the requests from the

00:41:15,680 --> 00:41:20,720
nvme driver is

00:41:18,800 --> 00:41:23,200
it reduces latency and it got us the

00:41:20,720 --> 00:41:26,319
highest the highest iops that we've

00:41:23,200 --> 00:41:29,920
achieved so far we had this 120 k iops

00:41:26,319 --> 00:41:32,160
bare metal so virtio block today

00:41:29,920 --> 00:41:34,000
the guest driver for virtio block does

00:41:32,160 --> 00:41:35,599
not implement this interface yet but

00:41:34,000 --> 00:41:36,800
it's a driver interface in fact it's

00:41:35,599 --> 00:41:37,920
this one function that we need to

00:41:36,800 --> 00:41:41,440
implement

00:41:37,920 --> 00:41:42,160
and so i have also written a prototype

00:41:41,440 --> 00:41:45,040
for that

00:41:42,160 --> 00:41:46,720
um it only supports qdep1 because that's

00:41:45,040 --> 00:41:48,640
what i was benchmarking it's not a

00:41:46,720 --> 00:41:50,000
full implementation it's a prototype to

00:41:48,640 --> 00:41:51,200
check

00:41:50,000 --> 00:41:53,359
what kind of effect it has on

00:41:51,200 --> 00:41:53,920
performance and the link to the git

00:41:53,359 --> 00:41:57,440
branches

00:41:53,920 --> 00:41:59,920
is on the slide so we'll look at that

00:41:57,440 --> 00:42:01,040
here we are so this is the final

00:41:59,920 --> 00:42:02,960
incremental

00:42:01,040 --> 00:42:05,520
applying all of these optimizations on

00:42:02,960 --> 00:42:07,520
top of each other and how far it gets us

00:42:05,520 --> 00:42:09,760
on the left hand side the starting

00:42:07,520 --> 00:42:12,240
position we want to look at bare metal

00:42:09,760 --> 00:42:14,319
and without i o pull bare metal is at

00:42:12,240 --> 00:42:18,079
78k iops

00:42:14,319 --> 00:42:21,200
now when we configure qmu with a file

00:42:18,079 --> 00:42:24,240
um aio equals native so this is a

00:42:21,200 --> 00:42:27,040
a standard non-optimized setup

00:42:24,240 --> 00:42:27,839
and we don't use the i o thread then we

00:42:27,040 --> 00:42:30,160
start at

00:42:27,839 --> 00:42:32,319
21k io so that's extremely low we can

00:42:30,160 --> 00:42:34,000
see there's a lot of overhead

00:42:32,319 --> 00:42:36,880
i wouldn't necessarily say that this is

00:42:34,000 --> 00:42:38,800
what most qmu users experience today

00:42:36,880 --> 00:42:40,480
because i o threads is recommended and

00:42:38,800 --> 00:42:43,599
more and more of the management

00:42:40,480 --> 00:42:44,640
tools built on top of kvm and qmu have

00:42:43,599 --> 00:42:48,480
been using it

00:42:44,640 --> 00:42:50,800
by default so hopefully more

00:42:48,480 --> 00:42:51,760
hopefully most users today are around

00:42:50,800 --> 00:42:54,000
the second

00:42:51,760 --> 00:42:56,480
blue bar the i o thread bar so when we

00:42:54,000 --> 00:42:59,680
add i o threads

00:42:56,480 --> 00:43:00,000
then we are at around 46k iops there's

00:42:59,680 --> 00:43:02,880
still

00:43:00,000 --> 00:43:04,560
significant overhead right it's still

00:43:02,880 --> 00:43:06,720
pretty bad

00:43:04,560 --> 00:43:08,000
so next up we can enable virtio block

00:43:06,720 --> 00:43:09,680
multi queue

00:43:08,000 --> 00:43:11,440
now doing it at this stage actually

00:43:09,680 --> 00:43:13,599
turned out not to be very

00:43:11,440 --> 00:43:14,800
instructive although it slightly

00:43:13,599 --> 00:43:16,640
improved performance

00:43:14,800 --> 00:43:17,920
it wasn't it wasn't very significant in

00:43:16,640 --> 00:43:20,480
this graph

00:43:17,920 --> 00:43:22,319
but it's still essential part of the

00:43:20,480 --> 00:43:23,680
reason why it didn't improve performance

00:43:22,319 --> 00:43:25,280
very much in this graph because i was

00:43:23,680 --> 00:43:27,280
already using pinning both inside the

00:43:25,280 --> 00:43:28,960
guest and on the host and so on so

00:43:27,280 --> 00:43:30,640
everything was already set up optimally

00:43:28,960 --> 00:43:32,079
adding the cues didn't help the i o

00:43:30,640 --> 00:43:34,880
scheduler was already

00:43:32,079 --> 00:43:36,480
none so that didn't help and so on but

00:43:34,880 --> 00:43:38,000
it is an essential part

00:43:36,480 --> 00:43:41,520
of making things scale and making things

00:43:38,000 --> 00:43:45,040
work so keep multi-cue

00:43:41,520 --> 00:43:46,240
next up we introduce the user space nvme

00:43:45,040 --> 00:43:48,319
driver in qmu

00:43:46,240 --> 00:43:49,839
and that boosts performance so we jump

00:43:48,319 --> 00:43:53,520
almost

00:43:49,839 --> 00:43:55,520
10k from 46k to 55k

00:43:53,520 --> 00:43:57,760
so that's that's a nice boost getting us

00:43:55,520 --> 00:44:00,800
closer to bare metal

00:43:57,760 --> 00:44:02,880
now what happens when we try the vertio

00:44:00,800 --> 00:44:06,240
block guest drivers iopal

00:44:02,880 --> 00:44:09,440
prototype so adding that on

00:44:06,240 --> 00:44:11,520
brings us up to the above the initial

00:44:09,440 --> 00:44:14,240
bare metal number that we collected

00:44:11,520 --> 00:44:16,960
without iopol on the host side

00:44:14,240 --> 00:44:18,400
so what this is doing is um now that

00:44:16,960 --> 00:44:19,520
we're we're polling and we're using more

00:44:18,400 --> 00:44:22,000
cpu cycles

00:44:19,520 --> 00:44:23,520
we are able to make some ground there

00:44:22,000 --> 00:44:26,800
we're able to

00:44:23,520 --> 00:44:28,480
reduce the latency that we had so this

00:44:26,800 --> 00:44:29,760
is looking good but it's also unfair

00:44:28,480 --> 00:44:30,960
because now really we should be

00:44:29,760 --> 00:44:33,520
comparing against

00:44:30,960 --> 00:44:34,640
a bare metal that is also using i o poll

00:44:33,520 --> 00:44:36,160
so let's do that

00:44:34,640 --> 00:44:39,040
on the right hand side of the graph you

00:44:36,160 --> 00:44:42,079
see the gray bar that is 121

00:44:39,040 --> 00:44:45,280
120 k iops that's bare metal with

00:44:42,079 --> 00:44:46,560
nvme io pol so we're still behind we

00:44:45,280 --> 00:44:48,079
still have overhead but

00:44:46,560 --> 00:44:50,079
our absolute number of iops has

00:44:48,079 --> 00:44:53,040
increased um

00:44:50,079 --> 00:44:53,680
quite quite well and we're not done yet

00:44:53,040 --> 00:44:56,720
so next

00:44:53,680 --> 00:44:58,480
up we can try the nvme user space

00:44:56,720 --> 00:45:01,839
drivers polling cues

00:44:58,480 --> 00:45:03,119
where we are polling in qemu in the i o

00:45:01,839 --> 00:45:06,240
thread

00:45:03,119 --> 00:45:08,240
and this brings us up to 94k

00:45:06,240 --> 00:45:10,319
iops so that's definitely a worthwhile

00:45:08,240 --> 00:45:12,880
improvement a nice jump there

00:45:10,319 --> 00:45:14,079
and then finally the aio fast path that

00:45:12,880 --> 00:45:16,560
i just mentioned

00:45:14,079 --> 00:45:17,760
so this is the final optimization that

00:45:16,560 --> 00:45:20,720
i'm going to show today the final

00:45:17,760 --> 00:45:22,880
prototype that that i wanted to share

00:45:20,720 --> 00:45:24,960
and that bypasses the the full request

00:45:22,880 --> 00:45:25,599
processing in qmu and as you can see so

00:45:24,960 --> 00:45:28,960
that that

00:45:25,599 --> 00:45:31,440
that is another 10k uh

00:45:28,960 --> 00:45:32,720
further towards closing the gap to bare

00:45:31,440 --> 00:45:34,079
metal

00:45:32,720 --> 00:45:35,760
so that's the status that's what i

00:45:34,079 --> 00:45:38,560
wanted to share with you

00:45:35,760 --> 00:45:42,480
and i am working on upstreaming these

00:45:38,560 --> 00:45:42,480
optimizations so that they can be used

00:45:43,119 --> 00:45:46,960
but the entire vertical block and nvme

00:45:46,160 --> 00:45:50,160
approach

00:45:46,960 --> 00:45:51,920
with the nvme driver nvme user space

00:45:50,160 --> 00:45:54,240
driver has still left us with

00:45:51,920 --> 00:45:55,200
a similar limitation as pci paths do we

00:45:54,240 --> 00:45:59,520
still need

00:45:55,200 --> 00:46:02,000
one device per guest luckily this year

00:45:59,520 --> 00:46:04,160
a new tool has been added to qmu called

00:46:02,000 --> 00:46:06,560
qmu storage daemon

00:46:04,160 --> 00:46:09,760
and this is a separate program that has

00:46:06,560 --> 00:46:11,680
qmu storage related functionality

00:46:09,760 --> 00:46:13,920
in addition to that a vhost user block

00:46:11,680 --> 00:46:16,240
server has also been added to qmu which

00:46:13,920 --> 00:46:18,720
is very convenient it means that now

00:46:16,240 --> 00:46:19,359
we can use the storage daemon we can

00:46:18,720 --> 00:46:22,960
host

00:46:19,359 --> 00:46:25,760
the nvme user space driver inside it

00:46:22,960 --> 00:46:26,880
and that one daemon can serve multiple

00:46:25,760 --> 00:46:29,760
guests

00:46:26,880 --> 00:46:30,480
so we now have the ability to share a

00:46:29,760 --> 00:46:32,720
single

00:46:30,480 --> 00:46:34,720
pci device and use the user space driver

00:46:32,720 --> 00:46:36,160
and have multiple guests so that solves

00:46:34,720 --> 00:46:39,200
that limitation

00:46:36,160 --> 00:46:41,680
uh it's already available in cumu.kit

00:46:39,200 --> 00:46:43,440
but the code path is different from the

00:46:41,680 --> 00:46:44,079
vertical block results that i presented

00:46:43,440 --> 00:46:45,839
to you

00:46:44,079 --> 00:46:47,599
so those optimizations don't apply it

00:46:45,839 --> 00:46:48,240
some of them still need to be ported to

00:46:47,599 --> 00:46:51,200
this

00:46:48,240 --> 00:46:52,000
so over time we can expect this to equal

00:46:51,200 --> 00:46:53,839
the the

00:46:52,000 --> 00:46:55,040
results that i just showed and then this

00:46:53,839 --> 00:46:57,599
will be an excellent way

00:46:55,040 --> 00:46:58,160
for if you need to share drives on top

00:46:57,599 --> 00:47:00,079
of this

00:46:58,160 --> 00:47:01,200
the qm storage demand offers a lot of

00:47:00,079 --> 00:47:04,319
other functionality

00:47:01,200 --> 00:47:05,839
some of the cool things are nbd exports

00:47:04,319 --> 00:47:09,200
that would allow you to

00:47:05,839 --> 00:47:12,240
also attach those drives on the host

00:47:09,200 --> 00:47:14,160
or applications can use them

00:47:12,240 --> 00:47:15,440
and infuse exports are also in

00:47:14,160 --> 00:47:16,880
development

00:47:15,440 --> 00:47:18,560
they're the block jobs features are

00:47:16,880 --> 00:47:19,440
available so human search human will be

00:47:18,560 --> 00:47:20,880
a nice utility

00:47:19,440 --> 00:47:22,640
and i think we're going to see more use

00:47:20,880 --> 00:47:24,880
of it in the future

00:47:22,640 --> 00:47:26,160
okay now if you saw the kimi storage

00:47:24,880 --> 00:47:27,839
demon site you might have thought wait a

00:47:26,160 --> 00:47:29,280
second this is a familiar architecture

00:47:27,839 --> 00:47:31,359
we know this approach yes

00:47:29,280 --> 00:47:33,200
it's very similar to spdk storage

00:47:31,359 --> 00:47:34,720
performance development kit

00:47:33,200 --> 00:47:36,160
and that also uses a polling

00:47:34,720 --> 00:47:36,640
architecture and it's been around for

00:47:36,160 --> 00:47:39,680
years

00:47:36,640 --> 00:47:42,880
in fact the v host user block

00:47:39,680 --> 00:47:45,200
interface was created in order to

00:47:42,880 --> 00:47:47,040
connect qmu and spdk

00:47:45,200 --> 00:47:48,640
so we're very thankful that that already

00:47:47,040 --> 00:47:51,119
exists and we can reuse it

00:47:48,640 --> 00:47:51,760
so i wanted to mention spdk because

00:47:51,119 --> 00:47:53,280
obviously

00:47:51,760 --> 00:47:55,440
this has some influence and it's great

00:47:53,280 --> 00:47:56,079
project to check out if you want to find

00:47:55,440 --> 00:47:59,200
out more

00:47:56,079 --> 00:48:02,480
about what's going on in improving

00:47:59,200 --> 00:48:05,440
the general non-nvme case please

00:48:02,480 --> 00:48:06,240
check out stefano gazzarella's talk this

00:48:05,440 --> 00:48:08,880
year at

00:48:06,240 --> 00:48:10,720
kvm forum he'll be going into what he's

00:48:08,880 --> 00:48:13,520
done with i o u ring and some of the new

00:48:10,720 --> 00:48:16,559
stuff that he's working on

00:48:13,520 --> 00:48:17,680
finally the future direction so in the

00:48:16,559 --> 00:48:20,319
short term

00:48:17,680 --> 00:48:23,599
it's time to get these prototypes into a

00:48:20,319 --> 00:48:25,200
polished state get them upstream

00:48:23,599 --> 00:48:26,800
that will allow us to reach the

00:48:25,200 --> 00:48:28,400
performance that i've shown you here on

00:48:26,800 --> 00:48:31,359
these slides

00:48:28,400 --> 00:48:31,839
and in the longer term i think what's

00:48:31,359 --> 00:48:35,200
clear

00:48:31,839 --> 00:48:35,760
is that pci device assignment because it

00:48:35,200 --> 00:48:39,440
gives us

00:48:35,760 --> 00:48:41,280
bare metal performance it's important to

00:48:39,440 --> 00:48:43,040
find more ways to pass through

00:48:41,280 --> 00:48:44,559
devices because when the hypervisor is

00:48:43,040 --> 00:48:45,440
not involved when there's no software

00:48:44,559 --> 00:48:47,680
path

00:48:45,440 --> 00:48:48,480
that's how we get the best performance

00:48:47,680 --> 00:48:52,160
so

00:48:48,480 --> 00:48:53,680
summary what have we looked at well

00:48:52,160 --> 00:48:55,280
there's the basic configuration and

00:48:53,680 --> 00:48:57,839
tuning that is essential

00:48:55,280 --> 00:48:58,880
the pneuma cpu idle helpful and i o

00:48:57,839 --> 00:49:01,280
thread setup

00:48:58,880 --> 00:49:02,720
that gives you a basic performant

00:49:01,280 --> 00:49:04,480
starting point

00:49:02,720 --> 00:49:06,559
and then you have the big choice do you

00:49:04,480 --> 00:49:08,000
want to use pci device assignment

00:49:06,559 --> 00:49:08,960
because that way you'll have the minimal

00:49:08,000 --> 00:49:11,040
overhead

00:49:08,960 --> 00:49:12,160
that's the best way to go if performance

00:49:11,040 --> 00:49:13,599
is critical

00:49:12,160 --> 00:49:16,400
but you need to keep in mind the

00:49:13,599 --> 00:49:19,520
limitations of that feature

00:49:16,400 --> 00:49:20,800
and if you decide that you can't use pci

00:49:19,520 --> 00:49:23,040
device assignment then you can use

00:49:20,800 --> 00:49:23,520
virtio block with the user space nvme

00:49:23,040 --> 00:49:25,760
driver

00:49:23,520 --> 00:49:27,440
that will boost performance and finally

00:49:25,760 --> 00:49:30,319
the qmu storage daemon

00:49:27,440 --> 00:49:30,880
now allows the sharing of user space

00:49:30,319 --> 00:49:35,359
nvme

00:49:30,880 --> 00:49:39,839
drives with multiple guests

00:49:35,359 --> 00:49:42,559
thank you i also published the

00:49:39,839 --> 00:49:43,280
ansible playbooks that i used to collect

00:49:42,559 --> 00:49:44,640
the data

00:49:43,280 --> 00:49:46,480
if you want to go and look at the

00:49:44,640 --> 00:49:48,480
specifics of the benchmarks

00:49:46,480 --> 00:49:57,520
there's a url in the slide thank you

00:49:48,480 --> 00:49:57,520

YouTube URL: https://www.youtube.com/watch?v=y-MAQL9AOnc


