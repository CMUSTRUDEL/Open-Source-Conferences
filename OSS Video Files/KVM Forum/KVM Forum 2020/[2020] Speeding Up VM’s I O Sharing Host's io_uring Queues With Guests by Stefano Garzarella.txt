Title: [2020] Speeding Up VMâ€™s I O Sharing Host's io_uring Queues With Guests by Stefano Garzarella
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	io_uring is the newest Linux I/O interface. It provides submission and completion queues for performing asynchronous I/O operations.

The queues are located in a memory region shared between the userspace application and the kernel. This aims to reduce the number of syscalls required for I/O operations and provides a way to poll efficiently. io_uring achieves good performance and it makes exposing submission and completion queues to guests an attractive idea for improving I/O performance in virtualization.

Stefano will give a brief overview of io_uring API. Then, he will illustrate how the host's io_uring queues can be shared with guests to improve I/O performance of a block device and which io_uring changes are required to safely give queues access to the guest. Finally, Stefano will show the performance boost achieved with the proposed approach and future steps.

---

Stefano Garzarella
Red Hat, Senior Software Engineer
Italy

Stefano is a Senior Software Engineer at Red Hat. He is working on virtualization and networking topics in QEMU and Linux kernel. He is a co-maintainer of Linux's virtio-vsock. Current projects cover virtio-vsock, QEMU network and storage, and lightweight VMs.
Captions: 
	00:00:06,160 --> 00:00:08,800
hello everyone

00:00:06,960 --> 00:00:10,800
and thank you for attending this talk

00:00:08,800 --> 00:00:12,240
i'm stefano garzarella i'm a senior

00:00:10,800 --> 00:00:15,120
software engineer in

00:00:12,240 --> 00:00:16,960
the red dot virtualization team today

00:00:15,120 --> 00:00:17,840
we'll take a look on how to speed up the

00:00:16,960 --> 00:00:20,240
hamsa yo

00:00:17,840 --> 00:00:22,560
sharing all's are your urine cues with

00:00:20,240 --> 00:00:22,560
guests

00:00:22,800 --> 00:00:27,279
this is the agenda of the tour first of

00:00:25,439 --> 00:00:27,840
all we'll take an overview of value

00:00:27,279 --> 00:00:30,240
urine

00:00:27,840 --> 00:00:31,840
looking at the system codes how the

00:00:30,240 --> 00:00:34,559
queues are organized

00:00:31,840 --> 00:00:36,079
and some interesting features like

00:00:34,559 --> 00:00:40,000
resource registration

00:00:36,079 --> 00:00:43,120
and poly then we look at premium

00:00:40,000 --> 00:00:44,960
and how we use it are you urine at that

00:00:43,120 --> 00:00:48,000
point we'll see how to speed up

00:00:44,960 --> 00:00:51,120
brother your block sharing iu urine cues

00:00:48,000 --> 00:00:53,600
directly with a guest we'll also see

00:00:51,120 --> 00:00:55,120
some alternatives to this approach like

00:00:53,600 --> 00:00:59,120
b host block

00:00:55,120 --> 00:01:03,199
and vdpa block and we'll compare them

00:00:59,120 --> 00:01:03,199
finally we'll talk about next steps

00:01:05,119 --> 00:01:08,400
io urine is a new linux interface

00:01:07,920 --> 00:01:10,560
between

00:01:08,400 --> 00:01:12,000
user space and kernel to do a

00:01:10,560 --> 00:01:14,080
synchronous io

00:01:12,000 --> 00:01:15,439
it's not only oriented to block

00:01:14,080 --> 00:01:18,000
operation but

00:01:15,439 --> 00:01:19,360
it's evolved like a generic interface to

00:01:18,000 --> 00:01:22,560
do asynchronous

00:01:19,360 --> 00:01:25,360
system goals the interface consists

00:01:22,560 --> 00:01:26,240
of a pair of rings allocated by the

00:01:25,360 --> 00:01:29,759
kernel

00:01:26,240 --> 00:01:31,360
and shared with the user space one ring

00:01:29,759 --> 00:01:34,000
is used by the application

00:01:31,360 --> 00:01:34,479
to submit new requests to do and it's

00:01:34,000 --> 00:01:37,680
called

00:01:34,479 --> 00:01:41,040
submission queue ascu the harder ring

00:01:37,680 --> 00:01:43,280
called completion queue cq is used by

00:01:41,040 --> 00:01:46,479
the kernel to return to the user

00:01:43,280 --> 00:01:49,600
the result of the submitted request

00:01:46,479 --> 00:01:53,040
there are three system calls exposed by

00:01:49,600 --> 00:01:56,079
io urine that we are going to explore

00:01:53,040 --> 00:01:59,360
the the first one

00:01:56,079 --> 00:02:01,600
is i o urine setup it's the first system

00:01:59,360 --> 00:02:04,640
called to invoke to set up the context

00:02:01,600 --> 00:02:06,560
for performing a synchronous io

00:02:04,640 --> 00:02:09,599
several flags and parameters can be

00:02:06,560 --> 00:02:12,319
specified such as the ring size

00:02:09,599 --> 00:02:14,800
it returns a file descriptor that

00:02:12,319 --> 00:02:18,640
identifies the context and it must be

00:02:14,800 --> 00:02:21,840
used with the harder system codes

00:02:18,640 --> 00:02:23,760
the second one is iu urine register this

00:02:21,840 --> 00:02:26,160
system code is also used during the

00:02:23,760 --> 00:02:28,959
initialization phase of the rings

00:02:26,160 --> 00:02:29,760
or even afterwards to change registered

00:02:28,959 --> 00:02:34,000
stuff

00:02:29,760 --> 00:02:37,840
but it's not used in the critical part

00:02:34,000 --> 00:02:37,840
we'll talk more about it in a few slides

00:02:38,000 --> 00:02:41,360
the last one is are you during enter

00:02:40,560 --> 00:02:43,840
it's the most

00:02:41,360 --> 00:02:46,000
used system called during the life cycle

00:02:43,840 --> 00:02:49,840
of the context because it's used

00:02:46,000 --> 00:02:51,120
to initiate and or to complete a

00:02:49,840 --> 00:02:54,400
synchronous io

00:02:51,120 --> 00:02:57,680
so with a single system call we can

00:02:54,400 --> 00:03:00,239
submit new operation to do and reap

00:02:57,680 --> 00:03:05,840
operations done using the rings that we

00:03:00,239 --> 00:03:05,840
are going to see in the next slides

00:03:06,239 --> 00:03:09,360
the submission queue is used by the

00:03:08,560 --> 00:03:12,080
application

00:03:09,360 --> 00:03:13,280
to submit new requests producing a new

00:03:12,080 --> 00:03:16,640
sq entry

00:03:13,280 --> 00:03:19,599
sqe that contains the operation to do

00:03:16,640 --> 00:03:21,040
and its parameters like file descriptor

00:03:19,599 --> 00:03:24,400
buffer address

00:03:21,040 --> 00:03:24,400
offset et cetera

00:03:24,720 --> 00:03:28,159
when the application has one or more sql

00:03:27,360 --> 00:03:30,560
ready

00:03:28,159 --> 00:03:32,400
it increases the day and calls are your

00:03:30,560 --> 00:03:34,480
urine enter to pass control to the

00:03:32,400 --> 00:03:37,440
corner

00:03:34,480 --> 00:03:38,640
at this point the kernel consumes sqe

00:03:37,440 --> 00:03:41,120
updates the head

00:03:38,640 --> 00:03:44,239
and it schedules the work to execute the

00:03:41,120 --> 00:03:44,239
operation requested

00:03:44,640 --> 00:03:48,159
the canal wheel processes the operation

00:03:47,440 --> 00:03:50,640
schedule

00:03:48,159 --> 00:03:51,280
and when they are done it prepares a new

00:03:50,640 --> 00:03:54,159
cq

00:03:51,280 --> 00:03:56,000
entry cqe for each submitted request

00:03:54,159 --> 00:03:59,120
that contains the result of the

00:03:56,000 --> 00:03:59,120
operation requested

00:03:59,360 --> 00:04:04,799
the cqe which also contains the same

00:04:02,640 --> 00:04:05,599
user data values specified by the

00:04:04,799 --> 00:04:08,879
application

00:04:05,599 --> 00:04:12,319
in the corresponding sqe so it's

00:04:08,879 --> 00:04:14,080
an apac value for the kernel and can be

00:04:12,319 --> 00:04:16,239
used by the application to

00:04:14,080 --> 00:04:17,519
match the result with the submitted

00:04:16,239 --> 00:04:19,359
operation

00:04:17,519 --> 00:04:21,199
the kernel increases the tail of the

00:04:19,359 --> 00:04:24,079
completion queue when it adds

00:04:21,199 --> 00:04:24,079
new cqe

00:04:24,960 --> 00:04:28,639
and the application will consume cqes

00:04:27,680 --> 00:04:30,479
moving the head

00:04:28,639 --> 00:04:34,960
and checking the result of the operation

00:04:30,479 --> 00:04:34,960
submitted with the same user data

00:04:37,520 --> 00:04:42,320
for each request the kernel must take an

00:04:40,000 --> 00:04:44,000
internal reference to the file pointed

00:04:42,320 --> 00:04:46,160
by the file descriptor

00:04:44,000 --> 00:04:47,120
and release it when the operation is

00:04:46,160 --> 00:04:50,240
done

00:04:47,120 --> 00:04:50,800
it also need to map and then map every

00:04:50,240 --> 00:04:53,360
time

00:04:50,800 --> 00:04:55,280
for each request the user buffer in the

00:04:53,360 --> 00:04:58,560
kernel virtual memory

00:04:55,280 --> 00:05:01,600
in order to reduce the overhead for

00:04:58,560 --> 00:05:04,080
each request if the application has

00:05:01,600 --> 00:05:04,720
a set of file descriptors and user

00:05:04,080 --> 00:05:07,840
buffers

00:05:04,720 --> 00:05:09,840
users very often we can pre-register

00:05:07,840 --> 00:05:10,880
them with the higher urine register

00:05:09,840 --> 00:05:13,840
system code

00:05:10,880 --> 00:05:16,400
and use an index in the sqe in this way

00:05:13,840 --> 00:05:19,840
the kernel already has the reference

00:05:16,400 --> 00:05:22,080
and it used that index to get it

00:05:19,840 --> 00:05:25,199
this system call can be used also to

00:05:22,080 --> 00:05:27,280
register other resources like an eventfd

00:05:25,199 --> 00:05:29,280
to receive notification when some

00:05:27,280 --> 00:05:31,840
request is completed

00:05:29,280 --> 00:05:34,400
or it can be used to pro by urine to get

00:05:31,840 --> 00:05:37,600
information about the code supported

00:05:34,400 --> 00:05:38,800
by the running kernel it can be also

00:05:37,600 --> 00:05:41,600
used to register

00:05:38,800 --> 00:05:42,880
personality to issue sqe with certain

00:05:41,600 --> 00:05:45,759
credentials

00:05:42,880 --> 00:05:46,720
or as we can see later to register

00:05:45,759 --> 00:05:52,320
restriction

00:05:46,720 --> 00:05:54,800
and enable ring processing

00:05:52,320 --> 00:05:55,360
another good feature provided by io

00:05:54,800 --> 00:05:57,600
urine

00:05:55,360 --> 00:05:59,280
is the poly we have the possibility to

00:05:57,600 --> 00:06:02,240
enable the sq polling

00:05:59,280 --> 00:06:03,919
and the eu poly in the first case a

00:06:02,240 --> 00:06:06,080
kernel trade is created

00:06:03,919 --> 00:06:08,160
to poll the submission queue avoiding

00:06:06,080 --> 00:06:09,919
the needs of system call to pass

00:06:08,160 --> 00:06:12,639
controlled again

00:06:09,919 --> 00:06:13,600
an idle time is configurable so if the

00:06:12,639 --> 00:06:15,759
kernel trial

00:06:13,600 --> 00:06:17,840
is either for more than a configured

00:06:15,759 --> 00:06:19,840
time it goes to sleep

00:06:17,840 --> 00:06:20,880
and the application must call are you

00:06:19,840 --> 00:06:23,120
uring enter

00:06:20,880 --> 00:06:25,520
with a special flag to wake up the

00:06:23,120 --> 00:06:28,000
current trial

00:06:25,520 --> 00:06:30,000
when this feature is enabled potentially

00:06:28,000 --> 00:06:32,960
the application can submit

00:06:30,000 --> 00:06:34,880
and reap request without doing a single

00:06:32,960 --> 00:06:38,000
system code

00:06:34,880 --> 00:06:38,560
we can also enable io poly doing busy

00:06:38,000 --> 00:06:41,759
wait

00:06:38,560 --> 00:06:43,759
for io completion instead of waiting for

00:06:41,759 --> 00:06:47,440
an asynchronous notification

00:06:43,759 --> 00:06:49,599
such as an interrupt from the device

00:06:47,440 --> 00:06:51,280
this feature can be used only if the

00:06:49,599 --> 00:06:55,199
device or the file system

00:06:51,280 --> 00:06:55,199
support block io polling

00:06:57,680 --> 00:07:02,639
starting from queen 5.2 released the

00:07:00,479 --> 00:07:03,440
inhere previous year io urine is

00:07:02,639 --> 00:07:06,560
available

00:07:03,440 --> 00:07:09,280
in the qimo synchronous iosum system

00:07:06,560 --> 00:07:09,759
thanks to arushi julian stefan we have a

00:07:09,280 --> 00:07:12,720
new

00:07:09,759 --> 00:07:13,199
aio engine that we can use with a dash

00:07:12,720 --> 00:07:16,639
drive

00:07:13,199 --> 00:07:19,599
option the engine will do

00:07:16,639 --> 00:07:20,319
the standard block iu operation read

00:07:19,599 --> 00:07:23,360
write

00:07:20,319 --> 00:07:24,880
f sync in the synchronous way using

00:07:23,360 --> 00:07:28,160
their urine queues

00:07:24,880 --> 00:07:28,160
and operations

00:07:29,039 --> 00:07:33,199
now let's go into the main topic of the

00:07:31,280 --> 00:07:36,240
talk

00:07:33,199 --> 00:07:39,520
so how to speed up block io

00:07:36,240 --> 00:07:41,280
in qmu this is the starting point we

00:07:39,520 --> 00:07:44,639
have a virtual block device

00:07:41,280 --> 00:07:47,120
emulated increment that uses the io

00:07:44,639 --> 00:07:49,919
urine a ion joint to do

00:07:47,120 --> 00:07:50,800
block operation so we have two

00:07:49,919 --> 00:07:54,560
communication

00:07:50,800 --> 00:07:57,759
channels a vert queue between gas kernel

00:07:54,560 --> 00:08:01,680
and pmu and iou urine cues

00:07:57,759 --> 00:08:04,720
between qmu and the host kernel

00:08:01,680 --> 00:08:07,039
so there is a kind of translation

00:08:04,720 --> 00:08:08,240
made by kuemu from word queue

00:08:07,039 --> 00:08:12,800
descriptors

00:08:08,240 --> 00:08:16,319
to iou urine q100s and vice versa

00:08:12,800 --> 00:08:19,039
so if if we don't need the features of

00:08:16,319 --> 00:08:19,599
qima block layer for example if we are

00:08:19,039 --> 00:08:23,199
using

00:08:19,599 --> 00:08:25,599
raw files or devices we can bypass it

00:08:23,199 --> 00:08:26,479
and pass through the higher urine cues

00:08:25,599 --> 00:08:29,840
directly

00:08:26,479 --> 00:08:29,840
in the gas current memory

00:08:30,960 --> 00:08:36,320
so to realize a using posture the

00:08:34,000 --> 00:08:38,959
submission and completion cues

00:08:36,320 --> 00:08:39,599
are mapped in the guest memory and we

00:08:38,959 --> 00:08:42,719
modify

00:08:39,599 --> 00:08:45,440
the verdio block driver to use this new

00:08:42,719 --> 00:08:47,279
short path instead of instead of vert

00:08:45,440 --> 00:08:49,920
queues

00:08:47,279 --> 00:08:51,120
it will submit and reheat requests

00:08:49,920 --> 00:08:55,519
directly from the

00:08:51,120 --> 00:08:56,560
sq and securings we used a registered

00:08:55,519 --> 00:08:59,760
event of dna

00:08:56,560 --> 00:09:01,920
urine to inject interrupt in the guest

00:08:59,760 --> 00:09:03,360
when there are new security available

00:09:01,920 --> 00:09:05,839
also if we implementing

00:09:03,360 --> 00:09:08,959
a polling strategies where we disable

00:09:05,839 --> 00:09:10,480
these notifications about pauline we

00:09:08,959 --> 00:09:13,200
used the set of patches

00:09:10,480 --> 00:09:14,399
developed by stephanie noxie to enable

00:09:13,200 --> 00:09:17,839
the device polling

00:09:14,399 --> 00:09:22,480
through linux block io poll interface

00:09:17,839 --> 00:09:25,040
in the verdeo block driver

00:09:22,480 --> 00:09:27,760
we modify it to pull the completion

00:09:25,040 --> 00:09:30,880
queue in order to avoid interrupts

00:09:27,760 --> 00:09:32,959
in the host we we enable the sq

00:09:30,880 --> 00:09:35,519
polling to avoid notification from the

00:09:32,959 --> 00:09:38,399
guest reducing the vm exit

00:09:35,519 --> 00:09:39,680
and we also enable the yield poly to

00:09:38,399 --> 00:09:42,959
avoid the interrupts

00:09:39,680 --> 00:09:42,959
from the hardware device

00:09:45,120 --> 00:09:49,440
in order to share submission and

00:09:47,279 --> 00:09:52,240
completion cues with the guest

00:09:49,440 --> 00:09:52,959
we needed some changes in io urine the

00:09:52,240 --> 00:09:56,560
first one

00:09:52,959 --> 00:09:58,800
was a way to enable and disable event fd

00:09:56,560 --> 00:10:01,200
notification at runtime

00:09:58,800 --> 00:10:03,120
we use this feature to disable interrupt

00:10:01,200 --> 00:10:05,920
in the guest when we are following the

00:10:03,120 --> 00:10:05,920
completion cue

00:10:06,880 --> 00:10:10,160
the second change are the most important

00:10:09,040 --> 00:10:12,880
part we need

00:10:10,160 --> 00:10:14,640
a way to restrict the operations allowed

00:10:12,880 --> 00:10:18,079
in an eye urine context

00:10:14,640 --> 00:10:18,959
to safely share the rings with untrusted

00:10:18,079 --> 00:10:22,160
processes

00:10:18,959 --> 00:10:24,959
or guests i put a link to a good article

00:10:22,160 --> 00:10:28,480
on lwhan.net about this feature

00:10:24,959 --> 00:10:30,079
that we will discuss in the next slides

00:10:28,480 --> 00:10:32,079
the last change concern memory

00:10:30,079 --> 00:10:34,640
translation because io urine

00:10:32,079 --> 00:10:35,279
expat hosts virtual addresses but the

00:10:34,640 --> 00:10:38,480
driver

00:10:35,279 --> 00:10:40,640
in the guests use gas fist guest

00:10:38,480 --> 00:10:43,279
physical addresses so we need a

00:10:40,640 --> 00:10:45,440
mechanism to register the memory mapping

00:10:43,279 --> 00:10:46,640
allowing io urine to translate these

00:10:45,440 --> 00:10:48,480
addresses

00:10:46,640 --> 00:10:50,959
unfortunately this feature is not yet

00:10:48,480 --> 00:10:50,959
available

00:10:52,240 --> 00:10:56,720
as we saw we had the possibility to

00:10:54,720 --> 00:10:59,120
restrict the higher urine cues

00:10:56,720 --> 00:11:01,360
to share them with the guests for

00:10:59,120 --> 00:11:04,000
example we don't want to allow

00:11:01,360 --> 00:11:05,200
a guest to use all file descriptors

00:11:04,000 --> 00:11:08,160
opened by premium

00:11:05,200 --> 00:11:08,800
or to do any kind of operations we want

00:11:08,160 --> 00:11:12,160
to enable

00:11:08,800 --> 00:11:15,279
only some operation like rare read

00:11:12,160 --> 00:11:16,240
write sync on a subset of file

00:11:15,279 --> 00:11:18,480
descriptor

00:11:16,240 --> 00:11:20,160
so with the higher urine restriction

00:11:18,480 --> 00:11:23,200
feature we can install

00:11:20,160 --> 00:11:25,839
an allow list or an io urine context

00:11:23,200 --> 00:11:26,320
and only the operation defined in that

00:11:25,839 --> 00:11:29,519
list

00:11:26,320 --> 00:11:30,480
can be executed this also prevents that

00:11:29,519 --> 00:11:32,880
a new

00:11:30,480 --> 00:11:33,519
urine features accidentally become

00:11:32,880 --> 00:11:36,800
available

00:11:33,519 --> 00:11:39,040
for the guest the allow list

00:11:36,800 --> 00:11:40,560
can be installed using the audio urine

00:11:39,040 --> 00:11:43,040
register system call

00:11:40,560 --> 00:11:44,640
but the rings must start disabled using

00:11:43,040 --> 00:11:47,040
the air disable flag

00:11:44,640 --> 00:11:49,519
during the setup in this state no

00:11:47,040 --> 00:11:51,839
operation can be submitted

00:11:49,519 --> 00:11:54,560
when the restrictions are installed we

00:11:51,839 --> 00:11:57,360
can enable the ring processing using the

00:11:54,560 --> 00:11:59,279
enable rings of code with our urine

00:11:57,360 --> 00:12:02,320
register system call

00:11:59,279 --> 00:12:04,639
this allow us to avoid critical races

00:12:02,320 --> 00:12:07,920
between the creation of the rings

00:12:04,639 --> 00:12:10,320
and the installation of the restrictions

00:12:07,920 --> 00:12:12,000
with the allow list we can restrict the

00:12:10,320 --> 00:12:14,720
io urine register

00:12:12,000 --> 00:12:17,200
op codes for example disabling the

00:12:14,720 --> 00:12:19,760
possibility to register new buffers

00:12:17,200 --> 00:12:20,320
or file descriptors in this way the

00:12:19,760 --> 00:12:22,240
guest

00:12:20,320 --> 00:12:25,120
can use only the file descriptors that

00:12:22,240 --> 00:12:28,720
we already registered

00:12:25,120 --> 00:12:31,440
we can also limit the sqe of codes

00:12:28,720 --> 00:12:34,000
allowing only a subset of operation and

00:12:31,440 --> 00:12:36,839
we can specify which sqe flags

00:12:34,000 --> 00:12:39,279
are allowed or required for each

00:12:36,839 --> 00:12:42,880
operation

00:12:39,279 --> 00:12:46,160
for example if we want that each sqe

00:12:42,880 --> 00:12:48,399
uses only the file descriptor registered

00:12:46,160 --> 00:12:49,200
we need to require that the fixed file

00:12:48,399 --> 00:12:53,519
flag

00:12:49,200 --> 00:12:55,680
must be set in each in each sqle

00:12:53,519 --> 00:12:56,720
with this mechanism implemented in io

00:12:55,680 --> 00:13:00,000
urine we can

00:12:56,720 --> 00:13:00,639
safely share submission and completion

00:13:00,000 --> 00:13:03,279
cues

00:13:00,639 --> 00:13:03,279
with a guest

00:13:04,800 --> 00:13:08,480
we realized a proof of concept to

00:13:06,959 --> 00:13:11,440
analyze the performance

00:13:08,480 --> 00:13:13,920
and we compared it with bare metal and

00:13:11,440 --> 00:13:17,360
vertio block device simulation in qmu

00:13:13,920 --> 00:13:18,399
that we saw some slides ago in our test

00:13:17,360 --> 00:13:21,920
we run

00:13:18,399 --> 00:13:25,120
fiu with iou urine engine and 4k

00:13:21,920 --> 00:13:27,440
block sites we measure the

00:13:25,120 --> 00:13:31,040
number of io operation per second that

00:13:27,440 --> 00:13:31,040
we put in the vertical axis

00:13:31,120 --> 00:13:38,720
so the the unit

00:13:34,880 --> 00:13:39,920
is kilojoules so thousand io operation

00:13:38,720 --> 00:13:42,000
per second

00:13:39,920 --> 00:13:43,600
and the result are really encouraging

00:13:42,000 --> 00:13:48,399
since in the worst case

00:13:43,600 --> 00:13:52,160
where i adapt is one

00:13:48,399 --> 00:13:55,600
there is only one request in fly

00:13:52,160 --> 00:13:58,320
and in this case the gap

00:13:55,600 --> 00:14:00,800
between a urine pasture and bare metal

00:13:58,320 --> 00:14:03,279
is less than 13

00:14:00,800 --> 00:14:04,800
so this gap is caused by the fact that

00:14:03,279 --> 00:14:07,760
we we have to cross

00:14:04,800 --> 00:14:08,639
twice the linux block layer one time in

00:14:07,760 --> 00:14:11,760
the guest

00:14:08,639 --> 00:14:14,480
and another one in the host

00:14:11,760 --> 00:14:17,600
as we can see increasing the number of

00:14:14,480 --> 00:14:21,120
operation in flight

00:14:17,600 --> 00:14:22,959
the gap go to zero

00:14:21,120 --> 00:14:25,120
compared with virtaio block device

00:14:22,959 --> 00:14:29,040
simulation in queen mu

00:14:25,120 --> 00:14:32,480
the first the first bar in the graph

00:14:29,040 --> 00:14:35,440
we are always faster as we skip a big

00:14:32,480 --> 00:14:36,079
piece of software stack and we avoid the

00:14:35,440 --> 00:14:39,120
trust

00:14:36,079 --> 00:14:44,320
the the translation from word queue to

00:14:39,120 --> 00:14:46,880
higher urine queues

00:14:44,320 --> 00:14:49,199
an alternative to a urine pass through

00:14:46,880 --> 00:14:52,079
is to move the device simulation

00:14:49,199 --> 00:14:52,560
in the kernel using v-host also in this

00:14:52,079 --> 00:14:54,639
case

00:14:52,560 --> 00:14:56,880
we will have a single communication

00:14:54,639 --> 00:14:57,760
channel since the word queue is shared

00:14:56,880 --> 00:15:00,959
between guests

00:14:57,760 --> 00:15:02,880
and those kernels in the last years some

00:15:00,959 --> 00:15:06,480
v-host block implementation was

00:15:02,880 --> 00:15:10,639
published of the same but never measured

00:15:06,480 --> 00:15:13,519
the first version from isis used the bio

00:15:10,639 --> 00:15:14,079
api the lowest api just up to the block

00:15:13,519 --> 00:15:17,920
driver

00:15:14,079 --> 00:15:21,600
drivers the second version from vitaly

00:15:17,920 --> 00:15:24,240
move it to vfs api this allows us to

00:15:21,600 --> 00:15:25,519
use also raw files stored in a file

00:15:24,240 --> 00:15:28,880
system

00:15:25,519 --> 00:15:30,639
vfs adds some of red of course but it's

00:15:28,880 --> 00:15:33,839
negligible and it's also

00:15:30,639 --> 00:15:36,880
the same interface used by iouri

00:15:33,839 --> 00:15:39,759
so i compared this last version with

00:15:36,880 --> 00:15:40,880
io urine passthrough improving a bit the

00:15:39,759 --> 00:15:43,920
implementation

00:15:40,880 --> 00:15:46,399
adding vert queue and block device

00:15:43,920 --> 00:15:46,399
polling

00:15:46,800 --> 00:15:51,120
i run the same fio configuration that we

00:15:49,600 --> 00:15:54,240
saw before

00:15:51,120 --> 00:15:56,160
the first bar is obtained without

00:15:54,240 --> 00:16:00,320
polling so it's the original

00:15:56,160 --> 00:16:02,639
version in the second bar i had

00:16:00,320 --> 00:16:04,000
the vert cube polling in the vios block

00:16:02,639 --> 00:16:06,000
emulation

00:16:04,000 --> 00:16:07,279
so we avoided the notification from the

00:16:06,000 --> 00:16:10,079
guest

00:16:07,279 --> 00:16:11,519
in the yellow bar we enable the block

00:16:10,079 --> 00:16:14,160
iopali so we do

00:16:11,519 --> 00:16:15,040
busy weight in the host kernel avoiding

00:16:14,160 --> 00:16:20,320
interrupts

00:16:15,040 --> 00:16:20,320
from the device and in the green bar

00:16:20,480 --> 00:16:24,720
we enable sq pawling in a fio running in

00:16:23,199 --> 00:16:28,720
the guest

00:16:24,720 --> 00:16:30,720
as you can see also with polling

00:16:28,720 --> 00:16:32,000
there is still a gap without your urine

00:16:30,720 --> 00:16:34,160
buster and

00:16:32,000 --> 00:16:35,279
it should be related to the rings

00:16:34,160 --> 00:16:38,320
allocation

00:16:35,279 --> 00:16:40,720
with vhost block the bird queue and

00:16:38,320 --> 00:16:42,959
descriptors are allocated in the guest

00:16:40,720 --> 00:16:46,240
memory so the host kernel

00:16:42,959 --> 00:16:47,759
must call copy in and copy to for each

00:16:46,240 --> 00:16:50,320
request

00:16:47,759 --> 00:16:50,959
this is not needed without your urine

00:16:50,320 --> 00:16:53,519
posture

00:16:50,959 --> 00:16:55,920
where the submission and completion cues

00:16:53,519 --> 00:17:00,160
are allocated in the host kernel

00:16:55,920 --> 00:17:00,160
and then mapped in the guest memory

00:17:01,120 --> 00:17:05,280
as i said v-host block was never merged

00:17:04,640 --> 00:17:07,919
upstream

00:17:05,280 --> 00:17:09,199
but recently a new framework has been

00:17:07,919 --> 00:17:11,600
developed

00:17:09,199 --> 00:17:12,959
especially to float vertical processing

00:17:11,600 --> 00:17:16,480
to the allower

00:17:12,959 --> 00:17:19,919
this framework is called vdpa vertio

00:17:16,480 --> 00:17:22,720
data path acceleration our idea is to

00:17:19,919 --> 00:17:25,520
implement a vdpa block software device

00:17:22,720 --> 00:17:27,839
very similar to vios block but using

00:17:25,520 --> 00:17:30,320
this new framework

00:17:27,839 --> 00:17:30,960
this allows us to unify the software

00:17:30,320 --> 00:17:33,440
stack

00:17:30,960 --> 00:17:34,240
and reuse the same code for example in

00:17:33,440 --> 00:17:35,840
premiere

00:17:34,240 --> 00:17:38,400
when hardware implementation will be

00:17:35,840 --> 00:17:41,360
available in addition

00:17:38,400 --> 00:17:43,760
with vdpa we have more control than we

00:17:41,360 --> 00:17:46,400
host on device lifecycle

00:17:43,760 --> 00:17:47,840
and the gas pages are pinned in memory

00:17:46,400 --> 00:17:50,880
so we don't need to

00:17:47,840 --> 00:17:54,480
memory map user buffers or do copy

00:17:50,880 --> 00:17:57,360
in and copy to for each descriptors

00:17:54,480 --> 00:17:59,200
this should allow us to fill the gap

00:17:57,360 --> 00:18:02,080
with our urine posture

00:17:59,200 --> 00:18:04,880
on the other hand the pin pages don't

00:18:02,080 --> 00:18:07,760
allow us to over commit the guest memory

00:18:04,880 --> 00:18:09,039
and we also need to implement the

00:18:07,760 --> 00:18:12,840
polling strategies

00:18:09,039 --> 00:18:14,080
and bfs integration that we are already

00:18:12,840 --> 00:18:17,120
available

00:18:14,080 --> 00:18:17,120
in io urine

00:18:18,160 --> 00:18:23,200
concluding in the next months we will

00:18:20,240 --> 00:18:26,240
implement a proof of concept of vdba

00:18:23,200 --> 00:18:28,640
blocks software device starting from a

00:18:26,240 --> 00:18:31,760
bdba block simulator in the kernel

00:18:28,640 --> 00:18:32,240
then we will add the support of bdpa

00:18:31,760 --> 00:18:35,600
block

00:18:32,240 --> 00:18:38,640
on queen u and we will develop the linux

00:18:35,600 --> 00:18:40,720
vtpa driver with device emulation and

00:18:38,640 --> 00:18:43,440
bfs integration

00:18:40,720 --> 00:18:44,480
we will also work on the block io poll

00:18:43,440 --> 00:18:47,520
optimization

00:18:44,480 --> 00:18:49,600
for the verteio block driver and we will

00:18:47,520 --> 00:18:52,320
try to add the missing features

00:18:49,600 --> 00:18:55,280
to io urine to complete their urine pass

00:18:52,320 --> 00:18:55,280
through implementation

00:18:55,679 --> 00:19:08,720
so thank you very much and now it's time

00:18:58,400 --> 00:19:08,720

YouTube URL: https://www.youtube.com/watch?v=aPyDk7I5_8Y


