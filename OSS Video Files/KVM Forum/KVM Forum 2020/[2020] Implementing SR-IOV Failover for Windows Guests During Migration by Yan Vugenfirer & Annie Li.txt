Title: [2020] Implementing SR-IOV Failover for Windows Guests During Migration by Yan Vugenfirer & Annie Li
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	In the past, there were several attempted to enable live migration for VMs that are using SR-IOV NICs. We are going to discuss the recent development based on the SR-IOV failover feature in virtio specification and its implementation for the Windows guests.  In this session, Annie Li and Yan Vugenfirer will provide an overview of the failover feature and discuss specifics of the Windows guest implementation.

---

Yan Vugenfirer
Daynix, CEO

Yan is the CEO of Daynix Computing. He is an upstream maintainer fo the virtio-win drivers https://github.com/virtio-win/kvm-guest-drivers-windows/. Yan has more than 20 years of kernel development and 14 years of virtualization related development.

Yansu Li
Oracle, Principal Software Engineer

Annie is a principal software developer at Oracle America, Inc. Her role is developing Virtualization drivers in Windows, and currently, she is working on VirtIO 2-netdev model for supporting SR-IOV live migration in Windows. She has 10+ years experience of Windows driver development.
Captions: 
	00:00:07,759 --> 00:00:11,360
hello

00:00:08,639 --> 00:00:12,240
my name is jan vogenfire and i'm a ceo

00:00:11,360 --> 00:00:15,040
at denix

00:00:12,240 --> 00:00:16,720
and a contractor at red hat here with me

00:00:15,040 --> 00:00:17,680
is annie lee principal software engineer

00:00:16,720 --> 00:00:19,359
at oracle

00:00:17,680 --> 00:00:21,680
and to together we are going to talk

00:00:19,359 --> 00:00:24,560
about implementing srv failover

00:00:21,680 --> 00:00:26,560
for windows gas during the migration so

00:00:24,560 --> 00:00:27,680
during our presentation we are going to

00:00:26,560 --> 00:00:29,359
discuss

00:00:27,680 --> 00:00:30,880
real-time wind drivers in general and

00:00:29,359 --> 00:00:34,079
give you some

00:00:30,880 --> 00:00:36,000
overview on the vertebrae drivers uh

00:00:34,079 --> 00:00:37,760
we will talk about some windows gas

00:00:36,000 --> 00:00:38,879
terminology for the people that are less

00:00:37,760 --> 00:00:41,120
aware of that

00:00:38,879 --> 00:00:42,719
discuss the problem with the srv

00:00:41,120 --> 00:00:45,360
migration

00:00:42,719 --> 00:00:46,640
go about a little bit overview about

00:00:45,360 --> 00:00:49,760
different solutions

00:00:46,640 --> 00:00:51,440
and then we will discuss our solution

00:00:49,760 --> 00:00:54,079
so let's talk about retirement drivers

00:00:51,440 --> 00:00:56,559
first so here we have a link

00:00:54,079 --> 00:00:58,320
to the github uh where there is our

00:00:56,559 --> 00:01:00,239
office in the repository

00:00:58,320 --> 00:01:01,680
and this repository can find all the

00:01:00,239 --> 00:01:03,440
major drivers

00:01:01,680 --> 00:01:04,799
all the major retail drivers we have a

00:01:03,440 --> 00:01:08,159
virtual network

00:01:04,799 --> 00:01:10,000
block with a scasi and other drivers

00:01:08,159 --> 00:01:12,240
in this directory as well there are some

00:01:10,000 --> 00:01:14,320
other guest drivers that are not related

00:01:12,240 --> 00:01:17,280
to retail like panic driver

00:01:14,320 --> 00:01:19,119
and firmware complete and also some inf

00:01:17,280 --> 00:01:23,360
files that help us to define

00:01:19,119 --> 00:01:27,840
uh the system for example sm bias in for

00:01:23,360 --> 00:01:27,840
a q35 chipset or pci serial

00:01:28,479 --> 00:01:32,079
so what are those drivers and how they

00:01:31,119 --> 00:01:34,000
are built

00:01:32,079 --> 00:01:35,200
so the network and the storage driver

00:01:34,000 --> 00:01:36,960
they are built in the architecture

00:01:35,200 --> 00:01:38,079
microsoft architecture called minifor

00:01:36,960 --> 00:01:40,799
drivers

00:01:38,079 --> 00:01:41,840
so each and their uh respected driver

00:01:40,799 --> 00:01:44,320
technology

00:01:41,840 --> 00:01:44,880
uh for network it's endius and for

00:01:44,320 --> 00:01:47,439
storage

00:01:44,880 --> 00:01:48,000
store port or sky support and other

00:01:47,439 --> 00:01:50,799
drivers

00:01:48,000 --> 00:01:51,520
are wdf drivers so it's a microsoft

00:01:50,799 --> 00:01:53,600
framework

00:01:51,520 --> 00:01:55,520
that allows you to easily write kernel

00:01:53,600 --> 00:01:57,759
drivers

00:01:55,520 --> 00:01:58,960
what are the supported oasis so we

00:01:57,759 --> 00:02:01,280
support all the os

00:01:58,960 --> 00:02:02,079
starting from windows xp up to windows

00:02:01,280 --> 00:02:05,320
00:02:02,079 --> 00:02:07,040
and same with windows servers 2003 into

00:02:05,320 --> 00:02:09,759
00:02:07,040 --> 00:02:10,319
from the windows for windows 10 we also

00:02:09,759 --> 00:02:14,800
supply

00:02:10,319 --> 00:02:14,800
support arm 64 platform

00:02:14,959 --> 00:02:19,280
how can you contribute so please sign

00:02:17,280 --> 00:02:22,080
pull request

00:02:19,280 --> 00:02:23,440
the code changes should pass microsoft

00:02:22,080 --> 00:02:26,000
certifications

00:02:23,440 --> 00:02:27,040
but don't worry we are running ci on the

00:02:26,000 --> 00:02:30,239
upstream

00:02:27,040 --> 00:02:31,920
so you are covered for that and who are

00:02:30,239 --> 00:02:32,720
the contributors for the project during

00:02:31,920 --> 00:02:35,120
the years

00:02:32,720 --> 00:02:37,599
so main contributors are coming from red

00:02:35,120 --> 00:02:38,560
hat but we had also contributions from

00:02:37,599 --> 00:02:42,319
duty ozo

00:02:38,560 --> 00:02:46,319
oracle google microsoft aws

00:02:42,319 --> 00:02:49,440
and others so now let's talk about

00:02:46,319 --> 00:02:52,239
uh windows terms and then

00:02:49,440 --> 00:02:52,800
how the network driver architecture

00:02:52,239 --> 00:02:55,599
looks

00:02:52,800 --> 00:02:56,959
like in windows so you'll hear a lot

00:02:55,599 --> 00:02:59,280
during this presentation

00:02:56,959 --> 00:03:01,200
and it's so what is andy's network

00:02:59,280 --> 00:03:04,800
driver interface specification

00:03:01,200 --> 00:03:07,280
it's also api for network drivers

00:03:04,800 --> 00:03:08,720
it's also the architecture of the

00:03:07,280 --> 00:03:10,640
network drivers and camera

00:03:08,720 --> 00:03:11,920
and there is andis.cs which is a

00:03:10,640 --> 00:03:14,239
microsoft driver

00:03:11,920 --> 00:03:15,120
that you can see in windows kernel and

00:03:14,239 --> 00:03:17,680
it implements

00:03:15,120 --> 00:03:19,040
part of the in this functionality so if

00:03:17,680 --> 00:03:22,080
you will go from

00:03:19,040 --> 00:03:24,000
bottom to top in the bottom you will see

00:03:22,080 --> 00:03:25,680
hardware devices and on top of them

00:03:24,000 --> 00:03:27,920
there is a mini port driver

00:03:25,680 --> 00:03:31,120
that usually supplied by the vendor and

00:03:27,920 --> 00:03:34,159
this driver drives the specific device

00:03:31,120 --> 00:03:35,599
in the simple case that you can see from

00:03:34,159 --> 00:03:38,080
the right side

00:03:35,599 --> 00:03:40,159
uh there are binded protocol drivers

00:03:38,080 --> 00:03:42,159
just on top of the mini core driver

00:03:40,159 --> 00:03:44,400
the more complicated case is when we

00:03:42,159 --> 00:03:45,920
have also intermediate driver

00:03:44,400 --> 00:03:47,760
and then the protocol drivers are

00:03:45,920 --> 00:03:50,319
binding to the intermediate driver

00:03:47,760 --> 00:03:51,519
so intermediate drivers towards the mini

00:03:50,319 --> 00:03:54,319
port driver has

00:03:51,519 --> 00:03:56,640
api of the protocol drivers so many poor

00:03:54,319 --> 00:03:58,400
things that he stock the protocol driver

00:03:56,640 --> 00:04:00,000
and towards the protocol driver it has a

00:03:58,400 --> 00:04:02,159
mini port api so

00:04:00,000 --> 00:04:03,439
protocol drivers think that they talk to

00:04:02,159 --> 00:04:05,840
mini for driver

00:04:03,439 --> 00:04:07,439
why do we need such a thing so one of

00:04:05,840 --> 00:04:09,439
the example is the moves driver

00:04:07,439 --> 00:04:11,120
that can sit on top of several miniport

00:04:09,439 --> 00:04:15,439
drivers and present

00:04:11,120 --> 00:04:17,280
one virtual nic to the top layers

00:04:15,439 --> 00:04:19,759
also in the user space we have notify

00:04:17,280 --> 00:04:21,759
object notify object can get

00:04:19,759 --> 00:04:22,960
callbacks from the network configuration

00:04:21,759 --> 00:04:26,840
subsystem

00:04:22,960 --> 00:04:29,840
and act on on those callbacks

00:04:26,840 --> 00:04:34,479
by changing network configuration

00:04:29,840 --> 00:04:37,199
removing drivers installing drivers etc

00:04:34,479 --> 00:04:38,560
so how their util network net kvm driver

00:04:37,199 --> 00:04:40,720
for windows looks like

00:04:38,560 --> 00:04:42,160
so it's nds minifor driver as we

00:04:40,720 --> 00:04:44,240
mentioned before

00:04:42,160 --> 00:04:46,320
and the basic driver project package

00:04:44,240 --> 00:04:48,240
looks like you have the inf file

00:04:46,320 --> 00:04:49,360
which is an installation descriptor you

00:04:48,240 --> 00:04:51,680
have a sys file

00:04:49,360 --> 00:04:52,479
which is a driver binary you have a pdf

00:04:51,680 --> 00:04:54,960
file

00:04:52,479 --> 00:04:56,880
those are symbols for debugging and cut

00:04:54,960 --> 00:04:59,360
file which is the package digital

00:04:56,880 --> 00:05:03,039
signature

00:04:59,360 --> 00:05:05,840
so now let's discuss the problem why we

00:05:03,039 --> 00:05:08,560
needed to do something so when we are

00:05:05,840 --> 00:05:12,160
talking about para virtualized devices

00:05:08,560 --> 00:05:14,880
and drivers and all fully emulated

00:05:12,160 --> 00:05:15,680
devices all the code is resides inside

00:05:14,880 --> 00:05:18,720
of qmu

00:05:15,680 --> 00:05:19,440
and qmu also controls the data path so

00:05:18,720 --> 00:05:22,000
when we

00:05:19,440 --> 00:05:23,680
want to migrate such device it's just

00:05:22,000 --> 00:05:24,000
migrated with virtual machine all the

00:05:23,680 --> 00:05:26,960
device

00:05:24,000 --> 00:05:27,840
that is migrated qmo fully controls the

00:05:26,960 --> 00:05:30,800
data path

00:05:27,840 --> 00:05:32,639
etc but when we have an external

00:05:30,800 --> 00:05:33,759
hardware device when qmu does not

00:05:32,639 --> 00:05:36,880
control the data path

00:05:33,759 --> 00:05:39,919
the dma continues to run we need somehow

00:05:36,880 --> 00:05:41,360
to migrate the hardware state

00:05:39,919 --> 00:05:43,360
and therefore there are several

00:05:41,360 --> 00:05:44,000
solutions that were proposed over the

00:05:43,360 --> 00:05:46,240
years

00:05:44,000 --> 00:05:47,759
so we will have a small overview about

00:05:46,240 --> 00:05:50,160
them

00:05:47,759 --> 00:05:51,120
somewhere more vendor specific we'll

00:05:50,160 --> 00:05:54,560
talk about

00:05:51,120 --> 00:05:56,960
uh what microsoft did in hyper-v and

00:05:54,560 --> 00:06:00,160
there's a solution in linux with three

00:05:56,960 --> 00:06:00,160
not that model

00:06:00,639 --> 00:06:04,960
so regarding previous effort i think

00:06:02,400 --> 00:06:05,520
almost every year on the kvm forum we

00:06:04,960 --> 00:06:07,520
have

00:06:05,520 --> 00:06:10,160
at least one presentation about sriv

00:06:07,520 --> 00:06:13,440
migration so they are ranging between

00:06:10,160 --> 00:06:16,400
very vendor specific or device specific

00:06:13,440 --> 00:06:16,880
solutions to just now there's a parallel

00:06:16,400 --> 00:06:21,440
session

00:06:16,880 --> 00:06:24,319
about more generic solution

00:06:21,440 --> 00:06:25,039
and now i am passing the presentation to

00:06:24,319 --> 00:06:27,199
annie

00:06:25,039 --> 00:06:29,199
and she's going to give the overview

00:06:27,199 --> 00:06:35,840
about the solutions that microsoft did

00:06:29,199 --> 00:06:35,840
and about our solution

00:06:40,639 --> 00:06:46,560
thanks yan hi everyone this is ani

00:06:43,919 --> 00:06:49,440
from oracle today i'm going to talk

00:06:46,560 --> 00:06:53,919
about the software solutions of

00:06:49,440 --> 00:06:57,199
srlv migration in windows

00:06:53,919 --> 00:06:58,479
so the these solutions focus on

00:06:57,199 --> 00:07:01,520
switching data paths

00:06:58,479 --> 00:07:03,280
seamlessly between vf network and the

00:07:01,520 --> 00:07:06,800
word out network

00:07:03,280 --> 00:07:09,759
before initiating the migration the vf

00:07:06,800 --> 00:07:11,599
network adapter will be hardly removed

00:07:09,759 --> 00:07:14,000
and all network traffic

00:07:11,599 --> 00:07:14,840
will be redirected to the vertical

00:07:14,000 --> 00:07:18,160
network

00:07:14,840 --> 00:07:21,039
datapath after the migration is done

00:07:18,160 --> 00:07:21,680
the vs network adapter will be hard

00:07:21,039 --> 00:07:24,639
added

00:07:21,680 --> 00:07:26,240
under target so all the network data

00:07:24,639 --> 00:07:30,720
will go through the

00:07:26,240 --> 00:07:33,919
vf network path today i will talk about

00:07:30,720 --> 00:07:37,199
the existing solutions

00:07:33,919 --> 00:07:39,759
first there are the windows nick teaming

00:07:37,199 --> 00:07:41,280
windows max intermediate driver and

00:07:39,759 --> 00:07:44,319
hyper-v solution

00:07:41,280 --> 00:07:45,280
after that i will talk about the tonight

00:07:44,319 --> 00:07:49,199
down model

00:07:45,280 --> 00:07:49,199
in windows virtual driver

00:07:50,639 --> 00:07:57,520
so the windows snake teaming is built

00:07:53,680 --> 00:08:00,720
in windows since windows server 2012.

00:07:57,520 --> 00:08:03,120
it is similar to the boundary in linux

00:08:00,720 --> 00:08:04,080
and the windows snake teaming provides

00:08:03,120 --> 00:08:07,280
the fair lower

00:08:04,080 --> 00:08:08,080
capability user can put divert our

00:08:07,280 --> 00:08:12,240
network

00:08:08,080 --> 00:08:14,400
and the vf network into one team and

00:08:12,240 --> 00:08:17,840
configure the vertical network

00:08:14,400 --> 00:08:19,360
as done by so when the vf adapter is

00:08:17,840 --> 00:08:22,319
hard to remove

00:08:19,360 --> 00:08:24,479
the windows nick teaming will set the

00:08:22,319 --> 00:08:28,319
vertical adapter as

00:08:24,479 --> 00:08:31,360
active and switch the data path to the

00:08:28,319 --> 00:08:35,760
word at all after the

00:08:31,360 --> 00:08:39,120
vf adapter is hard added

00:08:35,760 --> 00:08:42,640
the windows negative will set the

00:08:39,120 --> 00:08:44,320
adapter as standby and then the data

00:08:42,640 --> 00:08:47,519
pathway goes through wf

00:08:44,320 --> 00:08:48,560
adapter so the windows negativing can be

00:08:47,519 --> 00:08:51,279
configured to

00:08:48,560 --> 00:08:52,000
through gui or the porsche in the user

00:08:51,279 --> 00:08:55,360
space

00:08:52,000 --> 00:08:57,399
however we prefer the solution in kernel

00:08:55,360 --> 00:09:00,320
space to switch the data path

00:08:57,399 --> 00:09:00,959
automatically and the user doesn't need

00:09:00,320 --> 00:09:05,680
to spend

00:09:00,959 --> 00:09:08,959
time or effort in configuring user space

00:09:05,680 --> 00:09:10,720
the windows max intermediate driver is a

00:09:08,959 --> 00:09:13,680
kernel solution

00:09:10,720 --> 00:09:15,040
it is supposed to one or more virtual

00:09:13,680 --> 00:09:17,519
adapters

00:09:15,040 --> 00:09:18,880
based on the relationship between the

00:09:17,519 --> 00:09:22,320
virtual adapter and

00:09:18,880 --> 00:09:25,440
underlying network adapter it has

00:09:22,320 --> 00:09:28,640
the various models today i only

00:09:25,440 --> 00:09:32,320
introduced the 122 gel model for the

00:09:28,640 --> 00:09:35,279
srlv lamp migration this model

00:09:32,320 --> 00:09:36,720
is similar to the nic teaming in fill

00:09:35,279 --> 00:09:40,480
over mode

00:09:36,720 --> 00:09:44,480
its architecture is also similar to the

00:09:40,480 --> 00:09:44,480
three network model in linux

00:09:45,360 --> 00:09:49,040
so this slide shows the architecture of

00:09:48,480 --> 00:09:52,399
the

00:09:49,040 --> 00:09:55,440
to two max driver model as you can see

00:09:52,399 --> 00:09:57,200
the bottom are the vertical network and

00:09:55,440 --> 00:09:59,600
the vf function network

00:09:57,200 --> 00:10:01,680
they have their own mini product driver

00:09:59,600 --> 00:10:05,279
serve them that's a net qvm

00:10:01,680 --> 00:10:07,360
driver and a vf minipaw driver on top of

00:10:05,279 --> 00:10:10,640
them that's the one to two max

00:10:07,360 --> 00:10:13,279
intermediate driver this max driver

00:10:10,640 --> 00:10:16,079
exposed the protocol driver as a lower

00:10:13,279 --> 00:10:17,680
edge to bend to the underlying minipod

00:10:16,079 --> 00:10:21,120
driver

00:10:17,680 --> 00:10:21,440
also it's exposed the mini power driver

00:10:21,120 --> 00:10:24,640
as

00:10:21,440 --> 00:10:28,320
upper add to bend to the tcp

00:10:24,640 --> 00:10:29,200
and other protocols inside the max

00:10:28,320 --> 00:10:33,200
driver

00:10:29,200 --> 00:10:36,320
the mini part virtual adapter

00:10:33,200 --> 00:10:39,600
banned to the its own protocol driver

00:10:36,320 --> 00:10:41,440
internally so the ndc isn't aware of

00:10:39,600 --> 00:10:43,920
their spending

00:10:41,440 --> 00:10:46,000
the mass driver has full control on the

00:10:43,920 --> 00:10:49,600
network data here so it

00:10:46,000 --> 00:10:53,040
can switch the data path between the

00:10:49,600 --> 00:10:56,880
net qvm and we have mini power driver

00:10:53,040 --> 00:10:58,560
so the thing is one the underlying

00:10:56,880 --> 00:11:00,640
meaning of the driver

00:10:58,560 --> 00:11:02,160
is supposed to and this file banning

00:11:00,640 --> 00:11:04,240
interface

00:11:02,160 --> 00:11:05,680
to avoid the confusion between the

00:11:04,240 --> 00:11:08,880
banning interface

00:11:05,680 --> 00:11:13,120
and this file with the in this version

00:11:08,880 --> 00:11:15,440
so i will only use the term end is here

00:11:13,120 --> 00:11:16,560
as you can see the protocol driver in

00:11:15,440 --> 00:11:19,120
mass driver

00:11:16,560 --> 00:11:21,200
is supposed on this binding interface as

00:11:19,120 --> 00:11:24,000
well as the tcp and

00:11:21,200 --> 00:11:26,079
other protocols this means this

00:11:24,000 --> 00:11:29,839
underlying meaning procedural

00:11:26,079 --> 00:11:33,040
bind into the max java protocol

00:11:29,839 --> 00:11:35,680
also been to the up layer protocol

00:11:33,040 --> 00:11:36,480
however the mass driver only wants to

00:11:35,680 --> 00:11:38,959
use expose

00:11:36,480 --> 00:11:40,240
the virtual adapter here not the

00:11:38,959 --> 00:11:43,440
underlying

00:11:40,240 --> 00:11:46,720
media power driver so a notified object

00:11:43,440 --> 00:11:49,839
is involved here to unbend

00:11:46,720 --> 00:11:52,160
the upper layer protocol driver from the

00:11:49,839 --> 00:11:53,920
underlying net qvm and we have

00:11:52,160 --> 00:11:56,800
manipulator

00:11:53,920 --> 00:11:57,440
here i will give the details about the

00:11:56,800 --> 00:12:00,880
notify

00:11:57,440 --> 00:12:03,839
object and i will go into more depth

00:12:00,880 --> 00:12:03,839
on it later

00:12:04,720 --> 00:12:08,959
so this snapshot shows the binding

00:12:07,200 --> 00:12:12,480
details

00:12:08,959 --> 00:12:15,760
nick of on max driver so the

00:12:12,480 --> 00:12:16,880
they see they show their similar venues

00:12:15,760 --> 00:12:20,639
so i only paste

00:12:16,880 --> 00:12:24,560
one for both so the ethernet 14 and

00:12:20,639 --> 00:12:25,839
ethernet file are the vf and virtual

00:12:24,560 --> 00:12:28,639
network adapter

00:12:25,839 --> 00:12:30,240
connection they're only bent to the

00:12:28,639 --> 00:12:34,079
network adapter

00:12:30,240 --> 00:12:36,880
multiplexer protocol and srov

00:12:34,079 --> 00:12:39,839
network connection is generated by the

00:12:36,880 --> 00:12:42,880
link teaming of the max java model

00:12:39,839 --> 00:12:43,680
it blended to all necessary app layer

00:12:42,880 --> 00:12:47,040
protocols

00:12:43,680 --> 00:12:49,360
but it doesn't bend into the its own

00:12:47,040 --> 00:12:52,560
protocol driver

00:12:49,360 --> 00:12:55,680
so as we know that hyper-v supported the

00:12:52,560 --> 00:12:57,279
srlv land migration so let's see how

00:12:55,680 --> 00:13:01,040
hyper-v works

00:12:57,279 --> 00:13:04,560
the important part of the vm network

00:13:01,040 --> 00:13:08,399
is the network virtual service client

00:13:04,560 --> 00:13:11,040
that's the net vrc native lc

00:13:08,399 --> 00:13:12,240
can also communicate with the network

00:13:11,040 --> 00:13:15,519
virtual service

00:13:12,240 --> 00:13:18,560
provider through the vm bus in

00:13:15,519 --> 00:13:20,880
parent partition so that's the synthetic

00:13:18,560 --> 00:13:23,760
data path

00:13:20,880 --> 00:13:26,079
the network sc driver also communicates

00:13:23,760 --> 00:13:29,440
with the vf municipal driver

00:13:26,079 --> 00:13:32,800
for the srlv lab migration

00:13:29,440 --> 00:13:36,240
and the network also provides two

00:13:32,800 --> 00:13:37,360
installation files one is for installing

00:13:36,240 --> 00:13:39,600
the nano sd

00:13:37,360 --> 00:13:42,399
mini powder driver another for

00:13:39,600 --> 00:13:44,639
installing the nano sc protocol driver

00:13:42,399 --> 00:13:46,079
both the two drivers share the same

00:13:44,639 --> 00:13:48,480
driver binary

00:13:46,079 --> 00:13:50,160
normally they are tagged their names are

00:13:48,480 --> 00:13:55,839
tagged with the

00:13:50,160 --> 00:13:55,839
endless version for example network c63

00:13:57,199 --> 00:14:02,880
so let's see the architectural hyper-v

00:14:00,199 --> 00:14:06,399
srvvf failover

00:14:02,880 --> 00:14:09,199
so as you can see the vf mini part

00:14:06,399 --> 00:14:10,000
driver in hyper-v is exposed to the

00:14:09,199 --> 00:14:14,320
binding

00:14:10,000 --> 00:14:17,120
interface operand as the ndcvf

00:14:14,320 --> 00:14:18,079
and the network protocol driver is the

00:14:17,120 --> 00:14:21,040
only input

00:14:18,079 --> 00:14:22,639
driver that is posted and is we have

00:14:21,040 --> 00:14:25,120
binding interface

00:14:22,639 --> 00:14:26,079
so this means these two drivers can bend

00:14:25,120 --> 00:14:29,360
together

00:14:26,079 --> 00:14:32,560
it's cl exclusively and there's

00:14:29,360 --> 00:14:35,839
no notify object involved

00:14:32,560 --> 00:14:39,760
neither no new virtual adapter

00:14:35,839 --> 00:14:43,839
is generated also there's no band or

00:14:39,760 --> 00:14:45,760
teaming involved so the network protocol

00:14:43,839 --> 00:14:48,720
driver sitting in the same

00:14:45,760 --> 00:14:49,839
java binary as the network sc mini power

00:14:48,720 --> 00:14:53,519
driver

00:14:49,839 --> 00:14:56,240
so as you can see because of this

00:14:53,519 --> 00:14:59,199
it is possible for the protocol driver

00:14:56,240 --> 00:14:59,519
to access the network data from network

00:14:59,199 --> 00:15:02,480
c

00:14:59,519 --> 00:15:04,480
mini power driver and forward them to

00:15:02,480 --> 00:15:06,880
the vf manipulator

00:15:04,480 --> 00:15:07,680
finally reach the virtual function

00:15:06,880 --> 00:15:10,800
device

00:15:07,680 --> 00:15:14,079
and the west versa

00:15:10,800 --> 00:15:17,680
so here is the network binding of

00:15:14,079 --> 00:15:21,360
hyper-v the hyper the ethernet

00:15:17,680 --> 00:15:24,079
file is the vf network connection

00:15:21,360 --> 00:15:24,639
it only bound into the network sd field

00:15:24,079 --> 00:15:28,399
over

00:15:24,639 --> 00:15:29,120
vs protocol driver the ethernet 4 is the

00:15:28,399 --> 00:15:31,920
hyper-v

00:15:29,120 --> 00:15:33,600
virtual adapter connection it abandoned

00:15:31,920 --> 00:15:37,839
to the tcp

00:15:33,600 --> 00:15:40,560
and other protocols but the hyper-v

00:15:37,839 --> 00:15:42,560
failover vf protocol driver is hidden to

00:15:40,560 --> 00:15:44,959
it

00:15:42,560 --> 00:15:46,560
so as we can see the mass driver model

00:15:44,959 --> 00:15:49,920
is complicated

00:15:46,560 --> 00:15:53,199
a new virtual adapter is generated this

00:15:49,920 --> 00:15:54,480
requires deployment of the new virtual

00:15:53,199 --> 00:15:57,279
miniport driver

00:15:54,480 --> 00:15:58,639
and the offload have to be restored in

00:15:57,279 --> 00:16:02,000
the max driver

00:15:58,639 --> 00:16:05,120
and also the notify object in the

00:16:02,000 --> 00:16:06,800
model is complicated the installation

00:16:05,120 --> 00:16:08,800
involves installing the

00:16:06,800 --> 00:16:10,399
virtual adapter meaning power driver

00:16:08,800 --> 00:16:13,759
also

00:16:10,399 --> 00:16:16,320
upload the protocol driver so this means

00:16:13,759 --> 00:16:17,279
more efforts are required for the

00:16:16,320 --> 00:16:20,480
deployment

00:16:17,279 --> 00:16:20,839
of the max drive model the hyper-v model

00:16:20,480 --> 00:16:24,880
is

00:16:20,839 --> 00:16:26,160
simplified but it is only appropriate

00:16:24,880 --> 00:16:30,560
for hyper-v

00:16:26,160 --> 00:16:34,160
in linux the mailbox mechanism

00:16:30,560 --> 00:16:37,199
is implemented for the vf and pf

00:16:34,160 --> 00:16:41,040
communication however different

00:16:37,199 --> 00:16:44,959
mechanism is implemented for the hyper-v

00:16:41,040 --> 00:16:48,000
in windows as a result same device

00:16:44,959 --> 00:16:49,519
vf device are advertised by different

00:16:48,000 --> 00:16:52,639
device id

00:16:49,519 --> 00:16:54,360
and the windows vf mini project

00:16:52,639 --> 00:16:56,399
end up with the different

00:16:54,360 --> 00:16:59,279
implementations too

00:16:56,399 --> 00:17:01,680
as well as the banning interface are

00:16:59,279 --> 00:17:04,679
exposed differently

00:17:01,680 --> 00:17:06,079
this means we cannot use the hyper-v

00:17:04,679 --> 00:17:09,439
implementation

00:17:06,079 --> 00:17:12,000
directly in kvm so

00:17:09,439 --> 00:17:12,480
here the question comes what should we

00:17:12,000 --> 00:17:16,959
do

00:17:12,480 --> 00:17:20,000
for the srv lab migration human dose gas

00:17:16,959 --> 00:17:23,039
in qvm the idea here is

00:17:20,000 --> 00:17:25,360
combine the max gel mod and have a

00:17:23,039 --> 00:17:28,079
remodel solution together

00:17:25,360 --> 00:17:29,520
and that's the tonight that model in

00:17:28,079 --> 00:17:32,880
windows

00:17:29,520 --> 00:17:35,919
so let's at first let's take a look

00:17:32,880 --> 00:17:38,160
at the regular network and

00:17:35,919 --> 00:17:39,760
virtual network and the virg virtual

00:17:38,160 --> 00:17:43,120
function network

00:17:39,760 --> 00:17:45,600
they have their own miniport driver to

00:17:43,120 --> 00:17:46,320
driver to serve them that's the net qvm

00:17:45,600 --> 00:17:49,919
and the via

00:17:46,320 --> 00:17:53,919
mini project this drivers bend to the

00:17:49,919 --> 00:17:53,919
uplayer protocol directly

00:17:54,240 --> 00:18:00,320
let's see what's new in the two network

00:17:56,799 --> 00:18:03,440
model for the srlv lamp migration

00:18:00,320 --> 00:18:07,360
so a new world io protocol driver

00:18:03,440 --> 00:18:11,039
is implemented here it shares the same

00:18:07,360 --> 00:18:13,679
binary as the net qvm input driver

00:18:11,039 --> 00:18:16,559
so that's you that's very convenient for

00:18:13,679 --> 00:18:20,080
them to share the data between them

00:18:16,559 --> 00:18:22,080
also the vf miniport driver is post the

00:18:20,080 --> 00:18:25,120
banning interface and is

00:18:22,080 --> 00:18:26,720
here as you can see the word our

00:18:25,120 --> 00:18:29,840
protocol driver

00:18:26,720 --> 00:18:33,280
and the up layer tcp other protocol

00:18:29,840 --> 00:18:36,960
also exposed the nd spanning interface

00:18:33,280 --> 00:18:39,600
that means the vertical meaning part

00:18:36,960 --> 00:18:40,720
we have manipulated to the vertical

00:18:39,600 --> 00:18:44,880
protocol driver

00:18:40,720 --> 00:18:48,160
also bend to the tcp other particles

00:18:44,880 --> 00:18:50,480
so a notify object is implemented

00:18:48,160 --> 00:18:51,919
here to guarantee the binding between

00:18:50,480 --> 00:18:54,559
word out particle

00:18:51,919 --> 00:18:56,080
and we have municipal java eye in one to

00:18:54,559 --> 00:18:59,520
one mode

00:18:56,080 --> 00:19:02,320
so in the notify object is a com

00:18:59,520 --> 00:19:03,360
object that sits in the dynamic link

00:19:02,320 --> 00:19:05,600
library

00:19:03,360 --> 00:19:07,360
when the word out protocol driver is

00:19:05,600 --> 00:19:10,799
being installed

00:19:07,360 --> 00:19:12,000
the network transport class installer

00:19:10,799 --> 00:19:14,880
will register

00:19:12,000 --> 00:19:16,640
a notification object for this with our

00:19:14,880 --> 00:19:20,960
protocol driver

00:19:16,640 --> 00:19:24,320
so when the vf device is hard added

00:19:20,960 --> 00:19:26,880
any new bending generated to the

00:19:24,320 --> 00:19:28,000
overt our protocol driver although we

00:19:26,880 --> 00:19:31,360
have main project

00:19:28,000 --> 00:19:34,720
will be detected so if the bending is

00:19:31,360 --> 00:19:35,679
between the orbital protocol and the vf

00:19:34,720 --> 00:19:38,559
mini part

00:19:35,679 --> 00:19:42,640
it will be allowed any other bindings

00:19:38,559 --> 00:19:42,640
bended to them will be disabled

00:19:43,600 --> 00:19:48,080
so this guarantee the the binding

00:19:45,919 --> 00:19:49,200
between the vertical protocol driver and

00:19:48,080 --> 00:19:53,840
the bf meaning

00:19:49,200 --> 00:19:53,840
driver are exclusive

00:19:54,000 --> 00:19:59,679
so the protocol driver is the important

00:19:57,360 --> 00:20:02,320
part in the two net dev models

00:19:59,679 --> 00:20:03,280
i will talk about it in more details

00:20:02,320 --> 00:20:05,760
here

00:20:03,280 --> 00:20:06,559
at first the protocol driver behave like

00:20:05,760 --> 00:20:09,679
a bridge

00:20:06,559 --> 00:20:11,679
between the mini product driver

00:20:09,679 --> 00:20:14,000
whatever mini port driver and the vf

00:20:11,679 --> 00:20:16,799
minipro driver

00:20:14,000 --> 00:20:19,280
and also internet down model the vf

00:20:16,799 --> 00:20:20,320
adapter is coupled to the vertical

00:20:19,280 --> 00:20:23,919
adapter

00:20:20,320 --> 00:20:26,960
with the same mac address when the vf

00:20:23,919 --> 00:20:28,240
adapter is hard added the protocol

00:20:26,960 --> 00:20:31,280
driver will search

00:20:28,240 --> 00:20:34,320
for the matched mac address among all

00:20:31,280 --> 00:20:37,039
users to invert our network devices

00:20:34,320 --> 00:20:38,000
if there's matched one the protocol

00:20:37,039 --> 00:20:41,200
driver will

00:20:38,000 --> 00:20:44,240
bend it to the vf miniport driver

00:20:41,200 --> 00:20:47,120
and switch the datapath to it

00:20:44,240 --> 00:20:48,240
so when the vf network adapter is had

00:20:47,120 --> 00:20:50,320
removed

00:20:48,240 --> 00:20:52,080
the protocol driver will shut down

00:20:50,320 --> 00:20:54,240
expanding and

00:20:52,080 --> 00:20:56,159
switch the data path back to the

00:20:54,240 --> 00:20:59,440
vertical

00:20:56,159 --> 00:21:01,120
for the ts network data the protocol

00:20:59,440 --> 00:21:05,200
driver will set the source

00:21:01,120 --> 00:21:08,240
handle of the net buffer list as the vf

00:21:05,200 --> 00:21:11,280
mean part bending handle and forward the

00:21:08,240 --> 00:21:11,760
network data to the vf mini part through

00:21:11,280 --> 00:21:15,039
this

00:21:11,760 --> 00:21:18,559
handle for the rx network

00:21:15,039 --> 00:21:22,400
data the protocol driver will indicate

00:21:18,559 --> 00:21:24,559
all the network data from vf mean part

00:21:22,400 --> 00:21:26,159
to the app layer protocol through the

00:21:24,559 --> 00:21:30,320
vertical mini part

00:21:26,159 --> 00:21:31,039
handle the object identifiers are

00:21:30,320 --> 00:21:35,360
wrapped and

00:21:31,039 --> 00:21:38,960
forwarded offload are propagated in this

00:21:35,360 --> 00:21:42,720
same way and the work of the

00:21:38,960 --> 00:21:44,159
propagation in this case is much less

00:21:42,720 --> 00:21:48,080
than the restoring

00:21:44,159 --> 00:21:48,080
offload working in max driver

00:21:48,159 --> 00:21:55,360
so here is the bending details of the

00:21:52,159 --> 00:21:59,039
vertel srlv the

00:21:55,360 --> 00:22:02,080
ethernet file is the vf network

00:21:59,039 --> 00:22:03,440
it only bends to the red hybrid l net qm

00:22:02,080 --> 00:22:06,640
protocol driver

00:22:03,440 --> 00:22:09,919
it doesn't bend to any other drivers

00:22:06,640 --> 00:22:13,200
ethernet 11 is the vertical

00:22:09,919 --> 00:22:15,760
connection event to the tcp ip

00:22:13,200 --> 00:22:16,400
other particle but it doesn't bend to

00:22:15,760 --> 00:22:20,240
the

00:22:16,400 --> 00:22:20,240
its own protocol driver

00:22:20,559 --> 00:22:26,240
so uh you can see that's now that

00:22:23,600 --> 00:22:26,720
you know how the internet dev model

00:22:26,240 --> 00:22:30,240
works

00:22:26,720 --> 00:22:30,799
in the windows vert our driver so i will

00:22:30,240 --> 00:22:32,960
hand it

00:22:30,799 --> 00:22:35,520
over to yen to talk about the current

00:22:32,960 --> 00:22:41,840
status of the two network module

00:22:35,520 --> 00:22:41,840
yeah please take it over thank you

00:22:45,200 --> 00:22:50,080
thank you very much any so let's talk

00:22:48,559 --> 00:22:53,280
about the status and the

00:22:50,080 --> 00:22:55,120
known issues so the code for this

00:22:53,280 --> 00:22:56,720
solution is already upstream and you can

00:22:55,120 --> 00:22:58,960
use it

00:22:56,720 --> 00:23:00,159
and the known issues that we have is are

00:22:58,960 --> 00:23:02,320
the followings

00:23:00,159 --> 00:23:04,720
so first of all uh the support is only

00:23:02,320 --> 00:23:07,360
for the newer operating systems

00:23:04,720 --> 00:23:09,039
uh second the statistics for the vf is

00:23:07,360 --> 00:23:11,760
missing because they are not propagated

00:23:09,039 --> 00:23:14,000
to the net kvm driver

00:23:11,760 --> 00:23:15,280
we have some issues that are related on

00:23:14,000 --> 00:23:17,679
the order

00:23:15,280 --> 00:23:19,520
of starting of the devices so one of

00:23:17,679 --> 00:23:20,960
them is the dhtp issue that we might

00:23:19,520 --> 00:23:23,919
have

00:23:20,960 --> 00:23:25,600
and another thing that you should know

00:23:23,919 --> 00:23:27,600
is that

00:23:25,600 --> 00:23:28,640
if you want the solution to work on the

00:23:27,600 --> 00:23:30,960
specific

00:23:28,640 --> 00:23:31,679
vf you need to add the plug-and-play id

00:23:30,960 --> 00:23:35,600
of the cf

00:23:31,679 --> 00:23:39,600
either to the notification object code

00:23:35,600 --> 00:23:43,039
or to the registry

00:23:39,600 --> 00:23:44,559
uh regarding the current solution with a

00:23:43,039 --> 00:23:48,000
virtual netspec and the

00:23:44,559 --> 00:23:50,159
net fs standby so we are not using this

00:23:48,000 --> 00:23:52,640
capability right now because they're we

00:23:50,159 --> 00:23:55,039
are relying on the notification object

00:23:52,640 --> 00:23:56,640
to notify us about the appearance and

00:23:55,039 --> 00:23:57,279
disappearance of the devices in the

00:23:56,640 --> 00:24:00,960
system

00:23:57,279 --> 00:24:04,000
uh in the future we might use it

00:24:00,960 --> 00:24:07,120
some changes in the installation uh so

00:24:04,000 --> 00:24:09,679
first of all what we had before we had

00:24:07,120 --> 00:24:12,320
one inf file for the miniport driver

00:24:09,679 --> 00:24:12,960
and after those changes we have one inf

00:24:12,320 --> 00:24:15,120
file

00:24:12,960 --> 00:24:16,480
for the mini port driver and we have

00:24:15,120 --> 00:24:18,559
another inf

00:24:16,480 --> 00:24:20,000
for the protocol driver definition and

00:24:18,559 --> 00:24:24,159
for the notify object

00:24:20,000 --> 00:24:27,440
so it's kind of a dual installation here

00:24:24,159 --> 00:24:29,360
regarding whk certification

00:24:27,440 --> 00:24:31,919
before we were certifying the miniport

00:24:29,360 --> 00:24:34,159
driver and the microsoft automatically

00:24:31,919 --> 00:24:35,600
review the test package

00:24:34,159 --> 00:24:37,760
and currently it's a two-step

00:24:35,600 --> 00:24:39,600
certification so first we need to verify

00:24:37,760 --> 00:24:41,679
the miniport driver

00:24:39,600 --> 00:24:43,760
and there is automatic review for that

00:24:41,679 --> 00:24:47,039
and then we should certify verify

00:24:43,760 --> 00:24:50,240
the whole solution and

00:24:47,039 --> 00:24:54,799
submit the test results and

00:24:50,240 --> 00:24:54,799
it's a manual review for the second time

00:24:55,039 --> 00:24:58,720
uh let's take a look at the performance

00:24:57,360 --> 00:25:02,240
numbers

00:24:58,720 --> 00:25:05,279
and here's the performance number on

00:25:02,240 --> 00:25:08,240
100 gigabit per second card

00:25:05,279 --> 00:25:11,200
uh between the hosts and uh you can see

00:25:08,240 --> 00:25:13,279
it a vm to remote host traffic

00:25:11,200 --> 00:25:15,679
uh so there are several things so first

00:25:13,279 --> 00:25:19,360
of all what we wanted to see is

00:25:15,679 --> 00:25:22,159
almost no degradation uh between

00:25:19,360 --> 00:25:22,480
uh the usage of the vf and the usage of

00:25:22,159 --> 00:25:25,840
the

00:25:22,480 --> 00:25:27,039
to network model in our solution and you

00:25:25,840 --> 00:25:29,279
can see it here

00:25:27,039 --> 00:25:31,520
and in order to achieve it uh what we

00:25:29,279 --> 00:25:34,559
have to do is of course to

00:25:31,520 --> 00:25:36,799
propagate the oids correctly

00:25:34,559 --> 00:25:38,559
in order to ensure that that the offload

00:25:36,799 --> 00:25:40,480
is propagated correctly from

00:25:38,559 --> 00:25:42,320
offloads offload settings are correctly

00:25:40,480 --> 00:25:45,760
propagated from the vf

00:25:42,320 --> 00:25:48,640
to the protocol driver and um

00:25:45,760 --> 00:25:48,960
also uh ensure the correct settings of

00:25:48,640 --> 00:25:53,919
the

00:25:48,960 --> 00:25:57,039
jumbo frames so you see here use a 9000.

00:25:53,919 --> 00:25:59,840
this is a data for a remote host

00:25:57,039 --> 00:26:02,480
vm as well you can see that the

00:25:59,840 --> 00:26:05,440
performance here is almost not

00:26:02,480 --> 00:26:07,200
diminished when we are using our

00:26:05,440 --> 00:26:11,440
solution

00:26:07,200 --> 00:26:14,799
and another results are vm to vm

00:26:11,440 --> 00:26:17,760
that are running on the remote costs

00:26:14,799 --> 00:26:18,720
so thank you very much uh if you have

00:26:17,760 --> 00:26:21,279
more questions

00:26:18,720 --> 00:26:22,080
please ask us in the chat or send us

00:26:21,279 --> 00:26:23,520
emails

00:26:22,080 --> 00:26:25,120
with questions and comments we'll be

00:26:23,520 --> 00:26:38,400
happy to answer them

00:26:25,120 --> 00:26:40,480
thank you very much

00:26:38,400 --> 00:26:40,480

YouTube URL: https://www.youtube.com/watch?v=6_TLq9VSXLE


