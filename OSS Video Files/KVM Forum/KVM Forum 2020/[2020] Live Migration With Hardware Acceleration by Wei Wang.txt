Title: [2020] Live Migration With Hardware Acceleration by Wei Wang
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	Guests with memory write intensive workloads are difficult to live migrate and guests with large memory size take long time to migrate. The existing solutions reduce the amount of data to migrate using extra CPU cycles to compress the memory or perform delta operations to migrate the updated bytes. Those do not work as fast as expected, and optimizations like multi-threading compression consume lots of host CPUs. This talk introduces some features enhanced to the migration framework to use hardware accelerators to process the guest memory. Initial results with QAT-based compression show ~5x larger migration throughput compared to compression using 16 CPUs, which consequently supports higher guest dirty rate and has shorter migration time. DSA-based delta operation is work in progress and it performs better when the delta encoding rate is higher than the compress rate.

---

Wei Wang
Intel Corp., Senior Software Engineer

Wei is currently a software developer at Intel. He earned a Master degree from the University of Ottawa, Canada. Wei has rich experience in the virtualization field and he worked on many projects such as network virtualization, live migration, memory ballooning, PMU virtualization, FPGA virtualization etc. Wei presented at many academic and technical conferences before, such as CODES+ISSS, KVM Forum, Intel OSTS, DPDK Summit etc. So far, he has 4 virtualization related PCTs granted.
Captions: 
	00:00:01,839 --> 00:00:05,600
good afternoon my name is

00:00:03,280 --> 00:00:07,359
weiwang and i'm from intel in this

00:00:05,600 --> 00:00:09,920
presentation i'm going to introduce

00:00:07,359 --> 00:00:10,800
using hardware x emulators to accentuate

00:00:09,920 --> 00:00:13,519
the

00:00:10,800 --> 00:00:15,280
migration of virtual machines those guys

00:00:13,519 --> 00:00:19,119
here are also from intel and they are

00:00:15,280 --> 00:00:19,119
contributors to this project

00:00:20,720 --> 00:00:24,880
let's have a look at the agenda of this

00:00:22,880 --> 00:00:27,279
presentation so in total i have

00:00:24,880 --> 00:00:30,080
five parts in the first part i'm going

00:00:27,279 --> 00:00:32,399
to introduce the goals of this project

00:00:30,080 --> 00:00:34,800
and in the second part i will give a

00:00:32,399 --> 00:00:36,800
high level

00:00:34,800 --> 00:00:38,719
give an introduction of the high level

00:00:36,800 --> 00:00:40,719
architecture of this solution

00:00:38,719 --> 00:00:42,559
and in the third part i will introduce

00:00:40,719 --> 00:00:45,120
some important features that are

00:00:42,559 --> 00:00:47,680
used in this solution and in the fourth

00:00:45,120 --> 00:00:49,520
part i will show some test results

00:00:47,680 --> 00:00:53,440
and in the last part i will introduce

00:00:49,520 --> 00:00:55,360
some future works that we plan to do

00:00:53,440 --> 00:00:57,199
okay let's start from the projected

00:00:55,360 --> 00:01:00,559
goals

00:00:57,199 --> 00:01:03,120
so um today there are some 10 points

00:01:00,559 --> 00:01:05,199
in now migration so the first one is

00:01:03,120 --> 00:01:07,360
that the virtual machines with memory

00:01:05,199 --> 00:01:07,920
intensive light intensive workloads are

00:01:07,360 --> 00:01:11,119
difficult

00:01:07,920 --> 00:01:13,200
to migrate so this is because the guest

00:01:11,119 --> 00:01:14,720
rights to the memory they dirty more

00:01:13,200 --> 00:01:17,360
pages than the pages that

00:01:14,720 --> 00:01:18,080
can be transferred to migration so the

00:01:17,360 --> 00:01:20,720
second one

00:01:18,080 --> 00:01:22,880
is vms with large memory size usually

00:01:20,720 --> 00:01:26,000
takes long time to migrate

00:01:22,880 --> 00:01:26,960
the third one is the vm migration may

00:01:26,000 --> 00:01:30,159
consume

00:01:26,960 --> 00:01:30,479
large network bandwidth so there are

00:01:30,159 --> 00:01:33,520
some

00:01:30,479 --> 00:01:36,880
existing solutions in the current

00:01:33,520 --> 00:01:39,680
queuing for example

00:01:36,880 --> 00:01:41,119
people may choose to use the due to

00:01:39,680 --> 00:01:42,960
compression like a

00:01:41,119 --> 00:01:45,040
multi-threaded compression with the

00:01:42,960 --> 00:01:48,079
z-lip and the problem is that

00:01:45,040 --> 00:01:50,799
it's snow based on our experiments

00:01:48,079 --> 00:01:52,960
and the second issue is it consumes too

00:01:50,799 --> 00:01:56,399
many cpus from the host

00:01:52,960 --> 00:01:59,680
so this is not expected from the

00:01:56,399 --> 00:02:01,920
cloud cloud vendors so

00:01:59,680 --> 00:02:03,119
our solution is to offload the

00:02:01,920 --> 00:02:06,159
compression part to

00:02:03,119 --> 00:02:07,360
interactivity click assistance in

00:02:06,159 --> 00:02:09,759
technology

00:02:07,360 --> 00:02:10,959
with efficient approaches so by

00:02:09,759 --> 00:02:13,680
efficient i mean

00:02:10,959 --> 00:02:14,480
higher migration throughput so this is

00:02:13,680 --> 00:02:17,520
measured by

00:02:14,480 --> 00:02:19,200
how many pages we can transfer the

00:02:17,520 --> 00:02:20,720
phone source to destination to

00:02:19,200 --> 00:02:23,280
immigration

00:02:20,720 --> 00:02:23,920
so the higher the better and the second

00:02:23,280 --> 00:02:26,160
one is

00:02:23,920 --> 00:02:27,120
lower cpu buttonization so we don't want

00:02:26,160 --> 00:02:30,400
to waste

00:02:27,120 --> 00:02:32,560
more cpus on the horse so the

00:02:30,400 --> 00:02:33,920
second goal we have here is to have a

00:02:32,560 --> 00:02:36,959
comedy line ready for

00:02:33,920 --> 00:02:39,360
future more accelerators to draw in so

00:02:36,959 --> 00:02:40,720
on the upcoming interest of graphic cpus

00:02:39,360 --> 00:02:43,360
we will have

00:02:40,720 --> 00:02:44,000
data streaming accelerator short for dsc

00:02:43,360 --> 00:02:46,879
and

00:02:44,000 --> 00:02:47,599
intel analytics accelerator shot for

00:02:46,879 --> 00:02:50,000
iago

00:02:47,599 --> 00:02:52,480
iex so we have these two more

00:02:50,000 --> 00:02:53,599
accelerators to be integrated into the

00:02:52,480 --> 00:02:56,800
cpu chip

00:02:53,599 --> 00:02:58,720
and we want to take advantage of them to

00:02:56,800 --> 00:03:00,720
process the guest memory during

00:02:58,720 --> 00:03:02,959
migration as well

00:03:00,720 --> 00:03:04,159
and also we wanted to have a smarter

00:03:02,959 --> 00:03:07,040
selection technique

00:03:04,159 --> 00:03:07,280
which is smartly and dynamically select

00:03:07,040 --> 00:03:10,959
an

00:03:07,280 --> 00:03:13,280
appropriate examinator to accent return

00:03:10,959 --> 00:03:16,959
on migration so i will introduce this

00:03:13,280 --> 00:03:16,959
a little bit more in later slides

00:03:17,760 --> 00:03:23,040
so let's have a look at the architecture

00:03:21,440 --> 00:03:25,599
of this solution

00:03:23,040 --> 00:03:27,360
so on the source machine so on the

00:03:25,599 --> 00:03:30,159
bottom here you can see that

00:03:27,360 --> 00:03:31,360
we could have multiple accelerators here

00:03:30,159 --> 00:03:34,159
and

00:03:31,360 --> 00:03:35,040
the each of them have their own software

00:03:34,159 --> 00:03:40,080
stack like

00:03:35,040 --> 00:03:42,560
the qt library and the driver

00:03:40,080 --> 00:03:43,120
and on the on the top there are two

00:03:42,560 --> 00:03:46,720
threads

00:03:43,120 --> 00:03:50,159
here so the migration migrations that is

00:03:46,720 --> 00:03:52,720
the one that's already exist between you

00:03:50,159 --> 00:03:53,519
so in the migration thread there i

00:03:52,720 --> 00:03:56,879
conclude

00:03:53,519 --> 00:03:59,920
it into four steps the first step is the

00:03:56,879 --> 00:04:01,840
migration setup so basically it will do

00:03:59,920 --> 00:04:04,239
some preparation for migration

00:04:01,840 --> 00:04:06,400
including the accelerator device in this

00:04:04,239 --> 00:04:08,560
initialization and the device

00:04:06,400 --> 00:04:10,959
and the device according throughout the

00:04:08,560 --> 00:04:13,040
creation

00:04:10,959 --> 00:04:15,040
so in the second step of the microphone

00:04:13,040 --> 00:04:17,600
step the migrations that will

00:04:15,040 --> 00:04:19,120
do the page searching so it searches for

00:04:17,600 --> 00:04:22,880
30 pages to

00:04:19,120 --> 00:04:25,120
process and send it to the destination

00:04:22,880 --> 00:04:26,160
so in the third part in the in the third

00:04:25,120 --> 00:04:29,199
step

00:04:26,160 --> 00:04:31,680
we will do a snap a smarter selection

00:04:29,199 --> 00:04:33,040
so the migration thread will select an

00:04:31,680 --> 00:04:35,440
appropriate

00:04:33,040 --> 00:04:38,320
accelerator based on the history of the

00:04:35,440 --> 00:04:40,880
acceleration efficiency

00:04:38,320 --> 00:04:43,120
so once it decides that for example

00:04:40,880 --> 00:04:44,639
using acuity is more efficient so it

00:04:43,120 --> 00:04:49,280
will choose security to

00:04:44,639 --> 00:04:52,479
process memory for the following pages

00:04:49,280 --> 00:04:53,360
and the the fourth step is to dispatch

00:04:52,479 --> 00:04:56,000
requests

00:04:53,360 --> 00:04:57,040
so once the accelerator is selected it

00:04:56,000 --> 00:05:00,639
will compose

00:04:57,040 --> 00:05:01,199
a request data structure and submit data

00:05:00,639 --> 00:05:04,000
to

00:05:01,199 --> 00:05:05,680
the device to due process for example if

00:05:04,000 --> 00:05:08,240
it's using qet

00:05:05,680 --> 00:05:09,520
and there may have multiple qt

00:05:08,240 --> 00:05:12,479
compression engines

00:05:09,520 --> 00:05:12,880
so it may deliver dispatch the requests

00:05:12,479 --> 00:05:15,440
to

00:05:12,880 --> 00:05:18,240
the those compression engines in one

00:05:15,440 --> 00:05:18,240
looping fashion

00:05:18,960 --> 00:05:23,280
and for the device including that its

00:05:20,960 --> 00:05:25,680
main job is to pull for responses

00:05:23,280 --> 00:05:26,720
from the device so the response here

00:05:25,680 --> 00:05:29,280
means that

00:05:26,720 --> 00:05:30,880
the previously submitted request has

00:05:29,280 --> 00:05:34,400
been processed

00:05:30,880 --> 00:05:37,360
and once the response is is got

00:05:34,400 --> 00:05:38,560
is obtained by the polling site it will

00:05:37,360 --> 00:05:41,600
release the

00:05:38,560 --> 00:05:44,560
the request the data structure

00:05:41,600 --> 00:05:46,960
and it blocks when there is no response

00:05:44,560 --> 00:05:46,960
ready

00:05:47,520 --> 00:05:53,280
so the second thing the police are

00:05:50,560 --> 00:05:54,320
doing here is to send the compressed

00:05:53,280 --> 00:05:56,319
data

00:05:54,320 --> 00:05:59,039
along with the related header to the

00:05:56,319 --> 00:06:01,360
destination through the network

00:05:59,039 --> 00:06:03,199
so in this model we basically have a

00:06:01,360 --> 00:06:05,120
split of the

00:06:03,199 --> 00:06:07,360
migration flow so in the current

00:06:05,120 --> 00:06:09,919
migration flow

00:06:07,360 --> 00:06:10,639
the migratory thread is responsible for

00:06:09,919 --> 00:06:13,840
searching for

00:06:10,639 --> 00:06:14,639
30 pages and then send it to the to the

00:06:13,840 --> 00:06:18,840
network

00:06:14,639 --> 00:06:23,199
so in this model we the data transform

00:06:18,840 --> 00:06:25,759
is is given to the polling thread that

00:06:23,199 --> 00:06:25,759
you send

00:06:26,400 --> 00:06:30,000
so here is the public on the destination

00:06:28,560 --> 00:06:31,759
side it's

00:06:30,000 --> 00:06:33,120
similar to the one that we saw on the

00:06:31,759 --> 00:06:35,600
south side

00:06:33,120 --> 00:06:36,560
so on the bottom here you also have

00:06:35,600 --> 00:06:39,680
those

00:06:36,560 --> 00:06:40,800
accent litters and on the top it has the

00:06:39,680 --> 00:06:44,639
two

00:06:40,800 --> 00:06:47,120
threads as before and

00:06:44,639 --> 00:06:48,560
the migration setup is similar as the

00:06:47,120 --> 00:06:50,560
source side so it will do some

00:06:48,560 --> 00:06:54,000
initialization work so the difference

00:06:50,560 --> 00:06:57,440
here is it has a page receiving step so

00:06:54,000 --> 00:06:58,960
this step is it receives the data from

00:06:57,440 --> 00:07:02,479
the network

00:06:58,960 --> 00:07:03,520
and then the migration in part process

00:07:02,479 --> 00:07:05,680
the

00:07:03,520 --> 00:07:08,400
the migration protocol for example we

00:07:05,680 --> 00:07:11,039
added a multi-page protocol so there is

00:07:08,400 --> 00:07:13,360
a multi-agent header

00:07:11,039 --> 00:07:14,479
and also either we are select an

00:07:13,360 --> 00:07:18,479
accelerator

00:07:14,479 --> 00:07:21,680
to to process the memory so

00:07:18,479 --> 00:07:24,639
the the the data the the header like

00:07:21,680 --> 00:07:25,680
the header tells the migration that on

00:07:24,639 --> 00:07:27,599
the designer side

00:07:25,680 --> 00:07:29,440
that which accelerator to use for

00:07:27,599 --> 00:07:31,280
example if the south side use the

00:07:29,440 --> 00:07:32,720
accuracy to do compression

00:07:31,280 --> 00:07:36,880
then on the destination side they need

00:07:32,720 --> 00:07:40,720
to be a selected qt to do decompression

00:07:36,880 --> 00:07:43,919
so for the device supporting thread and

00:07:40,720 --> 00:07:44,800
on the destiny side its job is simple so

00:07:43,919 --> 00:07:48,240
in the chat says

00:07:44,800 --> 00:07:51,440
it adjusts the post for responses

00:07:48,240 --> 00:07:52,720
so once a response is obtained then it

00:07:51,440 --> 00:07:59,120
will

00:07:52,720 --> 00:08:01,280
release the request data structure

00:07:59,120 --> 00:08:04,160
uh it also blocks when there is no

00:08:01,280 --> 00:08:04,160
response ready

00:08:04,639 --> 00:08:10,240
and the uh for the device the decompress

00:08:08,960 --> 00:08:13,840
the data idea made

00:08:10,240 --> 00:08:13,840
to the premium memory directly

00:08:14,479 --> 00:08:20,560
okay let's go to the the third part

00:08:17,759 --> 00:08:21,199
um so i will introduce some features

00:08:20,560 --> 00:08:24,560
that

00:08:21,199 --> 00:08:27,599
are used in this solution so

00:08:24,560 --> 00:08:30,240
the first feature we use is zero copy so

00:08:27,599 --> 00:08:33,440
it allows with this feature it allows

00:08:30,240 --> 00:08:37,120
the accelerator device to directly

00:08:33,440 --> 00:08:41,360
access to the guest memory so the second

00:08:37,120 --> 00:08:41,360
one we use is the multi-page process

00:08:41,680 --> 00:08:44,880
so the current migration flow only

00:08:43,760 --> 00:08:47,200
supports the

00:08:44,880 --> 00:08:49,600
single page processing meaning that the

00:08:47,200 --> 00:08:52,480
ether finds the only one

00:08:49,600 --> 00:08:52,720
dirty page and then compress this page

00:08:52,480 --> 00:08:55,519
and

00:08:52,720 --> 00:08:56,560
then send it to the migration to the

00:08:55,519 --> 00:08:58,880
network

00:08:56,560 --> 00:09:00,399
so with this feature the microsoft

00:08:58,880 --> 00:09:03,519
follow is able to

00:09:00,399 --> 00:09:05,920
process multiple pages each time and the

00:09:03,519 --> 00:09:06,320
third feature is the extension request

00:09:05,920 --> 00:09:09,680
of

00:09:06,320 --> 00:09:13,120
caching so this feature just caches the

00:09:09,680 --> 00:09:16,160
acceleration request the data structure

00:09:13,120 --> 00:09:18,720
for efficient memory allocation so i

00:09:16,160 --> 00:09:22,000
will introduce more details about

00:09:18,720 --> 00:09:24,959
this those three features so

00:09:22,000 --> 00:09:26,000
for zero copy so at the migration setup

00:09:24,959 --> 00:09:28,399
step

00:09:26,000 --> 00:09:30,720
uh the migration strategy is to

00:09:28,399 --> 00:09:32,320
pre-allocate and the pin

00:09:30,720 --> 00:09:33,920
on the clean memory disk to be

00:09:32,320 --> 00:09:37,279
pre-allocated and

00:09:33,920 --> 00:09:40,399
pinned so this is uh to prevent

00:09:37,279 --> 00:09:42,800
the memory to be swept out during

00:09:40,399 --> 00:09:46,640
migration

00:09:42,800 --> 00:09:48,880
and so on the destination side

00:09:46,640 --> 00:09:51,839
the memory will be amping the when the

00:09:48,880 --> 00:09:51,839
migration is done

00:09:54,480 --> 00:09:58,240
but this is not indeed the in the future

00:09:56,800 --> 00:10:01,600
when we have vfi will be

00:09:58,240 --> 00:10:05,600
the best is the driver

00:10:01,600 --> 00:10:09,839
so i will introduce in our future work

00:10:05,600 --> 00:10:09,839
so for the request composing

00:10:10,000 --> 00:10:14,320
on the source side the dma will set up a

00:10:12,800 --> 00:10:17,920
dma buffer for the

00:10:14,320 --> 00:10:19,040
for the source device to to process the

00:10:17,920 --> 00:10:21,920
memory

00:10:19,040 --> 00:10:24,320
so the the dna read the buffer uh it

00:10:21,920 --> 00:10:27,440
points to the premium memory so like uh

00:10:24,320 --> 00:10:29,600
the the creative device can directly you

00:10:27,440 --> 00:10:34,240
dma reader from the cumulative memory

00:10:29,600 --> 00:10:36,480
and then compress compress the gas data

00:10:34,240 --> 00:10:37,920
and for the dma right buffer it's

00:10:36,480 --> 00:10:40,480
allocated by the

00:10:37,920 --> 00:10:41,519
library for example the qet library so

00:10:40,480 --> 00:10:43,680
when the

00:10:41,519 --> 00:10:45,120
the compressed when the compression is

00:10:43,680 --> 00:10:48,000
ready it is done

00:10:45,120 --> 00:10:48,800
the data idea made the idea made right

00:10:48,000 --> 00:10:52,240
to the

00:10:48,800 --> 00:10:55,440
to the to the to the buffer

00:10:52,240 --> 00:10:57,360
and on the destination side the dma grid

00:10:55,440 --> 00:10:59,440
buffer is allocated by the

00:10:57,360 --> 00:11:01,040
by the library so there is a piece of

00:10:59,440 --> 00:11:03,600
buffer

00:11:01,040 --> 00:11:07,360
and the dna right buffer points to the

00:11:03,600 --> 00:11:10,399
extremely memory so the device

00:11:07,360 --> 00:11:12,880
reads the compress the data from this

00:11:10,399 --> 00:11:14,560
library buffer and then do decompression

00:11:12,880 --> 00:11:15,839
so once the compression is done the

00:11:14,560 --> 00:11:18,079
device

00:11:15,839 --> 00:11:19,279
due dm right to the cream in the memory

00:11:18,079 --> 00:11:23,200
directly

00:11:19,279 --> 00:11:25,760
so this achieves the zero recording

00:11:23,200 --> 00:11:26,720
so for the multi-page processing so here

00:11:25,760 --> 00:11:30,320
is an example that

00:11:26,720 --> 00:11:31,120
we have 12 pages so the migration thread

00:11:30,320 --> 00:11:34,240
will

00:11:31,120 --> 00:11:35,920
find the multiple dirty pages one time

00:11:34,240 --> 00:11:38,160
for example here it defines that the

00:11:35,920 --> 00:11:42,320
page zero one is dirty

00:11:38,160 --> 00:11:45,279
and it it composes a data structure that

00:11:42,320 --> 00:11:46,000
the data page starts from 0 and size 2

00:11:45,279 --> 00:11:49,279
meaning that

00:11:46,000 --> 00:11:52,160
there are two pages consecutive pages

00:11:49,279 --> 00:11:54,720
and the second group of dirty pages

00:11:52,160 --> 00:11:55,760
starts from page 3 and it has the four

00:11:54,720 --> 00:11:58,079
pages

00:11:55,760 --> 00:11:59,200
and the third group starts from page 8

00:11:58,079 --> 00:12:03,600
and page 9 and

00:11:59,200 --> 00:12:05,519
it has 3 30 pages and then

00:12:03,600 --> 00:12:07,360
the migration strategy user request

00:12:05,519 --> 00:12:11,360
composing conclusion and

00:12:07,360 --> 00:12:14,560
it compose the request

00:12:11,360 --> 00:12:16,079
which is submittable to the device and

00:12:14,560 --> 00:12:19,519
it also needs to set up

00:12:16,079 --> 00:12:20,959
the dma the dma dma buffer so it's a

00:12:19,519 --> 00:12:24,079
scale together buffer

00:12:20,959 --> 00:12:25,200
so like here it has live buffers so the

00:12:24,079 --> 00:12:28,079
dna

00:12:25,200 --> 00:12:29,360
the dna buffer points to these nine

00:12:28,079 --> 00:12:32,399
pages

00:12:29,360 --> 00:12:35,600
and it's joined together

00:12:32,399 --> 00:12:37,600
and the the orange box here

00:12:35,600 --> 00:12:39,519
is the buffer allocated by the

00:12:37,600 --> 00:12:42,959
biosecurity library

00:12:39,519 --> 00:12:46,160
so the qet device factory is

00:12:42,959 --> 00:12:47,360
data from the memory and then you do

00:12:46,160 --> 00:12:50,720
compression

00:12:47,360 --> 00:12:53,760
so once the compression is done the

00:12:50,720 --> 00:12:56,480
data those live pages

00:12:53,760 --> 00:12:58,720
and the compression of these languages

00:12:56,480 --> 00:13:01,360
uh the result is written into the

00:12:58,720 --> 00:13:02,720
orange box which is allocated by the

00:13:01,360 --> 00:13:04,880
create library

00:13:02,720 --> 00:13:06,000
and then the device opponent slider will

00:13:04,880 --> 00:13:08,320
transfer those

00:13:06,000 --> 00:13:09,760
compressed data as a whole to the

00:13:08,320 --> 00:13:12,240
desktop side

00:13:09,760 --> 00:13:14,160
and for the data either each one is

00:13:12,240 --> 00:13:15,519
associated with the multi-page header

00:13:14,160 --> 00:13:18,560
which pairs

00:13:15,519 --> 00:13:21,839
the address of the address of the

00:13:18,560 --> 00:13:21,839
premium page

00:13:22,560 --> 00:13:25,600
like on the destination side when the

00:13:24,399 --> 00:13:28,959
data is received

00:13:25,600 --> 00:13:32,240
here and so it it knows that

00:13:28,959 --> 00:13:34,800
there is multi-page header and

00:13:32,240 --> 00:13:36,639
and the payload is the compress the data

00:13:34,800 --> 00:13:38,160
and then

00:13:36,639 --> 00:13:40,800
the migration thread on the destination

00:13:38,160 --> 00:13:44,959
side will compose the request

00:13:40,800 --> 00:13:48,160
and it defines uh for the dma buffer

00:13:44,959 --> 00:13:51,279
the destination side address is

00:13:48,160 --> 00:13:52,720
is uh calculated by the multi-page

00:13:51,279 --> 00:13:54,959
header which pairs

00:13:52,720 --> 00:13:56,079
where is the communication that should

00:13:54,959 --> 00:13:58,959
hold those

00:13:56,079 --> 00:14:01,360
decompress the data so the device reads

00:13:58,959 --> 00:14:04,079
data from the compressor data and

00:14:01,360 --> 00:14:05,120
decompression and then write those data

00:14:04,079 --> 00:14:07,440
to the

00:14:05,120 --> 00:14:10,320
to the to the destination side the

00:14:07,440 --> 00:14:10,320
premium memory

00:14:11,920 --> 00:14:16,240
so for acceleration request attention

00:14:14,880 --> 00:14:18,959
this is a common

00:14:16,240 --> 00:14:20,320
technique so during the device setup

00:14:18,959 --> 00:14:23,519
stage

00:14:20,320 --> 00:14:26,399
um the migration thread pre-allocates

00:14:23,519 --> 00:14:28,480
some amount of acceleration requests

00:14:26,399 --> 00:14:30,160
data structure and the fuse them into

00:14:28,480 --> 00:14:32,720
the cache code

00:14:30,160 --> 00:14:33,839
and during the request composing stage

00:14:32,720 --> 00:14:36,880
the

00:14:33,839 --> 00:14:39,360
the migration thread will allocate the

00:14:36,880 --> 00:14:42,639
request

00:14:39,360 --> 00:14:44,800
so instead of doing the malloc either

00:14:42,639 --> 00:14:46,399
directly take requests from the cash

00:14:44,800 --> 00:14:49,680
pool once the

00:14:46,399 --> 00:14:51,920
requests that are used up from the cash

00:14:49,680 --> 00:14:54,560
code it will do open up

00:14:51,920 --> 00:14:58,399
so and then it initialize the request

00:14:54,560 --> 00:15:01,040
based on the new pages with them

00:14:58,399 --> 00:15:02,160
so for the response pulling that well

00:15:01,040 --> 00:15:05,199
when a response

00:15:02,160 --> 00:15:05,920
is obtained and the threat will freeze

00:15:05,199 --> 00:15:09,199
the request

00:15:05,920 --> 00:15:12,959
to the cash pool so instead of doing the

00:15:09,199 --> 00:15:15,199
coin the free system

00:15:12,959 --> 00:15:16,240
okay let's have a look at the test

00:15:15,199 --> 00:15:19,120
results

00:15:16,240 --> 00:15:20,079
so for the tests we tested on the inter

00:15:19,120 --> 00:15:23,800
xeon

00:15:20,079 --> 00:15:26,560
cpu e5 2699 before running at

00:15:23,800 --> 00:15:27,360
2.2 gigahertz and this is the broadway

00:15:26,560 --> 00:15:30,560
cpu

00:15:27,360 --> 00:15:33,839
and for qt we use the pcie gen 3

00:15:30,560 --> 00:15:37,680
qt card so it's a pcie card

00:15:33,839 --> 00:15:40,720
plugged into the psi slot

00:15:37,680 --> 00:15:43,920
but in all separate cpus we will have qt

00:15:40,720 --> 00:15:44,720
integrated into cpu and it uses pcie

00:15:43,920 --> 00:15:48,079
gen4

00:15:44,720 --> 00:15:51,199
so the speed will be much faster

00:15:48,079 --> 00:15:55,040
on the upcoming in terms of rapid cpu

00:15:51,199 --> 00:15:59,279
so for the dram and we use the ddr4

00:15:55,040 --> 00:15:59,279
and the running at uh

00:16:01,079 --> 00:16:05,360
00:16:02,320 --> 00:16:06,000
megahertz and for the network card we

00:16:05,360 --> 00:16:09,040
use the

00:16:06,000 --> 00:16:11,279
40 giga network card so for the

00:16:09,040 --> 00:16:14,399
migration setup the downtime we use

00:16:11,279 --> 00:16:17,279
is 300 a microsecond so it's

00:16:14,399 --> 00:16:18,079
the default uh downtime using qmu and

00:16:17,279 --> 00:16:20,320
for the level of

00:16:18,079 --> 00:16:21,600
bandwidth we didn't set it at a limit so

00:16:20,320 --> 00:16:24,639
it can use up to

00:16:21,600 --> 00:16:27,839
40 gigahertz but in reality it won't

00:16:24,639 --> 00:16:30,000
consume those much boundaries

00:16:27,839 --> 00:16:31,519
for the compressed level we set it to

00:16:30,000 --> 00:16:34,639
one so this is the

00:16:31,519 --> 00:16:36,639
fastest speed to compress for the

00:16:34,639 --> 00:16:37,600
multi-page we added a new parameter

00:16:36,639 --> 00:16:40,480
called multi-page

00:16:37,600 --> 00:16:42,240
so meaning that the migration so that

00:16:40,480 --> 00:16:45,600
they can

00:16:42,240 --> 00:16:47,839
can can process multiple pages each time

00:16:45,600 --> 00:16:50,560
so we set the value to 63.

00:16:47,839 --> 00:16:51,279
this is the maximum value can be

00:16:50,560 --> 00:16:53,920
supported

00:16:51,279 --> 00:16:55,120
currently so for the guests we have

00:16:53,920 --> 00:16:58,639
three types of guests

00:16:55,120 --> 00:17:01,759
to test the first typing of gaster has

00:16:58,639 --> 00:17:04,160
the four vcpus and 32gb ram and

00:17:01,759 --> 00:17:06,079
it runs a workload with the writing

00:17:04,160 --> 00:17:08,480
compression friendly data

00:17:06,079 --> 00:17:10,160
and the second type of guest has the

00:17:08,480 --> 00:17:13,439
four vcpus and

00:17:10,160 --> 00:17:15,520
it has a 32gb ram but it runs the

00:17:13,439 --> 00:17:17,839
workload which provides

00:17:15,520 --> 00:17:19,600
sequence numbers to memory so sequence

00:17:17,839 --> 00:17:22,559
number is not a

00:17:19,600 --> 00:17:25,839
very compression friendly but it's still

00:17:22,559 --> 00:17:28,880
okay to compress not difficult also

00:17:25,839 --> 00:17:33,000
so the for the third type of gas it has

00:17:28,880 --> 00:17:36,320
the eight of eight of these cpus and

00:17:33,000 --> 00:17:36,799
128 gig ram and either runs a memory

00:17:36,320 --> 00:17:39,200
cache

00:17:36,799 --> 00:17:41,039
workload with the right reading and

00:17:39,200 --> 00:17:44,240
writing random numbers

00:17:41,039 --> 00:17:44,799
so random numbers are relatively

00:17:44,240 --> 00:17:48,160
difficult

00:17:44,799 --> 00:17:52,000
to compress we will see the conversion

00:17:48,160 --> 00:17:55,600
ratio later so let's have a look at

00:17:52,000 --> 00:17:57,120
the first task so we want this vertical

00:17:55,600 --> 00:18:00,640
workload inside of the guest

00:17:57,120 --> 00:18:02,880
which writes data to the

00:18:00,640 --> 00:18:04,480
to the guess the memory you know specify

00:18:02,880 --> 00:18:07,520
the dirty grid

00:18:04,480 --> 00:18:11,520
and like here we can set it to write uh

00:18:07,520 --> 00:18:14,640
100 megabytes per second

00:18:11,520 --> 00:18:16,480
and for the data so

00:18:14,640 --> 00:18:18,080
there is something probably you need to

00:18:16,480 --> 00:18:20,320
understand first so for the

00:18:18,080 --> 00:18:21,760
stroke it's the migration throughput and

00:18:20,320 --> 00:18:23,919
it's made by

00:18:21,760 --> 00:18:25,919
how many pages we are transferred from

00:18:23,919 --> 00:18:27,039
the source to destination so the higher

00:18:25,919 --> 00:18:29,679
the better

00:18:27,039 --> 00:18:31,039
and to make the numbers simple so i put

00:18:29,679 --> 00:18:34,400
the multiplier

00:18:31,039 --> 00:18:37,520
here so for the num compression case

00:18:34,400 --> 00:18:40,720
it can send like a

00:18:37,520 --> 00:18:43,919
um 17 multiplier ten thousands

00:18:40,720 --> 00:18:46,960
pages per second to 29

00:18:43,919 --> 00:18:50,720
multiply this number so for the

00:18:46,960 --> 00:18:54,000
16 cpu case so it's a little bit

00:18:50,720 --> 00:18:55,360
it's almost two times larger

00:18:54,000 --> 00:18:58,320
more than two times nineteen for the

00:18:55,360 --> 00:19:00,400
acuity so it's uh

00:18:58,320 --> 00:19:01,360
around the five times larger so we can

00:19:00,400 --> 00:19:04,799
see the

00:19:01,360 --> 00:19:05,200
normalized throughput here so the q8

00:19:04,799 --> 00:19:08,400
case

00:19:05,200 --> 00:19:11,840
has much larger migration slope

00:19:08,400 --> 00:19:12,400
than the lung compression case so for

00:19:11,840 --> 00:19:15,039
the

00:19:12,400 --> 00:19:15,440
largest migrateable data reader it means

00:19:15,039 --> 00:19:18,480
that

00:19:15,440 --> 00:19:20,080
we want we turn this number this dirty

00:19:18,480 --> 00:19:23,120
rate inside of the gas

00:19:20,080 --> 00:19:26,240
and we get the data if we

00:19:23,120 --> 00:19:27,440
turn the number to like 100 and 200 then

00:19:26,240 --> 00:19:30,480
this vm cannot

00:19:27,440 --> 00:19:31,919
be migrated with uh with without the

00:19:30,480 --> 00:19:36,720
compression

00:19:31,919 --> 00:19:39,840
so this one um 130 lit

00:19:36,720 --> 00:19:41,520
inside of the gas is the largest w lead

00:19:39,840 --> 00:19:44,880
that the guest can have

00:19:41,520 --> 00:19:46,320
to ensure it can be migrated so for the

00:19:44,880 --> 00:19:48,880
16 cpu case

00:19:46,320 --> 00:19:49,600
it supports this number and for the qt

00:19:48,880 --> 00:19:51,520
case it's

00:19:49,600 --> 00:19:53,760
much larger than the low compression

00:19:51,520 --> 00:19:55,760
piece

00:19:53,760 --> 00:19:58,080
and for the extra cpu you know as usual

00:19:55,760 --> 00:20:00,880
it means how many cpus are used

00:19:58,080 --> 00:20:02,480
in addition to the migration flat so for

00:20:00,880 --> 00:20:06,080
the lung compression case

00:20:02,480 --> 00:20:09,520
there is no extra thread so it doesn't

00:20:06,080 --> 00:20:11,559
consume extra cpu so for the 16 cpu case

00:20:09,520 --> 00:20:16,240
it consumes

00:20:11,559 --> 00:20:19,440
678 cpu percentage cpu it doesn't

00:20:16,240 --> 00:20:22,640
consume all the 16 cpus because this

00:20:19,440 --> 00:20:24,799
data pattern is easy to compress

00:20:22,640 --> 00:20:26,159
so for the qt case it's less than 40

00:20:24,799 --> 00:20:30,480
percent each so it's

00:20:26,159 --> 00:20:32,480
much less than the cpu compression case

00:20:30,480 --> 00:20:34,240
so for the compaction visual you can see

00:20:32,480 --> 00:20:36,480
that uh

00:20:34,240 --> 00:20:37,600
the qt compaction has a higher ratio

00:20:36,480 --> 00:20:40,159
than the

00:20:37,600 --> 00:20:42,159
the cpu zdp compression so the

00:20:40,159 --> 00:20:42,720
compression algorithm we use here is

00:20:42,159 --> 00:20:45,840
similar

00:20:42,720 --> 00:20:47,360
its same is the same but the comparation

00:20:45,840 --> 00:20:48,240
is different this is because the

00:20:47,360 --> 00:20:51,039
multi-page

00:20:48,240 --> 00:20:52,320
comparison so when we have multiple

00:20:51,039 --> 00:20:55,039
pages because the

00:20:52,320 --> 00:20:55,840
for this workload it writes the all ones

00:20:55,039 --> 00:20:59,679
through

00:20:55,840 --> 00:21:03,120
to to the memory so this repeated

00:20:59,679 --> 00:21:06,000
one can be can generate

00:21:03,120 --> 00:21:07,360
only for example one or single one as

00:21:06,000 --> 00:21:10,720
the compression payload

00:21:07,360 --> 00:21:13,520
so in this case multiple pages can have

00:21:10,720 --> 00:21:15,200
the higher compression ratio if we don't

00:21:13,520 --> 00:21:17,280
have this repeated numbers the

00:21:15,200 --> 00:21:19,280
compression ratio between the 2d and the

00:21:17,280 --> 00:21:20,240
cpu compression will be similar you can

00:21:19,280 --> 00:21:23,440
see from the last

00:21:20,240 --> 00:21:23,919
slide here for this sequence number so

00:21:23,440 --> 00:21:26,960
it's not

00:21:23,919 --> 00:21:31,039
repeated it's just a number from 0

00:21:26,960 --> 00:21:33,120
1 2 3. so

00:21:31,039 --> 00:21:34,320
so we run this workload inside of the

00:21:33,120 --> 00:21:36,400
gas and uh

00:21:34,320 --> 00:21:38,640
for the migrations locator we can see

00:21:36,400 --> 00:21:41,679
that uh

00:21:38,640 --> 00:21:42,960
the qd case is here much larger than the

00:21:41,679 --> 00:21:45,520
compression piece

00:21:42,960 --> 00:21:46,000
so interesting here is that we find the

00:21:45,520 --> 00:21:48,480
with

00:21:46,000 --> 00:21:49,120
16 cpu compression the migration

00:21:48,480 --> 00:21:51,760
throughput

00:21:49,120 --> 00:21:53,120
is even lower than the low compaction

00:21:51,760 --> 00:21:56,159
piece this is because

00:21:53,120 --> 00:21:56,880
the compression isn't efficient with cpu

00:21:56,159 --> 00:21:59,919
compression

00:21:56,880 --> 00:22:03,200
so it actually

00:21:59,919 --> 00:22:05,120
de-accelerated the migration so

00:22:03,200 --> 00:22:06,400
for the lattice the migrateable data

00:22:05,120 --> 00:22:08,559
read so

00:22:06,400 --> 00:22:10,159
for the qd case it's still larger than

00:22:08,559 --> 00:22:12,480
the low compression case

00:22:10,159 --> 00:22:14,320
and as for the cpu utilization the cpu

00:22:12,480 --> 00:22:17,520
compression keys

00:22:14,320 --> 00:22:18,960
consumes all the 16 cpu and for security

00:22:17,520 --> 00:22:21,520
conversion case it's

00:22:18,960 --> 00:22:23,520
less than 70 each so the compaction

00:22:21,520 --> 00:22:26,240
ratio between the cpu and

00:22:23,520 --> 00:22:27,360
liquidity is a signal here because they

00:22:26,240 --> 00:22:30,960
use the same

00:22:27,360 --> 00:22:31,360
compression algorithm so for the third

00:22:30,960 --> 00:22:34,559
test

00:22:31,360 --> 00:22:35,440
we use the we set up the memory cache

00:22:34,559 --> 00:22:39,039
key environment

00:22:35,440 --> 00:22:41,280
using this so basically it's a

00:22:39,039 --> 00:22:42,720
client a memory hd client called mem

00:22:41,280 --> 00:22:45,440
step kind and it divides

00:22:42,720 --> 00:22:47,120
random numbers to these memory cache

00:22:45,440 --> 00:22:50,159
memory pages

00:22:47,120 --> 00:22:52,559
so the random number is much more

00:22:50,159 --> 00:22:53,919
difficult to compress than the previous

00:22:52,559 --> 00:22:57,039
data pattern we can see from the

00:22:53,919 --> 00:22:59,760
compression ritual here is only 1.6

00:22:57,039 --> 00:23:00,480
but the 3d comparison cases here has an

00:22:59,760 --> 00:23:04,480
advantage

00:23:00,480 --> 00:23:07,679
over those two cases so

00:23:04,480 --> 00:23:08,080
it's like a more than two times faster

00:23:07,679 --> 00:23:11,760
for the

00:23:08,080 --> 00:23:14,559
migration circuit and the migration time

00:23:11,760 --> 00:23:16,480
here infinite means that uh the mic the

00:23:14,559 --> 00:23:19,600
vm cannot be migrated

00:23:16,480 --> 00:23:22,480
like in the in the low compression case

00:23:19,600 --> 00:23:23,200
so or the cpu comparison case the vm

00:23:22,480 --> 00:23:26,240
cannot be

00:23:23,200 --> 00:23:29,360
migrated but with security case it takes

00:23:26,240 --> 00:23:32,320
around 60 seconds to successfully

00:23:29,360 --> 00:23:32,320
migrate the vm

00:23:34,080 --> 00:23:38,159
okay let's have a look at some future

00:23:36,400 --> 00:23:41,440
works that we plan to do

00:23:38,159 --> 00:23:44,320
so the first one is a vfio driver based

00:23:41,440 --> 00:23:46,240
zero copy so the current vehicle zero

00:23:44,320 --> 00:23:48,080
copy is implemented based on the

00:23:46,240 --> 00:23:50,480
uio-based prt driver

00:23:48,080 --> 00:23:52,480
so this requires the chimney to be root

00:23:50,480 --> 00:23:54,480
privileged to gather the virtual grass

00:23:52,480 --> 00:23:55,520
to physical grass mapping by the pitch

00:23:54,480 --> 00:23:58,240
map

00:23:55,520 --> 00:23:59,200
and this may be difficult for some cloud

00:23:58,240 --> 00:24:00,720
vendors because

00:23:59,200 --> 00:24:02,400
the aquarium doesn't have root

00:24:00,720 --> 00:24:04,880
percentage

00:24:02,400 --> 00:24:05,919
and this also requires accumulative

00:24:04,880 --> 00:24:08,159
penis memory

00:24:05,919 --> 00:24:09,679
with the vf with io based driver we

00:24:08,159 --> 00:24:13,600
actually have the

00:24:09,679 --> 00:24:13,600
the wear file support to do this

00:24:16,480 --> 00:24:20,640
um so for the communities where file

00:24:19,039 --> 00:24:22,720
based the user service driver is

00:24:20,640 --> 00:24:26,240
still working progress and we will be

00:24:22,720 --> 00:24:29,360
able to switch to that later

00:24:26,240 --> 00:24:31,120
and the second uh the second work

00:24:29,360 --> 00:24:32,480
we plan to do is smart acceleration

00:24:31,120 --> 00:24:35,679
support so

00:24:32,480 --> 00:24:38,960
this comes with the idea that dsa

00:24:35,679 --> 00:24:41,880
can do a direction of the dirty memory

00:24:38,960 --> 00:24:43,039
so the functionality is the same as the

00:24:41,880 --> 00:24:45,440
xbzre

00:24:43,039 --> 00:24:46,640
in the current preview so it finds out

00:24:45,440 --> 00:24:49,360
uh the exact

00:24:46,640 --> 00:24:49,679
page that the guest is the elector buys

00:24:49,360 --> 00:24:52,000
that

00:24:49,679 --> 00:24:52,880
the guess the dirtiest for example if

00:24:52,000 --> 00:24:55,679
the if

00:24:52,880 --> 00:24:56,799
the guest only writes one byte so there

00:24:55,679 --> 00:24:58,880
is no need to

00:24:56,799 --> 00:25:00,400
send the entire 4kb page to the

00:24:58,880 --> 00:25:04,080
destination it can

00:25:00,400 --> 00:25:06,799
dsa can help to find out the one byte

00:25:04,080 --> 00:25:08,880
um so this works efficiently when the

00:25:06,799 --> 00:25:09,919
guest only modifies a smaller part of

00:25:08,880 --> 00:25:12,960
page

00:25:09,919 --> 00:25:14,159
but we know if the guest writes the

00:25:12,960 --> 00:25:17,200
entire 40

00:25:14,159 --> 00:25:20,320
pages of the the 4k bytes each time

00:25:17,200 --> 00:25:23,840
then and the s bsc

00:25:20,320 --> 00:25:27,120
galaxy encoding might not be efficient

00:25:23,840 --> 00:25:29,440
so in that case we can use securityq

00:25:27,120 --> 00:25:31,440
compression the foot device instead of

00:25:29,440 --> 00:25:34,799
instead of doing the dirt

00:25:31,440 --> 00:25:37,120
processing so we wanted to have a smart

00:25:34,799 --> 00:25:39,520
acceleration here

00:25:37,120 --> 00:25:41,360
so with this technical the migrant

00:25:39,520 --> 00:25:44,480
migration that will be able to

00:25:41,360 --> 00:25:46,960
dynamically switch to use security and

00:25:44,480 --> 00:25:48,400
ix ix is also accelerator that can be

00:25:46,960 --> 00:25:51,679
used to compression

00:25:48,400 --> 00:25:55,120
so it can select either qt or is

00:25:51,679 --> 00:25:56,159
to do compression or use dsc device to

00:25:55,120 --> 00:25:59,600
do a direct

00:25:56,159 --> 00:26:02,960
processing to non-migration so this will

00:25:59,600 --> 00:26:05,440
rely on a prediction um based on the

00:26:02,960 --> 00:26:07,039
compression visual history and the data

00:26:05,440 --> 00:26:10,400
encoding

00:26:07,039 --> 00:26:13,440
history so for example we can choose to

00:26:10,400 --> 00:26:14,480
do uh to compress the 10 requests at the

00:26:13,440 --> 00:26:17,520
beginning and

00:26:14,480 --> 00:26:20,799
find out what's the compression ratio

00:26:17,520 --> 00:26:24,559
and also do using the essay to

00:26:20,799 --> 00:26:26,880
to find out the encoding read

00:26:24,559 --> 00:26:28,080
if the encoding rate is higher than for

00:26:26,880 --> 00:26:30,320
the upcoming

00:26:28,080 --> 00:26:32,480
pages we can choose dsc if the

00:26:30,320 --> 00:26:34,960
compression rate is higher we can

00:26:32,480 --> 00:26:37,279
use security for the following

00:26:34,960 --> 00:26:45,840
processing

00:26:37,279 --> 00:26:45,840

YouTube URL: https://www.youtube.com/watch?v=c_DyiOmJdtI


