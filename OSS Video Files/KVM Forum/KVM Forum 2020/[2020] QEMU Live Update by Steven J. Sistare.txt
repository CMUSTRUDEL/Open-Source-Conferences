Title: [2020] QEMU Live Update by Steven J. Sistare
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	The ability to update software with critical bug fixes and security mitigations while minimizing downtime is valued highly by customers and providers. In this talk, Steve presents a new method for updating a running instance of QEMU to a new version while minimizing the impact on the VM guest. The guest pauses briefly, for less than 100 msec in the prototype, without loss of internal state or external connections. The old QEMU process exec's the new QEMU binary, and preserves anonymous guest RAM at the same virtual address via a proposed Linux madvise variant. Descriptors for external connections are preserved, and VFIO pass through devices are supported by preserving the VFIO device descriptors and attaching them to a new KVM instance after exec. The update method requires code changes to QEMU, but no changes are required in system libraries or the KVM kernel module.

---

Steven Sistare
Oracle Corporation, Software Architect

Steve is a software architect for the Oracle Linux kernel team, with particular interests in virtualization, performance, scalability, virtual memory, scheduling, security, tools, boot time, and processor support. He previously did similar work in the Solaris kernel. Steve graduated from Harvard University with a PhD in Computer Science. When he is not at the keyboard, Steve enjoys woodworking, astronomy, exercise, and good beer.
Captions: 
	00:00:07,440 --> 00:00:10,320
hi folks

00:00:08,240 --> 00:00:12,160
i'm steve sestere and i work on linux

00:00:10,320 --> 00:00:12,639
kernel and virtualization features at

00:00:12,160 --> 00:00:14,880
oracle

00:00:12,639 --> 00:00:15,920
with my colleagues anthony snaga and

00:00:14,880 --> 00:00:18,160
mark

00:00:15,920 --> 00:00:20,480
welcome to our presentation of qmu live

00:00:18,160 --> 00:00:20,480
update

00:00:23,359 --> 00:00:26,560
live update is a method to update qmu to

00:00:25,680 --> 00:00:28,640
a new version

00:00:26,560 --> 00:00:30,400
while keeping the guest alive has

00:00:28,640 --> 00:00:32,559
minimal impact on the guest

00:00:30,400 --> 00:00:34,239
the guest pauses briefly for about 100

00:00:32,559 --> 00:00:36,719
milliseconds

00:00:34,239 --> 00:00:38,559
update is transparent to guest clients

00:00:36,719 --> 00:00:39,280
they suffer no loss of connectivity to

00:00:38,559 --> 00:00:42,559
the guest

00:00:39,280 --> 00:00:44,960
and only experience the brief pause

00:00:42,559 --> 00:00:46,000
the method supports so iov without guest

00:00:44,960 --> 00:00:47,520
cooperation

00:00:46,000 --> 00:00:49,680
so there's no restriction on the guest

00:00:47,520 --> 00:00:51,760
operating system

00:00:49,680 --> 00:00:53,360
we do this to enable critical bug fixes

00:00:51,760 --> 00:00:55,680
and security mitigations

00:00:53,360 --> 00:00:58,800
in a timely manner to keep our guests

00:00:55,680 --> 00:01:00,960
safe without requiring them to reboot

00:00:58,800 --> 00:01:02,559
however because we update to an entirely

00:01:00,960 --> 00:01:05,920
new version of qmu

00:01:02,559 --> 00:01:07,680
we enable new features as well

00:01:05,920 --> 00:01:09,680
live migration can be used to achieve

00:01:07,680 --> 00:01:10,960
the same results but is more resource

00:01:09,680 --> 00:01:12,960
intensive

00:01:10,960 --> 00:01:15,520
it ties up the source and target hosts

00:01:12,960 --> 00:01:17,840
decreasing fleet utilization

00:01:15,520 --> 00:01:19,200
it consumes memory network bandwidth

00:01:17,840 --> 00:01:21,680
impacting the performance of all the

00:01:19,200 --> 00:01:23,119
processes on the guest and the host

00:01:21,680 --> 00:01:25,280
the duration of the impact is

00:01:23,119 --> 00:01:27,840
indeterminate as it depends on when the

00:01:25,280 --> 00:01:29,920
copy phase converges

00:01:27,840 --> 00:01:32,320
lastly live migration is prohibitively

00:01:29,920 --> 00:01:40,320
expensive if large local storage must be

00:01:32,320 --> 00:01:42,000
copied across the network to the target

00:01:40,320 --> 00:01:43,840
live update is based on the following

00:01:42,000 --> 00:01:46,320
design elements

00:01:43,840 --> 00:01:47,040
the old qmu process execs the new qmu

00:01:46,320 --> 00:01:48,799
binary

00:01:47,040 --> 00:01:50,399
allowing various aspects of the

00:01:48,799 --> 00:01:53,280
execution environment to be carried

00:01:50,399 --> 00:01:55,119
forward into the new

00:01:53,280 --> 00:01:57,360
guest memory is preserved in place in

00:01:55,119 --> 00:01:59,439
ram so dma operations may safely

00:01:57,360 --> 00:02:01,119
continue

00:01:59,439 --> 00:02:04,560
external descriptors are kept open

00:02:01,119 --> 00:02:06,479
across the exec preserving connectivity

00:02:04,560 --> 00:02:08,640
this includes for example serial

00:02:06,479 --> 00:02:11,360
consoles qmu monitor

00:02:08,640 --> 00:02:13,360
vnc sessions pseudoterminals and vhost

00:02:11,360 --> 00:02:15,680
devices

00:02:13,360 --> 00:02:17,440
vfio device descriptors are preserved

00:02:15,680 --> 00:02:20,000
which keeps them alive

00:02:17,440 --> 00:02:22,239
however the kvm descriptor is closed

00:02:20,000 --> 00:02:23,599
which destroys the instance cutting the

00:02:22,239 --> 00:02:26,959
cord between kvm

00:02:23,599 --> 00:02:29,360
and the vfio kernel state

00:02:26,959 --> 00:02:32,480
lastly the qmu back-end device state is

00:02:29,360 --> 00:02:34,560
serialized and saved to a file

00:02:32,480 --> 00:02:36,480
these elements are executed by two new

00:02:34,560 --> 00:02:39,760
qmu monitor interfaces

00:02:36,480 --> 00:02:42,080
cpr save and cpr load where cpr stands

00:02:39,760 --> 00:02:46,879
for checkpoint and restart

00:02:42,080 --> 00:02:46,879
qmp and h versions of each are provided

00:02:47,599 --> 00:02:51,440
live update has been a hot topic this

00:02:49,360 --> 00:02:53,440
year and you may notice some overlap

00:02:51,440 --> 00:02:55,599
between our work and others

00:02:53,440 --> 00:02:57,599
however we've been working independently

00:02:55,599 --> 00:02:59,280
in this area for quite some time

00:02:57,599 --> 00:03:05,840
i believe we are the first to submit our

00:02:59,280 --> 00:03:05,840
patches to the community

00:03:07,120 --> 00:03:11,200
to preserve guest memory in place we

00:03:09,440 --> 00:03:12,080
propose an extension to the m advised

00:03:11,200 --> 00:03:15,360
system called

00:03:12,080 --> 00:03:17,040
m advise do exec this preserves mappings

00:03:15,360 --> 00:03:19,840
in an address range across exec

00:03:17,040 --> 00:03:21,519
the same virtual address it works for

00:03:19,840 --> 00:03:23,280
memory created with map and on

00:03:21,519 --> 00:03:25,599
which otherwise would disappear after

00:03:23,280 --> 00:03:25,599
exec

00:03:26,080 --> 00:03:29,599
the executed binary must explicitly

00:03:28,159 --> 00:03:32,239
allow incoming mappings

00:03:29,599 --> 00:03:35,280
via an elf node this prevents unexpected

00:03:32,239 --> 00:03:37,280
sharing of content across the exec

00:03:35,280 --> 00:03:39,519
the implementation is straightforward

00:03:37,280 --> 00:03:41,120
about 300 lines of kernel code

00:03:39,519 --> 00:03:43,440
most of that is for reading and checking

00:03:41,120 --> 00:03:45,760
the elf note

00:03:43,440 --> 00:03:47,840
the m advise call sets a new keep flag

00:03:45,760 --> 00:03:51,120
on the vmas that span the range

00:03:47,840 --> 00:03:53,360
splitting them if necessary exec

00:03:51,120 --> 00:03:55,920
copies the mark vmas from the old mm to

00:03:53,360 --> 00:03:56,640
the new almost exactly like the vm dupe

00:03:55,920 --> 00:04:00,000
operation

00:03:56,640 --> 00:04:02,000
in fork for details see the m advise

00:04:00,000 --> 00:04:04,640
do exec kernel patches that anthony and

00:04:02,000 --> 00:04:04,640
i submitted

00:04:11,040 --> 00:04:16,799
when m buys is used for qmu the dma

00:04:13,840 --> 00:04:19,040
mappings remain valid at all times

00:04:16,799 --> 00:04:19,919
dma activity from post requests

00:04:19,040 --> 00:04:23,040
continues

00:04:19,919 --> 00:04:25,600
even while the guest is paused

00:04:23,040 --> 00:04:26,160
it is safe to translate iova to virtual

00:04:25,600 --> 00:04:28,560
address

00:04:26,160 --> 00:04:30,400
and page throughout the transition so

00:04:28,560 --> 00:04:34,479
asynchronous kernel threads may safely

00:04:30,400 --> 00:04:34,479
create and access the dma regions

00:04:35,120 --> 00:04:38,720
qmu saves the address and length of the

00:04:37,360 --> 00:04:40,720
preserved memory regions

00:04:38,720 --> 00:04:42,160
in environment variables tagged with the

00:04:40,720 --> 00:04:44,320
name of the region

00:04:42,160 --> 00:04:46,400
in the example at right the pc.ram

00:04:44,320 --> 00:04:48,080
region is remembered in the environment

00:04:46,400 --> 00:04:50,479
with values for both the address and the

00:04:48,080 --> 00:04:52,639
length

00:04:50,479 --> 00:04:55,199
after exec qmu looks for variables of

00:04:52,639 --> 00:04:57,280
this form and retrieves the address

00:04:55,199 --> 00:04:58,960
the address is attached to the new kvm

00:04:57,280 --> 00:05:01,600
instance via the set memory region

00:04:58,960 --> 00:05:01,600
ioctal

00:05:01,680 --> 00:05:04,960
the first time we did this we were

00:05:03,039 --> 00:05:07,039
surprised to find that the ioctal time

00:05:04,960 --> 00:05:08,720
rose linearly with page count

00:05:07,039 --> 00:05:10,960
adding hundreds of milliseconds for

00:05:08,720 --> 00:05:12,960
larger memories

00:05:10,960 --> 00:05:14,800
anthony investigated and found it to be

00:05:12,960 --> 00:05:17,759
an accident of the implementation

00:05:14,800 --> 00:05:19,759
easily fixed he eliminated the linear

00:05:17,759 --> 00:05:25,840
cost with the following kernel patch

00:05:19,759 --> 00:05:25,840
which is available in kernel 5.8

00:05:28,080 --> 00:05:31,840
to support vfio devices we preserve

00:05:30,720 --> 00:05:33,919
their descriptors across

00:05:31,840 --> 00:05:35,440
exec which preserves the kernel state of

00:05:33,919 --> 00:05:38,240
the device

00:05:35,440 --> 00:05:39,680
after exec qmu finds the descriptors and

00:05:38,240 --> 00:05:42,320
rebuilds the data structures that

00:05:39,680 --> 00:05:44,880
represent the device

00:05:42,320 --> 00:05:48,479
the pci bar and config memory regions

00:05:44,880 --> 00:05:50,720
are accessible via the vfio device fd

00:05:48,479 --> 00:05:53,840
after exec qmu maps the bars and

00:05:50,720 --> 00:05:55,759
re-reads the config

00:05:53,840 --> 00:05:58,000
the dma mappings are kept alive by

00:05:55,759 --> 00:06:01,759
preserving the iommu group fd

00:05:58,000 --> 00:06:03,840
in the container fd the interrupt state

00:06:01,759 --> 00:06:06,080
is captured by the event fds and the

00:06:03,840 --> 00:06:08,400
msix data

00:06:06,080 --> 00:06:10,080
event ft's are created and preserved for

00:06:08,400 --> 00:06:14,000
the error and request irqs

00:06:10,080 --> 00:06:16,160
in an msix irq per vector

00:06:14,000 --> 00:06:20,560
the msix table and pending bit are saved

00:06:16,160 --> 00:06:22,400
to when restored from the vm state file

00:06:20,560 --> 00:06:23,919
the values of the descriptors are saved

00:06:22,400 --> 00:06:26,880
in the environment

00:06:23,919 --> 00:06:27,520
the box on right shows all the fds saved

00:06:26,880 --> 00:06:30,800
for one

00:06:27,520 --> 00:06:31,199
vfio device the name is not pretty but

00:06:30,800 --> 00:06:33,039
they

00:06:31,199 --> 00:06:35,039
completely describe and identify the

00:06:33,039 --> 00:06:37,280
descriptor

00:06:35,039 --> 00:06:39,520
for example the highlighted entry is the

00:06:37,280 --> 00:06:42,479
kvm irq chip notifier

00:06:39,520 --> 00:06:46,479
for vector 0 of device 3a colon 10

00:06:42,479 --> 00:06:46,479
descriptor number 163

00:06:46,720 --> 00:06:51,520
after exec q mu attaches the via file

00:06:49,280 --> 00:06:53,440
descriptors to the new kvm instance

00:06:51,520 --> 00:06:54,960
using the appropriate eye octals shown

00:06:53,440 --> 00:06:56,960
here

00:06:54,960 --> 00:06:59,440
the required code changes to achieve all

00:06:56,960 --> 00:07:02,000
this are surprisingly small

00:06:59,440 --> 00:07:03,120
wherever a vfi descriptor is created we

00:07:02,000 --> 00:07:05,680
check the environment

00:07:03,120 --> 00:07:08,080
and use that value instead then execute

00:07:05,680 --> 00:07:10,560
the existing code paths

00:07:08,080 --> 00:07:12,319
we remember that the fda is reused and

00:07:10,560 --> 00:07:15,599
skip in any ioctals that would

00:07:12,319 --> 00:07:15,599
reconfigure the device

00:07:15,759 --> 00:07:19,199
we have tested this with interrupts

00:07:17,599 --> 00:07:22,000
delivered to qmu

00:07:19,199 --> 00:07:23,840
to the kernel kvm irq chip and posted

00:07:22,000 --> 00:07:27,680
directly to the guest

00:07:23,840 --> 00:07:27,680
all work robustly across the update

00:07:28,840 --> 00:07:31,840
operation

00:07:34,319 --> 00:07:38,319
to handle other qmu device state we

00:07:36,560 --> 00:07:40,160
leverage the vm state framework that

00:07:38,319 --> 00:07:42,000
live migration uses

00:07:40,160 --> 00:07:44,000
we modify the code so that the save and

00:07:42,000 --> 00:07:46,479
restore handlers can be selected based

00:07:44,000 --> 00:07:49,440
on the operation such as cpr versus

00:07:46,479 --> 00:07:51,520
snapshot versus migration

00:07:49,440 --> 00:07:52,639
objects are serialized to an ordinary

00:07:51,520 --> 00:07:54,800
file

00:07:52,639 --> 00:07:57,120
not a socket like live migration and not

00:07:54,800 --> 00:07:59,039
to a qcow snapshot

00:07:57,120 --> 00:08:02,319
this allows us to support a variety of

00:07:59,039 --> 00:08:04,160
image formats and guest boot devices

00:08:02,319 --> 00:08:05,440
however because the block devices are

00:08:04,160 --> 00:08:07,280
not snapshotted

00:08:05,440 --> 00:08:10,240
one must not modify the blocks between

00:08:07,280 --> 00:08:12,319
the save and the restore

00:08:10,240 --> 00:08:13,599
the save file is small less than one

00:08:12,319 --> 00:08:15,520
megabyte

00:08:13,599 --> 00:08:18,960
writing the file is very fast adding

00:08:15,520 --> 00:08:18,960
little to the guest pause time

00:08:22,639 --> 00:08:27,199
the cpr save command puts it all

00:08:24,560 --> 00:08:29,440
together you specify the file for saving

00:08:27,199 --> 00:08:31,440
state and a mode argument

00:08:29,440 --> 00:08:34,800
the mode is the keyword restart for live

00:08:31,440 --> 00:08:37,760
update i'll show another mode shortly

00:08:34,800 --> 00:08:40,560
qmu pauses the guest fee cpus and saves

00:08:37,760 --> 00:08:42,640
device state to the file it calls m

00:08:40,560 --> 00:08:46,959
advise due exec for all the ram segments

00:08:42,640 --> 00:08:48,800
such as main memory video ram and others

00:08:46,959 --> 00:08:50,880
it clears the close on exec flag for

00:08:48,800 --> 00:08:53,360
vfio and other descriptors and remembers

00:08:50,880 --> 00:08:55,279
their values in the environment

00:08:53,360 --> 00:08:58,480
it destroys the old kvm instance and

00:08:55,279 --> 00:08:58,480
exacts the new qmu

00:08:58,560 --> 00:09:04,560
however if user bin qmu exec exists

00:09:01,680 --> 00:09:06,480
we call that instead a site may provide

00:09:04,560 --> 00:09:07,360
this binary to customize the update

00:09:06,480 --> 00:09:10,080
procedure

00:09:07,360 --> 00:09:11,040
by changing the qmu binary path changing

00:09:10,080 --> 00:09:14,000
the rv

00:09:11,040 --> 00:09:16,080
or modifying the execution environment

00:09:14,000 --> 00:09:18,480
we use it to run qmu in a container

00:09:16,080 --> 00:09:21,120
environment for example

00:09:18,480 --> 00:09:24,240
qmu exec finishes off by executing the

00:09:21,120 --> 00:09:26,560
new qmu binary

00:09:24,240 --> 00:09:27,680
new qmu starts and creates a new kvm

00:09:26,560 --> 00:09:30,240
instance

00:09:27,680 --> 00:09:32,880
it finds and reuses ram segments finds

00:09:30,240 --> 00:09:35,040
and reuses vfio and other descriptors

00:09:32,880 --> 00:09:37,839
and attaches vfio to the new kvm

00:09:35,040 --> 00:09:37,839
instance

00:09:41,360 --> 00:09:46,000
qmu is now in the pre-launch state the

00:09:44,000 --> 00:09:47,600
management layer now has the opportunity

00:09:46,000 --> 00:09:49,600
to send device ad commands

00:09:47,600 --> 00:09:51,040
that supplement the devices defined by

00:09:49,600 --> 00:09:53,360
the rgv

00:09:51,040 --> 00:09:56,320
this is why update is divided into cpr

00:09:53,360 --> 00:09:58,720
save and cpr load phases

00:09:56,320 --> 00:10:00,000
cpr load is fairly simple it loads

00:09:58,720 --> 00:10:03,200
device state from the file

00:10:00,000 --> 00:10:04,800
and continues the vcpus the guest is

00:10:03,200 --> 00:10:06,640
running again controlled by a new

00:10:04,800 --> 00:10:08,880
version of qmu

00:10:06,640 --> 00:10:10,480
the pause time is about 100 milliseconds

00:10:08,880 --> 00:10:11,839
and this on a four year old xenon

00:10:10,480 --> 00:10:14,160
processor

00:10:11,839 --> 00:10:15,839
we've not spent any time profiling this

00:10:14,160 --> 00:10:17,040
and i expect with optimizations and a

00:10:15,839 --> 00:10:20,320
recent processor

00:10:17,040 --> 00:10:20,320
this can be quite a bit faster

00:10:24,480 --> 00:10:28,800
here's an example using the interactive

00:10:26,399 --> 00:10:28,800
monitor

00:10:31,760 --> 00:10:36,399
in window one on the left we start qmu

00:10:34,640 --> 00:10:37,600
the status command shows that the guest

00:10:36,399 --> 00:10:39,920
is running

00:10:37,600 --> 00:10:40,959
in window 2 on the right we use yum to

00:10:39,920 --> 00:10:43,519
update qmu

00:10:40,959 --> 00:10:44,640
on disk this does not affect the running

00:10:43,519 --> 00:10:47,279
qmu process

00:10:44,640 --> 00:10:49,200
and the guest is still live and running

00:10:47,279 --> 00:10:50,160
on the left we issue the cpr save

00:10:49,200 --> 00:10:53,680
command

00:10:50,160 --> 00:10:55,760
it execs the new qmu binary and returns

00:10:53,680 --> 00:10:56,800
status shows the vm is in the pre-launch

00:10:55,760 --> 00:10:59,360
state

00:10:56,800 --> 00:11:03,440
to finish off we issue cpr load the

00:10:59,360 --> 00:11:03,440
guest resumes and the update is complete

00:11:08,160 --> 00:11:15,839
now for a short demonstration

00:11:17,839 --> 00:11:22,160
in this demo i run a script that updates

00:11:20,079 --> 00:11:24,160
qmu in our container environment and

00:11:22,160 --> 00:11:26,079
issues the monitor commands

00:11:24,160 --> 00:11:27,360
the host is on the left and is running a

00:11:26,079 --> 00:11:30,000
single instance with

00:11:27,360 --> 00:11:33,040
qmu version dash one which is shown by

00:11:30,000 --> 00:11:33,040
querying the monitor

00:11:33,279 --> 00:11:36,399
the guest is on the right and i start a

00:11:35,040 --> 00:11:38,079
counting program there

00:11:36,399 --> 00:11:41,120
this will show that the guest is live

00:11:38,079 --> 00:11:41,120
throughout the demonstration

00:11:41,279 --> 00:11:44,880
now i type update commands on the host

00:11:43,680 --> 00:11:48,399
the prepare command

00:11:44,880 --> 00:11:50,639
mounts the new qmu version

00:11:48,399 --> 00:11:51,760
suspend stops the guest and execs the

00:11:50,639 --> 00:11:55,200
new version

00:11:51,760 --> 00:11:58,959
our count stops resume continues the

00:11:55,200 --> 00:11:58,959
guest and our account resumes

00:11:59,040 --> 00:12:04,320
now we are running the qmu dash version

00:12:01,279 --> 00:12:04,320
the update is complete

00:12:04,399 --> 00:12:07,440
now let's do it all with a single

00:12:05,839 --> 00:12:09,519
restart command

00:12:07,440 --> 00:12:11,839
watch the count it barely stops barely

00:12:09,519 --> 00:12:11,839
hiccups

00:12:14,560 --> 00:12:18,160
now let's go back and forth between the

00:12:16,399 --> 00:12:21,040
old new versions repeatedly

00:12:18,160 --> 00:12:22,000
show how robust the update feature is

00:12:21,040 --> 00:12:23,680
for each update

00:12:22,000 --> 00:12:27,839
the guest pause time is measured and

00:12:23,680 --> 00:12:27,839
printed and is about 100 milliseconds

00:12:28,639 --> 00:12:39,839
we are updating back and forth

00:12:30,000 --> 00:12:39,839
continuously but the guest marches on

00:12:46,000 --> 00:12:49,279
i've shown you how to update qmu using

00:12:48,079 --> 00:12:50,720
cpr save

00:12:49,279 --> 00:12:53,600
which implies you must start with a

00:12:50,720 --> 00:12:55,600
recent version that supports the command

00:12:53,600 --> 00:12:57,920
however we can also update a legacy

00:12:55,600 --> 00:13:01,839
version by dynamically injecting code

00:12:57,920 --> 00:13:03,839
that performs the equivalent of cpr save

00:13:01,839 --> 00:13:05,120
the vm save shared object provides the

00:13:03,839 --> 00:13:07,279
injected code

00:13:05,120 --> 00:13:08,160
it accesses qmu data structures and

00:13:07,279 --> 00:13:10,240
globals

00:13:08,160 --> 00:13:12,079
such as the list of ram handlers the

00:13:10,240 --> 00:13:14,800
list of vm state handlers

00:13:12,079 --> 00:13:17,279
character devices and vhost devices just

00:13:14,800 --> 00:13:19,600
to name a few

00:13:17,279 --> 00:13:21,600
however deal open does not resolve the

00:13:19,600 --> 00:13:22,320
address of these globals when vmsave is

00:13:21,600 --> 00:13:24,720
loaded

00:13:22,320 --> 00:13:26,480
because qmu is loaded with the rtld

00:13:24,720 --> 00:13:29,040
local flag

00:13:26,480 --> 00:13:32,480
so we wrote code to find the addresses

00:13:29,040 --> 00:13:34,560
by looking them up in the symbol table

00:13:32,480 --> 00:13:36,240
the code deletes some vm state handlers

00:13:34,560 --> 00:13:37,200
such as those specifically for live

00:13:36,240 --> 00:13:40,560
migration

00:13:37,200 --> 00:13:42,399
and registers a new handler for vfio

00:13:40,560 --> 00:13:45,040
it calls m advised to exec and guest

00:13:42,399 --> 00:13:49,839
memory it finds devices and lists

00:13:45,040 --> 00:13:49,839
and preserves their descriptors

00:13:51,519 --> 00:13:56,000
to invoke the code we hot patch over the

00:13:53,440 --> 00:13:57,760
text of a qmu monitor function by

00:13:56,000 --> 00:13:59,920
writing to proc mem

00:13:57,760 --> 00:14:02,480
we then call the monitor function using

00:13:59,920 --> 00:14:05,040
an already created monitor socket

00:14:02,480 --> 00:14:05,519
the code patch is small a deal opens vm

00:14:05,040 --> 00:14:08,399
save

00:14:05,519 --> 00:14:09,040
and then calls its entry point vmsave

00:14:08,399 --> 00:14:12,639
runs

00:14:09,040 --> 00:14:15,360
saves state and execs the new qmu

00:14:12,639 --> 00:14:16,000
the new qmu includes live update so we

00:14:15,360 --> 00:14:18,480
simply call

00:14:16,000 --> 00:14:20,880
cprload to finish the update and resume

00:14:18,480 --> 00:14:20,880
the guest

00:14:21,120 --> 00:14:25,519
the vmsave library has binary

00:14:23,120 --> 00:14:26,480
dependencies on qmu data structures and

00:14:25,519 --> 00:14:28,560
variables

00:14:26,480 --> 00:14:30,639
so we build a separate vmsave library

00:14:28,560 --> 00:14:34,240
for each legacy qmu version

00:14:30,639 --> 00:14:36,480
indexed by gcc build id we extract the

00:14:34,240 --> 00:14:40,079
build id from the running qmu process

00:14:36,480 --> 00:14:42,399
and inject the matching vm save object

00:14:40,079 --> 00:14:44,000
the technique is fast and reliable the

00:14:42,399 --> 00:14:46,320
guest pause time is roughly

00:14:44,000 --> 00:14:48,079
the same as for cpr save and we've

00:14:46,320 --> 00:14:52,399
successfully tested updates from

00:14:48,079 --> 00:14:59,839
qmu 2.x to 3.x and 3.x to 4.x

00:14:52,399 --> 00:14:59,839
it's pretty cool

00:15:00,639 --> 00:15:04,399
many critical fixes can be applied by

00:15:02,720 --> 00:15:05,920
updating only qmu

00:15:04,399 --> 00:15:08,000
but if you need to update the host

00:15:05,920 --> 00:15:11,279
kernel we have a method for doing so

00:15:08,000 --> 00:15:13,760
in the cpr framework the mode argument

00:15:11,279 --> 00:15:16,720
in this case is reboot

00:15:13,760 --> 00:15:18,959
after cpr save uk exec boot the kernel

00:15:16,720 --> 00:15:20,880
and issue cpr load

00:15:18,959 --> 00:15:22,639
the guest pause time is longer but

00:15:20,880 --> 00:15:23,920
connections from the guest kernel to the

00:15:22,639 --> 00:15:27,040
outside world

00:15:23,920 --> 00:15:28,959
survive the reboot

00:15:27,040 --> 00:15:30,240
the guest ram must be backed by a shared

00:15:28,959 --> 00:15:32,320
memory segment

00:15:30,240 --> 00:15:34,320
the segment is preserved across k exec

00:15:32,320 --> 00:15:36,560
reboot by anthony's pk ram kernel

00:15:34,320 --> 00:15:38,639
patches

00:15:36,560 --> 00:15:41,279
the pages of the segment are visited to

00:15:38,639 --> 00:15:43,199
find the pfns that must be preserved

00:15:41,279 --> 00:15:45,920
the eight by pfns are packed onto free

00:15:43,199 --> 00:15:49,839
pages those pages are linked together

00:15:45,920 --> 00:15:52,000
and the head is passed across the k exec

00:15:49,839 --> 00:15:54,079
early in reboot the pfns are mapped to

00:15:52,000 --> 00:15:56,639
pages and those pages are removed

00:15:54,079 --> 00:15:57,519
from the free list the shmem my note is

00:15:56,639 --> 00:15:59,199
recreated

00:15:57,519 --> 00:16:01,839
and pages are attached to the file's

00:15:59,199 --> 00:16:01,839
address space

00:16:04,399 --> 00:16:08,480
page visitation and reclaim are

00:16:06,160 --> 00:16:10,160
parallelized for speed and hundreds of

00:16:08,480 --> 00:16:11,279
gigabytes can be preserved in less than

00:16:10,160 --> 00:16:13,279
one second

00:16:11,279 --> 00:16:15,839
see the pk ram patch series for more

00:16:13,279 --> 00:16:15,839
details

00:16:16,560 --> 00:16:20,720
we support sri ov devices if the guest

00:16:19,199 --> 00:16:21,759
provides an agent that implements

00:16:20,720 --> 00:16:25,839
suspender ram

00:16:21,759 --> 00:16:27,199
such as qmuga the update sequence starts

00:16:25,839 --> 00:16:30,639
with suspender ram

00:16:27,199 --> 00:16:33,600
followed by cpr save k exec reboot

00:16:30,639 --> 00:16:35,360
cpr load and finishes with a system wake

00:16:33,600 --> 00:16:37,680
up

00:16:35,360 --> 00:16:39,600
on suspender ram the guest device

00:16:37,680 --> 00:16:42,160
drivers flush posted requests

00:16:39,600 --> 00:16:43,920
and reinitialized to a reset state the

00:16:42,160 --> 00:16:45,440
same state reached after the host

00:16:43,920 --> 00:16:47,279
reboots

00:16:45,440 --> 00:16:51,839
thus when the guest resumes the guests

00:16:47,279 --> 00:16:51,839
and the host agree on the state

00:17:02,160 --> 00:17:06,000
that's a wrap on a work to date in the

00:17:04,559 --> 00:17:08,160
future we would like to merge with

00:17:06,000 --> 00:17:10,720
intel's of emm fast restart work

00:17:08,160 --> 00:17:12,799
which keeps sriov devices alive and

00:17:10,720 --> 00:17:15,039
configured across reboot

00:17:12,799 --> 00:17:17,039
this is faster than suspender ram and

00:17:15,039 --> 00:17:18,880
eliminates the guest agent

00:17:17,039 --> 00:17:21,520
see jason's current kvm forum

00:17:18,880 --> 00:17:24,319
presentation

00:17:21,520 --> 00:17:24,959
the m advise do do exec extension works

00:17:24,319 --> 00:17:26,720
great

00:17:24,959 --> 00:17:28,559
but our upstream reviewers are not keen

00:17:26,720 --> 00:17:30,720
on accepting it so i'm pondering

00:17:28,559 --> 00:17:32,960
alternate solutions

00:17:30,720 --> 00:17:35,600
i'm considering standard system calls to

00:17:32,960 --> 00:17:38,799
reattach to guest memory after exec

00:17:35,600 --> 00:17:41,919
such as schmatt or mmap of a mfd

00:17:38,799 --> 00:17:43,039
plus nui octals to tell vfio that the va

00:17:41,919 --> 00:17:46,559
is changing

00:17:43,039 --> 00:17:48,640
stay tuned on that lastly we received

00:17:46,559 --> 00:17:50,000
great feedback from our qmu reviewers in

00:17:48,640 --> 00:17:51,600
version one

00:17:50,000 --> 00:17:55,120
of the live update patches and we're

00:17:51,600 --> 00:17:55,120
busily working on version two

00:17:55,280 --> 00:17:58,720
we very much look forward to making live

00:17:56,960 --> 00:18:00,240
update available to the community

00:17:58,720 --> 00:18:02,799
here are some references to follow our

00:18:00,240 --> 00:18:05,120
work if you're interested

00:18:02,799 --> 00:18:06,640
i hope you've enjoyed this chalk talk

00:18:05,120 --> 00:18:15,600
and thanks for attending

00:18:06,640 --> 00:18:17,679
i'll now take your questions over chad

00:18:15,600 --> 00:18:17,679

YouTube URL: https://www.youtube.com/watch?v=Qujgm-EPyvY


