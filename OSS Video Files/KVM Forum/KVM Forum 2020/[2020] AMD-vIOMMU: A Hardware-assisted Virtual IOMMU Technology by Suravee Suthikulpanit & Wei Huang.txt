Title: [2020] AMD-vIOMMU: A Hardware-assisted Virtual IOMMU Technology by Suravee Suthikulpanit & Wei Huang
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	There have been various usages of IOMMU in virtual machines (VMs), especially for supporting pass-through devices within a VM. Several virtual IOMMU (vIOMMU) solutions have been proposed and implemented, which are mostly done in an emulated fashion. This talk will focus on the technical details of a new hardware-assisted vIOMMU technology introduced in the AMD second-generation EPYC platforms. The goal of this technology is to improve the performance of vIOMMU for pass-through devices. We will discuss how the support is implemented in AMD IOMMU driver for when it is running in the host, as well as how it is being modified to use the v2 I/O page table for DMA-API when running in the guest. As a proof of concept, QEMU is modified to leverage the vIOMMU hardware via a new ioctl interface. This presentation will cover the implementation details and performance results of our initial design.

---

Wei Huang
AMD, Open-Source Contributor

Wei Huang is a member of AMD Server Software Group, with current focus on server OS and x86 virtualization. Wei has contributed to Linux kernel and various open source virtualization projects (Xen, KVM/QEMU, etc.), and presented a number of times at various technical conferences.

Suravee Suthikulpanit
AMD,  Linux Contributor, Open-Source Contributor
Thailand

Suravee Suthikulpanit works for AMD Server Software Group. His work mainly focus on Linux kernel and the open-source virtualization software. Within AMD, Suravee works with the hardware design and performance teams on future feature definitions. Suravee has been a regular contributor to kernel, KVM, and QEMU.
Captions: 
	00:00:08,960 --> 00:00:11,519
thank you for joining today's

00:00:10,559 --> 00:00:14,480
presentation

00:00:11,519 --> 00:00:16,080
my name is hui huang today suavi and i

00:00:14,480 --> 00:00:19,119
are going to talk about

00:00:16,080 --> 00:00:19,920
a new hardware assisted viola memory

00:00:19,119 --> 00:00:23,039
technology

00:00:19,920 --> 00:00:23,039
offered by amd

00:00:24,400 --> 00:00:27,760
here is the agenda for today's

00:00:26,000 --> 00:00:29,439
presentation

00:00:27,760 --> 00:00:30,880
at the beginning i will give a brief

00:00:29,439 --> 00:00:33,920
overview about amd

00:00:30,880 --> 00:00:38,320
iom mu and how does dma remap works

00:00:33,920 --> 00:00:40,879
in this hardware for vio immunosupport

00:00:38,320 --> 00:00:43,120
inside a guest vm there are two types of

00:00:40,879 --> 00:00:43,120
the

00:00:43,440 --> 00:00:47,920
vio menu the first one is qumu emulated

00:00:46,800 --> 00:00:51,760
vr menu

00:00:47,920 --> 00:00:53,520
we also know as software vi or menu

00:00:51,760 --> 00:00:54,800
and the second one is the call of

00:00:53,520 --> 00:00:59,600
today's presentation

00:00:54,800 --> 00:01:02,160
is a hardware assisted amd vio menu

00:00:59,600 --> 00:01:03,280
in the rest of presentation surawe will

00:01:02,160 --> 00:01:06,080
talk

00:01:03,280 --> 00:01:07,680
in details about the hardware changes

00:01:06,080 --> 00:01:11,040
for

00:01:07,680 --> 00:01:11,600
amd vio memo and also the software

00:01:11,040 --> 00:01:13,520
changes

00:01:11,600 --> 00:01:15,360
in different system software component

00:01:13,520 --> 00:01:19,280
we are proposing

00:01:15,360 --> 00:01:22,400
at the end we'll open up the summary and

00:01:19,280 --> 00:01:22,400
for the discussion

00:01:24,640 --> 00:01:31,920
okay so this slide shows you the amd

00:01:28,080 --> 00:01:33,200
iom mu design as you can see on the

00:01:31,920 --> 00:01:36,400
right hand side

00:01:33,200 --> 00:01:39,119
in the diagram the hardware use

00:01:36,400 --> 00:01:42,320
a lot of different data structures for

00:01:39,119 --> 00:01:46,640
the iron manual management

00:01:42,320 --> 00:01:51,920
and the main registers

00:01:46,640 --> 00:01:55,520
are inside for two 4kb mmr region

00:01:51,920 --> 00:01:58,320
the first 4kb contains for example

00:01:55,520 --> 00:01:59,040
base and config register those register

00:01:58,320 --> 00:02:02,320
point to

00:01:59,040 --> 00:02:04,159
the starting address of those data

00:02:02,320 --> 00:02:08,239
structure

00:02:04,159 --> 00:02:10,560
and the second 4kb mmr region contains

00:02:08,239 --> 00:02:13,599
head and tail register

00:02:10,560 --> 00:02:16,959
that point into inside those

00:02:13,599 --> 00:02:18,640
data structure for example the

00:02:16,959 --> 00:02:22,560
head and tail register into command

00:02:18,640 --> 00:02:25,280
buffer and ppr logs

00:02:22,560 --> 00:02:26,720
the main functionality for iom mmu is to

00:02:25,280 --> 00:02:30,400
do the dma remap

00:02:26,720 --> 00:02:31,760
so mdio menu support two type of i o

00:02:30,400 --> 00:02:36,319
page table

00:02:31,760 --> 00:02:39,680
the first one is host io page table this

00:02:36,319 --> 00:02:43,120
is being used currently by

00:02:39,680 --> 00:02:45,599
linux dma api and vfio

00:02:43,120 --> 00:02:47,680
we call this page table v1 page table

00:02:45,599 --> 00:02:50,160
and this table does a translation from

00:02:47,680 --> 00:02:52,160
gpa to spa

00:02:50,160 --> 00:02:55,040
and the second page table is a little

00:02:52,160 --> 00:02:59,360
bit different it's called v2 table

00:02:55,040 --> 00:03:03,040
and it does support the x86

00:02:59,360 --> 00:03:07,360
cpu page table format this page table

00:03:03,040 --> 00:03:07,360
is currently used by linux kfd driver

00:03:10,879 --> 00:03:15,920
so as we know iomu offers a lot of

00:03:13,360 --> 00:03:18,239
benefits

00:03:15,920 --> 00:03:20,720
you can do device pass-through you can

00:03:18,239 --> 00:03:24,239
enhance the io security

00:03:20,720 --> 00:03:28,560
uh by using dma isolation

00:03:24,239 --> 00:03:29,200
so over time end user or customer always

00:03:28,560 --> 00:03:31,360
want

00:03:29,200 --> 00:03:33,440
to enable this feature for their guest

00:03:31,360 --> 00:03:35,840
vm

00:03:33,440 --> 00:03:36,879
to support this feature over the past

00:03:35,840 --> 00:03:39,680
several years

00:03:36,879 --> 00:03:40,560
the community has come up with different

00:03:39,680 --> 00:03:43,120
solutions

00:03:40,560 --> 00:03:44,080
based on different device model for

00:03:43,120 --> 00:03:47,200
example

00:03:44,080 --> 00:03:50,560
we do have intel io mmu

00:03:47,200 --> 00:03:53,360
support in the qemu similarly for arm

00:03:50,560 --> 00:03:54,879
smemu we have the same capability

00:03:53,360 --> 00:03:58,640
capability

00:03:54,879 --> 00:04:00,959
for mdio mmu we do support pass-through

00:03:58,640 --> 00:04:05,120
for the emulator device

00:04:00,959 --> 00:04:08,640
but for the vfi or pci passthrough

00:04:05,120 --> 00:04:11,200
this project is still working progress

00:04:08,640 --> 00:04:14,400
recently we just submit a series of

00:04:11,200 --> 00:04:17,440
patches to enable this feature

00:04:14,400 --> 00:04:22,079
to use this feature it's very simple

00:04:17,440 --> 00:04:25,680
when you start the qemi command you just

00:04:22,079 --> 00:04:29,040
specify amd io manual device and

00:04:25,680 --> 00:04:32,240
add that the vfi or pci host device

00:04:29,040 --> 00:04:35,120
and attach to this mdio memory

00:04:32,240 --> 00:04:35,120
and that should be it

00:04:37,919 --> 00:04:43,199
for parcel device inside a guest vm

00:04:41,040 --> 00:04:44,560
they can actually be classified into two

00:04:43,199 --> 00:04:48,479
categories

00:04:44,560 --> 00:04:51,120
in the next i will talk about why we

00:04:48,479 --> 00:04:52,400
differentiate them from each other the

00:04:51,120 --> 00:04:55,440
first category

00:04:52,400 --> 00:04:56,639
is called emulated pci device for

00:04:55,440 --> 00:05:00,160
example

00:04:56,639 --> 00:05:04,639
like intel e1000 nic

00:05:00,160 --> 00:05:07,919
right inside your guest vm

00:05:04,639 --> 00:05:08,639
and the second one is pci password

00:05:07,919 --> 00:05:12,000
device

00:05:08,639 --> 00:05:13,680
like vfio password device for example

00:05:12,000 --> 00:05:16,160
you want to pass through a 40 gig

00:05:13,680 --> 00:05:16,960
nic into your guest vm and in the guest

00:05:16,160 --> 00:05:19,919
vm you

00:05:16,960 --> 00:05:21,759
want this device be managed by amd iom

00:05:19,919 --> 00:05:25,039
emu driver

00:05:21,759 --> 00:05:28,400
both device are supported

00:05:25,039 --> 00:05:31,039
and for the dma remapping and interrupt

00:05:28,400 --> 00:05:31,039
remapping

00:05:31,120 --> 00:05:36,800
next one but

00:05:34,240 --> 00:05:37,440
the implementation for dma remapping

00:05:36,800 --> 00:05:41,280
actually

00:05:37,440 --> 00:05:43,280
a little bit different as you can see on

00:05:41,280 --> 00:05:46,479
the right hand side

00:05:43,280 --> 00:05:47,360
for the emulated pci device all the dma

00:05:46,479 --> 00:05:50,720
coming up

00:05:47,360 --> 00:05:53,840
will be managed by emulated iomu

00:05:50,720 --> 00:05:56,319
and this immigration irma maintained

00:05:53,840 --> 00:05:57,120
an emulated host table that does a

00:05:56,319 --> 00:06:00,560
translation

00:05:57,120 --> 00:06:04,080
from guest iova to gpa

00:06:00,560 --> 00:06:07,440
but for pci password device because

00:06:04,080 --> 00:06:10,800
this device actually is managed by host

00:06:07,440 --> 00:06:13,199
iomu the real io memory hardware

00:06:10,800 --> 00:06:14,400
the software needs to create a shutter

00:06:13,199 --> 00:06:18,840
host table

00:06:14,400 --> 00:06:21,759
that does a translation from gas io to

00:06:18,840 --> 00:06:23,759
spa obviously

00:06:21,759 --> 00:06:26,400
this shuttle host table maintenance

00:06:23,759 --> 00:06:28,960
creates some performance hit

00:06:26,400 --> 00:06:31,199
imagine that anytime the guest io memory

00:06:28,960 --> 00:06:34,319
driver updates its page table

00:06:31,199 --> 00:06:35,280
we need to reflect that change in the

00:06:34,319 --> 00:06:38,639
whole

00:06:35,280 --> 00:06:40,639
shuttle host table used by the real

00:06:38,639 --> 00:06:43,440
hardware

00:06:40,639 --> 00:06:44,160
in the meanwhile there are other

00:06:43,440 --> 00:06:48,319
performance

00:06:44,160 --> 00:06:54,000
hit for example we need to emulate

00:06:48,319 --> 00:06:54,000
guest iom driver access to sma mio

00:06:54,080 --> 00:06:57,919
on top of that uh the ioma mu command

00:06:56,800 --> 00:07:01,919
processing

00:06:57,919 --> 00:07:04,319
and even log event and ppr log access

00:07:01,919 --> 00:07:07,599
also needs to be emulated

00:07:04,319 --> 00:07:09,440
combining those things together we can

00:07:07,599 --> 00:07:12,720
imagine that the performance for pci

00:07:09,440 --> 00:07:12,720
password is not great

00:07:13,440 --> 00:07:20,160
next one so

00:07:16,880 --> 00:07:23,039
to solve this problem amd come up with

00:07:20,160 --> 00:07:23,919
a new hardware based approach it's

00:07:23,039 --> 00:07:27,840
called

00:07:23,919 --> 00:07:30,720
hardware vio mmu this hardware new

00:07:27,840 --> 00:07:32,880
feature tries to solve the problem

00:07:30,720 --> 00:07:34,880
we just mentioned in the last slide for

00:07:32,880 --> 00:07:37,440
example for shadowholds table

00:07:34,880 --> 00:07:38,000
we want to utilize the nested io page

00:07:37,440 --> 00:07:41,520
table

00:07:38,000 --> 00:07:43,840
instead and for the mmr register access

00:07:41,520 --> 00:07:46,800
we want to allow guests to directly

00:07:43,840 --> 00:07:50,560
access those registers

00:07:46,800 --> 00:07:53,280
in the meanwhile the

00:07:50,560 --> 00:07:54,479
hardware the new feature create a

00:07:53,280 --> 00:07:56,960
private

00:07:54,479 --> 00:08:00,639
mapping allow the hardware to access

00:07:56,960 --> 00:08:05,039
guest iomu command directory

00:08:00,639 --> 00:08:07,440
so those new hardware features

00:08:05,039 --> 00:08:08,639
will solve the problem we just mentioned

00:08:07,440 --> 00:08:12,400
in last slides

00:08:08,639 --> 00:08:13,840
and well beneficial for end users who

00:08:12,400 --> 00:08:17,199
want to pass through the

00:08:13,840 --> 00:08:19,680
native device into the gas sphere

00:08:17,199 --> 00:08:20,400
there's ongoing development effort

00:08:19,680 --> 00:08:22,720
around

00:08:20,400 --> 00:08:23,759
different areas in the system software

00:08:22,720 --> 00:08:25,520
components

00:08:23,759 --> 00:08:28,479
which will be covered by soravi in

00:08:25,520 --> 00:08:28,479
detail later on

00:08:31,680 --> 00:08:37,360
i want to go back a little bit about

00:08:33,519 --> 00:08:41,680
nested io page table support

00:08:37,360 --> 00:08:43,599
so this nested io page support

00:08:41,680 --> 00:08:44,959
is different from shutter host page

00:08:43,599 --> 00:08:48,000
table support

00:08:44,959 --> 00:08:49,920
uh i just mentioned in the last two

00:08:48,000 --> 00:08:53,680
slides

00:08:49,920 --> 00:08:56,720
in this new design the gas

00:08:53,680 --> 00:08:59,040
iommu driver will use the

00:08:56,720 --> 00:09:03,040
v2 table the guest table that does the

00:08:59,040 --> 00:09:06,000
translation from guest iova to gpa

00:09:03,040 --> 00:09:08,240
and the host io mmu actually will use a

00:09:06,000 --> 00:09:09,040
host table that does a translation from

00:09:08,240 --> 00:09:12,560
gpa

00:09:09,040 --> 00:09:15,040
and spa so you can imagine that this

00:09:12,560 --> 00:09:17,440
nested io page table is very similar to

00:09:15,040 --> 00:09:20,880
cpu side of nested paging

00:09:17,440 --> 00:09:22,399
so we expect this will be able to solve

00:09:20,880 --> 00:09:28,080
some performance

00:09:22,399 --> 00:09:28,080
hit we mentioned in the previous slides

00:09:30,160 --> 00:09:35,920
now putting these two design together we

00:09:33,200 --> 00:09:38,080
create a hybrid system model

00:09:35,920 --> 00:09:40,320
depending what kind of pass-through

00:09:38,080 --> 00:09:43,600
device you're going to use

00:09:40,320 --> 00:09:45,760
if you want to use emulated pci device

00:09:43,600 --> 00:09:47,120
we will go back to use the current

00:09:45,760 --> 00:09:50,480
software-based

00:09:47,120 --> 00:09:51,360
approach the emulated iomu that goes dma

00:09:50,480 --> 00:09:54,160
goes through the

00:09:51,360 --> 00:09:57,040
emulated host table as shown on the

00:09:54,160 --> 00:10:00,160
right hand side the top part

00:09:57,040 --> 00:10:01,040
if end user wants to pass through a pci

00:10:00,160 --> 00:10:03,519
device

00:10:01,040 --> 00:10:05,519
then they will use a nested io page

00:10:03,519 --> 00:10:09,920
table

00:10:05,519 --> 00:10:13,839
and that requires adding a new

00:10:09,920 --> 00:10:13,839
amd vi or memory device model

00:10:13,920 --> 00:10:19,040
so by far i just give you a brief

00:10:16,640 --> 00:10:21,279
introduction about the amdio menu and

00:10:19,040 --> 00:10:23,760
the hardware vi or memo

00:10:21,279 --> 00:10:26,079
the rest of presentations ravi will talk

00:10:23,760 --> 00:10:29,120
about the detailed hardware design

00:10:26,079 --> 00:10:31,360
and the changes proposed in different

00:10:29,120 --> 00:10:33,839
system software component

00:10:31,360 --> 00:10:33,839
so are we

00:10:34,560 --> 00:10:40,640
hi so in the next section i will start

00:10:38,560 --> 00:10:44,800
discussing the detail of the changes for

00:10:40,640 --> 00:10:44,800
the hardware assisted vio menu feature

00:10:45,680 --> 00:10:50,079
starting with the hardware changes we're

00:10:47,920 --> 00:10:53,120
introducing the iomu

00:10:50,079 --> 00:10:53,440
private address space this address space

00:10:53,120 --> 00:10:57,040
is

00:10:53,440 --> 00:11:00,160
used by the io memo hardware to access

00:10:57,040 --> 00:11:01,440
per guest iomu data structures on the

00:11:00,160 --> 00:11:04,240
left hand side

00:11:01,440 --> 00:11:06,399
there's a diagram showing the layout of

00:11:04,240 --> 00:11:08,959
the private address space

00:11:06,399 --> 00:11:10,240
and you can see starting from the top we

00:11:08,959 --> 00:11:13,440
have the event

00:11:10,240 --> 00:11:17,279
and ppr lock the command buffer and the

00:11:13,440 --> 00:11:21,600
guest mmio registers at the bottom

00:11:17,279 --> 00:11:24,000
also it is used to access the viomu

00:11:21,600 --> 00:11:26,399
specific data structures which are the

00:11:24,000 --> 00:11:28,959
domain id mapping table

00:11:26,399 --> 00:11:30,320
that maps the host domain id to guest

00:11:28,959 --> 00:11:32,160
domain id

00:11:30,320 --> 00:11:34,000
and the device id mapping table that

00:11:32,160 --> 00:11:36,959
maps the host device id

00:11:34,000 --> 00:11:38,399
to the guest device id and this is for

00:11:36,959 --> 00:11:42,240
the password device

00:11:38,399 --> 00:11:45,920
the pci password that used the vfio

00:11:42,240 --> 00:11:49,200
also another data structure is the

00:11:45,920 --> 00:11:51,600
command buffer dirty status table

00:11:49,200 --> 00:11:53,360
and all the mapping here is being done

00:11:51,600 --> 00:11:57,040
using the

00:11:53,360 --> 00:12:00,320
iomu v1 page table that maps the private

00:11:57,040 --> 00:12:04,959
address to the system physical address

00:12:00,320 --> 00:12:04,959
and it can support up to 64k vms

00:12:06,399 --> 00:12:10,639
next hardware changes are the

00:12:08,720 --> 00:12:14,880
introduction of

00:12:10,639 --> 00:12:17,680
the vf and vf control mmio bars

00:12:14,880 --> 00:12:19,200
so the bars are used mainly by the

00:12:17,680 --> 00:12:22,560
hypervisor to access

00:12:19,200 --> 00:12:25,440
the per gas mmi oh registers

00:12:22,560 --> 00:12:25,920
as we mentioned earlier we have two set

00:12:25,440 --> 00:12:29,360
of

00:12:25,920 --> 00:12:32,000
mmio registers one is used for control

00:12:29,360 --> 00:12:33,600
and one is and another one is used for

00:12:32,000 --> 00:12:36,800
hit and till pointers of

00:12:33,600 --> 00:12:39,519
the data structure inside io memo

00:12:36,800 --> 00:12:41,120
so the control one is is basically

00:12:39,519 --> 00:12:44,399
accessed via the vf control

00:12:41,120 --> 00:12:49,200
mmo bar the pointers ones

00:12:44,399 --> 00:12:50,959
are accessed using the vf mmo bar

00:12:49,200 --> 00:12:53,680
and the diagram on the right hand side

00:12:50,959 --> 00:12:56,399
just show the breakdown of the

00:12:53,680 --> 00:12:56,720
the regions of the bar you can see that

00:12:56,399 --> 00:13:00,880
it

00:12:56,720 --> 00:13:03,920
split into different 4k regions

00:13:00,880 --> 00:13:04,839
indexed by the guest id same for dbf

00:13:03,920 --> 00:13:08,720
control

00:13:04,839 --> 00:13:10,320
mmio so

00:13:08,720 --> 00:13:11,839
next hardware change are the

00:13:10,320 --> 00:13:16,160
introduction of the new

00:13:11,839 --> 00:13:16,160
iomu commands and events

00:13:16,399 --> 00:13:22,480
for the event typically when an iomu

00:13:19,519 --> 00:13:23,680
encounter an error it will generate

00:13:22,480 --> 00:13:28,160
event locks

00:13:23,680 --> 00:13:31,680
into the event buffer

00:13:28,160 --> 00:13:33,760
and in this case when we have the guest

00:13:31,680 --> 00:13:35,680
iomu

00:13:33,760 --> 00:13:36,800
the errors inside the guest will

00:13:35,680 --> 00:13:41,360
actually be locked

00:13:36,800 --> 00:13:43,440
in the host iomu so

00:13:41,360 --> 00:13:45,199
to be able to do in order to be able to

00:13:43,440 --> 00:13:48,000
identify that the event

00:13:45,199 --> 00:13:48,800
belongs to the guest we're introducing

00:13:48,000 --> 00:13:52,000
bit fields

00:13:48,800 --> 00:13:53,760
for the existing i o mmu event and it

00:13:52,000 --> 00:13:57,199
listed here

00:13:53,760 --> 00:13:57,199
um so

00:13:57,360 --> 00:14:02,880
next will be the i o maybe host driver

00:14:00,639 --> 00:14:05,519
will process the event lock

00:14:02,880 --> 00:14:06,399
and if it wants to inject that event

00:14:05,519 --> 00:14:09,199
into the guest

00:14:06,399 --> 00:14:10,880
it can do so using the insert guest

00:14:09,199 --> 00:14:14,079
event command

00:14:10,880 --> 00:14:15,519
which will place the event into the

00:14:14,079 --> 00:14:19,040
guest

00:14:15,519 --> 00:14:22,720
event lock and

00:14:19,040 --> 00:14:25,279
another scenario is when we run into

00:14:22,720 --> 00:14:26,480
errors that related to vio mu

00:14:25,279 --> 00:14:29,040
specifically

00:14:26,480 --> 00:14:31,279
the hardware will lock the v i o m u

00:14:29,040 --> 00:14:34,639
highway error event

00:14:31,279 --> 00:14:35,360
and usually this will cause the guests

00:14:34,639 --> 00:14:38,560
to

00:14:35,360 --> 00:14:40,480
to fail and when we try to

00:14:38,560 --> 00:14:42,480
re-initialize and recover from that

00:14:40,480 --> 00:14:45,680
failure

00:14:42,480 --> 00:14:49,519
there's a command to reset the vmmio

00:14:45,680 --> 00:14:52,560
um sorry there's a command

00:14:49,519 --> 00:14:58,000
called reset vmmio that will reset the

00:14:52,560 --> 00:15:01,199
guess mmo registers

00:14:58,000 --> 00:15:05,120
next are changes to the

00:15:01,199 --> 00:15:07,199
host iommu driver starting from the boot

00:15:05,120 --> 00:15:09,680
time initialization

00:15:07,199 --> 00:15:12,160
first we add the logic to detect and

00:15:09,680 --> 00:15:13,600
enable the vio memo feature in the

00:15:12,160 --> 00:15:17,279
hardware

00:15:13,600 --> 00:15:20,800
then we set up the io menu v1 page table

00:15:17,279 --> 00:15:20,800
for the private address mapping

00:15:22,240 --> 00:15:28,240
the first part also

00:15:25,360 --> 00:15:29,040
require a locating map i'm sorry

00:15:28,240 --> 00:15:32,399
allocating

00:15:29,040 --> 00:15:34,959
and map the the tables that listed here

00:15:32,399 --> 00:15:36,800
we have the domain id table device id

00:15:34,959 --> 00:15:40,240
table and the command

00:15:36,800 --> 00:15:43,279
buffer dirty bit um status table

00:15:40,240 --> 00:15:45,839
and those those are listed um on the

00:15:43,279 --> 00:15:48,399
right hand side here for the boot time

00:15:45,839 --> 00:15:49,040
then we have the per vm initialization

00:15:48,399 --> 00:15:53,600
code

00:15:49,040 --> 00:15:57,199
that is going to be used when we

00:15:53,600 --> 00:16:00,399
launch the vm and basically it will

00:15:57,199 --> 00:16:01,279
needs to create mapping for the private

00:16:00,399 --> 00:16:04,480
address

00:16:01,279 --> 00:16:07,040
to the spa mapping and that's

00:16:04,480 --> 00:16:09,920
basically the black arrows that shows

00:16:07,040 --> 00:16:10,880
here for event in vpr lock for command

00:16:09,920 --> 00:16:13,920
buffer

00:16:10,880 --> 00:16:16,639
for guest mmio registers

00:16:13,920 --> 00:16:17,920
also we need to do host to guest mapping

00:16:16,639 --> 00:16:20,959
for the device id

00:16:17,920 --> 00:16:22,320
and the domain id and we also need to

00:16:20,959 --> 00:16:24,880
trap

00:16:22,320 --> 00:16:26,639
set up for trapping when the guests try

00:16:24,880 --> 00:16:29,040
to access the first 4k

00:16:26,639 --> 00:16:30,480
of the mmo mmo region which is the

00:16:29,040 --> 00:16:33,600
control regions

00:16:30,480 --> 00:16:33,600
of the mi mio

00:16:33,759 --> 00:16:36,880
also we need to add code for supporting

00:16:35,920 --> 00:16:40,720
new iom

00:16:36,880 --> 00:16:40,720
iom commands and events

00:16:41,759 --> 00:16:49,519
so the next part are changes to the qemu

00:16:46,240 --> 00:16:54,000
basically the change are introducing

00:16:49,519 --> 00:16:56,160
the new hardware vio memory device model

00:16:54,000 --> 00:16:57,759
which is different than the current

00:16:56,160 --> 00:17:00,800
software emulated

00:16:57,759 --> 00:17:02,240
vio memory model so on the right hand

00:17:00,800 --> 00:17:05,439
side shows an example

00:17:02,240 --> 00:17:08,480
of a guest vm where we pass through

00:17:05,439 --> 00:17:11,600
three next devices um

00:17:08,480 --> 00:17:13,199
nick one and nick zero is part of the i

00:17:11,600 --> 00:17:15,760
o m u zero

00:17:13,199 --> 00:17:16,480
and nix four is actually on i o memory

00:17:15,760 --> 00:17:18,240
one

00:17:16,480 --> 00:17:20,240
so when we pass through these three

00:17:18,240 --> 00:17:23,439
nicks into a same gas

00:17:20,240 --> 00:17:27,679
we actually need to create two viomu

00:17:23,439 --> 00:17:30,160
instances and associate them accordingly

00:17:27,679 --> 00:17:30,880
because the hardware has to be able to

00:17:30,160 --> 00:17:32,880
virtualize

00:17:30,880 --> 00:17:34,880
all the command buffers and all the data

00:17:32,880 --> 00:17:39,679
structure belongs to

00:17:34,880 --> 00:17:42,799
um that that the nic is associated to

00:17:39,679 --> 00:17:44,320
so this requires changes to several

00:17:42,799 --> 00:17:46,880
things in qmu

00:17:44,320 --> 00:17:48,240
the first thing is we need to be able to

00:17:46,880 --> 00:17:51,280
specify more than one

00:17:48,240 --> 00:17:53,520
i o mu in a guess currently with the

00:17:51,280 --> 00:17:55,120
emulated i o menu we only can support

00:17:53,520 --> 00:17:58,400
one

00:17:55,120 --> 00:18:00,160
we also need to be able to specify pci

00:17:58,400 --> 00:18:03,600
topology for the vm and

00:18:00,160 --> 00:18:04,559
associative associativity uh with the

00:18:03,600 --> 00:18:06,720
device

00:18:04,559 --> 00:18:07,760
for example at the bottom here shows the

00:18:06,720 --> 00:18:11,039
command

00:18:07,760 --> 00:18:14,480
for qemu to launch the guest with

00:18:11,039 --> 00:18:15,520
two vio memo the red one is for the i o

00:18:14,480 --> 00:18:20,240
menu zero

00:18:15,520 --> 00:18:20,240
and the green one is for the i o m u one

00:18:23,440 --> 00:18:26,960
and to be able to support that we also

00:18:26,000 --> 00:18:30,080
introduced

00:18:26,960 --> 00:18:33,280
new device fs and iocto interface

00:18:30,080 --> 00:18:34,720
the new device of this is called amd vio

00:18:33,280 --> 00:18:38,000
mmu

00:18:34,720 --> 00:18:41,360
and this is implemented by

00:18:38,000 --> 00:18:45,039
the iommu host driver also

00:18:41,360 --> 00:18:46,720
the new um vio menu specific iot

00:18:45,039 --> 00:18:48,720
interface listed here

00:18:46,720 --> 00:18:50,960
to support all the operations that's

00:18:48,720 --> 00:18:54,080
required for setting up vm's

00:18:50,960 --> 00:18:56,799
device domain and all the memory

00:18:54,080 --> 00:18:59,200
mmio access that's that's needed to be

00:18:56,799 --> 00:18:59,200
trapped

00:19:01,760 --> 00:19:06,880
the last part is the change for the

00:19:04,640 --> 00:19:10,720
guest i o menu driver

00:19:06,880 --> 00:19:13,760
as we mentioned we now tr

00:19:10,720 --> 00:19:17,360
making use of the nested io page table

00:19:13,760 --> 00:19:18,960
which required the guest to be using the

00:19:17,360 --> 00:19:22,960
v2 table

00:19:18,960 --> 00:19:26,000
to do the g the guess iova

00:19:22,960 --> 00:19:28,640
translation to gpa

00:19:26,000 --> 00:19:29,520
because the first table or the sorry the

00:19:28,640 --> 00:19:32,320
v1 table

00:19:29,520 --> 00:19:34,000
is already being used by the vfio to do

00:19:32,320 --> 00:19:37,440
gpa to spa

00:19:34,000 --> 00:19:38,640
the problem is currently for io memory

00:19:37,440 --> 00:19:42,880
driver

00:19:38,640 --> 00:19:45,919
we only support v1 table for dma api

00:19:42,880 --> 00:19:48,799
so we need to make some changes here

00:19:45,919 --> 00:19:50,720
for the guest driver and the changes are

00:19:48,799 --> 00:19:53,200
split into two part

00:19:50,720 --> 00:19:55,679
first is to refactor the current code to

00:19:53,200 --> 00:19:57,200
use the generic io page table framework

00:19:55,679 --> 00:19:59,600
that has been introduced

00:19:57,200 --> 00:20:01,280
and currently used by arm the code has

00:19:59,600 --> 00:20:02,480
been submitted upstream already for

00:20:01,280 --> 00:20:04,640
review

00:20:02,480 --> 00:20:06,720
next part will be adding support for the

00:20:04,640 --> 00:20:09,679
page table

00:20:06,720 --> 00:20:12,400
sorry for the v2 page table for dma api

00:20:09,679 --> 00:20:14,799
and it's also going to be using the same

00:20:12,400 --> 00:20:17,360
generic io page table framework this is

00:20:14,799 --> 00:20:21,280
work in progress

00:20:17,360 --> 00:20:24,480
so to summary so far we've been

00:20:21,280 --> 00:20:26,720
comparing the software versus the

00:20:24,480 --> 00:20:30,960
hardware vio memu

00:20:26,720 --> 00:20:34,960
and try to show how the hardware vi menu

00:20:30,960 --> 00:20:37,440
supposed to help improve performance

00:20:34,960 --> 00:20:38,559
for the guest i o menu for the pass

00:20:37,440 --> 00:20:42,159
through device

00:20:38,559 --> 00:20:43,679
by instead of using the host page table

00:20:42,159 --> 00:20:45,440
we're now going to be using the nested

00:20:43,679 --> 00:20:48,720
io page table

00:20:45,440 --> 00:20:49,520
and now guests can directly access mmo

00:20:48,720 --> 00:20:51,280
register

00:20:49,520 --> 00:20:53,120
instead of having to be emulated by the

00:20:51,280 --> 00:20:56,480
hypervisor

00:20:53,120 --> 00:20:58,159
um last part is all the emulation that

00:20:56,480 --> 00:21:00,480
needs to be done for command buffer

00:20:58,159 --> 00:21:05,039
event buffer and ppr buffer

00:21:00,480 --> 00:21:09,280
now it's being escalated by the hardware

00:21:05,039 --> 00:21:12,159
so for the for the end we would like to

00:21:09,280 --> 00:21:13,760
um start some discussion with the

00:21:12,159 --> 00:21:18,080
audience

00:21:13,760 --> 00:21:19,679
around a few topics here first is

00:21:18,080 --> 00:21:22,559
we would like to get some feedback on

00:21:19,679 --> 00:21:25,600
the new hardware vio menu device model

00:21:22,559 --> 00:21:28,880
that we are proposing

00:21:25,600 --> 00:21:31,039
second part is the hybrid vi immune

00:21:28,880 --> 00:21:33,840
system model where we have both

00:21:31,039 --> 00:21:36,400
software and hardware vm viomu in the

00:21:33,840 --> 00:21:36,400
same guest

00:21:36,880 --> 00:21:43,919
how that will scale and how would that

00:21:41,039 --> 00:21:44,960
be included in the qmu are there is pros

00:21:43,919 --> 00:21:46,559
and cons we would like to get some

00:21:44,960 --> 00:21:49,679
feedback on that one

00:21:46,559 --> 00:21:51,600
next one is the i also interface design

00:21:49,679 --> 00:21:54,159
because we're actually using a new a

00:21:51,600 --> 00:21:56,400
brand new iocto interface

00:21:54,159 --> 00:21:58,400
there were some discussion on whether we

00:21:56,400 --> 00:22:02,080
should extending the existing

00:21:58,400 --> 00:22:03,919
vfio iota interface or not

00:22:02,080 --> 00:22:05,919
and for the last part we would like to

00:22:03,919 --> 00:22:06,720
get some feedback on additional usage

00:22:05,919 --> 00:22:08,880
models

00:22:06,720 --> 00:22:10,799
based on the proposed design that we are

00:22:08,880 --> 00:22:13,840
that we have presented

00:22:10,799 --> 00:22:13,840

YouTube URL: https://www.youtube.com/watch?v=KlBgB4br1HM


