Title: [2020] KVM Live Upgrade with Properly Handling of Passthrough Devices by Zhimin Feng
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	VMM live upgrade is an emerging approach to upgrade the VMM without the host shutdown. There are several implementations of live upgrade for KVM. However, none of them seems handling the passthrough devices flawlessly. In this talk, we will analyze the requirements for the passthrough devices handling, and present how we follow those requirements to properly handle passthrough devices in our KVM live upgrade implementation. In addition, we also optimize the startup and suspend of VM, Our experiment shows that the total downtime is 13ms for VMM live upgrade(VM has 8 virtual CPUSs, 8GB memory, 1 disk and 1 network card.)

---

Zhimin Feng
ByteDance, Software Engineer

I am now working for ByteDance., currently focusing on QEMU/Virtualization related projects.
Captions: 
	00:00:06,560 --> 00:00:11,040
hello

00:00:07,520 --> 00:00:15,200
my name is funsumi i'm now working

00:00:11,040 --> 00:00:18,320
for patterns and currently

00:00:15,200 --> 00:00:22,240
folks or what's like this late

00:00:18,320 --> 00:00:25,519
project today i will share my topic

00:00:22,240 --> 00:00:29,679
kvm live upgrade with

00:00:25,519 --> 00:00:33,200
properly hiding of a pass-through device

00:00:29,679 --> 00:00:37,840
next i will need to introduce it

00:00:33,200 --> 00:00:37,840
from the four aspects

00:00:38,879 --> 00:00:42,239
with the development of technology the

00:00:41,600 --> 00:00:44,879
vm

00:00:42,239 --> 00:00:46,320
has to be frequently updated and

00:00:44,879 --> 00:00:49,920
restarted to add

00:00:46,320 --> 00:00:53,199
security pads and new features

00:00:49,920 --> 00:00:57,039
there are two existing live

00:00:53,199 --> 00:01:00,800
live update methods to improve the cloud

00:00:57,039 --> 00:01:04,559
availability kvm live pets

00:01:00,800 --> 00:01:08,400
and virtual machine live migration

00:01:04,559 --> 00:01:12,000
however labels has a serious

00:01:08,400 --> 00:01:15,439
drawbacks kernel live pads cannot

00:01:12,000 --> 00:01:19,600
handle complex change for example

00:01:15,439 --> 00:01:22,880
change to persistence data structure

00:01:19,600 --> 00:01:24,960
vm live magnesium cannot handle the

00:01:22,880 --> 00:01:28,080
pass-through devices

00:01:24,960 --> 00:01:31,439
and it may be

00:01:28,080 --> 00:01:34,640
incur unacceptable a lot less

00:01:31,439 --> 00:01:38,079
because uh life magazine needs to

00:01:34,640 --> 00:01:41,680
copy memory formula or cumule to

00:01:38,079 --> 00:01:45,600
new cumule in this topic

00:01:41,680 --> 00:01:49,040
i will introduce our live upgrade method

00:01:45,600 --> 00:01:52,840
we can probably update the kvm

00:01:49,040 --> 00:01:55,680
and the qmu without interrupt customer

00:01:52,840 --> 00:02:00,159
vms

00:01:55,680 --> 00:02:04,000
very difficult for vm live upgrade

00:02:00,159 --> 00:02:04,320
how to handle the password device during

00:02:04,000 --> 00:02:08,000
the

00:02:04,320 --> 00:02:11,920
live upgrade and minimized

00:02:08,000 --> 00:02:15,599
service downtime is a major

00:02:11,920 --> 00:02:19,120
concern of cloud providers

00:02:15,599 --> 00:02:27,520
in this talk we will analyze the

00:02:19,120 --> 00:02:30,400
requirements for the foreign

00:02:27,520 --> 00:02:31,200
we how we how we follow those

00:02:30,400 --> 00:02:34,879
requirements

00:02:31,200 --> 00:02:38,519
to properly handle pass-through devices

00:02:34,879 --> 00:02:41,519
in our kvm live upgrade

00:02:38,519 --> 00:02:45,280
implementation in this

00:02:41,519 --> 00:02:48,560
we also optimize the startup

00:02:45,280 --> 00:02:51,680
and suspend of vm to

00:02:48,560 --> 00:02:54,480
decrease the door time during the

00:02:51,680 --> 00:02:54,480
live upgrade

00:02:55,680 --> 00:03:02,319
this is the framework for live

00:02:58,959 --> 00:03:05,599
upgrade in order to live

00:03:02,319 --> 00:03:08,959
upgrade the kvm we modify

00:03:05,599 --> 00:03:12,480
kvm module and allow it to be

00:03:08,959 --> 00:03:17,800
complied multiple modules named by

00:03:12,480 --> 00:03:21,040
km1 module qmi2 module and so on

00:03:17,800 --> 00:03:24,400
specific implement

00:03:21,040 --> 00:03:27,840
implementation is that we move

00:03:24,400 --> 00:03:30,959
both most of the qm module function

00:03:27,840 --> 00:03:35,200
into qram entire module to

00:03:30,959 --> 00:03:39,440
load the multiple uh copies of

00:03:35,200 --> 00:03:42,560
kim internal modules we associate

00:03:39,440 --> 00:03:43,440
all the already all all the original

00:03:42,560 --> 00:03:46,879
global

00:03:43,440 --> 00:03:50,400
uh variables

00:03:46,879 --> 00:03:53,840
where variables in the in kvm

00:03:50,400 --> 00:03:58,480
model with a chemical model

00:03:53,840 --> 00:04:02,640
and make all the global function local

00:03:58,480 --> 00:04:04,000
in linux a device password is enabled by

00:04:02,640 --> 00:04:07,760
a wav file

00:04:04,000 --> 00:04:11,200
during the live upgrade we inherit

00:04:07,760 --> 00:04:15,360
the vfl connector

00:04:11,200 --> 00:04:18,560
from the outcome to the new cumule

00:04:15,360 --> 00:04:22,240
and the vm's memory is shared by the

00:04:18,560 --> 00:04:22,800
new at old community process the mapping

00:04:22,240 --> 00:04:26,240
from

00:04:22,800 --> 00:04:31,520
gpa to hpa is not changed during the

00:04:26,240 --> 00:04:36,080
live upgrade at the iomu table

00:04:31,520 --> 00:04:39,280
under the i o mmu translation table is

00:04:36,080 --> 00:04:43,520
a uh my

00:04:39,280 --> 00:04:46,639
demand validity so device

00:04:43,520 --> 00:04:48,080
dma operation can continue executing

00:04:46,639 --> 00:04:53,280
without

00:04:48,080 --> 00:04:53,280
interruption even when the vma stop

00:04:53,919 --> 00:04:57,840
for password device how to install

00:04:57,040 --> 00:05:01,360
interrupt

00:04:57,840 --> 00:05:04,400
is not lost during the live upgrade

00:05:01,360 --> 00:05:07,120
this is difficult because password

00:05:04,400 --> 00:05:09,759
device is not suspended

00:05:07,120 --> 00:05:10,720
for this reason the password device

00:05:09,759 --> 00:05:14,479
interrupts

00:05:10,720 --> 00:05:18,240
can incur at any time so

00:05:14,479 --> 00:05:21,360
we cannot completely copy the

00:05:18,240 --> 00:05:23,520
password device interrupt from the old

00:05:21,360 --> 00:05:27,120
commute to the nucleus

00:05:23,520 --> 00:05:27,120
during the life upgrade

00:05:27,280 --> 00:05:35,759
there is a existing solution

00:05:31,440 --> 00:05:39,600
inject inject additional virtual

00:05:35,759 --> 00:05:42,880
erq is quite ideal first

00:05:39,600 --> 00:05:46,400
new cumule inherits vfl event have this

00:05:42,880 --> 00:05:49,600
from the old commune second

00:05:46,400 --> 00:05:53,600
then you accumulate the switch from

00:05:49,600 --> 00:05:57,919
unhappy and receive the paint interrupts

00:05:53,600 --> 00:05:59,600
in last inject additional virtual aiq

00:05:57,919 --> 00:06:04,160
into the vm

00:05:59,600 --> 00:06:04,160
after having over device

00:06:05,120 --> 00:06:11,680
uh in this topic we use

00:06:08,560 --> 00:06:14,800
post interrupt technology to inject

00:06:11,680 --> 00:06:18,560
interrupt a device will

00:06:14,800 --> 00:06:20,400
set post interrupt request speed

00:06:18,560 --> 00:06:21,759
where the device is reading and

00:06:20,400 --> 00:06:25,360
interrupt

00:06:21,759 --> 00:06:28,400
so we need only to insure the sim

00:06:25,360 --> 00:06:29,199
pier descriptor data between the old

00:06:28,400 --> 00:06:32,560
cumule

00:06:29,199 --> 00:06:35,759
and the new communal process

00:06:32,560 --> 00:06:38,560
we allow pierre descriptor data is

00:06:35,759 --> 00:06:39,440
shared between the nucleus and the

00:06:38,560 --> 00:06:43,360
autocumula

00:06:39,440 --> 00:06:43,360
during the live upgrade

00:06:44,400 --> 00:06:51,759
this picture saw pier descriptor data

00:06:49,080 --> 00:06:54,960
initialization compared to

00:06:51,759 --> 00:06:58,400
original qbm design

00:06:54,960 --> 00:07:02,240
in order to share pi descriptor data

00:06:58,400 --> 00:07:05,520
in the new cumule we

00:07:02,240 --> 00:07:09,039
locate we allocate the memory for

00:07:05,520 --> 00:07:12,240
four pi descriptor data structure

00:07:09,039 --> 00:07:12,240
uh in the cumule

00:07:12,800 --> 00:07:16,520
there are three key points for peer

00:07:15,120 --> 00:07:19,840
descriptor structure

00:07:16,520 --> 00:07:22,880
initialization in the new cumule

00:07:19,840 --> 00:07:26,000
first pi described predictor

00:07:22,880 --> 00:07:29,280
shouldn't not be initialized

00:07:26,000 --> 00:07:31,360
in any new slide while the new

00:07:29,280 --> 00:07:35,280
cumulative is neutralized

00:07:31,360 --> 00:07:37,919
second the nucleus don't need to seek

00:07:35,280 --> 00:07:39,680
post-interrupt requesting data from the

00:07:37,919 --> 00:07:42,400
outcome

00:07:39,680 --> 00:07:43,599
because pierre descriptor data is shared

00:07:42,400 --> 00:07:47,280
between

00:07:43,599 --> 00:07:51,120
the new cumule and the automobile

00:07:47,280 --> 00:07:52,240
in last the new queue don't need to

00:07:51,120 --> 00:07:57,120
update interrupt

00:07:52,240 --> 00:07:57,120
mapping table during the live upgrade

00:07:57,280 --> 00:08:04,479
next i will introduce

00:08:00,400 --> 00:08:08,479
how to optimize the vm downtime

00:08:04,479 --> 00:08:08,479
during the live upgrade

00:08:08,879 --> 00:08:16,879
this picture saw the live upgrade

00:08:12,800 --> 00:08:20,400
flow diagram the first step

00:08:16,879 --> 00:08:24,240
we focus uh choosing process and

00:08:20,400 --> 00:08:27,599
execute the new qmu binary

00:08:24,240 --> 00:08:30,879
and the community is in this slide

00:08:27,599 --> 00:08:31,599
the second step we stopped the old

00:08:30,879 --> 00:08:34,959
cumule

00:08:31,599 --> 00:08:38,640
and the silver way and the state

00:08:34,959 --> 00:08:41,760
the last standard the nuclear load

00:08:38,640 --> 00:08:43,200
state from loudoun and start the new

00:08:41,760 --> 00:08:47,279
commune

00:08:43,200 --> 00:08:50,720
it is obvious that vm.time

00:08:47,279 --> 00:08:54,480
contains the following feed stop

00:08:50,720 --> 00:08:57,600
the old commute save the old chemistry

00:08:54,480 --> 00:09:02,399
state load the state from the

00:08:57,600 --> 00:09:02,399
other and start a new cumule

00:09:03,360 --> 00:09:09,680
when stopping the old cumule

00:09:06,560 --> 00:09:10,640
we find the cleaner way for the cleaner

00:09:09,680 --> 00:09:14,320
primitive

00:09:10,640 --> 00:09:17,440
is taking a long time when devices have

00:09:14,320 --> 00:09:20,959
multiple cues for example

00:09:17,440 --> 00:09:24,000
uh what higher net what are blockers or

00:09:20,959 --> 00:09:27,680
the old communal process will be killed

00:09:24,000 --> 00:09:30,720
after the life upgrade under

00:09:27,680 --> 00:09:34,160
under normal human initialization

00:09:30,720 --> 00:09:37,200
locked locked

00:09:34,160 --> 00:09:39,120
uh have they will be free by cumin

00:09:37,200 --> 00:09:41,920
process

00:09:39,120 --> 00:09:42,640
however the device you would have the

00:09:41,920 --> 00:09:46,240
will be

00:09:42,640 --> 00:09:49,680
free by a by the

00:09:46,240 --> 00:09:53,279
uh by the uh

00:09:49,680 --> 00:09:56,399
by the system uh if the cumulative

00:09:53,279 --> 00:09:58,800
fleet so devices might have the

00:09:56,399 --> 00:09:59,839
needed to clean up by the autocoming

00:09:58,800 --> 00:10:05,760
process

00:09:59,839 --> 00:10:10,480
while the life upgrade is successful

00:10:05,760 --> 00:10:13,519
other normal vm startup logic

00:10:10,480 --> 00:10:15,279
first even have the new slide with the

00:10:13,519 --> 00:10:19,440
devices startup

00:10:15,279 --> 00:10:22,640
then vcpu is resumed

00:10:19,440 --> 00:10:26,160
so the vmware time content

00:10:22,640 --> 00:10:30,800
contents uh the initialize

00:10:26,160 --> 00:10:30,800
of devices will have the industry

00:10:30,839 --> 00:10:36,880
inspired by the all

00:10:33,839 --> 00:10:39,839
optimization of vm suspender uh

00:10:36,880 --> 00:10:40,720
we can pre-create the device symmetric

00:10:39,839 --> 00:10:44,160
during

00:10:40,720 --> 00:10:47,839
the qmu initialization because

00:10:44,160 --> 00:10:51,200
the vm don't the

00:10:47,839 --> 00:10:54,480
new communal initialization we can

00:10:51,200 --> 00:10:57,680
decrease the startup

00:10:54,480 --> 00:10:57,680
time initially

00:10:59,200 --> 00:11:06,560
in last we we

00:11:02,880 --> 00:11:10,640
we use the shared memory to save

00:11:06,560 --> 00:11:13,760
the outcome state and the loading state

00:11:10,640 --> 00:11:17,120
in the new cumule happens

00:11:13,760 --> 00:11:19,839
concurrently with receiving state in the

00:11:17,120 --> 00:11:19,839
old cameo

00:11:22,240 --> 00:11:26,480
we use the different recipient number

00:11:25,519 --> 00:11:29,839
and

00:11:26,480 --> 00:11:33,600
memory size to measure the

00:11:29,839 --> 00:11:36,959
recipient time with the development

00:11:33,600 --> 00:11:40,959
with a different workload we

00:11:36,959 --> 00:11:44,880
use the different benchmark tools to

00:11:40,959 --> 00:11:48,880
simulate the command use the case of

00:11:44,880 --> 00:11:52,880
the cloud service including

00:11:48,880 --> 00:11:56,800
computation storage and memory

00:11:52,880 --> 00:12:00,240
we use the following tools streets

00:11:56,800 --> 00:12:03,440
memory tester and file to

00:12:00,240 --> 00:12:08,560
simulate the computation

00:12:03,440 --> 00:12:08,560
memory and and storage

00:12:09,120 --> 00:12:16,000
uh this picture saw the distribution

00:12:12,880 --> 00:12:19,920
of vcpu downtime under

00:12:16,000 --> 00:12:23,839
the vmi uh idle

00:12:19,920 --> 00:12:24,399
we can see the distribution of vm.time

00:12:23,839 --> 00:12:27,440
is

00:12:24,399 --> 00:12:30,880
11 milliseconds to

00:12:27,440 --> 00:12:30,880
34 milliseconds

00:12:31,120 --> 00:12:39,120
next we use the streams tools

00:12:34,399 --> 00:12:43,839
to simulate the competition workload

00:12:39,120 --> 00:12:47,040
while running the street tools in vm

00:12:43,839 --> 00:12:50,240
we update the vm in host

00:12:47,040 --> 00:12:53,360
at we can see the distribution of

00:12:50,240 --> 00:12:59,040
vm.time is 12

00:12:53,360 --> 00:12:59,040
12 milliseconds to 34 milliseconds

00:13:00,560 --> 00:13:07,120
for the memory tester we use

00:13:03,920 --> 00:13:12,320
the memory

00:13:07,120 --> 00:13:16,240
we we use the memory memory tester tools

00:13:12,320 --> 00:13:19,920
we use we use 4 gb memory

00:13:16,240 --> 00:13:24,000
in vm and we offer upward

00:13:19,920 --> 00:13:27,360
the way all we and the way

00:13:24,000 --> 00:13:31,440
upwards the distribution of

00:13:27,360 --> 00:13:34,560
vmware downtime is 12 milliseconds to

00:13:31,440 --> 00:13:38,240
34 milliseconds in

00:13:34,560 --> 00:13:41,279
last we use the file tools to

00:13:38,240 --> 00:13:44,480
simulate the storage workload

00:13:41,279 --> 00:13:47,600
we would write a 14 gb

00:13:44,480 --> 00:13:52,160
to disk a we

00:13:47,600 --> 00:13:56,000
observed the distribution of

00:13:52,160 --> 00:13:59,360
vm.time is a 12 millisecond

00:13:56,000 --> 00:14:03,760
to 38 milliseconds

00:13:59,360 --> 00:14:06,800
based on the above tester allowed

00:14:03,760 --> 00:14:08,320
we find that the relationship between

00:14:06,800 --> 00:14:14,079
the dispute on time

00:14:08,320 --> 00:14:14,079
and the vm workload will not close

00:14:14,160 --> 00:14:19,839
okay that's how

00:14:17,519 --> 00:14:21,120
thank you for your list in my topic if

00:14:19,839 --> 00:14:24,639
you any

00:14:21,120 --> 00:14:24,959
question for this topic please contact

00:14:24,639 --> 00:14:31,839
me

00:14:24,959 --> 00:14:31,839
by this email thank you

00:14:32,880 --> 00:14:34,959

YouTube URL: https://www.youtube.com/watch?v=89IYamch9VM


