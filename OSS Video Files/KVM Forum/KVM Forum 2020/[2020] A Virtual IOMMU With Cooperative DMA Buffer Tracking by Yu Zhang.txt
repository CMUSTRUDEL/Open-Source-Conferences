Title: [2020] A Virtual IOMMU With Cooperative DMA Buffer Tracking by Yu Zhang
Publication date: 2020-12-09
Playlist: KVM Forum 2020
Description: 
	Direct assignment of I/O devices requires the host to statically pin the entire guest memory, thus hindering the efficiency of memory management. Presenting a vIOMMU can fix this but suffers from non-negligible cost of emulating the guest DMA remapping operations. Yu proposes a new vIOMMU architecture with a cooperative DMA buffer tracking mechanism, which is dedicated to achieving fine-grained pinning and is orthogonal to the costly DMA remapping interface. The new mechanism minimizes the VM-exits when enabling host/guest to coordinate the mapping/pinning requirement of active DMA buffers. It is designed in a vendor-agnostic way, thus can be applied to either emulated or para-virtualized vIOMMUs, Paper of this idea was accepted by USENIX ATC’20. In this talk, Yu'd like to talk more about the design/implementation challenges in KVM/Qemu, current status and upstreaming plan.

---

Yu Zhang
Intel Corporation, Virtualization Developer
China

Yu is a virtualization developer from Intel's virtualization team. He had 10+ years’ experiences in virtualization areas from I/O to CPU/memory virtualization, from performance tuning to security enhancements. Yu’s public presentation experience includes Xen summit/LC3 conference/Intel open source technology summit etc.
Captions: 
	00:00:05,839 --> 00:00:09,599
hello everyone

00:00:06,879 --> 00:00:10,400
this is john from intel's virtualization

00:00:09,599 --> 00:00:14,240
team

00:00:10,400 --> 00:00:18,240
and my topic today is a virtual aliamamu

00:00:14,240 --> 00:00:18,240
with cooperative rebuffer tracking

00:00:18,880 --> 00:00:24,000
okay here is today's agenda first i'd

00:00:22,240 --> 00:00:27,680
like to talk about the background

00:00:24,000 --> 00:00:29,679
of static pinning and the indirect tile

00:00:27,680 --> 00:00:31,519
and the problem of static pinning and

00:00:29,679 --> 00:00:35,760
our vrmmu

00:00:31,519 --> 00:00:38,239
and why why do we need the dna tracking

00:00:35,760 --> 00:00:39,440
next i will introduce the concept of

00:00:38,239 --> 00:00:42,000
kuala lumu

00:00:39,440 --> 00:00:43,280
a virtual rmm with cooperative dna

00:00:42,000 --> 00:00:46,719
buffer tracking for

00:00:43,280 --> 00:00:47,840
tile and in the end i'd like to discuss

00:00:46,719 --> 00:00:52,239
the upstream

00:00:47,840 --> 00:00:52,239
considerations of our mmu

00:00:52,320 --> 00:00:56,399
okay as you may know direct io is the

00:00:54,640 --> 00:00:57,199
best performance io virtualization

00:00:56,399 --> 00:00:59,280
method

00:00:57,199 --> 00:01:00,559
widely deployed in cloud and data

00:00:59,280 --> 00:01:03,280
centers

00:01:00,559 --> 00:01:04,320
by assigning a hardware device to the

00:01:03,280 --> 00:01:06,479
virtual machine

00:01:04,320 --> 00:01:08,720
the guests can perform dma operations

00:01:06,479 --> 00:01:10,320
directly without the need of hosting

00:01:08,720 --> 00:01:13,119
information

00:01:10,320 --> 00:01:15,840
at the host hyphyzer programmed the

00:01:13,119 --> 00:01:19,920
hardware alarm a new pitch table

00:01:15,840 --> 00:01:22,799
to provide the intercostal protection

00:01:19,920 --> 00:01:23,759
however direct io faces the problem of

00:01:22,799 --> 00:01:26,799
static pinning

00:01:23,759 --> 00:01:30,159
due to two reasons first

00:01:26,799 --> 00:01:32,479
most devices do not support dna pitfalls

00:01:30,159 --> 00:01:33,439
that means dma buffer needs to be pinned

00:01:32,479 --> 00:01:36,479
in hardware or

00:01:33,439 --> 00:01:39,119
mmu before the dma operation

00:01:36,479 --> 00:01:40,000
well upper pin will mean the dma

00:01:39,119 --> 00:01:43,119
platform need to be

00:01:40,000 --> 00:01:45,360
pre-allocated and mapped in our mmu page

00:01:43,119 --> 00:01:48,640
table

00:01:45,360 --> 00:01:51,759
second since hyphenser has no visibility

00:01:48,640 --> 00:01:54,000
of the guest dma activities it has to

00:01:51,759 --> 00:01:57,759
assume that august pages could be

00:01:54,000 --> 00:01:57,759
used as a dna buffer

00:01:58,399 --> 00:02:01,600
therefore the hyphenser will have to

00:02:00,719 --> 00:02:03,600
pre-allocate

00:02:01,600 --> 00:02:05,200
and map the entire guest memory in

00:02:03,600 --> 00:02:07,920
hardware rmmu

00:02:05,200 --> 00:02:09,360
when the vm is created and all the

00:02:07,920 --> 00:02:13,520
customary shall be pinned

00:02:09,360 --> 00:02:13,520
during the whole lifecycle of the vm

00:02:13,920 --> 00:02:17,840
well the problem of static pinning is

00:02:16,480 --> 00:02:20,400
quite obvious

00:02:17,840 --> 00:02:22,319
we have to tolerate the much increased

00:02:20,400 --> 00:02:25,840
value creation time

00:02:22,319 --> 00:02:27,599
and also the greatly reduced memory

00:02:25,840 --> 00:02:30,160
utilization features

00:02:27,599 --> 00:02:31,040
for example advanced features like page

00:02:30,160 --> 00:02:33,519
migration

00:02:31,040 --> 00:02:35,920
memory over commitment later allocation

00:02:33,519 --> 00:02:38,160
and swiping is not possible with static

00:02:35,920 --> 00:02:38,160
pin

00:02:38,400 --> 00:02:42,959
well one possible solution of static

00:02:40,800 --> 00:02:45,840
cleaning is to expose a virtual

00:02:42,959 --> 00:02:48,640
automobile to the test

00:02:45,840 --> 00:02:50,560
the primary purpose of a virtual iron

00:02:48,640 --> 00:02:51,519
mill is to provide the intracase to

00:02:50,560 --> 00:02:54,640
protection

00:02:51,519 --> 00:02:55,519
with virtual dma reviving well as a side

00:02:54,640 --> 00:02:57,840
effect

00:02:55,519 --> 00:03:00,080
front grid pinion is possible with

00:02:57,840 --> 00:03:02,560
vmware mmu

00:03:00,080 --> 00:03:03,120
well when the dna reviving is enabled in

00:03:02,560 --> 00:03:05,599
the test

00:03:03,120 --> 00:03:07,280
the guest will use vrm multi-map and

00:03:05,599 --> 00:03:09,519
mfdma buffers

00:03:07,280 --> 00:03:10,720
and all these requests will be forwarded

00:03:09,519 --> 00:03:13,120
to

00:03:10,720 --> 00:03:13,840
to hypervisor to dynamically pin and

00:03:13,120 --> 00:03:17,280
unpinged

00:03:13,840 --> 00:03:18,400
dma buffers in such cases with the

00:03:17,280 --> 00:03:21,040
inside of gas

00:03:18,400 --> 00:03:21,519
dna activities static cleaning is no

00:03:21,040 --> 00:03:25,040
longer

00:03:21,519 --> 00:03:28,080
necessary well also

00:03:25,040 --> 00:03:30,000
like uh virtual devices vlr mimi

00:03:28,080 --> 00:03:33,840
could be an emulated one or a

00:03:30,000 --> 00:03:33,840
prioritized one

00:03:34,000 --> 00:03:40,319
however the vrml has its problems

00:03:37,519 --> 00:03:42,319
the emulation cost of the current vrm

00:03:40,319 --> 00:03:45,200
could be significant

00:03:42,319 --> 00:03:47,360
for example we observed more than 96

00:03:45,200 --> 00:03:48,640
percent performance downgraded in

00:03:47,360 --> 00:03:51,680
memphis d

00:03:48,640 --> 00:03:52,480
in the gaster when dma reviving is

00:03:51,680 --> 00:03:56,640
enabled

00:03:52,480 --> 00:03:58,879
in vlr mmu as a result

00:03:56,640 --> 00:04:00,640
virtual dma remapping is not used in

00:03:58,879 --> 00:04:03,840
most gas devices

00:04:00,640 --> 00:04:07,200
for example a vm can be created with

00:04:03,840 --> 00:04:10,879
no virtual memory at all or even with

00:04:07,200 --> 00:04:13,280
vr memory exposed the vm can choose to

00:04:10,879 --> 00:04:14,640
run in password mode which disables a

00:04:13,280 --> 00:04:18,160
dma remarking

00:04:14,640 --> 00:04:18,959
as well we'll focus the security

00:04:18,160 --> 00:04:22,639
requirements

00:04:18,959 --> 00:04:23,280
in various for example dma remapping is

00:04:22,639 --> 00:04:26,880
needed

00:04:23,280 --> 00:04:29,600
when the untrusted device is plugged in

00:04:26,880 --> 00:04:32,400
when the device is assigned to its user

00:04:29,600 --> 00:04:32,400
space drivers

00:04:34,080 --> 00:04:39,120
so although vrml provides an

00:04:36,639 --> 00:04:40,479
architectural way for landing sdma

00:04:39,120 --> 00:04:43,040
buffers

00:04:40,479 --> 00:04:46,000
it is not a reliable solution to achieve

00:04:43,040 --> 00:04:48,880
fine-grained painting

00:04:46,000 --> 00:04:49,840
meanwhile we argue that the requirements

00:04:48,880 --> 00:04:52,320
of protection

00:04:49,840 --> 00:04:53,520
and pinning through the same costly dma

00:04:52,320 --> 00:04:57,360
remapping interface

00:04:53,520 --> 00:05:00,080
is needlessly constraining

00:04:57,360 --> 00:05:01,520
because intracased protection is an

00:05:00,080 --> 00:05:03,840
optional gaster side

00:05:01,520 --> 00:05:05,199
requirement while the fine-grained

00:05:03,840 --> 00:05:07,680
pinning is a

00:05:05,199 --> 00:05:08,240
general hosta side requirement for

00:05:07,680 --> 00:05:10,880
efficient

00:05:08,240 --> 00:05:11,840
memory management the host needs the

00:05:10,880 --> 00:05:16,240
capability

00:05:11,840 --> 00:05:16,240
to efficiently tracklist the dma buffers

00:05:16,720 --> 00:05:20,560
so how about we decouple the dma

00:05:19,120 --> 00:05:23,039
tracking and the dma remarketing

00:05:20,560 --> 00:05:25,680
interfaces in blmu

00:05:23,039 --> 00:05:28,800
that means we want a separate separate

00:05:25,680 --> 00:05:31,680
dma buffer tracking mechanism in blmu

00:05:28,800 --> 00:05:32,880
without relying on any semantics of dna

00:05:31,680 --> 00:05:35,199
remapping

00:05:32,880 --> 00:05:36,400
and if this mechanism is efficient

00:05:35,199 --> 00:05:39,199
enough

00:05:36,400 --> 00:05:42,479
we may expect most guests to always

00:05:39,199 --> 00:05:42,479
enable fingering pinning

00:05:42,720 --> 00:05:46,479
for this dma buffer tracking mechanism

00:05:45,440 --> 00:05:48,880
we except

00:05:46,479 --> 00:05:50,400
we expect it to be orthogonal to dma

00:05:48,880 --> 00:05:52,400
remapping

00:05:50,400 --> 00:05:54,400
enabling our dma buffer tracking should

00:05:52,400 --> 00:05:58,160
not affect the desired

00:05:54,400 --> 00:06:00,880
protection of the dma remarking also

00:05:58,160 --> 00:06:02,720
dma buffer tracking should incur only

00:06:00,880 --> 00:06:05,440
negligible cost

00:06:02,720 --> 00:06:07,039
the performance expectations with under

00:06:05,440 --> 00:06:09,919
different protection policies

00:06:07,039 --> 00:06:09,919
shall be sustained

00:06:10,440 --> 00:06:15,520
non-intrusiveness we try to minimize the

00:06:13,600 --> 00:06:18,319
changes in the guest stack

00:06:15,520 --> 00:06:18,319
software stack

00:06:19,120 --> 00:06:25,520
well such solution should be uh widely

00:06:22,720 --> 00:06:27,039
applicable i mean it should work with

00:06:25,520 --> 00:06:28,880
all kinds of our devices

00:06:27,039 --> 00:06:32,960
and it shall be easily ported to

00:06:28,880 --> 00:06:35,520
different vrm implementations

00:06:32,960 --> 00:06:37,440
extensible the solution should be

00:06:35,520 --> 00:06:40,000
extensible to help address other

00:06:37,440 --> 00:06:42,319
limitations in memory management

00:06:40,000 --> 00:06:44,319
for example it can help to track the

00:06:42,319 --> 00:06:47,039
dirty dma

00:06:44,319 --> 00:06:49,600
the 30 dma pages during the vm line

00:06:47,039 --> 00:06:49,600
algorithm

00:06:50,960 --> 00:06:54,560
here we propose a cooperative dma buffer

00:06:54,080 --> 00:06:58,160
tracking

00:06:54,560 --> 00:07:01,440
as a power virtualized interface

00:06:58,160 --> 00:07:03,520
by cooperative we mean bi-directionally

00:07:01,440 --> 00:07:04,639
share the information between the guest

00:07:03,520 --> 00:07:07,599
and the host

00:07:04,639 --> 00:07:09,599
about the dma buffering information and

00:07:07,599 --> 00:07:10,080
the fundamental information shared is

00:07:09,599 --> 00:07:12,880
the pin

00:07:10,080 --> 00:07:14,479
status and the mapped status for each

00:07:12,880 --> 00:07:17,360
guest page

00:07:14,479 --> 00:07:19,280
well the host tells the guest whether a

00:07:17,360 --> 00:07:21,840
page is already pinned

00:07:19,280 --> 00:07:23,680
here by pinned i mean the page is

00:07:21,840 --> 00:07:24,479
allocated by the host on the mapping

00:07:23,680 --> 00:07:27,520
hardware

00:07:24,479 --> 00:07:29,599
mmu and the gesture tells the host of

00:07:27,520 --> 00:07:32,800
whether a page is mapped by its

00:07:29,599 --> 00:07:35,759
various dma api so with this

00:07:32,800 --> 00:07:37,840
information we can minimize the number

00:07:35,759 --> 00:07:41,120
of vmaxes when that's the maps

00:07:37,840 --> 00:07:43,360
of pages so it's dma api i mean

00:07:41,120 --> 00:07:45,759
the page pinning requests are only

00:07:43,360 --> 00:07:47,280
needed for guest pages that are not pin

00:07:45,759 --> 00:07:49,919
the yet

00:07:47,280 --> 00:07:52,000
we can also eliminate the number of

00:07:49,919 --> 00:07:55,520
webmasters when that's the imac's

00:07:52,000 --> 00:07:57,919
dma page also

00:07:55,520 --> 00:07:58,960
our solution enables flexible host

00:07:57,919 --> 00:08:02,000
memory management

00:07:58,960 --> 00:08:04,479
policies for example the host can

00:08:02,000 --> 00:08:07,199
mp highlight pages which are no longer

00:08:04,479 --> 00:08:07,199
dma mapped

00:08:07,280 --> 00:08:10,960
here we name the lmmu with such design

00:08:10,240 --> 00:08:14,240
as cool

00:08:10,960 --> 00:08:15,440
mmu over to our mmu with cooperative dma

00:08:14,240 --> 00:08:18,319
buffer tracking

00:08:15,440 --> 00:08:18,319
in direct io

00:08:19,919 --> 00:08:24,560
okay this page is about the architecture

00:08:22,720 --> 00:08:26,800
of cool mmu

00:08:24,560 --> 00:08:28,639
first we introduce the dma tracking

00:08:26,800 --> 00:08:30,319
table the dtt

00:08:28,639 --> 00:08:32,080
to hold the share the dma buffer

00:08:30,319 --> 00:08:34,320
information for example

00:08:32,080 --> 00:08:36,800
the printed status and the vapor status

00:08:34,320 --> 00:08:39,839
of each gel phone

00:08:36,800 --> 00:08:42,320
and in the cluster the cocoa mmm driver

00:08:39,839 --> 00:08:45,760
is hooked to that's the dma api drive

00:08:42,320 --> 00:08:48,480
the api layer actually uh

00:08:45,760 --> 00:08:50,399
it's just a virtual rmm driver with our

00:08:48,480 --> 00:08:53,760
pva extensions

00:08:50,399 --> 00:08:55,040
so this driver intercepts the dma api

00:08:53,760 --> 00:08:57,680
operations

00:08:55,040 --> 00:08:59,120
in the guest and updates the dtt

00:08:57,680 --> 00:09:01,440
accordingly

00:08:59,120 --> 00:09:03,360
for paid is not pinned yet the driver

00:09:01,440 --> 00:09:05,040
will send the page pinning requests to

00:09:03,360 --> 00:09:08,399
the back end

00:09:05,040 --> 00:09:10,320
and either the hot poster the core

00:09:08,399 --> 00:09:12,800
mailbag and handles the page pinning

00:09:10,320 --> 00:09:16,240
requests and updates the dtt

00:09:12,800 --> 00:09:19,680
citing the penis leaders for the gfriend

00:09:16,240 --> 00:09:22,480
also the hoster kuala lumium can

00:09:19,680 --> 00:09:26,080
a synchronously the previous based on

00:09:22,480 --> 00:09:26,080
the map status in dtt

00:09:27,680 --> 00:09:31,600
okay let's take a look at the

00:09:29,839 --> 00:09:35,600
organization of the dtt

00:09:31,600 --> 00:09:37,920
dma tracking table well as you can see

00:09:35,600 --> 00:09:39,040
it's a hierarchical feeding structure

00:09:37,920 --> 00:09:41,839
shared between the

00:09:39,040 --> 00:09:43,120
cluster under the test indexed by

00:09:41,839 --> 00:09:45,680
jefferson

00:09:43,120 --> 00:09:47,760
so for each jfan there is a tracking

00:09:45,680 --> 00:09:50,399
unit

00:09:47,760 --> 00:09:51,760
each tracking unit holds the dma buffer

00:09:50,399 --> 00:09:54,880
information such as

00:09:51,760 --> 00:09:57,839
microstatus which indicates if a page

00:09:54,880 --> 00:10:00,320
is currently mapped by gaster dna api it

00:09:57,839 --> 00:10:02,399
is set by and cleared by the gesture dna

00:10:00,320 --> 00:10:05,760
operations

00:10:02,399 --> 00:10:08,240
underpin pin flag which indicates if a

00:10:05,760 --> 00:10:10,720
page is already pinned by the host

00:10:08,240 --> 00:10:12,000
it is cited and declared by the host

00:10:10,720 --> 00:10:15,279
after a page is pinned

00:10:12,000 --> 00:10:18,079
and unpainted later they

00:10:15,279 --> 00:10:20,959
access the flag which indicates if a

00:10:18,079 --> 00:10:23,360
page has been used for dma recently

00:10:20,959 --> 00:10:24,160
it is sent by the guest dma mapping

00:10:23,360 --> 00:10:28,320
operations

00:10:24,160 --> 00:10:31,680
and declared by the host periodically

00:10:28,320 --> 00:10:35,040
also there are five reserved bits

00:10:31,680 --> 00:10:37,519
which can be extended for example

00:10:35,040 --> 00:10:38,720
or we can either a dirty flag to assist

00:10:37,519 --> 00:10:41,680
the dirty page tracking

00:10:38,720 --> 00:10:41,680
in live vibration

00:10:42,000 --> 00:10:46,800
okay let's take a look at the process of

00:10:44,480 --> 00:10:48,320
gas dma mapping operation mapping

00:10:46,800 --> 00:10:52,240
operations

00:10:48,320 --> 00:10:54,800
file dna map the test core rml

00:10:52,240 --> 00:10:56,320
driver will set the map and access the

00:10:54,800 --> 00:10:59,680
file in dtt entry

00:10:56,320 --> 00:11:02,079
for each targeted different meanwhile

00:10:59,680 --> 00:11:02,959
it will check the pin status of this dma

00:11:02,079 --> 00:11:06,000
page

00:11:02,959 --> 00:11:10,079
and the pinning request is necessary

00:11:06,000 --> 00:11:10,079
only when the pin flag is zero

00:11:10,399 --> 00:11:16,240
well the good news is that we find more

00:11:13,600 --> 00:11:17,839
than 99 percent of our pinning requests

00:11:16,240 --> 00:11:20,959
can be avoided

00:11:17,839 --> 00:11:22,240
thanks to the dma buffer locality so it

00:11:20,959 --> 00:11:25,200
is very likely

00:11:22,240 --> 00:11:25,920
that a recently used dma buffer will be

00:11:25,200 --> 00:11:30,000
reused

00:11:25,920 --> 00:11:30,000
in the following dma operations

00:11:30,399 --> 00:11:35,519
and when the test unmapped the dma page

00:11:33,200 --> 00:11:36,720
the choir mail driver just clears the

00:11:35,519 --> 00:11:40,160
map and flight indeed

00:11:36,720 --> 00:11:43,839
entry for the target jfr so there is

00:11:40,160 --> 00:11:43,839
nowhere maxed it at all

00:11:45,519 --> 00:11:49,440
and at the hospital side the corner mill

00:11:48,480 --> 00:11:52,800
backend performs

00:11:49,440 --> 00:11:55,600
lazy and pinning in a separate slide

00:11:52,800 --> 00:11:57,440
it periodically checks the map status of

00:11:55,600 --> 00:11:59,839
each pin page

00:11:57,440 --> 00:12:01,200
for each pin page with microflight with

00:11:59,839 --> 00:12:04,079
zipping 0

00:12:01,200 --> 00:12:06,079
we will check the access the flag which

00:12:04,079 --> 00:12:08,839
indicates this page is not used by

00:12:06,079 --> 00:12:12,639
guys.dma recently

00:12:08,839 --> 00:12:16,160
so if this accessor flag is

00:12:12,639 --> 00:12:19,360
zero we can go ahead to amping it

00:12:16,160 --> 00:12:23,360
and the any amp in the pages can be

00:12:19,360 --> 00:12:26,240
considered as reclaimable by the host

00:12:23,360 --> 00:12:28,639
in the end they access the flag will be

00:12:26,240 --> 00:12:31,120
cleared regardless of its previous

00:12:28,639 --> 00:12:31,120
status

00:12:33,200 --> 00:12:38,560
okay damien tracking versus dma

00:12:36,839 --> 00:12:41,360
remapping

00:12:38,560 --> 00:12:44,560
well we know that in the majority cases

00:12:41,360 --> 00:12:47,200
dma remapping is not used by the guest

00:12:44,560 --> 00:12:48,000
and the dma tracking with kuala lumium

00:12:47,200 --> 00:12:50,000
could be an

00:12:48,000 --> 00:12:51,279
efficient solution to achieve funded

00:12:50,000 --> 00:12:53,680
pinning

00:12:51,279 --> 00:12:55,040
however sometimes the introduced

00:12:53,680 --> 00:12:59,120
protection may be

00:12:55,040 --> 00:13:01,680
needed for example dma remapping

00:12:59,120 --> 00:13:04,800
can be conditionally enabled for some

00:13:01,680 --> 00:13:07,680
untrusted devices

00:13:04,800 --> 00:13:09,760
in current implementation the hypervisor

00:13:07,680 --> 00:13:12,079
must fall back to static pinning

00:13:09,760 --> 00:13:13,920
as long as there's other assigned

00:13:12,079 --> 00:13:17,519
devices which are not

00:13:13,920 --> 00:13:18,800
using the american another example is

00:13:17,519 --> 00:13:21,120
that

00:13:18,800 --> 00:13:23,040
dma remapping can be enabled when the

00:13:21,120 --> 00:13:24,240
device is assigned to press the user

00:13:23,040 --> 00:13:26,880
space driver

00:13:24,240 --> 00:13:29,440
and later disabled when the device is

00:13:26,880 --> 00:13:32,480
returned back to the color driver

00:13:29,440 --> 00:13:34,560
so in current implementation that means

00:13:32,480 --> 00:13:37,680
to switch between the static pinning

00:13:34,560 --> 00:13:39,600
and the fan grid pinning

00:13:37,680 --> 00:13:42,160
which leads to increase the overhead

00:13:39,600 --> 00:13:42,720
because the intercostal memory will have

00:13:42,160 --> 00:13:46,240
to be

00:13:42,720 --> 00:13:49,199
unpinned and pinned during such switch

00:13:46,240 --> 00:13:50,399
so in such cases dma tracking offered by

00:13:49,199 --> 00:13:53,519
coal armament

00:13:50,399 --> 00:13:56,399
can help provide a reliable solution for

00:13:53,519 --> 00:13:56,399
vanguard pinning

00:13:56,560 --> 00:14:01,120
so what if the dme revamping is always

00:13:59,519 --> 00:14:04,399
enabled in the quest

00:14:01,120 --> 00:14:07,839
i mean for all the devices

00:14:04,399 --> 00:14:11,040
at all times well dma tracking

00:14:07,839 --> 00:14:14,160
in such scenario is only optional

00:14:11,040 --> 00:14:16,800
but our evaluations show that

00:14:14,160 --> 00:14:18,079
even with dma tracking enabled the

00:14:16,800 --> 00:14:21,040
performance over high data

00:14:18,079 --> 00:14:21,040
is negligible

00:14:22,560 --> 00:14:30,079
well how is the implementation

00:14:26,880 --> 00:14:33,600
our previous poc is done by extending

00:14:30,079 --> 00:14:36,320
existing virtual td well as you can see

00:14:33,600 --> 00:14:38,000
there is no ad hoc changes in ironing as

00:14:36,320 --> 00:14:40,639
the device driver

00:14:38,000 --> 00:14:41,279
and we believe such concept can be

00:14:40,639 --> 00:14:45,120
applied

00:14:41,279 --> 00:14:48,160
can be applied in both emulated ioma

00:14:45,120 --> 00:14:51,440
meals and the pyruvate last ones

00:14:48,160 --> 00:14:52,320
so to as to the upstream plan well i

00:14:51,440 --> 00:14:56,240
would like to

00:14:52,320 --> 00:14:59,440
implement it in retail our memory

00:14:56,240 --> 00:15:03,120
so for whatever meal

00:14:59,440 --> 00:15:06,240
we may need a new group of interfaces

00:15:03,120 --> 00:15:07,199
and as to other logics such as gas dma

00:15:06,240 --> 00:15:10,800
buffer tracking

00:15:07,199 --> 00:15:14,959
in dtt and the hostile opinion

00:15:10,800 --> 00:15:18,800
we believe the same code can be reusable

00:15:14,959 --> 00:15:18,800
for different vr previews

00:15:20,800 --> 00:15:27,360
okay i'd like to talk about our upstream

00:15:24,160 --> 00:15:30,639
proposals and opens

00:15:27,360 --> 00:15:31,680
the first is we would like to propose a

00:15:30,639 --> 00:15:34,720
new group of

00:15:31,680 --> 00:15:35,759
the interfaces involved our mmu for

00:15:34,720 --> 00:15:38,399
example the future

00:15:35,759 --> 00:15:38,959
negotiation of the capability of

00:15:38,399 --> 00:15:44,320
fundraising

00:15:38,959 --> 00:15:46,480
pinning i mean the dma buffer tracking

00:15:44,320 --> 00:15:47,680
also the base address of a device speed

00:15:46,480 --> 00:15:51,199
map

00:15:47,680 --> 00:15:52,000
this speed map is a bit indexed by pdf

00:15:51,199 --> 00:15:55,600
of gas

00:15:52,000 --> 00:15:59,440
devices to indicate if this device is

00:15:55,600 --> 00:16:03,120
assigned one because our dma tracking

00:15:59,440 --> 00:16:07,279
shall be only applied to assign devices

00:16:03,120 --> 00:16:10,560
emulated devices are not our concern

00:16:07,279 --> 00:16:14,639
also the base address of ddt

00:16:10,560 --> 00:16:17,279
the dma tracking table meanwhile

00:16:14,639 --> 00:16:18,480
we may need we will definitely need a

00:16:17,279 --> 00:16:22,880
page pinning request

00:16:18,480 --> 00:16:26,000
in their hotel our mmu

00:16:22,880 --> 00:16:26,880
well about the host lazy opinion we

00:16:26,000 --> 00:16:30,560
created

00:16:26,880 --> 00:16:33,120
a separate cumulus right which is

00:16:30,560 --> 00:16:34,720
wake up periodically to perform the

00:16:33,120 --> 00:16:36,959
laser opinion

00:16:34,720 --> 00:16:38,160
where the ampening interval can be

00:16:36,959 --> 00:16:42,000
manually configured

00:16:38,160 --> 00:16:46,560
in cubic command line and i suppose

00:16:42,000 --> 00:16:46,560
uh adaptive interval may be multi-zero

00:16:47,519 --> 00:16:54,399
moreover you may notice that the current

00:16:50,639 --> 00:16:55,360
policy is our i o based so maybe we can

00:16:54,399 --> 00:16:58,560
examine more

00:16:55,360 --> 00:16:58,560
policies in the future

00:17:00,639 --> 00:17:07,360
that's cooperation limitations

00:17:04,000 --> 00:17:10,640
well uh this is a headache uh

00:17:07,360 --> 00:17:13,439
that means when the host creates a vm

00:17:10,640 --> 00:17:15,600
it has no idea if coil memory will be

00:17:13,439 --> 00:17:20,640
enabled by the test

00:17:15,600 --> 00:17:22,799
the same issue is in current blm use

00:17:20,640 --> 00:17:24,720
well current solution is to prepare the

00:17:22,799 --> 00:17:27,679
entire gas memory first

00:17:24,720 --> 00:17:28,160
and the performance i'm pinning during

00:17:27,679 --> 00:17:30,799
the

00:17:28,160 --> 00:17:31,600
in the address space switching logic

00:17:30,799 --> 00:17:36,080
where their

00:17:31,600 --> 00:17:36,080
meal is enabled later in the test

00:17:36,799 --> 00:17:44,400
also guest barrels may use direct health

00:17:40,640 --> 00:17:48,480
well even the gas bills

00:17:44,400 --> 00:17:49,919
hide driver and even this driver could

00:17:48,480 --> 00:17:53,440
be power virtualized

00:17:49,919 --> 00:17:56,559
we still face the same problem mentioned

00:17:53,440 --> 00:17:59,679
about so

00:17:56,559 --> 00:18:01,840
i guess in the short term maybe we will

00:17:59,679 --> 00:18:04,720
have to follow the

00:18:01,840 --> 00:18:06,080
same solution in blmu i mean by

00:18:04,720 --> 00:18:08,720
pre-pinning the guest

00:18:06,080 --> 00:18:09,760
memory and the mp8 when that's the

00:18:08,720 --> 00:18:12,880
enable sql

00:18:09,760 --> 00:18:15,039
mmu but

00:18:12,880 --> 00:18:16,880
you know in some scenario where the

00:18:15,039 --> 00:18:19,520
guest kernel is

00:18:16,880 --> 00:18:21,679
controlled by the maybe for example by

00:18:19,520 --> 00:18:25,039
the cloud provider

00:18:21,679 --> 00:18:28,320
there is no waterfalls in the vm

00:18:25,039 --> 00:18:31,520
also so things would be

00:18:28,320 --> 00:18:33,520
much simpler moreover it

00:18:31,520 --> 00:18:34,880
is possible that a selfish guest may

00:18:33,520 --> 00:18:38,240
deliberately report

00:18:34,880 --> 00:18:40,840
fake dma pds so in the future we may

00:18:38,240 --> 00:18:42,960
choose to build a quota

00:18:40,840 --> 00:18:46,320
mechanism

00:18:42,960 --> 00:18:47,120
about huge pig piping well in our

00:18:46,320 --> 00:18:50,080
implementation

00:18:47,120 --> 00:18:50,799
the dtd track sketch pages are only in

00:18:50,080 --> 00:18:53,840
4kb

00:18:50,799 --> 00:18:56,480
granularity and then

00:18:53,840 --> 00:18:58,480
of course in the backend can choose to

00:18:56,480 --> 00:19:00,400
conduct huge pitch pinning by merging

00:18:58,480 --> 00:19:03,039
continuous guest videos

00:19:00,400 --> 00:19:04,960
however we realized that such

00:19:03,039 --> 00:19:09,360
optimization will complex

00:19:04,960 --> 00:19:12,320
the lazy opening logic well

00:19:09,360 --> 00:19:12,720
fortunately our evaluations show that

00:19:12,320 --> 00:19:15,760
most

00:19:12,720 --> 00:19:19,200
guest dma workloads

00:19:15,760 --> 00:19:22,320
are using frequent mapping operations

00:19:19,200 --> 00:19:25,840
only on many scattered

00:19:22,320 --> 00:19:29,600
4kb pages with one exception is

00:19:25,840 --> 00:19:30,480
the gpu workload without huge pitch

00:19:29,600 --> 00:19:33,600
cleaning

00:19:30,480 --> 00:19:36,799
we observed about four percent

00:19:33,600 --> 00:19:37,520
performance drop in open arena due to

00:19:36,799 --> 00:19:40,960
ltlb

00:19:37,520 --> 00:19:40,960
missed penalties

00:19:43,760 --> 00:19:49,919
another issue is subpage mapping

00:19:47,520 --> 00:19:51,440
well we realized that multiple dma

00:19:49,919 --> 00:19:54,640
buffers may collocate

00:19:51,440 --> 00:19:55,360
in the same 4kb guest page for example

00:19:54,640 --> 00:19:58,480
the small

00:19:55,360 --> 00:20:01,679
network package that implies

00:19:58,480 --> 00:20:03,520
that one guest page can be used

00:20:01,679 --> 00:20:05,360
can be mapped and unmapped multiple

00:20:03,520 --> 00:20:08,720
times concurrently

00:20:05,360 --> 00:20:09,840
so in our implementation the the dtd

00:20:08,720 --> 00:20:12,960
infantry

00:20:09,840 --> 00:20:16,080
tracks the that's the dma mapping count

00:20:12,960 --> 00:20:17,919
for each of the page

00:20:16,080 --> 00:20:19,600
so only when this mapping counter our

00:20:17,919 --> 00:20:21,360
guest page reaches zero

00:20:19,600 --> 00:20:23,600
will the map the flag be cleared for

00:20:21,360 --> 00:20:26,720
this page

00:20:23,600 --> 00:20:31,280
well the last opening is what if the

00:20:26,720 --> 00:20:34,400
the science device is sva capable

00:20:31,280 --> 00:20:35,200
i mean for sv workloads on demand

00:20:34,400 --> 00:20:38,400
pinning

00:20:35,200 --> 00:20:40,080
is already possible it it can be done in

00:20:38,400 --> 00:20:41,840
our memory default

00:20:40,080 --> 00:20:44,559
so what's the value of dma buffer

00:20:41,840 --> 00:20:48,159
tracking for such devices

00:20:44,559 --> 00:20:49,520
well the reason is that a typical sva

00:20:48,159 --> 00:20:53,039
capable device

00:20:49,520 --> 00:20:56,159
have has to support mixed workloads

00:20:53,039 --> 00:20:59,679
with sva overloads are

00:20:56,159 --> 00:21:01,760
non-isolated workloads and it also may

00:20:59,679 --> 00:21:03,039
submit a global configuration data

00:21:01,760 --> 00:21:06,400
structures

00:21:03,039 --> 00:21:08,240
all these are not affordable so a

00:21:06,400 --> 00:21:11,440
cooperative dma buffer tracking could

00:21:08,240 --> 00:21:11,440
still be desirable

00:21:12,559 --> 00:21:16,400
there is a performance evaluation based

00:21:15,440 --> 00:21:19,600
on our

00:21:16,400 --> 00:21:21,600
previous poc well we choose the wide

00:21:19,600 --> 00:21:23,919
range of benchmarks to evaluate

00:21:21,600 --> 00:21:26,559
the performance and the memory footprint

00:21:23,919 --> 00:21:29,840
in some devices such as

00:21:26,559 --> 00:21:33,200
the 40g

00:21:29,840 --> 00:21:37,120
next on the miami ssd and the intel

00:21:33,200 --> 00:21:40,880
gpus uh also benchmarks

00:21:37,120 --> 00:21:44,240
show near to 100 perform 100 performance

00:21:40,880 --> 00:21:46,960
compared with directio without vr memory

00:21:44,240 --> 00:21:50,159
dmv remapping

00:21:46,960 --> 00:21:52,640
meanwhile we found that the

00:21:50,159 --> 00:21:53,280
pin guest pages are much fewer for

00:21:52,640 --> 00:21:55,520
example

00:21:53,280 --> 00:21:56,559
the maximum number of pin pdis we

00:21:55,520 --> 00:22:00,159
observed is

00:21:56,559 --> 00:22:03,360
only just about one percent

00:22:00,159 --> 00:22:07,679
of the entire gas memory which is

00:22:03,360 --> 00:22:10,960
32 gigabytes in our configuration

00:22:07,679 --> 00:22:12,720
also if we do not pre-pin the customary

00:22:10,960 --> 00:22:15,200
we completing this much reduced

00:22:12,720 --> 00:22:18,320
development creation time

00:22:15,200 --> 00:22:21,120
so for the detailed environment

00:22:18,320 --> 00:22:22,480
configurations and the performance data

00:22:21,120 --> 00:22:26,799
you can find it

00:22:22,480 --> 00:22:30,880
in our youth next paper

00:22:26,799 --> 00:22:34,000
so in summary uh current vlr mammals

00:22:30,880 --> 00:22:36,000
cannot reliably eliminate static pinning

00:22:34,000 --> 00:22:38,640
in direct dial

00:22:36,000 --> 00:22:40,080
well cool our mammal can offer a

00:22:38,640 --> 00:22:42,320
reliable approach

00:22:40,080 --> 00:22:45,440
to achieve fundraiser pinning with a

00:22:42,320 --> 00:22:48,159
cooperative dma buffer tracking method

00:22:45,440 --> 00:22:50,320
it dramatically improves the efficiency

00:22:48,159 --> 00:22:51,600
of memory management with negligible

00:22:50,320 --> 00:22:53,919
cost

00:22:51,600 --> 00:22:55,360
meanwhile it suggests that desire the

00:22:53,919 --> 00:22:58,240
security requirement

00:22:55,360 --> 00:22:58,960
in different protection usages i think

00:22:58,240 --> 00:23:02,480
it can

00:22:58,960 --> 00:23:07,039
easily be applied in various uh blm

00:23:02,480 --> 00:23:10,000
implementations and of course

00:23:07,039 --> 00:23:10,320
as previously mentioned core lr memory

00:23:10,000 --> 00:23:13,919
is

00:23:10,320 --> 00:23:18,000
not perfect so any comments and

00:23:13,919 --> 00:23:18,000
any suggestion will be welcome

00:23:18,159 --> 00:23:24,080
okay that's all for this

00:23:21,360 --> 00:23:25,520
presentation so please feel free to

00:23:24,080 --> 00:23:28,720
raise your comments

00:23:25,520 --> 00:23:32,080
and also you can always mail us for

00:23:28,720 --> 00:23:34,840
any questions about corel mmu

00:23:32,080 --> 00:23:37,840
that's that's how thank you thank you

00:23:34,840 --> 00:23:37,840

YouTube URL: https://www.youtube.com/watch?v=7FOrIilvnyc


