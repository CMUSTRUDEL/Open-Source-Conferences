Title: [2017] Introducing vhost-user-scsi and It's Applications by Felipe Franciosi & James Harris
Publication date: 2017-11-22
Playlist: KVM Forum 2017
Description: 
	Qemu provides many powerful storage backends supporting several disk format types. In some cases, however, it may be a design requirement to have a single process managing a storage backend for multiple VMs (similar to what vhost-user-net delivers for networking).

To bring the same concept for storage, we are introducing vhost-user-scsi. This is a new Qemu backend now merged in master which allows a separate process to take ownership of a virtio-scsi device. The flexibility of this model allows such an application to be implemented as a single process for multiple controllers or as one process per controller for added isolation.

In this joint talk by Nutanix and Intel we will introduce the vhost-user-scsi backend, discuss its implementation in Qemu and show how it can be used to achieve amazing performance when integrated with SPDK to explore local and remote NVMe controllers.

---

Felipe Franciosi
Nutanix
Software Engineer
Cambridge, UK

Felipe is a Staff SW Engineer working for Nutanix since October 2015, more specifically on the Acropolis Hypervisor. Previously, he worked for Citrix during four years on the performance of XenServer's storage datapath. Before that, he finished a PhD at Imperial College London on the same subject. Besides performance of virtualised storage, his interests also include operating systems, distributed systems, HPC and computer networks.

Jim Harris
Principal Software Engineer
Intel
Chandler, AZ

Jim is a principal software engineer in Intel's Data Center Group and a core maintainer of the Storage Performance Development Kit (SPDK) open source project. Jim was instrumental in starting the SPDK project in 2013 to provide a framework for building high performance storage software for current and next generation non-volatile media. Jim has been at Intel for 16 years and holds an MS in computer science from Case Western Reserve University.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:07,049 --> 00:00:11,940
thanks everyone for coming my name is

00:00:09,210 --> 00:00:14,130
Philippi I work for Nutanix on the

00:00:11,940 --> 00:00:17,190
engineering of our hypervisor product

00:00:14,130 --> 00:00:21,060
OHV just based on cue in KPM I'm here

00:00:17,190 --> 00:00:22,859
with Jim I'm a Software Architect at

00:00:21,060 --> 00:00:25,439
Intel focused on the storage performance

00:00:22,859 --> 00:00:27,810
development care and we'll be talking

00:00:25,439 --> 00:00:29,340
about if you hose users Kazi which as

00:00:27,810 --> 00:00:31,679
you might have heard from other talks

00:00:29,340 --> 00:00:33,630
there's a mechanism to offload the data

00:00:31,679 --> 00:00:36,329
path over various cursor device to use a

00:00:33,630 --> 00:00:39,030
space we have a little disclaimer for

00:00:36,329 --> 00:00:40,200
you to read you can do that later if if

00:00:39,030 --> 00:00:45,559
that's not enough for you we have

00:00:40,200 --> 00:00:47,670
another one I think basically just is

00:00:45,559 --> 00:00:49,230
don't buy our products based on what

00:00:47,670 --> 00:00:52,710
you're seeing here and we're not

00:00:49,230 --> 00:00:54,960
committed to release any of that so just

00:00:52,710 --> 00:00:58,280
to get us warmed up and started I wanted

00:00:54,960 --> 00:01:01,559
to begin by reviewing a very typical

00:00:58,280 --> 00:01:03,690
vernier scale see data path on qmu KVM

00:01:01,559 --> 00:01:04,920
that's how we've been using it until

00:01:03,690 --> 00:01:07,320
recently there are other ways of

00:01:04,920 --> 00:01:09,210
configuring like um you to behave but

00:01:07,320 --> 00:01:10,500
you've probably seemed very similar

00:01:09,210 --> 00:01:12,270
diagrams today when you have your

00:01:10,500 --> 00:01:14,549
hardware in the bottom there and then

00:01:12,270 --> 00:01:16,259
you have a host running a KVM

00:01:14,549 --> 00:01:19,590
accelerator and the more under colonel

00:01:16,259 --> 00:01:22,710
and commune is a space with all of its

00:01:19,590 --> 00:01:25,409
treads for V CPUs and device simulation

00:01:22,710 --> 00:01:27,960
and then you have a virtual hardware

00:01:25,409 --> 00:01:29,670
abstraction and on a topic a virtual

00:01:27,960 --> 00:01:32,670
machine with some sort of storage

00:01:29,670 --> 00:01:36,240
workload driving the device via various

00:01:32,670 --> 00:01:38,400
crazy driver so the ID set of a risk is

00:01:36,240 --> 00:01:40,140
a driver first place the requests in a

00:01:38,400 --> 00:01:42,630
memory somewhere populating the

00:01:40,140 --> 00:01:45,540
controllers v cues and then sends a kick

00:01:42,630 --> 00:01:49,290
right that kick is strapped but it gave

00:01:45,540 --> 00:01:50,399
him module and delivered to qme via an

00:01:49,290 --> 00:01:52,229
event file descriptor

00:01:50,399 --> 00:01:54,479
so cami looks at of how the shifter

00:01:52,229 --> 00:01:56,250
knows which cue that came from and he

00:01:54,479 --> 00:01:58,409
can go on a gas memory and pick those

00:01:56,250 --> 00:02:01,500
requests from that key and then submit

00:01:58,409 --> 00:02:03,689
it to the corresponding back end so the

00:02:01,500 --> 00:02:07,170
vm on your left there that could be an

00:02:03,689 --> 00:02:09,690
hour card with some sort of MBD or in a

00:02:07,170 --> 00:02:14,040
fast back-end on the other one the disk

00:02:09,690 --> 00:02:15,599
could be a cue CalFire or raw disk over

00:02:14,040 --> 00:02:19,890
a logical volume partition for example

00:02:15,599 --> 00:02:21,540
and once this request completes then he

00:02:19,890 --> 00:02:23,579
populates the response structures on

00:02:21,540 --> 00:02:25,769
those Vicky's and he notifies the gas

00:02:23,579 --> 00:02:28,049
via an interrupt which again it's

00:02:25,769 --> 00:02:30,180
usually file descriptor that he writes

00:02:28,049 --> 00:02:33,030
KVM and KVM delivers the interrupt

00:02:30,180 --> 00:02:36,120
so with this model in mind the

00:02:33,030 --> 00:02:38,250
motivation we had for this work was to

00:02:36,120 --> 00:02:41,579
address search certain limitations that

00:02:38,250 --> 00:02:43,590
that we notice there and the q1 is

00:02:41,579 --> 00:02:47,220
basically this concept that we have this

00:02:43,590 --> 00:02:48,870
one-to-one mapping between a VM and a

00:02:47,220 --> 00:02:51,829
host process they handle the view queues

00:02:48,870 --> 00:02:55,260
and we've we've been talking a lot about

00:02:51,829 --> 00:02:56,519
using perhaps multiple processes etc but

00:02:55,260 --> 00:02:58,590
the key things and all of these models

00:02:56,519 --> 00:03:01,590
give you a way of having a single

00:02:58,590 --> 00:03:03,870
process managing multiple VMs and and

00:03:01,590 --> 00:03:06,120
that's something you can do if you can

00:03:03,870 --> 00:03:09,510
offload the data path with the

00:03:06,120 --> 00:03:11,790
technology likely host user because you

00:03:09,510 --> 00:03:17,939
cannot have a single process handling

00:03:11,790 --> 00:03:20,340
multiple VMs you also cannot effectively

00:03:17,939 --> 00:03:22,680
pull on multiple VMs so if you want to

00:03:20,340 --> 00:03:25,109
get rid of the interrupt model and have

00:03:22,680 --> 00:03:27,419
a process it spinning on all of these

00:03:25,109 --> 00:03:28,680
GM's you cannot do that and you cannot

00:03:27,419 --> 00:03:30,389
do that efficiently obviously could

00:03:28,680 --> 00:03:31,709
still have each km you spinning by then

00:03:30,389 --> 00:03:34,440
you gotta burn a core perfume which is

00:03:31,709 --> 00:03:35,970
obviously too much and with a single

00:03:34,440 --> 00:03:39,090
process you can do that by spinning on a

00:03:35,970 --> 00:03:41,340
single process right and it also because

00:03:39,090 --> 00:03:45,060
of this it becomes non trivial to

00:03:41,340 --> 00:03:47,760
virtualize a single nvme device in user

00:03:45,060 --> 00:03:49,349
space using technologies like SVD k

00:03:47,760 --> 00:03:52,530
which team we'll talk about later

00:03:49,349 --> 00:03:54,359
so the solution for that is to create

00:03:52,530 --> 00:03:56,310
this offloading mechanism and a

00:03:54,359 --> 00:03:58,590
precedence for that is we host user NAT

00:03:56,310 --> 00:04:01,129
which did it pretty much the same thing

00:03:58,590 --> 00:04:04,590
for now working with obvious and the PDK

00:04:01,129 --> 00:04:07,290
so if you go back to this diagram this

00:04:04,590 --> 00:04:10,560
will basically look like replacing this

00:04:07,290 --> 00:04:12,290
main loop with this views replication

00:04:10,560 --> 00:04:15,629
this this would be in its simplest form

00:04:12,290 --> 00:04:17,840
so you could have Pro another process

00:04:15,629 --> 00:04:19,799
which is not qme handling the i/o

00:04:17,840 --> 00:04:21,599
ideally for performance reasons you

00:04:19,799 --> 00:04:24,300
probably want to have a single process

00:04:21,599 --> 00:04:26,280
managing multiple VMs in a model like

00:04:24,300 --> 00:04:30,520
that it would work like

00:04:26,280 --> 00:04:31,419
so the trade-offs is pretty much we're

00:04:30,520 --> 00:04:34,419
trying to achieve better performance

00:04:31,419 --> 00:04:36,969
right and in terms of benefits it

00:04:34,419 --> 00:04:39,699
becomes for instance easier to implement

00:04:36,969 --> 00:04:42,490
multi cue using multiple threads so

00:04:39,699 --> 00:04:44,979
that's that's being pursued in Psych um

00:04:42,490 --> 00:04:47,169
you know with data plane and and

00:04:44,979 --> 00:04:49,330
multiple threads but but it's been in

00:04:47,169 --> 00:04:52,150
the works for a while it's it's it's

00:04:49,330 --> 00:04:53,949
been proving to be challenging due to

00:04:52,150 --> 00:04:56,050
various things like all the locks that I

00:04:53,949 --> 00:04:59,860
came it has in place for for for the i/o

00:04:56,050 --> 00:05:01,360
path it's also easier to do batching so

00:04:59,860 --> 00:05:03,819
this is something that sometimes is

00:05:01,360 --> 00:05:05,830
overlooked but one thing you can do with

00:05:03,819 --> 00:05:08,259
this model is also collect request from

00:05:05,830 --> 00:05:12,580
multiple views and then submit them in

00:05:08,259 --> 00:05:15,879
one go even if you are not using using

00:05:12,580 --> 00:05:17,590
SP DK you could still think of a single

00:05:15,879 --> 00:05:20,169
write operation to a socket or a single

00:05:17,590 --> 00:05:25,240
I also MIT from from different threads

00:05:20,169 --> 00:05:27,430
right and obviously the polygons go

00:05:25,240 --> 00:05:28,930
which which is interesting and that

00:05:27,430 --> 00:05:31,360
would be if you have a single process

00:05:28,930 --> 00:05:33,729
then again looking at all those wikis

00:05:31,360 --> 00:05:36,129
you can check if a VQ has requests in

00:05:33,729 --> 00:05:37,960
nanoseconds so you can realistically

00:05:36,129 --> 00:05:41,949
look at hundreds of accuse in a host

00:05:37,960 --> 00:05:43,839
very very quickly right there there are

00:05:41,949 --> 00:05:47,440
a few drawbacks with that one that's

00:05:43,839 --> 00:05:49,000
very important to note is if you're

00:05:47,440 --> 00:05:51,009
gonna be using a single process to

00:05:49,000 --> 00:05:54,900
handle multiple films VMs is a security

00:05:51,009 --> 00:05:58,060
angle right so if you have a bug in your

00:05:54,900 --> 00:06:00,520
week or your data structures handling

00:05:58,060 --> 00:06:02,169
routines and one guests attacks your

00:06:00,520 --> 00:06:04,029
process and cracks into it then you

00:06:02,169 --> 00:06:05,650
would be exposing all of the other

00:06:04,029 --> 00:06:08,169
recuse which is obviously a bad thing

00:06:05,650 --> 00:06:11,050
you have to be creative to find ways

00:06:08,169 --> 00:06:14,529
around that the others instability so

00:06:11,050 --> 00:06:16,629
also services are concerning if you have

00:06:14,529 --> 00:06:17,860
a bug that crashes your process you

00:06:16,629 --> 00:06:20,469
probably should start thinking about

00:06:17,860 --> 00:06:23,650
ways of restarting this process and

00:06:20,469 --> 00:06:25,120
having the key M is reconnect somehow if

00:06:23,650 --> 00:06:27,159
you have a single process then this is

00:06:25,120 --> 00:06:29,680
probably not very different than what

00:06:27,159 --> 00:06:31,569
you have today another key difference

00:06:29,680 --> 00:06:34,209
between that and that's important to

00:06:31,569 --> 00:06:35,979
mention because it's different from the

00:06:34,209 --> 00:06:38,490
hosts gauzy which is what you have in

00:06:35,979 --> 00:06:41,970
the kernel is that

00:06:38,490 --> 00:06:43,979
this-this-this fuels uses custom

00:06:41,970 --> 00:06:47,370
limitations they are no longer aware of

00:06:43,979 --> 00:06:48,840
the lungs so all the qmu knows about is

00:06:47,370 --> 00:06:51,360
a controller itself that it's

00:06:48,840 --> 00:06:52,830
virtualizing say guests and it doesn't

00:06:51,360 --> 00:06:54,840
know which targets or lungs are attached

00:06:52,830 --> 00:06:56,940
to that controller and that means that

00:06:54,840 --> 00:06:59,190
if you want to plug discs or manage this

00:06:56,940 --> 00:07:02,280
hot plug etc have to do that outside of

00:06:59,190 --> 00:07:04,020
of qme and libvirt is obviously

00:07:02,280 --> 00:07:06,030
completely unaware of what your

00:07:04,020 --> 00:07:09,599
potential target application will look

00:07:06,030 --> 00:07:12,060
like so you can also you can you cannot

00:07:09,599 --> 00:07:14,460
use liver for that either we've been

00:07:12,060 --> 00:07:16,530
thinking about extending the verge to

00:07:14,460 --> 00:07:18,780
have some sort of plugging mechanism so

00:07:16,530 --> 00:07:19,949
that you can write your own plugins and

00:07:18,780 --> 00:07:22,409
then you can use the same interface

00:07:19,949 --> 00:07:25,580
through lipfird to add or remove discs

00:07:22,409 --> 00:07:28,289
etc or resize discs for example and

00:07:25,580 --> 00:07:31,380
finally that's pretty obvious you lose

00:07:28,289 --> 00:07:34,080
all of the key mu block layer features

00:07:31,380 --> 00:07:36,569
if that's something you rely on probably

00:07:34,080 --> 00:07:38,610
if you're pursuing a technology like

00:07:36,569 --> 00:07:41,099
this you you're not gonna be using that

00:07:38,610 --> 00:07:44,580
anyway so that's not so much of a

00:07:41,099 --> 00:07:47,699
problem so if you put what we host user

00:07:44,580 --> 00:07:49,949
and viho side by side looked like at the

00:07:47,699 --> 00:07:52,469
time where you're starting your process

00:07:49,949 --> 00:07:55,860
on the vm on the left on your left you

00:07:52,469 --> 00:07:57,659
already have this v use vhosts use

00:07:55,860 --> 00:08:01,169
discuss your voice application and user

00:07:57,659 --> 00:08:03,360
space there next to Q mu in contrast

00:08:01,169 --> 00:08:06,210
with v host scuzzy which you have on the

00:08:03,360 --> 00:08:07,650
right and that's the module on the

00:08:06,210 --> 00:08:10,229
corner of the screen bear in the kernel

00:08:07,650 --> 00:08:13,110
side so as far as the gas is concerned

00:08:10,229 --> 00:08:13,770
verdigris case device is identical you

00:08:13,110 --> 00:08:15,360
would do

00:08:13,770 --> 00:08:18,569
part of its initialization like

00:08:15,360 --> 00:08:21,030
allocating this V keys and then telling

00:08:18,569 --> 00:08:23,669
the the device where the V cues are via

00:08:21,030 --> 00:08:25,669
PCI config space and then here's where

00:08:23,669 --> 00:08:30,659
the differences begin

00:08:25,669 --> 00:08:32,640
so when every hosts user case q mu has a

00:08:30,659 --> 00:08:35,190
unique socket with your application and

00:08:32,640 --> 00:08:38,760
then it sends various messages to this

00:08:35,190 --> 00:08:41,880
application including things like the

00:08:38,760 --> 00:08:44,099
guest memory is this file descriptor is

00:08:41,880 --> 00:08:46,020
you can a map and if you queues or these

00:08:44,099 --> 00:08:48,180
addresses and they have these sizes and

00:08:46,020 --> 00:08:50,220
hear your father scripters for kicking

00:08:48,180 --> 00:08:51,860
and in receiving kicks and delivery

00:08:50,220 --> 00:08:55,579
interrupts of the guest

00:08:51,860 --> 00:08:58,570
and on the kernel side of things this

00:08:55,579 --> 00:09:00,769
works via an i/o control interface and

00:08:58,570 --> 00:09:03,410
I'll mention there's a few differences

00:09:00,769 --> 00:09:05,450
between these two processes that I think

00:09:03,410 --> 00:09:07,040
in the first design may have been

00:09:05,450 --> 00:09:09,200
overlooked and other people are using

00:09:07,040 --> 00:09:11,420
them more actively they're noticing some

00:09:09,200 --> 00:09:12,769
some issues with that but the idea is

00:09:11,420 --> 00:09:15,019
you said I don't control and you send

00:09:12,769 --> 00:09:18,190
pretty much the same set of messages and

00:09:15,019 --> 00:09:22,700
then you can start your device handling

00:09:18,190 --> 00:09:27,290
tasks and queues in the kernel so some

00:09:22,700 --> 00:09:29,779
of the important highlights here is that

00:09:27,290 --> 00:09:31,399
in terms of implementation power all of

00:09:29,779 --> 00:09:33,079
us that we do there's more with staff

00:09:31,399 --> 00:09:36,190
and remember actually I said we did a

00:09:33,079 --> 00:09:41,450
small refactor polar left I don't know

00:09:36,190 --> 00:09:43,970
of the vhosts Kazi psych ume so we split

00:09:41,450 --> 00:09:46,040
vo skies into a V host Kazakh common and

00:09:43,970 --> 00:09:48,260
IV host gauzy just cuz it come obviously

00:09:46,040 --> 00:09:50,240
being the parent and then we introduce

00:09:48,260 --> 00:09:54,380
the V host users cozy underneath that as

00:09:50,240 --> 00:09:56,779
a sibling to to the viscosity in terms

00:09:54,380 --> 00:10:01,490
of life migration he's actually quite

00:09:56,779 --> 00:10:04,579
straightforward we the the first patches

00:10:01,490 --> 00:10:06,380
had support for that and at the time the

00:10:04,579 --> 00:10:08,990
patches got merged unfortunately there

00:10:06,380 --> 00:10:11,149
were some mother changes and then the

00:10:08,990 --> 00:10:13,880
live migration handlers were dropped but

00:10:11,149 --> 00:10:17,180
we probably riad this soon and it's

00:10:13,880 --> 00:10:18,920
quite straightforward what you need to

00:10:17,180 --> 00:10:21,350
do is make sure your two stack starts

00:10:18,920 --> 00:10:24,800
your target application on the receiving

00:10:21,350 --> 00:10:26,839
side right before you begin migrating or

00:10:24,800 --> 00:10:30,560
have some sort of wrapper script that

00:10:26,839 --> 00:10:32,779
would do that for you and that trick to

00:10:30,560 --> 00:10:35,060
us the device is to use the get Vereen

00:10:32,779 --> 00:10:37,310
base V host message and what that

00:10:35,060 --> 00:10:39,769
message does is basically it's qmu

00:10:37,310 --> 00:10:42,410
asking the device which could be the

00:10:39,769 --> 00:10:44,990
hostess in the kernel or or even people

00:10:42,410 --> 00:10:47,779
snapped asking the the device basically

00:10:44,990 --> 00:10:50,390
where are you on the avail ring right

00:10:47,779 --> 00:10:52,070
which position you are and uses that you

00:10:50,390 --> 00:10:53,149
to send that over the migration stream

00:10:52,070 --> 00:10:55,430
to the other side to say you have to

00:10:53,149 --> 00:10:57,050
continue from there so obviously when

00:10:55,430 --> 00:10:59,990
you get this message your device needs

00:10:57,050 --> 00:11:03,069
to stop picking up request from the ring

00:10:59,990 --> 00:11:05,300
because the X is going to be used and

00:11:03,069 --> 00:11:05,590
what you do at that point is just you

00:11:05,300 --> 00:11:09,100
cue

00:11:05,590 --> 00:11:12,580
the device or that particular vq where

00:11:09,100 --> 00:11:15,820
the message came in for one thing

00:11:12,580 --> 00:11:18,760
pending for migration is throttling so

00:11:15,820 --> 00:11:20,820
that that's to penning and it's not

00:11:18,760 --> 00:11:24,220
specific discuss I think it's a generic

00:11:20,820 --> 00:11:26,170
V host user thing which basically is

00:11:24,220 --> 00:11:27,730
when your life migrating and you're

00:11:26,170 --> 00:11:30,100
trying to convert your migration you

00:11:27,730 --> 00:11:32,680
might want to start throttling your CPUs

00:11:30,100 --> 00:11:35,170
so the guest slows down on how much

00:11:32,680 --> 00:11:36,970
memory it's dirty and there is no

00:11:35,170 --> 00:11:40,030
mechanism in place that I know of to

00:11:36,970 --> 00:11:43,780
tell the device to throttle as well so

00:11:40,030 --> 00:11:46,000
not on the vehicle side and that means

00:11:43,780 --> 00:11:48,030
that the device itself can continue to

00:11:46,000 --> 00:11:50,800
dirty memory at a very very high rate

00:11:48,030 --> 00:11:52,990
now the interesting thing to note as

00:11:50,800 --> 00:11:55,180
well as more people sorry uses and

00:11:52,990 --> 00:11:57,400
develop the host and V host user devices

00:11:55,180 --> 00:11:58,630
is you might start finding other areas

00:11:57,400 --> 00:11:59,260
like this for improvement and perhaps

00:11:58,630 --> 00:12:01,540
some bugs

00:11:59,260 --> 00:12:05,230
we picked a couple which are generic -

00:12:01,540 --> 00:12:06,370
TV host doing it during this work and as

00:12:05,230 --> 00:12:09,970
I mentioned there is a key difference

00:12:06,370 --> 00:12:12,730
between V host and vo user which is the

00:12:09,970 --> 00:12:15,670
fact that the UNIX socket messages there

00:12:12,730 --> 00:12:18,850
are sense they are synchronous and the

00:12:15,670 --> 00:12:20,320
i/o controls are used to derive vhosts

00:12:18,850 --> 00:12:22,810
in the kernel message they are not the

00:12:20,320 --> 00:12:27,520
i/o control blocks until the kernel

00:12:22,810 --> 00:12:32,260
module is is done and the reason this is

00:12:27,520 --> 00:12:35,410
tricky is imagine QM is telling the

00:12:32,260 --> 00:12:36,850
device that some memory was hot plot so

00:12:35,410 --> 00:12:39,850
if you do an i/o control and you say

00:12:36,850 --> 00:12:41,770
some memories being hot plot you know

00:12:39,850 --> 00:12:45,490
that when they are control returned the

00:12:41,770 --> 00:12:47,230
devices is it's completely finished

00:12:45,490 --> 00:12:49,750
mapping then your memory into its own

00:12:47,230 --> 00:12:50,980
space and if this is a UNIX socket

00:12:49,750 --> 00:12:53,380
message then you can tell your device

00:12:50,980 --> 00:12:56,700
there are some new file descriptor here

00:12:53,380 --> 00:12:59,470
with a new memory region on this VM and

00:12:56,700 --> 00:13:01,330
what happens is that Jamie sends this

00:12:59,470 --> 00:13:03,280
message and then tells the guest to

00:13:01,330 --> 00:13:06,280
carry on executing it's known with this

00:13:03,280 --> 00:13:09,850
new memory but your device may not have

00:13:06,280 --> 00:13:11,170
finished mapping there yet so during

00:13:09,850 --> 00:13:13,000
this work one of the things we did is

00:13:11,170 --> 00:13:15,490
introduced a new feature for the V host

00:13:13,000 --> 00:13:17,920
use a protocol which is called reply ACK

00:13:15,490 --> 00:13:19,810
and the idea is that for certain

00:13:17,920 --> 00:13:22,510
messages you can take

00:13:19,810 --> 00:13:24,760
their flag and that causes qmu to wait

00:13:22,510 --> 00:13:26,770
for you to reply to a message that

00:13:24,760 --> 00:13:30,010
previously or in the original protocol

00:13:26,770 --> 00:13:32,260
specification didn't really really

00:13:30,010 --> 00:13:34,230
expect responses print is not in there

00:13:32,260 --> 00:13:38,260
she was the one who did that work

00:13:34,230 --> 00:13:41,260
cool and with that our pathology Jim to

00:13:38,260 --> 00:13:44,320
start talking about such a target

00:13:41,260 --> 00:13:49,029
application example using SP okay okay

00:13:44,320 --> 00:13:49,930
thanks for Lipe okay so first I just

00:13:49,029 --> 00:13:54,220
want to give a little bit of a

00:13:49,930 --> 00:13:56,260
background on SPD Kay and what it is

00:13:54,220 --> 00:14:01,089
before I start getting into details on

00:13:56,260 --> 00:14:02,770
our V host user scuzzy implementation so

00:14:01,089 --> 00:14:05,860
SP DK is basically a set of software

00:14:02,770 --> 00:14:08,380
building blocks it really started out

00:14:05,860 --> 00:14:10,390
with pole mode nvme driver kind of

00:14:08,380 --> 00:14:12,160
taking what DP DK did for networking and

00:14:10,390 --> 00:14:15,010
applying that to a storage context with

00:14:12,160 --> 00:14:17,140
nvm there's a number of other drivers

00:14:15,010 --> 00:14:18,310
and libraries and applications that have

00:14:17,140 --> 00:14:20,470
been built on that now and they're all

00:14:18,310 --> 00:14:21,730
based around storage so storage storage

00:14:20,470 --> 00:14:26,890
networking and how storage

00:14:21,730 --> 00:14:28,630
virtualization as well SP DK primarily

00:14:26,890 --> 00:14:30,820
leverages DP DK that's our default

00:14:28,630 --> 00:14:33,339
environment we do that for things like

00:14:30,820 --> 00:14:35,620
memory management PCI device enumeration

00:14:33,339 --> 00:14:37,810
we have some abstractions for people who

00:14:35,620 --> 00:14:40,570
maybe want to use libraries without

00:14:37,810 --> 00:14:42,700
using DP DK but primarily we're using DP

00:14:40,570 --> 00:14:44,530
DK in fact early on when we started this

00:14:42,700 --> 00:14:49,150
project it was actually called DP DK for

00:14:44,530 --> 00:14:52,950
storage the project was started

00:14:49,150 --> 00:14:56,470
internally at Intel about four years ago

00:14:52,950 --> 00:14:59,020
2015 it was open sourced its BSD

00:14:56,470 --> 00:15:04,660
licensed and you can get more details on

00:14:59,020 --> 00:15:05,890
the SPD K dot IO website so next this is

00:15:04,660 --> 00:15:07,750
a little bit of an eye chart this is

00:15:05,890 --> 00:15:10,620
just kind of showing some of the

00:15:07,750 --> 00:15:13,240
different building blocks that sbk has

00:15:10,620 --> 00:15:16,360
today I'm gonna mostly be focusing on

00:15:13,240 --> 00:15:18,010
the V host user scuzzy target the the

00:15:16,360 --> 00:15:19,630
pull mode nvme driver and then I'm going

00:15:18,010 --> 00:15:21,220
to talk a little bit about our logical

00:15:19,630 --> 00:15:25,470
volumes implementation and how we use

00:15:21,220 --> 00:15:25,470
that for sharing SSDs

00:15:27,000 --> 00:15:34,180
okay so basic architecture so you see

00:15:31,090 --> 00:15:36,100
we've got an nvme SSD there when you

00:15:34,180 --> 00:15:39,910
start configuring this we basically have

00:15:36,100 --> 00:15:42,880
a JSON RPC interface with SB DK for

00:15:39,910 --> 00:15:46,510
configuration so when you're gonna set

00:15:42,880 --> 00:15:48,700
up a V host Guzzi so first whenever I

00:15:46,510 --> 00:15:50,350
say V host Guzzi here this means via

00:15:48,700 --> 00:15:52,240
user scuzzy we're not talking anything

00:15:50,350 --> 00:15:53,560
kernel here just to make sure that

00:15:52,240 --> 00:15:55,000
that's clear I was a little bit lazy I

00:15:53,560 --> 00:15:59,770
didn't want to type out the user part

00:15:55,000 --> 00:16:02,770
and all these boxes so it's gonna create

00:15:59,770 --> 00:16:04,450
all these SPD K constructs for the V

00:16:02,770 --> 00:16:06,820
host device in this case a V Hill scuzzy

00:16:04,450 --> 00:16:09,130
controller in the backing storage and

00:16:06,820 --> 00:16:12,160
then like Felipe mentioned earlier we

00:16:09,130 --> 00:16:15,730
create this domain socket which we

00:16:12,160 --> 00:16:17,470
create / v host Guzzi controller so as

00:16:15,730 --> 00:16:19,030
you start building more and more of

00:16:17,470 --> 00:16:20,350
these controllers for more and more vm

00:16:19,030 --> 00:16:22,300
each one's going to have their domain

00:16:20,350 --> 00:16:24,310
socket and connecting to that domain

00:16:22,300 --> 00:16:31,270
socket is basically how that VN gets

00:16:24,310 --> 00:16:33,310
access to their storage okay so now it's

00:16:31,270 --> 00:16:35,350
been configured the VM is now going to

00:16:33,310 --> 00:16:37,750
attach so QE mu connects to the domain

00:16:35,350 --> 00:16:39,550
socket starts passing v host messages to

00:16:37,750 --> 00:16:41,050
give SPD K information again like Felipe

00:16:39,550 --> 00:16:44,190
was talking about you know where's the

00:16:41,050 --> 00:16:48,010
memory information about the queues etc

00:16:44,190 --> 00:16:51,190
and so then SPD K get started here so

00:16:48,010 --> 00:16:52,870
you'll see here here we just got two

00:16:51,190 --> 00:16:58,270
logical cores it can be setup on

00:16:52,870 --> 00:16:59,560
multiple logical cores so first for load

00:16:58,270 --> 00:17:01,270
balancing from a performance perspective

00:16:59,560 --> 00:17:02,860
we're gonna pick a logical core to run

00:17:01,270 --> 00:17:06,370
these Polar's on and so first we're

00:17:02,860 --> 00:17:08,380
going to start a V host Guzzi polar and

00:17:06,370 --> 00:17:09,640
it's basically I'm gonna get into a

00:17:08,380 --> 00:17:11,320
little bit detail later about how

00:17:09,640 --> 00:17:13,330
exactly this works but it's just gonna

00:17:11,320 --> 00:17:18,670
pull the vert queues that are associated

00:17:13,330 --> 00:17:21,130
with that video scuzzy controller and

00:17:18,670 --> 00:17:22,270
then for nvme so nvme has a really nice

00:17:21,130 --> 00:17:24,700
protocol for those that aren't familiar

00:17:22,270 --> 00:17:26,980
you can allocate queue pairs which are

00:17:24,700 --> 00:17:28,300
completely independent of one another so

00:17:26,980 --> 00:17:30,700
at this point we're going to allocate an

00:17:28,300 --> 00:17:35,800
nvme queue pair and we're gonna set up a

00:17:30,700 --> 00:17:37,120
polar for that queue pair as well and

00:17:35,800 --> 00:17:39,470
then we're going to repeat this for

00:17:37,120 --> 00:17:41,360
additional vm so

00:17:39,470 --> 00:17:44,270
configuration tools gonna set up these

00:17:41,360 --> 00:17:46,490
SP DK scuzzy structures for other VMs

00:17:44,270 --> 00:17:50,200
and as they get connected we're gonna

00:17:46,490 --> 00:17:50,200
start these Polar's across other cores

00:17:52,090 --> 00:17:57,800
okay so next a little bit about polling

00:17:55,790 --> 00:17:59,300
so you know polling there's a couple

00:17:57,800 --> 00:18:01,400
different ways to do polling in some

00:17:59,300 --> 00:18:03,980
cases it's you know synchronous polling

00:18:01,400 --> 00:18:05,960
where you submit an IOU poll until that

00:18:03,980 --> 00:18:07,220
i/o is complete everything with SP DK

00:18:05,960 --> 00:18:11,060
though is completely asynchronous

00:18:07,220 --> 00:18:13,580
non-blocking so effectively what we have

00:18:11,060 --> 00:18:15,260
with SP DK is a little thing we call a

00:18:13,580 --> 00:18:16,910
reactor running in each L core and it's

00:18:15,260 --> 00:18:19,280
effectively just a little mini scheduler

00:18:16,910 --> 00:18:20,510
with SP DK we're gonna take these cores

00:18:19,280 --> 00:18:22,010
it's the only thing that's going to run

00:18:20,510 --> 00:18:24,470
on these physical cores on the system

00:18:22,010 --> 00:18:26,030
and this reactor is basically you're

00:18:24,470 --> 00:18:29,000
just gonna iterate through these Polar's

00:18:26,030 --> 00:18:32,030
round-robin this is actually pretty

00:18:29,000 --> 00:18:33,620
effective because when you look at you

00:18:32,030 --> 00:18:36,620
know the cost of checking one of ER q

00:18:33,620 --> 00:18:38,570
has new IO to do or checking one an nvme

00:18:36,620 --> 00:18:42,110
device is posted completions it's really

00:18:38,570 --> 00:18:44,630
really cheap you know on Intel Xeon

00:18:42,110 --> 00:18:47,570
processors you have DD i/o so when an

00:18:44,630 --> 00:18:49,670
nvme device posts a completion that

00:18:47,570 --> 00:18:51,380
completion entry is going to be in LLC

00:18:49,670 --> 00:18:56,930
and so it's super cheap to go check and

00:18:51,380 --> 00:18:58,010
see when that has been updated and so

00:18:56,930 --> 00:19:00,500
thus ends up kind of working a little

00:18:58,010 --> 00:19:02,240
bit like a ping pong type operation so

00:19:00,500 --> 00:19:03,740
you know first the V host user scuzzy

00:19:02,240 --> 00:19:05,690
controller it's gonna pull four new i/o

00:19:03,740 --> 00:19:07,250
requests once it finds a request it's

00:19:05,690 --> 00:19:09,920
gonna build the data structures and it's

00:19:07,250 --> 00:19:11,150
gonna send it down the stack and it's

00:19:09,920 --> 00:19:13,940
going to result in an i/o getting

00:19:11,150 --> 00:19:16,550
submitted to the nvme SSD and at that

00:19:13,940 --> 00:19:18,230
point it's done the V host gusty Poehler

00:19:16,550 --> 00:19:22,010
is not responsible for checking for any

00:19:18,230 --> 00:19:23,960
of the completions that responsible that

00:19:22,010 --> 00:19:25,880
responsibility goes to the beat of nvme

00:19:23,960 --> 00:19:27,800
polar so when this polar run it's gonna

00:19:25,880 --> 00:19:29,120
basically be checking these cue pairs to

00:19:27,800 --> 00:19:32,900
see if there's been completion entries

00:19:29,120 --> 00:19:34,550
posted once it does there's a callback

00:19:32,900 --> 00:19:36,020
that was registered with that i/o it's

00:19:34,550 --> 00:19:38,300
going to invoke that callback it's gonna

00:19:36,020 --> 00:19:40,940
go back up the stack and then the V host

00:19:38,300 --> 00:19:43,970
user scuzzy polar is gonna you know

00:19:40,940 --> 00:19:46,570
basically notify the VM that its entry

00:19:43,970 --> 00:19:46,570
is completed

00:19:47,270 --> 00:19:56,030
okay so now we start to get into some of

00:19:51,890 --> 00:20:00,470
the a little bit of the drawbacks of of

00:19:56,030 --> 00:20:01,850
userspace implementation so one of the

00:20:00,470 --> 00:20:03,350
big things is you want to share SSDs in

00:20:01,850 --> 00:20:05,030
user space right typically you're not

00:20:03,350 --> 00:20:07,520
going to take an SSD and just assign it

00:20:05,030 --> 00:20:09,650
to one VM if you were going to do that

00:20:07,520 --> 00:20:10,700
probably just doing direct assignment is

00:20:09,650 --> 00:20:16,550
probably going to be a more effective

00:20:10,700 --> 00:20:19,550
strategy so what about SR io v the nvme

00:20:16,550 --> 00:20:22,130
specification they recently added I

00:20:19,550 --> 00:20:24,050
think it's on the 1.3 version so it's

00:20:22,130 --> 00:20:26,750
been formalized how you do sr iove with

00:20:24,050 --> 00:20:28,280
nvme SSDs but they're really not out

00:20:26,750 --> 00:20:31,250
there today there there may be some I'm

00:20:28,280 --> 00:20:33,950
not aware of any but it's not really

00:20:31,250 --> 00:20:34,970
common out there today you know and you

00:20:33,950 --> 00:20:36,080
know and other things if you want to

00:20:34,970 --> 00:20:37,970
start doing features like snapshots

00:20:36,080 --> 00:20:42,160
that's pretty hard to do you know in

00:20:37,970 --> 00:20:46,490
hardware with SR io v so that's another

00:20:42,160 --> 00:20:49,790
drawback there so then what about LVM

00:20:46,490 --> 00:20:51,020
well LVM is great but it's in the kernel

00:20:49,790 --> 00:20:55,760
and we're doing everything in user space

00:20:51,020 --> 00:20:57,470
so trying to get a Linux kernel you know

00:20:55,760 --> 00:21:05,660
piece to work with the user space block

00:20:57,470 --> 00:21:06,590
driver is challenging at best so next

00:21:05,660 --> 00:21:08,660
I'm going to talk a little bit about

00:21:06,590 --> 00:21:11,390
what we're trying to do to provide some

00:21:08,660 --> 00:21:14,590
of the similar functionality with sbk in

00:21:11,390 --> 00:21:14,590
a user space context

00:21:14,800 --> 00:21:21,860
okay so first blobstore so what

00:21:20,030 --> 00:21:24,440
blobstore basically is it's a user space

00:21:21,860 --> 00:21:25,880
block allocator and you know as we

00:21:24,440 --> 00:21:27,860
started getting into SPD K when we

00:21:25,880 --> 00:21:30,260
started out it was you know basically an

00:21:27,860 --> 00:21:33,640
nvme it was very block oriented right it

00:21:30,260 --> 00:21:36,200
was I scuzzy it was envy me over fabrics

00:21:33,640 --> 00:21:37,690
nvme pulled mo drivers but we started

00:21:36,200 --> 00:21:40,790
seeing cases where people wanted to have

00:21:37,690 --> 00:21:42,530
some like lightweight file system type

00:21:40,790 --> 00:21:44,300
operations or even things like logical

00:21:42,530 --> 00:21:45,559
volumes and so this is really dedicated

00:21:44,300 --> 00:21:49,190
for that this is not meant to be a

00:21:45,559 --> 00:21:52,010
replacement for you know a full-blown

00:21:49,190 --> 00:21:55,120
fully featured kernel file system it's

00:21:52,010 --> 00:21:57,380
minimalistic for some of these use cases

00:21:55,120 --> 00:21:59,360
I'm gonna get into logical volumes in a

00:21:57,380 --> 00:22:00,390
minute but for you know for rocks TB for

00:21:59,360 --> 00:22:03,900
those that are you know from

00:22:00,390 --> 00:22:05,670
it's a log structured merge tree key

00:22:03,900 --> 00:22:07,800
value store implementation and so these

00:22:05,670 --> 00:22:09,510
types of implementations have you know

00:22:07,800 --> 00:22:12,720
typically just a few hundred files or

00:22:09,510 --> 00:22:14,940
fairly large append-only so you can you

00:22:12,720 --> 00:22:16,860
can do a simpler implementation of that

00:22:14,940 --> 00:22:19,260
compared to doing a full-blown file

00:22:16,860 --> 00:22:21,660
system it's a really blobstore sort of

00:22:19,260 --> 00:22:23,490
the base it's really focused around just

00:22:21,660 --> 00:22:25,620
block allocation just the basics to

00:22:23,490 --> 00:22:28,650
enable some higher order applications to

00:22:25,620 --> 00:22:30,240
run on top of it and then it's really

00:22:28,650 --> 00:22:31,740
designed for fast storage media so this

00:22:30,240 --> 00:22:33,360
has really been designed from the ground

00:22:31,740 --> 00:22:36,000
up with nvme in mind this is not

00:22:33,360 --> 00:22:40,340
designed to run on you know hard disks

00:22:36,000 --> 00:22:40,340
it's it's really focused on flash

00:22:41,120 --> 00:22:45,860
so blobstore design at a high level

00:22:45,950 --> 00:22:49,260
basically you end up with a bunch of

00:22:47,730 --> 00:22:51,030
things that we call blobs and they're

00:22:49,260 --> 00:22:52,860
basically allocated in clusters the

00:22:51,030 --> 00:22:54,450
clusters are gonna be fairly large by

00:22:52,860 --> 00:22:55,650
default one megabyte but it can be

00:22:54,450 --> 00:22:58,140
adjusted

00:22:55,650 --> 00:23:00,780
there's no naming here it's it's all

00:22:58,140 --> 00:23:04,350
based on IDs any naming is done at a

00:23:00,780 --> 00:23:05,880
higher level and again it's completely

00:23:04,350 --> 00:23:07,830
asynchronous so everything it's non

00:23:05,880 --> 00:23:09,450
blocking there's no queuing there's no

00:23:07,830 --> 00:23:11,610
waiting you do a read or write operation

00:23:09,450 --> 00:23:12,870
you're passing a callback function and a

00:23:11,610 --> 00:23:15,090
callback argument you're gonna get

00:23:12,870 --> 00:23:18,930
called back once that operation is

00:23:15,090 --> 00:23:21,960
complete and it's fully parallel so

00:23:18,930 --> 00:23:26,610
there's no locks in the i/o path and

00:23:21,960 --> 00:23:28,440
then we rely on the atomicity guarantees

00:23:26,610 --> 00:23:30,930
as you get like with nvm Express so nvm

00:23:28,440 --> 00:23:33,030
Express guarantees at least 4k write

00:23:30,930 --> 00:23:34,890
atomicity so that's really baked into

00:23:33,030 --> 00:23:36,470
the design of how we do the metadata so

00:23:34,890 --> 00:23:40,860
we don't have like a metadata journal

00:23:36,470 --> 00:23:43,500
metadata is always updated on a 4k you

00:23:40,860 --> 00:23:44,460
know basis we do have support for cases

00:23:43,500 --> 00:23:46,290
where it has to span across multiple

00:23:44,460 --> 00:23:47,700
pages and then we basically just have

00:23:46,290 --> 00:23:53,310
chain pointer so that we can still

00:23:47,700 --> 00:23:56,190
guarantee that kind of atomicity okay so

00:23:53,310 --> 00:23:58,290
next logical volume so if you remember

00:23:56,190 --> 00:24:01,080
back on the first slide you know kind of

00:23:58,290 --> 00:24:03,060
showed how you know the stack looked and

00:24:01,080 --> 00:24:04,770
it basically just started with this you

00:24:03,060 --> 00:24:07,560
know the beat of nvme talking to the

00:24:04,770 --> 00:24:09,240
nvme SSD and now you can see on top of

00:24:07,560 --> 00:24:12,300
that we have the blobstore in the

00:24:09,240 --> 00:24:14,220
logical volumes store so really the more

00:24:12,300 --> 00:24:16,169
common use case with SPD Cavey

00:24:14,220 --> 00:24:18,450
host user scuzzy is the vm's going to be

00:24:16,169 --> 00:24:22,830
attached to a logical volume not the

00:24:18,450 --> 00:24:26,580
nvme SSD so logical volume

00:24:22,830 --> 00:24:28,289
implementation is basically just UUID X

00:24:26,580 --> 00:24:30,990
adders we have some limited X at our

00:24:28,289 --> 00:24:32,909
support with SPD K so we have UUID on

00:24:30,990 --> 00:24:36,090
the else the logical volume store and

00:24:32,909 --> 00:24:37,770
the l vols you know we provide friendly

00:24:36,090 --> 00:24:41,190
names similar to kind of like what LVM

00:24:37,770 --> 00:24:43,020
does so that we can guarantee uniqueness

00:24:41,190 --> 00:24:48,900
within the logical volume store and the

00:24:43,020 --> 00:24:51,659
application and then in the future

00:24:48,900 --> 00:24:54,120
currently we don't support it but it

00:24:51,659 --> 00:24:55,530
actually be not too difficult to add

00:24:54,120 --> 00:24:57,030
snapshot support so that's something

00:24:55,530 --> 00:25:01,549
that that the community is looking at

00:24:57,030 --> 00:25:03,990
adding here in the future okay so next

00:25:01,549 --> 00:25:09,450
I'm gonna talk a little bit about some

00:25:03,990 --> 00:25:12,960
of the vhosts performance so basically

00:25:09,450 --> 00:25:14,669
what we took here was a dual Xeon the

00:25:12,960 --> 00:25:17,640
kind of the latest and greatest skylake

00:25:14,669 --> 00:25:20,250
based processors basically we're taking

00:25:17,640 --> 00:25:22,159
46 cores for the VMS and we're taking 10

00:25:20,250 --> 00:25:26,970
cores that are gonna be dedicated for

00:25:22,159 --> 00:25:28,470
for this V host application and this

00:25:26,970 --> 00:25:29,970
benchmark is really kind of to try to

00:25:28,470 --> 00:25:31,860
show you know what's the maximum

00:25:29,970 --> 00:25:33,030
performance and we can get here you know

00:25:31,860 --> 00:25:34,230
some of the workloads it's maybe not

00:25:33,030 --> 00:25:35,580
what you're typically gonna see from a

00:25:34,230 --> 00:25:37,049
VM but it's really meant to sort of

00:25:35,580 --> 00:25:38,280
demonstrate some of the efficiency

00:25:37,049 --> 00:25:50,370
improvements that you can get with this

00:25:38,280 --> 00:25:53,520
kind of solution oh sorry okay so the

00:25:50,370 --> 00:25:55,110
question was it's accounting like hyper

00:25:53,520 --> 00:25:57,360
threads are just kind of physical cores

00:25:55,110 --> 00:25:58,799
so this here basically we have hyper

00:25:57,360 --> 00:26:00,720
threading disabled so we're just running

00:25:58,799 --> 00:26:06,330
with just a physical it's just 28

00:26:00,720 --> 00:26:09,419
physical cores per socket okay and then

00:26:06,330 --> 00:26:10,740
I don't know how many people are

00:26:09,419 --> 00:26:13,230
familiar I mean sure everybody's heard

00:26:10,740 --> 00:26:15,990
about 3d crosspoint and so we were

00:26:13,230 --> 00:26:20,039
actually able to get a whole bunch of

00:26:15,990 --> 00:26:21,870
the opt in SSDs to put in here from an

00:26:20,039 --> 00:26:23,370
efficiency perspective and throughput

00:26:21,870 --> 00:26:25,169
you know you get about the same number

00:26:23,370 --> 00:26:27,549
of eye ops per second from an opt in

00:26:25,169 --> 00:26:28,600
SSDs you get from an and SSD

00:26:27,549 --> 00:26:30,159
but I'm gonna be talking a little bit

00:26:28,600 --> 00:26:31,629
about some of the latency overhead and

00:26:30,159 --> 00:26:33,309
that's where octane really shines and

00:26:31,629 --> 00:26:36,369
that's why we you know chose that for

00:26:33,309 --> 00:26:37,869
for this configuration and so basically

00:26:36,369 --> 00:26:40,210
we're putting a logical volume store or

00:26:37,869 --> 00:26:42,369
an LV M LV group on each of these SSDs

00:26:40,210 --> 00:26:44,769
and then we've got you know basically

00:26:42,369 --> 00:26:47,289
one VM on each of these 46 other cores

00:26:44,769 --> 00:26:48,999
they've each got a logical volume so

00:26:47,289 --> 00:26:56,169
effectively we end up with two VMs

00:26:48,999 --> 00:26:59,470
talking to each SSD okay so first

00:26:56,169 --> 00:27:01,840
showing the some of the performance so

00:26:59,470 --> 00:27:06,009
the first one here is basically iOS per

00:27:01,840 --> 00:27:10,419
second so this is a comparison of s PDK

00:27:06,009 --> 00:27:13,149
with Linux kernel vhosts gauzy I I don't

00:27:10,419 --> 00:27:14,799
have the QE mu comparison with this

00:27:13,149 --> 00:27:18,159
system typically what we've seen is that

00:27:14,799 --> 00:27:19,749
from a throughput perspective QE mu the

00:27:18,159 --> 00:27:22,149
back end is actually a little bit less

00:27:19,749 --> 00:27:23,340
efficient than than Linux vhosts Guzzi I

00:27:22,149 --> 00:27:26,019
don't have that for this configuration

00:27:23,340 --> 00:27:28,600
unfortunately but what you can see here

00:27:26,019 --> 00:27:30,730
is with this configuration we get around

00:27:28,600 --> 00:27:32,999
2 million iOS per second with those 10

00:27:30,730 --> 00:27:35,409
cores using the Linux kernel target

00:27:32,999 --> 00:27:38,999
where it's somewhere in the seven and a

00:27:35,409 --> 00:27:41,169
half to eight million range for SP DK

00:27:38,999 --> 00:27:42,489
but the big one and I think this is

00:27:41,169 --> 00:27:44,080
really where this kind of solution

00:27:42,489 --> 00:27:47,980
shines is when you start looking at

00:27:44,080 --> 00:27:51,309
queue depth one latency for so the the

00:27:47,980 --> 00:27:53,409
opt in SSDs can get the media itself if

00:27:51,309 --> 00:27:55,359
you take the software driver overhead

00:27:53,409 --> 00:27:57,549
out of the picture is somewhere in the

00:27:55,359 --> 00:27:59,169
six and a half to seven microsecond

00:27:57,549 --> 00:28:01,840
range and then the drivers typically a

00:27:59,169 --> 00:28:06,220
host driver is gonna add maybe three to

00:28:01,840 --> 00:28:07,809
four microseconds on top of that so so

00:28:06,220 --> 00:28:09,730
if you're if this is just on the host

00:28:07,809 --> 00:28:10,960
you're talking about 10 microseconds and

00:28:09,730 --> 00:28:13,659
so now we're talking about what does it

00:28:10,960 --> 00:28:15,249
look like when it gets a virtualized you

00:28:13,659 --> 00:28:19,480
know plus going through a logical volume

00:28:15,249 --> 00:28:21,190
type stack so for Linux and for Kumu

00:28:19,480 --> 00:28:22,629
we're looking in the 30 to 40

00:28:21,190 --> 00:28:24,609
microsecond range you know fully

00:28:22,629 --> 00:28:29,590
virtualized we're with SPD Kay it's

00:28:24,609 --> 00:28:31,330
around 14 microseconds and so the really

00:28:29,590 --> 00:28:32,619
the big advantages here are a number of

00:28:31,330 --> 00:28:34,840
things which Philippe alluded to earlier

00:28:32,619 --> 00:28:36,909
so one since we're polling we don't have

00:28:34,840 --> 00:28:40,179
any VM exits on the i/o submission path

00:28:36,909 --> 00:28:40,679
so we can you know notify the guest VM

00:28:40,179 --> 00:28:42,179
that he does

00:28:40,679 --> 00:28:44,159
have to notify us when he submits an IO

00:28:42,179 --> 00:28:47,039
we're polling on and so we avoid that VM

00:28:44,159 --> 00:28:49,230
exit unfortunately the guest is not

00:28:47,039 --> 00:28:52,139
polling so we still have to send the the

00:28:49,230 --> 00:28:53,580
the interrupt back to the guest and and

00:28:52,139 --> 00:28:54,809
actually in some of the benchmarks we've

00:28:53,580 --> 00:28:56,580
been doing that actually ends up

00:28:54,809 --> 00:28:59,490
consuming thirty five to forty percent

00:28:56,580 --> 00:29:03,600
of the CPU on these 10 cores it's

00:28:59,490 --> 00:29:05,580
running SPD k and then on the nvme side

00:29:03,600 --> 00:29:08,429
as well so you know we're avoiding that

00:29:05,580 --> 00:29:09,809
three to five microsecond overhead you

00:29:08,429 --> 00:29:11,639
know it's typically more about 500

00:29:09,809 --> 00:29:13,139
nanoseconds for SPD K as far as the

00:29:11,639 --> 00:29:21,889
submission and the completion path the

00:29:13,139 --> 00:29:23,879
cost of doing a single i/o ok so future

00:29:21,889 --> 00:29:26,610
so it's been a number of mentions of

00:29:23,879 --> 00:29:28,830
this already supporting V host user

00:29:26,610 --> 00:29:31,200
blocks so there's some patches on the

00:29:28,830 --> 00:29:33,539
mailing list right now that are getting

00:29:31,200 --> 00:29:37,080
reviewed from from one of my colleagues

00:29:33,539 --> 00:29:39,119
at Intel live migration so Felipe you

00:29:37,080 --> 00:29:40,590
know mentioned this earlier our focus so

00:29:39,119 --> 00:29:42,779
far has been on like a ephemeral storage

00:29:40,590 --> 00:29:45,720
live migration is gonna be a big piece

00:29:42,779 --> 00:29:48,570
we have SPD K support for doing like

00:29:45,720 --> 00:29:51,419
nvme over fabrics we don't have a nice

00:29:48,570 --> 00:29:52,529
Guzzi driver yet that's kind of on our

00:29:51,419 --> 00:29:54,720
roadmap I think it's something that

00:29:52,529 --> 00:29:56,519
we're gonna be looking at so you know

00:29:54,720 --> 00:29:58,080
live migration with SPD K I think is

00:29:56,519 --> 00:29:58,950
gonna be an interesting use case and

00:29:58,080 --> 00:30:03,029
that's something that we're gonna want

00:29:58,950 --> 00:30:04,529
to focus on and then being able do

00:30:03,029 --> 00:30:05,999
logical volume snapshots so to be able

00:30:04,529 --> 00:30:08,070
to provide some of those you know higher

00:30:05,999 --> 00:30:09,409
order services that you get from you

00:30:08,070 --> 00:30:11,730
know doing things in the linux kernel

00:30:09,409 --> 00:30:13,019
you know that is one of the it's one of

00:30:11,730 --> 00:30:14,700
the downsides with this solution is that

00:30:13,019 --> 00:30:15,929
when you're doing things in user space

00:30:14,700 --> 00:30:17,070
all the things you get with the kernel

00:30:15,929 --> 00:30:18,779
they're not there anymore right you

00:30:17,070 --> 00:30:20,639
don't get Iost at you don't get all

00:30:18,779 --> 00:30:22,289
these other things and so you know

00:30:20,639 --> 00:30:24,240
that's a lot of things that's one of the

00:30:22,289 --> 00:30:25,590
things that we're looking at is you know

00:30:24,240 --> 00:30:27,090
prioritizing what are those things that

00:30:25,590 --> 00:30:32,279
we need to add to provide as much

00:30:27,090 --> 00:30:33,840
feature parity as we can so finally if

00:30:32,279 --> 00:30:36,990
you're interested here's where you can

00:30:33,840 --> 00:30:38,669
get more information about SPD k you can

00:30:36,990 --> 00:30:40,230
go out to github

00:30:38,669 --> 00:30:41,759
you know Gerrit hub is where we do all

00:30:40,230 --> 00:30:45,419
of our code reviews and we've got all of

00:30:41,759 --> 00:30:47,399
our CI feedback and if you're if you're

00:30:45,419 --> 00:30:50,070
on there and you've got questions pop on

00:30:47,399 --> 00:30:52,190
IRC we actually have there's people in

00:30:50,070 --> 00:30:54,840
SPD k community and

00:30:52,190 --> 00:30:56,309
Shanghai Poland and the u.s. so

00:30:54,840 --> 00:31:01,110
typically there's somebody on there 24

00:30:56,309 --> 00:31:03,350
hours a day and with that we'll take

00:31:01,110 --> 00:31:03,350
questions

00:31:10,700 --> 00:31:15,440
it's been a really good presentation

00:31:13,000 --> 00:31:17,750
either we explain everything or they

00:31:15,440 --> 00:31:26,330
didn't understand we have a question

00:31:17,750 --> 00:31:28,010
here so I work in sustaining engineering

00:31:26,330 --> 00:31:30,470
which is like basically getting

00:31:28,010 --> 00:31:33,350
everything that is done and supporting

00:31:30,470 --> 00:31:36,680
right so what I'm seeing on changes like

00:31:33,350 --> 00:31:38,840
this is that final users are not getting

00:31:36,680 --> 00:31:40,970
the aspect of pinning things and what

00:31:38,840 --> 00:31:43,250
you said about putting everything in an

00:31:40,970 --> 00:31:45,470
external process and relying on the host

00:31:43,250 --> 00:31:47,360
user for example it's awesome if you

00:31:45,470 --> 00:31:49,160
ping the process that responsible for

00:31:47,360 --> 00:31:51,650
doing the back end that the V host would

00:31:49,160 --> 00:31:53,390
be doing the kernel right so my question

00:31:51,650 --> 00:31:55,490
to you is that when you release things

00:31:53,390 --> 00:31:57,770
like this is advertised that you were

00:31:55,490 --> 00:32:00,050
supposed to be pinning there's the the

00:31:57,770 --> 00:32:01,580
userland proxies in such ways to

00:32:00,050 --> 00:32:03,650
guarantee the performance your scene

00:32:01,580 --> 00:32:06,110
benchmark you know because if you

00:32:03,650 --> 00:32:08,420
release this the final user would put

00:32:06,110 --> 00:32:10,910
this together with a bunch of a bunch of

00:32:08,420 --> 00:32:13,850
v cpu threads and they would compete and

00:32:10,910 --> 00:32:14,420
a match would be created like so yes you

00:32:13,850 --> 00:32:18,410
see my point

00:32:14,420 --> 00:32:19,910
no I do yeah yeah what's that yeah so no

00:32:18,410 --> 00:32:23,750
so that's a great yeah so that's you

00:32:19,910 --> 00:32:25,910
know we we have you know we have some

00:32:23,750 --> 00:32:28,070
guides with SP DK on how to do this and

00:32:25,910 --> 00:32:29,840
a lot of it can be based on it it's hard

00:32:28,070 --> 00:32:31,250
to automatically figure out how many you

00:32:29,840 --> 00:32:32,930
need to do and so there is some amount

00:32:31,250 --> 00:32:34,220
of user configuration understanding your

00:32:32,930 --> 00:32:35,450
workload and what you need to do but

00:32:34,220 --> 00:32:37,430
you're right yeah I mean these need to

00:32:35,450 --> 00:32:39,080
be pin you need to understand what's

00:32:37,430 --> 00:32:41,080
running on your system and so that you

00:32:39,080 --> 00:32:43,130
don't have this come kick is competing

00:32:41,080 --> 00:32:44,210
swapping out the SP DK thread for

00:32:43,130 --> 00:32:49,270
something else that's gonna be really

00:32:44,210 --> 00:32:49,270
bad yeah it's a good point thank you

00:32:58,080 --> 00:33:02,350
all right so I had just one question you

00:33:00,370 --> 00:33:05,710
mentioned that the the guest itself is

00:33:02,350 --> 00:33:08,730
not polling for completions so I think

00:33:05,710 --> 00:33:10,900
that Linux now has the block MQ poll

00:33:08,730 --> 00:33:12,520
feature in the multi cue block layer

00:33:10,900 --> 00:33:14,260
have you tried that

00:33:12,520 --> 00:33:17,410
or do you I mean do you think that will

00:33:14,260 --> 00:33:21,730
that will help its so yeah it certainly

00:33:17,410 --> 00:33:23,260
will so we actually you probably missed

00:33:21,730 --> 00:33:24,640
it but when I had that big ID chart of

00:33:23,260 --> 00:33:25,660
all the blocks so we've actually one of

00:33:24,640 --> 00:33:28,930
the things that we're working on right

00:33:25,660 --> 00:33:30,160
now is a Verdi of scuzzy pulled no

00:33:28,930 --> 00:33:31,960
driver that you could run in the guest

00:33:30,160 --> 00:33:33,760
and so we've already done tests where I

00:33:31,960 --> 00:33:35,260
mean we can you know within our guest we

00:33:33,760 --> 00:33:37,480
can check if the interrupt flag is clear

00:33:35,260 --> 00:33:38,410
and then we won't send the we won't send

00:33:37,480 --> 00:33:41,830
the guest as one that we haven't done

00:33:38,410 --> 00:33:43,210
any benchmarking with that yet but that

00:33:41,830 --> 00:33:44,710
is something that we should you know

00:33:43,210 --> 00:33:46,330
definitely take a look at to see if as

00:33:44,710 --> 00:33:49,050
far as like reducing the leak I mean on

00:33:46,330 --> 00:33:51,550
both cases right like that should help

00:33:49,050 --> 00:33:55,840
both on the SPD case ID and the kernel V

00:33:51,550 --> 00:33:57,790
host cuz you target I have another

00:33:55,840 --> 00:33:59,620
question you said that you do not have

00:33:57,790 --> 00:34:01,540
yet the libvirt integration or something

00:33:59,620 --> 00:34:03,340
like that and in the end useless quite

00:34:01,540 --> 00:34:06,450
often use things like word minute or

00:34:03,340 --> 00:34:08,830
whatever which basically used libvirt

00:34:06,450 --> 00:34:12,220
directly or indirectly do you have any

00:34:08,830 --> 00:34:14,679
plans of adding support to live ver to

00:34:12,220 --> 00:34:19,120
actually build something like that or

00:34:14,679 --> 00:34:20,470
are any plans so that's really a big

00:34:19,120 --> 00:34:21,909
thing and I should have put that in the

00:34:20,470 --> 00:34:23,889
you know in the future part here because

00:34:21,909 --> 00:34:26,080
that's another you know big part that we

00:34:23,889 --> 00:34:27,370
want to look at as far as like how do we

00:34:26,080 --> 00:34:29,440
get this integrated into something like

00:34:27,370 --> 00:34:31,090
libvirt because we know that that's how

00:34:29,440 --> 00:34:35,409
this is typically going to be deployed

00:34:31,090 --> 00:34:39,810
and used and so that's a something else

00:34:35,409 --> 00:34:39,810
communities actively looking at yeah

00:34:42,370 --> 00:34:47,050
it's probably gonna look like some sort

00:34:44,750 --> 00:34:49,850
of plug-in interface so the idea is that

00:34:47,050 --> 00:34:51,530
if you're writing your own target

00:34:49,850 --> 00:34:53,330
application then you write a

00:34:51,530 --> 00:34:54,770
corresponding liver plug-in so that

00:34:53,330 --> 00:34:57,380
liver knows how to talk to your

00:34:54,770 --> 00:34:59,900
application and in this example I have

00:34:57,380 --> 00:35:02,420
the despot ek based application we would

00:34:59,900 --> 00:35:04,130
probably submit this lipfird interface

00:35:02,420 --> 00:35:07,280
together with plugins so that you know

00:35:04,130 --> 00:35:10,520
how to talk to that yeah the key things

00:35:07,280 --> 00:35:13,760
are things like laundry size so if you

00:35:10,520 --> 00:35:15,440
resize a virtual desk you have to tell

00:35:13,760 --> 00:35:18,950
the target application which is handling

00:35:15,440 --> 00:35:21,500
those event via cues to then tell the

00:35:18,950 --> 00:35:26,720
driver that someone has changed or

00:35:21,500 --> 00:35:29,600
plugin remove yeah I think the other

00:35:26,720 --> 00:35:30,980
thing is just looking at um so I think

00:35:29,600 --> 00:35:32,930
it's really easy to sort of like

00:35:30,980 --> 00:35:34,070
genericized the scuzzy part of this but

00:35:32,930 --> 00:35:35,510
then when you want to do things like

00:35:34,070 --> 00:35:36,920
logical volume management you know

00:35:35,510 --> 00:35:38,030
having the plugins for that to it so I

00:35:36,920 --> 00:35:40,040
mean there's definitely some work there

00:35:38,030 --> 00:35:41,210
I think the plug-in the plug-in strategy

00:35:40,040 --> 00:35:42,710
is good I think there's just some work

00:35:41,210 --> 00:35:44,330
to figure out what that API needs to

00:35:42,710 --> 00:35:51,230
look like to cover as many use cases as

00:35:44,330 --> 00:35:53,710
we can well yes absolutely there's a

00:35:51,230 --> 00:35:53,710
question over here

00:36:00,310 --> 00:36:10,280
and how many CPU should you dedicate for

00:36:04,750 --> 00:36:13,930
polling to get best performance I hate

00:36:10,280 --> 00:36:13,930
to take the cheap answer but it depends

00:36:14,140 --> 00:36:23,390
what is yours - what's that so we've I

00:36:21,320 --> 00:36:24,680
mean so we've gotten to where you know

00:36:23,390 --> 00:36:27,340
if you're just doing like local

00:36:24,680 --> 00:36:30,020
ephemeral storage you know you can get

00:36:27,340 --> 00:36:33,860
effectively a million i ops on you know

00:36:30,020 --> 00:36:35,540
using a single core now if you start

00:36:33,860 --> 00:36:40,160
doing things like how many for how many

00:36:35,540 --> 00:36:43,700
disks Oh VMs running in parallel so it

00:36:40,160 --> 00:36:44,870
would depend on how many VMs what could

00:36:43,700 --> 00:36:47,030
be any mean it could be anywhere from

00:36:44,870 --> 00:36:48,650
one vm to ten so it's really it's it's

00:36:47,030 --> 00:36:50,690
really cheap this polar strategy it's

00:36:48,650 --> 00:36:52,820
very cheap to you know whether it's

00:36:50,690 --> 00:36:54,230
polling one or ten there's very little

00:36:52,820 --> 00:36:56,390
additional overhead when your poll

00:36:54,230 --> 00:36:58,369
more VMS so part of it kind of depends

00:36:56,390 --> 00:37:00,020
on you know what's your what does your

00:36:58,369 --> 00:37:01,430
system look like how many VMS do they

00:37:00,020 --> 00:37:05,150
have and what kind of i/o load is each

00:37:01,430 --> 00:37:07,359
one gonna drive but in a rough scale

00:37:05,150 --> 00:37:11,000
it's somewhere on the order of you know

00:37:07,359 --> 00:37:11,960
800,000 to a million I ops procore now

00:37:11,000 --> 00:37:13,220
when you start looking at some other

00:37:11,960 --> 00:37:15,260
protocol so like let's say you were

00:37:13,220 --> 00:37:16,850
gonna do you know I scuzzy and so

00:37:15,260 --> 00:37:19,369
instead of this you know basically

00:37:16,850 --> 00:37:21,440
proxying virtio to a local nvme device

00:37:19,369 --> 00:37:23,240
it was I scuzzy instead I scuzzy is

00:37:21,440 --> 00:37:24,740
going to have more protocol more

00:37:23,240 --> 00:37:26,240
protocol overheads you're probably going

00:37:24,740 --> 00:37:27,350
to need to allocate more cores because

00:37:26,240 --> 00:37:32,750
it's not gonna be able to hit that same

00:37:27,350 --> 00:37:33,680
IAP level per per polling core okay does

00:37:32,750 --> 00:37:36,850
that answer your question

00:37:33,680 --> 00:37:36,850
nope thanks okay

00:37:41,990 --> 00:37:47,590
I think we're hitting time limit one

00:37:46,130 --> 00:37:50,290
minute left

00:37:47,590 --> 00:37:54,760
okay thank you everyone thank you

00:37:50,290 --> 00:38:00,619
[Applause]

00:37:54,760 --> 00:38:00,619

YouTube URL: https://www.youtube.com/watch?v=DxqEaCPijlI


