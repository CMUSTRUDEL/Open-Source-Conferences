Title: [2017] KVM Performance Tuning on Alibaba Cloud by Yang Zhang
Publication date: 2017-11-24
Playlist: KVM Forum 2017
Description: 
	Millions of users are deploying their services in Alibaba Cloud which based on KVM. The performance and capacity of KVM are two critical indicator for the whole system. In this presentation, Yang will demonstrate some real performance issues that reported by end users pertained to KVM, and the structural way to analyze and solve the problems. The performance tuning involves timer, IPI, memory and scheduler. In this implementation, lots of KVM specified features like exit-less timer, PV interrupt and VCPU ware scheduler in Linux kernel and KVM are systematically developed and deployed, which will be submitted to KVM community soon. At the end, numbers of performance improvements data of Alibaba Cloud after the overall optimization are showed in the presentation, and certain open-loop issues that still not handled well in KVM which are important to end customers are raised up to audiences.

---

Yang Zhang

Yang is an expert in virtualization field. He first participated in Xen and KVM community in 2008. Most of his contribution was related on Xen and KVM part. He had been a maintainer on Xen VT-D component for two years and the main contributor on Xen nested virtualization and KVM interrupt virtualization. Currently, he is the architect of virtualization platform of Alibaba Cloud.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:05,480 --> 00:00:11,550
okay

00:00:06,750 --> 00:00:16,410
God knew everyone and I'm young from

00:00:11,550 --> 00:00:18,510
Alibaba Crowder I have worked in

00:00:16,410 --> 00:00:22,529
virtualization field for about 10 years

00:00:18,510 --> 00:00:25,830
which since 2008 when I first draw Intel

00:00:22,529 --> 00:00:29,789
and I'm working in the same project at

00:00:25,830 --> 00:00:33,079
the time now I'm working in Alibaba

00:00:29,789 --> 00:00:39,840
crowd and my main job is to maintain

00:00:33,079 --> 00:00:43,440
have water or KVM before I go into more

00:00:39,840 --> 00:00:46,219
detail I would like to share some

00:00:43,440 --> 00:00:49,440
background or al Bubba cloud so you guys

00:00:46,219 --> 00:00:53,250
according to the latest research from

00:00:49,440 --> 00:00:56,940
the Gartner Alibaba cloud have become

00:00:53,250 --> 00:01:00,870
this so the largest public cloud vendor

00:00:56,940 --> 00:01:04,979
just after the Amazon Web service and Lu

00:01:00,870 --> 00:01:08,549
and also we are the biggest public cloud

00:01:04,979 --> 00:01:12,630
vendor in the China which occupies a

00:01:08,549 --> 00:01:16,320
market share of 40% in the whole China

00:01:12,630 --> 00:01:22,159
marketing and we have millions of

00:01:16,320 --> 00:01:25,380
customers who is using our cloud also

00:01:22,159 --> 00:01:29,220
because I'm the web service use then add

00:01:25,380 --> 00:01:30,210
hot water and a you used hyper-v as a

00:01:29,220 --> 00:01:33,479
hypervisor

00:01:30,210 --> 00:01:36,450
so actually we may be the one of the

00:01:33,479 --> 00:01:38,820
biggest QM based cloud vendor in the

00:01:36,450 --> 00:01:42,509
world and now

00:01:38,820 --> 00:01:50,939
Maiden's OPM's are running in our cloud

00:01:42,509 --> 00:01:54,899
everyday so how about the states of the

00:01:50,939 --> 00:01:57,570
QM in the alfaab a cloud actually we

00:01:54,899 --> 00:02:02,520
have used them for several times in the

00:01:57,570 --> 00:02:05,909
early stage and compare with them we

00:02:02,520 --> 00:02:08,789
think even be the most stable the

00:02:05,909 --> 00:02:10,929
problem we are in constant in QM is only

00:02:08,789 --> 00:02:16,019
about the 10%

00:02:10,929 --> 00:02:19,510
/ to them and if I remember correctly

00:02:16,019 --> 00:02:22,150
the last stable issue we encounter in

00:02:19,510 --> 00:02:25,000
the QM is in the last year

00:02:22,150 --> 00:02:30,579
this means we didn't encounter any

00:02:25,000 --> 00:02:34,329
issues for Kibum for about 1 years so QM

00:02:30,579 --> 00:02:39,000
is really stable and also for the

00:02:34,329 --> 00:02:42,010
performance it is good in the most case

00:02:39,000 --> 00:02:46,450
according our testing and the mana team

00:02:42,010 --> 00:02:50,609
we found the overhead in KVM is less

00:02:46,450 --> 00:02:57,040
than 10% in Alibaba cloud when running

00:02:50,609 --> 00:03:04,659
custom business but in some cold case we

00:02:57,040 --> 00:03:08,290
still have some problems and according

00:03:04,659 --> 00:03:11,409
to our analyzed from and the user here

00:03:08,290 --> 00:03:15,449
is the Thule problem list we think is

00:03:11,409 --> 00:03:19,750
typical in the real business scenario

00:03:15,449 --> 00:03:22,810
the first problem is idle latency which

00:03:19,750 --> 00:03:25,780
means they always had or enter and exit

00:03:22,810 --> 00:03:29,829
a dope-ass in the kernel which is bigger

00:03:25,780 --> 00:03:33,220
than the native the second one is the

00:03:29,829 --> 00:03:36,430
timer the access of high precision timer

00:03:33,220 --> 00:03:40,299
cost performance ticker ischium in some

00:03:36,430 --> 00:03:44,650
online came across which access timer in

00:03:40,299 --> 00:03:47,409
val high frequency and the last issue is

00:03:44,650 --> 00:03:51,129
about the interrupt actually here means

00:03:47,409 --> 00:03:53,859
the multicast IP i which also blue in

00:03:51,129 --> 00:03:58,739
the performance technician in some

00:03:53,859 --> 00:03:58,739
extreme case when running the business

00:03:59,879 --> 00:04:05,799
okay the first issue is idle agency

00:04:02,979 --> 00:04:09,310
actually we have discussed that either

00:04:05,799 --> 00:04:14,079
latency several years ago in the QM

00:04:09,310 --> 00:04:17,799
forum 2013 Rick from Red Hat first

00:04:14,079 --> 00:04:22,210
described this usual and later David

00:04:17,799 --> 00:04:23,600
from Google also talks this issue in the

00:04:22,210 --> 00:04:27,200
kibum forum to sound

00:04:23,600 --> 00:04:29,570
fifteen the main problem for the

00:04:27,200 --> 00:04:33,620
isolation is that so for some

00:04:29,570 --> 00:04:37,070
message-passing workload it will be more

00:04:33,620 --> 00:04:40,580
than doubles law in the gas design

00:04:37,070 --> 00:04:42,920
native and the main over had is coming

00:04:40,580 --> 00:04:47,960
from the transition between running and

00:04:42,920 --> 00:04:52,090
idle in the vm and the conclusion from

00:04:47,960 --> 00:04:55,130
the QM forum 2015 even with some

00:04:52,090 --> 00:04:58,300
optimization in current KVM but we will

00:04:55,130 --> 00:05:04,520
still have some problem for the idle

00:04:58,300 --> 00:05:07,460
latency and here the data we get from

00:05:04,520 --> 00:05:10,130
one or little customer who is running

00:05:07,460 --> 00:05:15,230
the massive passive message-passing

00:05:10,130 --> 00:05:22,970
workload we can see from the picture the

00:05:15,230 --> 00:05:25,430
cube has a job from 100,000 to 6500 at

00:05:22,970 --> 00:05:28,310
fixed five sound when running in such

00:05:25,430 --> 00:05:31,960
work rudder in the VM and as a gap

00:05:28,310 --> 00:05:36,980
between the belmont and the vm is about

00:05:31,960 --> 00:05:41,290
35% so what happens to the vm when

00:05:36,980 --> 00:05:41,290
running such message passing workload

00:05:44,710 --> 00:05:52,430
here the typical workflow de all the

00:05:48,590 --> 00:05:57,140
message passing workload when running in

00:05:52,430 --> 00:05:59,330
the sim federal machine we can see from

00:05:57,140 --> 00:06:03,560
the picture firstly the cleanser will

00:05:59,330 --> 00:06:05,750
send a request to the server before

00:06:03,560 --> 00:06:11,240
receiving the lucrative the server is in

00:06:05,750 --> 00:06:15,680
the idle state and at after the request

00:06:11,240 --> 00:06:18,650
arrives the service state will translate

00:06:15,680 --> 00:06:22,070
the from the idle to run him to handle

00:06:18,650 --> 00:06:25,430
the request at the same time Crandall

00:06:22,070 --> 00:06:29,680
goes to idle it was an opening task in

00:06:25,430 --> 00:06:34,340
the CPU where the Kranti is running and

00:06:29,680 --> 00:06:37,280
aft server sent back to the results the

00:06:34,340 --> 00:06:40,480
current will go into running again

00:06:37,280 --> 00:06:44,360
to handle the look at handle the result

00:06:40,480 --> 00:06:51,740
so this is a one typical transaction to

00:06:44,360 --> 00:06:55,450
pass the message so what's happened to

00:06:51,740 --> 00:06:58,450
such a simple transaction

00:06:55,450 --> 00:07:03,680
this picture shows some low-level

00:06:58,450 --> 00:07:07,280
operations from Colonel aspect the first

00:07:03,680 --> 00:07:11,750
is the IP I when server is in other

00:07:07,280 --> 00:07:15,050
states the CPU ik ranch will send up

00:07:11,750 --> 00:07:17,390
here to wake up as a CP or server and we

00:07:15,050 --> 00:07:20,780
know the IP I will introduce cost under

00:07:17,390 --> 00:07:24,320
the key VM so this is the first issue

00:07:20,780 --> 00:07:29,200
the second part is part is a scheduler

00:07:24,320 --> 00:07:32,060
time before call into the low idle State

00:07:29,200 --> 00:07:34,600
kernel will stop the scheduler timer if

00:07:32,060 --> 00:07:38,560
we are using the ticklish kernel and

00:07:34,600 --> 00:07:41,810
this will result in the AP time access

00:07:38,560 --> 00:07:45,880
also a a peak time access will cause the

00:07:41,810 --> 00:07:51,500
via marketed and introduces a cost and

00:07:45,880 --> 00:07:54,190
the last one is the hot instruction the

00:07:51,500 --> 00:07:57,320
kernel will excuse the hot instruction

00:07:54,190 --> 00:08:01,250
try to call into the low idle State and

00:07:57,320 --> 00:08:03,880
the explosion the hot instruction will

00:08:01,250 --> 00:08:08,390
trap into the key BM to do the emulation

00:08:03,880 --> 00:08:11,510
so this will be the actual cost under

00:08:08,390 --> 00:08:15,050
the kibum and the the whole story path

00:08:11,510 --> 00:08:20,870
will make the whole path longer in the

00:08:15,050 --> 00:08:23,450
guest than the native actually this

00:08:20,870 --> 00:08:27,830
issue has been existed for a long time

00:08:23,450 --> 00:08:31,610
and we have did some optimizations in

00:08:27,830 --> 00:08:34,270
the past the first choice is to

00:08:31,610 --> 00:08:37,370
searching idle to the pole

00:08:34,270 --> 00:08:40,550
when you such a de trapo will limit the

00:08:37,370 --> 00:08:44,260
overhead or IP i and the hot but the

00:08:40,550 --> 00:08:47,390
side effects also is value obviously

00:08:44,260 --> 00:08:48,480
polling will waster the CPU cycles since

00:08:47,390 --> 00:08:53,220
we are doing

00:08:48,480 --> 00:08:55,980
the the full-time volume also it will

00:08:53,220 --> 00:08:58,829
help hyper-threading performance since

00:08:55,980 --> 00:09:03,120
the hyper-threading technology shares a

00:08:58,829 --> 00:09:06,930
cpu she has a cpu with another Hobart

00:09:03,120 --> 00:09:10,680
ready so if one CPU is doing the polling

00:09:06,930 --> 00:09:13,260
it will occupy the community computer

00:09:10,680 --> 00:09:18,870
unit inside the CPU and cause the other

00:09:13,260 --> 00:09:21,660
shredding depleted performance and

00:09:18,870 --> 00:09:23,880
another solution that we can disable the

00:09:21,660 --> 00:09:27,000
Chiklis kernel to avoid excessive

00:09:23,880 --> 00:09:30,510
scheduled time but problem is that the

00:09:27,000 --> 00:09:33,589
modern dispute distribution turns out

00:09:30,510 --> 00:09:37,230
into by default which means the Lu

00:09:33,589 --> 00:09:40,529
customer will leave it as what we

00:09:37,230 --> 00:09:44,579
shipped to them actually we never see

00:09:40,529 --> 00:09:49,500
one user to turn off it in Alibaba cloud

00:09:44,579 --> 00:09:52,740
and we have millions of users so no one

00:09:49,500 --> 00:09:57,779
have doing it so it's a review what's

00:09:52,740 --> 00:10:00,180
the distribution released and the last

00:09:57,779 --> 00:10:03,779
opposition that we have we called the

00:10:00,180 --> 00:10:05,610
kibum hot polling Kibum code polling

00:10:03,779 --> 00:10:08,790
will elements' the overhead from

00:10:05,610 --> 00:10:14,240
schedule after we executed a hot

00:10:08,790 --> 00:10:19,889
instruction in the guest post a three

00:10:14,240 --> 00:10:23,760
part will help to eliminate the part of

00:10:19,889 --> 00:10:31,649
the cost but now Adam can solve the

00:10:23,760 --> 00:10:35,880
problem even we use the hot polling in

00:10:31,649 --> 00:10:40,260
the QM we still see some latency inside

00:10:35,880 --> 00:10:46,260
of us hear the conclusion problem came

00:10:40,260 --> 00:10:52,970
column to sound 15 it'll remain has the

00:10:46,260 --> 00:10:52,970
latency in the latest KVM

00:10:53,340 --> 00:10:59,940
the first listen to come from the msi

00:10:56,100 --> 00:11:02,840
rights to the epic timer and it will

00:10:59,940 --> 00:11:07,620
bring a lot of fluid to five

00:11:02,840 --> 00:11:10,050
microseconds latency and also ipi costs

00:11:07,620 --> 00:11:14,850
about the to microsecond in this case

00:11:10,050 --> 00:11:20,310
and the hottest will add about three

00:11:14,850 --> 00:11:23,130
microsecond latency and the situation is

00:11:20,310 --> 00:11:26,430
even worse when we run the multi PMS

00:11:23,130 --> 00:11:34,770
because the hot polling will not in fact

00:11:26,430 --> 00:11:37,580
in this case so here is our solution we

00:11:34,770 --> 00:11:40,860
use the idle pole instead hot pole

00:11:37,580 --> 00:11:44,760
actually we do the poll inside the guest

00:11:40,860 --> 00:11:47,330
not in the qm and we do the poll just

00:11:44,760 --> 00:11:51,360
after setting the polling beat and

00:11:47,330 --> 00:11:56,520
before touch the scheduler timer in the

00:11:51,360 --> 00:11:59,880
idle code path and if the port is

00:11:56,520 --> 00:12:02,450
successful actually there's no need to

00:11:59,880 --> 00:12:07,350
go into the idle and there's no need to

00:12:02,450 --> 00:12:09,150
execute the hot instruction instead cpu

00:12:07,350 --> 00:12:13,580
will continue to handle the pending

00:12:09,150 --> 00:12:16,980
tasks and still in the running state

00:12:13,580 --> 00:12:20,190
also because of a pope before the before

00:12:16,980 --> 00:12:23,370
we touch the scheduled timer so if post

00:12:20,190 --> 00:12:26,940
successful there's no need to call in to

00:12:23,370 --> 00:12:31,190
to stop at the scheduled time and it has

00:12:26,940 --> 00:12:31,190
no apical time accessed via magazine

00:12:31,760 --> 00:12:40,620
also we are using a dynamic poll we

00:12:36,990 --> 00:12:42,810
don't use using a full time call we will

00:12:40,620 --> 00:12:45,210
change the port time according to the

00:12:42,810 --> 00:12:49,860
run time calculation this means if

00:12:45,210 --> 00:12:51,900
there's no more cloud in the cpu we will

00:12:49,860 --> 00:12:55,800
know to the power and this will help to

00:12:51,900 --> 00:12:58,190
reduce the cpu wasting simple cycle

00:12:55,800 --> 00:12:58,190
wasting

00:12:59,130 --> 00:13:08,910
and here the days way from our customer

00:13:04,820 --> 00:13:15,030
we can see the QPR is improved from the

00:13:08,910 --> 00:13:19,610
65 70 to 90 Solander with at the poll

00:13:15,030 --> 00:13:27,060
and the gap is only about the 10% after

00:13:19,610 --> 00:13:30,150
using the idol poll and here the data

00:13:27,060 --> 00:13:33,090
from our benchmark we are one in the a

00:13:30,150 --> 00:13:36,960
file relating the test to compare the

00:13:33,090 --> 00:13:38,940
hot pot and the idle poll in this case

00:13:36,960 --> 00:13:41,210
we will measure the latency of the

00:13:38,940 --> 00:13:44,730
forked escalator

00:13:41,210 --> 00:13:48,390
also in the each region we will reach

00:13:44,730 --> 00:13:52,860
some microsecond before the next grid to

00:13:48,390 --> 00:13:56,400
match a new scenario in our cloud so we

00:13:52,860 --> 00:13:58,740
can see from the second column it's a

00:13:56,400 --> 00:14:00,930
default value which means that we will

00:13:58,740 --> 00:14:06,600
turn off of the hot pot at the time of

00:14:00,930 --> 00:14:11,820
the idle power so in this case it's the

00:14:06,600 --> 00:14:15,480
default result in the QM and from the

00:14:11,820 --> 00:14:19,050
right column data we can see both the

00:14:15,480 --> 00:14:22,860
hot pot and at the poor can reduce the

00:14:19,050 --> 00:14:25,950
latency and the from our did the idle

00:14:22,860 --> 00:14:30,420
poll looks like better than the hot pole

00:14:25,950 --> 00:14:34,800
in this testing and we can see the best

00:14:30,420 --> 00:14:38,280
result is we tried to channel the hot

00:14:34,800 --> 00:14:41,060
pot and Idaho together in this case

00:14:38,280 --> 00:14:44,820
it'll be ok with the best performance

00:14:41,060 --> 00:14:48,450
but also during a testing we saw some

00:14:44,820 --> 00:14:51,390
problem for with the Idaho we can see

00:14:48,450 --> 00:14:54,390
the standard would understand the

00:14:51,390 --> 00:15:00,030
deviation in the 5 micro second detached

00:14:54,390 --> 00:15:03,690
team the division is about 9 compared to

00:15:00,030 --> 00:15:07,170
previously only about 3 so this means

00:15:03,690 --> 00:15:11,000
our dynamic algorithm cannot meet all

00:15:07,170 --> 00:15:16,700
the kids and we still have

00:15:11,000 --> 00:15:17,600
room to improve our algorithm this is

00:15:16,700 --> 00:15:20,450
the father

00:15:17,600 --> 00:15:24,430
I do letting the problem and our

00:15:20,450 --> 00:15:29,690
solution to so wait

00:15:24,430 --> 00:15:32,440
and I mentioned before we have a solute

00:15:29,690 --> 00:15:37,820
problem and the second one is the timer

00:15:32,440 --> 00:15:40,370
actually it is high pollution time we

00:15:37,820 --> 00:15:45,650
know the timer is used widely in the

00:15:40,370 --> 00:15:49,010
whole scenario but typically in unlikely

00:15:45,650 --> 00:15:55,610
means narrow they use a lot of high

00:15:49,010 --> 00:15:58,480
precision time to do the poll and in the

00:15:55,610 --> 00:16:01,610
loose narrow we have seen lots of timer

00:15:58,480 --> 00:16:06,910
around the tens of microseconds so it's

00:16:01,610 --> 00:16:10,400
a very small timer so in such case the

00:16:06,910 --> 00:16:13,010
latency of the timer will impact the

00:16:10,400 --> 00:16:15,920
performance obviously because the timer

00:16:13,010 --> 00:16:19,040
is so small it's only about the ten tens

00:16:15,920 --> 00:16:21,320
of microseconds so from the picture we

00:16:19,040 --> 00:16:25,190
can see this is a typical time flow

00:16:21,320 --> 00:16:27,320
inside the VM so where VM tries to set

00:16:25,190 --> 00:16:31,490
the timer it should be alright right to

00:16:27,320 --> 00:16:33,980
the local apical largest because we are

00:16:31,490 --> 00:16:36,170
using a local epoch a timer and writer

00:16:33,980 --> 00:16:40,760
too low epoch largest will cause the VM

00:16:36,170 --> 00:16:43,790
accident KVM to do the emulation and at

00:16:40,760 --> 00:16:47,270
the same time the vc pol depositor and a

00:16:43,790 --> 00:16:51,800
tubular chap to the k vm also after

00:16:47,270 --> 00:16:55,130
timer is expired the timer event will

00:16:51,800 --> 00:16:59,210
interrupt the vm if it is running in the

00:16:55,130 --> 00:17:02,410
CPU and the CPU will chop into the Kibum

00:16:59,210 --> 00:17:05,990
again to do the virtual time injection

00:17:02,410 --> 00:17:09,290
so from this picture we can see one time

00:17:05,990 --> 00:17:15,250
a program actually will cost to the AMEX

00:17:09,290 --> 00:17:18,680
it and it will chap to have wiser twice

00:17:15,250 --> 00:17:21,110
actually for the timer whole whole

00:17:18,680 --> 00:17:24,800
explanation is far away from now it

00:17:21,110 --> 00:17:27,470
shouldn't hotter performance but

00:17:24,800 --> 00:17:30,520
the time expansion time is very far

00:17:27,470 --> 00:17:34,940
behind now but for the time I mentioned

00:17:30,520 --> 00:17:38,300
before who experimentation time is only

00:17:34,940 --> 00:17:41,420
about the tens of microseconds those vm

00:17:38,300 --> 00:17:46,760
exceed will be a big problem in such

00:17:41,420 --> 00:17:52,490
case actually so far there is no good

00:17:46,760 --> 00:17:57,440
solution in the x86 platform and after

00:17:52,490 --> 00:18:01,150
an any after analysis we develop a pivot

00:17:57,440 --> 00:18:05,300
timer which we called accident this time

00:18:01,150 --> 00:18:09,250
with this accident is time we use shadow

00:18:05,300 --> 00:18:12,860
page to store the timer information and

00:18:09,250 --> 00:18:16,700
those chapel is shared between VM and

00:18:12,860 --> 00:18:19,100
hypervisor for example when VM tried to

00:18:16,700 --> 00:18:23,870
set a timer it will store the time

00:18:19,100 --> 00:18:28,250
information into the share page then the

00:18:23,870 --> 00:18:31,130
agent from another tip you who will scan

00:18:28,250 --> 00:18:35,030
the page regularly and the set of time

00:18:31,130 --> 00:18:39,860
in the new hardware and after the timer

00:18:35,030 --> 00:18:43,430
is a father actually it is on the CPU

00:18:39,860 --> 00:18:48,710
whether 18 the timer stays in so it will

00:18:43,430 --> 00:18:51,310
not interrupt the VM and we will delete

00:18:48,710 --> 00:18:54,770
deliver the virtual time interrupt to VM

00:18:51,310 --> 00:19:01,310
through the post interrupts which will

00:18:54,770 --> 00:19:05,350
not cause any vm boxes to the VM so here

00:19:01,310 --> 00:19:08,900
the detail or the share page as I

00:19:05,350 --> 00:19:11,630
mentioned before we used share page to

00:19:08,900 --> 00:19:14,890
share information between guest and the

00:19:11,630 --> 00:19:18,980
AVM actually there are two messages

00:19:14,890 --> 00:19:22,010
inside this cabbage one is the gas the

00:19:18,980 --> 00:19:24,770
timer information which will be check

00:19:22,010 --> 00:19:27,020
the next in the guest and the other

00:19:24,770 --> 00:19:29,510
another one is the next synchronised

00:19:27,020 --> 00:19:32,120
information which is right by the edge

00:19:29,510 --> 00:19:34,250
in the time here we are not using

00:19:32,120 --> 00:19:36,800
polling in the

00:19:34,250 --> 00:19:39,800
since as I mentioned before polling will

00:19:36,800 --> 00:19:42,500
kill back to man to marry CPU cycles and

00:19:39,800 --> 00:19:48,410
hotter performance so in this solution

00:19:42,500 --> 00:19:51,380
we use a periodic timer as agent and we

00:19:48,410 --> 00:19:53,810
will record the time step over the next

00:19:51,380 --> 00:19:56,630
period other time in the Shred page and

00:19:53,810 --> 00:20:01,820
the gas caster can catch the information

00:19:56,630 --> 00:20:05,720
without any be a magazine for example

00:20:01,820 --> 00:20:08,840
when a BCP you try to set a timer it

00:20:05,720 --> 00:20:14,240
will record the next signal time step

00:20:08,840 --> 00:20:17,900
and a comparator with the timer I tried

00:20:14,240 --> 00:20:21,760
to program it was an app to timer is

00:20:17,900 --> 00:20:24,620
longer than next signal the time step

00:20:21,760 --> 00:20:27,470
then the visa view we just store the

00:20:24,620 --> 00:20:30,110
time information in the shop page and

00:20:27,470 --> 00:20:35,300
it's a continue to learn it without each

00:20:30,110 --> 00:20:38,720
other to have wiser and when the signal

00:20:35,300 --> 00:20:41,330
timer is powder in the agency timer it

00:20:38,720 --> 00:20:45,050
will scan the page type a share page and

00:20:41,330 --> 00:20:47,150
to look at the gas a time also a table

00:20:45,050 --> 00:20:51,200
set to this gas the virtual time in the

00:20:47,150 --> 00:20:54,110
hard work and at the same time he will

00:20:51,200 --> 00:20:57,440
record the next signal the time step in

00:20:54,110 --> 00:21:03,050
the shop page again and some time later

00:20:57,440 --> 00:21:04,670
the gas virtual time is expired and at

00:21:03,050 --> 00:21:09,500
the time in trouble we delivered to a

00:21:04,670 --> 00:21:12,650
CPU we're entering the time stays in the

00:21:09,500 --> 00:21:15,590
aging time handler it will inject the

00:21:12,650 --> 00:21:20,120
virtual time virtual time into VM

00:21:15,590 --> 00:21:24,080
through the post interrupt and here had

00:21:20,120 --> 00:21:26,210
an OPM exit and a for the next time it

00:21:24,080 --> 00:21:28,790
is smaller than the next signal timer

00:21:26,210 --> 00:21:31,130
then we were back to the current

00:21:28,790 --> 00:21:32,930
approach we still will check the IBM

00:21:31,130 --> 00:21:37,060
magazine and the chabot through the

00:21:32,930 --> 00:21:37,060
hypervisor to do the emulation

00:21:38,999 --> 00:21:45,479
and here is the detail with age in the

00:21:41,879 --> 00:21:50,489
time as I mentioned before it's abuse

00:21:45,479 --> 00:21:53,399
can share page periodically and that the

00:21:50,489 --> 00:21:56,970
for the time interval in our solution is

00:21:53,399 --> 00:22:01,859
one millisecond but the guest but the

00:21:56,970 --> 00:22:04,470
user can change it manually and in the

00:22:01,859 --> 00:22:07,049
handler in the aging the timer it will

00:22:04,470 --> 00:22:12,769
store the next signal the time step in

00:22:07,049 --> 00:22:12,769
the shop page then the VM will know it

00:22:14,090 --> 00:22:21,330
also in our solution aging the timer

00:22:18,239 --> 00:22:25,159
stays in a dedicated CPU which is

00:22:21,330 --> 00:22:28,019
different from where the VM will

00:22:25,159 --> 00:22:30,029
otherwise the time Avenger may interrupt

00:22:28,019 --> 00:22:33,989
the VM and for the eight-minute ham and

00:22:30,029 --> 00:22:36,419
the VM in the same physical CPU also in

00:22:33,989 --> 00:22:41,580
this case the post interrupt cannot take

00:22:36,419 --> 00:22:50,429
effect so here is our exit at this time

00:22:41,580 --> 00:22:53,369
and here is the date we get when running

00:22:50,429 --> 00:22:57,450
a time intensive workloads actually it's

00:22:53,369 --> 00:23:03,809
a route benchmark from our user which

00:22:57,450 --> 00:23:05,539
will upload the time intensive the left

00:23:03,809 --> 00:23:10,669
pillow is the latency when running

00:23:05,539 --> 00:23:15,840
environment we can see it is about 2100

00:23:10,669 --> 00:23:19,559
nanosecond and the Middle Pillar is is

00:23:15,840 --> 00:23:22,739
running in the VM without our

00:23:19,559 --> 00:23:28,529
occidentalis time we can say the latency

00:23:22,739 --> 00:23:33,440
about 3500 not a second and the right

00:23:28,529 --> 00:23:36,330
one is latency after using our oxygen

00:23:33,440 --> 00:23:42,869
exceeding this time we can see the

00:23:36,330 --> 00:23:46,580
latencies about 2300 nanosecond and this

00:23:42,869 --> 00:23:49,739
is a very close to the per mental data

00:23:46,580 --> 00:23:53,239
so actually accidentally the timer helps

00:23:49,739 --> 00:23:53,239
a lot in this case

00:23:58,400 --> 00:24:04,280
and the last issue I want to talk about

00:24:01,250 --> 00:24:06,950
is interrupt actually to be more

00:24:04,280 --> 00:24:11,270
accurate the in trouble here means the

00:24:06,950 --> 00:24:13,990
multicast IP I in some sneller

00:24:11,270 --> 00:24:18,470
we know that shall be flash is a larger

00:24:13,990 --> 00:24:22,190
user in the kernel so what happened to

00:24:18,470 --> 00:24:27,680
the motor IP is when in the km before

00:24:22,190 --> 00:24:30,950
calling to detail that has to have some

00:24:27,680 --> 00:24:35,900
knowledge about the apical how to send

00:24:30,950 --> 00:24:38,900
an IP I in the x86 platform we use a

00:24:35,900 --> 00:24:41,780
people to send IP I and actually a pika

00:24:38,900 --> 00:24:44,990
have the two mode one is X epic mode

00:24:41,780 --> 00:24:48,290
which will call the last mode and the

00:24:44,990 --> 00:24:50,390
other one is at - epic mode and the

00:24:48,290 --> 00:24:55,130
different mode use a different way to

00:24:50,390 --> 00:24:59,030
send an IP I from this slide we can see

00:24:55,130 --> 00:25:02,320
in the X epic mode leaders cannot handle

00:24:59,030 --> 00:25:07,160
the IP I also in the two different way

00:25:02,320 --> 00:25:09,320
when the CPU number is less than 8 the

00:25:07,160 --> 00:25:14,800
colonel will set a pig in the logic

00:25:09,320 --> 00:25:20,470
model and with flat with the flat model

00:25:14,800 --> 00:25:25,760
in this mode right to the apical address

00:25:20,470 --> 00:25:28,610
once can send IP I to all CPUs and the

00:25:25,760 --> 00:25:33,380
right to apical just once also can send

00:25:28,610 --> 00:25:36,710
an IP I to multi CPUs but if the CPU

00:25:33,380 --> 00:25:40,010
number is more than 8 Connor will set a

00:25:36,710 --> 00:25:43,870
pig in physical motor in the physical

00:25:40,010 --> 00:25:46,550
model if we won send IP i to more CPUs

00:25:43,870 --> 00:25:50,510
it need to write to the apical regice

00:25:46,550 --> 00:25:53,810
more times because one time for one IP I

00:25:50,510 --> 00:25:56,780
for for example if we wonder sender's IP

00:25:53,810 --> 00:26:00,080
I to three CPUs actually when you write

00:25:56,780 --> 00:26:07,060
to apical register three times and this

00:26:00,080 --> 00:26:09,750
is how X epic model walk in the QM and

00:26:07,060 --> 00:26:13,860
the fall up to a pig

00:26:09,750 --> 00:26:17,610
they also have two modes one in the

00:26:13,860 --> 00:26:20,850
class mode which is the preferred mode

00:26:17,610 --> 00:26:23,940
by Cano if actually peak mode is

00:26:20,850 --> 00:26:27,660
available in hardware Connor will use a

00:26:23,940 --> 00:26:31,860
class mode by default and the in class

00:26:27,660 --> 00:26:37,020
mode it can send the IP i per cast also

00:26:31,860 --> 00:26:40,110
one cluster can have at most 16 CPUs so

00:26:37,020 --> 00:26:43,440
this means we can send IP ID to 16 CPU

00:26:40,110 --> 00:26:50,130
without with one right to the apical

00:26:43,440 --> 00:26:53,580
register but sometimes the tactical CPU

00:26:50,130 --> 00:26:58,290
may not in one class so we still need

00:26:53,580 --> 00:27:02,360
more right to epoch Rajas and the time

00:26:58,290 --> 00:27:04,920
depends on how many classes we have and

00:27:02,360 --> 00:27:09,540
for the physical model at actually it

00:27:04,920 --> 00:27:11,700
seemed in the epoch mode we need more

00:27:09,540 --> 00:27:19,290
lights to apical register if we want to

00:27:11,700 --> 00:27:19,830
send IP I to more CPUs so what's the

00:27:19,290 --> 00:27:23,880
problem

00:27:19,830 --> 00:27:27,480
under the key VM with the API actually

00:27:23,880 --> 00:27:29,940
in the virtualization fusion center IPR

00:27:27,480 --> 00:27:33,360
is emulated by the key VM this means

00:27:29,940 --> 00:27:35,670
each write to the apical register to

00:27:33,360 --> 00:27:39,930
send the IPA will cause a VM exit and

00:27:35,670 --> 00:27:42,210
champion to the VM and as I mentioned

00:27:39,930 --> 00:27:47,310
before it with the be CPU number is less

00:27:42,210 --> 00:27:49,800
than 8 then we can use X a pig with the

00:27:47,310 --> 00:27:54,900
logical flood mode to send IP 8 motor

00:27:49,800 --> 00:27:58,350
CPU and with one a pinata is enough but

00:27:54,900 --> 00:28:01,050
if CPU number is more than 8 multicast

00:27:58,350 --> 00:28:02,630
IP I will cost motive VM exit 2

00:28:01,050 --> 00:28:05,340
hypervisor

00:28:02,630 --> 00:28:08,930
also because there is no interrupt will

00:28:05,340 --> 00:28:13,410
be mapped in supporter in current Olivia

00:28:08,930 --> 00:28:18,750
so after a pig with class motor will not

00:28:13,410 --> 00:28:21,179
work in the guest and the kernel running

00:28:18,750 --> 00:28:23,590
inside the key inside the VM can only

00:28:21,179 --> 00:28:26,139
set the apt to epic mode

00:28:23,590 --> 00:28:29,350
in physical manga and as I mentioned

00:28:26,139 --> 00:28:34,480
before in feeding mode one writer too

00:28:29,350 --> 00:28:37,269
epic can only send one IP so in the VM

00:28:34,480 --> 00:28:40,210
if we want to send the IPL to motiva

00:28:37,269 --> 00:28:45,179
spew etting you to write epic man tab

00:28:40,210 --> 00:28:51,539
and each light will cause one VM exit

00:28:45,179 --> 00:28:56,889
and in our solution we use P VI P H so

00:28:51,539 --> 00:28:59,080
this problem with the PV idea that no

00:28:56,889 --> 00:29:03,909
need to write a people register to send

00:28:59,080 --> 00:29:06,700
IP I instead FV want to send an IP h mo

00:29:03,909 --> 00:29:11,619
disappears it will set the destination

00:29:06,700 --> 00:29:14,320
Scipio in a bitmap their interview

00:29:11,619 --> 00:29:18,820
trigger-happy code to notify the adviser

00:29:14,320 --> 00:29:21,789
that he wanted to send the IPS and in a

00:29:18,820 --> 00:29:24,100
happier in the IP I handle gibeom will

00:29:21,789 --> 00:29:26,950
scan the bitmap to catch the target the

00:29:24,100 --> 00:29:31,809
CPU which is in the bitmap and send IP

00:29:26,950 --> 00:29:35,230
ID to them and in this case regardless

00:29:31,809 --> 00:29:40,470
how much CPU we want to send IP I only

00:29:35,230 --> 00:29:44,610
need one epic one right and it only

00:29:40,470 --> 00:29:44,610
costs one via magazine

00:29:46,860 --> 00:29:53,110
so here the data with the message around

00:29:49,899 --> 00:29:56,230
the middle world message-oriented

00:29:53,110 --> 00:29:59,110
middleware will use family social

00:29:56,230 --> 00:30:02,289
messages and the share memory will cost

00:29:59,110 --> 00:30:06,759
a lot so Java flash which will Center

00:30:02,289 --> 00:30:10,299
multi ideas and in this testing we can

00:30:06,759 --> 00:30:17,470
see the Verity the old ipi is reduced

00:30:10,299 --> 00:30:21,610
from 800,000 to 150 sound with our p

00:30:17,470 --> 00:30:29,190
rocker so from this data we can say P

00:30:21,610 --> 00:30:29,190
VIP I can reduce the VM exit helpful

00:30:32,630 --> 00:30:38,340
okay this is the searching that I want

00:30:36,420 --> 00:30:45,869
to check so an attached

00:30:38,340 --> 00:30:45,869
[Applause]

00:30:50,509 --> 00:30:55,950
in through your slice so I saw all the

00:30:53,909 --> 00:30:58,049
benchmarking example is on the Linux VM

00:30:55,950 --> 00:31:00,450
do you ever done anything on the windows

00:30:58,049 --> 00:31:01,830
VM or the windows VM doesn't have if you

00:31:00,450 --> 00:31:04,230
run the same were loaded windows VM that

00:31:01,830 --> 00:31:07,230
I have the problem yeah because the

00:31:04,230 --> 00:31:11,159
window some modification can get kernel

00:31:07,230 --> 00:31:14,070
so it is really hard to do in the

00:31:11,159 --> 00:31:16,889
windows together so our benchmark I run

00:31:14,070 --> 00:31:20,639
in the VM guests and our patch also

00:31:16,889 --> 00:31:23,039
developer by the Linux gangster right

00:31:20,639 --> 00:31:24,359
exactly so that's that's my next

00:31:23,039 --> 00:31:26,070
question it's on Windows so you actually

00:31:24,359 --> 00:31:29,039
kind of have to provide any support

00:31:26,070 --> 00:31:31,080
right so if you use to the older hyper-v

00:31:29,039 --> 00:31:35,549
latinum support then you might I was

00:31:31,080 --> 00:31:38,369
running into those problem actually if

00:31:35,549 --> 00:31:40,470
we if I happy we came back puzzled

00:31:38,369 --> 00:31:43,320
patched and I think type of account also

00:31:40,470 --> 00:31:45,269
benefited from the opportunity no I'm

00:31:43,320 --> 00:31:47,249
saying the KVM probably already

00:31:45,269 --> 00:31:50,129
supported all those are high very

00:31:47,249 --> 00:31:52,919
enlightening so yeah if you run the same

00:31:50,129 --> 00:31:56,429
similar workload on Windows you might

00:31:52,919 --> 00:31:59,669
not run into those issues we never tried

00:31:56,429 --> 00:32:02,519
the cloud in the windows actually we

00:31:59,669 --> 00:32:05,460
don't know which happy mail item and the

00:32:02,519 --> 00:32:07,200
feature can solve the problem my mirrors

00:32:05,460 --> 00:32:08,609
they have some feature in the windows

00:32:07,200 --> 00:32:17,519
already yeah they do

00:32:08,609 --> 00:32:19,679
okay so I don't want to to make a long

00:32:17,519 --> 00:32:21,960
question but I'm interested in the part

00:32:19,679 --> 00:32:25,080
you said about the pooling CPU where you

00:32:21,960 --> 00:32:27,330
cause 2 VM exits so we're facing kind of

00:32:25,080 --> 00:32:30,299
the same situation I want to know from

00:32:27,330 --> 00:32:33,239
you if you have tested for example

00:32:30,299 --> 00:32:36,269
different sea states level when the VM

00:32:33,239 --> 00:32:40,080
exit is going you know and then you have

00:32:36,269 --> 00:32:42,509
the CPU idling or having no callbacks

00:32:40,080 --> 00:32:46,039
for our see you in that CPU if you have

00:32:42,509 --> 00:32:48,389
tested this having no rco callbacks and

00:32:46,039 --> 00:32:51,359
also they know Hertz foo if you have

00:32:48,389 --> 00:32:53,220
tested no hurtsfoe option or if you have

00:32:51,359 --> 00:32:55,300
tested all of them which is the best

00:32:53,220 --> 00:32:58,300
combination that provides

00:32:55,300 --> 00:33:03,330
the best timing for not having so much

00:32:58,300 --> 00:33:07,090
frequent CPU idling for that situation

00:33:03,330 --> 00:33:10,890
yeah so you mean we use Angwin

00:33:07,090 --> 00:33:14,140
yeah the turbo vm exit so you basically

00:33:10,890 --> 00:33:16,480
provided a patch right to make a PO and

00:33:14,140 --> 00:33:18,850
not allow the CPU to idle so much I

00:33:16,480 --> 00:33:21,790
wanted to know if you have tested other

00:33:18,850 --> 00:33:24,640
options like disabling callbacks for RC

00:33:21,790 --> 00:33:27,040
use for that CPU having no Hertz who

00:33:24,640 --> 00:33:29,320
enabled for that CPU also you know that

00:33:27,040 --> 00:33:33,370
the one CPU that faces this have you

00:33:29,320 --> 00:33:36,550
tested this no actually we didn't ICO

00:33:33,370 --> 00:33:39,490
cause in our testing all the testing we

00:33:36,550 --> 00:33:42,010
have seen for example in the time case

00:33:39,490 --> 00:33:44,950
actually we only see the we are

00:33:42,010 --> 00:33:48,010
magaziner to program the timer and for

00:33:44,950 --> 00:33:51,700
the idle case actually we only see the

00:33:48,010 --> 00:33:57,120
three cost and then no hace you involved

00:33:51,700 --> 00:33:57,120
in our testing okay thank you

00:34:10,160 --> 00:34:16,250
I lost a short question about your cloud

00:34:13,970 --> 00:34:19,520
infrastructure you're obviously using

00:34:16,250 --> 00:34:22,750
KVM are you also using qmu to try that

00:34:19,520 --> 00:34:27,800
KVM or do you have your own user space

00:34:22,750 --> 00:34:31,670
do you use qmu as a user space for KVM

00:34:27,800 --> 00:34:35,930
or do you have your private user space

00:34:31,670 --> 00:34:38,380
application as a hypervisor stand

00:34:35,930 --> 00:34:38,380
accumulo

00:34:49,970 --> 00:34:56,790
tlb flash have you tried to implement a

00:34:53,069 --> 00:34:59,579
para virtual they'll be flush instead of

00:34:56,790 --> 00:35:01,170
the para virtual IP I to do it because

00:34:59,579 --> 00:35:04,410
there could be some more optimizations

00:35:01,170 --> 00:35:09,380
to make the KVM would do the TLB flush

00:35:04,410 --> 00:35:12,299
for the guest instead of sending an IP I

00:35:09,380 --> 00:35:14,490
actually this is jump-roping from gas

00:35:12,299 --> 00:35:24,569
aside it's not funky ramchandra

00:35:14,490 --> 00:35:28,230
if ya interrupt to all of the CPUs that

00:35:24,569 --> 00:35:30,630
need the TLB flush and then the B CPUs

00:35:28,230 --> 00:35:33,420
actually received the interrupt and do

00:35:30,630 --> 00:35:36,480
something but KVM could do the TLB flush

00:35:33,420 --> 00:35:47,670
itself without bothering the target miss

00:35:36,480 --> 00:35:50,849
abuse to deliver the interrupts itself

00:35:47,670 --> 00:35:55,319
we can just do the deal be flushing KVM

00:35:50,849 --> 00:35:57,720
without like delivering it so instead of

00:35:55,319 --> 00:36:01,470
a virtual IP I if you've tried the para

00:35:57,720 --> 00:36:03,650
virtual TLB flush I prefer so you mean

00:36:01,470 --> 00:36:07,530
with a Java project

00:36:03,650 --> 00:36:11,549
yeah but yeah you need the VM exited the

00:36:07,530 --> 00:36:14,420
CPU is running but if it's not then KVM

00:36:11,549 --> 00:36:14,420
can take care of it

00:36:19,330 --> 00:36:24,370
I think there's a part of the Linux

00:36:20,800 --> 00:36:29,380
memory management that relies on the TLB

00:36:24,370 --> 00:36:31,750
flush epi for synchronization so in

00:36:29,380 --> 00:36:35,340
addition to doing a per virtual TLB

00:36:31,750 --> 00:36:35,340
flush you would also have to fix that

00:36:36,360 --> 00:36:41,920
but you could get even better

00:36:38,800 --> 00:36:44,170
performance then and a pair of virtual

00:36:41,920 --> 00:36:46,870
IP because if a vcp is halted you can

00:36:44,170 --> 00:36:49,530
just wait until it resumed to do ta

00:36:46,870 --> 00:36:49,530
flush them

00:37:00,820 --> 00:37:06,490
wait up you just had that they share you

00:37:03,820 --> 00:37:09,760
for your the Castro right information to

00:37:06,490 --> 00:37:12,610
the shared info and then at a hypervisor

00:37:09,760 --> 00:37:16,540
the hypervisor scan it's theoretically

00:37:12,610 --> 00:37:19,060
right is that any problem for for the

00:37:16,540 --> 00:37:21,760
timer for example if the timer is a lot

00:37:19,060 --> 00:37:28,600
of a salutely timer is our relative

00:37:21,760 --> 00:37:31,540
timer so is there any influence to it so

00:37:28,600 --> 00:37:34,540
I mean if you don't if the guests don't

00:37:31,540 --> 00:37:37,120
use the timer like to see tell a timer

00:37:34,540 --> 00:37:40,090
there Lattimer is absolutely timer so is

00:37:37,120 --> 00:37:44,080
okay but if the timer is our relative

00:37:40,090 --> 00:37:47,740
timer it depends on that it depends on

00:37:44,080 --> 00:37:51,670
the time you scan the bit so the baby

00:37:47,740 --> 00:37:53,220
maybe there is some screw of the timer

00:37:51,670 --> 00:37:55,870
[Music]

00:37:53,220 --> 00:37:59,200
for the kernel there's only two options

00:37:55,870 --> 00:38:01,600
wise Larkin local pick a time and one in

00:37:59,200 --> 00:38:04,330
the chassis deadlines hammer and if we

00:38:01,600 --> 00:38:07,660
just use another time source for example

00:38:04,330 --> 00:38:11,550
the pits time actually we cannot help in

00:38:07,660 --> 00:38:11,550
this case okay thank you

00:38:27,119 --> 00:38:36,539
oh I have a question about the PV timer

00:38:31,499 --> 00:38:39,749
so what's the period of the synchronizer

00:38:36,539 --> 00:38:42,420
timer of the dedicated CPU but the

00:38:39,749 --> 00:38:44,269
folder will use one millisecond but it

00:38:42,420 --> 00:38:48,779
can be cheated by the user

00:38:44,269 --> 00:38:51,329
okay I mean actually it was the time the

00:38:48,779 --> 00:38:56,369
timer is very short mm-hmm

00:38:51,329 --> 00:38:57,539
I think and period to expiration of the

00:38:56,369 --> 00:39:01,259
guest a timer

00:38:57,539 --> 00:39:05,640
maybe maybe shutters and you're saying

00:39:01,259 --> 00:39:07,109
synchronize the timer so yeah handle

00:39:05,640 --> 00:39:09,239
with this situation

00:39:07,109 --> 00:39:11,369
yeah actually with the pivot timer we

00:39:09,239 --> 00:39:13,289
can not eliminate all the be a Mac user

00:39:11,369 --> 00:39:15,119
to program time I with the timer is a

00:39:13,289 --> 00:39:17,009
very short actually we will go back to

00:39:15,119 --> 00:39:21,210
the other way to check it'll be a

00:39:17,009 --> 00:39:24,239
magazine to handle it but in lieu in the

00:39:21,210 --> 00:39:27,539
newest nano we we observe that there are

00:39:24,239 --> 00:39:29,880
lots of time in the system and we can

00:39:27,539 --> 00:39:32,640
have we can limit Pat Holzem and not

00:39:29,880 --> 00:39:35,400
audible but if we you set the interval

00:39:32,640 --> 00:39:38,849
to be a smaller value than you can a

00:39:35,400 --> 00:39:41,549
limo most part of it but it may hurt the

00:39:38,849 --> 00:39:45,680
performance because you use a lot of

00:39:41,549 --> 00:39:45,680
timers yeah okay thank you

00:40:02,269 --> 00:40:06,930
faces will sorry the power of virtual

00:40:04,920 --> 00:40:10,289
interfaces will help Linux but it will

00:40:06,930 --> 00:40:15,150
be hard to get into Windows do you have

00:40:10,289 --> 00:40:18,150
any ideas of hypervisor or Hardware

00:40:15,150 --> 00:40:19,859
extensions that could also fix these

00:40:18,150 --> 00:40:25,589
performance problems so that any guest

00:40:19,859 --> 00:40:29,069
to us would perform well do you have any

00:40:25,589 --> 00:40:33,450
ideas of like virtualization extensions

00:40:29,069 --> 00:40:36,599
like new extensions to the intel vt-x

00:40:33,450 --> 00:40:38,749
instructions that could get the same

00:40:36,599 --> 00:40:55,140
performance gains so that any guest OS

00:40:38,749 --> 00:40:57,690
would you know poor for perform well how

00:40:55,140 --> 00:41:01,440
do I support yeah do you have any ideas

00:40:57,690 --> 00:41:04,499
of new hardware support that could they

00:41:01,440 --> 00:41:07,499
could be useful yes so actually we

00:41:04,499 --> 00:41:10,079
didn't have any hardware support in this

00:41:07,499 --> 00:41:12,900
case but we have talking such idea to

00:41:10,079 --> 00:41:15,779
Intel for example the pivot hammer we

00:41:12,900 --> 00:41:18,059
want to interact to enhance in in the

00:41:15,779 --> 00:41:20,579
hardware actually I know in the arm

00:41:18,059 --> 00:41:22,979
platform which has such a virtual time

00:41:20,579 --> 00:41:26,609
that caster can program it interactive

00:41:22,979 --> 00:41:28,619
without an abuser but so far I don't see

00:41:26,609 --> 00:41:31,249
the Intel have such branches all date

00:41:28,619 --> 00:41:31,249
yeah

00:41:37,040 --> 00:41:42,660
okay okay that's always thank you

00:41:41,400 --> 00:41:48,940
[Applause]

00:41:42,660 --> 00:41:48,940

YouTube URL: https://www.youtube.com/watch?v=i3kNI7hTF8g


