Title: [2017] Improve VNF Safety with Vhost-User DPDK IOMMU Support by Maxime Coquelin
Publication date: 2017-11-21
Playlist: KVM Forum 2017
Description: 
	This talk will cover the challenge of improving VNFs safety relying on Virtio and Vhost-user backend on host side.
Maxime will first provide a brief overview of a VNF architecture relying on Virtio/Vhost-user, to take the opportunity to highlight the possible safety concerns.
In a second time, he will talk about new developments that introduces IOMMU support to Vhost-user backend, as a follow-up of last year talk from Peter Xu and Wei Xu about introducing IOMMU support for Vhost-kernel backend, and will provide some benchmarks to show the impact of enabling this feature.
Finally, Maxime will talk about the remaining safety and performance gaps, and present the envisaged ways to fill them.

---

Maxime Coquelin
SW Engineer, Red Hat, Inc.

Maxime is software engineer at Red Hat and Virtio/Vhost-user maintainer for the DPDK project. | Before joining Red Hat, he was Kernel developer for a semi-conductor company and kernel maintainer for two ARM platforms.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:05,930 --> 00:00:12,690
and so my name is Maksim koknal software

00:00:10,349 --> 00:00:16,619
engineer trade at work in the

00:00:12,690 --> 00:00:18,779
vectorization Tim and I mainly

00:00:16,619 --> 00:00:21,510
contribute to the PDK where I'm

00:00:18,779 --> 00:00:24,480
commentator for the theists and vertigo

00:00:21,510 --> 00:00:28,619
driver in DB decay I will see also

00:00:24,480 --> 00:00:31,710
contribute little to qmu and today I

00:00:28,619 --> 00:00:36,000
will talk about how to improve vnf

00:00:31,710 --> 00:00:40,950
safety when using iommu support with via

00:00:36,000 --> 00:00:44,000
choose or backends so first our we start

00:00:40,950 --> 00:00:44,000
with some background information

00:00:44,450 --> 00:00:53,360
explaining why we need an iommu support

00:00:47,809 --> 00:00:56,840
what what it is used to protect us to

00:00:53,360 --> 00:01:00,180
and I was so I will also cover some

00:00:56,840 --> 00:01:02,520
provide some information on juist canal

00:01:00,180 --> 00:01:04,890
I am in transition but I'd been

00:01:02,520 --> 00:01:09,260
presented just here but we're shoe and

00:01:04,890 --> 00:01:12,570
peter shoe and then after this i will

00:01:09,260 --> 00:01:16,650
talk about implementation we've done in

00:01:12,570 --> 00:01:19,619
the vo studio back and in d pd k after

00:01:16,650 --> 00:01:21,540
visa will give some benchmark to so that

00:01:19,619 --> 00:01:25,860
we can see the impacts the performance

00:01:21,540 --> 00:01:29,390
impact of using the iommu and I will

00:01:25,860 --> 00:01:33,119
then propose some possible improvements

00:01:29,390 --> 00:01:36,119
but we could do in a enjoy in the future

00:01:33,119 --> 00:01:40,979
and then I will shortly compute and we

00:01:36,119 --> 00:01:42,649
can have some questions ok so let's

00:01:40,979 --> 00:01:45,770
start with the background information so

00:01:42,649 --> 00:01:48,390
first why do we need any support so

00:01:45,770 --> 00:01:51,210
currently we don't support IOM you with

00:01:48,390 --> 00:01:55,049
your back-end and what it means is that

00:01:51,210 --> 00:01:58,290
when you run the PDK in guest you have

00:01:55,049 --> 00:02:01,290
to use either UI o or v fi o to bind

00:01:58,290 --> 00:02:04,969
your vertical device and that v fi o

00:02:01,290 --> 00:02:07,829
we've probed in a non safe mode in the

00:02:04,969 --> 00:02:09,520
so it means that first it taints the

00:02:07,829 --> 00:02:12,610
camera

00:02:09,520 --> 00:02:15,370
with some distribution it requires to to

00:02:12,610 --> 00:02:20,650
rebuild it like for example with Fedora

00:02:15,370 --> 00:02:24,160
or Debian and the the problem is that it

00:02:20,650 --> 00:02:27,100
directly use get physical addresses to

00:02:24,160 --> 00:02:29,620
the duplicate application directly use a

00:02:27,100 --> 00:02:33,180
physical get figure addresses for two

00:02:29,620 --> 00:02:37,750
photo that queues and buffer addresses

00:02:33,180 --> 00:02:40,300
so and so the problem is that on earth

00:02:37,750 --> 00:02:44,200
side the viewers choose our back-end

00:02:40,300 --> 00:02:47,740
which ran in a dedicated process and

00:02:44,200 --> 00:02:50,800
maps all the guests memory and can read

00:02:47,740 --> 00:02:53,380
and write all the guest memory so if you

00:02:50,800 --> 00:02:58,740
had an application the PDK application

00:02:53,380 --> 00:03:01,240
running in the guest that would but but

00:02:58,740 --> 00:03:05,470
the PDK application in guest could make

00:03:01,240 --> 00:03:09,430
we asked to access memory so application

00:03:05,470 --> 00:03:11,920
as an access to so for example the DPD

00:03:09,430 --> 00:03:15,910
key application could pass random

00:03:11,920 --> 00:03:19,180
physical addresses as buffer descriptors

00:03:15,910 --> 00:03:22,600
addresses and if this memory is some

00:03:19,180 --> 00:03:26,370
canon memory dispatches some color

00:03:22,600 --> 00:03:29,380
memory the vias back-end could overwrite

00:03:26,370 --> 00:03:31,720
kernel memory guest can a memory with

00:03:29,380 --> 00:03:33,910
the content of a packet in the cast of

00:03:31,720 --> 00:03:35,950
the receive pass or in the case of the

00:03:33,910 --> 00:03:41,560
transistor pass it could leak some kind

00:03:35,950 --> 00:03:46,570
of memory to the outside world so in

00:03:41,560 --> 00:03:49,690
guest we currently support our new boss

00:03:46,570 --> 00:03:55,360
in the canal and in user space with DP

00:03:49,690 --> 00:03:57,610
DK in the caner side it relies on the

00:03:55,360 --> 00:04:03,910
DMA framework but ends up in the enemy

00:03:57,610 --> 00:04:05,790
driver and the same and with DP DK is

00:04:03,910 --> 00:04:10,420
reported since

00:04:05,790 --> 00:04:16,000
16.7 for the virtual PMD and it relies

00:04:10,420 --> 00:04:21,419
on dedicated Iowa I will control from

00:04:16,000 --> 00:04:21,419
the via file to to map the memory pools

00:04:21,620 --> 00:04:26,250
so with two different setups of big

00:04:25,500 --> 00:04:29,039
difference

00:04:26,250 --> 00:04:31,379
we forgot to buy of MU because we can

00:04:29,039 --> 00:04:34,169
say that we have to kind of set up what

00:04:31,379 --> 00:04:37,860
we call dynamic for photo camera because

00:04:34,169 --> 00:04:41,490
the candle when we when using vertical

00:04:37,860 --> 00:04:47,069
driver in the kernel we can we have to

00:04:41,490 --> 00:04:50,129
to map and and map the buffers for every

00:04:47,069 --> 00:04:54,319
packet and it means that on all side

00:04:50,129 --> 00:04:56,610
that we we have an ale TLB miss and

00:04:54,319 --> 00:04:59,669
invalidate for each packet so this is

00:04:56,610 --> 00:05:04,529
very bad for performance and we have the

00:04:59,669 --> 00:05:09,000
DPD k case with vertigo PMD where the

00:05:04,529 --> 00:05:12,960
iommu map is done single time at when

00:05:09,000 --> 00:05:15,689
the vertigo PMD device is problem so it

00:05:12,960 --> 00:05:18,449
means that we will only have IO TLB miss

00:05:15,689 --> 00:05:22,169
on outside when the first time we will

00:05:18,449 --> 00:05:24,240
access the page if the diodes will be

00:05:22,169 --> 00:05:28,259
cash in the vo studio back and is large

00:05:24,240 --> 00:05:34,650
enough we not have any IO TLB miss after

00:05:28,259 --> 00:05:38,909
world so now let's look at the support

00:05:34,650 --> 00:05:41,240
of VI um mu in 3 mu so we have two type

00:05:38,909 --> 00:05:44,819
of devices so the emulated ones

00:05:41,240 --> 00:05:48,270
currently with platform supported by the

00:05:44,819 --> 00:05:51,479
PDK we have the Internet's 86 PowerPC

00:05:48,270 --> 00:05:56,250
and that are supported so the arm

00:05:51,479 --> 00:05:58,849
support is is being developed and we can

00:05:56,250 --> 00:06:02,389
also have perfect release the device

00:05:58,849 --> 00:06:05,550
like the vertigo immu

00:06:02,389 --> 00:06:10,439
so the specification is being discussed

00:06:05,550 --> 00:06:12,629
currently so and Erica so talk tomorrow

00:06:10,439 --> 00:06:18,419
I think about this so I think you should

00:06:12,629 --> 00:06:20,130
go there and as physical ummm use the

00:06:18,419 --> 00:06:22,470
Viera menu provides I

00:06:20,130 --> 00:06:26,400
iÃ¶ translation and device is relation

00:06:22,470 --> 00:06:31,200
and all these devices relies on a

00:06:26,400 --> 00:06:34,470
generic ap is for IU TLB and iommu for

00:06:31,200 --> 00:06:39,140
example to get an IV and hotel be entry

00:06:34,470 --> 00:06:44,100
from an IO VA and its permission or to

00:06:39,140 --> 00:06:47,460
the user can register some to the IOM

00:06:44,100 --> 00:06:50,250
identifiers to be notified when the

00:06:47,460 --> 00:06:59,460
guest performed an iommu map or an iommu

00:06:50,250 --> 00:07:02,250
and so then is a support for the photo

00:06:59,460 --> 00:07:05,400
even if they only knew in the vo stay in

00:07:02,250 --> 00:07:09,660
q mu so it was introduced last year

00:07:05,400 --> 00:07:13,650
initially for the kernel backyard and it

00:07:09,660 --> 00:07:16,800
implements using the API as I mentioned

00:07:13,650 --> 00:07:19,830
earlier it implements ATS address and

00:07:16,800 --> 00:07:24,690
patient service but I specified by the

00:07:19,830 --> 00:07:28,920
API specification and the changes and

00:07:24,690 --> 00:07:32,220
unveils back end are mainly adding some

00:07:28,920 --> 00:07:37,470
herbs to to be able to notify the

00:07:32,220 --> 00:07:40,760
backend for i/o TLB invalidate and

00:07:37,470 --> 00:07:46,860
updates and also it need to be able to

00:07:40,760 --> 00:07:55,950
to handle io TLB requests from from the

00:07:46,860 --> 00:07:59,270
backend so in in the corner to the put

00:07:55,950 --> 00:08:02,820
the channel to discuss who the kernel is

00:07:59,270 --> 00:08:05,820
the same than that the one used usually

00:08:02,820 --> 00:08:08,010
to for the viewers can protocol except

00:08:05,820 --> 00:08:10,850
that we use red and white swabs for the

00:08:08,010 --> 00:08:10,850
of the charge

00:08:11,510 --> 00:08:19,110
this is to be able to to pull for a you

00:08:15,600 --> 00:08:24,990
TLB miss request from the from the us

00:08:19,110 --> 00:08:27,360
back end so a new message message

00:08:24,990 --> 00:08:29,400
structure has been defined with

00:08:27,360 --> 00:08:31,380
different types - for the different

00:08:29,400 --> 00:08:33,260
requests so we can have missed but our

00:08:31,380 --> 00:08:35,330
quick resin Cherie

00:08:33,260 --> 00:08:38,600
selphie by the slave Soquel back on

00:08:35,330 --> 00:08:44,800
updates and invalidates but initiated by

00:08:38,600 --> 00:08:48,500
q mu and in the kernel reveals driver we

00:08:44,800 --> 00:08:51,710
devise a hotel big cash implemented so

00:08:48,500 --> 00:08:54,110
that so that we don't send io TLB miss

00:08:51,710 --> 00:08:58,970
each time we have a needed for an io

00:08:54,110 --> 00:09:01,460
translation and and to improve

00:08:58,970 --> 00:09:04,070
performance the cache relies on interval

00:09:01,460 --> 00:09:06,620
tree and we have also a dedicated cash

00:09:04,070 --> 00:09:08,870
for the viet queues because there are

00:09:06,620 --> 00:09:14,200
just much more often so to avoid passing

00:09:08,870 --> 00:09:14,200
the tree does the dictated cash

00:09:14,560 --> 00:09:20,240
so let's now let's have a look at user

00:09:18,740 --> 00:09:24,980
implementation with us to turn

00:09:20,240 --> 00:09:27,440
commentation in d pd k so the goal was

00:09:24,980 --> 00:09:29,300
to be as close as possible as as a

00:09:27,440 --> 00:09:32,300
design that had been done for the do

00:09:29,300 --> 00:09:36,230
scanner back-end to have the same

00:09:32,300 --> 00:09:39,740
protocol like photo channel one problem

00:09:36,230 --> 00:09:42,730
is that we need the special way to to

00:09:39,740 --> 00:09:48,890
undervalue TLB miss requests because

00:09:42,730 --> 00:09:52,550
currently only only the master cream you

00:09:48,890 --> 00:09:57,500
can initiate a request so instead of

00:09:52,550 --> 00:10:02,600
doing making the current circuit a bit

00:09:57,500 --> 00:10:05,830
direction we full duplex we decided to

00:10:02,600 --> 00:10:10,700
to use a new circuit for for this liquid

00:10:05,830 --> 00:10:14,800
so we implemented the shooter protocol

00:10:10,700 --> 00:10:17,210
to to enable slave initiated requests

00:10:14,800 --> 00:10:22,700
that are advertised with a dedicated

00:10:17,210 --> 00:10:26,840
flag and if if the the back country pots

00:10:22,700 --> 00:10:29,540
the the protocol feature a new circuit

00:10:26,840 --> 00:10:33,040
parish is created and one of the FD is

00:10:29,540 --> 00:10:40,209
sent to the backend using

00:10:33,040 --> 00:10:45,260
dedicated requests so for the payload

00:10:40,209 --> 00:10:48,890
III uses the same structure as as a

00:10:45,260 --> 00:10:51,709
master a necessity to request and today

00:10:48,890 --> 00:10:55,160
this this feature is on this new channel

00:10:51,709 --> 00:11:02,510
is only used by the iommu but but we

00:10:55,160 --> 00:11:06,649
could extend it to 20 over purpose if we

00:11:02,510 --> 00:11:08,690
now look at the IO TLB protocol update

00:11:06,649 --> 00:11:11,779
now that we have the two channels that

00:11:08,690 --> 00:11:15,380
we need so it has been introduced in

00:11:11,779 --> 00:11:17,300
last creamy release and the Specht is

00:11:15,380 --> 00:11:22,220
updated so you can have a look there if

00:11:17,300 --> 00:11:27,560
you want more details so from the master

00:11:22,220 --> 00:11:29,720
we introduced a new message type a new

00:11:27,560 --> 00:11:33,260
request to to be able to send updates

00:11:29,720 --> 00:11:35,300
and invalidation request we reuse the

00:11:33,260 --> 00:11:39,440
payload from the that has been specified

00:11:35,300 --> 00:11:41,959
for the ghost cannon backend and for

00:11:39,440 --> 00:11:46,910
this request we need to have the reply

00:11:41,959 --> 00:11:49,760
act feature mandatory because when we

00:11:46,910 --> 00:11:53,530
when we invalidate we need to be sure

00:11:49,760 --> 00:11:56,380
that it is not no more in the

00:11:53,530 --> 00:11:59,530
in the IOT device I would see a big

00:11:56,380 --> 00:12:04,150
cache before continuing the the

00:11:59,530 --> 00:12:07,450
processing and we have for the slave

00:12:04,150 --> 00:12:10,720
side we have a new request or so for the

00:12:07,450 --> 00:12:13,090
Miss and uses the same payload but in

00:12:10,720 --> 00:12:17,980
this time the reply act feature is

00:12:13,090 --> 00:12:22,240
optional we can we are not used to we

00:12:17,980 --> 00:12:27,610
don't have to use it so if we look at

00:12:22,240 --> 00:12:31,450
the other sequence now so we have D P D

00:12:27,610 --> 00:12:34,300
K and Q mu processes and the PMD thread

00:12:31,450 --> 00:12:34,930
is the processing data processing data

00:12:34,300 --> 00:12:39,190
pass

00:12:34,930 --> 00:12:41,590
processing thread in nd PDK and we have

00:12:39,190 --> 00:12:44,040
a dedicated freight for the four unlink

00:12:41,590 --> 00:12:49,990
the via Chuseok protocol so - - under

00:12:44,040 --> 00:12:53,080
requests and buy a bike EMU so when the

00:12:49,990 --> 00:12:55,480
PDK need to translate an address it

00:12:53,080 --> 00:12:59,590
sends a miss request to - q mu to the

00:12:55,480 --> 00:13:04,570
main thread and Kumu replies back with

00:12:59,590 --> 00:13:06,190
an update request the the hotel be

00:13:04,570 --> 00:13:12,010
untrue is inserted into the device IO

00:13:06,190 --> 00:13:15,070
TLB cache on down an accent - back to mu

00:13:12,010 --> 00:13:22,380
and optionally as I said you cream you

00:13:15,070 --> 00:13:22,380
can send back a key - to the PMD thread

00:13:23,820 --> 00:13:30,010
for to invalidate when the guest

00:13:27,150 --> 00:13:36,310
performing I'll name you and map it gets

00:13:30,010 --> 00:13:37,600
trapped by back mu and we send an eye

00:13:36,310 --> 00:13:40,030
which shall be invalidate request

00:13:37,600 --> 00:13:44,140
corresponding to the unmapped i will be

00:13:40,030 --> 00:13:49,270
on trees and she's removed from the cash

00:13:44,140 --> 00:13:52,150
in the PDK and then we can with AK but

00:13:49,270 --> 00:13:54,870
it is no more indication to cream you

00:13:52,150 --> 00:13:54,870
can on track

00:13:58,120 --> 00:14:05,170
so now let's have a look at how would we

00:14:02,949 --> 00:14:10,360
implemented the device a hotel B into D

00:14:05,170 --> 00:14:14,319
P D K so so as I said earlier we need an

00:14:10,360 --> 00:14:18,240
IOT lb cash in DP DK to avoid querying

00:14:14,319 --> 00:14:22,420
every time q mu we need on translation

00:14:18,240 --> 00:14:25,589
this cash is as a single writer which

00:14:22,420 --> 00:14:28,600
which is via choose of protocol and

00:14:25,589 --> 00:14:31,059
multiple readers can be the the PMD

00:14:28,600 --> 00:14:36,459
thread or the obvious user protocol

00:14:31,059 --> 00:14:38,559
thread so this is a great case for ICU

00:14:36,459 --> 00:14:41,860
because single write or multiple readers

00:14:38,559 --> 00:14:44,019
but the problem I put two typed it and

00:14:41,860 --> 00:14:47,769
tested it but the problem is we have

00:14:44,019 --> 00:14:50,139
first to listen license program with DP

00:14:47,769 --> 00:14:53,220
decay performance really matters so we

00:14:50,139 --> 00:14:56,709
try to rain line as much as possible and

00:14:53,220 --> 00:15:02,740
Lib userspace ICU is the licensed under

00:14:56,709 --> 00:15:04,809
GPL v2 so we could only inline small

00:15:02,740 --> 00:15:08,889
function less than 10 lines if I recall

00:15:04,809 --> 00:15:10,959
correctly and another problem was that

00:15:08,889 --> 00:15:16,269
from the build system it had some more

00:15:10,959 --> 00:15:18,329
dependency and DP DK tries to avoid as

00:15:16,269 --> 00:15:22,629
much as possible external dependency

00:15:18,329 --> 00:15:25,089
dependencies and the last problem is

00:15:22,629 --> 00:15:31,269
that some distributions doesn't even

00:15:25,089 --> 00:15:35,290
ship with with this library so as a

00:15:31,269 --> 00:15:38,410
fallback decided to use the readers and

00:15:35,290 --> 00:15:41,139
writers locks but are still better than

00:15:38,410 --> 00:15:44,079
regular matrixes for father for what we

00:15:41,139 --> 00:15:47,579
are trying to achieve but still even

00:15:44,079 --> 00:15:50,319
when you only have readers on this lock

00:15:47,579 --> 00:15:55,070
the implementation in the PDK make use

00:15:50,319 --> 00:15:59,990
of atomic operation for and it has

00:15:55,070 --> 00:16:02,330
a big overhead so initially we design it

00:15:59,990 --> 00:16:05,420
with having per device

00:16:02,330 --> 00:16:07,670
I would TLB cache but we moved to to use

00:16:05,420 --> 00:16:09,770
a pair of Viet Kieu because the more

00:16:07,670 --> 00:16:16,670
work you add the worse was the

00:16:09,770 --> 00:16:22,310
performance and also to perform less

00:16:16,670 --> 00:16:26,630
code with to twist instructions we we

00:16:22,310 --> 00:16:29,000
move to to take the log for packet to up

00:16:26,630 --> 00:16:34,940
for burst of packets instead of for

00:16:29,000 --> 00:16:38,330
every every packet so the the cache

00:16:34,940 --> 00:16:40,660
itself is implemented using a sorted

00:16:38,330 --> 00:16:43,730
list so this is just an initial

00:16:40,660 --> 00:16:47,060
implementation this is not really

00:16:43,730 --> 00:16:50,380
efficient or not efficient at all but it

00:16:47,060 --> 00:16:53,660
is enough today with one gig huge pages

00:16:50,380 --> 00:16:57,250
but still in the future we need a better

00:16:53,660 --> 00:17:02,540
implementation like 24 trees for for the

00:16:57,250 --> 00:17:05,150
canal back-end and what we need also

00:17:02,540 --> 00:17:08,449
what we did also is to to have a cache

00:17:05,150 --> 00:17:11,320
that is large enough because when using

00:17:08,449 --> 00:17:16,430
the static mappings with DPD can guest

00:17:11,320 --> 00:17:17,089
what we want is to to avoid any cache

00:17:16,430 --> 00:17:20,180
eviction

00:17:17,089 --> 00:17:22,490
so normally cache eviction in this

00:17:20,180 --> 00:17:24,560
design should only happen when either

00:17:22,490 --> 00:17:26,750
you guessed your Gettysburg II or you

00:17:24,560 --> 00:17:36,020
have a malicious guest but try to make

00:17:26,750 --> 00:17:40,700
you send you a lot of updates so now if

00:17:36,020 --> 00:17:45,170
we look at the benchmark pay presented

00:17:40,700 --> 00:17:48,290
some PvP man schmuck using obvious so I

00:17:45,170 --> 00:17:50,150
did the same so this benchmark is

00:17:48,290 --> 00:17:55,550
documented on DP decades a reference

00:17:50,150 --> 00:17:57,770
benchmark where we used test PMD both on

00:17:55,550 --> 00:18:00,080
the Austin the guest so the idea is to

00:17:57,770 --> 00:18:04,190
forward packets sent by a packet

00:18:00,080 --> 00:18:05,320
generator from one Nick to Z over going

00:18:04,190 --> 00:18:11,919
through the VM

00:18:05,320 --> 00:18:16,779
and so we use ear to to two vital

00:18:11,919 --> 00:18:19,750
devices and tunics as said pay we need

00:18:16,779 --> 00:18:28,240
six core to do this it read a row

00:18:19,750 --> 00:18:31,179
represent one one CPU and on outside we

00:18:28,240 --> 00:18:33,009
just do a you forwarding we don't touch

00:18:31,179 --> 00:18:35,590
the packet at all we take two packet

00:18:33,009 --> 00:18:38,860
from one interface and when physical

00:18:35,590 --> 00:18:44,590
interface and send it to the to the

00:18:38,860 --> 00:18:47,320
first one and in the guest side we do

00:18:44,590 --> 00:18:52,450
some Mac swapping to so that we can

00:18:47,320 --> 00:18:54,669
simulate more realistic load because

00:18:52,450 --> 00:18:59,259
doing Mac swapping we access the packet

00:18:54,669 --> 00:19:03,340
adder so packet generator generator we

00:18:59,259 --> 00:19:05,679
can use moonshine or physic out where a

00:19:03,340 --> 00:19:09,279
packet generator likes each year in my

00:19:05,679 --> 00:19:10,899
case I use t-rex but it does not really

00:19:09,279 --> 00:19:15,669
matter it should be the same whatever

00:19:10,899 --> 00:19:20,500
the package generator is used so what we

00:19:15,669 --> 00:19:23,259
can see is that we don't we measured we

00:19:20,500 --> 00:19:27,000
the test was done with sending the

00:19:23,259 --> 00:19:30,610
smallest package 64 byte packets with

00:19:27,000 --> 00:19:32,649
with bit directional testing and I run

00:19:30,610 --> 00:19:35,919
the test with one kick judge patches and

00:19:32,649 --> 00:19:39,940
we've to make huge pages to see the

00:19:35,919 --> 00:19:45,159
performance degradation we've to make

00:19:39,940 --> 00:19:49,210
each pages because the more the smaller

00:19:45,159 --> 00:19:50,590
the page is the the I also a number of

00:19:49,210 --> 00:19:54,159
utility entries you will have in your

00:19:50,590 --> 00:19:57,789
device a which a big cash so what we see

00:19:54,159 --> 00:20:02,049
is that we have 25 percent performance

00:19:57,789 --> 00:20:07,960
degradation when using the MMU we've to

00:20:02,049 --> 00:20:12,159
make each pages so so we see the cash

00:20:07,960 --> 00:20:14,730
flow cap overhead here surprisingly we

00:20:12,159 --> 00:20:14,730
don't she

00:20:14,840 --> 00:20:20,899
over-read when using one Gidget pages so

00:20:18,299 --> 00:20:23,639
I expected to have a small overhead

00:20:20,899 --> 00:20:26,029
smaller than with two mags but two small

00:20:23,639 --> 00:20:29,880
override anyway because we do master fan

00:20:26,029 --> 00:20:33,120
on the outside and the reason here is

00:20:29,880 --> 00:20:35,100
because vertigo PMG is a bottleneck so

00:20:33,120 --> 00:20:39,899
we cannot measure measures the

00:20:35,100 --> 00:20:43,350
performance impact on the other side so

00:20:39,899 --> 00:20:47,100
if what I did to try to see the impact

00:20:43,350 --> 00:20:50,850
of one-digit pages when having a new one

00:20:47,100 --> 00:20:53,879
is to do to run some micro benchmarks

00:20:50,850 --> 00:20:56,929
using test PMD so the idea was to just

00:20:53,879 --> 00:21:01,730
to use this page as packet generator and

00:20:56,929 --> 00:21:04,830
you can run it for doing guest to ask

00:21:01,730 --> 00:21:07,559
traffic or to guests or I you look back

00:21:04,830 --> 00:21:08,820
you you do forwarding and insert some

00:21:07,559 --> 00:21:15,000
packets in the loop

00:21:08,820 --> 00:21:17,460
and here we see an impact the surprises

00:21:15,000 --> 00:21:22,820
that unfollowed to make pages we don't

00:21:17,460 --> 00:21:28,100
see as big impact as with PvP benchmark

00:21:22,820 --> 00:21:32,850
I think that the reason that we we use a

00:21:28,100 --> 00:21:38,140
smaller number of buffers and so we

00:21:32,850 --> 00:21:40,630
always end up into the cache to copies

00:21:38,140 --> 00:21:48,100
we don't go through all the entries of

00:21:40,630 --> 00:21:54,549
the cache so what could we do next to to

00:21:48,100 --> 00:21:58,360
improve the performance we we could

00:21:54,549 --> 00:22:01,779
first try to to merge our TLB entries

00:21:58,360 --> 00:22:08,559
that are contiguous into the device IO

00:22:01,779 --> 00:22:10,510
TLB when inserting the entries so I've

00:22:08,559 --> 00:22:13,389
done some test and what what we can see

00:22:10,510 --> 00:22:15,639
is that most of the entries when using

00:22:13,389 --> 00:22:19,029
to make pages are both physically and

00:22:15,639 --> 00:22:21,870
virtually contiguous so if the the

00:22:19,029 --> 00:22:25,090
permissions are the same we could just

00:22:21,870 --> 00:22:29,440
merge them into a single entry to speed

00:22:25,090 --> 00:22:32,139
up the root cap so it works now we still

00:22:29,440 --> 00:22:36,250
have to and it fixes the performance

00:22:32,139 --> 00:22:38,889
regressions that we measured and no no

00:22:36,250 --> 00:22:41,740
we have to define whether we when we

00:22:38,889 --> 00:22:44,649
receive an invalidation if we invalidate

00:22:41,740 --> 00:22:48,250
all the match and tree or or split it

00:22:44,649 --> 00:22:51,639
into smaller ones and the question also

00:22:48,250 --> 00:22:53,679
is that that it could also add an

00:22:51,639 --> 00:22:56,519
overhead when using dynamic mappings

00:22:53,679 --> 00:23:00,190
with using what eyelet kernel driver

00:22:56,519 --> 00:23:02,049
because we are we have a lot of i/o TLB

00:23:00,190 --> 00:23:10,600
and trail update and misses and

00:23:02,049 --> 00:23:12,760
invalidates in this case another room

00:23:10,600 --> 00:23:15,940
for improvement would be to use interval

00:23:12,760 --> 00:23:19,840
trees as it has been done for via

00:23:15,940 --> 00:23:22,510
scanner to have so we could have the

00:23:19,840 --> 00:23:24,460
both optimizations here to merge putting

00:23:22,510 --> 00:23:27,490
boots on tree and I've used interval

00:23:24,460 --> 00:23:32,139
tree for for all the ones that are not

00:23:27,490 --> 00:23:35,620
contiguous it requires a new library in

00:23:32,139 --> 00:23:36,100
the PDK so so there are some what to do

00:23:35,620 --> 00:23:39,090
here

00:23:36,100 --> 00:23:39,090
and

00:23:39,980 --> 00:23:46,850
we are sure that it would bring some

00:23:42,470 --> 00:23:52,840
some performance improvement another

00:23:46,850 --> 00:23:57,200
idea that maybe is but that may improve

00:23:52,840 --> 00:24:00,980
the case of the dynamic mappings could

00:23:57,200 --> 00:24:04,549
be to use to to burst I would tell the

00:24:00,980 --> 00:24:10,580
miss requests so DP DK walks by a burst

00:24:04,549 --> 00:24:14,269
of packets each time we process when

00:24:10,580 --> 00:24:17,149
having a IMU on every time we we process

00:24:14,269 --> 00:24:19,610
a packet we have a little bit miss so we

00:24:17,149 --> 00:24:22,909
exit the bursts waiting for the update

00:24:19,610 --> 00:24:25,879
so when we face this case maybe the

00:24:22,909 --> 00:24:28,940
better thing could be to to see all the

00:24:25,879 --> 00:24:33,080
the bursts all the packets and send all

00:24:28,940 --> 00:24:36,830
the IO TLB misses once fall and wait for

00:24:33,080 --> 00:24:39,889
all to be to be updated before starting

00:24:36,830 --> 00:24:41,629
the processing of the burst I think that

00:24:39,889 --> 00:24:44,059
maybe it could bring some improvement

00:24:41,629 --> 00:24:50,090
when when you're p.m. this thread is

00:24:44,059 --> 00:24:53,000
used is used to with when you have

00:24:50,090 --> 00:25:00,259
multiple p.m. DS on the same thread then

00:24:53,000 --> 00:25:02,509
same CPU okay so let's see the

00:25:00,259 --> 00:25:07,370
conclusion so we have a design that is

00:25:02,509 --> 00:25:09,740
close to the vos connect back on it will

00:25:07,370 --> 00:25:12,970
be available in the next DP decay

00:25:09,740 --> 00:25:15,649
release that would be out net next month

00:25:12,970 --> 00:25:17,990
we still have room from improvement but

00:25:15,649 --> 00:25:20,480
with large root pages it's it's really

00:25:17,990 --> 00:25:25,039
usable when you use the PDK in the guest

00:25:20,480 --> 00:25:28,070
but still with camel with the pilot car

00:25:25,039 --> 00:25:32,799
driver the performance is very low but

00:25:28,070 --> 00:25:36,789
we have to think of over was of

00:25:32,799 --> 00:25:41,269
improvements to to to fix this

00:25:36,789 --> 00:25:44,389
I'd like to thank say thanks to to Jason

00:25:41,269 --> 00:25:46,429
way but Brian me some support we'll get

00:25:44,389 --> 00:25:47,510
to via scanner in support and to to

00:25:46,429 --> 00:25:50,180
peter shoe

00:25:47,510 --> 00:25:56,570
yeah we've done most of the work for the

00:25:50,180 --> 00:25:59,350
VR VR um you and and yes thank you do

00:25:56,570 --> 00:25:59,350
you have some question

00:26:11,080 --> 00:26:13,769
yeah

00:26:18,499 --> 00:26:21,499
yes

00:26:27,610 --> 00:26:44,500
you mean value of MU so it's a the first

00:26:35,830 --> 00:26:47,350
to UM mu is yes it's another device okay

00:26:44,500 --> 00:26:51,640
so I guess so like here you mean V asked

00:26:47,350 --> 00:26:53,920
are you a news report so yes it if it's

00:26:51,640 --> 00:26:58,920
transparent so it could be used for via

00:26:53,920 --> 00:26:58,920
scusi I haven't tested but it works

00:27:08,250 --> 00:27:24,940
yes yeah sorry yeah so yeah so the

00:27:22,690 --> 00:27:28,200
problem is that most maps also a guest

00:27:24,940 --> 00:27:31,450
memory and if you don't choose an iommu

00:27:28,200 --> 00:27:35,890
when you enter the descriptors you

00:27:31,450 --> 00:27:39,250
cannot check whether the address the

00:27:35,890 --> 00:27:44,470
buffer address so guest provides you is

00:27:39,250 --> 00:27:46,540
valid or not is a price or not but when

00:27:44,470 --> 00:27:49,540
you use iommu you will map the buffers

00:27:46,540 --> 00:27:51,070
and so you will have you you can walk

00:27:49,540 --> 00:27:54,130
through the are you aware a new tables

00:27:51,070 --> 00:27:57,220
in the DI owna new - to check whether

00:27:54,130 --> 00:27:59,970
for this device you you can access this

00:27:57,220 --> 00:27:59,970
memory region

00:28:00,679 --> 00:28:06,440
a bitmap per page sorry you just using

00:28:05,210 --> 00:28:16,220
tutor access permissions but not

00:28:06,440 --> 00:28:22,669
translation says idea was to have to use

00:28:16,220 --> 00:28:24,740
a generic a path from so I I have seen a

00:28:22,669 --> 00:28:27,230
few are you using the disk aligning with

00:28:24,740 --> 00:28:31,960
you or not no it's a better Avenue

00:28:27,230 --> 00:28:31,960
so emulation in queue in simulation

00:28:46,179 --> 00:28:49,419
can you show

00:28:58,570 --> 00:29:06,580
you can see it's a bit better when doing

00:29:03,100 --> 00:29:08,800
micro benchmark when we have the TLB

00:29:06,580 --> 00:29:13,000
service but IMU is off

00:29:08,800 --> 00:29:15,520
I don't really explain it I think that

00:29:13,000 --> 00:29:18,280
just I actually the difference when

00:29:15,520 --> 00:29:20,800
we've on without the series is that we

00:29:18,280 --> 00:29:24,550
have some some if conditions and I think

00:29:20,800 --> 00:29:29,400
that it changes our the pipeline view

00:29:24,550 --> 00:29:29,400
pipeline to the tire chain

00:29:41,180 --> 00:29:48,030
in cream it's available and the PDK next

00:29:45,510 --> 00:29:49,920
month if you in few weeks it's already

00:29:48,030 --> 00:29:52,340
it's already upstream but it will be

00:29:49,920 --> 00:29:52,340
next three days

00:29:54,380 --> 00:30:01,830
yes so currently it's only tested with

00:29:57,630 --> 00:30:04,710
x86 we would need over dirty way of MU

00:30:01,830 --> 00:30:09,720
or I'm a senior ship emulated support in

00:30:04,710 --> 00:30:12,720
a queue and and it has been tested I

00:30:09,720 --> 00:30:16,640
think I make some cave young guys but

00:30:12,720 --> 00:30:16,640
with out of three patches

00:30:23,660 --> 00:30:32,700
[Applause]

00:30:26,830 --> 00:30:32,700

YouTube URL: https://www.youtube.com/watch?v=amlhowbtlSw


