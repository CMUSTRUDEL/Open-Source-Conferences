Title: [2017] Improving the Performance of the qcow2 Format by Alberto Garcia
Publication date: 2017-11-16
Playlist: KVM Forum 2017
Description: 
	qcow2 is QEMU's native file format for storing disk images. One of its features is that it grows dynamically, so disk space is only allocated when the virtual machine needs to store data. This makes the format efficient in terms of space requirements, but has an impact on its I/O performance. This presentation will describe some of those performance problems and will discuss possible ways to address them. Some of them can be solved by simply adjusting configuration parameters, others require improving the qcow2 driver in QEMU, and others need extending the file format itself.

---

Alberto Garcia
Igalia
Software Engineer

Alberto Garcia is a software engineer working at Igalia. He has more than 15 years of professional experience working with Linux-based systems and has been contributing to the QEMU project for the last couple of years. In addition to that, he was also involved in the development of the Maemo and MeeGo operating systems, has worked on the GTK+ port of WebKit and is an active Debian developer.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:06,259 --> 00:00:11,070
thank you everyone for being here

00:00:08,490 --> 00:00:14,429
my name is Alberto Katya I were free

00:00:11,070 --> 00:00:16,260
Galia I've been working in the blood

00:00:14,429 --> 00:00:18,810
layer of kiramune for the past few years

00:00:16,260 --> 00:00:20,730
and I'm going to talk a bit about the

00:00:18,810 --> 00:00:22,500
kill code to format and the work that

00:00:20,730 --> 00:00:26,939
we've been doing to try to make it

00:00:22,500 --> 00:00:30,210
faster so first I will start to by

00:00:26,939 --> 00:00:33,899
giving a brief introduced in overview of

00:00:30,210 --> 00:00:35,070
the you go to format queue go to format

00:00:33,899 --> 00:00:37,800
as you know is the native file format

00:00:35,070 --> 00:00:39,960
used by qmu for starting disk images it

00:00:37,800 --> 00:00:42,120
has multiple features can grow on demand

00:00:39,960 --> 00:00:45,420
support files snapshots

00:00:42,120 --> 00:00:46,950
encryption compression and under certain

00:00:45,420 --> 00:00:49,379
circumstances you can achieve good

00:00:46,950 --> 00:00:51,750
performance comparable to that of RAW

00:00:49,379 --> 00:00:54,390
files however that's not always the case

00:00:51,750 --> 00:00:56,520
so in this talk I will try to describe

00:00:54,390 --> 00:00:58,980
the problems that the cuecore 2 format

00:00:56,520 --> 00:01:00,539
has and some ways to improve this

00:00:58,980 --> 00:01:02,219
performance there's three different

00:01:00,539 --> 00:01:05,489
approaches one of one of them requires

00:01:02,219 --> 00:01:07,229
simply configuration changes another

00:01:05,489 --> 00:01:09,689
approach requires changes to the kyouko

00:01:07,229 --> 00:01:12,630
to driver itself and the Thera proach

00:01:09,689 --> 00:01:16,380
would need changes in the on disk format

00:01:12,630 --> 00:01:18,630
kyouko to so let's start by giving a

00:01:16,380 --> 00:01:20,369
very brief overview of how you code to

00:01:18,630 --> 00:01:21,869
file looks like if you go to file is

00:01:20,369 --> 00:01:24,330
divided into clusters of the same size

00:01:21,869 --> 00:01:28,759
the cluster size can be configured when

00:01:24,330 --> 00:01:31,259
you create the image its 64k by default

00:01:28,759 --> 00:01:33,390
each class roster has a different type

00:01:31,259 --> 00:01:35,460
there's different these are not all the

00:01:33,390 --> 00:01:39,000
possible cluster types but are some of

00:01:35,460 --> 00:01:41,460
the most common ones the data clusters

00:01:39,000 --> 00:01:43,950
are the clusters that contain the data

00:01:41,460 --> 00:01:48,149
that the guest can see so in order to

00:01:43,950 --> 00:01:49,740
map the that data is the gases into the

00:01:48,149 --> 00:01:52,560
data in the queue go to file we need

00:01:49,740 --> 00:01:54,780
some data structures those are the l1

00:01:52,560 --> 00:01:58,740
and l2 tables so with the l1 l2 tables

00:01:54,780 --> 00:02:01,280
we can convert a guest address into hole

00:01:58,740 --> 00:02:05,729
saturation to the cue co2 file

00:02:01,280 --> 00:02:07,979
so these l1 l2 Devils are simply add to

00:02:05,729 --> 00:02:10,410
level data structure with pointers to

00:02:07,979 --> 00:02:13,510
the data clusters there's nothing

00:02:10,410 --> 00:02:15,730
particularly complicated about that

00:02:13,510 --> 00:02:16,840
in this example the entries that are in

00:02:15,730 --> 00:02:18,519
white means that haven't been

00:02:16,840 --> 00:02:20,110
initialized so that means that the

00:02:18,519 --> 00:02:23,590
cluster for that data are being

00:02:20,110 --> 00:02:28,750
allocated there so if the guest tries to

00:02:23,590 --> 00:02:30,819
read data from them it will get zero in

00:02:28,750 --> 00:02:32,950
addition to that the cuecore 2 format

00:02:30,819 --> 00:02:34,900
also supports backing files that means

00:02:32,950 --> 00:02:37,569
that when the guest tries to read data

00:02:34,900 --> 00:02:40,659
from a pacific form a part of the disk

00:02:37,569 --> 00:02:42,939
if that data hasn't been allocated in a

00:02:40,659 --> 00:02:44,379
particular queue go to file then you

00:02:42,939 --> 00:02:46,989
will go to the backing file and try to

00:02:44,379 --> 00:02:49,359
see if the data is there backing files

00:02:46,989 --> 00:02:52,269
can be changed or each backing file can

00:02:49,359 --> 00:02:54,909
have one backing file and so on and so

00:02:52,269 --> 00:02:56,440
forth and it's also important to note

00:02:54,909 --> 00:02:58,599
that backing files to need to have the

00:02:56,440 --> 00:03:03,310
same format or cluster size after the

00:02:58,599 --> 00:03:05,859
original image so well after we have

00:03:03,310 --> 00:03:08,500
seen the basic data structure that Maps

00:03:05,859 --> 00:03:09,940
guest into host clusters we're going to

00:03:08,500 --> 00:03:14,859
see what are the problems that are the

00:03:09,940 --> 00:03:20,500
result of that first of all when you

00:03:14,859 --> 00:03:24,370
need to access the vehicle to image from

00:03:20,500 --> 00:03:25,959
the guest you need to before go into the

00:03:24,370 --> 00:03:27,879
data clutter you need to first figure

00:03:25,959 --> 00:03:31,780
out where it is so that means reading

00:03:27,879 --> 00:03:33,459
the l1 table go into the appropriate l2

00:03:31,780 --> 00:03:36,519
table and riddling and then go into the

00:03:33,459 --> 00:03:38,769
actual data cluster as you can see that

00:03:36,519 --> 00:03:41,739
requires extra i/o and that has a big

00:03:38,769 --> 00:03:44,019
impact in performance the solution of

00:03:41,739 --> 00:03:46,419
course is to keep this metadata in

00:03:44,019 --> 00:03:47,739
memory for the case of the one table

00:03:46,419 --> 00:03:49,629
that's not a problem because the l1

00:03:47,739 --> 00:03:51,970
table is small and can be kept in RAM

00:03:49,629 --> 00:03:53,769
all the time l 2 tables however a

00:03:51,970 --> 00:03:55,629
different thing because they are

00:03:53,769 --> 00:03:59,290
allocated on demand as the image grows

00:03:55,629 --> 00:04:01,209
and if the image is large they can

00:03:59,290 --> 00:04:03,340
neither they can take a lot of the disk

00:04:01,209 --> 00:04:04,680
space so we can keep everything in

00:04:03,340 --> 00:04:08,169
memory

00:04:04,680 --> 00:04:11,500
kyouko to chemo has a kicker to cash for

00:04:08,169 --> 00:04:14,500
the l2 tables and that can be used to

00:04:11,500 --> 00:04:18,909
speed up disk access the maximum amount

00:04:14,500 --> 00:04:20,470
of metadata can be very large depending

00:04:18,909 --> 00:04:21,849
on the image so we cannot keep

00:04:20,470 --> 00:04:24,909
everything in memory here's there's a

00:04:21,849 --> 00:04:25,940
table that shows was the metadata that

00:04:24,909 --> 00:04:27,740
is necessary for

00:04:25,940 --> 00:04:29,540
cooller image the size of the metadata

00:04:27,740 --> 00:04:32,870
depends on the cluster size and on the

00:04:29,540 --> 00:04:37,730
image size so for one terabyte disk

00:04:32,870 --> 00:04:44,720
image we can see how much l2 metadata we

00:04:37,730 --> 00:04:46,340
need for each type of laughter sighs so

00:04:44,720 --> 00:04:47,870
how to use the cue code to cache the

00:04:46,340 --> 00:04:50,090
keeper to Cassies enabled it by default

00:04:47,870 --> 00:04:52,660
and has a size of one megabyte it can be

00:04:50,090 --> 00:04:56,300
changed with the l2 cache size option

00:04:52,660 --> 00:04:58,250
and we define default cluster of 64

00:04:56,300 --> 00:05:00,380
kilobytes that's enough for a 8 gigabyte

00:04:58,250 --> 00:05:02,690
disk image so if your image is saying

00:05:00,380 --> 00:05:04,010
abide are less and you're using the

00:05:02,690 --> 00:05:06,470
default cluster then you don't need to

00:05:04,010 --> 00:05:08,240
worry about however if you're using a

00:05:06,470 --> 00:05:11,330
different image you might need to take a

00:05:08,240 --> 00:05:12,910
look at that because the performant the

00:05:11,330 --> 00:05:16,460
effects on performance can be dramatic

00:05:12,910 --> 00:05:20,210
this table I made it with a 20 gigabyte

00:05:16,460 --> 00:05:23,090
image fully populated and the last row

00:05:20,210 --> 00:05:24,770
with it 2.5 megabytes of cache that's

00:05:23,090 --> 00:05:27,800
the maximum amount of cash that you need

00:05:24,770 --> 00:05:28,820
for this kind of for this image size so

00:05:27,800 --> 00:05:31,660
that's the performance that you see

00:05:28,820 --> 00:05:34,040
there it's around 64 thousand I hope

00:05:31,660 --> 00:05:36,590
that's what you get more or less with

00:05:34,040 --> 00:05:38,960
RAW files so in this case the cuecore 2

00:05:36,590 --> 00:05:41,000
is comparable to a row file however if

00:05:38,960 --> 00:05:44,090
the cache size is smaller as you can see

00:05:41,000 --> 00:05:45,800
the performance is a fraction of that so

00:05:44,090 --> 00:05:47,270
you really need to increase the cache

00:05:45,800 --> 00:05:51,500
size if you want to get good performance

00:05:47,270 --> 00:05:53,360
here now the problem is how do we know

00:05:51,500 --> 00:05:54,890
how much cash to a we need there's a

00:05:53,360 --> 00:05:56,510
formula for that is the one that you see

00:05:54,890 --> 00:05:58,610
here and the problem is that the formula

00:05:56,510 --> 00:06:00,320
is too complicated it's not obvious and

00:05:58,610 --> 00:06:02,660
the user shouldn't really need to know

00:06:00,320 --> 00:06:05,000
about it so maybe qme should have a

00:06:02,660 --> 00:06:07,400
better default but problem is what's a

00:06:05,000 --> 00:06:09,140
good default that's still an open

00:06:07,400 --> 00:06:12,410
question and we haven't really changed

00:06:09,140 --> 00:06:15,290
anything there's another alternative is

00:06:12,410 --> 00:06:18,230
that instead of saying how much memory

00:06:15,290 --> 00:06:19,750
we need what we can say is how much disk

00:06:18,230 --> 00:06:22,280
space we want to cover with the cache

00:06:19,750 --> 00:06:23,570
then of course you wouldn't know how

00:06:22,280 --> 00:06:25,100
much memory you're actually going to use

00:06:23,570 --> 00:06:27,500
so you have the problem from the other

00:06:25,100 --> 00:06:29,810
side but there's still an ongoing

00:06:27,500 --> 00:06:32,150
discussion there's a bug in the record

00:06:29,810 --> 00:06:34,280
box he'll about this but nothing has

00:06:32,150 --> 00:06:35,770
been concluded yet there's a few patches

00:06:34,280 --> 00:06:39,759
but

00:06:35,770 --> 00:06:42,460
this thing needs to be retaken there's

00:06:39,759 --> 00:06:45,520
however a general pattern that if we

00:06:42,460 --> 00:06:48,580
increase the cluster size we decrease

00:06:45,520 --> 00:06:50,229
the metadata size so one one easy way of

00:06:48,580 --> 00:06:52,389
reduction are reducing the amount of

00:06:50,229 --> 00:06:54,580
cash that we need is by increasing the

00:06:52,389 --> 00:06:56,560
cluster size that has the benefit that

00:06:54,580 --> 00:06:58,960
we have the same earth form as with a

00:06:56,560 --> 00:07:00,879
smaller cash has also the benefit that

00:06:58,960 --> 00:07:03,210
that it reduces fragmentation in the Q

00:07:00,879 --> 00:07:05,500
code to image but has to promise

00:07:03,210 --> 00:07:06,639
allocations are lower because every time

00:07:05,500 --> 00:07:09,400
you allocate the cluster you had to

00:07:06,639 --> 00:07:12,849
allocate a larger cluster and it weights

00:07:09,400 --> 00:07:15,099
more disk space of course there was

00:07:12,849 --> 00:07:17,020
another problem is that the Q code to

00:07:15,099 --> 00:07:20,229
cash is not there's not just one cache

00:07:17,020 --> 00:07:22,300
for the whole VM the cache is attached

00:07:20,229 --> 00:07:24,009
to each one of the cuecore 2 images so

00:07:22,300 --> 00:07:28,630
you have many images each one of them

00:07:24,009 --> 00:07:31,479
needs its own cache so in this example

00:07:28,630 --> 00:07:32,919
we have a backing image with some areas

00:07:31,479 --> 00:07:35,500
that have been allocated at the areas in

00:07:32,919 --> 00:07:37,440
blue and the active image is empty so

00:07:35,500 --> 00:07:39,550
every time the guest tries to read data

00:07:37,440 --> 00:07:40,930
qme goes to the backing image because

00:07:39,550 --> 00:07:43,500
the active image doesn't have anything

00:07:40,930 --> 00:07:48,009
so that means that human is the load

00:07:43,500 --> 00:07:50,620
metadata for mapping the clusters in the

00:07:48,009 --> 00:07:53,199
backing image now what happens if you

00:07:50,620 --> 00:07:55,479
start to write data that PMO is going to

00:07:53,199 --> 00:07:57,490
write into the active image and what

00:07:55,479 --> 00:08:00,009
happens after a while is that all the

00:07:57,490 --> 00:08:01,810
sections that are now in green are

00:08:00,009 --> 00:08:02,259
sections that the guest cannot see

00:08:01,810 --> 00:08:03,759
anymore

00:08:02,259 --> 00:08:06,849
because the data for those clusters is

00:08:03,759 --> 00:08:09,130
now in the active file that has the

00:08:06,849 --> 00:08:12,669
consequence that all the metadata that

00:08:09,130 --> 00:08:14,529
we read earlier in order to read those

00:08:12,669 --> 00:08:16,630
clusters in the backing file is now is

00:08:14,529 --> 00:08:19,719
list is now in memory and we don't need

00:08:16,630 --> 00:08:22,150
it anymore if we have a longer backing

00:08:19,719 --> 00:08:25,210
chain that this problem thing can be

00:08:22,150 --> 00:08:27,400
bigger of course there's a way to work

00:08:25,210 --> 00:08:28,900
around this and this was introduced a

00:08:27,400 --> 00:08:31,930
couple of years ago and it's a setting

00:08:28,900 --> 00:08:33,669
called cache clean interval and the way

00:08:31,930 --> 00:08:37,770
this works is that you define a timeout

00:08:33,669 --> 00:08:40,240
and then um you checks the cache every

00:08:37,770 --> 00:08:41,680
60 seconds or whatever you decide and

00:08:40,240 --> 00:08:44,770
removes the entries that haven't been

00:08:41,680 --> 00:08:47,220
used since then this way they're all the

00:08:44,770 --> 00:08:48,960
entries that were necessary to to

00:08:47,220 --> 00:08:50,580
address the

00:08:48,960 --> 00:08:54,270
in Greenwood disappear and the memory

00:08:50,580 --> 00:08:57,330
would be safe there's another problem

00:08:54,270 --> 00:08:59,250
that is a consequence of increasing the

00:08:57,330 --> 00:09:01,080
the cluster size when you increase the

00:08:59,250 --> 00:09:04,500
cluster size to increase the l2 table

00:09:01,080 --> 00:09:06,300
size because it's the same and since the

00:09:04,500 --> 00:09:08,610
cue card - cash always treats complete

00:09:06,300 --> 00:09:11,610
tables this means that if we need to get

00:09:08,610 --> 00:09:14,010
a l2 metadata qmo needs to load the

00:09:11,610 --> 00:09:15,540
whole table and if we need to update one

00:09:14,010 --> 00:09:20,480
table demo needs to write the whole

00:09:15,540 --> 00:09:22,830
table that means Mirai oh it's also a

00:09:20,480 --> 00:09:25,200
more inflexible and inefficient use of

00:09:22,830 --> 00:09:28,920
the cache memory and we will see it with

00:09:25,200 --> 00:09:33,150
this example here we have a 512 gigabyte

00:09:28,920 --> 00:09:34,740
hard drive then the cluster size is one

00:09:33,150 --> 00:09:39,390
megabyte so with this set up we need

00:09:34,740 --> 00:09:40,950
only for l2 tables and the way the

00:09:39,390 --> 00:09:43,470
addressing works is that each one of the

00:09:40,950 --> 00:09:48,030
l2 tables contain sentries to map the

00:09:43,470 --> 00:09:51,150
clusters each one of these four chunks

00:09:48,030 --> 00:09:55,590
of 128 gigabytes so if you need to

00:09:51,150 --> 00:09:57,150
perform IO in the first chunk so no

00:09:55,590 --> 00:10:01,140
matter how as long as you are performing

00:09:57,150 --> 00:10:03,030
IO in the before the 0 and the first 128

00:10:01,140 --> 00:10:05,130
gigabytes all the metadata that you need

00:10:03,030 --> 00:10:06,300
is located in the first table so with

00:10:05,130 --> 00:10:08,390
the first table you're fine you load the

00:10:06,300 --> 00:10:11,520
first table in memory and your time

00:10:08,390 --> 00:10:13,620
however if you're well the region where

00:10:11,520 --> 00:10:15,960
you're performing the IO overlaps two of

00:10:13,620 --> 00:10:18,660
these big chunks you actually need

00:10:15,960 --> 00:10:19,680
metadata from the two tables so that

00:10:18,660 --> 00:10:22,200
means that you need to keep two

00:10:19,680 --> 00:10:24,270
megabytes of metadata memory even though

00:10:22,200 --> 00:10:27,200
in practice you are only using a few

00:10:24,270 --> 00:10:30,300
entries from each one of those tables

00:10:27,200 --> 00:10:33,150
this can be solved easily by reducing

00:10:30,300 --> 00:10:35,700
the cache granularity so the cash in qmo

00:10:33,150 --> 00:10:37,910
asset reads and writes complete l2

00:10:35,700 --> 00:10:40,050
tables but there's no need to do that

00:10:37,910 --> 00:10:42,690
instead of reading complete tables we

00:10:40,050 --> 00:10:43,830
can make the cache read slices so we'll

00:10:42,690 --> 00:10:46,170
be like it smaller purchase of the

00:10:43,830 --> 00:10:47,820
tables that will only contain contain

00:10:46,170 --> 00:10:48,710
less information and that will be enough

00:10:47,820 --> 00:10:51,930
for our needs

00:10:48,710 --> 00:10:55,200
these symbols this means that you would

00:10:51,930 --> 00:10:57,270
have less disk i/o also the size of the

00:10:55,200 --> 00:11:01,200
slice can be adjusted so it would match

00:10:57,270 --> 00:11:02,520
the the host file system the benefit of

00:11:01,200 --> 00:11:04,530
this is that the

00:11:02,520 --> 00:11:06,210
this form of cuter - doesn't need to

00:11:04,530 --> 00:11:07,710
change the cue go to formal itself

00:11:06,210 --> 00:11:10,800
doesn't know about slices this is a

00:11:07,710 --> 00:11:14,160
purely internal parameter of the Yuko -

00:11:10,800 --> 00:11:17,160
driver the driver itself of course needs

00:11:14,160 --> 00:11:19,500
changes but it's relatively it need this

00:11:17,160 --> 00:11:20,910
relatively few changes and that's

00:11:19,500 --> 00:11:22,560
already patches available in the many

00:11:20,910 --> 00:11:26,550
lists I send them a couple of weeks ago

00:11:22,560 --> 00:11:29,160
and available for review this is a quick

00:11:26,550 --> 00:11:32,340
example that I tested I made with 4k

00:11:29,160 --> 00:11:34,650
random risks with an SSD button and as

00:11:32,340 --> 00:11:36,450
you can see the performance is clearly

00:11:34,650 --> 00:11:42,120
better with four keys case lassies

00:11:36,450 --> 00:11:43,350
especially with larger clusters so now

00:11:42,120 --> 00:11:46,530
that we've seen the problems there are

00:11:43,350 --> 00:11:48,630
consequence of the l2 and l2 l1 l2

00:11:46,530 --> 00:11:52,140
tables let's see the problems there are

00:11:48,630 --> 00:11:57,120
consequence of the way the clusters are

00:11:52,140 --> 00:11:59,340
allocated in QM so clusters are the

00:11:57,120 --> 00:12:00,690
smaller units of allocations and you

00:11:59,340 --> 00:12:03,120
when you allocate the new cluster you

00:12:00,690 --> 00:12:05,670
have to fill it with data so it for

00:12:03,120 --> 00:12:07,680
example you're writing in the credit

00:12:05,670 --> 00:12:09,540
region over there you had to fill the

00:12:07,680 --> 00:12:12,060
rest of the cluster with the data that

00:12:09,540 --> 00:12:13,380
was there before there was no data you

00:12:12,060 --> 00:12:15,630
had to go either to the backing file or

00:12:13,380 --> 00:12:16,760
Phillip with zeros if there was no

00:12:15,630 --> 00:12:20,220
backing file

00:12:16,760 --> 00:12:22,350
so in this case what the basic algorithm

00:12:20,220 --> 00:12:26,070
and what clemmer was doing is it first

00:12:22,350 --> 00:12:28,770
writes into that region then it reads

00:12:26,070 --> 00:12:31,230
the region immediately before from the

00:12:28,770 --> 00:12:33,090
marking file then it writes that then it

00:12:31,230 --> 00:12:35,070
reads the region afterwards and then it

00:12:33,090 --> 00:12:37,920
writes it that's a total of five

00:12:35,070 --> 00:12:40,980
operations and as you can imagine that's

00:12:37,920 --> 00:12:43,800
not very optimal way of doing it

00:12:40,980 --> 00:12:47,700
however luckily this was fixed recently

00:12:43,800 --> 00:12:49,170
and now it's in schema 2.10 we are only

00:12:47,700 --> 00:12:50,820
doing two operations we are reading the

00:12:49,170 --> 00:12:52,410
whole cluster from the backing image and

00:12:50,820 --> 00:12:55,500
we are writing it with the modified data

00:12:52,410 --> 00:12:58,260
into the new image this the results of

00:12:55,500 --> 00:13:00,210
this depend a lot on the scenario

00:12:58,260 --> 00:13:02,640
depends a lot on the cluster size and

00:13:00,210 --> 00:13:05,460
all the type of back-end but some

00:13:02,640 --> 00:13:08,490
averages that I produce when my test is

00:13:05,460 --> 00:13:12,800
60% faster in the case of rotating disks

00:13:08,490 --> 00:13:12,800
of 15% faster in the case of SSDs

00:13:12,880 --> 00:13:18,430
another way to make a location faster is

00:13:15,100 --> 00:13:20,500
that not to allocate whole clusters at

00:13:18,430 --> 00:13:22,870
the same time we could divide the

00:13:20,500 --> 00:13:24,490
cluster into sub clusters and it's time

00:13:22,870 --> 00:13:26,680
we do a location we only allocate one of

00:13:24,490 --> 00:13:29,380
the sub clusters in order to do that we

00:13:26,680 --> 00:13:31,569
would need to update the l2 table

00:13:29,380 --> 00:13:33,550
entries so they would store a bitmap

00:13:31,569 --> 00:13:36,639
that would indicate which one of the sub

00:13:33,550 --> 00:13:39,519
blasters are allocated this would reduce

00:13:36,639 --> 00:13:41,050
the allocation overhead while keeping

00:13:39,519 --> 00:13:44,829
some of the benefits of having large

00:13:41,050 --> 00:13:47,290
clusters the status of this is that this

00:13:44,829 --> 00:13:48,730
was proposed in April there was a long

00:13:47,290 --> 00:13:51,209
discussion in the mailing lists my

00:13:48,730 --> 00:13:53,529
prototype shows two to four times more

00:13:51,209 --> 00:13:57,610
IO operations per second doing

00:13:53,529 --> 00:13:59,529
allocations if you actually adjust the

00:13:57,610 --> 00:14:02,589
sub cluster size so it's equal to the

00:13:59,529 --> 00:14:05,470
request size for example you can adjust

00:14:02,589 --> 00:14:07,480
it to the whole file system block then

00:14:05,470 --> 00:14:10,870
there's no copy-on-write at all and the

00:14:07,480 --> 00:14:12,160
results are much faster other benefits

00:14:10,870 --> 00:14:13,839
is that you could actually create

00:14:12,160 --> 00:14:15,190
pre-allocated Keuka to images with

00:14:13,839 --> 00:14:18,310
backing files which is currently not

00:14:15,190 --> 00:14:20,529
possible but there are several problems

00:14:18,310 --> 00:14:22,380
with this approach one of them is the

00:14:20,529 --> 00:14:24,490
the most clear one is that it

00:14:22,380 --> 00:14:27,130
incompatible you need incompatible

00:14:24,490 --> 00:14:31,149
format changes so any image is created

00:14:27,130 --> 00:14:33,910
with sub cluster cannot be read with all

00:14:31,149 --> 00:14:35,620
the reverses of qmu it also increases

00:14:33,910 --> 00:14:38,529
the complexity of the queue code to

00:14:35,620 --> 00:14:40,290
driver more significantly that the

00:14:38,529 --> 00:14:42,250
previous changes that I was discussing

00:14:40,290 --> 00:14:43,870
and it also increases the data

00:14:42,250 --> 00:14:47,620
fragmentation in the image since we are

00:14:43,870 --> 00:14:50,920
allocating smaller portions now one

00:14:47,620 --> 00:14:52,750
other way to improve the allocation is

00:14:50,920 --> 00:14:54,490
this one so when we are right into a

00:14:52,750 --> 00:14:56,350
newly allocated cluster we must fill it

00:14:54,490 --> 00:14:58,689
with the old late as I said before if

00:14:56,350 --> 00:15:00,519
there was no later than what GME does is

00:14:58,689 --> 00:15:02,620
but the request with zeros and writes

00:15:00,519 --> 00:15:04,990
everything so in other words it write

00:15:02,620 --> 00:15:06,360
zeros into the disk so instead of

00:15:04,990 --> 00:15:09,399
writing those zeros we could use

00:15:06,360 --> 00:15:12,009
pre-allocate the cluster using F

00:15:09,399 --> 00:15:14,230
allocate and then only write the actual

00:15:12,009 --> 00:15:16,060
data that the guest was sending this

00:15:14,230 --> 00:15:19,329
needs support from the OS and the file

00:15:16,060 --> 00:15:21,100
system it should work in X t4 and XFS

00:15:19,329 --> 00:15:22,750
and this already patches in the mailing

00:15:21,100 --> 00:15:24,980
list in this case the worker was not

00:15:22,750 --> 00:15:26,360
involved in this work but I planned

00:15:24,980 --> 00:15:30,800
to review it and help bringing it

00:15:26,360 --> 00:15:33,470
forward so the last thing I would like

00:15:30,800 --> 00:15:35,750
to mention one other problem that I

00:15:33,470 --> 00:15:37,970
detected this year and although it's not

00:15:35,750 --> 00:15:40,699
the big problem in general if your

00:15:37,970 --> 00:15:41,560
storage back-end is fast it can be

00:15:40,699 --> 00:15:45,410
noticeable

00:15:41,560 --> 00:15:47,420
so when qmu works writes data and the

00:15:45,410 --> 00:15:49,940
disk their sanity check to prevent

00:15:47,420 --> 00:15:53,170
corruption so before writing data to a

00:15:49,940 --> 00:15:56,420
data clusters it first verifies that the

00:15:53,170 --> 00:16:00,100
actual place that we are writing doesn't

00:15:56,420 --> 00:16:03,130
overlap with existing metadata clusters

00:16:00,100 --> 00:16:07,430
this checks have been there for a while

00:16:03,130 --> 00:16:09,199
GM 1.7 and they are normally fast and

00:16:07,430 --> 00:16:12,380
you don't need to worry about them but

00:16:09,199 --> 00:16:14,389
some of them are relatively expensive so

00:16:12,380 --> 00:16:15,949
I'm not going to describe all of them

00:16:14,389 --> 00:16:17,750
because it doesn't I don't think it

00:16:15,949 --> 00:16:20,149
makes sense for this talk now but

00:16:17,750 --> 00:16:22,279
there's three types of tests the first

00:16:20,149 --> 00:16:24,079
type is the testers running short on

00:16:22,279 --> 00:16:26,630
time those are very fast and you don't

00:16:24,079 --> 00:16:28,370
need to worry about them the second type

00:16:26,630 --> 00:16:29,839
is the the tests that don't run in

00:16:28,370 --> 00:16:31,399
constant time but all the data that they

00:16:29,839 --> 00:16:33,680
need are in memory so they are usually

00:16:31,399 --> 00:16:36,350
also very fast and the third time

00:16:33,680 --> 00:16:38,240
actually needs to the third kind needs

00:16:36,350 --> 00:16:41,480
to go to the disk in order to to perform

00:16:38,240 --> 00:16:43,610
the check this third type of checks

00:16:41,480 --> 00:16:45,319
there's only one in this in the category

00:16:43,610 --> 00:16:48,230
is disabled by default because it's very

00:16:45,319 --> 00:16:49,819
slow as for the others there's one in

00:16:48,230 --> 00:16:51,139
particular that is the refcon block and

00:16:49,819 --> 00:16:53,810
that is particularly expensive because

00:16:51,139 --> 00:16:56,480
it needs to go over the whole drift on

00:16:53,810 --> 00:16:59,389
the table and check if all the entries

00:16:56,480 --> 00:17:03,490
and see if our request is over I'll be

00:16:59,389 --> 00:17:06,559
writing any of those entries this test

00:17:03,490 --> 00:17:08,059
the effect of this the negative impact

00:17:06,559 --> 00:17:09,740
of this test can be measured if your

00:17:08,059 --> 00:17:11,720
storage back-end is fast in my laptop I

00:17:09,740 --> 00:17:15,770
can measure it very easily if I store

00:17:11,720 --> 00:17:17,660
the Keuka to image in RAM so you want to

00:17:15,770 --> 00:17:21,410
point unfortunately we found a way to

00:17:17,660 --> 00:17:24,980
optimize it so now it should be it

00:17:21,410 --> 00:17:27,410
should be working fine and should be not

00:17:24,980 --> 00:17:29,360
noticeable at least in most cases but

00:17:27,410 --> 00:17:30,830
you might want to take a look anyway of

00:17:29,360 --> 00:17:32,720
those checks and

00:17:30,830 --> 00:17:36,830
disabled in your case and then see if

00:17:32,720 --> 00:17:39,590
they have an impact so that was

00:17:36,830 --> 00:17:41,990
basically all that I wanted to talk

00:17:39,590 --> 00:17:44,210
about I will now be given very brief

00:17:41,990 --> 00:17:46,610
summary on every cough everything so for

00:17:44,210 --> 00:17:48,470
the Q code to l2 cache the caches has

00:17:46,610 --> 00:17:50,500
been working there for a while there's

00:17:48,470 --> 00:17:53,750
the parameters to configure it already

00:17:50,500 --> 00:17:55,279
it may be nice default better default or

00:17:53,750 --> 00:17:56,480
configuration options but that's up to

00:17:55,279 --> 00:17:58,190
discussions and we don't have a

00:17:56,480 --> 00:18:01,399
conclusion of that from that yet

00:17:58,190 --> 00:18:05,510
as for l2 slices the patches on the

00:18:01,399 --> 00:18:07,909
mainly list I will be hopefully revealed

00:18:05,510 --> 00:18:10,610
soon so if everything goes fine we

00:18:07,909 --> 00:18:14,929
should be which is happening soon into

00:18:10,610 --> 00:18:17,539
mu about performing copy Android with

00:18:14,929 --> 00:18:20,330
two operations instead of five this has

00:18:17,539 --> 00:18:23,539
been already included recently so it's

00:18:20,330 --> 00:18:25,250
available in to him at 2.10 about the Q

00:18:23,539 --> 00:18:27,799
copyright with pre location instead of

00:18:25,250 --> 00:18:28,970
writing zeroes there's patches in the

00:18:27,799 --> 00:18:32,120
mailing list there still need to be

00:18:28,970 --> 00:18:34,100
reviewed subclass Terra location is

00:18:32,120 --> 00:18:37,190
still in RFC status I would like to

00:18:34,100 --> 00:18:39,649
actually to review this after all the

00:18:37,190 --> 00:18:43,850
rest of the things are immersed to see

00:18:39,649 --> 00:18:44,419
if we still make sense and decide what

00:18:43,850 --> 00:18:46,880
to do about it

00:18:44,419 --> 00:18:49,309
and then the metadata overlap checks as

00:18:46,880 --> 00:18:50,960
I said since kamo 2.9 I think all of

00:18:49,309 --> 00:18:53,179
them should be fine but you might want

00:18:50,960 --> 00:18:57,289
to check them manually in case it up

00:18:53,179 --> 00:19:00,080
they applied to your to your case and I

00:18:57,289 --> 00:19:01,279
think that's all from my side I would

00:19:00,080 --> 00:19:03,770
like to take the opportunity to thank

00:19:01,279 --> 00:19:07,929
article for funding my work and sponsor

00:19:03,770 --> 00:19:07,929
in this and if you have any questions

00:19:14,360 --> 00:19:18,840
with the sub cluster allocation you said

00:19:17,070 --> 00:19:20,340
the problem well one of the big

00:19:18,840 --> 00:19:22,710
drawbacks was he needed to change to on

00:19:20,340 --> 00:19:26,880
disk format but it's not obvious to me

00:19:22,710 --> 00:19:28,590
why the why the units you pull into your

00:19:26,880 --> 00:19:30,690
cache have to be tied to the oldest

00:19:28,590 --> 00:19:32,760
cluster size couldn't you pull in sub

00:19:30,690 --> 00:19:42,779
pieces of the LT tables into your cache

00:19:32,760 --> 00:19:44,309
without changing the on disk format the

00:19:42,779 --> 00:19:46,260
so cluster allocation the problem he

00:19:44,309 --> 00:19:48,120
seems to be addressing is that the

00:19:46,260 --> 00:19:51,270
granularity of caching is very large

00:19:48,120 --> 00:19:53,549
it's an entire cluster but I don't see

00:19:51,270 --> 00:19:56,159
why the granularity of that caching has

00:19:53,549 --> 00:19:58,710
to be tied to the on this cluster size

00:19:56,159 --> 00:20:00,809
can't that be smaller yeah there's

00:19:58,710 --> 00:20:05,700
another dimension there was one of the

00:20:00,809 --> 00:20:07,140
proposals was to change the granularity

00:20:05,700 --> 00:20:09,720
of the cache and use the slices instead

00:20:07,140 --> 00:20:12,779
of right paren see why that affects the

00:20:09,720 --> 00:20:16,620
only score man has to be on this format

00:20:12,779 --> 00:20:18,000
the sub classification the way it works

00:20:16,620 --> 00:20:19,620
is that in order to know which one of

00:20:18,000 --> 00:20:21,450
the sub blasters has been allocated you

00:20:19,620 --> 00:20:23,429
need a bitmap somewhere to store that

00:20:21,450 --> 00:20:28,470
information so that you need to store on

00:20:23,429 --> 00:20:31,470
this oh I say it's just right okay so

00:20:28,470 --> 00:20:34,380
you said that there's the cache cleaning

00:20:31,470 --> 00:20:39,210
interval is that operation expensive

00:20:34,380 --> 00:20:41,130
can you just pick any number and you can

00:20:39,210 --> 00:20:42,960
be any number basically it just sets the

00:20:41,130 --> 00:20:45,510
timer for the time that you say it and

00:20:42,960 --> 00:20:46,770
then it goes over the cache should be

00:20:45,510 --> 00:20:50,460
very quick because it's everything is in

00:20:46,770 --> 00:20:52,830
memory and it just frees that memory

00:20:50,460 --> 00:20:55,970
okay so it does not really need to be

00:20:52,830 --> 00:20:58,799
configured precisely by the users no I

00:20:55,970 --> 00:21:00,960
mean maybe that there wasn't really a

00:20:58,799 --> 00:21:02,340
discussion of is the same about default

00:21:00,960 --> 00:21:04,590
there has never been a discussion I

00:21:02,340 --> 00:21:06,899
guess one argument I guys making it a

00:21:04,590 --> 00:21:09,899
default is that it would perhaps make

00:21:06,899 --> 00:21:12,000
the amount of memory that the qmu uses a

00:21:09,899 --> 00:21:13,620
bit more volatile because you go up and

00:21:12,000 --> 00:21:15,510
down so perhaps the user doesn't want

00:21:13,620 --> 00:21:17,580
that but other than that there's it

00:21:15,510 --> 00:21:20,029
shouldn't be an expensive operation okay

00:21:17,580 --> 00:21:20,029
thanks

00:21:24,780 --> 00:21:34,560
no more questions well then I guess

00:21:31,300 --> 00:21:40,430
we're done thank you

00:21:34,560 --> 00:21:40,430

YouTube URL: https://www.youtube.com/watch?v=kmUxIOTiGNo


