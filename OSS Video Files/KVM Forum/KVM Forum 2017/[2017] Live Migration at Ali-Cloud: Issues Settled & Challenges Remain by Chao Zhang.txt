Title: [2017] Live Migration at Ali-Cloud: Issues Settled & Challenges Remain by Chao Zhang
Publication date: 2017-11-15
Playlist: KVM Forum 2017
Description: 
	Live migration technology, like vMotion, has been widely used and been playing an important role in private cloud deployments for years. However, application of this technology remains challenging in the public cloud environment. Here in Alibaba Cloud, we have been working on enabling this powerful system maintenance tool for some periods, like load balance, fault handling, hardware maintenance. In this presentation, we’d like to share how live migration is used in Alibaba Cloud for millions of VMs, the challenges we encountered and how we address them. These challenges include: reduce the downtime below certain threshold, improve the reliability of migration，and backward compatibility between different physical machines. Also, some real application scenarios and data will be presented to show how this technology is being used and becomes more and more important in the Alibaba Cloud.

---

Chao Zhang
Alibaba Cloud

Chao, as a initiated participant of the Live migration team at Ali-Cloud, has completely witnessed how this technology implanted itself into the Ali-Cloud system along a tough way. Before join Alibaba, Chao is a kernel developer engineer working at Huawei EulerLinux team for five years.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:06,200 --> 00:00:13,170
and good afternoon everybody welcome to

00:00:09,900 --> 00:00:16,560
this session and I think it is really

00:00:13,170 --> 00:00:20,369
unexpected to have so many people here

00:00:16,560 --> 00:00:22,590
this afternoon because if I didn't have

00:00:20,369 --> 00:00:25,050
the presentation I think I would

00:00:22,590 --> 00:00:28,680
definitely go out and enjoying some

00:00:25,050 --> 00:00:32,489
silencing this afternoon so thank you

00:00:28,680 --> 00:00:35,250
very much and today I want to talk about

00:00:32,489 --> 00:00:42,690
the live migration we've been doing at

00:00:35,250 --> 00:00:46,110
Al Baba cloud and this includes the the

00:00:42,690 --> 00:00:48,350
challenges and problems were met during

00:00:46,110 --> 00:00:52,140
the process with what we're trying to

00:00:48,350 --> 00:00:54,660
employ this technology to our cloud and

00:00:52,140 --> 00:00:57,899
we have done some performance tooling

00:00:54,660 --> 00:01:01,199
and a robust the improvement to enable

00:00:57,899 --> 00:01:03,949
each work fund with our cloud so most of

00:01:01,199 --> 00:01:08,430
my topic we are focused on the

00:01:03,949 --> 00:01:10,799
difference to do a migration in between

00:01:08,430 --> 00:01:15,000
the the migration in a virtualized

00:01:10,799 --> 00:01:17,670
environment and in the cloud and at last

00:01:15,000 --> 00:01:20,840
I will show some of that love migration

00:01:17,670 --> 00:01:24,570
application in our cloud to in trigger

00:01:20,840 --> 00:01:28,369
how powerful these tools can help us to

00:01:24,570 --> 00:01:32,220
improve efficiency in the cloud

00:01:28,369 --> 00:01:34,530
before we go go into the details last we

00:01:32,220 --> 00:01:36,030
have a quick review about the

00:01:34,530 --> 00:01:40,759
traditional live migration in

00:01:36,030 --> 00:01:44,070
virtualization this picture illustrates

00:01:40,759 --> 00:01:46,860
the traditional live migration the top

00:01:44,070 --> 00:01:50,009
the top layer is a source host and the

00:01:46,860 --> 00:01:53,340
lower layer is a destination host in the

00:01:50,009 --> 00:01:55,100
pre copy procedure memories are copied

00:01:53,340 --> 00:01:56,549
from the source to the destination

00:01:55,100 --> 00:02:00,930
iteratively

00:01:56,549 --> 00:02:02,930
and then are some sometimes later that

00:02:00,930 --> 00:02:06,840
iterative becomes to congressional and

00:02:02,930 --> 00:02:12,000
the VM is posed and then the last memory

00:02:06,840 --> 00:02:13,959
is copied with state of sort with device

00:02:12,000 --> 00:02:18,010
status and VM Staters

00:02:13,959 --> 00:02:18,640
so we we refer to this this time that we

00:02:18,010 --> 00:02:22,239
unstopped

00:02:18,640 --> 00:02:24,730
as the VM Downton and then the VM we are

00:02:22,239 --> 00:02:28,569
well be restarted running and as a

00:02:24,730 --> 00:02:30,810
destination and and also the storage we

00:02:28,569 --> 00:02:33,329
are be reopened network will be

00:02:30,810 --> 00:02:38,349
reconnected

00:02:33,329 --> 00:02:40,239
but in alibaba cloud what we required

00:02:38,349 --> 00:02:44,920
what's the difference between the

00:02:40,239 --> 00:02:48,519
traditional migration with our cloud is

00:02:44,920 --> 00:02:51,730
as this picture shows we would we

00:02:48,519 --> 00:02:54,459
require is that not a single virtualized

00:02:51,730 --> 00:02:57,220
instance migrating as the picture

00:02:54,459 --> 00:03:00,609
illustrates at the lower side we have

00:02:57,220 --> 00:03:05,609
cloud disk lipids in network and the

00:03:00,609 --> 00:03:08,680
upper outside we have security services

00:03:05,609 --> 00:03:13,389
load-balanced services and other cloud

00:03:08,680 --> 00:03:15,579
services so we require that a weak monk

00:03:13,389 --> 00:03:17,829
that love migration should be

00:03:15,579 --> 00:03:21,970
transparent to the whole cloud system

00:03:17,829 --> 00:03:25,060
not the VM itself another challenge we

00:03:21,970 --> 00:03:28,239
have is that because the skill hour of

00:03:25,060 --> 00:03:30,299
our cloud is the increasing every day so

00:03:28,239 --> 00:03:32,769
the hardware and software back

00:03:30,299 --> 00:03:35,109
compatibility is also a very big

00:03:32,769 --> 00:03:37,720
challenge for us how to migrate between

00:03:35,109 --> 00:03:40,599
different versions of hardware and

00:03:37,720 --> 00:03:46,180
software another way is the robust a lot

00:03:40,599 --> 00:03:49,509
of migration as the stableness is a very

00:03:46,180 --> 00:03:53,319
crucial crucial parameter for the cloud

00:03:49,509 --> 00:03:57,129
so we can see that our live machine have

00:03:53,319 --> 00:04:01,120
a 90 percent of success rate we need to

00:03:57,129 --> 00:04:04,209
and it very it is very open to say that

00:04:01,120 --> 00:04:07,930
almost every time some components will

00:04:04,209 --> 00:04:09,939
not return what you expected at the

00:04:07,930 --> 00:04:13,750
migration so what what when we work

00:04:09,939 --> 00:04:19,479
should we do when something ever happens

00:04:13,750 --> 00:04:23,650
to the to the migration so in order to

00:04:19,479 --> 00:04:26,020
do all these things as we can see from

00:04:23,650 --> 00:04:27,539
the picture the migration operations

00:04:26,020 --> 00:04:31,500
required in the cloud

00:04:27,539 --> 00:04:34,270
becomes several times increase increased

00:04:31,500 --> 00:04:37,419
many other operations should be done in

00:04:34,270 --> 00:04:39,940
the control system some truly should be

00:04:37,419 --> 00:04:42,400
done in the virtualized plane and other

00:04:39,940 --> 00:04:46,259
cloud services also need to be

00:04:42,400 --> 00:04:49,720
coordinated to to prepare some

00:04:46,259 --> 00:04:52,780
operations accordingly to leave what was

00:04:49,720 --> 00:04:55,389
worse is that all these operations are

00:04:52,780 --> 00:05:01,060
relevant to each other and they should

00:04:55,389 --> 00:05:05,710
be done defined orderly way so what

00:05:01,060 --> 00:05:09,400
should we do the first thing we think

00:05:05,710 --> 00:05:11,979
about is to like the migration in the

00:05:09,400 --> 00:05:16,270
tradition of virtualization environment

00:05:11,979 --> 00:05:20,220
will define the migration steps into

00:05:16,270 --> 00:05:24,070
five steps and for each staff with

00:05:20,220 --> 00:05:27,880
defined engine status interest standard

00:05:24,070 --> 00:05:31,060
which means that when the migration want

00:05:27,880 --> 00:05:34,360
to go from one step to another every

00:05:31,060 --> 00:05:38,110
components should should enter their

00:05:34,360 --> 00:05:41,020
defined States like if we want to enter

00:05:38,110 --> 00:05:43,270
the pre-migration that disk must be

00:05:41,020 --> 00:05:46,449
opened with read-only and the session

00:05:43,270 --> 00:05:49,449
Cobie must be start so any operations in

00:05:46,449 --> 00:05:52,210
this Stellar's field and would cost or

00:05:49,449 --> 00:05:55,889
the course whole migration to be for

00:05:52,210 --> 00:05:58,990
back this have this house three

00:05:55,889 --> 00:06:01,509
advantages the first one is that after

00:05:58,990 --> 00:06:04,270
decoupling many of the operations could

00:06:01,509 --> 00:06:09,419
be done in a paralyzed way and the

00:06:04,270 --> 00:06:13,449
second is that we do not care about the

00:06:09,419 --> 00:06:19,509
internal implementation of the specific

00:06:13,449 --> 00:06:24,009
department the implication becomes

00:06:19,509 --> 00:06:28,419
decoupled then the optimization can be

00:06:24,009 --> 00:06:33,840
performed that you can be performed due

00:06:28,419 --> 00:06:37,089
to the bonus of the of the decoupling

00:06:33,840 --> 00:06:40,039
the first one is that critical path

00:06:37,089 --> 00:06:42,529
parallel after mmm

00:06:40,039 --> 00:06:45,169
after the cream this is used to to

00:06:42,529 --> 00:06:48,259
reduce the downturn so that things can

00:06:45,169 --> 00:06:51,020
be done in parallel and another

00:06:48,259 --> 00:06:54,050
operation is that heavy operations in

00:06:51,020 --> 00:06:56,839
the cloud is dismantled and rearranged

00:06:54,050 --> 00:07:00,289
like a PDR a flash and session called a

00:06:56,839 --> 00:07:04,159
Sun has been put ahead of the critical

00:07:00,289 --> 00:07:06,830
path and some being postponed in the

00:07:04,159 --> 00:07:11,439
virtualization we also add a pre last

00:07:06,830 --> 00:07:19,729
copy stage this is used to be used to

00:07:11,439 --> 00:07:22,789
make the VM downtown to be stable and to

00:07:19,729 --> 00:07:26,949
be stable especially when the VM had a

00:07:22,789 --> 00:07:31,939
sudden surge of the load when we try to

00:07:26,949 --> 00:07:36,800
switch the running host unlike some

00:07:31,939 --> 00:07:40,009
unlike the qumu as upstream we do not

00:07:36,800 --> 00:07:40,819
compress the memory as through or the

00:07:40,009 --> 00:07:44,180
narration

00:07:40,819 --> 00:07:48,139
we only compress the last copy of the

00:07:44,180 --> 00:07:52,069
memory so - this is used to elevate the

00:07:48,139 --> 00:07:56,990
CPU impact to the host and the area also

00:07:52,069 --> 00:08:03,229
as useful as compression the memories

00:07:56,990 --> 00:08:05,479
through arson love migration another

00:08:03,229 --> 00:08:10,879
thing we have done is the cloud disk

00:08:05,479 --> 00:08:13,610
optimization at first the faulty scraper

00:08:10,879 --> 00:08:17,029
descriptor at a destination is open

00:08:13,610 --> 00:08:21,259
read-only at the destination and then

00:08:17,029 --> 00:08:24,830
the sauce we are closed FD and after the

00:08:21,259 --> 00:08:27,199
VM transfers is transverse the running

00:08:24,830 --> 00:08:29,599
host the destination we are reopen the

00:08:27,199 --> 00:08:32,529
FT with read our white flag

00:08:29,599 --> 00:08:35,719
but however what we finally do is that

00:08:32,529 --> 00:08:39,050
closing after you in a distributed file

00:08:35,719 --> 00:08:42,500
system is very time consuming as a lot

00:08:39,050 --> 00:08:45,709
of math data and the negotiated message

00:08:42,500 --> 00:08:48,199
should we all be transferred between the

00:08:45,709 --> 00:08:51,019
client and the server so here we

00:08:48,199 --> 00:08:53,570
introduced a post operation for the FT

00:08:51,019 --> 00:08:57,230
it is just very light actions

00:08:53,570 --> 00:09:01,210
his only block IO from the block IO from

00:08:57,230 --> 00:09:05,060
the host so that the destination can be

00:09:01,210 --> 00:09:07,970
opened reopens after you with rewrites

00:09:05,060 --> 00:09:10,790
operation very as soon as a migration

00:09:07,970 --> 00:09:13,850
heaven it just likes to tell the fall

00:09:10,790 --> 00:09:20,270
systems that take it easy and just like

00:09:13,850 --> 00:09:25,520
in between of the migration and in the

00:09:20,270 --> 00:09:28,100
repeal of APC and I sent in Network a

00:09:25,520 --> 00:09:31,040
lot of migration optimization should be

00:09:28,100 --> 00:09:33,560
made to because in the tradition like in

00:09:31,040 --> 00:09:36,410
the VP say a nesting Network the

00:09:33,560 --> 00:09:39,350
management of the network is removed

00:09:36,410 --> 00:09:42,620
from the switches and Reuters to a

00:09:39,350 --> 00:09:46,100
centralized manager so when we do a

00:09:42,620 --> 00:09:50,150
migration like we migrate when one to

00:09:46,100 --> 00:09:53,690
another host we we have to notify the

00:09:50,150 --> 00:09:56,270
centralized and network manager that we

00:09:53,690 --> 00:09:58,790
the migration begin and the usually

00:09:56,270 --> 00:10:02,990
install the new flourish used to the new

00:09:58,790 --> 00:10:05,270
host but it's very time consuming to to

00:10:02,990 --> 00:10:08,600
wait for the Nana manager to install all

00:10:05,270 --> 00:10:14,510
these florals one by one and it really

00:10:08,600 --> 00:10:16,310
takes passion patience to just wait in

00:10:14,510 --> 00:10:20,270
order to soldiers so we have

00:10:16,310 --> 00:10:24,590
investigated a lot of things in vbc and

00:10:20,270 --> 00:10:30,080
a sting network alas we found that in a

00:10:24,590 --> 00:10:33,080
sting in a sting scenery the for back of

00:10:30,080 --> 00:10:36,790
a sting of England in the esting Network

00:10:33,080 --> 00:10:42,680
is quite similar with a migration here

00:10:36,790 --> 00:10:45,830
so some idea of the for back of esteem

00:10:42,680 --> 00:10:48,470
was broadening when we migrated Vienna

00:10:45,830 --> 00:10:51,110
from the source to the destination a

00:10:48,470 --> 00:10:53,960
relay forwarding route is established

00:10:51,110 --> 00:10:57,680
between the source host and the

00:10:53,960 --> 00:10:59,620
destination host but it is not enabled

00:10:57,680 --> 00:11:01,880
as a Viennese keeps running on a source

00:10:59,620 --> 00:11:04,370
when the way when when the Williams

00:11:01,880 --> 00:11:07,260
wrong stage transferred from the source

00:11:04,370 --> 00:11:10,560
to the destination the relay forwarding

00:11:07,260 --> 00:11:13,350
roots is then enabled and it collects

00:11:10,560 --> 00:11:15,780
all the packets that received in the

00:11:13,350 --> 00:11:18,750
sauce and forward forwarding them to

00:11:15,780 --> 00:11:22,140
them to the destination host which we

00:11:18,750 --> 00:11:25,920
are ultimately be sent to the vn1 in the

00:11:22,140 --> 00:11:29,180
destination and after a few times the

00:11:25,920 --> 00:11:33,440
fluid will be eventually installed on

00:11:29,180 --> 00:11:35,640
the host like Ram - and we m1 and then

00:11:33,440 --> 00:11:38,820
when - and we oh man

00:11:35,640 --> 00:11:42,840
we'll talk directly and this this

00:11:38,820 --> 00:11:46,850
introduced dynamic the network Brickton

00:11:42,840 --> 00:11:49,710
a lot also in the VP Singh Network when

00:11:46,850 --> 00:11:52,850
we need to copy the network session

00:11:49,710 --> 00:12:00,900
tables and we copy it as the memories

00:11:52,850 --> 00:12:03,540
iterative loop as we can see from the

00:12:00,900 --> 00:12:05,900
previous picture what we require we

00:12:03,540 --> 00:12:09,570
require that lamb equation should be

00:12:05,900 --> 00:12:13,020
friendly to the cloud services which

00:12:09,570 --> 00:12:15,660
means that the lava the cloud services

00:12:13,020 --> 00:12:18,990
should stay intact when we doing the

00:12:15,660 --> 00:12:20,760
live migration and this has really well

00:12:18,990 --> 00:12:23,100
when it has an exception that you

00:12:20,760 --> 00:12:26,490
already have have two reasons the first

00:12:23,100 --> 00:12:29,900
one is that indirect reham hosts

00:12:26,490 --> 00:12:33,060
relationship which means that the client

00:12:29,900 --> 00:12:36,360
cloud services has some information

00:12:33,060 --> 00:12:39,570
stored on the source host so when we're

00:12:36,360 --> 00:12:42,240
doing the migration all this information

00:12:39,570 --> 00:12:46,230
should be migrated as a memory from the

00:12:42,240 --> 00:12:49,080
source host to the destination another

00:12:46,230 --> 00:12:52,590
category is that direct VN host

00:12:49,080 --> 00:12:55,560
relationship which means that the class

00:12:52,590 --> 00:12:58,190
service is architectural attached to the

00:12:55,560 --> 00:13:01,500
location of the wheel like we have shown

00:12:58,190 --> 00:13:05,970
the people like we we have shown the

00:13:01,500 --> 00:13:09,300
picture here a cloud service which is

00:13:05,970 --> 00:13:12,810
based on the DB TK in as a part of

00:13:09,300 --> 00:13:15,330
networking processing system well the

00:13:12,810 --> 00:13:18,690
ones of when the migration of the VM

00:13:15,330 --> 00:13:20,940
happens the DB DK will get confused

00:13:18,690 --> 00:13:22,890
because the packet from the

00:13:20,940 --> 00:13:30,120
sym flow may come from a different

00:13:22,890 --> 00:13:33,350
network cream there are many ways to to

00:13:30,120 --> 00:13:36,720
fix this one way is to just do a

00:13:33,350 --> 00:13:40,020
rescheduling in the in the DPD kaeleen

00:13:36,720 --> 00:13:42,900
software and does this we are resulting

00:13:40,020 --> 00:13:46,730
in some a panel performance penalty so

00:13:42,900 --> 00:13:51,780
we talked to our device providers and

00:13:46,730 --> 00:13:55,320
after few discussion we decided that the

00:13:51,780 --> 00:13:58,970
provider of the devices shouldn't modify

00:13:55,320 --> 00:14:02,850
their firmware and offer a unique

00:13:58,970 --> 00:14:03,540
formula which will reschedule the

00:14:02,850 --> 00:14:05,880
package

00:14:03,540 --> 00:14:10,470
according some to some invariant

00:14:05,880 --> 00:14:14,430
parameters in the package so in order to

00:14:10,470 --> 00:14:18,440
makes love migration work in our system

00:14:14,430 --> 00:14:21,710
we have done a lot of efforts to make

00:14:18,440 --> 00:14:25,020
the clouds ecosystem of our cloud

00:14:21,710 --> 00:14:30,410
friendly to the live migration and this

00:14:25,020 --> 00:14:30,410
really takes a lot of effort to do so

00:14:33,260 --> 00:14:40,530
last but not least the control system

00:14:37,130 --> 00:14:42,930
the most important thing would do we

00:14:40,530 --> 00:14:45,090
tried in the control system used to

00:14:42,930 --> 00:14:47,370
downwards the time critical operations

00:14:45,090 --> 00:14:50,970
from the control system to the

00:14:47,370 --> 00:14:53,790
virtualization plane so as we finished

00:14:50,970 --> 00:14:57,360
the architecture of elaboration becomes

00:14:53,790 --> 00:15:00,180
as follows most of the important time

00:14:57,360 --> 00:15:02,839
critical operations is finished in the

00:15:00,180 --> 00:15:05,610
virtualization play and the manager is

00:15:02,839 --> 00:15:09,480
this allows the manager to focused on

00:15:05,610 --> 00:15:12,990
that migration trigger point like when

00:15:09,480 --> 00:15:18,270
to trigger level migration or if we need

00:15:12,990 --> 00:15:20,339
to cancel the migration now so that

00:15:18,270 --> 00:15:25,290
control system can mainly focus on the

00:15:20,339 --> 00:15:30,450
control policy after all this

00:15:25,290 --> 00:15:34,660
optimization being employed the test

00:15:30,450 --> 00:15:37,410
later week actually it is this is the

00:15:34,660 --> 00:15:42,360
testator we currently have in our cloud

00:15:37,410 --> 00:15:45,670
in order to able to to easy is a compare

00:15:42,360 --> 00:15:48,790
comparable of these deserts so I just

00:15:45,670 --> 00:15:52,209
show the migration of idle guy guests

00:15:48,790 --> 00:15:53,769
always some stable stable stress because

00:15:52,209 --> 00:15:58,060
of migration the very sensitive would

00:15:53,769 --> 00:16:00,129
choose an instant ven stress and we can

00:15:58,060 --> 00:16:03,730
see that currently we have a web

00:16:00,129 --> 00:16:07,420
downtown around 70 to 80 milliseconds

00:16:03,730 --> 00:16:12,250
and even we have some stress on edge it

00:16:07,420 --> 00:16:16,930
is still very small I think it is

00:16:12,250 --> 00:16:20,529
bearable in the cloud before all these

00:16:16,930 --> 00:16:26,740
migrations being made in our cloud we

00:16:20,529 --> 00:16:30,550
have maybe just like when we first done

00:16:26,740 --> 00:16:33,310
the migration in our cloud we have after

00:16:30,550 --> 00:16:37,360
we have doing some fixes and enables a

00:16:33,310 --> 00:16:43,689
lot of migration the the VM downtown is

00:16:37,360 --> 00:16:45,759
about around 18 18 seconds so after all

00:16:43,689 --> 00:16:48,519
these works has been done that downtown

00:16:45,759 --> 00:16:54,459
has become to turn down to from the 18

00:16:48,519 --> 00:16:56,110
seconds to the 100 milliseconds so after

00:16:54,459 --> 00:17:00,069
all these things have done I think

00:16:56,110 --> 00:17:03,100
migration can be can gradually to take

00:17:00,069 --> 00:17:06,939
effect in our cloud so what is the value

00:17:03,100 --> 00:17:12,520
it brings to us the first one the server

00:17:06,939 --> 00:17:15,850
attendance the hardware you the hardware

00:17:12,520 --> 00:17:20,829
menu from malfunctions a lot but most of

00:17:15,850 --> 00:17:24,220
time it is not fatal so for this not on

00:17:20,829 --> 00:17:28,270
fatal errors we can just migrate all the

00:17:24,220 --> 00:17:32,110
VMS to a healthy host which and then the

00:17:28,270 --> 00:17:39,360
host can the error host can be repaired

00:17:32,110 --> 00:17:42,659
and repaired and user game but it and

00:17:39,360 --> 00:17:46,510
all this host and all these main

00:17:42,659 --> 00:17:48,370
operations made made the customer of our

00:17:46,510 --> 00:17:54,039
cloud mutants are

00:17:48,370 --> 00:17:57,039
Holloway our failure in addition to the

00:17:54,039 --> 00:17:59,679
hardware fear failure colonel and

00:17:57,039 --> 00:18:02,470
formers need software's need to be

00:17:59,679 --> 00:18:06,370
upgrading to although we have enabled

00:18:02,470 --> 00:18:09,820
the life - love passion and hope hot

00:18:06,370 --> 00:18:12,700
upgrade technology in our cloud but

00:18:09,820 --> 00:18:15,929
sometimes like Colonel and formworks

00:18:12,700 --> 00:18:20,320
they still need to they still need the

00:18:15,929 --> 00:18:22,899
host to be rebooted to take effect so we

00:18:20,320 --> 00:18:26,590
build a ruling system in our cloud which

00:18:22,899 --> 00:18:30,220
attack is attached to our Alibaba

00:18:26,590 --> 00:18:32,649
maintenance system the rolling system we

00:18:30,220 --> 00:18:34,779
are gradually trying to migrate variants

00:18:32,649 --> 00:18:38,950
on the NSA and the triggers a

00:18:34,779 --> 00:18:42,760
maintenance system to upgrade the kernel

00:18:38,950 --> 00:18:45,760
and former in the cluster as this table

00:18:42,760 --> 00:18:48,850
shows some improvements we've get after

00:18:45,760 --> 00:18:53,070
the kernel and firmware upgrading we can

00:18:48,850 --> 00:18:57,190
say that average about 8% improvement

00:18:53,070 --> 00:19:01,809
improvement is obtained after the coke

00:18:57,190 --> 00:19:03,159
last term upgrading and also and all

00:19:01,809 --> 00:19:09,190
these things and means these old

00:19:03,159 --> 00:19:11,980
machines writing again we also try some

00:19:09,190 --> 00:19:16,240
advanced scare cloud scheduling

00:19:11,980 --> 00:19:20,909
technologies in our cloud like resource

00:19:16,240 --> 00:19:24,789
to fragments and resource balance

00:19:20,909 --> 00:19:27,490
refreshed resource to fragments means

00:19:24,789 --> 00:19:30,640
that from the view of the cluster we

00:19:27,490 --> 00:19:32,880
still have plenty of unused resources in

00:19:30,640 --> 00:19:36,669
the cluster but when we're trying to

00:19:32,880 --> 00:19:39,760
allocate we end with mostly breweries

00:19:36,669 --> 00:19:43,899
which requires mostly pure resource and

00:19:39,760 --> 00:19:47,350
the larger memory which we find that we

00:19:43,899 --> 00:19:51,669
cannot find an available host in the

00:19:47,350 --> 00:19:55,090
whole cluster because the resources have

00:19:51,669 --> 00:19:57,220
been fragments and live migration allow

00:19:55,090 --> 00:20:00,200
us to reorder the viens

00:19:57,220 --> 00:20:05,330
the the location of the VMS in the class

00:20:00,200 --> 00:20:10,029
and thus can help us to increase the

00:20:05,330 --> 00:20:13,549
usage of the cloud of that classroom

00:20:10,029 --> 00:20:17,149
resource balance is used when there is a

00:20:13,549 --> 00:20:19,789
search over the CPU usage or memory

00:20:17,149 --> 00:20:25,090
usage in the host and then the VM will

00:20:19,789 --> 00:20:30,289
be migration will be triggered to to

00:20:25,090 --> 00:20:33,080
avoid the contention in our cloud also

00:20:30,289 --> 00:20:34,759
the lab migration can use to help the

00:20:33,080 --> 00:20:38,210
power management of the whole cluster

00:20:34,759 --> 00:20:45,019
but there is a lot a lot of things needs

00:20:38,210 --> 00:20:49,369
us to be done in our cloud so it is an

00:20:45,019 --> 00:20:51,470
order to do list after all the all these

00:20:49,369 --> 00:20:54,289
things have done love migration is

00:20:51,470 --> 00:20:57,320
gradually taking a very important role

00:20:54,289 --> 00:21:01,179
in our cloud but still some of the

00:20:57,320 --> 00:21:06,139
challenges need to be fixed

00:21:01,179 --> 00:21:09,649
the first one is that as our way I away

00:21:06,139 --> 00:21:12,379
or pass through migration this is not a

00:21:09,649 --> 00:21:15,320
new problem but it is getting worse and

00:21:12,379 --> 00:21:18,440
worse in our cloud because more and more

00:21:15,320 --> 00:21:21,919
devices start to using as our over your

00:21:18,440 --> 00:21:24,769
past role in our cloud to improve our

00:21:21,919 --> 00:21:27,409
performance but currently most of

00:21:24,769 --> 00:21:30,590
solutions related to this device

00:21:27,409 --> 00:21:35,210
migrating is focused on the hot-plug of

00:21:30,590 --> 00:21:37,730
these devices but we don't think that it

00:21:35,210 --> 00:21:41,629
is practical in the actual production

00:21:37,730 --> 00:21:44,210
environment the worst thing is that the

00:21:41,629 --> 00:21:47,389
SRO way and past room makes the guests

00:21:44,210 --> 00:21:50,899
aware of the live migration which which

00:21:47,389 --> 00:21:53,029
is really frustrating because before

00:21:50,899 --> 00:21:55,220
doing even research given life

00:21:53,029 --> 00:21:58,580
innovation we have been made a lot of

00:21:55,220 --> 00:22:01,669
effort in them to enable the den

00:21:58,580 --> 00:22:04,999
evaluation in our cloud and like then

00:22:01,669 --> 00:22:09,919
then have dense lab migration is relied

00:22:04,999 --> 00:22:12,790
on the APS API to suspend a VM so the

00:22:09,919 --> 00:22:14,650
the drivers in the VN I

00:22:12,790 --> 00:22:17,830
in war will introduce the lover may we

00:22:14,650 --> 00:22:20,470
live migration and this is all we have

00:22:17,830 --> 00:22:22,600
been doing a lot of message a lot of

00:22:20,470 --> 00:22:24,880
effort to handle different drivers

00:22:22,600 --> 00:22:28,660
different versions of drivers and

00:22:24,880 --> 00:22:35,980
different film distributions this guest

00:22:28,660 --> 00:22:40,660
aware is really frustrating and another

00:22:35,980 --> 00:22:44,140
way is to another thing I want to

00:22:40,660 --> 00:22:48,550
mention is how to use lab migration

00:22:44,140 --> 00:22:52,360
wearing that house as currently we can

00:22:48,550 --> 00:22:56,560
say from the graph we have a variety of

00:22:52,360 --> 00:22:58,510
instance types in our cloud some some of

00:22:56,560 --> 00:23:01,540
them focused on the performance of the

00:22:58,510 --> 00:23:04,570
Vienna some they are fixed on the price

00:23:01,540 --> 00:23:06,760
so they have different requirements to

00:23:04,570 --> 00:23:09,850
the service levels or the robot needs of

00:23:06,760 --> 00:23:16,200
the migration of the service the cloud

00:23:09,850 --> 00:23:20,110
can offer so so it is very important to

00:23:16,200 --> 00:23:22,150
to when we need to do the migration to

00:23:20,110 --> 00:23:25,630
find which we am to be should be

00:23:22,150 --> 00:23:28,690
migrated to and when to the tsardom

00:23:25,630 --> 00:23:31,480
lacrimation to be taken place and where

00:23:28,690 --> 00:23:35,580
to migrate this VM and how to my

00:23:31,480 --> 00:23:41,970
resident now another thing that is

00:23:35,580 --> 00:23:45,070
difficult to handle is that the we have

00:23:41,970 --> 00:23:50,980
heterogeneous architecture in our cloud

00:23:45,070 --> 00:23:53,560
like given then GPU and have PGA all

00:23:50,980 --> 00:23:56,020
these devices are included in the

00:23:53,560 --> 00:24:00,250
cluster so how to navigate through this

00:23:56,020 --> 00:24:03,400
has heterogeneous architectures is a is

00:24:00,250 --> 00:24:11,220
also a very difficult thing to did t

00:24:03,400 --> 00:24:15,730
always okay I think all about is about

00:24:11,220 --> 00:24:19,690
since I want to introduce about the what

00:24:15,730 --> 00:24:22,170
we have done is allow migration any

00:24:19,690 --> 00:24:22,170
questions

00:24:43,700 --> 00:24:47,020
okay thank you

00:24:46,210 --> 00:24:53,270
[Applause]

00:24:47,020 --> 00:24:53,270

YouTube URL: https://www.youtube.com/watch?v=-IcHsnQ1S30


