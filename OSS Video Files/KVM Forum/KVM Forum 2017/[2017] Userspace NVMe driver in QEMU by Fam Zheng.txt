Title: [2017] Userspace NVMe driver in QEMU by Fam Zheng
Publication date: 2017-11-09
Playlist: KVM Forum 2017
Description: 
	Just like how DPDK enables userspace applications to archieve fast networking, VFIO and SPDK makes it feasible for a virtual machine to use host's NVMe devices to back the virtual disk thus allows for better performance/flexibility balance. In this presentation, Fam Zheng will explore the integration of QEMU block layer with VFIO and SPDK, discuss the challenges hidden in the details, compare the result with linux-aio and PCI passthrough and talk about the further work.

---

Fam Zheng
Red Hat, Inc.
Senior Software Engineer

Fam Zheng is a developer in the Red Hat virtualization team. He worked on various aspects of QEMU and KVM in past years, and is now focused on VirtIO and block performance. He has presented Improving the QEMU Event Loop in KVM Forum 2015 and the introduction to QEMU block layer in FOSSASIA 2013.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:06,319 --> 00:00:19,560
I work for Kevin forum sorry I work for

00:00:13,410 --> 00:00:21,240
Red Hat and mostly in blockly of Q so in

00:00:19,560 --> 00:00:22,680
this talk I'm going to talk about the

00:00:21,240 --> 00:00:27,050
user space and we have me dry very

00:00:22,680 --> 00:00:31,109
creamy so first of all a few words about

00:00:27,050 --> 00:00:33,390
me I'm me it's a set of a standard

00:00:31,109 --> 00:00:38,100
that's relatively more than two fully

00:00:33,390 --> 00:00:43,440
utilize the advantage of non-volatile

00:00:38,100 --> 00:00:49,440
memory including SSD so it has its

00:00:43,440 --> 00:00:51,690
benefits in that it uses a faster passes

00:00:49,440 --> 00:00:53,640
and controllers to achieve better

00:00:51,690 --> 00:00:57,149
performance and it also has built-in

00:00:53,640 --> 00:01:00,090
support abilities of high numbers of

00:00:57,149 --> 00:01:04,769
Iowa cues and the cute deaths

00:01:00,090 --> 00:01:08,040
it supports 64k our cues and the each

00:01:04,769 --> 00:01:09,689
cue can handle 64 commands per Q so it

00:01:08,040 --> 00:01:12,119
has a patient command issue a and

00:01:09,689 --> 00:01:16,439
completion handling as well which means

00:01:12,119 --> 00:01:19,490
if you want to submit a request from the

00:01:16,439 --> 00:01:23,790
driver or handling a completion from the

00:01:19,490 --> 00:01:26,490
from the device you don't have to do a

00:01:23,790 --> 00:01:28,500
lot of in operations all you have to do

00:01:26,490 --> 00:01:33,600
is basically a simple write or read from

00:01:28,500 --> 00:01:36,860
the mapped memory so that you can talk

00:01:33,600 --> 00:01:39,750
to the device so about the

00:01:36,860 --> 00:01:43,369
functionalities it has a extensive port

00:01:39,750 --> 00:01:48,630
command sets which supports physically

00:01:43,369 --> 00:01:51,060
most commonly the storage device which

00:01:48,630 --> 00:01:53,909
we will see in this talk and for the

00:01:51,060 --> 00:01:57,210
past it can be attached that's the PCIe

00:01:53,909 --> 00:02:00,000
device for em - or even fabrics which

00:01:57,210 --> 00:02:05,180
allows you to connect your hosts with

00:02:00,000 --> 00:02:05,180
external storage which FC or our DMA

00:02:05,450 --> 00:02:12,239
these are relatively new but we will

00:02:08,429 --> 00:02:15,170
currently focus on the PCIe device so

00:02:12,239 --> 00:02:19,230
that's a picture

00:02:15,170 --> 00:02:22,980
the SSD storage card from intel which

00:02:19,230 --> 00:02:26,010
also it's also what I work with for the

00:02:22,980 --> 00:02:28,770
user base driver for nvme so it's

00:02:26,010 --> 00:02:31,410
basically it's a storage device if you

00:02:28,770 --> 00:02:35,430
plug into the system you can find a new

00:02:31,410 --> 00:02:38,220
block device under your tab and you can

00:02:35,430 --> 00:02:46,500
create partitions and file systems on

00:02:38,220 --> 00:02:49,860
top of it so why do we want a new driver

00:02:46,500 --> 00:02:52,290
in torino and instead of using the in

00:02:49,860 --> 00:02:54,360
kernel driver because we already have

00:02:52,290 --> 00:02:58,890
the in kernel driver that can allows you

00:02:54,360 --> 00:03:00,209
to access the devices storage and you

00:02:58,890 --> 00:03:05,550
can have all the features that is

00:03:00,209 --> 00:03:09,630
already available on your host why do we

00:03:05,550 --> 00:03:12,840
do a user space driver so the the answer

00:03:09,630 --> 00:03:15,530
is relatively simple we want to better

00:03:12,840 --> 00:03:22,440
overhead when we introduce

00:03:15,530 --> 00:03:28,190
virtualization because in the old days

00:03:22,440 --> 00:03:32,880
we have a spindle disks which handles

00:03:28,190 --> 00:03:36,000
random I all relatively poorly so this

00:03:32,880 --> 00:03:41,730
is a modern SATA disk that it has a

00:03:36,000 --> 00:03:47,070
spindle on a server and we run a set of

00:03:41,730 --> 00:03:50,070
FIO workloads on a disk on bare metal so

00:03:47,070 --> 00:03:54,900
hot path is only about a few hundreds i

00:03:50,070 --> 00:03:57,959
hopes per second that's a lot of very

00:03:54,900 --> 00:04:03,060
good number and it's not surprising

00:03:57,959 --> 00:04:06,330
either so with that we can create a VM

00:04:03,060 --> 00:04:10,019
and try if the VM paths as well as the

00:04:06,330 --> 00:04:14,370
host the answer is well it's fine

00:04:10,019 --> 00:04:16,940
because you know the device is slow but

00:04:14,370 --> 00:04:21,030
the CPU is parsed so the virtualization

00:04:16,940 --> 00:04:24,180
only adds only a tiny bit of overhead on

00:04:21,030 --> 00:04:26,070
top of that so the numbers now are

00:04:24,180 --> 00:04:29,100
basically the same as

00:04:26,070 --> 00:04:31,130
performance maybe you can notice a

00:04:29,100 --> 00:04:33,920
little bit of gap at least but it's more

00:04:31,130 --> 00:04:37,710
noise other than the real overhead

00:04:33,920 --> 00:04:43,350
because the actual overhead the incurred

00:04:37,710 --> 00:04:47,520
by virtualizing is mostly normal so if

00:04:43,350 --> 00:04:51,360
we know each TDS are bad at random i/o

00:04:47,520 --> 00:04:54,900
but we have butter devices that is SSD

00:04:51,360 --> 00:04:59,600
based ones so previously we have a SATA

00:04:54,900 --> 00:05:03,750
attached SSDs which can of course do

00:04:59,600 --> 00:05:08,340
random i/o much better than the old

00:05:03,750 --> 00:05:12,000
spindles it can achieve like hundreds of

00:05:08,340 --> 00:05:17,100
times of better I opted per second I was

00:05:12,000 --> 00:05:20,430
per second and we can imagine that if

00:05:17,100 --> 00:05:23,940
it's put into a BM you might see

00:05:20,430 --> 00:05:27,420
something different so that if we runs

00:05:23,940 --> 00:05:32,430
the same workload in BM we see a little

00:05:27,420 --> 00:05:38,160
bit of overhead that's not too big but

00:05:32,430 --> 00:05:41,310
seen it's also very noticeable and now

00:05:38,160 --> 00:05:43,680
we have better devices than in SATA SSDs

00:05:41,310 --> 00:05:49,650
we have fusion-io and we happen we I

00:05:43,680 --> 00:05:54,690
need so how do they work in such case ok

00:05:49,650 --> 00:05:58,320
now let's put into the picture with my

00:05:54,690 --> 00:06:02,970
ami device and also a ram disk so the

00:05:58,320 --> 00:06:06,720
only made any device is a of course it's

00:06:02,970 --> 00:06:10,920
better in random to pass the energy as

00:06:06,720 --> 00:06:14,610
subtle as disks and when put into the VM

00:06:10,920 --> 00:06:19,440
it has a more noticeable more visible

00:06:14,610 --> 00:06:22,310
overhead the bran disk is displayed here

00:06:19,440 --> 00:06:26,400
to demonstrate that if you really

00:06:22,310 --> 00:06:29,060
further increase the capability of your

00:06:26,400 --> 00:06:32,840
host device your overhead or

00:06:29,060 --> 00:06:32,840
personalizing it will be more noticeable

00:06:33,890 --> 00:06:40,710
so we see the pattern

00:06:37,370 --> 00:06:45,750
and I happen to have a fusion-io disc

00:06:40,710 --> 00:06:48,810
and I compared its performance whereas

00:06:45,750 --> 00:06:52,680
the other two types of as accessory I

00:06:48,810 --> 00:06:55,919
don't get physically similar results so

00:06:52,680 --> 00:07:00,569
this chart is a in a different unit it

00:06:55,919 --> 00:07:03,539
it tests the latency of each request in

00:07:00,569 --> 00:07:07,050
average that needs to be completely from

00:07:03,539 --> 00:07:09,479
the host and from the guest so the grip

00:07:07,050 --> 00:07:13,110
heart is specifically can be seen as

00:07:09,479 --> 00:07:19,319
overhead incurred when you put a

00:07:13,110 --> 00:07:24,479
workload from host to VM so this has

00:07:19,319 --> 00:07:27,270
numbers and we can extract the gray bars

00:07:24,479 --> 00:07:31,080
to a separate graph to look at how it

00:07:27,270 --> 00:07:36,150
looks so we can see that with different

00:07:31,080 --> 00:07:39,000
types of disks the result in this

00:07:36,150 --> 00:07:45,860
respect is physically similar we can see

00:07:39,000 --> 00:07:45,860
there are 5 to 20 or 30 microseconds of

00:07:45,979 --> 00:07:55,770
latency added when you run it in OPM so

00:07:51,469 --> 00:07:58,680
what we want to do here is naturally to

00:07:55,770 --> 00:08:05,669
reduce this amount of time that that I

00:07:58,680 --> 00:08:08,460
added to the overall latency when we run

00:08:05,669 --> 00:08:10,590
any workload in the BM so how do we do

00:08:08,460 --> 00:08:14,699
that we have several because this thing

00:08:10,590 --> 00:08:18,900
approaches first of all it's a KVM

00:08:14,699 --> 00:08:22,229
especially the specific specific sorry

00:08:18,900 --> 00:08:25,819
specific optimizations there are two

00:08:22,229 --> 00:08:28,590
meter whines - pollo steve m wat po

00:08:25,819 --> 00:08:31,770
functioning in the kernel which of

00:08:28,590 --> 00:08:34,079
course reduces the latency of the

00:08:31,770 --> 00:08:37,500
message passing workload and the other

00:08:34,079 --> 00:08:42,029
one is the recently added QE our context

00:08:37,500 --> 00:08:44,219
coding which allows you to look at the

00:08:42,029 --> 00:08:47,940
device and the look at the word cure

00:08:44,219 --> 00:08:49,200
before you even yelled your cpu so that

00:08:47,940 --> 00:08:52,800
you can achieve

00:08:49,200 --> 00:08:58,710
faster come processing of the completed

00:08:52,800 --> 00:09:02,070
requests so yeah Stefan has a talk

00:08:58,710 --> 00:09:04,640
tomorrow at the same time about in this

00:09:02,070 --> 00:09:09,270
room so if you're interested you can

00:09:04,640 --> 00:09:13,830
come here and listen to him and the

00:09:09,270 --> 00:09:16,560
other one is Colonel optimizations this

00:09:13,830 --> 00:09:19,380
is a this involves a cast a colonel but

00:09:16,560 --> 00:09:23,880
it has a it is a good approach to reduce

00:09:19,380 --> 00:09:27,529
the latency not only for Williams but

00:09:23,880 --> 00:09:31,740
also for euro workloads on the host

00:09:27,529 --> 00:09:34,350
there is IO pour by jens and what it

00:09:31,740 --> 00:09:38,490
does is a little bit similar to the

00:09:34,350 --> 00:09:40,560
cameo x-ray i'll contact supporting but

00:09:38,490 --> 00:09:43,020
there is one small restriction about

00:09:40,560 --> 00:09:45,660
this which is it only supports a

00:09:43,020 --> 00:09:50,040
synchronous i/o so if you use a

00:09:45,660 --> 00:09:55,560
synchronous interface you don't get this

00:09:50,040 --> 00:09:58,110
optimization which in the context of qmu

00:09:55,560 --> 00:10:01,620
it means if you have a y equals red you

00:09:58,110 --> 00:10:06,690
can you use this threaded worker and you

00:10:01,620 --> 00:10:08,970
use synchronized it's cause for do i oh

00:10:06,690 --> 00:10:11,910
so you benefit from this if you use AI

00:10:08,970 --> 00:10:15,690
equals native you don't because it you

00:10:11,910 --> 00:10:20,160
use the asynchronous one and the third

00:10:15,690 --> 00:10:22,200
one is device retirement which is where

00:10:20,160 --> 00:10:25,620
hundreds of the bios it has its own

00:10:22,200 --> 00:10:28,140
limitations but it achieves better much

00:10:25,620 --> 00:10:32,070
better performance than if you use a

00:10:28,140 --> 00:10:37,080
great relief regular spy or device to

00:10:32,070 --> 00:10:40,430
back your virtual block device so lastly

00:10:37,080 --> 00:10:43,830
there is a user space device driver

00:10:40,430 --> 00:10:47,630
which is also based on via file we have

00:10:43,830 --> 00:10:52,950
the PDK phone network and we have OSP DK

00:10:47,630 --> 00:10:54,870
which that that's for storage so they

00:10:52,950 --> 00:10:57,589
all use very host interface to interact

00:10:54,870 --> 00:11:01,110
to integrate with QEMU

00:10:57,589 --> 00:11:01,750
so we have now proposed to be hosted

00:11:01,110 --> 00:11:05,350
epilogue

00:11:01,750 --> 00:11:10,480
and we host the others Cassie which are

00:11:05,350 --> 00:11:13,110
basically a new architecture to enable

00:11:10,480 --> 00:11:20,170
form of the drivers for storage as well

00:11:13,110 --> 00:11:23,020
but that is a relatively different from

00:11:20,170 --> 00:11:25,690
architecture from what we usually use

00:11:23,020 --> 00:11:29,650
the in you because if you use very host

00:11:25,690 --> 00:11:31,750
you don't use the whole IO pass of a

00:11:29,650 --> 00:11:33,640
criminal you don't have the plot lay

00:11:31,750 --> 00:11:38,350
your code and you don't have the blocker

00:11:33,640 --> 00:11:42,880
your features so what I am proposing

00:11:38,350 --> 00:11:47,290
here is to add a we have i/o driver you

00:11:42,880 --> 00:11:49,630
qmu which that's basically the similar

00:11:47,290 --> 00:11:52,480
scene with SP DK but it drives the

00:11:49,630 --> 00:11:56,740
device directly from Kirino under the

00:11:52,480 --> 00:12:00,970
block layer all these optimizations have

00:11:56,740 --> 00:12:03,370
common points and some are exclusive

00:12:00,970 --> 00:12:04,900
with each other we will compare the

00:12:03,370 --> 00:12:11,380
compunction in oddities and the

00:12:04,900 --> 00:12:14,770
performance later so we will we are piyo

00:12:11,380 --> 00:12:17,440
driver we naturally benefit from the

00:12:14,770 --> 00:12:20,160
creaming hot pole and our contact

00:12:17,440 --> 00:12:22,360
spooning as well but for the kernel

00:12:20,160 --> 00:12:25,240
optimizations because we already

00:12:22,360 --> 00:12:27,339
bypassed all the stuff in the kernel or

00:12:25,240 --> 00:12:31,030
the driver and we use we apply OPI

00:12:27,339 --> 00:12:39,240
kernel mode but we cannot benefit from

00:12:31,030 --> 00:12:45,660
the IO IO poop optimization so this is a

00:12:39,240 --> 00:12:49,320
current architecture of a Kamiel block

00:12:45,660 --> 00:12:53,380
from the host to guests

00:12:49,320 --> 00:12:57,250
so on the bottom is there is a device

00:12:53,380 --> 00:12:59,280
that actually saves your data and on top

00:12:57,250 --> 00:13:02,200
of that you have a block device and

00:12:59,280 --> 00:13:05,010
block layer of kernel and the BFS of

00:13:02,200 --> 00:13:08,860
kernel so you can create five systems

00:13:05,010 --> 00:13:10,120
human interacts with kernel with the

00:13:08,860 --> 00:13:12,499
POSIX or Linux i/o

00:13:10,120 --> 00:13:14,809
sis cost so it calls into

00:13:12,499 --> 00:13:17,959
colonel saying that i want to read or

00:13:14,809 --> 00:13:20,079
write this portion of the file and the

00:13:17,959 --> 00:13:23,989
actual reading and writing is handled

00:13:20,079 --> 00:13:29,419
you know the host kernel drivers on top

00:13:23,989 --> 00:13:32,449
of the processing method there is a

00:13:29,419 --> 00:13:36,199
ferment driver which handles the logic

00:13:32,449 --> 00:13:39,979
of cocoa to image format it parses the

00:13:36,199 --> 00:13:41,509
metadata and forwards the i/o to radar

00:13:39,979 --> 00:13:45,109
right from the right position of the

00:13:41,509 --> 00:13:47,809
file and there is a plot layer which is

00:13:45,109 --> 00:13:53,749
on top of the drivers support layer

00:13:47,809 --> 00:13:57,109
provides features like throttling and

00:13:53,749 --> 00:14:03,559
the back cap and the mirror and blah

00:13:57,109 --> 00:14:09,009
blah which is nice because it has a it

00:14:03,559 --> 00:14:09,009
provides you a lot of flexibility and

00:14:09,369 --> 00:14:15,889
it's widely used by the management layer

00:14:13,329 --> 00:14:19,369
and there is a block back-end

00:14:15,889 --> 00:14:25,039
abstraction which is basically the

00:14:19,369 --> 00:14:27,349
object that is links the back end to the

00:14:25,039 --> 00:14:29,629
front end so the front end is the part I

00:14:27,349 --> 00:14:32,839
devised which is basically device

00:14:29,629 --> 00:14:34,879
simulation code in Torino and to any

00:14:32,839 --> 00:14:38,049
order to talk to that inquest Colonel

00:14:34,879 --> 00:14:38,049
you need a workout driver

00:14:41,079 --> 00:14:47,599
so yeah the blue part is whatever you

00:14:45,109 --> 00:14:50,839
really want to keep while improving the

00:14:47,599 --> 00:14:55,220
performance I mean if you have a broad

00:14:50,839 --> 00:14:58,279
pack and you have all the features that

00:14:55,220 --> 00:15:00,859
is already available in your new

00:14:58,279 --> 00:15:03,589
architecture so we don't touch that

00:15:00,859 --> 00:15:07,369
instead we only replace the POSIX driver

00:15:03,589 --> 00:15:09,949
with new BFI only a new driver which

00:15:07,369 --> 00:15:11,749
means when a request comes the q2 driver

00:15:09,949 --> 00:15:14,689
will then forward the read or read

00:15:11,749 --> 00:15:18,159
request to the via pile driver and we

00:15:14,689 --> 00:15:21,699
have our driver will translate this to a

00:15:18,159 --> 00:15:25,070
operational memory card which means they

00:15:21,699 --> 00:15:27,910
know the FF driver will

00:15:25,070 --> 00:15:33,640
talk to the colonel with vfi which is a

00:15:27,910 --> 00:15:40,900
interface so we bypass all the colonel

00:15:33,640 --> 00:15:45,230
rock layer and BFS so the implementation

00:15:40,900 --> 00:15:48,320
it's basically consists of two parts one

00:15:45,230 --> 00:15:51,800
is a very power helpers which handles

00:15:48,320 --> 00:15:53,110
all the interfacing with a via file

00:15:51,800 --> 00:15:56,030
interface of colonel

00:15:53,110 --> 00:15:59,750
namely during the i/o controls to the

00:15:56,030 --> 00:16:02,240
verify out file descriptor and it also

00:15:59,750 --> 00:16:04,940
manages per device IO virtual address

00:16:02,240 --> 00:16:08,180
space because when you want to interact

00:16:04,940 --> 00:16:10,550
with the device you want to set up the

00:16:08,180 --> 00:16:13,720
iommu address translation so that device

00:16:10,550 --> 00:16:18,610
can access a memory correctly

00:16:13,720 --> 00:16:21,830
yes it's mandatory for security and it's

00:16:18,610 --> 00:16:23,990
required by the way file interface so

00:16:21,830 --> 00:16:27,650
this is a part of the helper library as

00:16:23,990 --> 00:16:30,470
well and to manage this virtual address

00:16:27,650 --> 00:16:34,970
space efficiently it is optimized to

00:16:30,470 --> 00:16:38,930
call a block layer io specifically as

00:16:34,970 --> 00:16:41,090
well it passes by pre allocating IO

00:16:38,930 --> 00:16:43,820
virtual addresses for the whole cast

00:16:41,090 --> 00:16:46,720
around it's not costly because all you

00:16:43,820 --> 00:16:53,320
need to is to set up the table correctly

00:16:46,720 --> 00:16:58,030
before the mystic starts and it also

00:16:53,320 --> 00:17:01,130
needs to service the temporary buffers

00:16:58,030 --> 00:17:05,689
which are not from the caster Ram this

00:17:01,130 --> 00:17:08,390
is also found in this helper library we

00:17:05,689 --> 00:17:11,990
were still how it manages this address

00:17:08,390 --> 00:17:15,980
space later and the other part is only

00:17:11,990 --> 00:17:21,730
any driver which actually handles the

00:17:15,980 --> 00:17:24,790
nvme specification or protocol as by

00:17:21,730 --> 00:17:28,400
setting up right queues and the right

00:17:24,790 --> 00:17:30,590
command structures it also integrates

00:17:28,400 --> 00:17:34,100
with support layer by registering on

00:17:30,590 --> 00:17:36,850
your new driver structure named

00:17:34,100 --> 00:17:36,850
O'Mahoney

00:17:38,230 --> 00:17:44,270
okay so an attitude characteristics is a

00:17:41,720 --> 00:17:47,480
it integrates with the AI context

00:17:44,270 --> 00:17:53,600
pudding by statement which improves the

00:17:47,480 --> 00:17:58,600
latency and it also tries to be prepared

00:17:53,600 --> 00:17:58,600
for queue block layer multi cue features

00:18:01,240 --> 00:18:06,890
so what's the driver does is basically

00:18:04,160 --> 00:18:10,820
to handle read and write and flash which

00:18:06,890 --> 00:18:14,720
are basically what I mean you can do it

00:18:10,820 --> 00:18:16,429
the i/o request I'll Iowa based which

00:18:14,720 --> 00:18:19,820
means from the castle driver you don't

00:18:16,429 --> 00:18:22,160
need to allocate powers to to i/o so

00:18:19,820 --> 00:18:27,169
there is no pump offering and you

00:18:22,160 --> 00:18:30,350
achieve zero copy so that you don't have

00:18:27,169 --> 00:18:34,040
any overhead or from the memory

00:18:30,350 --> 00:18:36,080
operations it uses one I hope you a cube

00:18:34,040 --> 00:18:39,230
pair for now

00:18:36,080 --> 00:18:43,610
even though Mme naturally supports a lot

00:18:39,230 --> 00:18:45,710
of cues the reason is that currently in

00:18:43,610 --> 00:18:52,580
Kirino we handle all the requests in one

00:18:45,710 --> 00:18:56,330
thread so enabling multi cue is easy but

00:18:52,580 --> 00:18:59,110
it would not give us a lot of advantages

00:18:56,330 --> 00:19:02,090
for now but it can be done in the future

00:18:59,110 --> 00:19:05,000
so the good thing about this

00:19:02,090 --> 00:19:07,450
architecture is it can handle cast iron

00:19:05,000 --> 00:19:11,030
more efficiently but there are downsides

00:19:07,450 --> 00:19:13,850
one is it it's less efficient for punch

00:19:11,030 --> 00:19:16,730
buffered i/o and also utility how back

00:19:13,850 --> 00:19:21,440
from showing your image convert our

00:19:16,730 --> 00:19:26,809
cotton compared and what job I was like

00:19:21,440 --> 00:19:30,559
from drive mirror and a copy but we can

00:19:26,809 --> 00:19:33,410
take a look at this issue later and the

00:19:30,559 --> 00:19:35,090
other thing is naturally when you use to

00:19:33,410 --> 00:19:37,490
verify out to access out the OP device

00:19:35,090 --> 00:19:41,390
it's not possible to share it with

00:19:37,490 --> 00:19:44,000
multiple we ends with multiple

00:19:41,390 --> 00:19:46,330
processors unless you have a shall we be

00:19:44,000 --> 00:19:46,330
cetera

00:19:48,510 --> 00:19:55,390
so let's take a look at the i/o request

00:19:51,640 --> 00:19:58,870
of lifecycle here from top its

00:19:55,390 --> 00:20:02,440
from the cast when an application in

00:19:58,870 --> 00:20:05,470
guest wants to do i oh it will go into

00:20:02,440 --> 00:20:09,490
the kernel and the kernel will finally a

00:20:05,470 --> 00:20:12,040
latrine had a request structure that it

00:20:09,490 --> 00:20:15,040
can be put on for the word I applaud our

00:20:12,040 --> 00:20:17,380
house can see command Q and the

00:20:15,040 --> 00:20:22,690
structure is defined in Universal and

00:20:17,380 --> 00:20:25,510
which includes the peppers also dresses

00:20:22,690 --> 00:20:27,640
are expressed in cast of a cast of

00:20:25,510 --> 00:20:32,740
physical address all you people have

00:20:27,640 --> 00:20:37,570
virtual iommu it will be virtual IO IO

00:20:32,740 --> 00:20:40,360
motorists addresses dependently and with

00:20:37,570 --> 00:20:43,030
that address cream you will try to

00:20:40,360 --> 00:20:47,380
translate it into the host virtual

00:20:43,030 --> 00:20:52,240
address in the device model so that

00:20:47,380 --> 00:20:54,850
further I all can happen easily without

00:20:52,240 --> 00:20:59,980
looking up the address in some page

00:20:54,850 --> 00:21:03,640
tables etc so from the second line

00:20:59,980 --> 00:21:06,070
onwards the addresses that is carried

00:21:03,640 --> 00:21:11,560
out over a so basically the host

00:21:06,070 --> 00:21:14,310
pointers which is easy to work with then

00:21:11,560 --> 00:21:16,810
down the road we have word hello block

00:21:14,310 --> 00:21:21,790
device simulation which partly which

00:21:16,810 --> 00:21:24,910
passes the request and translates it

00:21:21,790 --> 00:21:27,310
into actual read or write operations

00:21:24,910 --> 00:21:31,990
depending on the command that is put

00:21:27,310 --> 00:21:34,630
into the queue so the device will call

00:21:31,990 --> 00:21:38,710
into the plot layer with a function call

00:21:34,630 --> 00:21:44,230
like block al reader all right which

00:21:38,710 --> 00:21:47,890
will be translated into actual coin to

00:21:44,230 --> 00:21:50,950
the only I me driver so the enemy driver

00:21:47,890 --> 00:21:54,430
is responsible for actually serving the

00:21:50,950 --> 00:21:56,650
i/o requests to do this it sends a

00:21:54,430 --> 00:21:57,550
request to the device either is already

00:21:56,650 --> 00:22:00,130
command of

00:21:57,550 --> 00:22:03,310
and we army divides our right of lash

00:22:00,130 --> 00:22:07,270
researcher so the last step is to clear

00:22:03,310 --> 00:22:11,470
off this driver which which consists of

00:22:07,270 --> 00:22:13,930
four basically six steps the first one

00:22:11,470 --> 00:22:16,180
is to check that addresses and lens out

00:22:13,930 --> 00:22:18,220
aligned it is important because the

00:22:16,180 --> 00:22:20,980
tries the device won't be able to handle

00:22:18,220 --> 00:22:22,840
your request unless the the ranges you

00:22:20,980 --> 00:22:27,640
want to read or write is properly

00:22:22,840 --> 00:22:30,630
aligned aligned if it's aligned that's

00:22:27,640 --> 00:22:34,420
fine if it's not the driver has to

00:22:30,630 --> 00:22:36,310
resort to a balanced paper so it can

00:22:34,420 --> 00:22:40,210
continue to serve it and once the

00:22:36,310 --> 00:22:42,490
request is down the post part content

00:22:40,210 --> 00:22:44,200
will be copied back to the original

00:22:42,490 --> 00:22:48,100
paper if necessary if it's a read

00:22:44,200 --> 00:22:50,050
request so the second step is to map the

00:22:48,100 --> 00:22:52,390
host address host a virtual address to

00:22:50,050 --> 00:22:54,730
the i/o were to address of the device

00:22:52,390 --> 00:22:58,540
which is in the context of the hood

00:22:54,730 --> 00:23:03,250
iommu that is set up by the we have i/o

00:22:58,540 --> 00:23:07,090
interface so to do this we will have a

00:23:03,250 --> 00:23:08,980
few small communications later but we

00:23:07,090 --> 00:23:11,530
will say now that's good for the step

00:23:08,980 --> 00:23:14,140
three which is to prepare the only a

00:23:11,530 --> 00:23:16,390
mere request structure this is the DD

00:23:14,140 --> 00:23:19,930
part you just fill in the comments and

00:23:16,390 --> 00:23:20,290
the addresses of the variables to do I

00:23:19,930 --> 00:23:22,750
owe

00:23:20,290 --> 00:23:26,440
once you have successfully map today

00:23:22,750 --> 00:23:29,800
yeah and then notifies device about the

00:23:26,440 --> 00:23:34,090
new request and it will process your our

00:23:29,800 --> 00:23:37,620
command itself and stand interrupt

00:23:34,090 --> 00:23:41,140
afterwards so the step five is a bit

00:23:37,620 --> 00:23:45,640
special here because it basically also

00:23:41,140 --> 00:23:47,880
tested the pudding optimization which

00:23:45,640 --> 00:23:52,990
allows you to look at the result earlier

00:23:47,880 --> 00:23:56,020
than the interrupt arrives it does not

00:23:52,990 --> 00:23:59,080
only look for the results of current

00:23:56,020 --> 00:24:02,710
request but also for the previous ones

00:23:59,080 --> 00:24:06,370
so that in case in case any request has

00:24:02,710 --> 00:24:09,630
completed in the above steps or earlier

00:24:06,370 --> 00:24:13,450
but the interrupt is not

00:24:09,630 --> 00:24:18,220
here yet we already know that we can

00:24:13,450 --> 00:24:20,560
handle something and it is processed the

00:24:18,220 --> 00:24:22,420
last step is of course if you don't have

00:24:20,560 --> 00:24:25,960
anything to do with yellow and wait for

00:24:22,420 --> 00:24:30,910
the interrupted life in the interface of

00:24:25,960 --> 00:24:33,430
reify oh it is an actual event have T so

00:24:30,910 --> 00:24:37,030
the in the handler of that you're not

00:24:33,430 --> 00:24:39,790
happy you look at the completion cure

00:24:37,030 --> 00:24:46,470
game and just find whatever is completed

00:24:39,790 --> 00:24:49,470
and then send it back to the guest so

00:24:46,470 --> 00:24:52,360
here comes the address translation

00:24:49,470 --> 00:24:54,660
supposed to be have a buffer from the

00:24:52,360 --> 00:24:59,620
gas tab which has a basically four

00:24:54,660 --> 00:25:01,600
addresses which is made up without in

00:24:59,620 --> 00:25:04,510
order to to Iook after Colonel we will

00:25:01,600 --> 00:25:06,340
try to translate the guests were to

00:25:04,510 --> 00:25:08,680
address in to cast a physical dress

00:25:06,340 --> 00:25:13,600
I'll get the i/o address more precisely

00:25:08,680 --> 00:25:16,720
so that the word hiyo device can handle

00:25:13,600 --> 00:25:21,300
that and map that correctly this is a

00:25:16,720 --> 00:25:25,360
cast of paging part which is

00:25:21,300 --> 00:25:28,000
straightforward and for that cast

00:25:25,360 --> 00:25:33,160
physical dress to be translated into hot

00:25:28,000 --> 00:25:38,020
virtual dress this there's already the

00:25:33,160 --> 00:25:41,110
the infrastructure or which is efficient

00:25:38,020 --> 00:25:45,250
as we have a basically a linear mapping

00:25:41,110 --> 00:25:48,220
from the castle ramp to host RAM so the

00:25:45,250 --> 00:25:51,160
translation won't be very slow it's not

00:25:48,220 --> 00:25:58,890
more complicated only as part of the

00:25:51,160 --> 00:26:02,940
algorithm is like constant and

00:25:58,890 --> 00:26:05,820
eventually we want to talk to the device

00:26:02,940 --> 00:26:10,500
there is a submission queue we want to

00:26:05,820 --> 00:26:16,180
initialize our command with the proper

00:26:10,500 --> 00:26:19,840
fields set to whatever IO types you want

00:26:16,180 --> 00:26:21,950
and also point to the memory where you

00:26:19,840 --> 00:26:25,290
want to read or write later

00:26:21,950 --> 00:26:28,050
so there are few things involving the

00:26:25,290 --> 00:26:32,760
addresses first of all is the page list

00:26:28,050 --> 00:26:37,770
page list is very much to do I owe it is

00:26:32,760 --> 00:26:39,930
the in the Iove address not in the host

00:26:37,770 --> 00:26:42,990
virtual address or host or physical

00:26:39,930 --> 00:26:45,120
address so it means for in order to get

00:26:42,990 --> 00:26:47,790
this address you need to convert the our

00:26:45,120 --> 00:26:50,610
menu and set up the mapping correctly

00:26:47,790 --> 00:26:53,430
and there is also the point her you know

00:26:50,610 --> 00:26:56,940
submission queues command structure that

00:26:53,430 --> 00:26:59,580
points to the page list that's the same

00:26:56,940 --> 00:27:04,140
you need to have a proper matching in

00:26:59,580 --> 00:27:06,120
Iowa MMU so at the i/o BIA is translated

00:27:04,140 --> 00:27:14,280
and the device can look at the write a

00:27:06,120 --> 00:27:23,030
physical page but yeah so what what we

00:27:14,280 --> 00:27:23,030
do to really catalyst the interface is

00:27:25,040 --> 00:27:31,430
already defined by via file which will

00:27:28,710 --> 00:27:33,930
basically do I'll control to the verify

00:27:31,430 --> 00:27:38,070
party scripter and the saying that I

00:27:33,930 --> 00:27:41,580
want to map this host of course pointer

00:27:38,070 --> 00:27:44,490
with this ties to this I only aim and

00:27:41,580 --> 00:27:45,380
the RV is decided by you it can be any

00:27:44,490 --> 00:27:48,030
address

00:27:45,380 --> 00:27:51,900
it's you know in a separate address

00:27:48,030 --> 00:27:56,730
space this is this is a function call

00:27:51,900 --> 00:27:59,370
this is a state call and for each sis

00:27:56,730 --> 00:28:07,140
call you have a cost but in order to

00:27:59,370 --> 00:28:10,200
fill in the five five slots of the of

00:28:07,140 --> 00:28:13,860
the page number in Iowa which cannot

00:28:10,200 --> 00:28:16,860
afford the 2500 controls or even one

00:28:13,860 --> 00:28:19,890
because our control satisfy and it costs

00:28:16,860 --> 00:28:24,030
a fraction of second and the old

00:28:19,890 --> 00:28:29,220
agencies will be hurt badly so so

00:28:24,030 --> 00:28:32,130
fortunately one is easy it is a the

00:28:29,220 --> 00:28:34,950
address of the page list the specialist

00:28:32,130 --> 00:28:37,820
should should be allocated

00:28:34,950 --> 00:28:41,070
your hand when you set up the driver and

00:28:37,820 --> 00:28:43,830
you can premiere it already so you know

00:28:41,070 --> 00:28:48,840
the address of each page the page least

00:28:43,830 --> 00:28:53,809
power to feel so this list address is

00:28:48,840 --> 00:28:57,690
already known so we have four prettiest

00:28:53,809 --> 00:29:01,200
left that should be pointing to the host

00:28:57,690 --> 00:29:03,779
virtual addresses so how do we do that

00:29:01,200 --> 00:29:06,809
it's all so simple because we know that

00:29:03,779 --> 00:29:09,240
the hosters RAM that is allocated for

00:29:06,809 --> 00:29:17,840
the castor Ram is basically linear it's

00:29:09,240 --> 00:29:17,840
a big chunks of of MEMS we can do that

00:29:18,200 --> 00:29:25,769
with only a couple of i/o controls right

00:29:23,070 --> 00:29:29,899
before the class starts so that it Maps

00:29:25,769 --> 00:29:33,659
linearly as well that's what we do so we

00:29:29,899 --> 00:29:38,340
listens to any initialization of caste

00:29:33,659 --> 00:29:41,750
Rams and according to the verify or

00:29:38,340 --> 00:29:44,429
interface and they are mapped so

00:29:41,750 --> 00:29:46,820
whenever a request comes and you see

00:29:44,429 --> 00:29:52,889
that this is from a castle Ram you can

00:29:46,820 --> 00:29:59,549
add it to an offset and productive

00:29:52,889 --> 00:30:02,179
reproduce its correct IO address so that

00:29:59,549 --> 00:30:02,179
is solved

00:30:05,020 --> 00:30:09,429
now how about host buffers

00:30:10,270 --> 00:30:17,090
host buffers are dynamically allocated

00:30:13,850 --> 00:30:20,090
we cannot know what address is it it has

00:30:17,090 --> 00:30:24,920
so we cannot promote it this will be a

00:30:20,090 --> 00:30:28,390
problem so the slowest thing that can be

00:30:24,920 --> 00:30:31,700
done is we can just find each unknown

00:30:28,390 --> 00:30:34,520
new address when it comes and just the

00:30:31,700 --> 00:30:37,970
2d I out I am in your map in place

00:30:34,520 --> 00:30:40,160
that's using but it's slow it can it can

00:30:37,970 --> 00:30:44,660
be a good approach for some rare cases

00:30:40,160 --> 00:30:46,780
like format probing but it will make the

00:30:44,660 --> 00:30:50,770
the long-running processes are

00:30:46,780 --> 00:30:54,080
performance critical called the hot pass

00:30:50,770 --> 00:30:56,710
really slow so we introduce the

00:30:54,080 --> 00:30:58,880
interface which is the register path and

00:30:56,710 --> 00:31:01,750
unregister path what it does is

00:30:58,880 --> 00:31:06,170
basically tells the driver that this

00:31:01,750 --> 00:31:10,480
area is basically as hot as cast around

00:31:06,170 --> 00:31:16,010
please free allocates li io be a volume

00:31:10,480 --> 00:31:18,590
and then in the future when outcomes

00:31:16,010 --> 00:31:23,540
these realms will be usually translated

00:31:18,590 --> 00:31:29,090
to the area as well so how do we manage

00:31:23,540 --> 00:31:32,690
the temporary mappings and the fixed

00:31:29,090 --> 00:31:35,720
imagines that are from the Raymond said

00:31:32,690 --> 00:31:38,720
in her said register and unregister in

00:31:35,720 --> 00:31:41,510
her piece we split the address space

00:31:38,720 --> 00:31:46,390
into three areas one is the fixed one

00:31:41,510 --> 00:31:46,390
and the higher one is the temporary one

00:31:46,420 --> 00:31:49,420
okay

00:31:52,880 --> 00:32:01,200
so a pair of self incrementing counters

00:31:57,180 --> 00:32:03,060
are used to track how many fixed

00:32:01,200 --> 00:32:09,480
mappings we have and how many temporary

00:32:03,060 --> 00:32:12,240
ones we have okay that's just a jump to

00:32:09,480 --> 00:32:14,970
the usage I have Patrick sound released

00:32:12,240 --> 00:32:17,850
about the latest reasons merchants are

00:32:14,970 --> 00:32:21,060
not merged yes so if you want to build

00:32:17,850 --> 00:32:26,910
it right you can chrome from github and

00:32:21,060 --> 00:32:30,870
the ways we use it is basically the same

00:32:26,910 --> 00:32:33,480
as the euro Drive command line of block

00:32:30,870 --> 00:32:35,930
command line except in the file name you

00:32:33,480 --> 00:32:41,840
specify the only me and the pc IP

00:32:35,930 --> 00:32:48,450
address so here are the results numbers

00:32:41,840 --> 00:32:50,310
the second bar is via me of comparators

00:32:48,450 --> 00:32:54,270
of the first part which is the Linux a

00:32:50,310 --> 00:32:58,890
IO and the third PI is VI Phi or PCI and

00:32:54,270 --> 00:33:01,440
the last one is host performance so that

00:32:58,890 --> 00:33:05,780
we can see the new driver will close the

00:33:01,440 --> 00:33:12,840
gap between durable nukes a IO and

00:33:05,780 --> 00:33:16,080
verify OPI pass through these other

00:33:12,840 --> 00:33:21,980
proportions compared with Linux IO we

00:33:16,080 --> 00:33:21,980
have in average 10% of higher latency

00:33:22,160 --> 00:33:31,680
sorry higher apps okay so they these are

00:33:28,160 --> 00:33:36,330
comparing the configuration limitations

00:33:31,680 --> 00:33:38,730
of different ways you use any device

00:33:36,330 --> 00:33:40,980
with POSIX you basically is very

00:33:38,730 --> 00:33:43,140
flexible to use any way you want

00:33:40,980 --> 00:33:45,170
which via me like I said you cannot

00:33:43,140 --> 00:33:51,920
share when the right device with

00:33:45,170 --> 00:33:56,180
multiple VMs and with SDK you basically

00:33:51,920 --> 00:34:00,000
need to setup huge pages in addition and

00:33:56,180 --> 00:34:03,560
the guests will always see the very

00:34:00,000 --> 00:34:06,370
device type instead of

00:34:03,560 --> 00:34:10,340
in the posix case you can have

00:34:06,370 --> 00:34:13,310
compatible ones like idea which device

00:34:10,340 --> 00:34:16,730
assignment you have a similar

00:34:13,310 --> 00:34:19,700
restriction and the device type in the

00:34:16,730 --> 00:34:23,510
cursor will be only I mean so for the

00:34:19,700 --> 00:34:26,510
features only I need doesn't have host

00:34:23,510 --> 00:34:28,340
block features only any driver doesn't

00:34:26,510 --> 00:34:31,510
have the host block features because it

00:34:28,340 --> 00:34:34,399
bypasses the cost kernel and the SPD K

00:34:31,510 --> 00:34:35,870
at the same in addition it doesn't help

00:34:34,399 --> 00:34:39,230
the people are clear because it bypasses

00:34:35,870 --> 00:34:40,820
killing you blah clear as well and there

00:34:39,230 --> 00:34:46,700
is device assignments you don't have

00:34:40,820 --> 00:34:50,210
anything even including migration so

00:34:46,700 --> 00:34:52,690
this is the general compression and

00:34:50,210 --> 00:34:56,360
including performance and functionality

00:34:52,690 --> 00:34:59,890
so nvme is a different balance point

00:34:56,360 --> 00:35:07,790
between the between the flexibility and

00:34:59,890 --> 00:35:11,890
latency okay so the still has the links

00:35:07,790 --> 00:35:16,940
to the current latest patch and also the

00:35:11,890 --> 00:35:20,360
github link so next thing is to cut it

00:35:16,940 --> 00:35:22,070
emerged and also when the multi cubic

00:35:20,360 --> 00:35:26,450
layer comes in to remove a racially

00:35:22,070 --> 00:35:32,150
integrated visit these are the packing

00:35:26,450 --> 00:35:34,200
for about benchmark I used yeah thank

00:35:32,150 --> 00:35:37,020
you

00:35:34,200 --> 00:35:43,260
[Applause]

00:35:37,020 --> 00:35:43,260

YouTube URL: https://www.youtube.com/watch?v=bwyHxb4tng0


