Title: [2017] To EL2, and Beyond! by Christoffer Dall
Publication date: 2017-11-06
Playlist: KVM Forum 2017
Description: 
	A key drawback in the use of full system virtualization is the performance penalty introduced by hypervisors. This problem is especially present on ARM, which has significantly higher overhead for some workloads compared to x86, due to differences in the hardware virtualization support. The key reason for the overhead on ARM is the need to multiplex kernel mode state between the hypervisor and VMs, which each run their own kernel. This talk will cover how we have redesigned and optimized KVM/ARM, resulting in an order of magnitude reduction in overhead, and resulted in less overhead than x86 on key hypervisor operations. Our optimizations rely on new hardware support in ARMv8.1, the Virtualization Host Extensions (VHE), but also support legacy hardware through invasive modifications to Linux to support running the kernel in the hypervisor-specific CPU mode, EL2.

---

Christoffer Dall
Linaro
Virtualization Tech Lead

Christoffer Dall is the Virtualization Tech Lead at Linaro and completed his PhD at Columbia University where he wrote the initial ARM support for the KVM in Linux and has co-maintained KVM/ARM since its merge in v3.9. Dr. Dall has published several academic papers on hypervisor designs, container virtualization on Android phones, on architectural support for virtualization, and on performance of hypervisors. Previously he worked for VMware and a startup in Silicon Valley.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:05,359 --> 00:00:13,320
okay great morning

00:00:09,380 --> 00:00:15,570
so virtualization is a really great

00:00:13,320 --> 00:00:17,070
technology right and I'm sure everybody

00:00:15,570 --> 00:00:20,869
in this room agrees that KBM is a

00:00:17,070 --> 00:00:22,949
fantastic hypervisor right

00:00:20,869 --> 00:00:25,949
the thing about virtualization is is

00:00:22,949 --> 00:00:30,119
that it comes with a cost and one of

00:00:25,949 --> 00:00:32,099
those costs is performance so my name is

00:00:30,119 --> 00:00:35,130
Christopher Dell I work on KBM arm for

00:00:32,099 --> 00:00:36,570
Lennar oh I'm presenting joint work that

00:00:35,130 --> 00:00:38,670
I did with Shi Whaley from Columbia

00:00:36,570 --> 00:00:41,070
University and this work is really

00:00:38,670 --> 00:00:43,440
something I came up with in an attempt

00:00:41,070 --> 00:00:45,210
to not be reviewing marks patches all

00:00:43,440 --> 00:00:47,760
the time but to write some patches that

00:00:45,210 --> 00:00:49,980
he then gets the review so today I'm

00:00:47,760 --> 00:00:52,920
gonna talk about how we've significantly

00:00:49,980 --> 00:00:57,629
redesigned and re-implemented core parts

00:00:52,920 --> 00:01:01,590
of KVM arm to improve performance so if

00:00:57,629 --> 00:01:04,260
we go back to 1974 and look at one of

00:01:01,590 --> 00:01:06,290
these sort of fundamental papers about

00:01:04,260 --> 00:01:09,270
virtualization from public and goldberg

00:01:06,290 --> 00:01:11,010
they said that a virtual machine is an

00:01:09,270 --> 00:01:12,780
efficient isolated duplicate of a real

00:01:11,010 --> 00:01:14,580
machine and when they talked about

00:01:12,780 --> 00:01:16,080
efficiency they basically were saying

00:01:14,580 --> 00:01:18,060
that if you execute most of your

00:01:16,080 --> 00:01:19,920
instructions natively on the CPU without

00:01:18,060 --> 00:01:23,729
having to trap to the hypervisor then

00:01:19,920 --> 00:01:25,049
you're good and you're efficient but the

00:01:23,729 --> 00:01:26,070
thing is that that was written in a time

00:01:25,049 --> 00:01:30,869
when computers looked something like

00:01:26,070 --> 00:01:32,700
this so that's a IBM 360 this happened

00:01:30,869 --> 00:01:34,439
to be from the Columbia University

00:01:32,700 --> 00:01:38,520
computer room where I went to school but

00:01:34,439 --> 00:01:39,509
roughly 40 years before my time but

00:01:38,520 --> 00:01:41,310
that's not actually really fair because

00:01:39,509 --> 00:01:43,619
they also talked about a pdp-10 in their

00:01:41,310 --> 00:01:46,140
paper like this one but this one is

00:01:43,619 --> 00:01:47,549
actually too new one to compare with

00:01:46,140 --> 00:01:48,540
what they're talking about because what

00:01:47,549 --> 00:01:50,310
they were they were talking about they

00:01:48,540 --> 00:01:53,520
were talking about one with the k10 CPU

00:01:50,310 --> 00:01:58,049
that had a whopping 1k maximum amount of

00:01:53,520 --> 00:01:59,820
addressable memory and no paging so

00:01:58,049 --> 00:02:02,820
obviously we've come quite a long way

00:01:59,820 --> 00:02:04,860
today we have machines like these the

00:02:02,820 --> 00:02:07,099
dual socket cab um Thunder X with 96

00:02:04,860 --> 00:02:09,330
cores and a lot of high-speed i/o and

00:02:07,099 --> 00:02:11,610
when we talk about virtualization

00:02:09,330 --> 00:02:13,319
performance in modern day computing it's

00:02:11,610 --> 00:02:15,659
not enough that we just

00:02:13,319 --> 00:02:19,230
execute most instructions for virtual

00:02:15,659 --> 00:02:21,209
machines natively we also have to be

00:02:19,230 --> 00:02:23,219
fast when the hypervisor has to support

00:02:21,209 --> 00:02:24,719
the VM for doing things like fast i/o or

00:02:23,219 --> 00:02:30,349
inter processor communication between

00:02:24,719 --> 00:02:34,469
your 96 cores so back in the day we did

00:02:30,349 --> 00:02:36,389
virtualization with trap and emulate on

00:02:34,469 --> 00:02:37,980
virtual eyes herbal architectures right

00:02:36,389 --> 00:02:39,299
we did that by running the hypervisor of

00:02:37,980 --> 00:02:41,609
the privileged CPU mode and we ran all

00:02:39,299 --> 00:02:45,810
of our VM including the kernel and user

00:02:41,609 --> 00:02:48,750
space in an unprivileged mode the

00:02:45,810 --> 00:02:50,400
problem is that some architectures are

00:02:48,750 --> 00:02:52,319
not what we call virtual izybelle in

00:02:50,400 --> 00:02:53,489
fact that's what the 1974 paper was

00:02:52,319 --> 00:02:55,079
about it gave you a strict set of

00:02:53,489 --> 00:02:56,459
requirements for when an architecture is

00:02:55,079 --> 00:02:58,400
virtual izybelle and that means that you

00:02:56,459 --> 00:03:00,599
can do trap and emulate virtualization

00:02:58,400 --> 00:03:01,949
arm and x86 are examples of

00:03:00,599 --> 00:03:04,500
architectures which are not virtual

00:03:01,949 --> 00:03:05,699
izybelle and what they did is that they

00:03:04,500 --> 00:03:08,370
introduced hardware support for

00:03:05,699 --> 00:03:09,959
virtualization right and KVM came out as

00:03:08,370 --> 00:03:12,109
a hypervisor specifically designed to

00:03:09,959 --> 00:03:14,969
support hardware-assisted virtualization

00:03:12,109 --> 00:03:16,769
but the thing about arm is that the way

00:03:14,969 --> 00:03:18,030
that arm designed their hardware support

00:03:16,769 --> 00:03:21,569
for virtualization was very very

00:03:18,030 --> 00:03:23,970
different from what Intel did so if we

00:03:21,569 --> 00:03:26,669
look at conceptualized version of what

00:03:23,970 --> 00:03:28,500
Intel does they basically took your

00:03:26,669 --> 00:03:31,079
entire CPU protection mechanism and all

00:03:28,500 --> 00:03:33,720
your CPU modes and duplicated that and

00:03:31,079 --> 00:03:36,989
added a concept of Ruud versus non Ruud

00:03:33,720 --> 00:03:39,329
mode really orthogonal to your to your

00:03:36,989 --> 00:03:41,099
CPU modes and root mode was designed to

00:03:39,329 --> 00:03:42,599
run the hypervisor or native operating

00:03:41,099 --> 00:03:45,569
systems and non root mode for running

00:03:42,599 --> 00:03:47,849
VMs and then you switch between these

00:03:45,569 --> 00:03:49,259
two using specialized hardware

00:03:47,849 --> 00:03:51,659
mechanisms the VM entries in the VM

00:03:49,259 --> 00:03:53,159
exits what they do is they essentially

00:03:51,659 --> 00:03:55,079
save and restore all of your state of

00:03:53,159 --> 00:03:56,939
your CPU into a specialized structure in

00:03:55,079 --> 00:04:00,540
memory the vm control structure or the

00:03:56,939 --> 00:04:03,509
MCS arm did something very different

00:04:00,540 --> 00:04:05,659
they decided to build virtualization

00:04:03,509 --> 00:04:08,340
support into their protection mechanism

00:04:05,659 --> 00:04:11,129
so the basic arm protection mechanism

00:04:08,340 --> 00:04:13,289
has to execution modes yield 0 to run

00:04:11,129 --> 00:04:17,849
user space and he'll want to run the

00:04:13,289 --> 00:04:21,060
kernel the arm virtualization extensions

00:04:17,849 --> 00:04:22,949
add an additional mode not orthogonal

00:04:21,060 --> 00:04:27,200
your CPU modes but part of that scheme

00:04:22,949 --> 00:04:31,260
to run hypervisors it's called the l2

00:04:27,200 --> 00:04:33,540
l2 is a mode specifically designed to

00:04:31,260 --> 00:04:36,780
run hypervisors and not full operating

00:04:33,540 --> 00:04:40,530
systems what does that mean it means

00:04:36,780 --> 00:04:44,070
that it has less features than l1

00:04:40,530 --> 00:04:46,050
designed to run the kernel it has

00:04:44,070 --> 00:04:48,180
reduced virtual memory support and it

00:04:46,050 --> 00:04:52,560
has very limited support within - for

00:04:48,180 --> 00:04:54,360
interacting with user space in l0 that

00:04:52,560 --> 00:04:56,400
works really well for a type 1

00:04:54,360 --> 00:04:58,320
hypervisor like Zen because you just

00:04:56,400 --> 00:05:01,380
write the hypervisor to run in l2 and

00:04:58,320 --> 00:05:05,070
you run your VMs run Linux in yellow one

00:05:01,380 --> 00:05:08,090
and yield zero but for KBM we kind of

00:05:05,070 --> 00:05:08,090
have to put everything upside down

00:05:08,360 --> 00:05:13,680
the problem with KVM is that it's

00:05:10,830 --> 00:05:15,570
integrated with Linux Linux is a full

00:05:13,680 --> 00:05:19,500
operating system designed to run in the

00:05:15,570 --> 00:05:22,590
l1 and KVM can't run VMs on arm without

00:05:19,500 --> 00:05:26,520
running something in yell - so what do

00:05:22,590 --> 00:05:28,020
we do well the way that KBR works is

00:05:26,520 --> 00:05:29,880
that it uses something called split mode

00:05:28,020 --> 00:05:31,800
virtualization and the idea is that you

00:05:29,880 --> 00:05:35,790
split the execution of the hypervisor

00:05:31,800 --> 00:05:37,830
across both l1 and l2 but really this

00:05:35,790 --> 00:05:40,110
very unfortunate side effect of that is

00:05:37,830 --> 00:05:42,030
that you're now multiplexing l1 state

00:05:40,110 --> 00:05:46,919
between your VM kernel and your host

00:05:42,030 --> 00:05:48,810
kernel and interestingly x86 sort of has

00:05:46,919 --> 00:05:50,400
to do that - but they have their fancy

00:05:48,810 --> 00:05:52,980
hardware mechanisms to multiplex their

00:05:50,400 --> 00:05:55,320
CPU mode on arm we have to do it in

00:05:52,980 --> 00:05:59,790
software all right we do that in the

00:05:55,320 --> 00:06:01,950
part of KB I'm running down in the l2 so

00:05:59,790 --> 00:06:03,780
if we consider a hyper call from a VM

00:06:01,950 --> 00:06:06,690
and see what the kind of work we have to

00:06:03,780 --> 00:06:08,160
do to handle that hyper call what will

00:06:06,690 --> 00:06:10,890
happen is that the VM kernel will issue

00:06:08,160 --> 00:06:12,870
a hyper call instruction trap down to l2

00:06:10,890 --> 00:06:14,310
which will switch all of your l1 State

00:06:12,870 --> 00:06:16,919
around in software by saving and

00:06:14,310 --> 00:06:18,900
restoring loads registers then return

00:06:16,919 --> 00:06:21,660
into your host Linux dense instance

00:06:18,900 --> 00:06:23,190
which can handle your hyper call but

00:06:21,660 --> 00:06:24,750
then to return to the VM kernel has to

00:06:23,190 --> 00:06:27,030
make another hyper call instruction to

00:06:24,750 --> 00:06:28,860
enter yell to again switch the state

00:06:27,030 --> 00:06:31,890
around in software and then return to

00:06:28,860 --> 00:06:34,200
your VM Chrome so that's obviously a

00:06:31,890 --> 00:06:36,180
fair amount of work but what if we could

00:06:34,200 --> 00:06:38,490
do something like this instead where we

00:06:36,180 --> 00:06:39,360
could just run all of Linux with KVM in

00:06:38,490 --> 00:06:41,290
l2

00:06:39,360 --> 00:06:42,940
that would be really really great

00:06:41,290 --> 00:06:44,740
because then if we have to handle a trap

00:06:42,940 --> 00:06:48,670
from the BM we just handle the trap

00:06:44,740 --> 00:06:50,800
directly and we return so opposite the

00:06:48,670 --> 00:06:53,410
question is how do we do that well we

00:06:50,800 --> 00:06:55,330
got hardware to do that so that's where

00:06:53,410 --> 00:06:57,490
Vig comes in vhe is short for the

00:06:55,330 --> 00:06:58,840
virtualization host extensions and it's

00:06:57,490 --> 00:07:02,560
a modification to the ARM architecture

00:06:58,840 --> 00:07:05,710
pressing the RMV 8.1 what it does is it

00:07:02,560 --> 00:07:09,100
allows us to run an unmodified operating

00:07:05,710 --> 00:07:10,780
system designed to run an l1 in l2 ok

00:07:09,100 --> 00:07:15,700
that's actually a pretty controversial

00:07:10,780 --> 00:07:16,840
thing it gives us five things first of

00:07:15,700 --> 00:07:18,250
all we achieve can be completely

00:07:16,840 --> 00:07:20,440
disabled to provide backwards

00:07:18,250 --> 00:07:25,060
compatibility so Zen won't enable me AG

00:07:20,440 --> 00:07:26,860
and everything works like before the

00:07:25,060 --> 00:07:29,830
second thing that BG gives you is that

00:07:26,860 --> 00:07:31,540
it expands the functionality of l2 so if

00:07:29,830 --> 00:07:33,190
you remember I said that L 2 was

00:07:31,540 --> 00:07:35,050
specifically designed to only run

00:07:33,190 --> 00:07:38,740
hypervisors and be limited compared to L

00:07:35,050 --> 00:07:41,080
1 well now you inherit all of the l1 and

00:07:38,740 --> 00:07:43,780
a new features you get a new timer

00:07:41,080 --> 00:07:45,820
because you'll one has two timers yield

00:07:43,780 --> 00:07:47,800
to only hat once and now you get two so

00:07:45,820 --> 00:07:49,510
you have the same and the end result is

00:07:47,800 --> 00:07:52,420
that you end up having a corresponding

00:07:49,510 --> 00:07:56,800
l2 root system register for every year

00:07:52,420 --> 00:07:58,450
one system register ok the third thing

00:07:56,800 --> 00:08:00,670
is that you get support for running user

00:07:58,450 --> 00:08:04,360
space in l0 interacting with a kernel

00:08:00,670 --> 00:08:08,740
directly running in l2 there actually

00:08:04,360 --> 00:08:11,230
was sort of a feature called GG for trap

00:08:08,740 --> 00:08:13,390
general exceptions already prior to vhe

00:08:11,230 --> 00:08:16,000
but that was designed to run bare metal

00:08:13,390 --> 00:08:18,310
applications on top of a standalone type

00:08:16,000 --> 00:08:19,840
1 hypervisor and when I say bare metal

00:08:18,310 --> 00:08:21,610
applications what I mean is applications

00:08:19,840 --> 00:08:23,110
without virtual memory so the way the

00:08:21,610 --> 00:08:26,410
architecture was designed believe it or

00:08:23,110 --> 00:08:29,170
not was that if you set this bit and you

00:08:26,410 --> 00:08:31,300
ran code in l0 yes exceptions would go

00:08:29,170 --> 00:08:33,360
to l2 instead of l1 where they normally

00:08:31,300 --> 00:08:34,810
would but the MMU would just be disabled

00:08:33,360 --> 00:08:37,360
unfortunate side-effect

00:08:34,810 --> 00:08:39,550
so with VG that no longer happens now

00:08:37,360 --> 00:08:41,440
you can run code and el0 with virtual

00:08:39,550 --> 00:08:45,430
memory and you can trap to the kernel

00:08:41,440 --> 00:08:47,740
running in yield to the fourth thing

00:08:45,430 --> 00:08:49,839
that bt gives you is that you can use

00:08:47,740 --> 00:08:52,320
the same page tables in yield to a zl1

00:08:49,839 --> 00:08:53,760
so it used to be that yell two had

00:08:52,320 --> 00:08:57,330
format for the page tables that could do

00:08:53,760 --> 00:09:00,540
less things but now you can use the same

00:08:57,330 --> 00:09:03,570
format and you also use the page tables

00:09:00,540 --> 00:09:06,780
that el2 configures when you run user

00:09:03,570 --> 00:09:08,280
space applications in l0 because after

00:09:06,780 --> 00:09:11,660
all it is the kernel that should decide

00:09:08,280 --> 00:09:15,420
what page table gets used by user space

00:09:11,660 --> 00:09:18,150
and the fifth thing which is a fun thing

00:09:15,420 --> 00:09:22,170
is that we get system register

00:09:18,150 --> 00:09:24,000
redirection so the problem is that Linux

00:09:22,170 --> 00:09:26,790
is written to run in l1 and I said that

00:09:24,000 --> 00:09:31,620
a VHD a lot of to run Linux unmodified

00:09:26,790 --> 00:09:33,630
in yell too right but l1 is controlled

00:09:31,620 --> 00:09:37,980
by l1 registers yield to is controlled

00:09:33,630 --> 00:09:40,230
by l2 registers so we now have a linux

00:09:37,980 --> 00:09:42,690
kernel which because it's unmodified

00:09:40,230 --> 00:09:45,420
will try to modify l1 registers to

00:09:42,690 --> 00:09:47,370
change its execution to read registers

00:09:45,420 --> 00:09:48,660
to tell us something like what cost an

00:09:47,370 --> 00:09:50,490
exception from user space for example

00:09:48,660 --> 00:09:53,520
alright and that will be completely

00:09:50,490 --> 00:09:55,380
useless so what we really want is we

00:09:53,520 --> 00:09:57,780
want to have this unmodified Linux

00:09:55,380 --> 00:10:02,250
kernel that then accesses l2 registers

00:09:57,780 --> 00:10:05,550
instead so let's think about how you

00:10:02,250 --> 00:10:08,130
access a system register on arm or I'm

00:10:05,550 --> 00:10:10,530
64 well you execute an instruction which

00:10:08,130 --> 00:10:12,300
has an assembly name but really that

00:10:10,530 --> 00:10:14,150
assembly name is just a name that ties

00:10:12,300 --> 00:10:16,590
to a certain instruction encoding right

00:10:14,150 --> 00:10:20,880
so that name tells you that you're gonna

00:10:16,590 --> 00:10:22,260
read an l1 register okay but we can

00:10:20,880 --> 00:10:25,110
actually take couple that right and

00:10:22,260 --> 00:10:27,180
that's exactly what bhe does so when bt

00:10:25,110 --> 00:10:28,890
is disabled things work like before you

00:10:27,180 --> 00:10:31,050
execute this instruction you read anneal

00:10:28,890 --> 00:10:33,390
1 register but when you enable the eg in

00:10:31,050 --> 00:10:35,100
your in l2 and you execute this

00:10:33,390 --> 00:10:40,560
instruction you read anneal to register

00:10:35,100 --> 00:10:42,450
instead this tends to confuse people but

00:10:40,560 --> 00:10:44,640
what if you actually did want to read

00:10:42,450 --> 00:10:45,960
the l1 register you can't use the

00:10:44,640 --> 00:10:47,190
instruction that normally says you read

00:10:45,960 --> 00:10:48,570
any other one register because we just

00:10:47,190 --> 00:10:50,760
said we changed it to go to an y'all to

00:10:48,570 --> 00:10:52,230
register instead right and sometimes we

00:10:50,760 --> 00:10:54,450
do want to read anneal one register for

00:10:52,230 --> 00:10:55,920
example if you want to run a VM so you

00:10:54,450 --> 00:10:58,140
get a new set of instructions for that

00:10:55,920 --> 00:10:59,490
that's the l1 two instructions right

00:10:58,140 --> 00:11:01,890
they always access the yield one

00:10:59,490 --> 00:11:03,880
register and they're called that way

00:11:01,890 --> 00:11:07,240
I think the idea

00:11:03,880 --> 00:11:10,410
you access the l1 register from l2 so

00:11:07,240 --> 00:11:13,120
that's the thing you need to think about

00:11:10,410 --> 00:11:15,160
but wait there's more because some

00:11:13,120 --> 00:11:17,640
registers are don't have the same format

00:11:15,160 --> 00:11:20,410
between l1 and l2 they have the same

00:11:17,640 --> 00:11:21,940
they control the same things but their

00:11:20,410 --> 00:11:24,040
bit positions might be shifted or have

00:11:21,940 --> 00:11:25,690
slightly different semantics so you need

00:11:24,040 --> 00:11:27,270
to set two in one case and only one bit

00:11:25,690 --> 00:11:30,790
in another case and so on

00:11:27,270 --> 00:11:32,890
so all the registers where you have the

00:11:30,790 --> 00:11:34,000
simply the same bit layout it's that the

00:11:32,890 --> 00:11:36,100
register redirection is pretty trivial

00:11:34,000 --> 00:11:37,630
but when you have different registers

00:11:36,100 --> 00:11:39,040
what do you do well the hardware just

00:11:37,630 --> 00:11:40,150
says well when you enable the AG we'll

00:11:39,040 --> 00:11:43,140
just gonna change the meaning of the

00:11:40,150 --> 00:11:48,300
yell to register shift it around

00:11:43,140 --> 00:11:53,380
so with VHD we can run Linux in l2

00:11:48,300 --> 00:11:55,660
unmodified and that's great but KBM has

00:11:53,380 --> 00:11:58,270
to actually be modified to work to just

00:11:55,660 --> 00:12:00,550
work with with vhe so if we go back to

00:11:58,270 --> 00:12:02,320
our legacy split mode KVM arm design

00:12:00,550 --> 00:12:04,780
what we had to do there is when linux

00:12:02,320 --> 00:12:07,120
decided to run a vm basically because

00:12:04,780 --> 00:12:08,800
you called my octal from user space you

00:12:07,120 --> 00:12:11,920
would run some code in KBM as part of

00:12:08,800 --> 00:12:13,540
linux and then you would issue a trap to

00:12:11,920 --> 00:12:16,000
enter the part of KB I'm running and

00:12:13,540 --> 00:12:20,440
yell to and then you would eventually

00:12:16,000 --> 00:12:23,860
run your VM slightly oversimplified all

00:12:20,440 --> 00:12:25,690
we have to do to make KBM work with vhe

00:12:23,860 --> 00:12:27,460
is we have to change a bunch of

00:12:25,690 --> 00:12:29,410
instructions that were accessing l1

00:12:27,460 --> 00:12:31,900
registers to use the new l1 to accessors

00:12:29,410 --> 00:12:33,820
and we have to change the trap to a

00:12:31,900 --> 00:12:38,410
function call now everything runs in the

00:12:33,820 --> 00:12:40,240
l2 and you're golden the problem is we

00:12:38,410 --> 00:12:41,620
don't have any DHT hardware so we have

00:12:40,240 --> 00:12:45,070
no clue how this is going to perform

00:12:41,620 --> 00:12:46,960
some vhe hardware is beginning to appear

00:12:45,070 --> 00:12:49,000
but there's really nothing publicly

00:12:46,960 --> 00:12:51,910
available yet we can't really use

00:12:49,000 --> 00:12:55,360
software models to validly evaluate

00:12:51,910 --> 00:12:57,040
performance so what do we do well what

00:12:55,360 --> 00:13:01,360
we did was we modified Linux to run in

00:12:57,040 --> 00:13:06,960
el2 so that would give you a good

00:13:01,360 --> 00:13:08,980
measure for how v80 would perform on

00:13:06,960 --> 00:13:10,990
current hardware that doesn't have ug

00:13:08,980 --> 00:13:12,550
right and what we do is we modify Linux

00:13:10,990 --> 00:13:15,430
to access yield to registers instead of

00:13:12,550 --> 00:13:17,680
the old one registers to use the yield

00:13:15,430 --> 00:13:21,610
to special virtual memory stops

00:13:17,680 --> 00:13:23,230
instead of the year one one and we

00:13:21,610 --> 00:13:27,069
figure out a way to deal with exceptions

00:13:23,230 --> 00:13:29,949
from user space running in yield 0 the

00:13:27,069 --> 00:13:32,740
system register access thing is pretty

00:13:29,949 --> 00:13:35,920
easy or pretty crude as well we just use

00:13:32,740 --> 00:13:37,269
an if def you could use an alternative

00:13:35,920 --> 00:13:38,980
instruction patching at least for stuff

00:13:37,269 --> 00:13:42,610
that's running after the alternatives

00:13:38,980 --> 00:13:45,879
are up we didn't bother for the purpose

00:13:42,610 --> 00:13:47,110
of what we're doing the second challenge

00:13:45,879 --> 00:13:49,509
is memory and there's that's an

00:13:47,110 --> 00:13:52,119
interesting one so if we look at the l1

00:13:49,509 --> 00:13:54,069
virtual memory subsystem on arm and

00:13:52,119 --> 00:13:55,839
assume 139 bits virtual address space it

00:13:54,069 --> 00:13:58,389
looks something like this you have two

00:13:55,839 --> 00:14:00,129
distinct virtual address regions using

00:13:58,389 --> 00:14:02,559
separate page tables with separate page

00:14:00,129 --> 00:14:04,209
table based registers and the way Linux

00:14:02,559 --> 00:14:05,439
uses that is the upper range is used for

00:14:04,209 --> 00:14:10,179
the kernel and the lower range is used

00:14:05,439 --> 00:14:12,040
for user space in yell to you only have

00:14:10,179 --> 00:14:13,089
one so the question is where do we put

00:14:12,040 --> 00:14:17,730
the kernel and where do we put user

00:14:13,089 --> 00:14:21,220
space what we do is we split the VA

00:14:17,730 --> 00:14:23,920
between kernel user space using paste

00:14:21,220 --> 00:14:25,509
tables similar to what you do in x86 so

00:14:23,920 --> 00:14:28,660
you have an upper page table level

00:14:25,509 --> 00:14:30,339
pointing to some shared lower level page

00:14:28,660 --> 00:14:33,490
tables for the kernel which are then

00:14:30,339 --> 00:14:34,839
used across all processes but there are

00:14:33,490 --> 00:14:36,939
some problems with doing that with Linux

00:14:34,839 --> 00:14:38,019
running and yield - first of all we

00:14:36,939 --> 00:14:39,220
compress the address space you have

00:14:38,019 --> 00:14:42,009
obviously have less address space

00:14:39,220 --> 00:14:43,389
available than if you ran anneal one the

00:14:42,009 --> 00:14:45,069
page table formats are really not

00:14:43,389 --> 00:14:48,610
designed to do what we're trying to do

00:14:45,069 --> 00:14:50,139
here and you end up having to invalidate

00:14:48,610 --> 00:14:52,120
the TLB a lot more than you would

00:14:50,139 --> 00:14:54,069
normally so I want to stress that this

00:14:52,120 --> 00:14:56,970
is only a problem on non vhe hardware

00:14:54,069 --> 00:15:00,429
when you run linux neil - not on the AG

00:14:56,970 --> 00:15:01,870
so this thing about the page tables and

00:15:00,429 --> 00:15:04,569
the thing is we're gonna use the same

00:15:01,870 --> 00:15:08,410
page table between the kernel and use of

00:15:04,569 --> 00:15:10,660
space but we're gonna run user space in

00:15:08,410 --> 00:15:13,240
the kernel in two different modes l0 and

00:15:10,660 --> 00:15:14,679
l2 and these modes are really designed

00:15:13,240 --> 00:15:16,269
to have different page table formats so

00:15:14,679 --> 00:15:20,319
that means they will interpret the page

00:15:16,269 --> 00:15:23,499
tables differently ok so these three

00:15:20,319 --> 00:15:25,480
bits the ap1 bit you X n xn and PX n

00:15:23,499 --> 00:15:28,689
bits are interpreted in different ways

00:15:25,480 --> 00:15:30,070
between yield 0 and L 2 and that gives

00:15:28,689 --> 00:15:34,509
us some problems

00:15:30,070 --> 00:15:36,339
the ap1 bit basically says if it's set

00:15:34,509 --> 00:15:38,459
user space can act as a page if it's not

00:15:36,339 --> 00:15:40,990
set it cannot

00:15:38,459 --> 00:15:42,790
that means we obviously have to set it

00:15:40,990 --> 00:15:44,139
to zero for kernel mappings right

00:15:42,790 --> 00:15:47,230
because otherwise user space can access

00:15:44,139 --> 00:15:48,759
the kernel we don't want that but the

00:15:47,230 --> 00:15:51,759
architecture says well this bit is rest

00:15:48,759 --> 00:15:53,560
one so that really means we're served

00:15:51,759 --> 00:15:54,940
one she's sort of feel like well you

00:15:53,560 --> 00:15:57,399
have to write a one what happens if we

00:15:54,940 --> 00:16:00,370
put zero when we read that page table

00:15:57,399 --> 00:16:02,529
from the kernel so we look into that and

00:16:00,370 --> 00:16:04,240
we say for our me 8.0 specifically

00:16:02,529 --> 00:16:05,920
there's no guarantees that will work on

00:16:04,240 --> 00:16:08,139
future hardware but we don't care about

00:16:05,920 --> 00:16:09,759
that what it really means is that it

00:16:08,139 --> 00:16:12,160
should be reads as written with no

00:16:09,759 --> 00:16:14,620
effect on the behavior of the CPU so a

00:16:12,160 --> 00:16:17,560
kernel in yield two should be able to

00:16:14,620 --> 00:16:19,420
write a zero into that page table entry

00:16:17,560 --> 00:16:22,180
and read back at zero and it shouldn't

00:16:19,420 --> 00:16:23,680
affect the CPU okay we don't care

00:16:22,180 --> 00:16:28,690
and actually it works on all the systems

00:16:23,680 --> 00:16:30,699
we run on there are two more bits we

00:16:28,690 --> 00:16:32,889
have to worry about the pxn bit means

00:16:30,699 --> 00:16:35,529
privileged execute never and is used to

00:16:32,889 --> 00:16:37,810
say okay this page can't cannot be

00:16:35,529 --> 00:16:38,740
executed from the kernel if you had the

00:16:37,810 --> 00:16:40,300
kernel runs in the other one

00:16:38,740 --> 00:16:41,740
but since we don't run anything in your

00:16:40,300 --> 00:16:42,880
one and it only affects your one we

00:16:41,740 --> 00:16:46,180
don't it doesn't matter what we set it

00:16:42,880 --> 00:16:48,490
to the you X and X n means execute never

00:16:46,180 --> 00:16:51,250
and it results the result is it works

00:16:48,490 --> 00:16:53,350
the same way in both modes so basically

00:16:51,250 --> 00:16:55,269
we don't have any way to say this page

00:16:53,350 --> 00:16:57,069
is executable in one mode and not the

00:16:55,269 --> 00:16:58,480
other you either decide if a page is

00:16:57,069 --> 00:17:00,250
executable and then it becomes

00:16:58,480 --> 00:17:03,250
executable by both user space and the

00:17:00,250 --> 00:17:06,970
kernel okay that's potentially bad for

00:17:03,250 --> 00:17:07,659
two reasons first of all you don't have

00:17:06,970 --> 00:17:09,520
that privilege

00:17:07,659 --> 00:17:10,809
execute never functionality so you don't

00:17:09,520 --> 00:17:13,600
have any protection against return to

00:17:10,809 --> 00:17:19,449
user attacks beyond your kernel being

00:17:13,600 --> 00:17:21,939
correct and it also means that user

00:17:19,449 --> 00:17:25,409
space can execute kernel code by just

00:17:21,939 --> 00:17:28,600
jumping to an address that sounds scary

00:17:25,409 --> 00:17:30,309
it can only execute kernel code with

00:17:28,600 --> 00:17:32,169
user mode privileges though it can't

00:17:30,309 --> 00:17:33,700
read or write kernel code so it's sort

00:17:32,169 --> 00:17:37,120
of similar to doing dot slash and VM

00:17:33,700 --> 00:17:40,870
Linux but yeah I mean the best I can say

00:17:37,120 --> 00:17:43,720
is that it's not more secure another

00:17:40,870 --> 00:17:45,580
problem is we don't have a sits in l2

00:17:43,720 --> 00:17:47,320
so address base identifies as this thing

00:17:45,580 --> 00:17:51,700
where depending on your context you can

00:17:47,320 --> 00:17:53,860
attach a tag to interest in your TLB and

00:17:51,700 --> 00:17:55,750
that avoids conflicts in the TLB if you

00:17:53,860 --> 00:17:56,590
access multiple virtual addresses that

00:17:55,750 --> 00:18:00,519
are mapped to different physical

00:17:56,590 --> 00:18:01,870
addresses in different context it's not

00:18:00,519 --> 00:18:04,679
really a problem for the kernel because

00:18:01,870 --> 00:18:07,419
it's global a car across all processes

00:18:04,679 --> 00:18:08,590
but when you do get user a put user you

00:18:07,419 --> 00:18:11,139
start asking access and user space

00:18:08,590 --> 00:18:13,929
addresses right and they can have

00:18:11,139 --> 00:18:18,070
conflicts so if you run your kernel meal

00:18:13,929 --> 00:18:21,220
one as normal you will use the a set of

00:18:18,070 --> 00:18:22,990
the process you're executing under but

00:18:21,220 --> 00:18:24,669
in the l2 yield to just doesn't use it

00:18:22,990 --> 00:18:26,350
doesn't use it so you have to invalidate

00:18:24,669 --> 00:18:29,500
all of the yield two enters in the TLB

00:18:26,350 --> 00:18:33,639
every time you switch a process on the

00:18:29,500 --> 00:18:34,929
host and then the third challenge was

00:18:33,639 --> 00:18:38,259
this thing about running user space on

00:18:34,929 --> 00:18:39,730
top of kernel and yell to so we

00:18:38,259 --> 00:18:41,559
generally want to take and handle

00:18:39,730 --> 00:18:42,940
exceptions in our kernel no matter where

00:18:41,559 --> 00:18:44,379
it runs right so if you take an

00:18:42,940 --> 00:18:46,389
exception from user space you want it to

00:18:44,379 --> 00:18:47,679
go to the kernel if it runs near one if

00:18:46,389 --> 00:18:49,870
you take an exception from within the

00:18:47,679 --> 00:18:51,940
kernel itself you also want that to go

00:18:49,870 --> 00:18:54,460
to the kernel inial one right same thing

00:18:51,940 --> 00:18:56,529
if you run the kernel meal to but the

00:18:54,460 --> 00:19:01,029
hardware will route a lot of exceptions

00:18:56,529 --> 00:19:02,740
to yield one it's really tempting to use

00:19:01,029 --> 00:19:04,269
that trap general exceptions bit but as

00:19:02,740 --> 00:19:08,440
I told you before it disables virtual

00:19:04,269 --> 00:19:10,600
memory in user space which sucks so what

00:19:08,440 --> 00:19:12,159
we do is we install a little shim which

00:19:10,600 --> 00:19:14,110
is basically a one page of code that

00:19:12,159 --> 00:19:15,970
just in software affords the exception

00:19:14,110 --> 00:19:19,139
from year one down to l2 and then we

00:19:15,970 --> 00:19:22,509
modified the exception entry path in the

00:19:19,139 --> 00:19:24,159
in the yield to kernel to figure out

00:19:22,509 --> 00:19:25,509
that oh this exception really came from

00:19:24,159 --> 00:19:28,350
yelled zero but it looks like it comes

00:19:25,509 --> 00:19:28,350
from yell one and so on

00:19:28,440 --> 00:19:33,539
so the bad news about linux neil - is

00:19:31,779 --> 00:19:37,360
that is less secure than linux in the l1

00:19:33,539 --> 00:19:39,549
it relies on a strictly correct

00:19:37,360 --> 00:19:41,769
interpretation of the architecture when

00:19:39,549 --> 00:19:42,759
you implement your CPU and it

00:19:41,769 --> 00:19:45,669
potentially gives you worse performance

00:19:42,759 --> 00:19:47,919
for hosts were closed right that's why

00:19:45,669 --> 00:19:48,940
I'm not trying to upstream this or argue

00:19:47,919 --> 00:19:51,039
that this is something you don't want to

00:19:48,940 --> 00:19:52,419
use in production but the good news is

00:19:51,039 --> 00:19:54,909
that it's a really good prototyping tool

00:19:52,419 --> 00:19:56,750
and it's allowed us to optimize KVM arm

00:19:54,909 --> 00:19:59,540
before we had me AG hardware

00:19:56,750 --> 00:20:02,390
for BHG it's very closely emulates the

00:19:59,540 --> 00:20:07,550
performance of what a current system

00:20:02,390 --> 00:20:10,370
that had VG would would be so to measure

00:20:07,550 --> 00:20:12,800
how that then performs we used an MD

00:20:10,370 --> 00:20:17,570
Seattle for this work and we ran some

00:20:12,800 --> 00:20:18,950
numbers so if we look at the cost of a

00:20:17,570 --> 00:20:20,330
hyper call so hyper call is a good

00:20:18,950 --> 00:20:22,850
little micro benchmark right because it

00:20:20,330 --> 00:20:24,560
gives you the basic transition cost of

00:20:22,850 --> 00:20:29,240
going from the VM to the hypervisor and

00:20:24,560 --> 00:20:31,490
back so this table here shows you clock

00:20:29,240 --> 00:20:33,890
cycles between the first column which is

00:20:31,490 --> 00:20:36,980
the unoptimized KVM arm version without

00:20:33,890 --> 00:20:38,830
deg and the second one is emulating VG

00:20:36,980 --> 00:20:40,970
performance with Linux running anneal -

00:20:38,830 --> 00:20:42,470
and it's a little bit of a disappointing

00:20:40,970 --> 00:20:45,140
result we only get like about a hundred

00:20:42,470 --> 00:20:49,400
cycles improvement all right so what the

00:20:45,140 --> 00:20:51,050
heck the reason is really that all we

00:20:49,400 --> 00:20:52,520
really did in terms of KVM performance

00:20:51,050 --> 00:20:54,110
once we change the trap to a function

00:20:52,520 --> 00:20:56,710
call and that gives you about those

00:20:54,110 --> 00:20:58,790
hundred cycles and that's about it

00:20:56,710 --> 00:21:01,850
so what we have to do is we have to more

00:20:58,790 --> 00:21:03,830
fundamentally change the design of KVM

00:21:01,850 --> 00:21:07,700
arm to improve performance and take

00:21:03,830 --> 00:21:10,160
advantage of Vig of running in the l2 so

00:21:07,700 --> 00:21:11,690
if we look at the K being run loop right

00:21:10,160 --> 00:21:13,250
what it is it's a it's a loop inside the

00:21:11,690 --> 00:21:14,000
kernel the loops around and for every

00:21:13,250 --> 00:21:16,100
iteration of the loop

00:21:14,000 --> 00:21:18,350
you run guest code directly on the CPU

00:21:16,100 --> 00:21:21,530
right and the idea is if we want to be

00:21:18,350 --> 00:21:25,100
able to quickly support our our VM for

00:21:21,530 --> 00:21:27,830
doing things like inter processor

00:21:25,100 --> 00:21:29,300
interrupts or doing fast i/o we want to

00:21:27,830 --> 00:21:31,550
do as little work as possible for each

00:21:29,300 --> 00:21:32,780
iteration of the loop because then we

00:21:31,550 --> 00:21:35,990
execute more we spend more time

00:21:32,780 --> 00:21:41,720
executing code on the in the guest and

00:21:35,990 --> 00:21:43,730
we're quicker to service the VM so we

00:21:41,720 --> 00:21:45,410
can make this loop do less work by

00:21:43,730 --> 00:21:47,720
moving logic into the B CPU load and

00:21:45,410 --> 00:21:48,950
because if you put functions so what

00:21:47,720 --> 00:21:50,510
these functions are their hooks that are

00:21:48,950 --> 00:21:51,650
called when you enter the run loop and

00:21:50,510 --> 00:21:53,390
when you exit the run loop and they're

00:21:51,650 --> 00:21:55,100
also called by preamp notifiers if your

00:21:53,390 --> 00:21:59,570
thread gets scheduled off the system and

00:21:55,100 --> 00:22:02,180
when it gets scheduled back in so the

00:21:59,570 --> 00:22:04,280
general approach is that we move

00:22:02,180 --> 00:22:06,230
hardware configuration logic out of the

00:22:04,280 --> 00:22:08,360
run loop into B CPU load and be CPU put

00:22:06,230 --> 00:22:09,780
and that's really only possible when you

00:22:08,360 --> 00:22:11,920
run your Colonel O'Neill

00:22:09,780 --> 00:22:13,240
because if you try to do this when you

00:22:11,920 --> 00:22:14,410
ran the Colonel O'Neill one you'd be

00:22:13,240 --> 00:22:16,030
stepping on your own state and

00:22:14,410 --> 00:22:19,300
potentially panicking your health

00:22:16,030 --> 00:22:21,010
colonel so let's take a look at some

00:22:19,300 --> 00:22:22,510
examples of the stuff we did you through

00:22:21,010 --> 00:22:25,200
this technique so we can start with the

00:22:22,510 --> 00:22:28,240
timers so the arm generic timers is a

00:22:25,200 --> 00:22:29,500
the official name of timers for the ARM

00:22:28,240 --> 00:22:31,300
architecture that's what they're called

00:22:29,500 --> 00:22:33,430
in the architecture document we call

00:22:31,300 --> 00:22:35,350
them most often the architecture timers

00:22:33,430 --> 00:22:39,220
or the arch timers the reason for that

00:22:35,350 --> 00:22:41,130
is that there used to be this mess of

00:22:39,220 --> 00:22:43,090
arm systems that had all sorts of random

00:22:41,130 --> 00:22:45,250
peripherals as timers just attach to

00:22:43,090 --> 00:22:47,230
your system and at some point arm came

00:22:45,250 --> 00:22:48,720
around and said let's build some timer

00:22:47,230 --> 00:22:50,410
functionality into the architecture and

00:22:48,720 --> 00:22:52,030
therefore people started calling it

00:22:50,410 --> 00:22:53,860
architecture timers are supposed to dien

00:22:52,030 --> 00:22:55,180
on architecture timers so that's what

00:22:53,860 --> 00:22:58,060
the name of the code as well if you go

00:22:55,180 --> 00:23:02,380
look what they give you is that they

00:22:58,060 --> 00:23:04,690
allow guests to directly program timer

00:23:02,380 --> 00:23:06,430
Hardware to enable and disable timers to

00:23:04,690 --> 00:23:08,350
change the deadline of when the timer is

00:23:06,430 --> 00:23:09,670
going to fire but when the time where

00:23:08,350 --> 00:23:11,890
then does fire that just generates a

00:23:09,670 --> 00:23:12,940
normal physical interrupt which always

00:23:11,890 --> 00:23:15,510
traps to the hypervisor and the

00:23:12,940 --> 00:23:18,310
hypervisor has to deal with it somehow

00:23:15,510 --> 00:23:20,110
the way we use timers and KBM arm was a

00:23:18,310 --> 00:23:21,900
little bit interesting though what we

00:23:20,110 --> 00:23:24,130
did is on every entry to the guest

00:23:21,900 --> 00:23:26,530
we took the guest state from memory and

00:23:24,130 --> 00:23:28,390
programmed that into the hardware then

00:23:26,530 --> 00:23:29,980
we were running the the guest and the

00:23:28,390 --> 00:23:32,430
guest could mess around with the timers

00:23:29,980 --> 00:23:35,860
without trapping to the hypervisor and

00:23:32,430 --> 00:23:37,900
then when we exited from the VM before

00:23:35,860 --> 00:23:39,430
we enabled interrupt we would capture

00:23:37,900 --> 00:23:42,670
all the state from the Hardware write it

00:23:39,430 --> 00:23:44,770
into memory disable the timer and never

00:23:42,670 --> 00:23:46,390
actually take any interrupt or handle

00:23:44,770 --> 00:23:48,400
any interrupts and then figure out in

00:23:46,390 --> 00:23:52,450
software if we needed to inject a timer

00:23:48,400 --> 00:23:54,580
into the guest with optimized caviar

00:23:52,450 --> 00:23:56,610
what we do is at be cpu load time we

00:23:54,580 --> 00:23:59,380
program the timer with the guest state

00:23:56,610 --> 00:24:00,520
again when the BP CPU is running the

00:23:59,380 --> 00:24:03,520
guest can program the timer without

00:24:00,520 --> 00:24:04,270
exiting to the hypervisor when the timer

00:24:03,520 --> 00:24:06,670
then fires

00:24:04,270 --> 00:24:08,500
we don't disable it we just enable

00:24:06,670 --> 00:24:10,530
interrupts on the host and now we can

00:24:08,500 --> 00:24:12,460
handle interrupts on the host side and

00:24:10,530 --> 00:24:14,770
inject virtual interrupts of the guest

00:24:12,460 --> 00:24:16,690
directly from the ISR and the benefit

00:24:14,770 --> 00:24:22,720
here is that we end up doing no work on

00:24:16,690 --> 00:24:23,200
the CPU entry and be CPU exit we also

00:24:22,720 --> 00:24:26,139
improve

00:24:23,200 --> 00:24:27,669
how we handle l1 system registers we

00:24:26,139 --> 00:24:30,610
avoid saving and restoring the yield one

00:24:27,669 --> 00:24:32,559
state every time we go between the VM

00:24:30,610 --> 00:24:36,000
and the hypervisor because we now can do

00:24:32,559 --> 00:24:36,000
that and we cpu load and BC if you put

00:24:36,720 --> 00:24:41,110
another thing we do is we stop enabling

00:24:38,950 --> 00:24:42,519
and disabling virtualization features or

00:24:41,110 --> 00:24:44,710
traps every time we go from the

00:24:42,519 --> 00:24:47,710
hypervisor to the VM so why did we do

00:24:44,710 --> 00:24:49,539
that before well you had your host Linux

00:24:47,710 --> 00:24:51,700
instance running in year one and you had

00:24:49,539 --> 00:24:54,070
your VM kernel running in the ill one

00:24:51,700 --> 00:24:55,330
you didn't want your VM kernel to be

00:24:54,070 --> 00:24:57,070
able to manage your underlying Hardware

00:24:55,330 --> 00:24:58,360
access all memory on your system but you

00:24:57,070 --> 00:25:01,149
did want your host kernel to be able to

00:24:58,360 --> 00:25:03,519
do that so the way we did that was

00:25:01,149 --> 00:25:06,789
whenever we entered the VM we said let's

00:25:03,519 --> 00:25:08,289
enable states to memory translation and

00:25:06,789 --> 00:25:11,289
when we came back we disabled it so we

00:25:08,289 --> 00:25:13,600
essentially made the host Linux instance

00:25:11,289 --> 00:25:14,710
more privileged than the VM kernel even

00:25:13,600 --> 00:25:20,230
though they're running in the same CPU

00:25:14,710 --> 00:25:22,059
mode okay in the optimized version we

00:25:20,230 --> 00:25:24,190
just configure the hardware once and we

00:25:22,059 --> 00:25:26,380
never touch it again and why can we do

00:25:24,190 --> 00:25:29,200
that well that's because l2 is never

00:25:26,380 --> 00:25:30,850
affected by the configuration of the

00:25:29,200 --> 00:25:34,470
virtualization features it never really

00:25:30,850 --> 00:25:37,029
traps and yield zero

00:25:34,470 --> 00:25:43,210
follows that when it runs with GG as

00:25:37,029 --> 00:25:45,190
part of the hosts Linux context finally

00:25:43,210 --> 00:25:47,590
we in completely rewrote what we call

00:25:45,190 --> 00:25:49,809
our well switch code so our well switch

00:25:47,590 --> 00:25:51,279
code is essentially the software

00:25:49,809 --> 00:25:55,659
implementation of the VM enter in VM

00:25:51,279 --> 00:25:58,510
exits and x86 so basically what we do is

00:25:55,659 --> 00:26:00,340
we have a single static key in the run

00:25:58,510 --> 00:26:03,220
loop that says if you're on a vhe system

00:26:00,340 --> 00:26:06,070
call this function if you're not on a

00:26:03,220 --> 00:26:08,230
VHD system then make a hyper call

00:26:06,070 --> 00:26:12,190
instruction and jump into l2 and run

00:26:08,230 --> 00:26:15,010
another function the vhe function ends

00:26:12,190 --> 00:26:17,289
up being very very simple it only

00:26:15,010 --> 00:26:20,889
switches a few registers around and

00:26:17,289 --> 00:26:24,929
that's it the non vhe one is pretty

00:26:20,889 --> 00:26:27,070
complicated we actually tried first just

00:26:24,929 --> 00:26:30,309
conditional icing and using static keys

00:26:27,070 --> 00:26:34,330
all over for the for the non bhg version

00:26:30,309 --> 00:26:36,730
but we just couldn't get it as fast so

00:26:34,330 --> 00:26:37,149
with our redesigned and optimized a new

00:26:36,730 --> 00:26:40,570
improve

00:26:37,149 --> 00:26:43,059
KVM armed we then also measure

00:26:40,570 --> 00:26:44,289
performance this time we also provide

00:26:43,059 --> 00:26:47,440
some x86 results as a baseline

00:26:44,289 --> 00:26:50,200
comparison and we used xeon e5 2450 for

00:26:47,440 --> 00:26:51,460
that and any workloads that involve the

00:26:50,200 --> 00:26:53,499
client and a server you use 10 gig

00:26:51,460 --> 00:26:55,570
Ethernet with me if I ou pass through

00:26:53,499 --> 00:26:56,950
and we always made sure that the client

00:26:55,570 --> 00:27:00,909
wasn't saturated and we configured all

00:26:56,950 --> 00:27:02,499
VMs and hosts in very similar ways using

00:27:00,909 --> 00:27:06,700
exactly the same software setup same

00:27:02,499 --> 00:27:08,739
kernel configuration and so on so these

00:27:06,700 --> 00:27:10,149
numbers again show cycles so we can try

00:27:08,739 --> 00:27:12,070
to compare across architectures and

00:27:10,149 --> 00:27:13,450
across differently clock platforms and

00:27:12,070 --> 00:27:16,389
what you have here is you have our non

00:27:13,450 --> 00:27:18,940
vhe legacy result we have our optimized

00:27:16,389 --> 00:27:22,989
k vm our result and we have x86 as a

00:27:18,940 --> 00:27:25,419
comparison so we see that we get more

00:27:22,989 --> 00:27:28,899
than a 75% reduction in the over Holika

00:27:25,419 --> 00:27:29,379
had in the hyper call cost which is

00:27:28,899 --> 00:27:34,659
pretty awesome

00:27:29,379 --> 00:27:36,909
we go from around 3000 cycles to 750 IO

00:27:34,659 --> 00:27:39,909
kernel is the cost of accessing an IO

00:27:36,909 --> 00:27:42,429
device from a VM kernel in the Indy host

00:27:39,909 --> 00:27:44,830
kernel like accessing your interrupt

00:27:42,429 --> 00:27:48,279
controller and it goes from around 4000

00:27:44,830 --> 00:27:50,549
cycles for 1600 so also pretty good IO

00:27:48,279 --> 00:27:52,690
user actually goes up a little bit

00:27:50,549 --> 00:27:56,159
that's because we do a lot of work and B

00:27:52,690 --> 00:27:58,119
CPU put and then virtual IP I goes from

00:27:56,159 --> 00:28:00,279
14,000 cycles to around two and a half

00:27:58,119 --> 00:28:02,440
thousand cycles that's a really really

00:28:00,279 --> 00:28:03,429
important result we also see that arm

00:28:02,440 --> 00:28:04,599
actually ends up doing a little bit

00:28:03,429 --> 00:28:09,639
better than x86 and some of these

00:28:04,599 --> 00:28:13,059
results that's also interesting we also

00:28:09,639 --> 00:28:15,159
looked at some real applications so we

00:28:13,059 --> 00:28:18,460
ran you know current bench to measure

00:28:15,159 --> 00:28:20,799
CPU intensive workload hack bench which

00:28:18,460 --> 00:28:22,210
really ends up measuring IPI performance

00:28:20,799 --> 00:28:23,700
because Linux really likes to do IP eyes

00:28:22,210 --> 00:28:26,109
when you do something with the scheduler

00:28:23,700 --> 00:28:28,599
net perf and various configurations to

00:28:26,109 --> 00:28:30,749
measure network performance Apache to

00:28:28,599 --> 00:28:33,309
measure on it web server performs and

00:28:30,749 --> 00:28:37,089
memcache D configured to be really

00:28:33,309 --> 00:28:39,339
related to sensitive so it looks like

00:28:37,089 --> 00:28:41,259
this so the blue bars here are our non

00:28:39,339 --> 00:28:43,479
vhe resolved the green bars is our

00:28:41,259 --> 00:28:46,499
optimized great kbm arm hypervisor and

00:28:43,479 --> 00:28:50,500
the yellow bars is x86 for comparison

00:28:46,499 --> 00:28:53,330
this slide shows overhead so

00:28:50,500 --> 00:28:55,070
virtualized performance normalized to 1

00:28:53,330 --> 00:28:58,730
for native performance so lower here is

00:28:55,070 --> 00:29:02,870
better and means less overhead ok so we

00:28:58,730 --> 00:29:04,790
basically see that for most network

00:29:02,870 --> 00:29:07,120
related benchmarks that had overhead

00:29:04,790 --> 00:29:11,690
before and for hack bench we reduced the

00:29:07,120 --> 00:29:13,640
cost by about 50% we also see that we

00:29:11,690 --> 00:29:15,890
reduce a latency sensitive workloads

00:29:13,640 --> 00:29:20,060
overhead to 1/5 of what it was before

00:29:15,890 --> 00:29:23,300
and if we compare optimized kbm armed

00:29:20,060 --> 00:29:24,560
with x86 some cases x86 is a little

00:29:23,300 --> 00:29:25,970
better some case arm is a little bit

00:29:24,560 --> 00:29:33,800
better but we're sort of leveling the

00:29:25,970 --> 00:29:36,130
playing field so in conclusion we had to

00:29:33,800 --> 00:29:39,560
pretty significantly redesign and

00:29:36,130 --> 00:29:41,390
re-implement KVM arm to optimize it and

00:29:39,560 --> 00:29:43,520
that gives us some significant

00:29:41,390 --> 00:29:47,150
performance improvements for both micro

00:29:43,520 --> 00:29:48,140
and real application level benchmarks we

00:29:47,150 --> 00:29:49,790
end up having similar or better

00:29:48,140 --> 00:29:53,180
performance characteristics compared to

00:29:49,790 --> 00:29:54,590
x86 and this work is published in UNIX

00:29:53,180 --> 00:29:57,050
this year so if you want to go read a

00:29:54,590 --> 00:30:00,950
lengthy analysis of the results I

00:29:57,050 --> 00:30:04,370
recommend that you read the paper in

00:30:00,950 --> 00:30:05,720
terms of code we already have before of

00:30:04,370 --> 00:30:08,240
the timer optimization patches on the

00:30:05,720 --> 00:30:09,500
list the core optimization patches are

00:30:08,240 --> 00:30:13,630
in their first version they're on the

00:30:09,500 --> 00:30:15,430
list the whole Linux and ul2 thing is

00:30:13,630 --> 00:30:17,540
hosted on github

00:30:15,430 --> 00:30:18,920
disclaimer it's completely unsupported

00:30:17,540 --> 00:30:20,390
like you can look at it you can have fun

00:30:18,920 --> 00:30:21,770
with it play with it it works on the

00:30:20,390 --> 00:30:24,650
kernel version that's there but I don't

00:30:21,770 --> 00:30:28,220
plan on doing more work on it I hope to

00:30:24,650 --> 00:30:33,140
upstream the first 2x4 16 but it depends

00:30:28,220 --> 00:30:35,330
on how things will go and then here's

00:30:33,140 --> 00:30:37,280
the thing if you feel really really sad

00:30:35,330 --> 00:30:39,200
because you missed your chance to review

00:30:37,280 --> 00:30:40,970
the arm 64 hypervisor implementation the

00:30:39,200 --> 00:30:42,350
first time mark did it or when he did it

00:30:40,970 --> 00:30:43,970
a second time when he rewrote all the

00:30:42,350 --> 00:30:45,230
assembly code into C code and you just

00:30:43,970 --> 00:30:47,120
can't get over the fact that you missed

00:30:45,230 --> 00:30:49,460
the chance right then here it is here's

00:30:47,120 --> 00:30:51,290
your final chance to review core arm 64

00:30:49,460 --> 00:30:54,290
hypervisor implementations okay it's a

00:30:51,290 --> 00:30:55,810
gift from me to you and that's it

00:30:54,290 --> 00:31:04,020
thanks for listening

00:30:55,810 --> 00:31:04,020
[Applause]

00:31:06,120 --> 00:31:09,540
any questions

00:31:17,110 --> 00:31:22,510
nobody's wondering why it became more

00:31:19,390 --> 00:31:23,640
expensive to go to el1 or for it to go

00:31:22,510 --> 00:31:27,940
back to userspace

00:31:23,640 --> 00:31:31,060
when you did a an IOU sir exes after the

00:31:27,940 --> 00:31:32,680
optimizations we move logic from the run

00:31:31,060 --> 00:31:33,940
loop out into BC if you put right but

00:31:32,680 --> 00:31:38,530
it's the same logic so why did it become

00:31:33,940 --> 00:31:40,300
more expensive the reason is that we

00:31:38,530 --> 00:31:43,570
have to set up this exception forwarding

00:31:40,300 --> 00:31:45,850
shim in the l1 right so we actually end

00:31:43,570 --> 00:31:49,330
up having to restore more State than we

00:31:45,850 --> 00:31:51,730
did before and we don't actually expect

00:31:49,330 --> 00:31:52,780
that number to be as high on real beauty

00:31:51,730 --> 00:31:55,080
systems because you don't have to do

00:31:52,780 --> 00:31:55,080
that work

00:32:06,809 --> 00:32:12,580
just as and when you compare the

00:32:09,280 --> 00:32:14,080
application of work loaded to you I you

00:32:12,580 --> 00:32:16,929
use x86 right

00:32:14,080 --> 00:32:19,870
is that because you treated exhibit x86

00:32:16,929 --> 00:32:22,540
as a speed of light oh oh like there's

00:32:19,870 --> 00:32:25,120
other concern because I was curious if

00:32:22,540 --> 00:32:26,710
you have a number of a ten because it

00:32:25,120 --> 00:32:28,750
have one right

00:32:26,710 --> 00:32:31,630
I mean as you said have one you don't

00:32:28,750 --> 00:32:33,040
have to do too much modification so

00:32:31,630 --> 00:32:36,309
sorry you I didn't hear your question

00:32:33,040 --> 00:32:38,530
about x86 so I'm just basically curious

00:32:36,309 --> 00:32:41,380
if you have the number of when when you

00:32:38,530 --> 00:32:42,700
run the application on Zen as part of

00:32:41,380 --> 00:32:44,290
this work I didn't actually compare

00:32:42,700 --> 00:32:46,210
against then but I did do a performance

00:32:44,290 --> 00:32:49,179
study before of KVM arm before it was

00:32:46,210 --> 00:32:51,460
optimized against then and there's a

00:32:49,179 --> 00:32:53,650
paper from SK last year which outlines

00:32:51,460 --> 00:32:56,440
that and what that actually really shows

00:32:53,650 --> 00:33:00,490
is that then then ends up performing

00:32:56,440 --> 00:33:03,760
worse without pass-through if you do

00:33:00,490 --> 00:33:08,140
Verdi o or Zen pv then KVM arm when k vm

00:33:03,760 --> 00:33:09,610
r was not optimized yet so for pass

00:33:08,140 --> 00:33:11,049
through i don't know how the picture

00:33:09,610 --> 00:33:12,340
will look exactly but i don't know how

00:33:11,049 --> 00:33:14,830
that will look

00:33:12,340 --> 00:33:19,290
on future hardware my pet escape in will

00:33:14,830 --> 00:33:22,799
do better but still answer the question

00:33:19,290 --> 00:33:22,799
there was a question here

00:33:34,850 --> 00:33:40,799
so you already had two three iterations

00:33:37,860 --> 00:33:43,980
of folk so you are still trying more or

00:33:40,799 --> 00:33:47,250
you are just done with by comparing with

00:33:43,980 --> 00:33:49,080
x86 and whatever numbers you are you

00:33:47,250 --> 00:33:50,850
have caught and you're okay with that

00:33:49,080 --> 00:33:52,710
so the question is if I'm planning on

00:33:50,850 --> 00:33:54,600
doing more experiments with more results

00:33:52,710 --> 00:33:56,220
later six and armed basically yeah

00:33:54,600 --> 00:33:57,679
as soon as I get my hands on v.i.c

00:33:56,220 --> 00:34:09,980
Hardware I'd love to do some more

00:33:57,679 --> 00:34:13,050
measurements okay thank you

00:34:09,980 --> 00:34:19,389
[Applause]

00:34:13,050 --> 00:34:19,389

YouTube URL: https://www.youtube.com/watch?v=j0bp4fnO98w


