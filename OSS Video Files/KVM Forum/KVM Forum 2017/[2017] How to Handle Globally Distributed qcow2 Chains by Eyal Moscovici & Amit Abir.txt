Title: [2017] How to Handle Globally Distributed qcow2 Chains by Eyal Moscovici & Amit Abir
Publication date: 2017-11-16
Playlist: KVM Forum 2017
Description: 
	In Oracle Ravello we leverage QCOW2 and the public cloud Object Storage as the backbone of our virtual cloud block storage. Guest disk images are stored in QCOW2 chains that are distributed globally across multiple regions of the cloud's object storage. Long chains incur performance penalty and larger memory consumption. However, shortening the chains is not trivial.

---

Amit Abir
Oracle Ravello
Virtual Storage & Networking Team Leader
Israel

Amit is working in Oracle Ravello for the past 6 years. He has been involved in multiple areas of the company's cloud-based virtualization solution, and is currently in the role of Virtual Storage and Networking team leader. He has been involved n the past few years in optimizing and improving the storage and network overlay of the product.

Eyal Moscovici
Oracle Ravello Systems
Software Engineer, Virtualization Group

Eyal is a software engineer at Virtualization Group at Oracle Ravello, focusing on the Linux kernel and QEMU. He has a M.Sc. degree in computer science at the Technion, Israel Institute of Technology under the advisement of Prof. Dan Tsafrir. He previously worked on vhost enhancements as part of his research which he presented in KVM Furom 2015, and Systor 2016.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:07,099 --> 00:00:14,429
hi everyone hello let's start

00:00:10,889 --> 00:00:16,289
my name is Amitabh ear this is Al we're

00:00:14,429 --> 00:00:21,480
going to discuss today our use case for

00:00:16,289 --> 00:00:23,550
using Keuka to change using qmu so a

00:00:21,480 --> 00:00:26,760
little bit about us so I am a mate I

00:00:23,550 --> 00:00:29,519
joined Oracle Ravello in 2011 and I'm

00:00:26,760 --> 00:00:32,130
currently the team leader of virtual and

00:00:29,519 --> 00:00:34,739
network and storage and this is the owl

00:00:32,130 --> 00:00:38,100
he is a programmer software engineer in

00:00:34,739 --> 00:00:39,840
our virtualization group so this is the

00:00:38,100 --> 00:00:42,960
agenda for today we are going to discuss

00:00:39,840 --> 00:00:44,969
how we use in your Cobra Velo kyouko

00:00:42,960 --> 00:00:47,010
chains in q mu what is our unique use

00:00:44,969 --> 00:00:50,309
case and for that I will need to do a

00:00:47,010 --> 00:00:52,140
short introduction about our product our

00:00:50,309 --> 00:00:54,590
product works then we are going to

00:00:52,140 --> 00:00:58,170
discuss our design for the storage layer

00:00:54,590 --> 00:01:01,680
simplement ations and challenges winged

00:00:58,170 --> 00:01:03,120
carded and their solutions so let me

00:01:01,680 --> 00:01:04,589
begin by describing what is a

00:01:03,120 --> 00:01:06,299
recoverable oh this is important to

00:01:04,589 --> 00:01:09,659
understand the rest of the talk

00:01:06,299 --> 00:01:12,030
so oracle was founded in 2011 by the

00:01:09,659 --> 00:01:16,229
same guys who started coronet which

00:01:12,030 --> 00:01:20,070
created HDX KVM sorry and it was

00:01:16,229 --> 00:01:21,570
acquired by Oracle in 2016 so Oracle

00:01:20,070 --> 00:01:24,299
Ravello is basically what we call a

00:01:21,570 --> 00:01:26,880
virtual cloud provider so it runs just

00:01:24,299 --> 00:01:29,670
like any other cloud provider but on top

00:01:26,880 --> 00:01:32,310
of other cloud providers such as AWS

00:01:29,670 --> 00:01:34,290
Amazon's Cloud and Google Cloud and also

00:01:32,310 --> 00:01:37,290
OCI which is the Oracle cloud

00:01:34,290 --> 00:01:40,530
infrastructure what we do is to allow

00:01:37,290 --> 00:01:43,470
easy lift and shift lifting shift is the

00:01:40,530 --> 00:01:46,530
migration of on-premise data center

00:01:43,470 --> 00:01:48,930
virtualization environment workloads to

00:01:46,530 --> 00:01:51,869
the public cloud and we do it without

00:01:48,930 --> 00:01:53,790
need to change not the VM images not

00:01:51,869 --> 00:01:56,040
even a single bit not the network

00:01:53,790 --> 00:01:59,189
configuration and without changing any

00:01:56,040 --> 00:02:00,960
storage configuration so what are the

00:01:59,189 --> 00:02:03,810
challenges when migrating workloads to

00:02:00,960 --> 00:02:06,329
the cloud first of all the virtual

00:02:03,810 --> 00:02:08,190
Hardware different hypervisors have

00:02:06,329 --> 00:02:09,060
different virtual hardware the support

00:02:08,190 --> 00:02:11,879
to their guests

00:02:09,060 --> 00:02:13,250
for example they support they expose

00:02:11,879 --> 00:02:15,440
different chipsets

00:02:13,250 --> 00:02:18,020
from disconnect controllers different

00:02:15,440 --> 00:02:21,290
firmwares different PCI topology and so

00:02:18,020 --> 00:02:24,110
on we must bridge over these gaps in

00:02:21,290 --> 00:02:27,440
order to allow seamless running of the

00:02:24,110 --> 00:02:30,140
VMS on the public cloud also there is a

00:02:27,440 --> 00:02:32,750
difference in network capabilities

00:02:30,140 --> 00:02:35,660
anthropology today the public clouds do

00:02:32,750 --> 00:02:38,209
not support support only l3 ip-based

00:02:35,660 --> 00:02:41,480
communications it means there are no

00:02:38,209 --> 00:02:46,550
switches no violence no me reports and

00:02:41,480 --> 00:02:48,370
no any non IP traffic they support so

00:02:46,550 --> 00:02:50,989
this is another challenge we need to

00:02:48,370 --> 00:02:53,739
handle how do we do that

00:02:50,989 --> 00:02:57,680
Ravel or Oracle do that by utilizing

00:02:53,739 --> 00:03:00,410
nested virtualization we have created

00:02:57,680 --> 00:03:03,110
our own binary translation hypervisor

00:03:00,410 --> 00:03:05,840
that is called hbx which is optimized to

00:03:03,110 --> 00:03:09,680
run on other hypervisors such as KVM or

00:03:05,840 --> 00:03:13,450
Zen on top of the public clouds cloud

00:03:09,680 --> 00:03:17,390
instances when the public clouds exposes

00:03:13,450 --> 00:03:20,120
virtual hardware cysts we can run KVM

00:03:17,390 --> 00:03:23,480
directly and we do that also we have our

00:03:20,120 --> 00:03:26,239
own enhanced Q mu and Fillmore's for

00:03:23,480 --> 00:03:29,180
supporting for example virtual devices

00:03:26,239 --> 00:03:32,180
from VMware such as VM ixnay 3 and pv

00:03:29,180 --> 00:03:35,330
Scaasi which we contributed to the

00:03:32,180 --> 00:03:38,000
mainland to Mew today also we have our

00:03:35,330 --> 00:03:42,620
enhanced framework to support for

00:03:38,000 --> 00:03:48,980
example running games from es6 insects

00:03:42,620 --> 00:03:51,220
and so on to bridge over the gaps are in

00:03:48,980 --> 00:03:54,709
network capabilities we created our own

00:03:51,220 --> 00:03:58,160
software-defined network or Sdn what it

00:03:54,709 --> 00:04:01,310
does its leverage leveraging Linux Sdn

00:03:58,160 --> 00:04:03,769
components such as turn tap devices PC

00:04:01,310 --> 00:04:06,110
actions and so on to create a fully

00:04:03,769 --> 00:04:08,630
distributed network with distributed

00:04:06,110 --> 00:04:11,329
network functions such as DNS and DHCP

00:04:08,630 --> 00:04:14,299
servers and virtual switch that

00:04:11,329 --> 00:04:15,829
leverages open V switch capabilities but

00:04:14,299 --> 00:04:18,380
this is a conversation for a whole

00:04:15,829 --> 00:04:20,060
different talk so let me describe to you

00:04:18,380 --> 00:04:23,510
the flow from the customers perspective

00:04:20,060 --> 00:04:25,650
what what's happening so the customer as

00:04:23,510 --> 00:04:28,470
you see at the bottom left

00:04:25,650 --> 00:04:30,000
the customer has some data center with

00:04:28,470 --> 00:04:32,990
some specific hardware and some

00:04:30,000 --> 00:04:36,449
hypervisor like VirtualBox hyper-v

00:04:32,990 --> 00:04:38,910
whatever that runs some VM 3b ends with

00:04:36,449 --> 00:04:40,919
a specific network configuration then

00:04:38,910 --> 00:04:44,070
the user uses Cervelo input tool to

00:04:40,919 --> 00:04:46,919
upload the VM images to Ravello image

00:04:44,070 --> 00:04:50,340
storage I will elaborate on that a

00:04:46,919 --> 00:04:52,919
little bit later then the second step as

00:04:50,340 --> 00:04:54,660
you can see on the bottom right the user

00:04:52,919 --> 00:04:58,199
drags and drops in our management

00:04:54,660 --> 00:04:59,850
console the VMS he then configures the

00:04:58,199 --> 00:05:02,220
network configuration or any other

00:04:59,850 --> 00:05:04,530
configuration needed for those VM then

00:05:02,220 --> 00:05:06,660
he clicks on one button publish and we

00:05:04,530 --> 00:05:10,169
automatically create a cloud instance on

00:05:06,660 --> 00:05:13,169
some public cloud any public cloud where

00:05:10,169 --> 00:05:15,300
the transit the public like the public

00:05:13,169 --> 00:05:17,870
cloud Harper hypervisor such as KB mu

00:05:15,300 --> 00:05:21,210
then on top of that we start our own

00:05:17,870 --> 00:05:24,030
hypervisor and on top of that nested

00:05:21,210 --> 00:05:26,669
virtualization we start we start the

00:05:24,030 --> 00:05:28,169
user VMs and we run it the same way as

00:05:26,669 --> 00:05:31,229
it was in the data center the user

00:05:28,169 --> 00:05:34,200
cannot notice the difference okay so

00:05:31,229 --> 00:05:37,260
this was our product and what I'm going

00:05:34,200 --> 00:05:38,789
to discuss today the conv the topic of

00:05:37,260 --> 00:05:42,870
the conversation would be our storage

00:05:38,789 --> 00:05:44,910
layer what challenges did we have while

00:05:42,870 --> 00:05:47,190
trying to solve this problem so the

00:05:44,910 --> 00:05:49,590
first and most important question is to

00:05:47,190 --> 00:05:54,570
where to place the VM disk data okay it

00:05:49,590 --> 00:05:57,030
has to be somewhere the the the choice

00:05:54,570 --> 00:05:59,130
we need to make should support multiple

00:05:57,030 --> 00:06:01,169
clouds and multiple regions because we

00:05:59,130 --> 00:06:03,360
want to be able to support more clouds

00:06:01,169 --> 00:06:05,940
and more regions of those clouds in the

00:06:03,360 --> 00:06:07,830
future also the data must be fetched in

00:06:05,940 --> 00:06:10,260
real time the user cannot sit and wait

00:06:07,830 --> 00:06:12,750
for the VM to boot it must start to boot

00:06:10,260 --> 00:06:15,300
immediately another core business of

00:06:12,750 --> 00:06:17,610
ours is to clone a VM fast this is in

00:06:15,300 --> 00:06:19,410
order to support for example Devon test

00:06:17,610 --> 00:06:21,120
scenarios where the user creates its

00:06:19,410 --> 00:06:23,940
initiate the initial environment and

00:06:21,120 --> 00:06:26,520
then replicate it multiple times to test

00:06:23,940 --> 00:06:29,340
different scenarios also the clone

00:06:26,520 --> 00:06:31,410
scenarios is used for global training

00:06:29,340 --> 00:06:33,800
like those reddit does today

00:06:31,410 --> 00:06:36,180
the Doretta delivers all of their

00:06:33,800 --> 00:06:38,760
international air training courses on

00:06:36,180 --> 00:06:39,389
top of Ravello they did it by creating

00:06:38,760 --> 00:06:41,310
an initial

00:06:39,389 --> 00:06:46,050
an initial training environment and

00:06:41,310 --> 00:06:50,759
replicates it to different regions to

00:06:46,050 --> 00:06:53,039
reach to reach global regions and also

00:06:50,759 --> 00:06:55,080
the last point is that Rises the disc

00:06:53,039 --> 00:06:57,539
must be persistent if the user did some

00:06:55,080 --> 00:07:00,479
writes change the VM maybe it was a data

00:06:57,539 --> 00:07:02,789
base the write should be kept somewhere

00:07:00,479 --> 00:07:05,129
so the next time the user stops the VM

00:07:02,789 --> 00:07:07,860
whether it's in the same cloud in region

00:07:05,129 --> 00:07:13,199
or in other regions the data should be

00:07:07,860 --> 00:07:15,330
kept persistently ok so let's try to

00:07:13,199 --> 00:07:17,460
solve this problem let's try the most

00:07:15,330 --> 00:07:20,580
basic solution we can place the VM data

00:07:17,460 --> 00:07:23,129
directly on the cloud volume EBS for

00:07:20,580 --> 00:07:25,169
those of you who know that the cloud

00:07:23,129 --> 00:07:27,840
volume is a persistent block storage

00:07:25,169 --> 00:07:31,620
device that is exposed to the cloud

00:07:27,840 --> 00:07:37,229
instance you can attach it and you can

00:07:31,620 --> 00:07:39,180
by that expand the disks for the VM so

00:07:37,229 --> 00:07:41,580
we can place the data on the volume

00:07:39,180 --> 00:07:45,569
whether the VM was started and whether

00:07:41,580 --> 00:07:47,759
it is topped once the VM starts we

00:07:45,569 --> 00:07:51,569
attach the volume with the data to the

00:07:47,759 --> 00:07:54,479
2d instance and we exposed to Q mu the

00:07:51,569 --> 00:07:58,099
volume exposed device as its disk

00:07:54,479 --> 00:08:02,120
backing for in order to run the VM and

00:07:58,099 --> 00:08:05,159
the right to the volume would be done

00:08:02,120 --> 00:08:06,659
locally on the volume once we're done we

00:08:05,159 --> 00:08:09,180
can detach the volume and keep it for

00:08:06,659 --> 00:08:10,650
later use so the advantage of this

00:08:09,180 --> 00:08:12,810
solution is obviously the performance

00:08:10,650 --> 00:08:15,300
the data is available it is right there

00:08:12,810 --> 00:08:18,000
and we have zero time to first byte the

00:08:15,300 --> 00:08:21,020
VM can start the boot immediately but it

00:08:18,000 --> 00:08:23,479
has some disadvantages first of all

00:08:21,020 --> 00:08:26,189
currently the cloud volumes are

00:08:23,479 --> 00:08:28,680
obviously cloud dependent and cloud

00:08:26,189 --> 00:08:30,930
bounded and also region bounded you

00:08:28,680 --> 00:08:33,630
cannot move a volume between different

00:08:30,930 --> 00:08:35,370
regions in the same cloud also we have a

00:08:33,630 --> 00:08:37,320
long cloning time even in the same

00:08:35,370 --> 00:08:39,360
region because you have to copy the

00:08:37,320 --> 00:08:41,070
entire data to a different volume so you

00:08:39,360 --> 00:08:44,430
can attach it to a different VM or to

00:08:41,070 --> 00:08:45,930
the same VM in different place and the

00:08:44,430 --> 00:08:47,760
third point is that it is way too

00:08:45,930 --> 00:08:50,040
expensive keeping the data in the

00:08:47,760 --> 00:08:53,190
volumes all the time whether the VM was

00:08:50,040 --> 00:08:56,250
started or not is not very good

00:08:53,190 --> 00:08:58,470
regarding the costs so let's try an

00:08:56,250 --> 00:09:00,420
alternative solution we can place the

00:08:58,470 --> 00:09:03,990
data in the clouds object storage like

00:09:00,420 --> 00:09:05,850
the s3 of Amazon the object storage is a

00:09:03,990 --> 00:09:07,980
persistent data storage meant to kept

00:09:05,850 --> 00:09:11,730
large amount of data and to retrieve it

00:09:07,980 --> 00:09:13,950
globally and hopefully efficiently so we

00:09:11,730 --> 00:09:15,630
can place the raw file at the file in

00:09:13,950 --> 00:09:17,400
the RAW format directly on the object

00:09:15,630 --> 00:09:19,470
storage and then whenever the user

00:09:17,400 --> 00:09:21,720
starts the VM we can download the data

00:09:19,470 --> 00:09:24,390
to an empty volume that we will attach

00:09:21,720 --> 00:09:26,100
to the VM when it started and then the

00:09:24,390 --> 00:09:28,050
process would be the same as the

00:09:26,100 --> 00:09:30,090
previous solution writes would be done

00:09:28,050 --> 00:09:31,650
locally and when we're done we can

00:09:30,090 --> 00:09:35,490
upload the file back to the object

00:09:31,650 --> 00:09:37,500
storage so the advantages comparing to

00:09:35,490 --> 00:09:39,870
the previous solution now the data is

00:09:37,500 --> 00:09:42,240
globally available we can use it in any

00:09:39,870 --> 00:09:44,700
clouds or any region this is the purpose

00:09:42,240 --> 00:09:46,650
of the object storage also the cloning

00:09:44,700 --> 00:09:48,480
is fast you can start multiple VMs at

00:09:46,650 --> 00:09:50,490
the same time and they would all all

00:09:48,480 --> 00:09:53,610
download the data at the same time the

00:09:50,490 --> 00:09:55,350
object storage supports that comparing

00:09:53,610 --> 00:09:57,840
to keeping the data on the volumes

00:09:55,350 --> 00:10:01,770
themselves it is much more it is very

00:09:57,840 --> 00:10:04,200
inexpensive comparing to that but this

00:10:01,770 --> 00:10:06,960
solution also have some disadvantages

00:10:04,200 --> 00:10:08,610
first of all we have a long boot time in

00:10:06,960 --> 00:10:11,370
order to start the VM we first need to

00:10:08,610 --> 00:10:13,320
download the entire file this is in

00:10:11,370 --> 00:10:16,770
order to expose to the qmu as the

00:10:13,320 --> 00:10:19,080
backing so the data should be first

00:10:16,770 --> 00:10:20,630
downloaded also we have a very long

00:10:19,080 --> 00:10:23,070
snapshot time the data has to be

00:10:20,630 --> 00:10:25,350
uploaded back to the object storage and

00:10:23,070 --> 00:10:28,290
the VM and the volume should be kept

00:10:25,350 --> 00:10:31,500
alive while doing that and if we're

00:10:28,290 --> 00:10:33,840
discussing saving different snapshots of

00:10:31,500 --> 00:10:35,400
the VMs in different times we have to

00:10:33,840 --> 00:10:37,740
store the same sectors over and over

00:10:35,400 --> 00:10:39,960
again because the fact the writes are

00:10:37,740 --> 00:10:43,380
done locally and when we upload them to

00:10:39,960 --> 00:10:45,780
the cloud we just save another snapshot

00:10:43,380 --> 00:10:51,690
of the same file another version so a

00:10:45,780 --> 00:10:54,480
lot of data is being duplicated okay so

00:10:51,690 --> 00:10:56,940
let me describe our solution which is to

00:10:54,480 --> 00:10:59,040
place the base image in the object

00:10:56,940 --> 00:11:01,080
storage and then upload only the deltas

00:10:59,040 --> 00:11:03,330
only the new writes every time the VM

00:11:01,080 --> 00:11:06,330
needs to upload the data whether it

00:11:03,330 --> 00:11:07,080
stops or a snapshot is taken so what we

00:11:06,330 --> 00:11:09,810
do

00:11:07,080 --> 00:11:12,029
we attach a volume to the VM but it is

00:11:09,810 --> 00:11:14,730
meant only for the new writes the new

00:11:12,029 --> 00:11:16,800
writes are written to the tip file which

00:11:14,730 --> 00:11:19,200
is the local file that holds the new

00:11:16,800 --> 00:11:22,200
writes and when we're done we can upload

00:11:19,200 --> 00:11:25,500
the new file as a new link in the chain

00:11:22,200 --> 00:11:27,180
in the image chain okay then the reads

00:11:25,500 --> 00:11:30,510
could be done remotely we don't need to

00:11:27,180 --> 00:11:32,579
download any data in advance so what are

00:11:30,510 --> 00:11:34,170
the advantages of that comparing to the

00:11:32,579 --> 00:11:36,360
previous solution the boot can start

00:11:34,170 --> 00:11:39,180
immediately as I said we can download

00:11:36,360 --> 00:11:41,430
the data only there the factors we need

00:11:39,180 --> 00:11:42,600
directly from the object storage we

00:11:41,430 --> 00:11:46,410
don't need to wait to download the

00:11:42,600 --> 00:11:49,200
entire chain also we store only new data

00:11:46,410 --> 00:11:53,399
we don't need to store duplicate sectors

00:11:49,200 --> 00:11:54,899
and same as before it is globally

00:11:53,399 --> 00:11:56,760
available because it's in the object

00:11:54,899 --> 00:11:58,829
storage the cloning is fast we can

00:11:56,760 --> 00:12:01,740
create multiple tapes to the same chain

00:11:58,829 --> 00:12:04,649
that's creating a tree and it's quite

00:12:01,740 --> 00:12:07,890
inexpensive but it has one disadvantage

00:12:04,649 --> 00:12:10,800
and which is the performance penalty we

00:12:07,890 --> 00:12:12,839
now we don't keep the file as one single

00:12:10,800 --> 00:12:14,190
file and rough format we're keeping a

00:12:12,839 --> 00:12:16,560
chain and there is some performance

00:12:14,190 --> 00:12:20,910
drawbacks from reading the metadata and

00:12:16,560 --> 00:12:24,570
so on so let me describe the

00:12:20,910 --> 00:12:27,600
architecture for our storage layer so

00:12:24,570 --> 00:12:30,899
our VM disk data are backed by Q cup -

00:12:27,600 --> 00:12:32,970
image chain as I told you as you can see

00:12:30,899 --> 00:12:35,279
in the drawing the cloud volumes holds

00:12:32,970 --> 00:12:37,680
the tip file which is the file that

00:12:35,279 --> 00:12:40,079
holds all the rights and all the weights

00:12:37,680 --> 00:12:42,959
are being done remotely okay

00:12:40,079 --> 00:12:45,570
I mean reads from previous parts of the

00:12:42,959 --> 00:12:48,089
chain so this is done using our cloud

00:12:45,570 --> 00:12:50,970
file system we call cloud fest it's a

00:12:48,089 --> 00:12:53,070
read-only storage layer file system what

00:12:50,970 --> 00:12:55,709
it basically does is it it just

00:12:53,070 --> 00:12:57,690
translates disk reads when Q you read

00:12:55,709 --> 00:13:00,779
part of the chain it just translates

00:12:57,690 --> 00:13:03,300
translated to HTTP request from the

00:13:00,779 --> 00:13:05,970
object storage also it supports multiple

00:13:03,300 --> 00:13:07,769
clouds in multiple regions all the

00:13:05,970 --> 00:13:10,110
clouds I mentioned before and all the

00:13:07,769 --> 00:13:12,360
regions and what it also does is to

00:13:10,110 --> 00:13:14,430
cache read data locally as you can see

00:13:12,360 --> 00:13:16,230
it's in the drawing on the local volume

00:13:14,430 --> 00:13:18,630
we keep a cache of all the read data

00:13:16,230 --> 00:13:20,550
this is another two E's later reads of

00:13:18,630 --> 00:13:22,800
course so we don't have to

00:13:20,550 --> 00:13:25,050
go to the object storage every time we

00:13:22,800 --> 00:13:27,899
read the same sector over and over again

00:13:25,050 --> 00:13:31,550
and we implemented it using fuse so it

00:13:27,899 --> 00:13:33,990
is implemented in user space currently

00:13:31,550 --> 00:13:37,110
let's see an example for the read flow

00:13:33,990 --> 00:13:39,180
whenever qmu reads a disk let's say a

00:13:37,110 --> 00:13:42,209
file let's say there is some file before

00:13:39,180 --> 00:13:44,670
it is somewhere in an image chain or the

00:13:42,209 --> 00:13:47,250
tip file but some file from a previous

00:13:44,670 --> 00:13:51,300
state of the VM so there's a read

00:13:47,250 --> 00:13:53,760
command executed by Q with some offset

00:13:51,300 --> 00:13:56,550
in some size with then catch it in our

00:13:53,760 --> 00:13:59,279
cloud of test code as a fuse operation

00:13:56,550 --> 00:14:04,829
and then we translate it in this example

00:13:59,279 --> 00:14:07,860
to HTTP GET request from the s3 of AWS

00:14:04,829 --> 00:14:10,380
we also add a special range header to

00:14:07,860 --> 00:14:12,329
the request to get only the needed bytes

00:14:10,380 --> 00:14:15,000
so we don't need to download the entire

00:14:12,329 --> 00:14:17,040
file we just read the factors that we

00:14:15,000 --> 00:14:20,130
need the next step would be to cash

00:14:17,040 --> 00:14:22,860
these read bytes to the local storage to

00:14:20,130 --> 00:14:25,620
the local volume so we don't need to

00:14:22,860 --> 00:14:28,649
read them over and over again let's see

00:14:25,620 --> 00:14:31,290
an example for the right flow okay so as

00:14:28,649 --> 00:14:33,839
I told you we keep a tip and now emit

00:14:31,290 --> 00:14:36,209
the cloud volume okay you can assume

00:14:33,839 --> 00:14:39,060
that the cloud VM has some volume

00:14:36,209 --> 00:14:42,120
attached to it so the tip is located on

00:14:39,060 --> 00:14:45,480
the on the cloud volume and it is

00:14:42,120 --> 00:14:48,720
holding new reads we created using cue

00:14:45,480 --> 00:14:51,390
image create command in two cases first

00:14:48,720 --> 00:14:53,670
before VM starts in order to hold it up

00:14:51,390 --> 00:14:55,410
coming right and also before a snapshot

00:14:53,670 --> 00:14:58,410
is taken if one was specifically

00:14:55,410 --> 00:15:00,720
requested bar by our customer if the

00:14:58,410 --> 00:15:04,050
guest is live and we don't want to turn

00:15:00,720 --> 00:15:06,480
it down turn it off we use qmp block the

00:15:04,050 --> 00:15:09,270
sanctioned snapshots increment what it

00:15:06,480 --> 00:15:11,790
does is to replace the older tip with

00:15:09,270 --> 00:15:16,110
the new one so the older tip could be

00:15:11,790 --> 00:15:18,240
then uploaded to the object storage so

00:15:16,110 --> 00:15:21,060
as I explained the tip is uploaded to

00:15:18,240 --> 00:15:25,260
the cloud storage in two cases first

00:15:21,060 --> 00:15:26,670
when the VM stops for a persistency if

00:15:25,260 --> 00:15:29,550
the user would like to start the VM

00:15:26,670 --> 00:15:31,800
again later time different cloud can

00:15:29,550 --> 00:15:33,040
just do it or doing a snapshot to keep

00:15:31,800 --> 00:15:36,250
special

00:15:33,040 --> 00:15:40,660
specific state of time state in time of

00:15:36,250 --> 00:15:44,769
the VN how do we accelerate remote reads

00:15:40,660 --> 00:15:46,269
okay going to the requesting data from

00:15:44,769 --> 00:15:50,350
the object storage all the time is not

00:15:46,269 --> 00:15:53,410
so efficient so we have some

00:15:50,350 --> 00:15:55,630
optimizations we've done first of all we

00:15:53,410 --> 00:15:58,540
extend small requests to two megabyte

00:15:55,630 --> 00:16:00,040
requests okay this is first of all first

00:15:58,540 --> 00:16:03,100
of all because we assume that a read

00:16:00,040 --> 00:16:05,709
locality if the guest needed some data

00:16:03,100 --> 00:16:08,889
from the disk then it would probably

00:16:05,709 --> 00:16:12,759
need some neighboring sectors as well so

00:16:08,889 --> 00:16:14,529
we take what we can also we try to

00:16:12,759 --> 00:16:16,540
balance latency and throughput we've

00:16:14,529 --> 00:16:19,680
discovered that extending smaller

00:16:16,540 --> 00:16:22,540
requests to megabytes does not impact

00:16:19,680 --> 00:16:25,870
latency significantly and it's it is

00:16:22,540 --> 00:16:27,579
greatly improves the throughput so we

00:16:25,870 --> 00:16:29,860
did some experiments and we found out

00:16:27,579 --> 00:16:33,550
that two megabyte is the optimal size in

00:16:29,860 --> 00:16:37,449
average for the clouds we use today so

00:16:33,550 --> 00:16:39,339
this is that what we do also we we found

00:16:37,449 --> 00:16:43,380
out that giving the cue code change

00:16:39,339 --> 00:16:46,060
random filenames helps for requesting

00:16:43,380 --> 00:16:48,430
data in parallel from the object

00:16:46,060 --> 00:16:52,149
storages what basically happens is that

00:16:48,430 --> 00:16:54,819
different cloud workers handle requests

00:16:52,149 --> 00:16:56,920
according to the file name so if we do

00:16:54,819 --> 00:16:59,500
several requests in parallel if the

00:16:56,920 --> 00:17:01,360
files are distributed uniformly the

00:16:59,500 --> 00:17:04,329
names then they hit different cloud

00:17:01,360 --> 00:17:08,709
workers and so improved performance by

00:17:04,329 --> 00:17:10,240
handling more requests in parallel let

00:17:08,709 --> 00:17:12,579
me refer to the title of the

00:17:10,240 --> 00:17:15,850
conversation globally distributed chains

00:17:12,579 --> 00:17:18,280
what do I mean by that so every time the

00:17:15,850 --> 00:17:20,589
VM starts it can start on any cloud or

00:17:18,280 --> 00:17:22,959
any region the user their customer can

00:17:20,589 --> 00:17:24,880
choose that and what I didn't say is

00:17:22,959 --> 00:17:27,220
that the new data the tip of the chain

00:17:24,880 --> 00:17:29,890
is uploaded to the same local region

00:17:27,220 --> 00:17:32,110
where the VM was started this is because

00:17:29,890 --> 00:17:34,059
we assume that locality the VM would

00:17:32,110 --> 00:17:37,210
probably start again on the same region

00:17:34,059 --> 00:17:40,210
the same cloud it was started before but

00:17:37,210 --> 00:17:42,460
from time to time we see that users do

00:17:40,210 --> 00:17:44,860
want to start VMs on different cloud or

00:17:42,460 --> 00:17:46,419
different regions so as you can see in

00:17:44,860 --> 00:17:48,610
the example

00:17:46,419 --> 00:17:50,860
situation can be created where a VM was

00:17:48,610 --> 00:17:53,500
started several times in AWS Sydney

00:17:50,860 --> 00:17:56,470
region and then a few times in OC Eric

00:17:53,500 --> 00:17:59,260
the Oracle cloud in Phoenix and then one

00:17:56,470 --> 00:18:01,299
more time in Google Cloud in Frankfort

00:17:59,260 --> 00:18:02,860
that's creating a distributed chain in

00:18:01,299 --> 00:18:05,740
different regions in different object

00:18:02,860 --> 00:18:07,720
storages and the obvious problem that

00:18:05,740 --> 00:18:11,500
arises from that is that if you start

00:18:07,720 --> 00:18:13,870
the VM one more time in Frankfurt it has

00:18:11,500 --> 00:18:15,820
to copy to read all the data from other

00:18:13,870 --> 00:18:18,279
regions from Sydney and from Phoenix

00:18:15,820 --> 00:18:22,210
this is very very inefficient it can

00:18:18,279 --> 00:18:24,600
take really lots of time so in order to

00:18:22,210 --> 00:18:27,600
solve that we added a regional cache

00:18:24,600 --> 00:18:30,850
okay every region has its own special

00:18:27,600 --> 00:18:33,070
cache to keep part of the chain from

00:18:30,850 --> 00:18:35,740
different regions so every time we read

00:18:33,070 --> 00:18:39,700
a remote sector or remote part of the

00:18:35,740 --> 00:18:42,669
file from remote region or another cloud

00:18:39,700 --> 00:18:44,830
we store it in the regional cache so

00:18:42,669 --> 00:18:48,010
that after some time the VM would run as

00:18:44,830 --> 00:18:50,919
if all the data was located in its local

00:18:48,010 --> 00:18:55,860
in its regional object storage and it

00:18:50,919 --> 00:18:57,100
would be effective or efficient okay

00:18:55,860 --> 00:18:59,620
great

00:18:57,100 --> 00:19:01,840
so now I want to discuss a little bit in

00:18:59,620 --> 00:19:05,220
detail some performance drawbacks we

00:19:01,840 --> 00:19:09,370
found out when using Q code change

00:19:05,220 --> 00:19:11,769
okay so this was supposed to be the AOS

00:19:09,370 --> 00:19:15,669
part of the talk but unfortunately he

00:19:11,769 --> 00:19:19,480
lost his voice so I'm taking over so

00:19:15,669 --> 00:19:20,889
first of all Q cow keeps minimal

00:19:19,480 --> 00:19:22,809
information about the entire Chinese

00:19:20,889 --> 00:19:25,330
backing file as you know in the meta

00:19:22,809 --> 00:19:27,970
data currently the format only saves the

00:19:25,330 --> 00:19:29,620
immediate backing gear file and no

00:19:27,970 --> 00:19:32,260
information about the rest of the chain

00:19:29,620 --> 00:19:35,799
so Q mu must often walk the chain

00:19:32,260 --> 00:19:38,019
especially in our scenario and so it has

00:19:35,799 --> 00:19:40,659
to load for example in QV starts it has

00:19:38,019 --> 00:19:42,580
to load the metadata of every file in

00:19:40,659 --> 00:19:44,950
the chain when it's open all the file

00:19:42,580 --> 00:19:48,279
when it's opening all the files and it

00:19:44,950 --> 00:19:51,610
keeps the l1 table in the ROM up to a

00:19:48,279 --> 00:19:54,010
size of one megabyte also some metadata

00:19:51,610 --> 00:19:56,919
of the files are spread especially the

00:19:54,010 --> 00:19:59,169
l2 tables is spread across the image so

00:19:56,919 --> 00:20:00,169
a single read request could actually

00:19:59,169 --> 00:20:04,369
cause multiple

00:20:00,169 --> 00:20:06,649
random random remote reads from multiple

00:20:04,369 --> 00:20:08,090
store files for multiple files in the

00:20:06,649 --> 00:20:11,149
chain okay

00:20:08,090 --> 00:20:13,730
also the queue image commands they work

00:20:11,149 --> 00:20:15,859
on the entire virtual disk as a whole we

00:20:13,730 --> 00:20:18,590
cannot split it into several parts and

00:20:15,859 --> 00:20:20,450
this is me this makes us very hard to

00:20:18,590 --> 00:20:22,220
bound execution time when we try for

00:20:20,450 --> 00:20:25,609
example to run comments such as they

00:20:22,220 --> 00:20:28,399
accuse me a rebase we have to wait until

00:20:25,609 --> 00:20:33,080
it finishes all the virtual disk this

00:20:28,399 --> 00:20:35,210
can be very inefficient so first of all

00:20:33,080 --> 00:20:37,909
we try we're trying to keep the kyouko

00:20:35,210 --> 00:20:39,980
two chains short as I explained a new

00:20:37,909 --> 00:20:42,200
tip is created every time a VM starts

00:20:39,980 --> 00:20:44,119
and every time snippet is requested and

00:20:42,200 --> 00:20:46,369
the problem is that the chains are

00:20:44,119 --> 00:20:48,830
getting longer and longer for example at

00:20:46,369 --> 00:20:53,239
the end that was started 100 times has a

00:20:48,830 --> 00:20:56,179
chain of 100 links okay this is a lot

00:20:53,239 --> 00:20:59,269
and long chain just causes some several

00:20:56,179 --> 00:21:00,859
problems for example high latency right

00:20:59,269 --> 00:21:03,080
because Kimmy needs to work the chain

00:21:00,859 --> 00:21:05,509
and find information in all of the queue

00:21:03,080 --> 00:21:07,629
cow to files in the chain which can be

00:21:05,509 --> 00:21:09,710
also being remote regions in our case

00:21:07,629 --> 00:21:12,739
also we have a very high memory usage

00:21:09,710 --> 00:21:16,100
because for each file qmu keeps an l1

00:21:12,739 --> 00:21:19,429
cache of up to one megabyte size which

00:21:16,100 --> 00:21:21,109
can be some - quite a lot of overhead if

00:21:19,429 --> 00:21:24,830
we're talking about starting on one

00:21:21,109 --> 00:21:28,899
cloud instance many VMs ok of course we

00:21:24,830 --> 00:21:33,559
can change the cache size but it is very

00:21:28,899 --> 00:21:36,139
problematic for performance right so in

00:21:33,559 --> 00:21:39,529
order to try and keep the change sources

00:21:36,139 --> 00:21:42,619
as possible we can obviously merge the

00:21:39,529 --> 00:21:44,269
tip with its backing file before upload

00:21:42,619 --> 00:21:46,220
ok let's see the example in the drawing

00:21:44,269 --> 00:21:48,200
we have the virtual disk and the tip

00:21:46,220 --> 00:21:51,320
file that holds the right it has its

00:21:48,200 --> 00:21:54,350
immediate parent that is file a and B

00:21:51,320 --> 00:21:56,090
which is the rebase target so what we do

00:21:54,350 --> 00:21:59,919
is to merge the tip with its immediate

00:21:56,090 --> 00:22:05,330
parent a its backing file in order to

00:21:59,919 --> 00:22:07,549
and so a new link is replaced by the

00:22:05,330 --> 00:22:10,429
other link and the chain does not get

00:22:07,549 --> 00:22:13,940
any longer right because we merge the

00:22:10,429 --> 00:22:17,930
new writes into the previous uploaded

00:22:13,940 --> 00:22:20,090
tip but we can do it only when the tip

00:22:17,930 --> 00:22:22,460
is relatively small we chose the size of

00:22:20,090 --> 00:22:24,470
300 megabytes in order to keep the

00:22:22,460 --> 00:22:26,150
snapshot time minimal remember that we

00:22:24,470 --> 00:22:28,160
have to download the entire parent

00:22:26,150 --> 00:22:31,400
before we can do this process and we

00:22:28,160 --> 00:22:34,130
might not or might not need to okay

00:22:31,400 --> 00:22:36,290
during the normal width of the disk so

00:22:34,130 --> 00:22:37,000
this is an overhead and we're trying to

00:22:36,290 --> 00:22:40,070
solve

00:22:37,000 --> 00:22:42,500
Mystikal e most of the cases we've seen

00:22:40,070 --> 00:22:44,990
that usually new writes tend to be less

00:22:42,500 --> 00:22:46,390
than 300 megabytes so it's quite all

00:22:44,990 --> 00:22:51,950
right

00:22:46,390 --> 00:22:53,990
also we do it in two cases either the if

00:22:51,950 --> 00:22:56,450
the guest is live or either if the guest

00:22:53,990 --> 00:22:58,850
is offline life would use qmp block

00:22:56,450 --> 00:23:01,640
stream job command in order to merge the

00:22:58,850 --> 00:23:04,670
changes from the backing file to the tip

00:23:01,640 --> 00:23:07,130
or using qmu imagery base over the

00:23:04,670 --> 00:23:12,650
rebase target which is the grandparent

00:23:07,130 --> 00:23:15,770
file B okay so let me discuss about QM

00:23:12,650 --> 00:23:18,130
image we base command okay the problem

00:23:15,770 --> 00:23:22,220
is that currently it works in a rather

00:23:18,130 --> 00:23:24,800
peculiar way what it does is to read the

00:23:22,220 --> 00:23:27,350
sector's from the old backing file which

00:23:24,800 --> 00:23:30,260
is a the parent and the new backing file

00:23:27,350 --> 00:23:33,460
is the rebase target B and compare them

00:23:30,260 --> 00:23:36,140
byte by byte comparison okay

00:23:33,460 --> 00:23:40,250
so first of all the logic is different

00:23:36,140 --> 00:23:42,740
from qmp block stream rebase which

00:23:40,250 --> 00:23:44,990
happens completely differently but in

00:23:42,740 --> 00:23:46,790
our case and actually in all cases it

00:23:44,990 --> 00:23:50,680
requires actually to read all these

00:23:46,790 --> 00:23:53,120
sectors and for us it's very painful

00:23:50,680 --> 00:23:55,250
obviously this comparison is needed if

00:23:53,120 --> 00:23:57,050
the files are in a different chain but

00:23:55,250 --> 00:24:00,560
in our case since the file are in the

00:23:57,050 --> 00:24:03,890
same chain a very easy optimization can

00:24:00,560 --> 00:24:06,500
be made by understanding that if you can

00:24:03,890 --> 00:24:08,240
see in the drawing that we don't need to

00:24:06,500 --> 00:24:10,580
compare sectors that were not changed

00:24:08,240 --> 00:24:16,070
since the rebase target okay so in the

00:24:10,580 --> 00:24:18,110
drawing the extra right sectors that

00:24:16,070 --> 00:24:20,090
were allocated we do not need to compare

00:24:18,110 --> 00:24:22,610
that right because we just want to merge

00:24:20,090 --> 00:24:25,490
the immediate backing file with the tip

00:24:22,610 --> 00:24:27,170
so what we did is to add code to imagery

00:24:25,490 --> 00:24:27,960
base that we want to contribute to the

00:24:27,170 --> 00:24:29,850
commune

00:24:27,960 --> 00:24:32,580
we checks whether the files are in the

00:24:29,850 --> 00:24:34,859
same chain and if so we use video visa

00:24:32,580 --> 00:24:37,950
allocated above in order to understand

00:24:34,859 --> 00:24:40,649
whether the block was changed since the

00:24:37,950 --> 00:24:43,889
rebase target okay and it's it improves

00:24:40,649 --> 00:24:45,929
performance for us very significantly

00:24:43,889 --> 00:24:48,539
for very large discs a few terabytes

00:24:45,929 --> 00:24:50,609
reading all the metadata and reading all

00:24:48,539 --> 00:24:52,619
the sectors for comparison could

00:24:50,609 --> 00:24:56,070
practically mean downloading the entire

00:24:52,619 --> 00:24:59,700
disk okay it is very inefficient so this

00:24:56,070 --> 00:25:01,710
is how we solved this problem one more

00:24:59,700 --> 00:25:03,539
problem we encountered with this with

00:25:01,710 --> 00:25:06,779
our use case with our storage layer is

00:25:03,539 --> 00:25:09,450
high latency on first remote read data

00:25:06,779 --> 00:25:12,359
as I explained we need the user to see

00:25:09,450 --> 00:25:14,220
the VM starts its boot immediately we

00:25:12,359 --> 00:25:16,889
don't want to wait and there is a very

00:25:14,220 --> 00:25:19,349
high latency while fetching the the

00:25:16,889 --> 00:25:22,139
sectors needed for boot time from the

00:25:19,349 --> 00:25:24,210
object storage so it prolongs the boot

00:25:22,139 --> 00:25:26,970
time it prolongs the user application

00:25:24,210 --> 00:25:28,739
startups it could be it could be very

00:25:26,970 --> 00:25:30,599
critical and of course it gets

00:25:28,739 --> 00:25:32,309
worthwhile with the longer chains

00:25:30,599 --> 00:25:34,849
because we have to walk the chain and

00:25:32,309 --> 00:25:38,129
read the metadata as I explained before

00:25:34,849 --> 00:25:40,830
so a solution to this problem would be

00:25:38,129 --> 00:25:42,779
to prefetch the disk data while the VM

00:25:40,830 --> 00:25:44,729
is running in a different process on the

00:25:42,779 --> 00:25:46,859
same cloud VM on the same cloud instance

00:25:44,729 --> 00:25:50,159
we just start reading the disk data from

00:25:46,859 --> 00:25:52,859
the cloud one disk one and all discs in

00:25:50,159 --> 00:25:55,080
parallel sorry and we try to do it only

00:25:52,859 --> 00:25:57,509
in relatively idle times in order to not

00:25:55,080 --> 00:26:01,320
interfere with the guests normal with

00:25:57,509 --> 00:26:03,659
the VN normal operations okay so a nice

00:26:01,320 --> 00:26:06,539
solution to do that would be to just

00:26:03,659 --> 00:26:09,840
download all the backing files all the

00:26:06,539 --> 00:26:13,139
files in the chain one by one for each

00:26:09,840 --> 00:26:15,720
disk the problem is that we can buy that

00:26:13,139 --> 00:26:18,869
with a lot of redundant data see this

00:26:15,720 --> 00:26:22,679
example in this example B is the older

00:26:18,869 --> 00:26:25,889
file and a overwritten most of the all

00:26:22,679 --> 00:26:28,830
of sorry all of the changes that be made

00:26:25,889 --> 00:26:30,690
so we don't need to download file B from

00:26:28,830 --> 00:26:35,639
the object storage it could be very

00:26:30,690 --> 00:26:38,309
large and very inefficient ok so what we

00:26:35,639 --> 00:26:40,440
did to overcome that is to let Q mu do

00:26:38,309 --> 00:26:41,570
the work for us we want to fetch the

00:26:40,440 --> 00:26:44,150
data according to the

00:26:41,570 --> 00:26:47,870
disk to read only the latest data right

00:26:44,150 --> 00:26:51,700
so we use qu and BD for that we mount

00:26:47,870 --> 00:26:55,370
the the tip image and the whole chain

00:26:51,700 --> 00:26:58,970
using Q and B D and then we read the

00:26:55,370 --> 00:27:01,190
data using D D and let's Kim you to work

00:26:58,970 --> 00:27:03,950
for us we know that Kim you would fetch

00:27:01,190 --> 00:27:05,570
only the relevant data and we don't need

00:27:03,950 --> 00:27:09,250
to figure out ourselves

00:27:05,570 --> 00:27:13,580
why were the the most updated sectors

00:27:09,250 --> 00:27:16,730
right so this way we can read only the

00:27:13,580 --> 00:27:19,760
relevant data but it introduces another

00:27:16,730 --> 00:27:23,690
problem when we use that when use D D to

00:27:19,760 --> 00:27:25,820
read all the sectors the problem is that

00:27:23,690 --> 00:27:28,460
we read a lot of unallocated data right

00:27:25,820 --> 00:27:31,310
usually the virtual disk is rather empty

00:27:28,460 --> 00:27:32,540
there are many empty sectors so when we

00:27:31,310 --> 00:27:34,760
use D do that

00:27:32,540 --> 00:27:36,920
we just waste a lot of CPU cycles by

00:27:34,760 --> 00:27:39,320
reading empty data so we would like to

00:27:36,920 --> 00:27:41,390
know where are the where is the real

00:27:39,320 --> 00:27:45,020
data right which sectors are allocated

00:27:41,390 --> 00:27:47,090
and which not so we use Q new image map

00:27:45,020 --> 00:27:49,160
comment for that what it does is to

00:27:47,090 --> 00:27:51,500
return a map of all the allocated

00:27:49,160 --> 00:27:53,420
sectors and where are they located in

00:27:51,500 --> 00:27:54,680
the chain so we don't need that that I

00:27:53,420 --> 00:27:57,350
would just need to know where are the

00:27:54,680 --> 00:28:01,010
allocated sectors right now it allows us

00:27:57,350 --> 00:28:03,980
to just read the allocated sectors since

00:28:01,010 --> 00:28:07,160
we know exactly where to look this is an

00:28:03,980 --> 00:28:10,340
example of how use how to use the Q

00:28:07,160 --> 00:28:12,500
image command but there's another

00:28:10,340 --> 00:28:15,200
problem Q image map works in the whole

00:28:12,500 --> 00:28:17,690
disk as we explained before it takes a

00:28:15,200 --> 00:28:19,790
long long time to finish on longer

00:28:17,690 --> 00:28:21,770
chains because we have to read the

00:28:19,790 --> 00:28:24,500
metadata from all the backing files one

00:28:21,770 --> 00:28:26,600
by one and there is no bound to

00:28:24,500 --> 00:28:29,750
execution time right and the whole point

00:28:26,600 --> 00:28:32,780
of this prefetch was to improve the boot

00:28:29,750 --> 00:28:34,730
time to start the VM faster and we can't

00:28:32,780 --> 00:28:38,690
prefetch until we understand where are

00:28:34,730 --> 00:28:42,050
the allocated sectors so this is very

00:28:38,690 --> 00:28:45,140
problematic so what we had to do is to

00:28:42,050 --> 00:28:47,420
add more parameters to the queue much

00:28:45,140 --> 00:28:49,550
map command this is an internal work we

00:28:47,420 --> 00:28:51,470
did just add offset and length

00:28:49,550 --> 00:28:55,070
parameters so we can break down the

00:28:51,470 --> 00:28:56,810
queue image command into several parts

00:28:55,070 --> 00:29:02,270
now we can bound execution time and we

00:28:56,810 --> 00:29:05,180
can we chose one gigabyte parts so we

00:29:02,270 --> 00:29:07,340
can first map the first one gigabyte of

00:29:05,180 --> 00:29:09,830
the virtual disk understand where the

00:29:07,340 --> 00:29:13,670
allocated sectors and then read using D

00:29:09,830 --> 00:29:15,800
D and Q NB d only these factors right

00:29:13,670 --> 00:29:18,230
and if we see that this gigabyte

00:29:15,800 --> 00:29:20,900
contains only unallocated data we don't

00:29:18,230 --> 00:29:22,940
need to read it so this helps us to

00:29:20,900 --> 00:29:25,480
start prefetch data quickly and it's

00:29:22,940 --> 00:29:28,910
really improved performance in boot time

00:29:25,480 --> 00:29:34,280
for our customers so it was a good

00:29:28,910 --> 00:29:37,610
optimization so let me summarize what we

00:29:34,280 --> 00:29:39,980
do in Oracle Ravello is to we

00:29:37,610 --> 00:29:40,580
implemented our storage layer using q co

00:29:39,980 --> 00:29:43,460
2 chainz

00:29:40,580 --> 00:29:46,130
and we store the data on the public

00:29:43,460 --> 00:29:49,130
cloud object storage s3 or Google

00:29:46,130 --> 00:29:51,260
storage and so on q co 2 and Q mu

00:29:49,130 --> 00:29:53,390
implementations implementations are not

00:29:51,260 --> 00:29:55,970
that ideal for our use case

00:29:53,390 --> 00:29:58,130
first of all queue calls to format keeps

00:29:55,970 --> 00:30:00,440
only immediate parent information and

00:29:58,130 --> 00:30:03,080
not about not their whole chain

00:30:00,440 --> 00:30:05,750
information so Q mu must often often

00:30:03,080 --> 00:30:08,810
walk the chain to understand the entire

00:30:05,750 --> 00:30:10,910
structure also the metadata specially

00:30:08,810 --> 00:30:14,210
the l2 tables is spread across the file

00:30:10,910 --> 00:30:16,970
so multiple so one disk request could be

00:30:14,210 --> 00:30:20,600
translated to many remote request from

00:30:16,970 --> 00:30:22,970
the object storage and its chains are

00:30:20,600 --> 00:30:26,420
getting longer we have performance robic

00:30:22,970 --> 00:30:28,190
from walking the chain so I think what

00:30:26,420 --> 00:30:30,110
we're trying to say that we would very

00:30:28,190 --> 00:30:32,390
much like to work with the community and

00:30:30,110 --> 00:30:36,800
try to improve the performance for I use

00:30:32,390 --> 00:30:39,860
case the Q and the Q call format the Q :

00:30:36,800 --> 00:30:42,380
q code format I think often relies on

00:30:39,860 --> 00:30:45,080
the fact that the chain is all available

00:30:42,380 --> 00:30:46,880
locally on the local machine or at least

00:30:45,080 --> 00:30:50,240
that all the files are placed in the

00:30:46,880 --> 00:30:52,580
same place so this is not always the

00:30:50,240 --> 00:30:55,510
case for our use case so we would very

00:30:52,580 --> 00:30:58,910
much like to see improvement in that

00:30:55,510 --> 00:31:01,660
that's it if you have any questions I

00:30:58,910 --> 00:31:01,660
would love to answer

00:31:13,090 --> 00:31:17,980
hey thanks for the presentation you

00:31:15,879 --> 00:31:20,230
mentioned that you use qm image freebase

00:31:17,980 --> 00:31:22,629
in order to move data from a buckin file

00:31:20,230 --> 00:31:26,529
into the active file isn't it faster to

00:31:22,629 --> 00:31:28,179
or to use block stream instead are you

00:31:26,529 --> 00:31:32,399
talking about the rebase sorry

00:31:28,179 --> 00:31:35,789
yeah comparing to what block stream

00:31:32,399 --> 00:31:39,090
freebase only when the guest is offline

00:31:35,789 --> 00:31:39,090
right we use

00:31:56,100 --> 00:32:17,220
right right but what if you don't mind

00:32:14,640 --> 00:32:22,410
is done only when the VM is when the

00:32:17,220 --> 00:32:27,450
train stopped without way I'm to perform

00:32:22,410 --> 00:32:30,450
only gisquette duration desk I don't

00:32:27,450 --> 00:32:32,610
know how to do it it's not what

00:32:30,450 --> 00:32:34,830
difficult if you are working out for

00:32:32,610 --> 00:32:38,910
automation this could be done there's a

00:32:34,830 --> 00:32:42,690
several comments ok thank you thank you

00:32:38,910 --> 00:32:45,810
and speaking about Gigi and mob there is

00:32:42,690 --> 00:32:48,960
a comment who llamo I'm Gigi which is do

00:32:45,810 --> 00:32:54,780
the trick without all nasty films with

00:32:48,960 --> 00:32:57,660
mouth we wanted something really really

00:32:54,780 --> 00:33:00,810
simple it's really simple you can start

00:32:57,660 --> 00:33:03,240
who am I am Gigi which is working

00:33:00,810 --> 00:33:05,730
exactly like dee dee without and biggie

00:33:03,240 --> 00:33:13,550
stuff and busy and without zeros in the

00:33:05,730 --> 00:33:13,550
wire okay thank you thank you

00:33:14,170 --> 00:33:18,680
just start speaking okay so I was

00:33:18,320 --> 00:33:20,540
wondering

00:33:18,680 --> 00:33:22,880
so you mentioned using the stream

00:33:20,540 --> 00:33:26,210
command I was wondering did you evaluate

00:33:22,880 --> 00:33:28,730
whether the copy on read option on

00:33:26,210 --> 00:33:30,410
drives would be useful too because when

00:33:28,730 --> 00:33:32,450
stream was added it was actually added

00:33:30,410 --> 00:33:34,040
together with the copy on read and what

00:33:32,450 --> 00:33:37,460
copy and read simply means is that when

00:33:34,040 --> 00:33:39,260
the guest does the read the data is

00:33:37,460 --> 00:33:41,120
copied into the image it basically turns

00:33:39,260 --> 00:33:43,520
into a write and that way you can bring

00:33:41,120 --> 00:33:46,550
the data up from the backing files into

00:33:43,520 --> 00:33:47,960
the top level as the guest is running so

00:33:46,550 --> 00:33:50,000
that's nice if you're provisioning a new

00:33:47,960 --> 00:33:52,040
guest and you want that data to become

00:33:50,000 --> 00:33:54,110
local so it will will be high

00:33:52,040 --> 00:33:56,740
performance was that interesting or did

00:33:54,110 --> 00:34:00,500
you look into it didn't look into it but

00:33:56,740 --> 00:34:03,710
yeah you could in theory create two tips

00:34:00,500 --> 00:34:06,910
and then one use for snapshotting later

00:34:03,710 --> 00:34:13,990
and the other use for prefetching may

00:34:06,910 --> 00:34:18,560
may be to work better for us maybe i

00:34:13,990 --> 00:34:22,160
noticed you had a combination between NB

00:34:18,560 --> 00:34:25,400
d and d d and q image all working

00:34:22,160 --> 00:34:27,740
together just heads up that the NB d

00:34:25,400 --> 00:34:29,930
spec is trying to add block status so

00:34:27,740 --> 00:34:31,760
that a single NB d connection will give

00:34:29,930 --> 00:34:35,570
you status of where the holes are so

00:34:31,760 --> 00:34:37,250
that you don't need d d and map all on

00:34:35,570 --> 00:34:39,380
top of that so there are things coming

00:34:37,250 --> 00:34:41,480
down the pipeline that sounds like your

00:34:39,380 --> 00:34:43,190
use case so pay attention to the

00:34:41,480 --> 00:34:45,890
community and chime in if you have

00:34:43,190 --> 00:34:55,210
tweaks to it they've been help us thank

00:34:45,890 --> 00:34:58,660
you very much ok so we are I'm so sleepy

00:34:55,210 --> 00:34:58,660
thank you thank you

00:35:00,829 --> 00:35:07,619
you

00:35:01,310 --> 00:35:07,619

YouTube URL: https://www.youtube.com/watch?v=EMK7KVDHSNg


