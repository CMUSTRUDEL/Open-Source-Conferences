Title: [2017] Live Migration with Mediated Device by Yulei Zhang
Publication date: 2017-11-10
Playlist: KVM Forum 2017
Description: 
	Mediated device is a new feature introduced in VFIO for efficient I/O sharing in virtualization environment. For example, it’s a key enabling technology to GPU virtualization for cloud graphics, media processing and computing. However mediated device doesn’t support live migration today, which is a key gap in data center and cloud usages.
In this talk we will introduce the challenges and gaps about how to live migrate mediated device, and then elaborate the techniques by extending VFIO mdev framework to bridge the gaps. Our work is based on Intel GVT-g (a.k.a KVMGT) which is Intel’s vGPU implementation using mediated device technique. By proving the feasibility on the most complex I/O device, we believe other mediated devices can leverage same technique to enable live migration usage. A simple demo video will also be provided to demonstrate the feature.

---

Yulei Zhang
Intel

More than 10 years experienced software developer working in Intel China. Currently design and implement Intel GPU virtualization technology(a.k.a Intel GVT-g). Recent presentation was: as technique speaker presented "Enable media cloud with intel Graphics virtualization technology" at Alibaba computer conference in 2015
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:07,080 --> 00:00:13,019
good afternoon everyone my name is Billy

00:00:10,230 --> 00:00:16,080
Jo I'm a software engineer working for

00:00:13,019 --> 00:00:20,730
Intel open source Technical Center and

00:00:16,080 --> 00:00:23,430
the folks on GPU virtualization this is

00:00:20,730 --> 00:00:25,590
today's agenda into this session we

00:00:23,430 --> 00:00:28,199
would like to talk about the live

00:00:25,590 --> 00:00:30,929
migration design of middle device and

00:00:28,199 --> 00:00:36,120
also gave an update in hell beach view

00:00:30,929 --> 00:00:38,820
lab migration implementation as we all

00:00:36,120 --> 00:00:41,610
know the immediate device is a new

00:00:38,820 --> 00:00:43,800
future introduced in vfi all for

00:00:41,610 --> 00:00:45,480
efficient I all sharing in

00:00:43,800 --> 00:00:48,390
virtualization environment since the

00:00:45,480 --> 00:00:51,989
last year for example we have enabled it

00:00:48,390 --> 00:00:53,870
for GPO network adapter and other

00:00:51,989 --> 00:00:56,969
computing accelerators

00:00:53,870 --> 00:00:59,850
but however media device stands support

00:00:56,969 --> 00:01:03,690
line migration today which is a key cap

00:00:59,850 --> 00:01:05,400
in data center and crowd usage in this

00:01:03,690 --> 00:01:10,290
talk we will introduce the challenges

00:01:05,400 --> 00:01:13,350
and gaps about how to live migrate media

00:01:10,290 --> 00:01:17,100
device and as I elaborate techniques by

00:01:13,350 --> 00:01:19,830
extending Vayu media device framework to

00:01:17,100 --> 00:01:22,799
bridge the gaps in how has finished the

00:01:19,830 --> 00:01:25,890
prototype to enable them languishing on

00:01:22,799 --> 00:01:33,450
inhale GV TG which is also known as the

00:01:25,890 --> 00:01:36,210
KPMG T for the GPO virtualization also

00:01:33,450 --> 00:01:39,090
for the migration of media device there

00:01:36,210 --> 00:01:42,000
are four kinds of theatres we hope to

00:01:39,090 --> 00:01:45,149
take care of the first is the emulator

00:01:42,000 --> 00:01:47,430
virtual Mme all status and the second

00:01:45,149 --> 00:01:50,340
one is the hardware status on the media

00:01:47,430 --> 00:01:52,409
device such as graphics memories and

00:01:50,340 --> 00:01:57,090
page tables or pending working queues

00:01:52,409 --> 00:01:59,969
and the third is the pending interrupts

00:01:57,090 --> 00:02:03,570
on the immediate media to the device the

00:01:59,969 --> 00:02:05,880
last one is the dirty memory pages which

00:02:03,570 --> 00:02:08,240
is accessed by a media device through

00:02:05,880 --> 00:02:08,240
DNA

00:02:10,340 --> 00:02:15,709
so in general migration fro after my

00:02:13,940 --> 00:02:17,660
precious thought it will start copy the

00:02:15,709 --> 00:02:20,530
entire cast memory to a target at the

00:02:17,660 --> 00:02:24,380
first and as n it well to the iterative

00:02:20,530 --> 00:02:27,050
memory copy so after that after VM

00:02:24,380 --> 00:02:30,650
stopped it world with a static copy and

00:02:27,050 --> 00:02:32,930
he also migrate the device status so a

00:02:30,650 --> 00:02:36,230
common virtual device could have put it

00:02:32,930 --> 00:02:38,780
status in the fields of VM state so

00:02:36,230 --> 00:02:41,180
Camillo copy this state to photography

00:02:38,780 --> 00:02:44,870
and throw the VM status a we interface

00:02:41,180 --> 00:02:47,030
and on the tag be inside it can store

00:02:44,870 --> 00:02:52,459
this information through VMs dataload

00:02:47,030 --> 00:02:54,380
interface before resume the VN however

00:02:52,459 --> 00:02:56,480
immediately device

00:02:54,380 --> 00:03:00,230
lots of resource had been passed through

00:02:56,480 --> 00:03:03,709
so Kim you don't have this stuff which

00:03:00,230 --> 00:03:08,390
have to extend the framework of via file

00:03:03,709 --> 00:03:11,510
and dev together as the picture shows so

00:03:08,390 --> 00:03:14,420
for the migration of the device we need

00:03:11,510 --> 00:03:17,900
consider story most apt the first is

00:03:14,420 --> 00:03:20,930
about how to retrieve and the restore

00:03:17,900 --> 00:03:23,239
the media device status contacts status

00:03:20,930 --> 00:03:25,640
and the second one is control the

00:03:23,239 --> 00:03:28,760
running status of the media device to

00:03:25,640 --> 00:03:37,730
stop and the resume the last one is the

00:03:28,760 --> 00:03:41,299
dirty memory pages by leveraging the

00:03:37,730 --> 00:03:44,329
Bahama VFL framework we decided to use a

00:03:41,299 --> 00:03:46,609
new via file reading to do the media

00:03:44,329 --> 00:03:49,850
device contacts transmission we will

00:03:46,609 --> 00:03:53,660
register new VFL reading named without

00:03:49,850 --> 00:03:57,139
reading subtype device status which will

00:03:53,660 --> 00:04:00,109
be initialized that during the initial

00:03:57,139 --> 00:04:02,120
of the mediator device driver so cool

00:04:00,109 --> 00:04:04,639
cumin will be able to enable the live

00:04:02,120 --> 00:04:08,870
migration if it detect the media devices

00:04:04,639 --> 00:04:11,389
supportassist upper region and so during

00:04:08,870 --> 00:04:15,410
the migration Camille would be easier to

00:04:11,389 --> 00:04:19,700
access these sub regions through the

00:04:15,410 --> 00:04:22,450
device FD to receive and restore the

00:04:19,700 --> 00:04:22,450
device contacts

00:04:23,690 --> 00:04:31,020
and also we would like to use the same

00:04:28,860 --> 00:04:35,670
region to control the Iranian status of

00:04:31,020 --> 00:04:38,180
the media device so we would like to use

00:04:35,670 --> 00:04:40,260
the beginning of this region as the

00:04:38,180 --> 00:04:44,010
device state control field

00:04:40,260 --> 00:04:46,380
so if Kimmy will write the device stop

00:04:44,010 --> 00:04:48,450
commander - yet the when the driver will

00:04:46,380 --> 00:04:52,700
be able to stop the media device and

00:04:48,450 --> 00:04:56,340
remove it from the schedule so it can be

00:04:52,700 --> 00:04:58,740
so it can be migrated later and vice

00:04:56,340 --> 00:05:01,440
versa on the Tachyon side it

00:04:58,740 --> 00:05:04,169
Kamiya what right the device staff

00:05:01,440 --> 00:05:06,570
commander to this field to adapt added

00:05:04,169 --> 00:05:09,810
device back to the scheduler and the

00:05:06,570 --> 00:05:19,650
kickoff the execution after the VM

00:05:09,810 --> 00:05:22,320
resumed as part of the device resume

00:05:19,650 --> 00:05:25,080
places when important thing is to

00:05:22,320 --> 00:05:28,290
reconstruct the PC configuration region

00:05:25,080 --> 00:05:32,310
the sauce be inside this config region

00:05:28,290 --> 00:05:35,700
is initialized by guest itself during

00:05:32,310 --> 00:05:37,979
its boot up but on the top the VM

00:05:35,700 --> 00:05:41,040
cytochrome you have to go through the

00:05:37,979 --> 00:05:43,350
same piece accomplishing reading to

00:05:41,040 --> 00:05:46,200
reconstruct the device the media device

00:05:43,350 --> 00:05:49,860
resource mapping and reboot II were to

00:05:46,200 --> 00:05:52,890
interrupt the injection paths so that

00:05:49,860 --> 00:05:55,200
the media device driver would be able to

00:05:52,890 --> 00:05:58,560
inject the virtual table into Guster's

00:05:55,200 --> 00:06:00,979
through event FD after the toggle eme

00:05:58,560 --> 00:06:00,979
resumed

00:06:07,130 --> 00:06:12,680
the last one is talking about the dirty

00:06:10,070 --> 00:06:15,200
memory pages how do we handle the dirty

00:06:12,680 --> 00:06:17,900
memory pages so far we know there are

00:06:15,200 --> 00:06:19,610
two parts to do it one is led to when

00:06:17,900 --> 00:06:22,100
the driver to report the dirty bit

00:06:19,610 --> 00:06:24,680
matter to Kumu and the other way is to

00:06:22,100 --> 00:06:27,920
query the memory mapping from the via

00:06:24,680 --> 00:06:37,040
file I'm a moot driver to be able to

00:06:27,920 --> 00:06:39,890
dirty pimp as well this is as we know

00:06:37,040 --> 00:06:43,250
that in media to device driver it will

00:06:39,890 --> 00:06:47,000
create the shadow page table for the its

00:06:43,250 --> 00:06:49,250
DMA access so when the driver called

00:06:47,000 --> 00:06:51,410
attracts this guest friend numbers and

00:06:49,250 --> 00:06:54,200
use them to build the European map of a

00:06:51,410 --> 00:06:58,030
Kumu to transfer the dirty the memory

00:06:54,200 --> 00:06:58,030
pages during the migration

00:07:06,090 --> 00:07:11,460
and the only other the other way is D in

00:07:09,389 --> 00:07:13,770
the verifiable framework the vendor

00:07:11,460 --> 00:07:17,400
driver will ping the memory in rent on

00:07:13,770 --> 00:07:19,740
through the VFR our new driver so this

00:07:17,400 --> 00:07:22,230
mapping is also tracked by VFL Amemiya

00:07:19,740 --> 00:07:23,990
driver during the migration camille

00:07:22,230 --> 00:07:26,940
could also parish this mapping from

00:07:23,990 --> 00:07:28,830
mohamed driver and build up the tip and

00:07:26,940 --> 00:07:32,990
map for commute will transfer this

00:07:28,830 --> 00:07:32,990
memory used by a mediator device

00:07:43,640 --> 00:07:48,980
and this is the migration policy for

00:07:46,100 --> 00:07:52,910
individual resources the first one is

00:07:48,980 --> 00:07:55,570
the virtual mammalogists we just copy

00:07:52,910 --> 00:07:58,490
and restore the entire memo reading

00:07:55,570 --> 00:08:01,550
through the sub regime the VFL the new

00:07:58,490 --> 00:08:05,390
BFF super aging and the second part is

00:08:01,550 --> 00:08:08,150
about the virtual interrupts in vgpu so

00:08:05,390 --> 00:08:11,020
on the talk vm side that we were able to

00:08:08,150 --> 00:08:16,540
inject the pending interrupt

00:08:11,020 --> 00:08:20,600
after the VN resume the soul part is the

00:08:16,540 --> 00:08:22,250
graphic demon memory just as we

00:08:20,600 --> 00:08:25,070
mentioned in the last slice we would

00:08:22,250 --> 00:08:27,440
like to use the second one like the

00:08:25,070 --> 00:08:30,740
letter Q to query the pilgrim mapping

00:08:27,440 --> 00:08:33,470
from the aluminum jar and the transfers

00:08:30,740 --> 00:08:37,490
and during the micro regime the last

00:08:33,470 --> 00:08:41,000
part is the GU contacts like the render

00:08:37,490 --> 00:08:43,789
engine contacts and graphic page tables

00:08:41,000 --> 00:08:47,890
so this part we will recreate the

00:08:43,789 --> 00:08:51,490
shadowing in the on the talkative inside

00:08:47,890 --> 00:08:51,490
after VM resume

00:08:57,980 --> 00:09:04,970
this is the current vici people

00:09:01,040 --> 00:09:08,540
languishing fro so on the VM size sauce

00:09:04,970 --> 00:09:12,910
VM side during the migration after vici

00:09:08,540 --> 00:09:17,590
fuel stops the kimi o well right the

00:09:12,910 --> 00:09:20,720
device stop command to the new averaging

00:09:17,590 --> 00:09:24,320
devices status field to stop the video

00:09:20,720 --> 00:09:27,080
and remove it and the letter device

00:09:24,320 --> 00:09:30,050
driver removed from the scheduler and

00:09:27,080 --> 00:09:33,500
then the cumin would query the platypus

00:09:30,050 --> 00:09:36,680
map from the aluminum driver and the

00:09:33,500 --> 00:09:39,620
right the map to the community list for

00:09:36,680 --> 00:09:44,330
memory synchronization at the last it

00:09:39,620 --> 00:09:47,180
her which the QL retrieved the vgpu memo

00:09:44,330 --> 00:09:50,510
or other contacts from the sub region

00:09:47,180 --> 00:09:54,320
and copied through the chemical fire or

00:09:50,510 --> 00:09:57,070
tcp/ip and other protocol so on the taça

00:09:54,320 --> 00:10:00,560
vm side after receive this information

00:09:57,070 --> 00:10:04,270
it will at first to reconstruct the VDP

00:10:00,560 --> 00:10:08,000
OPC configuration space to rebuild the

00:10:04,270 --> 00:10:11,750
resource mapping and set up the virtual

00:10:08,000 --> 00:10:15,740
interrupt the injection parts and a

00:10:11,750 --> 00:10:18,800
second it will restore the MMO and other

00:10:15,740 --> 00:10:23,150
contacts through the sub region by right

00:10:18,800 --> 00:10:27,500
this information to the vendor driver

00:10:23,150 --> 00:10:29,840
and then after the witch peel resume the

00:10:27,500 --> 00:10:33,140
Camille will write the devices start

00:10:29,840 --> 00:10:36,050
command to the device estate field in

00:10:33,140 --> 00:10:38,990
the server region then the device driver

00:10:36,050 --> 00:10:41,030
will be able to put the which view into

00:10:38,990 --> 00:10:43,550
scheduler and the kick off the execution

00:10:41,030 --> 00:10:47,260
then the vita people will be able to

00:10:43,550 --> 00:10:47,260
resume after the migration

00:10:52,560 --> 00:10:59,200
this is the current status of our vgpu

00:10:56,070 --> 00:11:01,870
Lima watching implementation so Intel

00:10:59,200 --> 00:11:05,460
has finished the experimental vitriolic

00:11:01,870 --> 00:11:08,050
rating support for both KVM and the Zen

00:11:05,460 --> 00:11:10,660
current rating we own hardware platform

00:11:08,050 --> 00:11:13,750
we support Intel sixth and the seventh

00:11:10,660 --> 00:11:16,000
generation Core processors and we also

00:11:13,750 --> 00:11:20,140
test the several benchmarks for the

00:11:16,000 --> 00:11:22,720
windows cast we have tested the heavens

00:11:20,140 --> 00:11:25,780
really mark topic and the media decoder

00:11:22,720 --> 00:11:28,900
encoder workload on the Linux VM we also

00:11:25,780 --> 00:11:31,390
test who is really like life's mark or

00:11:28,900 --> 00:11:40,060
the 2d bench markers and other media

00:11:31,390 --> 00:11:43,030
media media workloads and we also bring

00:11:40,060 --> 00:11:45,430
a demo video about the individual our

00:11:43,030 --> 00:11:48,040
magazine with this really workload this

00:11:45,430 --> 00:11:51,870
is the hardware setup so we have to

00:11:48,040 --> 00:11:55,300
inhale 6th generation Core processor

00:11:51,870 --> 00:12:00,580
platform setup together we create the

00:11:55,300 --> 00:12:04,030
weakest VM with to beat CPUs and it has

00:12:00,580 --> 00:12:07,720
2 Giga system memory and the 512

00:12:04,030 --> 00:12:10,050
megabyte graphics memory the physical

00:12:07,720 --> 00:12:14,730
machine will connect together with

00:12:10,050 --> 00:12:17,830
through 10 Giga bps network adapter and

00:12:14,730 --> 00:12:19,860
after measuring the migration in the

00:12:17,830 --> 00:12:22,660
service downtown is less than 5

00:12:19,860 --> 00:12:24,790
milliseconds 500 milliseconds and a

00:12:22,660 --> 00:12:28,920
total emigration is about two and a half

00:12:24,790 --> 00:12:28,920
to three seconds

00:12:32,030 --> 00:12:42,850
okay this is the video for the lamb

00:12:36,620 --> 00:12:45,890
publishing on Intel vgpu so we use

00:12:42,850 --> 00:12:50,960
Winton cast which running also raining

00:12:45,890 --> 00:12:54,140
the tropics on yet it were migration

00:12:50,960 --> 00:12:57,850
from the machinery to machine B and vice

00:12:54,140 --> 00:12:57,850
versa for several tons

00:13:32,589 --> 00:13:36,069
and that's all

00:13:36,540 --> 00:13:43,590
and the last we would like to talk about

00:13:39,390 --> 00:13:44,850
the future works the FC for the VG

00:13:43,590 --> 00:13:49,020
people are my whooshing has been send

00:13:44,850 --> 00:13:50,730
out and we try to upstream the continent

00:13:49,020 --> 00:13:54,360
implementation for the VG people

00:13:50,730 --> 00:13:56,970
migration and also we want to leverage

00:13:54,360 --> 00:14:04,950
the current work to support the pastor

00:13:56,970 --> 00:14:07,920
or SLV device migration later this is

00:14:04,950 --> 00:14:14,160
the link to our project website you can

00:14:07,920 --> 00:14:26,040
scan to the magic dimensional code to

00:14:14,160 --> 00:14:30,410
get access to our website I think that's

00:14:26,040 --> 00:14:30,410
all for my presentation is that no Cushi

00:14:39,699 --> 00:14:50,019
I see it is but it's used for the beauty

00:14:46,720 --> 00:14:53,439
I think it's used evil GPO DMA access

00:14:50,019 --> 00:14:56,129
right so we think this memory is totally

00:14:53,439 --> 00:14:59,129
by the GPO so we have to copy it to the

00:14:56,129 --> 00:14:59,129
targeted

00:15:17,320 --> 00:15:21,660
why are they sending

00:15:28,670 --> 00:15:35,399
your solutions you have a two ways wines

00:15:31,439 --> 00:15:37,379
to reported 30 pages report dedicated

00:15:35,399 --> 00:15:40,529
yourself in other ways to query the

00:15:37,379 --> 00:15:42,180
query the island view right so yeah but

00:15:40,529 --> 00:15:44,399
I think the second solution might not

00:15:42,180 --> 00:15:46,879
work because when you have another

00:15:44,399 --> 00:15:50,009
pass-through device inside the VM

00:15:46,879 --> 00:15:53,399
because that will make all the Marika's

00:15:50,009 --> 00:15:55,920
pinda I mean we have another pass

00:15:53,399 --> 00:15:57,810
through devices in the VM yes so I think

00:15:55,920 --> 00:16:00,329
in this case we cannot do the migration

00:15:57,810 --> 00:16:03,060
right so exactly we have some data

00:16:00,329 --> 00:16:05,819
driver for this pass-through device so

00:16:03,060 --> 00:16:10,019
if and if this two device are in the

00:16:05,819 --> 00:16:13,410
same and our a new memory group so they

00:16:10,019 --> 00:16:17,129
will also use this maybe also uses ping

00:16:13,410 --> 00:16:19,170
message method to clean a DMM buffers so

00:16:17,129 --> 00:16:22,170
we all can copy these buffers to the

00:16:19,170 --> 00:16:24,810
target even as well another thing

00:16:22,170 --> 00:16:29,670
another question is you said you have

00:16:24,810 --> 00:16:35,730
500 is a 500 megabyte of frame buffer

00:16:29,670 --> 00:16:38,370
right so tell from a provider for

00:16:35,730 --> 00:16:41,490
graphics memory maths I'll assume a lot

00:16:38,370 --> 00:16:43,439
of entire Falcone's megabyte a graphic

00:16:41,490 --> 00:16:46,379
memory will be marked as dirty when you

00:16:43,439 --> 00:16:49,439
do the stop and coffee right that's

00:16:46,379 --> 00:16:52,680
about that is a you copied it through

00:16:49,439 --> 00:16:57,629
the stop coffee face you mean we will

00:16:52,680 --> 00:16:59,160
copy the 520 megabytes yeah yeah it

00:16:57,629 --> 00:17:01,740
depends on the workload I think I'm

00:16:59,160 --> 00:17:04,169
which may not be may not need to copy

00:17:01,740 --> 00:17:06,720
the entire graphics memory right oh just

00:17:04,169 --> 00:17:09,059
copy the memories at the GPO walk loader

00:17:06,720 --> 00:17:11,610
is using ok so you have a internally you

00:17:09,059 --> 00:17:13,829
help you know which part of the frame

00:17:11,610 --> 00:17:18,350
buffer is pinned on are pinda right uh

00:17:13,829 --> 00:17:18,350
yeah ok good good thank you

00:17:32,550 --> 00:17:40,090
then transfer the pages of the dirty

00:17:36,820 --> 00:17:44,220
have you tried integrating it into the

00:17:40,090 --> 00:17:49,390
iterative mechanism so that you transfer

00:17:44,220 --> 00:17:51,040
most of the pages before you stop so you

00:17:49,390 --> 00:17:54,610
are talking about the iterative

00:17:51,040 --> 00:17:56,850
copyright 40 also use the intuitive cop

00:17:54,610 --> 00:18:00,310
logic for the graphics memory right yeah

00:17:56,850 --> 00:18:02,890
that's I see we are trying some method

00:18:00,310 --> 00:18:06,280
for that because currently we do not

00:18:02,890 --> 00:18:10,570
have the rat rat or protector for the

00:18:06,280 --> 00:18:12,880
DMA pages so we may not use the common

00:18:10,570 --> 00:18:15,430
way to write protect the memory ended

00:18:12,880 --> 00:18:17,860
with the iterative copy so we have some

00:18:15,430 --> 00:18:20,200
alternative way to do so so since these

00:18:17,860 --> 00:18:22,300
purchases - under testing so it

00:18:20,200 --> 00:18:26,640
definitely will shut the system down

00:18:22,300 --> 00:18:26,640
time yeah

00:18:29,750 --> 00:18:33,310
not yes all that not here

00:18:44,070 --> 00:18:50,450
so thank you

00:18:46,640 --> 00:18:56,700
[Applause]

00:18:50,450 --> 00:18:56,700

YouTube URL: https://www.youtube.com/watch?v=ZAzv0c-fdAc


