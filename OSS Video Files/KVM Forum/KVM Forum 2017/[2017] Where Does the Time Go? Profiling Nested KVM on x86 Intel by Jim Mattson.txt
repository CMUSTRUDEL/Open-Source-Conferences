Title: [2017] Where Does the Time Go? Profiling Nested KVM on x86 Intel by Jim Mattson
Publication date: 2017-11-09
Playlist: KVM Forum 2017
Description: 
	Nested x86 virtualization under kvm is quite slow. In part, this is because nested virtualization is just slow by its very nature, and there is very little in the way of hardware acceleration to help improve its performance. However, part of the problem is that development has been (rightly) focused on correctness rather than performance. Now that nested x86 virtualization in kvm is a bit more mature, it is time to start looking for the low-hanging fruit to improve performance.

We will begin by comparing timings of primitive VMX operations under kvm and under VMware Player. Next, we will look at some macro benchmarks, and we will compare the number of VM-exits with kvm under kvm to the number of VM-exits with kvm under VMware player. Finally, we will look at runtime profiling information to examine the nested virtualization overheads of running kvm under kvm.

With apologies to AMD, this talk covers only the Intel x86 virtualization extensions.

---

Jim Mattson
Google
Software Engineer

Jim Mattson is a software engineer working on x86 virtualization for Google Cloud Platform. One of his current projects is to drive nested virtualization from beta to GA. Before joining Google, he worked on the virtual machine monitor at VMware, code-morphing at Transmeta, and compiler optimizations at HP. He also contributed to the Glasgow Haskell Compiler as a post-doc. Jim has a PhD in Computer Science from UCSD and a BA from Princeton University.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:06,259 --> 00:00:13,200
hi everybody my name is Jim Madsen I

00:00:10,290 --> 00:00:14,910
work for Google but I'm not here today

00:00:13,200 --> 00:00:16,560
to talk about something that I did or

00:00:14,910 --> 00:00:18,449
something that my team did I'm here to

00:00:16,560 --> 00:00:20,340
talk about something that KVM community

00:00:18,449 --> 00:00:24,050
has been working on for a number of

00:00:20,340 --> 00:00:26,880
years which is x86 nested virtualization

00:00:24,050 --> 00:00:29,760
only on Intel I'm sorry if there any AMD

00:00:26,880 --> 00:00:32,520
people in the audience I only have

00:00:29,760 --> 00:00:35,010
access to Intel machines really and so

00:00:32,520 --> 00:00:41,820
that's why I focused on everyone knows

00:00:35,010 --> 00:00:47,489
that x86 oops x86 virtualization nested

00:00:41,820 --> 00:00:51,090
virtualization is slow but the question

00:00:47,489 --> 00:00:53,460
is why is it slow and I think the answer

00:00:51,090 --> 00:00:56,129
is that no one has really looked at

00:00:53,460 --> 00:00:58,129
performance we've been so concerned with

00:00:56,129 --> 00:01:00,690
trying to get the correctness right and

00:00:58,129 --> 00:01:05,129
at this point in time correctness is

00:01:00,690 --> 00:01:08,689
pretty much there but the performance

00:01:05,129 --> 00:01:11,070
really isn't where it should be and I

00:01:08,689 --> 00:01:12,689
think there's a lot of low-hanging fruit

00:01:11,070 --> 00:01:14,430
and I hope to demonstrate in this talk

00:01:12,689 --> 00:01:17,490
that there is a lot of low-hanging fruit

00:01:14,430 --> 00:01:19,770
and I hope to motivate you all to go out

00:01:17,490 --> 00:01:21,720
and find some of it and pick it and

00:01:19,770 --> 00:01:26,009
reduce the performance penalties that

00:01:21,720 --> 00:01:27,869
we're seeing to that extent I think my

00:01:26,009 --> 00:01:30,030
talk has already been successful because

00:01:27,869 --> 00:01:33,030
as soon as I proposed it

00:01:30,030 --> 00:01:36,420
Paulo went out and found a very

00:01:33,030 --> 00:01:39,240
egregious function that he basically cut

00:01:36,420 --> 00:01:42,540
the overhead in half on with with one

00:01:39,240 --> 00:01:44,509
commit so already it's it's starting to

00:01:42,540 --> 00:01:47,670
pay off

00:01:44,509 --> 00:01:50,159
I'm going to talk about specifically

00:01:47,670 --> 00:01:53,970
some performance profiling that I've

00:01:50,159 --> 00:01:56,420
done on a Broadwell machine or I think

00:01:53,970 --> 00:01:58,890
that's fifth generation Intel Core

00:01:56,420 --> 00:02:03,659
processor for those of you who are

00:01:58,890 --> 00:02:06,240
keeping track by marketing names the the

00:02:03,659 --> 00:02:08,190
processor has plenty of threads plenty

00:02:06,240 --> 00:02:10,920
of RAM there's no over commit or

00:02:08,190 --> 00:02:14,880
anything at any level of

00:02:10,920 --> 00:02:20,310
the test the distribution I used was

00:02:14,880 --> 00:02:24,120
Debian 8 that's in the VM and the nested

00:02:20,310 --> 00:02:26,340
VM so that's L 1 and L 2 for those of

00:02:24,120 --> 00:02:29,190
you not familiar with the nested

00:02:26,340 --> 00:02:31,770
terminology at L 0 the software running

00:02:29,190 --> 00:02:35,720
on the host there isn't really a distro

00:02:31,770 --> 00:02:38,490
it's it's the Google you know internal

00:02:35,720 --> 00:02:41,070
distribution but I did run the same

00:02:38,490 --> 00:02:44,550
kernel in every level not the kernel we

00:02:41,070 --> 00:02:47,490
run in production at Google but for 13/4

00:02:44,550 --> 00:02:49,230
which is the at least when I did the

00:02:47,490 --> 00:02:52,680
test it was the latest stable release

00:02:49,230 --> 00:02:55,290
and I did cherry pick that one commits

00:02:52,680 --> 00:02:58,500
from Paulo that you know greatly

00:02:55,290 --> 00:03:01,260
impacted nested VM performance because I

00:02:58,500 --> 00:03:02,730
mean he's already done that so I thought

00:03:01,260 --> 00:03:04,110
I'd include that as well even and it's

00:03:02,730 --> 00:03:08,250
in all the kernels even though it only

00:03:04,110 --> 00:03:10,260
makes a difference at l0 you probably

00:03:08,250 --> 00:03:13,800
all know that we don't use kimu at

00:03:10,260 --> 00:03:18,269
Google but I use key mu for these

00:03:13,800 --> 00:03:22,310
experiments key mu to 10 zero and to

00:03:18,269 --> 00:03:26,400
compare I grabbed VMware Workstation 14

00:03:22,310 --> 00:03:30,450
which was just recently released VMware

00:03:26,400 --> 00:03:33,239
has supported nested virtualization in

00:03:30,450 --> 00:03:35,250
workstation for five years I think

00:03:33,239 --> 00:03:38,519
VMware Workstation 9 was the first

00:03:35,250 --> 00:03:41,700
release to officially support nested

00:03:38,519 --> 00:03:45,450
virtualization so this will give us an

00:03:41,700 --> 00:03:47,489
idea of how we're doing in comparison

00:03:45,450 --> 00:03:50,820
with our competitors who have a bit of a

00:03:47,489 --> 00:03:54,269
leg up on us I do have to note that I

00:03:50,820 --> 00:03:57,330
had to patch workstation 14 to actually

00:03:54,269 --> 00:03:59,269
work on before 13 for kernel because

00:03:57,330 --> 00:04:02,570
they're usually a little bit behind and

00:03:59,269 --> 00:04:04,440
as usual they were a little bit behind

00:04:02,570 --> 00:04:10,010
so the first thing I'm going to talk

00:04:04,440 --> 00:04:13,380
about is a micro benchmark totally bogus

00:04:10,010 --> 00:04:16,650
concocted benchmark that is aimed to

00:04:13,380 --> 00:04:23,139
make nested virtualization look as bad

00:04:16,650 --> 00:04:26,860
as possible and this micro benchmark is

00:04:23,139 --> 00:04:28,599
cpuid in a loop it's the tightest loop I

00:04:26,860 --> 00:04:32,379
can make it it's for instructions long

00:04:28,599 --> 00:04:35,919
and I run this CPUID instruction in the

00:04:32,379 --> 00:04:38,069
loop for 30 seconds then it's terminated

00:04:35,919 --> 00:04:41,710
by a single arm and I see well how many

00:04:38,069 --> 00:04:45,909
iterations did I get through and what's

00:04:41,710 --> 00:04:49,150
the cycle count per iteration the test

00:04:45,909 --> 00:04:50,830
environment got described here LLVM in

00:04:49,150 --> 00:04:56,279
this case is not a compiler it's the

00:04:50,830 --> 00:04:58,930
lowest level VM so when I do my l1

00:04:56,279 --> 00:05:01,300
benchmarking I'm talking about the

00:04:58,930 --> 00:05:03,759
lowest level VM being l1 and then when I

00:05:01,300 --> 00:05:06,610
do l to the lowest level VM is l2 so I

00:05:03,759 --> 00:05:10,000
give the same amount of memory and the

00:05:06,610 --> 00:05:15,219
same count of CPUs at the terminal level

00:05:10,000 --> 00:05:17,650
of the test so if I am running an l2 VM

00:05:15,219 --> 00:05:19,210
then the parent has to be a little bit

00:05:17,650 --> 00:05:22,090
bigger so that there's no contention for

00:05:19,210 --> 00:05:25,659
resources you might wonder why did I use

00:05:22,090 --> 00:05:29,020
15 V CPUs and 32 gigabytes of RAM to do

00:05:25,659 --> 00:05:31,000
this cpuid test well you'll see I use

00:05:29,020 --> 00:05:32,560
the same thing for some other benchmarks

00:05:31,000 --> 00:05:35,949
later on and it was just easier to have

00:05:32,560 --> 00:05:39,629
one configuration so how do we think how

00:05:35,949 --> 00:05:44,250
do you think we do on the cpu ideal ooh

00:05:39,629 --> 00:05:53,099
well it's not great

00:05:44,250 --> 00:05:53,099
l2 KVM key mu is 25 times slower than l1

00:05:53,789 --> 00:06:02,050
not too good and vmware workstation

00:05:58,139 --> 00:06:05,830
clocks in the l2 is only 11 times slower

00:06:02,050 --> 00:06:09,520
than l1 so i think there's definitely

00:06:05,830 --> 00:06:12,759
some room for improvement in our nested

00:06:09,520 --> 00:06:15,879
virtualization strategy you'll also note

00:06:12,759 --> 00:06:19,419
VMware Workstation kind of edges out KVM

00:06:15,879 --> 00:06:20,800
and Kamui l1 just by a little bit and so

00:06:19,419 --> 00:06:22,750
maybe there's a little bit of room for

00:06:20,800 --> 00:06:26,860
improvement there but I didn't look at

00:06:22,750 --> 00:06:29,650
that level at all those of you who were

00:06:26,860 --> 00:06:33,459
in the arm nested virtualization talk

00:06:29,650 --> 00:06:35,430
earlier today will may recall that he

00:06:33,459 --> 00:06:38,940
talked about

00:06:35,430 --> 00:06:41,760
the problem of l1 kind of inducing a

00:06:38,940 --> 00:06:48,180
bunch of VM exits as its responding to

00:06:41,760 --> 00:06:50,190
an l2 exit that is that that is caused

00:06:48,180 --> 00:06:52,650
by l2 and so that's one of the first

00:06:50,190 --> 00:06:55,350
things I looked at what I'm calling

00:06:52,650 --> 00:06:59,370
ancillary l1 vm exit so these are VM

00:06:55,350 --> 00:07:02,630
exits that l1 causes because it's

00:06:59,370 --> 00:07:06,930
processing an exit that was forwarded to

00:07:02,630 --> 00:07:11,280
it from l0 you know because L to had it

00:07:06,930 --> 00:07:14,940
had to be AM exit and first thing we

00:07:11,280 --> 00:07:18,570
looked at vm resume so vm resume is l1

00:07:14,940 --> 00:07:22,080
saying okay I want to go back into the

00:07:18,570 --> 00:07:24,930
l2 execution context and you kind of

00:07:22,080 --> 00:07:28,740
figure unless you get very very clever

00:07:24,930 --> 00:07:34,830
you're gonna have one VM resume exit for

00:07:28,740 --> 00:07:36,449
every cpuid exit that is done by l2 and

00:07:34,830 --> 00:07:39,780
I'm gonna argue that that's probably all

00:07:36,449 --> 00:07:42,840
you need is that one VM resume exit but

00:07:39,780 --> 00:07:47,099
we see that with KVM and key mu we have

00:07:42,840 --> 00:07:49,800
two VM read exits one VM write exit one

00:07:47,099 --> 00:07:56,120
X set B V exit one read em sorry exit

00:07:49,800 --> 00:07:58,320
all for that one cpuid exit done by l2

00:07:56,120 --> 00:08:00,690
I'm not gonna worry so much about what

00:07:58,320 --> 00:08:02,570
VMware does but you know they do a

00:08:00,690 --> 00:08:06,750
little bit better

00:08:02,570 --> 00:08:10,289
the X set BV exit I kind of think might

00:08:06,750 --> 00:08:12,960
be a leftover from the days when Igor

00:08:10,289 --> 00:08:14,310
FPU could be false I'm not sure I

00:08:12,960 --> 00:08:18,449
haven't really looked into it in detail

00:08:14,310 --> 00:08:22,800
but you know this is setting up xer0

00:08:18,449 --> 00:08:25,949
for the l2 guest and it seems like that

00:08:22,800 --> 00:08:30,900
context ought to be l2s context as long

00:08:25,949 --> 00:08:36,630
as you're kind of in the in the DCP you

00:08:30,900 --> 00:08:38,089
run loop within within l1 but we can

00:08:36,630 --> 00:08:40,919
look a little closer at some of these

00:08:38,089 --> 00:08:44,080
exists the VM reading VM write and read

00:08:40,919 --> 00:08:46,690
MSR exits I'll point out that

00:08:44,080 --> 00:08:49,900
the processor is using the Broadwell cpu

00:08:46,690 --> 00:08:55,230
does have support for VM CS shadowing

00:08:49,900 --> 00:08:58,930
and the MCS shadowing is a it's a vmx

00:08:55,230 --> 00:09:02,890
capability that Intel introduced to help

00:08:58,930 --> 00:09:05,560
nested virtualization so that L one can

00:09:02,890 --> 00:09:10,540
perform VM read and VM write operations

00:09:05,560 --> 00:09:13,990
without causing a VM exit so if it can

00:09:10,540 --> 00:09:18,120
do that why didn't it and you'll see on

00:09:13,990 --> 00:09:23,710
the next slide that the problem was that

00:09:18,120 --> 00:09:27,550
L 0 can confuse myself actually talking

00:09:23,710 --> 00:09:29,920
about all these different levels l 0 has

00:09:27,550 --> 00:09:34,890
a list of fields that it's going to

00:09:29,920 --> 00:09:37,240
allow L 1 to VM read or to VM write and

00:09:34,890 --> 00:09:41,100
some of these fields that are frequently

00:09:37,240 --> 00:09:45,070
accessed by L 1 just aren't on that list

00:09:41,100 --> 00:09:47,350
PML index and interrupt status basically

00:09:45,070 --> 00:09:50,770
are the cause of most of the VM reads

00:09:47,350 --> 00:09:53,740
the other two here pretty much

00:09:50,770 --> 00:09:56,470
inconsequential and VM right it

00:09:53,740 --> 00:10:01,860
basically looks like we write the vm x

00:09:56,470 --> 00:10:03,940
preemption timer on every round trip so

00:10:01,860 --> 00:10:06,430
there's another one that probably should

00:10:03,940 --> 00:10:10,410
be in in that list and isn't and for the

00:10:06,430 --> 00:10:16,150
read MSR it's kind of depressing we read

00:10:10,410 --> 00:10:19,660
ia32 debug control every time we handle

00:10:16,150 --> 00:10:22,870
a nested vm exit and i don't know about

00:10:19,660 --> 00:10:25,890
you but i figure out in my machine debug

00:10:22,870 --> 00:10:28,500
control is 0 100 percent of the time so

00:10:25,890 --> 00:10:31,740
maybe I could do something about that

00:10:28,500 --> 00:10:31,740
particular problem

00:10:31,840 --> 00:10:35,250
so that's it yes

00:10:36,650 --> 00:10:39,740
[Music]

00:10:41,390 --> 00:10:46,410
yeah yeah I'll get to that when I get to

00:10:43,860 --> 00:10:48,390
my lessons learned at the end but thanks

00:10:46,410 --> 00:10:50,160
thank you for pointing that out yes as

00:10:48,390 --> 00:10:52,920
Paulo was saying the the VM read in

00:10:50,160 --> 00:10:57,260
being right fields this has to do with

00:10:52,920 --> 00:11:00,090
the list being generated a while ago and

00:10:57,260 --> 00:11:03,480
new features coming along and no one

00:11:00,090 --> 00:11:04,740
really monitoring what had to be done

00:11:03,480 --> 00:11:08,970
with that list to handle the new

00:11:04,740 --> 00:11:10,980
features so I did a few macro benchmarks

00:11:08,970 --> 00:11:13,970
as well I didn't you know I didn't do

00:11:10,980 --> 00:11:17,660
any real industry standard benchmarks

00:11:13,970 --> 00:11:21,150
but I I tried to pick a few different

00:11:17,660 --> 00:11:24,810
interesting cases that would exercise

00:11:21,150 --> 00:11:29,300
you know different subsystems so the

00:11:24,810 --> 00:11:33,240
first one I did was kernel compile and

00:11:29,300 --> 00:11:37,350
this was done with the same setup as CPU

00:11:33,240 --> 00:11:39,450
ID so at the lowest level VM I had 15

00:11:37,350 --> 00:11:42,480
CPUs 32 gigabytes of memory

00:11:39,450 --> 00:11:44,520
I took 80% of that memory and made a ram

00:11:42,480 --> 00:11:46,110
disk and then I put the files for

00:11:44,520 --> 00:11:48,870
building the kernel on the RAM disk and

00:11:46,110 --> 00:11:52,050
then I timed how long it took to compile

00:11:48,870 --> 00:11:55,320
the kernel and all the modules now it's

00:11:52,050 --> 00:11:58,290
uh it's actually quite promising I think

00:11:55,320 --> 00:12:03,810
to see that in this case the nested VM

00:11:58,290 --> 00:12:06,810
on KBM kemu was only 4% slower than the

00:12:03,810 --> 00:12:09,150
non nested vm this is primarily a CPU

00:12:06,810 --> 00:12:12,750
bound benchmark so there probably aren't

00:12:09,150 --> 00:12:15,390
a whole lot of VM exits and nested

00:12:12,750 --> 00:12:20,250
virtualization actually does really well

00:12:15,390 --> 00:12:22,740
here now despite doing you know pretty

00:12:20,250 --> 00:12:26,220
much blowing KVM out of the out of the

00:12:22,740 --> 00:12:30,240
water on the little micro cpuid

00:12:26,220 --> 00:12:36,110
benchmark workstation fell behind here

00:12:30,240 --> 00:12:39,420
and it was 7% slower than its l1 and I

00:12:36,110 --> 00:12:43,380
looked into it when you when you get the

00:12:39,420 --> 00:12:44,910
slides in PDF after the conference

00:12:43,380 --> 00:12:47,460
you'll see in the backup slides I've got

00:12:44,910 --> 00:12:50,730
a big table of the VM exits they were

00:12:47,460 --> 00:12:55,140
taken by by each hyper

00:12:50,730 --> 00:12:57,300
earning these benchmarks it I would show

00:12:55,140 --> 00:13:00,540
it but it it's too small for anyone to

00:12:57,300 --> 00:13:03,930
read but when you look into it looks

00:13:00,540 --> 00:13:08,820
like essentially workstation is paying

00:13:03,930 --> 00:13:11,790
for a few things one is there's no VMM

00:13:08,820 --> 00:13:14,340
exclusive for workstations there's VM

00:13:11,790 --> 00:13:17,310
gimmicks on BMX off the em pointer load

00:13:14,340 --> 00:13:19,380
BM clear all these things are causing vm

00:13:17,310 --> 00:13:23,900
exits these are new sort of ancillary

00:13:19,380 --> 00:13:28,680
exits every time that the vmware

00:13:23,900 --> 00:13:32,280
equivalent of DCP you run that i octal

00:13:28,680 --> 00:13:36,050
exits back to user space and they also

00:13:32,280 --> 00:13:38,910
seem to pay quite a bit for not actually

00:13:36,050 --> 00:13:40,710
living in coexistence with the kernel

00:13:38,910 --> 00:13:45,420
right so they have a virtual machine

00:13:40,710 --> 00:13:48,450
monitor that runs in ring 0 on its own

00:13:45,420 --> 00:13:50,490
page tables with its own interrupted

00:13:48,450 --> 00:13:52,710
scripture table and everything else kind

00:13:50,490 --> 00:13:54,720
of completely disjoint from the linux

00:13:52,710 --> 00:13:56,610
kernel and it looks like the setup and

00:13:54,720 --> 00:14:00,030
teardown of that cost them quite a bit

00:13:56,610 --> 00:14:02,190
to so that's why they've all behind the

00:14:00,030 --> 00:14:05,540
next point in was an eye perf benchmark

00:14:02,190 --> 00:14:09,900
and for this benchmark I had to l2 yes

00:14:05,540 --> 00:14:13,160
each with only 7 V CPUs and 16 gigabytes

00:14:09,900 --> 00:14:16,230
of memory and I had the networking

00:14:13,160 --> 00:14:18,990
connection between the two l2 VM so that

00:14:16,230 --> 00:14:22,380
no hardware networking or anything else

00:14:18,990 --> 00:14:27,720
was involved this was entirely you know

00:14:22,380 --> 00:14:29,730
software networking and here there's a

00:14:27,720 --> 00:14:34,200
surprise that I actually still can't

00:14:29,730 --> 00:14:38,670
explain under KVM and key mu we actually

00:14:34,200 --> 00:14:44,220
did 10% better in the nested vm than we

00:14:38,670 --> 00:14:45,870
did in the non nested vm if anyone can

00:14:44,220 --> 00:14:51,800
come up to me after the talk and tell me

00:14:45,870 --> 00:14:55,350
why I'd love to know workstation just I

00:14:51,800 --> 00:14:58,350
don't know so they don't have I used

00:14:55,350 --> 00:14:59,760
Verdi o-net in KVM Camus and they don't

00:14:58,350 --> 00:15:02,280
have that in workstation but I use their

00:14:59,760 --> 00:15:04,390
BMX net 3 which is sort of the

00:15:02,280 --> 00:15:09,850
equivalent you know pair of virtual

00:15:04,390 --> 00:15:12,940
work device and I got the results I

00:15:09,850 --> 00:15:15,940
expected I think for comparing L 2 to L

00:15:12,940 --> 00:15:19,030
1 but the numbers are just so abysmal I

00:15:15,940 --> 00:15:21,130
I think it's not even apples to apples

00:15:19,030 --> 00:15:24,970
I'd have no explanation for why it could

00:15:21,130 --> 00:15:28,450
be so bad and then the third thing I

00:15:24,970 --> 00:15:31,090
looked at was a benchmark called SCH

00:15:28,450 --> 00:15:32,590
bench I wish people would put more

00:15:31,090 --> 00:15:35,100
letters in there so it's pronounceable

00:15:32,590 --> 00:15:39,190
so this is essentially a scheduling

00:15:35,100 --> 00:15:43,350
latency benchmark and I think it came

00:15:39,190 --> 00:15:48,670
out of Facebook if I recall correctly

00:15:43,350 --> 00:15:52,090
and I'll let you know that all these

00:15:48,670 --> 00:15:54,340
numbers l1 and l2 are really horrible I

00:15:52,090 --> 00:15:55,930
didn't give the l0 numbers but but these

00:15:54,340 --> 00:15:57,910
are just miserable numbers and you all

00:15:55,930 --> 00:16:00,940
know that sort of message passing

00:15:57,910 --> 00:16:06,970
workloads on a VM whether it's nested or

00:16:00,940 --> 00:16:12,090
not they're just not very good here the

00:16:06,970 --> 00:16:17,290
nested KVM kemu vm was 60% worse than

00:16:12,090 --> 00:16:20,920
the non nested case vmware started worse

00:16:17,290 --> 00:16:22,480
to begin with but nesting was only 10%

00:16:20,920 --> 00:16:25,920
worse and I think a lot of that again

00:16:22,480 --> 00:16:33,550
probably comes down to the reduced

00:16:25,920 --> 00:16:37,420
ancillary l1 vm exits but I think for

00:16:33,550 --> 00:16:41,260
kind of real-world workload this is

00:16:37,420 --> 00:16:44,020
probably the worst case for you know a

00:16:41,260 --> 00:16:48,550
real application on nested games which

00:16:44,020 --> 00:16:53,080
is a 60% reduction in performance or you

00:16:48,550 --> 00:16:57,580
know 64 in 1.6 times as slow or whatever

00:16:53,080 --> 00:16:59,650
you want to say so I wanted to look in a

00:16:57,580 --> 00:17:02,830
little more detail see I mean okay so we

00:16:59,650 --> 00:17:04,480
know it's bad in some cases I want to

00:17:02,830 --> 00:17:07,420
look it'll in a little more detail at

00:17:04,480 --> 00:17:09,430
why it's bad and so I used perf which is

00:17:07,420 --> 00:17:11,050
a great tool you should all use it all

00:17:09,430 --> 00:17:14,550
the time every time you check something

00:17:11,050 --> 00:17:16,510
in use perf and see what happens because

00:17:14,550 --> 00:17:18,160
then we wouldn't get into this situation

00:17:16,510 --> 00:17:20,650
I think we're sometime

00:17:18,160 --> 00:17:27,610
there's just this these really easy easy

00:17:20,650 --> 00:17:30,880
pickings on the cpuid benchmark we can

00:17:27,610 --> 00:17:34,930
see the top hit here is nested BMX

00:17:30,880 --> 00:17:37,600
disable intercept for MSR the next two

00:17:34,930 --> 00:17:40,690
you might kind of expect nested BMX run

00:17:37,600 --> 00:17:45,520
is the function that essentially handles

00:17:40,690 --> 00:17:49,960
emulating vm entry from l1 to l2 and vm

00:17:45,520 --> 00:17:53,530
x v cpu run is the function that

00:17:49,960 --> 00:17:57,640
actually does the vm inter from l0 to

00:17:53,530 --> 00:18:00,250
either l1 or l2 I've highlighted those

00:17:57,640 --> 00:18:02,530
in red because they appear on every one

00:18:00,250 --> 00:18:07,530
of these slides that I did the perf on

00:18:02,530 --> 00:18:10,180
so those are generally fairly important

00:18:07,530 --> 00:18:14,920
some of the others here copies shadow to

00:18:10,180 --> 00:18:23,230
the MCS one - that's copying the state

00:18:14,920 --> 00:18:25,210
from the shadow V MCS that allows which

00:18:23,230 --> 00:18:35,830
shadow of the MCS is this so this is the

00:18:25,210 --> 00:18:38,800
shadow of the MCS that l0 uses that l1

00:18:35,830 --> 00:18:45,430
can do VM reads and VM writes without

00:18:38,800 --> 00:18:47,260
causing a VM exit and I think as you add

00:18:45,430 --> 00:18:50,980
more things to that list that I was

00:18:47,260 --> 00:18:53,380
talking about earlier this is just gonna

00:18:50,980 --> 00:18:57,030
get more expensive but it's not

00:18:53,380 --> 00:18:57,030
obviously there's a lot to save there

00:18:57,360 --> 00:19:04,600
looking next it kernel compile again we

00:19:01,930 --> 00:19:06,640
have the three hot ones from before and

00:19:04,600 --> 00:19:09,430
kind of a surprise to me was that queued

00:19:06,640 --> 00:19:12,250
spin locks low path shows up way at the

00:19:09,430 --> 00:19:13,840
top I don't think we should worry about

00:19:12,250 --> 00:19:19,030
this one this is I think you know this

00:19:13,840 --> 00:19:20,920
is pair of verts spin locks if you're up

00:19:19,030 --> 00:19:24,240
there spinning on a lock you probably

00:19:20,920 --> 00:19:27,490
don't have any work to do anyway so

00:19:24,240 --> 00:19:29,399
maybe it's not a big deal if that's way

00:19:27,490 --> 00:19:31,679
up at the top

00:19:29,399 --> 00:19:35,610
I I /

00:19:31,679 --> 00:19:38,309
looks fairly similar to the CPI D but

00:19:35,610 --> 00:19:42,929
read TSC kind of creeps up there a

00:19:38,309 --> 00:19:48,299
little bit and I think I looked into

00:19:42,929 --> 00:19:51,600
that and it seems like pretty much on

00:19:48,299 --> 00:19:53,100
every nested VM entry we're reading the

00:19:51,600 --> 00:19:56,700
time stamp counter I'm not sure there's

00:19:53,100 --> 00:19:59,220
any way to save a lot there and then

00:19:56,700 --> 00:20:02,690
this scheduling benchmark again queued

00:19:59,220 --> 00:20:06,179
spin lock slow path shoots up as does

00:20:02,690 --> 00:20:10,529
underbar underbar lock acquire I don't

00:20:06,179 --> 00:20:13,230
know why that ones up there but the

00:20:10,529 --> 00:20:14,730
three in red again show up pretty high

00:20:13,230 --> 00:20:20,940
so I wanted to look at those in a little

00:20:14,730 --> 00:20:23,159
more detail and so I used perf annotate

00:20:20,940 --> 00:20:27,330
to take a look at at those particular

00:20:23,159 --> 00:20:30,509
functions and we look at nested vmx run

00:20:27,330 --> 00:20:32,549
i skiped nested vmx disable intercept

00:20:30,509 --> 00:20:36,600
for MSR because i think that should just

00:20:32,549 --> 00:20:39,659
hopefully go away completely but nested

00:20:36,600 --> 00:20:42,539
vmx run the hottest thing in there is

00:20:39,659 --> 00:20:45,029
the loop that calls nested BMX disable

00:20:42,539 --> 00:20:51,539
intercept Paramus R and this all has to

00:20:45,029 --> 00:20:54,240
do with x2 APEC virtualization so you

00:20:51,539 --> 00:20:56,340
know that's unfortunate the other thing

00:20:54,240 --> 00:21:01,529
that's very high there is the memset of

00:20:56,340 --> 00:21:07,049
the MSR permission bitmap that l0 is

00:21:01,529 --> 00:21:09,330
going to use when it's running l2 so you

00:21:07,049 --> 00:21:12,119
know l0 has this idea of MSR is it's

00:21:09,330 --> 00:21:15,269
willing to pass through to its vm's l1

00:21:12,119 --> 00:21:17,070
has an idea of the MSR as it's willing

00:21:15,269 --> 00:21:19,279
to pass through to its VMs and we kind

00:21:17,070 --> 00:21:23,850
of have to create the intersection of

00:21:19,279 --> 00:21:26,309
those permissions for l0 to run l2 and

00:21:23,850 --> 00:21:29,009
as part of creating that intersection we

00:21:26,309 --> 00:21:31,320
first do this memset to say well okay

00:21:29,009 --> 00:21:33,720
let's start with no MSR is being allowed

00:21:31,320 --> 00:21:39,539
and then we'll take a look at the ones

00:21:33,720 --> 00:21:41,850
that can be allowed in DMX be CPU run we

00:21:39,539 --> 00:21:47,580
now get to see that debug control and

00:21:41,850 --> 00:21:51,210
SR right up there at the top notice that

00:21:47,580 --> 00:21:53,700
this is actually running it L zeros this

00:21:51,210 --> 00:21:58,470
isn't because there's a read MSR

00:21:53,700 --> 00:22:01,140
intercept this is actually because we're

00:21:58,470 --> 00:22:03,059
doing a hardware read MSR which is like

00:22:01,140 --> 00:22:09,720
an ad cycle instruction or something

00:22:03,059 --> 00:22:11,789
like that and so ideally since the

00:22:09,720 --> 00:22:16,650
kernel set the MSR commune with it might

00:22:11,789 --> 00:22:18,480
remember what it said it to and just you

00:22:16,650 --> 00:22:24,120
know get a value out of memory instead

00:22:18,480 --> 00:22:26,100
of going and querying the MSR the 23% DM

00:22:24,120 --> 00:22:29,190
resume in a couple instructions after I

00:22:26,100 --> 00:22:31,169
think that's kind of what you have to

00:22:29,190 --> 00:22:35,280
pay I mean that's just the hardware cost

00:22:31,169 --> 00:22:40,559
of doing a VM enter and then somewhere

00:22:35,280 --> 00:22:43,140
in there was the read TSC I forget the

00:22:40,559 --> 00:22:47,570
context around there exactly but I don't

00:22:43,140 --> 00:22:50,580
think there's much to be saved there so

00:22:47,570 --> 00:22:52,440
some lessons that I would suggest taking

00:22:50,580 --> 00:22:56,429
away from this one is continuously

00:22:52,440 --> 00:22:59,190
monitor performance these BM read and VM

00:22:56,429 --> 00:23:01,470
write vm exits the kind of stuck and

00:22:59,190 --> 00:23:05,900
snuck in as we added new features but

00:23:01,470 --> 00:23:08,370
didn't update the vm CS shadowing lists

00:23:05,900 --> 00:23:09,870
that should really never have happened

00:23:08,370 --> 00:23:11,580
right I mean someone should have been

00:23:09,870 --> 00:23:13,650
monitoring the performance and someone

00:23:11,580 --> 00:23:18,659
some red flag should have gone up and

00:23:13,650 --> 00:23:23,460
said wait update those arrays to avoid

00:23:18,659 --> 00:23:26,130
this this heavy performance penalty we

00:23:23,460 --> 00:23:28,530
should avoid reconstructing that MSR

00:23:26,130 --> 00:23:32,100
permission bitmap you know the mem set

00:23:28,530 --> 00:23:35,460
of a 4k page plus this loop that we go

00:23:32,100 --> 00:23:37,409
through of 256 X to a pic registers and

00:23:35,460 --> 00:23:41,870
you know I have to believe that this

00:23:37,409 --> 00:23:41,870
this bitmap is pretty much constant

00:23:42,170 --> 00:24:00,720
but like like of the four hundred four

00:23:58,380 --> 00:24:02,550
thousand ninety six five four thousand

00:24:00,720 --> 00:24:04,500
that are always the same and the other

00:24:02,550 --> 00:24:07,800
ninety six we can just process them

00:24:04,500 --> 00:24:09,780
64-bit so that I'm not one yes and we

00:24:07,800 --> 00:24:13,500
would steal the contact but we will be

00:24:09,780 --> 00:24:16,380
six times faster right right so I mean

00:24:13,500 --> 00:24:19,380
use a little more cleverness I guess in

00:24:16,380 --> 00:24:21,450
in reconstructing it be sure to use the

00:24:19,380 --> 00:24:22,950
V MCS shot shadowing effectively that

00:24:21,450 --> 00:24:26,790
kind of ties in with the first lesson

00:24:22,950 --> 00:24:28,890
and and then cash that I thirty-two

00:24:26,790 --> 00:24:31,440
debug control value in memory don't

00:24:28,890 --> 00:24:33,450
don't go out and read Nemus are you're

00:24:31,440 --> 00:24:35,370
acting like Microsoft when they wit went

00:24:33,450 --> 00:24:37,730
and read the TPR all the time and you

00:24:35,370 --> 00:24:41,430
know what what that did to

00:24:37,730 --> 00:24:44,010
virtualization so you know just like

00:24:41,430 --> 00:24:46,560
Microsoft started reading the IR QL out

00:24:44,010 --> 00:24:50,220
of memory if you've got something that's

00:24:46,560 --> 00:24:53,310
in a slow to access hardware register or

00:24:50,220 --> 00:24:56,280
whatever it's called putting in memory

00:24:53,310 --> 00:24:59,610
so you can get at it quicker and that's

00:24:56,280 --> 00:25:02,330
about all I have to say so I'll open the

00:24:59,610 --> 00:25:02,330
floor for questions

00:25:15,100 --> 00:25:20,590
so for your iperf thing where you had

00:25:17,380 --> 00:25:22,840
better performance for better

00:25:20,590 --> 00:25:24,039
performance and I perf yeah well that

00:25:22,840 --> 00:25:26,350
was a bandwidth thing right it wasn't

00:25:24,039 --> 00:25:27,580
the latency back and forth you were

00:25:26,350 --> 00:25:32,919
trying to measure the amount of traffic

00:25:27,580 --> 00:25:35,919
you could push through that was just

00:25:32,919 --> 00:25:38,110
iperf sending one Direction's I had a

00:25:35,919 --> 00:25:40,960
client and a server there was traffic

00:25:38,110 --> 00:25:42,820
one direction all virtual networking

00:25:40,960 --> 00:25:44,649
right so we saw something similar when

00:25:42,820 --> 00:25:47,620
we compared arm and x86 and what we saw

00:25:44,649 --> 00:25:50,289
was that arm improved where Exodus 6

00:25:47,620 --> 00:25:52,179
decreased and the reason was because arm

00:25:50,289 --> 00:25:54,250
was slower in absolute terms compared to

00:25:52,179 --> 00:25:56,409
the line rate you ended up getting fewer

00:25:54,250 --> 00:25:59,049
interrupts because you could buffer more

00:25:56,409 --> 00:26:00,730
data per interrupt and that ended up

00:25:59,049 --> 00:26:02,799
improving your performance so you might

00:26:00,730 --> 00:26:05,500
want to count the interrupt you see for

00:26:02,799 --> 00:26:09,600
your workload at the l2 level and I pair

00:26:05,500 --> 00:26:09,600
those ok thank you very much

00:26:18,370 --> 00:26:24,100
can I recommend a program called super

00:26:20,919 --> 00:26:25,750
nested it runs nested nested nested

00:26:24,100 --> 00:26:28,179
nested until things fall over and

00:26:25,750 --> 00:26:32,350
against about l4 on my big machine at

00:26:28,179 --> 00:26:35,640
home well I think before I do that I'm

00:26:32,350 --> 00:26:39,330
going to finish my implementation of

00:26:35,640 --> 00:26:42,360
virtualized vm CS shadowing for KBM

00:26:39,330 --> 00:26:44,950
because that'll help get a lot deeper

00:26:42,360 --> 00:26:52,020
when you can start using VNC s shadowing

00:26:44,950 --> 00:26:52,020
below l1 or yeah blo l1

00:27:02,440 --> 00:27:07,659
I'm pretty sure that you would try to

00:27:04,750 --> 00:27:10,389
tweak some of these things already do

00:27:07,659 --> 00:27:12,429
you have any numbers how how good you

00:27:10,389 --> 00:27:13,750
were able to improve there no no I

00:27:12,429 --> 00:27:16,120
haven't tried to tweak any of these

00:27:13,750 --> 00:27:18,549
things these are all for people out

00:27:16,120 --> 00:27:21,250
there to pick up and start start doing

00:27:18,549 --> 00:27:22,899
these these are easy things and then you

00:27:21,250 --> 00:27:25,450
know we'll do round two and I think

00:27:22,899 --> 00:27:30,269
we'll be closing in on VMware pretty

00:27:25,450 --> 00:27:30,269
fast okay see you at next get informed

00:27:36,570 --> 00:27:47,909
data for Zen I I have no Zen data sorry

00:27:48,389 --> 00:28:00,310
one more question all right thank you

00:27:56,660 --> 00:28:06,619
[Applause]

00:28:00,310 --> 00:28:06,619

YouTube URL: https://www.youtube.com/watch?v=PxDHNfrpwHE


