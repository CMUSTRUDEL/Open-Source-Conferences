Title: [2017] Helping Users Maximize VM Performance by Martin Polednik
Publication date: 2017-11-04
Playlist: KVM Forum 2017
Description: 
	QEMU supports numerous options to fine tune the virtual machine, starting from the big items such as number of CPUs, NUMA nodes or amount of memory all the way down to the choice of USB controllers, disk controllers, and thread pinning. Different use cases require careful tuning to reach desired performance, and certain settings may interfere with configuration's expected performance. Management software can let users fully build the virtual machine, but it may also help them by warning about conflicting or suboptimal choices. In this talk, we will present samples of common configurations, some of them from oVirt users' community, and our ideas how to make their life easier by suggesting changes that should lead to performance improvement in most cases.

---

Martin Polednik
Software Engineer, Red Hat
Martin Polednik works on the oVirt project as a Software Engineer at Red Hat. As part of the oVirt virtualization team, he is responsible for integrating KVM, QEMU and libvirt virtualization features into oVirt.
Captions: 
	00:00:00,390 --> 00:00:02,709
[Music]

00:00:06,170 --> 00:00:13,080
welcome to his talk about optimizing the

00:00:10,080 --> 00:00:15,780
VM or helping users optimize and

00:00:13,080 --> 00:00:18,480
maximize the VM performance so wrote me

00:00:15,780 --> 00:00:21,240
I'm Martin Plotnik and I work as a

00:00:18,480 --> 00:00:22,740
software engineer at Red Hat now maybe

00:00:21,240 --> 00:00:26,580
the interesting part is that I'm not

00:00:22,740 --> 00:00:29,070
directly working on KVM or ki mo or

00:00:26,580 --> 00:00:31,800
livered but what I'm working on is

00:00:29,070 --> 00:00:35,160
management solution called Eggert on top

00:00:31,800 --> 00:00:37,980
of livered so this is actually kind of a

00:00:35,160 --> 00:00:41,430
high-level talk that should serve as a

00:00:37,980 --> 00:00:46,410
bridge between two groups of people one

00:00:41,430 --> 00:00:49,920
side there are users that 1ds you know

00:00:46,410 --> 00:00:52,469
nice click evap you eyes and on the

00:00:49,920 --> 00:00:56,219
other hand there's k vm k emily bird

00:00:52,469 --> 00:00:59,640
that we are trying to get best

00:00:56,219 --> 00:01:02,609
performance from Wow

00:00:59,640 --> 00:01:04,920
so what it boils down to looks like this

00:01:02,609 --> 00:01:06,900
slider and I was told that this slider

00:01:04,920 --> 00:01:08,820
looks really bad because you don't know

00:01:06,900 --> 00:01:10,890
you know which side is actually getting

00:01:08,820 --> 00:01:13,950
better so if you don't like it just

00:01:10,890 --> 00:01:16,290
imagine scale in its place the main

00:01:13,950 --> 00:01:20,340
thing is on one side you have a

00:01:16,290 --> 00:01:23,280
flexibility so in terms of VMs flexible

00:01:20,340 --> 00:01:27,180
VMs means it's you have the ability to

00:01:23,280 --> 00:01:30,869
migrate the VM you can run it on one of

00:01:27,180 --> 00:01:32,700
the hosts in your cluster or you can

00:01:30,869 --> 00:01:35,070
connect it and graphically by a spice or

00:01:32,700 --> 00:01:36,990
VNC on the other side you have

00:01:35,070 --> 00:01:38,759
performance now I'm talking raw

00:01:36,990 --> 00:01:41,189
performance networking throughput

00:01:38,759 --> 00:01:43,979
storage bandwidth or just the generic

00:01:41,189 --> 00:01:46,049
compute performance so you see when you

00:01:43,979 --> 00:01:48,350
start moving the slider to either

00:01:46,049 --> 00:01:51,720
direction what happens is that you

00:01:48,350 --> 00:01:55,200
either reduce flexibility or you reduce

00:01:51,720 --> 00:01:58,979
the performance now that's in theory in

00:01:55,200 --> 00:02:00,689
practice what actually happens is if you

00:01:58,979 --> 00:02:03,119
try to improve performance there's a

00:02:00,689 --> 00:02:05,460
high probability of you just reducing

00:02:03,119 --> 00:02:09,360
the flexibility while not getting any

00:02:05,460 --> 00:02:10,810
performance improvement and because that

00:02:09,360 --> 00:02:14,830
happens

00:02:10,810 --> 00:02:18,370
quite often I was thinking of way to

00:02:14,830 --> 00:02:22,390
analyze it and one of the ways seems to

00:02:18,370 --> 00:02:26,590
be some kind of data analysis so this is

00:02:22,390 --> 00:02:29,650
kind of rare because talking of overt

00:02:26,590 --> 00:02:31,540
it's a on premise solution so you don't

00:02:29,650 --> 00:02:33,400
normally have any kind of runtime data

00:02:31,540 --> 00:02:35,350
that you could analyze on it's not kind

00:02:33,400 --> 00:02:37,480
of cloud that you just type into

00:02:35,350 --> 00:02:39,880
database and see what happens but you

00:02:37,480 --> 00:02:43,630
really have to go and grab the data from

00:02:39,880 --> 00:02:44,590
somewhere in this case it was SOS report

00:02:43,630 --> 00:02:47,530
databases

00:02:44,590 --> 00:02:48,880
that's great thing about our SOS report

00:02:47,530 --> 00:02:51,550
that it actually contains the whole

00:02:48,880 --> 00:02:54,430
database and putting this all together

00:02:51,550 --> 00:02:57,430
led to quite a big data set so we can

00:02:54,430 --> 00:03:00,940
see there's like 40 KVM definitions 700

00:02:57,430 --> 00:03:03,489
clusters over 2000 hosts by the way I'll

00:03:00,940 --> 00:03:07,420
be using host instead of hypervisor but

00:03:03,489 --> 00:03:11,470
I mean the same thing and around 60 K

00:03:07,420 --> 00:03:14,709
disks before we go on I've used the term

00:03:11,470 --> 00:03:18,519
cluster that probably deserves some kind

00:03:14,709 --> 00:03:22,320
of definition because it's kind of

00:03:18,519 --> 00:03:26,110
foreign concept to KVM so what it means

00:03:22,320 --> 00:03:28,930
let's consider cluster as some kind of

00:03:26,110 --> 00:03:32,200
migration domain and scheduling domain

00:03:28,930 --> 00:03:36,610
so it's VMs hosts they run on their

00:03:32,200 --> 00:03:38,829
disks networks everything together since

00:03:36,610 --> 00:03:41,380
we will be talking about clusters and

00:03:38,829 --> 00:03:42,280
actually machine types there we should

00:03:41,380 --> 00:03:45,850
establish some kind of relationship

00:03:42,280 --> 00:03:49,450
between the two and there's a great

00:03:45,850 --> 00:03:52,269
timeline that you can see how Machine

00:03:49,450 --> 00:03:52,810
type and cluster version is related it

00:03:52,269 --> 00:03:57,400
begins

00:03:52,810 --> 00:03:59,440
in this case with over 3.6 where the

00:03:57,400 --> 00:04:02,470
cluster version used was rel 6500

00:03:59,440 --> 00:04:05,650
cluster version then we go on and see

00:04:02,470 --> 00:04:10,630
4.0 which maps to 7 - o cluster version

00:04:05,650 --> 00:04:15,400
the current newest release is 4.1 which

00:04:10,630 --> 00:04:18,220
maps to cluster version rel 73 now there

00:04:15,400 --> 00:04:21,270
are cluster versions before 3.6 and

00:04:18,220 --> 00:04:23,979
there will be class versions above 4.1

00:04:21,270 --> 00:04:24,550
it's just that in this case the database

00:04:23,979 --> 00:04:27,849
is very

00:04:24,550 --> 00:04:29,740
really from that point of time so it's

00:04:27,849 --> 00:04:32,080
important to remember that this is

00:04:29,740 --> 00:04:35,680
really some limited data set that's not

00:04:32,080 --> 00:04:38,169
like all of the history first let's look

00:04:35,680 --> 00:04:41,349
at the Machine times and this is pretty

00:04:38,169 --> 00:04:43,810
interesting on the chart you can see

00:04:41,349 --> 00:04:46,300
that the most used machine type is not

00:04:43,810 --> 00:04:48,699
actually rel 73 which would be the

00:04:46,300 --> 00:04:51,960
newest one dictionary around seven point

00:04:48,699 --> 00:04:54,879
two which is one version behind and

00:04:51,960 --> 00:04:58,210
there's actually a reason for that and

00:04:54,879 --> 00:05:01,690
that is the cluster update it's not a

00:04:58,210 --> 00:05:04,870
trivial process it requires some kind of

00:05:01,690 --> 00:05:06,940
extraneous because how that works if you

00:05:04,870 --> 00:05:09,960
want to update a host in the cluster you

00:05:06,940 --> 00:05:12,909
first have to grab the VMS from the host

00:05:09,960 --> 00:05:16,000
move them somewhere else in the cluster

00:05:12,909 --> 00:05:19,050
and then only then you can update a host

00:05:16,000 --> 00:05:21,699
so there are some resource requirements

00:05:19,050 --> 00:05:23,620
another reason why people are running

00:05:21,699 --> 00:05:25,870
all through clusters is that there are

00:05:23,620 --> 00:05:27,759
simply no new and interesting features

00:05:25,870 --> 00:05:32,770
that they could use

00:05:27,759 --> 00:05:37,210
so why move on so now let's transition

00:05:32,770 --> 00:05:39,370
to more textual VM configurations and

00:05:37,210 --> 00:05:42,759
let's see what the data set were

00:05:39,370 --> 00:05:45,389
revealed and we'll start at one of the

00:05:42,759 --> 00:05:49,800
most important two novels which is Numa

00:05:45,389 --> 00:05:51,880
now Numa is interesting because it's not

00:05:49,800 --> 00:05:54,069
we are not using it only for the

00:05:51,880 --> 00:05:56,020
performance benefit itself but there's

00:05:54,069 --> 00:05:59,020
also a certain class of applications

00:05:56,020 --> 00:06:02,710
like those huge databases and ERP

00:05:59,020 --> 00:06:05,259
systems that scan the Numa and try to

00:06:02,710 --> 00:06:08,440
optimize themselves within the glass so

00:06:05,259 --> 00:06:11,800
if we don't expose new motto ecology

00:06:08,440 --> 00:06:13,930
that some how much is the host the

00:06:11,800 --> 00:06:16,000
application will simply do a bad thing

00:06:13,930 --> 00:06:17,500
so there will be no performance

00:06:16,000 --> 00:06:21,610
improvement it's actually going to be

00:06:17,500 --> 00:06:25,479
worse also some context how you might

00:06:21,610 --> 00:06:27,400
set up it's important to know that in

00:06:25,479 --> 00:06:29,710
over it's all menu

00:06:27,400 --> 00:06:31,990
you don't really help the user you know

00:06:29,710 --> 00:06:34,659
create this kind of node and this kind

00:06:31,990 --> 00:06:37,870
of node no it's just a select number of

00:06:34,659 --> 00:06:40,570
nodes select which notes those pin to

00:06:37,870 --> 00:06:43,570
so there's kind of no holding hand your

00:06:40,570 --> 00:06:47,770
left on your album and thanks to this we

00:06:43,570 --> 00:06:50,790
have discovered two issues the first one

00:06:47,770 --> 00:06:54,150
is what I call soft Numa violation and

00:06:50,790 --> 00:06:56,490
it's what you can see on the slide

00:06:54,150 --> 00:07:00,040
software violation happens when there's

00:06:56,490 --> 00:07:02,080
new mono and if host contains in the

00:07:00,040 --> 00:07:06,160
mono that's actually smaller than the

00:07:02,080 --> 00:07:08,590
new mono that BM requests and in this

00:07:06,160 --> 00:07:10,240
example it's just single node V M so

00:07:08,590 --> 00:07:13,840
it's pretty obvious that it doesn't fit

00:07:10,240 --> 00:07:16,479
into the smaller of host nodes this can

00:07:13,840 --> 00:07:19,780
actually be solved by pinning the nodes

00:07:16,479 --> 00:07:21,460
to the bigger road or actually removing

00:07:19,780 --> 00:07:24,580
the holes all together from the

00:07:21,460 --> 00:07:28,770
classroom and we have seen this happen

00:07:24,580 --> 00:07:31,780
in roughly 17% of definitions now

00:07:28,770 --> 00:07:35,050
important thing is that those are not

00:07:31,780 --> 00:07:38,320
all VMs in the clusters and all sorry Oh

00:07:35,050 --> 00:07:40,720
VMs in the database and all who's in the

00:07:38,320 --> 00:07:43,900
database this is really looking at the

00:07:40,720 --> 00:07:47,910
cluster itself so this is actual

00:07:43,900 --> 00:07:52,810
happening scenario nothing's theoretical

00:07:47,910 --> 00:07:54,340
the number is pretty high but as it

00:07:52,810 --> 00:07:56,639
happens only in the worst case of

00:07:54,340 --> 00:07:59,190
cluster scheduling and who's scheduling

00:07:56,639 --> 00:08:02,250
we don't really believe that this is too

00:07:59,190 --> 00:08:04,599
significant on the other hand we have

00:08:02,250 --> 00:08:08,470
harden of evaluation and that is when

00:08:04,599 --> 00:08:10,840
the VMS no mono simply exceeds both both

00:08:08,470 --> 00:08:14,349
not both all of the numa nodes on the

00:08:10,840 --> 00:08:18,010
host so it's not possible to simply fit

00:08:14,349 --> 00:08:21,220
the VM onto the onto the host without

00:08:18,010 --> 00:08:26,280
somehow splitting the nodes in two for

00:08:21,220 --> 00:08:29,110
example now this has been seen in

00:08:26,280 --> 00:08:32,050
roughly nine point 74 percent of

00:08:29,110 --> 00:08:34,180
definitions and again not theoretical

00:08:32,050 --> 00:08:37,599
cases these are practical actual

00:08:34,180 --> 00:08:39,700
happening places unlike with the soft

00:08:37,599 --> 00:08:41,469
Numa violation you can't really solve

00:08:39,700 --> 00:08:43,839
this by pinning the node because it

00:08:41,469 --> 00:08:46,030
doesn't fit anywhere so this is worst

00:08:43,839 --> 00:08:49,120
case in the cluster scheduling only and

00:08:46,030 --> 00:08:53,670
that means it's quite significant issue

00:08:49,120 --> 00:08:57,850
that we should do something about and

00:08:53,670 --> 00:08:59,589
there's a possible solution so as you

00:08:57,850 --> 00:09:02,170
can see based on this presentation we

00:08:59,589 --> 00:09:04,540
have the data required you know to see

00:09:02,170 --> 00:09:07,960
whether this can happen in the cluster

00:09:04,540 --> 00:09:11,589
it can happen in the cluster we can warn

00:09:07,960 --> 00:09:14,140
the user we could probably even make

00:09:11,589 --> 00:09:16,150
something more intelligent but let's say

00:09:14,140 --> 00:09:18,940
right now the software doesn't really

00:09:16,150 --> 00:09:20,830
want to take the responsibility and do

00:09:18,940 --> 00:09:25,480
that on users behalf

00:09:20,830 --> 00:09:28,720
so our save ground in this case is some

00:09:25,480 --> 00:09:31,480
kind of warning dialog maybe in the

00:09:28,720 --> 00:09:33,850
future as confidence in this code you

00:09:31,480 --> 00:09:38,610
know increases and there are no user

00:09:33,850 --> 00:09:41,050
issues then we can do that automatically

00:09:38,610 --> 00:09:45,970
now depending was mentioned multiple

00:09:41,050 --> 00:09:50,050
times but it turns out people don't pin

00:09:45,970 --> 00:09:53,230
their VMs at all why is that now again

00:09:50,050 --> 00:09:56,140
this is case of not exactly user

00:09:53,230 --> 00:09:59,020
friendly approach we don't give a hints

00:09:56,140 --> 00:10:01,240
on how should you pin your VMs where

00:09:59,020 --> 00:10:03,760
should they be pinned and that they

00:10:01,240 --> 00:10:06,910
should be been at all so this is not

00:10:03,760 --> 00:10:09,010
really used at all even worse than new

00:10:06,910 --> 00:10:11,140
map and new opening itself is the CP

00:10:09,010 --> 00:10:12,670
opening where we have even custom syntax

00:10:11,140 --> 00:10:15,310
that we have to figure out how to use

00:10:12,670 --> 00:10:16,709
and again there's no information about

00:10:15,310 --> 00:10:19,600
that

00:10:16,709 --> 00:10:22,930
additionally some management level

00:10:19,600 --> 00:10:25,870
constraints rise from pinning so no

00:10:22,930 --> 00:10:29,350
migration is the migration is disabled

00:10:25,870 --> 00:10:32,350
at management level you're also reducing

00:10:29,350 --> 00:10:35,589
the amount of hose that you can run on

00:10:32,350 --> 00:10:37,000
by some kind of pinning and in that case

00:10:35,589 --> 00:10:39,910
you're hitting an issue with high

00:10:37,000 --> 00:10:42,790
availability to just ensure high

00:10:39,910 --> 00:10:46,540
availability is you trying to have a VM

00:10:42,790 --> 00:10:50,470
that has maximum possible uptime and if

00:10:46,540 --> 00:10:52,720
the VM dies you restarted elsewhere that

00:10:50,470 --> 00:10:55,390
kind of conflicting with the previous

00:10:52,720 --> 00:10:57,820
point because now the place is the host

00:10:55,390 --> 00:11:00,760
where the VM can be started our only

00:10:57,820 --> 00:11:01,710
subset of the previous version so we may

00:11:00,760 --> 00:11:04,800
hit

00:11:01,710 --> 00:11:09,270
eh-eh the question is can we changed it

00:11:04,800 --> 00:11:09,780
and it turns out there are approaches to

00:11:09,270 --> 00:11:12,600
do that

00:11:09,780 --> 00:11:15,750
so first is of course obviously lifting

00:11:12,600 --> 00:11:18,030
the management the limitations but then

00:11:15,750 --> 00:11:21,180
the pinning could be done done in some

00:11:18,030 --> 00:11:24,330
kind of algorithmic Bay so we have this

00:11:21,180 --> 00:11:27,330
example here of pretty beefy host with

00:11:24,330 --> 00:11:30,990
four new models and 12 CPUs permanent

00:11:27,330 --> 00:11:34,050
and in this case we've kind of borrowed

00:11:30,990 --> 00:11:37,080
an idea from real-time KVM where you

00:11:34,050 --> 00:11:39,990
reserved portion of the CPUs in each new

00:11:37,080 --> 00:11:42,750
model for some kind of service tasks so

00:11:39,990 --> 00:11:44,940
we do have some kind of demon libertage

00:11:42,750 --> 00:11:48,240
cannon run there and there's emulation

00:11:44,940 --> 00:11:50,760
threads and i/o trends so this can run

00:11:48,240 --> 00:11:53,010
in this example on the first two CPUs of

00:11:50,760 --> 00:11:58,470
the numa node where you live the other

00:11:53,010 --> 00:12:02,850
10 CPUs or sorry course a sexual v CPUs

00:11:58,470 --> 00:12:04,620
for VMs to use if the number of V CPUs

00:12:02,850 --> 00:12:07,830
is actually bigger than you can actually

00:12:04,620 --> 00:12:11,550
fit into the node again we are asking

00:12:07,830 --> 00:12:16,260
the user oh well this may not be optimal

00:12:11,550 --> 00:12:19,470
create another V note and since really

00:12:16,260 --> 00:12:22,560
this is taken from real time ideas in

00:12:19,470 --> 00:12:25,500
case we want to adopt real time KBM you

00:12:22,560 --> 00:12:26,970
already have some kind of good starter

00:12:25,500 --> 00:12:31,200
because we would just have to think

00:12:26,970 --> 00:12:33,710
about as all CPUs and no is it for for

00:12:31,200 --> 00:12:37,800
the compute CPS so that's pretty nice

00:12:33,710 --> 00:12:39,750
benefit now one thing to remember is in

00:12:37,800 --> 00:12:41,430
this case what you want to do is still

00:12:39,750 --> 00:12:44,790
let the user configure the number of

00:12:41,430 --> 00:12:48,150
service CPUs because in case of you know

00:12:44,790 --> 00:12:51,000
a host bit only for CPS parameters you

00:12:48,150 --> 00:12:55,200
don't still half of your compute

00:12:51,000 --> 00:12:59,580
capability just for services now before

00:12:55,200 --> 00:13:03,090
we leave the topic of CPU there's one

00:12:59,580 --> 00:13:05,880
more tunable now this is really simple

00:13:03,090 --> 00:13:09,420
for us because as you can see in Kim was

00:13:05,880 --> 00:13:12,380
just l3 cache equals on delivered is

00:13:09,420 --> 00:13:15,650
just sub element cache level and

00:13:12,380 --> 00:13:18,320
some kind of mode this is extremely

00:13:15,650 --> 00:13:22,790
simple thing to do from management level

00:13:18,320 --> 00:13:25,130
and in case of l3 cache it has turned

00:13:22,790 --> 00:13:28,790
out as something you really want to use

00:13:25,130 --> 00:13:32,390
in case you have huge VMs that is

00:13:28,790 --> 00:13:34,910
because in case of machines with

00:13:32,390 --> 00:13:37,760
multiple physical sockets at least the

00:13:34,910 --> 00:13:41,240
outer or the commit message tried some

00:13:37,760 --> 00:13:43,520
kind of s AP test shield and came to a

00:13:41,240 --> 00:13:45,860
conclusion that there is roughly fifteen

00:13:43,520 --> 00:13:51,140
percent on average increase in CPU

00:13:45,860 --> 00:13:55,460
performance so this is almost no no work

00:13:51,140 --> 00:14:00,740
for us and depend actual benefit may be

00:13:55,460 --> 00:14:04,850
huge let's get into maybe more

00:14:00,740 --> 00:14:07,040
interesting areas and extremely

00:14:04,850 --> 00:14:08,930
significant improvement that we can make

00:14:07,040 --> 00:14:11,870
is using huge pages

00:14:08,930 --> 00:14:14,150
so from Haller high-level perspective

00:14:11,870 --> 00:14:18,230
what we want to do is instead of using

00:14:14,150 --> 00:14:20,630
the standard Ram anonymous back memory

00:14:18,230 --> 00:14:25,250
we're actually allocating some kind of

00:14:20,630 --> 00:14:26,870
file back to huge pages that let's say

00:14:25,250 --> 00:14:29,350
it should improve the performance in

00:14:26,870 --> 00:14:32,390
some way they're actually required by

00:14:29,350 --> 00:14:35,150
some applications like if you're using

00:14:32,390 --> 00:14:38,420
an IV with DP DK you were actually

00:14:35,150 --> 00:14:42,230
required to use each pages out of that I

00:14:38,420 --> 00:14:44,570
do not have exact benchmarks so it's

00:14:42,230 --> 00:14:49,700
worth benchmarking whether they really

00:14:44,570 --> 00:14:51,800
help or not also if you're only talking

00:14:49,700 --> 00:14:54,770
about one platform it's pretty simple

00:14:51,800 --> 00:14:58,850
because on x86 64 the huge pages happen

00:14:54,770 --> 00:15:00,050
to come in two sizes versus now I don't

00:14:58,850 --> 00:15:02,660
want to be wrong it should be two

00:15:00,050 --> 00:15:03,290
megabytes the second size is one

00:15:02,660 --> 00:15:05,630
gigabyte

00:15:03,290 --> 00:15:07,940
but there's the caveat that some CPUs

00:15:05,630 --> 00:15:13,040
older ones I believe it a newer newer

00:15:07,940 --> 00:15:15,860
ones supported some of them have one

00:15:13,040 --> 00:15:20,570
given each pages that let's say should

00:15:15,860 --> 00:15:22,230
bring more improvement in performance of

00:15:20,570 --> 00:15:24,360
course there's

00:15:22,230 --> 00:15:28,800
small difficulty voltage pages and

00:15:24,360 --> 00:15:32,400
that's two kinds of allocation policies

00:15:28,800 --> 00:15:34,860
so the first policies using

00:15:32,400 --> 00:15:37,740
pre-allocated huge pages and that's

00:15:34,860 --> 00:15:40,680
pretty simple what you do is specify

00:15:37,740 --> 00:15:43,110
just huge pages in their size on Colonel

00:15:40,680 --> 00:15:45,450
come online and when the system boots up

00:15:43,110 --> 00:15:49,200
the each pages are there and there are

00:15:45,450 --> 00:15:53,310
no issues the problem with this approach

00:15:49,200 --> 00:15:55,080
is of course if you wanna run more VMS

00:15:53,310 --> 00:15:57,840
than you initially allocated each page

00:15:55,080 --> 00:15:59,250
is for sorry you're out of luck the host

00:15:57,840 --> 00:16:02,700
doesn't have more huge pages to

00:15:59,250 --> 00:16:06,000
accommodate the VM so there's a second

00:16:02,700 --> 00:16:09,630
approach dynamic allocation that one is

00:16:06,000 --> 00:16:12,150
done pretty easily through CSFs where

00:16:09,630 --> 00:16:16,560
you simply echo the number of huge pages

00:16:12,150 --> 00:16:18,720
that you require now it almost seems

00:16:16,560 --> 00:16:23,280
better choice than pre-allocated huge

00:16:18,720 --> 00:16:27,090
pages it turns out that allocating let's

00:16:23,280 --> 00:16:32,910
say higher number of huge pages of big

00:16:27,090 --> 00:16:35,510
size can take ages and even after ages

00:16:32,910 --> 00:16:39,390
there's a probability of that failing if

00:16:35,510 --> 00:16:42,560
memory is too fragmented so we have

00:16:39,390 --> 00:16:44,640
figured out that although the benefit of

00:16:42,560 --> 00:16:48,360
dynamically allocated which pages would

00:16:44,640 --> 00:16:52,040
be huge for now let's keep the disabled

00:16:48,360 --> 00:16:56,100
and just deal with standard

00:16:52,040 --> 00:16:58,800
pre-allocated pages so this is one of

00:16:56,100 --> 00:17:02,550
the cases with this hard resource limit

00:16:58,800 --> 00:17:05,040
if you run out of the huge pages no more

00:17:02,550 --> 00:17:09,480
VMs can be started it required me huge

00:17:05,040 --> 00:17:12,780
pages that means no over commitment this

00:17:09,480 --> 00:17:14,910
is the hip on flexibility that you know

00:17:12,780 --> 00:17:16,920
people expect from virtualization that

00:17:14,910 --> 00:17:21,060
you can overcome it you can run multiple

00:17:16,920 --> 00:17:23,670
VMs bit more total memory than your host

00:17:21,060 --> 00:17:26,630
actually has well in case of huge pages

00:17:23,670 --> 00:17:30,050
you can't and we have just defeated

00:17:26,630 --> 00:17:32,640
let's advantage of virtualization

00:17:30,050 --> 00:17:33,270
another thing are the management

00:17:32,640 --> 00:17:37,470
contraire

00:17:33,270 --> 00:17:40,230
it constraints there's no hope LaHood

00:17:37,470 --> 00:17:43,980
unplug no migration the case of

00:17:40,230 --> 00:17:46,320
migration is kind of questionable I've

00:17:43,980 --> 00:17:50,190
been warned that using one gig huge

00:17:46,320 --> 00:17:52,470
pages has pretty low convergence rate on

00:17:50,190 --> 00:17:55,290
the other hand that may change and we

00:17:52,470 --> 00:17:57,150
may figure out that the convergence can

00:17:55,290 --> 00:18:01,560
actually happen and again start lifting

00:17:57,150 --> 00:18:05,540
these management restrictions let's move

00:18:01,560 --> 00:18:09,390
to some more interesting tweaks because

00:18:05,540 --> 00:18:12,900
what we had previously where what we

00:18:09,390 --> 00:18:14,880
call low level tweaks they usually have

00:18:12,900 --> 00:18:18,840
some kind of flexibility versus

00:18:14,880 --> 00:18:22,370
performance trade-offs but in case of vm

00:18:18,840 --> 00:18:25,860
devices we're actually seeing maybe even

00:18:22,370 --> 00:18:28,650
changes in usage patterns there are some

00:18:25,860 --> 00:18:32,970
maybe UI changes if some device is

00:18:28,650 --> 00:18:36,810
disabled or enabled so this really

00:18:32,970 --> 00:18:40,260
changes the way you may use your vm

00:18:36,810 --> 00:18:43,350
management software the first device

00:18:40,260 --> 00:18:47,520
you'll started is pretty mild in this

00:18:43,350 --> 00:18:48,240
case it's pretty much invisible until

00:18:47,520 --> 00:18:52,560
you need it

00:18:48,240 --> 00:18:55,370
so it's pretty simple device as far as I

00:18:52,560 --> 00:18:57,480
understand it what it does is it takes

00:18:55,370 --> 00:19:00,390
entropy from the host and makes it

00:18:57,480 --> 00:19:02,730
available within the guest and this

00:19:00,390 --> 00:19:06,060
turns out to be extremely valuable

00:19:02,730 --> 00:19:09,240
because if you're installing OS there's

00:19:06,060 --> 00:19:12,150
probably some kind of PRNG operation if

00:19:09,240 --> 00:19:14,670
you're generating GPG case there's some

00:19:12,150 --> 00:19:17,280
kind of PNG operation and the same thing

00:19:14,670 --> 00:19:20,900
is installing some kind of software this

00:19:17,280 --> 00:19:24,570
is often free IP a kind of software

00:19:20,900 --> 00:19:28,980
where the installation took ages so we

00:19:24,570 --> 00:19:30,990
had users coming saying this installs

00:19:28,980 --> 00:19:33,840
like for two hours while on bare metal

00:19:30,990 --> 00:19:36,540
it's almost instant and it turns out

00:19:33,840 --> 00:19:39,060
that just enabling vertical energy is

00:19:36,540 --> 00:19:42,180
soft all their problems which is great

00:19:39,060 --> 00:19:44,010
you know happy user having developer so

00:19:42,180 --> 00:19:46,890
previously this was optional

00:19:44,010 --> 00:19:49,560
there was a checkbox to

00:19:46,890 --> 00:19:52,980
in the end we have figure out there are

00:19:49,560 --> 00:19:56,760
probably no probably no downsides so why

00:19:52,980 --> 00:19:59,340
don't we enable that by default and you

00:19:56,760 --> 00:20:03,260
are just some results of the change so

00:19:59,340 --> 00:20:06,750
you can see that in 3.6 and 4.0 versions

00:20:03,260 --> 00:20:12,600
it wasn't really used we're talking less

00:20:06,750 --> 00:20:16,470
than 4% of VMs in 4.0 be sorry 4.1 we

00:20:12,600 --> 00:20:18,360
can see quite a big jump this jump is

00:20:16,470 --> 00:20:20,730
still pretty small in terms of total

00:20:18,360 --> 00:20:23,430
number of VMs and that is because such

00:20:20,730 --> 00:20:26,520
default change only affects a new VM so

00:20:23,430 --> 00:20:29,130
these are VMs created in 4.1 cluster

00:20:26,520 --> 00:20:32,730
version not vmstat were created in 4.0

00:20:29,130 --> 00:20:35,400
and then just upgraded to 4.1 and I've

00:20:32,730 --> 00:20:38,040
just tried some kind of benchmark to

00:20:35,400 --> 00:20:41,580
show the the improvement it actually

00:20:38,040 --> 00:20:43,560
delivers turns out orangey test kind of

00:20:41,580 --> 00:20:47,040
proved the point although it's

00:20:43,560 --> 00:20:48,450
artificial and the actual use case isn't

00:20:47,040 --> 00:20:50,730
really the benchmark it's really

00:20:48,450 --> 00:20:55,650
installing software where many more

00:20:50,730 --> 00:20:58,110
pieces take place you can see actually

00:20:55,650 --> 00:21:00,240
the blue line was roughly four seconds

00:20:58,110 --> 00:21:04,920
while the Green Line was 80 seconds

00:21:00,240 --> 00:21:08,520
pretty good improvement now moving on we

00:21:04,920 --> 00:21:10,920
have network and it turns out network is

00:21:08,520 --> 00:21:13,290
pretty boring because I was interested

00:21:10,920 --> 00:21:16,620
in what could we do on the network level

00:21:13,290 --> 00:21:18,750
- you know I do improve performance

00:21:16,620 --> 00:21:21,210
flexibility or just find better choice

00:21:18,750 --> 00:21:23,490
for for the people

00:21:21,210 --> 00:21:26,150
turns out the choice we would want

00:21:23,490 --> 00:21:29,790
people to make they've already made it

00:21:26,150 --> 00:21:33,420
almost everyone is using real tile the

00:21:29,790 --> 00:21:35,910
minority is using emulated Nick which is

00:21:33,420 --> 00:21:37,770
probably just for compatibility proof of

00:21:35,910 --> 00:21:41,490
purposes

00:21:37,770 --> 00:21:45,210
on the other hand we have Sree meant for

00:21:41,490 --> 00:21:49,620
really networking intensive applications

00:21:45,210 --> 00:21:52,020
such as nav these are used but the

00:21:49,620 --> 00:21:54,330
expected percentage of VMs that use this

00:21:52,020 --> 00:21:56,970
RV is so low that it's not really even

00:21:54,330 --> 00:21:59,230
reflected in data but it's there and

00:21:56,970 --> 00:22:01,690
that's actually what we want it to

00:21:59,230 --> 00:22:07,240
say the distribution is great in this

00:22:01,690 --> 00:22:10,390
case so networking is not a problem this

00:22:07,240 --> 00:22:13,780
son brother on the other hand are a

00:22:10,390 --> 00:22:16,600
whole different kind of Beast so there's

00:22:13,780 --> 00:22:21,220
a lot of choice you can see roughly four

00:22:16,600 --> 00:22:25,120
choices and those are once offered to

00:22:21,220 --> 00:22:27,700
the users and unfortunately they have

00:22:25,120 --> 00:22:30,040
quite a lot of choices but not a lot of

00:22:27,700 --> 00:22:34,720
information regarding which one should

00:22:30,040 --> 00:22:36,940
you pick on top of that depending on

00:22:34,720 --> 00:22:40,000
which one you have which one you pick

00:22:36,940 --> 00:22:42,429
there's some kind of difference not only

00:22:40,000 --> 00:22:44,860
in how you treat the VM from management

00:22:42,429 --> 00:22:47,710
perspective but even have to start

00:22:44,860 --> 00:22:49,900
thinking inside a guess so if I use

00:22:47,710 --> 00:22:52,870
vertex Casa I will need different driver

00:22:49,900 --> 00:22:57,070
than Berta blog and maybe my OS doesn't

00:22:52,870 --> 00:22:59,830
have driver for one let's that's a

00:22:57,070 --> 00:23:03,160
problem on top of that the vertical

00:22:59,830 --> 00:23:06,220
controller pretty much has to be even in

00:23:03,160 --> 00:23:09,340
VMs that do not need by default vertex

00:23:06,220 --> 00:23:11,620
Casa and that is again flexibility

00:23:09,340 --> 00:23:13,780
choice that we did in case you have

00:23:11,620 --> 00:23:15,669
birthday I'd block only VM and you want

00:23:13,780 --> 00:23:17,620
to hurt black widow scuzzy disks later

00:23:15,669 --> 00:23:19,960
you would have to shut it down so that's

00:23:17,620 --> 00:23:24,490
not really a hot block adding the

00:23:19,960 --> 00:23:27,790
controller at least sauce that so it was

00:23:24,490 --> 00:23:30,309
slightest for a long time until trim

00:23:27,790 --> 00:23:34,419
came along and it turns out everyone

00:23:30,309 --> 00:23:38,440
wants trim it's probably the most

00:23:34,419 --> 00:23:40,299
requested feature in in in case of

00:23:38,440 --> 00:23:44,590
storage so everyone is coming in trim

00:23:40,299 --> 00:23:47,049
trim trim that actually was the deciding

00:23:44,590 --> 00:23:51,130
factor in changing the default which was

00:23:47,049 --> 00:23:53,309
previously retire blog in 4.1 we have

00:23:51,130 --> 00:23:55,630
moved over to great escazu

00:23:53,309 --> 00:23:58,419
that being said it's not like you're

00:23:55,630 --> 00:24:01,450
duplicating the deprecating vert I'll

00:23:58,419 --> 00:24:05,440
block it still considered more of a

00:24:01,450 --> 00:24:08,740
performing choice if you have some case

00:24:05,440 --> 00:24:10,150
of workloads that I really discontent

00:24:08,740 --> 00:24:12,429
see if you're supposed to pick retire

00:24:10,150 --> 00:24:14,770
Bach at least you know have warning

00:24:12,429 --> 00:24:20,080
about that but we can't do that pic for

00:24:14,770 --> 00:24:21,760
you and the distribution again like we

00:24:20,080 --> 00:24:24,250
can see the change people are people are

00:24:21,760 --> 00:24:26,620
adopting and people are actually happy

00:24:24,250 --> 00:24:30,059
so this is the biggest benchmark that's

00:24:26,620 --> 00:24:33,309
available in case of this kind of data

00:24:30,059 --> 00:24:35,409
if people are happy your mailing lists

00:24:33,309 --> 00:24:37,630
are not spent and people don't have

00:24:35,409 --> 00:24:40,780
issues you can't really benchmark it

00:24:37,630 --> 00:24:42,909
because everyone has different kind of

00:24:40,780 --> 00:24:45,580
workloads there are databases there are

00:24:42,909 --> 00:24:47,919
big systems there are test VMs it's all

00:24:45,580 --> 00:24:49,780
mixed together on the other hand you can

00:24:47,919 --> 00:24:53,049
see the distribution changing in favor

00:24:49,780 --> 00:24:56,380
of vertex cuz a while people are happy

00:24:53,049 --> 00:25:00,030
with that quite interesting point about

00:24:56,380 --> 00:25:02,679
this data is that they're still ID and

00:25:00,030 --> 00:25:06,460
we don't really expect ID to die there

00:25:02,679 --> 00:25:07,679
are still cd-roms in IDE which although

00:25:06,460 --> 00:25:12,010
may not be ideal

00:25:07,679 --> 00:25:14,110
at least from perspective of performance

00:25:12,010 --> 00:25:19,630
it's definitely compatible with like

00:25:14,110 --> 00:25:22,690
everything there is more maybe even more

00:25:19,630 --> 00:25:27,159
interesting topic than disks itself are

00:25:22,690 --> 00:25:28,780
IO trends because it turns out almost no

00:25:27,159 --> 00:25:30,760
one although this is dangerous in this

00:25:28,780 --> 00:25:34,480
room but almost no one understands IO

00:25:30,760 --> 00:25:38,230
threads in higher level software let's

00:25:34,480 --> 00:25:41,080
say so this is case of shifting

00:25:38,230 --> 00:25:44,020
responsibility from one layer to the

00:25:41,080 --> 00:25:47,140
other layer so we have PMO that allows

00:25:44,020 --> 00:25:50,919
you to specify I think between 1 and 16

00:25:47,140 --> 00:25:52,990
out read or even whatever number then

00:25:50,919 --> 00:25:56,710
there is slivered that guess what allows

00:25:52,990 --> 00:25:59,080
you to specify whatever number so we

00:25:56,710 --> 00:26:02,530
thought that really great idea to ask

00:25:59,080 --> 00:26:05,559
user to specify number between 1 and 16

00:26:02,530 --> 00:26:08,860
right I think it was limited by 16 but

00:26:05,559 --> 00:26:11,230
I'm not sure about that anyway there was

00:26:08,860 --> 00:26:15,010
just this checkbox very were supposed to

00:26:11,230 --> 00:26:17,429
enter arbitrary number and then was it

00:26:15,010 --> 00:26:20,440
no help no nothing

00:26:17,429 --> 00:26:24,280
yeah turns out

00:26:20,440 --> 00:26:27,670
some people got it right at least in

00:26:24,280 --> 00:26:31,180
terms of what we found out on the other

00:26:27,670 --> 00:26:33,010
hand like this 3 10 16 those are pretty

00:26:31,180 --> 00:26:41,350
interesting number I would love to hear

00:26:33,010 --> 00:26:45,520
background how people chose them so at

00:26:41,350 --> 00:26:49,330
least the expert the experience and tips

00:26:45,520 --> 00:26:52,210
we go the information has reduced the

00:26:49,330 --> 00:26:54,370
choice to one out fret so I don't want

00:26:52,210 --> 00:26:56,760
to say that one out threat is ideal in

00:26:54,370 --> 00:27:01,300
every case that would be that I guess

00:26:56,760 --> 00:27:03,730
but we don't in the UI so in an easy to

00:27:01,300 --> 00:27:06,310
configure way we don't allow people to

00:27:03,730 --> 00:27:07,960
configure arbitrary numbers on the other

00:27:06,310 --> 00:27:10,420
hand since there are definitely

00:27:07,960 --> 00:27:15,490
workloads where this can make sense this

00:27:10,420 --> 00:27:18,520
has to be over a table so this was a

00:27:15,490 --> 00:27:23,530
trip of flexibility devices you know you

00:27:18,520 --> 00:27:25,720
had Verto rng as first device where your

00:27:23,530 --> 00:27:28,140
flexibility didn't take hit it improved

00:27:25,720 --> 00:27:32,050
the performance you had networking where

00:27:28,140 --> 00:27:35,200
it would be similar case then we had

00:27:32,050 --> 00:27:38,080
disks where you start create you start

00:27:35,200 --> 00:27:41,440
having to choose between some kind of

00:27:38,080 --> 00:27:45,910
features and some kind of performance

00:27:41,440 --> 00:27:49,630
benefit now there's the biggest issue

00:27:45,910 --> 00:27:53,140
and it's host devices that's using real

00:27:49,630 --> 00:27:56,530
hardware that means GPUs makes nvme

00:27:53,140 --> 00:27:58,930
disks this is the recommended choice if

00:27:56,530 --> 00:28:01,690
you really want to push performance of

00:27:58,930 --> 00:28:04,690
one aspect to the maximum on the other

00:28:01,690 --> 00:28:09,400
hand for us as the management layer it

00:28:04,690 --> 00:28:12,820
brings various issues so huge pages this

00:28:09,400 --> 00:28:16,120
is different limit if you have a single

00:28:12,820 --> 00:28:18,490
VM on your host that requires GPU and

00:28:16,120 --> 00:28:20,530
you only have one GPU then you are out

00:28:18,490 --> 00:28:24,280
of luck if you start to start another VM

00:28:20,530 --> 00:28:26,830
on top of that there's a new mallow

00:28:24,280 --> 00:28:28,919
locality so I've drawn this picture

00:28:26,830 --> 00:28:32,220
myself by the way

00:28:28,919 --> 00:28:34,980
of a host with two models and some let's

00:28:32,220 --> 00:28:38,880
say four GPUs that may or may not be

00:28:34,980 --> 00:28:40,940
physically for GPUs and eg GPU is

00:28:38,880 --> 00:28:44,010
connected to one of the Numa nodes and

00:28:40,940 --> 00:28:46,320
this is kind of complicated because not

00:28:44,010 --> 00:28:49,140
only you have to think of in which new

00:28:46,320 --> 00:28:51,240
mono the VM should run you also have to

00:28:49,140 --> 00:28:56,700
consider that it should run in the

00:28:51,240 --> 00:28:59,760
gnomon out there your devices are now

00:28:56,700 --> 00:29:02,940
this is not bad bad if all your devices

00:28:59,760 --> 00:29:05,279
are from single Numa node but I believe

00:29:02,940 --> 00:29:09,809
and it was even mentioned in the

00:29:05,279 --> 00:29:11,880
previous talk if you have devices from

00:29:09,809 --> 00:29:14,490
multiple nodes and you have VM spanning

00:29:11,880 --> 00:29:18,899
multiple nodes you have to figure out

00:29:14,490 --> 00:29:21,090
how to pin it so you know your VM may

00:29:18,899 --> 00:29:23,549
have four models because your VMs are

00:29:21,090 --> 00:29:26,279
split between your devices are split

00:29:23,549 --> 00:29:31,289
between four mo nodes it gets really

00:29:26,279 --> 00:29:33,630
complicated and this is something that I

00:29:31,289 --> 00:29:38,909
would love to solve automatically at

00:29:33,630 --> 00:29:41,580
some point the problem is this by itself

00:29:38,909 --> 00:29:43,260
requires us to Fred in the models and

00:29:41,580 --> 00:29:45,419
that was our safe limit we are not doing

00:29:43,260 --> 00:29:47,720
that we are only warning the user so

00:29:45,419 --> 00:29:51,690
this gets really complicated and

00:29:47,720 --> 00:29:53,789
unfortunately it's not solved yet if you

00:29:51,690 --> 00:29:57,419
have this kind of setup you know you

00:29:53,789 --> 00:30:00,570
pretty much have to tune it manually one

00:29:57,419 --> 00:30:02,130
positive thing about host devices is

00:30:00,570 --> 00:30:03,750
that there are more and more devices

00:30:02,130 --> 00:30:06,120
with some kind of virtualization

00:30:03,750 --> 00:30:10,260
capabilities so you have next witness

00:30:06,120 --> 00:30:13,889
our Iove you have GPUs with v GPU or g v

00:30:10,260 --> 00:30:17,220
TG or you may hopefully soon have

00:30:13,889 --> 00:30:20,399
storage with NPI v that's very case

00:30:17,220 --> 00:30:24,000
that's helping at least in the sense

00:30:20,399 --> 00:30:28,230
that you can use device for more VMS

00:30:24,000 --> 00:30:30,870
than just single one now there are some

00:30:28,230 --> 00:30:33,539
other devices that it makes sense for

00:30:30,870 --> 00:30:36,389
management software to include so image

00:30:33,539 --> 00:30:40,820
in graphics video we wanted for spice or

00:30:36,389 --> 00:30:42,570
VNC sound card you just want sound card

00:30:40,820 --> 00:30:44,669
USB stand there

00:30:42,570 --> 00:30:47,519
smartcard watch dog and balloon these

00:30:44,669 --> 00:30:49,740
are devices that we really include by by

00:30:47,519 --> 00:30:51,450
default because you know there's maybe

00:30:49,740 --> 00:30:54,110
someone who needs smart card in their

00:30:51,450 --> 00:30:56,639
VMs and you know instead of making that

00:30:54,110 --> 00:30:58,710
explicit choice it's just better that if

00:30:56,639 --> 00:31:02,309
you start the VM you just find oh yeah

00:30:58,710 --> 00:31:04,190
it is it has a smarter unfortunately

00:31:02,309 --> 00:31:08,630
when we start thinking of performance

00:31:04,190 --> 00:31:10,830
sometimes they're conflicting choices or

00:31:08,630 --> 00:31:14,399
it's questionable whether some of the

00:31:10,830 --> 00:31:16,049
devices are needed in some cases we have

00:31:14,399 --> 00:31:18,600
no idea whether there's some performance

00:31:16,049 --> 00:31:21,590
hit benefit

00:31:18,600 --> 00:31:26,009
so consider removal of some maybe

00:31:21,590 --> 00:31:30,840
reducing the surface their performance

00:31:26,009 --> 00:31:34,139
bottlenecks can be and unless it hurts

00:31:30,840 --> 00:31:35,809
it hurts the use case remove removing

00:31:34,139 --> 00:31:37,529
them should be quite simple

00:31:35,809 --> 00:31:40,259
unfortunately we really don't have

00:31:37,529 --> 00:31:42,330
performance data and if someone was

00:31:40,259 --> 00:31:48,960
working on that I'm really interested to

00:31:42,330 --> 00:31:51,659
see there may be one trick we've talked

00:31:48,960 --> 00:31:54,779
about removing video and graphics

00:31:51,659 --> 00:31:56,759
devices well if you do that you have

00:31:54,779 --> 00:31:59,730
just lost your graphic graphical

00:31:56,759 --> 00:32:03,539
connection to the VM which is not

00:31:59,730 --> 00:32:07,259
exactly practical normally what you do

00:32:03,539 --> 00:32:10,409
is use console this is getting more

00:32:07,259 --> 00:32:12,360
complicated if your data center is in

00:32:10,409 --> 00:32:15,179
some kind of subnet that's not

00:32:12,360 --> 00:32:17,639
accessible so for that you need some

00:32:15,179 --> 00:32:21,120
kind of proxy so this is all doable it's

00:32:17,639 --> 00:32:25,049
just one complication on top of another

00:32:21,120 --> 00:32:29,730
that's making the use of your software

00:32:25,049 --> 00:32:32,639
less convenient so in the end how we

00:32:29,730 --> 00:32:34,649
implement it all this well in three

00:32:32,639 --> 00:32:38,039
tiers of changes there are some of the

00:32:34,649 --> 00:32:41,909
safe changes so making convert Iorg the

00:32:38,039 --> 00:32:44,610
default or if we have host devices in a

00:32:41,909 --> 00:32:46,320
single new mono then we just make sure

00:32:44,610 --> 00:32:48,629
that's the preferred one out for the VM

00:32:46,320 --> 00:32:53,220
we've actually used the mode preferred

00:32:48,629 --> 00:32:55,120
not strict again to realize the

00:32:53,220 --> 00:32:57,490
responsibility that we have

00:32:55,120 --> 00:33:00,999
on the other hand there are some kind of

00:32:57,490 --> 00:33:04,179
warnings so if you're creating a VM that

00:33:00,999 --> 00:33:08,200
you expect to perform on the top level

00:33:04,179 --> 00:33:09,519
or some kind of high performance VM then

00:33:08,200 --> 00:33:13,480
you should probably use the huge pages

00:33:09,519 --> 00:33:17,019
or in case you are violating or there is

00:33:13,480 --> 00:33:19,240
chance of you violating the Numa and

00:33:17,019 --> 00:33:23,289
this is the warning like make sure this

00:33:19,240 --> 00:33:25,090
is working maybe in future and that

00:33:23,289 --> 00:33:27,460
would be beautiful would be really

00:33:25,090 --> 00:33:30,730
boring about possibly use case of host

00:33:27,460 --> 00:33:33,190
devices seeing maybe inspecting the

00:33:30,730 --> 00:33:34,720
workload of the VM and telling Co there

00:33:33,190 --> 00:33:37,480
are these VMs running this and this it

00:33:34,720 --> 00:33:39,519
might make sense to sorry OB and that

00:33:37,480 --> 00:33:40,990
there's some there are few changes in

00:33:39,519 --> 00:33:44,440
default so it was the Vertigo block

00:33:40,990 --> 00:33:46,659
toward a scuzzy transition or the i/o

00:33:44,440 --> 00:33:50,019
treads change that I'm really interested

00:33:46,659 --> 00:33:53,080
in seeing the data in the future so this

00:33:50,019 --> 00:33:56,649
is don't try to relate it's just an

00:33:53,080 --> 00:33:59,350
example how that could look this is case

00:33:56,649 --> 00:34:01,629
of huge pages so if you don't have huge

00:33:59,350 --> 00:34:03,909
pages and you have high-performance VMs

00:34:01,629 --> 00:34:07,869
this tells you oh you may be doing

00:34:03,909 --> 00:34:10,540
something wrong you're not maybe you're

00:34:07,869 --> 00:34:13,359
not drunk all the time but make sure you

00:34:10,540 --> 00:34:17,050
know what you're doing in this case it's

00:34:13,359 --> 00:34:19,510
supposed to be a knowing I guess some

00:34:17,050 --> 00:34:23,280
benchmarks and this is kind of a letdown

00:34:19,510 --> 00:34:26,139
in a sense that benchmarks that would

00:34:23,280 --> 00:34:28,119
probably show the benefit the most are

00:34:26,139 --> 00:34:31,720
these that you actually have to hire

00:34:28,119 --> 00:34:34,929
expert to run so you have this kind of s

00:34:31,720 --> 00:34:39,190
AP benchmarks and other proprietary

00:34:34,929 --> 00:34:42,129
benchmarks these improvements are mainly

00:34:39,190 --> 00:34:43,750
aimed at those kind of applications and

00:34:42,129 --> 00:34:46,329
these are the ones that we can really

00:34:43,750 --> 00:34:48,730
test so I've tried some synthetic

00:34:46,329 --> 00:34:51,069
benchmarks that were supposed to be

00:34:48,730 --> 00:34:54,369
sanity tests so the first one is

00:34:51,069 --> 00:34:56,139
actually the one that may take advantage

00:34:54,369 --> 00:35:02,470
of the changes it's PG bench so it's

00:34:56,139 --> 00:35:04,920
from it's running Postgres benchmark it

00:35:02,470 --> 00:35:07,599
was like hundred thousand transaction

00:35:04,920 --> 00:35:08,860
transactions and this was pretty much

00:35:07,599 --> 00:35:12,490
the result so there

00:35:08,860 --> 00:35:15,760
performance improvement of 10% which was

00:35:12,490 --> 00:35:19,330
pretty nice on the other hand encoding

00:35:15,760 --> 00:35:21,970
flag shouldn't really be different on

00:35:19,330 --> 00:35:24,880
this kind of of configuration so there

00:35:21,970 --> 00:35:27,340
was roughly 0.1% improvement which is

00:35:24,880 --> 00:35:32,500
more of a statistical error on the other

00:35:27,340 --> 00:35:34,780
hand this the results will be visible in

00:35:32,500 --> 00:35:37,660
the next year maybe in the next two

00:35:34,780 --> 00:35:40,030
years as people start creating VMs with

00:35:37,660 --> 00:35:42,370
this kind of configuration and actually

00:35:40,030 --> 00:35:47,770
report their experience that's the most

00:35:42,370 --> 00:35:51,340
important point so the summary kind of

00:35:47,770 --> 00:35:55,090
goes in the spirit of for the users

00:35:51,340 --> 00:35:58,750
whatever automation does for you in this

00:35:55,090 --> 00:36:00,640
area you still have to benchmark and

00:35:58,750 --> 00:36:05,230
tune your workload accordingly because

00:36:00,640 --> 00:36:07,960
there's no magic guessing this is almost

00:36:05,230 --> 00:36:10,300
like if you wanted maximum performance

00:36:07,960 --> 00:36:12,970
we would have to take a silver ball and

00:36:10,300 --> 00:36:15,400
just let it guess the configuration so

00:36:12,970 --> 00:36:18,520
make sure the benchmark the workload and

00:36:15,400 --> 00:36:22,390
see how the change is affected and keep

00:36:18,520 --> 00:36:27,130
going in this sense and the second line

00:36:22,390 --> 00:36:29,380
is aimed at KVM developers that there

00:36:27,130 --> 00:36:32,080
are some ways that you can help us

00:36:29,380 --> 00:36:34,180
making these decisions and seeing things

00:36:32,080 --> 00:36:36,880
like good commit messages that was the

00:36:34,180 --> 00:36:39,010
case with l3 cache where the common

00:36:36,880 --> 00:36:42,100
message explained what it does there are

00:36:39,010 --> 00:36:46,120
some benchmarks it was just pretty easy

00:36:42,100 --> 00:36:49,210
to adopt of course that depends on you

00:36:46,120 --> 00:36:50,800
like I'm not saying that everything is

00:36:49,210 --> 00:36:52,570
wrong it's just that sometimes it's

00:36:50,800 --> 00:36:55,150
really hard to figure out whether

00:36:52,570 --> 00:36:58,810
something should be used or should not

00:36:55,150 --> 00:37:03,720
be used and is it from me so if you have

00:36:58,810 --> 00:37:03,720
any questions or yeah go ahead

00:37:09,150 --> 00:37:14,890
so my remark does it work yes

00:37:12,610 --> 00:37:18,270
so my remark is about the host devices I

00:37:14,890 --> 00:37:22,450
think you made a good point that it's

00:37:18,270 --> 00:37:25,030
really the most important part of

00:37:22,450 --> 00:37:28,630
getting high performance devices and I

00:37:25,030 --> 00:37:31,780
think I have two remarks on that one of

00:37:28,630 --> 00:37:36,250
that one remark is that having a host

00:37:31,780 --> 00:37:38,650
device doesn't actually require using

00:37:36,250 --> 00:37:40,480
pci pesto for that device even if it's

00:37:38,650 --> 00:37:42,700
an nvme device for example you could

00:37:40,480 --> 00:37:46,840
still be using data block to expose it

00:37:42,700 --> 00:37:49,440
but still have the pinning so having

00:37:46,840 --> 00:37:52,630
something in the UI that says pinning

00:37:49,440 --> 00:37:54,040
this disc to this physical disc is

00:37:52,630 --> 00:37:58,090
probably more important than actually

00:37:54,040 --> 00:38:00,700
doing Silv of that disk because having

00:37:58,090 --> 00:38:05,320
the local storage is what really matters

00:38:00,700 --> 00:38:07,060
and at that point if it's vertical block

00:38:05,320 --> 00:38:09,070
device for example you could put the

00:38:07,060 --> 00:38:13,450
youth head on the same Numa node as the

00:38:09,070 --> 00:38:15,760
physical hard hardware as well so in

00:38:13,450 --> 00:38:19,120
that case the answer of one eye or

00:38:15,760 --> 00:38:21,550
thread per VM is the best one but in

00:38:19,120 --> 00:38:25,360
addition to that you might have one per

00:38:21,550 --> 00:38:30,460
Newman or the if there are pin device

00:38:25,360 --> 00:38:35,110
form from other Numa nodes and likewise

00:38:30,460 --> 00:38:37,010
for example if it's in the future we are

00:38:35,110 --> 00:38:40,280
going to have for example the

00:38:37,010 --> 00:38:43,609
device driver direct link um possibly

00:38:40,280 --> 00:38:46,820
then you could still expose it as a

00:38:43,609 --> 00:38:49,550
potato block device but use the nvme

00:38:46,820 --> 00:38:52,880
driver in chemo so my suggestion

00:38:49,550 --> 00:38:57,380
regarding the UI is to the couple the

00:38:52,880 --> 00:38:59,180
pinning from the pestle and the pinning

00:38:57,380 --> 00:39:01,220
is just a requirement for the past or

00:38:59,180 --> 00:39:04,210
you can still use vertical block and

00:39:01,220 --> 00:39:07,490
vertigo Scaasi or whatever you want even

00:39:04,210 --> 00:39:09,020
if you choose to pin and the same for

00:39:07,490 --> 00:39:11,060
scuzzy pass-through you can have a

00:39:09,020 --> 00:39:13,820
scuzzy device that it's that one it's a

00:39:11,060 --> 00:39:15,440
direct path to but still wanted for beta

00:39:13,820 --> 00:39:21,200
block for a little bit more of personal

00:39:15,440 --> 00:39:23,330
performance for example I believe you

00:39:21,200 --> 00:39:26,420
have kind of skipped a year ahead

00:39:23,330 --> 00:39:31,550
because this is kind of in the making

00:39:26,420 --> 00:39:33,080
but it's added Thor level so that set

00:39:31,550 --> 00:39:34,580
like your thoughts are extremely

00:39:33,080 --> 00:39:37,520
valuable and you could actually help us

00:39:34,580 --> 00:39:40,010
build it that would be awesome

00:39:37,520 --> 00:39:42,500
but yeah this is this was all in the

00:39:40,010 --> 00:39:44,600
thinking unfortunately like something

00:39:42,500 --> 00:39:47,330
like that we couldn't make just yet

00:39:44,600 --> 00:39:49,369
because it's this itself was a lot of

00:39:47,330 --> 00:39:51,170
work even if it doesn't look like that

00:39:49,369 --> 00:39:54,609
like rating the glass and everything is

00:39:51,170 --> 00:39:57,890
apparently complex so yeah thank you I

00:39:54,609 --> 00:40:01,840
would like to put attention to two

00:39:57,890 --> 00:40:05,359
topics first topic is huge pages and KSM

00:40:01,840 --> 00:40:09,380
interaction once chasm starts to merge

00:40:05,359 --> 00:40:12,109
pages you're transparent you're huge

00:40:09,380 --> 00:40:15,980
pages will become dissolved and you will

00:40:12,109 --> 00:40:19,480
find the performance a bit degraded so

00:40:15,980 --> 00:40:25,070
if they are going to the system near

00:40:19,480 --> 00:40:31,490
overcome it a difficult choice start

00:40:25,070 --> 00:40:34,430
chasm or not stop chasm but but we still

00:40:31,490 --> 00:40:37,609
want want to get a best performance in

00:40:34,430 --> 00:40:40,160
the conditions we have yeah we currently

00:40:37,609 --> 00:40:41,240
disabled the key in the chasm that was

00:40:40,160 --> 00:40:44,210
the choice

00:40:41,240 --> 00:40:48,140
and and the second question is about

00:40:44,210 --> 00:40:48,890
iron mode you will have or your virtual

00:40:48,140 --> 00:40:51,710
disc

00:40:48,890 --> 00:40:54,980
do you have any statistics whether you

00:40:51,710 --> 00:40:58,160
use Linux negative back end all wars

00:40:54,980 --> 00:41:00,710
reddit back-end cache uncashed mode

00:40:58,160 --> 00:41:02,780
because this has a big impact

00:41:00,710 --> 00:41:04,900
yeah I actually don't have the

00:41:02,780 --> 00:41:08,690
statistics like for this presentation

00:41:04,900 --> 00:41:10,730
I've did some kind of research for a

00:41:08,690 --> 00:41:14,570
blog post that you could find under my

00:41:10,730 --> 00:41:17,300
handle but the other isn't the rough

00:41:14,570 --> 00:41:21,230
testing from my point of view in this

00:41:17,300 --> 00:41:23,330
regard and it's also the main reason why

00:41:21,230 --> 00:41:26,690
you know this we would need to benchmark

00:41:23,330 --> 00:41:32,600
this and this is usually your based on

00:41:26,690 --> 00:41:34,460
shared storage right so sorry we are out

00:41:32,600 --> 00:41:38,080
of the time so if you have any question

00:41:34,460 --> 00:41:38,080
you can discuss its marked indirectly

00:41:39,880 --> 00:41:50,379
[Applause]

00:41:44,510 --> 00:41:50,379

YouTube URL: https://www.youtube.com/watch?v=_SlUlQRcnQg


