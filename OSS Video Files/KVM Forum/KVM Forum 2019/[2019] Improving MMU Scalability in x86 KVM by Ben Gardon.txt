Title: [2019] Improving MMU Scalability in x86 KVM by Ben Gardon
Publication date: 2019-11-12
Playlist: KVM Forum 2019
Description: 
	The x86 KVM MMU has significant scaling issues with many VCPUs and lots of RAM. Over the last year, we have made substantial improvements to the x86 KVM MMU in the direct-mapped TDP case, to reduce lock contention and memory overheads, with the goal of migrating VMs with 416 VCPUs and 12TB of memory. With these changes, the x86 KVM MMU can handle EPT/NPT violations from all VCPUs in parallel, requires ~99% less MMU memory overhead in steady state with 2M pages, simplifies the implementation of MMU operations, and more. This talk will cover new synchronization models, abstractions, and data structures, and details of the performance we have gained from them.

---

Ben Gardon
Google
Software Engineer

I work to make the x86 KVM MMU more scalable and performant.
Captions: 
	00:00:00,390 --> 00:00:02,750
[Music]

00:00:06,799 --> 00:00:11,700
hello everybody my name is Ben and today

00:00:10,620 --> 00:00:14,580
I'm gonna be talking about some

00:00:11,700 --> 00:00:16,260
improvements we've made to the KVM x86

00:00:14,580 --> 00:00:20,160
MMU over the last year and a half

00:00:16,260 --> 00:00:23,010
specifically the TDP direct case so over

00:00:20,160 --> 00:00:24,960
time VMS are getting larger last year my

00:00:23,010 --> 00:00:27,779
colleague canon came and gave a talk

00:00:24,960 --> 00:00:32,550
about demand paging with 160 V CPUs and

00:00:27,779 --> 00:00:34,950
this year were back with 416 B CPUs so

00:00:32,550 --> 00:00:37,200
we've we've got these two VMs with 12

00:00:34,950 --> 00:00:39,480
terabytes of RAM and we still need to

00:00:37,200 --> 00:00:42,329
live migrate them for host maintenance

00:00:39,480 --> 00:00:45,149
and software upgrades and we want to run

00:00:42,329 --> 00:00:47,489
long-lived latency-sensitive workloads

00:00:45,149 --> 00:00:51,149
on these VMs and we need to be able to

00:00:47,489 --> 00:00:52,710
migrate them under load so as we add V

00:00:51,149 --> 00:00:55,230
CPUs and memory obviously it becomes

00:00:52,710 --> 00:00:58,079
harder to live migrate VMs and when we

00:00:55,230 --> 00:01:00,719
started development on these 416 BC few

00:00:58,079 --> 00:01:03,149
VMs we found that they softlocked and

00:01:00,719 --> 00:01:06,229
crashed we tried to migrate them and so

00:01:03,149 --> 00:01:08,580
we set out to fix this problem so

00:01:06,229 --> 00:01:12,060
specifically the stage of migration that

00:01:08,580 --> 00:01:13,979
crashed the VMS was demand paging which

00:01:12,060 --> 00:01:15,990
is the last stage of live migration so

00:01:13,979 --> 00:01:18,390
we've we've transferred most of guest

00:01:15,990 --> 00:01:20,970
memory we've stopped the V CPUs on the

00:01:18,390 --> 00:01:23,280
under source host and we started them on

00:01:20,970 --> 00:01:26,250
the target and now the memory transfers

00:01:23,280 --> 00:01:28,560
initiated by the V CPU page faults so

00:01:26,250 --> 00:01:30,420
with user fault ft the guest tries to

00:01:28,560 --> 00:01:32,670
access some page it takes an EPT

00:01:30,420 --> 00:01:35,100
violation that results in a host page

00:01:32,670 --> 00:01:37,619
fault and then a user fault thread

00:01:35,100 --> 00:01:40,560
pulling on a user fault FD is woken up

00:01:37,619 --> 00:01:42,630
it copies the guest memory in to the the

00:01:40,560 --> 00:01:44,070
backing physical memory and then the

00:01:42,630 --> 00:01:48,600
page faults are fixed and the guest can

00:01:44,070 --> 00:01:50,640
receive and so to get an idea of what

00:01:48,600 --> 00:01:52,619
exactly was was problematic when we were

00:01:50,640 --> 00:01:56,369
doing demand paging with a lot of V CPUs

00:01:52,619 --> 00:01:58,649
I wrote this KVM self-test demand paging

00:01:56,369 --> 00:02:01,229
test and the idea here was that we

00:01:58,649 --> 00:02:04,049
wanted a demand paging micro benchmark

00:02:01,229 --> 00:02:06,689
which would simulate a user space where

00:02:04,049 --> 00:02:08,399
we had no network overheads and we had

00:02:06,689 --> 00:02:11,160
perfectly tuned the user space for the

00:02:08,399 --> 00:02:12,750
guest workload so we take guest memory

00:02:11,160 --> 00:02:14,370
and we chop it up into n

00:02:12,750 --> 00:02:17,580
segments and then for each of those

00:02:14,370 --> 00:02:19,710
segments we have one V CPU and the DCP

00:02:17,580 --> 00:02:21,960
you will will access memory and then for

00:02:19,710 --> 00:02:24,360
each segment we have a user fault FD

00:02:21,960 --> 00:02:26,520
that's that's polling and it's waiting

00:02:24,360 --> 00:02:28,200
for the V CPU to take a page fault then

00:02:26,520 --> 00:02:30,630
when the V CPUs take page faults the

00:02:28,200 --> 00:02:34,050
user fault FG fixes them and the V CPUs

00:02:30,630 --> 00:02:36,810
can proceed and the purpose of this was

00:02:34,050 --> 00:02:39,780
to was to push as much of the problem

00:02:36,810 --> 00:02:41,430
into KVM and out of the main mm

00:02:39,780 --> 00:02:45,060
subsystem and the user space

00:02:41,430 --> 00:02:47,370
implementation and so we ran this

00:02:45,060 --> 00:02:49,410
benchmark with Ford and 16 V CPUs and we

00:02:47,370 --> 00:02:52,440
did a perf trace on it and we found that

00:02:49,410 --> 00:02:54,720
98% at the time was spent waiting for

00:02:52,440 --> 00:02:56,280
the kV mmm u-lock and so as these

00:02:54,720 --> 00:02:58,110
deceive us take page faults they all

00:02:56,280 --> 00:02:59,550
want to take the MMU lock to handle

00:02:58,110 --> 00:03:02,100
their page faults and they queue up

00:02:59,550 --> 00:03:06,260
behind each other and this leads to a

00:03:02,100 --> 00:03:08,670
lock in tension and the guest workload

00:03:06,260 --> 00:03:10,620
runs very slowly and then the soft

00:03:08,670 --> 00:03:13,500
lockup detector and the guest triggers

00:03:10,620 --> 00:03:15,600
and the guest crashes and so this talk

00:03:13,500 --> 00:03:18,239
is really about how we handled page

00:03:15,600 --> 00:03:20,519
faults in parallel in order to eliminate

00:03:18,239 --> 00:03:24,450
this lock contention and speed up demand

00:03:20,519 --> 00:03:26,280
paging by 90% so why should you care

00:03:24,450 --> 00:03:29,310
about parallel page fault handling I

00:03:26,280 --> 00:03:31,320
know that not everybody who uses KVM

00:03:29,310 --> 00:03:33,570
uses demand paging to the same extent we

00:03:31,320 --> 00:03:35,730
do but there's a lot of other operations

00:03:33,570 --> 00:03:37,260
that cause really high page fault rates

00:03:35,730 --> 00:03:40,200
for example when you disable dirty

00:03:37,260 --> 00:03:43,709
logging potentially all the entries in

00:03:40,200 --> 00:03:45,630
the EPT could be cleared and the lock

00:03:43,709 --> 00:03:46,980
contention that that causes this this

00:03:45,630 --> 00:03:49,320
bad performance of parallel page file

00:03:46,980 --> 00:03:52,380
handling is a fundamental MMU scaling

00:03:49,320 --> 00:03:54,150
issue so as we want to run more vc views

00:03:52,380 --> 00:03:56,970
and run with more ram we need to

00:03:54,150 --> 00:03:58,890
paralyze various MMU operations and the

00:03:56,970 --> 00:04:01,019
page fault handler is the most

00:03:58,890 --> 00:04:03,390
complicated Mme operation and so if we

00:04:01,019 --> 00:04:05,100
can paralyze that then we have an easier

00:04:03,390 --> 00:04:07,530
road to paralyzing a lot of other enemy

00:04:05,100 --> 00:04:08,880
operations and so when we were scoping

00:04:07,530 --> 00:04:10,739
out this work one of the things that we

00:04:08,880 --> 00:04:12,420
found was that the existing shadow

00:04:10,739 --> 00:04:15,229
paging implementation which uses our

00:04:12,420 --> 00:04:17,970
maps and the the structs kV MMU pages is

00:04:15,229 --> 00:04:19,829
really complicated and difficult to

00:04:17,970 --> 00:04:21,540
paralyze and there's not a lot of

00:04:19,829 --> 00:04:23,790
unified

00:04:21,540 --> 00:04:25,650
code it's all sort of done at hawk in

00:04:23,790 --> 00:04:27,030
various different places and so we

00:04:25,650 --> 00:04:29,880
wanted to start from the ground up and

00:04:27,030 --> 00:04:32,130
build a scalable MMU for this non nested

00:04:29,880 --> 00:04:34,050
TB case which i think is the most common

00:04:32,130 --> 00:04:36,480
case that most public cloud providers

00:04:34,050 --> 00:04:38,220
are running and so I'm going to break

00:04:36,480 --> 00:04:40,470
this this solving this problem up into

00:04:38,220 --> 00:04:42,420
two segments one is sort of how we deal

00:04:40,470 --> 00:04:43,950
with this data structure problem of how

00:04:42,420 --> 00:04:46,380
are we going to modify this tree with a

00:04:43,950 --> 00:04:47,580
bunch of data in it concurrently and

00:04:46,380 --> 00:04:50,610
then we're going to talk about some of

00:04:47,580 --> 00:04:54,510
the hardware concerns that running vcp

00:04:50,610 --> 00:04:57,780
use provide so the purpose of the KVM

00:04:54,510 --> 00:04:59,280
MMU in the in the TDP direct case is is

00:04:57,780 --> 00:05:00,930
really just to translate guest physical

00:04:59,280 --> 00:05:03,360
addresses to host physical addresses and

00:05:00,930 --> 00:05:06,000
so we've we've got this MIT this mapping

00:05:03,360 --> 00:05:07,260
in the mem slots that translates s

00:05:06,000 --> 00:05:09,270
physical addresses to host virtual

00:05:07,260 --> 00:05:10,860
addresses and then we've got the mapping

00:05:09,270 --> 00:05:12,480
and the host page tables translating

00:05:10,860 --> 00:05:14,160
host virtual addresses to physical

00:05:12,480 --> 00:05:15,960
addresses and when either of these

00:05:14,160 --> 00:05:18,270
mappings change those changes need to

00:05:15,960 --> 00:05:19,340
get propagated in a timely manner to the

00:05:18,270 --> 00:05:23,070
EPT

00:05:19,340 --> 00:05:25,290
and the the one specifically the host

00:05:23,070 --> 00:05:27,360
page tables changed the way that KVM

00:05:25,290 --> 00:05:29,880
finds out about this are through the MMU

00:05:27,360 --> 00:05:32,100
notifiers so the MMU notifiers are at a

00:05:29,880 --> 00:05:34,620
generic interface by which the memory

00:05:32,100 --> 00:05:36,510
management subsystem tells secondary mmm

00:05:34,620 --> 00:05:39,210
use that that the page table mappings

00:05:36,510 --> 00:05:40,680
have changed and in this case there's

00:05:39,210 --> 00:05:42,060
six of them but we're just going to talk

00:05:40,680 --> 00:05:43,740
about these first two and validate

00:05:42,060 --> 00:05:45,300
arrange start and end because I think

00:05:43,740 --> 00:05:46,650
they motivate a lot of the the

00:05:45,300 --> 00:05:50,730
synchronization changes that we're gonna

00:05:46,650 --> 00:05:53,130
make here so how do these work so let's

00:05:50,730 --> 00:05:54,540
say the main mm wants to remap some

00:05:53,130 --> 00:05:56,370
region of physical memory maybe it's

00:05:54,540 --> 00:05:59,250
doing this for swap or maybe it's doing

00:05:56,370 --> 00:06:00,230
this for for using huge pages or

00:05:59,250 --> 00:06:03,150
something like that

00:06:00,230 --> 00:06:06,000
so KVM will get an invalidate arrange

00:06:03,150 --> 00:06:08,070
start and in response that needs to

00:06:06,000 --> 00:06:10,110
prevent access to that range of memory

00:06:08,070 --> 00:06:12,240
so it gets a host virtual address range

00:06:10,110 --> 00:06:14,040
and then that corresponds in this case

00:06:12,240 --> 00:06:17,490
to a guest physical address range that

00:06:14,040 --> 00:06:19,500
it needs to prevent access to once it's

00:06:17,490 --> 00:06:22,140
prevented access by removing the EPT

00:06:19,500 --> 00:06:24,390
mappings the main mm can remap that

00:06:22,140 --> 00:06:26,370
memory and then we get an ax validate at

00:06:24,390 --> 00:06:27,990
range n which means that we can restore

00:06:26,370 --> 00:06:29,370
these mappings which will in this case

00:06:27,990 --> 00:06:32,160
point to different physical addresses

00:06:29,370 --> 00:06:32,810
now and the v cpus can proceed to access

00:06:32,160 --> 00:06:35,720
that memory

00:06:32,810 --> 00:06:37,490
and importantly when when we get these

00:06:35,720 --> 00:06:39,980
invalid a range start and ends we can't

00:06:37,490 --> 00:06:41,870
let the V CPU page fault handlers run

00:06:39,980 --> 00:06:43,010
concurrently with invalid a range start

00:06:41,870 --> 00:06:46,460
because we wouldn't be able to provide

00:06:43,010 --> 00:06:48,680
some of the guarantees we need to so

00:06:46,460 --> 00:06:50,660
that's that's MMU notifiers in a

00:06:48,680 --> 00:06:52,070
nutshell and there's two actors in this

00:06:50,660 --> 00:06:53,360
there's the MM unit of fire so I just

00:06:52,070 --> 00:06:55,850
talked about and then there's the

00:06:53,360 --> 00:06:58,100
parallel page fault handlers and so I've

00:06:55,850 --> 00:06:59,720
broken down the changes that we needed

00:06:58,100 --> 00:07:01,460
to make for parallel page fault handling

00:06:59,720 --> 00:07:02,780
into these five requirements and I'm

00:07:01,460 --> 00:07:05,480
going to go through each one and talk

00:07:02,780 --> 00:07:06,860
about how we chipped it so the first is

00:07:05,480 --> 00:07:08,840
that we need to be able to concurrently

00:07:06,860 --> 00:07:10,700
access page table memory from a bunch of

00:07:08,840 --> 00:07:14,480
threads and then we need to be able to

00:07:10,700 --> 00:07:15,860
safely free that page table memory so in

00:07:14,480 --> 00:07:18,980
order to do this we introduce this

00:07:15,860 --> 00:07:20,660
direct walk iterator and currently KPM

00:07:18,980 --> 00:07:24,140
has has a bunch of iterators including

00:07:20,660 --> 00:07:25,970
our map iterators and hva iterators but

00:07:24,140 --> 00:07:27,950
the direct walk iterator just implements

00:07:25,970 --> 00:07:32,240
a preorder traversal of the

00:07:27,950 --> 00:07:34,970
two-dimensional paging structures and it

00:07:32,240 --> 00:07:36,620
provides a common pattern for MMU

00:07:34,970 --> 00:07:39,200
operations for all the MMU sort of

00:07:36,620 --> 00:07:40,820
feature functions in which they set up

00:07:39,200 --> 00:07:42,440
some traversal range they modify all the

00:07:40,820 --> 00:07:45,919
page table entries in that range based

00:07:42,440 --> 00:07:47,690
on their their their value or the guest

00:07:45,919 --> 00:07:49,220
physical address they map and then at

00:07:47,690 --> 00:07:52,340
the end it'll clean up and synchronize

00:07:49,220 --> 00:07:54,770
the V CPU caches and the big benefit to

00:07:52,340 --> 00:07:56,390
this direct walk interrater is that it

00:07:54,770 --> 00:07:58,970
transparently handles a lot of the

00:07:56,390 --> 00:08:00,770
synchronization concerns associated with

00:07:58,970 --> 00:08:02,870
modifying page table entries that we're

00:08:00,770 --> 00:08:04,700
going to talk about in this talk and it

00:08:02,870 --> 00:08:06,950
unifies the code for handling these

00:08:04,700 --> 00:08:08,720
changes which currently is distributed

00:08:06,950 --> 00:08:12,979
around a ton of functions all over the

00:08:08,720 --> 00:08:15,200
MMU the other benefit to the direct walk

00:08:12,979 --> 00:08:16,729
iterator is that because we have only

00:08:15,200 --> 00:08:18,530
one paging structure since we're not

00:08:16,729 --> 00:08:21,710
running a nested guest and we're not

00:08:18,530 --> 00:08:23,330
doing x86 shadow paging we can paralyze

00:08:21,710 --> 00:08:24,590
our accesses really well so if we're in

00:08:23,330 --> 00:08:26,180
two different branches of the paging

00:08:24,590 --> 00:08:28,550
structure threads aren't going to step

00:08:26,180 --> 00:08:30,350
on each other's toes and we get some

00:08:28,550 --> 00:08:32,539
free memory savings so we can avoid

00:08:30,350 --> 00:08:35,180
allocating the our map in this case and

00:08:32,539 --> 00:08:39,380
we can avoid allocating any structs KVM

00:08:35,180 --> 00:08:43,640
MMU pages and on our 12 terabytes in PCB

00:08:39,380 --> 00:08:45,050
um C V CPU VMs that saves us about 24

00:08:43,640 --> 00:08:47,620
gigabytes which is a pretty substantial

00:08:45,050 --> 00:08:47,620
saving

00:08:48,180 --> 00:08:52,780
so the director waka trader allows us to

00:08:50,770 --> 00:08:54,820
efficiently access page table memory in

00:08:52,780 --> 00:08:57,670
parallel but we also need to be able to

00:08:54,820 --> 00:08:59,680
free that page table memory and so we do

00:08:57,670 --> 00:09:01,750
this with our Cu the direct walk

00:08:59,680 --> 00:09:03,460
iterator will we'll grab an RS you read

00:09:01,750 --> 00:09:05,740
lock whenever a traversal if the paging

00:09:03,460 --> 00:09:08,080
structure starts and then it'll release

00:09:05,740 --> 00:09:11,170
it when it needs to yield or reschedule

00:09:08,080 --> 00:09:13,120
and whenever we change a page table

00:09:11,170 --> 00:09:15,130
entry from present and maybe pointing

00:09:13,120 --> 00:09:18,580
some page of page table memory or a page

00:09:15,130 --> 00:09:20,620
of physical memory to to non present or

00:09:18,580 --> 00:09:22,930
pointing somewhere else then the direct

00:09:20,620 --> 00:09:25,030
walk iterator handles freeing the page

00:09:22,930 --> 00:09:26,980
after an RC grace period and that

00:09:25,030 --> 00:09:28,600
ensures that all of our all of our k vm

00:09:26,980 --> 00:09:32,740
threads that are walking the page tables

00:09:28,600 --> 00:09:34,450
don't access freed memory so now that we

00:09:32,740 --> 00:09:35,950
can safely access all the memory we need

00:09:34,450 --> 00:09:37,840
for these for this paging structure we

00:09:35,950 --> 00:09:39,340
need to be able to change it and we need

00:09:37,840 --> 00:09:41,020
to be able to track these changes and

00:09:39,340 --> 00:09:43,150
because we're handling page faults in

00:09:41,020 --> 00:09:46,150
parallel a lot of the page table entries

00:09:43,150 --> 00:09:48,280
could be changing quite fast and so we

00:09:46,150 --> 00:09:49,660
replace all the operations on a page

00:09:48,280 --> 00:09:53,710
table memory that exists in the current

00:09:49,660 --> 00:09:56,350
MMU with atomic compare exchanges and on

00:09:53,710 --> 00:09:58,780
success the direct walk iterator then

00:09:56,350 --> 00:10:00,460
knows how to handle all the bookkeeping

00:09:58,780 --> 00:10:02,470
associated with that change which

00:10:00,460 --> 00:10:05,530
currently is spread all over the MMU and

00:10:02,470 --> 00:10:07,090
on failure a transparently retry as that

00:10:05,530 --> 00:10:09,310
page table entry on the next iteration

00:10:07,090 --> 00:10:10,870
of the loop and that makes these these

00:10:09,310 --> 00:10:13,510
feature functions that implement various

00:10:10,870 --> 00:10:15,310
MMU operations really clean and they

00:10:13,510 --> 00:10:19,630
don't have to handle a lot of these sort

00:10:15,310 --> 00:10:20,800
of bookkeeping corner cases so that lets

00:10:19,630 --> 00:10:23,200
us change page table entries

00:10:20,800 --> 00:10:24,940
concurrently now we have to interoperate

00:10:23,200 --> 00:10:26,940
with MMU notifiers

00:10:24,940 --> 00:10:30,070
and all of the all the different

00:10:26,940 --> 00:10:31,900
functionality of the MMU so we wanted

00:10:30,070 --> 00:10:34,900
parallel page fault handlers but we

00:10:31,900 --> 00:10:36,820
didn't want to have to touch any of the

00:10:34,900 --> 00:10:39,040
other operations very much and so we

00:10:36,820 --> 00:10:41,170
split the KVM MMU lock into a

00:10:39,040 --> 00:10:43,750
reader/writer lock and the page fault

00:10:41,170 --> 00:10:45,280
handlers can take the read lock and do

00:10:43,750 --> 00:10:46,960
whatever they need to do and then any

00:10:45,280 --> 00:10:49,510
other operations we want to paralyze

00:10:46,960 --> 00:10:52,270
later can also take that reader lock and

00:10:49,510 --> 00:10:54,160
perform their operation the operations

00:10:52,270 --> 00:10:56,410
that they can't run in parallel with the

00:10:54,160 --> 00:10:59,230
page fault handlers like for example the

00:10:56,410 --> 00:11:02,680
MMU notifiers or interoperating with

00:10:59,230 --> 00:11:04,540
paging for to run nested yes just take

00:11:02,680 --> 00:11:05,890
the right lock and then the the

00:11:04,540 --> 00:11:10,570
synchronization guarantees that they

00:11:05,890 --> 00:11:12,670
have before still hold okay so those are

00:11:10,570 --> 00:11:14,320
the data structure II sides of the sides

00:11:12,670 --> 00:11:17,080
of the problem and now I want to talk

00:11:14,320 --> 00:11:20,530
about how we how we work with V CPU

00:11:17,080 --> 00:11:21,580
caches so if the V CPS had no caches

00:11:20,530 --> 00:11:24,250
this would be sort of the end of the

00:11:21,580 --> 00:11:27,340
story but the V CPU caches let add a lot

00:11:24,250 --> 00:11:28,870
of complexity to the MMU so the first

00:11:27,340 --> 00:11:32,500
one is the TLB which we all know and

00:11:28,870 --> 00:11:35,530
love and occasionally we're going to do

00:11:32,500 --> 00:11:40,330
some mu operation and we need to flush

00:11:35,530 --> 00:11:42,970
the TLB and so the reason we might not

00:11:40,330 --> 00:11:45,640
want to flush the TLB is that it's kind

00:11:42,970 --> 00:11:47,530
of a slow operation specifically KVM

00:11:45,640 --> 00:11:49,870
flush remotio B's asked to do a

00:11:47,530 --> 00:11:51,880
potentially asked to do an IP I to all

00:11:49,870 --> 00:11:54,600
the other running CPUs and as we have a

00:11:51,880 --> 00:11:58,450
lot of CPUs that becomes very slow and

00:11:54,600 --> 00:12:01,690
so let's say that that some V CPU caches

00:11:58,450 --> 00:12:02,770
some translation and then some mu

00:12:01,690 --> 00:12:04,360
function let's say the page fault

00:12:02,770 --> 00:12:08,380
handler wants to clear a page table

00:12:04,360 --> 00:12:11,110
entry mapping so it does and then the

00:12:08,380 --> 00:12:13,510
main mm wants to remap the backing guest

00:12:11,110 --> 00:12:16,870
physical memory and so it scans over

00:12:13,510 --> 00:12:18,400
these range of EPT entries and it looks

00:12:16,870 --> 00:12:20,500
for present entries it doesn't find any

00:12:18,400 --> 00:12:21,700
because we just cleared one and so it

00:12:20,500 --> 00:12:23,620
concludes that it doesn't need to do

00:12:21,700 --> 00:12:26,470
anything that returns and then the

00:12:23,620 --> 00:12:29,530
memories remapped but now if the guest

00:12:26,470 --> 00:12:31,840
tries to access some address that it had

00:12:29,530 --> 00:12:34,210
cashed in steal B it'll succeed because

00:12:31,840 --> 00:12:36,010
we didn't flush the TLB and so the

00:12:34,210 --> 00:12:38,620
current mu implementation solves this

00:12:36,010 --> 00:12:40,930
problem by flushing the TLB x' right

00:12:38,620 --> 00:12:44,050
after step two where we clear the Pte

00:12:40,930 --> 00:12:45,610
mapping and and this works fine but it

00:12:44,050 --> 00:12:48,130
means that the page fault handler would

00:12:45,610 --> 00:12:52,060
have to wait for the Fatimids to be

00:12:48,130 --> 00:12:53,950
flushed and so instead we extend this TL

00:12:52,060 --> 00:12:56,530
B's dirty mechanism which is currently

00:12:53,950 --> 00:12:59,170
used by some nested operations to defer

00:12:56,530 --> 00:13:01,030
to you B flushes and so the page fault

00:12:59,170 --> 00:13:03,370
handler can simply increment this TL B's

00:13:01,030 --> 00:13:06,870
dirty counter instead of doing a flush

00:13:03,370 --> 00:13:10,300
and then when invalidate range runs

00:13:06,870 --> 00:13:12,160
it'll see the elevated TLB count 30lbs

00:13:10,300 --> 00:13:12,570
dirty count and it knows that it has to

00:13:12,160 --> 00:13:14,310
flush

00:13:12,570 --> 00:13:16,350
TOB is in order to guarantee that the

00:13:14,310 --> 00:13:17,730
guests won't access memory and now if

00:13:16,350 --> 00:13:19,890
the guest tries to access that memory

00:13:17,730 --> 00:13:21,840
it'll take an EPT violation because the

00:13:19,890 --> 00:13:23,790
memory is not there the mappings not

00:13:21,840 --> 00:13:28,290
there anymore in the in memory paging

00:13:23,790 --> 00:13:30,660
structure excuse me so that stops the

00:13:28,290 --> 00:13:33,060
guests from accessing remapped memory

00:13:30,660 --> 00:13:34,980
remapped physical memory but we also

00:13:33,060 --> 00:13:39,390
have to stop the guests from translating

00:13:34,980 --> 00:13:41,310
through freed memory so there's there's

00:13:39,390 --> 00:13:43,650
also a set of paging structure caches

00:13:41,310 --> 00:13:45,750
which cache partial walks through the

00:13:43,650 --> 00:13:47,460
paging structures so maybe the guest

00:13:45,750 --> 00:13:50,160
will try to we'll try to access some

00:13:47,460 --> 00:13:53,250
memory and then it'll save a pointer to

00:13:50,160 --> 00:13:56,490
maybe the level one EPT page that map's

00:13:53,250 --> 00:13:59,990
that memory and and this this cache is

00:13:56,490 --> 00:14:03,020
also cleared by KVM flush from of POVs

00:13:59,990 --> 00:14:05,820
so how does this cache cause problems

00:14:03,020 --> 00:14:08,940
well let's say that some V CPU cache is

00:14:05,820 --> 00:14:11,190
a partial translation for some some

00:14:08,940 --> 00:14:13,680
guest physical address and so it saves a

00:14:11,190 --> 00:14:15,960
pointer to some page of EPT memory and

00:14:13,680 --> 00:14:17,970
then the page fault handler comes in and

00:14:15,960 --> 00:14:20,190
it wants to clear some PT heed on some

00:14:17,970 --> 00:14:22,740
non-leaf PDE and it disconnects that

00:14:20,190 --> 00:14:25,500
page of page table memory then it'll

00:14:22,740 --> 00:14:27,510
free the page but now if the guest tries

00:14:25,500 --> 00:14:29,790
to access let's say the next page of

00:14:27,510 --> 00:14:31,470
guest physical memory we have a problem

00:14:29,790 --> 00:14:33,360
because it'll translate through this

00:14:31,470 --> 00:14:36,090
freed page of page table memory

00:14:33,360 --> 00:14:37,650
it'll get some bogus page table entry

00:14:36,090 --> 00:14:39,810
and it'll access some random host

00:14:37,650 --> 00:14:42,180
physical address that could crash the

00:14:39,810 --> 00:14:44,790
guest or lead to data leakage or

00:14:42,180 --> 00:14:46,770
something and so this gives us two

00:14:44,790 --> 00:14:49,410
prerequisites for freeing page table

00:14:46,770 --> 00:14:51,390
memory we have to wait for an RCU grace

00:14:49,410 --> 00:14:53,520
period like I talked about earlier to

00:14:51,390 --> 00:14:55,590
keep a vm threads from accessing freed

00:14:53,520 --> 00:14:58,110
page table memory and we also have to

00:14:55,590 --> 00:14:59,700
wait for a TLB flush so that we can

00:14:58,110 --> 00:15:02,580
ensure that b cpus don't access that

00:14:59,700 --> 00:15:03,960
free a page table memory so I realize

00:15:02,580 --> 00:15:07,530
this is quite a large diagram but I'll

00:15:03,960 --> 00:15:09,450
try to go through it the so so as before

00:15:07,530 --> 00:15:12,360
the guest is cached some partial

00:15:09,450 --> 00:15:14,730
translation and then we clear some page

00:15:12,360 --> 00:15:17,400
table entry but instead of freeing that

00:15:14,730 --> 00:15:19,050
page table entry immediately we first

00:15:17,400 --> 00:15:21,300
fill it with non present page table

00:15:19,050 --> 00:15:24,060
entries and then add it to a queue of

00:15:21,300 --> 00:15:25,610
pages that are waiting to be freed and

00:15:24,060 --> 00:15:28,190
now if the guest tries to access

00:15:25,610 --> 00:15:29,600
that page of memories for a translation

00:15:28,190 --> 00:15:32,360
it'll take a page fault because it's

00:15:29,600 --> 00:15:34,160
full of non present entries then at some

00:15:32,360 --> 00:15:36,230
point some thread needs to flush the TLB

00:15:34,160 --> 00:15:39,110
so it'll take a snapshot of this queue

00:15:36,230 --> 00:15:41,360
of pages flush the TLB z' and then move

00:15:39,110 --> 00:15:45,260
that snapshot into another queue which

00:15:41,360 --> 00:15:47,180
is the free list and then now if the

00:15:45,260 --> 00:15:48,920
guest tries to do it tries to access

00:15:47,180 --> 00:15:52,040
some some page in that that was mapped

00:15:48,920 --> 00:15:53,270
by that page before it'll use the ax

00:15:52,040 --> 00:15:55,040
memory paging structures because we

00:15:53,270 --> 00:15:58,490
flush that he'll be so we're still safe

00:15:55,040 --> 00:16:00,590
and then at some point some casket will

00:15:58,490 --> 00:16:02,300
process through the free list and then

00:16:00,590 --> 00:16:03,800
for each page in the free list it'll

00:16:02,300 --> 00:16:05,620
queue an RC u callback which eventually

00:16:03,800 --> 00:16:10,610
frees the page so that gives us our two

00:16:05,620 --> 00:16:12,020
prerequisites before we free memory so

00:16:10,610 --> 00:16:13,970
those are our four improvements we've

00:16:12,020 --> 00:16:15,680
got the simplified access pattern that

00:16:13,970 --> 00:16:17,630
does all of these things that I just

00:16:15,680 --> 00:16:19,400
talked about it does the the moving

00:16:17,630 --> 00:16:22,310
pages between the different queues and

00:16:19,400 --> 00:16:24,770
and and freeing them and updating all

00:16:22,310 --> 00:16:27,320
the bookkeeping we've got some memory

00:16:24,770 --> 00:16:29,540
savings that we got by by iterating

00:16:27,320 --> 00:16:31,160
directly over the paging structures we

00:16:29,540 --> 00:16:33,170
can handle page faults in parallel which

00:16:31,160 --> 00:16:35,870
substantially reduces vcp you exit

00:16:33,170 --> 00:16:38,090
latencies and we can reduce the total

00:16:35,870 --> 00:16:42,110
number of TLB flushes and defer TLB

00:16:38,090 --> 00:16:44,750
flushes to other operations so what is

00:16:42,110 --> 00:16:45,920
that all that work get us here we've run

00:16:44,750 --> 00:16:49,130
that benchmark I was talking about

00:16:45,920 --> 00:16:52,730
earlier with a variable number of V CPUs

00:16:49,130 --> 00:16:55,660
from 1 to 416 and you can see that if we

00:16:52,730 --> 00:16:58,460
handle all the page faults serialized

00:16:55,660 --> 00:17:00,260
the total time required to demand page

00:16:58,460 --> 00:17:02,480
in all this memory increases quite

00:17:00,260 --> 00:17:05,089
quickly but if we can handle the page

00:17:02,480 --> 00:17:08,030
faults on parallel then the scaling is

00:17:05,089 --> 00:17:09,890
not nearly so bad and we save about 89

00:17:08,030 --> 00:17:13,520
percent of the total execution time and

00:17:09,890 --> 00:17:16,190
that 89 percent savings is entirely time

00:17:13,520 --> 00:17:21,440
that we spent with PC views blocked

00:17:16,190 --> 00:17:22,760
waiting for the MMU lock so these are a

00:17:21,440 --> 00:17:24,740
lot of changes and we're hoping to

00:17:22,760 --> 00:17:26,860
solicit some feedback for the community

00:17:24,740 --> 00:17:29,930
about how best to integrate these

00:17:26,860 --> 00:17:32,180
there's two sort of main approaches that

00:17:29,930 --> 00:17:34,100
we've come up with one is extending this

00:17:32,180 --> 00:17:36,830
direct walk iterator pattern

00:17:34,100 --> 00:17:39,200
to work for xa6 shadow paging and nested

00:17:36,830 --> 00:17:41,630
shadow paging and then replacing large

00:17:39,200 --> 00:17:44,390
portions the MMU with this more scalable

00:17:41,630 --> 00:17:47,330
implementation or we could modular eyes

00:17:44,390 --> 00:17:50,290
the MMU and formalize the MMU interface

00:17:47,330 --> 00:17:53,330
and then split out a separate MMU for

00:17:50,290 --> 00:17:56,600
two dimensional paging and then one MMU

00:17:53,330 --> 00:17:58,490
for x86 shadow paging and that way those

00:17:56,600 --> 00:18:01,040
2mm use could be optimized for different

00:17:58,490 --> 00:18:02,660
things and one would work well for for

00:18:01,040 --> 00:18:04,310
these the newer hardware and one would

00:18:02,660 --> 00:18:07,340
work well for ski for sort of legacy

00:18:04,310 --> 00:18:09,290
hardware and that's all I've got for you

00:18:07,340 --> 00:18:11,420
so if you want to view the RFC that I

00:18:09,290 --> 00:18:12,830
sent out about a month ago it's on the

00:18:11,420 --> 00:18:14,900
cavea mailing list and we also have a

00:18:12,830 --> 00:18:17,890
Garrett instance I was to Google that

00:18:14,900 --> 00:18:17,890
you can look at the code on too

00:18:19,540 --> 00:18:28,509
[Applause]

00:18:26,039 --> 00:18:32,219
for handling page faults you're taking a

00:18:28,509 --> 00:18:37,899
read lock on read write or semaphore

00:18:32,219 --> 00:18:40,089
it's a read write spin lock look it's

00:18:37,899 --> 00:18:41,919
nice how far okay so then you're also

00:18:40,089 --> 00:18:44,649
taking that for right when you do an

00:18:41,919 --> 00:18:46,960
invalid a range yeah how do you handle

00:18:44,649 --> 00:18:49,539
the scenario we're at Q mu is M on

00:18:46,960 --> 00:18:51,940
mapping a range that you're also taking

00:18:49,539 --> 00:18:54,820
a fault on in which case it will hold

00:18:51,940 --> 00:18:56,409
the M app 74:4 right and you'll try and

00:18:54,820 --> 00:19:00,330
take that for read during your page

00:18:56,409 --> 00:19:00,330
fault and you can potentially deadlock

00:19:02,969 --> 00:19:08,440
to be totally honest I don't know a lot

00:19:05,469 --> 00:19:09,969
about semaphores I don't think that the

00:19:08,440 --> 00:19:14,320
lock that we're using will deadlock in

00:19:09,969 --> 00:19:16,809
that case you so in the page fault

00:19:14,320 --> 00:19:18,489
handler when we go through the MMU to

00:19:16,809 --> 00:19:20,109
take a normal page fault sorry the

00:19:18,489 --> 00:19:23,229
corium mu to take a normal page fault

00:19:20,109 --> 00:19:25,539
and bring in some memory we'll take the

00:19:23,229 --> 00:19:27,549
m map semaphore for read in that case

00:19:25,539 --> 00:19:29,019
but it won't be able to me a map

00:19:27,549 --> 00:19:31,149
semaphore sorry so it won't take that

00:19:29,019 --> 00:19:35,349
for read because it's being held for

00:19:31,149 --> 00:19:36,940
right by the user space process and the

00:19:35,349 --> 00:19:39,519
user space process can't make forward

00:19:36,940 --> 00:19:43,769
progress because it's waiting for your

00:19:39,519 --> 00:19:48,129
new lock and the MMU to Bute released I

00:19:43,769 --> 00:19:51,330
believe let's let's meet after after

00:19:48,129 --> 00:19:53,889
this talk and we can write it on paper

00:19:51,330 --> 00:19:55,690
I'm sure there's a lot of like very

00:19:53,889 --> 00:20:00,909
subtle races that that are very

00:19:55,690 --> 00:20:03,429
interesting and worth hashing out how

00:20:00,909 --> 00:20:07,239
much of this is a common code and how

00:20:03,429 --> 00:20:09,129
much is x86 specific how much of this is

00:20:07,239 --> 00:20:12,309
common code how much this is x86

00:20:09,129 --> 00:20:14,440
specific so all the code for traversing

00:20:12,309 --> 00:20:18,429
the paging structures and modifying page

00:20:14,440 --> 00:20:23,919
table entries is all x86 specific does

00:20:18,429 --> 00:20:25,929
it have to be perhaps not but in the

00:20:23,919 --> 00:20:29,469
current implementation of the MMU it all

00:20:25,929 --> 00:20:32,710
is as well a lot of the code for dealing

00:20:29,469 --> 00:20:34,389
with MMU notifiers is it's not x86

00:20:32,710 --> 00:20:35,649
specific but when you get down to

00:20:34,389 --> 00:20:37,860
actually traversing the page you start

00:20:35,649 --> 00:20:40,650
paging structures it is

00:20:37,860 --> 00:20:41,940
most of the other things are not x86

00:20:40,650 --> 00:20:44,400
specific sorry

00:20:41,940 --> 00:20:47,610
so most of most of the other things are

00:20:44,400 --> 00:20:49,020
not actually exactly six specific the

00:20:47,610 --> 00:20:51,980
things the other things that you

00:20:49,020 --> 00:20:51,980
presented

00:21:02,440 --> 00:21:06,880
Oh what I was saying all the MMU

00:21:04,390 --> 00:21:09,549
notifier stuff is not x86 specific

00:21:06,880 --> 00:21:13,679
anything underneath that 99% of this

00:21:09,549 --> 00:21:15,850
presentation is x86 Pacific in the end a

00:21:13,679 --> 00:21:21,299
question on the slide where you showed

00:21:15,850 --> 00:21:24,429
that you are zapping a non-leaf EPT page

00:21:21,299 --> 00:21:27,130
mm-hmm you said that you zero out the

00:21:24,429 --> 00:21:27,909
page that or you fill it with non

00:21:27,130 --> 00:21:31,929
present

00:21:27,909 --> 00:21:33,549
PT's yeah why do we need to fill it with

00:21:31,929 --> 00:21:36,039
non present yet between these entries in

00:21:33,549 --> 00:21:40,149
that case that's a great question so at

00:21:36,039 --> 00:21:42,210
this point the the thread that's that's

00:21:40,149 --> 00:21:44,770
detached that page at page table memory

00:21:42,210 --> 00:21:46,659
is basically done and it can it can

00:21:44,770 --> 00:21:48,460
finish doing that ever it's doing and so

00:21:46,659 --> 00:21:50,860
we have this page of page table memory

00:21:48,460 --> 00:21:54,820
that's an accused somewhere but it's

00:21:50,860 --> 00:21:56,380
still mapping physical memory and if we

00:21:54,820 --> 00:22:00,370
left those mappings that the page

00:21:56,380 --> 00:22:01,659
contained intact then if we got a ma

00:22:00,370 --> 00:22:04,029
notifier invalidate at range or

00:22:01,659 --> 00:22:06,820
something we would have to scan over all

00:22:04,029 --> 00:22:08,559
this free memory to ensure that it

00:22:06,820 --> 00:22:10,990
wasn't still mapping any any of the

00:22:08,559 --> 00:22:12,970
physical memory we cared about and that

00:22:10,990 --> 00:22:15,539
would add complexity and reuse

00:22:12,970 --> 00:22:17,860
performance we don't want these these

00:22:15,539 --> 00:22:24,520
potentially valid mappings just kind of

00:22:17,860 --> 00:22:27,480
hanging around but when you clear we'll

00:22:24,520 --> 00:22:27,480
talk after yeah

00:22:36,369 --> 00:22:44,720
my boss has a question for me why didn't

00:22:41,470 --> 00:22:48,049
the blue line why why didn't it scale

00:22:44,720 --> 00:22:50,479
perfectly in the the your red and blue

00:22:48,049 --> 00:22:53,509
line graph where's the remaining

00:22:50,479 --> 00:22:57,849
serialization so the remaining

00:22:53,509 --> 00:23:00,529
serialization is in this backup slide so

00:22:57,849 --> 00:23:02,840
it's this is a lot of text I realized

00:23:00,529 --> 00:23:05,330
but basically it's all pushed out of KBM

00:23:02,840 --> 00:23:08,659
so it's down in readwrite semaphores in

00:23:05,330 --> 00:23:11,629
the main MMU or the main mm and a lot of

00:23:08,659 --> 00:23:14,139
it is spent copying data into the into

00:23:11,629 --> 00:23:14,139
guest memory

00:23:19,960 --> 00:23:24,299
okay thank you thank you

00:23:24,710 --> 00:23:31,569

YouTube URL: https://www.youtube.com/watch?v=iQwO2PudbNY


