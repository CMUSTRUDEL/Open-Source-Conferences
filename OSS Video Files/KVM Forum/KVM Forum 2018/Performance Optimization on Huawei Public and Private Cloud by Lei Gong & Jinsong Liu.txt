Title: Performance Optimization on Huawei Public and Private Cloud by Lei Gong & Jinsong Liu
Publication date: 2018-11-14
Playlist: KVM Forum 2018
Description: 
	With the increasing demand for big data processing and faster memory databases, such as SAP HANA, the demand for large-size virtual machines is getting stronger. Meanwhile, for enterprise virtualization and private cloud scenarios (such as VSI), virtual machine density improvement is also an important means of saving cost, that is, CPU over-commitment.
In this session, we will share some optimization ways in virtualization on the KVM platform for public and/or private cloud. The first one is for the optimization of the spinlock holder of the guest, so that we can improve the synchronization efficiency between the VCPUs of the large-scale virtual machine. The second one is the "balance scheduler", as far as possible to ensure that different VCPUs of the same virtual machine can be scheduled at the same time to improve performance. The last one is optimization of the RTC clock compensation scheme is moved from userspace to kernel to reduce overhead of context switching and increase the density of Windows virtual machines in the private cloud.

---

Lei Gong
Huawei
￼
Jinsong Liu
Huawei
Captions: 
	00:00:01,040 --> 00:00:13,860
[Music]

00:00:05,690 --> 00:00:16,949
okay without producer got a matter today

00:00:13,860 --> 00:00:19,890
I were introduced I am our team member

00:00:16,949 --> 00:00:22,890
were introduced to some fertilization

00:00:19,890 --> 00:00:28,519
optimization on Kevin and Wow is perfect

00:00:22,890 --> 00:00:32,189
route yeah

00:00:28,519 --> 00:00:35,430
our motorization optimization considered

00:00:32,189 --> 00:00:38,129
consist of seven points the first point

00:00:35,430 --> 00:00:47,489
is that a gypsy pew-pew-pew

00:00:38,129 --> 00:00:52,800
weatherization optimization the cheapest

00:00:47,489 --> 00:00:56,430
instance in what with public cloud some

00:00:52,800 --> 00:01:03,420
in some polled especially for the HPC

00:00:56,430 --> 00:01:06,210
and a iCloud so basically there are two

00:01:03,420 --> 00:01:09,210
technology ways of cheap civilization

00:01:06,210 --> 00:01:13,950
wise which infuse mostly used at the

00:01:09,210 --> 00:01:16,710
Greg scenario for example what resin

00:01:13,950 --> 00:01:21,689
tests have such kind of things and the

00:01:16,710 --> 00:01:26,909
other türkçe is to GPU computing 4 GB GB

00:01:21,689 --> 00:01:28,650
computing a for HP cai cloud the vgpu is

00:01:26,909 --> 00:01:32,520
a lot necessary because the workload for

00:01:28,650 --> 00:01:35,759
each GPUs is very high so with we have

00:01:32,520 --> 00:01:38,930
we don't need to separate the GPUs into

00:01:35,759 --> 00:01:42,710
different different of interviews so

00:01:38,930 --> 00:01:46,920
under this situation we use GP pass rule

00:01:42,710 --> 00:01:51,420
but GP pass rule itself is quite

00:01:46,920 --> 00:01:55,229
straightforward and simple but for HPC

00:01:51,420 --> 00:02:00,119
and AI the GP directly passer is not

00:01:55,229 --> 00:02:04,969
enough there are two standard for GPU

00:02:00,119 --> 00:02:06,180
used in AI one is GPU computing

00:02:04,969 --> 00:02:08,910
capability

00:02:06,180 --> 00:02:12,750
the second is GPU communication

00:02:08,910 --> 00:02:14,400
capability the GPU computing capability

00:02:12,750 --> 00:02:17,250
usually we directly pass through that

00:02:14,400 --> 00:02:19,740
gpus to eat verbs and then that's okay

00:02:17,250 --> 00:02:26,520
but for cheaper communication capability

00:02:19,740 --> 00:02:35,430
is a big problem first we use Nvidia P

00:02:26,520 --> 00:02:37,860
100 GPUs in some of our GP instance that

00:02:35,430 --> 00:02:40,470
a variant works well

00:02:37,860 --> 00:02:43,950
a little P 100 works well under latest

00:02:40,470 --> 00:02:47,910
but under the motorisation will meet big

00:02:43,950 --> 00:02:50,010
problem for example in normal in normal

00:02:47,910 --> 00:02:52,980
ways we direct pass through the GPU to

00:02:50,010 --> 00:02:56,250
the guest to the worms the pennyways is

00:02:52,980 --> 00:03:01,260
only about 40 percent of the lead native

00:02:56,250 --> 00:03:06,830
performance and the latency is about 180

00:03:01,260 --> 00:03:06,830
per percent of radius so the bandwidth

00:03:07,400 --> 00:03:19,050
chop-chop very big and the latency the

00:03:11,430 --> 00:03:21,870
girls were very very big okay this

00:03:19,050 --> 00:03:24,750
topology is the GPU PDP communication

00:03:21,870 --> 00:03:27,450
topology generally there are three ways

00:03:24,750 --> 00:03:30,870
for GPU communicate with another chips

00:03:27,450 --> 00:03:33,120
you sorry I would say that why chips you

00:03:30,870 --> 00:03:35,600
communication is important for the eyes

00:03:33,120 --> 00:03:39,450
because the for example the a is

00:03:35,600 --> 00:03:42,030
consistent different layers the layer

00:03:39,450 --> 00:03:44,549
one candidate computer and then transfer

00:03:42,030 --> 00:03:45,930
the data to layer two and layer two the

00:03:44,549 --> 00:03:49,410
transfer to the nursery

00:03:45,930 --> 00:03:52,410
so under AI such issues the GPU

00:03:49,410 --> 00:03:55,140
communication capability is a very very

00:03:52,410 --> 00:03:56,660
important especially for AI for example

00:03:55,140 --> 00:04:03,269
the coffee and other

00:03:56,660 --> 00:04:05,940
Eckerson so this is the topology of the

00:04:03,269 --> 00:04:08,370
GPU PDP communication generally there

00:04:05,940 --> 00:04:12,720
are three ways the shortest way started

00:04:08,370 --> 00:04:16,080
for example that the read by from gp120

00:04:12,720 --> 00:04:22,140
to GPU one the paper path is that the

00:04:16,080 --> 00:04:24,320
CPU GPU zero send the transfer rotate to

00:04:22,140 --> 00:04:26,070
the P side switch then directly

00:04:24,320 --> 00:04:30,380
translates to another

00:04:26,070 --> 00:04:34,170
Hue's the middle long pass is that the

00:04:30,380 --> 00:04:37,740
the blue line the blue lines that the

00:04:34,170 --> 00:04:40,890
dates transfer from GPU then to pieces

00:04:37,740 --> 00:04:44,790
which then to the root complex root

00:04:40,890 --> 00:04:49,080
complex and then go down the longest way

00:04:44,790 --> 00:04:54,650
is that the GPU Tate need to go upstairs

00:04:49,080 --> 00:04:57,960
to the CP one and then through the QPR

00:04:54,650 --> 00:05:03,030
you pi/2 and other CPUs and then go

00:04:57,960 --> 00:05:05,420
downstairs to another GPUs so for a way

00:05:03,030 --> 00:05:06,930
they are native they usually use the

00:05:05,420 --> 00:05:09,390
press one

00:05:06,930 --> 00:05:12,480
that means the shortest one but other

00:05:09,390 --> 00:05:19,130
than under the fertilization will be the

00:05:12,480 --> 00:05:23,490
big problem because you know for example

00:05:19,130 --> 00:05:27,930
the other other fertilization

00:05:23,490 --> 00:05:31,410
environment the date cannot directly go

00:05:27,930 --> 00:05:35,510
through from the GPU zero to the precise

00:05:31,410 --> 00:05:39,210
which can go down to the GPU one because

00:05:35,510 --> 00:05:39,930
the GPA is usually need to translate by

00:05:39,210 --> 00:05:43,200
iommu

00:05:39,930 --> 00:05:45,930
that means that it should go to go

00:05:43,200 --> 00:05:47,940
through the GPU switch then upstairs to

00:05:45,930 --> 00:05:48,750
the root root complex and that's a

00:05:47,940 --> 00:05:53,700
little better

00:05:48,750 --> 00:05:58,080
I already used and done some technology

00:05:53,700 --> 00:06:02,070
can really release this problem for

00:05:58,080 --> 00:06:04,530
example pca 80s but unfortunately the

00:06:02,070 --> 00:06:07,350
chips you of Alinea don't support eight

00:06:04,530 --> 00:06:09,540
years so each transaction did go

00:06:07,350 --> 00:06:12,980
upstairs to the album you then translate

00:06:09,540 --> 00:06:18,840
and then go down so that's the problem

00:06:12,980 --> 00:06:20,670
so you know our optimization we solve

00:06:18,840 --> 00:06:23,160
two problems one problem is the

00:06:20,670 --> 00:06:25,620
performance issue and another problem is

00:06:23,160 --> 00:06:29,790
a security issue for performance for

00:06:25,620 --> 00:06:32,760
example we fake the part we used we fake

00:06:29,790 --> 00:06:35,880
the bar because each

00:06:32,760 --> 00:06:40,589
GPU native their resource did in the

00:06:35,880 --> 00:06:45,129
confliction so we can't directly use the

00:06:40,589 --> 00:06:48,270
bar of Latif is the vitalization bar is

00:06:45,129 --> 00:06:52,389
the V Empire so we'll call it a fig bar

00:06:48,270 --> 00:06:54,669
in that way the GPA equal to the HPA so

00:06:52,389 --> 00:06:59,409
we don't need to translate we don't need

00:06:54,669 --> 00:07:01,870
to translate by mmm use the fit part is

00:06:59,409 --> 00:07:03,189
the step one step two is that we need to

00:07:01,870 --> 00:07:06,159
control the date

00:07:03,189 --> 00:07:09,309
don't dare did we go out go upstairs to

00:07:06,159 --> 00:07:12,969
the road complex to translate so we used

00:07:09,309 --> 00:07:16,120
pci-dss to control the date and then

00:07:12,969 --> 00:07:20,649
that the dates just go to the PSI switch

00:07:16,120 --> 00:07:25,270
then called pastels trans James to

00:07:20,649 --> 00:07:29,580
diffuse so in this way i know i am i all

00:07:25,270 --> 00:07:33,149
memory you for GPU to GPU communication

00:07:29,580 --> 00:07:36,219
it was in this way the performance is

00:07:33,149 --> 00:07:38,680
exactly the latest performers the

00:07:36,219 --> 00:07:41,740
bandwidth and the latency is exactly as

00:07:38,680 --> 00:07:44,349
good as natives but in this solution

00:07:41,740 --> 00:07:46,569
there comes from there comes the

00:07:44,349 --> 00:07:50,169
security issue security should be

00:07:46,569 --> 00:07:56,529
started if we don't prevent malicious

00:07:50,169 --> 00:08:00,939
access from gp1 to GPU X that means the

00:07:56,529 --> 00:08:04,889
GPU of one VMs can steal the date of

00:08:00,939 --> 00:08:11,649
other VMs so we'll need to prevent a

00:08:04,889 --> 00:08:14,229
security issue most of the cloud

00:08:11,649 --> 00:08:19,020
provider they don't agree that they

00:08:14,229 --> 00:08:22,270
don't support this because they usually

00:08:19,020 --> 00:08:25,990
allocate all the GPUs under one piece I

00:08:22,270 --> 00:08:29,620
switch for example maybe a fourth GPUs

00:08:25,990 --> 00:08:32,169
to just one VMs they cannot separate

00:08:29,620 --> 00:08:35,490
them into two into two and the two and

00:08:32,169 --> 00:08:39,510
then assigned you get worms because that

00:08:35,490 --> 00:08:42,849
they cannot prevent the security issue

00:08:39,510 --> 00:08:43,750
but now that the GPU instance is very

00:08:42,849 --> 00:08:48,490
very expensive

00:08:43,750 --> 00:08:52,460
if the finest the smallest unit we need

00:08:48,490 --> 00:08:54,770
to assign to a guest is 4 or 8

00:08:52,460 --> 00:08:58,340
that means the customers should pay a

00:08:54,770 --> 00:09:02,080
lot of magnitude - why wait you to have

00:08:58,340 --> 00:09:05,270
instance but the AI cooperation they

00:09:02,080 --> 00:09:08,240
spent a long time to train their model

00:09:05,270 --> 00:09:10,970
in this stage in this stage they don't

00:09:08,240 --> 00:09:14,140
need many many GPUs they only need for

00:09:10,970 --> 00:09:17,390
example huge abuses okay so they came so

00:09:14,140 --> 00:09:20,420
the it's a requirement from customer

00:09:17,390 --> 00:09:25,610
that we can separate the GPU Lambert's

00:09:20,420 --> 00:09:28,360
to the games so we if we secured we we

00:09:25,610 --> 00:09:30,890
solve the security issue then we can

00:09:28,360 --> 00:09:34,250
separate different the Lambert's Ibiza

00:09:30,890 --> 00:09:37,910
to give the various the ways to the ways

00:09:34,250 --> 00:09:41,900
to solve the security issue is we use

00:09:37,910 --> 00:09:48,230
the psi egress psi egress can control

00:09:41,900 --> 00:09:50,780
that one point one port controller one

00:09:48,230 --> 00:09:55,460
put the data flow of one port to some

00:09:50,780 --> 00:09:59,000
port it allowed so if this part belongs

00:09:55,460 --> 00:10:03,410
to one VM so we allow it the date from

00:09:59,000 --> 00:10:07,010
port 1 2 / - but if this port if this

00:10:03,410 --> 00:10:10,640
purpose are not belong to one of them's

00:10:07,010 --> 00:10:13,790
we don't we don't we don't allow this

00:10:10,640 --> 00:10:16,280
access in this way we separate the

00:10:13,790 --> 00:10:21,320
different GPUs groups two different

00:10:16,280 --> 00:10:25,310
variants okay after the method after the

00:10:21,320 --> 00:10:29,300
optimization we have some dates about

00:10:25,310 --> 00:10:36,260
our result the right side is the latest

00:10:29,300 --> 00:10:40,090
and the left side is our VM GPUs from

00:10:36,260 --> 00:10:45,020
this date we will say that the

00:10:40,090 --> 00:10:49,760
performance of GPU in our VMs is exactly

00:10:45,020 --> 00:10:56,000
as good as parameter that's our chip

00:10:49,760 --> 00:11:00,020
utilization optimization and next three

00:10:56,000 --> 00:11:02,980
of optimization where Pete Rock by by my

00:11:00,020 --> 00:11:02,980
colleague Guney

00:11:05,870 --> 00:11:12,360
good afternoon everyone

00:11:08,540 --> 00:11:15,120
okay let's talk about it the next agenda

00:11:12,360 --> 00:11:22,800
the first one is optimization for block

00:11:15,120 --> 00:11:24,780
rotor preemption maybe more people here

00:11:22,800 --> 00:11:25,740
maybe know about as a lock holder

00:11:24,780 --> 00:11:27,900
preemption

00:11:25,740 --> 00:11:30,000
it is very obviously in which were

00:11:27,900 --> 00:11:32,150
decision environment because of the

00:11:30,000 --> 00:11:37,140
recipients guttering and the task

00:11:32,150 --> 00:11:40,230
preemption and the look how the

00:11:37,140 --> 00:11:43,460
preemption issue may be bring some side

00:11:40,230 --> 00:11:46,860
effects for example it potential

00:11:43,460 --> 00:11:49,020
Barack's in the process of a otherwise

00:11:46,860 --> 00:11:52,470
abused waiting to acquire the same block

00:11:49,020 --> 00:11:55,860
same Rock and increasing synchronization

00:11:52,470 --> 00:12:00,590
latency and the wintry suffered the

00:11:55,860 --> 00:12:03,710
performance degradation so how to

00:12:00,590 --> 00:12:09,570
elevate to look how the preemption issue

00:12:03,710 --> 00:12:12,620
we list for solutions to elevate the

00:12:09,570 --> 00:12:15,960
local preemption the first one is

00:12:12,620 --> 00:12:19,860
positive exerting and in the second one

00:12:15,960 --> 00:12:23,130
is delay lock hoda scheduling named by

00:12:19,860 --> 00:12:25,200
our servants and the third one is cost

00:12:23,130 --> 00:12:28,260
gathering the last why is a balanced

00:12:25,200 --> 00:12:31,890
schedule so let's talk about some

00:12:28,260 --> 00:12:38,520
details about its solution the first one

00:12:31,890 --> 00:12:43,340
pause loop exceeding it is many hard

00:12:38,520 --> 00:12:46,020
work a hardware exists a the

00:12:43,340 --> 00:12:51,390
visualization the technology you know

00:12:46,020 --> 00:12:53,370
and it requires some hardware support

00:12:51,390 --> 00:12:56,730
some like something like a whimsy as

00:12:53,370 --> 00:13:01,110
configuration and it is an optimization

00:12:56,730 --> 00:13:03,960
for lock waiters to Agra to VM

00:13:01,110 --> 00:13:06,990
exiting and there were the waste with

00:13:03,960 --> 00:13:11,430
CPU cycles for in worried the spin so

00:13:06,990 --> 00:13:14,040
the process is the Puri is supported

00:13:11,430 --> 00:13:17,580
well by absolute up a string just like

00:13:14,040 --> 00:13:20,399
then have a wider and KVM

00:13:17,580 --> 00:13:23,370
but because it's setting appropriate

00:13:20,399 --> 00:13:26,910
virus of Pirie gap and a purely window

00:13:23,370 --> 00:13:29,279
are difficult and they are usually need

00:13:26,910 --> 00:13:34,680
to be at adjusted according to defined

00:13:29,279 --> 00:13:39,410
work road it is also different to find

00:13:34,680 --> 00:13:43,550
an appropriate which appeared to yield

00:13:39,410 --> 00:13:49,079
okay the delay lock holder scattering is

00:13:43,550 --> 00:13:52,380
realized by ourselves before I describe

00:13:49,079 --> 00:13:55,529
the DHS I'd like to introduce some

00:13:52,380 --> 00:13:57,959
background usually the lock holders are

00:13:55,529 --> 00:14:01,200
only interrupted disable contests and

00:13:57,959 --> 00:14:05,310
anomaly the period of holding rock is a

00:14:01,200 --> 00:14:08,209
shot tree we need to choose some hard

00:14:05,310 --> 00:14:10,740
hardware support just like the in

00:14:08,209 --> 00:14:15,240
interrupt a window uranium function of

00:14:10,740 --> 00:14:19,860
winter with eggs and we need to in the

00:14:15,240 --> 00:14:23,850
edge a timer so firstly we set a grace

00:14:19,860 --> 00:14:29,040
period perrault for lock holder we cpu

00:14:23,850 --> 00:14:32,670
before it is be scheduled and if one we

00:14:29,040 --> 00:14:35,910
cpu is the lock holder we start one

00:14:32,670 --> 00:14:40,170
extra timer and whose expired

00:14:35,910 --> 00:14:42,810
time is a grace period and then we set

00:14:40,170 --> 00:14:51,300
the interrupt window exceeding for the

00:14:42,810 --> 00:14:55,730
we CPU and there is two cases so first

00:14:51,300 --> 00:15:00,180
what is the edge at Hama expire before

00:14:55,730 --> 00:15:04,279
the we CP release block and the second

00:15:00,180 --> 00:15:08,430
one is the edge of time expired pattern

00:15:04,279 --> 00:15:10,620
in the recipients lock so for the first

00:15:08,430 --> 00:15:13,680
one we just need a clear the interrupt

00:15:10,620 --> 00:15:17,790
the window and continue to schedule to

00:15:13,680 --> 00:15:21,720
receive you for for the second case we

00:15:17,790 --> 00:15:25,100
need to have solutions to charge the we

00:15:21,720 --> 00:15:27,899
civil release the lock something like

00:15:25,100 --> 00:15:30,240
really happened or the interrupt the

00:15:27,899 --> 00:15:34,180
window a grading happened

00:15:30,240 --> 00:15:37,390
then we need to conserve in the Etra

00:15:34,180 --> 00:15:42,340
timer and release the grace period then

00:15:37,390 --> 00:15:44,530
schedule disappear immediately we'd love

00:15:42,340 --> 00:15:49,210
to see the performance we uses a heck

00:15:44,530 --> 00:15:52,570
Pendrell test the VM performance in the

00:15:49,210 --> 00:15:55,350
guests and as a CPU overcommit reto

00:15:52,570 --> 00:15:55,350
sorry

00:15:56,010 --> 00:16:10,780
it's 1 2 3 16 P CPUs 248 with EPOC in 3

00:16:06,310 --> 00:16:15,310
we amps and the result lower is better

00:16:10,780 --> 00:16:19,650
you know the top three lies are the

00:16:15,310 --> 00:16:23,560
results of non Patchett which machines

00:16:19,650 --> 00:16:27,180
but the Bulow's reliance is the result

00:16:23,560 --> 00:16:27,180
of patchy the virtual machines

00:16:27,780 --> 00:16:38,580
ok the next is Baron scattering before

00:16:36,150 --> 00:16:41,590
introduces the baron schedule adding 2

00:16:38,580 --> 00:16:45,730
also introduced the cost gathering and

00:16:41,590 --> 00:16:50,320
the parents gathering we can see in the

00:16:45,730 --> 00:16:52,830
left of the left background it is the

00:16:50,320 --> 00:16:57,010
cost gathering somatic diagram

00:16:52,830 --> 00:17:00,670
it requires the holes they have in love

00:16:57,010 --> 00:17:04,689
either piece appeals to run always abuse

00:17:00,670 --> 00:17:06,930
and at the same time and and in the

00:17:04,689 --> 00:17:09,000
right side is the parent scheduling

00:17:06,930 --> 00:17:13,990
scheme

00:17:09,000 --> 00:17:17,459
schematic diagram it disappears or PCP

00:17:13,990 --> 00:17:25,290
always appears to defend page views as

00:17:17,459 --> 00:17:31,270
much as possible so about cost gathering

00:17:25,290 --> 00:17:33,880
may be caused to problems CP of

00:17:31,270 --> 00:17:38,730
recommendation and the priority in

00:17:33,880 --> 00:17:41,380
musician CPU fragmentation may reduce

00:17:38,730 --> 00:17:43,030
serialization and the delay with severe

00:17:41,380 --> 00:17:47,890
execution

00:17:43,030 --> 00:17:50,830
we can say from the bureau's figure we

00:17:47,890 --> 00:17:57,580
CPU zero and the recipe one can't be

00:17:50,830 --> 00:18:00,010
scheduled until t1 because there is

00:17:57,580 --> 00:18:05,470
owning one idle CPU

00:18:00,010 --> 00:18:08,680
I know piece of U at T zero and the

00:18:05,470 --> 00:18:11,590
priority inversion is a were a higher

00:18:08,680 --> 00:18:15,450
priority task is scheduled after lower

00:18:11,590 --> 00:18:18,160
priority tasks for example the I

00:18:15,450 --> 00:18:18,730
apologize origin to run when our air is

00:18:18,160 --> 00:18:23,020
ready

00:18:18,730 --> 00:18:25,990
however it cannot run because all PC

00:18:23,020 --> 00:18:31,960
pews are allocated to the coschedule

00:18:25,990 --> 00:18:35,050
huskers so the priority in musician

00:18:31,960 --> 00:18:38,740
profit me adversary affecting

00:18:35,050 --> 00:18:45,190
interactive or IO bounded jobs and on

00:18:38,740 --> 00:18:50,950
the utilized other resources such as the

00:18:45,190 --> 00:18:55,840
discus we can also see their figure the

00:18:50,950 --> 00:19:00,750
I upon the job it is ready between t0 to

00:18:55,840 --> 00:19:04,960
t1 but it can't be runnable because at

00:19:00,750 --> 00:19:08,560
t1 because the schedule as I in the

00:19:04,960 --> 00:19:14,970
piece of view 0 and piece of u1 on we

00:19:08,560 --> 00:19:18,520
secured 0 and we see P 1 so the

00:19:14,970 --> 00:19:25,690
eye-opener job can run can run a ball

00:19:18,520 --> 00:19:30,600
until t2 so the longer that we the time

00:19:25,690 --> 00:19:37,770
slice of whistle p1 t to substract 31

00:19:30,600 --> 00:19:37,770
the longer its I opened jobs latency

00:19:39,800 --> 00:19:45,150
okay let's see the Barons gathering bias

00:19:42,750 --> 00:19:48,210
gathering it's used to to ban Swiss

00:19:45,150 --> 00:19:52,590
appearance on different piece of use and

00:19:48,210 --> 00:19:56,940
it is without precisely scattering the

00:19:52,590 --> 00:20:02,490
recipients simultaneously so how did

00:19:56,940 --> 00:20:06,450
that we use a bitmap to record all the

00:20:02,490 --> 00:20:11,850
PC pews for one which machines and then

00:20:06,450 --> 00:20:16,140
we modified links chronal schedule just

00:20:11,850 --> 00:20:20,580
like a set bit when one we CPU include

00:20:16,140 --> 00:20:24,900
and a cleared a bit if d killed

00:20:20,580 --> 00:20:27,930
we CPU tasks and also we need to check

00:20:24,900 --> 00:20:34,560
the bitmap immigration find idle CPUs

00:20:27,930 --> 00:20:37,410
like tasks RQ x era of course if the

00:20:34,560 --> 00:20:40,740
scheduler can't find a proper

00:20:37,410 --> 00:20:43,680
appropriate pcp you according to use the

00:20:40,740 --> 00:20:46,740
bitmap know now that consider the bitmap

00:20:43,680 --> 00:20:52,340
requirement but as the original affinity

00:20:46,740 --> 00:20:57,450
requirement so it's not forced to

00:20:52,340 --> 00:21:00,240
require the used bitmap let's see the

00:20:57,450 --> 00:21:02,880
performance evaluation we use the puja

00:21:00,240 --> 00:21:07,440
server work loading Huawei pride with

00:21:02,880 --> 00:21:11,520
private crowd and we tested continuously

00:21:07,440 --> 00:21:16,740
for 24 hours and the results shows

00:21:11,520 --> 00:21:19,410
before optimization 70% of building time

00:21:16,740 --> 00:21:24,540
less than 10 minutes seconds and with

00:21:19,410 --> 00:21:27,020
balance gathering optimization 93.5% of

00:21:24,540 --> 00:21:30,780
building chain time less than 10

00:21:27,020 --> 00:21:34,560
millisecond with one-to-one we see

00:21:30,780 --> 00:21:38,790
people there are ninety five point three

00:21:34,560 --> 00:21:42,990
percent of building Trenton less than 10

00:21:38,790 --> 00:21:44,160
milliseconds and the result shows the

00:21:42,990 --> 00:21:46,410
balance gathering

00:21:44,160 --> 00:21:51,500
three factors of balances scheduling is

00:21:46,410 --> 00:21:51,500
near to the bishop you pin

00:21:51,690 --> 00:22:01,900
okay the Leicester topic is a DC

00:21:57,010 --> 00:22:04,510
optimization on KVM Adisa is used to

00:22:01,900 --> 00:22:05,380
work hahnel on windows guests at the

00:22:04,510 --> 00:22:10,810
crock sauce

00:22:05,380 --> 00:22:16,420
l Crotty when the device RTC emulation

00:22:10,810 --> 00:22:20,370
in Kumi on k vm and there are three

00:22:16,420 --> 00:22:20,370
timers in the queue

00:22:20,430 --> 00:22:26,800
the current realization had some 10

00:22:23,590 --> 00:22:29,920
points because some operations need to

00:22:26,800 --> 00:22:32,350
help that bigger Kumi Rock and lots of

00:22:29,920 --> 00:22:34,360
contrasts switching between user space

00:22:32,350 --> 00:22:37,890
and Aquino space and we needed to

00:22:34,360 --> 00:22:40,300
injector interactors from the user space

00:22:37,890 --> 00:22:43,180
finally we will suffer performance

00:22:40,300 --> 00:22:45,100
degradation something like that latency

00:22:43,180 --> 00:22:50,020
increase and windows gas the density

00:22:45,100 --> 00:22:53,290
decrease okay let's see the ADIZ

00:22:50,020 --> 00:22:56,280
optimization on kayvyun we minimize the

00:22:53,290 --> 00:22:58,300
influence influence of a big human rock

00:22:56,280 --> 00:23:02,050
replacing a distant memory region

00:22:58,300 --> 00:23:06,790
outside a big creamy rock and using IR

00:23:02,050 --> 00:23:09,190
qfd injecting interruptus was also we

00:23:06,790 --> 00:23:11,350
need we use lots of happy wave features

00:23:09,190 --> 00:23:14,370
just like have a weak lock and as some

00:23:11,350 --> 00:23:21,490
other help away features to disgrace

00:23:14,370 --> 00:23:28,900
decrease the i/o port access on 70 0 so

00:23:21,490 --> 00:23:31,410
I aborted 70 and 71 also we synced move

00:23:28,900 --> 00:23:35,980
add is an emulation to hypervisor and

00:23:31,410 --> 00:23:39,780
also I discussed this idea with the

00:23:35,980 --> 00:23:39,780
power and the processor this

00:23:40,890 --> 00:23:48,700
optimization has a big service attacks

00:23:45,010 --> 00:23:50,680
service it is comfortable with new

00:23:48,700 --> 00:23:54,700
features such as a spirit

00:23:50,680 --> 00:23:58,620
I could ship finally we used a new

00:23:54,700 --> 00:23:58,620
artistic conversation solution

00:23:59,470 --> 00:24:05,210
that is a common conversation solution

00:24:02,750 --> 00:24:07,760
is ruling artists accusing avoid the

00:24:05,210 --> 00:24:11,840
directory we need to count to the

00:24:07,760 --> 00:24:13,130
consistent khalessi the interruptus when

00:24:11,840 --> 00:24:15,970
the artists interrupts interacting

00:24:13,130 --> 00:24:18,410
fields we need to counter the

00:24:15,970 --> 00:24:22,390
interruptus and adjusted the counter

00:24:18,410 --> 00:24:22,390
where artists injector rate changing

00:24:22,420 --> 00:24:28,100
also we inject the khalessi the

00:24:25,340 --> 00:24:31,700
interruptus after your I handle if we

00:24:28,100 --> 00:24:37,310
exists so we don't need a separate the

00:24:31,700 --> 00:24:39,470
timer to do that and it's more timely of

00:24:37,310 --> 00:24:40,730
course we need to destroy all the speed

00:24:39,470 --> 00:24:43,430
if there is the tamerica

00:24:40,730 --> 00:24:48,220
khalessi the interruptus because the

00:24:43,430 --> 00:24:50,540
windows may be crash

00:24:48,220 --> 00:24:52,610
finally we also think that about the

00:24:50,540 --> 00:24:54,740
live migration support so we need to

00:24:52,610 --> 00:24:58,040
save in the colon see the interruptus in

00:24:54,740 --> 00:25:01,100
Southside and restore them in a just

00:24:58,040 --> 00:25:05,870
destination site those Kiwi Amana

00:25:01,100 --> 00:25:08,660
community to be pageant ok that's fine

00:25:05,870 --> 00:25:10,610
let us see the optimization will you are

00:25:08,660 --> 00:25:14,420
you Asian we used as a benchmark is

00:25:10,610 --> 00:25:16,460
locking we si raga Mirza was designed to

00:25:14,420 --> 00:25:25,070
perform benchmarks for vdi walk rose

00:25:16,460 --> 00:25:28,870
slow system generation we can we can

00:25:25,070 --> 00:25:37,280
google it that login we s I feel don't

00:25:28,870 --> 00:25:41,720
not see the results above figure is the

00:25:37,280 --> 00:25:48,560
result before optimization we assign max

00:25:41,720 --> 00:25:51,680
is 68 and the below figure is the with

00:25:48,560 --> 00:25:56,570
after is the result of the optimization

00:25:51,680 --> 00:26:00,890
and at the Whistle result is 78% and we

00:25:56,570 --> 00:26:07,690
finally increased 10 which machines so

00:26:00,890 --> 00:26:07,690
the factor is very obviously

00:26:10,210 --> 00:26:15,940
so thank you

00:26:12,400 --> 00:26:15,940
any process

00:26:54,690 --> 00:27:01,009
hello this is a great tour of

00:26:58,700 --> 00:27:05,749
optimizations and performance problems

00:27:01,009 --> 00:27:09,749
one question do you think do you think

00:27:05,749 --> 00:27:11,090
course scheduling is going to be what

00:27:09,749 --> 00:27:17,599
are your thoughts on course scheduling

00:27:11,090 --> 00:27:26,519
in relation to l1 TF and hyper-threading

00:27:17,599 --> 00:27:31,820
no.1 TF you mean yes okay

00:27:26,519 --> 00:27:35,279
l1 TF is usually because of the

00:27:31,820 --> 00:27:41,039
hyper shred of visible feature you know

00:27:35,279 --> 00:27:45,239
and if we the cost scheduling can be can

00:27:41,039 --> 00:27:50,840
be used with that scenario maybe it is

00:27:45,239 --> 00:27:54,840
useful but this was gathering can't

00:27:50,840 --> 00:27:58,049
require the we see PPC peels are the

00:27:54,840 --> 00:28:02,009
same call but maybe it's a cost carrier

00:27:58,049 --> 00:28:05,639
it's not that cost gathering I mean I

00:28:02,009 --> 00:28:07,529
think I think I think you're saying

00:28:05,639 --> 00:28:09,929
strict Co scheduling for the whole VM

00:28:07,529 --> 00:28:12,960
isn't necessary but just that the hyper

00:28:09,929 --> 00:28:14,690
thread pair kind of basis yeah I mean I

00:28:12,960 --> 00:28:18,809
reckon all the public clouds

00:28:14,690 --> 00:28:22,859
you know everyone didn't turn off hyper

00:28:18,809 --> 00:28:24,720
threading so what are they doing I went

00:28:22,859 --> 00:28:26,279
I wonder and there's the Amazon that

00:28:24,720 --> 00:28:27,330
patch set that's gone upstream as

00:28:26,279 --> 00:28:29,489
there's the wondering if it

00:28:27,330 --> 00:28:32,879
comments on the your perspective on the

00:28:29,489 --> 00:28:34,289
future of co scheduling and and whether

00:28:32,879 --> 00:28:36,330
or not that's going to be the the way to

00:28:34,289 --> 00:28:41,059
do safe hyper-threading for virtual

00:28:36,330 --> 00:28:41,059
machines I mean I think it is yeah

00:28:42,859 --> 00:28:50,929
twelve O'Clock continue well usually a

00:28:45,869 --> 00:28:55,909
vine to sell alcohol to same customers

00:28:50,929 --> 00:28:59,489
to avoid to the the profits panting so

00:28:55,909 --> 00:29:02,129
the information clue in the a through

00:28:59,489 --> 00:29:05,089
cash belong to one to verse so no

00:29:02,129 --> 00:29:05,089
problem yeah

00:29:09,000 --> 00:29:16,080
okay that's all thank you thank you

00:29:12,710 --> 00:29:21,859
[Applause]

00:29:16,080 --> 00:29:21,859

YouTube URL: https://www.youtube.com/watch?v=TT1b1WdnbM0


