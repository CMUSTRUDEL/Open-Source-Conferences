Title: Adaptive Live Migration by Xiao Guangrong & Yulei Zhang
Publication date: 2018-11-14
Playlist: KVM Forum 2018
Description: 
	Pre-copy migration could fail for many cases, e.g, if there are memory intensive workloads in VM. Fortunately, QEMU/KVM gains some features to improve it, however these features require the user need to pre-know the workloads in VM, e.g, enable compression only if the data is compressible and system has enough resource to do compression that is not friendly to the public cloud providers. Post-copy migration improves the situations indeed, however, it suffers some shortcomings, e.g, it is unrecoverable, poor performance, etc.

We will present adaptive live migration which speculates VM's workload from host side then enables pre-copy features and adjusts its parameters dynamically, during live migration, it detects the tendency and try next feature if current tendency shows live migration is impossible to success. Try post-copy if we have used all ways but pre-copy still can not success.

---

Xiao Guangrong
Senior Software Engineer
Tencent Cloud

Xiao Guangrong is a Linux Kernel developer working on Ftrace, MM, Btrfs but his main interest is KVM. As a active contributor, he was invited to give some presentations at some conferences: Japan LinuxCon 2011, Japan LinuxCon 2012 China CLK 2012, KVM Forum 2016 and KVM Forum 2017. He is the maintainer of NVDIMM in Qemuâ€™s community who designed and implemented NVDIMM in KVM and Qemu.

Yulei Zhang
Tencent Cloud
Captions: 
	00:00:01,040 --> 00:00:09,269
[Music]

00:00:05,930 --> 00:00:12,090
thanks for joining this session my name

00:00:09,269 --> 00:00:14,940
is Julie John I'm working for a 10 cent

00:00:12,090 --> 00:00:16,400
cloud which is one of the leading public

00:00:14,940 --> 00:00:20,880
cloud providers

00:00:16,400 --> 00:00:24,180
jaguaron is our open source leader who

00:00:20,880 --> 00:00:26,970
was initially this idea but due to the

00:00:24,180 --> 00:00:29,039
travel conflict he cannot make the trip

00:00:26,970 --> 00:00:33,090
to Edinburgh so I will present this

00:00:29,039 --> 00:00:35,250
topic on behalf of him and after this

00:00:33,090 --> 00:00:37,290
after this section if you are interested

00:00:35,250 --> 00:00:40,110
in this topic you can reach us through

00:00:37,290 --> 00:00:46,140
the email address any question or

00:00:40,110 --> 00:00:48,570
suggestions are welcomed this is our

00:00:46,140 --> 00:00:50,460
agenda for today's topic but first we

00:00:48,570 --> 00:00:52,230
will talk about the status of current or

00:00:50,460 --> 00:00:55,050
live migration and then we will

00:00:52,230 --> 00:00:57,149
elaborate the adaptable emigration we

00:00:55,050 --> 00:01:00,899
propose to improve the growing success

00:00:57,149 --> 00:01:03,359
rate indeed house and at last we will

00:01:00,899 --> 00:01:08,640
discuss the works need to be done in the

00:01:03,359 --> 00:01:10,710
future I think we all know the

00:01:08,640 --> 00:01:13,500
importance of land migration in call

00:01:10,710 --> 00:01:15,240
usage as you can see just this afternoon

00:01:13,500 --> 00:01:18,869
we'll have several topics about our

00:01:15,240 --> 00:01:21,750
migration and I just heard about another

00:01:18,869 --> 00:01:25,320
public cloud provider use F of T to

00:01:21,750 --> 00:01:29,970
predict workload in cloud that's very

00:01:25,320 --> 00:01:32,820
interesting and as we know so far the

00:01:29,970 --> 00:01:36,360
new cloud usage such as data learning

00:01:32,820 --> 00:01:38,670
like deep learning data mining and as a

00:01:36,360 --> 00:01:42,360
memory intensive workload bringing

00:01:38,670 --> 00:01:44,729
challenges for migration the memory data

00:01:42,360 --> 00:01:47,790
rate may be much faster than the

00:01:44,729 --> 00:01:49,890
networking transfer speed and also I

00:01:47,790 --> 00:01:53,369
always sensitive VM requests the extreme

00:01:49,890 --> 00:01:56,250
low downtown and the low latency so we

00:01:53,369 --> 00:02:00,329
think there should be something in

00:01:56,250 --> 00:02:03,030
improvement needed in this area and come

00:02:00,329 --> 00:02:05,719
up with this idea which called adaptive

00:02:03,030 --> 00:02:05,719
lab migration

00:02:08,259 --> 00:02:12,810
and the disease lies I want to ask this

00:02:10,090 --> 00:02:15,280
what tension cloud has contributed

00:02:12,810 --> 00:02:17,620
contributed for the llama crushing in

00:02:15,280 --> 00:02:20,860
the past the first one is what we

00:02:17,620 --> 00:02:22,360
lectured in last year given forum which

00:02:20,860 --> 00:02:24,519
is presented by a shaman

00:02:22,360 --> 00:02:27,970
it's called the faster ride protect and

00:02:24,519 --> 00:02:29,739
fast 30 local bitmap sync up and the

00:02:27,970 --> 00:02:33,489
second one is also presented by Joe

00:02:29,739 --> 00:02:39,790
Warren in this year's Asia RLC you can

00:02:33,489 --> 00:02:43,299
find the details through the link that's

00:02:39,790 --> 00:02:50,799
then let's talk about shortcomings in

00:02:43,299 --> 00:02:54,360
current land by Beijing the window in

00:02:50,799 --> 00:02:58,180
pre copy memory migration the hypervisor

00:02:54,360 --> 00:03:00,250
typically copies odd memory pages from

00:02:58,180 --> 00:03:03,760
source to destination while the VM is

00:03:00,250 --> 00:03:06,160
still running on the source if some

00:03:03,760 --> 00:03:08,620
memory pages changes during this process

00:03:06,160 --> 00:03:11,650
it will be recovered until the rate of

00:03:08,620 --> 00:03:16,959
recovery page is not less than the page

00:03:11,650 --> 00:03:20,790
30 read so far 16 optimization methods

00:03:16,959 --> 00:03:23,440
such as memory compression Expedia re

00:03:20,790 --> 00:03:27,819
but due to the work of the different

00:03:23,440 --> 00:03:31,269
depreciation as I said before currently

00:03:27,819 --> 00:03:36,220
no optimizing is all purpose so let's go

00:03:31,269 --> 00:03:38,859
through some one by one the first world

00:03:36,220 --> 00:03:43,690
talk is auto converge Auto Komachi is

00:03:38,859 --> 00:03:46,569
dynamically throughout the VCO to foster

00:03:43,690 --> 00:03:49,150
VM to dirty less memory but the

00:03:46,569 --> 00:03:52,209
disadvantages obviously as it would make

00:03:49,150 --> 00:03:56,139
VM completely unusable if the migration

00:03:52,209 --> 00:03:58,630
is remaining unsuccessful and also it

00:03:56,139 --> 00:04:01,299
may cause a big letting say to handle

00:03:58,630 --> 00:04:04,269
our request for the VM

00:04:01,299 --> 00:04:07,389
for example it may loss even at a packet

00:04:04,269 --> 00:04:10,720
caused pin failure and the bring better

00:04:07,389 --> 00:04:13,810
user experience to customers so from

00:04:10,720 --> 00:04:15,850
this fact we may need to put our status

00:04:13,810 --> 00:04:17,859
into consideration before we want to

00:04:15,850 --> 00:04:20,370
enable the auto coverage in long

00:04:17,859 --> 00:04:20,370
migration

00:04:21,030 --> 00:04:26,590
then is the memory compression during

00:04:24,250 --> 00:04:28,900
the pre copy it will use multiple thread

00:04:26,590 --> 00:04:31,600
to compress the data before post them to

00:04:28,900 --> 00:04:35,530
the network and we can see the

00:04:31,600 --> 00:04:38,350
disadvantage of memory compression is it

00:04:35,530 --> 00:04:40,360
will cost highly CPU usage and it

00:04:38,350 --> 00:04:44,680
depends on if the memory is compressible

00:04:40,360 --> 00:04:47,260
so in order to use the compression we

00:04:44,680 --> 00:04:49,000
may put the system CPU usage and the

00:04:47,260 --> 00:04:57,220
guest memory status into account the

00:04:49,000 --> 00:04:59,100
regime XP is also a kind of a

00:04:57,220 --> 00:05:01,600
compression instead of sending the

00:04:59,100 --> 00:05:04,810
changing gassed pages it will send a

00:05:01,600 --> 00:05:08,350
compressed version of updated in gassed

00:05:04,810 --> 00:05:10,240
pages but there is also disadvantages it

00:05:08,350 --> 00:05:12,760
requests additional system memories and

00:05:10,240 --> 00:05:14,890
the due to the migration the system

00:05:12,760 --> 00:05:18,100
results may be changed all the hand so

00:05:14,890 --> 00:05:20,830
if it becomes out of system memory it

00:05:18,100 --> 00:05:26,800
will not it is not properly to enable

00:05:20,830 --> 00:05:29,470
the expedia and it may cause the

00:05:26,800 --> 00:05:31,870
compression very hard to finish so we

00:05:29,470 --> 00:05:34,860
may put the following to factor into

00:05:31,870 --> 00:05:39,190
consideration to enable the expiry

00:05:34,860 --> 00:05:47,740
that's the vm memory is friendly to

00:05:39,190 --> 00:05:50,410
expiry and the system memory usage ok

00:05:47,740 --> 00:05:53,490
the last one is the post copy post copy

00:05:50,410 --> 00:05:56,560
means the vm starts running on the

00:05:53,490 --> 00:05:59,740
destination host as soon as possible and

00:05:56,560 --> 00:06:02,430
the ran from the host source host is

00:05:59,740 --> 00:06:06,940
paging forth in to destination over time

00:06:02,430 --> 00:06:09,610
it's it's discounted the advantage to

00:06:06,940 --> 00:06:12,070
use post copy is during the post copy

00:06:09,610 --> 00:06:15,100
sometimes it has to suffer the poor

00:06:12,070 --> 00:06:19,360
performance because we need to wait

00:06:15,100 --> 00:06:22,480
memory copy from the source host and if

00:06:19,360 --> 00:06:24,880
we say something happens during the post

00:06:22,480 --> 00:06:28,870
copy we may not we may not be able to

00:06:24,880 --> 00:06:30,970
recover the VM so to our perspective the

00:06:28,870 --> 00:06:33,660
post copy will be the last ones for our

00:06:30,970 --> 00:06:33,660
macro regime

00:06:34,900 --> 00:06:38,890
based on this description we can

00:06:36,580 --> 00:06:41,410
understand how to successfully and

00:06:38,890 --> 00:06:43,630
safely do the live migration says that

00:06:41,410 --> 00:06:46,960
two things are certain we need to try

00:06:43,630 --> 00:06:50,500
the best to make Perico a success and it

00:06:46,960 --> 00:06:53,140
is better not to use the post copy in

00:06:50,500 --> 00:06:56,200
order to achieve this it comes out out

00:06:53,140 --> 00:06:58,960
our proposal to enable adaptive la

00:06:56,200 --> 00:07:00,970
migration which is provided which will

00:06:58,960 --> 00:07:06,880
provide the higher successful rate and

00:07:00,970 --> 00:07:10,690
the better user experience so what is

00:07:06,880 --> 00:07:13,630
adaptive lab migration it has four steps

00:07:10,690 --> 00:07:16,270
at first we will speculate the workload

00:07:13,630 --> 00:07:19,510
in VN from the host site to get

00:07:16,270 --> 00:07:25,590
information such as total memory rate VM

00:07:19,510 --> 00:07:27,970
our status statistics and then second

00:07:25,590 --> 00:07:32,320
adaptively enable or disable the

00:07:27,970 --> 00:07:35,500
optimization method method measures such

00:07:32,320 --> 00:07:41,550
as compression auto coverage and ex busy

00:07:35,500 --> 00:07:44,260
our area and after enable the

00:07:41,550 --> 00:07:46,900
optimization method we will dynamically

00:07:44,260 --> 00:07:49,390
fight you the parameter the parameters

00:07:46,900 --> 00:07:53,100
of it to make the migration successful

00:07:49,390 --> 00:07:56,020
at last if there's no way to make the

00:07:53,100 --> 00:08:03,100
recovery successful we will turn on the

00:07:56,020 --> 00:08:05,860
post copy this is the overview picture

00:08:03,100 --> 00:08:10,420
shows the migration path of adaptive

00:08:05,860 --> 00:08:12,210
llama regime so you can see in the pre

00:08:10,420 --> 00:08:17,830
copied stage we will try different

00:08:12,210 --> 00:08:20,950
optimization method colleges to make

00:08:17,830 --> 00:08:23,320
sure the pre copy will success but

00:08:20,950 --> 00:08:25,750
unfortunately if unfortunately the pre

00:08:23,320 --> 00:08:30,250
copy not success we have to enable the

00:08:25,750 --> 00:08:32,610
postcode way to help the macro regime

00:08:30,250 --> 00:08:32,610
success

00:08:35,409 --> 00:08:40,760
this is the architecture diagram of

00:08:37,729 --> 00:08:43,820
adaptive la migration you can see we

00:08:40,760 --> 00:08:46,700
introduced the adaptive llama wishing

00:08:43,820 --> 00:08:49,670
come true demon in the backend to handle

00:08:46,700 --> 00:08:52,910
the magic inside it we have two major

00:08:49,670 --> 00:08:54,920
components one is called the tendency

00:08:52,910 --> 00:08:57,589
detector and the other one is called the

00:08:54,920 --> 00:09:00,080
performance accessor as you can see that

00:08:57,589 --> 00:09:01,430
tenon sir detector will fetch the random

00:09:00,080 --> 00:09:03,560
status of migration

00:09:01,430 --> 00:09:06,890
if the migration cannot be success it

00:09:03,560 --> 00:09:09,290
will ask her performance accessor to try

00:09:06,890 --> 00:09:11,589
different optimizing session Master

00:09:09,290 --> 00:09:11,589
logic

00:09:20,400 --> 00:09:27,540
and meanwhile the performance accessor

00:09:22,590 --> 00:09:30,990
were selected information from from

00:09:27,540 --> 00:09:33,450
posts tenancy detector and the host

00:09:30,990 --> 00:09:35,780
system and they rely on this information

00:09:33,450 --> 00:09:43,020
that can find here the parameters or

00:09:35,780 --> 00:09:46,350
disabled enabled optimization this is

00:09:43,020 --> 00:09:50,130
the tenancy detector tenancy detector

00:09:46,350 --> 00:09:52,440
will dynamically detect if migration can

00:09:50,130 --> 00:09:55,050
success based on the performance

00:09:52,440 --> 00:09:57,900
statistic of migration country we use

00:09:55,050 --> 00:10:02,240
the data page rate you can see from the

00:09:57,900 --> 00:10:04,740
picture if the dirty bit rate become

00:10:02,240 --> 00:10:06,930
diverging means current migration cannot

00:10:04,740 --> 00:10:07,110
be success and we need to try something

00:10:06,930 --> 00:10:12,260
new

00:10:07,110 --> 00:10:12,260
until the tenancy becomes converging

00:10:14,870 --> 00:10:24,440
this is the flowchart about performance

00:10:19,740 --> 00:10:27,780
accessor as we can see so after we

00:10:24,440 --> 00:10:30,750
enable optimization method that we were

00:10:27,780 --> 00:10:32,700
tune the parameters for it and we will

00:10:30,750 --> 00:10:35,760
check if the migration can be successful

00:10:32,700 --> 00:10:38,160
if not we will check if we have tried

00:10:35,760 --> 00:10:40,530
all the optimization method if not we

00:10:38,160 --> 00:10:44,820
will choose another one and after that

00:10:40,530 --> 00:10:48,920
if we have used all the method we will

00:10:44,820 --> 00:10:48,920
have to enable the postal copy and

00:10:49,310 --> 00:10:55,200
probably we could use the optimization

00:10:51,990 --> 00:10:57,360
method in order like at first we will

00:10:55,200 --> 00:11:02,960
try the auto converge and then the

00:10:57,360 --> 00:11:02,960
memory compression and then d XP grr

00:11:05,950 --> 00:11:12,339
the histogram shows how we plan to

00:11:09,209 --> 00:11:14,709
select the system and we and information

00:11:12,339 --> 00:11:19,089
to find him in the parameters for camera

00:11:14,709 --> 00:11:21,490
compression for example from the system

00:11:19,089 --> 00:11:23,980
side if the network speed is fast enough

00:11:21,490 --> 00:11:26,079
for normal cast page transfer then we

00:11:23,980 --> 00:11:30,459
didn't need to enable the comparison at

00:11:26,079 --> 00:11:32,980
all and we also collect the hostess if

00:11:30,459 --> 00:11:36,910
usage to decide how many compression is

00:11:32,980 --> 00:11:41,920
read we could use to create and choose

00:11:36,910 --> 00:11:44,949
the proper compression level for the

00:11:41,920 --> 00:11:47,050
expedia is the same we could crack the

00:11:44,949 --> 00:11:51,180
system memory usage from the host site

00:11:47,050 --> 00:11:55,149
based on that to choose how many

00:11:51,180 --> 00:11:58,630
shredder could use to do the expiry and

00:11:55,149 --> 00:12:06,910
how much the expedite cache size could

00:11:58,630 --> 00:12:09,190
be and for auto convey or to converge

00:12:06,910 --> 00:12:17,050
recently as a new parameter is

00:12:09,190 --> 00:12:20,260
introduced called the max CPU throttles

00:12:17,050 --> 00:12:22,990
which is prevent to squeeze the cast cpu

00:12:20,260 --> 00:12:26,860
too much and make the vm completed and

00:12:22,990 --> 00:12:29,829
useful but in addition we think based on

00:12:26,860 --> 00:12:33,760
the i/o status of vm instead of kicked

00:12:29,829 --> 00:12:36,070
vcp you out of exclusion and the sleep

00:12:33,760 --> 00:12:39,480
advice views rather without interrupted

00:12:36,070 --> 00:12:43,269
response in us space we would like to

00:12:39,480 --> 00:12:45,279
block the v cpu in the given module so

00:12:43,269 --> 00:12:47,910
that it could still responds to the our

00:12:45,279 --> 00:12:47,910
Christina

00:12:54,129 --> 00:13:07,339
so any question for this so at last we

00:13:04,129 --> 00:13:10,459
will talk about the future works and

00:13:07,339 --> 00:13:13,429
what we have done and we would do to

00:13:10,459 --> 00:13:16,879
optimize the land migration for example

00:13:13,429 --> 00:13:19,189
we introduced our Lockleys shred mode

00:13:16,879 --> 00:13:22,040
for memory compression which is the

00:13:19,189 --> 00:13:24,259
implemented based on the PTR ring and

00:13:22,040 --> 00:13:27,379
also we increased the job request number

00:13:24,259 --> 00:13:30,290
for eg compresses thread which has

00:13:27,379 --> 00:13:32,809
proved to be much more efficient during

00:13:30,290 --> 00:13:35,209
the migration and the for Expedia early

00:13:32,809 --> 00:13:37,730
we would like to introduce also

00:13:35,209 --> 00:13:40,220
introduce the models read mode for it

00:13:37,730 --> 00:13:46,129
which is currently not supported in

00:13:40,220 --> 00:13:47,839
upstream code and we will do more tasks

00:13:46,129 --> 00:13:50,149
to enable the dynamic parameter

00:13:47,839 --> 00:13:52,850
adjustment and the finally integrates

00:13:50,149 --> 00:13:58,839
them into the adaptive lama

00:13:52,850 --> 00:14:02,740
version control demo that's all from me

00:13:58,839 --> 00:14:02,740
thank you so any question

00:14:11,350 --> 00:14:18,430
very cool yeah thank you the auto

00:14:15,310 --> 00:14:23,130
converge know how what mechanism do you

00:14:18,430 --> 00:14:26,110
use to attenuate CPU speed V CPU speed

00:14:23,130 --> 00:14:28,600
how do you slow them down so them down

00:14:26,110 --> 00:14:31,780
just kicking them out of the excursion

00:14:28,600 --> 00:14:34,330
but we still keep the keep it in the

00:14:31,780 --> 00:14:38,470
given module in the condom on you so

00:14:34,330 --> 00:14:40,240
it's used the VCO brock method so it can

00:14:38,470 --> 00:14:42,550
still responds to the external

00:14:40,240 --> 00:14:45,240
interrupts and as after the kept

00:14:42,550 --> 00:14:47,770
interrupted it can go back to the VM to

00:14:45,240 --> 00:14:51,460
finish his job and we were quickly out

00:14:47,770 --> 00:14:55,540
again did you add a new I octal to do

00:14:51,460 --> 00:14:58,810
that I think we will add a new i/o

00:14:55,540 --> 00:14:59,500
control for the KVM CPU CPU Kevin yeah

00:14:58,810 --> 00:15:07,960
to do that

00:14:59,500 --> 00:15:10,990
thank you I have to test the multi until

00:15:07,960 --> 00:15:14,770
the new code that there is a queue for

00:15:10,990 --> 00:15:19,530
that multi of the three copy with

00:15:14,770 --> 00:15:23,440
several several streams at the same time

00:15:19,530 --> 00:15:25,660
excuse me you think you knew there is a

00:15:23,440 --> 00:15:28,450
new code that I wrote that it was multi

00:15:25,660 --> 00:15:30,790
FD that are asked to do migration with

00:15:28,450 --> 00:15:33,880
several strips at the same time that is

00:15:30,790 --> 00:15:36,580
going faster than what we have at the

00:15:33,880 --> 00:15:42,750
previous Pro copy you have the chance to

00:15:36,580 --> 00:15:42,750
test it you know we're working on it

00:15:48,370 --> 00:15:56,759
last time I looked at the way qmu

00:15:52,120 --> 00:16:15,790
throttle the CPUs for 4l divert

00:15:56,759 --> 00:16:18,129
naturally because there were problems to

00:16:15,790 --> 00:16:20,579
do with the CPU scheduling he was left

00:16:18,129 --> 00:16:23,949
too much in that this fuses with either

00:16:20,579 --> 00:16:26,589
run or not respect the throttle or not

00:16:23,949 --> 00:16:29,589
run and sleep too much is that something

00:16:26,589 --> 00:16:31,269
you encountered at all that's on your

00:16:29,589 --> 00:16:33,699
list of maybe future works to look into

00:16:31,269 --> 00:16:47,319
I think definitely we will look into

00:16:33,699 --> 00:16:57,249
that and I hope to talk with you well

00:16:47,319 --> 00:16:59,410
they did some similar study on - not yet

00:16:57,249 --> 00:17:02,040
but if you can't upon me - that I was

00:16:59,410 --> 00:17:02,040
appreciative

00:17:09,240 --> 00:17:16,010
so no other wish oh thank you thank you

00:17:12,790 --> 00:17:16,010
[Applause]

00:17:16,040 --> 00:17:21,829

YouTube URL: https://www.youtube.com/watch?v=qLoZ2z2TcMY


