Title: Facilitating Incremental Backup by Eric Blake
Publication date: 2018-11-17
Playlist: KVM Forum 2018
Description: 
	As any good sysadmin will tell you, collecting data backups are essential for future recovery needs. But making them more efficient and flexible has been an ongoing quest, involving multiple improvements throughout the virtualization stack. In this talk, Eric Blake will give a demonstration of the newest technique available through the qemu+libvirt stack, which now support the ability to orchestrate online incremental backups where third-party clients can grab just the dirty clusters at their own speed, rather than waiting for qemu to push a full backup image to a destination location.

---

Eric Blake
Principal Software Engineer
Red Hat

I have worked with libvirt and qemu since 2010 as part of the Red Hat virtualization team. In that time, I have spoken at several conferences on topics related to virtualization; most recently as a presenter at KVM Forum 2015 and a BOF session lead at KVM Forum 2017.
Captions: 
	00:00:01,040 --> 00:00:09,059
[Music]

00:00:06,230 --> 00:00:11,370
welcome this is my chance to talk about

00:00:09,059 --> 00:00:14,370
facilitating incremental backup a little

00:00:11,370 --> 00:00:15,960
about about me I'm Eric Blake then with

00:00:14,370 --> 00:00:19,050
red hat the last eight and a half years

00:00:15,960 --> 00:00:21,630
on Lippert and Kumu doing a lot of

00:00:19,050 --> 00:00:24,449
design in the virtualization stack at

00:00:21,630 --> 00:00:28,410
the lower layers and get to have fun

00:00:24,449 --> 00:00:30,210
with this topic today we'll look at what

00:00:28,410 --> 00:00:33,809
liberabit can already do for capturing

00:00:30,210 --> 00:00:37,770
disk backups then the new API that I'm

00:00:33,809 --> 00:00:40,739
proposing for doing backups with push

00:00:37,770 --> 00:00:42,989
mode and Paul mode how the third-party

00:00:40,739 --> 00:00:45,890
applications can access those backups

00:00:42,989 --> 00:00:48,600
and what the power of the new API adds

00:00:45,890 --> 00:00:51,750
especially in the regards to incremental

00:00:48,600 --> 00:00:54,660
backups and with any luck my demo will

00:00:51,750 --> 00:00:56,670
show it in action I've got some

00:00:54,660 --> 00:00:58,920
conventions my slides are a bit wordy if

00:00:56,670 --> 00:01:00,390
you want to learn about this talk read

00:00:58,920 --> 00:01:02,820
the slides later I'm gonna skip over

00:01:00,390 --> 00:01:04,769
details to get to the demo but most

00:01:02,820 --> 00:01:05,430
importantly is what I will be using in

00:01:04,769 --> 00:01:09,570
the demo

00:01:05,430 --> 00:01:11,820
I've got images with contents that are

00:01:09,570 --> 00:01:15,000
they're visible to whoever reads the

00:01:11,820 --> 00:01:17,189
file and as the guest modifies those

00:01:15,000 --> 00:01:20,100
images those changes show up on the

00:01:17,189 --> 00:01:22,710
screen and try to use that as a data

00:01:20,100 --> 00:01:24,270
flow to figure out well where is the

00:01:22,710 --> 00:01:26,310
data moving which files gets which

00:01:24,270 --> 00:01:31,549
contents and how do I know it's a backup

00:01:26,310 --> 00:01:36,720
so I'll try my best to explain those

00:01:31,549 --> 00:01:40,200
usage patterns as a baseline I used QE

00:01:36,720 --> 00:01:42,360
3.0 as it was released and shipped by

00:01:40,200 --> 00:01:44,159
fedora there are some bug's seeing qmo

00:01:42,360 --> 00:01:46,860
if you use it just wrong it will crash

00:01:44,159 --> 00:01:48,270
that have been fixed since but they

00:01:46,860 --> 00:01:50,880
won't hurt the demo today

00:01:48,270 --> 00:01:52,860
live for 4.8 plus my patches to add the

00:01:50,880 --> 00:01:55,320
new API and they were posted to the

00:01:52,860 --> 00:01:57,060
mailing list but still undergoing a lot

00:01:55,320 --> 00:01:59,310
of review a lot of discussions so there

00:01:57,060 --> 00:02:00,509
may be some tweaks to what finally lands

00:01:59,310 --> 00:02:04,110
in the tree compared to what you see

00:02:00,509 --> 00:02:09,300
today of course we want to see what can

00:02:04,110 --> 00:02:11,819
livered already do we can start by

00:02:09,300 --> 00:02:13,500
creating a guest with two disks 9/2 is

00:02:11,819 --> 00:02:16,650
more fun than one we want to make sure

00:02:13,500 --> 00:02:20,610
this works for builders an amazing

00:02:16,650 --> 00:02:24,270
product thanks to rich and Friends I'm

00:02:20,610 --> 00:02:27,240
gonna use vert install to create a

00:02:24,270 --> 00:02:29,040
Fedora 25 guest why 25 because that's

00:02:27,240 --> 00:02:30,660
what I had cached on my laptop I'm not

00:02:29,040 --> 00:02:33,720
going to waste your time downloading a

00:02:30,660 --> 00:02:35,640
full image and we're gonna start this

00:02:33,720 --> 00:02:39,870
guest it's going to have two discs each

00:02:35,640 --> 00:02:42,840
with a backing file so data wise we

00:02:39,870 --> 00:02:46,170
started with the OS install then we

00:02:42,840 --> 00:02:48,860
created the overlay files and as the

00:02:46,170 --> 00:02:51,900
guest writes data into the overlays

00:02:48,860 --> 00:02:54,420
there's two different sets of datas in

00:02:51,900 --> 00:03:00,750
the file and a third set of data is

00:02:54,420 --> 00:03:07,489
what's logically what the guest sees and

00:03:00,750 --> 00:03:07,489
a wise

00:03:14,959 --> 00:03:20,430
so again to show everything I am running

00:03:18,720 --> 00:03:24,750
with selinux and forcing on a good

00:03:20,430 --> 00:03:28,110
citizen run with myself built libvirt

00:03:24,750 --> 00:03:30,840
buttstock camo vert builder's some time

00:03:28,110 --> 00:03:35,129
compression they're amazing how fast you

00:03:30,840 --> 00:03:37,739
can build an image build my disc's love

00:03:35,129 --> 00:03:40,409
my vert install with two disks create

00:03:37,739 --> 00:03:44,000
everything start my domain and boom we

00:03:40,409 --> 00:03:44,000
have a demo ready to go

00:03:57,360 --> 00:04:01,650
now we've got our demo on the first

00:03:59,070 --> 00:04:05,760
thing we can do is verse block copy as a

00:04:01,650 --> 00:04:09,150
way to copy data the concept here is

00:04:05,760 --> 00:04:13,830
that we set up a live mirror everything

00:04:09,150 --> 00:04:16,079
that the guest already has written q mu

00:04:13,830 --> 00:04:18,750
starts to copy into the destination in

00:04:16,079 --> 00:04:20,790
the background whether that's the

00:04:18,750 --> 00:04:23,160
shallow copy I did that on a shallow

00:04:20,790 --> 00:04:25,710
copy on the first disk or a deep copy on

00:04:23,160 --> 00:04:28,560
the second disk and as the guest comes

00:04:25,710 --> 00:04:29,910
along and rights the rights override

00:04:28,560 --> 00:04:32,490
anything that has already been copied

00:04:29,910 --> 00:04:34,080
and you always have the latest state of

00:04:32,490 --> 00:04:37,080
what the guest sees into you're

00:04:34,080 --> 00:04:38,669
mirroring when you are happy that the

00:04:37,080 --> 00:04:41,400
mirroring has captured the entire

00:04:38,669 --> 00:04:46,020
contents of the disk then you have to

00:04:41,400 --> 00:04:49,350
stop the guest tell liver to copy that

00:04:46,020 --> 00:04:51,720
to close out the copies and then resume

00:04:49,350 --> 00:04:55,700
the disk the downtime is small but it is

00:04:51,720 --> 00:04:58,560
downtime as a result your backup file

00:04:55,700 --> 00:05:00,090
has the data at the time you ended the

00:04:58,560 --> 00:05:02,340
command and anything the guest writes

00:05:00,090 --> 00:05:04,110
after you end the command is now

00:05:02,340 --> 00:05:07,680
separate from the copy you created so

00:05:04,110 --> 00:05:09,479
you have a backup a lot of api's were

00:05:07,680 --> 00:05:11,280
involved ver domain block copy which

00:05:09,479 --> 00:05:14,610
called Drive near I'm not going to read

00:05:11,280 --> 00:05:16,290
the slide to you comparison table point

00:05:14,610 --> 00:05:19,229
in time is at the end that's lousy

00:05:16,290 --> 00:05:21,660
nobody likes that but it works domain

00:05:19,229 --> 00:05:23,729
model we didn't have to change the

00:05:21,660 --> 00:05:25,430
domain we had the same set of disks from

00:05:23,729 --> 00:05:27,660
livers point of view the whole operation

00:05:25,430 --> 00:05:28,440
multiple disk I had to pause the guest

00:05:27,660 --> 00:05:30,660
oh well

00:05:28,440 --> 00:05:34,830
shallow copy we can do shallow or deep

00:05:30,660 --> 00:05:38,100
if we have a chain of disks API calls I

00:05:34,830 --> 00:05:40,250
used a bunch third party use q mu did

00:05:38,100 --> 00:05:42,870
all the writing nobody else got a chance

00:05:40,250 --> 00:05:45,479
incremental no it's all or nothing you

00:05:42,870 --> 00:05:47,820
get the disk or nothing else or the disk

00:05:45,479 --> 00:05:50,280
as it is so can we do better

00:05:47,820 --> 00:05:51,870
we have snapshot and commit every time

00:05:50,280 --> 00:05:55,370
we create a snapshot we create another

00:05:51,870 --> 00:05:57,630
layer of chains of disks if we are

00:05:55,370 --> 00:05:59,370
temporary about it then we can use it

00:05:57,630 --> 00:06:01,160
for backup purposes if you're permanent

00:05:59,370 --> 00:06:04,080
you can do a little bit more playing

00:06:01,160 --> 00:06:07,020
with the temporary come at a snapshot

00:06:04,080 --> 00:06:09,990
your snapshot starts out empty as the

00:06:07,020 --> 00:06:11,070
guest writes data the guest visible data

00:06:09,990 --> 00:06:14,640
changes

00:06:11,070 --> 00:06:17,490
the snaps the new layer in the chain

00:06:14,640 --> 00:06:21,480
changes but the old layer stays put

00:06:17,490 --> 00:06:25,170
therefore I can use third-party tools to

00:06:21,480 --> 00:06:27,180
read that old layer copy with ref link

00:06:25,170 --> 00:06:29,820
always is nice and fast if you have a

00:06:27,180 --> 00:06:33,660
sufficient file system human image

00:06:29,820 --> 00:06:35,460
converts if you have some super cool I'm

00:06:33,660 --> 00:06:38,250
going to log into my storage array and

00:06:35,460 --> 00:06:42,630
tell him to do the snapshot whatever you

00:06:38,250 --> 00:06:45,720
have you can copy the read-only file

00:06:42,630 --> 00:06:47,400
while the guest is still writing and you

00:06:45,720 --> 00:06:49,770
will see the data at the point you

00:06:47,400 --> 00:06:51,360
created your overlay and then when

00:06:49,770 --> 00:06:54,450
you're done with your overlay you commit

00:06:51,360 --> 00:06:58,560
things back in block commit has to do

00:06:54,450 --> 00:07:00,930
multiple commands and liberabit will say

00:06:58,560 --> 00:07:02,640
I remember what was in my temporary

00:07:00,930 --> 00:07:05,490
overlay as part of committing it I will

00:07:02,640 --> 00:07:07,800
merge it back down and as the guest

00:07:05,490 --> 00:07:12,030
continues writing your snapshot your

00:07:07,800 --> 00:07:17,700
copy stays unchanged so you have a valid

00:07:12,030 --> 00:07:18,960
snapshot and lots of api's again so we

00:07:17,700 --> 00:07:22,140
got a point of time at the start that's

00:07:18,960 --> 00:07:24,300
better we had to change our domain XML

00:07:22,140 --> 00:07:25,950
boo if we have to reboot things and

00:07:24,300 --> 00:07:27,900
we're manually generating the XML every

00:07:25,950 --> 00:07:30,570
time that means we have to track all the

00:07:27,900 --> 00:07:33,480
XML changes although it gives us some

00:07:30,570 --> 00:07:35,700
flexibility the fact that we created a

00:07:33,480 --> 00:07:37,350
snapshot we can create multiple disks at

00:07:35,700 --> 00:07:38,970
the same point in time with no effort we

00:07:37,350 --> 00:07:42,540
didn't have to pause the guests to

00:07:38,970 --> 00:07:45,810
create consistent guests shallow copy

00:07:42,540 --> 00:07:48,990
still supported one less API call a

00:07:45,810 --> 00:07:51,960
third party you can only copy

00:07:48,990 --> 00:07:53,250
what is a standalone image you can do

00:07:51,960 --> 00:07:55,260
whatever you want with standalone images

00:07:53,250 --> 00:07:56,970
if you want to diff it great but diff is

00:07:55,260 --> 00:08:01,110
just as good as reading the whole image

00:07:56,970 --> 00:08:03,840
yourself so incremental again if you if

00:08:01,110 --> 00:08:06,360
you have a snapshot paradigm where you

00:08:03,840 --> 00:08:07,680
create a snapshot every day you're sort

00:08:06,360 --> 00:08:10,950
of keeping incremental debt backups

00:08:07,680 --> 00:08:13,170
already but you're doing all the work so

00:08:10,950 --> 00:08:17,580
the new API first thing everything

00:08:13,170 --> 00:08:20,070
livered is XML so I'm going to create an

00:08:17,580 --> 00:08:23,220
XML I want to do a domain backup I'm

00:08:20,070 --> 00:08:24,370
going to do a push mode push says QM ooh

00:08:23,220 --> 00:08:27,790
has the data and will

00:08:24,370 --> 00:08:30,550
push it into the destination I have my

00:08:27,790 --> 00:08:33,700
two disks in my thing I'm gonna push to

00:08:30,550 --> 00:08:36,700
back up 1.1 Q cow - I want a shallow

00:08:33,700 --> 00:08:41,440
coffee and I'm gonna push my second disc

00:08:36,700 --> 00:08:43,180
to back up 2 . full raw so again you

00:08:41,440 --> 00:08:44,200
have some flexibility whatever Q mu

00:08:43,180 --> 00:08:47,339
knows how to write

00:08:44,200 --> 00:08:50,710
you can put your back up into that image

00:08:47,339 --> 00:08:53,200
we're going to begin the job verse H

00:08:50,710 --> 00:08:55,230
back up begin with my domain and this

00:08:53,200 --> 00:08:58,480
XML that I just described everything

00:08:55,230 --> 00:09:00,640
creates the backup job this looks very

00:08:58,480 --> 00:09:02,110
similar to the mirroring job that I

00:09:00,640 --> 00:09:06,970
showed earlier the difference is that

00:09:02,110 --> 00:09:09,790
now Q and Munoz - copy off the point in

00:09:06,970 --> 00:09:12,460
time at the beginning so q mu starts by

00:09:09,790 --> 00:09:15,070
saying well if the guest beats me guest

00:09:12,460 --> 00:09:21,070
takes priority anytime the guest writes

00:09:15,070 --> 00:09:24,310
to the active layer the new data goes to

00:09:21,070 --> 00:09:27,070
the active layer and the old data goes

00:09:24,310 --> 00:09:28,870
to the backup so that we have the point

00:09:27,070 --> 00:09:31,660
in time at the time I started the backup

00:09:28,870 --> 00:09:33,790
and meanwhile q mu in the background is

00:09:31,660 --> 00:09:36,190
scanning the entire image everything

00:09:33,790 --> 00:09:38,740
else the guest could see and gets copied

00:09:36,190 --> 00:09:41,080
in to the backup if it's a full backup

00:09:38,740 --> 00:09:43,870
if it's a partial backup or if you a

00:09:41,080 --> 00:09:49,600
shallow backup it only copies what the

00:09:43,870 --> 00:09:54,220
shallow needs the operation is resilient

00:09:49,600 --> 00:09:57,550
the operation runs for as long as the

00:09:54,220 --> 00:10:00,550
guest is live if liver tree starts the

00:09:57,550 --> 00:10:01,779
operation is still going and then we do

00:10:00,550 --> 00:10:03,250
have to wait for it to complete because

00:10:01,779 --> 00:10:04,930
Q mu is pushing the data and it's

00:10:03,250 --> 00:10:07,630
pushing the data for the entire disk we

00:10:04,930 --> 00:10:09,580
have to wait around until Q mu says I'm

00:10:07,630 --> 00:10:13,390
done and which point he'll send us an

00:10:09,580 --> 00:10:15,910
event where we can pull for it using Dom

00:10:13,390 --> 00:10:17,380
job info when we have our answer that

00:10:15,910 --> 00:10:20,320
it's all there then we can say go ahead

00:10:17,380 --> 00:10:22,600
and enter the job at which point we have

00:10:20,320 --> 00:10:27,130
a backup and the guest continues to

00:10:22,600 --> 00:10:29,470
write data new API is ver domain backup

00:10:27,130 --> 00:10:32,440
begin is in there some of the existing

00:10:29,470 --> 00:10:34,630
api's gets a little bit smarter ver

00:10:32,440 --> 00:10:36,430
domain get job stats can now query about

00:10:34,630 --> 00:10:39,910
the backup job

00:10:36,430 --> 00:10:41,860
polling still works so we've improved we

00:10:39,910 --> 00:10:43,450
still do it at the start now that XML

00:10:41,860 --> 00:10:45,490
for the domain is unchanged to the

00:10:43,450 --> 00:10:48,250
entire operation we do multiple disks as

00:10:45,490 --> 00:10:50,110
an atomic group shallow copy still

00:10:48,250 --> 00:10:51,730
they're a lot fewer API is that's good

00:10:50,110 --> 00:10:54,640
means live firts doing all the work for

00:10:51,730 --> 00:10:58,350
you third party use not yet

00:10:54,640 --> 00:11:04,240
we'll get there and incremental yes

00:10:58,350 --> 00:11:06,790
we'll get there too NBD we're gonna

00:11:04,240 --> 00:11:10,890
change from push mode to pull mode all I

00:11:06,790 --> 00:11:13,810
did was change the mode line and add a

00:11:10,890 --> 00:11:16,660
server line that says I want an NB d

00:11:13,810 --> 00:11:18,899
server living here to expose my images

00:11:16,660 --> 00:11:21,790
I'm going to expose image V da as is

00:11:18,899 --> 00:11:23,589
image VDB I don't like what you're

00:11:21,790 --> 00:11:26,709
choosing I'm gonna give you a file name

00:11:23,589 --> 00:11:28,149
that you have to go through this is

00:11:26,709 --> 00:11:30,010
great if you have an image stored on a

00:11:28,149 --> 00:11:32,410
remote storage but you want your scratch

00:11:30,010 --> 00:11:36,520
on local storage for obvious efficiency

00:11:32,410 --> 00:11:38,080
reasons and then if I tell liver that

00:11:36,520 --> 00:11:42,010
needs a scratch file then I better have

00:11:38,080 --> 00:11:44,589
that scratch file available begin the

00:11:42,010 --> 00:11:45,790
job same line as before it's this time

00:11:44,589 --> 00:11:47,920
all I did was passed a different XML

00:11:45,790 --> 00:11:50,680
file same setup as before libvirt

00:11:47,920 --> 00:11:52,450
creates a file name if it if I didn't or

00:11:50,680 --> 00:11:55,690
it uses the file I'm I told it to if I

00:11:52,450 --> 00:11:58,390
did same things happening the scratch

00:11:55,690 --> 00:12:00,760
file starts out blank and QM uses it as

00:11:58,390 --> 00:12:03,700
it needs why do I have a scratch file

00:12:00,760 --> 00:12:07,300
well in push mode anytime the guest

00:12:03,700 --> 00:12:09,640
writes qumu takes the old data and

00:12:07,300 --> 00:12:12,520
writes the old data on your backup in

00:12:09,640 --> 00:12:14,410
pull mode anytime the guest writes

00:12:12,520 --> 00:12:15,790
Qumu who's not writing to the

00:12:14,410 --> 00:12:17,890
destination so it has to store it

00:12:15,790 --> 00:12:20,470
somewhere until a reader comes along so

00:12:17,890 --> 00:12:23,560
cue is writing to scratch waiting for

00:12:20,470 --> 00:12:25,240
the reader to come along and the guest

00:12:23,560 --> 00:12:26,589
still sees the new image so even though

00:12:25,240 --> 00:12:28,660
it looks like you're backing chain

00:12:26,589 --> 00:12:31,870
dependency we've got a little bit

00:12:28,660 --> 00:12:33,490
different semantics than usual now we've

00:12:31,870 --> 00:12:36,490
got an NB d server what can we do with

00:12:33,490 --> 00:12:39,970
it well the easiest is qumu image can

00:12:36,490 --> 00:12:43,329
connect to NPD servers and pull it off

00:12:39,970 --> 00:12:46,060
in whatever format q image knows so

00:12:43,329 --> 00:12:47,740
while the disk while the guest is

00:12:46,060 --> 00:12:48,730
running and while q is copying data

00:12:47,740 --> 00:12:52,720
around i connect

00:12:48,730 --> 00:12:55,930
my human image command and start reading

00:12:52,720 --> 00:12:58,180
off data as fast as I can see it if a

00:12:55,930 --> 00:13:00,910
guest right comes in in the middle then

00:12:58,180 --> 00:13:02,829
qmu copies the old data and again I'm

00:13:00,910 --> 00:13:04,570
still reading the old data so I see the

00:13:02,829 --> 00:13:07,300
same data whether or not the guest makes

00:13:04,570 --> 00:13:11,019
changes it makes it very nice that I see

00:13:07,300 --> 00:13:13,230
a consistent view of the guest that's

00:13:11,019 --> 00:13:16,720
not fast enough for you well we can do

00:13:13,230 --> 00:13:19,630
use the kernel and BT client use

00:13:16,720 --> 00:13:22,120
modprobe MBD connect up with qm BD to

00:13:19,630 --> 00:13:25,000
turn on dev and BD now I have dev NBD I

00:13:22,120 --> 00:13:26,800
have a raw block device that shows me

00:13:25,000 --> 00:13:29,380
the guest contents I can do whatever I

00:13:26,800 --> 00:13:33,670
want I can do DD capture a subset of the

00:13:29,380 --> 00:13:36,389
image yay I have captured one megabyte

00:13:33,670 --> 00:13:38,260
out of the entire image by reading

00:13:36,389 --> 00:13:41,350
regardless of whether the guest is

00:13:38,260 --> 00:13:45,670
written to that area or not or even

00:13:41,350 --> 00:13:48,399
fancier I can ask Q mu where are the

00:13:45,670 --> 00:13:53,019
important parts to read I'm going to do

00:13:48,399 --> 00:13:54,940
aq mu image map with my source file as

00:13:53,019 --> 00:13:59,170
information and say where is the data

00:13:54,940 --> 00:14:02,829
where is the holes and if the line has

00:13:59,170 --> 00:14:06,639
data then I will do a regular expression

00:14:02,829 --> 00:14:09,310
match and a Q mu I copy on read so I

00:14:06,639 --> 00:14:12,430
know there was data I'm going to read it

00:14:09,310 --> 00:14:15,940
the act of reading it copies it and now

00:14:12,430 --> 00:14:17,380
my backup has a copy of that data if it

00:14:15,940 --> 00:14:19,029
was not data then I just continued the

00:14:17,380 --> 00:14:20,860
loop and then I skipped all the holes so

00:14:19,029 --> 00:14:22,720
it's a little bit faster I only have to

00:14:20,860 --> 00:14:26,500
read a subset of the disk because I used

00:14:22,720 --> 00:14:29,350
the image map details and then when I'm

00:14:26,500 --> 00:14:31,329
done I started my image back pointing to

00:14:29,350 --> 00:14:35,100
my NBD server my MBD server is going

00:14:31,329 --> 00:14:38,800
away so i rebase it to be standalone

00:14:35,100 --> 00:14:42,010
again human image map tells me here's

00:14:38,800 --> 00:14:45,519
your data of interest at which point my

00:14:42,010 --> 00:14:48,399
copy process can read the data of

00:14:45,519 --> 00:14:51,550
interest into money now I read it into Q

00:14:48,399 --> 00:14:53,709
cow too but if you write your own NBD

00:14:51,550 --> 00:14:55,480
client you can write it into whatever

00:14:53,709 --> 00:14:59,860
format you desire at whatever speed you

00:14:55,480 --> 00:15:01,930
desire when I'm done copying my data

00:14:59,860 --> 00:15:02,560
notice I had multiple clients they all

00:15:01,930 --> 00:15:05,200
read the same

00:15:02,560 --> 00:15:09,040
data they all ran at the same time when

00:15:05,200 --> 00:15:11,290
I was done I tell libvirt that I don't

00:15:09,040 --> 00:15:14,230
need the NBD server anymore verse backup

00:15:11,290 --> 00:15:17,680
and get rid of any scratch files that I

00:15:14,230 --> 00:15:19,330
had to create and I have a backup file

00:15:17,680 --> 00:15:23,230
that has everything I needed and the

00:15:19,330 --> 00:15:25,240
guest continues to write data under the

00:15:23,230 --> 00:15:28,660
hood it's the same verse ver domain

00:15:25,240 --> 00:15:31,750
backup begin except this time all the

00:15:28,660 --> 00:15:34,089
work was in the push parameter - domain

00:15:31,750 --> 00:15:36,880
backup so the same API call does both

00:15:34,089 --> 00:15:40,029
push and pull a little bit of comparison

00:15:36,880 --> 00:15:41,560
they look the same until a low copy we

00:15:40,029 --> 00:15:43,810
don't have sha whoa copy and backup yet

00:15:41,560 --> 00:15:46,180
but that's XML can do it

00:15:43,810 --> 00:15:47,740
so we can get that in the future one

00:15:46,180 --> 00:15:49,630
less API that's because I didn't have to

00:15:47,740 --> 00:15:53,950
pull for qme to be done I had to decide

00:15:49,630 --> 00:15:56,290
when I was done and third-party use yay

00:15:53,950 --> 00:15:59,890
I just showed some third-party reading

00:15:56,290 --> 00:16:01,480
it future enhancements will want to let

00:15:59,890 --> 00:16:04,540
liver tick the port instead of you

00:16:01,480 --> 00:16:08,200
having to pre-specify will want TCP

00:16:04,540 --> 00:16:09,850
encryption or UNIX sockets we want

00:16:08,200 --> 00:16:11,620
queued allow more than one job in

00:16:09,850 --> 00:16:13,000
parallel there's if I'm reading a backup

00:16:11,620 --> 00:16:13,780
and somebody else wants to read a backup

00:16:13,000 --> 00:16:15,490
that should work

00:16:13,780 --> 00:16:18,370
Kumu doesn't support it yet we'll get

00:16:15,490 --> 00:16:20,200
there but the API permits it because

00:16:18,370 --> 00:16:21,850
everything that the API does returns a

00:16:20,200 --> 00:16:25,180
job ID and we can have more than one job

00:16:21,850 --> 00:16:26,980
ID and everybody came here for

00:16:25,180 --> 00:16:27,870
differential backups let's see what

00:16:26,980 --> 00:16:30,730
happens

00:16:27,870 --> 00:16:33,339
definition wise as your guest is

00:16:30,730 --> 00:16:34,960
executing you create a check point that

00:16:33,339 --> 00:16:37,300
says this is the point in time please

00:16:34,960 --> 00:16:38,920
remember all changes since this time

00:16:37,300 --> 00:16:41,860
it's called change block tracking as

00:16:38,920 --> 00:16:43,450
another name Liberty or a cumulus

00:16:41,860 --> 00:16:45,730
implementation of that is it creates a

00:16:43,450 --> 00:16:47,680
bitmap every time you write the bitmap

00:16:45,730 --> 00:16:50,170
gets a bit set saying this cluster is

00:16:47,680 --> 00:16:52,150
now dirty you can create a new check

00:16:50,170 --> 00:16:54,520
point that says I want a new point in

00:16:52,150 --> 00:16:56,350
time to track liberties implementation

00:16:54,520 --> 00:16:58,990
of that says well okay stop this bitmap

00:16:56,350 --> 00:17:01,570
he's now frozen create a new bitmap to

00:16:58,990 --> 00:17:03,970
track the new changes an incremental

00:17:01,570 --> 00:17:06,490
backup says read to my most recent

00:17:03,970 --> 00:17:09,250
checkpoint no questions asked a

00:17:06,490 --> 00:17:10,839
differential says read to an arbitrary

00:17:09,250 --> 00:17:12,970
checkpoint and then live vert under the

00:17:10,839 --> 00:17:14,589
hood says well I know that that involved

00:17:12,970 --> 00:17:15,790
multiple bitmaps I'll merge those data

00:17:14,589 --> 00:17:17,860
together

00:17:15,790 --> 00:17:22,150
and now you have a single picture of all

00:17:17,860 --> 00:17:23,470
changes since that point in time first

00:17:22,150 --> 00:17:26,740
thing we can do with how much is the

00:17:23,470 --> 00:17:28,540
guests dirtying we can create a check

00:17:26,740 --> 00:17:30,790
map and then query with the size

00:17:28,540 --> 00:17:32,770
parameter and see the guest has touched

00:17:30,790 --> 00:17:34,750
this much data if I do a backup now

00:17:32,770 --> 00:17:37,530
that's approximately how much data it's

00:17:34,750 --> 00:17:40,030
a live number it's always growing so

00:17:37,530 --> 00:17:41,410
overestimate if you're actually sizing

00:17:40,030 --> 00:17:44,220
your destination to exactly what you

00:17:41,410 --> 00:17:46,390
read you may find yourself running short

00:17:44,220 --> 00:17:51,630
check point delete when we're done with

00:17:46,390 --> 00:17:54,820
it but we want the there's the XML again

00:17:51,630 --> 00:17:57,070
but we want to go on to incrementals so

00:17:54,820 --> 00:17:59,410
the same XML before I'm gonna go as

00:17:57,070 --> 00:18:00,880
simple as possible but this time I'm

00:17:59,410 --> 00:18:03,220
going to create a check point at the

00:18:00,880 --> 00:18:05,350
same time so now my command has my

00:18:03,220 --> 00:18:07,540
backup description and my check point

00:18:05,350 --> 00:18:11,770
commit description both of them get

00:18:07,540 --> 00:18:14,770
created at once activity wise as the

00:18:11,770 --> 00:18:16,660
data goes into the guest their check

00:18:14,770 --> 00:18:19,000
point starts tracking what the guest has

00:18:16,660 --> 00:18:22,450
written at the same time as the backup

00:18:19,000 --> 00:18:24,310
is reading the old data and then when

00:18:22,450 --> 00:18:28,480
that things are done now I create my

00:18:24,310 --> 00:18:30,520
next backup and I want to be able to

00:18:28,480 --> 00:18:33,700
read just those dirty bits we need help

00:18:30,520 --> 00:18:35,980
from NBD so NBT has added an extension

00:18:33,700 --> 00:18:39,220
called NBD command block status and qumu

00:18:35,980 --> 00:18:42,880
has added qumu dirty bit map name which

00:18:39,220 --> 00:18:45,250
exposes as block status here are the

00:18:42,880 --> 00:18:48,880
portions that my bit map says are dirty

00:18:45,250 --> 00:18:51,850
so with a little bit of fancy setup and

00:18:48,880 --> 00:18:53,380
a lot of luck in this same loop that

00:18:51,850 --> 00:18:55,390
looks like we saw before when we read

00:18:53,380 --> 00:18:57,310
the full image we're using map to say

00:18:55,390 --> 00:19:00,040
where the dirty bits and the loop to

00:18:57,310 --> 00:19:03,010
read the just those bits and INRI based

00:19:00,040 --> 00:19:06,400
on the end and we have an incremental

00:19:03,010 --> 00:19:08,680
snapshot again the data has flowed

00:19:06,400 --> 00:19:10,450
through we've captured only the parts

00:19:08,680 --> 00:19:12,910
that the guest wrote since the last time

00:19:10,450 --> 00:19:15,450
we can rinse and repeat do this as many

00:19:12,910 --> 00:19:18,940
times as you want get a longer chain

00:19:15,450 --> 00:19:20,800
notice that the chain that the backups

00:19:18,940 --> 00:19:23,590
have have slightly different contents

00:19:20,800 --> 00:19:26,620
than the chain that the guest has that's

00:19:23,590 --> 00:19:29,230
okay as long as when you read the chain

00:19:26,620 --> 00:19:29,620
you see the same data as the guest saw

00:19:29,230 --> 00:19:33,760
that

00:19:29,620 --> 00:19:35,350
point in time you don't have to create a

00:19:33,760 --> 00:19:38,590
snapshot you can do an incremental diff

00:19:35,350 --> 00:19:40,030
with just without a checkpoint and then

00:19:38,590 --> 00:19:42,400
that just lets you snoop the disk and

00:19:40,030 --> 00:19:43,840
again the differential the point the

00:19:42,400 --> 00:19:46,540
fact that I can jump back to an earlier

00:19:43,840 --> 00:19:50,430
point in time and show the older chat

00:19:46,540 --> 00:19:52,930
older snapshot notice when I do a new

00:19:50,430 --> 00:19:54,970
run that I see different data in the

00:19:52,930 --> 00:19:57,220
guest and in my second backup than I do

00:19:54,970 --> 00:19:59,110
in my first that's because the guest is

00:19:57,220 --> 00:20:00,610
always running your incremental backup

00:19:59,110 --> 00:20:02,260
is at the point in time you start it and

00:20:00,610 --> 00:20:05,620
if you start at two different times

00:20:02,260 --> 00:20:07,929
you'll see two different guest data so

00:20:05,620 --> 00:20:09,580
there's the XML livered is running a lot

00:20:07,929 --> 00:20:12,940
of qmv commands under the hood for your

00:20:09,580 --> 00:20:14,470
benefit and we have more beyond this

00:20:12,940 --> 00:20:17,050
talk if you stick around for vladimir

00:20:14,470 --> 00:20:20,850
you'll learn more about how they run now

00:20:17,050 --> 00:20:20,850
on to the demo

00:20:34,639 --> 00:20:41,669
with the full bitmap or full backup I'm

00:20:38,669 --> 00:20:47,159
going to show I created the XML ran my

00:20:41,669 --> 00:20:50,700
guest I am actually SSA Qing into the

00:20:47,159 --> 00:20:53,759
guest to make some changes on the fly to

00:20:50,700 --> 00:20:56,100
show that yes this is a live guest

00:20:53,759 --> 00:20:59,669
running and that I captured the point in

00:20:56,100 --> 00:21:01,679
time before I made those changes so for

00:20:59,669 --> 00:21:06,509
example I started my backup before I

00:21:01,679 --> 00:21:09,869
touched file a and then use my various

00:21:06,509 --> 00:21:12,479
means of reading QH convert or modprobe

00:21:09,869 --> 00:21:18,479
and use the kernel dev NBD or use my

00:21:12,479 --> 00:21:20,129
fancy loop of reading which parts of the

00:21:18,479 --> 00:21:24,659
disk are interesting read just those

00:21:20,129 --> 00:21:27,960
parts and at the end of the day when I

00:21:24,659 --> 00:21:30,419
have my image I can remove my scratch

00:21:27,960 --> 00:21:34,979
file I'm gonna ask guest fish what's on

00:21:30,419 --> 00:21:36,779
my incremental image fsck my disk was

00:21:34,979 --> 00:21:38,309
slightly dirty but it was cleanable so

00:21:36,779 --> 00:21:39,929
it was like a hard power I pulled the

00:21:38,309 --> 00:21:42,269
power out you could also use

00:21:39,929 --> 00:21:45,019
Teemu guest agent and freeze your data

00:21:42,269 --> 00:21:48,179
so that it's consistent from the get-go

00:21:45,019 --> 00:21:49,950
my data shows that my disk was empty so

00:21:48,179 --> 00:21:51,929
even though I touched file a it was

00:21:49,950 --> 00:21:55,259
after the backup began and we didn't see

00:21:51,929 --> 00:22:01,440
file a that's a good thing and then

00:21:55,259 --> 00:22:04,830
incremental backup here we go we can

00:22:01,440 --> 00:22:06,479
create something of a checkpoint notice

00:22:04,830 --> 00:22:10,379
that nothing's changed right away I

00:22:06,479 --> 00:22:13,320
touch a file we check again and data has

00:22:10,379 --> 00:22:15,809
changed in the OS disk is changing a lot

00:22:13,320 --> 00:22:19,769
faster than my very specialized second

00:22:15,809 --> 00:22:22,139
data disk but you can use Q image

00:22:19,769 --> 00:22:24,929
measure to estimate this much guest data

00:22:22,139 --> 00:22:29,519
will transfer into a q cow to size of

00:22:24,929 --> 00:22:33,029
that much continuing on as we run the

00:22:29,519 --> 00:22:35,249
backup I'm gonna do my first poll I got

00:22:33,029 --> 00:22:38,639
tired of typing that fancy loops so I

00:22:35,249 --> 00:22:41,730
pulled it into a cell function that sets

00:22:38,639 --> 00:22:43,679
everything up and then shares the code

00:22:41,730 --> 00:22:45,720
between a full read and an incremental

00:22:43,679 --> 00:22:48,660
read based on what parameter

00:22:45,720 --> 00:22:50,730
I passed but the gist of it is still I

00:22:48,660 --> 00:22:53,070
ask human image map what are the

00:22:50,730 --> 00:22:55,680
portions that are interesting and then

00:22:53,070 --> 00:22:58,710
from those interesting portions do a

00:22:55,680 --> 00:23:00,900
cumuli a copy on read to copy them into

00:22:58,710 --> 00:23:03,510
my Keuka too if you want to write your

00:23:00,900 --> 00:23:05,850
own NBD client that does all this go

00:23:03,510 --> 00:23:07,650
ahead you don't need to use QM Ohio or

00:23:05,850 --> 00:23:12,090
key move image this is just a

00:23:07,650 --> 00:23:15,540
demonstration of the NBD worked use my

00:23:12,090 --> 00:23:17,550
function my full backup the touching one

00:23:15,540 --> 00:23:19,290
file touches a lot of your disk well

00:23:17,550 --> 00:23:23,100
it's also the fact that formatting it

00:23:19,290 --> 00:23:24,900
and turning into ext4 and etc my

00:23:23,100 --> 00:23:27,660
incremental backup was a lot faster

00:23:24,900 --> 00:23:29,790
touching one file touched four sectors

00:23:27,660 --> 00:23:33,300
of my disk well I guess there's the file

00:23:29,790 --> 00:23:35,030
the directory it was in you know it

00:23:33,300 --> 00:23:39,780
makes sense but it's a lot less

00:23:35,030 --> 00:23:42,270
continuing on we'll do another poll run

00:23:39,780 --> 00:23:44,160
everything keep on touching files in

00:23:42,270 --> 00:23:46,980
between to see that it really is like an

00:23:44,160 --> 00:23:48,570
incremental verse H has some fun that

00:23:46,980 --> 00:23:52,350
you can see what checkpoints do I have

00:23:48,570 --> 00:23:55,740
checkpoint three is my current one when

00:23:52,350 --> 00:23:57,660
I do my backup without a checkpoint and

00:23:55,740 --> 00:24:00,630
capture my data there now I'm gonna use

00:23:57,660 --> 00:24:05,070
guest fish to inspect them if I inspect

00:24:00,630 --> 00:24:09,090
my most recent image error for it could

00:24:05,070 --> 00:24:11,340
not fsck my file by itself does not have

00:24:09,090 --> 00:24:12,840
a complete file system that's good

00:24:11,340 --> 00:24:14,010
because it's an incremental snapshot

00:24:12,840 --> 00:24:16,920
that shouldn't have an entire file

00:24:14,010 --> 00:24:19,950
system but when I rebase it to be on top

00:24:16,920 --> 00:24:23,880
of my earlier snapshot now the same

00:24:19,950 --> 00:24:25,410
command fsck says it was power unclean

00:24:23,880 --> 00:24:28,500
but I was able to clean it now it's

00:24:25,410 --> 00:24:31,020
clean and there's every file that I have

00:24:28,500 --> 00:24:33,960
listed on my disk run on my earlier

00:24:31,020 --> 00:24:36,510
snapshot and file a is not there because

00:24:33,960 --> 00:24:39,840
it was an earlier point in time so with

00:24:36,510 --> 00:24:42,180
that I have done incremental backups any

00:24:39,840 --> 00:24:44,880
questions early at a time we're out of

00:24:42,180 --> 00:24:47,480
time thank you

00:24:44,880 --> 00:24:53,269
[Applause]

00:24:47,480 --> 00:24:53,269

YouTube URL: https://www.youtube.com/watch?v=zQK5ANionpU


