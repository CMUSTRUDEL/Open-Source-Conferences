Title: Demand Paging Performance with 160 vCPUs - Cannon Matthews
Publication date: 2018-11-14
Playlist: KVM Forum 2018
Description: 
	As physical machine memory sizes and CPU counts rise, so does the demand for large virtual workloads as well. One particular challenge of scaling large VMs is migrating them to new hosts for maintenance. This talk will look at the performance of the demand paging mechanism userfaultfd and how well it performs under stress, in particular how it scales with large number of vCPUs. Then, the talk will examine the guest memory and CPU performance during a demand paging migration of a VM on a 4-socket host with 160 vCPUs, and additionally compare userfaultfd to an alternate approach.

---

Cannon Matthews
Software Engineer
Google

Cannon is a Software Engineer working for Google Cloud on virtualization for Google Compute Engine
Captions: 
	00:00:01,040 --> 00:00:10,200
[Music]

00:00:06,020 --> 00:00:12,450
hello so my name is Kenan Matthews I

00:00:10,200 --> 00:00:15,540
work for Google on Google compute engine

00:00:12,450 --> 00:00:18,180
doing memory virtualization and I'm

00:00:15,540 --> 00:00:19,380
going to talk about demand paging and

00:00:18,180 --> 00:00:21,300
particularly I'm going to talk about

00:00:19,380 --> 00:00:25,410
demand paging when you have a lot of

00:00:21,300 --> 00:00:31,669
CPUs and the difficulties of getting

00:00:25,410 --> 00:00:34,590
that to scale so a high-level summary we

00:00:31,669 --> 00:00:37,350
currently use a different implementation

00:00:34,590 --> 00:00:40,710
than user fault FD to do post copy live

00:00:37,350 --> 00:00:42,660
migration we would like to adopt user

00:00:40,710 --> 00:00:46,140
fault FD that's the upstream

00:00:42,660 --> 00:00:49,010
implementation and but parts of the

00:00:46,140 --> 00:00:51,059
architecture of it do not scale

00:00:49,010 --> 00:00:54,180
extremely well when you have hundreds of

00:00:51,059 --> 00:00:56,550
CPUs and so there's some more work that

00:00:54,180 --> 00:01:00,059
we need to do to get this to scale

00:00:56,550 --> 00:01:01,739
better and I started investigating what

00:01:00,059 --> 00:01:05,430
that would take and this is what I

00:01:01,739 --> 00:01:08,430
figured out I'm gonna start by talking

00:01:05,430 --> 00:01:11,520
about the background what the problem is

00:01:08,430 --> 00:01:12,780
in general what migration is if you're

00:01:11,520 --> 00:01:15,600
not super familiar with it in particular

00:01:12,780 --> 00:01:18,210
what post copy live migration is and

00:01:15,600 --> 00:01:20,100
then what user fault FD does and how

00:01:18,210 --> 00:01:21,450
that works with live migration and then

00:01:20,100 --> 00:01:27,689
look at how the scales with a lot of

00:01:21,450 --> 00:01:29,520
CPUs and see what I found out so the

00:01:27,689 --> 00:01:31,950
problem the angle that I've come at this

00:01:29,520 --> 00:01:36,450
from is that Google compute engine

00:01:31,950 --> 00:01:39,030
offers very large VMs with 160 CPUs and

00:01:36,450 --> 00:01:40,710
almost 4 terabytes of RAM and live

00:01:39,030 --> 00:01:45,270
migration is sort of a non-negotiable

00:01:40,710 --> 00:01:46,590
part of that product we have to have

00:01:45,270 --> 00:01:49,500
migration otherwise we have these

00:01:46,590 --> 00:01:52,229
maintenance events and the PM's don't

00:01:49,500 --> 00:01:57,210
like us to not have a migration so we

00:01:52,229 --> 00:01:59,399
have to figure it out and we wrote this

00:01:57,210 --> 00:02:01,350
different demand page and implementation

00:01:59,399 --> 00:02:04,740
that's diverged significantly from how

00:02:01,350 --> 00:02:06,930
upstream has done it and this is a huge

00:02:04,740 --> 00:02:09,599
maintenance burden because we have to

00:02:06,930 --> 00:02:10,619
rebase these patches and it's unlikely

00:02:09,599 --> 00:02:12,360
that we're gonna

00:02:10,619 --> 00:02:14,280
- option at this point because they're

00:02:12,360 --> 00:02:17,250
so different and we didn't upstream that

00:02:14,280 --> 00:02:19,620
first but if we were able to switch to

00:02:17,250 --> 00:02:22,019
using user felt ft we could remove a lot

00:02:19,620 --> 00:02:24,599
of these proprietary patches that are in

00:02:22,019 --> 00:02:27,420
this burger so but first we have to get

00:02:24,599 --> 00:02:31,560
the performance to where it's acceptable

00:02:27,420 --> 00:02:33,660
for our needs and so to talk about what

00:02:31,560 --> 00:02:36,450
post copy migration is so it's a live

00:02:33,660 --> 00:02:40,140
migration in general is we want to move

00:02:36,450 --> 00:02:42,019
a running VM from one host to a new

00:02:40,140 --> 00:02:44,900
destination host and we want to minimize

00:02:42,019 --> 00:02:48,480
the disruption that the guest

00:02:44,900 --> 00:02:50,370
experiences and RAM is usually the

00:02:48,480 --> 00:02:52,739
biggest my gradable state and you can't

00:02:50,370 --> 00:02:54,299
just copy it all at once when it gets

00:02:52,739 --> 00:02:57,569
really big cuz that's not life

00:02:54,299 --> 00:03:00,239
so there's two approaches to this

00:02:57,569 --> 00:03:02,730
there's pre copy where you copy memory

00:03:00,239 --> 00:03:05,310
in the background while V CPUs are

00:03:02,730 --> 00:03:06,720
running and this minimizes disruption

00:03:05,310 --> 00:03:09,379
but it's not guaranteed to ever finish

00:03:06,720 --> 00:03:12,150
if the guest is dirtying a lot of memory

00:03:09,379 --> 00:03:15,630
it can do it faster than you can keep up

00:03:12,150 --> 00:03:18,030
with it so the other approach is doing

00:03:15,630 --> 00:03:19,829
post copy migration and you might do

00:03:18,030 --> 00:03:22,049
both of these of course

00:03:19,829 --> 00:03:25,290
post copy you would then copy memory on

00:03:22,049 --> 00:03:27,630
demand so you wait until ad CPU tries to

00:03:25,290 --> 00:03:30,239
access memory you moot you start running

00:03:27,630 --> 00:03:32,489
the VM on the next host and then once a

00:03:30,239 --> 00:03:35,400
V CPU touches memory you pause that B

00:03:32,489 --> 00:03:38,639
CPU and copy the memory in and this will

00:03:35,400 --> 00:03:40,349
always eventually converge but you have

00:03:38,639 --> 00:03:45,840
to interrupt the V CPUs a lot so this is

00:03:40,349 --> 00:03:47,970
has worse Purvi cpu performance so in

00:03:45,840 --> 00:03:49,739
practice you might use both of these to

00:03:47,970 --> 00:03:54,239
do pre copy until some point so we

00:03:49,739 --> 00:03:57,959
should post copy so then where does user

00:03:54,239 --> 00:04:00,239
fault FD come in so user fault FD is a

00:03:57,959 --> 00:04:02,910
linux memory management interface for

00:04:00,239 --> 00:04:05,790
handling page faults in user space

00:04:02,910 --> 00:04:07,290
program and so instead of having the

00:04:05,790 --> 00:04:10,010
kernel handle all the page faults you

00:04:07,290 --> 00:04:12,209
can tell create this file descriptor and

00:04:10,010 --> 00:04:14,430
register a section of memory with it and

00:04:12,209 --> 00:04:16,500
it will notify you on this file

00:04:14,430 --> 00:04:18,510
descriptor anytime there are certain

00:04:16,500 --> 00:04:22,590
types of paging events in this case page

00:04:18,510 --> 00:04:24,450
faults in that region so you you pull

00:04:22,590 --> 00:04:25,980
and read on this file descriptor

00:04:24,450 --> 00:04:28,230
tells you that you need to copy some

00:04:25,980 --> 00:04:33,480
memory in and you copy it in with an i

00:04:28,230 --> 00:04:39,000
octal that's this uff di o copy from a

00:04:33,480 --> 00:04:42,510
separate thread so for post copy this

00:04:39,000 --> 00:04:45,540
works by having the guest memory marked

00:04:42,510 --> 00:04:49,410
as FF against memory is not present

00:04:45,540 --> 00:04:51,780
there's no page tables for it and when a

00:04:49,410 --> 00:04:53,460
piece of you accesses sub memory causes

00:04:51,780 --> 00:04:55,980
a page fault and your handle of thread

00:04:53,460 --> 00:04:59,520
gets notified it requests the missing

00:04:55,980 --> 00:05:01,340
Ram from the first host and then it's

00:04:59,520 --> 00:05:05,040
copied over the network copied into

00:05:01,340 --> 00:05:08,220
memory using the octal and then when you

00:05:05,040 --> 00:05:10,680
copy it in the kernel user fault FD code

00:05:08,220 --> 00:05:12,960
will find the thread that was waiting on

00:05:10,680 --> 00:05:19,050
that section remember I'm waking up new

00:05:12,960 --> 00:05:20,970
vzv of continues this is a little so I

00:05:19,050 --> 00:05:26,040
was curious how does this scale if you

00:05:20,970 --> 00:05:29,040
start adding lots and lots of CPUs and

00:05:26,040 --> 00:05:30,450
particularly doesn't use Queenie and so

00:05:29,040 --> 00:05:32,850
I didn't I don't really know very much

00:05:30,450 --> 00:05:36,480
about premium and I don't did not want

00:05:32,850 --> 00:05:38,040
to really get into debugging like a user

00:05:36,480 --> 00:05:39,960
space so I wanted to really look at the

00:05:38,040 --> 00:05:42,120
kernel interface specifically so I tried

00:05:39,960 --> 00:05:46,080
to use a little micro benchmark around

00:05:42,120 --> 00:05:48,980
user fault FDA itself and so I started

00:05:46,080 --> 00:05:51,990
160 V CPUs with their own little

00:05:48,980 --> 00:05:55,260
disjoint memory section that all of the

00:05:51,990 --> 00:05:58,200
memory was not present and the guest

00:05:55,260 --> 00:06:00,300
code was just like a loop that touches

00:05:58,200 --> 00:06:02,580
every page of memory and then exit and

00:06:00,300 --> 00:06:05,010
so I wanted to see how long I would take

00:06:02,580 --> 00:06:07,410
each VC beer to finish touching a block

00:06:05,010 --> 00:06:11,220
of memory if all of that memory was on

00:06:07,410 --> 00:06:12,780
demand and this benchmark sort of

00:06:11,220 --> 00:06:15,540
assumes that you have infinite network

00:06:12,780 --> 00:06:19,320
bandwidth and so it causes a worst-case

00:06:15,540 --> 00:06:22,980
stress on the handling code if that

00:06:19,320 --> 00:06:26,970
makes sense because there's no DQ PS

00:06:22,980 --> 00:06:28,650
you're getting and handling requests is

00:06:26,970 --> 00:06:30,240
much faster than you might get in real

00:06:28,650 --> 00:06:33,290
life if everything is being slowed down

00:06:30,240 --> 00:06:33,290
by the network

00:06:35,060 --> 00:06:43,530
so I did this with four gigabytes per

00:06:38,970 --> 00:06:45,389
CPU and I had just one thread in

00:06:43,530 --> 00:06:49,350
userspace handling faults to start with

00:06:45,389 --> 00:06:52,889
at a 4 kilobyte page size and so with

00:06:49,350 --> 00:06:56,550
one CPU you could get about 200

00:06:52,889 --> 00:07:00,330
megabytes a second 220 just not very not

00:06:56,550 --> 00:07:02,120
very fast but that's not this is you

00:07:00,330 --> 00:07:04,199
know kind of a synthetic benchmark but

00:07:02,120 --> 00:07:06,210
what was really disappointing was that

00:07:04,199 --> 00:07:08,100
it if you started adding more CPUs it

00:07:06,210 --> 00:07:11,449
scaled up until you got to 4 and then it

00:07:08,100 --> 00:07:11,449
just really starts to trickle off and

00:07:11,720 --> 00:07:16,100
network bandwidth could easily be higher

00:07:13,800 --> 00:07:18,180
than than this without 2 gigabits I mean

00:07:16,100 --> 00:07:19,560
we have 100 gigabit NICs

00:07:18,180 --> 00:07:22,860
you can't get a hundred gigabytes of

00:07:19,560 --> 00:07:26,699
data transfer a bit faster than two and

00:07:22,860 --> 00:07:29,340
today there's room for improvement so to

00:07:26,699 --> 00:07:33,090
just sort of expectations that I wanted

00:07:29,340 --> 00:07:36,060
to get out of this post copy interface

00:07:33,090 --> 00:07:38,370
we I looked at what Google is able to do

00:07:36,060 --> 00:07:42,990
currently with our implementation that

00:07:38,370 --> 00:07:45,210
we haven't up streamed and it does

00:07:42,990 --> 00:07:47,370
better but so this this does it by

00:07:45,210 --> 00:07:50,159
exiting on page faults which I can talk

00:07:47,370 --> 00:07:52,139
about later if people are curious but it

00:07:50,159 --> 00:07:54,300
needs to be a little bit about two to

00:07:52,139 --> 00:07:56,699
four times as performant before we could

00:07:54,300 --> 00:07:58,139
jump to it and so we're interested in

00:07:56,699 --> 00:08:03,930
trying to get it there but it's not the

00:07:58,139 --> 00:08:07,139
only scaling problem so another big

00:08:03,930 --> 00:08:11,060
scaling problem is that MMU lock so in

00:08:07,139 --> 00:08:14,460
the in the k vm x86 in mu there's this

00:08:11,060 --> 00:08:15,690
monolithic lock that's required for many

00:08:14,460 --> 00:08:19,620
things touching the EPT

00:08:15,690 --> 00:08:22,320
and so during post copy you have to do a

00:08:19,620 --> 00:08:25,440
lot of stuff to create these extended

00:08:22,320 --> 00:08:27,330
page table mappings and this is actually

00:08:25,440 --> 00:08:30,270
still in even in our implementation is

00:08:27,330 --> 00:08:34,050
like 75% of the execution time when you

00:08:30,270 --> 00:08:36,719
have 160 CPUs is spent waiting on a new

00:08:34,050 --> 00:08:39,390
lock and this is but this is a separate

00:08:36,719 --> 00:08:41,390
opportunity I wanted to look at

00:08:39,390 --> 00:08:44,430
specifically with user felt FD though

00:08:41,390 --> 00:08:46,320
why doesn't it scale at all if you add

00:08:44,430 --> 00:08:51,510
sixty CPUs you only get a little bit a

00:08:46,320 --> 00:08:55,640
30% improvement and so I thought maybe

00:08:51,510 --> 00:08:58,050
you could add more user space workers to

00:08:55,640 --> 00:09:01,260
make this faster because one thread can

00:08:58,050 --> 00:09:05,550
only make the system calls so fast could

00:09:01,260 --> 00:09:06,830
you simply add more threads and the

00:09:05,550 --> 00:09:09,720
answer is not really

00:09:06,830 --> 00:09:11,280
if you start adding more threads to make

00:09:09,720 --> 00:09:13,980
these system calls every system call

00:09:11,280 --> 00:09:17,070
just starts taking longer and that was

00:09:13,980 --> 00:09:20,910
that was I found that curious and also

00:09:17,070 --> 00:09:22,590
the CPU usage starts going very high but

00:09:20,910 --> 00:09:23,970
you don't really get any benefit from

00:09:22,590 --> 00:09:28,290
that so I was really curious where this

00:09:23,970 --> 00:09:31,890
time was going and so I used the perf

00:09:28,290 --> 00:09:34,620
utility to kind of profile this and the

00:09:31,890 --> 00:09:38,220
time spent in the I octal actually just

00:09:34,620 --> 00:09:40,680
kind of grows with that shape as you add

00:09:38,220 --> 00:09:44,790
more than like 64 threads it just gets

00:09:40,680 --> 00:09:47,370
very very slow and what perf showed us

00:09:44,790 --> 00:09:50,400
was that a lot of this time is being

00:09:47,370 --> 00:09:53,220
spent in some spent locks and in

00:09:50,400 --> 00:09:58,440
particular user fault FD has a spin lock

00:09:53,220 --> 00:10:00,540
for waiting threads so when you handle a

00:09:58,440 --> 00:10:02,910
user fault so the first time the V CPU

00:10:00,540 --> 00:10:06,180
touches memory that's not present it has

00:10:02,910 --> 00:10:07,710
to add its thread object to wake you and

00:10:06,180 --> 00:10:09,960
that requires getting to spend lock I

00:10:07,710 --> 00:10:11,880
didn't win it when you read it from user

00:10:09,960 --> 00:10:13,050
space to read the notification as the

00:10:11,880 --> 00:10:14,700
read from the queue and get to spend

00:10:13,050 --> 00:10:17,610
lock and then when you try to copy the

00:10:14,700 --> 00:10:19,680
memory back into the process it has to

00:10:17,610 --> 00:10:22,050
wake up a thread and it gets a spin lock

00:10:19,680 --> 00:10:24,510
again to find the thread in the wake you

00:10:22,050 --> 00:10:27,960
and so when you start adding lots of

00:10:24,510 --> 00:10:31,530
threads faulting and the V CPU threads

00:10:27,960 --> 00:10:33,240
are are fixed like we have this we offer

00:10:31,530 --> 00:10:35,790
this vm that has a certain number of a

00:10:33,240 --> 00:10:37,310
CPUs and we can't really change that so

00:10:35,790 --> 00:10:41,880
that's fixed and then if we try to add

00:10:37,310 --> 00:10:44,190
more userspace threads to copy memory

00:10:41,880 --> 00:10:49,140
and the lock contention gets gets very

00:10:44,190 --> 00:10:51,150
very bad and so this was this diagram

00:10:49,140 --> 00:10:53,850
from earlier but we kind of see that

00:10:51,150 --> 00:10:55,230
like a lot of this path has to go

00:10:53,850 --> 00:11:04,500
through this one spend lot

00:10:55,230 --> 00:11:06,540
and so to summarize anyway so the number

00:11:04,500 --> 00:11:08,280
of ECB is is fixed and if you only have

00:11:06,540 --> 00:11:09,840
one thread trying to copy memory and

00:11:08,280 --> 00:11:11,760
it's not sufficient for the memory

00:11:09,840 --> 00:11:13,710
bandwidth you need and if you add more

00:11:11,760 --> 00:11:15,630
threads this causes a lot contention and

00:11:13,710 --> 00:11:18,390
the gates potential for this throughput

00:11:15,630 --> 00:11:23,550
improvement and it drives CPU usage

00:11:18,390 --> 00:11:27,600
really high the this is the percentage

00:11:23,550 --> 00:11:29,820
of of the total execution time that was

00:11:27,600 --> 00:11:31,260
just spent in the spin lock function the

00:11:29,820 --> 00:11:33,680
more threads you add it just gets close

00:11:31,260 --> 00:11:36,660
to like 90% of the time you're spending

00:11:33,680 --> 00:11:43,230
waiting on a spin lock and you're using

00:11:36,660 --> 00:11:45,960
160 times as much CPU so so some next

00:11:43,230 --> 00:11:51,860
steps you know I haven't I haven't

00:11:45,960 --> 00:11:56,100
written a solution to this yet and so

00:11:51,860 --> 00:11:57,330
there's many potential solutions might

00:11:56,100 --> 00:11:59,490
be potentially straightforward to shard

00:11:57,330 --> 00:12:02,210
the lock chart by faulting address or

00:11:59,490 --> 00:12:05,820
something else I'm not going to write a

00:12:02,210 --> 00:12:10,070
lock free implementation but lock for

00:12:05,820 --> 00:12:13,950
implementations are hard and complex and

00:12:10,070 --> 00:12:17,100
just make these types and scaling

00:12:13,950 --> 00:12:19,050
problems more subtle sometimes some

00:12:17,100 --> 00:12:20,250
other ideas that I'd be interested in if

00:12:19,050 --> 00:12:21,870
people have other ideas but about

00:12:20,250 --> 00:12:24,600
creating multiple user fault if these

00:12:21,870 --> 00:12:26,880
and trying to use you know many of them

00:12:24,600 --> 00:12:29,460
on different regions and avoiding locks

00:12:26,880 --> 00:12:32,010
like that but all of these things will

00:12:29,460 --> 00:12:34,220
have other trade-offs and I haven't done

00:12:32,010 --> 00:12:37,170
enough enough testing to have any like

00:12:34,220 --> 00:12:40,230
clear this is the solution results

00:12:37,170 --> 00:12:43,920
unfortunately but at the same time this

00:12:40,230 --> 00:12:47,310
is not the be-all end-all like this is

00:12:43,920 --> 00:12:50,160
the scaling problem this is only one

00:12:47,310 --> 00:12:55,530
spin lock and it's only one bottleneck

00:12:50,160 --> 00:12:57,480
and bottle acts are very nested and they

00:12:55,530 --> 00:12:59,370
like to hide behind other bottlenecks

00:12:57,480 --> 00:13:02,400
and you remove one and you find three

00:12:59,370 --> 00:13:06,590
more and as I mentioned earlier the the

00:13:02,400 --> 00:13:06,590
MMU spin lock has been the

00:13:06,650 --> 00:13:11,240
great enemy of our implementation and

00:13:09,560 --> 00:13:12,800
that we don't have this week you spin

00:13:11,240 --> 00:13:15,230
lock but we start hitting this mm you

00:13:12,800 --> 00:13:16,940
spin lock and it scales basically just

00:13:15,230 --> 00:13:21,970
as poorly when you get to hundreds of

00:13:16,940 --> 00:13:24,260
CPUs one of my colleagues somewhere is

00:13:21,970 --> 00:13:26,780
working on the prototype to try to

00:13:24,260 --> 00:13:30,380
eliminate this spin lock which was on

00:13:26,780 --> 00:13:32,810
one of the graphs earlier but that seems

00:13:30,380 --> 00:13:34,400
promising as well but it's almost

00:13:32,810 --> 00:13:35,450
certain that once we find that you know

00:13:34,400 --> 00:13:40,580
there's there's gonna be something else

00:13:35,450 --> 00:13:47,390
and adding this many CPUs is just a hard

00:13:40,580 --> 00:13:50,360
problem and this is making vm's very

00:13:47,390 --> 00:13:53,770
very large just brings other problems

00:13:50,360 --> 00:13:56,690
that are not even necessarily related to

00:13:53,770 --> 00:14:01,880
parallelism by just problems related to

00:13:56,690 --> 00:14:07,730
having a lot of memory for instance with

00:14:01,880 --> 00:14:12,680
user felt ft the page fault path for the

00:14:07,730 --> 00:14:16,580
like guess virtual memory areas are tied

00:14:12,680 --> 00:14:18,560
into the demand paging and so you can't

00:14:16,580 --> 00:14:20,660
for instance it mapped the region and an

00:14:18,560 --> 00:14:22,730
EM lock it and populate it have all the

00:14:20,660 --> 00:14:27,170
page table set up and then do demand

00:14:22,730 --> 00:14:29,510
paging I'd say the EPT level you you

00:14:27,170 --> 00:14:32,090
still have the the Linux in them page

00:14:29,510 --> 00:14:33,290
faulty me going on and so this adds some

00:14:32,090 --> 00:14:35,450
latency

00:14:33,290 --> 00:14:38,840
I haven't measured exactly how much

00:14:35,450 --> 00:14:42,830
latency but it puts it on the V CPU

00:14:38,840 --> 00:14:44,030
critical path and at the point where

00:14:42,830 --> 00:14:47,330
you're scaling two terabytes of memory

00:14:44,030 --> 00:14:49,550
and hundreds of CPUs getting as much

00:14:47,330 --> 00:14:51,800
stuff out of the the critical path of V

00:14:49,550 --> 00:14:53,630
CPU execution is becomes really

00:14:51,800 --> 00:14:56,600
important because you don't have much

00:14:53,630 --> 00:14:59,080
margins and you're really starting to

00:14:56,600 --> 00:15:01,430
stretch limits of everything

00:14:59,080 --> 00:15:02,990
another problem although we were talking

00:15:01,430 --> 00:15:07,670
about this in the hallway earlier today

00:15:02,990 --> 00:15:11,440
is the backing memory page table sizes

00:15:07,670 --> 00:15:15,560
and since user felt FD operates at the

00:15:11,440 --> 00:15:18,020
like file system or system page table

00:15:15,560 --> 00:15:20,989
levels you can't have like one gigabyte

00:15:18,020 --> 00:15:23,639
EPT mappings or one gigabyte

00:15:20,989 --> 00:15:26,039
mappings on the file in the memory file

00:15:23,639 --> 00:15:28,679
and then do demand paging at a 4

00:15:26,039 --> 00:15:31,939
kilobyte level unless you're using like

00:15:28,679 --> 00:15:34,079
the right type of memory filesystem and

00:15:31,939 --> 00:15:39,149
there's some ideas on how this could be

00:15:34,079 --> 00:15:40,470
done but it's currently not not working

00:15:39,149 --> 00:15:43,289
perfectly

00:15:40,470 --> 00:15:44,789
that's so where I plan to go with this I

00:15:43,289 --> 00:15:47,039
plan to kind of continue investigating

00:15:44,789 --> 00:15:49,319
this performance and trying to get it

00:15:47,039 --> 00:15:50,999
down to where it's it's worked really

00:15:49,319 --> 00:15:54,629
well for these big sizes and it's

00:15:50,999 --> 00:15:56,759
something that like Google can use and

00:15:54,629 --> 00:16:01,319
that we can develop these things and do

00:15:56,759 --> 00:16:02,789
them upstream first and not have to end

00:16:01,319 --> 00:16:04,229
up in this problem where we have some

00:16:02,789 --> 00:16:06,149
proprietary thing that we're trying to

00:16:04,229 --> 00:16:08,009
rebase and it's maintenance that we're

00:16:06,149 --> 00:16:10,199
not being really good community

00:16:08,009 --> 00:16:11,729
participants and but I also want to

00:16:10,199 --> 00:16:14,189
continue to push limits of VM

00:16:11,729 --> 00:16:15,839
scalability and see how big we can make

00:16:14,189 --> 00:16:19,769
the ends and how performant we can make

00:16:15,839 --> 00:16:24,809
those big VMs and all the challenges

00:16:19,769 --> 00:16:26,459
that will come with them thanks that's

00:16:24,809 --> 00:16:28,819
all I haven't if anybody has any

00:16:26,459 --> 00:16:28,819
questions

00:16:36,310 --> 00:16:44,810
can you hear okay thanks for the great

00:16:40,069 --> 00:16:48,290
presentation so there's one big shoe

00:16:44,810 --> 00:16:51,500
which was not mentioned which was wait

00:16:48,290 --> 00:16:55,519
wha Norway called Beaver because the

00:16:51,500 --> 00:16:58,759
current limitation in cream is doing you

00:16:55,519 --> 00:17:02,120
know things only with one thread and so

00:16:58,759 --> 00:17:04,189
the moment you put more uff idea yo and

00:17:02,120 --> 00:17:06,110
you have to do that from multiple CPUs

00:17:04,189 --> 00:17:08,390
to make it scale because it looks like

00:17:06,110 --> 00:17:12,110
your proprietary implementation actually

00:17:08,390 --> 00:17:16,370
did exactly that scaling by copying

00:17:12,110 --> 00:17:18,650
memory from multiple CPUs so when you

00:17:16,370 --> 00:17:21,319
actually add more threads doing uff Edo

00:17:18,650 --> 00:17:23,329
it matters a lot are you listening for

00:17:21,319 --> 00:17:25,159
the new user fault of the events coming

00:17:23,329 --> 00:17:27,260
and currently in the kernel is the

00:17:25,159 --> 00:17:29,960
upstream kernel even if you do blocking

00:17:27,260 --> 00:17:32,450
read and even if you do Paul which are

00:17:29,960 --> 00:17:34,220
the two ways to listen for events you're

00:17:32,450 --> 00:17:36,409
going to get the we call event so every

00:17:34,220 --> 00:17:38,120
user forthcoming in wakes up all the

00:17:36,409 --> 00:17:39,799
threads waiting which are going to hit

00:17:38,120 --> 00:17:41,600
all on the same spin lock so I think

00:17:39,799 --> 00:17:44,150
there will be just a nice improvement

00:17:41,600 --> 00:17:46,010
but just using wake one and patch to do

00:17:44,150 --> 00:17:48,980
that it's trivial for the blocking reads

00:17:46,010 --> 00:17:53,150
not trivial for the ball unfortunately

00:17:48,980 --> 00:17:55,159
and so but if use blocky reads we can

00:17:53,150 --> 00:17:58,010
fix that and sound differently we should

00:17:55,159 --> 00:18:02,270
work on optimizing C's so I think it's

00:17:58,010 --> 00:18:04,340
it's great it's great - thanks mom -

00:18:02,270 --> 00:18:06,350
that - I that was one of the things I

00:18:04,340 --> 00:18:08,840
did try but I didn't really mention

00:18:06,350 --> 00:18:11,720
because I tried to pull from a bunch of

00:18:08,840 --> 00:18:13,520
threads but it's exactly what you said

00:18:11,720 --> 00:18:15,470
and so that all the threads get woken up

00:18:13,520 --> 00:18:17,840
on every event and then they just raced

00:18:15,470 --> 00:18:20,960
to the read and it doesn't really it

00:18:17,840 --> 00:18:23,900
doesn't scale to add a little bit about

00:18:20,960 --> 00:18:28,130
the what our implementation does is it

00:18:23,900 --> 00:18:29,330
every time ad CPU so we mark the pages

00:18:28,130 --> 00:18:33,260
demanded in the EPT

00:18:29,330 --> 00:18:35,690
in a KVM data structure bitmap and we

00:18:33,260 --> 00:18:38,570
trap on accesses to the EPT and then

00:18:35,690 --> 00:18:41,480
take a VM exit and exit the V CPU thread

00:18:38,570 --> 00:18:43,970
exits from the run I octal and then in

00:18:41,480 --> 00:18:46,000
the user space handler from that loop

00:18:43,970 --> 00:18:48,250
it just says oh I exited because memory

00:18:46,000 --> 00:18:50,680
is not present and the vcp threat itself

00:18:48,250 --> 00:18:53,320
goes and fetches the memory and installs

00:18:50,680 --> 00:18:59,320
it so you sort of automatically get

00:18:53,320 --> 00:19:00,460
parallelism by numbers of CPUs but yeah

00:18:59,320 --> 00:19:01,240
that I didn't I didn't try doing

00:19:00,460 --> 00:19:03,280
blocking reads

00:19:01,240 --> 00:19:08,880
but but you're saying that blocking

00:19:03,280 --> 00:19:08,880
reads today don't don't support okay

00:19:16,020 --> 00:19:19,050
thank you

00:19:21,240 --> 00:19:28,920
does anybody else have other question

00:19:25,920 --> 00:19:28,920
comments

00:19:32,770 --> 00:19:38,950
thank you my question is about is it a

00:19:35,830 --> 00:19:42,520
post economy you know what's the low

00:19:38,950 --> 00:19:45,580
back situation that is the poster copies

00:19:42,520 --> 00:19:49,450
the bottom can you repeat them well I

00:19:45,580 --> 00:19:55,410
mean it's a roll back the roll back when

00:19:49,450 --> 00:19:55,410
we do post a copy line migration how to

00:19:57,059 --> 00:20:09,370
situation like so if you have to cancel

00:19:59,920 --> 00:20:11,050
the migration is that what you do in the

00:20:09,370 --> 00:20:17,490
in the social sciences

00:20:11,050 --> 00:20:20,559
you can't know if you post a copy

00:20:17,490 --> 00:20:23,350
situation what's your like it's a pretty

00:20:20,559 --> 00:20:28,929
copy yeah you can cancel their own

00:20:23,350 --> 00:20:30,190
migration I'm not so I think if I like

00:20:28,929 --> 00:20:33,309
am i understanding your Chris you're

00:20:30,190 --> 00:20:35,320
asking how can you roll back or cancel a

00:20:33,309 --> 00:20:39,660
migration once you've started doing post

00:20:35,320 --> 00:20:41,559
copy yeah I don't I don't know that I

00:20:39,660 --> 00:20:42,910
don't I don't actually have an answer

00:20:41,559 --> 00:20:45,370
for that I I don't know that it's

00:20:42,910 --> 00:20:47,860
actually possible with I just I don't

00:20:45,370 --> 00:20:50,170
know how Queeny works so it might be to

00:20:47,860 --> 00:20:51,760
do it I don't know that we do this but I

00:20:50,170 --> 00:20:53,230
think it might be once you've hit a

00:20:51,760 --> 00:20:56,530
certain point you're kind of committed

00:20:53,230 --> 00:20:57,429
to to finishing it and if if you get to

00:20:56,530 --> 00:21:01,090
the point where you're writing on the

00:20:57,429 --> 00:21:02,679
target and like the source crashes the

00:21:01,090 --> 00:21:05,140
Machine blows up or something

00:21:02,679 --> 00:21:06,940
and vcp you tries to access a memory

00:21:05,140 --> 00:21:09,850
you're gonna copy it from the source and

00:21:06,940 --> 00:21:12,130
the source is gone you know like if this

00:21:09,850 --> 00:21:13,840
happens in GCE your VM will just die

00:21:12,130 --> 00:21:15,130
because we don't want to corrupt guest

00:21:13,840 --> 00:21:17,740
memory and there's nothing else you can

00:21:15,130 --> 00:21:20,050
do if the memory is gone

00:21:17,740 --> 00:21:21,309
so I think once you hit the the blackout

00:21:20,050 --> 00:21:22,809
point once you hit where you're running

00:21:21,309 --> 00:21:24,940
on the target you've kind of hit a point

00:21:22,809 --> 00:21:28,870
of no return like you then have to

00:21:24,940 --> 00:21:31,260
complete the migration okay does that

00:21:28,870 --> 00:21:31,260
answer your question

00:21:32,760 --> 00:21:37,550
but nobody has anything else thank you

00:21:35,470 --> 00:21:42,120
for listening

00:21:37,550 --> 00:21:47,900
[Applause]

00:21:42,120 --> 00:21:47,900

YouTube URL: https://www.youtube.com/watch?v=E_2LjeX3BHU


