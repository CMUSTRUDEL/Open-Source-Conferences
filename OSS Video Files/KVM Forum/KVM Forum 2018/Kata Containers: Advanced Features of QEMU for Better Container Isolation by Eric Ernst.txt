Title: Kata Containers: Advanced Features of QEMU for Better Container Isolation by Eric Ernst
Publication date: 2018-11-14
Playlist: KVM Forum 2018
Description: 
	Full talk name: Kata Containers: Leveraging Advanced Features of QEMU to Provide Better Container Isolation

Kata Containers is an open source project that brings the security of hardware virtualization to containers through lightweight VMs. In its effort to look and feel like a container, Kata leverages many of the features in KVM/QEMU which are typically not needed for a cloud virtual machine.

How many developers use VFIO? How many use VFIO-hotplug? And DAX and nvdimm and CPU hotplug?

This session details how Kata Containers use features of KVM/QEMU and some of the problem areas we encountered along the way. Finally, we discuss areas in the hypervisor weâ€™re looking to focus on going forward.

---

Eric Ernst
Senior Software Engineer
Intel - Open Source Technology Center

Eric is a senior software engineer at Intelâ€™s Open Source Technology Center, based out of Portland, Oregon. Eric has spent the last several years working on embedded firmware and the Linux kernel. Eric has been a developer and technical lead for the Intel Clear Containers project for the last two years and is very excited to be a part of the Kata
Captions: 
	00:00:01,040 --> 00:00:07,560
[Music]

00:00:05,359 --> 00:00:11,099
alright hi everybody hope you had a good

00:00:07,560 --> 00:00:13,139
lunch welcome back my name is Eric Ernst

00:00:11,099 --> 00:00:14,549
I work at Intel at the open source

00:00:13,139 --> 00:00:18,930
Technology Center and I want to talk

00:00:14,549 --> 00:00:21,539
about kata today kata containers and the

00:00:18,930 --> 00:00:22,769
quick background kata containers we're

00:00:21,539 --> 00:00:26,189
looking to provide an extra layer of

00:00:22,769 --> 00:00:28,650
isolation to provide better security in

00:00:26,189 --> 00:00:31,080
the container ecosystem and through that

00:00:28,650 --> 00:00:34,500
I you know we're working on the kata

00:00:31,080 --> 00:00:36,450
containers project I want to give a

00:00:34,500 --> 00:00:38,579
quick background on what kata containers

00:00:36,450 --> 00:00:40,410
is and why we felt like this is

00:00:38,579 --> 00:00:43,469
important and why many people felt this

00:00:40,410 --> 00:00:46,020
is important and then really I want to

00:00:43,469 --> 00:00:49,440
talk about how we leverage cane qumu and

00:00:46,020 --> 00:00:53,219
KVM and kind of maybe get some feedback

00:00:49,440 --> 00:00:56,239
and kind of highlight some areas where

00:00:53,219 --> 00:00:59,070
we think there's future work so really

00:00:56,239 --> 00:01:01,289
working on kata is especially recently

00:00:59,070 --> 00:01:05,220
just turned into what are all the

00:01:01,289 --> 00:01:10,740
options in the man pages which itself is

00:01:05,220 --> 00:01:15,689
is quite a long task but what is a

00:01:10,740 --> 00:01:17,460
container containers are it's you have

00:01:15,689 --> 00:01:21,140
processes running and you're leveraging

00:01:17,460 --> 00:01:24,090
the Linux kernel to provide isolation

00:01:21,140 --> 00:01:27,930
the tools that are gained leveraged like

00:01:24,090 --> 00:01:30,119
namespaces see groups using capabilities

00:01:27,930 --> 00:01:31,619
set comp filter none of these have

00:01:30,119 --> 00:01:33,060
anything to do with containers really

00:01:31,619 --> 00:01:35,310
these are existing solutions for

00:01:33,060 --> 00:01:38,100
processes and for hierarchies of

00:01:35,310 --> 00:01:39,720
processes so namespaces just make it so

00:01:38,100 --> 00:01:43,229
that way if you have a couple of

00:01:39,720 --> 00:01:45,210
processes that you can make it seem like

00:01:43,229 --> 00:01:48,000
they own the hosts themselves they can't

00:01:45,210 --> 00:01:49,439
see anything from the others within that

00:01:48,000 --> 00:01:51,829
namespace so if you look at like a

00:01:49,439 --> 00:01:54,479
network namespace amount namespace

00:01:51,829 --> 00:01:56,130
process a namespace it makes it so that

00:01:54,479 --> 00:01:57,630
way you you can't see what you may be

00:01:56,130 --> 00:02:01,079
your neighbor has or what the host has

00:01:57,630 --> 00:02:02,820
available to it which is nice but you

00:02:01,079 --> 00:02:05,369
can still see signs of the other person

00:02:02,820 --> 00:02:09,330
being there if the other process is

00:02:05,369 --> 00:02:10,860
using all the CPU you can kind of feel

00:02:09,330 --> 00:02:13,080
it from a denial of service

00:02:10,860 --> 00:02:13,850
same thing for memory so that's where

00:02:13,080 --> 00:02:15,920
control groups

00:02:13,850 --> 00:02:18,680
are used to be able to kind of throttle

00:02:15,920 --> 00:02:21,560
and manage the actual resource

00:02:18,680 --> 00:02:24,470
utilization per process or per container

00:02:21,560 --> 00:02:25,970
in this case so that's great and then on

00:02:24,470 --> 00:02:27,020
top of that you also have different

00:02:25,970 --> 00:02:28,610
filters

00:02:27,020 --> 00:02:30,110
I call them filters it's probably not

00:02:28,610 --> 00:02:31,490
the right word but that's where I think

00:02:30,110 --> 00:02:32,660
of like SATCOM or you're saying hey

00:02:31,490 --> 00:02:34,370
these are sis calls that you shouldn't

00:02:32,660 --> 00:02:38,090
be using these are sis calls that you're

00:02:34,370 --> 00:02:40,400
allowed to use capabilities you can't

00:02:38,090 --> 00:02:42,260
change the routing tables sorry or you

00:02:40,400 --> 00:02:44,090
can't reboot the machine these are clear

00:02:42,260 --> 00:02:46,220
seeing things that you should not allow

00:02:44,090 --> 00:02:50,240
a container to be able to do so out of

00:02:46,220 --> 00:02:53,120
the box you get these for free well you

00:02:50,240 --> 00:02:55,700
get them they're not all free but the

00:02:53,120 --> 00:02:57,530
main thing that we see is that these are

00:02:55,700 --> 00:02:59,240
great but it's a single layer of

00:02:57,530 --> 00:03:00,740
isolation there's only a single

00:02:59,240 --> 00:03:02,960
interface and a single interface is the

00:03:00,740 --> 00:03:05,960
host kernel they all share the same host

00:03:02,960 --> 00:03:08,480
kernel and while it's a great layer it

00:03:05,960 --> 00:03:10,580
we think of it as a single layer that's

00:03:08,480 --> 00:03:11,870
why usually if you were to go to a cloud

00:03:10,580 --> 00:03:13,910
provider and say hey I want to run

00:03:11,870 --> 00:03:15,170
containers they're not gonna say sure

00:03:13,910 --> 00:03:17,120
just we trust you

00:03:15,170 --> 00:03:20,150
you have bare-metal access just go run

00:03:17,120 --> 00:03:21,440
your container work load they would give

00:03:20,150 --> 00:03:26,030
you a virtual machine and then you can

00:03:21,440 --> 00:03:27,200
go play as you would like this is

00:03:26,030 --> 00:03:32,240
essentially what we're doing Khattak

00:03:27,200 --> 00:03:34,250
containers so via crayola what we're

00:03:32,240 --> 00:03:36,770
doing is every container workload we're

00:03:34,250 --> 00:03:38,780
putting inside of a virtual machine its

00:03:36,770 --> 00:03:40,520
own virtual machine each of them can

00:03:38,780 --> 00:03:42,440
have its own unique kernel even you

00:03:40,520 --> 00:03:45,380
could have a 411 you could have a 419

00:03:42,440 --> 00:03:53,510
you could have a 310 for each of those

00:03:45,380 --> 00:03:55,790
individually yeah so question is can you

00:03:53,510 --> 00:03:58,130
put several containers in a in a single

00:03:55,790 --> 00:04:00,830
virtual machine and yes if you are using

00:03:58,130 --> 00:04:05,570
kubernetes and it's a call to pod but

00:04:00,830 --> 00:04:10,030
you can't just say I want to run these

00:04:05,570 --> 00:04:12,800
20 random containers in here there is no

00:04:10,030 --> 00:04:16,160
the answer is no but what we're trying

00:04:12,800 --> 00:04:17,570
to do is not change and this will kind

00:04:16,160 --> 00:04:20,540
of get maybe into the next one let's

00:04:17,570 --> 00:04:22,400
just change the slide what we are is an

00:04:20,540 --> 00:04:24,440
OC a compliant runtime so usually you

00:04:22,400 --> 00:04:26,389
just say docker run laughs and that's it

00:04:24,440 --> 00:04:29,330
and now you have on a boon to look

00:04:26,389 --> 00:04:32,110
Dean shell that you're running in that's

00:04:29,330 --> 00:04:35,389
very exciting and what we want to do is

00:04:32,110 --> 00:04:37,490
we want to provide extra security but we

00:04:35,389 --> 00:04:39,439
don't want to change anything for you or

00:04:37,490 --> 00:04:42,469
for anybody in the kind of micro

00:04:39,439 --> 00:04:44,060
services cloud native design pattern

00:04:42,469 --> 00:04:45,590
space like we don't want them to have to

00:04:44,060 --> 00:04:46,909
change what they're doing this is meant

00:04:45,590 --> 00:04:49,009
to be hidden from you

00:04:46,909 --> 00:04:50,539
you should never know unless you notice

00:04:49,009 --> 00:04:53,029
start looking at processes footprints

00:04:50,539 --> 00:04:56,330
things like this that it's running so if

00:04:53,029 --> 00:05:00,259
we do use kata what you do is just

00:04:56,330 --> 00:05:02,779
docker in as an example doctor you can

00:05:00,259 --> 00:05:04,939
set default run times so up there

00:05:02,779 --> 00:05:07,819
there's a lot of text somewhere it says

00:05:04,939 --> 00:05:10,009
default run time run see that's like the

00:05:07,819 --> 00:05:12,620
canonical default standard so when

00:05:10,009 --> 00:05:14,750
someone says hey kata why you replacing

00:05:12,620 --> 00:05:16,159
docker it's like well no we're not we're

00:05:14,750 --> 00:05:18,740
complementing an existing solution

00:05:16,159 --> 00:05:21,050
within docker so run sees an OC I

00:05:18,740 --> 00:05:23,389
complained runtime as is kata containers

00:05:21,050 --> 00:05:26,509
so you would add it in there and what it

00:05:23,389 --> 00:05:29,449
looks like then this is magical power

00:05:26,509 --> 00:05:31,310
point execution you would see just

00:05:29,449 --> 00:05:32,930
container is running when you say run

00:05:31,310 --> 00:05:35,569
time equals run C and if you do with the

00:05:32,930 --> 00:05:37,219
kata is the exact same thing only that

00:05:35,569 --> 00:05:42,139
process that you're looking at is

00:05:37,219 --> 00:05:44,629
actually inside of a virtual machine so

00:05:42,139 --> 00:05:46,399
what does it actually look like I say

00:05:44,629 --> 00:05:47,990
kata run time that's the run C

00:05:46,399 --> 00:05:50,000
equivalent so if you have an upper layer

00:05:47,990 --> 00:05:51,889
orchestrate or like kubernetes or like

00:05:50,000 --> 00:05:55,490
docker what it's gonna do is you're

00:05:51,889 --> 00:05:56,930
gonna say cue cuddle apply this workload

00:05:55,490 --> 00:05:59,180
and it's going to create the workload

00:05:56,930 --> 00:06:01,849
and then run the workload right so what

00:05:59,180 --> 00:06:03,199
we do is receive the command this says

00:06:01,849 --> 00:06:06,080
create it and then what we're gonna do a

00:06:03,199 --> 00:06:09,830
medial is create a virtual machine today

00:06:06,080 --> 00:06:11,659
we use qmu KVM we have a minimal kernel

00:06:09,830 --> 00:06:13,250
that's going to boot again we're not

00:06:11,659 --> 00:06:16,039
using a host kernel anymore this is the

00:06:13,250 --> 00:06:17,419
guest a traditional virtual machine but

00:06:16,039 --> 00:06:18,649
it's minimally configured because that's

00:06:17,419 --> 00:06:19,969
kind of a best practice if you have a

00:06:18,649 --> 00:06:23,120
container or less you don't want to have

00:06:19,969 --> 00:06:25,669
all these features just adding an attack

00:06:23,120 --> 00:06:28,069
surface essentially and also a

00:06:25,669 --> 00:06:30,050
lighter-weight footprint by reducing it

00:06:28,069 --> 00:06:32,149
then we just have a really minimal read

00:06:30,050 --> 00:06:34,849
FS that is essentially just starting up

00:06:32,149 --> 00:06:36,680
system D and running an agent this agent

00:06:34,849 --> 00:06:39,080
is like that run C equivalent the

00:06:36,680 --> 00:06:40,129
canonic like the de facto standard and

00:06:39,080 --> 00:06:41,959
all it does is actually

00:06:40,129 --> 00:06:43,849
handle the lifecycle of creating a

00:06:41,959 --> 00:06:45,259
virtual machine so I'm not saying

00:06:43,849 --> 00:06:46,399
containers are terrible we should use

00:06:45,259 --> 00:06:48,830
use virtual machines I'm saying we

00:06:46,399 --> 00:06:51,469
should add an additional layer so we

00:06:48,830 --> 00:06:53,449
still are using namespaces see groups

00:06:51,469 --> 00:06:54,830
we're looking at and figuring out

00:06:53,449 --> 00:06:59,330
whether we want to use set comp on the

00:06:54,830 --> 00:07:01,939
inside or not but the upper layer

00:06:59,330 --> 00:07:04,219
process the orchestrator is expecting to

00:07:01,939 --> 00:07:05,869
see the container everything is kind of

00:07:04,219 --> 00:07:07,129
made to assume that you just have a

00:07:05,869 --> 00:07:08,330
process where none of your hosts and

00:07:07,129 --> 00:07:10,610
you're gonna interact with it through

00:07:08,330 --> 00:07:12,319
there so what we do is we create a

00:07:10,610 --> 00:07:13,939
couple in process and there's a one to

00:07:12,319 --> 00:07:16,249
one ratio and it's tied to the lifecycle

00:07:13,939 --> 00:07:19,759
of the actual container process running

00:07:16,249 --> 00:07:21,439
inside the virtual machine so that means

00:07:19,759 --> 00:07:24,289
now they can go and look at what's the

00:07:21,439 --> 00:07:25,969
standard IO what's the standard air I

00:07:24,289 --> 00:07:28,129
want to send signals to the container

00:07:25,969 --> 00:07:29,839
process that I think is running on the

00:07:28,129 --> 00:07:31,639
host so this is the way we lie to the

00:07:29,839 --> 00:07:35,269
orchestrator and then any signaling

00:07:31,639 --> 00:07:37,249
that's handled would go over inside to

00:07:35,269 --> 00:07:41,059
the agent and the agent would go ahead

00:07:37,249 --> 00:07:43,249
and kill whoever it needs to kill we use

00:07:41,059 --> 00:07:46,909
a V sock we're using vertice really

00:07:43,249 --> 00:07:47,809
before but V sock is a lot cleaner as

00:07:46,909 --> 00:07:52,189
far as being able to have multiple

00:07:47,809 --> 00:07:55,009
clients and it's a G RPC protocol that

00:07:52,189 --> 00:07:57,050
we use for communicating so now you guys

00:07:55,009 --> 00:07:58,669
know everything about kata containers I

00:07:57,050 --> 00:08:01,519
want to start talking about different

00:07:58,669 --> 00:08:04,339
areas of kata containers and what we

00:08:01,519 --> 00:08:07,339
leverage to pretend to not be a virtual

00:08:04,339 --> 00:08:10,689
machine and to hopefully make it later

00:08:07,339 --> 00:08:13,599
wait so boot time is a really easy one

00:08:10,689 --> 00:08:16,610
to kind of look at it's a very you know

00:08:13,599 --> 00:08:17,869
ephemeral workloads containers you think

00:08:16,610 --> 00:08:20,029
you just like you boot it up and it's

00:08:17,869 --> 00:08:22,429
gone you know in like three seconds you

00:08:20,029 --> 00:08:24,829
did something and then it's gone so boot

00:08:22,429 --> 00:08:27,800
time is a very clear like it makes sense

00:08:24,829 --> 00:08:31,309
and it's so easy to measure so some

00:08:27,800 --> 00:08:35,180
things that we do we make use of hot

00:08:31,309 --> 00:08:36,919
plug a good amount so basically a hot

00:08:35,180 --> 00:08:39,319
plug is allowing us to do things in

00:08:36,919 --> 00:08:41,630
parallel so while we are creating the

00:08:39,319 --> 00:08:44,240
virtual machine we can start working on

00:08:41,630 --> 00:08:47,899
preparing the actual container workload

00:08:44,240 --> 00:08:49,490
itself and any potential of volumes that

00:08:47,899 --> 00:08:51,649
that workload may want we can prepare

00:08:49,490 --> 00:08:53,970
them and then once it's up we just hop

00:08:51,649 --> 00:08:56,790
plug them directly in we use qmp

00:08:53,970 --> 00:08:58,920
similar for any devices we also hot plug

00:08:56,790 --> 00:09:01,379
them if a particular container wants to

00:08:58,920 --> 00:09:03,680
make use of a GPU we would hot plug that

00:09:01,379 --> 00:09:06,540
GPU so that way it's off that initial

00:09:03,680 --> 00:09:07,949
initial critical path and doing this to

00:09:06,540 --> 00:09:09,449
allows when people want to start doing

00:09:07,949 --> 00:09:12,120
tricks like having a virtual machine up

00:09:09,449 --> 00:09:14,040
there ready to go but no workload yet so

00:09:12,120 --> 00:09:16,259
you can kind of have like a pool ready

00:09:14,040 --> 00:09:19,529
and then inject workloads into it so

00:09:16,259 --> 00:09:22,620
that way you can kind of mask that those

00:09:19,529 --> 00:09:25,620
the the actual boot time for the virtual

00:09:22,620 --> 00:09:30,810
machine so some other things that we do

00:09:25,620 --> 00:09:33,240
today by default in our packages we use

00:09:30,810 --> 00:09:34,769
queue light which essentially means it's

00:09:33,240 --> 00:09:38,220
cue me with a couple of patches that we

00:09:34,769 --> 00:09:41,009
had on top of it what are you seeing yes

00:09:38,220 --> 00:09:42,959
and basically what we look to do is not

00:09:41,009 --> 00:09:44,339
use firmware we don't want to use forum

00:09:42,959 --> 00:09:45,750
or we don't want any extra steps or

00:09:44,339 --> 00:09:47,399
anything like that so let's just go

00:09:45,750 --> 00:09:50,279
ahead and just add what the firmware is

00:09:47,399 --> 00:09:53,430
doing put it in queue and then just boot

00:09:50,279 --> 00:09:56,759
directly to the kernel this isn't really

00:09:53,430 --> 00:09:59,370
a nice thing to maintain upstream which

00:09:56,759 --> 00:10:02,279
you know makes sense so queue boot was

00:09:59,370 --> 00:10:06,000
introduced which essentially addresses

00:10:02,279 --> 00:10:07,860
all what we wanted out of cumulate so I

00:10:06,000 --> 00:10:09,870
this is a firmware that does nothing

00:10:07,860 --> 00:10:13,139
except for get ready to boot into the

00:10:09,870 --> 00:10:17,449
kernel and to actually link the ACPA

00:10:13,139 --> 00:10:19,740
tables for us so we've been testing this

00:10:17,449 --> 00:10:23,100
because the deadline of wanting to talk

00:10:19,740 --> 00:10:25,379
about it here it works and it looks fine

00:10:23,100 --> 00:10:28,709
so we're going to be looking to move to

00:10:25,379 --> 00:10:32,459
this as we adjust what type of

00:10:28,709 --> 00:10:36,959
hypervisor we use so what about

00:10:32,459 --> 00:10:38,579
footprint we have a minimal kernel I

00:10:36,959 --> 00:10:40,889
talked about that at you know security

00:10:38,579 --> 00:10:43,439
surface great idea also helps with our

00:10:40,889 --> 00:10:45,360
footprint minimal root of s makes sense

00:10:43,439 --> 00:10:47,459
a minimally configured qmu so when we

00:10:45,360 --> 00:10:52,620
build it only the modules that we really

00:10:47,459 --> 00:10:55,529
need we also used X and env dim and we

00:10:52,620 --> 00:10:58,350
do deduplication of memory using KSM so

00:10:55,529 --> 00:11:02,449
looking at it in a picture essentially

00:10:58,350 --> 00:11:05,880
we used ax a direct execute someplace

00:11:02,449 --> 00:11:07,840
yeah and we use that for the root of s

00:11:05,880 --> 00:11:09,130
so the root of s we put into each four

00:11:07,840 --> 00:11:11,140
drill machine so if you have five

00:11:09,130 --> 00:11:13,420
containers you're gonna share the same

00:11:11,140 --> 00:11:15,970
minimal route FS it doesn't really make

00:11:13,420 --> 00:11:18,790
sense that you have five times root of s

00:11:15,970 --> 00:11:21,880
size so what we do is we use Dax for

00:11:18,790 --> 00:11:24,640
that and then we also market for env dim

00:11:21,880 --> 00:11:25,270
and doing that is essentially saying qmu

00:11:24,640 --> 00:11:27,970
please

00:11:25,270 --> 00:11:30,190
guests don't cash this just use the host

00:11:27,970 --> 00:11:32,620
page cash instead so then everybody is

00:11:30,190 --> 00:11:35,320
utilizing the same bits off of the host

00:11:32,620 --> 00:11:36,910
this is a good amount of memory while we

00:11:35,320 --> 00:11:39,370
try to minimize root of s this is now

00:11:36,910 --> 00:11:40,750
amortized the cost of the root of s so

00:11:39,370 --> 00:11:43,890
the more containers we have the cheaper

00:11:40,750 --> 00:11:47,470
it is effectively same idea for KS m

00:11:43,890 --> 00:11:49,360
only this is more active now essentially

00:11:47,470 --> 00:11:51,700
what we'll do is we'll have a throttle

00:11:49,360 --> 00:11:56,590
KSM where well this is an optional thing

00:11:51,700 --> 00:11:58,540
but once a workload container starts up

00:11:56,590 --> 00:11:59,740
it'll kind of kick KSM in case then

00:11:58,540 --> 00:12:02,230
we'll go through and check out the pages

00:11:59,740 --> 00:12:05,500
and then merge them and then get out of

00:12:02,230 --> 00:12:07,420
the way because scanning pages is CPU

00:12:05,500 --> 00:12:09,790
intensive and we don't want to use all

00:12:07,420 --> 00:12:11,530
the CPUs so with these two it's a it's

00:12:09,790 --> 00:12:14,140
another way to help reduce the footprint

00:12:11,530 --> 00:12:15,700
and again this really depends on the

00:12:14,140 --> 00:12:20,350
scale and type of workloads that you're

00:12:15,700 --> 00:12:22,150
using what you're gonna see so totally

00:12:20,350 --> 00:12:26,020
other topic now let's talk about CPU

00:12:22,150 --> 00:12:30,790
topology what we end up doing generally

00:12:26,020 --> 00:12:33,010
is booting container rqm you are caught

00:12:30,790 --> 00:12:34,990
a container with just a single V CPU by

00:12:33,010 --> 00:12:37,390
default and then however many CPUs the

00:12:34,990 --> 00:12:40,540
workload needs will hot-plug in this

00:12:37,390 --> 00:12:42,670
helps from a boot time standpoint as

00:12:40,540 --> 00:12:45,010
well and now we start thinking about how

00:12:42,670 --> 00:12:46,870
should we hot plug it there's really

00:12:45,010 --> 00:12:48,340
kind of three options and I guess you

00:12:46,870 --> 00:12:52,300
can permeate on that so maybe there's

00:12:48,340 --> 00:12:55,180
more but first we can just say how many

00:12:52,300 --> 00:12:57,610
threads just add a thread other option

00:12:55,180 --> 00:12:59,170
let's just add a core third option let's

00:12:57,610 --> 00:13:01,150
just add a socket so every time you want

00:12:59,170 --> 00:13:02,560
to add a vcp you one more socket please

00:13:01,150 --> 00:13:05,380
one more socket please one more socket

00:13:02,560 --> 00:13:07,840
please and through experimentation we

00:13:05,380 --> 00:13:11,020
saw that just giving a socket is a

00:13:07,840 --> 00:13:12,670
little bit more friendly for CPU

00:13:11,020 --> 00:13:14,020
performance and some different testing

00:13:12,670 --> 00:13:15,840
particularly when we're looking at

00:13:14,020 --> 00:13:19,570
Network IO it was just like a slight

00:13:15,840 --> 00:13:21,750
measurable improvement we didn't really

00:13:19,570 --> 00:13:26,160
see a lot of downside

00:13:21,750 --> 00:13:27,810
I think I've seen other research kind of

00:13:26,160 --> 00:13:29,040
highlighting that sometimes using just a

00:13:27,810 --> 00:13:31,320
socket can be a little bit more

00:13:29,040 --> 00:13:33,780
effective I'd be curious if if people

00:13:31,320 --> 00:13:36,150
know or have feedback on that but to us

00:13:33,780 --> 00:13:41,580
it seems this is the most effective for

00:13:36,150 --> 00:13:43,560
our use case memory just like how we do

00:13:41,580 --> 00:13:46,590
the CPUs we often will hot plug memory

00:13:43,560 --> 00:13:47,550
into the virtual machine which a Deena

00:13:46,590 --> 00:13:49,710
is fine

00:13:47,550 --> 00:13:51,270
but removing it when it's not needed so

00:13:49,710 --> 00:13:53,700
sometimes in a container life cycle you

00:13:51,270 --> 00:13:56,010
could have a pod a pod is a group of

00:13:53,700 --> 00:13:58,590
containers all working together in

00:13:56,010 --> 00:14:02,130
kubernetes it's the smallest scheduled

00:13:58,590 --> 00:14:03,300
executable bit and in a pod you can have

00:14:02,130 --> 00:14:05,220
some work that gets done at the very

00:14:03,300 --> 00:14:08,040
beginning and then it's never done again

00:14:05,220 --> 00:14:10,890
so like an init container that'll go and

00:14:08,040 --> 00:14:12,180
make changes in that pod and then it'll

00:14:10,890 --> 00:14:14,580
never execute anymore so you can imagine

00:14:12,180 --> 00:14:16,800
that the resource utilization isn't

00:14:14,580 --> 00:14:19,920
linear in your workload just like most

00:14:16,800 --> 00:14:22,170
workloads wouldn't be so if we can do

00:14:19,920 --> 00:14:24,150
anything to free up system memory it's

00:14:22,170 --> 00:14:25,170
probably a good idea so starting to look

00:14:24,150 --> 00:14:28,890
at how we do this

00:14:25,170 --> 00:14:30,540
ballooning is one option and we kind of

00:14:28,890 --> 00:14:33,030
looked at it you can return any amount

00:14:30,540 --> 00:14:35,460
it doesn't really freeing any slots why

00:14:33,030 --> 00:14:37,020
do I have to even worry about slots and

00:14:35,460 --> 00:14:38,790
now I have to start thinking about I

00:14:37,020 --> 00:14:40,020
want to set a max memory then so that

00:14:38,790 --> 00:14:43,320
way the balloon can take it away but

00:14:40,020 --> 00:14:44,790
then I can add it again if I want the

00:14:43,320 --> 00:14:46,500
maximum set of memory the amount of

00:14:44,790 --> 00:14:49,620
memory that I give to a virtual machine

00:14:46,500 --> 00:14:52,290
costs me as far as footprint so each

00:14:49,620 --> 00:14:54,420
gigabytes about two megabytes worth of

00:14:52,290 --> 00:14:56,580
overhead just kind of managing all the

00:14:54,420 --> 00:14:58,320
memory tables for it if you're a nested

00:14:56,580 --> 00:14:59,760
it's gonna be more at that point as well

00:14:58,320 --> 00:15:01,920
so we don't want to just kind of

00:14:59,760 --> 00:15:03,180
willy-nilly say like you guys have a

00:15:01,920 --> 00:15:06,600
terabyte but we're gonna balloon like

00:15:03,180 --> 00:15:08,820
crazy or anything like this hot remove

00:15:06,600 --> 00:15:11,310
is kind of the other option that I was

00:15:08,820 --> 00:15:14,400
looking at well I'm not looking at the

00:15:11,310 --> 00:15:16,170
team is looking at all right Nakata and

00:15:14,400 --> 00:15:17,490
you know with that you can free slots

00:15:16,170 --> 00:15:20,820
but now you're kind of dictated about

00:15:17,490 --> 00:15:23,010
how big is a slot actually and you know

00:15:20,820 --> 00:15:25,710
but at least I can add memory and it's

00:15:23,010 --> 00:15:26,940
kind of consistent from there what I've

00:15:25,710 --> 00:15:28,320
heard is that there's some work being

00:15:26,940 --> 00:15:31,380
done on virtio mem

00:15:28,320 --> 00:15:32,580
so like para virtualizing memory and you

00:15:31,380 --> 00:15:34,529
know at this point I can do something

00:15:32,580 --> 00:15:37,589
seemingly at a much finer granule

00:15:34,529 --> 00:15:40,170
this sounds great but it's it's working

00:15:37,589 --> 00:15:42,089
progress so we'll see where this ends up

00:15:40,170 --> 00:15:46,649
it seems very attractive for our use

00:15:42,089 --> 00:15:48,839
case and I'm curious also when doing

00:15:46,649 --> 00:15:53,279
this if I'm using a BF io and hot

00:15:48,839 --> 00:15:55,319
plugging PCI devices in AI I don't fully

00:15:53,279 --> 00:15:59,069
understand yet and we're doing some

00:15:55,319 --> 00:16:02,430
testing now on what the impact is as far

00:15:59,069 --> 00:16:03,959
as the DMA like it is that slot that

00:16:02,430 --> 00:16:07,829
happens to have the address space get

00:16:03,959 --> 00:16:11,009
locked down can i no longer do this hot

00:16:07,829 --> 00:16:13,079
removal this is an area that we're still

00:16:11,009 --> 00:16:16,499
looking at and you know I would love to

00:16:13,079 --> 00:16:21,809
talk about speaking of hot plugging VF

00:16:16,499 --> 00:16:22,410
IO devices v fi o is great it's fun it's

00:16:21,809 --> 00:16:24,569
fine

00:16:22,410 --> 00:16:26,399
and what we're looking at is depending

00:16:24,569 --> 00:16:27,930
on the machine type we use we hit kind

00:16:26,399 --> 00:16:29,459
of leave it configurable right now right

00:16:27,930 --> 00:16:31,620
now we support - we're adding a third

00:16:29,459 --> 00:16:33,569
maybe we support three I don't know the

00:16:31,620 --> 00:16:35,759
exact right answer for that but we use

00:16:33,569 --> 00:16:39,750
PC machine typing to use q35 machine

00:16:35,759 --> 00:16:43,649
type if it's PC we do a CPI hot plug if

00:16:39,750 --> 00:16:47,040
it's q 35 we do PCIe but in either case

00:16:43,649 --> 00:16:48,420
it's a it's a PCIe device and we're hot

00:16:47,040 --> 00:16:51,180
plugging it in but it's getting placed

00:16:48,420 --> 00:16:54,839
on a PCI bus inside the guest so what

00:16:51,180 --> 00:16:58,019
I'm wondering is does anybody care those

00:16:54,839 --> 00:17:00,149
lost capabilities in our situation I'm

00:16:58,019 --> 00:17:01,649
not really sure we do know but I'm not

00:17:00,149 --> 00:17:04,670
sure how important it is like some of

00:17:01,649 --> 00:17:04,670
the errors stuff yeah

00:17:05,540 --> 00:17:08,690
[Music]

00:17:11,800 --> 00:17:14,829
[Music]

00:17:18,339 --> 00:17:27,830
yeah yeah exactly yeah so that that's

00:17:25,400 --> 00:17:31,400
what we need to understand if in the

00:17:27,830 --> 00:17:33,650
container type of workload is this gonna

00:17:31,400 --> 00:17:36,710
bite us and then on top of that it's

00:17:33,650 --> 00:17:38,480
also if we had a new machine type if we

00:17:36,710 --> 00:17:41,330
if we could pick we had all the options

00:17:38,480 --> 00:17:44,240
is there a real strong preference pcie

00:17:41,330 --> 00:17:46,820
versus a CPI from my perspective a CPI

00:17:44,240 --> 00:17:48,020
is less what's the timing of hardware

00:17:46,820 --> 00:17:50,450
and everything else you know I'm not

00:17:48,020 --> 00:17:54,620
tied to the PCI spec as much where a CPI

00:17:50,450 --> 00:17:57,230
seems a little bit more modern direct so

00:17:54,620 --> 00:17:59,660
I'm curious you know what the difference

00:17:57,230 --> 00:18:03,580
is but from a performance standpoint you

00:17:59,660 --> 00:18:03,580
know we don't see much difference yeah

00:18:05,950 --> 00:18:16,760
this is a good question yeah let's let's

00:18:15,200 --> 00:18:18,590
follow up and talk about that after I

00:18:16,760 --> 00:18:20,150
have way too many slides but I do want

00:18:18,590 --> 00:18:23,630
to that's exactly conversations I want

00:18:20,150 --> 00:18:29,480
to have so networking how do we how do

00:18:23,630 --> 00:18:31,130
we get I Oh in and out so generally in

00:18:29,480 --> 00:18:33,800
containers like the most straightforward

00:18:31,130 --> 00:18:35,990
basic configuration is you have a ve on

00:18:33,800 --> 00:18:38,510
your host and it drops the other half of

00:18:35,990 --> 00:18:40,880
that ve into a network namespace what we

00:18:38,510 --> 00:18:42,470
have to do then is take a ve 'the living

00:18:40,880 --> 00:18:45,500
inside this namespace and attach it to a

00:18:42,470 --> 00:18:48,380
virtual machine that there are a couple

00:18:45,500 --> 00:18:49,730
options that we looked at McAfee tab is

00:18:48,380 --> 00:18:53,900
what we have right now we're also

00:18:49,730 --> 00:18:56,090
looking at TC meri by default well you

00:18:53,900 --> 00:19:00,080
have by default uses v host threads as

00:18:56,090 --> 00:19:03,530
well I'm not sure if this is the the

00:19:00,080 --> 00:19:05,150
best thing to do or not you know I think

00:19:03,530 --> 00:19:06,320
that stuffin kind of brought up a little

00:19:05,150 --> 00:19:08,210
bit like you could use vhosts user

00:19:06,320 --> 00:19:08,900
instead so that way we're not going all

00:19:08,210 --> 00:19:11,180
the way the guest

00:19:08,900 --> 00:19:14,600
now all senior in the kernel the host

00:19:11,180 --> 00:19:17,690
but for now this is what we use and then

00:19:14,600 --> 00:19:20,120
we use multi cue in I was doing some

00:19:17,690 --> 00:19:22,490
performance testing around nesting and

00:19:20,120 --> 00:19:24,500
then also just in an l1 kind of

00:19:22,490 --> 00:19:24,880
configuration for kata and I saw that

00:19:24,500 --> 00:19:26,740
Bay

00:19:24,880 --> 00:19:28,390
sigelei num keys should be equivalent to

00:19:26,740 --> 00:19:30,760
the number of CPUs until you get to like

00:19:28,390 --> 00:19:32,380
four after that maybe you can get a

00:19:30,760 --> 00:19:34,720
little bit more but these are resources

00:19:32,380 --> 00:19:37,210
that are eating CPUs and eating memory

00:19:34,720 --> 00:19:52,660
that's it's really not getting much

00:19:37,210 --> 00:19:54,760
return whenever I look at performance

00:19:52,660 --> 00:19:56,080
stuff I know they when you look at this

00:19:54,760 --> 00:19:59,140
they're like okay the first thing you do

00:19:56,080 --> 00:20:02,320
is pin the V CPU threads pin the V host

00:19:59,140 --> 00:20:05,050
threads and certainly if you're doing

00:20:02,320 --> 00:20:07,390
that you should be using a VF maybe

00:20:05,050 --> 00:20:09,040
instead I don't know I'm not sure maybe

00:20:07,390 --> 00:20:11,440
maybe you can use that sorry a V or

00:20:09,040 --> 00:20:14,350
maybe you would you know I'm not

00:20:11,440 --> 00:20:15,580
we don't give it's not easy for us to

00:20:14,350 --> 00:20:18,070
start pinning and everything cuz again

00:20:15,580 --> 00:20:20,950
this is this isn't a virtual machine

00:20:18,070 --> 00:20:23,350
anymore this is a container and it'll be

00:20:20,950 --> 00:20:27,370
hard to figure out how to balance that

00:20:23,350 --> 00:20:30,790
but speaking of not using V host another

00:20:27,370 --> 00:20:33,040
way is to use vhosts user so I we

00:20:30,790 --> 00:20:35,350
provide the ability if you have a V host

00:20:33,040 --> 00:20:37,150
user socket so in this drawing you

00:20:35,350 --> 00:20:39,130
wouldn't notice necessarily I have a

00:20:37,150 --> 00:20:40,240
name space now I just have this socket

00:20:39,130 --> 00:20:44,710
and I don't have a network name space

00:20:40,240 --> 00:20:45,850
that's because there it's it's not part

00:20:44,710 --> 00:20:47,320
of the network name space it's not name

00:20:45,850 --> 00:20:49,660
space it's just a file in your system

00:20:47,320 --> 00:20:52,030
right so that one we can just attach

00:20:49,660 --> 00:20:54,730
directly so if you're using something

00:20:52,030 --> 00:20:57,190
like a DP DK or an open V switch or a

00:20:54,730 --> 00:20:59,860
VPP any of these things this is a nice

00:20:57,190 --> 00:21:02,350
way to get connected inside of a

00:20:59,860 --> 00:21:04,240
container as well and then there's

00:21:02,350 --> 00:21:06,310
physical networking you can actually

00:21:04,240 --> 00:21:10,450
just pass the NIC directly in or pass a

00:21:06,310 --> 00:21:13,420
VF directly in and use it so storage

00:21:10,450 --> 00:21:15,190
this is kind of a there's two different

00:21:13,420 --> 00:21:16,570
ways that this could go down it could be

00:21:15,190 --> 00:21:19,990
really nice or it could be awful and I

00:21:16,570 --> 00:21:22,180
don't want to talk about it if it's

00:21:19,990 --> 00:21:24,130
block based cool

00:21:22,180 --> 00:21:27,010
well even still it could be painful but

00:21:24,130 --> 00:21:30,670
if it's black based we default we use

00:21:27,010 --> 00:21:33,880
Verdejo scuzzy but you could we use

00:21:30,670 --> 00:21:36,130
Verdejo block in the past as well we've

00:21:33,880 --> 00:21:37,930
also looked at can we use vhosts user

00:21:36,130 --> 00:21:38,240
here can you start using to something

00:21:37,930 --> 00:21:41,690
like

00:21:38,240 --> 00:21:43,730
BDK if it's not black based it's a

00:21:41,690 --> 00:21:47,000
filesystem base like something like

00:21:43,730 --> 00:21:49,700
overlay then it gets a bit more painful

00:21:47,000 --> 00:21:52,190
we use nine PFS and I don't know how

00:21:49,700 --> 00:21:56,900
often people use nine PFS but it's it

00:21:52,190 --> 00:21:58,490
might be suboptimal so some questions is

00:21:56,900 --> 00:22:01,880
I kind of go through here and try to do

00:21:58,490 --> 00:22:04,640
some performance I by default wheeze

00:22:01,880 --> 00:22:06,020
Boreas Guzzi I feel like from all the

00:22:04,640 --> 00:22:08,120
reading and talking to people it seems

00:22:06,020 --> 00:22:10,040
like all right this is where the

00:22:08,120 --> 00:22:13,610
development is going as far as black

00:22:10,040 --> 00:22:16,760
storage we have configurable to use i/o

00:22:13,610 --> 00:22:18,679
threads it's not set by default I'm

00:22:16,760 --> 00:22:22,130
still working to see and really get

00:22:18,679 --> 00:22:24,020
performance impact of this to see four

00:22:22,130 --> 00:22:25,850
cache settings we're using default right

00:22:24,020 --> 00:22:27,380
now but I mean it really kind of depends

00:22:25,850 --> 00:22:29,390
on the workload so I'm trying to figure

00:22:27,380 --> 00:22:32,390
out you know is should we make that

00:22:29,390 --> 00:22:35,300
easily configurable per container or is

00:22:32,390 --> 00:22:37,880
there a sane default and also we've kind

00:22:35,300 --> 00:22:39,380
of looked at SB DK briefly as well for

00:22:37,880 --> 00:22:43,429
like a V host user scuzzy type of

00:22:39,380 --> 00:22:45,380
interface so that's great if it's block

00:22:43,429 --> 00:22:47,330
again if it's not block if used in

00:22:45,380 --> 00:22:51,140
something like overlay which is very

00:22:47,330 --> 00:22:53,300
popular you get to use nine PFS so you

00:22:51,140 --> 00:22:55,910
know a lack of general POSIX compliance

00:22:53,300 --> 00:22:58,160
how many times I get someone say hey I

00:22:55,910 --> 00:22:59,660
was trying this database workload that

00:22:58,160 --> 00:23:01,850
nobody would really use but it's a nice

00:22:59,660 --> 00:23:04,340
way to quickly poke a database and they

00:23:01,850 --> 00:23:08,030
did an EM map and then you I don't have

00:23:04,340 --> 00:23:10,280
a container anymore that's happened it's

00:23:08,030 --> 00:23:14,360
generally not very performant a lot of

00:23:10,280 --> 00:23:16,790
people come and say I've had someone

00:23:14,360 --> 00:23:17,809
who's running well either way it's not

00:23:16,790 --> 00:23:19,309
performing I don't need an example for

00:23:17,809 --> 00:23:21,350
that you you probably know that it's not

00:23:19,309 --> 00:23:23,540
really particularly well supported it's

00:23:21,350 --> 00:23:25,429
not like people are passionate about 90

00:23:23,540 --> 00:23:28,880
FS and fixing it but the good thing is

00:23:25,429 --> 00:23:31,550
it's available and that's pretty much

00:23:28,880 --> 00:23:35,059
what it has going for us you know as far

00:23:31,550 --> 00:23:37,880
as hitting our use case we're looking at

00:23:35,059 --> 00:23:39,559
you know future alternatives and FS over

00:23:37,880 --> 00:23:42,740
V sock sounded like something that would

00:23:39,559 --> 00:23:45,010
just cure all of our problems I'm not

00:23:42,740 --> 00:23:47,000
sure that that is the case but then also

00:23:45,010 --> 00:23:48,740
you know it's a very very earlier

00:23:47,000 --> 00:23:50,420
discussion of you know can we can we

00:23:48,740 --> 00:23:53,630
make something like a Verdejo

00:23:50,420 --> 00:23:55,610
FS so I'm really excited about that I

00:23:53,630 --> 00:23:58,040
think that's pretty early on though so

00:23:55,610 --> 00:24:01,580
we'll just keep telling people not to

00:23:58,040 --> 00:24:03,680
use overlay some other stuff around

00:24:01,580 --> 00:24:04,940
storage is I was doing some nested

00:24:03,680 --> 00:24:06,560
testing I did a presentation yesterday

00:24:04,940 --> 00:24:08,690
talking about the results for it and

00:24:06,560 --> 00:24:11,270
this is what it looks like for costs if

00:24:08,690 --> 00:24:13,940
you're doing in an l1 it costs point six

00:24:11,270 --> 00:24:16,280
percent 0.2 percent that's that's less

00:24:13,940 --> 00:24:18,350
than a CPU it's a 16 core don't worry

00:24:16,280 --> 00:24:20,660
about it but if you look at the l2 it's

00:24:18,350 --> 00:24:22,720
it's quite quite expensive relatively

00:24:20,660 --> 00:24:27,200
we're looking at about a 20x difference

00:24:22,720 --> 00:24:30,260
between the two so this this has me

00:24:27,200 --> 00:24:31,760
thinking that the use case when you get

00:24:30,260 --> 00:24:34,160
into nested people are excited about

00:24:31,760 --> 00:24:35,470
nested is to be passing something from

00:24:34,160 --> 00:24:38,000
l0 to l2

00:24:35,470 --> 00:24:39,410
there's there is no l1 so we need some

00:24:38,000 --> 00:24:41,480
kind of way to be enlightened and to be

00:24:39,410 --> 00:24:43,550
able to pass devices all the way to l2

00:24:41,480 --> 00:24:44,870
and not be going through two layers of

00:24:43,550 --> 00:24:46,340
black storage or anything like this

00:24:44,870 --> 00:24:48,650
so I think this is a pretty interesting

00:24:46,340 --> 00:24:49,880
area and even if you do that now we have

00:24:48,650 --> 00:24:51,170
to figure out how the interrupter outing

00:24:49,880 --> 00:24:52,930
and everything's gonna handle that's

00:24:51,170 --> 00:25:04,570
still going to be quite an overhead so I

00:24:52,930 --> 00:25:04,570
don't know we'll see okay

00:25:06,970 --> 00:25:11,620
so one more area to look at which is

00:25:09,910 --> 00:25:12,880
kind of a beefier area and I've gotten

00:25:11,620 --> 00:25:14,740
some good feedback from the community

00:25:12,880 --> 00:25:16,750
before already it's on security we are

00:25:14,740 --> 00:25:19,240
providing stronger isolation so it's

00:25:16,750 --> 00:25:20,800
pretty fair to say that we should be

00:25:19,240 --> 00:25:23,140
very concerned about security making

00:25:20,800 --> 00:25:25,990
sure we're doing best practices it's

00:25:23,140 --> 00:25:28,990
hard how many containers can you put in

00:25:25,990 --> 00:25:33,610
a gigabyte very important how secure is

00:25:28,990 --> 00:25:36,010
something is very it's really hard to

00:25:33,610 --> 00:25:37,270
quantify so it's it's been interesting

00:25:36,010 --> 00:25:38,230
and it's always a trade-off and we're

00:25:37,270 --> 00:25:40,900
trying to figure out what the best

00:25:38,230 --> 00:25:42,910
practices should be and having either

00:25:40,900 --> 00:25:46,630
like an ultra secure configuration and

00:25:42,910 --> 00:25:49,180
then a realistic and density favorable

00:25:46,630 --> 00:25:52,390
configuration as well so different under

00:25:49,180 --> 00:25:54,550
the umbrella of security we ran into

00:25:52,390 --> 00:25:57,130
issues for a little bit because our

00:25:54,550 --> 00:25:59,230
containers would essentially block

00:25:57,130 --> 00:26:01,030
because we're waiting to have enough

00:25:59,230 --> 00:26:03,040
entropy to give to the virtual machine

00:26:01,030 --> 00:26:04,630
so it can continue to boot or you know

00:26:03,040 --> 00:26:06,460
if the first thing your containers going

00:26:04,630 --> 00:26:09,040
to do is generate an SSH key or

00:26:06,460 --> 00:26:11,950
something I often we have some issues so

00:26:09,040 --> 00:26:14,980
that was easy just use dev you random

00:26:11,950 --> 00:26:16,900
not dev random that's fine generally

00:26:14,980 --> 00:26:19,270
it's fine

00:26:16,900 --> 00:26:21,610
for most people it's fine if you're that

00:26:19,270 --> 00:26:23,400
concern then you shouldn't be using the

00:26:21,610 --> 00:26:26,260
container anyway maybe I don't know

00:26:23,400 --> 00:26:28,780
enhance security in the guest so right

00:26:26,260 --> 00:26:30,520
now again keep best practices keep it

00:26:28,780 --> 00:26:33,160
rid of fast in the kernel is minimal as

00:26:30,520 --> 00:26:34,660
possible that's fine other things that

00:26:33,160 --> 00:26:37,240
we're looking at is you know we've

00:26:34,660 --> 00:26:39,640
gotten feedback you should should we

00:26:37,240 --> 00:26:41,290
apply a second filter inside of it as

00:26:39,640 --> 00:26:42,940
well there is a default that would come

00:26:41,290 --> 00:26:44,200
with docker and everything else and

00:26:42,940 --> 00:26:46,000
they're creating one for kubernetes now

00:26:44,200 --> 00:26:49,570
doesn't make sense to also run set comp

00:26:46,000 --> 00:26:52,750
inside we can do this but now we need to

00:26:49,570 --> 00:26:54,700
include all the bits for set comp which

00:26:52,750 --> 00:26:58,090
is going to hurt density I'm about 20

00:26:54,700 --> 00:26:59,710
percent or so so we're trying to fight

00:26:58,090 --> 00:27:04,090
that trade-off right now and how easily

00:26:59,710 --> 00:27:05,770
can we make it optional extending

00:27:04,090 --> 00:27:08,470
identity the container workload so using

00:27:05,770 --> 00:27:12,670
virtual TPM we don't do this right now

00:27:08,470 --> 00:27:14,110
but I think often this is extending the

00:27:12,670 --> 00:27:15,760
measured boot so that way container can

00:27:14,110 --> 00:27:18,730
say what am I actually running on

00:27:15,760 --> 00:27:20,830
including myself as well so

00:27:18,730 --> 00:27:22,960
for me I questioned how much people

00:27:20,830 --> 00:27:24,880
really value a virtual TPM if you

00:27:22,960 --> 00:27:26,740
there's enough trust in that and

00:27:24,880 --> 00:27:28,630
extending the TPM with a virtual one but

00:27:26,740 --> 00:27:30,550
it sounds like people are doing this so

00:27:28,630 --> 00:27:33,580
I think it's particularly useful in a

00:27:30,550 --> 00:27:35,830
container or less where you're not ever

00:27:33,580 --> 00:27:39,990
adding packages all the time and having

00:27:35,830 --> 00:27:39,990
changes from the boot it's very static

00:27:40,050 --> 00:27:46,450
other area is protecting the host so

00:27:42,940 --> 00:27:49,960
running non-root qmu we started looking

00:27:46,450 --> 00:27:52,930
at this you can make changes you know

00:27:49,960 --> 00:27:53,680
dev KVM okay find every host okay we can

00:27:52,930 --> 00:27:55,750
work around it

00:27:53,680 --> 00:27:58,360
an IBM fast file system we can do it

00:27:55,750 --> 00:27:59,740
through a proxy helper block devices we

00:27:58,360 --> 00:28:01,090
can do it but then we have to figure out

00:27:59,740 --> 00:28:02,020
what's the containers gone we got to

00:28:01,090 --> 00:28:06,400
assign it back to the original

00:28:02,020 --> 00:28:08,260
permissions and ownership so we've been

00:28:06,400 --> 00:28:11,890
looking at it and we need to do this

00:28:08,260 --> 00:28:13,540
it's hard think you know libvirt does

00:28:11,890 --> 00:28:14,620
this already so I've got the curse I'm

00:28:13,540 --> 00:28:19,030
like how do you guys not doing this

00:28:14,620 --> 00:28:21,730
anymore so we were scared of Liberty

00:28:19,030 --> 00:28:22,690
initially it's too big XML what am I

00:28:21,730 --> 00:28:26,860
going to do with this it's going to add

00:28:22,690 --> 00:28:29,620
complexity we use go vmm right now but

00:28:26,860 --> 00:28:30,730
we see now there's go bindings available

00:28:29,620 --> 00:28:33,700
from the birth so we're starting to

00:28:30,730 --> 00:28:35,260
investigate and look at it and see you

00:28:33,700 --> 00:28:36,730
know this is something we should look at

00:28:35,260 --> 00:28:39,880
because apparently it's gonna solve all

00:28:36,730 --> 00:28:42,190
my problems in the world so and

00:28:39,880 --> 00:28:45,430
similarly if I want to constrain qmu

00:28:42,190 --> 00:28:47,230
itself so cgroups namespaces oh okay

00:28:45,430 --> 00:28:48,760
they do this already we should look at

00:28:47,230 --> 00:28:50,860
it I just don't want all deliberate I

00:28:48,760 --> 00:28:53,320
just want what I want out of it so we

00:28:50,860 --> 00:28:54,780
need to really look at that and then the

00:28:53,320 --> 00:28:57,700
last thing that we're looking at is

00:28:54,780 --> 00:28:59,920
reducing the code footprint for the

00:28:57,700 --> 00:29:02,380
actual hypervisor itself and this is

00:28:59,920 --> 00:29:04,210
another project kind of outside ekkada

00:29:02,380 --> 00:29:07,570
and I'm a consumer of it and I'm excited

00:29:04,210 --> 00:29:10,510
about it for how many essentially

00:29:07,570 --> 00:29:14,700
getting rid of as many machine using a

00:29:10,510 --> 00:29:17,320
new machine type that really eliminates

00:29:14,700 --> 00:29:19,030
emulation as much as possible I'm so

00:29:17,320 --> 00:29:20,830
like a no emulation hypervisor with

00:29:19,030 --> 00:29:23,200
minimal devices available and everything

00:29:20,830 --> 00:29:25,960
else so there's a talk after this about

00:29:23,200 --> 00:29:27,730
that if you don't leave you'll hear it I

00:29:25,960 --> 00:29:29,450
think it's pretty interesting so that'll

00:29:27,730 --> 00:29:31,909
help as well from a footprint

00:29:29,450 --> 00:29:33,649
everything else but if you want to learn

00:29:31,909 --> 00:29:34,850
more you want to contribute

00:29:33,649 --> 00:29:38,419
you want to make liberabit work well

00:29:34,850 --> 00:29:42,880
with cada you can find us we're all over

00:29:38,419 --> 00:29:45,590
the place and that's all I have so

00:29:42,880 --> 00:29:46,460
feedback questions for a few minutes and

00:29:45,590 --> 00:29:52,429
then I would love to talk more

00:29:46,460 --> 00:29:54,790
afterwards as well I'm leaving now so

00:29:52,429 --> 00:29:58,640
you don't talk to me

00:29:54,790 --> 00:30:04,429
[Applause]

00:29:58,640 --> 00:30:04,429

YouTube URL: https://www.youtube.com/watch?v=Cgnb7jMZfPs


