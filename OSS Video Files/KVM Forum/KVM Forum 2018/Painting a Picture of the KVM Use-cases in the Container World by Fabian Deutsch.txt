Title: Painting a Picture of the KVM Use-cases in the Container World by Fabian Deutsch
Publication date: 2018-11-14
Playlist: KVM Forum 2018
Description: 
	KVM is a hypervisor offering strong hardware isolation of a guest from it's hosts.

Containers are now a new software based isolation mechanism for workloads, and it might be a small surprise to see that KVM is surfacing in this context quite often.

In this talk we'll look at how KVM is used on the containers and Kubernetes context.
Specifically we'll be looking at the projects KubeVirt, Katacontainers, gVisor, and virtlet, to understand how KVM is used by them to support certain use-cases.

---

Fabian Deutsch
Red Hat
Manager

Fabian Deutsch is working at Red Hat and used to be much more active in the Fedora community as he is today, worked on the oVirt project for a few years, and is now involved in KubeVirt. In the past he had the opportunity to speak at several conferences (KubeCon, oVirt workshops, FrOsCon, LinuxCon, and devconf.cz).
Captions: 
	00:00:00,260 --> 00:00:02,709
[Music]

00:00:06,470 --> 00:00:14,670
so if you don't mind I would start

00:00:09,809 --> 00:00:15,870
talking if you've got a question feel

00:00:14,670 --> 00:00:18,240
free to find it good it's a lot to ask

00:00:15,870 --> 00:00:19,320
it please interrupt me during the talk

00:00:18,240 --> 00:00:20,820
that should not be much of an issue

00:00:19,320 --> 00:00:22,380
after all I'm here to answer your

00:00:20,820 --> 00:00:26,820
questions and give you an insight into

00:00:22,380 --> 00:00:30,090
the KVM use cases in a container world I

00:00:26,820 --> 00:00:31,740
am myself I'm far beyond Deutsch I'm

00:00:30,090 --> 00:00:37,350
working at red head on virtualization

00:00:31,740 --> 00:00:39,840
for the last six years I started with

00:00:37,350 --> 00:00:43,920
over but that's pretty cross the stack

00:00:39,840 --> 00:00:46,110
so worked on fedora on packaging on the

00:00:43,920 --> 00:00:48,300
low-level tunings pretty interesting

00:00:46,110 --> 00:00:51,899
work I really liked what I do and I

00:00:48,300 --> 00:00:55,949
still like what I'm doing today so what

00:00:51,899 --> 00:00:59,609
am i doing today are working on the

00:00:55,949 --> 00:01:01,350
context of a virtualization the in the

00:00:59,609 --> 00:01:03,059
container context so with containers and

00:01:01,350 --> 00:01:05,970
how do they fit together but before we

00:01:03,059 --> 00:01:11,970
get to that meat or the vegetarian

00:01:05,970 --> 00:01:16,439
option might be an almost human KVM who

00:01:11,970 --> 00:01:19,740
of you does not know cumin KVM very good

00:01:16,439 --> 00:01:21,330
so are stronger together and that for

00:01:19,740 --> 00:01:23,520
quite a few years

00:01:21,330 --> 00:01:26,520
I think Kumaran it came up with KVM and

00:01:23,520 --> 00:01:29,460
cumin even predates that right both of

00:01:26,520 --> 00:01:31,560
them are let's green here both of them

00:01:29,460 --> 00:01:33,000
are very versatile components and they

00:01:31,560 --> 00:01:35,310
are battle tested I mean they are

00:01:33,000 --> 00:01:38,280
appearing in a range of products of

00:01:35,310 --> 00:01:41,040
companies so OpenStack is using Liberty

00:01:38,280 --> 00:01:42,960
Merck a VM we've got over from red head

00:01:41,040 --> 00:01:45,030
but although others they are used in the

00:01:42,960 --> 00:01:47,820
cloud to support all those cloud

00:01:45,030 --> 00:01:50,220
workloads so they're really they they

00:01:47,820 --> 00:01:51,750
did their mileage they provide machine

00:01:50,220 --> 00:01:53,189
devil abstraction a strong isolation all

00:01:51,750 --> 00:01:55,170
of you could probably find more

00:01:53,189 --> 00:02:00,000
attributes describing what queue medium

00:01:55,170 --> 00:02:02,369
can deliver to you so but then I mean

00:02:00,000 --> 00:02:04,320
they empowered all of our workloads for

00:02:02,369 --> 00:02:06,630
so long and then containers came along

00:02:04,320 --> 00:02:08,429
like three or four years ago and they

00:02:06,630 --> 00:02:10,110
and today you know everything is

00:02:08,429 --> 00:02:12,750
containers if you go to Q Khan which

00:02:10,110 --> 00:02:13,830
exploded from I don't know 1,500 people

00:02:12,750 --> 00:02:18,330
two years ago to

00:02:13,830 --> 00:02:21,210
7,000 people this year I guess

00:02:18,330 --> 00:02:22,500
cube con then it's pretty much like what

00:02:21,210 --> 00:02:25,260
we see we Vulcans always saw with

00:02:22,500 --> 00:02:27,120
OpenStack right and the question is what

00:02:25,260 --> 00:02:28,740
about virtualization and we move KVM

00:02:27,120 --> 00:02:31,590
forum here which is a nice venue it's a

00:02:28,740 --> 00:02:33,750
nice place great to be in Edinburgh but

00:02:31,590 --> 00:02:36,390
where do we seek a VM I mean kayvyun

00:02:33,750 --> 00:02:39,750
really showed it proved that it's useful

00:02:36,390 --> 00:02:43,440
we all know what we what values we have

00:02:39,750 --> 00:02:45,270
from Q and KVM does it have any place in

00:02:43,440 --> 00:02:47,700
the container world or can we just get

00:02:45,270 --> 00:02:49,890
rid of it I mean it's easy to duplicate

00:02:47,700 --> 00:02:52,110
code we need to do some discussions of

00:02:49,890 --> 00:02:54,870
how to duplicate stuff but in the end we

00:02:52,110 --> 00:02:56,580
could say right containers are are there

00:02:54,870 --> 00:02:58,800
to stay we can address all of the our

00:02:56,580 --> 00:03:04,530
use cases with containers so let's get

00:02:58,800 --> 00:03:06,690
rid of kV mqn zones for right and to

00:03:04,530 --> 00:03:08,970
answer that we need to see at what do

00:03:06,690 --> 00:03:11,010
containers actually give to us why are

00:03:08,970 --> 00:03:14,820
they so successful and what what makes

00:03:11,010 --> 00:03:17,730
them different to VMs so I think one of

00:03:14,820 --> 00:03:20,580
the things which makes containers so

00:03:17,730 --> 00:03:22,560
attractive is that they focus on the

00:03:20,580 --> 00:03:24,390
applications and and the user workflows

00:03:22,560 --> 00:03:26,310
so vm's were good right

00:03:24,390 --> 00:03:28,550
games were good in that sense that they

00:03:26,310 --> 00:03:31,739
allowed us to petition a host a node

00:03:28,550 --> 00:03:34,620
with with VMs specifically on Linux with

00:03:31,739 --> 00:03:36,989
KVM and Kumu it was so easy I still

00:03:34,620 --> 00:03:38,250
remember with cavium I don't know 0 6 or

00:03:36,989 --> 00:03:39,690
what it was back then you know you

00:03:38,250 --> 00:03:41,280
launch queueing were in another queue

00:03:39,690 --> 00:03:43,709
instance and your two VMs on that same

00:03:41,280 --> 00:03:48,330
note a co-located with different

00:03:43,709 --> 00:03:50,190
services that what so so nice but in the

00:03:48,330 --> 00:03:51,959
end you have to have the knowledge of

00:03:50,190 --> 00:03:53,690
how to set up the operating system how

00:03:51,959 --> 00:03:55,950
to automate all that stuff so it's a

00:03:53,690 --> 00:03:57,300
developer who wanted to use a virtual

00:03:55,950 --> 00:03:58,739
machine and to develop this in the end

00:03:57,300 --> 00:04:02,100
the guy who's developing the application

00:03:58,739 --> 00:04:04,860
for for for some use case right he had

00:04:02,100 --> 00:04:06,750
to know a lot of that stuff he had to

00:04:04,860 --> 00:04:09,660
know the operating system eventually how

00:04:06,750 --> 00:04:13,040
to debug grub booting errors or funny I

00:04:09,660 --> 00:04:15,870
don't know Flags when booting DBM and

00:04:13,040 --> 00:04:19,380
containers removed quite a bit of that

00:04:15,870 --> 00:04:22,080
burden and even more cubic a vm focus

00:04:19,380 --> 00:04:23,490
much around amount running the vm but

00:04:22,080 --> 00:04:24,960
how do we get there how's the image

00:04:23,490 --> 00:04:27,250
created how does the content get into

00:04:24,960 --> 00:04:29,980
the image tooling was created

00:04:27,250 --> 00:04:31,930
guest fish helped us to build images and

00:04:29,980 --> 00:04:34,120
we got other tuning our cloud in it

00:04:31,930 --> 00:04:37,570
allows us to modify content in a VM too

00:04:34,120 --> 00:04:41,860
to personalize instances but that all

00:04:37,570 --> 00:04:44,110
grow over time with containers that was

00:04:41,860 --> 00:04:47,290
a smart move to to define that flow of

00:04:44,110 --> 00:04:49,270
how to create it and to run it they they

00:04:47,290 --> 00:04:51,400
did that very well and that attracted

00:04:49,270 --> 00:04:53,080
developers because that burden you know

00:04:51,400 --> 00:04:55,120
all that operating system burden was not

00:04:53,080 --> 00:04:58,090
something the developers himself or

00:04:55,120 --> 00:04:59,560
herself had to care about so that's why

00:04:58,090 --> 00:05:01,690
there's this smaller sentence at the end

00:04:59,560 --> 00:05:04,210
Edwards and John virtualization because

00:05:01,690 --> 00:05:04,840
they really got the power I should

00:05:04,210 --> 00:05:06,910
rephrase it

00:05:04,840 --> 00:05:09,220
got the power to to define the VM and

00:05:06,910 --> 00:05:10,600
developers really enjoy more containers

00:05:09,220 --> 00:05:12,220
because they don't have to worry about

00:05:10,600 --> 00:05:18,220
the operating system they want to focus

00:05:12,220 --> 00:05:20,230
on their application now caving koomer

00:05:18,220 --> 00:05:21,520
were there but they weren't chosen to be

00:05:20,230 --> 00:05:23,530
the solution of the technical

00:05:21,520 --> 00:05:26,050
implementation to deliver this new user

00:05:23,530 --> 00:05:28,180
experience all of this was brought to

00:05:26,050 --> 00:05:30,220
you by the new technology which had no

00:05:28,180 --> 00:05:32,980
bugs and it was so so great you know it

00:05:30,220 --> 00:05:35,440
could solve everything it was brand new

00:05:32,980 --> 00:05:37,210
well not so brand new because it was

00:05:35,440 --> 00:05:40,570
picking up some existing kernel features

00:05:37,210 --> 00:05:44,530
like C groups and namespaces sure that

00:05:40,570 --> 00:05:46,180
grow over time grew over time and it's

00:05:44,530 --> 00:05:50,169
today much richer than it was two or

00:05:46,180 --> 00:05:52,390
three years ago but there were very few

00:05:50,169 --> 00:05:56,740
overlapping things between the virtual

00:05:52,390 --> 00:05:58,630
machines and containers now fast forward

00:05:56,740 --> 00:06:00,790
today we said containers came up and

00:05:58,630 --> 00:06:02,650
they dressed use cases where do we stand

00:06:00,790 --> 00:06:06,370
today actually after these two or three

00:06:02,650 --> 00:06:07,870
years of experience so today it's not

00:06:06,370 --> 00:06:12,970
that we have a single container running

00:06:07,870 --> 00:06:14,530
on a single host no instead we have

00:06:12,970 --> 00:06:17,890
multiple containers running and host and

00:06:14,530 --> 00:06:21,270
that not only you know that's not

00:06:17,890 --> 00:06:23,500
helpful ah very nice thank you

00:06:21,270 --> 00:06:26,050
we've got multiple containers running a

00:06:23,500 --> 00:06:28,479
single host and usually that's not even

00:06:26,050 --> 00:06:30,970
useful so what appeared is a container

00:06:28,479 --> 00:06:33,850
Orchestrator which is managing a cluster

00:06:30,970 --> 00:06:36,010
of hosts for you to run containers for

00:06:33,850 --> 00:06:38,080
multiple tenants so actually that was

00:06:36,010 --> 00:06:40,419
fast forward into what we know with or

00:06:38,080 --> 00:06:41,200
what we have seen with the Android we

00:06:40,419 --> 00:06:43,420
have large

00:06:41,200 --> 00:06:45,310
just like OpenStack where you can

00:06:43,420 --> 00:06:48,130
workloads from multiple tenants that's

00:06:45,310 --> 00:06:51,460
actually where containers are today open

00:06:48,130 --> 00:06:53,410
shift or kubernetes tectonic all those

00:06:51,460 --> 00:06:55,540
are our distributions of kubernetes for

00:06:53,410 --> 00:06:58,900
example which is one container

00:06:55,540 --> 00:07:00,850
Orchestrator allows you to run

00:06:58,900 --> 00:07:03,820
containers of production for multiple

00:07:00,850 --> 00:07:06,640
tenants so from the high level today we

00:07:03,820 --> 00:07:08,260
are where we were with I don't know what

00:07:06,640 --> 00:07:11,410
we will revert like two or three years

00:07:08,260 --> 00:07:13,420
ago being able to run workloads of

00:07:11,410 --> 00:07:17,830
multiple tenants on a single cluster in

00:07:13,420 --> 00:07:19,090
a production style manner but containers

00:07:17,830 --> 00:07:21,760
and production environments have

00:07:19,090 --> 00:07:23,440
production particular problems so like

00:07:21,760 --> 00:07:25,660
four or five years of three or four

00:07:23,440 --> 00:07:28,320
years ago containers were new and they

00:07:25,660 --> 00:07:31,150
were driven by companies like docker and

00:07:28,320 --> 00:07:34,780
and they could run their production

00:07:31,150 --> 00:07:36,790
workloads on containers because because

00:07:34,780 --> 00:07:38,860
they knew them inside out right but it

00:07:36,790 --> 00:07:41,020
was very hard for customers to run to

00:07:38,860 --> 00:07:42,520
run their workloads inside containers

00:07:41,020 --> 00:07:44,950
because there was no knowledge about

00:07:42,520 --> 00:07:46,240
them today it's different today with

00:07:44,950 --> 00:07:48,370
kubernetes with well-defined

00:07:46,240 --> 00:07:51,520
orchestrators which can run containers

00:07:48,370 --> 00:07:52,780
and because we're now running contain

00:07:51,520 --> 00:07:55,050
some production we also gain more

00:07:52,780 --> 00:08:00,100
experience of how content is behaving in

00:07:55,050 --> 00:08:02,920
production and people know people

00:08:00,100 --> 00:08:07,390
learned quite a lot in the recent years

00:08:02,920 --> 00:08:08,740
and and that's now after ramping up the

00:08:07,390 --> 00:08:12,670
talk and looking you know where

00:08:08,740 --> 00:08:15,100
containers came from that's I think

00:08:12,670 --> 00:08:18,720
working start to compare so some of the

00:08:15,100 --> 00:08:21,520
problems we see in in production is that

00:08:18,720 --> 00:08:25,570
people realize which who run containers

00:08:21,520 --> 00:08:27,700
huh workloads you know can be insecure

00:08:25,570 --> 00:08:29,590
right this that get a repository which

00:08:27,700 --> 00:08:33,370
somebody committed or pushed a PR to and

00:08:29,590 --> 00:08:35,890
we merged it now this is run on my hosts

00:08:33,370 --> 00:08:37,930
alongside my my Oracle database or

00:08:35,890 --> 00:08:40,180
Microsoft database to you know

00:08:37,930 --> 00:08:43,030
maintaining our billing or whatever and

00:08:40,180 --> 00:08:45,130
you know they're just separated by

00:08:43,030 --> 00:08:46,960
namespaces oh well and you know I did I

00:08:45,130 --> 00:08:49,150
did that a shared mount between these

00:08:46,960 --> 00:08:51,490
two namespaces so how secure is that

00:08:49,150 --> 00:08:53,500
after all so people started to worry

00:08:51,490 --> 00:08:55,620
about you know the isolation features of

00:08:53,500 --> 00:08:58,360
containers are they really

00:08:55,620 --> 00:09:00,339
are they really strict are they a strict

00:08:58,360 --> 00:09:02,740
actually as you know that other thing

00:09:00,339 --> 00:09:04,480
here how was it called KVM cave-in Kumu

00:09:02,740 --> 00:09:07,329
right they gave us a strict separation

00:09:04,480 --> 00:09:08,800
between tenants on the single-mode kent

00:09:07,329 --> 00:09:13,089
containers actually provide the same

00:09:08,800 --> 00:09:14,560
feature set from that perspective so

00:09:13,089 --> 00:09:16,390
that is one thing that really came up

00:09:14,560 --> 00:09:20,410
and and it's an issue today

00:09:16,390 --> 00:09:21,880
if you run containers you have to ask

00:09:20,410 --> 00:09:24,670
yourself what kind of workload to are

00:09:21,880 --> 00:09:27,160
run on a node in in container and how do

00:09:24,670 --> 00:09:28,930
I separate that that's not even an issue

00:09:27,160 --> 00:09:32,079
for for setups where you have multiple

00:09:28,930 --> 00:09:34,269
tenants but it's also an issue if you

00:09:32,079 --> 00:09:35,680
are the only tenant on a cluster because

00:09:34,269 --> 00:09:38,649
also there you might have your own

00:09:35,680 --> 00:09:41,620
private important projects which run

00:09:38,649 --> 00:09:44,730
alongside build slaves for example which

00:09:41,620 --> 00:09:49,390
which doesn't run untrusted content

00:09:44,730 --> 00:09:52,240
another problem people run into is right

00:09:49,390 --> 00:09:54,339
so I migrated like 90% of my workloads

00:09:52,240 --> 00:09:56,230
and they run great in containers I don't

00:09:54,339 --> 00:09:58,390
worry that much about separation but you

00:09:56,230 --> 00:10:01,839
know what the heck do I now do about

00:09:58,390 --> 00:10:04,839
this specific workload so I'm I was

00:10:01,839 --> 00:10:07,060
doing my kernel module testing and if I

00:10:04,839 --> 00:10:08,500
do that in my VM somehow the the node is

00:10:07,060 --> 00:10:09,579
crashing all my containers whenever

00:10:08,500 --> 00:10:13,089
there's something wrong with my kernel

00:10:09,579 --> 00:10:15,070
module or I got my my old application

00:10:13,089 --> 00:10:18,190
which is very much tuned to watch a

00:10:15,070 --> 00:10:21,310
specific chip said or or hardware

00:10:18,190 --> 00:10:23,339
topology I cannot really I cannot really

00:10:21,310 --> 00:10:26,079
create that environment in the container

00:10:23,339 --> 00:10:28,000
another thing is for example doing doing

00:10:26,079 --> 00:10:29,709
testing with different operating systems

00:10:28,000 --> 00:10:32,140
so I want to test my agent which my

00:10:29,709 --> 00:10:33,730
company's writing on Windows but I also

00:10:32,140 --> 00:10:37,980
want to test them on FreeBSD NetBSD and

00:10:33,730 --> 00:10:39,970
I don't know some UNIX David or Mac OS I

00:10:37,980 --> 00:10:42,730
VMs helped me to do that

00:10:39,970 --> 00:10:46,149
containers don't help me to do that so I

00:10:42,730 --> 00:10:48,970
think what I want to say is the more

00:10:46,149 --> 00:10:52,329
people move to containers the more we

00:10:48,970 --> 00:10:55,089
see what containers cannot deliver to us

00:10:52,329 --> 00:10:58,449
and that's where the community started

00:10:55,089 --> 00:11:00,300
to to to process you know to think what

00:10:58,449 --> 00:11:03,250
can we do about these issues

00:11:00,300 --> 00:11:08,860
please look at that circle

00:11:03,250 --> 00:11:10,360
you all want to and they process what

00:11:08,860 --> 00:11:12,100
they saw and they looked at the problems

00:11:10,360 --> 00:11:14,920
and they wondered what can we do about

00:11:12,100 --> 00:11:17,260
that and I mean these two selected

00:11:14,920 --> 00:11:20,560
problems I just presented actually just

00:11:17,260 --> 00:11:22,900
a subset of all the I wouldn't say

00:11:20,560 --> 00:11:24,760
problems I see but all of all the new

00:11:22,900 --> 00:11:27,220
things we learned about containers it's

00:11:24,760 --> 00:11:30,100
just not that these two problems exist

00:11:27,220 --> 00:11:31,990
but there are more and depending on

00:11:30,100 --> 00:11:33,910
where you come from you will see your

00:11:31,990 --> 00:11:36,040
new problems in that container world

00:11:33,910 --> 00:11:37,570
like somebody with a networking

00:11:36,040 --> 00:11:39,070
background will have fun going to

00:11:37,570 --> 00:11:43,240
kubernetes and looking how to solve

00:11:39,070 --> 00:11:46,840
fancy V&F problems right but we continue

00:11:43,240 --> 00:11:49,240
to look at that circle and we continue

00:11:46,840 --> 00:11:52,890
to look at the at this problem scope

00:11:49,240 --> 00:11:56,110
from from a KVM and Kumu perspective and

00:11:52,890 --> 00:11:58,090
as outlined in the description of this

00:11:56,110 --> 00:12:00,700
talk we're going to look at a few

00:11:58,090 --> 00:12:04,090
projects which now pick up a KVM in

00:12:00,700 --> 00:12:08,350
humor to address these two problems so

00:12:04,090 --> 00:12:09,910
cata containers is named first because

00:12:08,350 --> 00:12:12,760
it's actually building and that's

00:12:09,910 --> 00:12:15,010
written in the last line on three pretty

00:12:12,760 --> 00:12:16,300
old projects which were frankly hyper SH

00:12:15,010 --> 00:12:18,670
and clear containers and they merged

00:12:16,300 --> 00:12:20,380
because we're so similar and they

00:12:18,670 --> 00:12:23,020
actually provide an isolation layer

00:12:20,380 --> 00:12:24,880
around containers so if you've got an

00:12:23,020 --> 00:12:26,800
untrusted workload you build a github

00:12:24,880 --> 00:12:28,750
repository right

00:12:26,800 --> 00:12:31,930
Katty containers tries to solve that

00:12:28,750 --> 00:12:36,130
problem by wrapping this container in a

00:12:31,930 --> 00:12:40,480
QE movie n which is very convenient they

00:12:36,130 --> 00:12:42,250
actually drive quite a lot of invention

00:12:40,480 --> 00:12:43,839
which we'll get to in a minute but the

00:12:42,250 --> 00:12:46,870
goal here really is to take a container

00:12:43,839 --> 00:12:49,150
and bring the security problem isolation

00:12:46,870 --> 00:12:51,430
properties we know from caving Kumu to

00:12:49,150 --> 00:12:53,910
the container side it has penalties

00:12:51,430 --> 00:12:58,360
right one big difference between

00:12:53,910 --> 00:13:00,250
containers and VMs is the boot time so

00:12:58,360 --> 00:13:01,450
how long does it take from the request

00:13:00,250 --> 00:13:03,040
until you have a really working

00:13:01,450 --> 00:13:04,570
environment you can work with and

00:13:03,040 --> 00:13:06,910
catechin Tainos initially suffered they

00:13:04,570 --> 00:13:10,690
did a lot of good work to to prove stock

00:13:06,910 --> 00:13:13,360
well to fork stock qumu and modified in

00:13:10,690 --> 00:13:15,100
order to speed up the boot process a lot

00:13:13,360 --> 00:13:16,690
of good ideas came from that and they

00:13:15,100 --> 00:13:19,000
also actively work on the back

00:13:16,690 --> 00:13:21,040
creating these changes to cumin to make

00:13:19,000 --> 00:13:23,290
it also available to other users which

00:13:21,040 --> 00:13:28,570
might require these features of having a

00:13:23,290 --> 00:13:33,400
more efficient to process who've you did

00:13:28,570 --> 00:13:35,920
work with communities before okay that's

00:13:33,400 --> 00:13:38,110
not too many but but a few so kubernetes

00:13:35,920 --> 00:13:40,780
as I said is an Orchestrator and this is

00:13:38,110 --> 00:13:43,990
probably I will show you some syntax now

00:13:40,780 --> 00:13:49,150
or config file that defines how to run a

00:13:43,990 --> 00:13:50,500
container on communities so a reminder

00:13:49,150 --> 00:13:52,660
for those who looked into kubernetes and

00:13:50,500 --> 00:13:54,910
it's a general introduction who to those

00:13:52,660 --> 00:13:56,530
who didn't look into kubernetes in

00:13:54,910 --> 00:13:58,990
kubernetes you define what kind of

00:13:56,530 --> 00:14:00,490
workload you want to run by defining the

00:13:58,990 --> 00:14:02,730
containers you want to run so your whole

00:14:00,490 --> 00:14:06,640
application is inside such a container

00:14:02,730 --> 00:14:10,240
in general the unit which is run on a

00:14:06,640 --> 00:14:12,820
kubernetes cluster is a pod so a policy

00:14:10,240 --> 00:14:16,060
group of whales traditionally and

00:14:12,820 --> 00:14:17,710
because you know that that that align

00:14:16,060 --> 00:14:19,660
with between whales that docker image

00:14:17,710 --> 00:14:21,430
and a group of containers so I think

00:14:19,660 --> 00:14:22,840
that's where pod comes from so pod

00:14:21,430 --> 00:14:25,720
contain can contain one or more

00:14:22,840 --> 00:14:27,880
containers and I showed this slide

00:14:25,720 --> 00:14:31,650
because this single line is actually

00:14:27,880 --> 00:14:34,420
deciding if you run your container

00:14:31,650 --> 00:14:38,050
untrusted in the namespace in a regular

00:14:34,420 --> 00:14:39,790
Linux namespace on your host or if katas

00:14:38,050 --> 00:14:43,410
jumping in and running it for you inside

00:14:39,790 --> 00:14:46,200
a kuma VM so the important thing here is

00:14:43,410 --> 00:14:49,630
what we spoke about early on that

00:14:46,200 --> 00:14:51,280
workflow right we've I mean the workflow

00:14:49,630 --> 00:14:53,560
for user what he's running is still a

00:14:51,280 --> 00:14:56,800
container he's not working with the VM

00:14:53,560 --> 00:14:59,320
he says I want to run a container so

00:14:56,800 --> 00:15:00,640
from a user perspective if you think

00:14:59,320 --> 00:15:02,290
about what you would be doing it would

00:15:00,640 --> 00:15:04,000
still be building containers you would

00:15:02,290 --> 00:15:05,620
still be uploading or pushing containers

00:15:04,000 --> 00:15:07,930
to registry and he would tell

00:15:05,620 --> 00:15:09,760
communities to run a container in the

00:15:07,930 --> 00:15:11,620
background however for you a VM would

00:15:09,760 --> 00:15:14,350
respond in order to isolate that

00:15:11,620 --> 00:15:17,440
container at runtime it's important

00:15:14,350 --> 00:15:19,600
because the workflow is not that a

00:15:17,440 --> 00:15:22,450
user's building some VM and pushing that

00:15:19,600 --> 00:15:24,790
to communities that is not the case the

00:15:22,450 --> 00:15:27,770
focus is really really to secure a

00:15:24,790 --> 00:15:31,880
container I might be repeating myself

00:15:27,770 --> 00:15:35,930
so alright Nexus neighbor which is

00:15:31,880 --> 00:15:41,680
actually pretty pretty fresh name who is

00:15:35,930 --> 00:15:45,860
is a project I don't know which was

00:15:41,680 --> 00:15:50,530
brought up at last cube con so earlier

00:15:45,860 --> 00:15:55,070
this year right and it is about

00:15:50,530 --> 00:15:56,870
optimizing Q EMU for not necessary for

00:15:55,070 --> 00:15:59,480
katha countenance as I learned but in

00:15:56,870 --> 00:16:00,860
general to optimize Q more to to be more

00:15:59,480 --> 00:16:02,510
efficient when it comes to boot

00:16:00,860 --> 00:16:04,640
processes and that is supporting kata

00:16:02,510 --> 00:16:06,910
why do i why do i point out in a move

00:16:04,640 --> 00:16:09,800
here because the talk is about you know

00:16:06,910 --> 00:16:11,450
what impact or how what relationship do

00:16:09,800 --> 00:16:14,270
we have got between containers and cave

00:16:11,450 --> 00:16:17,630
in Kumu today a name was the result of

00:16:14,270 --> 00:16:19,730
that containers brought up the

00:16:17,630 --> 00:16:22,670
motivation to look at queue and see what

00:16:19,730 --> 00:16:23,870
can we change in order to meet aspects

00:16:22,670 --> 00:16:25,970
of containers which we don't have in

00:16:23,870 --> 00:16:30,470
human KVM today which is to be very

00:16:25,970 --> 00:16:35,240
efficient at booting to be very quick in

00:16:30,470 --> 00:16:37,160
order to be or to stay competitive yeah

00:16:35,240 --> 00:16:40,420
and it came in that next slide because

00:16:37,160 --> 00:16:45,710
it was very closely related to cattle

00:16:40,420 --> 00:16:48,380
alright another project which worries

00:16:45,710 --> 00:16:50,480
about KVM but not QE more actually

00:16:48,380 --> 00:16:53,210
that's interesting here is g visor so g

00:16:50,480 --> 00:16:56,000
oh by the way all slides have that small

00:16:53,210 --> 00:16:57,290
paperclip icon down here so if you want

00:16:56,000 --> 00:16:58,820
to learn more about a specific project

00:16:57,290 --> 00:17:01,070
just click on that link in the slides

00:16:58,820 --> 00:17:04,880
which i'll share afterwards in him and

00:17:01,070 --> 00:17:07,850
all the glory details g visor as the

00:17:04,880 --> 00:17:10,400
prefixed g indicates was was launched by

00:17:07,850 --> 00:17:12,620
Google and they also wanted to address

00:17:10,400 --> 00:17:15,320
the problem of isolating untrusted

00:17:12,620 --> 00:17:17,450
workloads so traditionally they had an

00:17:15,320 --> 00:17:21,280
application engine called App Engine

00:17:17,450 --> 00:17:24,230
Google App Engine I remember now and

00:17:21,280 --> 00:17:28,490
they wanted to allow to secure this

00:17:24,230 --> 00:17:30,560
workload and they saw that's very hard

00:17:28,490 --> 00:17:33,830
to do with pure namespaces so how do we

00:17:30,560 --> 00:17:35,930
do that and G wise is actually something

00:17:33,830 --> 00:17:37,370
like a like a meta runtime for

00:17:35,930 --> 00:17:40,940
containers because it has different

00:17:37,370 --> 00:17:41,600
backends it has the sandbox which is

00:17:40,940 --> 00:17:44,029
based on Kate

00:17:41,600 --> 00:17:46,159
and so all the Siskel's are proxied

00:17:44,029 --> 00:17:48,230
through KVM in order to to secure them

00:17:46,159 --> 00:17:51,259
but there also is a different back-end

00:17:48,230 --> 00:17:53,419
which is using second to just filter out

00:17:51,259 --> 00:17:55,850
the this is calls which are not

00:17:53,419 --> 00:17:57,740
permitted and which is not using KVM and

00:17:55,850 --> 00:17:59,990
then they have the proper proprietary

00:17:57,740 --> 00:18:02,179
approach if i'm not mistaken which isn't

00:17:59,990 --> 00:18:05,659
the Google own back-end to do it in

00:18:02,179 --> 00:18:09,830
their data center but my point is even G

00:18:05,659 --> 00:18:12,289
visor picked up KVM again because KVM

00:18:09,830 --> 00:18:14,029
provided more isolation primitives than

00:18:12,289 --> 00:18:19,429
the pure namespaces and cgroups provider

00:18:14,029 --> 00:18:21,889
of the stock linux kernel another

00:18:19,429 --> 00:18:26,059
project and this is project number one

00:18:21,889 --> 00:18:28,580
two three four right also picked up k vm

00:18:26,059 --> 00:18:30,440
aqm ooh and the pattern we see it's

00:18:28,580 --> 00:18:34,419
emerging right they also look at

00:18:30,440 --> 00:18:37,279
isolation isolating containers again

00:18:34,419 --> 00:18:39,019
they use they have little I mean that's

00:18:37,279 --> 00:18:41,480
pretty much like Hatter containers right

00:18:39,019 --> 00:18:43,220
these you just queuing you and KVM to

00:18:41,480 --> 00:18:47,269
launch it around a container in order to

00:18:43,220 --> 00:18:48,980
secured the the approach is a little bit

00:18:47,269 --> 00:18:52,580
different because they're focuses on

00:18:48,980 --> 00:18:54,740
keeping Kumu as it is so cata is really

00:18:52,580 --> 00:18:57,679
looking at providing all the features in

00:18:54,740 --> 00:18:59,149
a very efficient way so to pass through

00:18:57,679 --> 00:19:00,980
block devices in order to get high

00:18:59,149 --> 00:19:02,809
performance to really attach network

00:19:00,980 --> 00:19:06,019
interfaces in a high-performing way

00:19:02,809 --> 00:19:08,090
whereas run queue is more looking at at

00:19:06,019 --> 00:19:10,250
simplicity so how can we make that

00:19:08,090 --> 00:19:11,960
simple also in order to make it portable

00:19:10,250 --> 00:19:14,269
because if you're always taking that is

00:19:11,960 --> 00:19:19,610
actually that comes from the background

00:19:14,269 --> 00:19:21,230
of this IBM z series from mistaking so

00:19:19,610 --> 00:19:23,919
that might be even be able to run on a

00:19:21,230 --> 00:19:27,889
different architecture than make 6864

00:19:23,919 --> 00:19:29,779
but the motivation and goal is pretty

00:19:27,889 --> 00:19:32,690
much the same as the previous projects

00:19:29,779 --> 00:19:34,730
and also how you run it how you indicate

00:19:32,690 --> 00:19:37,340
is as we saw in the previous slide you

00:19:34,730 --> 00:19:41,960
put an annotation on a pod in order to

00:19:37,340 --> 00:19:43,460
run that pot isolated now the next

00:19:41,960 --> 00:19:45,759
project Birdland is a little bit

00:19:43,460 --> 00:19:45,759
different

00:19:46,000 --> 00:19:50,840
bird'll it's a little bit different so

00:19:48,200 --> 00:19:53,450
vert led has a different motivation it

00:19:50,840 --> 00:19:54,930
allows you to run vm appliances with a

00:19:53,450 --> 00:19:57,900
container api using

00:19:54,930 --> 00:19:59,970
Kumu cavium subverted again is a project

00:19:57,900 --> 00:20:02,940
in the kubernetes space

00:19:59,970 --> 00:20:06,780
it reached verse 1 I think during this

00:20:02,940 --> 00:20:13,410
year or end of last year and allows you

00:20:06,780 --> 00:20:16,950
to run VMs just like you would run

00:20:13,410 --> 00:20:21,420
containers what does that mean if we

00:20:16,950 --> 00:20:23,610
look at the definition of a pod which we

00:20:21,420 --> 00:20:26,250
also looked at before you see that up

00:20:23,610 --> 00:20:28,500
here the definition is still a pod right

00:20:26,250 --> 00:20:30,950
so to kubernetes that would mean the

00:20:28,500 --> 00:20:33,930
user wants to run a container but

00:20:30,950 --> 00:20:36,630
because of this special annotation over

00:20:33,930 --> 00:20:38,190
here it is telling communities please

00:20:36,630 --> 00:20:40,380
use different runtime for that container

00:20:38,190 --> 00:20:45,210
which happens to be vert let and vert

00:20:40,380 --> 00:20:47,790
let us then interpreting that spec part

00:20:45,210 --> 00:20:50,070
of that pot definition differently so

00:20:47,790 --> 00:20:51,750
the image down here which a regular

00:20:50,070 --> 00:20:53,580
container runtime would use you know it

00:20:51,750 --> 00:20:55,080
would go to the red docker registry and

00:20:53,580 --> 00:20:57,420
pull down the image and then run it

00:20:55,080 --> 00:21:00,300
inside a container runtime what vert

00:20:57,420 --> 00:21:02,310
that is doing it is actually looking it

00:21:00,300 --> 00:21:04,950
is understanding this image to be a

00:21:02,310 --> 00:21:07,200
regular cue cow or raw image it will

00:21:04,950 --> 00:21:09,690
pull down that image it would create a

00:21:07,200 --> 00:21:11,520
create a queue in more instance and

00:21:09,690 --> 00:21:14,550
attach that image wire up there

00:21:11,520 --> 00:21:16,860
networking you usually have some more

00:21:14,550 --> 00:21:19,100
specifications like or you can set like

00:21:16,860 --> 00:21:22,350
amount of memory and number of CPUs

00:21:19,100 --> 00:21:23,820
which is the regular pot API and would

00:21:22,350 --> 00:21:27,480
create the VM based on these

00:21:23,820 --> 00:21:31,470
informations the limitation aware of

00:21:27,480 --> 00:21:32,790
this approach is that you can only and

00:21:31,470 --> 00:21:35,520
don't blame you if somebody's from there

00:21:32,790 --> 00:21:37,560
ladies here because I'll get to that the

00:21:35,520 --> 00:21:40,650
limitation is that the VM you are

00:21:37,560 --> 00:21:44,220
creating around this image is limited to

00:21:40,650 --> 00:21:46,140
what the pot definition can express the

00:21:44,220 --> 00:21:47,880
best example is always locked migration

00:21:46,140 --> 00:21:48,930
so large migration is one of these

00:21:47,880 --> 00:21:51,240
features we have in the virtualization

00:21:48,930 --> 00:21:56,520
world who is not aware of lack of

00:21:51,240 --> 00:21:58,620
migration good or what so handle no live

00:21:56,520 --> 00:22:00,240
migrations not expressible because live

00:21:58,620 --> 00:22:01,080
migration does not exist in the

00:22:00,240 --> 00:22:03,150
container world

00:22:01,080 --> 00:22:06,919
it's an anti-pattern over there right so

00:22:03,150 --> 00:22:10,759
there's no API to trigger or or money

00:22:06,919 --> 00:22:14,409
for a live migration another thing is

00:22:10,759 --> 00:22:16,669
for example ballooning or the type of

00:22:14,409 --> 00:22:18,619
graphical interface you know the the

00:22:16,669 --> 00:22:22,519
kind of disk bus you want to use all

00:22:18,619 --> 00:22:25,340
these needy little features you need in

00:22:22,519 --> 00:22:28,399
order to make a VM suited for specific

00:22:25,340 --> 00:22:31,609
guest right that cannot be expressed

00:22:28,399 --> 00:22:35,029
here all what I said it was not so true

00:22:31,609 --> 00:22:37,489
because in the end they do support to

00:22:35,029 --> 00:22:39,889
provide certain configuration options in

00:22:37,489 --> 00:22:43,220
a wild-card annotation field so you for

00:22:39,889 --> 00:22:45,980
certain fields you can say I want to use

00:22:43,220 --> 00:22:49,279
a vertical disk bus by default or I want

00:22:45,980 --> 00:22:51,019
to use I actually don't know the full

00:22:49,279 --> 00:22:52,940
feature scope but there are annotations

00:22:51,019 --> 00:22:55,759
which allows you to set certain parts of

00:22:52,940 --> 00:22:58,119
the domain definition so there

00:22:55,759 --> 00:23:01,929
internally is they also using different

00:22:58,119 --> 00:23:05,239
the benefit is that to kubernetes

00:23:01,929 --> 00:23:07,789
running in vm is totally transparent so

00:23:05,239 --> 00:23:09,350
kubernetes can take such a definition

00:23:07,789 --> 00:23:12,409
and would treat it just like a container

00:23:09,350 --> 00:23:16,220
and that is helpful because kubernetes

00:23:12,409 --> 00:23:17,809
has constructs like replica sets which

00:23:16,220 --> 00:23:19,759
is something like a scale group so you

00:23:17,809 --> 00:23:22,190
can say I want a replica set of this

00:23:19,759 --> 00:23:25,519
image and if you increase the count to

00:23:22,190 --> 00:23:28,460
10 then you get 10 instances of that of

00:23:25,519 --> 00:23:30,799
that image so you would have 10 VMs

00:23:28,460 --> 00:23:32,960
running on your cluster scheduled by the

00:23:30,799 --> 00:23:34,609
kubernetes scheduler and that would be

00:23:32,960 --> 00:23:37,539
running all alongside each other on that

00:23:34,609 --> 00:23:39,470
cluster on given nodes

00:23:37,539 --> 00:23:41,179
so that is vert lid

00:23:39,470 --> 00:23:42,799
I hope the distinction is clear because

00:23:41,179 --> 00:23:49,429
of different problem was addressed yes

00:23:42,799 --> 00:23:54,080
please yes oh can you still have several

00:23:49,429 --> 00:23:58,669
images running within a single pot as

00:23:54,080 --> 00:24:01,340
you could the classic containers as far

00:23:58,669 --> 00:24:04,480
as I know not but I might be mistaken

00:24:01,340 --> 00:24:07,460
there is also movement still there

00:24:04,480 --> 00:24:11,389
someone else did a different answer on

00:24:07,460 --> 00:24:13,759
the details no okay but so that is my

00:24:11,389 --> 00:24:16,159
last knowledge but there's a link also

00:24:13,759 --> 00:24:18,040
on that slide before so feel free to

00:24:16,159 --> 00:24:22,930
look there

00:24:18,040 --> 00:24:22,930
okay cool

00:24:23,060 --> 00:24:30,230
and then we have a little slide next one

00:24:29,090 --> 00:24:32,420
then with Qbert

00:24:30,230 --> 00:24:34,340
oh by the way I always put logos there

00:24:32,420 --> 00:24:36,710
where project had a logo sets and bytes

00:24:34,340 --> 00:24:40,970
then there's Qbert which is yet another

00:24:36,710 --> 00:24:42,980
project which I happen to work on it

00:24:40,970 --> 00:24:45,710
allows you to run the images on

00:24:42,980 --> 00:24:49,100
kubernetes as well using livered command

00:24:45,710 --> 00:24:51,250
kbm in general libvirt

00:24:49,100 --> 00:24:55,130
can be considered to be something like a

00:24:51,250 --> 00:24:59,780
cluster delivered api as much as you can

00:24:55,130 --> 00:25:01,130
get it what does that mean so that's

00:24:59,780 --> 00:25:02,960
really the other side of the spectrum

00:25:01,130 --> 00:25:04,550
right we said on the one hand side it's

00:25:02,960 --> 00:25:06,800
totally transparent to users that VMs

00:25:04,550 --> 00:25:09,530
are used for isolation purpose here with

00:25:06,800 --> 00:25:11,510
Qbert it's an explicit goal to to give

00:25:09,530 --> 00:25:13,870
the user the explicit power to define

00:25:11,510 --> 00:25:19,490
what kind of VM will be created

00:25:13,870 --> 00:25:21,620
so oops so up here previously in the

00:25:19,490 --> 00:25:23,540
kind we always saw that it's about a pod

00:25:21,620 --> 00:25:26,030
a group of containers or container

00:25:23,540 --> 00:25:29,120
itself here it's a virtual machine

00:25:26,030 --> 00:25:30,110
instance and the definition down here is

00:25:29,120 --> 00:25:32,300
also different

00:25:30,110 --> 00:25:33,830
if anybody of you has happened to work

00:25:32,300 --> 00:25:36,440
with Liberty you might see some

00:25:33,830 --> 00:25:39,830
similarities so you can really define

00:25:36,440 --> 00:25:42,770
the devices the disks the disk buses the

00:25:39,830 --> 00:25:44,990
number of CPU cores the CPU topology

00:25:42,770 --> 00:25:47,750
sockets and so on and so forth so this

00:25:44,990 --> 00:25:53,270
is explicitly about defining the kind of

00:25:47,750 --> 00:25:56,410
VM you want to run in the cluster for

00:25:53,270 --> 00:26:00,740
Cubert that is important because we say

00:25:56,410 --> 00:26:03,500
different guests Windows Mac OS Linux

00:26:00,740 --> 00:26:05,150
have different requirements on on des on

00:26:03,500 --> 00:26:10,550
the virtual Hardware it's running on

00:26:05,150 --> 00:26:12,200
right out of the box windows will not

00:26:10,550 --> 00:26:13,700
support the Vertigo bus you need to

00:26:12,200 --> 00:26:16,340
install the relevant drivers then it

00:26:13,700 --> 00:26:17,960
will but out of the box it doesn't there

00:26:16,340 --> 00:26:22,370
are some microkernel operating systems

00:26:17,960 --> 00:26:24,920
and I know what is is not here but they

00:26:22,370 --> 00:26:27,260
require explicitly in Vertigo bus so we

00:26:24,920 --> 00:26:31,160
want to give the and and the stated goal

00:26:27,260 --> 00:26:31,850
of Qbert is to allow to run every guest

00:26:31,160 --> 00:26:33,860
out there

00:26:31,850 --> 00:26:35,930
every with a little star which says as

00:26:33,860 --> 00:26:37,070
soon as long as it's reasonable I mean

00:26:35,930 --> 00:26:38,810
there was a discussion about machine

00:26:37,070 --> 00:26:40,730
pipes and so on and so forth but the

00:26:38,810 --> 00:26:43,040
goal was to give the user the power to

00:26:40,730 --> 00:26:45,740
define the VMS they need in order to run

00:26:43,040 --> 00:26:49,550
a specific guest and to give it really

00:26:45,740 --> 00:26:51,880
the level or the power as you have it in

00:26:49,550 --> 00:26:51,880
the booth

00:26:52,150 --> 00:26:58,850
yeah so liver Qbert provides these guest

00:26:56,240 --> 00:27:00,950
templates in the option repository so

00:26:58,850 --> 00:27:04,880
that you can select fedora or ascent OS

00:27:00,950 --> 00:27:06,530
or Windows and at the right API will be

00:27:04,880 --> 00:27:09,980
defined for you in order to launch a

00:27:06,530 --> 00:27:11,930
given image yeah that's the difference

00:27:09,980 --> 00:27:14,480
and here the stated goal is to run to

00:27:11,930 --> 00:27:16,520
run him by the way you just not only run

00:27:14,480 --> 00:27:18,920
the VM but because it's running on top

00:27:16,520 --> 00:27:20,570
of kubernetes inside a pod I mean in the

00:27:18,920 --> 00:27:23,780
end the pod is still the context in

00:27:20,570 --> 00:27:25,790
which the VM is running that VM can be

00:27:23,780 --> 00:27:28,100
integrated with the other parts running

00:27:25,790 --> 00:27:29,870
on that cluster right so it could it

00:27:28,100 --> 00:27:32,300
will speak to the same network and use

00:27:29,870 --> 00:27:35,030
the same storage devices can use the

00:27:32,300 --> 00:27:36,680
same infrastructure just like all other

00:27:35,030 --> 00:27:38,780
containers the different workloads on

00:27:36,680 --> 00:27:41,690
that cluster by the way that's also true

00:27:38,780 --> 00:27:43,730
for these are the projects as long as

00:27:41,690 --> 00:27:46,580
the container runtimes are installed on

00:27:43,730 --> 00:27:48,590
the cluster nodes isolated pots and

00:27:46,580 --> 00:27:52,810
regular pots can be mixed and matched on

00:27:48,590 --> 00:27:56,270
that on the cluster okay

00:27:52,810 --> 00:27:59,360
that's it so far that were those aren't

00:27:56,270 --> 00:28:02,360
all projects but I think some the large

00:27:59,360 --> 00:28:03,710
and active projects one important one oh

00:28:02,360 --> 00:28:05,300
well it's not so much and they know

00:28:03,710 --> 00:28:07,430
that's I think that's a good chunk of

00:28:05,300 --> 00:28:10,250
the projects I think we're like six of

00:28:07,430 --> 00:28:14,120
them which now mainly address the

00:28:10,250 --> 00:28:16,070
isolation use case or the traditional

00:28:14,120 --> 00:28:18,710
more traditional VMs which is actually

00:28:16,070 --> 00:28:20,690
outlined here darn I should have taken

00:28:18,710 --> 00:28:25,640
out the legacy right that's so it sounds

00:28:20,690 --> 00:28:27,980
so old but awful it isn't to summarize

00:28:25,640 --> 00:28:29,330
so gee visor cannot contain as an emu

00:28:27,980 --> 00:28:31,340
and run queue are rather on the

00:28:29,330 --> 00:28:33,950
isolation side whereas cube root invert

00:28:31,340 --> 00:28:36,530
let are rather on this side to run VM

00:28:33,950 --> 00:28:39,470
workloads on top of communities and all

00:28:36,530 --> 00:28:43,160
of them really rely on human KVM in

00:28:39,470 --> 00:28:45,690
order to to achieve their goal I think

00:28:43,160 --> 00:28:50,629
what I want to highlight here is also

00:28:45,690 --> 00:28:54,120
that you know before the container times

00:28:50,629 --> 00:28:57,480
this was really the target the use case

00:28:54,120 --> 00:28:58,889
of cumin KVM and I think what we saw now

00:28:57,480 --> 00:29:00,360
with this development in the recent

00:28:58,889 --> 00:29:03,720
years with with the rise of these

00:29:00,360 --> 00:29:06,240
projects is that that KVM and cumin with

00:29:03,720 --> 00:29:09,419
with the additional tooling actually you

00:29:06,240 --> 00:29:10,919
know conquered a new use case which is

00:29:09,419 --> 00:29:12,539
the container isolation which is not

00:29:10,919 --> 00:29:16,679
just the isolation but if you look from

00:29:12,539 --> 00:29:18,570
a user perspective it's a different flow

00:29:16,679 --> 00:29:20,309
which allows us to leverage VMs and that

00:29:18,570 --> 00:29:22,139
is the container workflow you know using

00:29:20,309 --> 00:29:24,539
that whole build chain to get stuff into

00:29:22,139 --> 00:29:29,179
an isolated into VM environment that is

00:29:24,539 --> 00:29:29,179
what we actually gained from that change

00:29:29,690 --> 00:29:35,669
right all of that is not just a one-way

00:29:33,330 --> 00:29:37,950
ticket so it's not just that that these

00:29:35,669 --> 00:29:40,019
projects consume stuff from Qumu KVM you

00:29:37,950 --> 00:29:43,289
know we it's not just that they that

00:29:40,019 --> 00:29:45,960
it's used in order to provide value to

00:29:43,289 --> 00:29:48,509
user but it's also it's actually

00:29:45,960 --> 00:29:49,980
enriching the discussion right so there

00:29:48,509 --> 00:29:53,700
are a few examples

00:29:49,980 --> 00:29:55,500
cata contains for example is didn't have

00:29:53,700 --> 00:29:57,509
issues but they looked at you know how

00:29:55,500 --> 00:29:59,009
can we improve sharing sharing the

00:29:57,509 --> 00:30:01,889
storage which was traditionally

00:29:59,009 --> 00:30:04,320
filesystem based in communities with DVM

00:30:01,889 --> 00:30:05,940
so they they started with an IP of not

00:30:04,320 --> 00:30:07,950
mistaking and then they you know

00:30:05,940 --> 00:30:09,720
continued that discussion to see you

00:30:07,950 --> 00:30:11,220
know how can we improve that stack in

00:30:09,720 --> 00:30:12,659
order to make it more efficient what can

00:30:11,220 --> 00:30:15,840
we do doesn't make sense increase

00:30:12,659 --> 00:30:17,789
improving IP or are there odd I think I

00:30:15,840 --> 00:30:18,120
now wrote down vert if s it should have

00:30:17,789 --> 00:30:21,299
been

00:30:18,120 --> 00:30:22,679
NFS of avert I of no mistaken so you

00:30:21,299 --> 00:30:25,129
know they went into that discussion to

00:30:22,679 --> 00:30:28,610
see what can be improved and in the end

00:30:25,129 --> 00:30:31,409
the traditional virtualization stack

00:30:28,610 --> 00:30:32,850
gained these features right so it's

00:30:31,409 --> 00:30:34,350
valuable to other parties the

00:30:32,850 --> 00:30:36,929
traditional uses of the traditional

00:30:34,350 --> 00:30:39,419
virtualization solutions as well like

00:30:36,929 --> 00:30:42,480
nemu Foster's the discussion on fair and

00:30:39,419 --> 00:30:44,279
wise devices about machine x what can we

00:30:42,480 --> 00:30:46,049
do here to make that more efficient is

00:30:44,279 --> 00:30:48,860
there room to duplicate some stuff is

00:30:46,049 --> 00:30:51,509
that really still needed

00:30:48,860 --> 00:30:52,919
Cubert for example is more focusing on

00:30:51,509 --> 00:30:57,419
the higher level stack you know how can

00:30:52,919 --> 00:30:59,460
we make it more how can we provide how

00:30:57,419 --> 00:31:01,080
can we do the VM configurations

00:30:59,460 --> 00:31:03,029
where can we store the informations of

00:31:01,080 --> 00:31:04,770
how how a via needs to look to run a

00:31:03,029 --> 00:31:06,809
specific guests very good how can we

00:31:04,770 --> 00:31:08,340
store that in a reusable manner that

00:31:06,809 --> 00:31:10,470
comes for example from the Cuban side

00:31:08,340 --> 00:31:12,390
and another thing you know really

00:31:10,470 --> 00:31:14,429
technically bird which is also been done

00:31:12,390 --> 00:31:16,100
not not written down here you know to

00:31:14,429 --> 00:31:19,760
break it up into smaller components

00:31:16,100 --> 00:31:22,529
liver Jim work is the key word here

00:31:19,760 --> 00:31:24,690
that's also coming from these new use

00:31:22,529 --> 00:31:28,770
cases so I think the gist I want to say

00:31:24,690 --> 00:31:30,179
is I think all that all that demand

00:31:28,770 --> 00:31:32,309
which is coming from the container side

00:31:30,179 --> 00:31:34,580
is enriching the whole virtualization

00:31:32,309 --> 00:31:39,120
solution or the whole virtualization

00:31:34,580 --> 00:31:43,049
ecosystem even more you know it's making

00:31:39,120 --> 00:31:46,080
more vivid obviously obviously that is

00:31:43,049 --> 00:31:49,260
leads to changes so what's the takeaway

00:31:46,080 --> 00:31:51,210
in general virtualization is here it's

00:31:49,260 --> 00:31:55,760
still here it's going to stay I think

00:31:51,210 --> 00:31:58,679
that's clear because what we see is

00:31:55,760 --> 00:32:00,899
first for the core virtualization

00:31:58,679 --> 00:32:02,730
primitives like KVM Kumu it was clear

00:32:00,899 --> 00:32:05,700
that this isolation is something which

00:32:02,730 --> 00:32:08,090
is still desired right but also for VMs

00:32:05,700 --> 00:32:10,350
you know for that form factor of vm i

00:32:08,090 --> 00:32:12,299
think it's pretty clear that this will

00:32:10,350 --> 00:32:14,820
also not go away containers cannot just

00:32:12,299 --> 00:32:16,610
want to one replace VMs so they are here

00:32:14,820 --> 00:32:19,350
to stay

00:32:16,610 --> 00:32:21,270
the development of containers and the

00:32:19,350 --> 00:32:22,590
new insights into these workflows and to

00:32:21,270 --> 00:32:24,240
the applications which are running

00:32:22,590 --> 00:32:26,159
interconnects also have an impact on the

00:32:24,240 --> 00:32:27,659
KVM ecosystem which might not be so

00:32:26,159 --> 00:32:30,210
straightforward if we go two years back

00:32:27,659 --> 00:32:31,500
it was not so clear that what that there

00:32:30,210 --> 00:32:33,149
would be a positive impact on the

00:32:31,500 --> 00:32:36,570
development on vert on the whole vert

00:32:33,149 --> 00:32:38,909
stack right but in the end the new use

00:32:36,570 --> 00:32:41,520
cases probably shifted the requirements

00:32:38,909 --> 00:32:43,649
so if we were on all a lot our previous

00:32:41,520 --> 00:32:45,690
trails on the track we've been on with

00:32:43,649 --> 00:32:48,000
the traditional virtualization we might

00:32:45,690 --> 00:32:49,799
have continued down a different path so

00:32:48,000 --> 00:32:52,020
all this container workloads use cases

00:32:49,799 --> 00:32:53,730
workflows they surely had an impact

00:32:52,020 --> 00:32:55,919
which which our drivers eventually in

00:32:53,730 --> 00:32:58,230
two different directions again let me

00:32:55,919 --> 00:33:01,250
point out that Nemo Nemo work which i

00:32:58,230 --> 00:33:04,919
think is a highlight of how drastic

00:33:01,250 --> 00:33:08,789
these new requirements impact impact

00:33:04,919 --> 00:33:11,309
word right and a little bit early but

00:33:08,789 --> 00:33:13,140
that it from my side and there's some

00:33:11,309 --> 00:33:17,280
room for questions

00:33:13,140 --> 00:33:18,630
I believe that every project you've

00:33:17,280 --> 00:33:20,760
talked about has a kind of one-to-one

00:33:18,630 --> 00:33:22,350
relationship between containers and VMs

00:33:20,760 --> 00:33:24,990
are there any if anyone looking at

00:33:22,350 --> 00:33:29,280
putting multiple containers from one

00:33:24,990 --> 00:33:31,049
tenant into single VMs sorry I could you

00:33:29,280 --> 00:33:32,970
repeat that please for me I believe that

00:33:31,049 --> 00:33:34,559
the projects you looked at have a pretty

00:33:32,970 --> 00:33:37,170
much a one-to-one relationship between

00:33:34,559 --> 00:33:39,090
the container and the VM are there any

00:33:37,170 --> 00:33:42,920
projects that look at putting multiple

00:33:39,090 --> 00:33:46,580
containers from one tenant into a VM

00:33:42,920 --> 00:33:49,380
yeah I think I was not precise enough I

00:33:46,580 --> 00:33:52,020
apologize so a specific I know it for

00:33:49,380 --> 00:33:54,809
cutter so there multiple containers run

00:33:52,020 --> 00:34:01,890
in the same VM context so that it's

00:33:54,809 --> 00:34:05,460
explicitly the case what about any

00:34:01,890 --> 00:34:11,790
projects of contain containers running

00:34:05,460 --> 00:34:13,560
alongside the current position yeah so

00:34:11,790 --> 00:34:17,340
Cubert allows you to do that right it

00:34:13,560 --> 00:34:20,760
allows you to run VMs it allows you to

00:34:17,340 --> 00:34:22,649
run VMs alongside alongside containers

00:34:20,760 --> 00:34:24,690
and they can speak to each other a

00:34:22,649 --> 00:34:26,159
different approach the to be honest this

00:34:24,690 --> 00:34:28,560
is OpenStack Magnum is not mistaken

00:34:26,159 --> 00:34:30,869
which allows you to run containers on

00:34:28,560 --> 00:34:32,399
top of OpenStack so there are different

00:34:30,869 --> 00:34:35,520
approaches if you look at that use case

00:34:32,399 --> 00:34:39,659
you know how can I orchestrate both VMs

00:34:35,520 --> 00:34:41,639
and containers at the same time and then

00:34:39,659 --> 00:34:43,590
I would say cuber today inverted to some

00:34:41,639 --> 00:34:46,340
degree in OpenStack Magnum would be

00:34:43,590 --> 00:34:46,340
would be options

00:34:51,429 --> 00:34:57,920
hi a couple of years ago there was a

00:34:54,560 --> 00:34:59,960
push to package up workloads in in

00:34:57,920 --> 00:35:02,240
unique onal setups because people wanted

00:34:59,960 --> 00:35:06,020
to have the isolation of VMs but have a

00:35:02,240 --> 00:35:08,210
single workload per VM do you have any

00:35:06,020 --> 00:35:09,560
view on where that's going or whether

00:35:08,210 --> 00:35:11,330
that's just dead in the water because

00:35:09,560 --> 00:35:15,380
people prefer the flexibility of the

00:35:11,330 --> 00:35:16,790
container type workflow so I think if I

00:35:15,380 --> 00:35:18,320
understood you correctly then I would

00:35:16,790 --> 00:35:22,520
say gee visor has more on that side

00:35:18,320 --> 00:35:24,230
right because dividers really for a

00:35:22,520 --> 00:35:25,930
single application inputting that you

00:35:24,230 --> 00:35:29,720
know isolating that single application

00:35:25,930 --> 00:35:32,600
that is the one-to-one mapping a single

00:35:29,720 --> 00:35:35,450
application with KVM so that is really

00:35:32,600 --> 00:35:37,040
you know is that yes I mean that the

00:35:35,450 --> 00:35:40,520
idea of a union kernel is you compiled

00:35:37,040 --> 00:35:43,100
your application as a kernel in one

00:35:40,520 --> 00:35:45,380
thing so he didn't have a guest user

00:35:43,100 --> 00:35:47,390
space to come to the guest colonel to

00:35:45,380 --> 00:35:50,470
how his kernel transition you just have

00:35:47,390 --> 00:35:54,560
to had one operating level inside the VM

00:35:50,470 --> 00:35:56,800
so I'd least had one guy who did that

00:35:54,560 --> 00:36:00,410
with a microkernel I think what was it

00:35:56,800 --> 00:36:02,750
ma no it wasn't one of the other there

00:36:00,410 --> 00:36:05,630
were several but I have not seen a

00:36:02,750 --> 00:36:07,930
serious open source project we're just

00:36:05,630 --> 00:36:07,930
doing that

00:36:13,510 --> 00:36:18,760
I just wanted to clarify for the kind of

00:36:15,790 --> 00:36:22,150
question or the tenant kind of a

00:36:18,760 --> 00:36:23,950
question it's the package granularity

00:36:22,150 --> 00:36:26,320
that you would have is for a pod so you

00:36:23,950 --> 00:36:28,630
can have multiple containers within a

00:36:26,320 --> 00:36:30,040
single kind of e/m but those would only

00:36:28,630 --> 00:36:33,820
be in the pod you can't just say like

00:36:30,040 --> 00:36:39,430
tenant foo and you get this Jagan's ovm

00:36:33,820 --> 00:36:40,869
and tenant bar the other but in that

00:36:39,430 --> 00:36:42,940
case you know you could just create a VM

00:36:40,869 --> 00:36:45,000
sorry about that you know and just

00:36:42,940 --> 00:36:49,480
schedule all your containers in that

00:36:45,000 --> 00:36:50,890
it'd be easier I mean that is yeah

00:36:49,480 --> 00:36:53,590
that's actually the solution we

00:36:50,890 --> 00:36:55,270
otherwise see without VMs that that

00:36:53,590 --> 00:37:04,060
people create dedicated notes for

00:36:55,270 --> 00:37:07,030
untrusted workers or for tenants so

00:37:04,060 --> 00:37:10,300
there are basically two basic metrics

00:37:07,030 --> 00:37:13,359
that differ between containers and VMs

00:37:10,300 --> 00:37:17,859
and at least in my opinion and this is

00:37:13,359 --> 00:37:20,080
startup time and memory consumption so

00:37:17,859 --> 00:37:23,109
startup time is I did some measurements

00:37:20,080 --> 00:37:25,810
on that some benchmarking something like

00:37:23,109 --> 00:37:27,970
a penalty of currently 1,000

00:37:25,810 --> 00:37:31,750
milliseconds or something I guess you

00:37:27,970 --> 00:37:36,430
can reduce this by some degree so this

00:37:31,750 --> 00:37:40,660
is pretty fine for me but the memory

00:37:36,430 --> 00:37:45,010
footprint is completely different do you

00:37:40,660 --> 00:37:48,400
see any progress on this issue in the

00:37:45,010 --> 00:37:50,950
near future on this subject

00:37:48,400 --> 00:37:52,600
yes so first today I would not take

00:37:50,950 --> 00:37:56,460
these two aspects to compare VMs and

00:37:52,600 --> 00:38:02,260
crdenas but to answer your question I

00:37:56,460 --> 00:38:06,180
don't know no I actually wonder about

00:38:02,260 --> 00:38:08,710
that whole was it pmm stuff you know

00:38:06,180 --> 00:38:10,359
persistent memory where where you don't

00:38:08,710 --> 00:38:12,130
have that disti I mean the problem is

00:38:10,359 --> 00:38:14,380
right which VMs have is that there's

00:38:12,130 --> 00:38:17,200
this clear distinction between RAM you

00:38:14,380 --> 00:38:19,540
know fast memory and slower memory and I

00:38:17,200 --> 00:38:22,300
wonder if stuff like peameal might might

00:38:19,540 --> 00:38:24,040
realize you know make that make it make

00:38:22,300 --> 00:38:25,960
make that less of an issue because the

00:38:24,040 --> 00:38:30,970
distinction doesn't need to be there

00:38:25,960 --> 00:38:32,470
to make a sense well what I want to say

00:38:30,970 --> 00:38:34,750
is why I wouldn't say I wouldn't compare

00:38:32,470 --> 00:38:38,140
them until they do - on these two

00:38:34,750 --> 00:38:40,119
objectives is because in the end to me

00:38:38,140 --> 00:38:41,859
really the distinction between VMs and

00:38:40,119 --> 00:38:43,510
containers is about the use of workflows

00:38:41,859 --> 00:38:45,430
it's not it's not so much about

00:38:43,510 --> 00:38:48,420
technical aspects that's really I think

00:38:45,430 --> 00:38:52,990
we need to measure it on the workflows

00:38:48,420 --> 00:38:54,640
that is what yeah from a user

00:38:52,990 --> 00:38:57,400
perspective what I think makes makes the

00:38:54,640 --> 00:38:59,589
difference between them so just

00:38:57,400 --> 00:39:02,080
something I wanted to say is that there

00:38:59,589 --> 00:39:04,330
is an idea of how to bootstrap like

00:39:02,080 --> 00:39:06,070
container like walk load in a VM

00:39:04,330 --> 00:39:09,070
environment what you do is basically you

00:39:06,070 --> 00:39:11,710
hold a VM that is already booted in

00:39:09,070 --> 00:39:13,900
suspend and then you do like a fork

00:39:11,710 --> 00:39:14,260
memory by doing a copy and right on the

00:39:13,900 --> 00:39:17,140
EPT

00:39:14,260 --> 00:39:20,550
and this basically gives you the

00:39:17,140 --> 00:39:22,599
container like experience if you and

00:39:20,550 --> 00:39:24,700
workloads that are based on the same

00:39:22,599 --> 00:39:27,369
operation system that you hold on the

00:39:24,700 --> 00:39:30,400
side this is similar to what I think

00:39:27,369 --> 00:39:34,589
Microsoft is doing with their a I think

00:39:30,400 --> 00:39:34,589
it's called herb garden mechanism

00:39:38,830 --> 00:39:46,400
thank you so I agree with you if I mean

00:39:44,390 --> 00:39:48,470
about the fact that if you only look at

00:39:46,400 --> 00:39:50,720
boot time and memory consumption you're

00:39:48,470 --> 00:39:55,160
really reducing the differences between

00:39:50,720 --> 00:39:57,619
VM and containers and the actual user

00:39:55,160 --> 00:39:59,150
workflow is is what really makes

00:39:57,619 --> 00:40:01,430
difference between VM and containers and

00:39:59,150 --> 00:40:03,830
you can do all sort of tricks to reduce

00:40:01,430 --> 00:40:06,770
boot time and everything but packaging

00:40:03,830 --> 00:40:09,670
is is another aspect of containers that

00:40:06,770 --> 00:40:12,680
you do not have with virtual machines

00:40:09,670 --> 00:40:15,650
overall the the the way people use

00:40:12,680 --> 00:40:18,800
containers is very different than the

00:40:15,650 --> 00:40:20,660
way they use virtual machines so it's

00:40:18,800 --> 00:40:22,460
it's not only a it's not only about

00:40:20,660 --> 00:40:25,490
technical issues and you can you can

00:40:22,460 --> 00:40:28,550
reduce boot time you can reduce memory

00:40:25,490 --> 00:40:31,070
consumption but there's a bunch of other

00:40:28,550 --> 00:40:33,290
technical aspect that you have to manage

00:40:31,070 --> 00:40:36,080
which is how do I make a virtual machine

00:40:33,290 --> 00:40:38,810
look like a container and this is this

00:40:36,080 --> 00:40:41,390
is this is probably 80% of the work that

00:40:38,810 --> 00:40:43,790
we did with with carry containers 20%

00:40:41,390 --> 00:40:46,010
was how to make a VM boot fast and not

00:40:43,790 --> 00:40:47,960
consumed too much memory and the rest

00:40:46,010 --> 00:40:50,750
was how do I actually make it look like

00:40:47,960 --> 00:40:54,290
a container to kubernetes to dock her to

00:40:50,750 --> 00:40:56,690
all the Ox trace results out there so it

00:40:54,290 --> 00:40:58,490
really is about the user workflow more

00:40:56,690 --> 00:41:02,200
than than technical issues and to answer

00:40:58,490 --> 00:41:06,369
the the memory consumption question

00:41:02,200 --> 00:41:08,540
there's it's not a binary thing it's not

00:41:06,369 --> 00:41:12,470
containers bare metal containers consume

00:41:08,540 --> 00:41:14,450
less memory than then then VM containers

00:41:12,470 --> 00:41:15,920
like scatter containers because the

00:41:14,450 --> 00:41:18,310
advantage of the virtual machine is that

00:41:15,920 --> 00:41:20,060
you control the the guest consumption

00:41:18,310 --> 00:41:22,730
from your IP visor

00:41:20,060 --> 00:41:23,990
whereas with with docker containers you

00:41:22,730 --> 00:41:26,510
actually don't do that you launch your

00:41:23,990 --> 00:41:28,580
container if you launch 1000 containers

00:41:26,510 --> 00:41:30,830
that do exactly the same thing they're

00:41:28,580 --> 00:41:32,810
gonna have a lot of duplicating memory

00:41:30,830 --> 00:41:34,670
across all containers

00:41:32,810 --> 00:41:36,560
whereas with with an advisor you can

00:41:34,670 --> 00:41:39,260
play a lot of tricks to to reduce that

00:41:36,560 --> 00:41:42,460
duplication for example and this is this

00:41:39,260 --> 00:41:44,930
is a common use case where people launch

00:41:42,460 --> 00:41:47,119
replicas of their pots and end up

00:41:44,930 --> 00:41:49,970
running a bunch of containers that are

00:41:47,119 --> 00:41:52,280
almost identical and share a lot of

00:41:49,970 --> 00:41:54,740
memory consumption patterns so you can

00:41:52,280 --> 00:41:56,420
do many tricks with with your hypervisor

00:41:54,740 --> 00:41:59,390
to reduce that that memory consumption

00:41:56,420 --> 00:42:00,980
to the point where the bigger the

00:41:59,390 --> 00:42:04,549
container is in terms of memory

00:42:00,980 --> 00:42:06,680
consumption the the more interesting it

00:42:04,549 --> 00:42:11,119
is to run it in in virtual machines so

00:42:06,680 --> 00:42:13,099
the offsets and the delta decreases with

00:42:11,119 --> 00:42:16,520
with bigger containers so it's not a

00:42:13,099 --> 00:42:18,319
black-and-white answer and it's not it's

00:42:16,520 --> 00:42:22,000
not true that containers always consume

00:42:18,319 --> 00:42:22,000
less memory and and then virtual machine

00:42:29,130 --> 00:42:33,820
regarding that I'm I'm not sure that's

00:42:31,510 --> 00:42:35,320
true because like ASM for example

00:42:33,820 --> 00:42:37,570
basically what it does it just takes

00:42:35,320 --> 00:42:40,210
pages inside the house doesn't matter if

00:42:37,570 --> 00:42:42,250
it comes from containers or VMs and do a

00:42:40,210 --> 00:42:46,710
copy and write on them and and therefore

00:42:42,250 --> 00:42:46,710
you have the exact same memory reduction

00:42:53,820 --> 00:42:59,890
and you have to make the community in

00:42:58,150 --> 00:43:01,570
the container community understand that

00:42:59,890 --> 00:43:04,750
they could benefit from dating from that

00:43:01,570 --> 00:43:07,720
as well the container memory is not

00:43:04,750 --> 00:43:09,850
marked as merge about by anything it is

00:43:07,720 --> 00:43:12,180
technically possible yes but it's it's a

00:43:09,850 --> 00:43:14,140
it's a completely different technical ok

00:43:12,180 --> 00:43:15,610
thank you very much for those insights

00:43:14,140 --> 00:43:18,670
interview discussion is there anything

00:43:15,610 --> 00:43:20,870
which is different then thank you very

00:43:18,670 --> 00:43:23,960
much for your time and enjoy just a

00:43:20,870 --> 00:43:29,820
[Applause]

00:43:23,960 --> 00:43:29,820

YouTube URL: https://www.youtube.com/watch?v=F6GDh_iv3ow


