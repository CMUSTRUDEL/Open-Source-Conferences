Title: [2014] Hardware Accelerated VirtIO Networking for NFV by Varun Sethi
Publication date: 2014-10-21
Playlist: KVM Forum 2014
Description: 
	Network function virtualization (NFV) leverage's virtualization to consolidate multiple network functions on to a standard platform.The network functions are decoupled from hardware, allowing them to run in software as virtual machines. Network I/O with in these virtual machine needs to be both fast and flexible. Virtio networking offers a flexible mechanism for sharing I/O interface among virtual machines. Despite the flexibility, there are performance challenges with virtio/ vhost-net for virtual network functions. For high traffic rates efficient packet classification and distribution is required with virtio for VMs. This can be achieved using hardware offloads. In this presentation we discuss the performance issues encountered and how these were mitigated with hardware offloads while running IP forwarding as a virtual network function under KVM hypervisor with Vhost-net networking.


Varun Sethi, Freescale Semiconductor
Varun Sethi is a Software Architect at Freescale Semiconductor and has been involved in virtualization software development for embedded Power Architecture SOCs. He has contributed to the KVM port for BookE.HV platforms and the e500mc core. He is the maintainer for Freescale PAMU (IOMMU) driver. He's working on enhancements for the ARM SMMU driver to support Freescale Layerscape platforms. Varun is also looking at Virtio performance analysis and optimizations. Varun has made presentations on KVM hypervisor at FTF and KVM Forum (2012). He has also conducted talks, on dependability in case of virtualized platforms at the IEEE DSN conference. He has a joint publication on KVM optimizations for BookE platforms without hardware virtualization assists, at ASPLOS 2013. He has conducted a joint talk on network virtualization at the Linux foundation collaboration summit held in 2013.


Slides: http://events.linuxfoundation.org/sites/events/files/slides/Hardware%20Accelerated%20Virtio%20networking%20for%20NFV_KVM_FORUM.pdf
Captions: 
	00:00:02,750 --> 00:00:10,469
okay good huh grab me or everyone we'll

00:00:06,330 --> 00:00:12,420
just start with the session so welcome

00:00:10,469 --> 00:00:14,730
to the session in this session we are

00:00:12,420 --> 00:00:17,850
going to talk about hardware-accelerated

00:00:14,730 --> 00:00:20,850
what IO for network function

00:00:17,850 --> 00:00:22,980
virtualization so I believe many of us

00:00:20,850 --> 00:00:24,480
must have attended D session prior to

00:00:22,980 --> 00:00:26,130
lunch where we talked about performance

00:00:24,480 --> 00:00:27,779
issues with network function

00:00:26,130 --> 00:00:30,570
virtualization and there were certain

00:00:27,779 --> 00:00:32,820
suggestions of for improving the

00:00:30,570 --> 00:00:35,700
performance so we are also kind of

00:00:32,820 --> 00:00:37,980
looking at that and we I mean we also

00:00:35,700 --> 00:00:39,270
have a proposed architecture through

00:00:37,980 --> 00:00:41,450
which we want to improve the performance

00:00:39,270 --> 00:00:45,510
for network function virtualization

00:00:41,450 --> 00:00:47,430
using what I Oh so we'd be talking about

00:00:45,510 --> 00:00:49,230
like we'll be looking at the performance

00:00:47,430 --> 00:00:51,809
issues that are related to what IO when

00:00:49,230 --> 00:00:54,629
we use network function virtualization

00:00:51,809 --> 00:00:57,199
and like we will see how we can actually

00:00:54,629 --> 00:01:00,989
accelerate it using the hardware

00:00:57,199 --> 00:01:03,600
hardware features so today like me and

00:01:00,989 --> 00:01:05,010
my colleague so I am Varun city and my

00:01:03,600 --> 00:01:09,420
colleague a small data like we'd be

00:01:05,010 --> 00:01:11,100
conducting this session so following

00:01:09,420 --> 00:01:14,040
would be it would be the agenda we

00:01:11,100 --> 00:01:15,570
briefly look at what is network function

00:01:14,040 --> 00:01:17,310
virtualization I think that this was

00:01:15,570 --> 00:01:18,720
adequately covered in the previous

00:01:17,310 --> 00:01:21,030
session but still I'll just touch upon

00:01:18,720 --> 00:01:23,970
it briefly and we'll just look at

00:01:21,030 --> 00:01:27,619
certain deployment scenarios after that

00:01:23,970 --> 00:01:31,009
we look at Iowa chelation mechanisms

00:01:27,619 --> 00:01:31,009
thank you so much

00:01:38,370 --> 00:01:42,910
so we started by looking at briefly

00:01:41,680 --> 00:01:45,550
talked about network function

00:01:42,910 --> 00:01:47,740
virtualization after that we look at i/o

00:01:45,550 --> 00:01:49,780
virtualization mechanisms and after that

00:01:47,740 --> 00:01:52,540
we would be actually touching upon V

00:01:49,780 --> 00:01:54,640
host net this is the this is the

00:01:52,540 --> 00:01:56,440
mechanism that we used in our particular

00:01:54,640 --> 00:01:59,950
test case where we were we ran an IP

00:01:56,440 --> 00:02:03,550
forwarding bench benchmark as a virtual

00:01:59,950 --> 00:02:05,110
machine and we look at what kind of

00:02:03,550 --> 00:02:07,470
issues we faced and what was the

00:02:05,110 --> 00:02:09,850
workaround that we used for solving

00:02:07,470 --> 00:02:11,260
functional issue and plus also trying to

00:02:09,850 --> 00:02:13,210
improve improve the performance for that

00:02:11,260 --> 00:02:15,550
benchmark and then finally we touch upon

00:02:13,210 --> 00:02:17,170
an architecture that we would like to

00:02:15,550 --> 00:02:19,300
propose to improve this performance

00:02:17,170 --> 00:02:21,750
which is based on using hardware

00:02:19,300 --> 00:02:21,750
acceleration

00:02:23,820 --> 00:02:29,080
okay so typically network function

00:02:27,520 --> 00:02:30,880
virtualization means that you're

00:02:29,080 --> 00:02:33,490
actually running various different

00:02:30,880 --> 00:02:36,040
network functions as virtual machines on

00:02:33,490 --> 00:02:37,540
top of a hypervisor so what that allows

00:02:36,040 --> 00:02:41,440
you to do is that it allows you to kind

00:02:37,540 --> 00:02:43,390
of provision services dynamically if it

00:02:41,440 --> 00:02:45,190
would be with a physical network

00:02:43,390 --> 00:02:47,590
infrastructure it's this gonna be very

00:02:45,190 --> 00:02:50,140
costly because to spawn a new service

00:02:47,590 --> 00:02:52,150
you would require new new hardware that

00:02:50,140 --> 00:02:54,340
increases cost and plus also is going to

00:02:52,150 --> 00:02:56,920
add to the power consumption with this

00:02:54,340 --> 00:02:59,500
you can use the adequate I mean you can

00:02:56,920 --> 00:03:01,660
actually use the existing hardware to

00:02:59,500 --> 00:03:04,000
actually spawn the new functionality you

00:03:01,660 --> 00:03:05,650
and all these network functions are

00:03:04,000 --> 00:03:08,410
actually managed by the hypervisor and

00:03:05,650 --> 00:03:10,060
this also offers agility and flexibility

00:03:08,410 --> 00:03:11,230
based on the load that you have to

00:03:10,060 --> 00:03:12,700
handle based on the number of customers

00:03:11,230 --> 00:03:15,850
that you are having you can actually

00:03:12,700 --> 00:03:17,530
scale up and scale down or spawn new

00:03:15,850 --> 00:03:19,269
services or kind of get rid of old

00:03:17,530 --> 00:03:20,590
services so that those are the kind of

00:03:19,269 --> 00:03:23,830
benefits that you get out of network

00:03:20,590 --> 00:03:25,690
function virtualization so there could

00:03:23,830 --> 00:03:28,330
be two possible deployment scenarios for

00:03:25,690 --> 00:03:30,910
network function virtualization one

00:03:28,330 --> 00:03:32,860
could be also like in this case what I

00:03:30,910 --> 00:03:35,890
have taken I have taken a server which

00:03:32,860 --> 00:03:39,269
is running multiple say server VMs in

00:03:35,890 --> 00:03:43,209
one case we are having network appliance

00:03:39,269 --> 00:03:45,850
which is capable of spawning network for

00:03:43,209 --> 00:03:49,170
virtual network functions using the

00:03:45,850 --> 00:03:51,660
hypervisor so this network

00:03:49,170 --> 00:03:53,040
Alliance is actually handling so it has

00:03:51,660 --> 00:03:55,440
multiple virtual machines which are

00:03:53,040 --> 00:03:56,720
catering to the virtual machines

00:03:55,440 --> 00:03:59,670
actually which are running on the server

00:03:56,720 --> 00:04:00,900
in the second case we have the network

00:03:59,670 --> 00:04:03,330
function which is actually running on

00:04:00,900 --> 00:04:06,750
top of the server itself which is again

00:04:03,330 --> 00:04:09,330
which is actually providing services to

00:04:06,750 --> 00:04:11,430
a server VM so I think this was kind of

00:04:09,330 --> 00:04:13,620
discussed in the previous presentation

00:04:11,430 --> 00:04:16,230
where we talked about the inter vm

00:04:13,620 --> 00:04:17,310
communication but this particular the

00:04:16,230 --> 00:04:19,620
scenario where you have a network

00:04:17,310 --> 00:04:21,660
appliance this is kind of also I mean

00:04:19,620 --> 00:04:23,940
this was also something which we need to

00:04:21,660 --> 00:04:25,560
look at because this is something which

00:04:23,940 --> 00:04:27,030
is going to be there in case of

00:04:25,560 --> 00:04:29,250
application delivery controllers which

00:04:27,030 --> 00:04:31,440
are actually front rating servers and a

00:04:29,250 --> 00:04:33,300
use for DCP connection termination and

00:04:31,440 --> 00:04:34,800
also a social connection termination so

00:04:33,300 --> 00:04:36,540
effectively how the packet flow is going

00:04:34,800 --> 00:04:37,919
to be that you get a packet from the

00:04:36,540 --> 00:04:40,530
network it goes to a corresponding

00:04:37,919 --> 00:04:43,350
network function which is running as a

00:04:40,530 --> 00:04:45,780
VM so the net function processes the

00:04:43,350 --> 00:04:47,190
packet after processing the packet had

00:04:45,780 --> 00:04:48,780
forwarded to the virtual machine and

00:04:47,190 --> 00:04:51,389
when the virtual machine has to send the

00:04:48,780 --> 00:04:53,220
packet it goes to the network function

00:04:51,389 --> 00:04:55,229
on the network appliance it would

00:04:53,220 --> 00:04:56,729
process it and send the packet off to

00:04:55,229 --> 00:04:58,320
the network and similarly for the case

00:04:56,729 --> 00:05:01,140
where the network function is running

00:04:58,320 --> 00:05:03,000
alongside the servers you get the packet

00:05:01,140 --> 00:05:06,030
in the network function which forwarded

00:05:03,000 --> 00:05:12,590
to the VM and subsequently from VM to

00:05:06,030 --> 00:05:12,590
network function then off to the network

00:05:12,830 --> 00:05:21,000
okay so in this like one thing to notice

00:05:17,820 --> 00:05:23,070
is that we are using a shared NIC both

00:05:21,000 --> 00:05:24,150
for I mean in the servers as well as the

00:05:23,070 --> 00:05:26,910
network appliance that we are talking

00:05:24,150 --> 00:05:29,100
about and so like the thing is that

00:05:26,910 --> 00:05:30,750
virtualization is required we need to

00:05:29,100 --> 00:05:32,100
virtualize the snake so that it can be

00:05:30,750 --> 00:05:35,370
shared across multiple virtual machines

00:05:32,100 --> 00:05:37,830
so certainly we I think we all would be

00:05:35,370 --> 00:05:39,390
aware that there are mechanisms like a

00:05:37,830 --> 00:05:41,100
fully virtualized mechanism or they

00:05:39,390 --> 00:05:42,570
could be a what I always from Canon

00:05:41,100 --> 00:05:44,729
through which you can do it and vert IO

00:05:42,570 --> 00:05:46,620
being a bit more efficient because you

00:05:44,729 --> 00:05:50,160
have a front end and back end drivers

00:05:46,620 --> 00:05:51,990
through which the the guest networking

00:05:50,160 --> 00:05:54,200
driver or network interface driver can

00:05:51,990 --> 00:05:57,180
interact with the host driver right and

00:05:54,200 --> 00:05:58,540
making a packet transmission more

00:05:57,180 --> 00:06:01,210
efficient

00:05:58,540 --> 00:06:02,980
and in case of what I own the

00:06:01,210 --> 00:06:04,450
communication between the front end and

00:06:02,980 --> 00:06:07,810
the back end driver happens using word

00:06:04,450 --> 00:06:09,370
cues so there are multiple search

00:06:07,810 --> 00:06:10,900
interfaces that can be supported with

00:06:09,370 --> 00:06:12,700
what I owe you have the block the

00:06:10,900 --> 00:06:22,450
networking and the transport mechanism

00:06:12,700 --> 00:06:24,700
is basically what I OPC a okay so like

00:06:22,450 --> 00:06:26,410
in this this particular slide we

00:06:24,700 --> 00:06:29,380
actually show how what I out networking

00:06:26,410 --> 00:06:32,650
is going to work so in case of Khemu or

00:06:29,380 --> 00:06:34,840
virtual machine the emulation for the

00:06:32,650 --> 00:06:36,400
for the for the what I unique is

00:06:34,840 --> 00:06:39,610
happening in became mu user space

00:06:36,400 --> 00:06:41,740
process right so the packet goes from

00:06:39,610 --> 00:06:43,360
the the guest OS to Khemu and then it is

00:06:41,740 --> 00:06:45,220
forwarded to the turn tap interface in

00:06:43,360 --> 00:06:46,600
the host from where it goes to the

00:06:45,220 --> 00:06:52,360
bridge and finally to the physical

00:06:46,600 --> 00:06:55,510
Ethernet device and to the to the

00:06:52,360 --> 00:06:57,700
network from their own so as like as I

00:06:55,510 --> 00:06:59,110
just mentioned the emulation or the

00:06:57,700 --> 00:07:01,300
backend driver is actually sitting in

00:06:59,110 --> 00:07:02,770
camera over here it's bit inefficient

00:07:01,300 --> 00:07:05,110
because you have to kind of switch to

00:07:02,770 --> 00:07:07,390
Khemu every time you need to transfer

00:07:05,110 --> 00:07:10,630
the packet so you have to switch from

00:07:07,390 --> 00:07:16,500
the guest state to the the host user

00:07:10,630 --> 00:07:16,500
space for this particular processing

00:07:16,710 --> 00:07:23,170
with v hostnet we avoid the overhead of

00:07:19,900 --> 00:07:24,910
cable processing by emulating the

00:07:23,170 --> 00:07:26,980
networking functionality through V

00:07:24,910 --> 00:07:28,420
hostnet running in the host kernel right

00:07:26,980 --> 00:07:31,180
so over here as we can see here

00:07:28,420 --> 00:07:33,460
bypassing Khemu and the the packet

00:07:31,180 --> 00:07:37,330
transfer is happening directly from the

00:07:33,460 --> 00:07:40,480
guest OS to a via host we host thread

00:07:37,330 --> 00:07:42,910
which is there in the host kernel so

00:07:40,480 --> 00:07:45,310
this is more efficient than the what I

00:07:42,910 --> 00:07:52,180
of networking being emulated in came as

00:07:45,310 --> 00:07:55,750
such okay bit more details on how we

00:07:52,180 --> 00:07:58,480
host functions so in in case of we host

00:07:55,750 --> 00:08:01,630
you have a we host thread that gets

00:07:58,480 --> 00:08:03,460
created which is running in the which is

00:08:01,630 --> 00:08:07,030
which is running in the host kernel

00:08:03,460 --> 00:08:09,490
right and this servicing so each v-neck

00:08:07,030 --> 00:08:10,510
is have virtual NIC what I mean ik is

00:08:09,490 --> 00:08:13,060
having an Eric

00:08:10,510 --> 00:08:16,090
humanity X cube so this vigoss thread is

00:08:13,060 --> 00:08:18,400
handling both the iring stops and the TX

00:08:16,090 --> 00:08:23,530
jobs on behalf on behalf of the guest

00:08:18,400 --> 00:08:25,450
Vinick right and after say after

00:08:23,530 --> 00:08:28,420
processing packets or receiving packets

00:08:25,450 --> 00:08:30,250
from the gas the the B host read is

00:08:28,420 --> 00:08:32,350
gonna transfer send them across the tap

00:08:30,250 --> 00:08:34,390
socket from where it gets forwarded to

00:08:32,350 --> 00:08:37,720
the bridge and do D Ethernet and in case

00:08:34,390 --> 00:08:39,160
of the RX RX case you the packet is

00:08:37,720 --> 00:08:39,850
received by the ethernet driver whose

00:08:39,160 --> 00:08:42,040
ethernet driver

00:08:39,850 --> 00:08:43,750
it's forwarded to the bridge then it

00:08:42,040 --> 00:08:47,830
goes to the tap socket then it goes to

00:08:43,750 --> 00:08:50,260
the vs red then finally to the v neck

00:08:47,830 --> 00:08:58,630
the virtual what ionic sitting in the

00:08:50,260 --> 00:09:00,580
guest so how like V the V goes thread

00:08:58,630 --> 00:09:01,960
that gets created is it's actually

00:09:00,580 --> 00:09:04,750
created through an i/o control interface

00:09:01,960 --> 00:09:06,490
through K mu K mu invokes and I have

00:09:04,750 --> 00:09:09,220
control to actually create this we host

00:09:06,490 --> 00:09:10,990
thread in the host right and as I just

00:09:09,220 --> 00:09:13,270
mentioned that we host thread is going

00:09:10,990 --> 00:09:15,850
to handle both the TX and are the arc

00:09:13,270 --> 00:09:25,510
shops on behalf of the what ionic

00:09:15,850 --> 00:09:29,020
guest:what ionic okay so in our in our

00:09:25,510 --> 00:09:31,960
set up we actually saw an issue with the

00:09:29,020 --> 00:09:36,190
RX processing specifically we saw that

00:09:31,960 --> 00:09:38,080
rx are the the V hostnet rx processing

00:09:36,190 --> 00:09:40,840
actually becomes a bottling so I'll just

00:09:38,080 --> 00:09:44,350
briefly touch upon how the rx processing

00:09:40,840 --> 00:09:47,740
works in case of V hostnet and then we

00:09:44,350 --> 00:09:49,300
would actually also talk about the the

00:09:47,740 --> 00:09:53,770
the workarounds and the proposed

00:09:49,300 --> 00:09:57,190
architecture to handle these issues so

00:09:53,770 --> 00:09:59,800
this is a complete kind of a blow-up of

00:09:57,190 --> 00:10:02,410
how the rx packet processing would

00:09:59,800 --> 00:10:05,380
happen in case of V host on it so

00:10:02,410 --> 00:10:07,030
initially make me have the the physical

00:10:05,380 --> 00:10:08,080
Nick which is receiving the packet so we

00:10:07,030 --> 00:10:09,760
have the host Trevor

00:10:08,080 --> 00:10:12,190
proteins that get called and then you

00:10:09,760 --> 00:10:14,580
have the nappy processing which happens

00:10:12,190 --> 00:10:18,100
in based on behalf of the host driver

00:10:14,580 --> 00:10:19,870
right and so this the entire part this

00:10:18,100 --> 00:10:21,459
is marked in green like with the green

00:10:19,870 --> 00:10:23,439
arrows is actually happening in

00:10:21,459 --> 00:10:25,449
the for the interrupt hardware

00:10:23,439 --> 00:10:26,769
interrupts and then after the how do I

00:10:25,449 --> 00:10:28,869
interrupt we have the software cue

00:10:26,769 --> 00:10:31,569
processing that is happening right so

00:10:28,869 --> 00:10:33,579
you have the whole routines for the

00:10:31,569 --> 00:10:37,059
driver little driver that get called and

00:10:33,579 --> 00:10:41,199
then finally once the packet is received

00:10:37,059 --> 00:10:43,269
by the driver now it has to pack it like

00:10:41,199 --> 00:10:45,990
and it has to forward it to the guest

00:10:43,269 --> 00:10:47,499
Winnick so first it has to send it to

00:10:45,990 --> 00:10:50,439
the bridge

00:10:47,499 --> 00:10:53,139
so again if you see like in the previous

00:10:50,439 --> 00:10:54,819
slide we were we were still in the the

00:10:53,139 --> 00:10:57,819
host driver routines which are being

00:10:54,819 --> 00:10:59,889
invoked in the as a part of the software

00:10:57,819 --> 00:11:01,839
key processing and after that when the

00:10:59,889 --> 00:11:04,149
when the driver has to forward it to the

00:11:01,839 --> 00:11:06,850
bridge again we are we are still in the

00:11:04,149 --> 00:11:08,350
software deep processing if you like

00:11:06,850 --> 00:11:11,980
this is the entire bridge processing

00:11:08,350 --> 00:11:14,709
that is happening and after that when

00:11:11,980 --> 00:11:16,420
the bridge has actually been able to

00:11:14,709 --> 00:11:17,889
process the packet and it identifies

00:11:16,420 --> 00:11:21,819
that it needs to be forwarded to a

00:11:17,889 --> 00:11:24,009
particular guest tap socket that again

00:11:21,819 --> 00:11:26,529
happens in that particular software

00:11:24,009 --> 00:11:28,839
processing itself so once this process

00:11:26,529 --> 00:11:30,939
once the packet is forwarded to the tap

00:11:28,839 --> 00:11:33,639
socket tap socket has to inform the

00:11:30,939 --> 00:11:35,740
vhosts thread that there is an Eric's

00:11:33,639 --> 00:11:39,540
job for it to process so it would

00:11:35,740 --> 00:11:43,679
actually wake up take the vhosts thread

00:11:39,540 --> 00:11:46,259
so once the vhosts thread is woken up so

00:11:43,679 --> 00:11:49,029
there are I mean there are few checks

00:11:46,259 --> 00:11:51,220
for the vhosts thread so finally it will

00:11:49,029 --> 00:11:53,949
it will actually see that there is some

00:11:51,220 --> 00:11:55,990
job for it in the work queue right so

00:11:53,949 --> 00:11:58,269
once if if there is a job which is

00:11:55,990 --> 00:12:00,249
available it would DQ the job and then

00:11:58,269 --> 00:12:03,839
DRX processing is going to start in the

00:12:00,249 --> 00:12:03,839
V host net driver right

00:12:08,529 --> 00:12:14,209
so this is the entire like processing

00:12:11,509 --> 00:12:17,689
that happens in the for the RX in the V

00:12:14,209 --> 00:12:19,279
host net driver so we host so this is

00:12:17,689 --> 00:12:21,920
handle Rx is the main routine that gets

00:12:19,279 --> 00:12:24,589
called so in the handle Eric's routine

00:12:21,920 --> 00:12:26,449
it actually checks D so we are dealing

00:12:24,589 --> 00:12:27,980
with the tap socket here it will see

00:12:26,449 --> 00:12:31,189
whether there is data available in the

00:12:27,980 --> 00:12:32,869
socket if yes then the entire processing

00:12:31,189 --> 00:12:35,660
is going to happen over here and then we

00:12:32,869 --> 00:12:38,989
are actually going to copy the data from

00:12:35,660 --> 00:12:41,540
the tap socket to the word queue what

00:12:38,989 --> 00:12:43,759
Hugh has the the buffer descriptors in

00:12:41,540 --> 00:12:46,220
queued by the guest where we need to

00:12:43,759 --> 00:12:47,720
copy the data so this data needs to be

00:12:46,220 --> 00:12:51,019
copied over there

00:12:47,720 --> 00:12:52,819
so once the once the data is copied so

00:12:51,019 --> 00:12:55,339
there is a check over here again which

00:12:52,819 --> 00:12:57,589
where we see like if we have handled so

00:12:55,339 --> 00:13:00,079
there is a specific budget defined over

00:12:57,589 --> 00:13:02,179
here so why do we why do we have this

00:13:00,079 --> 00:13:04,040
the thing is that the same vhosts thread

00:13:02,179 --> 00:13:07,970
is supposed to process the RX in the TX

00:13:04,040 --> 00:13:09,980
jobs so in case there are like we we

00:13:07,970 --> 00:13:11,509
have actually processed a significant

00:13:09,980 --> 00:13:13,309
number of jobs we break out from here

00:13:11,509 --> 00:13:15,470
and see if there is another work which

00:13:13,309 --> 00:13:18,499
has been include for us it could be a it

00:13:15,470 --> 00:13:20,660
could be a TX job also right so if we do

00:13:18,499 --> 00:13:22,610
not do this then we I mean if there is

00:13:20,660 --> 00:13:24,019
there are lot many packets for example

00:13:22,610 --> 00:13:26,329
coming in then will be kind of stuck

00:13:24,019 --> 00:13:29,149
stuck up here will never break out from

00:13:26,329 --> 00:13:31,100
here and go for the next work item which

00:13:29,149 --> 00:13:33,679
has been queued so this is check ensure

00:13:31,100 --> 00:13:35,239
that we after processing a specific

00:13:33,679 --> 00:13:42,439
number of packets we break out from here

00:13:35,239 --> 00:13:45,339
and we move to the next processing okay

00:13:42,439 --> 00:13:48,169
so till now we just touched upon the

00:13:45,339 --> 00:13:50,209
details of like a slight detail the

00:13:48,169 --> 00:13:51,829
water and then we saw like how we host

00:13:50,209 --> 00:13:53,869
networks and specifically they are

00:13:51,829 --> 00:13:57,199
excited processing so because that's

00:13:53,869 --> 00:13:59,629
what is I mean during our benchmarking

00:13:57,199 --> 00:14:01,759
we saw that that becames that big that

00:13:59,629 --> 00:14:05,480
actually became a major bottleneck right

00:14:01,759 --> 00:14:08,480
so we will see how what kind of issues

00:14:05,480 --> 00:14:10,850
we can encounter when we are running an

00:14:08,480 --> 00:14:14,089
NF weak network function virtualization

00:14:10,850 --> 00:14:16,129
use case with what i/o so again coming

00:14:14,089 --> 00:14:18,019
back to the deployment scenario I've

00:14:16,129 --> 00:14:20,629
taken the deployment scenario of our

00:14:18,019 --> 00:14:22,350
network appliance which is interfacing

00:14:20,629 --> 00:14:26,580
with the server viens

00:14:22,350 --> 00:14:31,680
great so we again so in this case I have

00:14:26,580 --> 00:14:33,840
this network function running a network

00:14:31,680 --> 00:14:36,090
function tree which is running on the

00:14:33,840 --> 00:14:40,920
network network appliance interfacing

00:14:36,090 --> 00:14:45,000
with the beam vm3 in the server so in in

00:14:40,920 --> 00:14:46,740
case of normal packet packet transfer so

00:14:45,000 --> 00:14:49,770
as we could see that packet comes in and

00:14:46,740 --> 00:14:51,060
goes to the server VM and again from the

00:14:49,770 --> 00:14:52,950
server VM and goes to the function

00:14:51,060 --> 00:14:57,000
Network function and to the network but

00:14:52,950 --> 00:14:59,670
if there is a packet flooding right now

00:14:57,000 --> 00:15:03,510
suppose because this this interface is

00:14:59,670 --> 00:15:05,010
shared across multiple VMs now there is

00:15:03,510 --> 00:15:08,040
a case where I am getting multiple

00:15:05,010 --> 00:15:10,380
packets for this VM right and there is

00:15:08,040 --> 00:15:12,060
packet flood host is going to keep on

00:15:10,380 --> 00:15:14,790
receiving the packet and it will try to

00:15:12,060 --> 00:15:16,350
forward it to the VM but say this VM is

00:15:14,790 --> 00:15:19,560
saturated it is not able to handle the

00:15:16,350 --> 00:15:20,730
packets right so the packets although

00:15:19,560 --> 00:15:22,680
are being received from the physical

00:15:20,730 --> 00:15:25,530
interface they are being forwarded to

00:15:22,680 --> 00:15:27,260
the VM but VM is the entire system is

00:15:25,530 --> 00:15:29,580
saturated VM is not getting a chance to

00:15:27,260 --> 00:15:32,400
consume those packets and maybe forward

00:15:29,580 --> 00:15:34,050
into that to the server VM so we are

00:15:32,400 --> 00:15:37,530
actually looking at two packets are

00:15:34,050 --> 00:15:40,590
being subsequently dropped here right so

00:15:37,530 --> 00:15:42,060
the system is not progressing packets

00:15:40,590 --> 00:15:44,070
are received but packets cannot be

00:15:42,060 --> 00:15:46,200
forwarded to the VM server VM

00:15:44,070 --> 00:15:47,730
so it's kind of a denial of service that

00:15:46,200 --> 00:15:52,440
happens and that's what we kind of

00:15:47,730 --> 00:15:55,950
encountered in our in our scenario so

00:15:52,440 --> 00:15:57,600
this is the test setup that we used so

00:15:55,950 --> 00:16:01,860
we have we have a guest

00:15:57,600 --> 00:16:04,860
k vm k vm guest running and so the guest

00:16:01,860 --> 00:16:07,650
had to what i/o interfaces which were

00:16:04,860 --> 00:16:10,470
serviced by - v host threads in the host

00:16:07,650 --> 00:16:12,660
and then each of these we host threads

00:16:10,470 --> 00:16:14,550
were actually connected to a tap socket

00:16:12,660 --> 00:16:16,350
and a bridge and there were two bridge

00:16:14,550 --> 00:16:17,850
instances each talk talking to a

00:16:16,350 --> 00:16:20,370
particular physical interfaces so there

00:16:17,850 --> 00:16:23,160
were two one g interfaces which were

00:16:20,370 --> 00:16:24,810
available on the system and we we had a

00:16:23,160 --> 00:16:28,580
bridge connected to each of these and

00:16:24,810 --> 00:16:32,820
and there was a separate tab socket for

00:16:28,580 --> 00:16:34,500
for each of the v host instances so

00:16:32,820 --> 00:16:36,060
essentially we were running an IP

00:16:34,500 --> 00:16:38,790
forwarding application here

00:16:36,060 --> 00:16:40,140
in the SMP guess so the IP forwarding

00:16:38,790 --> 00:16:42,060
application was supposed to receive

00:16:40,140 --> 00:16:43,470
packet from one particular interface and

00:16:42,060 --> 00:16:45,120
forwarded to the second interface and

00:16:43,470 --> 00:16:46,500
then subsequently from this particular

00:16:45,120 --> 00:16:48,510
interface it would receive packet and

00:16:46,500 --> 00:16:50,850
forwarded to this but the second and

00:16:48,510 --> 00:16:52,410
second interface so in this case we

00:16:50,850 --> 00:16:56,970
could actually see that at a high

00:16:52,410 --> 00:16:59,220
traffic rate the traffic and we okay we

00:16:56,970 --> 00:17:01,260
were having this the packet generator

00:16:59,220 --> 00:17:03,360
framework which was basically generating

00:17:01,260 --> 00:17:05,880
the packet so high traffic rate we could

00:17:03,360 --> 00:17:07,829
see that the RX rate on one particular

00:17:05,880 --> 00:17:10,020
port actually dropped to zero so we were

00:17:07,829 --> 00:17:13,980
not getting packets in one direction at

00:17:10,020 --> 00:17:16,350
all so it was so although one one

00:17:13,980 --> 00:17:19,020
particular port was receiving back at me

00:17:16,350 --> 00:17:21,360
the other one was not so on further

00:17:19,020 --> 00:17:23,400
analysis and for like lot of debugging

00:17:21,360 --> 00:17:26,010
we could actually figure out what was

00:17:23,400 --> 00:17:28,290
happening in the system so packet

00:17:26,010 --> 00:17:30,540
flooding or like the packet generator

00:17:28,290 --> 00:17:32,700
was sending a continuous stream of

00:17:30,540 --> 00:17:35,250
packets no packets were being received

00:17:32,700 --> 00:17:36,870
by the physical neck and then forwarded

00:17:35,250 --> 00:17:40,440
to the driver driver was warning to the

00:17:36,870 --> 00:17:43,710
bridge because the entire system was

00:17:40,440 --> 00:17:47,280
consumed in mostly in the soft irq

00:17:43,710 --> 00:17:49,530
processing the VM was not able to pick

00:17:47,280 --> 00:17:52,170
up packets because we host thread was

00:17:49,530 --> 00:17:53,880
not able to run it was not able to DQ DQ

00:17:52,170 --> 00:17:58,260
packets from the taps open and handed

00:17:53,880 --> 00:18:01,050
over to the the guest so the pack so

00:17:58,260 --> 00:18:03,060
would the so as a result the tap socket

00:18:01,050 --> 00:18:05,360
key was full and the packet skipped work

00:18:03,060 --> 00:18:07,560
was were actually dropped at this point

00:18:05,360 --> 00:18:09,930
packets were being dropped but whatever

00:18:07,560 --> 00:18:11,220
was then in the system would like the VM

00:18:09,930 --> 00:18:13,380
was not able to even process those

00:18:11,220 --> 00:18:14,790
packets which had been which which were

00:18:13,380 --> 00:18:16,970
which had been able to enter the system

00:18:14,790 --> 00:18:21,000
and were there in the tap socket queue

00:18:16,970 --> 00:18:22,560
so as a result one particular side was

00:18:21,000 --> 00:18:24,990
not able to function at all I mean

00:18:22,560 --> 00:18:27,330
although they in certain cases these

00:18:24,990 --> 00:18:29,160
other side was able to function but

00:18:27,330 --> 00:18:33,510
still the the overall throughput was

00:18:29,160 --> 00:18:35,490
quite low so one for one chord you had

00:18:33,510 --> 00:18:36,660
some some throughput for the other port

00:18:35,490 --> 00:18:40,220
it was completely dead you are not

00:18:36,660 --> 00:18:40,220
receiving any packets with it

00:18:43,070 --> 00:18:49,380
okay so we try to work around to kind of

00:18:47,220 --> 00:18:52,440
get get around this problem so it was

00:18:49,380 --> 00:18:56,070
not a neat solution but this is what we

00:18:52,440 --> 00:18:59,370
tried so as a part of the the software

00:18:56,070 --> 00:19:00,690
key processing we in the tap it was

00:18:59,370 --> 00:19:04,860
France this was primarily done at the

00:19:00,690 --> 00:19:06,300
top circuit level we actually could we

00:19:04,860 --> 00:19:08,820
would check for condition so condition

00:19:06,300 --> 00:19:12,270
was checked by the queue depth the tap

00:19:08,820 --> 00:19:14,580
socket so the tap stop it was full right

00:19:12,270 --> 00:19:15,930
in that case we simply disable the

00:19:14,580 --> 00:19:18,240
hardware interrupt so that we do not

00:19:15,930 --> 00:19:19,500
receive any more packets and Olivia

00:19:18,240 --> 00:19:21,480
laude package should be dropped at the

00:19:19,500 --> 00:19:25,020
hardware level itself for this

00:19:21,480 --> 00:19:26,880
particular what I own to face and if

00:19:25,020 --> 00:19:30,120
there was no congestion then yeah we

00:19:26,880 --> 00:19:32,690
continue to send packet forward and in

00:19:30,120 --> 00:19:35,640
the V host processing we would actually

00:19:32,690 --> 00:19:37,140
when we are actually receiving we were

00:19:35,640 --> 00:19:40,320
able to receive the package from the tap

00:19:37,140 --> 00:19:43,080
socket we were we were actually enabling

00:19:40,320 --> 00:19:45,710
the hardware interrupt say when when we

00:19:43,080 --> 00:19:48,690
had subsequent buffers to receive the

00:19:45,710 --> 00:19:50,400
packets or like when the V host threaded

00:19:48,690 --> 00:19:51,750
had was able to actually pick up the

00:19:50,400 --> 00:19:54,810
package from the tap socket didn't tap

00:19:51,750 --> 00:19:58,320
socket had buffer like tap socket had

00:19:54,810 --> 00:19:59,610
enough buffers to receive so actually

00:19:58,320 --> 00:20:01,380
tap socket had space to receive more

00:19:59,610 --> 00:20:03,270
packets so we would enable the

00:20:01,380 --> 00:20:05,160
interrupts and would allow the hardware

00:20:03,270 --> 00:20:14,550
to kind of pump in more packets into the

00:20:05,160 --> 00:20:16,650
OS so with this like we were able to

00:20:14,550 --> 00:20:19,860
avoid the situation that we had hit upon

00:20:16,650 --> 00:20:21,780
earlier where we had a complete denial

00:20:19,860 --> 00:20:23,580
of service because the tap socket queue

00:20:21,780 --> 00:20:26,220
was full and we had to drop packets over

00:20:23,580 --> 00:20:29,010
there so in so even when the tap socket

00:20:26,220 --> 00:20:31,050
so now what we had was a feedback

00:20:29,010 --> 00:20:33,060
mechanism which we kind of implemented

00:20:31,050 --> 00:20:35,340
from the tap socket to the physical lake

00:20:33,060 --> 00:20:37,380
right so when the tap socket could not

00:20:35,340 --> 00:20:39,540
receive packet it would it would allow

00:20:37,380 --> 00:20:41,850
it would actually inform the neck to not

00:20:39,540 --> 00:20:43,200
receive any packets on its behalf it

00:20:41,850 --> 00:20:45,870
would disable interrupts over there

00:20:43,200 --> 00:20:47,850
right and then subsequently this would

00:20:45,870 --> 00:20:49,590
give the the V hose thread adequate

00:20:47,850 --> 00:20:51,630
bandwidth to process the packets from

00:20:49,590 --> 00:20:54,090
the tap socket and then move it to the

00:20:51,630 --> 00:20:55,740
guest right so once the

00:20:54,090 --> 00:20:58,169
once that had been done we would

00:20:55,740 --> 00:20:59,370
actually disable the interview reenable

00:20:58,169 --> 00:21:07,470
the interrupts in the physical neck to

00:20:59,370 --> 00:21:10,380
get more package from the network okay

00:21:07,470 --> 00:21:13,470
so in this case as we can see in being

00:21:10,380 --> 00:21:16,370
unmodified in the unmodified case or

00:21:13,470 --> 00:21:19,080
existing design at a particular ingress

00:21:16,370 --> 00:21:23,130
ingress rate the egress rate would

00:21:19,080 --> 00:21:26,520
actually subsequently decline right so

00:21:23,130 --> 00:21:29,010
it would it would it would decline at a

00:21:26,520 --> 00:21:31,620
very high like when the ingress rate was

00:21:29,010 --> 00:21:33,210
very high so initially the egress was

00:21:31,620 --> 00:21:35,370
there but then subsequently the

00:21:33,210 --> 00:21:37,470
aggressor 8-shot started declining but

00:21:35,370 --> 00:21:39,899
with the the workaround that we

00:21:37,470 --> 00:21:42,779
implemented right even when the ingress

00:21:39,899 --> 00:21:44,460
rate was high egress rate would keep on

00:21:42,779 --> 00:21:46,529
progressing right so it was not a

00:21:44,460 --> 00:21:47,789
perfect solution right but it was a hack

00:21:46,529 --> 00:21:49,649
but we were able to avoid that

00:21:47,789 --> 00:21:51,990
saturation the system saturation where

00:21:49,649 --> 00:21:56,390
we were we were actually not able to

00:21:51,990 --> 00:21:56,390
proceed at all so that was not the case

00:21:58,640 --> 00:22:04,140
okay so the following slides would

00:22:02,279 --> 00:22:07,110
actually talk about the proposal or

00:22:04,140 --> 00:22:08,909
mechanism to do to implement what we did

00:22:07,110 --> 00:22:10,950
as a workaround in a much more much more

00:22:08,909 --> 00:22:13,649
better fashion and the idea is to kind

00:22:10,950 --> 00:22:15,299
of make this generic for the for most of

00:22:13,649 --> 00:22:18,510
the hardware platforms so although we

00:22:15,299 --> 00:22:20,159
tried I mean we are proposing this in

00:22:18,510 --> 00:22:21,750
accordance to the hardware features that

00:22:20,159 --> 00:22:23,490
are available on our platform but the

00:22:21,750 --> 00:22:28,740
idea is to make it as a generic

00:22:23,490 --> 00:22:30,840
architecture and push it okay so so

00:22:28,740 --> 00:22:32,789
generally what we want to do here is

00:22:30,840 --> 00:22:35,279
that we want to kind of cut down the

00:22:32,789 --> 00:22:39,809
layers between the guest word queue and

00:22:35,279 --> 00:22:41,669
the physical nic cues right so we we

00:22:39,809 --> 00:22:44,250
want to kind of reduce the layers so

00:22:41,669 --> 00:22:47,029
that packet can kind of seamlessly move

00:22:44,250 --> 00:22:50,460
from the the physical Nick hue to the

00:22:47,029 --> 00:22:52,860
the guesswork you right so what we are

00:22:50,460 --> 00:22:55,470
proposing here is that when we host so

00:22:52,860 --> 00:22:57,659
we are actually proposing a new kind of

00:22:55,470 --> 00:23:00,200
a driver framework which is sitting

00:22:57,659 --> 00:23:03,260
right underneath the V host net

00:23:00,200 --> 00:23:06,929
architecture so V host net can actually

00:23:03,260 --> 00:23:07,900
register of our X function so what does

00:23:06,929 --> 00:23:09,550
our X function

00:23:07,900 --> 00:23:12,100
essentially allows us to do is that

00:23:09,550 --> 00:23:14,140
whenever our the the hardware

00:23:12,100 --> 00:23:15,580
Niki's or the new driver that we're

00:23:14,140 --> 00:23:18,630
proposing he receives a packet it can

00:23:15,580 --> 00:23:21,640
directly transfer the packet from the

00:23:18,630 --> 00:23:23,560
physical cues to the word cue which are

00:23:21,640 --> 00:23:27,460
maintained by or which are actually

00:23:23,560 --> 00:23:31,180
interface with the B hostnet and also in

00:23:27,460 --> 00:23:34,060
case the buffers available with the

00:23:31,180 --> 00:23:36,190
hardware hardware NIC for the particular

00:23:34,060 --> 00:23:38,680
guest Winnick are getting depleted it

00:23:36,190 --> 00:23:40,900
can send a notification saying that like

00:23:38,680 --> 00:23:43,630
I do not have buffers please replenish

00:23:40,900 --> 00:23:47,170
buffers and be in the buffer pool for

00:23:43,630 --> 00:23:50,410
that particular v-neck on the DXi again

00:23:47,170 --> 00:23:53,380
we want the V host thread to transfer

00:23:50,410 --> 00:23:55,210
the buffers directly from the V from the

00:23:53,380 --> 00:23:57,370
word queue to the physical frame queues

00:23:55,210 --> 00:23:59,200
right so that's where we resistor the TX

00:23:57,370 --> 00:24:00,900
function handle and then we have the

00:23:59,200 --> 00:24:03,040
buffer pool management handle where

00:24:00,900 --> 00:24:06,190
there is an interface through which you

00:24:03,040 --> 00:24:08,380
can actually get the buffers so whatever

00:24:06,190 --> 00:24:09,970
buffers are being seated by the guess in

00:24:08,380 --> 00:24:11,950
the word queue you put them into the

00:24:09,970 --> 00:24:14,380
buffer queues which are used by the

00:24:11,950 --> 00:24:23,140
physical NIC whenever a packet is being

00:24:14,380 --> 00:24:24,700
received okay so this is the new driver

00:24:23,140 --> 00:24:28,090
and interface driver that we are

00:24:24,700 --> 00:24:29,980
actually talking about and this is how

00:24:28,090 --> 00:24:32,740
the initialization initialization would

00:24:29,980 --> 00:24:34,810
happen so whenever this V host in its

00:24:32,740 --> 00:24:36,250
initialization happens it also registers

00:24:34,810 --> 00:24:38,530
with this particular network

00:24:36,250 --> 00:24:40,300
acceleration interface driver right so

00:24:38,530 --> 00:24:42,310
it would interface it would actually

00:24:40,300 --> 00:24:44,200
register the RX routine and it would

00:24:42,310 --> 00:24:46,750
also reduce register the congestion

00:24:44,200 --> 00:24:48,820
handler right which which can be used by

00:24:46,750 --> 00:24:51,630
the network acceleration interface

00:24:48,820 --> 00:24:53,980
driver to inform B hostnet and

00:24:51,630 --> 00:24:58,320
correspondingly to the guest in case of

00:24:53,980 --> 00:24:58,320
buffer depletion and say in case of rx

00:24:58,800 --> 00:25:03,670
in case of rx you can actually so these

00:25:01,690 --> 00:25:07,030
are the RX rings which are maintained by

00:25:03,670 --> 00:25:10,030
the host driver right so they be the

00:25:07,030 --> 00:25:11,110
Ethernet port has a pass passing

00:25:10,030 --> 00:25:12,940
classification distribution

00:25:11,110 --> 00:25:14,920
functionality which is available based

00:25:12,940 --> 00:25:16,920
on that it can actually when it receives

00:25:14,920 --> 00:25:20,080
the packet it can distribute the packet

00:25:16,920 --> 00:25:21,200
across these frame queues so you have

00:25:20,080 --> 00:25:23,659
the RX and the t

00:25:21,200 --> 00:25:24,799
incuse so whenever you are receiving a

00:25:23,659 --> 00:25:26,990
packet you can distribute it to

00:25:24,799 --> 00:25:28,909
particular or extreme Q and for the

00:25:26,990 --> 00:25:31,700
transmission side or like when the

00:25:28,909 --> 00:25:33,110
packet are copied packets are actually

00:25:31,700 --> 00:25:34,789
put into a particular T extreme queue

00:25:33,110 --> 00:25:36,080
they can be transmitted from a picked up

00:25:34,789 --> 00:25:41,630
by the ethernet poe driver and

00:25:36,080 --> 00:25:44,960
transmitted over the network right okay

00:25:41,630 --> 00:25:47,360
so this is this actually at this slide

00:25:44,960 --> 00:25:49,130
is showing that how the V hostnet driver

00:25:47,360 --> 00:25:57,409
is going to interface with the the new

00:25:49,130 --> 00:25:59,539
driver that we are proposing okay so

00:25:57,409 --> 00:26:01,789
this is the TX ID where when you get to

00:25:59,539 --> 00:26:03,769
see the packet you can actually transfer

00:26:01,789 --> 00:26:06,200
the packet directly from the TX frame Q

00:26:03,769 --> 00:26:09,649
to the TX ring which this was this would

00:26:06,200 --> 00:26:11,659
be done using the the takes a function

00:26:09,649 --> 00:26:13,370
handle which was registered by V host

00:26:11,659 --> 00:26:14,720
thread who was V hose thread is the one

00:26:13,370 --> 00:26:18,230
which is interfacing with the TX ring

00:26:14,720 --> 00:26:20,779
here which is used by the guess mean

00:26:18,230 --> 00:26:23,210
virtual ik right

00:26:20,779 --> 00:26:24,559
okay so now with the with the new

00:26:23,210 --> 00:26:27,139
architecture that we are proposing how

00:26:24,559 --> 00:26:29,659
would the rx handling work so your

00:26:27,139 --> 00:26:30,950
packet comes in so one thing one

00:26:29,659 --> 00:26:32,210
important thing that this buffer pool

00:26:30,950 --> 00:26:36,460
that we're talking about this is going

00:26:32,210 --> 00:26:39,950
to have buffers seeded by the guest and

00:26:36,460 --> 00:26:43,370
this the Ethernet port would be able to

00:26:39,950 --> 00:26:46,070
pass pass classify the packet and get

00:26:43,370 --> 00:26:49,399
the buffers corresponding to the guest

00:26:46,070 --> 00:26:53,480
we guess minik from this buffer pool so

00:26:49,399 --> 00:26:55,820
that is an important functionality so if

00:26:53,480 --> 00:26:57,710
like when whenever the Ethernet port

00:26:55,820 --> 00:26:59,230
receives the packet it will try to

00:26:57,710 --> 00:27:04,929
acquire a buffer from the buffer pool

00:26:59,230 --> 00:27:07,190
right if the buffer is not available

00:27:04,929 --> 00:27:10,760
then the packet will be dropped here

00:27:07,190 --> 00:27:12,620
itself and at this point it can the this

00:27:10,760 --> 00:27:14,690
network acceleration interface driver

00:27:12,620 --> 00:27:18,889
can actually inform the guests asking it

00:27:14,690 --> 00:27:20,450
to seed in buffers now make me see that

00:27:18,889 --> 00:27:22,519
the V hose thread is sitting in buffers

00:27:20,450 --> 00:27:26,510
the buffers are not there so another

00:27:22,519 --> 00:27:28,340
packet comes in to seize the again you

00:27:26,510 --> 00:27:30,760
have the request of the buffer pool to

00:27:28,340 --> 00:27:34,020
get buffers for this particular packet

00:27:30,760 --> 00:27:37,350
so once the buffer is acquired

00:27:34,020 --> 00:27:39,179
like the packet would be kind of diamond

00:27:37,350 --> 00:27:41,220
directly to the guests memory so because

00:27:39,179 --> 00:27:42,809
this this memory like whatever buffer

00:27:41,220 --> 00:27:44,760
was acquired it correspond to the guests

00:27:42,809 --> 00:27:47,370
physical memory that we are having so

00:27:44,760 --> 00:27:50,400
directly this can be D made to the guest

00:27:47,370 --> 00:27:53,160
memory so essentially this was the

00:27:50,400 --> 00:27:54,870
architecture that we were we are

00:27:53,160 --> 00:27:57,210
actually proposing for enhancing the

00:27:54,870 --> 00:28:00,720
whattaya functionality and which would

00:27:57,210 --> 00:28:03,690
certainly help what I have to deal with

00:28:00,720 --> 00:28:05,700
use cases with respect to network

00:28:03,690 --> 00:28:07,740
function virtualization this is just to

00:28:05,700 --> 00:28:10,920
summarize like what I just presented and

00:28:07,740 --> 00:28:13,080
as we can see that in certain use cases

00:28:10,920 --> 00:28:16,110
we host net interfaces can get saturated

00:28:13,080 --> 00:28:18,270
with high traffic rate and there is no

00:28:16,110 --> 00:28:20,640
mechanism as of now to which you can

00:28:18,270 --> 00:28:23,610
communicate this saturation from the

00:28:20,640 --> 00:28:24,900
guest to the physical interface that is

00:28:23,610 --> 00:28:28,230
lacking and that really impacts the

00:28:24,900 --> 00:28:29,790
overall performance so with the more

00:28:28,230 --> 00:28:31,710
integrated solution which we are

00:28:29,790 --> 00:28:33,270
actually proposing it is certainly

00:28:31,710 --> 00:28:36,090
possible that it can provide a feedback

00:28:33,270 --> 00:28:39,390
and to the physical lake and improve the

00:28:36,090 --> 00:28:41,700
overall system performance and certainly

00:28:39,390 --> 00:28:43,980
as as I just showed we can use hardware

00:28:41,700 --> 00:28:47,220
acceleration for doing this so I think

00:28:43,980 --> 00:28:49,500
I'm so this is this is it for my

00:28:47,220 --> 00:28:51,800
presentation and we are open to

00:28:49,500 --> 00:28:51,800
questions

00:28:59,849 --> 00:29:03,209
any questions

00:29:25,090 --> 00:29:36,370
drinks are passing so we the buffer

00:29:33,800 --> 00:29:39,230
you're talking about the buffer pool

00:29:36,370 --> 00:30:05,860
okay the ring the the word work you

00:29:39,230 --> 00:30:05,860
rings meetings to the host

00:30:25,370 --> 00:30:27,370
I

00:30:38,140 --> 00:30:42,640
the Rings are already mapped in the

00:30:40,600 --> 00:30:45,600
guest space right so it is a shared

00:30:42,640 --> 00:30:45,600
memory between the kisser

00:30:45,780 --> 00:30:53,559
these rings okay yeah so I mean that

00:30:50,980 --> 00:30:57,179
could be one possibility but I mean we

00:30:53,559 --> 00:30:57,179
did think about it

00:31:28,470 --> 00:31:34,780
the traffic so we so currently the

00:31:32,440 --> 00:31:36,850
proposal is to kind of however one is to

00:31:34,780 --> 00:31:39,070
one kind of correspondence here we are

00:31:36,850 --> 00:31:41,260
not splitting currently we are not

00:31:39,070 --> 00:31:44,260
thinking about those lines and but yes

00:31:41,260 --> 00:31:47,350
that's a valid point where you would say

00:31:44,260 --> 00:31:49,270
that we can directly assign these here I

00:31:47,350 --> 00:31:52,630
mean that could be one thing that we can

00:31:49,270 --> 00:31:54,850
explore certainly but again the idea is

00:31:52,630 --> 00:31:57,880
to make this as generic as possible and

00:31:54,850 --> 00:32:02,260
not Hardware specific so we would need

00:31:57,880 --> 00:32:04,390
to value it as to how other nixar I mean

00:32:02,260 --> 00:32:06,669
for example a PC is IRA of Enochs how

00:32:04,390 --> 00:32:10,120
are they I mean how can they leverage it

00:32:06,669 --> 00:32:17,190
or what would be the case in in the case

00:32:10,120 --> 00:32:17,190
of those particular Phenix yes

00:32:22,000 --> 00:32:29,740
yes no not to put so it's not the port

00:32:26,560 --> 00:32:32,500
it's actually this particular frame view

00:32:29,740 --> 00:32:35,200
it's a cure so so essentially this port

00:32:32,500 --> 00:32:36,820
can cater to multiple queues so you have

00:32:35,200 --> 00:32:39,640
this passing classification distribution

00:32:36,820 --> 00:32:41,800
logic in the Ethernet port here right

00:32:39,640 --> 00:32:43,480
which can allow it to distribute packets

00:32:41,800 --> 00:32:45,250
through multiple frame queues and these

00:32:43,480 --> 00:32:49,480
each frame queue are actually catered to

00:32:45,250 --> 00:32:52,180
a different drink in the guest so it's

00:32:49,480 --> 00:32:53,620
not it's not actually dedicating an

00:32:52,180 --> 00:32:56,200
Ethernet port to the guests for that

00:32:53,620 --> 00:32:57,520
matter but it is like you if you have

00:32:56,200 --> 00:33:00,160
the hardware functionality where you can

00:32:57,520 --> 00:33:02,170
actually do the passing classification

00:33:00,160 --> 00:33:05,400
and distribution and have multiple frame

00:33:02,170 --> 00:33:05,400
queues so you can use

00:33:16,210 --> 00:33:19,210
okay

00:33:24,290 --> 00:33:27,890

YouTube URL: https://www.youtube.com/watch?v=9bCQXJJXF2k


