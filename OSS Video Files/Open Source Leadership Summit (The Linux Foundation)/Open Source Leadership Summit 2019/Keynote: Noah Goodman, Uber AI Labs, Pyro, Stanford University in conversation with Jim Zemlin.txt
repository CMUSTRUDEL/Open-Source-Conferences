Title: Keynote: Noah Goodman, Uber AI Labs, Pyro, Stanford University in conversation with Jim Zemlin
Publication date: 2019-03-13
Playlist: Open Source Leadership Summit 2019
Description: 
	Keynote: Noah Goodman, Fellow at Uber AI Labs, Co-Creator of Pyro and Associate Professor, Stanford University in conversation with Jim Zemlin, The Linux Foundation 

Jim Zemlin
The Linux Foundation
Executive Director
Jim Zemlin’s career spans three of the largest technology trends to rise over the last decade: mobile computing, cloud computing, and open source software. Today, as executive director of The Linux Foundation, he uses this experience to accelerate innovation in technology through the use of open source and Linux.At The Linux Foundation, Jim works with the world’s largest technology companies, including IBM, Intel, Google, Samsung, Qualcomm, and others to help define the future of computing on the server, in the cloud, and on a variety of mobile computing devices. His work at the vendor-neutral Linux Foundation gives him a unique and aggregate perspective on the global technology industry.Jim has been recognized for his insights on the changing economics of the technology industry, and he is a regular keynote speaker at industry events. He advises a variety of startups, including Splashtop, and sits on the boards of the Global Economic Symposium, Open Source For America, and Chinese Open Source Promotion Union.

Noah Goodman
Uber AI Labs
Fellow
Websitehttps://uber.com/us/en/uberai/
Noah D. Goodman is Associate Professor of Psychology and Computer Science at Stanford University, and Senior Research Fellow at Uber AI Labs. He studies the computational basis of human and machine intelligence, merging behavioral experiments with formal methods from statistics and programming languages. His research topics include language understanding, social reasoning, concept learning, and causality. In addition he explores related technologies such as probabilistic programming languages and deep generative models. He has released open-source software including the probabilistic programming languages Church, WebPPL, and Pyro. Professor Goodman received his Ph.D. in mathematics from the University of Texas at Austin in 2003. In 2005 he entered cognitive science, working as Postdoc and Research Scientist at MIT. In 2010 he moved to Stanford where he runs the Computation and Cognition Lab. His work has been recognized by the J. S. McDonnell Foundation Scholar Award, the Roger N. Shepard Distinguished Visiting Scholar Award, the Alfred P. Sloan Research Fellowship in Neuroscience, and six computational modeling prizes from the Cognitive Science Society, several best paper awards, etc.
Captions: 
	00:00:00,030 --> 00:00:06,089
all right so we talked about the

00:00:02,909 --> 00:00:09,030
successive open source about a graph QL

00:00:06,089 --> 00:00:12,059
an API is an a faster better way to

00:00:09,030 --> 00:00:14,880
manage API as we talked about continuous

00:00:12,059 --> 00:00:17,970
delivery we wouldn't be complete without

00:00:14,880 --> 00:00:21,090
artificial intelligence so our next

00:00:17,970 --> 00:00:25,560
speaker is a senior research fellow of

00:00:21,090 --> 00:00:27,660
AI at uber labs he is also an associate

00:00:25,560 --> 00:00:30,720
professor of psychology and computer

00:00:27,660 --> 00:00:32,219
science at Stanford University and he

00:00:30,720 --> 00:00:36,390
has released several open source

00:00:32,219 --> 00:00:38,969
projects including Church web ppl and

00:00:36,390 --> 00:00:43,890
pyro today he's going to join me to do

00:00:38,969 --> 00:00:48,260
AQ a on open source and AI so please

00:00:43,890 --> 00:00:48,260
welcome to the stage Noah Goodman

00:00:48,480 --> 00:00:58,120
[Music]

00:00:54,730 --> 00:00:58,120
all right

00:00:58,460 --> 00:01:05,560
artificial intelligence so there's a lot

00:01:02,720 --> 00:01:07,729
of folks here in the audience who are

00:01:05,560 --> 00:01:10,100
technologists but are also making

00:01:07,729 --> 00:01:14,210
decisions about you know how to make

00:01:10,100 --> 00:01:16,280
sense of AI ml and what technologies

00:01:14,210 --> 00:01:17,990
they should or shouldn't adopt well I

00:01:16,280 --> 00:01:21,280
think one of the biggest questions I

00:01:17,990 --> 00:01:24,020
always get is a what is the state of

00:01:21,280 --> 00:01:26,690
this category of commuting this segment

00:01:24,020 --> 00:01:29,390
of computing and open source and then

00:01:26,690 --> 00:01:30,710
you know across all the different

00:01:29,390 --> 00:01:33,860
technologies you know you're working

00:01:30,710 --> 00:01:36,619
with us on our deep learning foundation

00:01:33,860 --> 00:01:38,899
you know has contributed or wold

00:01:36,619 --> 00:01:42,229
other projects into that but people

00:01:38,899 --> 00:01:43,580
always ask me like when are we gonna see

00:01:42,229 --> 00:01:45,680
some standardization around here what's

00:01:43,580 --> 00:01:47,450
the state of all this code like help the

00:01:45,680 --> 00:01:49,729
audience understand like where are we at

00:01:47,450 --> 00:01:51,770
in terms of the technology and and the

00:01:49,729 --> 00:01:53,390
use of this this technology yeah that's

00:01:51,770 --> 00:01:56,000
a great question we're definitely at a

00:01:53,390 --> 00:01:58,400
very early stage there's a lot of you

00:01:56,000 --> 00:01:59,930
know rich chaos it's moving fast and

00:01:58,400 --> 00:02:01,700
it's really exciting

00:01:59,930 --> 00:02:02,900
you know narrowly speaking I would say

00:02:01,700 --> 00:02:04,310
there's there's some amount of

00:02:02,900 --> 00:02:07,340
convergence that's already happening

00:02:04,310 --> 00:02:08,840
it's not in necessarily the you know a

00:02:07,340 --> 00:02:11,019
reduction of the number of different

00:02:08,840 --> 00:02:13,549
things you can use but there's much more

00:02:11,019 --> 00:02:16,670
interface compatibility that's emerging

00:02:13,549 --> 00:02:19,100
you know the fact that numpy and pi

00:02:16,670 --> 00:02:23,530
torch used very very very similar

00:02:19,100 --> 00:02:26,269
interfaces is kind of an example of that

00:02:23,530 --> 00:02:29,000
I will say that I think it's gonna take

00:02:26,269 --> 00:02:30,860
a while for the dust to settle actually

00:02:29,000 --> 00:02:32,870
this morning when I was you know hearing

00:02:30,860 --> 00:02:35,299
you talk about the amazing success of

00:02:32,870 --> 00:02:37,640
Linux I was remembering myself 20 years

00:02:35,299 --> 00:02:39,739
ago as a grad student building a Linux

00:02:37,640 --> 00:02:43,190
kernel from scratch over and over and

00:02:39,739 --> 00:02:44,900
over and it was awesome but only if you

00:02:43,190 --> 00:02:46,700
were extremely technical and had a lot

00:02:44,900 --> 00:02:49,130
of time on your hands right and I think

00:02:46,700 --> 00:02:51,709
actually you know AI is a little bit at

00:02:49,130 --> 00:02:54,260
that stage where the the power is there

00:02:51,709 --> 00:02:56,630
but it takes a lot of work a lot of time

00:02:54,260 --> 00:02:58,519
and I think over the next five years

00:02:56,630 --> 00:02:59,989
what we're gonna see hopefully it'll be

00:02:58,519 --> 00:03:02,299
less than 20 years what we're going to

00:02:59,989 --> 00:03:04,430
see is that the there's convergence and

00:03:02,299 --> 00:03:06,320
the tools become much faster to where

00:03:04,430 --> 00:03:07,220
you can you know see the same tools

00:03:06,320 --> 00:03:09,560
across

00:03:07,220 --> 00:03:12,980
different applications in a much more

00:03:09,560 --> 00:03:14,540
reliable turnkey way yeah you know one

00:03:12,980 --> 00:03:15,830
of the things so the in the deep

00:03:14,540 --> 00:03:17,540
learning project of the Linux Foundation

00:03:15,830 --> 00:03:18,980
you know there's a variety of different

00:03:17,540 --> 00:03:21,620
efforts you know again you you

00:03:18,980 --> 00:03:23,300
contributed or vald pyro but you know

00:03:21,620 --> 00:03:25,730
you look at things like a Q Mo's from

00:03:23,300 --> 00:03:30,500
which came out of AT&T but is you know a

00:03:25,730 --> 00:03:33,380
tool that's I trying to make the use of

00:03:30,500 --> 00:03:36,440
data the packaging of it and deployment

00:03:33,380 --> 00:03:39,770
of models and so forth just easier for a

00:03:36,440 --> 00:03:41,750
general practitioner and again early

00:03:39,770 --> 00:03:43,340
days and in that project early days and

00:03:41,750 --> 00:03:46,490
the projects that you're working on from

00:03:43,340 --> 00:03:49,550
uber but like give us a little more

00:03:46,490 --> 00:03:54,440
detail on how how what we need to do to

00:03:49,550 --> 00:03:56,450
make a IML technology easier for in in

00:03:54,440 --> 00:03:59,030
being able to be implemented by a

00:03:56,450 --> 00:04:01,250
broader community yeah there's there's

00:03:59,030 --> 00:04:03,320
really two sides of this which is why

00:04:01,250 --> 00:04:04,610
it's so difficult because one side is

00:04:03,320 --> 00:04:06,020
exactly the things we've been hearing a

00:04:04,610 --> 00:04:07,250
lot about him and I love the continuous

00:04:06,020 --> 00:04:09,650
integration thing because I think

00:04:07,250 --> 00:04:11,660
continuous integration and really solid

00:04:09,650 --> 00:04:14,470
testing is the one of the biggest

00:04:11,660 --> 00:04:18,530
contributions of open-source community

00:04:14,470 --> 00:04:20,209
to AI software and so I think like you

00:04:18,530 --> 00:04:22,460
know I don't have a lot to say that's

00:04:20,209 --> 00:04:26,150
really novel beyond do what those guys

00:04:22,460 --> 00:04:27,350
are doing only in AI right but the other

00:04:26,150 --> 00:04:29,510
side and I think the thing that

00:04:27,350 --> 00:04:33,590
complicates it is that AI is still very

00:04:29,510 --> 00:04:35,870
much open research and the idea is the

00:04:33,590 --> 00:04:38,479
kind of very foundational mathematics

00:04:35,870 --> 00:04:40,340
and ideas are moving really fast so

00:04:38,479 --> 00:04:42,380
there are some things that exist and we

00:04:40,340 --> 00:04:44,630
know they exist and they're appropriate

00:04:42,380 --> 00:04:46,010
to start making it easier to use but

00:04:44,630 --> 00:04:48,950
there's other things that you know we

00:04:46,010 --> 00:04:50,870
just you know thought up yesterday that

00:04:48,950 --> 00:04:52,910
you know there have been 30 papers on

00:04:50,870 --> 00:04:54,770
the archive which is you know the main

00:04:52,910 --> 00:04:56,630
source of all AI knowledge these days

00:04:54,770 --> 00:04:59,120
but they were all in the last 30 seconds

00:04:56,630 --> 00:05:01,340
right right and so I think it's very

00:04:59,120 --> 00:05:03,229
hard to know how to make things easier

00:05:01,340 --> 00:05:04,790
when you're not quite sure what are the

00:05:03,229 --> 00:05:06,830
things that are really successful and

00:05:04,790 --> 00:05:09,290
whether they're gonna be supplanted by a

00:05:06,830 --> 00:05:11,419
new unification or new ideas tomorrow

00:05:09,290 --> 00:05:13,970
and so it's this kind of bootstrapping

00:05:11,419 --> 00:05:15,470
process of taking things that seem to be

00:05:13,970 --> 00:05:17,539
working and building tools to make them

00:05:15,470 --> 00:05:20,390
successful and hoping that you haven't

00:05:17,539 --> 00:05:22,130
committed yourself to research

00:05:20,390 --> 00:05:23,510
ideas that are going to be you know

00:05:22,130 --> 00:05:27,980
replaced with something else

00:05:23,510 --> 00:05:30,260
the next day yeah how did the the

00:05:27,980 --> 00:05:31,970
decision around you know using and

00:05:30,260 --> 00:05:33,980
leveraging open-source at uber for

00:05:31,970 --> 00:05:36,230
example like what what from your

00:05:33,980 --> 00:05:37,820
perspective was the value of saying like

00:05:36,230 --> 00:05:40,460
we want to put this stuff out there put

00:05:37,820 --> 00:05:41,780
it into neutral home have a solid

00:05:40,460 --> 00:05:43,610
governance structure behind that you

00:05:41,780 --> 00:05:49,400
know what role does open-source play in

00:05:43,610 --> 00:05:51,800
the AI ml world so to me let me take one

00:05:49,400 --> 00:05:53,570
step back you describe kind of what what

00:05:51,800 --> 00:05:55,640
I'm thinking about the role of

00:05:53,570 --> 00:05:59,360
open-source is so I've been thinking a

00:05:55,640 --> 00:06:01,370
lot about Highways recently not because

00:05:59,360 --> 00:06:03,350
I work it over in there's cars but

00:06:01,370 --> 00:06:05,450
because the the interstate highway

00:06:03,350 --> 00:06:08,570
system the Eisenhower highway system is

00:06:05,450 --> 00:06:10,490
this amazing example of a common good

00:06:08,570 --> 00:06:12,410
that could not have been created by any

00:06:10,490 --> 00:06:14,810
one company or really private

00:06:12,410 --> 00:06:15,620
organization it was created collectively

00:06:14,810 --> 00:06:17,840
and that case

00:06:15,620 --> 00:06:19,460
you know sponsored by the government but

00:06:17,840 --> 00:06:22,700
it radically increased the productivity

00:06:19,460 --> 00:06:24,890
of the US economy for decades after it

00:06:22,700 --> 00:06:27,440
was created around in right after World

00:06:24,890 --> 00:06:29,060
War two so that's an example of

00:06:27,440 --> 00:06:30,350
something that we had to have done

00:06:29,060 --> 00:06:34,160
collectively but it provides

00:06:30,350 --> 00:06:35,750
infrastructure for all of us so then you

00:06:34,160 --> 00:06:38,060
might ask okay what's the what's the

00:06:35,750 --> 00:06:40,220
equivalent of that now and I actually

00:06:38,060 --> 00:06:42,020
think these these projects these big

00:06:40,220 --> 00:06:43,910
open-source projects are exactly the

00:06:42,020 --> 00:06:46,040
equivalent of that there are things that

00:06:43,910 --> 00:06:50,210
we can't do individually even in big

00:06:46,040 --> 00:06:52,310
companies like Google and uber but we

00:06:50,210 --> 00:06:55,310
can do collectively and by doing it

00:06:52,310 --> 00:06:57,050
collectively everybody who is going to

00:06:55,310 --> 00:06:59,600
adopt it gains a huge productivity boost

00:06:57,050 --> 00:07:02,060
right and you might wonder okay well you

00:06:59,600 --> 00:07:03,550
know Ober is be again energetic why

00:07:02,060 --> 00:07:07,730
couldn't they just do this internally

00:07:03,550 --> 00:07:10,880
and I think the answer is that in

00:07:07,730 --> 00:07:13,610
general but especially in AI you need

00:07:10,880 --> 00:07:15,110
multiple multiple viewpoints and

00:07:13,610 --> 00:07:18,230
multiple contributors to make sure that

00:07:15,110 --> 00:07:19,700
the software is exercised and tested you

00:07:18,230 --> 00:07:21,230
need people contributing from a lot of

00:07:19,700 --> 00:07:23,450
different applications so that you don't

00:07:21,230 --> 00:07:25,850
kind of in machine learning speak over

00:07:23,450 --> 00:07:27,800
fit to one thing right and we've really

00:07:25,850 --> 00:07:29,000
seen that in you know the pyro project

00:07:27,800 --> 00:07:33,230
that I've been involved with and other

00:07:29,000 --> 00:07:35,450
open source AI projects they're kind of

00:07:33,230 --> 00:07:37,010
the community input that helps has

00:07:35,450 --> 00:07:38,780
helped us make really solid software

00:07:37,010 --> 00:07:41,150
that we can use and that can be used

00:07:38,780 --> 00:07:43,490
externally also the fact that there's a

00:07:41,150 --> 00:07:45,380
huge synergy the pyro project is an

00:07:43,490 --> 00:07:47,720
example where it's an open-source

00:07:45,380 --> 00:07:49,070
software project that exists only

00:07:47,720 --> 00:07:51,800
because we get to build on another

00:07:49,070 --> 00:07:54,320
open-source software project I torch for

00:07:51,800 --> 00:07:56,330
a facebook really and so this kind of

00:07:54,320 --> 00:07:58,130
you know collective building I think is

00:07:56,330 --> 00:08:00,950
why we just have to be doing it this way

00:07:58,130 --> 00:08:04,810
yeah yeah we you know one of the things

00:08:00,950 --> 00:08:07,960
as we're seeing a lot of major

00:08:04,810 --> 00:08:11,420
technologies kind of coming out 5g

00:08:07,960 --> 00:08:13,460
autonomous driving you know the kind of

00:08:11,420 --> 00:08:17,920
edge computing with low latency out

00:08:13,460 --> 00:08:22,610
there like give us a sneak peak of the

00:08:17,920 --> 00:08:24,770
implications of AI technology that is it

00:08:22,610 --> 00:08:27,170
going to be enabled by those adjacent

00:08:24,770 --> 00:08:29,360
kind of breakthroughs you know from both

00:08:27,170 --> 00:08:33,650
the network connectivity side and then

00:08:29,360 --> 00:08:34,790
you know in the autonomous world I don't

00:08:33,650 --> 00:08:36,440
have anything to say about network

00:08:34,790 --> 00:08:38,750
connectivity I'm afraid that's right

00:08:36,440 --> 00:08:40,430
those that that chart I show the story

00:08:38,750 --> 00:08:42,080
exactly it's nobody understands network

00:08:40,430 --> 00:08:45,380
I think there's a bunch of people here

00:08:42,080 --> 00:08:46,940
who do understand it with it so I'll

00:08:45,380 --> 00:08:51,050
restrict myself to something simple like

00:08:46,940 --> 00:08:52,640
official intelligence great yeah so you

00:08:51,050 --> 00:08:53,750
know predicting the impact of any

00:08:52,640 --> 00:08:56,090
technology is hard

00:08:53,750 --> 00:08:58,070
I think AI is interesting because by its

00:08:56,090 --> 00:09:00,050
very nature predicting what it will do

00:08:58,070 --> 00:09:01,340
is much harder right artificial

00:09:00,050 --> 00:09:03,980
intelligence is climbing I thought a I

00:09:01,340 --> 00:09:05,510
was about prediction well yeah exactly I

00:09:03,980 --> 00:09:08,690
don't know what your prediction is going

00:09:05,510 --> 00:09:10,490
to predict that's exactly the point

00:09:08,690 --> 00:09:14,060
AI is about making making better

00:09:10,490 --> 00:09:15,980
decisions making them from more data and

00:09:14,060 --> 00:09:19,040
making them more rapidly making more of

00:09:15,980 --> 00:09:21,170
them making them more integrated you

00:09:19,040 --> 00:09:23,450
know as you might imagine better

00:09:21,170 --> 00:09:25,340
decision making is potentially a

00:09:23,450 --> 00:09:28,910
game-changer for everything you know

00:09:25,340 --> 00:09:30,950
everything that humans do and so because

00:09:28,910 --> 00:09:33,530
of that and because we don't know yet

00:09:30,950 --> 00:09:35,570
which decisions are going to be made

00:09:33,530 --> 00:09:38,030
much better and which decisions are

00:09:35,570 --> 00:09:39,980
going to be made only a little better we

00:09:38,030 --> 00:09:42,230
don't actually know fully what AI is

00:09:39,980 --> 00:09:43,730
going to do I think this is both why

00:09:42,230 --> 00:09:45,980
everybody's paying attention and there's

00:09:43,730 --> 00:09:47,490
a lot of hype and also partly why

00:09:45,980 --> 00:09:50,730
there's too much hype

00:09:47,490 --> 00:09:53,009
so AI is going to change everything my

00:09:50,730 --> 00:09:55,259
own opinion is that it's going to change

00:09:53,009 --> 00:09:56,940
everything in sort of the way that a

00:09:55,259 --> 00:09:58,829
power saw changed things

00:09:56,940 --> 00:10:00,779
you know they had hand saws they could

00:09:58,829 --> 00:10:02,190
cut things all of a sudden when you have

00:10:00,779 --> 00:10:04,319
a power saw you can cut a lot more

00:10:02,190 --> 00:10:05,910
things right and at first you just cut

00:10:04,319 --> 00:10:08,699
more things but pretty soon you realize

00:10:05,910 --> 00:10:11,899
you can cut things differently and AI is

00:10:08,699 --> 00:10:14,940
gonna be like that it'll first change

00:10:11,899 --> 00:10:17,130
you know kind of change the way we do

00:10:14,940 --> 00:10:19,199
specific tasks that we already do it'll

00:10:17,130 --> 00:10:22,410
then grow out into the rest of our

00:10:19,199 --> 00:10:25,079
software development process some people

00:10:22,410 --> 00:10:29,579
call it software 2.0 which is catchy but

00:10:25,079 --> 00:10:32,550
a little bit over and pretty soon we'll

00:10:29,579 --> 00:10:34,259
we'll just come up with new things that

00:10:32,550 --> 00:10:37,199
we can do with it in almost every

00:10:34,259 --> 00:10:39,120
application mmm so I think what people

00:10:37,199 --> 00:10:40,589
have to do is keep track of where it's

00:10:39,120 --> 00:10:42,329
going and not imagine that they can

00:10:40,589 --> 00:10:45,540
predict right now what it's gonna do

00:10:42,329 --> 00:10:46,949
yeah yeah you know I I want to talk

00:10:45,540 --> 00:10:50,730
about like all the different moving

00:10:46,949 --> 00:10:52,519
parts ml DL models data reinforcement

00:10:50,730 --> 00:10:55,019
learning natural language processing

00:10:52,519 --> 00:10:56,459
training etc you know when you look at

00:10:55,019 --> 00:10:58,589
all the different components that need

00:10:56,459 --> 00:11:01,170
to come together you know any one of

00:10:58,589 --> 00:11:04,290
those things is complex there's all this

00:11:01,170 --> 00:11:06,630
talk of you know data is the new oil and

00:11:04,290 --> 00:11:09,480
you know how people will be able to you

00:11:06,630 --> 00:11:12,209
know sort of chord that aspect you know

00:11:09,480 --> 00:11:13,860
across those different components of a

00:11:12,209 --> 00:11:16,050
you know what's your thought I'm like

00:11:13,860 --> 00:11:19,079
where are we doing good where can we do

00:11:16,050 --> 00:11:20,459
better like you know it's just so many

00:11:19,079 --> 00:11:21,510
components I think a lot of people here

00:11:20,459 --> 00:11:24,120
in the audience we're trying to figure

00:11:21,510 --> 00:11:29,880
out like what is my strategy where do I

00:11:24,120 --> 00:11:31,649
invest it's funny you say all of those

00:11:29,880 --> 00:11:34,230
acronyms and I think yeah all the pieces

00:11:31,649 --> 00:11:36,000
of AI whereas I heard you know all the

00:11:34,230 --> 00:11:37,110
things about continuous integration I

00:11:36,000 --> 00:11:39,209
thought wow look at all those

00:11:37,110 --> 00:11:41,069
complicated pieces how keep track of all

00:11:39,209 --> 00:11:41,270
of that stuff so it's in the eye of the

00:11:41,069 --> 00:11:42,649
beholder

00:11:41,270 --> 00:11:45,569
[Music]

00:11:42,649 --> 00:11:51,420
but you know your your more general

00:11:45,569 --> 00:11:54,089
question I think the answer is maybe the

00:11:51,420 --> 00:11:56,790
good news is that five years ago it used

00:11:54,089 --> 00:11:58,319
to be that the different subfields of AI

00:11:56,790 --> 00:11:59,580
were very different and were powered by

00:11:58,319 --> 00:12:01,460
very different

00:11:59,580 --> 00:12:03,840
statistical techniques and technologies

00:12:01,460 --> 00:12:07,620
there's been actually a remarkable

00:12:03,840 --> 00:12:09,870
amount of convergence around first you

00:12:07,620 --> 00:12:11,490
know ideas from deep learning but also

00:12:09,870 --> 00:12:14,040
it turns out you know deep learning is

00:12:11,490 --> 00:12:16,160
not orthogonal to statistical technique

00:12:14,040 --> 00:12:20,490
so right now the ideas of probability

00:12:16,160 --> 00:12:23,970
deep learning those basic tools are

00:12:20,490 --> 00:12:26,340
behind most of the advances in AI and I

00:12:23,970 --> 00:12:31,380
think it's not at all accidental that

00:12:26,340 --> 00:12:33,390
they you know the arising of really

00:12:31,380 --> 00:12:35,660
important open-source projects has

00:12:33,390 --> 00:12:40,260
actually coincided with AI gaining

00:12:35,660 --> 00:12:42,360
gaining visibility and I think the

00:12:40,260 --> 00:12:44,490
causality is actually you know let me

00:12:42,360 --> 00:12:46,080
say it differently people talk about why

00:12:44,490 --> 00:12:49,890
is AI happening now and they talk about

00:12:46,080 --> 00:12:51,480
compute and data right right and - it's

00:12:49,890 --> 00:12:53,150
too large assent that's very true the

00:12:51,480 --> 00:12:57,330
algorithms have not changed radically

00:12:53,150 --> 00:12:58,440
and in recent years small advances but I

00:12:57,330 --> 00:13:01,770
think they're missing something else

00:12:58,440 --> 00:13:05,210
which is software something that has

00:13:01,770 --> 00:13:09,030
actually made the the world of modern AI

00:13:05,210 --> 00:13:12,060
proceed much much more rapidly to allow

00:13:09,030 --> 00:13:14,070
much more substantial software

00:13:12,060 --> 00:13:17,640
engineering projects is that there are

00:13:14,070 --> 00:13:18,960
these frameworks that build on also kind

00:13:17,640 --> 00:13:20,970
of old ideas like automatic

00:13:18,960 --> 00:13:23,280
differentiation but they're systematic

00:13:20,970 --> 00:13:26,760
frameworks that allow you to rapidly

00:13:23,280 --> 00:13:29,310
build much more complicated AI software

00:13:26,760 --> 00:13:31,050
projects also that allow you to deploy

00:13:29,310 --> 00:13:33,660
to hardware that otherwise is extremely

00:13:31,050 --> 00:13:38,400
hard to target Thank You P is right and

00:13:33,660 --> 00:13:42,210
so you know I think the the the compute

00:13:38,400 --> 00:13:44,220
is moving along doing its own thing the

00:13:42,210 --> 00:13:46,080
data is there and everybody knows that

00:13:44,220 --> 00:13:51,210
they need to have their own data and

00:13:46,080 --> 00:13:53,100
hopefully share it I think the the point

00:13:51,210 --> 00:13:56,340
of most leverage for investment is

00:13:53,100 --> 00:13:59,220
actually for software and it's sort of

00:13:56,340 --> 00:14:01,200
understanding which software tools are

00:13:59,220 --> 00:14:03,510
going to allow the most systematic

00:14:01,200 --> 00:14:06,210
software development that's integrated

00:14:03,510 --> 00:14:08,640
into the rest of your software right -

00:14:06,210 --> 00:14:10,110
the rest of your engineering and there

00:14:08,640 --> 00:14:12,060
are some that everybody knows oh yeah

00:14:10,110 --> 00:14:16,440
that's a good thing for AI right

00:14:12,060 --> 00:14:18,269
pi torch there's tensorflow but what's

00:14:16,440 --> 00:14:21,720
emerging now is a whole bunch of a very

00:14:18,269 --> 00:14:24,329
large ecosystem of kind of layers that

00:14:21,720 --> 00:14:27,630
are integrated or on top or useful for

00:14:24,329 --> 00:14:29,850
deployment and monitoring right just

00:14:27,630 --> 00:14:32,250
building one you know say deep neural

00:14:29,850 --> 00:14:34,260
net doesn't actually solve the problems

00:14:32,250 --> 00:14:36,029
that you have of you know deployment

00:14:34,260 --> 00:14:38,310
integration monitoring testing

00:14:36,029 --> 00:14:40,410
evaluation but there's a big ecosystem

00:14:38,310 --> 00:14:41,880
growing for that now right and I think

00:14:40,410 --> 00:14:44,040
actually the deep learning foundation is

00:14:41,880 --> 00:14:46,649
a kind of awesome way to foster that and

00:14:44,040 --> 00:14:48,690
focus the energy absolutely you know I

00:14:46,649 --> 00:14:50,459
hear every day people wanting to see

00:14:48,690 --> 00:14:52,560
more consolidation around these

00:14:50,459 --> 00:14:55,829
frameworks and you know help give

00:14:52,560 --> 00:14:57,720
guidance I want to come back to the the

00:14:55,829 --> 00:15:01,649
data question you know you said you know

00:14:57,720 --> 00:15:03,839
everyone you hope everyone shares laughs

00:15:01,649 --> 00:15:05,850
about a year and a half ago the Linux

00:15:03,839 --> 00:15:08,459
Foundation worked with our member

00:15:05,850 --> 00:15:11,339
companies attorneys to create an open

00:15:08,459 --> 00:15:13,680
data license so this was the the concept

00:15:11,339 --> 00:15:15,899
was let's kind of similar to how open

00:15:13,680 --> 00:15:18,329
source license have have allowed for the

00:15:15,899 --> 00:15:20,730
smooth sharing of code from an IP

00:15:18,329 --> 00:15:22,380
perspective let's try and ply those same

00:15:20,730 --> 00:15:25,050
practices today know where you have sort

00:15:22,380 --> 00:15:27,240
of a copyleft license where you know

00:15:25,050 --> 00:15:29,220
there's a shared share alike type of

00:15:27,240 --> 00:15:31,230
function and then more of a permissive

00:15:29,220 --> 00:15:36,360
license but two main license for data

00:15:31,230 --> 00:15:38,910
sharing how do you see the world of data

00:15:36,360 --> 00:15:41,670
sharing evolving are we gonna see more

00:15:38,910 --> 00:15:44,370
hoarding are we gonna see networks of

00:15:41,670 --> 00:15:46,860
data sharing like the uber urban compute

00:15:44,370 --> 00:15:48,209
thing I saw this morning it's just very

00:15:46,860 --> 00:15:49,889
interesting where you hear here you have

00:15:48,209 --> 00:15:51,930
this company this Yahoo borough is

00:15:49,889 --> 00:15:54,329
really important data that's being

00:15:51,930 --> 00:15:55,740
anonymized and shared with regulators I

00:15:54,329 --> 00:15:57,690
think that's super interesting are we

00:15:55,740 --> 00:16:01,740
gonna see more of that less of it what's

00:15:57,690 --> 00:16:03,870
your thoughts I hope very much that we

00:16:01,740 --> 00:16:06,510
see more of it I'm worried that we'll

00:16:03,870 --> 00:16:08,399
see less of it so I think you know

00:16:06,510 --> 00:16:12,209
there's a big pressure right now people

00:16:08,399 --> 00:16:15,180
realize that data are fundamental in

00:16:12,209 --> 00:16:17,220
some ways you know they're more horrible

00:16:15,180 --> 00:16:18,899
than software because software now needs

00:16:17,220 --> 00:16:21,449
to be built collectively for it to

00:16:18,899 --> 00:16:23,370
function and write properly we've got

00:16:21,449 --> 00:16:25,240
that part right out software is more

00:16:23,370 --> 00:16:29,350
horrible right it's like I'm a drain

00:16:25,240 --> 00:16:31,660
sitting on my bits right I think it

00:16:29,350 --> 00:16:33,129
could go the the direction which sadly

00:16:31,660 --> 00:16:35,529
it has been going that each company

00:16:33,129 --> 00:16:37,899
thinks okay my gold mine is my data and

00:16:35,529 --> 00:16:41,679
I'm not sharing this right hopefully we

00:16:37,899 --> 00:16:44,230
can switch the world view to start to

00:16:41,679 --> 00:16:46,660
think of data as something that is you

00:16:44,230 --> 00:16:48,790
know one a common good something that

00:16:46,660 --> 00:16:52,600
belongs to humanity much more than to

00:16:48,790 --> 00:16:54,790
you know individual companies and to

00:16:52,600 --> 00:16:57,129
something that is going to much like the

00:16:54,790 --> 00:16:59,649
interstate highway system allow us to do

00:16:57,129 --> 00:17:02,230
more if we pool it you know really kind

00:16:59,649 --> 00:17:03,939
of vastly superlinear away right it's

00:17:02,230 --> 00:17:05,289
not just okay we'll share and I'll get

00:17:03,939 --> 00:17:07,990
the benefit of my data and your data

00:17:05,289 --> 00:17:10,929
it's that if we all share the benefits

00:17:07,990 --> 00:17:12,730
we get are vastly bigger yeah you're

00:17:10,929 --> 00:17:15,760
starting to see this in some some of the

00:17:12,730 --> 00:17:18,490
cybersecurity realm where people are

00:17:15,760 --> 00:17:20,049
sharing sort of attack vector data you

00:17:18,490 --> 00:17:22,659
know which you know if you do share it

00:17:20,049 --> 00:17:24,789
then you can do AI predictive analytics

00:17:22,659 --> 00:17:26,169
on how to like stop those much quicker

00:17:24,789 --> 00:17:28,209
than if you were just trying to hoard

00:17:26,169 --> 00:17:31,120
all that day to yourself I mean probably

00:17:28,209 --> 00:17:33,850
are gonna be so companies that try and

00:17:31,120 --> 00:17:35,590
control that network externality and in

00:17:33,850 --> 00:17:38,260
profiteer from it but what we're seeing

00:17:35,590 --> 00:17:40,659
a lot is you know coordination instead

00:17:38,260 --> 00:17:43,270
of hoarding they're like it seems like

00:17:40,659 --> 00:17:45,159
you're seeing some of that but you'd

00:17:43,270 --> 00:17:48,039
like to see more yeah and I'd like

00:17:45,159 --> 00:17:49,899
people to come to believe that if you

00:17:48,039 --> 00:17:51,580
take two different piles of data that

00:17:49,899 --> 00:17:53,470
have very different information in it

00:17:51,580 --> 00:17:54,820
and you put them together you get

00:17:53,470 --> 00:17:57,399
something much better than either one

00:17:54,820 --> 00:18:00,370
alone would give you yeah and that the

00:17:57,399 --> 00:18:01,809
only way that that's going to work that

00:18:00,370 --> 00:18:04,270
that's actually going to work out is not

00:18:01,809 --> 00:18:07,210
bilateral agreements but something more

00:18:04,270 --> 00:18:11,020
like an open open data right open

00:18:07,210 --> 00:18:12,940
sharing yep yep so I'll give you the

00:18:11,020 --> 00:18:15,130
last word say that this is the

00:18:12,940 --> 00:18:18,370
obligatory question which is you know

00:18:15,130 --> 00:18:21,760
everyone is also concerned about how AI

00:18:18,370 --> 00:18:24,789
will affect you know how we move every

00:18:21,760 --> 00:18:27,700
day and will make us all jobless I often

00:18:24,789 --> 00:18:30,279
tell leanness and Greg Crowe Hartman and

00:18:27,700 --> 00:18:33,940
the kernel folks that as soon as we can

00:18:30,279 --> 00:18:34,900
get a self creating AI tool for software

00:18:33,940 --> 00:18:38,050
code that

00:18:34,900 --> 00:18:39,790
you know they're out of a job but I mean

00:18:38,050 --> 00:18:42,150
it seems like all that stuff's pretty

00:18:39,790 --> 00:18:46,150
far-fetched what what are your thoughts

00:18:42,150 --> 00:18:48,760
yeah I think self creating AI is pretty

00:18:46,150 --> 00:18:51,640
far down the road I think even you know

00:18:48,760 --> 00:18:54,460
general human-level AI is quite far down

00:18:51,640 --> 00:18:58,750
the road in my opinion I think what's

00:18:54,460 --> 00:19:03,040
close is AI as power tools right like

00:18:58,750 --> 00:19:05,290
the saw example so I think you know it

00:19:03,040 --> 00:19:06,940
does matter a lot when a brand new tool

00:19:05,290 --> 00:19:09,220
comes along and when it's a power tool

00:19:06,940 --> 00:19:12,010
instead of a hand tool it changes what

00:19:09,220 --> 00:19:14,170
you can do and how you do it and that

00:19:12,010 --> 00:19:15,670
will result in you know dislocation

00:19:14,170 --> 00:19:17,080
people who only know how to use hand

00:19:15,670 --> 00:19:19,360
tools and all of a sudden there are

00:19:17,080 --> 00:19:21,820
power tools you know they've got to do

00:19:19,360 --> 00:19:24,429
something else right I don't think it's

00:19:21,820 --> 00:19:26,410
going to you know replace all of

00:19:24,429 --> 00:19:28,420
humanity and all of work but I think

00:19:26,410 --> 00:19:31,030
it'll change a lot of things and we need

00:19:28,420 --> 00:19:33,360
to think actually carefully as a society

00:19:31,030 --> 00:19:36,100
about what to do about that

00:19:33,360 --> 00:19:38,440
hopefully planning ahead instead of

00:19:36,100 --> 00:19:41,559
simply reacting you know I'm hoping

00:19:38,440 --> 00:19:44,350
actually that large organizations like

00:19:41,559 --> 00:19:45,940
Linux Foundation and human centered AI

00:19:44,350 --> 00:19:49,330
initiative that we're starting at

00:19:45,940 --> 00:19:52,090
Stanford which are not part of companies

00:19:49,330 --> 00:19:54,370
themselves can form the kind of the

00:19:52,090 --> 00:19:56,950
Nexus and the middlemen for thinking

00:19:54,370 --> 00:20:00,610
about these questions and making choices

00:19:56,950 --> 00:20:04,210
that are you know Pro humanity ahead of

00:20:00,610 --> 00:20:06,040
the the impact of AI and the potential

00:20:04,210 --> 00:20:07,929
disruptions of these new tools yeah

00:20:06,040 --> 00:20:09,340
well that's certainly our goal and you

00:20:07,929 --> 00:20:11,350
know we really appreciate the work that

00:20:09,340 --> 00:20:13,780
you've done with us on the LF deep

00:20:11,350 --> 00:20:16,090
learning organization and just the work

00:20:13,780 --> 00:20:17,410
you're doing in general so thanks for

00:20:16,090 --> 00:20:19,750
coming and sharing your thoughts today

00:20:17,410 --> 00:20:21,250
and let's give a round of applause here

00:20:19,750 --> 00:20:27,350
for now

00:20:21,250 --> 00:20:27,350

YouTube URL: https://www.youtube.com/watch?v=eoo5OutzzgI


