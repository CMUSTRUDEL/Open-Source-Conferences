Title: Keynote: Moral Outsourcing: Humanity in the age of AI -Dr. Chowdhury, Accenture Applied Intelligence
Publication date: 2019-03-13
Playlist: Open Source Leadership Summit 2019
Description: 
	Keynote: Moral Outsourcing: Humanity in the age of AI - Dr. Rumman Chowdhury, Global Lead for Responsible AI, Accenture Applied Intelligence 

Dr. Rumman Chowdhury
Accenture Applied Intelligence
Global Lead for Responsible AI
Websiterummanchowdhury.com/
Rumman Chowdhury’s passion lies at the intersection of artificial intelligence and humanity. She holds degrees in quantitative social science and has been a practicing data scientist and AI developer since 2013. She is currently the Global Lead for Responsible AI at Accenture Applied Intelligence, where she works with C-suite clients to create cutting-edge technical solutions for ethical, explainable and transparent AI.
In her work as Accenture’s Responsible AI lead, she led the design of the Fairness Tool, a first-in-industry algorithmic tool to identify and mitigate bias in AI systems. The Fairness Tool has been utilized successfully at Accenture clients around the world. Dr. Chowdhury co-authored a Harvard Business Review piece on the tool.

Rumman has been featured in international media, including the Financial Times, Harvard Business Review, MIT Technology Review, BBC, Axios, Cheddar TV, CRN, The Verge, Fast Company, Quartz, Corrierre Della Serra, Optio, Australian Broadcasting Channel and Nikkei Business Times.
She is a TedX speaker, a Forbes Tech contributing author and has been named by InformationWeek as one of 10 influential AI and machine learning experts to follow on Twitter. She was also named one of BBC’s 100 Women for 2017, recognized as one of the Bay Area’s top 40 under 40, and honoured to be inducted to the British Royal Society of the Arts (RSA). She has also been named by Forbes as one of Five Who are Shaping AI. 
Rumman serves as co-chair of the RSA’s Citizen AI Jury and actively participates in IEEE standards committees, the Partnership on AI, as an advisor to the UK House of Lords Parliamentary group on AI, and other global AI and ethics organizations. She has presented at multiple international governing bodies, including the United Nations, the OECD and the UK Parliament.
Captions: 
	00:00:00,030 --> 00:00:04,470
well if those lights are really bright

00:00:01,410 --> 00:00:06,509
has anybody else commented on that my

00:00:04,470 --> 00:00:08,550
name is dr. Ahmad Chaudhry and as

00:00:06,509 --> 00:00:10,559
mentioned I lead responsible AI at

00:00:08,550 --> 00:00:12,599
Accenture and what I'm here to talk

00:00:10,559 --> 00:00:14,700
about is moral outsourcing in the role

00:00:12,599 --> 00:00:16,590
of humanity in the age of artificial

00:00:14,700 --> 00:00:18,720
intelligence so these are like just some

00:00:16,590 --> 00:00:20,779
stats that we like to spew to clients

00:00:18,720 --> 00:00:23,970
but they're all true

00:00:20,779 --> 00:00:26,220
85% of interactions online managed with

00:00:23,970 --> 00:00:29,310
AI by 2020 an industry worth over a

00:00:26,220 --> 00:00:30,660
hundred billion dollars by 2025 and most

00:00:29,310 --> 00:00:33,059
importantly people throw around this

00:00:30,660 --> 00:00:34,920
term at the AI and will lead in the

00:00:33,059 --> 00:00:37,680
usher in the fourth Industrial

00:00:34,920 --> 00:00:39,629
Revolution so we hear all these amazing

00:00:37,680 --> 00:00:41,340
things about the potential but then we

00:00:39,629 --> 00:00:43,559
hear these headlines that look kind of

00:00:41,340 --> 00:00:45,210
like this AI robots are sexist and

00:00:43,559 --> 00:00:49,710
racist and it was actually my personal

00:00:45,210 --> 00:00:51,239
favorite how to avoid racist algorithm

00:00:49,710 --> 00:00:51,870
so as a woman like there's some sort of

00:00:51,239 --> 00:00:54,960
a mace

00:00:51,870 --> 00:00:57,899
I can spray on algorithms and they will

00:00:54,960 --> 00:01:00,600
not be racist I'll be really great right

00:00:57,899 --> 00:01:02,520
so so but you know this is a very real

00:01:00,600 --> 00:01:04,260
phenomenon like I'm joking but at the

00:01:02,520 --> 00:01:07,680
same time we know there are some serious

00:01:04,260 --> 00:01:09,659
problems with bias in data with design

00:01:07,680 --> 00:01:11,580
of our algorithmic systems and you know

00:01:09,659 --> 00:01:12,810
not just from this you know one to one

00:01:11,580 --> 00:01:14,970
sense but if you think about these

00:01:12,810 --> 00:01:17,869
systems that grow scale and perpetuate

00:01:14,970 --> 00:01:21,630
these biases they become embedded in

00:01:17,869 --> 00:01:23,880
labyrinthian systems and that are only

00:01:21,630 --> 00:01:26,340
decipherable to a few so I'm also

00:01:23,880 --> 00:01:27,930
currently in pure conspiracy theory mode

00:01:26,340 --> 00:01:30,060
because I'm reading Shoshanna Xue Buffs

00:01:27,930 --> 00:01:31,500
the age of surveillance capitalism if

00:01:30,060 --> 00:01:34,049
you have not picked it up please pick it

00:01:31,500 --> 00:01:35,520
up probably something not good for

00:01:34,049 --> 00:01:37,860
somebody already working in ethics and

00:01:35,520 --> 00:01:39,720
AI to read but you know it is it is an

00:01:37,860 --> 00:01:42,180
eye-opening book and saying this as

00:01:39,720 --> 00:01:44,850
somebody who does this for a living it

00:01:42,180 --> 00:01:46,110
is an eye-opening book but to think

00:01:44,850 --> 00:01:48,750
through let's think through a little bit

00:01:46,110 --> 00:01:51,600
about these terrible things you know

00:01:48,750 --> 00:01:54,420
that that that we keep hearing about so

00:01:51,600 --> 00:01:56,549
we see you know we see magazine covers

00:01:54,420 --> 00:01:59,969
like this with the lien of robots and

00:01:56,549 --> 00:02:02,579
people completely out of jobs and we see

00:01:59,969 --> 00:02:04,530
a lot of termina a lot of terminator

00:02:02,579 --> 00:02:06,420
imagery to the point that the people in

00:02:04,530 --> 00:02:09,209
the ethics any eye sees we jokingly keep

00:02:06,420 --> 00:02:11,009
like tabs of I call androgynous Li hot

00:02:09,209 --> 00:02:13,020
robots one of which I have in this deck

00:02:11,009 --> 00:02:14,820
and also terminator because

00:02:13,020 --> 00:02:17,970
it is so far from what artificial

00:02:14,820 --> 00:02:19,230
intelligence actually is today but it's

00:02:17,970 --> 00:02:20,880
it's interesting to know that this these

00:02:19,230 --> 00:02:23,100
this is the imagery we're being fed

00:02:20,880 --> 00:02:24,450
right and we we wonder what future we're

00:02:23,100 --> 00:02:25,920
heading into so we're building all this

00:02:24,450 --> 00:02:27,240
wonderful technology those of us who

00:02:25,920 --> 00:02:30,090
live in Silicon Valley and actually

00:02:27,240 --> 00:02:32,550
around the rest of the world we we see

00:02:30,090 --> 00:02:35,010
it with this sort of techno optimism

00:02:32,550 --> 00:02:37,620
this hope this idea that we build it and

00:02:35,010 --> 00:02:40,230
it will be used for good and yet this is

00:02:37,620 --> 00:02:42,720
what the media gives back to us it gives

00:02:40,230 --> 00:02:44,430
back to us images of mass joblessness it

00:02:42,720 --> 00:02:45,600
gives back to us images of people being

00:02:44,430 --> 00:02:46,140
discriminated against if you are

00:02:45,600 --> 00:02:48,750
following

00:02:46,140 --> 00:02:50,310
ProPublica they do a lot of really great

00:02:48,750 --> 00:02:53,250
work they've data scientists on staff

00:02:50,310 --> 00:02:56,190
that look at bias and algorithms and if

00:02:53,250 --> 00:02:58,740
you from last year in them investigating

00:02:56,190 --> 00:02:59,820
the compass algorithm that was used but

00:02:58,740 --> 00:03:02,400
that was developed by a company called

00:02:59,820 --> 00:03:03,720
North Point to help private prisons

00:03:02,400 --> 00:03:05,820
determine whether or not prisoners

00:03:03,720 --> 00:03:07,650
should get parole now the only reason

00:03:05,820 --> 00:03:09,660
because these are opaque models built by

00:03:07,650 --> 00:03:11,460
a private company the only reason we

00:03:09,660 --> 00:03:13,770
even know how these algorithms were in

00:03:11,460 --> 00:03:15,570
some way developed was because we

00:03:13,770 --> 00:03:17,070
actually had to have court cases that

00:03:15,570 --> 00:03:19,770
required them to release the

00:03:17,070 --> 00:03:21,990
documentation you know about what

00:03:19,770 --> 00:03:23,070
they've built so we are building these

00:03:21,990 --> 00:03:24,209
things where our people are building

00:03:23,070 --> 00:03:26,370
these things at scale

00:03:24,209 --> 00:03:29,190
they are building them not being totally

00:03:26,370 --> 00:03:30,870
mindful so again as I mentioned we

00:03:29,190 --> 00:03:33,959
wonder what future we're heading into

00:03:30,870 --> 00:03:35,610
and what we can do about it

00:03:33,959 --> 00:03:38,610
so I am going to get a little

00:03:35,610 --> 00:03:41,220
philosophical in this room so you know

00:03:38,610 --> 00:03:43,709
dirty secret I am a social scientist by

00:03:41,220 --> 00:03:45,780
background my my I'm a quantitative

00:03:43,709 --> 00:03:49,020
social scientist my PhD is in political

00:03:45,780 --> 00:03:50,850
science and I actually have always

00:03:49,020 --> 00:03:53,100
viewed the field of data science as a

00:03:50,850 --> 00:03:54,870
quantitative social science why because

00:03:53,100 --> 00:03:57,180
we take data and we understand human

00:03:54,870 --> 00:03:59,690
behavior the point is not the data the

00:03:57,180 --> 00:04:02,700
point is actually the human behavior and

00:03:59,690 --> 00:04:05,700
you know as we see these systems grow

00:04:02,700 --> 00:04:09,090
and perpetuate and in our eyes commit

00:04:05,700 --> 00:04:11,330
bad acts at scale we we wonder why and

00:04:09,090 --> 00:04:14,280
how and how we got here so this image

00:04:11,330 --> 00:04:15,300
you may recognize by looking at the

00:04:14,280 --> 00:04:18,950
crowd

00:04:15,300 --> 00:04:22,530
so post-world War two there was a lot of

00:04:18,950 --> 00:04:24,660
discomfort would be a way to put it

00:04:22,530 --> 00:04:25,080
among philosophers among human beings

00:04:24,660 --> 00:04:27,409
and

00:04:25,080 --> 00:04:30,330
just to think about how how entire

00:04:27,409 --> 00:04:32,909
Nations of individuals became complicit

00:04:30,330 --> 00:04:35,729
in genocide literally the genocide of

00:04:32,909 --> 00:04:39,900
their neighbors and a philosopher Hana

00:04:35,729 --> 00:04:42,990
armed wanted to understand you know what

00:04:39,900 --> 00:04:45,539
was in the minds of these people and

00:04:42,990 --> 00:04:46,949
what she did was go to the Nazi trials

00:04:45,539 --> 00:04:49,229
in Jerusalem you can read her book

00:04:46,949 --> 00:04:51,509
Eichmann in Jerusalem and she coins this

00:04:49,229 --> 00:04:54,479
term and she she she coins us to him the

00:04:51,509 --> 00:04:58,199
banality of evil and which what her

00:04:54,479 --> 00:05:00,090
discovery was in essence you know we we

00:04:58,199 --> 00:05:02,310
want there to we always want there to be

00:05:00,090 --> 00:05:05,280
a Hitler we want there to be a bad

00:05:02,310 --> 00:05:06,780
person a bad person who is the person in

00:05:05,280 --> 00:05:08,909
charge of all the bad things happening

00:05:06,780 --> 00:05:11,520
but bad people or bad actions are often

00:05:08,909 --> 00:05:14,310
just simply enabled by bureaucratic

00:05:11,520 --> 00:05:16,860
indifference or by masses of people who

00:05:14,310 --> 00:05:18,930
think that I may contribute to a system

00:05:16,860 --> 00:05:21,690
but it is not my fault because I am not

00:05:18,930 --> 00:05:24,210
the one who directly did this so and so

00:05:21,690 --> 00:05:26,280
as she's watching Adolf Eichmann and

00:05:24,210 --> 00:05:28,020
also people like in of Eichmann being

00:05:26,280 --> 00:05:31,319
interviewed by this court they they're

00:05:28,020 --> 00:05:33,150
genuinely surprised sometimes and their

00:05:31,319 --> 00:05:35,009
feedback is often well I just I just did

00:05:33,150 --> 00:05:39,240
my job you know I didn't really kill the

00:05:35,009 --> 00:05:40,560
Jews I just ordered gas tanks I just

00:05:39,240 --> 00:05:43,860
helped build fences

00:05:40,560 --> 00:05:47,279
I just patrolled outside concentration

00:05:43,860 --> 00:05:49,650
camps but I did not I was not the one

00:05:47,279 --> 00:05:52,590
who killed the Jews right so so all this

00:05:49,650 --> 00:05:54,900
is to say that evil requires systems of

00:05:52,590 --> 00:05:57,389
indifference evil requires people to

00:05:54,900 --> 00:05:59,430
quote just do their jobs be another cog

00:05:57,389 --> 00:06:01,590
in machine another step along the way

00:05:59,430 --> 00:06:05,729
but ultimately that enables something

00:06:01,590 --> 00:06:07,289
bad to happen downstream so how is this

00:06:05,729 --> 00:06:10,560
related to what we're talking about so

00:06:07,289 --> 00:06:12,690
let's let's revisit those headlines it

00:06:10,560 --> 00:06:15,539
was really interesting when we look at

00:06:12,690 --> 00:06:17,159
how we have linguistically semantically

00:06:15,539 --> 00:06:19,349
structured these headlines right and we

00:06:17,159 --> 00:06:21,599
say AI robots are sexist and racist and

00:06:19,349 --> 00:06:23,430
how to avoid racist algorithms

00:06:21,599 --> 00:06:26,400
the one thing you actually don't have

00:06:23,430 --> 00:06:28,650
here is a human being being mentioned we

00:06:26,400 --> 00:06:32,039
have linguistically erased the human

00:06:28,650 --> 00:06:34,830
being from responsibility somehow we

00:06:32,039 --> 00:06:37,139
have decided that these that these

00:06:34,830 --> 00:06:38,370
algorithms that we've built have the

00:06:37,139 --> 00:06:41,070
properties

00:06:38,370 --> 00:06:43,169
and we modify them linguistically as if

00:06:41,070 --> 00:06:46,050
they are real and they're alive we would

00:06:43,169 --> 00:06:48,419
never say my car or my laptop is sexist

00:06:46,050 --> 00:06:50,310
or racist and yet we ascribed this sort

00:06:48,419 --> 00:06:52,440
of behavior on to artificial

00:06:50,310 --> 00:06:54,720
intelligence systems and in doing so as

00:06:52,440 --> 00:06:56,610
I mentioned we write ourselves out of

00:06:54,720 --> 00:06:59,220
the equation and actually that's that's

00:06:56,610 --> 00:07:00,690
kind of on purpose right because it's

00:06:59,220 --> 00:07:02,460
sort of beneficial to us as programmers

00:07:00,690 --> 00:07:05,310
as developers of artificial intelligence

00:07:02,460 --> 00:07:07,560
if we are not mentioned in the sentence

00:07:05,310 --> 00:07:08,910
and we are not culpable we build

00:07:07,560 --> 00:07:10,949
something and then we say well the

00:07:08,910 --> 00:07:11,910
technology did it I I didn't do and

00:07:10,949 --> 00:07:14,760
especially with artificial intelligence

00:07:11,910 --> 00:07:16,350
this notion of you know learning from

00:07:14,760 --> 00:07:19,169
your environment and evolving and

00:07:16,350 --> 00:07:21,510
growing etc I do think we tend to over

00:07:19,169 --> 00:07:23,580
exaggerate the designer the engineers

00:07:21,510 --> 00:07:26,370
and the developers role in creating

00:07:23,580 --> 00:07:28,020
objective functions for AI but you've

00:07:26,370 --> 00:07:29,460
noticed how linguistically we've written

00:07:28,020 --> 00:07:32,550
ourselves out of the equation right so

00:07:29,460 --> 00:07:36,600
but why is that so I suppose beneficial

00:07:32,550 --> 00:07:38,940
for us so a few years ago of predictive

00:07:36,600 --> 00:07:40,199
police saying well even today it's more

00:07:38,940 --> 00:07:41,970
of a discussion but a few years ago

00:07:40,199 --> 00:07:44,250
there was a presentation on the use of

00:07:41,970 --> 00:07:46,800
predictive using algorithms to determine

00:07:44,250 --> 00:07:48,539
where to deploy police office so so in a

00:07:46,800 --> 00:07:52,380
nutshell why predictive policing is

00:07:48,539 --> 00:07:54,479
problematic our measurement of crime is

00:07:52,380 --> 00:07:56,190
not a true measurement of crime and is a

00:07:54,479 --> 00:07:58,889
measurement of the crime that has been

00:07:56,190 --> 00:08:00,750
picked up in a neighborhood which in the

00:07:58,889 --> 00:08:03,000
US and in other parts of the world is

00:08:00,750 --> 00:08:05,250
not often a funk funk ssin of like it's

00:08:03,000 --> 00:08:07,289
not randomly distributed there is

00:08:05,250 --> 00:08:09,780
pattern to it and that pattern is racist

00:08:07,289 --> 00:08:11,460
so when we create algorithms to

00:08:09,780 --> 00:08:13,710
determine where police officers should

00:08:11,460 --> 00:08:17,220
be deployed guess what we do we take

00:08:13,710 --> 00:08:18,960
that racist and incomplete data and send

00:08:17,220 --> 00:08:21,000
police officers to places where they

00:08:18,960 --> 00:08:23,370
will guess what do their jobs and arrest

00:08:21,000 --> 00:08:24,960
other people therefore increasing

00:08:23,370 --> 00:08:27,150
community harassment in other places

00:08:24,960 --> 00:08:28,740
while still systematically ignoring the

00:08:27,150 --> 00:08:30,180
places in which there may be an

00:08:28,740 --> 00:08:32,610
equivalent amount of crime but they just

00:08:30,180 --> 00:08:34,560
don't bother to go so all this is to say

00:08:32,610 --> 00:08:36,539
we had you know at one of the

00:08:34,560 --> 00:08:38,070
conferences a few years ago we had an

00:08:36,539 --> 00:08:40,050
engineer we had some folks on stage

00:08:38,070 --> 00:08:42,000
presenting a predictive policing

00:08:40,050 --> 00:08:43,890
algorithm and then somebody asked well

00:08:42,000 --> 00:08:46,320
aren't you worried about how this tool

00:08:43,890 --> 00:08:48,180
would be used and the scientist who is

00:08:46,320 --> 00:08:50,220
presenting said well you know I don't

00:08:48,180 --> 00:08:50,660
know I don't know how it be used I'm I'm

00:08:50,220 --> 00:08:53,029
just

00:08:50,660 --> 00:08:54,800
an engineer so at Accenture it's

00:08:53,029 --> 00:08:57,800
interesting because we face this problem

00:08:54,800 --> 00:08:59,959
all the time and and as a tech

00:08:57,800 --> 00:09:01,730
consulting company we are actually in

00:08:59,959 --> 00:09:04,250
the unique position to not just design

00:09:01,730 --> 00:09:05,930
the technology but also to start

00:09:04,250 --> 00:09:07,910
executing control over how it is

00:09:05,930 --> 00:09:09,230
deployed and used and I think that's a

00:09:07,910 --> 00:09:11,420
part of the equation that we as

00:09:09,230 --> 00:09:14,120
technologists have not quite figured out

00:09:11,420 --> 00:09:15,740
yet right we can create an algorithm we

00:09:14,120 --> 00:09:18,439
can or we can create and put data and

00:09:15,740 --> 00:09:20,089
create a model for a particular use but

00:09:18,439 --> 00:09:22,040
then the problem is once it is out there

00:09:20,089 --> 00:09:24,050
in the world once it is open source we

00:09:22,040 --> 00:09:27,110
cannot always control how it is used

00:09:24,050 --> 00:09:30,560
so moral outsourcing so again when we

00:09:27,110 --> 00:09:32,420
use terms like racist algorithm we erase

00:09:30,560 --> 00:09:34,879
human responsibility in our language we

00:09:32,420 --> 00:09:37,459
and we purposely anthropomorphize the AI

00:09:34,879 --> 00:09:38,959
we give it a face we give it a body even

00:09:37,459 --> 00:09:41,360
the most artificial intelligence doesn't

00:09:38,959 --> 00:09:43,129
look anything like that we do that in

00:09:41,360 --> 00:09:45,230
order to shift the negative consequences

00:09:43,129 --> 00:09:47,480
of the blame from the humans to the

00:09:45,230 --> 00:09:49,189
algorithm and here's the kicker the

00:09:47,480 --> 00:09:51,920
problem with moral outsourcing it

00:09:49,189 --> 00:09:54,170
actually ends up feeding our human

00:09:51,920 --> 00:09:56,240
paranoia so like if we walk to the

00:09:54,170 --> 00:09:59,029
narrative we have decided artificial

00:09:56,240 --> 00:10:00,740
intelligence is and the prime or fake it

00:09:59,029 --> 00:10:02,660
takes action on its own and has some

00:10:00,740 --> 00:10:04,790
sort of free will we have effectively

00:10:02,660 --> 00:10:07,009
written ourselves out of the language

00:10:04,790 --> 00:10:09,139
when we refer to them so racist and

00:10:07,009 --> 00:10:11,000
sexist algorithms and oh by the way the

00:10:09,139 --> 00:10:14,959
only time we refer to them this way is

00:10:11,000 --> 00:10:17,120
when they do bad things so when alphago

00:10:14,959 --> 00:10:19,490
defeated léa Seydoux we always make sure

00:10:17,120 --> 00:10:21,620
to mention it was deep minds alphago

00:10:19,490 --> 00:10:24,050
right so we we give the positive benefit

00:10:21,620 --> 00:10:25,670
to the creator's linguistically when it

00:10:24,050 --> 00:10:28,250
does a good thing or it's a win for

00:10:25,670 --> 00:10:30,009
Humanity one of the sexist and racist we

00:10:28,250 --> 00:10:33,470
all linguistically step away and

00:10:30,009 --> 00:10:35,120
paradoxically that creates our fear this

00:10:33,470 --> 00:10:36,769
is what leads to our Terminator imagery

00:10:35,120 --> 00:10:39,019
this is what leads to us thinking that a

00:10:36,769 --> 00:10:40,970
physical robot will actually take our

00:10:39,019 --> 00:10:43,130
physical jobs and stand in the store in

00:10:40,970 --> 00:10:46,399
stock shelves or whatever right as as

00:10:43,130 --> 00:10:47,930
ridiculous as this may sound so what can

00:10:46,399 --> 00:10:49,819
we do and here's where I talk a little

00:10:47,930 --> 00:10:51,470
bit about government's which I know is

00:10:49,819 --> 00:10:54,829
an interest of a lot of the community in

00:10:51,470 --> 00:10:57,840
this room so you may have known her

00:10:54,829 --> 00:11:00,060
Daffy you weeks ago about open AI

00:10:57,840 --> 00:11:02,310
creating a natural language language

00:11:00,060 --> 00:11:04,320
generation algorithm that they felt was

00:11:02,310 --> 00:11:06,270
so potentially harmful they did not

00:11:04,320 --> 00:11:08,360
release it and I will tell you that

00:11:06,270 --> 00:11:10,500
actually the ethics and AI community

00:11:08,360 --> 00:11:12,870
largely our response actually this

00:11:10,500 --> 00:11:14,250
article was about the response of us

00:11:12,870 --> 00:11:15,930
pretty much saying who the hell do you

00:11:14,250 --> 00:11:17,580
think you are right because good

00:11:15,930 --> 00:11:18,960
governance does not come from hiding

00:11:17,580 --> 00:11:21,990
good governance does not come from

00:11:18,960 --> 00:11:23,640
unilaterally deciding that you are the

00:11:21,990 --> 00:11:25,140
person who is the arbiter of what should

00:11:23,640 --> 00:11:27,750
and should not be released and that was

00:11:25,140 --> 00:11:30,060
you know and with all respect to open AI

00:11:27,750 --> 00:11:31,590
know their intention was good but the

00:11:30,060 --> 00:11:33,210
response in the community was I don't

00:11:31,590 --> 00:11:35,610
think that was the best thing for you to

00:11:33,210 --> 00:11:37,320
do announce that you've built this thing

00:11:35,610 --> 00:11:39,480
that was so potentially harmful but oh

00:11:37,320 --> 00:11:43,380
by the way we're not gonna let you know

00:11:39,480 --> 00:11:45,210
what it is and this is a struggle that

00:11:43,380 --> 00:11:47,370
the ethics and AI community largely is

00:11:45,210 --> 00:11:51,180
having which is why political scientists

00:11:47,370 --> 00:11:53,070
beneficial right if business leaders and

00:11:51,180 --> 00:11:55,430
technologists try to create governance

00:11:53,070 --> 00:11:59,130
and democracy it will get up

00:11:55,430 --> 00:12:02,370
because often it is viewed as being this

00:11:59,130 --> 00:12:04,590
top-down methodology where we have will

00:12:02,370 --> 00:12:07,680
have the leaders at the top who will

00:12:04,590 --> 00:12:10,380
create rules and said rules will get

00:12:07,680 --> 00:12:12,780
enforced but actually like for people

00:12:10,380 --> 00:12:15,060
who have studied democratic systems that

00:12:12,780 --> 00:12:17,400
is not actually how don't like

00:12:15,060 --> 00:12:18,960
democratic countries work we operate by

00:12:17,400 --> 00:12:20,790
a rule of law and a rule of laws of

00:12:18,960 --> 00:12:23,010
social contract so this is kind of the

00:12:20,790 --> 00:12:24,600
equivalent of when you're going up an

00:12:23,010 --> 00:12:26,520
escalator why do people stand on the

00:12:24,600 --> 00:12:28,590
right and go and walk up the left this

00:12:26,520 --> 00:12:31,800
is just an implicit rule you will not

00:12:28,590 --> 00:12:33,090
get fined if you don't do it like people

00:12:31,800 --> 00:12:34,410
will give you dirty looks and someone

00:12:33,090 --> 00:12:36,840
probably tell you to move over but it is

00:12:34,410 --> 00:12:39,930
socially enforced it is actually not

00:12:36,840 --> 00:12:42,240
regulated so why do we do it because we

00:12:39,930 --> 00:12:43,710
as a community as a society have

00:12:42,240 --> 00:12:45,630
actually agreed that this is the best

00:12:43,710 --> 00:12:46,440
way to do things to make society run

00:12:45,630 --> 00:12:48,570
more efficiently

00:12:46,440 --> 00:12:51,360
so let's take this notion and put it

00:12:48,570 --> 00:12:53,400
into our our worlds right so how do we

00:12:51,360 --> 00:12:55,770
create good governance of AI systems

00:12:53,400 --> 00:12:57,270
well one telling the world you built a

00:12:55,770 --> 00:13:02,070
terrible thing and hiding it probably

00:12:57,270 --> 00:13:05,850
not the best way to do it right so there

00:13:02,070 --> 00:13:07,530
you go there you so what can we do one

00:13:05,850 --> 00:13:09,600
is community led governance which I

00:13:07,530 --> 00:13:09,960
appreciate is much much much easier said

00:13:09,600 --> 00:13:11,670
than done

00:13:09,960 --> 00:13:13,650
so we are actually the next

00:13:11,670 --> 00:13:15,540
two weeks releasing a governance toolkit

00:13:13,650 --> 00:13:18,510
and and again as I mentioned you know

00:13:15,540 --> 00:13:20,010
the I think the there's a naive way that

00:13:18,510 --> 00:13:21,780
people are looking at these things to

00:13:20,010 --> 00:13:23,760
think that the powers that be will

00:13:21,780 --> 00:13:25,440
create a tone from the top and then

00:13:23,760 --> 00:13:28,500
it'll kind of filter down like trickle

00:13:25,440 --> 00:13:30,360
down ethics and what we have added in

00:13:28,500 --> 00:13:32,880
our toolkit is something I called

00:13:30,360 --> 00:13:35,070
constructive dissent right where at work

00:13:32,880 --> 00:13:36,780
one of the biggest problems we're seeing

00:13:35,070 --> 00:13:38,640
in the tech and tech companies and there

00:13:36,780 --> 00:13:41,460
are many reasons we have employee

00:13:38,640 --> 00:13:43,020
whistleblowers walkouts etc but one of

00:13:41,460 --> 00:13:45,240
those I strongly believe is that we

00:13:43,020 --> 00:13:46,740
actually don't have good channels of

00:13:45,240 --> 00:13:49,830
dissent within organizations

00:13:46,740 --> 00:13:52,040
we do not incentivize people to take the

00:13:49,830 --> 00:13:54,390
right actions and do the right things

00:13:52,040 --> 00:13:56,010
structurally we also don't have the

00:13:54,390 --> 00:13:58,320
right kinds of channels of people to

00:13:56,010 --> 00:14:01,620
speak up and so and to feel safe

00:13:58,320 --> 00:14:04,980
speaking up so you know in the media in

00:14:01,620 --> 00:14:07,350
general there's an over an inordinate

00:14:04,980 --> 00:14:10,230
amount of weight placed on the power of

00:14:07,350 --> 00:14:13,260
a data scientist and having been a entry

00:14:10,230 --> 00:14:14,670
level data scientist myself we don't we

00:14:13,260 --> 00:14:16,530
don't have a lot of power we have

00:14:14,670 --> 00:14:19,170
deadlines to meet we have datasets to

00:14:16,530 --> 00:14:22,470
clean I mean we're you know yes we do

00:14:19,170 --> 00:14:25,320
make some sorts of decisions but we are

00:14:22,470 --> 00:14:27,420
not we're not gods we are answerable to

00:14:25,320 --> 00:14:29,100
other people so what we actually need to

00:14:27,420 --> 00:14:30,720
do is create governance from the bottom

00:14:29,100 --> 00:14:32,490
up and then cultural norms of dissent

00:14:30,720 --> 00:14:33,750
this is a really important thing and

00:14:32,490 --> 00:14:35,970
this is this is why I raised the whole

00:14:33,750 --> 00:14:37,350
concept of the escalator and sort of

00:14:35,970 --> 00:14:39,750
standing on the right and walking up the

00:14:37,350 --> 00:14:42,420
left right we actually do need norms in

00:14:39,750 --> 00:14:44,250
our own community so what are the other

00:14:42,420 --> 00:14:45,600
other communities we can look at well we

00:14:44,250 --> 00:14:48,120
can look at the bioethics community

00:14:45,600 --> 00:14:49,860
right in the 90s when all of this talk

00:14:48,120 --> 00:14:51,540
of genetic testing was going on and

00:14:49,860 --> 00:14:54,750
there was there were decisions being

00:14:51,540 --> 00:14:56,640
made about not doing testing on human

00:14:54,750 --> 00:14:59,460
embryos etc which I suppose has been

00:14:56,640 --> 00:15:02,220
broken but it was fascinating to see

00:14:59,460 --> 00:15:05,340
that globally you know this sort of

00:15:02,220 --> 00:15:07,170
agreement come to an agreement on this

00:15:05,340 --> 00:15:09,390
cultural norm of what we should or

00:15:07,170 --> 00:15:11,760
should not do right and we don't

00:15:09,390 --> 00:15:13,500
actually have that culture among data

00:15:11,760 --> 00:15:16,410
scientists and and to be fair it is

00:15:13,500 --> 00:15:18,420
actually quite different from a more

00:15:16,410 --> 00:15:20,700
centralized barrier to entry type

00:15:18,420 --> 00:15:22,590
community like biology or physics and it

00:15:20,700 --> 00:15:24,110
is with with data science because what

00:15:22,590 --> 00:15:26,330
we struggle with here

00:15:24,110 --> 00:15:28,459
is this desire and this need to

00:15:26,330 --> 00:15:30,560
democratize this education the skill set

00:15:28,459 --> 00:15:32,089
which is very very valuable right so we

00:15:30,560 --> 00:15:34,160
do want to make tools open those we do

00:15:32,089 --> 00:15:36,890
want to make algorithms open-source at

00:15:34,160 --> 00:15:40,250
the same time how do we take the

00:15:36,890 --> 00:15:42,709
responsibility for potential misuse who

00:15:40,250 --> 00:15:45,200
is responsible for it and how do we not

00:15:42,709 --> 00:15:47,450
then recreate this Hana Arendt type

00:15:45,200 --> 00:15:49,149
world you know or are our fears from

00:15:47,450 --> 00:15:52,130
World War two where we have all

00:15:49,149 --> 00:15:54,709
inadvertently actually led to a very bad

00:15:52,130 --> 00:15:56,300
outcome and that that's really the

00:15:54,709 --> 00:15:57,829
concern and really governance is the

00:15:56,300 --> 00:15:59,899
best answer for it and again

00:15:57,829 --> 00:16:01,579
community-led governance from the bottom

00:15:59,899 --> 00:16:03,350
up and most importantly creating

00:16:01,579 --> 00:16:06,140
cultural norms of dissent and cultural

00:16:03,350 --> 00:16:07,700
norms of compliance I think it's quite

00:16:06,140 --> 00:16:11,360
important in our own world in our own

00:16:07,700 --> 00:16:13,700
community to really ask people to to do

00:16:11,360 --> 00:16:16,160
the right thing and to reward doing the

00:16:13,700 --> 00:16:17,930
right thing and you know and again like

00:16:16,160 --> 00:16:19,760
in in sort of rank-and-file private

00:16:17,930 --> 00:16:21,740
organization data science we don't

00:16:19,760 --> 00:16:24,470
actually have those norms yet and while

00:16:21,740 --> 00:16:27,019
we may love this idea of the you know

00:16:24,470 --> 00:16:30,079
data science is a bit of like a like a

00:16:27,019 --> 00:16:32,690
Wild West kind of crazy person on the on

00:16:30,079 --> 00:16:35,000
the fringes out there kind of job there

00:16:32,690 --> 00:16:38,060
is something to be said about standards

00:16:35,000 --> 00:16:39,649
something to be said about creating

00:16:38,060 --> 00:16:41,390
auditability and traceability of

00:16:39,649 --> 00:16:43,610
algorithms of decision making because

00:16:41,390 --> 00:16:45,829
ultimately somebody has to be held held

00:16:43,610 --> 00:16:47,899
accountable there there is just

00:16:45,829 --> 00:16:50,450
significant issues with bias significant

00:16:47,899 --> 00:16:51,740
issues with them you know with with

00:16:50,450 --> 00:16:54,500
algorithms and artificial intelligence

00:16:51,740 --> 00:16:56,510
being put out there today so my ask of

00:16:54,500 --> 00:16:59,149
you the open source community is how do

00:16:56,510 --> 00:17:00,829
we start creating these norms how do we

00:16:59,149 --> 00:17:01,940
start cultivating this community no

00:17:00,829 --> 00:17:03,740
there's plenty of people in this room

00:17:01,940 --> 00:17:04,939
that have been working on this kind of

00:17:03,740 --> 00:17:06,679
thing that happened working on this kind

00:17:04,939 --> 00:17:09,020
of thing I personally would actually

00:17:06,679 --> 00:17:11,360
really love to engage with you as we try

00:17:09,020 --> 00:17:13,640
to build out these norms in corporations

00:17:11,360 --> 00:17:15,770
and at the enterprise level because it

00:17:13,640 --> 00:17:17,089
is it is sorely needed not just in

00:17:15,770 --> 00:17:18,230
technology companies you know the

00:17:17,089 --> 00:17:20,120
companies that are actually the most

00:17:18,230 --> 00:17:22,069
interested in this are the non

00:17:20,120 --> 00:17:24,470
technology companies retail

00:17:22,069 --> 00:17:27,050
organizations public sector banks as

00:17:24,470 --> 00:17:29,090
they have used more and more artificial

00:17:27,050 --> 00:17:31,340
intelligence they're actually kind of

00:17:29,090 --> 00:17:34,400
scared of our world because we don't

00:17:31,340 --> 00:17:36,890
have norms and they tend to we don't

00:17:34,400 --> 00:17:38,480
tend to follow rules so how do we

00:17:36,890 --> 00:17:40,640
create this culture how do we create an

00:17:38,480 --> 00:17:42,620
ethical culture that can then actually

00:17:40,640 --> 00:17:44,870
be permeated beyond technologists into

00:17:42,620 --> 00:17:46,370
other other parts of Industry and really

00:17:44,870 --> 00:17:48,680
achieved the artificial intelligence

00:17:46,370 --> 00:17:51,290
world that we're all trying to build so

00:17:48,680 --> 00:17:53,000
with that my time is up if you have some

00:17:51,290 --> 00:17:59,000
time follow me on Twitter or you can

00:17:53,000 --> 00:18:01,640
check out my website thank you thank you

00:17:59,000 --> 00:18:04,160
you know it's funny in the open-source

00:18:01,640 --> 00:18:06,430
community for a long time one of the

00:18:04,160 --> 00:18:09,080
community norms that is important to

00:18:06,430 --> 00:18:11,450
coders is the idea of complying with

00:18:09,080 --> 00:18:13,340
licenses so licenses are like a big part

00:18:11,450 --> 00:18:14,810
of the open-source community there's

00:18:13,340 --> 00:18:16,970
copyleft license that's permissive

00:18:14,810 --> 00:18:19,010
licenses particularly in linux which I

00:18:16,970 --> 00:18:21,710
was involved in early on people would

00:18:19,010 --> 00:18:23,300
kind of flout the license right I've

00:18:21,710 --> 00:18:25,070
particularly companies who would use it

00:18:23,300 --> 00:18:28,520
maybe through ignorance sometimes

00:18:25,070 --> 00:18:30,380
intentionally and the way we help create

00:18:28,520 --> 00:18:32,630
that norm around the license was

00:18:30,380 --> 00:18:35,090
explaining the value of complying with

00:18:32,630 --> 00:18:36,860
it in a business sense like here's the

00:18:35,090 --> 00:18:38,180
business value that's how we kind of

00:18:36,860 --> 00:18:40,550
started changing a bit like it's been

00:18:38,180 --> 00:18:42,080
like a huge experiment keep working on

00:18:40,550 --> 00:18:44,510
it and there actually is an initiative

00:18:42,080 --> 00:18:47,600
to create a responsible AI license I

00:18:44,510 --> 00:18:49,190
think it's like um some folks at IBM and

00:18:47,600 --> 00:18:50,630
some of some other put in there it's

00:18:49,190 --> 00:18:51,980
sort of in the works oh I think they're

00:18:50,630 --> 00:18:53,330
actually we actually are kind of headed

00:18:51,980 --> 00:18:55,760
in that direction if we can help in any

00:18:53,330 --> 00:18:58,210
way please let us know oh cool thank you

00:18:55,760 --> 00:18:58,210

YouTube URL: https://www.youtube.com/watch?v=l-pXqQh86aU


