Title: Keynote: AI Owes its Rebirth to Open Source and the Cloud - Mark Russinovich, CTO, Microsoft Azure
Publication date: 2018-03-07
Playlist: Open Source Leadership Summit 2018
Description: 
	Keynote: AI Owes its Rebirth to Open Source and the Cloud - Mark Russinovich, CTO, Microsoft Azure, Microsoft 

AI technologies and techniques are experiencing a renaissance. Open source technologies and communities have fostered the growth of self-taught machine learning developers with libraries and frameworks. The computing power of the cloud has made the processing of large data sets cost effective and commonplace. As more research continues to be done and shared throughout the communities we will continue to see more intelligent apps driving even greater adoption of open source technologies across all processing platforms.

About Mark Russinovich
Mark Russinovich is Chief Technology Officer for Microsoft Azure, Microsoft’s global enterprise-grade cloud platform. A widely recognized expert in distributed systems and operating systems, Mark earned a Ph.D. in computer engineering from Carnegie Mellon University. He later co-founded Winternals Software, joining Microsoft in 2006 when the company was acquired. Today he remains the primary author of the Sysinternals tools and website, which include dozens of popular Windows administration and diagnostic utilities. Mark is a popular speaker at industry conferences such as IPExpo, Microsoft Ignite and Build, and RSA Conference. He has also authored several nonfiction and fiction books, including the Microsoft Press Windows Internals book series, as well as fictional cyber security thrillers Zero Day, Trojan Horse and Rogue Code.
Captions: 
	00:00:00,030 --> 00:00:05,040
so it's thrilled to be here I been

00:00:02,669 --> 00:00:06,359
worked so as Jim said the first part of

00:00:05,040 --> 00:00:09,030
my career actually was working on

00:00:06,359 --> 00:00:11,010
Windows and so it's thrilled to be here

00:00:09,030 --> 00:00:13,500
keynoting at an open source Leadership

00:00:11,010 --> 00:00:15,750
Summit representing Microsoft of all

00:00:13,500 --> 00:00:17,699
things it's fantastic times that we're

00:00:15,750 --> 00:00:19,470
in now the talk this morning that I've

00:00:17,699 --> 00:00:22,590
got for you is to talk a little bit

00:00:19,470 --> 00:00:25,949
about how open-source and cloud have

00:00:22,590 --> 00:00:29,099
converged to really drive this the surge

00:00:25,949 --> 00:00:30,929
of AI and ml that we see and I'll start

00:00:29,099 --> 00:00:33,090
by talking about some of the examples

00:00:30,929 --> 00:00:35,430
that we've already seen in Asscher with

00:00:33,090 --> 00:00:37,440
some of our customers applying AI n ml

00:00:35,430 --> 00:00:39,780
and it shows you just kind of the

00:00:37,440 --> 00:00:41,850
difference that AI ml can make to

00:00:39,780 --> 00:00:44,070
businesses the first case here is

00:00:41,850 --> 00:00:45,510
rolls-royce you might be familiar with

00:00:44,070 --> 00:00:48,000
rolls-royce from the cars they make but

00:00:45,510 --> 00:00:50,010
they also make aircraft engines and if

00:00:48,000 --> 00:00:52,440
you take a link about what an aircraft

00:00:50,010 --> 00:00:54,090
engine is for a second it's a giant IOT

00:00:52,440 --> 00:00:57,750
device it's got sensors all over at

00:00:54,090 --> 00:01:01,559
pumping huge amounts of data and before

00:00:57,750 --> 00:01:02,789
the advent of cloud and AI capabilities

00:01:01,559 --> 00:01:05,820
it's really hard to make use of that

00:01:02,789 --> 00:01:07,350
data and the kinds of use you can make

00:01:05,820 --> 00:01:09,240
out of that data include things like

00:01:07,350 --> 00:01:11,640
fuel efficiency and predictive

00:01:09,240 --> 00:01:13,229
maintenance and so rolls-royce has built

00:01:11,640 --> 00:01:15,600
a platform where that data goes up into

00:01:13,229 --> 00:01:17,159
the cloud they apply AI and ml to it and

00:01:15,600 --> 00:01:19,200
they can predict the failure of an

00:01:17,159 --> 00:01:22,619
aircraft engine if an aircraft engine

00:01:19,200 --> 00:01:25,170
fails and parts need to arrive at the

00:01:22,619 --> 00:01:27,060
airport where it's failed that costs an

00:01:25,170 --> 00:01:29,369
airline about a million dollars a day so

00:01:27,060 --> 00:01:30,479
just buying able to predict win an

00:01:29,369 --> 00:01:32,040
engines gonna fail and have the

00:01:30,479 --> 00:01:34,740
maintenance happen before that happens

00:01:32,040 --> 00:01:37,020
or at least have the parts in place so

00:01:34,740 --> 00:01:38,430
that when it fails it can be serviced

00:01:37,020 --> 00:01:41,189
more quickly can save a huge amount of

00:01:38,430 --> 00:01:43,350
money and they also use a nml to track

00:01:41,189 --> 00:01:44,880
the efficiency of the engine in under

00:01:43,350 --> 00:01:48,570
various conditions of humidity and

00:01:44,880 --> 00:01:50,100
altitude and by using that they can

00:01:48,570 --> 00:01:52,979
optimize the fuel efficiency of an

00:01:50,100 --> 00:01:55,979
airline flight and guide the pilot to be

00:01:52,979 --> 00:02:00,360
able to save fuel an airline spends

00:01:55,979 --> 00:02:03,090
about 40% of its income on fuel so just

00:02:00,360 --> 00:02:04,680
a few percentage points of additional

00:02:03,090 --> 00:02:06,990
fuel efficiency can be millions of

00:02:04,680 --> 00:02:09,119
dollars a year for an airline the second

00:02:06,990 --> 00:02:10,319
case is actually a Microsoft case this

00:02:09,119 --> 00:02:13,080
is us

00:02:10,319 --> 00:02:13,620
exploring the possibilities of a IML and

00:02:13,080 --> 00:02:15,239
this was

00:02:13,620 --> 00:02:17,610
inspired this project here called

00:02:15,239 --> 00:02:21,120
diagnostic X inspired by a project at

00:02:17,610 --> 00:02:24,409
Stanford called checks net where we've

00:02:21,120 --> 00:02:26,879
taken a dataset NH curated data set of

00:02:24,409 --> 00:02:29,510
radiological images chest images and

00:02:26,879 --> 00:02:34,140
have applied AI MML to identify

00:02:29,510 --> 00:02:35,970
pneumonia to be able to train it to take

00:02:34,140 --> 00:02:38,940
a look at a new image and determine if

00:02:35,970 --> 00:02:42,230
there's a diagnosis likely of pneumonia

00:02:38,940 --> 00:02:46,170
and why is pneumonia such a focus well

00:02:42,230 --> 00:02:48,180
around the world pneumonia is the number

00:02:46,170 --> 00:02:51,540
one killer for children under five years

00:02:48,180 --> 00:02:53,250
of age about 16 percent of the deaths of

00:02:51,540 --> 00:02:55,230
children under 5 years of age are caused

00:02:53,250 --> 00:02:58,440
by pneumonia and us alone 50,000 kids

00:02:55,230 --> 00:03:01,079
die each year from pneumonia and two

00:02:58,440 --> 00:03:03,060
thirds of the world's population even

00:03:01,079 --> 00:03:06,060
despite having access to imaging devices

00:03:03,060 --> 00:03:08,730
do not have access to radiologists so if

00:03:06,060 --> 00:03:10,890
a nml can provide a diagnosis for a

00:03:08,730 --> 00:03:13,500
doctor physician that can potentially

00:03:10,890 --> 00:03:15,780
save a child's life so by taking a look

00:03:13,500 --> 00:03:19,950
at this a dataset we've been able to

00:03:15,780 --> 00:03:21,889
train it to identify pneumonia and so

00:03:19,950 --> 00:03:25,530
this is just another example of

00:03:21,889 --> 00:03:28,109
application of AI and ml the first case

00:03:25,530 --> 00:03:31,190
here on the right I was over there on

00:03:28,109 --> 00:03:34,620
the right side and this is the

00:03:31,190 --> 00:03:36,209
predictive maintenance use case the

00:03:34,620 --> 00:03:38,430
health use case that I just talked about

00:03:36,209 --> 00:03:39,900
is more one of the image recognition but

00:03:38,430 --> 00:03:43,049
you can see other use cases here and

00:03:39,900 --> 00:03:44,699
this shows you that AI ml developers are

00:03:43,049 --> 00:03:46,919
targeting a ride variety of use cases

00:03:44,699 --> 00:03:49,109
this is from a survey from slash data of

00:03:46,919 --> 00:03:51,359
a bunch of AI ml developers and what

00:03:49,109 --> 00:03:53,190
kind of algorithms what kind of use

00:03:51,359 --> 00:03:55,290
cases they're targeting you can see

00:03:53,190 --> 00:03:56,760
everything from analysis and prediction

00:03:55,290 --> 00:03:59,129
a customer behavior which is number one

00:03:56,760 --> 00:04:00,449
applies to every single industry all the

00:03:59,129 --> 00:04:01,980
way down to the ones that I've talked

00:04:00,449 --> 00:04:03,720
about that natural language processing

00:04:01,980 --> 00:04:06,060
speech recognition artificial

00:04:03,720 --> 00:04:09,030
intelligence and games being one so this

00:04:06,060 --> 00:04:12,629
AI ml phenomenon has really already

00:04:09,030 --> 00:04:15,209
permeated just about every industry in

00:04:12,629 --> 00:04:17,190
fact if you take a look there's a font

00:04:15,209 --> 00:04:19,500
problem on this slide but if you take a

00:04:17,190 --> 00:04:21,810
look at the most promising jobs of 2018

00:04:19,500 --> 00:04:25,890
so LinkedIn a company that I happen to

00:04:21,810 --> 00:04:27,540
like is publishes every year the most

00:04:25,890 --> 00:04:30,270
in-demand skills

00:04:27,540 --> 00:04:32,550
and for the last since 2014 the number

00:04:30,270 --> 00:04:34,710
one skill has been statistical analysis

00:04:32,550 --> 00:04:37,050
and data mining surpassed only this year

00:04:34,710 --> 00:04:39,740
by cloud but this shows you that this

00:04:37,050 --> 00:04:42,120
trend has been going on for some time

00:04:39,740 --> 00:04:44,070
now what's been powering this trend like

00:04:42,120 --> 00:04:45,780
I said cloud has a big part of it

00:04:44,070 --> 00:04:47,910
open-source has a big part of it

00:04:45,780 --> 00:04:49,260
hardware has a big part of it data has a

00:04:47,910 --> 00:04:51,390
big part of it and so let's break that

00:04:49,260 --> 00:04:53,010
down a little bit if you take a look at

00:04:51,390 --> 00:04:55,920
what cloud represents it's on demand

00:04:53,010 --> 00:04:58,110
compute and if you've got unto man

00:04:55,920 --> 00:05:01,160
compute what that promotes is

00:04:58,110 --> 00:05:04,350
experimentation and AI and ml is really

00:05:01,160 --> 00:05:05,340
requires it's an art at this point and

00:05:04,350 --> 00:05:07,350
it really requires a lot of

00:05:05,340 --> 00:05:08,520
experimentation if you've got a fixed

00:05:07,350 --> 00:05:10,440
amount of hardware if it's very hard to

00:05:08,520 --> 00:05:12,120
get access to hardware resources that's

00:05:10,440 --> 00:05:15,150
going to inhibit the ability to

00:05:12,120 --> 00:05:17,310
experiment when you got cloud on demand

00:05:15,150 --> 00:05:19,380
instant scale you can go experiment

00:05:17,310 --> 00:05:20,490
where you run train your models try

00:05:19,380 --> 00:05:22,440
different algorithms try different

00:05:20,490 --> 00:05:24,260
parameters and really tune something to

00:05:22,440 --> 00:05:28,470
be able to solve real world problems

00:05:24,260 --> 00:05:30,690
ai nml is fuelled by data and in the

00:05:28,470 --> 00:05:32,820
past data was relatively expensive so

00:05:30,690 --> 00:05:34,160
most companies were very careful about

00:05:32,820 --> 00:05:36,450
what types of data they kept

00:05:34,160 --> 00:05:38,940
unfortunately if they didn't keep long

00:05:36,450 --> 00:05:40,470
enough periods of data or if they threw

00:05:38,940 --> 00:05:42,090
away the data that might be the features

00:05:40,470 --> 00:05:43,590
that were most important for solving a

00:05:42,090 --> 00:05:45,060
particular problem then they couldn't

00:05:43,590 --> 00:05:47,490
have an efficient solution but with the

00:05:45,060 --> 00:05:50,270
advent of cloud storage extremely cheap

00:05:47,490 --> 00:05:52,650
now large data sets can be collected and

00:05:50,270 --> 00:05:54,120
insights can be gained at a later point

00:05:52,650 --> 00:05:56,070
in time even if you've got data and you

00:05:54,120 --> 00:05:57,660
might not be sure about what kind of

00:05:56,070 --> 00:05:59,250
insights you can gain from it you just

00:05:57,660 --> 00:06:01,740
store it and later you might come up

00:05:59,250 --> 00:06:04,290
with some way to make use of it and then

00:06:01,740 --> 00:06:06,780
finally we've seen a huge surge in

00:06:04,290 --> 00:06:08,880
hardware capability and hardware

00:06:06,780 --> 00:06:10,410
capability has really brought in to the

00:06:08,880 --> 00:06:12,660
realm of possibility solutions for many

00:06:10,410 --> 00:06:16,650
types of problems if you take a look at

00:06:12,660 --> 00:06:19,590
most a IML training it's done on GPUs

00:06:16,650 --> 00:06:22,200
and this graph here from Nvidia shows

00:06:19,590 --> 00:06:25,920
you the improvement in GPU capability

00:06:22,200 --> 00:06:28,110
it's really around 2010 over on the left

00:06:25,920 --> 00:06:29,700
side of the graph that GPUs became

00:06:28,110 --> 00:06:31,830
powerful enough to make deep neural

00:06:29,700 --> 00:06:33,300
networks of any complexity practical and

00:06:31,830 --> 00:06:36,270
you can see that we've gone up in

00:06:33,300 --> 00:06:39,419
roughly 10x in performance capability of

00:06:36,270 --> 00:06:41,430
GPUs today versus then and that opens

00:06:39,419 --> 00:06:43,740
the door to solving many more types of

00:06:41,430 --> 00:06:47,760
in a much more in a much shorter amount

00:06:43,740 --> 00:06:50,070
of time than was possible before if you

00:06:47,760 --> 00:06:52,139
take a look at cloud and you take a look

00:06:50,070 --> 00:06:54,690
at massive processing massive datasets

00:06:52,139 --> 00:06:56,190
you need a great network to shuttle data

00:06:54,690 --> 00:06:58,650
back and forth between the data sources

00:06:56,190 --> 00:07:00,570
and between the GPUs and even in between

00:06:58,650 --> 00:07:03,210
GPUs for a distributed neural network

00:07:00,570 --> 00:07:04,830
for example so the network is extremely

00:07:03,210 --> 00:07:06,930
important and we've seen huge

00:07:04,830 --> 00:07:09,300
innovations in networking in the data

00:07:06,930 --> 00:07:12,479
center this graph right here shows you

00:07:09,300 --> 00:07:14,880
for a data center the amount of

00:07:12,479 --> 00:07:17,160
bandwidth out of the whole data centers

00:07:14,880 --> 00:07:20,039
bandwidth that an individual server has

00:07:17,160 --> 00:07:25,139
over time so if you take a look for

00:07:20,039 --> 00:07:27,389
example in 2017 2016 40 gigabit net mix

00:07:25,139 --> 00:07:28,800
were added became standard on data

00:07:27,389 --> 00:07:31,080
center networks and you see a jump in

00:07:28,800 --> 00:07:35,250
the amount of bandwidth that can come

00:07:31,080 --> 00:07:37,169
from a individual server has as part of

00:07:35,250 --> 00:07:39,840
that overall data center network and

00:07:37,169 --> 00:07:41,340
what this means ultimately is you've got

00:07:39,840 --> 00:07:42,780
a 40 gigabit NIC you can see that the

00:07:41,340 --> 00:07:44,789
amount of bandwidth it's got is around 6

00:07:42,780 --> 00:07:46,349
gigabits that shows you that the

00:07:44,789 --> 00:07:48,240
oversubscription across the whole

00:07:46,349 --> 00:07:49,770
network continues to drop despite the

00:07:48,240 --> 00:07:52,650
fact that the individual bandwidth off a

00:07:49,770 --> 00:07:54,120
server continues to grow in other words

00:07:52,650 --> 00:07:55,590
the network is becoming flatter and

00:07:54,120 --> 00:07:58,410
flatter and bandwidth is becoming

00:07:55,590 --> 00:08:00,030
effectively free and so this is what's

00:07:58,410 --> 00:08:04,080
going to promote the processing of

00:08:00,030 --> 00:08:07,530
massive data sets and of course open

00:08:04,080 --> 00:08:09,060
source has a big role to play in this if

00:08:07,530 --> 00:08:10,470
you take a look at the kinds of

00:08:09,060 --> 00:08:12,510
technologies that are involved with data

00:08:10,470 --> 00:08:15,449
processing ml it starts with data

00:08:12,510 --> 00:08:17,550
storage platforms like sequel servers

00:08:15,449 --> 00:08:18,870
you see my sequel and post grasp the

00:08:17,550 --> 00:08:20,669
rise of those over the last 10 years

00:08:18,870 --> 00:08:22,620
open source based platforms

00:08:20,669 --> 00:08:25,530
Kassandra they're no sequel huge rides

00:08:22,620 --> 00:08:27,240
in that although that whole ecosystem no

00:08:25,530 --> 00:08:30,120
sequel has been entirely open-source

00:08:27,240 --> 00:08:31,349
driven the analytics and forecasting

00:08:30,120 --> 00:08:34,110
tools I've just got a couple examples

00:08:31,349 --> 00:08:36,320
there spark open source are is open

00:08:34,110 --> 00:08:38,669
source and if you take a look at the

00:08:36,320 --> 00:08:40,469
development languages and tools there

00:08:38,669 --> 00:08:42,150
I've got xamarin and I've got node and

00:08:40,469 --> 00:08:44,130
I've got Python and those are all

00:08:42,150 --> 00:08:47,250
largely open source driven by open

00:08:44,130 --> 00:08:48,990
source a couple of examples specific to

00:08:47,250 --> 00:08:51,890
what we see happening on trends in AI

00:08:48,990 --> 00:08:54,480
and ml the use of things of frameworks

00:08:51,890 --> 00:08:55,230
like C NT K which has come out of

00:08:54,480 --> 00:08:57,540
Microsoft this

00:08:55,230 --> 00:09:01,800
is a confident confident convolutional

00:08:57,540 --> 00:09:04,440
newer network CNN optimized platform for

00:09:01,800 --> 00:09:06,720
deep neural networks it's completely

00:09:04,440 --> 00:09:08,550
open source next to that is tensor flow

00:09:06,720 --> 00:09:12,660
which has come out of Google open source

00:09:08,550 --> 00:09:14,400
MX net which is a framework that's a

00:09:12,660 --> 00:09:17,490
collaboration between Facebook Microsoft

00:09:14,400 --> 00:09:19,380
and AWS showing a community of companies

00:09:17,490 --> 00:09:22,560
working on an open-source platform this

00:09:19,380 --> 00:09:24,780
Apache MX net and then finally onyx onyx

00:09:22,560 --> 00:09:26,460
is an attempt to define a standard

00:09:24,780 --> 00:09:29,370
interoperability standard across these

00:09:26,460 --> 00:09:31,140
different frameworks so one can export a

00:09:29,370 --> 00:09:32,760
model that can be consumed by another

00:09:31,140 --> 00:09:34,020
and so you can use the right tool at the

00:09:32,760 --> 00:09:38,760
right place for example if you want to

00:09:34,020 --> 00:09:40,680
use a dynamic graph model like pi torch

00:09:38,760 --> 00:09:42,420
you can export the model that you create

00:09:40,680 --> 00:09:44,520
from that and then plummet into tensor

00:09:42,420 --> 00:09:46,740
flow and get the optimizations of tensor

00:09:44,520 --> 00:09:48,780
flow and that's made possible by onyx

00:09:46,740 --> 00:09:55,620
which is again collaboration across

00:09:48,780 --> 00:09:59,390
Facebook Microsoft in AWS now what has

00:09:55,620 --> 00:10:02,310
really driven open source in AI and ml I

00:09:59,390 --> 00:10:06,660
found this paper from 2007 remember when

00:10:02,310 --> 00:10:09,450
Dick Cheney was still president 2007 and

00:10:06,660 --> 00:10:10,620
this paper is in a machine learning

00:10:09,450 --> 00:10:13,620
journal Journal of machine learning

00:10:10,620 --> 00:10:15,420
research a collection of roughly 20

00:10:13,620 --> 00:10:16,860
different authors published this paper

00:10:15,420 --> 00:10:19,680
the need for open source software and

00:10:16,860 --> 00:10:21,390
machine learning and this is screenshots

00:10:19,680 --> 00:10:22,920
from the paper you can see the top seven

00:10:21,390 --> 00:10:25,650
reasons why they believed why they

00:10:22,920 --> 00:10:26,640
argued that the community of scientific

00:10:25,650 --> 00:10:28,920
community machine learning committee

00:10:26,640 --> 00:10:31,440
should adopt open-source and some of

00:10:28,920 --> 00:10:33,210
these are just general applicable e to

00:10:31,440 --> 00:10:35,040
any open source any argument for open

00:10:33,210 --> 00:10:37,740
sourcing software the first two are

00:10:35,040 --> 00:10:40,200
specific to the research community for

00:10:37,740 --> 00:10:41,640
AI n ml well they're observation was

00:10:40,200 --> 00:10:43,740
that a lot of academic papers would be

00:10:41,640 --> 00:10:46,350
published they would have results but

00:10:43,740 --> 00:10:48,210
because the tools that were used to

00:10:46,350 --> 00:10:49,950
produce those results were not generally

00:10:48,210 --> 00:10:51,780
available and not open source that it

00:10:49,950 --> 00:10:53,640
was very difficult to go and validate

00:10:51,780 --> 00:10:56,580
those results and reproduce them and

00:10:53,640 --> 00:10:57,960
then second if they're not open source

00:10:56,580 --> 00:10:59,490
it was impossible for the scientific

00:10:57,960 --> 00:11:02,010
community to go look and see maybe if

00:10:59,490 --> 00:11:03,450
there's a problem a bug a bias somewhere

00:11:02,010 --> 00:11:05,070
in those tools that were throwing off

00:11:03,450 --> 00:11:06,240
the results so while the tool is

00:11:05,070 --> 00:11:08,070
producing some results that you could

00:11:06,240 --> 00:11:09,060
say are valid there was actually some

00:11:08,070 --> 00:11:10,920
flaw in them

00:11:09,060 --> 00:11:13,610
with open-source the community could go

00:11:10,920 --> 00:11:15,270
look at it and figure out and contribute

00:11:13,610 --> 00:11:19,560
enhancements improvements and find

00:11:15,270 --> 00:11:23,490
problems so already back then in 2007

00:11:19,560 --> 00:11:24,870
really before the GPUs really caused the

00:11:23,490 --> 00:11:26,220
surge in deep neural network and we

00:11:24,870 --> 00:11:28,620
already see the scientific community to

00:11:26,220 --> 00:11:32,100
say hey we need to do open source with

00:11:28,620 --> 00:11:33,630
AI and ml for all these reasons if you

00:11:32,100 --> 00:11:35,490
take a look at the kinds of tools that

00:11:33,630 --> 00:11:38,190
people are using now in the community

00:11:35,490 --> 00:11:39,510
this is the Kegel 2017 state of data

00:11:38,190 --> 00:11:41,850
science and machine learning survey

00:11:39,510 --> 00:11:44,550
these are the top five tools used by

00:11:41,850 --> 00:11:46,920
data scientists all five of these are

00:11:44,550 --> 00:11:48,750
open-source little pythons open source

00:11:46,920 --> 00:11:52,020
ARS open source if you take a look at

00:11:48,750 --> 00:11:54,240
sequel their sequel server here oh hi my

00:11:52,020 --> 00:11:56,430
sequel in Postgres so largely open

00:11:54,240 --> 00:11:56,970
source Jupiter notebooks open source and

00:11:56,430 --> 00:12:00,450
tensorflow

00:11:56,970 --> 00:12:01,860
open source and if you take a look at

00:12:00,450 --> 00:12:04,170
the libraries they're using here is the

00:12:01,860 --> 00:12:06,900
ranking of most popular libraries this

00:12:04,170 --> 00:12:09,750
is came from a data incubator com where

00:12:06,900 --> 00:12:12,090
they went in analyzed github activity

00:12:09,750 --> 00:12:14,610
Stack Overflow activity and Google

00:12:12,090 --> 00:12:16,770
search results to kind of determine what

00:12:14,610 --> 00:12:18,690
are the hot libraries right now and you

00:12:16,770 --> 00:12:20,820
can see they're all open source

00:12:18,690 --> 00:12:23,490
libraries the whole machine learning

00:12:20,820 --> 00:12:25,950
industry is really built on top of open

00:12:23,490 --> 00:12:27,600
source and if you take a look at what

00:12:25,950 --> 00:12:30,390
languages they're using besides things

00:12:27,600 --> 00:12:34,290
like R which is used for professional

00:12:30,390 --> 00:12:38,250
machine learning and AI this survey here

00:12:34,290 --> 00:12:40,110
also from slash data breaks down the

00:12:38,250 --> 00:12:42,750
preferences for different languages and

00:12:40,110 --> 00:12:44,490
frameworks across data scientists based

00:12:42,750 --> 00:12:46,050
on their educational background what

00:12:44,490 --> 00:12:48,930
they've come prevents elf taught if

00:12:46,050 --> 00:12:51,740
they've gone to university and the thing

00:12:48,930 --> 00:12:55,140
that strikes striking about this is that

00:12:51,740 --> 00:12:58,770
consistently across all of them that if

00:12:55,140 --> 00:13:02,550
they develop their own algorithms they

00:12:58,770 --> 00:13:04,650
do so in Java Python and C and all of

00:13:02,550 --> 00:13:08,490
these three are open source languages

00:13:04,650 --> 00:13:11,550
and then finally when it comes to

00:13:08,490 --> 00:13:14,010
machine learning and AI it's a community

00:13:11,550 --> 00:13:17,940
and there's it's a very complex subject

00:13:14,010 --> 00:13:20,640
it's quickly evolving and the developers

00:13:17,940 --> 00:13:22,420
in that space oftentimes our stock or

00:13:20,640 --> 00:13:23,769
looking for guidance or looking

00:13:22,420 --> 00:13:26,620
or inspiration for how to solve a

00:13:23,769 --> 00:13:29,829
problem the places they turn to for

00:13:26,620 --> 00:13:32,170
information are shown here also slash

00:13:29,829 --> 00:13:34,959
data results surveyed from them where do

00:13:32,170 --> 00:13:36,970
you go find information about or

00:13:34,959 --> 00:13:38,170
guidance about the kinds of projects

00:13:36,970 --> 00:13:40,990
that you're working on you see number

00:13:38,170 --> 00:13:43,510
one is github why is it github because

00:13:40,990 --> 00:13:46,839
they can go and find projects there that

00:13:43,510 --> 00:13:51,130
they can take learn from reuse fork

00:13:46,839 --> 00:13:52,720
deploy so nothing teaches more than the

00:13:51,130 --> 00:13:55,089
actual code and that's where github

00:13:52,720 --> 00:13:57,480
comes into play you can see number two

00:13:55,089 --> 00:14:00,010
is websites and community forums so

00:13:57,480 --> 00:14:01,120
again open source is about not just

00:14:00,010 --> 00:14:02,440
about open source in the code but

00:14:01,120 --> 00:14:04,690
actually creating community around that

00:14:02,440 --> 00:14:06,790
code to support that code and you see

00:14:04,690 --> 00:14:08,680
that in AI in ml with the communities

00:14:06,790 --> 00:14:10,149
that are built around top of the code

00:14:08,680 --> 00:14:12,519
inside of github and I talked about

00:14:10,149 --> 00:14:14,079
before things like MX net and C and TK

00:14:12,519 --> 00:14:15,639
all of that is in github which has

00:14:14,079 --> 00:14:19,149
become the repository for all things

00:14:15,639 --> 00:14:20,470
open source now going back to that

00:14:19,149 --> 00:14:23,139
project that I talked about at the

00:14:20,470 --> 00:14:28,750
beginning from Microsoft where we took

00:14:23,139 --> 00:14:30,279
the NIH radiological imaging library and

00:14:28,750 --> 00:14:31,899
have trained it with machine learning

00:14:30,279 --> 00:14:33,399
models I wanted to show you a breakdown

00:14:31,899 --> 00:14:34,959
of that architecture to show you how

00:14:33,399 --> 00:14:36,940
we've made heavy use of open source

00:14:34,959 --> 00:14:38,170
throughout that whole architecture so

00:14:36,940 --> 00:14:39,670
you got the data sources on the Left

00:14:38,170 --> 00:14:41,320
there's about a hundred and twelve

00:14:39,670 --> 00:14:43,260
thousand images by the way 14 different

00:14:41,320 --> 00:14:45,399
pathological labels including pneumonia

00:14:43,260 --> 00:14:49,120
across 30,000 patients

00:14:45,399 --> 00:14:51,310
that's ingested into cloud storage we

00:14:49,120 --> 00:14:52,570
then launch what's called a deep

00:14:51,310 --> 00:14:53,890
learning virtual machine this is a

00:14:52,570 --> 00:14:56,470
virtual machine you can launch an azure

00:14:53,890 --> 00:14:58,740
that has built into it roughly 30 40

00:14:56,470 --> 00:15:01,540
different languages visualization tools

00:14:58,740 --> 00:15:04,149
frameworks the vast majority of which

00:15:01,540 --> 00:15:05,620
are open source where you can quickly

00:15:04,149 --> 00:15:07,149
get up and running with your project

00:15:05,620 --> 00:15:10,870
without having to go and reinstall

00:15:07,149 --> 00:15:13,360
things the model that we use here for

00:15:10,870 --> 00:15:17,339
the training is called dense net 121

00:15:13,360 --> 00:15:19,899
this is actually a feed-forward

00:15:17,339 --> 00:15:21,160
convolutional neural network that was

00:15:19,899 --> 00:15:23,949
published last year in an academic

00:15:21,160 --> 00:15:25,510
academic journal and going back to what

00:15:23,949 --> 00:15:27,699
the Scientific Committee was asking for

00:15:25,510 --> 00:15:30,040
back in 2007 the entire thing is in

00:15:27,699 --> 00:15:32,170
github so anybody can come and take make

00:15:30,040 --> 00:15:34,060
use of this algorithm to apply it to

00:15:32,170 --> 00:15:36,070
problems like this and so that's what we

00:15:34,060 --> 00:15:37,750
did and you can see that the frameworks

00:15:36,070 --> 00:15:41,890
used on top of that algorithm apply

00:15:37,750 --> 00:15:43,900
torch and caress both open-source then

00:15:41,890 --> 00:15:45,190
we trade it trained it on Azure with

00:15:43,900 --> 00:15:47,410
Azure machine learning and Visual Studio

00:15:45,190 --> 00:15:50,710
for tools for AI and Visual Studio code

00:15:47,410 --> 00:15:53,320
is of course open source and then when

00:15:50,710 --> 00:15:56,560
we deploy the models we deployed in a

00:15:53,320 --> 00:15:57,340
target a bunch of different run runtimes

00:15:56,560 --> 00:15:59,860
and you can see

00:15:57,340 --> 00:16:01,930
onyx is one tensorflow is another core

00:15:59,860 --> 00:16:05,680
ml is another all of which are open

00:16:01,930 --> 00:16:08,230
source and then finally what the doctor

00:16:05,680 --> 00:16:10,630
would consume or what the analysts would

00:16:08,230 --> 00:16:12,220
consume here when they go and get a

00:16:10,630 --> 00:16:15,190
radiological image process it through

00:16:12,220 --> 00:16:18,160
this model is images over there on the

00:16:15,190 --> 00:16:19,690
right side that you saw before so open

00:16:18,160 --> 00:16:24,150
source infused throughout this whole

00:16:19,690 --> 00:16:26,710
pipeline and this project right here is

00:16:24,150 --> 00:16:28,300
available also in open source the

00:16:26,710 --> 00:16:29,800
completely open think they'll hold the

00:16:28,300 --> 00:16:31,480
whole thing from start to finish

00:16:29,800 --> 00:16:33,190
you can go access today we just publish

00:16:31,480 --> 00:16:34,690
the whole thing and github it's at least

00:16:33,190 --> 00:16:36,100
sometime today what could have already

00:16:34,690 --> 00:16:41,770
happen but the whole thing is going into

00:16:36,100 --> 00:16:43,150
up into github so if you take a look at

00:16:41,770 --> 00:16:46,360
what's been happening over the last ten

00:16:43,150 --> 00:16:47,500
years really this rise of AI and ml why

00:16:46,360 --> 00:16:49,690
is it why are we hearing about it so

00:16:47,500 --> 00:16:51,880
much it's the confluence of a few things

00:16:49,690 --> 00:16:54,370
cloud computing infinite storage

00:16:51,880 --> 00:16:56,110
low-cost on-demand computing Jeep the

00:16:54,370 --> 00:16:59,470
growth of GPUs and the capabilities of

00:16:56,110 --> 00:17:02,680
GPUs combined with the tools and

00:16:59,470 --> 00:17:04,300
runtimes and languages that people do

00:17:02,680 --> 00:17:06,520
this kind of technology with which are

00:17:04,300 --> 00:17:08,290
all open-source and contributed to by

00:17:06,520 --> 00:17:10,959
the open source community which has

00:17:08,290 --> 00:17:13,060
really created this massive wave that

00:17:10,959 --> 00:17:16,270
everybody is riding now to go solve

00:17:13,060 --> 00:17:18,730
problems like pneumonia and increasing

00:17:16,270 --> 00:17:20,560
the efficiency of airplane engines so

00:17:18,730 --> 00:17:24,730
that I want to thank you very much for

00:17:20,560 --> 00:17:27,130
again for being here and this AI and m/l

00:17:24,730 --> 00:17:28,420
phenomenon we just seen the beginning of

00:17:27,130 --> 00:17:30,340
it of course everybody's talking about

00:17:28,420 --> 00:17:34,210
how this thing is going to permeate

00:17:30,340 --> 00:17:35,590
everything we do again driven largely by

00:17:34,210 --> 00:17:37,750
open-source in the communities that

00:17:35,590 --> 00:17:41,390
we've all created so thank you very much

00:17:37,750 --> 00:17:41,390

YouTube URL: https://www.youtube.com/watch?v=N7ECB_obI4o


