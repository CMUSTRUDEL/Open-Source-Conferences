Title: Keynote Panel: Artificial Intelligence and Machine Learning
Publication date: 2018-03-07
Playlist: Open Source Leadership Summit 2018
Description: 
	Keynote Panel: Artificial Intelligence and Machine Learning - Deepak Agarwal, VP of Engineering, LinkedIn; Mazin Gilbert, VP of Advanced Technology, AT&T Labs; Rachel Thomas, Co-founder, fast.ai; Tarry Singh, Deep Learning Executive

About Deepak
Deepak Agarwal is a vice president of engineering at LinkedIn where he is responsible for all AI efforts across the company. He is well known for his work on recommender systems and has published a book on the topic. He has published extensively in top-tier computer science conferences and has coauthored several patents. He is a Fellow of the American Statistical Association and has served on the Executive Committee of Knowledge Discovery and Data Mining (KDD). Deepak regularly serves on program committees of various conferences in the field of AI and computer science. He is also an associate editor of two flagship statistics journals.

About Mazin Gilbert
Dr. Mazin Gilbert is the Vice President of Advanced Technology at AT&T Labs. He leads AT&T’s research and advanced technology of its software-defined network. In this role, Mazin oversees advancements in networking and IP network management, big data, video technologies, artificial intelligence, information systems and visualization, algorithms and optimization, and scalable, reliable software platforms.

About Tarry Singh
Tarry Singh is an industry-acknowledged Deep Learning Executive with over 17 years of experience setting up data analytics divisions for F500 multi-nationals. He is currently a mentor at Courser’a DeepLearning Specialization working with Andrew Ng, the world’s leading figure in Artificial Intelligence. Expected learners to be trained is around 1.5 million already by end 2018 via a MOOC program. This training was ranked #2 in the world out of 8000 trainings, according to the Inc. Magazine.

About Rachel Thomas
Rachel Thomas was selected by Forbes as one of 20 Incredible Women in AI, earned her math PhD at Duke, and was an early engineer at Uber. She is co-founder of fast.ai, which created the “Practical Deep Learning for Coders” course that over 100,000 students have taken. Rachel is a popular writer and keynote speaker. Her writing has been read by over half a million people; has been translated into Chinese, Spanish, Korean, & Portuguese; and has made the front page of Hacker News 7x. She is on twitter @math_rachel and her website is here.
Captions: 
	00:00:00,030 --> 00:00:04,250
all right well it wouldn't you know so

00:00:01,350 --> 00:00:06,420
we covered we covered kubernetes

00:00:04,250 --> 00:00:08,460
cybersecurity it wouldn't be a good

00:00:06,420 --> 00:00:11,610
morning if we didn't include artificial

00:00:08,460 --> 00:00:13,710
intelligence in the mix here this is

00:00:11,610 --> 00:00:16,199
we're very fortunate to have a set of

00:00:13,710 --> 00:00:18,720
experts here this morning to discuss the

00:00:16,199 --> 00:00:20,760
AI landscape how it intersects with open

00:00:18,720 --> 00:00:23,070
source so I want to welcome all of them

00:00:20,760 --> 00:00:26,849
to the stage why don't you all come on

00:00:23,070 --> 00:00:30,510
up and sit here and I'll introduce each

00:00:26,849 --> 00:00:32,630
of you so Deepak Agarwal he is the VP of

00:00:30,510 --> 00:00:36,090
engineering and artificial intelligence

00:00:32,630 --> 00:00:40,260
at linked-in how many people here use

00:00:36,090 --> 00:00:42,450
LinkedIn a few of you so he's

00:00:40,260 --> 00:00:45,120
responsible for AI efforts across the

00:00:42,450 --> 00:00:47,910
entire company please have a seat

00:00:45,120 --> 00:00:50,610
Madison Gilbert is vice president of

00:00:47,910 --> 00:00:53,430
advanced technology at AT&T Labs he is

00:00:50,610 --> 00:00:57,870
heading up a AI initiatives there as

00:00:53,430 --> 00:01:01,469
well make sure my list here

00:00:57,870 --> 00:01:04,949
teri Singh is an author AI machine

00:01:01,469 --> 00:01:09,119
learning expert in both deep learning

00:01:04,949 --> 00:01:12,320
and AI at Coursera and finally Rachel

00:01:09,119 --> 00:01:14,610
Thomas who's the co-founder of fast AI

00:01:12,320 --> 00:01:20,670
please give them a warm welcome

00:01:14,610 --> 00:01:23,220
I want to kick off by just having each

00:01:20,670 --> 00:01:25,770
of you quickly talk about what you're

00:01:23,220 --> 00:01:29,130
doing in AI what your organization is

00:01:25,770 --> 00:01:30,810
doing and what kind of business

00:01:29,130 --> 00:01:33,630
imperative it is for each of you so

00:01:30,810 --> 00:01:36,240
Deepak I'm gonna start off with you so

00:01:33,630 --> 00:01:38,460
I'll probably start with the mission and

00:01:36,240 --> 00:01:40,140
vision of LinkedIn so LinkedIn what

00:01:38,460 --> 00:01:42,509
those of you are on it and for those of

00:01:40,140 --> 00:01:45,570
you who are not I'll encourage you to

00:01:42,509 --> 00:01:48,720
sign up so our mission is to connect

00:01:45,570 --> 00:01:50,700
talent with opportunity at scale and

00:01:48,720 --> 00:01:52,200
again opportunity I don't mean it in a

00:01:50,700 --> 00:01:54,299
narrow sense of just finding a job

00:01:52,200 --> 00:01:57,750
opportunity could be any professional

00:01:54,299 --> 00:01:59,460
opportunity and so AI and machine

00:01:57,750 --> 00:02:02,790
learning is something that is embedded

00:01:59,460 --> 00:02:05,969
in all our product in fact we many times

00:02:02,790 --> 00:02:07,649
refer to be the oxygen of color product

00:02:05,969 --> 00:02:10,170
right so it kind of is a horizontal

00:02:07,649 --> 00:02:11,640
layer that permeates all our product so

00:02:10,170 --> 00:02:13,120
if you're on LinkedIn and you're looking

00:02:11,640 --> 00:02:14,920
for a job all the job recommend

00:02:13,120 --> 00:02:16,870
you get is all powered through machine

00:02:14,920 --> 00:02:18,580
learning in AI if you are recruit or

00:02:16,870 --> 00:02:19,930
trying to source a candidate then the

00:02:18,580 --> 00:02:21,580
search results which you are getting is

00:02:19,930 --> 00:02:23,799
powered through AI if you're on the

00:02:21,580 --> 00:02:25,030
newsfeed consuming content that's all

00:02:23,799 --> 00:02:26,890
powered through you if you're trying to

00:02:25,030 --> 00:02:29,409
connect with someone which you should

00:02:26,890 --> 00:02:30,940
for getting professional opportunities

00:02:29,409 --> 00:02:32,079
in the future those recommendations are

00:02:30,940 --> 00:02:34,150
powered for your so it's kind of

00:02:32,079 --> 00:02:37,959
ubiquitous and we have been doing it for

00:02:34,150 --> 00:02:40,180
a long time now it's very mature and we

00:02:37,959 --> 00:02:41,920
are no longer in a state where we think

00:02:40,180 --> 00:02:44,230
about it it is kind of become an

00:02:41,920 --> 00:02:46,540
integral part of everything we do it's

00:02:44,230 --> 00:02:47,859
embedded in everything we do I think we

00:02:46,540 --> 00:02:51,760
are now at a state where you're thinking

00:02:47,859 --> 00:02:53,680
of what can we do with AI to power the

00:02:51,760 --> 00:02:55,440
next generation of user experience on

00:02:53,680 --> 00:02:57,970
the platform thank you

00:02:55,440 --> 00:03:00,010
Mazen you guys are running some huge

00:02:57,970 --> 00:03:03,190
networks out there with hundreds of

00:03:00,010 --> 00:03:04,299
millions of users and 5 G's just around

00:03:03,190 --> 00:03:08,019
the corner yeah all right so you're

00:03:04,299 --> 00:03:10,510
tuned to sitting right now it's so I TNT

00:03:08,019 --> 00:03:13,390
which I'm hoping that 95% of you guys

00:03:10,510 --> 00:03:17,410
are 80 customers today so thank you very

00:03:13,390 --> 00:03:20,260
much so a tnt's mission is is to really

00:03:17,410 --> 00:03:22,690
connect people with their wealth whether

00:03:20,260 --> 00:03:24,639
they live where they work and really do

00:03:22,690 --> 00:03:26,739
it better than anybody else and when you

00:03:24,639 --> 00:03:29,410
think about a company who's whose their

00:03:26,739 --> 00:03:32,590
sole business is communication and

00:03:29,410 --> 00:03:34,690
entertainment ai is a foundation not

00:03:32,590 --> 00:03:37,329
just to drive one application but really

00:03:34,690 --> 00:03:39,849
to drive how we live and how we work as

00:03:37,329 --> 00:03:41,440
a society and basically globally so if

00:03:39,849 --> 00:03:44,709
you're thinking about well how would we

00:03:41,440 --> 00:03:48,340
drive Porgy and going to 5g and how do

00:03:44,709 --> 00:03:50,500
we really xscape cloud technologies all

00:03:48,340 --> 00:03:54,430
the way to the edge already how do we

00:03:50,500 --> 00:03:56,500
try to try to drive our network how do

00:03:54,430 --> 00:03:58,660
we operationalize our network we talked

00:03:56,500 --> 00:04:01,599
about security earlier on how do we make

00:03:58,660 --> 00:04:03,910
sure we get you know tens of millions of

00:04:01,599 --> 00:04:06,819
attacks every day on our network how do

00:04:03,910 --> 00:04:09,190
we really address those really AI is the

00:04:06,819 --> 00:04:13,030
foundation of pretty much our business

00:04:09,190 --> 00:04:15,160
from the get-go so I'm gonna start with

00:04:13,030 --> 00:04:19,329
the mission and the vision no I'm just

00:04:15,160 --> 00:04:22,030
kidding so my name is teri I am a

00:04:19,329 --> 00:04:24,030
founder CEO neuroscience researcher

00:04:22,030 --> 00:04:27,250
studying brain and that kind of stuff

00:04:24,030 --> 00:04:30,670
deep cough a today I am also mentor

00:04:27,250 --> 00:04:32,230
course are working with couple smart

00:04:30,670 --> 00:04:34,420
people Andrew and a bunch of other guys

00:04:32,230 --> 00:04:37,210
or developing deep learning

00:04:34,420 --> 00:04:38,680
specialization so help you know if

00:04:37,210 --> 00:04:40,240
you're this year we have a plan to get

00:04:38,680 --> 00:04:42,280
about a couple hundred thousand to about

00:04:40,240 --> 00:04:45,640
a million people are trained in deep

00:04:42,280 --> 00:04:48,040
learning in the eye and an amazing

00:04:45,640 --> 00:04:50,650
amazing training set up you know from

00:04:48,040 --> 00:04:52,510
Stanford and Andrews setting it up and I

00:04:50,650 --> 00:04:55,480
guess I know you do that a lot of stuff

00:04:52,510 --> 00:04:58,740
as well the faster I but so so you know

00:04:55,480 --> 00:05:01,630
very short what I do is I work with

00:04:58,740 --> 00:05:02,920
enterprises and and helping specifically

00:05:01,630 --> 00:05:05,980
they could be engineers software

00:05:02,920 --> 00:05:08,740
developers or they could be PhD postdocs

00:05:05,980 --> 00:05:10,320
as well training them to work with deep

00:05:08,740 --> 00:05:13,330
learning techniques right

00:05:10,320 --> 00:05:15,990
convolution neural networks or vision or

00:05:13,330 --> 00:05:19,810
text kind of stuff you know for the guys

00:05:15,990 --> 00:05:22,750
who don't understand but so the goal is

00:05:19,810 --> 00:05:26,380
for me the goal is to convert a whole

00:05:22,750 --> 00:05:28,840
lot of people to adopt artificial

00:05:26,380 --> 00:05:30,130
intelligence or deep learning and and

00:05:28,840 --> 00:05:32,500
the other part of the things that I do

00:05:30,130 --> 00:05:35,020
I'm not working with with the enterprise

00:05:32,500 --> 00:05:35,590
I've worked in the most difficult parts

00:05:35,020 --> 00:05:39,419
of the world

00:05:35,590 --> 00:05:42,550
I go to Tunisia I even go to Syria and

00:05:39,419 --> 00:05:44,290
and we work with women and these are

00:05:42,550 --> 00:05:46,330
young girls you know having all kinds of

00:05:44,290 --> 00:05:47,919
problems in the world and and for

00:05:46,330 --> 00:05:49,780
instance after this I'm good at Chinese

00:05:47,919 --> 00:05:52,510
here after I'm done with this conference

00:05:49,780 --> 00:05:55,870
and then we go to Turkey but we have you

00:05:52,510 --> 00:05:58,120
know people who were totally distracted

00:05:55,870 --> 00:06:00,190
with everything that life can take out

00:05:58,120 --> 00:06:02,169
of you and put those kids and say let's

00:06:00,190 --> 00:06:03,880
go and work and let's work with code

00:06:02,169 --> 00:06:05,650
let's work with tensorflow

00:06:03,880 --> 00:06:07,300
and and this is this is what what I do

00:06:05,650 --> 00:06:09,550
so I guess I this is a bit of an

00:06:07,300 --> 00:06:10,990
introduction that's amazing that is

00:06:09,550 --> 00:06:13,410
incredible

00:06:10,990 --> 00:06:16,450
Rachel I you're checked out your book

00:06:13,410 --> 00:06:19,570
this is the practical deep learning for

00:06:16,450 --> 00:06:23,350
coders so lots of coders in the audience

00:06:19,570 --> 00:06:25,210
here I'm Rachel Thomas and co-founder of

00:06:23,350 --> 00:06:27,370
fast AI which is a non-profit research

00:06:25,210 --> 00:06:29,560
lab and I'm also a professor at the

00:06:27,370 --> 00:06:31,060
University of San Francisco and with

00:06:29,560 --> 00:06:33,490
fast AI we're trying to make deep

00:06:31,060 --> 00:06:34,870
learning easier to use and so we do this

00:06:33,490 --> 00:06:37,419
both through kind of building the tools

00:06:34,870 --> 00:06:39,510
we have an open-source library and it's

00:06:37,419 --> 00:06:41,970
kind of very high level and encodes best

00:06:39,510 --> 00:06:43,860
practices called fast AI and we also

00:06:41,970 --> 00:06:45,870
have a free course practical deep

00:06:43,860 --> 00:06:47,460
learning for coders over a hundred

00:06:45,870 --> 00:06:49,650
thousand students have taken it we've

00:06:47,460 --> 00:06:52,350
had students get jobs at Google brain

00:06:49,650 --> 00:06:54,110
have their work featured on HBO

00:06:52,350 --> 00:06:56,040
but we're particularly trying to reach

00:06:54,110 --> 00:06:59,910
coders who don't have to have any

00:06:56,040 --> 00:07:01,740
advanced math background and we're

00:06:59,910 --> 00:07:03,980
interested in people that are I think

00:07:01,740 --> 00:07:06,390
particularly they're kind of working on

00:07:03,980 --> 00:07:08,190
projects outside the mainstream things

00:07:06,390 --> 00:07:10,530
they very care about very much care

00:07:08,190 --> 00:07:13,950
about and don't have access to a lot of

00:07:10,530 --> 00:07:16,890
resources and we've had students improve

00:07:13,950 --> 00:07:19,800
agricultural loans in India try to stop

00:07:16,890 --> 00:07:21,870
illegal deforestation of endangered

00:07:19,800 --> 00:07:24,360
rainforests help patients with

00:07:21,870 --> 00:07:28,260
Parkinsons diseases so a lot of

00:07:24,360 --> 00:07:30,840
interesting applications very cool

00:07:28,260 --> 00:07:31,800
well I want you know for a lot of I get

00:07:30,840 --> 00:07:33,870
these questions all the time a lot of

00:07:31,800 --> 00:07:37,530
people in the audience find sort of the

00:07:33,870 --> 00:07:39,090
AI landscape totally confusing right

00:07:37,530 --> 00:07:41,910
that there's all these different tools

00:07:39,090 --> 00:07:43,260
and there's and how you do it I think

00:07:41,910 --> 00:07:45,210
you're all kind of the exception and

00:07:43,260 --> 00:07:48,360
that you're in roles where you're just

00:07:45,210 --> 00:07:50,040
far down that path but like you know if

00:07:48,360 --> 00:07:51,570
you're talking to someone who is maybe

00:07:50,040 --> 00:07:53,160
confused about it what are some of the

00:07:51,570 --> 00:07:54,570
exciting areas what are some of the

00:07:53,160 --> 00:07:56,730
things that people should be looking at

00:07:54,570 --> 00:07:58,680
right now in AI I think I'll just kind

00:07:56,730 --> 00:08:01,680
of go back around Robin you're starting

00:07:58,680 --> 00:08:05,580
with you debug yeah so I think first of

00:08:01,680 --> 00:08:07,680
all AI before 2012 was very different

00:08:05,580 --> 00:08:10,740
than what it is today and in 2012

00:08:07,680 --> 00:08:13,590
something great happened so you know a

00:08:10,740 --> 00:08:17,220
bunch of professors researchers they

00:08:13,590 --> 00:08:19,620
were able to figure out a new way of

00:08:17,220 --> 00:08:21,180
computing things that scale by using GPU

00:08:19,620 --> 00:08:23,430
cards and this is what kind of ushered

00:08:21,180 --> 00:08:26,040
in the eurovia rate right so I would

00:08:23,430 --> 00:08:27,810
definitely want you to pay attention to

00:08:26,040 --> 00:08:30,330
everything that's happening in deep

00:08:27,810 --> 00:08:32,280
learning so what what has happened is

00:08:30,330 --> 00:08:34,770
that you have with the with the

00:08:32,280 --> 00:08:37,500
availability of cloud computing data

00:08:34,770 --> 00:08:40,020
management has become commoditized right

00:08:37,500 --> 00:08:41,910
and very and with the availability of

00:08:40,020 --> 00:08:43,500
deep learning tools and you know things

00:08:41,910 --> 00:08:46,020
like Ostia and what Andrew is doing

00:08:43,500 --> 00:08:48,970
slowly that's also becoming commoditized

00:08:46,020 --> 00:08:51,759
so if you have a problem where you know

00:08:48,970 --> 00:08:53,560
you want to predict something by using a

00:08:51,759 --> 00:08:56,019
lot of input signal this is slowly

00:08:53,560 --> 00:08:57,939
getting commoditize and this alone can

00:08:56,019 --> 00:08:59,740
be transformative look at what has

00:08:57,939 --> 00:09:00,850
happened in computer vision look at what

00:08:59,740 --> 00:09:02,470
has happened in natural language

00:09:00,850 --> 00:09:04,629
processing in speech I mean these things

00:09:02,470 --> 00:09:07,930
have become very more accurate than what

00:09:04,629 --> 00:09:10,300
they were like 10 years ago and the

00:09:07,930 --> 00:09:12,850
impact of that is pervasive right you

00:09:10,300 --> 00:09:14,980
know in every area now we are kind of we

00:09:12,850 --> 00:09:17,110
are able to use AI technology do think

00:09:14,980 --> 00:09:18,850
that we are not able to do before like

00:09:17,110 --> 00:09:21,069
for instance in the old days if you are

00:09:18,850 --> 00:09:22,750
a radiologist you will actually send the

00:09:21,069 --> 00:09:24,430
MRI image to India I know and then

00:09:22,750 --> 00:09:25,930
someone there is going to read the image

00:09:24,430 --> 00:09:27,279
and so in the morning you have it on

00:09:25,930 --> 00:09:29,170
your desk you don't have to do that

00:09:27,279 --> 00:09:31,750
anymore right I mean you can have AI

00:09:29,170 --> 00:09:33,730
software kind of do that for you and

00:09:31,750 --> 00:09:36,069
again I can go on and on with with the

00:09:33,730 --> 00:09:37,959
example so definitely super wise this is

00:09:36,069 --> 00:09:39,639
this class of problem area predicting

00:09:37,959 --> 00:09:41,350
some output based on input that's called

00:09:39,639 --> 00:09:43,300
supervised learning and this is slowly

00:09:41,350 --> 00:09:45,040
getting commoditize and this alone can

00:09:43,300 --> 00:09:46,629
actually have a very big impact in many

00:09:45,040 --> 00:09:48,490
different things that you do right so

00:09:46,629 --> 00:09:52,899
you have this paradigm where you can

00:09:48,490 --> 00:09:55,059
have data learn itself learn learnt a

00:09:52,899 --> 00:09:56,709
turns through through algorithms right

00:09:55,059 --> 00:09:58,449
you don't have to write rules to kind of

00:09:56,709 --> 00:10:00,069
program a computer you can have the

00:09:58,449 --> 00:10:03,160
computer learn there are other areas

00:10:00,069 --> 00:10:04,689
that are not very well that are not very

00:10:03,160 --> 00:10:06,160
well researched and we have a long way

00:10:04,689 --> 00:10:08,639
to go like if you look at unsupervised

00:10:06,160 --> 00:10:11,230
learning or human level intelligence

00:10:08,639 --> 00:10:13,660
that machine can learn that's still an

00:10:11,230 --> 00:10:16,420
open research area and I think in the

00:10:13,660 --> 00:10:18,339
next ten years or so maybe we will be we

00:10:16,420 --> 00:10:20,319
will be there and it may become as

00:10:18,339 --> 00:10:22,990
commoditized as as the supervised

00:10:20,319 --> 00:10:24,370
learning techniques are yeah how about

00:10:22,990 --> 00:10:25,839
you massive like you know you've been

00:10:24,370 --> 00:10:28,360
involved in some pretty cutting-edge

00:10:25,839 --> 00:10:31,209
stuff in terms of trying to get this

00:10:28,360 --> 00:10:34,300
technology out there and deployed you

00:10:31,209 --> 00:10:35,709
know a plug a cue miss a little bit for

00:10:34,300 --> 00:10:37,029
you you know in terms of building a

00:10:35,709 --> 00:10:39,639
marketplace where you these things can

00:10:37,029 --> 00:10:41,769
be reused what's exciting to you so just

00:10:39,639 --> 00:10:44,559
for those who who don't know me is it I

00:10:41,769 --> 00:10:47,019
I did my PhD in the 80s and neural nets

00:10:44,559 --> 00:10:50,110
for speech I was excited about AI

00:10:47,019 --> 00:10:53,019
because the the concept of getting a

00:10:50,110 --> 00:10:54,819
computer to use neural nets which at the

00:10:53,019 --> 00:10:57,160
time there is there was this resemblance

00:10:54,819 --> 00:10:58,990
of artificial neural Nets with the

00:10:57,160 --> 00:11:01,059
biological neural and that's still today

00:10:58,990 --> 00:11:03,000
there's that sort of association and

00:11:01,059 --> 00:11:05,580
confusion to get them to

00:11:03,000 --> 00:11:08,640
have a machine actually articulate

00:11:05,580 --> 00:11:10,890
sounds and speak just like humans do was

00:11:08,640 --> 00:11:12,210
was just amazing and completely

00:11:10,890 --> 00:11:14,760
fascinating and that's how he how

00:11:12,210 --> 00:11:16,260
started and we were at the time like few

00:11:14,760 --> 00:11:19,350
hundred people we will go to workshop

00:11:16,260 --> 00:11:21,240
we'll go to conferences and even in the

00:11:19,350 --> 00:11:24,210
past twenty years thirty years is that

00:11:21,240 --> 00:11:26,580
there's been phases of interest of AI

00:11:24,210 --> 00:11:29,820
Gwent fell buzz to a hype it went down

00:11:26,580 --> 00:11:31,860
again if you look at the literature deep

00:11:29,820 --> 00:11:34,050
learning and neural nets and AI never

00:11:31,860 --> 00:11:35,790
actually stopped in the community and

00:11:34,050 --> 00:11:38,850
the research community is being worked

00:11:35,790 --> 00:11:41,220
on for for at least three decades it is

00:11:38,850 --> 00:11:44,100
different now and I agree with Deepak

00:11:41,220 --> 00:11:49,200
here that the distributed computing the

00:11:44,100 --> 00:11:51,510
GPUs the the flux of larger data is is

00:11:49,200 --> 00:11:53,910
really made a big change well what I

00:11:51,510 --> 00:11:55,950
actually think that a bigger driver to

00:11:53,910 --> 00:11:57,870
that we started with a few hundred

00:11:55,950 --> 00:12:00,330
people today thanks to you guys that we

00:11:57,870 --> 00:12:03,150
have hundreds of thousands going to the

00:12:00,330 --> 00:12:05,970
millions really the big driver the big

00:12:03,150 --> 00:12:08,400
revolution is open source if you think

00:12:05,970 --> 00:12:10,170
before is there you know twenty years

00:12:08,400 --> 00:12:11,880
ago when I was working on this is there

00:12:10,170 --> 00:12:14,070
there are a few of us who can write some

00:12:11,880 --> 00:12:16,050
code we would join very large companies

00:12:14,070 --> 00:12:19,050
who have deep pocket to have big

00:12:16,050 --> 00:12:21,780
computers great great computers and

00:12:19,050 --> 00:12:23,850
others to be able to run these type of

00:12:21,780 --> 00:12:26,040
jobs that can require a lot of data

00:12:23,850 --> 00:12:29,190
today if you've gone to CES every

00:12:26,040 --> 00:12:31,410
company is now an AI company because to

00:12:29,190 --> 00:12:32,880
really be in that business now it's

00:12:31,410 --> 00:12:35,580
nothing more than just download a

00:12:32,880 --> 00:12:37,260
software and you can just get going and

00:12:35,580 --> 00:12:39,240
you can take a course at Coursera or

00:12:37,260 --> 00:12:40,950
somewhere and you can pretty much get

00:12:39,240 --> 00:12:43,290
going in a very very short period of

00:12:40,950 --> 00:12:45,900
time and by doing that that's really

00:12:43,290 --> 00:12:47,700
trying to have created a huge revolution

00:12:45,900 --> 00:12:49,500
in the industry so what we did however

00:12:47,700 --> 00:12:51,960
that even with that excitement

00:12:49,500 --> 00:12:53,400
revolution from a company like AT&T and

00:12:51,960 --> 00:12:56,370
probably most of you are the same is

00:12:53,400 --> 00:12:58,410
that we are hitting big bottlenecks and

00:12:56,370 --> 00:13:01,350
our bottlenecks we believe those

00:12:58,410 --> 00:13:03,990
bottlenecks are tremendously large

00:13:01,350 --> 00:13:07,410
bottlenecks that a I cannot move to that

00:13:03,990 --> 00:13:10,170
next step of scale without addressing

00:13:07,410 --> 00:13:13,160
those the first one is that it's still

00:13:10,170 --> 00:13:15,139
the case that there are lack of

00:13:13,160 --> 00:13:17,449
understanding and

00:13:15,139 --> 00:13:19,670
and and training and learning about what

00:13:17,449 --> 00:13:21,739
AI is of where it can be used there are

00:13:19,670 --> 00:13:23,449
a lot of tools out there and the

00:13:21,739 --> 00:13:25,160
question is that which tool do you use

00:13:23,449 --> 00:13:26,929
and are they interconnected with each

00:13:25,160 --> 00:13:29,989
other we needed to figure out a way to

00:13:26,929 --> 00:13:32,029
harmonize those number two when you ask

00:13:29,989 --> 00:13:33,679
your team I need to build an AI solution

00:13:32,029 --> 00:13:34,669
they would go and start from pretty much

00:13:33,679 --> 00:13:37,850
from scratch

00:13:34,669 --> 00:13:40,850
there is no reusability of AI these are

00:13:37,850 --> 00:13:43,009
very expensive things to build it takes

00:13:40,850 --> 00:13:44,869
months it may say take more years or so

00:13:43,009 --> 00:13:46,819
so what acumen is trying to do which we

00:13:44,869 --> 00:13:49,029
have announced under Linux Foundation is

00:13:46,819 --> 00:13:51,259
that to create a marketplace a

00:13:49,029 --> 00:13:54,589
marketplace a distributed marketplace

00:13:51,259 --> 00:13:57,049
for AI so think of the App Store with

00:13:54,589 --> 00:13:59,389
one difference is that with this App

00:13:57,049 --> 00:14:01,759
Store is that the applications are built

00:13:59,389 --> 00:14:03,649
by many different tools it's agnostic to

00:14:01,759 --> 00:14:06,679
the tool that's being used that's number

00:14:03,649 --> 00:14:08,929
one number two is that the applications

00:14:06,679 --> 00:14:11,749
that you build in this marketplace they

00:14:08,929 --> 00:14:13,639
interconnect and interoperate think of

00:14:11,749 --> 00:14:16,339
them as micro services that interconnect

00:14:13,639 --> 00:14:18,709
so you could be using tensorflow and she

00:14:16,339 --> 00:14:20,569
could be using cycled learn and you can

00:14:18,709 --> 00:14:24,100
actually connect the APIs of those to

00:14:20,569 --> 00:14:26,149
create new solutions the third thing is

00:14:24,100 --> 00:14:28,069
when we talk about machine learning a

00:14:26,149 --> 00:14:29,989
lot of people talk about data scientist

00:14:28,069 --> 00:14:31,759
and machine learners and so forth and

00:14:29,989 --> 00:14:33,980
then when you start thinking about what

00:14:31,759 --> 00:14:38,029
does it take to move that into

00:14:33,980 --> 00:14:40,069
production from an AT&T it's sometimes

00:14:38,029 --> 00:14:41,269
months and your two years you have to

00:14:40,069 --> 00:14:42,589
figure out finding you have to get our

00:14:41,269 --> 00:14:44,569
prioritization you have to put a team

00:14:42,589 --> 00:14:46,309
together the developers don't think the

00:14:44,569 --> 00:14:47,299
same way as the data scientists in fact

00:14:46,309 --> 00:14:49,399
these are complete different

00:14:47,299 --> 00:14:51,649
organizations okay that maybe they

00:14:49,399 --> 00:14:53,629
report to the same you know senior VP at

00:14:51,649 --> 00:14:56,929
some point when we try to do with a

00:14:53,629 --> 00:14:59,239
qumari to streamline the process from a

00:14:56,929 --> 00:15:01,249
data scientist building a model to that

00:14:59,239 --> 00:15:03,379
model being completely production and do

00:15:01,249 --> 00:15:05,149
that in a matter of minutes as opposed

00:15:03,379 --> 00:15:06,919
to what it takes today and really have

00:15:05,149 --> 00:15:08,720
that as part of a marketplace that you

00:15:06,919 --> 00:15:11,389
can just download and run on any cloud

00:15:08,720 --> 00:15:13,429
and be agnostic to the cloud so I this

00:15:11,389 --> 00:15:15,439
is a very big revolution it's a

00:15:13,429 --> 00:15:17,269
community we're all trying to really get

00:15:15,439 --> 00:15:19,759
together to change that I actually

00:15:17,269 --> 00:15:21,319
believe that without doing that it's

00:15:19,759 --> 00:15:23,299
going to be very hard for us to really

00:15:21,319 --> 00:15:26,269
move to where we are today where we are

00:15:23,299 --> 00:15:29,030
deploying AI for some applications to

00:15:26,269 --> 00:15:31,550
really making AI mainstream where

00:15:29,030 --> 00:15:34,850
a twelve-year-old kid who can design and

00:15:31,550 --> 00:15:36,890
build and deploy a website can basically

00:15:34,850 --> 00:15:38,510
do the same thing with a are well Terry

00:15:36,890 --> 00:15:40,040
it sounds like you to some degree are

00:15:38,510 --> 00:15:42,440
doing that right you're working with

00:15:40,040 --> 00:15:44,570
young kids and they're able to take this

00:15:42,440 --> 00:15:46,400
open-source tooling and do real things

00:15:44,570 --> 00:15:50,780
with it like what are your experiences

00:15:46,400 --> 00:15:52,160
there so yeah I think the the point that

00:15:50,780 --> 00:15:54,440
you've raised on open-source is

00:15:52,160 --> 00:15:56,690
something we we just kind of presume you

00:15:54,440 --> 00:15:58,310
know it's like like fresh water you pick

00:15:56,690 --> 00:16:01,310
it up and you start building stuff so I

00:15:58,310 --> 00:16:05,050
want to say first thing is I I wouldn't

00:16:01,310 --> 00:16:08,980
know how to take these technologies

00:16:05,050 --> 00:16:11,990
either to the enterprise customers or

00:16:08,980 --> 00:16:14,120
into Syria which I am going to be there

00:16:11,990 --> 00:16:15,920
on the 19th amazing you know we're gonna

00:16:14,120 --> 00:16:17,630
be getting getting coverage from CNN and

00:16:15,920 --> 00:16:22,790
BBC and a couple other guys that's

00:16:17,630 --> 00:16:25,430
amazing but I almost forget then if it

00:16:22,790 --> 00:16:27,050
was not for you know you know I

00:16:25,430 --> 00:16:28,340
contributed tons of flow as well you

00:16:27,050 --> 00:16:30,050
know you can just go how to be to

00:16:28,340 --> 00:16:32,930
developer it's all open source it's like

00:16:30,050 --> 00:16:35,780
free you know it's like free as in super

00:16:32,930 --> 00:16:38,030
free that's that's great you really I

00:16:35,780 --> 00:16:40,100
mean it's something I can just get out

00:16:38,030 --> 00:16:42,320
my hard disk and I can go and I can

00:16:40,100 --> 00:16:43,490
implement and and you can set up our

00:16:42,320 --> 00:16:45,500
servers you can set up virtual machines

00:16:43,490 --> 00:16:46,280
also free you can splurt up on

00:16:45,500 --> 00:16:48,650
VirtualBox

00:16:46,280 --> 00:16:50,390
you know free opera okay I think Oracle

00:16:48,650 --> 00:16:53,960
is bought it but it's still free

00:16:50,390 --> 00:16:56,750
you can download it so you know I think

00:16:53,960 --> 00:16:59,900
that is something which I realized when

00:16:56,750 --> 00:17:01,580
I had a question so Angela she said sent

00:16:59,900 --> 00:17:03,380
me a list of questions to all of us and

00:17:01,580 --> 00:17:05,300
I said hey you know this is the

00:17:03,380 --> 00:17:07,760
revolution it's like a silent revolution

00:17:05,300 --> 00:17:09,530
I'm you know guys like Richard Stallman

00:17:07,760 --> 00:17:11,630
all these guys everybody's been working

00:17:09,530 --> 00:17:14,270
on it it's it's really super we should

00:17:11,630 --> 00:17:16,460
be like super thankful for everybody all

00:17:14,270 --> 00:17:17,180
these millions of developers who make

00:17:16,460 --> 00:17:20,180
this happen

00:17:17,180 --> 00:17:21,890
so I can just pick up my laptop and and

00:17:20,180 --> 00:17:23,930
I can go to Tunis where I'm gonna be in

00:17:21,890 --> 00:17:25,640
Tunis and there are like people pulling

00:17:23,930 --> 00:17:27,440
me all over the place so the political

00:17:25,640 --> 00:17:29,360
parties saying why don't you talk about

00:17:27,440 --> 00:17:31,280
AI because we need to really clear the

00:17:29,360 --> 00:17:34,160
air said okay fine so I'm gonna make a

00:17:31,280 --> 00:17:35,420
business II kind of presentation as long

00:17:34,160 --> 00:17:39,200
as you don't talk about that stupid

00:17:35,420 --> 00:17:41,860
robot called Sophia but everything else

00:17:39,200 --> 00:17:44,170
is free you can take it and you can

00:17:41,860 --> 00:17:45,850
just implemented guys and and the

00:17:44,170 --> 00:17:48,790
practical example is this so I said ok

00:17:45,850 --> 00:17:52,270
so I have free stuff so what do I do and

00:17:48,790 --> 00:17:53,860
then you know so we said ok there is you

00:17:52,270 --> 00:17:55,929
know I can go into details I'll keep a

00:17:53,860 --> 00:17:58,630
little high level for example skin

00:17:55,929 --> 00:18:00,610
cancer detection is is something which

00:17:58,630 --> 00:18:03,730
you know you have you know human bias

00:18:00,610 --> 00:18:05,679
and then you have technology and and I

00:18:03,730 --> 00:18:07,929
said okay so I have you know that stuff

00:18:05,679 --> 00:18:09,790
is free as well you can download it I

00:18:07,929 --> 00:18:11,200
have all those data sets in facts you

00:18:09,790 --> 00:18:12,970
know like I don't know it's like 40 or

00:18:11,200 --> 00:18:15,070
50 gigabytes I can put it on my flash

00:18:12,970 --> 00:18:17,110
disk and just go anywhere and we can

00:18:15,070 --> 00:18:18,970
train on those data sets provided by IES

00:18:17,110 --> 00:18:21,690
I see this international skin image

00:18:18,970 --> 00:18:24,460
classification society here in the US

00:18:21,690 --> 00:18:27,190
there are a whole lot of datasets from

00:18:24,460 --> 00:18:28,960
skin cancer images and just identifying

00:18:27,190 --> 00:18:32,500
your moles or you know those kind of you

00:18:28,960 --> 00:18:34,210
know if it's a malignant or if it's

00:18:32,500 --> 00:18:35,290
benign of its neighbors you know people

00:18:34,210 --> 00:18:37,990
have been teaching me different things

00:18:35,290 --> 00:18:40,630
I'm not a surgeon but it's it's like

00:18:37,990 --> 00:18:42,880
okay so we have free software and then

00:18:40,630 --> 00:18:45,370
we have a problem which we can solve and

00:18:42,880 --> 00:18:47,380
then what that led to I gave a bunch of

00:18:45,370 --> 00:18:49,179
lectures on different you know we're

00:18:47,380 --> 00:18:50,950
researching capsule just like 10 people

00:18:49,179 --> 00:18:53,740
in the world who researching on capsules

00:18:50,950 --> 00:18:55,570
building stuff and in capsule networks

00:18:53,740 --> 00:18:58,570
is like the next let's say the

00:18:55,570 --> 00:19:01,570
convolution your network thing or the

00:18:58,570 --> 00:19:03,040
evolution of that and and so we take

00:19:01,570 --> 00:19:04,780
that and then I said okay so we're going

00:19:03,040 --> 00:19:07,390
to take a step further who wants to

00:19:04,780 --> 00:19:10,179
develop an app in Android free stuff

00:19:07,390 --> 00:19:11,559
again and okay core ml with Apple is

00:19:10,179 --> 00:19:13,750
also free you can download it in Kimball

00:19:11,559 --> 00:19:15,580
an iOS app as well and people are

00:19:13,750 --> 00:19:17,890
building apps right now so I'm just back

00:19:15,580 --> 00:19:20,650
from Finland where I know if Martin's

00:19:17,890 --> 00:19:22,690
still in the room you you know being in

00:19:20,650 --> 00:19:24,429
Espoo and they were like really smart

00:19:22,690 --> 00:19:26,860
researches and we're building apps so

00:19:24,429 --> 00:19:29,140
it's all possible I didn't have to go on

00:19:26,860 --> 00:19:30,490
anybody to ask money or permission from

00:19:29,140 --> 00:19:31,960
a manager who would say I need to talk

00:19:30,490 --> 00:19:33,340
to an account manager because this big

00:19:31,960 --> 00:19:35,470
corporate company needs to give you

00:19:33,340 --> 00:19:36,910
software and when you helps alter you

00:19:35,470 --> 00:19:40,630
need licenses you need this you need

00:19:36,910 --> 00:19:42,940
that so it's it's just available it's

00:19:40,630 --> 00:19:44,679
it's it's super amazing rich I want to

00:19:42,940 --> 00:19:47,080
ask you though a question in that I get

00:19:44,679 --> 00:19:48,580
a lot you know I I agree I think that

00:19:47,080 --> 00:19:52,120
we're standing on the shoulders of

00:19:48,580 --> 00:19:54,040
giants in terms of folks like Richard

00:19:52,120 --> 00:19:54,770
Stallman who came up with this concept

00:19:54,040 --> 00:19:57,290
of share

00:19:54,770 --> 00:19:58,850
critical open-source licenses and all

00:19:57,290 --> 00:20:00,500
the folks who followed that you know

00:19:58,850 --> 00:20:03,890
whether it you knows folks from Apache

00:20:00,500 --> 00:20:06,710
or other organizations but one question

00:20:03,890 --> 00:20:10,340
that we keep hearing is all right

00:20:06,710 --> 00:20:13,940
that's code what about data right how

00:20:10,340 --> 00:20:17,690
you know is is data the new proprietary

00:20:13,940 --> 00:20:19,460
right is this where one thing that so I

00:20:17,690 --> 00:20:21,530
think there's some misconceptions and

00:20:19,460 --> 00:20:24,020
that a lot of people think that yeah you

00:20:21,530 --> 00:20:26,090
need Google size datasets and you need

00:20:24,020 --> 00:20:28,910
you know like millions of dollars worth

00:20:26,090 --> 00:20:30,110
of GPU power to do deep learning and

00:20:28,910 --> 00:20:31,400
those aren't the case and so kind of

00:20:30,110 --> 00:20:34,340
getting to your question about this like

00:20:31,400 --> 00:20:38,450
what if you don't have the data a lot of

00:20:34,340 --> 00:20:40,910
people are releasing pre-trained models

00:20:38,450 --> 00:20:43,340
so you know someone has a large data set

00:20:40,910 --> 00:20:44,540
train a model they release that model

00:20:43,340 --> 00:20:46,790
and then you can do something called

00:20:44,540 --> 00:20:48,890
transfer learning where you are

00:20:46,790 --> 00:20:51,740
fine-tuning that to a much smaller data

00:20:48,890 --> 00:20:53,270
set so we had a student download I think

00:20:51,740 --> 00:20:55,370
it was just twenty pictures of people

00:20:53,270 --> 00:20:57,620
playing baseball twenty pictures of

00:20:55,370 --> 00:20:59,870
people playing cricket and trained to

00:20:57,620 --> 00:21:03,740
classify or to tell cricket from

00:20:59,870 --> 00:21:04,850
baseball just using forty images and

00:21:03,740 --> 00:21:07,310
that's something because they were using

00:21:04,850 --> 00:21:09,710
this pre-trained net you know and just

00:21:07,310 --> 00:21:11,510
fine-tuning the last layers and so

00:21:09,710 --> 00:21:14,330
there's really amazing potential there I

00:21:11,510 --> 00:21:16,730
think in terms of being able to get deep

00:21:14,330 --> 00:21:18,590
learning to work on smaller datasets and

00:21:16,730 --> 00:21:21,860
it's part of why it's so important that

00:21:18,590 --> 00:21:25,460
people do share their weights and their

00:21:21,860 --> 00:21:27,770
models openly through open-source yeah

00:21:25,460 --> 00:21:29,810
you know we worked on a data sharing

00:21:27,770 --> 00:21:31,820
license agreement that would with both a

00:21:29,810 --> 00:21:33,740
copyleft one which would be a share back

00:21:31,820 --> 00:21:35,360
yeah license then a permissive license

00:21:33,740 --> 00:21:37,610
which she just didn't didn't need to do

00:21:35,360 --> 00:21:40,250
that but we're trying to get ahead of

00:21:37,610 --> 00:21:41,660
this ability to kind of share yeah and

00:21:40,250 --> 00:21:44,150
there are issues because it's like I

00:21:41,660 --> 00:21:46,220
mean data you know this is an important

00:21:44,150 --> 00:21:48,650
part of you know the of how models are

00:21:46,220 --> 00:21:50,960
trained but there's also so much around

00:21:48,650 --> 00:21:53,900
privacy and anonymous you know true

00:21:50,960 --> 00:21:54,740
anonymization is almost impossible and

00:21:53,900 --> 00:21:56,450
there have been several kind of

00:21:54,740 --> 00:21:58,360
high-profile cases of people thinking

00:21:56,450 --> 00:22:01,730
they've been Onam eyes data and then

00:21:58,360 --> 00:22:02,930
it's been D anonymized and so I think

00:22:01,730 --> 00:22:04,580
there is kind of a bit of a tension

00:22:02,930 --> 00:22:06,180
sometimes between yeah wanting to

00:22:04,580 --> 00:22:09,480
protect privacy

00:22:06,180 --> 00:22:10,830
and and yeah like it is really important

00:22:09,480 --> 00:22:13,200
though to be sharing models and your

00:22:10,830 --> 00:22:15,270
training process yeah I want to have to

00:22:13,200 --> 00:22:17,370
mass in here because you know me you're

00:22:15,270 --> 00:22:19,530
the telecommunications work it's very

00:22:17,370 --> 00:22:22,500
very competitive but you know to her

00:22:19,530 --> 00:22:24,690
point what our data said that you would

00:22:22,500 --> 00:22:26,550
want to share I don't know cell tower

00:22:24,690 --> 00:22:30,150
maintenance or whatever that just isn't

00:22:26,550 --> 00:22:31,590
a competitive per se but kind of follows

00:22:30,150 --> 00:22:33,300
that open source philosophy of like hey

00:22:31,590 --> 00:22:34,650
this is just data we won models that we

00:22:33,300 --> 00:22:38,520
just want to share with it do you see

00:22:34,650 --> 00:22:42,270
patterns there and so I I think the idea

00:22:38,520 --> 00:22:43,950
of having companies share data is not

00:22:42,270 --> 00:22:46,170
new as being people been talking about

00:22:43,950 --> 00:22:48,920
this for several decades we've never

00:22:46,170 --> 00:22:53,070
cracked it we've we've never created an

00:22:48,920 --> 00:22:56,040
open shareable infrastructure community

00:22:53,070 --> 00:22:58,410
where people can easily with security

00:22:56,040 --> 00:23:00,900
can share data with HIPAA requirements

00:22:58,410 --> 00:23:03,000
and their privacy rules I think we're

00:23:00,900 --> 00:23:04,680
starting to get almost there I think

00:23:03,000 --> 00:23:06,930
with you guys and and a lot of the

00:23:04,680 --> 00:23:08,340
policies you guys putting in place and

00:23:06,930 --> 00:23:10,140
that's what I'm hoping we're gonna do

00:23:08,340 --> 00:23:12,630
with activist Hakeem is cannot really

00:23:10,140 --> 00:23:15,120
succeed without having a really clear

00:23:12,630 --> 00:23:16,920
understanding of the data behind it from

00:23:15,120 --> 00:23:19,050
an AT&T they're obviously data we can't

00:23:16,920 --> 00:23:22,050
share there's no doubt about that

00:23:19,050 --> 00:23:24,900
you know we carry data and we track data

00:23:22,050 --> 00:23:26,520
about cell coverage so with those kind

00:23:24,900 --> 00:23:28,680
of data we cannot share but there are

00:23:26,520 --> 00:23:30,300
other data that we actually we want to

00:23:28,680 --> 00:23:32,640
do as part of the arguments to consider

00:23:30,300 --> 00:23:34,020
whether we can open a community looking

00:23:32,640 --> 00:23:36,180
at that just like you mentioned with

00:23:34,020 --> 00:23:38,130
transfer learning one of the we're a

00:23:36,180 --> 00:23:39,630
very capital-intensive company so you

00:23:38,130 --> 00:23:42,210
can imagine that one of the things we do

00:23:39,630 --> 00:23:44,400
we send people up we own millions of

00:23:42,210 --> 00:23:46,920
poles we own thousands of cell sites

00:23:44,400 --> 00:23:49,200
macro cells small cells etc we send

00:23:46,920 --> 00:23:50,940
people literally up a pole to go and

00:23:49,200 --> 00:23:52,500
check a cell to see if there's something

00:23:50,940 --> 00:23:55,620
wrong with it if a wire is disconnected

00:23:52,500 --> 00:23:57,000
it's rusty if there is dirt etc what

00:23:55,620 --> 00:24:00,200
we're trying to do now with transfer

00:23:57,000 --> 00:24:03,690
learning and is there we sent a drum and

00:24:00,200 --> 00:24:06,570
that drone has visual capabilities that

00:24:03,690 --> 00:24:08,880
drone detect whether there is what the

00:24:06,570 --> 00:24:10,350
object it's looking at and we can't do

00:24:08,880 --> 00:24:11,880
that we don't have enough data to do

00:24:10,350 --> 00:24:14,280
that so we've just collected few

00:24:11,880 --> 00:24:16,260
hundreds of those data points and then

00:24:14,280 --> 00:24:18,750
that drones look set to see if that

00:24:16,260 --> 00:24:20,020
object has a rust and if that's the

00:24:18,750 --> 00:24:21,670
issue that we confront

00:24:20,020 --> 00:24:24,130
do something about or send somebody

00:24:21,670 --> 00:24:25,570
there or not 95% of the time we don't

00:24:24,130 --> 00:24:27,970
need to send somebody so there's a

00:24:25,570 --> 00:24:30,000
safety aspect we're using this for we

00:24:27,970 --> 00:24:31,929
could never do that by collecting

00:24:30,000 --> 00:24:34,240
significant amount of data we can only

00:24:31,929 --> 00:24:36,210
do it with small set of data but thanks

00:24:34,240 --> 00:24:38,470
to the open-source community by having

00:24:36,210 --> 00:24:41,590
models it's not just data people think

00:24:38,470 --> 00:24:43,059
of data as raw data and I think raw data

00:24:41,590 --> 00:24:46,600
is very important and there are

00:24:43,059 --> 00:24:49,090
situations we cannot share raw data for

00:24:46,600 --> 00:24:51,910
many reasons but there are they're

00:24:49,090 --> 00:24:54,190
derivatives of data there are probably

00:24:51,910 --> 00:24:56,020
an area we need to talk more about which

00:24:54,190 --> 00:24:59,650
is that when you build these models

00:24:56,020 --> 00:25:02,440
these models now reflect weights and and

00:24:59,650 --> 00:25:04,090
capabilities that that resemble the data

00:25:02,440 --> 00:25:06,670
you cannot probably do a reverse

00:25:04,090 --> 00:25:09,550
engineering they hide the privacy aspect

00:25:06,670 --> 00:25:11,950
but those models can be shared and those

00:25:09,550 --> 00:25:13,570
models with some additional new data can

00:25:11,950 --> 00:25:14,740
do something really remarkable and

00:25:13,570 --> 00:25:17,110
that's exactly the kind of things you're

00:25:14,740 --> 00:25:19,390
talking about deepak how about you like

00:25:17,110 --> 00:25:21,130
at linkedin are there similar patterns

00:25:19,390 --> 00:25:23,800
where you're finding these sort of

00:25:21,130 --> 00:25:25,540
commodity components like you know how

00:25:23,800 --> 00:25:27,970
do you make those decisions how are you

00:25:25,540 --> 00:25:29,650
doing yeah I think so I agree with

00:25:27,970 --> 00:25:32,110
everything so one thing I would like to

00:25:29,650 --> 00:25:34,510
mention even if you if you're a

00:25:32,110 --> 00:25:35,920
developer we provide developer API is

00:25:34,510 --> 00:25:37,929
where you can actually get information

00:25:35,920 --> 00:25:39,460
about LinkedIn public profile

00:25:37,929 --> 00:25:42,040
information right so for instance you

00:25:39,460 --> 00:25:45,820
can you can you can use the API is to

00:25:42,040 --> 00:25:47,200
get a person's job title or you know if

00:25:45,820 --> 00:25:48,520
you want more information about the

00:25:47,200 --> 00:25:50,350
company like how many employees work

00:25:48,520 --> 00:25:51,460
there and how many people have changed

00:25:50,350 --> 00:25:53,470
up so these are things that are

00:25:51,460 --> 00:25:54,610
available to the developer API is there

00:25:53,470 --> 00:25:56,140
are some other information that we

00:25:54,610 --> 00:25:58,900
provide to companies those are not

00:25:56,140 --> 00:26:00,490
available for free so you have to talk

00:25:58,900 --> 00:26:01,960
to us and based on the use case we can

00:26:00,490 --> 00:26:05,200
still provide that to you so that's

00:26:01,960 --> 00:26:06,880
already happening now I think one

00:26:05,200 --> 00:26:09,040
example for instance I can give you

00:26:06,880 --> 00:26:10,750
right away for instance you know one of

00:26:09,040 --> 00:26:14,320
the things challenges we face on our

00:26:10,750 --> 00:26:16,809
news feed is there's a lot of non

00:26:14,320 --> 00:26:19,210
professional content that gets like

00:26:16,809 --> 00:26:22,300
things like hate speech and porn and

00:26:19,210 --> 00:26:23,710
things like that so we don't build those

00:26:22,300 --> 00:26:25,450
models from the scratch like for

00:26:23,710 --> 00:26:26,950
instance there have been a lot of nice

00:26:25,450 --> 00:26:29,140
models that have been built by using

00:26:26,950 --> 00:26:31,240
image net data using resonate and again

00:26:29,140 --> 00:26:33,100
we use the same technique like the top

00:26:31,240 --> 00:26:35,940
of the last two layers of the neural net

00:26:33,100 --> 00:26:38,440
and then customize it for our use cases

00:26:35,940 --> 00:26:39,669
we have a lot of different teams that

00:26:38,440 --> 00:26:41,799
are working with different problems so

00:26:39,669 --> 00:26:44,110
we have a notion of what we call

00:26:41,799 --> 00:26:46,960
internally a feature marketplace right

00:26:44,110 --> 00:26:48,280
so features are signals that go into

00:26:46,960 --> 00:26:50,620
your machine learning models and we

00:26:48,280 --> 00:26:52,360
don't want every team to be building the

00:26:50,620 --> 00:26:54,190
same signals over and over again so

00:26:52,360 --> 00:26:56,770
there is this there is there is a

00:26:54,190 --> 00:26:59,140
framework which where you know if you

00:26:56,770 --> 00:27:01,210
create a certain user interest vector

00:26:59,140 --> 00:27:02,770
for instance or signal that kind of

00:27:01,210 --> 00:27:04,120
captures a user interest you can

00:27:02,770 --> 00:27:05,200
actually share it with every other team

00:27:04,120 --> 00:27:06,700
right so it goes into the future

00:27:05,200 --> 00:27:08,620
marketplace that anyone can then grab it

00:27:06,700 --> 00:27:10,840
and start using it in their model so

00:27:08,620 --> 00:27:12,820
that's how we are scaling it and you

00:27:10,840 --> 00:27:14,350
know we are in a world where we are we

00:27:12,820 --> 00:27:17,679
are not just doing machine learning

00:27:14,350 --> 00:27:19,179
where we don't have only experts doing

00:27:17,679 --> 00:27:22,150
machine learning anymore we have opened

00:27:19,179 --> 00:27:23,830
up the machine learning to every single

00:27:22,150 --> 00:27:25,059
software engineer in the company so we

00:27:23,830 --> 00:27:26,380
have training programs where every

00:27:25,059 --> 00:27:27,580
software engineer can get trained in

00:27:26,380 --> 00:27:29,530
machine learning we have each or

00:27:27,580 --> 00:27:31,990
marketplace these think of these as

00:27:29,530 --> 00:27:33,549
cookie cutter prefabricated features if

00:27:31,990 --> 00:27:36,039
you alright so if you are a developer

00:27:33,549 --> 00:27:37,270
you take a course you have prefabricated

00:27:36,039 --> 00:27:38,919
features available in the marketplace

00:27:37,270 --> 00:27:40,809
and you have a problem that you want to

00:27:38,919 --> 00:27:42,640
solve in the product you can go take

00:27:40,809 --> 00:27:43,990
this prefabricated feature build a model

00:27:42,640 --> 00:27:45,490
and deploy it in your product you don't

00:27:43,990 --> 00:27:48,490
even have to talk to an expert in many

00:27:45,490 --> 00:27:49,419
cases ok I had a question this morning

00:27:48,490 --> 00:27:50,679
that I want to I'll start with you

00:27:49,419 --> 00:27:52,770
terian didn't want to go to Rachel in

00:27:50,679 --> 00:27:54,880
Madison and come back to you debug that

00:27:52,770 --> 00:27:58,350
it's not necessarily on our list of

00:27:54,880 --> 00:27:58,350
questions so I'm going to surprise you

00:28:00,870 --> 00:28:05,919
surprise so you have a lot of developers

00:28:04,299 --> 00:28:09,370
and people who care about coding out

00:28:05,919 --> 00:28:11,080
here and Derek you'll remember this on

00:28:09,370 --> 00:28:13,510
the 25th anniversary of Linux I made a

00:28:11,080 --> 00:28:16,299
toast the kernel community and folks

00:28:13,510 --> 00:28:18,429
saying you know congratulations on 25

00:28:16,299 --> 00:28:20,679
years of Linux and I want to announce

00:28:18,429 --> 00:28:23,409
our next big project after Linux which

00:28:20,679 --> 00:28:25,840
is an artificial intelligence and that

00:28:23,409 --> 00:28:28,000
is actually self a self coding platform

00:28:25,840 --> 00:28:31,390
so drink up that's tonight's your last

00:28:28,000 --> 00:28:32,559
night of employment but this is I get

00:28:31,390 --> 00:28:34,659
this question all the time in their

00:28:32,559 --> 00:28:36,520
startup there's a start-up in Spain

00:28:34,659 --> 00:28:38,770
called sourced which sort of cashed all

00:28:36,520 --> 00:28:40,830
the code that's on github and ever been

00:28:38,770 --> 00:28:43,840
written in a lot of other repositories

00:28:40,830 --> 00:28:44,409
just as experts on it I'll start with

00:28:43,840 --> 00:28:46,600
you two

00:28:44,409 --> 00:28:49,600
when are we gonna get to self

00:28:46,600 --> 00:28:51,760
- aware Cody like to ability to either

00:28:49,600 --> 00:28:53,559
like as you're coding to use AI to

00:28:51,760 --> 00:28:57,580
improve the quality or to actually have

00:28:53,559 --> 00:29:03,429
self coding systems crazily I get this

00:28:57,580 --> 00:29:06,760
question all the time you know what you

00:29:03,429 --> 00:29:08,169
probably are never gonna get and I hope

00:29:06,760 --> 00:29:09,789
you're not gonna ask about these

00:29:08,169 --> 00:29:12,280
generally I kind of thinks because I

00:29:09,789 --> 00:29:15,970
kind of shut down I'll get it I told ya

00:29:12,280 --> 00:29:18,340
the evil robots - yeah this is

00:29:15,970 --> 00:29:20,350
specifically I think the most beautiful

00:29:18,340 --> 00:29:25,090
thing is that there's a whole lot of

00:29:20,350 --> 00:29:27,429
code up there the intuitions behind the

00:29:25,090 --> 00:29:29,350
way the code has been written is not

00:29:27,429 --> 00:29:31,990
something you can encapsulate in

00:29:29,350 --> 00:29:34,090
software and create a kind of an

00:29:31,990 --> 00:29:36,580
automated software development kind of

00:29:34,090 --> 00:29:38,919
library that says you know these guys

00:29:36,580 --> 00:29:41,200
are great in in in convolutional neural

00:29:38,919 --> 00:29:44,200
networks image classification kind of

00:29:41,200 --> 00:29:45,909
technology that these guys are great in

00:29:44,200 --> 00:29:48,309
in recurrent neural networks this

00:29:45,909 --> 00:29:51,039
advanced kind of you know text analysis

00:29:48,309 --> 00:29:53,080
and making kind of predictions what you

00:29:51,039 --> 00:29:55,659
will not get is the intuition of what is

00:29:53,080 --> 00:29:58,330
coming next so as long as you are

00:29:55,659 --> 00:30:01,510
building things for instance just an

00:29:58,330 --> 00:30:03,789
example we know you know there's been

00:30:01,510 --> 00:30:05,559
delay and and I've studied astronomy as

00:30:03,789 --> 00:30:07,870
well so I really really follow a whole

00:30:05,559 --> 00:30:10,299
lot of things in a code also on the side

00:30:07,870 --> 00:30:11,679
- to understand how we're kind of you

00:30:10,299 --> 00:30:14,169
know learning about gravitational waves

00:30:11,679 --> 00:30:16,780
and all that so we have web going into

00:30:14,169 --> 00:30:18,700
the space next year sometime in June and

00:30:16,780 --> 00:30:20,590
it keeps getting delayed it's really sad

00:30:18,700 --> 00:30:22,150
but anyways so we're gonna get huge

00:30:20,590 --> 00:30:24,850
humongous amount of data coming out of

00:30:22,150 --> 00:30:27,100
the universe at us and and to do that

00:30:24,850 --> 00:30:28,720
you that's you know that software which

00:30:27,100 --> 00:30:30,429
we are super excited and self

00:30:28,720 --> 00:30:32,740
congratulating that you know we have

00:30:30,429 --> 00:30:34,480
great software it's not gonna help to a

00:30:32,740 --> 00:30:37,299
certain extent it will definitely help

00:30:34,480 --> 00:30:38,530
in best practices and and you know doing

00:30:37,299 --> 00:30:40,600
unit testing and all those things

00:30:38,530 --> 00:30:42,970
definitely huge scope in making those

00:30:40,600 --> 00:30:44,500
things work and automating it I say

00:30:42,970 --> 00:30:46,330
really I think we should automate that

00:30:44,500 --> 00:30:47,860
so from that perspective I think

00:30:46,330 --> 00:30:50,620
shouldn't worry too much about jobs

00:30:47,860 --> 00:30:51,669
getting out of you know the door because

00:30:50,620 --> 00:30:53,020
there's a whole lot of beautiful things

00:30:51,669 --> 00:30:54,760
we need to do if you need to you know

00:30:53,020 --> 00:30:56,200
colonize Mars or here's something I

00:30:54,760 --> 00:30:57,490
wanted to do as well that's a kid I said

00:30:56,200 --> 00:30:59,710
I couldn't Ben you know this gentleman

00:30:57,490 --> 00:31:00,130
from South Africa who comes to us and

00:30:59,710 --> 00:31:02,110
says

00:31:00,130 --> 00:31:03,880
your company says hey okay makes sense

00:31:02,110 --> 00:31:05,140
but so there's a whole lot of you

00:31:03,880 --> 00:31:07,450
beautiful things we need to solve the

00:31:05,140 --> 00:31:09,340
universe so from that perspective that's

00:31:07,450 --> 00:31:11,230
definite like 60s how many percent of

00:31:09,340 --> 00:31:14,920
encapsulation of doing things which we

00:31:11,230 --> 00:31:19,000
don't need to do anymore can be grabbed

00:31:14,920 --> 00:31:20,950
literally from github and from even you

00:31:19,000 --> 00:31:23,530
know best practices on Stack Overflow

00:31:20,950 --> 00:31:25,390
whatever and you can put these and kind

00:31:23,530 --> 00:31:27,490
of provide guidance so people don't lose

00:31:25,390 --> 00:31:28,930
time they spend a lot of time doing a

00:31:27,490 --> 00:31:30,880
whole lot of stuff which we should not

00:31:28,930 --> 00:31:33,310
be doing so from that perspective I

00:31:30,880 --> 00:31:35,380
totally agree but I don't think my

00:31:33,310 --> 00:31:37,270
cognition as a human being a single

00:31:35,380 --> 00:31:39,370
human being I can already envision a

00:31:37,270 --> 00:31:40,990
universe I don't need like 200 people to

00:31:39,370 --> 00:31:43,000
do that I can do that already you know

00:31:40,990 --> 00:31:44,920
that power has been given to me the

00:31:43,000 --> 00:31:46,840
other thing is my intuitions of trying

00:31:44,920 --> 00:31:49,120
to solve a problem in it could be any

00:31:46,840 --> 00:31:51,580
problem here right now physical problem

00:31:49,120 --> 00:31:55,360
object detection problem is something is

00:31:51,580 --> 00:31:55,900
has doesn't exist at all in in software

00:31:55,360 --> 00:31:57,640
library

00:31:55,900 --> 00:31:59,650
what does definitely exists as best

00:31:57,640 --> 00:32:03,490
practices which I definitely search and

00:31:59,650 --> 00:32:05,500
seek out too and so yeah I think so I

00:32:03,490 --> 00:32:07,750
guess you know sixty seventy percent of

00:32:05,500 --> 00:32:09,970
that work definitely can be automated so

00:32:07,750 --> 00:32:11,770
salt encoding what we can call itself

00:32:09,970 --> 00:32:13,870
coding but basically it's just grabbed

00:32:11,770 --> 00:32:15,910
information understands how you want to

00:32:13,870 --> 00:32:18,210
follow that logic and eventually you

00:32:15,910 --> 00:32:20,530
know throw it into your into your

00:32:18,210 --> 00:32:22,990
algorithms and or in your software

00:32:20,530 --> 00:32:25,390
library and do that and the other 30%

00:32:22,990 --> 00:32:27,490
just keep you know hold on to it you

00:32:25,390 --> 00:32:30,150
know your intelligence in your beautiful

00:32:27,490 --> 00:32:32,320
cognition and your powers to you know

00:32:30,150 --> 00:32:34,180
grab the universe and make it your own

00:32:32,320 --> 00:32:36,070
are yours they're not going anyway

00:32:34,180 --> 00:32:38,530
they're not going away all right quick

00:32:36,070 --> 00:32:40,600
quick last word from Rachel Mazen and

00:32:38,530 --> 00:32:43,090
Deepak on you know advice to coders out

00:32:40,600 --> 00:32:45,760
there and how can they use MLA I to

00:32:43,090 --> 00:32:48,070
improve their projects - yeah I mean I'd

00:32:45,760 --> 00:32:49,840
say with them and one just to know that

00:32:48,070 --> 00:32:56,310
it's possible if you know how to code

00:32:49,840 --> 00:32:59,980
you can learn to use deep learning that

00:32:56,310 --> 00:33:02,860
domain expertise is still incredibly

00:32:59,980 --> 00:33:04,750
valuable and so I think something I hear

00:33:02,860 --> 00:33:07,870
a lot from companies is you know like oh

00:33:04,750 --> 00:33:09,850
it's so hard to hire like a Stanford PhD

00:33:07,870 --> 00:33:10,400
and it's just that's not what you need

00:33:09,850 --> 00:33:13,400
it

00:33:10,400 --> 00:33:15,230
like the people that are already working

00:33:13,400 --> 00:33:17,210
with and for you or kind of the right

00:33:15,230 --> 00:33:20,990
people like they understand your problem

00:33:17,210 --> 00:33:23,000
and your domain and so I guess again

00:33:20,990 --> 00:33:25,160
this is something that I think is like

00:33:23,000 --> 00:33:28,310
whatever around and I know here it's

00:33:25,160 --> 00:33:31,310
where a lot of coders but like having

00:33:28,310 --> 00:33:33,620
specialized knowledge around a domain is

00:33:31,310 --> 00:33:37,250
still super crucial so this showed up

00:33:33,620 --> 00:33:38,300
recently MIT released deep learning

00:33:37,250 --> 00:33:40,610
course and I don't know anything about

00:33:38,300 --> 00:33:42,230
the course but the image that they led

00:33:40,610 --> 00:33:44,960
with was like you know see why the

00:33:42,230 --> 00:33:46,910
algorithm predicted pneumothorax you

00:33:44,960 --> 00:33:49,610
know it's a picture of the lungs and a

00:33:46,910 --> 00:33:51,260
radiologist she's also machine learning

00:33:49,610 --> 00:33:52,730
specialist responded and was like oh

00:33:51,260 --> 00:33:54,830
that doesn't make sense you know like

00:33:52,730 --> 00:33:56,800
that model must have been over fitted

00:33:54,830 --> 00:33:59,600
you know and so that like that kind of

00:33:56,800 --> 00:34:02,120
domain expertise is going to remain very

00:33:59,600 --> 00:34:04,130
valuable for a long time and like one of

00:34:02,120 --> 00:34:06,860
our goals at fast AI is to kind of be

00:34:04,130 --> 00:34:08,840
taking the domain experts and teaching

00:34:06,860 --> 00:34:12,890
them deep learning as opposed to trying

00:34:08,840 --> 00:34:15,980
to engage deep learning specialists with

00:34:12,890 --> 00:34:22,130
your particular domain yeah amazing

00:34:15,980 --> 00:34:23,690
Deepak yes sir so I think you know I do

00:34:22,130 --> 00:34:25,490
machine learning or AI you need three

00:34:23,690 --> 00:34:27,800
things one is you need to know what your

00:34:25,490 --> 00:34:29,780
objectives are you need to know what

00:34:27,800 --> 00:34:33,020
you're trying to build the algorithm to

00:34:29,780 --> 00:34:34,280
do you need data label data and then you

00:34:33,020 --> 00:34:35,960
need algorithms that can learn from

00:34:34,280 --> 00:34:37,840
leader so if you have a simple objective

00:34:35,960 --> 00:34:40,340
like identifying a cat inner image

00:34:37,840 --> 00:34:41,870
that's easy I think that is something

00:34:40,340 --> 00:34:43,520
that you can put together very quickly

00:34:41,870 --> 00:34:45,440
like with few lines of code because all

00:34:43,520 --> 00:34:47,270
the other materials are available right

00:34:45,440 --> 00:34:48,770
as long as you have label data but let's

00:34:47,270 --> 00:34:51,260
say if I have to solve a more complex

00:34:48,770 --> 00:34:53,929
objective like I want more users to come

00:34:51,260 --> 00:34:56,270
to LinkedIn every day this is a very

00:34:53,929 --> 00:34:57,890
complex objective right like users can

00:34:56,270 --> 00:34:59,540
come to LinkedIn because they like the

00:34:57,890 --> 00:35:01,700
newsfeed because they want to connect to

00:34:59,540 --> 00:35:05,150
people because they want to search for

00:35:01,700 --> 00:35:06,830
someone how do you what we do the series

00:35:05,150 --> 00:35:09,250
of machine learning problems that can

00:35:06,830 --> 00:35:12,440
actually solve this objective is very

00:35:09,250 --> 00:35:14,900
difficult like this you cannot in Cacak

00:35:12,440 --> 00:35:17,060
encapsulate at least today in a software

00:35:14,900 --> 00:35:19,250
right so that's where you have to go and

00:35:17,060 --> 00:35:21,440
understand the domain very well do some

00:35:19,250 --> 00:35:23,150
data analysis and once you're able to

00:35:21,440 --> 00:35:24,980
formulate those series of

00:35:23,150 --> 00:35:27,020
objectives then again the coding is very

00:35:24,980 --> 00:35:29,180
easy right so I would say I think in a

00:35:27,020 --> 00:35:32,569
few years everything else will get

00:35:29,180 --> 00:35:35,359
commoditized pretty much not completely

00:35:32,569 --> 00:35:37,010
but pretty much and we have to spend a

00:35:35,359 --> 00:35:38,450
lot more time understanding what you're

00:35:37,010 --> 00:35:40,490
really trying to solve like if you want

00:35:38,450 --> 00:35:42,589
to do job recommendations without caring

00:35:40,490 --> 00:35:44,150
about diversity well you know if she if

00:35:42,589 --> 00:35:46,220
your algorithm does not take care of

00:35:44,150 --> 00:35:47,869
that it's not going to optimize for that

00:35:46,220 --> 00:35:49,849
it's just going to optimize for a number

00:35:47,869 --> 00:35:51,680
of applications but if you tell the

00:35:49,849 --> 00:35:53,660
algorithm no I care about that as well

00:35:51,680 --> 00:35:55,339
and put that as part of the objective

00:35:53,660 --> 00:35:58,069
then it's going to do something about it

00:35:55,339 --> 00:35:59,990
so that's that's probably what's going

00:35:58,069 --> 00:36:01,970
to happen in the next ten years as we

00:35:59,990 --> 00:36:03,470
could start commoditizing and we will be

00:36:01,970 --> 00:36:07,579
able to solve more complex problem than

00:36:03,470 --> 00:36:09,470
what we are able to do today um coders

00:36:07,579 --> 00:36:12,289
you know doesn't matter what your

00:36:09,470 --> 00:36:14,270
background is absolutely get involved do

00:36:12,289 --> 00:36:16,279
some training and learn this field in my

00:36:14,270 --> 00:36:18,170
organization every code every program or

00:36:16,279 --> 00:36:20,000
hast it doesn't matter what they're they

00:36:18,170 --> 00:36:22,099
have a PhD or a master's whatever it is

00:36:20,000 --> 00:36:23,599
100 percent compliance this year they

00:36:22,099 --> 00:36:25,460
all have to learn it they all have to be

00:36:23,599 --> 00:36:28,250
able to code machine learning and AI

00:36:25,460 --> 00:36:31,190
capabilities AT&T in general we put out

00:36:28,250 --> 00:36:32,750
six months ago a plan a program that

00:36:31,190 --> 00:36:34,819
everybody in the company we have three

00:36:32,750 --> 00:36:37,760
hundred thousand employees to be able to

00:36:34,819 --> 00:36:39,650
go through ml AI training even if you're

00:36:37,760 --> 00:36:41,029
a marketing or legal or whatever we all

00:36:39,650 --> 00:36:43,730
need to know this is not about just

00:36:41,029 --> 00:36:44,930
coders it's about everybody we the

00:36:43,730 --> 00:36:46,099
people in legal team they need to

00:36:44,930 --> 00:36:48,020
understand and the marketing team need

00:36:46,099 --> 00:36:50,059
to understand it financial people

00:36:48,020 --> 00:36:53,240
everyone really need to come to the same

00:36:50,059 --> 00:36:56,539
page so we go away from every problem AI

00:36:53,240 --> 00:36:58,160
is the solution that hype to really what

00:36:56,539 --> 00:37:00,470
are the key problems we need to solve

00:36:58,160 --> 00:37:02,720
for from an AT&T and for me many of the

00:37:00,470 --> 00:37:04,670
coders is there look where the problems

00:37:02,720 --> 00:37:06,680
are where the challenges are also I have

00:37:04,670 --> 00:37:08,930
always telling my team three things look

00:37:06,680 --> 00:37:10,730
where we're spending a lot of money to

00:37:08,930 --> 00:37:12,619
look where there are opportunities of

00:37:10,730 --> 00:37:14,329
revenue we can have and we could do with

00:37:12,619 --> 00:37:16,339
these technologies and three is safety

00:37:14,329 --> 00:37:18,109
there are places where we can apply

00:37:16,339 --> 00:37:20,599
these technologies and I mentioned the

00:37:18,109 --> 00:37:22,309
thing about polls and the 5g for safety

00:37:20,599 --> 00:37:24,440
and I think that's really if you can

00:37:22,309 --> 00:37:26,779
start with a real problem just like what

00:37:24,440 --> 00:37:29,270
deep I just mentioned real problem that

00:37:26,779 --> 00:37:31,789
really wants a solution that's probably

00:37:29,270 --> 00:37:33,380
a long way to go so here here if you're

00:37:31,789 --> 00:37:35,150
not training everybody in the

00:37:33,380 --> 00:37:36,390
organization you're gonna be caught up

00:37:35,150 --> 00:37:39,269
and it never ending pipes

00:37:36,390 --> 00:37:41,279
a call and that's a pretty interesting

00:37:39,269 --> 00:37:43,260
practical way that AT&T is handling that

00:37:41,279 --> 00:37:44,700
problem so thank you everyone I really

00:37:43,260 --> 00:37:51,660
appreciate you coming here today

00:37:44,700 --> 00:37:51,660

YouTube URL: https://www.youtube.com/watch?v=NlqT_MTH-nw


