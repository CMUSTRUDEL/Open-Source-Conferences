Title: Enabling Autonomous Teams With Policy Enforcement at Yubico - James Alseth & John Reese, Yubico
Publication date: 2020-12-07
Playlist: Cloud Native Security Day North America 2020
Description: 
	Enabling Autonomous Teams With Policy Enforcement at Yubico - James Alseth & John Reese, Yubico

In this talk, we will discuss the tools and processes created by Yubico to enable autonomous teams through policy.  Initially, Kubernetes RBAC and peer reviews from our Platform team allowed teams to adopt Kubernetes for their services. However, we knew that a dependency on a single team was not a scalable solution.  To give teams more autonomy over their services, and rely less on manual reviews, we began to enforce policies in our pipelines and clusters by leveraging the Open Policy Agent. The Open Policy Agent and its surrounding projects were the perfect fit for us; they are open source, flexible, performant, and have seen widespread adoption throughout the ecosystem.  We'll also discuss the tooling that was built that enabled us to test policies, automatically generate supporting documentation and audit how each policy is being used so that they can be safely promoted through our environments. Best of all? They are all open source!
Captions: 
	00:00:00,000 --> 00:00:03,439
hi welcome to our talk on enabling

00:00:02,080 --> 00:00:04,400
autonomous teams through policy

00:00:03,439 --> 00:00:05,759
enforcement

00:00:04,400 --> 00:00:07,759
before we get started a quick

00:00:05,759 --> 00:00:09,200
introduction about ourselves

00:00:07,759 --> 00:00:11,040
my name is james alsoth and i'm a

00:00:09,200 --> 00:00:13,759
security engineer at ubico currently

00:00:11,040 --> 00:00:15,759
focused on cloud infrastructure security

00:00:13,759 --> 00:00:17,600
presenting with me is john reese who is

00:00:15,759 --> 00:00:20,080
a software engineer at yubico with a lot

00:00:17,600 --> 00:00:21,760
of experience in go and kubernetes

00:00:20,080 --> 00:00:23,760
before we get started let's go over what

00:00:21,760 --> 00:00:25,039
we're going to be discussing today

00:00:23,760 --> 00:00:26,960
first we're going to start with a brief

00:00:25,039 --> 00:00:28,560
history of kubernetes at yubico

00:00:26,960 --> 00:00:30,560
how we got started on our kubernetes

00:00:28,560 --> 00:00:33,200
journey and some of the previous gaps in

00:00:30,560 --> 00:00:35,120
our kubernetes tech stack

00:00:33,200 --> 00:00:36,800
we're then going to discuss how policy

00:00:35,120 --> 00:00:38,800
helps to address those gaps and enable

00:00:36,800 --> 00:00:40,079
more autonomous teams

00:00:38,800 --> 00:00:41,760
we're definitely going to talk about the

00:00:40,079 --> 00:00:44,320
awesome open source tooling that enables

00:00:41,760 --> 00:00:46,000
us to enforce these policies

00:00:44,320 --> 00:00:47,920
and we're going to wrap up with

00:00:46,000 --> 00:00:49,440
discussing our journey thus far

00:00:47,920 --> 00:00:51,280
sort of where we are now and where we

00:00:49,440 --> 00:00:52,640
see ourselves in the future

00:00:51,280 --> 00:00:55,520
and of course there will be some time

00:00:52,640 --> 00:00:57,280
with them for questions

00:00:55,520 --> 00:00:58,800
so let's chat about how ubico got

00:00:57,280 --> 00:00:59,680
started on its kubernetes adoption

00:00:58,800 --> 00:01:01,840
journey

00:00:59,680 --> 00:01:03,120
for us that started about two years ago

00:01:01,840 --> 00:01:04,799
with an initiative led by our

00:01:03,120 --> 00:01:07,040
infrastructure team to standardize the

00:01:04,799 --> 00:01:09,280
platform that our services run on

00:01:07,040 --> 00:01:10,720
uh previous to this we were mostly

00:01:09,280 --> 00:01:12,080
running on virtual machines

00:01:10,720 --> 00:01:14,960
and we didn't have too many

00:01:12,080 --> 00:01:16,960
containerized workloads yet

00:01:14,960 --> 00:01:18,479
like many organizations we used to

00:01:16,960 --> 00:01:20,000
manage kubernetes service to get up and

00:01:18,479 --> 00:01:21,680
running as fast as possible

00:01:20,000 --> 00:01:24,840
and to help avoid some of the pain

00:01:21,680 --> 00:01:26,080
points of cluster setup and cluster

00:01:24,840 --> 00:01:27,280
management

00:01:26,080 --> 00:01:29,040
for us though probably the first

00:01:27,280 --> 00:01:30,400
question that pops into our head is how

00:01:29,040 --> 00:01:32,640
can we ensure that the kubernetes

00:01:30,400 --> 00:01:34,720
workloads are configured securely

00:01:32,640 --> 00:01:36,320
and what that's really asking is how do

00:01:34,720 --> 00:01:37,680
we control changes that are made to the

00:01:36,320 --> 00:01:39,439
cluster

00:01:37,680 --> 00:01:42,560
well we started where i think most

00:01:39,439 --> 00:01:45,200
organizations do leaning on three things

00:01:42,560 --> 00:01:48,320
authentication authorization as well as

00:01:45,200 --> 00:01:48,320
consistent peer review

00:01:48,640 --> 00:01:52,799
diving into the first of those we were

00:01:51,200 --> 00:01:54,079
able to take advantage of the managed

00:01:52,799 --> 00:01:55,840
kubernetes offering

00:01:54,079 --> 00:01:58,000
than that it allowed us to tie into our

00:01:55,840 --> 00:01:59,040
existing identity provider very easily

00:01:58,000 --> 00:02:01,200
so we got up and running with

00:01:59,040 --> 00:02:02,880
authentication pretty quick

00:02:01,200 --> 00:02:04,479
additionally it probably comes to no

00:02:02,880 --> 00:02:05,600
surprise to those of you who are

00:02:04,479 --> 00:02:07,680
familiar with yubico

00:02:05,600 --> 00:02:09,280
but we also require strong multi-factor

00:02:07,680 --> 00:02:12,720
authentication with web on

00:02:09,280 --> 00:02:14,480
web authent and yubikeys additionally we

00:02:12,720 --> 00:02:16,319
regularly expire the sessions for those

00:02:14,480 --> 00:02:18,080
with access to infrastructure

00:02:16,319 --> 00:02:21,920
requiring re-authentication with

00:02:18,080 --> 00:02:22,640
multi-factor authentication frequently

00:02:21,920 --> 00:02:24,720
moving on to

00:02:22,640 --> 00:02:26,640
the role-based authorization thankfully

00:02:24,720 --> 00:02:27,280
kubernetes has role-based access control

00:02:26,640 --> 00:02:29,680
built in

00:02:27,280 --> 00:02:30,879
and it has for quite a few versions now

00:02:29,680 --> 00:02:34,080
this allows us to tie

00:02:30,879 --> 00:02:36,400
users groups and service accounts to any

00:02:34,080 --> 00:02:37,120
any role these roles can either be

00:02:36,400 --> 00:02:39,760
scoped to

00:02:37,120 --> 00:02:41,360
a namespace or they can be applied

00:02:39,760 --> 00:02:42,879
cluster-wide

00:02:41,360 --> 00:02:44,400
these roles allow for extremely

00:02:42,879 --> 00:02:46,400
fine-grained permissions

00:02:44,400 --> 00:02:47,440
allowing you to specify the exact verbs

00:02:46,400 --> 00:02:50,080
an actor can use

00:02:47,440 --> 00:02:52,000
such as create update or delete what

00:02:50,080 --> 00:02:54,400
types of resources they can act on

00:02:52,000 --> 00:02:55,440
such as a deployment resource and it can

00:02:54,400 --> 00:02:57,200
even go as far as

00:02:55,440 --> 00:03:00,720
allow restricting it only to

00:02:57,200 --> 00:03:02,480
specifically named resources

00:03:00,720 --> 00:03:04,640
again our managed kubernetes offering

00:03:02,480 --> 00:03:09,280
made this easier by tying groups into

00:03:04,640 --> 00:03:09,280
from our idp into this system

00:03:09,760 --> 00:03:12,800
and the final tool in our toolbelt was

00:03:11,599 --> 00:03:14,560
peer review

00:03:12,800 --> 00:03:16,239
for us we enforce this using github

00:03:14,560 --> 00:03:17,680
branch protection rules ensuring that

00:03:16,239 --> 00:03:18,800
all changes happened through pull

00:03:17,680 --> 00:03:20,319
requests

00:03:18,800 --> 00:03:22,319
and on each of these pull requests we

00:03:20,319 --> 00:03:24,159
required at least one other person to

00:03:22,319 --> 00:03:26,720
review

00:03:24,159 --> 00:03:27,200
for us most of this peer review work

00:03:26,720 --> 00:03:29,440
landed

00:03:27,200 --> 00:03:30,959
on our infrastructure team because they

00:03:29,440 --> 00:03:32,879
had the most experience of kubernetes

00:03:30,959 --> 00:03:34,640
and had spent the most time to

00:03:32,879 --> 00:03:37,840
learn about all of the best practices

00:03:34,640 --> 00:03:37,840
security or otherwise

00:03:38,000 --> 00:03:42,239
about peer review though it definitely

00:03:40,560 --> 00:03:43,840
has some drawbacks when it's the only

00:03:42,239 --> 00:03:44,879
way that you're restricting changes to

00:03:43,840 --> 00:03:47,040
your clusters

00:03:44,879 --> 00:03:48,480
for one it needs to be consistent in

00:03:47,040 --> 00:03:50,239
order to be effective

00:03:48,480 --> 00:03:52,000
and when you have consistent review

00:03:50,239 --> 00:03:53,680
that's a significant time investment for

00:03:52,000 --> 00:03:55,599
the reviewers

00:03:53,680 --> 00:03:57,680
because of this it often bottlenecks on

00:03:55,599 --> 00:03:59,200
the team or individual that has the most

00:03:57,680 --> 00:04:00,319
experience with the technology

00:03:59,200 --> 00:04:03,200
which of course in this case is

00:04:00,319 --> 00:04:04,959
kubernetes and all of this adds up to

00:04:03,200 --> 00:04:06,799
slowing down the release cycle

00:04:04,959 --> 00:04:08,319
and this is important because when he

00:04:06,799 --> 00:04:09,920
teams hit too much friction

00:04:08,319 --> 00:04:12,640
they often start to work around your

00:04:09,920 --> 00:04:14,400
processes whether you know it or not

00:04:12,640 --> 00:04:16,079
this is of course bad for security

00:04:14,400 --> 00:04:17,919
because you no longer have control over

00:04:16,079 --> 00:04:19,759
these configurations

00:04:17,919 --> 00:04:24,079
but they're also bad for just general

00:04:19,759 --> 00:04:24,079
cluster consistency and maintainability

00:04:24,320 --> 00:04:27,520
none of this was really a surprise to us

00:04:26,800 --> 00:04:29,199
we kind of

00:04:27,520 --> 00:04:30,639
saw this coming from a mile away but

00:04:29,199 --> 00:04:31,680
that didn't make the problem any less

00:04:30,639 --> 00:04:34,880
real when we had to deal

00:04:31,680 --> 00:04:36,479
with it so what we did is we spent some

00:04:34,880 --> 00:04:38,000
time researching what other

00:04:36,479 --> 00:04:39,759
organizations were doing and what the

00:04:38,000 --> 00:04:41,600
kubernetes community was doing

00:04:39,759 --> 00:04:44,560
and for us the answer became abundantly

00:04:41,600 --> 00:04:46,240
clear policy was the way forward

00:04:44,560 --> 00:04:47,840
when people think of policy it's usually

00:04:46,240 --> 00:04:49,440
a negative reaction as they

00:04:47,840 --> 00:04:52,320
imagine having more hoops to jump

00:04:49,440 --> 00:04:54,880
through in order to get their work done

00:04:52,320 --> 00:04:55,440
however in our case since all of these

00:04:54,880 --> 00:04:57,199
changes

00:04:55,440 --> 00:04:59,759
to kubernetes happen through the api

00:04:57,199 --> 00:05:01,280
server working with structured json data

00:04:59,759 --> 00:05:04,000
we can automate the enforcement of these

00:05:01,280 --> 00:05:05,440
policies entirely

00:05:04,000 --> 00:05:07,120
but what do we mean when we say that in

00:05:05,440 --> 00:05:09,039
this context well

00:05:07,120 --> 00:05:11,520
it allows us to enforce what we actually

00:05:09,039 --> 00:05:13,600
care about for example we don't really

00:05:11,520 --> 00:05:15,039
care that a services team is deploying a

00:05:13,600 --> 00:05:17,280
new version of their service

00:05:15,039 --> 00:05:18,880
that's a part of their core job function

00:05:17,280 --> 00:05:19,280
however we do care that when they do

00:05:18,880 --> 00:05:22,400
that

00:05:19,280 --> 00:05:24,800
the resources are configured securely

00:05:22,400 --> 00:05:26,160
for example we probably want to ensure

00:05:24,800 --> 00:05:26,800
that the workloads aren't running as

00:05:26,160 --> 00:05:28,880
root

00:05:26,800 --> 00:05:31,600
and that they don't have any extra linux

00:05:28,880 --> 00:05:33,360
capabilities attached to them

00:05:31,600 --> 00:05:35,680
these policies can also easily extend

00:05:33,360 --> 00:05:38,479
past security related settings though

00:05:35,680 --> 00:05:38,960
for example we can require each resource

00:05:38,479 --> 00:05:40,800
in a

00:05:38,960 --> 00:05:42,080
namespace to have a certain label set

00:05:40,800 --> 00:05:43,360
that identifies the owner of that

00:05:42,080 --> 00:05:45,199
resource

00:05:43,360 --> 00:05:46,880
that makes it easy so when you're

00:05:45,199 --> 00:05:48,800
working on troubleshooting an issue

00:05:46,880 --> 00:05:50,400
or you just need to know who's who owns

00:05:48,800 --> 00:05:53,199
that resource it's right there in the

00:05:50,400 --> 00:05:54,560
metadata of the resource

00:05:53,199 --> 00:05:56,560
with that i'd like to turn it over to

00:05:54,560 --> 00:05:59,600
john to discuss the tooling we've

00:05:56,560 --> 00:06:02,000
selected to enforce these policies

00:05:59,600 --> 00:06:03,919
thanks james so as james mentioned we

00:06:02,000 --> 00:06:05,680
knew we wanted to use policy to solve a

00:06:03,919 --> 00:06:06,639
lot of the problems we were having at

00:06:05,680 --> 00:06:08,479
yubico

00:06:06,639 --> 00:06:10,639
we looked at a lot of the tools out

00:06:08,479 --> 00:06:13,280
there that solved this problem

00:06:10,639 --> 00:06:15,440
but opa was the clear winner in this

00:06:13,280 --> 00:06:16,400
space we saw a lot of adoption with

00:06:15,440 --> 00:06:18,400
other tools

00:06:16,400 --> 00:06:19,600
that we knew we wanted to leverage and

00:06:18,400 --> 00:06:22,400
it came with its own

00:06:19,600 --> 00:06:23,360
policy language but before getting into

00:06:22,400 --> 00:06:25,199
opa itself

00:06:23,360 --> 00:06:26,400
it's really important to understand what

00:06:25,199 --> 00:06:29,600
a policy is

00:06:26,400 --> 00:06:32,400
what it looks like and rego itself so

00:06:29,600 --> 00:06:33,360
rego is the policy language that opa

00:06:32,400 --> 00:06:36,720
knows

00:06:33,360 --> 00:06:38,720
that understands and on my screen here

00:06:36,720 --> 00:06:41,600
you can see a policy

00:06:38,720 --> 00:06:42,880
for a kubernetes manifest that says that

00:06:41,600 --> 00:06:45,520
it must have

00:06:42,880 --> 00:06:46,400
a owner label on it specifically

00:06:45,520 --> 00:06:49,120
namespaces

00:06:46,400 --> 00:06:50,319
must have an owner label so when a when

00:06:49,120 --> 00:06:53,039
a request comes in

00:06:50,319 --> 00:06:55,199
in the cluster for a namespace creation

00:06:53,039 --> 00:06:57,199
this policy will first check to see

00:06:55,199 --> 00:06:59,919
this input that's coming in this input

00:06:57,199 --> 00:07:01,919
document is it of type namespace

00:06:59,919 --> 00:07:03,919
and if it doesn't have an owner's label

00:07:01,919 --> 00:07:05,680
on it return a message that says

00:07:03,919 --> 00:07:08,319
namespaces must have an owner so the

00:07:05,680 --> 00:07:10,319
user the uh the individual trying to

00:07:08,319 --> 00:07:10,880
deploy this namespace knows how to fix

00:07:10,319 --> 00:07:14,000
it

00:07:10,880 --> 00:07:17,039
and so the important takeaway here

00:07:14,000 --> 00:07:20,080
is the input keyword the input keyword

00:07:17,039 --> 00:07:22,080
denotes a input document for rego and

00:07:20,080 --> 00:07:24,720
the input document is just the

00:07:22,080 --> 00:07:25,280
structured data in our case it's a yaml

00:07:24,720 --> 00:07:28,240
file

00:07:25,280 --> 00:07:30,000
so anything beyond the input dot should

00:07:28,240 --> 00:07:33,440
look really familiar we see kind we

00:07:30,000 --> 00:07:35,280
see metadata but the the input dot

00:07:33,440 --> 00:07:36,960
and anything after that is just

00:07:35,280 --> 00:07:39,360
dependent upon the data

00:07:36,960 --> 00:07:40,800
that you give it it could be terraform

00:07:39,360 --> 00:07:44,080
it could be a docker file

00:07:40,800 --> 00:07:47,120
any sort of of structured data so

00:07:44,080 --> 00:07:50,000
again very generic language um

00:07:47,120 --> 00:07:51,120
immensely immensely powerful and so now

00:07:50,000 --> 00:07:54,000
that we have this

00:07:51,120 --> 00:07:54,879
this rego file we need a way to actually

00:07:54,000 --> 00:07:57,360
determine

00:07:54,879 --> 00:07:59,199
if the document that we give it would be

00:07:57,360 --> 00:08:00,800
would be in violation and there's a few

00:07:59,199 --> 00:08:03,199
ways to do this right we could

00:08:00,800 --> 00:08:04,080
we could be the input document the the

00:08:03,199 --> 00:08:06,639
rego

00:08:04,080 --> 00:08:08,240
to james and he can verify on a

00:08:06,639 --> 00:08:09,120
case-by-case basis whether or not the

00:08:08,240 --> 00:08:11,599
document

00:08:09,120 --> 00:08:12,800
is uh is violated or not but we're we're

00:08:11,599 --> 00:08:15,120
all about automation so

00:08:12,800 --> 00:08:16,560
as briefly talked about before we

00:08:15,120 --> 00:08:18,000
decided to go with uh with the open

00:08:16,560 --> 00:08:19,840
policy agent and just like

00:08:18,000 --> 00:08:21,360
just look at the logo of of course we

00:08:19,840 --> 00:08:23,039
did um there's

00:08:21,360 --> 00:08:25,280
there's just there's no reason not to

00:08:23,039 --> 00:08:28,720
choose opa it's a work

00:08:25,280 --> 00:08:30,080
of art but no really we um

00:08:28,720 --> 00:08:32,080
the community is great again there's

00:08:30,080 --> 00:08:34,560
there's so much adoption around

00:08:32,080 --> 00:08:36,719
opa it was um it was it's been it's been

00:08:34,560 --> 00:08:39,440
a real joy to leverage

00:08:36,719 --> 00:08:40,320
opa um working in their their slack

00:08:39,440 --> 00:08:42,719
channel

00:08:40,320 --> 00:08:44,320
everyone's super friendly there's always

00:08:42,719 --> 00:08:45,839
someone there to help you out

00:08:44,320 --> 00:08:47,680
it was it was a really good a really

00:08:45,839 --> 00:08:51,200
really good choice for us

00:08:47,680 --> 00:08:52,560
and so how this how this works how opa

00:08:51,200 --> 00:08:54,959
works as a service

00:08:52,560 --> 00:08:56,160
you uh you deploy it somewhere be it

00:08:54,959 --> 00:08:58,080
kubernetes

00:08:56,160 --> 00:08:59,600
be it a web server wherever you want to

00:08:58,080 --> 00:09:01,040
put it as long as you can get a web

00:08:59,600 --> 00:09:03,680
request to it

00:09:01,040 --> 00:09:05,360
um and then you also include your your

00:09:03,680 --> 00:09:07,040
regular files with that with that

00:09:05,360 --> 00:09:08,080
deployment so opa can know which

00:09:07,040 --> 00:09:10,880
policies

00:09:08,080 --> 00:09:13,040
you want it to enforce and so you you

00:09:10,880 --> 00:09:14,240
submit an input document again be it a

00:09:13,040 --> 00:09:16,880
kubernetes manifest

00:09:14,240 --> 00:09:18,480
be it a docker file anything you want

00:09:16,880 --> 00:09:21,200
give it to the opa service

00:09:18,480 --> 00:09:22,560
it will validate the the document run

00:09:21,200 --> 00:09:23,120
through run it through the policies and

00:09:22,560 --> 00:09:25,600
tell you

00:09:23,120 --> 00:09:26,560
does this document violate any of the

00:09:25,600 --> 00:09:29,279
policies you have

00:09:26,560 --> 00:09:30,000
loaded into it and there's also the the

00:09:29,279 --> 00:09:32,080
really

00:09:30,000 --> 00:09:33,040
nice piece of the fact that it can also

00:09:32,080 --> 00:09:35,120
take external

00:09:33,040 --> 00:09:36,320
data so you can see here in the the

00:09:35,120 --> 00:09:39,360
bottom right the

00:09:36,320 --> 00:09:40,480
the data document that's external data

00:09:39,360 --> 00:09:42,399
and that can come from

00:09:40,480 --> 00:09:44,399
any number of sources so if we build on

00:09:42,399 --> 00:09:47,440
the previous example of

00:09:44,399 --> 00:09:50,000
the policy where that teams must have a

00:09:47,440 --> 00:09:51,600
namespace label all name spaces must

00:09:50,000 --> 00:09:53,120
have a label of owner on it

00:09:51,600 --> 00:09:54,880
we could also add a policy that says

00:09:53,120 --> 00:09:57,680
that owners

00:09:54,880 --> 00:09:58,080
teams can only own a single namespace

00:09:57,680 --> 00:10:00,560
and so

00:09:58,080 --> 00:10:03,120
in order to do that we would need some

00:10:00,560 --> 00:10:05,279
form of external data in this case a

00:10:03,120 --> 00:10:06,160
count of how many namespaces that

00:10:05,279 --> 00:10:09,440
they've already

00:10:06,160 --> 00:10:12,079
created and so when we create namespaces

00:10:09,440 --> 00:10:14,000
when we delete namespaces we can keep

00:10:12,079 --> 00:10:15,760
track of that number give it to opa and

00:10:14,000 --> 00:10:18,880
then opa can use that when it's

00:10:15,760 --> 00:10:20,720
when it's evaluating all of its policies

00:10:18,880 --> 00:10:22,399
and so while that was an example of

00:10:20,720 --> 00:10:24,720
using op as a service

00:10:22,399 --> 00:10:26,320
you can actually use opa as a library

00:10:24,720 --> 00:10:29,200
which makes it

00:10:26,320 --> 00:10:31,040
so much easier to to enforce policy

00:10:29,200 --> 00:10:33,920
there's a lot of tools out there

00:10:31,040 --> 00:10:34,399
that will actually take the opa engine

00:10:33,920 --> 00:10:37,040
um

00:10:34,399 --> 00:10:38,399
import it as a dependency and then run

00:10:37,040 --> 00:10:41,680
the the same checks

00:10:38,399 --> 00:10:44,640
that uh that opa itself would

00:10:41,680 --> 00:10:46,320
and so we really wanted to leverage this

00:10:44,640 --> 00:10:49,200
um this type of functionality

00:10:46,320 --> 00:10:51,200
in order to shift our policy enforcement

00:10:49,200 --> 00:10:51,839
to the left because we quickly realized

00:10:51,200 --> 00:10:55,360
that

00:10:51,839 --> 00:10:55,839
when we deployed opa while we were able

00:10:55,360 --> 00:10:57,600
to

00:10:55,839 --> 00:10:58,959
have this policy enforcement this policy

00:10:57,600 --> 00:11:01,120
validation

00:10:58,959 --> 00:11:02,240
our engineers didn't really know whether

00:11:01,120 --> 00:11:05,120
or not the

00:11:02,240 --> 00:11:07,040
code they were writing the policies and

00:11:05,120 --> 00:11:09,120
uh manifest they were writing

00:11:07,040 --> 00:11:10,640
would be in violation until they

00:11:09,120 --> 00:11:12,560
actually deployed it

00:11:10,640 --> 00:11:14,560
so we really wanted a solution that we

00:11:12,560 --> 00:11:16,240
could give them

00:11:14,560 --> 00:11:18,320
put on their local machine and they

00:11:16,240 --> 00:11:19,920
could just run it without having to

00:11:18,320 --> 00:11:21,519
worry about any of that and that silly

00:11:19,920 --> 00:11:23,920
networking stuff

00:11:21,519 --> 00:11:25,600
and so we found a tool called called

00:11:23,920 --> 00:11:29,360
comptest

00:11:25,600 --> 00:11:32,320
and comtess more or less lets you

00:11:29,360 --> 00:11:32,800
run opa on on on your local machine like

00:11:32,320 --> 00:11:35,279
opa

00:11:32,800 --> 00:11:36,160
you can but contest just makes the

00:11:35,279 --> 00:11:38,720
experience

00:11:36,160 --> 00:11:39,920
so much better so like when i said

00:11:38,720 --> 00:11:42,800
before

00:11:39,920 --> 00:11:43,839
you can give any input document to opa

00:11:42,800 --> 00:11:45,920
it doesn't really care

00:11:43,839 --> 00:11:46,880
it's like it's half true it still

00:11:45,920 --> 00:11:49,920
doesn't care but

00:11:46,880 --> 00:11:51,519
as long as it's yaml or json so

00:11:49,920 --> 00:11:53,440
com tests can actually take a lot of

00:11:51,519 --> 00:11:56,560
different file formats you can see here

00:11:53,440 --> 00:11:57,600
ini tomal hcl and it can convert those

00:11:56,560 --> 00:12:00,800
file formats

00:11:57,600 --> 00:12:02,639
into into json and then shepard into opa

00:12:00,800 --> 00:12:05,839
so you can actually use

00:12:02,639 --> 00:12:07,040
any format with with opa there's also

00:12:05,839 --> 00:12:09,360
some nicety about

00:12:07,040 --> 00:12:10,160
being able to take in files from from

00:12:09,360 --> 00:12:12,480
different folders

00:12:10,160 --> 00:12:15,040
anywhere on your on your machine and

00:12:12,480 --> 00:12:18,240
then print the result again in a user

00:12:15,040 --> 00:12:21,920
friendly way so com test is for

00:12:18,240 --> 00:12:24,720
local policy validation it's for

00:12:21,920 --> 00:12:25,760
pipeline validation in all of your your

00:12:24,720 --> 00:12:28,720
manifest

00:12:25,760 --> 00:12:30,560
and then anything beyond that would be

00:12:28,720 --> 00:12:32,959
like opa deployed as a service for

00:12:30,560 --> 00:12:34,480
continue enforcement

00:12:32,959 --> 00:12:37,040
and so you can see here this is a

00:12:34,480 --> 00:12:39,040
another example of a policy file on the

00:12:37,040 --> 00:12:40,560
left this time we're looking at

00:12:39,040 --> 00:12:41,839
deployments to make sure they're not

00:12:40,560 --> 00:12:45,040
running as root and that

00:12:41,839 --> 00:12:46,000
the container has app label for pawn

00:12:45,040 --> 00:12:47,760
selectors

00:12:46,000 --> 00:12:50,639
and on the right we have our input

00:12:47,760 --> 00:12:52,720
document our deploy.yaml

00:12:50,639 --> 00:12:54,880
itself and so if we actually run this

00:12:52,720 --> 00:12:56,480
through com test using the test command

00:12:54,880 --> 00:12:59,920
all you have to do is pass in the

00:12:56,480 --> 00:13:02,560
policy.rego as well as the deploy.yaml

00:12:59,920 --> 00:13:03,440
and then it will take that deploy.yml

00:13:02,560 --> 00:13:06,160
and see

00:13:03,440 --> 00:13:08,079
does that input document violate any of

00:13:06,160 --> 00:13:09,360
the policies that we have set forth

00:13:08,079 --> 00:13:11,920
in this case there are there are two

00:13:09,360 --> 00:13:14,000
policies that were that were violated

00:13:11,920 --> 00:13:15,040
and we can take this policy and we can

00:13:14,000 --> 00:13:17,279
apply it to

00:13:15,040 --> 00:13:19,040
the op as a service and get the exact

00:13:17,279 --> 00:13:21,680
same behavior so we're checking

00:13:19,040 --> 00:13:22,320
both the the local environment as well

00:13:21,680 --> 00:13:24,720
as the

00:13:22,320 --> 00:13:26,399
um as well as the production environment

00:13:24,720 --> 00:13:28,639
and then again this it can be run

00:13:26,399 --> 00:13:31,360
local machines pipelines and you can get

00:13:28,639 --> 00:13:33,120
that immediate feedback

00:13:31,360 --> 00:13:34,639
there's also the benefit with comtess to

00:13:33,120 --> 00:13:36,959
be able to

00:13:34,639 --> 00:13:37,920
share policies across the organization

00:13:36,959 --> 00:13:40,240
because

00:13:37,920 --> 00:13:42,160
it's a fair use case a valid use case

00:13:40,240 --> 00:13:43,360
that your policies are managed and

00:13:42,160 --> 00:13:45,600
written by a completely

00:13:43,360 --> 00:13:47,279
separate team a security team much like

00:13:45,600 --> 00:13:49,120
they are done at yubico

00:13:47,279 --> 00:13:51,839
we'll have a lot of different

00:13:49,120 --> 00:13:55,040
repositories we want to make sure that

00:13:51,839 --> 00:13:56,320
the our policies are being enforced and

00:13:55,040 --> 00:13:58,480
there's really no way to get those

00:13:56,320 --> 00:14:00,560
policies without

00:13:58,480 --> 00:14:02,079
trying them out in the cluster but with

00:14:00,560 --> 00:14:05,279
comptest you can actually

00:14:02,079 --> 00:14:06,959
push policies policy bundles to a oci

00:14:05,279 --> 00:14:09,199
compliant registry

00:14:06,959 --> 00:14:10,639
and then pull them down for for later

00:14:09,199 --> 00:14:11,120
use so you can see here in the example

00:14:10,639 --> 00:14:13,839
where

00:14:11,120 --> 00:14:14,959
we're pulling down a bundle of cluster

00:14:13,839 --> 00:14:17,360
policies

00:14:14,959 --> 00:14:19,519
and then we're actually running the

00:14:17,360 --> 00:14:21,839
comptest test command locally

00:14:19,519 --> 00:14:23,680
on that bundle that we just pulled on

00:14:21,839 --> 00:14:24,959
that same deployed.yaml and then the

00:14:23,680 --> 00:14:27,440
result is the same

00:14:24,959 --> 00:14:29,279
so this is a way to be able to push

00:14:27,440 --> 00:14:29,680
policies out there and then pull them

00:14:29,279 --> 00:14:32,160
down

00:14:29,680 --> 00:14:33,519
for for cross-team usage this is really

00:14:32,160 --> 00:14:35,519
huge as well in um

00:14:33,519 --> 00:14:36,800
in pipelines or other kind of approaches

00:14:35,519 --> 00:14:38,560
where you need to

00:14:36,800 --> 00:14:40,240
bundle together a lot of different

00:14:38,560 --> 00:14:43,680
policies

00:14:40,240 --> 00:14:46,720
and so artifact hub is

00:14:43,680 --> 00:14:49,120
an attempt to be able to

00:14:46,720 --> 00:14:50,160
expose a lot of different policy bundles

00:14:49,120 --> 00:14:52,000
because

00:14:50,160 --> 00:14:54,320
more or less like the policies that we

00:14:52,000 --> 00:14:57,279
really care about things like

00:14:54,320 --> 00:14:58,000
containers having resource constraints

00:14:57,279 --> 00:15:00,560
containers not

00:14:58,000 --> 00:15:01,680
running as root those types of policies

00:15:00,560 --> 00:15:03,680
should more or less be

00:15:01,680 --> 00:15:06,240
the same but we're kind of in a state

00:15:03,680 --> 00:15:07,920
now where teams companies organizations

00:15:06,240 --> 00:15:10,320
are all writing the same policies we

00:15:07,920 --> 00:15:13,440
don't have a good distribution mechanism

00:15:10,320 --> 00:15:14,160
and artifact hub is again the the the

00:15:13,440 --> 00:15:15,680
solution

00:15:14,160 --> 00:15:18,079
for that um it's currently it's a

00:15:15,680 --> 00:15:19,920
sandbox project there are a whole lot of

00:15:18,079 --> 00:15:21,120
policy bundles out there i think there's

00:15:19,920 --> 00:15:23,199
like one or two

00:15:21,120 --> 00:15:25,519
but i would definitely keep an eye on

00:15:23,199 --> 00:15:27,519
this contribute where you can

00:15:25,519 --> 00:15:29,199
if you do have a public bundle that you

00:15:27,519 --> 00:15:31,279
want to to contribute i

00:15:29,199 --> 00:15:33,360
recommend pushing it up there and let's

00:15:31,279 --> 00:15:35,199
try to make this successful though so we

00:15:33,360 --> 00:15:37,040
can actually start sharing policies with

00:15:35,199 --> 00:15:38,720
one another not having to uh

00:15:37,040 --> 00:15:40,320
to rewrite them over and over and over

00:15:38,720 --> 00:15:42,079
again for every team who wants to be

00:15:40,320 --> 00:15:44,320
able to use policy

00:15:42,079 --> 00:15:45,759
and so a lot of the talks of we've a lot

00:15:44,320 --> 00:15:47,920
of the conversation we've had

00:15:45,759 --> 00:15:49,199
is all about policy enforcement on the

00:15:47,920 --> 00:15:51,600
local environments

00:15:49,199 --> 00:15:53,360
but we also need to make sure that the

00:15:51,600 --> 00:15:56,560
policies are enforced in

00:15:53,360 --> 00:15:59,040
in production and so once you get beyond

00:15:56,560 --> 00:15:59,680
the local environment we decided to go

00:15:59,040 --> 00:16:02,959
with a

00:15:59,680 --> 00:16:05,519
tool called called gatekeeper and

00:16:02,959 --> 00:16:08,320
gatekeeper lets us enforce these

00:16:05,519 --> 00:16:09,279
policies continuously inside inside of a

00:16:08,320 --> 00:16:13,519
kubernetes

00:16:09,279 --> 00:16:15,759
cluster it's a mission controller

00:16:13,519 --> 00:16:16,639
so when you deploy a resource to

00:16:15,759 --> 00:16:17,600
kubernetes

00:16:16,639 --> 00:16:19,839
it'll go through the admission

00:16:17,600 --> 00:16:20,560
controller look at all the policy that

00:16:19,839 --> 00:16:23,680
you have

00:16:20,560 --> 00:16:24,160
loaded and if any of the the resources

00:16:23,680 --> 00:16:26,240
that you

00:16:24,160 --> 00:16:27,600
are attempting to add to the cluster it

00:16:26,240 --> 00:16:30,000
will reject

00:16:27,600 --> 00:16:32,480
or or let the policy go through it also

00:16:30,000 --> 00:16:34,800
has an and some audit functionality

00:16:32,480 --> 00:16:35,680
saying it'll continually audit your

00:16:34,800 --> 00:16:37,680
cluster

00:16:35,680 --> 00:16:39,040
are there any resources that that are

00:16:37,680 --> 00:16:41,360
violating policy if

00:16:39,040 --> 00:16:43,040
in the case that you're just starting to

00:16:41,360 --> 00:16:44,560
adopt gatekeeper

00:16:43,040 --> 00:16:46,560
and you want to see if i were to

00:16:44,560 --> 00:16:47,759
actually enforce this policy how many

00:16:46,560 --> 00:16:49,680
resources

00:16:47,759 --> 00:16:51,600
are kind of out of band or just in the

00:16:49,680 --> 00:16:53,120
case that gatekeeper went down for a

00:16:51,600 --> 00:16:55,600
little bit you know want to

00:16:53,120 --> 00:16:57,360
make sure that there wasn't a a resource

00:16:55,600 --> 00:16:58,079
that got in like during that that small

00:16:57,360 --> 00:16:59,680
window

00:16:58,079 --> 00:17:01,759
and again the huge one here is it's

00:16:59,680 --> 00:17:03,600
using the same policies so

00:17:01,759 --> 00:17:05,439
we're if you're checking on your local

00:17:03,600 --> 00:17:07,360
versus in production

00:17:05,439 --> 00:17:08,880
we can always continuously use the same

00:17:07,360 --> 00:17:10,240
policy that doesn't change from

00:17:08,880 --> 00:17:14,160
environment to environment

00:17:10,240 --> 00:17:17,839
but with that said um it's

00:17:14,160 --> 00:17:18,480
almost the same policies so uh we go

00:17:17,839 --> 00:17:21,039
back to

00:17:18,480 --> 00:17:22,640
the input document um when you're

00:17:21,039 --> 00:17:23,199
working with yaml the input document is

00:17:22,640 --> 00:17:25,760
going to be

00:17:23,199 --> 00:17:27,240
a yaml file so we would expect

00:17:25,760 --> 00:17:29,760
input.kind

00:17:27,240 --> 00:17:32,400
input.metadata just like we saw before

00:17:29,760 --> 00:17:34,480
but when we're in the context of

00:17:32,400 --> 00:17:36,600
admission control it's going to be a

00:17:34,480 --> 00:17:39,039
little different i bolded here

00:17:36,600 --> 00:17:39,360
input.review.object because this is what

00:17:39,039 --> 00:17:41,120
a

00:17:39,360 --> 00:17:43,200
admission review is going to look like

00:17:41,120 --> 00:17:45,600
to to gatekeeper

00:17:43,200 --> 00:17:46,480
this is the the document that is going

00:17:45,600 --> 00:17:49,520
to be received

00:17:46,480 --> 00:17:51,679
from opa so again it's not

00:17:49,520 --> 00:17:53,360
quite the same you would actually have

00:17:51,679 --> 00:17:56,080
to write a policy

00:17:53,360 --> 00:17:57,760
that had input.review.object in it but

00:17:56,080 --> 00:17:59,919
that would only work for gatekeeper

00:17:57,760 --> 00:18:01,919
and if you did just input.kind that

00:17:59,919 --> 00:18:04,559
wouldn't work for gatekeeper

00:18:01,919 --> 00:18:06,480
so what we ended up doing was really

00:18:04,559 --> 00:18:08,400
adopting this idea of rego being the

00:18:06,480 --> 00:18:11,440
source of truth your policies because

00:18:08,400 --> 00:18:14,240
it really it really is rego is

00:18:11,440 --> 00:18:16,400
generic it's it doesn't matter the uh

00:18:14,240 --> 00:18:19,200
the context that you're in

00:18:16,400 --> 00:18:20,559
it's really all about does this input

00:18:19,200 --> 00:18:23,280
document that you're giving me

00:18:20,559 --> 00:18:24,960
violate this policy that you have

00:18:23,280 --> 00:18:27,280
defined and so in every

00:18:24,960 --> 00:18:28,720
everything that's that you do that's

00:18:27,280 --> 00:18:30,960
based off of your rego

00:18:28,720 --> 00:18:33,520
should should adjust the changes of your

00:18:30,960 --> 00:18:35,760
regula and not the other way around

00:18:33,520 --> 00:18:37,919
and so to solve this problem we actually

00:18:35,760 --> 00:18:40,160
wrote a tool called constraint

00:18:37,919 --> 00:18:41,360
and constraint brings to the table three

00:18:40,160 --> 00:18:43,919
really important

00:18:41,360 --> 00:18:45,679
factors first and foremost it it

00:18:43,919 --> 00:18:47,280
actually does provide a library

00:18:45,679 --> 00:18:49,520
where you can write policies that work

00:18:47,280 --> 00:18:52,160
with both comptest and gatekeeper

00:18:49,520 --> 00:18:53,440
it's really just a a wrapper that

00:18:52,160 --> 00:18:56,799
normalizes

00:18:53,440 --> 00:18:59,120
a a polyfill that that will say

00:18:56,799 --> 00:18:59,919
if you're in the context of gatekeeper

00:18:59,120 --> 00:19:01,559
then

00:18:59,919 --> 00:19:03,039
spit out your policies for

00:19:01,559 --> 00:19:04,720
input.review.object if you're in the

00:19:03,039 --> 00:19:06,880
context of conf test

00:19:04,720 --> 00:19:07,919
it's just input dot and that handles all

00:19:06,880 --> 00:19:10,799
of that for you

00:19:07,919 --> 00:19:11,679
so your policies can be completely

00:19:10,799 --> 00:19:13,200
unchanged

00:19:11,679 --> 00:19:15,760
no matter what environment that you're

00:19:13,200 --> 00:19:17,919
running in the other benefit is the

00:19:15,760 --> 00:19:18,880
template and constraint creation and

00:19:17,919 --> 00:19:20,240
management

00:19:18,880 --> 00:19:22,320
so when you saw on the previous

00:19:20,240 --> 00:19:25,280
constraint uh previous screen

00:19:22,320 --> 00:19:27,840
the the rego there was actually embedded

00:19:25,280 --> 00:19:29,840
into the constraint template the yaml

00:19:27,840 --> 00:19:32,160
and that's because that's how gatekeeper

00:19:29,840 --> 00:19:34,400
is able to um

00:19:32,160 --> 00:19:36,160
to uh to load in your policies it's just

00:19:34,400 --> 00:19:38,799
done through a yaml file

00:19:36,160 --> 00:19:39,919
and so you're gonna have a rego file on

00:19:38,799 --> 00:19:42,799
disk

00:19:39,919 --> 00:19:44,080
and if you were to change that rego file

00:19:42,799 --> 00:19:45,919
you would also have to

00:19:44,080 --> 00:19:47,679
copy and paste your changes into that

00:19:45,919 --> 00:19:50,240
yaml which

00:19:47,679 --> 00:19:52,400
isn't the most ideal situation and so

00:19:50,240 --> 00:19:54,720
what constraint will actually do is

00:19:52,400 --> 00:19:56,480
it will look at all of your radio files

00:19:54,720 --> 00:19:59,120
and then it will actually

00:19:56,480 --> 00:20:00,000
generate the template and constraint for

00:19:59,120 --> 00:20:02,400
you so

00:20:00,000 --> 00:20:03,760
you never have to touch yaml you're only

00:20:02,400 --> 00:20:06,400
focused purely on

00:20:03,760 --> 00:20:08,240
on your rego file and then lastly it

00:20:06,400 --> 00:20:09,840
actually will generate documentation

00:20:08,240 --> 00:20:12,080
for your policies we really wanted to

00:20:09,840 --> 00:20:14,720
give our engineers the ability to see

00:20:12,080 --> 00:20:15,280
what policies were being enforced as

00:20:14,720 --> 00:20:17,520
well as

00:20:15,280 --> 00:20:20,720
how they can how they can resolve them

00:20:17,520 --> 00:20:22,880
if they ever run into a policy violation

00:20:20,720 --> 00:20:23,919
and so when it comes to the policies

00:20:22,880 --> 00:20:26,080
themselves

00:20:23,919 --> 00:20:27,840
it'll look relatively familiar that the

00:20:26,080 --> 00:20:29,679
biggest difference here is definitely

00:20:27,840 --> 00:20:32,400
the the comment header

00:20:29,679 --> 00:20:34,000
we added some some metadata to to the

00:20:32,400 --> 00:20:37,280
policy in form of a

00:20:34,000 --> 00:20:41,039
header comment where we say title

00:20:37,280 --> 00:20:42,720
is the the title of of the the policy

00:20:41,039 --> 00:20:44,080
here we're saying images must not use

00:20:42,720 --> 00:20:46,880
the latest tag

00:20:44,080 --> 00:20:48,000
and then why this policy exists or

00:20:46,880 --> 00:20:50,400
really any other flavor

00:20:48,000 --> 00:20:52,400
that you want to give it to give this

00:20:50,400 --> 00:20:53,120
policy be the description or anything

00:20:52,400 --> 00:20:55,200
else

00:20:53,120 --> 00:20:56,240
the the enforcement type here is saying

00:20:55,200 --> 00:20:59,120
deny

00:20:56,240 --> 00:20:59,679
the alternative is dry run in case you

00:20:59,120 --> 00:21:01,520
want to

00:20:59,679 --> 00:21:03,840
test out this policy in your cluster and

00:21:01,520 --> 00:21:06,080
not actually do any sort of

00:21:03,840 --> 00:21:08,960
enforcement and then which kinds which

00:21:06,080 --> 00:21:11,919
kubernetes resources that this policy

00:21:08,960 --> 00:21:14,559
will be enforced on be it just ingresses

00:21:11,919 --> 00:21:16,720
be it just namespaces workloads etc

00:21:14,559 --> 00:21:17,760
so we define here a list of resources

00:21:16,720 --> 00:21:19,280
that this policy

00:21:17,760 --> 00:21:21,760
isn't forced on and then we use this

00:21:19,280 --> 00:21:22,720
metadata to generate all the other yamls

00:21:21,760 --> 00:21:25,039
that we were previously

00:21:22,720 --> 00:21:27,120
talking about and in the violation

00:21:25,039 --> 00:21:28,840
itself you see here we're importing two

00:21:27,120 --> 00:21:30,159
libraries which is the constraint

00:21:28,840 --> 00:21:33,159
library

00:21:30,159 --> 00:21:34,640
the the important note here is that

00:21:33,159 --> 00:21:37,120
pods.container

00:21:34,640 --> 00:21:38,320
our pods library will actually look at

00:21:37,120 --> 00:21:41,200
containers from

00:21:38,320 --> 00:21:42,880
any possible source because pods can

00:21:41,200 --> 00:21:44,960
come from cron jobs they can come from

00:21:42,880 --> 00:21:47,120
deployment staple sets daemon sets

00:21:44,960 --> 00:21:48,799
there's a large list of those and

00:21:47,120 --> 00:21:50,640
they're all embedded in kubernetes

00:21:48,799 --> 00:21:53,679
resources differently

00:21:50,640 --> 00:21:54,559
so pods handles that for you and then we

00:21:53,679 --> 00:21:56,240
can take

00:21:54,559 --> 00:21:58,400
the the resulting container that comes

00:21:56,240 --> 00:22:01,120
out of that does that container have an

00:21:58,400 --> 00:22:02,960
image of latest and if it does

00:22:01,120 --> 00:22:04,480
give a note to the the user saying that

00:22:02,960 --> 00:22:07,039
images must not use the

00:22:04,480 --> 00:22:08,880
the latest tag and again we have two

00:22:07,039 --> 00:22:10,799
commands for that one to generate the

00:22:08,880 --> 00:22:12,880
the templates and the constraints and

00:22:10,799 --> 00:22:17,360
then one for for the markdown

00:22:12,880 --> 00:22:19,919
describing describing our our policies

00:22:17,360 --> 00:22:20,880
and then this is an example of what the

00:22:19,919 --> 00:22:24,840
documentation

00:22:20,880 --> 00:22:28,080
looks like you can see here the the id

00:22:24,840 --> 00:22:29,520
p1001 we also embedded something into

00:22:28,080 --> 00:22:31,120
into the tool that you can actually

00:22:29,520 --> 00:22:33,440
assign an id

00:22:31,120 --> 00:22:35,360
to a policy so you can refer back to it

00:22:33,440 --> 00:22:37,840
instead of just using a title

00:22:35,360 --> 00:22:39,840
the the severity is coming from the the

00:22:37,840 --> 00:22:40,640
severity of the rule because in rego you

00:22:39,840 --> 00:22:42,159
can have

00:22:40,640 --> 00:22:45,200
warning you can have deny you can have

00:22:42,159 --> 00:22:46,799
violation and based on that rule

00:22:45,200 --> 00:22:48,640
different things will happen so we pull

00:22:46,799 --> 00:22:50,400
the severity out of the rego and put

00:22:48,640 --> 00:22:52,960
that in the document for you

00:22:50,400 --> 00:22:54,240
same with the resources that it impacts

00:22:52,960 --> 00:22:56,799
the

00:22:54,240 --> 00:22:59,280
the description that dictates what this

00:22:56,799 --> 00:23:01,679
policy does or why we have it

00:22:59,280 --> 00:23:02,960
and then this is the the rego itself if

00:23:01,679 --> 00:23:05,360
you want that information

00:23:02,960 --> 00:23:06,799
again completely completely configurable

00:23:05,360 --> 00:23:08,559
if you change your rego

00:23:06,799 --> 00:23:09,840
again all the documentation is

00:23:08,559 --> 00:23:12,080
regenerated

00:23:09,840 --> 00:23:12,960
none of this is hand typed and then you

00:23:12,080 --> 00:23:14,880
also

00:23:12,960 --> 00:23:17,360
are given a link to to the source and

00:23:14,880 --> 00:23:19,600
that source can either be a relative url

00:23:17,360 --> 00:23:21,600
if the policy lives in the same location

00:23:19,600 --> 00:23:23,919
of the source the same repository

00:23:21,600 --> 00:23:25,120
or if it's a remote repository the

00:23:23,919 --> 00:23:28,960
source can also be

00:23:25,120 --> 00:23:32,159
a uh be a url so we um

00:23:28,960 --> 00:23:35,120
we found a lot of usage out of this tool

00:23:32,159 --> 00:23:37,200
we use it for um like almost every day

00:23:35,120 --> 00:23:39,360
it's really ingrained into

00:23:37,200 --> 00:23:41,120
into our workflows when it comes to

00:23:39,360 --> 00:23:42,960
gatekeeper and policy so

00:23:41,120 --> 00:23:44,799
if you're interested if it sounds

00:23:42,960 --> 00:23:48,799
something like you'd want to use

00:23:44,799 --> 00:23:52,000
always open to uh to talk about it

00:23:48,799 --> 00:23:53,760
contributions always welcome and uh

00:23:52,000 --> 00:23:55,039
that's really all i have for for the

00:23:53,760 --> 00:23:56,880
tooling piece of it

00:23:55,039 --> 00:23:58,320
uh james is going to kind of go into

00:23:56,880 --> 00:24:01,120
into deeper detail

00:23:58,320 --> 00:24:02,720
of how we actually have leveraged these

00:24:01,120 --> 00:24:06,240
tools in our policies

00:24:02,720 --> 00:24:06,240
and in our pipelines and processes

00:24:07,440 --> 00:24:10,159
thanks john now that we have an

00:24:08,880 --> 00:24:10,720
understanding of how the tooling works

00:24:10,159 --> 00:24:14,159
together

00:24:10,720 --> 00:24:15,520
let's dive into ubiko's policy journey

00:24:14,159 --> 00:24:17,600
in the beginning since we didn't have

00:24:15,520 --> 00:24:19,520
too many workloads on kubernetes yet

00:24:17,600 --> 00:24:20,960
and we were sure that we had consistent

00:24:19,520 --> 00:24:22,880
peer review happening

00:24:20,960 --> 00:24:24,720
we started with a simple plan because we

00:24:22,880 --> 00:24:27,360
didn't anticipate too many resources

00:24:24,720 --> 00:24:28,960
would violate these policies

00:24:27,360 --> 00:24:30,960
we would start with writing the policies

00:24:28,960 --> 00:24:32,799
and their tests after which we would

00:24:30,960 --> 00:24:34,720
engage with our services teams to add

00:24:32,799 --> 00:24:36,000
these checks to their ci flows using

00:24:34,720 --> 00:24:38,159
conf test

00:24:36,000 --> 00:24:39,919
simultaneous to that we would be working

00:24:38,159 --> 00:24:41,679
on deploying gatekeeper to our clusters

00:24:39,919 --> 00:24:43,360
in audit only mode

00:24:41,679 --> 00:24:45,120
and then finally after we had worked

00:24:43,360 --> 00:24:46,480
with the teams to remediate all of the

00:24:45,120 --> 00:24:48,000
identified issues

00:24:46,480 --> 00:24:51,440
we would flip the switch and move

00:24:48,000 --> 00:24:52,880
gatekeeper into enforcement mode

00:24:51,440 --> 00:24:55,120
as you might have expected from how i

00:24:52,880 --> 00:24:56,400
framed the previous slide this plan ran

00:24:55,120 --> 00:24:58,080
into some issues

00:24:56,400 --> 00:25:00,159
we made it as far as writing and testing

00:24:58,080 --> 00:25:00,799
the policies but when we moved to engage

00:25:00,159 --> 00:25:02,720
with

00:25:00,799 --> 00:25:04,000
some select teams to add conf tests to

00:25:02,720 --> 00:25:05,679
their ci flows

00:25:04,000 --> 00:25:08,080
the issues became pretty readily

00:25:05,679 --> 00:25:08,080
apparent

00:25:08,559 --> 00:25:11,679
well for one there were more violations

00:25:10,640 --> 00:25:13,760
than anticipated

00:25:11,679 --> 00:25:15,679
which meant that there was a potentially

00:25:13,760 --> 00:25:17,919
a very long window between when we

00:25:15,679 --> 00:25:19,440
identified these resources and when we

00:25:17,919 --> 00:25:21,120
would actually be able to

00:25:19,440 --> 00:25:23,360
apply remediation across all of our

00:25:21,120 --> 00:25:25,840
clusters

00:25:23,360 --> 00:25:26,400
this was compounded by the fact that we

00:25:25,840 --> 00:25:29,200
were all

00:25:26,400 --> 00:25:30,159
not only migrating existing workloads to

00:25:29,200 --> 00:25:32,400
kubernetes

00:25:30,159 --> 00:25:34,240
but we were in a growth period hiring

00:25:32,400 --> 00:25:37,200
and starting up some new services on

00:25:34,240 --> 00:25:37,200
kubernetes as well

00:25:37,600 --> 00:25:41,120
additionally since the policies were

00:25:39,440 --> 00:25:43,200
always essentially production

00:25:41,120 --> 00:25:46,320
there was no way to safely test new

00:25:43,200 --> 00:25:48,799
policies or changes to existing policies

00:25:46,320 --> 00:25:51,039
this meant that in a ci flow with conf

00:25:48,799 --> 00:25:51,279
test the ci flow would fail immediately

00:25:51,039 --> 00:25:54,960
if

00:25:51,279 --> 00:25:57,039
a violation was found and similarly

00:25:54,960 --> 00:26:00,960
gatekeeper would just reject changes to

00:25:57,039 --> 00:26:00,960
the cluster if it didn't meet policy

00:26:01,440 --> 00:26:05,440
additionally adding conf test to rci

00:26:03,919 --> 00:26:06,159
flows wasn't as easy as it could have

00:26:05,440 --> 00:26:08,080
been

00:26:06,159 --> 00:26:10,320
for one it kind of required our services

00:26:08,080 --> 00:26:11,760
teams to know the comp test flags and

00:26:10,320 --> 00:26:14,320
sort of how it worked

00:26:11,760 --> 00:26:15,520
and additionally these it took multiple

00:26:14,320 --> 00:26:17,760
steps to actually get

00:26:15,520 --> 00:26:20,880
policies from a remote source into the

00:26:17,760 --> 00:26:23,200
repo and then run the test

00:26:20,880 --> 00:26:24,880
but most importantly the contest results

00:26:23,200 --> 00:26:26,240
weren't surface to the teams working on

00:26:24,880 --> 00:26:28,320
the resources

00:26:26,240 --> 00:26:29,600
this meant that unless there was an

00:26:28,320 --> 00:26:31,279
actual violation

00:26:29,600 --> 00:26:33,200
no one would even know what tests were

00:26:31,279 --> 00:26:34,559
run or anything like that

00:26:33,200 --> 00:26:36,320
this was especially important to us

00:26:34,559 --> 00:26:38,240
because we do have some policies that we

00:26:36,320 --> 00:26:40,159
have labeled as just warnings

00:26:38,240 --> 00:26:42,000
which aren't blockers for deployment but

00:26:40,159 --> 00:26:43,840
they are a way that we want to

00:26:42,000 --> 00:26:44,799
communicate to our teams

00:26:43,840 --> 00:26:47,360
that the way that they have the

00:26:44,799 --> 00:26:48,159
resources configured might not be best

00:26:47,360 --> 00:26:51,679
practices

00:26:48,159 --> 00:26:53,840
practice finally

00:26:51,679 --> 00:26:55,760
policy admins such as myself had no

00:26:53,840 --> 00:26:58,159
visibility into the test results

00:26:55,760 --> 00:26:58,880
so we didn't really have any way to

00:26:58,159 --> 00:27:00,640
track which

00:26:58,880 --> 00:27:03,200
repositories or which teams had started

00:27:00,640 --> 00:27:05,279
using the policies or any of the results

00:27:03,200 --> 00:27:07,679
uh for which policies were making

00:27:05,279 --> 00:27:10,640
the test runs fail or which ones were

00:27:07,679 --> 00:27:12,559
just emitting warnings

00:27:10,640 --> 00:27:14,000
with these pain points identified we

00:27:12,559 --> 00:27:16,400
determined that there was essentially

00:27:14,000 --> 00:27:18,240
two things that we needed to do

00:27:16,400 --> 00:27:19,520
we needed to build a policy pipeline and

00:27:18,240 --> 00:27:21,279
that pipeline would ensure that the

00:27:19,520 --> 00:27:22,480
policies are safe to enforce throughout

00:27:21,279 --> 00:27:24,320
our clusters

00:27:22,480 --> 00:27:26,240
and we had to make the policy adoption

00:27:24,320 --> 00:27:28,399
as easy as possible

00:27:26,240 --> 00:27:29,840
if either of these weren't true it was a

00:27:28,399 --> 00:27:31,840
pretty good chance that they

00:27:29,840 --> 00:27:33,679
our policies wouldn't be adopted or it

00:27:31,840 --> 00:27:36,960
would just be a long uphill battle

00:27:33,679 --> 00:27:38,960
trying to get our teams to adopt them

00:27:36,960 --> 00:27:40,799
so first let's focus on what we did to

00:27:38,960 --> 00:27:42,000
make that policy adoption as easy as

00:27:40,799 --> 00:27:44,159
possible

00:27:42,000 --> 00:27:45,279
and to do that we created two github

00:27:44,159 --> 00:27:47,279
actions

00:27:45,279 --> 00:27:49,200
the first is a wrapper around conf test

00:27:47,279 --> 00:27:49,840
itself and it addresses some of the pain

00:27:49,200 --> 00:27:52,240
points

00:27:49,840 --> 00:27:53,840
from the previous slides it

00:27:52,240 --> 00:27:55,360
automatically pulls the latest policies

00:27:53,840 --> 00:27:57,200
from a remote source

00:27:55,360 --> 00:27:58,640
it surfaces the policy violation

00:27:57,200 --> 00:28:00,159
warnings and

00:27:58,640 --> 00:28:01,600
violations and warnings into the pull

00:28:00,159 --> 00:28:02,960
request comments so that the teams

00:28:01,600 --> 00:28:06,640
working on the resources

00:28:02,960 --> 00:28:08,559
can see the test results

00:28:06,640 --> 00:28:10,399
and it submits the results to a remote

00:28:08,559 --> 00:28:14,159
server so policy admins can

00:28:10,399 --> 00:28:14,159
monitor the deployments

00:28:15,039 --> 00:28:19,120
the second addresses an issue that we

00:28:17,919 --> 00:28:21,120
learned about later

00:28:19,120 --> 00:28:24,320
was that some of our teams had started

00:28:21,120 --> 00:28:26,640
to adopt flux cd for continuous delivery

00:28:24,320 --> 00:28:29,120
and we're using its helm operator and

00:28:26,640 --> 00:28:31,840
this home operator has a custom resource

00:28:29,120 --> 00:28:32,480
that lets you specify the helm chart

00:28:31,840 --> 00:28:34,320
source

00:28:32,480 --> 00:28:36,240
and then what values you want to apply

00:28:34,320 --> 00:28:38,240
to that chart and then it will

00:28:36,240 --> 00:28:39,679
go and fetch everything needed and just

00:28:38,240 --> 00:28:41,840
make those changes in your cluster for

00:28:39,679 --> 00:28:44,159
you

00:28:41,840 --> 00:28:45,360
however since helm templates are just

00:28:44,159 --> 00:28:47,039
that they're templates they aren't

00:28:45,360 --> 00:28:49,360
kubernetes resources

00:28:47,039 --> 00:28:50,960
we couldn't use config on them directly

00:28:49,360 --> 00:28:52,960
and this is because the data structure

00:28:50,960 --> 00:28:54,960
in the yaml was different

00:28:52,960 --> 00:28:56,559
and even if it was the same with

00:28:54,960 --> 00:28:58,000
templates you wouldn't actually have

00:28:56,559 --> 00:28:59,760
everything you needed until after

00:28:58,000 --> 00:29:02,399
execution

00:28:59,760 --> 00:29:03,039
so what this action does is it parses

00:29:02,399 --> 00:29:06,320
the

00:29:03,039 --> 00:29:08,320
the helm release resource

00:29:06,320 --> 00:29:10,080
it pulls the chart info the version all

00:29:08,320 --> 00:29:12,399
of those things

00:29:10,080 --> 00:29:14,080
it automatically sets up a helm

00:29:12,399 --> 00:29:16,240
repository so that it can pull those

00:29:14,080 --> 00:29:18,960
templates from the remote repository

00:29:16,240 --> 00:29:20,320
and then executes the templates so with

00:29:18,960 --> 00:29:22,320
this we can easily

00:29:20,320 --> 00:29:23,440
template out these resources before we

00:29:22,320 --> 00:29:25,360
run conf test

00:29:23,440 --> 00:29:27,840
so that it's nice and easy and it's a

00:29:25,360 --> 00:29:27,840
solid flow

00:29:28,960 --> 00:29:31,760
this makes it easy for our developers

00:29:30,399 --> 00:29:33,440
because they don't have to remember all

00:29:31,760 --> 00:29:35,279
of the flags for the helm template

00:29:33,440 --> 00:29:38,240
command or with the argument order

00:29:35,279 --> 00:29:39,600
or anything like that one thing to note

00:29:38,240 --> 00:29:41,679
here is that this currently only

00:29:39,600 --> 00:29:43,120
supports public helm repositories

00:29:41,679 --> 00:29:45,600
but we're working on adding support for

00:29:43,120 --> 00:29:48,080
private repositories too

00:29:45,600 --> 00:29:49,600
so with the ease of adoption addressed

00:29:48,080 --> 00:29:51,600
let's move on to the policy pipeline

00:29:49,600 --> 00:29:53,200
itself

00:29:51,600 --> 00:29:55,039
early on in the design of the policy

00:29:53,200 --> 00:29:57,039
pipeline we have this one rule

00:29:55,039 --> 00:29:58,159
is that we must not ever break

00:29:57,039 --> 00:29:59,840
production

00:29:58,159 --> 00:30:02,240
and for us that also includes any of the

00:29:59,840 --> 00:30:03,919
pipelines leading up to production

00:30:02,240 --> 00:30:06,159
this meant that for our teams that have

00:30:03,919 --> 00:30:08,080
an automated deployment from development

00:30:06,159 --> 00:30:10,720
to staging to production

00:30:08,080 --> 00:30:12,559
using custom metrics that breaking their

00:30:10,720 --> 00:30:13,919
access to the development cluster is the

00:30:12,559 --> 00:30:16,000
same as breaking their production

00:30:13,919 --> 00:30:18,320
pipeline

00:30:16,000 --> 00:30:19,760
in order to accomplish this we used two

00:30:18,320 --> 00:30:21,520
main methodologies

00:30:19,760 --> 00:30:23,919
the first was data-driven policy

00:30:21,520 --> 00:30:26,320
promotion and the second was a git ops

00:30:23,919 --> 00:30:28,399
deployment flow

00:30:26,320 --> 00:30:29,440
so diving into our policy promotion

00:30:28,399 --> 00:30:32,399
strategy

00:30:29,440 --> 00:30:33,840
we wanted to tackle one policy at a time

00:30:32,399 --> 00:30:35,840
which again is just

00:30:33,840 --> 00:30:36,880
reducing the window that new resources

00:30:35,840 --> 00:30:38,399
are introduced

00:30:36,880 --> 00:30:40,240
that violate the policy while you're

00:30:38,399 --> 00:30:43,200
working about on remediating the ones

00:30:40,240 --> 00:30:45,039
that you already know about

00:30:43,200 --> 00:30:47,360
another key component is we use

00:30:45,039 --> 00:30:49,679
gatekeepers enforcement action property

00:30:47,360 --> 00:30:50,559
to enforce to introduce the policies in

00:30:49,679 --> 00:30:53,279
dry run mode

00:30:50,559 --> 00:30:54,960
where they are not enforced however when

00:30:53,279 --> 00:30:57,039
they are in this mode we can still use

00:30:54,960 --> 00:30:59,200
gatekeepers audit functionality

00:30:57,039 --> 00:31:00,159
to audit our resources in our clusters

00:30:59,200 --> 00:31:04,000
and see which of them

00:31:00,159 --> 00:31:06,000
violate the policies

00:31:04,000 --> 00:31:07,360
we've made the decision to only switch

00:31:06,000 --> 00:31:09,440
to enforcement mode

00:31:07,360 --> 00:31:12,480
after we've identified that all of the

00:31:09,440 --> 00:31:14,720
offending resources have been remediated

00:31:12,480 --> 00:31:16,720
and as a side effect of this we have

00:31:14,720 --> 00:31:18,799
avoided setting hard deadlines for our

00:31:16,720 --> 00:31:20,000
teams to

00:31:18,799 --> 00:31:22,480
update their resources and make sure

00:31:20,000 --> 00:31:24,000
that they're remediated

00:31:22,480 --> 00:31:26,480
so one thing that we've done to work

00:31:24,000 --> 00:31:28,559
around that is in the case where a team

00:31:26,480 --> 00:31:30,960
says they just really can't

00:31:28,559 --> 00:31:32,000
get this fixed in the next couple weeks

00:31:30,960 --> 00:31:33,919
we can add temporary

00:31:32,000 --> 00:31:35,919
exceptions to the policy for those

00:31:33,919 --> 00:31:37,120
specific resources in specific name

00:31:35,919 --> 00:31:39,679
spaces

00:31:37,120 --> 00:31:41,840
but only when necessary that way we

00:31:39,679 --> 00:31:43,600
still have good coverage of you know 99

00:31:41,840 --> 00:31:47,440
of the resources in our clusters that

00:31:43,600 --> 00:31:50,720
are here adhering to the policy

00:31:47,440 --> 00:31:53,279
and finally we only promote policies to

00:31:50,720 --> 00:31:54,799
production when they're linked to a

00:31:53,279 --> 00:31:56,240
change management ticket which allows

00:31:54,799 --> 00:31:57,679
them to be scheduled

00:31:56,240 --> 00:31:59,440
this of course ensures that all the

00:31:57,679 --> 00:32:00,960
potentially impacted parties are aware

00:31:59,440 --> 00:32:03,039
of this upcoming change

00:32:00,960 --> 00:32:04,960
and it lets us schedule around times

00:32:03,039 --> 00:32:07,840
that we may be adding a feature release

00:32:04,960 --> 00:32:09,760
a product launch or something like that

00:32:07,840 --> 00:32:11,840
moving on to our get ops deployment flow

00:32:09,760 --> 00:32:13,360
approach it's a pretty standard approach

00:32:11,840 --> 00:32:15,600
where we use pull requests to move

00:32:13,360 --> 00:32:17,519
policies throughout the pipeline

00:32:15,600 --> 00:32:19,360
we use branch protection rules to ensure

00:32:17,519 --> 00:32:21,360
that peer review occurs and that all of

00:32:19,360 --> 00:32:22,559
our unit tests pass

00:32:21,360 --> 00:32:24,880
a couple of things that we're doing

00:32:22,559 --> 00:32:27,360
there is we require each policy have

00:32:24,880 --> 00:32:29,120
a unique policy identifier and we

00:32:27,360 --> 00:32:30,960
require that each policy have at least

00:32:29,120 --> 00:32:33,519
two unit tests

00:32:30,960 --> 00:32:35,039
one is for the positive path where

00:32:33,519 --> 00:32:36,000
gatekeeper blocks something that we

00:32:35,039 --> 00:32:38,080
expect it to

00:32:36,000 --> 00:32:39,679
and the second is the negative path

00:32:38,080 --> 00:32:41,200
where we

00:32:39,679 --> 00:32:44,960
a gatekeeper allow something through

00:32:41,200 --> 00:32:44,960
that we expect it to not lock

00:32:45,519 --> 00:32:48,799
we also take advantage of the github

00:32:47,360 --> 00:32:51,120
code owner's features

00:32:48,799 --> 00:32:52,480
which ensures that the peer reviews come

00:32:51,120 --> 00:32:53,919
from policy admins

00:32:52,480 --> 00:32:55,200
and that's because the policy admins are

00:32:53,919 --> 00:32:56,399
the ones who are the most familiar with

00:32:55,200 --> 00:32:58,960
the regular language

00:32:56,399 --> 00:33:02,159
and also some of the more intricate

00:32:58,960 --> 00:33:02,159
details of kubernetes

00:33:02,640 --> 00:33:06,159
and what's probably the most important

00:33:04,320 --> 00:33:06,640
point is that we use automation to

00:33:06,159 --> 00:33:09,840
create

00:33:06,640 --> 00:33:12,159
the gatekeeper and conf test resources

00:33:09,840 --> 00:33:13,200
this means that reviewers can focus on

00:33:12,159 --> 00:33:15,039
the policy

00:33:13,200 --> 00:33:17,200
and not the gatekeeper resources you

00:33:15,039 --> 00:33:18,960
know those giant emo files

00:33:17,200 --> 00:33:20,399
and it also makes it really obvious if

00:33:18,960 --> 00:33:21,519
someone is attempting to go around the

00:33:20,399 --> 00:33:23,360
tooling in place

00:33:21,519 --> 00:33:25,600
because no human should ever be trying

00:33:23,360 --> 00:33:28,080
to modify the gatekeeper or conf test

00:33:25,600 --> 00:33:29,440
resources directly

00:33:28,080 --> 00:33:32,000
with that let's go through the life of a

00:33:29,440 --> 00:33:34,159
policy in this policy pipeline

00:33:32,000 --> 00:33:36,000
so here we have a higher level view of

00:33:34,159 --> 00:33:38,399
how the policy flows

00:33:36,000 --> 00:33:39,360
we start by introducing a policy into

00:33:38,399 --> 00:33:41,279
the dev branch

00:33:39,360 --> 00:33:43,039
through a pull request and we want to

00:33:41,279 --> 00:33:45,360
make sure that the policy always has the

00:33:43,039 --> 00:33:47,120
enforcement action set to dry run

00:33:45,360 --> 00:33:48,559
after it's merged flux will

00:33:47,120 --> 00:33:51,200
automatically pick it up and start

00:33:48,559 --> 00:33:53,279
syncing it to the development clusters

00:33:51,200 --> 00:33:54,960
next we then promote this to staging and

00:33:53,279 --> 00:33:56,720
production branches

00:33:54,960 --> 00:33:58,240
still in dry run mode and this is to

00:33:56,720 --> 00:34:00,080
ensure that we have full visibility

00:33:58,240 --> 00:34:01,600
across all of our clusters

00:34:00,080 --> 00:34:03,919
because try as we might to keep all of

00:34:01,600 --> 00:34:04,559
these clusters configured identically

00:34:03,919 --> 00:34:06,720
through

00:34:04,559 --> 00:34:08,800
development staging and prod there's

00:34:06,720 --> 00:34:11,280
almost inevitably some variation in how

00:34:08,800 --> 00:34:13,520
they're configured

00:34:11,280 --> 00:34:15,119
so one thing to note is when we merge

00:34:13,520 --> 00:34:16,399
into the production branch is when we

00:34:15,119 --> 00:34:17,679
actually generate the conf test

00:34:16,399 --> 00:34:19,119
resources

00:34:17,679 --> 00:34:22,079
and one thing that we do there since

00:34:19,119 --> 00:34:25,359
conf test doesn't have the concept of

00:34:22,079 --> 00:34:26,960
an enforcement action is that we

00:34:25,359 --> 00:34:28,639
parse that enforcement action from the

00:34:26,960 --> 00:34:31,200
comment header and we

00:34:28,639 --> 00:34:32,720
in line rewrite any violation or deny

00:34:31,200 --> 00:34:35,280
rules to warnings

00:34:32,720 --> 00:34:36,079
so that when they're used in the ci

00:34:35,280 --> 00:34:37,760
flows

00:34:36,079 --> 00:34:39,760
the warnings are still surfaced to the

00:34:37,760 --> 00:34:44,079
teams working on those resources

00:34:39,760 --> 00:34:46,240
but it doesn't fail the ci flows yet

00:34:44,079 --> 00:34:48,159
so once we have all of these set up and

00:34:46,240 --> 00:34:49,679
running we then use the gatekeeper audit

00:34:48,159 --> 00:34:52,480
data as the source of truth

00:34:49,679 --> 00:34:53,919
to identify our existing resources that

00:34:52,480 --> 00:34:56,240
violate the policy

00:34:53,919 --> 00:34:58,079
and then we open tickets to work with

00:34:56,240 --> 00:35:00,320
our teams to remediate

00:34:58,079 --> 00:35:03,440
the issues that we've identified or to

00:35:00,320 --> 00:35:05,440
add exceptions where necessary

00:35:03,440 --> 00:35:07,760
after we've ensured that those are all

00:35:05,440 --> 00:35:10,160
remediated and the audit data shows this

00:35:07,760 --> 00:35:11,359
we switch the development clusters and

00:35:10,160 --> 00:35:14,240
the development branch to

00:35:11,359 --> 00:35:15,839
actually enforcement this is done by

00:35:14,240 --> 00:35:16,800
changing the enforcement action in the

00:35:15,839 --> 00:35:20,160
header from

00:35:16,800 --> 00:35:21,680
dry run to deny and once again

00:35:20,160 --> 00:35:24,079
after we make this change and merge to

00:35:21,680 --> 00:35:26,079
dev we are continuously monitoring the

00:35:24,079 --> 00:35:28,839
gatekeeper audit data to make sure that

00:35:26,079 --> 00:35:31,520
there isn't anything that we didn't

00:35:28,839 --> 00:35:33,280
expect finally once we do that we

00:35:31,520 --> 00:35:36,079
follow the same flow to promote to

00:35:33,280 --> 00:35:38,320
staging and then production

00:35:36,079 --> 00:35:39,839
it's worth noting again that when we

00:35:38,320 --> 00:35:42,480
make this change to production

00:35:39,839 --> 00:35:43,040
is when the conf test policies will

00:35:42,480 --> 00:35:46,720
actually

00:35:43,040 --> 00:35:47,680
be changed back to either a violation or

00:35:46,720 --> 00:35:50,640
a deny

00:35:47,680 --> 00:35:51,920
so at this point is when the ci flows

00:35:50,640 --> 00:35:53,440
will start failing

00:35:51,920 --> 00:35:55,920
if they have resources that violate the

00:35:53,440 --> 00:35:55,920
policy

00:35:56,880 --> 00:36:00,240
the comptest metrics here is useful for

00:35:59,119 --> 00:36:01,920
us to

00:36:00,240 --> 00:36:03,680
see what's going on across our

00:36:01,920 --> 00:36:05,839
organization to have a more full picture

00:36:03,680 --> 00:36:07,280
especially of newer repositories and

00:36:05,839 --> 00:36:08,880
newer projects that

00:36:07,280 --> 00:36:10,720
haven't actually made it into a cluster

00:36:08,880 --> 00:36:12,480
yet but they are not the

00:36:10,720 --> 00:36:14,960
source of truth for when we promote

00:36:12,480 --> 00:36:17,119
policies

00:36:14,960 --> 00:36:18,000
so where is ubico now on its policy

00:36:17,119 --> 00:36:19,920
journey

00:36:18,000 --> 00:36:21,680
well we have gatekeeper deployed to all

00:36:19,920 --> 00:36:24,079
of our clusters and we're tracking the

00:36:21,680 --> 00:36:25,359
resources that violate our policies

00:36:24,079 --> 00:36:27,359
additionally we're making our way

00:36:25,359 --> 00:36:29,920
through each policy and moving

00:36:27,359 --> 00:36:30,720
each to enforcement as we go we've

00:36:29,920 --> 00:36:33,280
noticed that in

00:36:30,720 --> 00:36:34,800
a lot of scenarios these policy

00:36:33,280 --> 00:36:35,440
violations are actually introduced

00:36:34,800 --> 00:36:37,280
upstream

00:36:35,440 --> 00:36:39,520
whether that be internal or external

00:36:37,280 --> 00:36:40,480
project and whenever we run into that we

00:36:39,520 --> 00:36:43,359
try and make our

00:36:40,480 --> 00:36:44,720
upstream our changes as well looking

00:36:43,359 --> 00:36:46,560
into the future though there's a few

00:36:44,720 --> 00:36:47,920
ways that we can work on making this

00:36:46,560 --> 00:36:49,520
better

00:36:47,920 --> 00:36:51,599
the first is to enable our teams to

00:36:49,520 --> 00:36:53,760
write their own policies for enforcing

00:36:51,599 --> 00:36:56,320
their own specific best practices

00:36:53,760 --> 00:36:58,320
this can be anything from a custom label

00:36:56,320 --> 00:36:58,720
to requiring that every deployment have

00:36:58,320 --> 00:37:01,920
a

00:36:58,720 --> 00:37:03,440
horizontal pod auto scaler attached

00:37:01,920 --> 00:37:05,200
additionally we're looking into syncing

00:37:03,440 --> 00:37:06,480
data from outside the cluster into

00:37:05,200 --> 00:37:09,040
gatekeeper to be used for

00:37:06,480 --> 00:37:10,960
more informed policy decisions for

00:37:09,040 --> 00:37:12,800
example we might want to sync our

00:37:10,960 --> 00:37:15,040
on-call rotation schedule into

00:37:12,800 --> 00:37:16,480
gatekeeper so that only the person who's

00:37:15,040 --> 00:37:18,240
on call for a given team

00:37:16,480 --> 00:37:20,320
can make changes to production in the

00:37:18,240 --> 00:37:22,079
event that a production issue or outage

00:37:20,320 --> 00:37:24,400
is occurring

00:37:22,079 --> 00:37:26,240
looking even further we are considering

00:37:24,400 --> 00:37:27,520
adding mutation controllers as well

00:37:26,240 --> 00:37:29,359
and those are a different type of

00:37:27,520 --> 00:37:30,000
controller that the api server can work

00:37:29,359 --> 00:37:32,079
with

00:37:30,000 --> 00:37:34,000
which rather than just rejecting a

00:37:32,079 --> 00:37:37,119
resource if it doesn't meet policy

00:37:34,000 --> 00:37:38,400
it can inline change the defaults as

00:37:37,119 --> 00:37:40,400
needed

00:37:38,400 --> 00:37:41,920
one thing to note there is gatekeeper

00:37:40,400 --> 00:37:44,480
currently has an open

00:37:41,920 --> 00:37:45,200
design for their implementation of that

00:37:44,480 --> 00:37:47,839
they haven't

00:37:45,200 --> 00:37:49,119
started on it yet so if you want to add

00:37:47,839 --> 00:37:50,960
your thoughts about how that should be

00:37:49,119 --> 00:37:52,480
shaped or anything like that

00:37:50,960 --> 00:37:54,240
go ahead and just search on the opa

00:37:52,480 --> 00:37:58,160
slack for

00:37:54,240 --> 00:37:58,160
mutation design and it'll come right up

00:37:58,800 --> 00:38:02,640
and that about wraps it up thank you

00:38:00,480 --> 00:38:05,839
everyone for attending and we are going

00:38:02,640 --> 00:38:05,839

YouTube URL: https://www.youtube.com/watch?v=lUU5rpMGI9s


