Title: Deep Dive: Linkerd - Zahari Dichev, Buoyant, Inc.
Publication date: 2020-09-11
Playlist: Cloud Native + Open Source Virtual Summit China 2020
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Deep Dive: Linkerd - Zahari Dichev, Buoyant, Inc. 

Linkerd 2.8 introduced new multi-cluster mTLS functionality that can be deployed to securely bridge cross-cluster communication. This deep dive session will be presented by Zahari Dichev, the engineer at Buoyant who led the development of this feature. In this session, he will explain how the feature works, trade-offs, and best-practices to consider as you roll it out to production. He will also review Linkerd's upcoming roadmap, highlighting opportunities for new contributors to get involved in the project. 

https://sched.co/cpBH
Captions: 
	00:00:01,040 --> 00:00:06,720
hello everyone my name is zachary dichev

00:00:03,679 --> 00:00:08,000
and i work at buoyant uh the creators of

00:00:06,720 --> 00:00:09,760
linkerbeam

00:00:08,000 --> 00:00:11,280
today we're going to do a lingerie deep

00:00:09,760 --> 00:00:14,080
dive and we're going to focus

00:00:11,280 --> 00:00:15,920
on one particular feature which is the

00:00:14,080 --> 00:00:17,119
multi-cluster functionality that was

00:00:15,920 --> 00:00:20,400
released in

00:00:17,119 --> 00:00:23,359
incredibly 2.8 so

00:00:20,400 --> 00:00:24,000
the agenda for this talk is um we're

00:00:23,359 --> 00:00:27,359
going to look

00:00:24,000 --> 00:00:28,480
at the what service meshes and why you

00:00:27,359 --> 00:00:30,080
might need one

00:00:28,480 --> 00:00:31,679
we're going to cover the multi-cluster

00:00:30,080 --> 00:00:33,920
concepts that

00:00:31,679 --> 00:00:35,440
govern the implementation of this

00:00:33,920 --> 00:00:36,640
feature we're going to look at the

00:00:35,440 --> 00:00:39,200
general architecture

00:00:36,640 --> 00:00:40,160
of multicluster then we're going to do a

00:00:39,200 --> 00:00:41,920
quick demo

00:00:40,160 --> 00:00:44,800
showing how this feature feels in the

00:00:41,920 --> 00:00:47,039
terminal and what you can do with it

00:00:44,800 --> 00:00:48,160
and then we're going to look at the life

00:00:47,039 --> 00:00:52,000
of a request

00:00:48,160 --> 00:00:54,800
across clusters so let's get started

00:00:52,000 --> 00:00:56,840
a lot has been said about service meshes

00:00:54,800 --> 00:00:58,719
and

00:00:56,840 --> 00:01:02,320
um in fact

00:00:58,719 --> 00:01:04,479
um i think the best way to describe it

00:01:02,320 --> 00:01:06,320
is through a quote that i read recently

00:01:04,479 --> 00:01:08,560
on our blog

00:01:06,320 --> 00:01:10,240
namely you might think that when you

00:01:08,560 --> 00:01:12,400
have a distributed system

00:01:10,240 --> 00:01:13,520
the most important thing is that amb can

00:01:12,400 --> 00:01:16,080
exchange packets

00:01:13,520 --> 00:01:18,240
and that's true it is important however

00:01:16,080 --> 00:01:20,400
as your system grows

00:01:18,240 --> 00:01:22,159
and becomes more complex you need to be

00:01:20,400 --> 00:01:25,439
able to provide

00:01:22,159 --> 00:01:30,159
more guarantees and

00:01:25,439 --> 00:01:33,280
what linkrd gives you is the following

00:01:30,159 --> 00:01:35,360
it provides the guarantee that amb

00:01:33,280 --> 00:01:37,040
can exchange packets in a way that

00:01:35,360 --> 00:01:39,680
validates the identity

00:01:37,040 --> 00:01:40,799
on both sides has clear authorization

00:01:39,680 --> 00:01:43,360
semantics

00:01:40,799 --> 00:01:44,399
is confidential to third parties and is

00:01:43,360 --> 00:01:48,640
measurable

00:01:44,399 --> 00:01:51,439
and inspectable so this is a pretty

00:01:48,640 --> 00:01:53,759
um concise quote but i think it captures

00:01:51,439 --> 00:01:56,880
the essence of

00:01:53,759 --> 00:01:59,119
and it goes and you know in

00:01:56,880 --> 00:02:00,799
a typical distributed system you might

00:01:59,119 --> 00:02:02,240
have an architecture that looks like

00:02:00,799 --> 00:02:04,719
this you have a client

00:02:02,240 --> 00:02:06,079
that communicates with an api gateway

00:02:04,719 --> 00:02:08,800
that happens to live

00:02:06,079 --> 00:02:10,319
in a kubernetes cluster this api gateway

00:02:08,800 --> 00:02:12,000
then routes requests to a number of

00:02:10,319 --> 00:02:14,080
internal services

00:02:12,000 --> 00:02:15,360
and these services might talk to a

00:02:14,080 --> 00:02:18,319
database or

00:02:15,360 --> 00:02:19,280
a redis cache or for that matter they

00:02:18,319 --> 00:02:21,520
might be

00:02:19,280 --> 00:02:23,280
talking to a third party api that's

00:02:21,520 --> 00:02:26,319
reachable over the internet

00:02:23,280 --> 00:02:29,680
and this is all great until

00:02:26,319 --> 00:02:31,840
things start to go sideways so

00:02:29,680 --> 00:02:33,599
for example your database might become

00:02:31,840 --> 00:02:36,959
unavailable

00:02:33,599 --> 00:02:39,200
or you might deploy a

00:02:36,959 --> 00:02:40,160
new service version that's significantly

00:02:39,200 --> 00:02:43,200
slower

00:02:40,160 --> 00:02:45,680
than the previous one

00:02:43,200 --> 00:02:45,680
causing

00:02:46,239 --> 00:02:53,120
all of your other services to exhibit

00:02:49,840 --> 00:02:56,239
behavior or somebody might

00:02:53,120 --> 00:02:58,159
by mistake cut the fiber optics cables

00:02:56,239 --> 00:03:00,879
and now you don't have connectivity to

00:02:58,159 --> 00:03:03,440
the internet and when this happens

00:03:00,879 --> 00:03:04,000
or any of these things happen you want

00:03:03,440 --> 00:03:08,159
to be able

00:03:04,000 --> 00:03:11,280
to spot the exact problem pinpointed

00:03:08,159 --> 00:03:13,040
and react quickly to it and this is one

00:03:11,280 --> 00:03:15,920
of the things that lingerie

00:03:13,040 --> 00:03:15,920
tries to

00:03:16,239 --> 00:03:19,680
try to help you do and the way we do

00:03:19,280 --> 00:03:22,879
that

00:03:19,680 --> 00:03:25,599
is by injecting a proxy into

00:03:22,879 --> 00:03:27,920
all of your mesh parts and this proxy

00:03:25,599 --> 00:03:30,480
effectively its purpose is to intercept

00:03:27,920 --> 00:03:31,760
the prop the traffic that goes in and

00:03:30,480 --> 00:03:34,319
out of the pot

00:03:31,760 --> 00:03:35,040
so instead of your usual traffic pattern

00:03:34,319 --> 00:03:37,120
now your

00:03:35,040 --> 00:03:38,720
traffic looks a bit like this everything

00:03:37,120 --> 00:03:43,200
goes through the proxy

00:03:38,720 --> 00:03:45,519
this proxy uh communicates with a set of

00:03:43,200 --> 00:03:47,360
components that we collectively refer to

00:03:45,519 --> 00:03:50,640
as the control plane

00:03:47,360 --> 00:03:53,040
uh through through um

00:03:50,640 --> 00:03:53,840
interface and that control plane runs

00:03:53,040 --> 00:03:57,680
alongside

00:03:53,840 --> 00:04:00,000
in in in its separate name space so

00:03:57,680 --> 00:04:01,040
this control plane gives you a few

00:04:00,000 --> 00:04:03,840
things it

00:04:01,040 --> 00:04:05,120
provides tls certificates for the proxy

00:04:03,840 --> 00:04:08,159
so now all of

00:04:05,120 --> 00:04:11,200
the communication

00:04:08,159 --> 00:04:13,760
between meshed pods in between proxies

00:04:11,200 --> 00:04:16,880
is encrypted out of the box it gives you

00:04:13,760 --> 00:04:20,799
service discovery utilizing dns

00:04:16,880 --> 00:04:22,880
and native kubernetes primitives

00:04:20,799 --> 00:04:24,479
it allows you to configure service

00:04:22,880 --> 00:04:27,919
profiles

00:04:24,479 --> 00:04:28,960
so now you can specify things like retry

00:04:27,919 --> 00:04:31,840
and timeout rules

00:04:28,960 --> 00:04:32,880
per service it supports automatic proxy

00:04:31,840 --> 00:04:35,919
injection

00:04:32,880 --> 00:04:37,280
and it also gives you a web dashboard

00:04:35,919 --> 00:04:39,199
with a bunch of metrics

00:04:37,280 --> 00:04:41,199
that allow you to look at the runtime

00:04:39,199 --> 00:04:44,800
characteristics of your traffic

00:04:41,199 --> 00:04:48,639
and it also exports a public api

00:04:44,800 --> 00:04:51,759
that we use for a number of cli tools

00:04:48,639 --> 00:04:53,840
that we ship with linker b

00:04:51,759 --> 00:04:55,440
the proxy on the other side that we

00:04:53,840 --> 00:04:58,320
refer to as the data plane

00:04:55,440 --> 00:05:01,120
is ultralight's transparent one written

00:04:58,320 --> 00:05:05,840
in rust specifically for link or d

00:05:01,120 --> 00:05:05,840
and specifically for kubernetes and

00:05:05,919 --> 00:05:08,639
it's um

00:05:08,960 --> 00:05:15,840
it supports a few things so it exposes

00:05:12,639 --> 00:05:19,360
an endpoint um that provides you

00:05:15,840 --> 00:05:20,160
automatic prometheus metrics for http

00:05:19,360 --> 00:05:21,680
and tcp

00:05:20,160 --> 00:05:23,919
traffic so now your permittee

00:05:21,680 --> 00:05:25,919
assistances can scrape

00:05:23,919 --> 00:05:27,039
this endpoint and give you a bunch of

00:05:25,919 --> 00:05:29,840
useful traffic

00:05:27,039 --> 00:05:31,199
traffic information for all the traffic

00:05:29,840 --> 00:05:34,080
that goes through the proxy

00:05:31,199 --> 00:05:35,280
it provides latency aware layer 7 load

00:05:34,080 --> 00:05:38,160
balancing

00:05:35,280 --> 00:05:38,880
as well as automatic tls and an

00:05:38,160 --> 00:05:42,400
on-demand

00:05:38,880 --> 00:05:43,680
diagnostic tap api which uh purposes to

00:05:42,400 --> 00:05:46,000
allow you to

00:05:43,680 --> 00:05:48,720
look at life requests uh in the live

00:05:46,000 --> 00:05:51,280
traffic that's going through your proxy

00:05:48,720 --> 00:05:54,000
so it's probably normal to ask how does

00:05:51,280 --> 00:05:55,840
that proxy actually end up in your parts

00:05:54,000 --> 00:05:57,440
well that's done through something that

00:05:55,840 --> 00:05:59,520
we refer to as injection

00:05:57,440 --> 00:06:01,280
and it's in this process is usually

00:05:59,520 --> 00:06:03,039
accomplished by the proxy injector

00:06:01,280 --> 00:06:05,919
component which is part of the control

00:06:03,039 --> 00:06:06,400
plane but it can also be done manually

00:06:05,919 --> 00:06:08,800
so

00:06:06,400 --> 00:06:09,520
um this actually puts two containers in

00:06:08,800 --> 00:06:11,520
your pots

00:06:09,520 --> 00:06:14,160
uh an init container and the proxy

00:06:11,520 --> 00:06:16,800
container so this is best represented

00:06:14,160 --> 00:06:18,800
with with the diagram so if you have

00:06:16,800 --> 00:06:20,479
your

00:06:18,800 --> 00:06:22,560
usual kubernetes is bought with an

00:06:20,479 --> 00:06:25,360
application container and some traffic

00:06:22,560 --> 00:06:26,639
going in and out of the pot what happens

00:06:25,360 --> 00:06:28,720
upon injection is that

00:06:26,639 --> 00:06:31,039
we add an init container and when that

00:06:28,720 --> 00:06:32,240
runs it sets up a bunch of iptables

00:06:31,039 --> 00:06:35,840
rules

00:06:32,240 --> 00:06:38,160
that are responsible for routing traffic

00:06:35,840 --> 00:06:40,160
to the pro for making the proxy

00:06:38,160 --> 00:06:42,960
intercept all the traffic

00:06:40,160 --> 00:06:43,600
and we also add the proxy so now all

00:06:42,960 --> 00:06:45,520
your aiming

00:06:43,600 --> 00:06:47,759
incoming traffic goes through the proxy

00:06:45,520 --> 00:06:48,639
first and then to the application

00:06:47,759 --> 00:06:51,759
container

00:06:48,639 --> 00:06:54,880
and then um similarly all of your

00:06:51,759 --> 00:06:56,080
outgoing traffic leaves the application

00:06:54,880 --> 00:06:59,280
goes through the proxy

00:06:56,080 --> 00:07:03,599
and then to where it's destined

00:06:59,280 --> 00:07:06,479
so some of the core concepts of linker d

00:07:03,599 --> 00:07:08,639
uh are observability uh which is

00:07:06,479 --> 00:07:11,280
collecting actionable traffic metrics

00:07:08,639 --> 00:07:12,240
allowing you to pinpoint problems and

00:07:11,280 --> 00:07:15,919
understand

00:07:12,240 --> 00:07:16,720
um the dynamics of your traffic security

00:07:15,919 --> 00:07:20,240
encrypting

00:07:16,720 --> 00:07:22,319
traffic between services automatically

00:07:20,240 --> 00:07:23,840
reliability as well as traffic

00:07:22,319 --> 00:07:26,000
management so you can

00:07:23,840 --> 00:07:27,120
use advanced patterns like traffic

00:07:26,000 --> 00:07:30,880
splitting and

00:07:27,120 --> 00:07:34,319
and all sorts of things

00:07:30,880 --> 00:07:37,039
and when we were um thinking about with

00:07:34,319 --> 00:07:37,759
multi-cluster we were wondering how can

00:07:37,039 --> 00:07:41,199
we

00:07:37,759 --> 00:07:44,639
um have these guarantees

00:07:41,199 --> 00:07:47,360
and expand them to span

00:07:44,639 --> 00:07:47,919
across clusters and first of all maybe

00:07:47,360 --> 00:07:50,720
it's

00:07:47,919 --> 00:07:52,879
worth asking why multiple clusters well

00:07:50,720 --> 00:07:56,160
there is a number of use cases but

00:07:52,879 --> 00:07:58,639
but they mostly come down to

00:07:56,160 --> 00:07:59,199
traffic migration across clusters so

00:07:58,639 --> 00:08:02,240
this

00:07:59,199 --> 00:08:04,879
this includes calorie deployments um

00:08:02,240 --> 00:08:07,120
different environments that allow you

00:08:04,879 --> 00:08:09,360
that allow your developers for example

00:08:07,120 --> 00:08:09,360
to

00:08:09,520 --> 00:08:13,199
split traffic across the development and

00:08:12,080 --> 00:08:16,720
production environment

00:08:13,199 --> 00:08:19,599
and use the production components um

00:08:16,720 --> 00:08:20,319
while developing locally as well as

00:08:19,599 --> 00:08:23,199
failover

00:08:20,319 --> 00:08:24,560
so a lot of people actually do a

00:08:23,199 --> 00:08:28,319
failover across

00:08:24,560 --> 00:08:31,759
clusters in different availability zones

00:08:28,319 --> 00:08:33,599
and that sort of thing so we

00:08:31,759 --> 00:08:35,360
tried to build upon the link or d

00:08:33,599 --> 00:08:37,680
concepts and um

00:08:35,360 --> 00:08:38,479
have our multi-cluster implementation be

00:08:37,680 --> 00:08:42,959
governed

00:08:38,479 --> 00:08:43,360
by by other core concepts that expand

00:08:42,959 --> 00:08:46,000
upon

00:08:43,360 --> 00:08:47,040
the linkery ones so our implementation

00:08:46,000 --> 00:08:49,440
is secure

00:08:47,040 --> 00:08:50,800
everything happens over pls even traffic

00:08:49,440 --> 00:08:54,399
across clusters

00:08:50,800 --> 00:08:57,279
it's kubernetes first so remote services

00:08:54,399 --> 00:08:59,680
appear as kubernetes services uh there

00:08:57,279 --> 00:09:01,760
is no special kind of service

00:08:59,680 --> 00:09:04,080
there is no single point of failure so

00:09:01,760 --> 00:09:06,880
no cluster in this cluster mesh

00:09:04,080 --> 00:09:07,600
is blessed or magical in any way if a

00:09:06,880 --> 00:09:09,680
cluster

00:09:07,600 --> 00:09:11,600
goes down then that doesn't affect the

00:09:09,680 --> 00:09:13,680
rest of the clusters

00:09:11,600 --> 00:09:15,920
this implementation is transparent so

00:09:13,680 --> 00:09:20,560
applications don't need to know

00:09:15,920 --> 00:09:22,800
whether a service is remote or local

00:09:20,560 --> 00:09:24,880
and uh of course this implementation is

00:09:22,800 --> 00:09:26,399
also network independent so as long as

00:09:24,880 --> 00:09:29,519
you have connectivity

00:09:26,399 --> 00:09:30,640
between clusters then the underlying

00:09:29,519 --> 00:09:34,880
network hierarchy

00:09:30,640 --> 00:09:36,160
doesn't doesn't actually matter at all

00:09:34,880 --> 00:09:38,640
so if we have to look at the

00:09:36,160 --> 00:09:41,040
architecture of this solution

00:09:38,640 --> 00:09:43,120
it uh looks like this if we have a

00:09:41,040 --> 00:09:44,080
cluster east and cluster west and we're

00:09:43,120 --> 00:09:47,200
going to look at

00:09:44,080 --> 00:09:49,040
the demo in a bit that's going to show

00:09:47,200 --> 00:09:51,680
this exact situation

00:09:49,040 --> 00:09:52,880
um these two clusters need to have

00:09:51,680 --> 00:09:55,519
linkard installed

00:09:52,880 --> 00:09:56,959
and it's important to know that linkerd

00:09:55,519 --> 00:09:59,640
needs to be installed with

00:09:56,959 --> 00:10:01,600
a trustroot that um

00:09:59,640 --> 00:10:04,800
[Music]

00:10:01,600 --> 00:10:09,040
with trust routes that's um

00:10:04,800 --> 00:10:12,079
that is shared so this enables um

00:10:09,040 --> 00:10:14,480
automatic tls across across

00:10:12,079 --> 00:10:16,240
clusters and then you have the linker d

00:10:14,480 --> 00:10:18,800
multi-cluster components which consist

00:10:16,240 --> 00:10:22,000
of the service mirror the gateway

00:10:18,800 --> 00:10:22,320
and cluster credentials and then you

00:10:22,000 --> 00:10:24,880
have

00:10:22,320 --> 00:10:26,480
your local services that might have pots

00:10:24,880 --> 00:10:28,959
or other

00:10:26,480 --> 00:10:30,880
pots behind them and then you have the

00:10:28,959 --> 00:10:33,519
remote services that are effective

00:10:30,880 --> 00:10:35,360
proxies for services that live on the

00:10:33,519 --> 00:10:38,079
target cluster

00:10:35,360 --> 00:10:40,160
so the service mirror controller is

00:10:38,079 --> 00:10:40,880
actually responsible for monitoring the

00:10:40,160 --> 00:10:43,279
export

00:10:40,880 --> 00:10:44,480
state of the target cluster and

00:10:43,279 --> 00:10:48,079
replicating it

00:10:44,480 --> 00:10:50,399
this is uh done through using the clango

00:10:48,079 --> 00:10:52,160
and kubernetes informers in order to

00:10:50,399 --> 00:10:54,160
continuously monitor the state of the

00:10:52,160 --> 00:10:56,399
services on the on the

00:10:54,160 --> 00:10:58,480
on the target cluster then we have the

00:10:56,399 --> 00:10:58,800
gateway component which is responsible

00:10:58,480 --> 00:11:00,959
that

00:10:58,800 --> 00:11:02,880
that usually lives on the target cluster

00:11:00,959 --> 00:11:06,240
and is responsible for

00:11:02,880 --> 00:11:08,640
routing requests that come in and

00:11:06,240 --> 00:11:09,360
you know effectively taking requests on

00:11:08,640 --> 00:11:12,079
one port

00:11:09,360 --> 00:11:14,800
and routing them to the correct um

00:11:12,079 --> 00:11:16,800
service to the correct internal service

00:11:14,800 --> 00:11:18,880
and then you have credentials and these

00:11:16,800 --> 00:11:20,959
credentials consist of two pieces

00:11:18,880 --> 00:11:22,720
so you have the service account that

00:11:20,959 --> 00:11:25,920
lives on the target cluster

00:11:22,720 --> 00:11:29,120
which allows you to monitor

00:11:25,920 --> 00:11:32,000
um which allows you to inspect services

00:11:29,120 --> 00:11:33,440
get lists and watch services and you

00:11:32,000 --> 00:11:37,040
have the secret

00:11:33,440 --> 00:11:39,920
uh kubernetes secret containing

00:11:37,040 --> 00:11:41,760
uh kubernetes api config and this lives

00:11:39,920 --> 00:11:43,760
on the source cluster and is used by the

00:11:41,760 --> 00:11:46,240
service mirror console controller

00:11:43,760 --> 00:11:47,920
in order to initiate an api connection

00:11:46,240 --> 00:11:49,600
to the target cluster and

00:11:47,920 --> 00:11:51,120
be able to spawn up spin up the

00:11:49,600 --> 00:11:54,880
informers

00:11:51,120 --> 00:11:58,480
so with all that said now it's time for

00:11:54,880 --> 00:11:59,360
demo so in this demo we're going to have

00:11:58,480 --> 00:12:02,639
two clusters

00:11:59,360 --> 00:12:03,279
east and west and each of these cluster

00:12:02,639 --> 00:12:06,959
heads

00:12:03,279 --> 00:12:10,399
has back-end service

00:12:06,959 --> 00:12:12,639
installed and a test client that lives

00:12:10,399 --> 00:12:13,519
on cluster east that we're going to use

00:12:12,639 --> 00:12:17,279
to uh

00:12:13,519 --> 00:12:18,000
fire requests and what we want is to be

00:12:17,279 --> 00:12:21,600
able to

00:12:18,000 --> 00:12:22,720
spin uh split the traffic to back-end

00:12:21,600 --> 00:12:25,839
service between

00:12:22,720 --> 00:12:25,839
east and west

00:12:27,200 --> 00:12:32,000
so let's actually look at the terminal

00:12:29,760 --> 00:12:32,000
here

00:12:34,639 --> 00:12:41,680
so as i said you have cluster east

00:12:38,000 --> 00:12:42,959
that has a back-end service and cluster

00:12:41,680 --> 00:12:46,000
west

00:12:42,959 --> 00:12:49,440
that has a back-end service too so

00:12:46,000 --> 00:12:52,880
now what we want to do is we want

00:12:49,440 --> 00:12:54,639
to install the linker d multi-cluster

00:12:52,880 --> 00:12:57,519
components on both

00:12:54,639 --> 00:12:57,519
cluster east

00:12:57,680 --> 00:13:07,839
and cluster west

00:13:15,279 --> 00:13:21,839
so now that this is done

00:13:18,839 --> 00:13:21,839
um

00:13:23,680 --> 00:13:27,680
we can see that the uh linker the

00:13:26,320 --> 00:13:29,519
gateway and the service mirror

00:13:27,680 --> 00:13:33,279
controller

00:13:29,519 --> 00:13:36,720
is um they're both running well

00:13:33,279 --> 00:13:36,720
the gateway is still not running but

00:13:38,560 --> 00:13:41,760
same for cluster east

00:13:43,680 --> 00:13:51,279
so right now we want to be

00:13:46,720 --> 00:13:53,519
able to link these two clusters together

00:13:51,279 --> 00:13:54,320
so what we are going to do is that we

00:13:53,519 --> 00:13:57,120
are going to use the

00:13:54,320 --> 00:13:59,279
multi-cluster link command to provide

00:13:57,120 --> 00:14:02,000
credentials to cluster yeast

00:13:59,279 --> 00:14:02,959
um and allow it to be able to monitor

00:14:02,000 --> 00:14:05,360
the state

00:14:02,959 --> 00:14:07,120
of cluster west and we're going to do

00:14:05,360 --> 00:14:07,519
that through the link or the uh through

00:14:07,120 --> 00:14:11,279
the

00:14:07,519 --> 00:14:13,760
multi-cluster link command so now the

00:14:11,279 --> 00:14:15,279
secret was the secret containing

00:14:13,760 --> 00:14:18,240
credentials was created

00:14:15,279 --> 00:14:18,240
in cluster east

00:14:18,480 --> 00:14:22,160
the other thing we need to do is to

00:14:21,120 --> 00:14:24,480
export

00:14:22,160 --> 00:14:26,079
this the backend service from cluster

00:14:24,480 --> 00:14:27,760
west and make it available

00:14:26,079 --> 00:14:30,560
on customer east and we're going to do

00:14:27,760 --> 00:14:33,199
that through the um

00:14:30,560 --> 00:14:34,160
export service command and this is this

00:14:33,199 --> 00:14:36,000
this service command

00:14:34,160 --> 00:14:37,519
this export service command pretty much

00:14:36,000 --> 00:14:39,920
adds some uh

00:14:37,519 --> 00:14:41,519
annotations and some metadata to your

00:14:39,920 --> 00:14:45,120
service yaml

00:14:41,519 --> 00:14:48,000
and um informs the um

00:14:45,120 --> 00:14:50,060
the service mirror that this services is

00:14:48,000 --> 00:14:51,920
can be exported

00:14:50,060 --> 00:14:55,440
[Music]

00:14:51,920 --> 00:14:55,760
so now that we have that um what we can

00:14:55,440 --> 00:14:59,600
do

00:14:55,760 --> 00:15:00,800
is uh take a look at the services on

00:14:59,600 --> 00:15:04,000
cluster

00:15:00,800 --> 00:15:06,160
east again

00:15:04,000 --> 00:15:08,079
and now you can see that apart from the

00:15:06,160 --> 00:15:10,079
local back-end service we have a

00:15:08,079 --> 00:15:13,199
back-end service

00:15:10,079 --> 00:15:16,480
dash west and this is a service that is

00:15:13,199 --> 00:15:18,639
um that is effectively

00:15:16,480 --> 00:15:20,079
um just a proxy service and all the

00:15:18,639 --> 00:15:21,920
traffic that goes through it's gonna be

00:15:20,079 --> 00:15:22,320
routed to the service and the pods that

00:15:21,920 --> 00:15:25,600
live

00:15:22,320 --> 00:15:29,040
on cluster west so

00:15:25,600 --> 00:15:31,600
now we can use the gateways

00:15:29,040 --> 00:15:31,600
command

00:15:32,240 --> 00:15:35,920
in order to see all the gateways that

00:15:35,600 --> 00:15:39,440
are

00:15:35,920 --> 00:15:42,560
available on uh for to cluster east

00:15:39,440 --> 00:15:43,600
so you can see that there is a lingerie

00:15:42,560 --> 00:15:46,240
gateway

00:15:43,600 --> 00:15:46,880
for cluster west that is available on

00:15:46,240 --> 00:15:49,440
cluster

00:15:46,880 --> 00:15:51,120
east and there is a one mirrored

00:15:49,440 --> 00:15:53,199
services

00:15:51,120 --> 00:15:54,240
and the latency to this gateway and

00:15:53,199 --> 00:15:57,040
there are some stats

00:15:54,240 --> 00:15:59,279
about the latency so what can we do now

00:15:57,040 --> 00:15:59,279
well

00:15:59,440 --> 00:16:09,839
we can use our

00:16:03,600 --> 00:16:09,839
pot here

00:16:11,920 --> 00:16:17,600
and in fact be able to hit both services

00:16:18,959 --> 00:16:27,120
so we have a response from cluster west

00:16:23,920 --> 00:16:29,759
and we have a response for from

00:16:27,120 --> 00:16:30,800
cluster east as well which is local now

00:16:29,759 --> 00:16:32,959
the interesting

00:16:30,800 --> 00:16:34,320
part is that we can actually traffic

00:16:32,959 --> 00:16:36,000
split between

00:16:34,320 --> 00:16:37,440
these two services between these two

00:16:36,000 --> 00:16:41,040
clusters right

00:16:37,440 --> 00:16:44,560
so we have this traffic split

00:16:41,040 --> 00:16:48,399
resource here that

00:16:44,560 --> 00:16:50,320
specifies to leave services

00:16:48,399 --> 00:16:52,639
and the traffic will be equally split

00:16:50,320 --> 00:16:55,920
between between these two

00:16:52,639 --> 00:16:59,360
so once we apply it and

00:16:55,920 --> 00:17:02,839
we um get into our

00:16:59,360 --> 00:17:05,839
pot again now we can run

00:17:02,839 --> 00:17:05,839
a

00:17:06,880 --> 00:17:12,720
a curl command that's um curls

00:17:09,919 --> 00:17:14,000
back-end service and what's going to

00:17:12,720 --> 00:17:16,559
happen is that

00:17:14,000 --> 00:17:18,319
the traffic is going to be split between

00:17:16,559 --> 00:17:19,600
between east and west and you can see

00:17:18,319 --> 00:17:22,799
that this is happening

00:17:19,600 --> 00:17:25,520
because we get different responses

00:17:22,799 --> 00:17:28,000
and what's also interesting to observe

00:17:25,520 --> 00:17:28,000
is that

00:17:28,640 --> 00:17:34,400
we can actually

00:17:32,480 --> 00:17:35,520
we can actually look uh use the link or

00:17:34,400 --> 00:17:38,080
dcli to

00:17:35,520 --> 00:17:40,320
look at our traffic splits and see that

00:17:38,080 --> 00:17:42,880
our back-end service has two leaves

00:17:40,320 --> 00:17:44,880
the back-end service which is local and

00:17:42,880 --> 00:17:45,360
the back-end service switches on cluster

00:17:44,880 --> 00:17:47,919
west

00:17:45,360 --> 00:17:48,640
and the success rate is 100 for both of

00:17:47,919 --> 00:17:50,240
them

00:17:48,640 --> 00:17:52,080
but what's interesting to observe is

00:17:50,240 --> 00:17:52,559
that the latency is quite different

00:17:52,080 --> 00:17:54,240
right

00:17:52,559 --> 00:17:56,080
so it's normal because one of our

00:17:54,240 --> 00:17:59,760
services is local

00:17:56,080 --> 00:18:04,320
while the other one is uh in fact

00:17:59,760 --> 00:18:06,880
a remote one so

00:18:04,320 --> 00:18:08,160
you might wonder how is that actually

00:18:06,880 --> 00:18:11,440
happening so how does that

00:18:08,160 --> 00:18:16,640
how does that actually work well

00:18:11,440 --> 00:18:16,640
if we go back to our presentation

00:18:16,880 --> 00:18:23,919
yeah there is there is um

00:18:20,640 --> 00:18:25,760
let's actually look at a uh the life of

00:18:23,919 --> 00:18:26,480
this curve request as it goes across

00:18:25,760 --> 00:18:29,120
clusters

00:18:26,480 --> 00:18:29,840
well we have our client port right and

00:18:29,120 --> 00:18:33,520
this clan

00:18:29,840 --> 00:18:37,039
pot uh fires

00:18:33,520 --> 00:18:40,400
uh it lives on cluster east and fires um

00:18:37,039 --> 00:18:43,039
http requests to backend service that

00:18:40,400 --> 00:18:44,880
for the purpose of this of this request

00:18:43,039 --> 00:18:46,000
let's say that this gets traffic split

00:18:44,880 --> 00:18:49,600
and goes to

00:18:46,000 --> 00:18:52,400
cluster west so once that happens

00:18:49,600 --> 00:18:54,240
once this request leaves the pot it's

00:18:52,400 --> 00:18:54,640
going to back in service west which is

00:18:54,240 --> 00:18:57,760
the

00:18:54,640 --> 00:19:01,520
service proxy that lives on our cluster

00:18:57,760 --> 00:19:04,160
now we do um this this

00:19:01,520 --> 00:19:05,280
back-end service has an endpoint

00:19:04,160 --> 00:19:09,039
associated with it

00:19:05,280 --> 00:19:12,559
that is uh that specifies the public ip

00:19:09,039 --> 00:19:15,120
and port of the gateway on cluster west

00:19:12,559 --> 00:19:16,160
and because this request goes through

00:19:15,120 --> 00:19:19,919
the proxy

00:19:16,160 --> 00:19:20,400
we can be a bit smarter about what we do

00:19:19,919 --> 00:19:22,960
with it

00:19:20,400 --> 00:19:25,120
so once this request hits the proxy the

00:19:22,960 --> 00:19:26,799
proxy uses the lingerie destination

00:19:25,120 --> 00:19:30,080
component which is

00:19:26,799 --> 00:19:32,480
um part of the control plane and issues

00:19:30,080 --> 00:19:33,679
a discovery query to it now the link rd

00:19:32,480 --> 00:19:36,080
destination

00:19:33,679 --> 00:19:37,200
a service knows that this is um this is

00:19:36,080 --> 00:19:39,919
a proxy service

00:19:37,200 --> 00:19:40,720
so it's going to return some useful

00:19:39,919 --> 00:19:43,840
information

00:19:40,720 --> 00:19:44,559
namely that's the fully qualified domain

00:19:43,840 --> 00:19:46,400
name

00:19:44,559 --> 00:19:48,240
of the service on the target cluster

00:19:46,400 --> 00:19:49,760
because it might be different

00:19:48,240 --> 00:19:52,400
um you know you might have different

00:19:49,760 --> 00:19:56,000
cluster domains for example so

00:19:52,400 --> 00:19:59,280
when um that happens it's also going to

00:19:56,000 --> 00:20:02,640
return you uh the expected identity that

00:19:59,280 --> 00:20:04,799
um for the gateway so you need to

00:20:02,640 --> 00:20:07,039
hit the gateway endpoint on the other

00:20:04,799 --> 00:20:07,760
side with the right tls identity in

00:20:07,039 --> 00:20:10,960
order to

00:20:07,760 --> 00:20:13,760
in order to establish uh mtls

00:20:10,960 --> 00:20:14,240
so once that happens this request the

00:20:13,760 --> 00:20:16,400
uri

00:20:14,240 --> 00:20:17,440
the proxy rewrites the uri authority

00:20:16,400 --> 00:20:21,280
with the correct

00:20:17,440 --> 00:20:22,880
port and um and and fully qualified

00:20:21,280 --> 00:20:26,159
domain name

00:20:22,880 --> 00:20:30,000
and when this request leaves the pot

00:20:26,159 --> 00:20:33,919
or the the cluster in fact it hits

00:20:30,000 --> 00:20:34,799
the external ip of the gateway on the

00:20:33,919 --> 00:20:37,600
other

00:20:34,799 --> 00:20:38,000
on the other cluster on cluster west and

00:20:37,600 --> 00:20:41,919
this

00:20:38,000 --> 00:20:44,559
gateway now knows which port

00:20:41,919 --> 00:20:47,360
because the uri authority is rewritten

00:20:44,559 --> 00:20:50,960
this gateway knows which port and which

00:20:47,360 --> 00:20:54,080
service this request is destined for

00:20:50,960 --> 00:20:55,520
so it sends it there it roots it to the

00:20:54,080 --> 00:20:58,640
correct service

00:20:55,520 --> 00:21:01,760
and it's important to um

00:20:58,640 --> 00:21:02,320
because to understand that because there

00:21:01,760 --> 00:21:04,799
is

00:21:02,320 --> 00:21:06,400
there are proxies running in each box

00:21:04,799 --> 00:21:07,120
that's involved in this communication

00:21:06,400 --> 00:21:09,120
pattern

00:21:07,120 --> 00:21:10,480
uh all of this traffic is encrypted even

00:21:09,120 --> 00:21:13,840
traffic that's

00:21:10,480 --> 00:21:16,080
that that's going across the clusters

00:21:13,840 --> 00:21:18,720
so this is how it pretty much works

00:21:16,080 --> 00:21:18,720
under the hood

00:21:18,960 --> 00:21:22,799
and um some of the future work that

00:21:21,520 --> 00:21:25,520
we're planning to do

00:21:22,799 --> 00:21:26,240
on this particular feature is that we've

00:21:25,520 --> 00:21:30,320
realized

00:21:26,240 --> 00:21:32,080
that the service mirror controller

00:21:30,320 --> 00:21:34,559
uh because it can monitor multiple

00:21:32,080 --> 00:21:37,679
clusters can get a bit complicated

00:21:34,559 --> 00:21:40,559
so um what we decided to do is to split

00:21:37,679 --> 00:21:42,080
this controller and have one controller

00:21:40,559 --> 00:21:44,320
for target cluster

00:21:42,080 --> 00:21:45,200
and hopefully that makes uh things a bit

00:21:44,320 --> 00:21:47,919
easier

00:21:45,200 --> 00:21:49,840
for debugging and reduces the cognitive

00:21:47,919 --> 00:21:52,320
luggage that one needs to carry when

00:21:49,840 --> 00:21:54,559
dealing with multiple clusters

00:21:52,320 --> 00:21:55,919
we are also going to introduce a crd to

00:21:54,559 --> 00:21:58,080
better represent

00:21:55,919 --> 00:21:59,440
the target cluster information because

00:21:58,080 --> 00:22:01,200
currently

00:21:59,440 --> 00:22:03,360
it's represented through a bunch of

00:22:01,200 --> 00:22:05,760
annotations on both services

00:22:03,360 --> 00:22:08,960
and kubernetes secrets now we're going

00:22:05,760 --> 00:22:10,640
to consolidate that and use the crd

00:22:08,960 --> 00:22:12,559
we are also going to support traffic

00:22:10,640 --> 00:22:13,840
policy and final grain permission

00:22:12,559 --> 00:22:15,679
control

00:22:13,840 --> 00:22:17,120
and uh one thing that we are currently

00:22:15,679 --> 00:22:19,120
working on is support

00:22:17,120 --> 00:22:20,240
for pcp traffic so currently that

00:22:19,120 --> 00:22:23,440
solution works with

00:22:20,240 --> 00:22:27,280
um http traffic and we want

00:22:23,440 --> 00:22:30,320
to be able to support um

00:22:27,280 --> 00:22:31,919
justice arbitration speed traffic so we

00:22:30,320 --> 00:22:33,760
that's part of a bigger pile of work

00:22:31,919 --> 00:22:35,520
that we are currently um that we're

00:22:33,760 --> 00:22:36,159
currently focusing on but we have a

00:22:35,520 --> 00:22:37,280
clear

00:22:36,159 --> 00:22:39,440
idea of how that's going to be

00:22:37,280 --> 00:22:42,240
accomplished so

00:22:39,440 --> 00:22:42,960
it's just a matter of time so with that

00:22:42,240 --> 00:22:46,080
being

00:22:42,960 --> 00:22:49,679
with all that being said um now there is

00:22:46,080 --> 00:22:53,760
some time for q a uh there is

00:22:49,679 --> 00:22:56,000
also a link here that

00:22:53,760 --> 00:22:58,159
a github repo where you can find this

00:22:56,000 --> 00:22:59,440
talk the slides for this talk with all

00:22:58,159 --> 00:23:02,960
the resources

00:22:59,440 --> 00:23:09,840
that um you can use in order to

00:23:02,960 --> 00:23:09,840
experiment with all that yourself so

00:23:11,679 --> 00:23:16,159
so hi there i'm happy to answer any

00:23:14,000 --> 00:23:19,200
questions

00:23:16,159 --> 00:23:21,440
uh yeah hey zahari you can you can take

00:23:19,200 --> 00:23:24,159
a look at the q a zone

00:23:21,440 --> 00:23:25,600
in zoom and then just take a pick up

00:23:24,159 --> 00:23:29,440
whatever questions you want

00:23:25,600 --> 00:23:32,080
thanks yeah sure um

00:23:29,440 --> 00:23:34,159
so the first question is is does

00:23:32,080 --> 00:23:37,280
multi-cluster support grpc

00:23:34,159 --> 00:23:41,679
and yes it does it's it does support

00:23:37,280 --> 00:23:41,679
your pc so you can use jpc across

00:23:48,840 --> 00:23:51,840
clusters

00:24:01,840 --> 00:24:05,840
uh we have three questions right now can

00:24:04,960 --> 00:24:09,360
you see them all

00:24:05,840 --> 00:24:12,000
or um i can see two questions

00:24:09,360 --> 00:24:13,120
okay yeah just go ahead so yeah the

00:24:12,000 --> 00:24:15,679
other question is

00:24:13,120 --> 00:24:18,000
istio has been very popular how did

00:24:15,679 --> 00:24:21,039
others typically choose to use

00:24:18,000 --> 00:24:24,320
linkedin instead of seo well

00:24:21,039 --> 00:24:27,760
like i'm not particularly

00:24:24,320 --> 00:24:29,200
yeah i i don't really know how why other

00:24:27,760 --> 00:24:32,960
people are considering

00:24:29,200 --> 00:24:34,880
linker d instead of ice steel um

00:24:32,960 --> 00:24:36,720
or how they are using it differently

00:24:34,880 --> 00:24:38,640
because

00:24:36,720 --> 00:24:40,320
yeah i don't really directly work with a

00:24:38,640 --> 00:24:43,760
lot of people but

00:24:40,320 --> 00:24:46,960
um i would say lingerie is quite simple

00:24:43,760 --> 00:24:48,880
and it's in its ux and getting things

00:24:46,960 --> 00:24:50,799
off the ground is really simple so maybe

00:24:48,880 --> 00:25:01,840
that's something that

00:24:50,799 --> 00:25:01,840
is kind of attracting people

00:25:06,400 --> 00:25:10,159
so yeah there's another question that

00:25:08,240 --> 00:25:10,960
the data plane of linkedin is written in

00:25:10,159 --> 00:25:14,240
rust

00:25:10,960 --> 00:25:15,279
with all the benefits of it but could

00:25:14,240 --> 00:25:18,480
this be a reason

00:25:15,279 --> 00:25:21,919
that people who don't have internal rust

00:25:18,480 --> 00:25:22,240
developers avoid linker d i i don't

00:25:21,919 --> 00:25:24,400
think

00:25:22,240 --> 00:25:25,840
that this could actually be a reason

00:25:24,400 --> 00:25:27,120
because or

00:25:25,840 --> 00:25:29,120
rather i don't think it should be a

00:25:27,120 --> 00:25:32,240
reason because the proxy is

00:25:29,120 --> 00:25:33,279
fairly transparent so and because linker

00:25:32,240 --> 00:25:35,600
is

00:25:33,279 --> 00:25:36,320
like very configuration light you don't

00:25:35,600 --> 00:25:37,679
really need

00:25:36,320 --> 00:25:39,440
to know a lot about the internal

00:25:37,679 --> 00:25:42,720
workings of the proxy

00:25:39,440 --> 00:25:43,840
or um you know all rust for that matter

00:25:42,720 --> 00:25:46,159
in order to be

00:25:43,840 --> 00:25:48,559
using clinker d to its full potential so

00:25:46,159 --> 00:25:57,840
yeah i don't i don't think that's

00:25:48,559 --> 00:25:57,840
that should be a decisive factor

00:25:58,159 --> 00:26:04,320
um if we update link d version

00:26:01,520 --> 00:26:06,400
in multi cluster will it cause any

00:26:04,320 --> 00:26:09,520
downtime to current workload

00:26:06,400 --> 00:26:11,440
um no i don't think it should

00:26:09,520 --> 00:26:13,120
um technically i don't think it should

00:26:11,440 --> 00:26:15,360
it it's all about

00:26:13,120 --> 00:26:16,320
you know it's all about service proxies

00:26:15,360 --> 00:26:18,000
so the great

00:26:16,320 --> 00:26:19,600
path at the moment is something that we

00:26:18,000 --> 00:26:21,919
are actually working on because

00:26:19,600 --> 00:26:23,520
there are some other changes that we are

00:26:21,919 --> 00:26:25,760
incorporating in

00:26:23,520 --> 00:26:26,720
in the link or demulti cluster solution

00:26:25,760 --> 00:26:28,559
but

00:26:26,720 --> 00:26:31,600
i think at the end of the day it should

00:26:28,559 --> 00:26:31,600
be quite quite

00:26:36,840 --> 00:26:39,840
smooth

00:26:46,400 --> 00:26:49,760
what's the most advantage for linkrd

00:26:48,159 --> 00:26:52,960
compared to istio

00:26:49,760 --> 00:26:54,559
i don't think i have the user experience

00:26:52,960 --> 00:26:58,080
to answer that question

00:26:54,559 --> 00:27:01,360
and i think that could be different for

00:26:58,080 --> 00:27:02,000
for different users so like i i can't

00:27:01,360 --> 00:27:07,840
really

00:27:02,000 --> 00:27:07,840
i can't really comment on that

00:27:16,559 --> 00:27:23,520
okay really great talk

00:27:20,000 --> 00:27:28,240
from zahari thank you thank you

00:27:23,520 --> 00:27:28,240

YouTube URL: https://www.youtube.com/watch?v=QXQJ5S1OAjI


