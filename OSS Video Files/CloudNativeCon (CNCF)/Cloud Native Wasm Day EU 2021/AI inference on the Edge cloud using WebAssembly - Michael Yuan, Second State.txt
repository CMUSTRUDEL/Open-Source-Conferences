Title: AI inference on the Edge cloud using WebAssembly - Michael Yuan, Second State
Publication date: 2021-05-04
Playlist: Cloud Native Wasm Day EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

AI inference on the Edge cloud using WebAssembly - Michael Yuan, Second State

Michael Yuan will discuss options to run Tensorflow model inference on WebAssembly. Using a Mobilenet image classification task as an example, he will cover the plain JS approach, Tensorflow.js, pure Rust libraries for Tensorflow, and WASI-like Tensorflow extensions that run on specialized inference chips. The session will go through the journey of 60,000x performance gain over the evolution of WebAssembly. He will also discuss what’s the future for WebAssembly-based AI on the edge cloud.
Captions: 
	00:00:00,080 --> 00:00:05,120
hello everyone um welcome to the cloud

00:00:03,120 --> 00:00:07,279
native webassembly day

00:00:05,120 --> 00:00:08,480
um in this talk we're gonna talk about

00:00:07,279 --> 00:00:09,760
ai inference

00:00:08,480 --> 00:00:11,840
you know how to do artificial

00:00:09,760 --> 00:00:14,480
intelligence with web assembly

00:00:11,840 --> 00:00:15,679
and where do we use it it's probably um

00:00:14,480 --> 00:00:17,520
on the edge of the cloud

00:00:15,679 --> 00:00:18,720
you know so we've got ai inference on

00:00:17,520 --> 00:00:20,880
the edge cloud

00:00:18,720 --> 00:00:23,039
so my name is michael yang and i'm from

00:00:20,880 --> 00:00:26,240
a company called second state

00:00:23,039 --> 00:00:28,240
um so this talk is about how webassembly

00:00:26,240 --> 00:00:30,720
make it easy to run tensorflow inference

00:00:28,240 --> 00:00:31,760
functions in production on edge devices

00:00:30,720 --> 00:00:35,840
and platforms

00:00:31,760 --> 00:00:38,320
okay well so um

00:00:35,840 --> 00:00:39,840
before we start getting to the the

00:00:38,320 --> 00:00:41,440
details of webassembly

00:00:39,840 --> 00:00:43,520
let's talk let's spend a couple minutes

00:00:41,440 --> 00:00:44,719
to talk about ai you know that's deep

00:00:43,520 --> 00:00:47,760
learning and ais

00:00:44,719 --> 00:00:50,480
hot topics today right so um when people

00:00:47,760 --> 00:00:50,960
think about ai people think about the

00:00:50,480 --> 00:00:52,559
data

00:00:50,960 --> 00:00:54,559
the algorithms and all that you know

00:00:52,559 --> 00:00:55,920
that's how everyone loves to train the

00:00:54,559 --> 00:00:59,039
greatest ai

00:00:55,920 --> 00:00:59,680
however you know um after years of

00:00:59,039 --> 00:01:02,559
development

00:00:59,680 --> 00:01:04,400
with deep learning i think um you know

00:01:02,559 --> 00:01:06,560
most people in the industry has realized

00:01:04,400 --> 00:01:08,880
that training the ai is not

00:01:06,560 --> 00:01:10,320
the difficult part now you know it's

00:01:08,880 --> 00:01:11,040
it's pretty much a low code work you

00:01:10,320 --> 00:01:13,280
know you don't even

00:01:11,040 --> 00:01:14,960
need to be a programmer you just need to

00:01:13,280 --> 00:01:15,840
know how to run a python script right

00:01:14,960 --> 00:01:17,680
you know how to

00:01:15,840 --> 00:01:19,119
collect those state collect those data

00:01:17,680 --> 00:01:21,040
and how to um

00:01:19,119 --> 00:01:22,560
label those data and then feed that into

00:01:21,040 --> 00:01:24,799
a computer algorithm

00:01:22,560 --> 00:01:26,640
then you get your um you know tensorflow

00:01:24,799 --> 00:01:28,560
model or you know onex model or whatever

00:01:26,640 --> 00:01:30,640
right

00:01:28,560 --> 00:01:32,880
however most of the work is actually

00:01:30,640 --> 00:01:35,119
most of the engineering workouts

00:01:32,880 --> 00:01:36,720
at least is actually on the inference

00:01:35,119 --> 00:01:38,880
side is to

00:01:36,720 --> 00:01:40,000
when you have a ai model it's that's

00:01:38,880 --> 00:01:41,759
that's trained

00:01:40,000 --> 00:01:44,240
and when you use it in your production

00:01:41,759 --> 00:01:46,320
system to say to recognize

00:01:44,240 --> 00:01:47,280
part of speech or recognize image

00:01:46,320 --> 00:01:48,799
recognize the face

00:01:47,280 --> 00:01:50,479
you know whatever you train the the

00:01:48,799 --> 00:01:53,200
algorithm for

00:01:50,479 --> 00:01:55,360
you know that's is the bulk of the

00:01:53,200 --> 00:01:58,159
consumer part of the computing part

00:01:55,360 --> 00:01:59,680
that's associated with ai today so if

00:01:58,159 --> 00:02:02,799
you look at this graph you know that's

00:01:59,680 --> 00:02:03,759
estimated from 2019 and the training

00:02:02,799 --> 00:02:05,280
only takes like

00:02:03,759 --> 00:02:07,439
five percent of the computing pattern

00:02:05,280 --> 00:02:10,720
the influence is 95

00:02:07,439 --> 00:02:12,239
so in order to deploy your ai system or

00:02:10,720 --> 00:02:14,800
deep learning system in production

00:02:12,239 --> 00:02:15,520
and to do it efficiently we have to

00:02:14,800 --> 00:02:19,920
optimize

00:02:15,520 --> 00:02:19,920
for that 95 percent the influence part

00:02:21,040 --> 00:02:25,200
however there's um inference is very

00:02:24,000 --> 00:02:26,800
important it's 95

00:02:25,200 --> 00:02:29,200
of the work but it's also difficult to

00:02:26,800 --> 00:02:30,160
optimize and if you look at this graph

00:02:29,200 --> 00:02:32,560
you know there's uh

00:02:30,160 --> 00:02:33,280
each dot each of the blue dots

00:02:32,560 --> 00:02:36,400
represents

00:02:33,280 --> 00:02:38,000
a trained um ai model for image

00:02:36,400 --> 00:02:41,280
recognition

00:02:38,000 --> 00:02:43,760
and on the xxl it's the time it's

00:02:41,280 --> 00:02:44,560
required to run this model and on the

00:02:43,760 --> 00:02:48,239
y-axis

00:02:44,560 --> 00:02:51,760
is the is the accuracy of this model

00:02:48,239 --> 00:02:55,200
so you can see the accuracy goes from 70

00:02:51,760 --> 00:02:56,720
to 85 it's about 20 improvements for the

00:02:55,200 --> 00:03:00,159
foot accuracy

00:02:56,720 --> 00:03:02,239
but the time needed to run ranges from

00:03:00,159 --> 00:03:03,360
a couple milliseconds to over 30

00:03:02,239 --> 00:03:06,720
milliseconds

00:03:03,360 --> 00:03:09,599
so in order to get 20

00:03:06,720 --> 00:03:10,560
improvements in accuracy the model has

00:03:09,599 --> 00:03:13,120
to run

00:03:10,560 --> 00:03:14,640
seven times eight times as long so that

00:03:13,120 --> 00:03:16,000
means it's difficult to scale those

00:03:14,640 --> 00:03:16,800
models right you know so when you have

00:03:16,000 --> 00:03:18,640
those models

00:03:16,800 --> 00:03:20,159
um if you have a slightly more accurate

00:03:18,640 --> 00:03:20,800
model you need a lot more computing

00:03:20,159 --> 00:03:23,280
power

00:03:20,800 --> 00:03:24,239
so to save computing power to make the

00:03:23,280 --> 00:03:27,200
models run as

00:03:24,239 --> 00:03:28,720
efficient as possible becomes a major

00:03:27,200 --> 00:03:29,360
issue in deploying those models in

00:03:28,720 --> 00:03:30,879
production

00:03:29,360 --> 00:03:32,959
you know when you need to deploy those

00:03:30,879 --> 00:03:34,959
models in production you really do need

00:03:32,959 --> 00:03:36,480
to pay attention how efficient they are

00:03:34,959 --> 00:03:38,640
how efficient you can make them wrong

00:03:36,480 --> 00:03:38,640
right

00:03:38,879 --> 00:03:43,440
so you know that's um so let's look into

00:03:41,920 --> 00:03:46,239
several ways that's how people

00:03:43,440 --> 00:03:47,840
um you know run their tensorflow models

00:03:46,239 --> 00:03:51,040
for inference

00:03:47,840 --> 00:03:53,360
so here

00:03:51,040 --> 00:03:54,720
so the title of this slide is why not

00:03:53,360 --> 00:03:58,080
python right you know

00:03:54,720 --> 00:03:59,840
so the official tensorflow serving is a

00:03:58,080 --> 00:04:02,000
application the python application

00:03:59,840 --> 00:04:03,599
because python is easy to use a lot of

00:04:02,000 --> 00:04:04,720
people use it to train the model so it

00:04:03,599 --> 00:04:06,640
certainly stands to reason

00:04:04,720 --> 00:04:08,319
people use it for inference as well once

00:04:06,640 --> 00:04:10,720
you train the model in python you can

00:04:08,319 --> 00:04:12,799
see the modeling action in python then

00:04:10,720 --> 00:04:14,239
perhaps you can you can use it in

00:04:12,799 --> 00:04:16,799
production as well

00:04:14,239 --> 00:04:17,600
however python as a language is very

00:04:16,799 --> 00:04:19,120
inefficient

00:04:17,600 --> 00:04:21,359
you know there has been a study that's

00:04:19,120 --> 00:04:23,680
you know there's um it's a famous study

00:04:21,359 --> 00:04:26,080
that published on the journal science

00:04:23,680 --> 00:04:28,240
it's um you know the authors took a

00:04:26,080 --> 00:04:30,400
machine learning algorithm and the

00:04:28,240 --> 00:04:33,680
implemented in python and then

00:04:30,400 --> 00:04:36,080
c the c version is

00:04:33,680 --> 00:04:37,919
sixty thousand times faster independence

00:04:36,080 --> 00:04:40,240
it's four orders of magnitude just by

00:04:37,919 --> 00:04:42,479
switching to a different language

00:04:40,240 --> 00:04:43,600
as a result you know when you use when

00:04:42,479 --> 00:04:45,600
you say i'm

00:04:43,600 --> 00:04:47,600
doing machine learning or tensorflow in

00:04:45,600 --> 00:04:49,360
python it's actually

00:04:47,600 --> 00:04:51,520
it doesn't actually do both of the box

00:04:49,360 --> 00:04:53,680
of work in python you know it's uh

00:04:51,520 --> 00:04:55,600
under the hood the python functions

00:04:53,680 --> 00:04:57,759
actually cause the c libraries

00:04:55,600 --> 00:04:59,759
this tensorflow library is written in c

00:04:57,759 --> 00:05:02,400
right so the the workload is actually

00:04:59,759 --> 00:05:05,039
done in native code reading c

00:05:02,400 --> 00:05:05,919
so you know that's that's sort of you

00:05:05,039 --> 00:05:07,759
know how people get

00:05:05,919 --> 00:05:09,680
uh reasonable performance autopilot you

00:05:07,759 --> 00:05:10,080
know python is mostly just the interface

00:05:09,680 --> 00:05:12,160
language

00:05:10,080 --> 00:05:14,240
you know that's uh makes the system easy

00:05:12,160 --> 00:05:18,000
to program and easy to use by

00:05:14,240 --> 00:05:19,039
um you know by the developers right you

00:05:18,000 --> 00:05:21,039
know that's uh

00:05:19,039 --> 00:05:23,520
and then when you really need heavy

00:05:21,039 --> 00:05:26,320
lifting when you really need computation

00:05:23,520 --> 00:05:27,680
you um goes down to the c libraries it

00:05:26,320 --> 00:05:30,240
goes down to the native library

00:05:27,680 --> 00:05:32,160
tensorflow libraries reading c right

00:05:30,240 --> 00:05:33,520
however even then

00:05:32,160 --> 00:05:35,199
the data pre-processing and

00:05:33,520 --> 00:05:36,160
post-processing are still written in

00:05:35,199 --> 00:05:38,479
python

00:05:36,160 --> 00:05:39,680
right you know because when you read say

00:05:38,479 --> 00:05:41,840
if you have a

00:05:39,680 --> 00:05:43,120
you know image recognition model that's

00:05:41,840 --> 00:05:46,080
you have to read the image

00:05:43,120 --> 00:05:47,759
and resize it and flatten it to make it

00:05:46,080 --> 00:05:48,479
appear to be the size of the tensor you

00:05:47,759 --> 00:05:50,080
want

00:05:48,479 --> 00:05:52,000
and then feed that to the tensorflow

00:05:50,080 --> 00:05:54,320
model and then to get the the result

00:05:52,000 --> 00:05:55,280
from the model and then map the model

00:05:54,320 --> 00:05:57,600
result back to

00:05:55,280 --> 00:05:58,960
a metadata file or label file to figure

00:05:57,600 --> 00:06:00,240
out what's the english name for the

00:05:58,960 --> 00:06:02,639
objects that are

00:06:00,240 --> 00:06:05,120
that's on that image right so that

00:06:02,639 --> 00:06:05,440
process only 10 of the work is actually

00:06:05,120 --> 00:06:08,000
done

00:06:05,440 --> 00:06:10,160
running the tensorflow model itself in

00:06:08,000 --> 00:06:12,080
negative code or in c library and the

00:06:10,160 --> 00:06:13,039
most of the data pre-processing and post

00:06:12,080 --> 00:06:15,120
processing

00:06:13,039 --> 00:06:17,199
are still written in python you know if

00:06:15,120 --> 00:06:17,759
you if you are you if you are writing

00:06:17,199 --> 00:06:19,520
this

00:06:17,759 --> 00:06:22,160
entire application in python so ninety

00:06:19,520 --> 00:06:24,240
percent will work is slow and can be

00:06:22,160 --> 00:06:25,600
uh heavily optimized as we can see you

00:06:24,240 --> 00:06:26,479
know there's orders there's other

00:06:25,600 --> 00:06:28,479
magnitudes

00:06:26,479 --> 00:06:30,400
or performance scan you can achieve by

00:06:28,479 --> 00:06:32,240
just using a different language

00:06:30,400 --> 00:06:33,600
and of course there's other issues when

00:06:32,240 --> 00:06:35,440
you know when we talk about edge

00:06:33,600 --> 00:06:38,479
computing when you know because

00:06:35,440 --> 00:06:40,880
a very large use case of ai

00:06:38,479 --> 00:06:42,319
is on the edge right you know so you say

00:06:40,880 --> 00:06:44,479
if you have a door camera

00:06:42,319 --> 00:06:46,800
and you have uh you have trained it to

00:06:44,479 --> 00:06:47,759
recognize all the people in your in your

00:06:46,800 --> 00:06:50,560
household and

00:06:47,759 --> 00:06:51,840
so it would uh uh want to see a new

00:06:50,560 --> 00:06:52,400
person that's standing in front of your

00:06:51,840 --> 00:06:54,160
door

00:06:52,400 --> 00:06:56,639
it would be able to tell whether it's uh

00:06:54,160 --> 00:06:58,000
it's a person in the household or it's

00:06:56,639 --> 00:06:59,840
somewhere else right you know you can

00:06:58,000 --> 00:07:02,639
unlock the door or you know do

00:06:59,840 --> 00:07:04,880
you can do things like that and uh for

00:07:02,639 --> 00:07:06,639
things like that you want it to run

00:07:04,880 --> 00:07:08,400
to you you want the ai model to run

00:07:06,639 --> 00:07:09,759
either on the camera or in the house

00:07:08,400 --> 00:07:12,080
on the router or in the server in the

00:07:09,759 --> 00:07:13,599
house you know something of that nature

00:07:12,080 --> 00:07:16,080
you don't want it to go all the way back

00:07:13,599 --> 00:07:18,880
to the internet and not only because

00:07:16,080 --> 00:07:19,199
the performance would be um you know to

00:07:18,880 --> 00:07:21,039
run

00:07:19,199 --> 00:07:22,319
on the remote server say in new york

00:07:21,039 --> 00:07:23,599
right you know that would be

00:07:22,319 --> 00:07:25,840
um you know you would see a big

00:07:23,599 --> 00:07:27,520
performance impact but also because you

00:07:25,840 --> 00:07:28,080
don't want the data to leave your house

00:07:27,520 --> 00:07:30,560
because

00:07:28,080 --> 00:07:31,280
for privacy reasons and of course you

00:07:30,560 --> 00:07:34,000
know another

00:07:31,280 --> 00:07:35,039
you know very important uh use case is

00:07:34,000 --> 00:07:37,360
self-driving cars

00:07:35,039 --> 00:07:38,160
right uh you have cameras and all that

00:07:37,360 --> 00:07:40,400
and you need

00:07:38,160 --> 00:07:41,199
those those data to be processed in real

00:07:40,400 --> 00:07:43,039
time

00:07:41,199 --> 00:07:44,639
on the edge inside of the car not go

00:07:43,039 --> 00:07:46,479
back to somewhere not not

00:07:44,639 --> 00:07:48,479
not go to the cloud and process and then

00:07:46,479 --> 00:07:50,800
return to the internet

00:07:48,479 --> 00:07:52,400
so in in many of those cases python

00:07:50,800 --> 00:07:54,080
doesn't run any of those devices you

00:07:52,400 --> 00:07:55,840
know it doesn't run very small devices

00:07:54,080 --> 00:07:58,000
it doesn't run a very old linux

00:07:55,840 --> 00:07:59,520
operating system it doesn't run on

00:07:58,000 --> 00:08:01,199
real-time operating system you know so

00:07:59,520 --> 00:08:03,039
there are lots of issues

00:08:01,199 --> 00:08:04,879
and python of course offers limited

00:08:03,039 --> 00:08:07,759
support in web and

00:08:04,879 --> 00:08:08,080
um in web and application frameworks you

00:08:07,759 --> 00:08:10,000
know

00:08:08,080 --> 00:08:12,000
there's um you know there's only certain

00:08:10,000 --> 00:08:14,560
frameworks that works with python and

00:08:12,000 --> 00:08:15,360
there's many others that's that's um you

00:08:14,560 --> 00:08:17,199
know

00:08:15,360 --> 00:08:18,879
for instance in node.js it would be

00:08:17,199 --> 00:08:19,520
difficult to call python program from

00:08:18,879 --> 00:08:22,560
node.js

00:08:19,520 --> 00:08:24,080
it's possible but it's not as easy so

00:08:22,560 --> 00:08:26,879
the little table i have here

00:08:24,080 --> 00:08:28,400
shows the performance characteristics of

00:08:26,879 --> 00:08:30,240
different programming languages

00:08:28,400 --> 00:08:31,680
and you can see c plus plus but of

00:08:30,240 --> 00:08:33,440
course by far is the best

00:08:31,680 --> 00:08:35,279
the ross will come to a very close

00:08:33,440 --> 00:08:37,279
second in java

00:08:35,279 --> 00:08:39,360
you know it runs fast enough but it

00:08:37,279 --> 00:08:42,240
consumes a huge amount of memory

00:08:39,360 --> 00:08:42,640
100 times more memory than everyone else

00:08:42,240 --> 00:08:45,200
and

00:08:42,640 --> 00:08:47,040
for python it runs very slow it's 100

00:08:45,200 --> 00:08:48,720
times slower than everyone else

00:08:47,040 --> 00:08:50,080
and consumer reasonable amount of memory

00:08:48,720 --> 00:08:51,760
so that's the performance

00:08:50,080 --> 00:08:54,880
characteristics of python

00:08:51,760 --> 00:08:58,240
make it make it suitable for machine

00:08:54,880 --> 00:09:00,640
for ai training but not suitable for

00:08:58,240 --> 00:09:03,200
using ai info using air influence in

00:09:00,640 --> 00:09:03,200
production

00:09:04,000 --> 00:09:08,240
so now we have established cc plus has

00:09:06,240 --> 00:09:10,399
the best performance and python

00:09:08,240 --> 00:09:11,839
may not right so what about native

00:09:10,399 --> 00:09:14,160
applications why don't we just

00:09:11,839 --> 00:09:15,040
write all our applications in cnc class

00:09:14,160 --> 00:09:18,080
stars

00:09:15,040 --> 00:09:21,279
um you know or at least the ai inference

00:09:18,080 --> 00:09:23,600
functions c and c c plus plus right

00:09:21,279 --> 00:09:24,720
you know that's uh um it's like the

00:09:23,600 --> 00:09:26,720
tensorflow's

00:09:24,720 --> 00:09:28,640
library is already written in c so you

00:09:26,720 --> 00:09:30,640
know why can't we write our application

00:09:28,640 --> 00:09:33,680
around it in cs one

00:09:30,640 --> 00:09:35,600
um but to do so we basically give up all

00:09:33,680 --> 00:09:37,360
advances that we made in the past

00:09:35,600 --> 00:09:38,640
40 years of computer engineering

00:09:37,360 --> 00:09:41,040
software engineering

00:09:38,640 --> 00:09:42,800
which is the goal to make software safer

00:09:41,040 --> 00:09:44,480
to make software more secure right and

00:09:42,800 --> 00:09:46,080
to make sure to make it easier to make

00:09:44,480 --> 00:09:48,720
it easier for developers

00:09:46,080 --> 00:09:50,399
and to make it easier for users as well

00:09:48,720 --> 00:09:52,640
so you know um

00:09:50,399 --> 00:09:54,320
specifically if you if we are writing c

00:09:52,640 --> 00:09:54,720
applications in native code and then

00:09:54,320 --> 00:09:57,279
wrong

00:09:54,720 --> 00:09:59,519
ai inference we have the problem of the

00:09:57,279 --> 00:10:00,640
the software being tied to a specific os

00:09:59,519 --> 00:10:03,360
or hardware platforms

00:10:00,640 --> 00:10:03,839
not portable anymore and you can't um

00:10:03,360 --> 00:10:05,680
you know

00:10:03,839 --> 00:10:07,839
that's especially a problem for each

00:10:05,680 --> 00:10:09,839
computing where you have

00:10:07,839 --> 00:10:11,279
um you know you may have a variety of

00:10:09,839 --> 00:10:13,760
different cpus and

00:10:11,279 --> 00:10:14,320
uh um and operating systems on different

00:10:13,760 --> 00:10:18,000
devices

00:10:14,320 --> 00:10:19,279
right and it cannot be managed and

00:10:18,000 --> 00:10:20,720
orchestrated because it's a native

00:10:19,279 --> 00:10:21,360
application just to start it and it

00:10:20,720 --> 00:10:23,519
could crash

00:10:21,360 --> 00:10:24,399
and it cannot be managed and stopped you

00:10:23,519 --> 00:10:27,600
know that's uh

00:10:24,399 --> 00:10:29,600
it's difficult to do that and it's it's

00:10:27,600 --> 00:10:30,079
not safe it's crashes from bugs or

00:10:29,600 --> 00:10:32,800
attacks

00:10:30,079 --> 00:10:33,519
you know that's uh um it could uh um you

00:10:32,800 --> 00:10:35,360
know you could

00:10:33,519 --> 00:10:37,760
make a mistake and it crashed the whole

00:10:35,360 --> 00:10:40,160
system right it's costly grain

00:10:37,760 --> 00:10:42,079
there's no um you know the access

00:10:40,160 --> 00:10:42,800
control is nice the operating system

00:10:42,079 --> 00:10:45,920
level

00:10:42,800 --> 00:10:46,880
you know so it's uh whoever the user the

00:10:45,920 --> 00:10:49,279
the you

00:10:46,880 --> 00:10:50,880
the the user who runs the application

00:10:49,279 --> 00:10:52,720
with the the application will have the

00:10:50,880 --> 00:10:53,839
same privilege and permission as a user

00:10:52,720 --> 00:10:56,000
who runs it

00:10:53,839 --> 00:10:57,440
right you know so if you have root user

00:10:56,000 --> 00:10:59,760
response application you

00:10:57,440 --> 00:11:01,839
have access to the entire system and

00:10:59,760 --> 00:11:03,600
which is generally a bad thing

00:11:01,839 --> 00:11:05,279
so the native applications there's a lot

00:11:03,600 --> 00:11:05,839
of issues with native applications

00:11:05,279 --> 00:11:08,240
that's why

00:11:05,839 --> 00:11:10,000
you know uh 20 years ago java gained

00:11:08,240 --> 00:11:12,240
popularity because of jvm

00:11:10,000 --> 00:11:14,079
and after that you know docker gained

00:11:12,240 --> 00:11:16,079
popularity as well you know that's uh

00:11:14,079 --> 00:11:17,360
because it provides isolated around time

00:11:16,079 --> 00:11:19,760
and portable around that

00:11:17,360 --> 00:11:20,640
for developers to run their applications

00:11:19,760 --> 00:11:23,200
so we don't

00:11:20,640 --> 00:11:24,000
want um just native applications we want

00:11:23,200 --> 00:11:26,160
something

00:11:24,000 --> 00:11:27,519
that is that is more modern but also

00:11:26,160 --> 00:11:30,640
have high performance not slow

00:11:27,519 --> 00:11:32,240
performance like python right

00:11:30,640 --> 00:11:33,920
so whereby somebody to the rescue you

00:11:32,240 --> 00:11:35,600
know that's uh um so

00:11:33,920 --> 00:11:37,279
we have two conflicting goals you know

00:11:35,600 --> 00:11:38,240
that's one is to have super high

00:11:37,279 --> 00:11:41,440
performance

00:11:38,240 --> 00:11:44,720
and the second is um to have

00:11:41,440 --> 00:11:46,079
security safety and the ease ease of use

00:11:44,720 --> 00:11:47,519
and all those nice features that the

00:11:46,079 --> 00:11:49,839
container will give you

00:11:47,519 --> 00:11:51,200
so the best of both worlds the

00:11:49,839 --> 00:11:52,800
compromise can be reached

00:11:51,200 --> 00:11:54,639
through something called beverson

00:11:52,800 --> 00:11:56,720
survivor assembly that's why we're here

00:11:54,639 --> 00:11:58,959
to discuss you know a cognitive web

00:11:56,720 --> 00:12:00,959
summary right

00:11:58,959 --> 00:12:02,480
so so this particular project that we

00:12:00,959 --> 00:12:04,160
are discussing here is a new sandbox

00:12:02,480 --> 00:12:07,040
project that we have contributed

00:12:04,160 --> 00:12:08,959
to the cncf and it's called the wasm age

00:12:07,040 --> 00:12:11,519
and it's also called ssvm which is

00:12:08,959 --> 00:12:13,360
a production and second state of vm or

00:12:11,519 --> 00:12:17,360
wasm edge wherever somebody on the edge

00:12:13,360 --> 00:12:20,399
right so whether somebody has some

00:12:17,360 --> 00:12:23,040
really interesting advantages over you

00:12:20,399 --> 00:12:25,200
know over both

00:12:23,040 --> 00:12:26,560
fully managed application and also over

00:12:25,200 --> 00:12:28,320
native applications

00:12:26,560 --> 00:12:29,680
the first is high performance it's much

00:12:28,320 --> 00:12:32,720
more performant than say

00:12:29,680 --> 00:12:35,760
python or node.js or javascript

00:12:32,720 --> 00:12:38,560
or or even java in the jvm

00:12:35,760 --> 00:12:40,240
and this has sandbox safety it's a it's

00:12:38,560 --> 00:12:42,560
it's a sandbox

00:12:40,240 --> 00:12:44,160
that contains the the the native

00:12:42,560 --> 00:12:46,240
application so it has safety

00:12:44,160 --> 00:12:47,519
it's got capability-based security

00:12:46,240 --> 00:12:49,600
meaning that's

00:12:47,519 --> 00:12:51,120
um when you start a web representative

00:12:49,600 --> 00:12:53,920
vm you can

00:12:51,120 --> 00:12:55,360
give it you know a set of policies or

00:12:53,920 --> 00:12:57,040
rules

00:12:55,360 --> 00:12:59,040
what kind of system resources you can

00:12:57,040 --> 00:13:00,880
access so it's

00:12:59,040 --> 00:13:02,800
that allows the webassembly runtime to

00:13:00,880 --> 00:13:05,920
have a different set of security

00:13:02,800 --> 00:13:07,440
permissions than users who run it right

00:13:05,920 --> 00:13:09,440
and it's language agnostic

00:13:07,440 --> 00:13:10,480
in in a way so you know that's uh

00:13:09,440 --> 00:13:12,160
meaning that any

00:13:10,480 --> 00:13:14,880
front-end language and chemi program

00:13:12,160 --> 00:13:17,279
that can be compiled to lvm

00:13:14,880 --> 00:13:18,160
is going to run on webassembly although

00:13:17,279 --> 00:13:20,480
language that

00:13:18,160 --> 00:13:22,160
requires a large runtime become somewhat

00:13:20,480 --> 00:13:22,800
impractical to run on web assembly for

00:13:22,160 --> 00:13:24,480
instance

00:13:22,800 --> 00:13:26,880
you know it's difficult to get java or

00:13:24,480 --> 00:13:30,320
python running on it however for rust

00:13:26,880 --> 00:13:30,880
c swift tiny go and you know languages

00:13:30,320 --> 00:13:33,360
like that

00:13:30,880 --> 00:13:34,880
you know that's it's you can compile all

00:13:33,360 --> 00:13:38,000
those languages to wherever somebody

00:13:34,880 --> 00:13:40,399
and just run them in the virtual machine

00:13:38,000 --> 00:13:42,880
once a virtual machine and the last

00:13:40,399 --> 00:13:45,519
thing is we see a product community fit

00:13:42,880 --> 00:13:46,560
here you know meaning that um you know

00:13:45,519 --> 00:13:48,880
there's a large

00:13:46,560 --> 00:13:50,160
already a large community for for web

00:13:48,880 --> 00:13:51,120
assembly products

00:13:50,160 --> 00:13:52,880
and there are lots of people

00:13:51,120 --> 00:13:54,560
contributing which we thought it's a

00:13:52,880 --> 00:13:55,360
it's a crucial factor for success of

00:13:54,560 --> 00:13:58,320
technology

00:13:55,360 --> 00:14:00,000
right so um to sum it up you know from

00:13:58,320 --> 00:14:01,519
our point of view we see

00:14:00,000 --> 00:14:03,680
the raster programming language on the

00:14:01,519 --> 00:14:04,560
front end and web assembly on the on the

00:14:03,680 --> 00:14:06,720
back end

00:14:04,560 --> 00:14:07,839
roughly equivalent to the java

00:14:06,720 --> 00:14:11,360
programming language

00:14:07,839 --> 00:14:11,920
plus the jvm about 20 years ago so we

00:14:11,360 --> 00:14:14,000
believe

00:14:11,920 --> 00:14:17,920
maybe this thing can can be as important

00:14:14,000 --> 00:14:20,959
or as a group as big as java

00:14:17,920 --> 00:14:22,560
so this is um um

00:14:20,959 --> 00:14:24,000
the web assembly runtime that we have

00:14:22,560 --> 00:14:26,639
donated to to the

00:14:24,000 --> 00:14:28,560
cncf um you know if you're interested

00:14:26,639 --> 00:14:31,120
you know just

00:14:28,560 --> 00:14:32,079
like it on github or you know fork it or

00:14:31,120 --> 00:14:34,320
check out just

00:14:32,079 --> 00:14:35,440
generally check it out and see you know

00:14:34,320 --> 00:14:37,440
this is a

00:14:35,440 --> 00:14:38,800
a popular web assembly vm space

00:14:37,440 --> 00:14:40,240
specifically optimized for high

00:14:38,800 --> 00:14:42,240
performance applications

00:14:40,240 --> 00:14:43,839
outside of the browser so there's a

00:14:42,240 --> 00:14:45,839
bunch of web assembly

00:14:43,839 --> 00:14:47,920
vms that's designed for the broader use

00:14:45,839 --> 00:14:49,040
case but this is not this is designed

00:14:47,920 --> 00:14:51,199
for the server

00:14:49,040 --> 00:14:53,120
for the use case outside of the bar that

00:14:51,199 --> 00:14:55,839
means you know and on the server or

00:14:53,120 --> 00:14:57,440
just or in the cloud right and it's

00:14:55,839 --> 00:15:00,399
called wasmage you know it's web

00:14:57,440 --> 00:15:00,399
assembly on the edge

00:15:00,560 --> 00:15:04,560
so in the next couple of slides i want

00:15:02,399 --> 00:15:07,120
to talk um you know using welcome edge

00:15:04,560 --> 00:15:09,040
and ss vms examples to talk to you

00:15:07,120 --> 00:15:10,880
to to discuss whatever somebody's

00:15:09,040 --> 00:15:12,560
approaches to tensorflow inference how

00:15:10,880 --> 00:15:13,199
do you do tensorflow influencers well

00:15:12,560 --> 00:15:15,279
assembly

00:15:13,199 --> 00:15:17,199
we all know how to do it in cc plus plus

00:15:15,279 --> 00:15:19,040
we also know how to do it in python

00:15:17,199 --> 00:15:21,040
but how to do that in webassembly and

00:15:19,040 --> 00:15:23,839
what are the implications

00:15:21,040 --> 00:15:25,760
so the first easiest one to for

00:15:23,839 --> 00:15:28,800
developers to understand is to just

00:15:25,760 --> 00:15:31,040
run web assembly interpreted on the cpu

00:15:28,800 --> 00:15:33,360
you know so the tensorflow operators are

00:15:31,040 --> 00:15:35,519
implemented in rust or c plus plus

00:15:33,360 --> 00:15:36,800
it is then compiled the web assembly and

00:15:35,519 --> 00:15:39,600
then

00:15:36,800 --> 00:15:40,000
on the host machines cpu we execute this

00:15:39,600 --> 00:15:42,480
program

00:15:40,000 --> 00:15:43,519
and read a tensorflow model and then you

00:15:42,480 --> 00:15:46,959
know um

00:15:43,519 --> 00:15:48,639
execute those those operators and uh

00:15:46,959 --> 00:15:50,320
here is example you know there's uh we

00:15:48,639 --> 00:15:52,079
have a we have an example of this

00:15:50,320 --> 00:15:54,320
approach this in our repository

00:15:52,079 --> 00:15:55,759
it does image recognition so it takes

00:15:54,320 --> 00:15:58,959
about 10 minutes

00:15:55,759 --> 00:16:00,160
to do uh to recognize to do an image

00:15:58,959 --> 00:16:02,560
recognition on a modern

00:16:00,160 --> 00:16:03,440
cpu so it's a very long time you know

00:16:02,560 --> 00:16:05,680
the performance is

00:16:03,440 --> 00:16:08,160
obviously not acceptable but you know

00:16:05,680 --> 00:16:08,560
that's uh um but that's that's perhaps

00:16:08,160 --> 00:16:09,920
the most

00:16:08,560 --> 00:16:12,639
straightforward way to do it right you

00:16:09,920 --> 00:16:14,160
know so um you know you

00:16:12,639 --> 00:16:15,680
provide the implementation of the

00:16:14,160 --> 00:16:18,079
tensorflow operators in

00:16:15,680 --> 00:16:19,040
new web assembly and then you know use

00:16:18,079 --> 00:16:22,000
the interpreter

00:16:19,040 --> 00:16:23,199
to to uh to run them you know that's the

00:16:22,000 --> 00:16:26,240
that's our benchmark

00:16:23,199 --> 00:16:28,560
takes about 10 minutes

00:16:26,240 --> 00:16:30,720
a much better way is to do the just in

00:16:28,560 --> 00:16:32,560
time compilation so it's the same web as

00:16:30,720 --> 00:16:36,240
a web assembly application

00:16:32,560 --> 00:16:39,120
but it runs in a jit runtimes like v8

00:16:36,240 --> 00:16:40,639
um for the gi the way git runtime works

00:16:39,120 --> 00:16:42,720
is that it's no longer

00:16:40,639 --> 00:16:43,759
interprets web assembly instructions

00:16:42,720 --> 00:16:46,079
step by step

00:16:43,759 --> 00:16:47,519
it tries to compile it as it's run it

00:16:46,079 --> 00:16:49,120
right you know so most of the

00:16:47,519 --> 00:16:51,040
execution happens in the native

00:16:49,120 --> 00:16:53,519
environment so

00:16:51,040 --> 00:16:54,320
in that case we immediately see a 300 to

00:16:53,519 --> 00:16:57,040
00:16:54,320 --> 00:16:57,440
times boosting performance you know the

00:16:57,040 --> 00:17:00,639
same

00:16:57,440 --> 00:17:01,920
image recognition task now takes two

00:17:00,639 --> 00:17:03,839
three seconds

00:17:01,920 --> 00:17:05,240
that's great right you know that's uh

00:17:03,839 --> 00:17:07,280
that's essentially how

00:17:05,240 --> 00:17:09,839
tensorflow.js does it right you know

00:17:07,280 --> 00:17:12,480
that's uh so official tensorflow library

00:17:09,839 --> 00:17:13,439
for in-browser application or for mobile

00:17:12,480 --> 00:17:17,280
devices

00:17:13,439 --> 00:17:20,720
that's um it's it runs inside the

00:17:17,280 --> 00:17:20,720
jit web assembly runtime

00:17:21,360 --> 00:17:24,880
of course we can go one step further to

00:17:23,360 --> 00:17:27,760
say you know

00:17:24,880 --> 00:17:29,600
the jit compiler compiles things as it

00:17:27,760 --> 00:17:32,480
goes right you know so as a vm

00:17:29,600 --> 00:17:34,640
and executes the program it compiles it

00:17:32,480 --> 00:17:35,440
we can compile ai models directly into

00:17:34,640 --> 00:17:38,400
webassembly

00:17:35,440 --> 00:17:39,120
bytecode programs that allows us to save

00:17:38,400 --> 00:17:42,080
the time

00:17:39,120 --> 00:17:43,919
to need it to map tensorflow operations

00:17:42,080 --> 00:17:45,919
to web assembly operations at runtime

00:17:43,919 --> 00:17:47,919
right you know because tensorflow model

00:17:45,919 --> 00:17:49,039
consists of a series of tensorflow

00:17:47,919 --> 00:17:51,039
operations

00:17:49,039 --> 00:17:53,280
the the web assembly program what it

00:17:51,039 --> 00:17:56,000
does is read the tensorflow model

00:17:53,280 --> 00:17:57,280
and then execute those those uh those

00:17:56,000 --> 00:18:00,000
those operations

00:17:57,280 --> 00:18:00,720
if we combine those two steps we just

00:18:00,000 --> 00:18:02,720
you know

00:18:00,720 --> 00:18:05,120
the steps that needed in webassembly to

00:18:02,720 --> 00:18:07,520
execute this this tensorflow operator

00:18:05,120 --> 00:18:09,120
is built directly into the web assembly

00:18:07,520 --> 00:18:10,160
into the web assembly application that

00:18:09,120 --> 00:18:12,799
would make it

00:18:10,160 --> 00:18:14,080
run even faster so in that case the

00:18:12,799 --> 00:18:17,200
image recognition task

00:18:14,080 --> 00:18:18,400
takes one to two seconds which i

00:18:17,200 --> 00:18:20,400
i have to say it's more or less

00:18:18,400 --> 00:18:21,440
acceptable for internet applications now

00:18:20,400 --> 00:18:23,520
you know that's uh

00:18:21,440 --> 00:18:25,120
if you have something in the web page on

00:18:23,520 --> 00:18:26,799
a web page that it can recognize a face

00:18:25,120 --> 00:18:29,600
in one to two seconds i think that's

00:18:26,799 --> 00:18:30,000
for most people it's acceptable right so

00:18:29,600 --> 00:18:32,160
um

00:18:30,000 --> 00:18:33,120
representative examples of those is the

00:18:32,160 --> 00:18:35,760
tbm project

00:18:33,120 --> 00:18:37,520
apache tbm project which compiles

00:18:35,760 --> 00:18:38,559
tensorflow models directly into

00:18:37,520 --> 00:18:41,520
webassembly

00:18:38,559 --> 00:18:42,080
and uh on the second stage qualcomm work

00:18:41,520 --> 00:18:44,480
with

00:18:42,080 --> 00:18:46,320
for the onyx comparator for wasm

00:18:44,480 --> 00:18:49,280
compiles

00:18:46,320 --> 00:18:50,720
you know onyx model ai model directly

00:18:49,280 --> 00:18:54,559
into webassembly and runs

00:18:50,720 --> 00:18:55,039
on qualcomm hardware let's take it one

00:18:54,559 --> 00:18:58,640
step

00:18:55,039 --> 00:18:59,760
further you know because let's recall

00:18:58,640 --> 00:19:01,919
how python does it

00:18:59,760 --> 00:19:03,840
python does you know does not call

00:19:01,919 --> 00:19:05,039
tensorflow library in the interpreter in

00:19:03,840 --> 00:19:08,080
the interpreter

00:19:05,039 --> 00:19:08,880
or in a jet compiler what it does is

00:19:08,080 --> 00:19:11,760
that

00:19:08,880 --> 00:19:12,960
when the python program needs to run a

00:19:11,760 --> 00:19:16,000
tensorflow model

00:19:12,960 --> 00:19:18,160
it drops out of the python and into

00:19:16,000 --> 00:19:19,760
the native c plus plus you receive

00:19:18,160 --> 00:19:20,559
environments and runs the tensorflow

00:19:19,760 --> 00:19:22,240
model there

00:19:20,559 --> 00:19:23,840
and then it gets a result and it goes

00:19:22,240 --> 00:19:24,960
back to the python environment we can do

00:19:23,840 --> 00:19:27,200
exactly the same

00:19:24,960 --> 00:19:29,600
with the web assembly as well so we can

00:19:27,200 --> 00:19:31,280
call native web summary libraries from

00:19:29,600 --> 00:19:32,640
we can call native tensorflow libraries

00:19:31,280 --> 00:19:35,600
from webassembly

00:19:32,640 --> 00:19:38,240
you know we uh we provide a roster api

00:19:35,600 --> 00:19:41,280
that compiles down to tensorflow cost

00:19:38,240 --> 00:19:42,960
and then your

00:19:41,280 --> 00:19:45,039
then your image then your data

00:19:42,960 --> 00:19:46,799
preparation and the post processing work

00:19:45,039 --> 00:19:49,360
can still be written in the height

00:19:46,799 --> 00:19:51,360
in a in a highly optimized and high

00:19:49,360 --> 00:19:54,799
performance language like rust

00:19:51,360 --> 00:19:56,880
so to do so um we can show

00:19:54,799 --> 00:19:58,720
that's the image recognition task now

00:19:56,880 --> 00:20:01,679
takes half a second

00:19:58,720 --> 00:20:03,520
that's 2000 times improvement from the

00:20:01,679 --> 00:20:05,280
original interpreted mode right

00:20:03,520 --> 00:20:07,360
you know so an example of this is the

00:20:05,280 --> 00:20:10,080
ssvm with wazi tensorflow

00:20:07,360 --> 00:20:11,520
it's a rust sdk and the web assembly

00:20:10,080 --> 00:20:15,039
extension that we wrote

00:20:11,520 --> 00:20:18,080
for the um for for the ssvm which we

00:20:15,039 --> 00:20:21,440
donated to cncf

00:20:18,080 --> 00:20:24,559
of course now um because um

00:20:21,440 --> 00:20:26,320
the tensorflow model is wrong um in the

00:20:24,559 --> 00:20:28,720
native library right in uh

00:20:26,320 --> 00:20:30,400
written by reading cc plus that

00:20:28,720 --> 00:20:31,760
particular native library can now take

00:20:30,400 --> 00:20:33,679
advantage of the

00:20:31,760 --> 00:20:34,880
the hardware features on that particular

00:20:33,679 --> 00:20:38,080
device right

00:20:34,880 --> 00:20:38,960
so we can use um you know a head of

00:20:38,080 --> 00:20:41,600
compilation

00:20:38,960 --> 00:20:42,880
techniques aot compilers to optimize for

00:20:41,600 --> 00:20:44,640
data preparation tasks

00:20:42,880 --> 00:20:46,159
that seem wasn't we can run the

00:20:44,640 --> 00:20:47,039
tensorflow library as specialized

00:20:46,159 --> 00:20:49,120
hardwares

00:20:47,039 --> 00:20:50,480
such as you know ai chips or gpus and

00:20:49,120 --> 00:20:53,840
you know things like that

00:20:50,480 --> 00:20:57,039
and the image recognition task now takes

00:20:53,840 --> 00:21:00,799
sees a another 10 volt improvements

00:20:57,039 --> 00:21:01,679
it only takes 0.05 seconds to recognize

00:21:00,799 --> 00:21:05,039
the image

00:21:01,679 --> 00:21:08,240
right you know that's um in that case

00:21:05,039 --> 00:21:09,039
we can do um 20 image recognitions per

00:21:08,240 --> 00:21:11,919
second

00:21:09,039 --> 00:21:14,400
the full video speed is 30 frames per

00:21:11,919 --> 00:21:16,080
second right so it's close to a

00:21:14,400 --> 00:21:18,000
to real-time video so if we have a

00:21:16,080 --> 00:21:19,840
real-time video feed we will be able to

00:21:18,000 --> 00:21:23,360
run a model on it and see what's in

00:21:19,840 --> 00:21:26,080
in each of the each of the video frames

00:21:23,360 --> 00:21:27,280
so the ssvm on tencent serverless um it

00:21:26,080 --> 00:21:29,919
shows an example of this

00:21:27,280 --> 00:21:31,200
it's a it's a it's a very nice example

00:21:29,919 --> 00:21:32,799
that's

00:21:31,200 --> 00:21:34,960
shows your webpage and allow you to

00:21:32,799 --> 00:21:37,280
upload a picture and it tells you what

00:21:34,960 --> 00:21:39,280
kind of food item is on that picture

00:21:37,280 --> 00:21:41,679
the the the whole application takes

00:21:39,280 --> 00:21:43,919
about five minutes to deploy

00:21:41,679 --> 00:21:46,880
it's a very nice way to demonstrate the

00:21:43,919 --> 00:21:46,880
power of this approach

00:21:46,960 --> 00:21:51,919
so now you know um for the next

00:21:50,480 --> 00:21:53,520
for the remaining of this talk i have

00:21:51,919 --> 00:21:55,440
about five minutes left

00:21:53,520 --> 00:21:57,120
so um i want to show you some code you

00:21:55,440 --> 00:21:59,679
know to show you how simple it is

00:21:57,120 --> 00:22:01,520
you know to use to run tensorflow model

00:21:59,679 --> 00:22:03,440
in in web assembly using

00:22:01,520 --> 00:22:04,720
the approach i have just discussed as a

00:22:03,440 --> 00:22:07,360
rosi tensorflow approach

00:22:04,720 --> 00:22:09,120
right you know so there's a function um

00:22:07,360 --> 00:22:11,200
in that's written in rust

00:22:09,120 --> 00:22:13,520
it's called infer and the info function

00:22:11,200 --> 00:22:14,799
takes a data array that is that

00:22:13,520 --> 00:22:17,440
represents the image

00:22:14,799 --> 00:22:17,840
and come back with a string which is a

00:22:17,440 --> 00:22:19,520
text

00:22:17,840 --> 00:22:21,600
description of what's in the image right

00:22:19,520 --> 00:22:23,679
so that's the message signature

00:22:21,600 --> 00:22:24,640
and in this screen you can see it's read

00:22:23,679 --> 00:22:26,880
the image

00:22:24,640 --> 00:22:29,679
and then it's resize the image and then

00:22:26,880 --> 00:22:32,080
flatten the image into

00:22:29,679 --> 00:22:33,039
a predefined format that's can be

00:22:32,080 --> 00:22:36,720
consumed by the

00:22:33,039 --> 00:22:39,200
by the tensorflow model now

00:22:36,720 --> 00:22:40,400
in the next in this slide we read the

00:22:39,200 --> 00:22:42,320
tensorflow model

00:22:40,400 --> 00:22:43,520
and we read the labels associated with

00:22:42,320 --> 00:22:46,480
the model meaning

00:22:43,520 --> 00:22:47,760
um you know there's there's you know

00:22:46,480 --> 00:22:50,159
when we tell

00:22:47,760 --> 00:22:51,520
we try to we try to train the model for

00:22:50,159 --> 00:22:53,200
what's on the picture

00:22:51,520 --> 00:22:55,360
we give the model labels right you know

00:22:53,200 --> 00:22:56,480
this picture has caps this picture stock

00:22:55,360 --> 00:22:59,520
you know things like that

00:22:56,480 --> 00:23:01,520
so those labels are in this label file

00:22:59,520 --> 00:23:03,760
and then we have a really simple api to

00:23:01,520 --> 00:23:07,440
run to our tensorflow session

00:23:03,760 --> 00:23:10,480
and then we gave the flattened image as

00:23:07,440 --> 00:23:11,600
input and then gets the output the

00:23:10,480 --> 00:23:15,120
predictions output

00:23:11,600 --> 00:23:16,960
from the from the tensorflow model so in

00:23:15,120 --> 00:23:18,559
in this process we drop to the operating

00:23:16,960 --> 00:23:21,200
system and run this model and get the

00:23:18,559 --> 00:23:21,200
result back

00:23:21,280 --> 00:23:25,760
now we get the result back the result is

00:23:24,000 --> 00:23:26,960
in the array it's in array of

00:23:25,760 --> 00:23:30,559
probabilities

00:23:26,960 --> 00:23:33,120
you know say if the um if the data has

00:23:30,559 --> 00:23:33,919
um if the model has a hundred associated

00:23:33,120 --> 00:23:35,919
labels

00:23:33,919 --> 00:23:38,080
you're gonna get a hundred numbers and

00:23:35,919 --> 00:23:41,039
each number corresponding to

00:23:38,080 --> 00:23:42,559
the probability of that label on that

00:23:41,039 --> 00:23:46,080
image right you know so

00:23:42,559 --> 00:23:46,640
if the uh if the label corresponding to

00:23:46,080 --> 00:23:49,919
cats

00:23:46,640 --> 00:23:51,360
is has probability of 0.8 that means

00:23:49,919 --> 00:23:53,840
there's 80 percent of chance

00:23:51,360 --> 00:23:55,279
this this picture contains a cat right

00:23:53,840 --> 00:23:57,360
so um here we just

00:23:55,279 --> 00:23:59,200
sort the return tensor the return array

00:23:57,360 --> 00:24:02,400
and find the object with the largest

00:23:59,200 --> 00:24:03,360
probability find the the index with the

00:24:02,400 --> 00:24:06,559
largest

00:24:03,360 --> 00:24:07,279
probability and then from there we we go

00:24:06,559 --> 00:24:09,840
to the

00:24:07,279 --> 00:24:10,960
um the labels file the metadata file to

00:24:09,840 --> 00:24:12,880
find out what's that's

00:24:10,960 --> 00:24:15,520
largest probability correspond to and

00:24:12,880 --> 00:24:17,440
then generate an english description

00:24:15,520 --> 00:24:18,799
for the model right you know so that's

00:24:17,440 --> 00:24:21,279
this whole rust program

00:24:18,799 --> 00:24:22,640
is now compiled can not compile into web

00:24:21,279 --> 00:24:26,080
assembly and run

00:24:22,640 --> 00:24:29,760
on ssvm in uh um in the cloud in

00:24:26,080 --> 00:24:32,000
edge cloud and that's that

00:24:29,760 --> 00:24:33,120
would allow it to detect you know

00:24:32,000 --> 00:24:35,279
objects from

00:24:33,120 --> 00:24:38,240
from any image that you send it to that

00:24:35,279 --> 00:24:38,240
cloud service right

00:24:38,400 --> 00:24:41,919
so if you're interested here are some

00:24:40,000 --> 00:24:43,279
you know there's we have the source code

00:24:41,919 --> 00:24:44,400
we have the demo and we have the

00:24:43,279 --> 00:24:46,960
tutorials

00:24:44,400 --> 00:24:49,039
and make sure you you know if you're

00:24:46,960 --> 00:24:51,120
interested to check them out

00:24:49,039 --> 00:24:53,039
so what's next there's uh there are lots

00:24:51,120 --> 00:24:53,840
of things that the field is still young

00:24:53,039 --> 00:24:55,840
and uh

00:24:53,840 --> 00:24:58,240
um but i think it solves a real big

00:24:55,840 --> 00:25:01,679
problem you know it's how to deploy

00:24:58,240 --> 00:25:04,240
uh production ready um ai inference

00:25:01,679 --> 00:25:05,360
application right you know um in the

00:25:04,240 --> 00:25:07,919
manner that is

00:25:05,360 --> 00:25:09,679
that is secure that is safe that is uh

00:25:07,919 --> 00:25:12,320
you know developer friendly right

00:25:09,679 --> 00:25:14,320
you know so um there's many things that

00:25:12,320 --> 00:25:16,480
we can do to improve what we have done

00:25:14,320 --> 00:25:18,400
so far right first we can support the

00:25:16,480 --> 00:25:19,200
was in neural network specification

00:25:18,400 --> 00:25:21,679
which is uh

00:25:19,200 --> 00:25:23,279
which is a specification developed by

00:25:21,679 --> 00:25:25,440
the community to unify

00:25:23,279 --> 00:25:26,880
you know the different ai mod different

00:25:25,440 --> 00:25:29,200
ai model approaches

00:25:26,880 --> 00:25:30,240
right so the approach we took is very uh

00:25:29,200 --> 00:25:32,000
so far it's very

00:25:30,240 --> 00:25:33,840
tensorflow specific it's called wazi

00:25:32,000 --> 00:25:36,159
tensorflow right so the washi n

00:25:33,840 --> 00:25:38,559
specification can accommodate more

00:25:36,159 --> 00:25:39,919
more ai frameworks and we are committed

00:25:38,559 --> 00:25:42,960
to work with

00:25:39,919 --> 00:25:44,880
people in that specification group

00:25:42,960 --> 00:25:46,000
and provide more sdks for front-end

00:25:44,880 --> 00:25:49,440
languages which we just

00:25:46,000 --> 00:25:52,159
saw we have a rust sdk and we call those

00:25:49,440 --> 00:25:52,880
api singles sdks to uh tensorflow

00:25:52,159 --> 00:25:54,960
inference

00:25:52,880 --> 00:25:56,720
but we can do that for for other

00:25:54,960 --> 00:25:57,679
languages that supported our website as

00:25:56,720 --> 00:26:00,400
well

00:25:57,679 --> 00:26:02,240
and on the other hand other end is to

00:26:00,400 --> 00:26:02,559
support more ai frameworks on the back

00:26:02,240 --> 00:26:04,320
end

00:26:02,559 --> 00:26:06,320
of tensorflow beyond the tensorflow

00:26:04,320 --> 00:26:10,159
right so there's onyx models

00:26:06,320 --> 00:26:11,760
pedal panel tangent mk net and mxnext

00:26:10,159 --> 00:26:13,440
you know things like that you know those

00:26:11,760 --> 00:26:14,080
uh you know you can just the idea is

00:26:13,440 --> 00:26:16,000
that you can just

00:26:14,080 --> 00:26:17,440
plug in a different model and then run

00:26:16,000 --> 00:26:19,760
the exact same code

00:26:17,440 --> 00:26:21,039
and then you would um be able to utilize

00:26:19,760 --> 00:26:22,880
that model right

00:26:21,039 --> 00:26:24,559
more ai hardware you know that's um

00:26:22,880 --> 00:26:27,919
that's could be supported by those

00:26:24,559 --> 00:26:30,159
very much so

00:26:27,919 --> 00:26:31,760
i think uh my time's up and uh um thank

00:26:30,159 --> 00:26:33,919
you very much for watching

00:26:31,760 --> 00:26:35,440
and uh i have another talk later in the

00:26:33,919 --> 00:26:37,360
afternoon to talk about

00:26:35,440 --> 00:26:39,279
how web assembly can become it can be

00:26:37,360 --> 00:26:42,400
used as a serverless runtime

00:26:39,279 --> 00:26:44,480
and a lot of this that use case is

00:26:42,400 --> 00:26:47,039
already illustrated here when we

00:26:44,480 --> 00:26:48,640
develop a ai inference function we

00:26:47,039 --> 00:26:49,440
actually deploy it as a serverless

00:26:48,640 --> 00:26:52,640
function

00:26:49,440 --> 00:26:55,679
either our own infrastructure or in

00:26:52,640 --> 00:26:57,760
tencent serverless or aws lambda

00:26:55,679 --> 00:26:58,720
you know that allows application

00:26:57,760 --> 00:27:00,960
developers to just

00:26:58,720 --> 00:27:02,080
write code upload code and then boom

00:27:00,960 --> 00:27:03,840
have the

00:27:02,080 --> 00:27:05,600
ai influence functions in production

00:27:03,840 --> 00:27:08,320
ready for everyone to use

00:27:05,600 --> 00:27:09,760
so i hope you enjoyed this talk go to

00:27:08,320 --> 00:27:14,640
our github repository

00:27:09,760 --> 00:27:19,919
on github.com second dash state

00:27:14,640 --> 00:27:19,919

YouTube URL: https://www.youtube.com/watch?v=auOryvmSmiQ


