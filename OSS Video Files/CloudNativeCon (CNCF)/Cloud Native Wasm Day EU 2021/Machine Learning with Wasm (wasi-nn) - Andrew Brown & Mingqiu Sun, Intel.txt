Title: Machine Learning with Wasm (wasi-nn) - Andrew Brown & Mingqiu Sun, Intel
Publication date: 2021-05-04
Playlist: Cloud Native Wasm Day EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Machine Learning with Wasm (wasi-nn) - Andrew Brown & Mingqiu Sun, Intel

Web Assembly provides an ideal portable form of deployment for machine learning models. Although a whole machine learning framework could be potentially compiled into Wasm, special hardware acceleration is often needed in order to be performant, which is difficult to do within Wasm with the limited support of parallelism. WASI-NN is a Web Assembly System Interface (WASI) module that allows for near-native performance on a variety of devices such as CPU with AVX512, GPU, TPU, and FPGA. In this talk, we will present the WASI-NN interface design and a reference implementation in Wasmtime. We will provide a step-by-step guide and a demo on how to build all relevant components, and write an image classification application with sample code in Rust and AssemblyScript. We will also demonstrate the performance advantages of WASI-NN over alternative approaches.
Captions: 
	00:00:00,560 --> 00:00:07,520
hello everyone uh my name is min tu sung

00:00:04,319 --> 00:00:09,040
i'm a senior principal engineer at intel

00:00:07,520 --> 00:00:10,880
today we're gonna talk about machine

00:00:09,040 --> 00:00:15,280
learning with web assembly

00:00:10,880 --> 00:00:15,280
together with my colleague andrew brown

00:00:15,440 --> 00:00:19,279
so here's our agenda we'll start with

00:00:17,920 --> 00:00:22,000
introduction to wally

00:00:19,279 --> 00:00:22,880
and the biker alliance then we'll get

00:00:22,000 --> 00:00:26,480
into

00:00:22,880 --> 00:00:28,960
some of the design guidelines for dnn

00:00:26,480 --> 00:00:30,240
uh such as you know loader versus

00:00:28,960 --> 00:00:32,480
builder

00:00:30,240 --> 00:00:34,719
and the machine learning model and the

00:00:32,480 --> 00:00:37,040
virtual i o type

00:00:34,719 --> 00:00:38,000
andrew then will start talking about how

00:00:37,040 --> 00:00:40,879
to

00:00:38,000 --> 00:00:42,320
write a ydn application in rust and

00:00:40,879 --> 00:00:43,920
assembly script

00:00:42,320 --> 00:00:46,480
and as a reference implementation

00:00:43,920 --> 00:00:48,960
architecture on one time

00:00:46,480 --> 00:00:50,480
and then he would then do a uh image

00:00:48,960 --> 00:00:54,079
classification demo

00:00:50,480 --> 00:00:56,559
with what dnn and then followed by

00:00:54,079 --> 00:00:57,680
performance advantage analysis of our

00:00:56,559 --> 00:00:59,520
dnn

00:00:57,680 --> 00:01:02,079
then we end the presentation with call

00:00:59,520 --> 00:01:02,079
for action

00:01:04,239 --> 00:01:11,040
so what is wazi why this stands for

00:01:07,600 --> 00:01:12,000
web assembly system interface it is a

00:01:11,040 --> 00:01:15,119
modular system

00:01:12,000 --> 00:01:18,640
interface for web assembly and with

00:01:15,119 --> 00:01:20,960
focus on security and portability

00:01:18,640 --> 00:01:21,920
uh so the body that's driving the

00:01:20,960 --> 00:01:27,040
specification

00:01:21,920 --> 00:01:31,600
is a subgroup of the w3c web assembly cg

00:01:27,040 --> 00:01:33,840
and ydnn is a worthy module for

00:01:31,600 --> 00:01:34,880
neural network and currently we're in

00:01:33,840 --> 00:01:41,280
phase 2

00:01:34,880 --> 00:01:44,240
with the wazi community group

00:01:41,280 --> 00:01:45,759
so let's talk briefly about the bico

00:01:44,240 --> 00:01:48,640
alliance

00:01:45,759 --> 00:01:50,720
which is a open source community

00:01:48,640 --> 00:01:51,920
dedicated to creating secure new

00:01:50,720 --> 00:01:54,320
software foundation

00:01:51,920 --> 00:01:55,040
you know built on top of a technology

00:01:54,320 --> 00:01:57,439
such as

00:01:55,040 --> 00:01:58,719
web assembly web assembly system

00:01:57,439 --> 00:02:03,759
interface

00:01:58,719 --> 00:02:05,840
interface type uh module linking etc

00:02:03,759 --> 00:02:07,600
and the weather implementation is

00:02:05,840 --> 00:02:10,800
definitely important goal for

00:02:07,600 --> 00:02:13,920
bicol allies and the first

00:02:10,800 --> 00:02:16,959
reference implementation for ydnn

00:02:13,920 --> 00:02:23,680
is built on top of wasm time

00:02:16,959 --> 00:02:27,520
which is a michael alliance project

00:02:23,680 --> 00:02:30,879
so let's dig deeper into you know why

00:02:27,520 --> 00:02:32,080
we want to do whatever then so the first

00:02:30,879 --> 00:02:35,120
question we want to ask

00:02:32,080 --> 00:02:37,599
is that you know why do we need the

00:02:35,120 --> 00:02:40,560
webassembly for machine learning

00:02:37,599 --> 00:02:42,959
and it turned out that people do a lot

00:02:40,560 --> 00:02:46,000
of machine learning training and then

00:02:42,959 --> 00:02:48,000
they typically deploy them to

00:02:46,000 --> 00:02:50,480
a variety of devices with different

00:02:48,000 --> 00:02:52,640
architecture and operating systems

00:02:50,480 --> 00:02:54,560
and that's where the web assembly comes

00:02:52,640 --> 00:02:56,000
into the picture because it provides the

00:02:54,560 --> 00:02:59,840
ideal

00:02:56,000 --> 00:03:02,879
portable for format for deployment

00:02:59,840 --> 00:03:04,800
of those models to those devices

00:03:02,879 --> 00:03:06,560
so the next question is that you know

00:03:04,800 --> 00:03:08,080
why do you need a web assembly system

00:03:06,560 --> 00:03:11,200
interface right

00:03:08,080 --> 00:03:12,000
uh the reason is very simple uh because

00:03:11,200 --> 00:03:15,440
typically

00:03:12,000 --> 00:03:16,879
uh machine learning workflow requires a

00:03:15,440 --> 00:03:19,200
lot of performance

00:03:16,879 --> 00:03:20,640
and the special hardware support

00:03:19,200 --> 00:03:23,680
acceleration

00:03:20,640 --> 00:03:27,120
are necessary uh you know example the

00:03:23,680 --> 00:03:30,959
avx 512 for cpu and

00:03:27,120 --> 00:03:33,599
access to gpu tpu etc

00:03:30,959 --> 00:03:34,159
and the current web assembly spec only

00:03:33,599 --> 00:03:37,280
supports

00:03:34,159 --> 00:03:39,920
a limit uh set of a parallelism

00:03:37,280 --> 00:03:41,440
you know especially with this 128-bit

00:03:39,920 --> 00:03:44,959
cmd

00:03:41,440 --> 00:03:46,640
so that's clearly not sufficient for

00:03:44,959 --> 00:03:48,400
near native performance for those

00:03:46,640 --> 00:03:51,440
machine learning workload

00:03:48,400 --> 00:03:53,439
on modern highway

00:03:51,440 --> 00:03:55,120
so later in the presentation we'll

00:03:53,439 --> 00:03:58,560
actually demonstrate

00:03:55,120 --> 00:04:00,799
7 to 32x performance improvement

00:03:58,560 --> 00:04:01,760
with this ynn approach that we are

00:04:00,799 --> 00:04:07,120
proposing

00:04:01,760 --> 00:04:07,120
over a pure web assembly approach

00:04:08,159 --> 00:04:11,599
let's take a look at the web assembly

00:04:10,159 --> 00:04:13,360
runtime environment

00:04:11,599 --> 00:04:14,959
and the use cases associated with each

00:04:13,360 --> 00:04:17,600
of those environment

00:04:14,959 --> 00:04:19,040
so there are three different type of

00:04:17,600 --> 00:04:22,639
environments

00:04:19,040 --> 00:04:23,520
for web assembly to execute so the first

00:04:22,639 --> 00:04:26,240
one is

00:04:23,520 --> 00:04:28,080
the standalone environment where you

00:04:26,240 --> 00:04:30,479
know a web assembly application

00:04:28,080 --> 00:04:31,680
run on top of a standalone web assembly

00:04:30,479 --> 00:04:34,800
virtual machine

00:04:31,680 --> 00:04:38,400
which called into the wazi software

00:04:34,800 --> 00:04:40,960
stack for system apis

00:04:38,400 --> 00:04:42,000
so typically in our standalone

00:04:40,960 --> 00:04:43,919
application

00:04:42,000 --> 00:04:45,520
or cloud applications such as the

00:04:43,919 --> 00:04:46,560
content delivery network function

00:04:45,520 --> 00:04:50,960
service

00:04:46,560 --> 00:04:53,600
envoy proxy type of cloud middleware

00:04:50,960 --> 00:04:54,160
and the resource constraint environment

00:04:53,600 --> 00:04:57,199
such as

00:04:54,160 --> 00:04:58,400
iot or embedded environment

00:04:57,199 --> 00:05:00,800
you know those are the typical

00:04:58,400 --> 00:05:02,080
applications that will uh make use of a

00:05:00,800 --> 00:05:05,520
standalone

00:05:02,080 --> 00:05:09,039
web assembly virtual machine uh so

00:05:05,520 --> 00:05:12,320
the second one is you know the current

00:05:09,039 --> 00:05:14,800
mainstream browser environment

00:05:12,320 --> 00:05:16,960
where that you have access to a

00:05:14,800 --> 00:05:19,440
javascript virtual machine

00:05:16,960 --> 00:05:21,520
where the the a web assembly virtual

00:05:19,440 --> 00:05:24,240
machine is a component of it

00:05:21,520 --> 00:05:26,560
and have access to web api through

00:05:24,240 --> 00:05:29,919
javascript environment

00:05:26,560 --> 00:05:33,280
that's where the web and then you know

00:05:29,919 --> 00:05:35,919
api lives and

00:05:33,280 --> 00:05:38,240
all the browser application or pwa will

00:05:35,919 --> 00:05:41,280
make use of this environment

00:05:38,240 --> 00:05:42,160
so the third category is the node.js

00:05:41,280 --> 00:05:46,160
environment

00:05:42,160 --> 00:05:48,800
which is a reuse of the va

00:05:46,160 --> 00:05:50,000
javascript engine in the standalone

00:05:48,800 --> 00:05:53,759
environment

00:05:50,000 --> 00:05:57,600
and it would make use of a web api

00:05:53,759 --> 00:05:59,039
as well as a wazi api so this is a case

00:05:57,600 --> 00:06:00,960
that

00:05:59,039 --> 00:06:02,160
when you don't have a lot of resource

00:06:00,960 --> 00:06:05,199
constraint

00:06:02,160 --> 00:06:06,160
and you especially when you need to have

00:06:05,199 --> 00:06:08,960
both javascript

00:06:06,160 --> 00:06:10,880
that wasn't web assembly support and

00:06:08,960 --> 00:06:14,240
this will be an ideal situation for

00:06:10,880 --> 00:06:14,240
those type of applications

00:06:15,520 --> 00:06:19,600
so let's take take a look at some of the

00:06:18,080 --> 00:06:23,600
design decisions we have made

00:06:19,600 --> 00:06:26,880
for ydnn uh in this case you know

00:06:23,600 --> 00:06:27,360
loader versus builder api so when we

00:06:26,880 --> 00:06:30,160
started

00:06:27,360 --> 00:06:31,919
designing uh what was the nn you know

00:06:30,160 --> 00:06:34,080
those are the two

00:06:31,919 --> 00:06:35,840
options that we could take and then we

00:06:34,080 --> 00:06:39,280
worked really closely with

00:06:35,840 --> 00:06:40,240
the webinar team and the you know they

00:06:39,280 --> 00:06:43,280
went through

00:06:40,240 --> 00:06:44,160
similar exercise uh so in the end we

00:06:43,280 --> 00:06:48,560
decided to

00:06:44,160 --> 00:06:52,160
take the model loader api first and

00:06:48,560 --> 00:06:55,599
push out the build api to a second phase

00:06:52,160 --> 00:06:58,240
so the reason for that multiple flow

00:06:55,599 --> 00:06:59,280
and you know influencing is obviously

00:06:58,240 --> 00:07:02,560
the main

00:06:59,280 --> 00:07:03,840
machine learning use case and that's our

00:07:02,560 --> 00:07:06,960
initial focus

00:07:03,840 --> 00:07:09,840
so a load of api is really

00:07:06,960 --> 00:07:11,759
an easy way to support influencing in

00:07:09,840 --> 00:07:13,520
terms of future completeness

00:07:11,759 --> 00:07:15,199
uh you know machine learning is still

00:07:13,520 --> 00:07:18,319
evolving rapidly

00:07:15,199 --> 00:07:19,599
with roughly 20 growth of new operation

00:07:18,319 --> 00:07:22,479
each year

00:07:19,599 --> 00:07:24,319
so a builder api will probably take

00:07:22,479 --> 00:07:27,599
multiple years

00:07:24,319 --> 00:07:31,199
to be you know usable

00:07:27,599 --> 00:07:32,880
and roughly functional

00:07:31,199 --> 00:07:34,319
complete to cover all the major

00:07:32,880 --> 00:07:37,520
operations

00:07:34,319 --> 00:07:40,960
versus a loader api which we will treat

00:07:37,520 --> 00:07:44,240
a model as opaque object and

00:07:40,960 --> 00:07:47,120
we can just define operation

00:07:44,240 --> 00:07:47,440
related to the machine learning loader

00:07:47,120 --> 00:07:49,919
and

00:07:47,440 --> 00:07:50,479
get done with it so that's a lot easier

00:07:49,919 --> 00:07:55,039
pass

00:07:50,479 --> 00:07:58,080
to to market and the loader api is a

00:07:55,039 --> 00:08:01,360
much simpler api with a better

00:07:58,080 --> 00:08:02,800
ip protection opportunity and the next

00:08:01,360 --> 00:08:06,400
slide we actually talked about

00:08:02,800 --> 00:08:09,680
how we treat the machine learning

00:08:06,400 --> 00:08:12,720
you know model format so that we

00:08:09,680 --> 00:08:15,360
can make the machine the the main

00:08:12,720 --> 00:08:16,080
web assembly program you know framework

00:08:15,360 --> 00:08:19,440
and the model

00:08:16,080 --> 00:08:22,639
format agnostic

00:08:19,440 --> 00:08:24,479
and it's also very easy to support a

00:08:22,639 --> 00:08:28,000
variety of devices

00:08:24,479 --> 00:08:31,120
you know cpu gpu fpga tpu

00:08:28,000 --> 00:08:31,120
for the loader api

00:08:31,280 --> 00:08:38,080
so uh for uh the model builder api

00:08:35,599 --> 00:08:39,440
you know the main advantage of it is

00:08:38,080 --> 00:08:42,479
that you can provide like

00:08:39,440 --> 00:08:44,399
operation specific acceleration uh you

00:08:42,479 --> 00:08:47,360
know this is very useful

00:08:44,399 --> 00:08:48,959
for some machine learning framework that

00:08:47,360 --> 00:08:51,600
the legs support

00:08:48,959 --> 00:08:52,640
you know buy some model converters or

00:08:51,600 --> 00:08:55,519
buy some

00:08:52,640 --> 00:08:56,800
back-end machine learning engine and in

00:08:55,519 --> 00:08:57,839
that case you know if you want to

00:08:56,800 --> 00:09:00,240
compile that

00:08:57,839 --> 00:09:01,839
specialized machine learning framework

00:09:00,240 --> 00:09:04,160
completely into

00:09:01,839 --> 00:09:06,080
web assembly and have some specific

00:09:04,160 --> 00:09:09,120
operation accelerated by the hardware

00:09:06,080 --> 00:09:12,160
then the builder api will be very useful

00:09:09,120 --> 00:09:14,880
and that we think we can look into

00:09:12,160 --> 00:09:16,240
supporting that sort of usage in the

00:09:14,880 --> 00:09:19,519
phase second phase of

00:09:16,240 --> 00:09:20,880
world dna and we could potentially

00:09:19,519 --> 00:09:22,720
leverage

00:09:20,880 --> 00:09:24,959
the work already done by the webinam

00:09:22,720 --> 00:09:24,959
team

00:09:26,959 --> 00:09:32,080
let's talk about machine learning model

00:09:29,600 --> 00:09:34,000
and the virtualized i o type

00:09:32,080 --> 00:09:36,000
our view of a machine learning model is

00:09:34,000 --> 00:09:38,320
a virtualized i o type

00:09:36,000 --> 00:09:39,920
which you know you think of a general y

00:09:38,320 --> 00:09:42,720
direction

00:09:39,920 --> 00:09:44,399
this type is just like a media type with

00:09:42,720 --> 00:09:45,680
its own data in the metadata and the

00:09:44,399 --> 00:09:48,720
defined operation

00:09:45,680 --> 00:09:51,360
associated with it such as the load

00:09:48,720 --> 00:09:53,120
so the approach we take is to you know

00:09:51,360 --> 00:09:54,000
to push the mapping to actual

00:09:53,120 --> 00:09:57,519
implementation

00:09:54,000 --> 00:09:58,399
to the edges uh that way we can keep the

00:09:57,519 --> 00:10:01,120
main

00:09:58,399 --> 00:10:02,839
web assembly program portable and the

00:10:01,120 --> 00:10:05,120
framework

00:10:02,839 --> 00:10:08,320
agnostic so there's not really

00:10:05,120 --> 00:10:11,360
two approaches to to this idea

00:10:08,320 --> 00:10:14,720
uh one is that we can perform

00:10:11,360 --> 00:10:16,880
a model conversion before we do anything

00:10:14,720 --> 00:10:19,200
else

00:10:16,880 --> 00:10:20,800
to whatever accept the format for this

00:10:19,200 --> 00:10:23,920
target platform

00:10:20,800 --> 00:10:25,839
or alternatively we can let the web

00:10:23,920 --> 00:10:27,519
assembly virtual machine

00:10:25,839 --> 00:10:28,880
to dispatch to different machine

00:10:27,519 --> 00:10:32,079
learning backend

00:10:28,880 --> 00:10:34,000
uh you know based on metadata available

00:10:32,079 --> 00:10:35,440
associated with this machine learning

00:10:34,000 --> 00:10:39,360
model

00:10:35,440 --> 00:10:44,240
so it could be you know openvino

00:10:39,360 --> 00:10:47,279
tensorflow or onix in this example

00:10:44,240 --> 00:10:50,399
so that's our approach to the model

00:10:47,279 --> 00:10:53,519
uh machine learning model and the next

00:10:50,399 --> 00:11:01,040
uh andrew will cover

00:10:53,519 --> 00:11:02,880
how to write one in an application

00:11:01,040 --> 00:11:04,720
hi my name is andrew brown i'm a

00:11:02,880 --> 00:11:06,320
software engineer at intel

00:11:04,720 --> 00:11:09,040
let me tell you a little bit more about

00:11:06,320 --> 00:11:11,600
the wes enn api

00:11:09,040 --> 00:11:13,120
what you see here is the widex

00:11:11,600 --> 00:11:16,720
specification

00:11:13,120 --> 00:11:20,640
of that api it's a five function

00:11:16,720 --> 00:11:23,120
api including things such as load

00:11:20,640 --> 00:11:25,440
initializing and execution context

00:11:23,120 --> 00:11:28,399
setting inputs retrieving outputs

00:11:25,440 --> 00:11:29,760
and most importantly computing the

00:11:28,399 --> 00:11:32,800
inference using the

00:11:29,760 --> 00:11:32,800
loaded graph

00:11:33,519 --> 00:11:36,959
here's an example of those bindings in

00:11:35,040 --> 00:11:37,680
assembly script what we're trying to do

00:11:36,959 --> 00:11:40,079
here

00:11:37,680 --> 00:11:40,959
is make it a lot easier for users to use

00:11:40,079 --> 00:11:44,000
wazion and

00:11:40,959 --> 00:11:47,600
from their webassembly applications

00:11:44,000 --> 00:11:50,160
here are the same bindings but in rust

00:11:47,600 --> 00:11:51,360
you can see that the same functions are

00:11:50,160 --> 00:11:53,279
exposed

00:11:51,360 --> 00:11:57,839
but you have higher level constructs

00:11:53,279 --> 00:11:57,839
which should make it a lot easier to use

00:11:58,399 --> 00:12:02,160
though wesleyan is not tied to any

00:12:00,160 --> 00:12:03,519
specific webassembly engine or machine

00:12:02,160 --> 00:12:06,399
learning framework

00:12:03,519 --> 00:12:08,320
we did have to start somewhere and so we

00:12:06,399 --> 00:12:09,600
used wasmtime as that engine and

00:12:08,320 --> 00:12:10,880
openvino is the machine learning

00:12:09,600 --> 00:12:13,040
framework

00:12:10,880 --> 00:12:14,720
this diagram shows you all the various

00:12:13,040 --> 00:12:16,720
components

00:12:14,720 --> 00:12:18,079
in our implementation starting on the

00:12:16,720 --> 00:12:21,200
left you'll see the

00:12:18,079 --> 00:12:22,639
user application code which when tied

00:12:21,200 --> 00:12:24,480
together with the wazing and bindings

00:12:22,639 --> 00:12:26,240
can get compiled down to a webassembly

00:12:24,480 --> 00:12:29,120
file

00:12:26,240 --> 00:12:30,160
that webassembly file will make use of

00:12:29,120 --> 00:12:33,120
tensor inputs

00:12:30,160 --> 00:12:36,320
as for example from an image and the

00:12:33,120 --> 00:12:39,279
model file or files

00:12:36,320 --> 00:12:39,839
and we'll pass those on to the engine in

00:12:39,279 --> 00:12:42,880
our case

00:12:39,839 --> 00:12:44,240
wasn't time wasn't time provides the

00:12:42,880 --> 00:12:46,480
wazing n

00:12:44,240 --> 00:12:48,480
implementation but when it comes to the

00:12:46,480 --> 00:12:51,600
machine learning heavy lifting

00:12:48,480 --> 00:12:54,079
it proxies calls down to openvino which

00:12:51,600 --> 00:12:58,079
will execute them on a cpu or gpu

00:12:54,079 --> 00:13:00,560
etc these middle

00:12:58,079 --> 00:13:02,000
boxes can be swapped out the engine and

00:13:00,560 --> 00:13:04,839
the machine learning framework

00:13:02,000 --> 00:13:07,839
could be swapped out for different

00:13:04,839 --> 00:13:07,839
implementations

00:13:08,000 --> 00:13:12,560
let me switch over to my id for this

00:13:10,240 --> 00:13:12,560
demo

00:13:13,680 --> 00:13:17,040
okay let's look at an example of

00:13:16,240 --> 00:13:20,480
classifying

00:13:17,040 --> 00:13:24,160
this image of a pizza inside

00:13:20,480 --> 00:13:26,800
wasn't time with wazian in enabled

00:13:24,160 --> 00:13:27,839
from assembly script so the first thing

00:13:26,800 --> 00:13:31,040
we're going to need

00:13:27,839 --> 00:13:33,839
is a version of wasm time with wazzy nnn

00:13:31,040 --> 00:13:35,680
enabled that'll look something like the

00:13:33,839 --> 00:13:38,079
following

00:13:35,680 --> 00:13:40,639
what i'm doing here is i'm compiling

00:13:38,079 --> 00:13:44,560
wasm time with the wasan feature

00:13:40,639 --> 00:13:48,560
and using some pre-installed openvino

00:13:44,560 --> 00:13:51,680
libraries as the backing implementation

00:13:48,560 --> 00:13:53,279
next what i'll do is i will place

00:13:51,680 --> 00:13:57,360
wasmtime on my path

00:13:53,279 --> 00:14:00,399
for convenience and i will tell

00:13:57,360 --> 00:14:04,800
my path to load up the

00:14:00,399 --> 00:14:08,000
paths for the openvino libraries

00:14:04,800 --> 00:14:08,000
let's go back to our example

00:14:08,560 --> 00:14:12,320
so i've written this assemblyscript code

00:14:10,959 --> 00:14:15,440
and you can see here

00:14:12,320 --> 00:14:18,800
that it's using the high level api

00:14:15,440 --> 00:14:22,399
the the bindings that we provide in this

00:14:18,800 --> 00:14:24,399
library it'll load the graph

00:14:22,399 --> 00:14:25,680
both the graph description and the graph

00:14:24,399 --> 00:14:27,760
weights

00:14:25,680 --> 00:14:28,720
attempt to use the open vino

00:14:27,760 --> 00:14:32,320
implementation

00:14:28,720 --> 00:14:33,120
on a cpu initialize the execution

00:14:32,320 --> 00:14:35,120
context

00:14:33,120 --> 00:14:36,880
load up the tensor compute the

00:14:35,120 --> 00:14:38,800
classification

00:14:36,880 --> 00:14:40,320
and then with the output tensor it will

00:14:38,800 --> 00:14:43,680
sort those results

00:14:40,320 --> 00:14:45,600
and print out the top five

00:14:43,680 --> 00:14:47,920
so to compile this assemblyscript

00:14:45,600 --> 00:14:50,160
example into webassembly

00:14:47,920 --> 00:14:51,519
we're going to need some tools which are

00:14:50,160 --> 00:14:54,880
provided

00:14:51,519 --> 00:14:58,000
in this repository and we will

00:14:54,880 --> 00:15:00,480
run as build this is

00:14:58,000 --> 00:15:03,360
this uses the assembly script compiler

00:15:00,480 --> 00:15:06,800
to compile the example i just showed you

00:15:03,360 --> 00:15:10,000
into webassembly

00:15:06,800 --> 00:15:13,839
let's take a look at

00:15:10,000 --> 00:15:13,839
uh what it generated here

00:15:14,959 --> 00:15:18,480
so if we look at the imports for the

00:15:17,360 --> 00:15:22,079
optimize.wat

00:15:18,480 --> 00:15:24,320
file you can see that it's using ynn

00:15:22,079 --> 00:15:25,199
as well as some other wazi apis for for

00:15:24,320 --> 00:15:28,320
example for

00:15:25,199 --> 00:15:31,680
reading files other things that show up

00:15:28,320 --> 00:15:33,600
in the uh build directory uh

00:15:31,680 --> 00:15:36,000
are an untouched version of that wasm

00:15:33,600 --> 00:15:36,800
file and both wasm and wap versions of

00:15:36,000 --> 00:15:40,240
these

00:15:36,800 --> 00:15:45,199
uh of these compiled artifacts

00:15:40,240 --> 00:15:48,399
i've placed in here the alexnet

00:15:45,199 --> 00:15:49,040
weights and model files as well as the

00:15:48,399 --> 00:15:51,680
image

00:15:49,040 --> 00:15:52,240
and image converted to a raw tensor

00:15:51,680 --> 00:15:55,440
we'll use

00:15:52,240 --> 00:16:00,240
those in our example

00:15:55,440 --> 00:16:00,240
to as inputs to the classification

00:16:02,720 --> 00:16:09,839
okay let's run the classification

00:16:10,800 --> 00:16:14,240
so what it's doing here is loading the

00:16:13,199 --> 00:16:18,320
graph uh

00:16:14,240 --> 00:16:18,639
files this uh helper function read bytes

00:16:18,320 --> 00:16:20,720
is

00:16:18,639 --> 00:16:21,680
probably not as efficient as it could be

00:16:20,720 --> 00:16:26,399
i believe it's

00:16:21,680 --> 00:16:26,399
reading each byte by byte

00:16:26,880 --> 00:16:30,639
and then we'll see it set up the

00:16:28,160 --> 00:16:33,680
execution context and actually

00:16:30,639 --> 00:16:37,199
run the classification

00:16:33,680 --> 00:16:38,959
okay so now we see that it's printed out

00:16:37,199 --> 00:16:42,320
the top five results

00:16:38,959 --> 00:16:44,880
for the raw tensor that we passed in of

00:16:42,320 --> 00:16:44,880
a pizza

00:16:45,040 --> 00:16:49,279
but what do these mean let's take a look

00:16:47,839 --> 00:16:53,279
at

00:16:49,279 --> 00:16:55,160
this classes file that i also place in

00:16:53,279 --> 00:16:56,639
our build directory and we'll grab for

00:16:55,160 --> 00:17:00,160
00:16:56,639 --> 00:17:04,079
we see okay 963 corresponds

00:17:00,160 --> 00:17:07,280
to a pizza so it is identifying a pizza

00:17:04,079 --> 00:17:09,439
from the image i showed you earlier

00:17:07,280 --> 00:17:11,120
so this would be an example of using the

00:17:09,439 --> 00:17:14,319
assembly script

00:17:11,120 --> 00:17:18,799
bindings to run wazingnn programs from

00:17:14,319 --> 00:17:20,799
within wesenton

00:17:18,799 --> 00:17:22,720
after proving to ourselves that we could

00:17:20,799 --> 00:17:25,520
make this work we wanted to see

00:17:22,720 --> 00:17:27,600
what are the performance benefits the

00:17:25,520 --> 00:17:30,720
intuition was that

00:17:27,600 --> 00:17:33,280
web assembly couldn't make full use of

00:17:30,720 --> 00:17:35,760
of the system capabilities whether that

00:17:33,280 --> 00:17:38,799
was longer cmd or or threading or

00:17:35,760 --> 00:17:38,799
special machine learning

00:17:38,960 --> 00:17:43,679
instructions and so to do that we took

00:17:42,000 --> 00:17:45,600
as a starting point some work that marat

00:17:43,679 --> 00:17:48,480
duken had done

00:17:45,600 --> 00:17:50,000
to run tensorflow models from within the

00:17:48,480 --> 00:17:51,679
browser

00:17:50,000 --> 00:17:53,600
we called that approach the wasm

00:17:51,679 --> 00:17:55,840
exclusive approach and we adapted it to

00:17:53,600 --> 00:17:58,880
work within a standalone engine

00:17:55,840 --> 00:18:00,559
node.js on the

00:17:58,880 --> 00:18:02,080
right side you'll see the wazing n

00:18:00,559 --> 00:18:03,200
approach which uses a different engine

00:18:02,080 --> 00:18:04,160
wasm time

00:18:03,200 --> 00:18:06,240
and a different machine learning

00:18:04,160 --> 00:18:08,799
framework openvino

00:18:06,240 --> 00:18:10,400
but what we're trying to do is compare

00:18:08,799 --> 00:18:12,160
approaches here

00:18:10,400 --> 00:18:13,440
not necessarily engines or machine

00:18:12,160 --> 00:18:14,799
learning frameworks

00:18:13,440 --> 00:18:17,440
we are trying to use the same machine

00:18:14,799 --> 00:18:21,200
learning model and the same

00:18:17,440 --> 00:18:24,320
input which will then pass down to the

00:18:21,200 --> 00:18:24,320
machine learning framework

00:18:26,160 --> 00:18:30,240
so we ran some classifications using

00:18:28,640 --> 00:18:31,760
both approaches

00:18:30,240 --> 00:18:34,799
what you see here is a chart of us

00:18:31,760 --> 00:18:37,280
running uh mobilenet classification

00:18:34,799 --> 00:18:38,880
a thousand times and measuring the

00:18:37,280 --> 00:18:42,000
average inference time

00:18:38,880 --> 00:18:45,360
on both a wasm exclusive setup

00:18:42,000 --> 00:18:48,320
and a wazing n setup you can see that

00:18:45,360 --> 00:18:49,520
even across various platforms the

00:18:48,320 --> 00:18:52,960
wezzing and approach is

00:18:49,520 --> 00:18:54,240
significantly faster we thought oh this

00:18:52,960 --> 00:18:56,880
is probably because

00:18:54,240 --> 00:18:56,880
you know threads

00:19:09,440 --> 00:19:12,640
see a significant speed up we think this

00:19:11,280 --> 00:19:15,679
is because of

00:19:12,640 --> 00:19:18,799
longer widths md and optimizations that

00:19:15,679 --> 00:19:18,799
openvino can provide

00:19:19,200 --> 00:19:22,799
just to make sure that our approach made

00:19:22,320 --> 00:19:26,240
sense

00:19:22,799 --> 00:19:29,679
we tested a more complex model

00:19:26,240 --> 00:19:32,880
so using the inception model we again

00:19:29,679 --> 00:19:34,880
ran a classification a thousand times

00:19:32,880 --> 00:19:36,160
and took the average inference times and

00:19:34,880 --> 00:19:39,520
you can see that

00:19:36,160 --> 00:19:41,760
uh still the wasn't in approach uh is

00:19:39,520 --> 00:19:43,039
significantly faster even when single

00:19:41,760 --> 00:19:47,039
threading

00:19:43,039 --> 00:19:50,080
uh the the implementation

00:19:47,039 --> 00:19:52,160
so we took as a summary that uh

00:19:50,080 --> 00:19:54,080
there's performance being left on the

00:19:52,160 --> 00:19:57,120
table and that

00:19:54,080 --> 00:19:58,720
uh the wazing and approach uh to using

00:19:57,120 --> 00:20:00,720
the full system capabilities is

00:19:58,720 --> 00:20:02,720
necessary to get

00:20:00,720 --> 00:20:05,280
the best possible performance you can

00:20:02,720 --> 00:20:08,720
see speed ups of up to 32x

00:20:05,280 --> 00:20:12,240
from the wazing approach

00:20:08,720 --> 00:20:14,080
and we hope that this motivates

00:20:12,240 --> 00:20:16,320
you know adding adding new engine

00:20:14,080 --> 00:20:19,200
implementations of wazing in

00:20:16,320 --> 00:20:19,840
and adding new uh machine learning

00:20:19,200 --> 00:20:24,159
framework

00:20:19,840 --> 00:20:24,159
implementations to those engines

00:20:25,280 --> 00:20:30,880
finally call for action if you are

00:20:28,320 --> 00:20:33,520
machine learning practitioner

00:20:30,880 --> 00:20:35,840
please go download watson time time and

00:20:33,520 --> 00:20:37,600
start using world vienna

00:20:35,840 --> 00:20:39,280
tell us what you like what you don't

00:20:37,600 --> 00:20:41,440
like what kind of improvement you would

00:20:39,280 --> 00:20:44,159
like to see

00:20:41,440 --> 00:20:45,679
if you are interested in the worthington

00:20:44,159 --> 00:20:49,440
proposal

00:20:45,679 --> 00:20:52,960
in terms of spec or architecture

00:20:49,440 --> 00:20:55,520
please engage us in the worthy community

00:20:52,960 --> 00:20:57,120
and to help us drive this proposal to

00:20:55,520 --> 00:21:00,000
final

00:20:57,120 --> 00:21:01,200
approval it's a lot easier to make

00:21:00,000 --> 00:21:03,520
changes right now

00:21:01,200 --> 00:21:05,679
then later as this proposal is in phase

00:21:03,520 --> 00:21:08,640
2 right now

00:21:05,679 --> 00:21:09,120
if you are a web assembly virtual

00:21:08,640 --> 00:21:11,280
machine

00:21:09,120 --> 00:21:12,159
implementer then you have this

00:21:11,280 --> 00:21:14,159
opportunity of

00:21:12,159 --> 00:21:16,480
providing a additional reference

00:21:14,159 --> 00:21:20,000
implementation for what dnn

00:21:16,480 --> 00:21:23,440
you know own your own virtual machine

00:21:20,000 --> 00:21:25,840
and perhaps with different backends

00:21:23,440 --> 00:21:27,520
such as tensorflow or onyx or anything

00:21:25,840 --> 00:21:30,400
else

00:21:27,520 --> 00:21:34,080
so that concludes our presentation today

00:21:30,400 --> 00:21:34,080

YouTube URL: https://www.youtube.com/watch?v=lz2I_4vvCuc


