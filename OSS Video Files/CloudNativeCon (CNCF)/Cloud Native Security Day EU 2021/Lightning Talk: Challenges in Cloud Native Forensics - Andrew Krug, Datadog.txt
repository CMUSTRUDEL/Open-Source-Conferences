Title: Lightning Talk: Challenges in Cloud Native Forensics - Andrew Krug, Datadog
Publication date: 2021-05-04
Playlist: Cloud Native Security Day EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Lightning Talk: Challenges in Cloud Native Forensics - Andrew Krug, Datadog

As more companies have gone cloud native the focus on resilience has largely focused on detection and speedy recovery. These are only two tactics that should be in the defense toolbox. Forensics is a discipline that has arguably suffered as log volumes and DevOps culture has become more normative. In my session, I’ll demonstrate where the gaps exist and how the ecosystem could improve capabilities around the art of forensics.
Captions: 
	00:00:00,399 --> 00:00:05,040
welcome to challenges in cloud native

00:00:02,480 --> 00:00:05,600
forensics here at cloud native security

00:00:05,040 --> 00:00:09,120
day

00:00:05,600 --> 00:00:10,880
happy to be here i'm andrew krug i'm a

00:00:09,120 --> 00:00:13,759
security geek at datadog

00:00:10,880 --> 00:00:15,280
and also a technical evangelist and up

00:00:13,759 --> 00:00:17,039
on screen here is of course where you

00:00:15,280 --> 00:00:18,320
can reach me if you have questions or

00:00:17,039 --> 00:00:22,400
feedback

00:00:18,320 --> 00:00:25,439
about the talk i started my career

00:00:22,400 --> 00:00:26,480
in forensics a long time ago

00:00:25,439 --> 00:00:28,800
i actually had the pleasure of

00:00:26,480 --> 00:00:29,760
participating in a college program to

00:00:28,800 --> 00:00:32,239
teach folks

00:00:29,760 --> 00:00:33,680
industrial and law enforcement style

00:00:32,239 --> 00:00:36,000
forensics

00:00:33,680 --> 00:00:37,760
in today's time there may be a variety

00:00:36,000 --> 00:00:38,559
of reasons that you have to perform

00:00:37,760 --> 00:00:40,640
forensics

00:00:38,559 --> 00:00:43,200
two primary domains being of course

00:00:40,640 --> 00:00:45,000
security and legal incidents

00:00:43,200 --> 00:00:46,960
things such as breaches due to

00:00:45,000 --> 00:00:47,440
misconfigurations which we see all the

00:00:46,960 --> 00:00:50,480
time

00:00:47,440 --> 00:00:52,879
or even incidents like employment law

00:00:50,480 --> 00:00:54,160
related things will force you to sift

00:00:52,879 --> 00:00:56,879
through

00:00:54,160 --> 00:00:58,239
logs and disk images and memory samples

00:00:56,879 --> 00:01:00,399
from your environment

00:00:58,239 --> 00:01:01,840
regardless of the reason though i tend

00:01:00,399 --> 00:01:05,360
to think of forensics

00:01:01,840 --> 00:01:06,400
as kind of the definition in that purple

00:01:05,360 --> 00:01:08,159
box there which is

00:01:06,400 --> 00:01:10,000
telling stories that occurred in a

00:01:08,159 --> 00:01:13,280
specific time window using

00:01:10,000 --> 00:01:16,159
facts that can be derived provably and

00:01:13,280 --> 00:01:16,159
repeatedly

00:01:16,400 --> 00:01:19,759
so let's just talk for a minute about

00:01:17,840 --> 00:01:23,439
what it means for a process to be

00:01:19,759 --> 00:01:25,680
provable and also repeatable

00:01:23,439 --> 00:01:27,759
we used to talk about this as using

00:01:25,680 --> 00:01:31,119
things that we called validated

00:01:27,759 --> 00:01:33,439
tooling and also keeping track

00:01:31,119 --> 00:01:35,119
of the chain of custody for pieces of

00:01:33,439 --> 00:01:37,600
evidence and tool validation

00:01:35,119 --> 00:01:39,360
put simply is just that we can use a

00:01:37,600 --> 00:01:40,880
tool over and over and that tool has

00:01:39,360 --> 00:01:42,880
been studied and proven

00:01:40,880 --> 00:01:45,360
to yield the same results hundreds or

00:01:42,880 --> 00:01:46,640
thousands of times in in academic tests

00:01:45,360 --> 00:01:49,920
in stress tests

00:01:46,640 --> 00:01:52,000
and also proven not to modify the actual

00:01:49,920 --> 00:01:53,759
artifacts that we've gathered as pieces

00:01:52,000 --> 00:01:56,799
of evidence because you might think of

00:01:53,759 --> 00:01:58,719
how important that actually is to have

00:01:56,799 --> 00:02:00,240
your artifacts be the same at the

00:01:58,719 --> 00:02:02,479
beginning of your investigation as they

00:02:00,240 --> 00:02:04,320
are at the end because we can prove

00:02:02,479 --> 00:02:06,000
beyond the shadow of a doubt that we

00:02:04,320 --> 00:02:08,160
haven't actually

00:02:06,000 --> 00:02:10,000
sort of fussed with the the evidence

00:02:08,160 --> 00:02:11,599
that we have available

00:02:10,000 --> 00:02:13,840
and then that chain of custody is just a

00:02:11,599 --> 00:02:15,120
complete record of of who checked out

00:02:13,840 --> 00:02:17,840
what piece of evidence

00:02:15,120 --> 00:02:19,200
who may have modified it uh etc so that

00:02:17,840 --> 00:02:20,239
we can go back and we can see the

00:02:19,200 --> 00:02:22,800
complete log

00:02:20,239 --> 00:02:24,000
of just what happened over the course of

00:02:22,800 --> 00:02:26,000
an investigation

00:02:24,000 --> 00:02:28,000
this is one of the things that's notably

00:02:26,000 --> 00:02:30,000
gotten a tiny bit easier with cloud

00:02:28,000 --> 00:02:33,519
provider control plane

00:02:30,000 --> 00:02:35,040
logs like aws cloudtrail

00:02:33,519 --> 00:02:36,879
which brings us to kind of the problem

00:02:35,040 --> 00:02:39,280
statement here which is that

00:02:36,879 --> 00:02:40,480
the more we embrace kind of this idea of

00:02:39,280 --> 00:02:43,680
devops and

00:02:40,480 --> 00:02:45,680
and cattle not pets the more

00:02:43,680 --> 00:02:48,000
challenged uh some of these forensic

00:02:45,680 --> 00:02:50,319
processes do become

00:02:48,000 --> 00:02:52,160
and that's due to like kind of three

00:02:50,319 --> 00:02:54,239
different distinct

00:02:52,160 --> 00:02:55,680
pillars uh that continue to challenge us

00:02:54,239 --> 00:02:56,879
when we talk about cloud native

00:02:55,680 --> 00:03:00,239
forensics

00:02:56,879 --> 00:03:02,480
ephemerality or short-lived instances

00:03:00,239 --> 00:03:03,760
scale which is of course the the number

00:03:02,480 --> 00:03:05,440
of workloads that we might have to

00:03:03,760 --> 00:03:07,680
perform forensics on

00:03:05,440 --> 00:03:10,239
and also the scope maybe it's a single

00:03:07,680 --> 00:03:11,760
aws account or one kubernetes cluster or

00:03:10,239 --> 00:03:13,280
it could be hundreds of kubernetes

00:03:11,760 --> 00:03:16,239
clusters depending

00:03:13,280 --> 00:03:19,040
and also technology um the the very

00:03:16,239 --> 00:03:22,319
technologies that we put in place to

00:03:19,040 --> 00:03:23,120
help us do security sometimes actually

00:03:22,319 --> 00:03:24,080
hinder us

00:03:23,120 --> 00:03:27,599
and we're going to talk a little bit

00:03:24,080 --> 00:03:29,440
about that but first let's talk about

00:03:27,599 --> 00:03:31,040
those short-lived instances or

00:03:29,440 --> 00:03:34,560
short-lived uh workloads

00:03:31,040 --> 00:03:36,640
ephemeral workloads

00:03:34,560 --> 00:03:37,680
i still remember a time not that long

00:03:36,640 --> 00:03:39,519
ago when you used to put

00:03:37,680 --> 00:03:41,440
servers in racks and you installed

00:03:39,519 --> 00:03:43,920
operating systems on them

00:03:41,440 --> 00:03:45,440
and then they ran for five years before

00:03:43,920 --> 00:03:48,080
unracking

00:03:45,440 --> 00:03:48,799
today we have much shorter and shorter

00:03:48,080 --> 00:03:51,840
lived

00:03:48,799 --> 00:03:52,400
workloads and what you're seeing on

00:03:51,840 --> 00:03:55,599
screen is

00:03:52,400 --> 00:03:56,879
is some some facts from a datadog report

00:03:55,599 --> 00:03:59,760
which is how long

00:03:56,879 --> 00:04:01,599
uh different uh classes of workloads

00:03:59,760 --> 00:04:03,439
exist so in serverless compute you might

00:04:01,599 --> 00:04:05,840
have a workload that lives for

00:04:03,439 --> 00:04:07,439
four minutes um in orchestrated

00:04:05,840 --> 00:04:10,720
containers they might live for

00:04:07,439 --> 00:04:12,560
for half a day to a day and

00:04:10,720 --> 00:04:14,319
in unorchestrated containers they might

00:04:12,560 --> 00:04:16,160
live you know for like something like

00:04:14,319 --> 00:04:18,479
four to six days still

00:04:16,160 --> 00:04:19,840
that's not very long compared to the

00:04:18,479 --> 00:04:22,479
span of years

00:04:19,840 --> 00:04:23,280
that uh something might be uh in

00:04:22,479 --> 00:04:26,479
production

00:04:23,280 --> 00:04:29,600
providing compute uh versus today

00:04:26,479 --> 00:04:31,919
so we're cycling things out quite a bit

00:04:29,600 --> 00:04:33,919
faster

00:04:31,919 --> 00:04:36,400
and that actually is a problem for

00:04:33,919 --> 00:04:38,560
forensics because it destroys evidence

00:04:36,400 --> 00:04:40,320
um when when you do a deploy your

00:04:38,560 --> 00:04:40,639
containers go away when you do a ploy

00:04:40,320 --> 00:04:43,199
your

00:04:40,639 --> 00:04:44,960
deploy your ec2 instances go away when

00:04:43,199 --> 00:04:47,840
your auto scale group

00:04:44,960 --> 00:04:49,440
expands and contracts we're actually

00:04:47,840 --> 00:04:52,720
throwing away

00:04:49,440 --> 00:04:54,160
valuable instance data

00:04:52,720 --> 00:04:55,919
and according to google's state of

00:04:54,160 --> 00:04:56,840
devops report the average company

00:04:55,919 --> 00:04:59,600
deploys

00:04:56,840 --> 00:05:00,720
626 times a year that's that's almost

00:04:59,600 --> 00:05:03,759
twice a day

00:05:00,720 --> 00:05:04,160
for kind of a medium competency devops

00:05:03,759 --> 00:05:06,000
shop

00:05:04,160 --> 00:05:08,160
that that's quite frequently and

00:05:06,000 --> 00:05:10,639
considering that the mean time to detect

00:05:08,160 --> 00:05:12,479
an incident or a breach could be

00:05:10,639 --> 00:05:14,560
weeks or months depending on how you

00:05:12,479 --> 00:05:16,479
find out about a breach

00:05:14,560 --> 00:05:18,080
you could already have kind of gotten

00:05:16,479 --> 00:05:19,680
rid of all of the evidence that would

00:05:18,080 --> 00:05:20,320
help you kind of break the case and

00:05:19,680 --> 00:05:22,080
figure out

00:05:20,320 --> 00:05:23,360
you know exactly how somebody got in

00:05:22,080 --> 00:05:26,400
what they got out

00:05:23,360 --> 00:05:28,240
etc so that's kind of a problem and

00:05:26,400 --> 00:05:31,919
scale adds another dimension

00:05:28,240 --> 00:05:34,320
to this as well so most environments

00:05:31,919 --> 00:05:35,840
running orchestrated containers or any

00:05:34,320 --> 00:05:39,039
sort of orchestration

00:05:35,840 --> 00:05:42,160
on top of aws or gcp

00:05:39,039 --> 00:05:44,560
or azure are running a lot of workloads

00:05:42,160 --> 00:05:46,960
and this is just a statistic from one

00:05:44,560 --> 00:05:48,880
single report so take it for what it is

00:05:46,960 --> 00:05:50,320
the bottom line though is that most

00:05:48,880 --> 00:05:52,880
environments are

00:05:50,320 --> 00:05:54,400
large environments kind of by design now

00:05:52,880 --> 00:05:56,880
and many are multi-cloud

00:05:54,400 --> 00:05:58,160
or multi-tenant they have containers and

00:05:56,880 --> 00:06:00,560
ec2 instances

00:05:58,160 --> 00:06:02,639
and this makes the evidence collection

00:06:00,560 --> 00:06:03,440
and custody chain problem even more

00:06:02,639 --> 00:06:06,639
prominent

00:06:03,440 --> 00:06:08,479
than it has ever been and

00:06:06,639 --> 00:06:10,160
for years now i've been doing talks on

00:06:08,479 --> 00:06:11,520
forensics in the cloud using cloud

00:06:10,160 --> 00:06:12,479
technology because i'm convinced that

00:06:11,520 --> 00:06:14,800
the only way

00:06:12,479 --> 00:06:16,479
to actually do incident response in the

00:06:14,800 --> 00:06:17,199
cloud and do this kind of analysis in

00:06:16,479 --> 00:06:20,240
the cloud is

00:06:17,199 --> 00:06:23,199
actually to use cloud compute

00:06:20,240 --> 00:06:24,720
to kind of scale out that analysis

00:06:23,199 --> 00:06:26,560
effort in the same way that we scale our

00:06:24,720 --> 00:06:28,800
workloads

00:06:26,560 --> 00:06:30,560
at some point though this becomes this

00:06:28,800 --> 00:06:31,919
incredibly heavy burden

00:06:30,560 --> 00:06:34,000
if you need to start to collect data

00:06:31,919 --> 00:06:38,720
from hundreds of instances or thousands

00:06:34,000 --> 00:06:41,360
of systems in a single fleet all at once

00:06:38,720 --> 00:06:42,000
and the pace of technology here that the

00:06:41,360 --> 00:06:44,479
tooling

00:06:42,000 --> 00:06:46,160
that allows us to do forensics isn't

00:06:44,479 --> 00:06:48,000
really keeping up necessarily with the

00:06:46,160 --> 00:06:49,759
security controls that are kind of

00:06:48,000 --> 00:06:51,520
preventing us from doing an effective

00:06:49,759 --> 00:06:54,639
job doing

00:06:51,520 --> 00:06:54,639
timeline reconstruction

00:06:55,039 --> 00:06:58,479
back in the good old days we talked

00:06:56,720 --> 00:06:59,840
about two different types of forensics

00:06:58,479 --> 00:07:00,560
right and and i want you just to

00:06:59,840 --> 00:07:01,919
remember these

00:07:00,560 --> 00:07:03,919
two distinctly different types of

00:07:01,919 --> 00:07:06,400
forensics one was what we called

00:07:03,919 --> 00:07:07,680
cold forensics or dead system which was

00:07:06,400 --> 00:07:09,599
where we'd actually take

00:07:07,680 --> 00:07:10,880
hard drives and take them out of systems

00:07:09,599 --> 00:07:13,199
image them

00:07:10,880 --> 00:07:14,240
carve the the actual file system for

00:07:13,199 --> 00:07:15,919
deleted files

00:07:14,240 --> 00:07:17,280
artifacts that would help us reconstruct

00:07:15,919 --> 00:07:20,639
a timeline

00:07:17,280 --> 00:07:22,319
the second is live forensics and that's

00:07:20,639 --> 00:07:25,199
the act of getting things from

00:07:22,319 --> 00:07:26,479
volatile system data that would go away

00:07:25,199 --> 00:07:27,520
when you turn the machine off and this

00:07:26,479 --> 00:07:28,800
wasn't just memory

00:07:27,520 --> 00:07:30,960
there's also things like network

00:07:28,800 --> 00:07:32,240
information process information

00:07:30,960 --> 00:07:34,720
things that wouldn't necessarily be

00:07:32,240 --> 00:07:36,400
resident on disk and for a time

00:07:34,720 --> 00:07:38,160
we thought this was actually going to be

00:07:36,400 --> 00:07:39,599
this massive boon

00:07:38,160 --> 00:07:42,160
to the industry right because we could

00:07:39,599 --> 00:07:44,560
do this so much faster

00:07:42,160 --> 00:07:45,199
using live memory samples to like crack

00:07:44,560 --> 00:07:48,879
the case

00:07:45,199 --> 00:07:51,759
then we could carve through a disk um

00:07:48,879 --> 00:07:53,520
it turned out though that despite the

00:07:51,759 --> 00:07:54,080
efficacy of this and the fact that we

00:07:53,520 --> 00:07:55,520
could

00:07:54,080 --> 00:07:57,840
we could use it to solve problems we

00:07:55,520 --> 00:08:01,520
couldn't use uh

00:07:57,840 --> 00:08:03,520
disk forensics for it was short-lived

00:08:01,520 --> 00:08:05,039
because operating systems started to add

00:08:03,520 --> 00:08:07,120
more and more security features

00:08:05,039 --> 00:08:09,039
uh kind of beginning in 2010 with data

00:08:07,120 --> 00:08:11,440
execution prevention

00:08:09,039 --> 00:08:12,639
and address space layout randomization

00:08:11,440 --> 00:08:15,680
which would take

00:08:12,639 --> 00:08:17,360
the address at which a given process was

00:08:15,680 --> 00:08:20,240
loaded into memory and sort of

00:08:17,360 --> 00:08:21,680
uh offset that randomly at any given

00:08:20,240 --> 00:08:23,360
time and then

00:08:21,680 --> 00:08:25,199
kernel level address space layout

00:08:23,360 --> 00:08:26,879
randomization came to be a thing

00:08:25,199 --> 00:08:28,400
and that was actually changing the

00:08:26,879 --> 00:08:31,759
location of pid

00:08:28,400 --> 00:08:34,560
0 or the kernel which made it inherently

00:08:31,759 --> 00:08:36,479
very very difficult to reconstruct

00:08:34,560 --> 00:08:39,760
memory samples

00:08:36,479 --> 00:08:42,479
from systems that were running k-a-s-l-r

00:08:39,760 --> 00:08:44,320
and this effectively broke an entire

00:08:42,479 --> 00:08:47,680
ecosystem of tools

00:08:44,320 --> 00:08:49,920
when the security features dropped and

00:08:47,680 --> 00:08:50,959
there are a variety of good tools that

00:08:49,920 --> 00:08:52,720
exist they just

00:08:50,959 --> 00:08:54,160
can't necessarily keep up with all of

00:08:52,720 --> 00:08:55,839
the innovations going on with kind of

00:08:54,160 --> 00:08:56,720
the randomization of memory and now if

00:08:55,839 --> 00:08:59,760
you look at

00:08:56,720 --> 00:09:02,000
hardware they're actually putting memory

00:08:59,760 --> 00:09:04,080
obfuscation features and hardware that

00:09:02,000 --> 00:09:06,560
makes it increasingly difficult

00:09:04,080 --> 00:09:07,680
to analyze memory samples and so often

00:09:06,560 --> 00:09:09,760
what we end up with

00:09:07,680 --> 00:09:11,760
even if we do everything right is a

00:09:09,760 --> 00:09:14,880
complete lack of ability

00:09:11,760 --> 00:09:16,560
to analyze live system data

00:09:14,880 --> 00:09:18,480
at any kind of practical scale

00:09:16,560 --> 00:09:21,680
especially as memory sizes increase

00:09:18,480 --> 00:09:25,360
instant sizes increase etc so

00:09:21,680 --> 00:09:27,519
we really need to make the art of

00:09:25,360 --> 00:09:29,360
forensics a first-class citizen

00:09:27,519 --> 00:09:31,600
we need to be thinking about this as we

00:09:29,360 --> 00:09:34,000
build out operating systems

00:09:31,600 --> 00:09:34,640
as we build out orchestrators that

00:09:34,000 --> 00:09:36,480
eventually

00:09:34,640 --> 00:09:38,000
during a security incident someone is

00:09:36,480 --> 00:09:40,480
going to need to kind of

00:09:38,000 --> 00:09:42,640
lawfully intercept if you will the data

00:09:40,480 --> 00:09:45,120
that's flowing through that system

00:09:42,640 --> 00:09:46,160
in a way that they can reconstruct it

00:09:45,120 --> 00:09:48,160
and that needs to be

00:09:46,160 --> 00:09:49,760
provable and validatable just like we

00:09:48,160 --> 00:09:50,000
talked about so if you think this is a

00:09:49,760 --> 00:09:53,440
good

00:09:50,000 --> 00:09:55,440
idea please go and plus one github

00:09:53,440 --> 00:09:56,320
issues on any of those projects i put on

00:09:55,440 --> 00:10:00,080
screen

00:09:56,320 --> 00:10:01,600
that have to do with aslr or kaslr

00:10:00,080 --> 00:10:03,440
or if you are on the board of a

00:10:01,600 --> 00:10:04,800
prominent project like kubernetes or the

00:10:03,440 --> 00:10:06,560
linux kernel

00:10:04,800 --> 00:10:08,720
let's have a chat let's think about

00:10:06,560 --> 00:10:10,800
forensics a little bit differently

00:10:08,720 --> 00:10:11,920
you too can be an advocate for all

00:10:10,800 --> 00:10:13,760
things forensics

00:10:11,920 --> 00:10:16,160
and we can work together to make the

00:10:13,760 --> 00:10:18,000
world a better place

00:10:16,160 --> 00:10:19,920
thanks again i'm andrew krug here's

00:10:18,000 --> 00:10:22,160
where to contact me i hope you enjoyed

00:10:19,920 --> 00:10:25,440
my lightning talk on concerns with cloud

00:10:22,160 --> 00:10:25,440

YouTube URL: https://www.youtube.com/watch?v=MkGeVWeVGJA


