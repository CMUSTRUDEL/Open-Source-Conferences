Title: Allocating Less: Really Thin Rust Cloud Apps - Evan Chan, UrbanLogiq
Publication date: 2021-05-03
Playlist: Cloud Native Rust Day EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Allocating Less: Really Thin Rust Cloud Apps - Evan Chan, UrbanLogiq

In this talk I discuss in depth one of the key selling points of Rust for cloud native apps: creating really thin and fast apps or infrastructure layers.  To do this, it is important to review and examine the memory allocation choices that we can make in Rust, and how to optimize them and data structures to ensure our Rust apps can reach the ceiling they are capable of.

- Why use Rust for thin cloud apps?
- Review memory usage in Rust
- How to profile Rust apps for memory usage
- Where to look for memory allocations in Rust apps
- Improving rust heap usage through better APIs, improving serialization, getting rid of clones, flattening data structures, etc.
- The effect of switching memory allocators (ie jemalloc) and why you might want to, or not
- A study of JSON processors: memory usage
- Different benchmarking stats to use for memory
- Traits and trait objects: how to avoid allocations and speed up performance (enum-dispatch and friends)
Captions: 
	00:00:00,160 --> 00:00:04,319
hi everyone my name is evan chan i'm a

00:00:02,800 --> 00:00:07,440
senior data engineer at

00:00:04,319 --> 00:00:10,000
urban logic and welcome to my

00:00:07,440 --> 00:00:10,000
presentation

00:00:11,120 --> 00:00:16,960
so urban logic we are

00:00:14,160 --> 00:00:18,560
an online platform that provides uh

00:00:16,960 --> 00:00:20,640
insights

00:00:18,560 --> 00:00:22,240
and we use thrust and machine learning

00:00:20,640 --> 00:00:26,000
amongst other things

00:00:22,240 --> 00:00:28,080
to give really great

00:00:26,000 --> 00:00:29,199
insights for transportation economic

00:00:28,080 --> 00:00:32,240
development and other

00:00:29,199 --> 00:00:32,240
community use cases

00:00:32,800 --> 00:00:40,239
so why thin cloud apps

00:00:37,360 --> 00:00:41,280
if we look at the progression of

00:00:40,239 --> 00:00:44,640
technology

00:00:41,280 --> 00:00:47,039
through the recent years we can see that

00:00:44,640 --> 00:00:48,719
we have moved from in the old days we

00:00:47,039 --> 00:00:51,039
used virtual machines

00:00:48,719 --> 00:00:52,000
to uh now we're using containers

00:00:51,039 --> 00:00:54,320
everywhere

00:00:52,000 --> 00:00:55,360
and we can see technologies on the

00:00:54,320 --> 00:00:57,680
horizon

00:00:55,360 --> 00:00:59,359
that are coming on fast such as

00:00:57,680 --> 00:01:01,359
serverless and web assembly

00:00:59,359 --> 00:01:04,159
and what we notice in this trend is that

00:01:01,359 --> 00:01:08,000
cloud infrastructure is getting smaller

00:01:04,159 --> 00:01:10,000
thinner and more concurrent right so

00:01:08,000 --> 00:01:11,520
things that maybe didn't used to be

00:01:10,000 --> 00:01:13,439
quite as important

00:01:11,520 --> 00:01:15,280
such as memory and allocations are

00:01:13,439 --> 00:01:16,960
becoming more important

00:01:15,280 --> 00:01:18,640
because when each unit is smaller you

00:01:16,960 --> 00:01:21,040
need to be more efficient

00:01:18,640 --> 00:01:21,920
and you know we also live in a data-rich

00:01:21,040 --> 00:01:23,840
world so

00:01:21,920 --> 00:01:26,000
we need to process more and more data so

00:01:23,840 --> 00:01:27,600
that has to be more and more efficient

00:01:26,000 --> 00:01:29,119
and finally using less memory it's more

00:01:27,600 --> 00:01:32,799
eco-friendly

00:01:29,119 --> 00:01:35,119
right so

00:01:32,799 --> 00:01:36,159
the the question for everyone here we're

00:01:35,119 --> 00:01:39,759
here to learn about

00:01:36,159 --> 00:01:41,119
rust is why use rust for thin cloud apps

00:01:39,759 --> 00:01:43,040
and

00:01:41,119 --> 00:01:45,119
they are actually mainly reasons for

00:01:43,040 --> 00:01:46,320
which i personally came to ross i came

00:01:45,119 --> 00:01:48,960
to russ from

00:01:46,320 --> 00:01:49,520
writing a distributed in-memory database

00:01:48,960 --> 00:01:53,520
called

00:01:49,520 --> 00:01:58,560
filodb which was on the jvm and

00:01:53,520 --> 00:02:02,000
um i came for the no compromise

00:01:58,560 --> 00:02:03,200
aspect of rust that it was so to me that

00:02:02,000 --> 00:02:06,079
you could get performance

00:02:03,200 --> 00:02:06,799
safety and abstractions at the same time

00:02:06,079 --> 00:02:09,280
usually

00:02:06,799 --> 00:02:10,959
you know you have to choose you know one

00:02:09,280 --> 00:02:14,319
or two of these

00:02:10,959 --> 00:02:16,480
and um i found that it was

00:02:14,319 --> 00:02:18,560
uh pretty much mostly true that you you

00:02:16,480 --> 00:02:21,360
could have all three things

00:02:18,560 --> 00:02:22,000
and also with rust you get great control

00:02:21,360 --> 00:02:26,080
over

00:02:22,000 --> 00:02:29,440
memory usage your allocations and

00:02:26,080 --> 00:02:31,120
you have many ways to you could even opt

00:02:29,440 --> 00:02:33,040
out of using a standard library so

00:02:31,120 --> 00:02:36,080
there's many ways that you could

00:02:33,040 --> 00:02:38,800
use less memory and that small

00:02:36,080 --> 00:02:40,239
profile makes it very appropriate for

00:02:38,800 --> 00:02:43,840
writing everything from apps

00:02:40,239 --> 00:02:46,239
to oss kernel level code hypervisors and

00:02:43,840 --> 00:02:48,560
so on

00:02:46,239 --> 00:02:50,319
right however to take advantage of it

00:02:48,560 --> 00:02:53,519
you have to learn how to control

00:02:50,319 --> 00:02:56,160
and measure the allocations so let's

00:02:53,519 --> 00:02:56,160
dive right in

00:02:56,879 --> 00:03:02,400
how rust apps use memory so let's start

00:03:00,319 --> 00:03:05,840
by reviewing

00:03:02,400 --> 00:03:05,840
rust memory model really quickly

00:03:06,959 --> 00:03:13,519
on the left side you see uh what does

00:03:10,000 --> 00:03:16,159
rust put on the stack so they would be

00:03:13,519 --> 00:03:17,280
primitives uh structs so you have a

00:03:16,159 --> 00:03:19,599
couple different fields

00:03:17,280 --> 00:03:21,040
but but everything in the structure is

00:03:19,599 --> 00:03:25,120
fixed size

00:03:21,040 --> 00:03:27,920
you also have fixed size arrays and

00:03:25,120 --> 00:03:29,280
pointers references two things on the

00:03:27,920 --> 00:03:30,959
heap

00:03:29,280 --> 00:03:33,120
so you have some stuff to work with and

00:03:30,959 --> 00:03:34,480
then everything else that is dynamic is

00:03:33,120 --> 00:03:37,040
put on here

00:03:34,480 --> 00:03:38,080
such as vects which are lists and arrays

00:03:37,040 --> 00:03:42,400
things like strings

00:03:38,080 --> 00:03:44,480
and other more complex objects

00:03:42,400 --> 00:03:46,959
as well as some miscellaneous things

00:03:44,480 --> 00:03:49,360
that we'll go over later

00:03:46,959 --> 00:03:50,720
now one thing is that russ does not have

00:03:49,360 --> 00:03:53,360
a garbage collector

00:03:50,720 --> 00:03:54,959
so how does it manage memory now this

00:03:53,360 --> 00:03:57,200
this is a really important point

00:03:54,959 --> 00:03:59,280
for folks those of you coming from

00:03:57,200 --> 00:04:02,799
dynamic languages this is the biggest

00:03:59,280 --> 00:04:06,400
differentiator above rust what

00:04:02,799 --> 00:04:07,760
rust's promises is that

00:04:06,400 --> 00:04:09,840
even though it does not have a garbage

00:04:07,760 --> 00:04:12,239
collector it will track

00:04:09,840 --> 00:04:13,599
how your data is used through a concept

00:04:12,239 --> 00:04:16,880
known as a lifetime

00:04:13,599 --> 00:04:17,519
so to keep track of when your data is

00:04:16,880 --> 00:04:20,079
created

00:04:17,519 --> 00:04:21,519
when it is borrowed and used and it

00:04:20,079 --> 00:04:25,280
tries to prevent

00:04:21,519 --> 00:04:26,960
unsafe use and sharing of your data

00:04:25,280 --> 00:04:30,400
and it tracks when your data is no

00:04:26,960 --> 00:04:33,759
longer used so it knows when to free it

00:04:30,400 --> 00:04:34,720
right this talk is not going to be about

00:04:33,759 --> 00:04:36,080
this

00:04:34,720 --> 00:04:38,000
but but this will be important to

00:04:36,080 --> 00:04:40,400
remember when we talk about allocating

00:04:38,000 --> 00:04:40,400
memory

00:04:41,759 --> 00:04:45,759
so let's with that let's dive into some

00:04:44,240 --> 00:04:48,560
basic

00:04:45,759 --> 00:04:51,120
data structures that are used a lot and

00:04:48,560 --> 00:04:54,560
how much memory they actually represent

00:04:51,120 --> 00:04:57,040
so we start off with strings

00:04:54,560 --> 00:04:58,080
and vects so remember effect is a list

00:04:57,040 --> 00:05:02,000
of a fixed

00:04:58,080 --> 00:05:05,440
type of item so on the stack

00:05:02,000 --> 00:05:07,360
they use on 64-bit architectures like

00:05:05,440 --> 00:05:10,479
the x86 and modern

00:05:07,360 --> 00:05:12,080
r machines they use 24 bytes the pointer

00:05:10,479 --> 00:05:14,400
would be eight bytes

00:05:12,080 --> 00:05:16,000
and then there is one field for capacity

00:05:14,400 --> 00:05:19,759
which is how

00:05:16,000 --> 00:05:20,240
much how many items this data show can

00:05:19,759 --> 00:05:21,840
hold

00:05:20,240 --> 00:05:23,440
like that is how many characters for a

00:05:21,840 --> 00:05:25,280
string or how many items

00:05:23,440 --> 00:05:27,600
for a vac and then there's a length

00:05:25,280 --> 00:05:29,759
which is how many items it actually has

00:05:27,600 --> 00:05:30,880
right now so these are both global data

00:05:29,759 --> 00:05:34,400
structures

00:05:30,880 --> 00:05:37,039
now then the pointer would point to

00:05:34,400 --> 00:05:37,759
an area in the heap that actually holds

00:05:37,039 --> 00:05:41,759
the

00:05:37,759 --> 00:05:41,759
the items or the characters

00:05:42,400 --> 00:05:49,039
now the complement to vects and strings

00:05:45,759 --> 00:05:52,160
are slice

00:05:49,039 --> 00:05:54,479
pointers and

00:05:52,160 --> 00:05:55,759
string slices both of these are

00:05:54,479 --> 00:05:58,400
immutable

00:05:55,759 --> 00:06:00,240
they are the same as the previous ones

00:05:58,400 --> 00:06:01,039
except there are 16 bytes because you

00:06:00,240 --> 00:06:03,280
have

00:06:01,039 --> 00:06:05,199
one pointer and you have a length

00:06:03,280 --> 00:06:09,759
because these things cannot grow

00:06:05,199 --> 00:06:11,199
so they just point at a location and

00:06:09,759 --> 00:06:13,680
they tell you how many items they

00:06:11,199 --> 00:06:13,680
represent

00:06:14,560 --> 00:06:18,400
now let's look at a more complex data

00:06:17,520 --> 00:06:23,039
structure

00:06:18,400 --> 00:06:27,120
the hashmap so for a hashmap

00:06:23,039 --> 00:06:30,240
this is a bit more complex a hashmap

00:06:27,120 --> 00:06:32,080
can be implemented using buckets right

00:06:30,240 --> 00:06:33,600
so all of your items are hashed into a

00:06:32,080 --> 00:06:37,440
fixed number of buckets

00:06:33,600 --> 00:06:40,960
each bucket in turn can have

00:06:37,440 --> 00:06:42,960
one or more items in it when there is a

00:06:40,960 --> 00:06:45,680
collision

00:06:42,960 --> 00:06:46,960
so what we notice here is that the list

00:06:45,680 --> 00:06:50,000
of buckets

00:06:46,960 --> 00:06:53,199
is basically a vector of a bucket

00:06:50,000 --> 00:06:55,280
and in each slot a bucket you know that

00:06:53,199 --> 00:06:58,000
points to a bucket and then each bucket

00:06:55,280 --> 00:07:00,960
entry would be

00:06:58,000 --> 00:07:02,000
your key and value pairs for the items

00:07:00,960 --> 00:07:05,039
and if my hash map

00:07:02,000 --> 00:07:08,240
is a string key and a string index

00:07:05,039 --> 00:07:09,520
that means that i'm storing as the key

00:07:08,240 --> 00:07:14,160
and value

00:07:09,520 --> 00:07:18,240
entry in each bucket slot i'm using

00:07:14,160 --> 00:07:22,080
24 bytes for the key for the

00:07:18,240 --> 00:07:24,720
key string struct and 24 bytes for

00:07:22,080 --> 00:07:25,280
the value string structs so that added

00:07:24,720 --> 00:07:29,120
together

00:07:25,280 --> 00:07:31,199
you know gives you 48 right and

00:07:29,120 --> 00:07:33,280
now assuming that the bucket let's

00:07:31,199 --> 00:07:34,639
assume that each bucket has like one

00:07:33,280 --> 00:07:37,199
entry for the case where there are no

00:07:34,639 --> 00:07:39,599
collisions then if you add in

00:07:37,199 --> 00:07:40,560
you know the pointer for the bucket

00:07:39,599 --> 00:07:42,400
which is basically

00:07:40,560 --> 00:07:44,160
the bucket itself is like effect right

00:07:42,400 --> 00:07:45,759
it's like a list so

00:07:44,160 --> 00:07:48,240
that pointer actually takes up another

00:07:45,759 --> 00:07:52,560
24 bytes meaning the overhead is

00:07:48,240 --> 00:07:56,080
up to 72 bytes per entry

00:07:52,560 --> 00:07:58,560
um so one thing you need to be very

00:07:56,080 --> 00:07:59,680
very aware of is that for more complex

00:07:58,560 --> 00:08:02,080
data structures

00:07:59,680 --> 00:08:03,440
you have these nesting of pointers that

00:08:02,080 --> 00:08:06,800
can create

00:08:03,440 --> 00:08:10,000
a non-trivial amount of metadata now

00:08:06,800 --> 00:08:11,440
if your items like are large then maybe

00:08:10,000 --> 00:08:14,080
it is not a problem but

00:08:11,440 --> 00:08:17,680
if your items are small then you might

00:08:14,080 --> 00:08:17,680
want to be careful and think about that

00:08:19,360 --> 00:08:22,479
now where could you be allocating memory

00:08:21,280 --> 00:08:25,759
in your apps

00:08:22,479 --> 00:08:26,879
so we'll go over each of these items in

00:08:25,759 --> 00:08:30,319
detail later

00:08:26,879 --> 00:08:34,000
but um starting in no random order

00:08:30,319 --> 00:08:36,560
um one item that could take up a lot of

00:08:34,000 --> 00:08:38,479
memory is sterilization that creates a

00:08:36,560 --> 00:08:40,800
lot of temporary objects

00:08:38,479 --> 00:08:42,479
and others one is when you use traits or

00:08:40,800 --> 00:08:45,040
trait objects

00:08:42,479 --> 00:08:46,320
when you um we'll go over that in a

00:08:45,040 --> 00:08:49,600
minute

00:08:46,320 --> 00:08:54,320
and um anytime you use clone and

00:08:49,600 --> 00:08:57,760
data structures and and so forth

00:08:54,320 --> 00:09:00,160
now um you could look at your

00:08:57,760 --> 00:09:01,360
your app and go through and find out

00:09:00,160 --> 00:09:03,680
look at these users

00:09:01,360 --> 00:09:04,959
but maybe a better way is to benchmark

00:09:03,680 --> 00:09:06,880
right so

00:09:04,959 --> 00:09:08,640
there's two ways you could benchmark

00:09:06,880 --> 00:09:11,760
your apps

00:09:08,640 --> 00:09:12,800
right so one is to do dynamic memory

00:09:11,760 --> 00:09:16,399
analysis

00:09:12,800 --> 00:09:19,600
this looks at you know starting from t0

00:09:16,399 --> 00:09:22,720
the time when you start your app what

00:09:19,600 --> 00:09:23,360
is being allocated how much and it would

00:09:22,720 --> 00:09:26,000
track this

00:09:23,360 --> 00:09:26,399
over time and it would figure out hey

00:09:26,000 --> 00:09:28,880
how

00:09:26,399 --> 00:09:31,519
you know what what is the even if you

00:09:28,880 --> 00:09:33,839
allocate memory and free it

00:09:31,519 --> 00:09:35,680
you know where where are we allocating

00:09:33,839 --> 00:09:37,600
freeing memory to most where is what's

00:09:35,680 --> 00:09:41,200
the memory churn so that's a dynamic

00:09:37,600 --> 00:09:44,320
uh benchmarking it there's also

00:09:41,200 --> 00:09:46,800
static memory analysis

00:09:44,320 --> 00:09:48,720
a static heap analysis this you might be

00:09:46,800 --> 00:09:52,240
more familiar with if you come from

00:09:48,720 --> 00:09:52,880
a gc language like you know java and so

00:09:52,240 --> 00:09:55,040
forth

00:09:52,880 --> 00:09:56,640
where basically you could have an

00:09:55,040 --> 00:09:59,040
analyzer that walks your heap

00:09:56,640 --> 00:10:00,000
and figures out hey for a given point in

00:09:59,040 --> 00:10:02,640
time

00:10:00,000 --> 00:10:04,160
uh what memory is being used up what is

00:10:02,640 --> 00:10:07,279
using up the most memory

00:10:04,160 --> 00:10:09,760
now there are tools for doing dynamic

00:10:07,279 --> 00:10:11,120
analysis one is called keep track

00:10:09,760 --> 00:10:14,640
another one is called

00:10:11,120 --> 00:10:16,160
d-hat and i'll go over examples of d-hat

00:10:14,640 --> 00:10:18,560
static keyboard analysis is a bit more

00:10:16,160 --> 00:10:21,279
difficult in rust

00:10:18,560 --> 00:10:22,640
we can get overall memory usage pretty

00:10:21,279 --> 00:10:25,440
easily

00:10:22,640 --> 00:10:26,160
using something like general control and

00:10:25,440 --> 00:10:28,880
we can

00:10:26,160 --> 00:10:30,560
actually diff memory usage you can also

00:10:28,880 --> 00:10:31,760
profile data structure using something

00:10:30,560 --> 00:10:34,959
called deep size

00:10:31,760 --> 00:10:37,760
but but there isn't really anything

00:10:34,959 --> 00:10:40,160
um comprehensive like you have with the

00:10:37,760 --> 00:10:40,880
jvm but i'll show you some stuff you can

00:10:40,160 --> 00:10:44,839
use

00:10:40,880 --> 00:10:48,959
for that so

00:10:44,839 --> 00:10:51,440
um remember

00:10:48,959 --> 00:10:52,240
in rust that usually the more things

00:10:51,440 --> 00:10:54,880
that you type

00:10:52,240 --> 00:10:56,320
like box the more you alkane so that's

00:10:54,880 --> 00:10:59,760
just a fun hint

00:10:56,320 --> 00:11:03,120
to remember so let's now let's go over

00:10:59,760 --> 00:11:04,959
uh some potentially exponential uses of

00:11:03,120 --> 00:11:08,800
memory and how we can

00:11:04,959 --> 00:11:11,920
help reduce it

00:11:08,800 --> 00:11:14,959
so the first thing is

00:11:11,920 --> 00:11:16,399
look at your method signatures what are

00:11:14,959 --> 00:11:19,120
we passing in

00:11:16,399 --> 00:11:20,160
for example do you see function

00:11:19,120 --> 00:11:23,279
signatures

00:11:20,160 --> 00:11:24,880
like this where i pass an effect of

00:11:23,279 --> 00:11:26,560
string so this is quite common you know

00:11:24,880 --> 00:11:27,519
in order to process some string lists

00:11:26,560 --> 00:11:29,440
right

00:11:27,519 --> 00:11:31,839
so at first appearance you might like

00:11:29,440 --> 00:11:35,120
okay that's you know a nice signature

00:11:31,839 --> 00:11:37,440
but there's two problems when you ask

00:11:35,120 --> 00:11:38,959
the caller to pass in a back of strings

00:11:37,440 --> 00:11:40,320
you're basically forcing them to

00:11:38,959 --> 00:11:42,560
allocate

00:11:40,320 --> 00:11:44,160
twice right once for the back and once

00:11:42,560 --> 00:11:47,360
for each string

00:11:44,160 --> 00:11:50,720
instead if we're able to

00:11:47,360 --> 00:11:51,360
uh change the signature to uh point at a

00:11:50,720 --> 00:11:54,160
string

00:11:51,360 --> 00:11:55,120
string slices which is the second

00:11:54,160 --> 00:11:58,480
signature there

00:11:55,120 --> 00:12:02,079
where we have this you know ampersand

00:11:58,480 --> 00:12:03,839
and unstring this gives the caller two

00:12:02,079 --> 00:12:05,920
chances to avoid allocations

00:12:03,839 --> 00:12:07,120
one is that they can point at existing

00:12:05,920 --> 00:12:09,200
strings instead of

00:12:07,120 --> 00:12:10,320
allocating a new string that saves a

00:12:09,200 --> 00:12:12,959
whole bunch of memory

00:12:10,320 --> 00:12:14,480
and the second one is that they can pass

00:12:12,959 --> 00:12:18,399
in a string slice instead of

00:12:14,480 --> 00:12:19,760
a slice of strings instead of a back

00:12:18,399 --> 00:12:21,920
right and if you want even more

00:12:19,760 --> 00:12:25,519
flexibility you can

00:12:21,920 --> 00:12:26,480
change the signature to pass in an

00:12:25,519 --> 00:12:29,040
iterator

00:12:26,480 --> 00:12:29,519
which gives you the chance to for them

00:12:29,040 --> 00:12:32,079
to pass

00:12:29,519 --> 00:12:33,680
in even non-less data structures there's

00:12:32,079 --> 00:12:35,600
anything that can provide an iterator

00:12:33,680 --> 00:12:36,959
for even more flexibility so that gives

00:12:35,600 --> 00:12:40,880
you flexibility and

00:12:36,959 --> 00:12:40,880
gives you a way to avoid allocations

00:12:41,519 --> 00:12:47,839
so the um

00:12:44,800 --> 00:12:51,360
the next thing is

00:12:47,839 --> 00:12:54,160
that we can try to

00:12:51,360 --> 00:12:55,519
flatten our data structures like vac of

00:12:54,160 --> 00:12:57,440
string backpack

00:12:55,519 --> 00:12:58,639
and i'm not going to go over all of

00:12:57,440 --> 00:13:01,680
these

00:12:58,639 --> 00:13:02,480
but there's a bunch of crates that will

00:13:01,680 --> 00:13:05,440
help you there

00:13:02,480 --> 00:13:06,079
such as a nest it that will save you a

00:13:05,440 --> 00:13:08,480
lot of

00:13:06,079 --> 00:13:09,839
storage if you're if you're trying to

00:13:08,480 --> 00:13:11,600
have a bunch of

00:13:09,839 --> 00:13:13,040
a list of strings for example and

00:13:11,600 --> 00:13:14,880
there's a whole bunch of crates that can

00:13:13,040 --> 00:13:17,680
help you with

00:13:14,880 --> 00:13:18,320
strings that are basically inlineable

00:13:17,680 --> 00:13:19,680
where

00:13:18,320 --> 00:13:21,519
when you have strings below a certain

00:13:19,680 --> 00:13:24,880
size they will be on the

00:13:21,519 --> 00:13:27,040
heat on the stack instead of the heat

00:13:24,880 --> 00:13:28,880
as well as things like small small back

00:13:27,040 --> 00:13:32,240
so there are

00:13:28,880 --> 00:13:35,279
ones we're doing smaller data structures

00:13:32,240 --> 00:13:38,720
and i did a test using a repo

00:13:35,279 --> 00:13:40,560
that you can feel free to visit where i

00:13:38,720 --> 00:13:42,000
show that by using nested instead of

00:13:40,560 --> 00:13:45,760
effect you can save

00:13:42,000 --> 00:13:49,839
you know like say 25 um you know total

00:13:45,760 --> 00:13:49,839
memory allocated

00:13:50,240 --> 00:13:58,480
so another area

00:13:54,240 --> 00:14:00,880
is by reducing clones you might notice

00:13:58,480 --> 00:14:01,680
many of you are writing code using async

00:14:00,880 --> 00:14:03,360
this is a

00:14:01,680 --> 00:14:04,959
really popular feature of rust now you

00:14:03,360 --> 00:14:07,360
can write code that

00:14:04,959 --> 00:14:09,360
um you know forks off work and you can

00:14:07,360 --> 00:14:11,279
do it away to wait wait for it which is

00:14:09,360 --> 00:14:13,120
which is great you might find yourself

00:14:11,279 --> 00:14:14,399
however having to clone a lot of data

00:14:13,120 --> 00:14:16,480
structures when you're calling

00:14:14,399 --> 00:14:17,519
your async functions and these

00:14:16,480 --> 00:14:21,440
enclosures

00:14:17,519 --> 00:14:22,959
because the data that is passed into a

00:14:21,440 --> 00:14:23,920
sync because it's a future and could run

00:14:22,959 --> 00:14:27,120
on another thread

00:14:23,920 --> 00:14:28,959
it needs to be thread safe one

00:14:27,120 --> 00:14:30,480
there's some quick tips one is to

00:14:28,959 --> 00:14:34,320
consider using arc

00:14:30,480 --> 00:14:36,320
instead of clone and this is something

00:14:34,320 --> 00:14:37,839
that makes sense especially for things

00:14:36,320 --> 00:14:39,360
like lists things that where you could

00:14:37,839 --> 00:14:42,880
pass in a lot of items

00:14:39,360 --> 00:14:44,880
clone will usually do a deep clone

00:14:42,880 --> 00:14:46,240
what has to do deep clone is to clone

00:14:44,880 --> 00:14:47,199
every item so that could be quite

00:14:46,240 --> 00:14:50,320
expensive

00:14:47,199 --> 00:14:53,040
using arc does cost you

00:14:50,320 --> 00:14:54,000
a an atomic you know couple atomic

00:14:53,040 --> 00:14:57,760
operations

00:14:54,000 --> 00:15:00,240
and it saves you a lot of memory another

00:14:57,760 --> 00:15:00,959
idea is to use something like an actor

00:15:00,240 --> 00:15:02,880
pattern

00:15:00,959 --> 00:15:04,639
this is where you try to keep your state

00:15:02,880 --> 00:15:06,880
local instead of

00:15:04,639 --> 00:15:08,000
passing your state around and so you

00:15:06,880 --> 00:15:09,920
keep your data

00:15:08,000 --> 00:15:12,160
structures within each actor or

00:15:09,920 --> 00:15:14,079
equivalently within each thread and you

00:15:12,160 --> 00:15:17,279
use channels to communicate

00:15:14,079 --> 00:15:19,199
and you pass you know small messages and

00:15:17,279 --> 00:15:21,360
events only so that's a pattern that can

00:15:19,199 --> 00:15:23,839
help and it has other benefits as well

00:15:21,360 --> 00:15:25,120
finally we consider using something like

00:15:23,839 --> 00:15:26,880
cow

00:15:25,120 --> 00:15:29,440
for example if you want to escape

00:15:26,880 --> 00:15:34,000
strings such as you want to

00:15:29,440 --> 00:15:35,440
um like for urls or something else

00:15:34,000 --> 00:15:36,880
where a lot of times the string is not

00:15:35,440 --> 00:15:37,680
changed but sometimes you need to create

00:15:36,880 --> 00:15:39,040
a new copy

00:15:37,680 --> 00:15:41,360
well instead of creating a new copy

00:15:39,040 --> 00:15:44,560
every time you can just copy only

00:15:41,360 --> 00:15:44,560
on right right

00:15:45,519 --> 00:15:50,639
well so how slow is arc really right

00:15:49,040 --> 00:15:52,399
like in case you're worried about using

00:15:50,639 --> 00:15:54,800
arc instead of cloning well

00:15:52,399 --> 00:15:56,240
well if the data is anything concise

00:15:54,800 --> 00:15:58,880
it's you know it is

00:15:56,240 --> 00:15:59,440
fast it is you know almost always faster

00:15:58,880 --> 00:16:01,600
uh

00:15:59,440 --> 00:16:03,440
actually but basically arc is just an

00:16:01,600 --> 00:16:04,959
atomic increment on the clone and at the

00:16:03,440 --> 00:16:08,079
top of the document on drop

00:16:04,959 --> 00:16:10,399
and roughly on an x86 they estimate that

00:16:08,079 --> 00:16:12,320
this is between 30 and 120 nanoseconds

00:16:10,399 --> 00:16:16,320
depending on which level of cache

00:16:12,320 --> 00:16:16,320
it might be faster and other hardware

00:16:16,720 --> 00:16:23,839
so now

00:16:20,720 --> 00:16:24,800
here's another area where you might be

00:16:23,839 --> 00:16:26,800
using memory is

00:16:24,800 --> 00:16:28,320
you might find that you have a signature

00:16:26,800 --> 00:16:29,279
like this where you're processing some

00:16:28,320 --> 00:16:31,120
item

00:16:29,279 --> 00:16:33,360
and you know you want to pass the

00:16:31,120 --> 00:16:35,680
different implementations of traits

00:16:33,360 --> 00:16:36,800
right so you make your signature have

00:16:35,680 --> 00:16:40,240
this dying

00:16:36,800 --> 00:16:41,839
keyword for dying my trade right and

00:16:40,240 --> 00:16:43,600
now in order to be able to pass that

00:16:41,839 --> 00:16:45,120
function usually you need to box it

00:16:43,600 --> 00:16:46,639
which means you need to allocate some

00:16:45,120 --> 00:16:48,560
heap memory for that

00:16:46,639 --> 00:16:50,800
unfortunately that means that every time

00:16:48,560 --> 00:16:53,120
you're calling this method you're

00:16:50,800 --> 00:16:55,199
you're doing this allocation which is

00:16:53,120 --> 00:16:58,079
you know not the fastest thing

00:16:55,199 --> 00:16:58,720
right so especially in a hot loop one

00:16:58,079 --> 00:17:01,120
trade

00:16:58,720 --> 00:17:03,279
uh one crate sorry that you can use that

00:17:01,120 --> 00:17:06,319
helps a lot is called enum dispatch

00:17:03,279 --> 00:17:08,959
which is really great what it does

00:17:06,319 --> 00:17:11,039
is if all of your trade implementations

00:17:08,959 --> 00:17:15,199
are within your control

00:17:11,039 --> 00:17:17,120
you can make it in any num and

00:17:15,199 --> 00:17:18,480
all of your implementations of my

00:17:17,120 --> 00:17:21,679
behavior in this case

00:17:18,480 --> 00:17:24,959
are in this enum might be really numb

00:17:21,679 --> 00:17:26,480
what we do is that we would annotate it

00:17:24,959 --> 00:17:28,559
with enum dispatch

00:17:26,480 --> 00:17:29,520
and in them this patch will will

00:17:28,559 --> 00:17:32,640
magically

00:17:29,520 --> 00:17:36,799
it will tie in with the trait and

00:17:32,640 --> 00:17:40,480
will actually make it so that your enum

00:17:36,799 --> 00:17:43,440
will implement uh the trait methods

00:17:40,480 --> 00:17:45,039
if all of the variants of the numbers

00:17:43,440 --> 00:17:47,039
also implement

00:17:45,039 --> 00:17:48,559
so basically you can change the

00:17:47,039 --> 00:17:50,559
signature here the process

00:17:48,559 --> 00:17:52,160
might be really numb and you can still

00:17:50,559 --> 00:17:54,640
call my method on it

00:17:52,160 --> 00:17:55,520
on all the bearings that pass in this

00:17:54,640 --> 00:17:57,600
this is

00:17:55,520 --> 00:17:59,679
a tremendous performance boost and it

00:17:57,600 --> 00:18:02,160
reduces allocations too so this is

00:17:59,679 --> 00:18:03,360
really really great i love it and i use

00:18:02,160 --> 00:18:06,160
it in

00:18:03,360 --> 00:18:06,160
one crate of mine

00:18:06,880 --> 00:18:13,600
so another area where

00:18:10,320 --> 00:18:17,600
we could allocate a lot is are with

00:18:13,600 --> 00:18:20,559
sterilization right so

00:18:17,600 --> 00:18:22,640
let's look at a quick sample sturdy json

00:18:20,559 --> 00:18:26,840
with the deserialized from raw json

00:18:22,640 --> 00:18:30,240
to an intermediate value type

00:18:26,840 --> 00:18:33,280
this certi json value thing

00:18:30,240 --> 00:18:34,160
this is quite common for serialization

00:18:33,280 --> 00:18:35,760
libraries

00:18:34,160 --> 00:18:37,919
and then it has to do another step it

00:18:35,760 --> 00:18:41,360
has to take this intermediate

00:18:37,919 --> 00:18:43,120
ir and create a you know say struct or

00:18:41,360 --> 00:18:46,559
something right

00:18:43,120 --> 00:18:48,160
so um one way that you could go with

00:18:46,559 --> 00:18:51,919
this

00:18:48,160 --> 00:18:54,640
uh is to

00:18:51,919 --> 00:18:56,160
uh use there are some faster crates such

00:18:54,640 --> 00:18:58,320
as json rust

00:18:56,160 --> 00:18:59,200
where the intermediate representations

00:18:58,320 --> 00:19:02,240
are more efficient

00:18:59,200 --> 00:19:04,799
like json rust has a short value type

00:19:02,240 --> 00:19:05,360
where short strings are on the stack so

00:19:04,799 --> 00:19:08,480
this

00:19:05,360 --> 00:19:12,000
makes it faster and use

00:19:08,480 --> 00:19:13,360
less memory you can also go to binary

00:19:12,000 --> 00:19:15,120
protocols

00:19:13,360 --> 00:19:16,799
although many of them have the same

00:19:15,120 --> 00:19:18,880
problem they need to translate to some

00:19:16,799 --> 00:19:20,720
integrated layer or something but

00:19:18,880 --> 00:19:22,799
but some of them can translate directly

00:19:20,720 --> 00:19:25,840
to you know say a struct or something

00:19:22,799 --> 00:19:25,840
like that

00:19:27,840 --> 00:19:32,480
however i think the best strategy is

00:19:30,160 --> 00:19:34,720
just to avoid civilization altogether

00:19:32,480 --> 00:19:36,480
no serialization so what does this mean

00:19:34,720 --> 00:19:37,360
evan this is what you ask what does it

00:19:36,480 --> 00:19:39,440
mean

00:19:37,360 --> 00:19:40,400
what we mean is using something like

00:19:39,440 --> 00:19:42,720
flat buffers

00:19:40,400 --> 00:19:44,400
you might have heard of it captain proto

00:19:42,720 --> 00:19:46,320
apache arrow

00:19:44,400 --> 00:19:47,840
it does take some work to actually

00:19:46,320 --> 00:19:49,120
create these formats

00:19:47,840 --> 00:19:51,360
but what is usually meant is that

00:19:49,120 --> 00:19:54,400
there's no deserialization meaning

00:19:51,360 --> 00:19:58,160
once i create a flat buffer i can

00:19:54,400 --> 00:20:00,000
send it over the wire when i get it

00:19:58,160 --> 00:20:01,760
you know i can actually examine the flat

00:20:00,000 --> 00:20:06,240
buffer directly from the network

00:20:01,760 --> 00:20:10,240
buffers and extract values out without

00:20:06,240 --> 00:20:11,760
creating another into you know without

00:20:10,240 --> 00:20:14,640
translating and deserializing it to my

00:20:11,760 --> 00:20:16,880
final form so this is really fast

00:20:14,640 --> 00:20:18,640
it you know usually you can do no copy

00:20:16,880 --> 00:20:21,360
or no deserialization this is

00:20:18,640 --> 00:20:23,919
really really good and i highly

00:20:21,360 --> 00:20:23,919
recommend it

00:20:25,200 --> 00:20:33,039
so just an example from prostate json

00:20:29,360 --> 00:20:37,600
that sorry

00:20:33,039 --> 00:20:40,159
that um in this case

00:20:37,600 --> 00:20:41,840
uh using my laptop and again the

00:20:40,159 --> 00:20:43,679
comparison is available in this repo

00:20:41,840 --> 00:20:49,679
that i have

00:20:43,679 --> 00:20:54,720
and i used d-hat for heat profiling

00:20:49,679 --> 00:20:54,720
what we find is that using

00:20:55,200 --> 00:21:00,080
json rust where it would reduce the

00:20:57,600 --> 00:21:03,200
maximum heat used

00:21:00,080 --> 00:21:04,880
so d-hat actually measures how much heap

00:21:03,200 --> 00:21:08,240
is used at the point

00:21:04,880 --> 00:21:11,919
where in the max of when the heap is um

00:21:08,240 --> 00:21:13,440
the largest in your application runtime

00:21:11,919 --> 00:21:14,960
and we can see that it is quite a bit

00:21:13,440 --> 00:21:15,679
faster too it's like maybe 30-some

00:21:14,960 --> 00:21:18,320
percent

00:21:15,679 --> 00:21:19,679
you know one-third faster because it has

00:21:18,320 --> 00:21:21,520
to allocate less

00:21:19,679 --> 00:21:24,080
uh and again this is the technique of

00:21:21,520 --> 00:21:27,440
using the civilization where

00:21:24,080 --> 00:21:29,840
uh it uh uses a stack value

00:21:27,440 --> 00:21:31,440
a short string right and if you used no

00:21:29,840 --> 00:21:32,799
zero legition it would be much faster

00:21:31,440 --> 00:21:35,280
than that

00:21:32,799 --> 00:21:36,080
um however uh for some reason the total

00:21:35,280 --> 00:21:38,159
allocations

00:21:36,080 --> 00:21:40,320
you know does not uh go down this is

00:21:38,159 --> 00:21:44,080
like basically allocate

00:21:40,320 --> 00:21:45,200
so on and just to show you i think this

00:21:44,080 --> 00:21:46,960
is uh good

00:21:45,200 --> 00:21:48,320
to show you what the d-hat output looks

00:21:46,960 --> 00:21:51,360
like

00:21:48,320 --> 00:21:54,159
um it basically gives you the top

00:21:51,360 --> 00:21:54,559
notes at a certain time but you can look

00:21:54,159 --> 00:21:58,480
at

00:21:54,559 --> 00:22:00,799
what are the top allocators for all time

00:21:58,480 --> 00:22:01,679
and it will tell you it will give you a

00:22:00,799 --> 00:22:03,360
stack trace

00:22:01,679 --> 00:22:05,280
here we can see that the stock trace you

00:22:03,360 --> 00:22:07,200
can easily trace it to 30 json

00:22:05,280 --> 00:22:09,280
when you know basically creating json

00:22:07,200 --> 00:22:12,720
objects that is using up

00:22:09,280 --> 00:22:15,120
that is 90 of your allocations

00:22:12,720 --> 00:22:16,960
and it will tell you things like what is

00:22:15,120 --> 00:22:20,159
the average size for the allocations

00:22:16,960 --> 00:22:21,679
and the lifetimes that on average is 73

00:22:20,159 --> 00:22:23,679
microseconds

00:22:21,679 --> 00:22:27,760
so it gives you a lot of really useful

00:22:23,679 --> 00:22:30,320
memory profiling information okay

00:22:27,760 --> 00:22:33,120
and just really quickly we'll talk about

00:22:30,320 --> 00:22:34,480
a few extra memory allocation topics

00:22:33,120 --> 00:22:36,559
in russ you can switch to memory

00:22:34,480 --> 00:22:38,400
allocator

00:22:36,559 --> 00:22:40,480
there are two popular alternatives to

00:22:38,400 --> 00:22:43,679
the standard allocator one is

00:22:40,480 --> 00:22:46,159
j malloc which came from uh well it

00:22:43,679 --> 00:22:48,960
originally came from

00:22:46,159 --> 00:22:50,159
bsd sorry but it was popularized by

00:22:48,960 --> 00:22:52,159
facebook

00:22:50,159 --> 00:22:54,480
and was created for reducing

00:22:52,159 --> 00:22:56,799
fragmentation and concurrency

00:22:54,480 --> 00:22:57,840
it does have a bit of overhead in terms

00:22:56,799 --> 00:23:01,600
of memory

00:22:57,840 --> 00:23:04,080
used but it is faster than the standard

00:23:01,600 --> 00:23:06,000
allocator another one to check out is

00:23:04,080 --> 00:23:07,840
called we malloc from microsoft

00:23:06,000 --> 00:23:09,360
it is designed to be a small secure

00:23:07,840 --> 00:23:12,080
replacement for malloc

00:23:09,360 --> 00:23:13,120
and in practice i do believe it is also

00:23:12,080 --> 00:23:16,720
faster

00:23:13,120 --> 00:23:16,720
so you can check that out

00:23:17,600 --> 00:23:21,120
and i have a benchmark that shows that

00:23:20,080 --> 00:23:23,840
sometimes

00:23:21,120 --> 00:23:23,840
it is faster

00:23:24,720 --> 00:23:31,200
okay finally for certain

00:23:27,840 --> 00:23:33,760
use cases you can use bomb arena

00:23:31,200 --> 00:23:33,760
allocators

00:23:35,440 --> 00:23:38,320
and um

00:23:38,559 --> 00:23:42,559
usually this is when for special cases

00:23:41,679 --> 00:23:44,880
where

00:23:42,559 --> 00:23:45,679
you know let's let's say uh you want a

00:23:44,880 --> 00:23:48,320
sandbox

00:23:45,679 --> 00:23:49,919
some memory for a part of your app you

00:23:48,320 --> 00:23:51,840
know for queries in a database or

00:23:49,919 --> 00:23:53,840
certain namespaces that kind of thing

00:23:51,840 --> 00:23:54,960
you can just take this elegant memory by

00:23:53,840 --> 00:23:56,960
bumping pointer

00:23:54,960 --> 00:23:58,559
and then you can free it all at once so

00:23:56,960 --> 00:23:59,520
for these things there is a crate called

00:23:58,559 --> 00:24:02,400
bumpalo which is

00:23:59,520 --> 00:24:02,400
which is really great

00:24:02,640 --> 00:24:06,000
and that can help sometimes when you

00:24:05,360 --> 00:24:09,279
want to

00:24:06,000 --> 00:24:09,279
control memory use

00:24:09,840 --> 00:24:13,520
finally you might be like evan so

00:24:12,720 --> 00:24:15,360
reducing

00:24:13,520 --> 00:24:18,880
keep allocation is great but i want to

00:24:15,360 --> 00:24:20,960
actually make my binary smaller

00:24:18,880 --> 00:24:22,880
you should check out the cargo bloat

00:24:20,960 --> 00:24:26,480
crate this will analyze

00:24:22,880 --> 00:24:28,880
your rust binaries and

00:24:26,480 --> 00:24:29,679
figure out where is your space being

00:24:28,880 --> 00:24:32,480
used

00:24:29,679 --> 00:24:34,000
but there are tons of ways that you

00:24:32,480 --> 00:24:34,400
could actually reduce you could get down

00:24:34,000 --> 00:24:36,400
to

00:24:34,400 --> 00:24:37,440
really really small like well below

00:24:36,400 --> 00:24:39,600
megabyte

00:24:37,440 --> 00:24:41,919
binaries you can check out this and by

00:24:39,600 --> 00:24:41,919
the way

00:24:42,320 --> 00:24:46,000
the slides will be shared and you can

00:24:45,440 --> 00:24:47,919
actually click

00:24:46,000 --> 00:24:49,440
you should be able to click on them

00:24:47,919 --> 00:24:53,279
there there's a url

00:24:49,440 --> 00:24:55,200
so there is a blog that gives

00:24:53,279 --> 00:24:57,440
many things for reducing the size of

00:24:55,200 --> 00:25:00,640
binary including stripping

00:24:57,440 --> 00:25:02,799
reducing debug things optimizing for

00:25:00,640 --> 00:25:05,520
size instead of speed

00:25:02,799 --> 00:25:06,880
and uh if you really want to remove the

00:25:05,520 --> 00:25:08,559
standard library that's a way you can

00:25:06,880 --> 00:25:12,000
get down to extremely small like c

00:25:08,559 --> 00:25:15,120
size things but be warned that

00:25:12,000 --> 00:25:18,720
that has a lot of trade-offs

00:25:15,120 --> 00:25:22,480
and i'm not quite sure if they are

00:25:18,720 --> 00:25:22,480
worth it but it depends on use case

00:25:22,640 --> 00:25:29,039
so um thank you very much you can feel

00:25:25,919 --> 00:25:33,120
free to reach out to me on

00:25:29,039 --> 00:25:37,360
instagram sorry twitter instagram

00:25:33,120 --> 00:25:37,360

YouTube URL: https://www.youtube.com/watch?v=FAz0qSclLss


