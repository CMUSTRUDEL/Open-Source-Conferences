Title: FOSDEM 2009 Help my system is slow
Publication date: 2011-12-22
Playlist: FOSDEM 2009
Description: 
	By Kris Kennaway

An understanding of the nature of your system workload is an important step in optimizing it for maximum performance on your hardware. I will discuss some useful tools and techniques for evaluating the workload of your FreeBSD system, and identifying the bottlenecks that are limiting performance.

FOSDEM (Free and Open Source Development European Meeting) is a European event centered around Free and Open Source software development. It is aimed at developers and all interested in the Free and Open Source news in the world. Its goals are to enable developers to meet and to promote the awareness and use of free and open source software. More info at http://fosdem.org
Captions: 
	00:00:08,889 --> 00:00:15,230
ok excellent ok so like to make a start

00:00:13,490 --> 00:00:18,260
Chris Kennaway I'm a free bc developer

00:00:15,230 --> 00:00:21,700
I've been involved in freebsd for about

00:00:18,260 --> 00:00:24,110
10 years and in a number of capacities

00:00:21,700 --> 00:00:25,970
most recently for the past two years

00:00:24,110 --> 00:00:29,090
I've been working a lot on system

00:00:25,970 --> 00:00:34,790
performance so I've done a lot of work

00:00:29,090 --> 00:00:36,590
on benchmarking profiling and we've I

00:00:34,790 --> 00:00:38,989
think made a lot of progress in recent

00:00:36,590 --> 00:00:43,070
years especially with freebsd seven in

00:00:38,989 --> 00:00:52,760
that area so today I'm going to be

00:00:43,070 --> 00:00:57,250
talking about some of the lessons okay

00:00:52,760 --> 00:00:57,250
how's that better that guy still

00:00:58,030 --> 00:01:11,080
straight yeah about that

00:01:15,300 --> 00:01:26,820
okay all right so I'm going to talk

00:01:22,230 --> 00:01:33,870
about some of the ways in which you as a

00:01:26,820 --> 00:01:36,180
freebsd user can analyze go by analyzing

00:01:33,870 --> 00:01:39,270
the workloads of your system with the

00:01:36,180 --> 00:01:42,620
view to to improving performance so i'm

00:01:39,270 --> 00:01:46,320
targeting my talk at kind of power users

00:01:42,620 --> 00:01:48,690
hopefully a lot of the the methodologies

00:01:46,320 --> 00:01:50,790
will be be be applicable to not just

00:01:48,690 --> 00:01:53,700
users of previously systems but more

00:01:50,790 --> 00:02:00,870
generally but as a freebsd developer

00:01:53,700 --> 00:02:02,430
this is what I'm focusing on so I some

00:02:00,870 --> 00:02:04,770
of the more advanced techniques i'll

00:02:02,430 --> 00:02:06,690
talk about towards the end I'm more

00:02:04,770 --> 00:02:08,970
focused on looking at Colonel

00:02:06,690 --> 00:02:10,260
performance so if you're not afraid to

00:02:08,970 --> 00:02:11,910
go and look at some kernel code this

00:02:10,260 --> 00:02:14,450
will definitely come in handy in

00:02:11,910 --> 00:02:18,900
applying these in your own your own

00:02:14,450 --> 00:02:22,650
environments but at least if you're not

00:02:18,900 --> 00:02:24,030
comfortable doing that these the kinds

00:02:22,650 --> 00:02:25,980
of information and that you'll be

00:02:24,030 --> 00:02:28,440
getting out of this out of running these

00:02:25,980 --> 00:02:30,480
commands will be useful to pass on to

00:02:28,440 --> 00:02:36,239
kernel developers if you need need help

00:02:30,480 --> 00:02:38,730
with your workload okay so there are

00:02:36,239 --> 00:02:41,340
really four parts firstly I'll talk

00:02:38,730 --> 00:02:43,760
about why it's important to understand

00:02:41,340 --> 00:02:45,600
what your system is actually doing the

00:02:43,760 --> 00:02:47,130
you're not going to get anywhere in

00:02:45,600 --> 00:02:48,510
trying to improve performance if you

00:02:47,130 --> 00:02:50,400
don't have a detailed understanding of

00:02:48,510 --> 00:02:52,860
what your your system is actually doing

00:02:50,400 --> 00:02:55,620
and as part of that there are a number

00:02:52,860 --> 00:03:01,560
of tools that are available on previous

00:02:55,620 --> 00:03:07,070
dia systems to to investigate those

00:03:01,560 --> 00:03:11,489
aspects and I'll talk at the end about

00:03:07,070 --> 00:03:14,580
some tuning advice that applies in some

00:03:11,489 --> 00:03:16,769
situations and finally if I have time or

00:03:14,580 --> 00:03:20,180
focus in more general aspects of

00:03:16,769 --> 00:03:22,800
benchmarking which seems to be quite a

00:03:20,180 --> 00:03:30,120
bit more difficult than people off

00:03:22,800 --> 00:03:32,160
and often realize so the important thing

00:03:30,120 --> 00:03:35,540
to realize when we're talking about

00:03:32,160 --> 00:03:38,010
performance is that it actually isn't a

00:03:35,540 --> 00:03:40,710
meaningful concept unless we qualify it

00:03:38,010 --> 00:03:42,270
and it only actually makes sense to talk

00:03:40,710 --> 00:03:45,450
about performance of a particular

00:03:42,270 --> 00:03:50,100
workload and with respect to a set of

00:03:45,450 --> 00:03:51,600
metrics so before you can go about

00:03:50,100 --> 00:03:53,310
improving performance you have to first

00:03:51,600 --> 00:03:55,680
know what you mean by performance and

00:03:53,310 --> 00:03:57,750
the first step towards this is to

00:03:55,680 --> 00:04:01,500
characterize exactly what your system is

00:03:57,750 --> 00:04:03,300
doing on a particular workload and what

00:04:01,500 --> 00:04:06,270
aspects of its operation you actually

00:04:03,300 --> 00:04:08,870
care about and depending on the answer

00:04:06,270 --> 00:04:13,380
to these questions the way you proceed

00:04:08,870 --> 00:04:17,070
to to improve things will vary so

00:04:13,380 --> 00:04:18,180
depending on some examples being you

00:04:17,070 --> 00:04:20,640
know if you have to have a web server

00:04:18,180 --> 00:04:22,560
you may care about the bulk throughput

00:04:20,640 --> 00:04:24,330
how many queries per second can your web

00:04:22,560 --> 00:04:26,280
server handle you may also care about

00:04:24,330 --> 00:04:28,290
the latency of the query so how how

00:04:26,280 --> 00:04:29,880
quickly is each query handled and these

00:04:28,290 --> 00:04:36,960
are different things that often require

00:04:29,880 --> 00:04:39,060
different different approaches so some

00:04:36,960 --> 00:04:41,430
of the ways in which your workloads can

00:04:39,060 --> 00:04:44,370
interact with with systems well there

00:04:41,430 --> 00:04:47,010
are there are lots of them the CPU use

00:04:44,370 --> 00:04:49,140
patterns of the workload can vary they

00:04:47,010 --> 00:04:51,930
may involve disk i/o they may try to

00:04:49,140 --> 00:04:55,320
talk to network they may be talking to

00:04:51,930 --> 00:04:57,600
other divided wear devices applications

00:04:55,320 --> 00:04:59,700
can be misconfigured so this is actually

00:04:57,600 --> 00:05:02,040
quite a common source of performance

00:04:59,700 --> 00:05:05,310
problems the application is just not

00:05:02,040 --> 00:05:08,160
configured properly and it's often easy

00:05:05,310 --> 00:05:09,600
to misidentify this class class of

00:05:08,160 --> 00:05:11,700
problem as being an operating system

00:05:09,600 --> 00:05:14,900
problem or a hardware problem but it's

00:05:11,700 --> 00:05:16,770
actually just a configuration issue

00:05:14,900 --> 00:05:18,800
ultimately you're going to be running

00:05:16,770 --> 00:05:21,330
into the limitations of the hardware

00:05:18,800 --> 00:05:23,760
assuming you can push things hard enough

00:05:21,330 --> 00:05:25,200
and so it's important to have an

00:05:23,760 --> 00:05:28,680
understanding of what is your hardware

00:05:25,200 --> 00:05:30,180
actually capable of workloads will

00:05:28,680 --> 00:05:33,210
usually interact with the colonel in

00:05:30,180 --> 00:05:35,970
some way either through system calls or

00:05:33,210 --> 00:05:37,690
through other operations

00:05:35,970 --> 00:05:40,210
multithreaded workloads are come in

00:05:37,690 --> 00:05:41,800
these days and there are a lot of badly

00:05:40,210 --> 00:05:45,040
written multi-threaded applications that

00:05:41,800 --> 00:05:46,810
often have high lock contention within

00:05:45,040 --> 00:05:50,590
the application so they can be

00:05:46,810 --> 00:05:51,700
application design problems that may or

00:05:50,590 --> 00:05:57,340
may not be possible to work around

00:05:51,700 --> 00:05:59,470
without code changes and finally this is

00:05:57,340 --> 00:06:02,440
another problem that's often overlooked

00:05:59,470 --> 00:06:05,710
is that if your your system is part of a

00:06:02,440 --> 00:06:07,120
pipeline or part of a a set of other

00:06:05,710 --> 00:06:10,150
systems that are all handing work to

00:06:07,120 --> 00:06:12,360
each other any given system may in fact

00:06:10,150 --> 00:06:15,070
not be getting enough work to do so it's

00:06:12,360 --> 00:06:18,010
not unheard of for problems to be

00:06:15,070 --> 00:06:19,690
reported people come to come to to us

00:06:18,010 --> 00:06:22,180
and say you know help what's going on

00:06:19,690 --> 00:06:23,770
here and the answer is really that the

00:06:22,180 --> 00:06:25,660
system isn't busy enough you're not

00:06:23,770 --> 00:06:27,130
giving enough work and this might be

00:06:25,660 --> 00:06:28,450
because of a bottleneck also in the

00:06:27,130 --> 00:06:30,100
system where it's not being fed in

00:06:28,450 --> 00:06:31,840
properly or it could be another

00:06:30,100 --> 00:06:34,350
configuration issue so they don't

00:06:31,840 --> 00:06:36,700
discount that kind of kind of problem

00:06:34,350 --> 00:06:38,860
typically there'll be at least one of

00:06:36,700 --> 00:06:44,950
these these issues that turns out to be

00:06:38,860 --> 00:06:48,280
the limiting factor so the way I like to

00:06:44,950 --> 00:06:50,080
approach studying this kind of issue is

00:06:48,280 --> 00:06:52,960
starting at a very high level and then

00:06:50,080 --> 00:06:56,440
moving down to two detailed

00:06:52,960 --> 00:07:00,130
investigation as you get pointed in the

00:06:56,440 --> 00:07:02,530
right direction and a very good tool so

00:07:00,130 --> 00:07:04,720
I should apologize some of these tools

00:07:02,530 --> 00:07:07,450
are very standard and I'm sure a lot of

00:07:04,720 --> 00:07:11,230
you will actually know in very great

00:07:07,450 --> 00:07:14,970
detail how they work but I hope that at

00:07:11,230 --> 00:07:14,970
least I will present some new

00:07:15,090 --> 00:07:23,110
information that you hadn't been aware

00:07:17,260 --> 00:07:24,750
of so top is of course it's we're

00:07:23,110 --> 00:07:27,970
probably most most most familiar with

00:07:24,750 --> 00:07:29,350
mostly familiar with it here but it's a

00:07:27,970 --> 00:07:31,240
great way of getting an overview of

00:07:29,350 --> 00:07:33,100
what's going on the system it shows you

00:07:31,240 --> 00:07:35,110
things like what is the the colonel

00:07:33,100 --> 00:07:36,670
doing at a very high level it tells you

00:07:35,110 --> 00:07:39,130
for example if your colonel is paging

00:07:36,670 --> 00:07:43,510
and this is going to be a kiss of death

00:07:39,130 --> 00:07:46,680
if your workload is sized such as it

00:07:43,510 --> 00:07:48,380
cannot fit in main memory then either

00:07:46,680 --> 00:07:50,030
transiently or

00:07:48,380 --> 00:07:52,160
I in a steady state it's going to be

00:07:50,030 --> 00:07:53,990
writing to and from swap and any time

00:07:52,160 --> 00:07:55,940
this happens if you get a disc involved

00:07:53,990 --> 00:07:59,590
in the critical path of your workload

00:07:55,940 --> 00:08:02,480
things are going to slow right down so

00:07:59,590 --> 00:08:04,250
this is again something that's possible

00:08:02,480 --> 00:08:08,660
to overlook unless you happen to think

00:08:04,250 --> 00:08:10,070
about it top shows you if if the system

00:08:08,660 --> 00:08:14,300
is spending a lot of time in the kernel

00:08:10,070 --> 00:08:15,950
or processing interrupts and then it

00:08:14,300 --> 00:08:19,030
breaks it down by thread so you can look

00:08:15,950 --> 00:08:23,330
at which processes are using using cpu

00:08:19,030 --> 00:08:25,100
in which threads using cpu it also shows

00:08:23,330 --> 00:08:26,780
you what that for processes that are

00:08:25,100 --> 00:08:28,970
running inside the colonel what are they

00:08:26,780 --> 00:08:30,530
doing or where they blocked if they're

00:08:28,970 --> 00:08:33,710
if they blocked waiting for a resource

00:08:30,530 --> 00:08:35,720
you can get a Nov view of what what's

00:08:33,710 --> 00:08:38,289
going on there unfortunately it involves

00:08:35,720 --> 00:08:41,630
these cryptic all kinds of abbreviations

00:08:38,289 --> 00:08:43,729
and at least at the moment there's no

00:08:41,630 --> 00:08:46,520
good reference that I know of a previous

00:08:43,729 --> 00:08:49,160
day at least that breaks down what are

00:08:46,520 --> 00:08:51,850
the common weight channels as they're

00:08:49,160 --> 00:08:54,470
called and explains what they mean but

00:08:51,850 --> 00:08:57,260
typical ones you might see are things

00:08:54,470 --> 00:08:59,630
like bio read by a right right drain

00:08:57,260 --> 00:09:01,940
which are telling you that the process

00:08:59,630 --> 00:09:07,120
is doing is block waiting for some kind

00:09:01,940 --> 00:09:10,400
of disk i/o the reader right SP wait

00:09:07,120 --> 00:09:11,900
shows up quite commonly and it's usually

00:09:10,400 --> 00:09:13,730
not a performance issue this just says

00:09:11,900 --> 00:09:17,420
that a socket is waiting for input or

00:09:13,730 --> 00:09:19,070
it's waiting for i 0 so this is sort of

00:09:17,420 --> 00:09:22,640
the typical state of a network server is

00:09:19,070 --> 00:09:24,260
not busy there are some wet channels

00:09:22,640 --> 00:09:26,660
that tell you that you have a threaded

00:09:24,260 --> 00:09:28,910
application that is waiting on on a lock

00:09:26,660 --> 00:09:31,040
or on a condition variable or so on you

00:09:28,910 --> 00:09:33,470
Conda mutex and there's a lot of these

00:09:31,040 --> 00:09:35,330
and unfortunately there's no so that the

00:09:33,470 --> 00:09:37,340
only way to really find out what what

00:09:35,330 --> 00:09:41,000
they mean is to go and grip the kernel

00:09:37,340 --> 00:09:42,980
tree but as you get experienced with

00:09:41,000 --> 00:09:44,450
with looking at these you start to

00:09:42,980 --> 00:09:49,400
recognize which ones they are and which

00:09:44,450 --> 00:09:53,840
one stand out as being important so top

00:09:49,400 --> 00:09:55,190
is is the usual first step for seeing

00:09:53,840 --> 00:09:58,709
what's going on and then you'll

00:09:55,190 --> 00:10:04,540
typically spot a problem and dig further

00:09:58,709 --> 00:10:07,529
a related tool that exists in freebsd

00:10:04,540 --> 00:10:11,050
what facility really is the ability to

00:10:07,529 --> 00:10:14,560
ask any any foreground process what it's

00:10:11,050 --> 00:10:16,540
doing and this is something that's

00:10:14,560 --> 00:10:17,980
missing from other operating systems

00:10:16,540 --> 00:10:19,779
example Linux doesn't have this

00:10:17,980 --> 00:10:26,500
disability as far as i know i would love

00:10:19,779 --> 00:10:28,750
it if it did you can by default the ctrl

00:10:26,500 --> 00:10:30,970
T key will send a cig info to the

00:10:28,750 --> 00:10:34,029
foreground process and the TTYL system

00:10:30,970 --> 00:10:37,329
will has a default handler for sig info

00:10:34,029 --> 00:10:38,950
and if you run your own your process and

00:10:37,329 --> 00:10:40,570
you want to know what's going on maybe

00:10:38,950 --> 00:10:43,029
it's not giving the expected output or

00:10:40,570 --> 00:10:44,920
it's taking too long so on you can just

00:10:43,029 --> 00:10:46,899
press ctrl T and I'll tell you that in

00:10:44,920 --> 00:10:49,990
this case the load average is 0 point 0

00:10:46,899 --> 00:10:53,170
for the foreground command is food this

00:10:49,990 --> 00:10:55,120
is the paired and this is typically an

00:10:53,170 --> 00:10:56,920
interesting field here which tells you

00:10:55,120 --> 00:10:59,470
the weight channel in this case it's

00:10:56,920 --> 00:11:01,660
telling me that the application was

00:10:59,470 --> 00:11:04,000
waiting for NFS request so this was

00:11:01,660 --> 00:11:05,680
actually doing NFS I oh and maybe I

00:11:04,000 --> 00:11:07,180
hadn't expected this maybe I thought it

00:11:05,680 --> 00:11:08,920
should be running to local disk but it

00:11:07,180 --> 00:11:11,410
was actually running to doing I owe to

00:11:08,920 --> 00:11:14,380
NFS and this is why it's taking a long

00:11:11,410 --> 00:11:17,589
time so it also shows you what is the

00:11:14,380 --> 00:11:20,980
current cpu use in user land and in

00:11:17,589 --> 00:11:22,930
system in the kernel and the cpu usage

00:11:20,980 --> 00:11:24,970
of the process and its resident memory

00:11:22,930 --> 00:11:26,470
set size this is all information you can

00:11:24,970 --> 00:11:29,860
get from other sources but having it

00:11:26,470 --> 00:11:31,959
available online just by pressing ctrl T

00:11:29,860 --> 00:11:33,880
is I find it invaluable for figuring out

00:11:31,959 --> 00:11:38,829
what's going on especially for shell

00:11:33,880 --> 00:11:40,000
commands run from the shell okay so this

00:11:38,829 --> 00:11:44,290
is this is what top looks like on

00:11:40,000 --> 00:11:45,730
previously it's got the standard colonel

00:11:44,290 --> 00:11:48,490
summary at the top here which shows us

00:11:45,730 --> 00:11:49,839
some things about load average how many

00:11:48,490 --> 00:11:53,110
processes are running how many are

00:11:49,839 --> 00:11:55,149
blocked and so on what percentage of

00:11:53,110 --> 00:11:56,769
time is being used in the system and so

00:11:55,149 --> 00:11:58,779
in this this example I've shown here

00:11:56,769 --> 00:12:02,050
this would stand out immediately that if

00:11:58,779 --> 00:12:03,670
you have a pro your system is we your

00:12:02,050 --> 00:12:05,769
machine is spending sixty-three percent

00:12:03,670 --> 00:12:09,069
of its time in in the colonel this is

00:12:05,769 --> 00:12:11,350
typically unusual and then we look down

00:12:09,069 --> 00:12:13,330
a bit further and we see that the

00:12:11,350 --> 00:12:15,010
the my sequel threads that are running

00:12:13,330 --> 00:12:17,890
in this case a lot of them are actually

00:12:15,010 --> 00:12:20,230
blocks on this weight channel which

00:12:17,890 --> 00:12:22,270
turns out to be buff object and if you

00:12:20,230 --> 00:12:25,540
dig around and find out what buff object

00:12:22,270 --> 00:12:29,320
means this is it turns out it's waiting

00:12:25,540 --> 00:12:31,000
on Buffy Oh we'll come back this example

00:12:29,320 --> 00:12:33,010
and I'll show you another way that that

00:12:31,000 --> 00:12:34,540
we could find out what's going on here

00:12:33,010 --> 00:12:35,830
well at this stage we don't really know

00:12:34,540 --> 00:12:42,910
what's going on but we'll come back to

00:12:35,830 --> 00:12:45,550
this example later and so useful options

00:12:42,910 --> 00:12:48,790
to Pratap a capital H which shows you

00:12:45,550 --> 00:12:50,350
the breaks down each process by by

00:12:48,790 --> 00:12:53,770
threads so in this case there's a single

00:12:50,350 --> 00:12:56,890
process and it show it's showing 5

00:12:53,770 --> 00:13:02,710
threads and the S option shows kernel

00:12:56,890 --> 00:13:05,470
threads or system processes so when we

00:13:02,710 --> 00:13:07,510
have applications that are interacting

00:13:05,470 --> 00:13:10,390
with the disks such as my sequel in the

00:13:07,510 --> 00:13:11,680
previous example typically they're going

00:13:10,390 --> 00:13:14,170
to be limited by one of two things

00:13:11,680 --> 00:13:17,080
either the bandwidth of the storage

00:13:14,170 --> 00:13:20,440
system or the latency which is response

00:13:17,080 --> 00:13:24,120
time for a given operation as opposed to

00:13:20,440 --> 00:13:26,380
the bog 3.4 all operations combined and

00:13:24,120 --> 00:13:28,840
depending on the i/o patterns the

00:13:26,380 --> 00:13:30,640
application generates it's going to put

00:13:28,840 --> 00:13:31,690
a different amount of stress on the disk

00:13:30,640 --> 00:13:33,760
for example if you're doing a lot of

00:13:31,690 --> 00:13:35,920
random access reads or writes that

00:13:33,760 --> 00:13:38,170
require that the head to seek back and

00:13:35,920 --> 00:13:40,540
forth then there's going to be a lot of

00:13:38,170 --> 00:13:42,430
seek time where the disc is not perhaps

00:13:40,540 --> 00:13:44,080
doing anything and this is going to

00:13:42,430 --> 00:13:47,020
limit the amount of throughput you can

00:13:44,080 --> 00:13:48,340
get whereas if your if your workload is

00:13:47,020 --> 00:13:51,880
structured so it's performing sequential

00:13:48,340 --> 00:13:53,740
io to sequential blocks then you're more

00:13:51,880 --> 00:13:57,100
likely be limited by the transfer rate

00:13:53,740 --> 00:14:00,040
of the desk or of the controller so

00:13:57,100 --> 00:14:04,180
there are some useful tools for studying

00:14:00,040 --> 00:14:05,830
I operations I've mentioned two of them

00:14:04,180 --> 00:14:08,790
here is that insists that can do this

00:14:05,830 --> 00:14:11,020
they have a lot of other metrics as well

00:14:08,790 --> 00:14:14,820
one very useful command that previously

00:14:11,020 --> 00:14:17,650
has is G stat which is part of the John

00:14:14,820 --> 00:14:20,170
storage layer system and this shows you

00:14:17,650 --> 00:14:21,459
for every John storage provider it

00:14:20,170 --> 00:14:23,290
breaks down the

00:14:21,459 --> 00:14:27,189
the operations that are currently

00:14:23,290 --> 00:14:30,249
pending so in this configuration here

00:14:27,189 --> 00:14:32,350
it's sampling once a second and it shows

00:14:30,249 --> 00:14:36,519
us for each of these storage providers

00:14:32,350 --> 00:14:39,089
86 is a solder disk in this case and it

00:14:36,519 --> 00:14:40,929
has various partitions and is also a

00:14:39,089 --> 00:14:46,089
cd-rom drive that's not actually doing

00:14:40,929 --> 00:14:48,730
anything so we can see here that the 86

00:14:46,089 --> 00:14:52,449
device is doing about 1,200 operations

00:14:48,730 --> 00:14:55,559
per second and there's a queue of about

00:14:52,449 --> 00:14:59,379
almost 1,200 operations backed up

00:14:55,559 --> 00:15:01,149
waiting to proceed only one of these

00:14:59,379 --> 00:15:06,970
operations was a read but but they're

00:15:01,149 --> 00:15:09,040
all rights and the interesting statistic

00:15:06,970 --> 00:15:11,470
here for measuring for determining

00:15:09,040 --> 00:15:14,110
whether a disk is overloaded is not the

00:15:11,470 --> 00:15:16,360
the last column as you might might

00:15:14,110 --> 00:15:18,189
expect but it's actually the the

00:15:16,360 --> 00:15:20,559
millisecond / read a millisecond / riot

00:15:18,189 --> 00:15:24,999
stats which tell you how long on average

00:15:20,559 --> 00:15:26,649
the operation take to succeed so this is

00:15:24,999 --> 00:15:28,749
what actually tells you if this is

00:15:26,649 --> 00:15:32,100
because overload it if it's taking very

00:15:28,749 --> 00:15:34,209
much more than the sort of steady-state

00:15:32,100 --> 00:15:36,549
latency for example the read here and it

00:15:34,209 --> 00:15:38,410
took 11 million seconds but the rights

00:15:36,549 --> 00:15:40,809
were taking as long as 300 milliseconds

00:15:38,410 --> 00:15:42,639
on average so this is indicating that

00:15:40,809 --> 00:15:46,569
the right bandwidth of the disk is being

00:15:42,639 --> 00:15:51,189
overloaded and so this is pointing to

00:15:46,569 --> 00:15:53,350
where an issue might lie the percent

00:15:51,189 --> 00:15:55,449
busy column by contrast I only tells you

00:15:53,350 --> 00:15:58,689
what percentage of time was at least one

00:15:55,449 --> 00:16:00,850
operation pending so in this case we

00:15:58,689 --> 00:16:04,269
have as many as 1,200 operations that

00:16:00,850 --> 00:16:09,790
are queued up so these are maybe

00:16:04,269 --> 00:16:11,589
overlapped by the the disk hardware and

00:16:09,790 --> 00:16:13,049
at any given time there may be several

00:16:11,589 --> 00:16:17,110
operations in the process of being

00:16:13,049 --> 00:16:19,329
completed or there may be as few as one

00:16:17,110 --> 00:16:20,499
in even time and so percent busy only

00:16:19,329 --> 00:16:22,480
tells you what percentage of the time

00:16:20,499 --> 00:16:24,939
disk was doing something doesn't tell

00:16:22,480 --> 00:16:26,410
you how hard it's working this is very

00:16:24,939 --> 00:16:28,360
common misconception when people look at

00:16:26,410 --> 00:16:30,339
G stat they think my disk is one hundred

00:16:28,360 --> 00:16:32,960
percent busy whereas actually it may be

00:16:30,339 --> 00:16:37,490
I want to do a lot more work by

00:16:32,960 --> 00:16:41,720
up operations so the the latency is the

00:16:37,490 --> 00:16:43,340
key thing to look out there okay so

00:16:41,720 --> 00:16:45,140
looking at so how do we find out what

00:16:43,340 --> 00:16:47,390
actual processes are doing the IO to the

00:16:45,140 --> 00:16:50,480
disks it turns out top can actually do

00:16:47,390 --> 00:16:52,520
this as well top has a switch minus M io

00:16:50,480 --> 00:16:55,880
which instead of displaying cpu usage

00:16:52,520 --> 00:16:58,160
displays io usage and you can sort

00:16:55,880 --> 00:16:59,930
various ways but sorting by total

00:16:58,160 --> 00:17:04,270
ordering is is usually the most

00:16:59,930 --> 00:17:07,010
interesting and this shows us here that

00:17:04,270 --> 00:17:11,030
it's in the same example as before the

00:17:07,010 --> 00:17:13,490
my sequel threads we're doing each of

00:17:11,030 --> 00:17:17,900
them were doing about 250 reads and

00:17:13,490 --> 00:17:20,330
writes per second and if we compare this

00:17:17,900 --> 00:17:22,339
to the the G stat we see they actually

00:17:20,330 --> 00:17:23,690
weren't many reads from the hardware

00:17:22,339 --> 00:17:26,240
these Reed's were actually satisfied

00:17:23,690 --> 00:17:29,570
from cash but the rights were actually

00:17:26,240 --> 00:17:30,980
had to hit the disk and it shows you

00:17:29,570 --> 00:17:33,950
what percentage of your total i owe each

00:17:30,980 --> 00:17:36,950
thread is using there's some other

00:17:33,950 --> 00:17:39,950
interesting stats here these first two

00:17:36,950 --> 00:17:41,660
columns showing the voluntary context

00:17:39,950 --> 00:17:44,240
switches and involuntary context

00:17:41,660 --> 00:17:48,560
switches i'll come back to what that

00:17:44,240 --> 00:17:50,150
means a bit later and just a minor

00:17:48,560 --> 00:17:52,730
caveat which is unfortunately at the

00:17:50,150 --> 00:17:54,500
moment ZFS doesn't support the io stats

00:17:52,730 --> 00:17:56,660
i'm not sure why this is but this is a

00:17:54,500 --> 00:17:58,070
bug with our setting at the moment so

00:17:56,660 --> 00:17:59,810
you won't see and it's to anything if

00:17:58,070 --> 00:18:05,410
you're looking at processes doing iotas

00:17:59,810 --> 00:18:08,180
NFS okay so suppose we've identified a

00:18:05,410 --> 00:18:11,870
disc problem what we think we're seeing

00:18:08,180 --> 00:18:14,510
some higher disk latency what can we do

00:18:11,870 --> 00:18:16,550
to fix it so well disk is is typically a

00:18:14,510 --> 00:18:19,340
shared resource that's accessed by many

00:18:16,550 --> 00:18:21,290
processes and if this sharing of

00:18:19,340 --> 00:18:22,730
resource of the resources what's causing

00:18:21,290 --> 00:18:25,400
your problem the obvious answer is to

00:18:22,730 --> 00:18:28,040
make a not shared so you can reduce this

00:18:25,400 --> 00:18:29,960
contention by moving the i/o jobs if you

00:18:28,040 --> 00:18:32,720
have two processes the reach doing I owe

00:18:29,960 --> 00:18:34,780
to the same disk either move them onto

00:18:32,720 --> 00:18:36,800
independent disks or if you can't

00:18:34,780 --> 00:18:40,910
restructure the applications use

00:18:36,800 --> 00:18:46,070
separate separate faster than paths you

00:18:40,910 --> 00:18:46,850
can look at striping multiple disks with

00:18:46,070 --> 00:18:49,400
something like Jesus

00:18:46,850 --> 00:18:51,320
tripe so that you provide one one

00:18:49,400 --> 00:18:53,300
logical file system to the applications

00:18:51,320 --> 00:18:55,430
but they're actually backed by multiple

00:18:53,300 --> 00:18:57,590
physical devices each of those can

00:18:55,430 --> 00:19:01,400
handle the i/o independently if things

00:18:57,590 --> 00:19:04,790
work out nicely so some some caveats to

00:19:01,400 --> 00:19:06,080
to be aware of when your striping across

00:19:04,790 --> 00:19:07,820
multiple disks you want to make sure

00:19:06,080 --> 00:19:10,520
that your file system boundary is

00:19:07,820 --> 00:19:14,000
actually a stripe aligned and that the

00:19:10,520 --> 00:19:15,470
stripe size is in agreement with the the

00:19:14,000 --> 00:19:18,740
block size of the underlying disk that's

00:19:15,470 --> 00:19:21,430
typically not an issue but this first

00:19:18,740 --> 00:19:25,160
one can be important if you for example

00:19:21,430 --> 00:19:28,640
are using 64k stripe sizes then you want

00:19:25,160 --> 00:19:30,470
to make sure that the the first that the

00:19:28,640 --> 00:19:32,600
start of the file system is actually

00:19:30,470 --> 00:19:36,050
also going to be stripe aligned on the

00:19:32,600 --> 00:19:39,260
disk otherwise I owe to the first well I

00:19:36,050 --> 00:19:40,850
owe to that is faster maligned will

00:19:39,260 --> 00:19:44,150
actually be split over multiple blocks

00:19:40,850 --> 00:19:47,120
and so writing a single block to the

00:19:44,150 --> 00:19:50,990
file system Kendra can require writing

00:19:47,120 --> 00:19:54,640
two blocks to each one inch to the

00:19:50,990 --> 00:19:58,930
underlying disk so this can cause

00:19:54,640 --> 00:19:58,930
performance problems when your striping

00:19:59,020 --> 00:20:02,960
finally there's always the option once

00:20:01,070 --> 00:20:05,150
you've determined that discard where is

00:20:02,960 --> 00:20:07,850
an issue of actually adding faster or

00:20:05,150 --> 00:20:10,370
better hardware I'm emphasizing that

00:20:07,850 --> 00:20:12,470
adding hardware should be a final step

00:20:10,370 --> 00:20:15,350
in the process not an early step in the

00:20:12,470 --> 00:20:17,840
process because there are a lot of cases

00:20:15,350 --> 00:20:19,550
where adding hardware max you either not

00:20:17,840 --> 00:20:21,470
solve the problem or in some cases can

00:20:19,550 --> 00:20:22,640
even make it worse so you really need to

00:20:21,470 --> 00:20:28,160
understand what's going on before you

00:20:22,640 --> 00:20:31,090
get to that point so something that is

00:20:28,160 --> 00:20:33,710
possible in some cases but not all is to

00:20:31,090 --> 00:20:36,140
restructure the workload so that you

00:20:33,710 --> 00:20:38,650
separate critical data which needs to be

00:20:36,140 --> 00:20:41,330
to be persistent across crashes or

00:20:38,650 --> 00:20:45,710
restarts from scratch data which need to

00:20:41,330 --> 00:20:48,980
be reconstructed cheaply or thrown away

00:20:45,710 --> 00:20:50,690
for example temporary files usually

00:20:48,980 --> 00:20:52,580
don't care if the application crashes

00:20:50,690 --> 00:20:55,010
and you have to restart you can just

00:20:52,580 --> 00:20:55,850
either forget about them or carry on

00:20:55,010 --> 00:20:59,960
from the

00:20:55,850 --> 00:21:01,850
the primary source and if you can

00:20:59,960 --> 00:21:04,280
separate our scratch data then I owe to

00:21:01,850 --> 00:21:06,320
the scratch data can be made unreliable

00:21:04,280 --> 00:21:10,070
in the sense that you can either use a

00:21:06,320 --> 00:21:12,679
file system that is may not keep the

00:21:10,070 --> 00:21:17,539
data after its after a crash for example

00:21:12,679 --> 00:21:18,980
if you mount asynchronously then unclean

00:21:17,539 --> 00:21:20,750
shutdowns if you have a sudden power

00:21:18,980 --> 00:21:23,690
loss or the kernel crashes or something

00:21:20,750 --> 00:21:26,900
the faster may get corrupted but if you

00:21:23,690 --> 00:21:30,220
can just ufs the file system and carry

00:21:26,900 --> 00:21:32,830
on where you left off it doesn't matter

00:21:30,220 --> 00:21:35,480
you can often go one step further and

00:21:32,830 --> 00:21:38,419
store temporary data or scratch data in

00:21:35,480 --> 00:21:41,600
memory and if you can eliminate the disk

00:21:38,419 --> 00:21:44,570
entirely and do I owe to a memory file

00:21:41,600 --> 00:21:47,299
system then you'll typically get a get a

00:21:44,570 --> 00:21:48,590
large performance increase from that so

00:21:47,299 --> 00:21:51,340
in previously this is typically you

00:21:48,590 --> 00:21:53,770
would use a a swap back memory disk

00:21:51,340 --> 00:21:57,080
something like like that command there

00:21:53,770 --> 00:21:59,450
and then mounted asynchronously the swap

00:21:57,080 --> 00:22:02,470
backing is a little bit misleading it

00:21:59,450 --> 00:22:06,860
will use swap if there is insufficient

00:22:02,470 --> 00:22:08,360
main memory to satisfy the request so if

00:22:06,860 --> 00:22:10,669
there is memory pressure and you're

00:22:08,360 --> 00:22:13,070
working set of the memory disk +

00:22:10,669 --> 00:22:15,950
application memory exceeds physical RAM

00:22:13,070 --> 00:22:17,900
then data will be pushed to swap but

00:22:15,950 --> 00:22:19,730
this only happens when memory is low so

00:22:17,900 --> 00:22:21,650
it's not going to be writing to swap

00:22:19,730 --> 00:22:23,929
with every every i/o request it's only

00:22:21,650 --> 00:22:25,280
going to happen if needed and typically

00:22:23,929 --> 00:22:29,200
you could then add more memory to

00:22:25,280 --> 00:22:29,200
prevent this or resize your application

00:22:30,370 --> 00:22:37,070
ok so we're moving on to to the next

00:22:33,620 --> 00:22:40,220
topic which is network activity so net

00:22:37,070 --> 00:22:43,130
state is one of the built-in tools for

00:22:40,220 --> 00:22:44,929
looking at what the network is doing so

00:22:43,130 --> 00:22:47,809
in that state minus W will show you a

00:22:44,929 --> 00:22:50,900
breakdown of traffic per second inbound

00:22:47,809 --> 00:22:52,850
and outbound on a given interface so if

00:22:50,900 --> 00:22:54,799
your application is talking to the

00:22:52,850 --> 00:22:56,450
network and things are going slowly you

00:22:54,799 --> 00:22:58,850
can check your notes that is is there a

00:22:56,450 --> 00:23:00,200
is the traffic matching expectations

00:22:58,850 --> 00:23:04,509
maybe there is not enough traffic coming

00:23:00,200 --> 00:23:08,519
in and so this can point you that at a

00:23:04,509 --> 00:23:10,899
and under utilized server for example

00:23:08,519 --> 00:23:12,339
nest I can show your protocol errors so

00:23:10,899 --> 00:23:15,820
there's quite detailed statistics for

00:23:12,339 --> 00:23:18,669
things like UDP Rio checks on failures

00:23:15,820 --> 00:23:21,759
to severe and few transmissions a whole

00:23:18,669 --> 00:23:24,579
variety of corrupted packet statistics

00:23:21,759 --> 00:23:26,979
and so on I can also tell you about

00:23:24,579 --> 00:23:31,359
interface errors which it's perhaps less

00:23:26,979 --> 00:23:35,440
common these days but depending on the

00:23:31,359 --> 00:23:37,929
hardware and if you have a bad switch

00:23:35,440 --> 00:23:40,479
involved it can miss negotiate the

00:23:37,929 --> 00:23:42,429
duplex settings or the the line rate

00:23:40,479 --> 00:23:46,229
settings and then you'll typically get

00:23:42,429 --> 00:23:49,329
very bad performance on that link and

00:23:46,229 --> 00:23:51,129
then there are there are various tools

00:23:49,329 --> 00:23:53,409
for studying in detail what's going on

00:23:51,129 --> 00:23:55,419
in the network TCP dump is the sort of

00:23:53,409 --> 00:23:57,219
classical 1n top is quite useful it

00:23:55,419 --> 00:23:59,319
shows you again process what is what

00:23:57,219 --> 00:24:04,929
what processes on the local machine are

00:23:59,319 --> 00:24:07,509
using using doing traffic Wireshark is

00:24:04,929 --> 00:24:09,219
very detailed tool for studying doing

00:24:07,509 --> 00:24:13,929
packet decoding protocol decoding that

00:24:09,219 --> 00:24:16,629
sort of thing so if you suspect a

00:24:13,929 --> 00:24:18,459
network problem what can you do about it

00:24:16,629 --> 00:24:20,499
well firstly you know check check

00:24:18,459 --> 00:24:22,690
everything is seems to be configured

00:24:20,499 --> 00:24:24,749
properly packet loss is going to kill

00:24:22,690 --> 00:24:29,679
you kill any sort of network throughput

00:24:24,749 --> 00:24:33,309
for some applications the size of the

00:24:29,679 --> 00:24:35,169
socket buffer may be important there are

00:24:33,309 --> 00:24:37,509
some sis controls assist control the

00:24:35,169 --> 00:24:40,209
current I PC Mac sock buff which sets

00:24:37,509 --> 00:24:41,739
the maximum socket buffer and some

00:24:40,209 --> 00:24:44,919
applications we found have been

00:24:41,739 --> 00:24:46,569
particularly old applications the soccer

00:24:44,919 --> 00:24:49,179
buffer was set explicitly to some very

00:24:46,569 --> 00:24:52,929
small value so 32k which may have made

00:24:49,179 --> 00:24:54,879
sense 20 years ago but is no longer an

00:24:52,929 --> 00:24:56,229
appropriate default so you can check

00:24:54,879 --> 00:24:57,369
into the code see if it's if it's

00:24:56,229 --> 00:25:00,190
setting the socket buffer size

00:24:57,369 --> 00:25:03,639
explicitly and it can be said anywhere

00:25:00,190 --> 00:25:08,109
up to the maximum and force by the

00:25:03,639 --> 00:25:10,749
kernel for UDP applications you may need

00:25:08,109 --> 00:25:13,749
to to increase the amount of space

00:25:10,749 --> 00:25:15,129
available for receiving UDP packets you

00:25:13,749 --> 00:25:17,259
do people drop packets the colonel will

00:25:15,129 --> 00:25:18,490
drop packets if the buffer is full so if

00:25:17,259 --> 00:25:21,070
your application is not able to

00:25:18,490 --> 00:25:22,780
drain UDP incoming packets quickly

00:25:21,070 --> 00:25:25,020
enough then you can get packet loss

00:25:22,780 --> 00:25:26,950
because the kernel buffer fills up and

00:25:25,020 --> 00:25:28,690
depending on the application you may

00:25:26,950 --> 00:25:31,750
need to increase the size of the kernel

00:25:28,690 --> 00:25:35,770
buffer to keep things keep this from

00:25:31,750 --> 00:25:37,840
happening the previous II tcp stack is

00:25:35,770 --> 00:25:42,580
pretty much self self-tuning so there

00:25:37,840 --> 00:25:44,320
aren't magic sis controls to do this was

00:25:42,580 --> 00:25:47,010
a lot of there are a lot of sis controls

00:25:44,320 --> 00:25:50,710
but these typically don't need to be set

00:25:47,010 --> 00:25:53,470
one possible exception which it's an

00:25:50,710 --> 00:25:56,200
issue that has been implicated in on

00:25:53,470 --> 00:25:57,910
occasion in performance issues I don't

00:25:56,200 --> 00:25:59,679
know if this is still the case it's

00:25:57,910 --> 00:26:02,860
possible this has been fixed but there's

00:25:59,679 --> 00:26:05,770
a setting the tcp in-flight sysctl is

00:26:02,860 --> 00:26:09,040
enabled by default this tries to do some

00:26:05,770 --> 00:26:11,410
bandwidth estimation for traffic on a

00:26:09,040 --> 00:26:12,850
land it's been rumored to cause problems

00:26:11,410 --> 00:26:15,550
so you could try tweeting this turn it

00:26:12,850 --> 00:26:18,490
off see if it helps it may may not it's

00:26:15,550 --> 00:26:19,900
likely to wait it won't help but if you

00:26:18,490 --> 00:26:22,270
do find this is the problem actually I'd

00:26:19,900 --> 00:26:25,270
like to hear about it because this has

00:26:22,270 --> 00:26:28,600
been sort of been a question mark over

00:26:25,270 --> 00:26:30,309
this for some time don't rule out

00:26:28,600 --> 00:26:32,650
hardware problems especially if you're

00:26:30,309 --> 00:26:36,400
using a fairly low ends Nick then these

00:26:32,650 --> 00:26:37,780
do fail it's sort of good advice to keep

00:26:36,400 --> 00:26:39,490
in mind generally people we tend to

00:26:37,780 --> 00:26:41,080
think of especially if we buy a very

00:26:39,490 --> 00:26:42,550
expensive piece of hardware we we like

00:26:41,080 --> 00:26:46,030
to tell ourselves it's never going to

00:26:42,550 --> 00:26:47,950
fail unfortunately this isn't true and

00:26:46,030 --> 00:26:53,800
so you know always give in mind that

00:26:47,950 --> 00:26:57,610
this can happen ok so the third topic I

00:26:53,800 --> 00:27:00,730
had on my list was device I oh and so

00:26:57,610 --> 00:27:05,320
this will show up in top as as a large

00:27:00,730 --> 00:27:08,830
amount of time spent charged interrupt

00:27:05,320 --> 00:27:10,510
processing in the top header and the vm

00:27:08,830 --> 00:27:12,100
stat- i command will break this down by

00:27:10,510 --> 00:27:14,140
device it will tell you exactly what

00:27:12,100 --> 00:27:16,960
what interrupts are firing to to

00:27:14,140 --> 00:27:19,600
generate this high load so in this

00:27:16,960 --> 00:27:22,860
example here this shows the various I

00:27:19,600 --> 00:27:24,820
our cues and the the rate at which the

00:27:22,860 --> 00:27:27,130
interrupts are firing over the past

00:27:24,820 --> 00:27:30,340
second so in this case we see we've got

00:27:27,130 --> 00:27:34,270
a thousand interrupts firing on IAQ 19

00:27:30,340 --> 00:27:35,650
and the Plus after the device named view

00:27:34,270 --> 00:27:37,270
indicates that it's a shared interrupt

00:27:35,650 --> 00:27:41,130
and actually it's being the same

00:27:37,270 --> 00:27:45,580
interrupt is being shared by multiple

00:27:41,130 --> 00:27:47,669
devices so this is can be implicated in

00:27:45,580 --> 00:27:50,770
performance problems if you have

00:27:47,669 --> 00:27:53,020
particularly if you have one or more

00:27:50,770 --> 00:27:56,490
devices showing that interrupts that are

00:27:53,020 --> 00:28:00,580
both still require the giant log fizzes

00:27:56,490 --> 00:28:03,520
sometimes legacy devices but if you have

00:28:00,580 --> 00:28:05,230
two devices that both require giant that

00:28:03,520 --> 00:28:08,020
Sharon interrupts whenever one of them

00:28:05,230 --> 00:28:10,029
gets in whenever an interrupt fires both

00:28:08,020 --> 00:28:11,500
devices will need to wake up grab the

00:28:10,029 --> 00:28:13,779
giant lock and then fight for it of

00:28:11,500 --> 00:28:15,789
course and check whether the interrupt

00:28:13,779 --> 00:28:19,659
was directed to them so if you have this

00:28:15,789 --> 00:28:22,809
the sharing situation then then giant

00:28:19,659 --> 00:28:27,130
log drivers can can cause a performance

00:28:22,809 --> 00:28:29,740
problem and even if not then well if you

00:28:27,130 --> 00:28:32,980
if you find okay if you find such a

00:28:29,740 --> 00:28:35,200
sharing a giant contention issue from

00:28:32,980 --> 00:28:37,750
your interrupts shared interrupt then

00:28:35,200 --> 00:28:39,279
you may be able to get away with either

00:28:37,750 --> 00:28:40,570
removing the device from your Colonel if

00:28:39,279 --> 00:28:45,010
you're not using it for example in this

00:28:40,570 --> 00:28:47,710
case if USB was implicated in in a giant

00:28:45,010 --> 00:28:50,080
contention problem then if i'm not using

00:28:47,710 --> 00:28:52,600
USB i can just remove my kernel and work

00:28:50,080 --> 00:28:54,340
around the problem sometimes you can you

00:28:52,600 --> 00:28:55,990
can resolve this by physically removing

00:28:54,340 --> 00:29:01,120
and moving a driest device to a

00:28:55,990 --> 00:29:04,289
different pci slot for example but that

00:29:01,120 --> 00:29:07,120
may not all be past always be possible

00:29:04,289 --> 00:29:11,940
okay so coming back to to context

00:29:07,120 --> 00:29:16,659
switches this was shown by the top I oh

00:29:11,940 --> 00:29:17,679
list and so the two the two types that

00:29:16,659 --> 00:29:19,179
were listed there are voluntary and

00:29:17,679 --> 00:29:22,179
involuntary so voluntary conduct

00:29:19,179 --> 00:29:24,490
switches occur when a process blocks

00:29:22,179 --> 00:29:26,679
waiting for a resource so it makes the

00:29:24,490 --> 00:29:29,080
decision to to try and acquire a

00:29:26,679 --> 00:29:31,740
resource and this will possibly block

00:29:29,080 --> 00:29:33,580
and called a voluntary complex which

00:29:31,740 --> 00:29:35,380
involuntary context switches are when

00:29:33,580 --> 00:29:37,240
the colonel decides at the process it's

00:29:35,380 --> 00:29:39,130
time for it to stop running it's had its

00:29:37,240 --> 00:29:42,280
its its charge of the CPU and now it's

00:29:39,130 --> 00:29:44,210
time to run something else so context

00:29:42,280 --> 00:29:46,400
switches can be indications of

00:29:44,210 --> 00:29:48,680
performance problems they can be a

00:29:46,400 --> 00:29:51,460
symptom for example of resource

00:29:48,680 --> 00:29:54,050
contention in the kernel for example if

00:29:51,460 --> 00:29:56,000
processes are blocked on a mutex or if

00:29:54,050 --> 00:29:59,180
they're contending on a mutex this will

00:29:56,000 --> 00:30:03,320
show up as contention as high context

00:29:59,180 --> 00:30:05,390
switches it can also be indicating an

00:30:03,320 --> 00:30:07,010
application design problem because for

00:30:05,390 --> 00:30:10,610
example if you have a multi-threaded

00:30:07,010 --> 00:30:13,190
workload and you assign you configured

00:30:10,610 --> 00:30:15,860
the system that the application to use

00:30:13,190 --> 00:30:17,180
too many worker threads compared to the

00:30:15,860 --> 00:30:20,510
amount of work that's being done per

00:30:17,180 --> 00:30:22,880
thread each thread will will run do a

00:30:20,510 --> 00:30:26,090
tiny bit of work go back to sleep or

00:30:22,880 --> 00:30:27,470
maybe block on a lock and so you spend a

00:30:26,090 --> 00:30:28,910
lot of time switching between threads

00:30:27,470 --> 00:30:32,600
and not enough time actually doing work

00:30:28,910 --> 00:30:41,500
so this can indicate issues either in

00:30:32,600 --> 00:30:44,060
the application or in the colonel so

00:30:41,500 --> 00:30:45,770
typically applications will interact

00:30:44,060 --> 00:30:49,160
with the colonel through through doing

00:30:45,770 --> 00:30:52,670
system calls and this is another way in

00:30:49,160 --> 00:30:54,950
which things can go wrong so here vmstat

00:30:52,670 --> 00:30:56,540
is is a tool that can show you the rate

00:30:54,950 --> 00:30:58,280
at siskel of syscalls that happen

00:30:56,540 --> 00:30:59,720
against is a high-level overview it

00:30:58,280 --> 00:31:02,690
shows you what's going on system-wide

00:30:59,720 --> 00:31:06,110
and the relevant column here is this sy

00:31:02,690 --> 00:31:09,410
column and the first line is a an

00:31:06,110 --> 00:31:11,560
overall average since the system booted

00:31:09,410 --> 00:31:13,730
and then subsequent lines are

00:31:11,560 --> 00:31:15,950
instantaneous values well over the

00:31:13,730 --> 00:31:18,860
previous second and this shows us that

00:31:15,950 --> 00:31:22,790
this workload was performing 700,000

00:31:18,860 --> 00:31:24,710
syscalls per second and this even on a

00:31:22,790 --> 00:31:27,110
largest and pieces and this is this is a

00:31:24,710 --> 00:31:31,610
lot of syscalls so in this case if you

00:31:27,110 --> 00:31:34,070
saw this you would this this this should

00:31:31,610 --> 00:31:36,020
raise a red flag and point to a problem

00:31:34,070 --> 00:31:38,150
it also typically shows up if you're

00:31:36,020 --> 00:31:39,770
doing a lot of system calls then each

00:31:38,150 --> 00:31:42,800
system call is operating inside the

00:31:39,770 --> 00:31:45,770
kernel and so it is charged as Colonel

00:31:42,800 --> 00:31:48,080
CPU use and we also see it appear here

00:31:45,770 --> 00:31:50,750
in the system CPU percentage years

00:31:48,080 --> 00:31:54,770
column here is what also sharp in top as

00:31:50,750 --> 00:31:56,380
being the system's abusive percentage so

00:31:54,770 --> 00:31:58,990
we're spending in this case about six

00:31:56,380 --> 00:32:03,160
sixty to sixty four percent of the time

00:31:58,990 --> 00:32:06,190
in the kernel satisfying or processing

00:32:03,160 --> 00:32:08,620
syscalls and that's that's unusual so

00:32:06,190 --> 00:32:11,380
that that points to a problem here so

00:32:08,620 --> 00:32:12,790
how can you go about digging further so

00:32:11,380 --> 00:32:14,890
there are various tools for example Kate

00:32:12,790 --> 00:32:18,700
rates and trusts estrace is another one

00:32:14,890 --> 00:32:20,110
that let you attach to a process and it

00:32:18,700 --> 00:32:23,560
prints out a list of various is call

00:32:20,110 --> 00:32:25,930
that the process does so it's quite a

00:32:23,560 --> 00:32:27,460
raw feed of data so it can be a lot of

00:32:25,930 --> 00:32:30,070
data that you have to post process or

00:32:27,460 --> 00:32:31,570
grapple so on but it tells you exactly

00:32:30,070 --> 00:32:34,300
what the process is doing what it's

00:32:31,570 --> 00:32:39,370
every time it enters the kernel and this

00:32:34,300 --> 00:32:40,990
if there's a a typically this kind of

00:32:39,370 --> 00:32:42,460
magnitude of problems will show out very

00:32:40,990 --> 00:32:43,750
clearly if you're doing seven thousand

00:32:42,460 --> 00:32:45,010
of them then typically this is this is

00:32:43,750 --> 00:32:48,180
going to be a small number of six calls

00:32:45,010 --> 00:32:51,640
that are happening very frequently

00:32:48,180 --> 00:32:54,400
another interesting and new way of

00:32:51,640 --> 00:32:58,060
studying what Apple processes are doing

00:32:54,400 --> 00:32:59,830
with how they're interacting with the

00:32:58,060 --> 00:33:01,600
colonel is using the audit subsystem

00:32:59,830 --> 00:33:03,910
which is I think it's been present since

00:33:01,600 --> 00:33:06,190
the previously five days but it

00:33:03,910 --> 00:33:08,320
certainly it's relatively new and it's

00:33:06,190 --> 00:33:11,830
probably not very well well known this

00:33:08,320 --> 00:33:13,630
this is intended partly as a an audit

00:33:11,830 --> 00:33:15,790
trail facility so that you can for

00:33:13,630 --> 00:33:17,470
example get Sakura logging water trails

00:33:15,790 --> 00:33:19,240
of what processes do and what what

00:33:17,470 --> 00:33:21,520
system activities occur such as logins

00:33:19,240 --> 00:33:24,220
and so on but it can be configured to do

00:33:21,520 --> 00:33:26,410
very fine fine logging of process

00:33:24,220 --> 00:33:29,260
activities including logging it just

00:33:26,410 --> 00:33:32,680
consists call with with arguments and so

00:33:29,260 --> 00:33:34,900
on so this can be can be great to

00:33:32,680 --> 00:33:36,850
actually if you turn on audit you get a

00:33:34,900 --> 00:33:40,360
feed of what's coming out for each

00:33:36,850 --> 00:33:43,420
process and you can filter this in

00:33:40,360 --> 00:33:45,610
various ways typically with these high

00:33:43,420 --> 00:33:47,020
volume kinds of data flows you want to

00:33:45,610 --> 00:33:49,030
try and log to memory disk if you can

00:33:47,020 --> 00:33:53,530
because you don't want the i/o to be

00:33:49,030 --> 00:33:55,870
slowing down the process so here if we

00:33:53,530 --> 00:33:59,800
were on Kate's race on my sequel process

00:33:55,870 --> 00:34:01,390
that was generating was I oh we see that

00:33:59,800 --> 00:34:03,220
over and over again it's doing the

00:34:01,390 --> 00:34:06,550
periods his call and reading a few bites

00:34:03,220 --> 00:34:08,200
out of the kernel and what this points

00:34:06,550 --> 00:34:09,500
to in the end is it's it's the

00:34:08,200 --> 00:34:12,409
application was missing

00:34:09,500 --> 00:34:13,610
again every time there was the caching

00:34:12,409 --> 00:34:14,929
parameters weren't set up properly is

00:34:13,610 --> 00:34:16,280
every time it wanted to read from the

00:34:14,929 --> 00:34:18,230
database and had to read from the file

00:34:16,280 --> 00:34:20,750
system it wasn't cashing it in his land

00:34:18,230 --> 00:34:21,980
so this is this kind of thing is going

00:34:20,750 --> 00:34:23,419
to kill the performance if you have to

00:34:21,980 --> 00:34:24,560
each time you wanted to read from the

00:34:23,419 --> 00:34:26,780
database you have to cross into the

00:34:24,560 --> 00:34:28,370
kernel and then it would be satisfied

00:34:26,780 --> 00:34:30,440
from cash but it's cash in the kernel

00:34:28,370 --> 00:34:37,340
which is too far from the application to

00:34:30,440 --> 00:34:39,770
be high performance okay so I've

00:34:37,340 --> 00:34:42,590
mentioned some aspects of Colonel

00:34:39,770 --> 00:34:49,149
activity that can can be implicated in

00:34:42,590 --> 00:34:49,149
performance issues something else that

00:34:50,139 --> 00:34:56,389
can show up on your workloads although

00:34:52,850 --> 00:34:59,200
hopefully will show up very rarely is hi

00:34:56,389 --> 00:35:01,670
lock contention on Colonel new taxes

00:34:59,200 --> 00:35:03,410
this can indicate a kernel scalability

00:35:01,670 --> 00:35:04,850
problem I can also indicate an

00:35:03,410 --> 00:35:07,190
application problem again perhaps the

00:35:04,850 --> 00:35:09,350
application is Miss designed so that is

00:35:07,190 --> 00:35:11,900
it is using for example it has whole lot

00:35:09,350 --> 00:35:13,160
contention on its pthread mutex is these

00:35:11,900 --> 00:35:15,560
will show up in the kernel because of

00:35:13,160 --> 00:35:17,630
the way the previous the user line lock

00:35:15,560 --> 00:35:19,370
implementation is it will actually in

00:35:17,630 --> 00:35:22,400
some some cases it will enter the kernel

00:35:19,370 --> 00:35:26,510
to like to block so it can again either

00:35:22,400 --> 00:35:27,800
indicator kernel or use LAN problem an

00:35:26,510 --> 00:35:29,060
interesting tool for studying what

00:35:27,800 --> 00:35:31,520
processes are doing in the colonel is

00:35:29,060 --> 00:35:34,640
proc stat which appeared in previously 7

00:35:31,520 --> 00:35:36,080
it has an option to obtain a stack trace

00:35:34,640 --> 00:35:38,630
of processes that are blocked in the

00:35:36,080 --> 00:35:41,750
kernel so if you have if you see from

00:35:38,630 --> 00:35:43,180
other other signals like top or some of

00:35:41,750 --> 00:35:46,070
these other these other ones we've seen

00:35:43,180 --> 00:35:48,290
but the process is entering the colonel

00:35:46,070 --> 00:35:50,300
a lot Kate sorry proc stat will show you

00:35:48,290 --> 00:35:51,950
exactly what is the stack trace and from

00:35:50,300 --> 00:35:53,390
this you can hand this to a developer

00:35:51,950 --> 00:35:56,660
and this will give information about

00:35:53,390 --> 00:35:58,400
what what's going on it also shows

00:35:56,660 --> 00:36:03,320
various other useful information about

00:35:58,400 --> 00:36:07,160
the process so looking at Colonel Locke

00:36:03,320 --> 00:36:08,450
operations very useful tool is logged

00:36:07,160 --> 00:36:12,170
profiling this is an option you can

00:36:08,450 --> 00:36:16,940
compile into your kernel and when you

00:36:12,170 --> 00:36:18,790
turn it on it will record every time a

00:36:16,940 --> 00:36:21,650
lock is acquired or mutex or a

00:36:18,790 --> 00:36:23,060
read/write lock or some of the various

00:36:21,650 --> 00:36:27,200
other kinds of locks locking perimeters

00:36:23,060 --> 00:36:28,520
that we have in freebsd there is a

00:36:27,200 --> 00:36:30,050
performance overhead when it's when it's

00:36:28,520 --> 00:36:32,330
in use when it's actively profiling

00:36:30,050 --> 00:36:34,370
which depends on the hardware time

00:36:32,330 --> 00:36:37,160
Carter because if it needs to to read

00:36:34,370 --> 00:36:40,160
the tie every time the lock is acquired

00:36:37,160 --> 00:36:44,780
and released this is to time counter

00:36:40,160 --> 00:36:46,310
calls so generally you want to use the

00:36:44,780 --> 00:36:49,640
fastest possible time calendar which is

00:36:46,310 --> 00:36:51,290
usually the tsc on modern hardware on

00:36:49,640 --> 00:36:53,500
very modern hardware this is actually

00:36:51,290 --> 00:36:57,350
usable on SMP systems on older hardware

00:36:53,500 --> 00:36:59,150
it wasn't so if you can get away with

00:36:57,350 --> 00:37:01,640
this you need to enable it with with the

00:36:59,150 --> 00:37:03,170
sis control here by the way my slides

00:37:01,640 --> 00:37:05,120
would be available on I'll be providing

00:37:03,170 --> 00:37:07,990
this is the FA's name guys afterwards so

00:37:05,120 --> 00:37:11,780
it should be on the first in website

00:37:07,990 --> 00:37:14,000
okay so how do you use this you enable

00:37:11,780 --> 00:37:15,860
us its control do your work load and

00:37:14,000 --> 00:37:17,450
then turn it off and there's another

00:37:15,860 --> 00:37:20,600
sysctl I'll show in on the next slide

00:37:17,450 --> 00:37:23,000
which dumps the the output but the data

00:37:20,600 --> 00:37:24,890
that's recorded are things like the file

00:37:23,000 --> 00:37:27,230
in line in the source code where the

00:37:24,890 --> 00:37:29,780
lock operation was to find in the mutex

00:37:27,230 --> 00:37:32,240
acquire or the ASX acquire or so on and

00:37:29,780 --> 00:37:34,220
then it aggregates useful statistics

00:37:32,240 --> 00:37:36,350
like what does the maximum time is luck

00:37:34,220 --> 00:37:38,780
was held so this shows you perhaps

00:37:36,350 --> 00:37:40,910
there's a code path where it's doing a

00:37:38,780 --> 00:37:43,910
very slow operation while holding a lock

00:37:40,910 --> 00:37:45,110
and while this is true while it's while

00:37:43,910 --> 00:37:48,080
it's this slow operation is in progress

00:37:45,110 --> 00:37:50,360
nothing else can acquire the lock it

00:37:48,080 --> 00:37:53,240
shows you the total time that that was

00:37:50,360 --> 00:37:55,490
spent across all acts all acquire

00:37:53,240 --> 00:37:56,870
operations that were spent waiting it

00:37:55,490 --> 00:37:59,780
was spent block waiting for something

00:37:56,870 --> 00:38:02,180
else to release it and the average times

00:37:59,780 --> 00:38:07,190
and how how many times the lock was

00:38:02,180 --> 00:38:09,140
contended and so on so this is what

00:38:07,190 --> 00:38:11,570
typical output will look like and this

00:38:09,140 --> 00:38:13,000
this was a somewhat contrived example I

00:38:11,570 --> 00:38:16,070
had to fiddle the numbers actually but

00:38:13,000 --> 00:38:20,990
the typical thing you see here so I'm

00:38:16,070 --> 00:38:24,820
sorting by total wait time and when you

00:38:20,990 --> 00:38:27,380
have a high contention situation the

00:38:24,820 --> 00:38:29,480
you'll typically find that is one one

00:38:27,380 --> 00:38:31,400
particular lock that is we spend a lot

00:38:29,480 --> 00:38:33,849
of time waiting for and this can point

00:38:31,400 --> 00:38:36,109
to a bottleneck in this case

00:38:33,849 --> 00:38:40,519
there was a bottleneck involving this

00:38:36,109 --> 00:38:42,049
his name cash mutex it was actually

00:38:40,519 --> 00:38:43,489
doing a lot of operations this was

00:38:42,049 --> 00:38:44,900
required every time somebody something

00:38:43,489 --> 00:38:49,069
something tried to stat a file i tried

00:38:44,900 --> 00:38:50,269
to resolve a name cache entry it was

00:38:49,069 --> 00:38:51,559
able to we were able to just convert

00:38:50,269 --> 00:38:53,029
this to a read/write lock and most of

00:38:51,559 --> 00:38:54,769
the operations can now be held with the

00:38:53,029 --> 00:38:57,469
reader lock instead of acquiring an

00:38:54,769 --> 00:38:58,910
exclusive lock and so in 8.0 we fix this

00:38:57,469 --> 00:39:01,989
in some workloads and now seeing twenty

00:38:58,910 --> 00:39:04,160
percent performance increases from this

00:39:01,989 --> 00:39:05,359
there's a tool I won't have time to go

00:39:04,160 --> 00:39:07,660
into it would be an entire talk on

00:39:05,359 --> 00:39:11,660
itself in fact it was an entire talk

00:39:07,660 --> 00:39:14,719
earlier today which I guess most of you

00:39:11,660 --> 00:39:17,809
missed out on unfortunately dtrace is

00:39:14,719 --> 00:39:21,019
part of freebsd as a previously 7.1 it's

00:39:17,809 --> 00:39:23,450
a system that son Jesus insularis and

00:39:21,019 --> 00:39:26,299
it's now part of OS 10 and previously

00:39:23,450 --> 00:39:28,339
and possibly other operating systems in

00:39:26,299 --> 00:39:31,069
future but it's really a very powerful

00:39:28,339 --> 00:39:36,170
way of writing small scripts that are

00:39:31,069 --> 00:39:37,700
executed in upon probe events so that

00:39:36,170 --> 00:39:39,140
there are a whole bunch of probe events

00:39:37,700 --> 00:39:40,459
to find either in the kernel or New

00:39:39,140 --> 00:39:42,589
Zealand there's gonna be things like

00:39:40,459 --> 00:39:45,949
functioning entry and exit or you can

00:39:42,589 --> 00:39:47,779
define your own trace points that occur

00:39:45,949 --> 00:39:49,369
perhaps of some higher-level operations

00:39:47,779 --> 00:39:52,039
like beginning an i/o ending and I oh

00:39:49,369 --> 00:39:54,890
that sort of thing and you can configure

00:39:52,039 --> 00:39:57,259
the script to be executed upon any of

00:39:54,890 --> 00:39:59,029
these probit probe events and then it

00:39:57,259 --> 00:40:00,650
can aggregate statistics like how many

00:39:59,029 --> 00:40:02,180
times was it called what was the average

00:40:00,650 --> 00:40:04,819
value of some argument of this at this

00:40:02,180 --> 00:40:07,130
time what is the latency between

00:40:04,819 --> 00:40:08,390
beginning and ending of an operation so

00:40:07,130 --> 00:40:10,699
this is a really powerful way of

00:40:08,390 --> 00:40:14,930
drilling down and finding out exactly

00:40:10,699 --> 00:40:17,440
what's going on in the anywhere in the

00:40:14,930 --> 00:40:20,779
in the system at the moment it's only

00:40:17,440 --> 00:40:23,959
supported for profile for profiling the

00:40:20,779 --> 00:40:26,119
colonel in previously 7.1 hopefully the

00:40:23,959 --> 00:40:29,539
near future will also finish user land

00:40:26,119 --> 00:40:30,829
tracing but this is a great thing that

00:40:29,539 --> 00:40:35,479
you should should check out there's a

00:40:30,829 --> 00:40:36,619
youtube video G trace review and son

00:40:35,479 --> 00:40:39,230
have some great docs on that as well so

00:40:36,619 --> 00:40:46,049
have a look at it it's really cool

00:40:39,230 --> 00:40:49,670
so modern modern CPUs have a lot of

00:40:46,049 --> 00:40:52,230
performance counters on the silicon and

00:40:49,670 --> 00:40:54,630
previously has it has a interface for

00:40:52,230 --> 00:40:58,369
accessing these and using it to profile

00:40:54,630 --> 00:41:02,759
the application and the colonel workload

00:40:58,369 --> 00:41:07,140
so you can profile things like how means

00:41:02,759 --> 00:41:10,049
well profiling things like where did the

00:41:07,140 --> 00:41:12,180
CPU spend spend most best i'm retiring

00:41:10,049 --> 00:41:14,880
instructions where where the cache

00:41:12,180 --> 00:41:17,880
misses that occurred where did it miss

00:41:14,880 --> 00:41:21,559
produce miss predict branches that kind

00:41:17,880 --> 00:41:23,400
of thing and the HW pmc tool in freebsd

00:41:21,559 --> 00:41:26,359
can either do instruction level

00:41:23,400 --> 00:41:28,499
profiling where I just tells you that

00:41:26,359 --> 00:41:29,819
some percentage of the time the

00:41:28,499 --> 00:41:32,249
instruction pointer was of this line of

00:41:29,819 --> 00:41:34,650
code when this event happened or it can

00:41:32,249 --> 00:41:37,380
also reconstruct the cool grass of the

00:41:34,650 --> 00:41:39,569
of the the process so i can tell you

00:41:37,380 --> 00:41:41,970
that exactly what set of functions were

00:41:39,569 --> 00:41:45,059
called to reach that point so this is a

00:41:41,970 --> 00:41:46,410
great way of profiling what either your

00:41:45,059 --> 00:41:49,289
user application is doing or also what

00:41:46,410 --> 00:41:50,400
the colonel is doing it has a very low

00:41:49,289 --> 00:41:52,440
overhead when it's running because it's

00:41:50,400 --> 00:41:54,089
actually using the hardware using things

00:41:52,440 --> 00:41:55,799
built into the hardware rather than

00:41:54,089 --> 00:41:59,130
having to emulate things or do profiling

00:41:55,799 --> 00:42:02,519
and software so this is a really useful

00:41:59,130 --> 00:42:05,809
tool and it post process is to jeepers

00:42:02,519 --> 00:42:09,119
off so you familiar with g prof it

00:42:05,809 --> 00:42:11,130
accepts the PMC stat output so again

00:42:09,119 --> 00:42:15,239
there's a short hell too about how to

00:42:11,130 --> 00:42:17,069
use this in freebsd there's a i'll just

00:42:15,239 --> 00:42:20,249
mentioned in passing there's a a nifty

00:42:17,069 --> 00:42:21,779
tool that you can use to visualize the

00:42:20,249 --> 00:42:23,309
scheduler activity so you can see how

00:42:21,779 --> 00:42:26,009
your processes are being scheduled where

00:42:23,309 --> 00:42:27,660
they're blocking maybe they spend a lot

00:42:26,009 --> 00:42:29,339
of time blocking on a resource or

00:42:27,660 --> 00:42:32,369
yielding super you to each other and

00:42:29,339 --> 00:42:37,259
this shows up it shows you graphically

00:42:32,369 --> 00:42:39,779
exactly what he was doing and and how y

00:42:37,259 --> 00:42:45,809
decisions were made to change to change

00:42:39,779 --> 00:42:48,319
scheduling previously 8 has a debugging

00:42:45,809 --> 00:42:51,119
talk with sleep you profiling which

00:42:48,319 --> 00:42:52,220
shows you the aggregate of these weight

00:42:51,119 --> 00:42:54,680
channels that

00:42:52,220 --> 00:42:56,300
I mentioned a few times earlier so it

00:42:54,680 --> 00:42:58,220
can show you how many times the process

00:42:56,300 --> 00:43:01,670
blocked in any given wait state for

00:42:58,220 --> 00:43:04,849
example this was a typical output here

00:43:01,670 --> 00:43:06,349
and it may show you that in spending

00:43:04,849 --> 00:43:11,210
more time waiting on certain resources

00:43:06,349 --> 00:43:14,599
than you thought it should okay so a few

00:43:11,210 --> 00:43:16,730
words about about Colonel tuning so free

00:43:14,599 --> 00:43:19,400
bc is largely self cheating so there's

00:43:16,730 --> 00:43:20,990
not a lot you need to do to to a typical

00:43:19,400 --> 00:43:22,849
system to make it work well out of the

00:43:20,990 --> 00:43:25,820
box that the defaults are pretty good

00:43:22,849 --> 00:43:27,770
and things will also auto auto size and

00:43:25,820 --> 00:43:30,530
what I tuned based on either the

00:43:27,770 --> 00:43:32,740
hardware that it sees or in some cases

00:43:30,530 --> 00:43:35,240
on the workload that it encounters the

00:43:32,740 --> 00:43:36,650
best advice for forgetting performance

00:43:35,240 --> 00:43:39,859
out of a previously Colonel is to run a

00:43:36,650 --> 00:43:42,200
modern Colonel we put a lot of work into

00:43:39,859 --> 00:43:43,609
previously seven and there's obviously a

00:43:42,200 --> 00:43:46,220
lot of ongoing work into improving

00:43:43,609 --> 00:43:47,480
performance so a good first stop is to

00:43:46,220 --> 00:43:52,609
make sure you're running the most recent

00:43:47,480 --> 00:43:54,290
version so the uls scheduler was new in

00:43:52,609 --> 00:43:55,640
freebsd seven well rather was near few

00:43:54,290 --> 00:43:58,670
years ago but the version is previously

00:43:55,640 --> 00:43:59,930
seven was rewritten and it now it's free

00:43:58,670 --> 00:44:01,160
from the performance problems that it

00:43:59,930 --> 00:44:04,040
had in the past and actually performs

00:44:01,160 --> 00:44:06,920
very well it's the default in previously

00:44:04,040 --> 00:44:10,400
7.1 so this is only relevant if you're

00:44:06,920 --> 00:44:13,070
using 70 but it will typically give a

00:44:10,400 --> 00:44:17,720
performance increase on on on most

00:44:13,070 --> 00:44:19,280
workloads and this comes from it does a

00:44:17,720 --> 00:44:21,050
lot of work to maintain CPU affinity

00:44:19,280 --> 00:44:22,430
which which can really help if you can

00:44:21,050 --> 00:44:25,190
keep the caches warm in between

00:44:22,430 --> 00:44:26,630
scheduling decisions then you'll get a

00:44:25,190 --> 00:44:29,740
lot better performance and if you if you

00:44:26,630 --> 00:44:32,869
have to keep satisfying cache misses

00:44:29,740 --> 00:44:34,940
brigus the 8 has a system called

00:44:32,869 --> 00:44:37,940
superpages which is the equivalent of

00:44:34,940 --> 00:44:40,220
linux is huge to your BFS which is using

00:44:37,940 --> 00:44:42,440
larger tlb entries the difference in

00:44:40,220 --> 00:44:43,460
freebsd is that it's all automatic you

00:44:42,440 --> 00:44:45,109
don't have to do any manual

00:44:43,460 --> 00:44:47,270
configuration any manual changes the

00:44:45,109 --> 00:44:50,000
application the current will

00:44:47,270 --> 00:44:53,330
automatically promote 4k pages to to

00:44:50,000 --> 00:44:55,700
larger pages on demand and then deals

00:44:53,330 --> 00:44:57,770
with the the fragmentation issues it can

00:44:55,700 --> 00:44:59,119
happen and so on and actually this on by

00:44:57,770 --> 00:45:01,880
default as well now so if you're running

00:44:59,119 --> 00:45:03,200
previously aids then superpages will be

00:45:01,880 --> 00:45:05,000
on by default this can also give

00:45:03,200 --> 00:45:06,990
depending on the workload

00:45:05,000 --> 00:45:08,490
it's common to get a ten or twenty

00:45:06,990 --> 00:45:10,200
percent performance increase from from

00:45:08,490 --> 00:45:12,660
running this especially if you have a

00:45:10,200 --> 00:45:14,040
very memory intensive workload such as

00:45:12,660 --> 00:45:17,160
Java for example really benefits from

00:45:14,040 --> 00:45:18,300
this if you're running development

00:45:17,160 --> 00:45:20,550
version like freebsd eight then

00:45:18,300 --> 00:45:21,570
debugging is enabled by default so you

00:45:20,550 --> 00:45:23,640
know obviously that's not going to help

00:45:21,570 --> 00:45:24,990
performance so make sure you turn that

00:45:23,640 --> 00:45:29,460
off if you're if you're trying out the

00:45:24,990 --> 00:45:31,079
devel vision some other applications do

00:45:29,460 --> 00:45:34,829
strange things with the time Carter for

00:45:31,079 --> 00:45:36,540
example Java 1.5 does an insane number

00:45:34,829 --> 00:45:38,250
of get time of day sees calls so it's

00:45:36,540 --> 00:45:40,740
really some reason it once in exactly

00:45:38,250 --> 00:45:41,940
what time it is all the time and it can

00:45:40,740 --> 00:45:44,910
actually matter that if you're using a

00:45:41,940 --> 00:45:47,190
slow time calendar so this is somewhat

00:45:44,910 --> 00:45:52,740
workload specific but keep that in mind

00:45:47,190 --> 00:45:54,930
as well okay so in my last few minutes I

00:45:52,740 --> 00:45:57,990
want to say a few words about how did

00:45:54,930 --> 00:46:00,000
how to go about benchmarking system so

00:45:57,990 --> 00:46:01,349
suppose you identified you think there's

00:46:00,000 --> 00:46:03,119
a problem and you think you have an idea

00:46:01,349 --> 00:46:07,020
about how to fix it or at least what to

00:46:03,119 --> 00:46:09,420
try to fix it benchmarking turns out

00:46:07,020 --> 00:46:11,369
beats one of these sort of annoying

00:46:09,420 --> 00:46:14,220
things to do properly and so people are

00:46:11,369 --> 00:46:15,780
tempted to skip steps and this often can

00:46:14,220 --> 00:46:20,670
bite them afterwards if they if they do

00:46:15,780 --> 00:46:22,349
that so that the the key that you want

00:46:20,670 --> 00:46:24,960
to do is to identify a self-contained

00:46:22,349 --> 00:46:27,660
workload that you can repeat as many

00:46:24,960 --> 00:46:29,250
times as you need to so if you're trying

00:46:27,660 --> 00:46:31,260
to measure things the idea is to

00:46:29,250 --> 00:46:32,760
minimize the number of variables at any

00:46:31,260 --> 00:46:34,770
stage so you want to keep everything

00:46:32,760 --> 00:46:36,750
constant but then vary only one thing

00:46:34,770 --> 00:46:38,609
and if your workload itself is varying

00:46:36,750 --> 00:46:40,109
then anything you do is going to be more

00:46:38,609 --> 00:46:43,230
than one so you want to keep the

00:46:40,109 --> 00:46:44,880
workload constant and repeatable so you

00:46:43,230 --> 00:46:47,670
can demonstrate the problem clearly and

00:46:44,880 --> 00:46:52,020
then make changes one at a time against

00:46:47,670 --> 00:46:55,170
that against their workload so measuring

00:46:52,020 --> 00:46:57,750
if you have a metric that can get a

00:46:55,170 --> 00:46:59,940
number out of this benchmark and you

00:46:57,750 --> 00:47:01,920
want to compare the numbers it's

00:46:59,940 --> 00:47:05,160
actually it turns out humans are very

00:47:01,920 --> 00:47:07,680
bad at comparing sets of data by a we

00:47:05,160 --> 00:47:10,290
tend to miss patterns we tend to read

00:47:07,680 --> 00:47:12,480
our own meaning into things we tend to

00:47:10,290 --> 00:47:14,940
if we think something should make an

00:47:12,480 --> 00:47:16,530
improvement we tend to only look at the

00:47:14,940 --> 00:47:17,790
data that shows an improvement and not

00:47:16,530 --> 00:47:18,090
sure look at the data that doesn't show

00:47:17,790 --> 00:47:21,000
the

00:47:18,090 --> 00:47:23,160
movement so you really want to trust in

00:47:21,000 --> 00:47:24,900
statistics to do this and previously has

00:47:23,160 --> 00:47:28,130
a really useful tool called mini stat

00:47:24,900 --> 00:47:32,910
that takes it is only job in life is to

00:47:28,130 --> 00:47:35,970
to take two or three sets of data say

00:47:32,910 --> 00:47:38,640
two sets of data which could be the

00:47:35,970 --> 00:47:42,240
output of a benchmark into before before

00:47:38,640 --> 00:47:44,190
and after the you made the change and

00:47:42,240 --> 00:47:48,870
then it will do some statistical tests

00:47:44,190 --> 00:47:51,780
on this data to determine statistically

00:47:48,870 --> 00:47:54,240
do these data sets represent different

00:47:51,780 --> 00:47:55,320
come from different data sources uses

00:47:54,240 --> 00:47:58,200
something called a student's t-test

00:47:55,320 --> 00:48:00,120
which which can distinguish between data

00:47:58,200 --> 00:48:02,460
that was coming from different different

00:48:00,120 --> 00:48:03,840
sources for example a colonel with a

00:48:02,460 --> 00:48:08,790
performance problem and a colonel with

00:48:03,840 --> 00:48:12,060
with no performance problem it's very

00:48:08,790 --> 00:48:13,680
easy to when you're looking at a set of

00:48:12,060 --> 00:48:15,690
data where some numbers are higher but

00:48:13,680 --> 00:48:17,310
some numbers are lower for the change

00:48:15,690 --> 00:48:19,290
you're not sure if it's made a problem

00:48:17,310 --> 00:48:22,890
it's easier to easy to assume that the

00:48:19,290 --> 00:48:24,540
numbers are actually going up where the

00:48:22,890 --> 00:48:26,430
data may actually be insufficient in a

00:48:24,540 --> 00:48:28,500
statistical sense to draw that

00:48:26,430 --> 00:48:30,450
conclusion so many state will tell you

00:48:28,500 --> 00:48:31,800
when this happens and then you need it

00:48:30,450 --> 00:48:36,740
tells you need some you need to get more

00:48:31,800 --> 00:48:42,950
data out so for example this was a a

00:48:36,740 --> 00:48:45,750
simple thing I ran my sequel benchmark

00:48:42,950 --> 00:48:48,120
with the two schedulers with the older

00:48:45,750 --> 00:48:49,590
for bsd scheduler and with the ule

00:48:48,120 --> 00:48:52,140
schedule it's now two fold in previously

00:48:49,590 --> 00:48:54,300
7.1 and the numbers that I got out

00:48:52,140 --> 00:48:55,950
represent the number of transactions per

00:48:54,300 --> 00:48:57,810
second the database is doing on this

00:48:55,950 --> 00:49:02,130
benchmark in this case higher is better

00:48:57,810 --> 00:49:03,600
and passing it's a mini stat the w60

00:49:02,130 --> 00:49:06,150
says the width of 60 which fits on my

00:49:03,600 --> 00:49:08,250
terminal it does a little histogram

00:49:06,150 --> 00:49:09,710
which is quite nice showing that X which

00:49:08,250 --> 00:49:12,660
is the 4-bit see numbers were down here

00:49:09,710 --> 00:49:15,180
and plus which is the early numbers are

00:49:12,660 --> 00:49:17,610
up here it shows you the average and

00:49:15,180 --> 00:49:18,780
also the median which in this case are

00:49:17,610 --> 00:49:21,660
on top of each other so we can't see

00:49:18,780 --> 00:49:24,300
them but a is average and underneath it

00:49:21,660 --> 00:49:27,360
and it shows you one standard deviation

00:49:24,300 --> 00:49:29,590
either side of that so it shows you what

00:49:27,360 --> 00:49:32,650
is the variance of the data

00:49:29,590 --> 00:49:34,560
but the most interesting thing for doing

00:49:32,650 --> 00:49:36,880
benchmarks is it actually tells you that

00:49:34,560 --> 00:49:38,890
you can be confident these numbers are

00:49:36,880 --> 00:49:40,840
actually representing a real change in

00:49:38,890 --> 00:49:44,170
the data rather than just a measurement

00:49:40,840 --> 00:49:45,820
fluctuation or some random event so the

00:49:44,170 --> 00:49:47,770
way you read this is that 94 you can be

00:49:45,820 --> 00:49:49,180
ninety-five percent confident that the

00:49:47,770 --> 00:49:53,110
second set of numbers which are the

00:49:49,180 --> 00:49:57,610
early numbers are sampled from a data

00:49:53,110 --> 00:49:58,960
set that has a amine 29 higher than the

00:49:57,610 --> 00:50:01,750
first one so this in this case it means

00:49:58,960 --> 00:50:03,220
that there's we getting sorry

00:50:01,750 --> 00:50:04,780
twenty-nine percent so we're getting a

00:50:03,220 --> 00:50:06,220
twenty-nine percent improvement on

00:50:04,780 --> 00:50:09,040
transactions per second on this

00:50:06,220 --> 00:50:10,540
benchmark so mini stat takes your two

00:50:09,040 --> 00:50:12,040
sets of data which might be noisy and

00:50:10,540 --> 00:50:16,230
hard to interpret and gives you an

00:50:12,040 --> 00:50:19,990
actual concrete interpretation to it

00:50:16,230 --> 00:50:22,900
okay so I mentioned throwing hardware

00:50:19,990 --> 00:50:24,430
the problem is is not always well I say

00:50:22,900 --> 00:50:29,500
it should be done after you've exhausted

00:50:24,430 --> 00:50:33,390
the actual debugging of the problem for

00:50:29,500 --> 00:50:35,650
example it's tempting sometimes to throw

00:50:33,390 --> 00:50:37,360
hard road the wrong problem for example

00:50:35,650 --> 00:50:38,830
if you're adding more CPU cores but

00:50:37,360 --> 00:50:41,070
you're actually your problem is a slow

00:50:38,830 --> 00:50:43,440
disk of course that's not going to help

00:50:41,070 --> 00:50:47,320
sometimes adding RAM can help if you'll

00:50:43,440 --> 00:50:49,240
have a either application workload that

00:50:47,320 --> 00:50:50,590
is too large to fit in memory or if

00:50:49,240 --> 00:50:53,010
you're doing a lot of reads from disk

00:50:50,590 --> 00:50:55,330
and you could benefit from extra caching

00:50:53,010 --> 00:50:57,670
but you're working set is too large to

00:50:55,330 --> 00:51:01,390
keep the cache data in memory you can

00:50:57,670 --> 00:51:02,950
fix that by adding more ram it's

00:51:01,390 --> 00:51:04,660
interesting to point out that sometimes

00:51:02,950 --> 00:51:06,730
adding more CPU cores can make the

00:51:04,660 --> 00:51:08,590
workload slower right if you have a

00:51:06,730 --> 00:51:10,300
workload that is very highly contended

00:51:08,590 --> 00:51:13,120
on on some shared resource such as a

00:51:10,300 --> 00:51:14,230
lock then adding more cpus that are all

00:51:13,120 --> 00:51:15,520
going to come in and content on that

00:51:14,230 --> 00:51:17,470
same lock it's going to slow the other

00:51:15,520 --> 00:51:19,210
ones down so you can actually make

00:51:17,470 --> 00:51:23,940
resource contention worse by adding more

00:51:19,210 --> 00:51:23,940
cpus which is sometimes counterintuitive

00:51:24,030 --> 00:51:32,380
okay so hopefully I've given some at

00:51:30,070 --> 00:51:34,690
least some approaches to take in

00:51:32,380 --> 00:51:37,870
investigating problems that you might

00:51:34,690 --> 00:51:41,800
encounter on your systems if you're

00:51:37,870 --> 00:51:42,810
still stuck then hopefully that the the

00:51:41,800 --> 00:51:44,460
kinds of

00:51:42,810 --> 00:51:47,430
next and yet the commands that I've

00:51:44,460 --> 00:51:51,000
given here will at least be helpful to a

00:51:47,430 --> 00:51:52,850
developer so so if you get to this point

00:51:51,000 --> 00:51:54,750
you still don't know what to do

00:51:52,850 --> 00:51:57,600
providing the output of these commands

00:51:54,750 --> 00:51:58,980
like example showing what top is showing

00:51:57,600 --> 00:52:00,780
it to people snapshot of what's going on

00:51:58,980 --> 00:52:02,850
showing if there's some sort of you know

00:52:00,780 --> 00:52:05,430
high cpu usage in the colonel you may be

00:52:02,850 --> 00:52:07,230
looking at vmstat may be turning on lock

00:52:05,430 --> 00:52:09,060
profiling all the more data you can

00:52:07,230 --> 00:52:11,010
provide to a developer the easier their

00:52:09,060 --> 00:52:12,900
job will be and we really like it when

00:52:11,010 --> 00:52:14,100
you guys come to us with all these

00:52:12,900 --> 00:52:15,480
output we don't have to do more round

00:52:14,100 --> 00:52:19,830
trips and say it please do this please

00:52:15,480 --> 00:52:21,360
do this so at the very least if you're

00:52:19,830 --> 00:52:22,560
not able to get to the end of the

00:52:21,360 --> 00:52:28,980
problem at least you've gone part of the

00:52:22,560 --> 00:52:31,110
way and in the context of freebsd if you

00:52:28,980 --> 00:52:33,720
need help with this kind of thing then

00:52:31,110 --> 00:52:35,790
the best list to ask on depending on

00:52:33,720 --> 00:52:38,160
sort of how technical your understanding

00:52:35,790 --> 00:52:41,130
of the issue is general support

00:52:38,160 --> 00:52:43,020
questions our best ass on the questions

00:52:41,130 --> 00:52:44,460
list more technical questions you have

00:52:43,020 --> 00:52:45,780
some insight into the code maybe you've

00:52:44,460 --> 00:52:46,680
already debugged a little bit you need

00:52:45,780 --> 00:52:48,390
some more help from somebody who

00:52:46,680 --> 00:52:51,240
understands the source code come to the

00:52:48,390 --> 00:52:53,010
hackers list and as i say hopefully at

00:52:51,240 --> 00:52:54,690
least given this information will

00:52:53,010 --> 00:52:59,130
actually be able to help you work things

00:52:54,690 --> 00:52:59,910
out okay so with that's a old finishing

00:52:59,130 --> 00:53:04,970
I'll be happy to take any questions

00:52:59,910 --> 00:53:04,970
anyone might have thanks

00:53:07,869 --> 00:53:10,529
yes

00:53:16,410 --> 00:53:19,100
sorry

00:53:19,759 --> 00:53:23,880
yes

00:53:22,200 --> 00:53:26,099
okay so the question is what is in the

00:53:23,880 --> 00:53:27,690
files that I passed a mini stat and this

00:53:26,099 --> 00:53:30,060
is just a rule list of numbers so in

00:53:27,690 --> 00:53:33,210
this case the first set of numbers was

00:53:30,060 --> 00:53:34,500
in the range of 2001 and 37 min to

00:53:33,210 --> 00:53:37,920
two-thousand 10 61 this was at the

00:53:34,500 --> 00:53:39,089
output of my benchmark my benchmark did

00:53:37,920 --> 00:53:40,829
a whole bunch of stuff and spat out a

00:53:39,089 --> 00:53:43,230
number at the end and this number

00:53:40,829 --> 00:53:45,990
represents in this case transactions per

00:53:43,230 --> 00:53:49,010
second on this database workload but the

00:53:45,990 --> 00:53:49,010
important thing is just the number

00:53:55,710 --> 00:54:01,440
okay so the questions is how do you get

00:53:58,740 --> 00:54:02,640
this this data from the benchmark so

00:54:01,440 --> 00:54:04,020
this this really depends on what the

00:54:02,640 --> 00:54:05,730
benchmark is so you need you need to

00:54:04,020 --> 00:54:07,859
have a way of turning your work load

00:54:05,730 --> 00:54:11,430
into a number and this number it could

00:54:07,859 --> 00:54:13,980
be you know maybe it's the bandwidth

00:54:11,430 --> 00:54:15,450
network interface is able to perform or

00:54:13,980 --> 00:54:17,580
maybe it's the number of queries per

00:54:15,450 --> 00:54:19,680
second it really depends on on your

00:54:17,580 --> 00:54:21,300
workload so whatever that is your turn

00:54:19,680 --> 00:54:22,680
it into any number and mini-set will

00:54:21,300 --> 00:54:25,020
tell you if these two sets of numbers

00:54:22,680 --> 00:54:27,240
are likely to be sampled from different

00:54:25,020 --> 00:54:29,160
data sets meeting that something changed

00:54:27,240 --> 00:54:30,450
from one to the other and will tell you

00:54:29,160 --> 00:54:32,130
when they don't change as well which is

00:54:30,450 --> 00:54:38,990
when you you didn't tune the right thing

00:54:32,130 --> 00:54:38,990
yeah any other questions

00:54:43,400 --> 00:54:50,410
okay I'm not see any hands so thank you

00:54:48,170 --> 00:54:50,410

YouTube URL: https://www.youtube.com/watch?v=uyJgTleinmM


