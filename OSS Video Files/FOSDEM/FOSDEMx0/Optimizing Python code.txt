Title: Optimizing Python code
Publication date: 2018-09-24
Playlist: FOSDEMx0
Description: 
	
Captions: 
	00:00:00,030 --> 00:00:06,180
hello everyone welcome to the talk about

00:00:02,760 --> 00:00:08,250
optimizing Python code before I get

00:00:06,180 --> 00:00:10,070
started who's familiar with Python

00:00:08,250 --> 00:00:16,109
already raise your hand

00:00:10,070 --> 00:00:20,699
fantastic so hi I'm Eric

00:00:16,109 --> 00:00:22,050
I work as a Python developer and I work

00:00:20,699 --> 00:00:24,390
for a company called a Demian you can

00:00:22,050 --> 00:00:27,990
find me at with the handle I think

00:00:24,390 --> 00:00:29,220
easily on github Twitter all right so

00:00:27,990 --> 00:00:32,610
our first question you might ask

00:00:29,220 --> 00:00:34,860
yourself since you made it to this room

00:00:32,610 --> 00:00:37,410
I assume you're curious about optimizing

00:00:34,860 --> 00:00:40,550
code so I'll walk you through a few

00:00:37,410 --> 00:00:43,860
methods that I use to optimize

00:00:40,550 --> 00:00:45,480
performance issues this is just my

00:00:43,860 --> 00:00:49,739
methods there are plenty of them out

00:00:45,480 --> 00:00:52,230
there so but yeah just pick one for it

00:00:49,739 --> 00:00:54,690
the first question you asked you do you

00:00:52,230 --> 00:00:57,989
need to ask yourself is obviously why do

00:00:54,690 --> 00:01:00,329
you want to optimize things and what you

00:00:57,989 --> 00:01:02,250
don't usually wake up in the morning

00:01:00,329 --> 00:01:04,019
wanting to optimize stuff unless you

00:01:02,250 --> 00:01:07,619
just want to talk about optimizing

00:01:04,019 --> 00:01:09,750
Python and that's not very good reason

00:01:07,619 --> 00:01:15,000
now optimization is usually a byproduct

00:01:09,750 --> 00:01:18,630
of solving a problem so you might have

00:01:15,000 --> 00:01:21,720
an issue with slow IO for instance and

00:01:18,630 --> 00:01:23,610
that's crippling your data analysis or

00:01:21,720 --> 00:01:26,490
your database queries or forcing you to

00:01:23,610 --> 00:01:29,430
collect less data or less often from

00:01:26,490 --> 00:01:31,470
your connected devices you might also

00:01:29,430 --> 00:01:34,170
want to run programs on the shared

00:01:31,470 --> 00:01:39,060
environment where you may have to pay

00:01:34,170 --> 00:01:41,579
for extra RAM and or if you like stuff

00:01:39,060 --> 00:01:42,780
too much processing on the same machine

00:01:41,579 --> 00:01:46,049
you might take down the machine

00:01:42,780 --> 00:01:48,200
altogether because of RAM usage you

00:01:46,049 --> 00:01:51,360
might also want to optimize your uptime

00:01:48,200 --> 00:01:54,000
I think I know you think it's a fetish

00:01:51,360 --> 00:01:56,880
for system illustrators with beard and

00:01:54,000 --> 00:02:01,590
sandals but uptime is very serious in

00:01:56,880 --> 00:02:04,710
some domains and some systems which use

00:02:01,590 --> 00:02:07,140
less resources tend to put less stress

00:02:04,710 --> 00:02:11,000
on the hardware they're running on and

00:02:07,140 --> 00:02:13,420
so therefore they're staying up longer a

00:02:11,000 --> 00:02:15,610
system which is designed to handle

00:02:13,420 --> 00:02:18,580
inputs and alls is also a system that

00:02:15,610 --> 00:02:21,099
will survive brutal network losses for

00:02:18,580 --> 00:02:23,260
instances or defective hardware and

00:02:21,099 --> 00:02:25,209
finally some systems should never fail

00:02:23,260 --> 00:02:27,819
because human lives depend on it

00:02:25,209 --> 00:02:32,530
so then sometimes you that's some stuff

00:02:27,819 --> 00:02:36,130
you want to improve and optimize handing

00:02:32,530 --> 00:02:37,600
a lot of time handling a lot of workload

00:02:36,130 --> 00:02:40,450
at the same time is a non-trivial

00:02:37,600 --> 00:02:41,860
exercise and especially in Python but

00:02:40,450 --> 00:02:43,870
that's pretty common scenario if you

00:02:41,860 --> 00:02:47,709
want to write let's say web server or

00:02:43,870 --> 00:02:51,160
web service an IOT controller or or a

00:02:47,709 --> 00:02:54,190
database front-end so finally you want

00:02:51,160 --> 00:02:56,530
to make things go faster and today we've

00:02:54,190 --> 00:02:58,690
grown into an impassioned Bunch

00:02:56,530 --> 00:03:01,200
time is money wasting time is wasting

00:02:58,690 --> 00:03:03,940
resources so faster is always better

00:03:01,200 --> 00:03:05,980
well maybe it's not everywhere but when

00:03:03,940 --> 00:03:08,799
it comes to computing nobody is going to

00:03:05,980 --> 00:03:10,450
complain if calculations go too fast or

00:03:08,799 --> 00:03:12,489
if an algorithm is too efficient

00:03:10,450 --> 00:03:14,440
obviously that's the kind of

00:03:12,489 --> 00:03:18,940
optimization we're going to have a look

00:03:14,440 --> 00:03:21,970
at today because you need to pick one

00:03:18,940 --> 00:03:25,299
each of those domains is a full-blown

00:03:21,970 --> 00:03:27,480
conference in itself also you don't want

00:03:25,299 --> 00:03:29,890
to get mixed up when you're optimizing

00:03:27,480 --> 00:03:32,859
we are going to do it scientifically

00:03:29,890 --> 00:03:35,560
with measurable outcomes you pick one

00:03:32,859 --> 00:03:37,780
category let's say conveniently CPU

00:03:35,560 --> 00:03:40,120
optimization and then you optimize that

00:03:37,780 --> 00:03:46,329
and then when you're satisfied you're

00:03:40,120 --> 00:03:48,160
free to optimize IO or reliability when

00:03:46,329 --> 00:03:50,620
I'm going to speak about optimizing for

00:03:48,160 --> 00:03:53,049
speed the method the method remains

00:03:50,620 --> 00:03:56,590
valid for all domains so it's always the

00:03:53,049 --> 00:04:01,630
same thing we do first you need to send

00:03:56,590 --> 00:04:04,359
some targets like this page must load

00:04:01,630 --> 00:04:06,040
below 200 milliseconds or one nitration

00:04:04,359 --> 00:04:08,530
of this loop must execute below 10

00:04:06,040 --> 00:04:10,780
milliseconds so the first one is usually

00:04:08,530 --> 00:04:13,600
stuff that marketing guys come and tell

00:04:10,780 --> 00:04:16,840
you ok we are getting bad SEO because

00:04:13,600 --> 00:04:18,430
the page takes too long to load or the

00:04:16,840 --> 00:04:20,979
second is more like an embedded system

00:04:18,430 --> 00:04:23,050
or time critical systems where you need

00:04:20,979 --> 00:04:24,970
to execute stuff every 10 milliseconds

00:04:23,050 --> 00:04:26,590
and not miss a beat

00:04:24,970 --> 00:04:28,120
or maybe you're running on the

00:04:26,590 --> 00:04:29,740
controller with which only has 8

00:04:28,120 --> 00:04:34,090
kilobytes of memory so you need to

00:04:29,740 --> 00:04:36,070
optimize for that for each target you

00:04:34,090 --> 00:04:38,230
define if it's not already obvious from

00:04:36,070 --> 00:04:39,760
the problem a set of metrics that you

00:04:38,230 --> 00:04:42,040
will compare for every change you make

00:04:39,760 --> 00:04:45,070
for CPU optimization is pretty easy it's

00:04:42,040 --> 00:04:48,430
this time spent for running a particular

00:04:45,070 --> 00:04:52,090
piece of code so at least if you are

00:04:48,430 --> 00:04:54,580
making things wrong and you're making

00:04:52,090 --> 00:04:57,580
things worse at least you know because

00:04:54,580 --> 00:05:00,130
when you're like you just using your

00:04:57,580 --> 00:05:01,990
judgment oh it feels faster oh it feels

00:05:00,130 --> 00:05:05,290
better or feels more efficient

00:05:01,990 --> 00:05:09,580
sometimes psychology is also running

00:05:05,290 --> 00:05:10,840
here and it allows you also to see if

00:05:09,580 --> 00:05:12,850
you reach your target because it's

00:05:10,840 --> 00:05:14,410
pretty easy to get sucked up and like oh

00:05:12,850 --> 00:05:16,360
I could make it a bit faster or a bit

00:05:14,410 --> 00:05:18,940
faster but if you already reach your

00:05:16,360 --> 00:05:22,570
goal then there's no need to get to dig

00:05:18,940 --> 00:05:23,650
further so the rules of an

00:05:22,570 --> 00:05:26,020
often-overlooked

00:05:23,650 --> 00:05:28,240
and it gets more true the more

00:05:26,020 --> 00:05:32,200
experience you get its benchmark

00:05:28,240 --> 00:05:35,050
benchmark benchmark its facts count more

00:05:32,200 --> 00:05:37,630
than experience you always have to

00:05:35,050 --> 00:05:40,480
measure compare review your progress

00:05:37,630 --> 00:05:44,320
because some obvious fixes sometimes

00:05:40,480 --> 00:05:47,680
aren't you can trust your gut but you

00:05:44,320 --> 00:05:49,450
need to verify it so how come you can't

00:05:47,680 --> 00:05:52,900
trust your guts because well you control

00:05:49,450 --> 00:05:55,600
your program but your program lays on

00:05:52,900 --> 00:05:58,660
top of the interpreter it's a written in

00:05:55,600 --> 00:06:00,730
another language which is C depending if

00:05:58,660 --> 00:06:04,060
you use C Python but let's take it for

00:06:00,730 --> 00:06:05,680
granted for now then this executes on

00:06:04,060 --> 00:06:07,480
the operating system which operating

00:06:05,680 --> 00:06:09,910
system there are not all made the same

00:06:07,480 --> 00:06:12,730
regarding schedulers memory memory

00:06:09,910 --> 00:06:15,790
allocator is and so on if you're doing

00:06:12,730 --> 00:06:18,520
IO what file system are you using to use

00:06:15,790 --> 00:06:21,730
red which red red one red zero red ten

00:06:18,520 --> 00:06:23,860
if you're doing GPU computations okay

00:06:21,730 --> 00:06:26,440
who provides drivers which version of

00:06:23,860 --> 00:06:29,200
the driver isn't there a slowdown in the

00:06:26,440 --> 00:06:31,900
driver itself and then this runs on a

00:06:29,200 --> 00:06:33,880
hardware where Jack architecture have

00:06:31,900 --> 00:06:36,790
significant influence as well as the

00:06:33,880 --> 00:06:38,559
presence of CPU extensions is it an old

00:06:36,790 --> 00:06:41,109
ID drive you get

00:06:38,559 --> 00:06:42,849
or is it a new SSD even the damn

00:06:41,109 --> 00:06:44,619
temperature in the material in which the

00:06:42,849 --> 00:06:46,959
network cables are made can interfere

00:06:44,619 --> 00:06:49,179
with your performance calculations so

00:06:46,959 --> 00:06:51,999
all of this to say that even if you have

00:06:49,179 --> 00:06:54,819
the strong in inner feeling that this

00:06:51,999 --> 00:06:58,719
kind of fix should make things faster

00:06:54,819 --> 00:07:01,029
sometimes you see either either the

00:06:58,719 --> 00:07:04,689
expected result sometimes the opposite

00:07:01,029 --> 00:07:06,879
and that's sometimes lays in the middle

00:07:04,689 --> 00:07:12,189
of all these layers of things that work

00:07:06,879 --> 00:07:12,999
to make your program run so a word of

00:07:12,189 --> 00:07:16,539
advice here

00:07:12,999 --> 00:07:18,249
you must never attempt changing a system

00:07:16,539 --> 00:07:20,979
if you don't have a solid version

00:07:18,249 --> 00:07:24,819
control system that can at least dip and

00:07:20,979 --> 00:07:27,519
commit and a solid test coverage because

00:07:24,819 --> 00:07:29,979
when you change things in the many

00:07:27,519 --> 00:07:32,499
different places to see if it's going to

00:07:29,979 --> 00:07:35,079
improve then you might wonder what

00:07:32,499 --> 00:07:38,349
exactly brought improvements so a good

00:07:35,079 --> 00:07:42,129
method is like okay I achieved a faster

00:07:38,349 --> 00:07:44,829
execution now I rewind and I apply my

00:07:42,129 --> 00:07:47,799
changes incrementally to just like okay

00:07:44,829 --> 00:07:51,279
I it's a deliberate action that brought

00:07:47,799 --> 00:07:53,799
improvements and just like not okay by

00:07:51,279 --> 00:07:56,409
some random chances I align all my bits

00:07:53,799 --> 00:07:57,929
in the memory and things went faster but

00:07:56,409 --> 00:08:01,479
it's completely out of my control

00:07:57,929 --> 00:08:03,249
so rewind your changes apply them and if

00:08:01,479 --> 00:08:06,639
you break something then you can just

00:08:03,249 --> 00:08:12,399
safely revert back and when you achieve

00:08:06,639 --> 00:08:15,609
your goals celebrates and sometimes you

00:08:12,399 --> 00:08:18,729
hit the end of the road sometimes

00:08:15,609 --> 00:08:21,849
despite your best efforts you can't do

00:08:18,729 --> 00:08:23,829
better but again there are many ways of

00:08:21,849 --> 00:08:25,839
meeting your targets maybe there are

00:08:23,829 --> 00:08:29,549
other ways of improvement just take a

00:08:25,839 --> 00:08:32,709
break look under a different angle

00:08:29,549 --> 00:08:34,589
there's always more than one tool in the

00:08:32,709 --> 00:08:39,819
box

00:08:34,589 --> 00:08:42,339
speaking of tools so my go-to tool for

00:08:39,819 --> 00:08:45,850
speed optimization in Python our first

00:08:42,339 --> 00:08:48,550
profiler I'll show you all of them don't

00:08:45,850 --> 00:08:52,180
worry so the profiler graphical analyzer

00:08:48,550 --> 00:08:54,820
the time-it function usually used in

00:08:52,180 --> 00:08:58,589
conjunction with an improved interpreter

00:08:54,820 --> 00:09:01,630
and the great part is profiling plugin

00:08:58,589 --> 00:09:10,300
so how many of you have already used by

00:09:01,630 --> 00:09:12,430
test not enough okay so what's a

00:09:10,300 --> 00:09:14,950
profiler it's it's a library that's

00:09:12,430 --> 00:09:18,010
going to capture every single call made

00:09:14,950 --> 00:09:20,260
while it's when the the profiler is

00:09:18,010 --> 00:09:22,390
active so let's say you got function a

00:09:20,260 --> 00:09:24,310
that cost function B somewhere the

00:09:22,390 --> 00:09:26,800
profiler will look at this interaction

00:09:24,310 --> 00:09:29,650
and log okay function a called function

00:09:26,800 --> 00:09:31,630
B an function B returned after let's say

00:09:29,650 --> 00:09:33,130
10 milliseconds and every time you're

00:09:31,630 --> 00:09:37,390
going to call it's going to add one line

00:09:33,130 --> 00:09:40,390
in the log so for this reason I suggest

00:09:37,390 --> 00:09:44,350
that you focus yeah it's going to

00:09:40,390 --> 00:09:47,500
capture a lot of noise as well because

00:09:44,350 --> 00:09:50,950
you might want to execute one piece of

00:09:47,500 --> 00:09:53,740
code but before and after a lot of code

00:09:50,950 --> 00:09:56,740
is also being run and which you have no

00:09:53,740 --> 00:09:59,260
issue is performance so I suggest that

00:09:56,740 --> 00:10:02,260
you focus on one specific top function

00:09:59,260 --> 00:10:07,390
and then from there you just figure out

00:10:02,260 --> 00:10:10,180
which function is having the worse the

00:10:07,390 --> 00:10:12,520
worst timing and then proceed narrowing

00:10:10,180 --> 00:10:16,000
the scope up until you find the function

00:10:12,520 --> 00:10:18,310
that really needs optimization the

00:10:16,000 --> 00:10:22,450
profiler capture can be dumped as Python

00:10:18,310 --> 00:10:24,040
stat format Colo so P stats just for any

00:10:22,450 --> 00:10:25,839
information you can tell you can call

00:10:24,040 --> 00:10:27,850
the profiler on the whole program I

00:10:25,839 --> 00:10:30,430
would not recommend that because there's

00:10:27,850 --> 00:10:33,220
too much noise getting into into play

00:10:30,430 --> 00:10:35,200
except you who you want to debug stuff

00:10:33,220 --> 00:10:38,410
like importing your models and so on and

00:10:35,200 --> 00:10:39,700
so on the only advantage that I see is

00:10:38,410 --> 00:10:46,920
that you don't need to change your code

00:10:39,700 --> 00:10:49,630
for doing so beware that a profiler look

00:10:46,920 --> 00:10:52,720
we look and log everything that occurs

00:10:49,630 --> 00:10:54,490
in your program so that's also the

00:10:52,720 --> 00:10:56,860
real-life equivalent of driving with the

00:10:54,490 --> 00:11:00,310
parking brake on so as soon as you're

00:10:56,860 --> 00:11:02,740
done profiling remove the profiler to

00:11:00,310 --> 00:11:04,810
see the real results because if you

00:11:02,740 --> 00:11:05,710
improve something like let's say you

00:11:04,810 --> 00:11:10,240
shave up

00:11:05,710 --> 00:11:12,760
half a second of program execution with

00:11:10,240 --> 00:11:14,529
the profiler still in place when you

00:11:12,760 --> 00:11:16,240
remove the profiler you will see the

00:11:14,529 --> 00:11:18,220
real accomplishment and gain it's

00:11:16,240 --> 00:11:24,160
probably going to be much better without

00:11:18,220 --> 00:11:26,620
the profiler so how do you so that's

00:11:24,160 --> 00:11:29,770
where the ability to read the code is

00:11:26,620 --> 00:11:31,750
going to be a bit more critical here how

00:11:29,770 --> 00:11:34,860
do you embed the profiler within your

00:11:31,750 --> 00:11:38,860
code just import the C profile library

00:11:34,860 --> 00:11:40,510
create a profiler objects enabled run

00:11:38,860 --> 00:11:45,000
your target function disable the

00:11:40,510 --> 00:11:48,250
profiler and dump the stats easy-peasy

00:11:45,000 --> 00:11:51,279
so that's what a status file looks like

00:11:48,250 --> 00:11:55,930
so if only we had a bigger screen you

00:11:51,279 --> 00:12:00,250
would see I can I find my mouse and I

00:11:55,930 --> 00:12:05,230
can have so yeah for those who can't

00:12:00,250 --> 00:12:07,779
read this is the timings so total time

00:12:05,230 --> 00:12:11,680
for a cool time cumulative time and

00:12:07,779 --> 00:12:15,610
there the function which got run and so

00:12:11,680 --> 00:12:18,790
again for those who can't read here I

00:12:15,610 --> 00:12:22,630
run the the statistics over the full

00:12:18,790 --> 00:12:25,600
program capture so I got a lot of import

00:12:22,630 --> 00:12:28,390
Lib bootstrap function calls they are

00:12:25,600 --> 00:12:30,850
not relevant for this exercise because

00:12:28,390 --> 00:12:33,520
it's just the Python interpreter booting

00:12:30,850 --> 00:12:36,100
itself and loading my modules so since

00:12:33,520 --> 00:12:38,500
my function is pretty fast to execute

00:12:36,100 --> 00:12:43,410
the only thing I can see from this

00:12:38,500 --> 00:12:43,410
capture is noise so it's not very useful

00:12:43,589 --> 00:12:48,610
on the other hand if I target just one

00:12:46,600 --> 00:12:53,020
function then I'll only see what's

00:12:48,610 --> 00:12:55,600
interesting to me so again so here you

00:12:53,020 --> 00:13:02,140
see it's only my own code that's being

00:12:55,600 --> 00:13:06,970
run here so let's face it it's not very

00:13:02,140 --> 00:13:08,740
easy to read to the human hi luckily we

00:13:06,970 --> 00:13:11,860
are blessed with a few more tool in the

00:13:08,740 --> 00:13:13,600
box there's a tool called G probe two

00:13:11,860 --> 00:13:16,270
dots and that will turn a status file

00:13:13,600 --> 00:13:19,370
into dot script so once dot anyone

00:13:16,270 --> 00:13:25,410
familiar with graph is and dots

00:13:19,370 --> 00:13:28,319
very good so so dot is a program that

00:13:25,410 --> 00:13:32,759
turns a description file into an actual

00:13:28,319 --> 00:13:35,310
graph that we can turn into an image and

00:13:32,759 --> 00:13:37,620
look at it and interpret it so that's

00:13:35,310 --> 00:13:39,959
the output of zero two dots

00:13:37,620 --> 00:13:42,120
based on on a stats file that got

00:13:39,959 --> 00:13:43,920
collected on my whole program and you

00:13:42,120 --> 00:13:48,029
see that there is a lot of things going

00:13:43,920 --> 00:13:50,430
on so and the more red the slowest the

00:13:48,029 --> 00:13:53,670
more blue the fastest or the less time

00:13:50,430 --> 00:13:55,920
spent so don't worry it's pretty tiny

00:13:53,670 --> 00:13:57,810
I'll show more examples later what's

00:13:55,920 --> 00:14:00,620
important to see here that each function

00:13:57,810 --> 00:14:05,670
compose one or more other functions and

00:14:00,620 --> 00:14:08,279
the color is kind of giving you clues

00:14:05,670 --> 00:14:11,189
about what kind of optimization path you

00:14:08,279 --> 00:14:13,410
want to improve there's no need to go in

00:14:11,189 --> 00:14:20,370
the blue space here you can just focus

00:14:13,410 --> 00:14:23,279
following the red here this is for this

00:14:20,370 --> 00:14:24,809
from the same program but only targeting

00:14:23,279 --> 00:14:28,920
the one function that I wanted to

00:14:24,809 --> 00:14:32,220
capture so you see that it's much

00:14:28,920 --> 00:14:34,769
narrower and there's only the function

00:14:32,220 --> 00:14:36,389
that so this is the the first function I

00:14:34,769 --> 00:14:38,790
wanted to call and it's just making

00:14:36,389 --> 00:14:42,990
calls to other sub functions but it's

00:14:38,790 --> 00:14:45,899
much clearer what is taking time and it

00:14:42,990 --> 00:14:48,360
just gives me compared to the previous

00:14:45,899 --> 00:14:51,870
slide here I had to look into let's say

00:14:48,360 --> 00:14:54,149
10 ish functions to figure out what was

00:14:51,870 --> 00:14:59,610
taking time in here it's just pretty

00:14:54,149 --> 00:15:01,800
obviously just two of them so we'll go

00:14:59,610 --> 00:15:03,779
back at looking at pictures later now

00:15:01,800 --> 00:15:07,860
I'll just show you a bit more of ipython

00:15:03,779 --> 00:15:11,040
and percentage time it magic coal that

00:15:07,860 --> 00:15:13,139
will execute a function for you and call

00:15:11,040 --> 00:15:16,529
it several times just to figure out how

00:15:13,139 --> 00:15:18,449
fast it is so you may have already tried

00:15:16,529 --> 00:15:20,730
to do some performance analyzers before

00:15:18,449 --> 00:15:25,170
like just timing how long something

00:15:20,730 --> 00:15:27,870
takes to run if it runs pretty fast like

00:15:25,170 --> 00:15:30,179
the the load of your computer may

00:15:27,870 --> 00:15:32,670
interfere with the actual runtime of the

00:15:30,179 --> 00:15:34,830
function so if you just

00:15:32,670 --> 00:15:39,680
happen to have your all your CPUs freeze

00:15:34,830 --> 00:15:39,680
and and nothing running on your device

00:15:39,800 --> 00:15:46,470
then it's the execution is going to be

00:15:42,300 --> 00:15:49,350
pretty fast and like if you're also

00:15:46,470 --> 00:15:51,060
having a chrome tab open somewhere and

00:15:49,350 --> 00:15:53,550
like with streaming video or something

00:15:51,060 --> 00:15:54,420
then your function is magically going to

00:15:53,550 --> 00:15:56,190
be slower

00:15:54,420 --> 00:15:58,980
why so it's because of the interference

00:15:56,190 --> 00:16:02,640
of your system into the measurement so

00:15:58,980 --> 00:16:06,060
time it runs like 1,000 time by default

00:16:02,640 --> 00:16:09,420
the code that you're passing it so it's

00:16:06,060 --> 00:16:12,090
kind of averaging the load of the system

00:16:09,420 --> 00:16:13,530
so it's not counting as much waiting as

00:16:12,090 --> 00:16:17,520
much in the back end in the final

00:16:13,530 --> 00:16:19,080
calculation it's very easy when you're

00:16:17,520 --> 00:16:21,660
on the terminal and you want to try

00:16:19,080 --> 00:16:23,010
several different parameters so you're

00:16:21,660 --> 00:16:24,990
let's say you're calling a function you

00:16:23,010 --> 00:16:27,510
want to see like okay if I call it with

00:16:24,990 --> 00:16:31,260
this parameter is it going to be better

00:16:27,510 --> 00:16:36,030
or worse so this it's pretty nice when

00:16:31,260 --> 00:16:39,900
you're using ipython so here we can see

00:16:36,030 --> 00:16:44,250
for those who can see that if I call my

00:16:39,900 --> 00:16:49,230
function with let's say this md5 hash

00:16:44,250 --> 00:16:53,190
method it's taking 207 microseconds and

00:16:49,230 --> 00:16:56,670
if I call it with sha-512 512 it's going

00:16:53,190 --> 00:17:05,700
to take 231 microseconds so this one is

00:16:56,670 --> 00:17:08,940
a bit slower but kind of weirdly each

00:17:05,700 --> 00:17:10,610
loop takes longer with md5 and less time

00:17:08,940 --> 00:17:14,430
with

00:17:10,610 --> 00:17:16,530
sha-512 so it might be that the total

00:17:14,430 --> 00:17:19,790
time was influenced by the load of a

00:17:16,530 --> 00:17:22,710
system but each loop was more or less

00:17:19,790 --> 00:17:25,340
2.19 microsecond with the 2nd locution

00:17:22,710 --> 00:17:28,560
so it you can tell pretty accurately

00:17:25,340 --> 00:17:30,720
what is the exact timing the cumulative

00:17:28,560 --> 00:17:34,230
timing and the individual timing of each

00:17:30,720 --> 00:17:36,600
loop and finally if you already have a

00:17:34,230 --> 00:17:38,940
good test coverage and you should if you

00:17:36,600 --> 00:17:41,100
remember my previous slide you can

00:17:38,940 --> 00:17:44,370
enable profiling on each of your unit

00:17:41,100 --> 00:17:46,630
tests and take and the plug-in here

00:17:44,370 --> 00:17:48,960
which is called

00:17:46,630 --> 00:17:53,230
pite is profiling which is standard

00:17:48,960 --> 00:17:54,660
plugin for pi tests this plugin will do

00:17:53,230 --> 00:17:58,090
the heavy lifting for you like

00:17:54,660 --> 00:17:59,980
activating the profiler them the stats

00:17:58,090 --> 00:18:04,240
called G probe 2 dot and then it will

00:17:59,980 --> 00:18:06,970
just output a few SVG or PNG s for you

00:18:04,240 --> 00:18:08,320
to watch so that's very convenient if

00:18:06,970 --> 00:18:09,970
you're a beginner so you don't have to

00:18:08,320 --> 00:18:15,220
go through all the steps manually every

00:18:09,970 --> 00:18:20,440
time so now we've got our tools let's

00:18:15,220 --> 00:18:22,870
see how to use them you will often hear

00:18:20,440 --> 00:18:26,350
people mention obvious performance hogs

00:18:22,870 --> 00:18:28,360
or low-hanging fruits well that's

00:18:26,350 --> 00:18:29,919
usually the first thing you need to look

00:18:28,360 --> 00:18:32,890
at because they are under your control

00:18:29,919 --> 00:18:35,500
and like CPU architecture or temperature

00:18:32,890 --> 00:18:37,570
if you remember what I said before so

00:18:35,500 --> 00:18:40,120
also a good refactoring will also

00:18:37,570 --> 00:18:43,539
usually bring a lot of good things with

00:18:40,120 --> 00:18:45,520
it and little drawbacks so I would

00:18:43,539 --> 00:18:47,289
recommend going for I would not

00:18:45,520 --> 00:18:49,330
recommend going for more advanced

00:18:47,289 --> 00:18:52,510
techniques before having exhausted of

00:18:49,330 --> 00:18:55,059
all of these low-hanging fruits even if

00:18:52,510 --> 00:18:59,169
it does not sound sexy it's actually the

00:18:55,059 --> 00:19:01,210
most efficient so for the presentation I

00:18:59,169 --> 00:19:02,980
will focus on a booming field which is

00:19:01,210 --> 00:19:05,740
password cracking using a brute force

00:19:02,980 --> 00:19:07,929
method there are very efficient ways to

00:19:05,740 --> 00:19:10,480
do this but I'm not going to show them

00:19:07,929 --> 00:19:13,210
because obviously there is nothing left

00:19:10,480 --> 00:19:16,450
to optimizing them so I decided to make

00:19:13,210 --> 00:19:21,429
my own password cracker like whole all

00:19:16,450 --> 00:19:22,990
the good kids do just a bit of recovery

00:19:21,429 --> 00:19:25,750
if you're not familiar with password

00:19:22,990 --> 00:19:28,150
cracking so what does it mean you're not

00:19:25,750 --> 00:19:30,760
cracking password you're just comparing

00:19:28,150 --> 00:19:32,320
the hash of a password with the hash of

00:19:30,760 --> 00:19:34,059
something that you don't know

00:19:32,320 --> 00:19:37,090
so you're just generating a lot of

00:19:34,059 --> 00:19:39,010
passwords possible passwords and your

00:19:37,090 --> 00:19:41,049
compassion them through a hashing

00:19:39,010 --> 00:19:45,850
function who knows what a hash

00:19:41,049 --> 00:19:49,299
information is great and so you're just

00:19:45,850 --> 00:19:51,640
comparing the hashes and with some luck

00:19:49,299 --> 00:19:54,549
you end up with a match which means that

00:19:51,640 --> 00:19:58,030
since hash functions are deterministic

00:19:54,549 --> 00:19:59,860
what you put in the input was the actual

00:19:58,030 --> 00:20:02,140
password that the person entered the

00:19:59,860 --> 00:20:04,179
first place so brute-forcing is

00:20:02,140 --> 00:20:06,490
obtaining all the possible inputs in

00:20:04,179 --> 00:20:09,490
hope to find one of the the one used

00:20:06,490 --> 00:20:12,240
initially and the salt is just a piece

00:20:09,490 --> 00:20:15,700
of data that you either prefix or suffix

00:20:12,240 --> 00:20:17,910
your password it's an added factor to

00:20:15,700 --> 00:20:22,960
increase the size of the input

00:20:17,910 --> 00:20:26,470
artificially so so this is absolutely

00:20:22,960 --> 00:20:30,880
unreadable so this is a thirty five

00:20:26,470 --> 00:20:34,590
lines of Python cracker and yeah that

00:20:30,880 --> 00:20:38,070
that's very very bad so here you might

00:20:34,590 --> 00:20:41,860
see that I'm using the very clever

00:20:38,070 --> 00:20:44,799
computer as a password so I'm just using

00:20:41,860 --> 00:20:47,919
the string computer and then I'm salting

00:20:44,799 --> 00:20:52,179
it with four digits which is like just

00:20:47,919 --> 00:20:55,059
don't do this and so I want to see how

00:20:52,179 --> 00:20:57,669
fast I can figure out this password

00:20:55,059 --> 00:21:01,480
using the five the list of the five

00:20:57,669 --> 00:21:05,049
hundred word password of history one at

00:21:01,480 --> 00:21:07,660
a time and yeah to spice things up I

00:21:05,049 --> 00:21:09,910
just sold my password with four digits

00:21:07,660 --> 00:21:13,690
as I said that I'll also use the very

00:21:09,910 --> 00:21:16,030
quick very insecure md5 function for

00:21:13,690 --> 00:21:19,750
hashing so just crossing my fingers that

00:21:16,030 --> 00:21:22,390
I end up somewhere so I just made a

00:21:19,750 --> 00:21:24,070
bunch of utility functions so there's

00:21:22,390 --> 00:21:28,679
this digest function that just

00:21:24,070 --> 00:21:31,210
transforms clear text into a hash

00:21:28,679 --> 00:21:35,320
numerical numeric soles that's

00:21:31,210 --> 00:21:37,540
generating souls so basically from zero

00:21:35,320 --> 00:21:41,740
zero zero zero to nine nine nine nine

00:21:37,540 --> 00:21:44,380
one function that combines both and

00:21:41,740 --> 00:21:49,540
generates the hash for all combinations

00:21:44,380 --> 00:21:52,660
for one password and all souls and then

00:21:49,540 --> 00:21:55,450
one function that just iterates over all

00:21:52,660 --> 00:21:57,070
possible passwords one by one and then

00:21:55,450 --> 00:22:00,750
calls a function that's appending with

00:21:57,070 --> 00:22:03,730
the souls and generating the hashes and

00:22:00,750 --> 00:22:06,910
comparing the hash that I computed with

00:22:03,730 --> 00:22:10,120
the target hash and if I get a match it

00:22:06,910 --> 00:22:12,520
returns me which password it was in

00:22:10,120 --> 00:22:18,820
which salt it was

00:22:12,520 --> 00:22:21,919
so here if I run the the profiler on my

00:22:18,820 --> 00:22:25,929
tool you see that the pass is pretty

00:22:21,919 --> 00:22:30,830
obvious and here you might see that

00:22:25,929 --> 00:22:31,940
there's one goal here to numericals

00:22:30,830 --> 00:22:35,390
numeric soles

00:22:31,940 --> 00:22:42,590
that's called 110 times we generate in

00:22:35,390 --> 00:22:45,520
turns 110 millions of calls because I'm

00:22:42,590 --> 00:22:52,880
generating one thousand souls times 1

00:22:45,520 --> 00:22:58,179
110 so that's and so that's a lot of

00:22:52,880 --> 00:23:02,960
wasted CPU that's a lot of wasted CPU

00:22:58,179 --> 00:23:07,659
because you'll see a bit further but

00:23:02,960 --> 00:23:10,460
these millions of calls could be avoided

00:23:07,659 --> 00:23:12,530
so what's what's invariant

00:23:10,460 --> 00:23:15,620
basically if you if you have a function

00:23:12,530 --> 00:23:20,840
a that calls B but B does not use any of

00:23:15,620 --> 00:23:25,210
the inputs of a scope then why are you

00:23:20,840 --> 00:23:28,280
even calling a from within B B could be

00:23:25,210 --> 00:23:31,549
outsides it located outside of a scope

00:23:28,280 --> 00:23:36,530
we could be cool and B is then in very

00:23:31,549 --> 00:23:38,809
an invariant so if you can squint your

00:23:36,530 --> 00:23:40,640
eyes and look at the codes can you

00:23:38,809 --> 00:23:46,700
figure out if there's an environment

00:23:40,640 --> 00:23:47,900
here so it's too hard to read so I'll

00:23:46,700 --> 00:23:51,140
just spoil it

00:23:47,900 --> 00:23:53,360
in this function in particular no the

00:23:51,140 --> 00:23:55,640
the obvious function call was generate

00:23:53,360 --> 00:23:58,190
hashes but it's used a clear text

00:23:55,640 --> 00:24:02,360
parameter here which comes from the for

00:23:58,190 --> 00:24:05,480
loop here so you could not take generate

00:24:02,360 --> 00:24:07,700
hashes one step above like outside of

00:24:05,480 --> 00:24:12,880
this this loop because we need this

00:24:07,700 --> 00:24:20,240
premiere here is there an obvious

00:24:12,880 --> 00:24:21,470
invariant in the squinting so here you

00:24:20,240 --> 00:24:24,280
see that there's a function called

00:24:21,470 --> 00:24:26,300
numeric soles called with soul space and

00:24:24,280 --> 00:24:28,850
only sold space but

00:24:26,300 --> 00:24:31,850
it's all space comes from the Kolar

00:24:28,850 --> 00:24:35,179
function so there is no good reason why

00:24:31,850 --> 00:24:38,690
numerical Souls would be called for each

00:24:35,179 --> 00:24:42,020
time generate hashes is being cold so we

00:24:38,690 --> 00:24:46,460
can take that function and move it up in

00:24:42,020 --> 00:24:50,750
the calling tree it's only cold by the

00:24:46,460 --> 00:24:54,320
color so if we extract numeric Souls we

00:24:50,750 --> 00:24:56,059
put it before the loop and so we get

00:24:54,320 --> 00:24:57,770
just the results and we just pass the

00:24:56,059 --> 00:25:00,290
results of the execution of this

00:24:57,770 --> 00:25:03,890
function it's going to be just executed

00:25:00,290 --> 00:25:08,059
once so if we run the profiler again

00:25:03,890 --> 00:25:09,890
obviously numeric Souls is gone from the

00:25:08,059 --> 00:25:13,370
graph because it's just being called

00:25:09,890 --> 00:25:17,929
once and it's not mandating that the the

00:25:13,370 --> 00:25:22,040
profiler will get it one quick tip also

00:25:17,929 --> 00:25:25,730
on UNIX you got the nice time function

00:25:22,040 --> 00:25:28,190
well utility and time here is telling me

00:25:25,730 --> 00:25:30,950
that okay I find my password okay so I

00:25:28,190 --> 00:25:33,530
changed my my example for increasing the

00:25:30,950 --> 00:25:35,809
the execution time but now the password

00:25:33,530 --> 00:25:40,340
dreams is also on the list and the salt

00:25:35,809 --> 00:25:45,740
was 5432 and it took my clever programs

00:25:40,340 --> 00:25:50,840
seven seconds using 99% of my cpu to

00:25:45,740 --> 00:25:53,059
come up with this conclusion now we're

00:25:50,840 --> 00:25:57,320
going to look at how to make it better

00:25:53,059 --> 00:25:59,690
when we can do parallel computing so an

00:25:57,320 --> 00:26:02,090
embarrassing leap our problem is one

00:25:59,690 --> 00:26:03,740
where little or no effort is needed to

00:26:02,090 --> 00:26:08,420
separate the problem into a number of

00:26:03,740 --> 00:26:09,650
parallel tasks Thank You Wikipedia so

00:26:08,420 --> 00:26:11,630
the difference between parallel and

00:26:09,650 --> 00:26:14,140
sequential probably you know it already

00:26:11,630 --> 00:26:18,080
but perilous yeah okay you can do it

00:26:14,140 --> 00:26:20,300
multiple time in parallel like one task

00:26:18,080 --> 00:26:23,510
does not depend on the output from one

00:26:20,300 --> 00:26:25,820
other sequential is okay I call I need

00:26:23,510 --> 00:26:27,950
to wait for the input world from the

00:26:25,820 --> 00:26:31,429
output of one function so I can use it

00:26:27,950 --> 00:26:33,050
in my other function luckily password

00:26:31,429 --> 00:26:36,710
cracking is impressing Lee parallel

00:26:33,050 --> 00:26:39,520
because I'm just like exploring a space

00:26:36,710 --> 00:26:41,590
of possible solutions I don't really

00:26:39,520 --> 00:26:46,960
care of like okay this function

00:26:41,590 --> 00:26:49,360
succeeded or not so this is a forty six

00:26:46,960 --> 00:26:55,000
lines of code parallel parts for cracker

00:26:49,360 --> 00:26:58,480
still unreadable how do you do this well

00:26:55,000 --> 00:27:01,780
I had to make a few changes on how

00:26:58,480 --> 00:27:05,340
things were done but it turns out you

00:27:01,780 --> 00:27:08,500
can you have you can make a pool of

00:27:05,340 --> 00:27:11,890
different processes so as you know

00:27:08,500 --> 00:27:15,460
Python is running for one interpreter is

00:27:11,890 --> 00:27:17,980
running in one in one only process if

00:27:15,460 --> 00:27:22,450
you want to and but your computer

00:27:17,980 --> 00:27:28,390
I hope has more than one CPU nowadays so

00:27:22,450 --> 00:27:31,600
if you want to use these processors you

00:27:28,390 --> 00:27:34,240
need to use the multi processing library

00:27:31,600 --> 00:27:37,600
so here it's a very crude example there

00:27:34,240 --> 00:27:41,680
are more elegant ways of doing so so

00:27:37,600 --> 00:27:44,410
this is just one way of doing so so you

00:27:41,680 --> 00:27:46,840
just create a pool of processes and then

00:27:44,410 --> 00:27:49,360
you just apply a synchronously meaning

00:27:46,840 --> 00:27:51,220
okay I just hand out work to these

00:27:49,360 --> 00:27:53,110
processes and I'm not waiting for the

00:27:51,220 --> 00:27:57,460
answer I'm not waiting for them to

00:27:53,110 --> 00:28:02,680
complete before giving a job to another

00:27:57,460 --> 00:28:05,590
worker and then I just so I pass them

00:28:02,680 --> 00:28:08,410
the clear text the souls so the list of

00:28:05,590 --> 00:28:11,800
all like one password all the souls and

00:28:08,410 --> 00:28:13,210
the target hash and I pass them a

00:28:11,800 --> 00:28:16,620
function that's going to print the

00:28:13,210 --> 00:28:20,770
results when define it that's not very

00:28:16,620 --> 00:28:22,570
smart I'm going to get a result fast but

00:28:20,770 --> 00:28:25,180
I didn't plan uninterrupted my

00:28:22,570 --> 00:28:26,950
interrupting my program when I find the

00:28:25,180 --> 00:28:29,770
solution meaning it's still going to

00:28:26,950 --> 00:28:31,930
exhaust all possible solutions is going

00:28:29,770 --> 00:28:34,000
to run through all passwords and all

00:28:31,930 --> 00:28:37,710
souls because it's a very crude example

00:28:34,000 --> 00:28:40,630
so I didn't think it very through truly

00:28:37,710 --> 00:28:43,060
but it's going to display very quickly

00:28:40,630 --> 00:28:44,830
the the problem as soon as it finds it

00:28:43,060 --> 00:28:48,390
it's going to display it I just need to

00:28:44,830 --> 00:28:52,049
terminate the program annually

00:28:48,390 --> 00:28:54,630
so how does it look well exactly the

00:28:52,049 --> 00:28:56,490
same as it was before so I get a clear

00:28:54,630 --> 00:29:00,299
text password I get a list of souls for

00:28:56,490 --> 00:29:03,360
each Souls I check if the target if the

00:29:00,299 --> 00:29:08,600
hash is matching the target if it does

00:29:03,360 --> 00:29:14,750
then I return so that's the output and

00:29:08,600 --> 00:29:19,380
again I use time and time reports 353

00:29:14,750 --> 00:29:25,770
CPU percent of CPU time meaning that ok

00:29:19,380 --> 00:29:29,280
it elapsed 15 seconds of CPU time but on

00:29:25,770 --> 00:29:31,080
my clock it only took four seconds to

00:29:29,280 --> 00:29:34,950
crack it so it's twice about twice

00:29:31,080 --> 00:29:41,730
faster as the iterative version but it

00:29:34,950 --> 00:29:44,610
used a lot more CPU time so if you

00:29:41,730 --> 00:29:48,720
squint a bit you might be able to see

00:29:44,610 --> 00:29:51,360
like this big bump in CPU usage so yeah

00:29:48,720 --> 00:29:54,480
that's what three hundred and fifty

00:29:51,360 --> 00:29:56,580
three percent CPU usage looks like on

00:29:54,480 --> 00:30:02,610
the four processor on the four core

00:29:56,580 --> 00:30:05,250
machine so one other tool in your

00:30:02,610 --> 00:30:07,890
toolbox throwing more hardware at it

00:30:05,250 --> 00:30:10,200
it's effective but often overlooked

00:30:07,890 --> 00:30:12,270
because well it's like a bit of cheating

00:30:10,200 --> 00:30:16,080
you're just postponing the problem most

00:30:12,270 --> 00:30:20,280
of the time so what better specs means

00:30:16,080 --> 00:30:23,130
it's me it means like changing maybe the

00:30:20,280 --> 00:30:26,910
CPU architecture some processor that I

00:30:23,130 --> 00:30:31,130
are meant for desktop computers are not

00:30:26,910 --> 00:30:34,410
very suited for power programming while

00:30:31,130 --> 00:30:41,520
some other architectures are more used

00:30:34,410 --> 00:30:45,290
more in servers are more efficient clock

00:30:41,520 --> 00:30:49,679
speed is obviously very important and

00:30:45,290 --> 00:30:51,750
the size of the l2 cache for very CPU

00:30:49,679 --> 00:30:54,299
intensive applications you want your

00:30:51,750 --> 00:30:57,210
cache to be full all the time and the

00:30:54,299 --> 00:31:00,690
bigger the cache the best for non power

00:30:57,210 --> 00:31:01,530
problems the only answer is faster CPU

00:31:00,690 --> 00:31:06,210
clock so

00:31:01,530 --> 00:31:08,730
you get three 3.5 gigahertz versus 3.2

00:31:06,210 --> 00:31:12,480
bujji guards 3.5 is always going to win

00:31:08,730 --> 00:31:15,420
for nonparallel programs but for

00:31:12,480 --> 00:31:19,410
parallel programs if you add more CPUs

00:31:15,420 --> 00:31:22,230
meaning okay I run on my for CPU machine

00:31:19,410 --> 00:31:25,530
here if I go on the 32 CPU machine it's

00:31:22,230 --> 00:31:28,530
going to be faster obviously and then

00:31:25,530 --> 00:31:31,620
what a minute while just use one machine

00:31:28,530 --> 00:31:35,190
with 32 CPUs why not going like renting

00:31:31,620 --> 00:31:37,440
the farm of machines like 50 computers

00:31:35,190 --> 00:31:40,680
50 nodes of a distributed computing

00:31:37,440 --> 00:31:42,750
system all running 32 cores of course is

00:31:40,680 --> 00:31:44,910
going to every time divided by the

00:31:42,750 --> 00:31:47,040
number of machines was the total number

00:31:44,910 --> 00:31:48,720
of course so if you get 100 cores

00:31:47,040 --> 00:31:51,540
it's going to be a hundred times faster

00:31:48,720 --> 00:31:56,340
give or take then just running on one

00:31:51,540 --> 00:32:01,080
single core yeah just one thing if you

00:31:56,340 --> 00:32:03,870
start using different computers to run

00:32:01,080 --> 00:32:06,290
your calculations you might not want to

00:32:03,870 --> 00:32:10,560
roll your own system for your own

00:32:06,290 --> 00:32:13,740
distributed system software so a quick

00:32:10,560 --> 00:32:16,440
way of achieving this is using something

00:32:13,740 --> 00:32:18,150
called salary maybe some of you are

00:32:16,440 --> 00:32:21,630
already familiar with it but it's

00:32:18,150 --> 00:32:24,540
handling all the networking queueing

00:32:21,630 --> 00:32:29,790
failover and so on for you so it's it's

00:32:24,540 --> 00:32:35,430
much convenient and then you've got high

00:32:29,790 --> 00:32:39,840
performance libraries because you don't

00:32:35,430 --> 00:32:41,750
want to reinvent the wheel so these high

00:32:39,840 --> 00:32:45,540
performance libraries we have in Python

00:32:41,750 --> 00:32:49,410
they rely on the concept that they call

00:32:45,540 --> 00:32:51,990
vectorizing or vectors basically in the

00:32:49,410 --> 00:32:55,350
iterative world if you want to make a

00:32:51,990 --> 00:32:56,880
sum what you need to do is okay for each

00:32:55,350 --> 00:32:59,550
line my lines

00:32:56,880 --> 00:33:01,770
the total is the addition of the

00:32:59,550 --> 00:33:03,090
previous total plus the amount of the

00:33:01,770 --> 00:33:08,100
current line and then the return of

00:33:03,090 --> 00:33:09,770
total the good thing is each line can be

00:33:08,100 --> 00:33:13,710
different so you can make a different

00:33:09,770 --> 00:33:15,559
you can have sometimes integers strings

00:33:13,710 --> 00:33:17,749
and make each line

00:33:15,559 --> 00:33:20,600
and we're computing insulin differently

00:33:17,749 --> 00:33:23,090
but imagine like if you're familiar with

00:33:20,600 --> 00:33:27,289
an excel sheet if you want to get the

00:33:23,090 --> 00:33:29,629
total of one column then usually the one

00:33:27,289 --> 00:33:31,580
column only holds one type of data it's

00:33:29,629 --> 00:33:34,700
always floats or always integer or

00:33:31,580 --> 00:33:39,259
always data always whatever so they're

00:33:34,700 --> 00:33:42,200
exploiting this trait of data of being

00:33:39,259 --> 00:33:44,960
like conveniently aligned like one

00:33:42,200 --> 00:33:48,320
couldn't represent one type of data when

00:33:44,960 --> 00:33:50,809
you have this then when your data is

00:33:48,320 --> 00:33:53,960
typed and your what I call your data set

00:33:50,809 --> 00:33:59,590
is homogeneous like same type in the

00:33:53,960 --> 00:33:59,590
same column then you can profit from

00:34:00,220 --> 00:34:07,940
from optimized calculation procedures

00:34:05,059 --> 00:34:10,460
either in CPUs or just faster loops like

00:34:07,940 --> 00:34:13,460
if you're having all your data with the

00:34:10,460 --> 00:34:16,760
same type like summing up a whole column

00:34:13,460 --> 00:34:19,220
is much faster even if it's just going

00:34:16,760 --> 00:34:21,200
into C or Fortran behind the scenes but

00:34:19,220 --> 00:34:23,750
what you see from Python is usually

00:34:21,200 --> 00:34:26,629
something along the lines of lines then

00:34:23,750 --> 00:34:29,000
square brackets amount and then calling

00:34:26,629 --> 00:34:32,000
a sum function that magically returns

00:34:29,000 --> 00:34:37,000
exactly the same result as the iterative

00:34:32,000 --> 00:34:39,619
one but way way faster so these

00:34:37,000 --> 00:34:44,119
high-performance library which are they

00:34:39,619 --> 00:34:47,780
so you got something called numpy who

00:34:44,119 --> 00:34:51,440
knows numpy already fantastic so I'm

00:34:47,780 --> 00:34:54,520
going to be quick here so the base

00:34:51,440 --> 00:34:59,869
object in the Empire is the ng array

00:34:54,520 --> 00:35:02,690
it's incentivizing you to use the same

00:34:59,869 --> 00:35:05,930
type over the same array the only

00:35:02,690 --> 00:35:10,069
drawback that I found is that usually an

00:35:05,930 --> 00:35:11,990
array is it's a bit shape like what

00:35:10,069 --> 00:35:16,790
number of rows by number of columns so

00:35:11,990 --> 00:35:19,280
it's not it's very dense so if you've

00:35:16,790 --> 00:35:21,770
got sparse data like it you just you're

00:35:19,280 --> 00:35:23,420
just interested by a diagonal you still

00:35:21,770 --> 00:35:27,890
need to represent in memory the whole

00:35:23,420 --> 00:35:34,050
matrix which can be a bit RAM consuming

00:35:27,890 --> 00:35:36,089
the syntax is a bit and friendly unless

00:35:34,050 --> 00:35:38,790
you come from C and Fortran where it

00:35:36,089 --> 00:35:41,250
makes more sense it's very efficient for

00:35:38,790 --> 00:35:42,990
numerical operations and it has a good

00:35:41,250 --> 00:35:46,740
integration with site and that we are

00:35:42,990 --> 00:35:52,140
going to see a bit further then you get

00:35:46,740 --> 00:35:55,950
pandas - is the more data analyst

00:35:52,140 --> 00:35:58,619
friendly version of numpy it's still

00:35:55,950 --> 00:36:00,720
dragging a lot from them pi that the

00:35:58,619 --> 00:36:04,380
base objects are the cirrie for just

00:36:00,720 --> 00:36:07,260
like one dimension which is like there's

00:36:04,380 --> 00:36:09,569
always an index but like just one kind

00:36:07,260 --> 00:36:11,550
of data the data frame if you've got am

00:36:09,569 --> 00:36:14,550
a genius data and the index which is

00:36:11,550 --> 00:36:18,089
which allows you to access your data

00:36:14,550 --> 00:36:20,910
faster it it is batteries included so

00:36:18,089 --> 00:36:23,640
num pi is also has some helper function

00:36:20,910 --> 00:36:27,000
to read from CSV read from different

00:36:23,640 --> 00:36:30,210
file formats pandas comes Moo is like

00:36:27,000 --> 00:36:34,710
way more integration with way more

00:36:30,210 --> 00:36:37,109
formats saner defaults so trying to

00:36:34,710 --> 00:36:38,880
parse the CSV we spend us is less time

00:36:37,109 --> 00:36:41,730
consuming and less error-prone than

00:36:38,880 --> 00:36:43,980
trying to parse the CSV with numpy even

00:36:41,730 --> 00:36:45,240
though behind the scene it's exactly the

00:36:43,980 --> 00:36:48,569
same thing which happening which is

00:36:45,240 --> 00:36:51,180
happening management of date date times

00:36:48,569 --> 00:36:54,720
and time zone is also a bit simplified

00:36:51,180 --> 00:36:58,319
and very robust and so basically it's

00:36:54,720 --> 00:37:01,099
more user friendly but not yet pythonic

00:36:58,319 --> 00:37:04,099
sometimes it still looks a bit like

00:37:01,099 --> 00:37:04,099
numpy

00:37:15,160 --> 00:37:19,819
no-no-no number I definitely numpy is

00:37:17,750 --> 00:37:25,089
definitely not it's it's still very

00:37:19,819 --> 00:37:29,060
close to the way data is laid out on the

00:37:25,089 --> 00:37:32,030
like in the see structures behind pandas

00:37:29,060 --> 00:37:35,780
it's ready making a big big improvement

00:37:32,030 --> 00:37:38,390
regarding the the I would say the

00:37:35,780 --> 00:37:44,990
modeling of the date of the data but

00:37:38,390 --> 00:37:48,410
still you're not the way you applied

00:37:44,990 --> 00:37:51,230
functions for instance is so it's not

00:37:48,410 --> 00:37:53,930
really the way everyone would be writing

00:37:51,230 --> 00:37:55,670
it it's using its making an excessive

00:37:53,930 --> 00:37:57,560
use of lambdas for instance or it's

00:37:55,670 --> 00:37:59,030
advising you to do so

00:37:57,560 --> 00:38:01,069
so it's not something I would recommend

00:37:59,030 --> 00:38:04,250
like the way I would recommend doing

00:38:01,069 --> 00:38:06,500
things but yeah it's like it's for me

00:38:04,250 --> 00:38:08,900
pandas is it's way more convenient to

00:38:06,500 --> 00:38:12,200
use them Empire I don't pretend that

00:38:08,900 --> 00:38:16,520
Mumbai is more user friendly opportunity

00:38:12,200 --> 00:38:20,079
and then pandas so yeah here I would

00:38:16,520 --> 00:38:23,839
like just to check the behavior of

00:38:20,079 --> 00:38:27,170
counting Titan in in pure Python I want

00:38:23,839 --> 00:38:30,950
to check how many leaked passwords from

00:38:27,170 --> 00:38:33,099
a big also password leak that's occurred

00:38:30,950 --> 00:38:35,630
a few years ago from the rocky website

00:38:33,099 --> 00:38:39,319
and I would just want to check how many

00:38:35,630 --> 00:38:43,099
times the Eric string can be found in

00:38:39,319 --> 00:38:47,300
some passwords there so this is my very

00:38:43,099 --> 00:38:49,790
crude and not very smart way of counting

00:38:47,300 --> 00:38:52,490
lines in Python you got also many ways

00:38:49,790 --> 00:38:54,280
different ways of doing so that's one of

00:38:52,490 --> 00:38:57,109
them

00:38:54,280 --> 00:38:59,210
executing these scripts finds sixteen

00:38:57,109 --> 00:39:03,619
thousand six hundred and eighty one

00:38:59,210 --> 00:39:08,960
matches into thirty three seconds again

00:39:03,619 --> 00:39:11,390
99% CPU and 33 seconds this is how you

00:39:08,960 --> 00:39:14,380
would do it in pandas so basically

00:39:11,390 --> 00:39:17,930
reading the file into an array and then

00:39:14,380 --> 00:39:20,599
so I'm also counting the total lines and

00:39:17,930 --> 00:39:23,480
then just looking for how many rows

00:39:20,599 --> 00:39:24,970
contain the string Eric and then counts

00:39:23,480 --> 00:39:28,400
these lines

00:39:24,970 --> 00:39:31,910
so here it's doing a bit more stuff it's

00:39:28,400 --> 00:39:34,550
count its first checking its so the

00:39:31,910 --> 00:39:38,650
total size of the array so it's it has

00:39:34,550 --> 00:39:41,840
loaded 14 million passwords I guess and

00:39:38,650 --> 00:39:44,450
it found out so it's pretty weird it

00:39:41,840 --> 00:39:47,420
finds a different amount so I put that

00:39:44,450 --> 00:39:49,220
on the account that's the the function

00:39:47,420 --> 00:39:53,780
which are comparing strings are made

00:39:49,220 --> 00:39:57,350
differently in the in operator in Python

00:39:53,780 --> 00:40:02,420
and in pandas so it's finding a few less

00:39:57,350 --> 00:40:08,360
passwords but still it runs a bit faster

00:40:02,420 --> 00:40:10,490
than my my pure Python example the thing

00:40:08,360 --> 00:40:13,820
is here I'm demonstrating over strings

00:40:10,490 --> 00:40:15,310
which are not the most optimized way

00:40:13,820 --> 00:40:17,810
because like manipulating strings

00:40:15,310 --> 00:40:22,670
whatever language is always going to be

00:40:17,810 --> 00:40:25,280
more costly than manipulating numerical

00:40:22,670 --> 00:40:29,560
data so here if I would have taken a

00:40:25,280 --> 00:40:32,030
different example with numbers then the

00:40:29,560 --> 00:40:36,050
computation time would have been more

00:40:32,030 --> 00:40:39,290
striking then if you really really want

00:40:36,050 --> 00:40:41,630
to reinvent the wheel so let's say

00:40:39,290 --> 00:40:45,020
you've got a very business-critical

00:40:41,630 --> 00:40:48,410
function that's not easy to multiple

00:40:45,020 --> 00:40:50,720
with a bunch of pandas calls then you

00:40:48,410 --> 00:40:57,230
might want to write it inside them

00:40:50,720 --> 00:41:00,140
so what's cyton it's turning a site on

00:40:57,230 --> 00:41:03,290
code which is like a bastard language

00:41:00,140 --> 00:41:06,380
between Python and C so you can inject

00:41:03,290 --> 00:41:10,400
some Cena's in Python code

00:41:06,380 --> 00:41:14,270
so that's hinting the site and compiler

00:41:10,400 --> 00:41:18,470
to make some optimization for you so you

00:41:14,270 --> 00:41:20,930
write if you if you want to write C code

00:41:18,470 --> 00:41:22,460
you're totally free to do so it's

00:41:20,930 --> 00:41:24,560
possible you just write your C code

00:41:22,460 --> 00:41:27,800
compile your code and then you see if if

00:41:24,560 --> 00:41:29,990
our C types to call your code but then

00:41:27,800 --> 00:41:33,290
you're really into sealant so you're

00:41:29,990 --> 00:41:35,720
dealing with the PI objects yourself and

00:41:33,290 --> 00:41:39,130
exceptions or you have often going to

00:41:35,720 --> 00:41:42,920
end up in tears and seg faults

00:41:39,130 --> 00:41:45,860
while Satan is pre compiling your Python

00:41:42,920 --> 00:41:48,800
code in C doing all the linking wrapping

00:41:45,860 --> 00:41:53,270
so your code can be imported as easily

00:41:48,800 --> 00:41:56,690
as from my module import my function and

00:41:53,270 --> 00:41:59,120
you got a nice and seamless transition

00:41:56,690 --> 00:42:02,540
between C and Python context so you can

00:41:59,120 --> 00:42:04,550
still raise an exception from your C

00:42:02,540 --> 00:42:07,220
code where your site and code and still

00:42:04,550 --> 00:42:09,500
going to be executed a bit faster but

00:42:07,220 --> 00:42:12,440
it's not going to seg fault you get a

00:42:09,500 --> 00:42:14,990
proper Python exception you can use

00:42:12,440 --> 00:42:17,960
print also well even though printf is

00:42:14,990 --> 00:42:20,270
fine and you don't have to deal with Pi

00:42:17,960 --> 00:42:22,580
objects if your if you don't want to sew

00:42:20,270 --> 00:42:26,480
your you have the ability to do so but

00:42:22,580 --> 00:42:28,580
if you prefer to just use the my my

00:42:26,480 --> 00:42:33,440
object dot something you're free to do

00:42:28,580 --> 00:42:35,300
so in seitan so that's a shamelessly

00:42:33,440 --> 00:42:39,200
stole an example from the site and

00:42:35,300 --> 00:42:42,200
website because it's pretty eloquent so

00:42:39,200 --> 00:42:46,790
that's your regular integrating function

00:42:42,200 --> 00:42:51,920
in Python so that's we just call it this

00:42:46,790 --> 00:42:55,940
way in seitan you will start seeing

00:42:51,920 --> 00:42:57,860
those sneaky see deaths so which have

00:42:55,940 --> 00:43:01,700
nothing to do with the DEF here it's

00:42:57,860 --> 00:43:04,880
just a different way of hinting sight on

00:43:01,700 --> 00:43:08,390
that and also here you can see that the

00:43:04,880 --> 00:43:13,820
variables have been typed it's different

00:43:08,390 --> 00:43:17,120
from the annotations from Python 3.6 and

00:43:13,820 --> 00:43:18,860
up here it's like more like C so if

00:43:17,120 --> 00:43:20,980
you're familiar with C it will look very

00:43:18,860 --> 00:43:24,080
familiar and here you get your

00:43:20,980 --> 00:43:27,650
definitions of okay is going to be an

00:43:24,080 --> 00:43:30,980
integer and SN DX are going to be double

00:43:27,650 --> 00:43:35,870
and this is already making a big

00:43:30,980 --> 00:43:39,890
improvement and then if you you can also

00:43:35,870 --> 00:43:43,190
type your function all together so if

00:43:39,890 --> 00:43:46,160
I'm I'm here this function is still a

00:43:43,190 --> 00:43:48,560
Python function okay I I can make some

00:43:46,160 --> 00:43:52,130
deductions based on the type of the

00:43:48,560 --> 00:43:54,020
variables which are going to already

00:43:52,130 --> 00:43:58,730
produce some performance improvements

00:43:54,020 --> 00:44:01,790
but it's still going to run in from

00:43:58,730 --> 00:44:06,320
within the patent as Python code

00:44:01,790 --> 00:44:10,370
this tells seitan okay this is just pure

00:44:06,320 --> 00:44:13,580
C you just executed as if it was C code

00:44:10,370 --> 00:44:16,910
you just don't look at anything but then

00:44:13,580 --> 00:44:21,200
indeed you risk having a poor exception

00:44:16,910 --> 00:44:25,070
handling mechanism by seitan gives you

00:44:21,200 --> 00:44:27,740
the except additional keyword that just

00:44:25,070 --> 00:44:30,380
tells you okay if there was an exception

00:44:27,740 --> 00:44:38,000
to a cure just return - - please don't

00:44:30,380 --> 00:44:42,140
seg fault and and it also provides you a

00:44:38,000 --> 00:44:44,180
nice tool that's helping you figure out

00:44:42,140 --> 00:44:46,400
which part of your code are still

00:44:44,180 --> 00:44:49,700
running in Python and which part of your

00:44:46,400 --> 00:44:52,280
code are running in Sealand so the more

00:44:49,700 --> 00:44:57,200
yellow the more Python so here if you

00:44:52,280 --> 00:44:59,960
can see it just shows like this function

00:44:57,200 --> 00:45:02,660
call is obviously going to be run in

00:44:59,960 --> 00:45:05,870
Python because that's the entry point so

00:45:02,660 --> 00:45:09,860
it's pretty hard to turn it into pure C

00:45:05,870 --> 00:45:12,200
but my integrate function here has

00:45:09,860 --> 00:45:14,630
turned completely white it means that

00:45:12,200 --> 00:45:17,840
Python is not going to be involved there

00:45:14,630 --> 00:45:20,690
so no casting between PI objects and

00:45:17,840 --> 00:45:22,850
integers like integers are integers for

00:45:20,690 --> 00:45:27,650
the whole time we are spending in this

00:45:22,850 --> 00:45:29,600
function call and here most of the

00:45:27,650 --> 00:45:31,940
function has been like the C devs

00:45:29,600 --> 00:45:35,810
obviously are in C this one is yellow

00:45:31,940 --> 00:45:38,890
ish so some bits are in Python some bits

00:45:35,810 --> 00:45:42,950
are in C so seitan is really helping you

00:45:38,890 --> 00:45:46,280
with this tool to figure out which piece

00:45:42,950 --> 00:45:49,780
you still need to convert into pure C to

00:45:46,280 --> 00:45:49,780
achieve maximum performance

00:45:50,140 --> 00:45:56,600
the only drawback with seitan it feels

00:45:54,170 --> 00:45:58,700
like okay that's that's great only

00:45:56,600 --> 00:46:00,740
problem is that you're actually

00:45:58,700 --> 00:46:03,470
introducing all the problems you have

00:46:00,740 --> 00:46:05,540
with deploying C code in your program

00:46:03,470 --> 00:46:07,520
meaning you need to pack

00:46:05,540 --> 00:46:09,320
and distributed meaning also need to

00:46:07,520 --> 00:46:12,200
compile it for each architecture you

00:46:09,320 --> 00:46:14,120
intended to run on that's why you have

00:46:12,200 --> 00:46:17,180
different versions of numpy in different

00:46:14,120 --> 00:46:20,120
versions of Python so the different

00:46:17,180 --> 00:46:22,190
version of PennDOT's

00:46:20,120 --> 00:46:26,960
for each kind of architecture and

00:46:22,190 --> 00:46:30,320
platforms out there still it helps you

00:46:26,960 --> 00:46:34,400
as much as it can by providing you if

00:46:30,320 --> 00:46:36,920
you set up tool hooks so if you want to

00:46:34,400 --> 00:46:41,390
turn a module into seitan it's pretty

00:46:36,920 --> 00:46:44,390
easy you just use the pyx extension

00:46:41,390 --> 00:46:47,300
called seitan eyes function from sight

00:46:44,390 --> 00:46:50,240
and itself and then every time you want

00:46:47,300 --> 00:46:52,580
to build your or rebuild your function

00:46:50,240 --> 00:46:54,800
you just called build X with the in

00:46:52,580 --> 00:47:00,290
place flag and it's going to turn your

00:46:54,800 --> 00:47:01,850
PI X code into a proper C module site on

00:47:00,290 --> 00:47:04,610
module and then from there it's very

00:47:01,850 --> 00:47:07,520
easy you just do from C integrate import

00:47:04,610 --> 00:47:11,600
integrate F and then you just call it as

00:47:07,520 --> 00:47:14,750
if it was a part in function which is

00:47:11,600 --> 00:47:18,140
pretty neat and that's hope for instance

00:47:14,750 --> 00:47:20,150
pandas is made in pandas you never know

00:47:18,140 --> 00:47:22,970
if you're running seitan or Python code

00:47:20,150 --> 00:47:28,490
because everything is hidden behind the

00:47:22,970 --> 00:47:30,380
curtain and last but not least pi pi

00:47:28,490 --> 00:47:31,610
it's an initiative from

00:47:30,380 --> 00:47:36,060
[Music]

00:47:31,610 --> 00:47:39,390
- guys I think they're not very much on

00:47:36,060 --> 00:47:44,610
this project it tries to bring

00:47:39,390 --> 00:47:49,710
just-in-time compilation on the Python

00:47:44,610 --> 00:47:52,290
language so like what is just-in-time

00:47:49,710 --> 00:47:54,560
optimization how many are familiar with

00:47:52,290 --> 00:47:58,890
the notion of Just In Time optimization

00:47:54,560 --> 00:48:02,400
that mini okay so I'm going to go fast

00:47:58,890 --> 00:48:04,470
they're like compilers that let's say

00:48:02,400 --> 00:48:09,270
the C compiler it just looks at your

00:48:04,470 --> 00:48:11,160
code like before it even starts and make

00:48:09,270 --> 00:48:13,800
some assumptions on how it's going to be

00:48:11,160 --> 00:48:15,690
executed but it doesn't take into

00:48:13,800 --> 00:48:17,670
account the actual way you're going to

00:48:15,690 --> 00:48:20,160
use it so maybe some function will not

00:48:17,670 --> 00:48:22,920
receive as much optimization as another

00:48:20,160 --> 00:48:25,200
one or one path of optimization will be

00:48:22,920 --> 00:48:27,630
used and that's not the one which will

00:48:25,200 --> 00:48:30,480
yield the best results but the problem

00:48:27,630 --> 00:48:33,390
is the only way you would know is after

00:48:30,480 --> 00:48:35,850
the fact when you've been running the

00:48:33,390 --> 00:48:39,440
function in production with actual data

00:48:35,850 --> 00:48:42,390
that's what just-in-time does it just

00:48:39,440 --> 00:48:45,540
instruments your code look at what's

00:48:42,390 --> 00:48:48,660
being executed and then from there it's

00:48:45,540 --> 00:48:52,020
being it's making improvement to the

00:48:48,660 --> 00:48:53,400
code and to the code pass on the fly so

00:48:52,020 --> 00:48:55,440
that's an alternative Python

00:48:53,400 --> 00:48:57,900
implementation which is hundred-person

00:48:55,440 --> 00:49:00,210
compatible with two point seven fifteen

00:48:57,900 --> 00:49:04,350
and three five something three five

00:49:00,210 --> 00:49:06,810
three I guess it's not yet 100%

00:49:04,350 --> 00:49:09,060
comfortable with some sea-based

00:49:06,810 --> 00:49:12,390
libraries namely pandas but it's

00:49:09,060 --> 00:49:15,810
comfortable with numpy so if you're able

00:49:12,390 --> 00:49:18,140
to achieve your work with just numpy

00:49:15,810 --> 00:49:21,150
then you can use pi pi as well

00:49:18,140 --> 00:49:24,420
it automatically automatically rewrites

00:49:21,150 --> 00:49:26,670
the internal logic of your code for

00:49:24,420 --> 00:49:29,790
better performance but it needs a lot of

00:49:26,670 --> 00:49:34,830
data to make better decisions which

00:49:29,790 --> 00:49:37,320
means that it has a very slow warmup

00:49:34,830 --> 00:49:40,950
time so if you execute it on a very

00:49:37,320 --> 00:49:43,110
small subset of data then you won't see

00:49:40,950 --> 00:49:44,680
any performance improvement you might

00:49:43,110 --> 00:49:49,660
even see a per for

00:49:44,680 --> 00:49:52,450
aggravation if but if you

00:49:49,660 --> 00:49:54,190
conversely use it with a very big data

00:49:52,450 --> 00:49:57,880
set then you're going to see a big

00:49:54,190 --> 00:49:59,800
improvement so that's that's a very

00:49:57,880 --> 00:50:02,770
simple example let's say I have a

00:49:59,800 --> 00:50:05,140
message I want to transmit and I'm

00:50:02,770 --> 00:50:08,260
making a lot of these messages and I'm

00:50:05,140 --> 00:50:15,970
storing them and I want to check the

00:50:08,260 --> 00:50:19,390
last message length so so I'm creating

00:50:15,970 --> 00:50:23,230
let's let's face it I'm creating 5 5 or

00:50:19,390 --> 00:50:26,490
15 no just five five million objects so

00:50:23,230 --> 00:50:30,570
that's quite a lot for the C C Python

00:50:26,490 --> 00:50:33,640
interpreter so making five million

00:50:30,570 --> 00:50:38,500
objects storing them and looking at the

00:50:33,640 --> 00:50:41,230
last it takes 20 seconds on C button use

00:50:38,500 --> 00:50:46,330
99 percent of the CPU and yeah but takes

00:50:41,230 --> 00:50:49,840
about 20 seconds in total with pi pi the

00:50:46,330 --> 00:50:52,150
same thing takes five mere seconds still

00:50:49,840 --> 00:50:54,610
using whole all of my CPU but only for

00:50:52,150 --> 00:50:57,340
about six seconds total

00:50:54,610 --> 00:51:00,280
why so it's because it had the time to

00:50:57,340 --> 00:51:04,650
train itself while making five million

00:51:00,280 --> 00:51:09,730
messages now if I try with five hundred

00:51:04,650 --> 00:51:12,730
then it's actually not better so python

00:51:09,730 --> 00:51:17,590
is doing the job in foresight in 0.04

00:51:12,730 --> 00:51:20,890
seconds but sigh pi pi is doing in 0.06

00:51:17,590 --> 00:51:25,210
so here again it's to take with a grain

00:51:20,890 --> 00:51:31,630
of salt because obviously measurements

00:51:25,210 --> 00:51:34,950
in precision might might occur here but

00:51:31,630 --> 00:51:37,500
still it's not like significantly better

00:51:34,950 --> 00:51:41,050
500 is not enough

00:51:37,500 --> 00:51:43,360
so yeah the good thing with just-in-time

00:51:41,050 --> 00:51:45,850
is you can take existing code you

00:51:43,360 --> 00:51:49,210
already have just running through pi pi

00:51:45,850 --> 00:51:52,300
and if you don't have any strong C

00:51:49,210 --> 00:51:54,670
dependency then it's going to be

00:51:52,300 --> 00:51:57,590
religiously fast I've tried this

00:51:54,670 --> 00:51:59,930
messages example with 15

00:51:57,590 --> 00:52:04,040
Millions and it's still like about

00:51:59,930 --> 00:52:07,460
between 10 and 16 times faster than the

00:52:04,040 --> 00:52:09,440
see bitin version yeah the only problem

00:52:07,460 --> 00:52:11,600
is if you need pandas then currently

00:52:09,440 --> 00:52:14,180
you're kind of stuck the guys working on

00:52:11,600 --> 00:52:16,250
pi PI are also working kind on

00:52:14,180 --> 00:52:17,720
compatibility improving the combative

00:52:16,250 --> 00:52:19,360
did you spend us but it's not yet

00:52:17,720 --> 00:52:22,550
achieved

00:52:19,360 --> 00:52:24,050
it's another interpreter that you need

00:52:22,550 --> 00:52:27,260
to maintain and so on if you're

00:52:24,050 --> 00:52:30,980
deploying on servers it works definitely

00:52:27,260 --> 00:52:33,110
better with pure Python types so like if

00:52:30,980 --> 00:52:38,810
you again have C structures and so on

00:52:33,110 --> 00:52:43,390
it might yield another performance and

00:52:38,810 --> 00:52:43,390
it definitely needs a warmup period

00:52:43,450 --> 00:52:49,460
the key takeaway is that you can't have

00:52:47,090 --> 00:52:53,360
it all every time you're optimizing in

00:52:49,460 --> 00:52:56,300
one way you're like reducing like

00:52:53,360 --> 00:52:58,700
readability maintainability ease of

00:52:56,300 --> 00:53:01,540
deployment compatibility with different

00:52:58,700 --> 00:53:03,830
libraries whatever time the kind of

00:53:01,540 --> 00:53:06,890
optimization you're doing you're always

00:53:03,830 --> 00:53:09,740
like sliding in one direction but like

00:53:06,890 --> 00:53:11,330
moving away from another thing that

00:53:09,740 --> 00:53:13,580
might be important so it's pretty hard

00:53:11,330 --> 00:53:18,860
to really find a sweet spot where you

00:53:13,580 --> 00:53:20,840
get it all in summary if you get a kind

00:53:18,860 --> 00:53:22,670
of simple code base and a wide

00:53:20,840 --> 00:53:26,450
deployment let's say you've written a

00:53:22,670 --> 00:53:29,330
funny library then your you should aim

00:53:26,450 --> 00:53:32,210
for the low-hanging fruits first then

00:53:29,330 --> 00:53:35,630
use maybe some optimized library let's

00:53:32,210 --> 00:53:37,970
say pandas or numpy and then finally get

00:53:35,630 --> 00:53:42,680
better hardware like more cores or

00:53:37,970 --> 00:53:45,620
faster CPU if your code is very easy to

00:53:42,680 --> 00:53:48,410
run in parallel then you should just aim

00:53:45,620 --> 00:53:52,060
for more threads or processes depending

00:53:48,410 --> 00:53:54,890
if your IO bound or CPU bound and

00:53:52,060 --> 00:53:56,300
usually just throwing more CPUs under

00:53:54,890 --> 00:53:58,940
the form or more cores on the same

00:53:56,300 --> 00:54:01,820
machine or more boxes running the same

00:53:58,940 --> 00:54:06,680
code is going to make it dramatically

00:54:01,820 --> 00:54:09,560
faster if you're unlucky and you have to

00:54:06,680 --> 00:54:11,660
deal with sequential code and your end

00:54:09,560 --> 00:54:14,630
or your deployment

00:54:11,660 --> 00:54:16,460
options are limited usually just finding

00:54:14,630 --> 00:54:19,220
better hardware is going to make an

00:54:16,460 --> 00:54:21,470
improvement like if you have a CPU that

00:54:19,220 --> 00:54:24,739
which is twice faster your program is

00:54:21,470 --> 00:54:28,279
going to run twice faster you can also

00:54:24,739 --> 00:54:31,759
try pi pi which for similar hardware

00:54:28,279 --> 00:54:37,819
runs sometimes significantly faster and

00:54:31,759 --> 00:54:41,269
or seitan thanks a lot for your time if

00:54:37,819 --> 00:54:43,970
you got any optimization related

00:54:41,269 --> 00:54:46,220
questions I still have four minutes if I

00:54:43,970 --> 00:54:53,960
my calculations are correct now I'm out

00:54:46,220 --> 00:54:55,260
for moment any question maybe no thank

00:54:53,960 --> 00:55:04,059
you very much

00:54:55,260 --> 00:55:04,059
[Applause]

00:55:09,190 --> 00:55:11,250

YouTube URL: https://www.youtube.com/watch?v=B4ul8rgnt3U


