Title: The unsupervised free CAT for low resource languages Building a pipeline for the communities
Publication date: 2020-07-15
Playlist: FOSDEM 2020
Description: 
	by Alberto Massidda

At: FOSDEM 2020
https://video.fosdem.org/2020/AW1.120/clc_the_unsupervised_free_cat_for_low_resource_languages.webm

We present: 1) a full pipeline for unsupervised machine translation training (making use of monolingual corpora) for languages with low available resources; 2) a translation server making use of that unsupervised MT with an HTTP API compatible with Moses toolkit, a once prominent MT system; 3) a Docker packaged version of the EU funded free Computer Aided Translation (CAT) tool MateCAT for ease of deployment.
This full translation pipeline enables a non technical user, speaking a non-FIGS language for which there is scarcity of parallel corpora, to start translating documents and software following translation industry standards.
Localization within community suffers from the fragmentation of technologies (too wide wedge between commercial Computer Aided Translation tools and free ones), available language resources (making difficult to train a Machine Translation) and lack of clear and robust pipelines to get started.
Low resource language communities suffer the most, since MT systems require training corpora of millions of words and industry has settled to expecting the massive corpora available to FIGS (French, Italian, German, Spanish) languages.
Moreover, the community suffers from a lack of adoption of established technologies and workflows, leading to reinventing the wheel and suboptimal effortsâ€™ outcomes.
Today we would like to present a connector for the implementation of an unsupervised MT (made by Artetxe et al.), that claims a BLEU of 26 on limited language resources (which is enough as a support system) integrated with MateCAT, an industry level, free, web based tool funded by EU, in order to provide a more viable alternative to resorting to Google Translate and commercial LSPs.

Room: AW1.120
Scheduled start: 2020-02-01 14:00:00
Captions: 
	00:00:06,240 --> 00:00:17,790
okay so good afternoon thank you for

00:00:14,530 --> 00:00:20,680
joining the session

00:00:17,790 --> 00:00:24,430
my name is Alberto and today we are

00:00:20,680 --> 00:00:28,390
going to talk about this unsupervised

00:00:24,430 --> 00:00:30,730
the free capped a catcher is the novel

00:00:28,390 --> 00:00:32,430
only the animal by in this case is best

00:00:30,730 --> 00:00:34,450
software how we call that in the

00:00:32,430 --> 00:00:39,250
translation language translation

00:00:34,450 --> 00:00:42,490
industry the software to translate I'm

00:00:39,250 --> 00:00:44,710
36 years older I started working more

00:00:42,490 --> 00:00:46,060
than 10 years ago I've been at the

00:00:44,710 --> 00:00:48,550
University I'm mastering computer

00:00:46,060 --> 00:00:52,450
science and I'm working most of my

00:00:48,550 --> 00:00:55,240
career in startups do today I look after

00:00:52,450 --> 00:00:57,579
as I sorry in develops a business unit

00:00:55,240 --> 00:01:00,540
of the company who sent me here source

00:00:57,579 --> 00:01:04,750
Anza and also I'm a machine learning

00:01:00,540 --> 00:01:07,030
major for the unit my forte is in

00:01:04,750 --> 00:01:09,640
kubernetes monitoring us continuous

00:01:07,030 --> 00:01:12,479
integration cloud hadoop so big data

00:01:09,640 --> 00:01:15,430
python everything which is around

00:01:12,479 --> 00:01:18,220
Jupiter notebook scikit-learn fanciful

00:01:15,430 --> 00:01:21,790
at night I attended in the spare time

00:01:18,220 --> 00:01:27,790
they have to model concerts and there's

00:01:21,790 --> 00:01:29,470
something here this diva one is just

00:01:27,790 --> 00:01:33,630
really annoying

00:01:29,470 --> 00:01:33,630
ok it won't go away

00:01:40,620 --> 00:01:46,770
great so we like to thank all so much

00:01:43,380 --> 00:01:49,230
the company was sending me here source

00:01:46,770 --> 00:01:50,970
sensor we are basically a consulting

00:01:49,230 --> 00:01:52,680
company completely devoted to open

00:01:50,970 --> 00:01:55,200
source software we've been existing

00:01:52,680 --> 00:01:57,660
since 20 years and we have a virus

00:01:55,200 --> 00:02:02,400
branches in Italy also one in London

00:01:57,660 --> 00:02:05,430
some so no more in Europe and we are we

00:02:02,400 --> 00:02:08,399
are delp's people we are coders we are

00:02:05,430 --> 00:02:09,660
data scientists and so we take after 360

00:02:08,399 --> 00:02:12,330
degrees everything we can

00:02:09,660 --> 00:02:14,400
so today the outline is as its follows

00:02:12,330 --> 00:02:17,460
so we're starting with the need for a

00:02:14,400 --> 00:02:20,400
standard tool chain for translating and

00:02:17,460 --> 00:02:23,220
localizing as we say software project

00:02:20,400 --> 00:02:25,050
then I will talk about the catch-all I'm

00:02:23,220 --> 00:02:27,800
bringing here today which is mate cat

00:02:25,050 --> 00:02:30,060
and what's the problem with today's

00:02:27,800 --> 00:02:33,410
machine translation which is the most

00:02:30,060 --> 00:02:36,810
fundamental supporting system for

00:02:33,410 --> 00:02:38,580
translation pipeline and what is the

00:02:36,810 --> 00:02:47,400
solution which is coming to the rescue

00:02:38,580 --> 00:02:49,440
and what we have here practically so so

00:02:47,400 --> 00:02:51,120
why do we need a standard tool chain

00:02:49,440 --> 00:02:55,610
although there are a lot of solutions

00:02:51,120 --> 00:02:58,620
out there to translate documents and

00:02:55,610 --> 00:03:01,350
software most importantly it feels that

00:02:58,620 --> 00:03:04,470
too many times we have too many ways to

00:03:01,350 --> 00:03:08,520
skin at the very same cat and this is

00:03:04,470 --> 00:03:10,739
because a lot of people are not into a

00:03:08,520 --> 00:03:13,440
translation industry so who doesn't

00:03:10,739 --> 00:03:15,270
understand reinvents from old from

00:03:13,440 --> 00:03:18,650
scratch and these always very dangerous

00:03:15,270 --> 00:03:21,720
because it ferments the efforts and so

00:03:18,650 --> 00:03:24,930
this software industry and particularly

00:03:21,720 --> 00:03:27,209
the false industry needs to adapt more

00:03:24,930 --> 00:03:29,970
to the industrial way of doing things

00:03:27,209 --> 00:03:31,980
which is the only one viable one and

00:03:29,970 --> 00:03:33,840
this because the industry actors which

00:03:31,980 --> 00:03:37,050
are the language service providers the

00:03:33,840 --> 00:03:39,600
LSP is a long set onto a consolidated

00:03:37,050 --> 00:03:42,030
set of processes of technologies and

00:03:39,600 --> 00:03:44,220
file formats there is all even a

00:03:42,030 --> 00:03:47,180
consortium which is called oasis which

00:03:44,220 --> 00:03:51,299
standardizes the file formats and the

00:03:47,180 --> 00:03:54,900
interchange technologies there allows to

00:03:51,299 --> 00:03:57,390
build the viable industry

00:03:54,900 --> 00:03:59,489
this man in this show is about exposing

00:03:57,390 --> 00:04:01,409
with battle-tested technologies to the

00:03:59,489 --> 00:04:03,569
community so we can return him back soon

00:04:01,409 --> 00:04:06,360
to what we love which is hacking and not

00:04:03,569 --> 00:04:09,599
coming up with new ways of translating a

00:04:06,360 --> 00:04:11,459
software or translating a document so

00:04:09,599 --> 00:04:13,769
the standard tool change for translation

00:04:11,459 --> 00:04:16,139
for translation company or translate a

00:04:13,769 --> 00:04:18,030
an open-source project who wants to be

00:04:16,139 --> 00:04:20,400
translated starts with a cat a

00:04:18,030 --> 00:04:24,150
computer-aided translation tool which is

00:04:20,400 --> 00:04:26,880
an editor that parses a bilingual file a

00:04:24,150 --> 00:04:29,970
bilingual file is a kind of an envelope

00:04:26,880 --> 00:04:32,100
container which represents our files

00:04:29,970 --> 00:04:35,400
which which is in the process of being

00:04:32,100 --> 00:04:35,970
translated so you put in this file in

00:04:35,400 --> 00:04:38,880
this envelope

00:04:35,970 --> 00:04:41,190
everything is extracted and you can

00:04:38,880 --> 00:04:44,340
manipulate the strings which is the text

00:04:41,190 --> 00:04:47,669
the buttons text labels the menu voices

00:04:44,340 --> 00:04:49,199
but even a simple docx document you can

00:04:47,669 --> 00:04:53,699
manipulate these strings into other

00:04:49,199 --> 00:04:56,190
languages and among other things it has

00:04:53,699 --> 00:04:59,639
to provide the true main cavities which

00:04:56,190 --> 00:05:01,710
is the tagger editing capability so you

00:04:59,639 --> 00:05:04,169
can manipulate the untranslatable

00:05:01,710 --> 00:05:07,979
entities like markup if you think about

00:05:04,169 --> 00:05:10,800
it a lot of the content in a well

00:05:07,979 --> 00:05:12,960
formatted file is the formatting which

00:05:10,800 --> 00:05:14,789
has nothing it doesn't need to be

00:05:12,960 --> 00:05:16,530
translated that you must not translate

00:05:14,789 --> 00:05:19,410
that you must preserve it otherwise you

00:05:16,530 --> 00:05:21,060
you're going to break the markup and so

00:05:19,410 --> 00:05:23,849
you have to manipulate that's the thing

00:05:21,060 --> 00:05:25,889
living that untouched and also number

00:05:23,849 --> 00:05:29,130
two format preservation so you must

00:05:25,889 --> 00:05:32,280
convert from the original file to a

00:05:29,130 --> 00:05:34,080
bilingual system then you have to

00:05:32,280 --> 00:05:36,750
translate all the strings and then you

00:05:34,080 --> 00:05:38,940
can pack the strings back in the origin

00:05:36,750 --> 00:05:40,889
file format preserving the formatting

00:05:38,940 --> 00:05:44,220
you don't want to break the file just

00:05:40,889 --> 00:05:46,260
because you change the strings another

00:05:44,220 --> 00:05:48,419
really fundamental component is the

00:05:46,260 --> 00:05:50,520
presence of a translation memory you can

00:05:48,419 --> 00:05:53,430
think a translation memory as a database

00:05:50,520 --> 00:05:56,070
of past translations that you can

00:05:53,430 --> 00:05:58,710
recycle or adapt for the incoming

00:05:56,070 --> 00:06:01,590
document which is coming here and the

00:05:58,710 --> 00:06:04,700
idea is that once you translate

00:06:01,590 --> 00:06:07,860
something you don't throw away all the

00:06:04,700 --> 00:06:09,719
singular translations but you keep them

00:06:07,860 --> 00:06:11,460
the next time you're gonna make a second

00:06:09,719 --> 00:06:14,099
revision for example of dishwashing

00:06:11,460 --> 00:06:16,889
machine manual you just want to use the

00:06:14,099 --> 00:06:19,319
very same sentences and just translate a

00:06:16,889 --> 00:06:21,060
new ones it's a matter of style guide

00:06:19,319 --> 00:06:25,740
it's a matter of preserving the

00:06:21,060 --> 00:06:26,550
experience like control panel from

00:06:25,740 --> 00:06:29,550
windows

00:06:26,550 --> 00:06:31,979
Italy is panel to control you cannot

00:06:29,550 --> 00:06:35,009
change that sentence to something else

00:06:31,979 --> 00:06:38,460
otherwise me all the window users who

00:06:35,009 --> 00:06:41,219
are customized the kind of sentence

00:06:38,460 --> 00:06:43,110
appearing somewhere in the menu will be

00:06:41,219 --> 00:06:48,300
confused if you change to somebody else

00:06:43,110 --> 00:06:50,939
like panel oh I don't know page control

00:06:48,300 --> 00:06:52,860
and for the control so you must preserve

00:06:50,939 --> 00:06:54,960
this style you must preserve everything

00:06:52,860 --> 00:06:56,819
that is part of the experience of the

00:06:54,960 --> 00:07:00,629
software and translation memory serves

00:06:56,819 --> 00:07:02,009
this purpose and last but not least

00:07:00,629 --> 00:07:03,960
component you need a machine translation

00:07:02,009 --> 00:07:06,270
which is a server that provides

00:07:03,960 --> 00:07:09,599
translations on the fly which is Google

00:07:06,270 --> 00:07:11,819
Translate for example so some dominant

00:07:09,599 --> 00:07:13,759
standards which are worth knowing is the

00:07:11,819 --> 00:07:15,569
ax life which is the exchange

00:07:13,759 --> 00:07:19,710
localization

00:07:15,569 --> 00:07:22,440
file format which is an xml-based file

00:07:19,710 --> 00:07:25,620
envelope that separates the strings from

00:07:22,440 --> 00:07:28,680
the markup so you to the doc X into X

00:07:25,620 --> 00:07:31,139
left envelope then you have like The

00:07:28,680 --> 00:07:33,210
Blob the binary of the original file who

00:07:31,139 --> 00:07:34,979
has been processed by removing all the

00:07:33,210 --> 00:07:37,529
strings and substituting that with

00:07:34,979 --> 00:07:40,469
placeholders then the placeholders are

00:07:37,529 --> 00:07:42,509
into a map that map's towards the

00:07:40,469 --> 00:07:44,759
strings you have them for the original

00:07:42,509 --> 00:07:47,789
language you are coming from but then

00:07:44,759 --> 00:07:50,550
you can add the new keys for the other

00:07:47,789 --> 00:07:52,289
languages you want to go into and so

00:07:50,550 --> 00:07:54,330
whenever you want to translate a

00:07:52,289 --> 00:07:56,250
document for example in English you just

00:07:54,330 --> 00:08:00,779
add the keys for the English that map

00:07:56,250 --> 00:08:02,969
then you pack telling the software to

00:08:00,779 --> 00:08:04,830
read only the keys in that language code

00:08:02,969 --> 00:08:07,439
and packs back the translated strings

00:08:04,830 --> 00:08:10,080
into the original document so replace

00:08:07,439 --> 00:08:12,479
the placeholders with the new strings

00:08:10,080 --> 00:08:14,550
all the document is saved and what you

00:08:12,479 --> 00:08:17,370
end up with is a file format with

00:08:14,550 --> 00:08:18,719
different language which feels like kind

00:08:17,370 --> 00:08:21,230
of magical up the first time you see it

00:08:18,719 --> 00:08:24,290
I'm gonna show it here

00:08:21,230 --> 00:08:25,940
TMX which is an XML exchange format for

00:08:24,290 --> 00:08:27,410
translation memories you want to export

00:08:25,940 --> 00:08:29,390
your translation memory and give it to

00:08:27,410 --> 00:08:32,090
another translator or to another member

00:08:29,390 --> 00:08:34,970
of the community you use a TM X which is

00:08:32,090 --> 00:08:37,220
a standard file format and the last

00:08:34,970 --> 00:08:40,190
format worth talking about is the PIO

00:08:37,220 --> 00:08:42,560
which is a the property property the

00:08:40,190 --> 00:08:46,190
specialized file format for the get text

00:08:42,560 --> 00:08:49,040
library which is a good new tool and is

00:08:46,190 --> 00:08:52,220
the one that you put the strings into

00:08:49,040 --> 00:08:54,560
and the software is expected to find the

00:08:52,220 --> 00:08:57,170
strings in that format so any anything

00:08:54,560 --> 00:08:59,480
that compiles with the get text library

00:08:57,170 --> 00:09:01,250
is able to be localized in this way when

00:08:59,480 --> 00:09:04,850
you start localizing a project you look

00:09:01,250 --> 00:09:07,400
for the get text peo files how not to do

00:09:04,850 --> 00:09:09,470
it for example iOS and Android

00:09:07,400 --> 00:09:11,000
there are long-standing offenders

00:09:09,470 --> 00:09:13,390
because they came up with their own

00:09:11,000 --> 00:09:15,620
proprietary file formats which is the

00:09:13,390 --> 00:09:19,730
localizable.strings and the strings.xml

00:09:15,620 --> 00:09:24,200
and there are tools to convert this

00:09:19,730 --> 00:09:25,850
stuff to the PIO format so it's under

00:09:24,200 --> 00:09:29,180
workflow is that you go you come with an

00:09:25,850 --> 00:09:31,790
original file you put it into a CAD tool

00:09:29,180 --> 00:09:33,200
or a cat server and that generates the

00:09:31,790 --> 00:09:34,970
next leaf container which is the

00:09:33,200 --> 00:09:36,890
envelope of the original file then you

00:09:34,970 --> 00:09:38,930
query the translation memory or a

00:09:36,890 --> 00:09:42,320
machine translation and you feel the

00:09:38,930 --> 00:09:44,390
content in it at any time you can export

00:09:42,320 --> 00:09:46,490
the container to an excel file and sign

00:09:44,390 --> 00:09:48,890
to an independent translator to have it

00:09:46,490 --> 00:09:49,880
translated this particularly important

00:09:48,890 --> 00:09:51,440
because you don't want to send the

00:09:49,880 --> 00:09:54,050
original file you don't want to develop

00:09:51,440 --> 00:09:56,930
the translator to gain access to the

00:09:54,050 --> 00:09:59,120
original 5 which may be reserved which

00:09:56,930 --> 00:10:02,480
isn't translating to also the file may

00:09:59,120 --> 00:10:05,510
be so big they want to split the load we

00:10:02,480 --> 00:10:08,360
among 10 translators so you have a 10

00:10:05,510 --> 00:10:12,080
small excel file with the 10 parallel

00:10:08,360 --> 00:10:14,210
pieces of stuff anytime you can export

00:10:12,080 --> 00:10:16,160
translation memory in a TMX or the

00:10:14,210 --> 00:10:18,700
machine translation in a modify you want

00:10:16,160 --> 00:10:21,860
deploy somewhere else and at any point

00:10:18,700 --> 00:10:24,830
you can take the extract container with

00:10:21,860 --> 00:10:27,500
the strings that bin translator until

00:10:24,830 --> 00:10:29,000
that point repackage back them into a

00:10:27,500 --> 00:10:31,160
translate file which is the thing you

00:10:29,000 --> 00:10:32,900
want to go back to your customer or to

00:10:31,160 --> 00:10:34,260
the community if you're working in a

00:10:32,900 --> 00:10:37,600
project

00:10:34,260 --> 00:10:40,450
so today we are showing the our kettle

00:10:37,600 --> 00:10:41,340
of choice is mate cat mate cat is an

00:10:40,450 --> 00:10:43,030
enterprise-grade

00:10:41,340 --> 00:10:46,750
completely free and open-source

00:10:43,030 --> 00:10:49,450
web-based the cat tool and it has been

00:10:46,750 --> 00:10:51,670
funded by European community in the 7th

00:10:49,450 --> 00:10:53,920
Framework Programme it costs the 3

00:10:51,670 --> 00:10:57,190
million euros ax and 2 years and half to

00:10:53,920 --> 00:10:58,990
develop it started with the four people

00:10:57,190 --> 00:11:03,220
team including me for the very first

00:10:58,990 --> 00:11:05,590
release it's currently its open software

00:11:03,220 --> 00:11:07,930
so you find it on github and it's

00:11:05,590 --> 00:11:10,000
evolved in the oppor and operated as a

00:11:07,930 --> 00:11:13,630
service by translated which is the

00:11:10,000 --> 00:11:16,360
company that took the tender to develop

00:11:13,630 --> 00:11:20,830
this technology and so now I'm showing

00:11:16,360 --> 00:11:22,870
you this so today I'm bringing you here

00:11:20,830 --> 00:11:24,340
there is the hosted version but the

00:11:22,870 --> 00:11:26,500
hosted version is not really interesting

00:11:24,340 --> 00:11:28,540
so today we are deploying that on our

00:11:26,500 --> 00:11:30,250
laptop in real time and this is the

00:11:28,540 --> 00:11:31,360
first thing I am bringing here which is

00:11:30,250 --> 00:11:35,560
a docker midgut

00:11:31,360 --> 00:11:37,360
and it is actually Arya packaging a

00:11:35,560 --> 00:11:39,400
proper packaging of this tool as a

00:11:37,360 --> 00:11:43,240
docker container so it's easy to deploy

00:11:39,400 --> 00:11:47,560
that so it's really easy you just have

00:11:43,240 --> 00:11:50,190
to clone and issue these really small

00:11:47,560 --> 00:11:53,470
commands so in this case now we are

00:11:50,190 --> 00:11:58,150
whoops let's start bringing up the my

00:11:53,470 --> 00:12:02,100
sequel container let's then bring up

00:11:58,150 --> 00:12:05,520
let's initialize the Massacre container

00:12:02,100 --> 00:12:08,530
testing if the container is connected

00:12:05,520 --> 00:12:14,080
and there we go ok

00:12:08,530 --> 00:12:17,340
everything is up to so my keyboard just

00:12:14,080 --> 00:12:21,340
switch to English thanks to them effect

00:12:17,340 --> 00:12:26,890
and then we bring me up the rest of the

00:12:21,340 --> 00:12:28,690
tool I started with some settings but

00:12:26,890 --> 00:12:31,000
you are free to customize everything I

00:12:28,690 --> 00:12:32,590
try to move everything from the build

00:12:31,000 --> 00:12:35,830
time after run time so you can just

00:12:32,590 --> 00:12:36,680
change your end time variable and to

00:12:35,830 --> 00:12:39,649
change

00:12:36,680 --> 00:12:41,860
accordingly the only thing which is

00:12:39,649 --> 00:12:46,339
really nasty it can be improved here

00:12:41,860 --> 00:12:47,930
actually is that it it expects you to it

00:12:46,339 --> 00:12:50,120
runs only on Chrome but that is by

00:12:47,930 --> 00:12:53,209
design okay but it expects you to have

00:12:50,120 --> 00:12:56,750
the dev Metacom for a matter of cookies

00:12:53,209 --> 00:12:58,580
and that were to find the API domains

00:12:56,750 --> 00:13:01,100
that these are Dwyer din a configuration

00:12:58,580 --> 00:13:02,660
file and Yuki I will make sure you can

00:13:01,100 --> 00:13:05,029
change your runtime in a later version

00:13:02,660 --> 00:13:09,529
but for now you just add the dev meet

00:13:05,029 --> 00:13:11,089
Capcom into your local lost because your

00:13:09,529 --> 00:13:15,140
PC must know where it is

00:13:11,089 --> 00:13:18,709
and is the catch-all interface so here

00:13:15,140 --> 00:13:20,570
you can upload anything and I'm studying

00:13:18,709 --> 00:13:23,360
with this docker and kubernetes

00:13:20,570 --> 00:13:29,570
presentation which is a file in english

00:13:23,360 --> 00:13:31,610
about docker and cuban Andes and let's

00:13:29,570 --> 00:13:34,399
just load the know that it's a oops

00:13:31,610 --> 00:13:38,890
sorry this is my fault because I forgot

00:13:34,399 --> 00:13:43,089
one thing to change the permissions

00:13:38,890 --> 00:13:43,089
otherwise they are complaining

00:13:47,779 --> 00:13:52,519
so the file has been uploaded now it's

00:13:50,120 --> 00:13:55,339
being converted to this intermediate

00:13:52,519 --> 00:13:58,480
presentation these supports over 72 file

00:13:55,339 --> 00:14:05,750
formats so all the offices stuff strings

00:13:58,480 --> 00:14:07,220
CAD is a really big array of formats it

00:14:05,750 --> 00:14:11,689
does the conversion for you and packs

00:14:07,220 --> 00:14:16,129
back so it's really nice now we start

00:14:11,689 --> 00:14:19,040
analyzing and of course I you can sign

00:14:16,129 --> 00:14:22,930
in with the Google account I put my

00:14:19,040 --> 00:14:25,610
develop my development keys in here and

00:14:22,930 --> 00:14:29,629
now it's a complaining that these up is

00:14:25,610 --> 00:14:29,930
not verified the urban environment don't

00:14:29,629 --> 00:14:32,750
worry

00:14:29,930 --> 00:14:35,720
a nice feature that this tool has is

00:14:32,750 --> 00:14:38,389
that it can scavenger for you your Docs

00:14:35,720 --> 00:14:40,160
from Google Drive and have them

00:14:38,389 --> 00:14:42,620
translated and re-upload it back to your

00:14:40,160 --> 00:14:45,379
drive which is really handy now it's

00:14:42,620 --> 00:14:48,560
analyzing all the all the words in order

00:14:45,379 --> 00:14:50,329
to show to find for duplicate the

00:14:48,560 --> 00:14:54,019
content so you don't have to translate

00:14:50,329 --> 00:14:56,329
something twice and for translate to for

00:14:54,019 --> 00:15:02,240
matches into a translation memory which

00:14:56,329 --> 00:15:04,129
I add and we just open the workbench so

00:15:02,240 --> 00:15:06,290
here you can see I've partially

00:15:04,129 --> 00:15:08,660
translated some stuff so it's locked I

00:15:06,290 --> 00:15:10,819
cannot touch it until i unlock the

00:15:08,660 --> 00:15:13,610
string but never mind because there are

00:15:10,819 --> 00:15:16,399
other strings here so for example I

00:15:13,610 --> 00:15:18,350
click here and I get a suggestion from a

00:15:16,399 --> 00:15:20,930
machine translation server which is in

00:15:18,350 --> 00:15:26,240
this case is going translate so we can

00:15:20,930 --> 00:15:27,829
just accept the translation or not oh so

00:15:26,240 --> 00:15:31,819
the new containers we don't translate

00:15:27,829 --> 00:15:33,550
the containers in Italian then we

00:15:31,819 --> 00:15:36,949
translate and it goes to the next one

00:15:33,550 --> 00:15:41,870
see here we have the tags these tags are

00:15:36,949 --> 00:15:43,459
the formatting of this PowerPoint so the

00:15:41,870 --> 00:15:46,040
problem is that this string has been

00:15:43,459 --> 00:15:47,809
form as been is in the middle of a tag

00:15:46,040 --> 00:15:52,699
which maybe is bolded or maybe the

00:15:47,809 --> 00:15:54,199
italic and I have to make sure that it's

00:15:52,699 --> 00:15:56,480
the same so for example spring you

00:15:54,199 --> 00:15:59,839
deploy your spring application spring

00:15:56,480 --> 00:16:02,800
these cases Beatriz SP area but is

00:15:59,839 --> 00:16:02,800
absolutely wrong

00:16:09,070 --> 00:16:18,410
and in this case spring application are

00:16:15,110 --> 00:16:20,690
in between the two different tags to

00:16:18,410 --> 00:16:26,000
nest the tags so what I'm doing here is

00:16:20,690 --> 00:16:33,550
I'm living there as it is so it's trying

00:16:26,000 --> 00:16:38,029
to guess all by itself come on okay

00:16:33,550 --> 00:16:40,790
accept it and I can go over and over

00:16:38,029 --> 00:16:44,089
over until all the strings have been

00:16:40,790 --> 00:16:47,350
translated there nice place to store

00:16:44,089 --> 00:16:49,970
your images okay fine

00:16:47,350 --> 00:16:53,380
anytime I can download the preview of

00:16:49,970 --> 00:16:56,600
how is it going and it's just

00:16:53,380 --> 00:17:00,050
downloading the file and I have an open

00:16:56,600 --> 00:17:02,750
it you can see has been translated

00:17:00,050 --> 00:17:04,670
Italian why all the formatting has been

00:17:02,750 --> 00:17:13,959
preserved which is pretty much

00:17:04,670 --> 00:17:13,959
impressive and there we go

00:17:16,490 --> 00:17:23,910
so a critic of all is good but for the

00:17:21,540 --> 00:17:26,130
most of my time here being relying on

00:17:23,910 --> 00:17:28,260
machine translation server to suggest me

00:17:26,130 --> 00:17:30,180
the correct sentences this because I

00:17:28,260 --> 00:17:32,250
just I had to accept or edit the

00:17:30,180 --> 00:17:35,370
suggestions this gives me a big

00:17:32,250 --> 00:17:37,080
opportunity boost but no matter how much

00:17:35,370 --> 00:17:40,050
data we translate we never have enough

00:17:37,080 --> 00:17:42,000
memories to reuse for the new project

00:17:40,050 --> 00:17:44,820
which is coming so machine translation

00:17:42,000 --> 00:17:48,930
system is here to actually fill the gaps

00:17:44,820 --> 00:17:51,150
which are always more than you have

00:17:48,930 --> 00:17:52,890
machine translation are machine learning

00:17:51,150 --> 00:17:55,410
systems they are trained at over

00:17:52,890 --> 00:17:59,010
datasets which are named parallel corpus

00:17:55,410 --> 00:18:02,250
or corpora parallel corpora serve as a

00:17:59,010 --> 00:18:04,710
be directional label data set objection

00:18:02,250 --> 00:18:07,170
a parallel corpora is a very very very

00:18:04,710 --> 00:18:09,060
long list of sentences in one language

00:18:07,170 --> 00:18:11,700
for example English and then a

00:18:09,060 --> 00:18:13,680
corresponding list of those sentences in

00:18:11,700 --> 00:18:15,930
the foreign language I'm trying to

00:18:13,680 --> 00:18:18,060
translate for example Italian so I have

00:18:15,930 --> 00:18:21,140
1 million centers in English and 1

00:18:18,060 --> 00:18:24,630
million same sentences translated in

00:18:21,140 --> 00:18:26,940
Italian and it serves as a big new

00:18:24,630 --> 00:18:29,610
actual label at the data set because I

00:18:26,940 --> 00:18:31,560
show an English sentence and then I show

00:18:29,610 --> 00:18:33,450
the corresponding Italian sentence but I

00:18:31,560 --> 00:18:36,000
can do in the opposite direction I want

00:18:33,450 --> 00:18:39,420
to start from Italian to English I start

00:18:36,000 --> 00:18:41,880
with the Italian string and I show as a

00:18:39,420 --> 00:18:47,550
label what what you should come up with

00:18:41,880 --> 00:18:50,520
the English string the problem with this

00:18:47,550 --> 00:18:52,410
thing is that you have to feed this

00:18:50,520 --> 00:18:55,140
machine learning server you have to come

00:18:52,410 --> 00:18:58,530
up with a lot of data and this has been

00:18:55,140 --> 00:19:01,320
particularly made worse by the advantage

00:18:58,530 --> 00:19:03,810
of the narrow technologies the neural

00:19:01,320 --> 00:19:07,590
machine translation since it's a neural

00:19:03,810 --> 00:19:11,790
network it requires a lot of data which

00:19:07,590 --> 00:19:15,270
in the hundreds of millions of are I

00:19:11,790 --> 00:19:17,220
need sentences ok a lot a lot a lot of

00:19:15,270 --> 00:19:19,950
stuff so since the technology is pretty

00:19:17,220 --> 00:19:21,810
much the same for all the players the

00:19:19,950 --> 00:19:25,890
winner is the one who has more data

00:19:21,810 --> 00:19:28,809
which namely is Google always that is

00:19:25,890 --> 00:19:30,220
the only really see

00:19:28,809 --> 00:19:32,139
use provider machine translation

00:19:30,220 --> 00:19:35,080
technology all the others are niche

00:19:32,139 --> 00:19:37,659
players will pretend to be very good but

00:19:35,080 --> 00:19:40,629
Google Translate actually is the

00:19:37,659 --> 00:19:43,929
top-notch player they start first they

00:19:40,629 --> 00:19:46,629
crawled a lot of data they align add a

00:19:43,929 --> 00:19:48,999
lot of data automatically or with manual

00:19:46,629 --> 00:19:53,769
effort to bootstrap then they really

00:19:48,999 --> 00:19:58,690
came up with the billions of words so

00:19:53,769 --> 00:20:00,999
the problem is that we we have to find

00:19:58,690 --> 00:20:03,429
this data there are efforts to procure a

00:20:00,999 --> 00:20:06,580
parallel data for free which is the

00:20:03,429 --> 00:20:08,679
opposite or spyler corpus it's a

00:20:06,580 --> 00:20:11,110
collection an open collection of

00:20:08,679 --> 00:20:13,690
parallel corpora which is grow every day

00:20:11,110 --> 00:20:15,610
for example they crawl pages in

00:20:13,690 --> 00:20:18,249
different languages and align them or

00:20:15,610 --> 00:20:21,159
for example they take books and they

00:20:18,249 --> 00:20:22,360
align the sentences the even though they

00:20:21,159 --> 00:20:24,909
are not exact the same translation

00:20:22,360 --> 00:20:28,059
because the literal translation is not

00:20:24,909 --> 00:20:30,009
really the same for example you know a

00:20:28,059 --> 00:20:34,470
book which is really really really easy

00:20:30,009 --> 00:20:38,559
to align the Bible the Bible is as

00:20:34,470 --> 00:20:41,619
notation for all the verses and all the

00:20:38,559 --> 00:20:44,919
different chapters is the most aligned a

00:20:41,619 --> 00:20:46,720
book or book in the world and it has all

00:20:44,919 --> 00:20:51,759
the languages in the world so although

00:20:46,720 --> 00:20:54,330
it's a way of writing is a little old it

00:20:51,759 --> 00:20:57,429
works very very well

00:20:54,330 --> 00:21:00,369
but the problem is that we have enough

00:20:57,429 --> 00:21:02,110
data to come up with a decent transition

00:21:00,369 --> 00:21:04,360
system because we can just use this

00:21:02,110 --> 00:21:06,129
stuff there are millions and tens of

00:21:04,360 --> 00:21:07,869
millions of sentences in this open

00:21:06,129 --> 00:21:09,190
corpora but just for FIGS

00:21:07,869 --> 00:21:12,129
which is French Italian German and

00:21:09,190 --> 00:21:15,909
Spanish if you have another language for

00:21:12,129 --> 00:21:18,460
example Norwegian or Sudanese you have

00:21:15,909 --> 00:21:20,230
no way of coming up with a decent

00:21:18,460 --> 00:21:23,259
machine translation system you just have

00:21:20,230 --> 00:21:27,879
not enough resources not enough parallel

00:21:23,259 --> 00:21:31,029
resources so we are stuck at that point

00:21:27,879 --> 00:21:33,820
because the technology we have is a very

00:21:31,029 --> 00:21:36,610
good to come up with the translations

00:21:33,820 --> 00:21:39,309
but only for a selected amount of

00:21:36,610 --> 00:21:41,410
languages and since those languages are

00:21:39,309 --> 00:21:43,990
those who can

00:21:41,410 --> 00:21:47,860
benefit from the presence of a

00:21:43,990 --> 00:21:50,860
translation industry there more and more

00:21:47,860 --> 00:21:53,560
resources are created in those languages

00:21:50,860 --> 00:21:56,890
because more and more documents and the

00:21:53,560 --> 00:21:58,900
interactions are produced and and so the

00:21:56,890 --> 00:22:03,220
more and more data they had it's a

00:21:58,900 --> 00:22:06,580
virtuous circle and the same can doesn't

00:22:03,220 --> 00:22:10,710
happen actually for the region Italian

00:22:06,580 --> 00:22:10,710
region for example or Japanese

00:22:10,950 --> 00:22:16,650
a bike because there's no interaction

00:22:14,700 --> 00:22:18,690
with the Japanese culture and black

00:22:16,650 --> 00:22:21,630
culture so you don't have aniline at the

00:22:18,690 --> 00:22:23,690
data set so you say okay what if we have

00:22:21,630 --> 00:22:27,090
the Japanese English and the English

00:22:23,690 --> 00:22:28,620
Hebrew we can do like a people we

00:22:27,090 --> 00:22:31,500
translate from Japanese to English and

00:22:28,620 --> 00:22:34,710
then from English to a bureau yes you

00:22:31,500 --> 00:22:38,429
and you lose a lot of fidelity because

00:22:34,710 --> 00:22:40,380
it's like we have Italy we have a game I

00:22:38,429 --> 00:22:43,049
don't know how you call that broad is

00:22:40,380 --> 00:22:45,419
the cordless phone in which you say a

00:22:43,049 --> 00:22:46,830
sentence into an air to somebody else

00:22:45,419 --> 00:22:48,870
and then the other has to tell to

00:22:46,830 --> 00:22:51,630
another in a ring of people and then at

00:22:48,870 --> 00:22:53,700
the end that you have to just do you

00:22:51,630 --> 00:22:56,130
have fun knowing what is the sentence

00:22:53,700 --> 00:22:57,510
that has arrived doing a pivoting in

00:22:56,130 --> 00:22:59,640
machine translation is really the same

00:22:57,510 --> 00:23:01,770
thing so you're just taking the output

00:22:59,640 --> 00:23:05,159
of a machine system and feeding it into

00:23:01,770 --> 00:23:07,080
the another machine which will add a lot

00:23:05,159 --> 00:23:09,330
of distortion and so you will not we

00:23:07,080 --> 00:23:13,169
will not be very happy because your

00:23:09,330 --> 00:23:15,179
translation will suck so let's just

00:23:13,169 --> 00:23:19,650
maybe a maybe since we are in a dead end

00:23:15,179 --> 00:23:20,909
we may take a step back the dead end

00:23:19,650 --> 00:23:24,289
comes from the fact that we have a

00:23:20,909 --> 00:23:27,120
supervised system the needs labeled data

00:23:24,289 --> 00:23:28,950
what do we just go with unsupervised

00:23:27,120 --> 00:23:32,220
training unsupervised training is a

00:23:28,950 --> 00:23:34,380
particular kind of a class of machine

00:23:32,220 --> 00:23:38,340
learning system which is not concerned

00:23:34,380 --> 00:23:40,470
with finding correlations between data

00:23:38,340 --> 00:23:44,280
and the label but finding a hidden

00:23:40,470 --> 00:23:47,820
structure into a corpus which is not as

00:23:44,280 --> 00:23:49,770
nullable so in this case we don't need

00:23:47,820 --> 00:23:51,419
the parallel examples to learn language

00:23:49,770 --> 00:23:55,850
it's not the way humans actually learn

00:23:51,419 --> 00:23:58,679
language so we learn Italian when we are

00:23:55,850 --> 00:24:01,169
we are our mother tongue language and

00:23:58,679 --> 00:24:03,539
then we learn another language but

00:24:01,169 --> 00:24:06,299
separated from Italian it's not that

00:24:03,539 --> 00:24:09,780
they just relentlessly show us examples

00:24:06,299 --> 00:24:11,940
of all the lore that we can come up with

00:24:09,780 --> 00:24:13,770
in our mother tongue language and they

00:24:11,940 --> 00:24:16,620
just throw it out the English

00:24:13,770 --> 00:24:18,659
translations for me I'm Italian until I

00:24:16,620 --> 00:24:22,380
learned English I just learned the

00:24:18,659 --> 00:24:23,670
English separately and then I started

00:24:22,380 --> 00:24:25,710
doing my mapping

00:24:23,670 --> 00:24:27,270
- you - languages and we can do actually

00:24:25,710 --> 00:24:29,720
the same how about learning two

00:24:27,270 --> 00:24:32,640
languages and then try to map between

00:24:29,720 --> 00:24:35,700
concepts which is easier because it's

00:24:32,640 --> 00:24:39,230
easy for me just to get a very very vast

00:24:35,700 --> 00:24:42,870
corpus of just Italian just a Norwegian

00:24:39,230 --> 00:24:44,910
learn those independently learn a model

00:24:42,870 --> 00:24:47,820
of how those languages are structured

00:24:44,910 --> 00:24:50,700
and then try to put some links some

00:24:47,820 --> 00:24:53,400
bridges in between when I master the two

00:24:50,700 --> 00:24:54,960
languages the point is that we are

00:24:53,400 --> 00:24:58,020
dealing with a machine so in order to

00:24:54,960 --> 00:25:00,060
map within between languages a computer

00:24:58,020 --> 00:25:02,820
needs to build a representation of that

00:25:00,060 --> 00:25:06,780
language and how can that be

00:25:02,820 --> 00:25:08,850
accomplished actually we may use

00:25:06,780 --> 00:25:12,630
language models language models are

00:25:08,850 --> 00:25:14,790
technology that allows a machine to come

00:25:12,630 --> 00:25:18,510
up with a structure our hidden structure

00:25:14,790 --> 00:25:21,650
because we're just showing it data

00:25:18,510 --> 00:25:24,990
samples and not teaching the rules and

00:25:21,650 --> 00:25:27,600
builds a model how a word the relates to

00:25:24,990 --> 00:25:31,020
another and what embedding is I think

00:25:27,600 --> 00:25:32,820
it's a technique that map's every word

00:25:31,020 --> 00:25:35,820
into a vector space

00:25:32,820 --> 00:25:38,190
remember geometry at school you have all

00:25:35,820 --> 00:25:41,040
these shapes in 3d environments which

00:25:38,190 --> 00:25:45,420
are vectors so these are an array of

00:25:41,040 --> 00:25:47,600
numbers and we can when the numbers are

00:25:45,420 --> 00:25:50,610
close to vectors are close in space

00:25:47,600 --> 00:25:55,860
words with near meanings will have near

00:25:50,610 --> 00:26:00,660
vectors in that space so for example I

00:25:55,860 --> 00:26:03,900
don't know Paris and Rome will be near

00:26:00,660 --> 00:26:06,540
in the space of the capitals because

00:26:03,900 --> 00:26:08,820
Paris will be in Rome would be very far

00:26:06,540 --> 00:26:12,300
from Milan for example because Milan is

00:26:08,820 --> 00:26:15,720
not a capital city or anything so Paris

00:26:12,300 --> 00:26:17,880
Rome Berlin would be very near and Milan

00:26:15,720 --> 00:26:20,610
maybe will be near Frankfurt among the

00:26:17,880 --> 00:26:26,000
Spay the dimension of all the cities

00:26:20,610 --> 00:26:29,310
were not capital to anything boy and the

00:26:26,000 --> 00:26:32,280
man will be near because they will be on

00:26:29,310 --> 00:26:34,580
the dimension of the type of sex you

00:26:32,280 --> 00:26:36,590
will be a dimensional just too

00:26:34,580 --> 00:26:39,230
two numbers actually like a binary

00:26:36,590 --> 00:26:41,990
dimension but anyway we have these very

00:26:39,230 --> 00:26:44,960
big vectors of 300 dimensions and we

00:26:41,990 --> 00:26:47,300
come up with a way to map from the world

00:26:44,960 --> 00:26:49,040
to disagree stuff and we can do even

00:26:47,300 --> 00:26:51,680
crazy things in theses vectors and

00:26:49,040 --> 00:26:53,660
numbers they actually allow us to do

00:26:51,680 --> 00:26:56,030
computations like you take the vector of

00:26:53,660 --> 00:26:59,720
Paris you subtract the vector of France

00:26:56,030 --> 00:27:02,600
and there are numbers you don't add the

00:26:59,720 --> 00:27:04,790
vector Italy and what you get is a

00:27:02,600 --> 00:27:08,690
vector which is almost the same as the

00:27:04,790 --> 00:27:11,300
vector of Rome and this means that if

00:27:08,690 --> 00:27:14,630
you we analyze enough sentences in one

00:27:11,300 --> 00:27:17,360
language we start to develop a very

00:27:14,630 --> 00:27:19,580
structured model or how that language

00:27:17,360 --> 00:27:25,360
works internally which is a language

00:27:19,580 --> 00:27:30,470
model our language model we need so we

00:27:25,360 --> 00:27:32,900
could induce a parallel corpus between

00:27:30,470 --> 00:27:36,320
the two independent languages by just

00:27:32,900 --> 00:27:38,690
mapping concepts between spaces in Latin

00:27:36,320 --> 00:27:40,340
spaces and we can bootstrap with a

00:27:38,690 --> 00:27:43,190
vocabulary for example we have

00:27:40,340 --> 00:27:46,190
vocabulary for Italian and Norwegian and

00:27:43,190 --> 00:27:49,220
so we know which some words translate

00:27:46,190 --> 00:27:51,140
into another and you can stop you can

00:27:49,220 --> 00:27:53,090
just bootstrap from them and then you

00:27:51,140 --> 00:27:56,230
can start mapping similar concepts or

00:27:53,090 --> 00:28:00,950
you can just use unique entities and

00:27:56,230 --> 00:28:03,640
frequencies so the the article is the

00:28:00,950 --> 00:28:06,290
very same frequency as the Italian

00:28:03,640 --> 00:28:10,070
corresponding article which is ill so

00:28:06,290 --> 00:28:12,500
you can hum as the very same frequency I

00:28:10,070 --> 00:28:16,490
can expect in a corpus as Casa Italian

00:28:12,500 --> 00:28:19,100
so we can just use this as eristic to

00:28:16,490 --> 00:28:24,650
guide our mapping in this very big Latin

00:28:19,100 --> 00:28:26,800
space so a very legit question is one

00:28:24,650 --> 00:28:29,450
the result be really really really noisy

00:28:26,800 --> 00:28:32,570
because you will end up with a lot a lot

00:28:29,450 --> 00:28:35,480
a lot of false positives yes but we

00:28:32,570 --> 00:28:40,970
could use statistics to compute means

00:28:35,480 --> 00:28:44,600
and among how many times house seems to

00:28:40,970 --> 00:28:48,170
align with dog or to casa

00:28:44,600 --> 00:28:51,950
and inferred the true positive throwing

00:28:48,170 --> 00:28:53,270
away all the false positives and here

00:28:51,950 --> 00:28:54,980
comes the phrase base machine

00:28:53,270 --> 00:28:56,990
translation which is which is the old

00:28:54,980 --> 00:28:59,060
technology there was a completely scrap

00:28:56,990 --> 00:29:01,250
the way when we came up with neural

00:28:59,060 --> 00:29:04,910
machine translation the first base

00:29:01,250 --> 00:29:07,490
machine translation had revolved around

00:29:04,910 --> 00:29:12,470
the concept that coal currencies of

00:29:07,490 --> 00:29:14,600
words are significantly our statistical

00:29:12,470 --> 00:29:18,020
significance sorry for example you see

00:29:14,600 --> 00:29:21,770
this character here what do you think is

00:29:18,020 --> 00:29:23,450
the translation of that character shrimp

00:29:21,770 --> 00:29:27,470
because it's the only character there of

00:29:23,450 --> 00:29:30,740
course always so you can assume that is

00:29:27,470 --> 00:29:37,010
the transition of shrimp and this is

00:29:30,740 --> 00:29:39,500
somewhat Berkeley if you happen to meet

00:29:37,010 --> 00:29:41,380
broccoli again in a world and you know

00:29:39,500 --> 00:29:44,300
the sentence and you just see this one

00:29:41,380 --> 00:29:46,940
maybe this is broccoli and these are a

00:29:44,300 --> 00:29:50,270
particle like something some connective

00:29:46,940 --> 00:29:53,090
that it's used it's a peculiar to the

00:29:50,270 --> 00:29:55,910
Chinese language and this is the way we

00:29:53,090 --> 00:29:58,340
used to train the old Google Translate

00:29:55,910 --> 00:30:00,020
now where Translate is narrow by in the

00:29:58,340 --> 00:30:02,630
old times it was a based on this

00:30:00,020 --> 00:30:04,190
technology which is a toolkit an

00:30:02,630 --> 00:30:07,300
open-source toolkit of course which is

00:30:04,190 --> 00:30:09,650
called Moses and has been developed by

00:30:07,300 --> 00:30:10,870
University of Toronto University of

00:30:09,650 --> 00:30:14,420
Edinburgh

00:30:10,870 --> 00:30:18,500
so the co-occurrences counts between

00:30:14,420 --> 00:30:21,860
words and sequence of words because I've

00:30:18,500 --> 00:30:24,910
been at school school can align with

00:30:21,860 --> 00:30:27,140
squalor in Italian but also I have been

00:30:24,910 --> 00:30:30,500
it's a sentence days alway always

00:30:27,140 --> 00:30:32,210
translates in sono stato in Italian so I

00:30:30,500 --> 00:30:34,850
can always count on only the

00:30:32,210 --> 00:30:37,130
co-occurrences of words school squad ah

00:30:34,850 --> 00:30:39,440
but also I've been so instead Oh

00:30:37,130 --> 00:30:41,990
and so I count the phrases and the

00:30:39,440 --> 00:30:44,600
phrases I treat them as a unit of

00:30:41,990 --> 00:30:48,050
translation and that is subject to call

00:30:44,600 --> 00:30:49,700
currencies counting too so I can use

00:30:48,050 --> 00:30:52,550
those who to calculate the translation

00:30:49,700 --> 00:30:55,030
probabilities a probability is just the

00:30:52,550 --> 00:30:59,150
idea that the seven times out of ten

00:30:55,030 --> 00:31:02,000
Casas is observed to koku with the house

00:30:59,150 --> 00:31:03,500
and that's a probability so I can just

00:31:02,000 --> 00:31:06,080
come up with the probabilities and

00:31:03,500 --> 00:31:09,980
choose the most probable sentence among

00:31:06,080 --> 00:31:11,960
all those who get to be aligned this

00:31:09,980 --> 00:31:13,700
probabilities are always I'm notated in

00:31:11,960 --> 00:31:15,830
a database a special database which is

00:31:13,700 --> 00:31:18,080
called phrase table creating a phrase

00:31:15,830 --> 00:31:21,500
table is the most expensive operation

00:31:18,080 --> 00:31:22,970
office-based machine translation so the

00:31:21,500 --> 00:31:24,800
most of China is based on three

00:31:22,970 --> 00:31:26,930
technologies which is the language model

00:31:24,800 --> 00:31:28,310
yes Telemark NLM is the language model

00:31:26,930 --> 00:31:30,740
calculates the probabilities of a

00:31:28,310 --> 00:31:32,960
sentence being meaningful then we have

00:31:30,740 --> 00:31:35,180
the aligner which is Gita and later was

00:31:32,960 --> 00:31:36,860
superseded by fast align and Moses which

00:31:35,180 --> 00:31:39,320
decodes the incoming message

00:31:36,860 --> 00:31:41,120
Moses projects the input sentence over

00:31:39,320 --> 00:31:43,100
phrase table to retrieve translation

00:31:41,120 --> 00:31:45,350
options such as among all the different

00:31:43,100 --> 00:31:47,750
options guided by the language model as

00:31:45,350 --> 00:31:50,650
a heuristic and that stops by itself

00:31:47,750 --> 00:31:54,020
when all input sentence has been covered

00:31:50,650 --> 00:31:56,840
and today we are presenting the second

00:31:54,020 --> 00:31:59,690
technologies which is Manos s mores as

00:31:56,840 --> 00:32:02,960
either stems from the paper from these

00:31:59,690 --> 00:32:05,420
three scientists and it's basically a

00:32:02,960 --> 00:32:07,760
toolkit to create a phrase tables from

00:32:05,420 --> 00:32:11,600
two more linguae datasets through word

00:32:07,760 --> 00:32:14,930
embedding then it creates a Moses model

00:32:11,600 --> 00:32:16,820
with this noisy thing does some fine

00:32:14,930 --> 00:32:21,440
tuning in the iteratively augment the

00:32:16,820 --> 00:32:23,810
data set by translating itself and so

00:32:21,440 --> 00:32:26,300
you can start with 1 million sentences

00:32:23,810 --> 00:32:28,520
and you rapidly go to 10 million

00:32:26,300 --> 00:32:31,280
sentences because you try different

00:32:28,520 --> 00:32:33,800
combinations between sentences that you

00:32:31,280 --> 00:32:37,190
have started with it's noisy

00:32:33,800 --> 00:32:40,490
sure but the sheer amount of data do you

00:32:37,190 --> 00:32:44,330
come up with ends up averaging out the

00:32:40,490 --> 00:32:48,620
noise in the long run and this is what I

00:32:44,330 --> 00:32:52,820
brought today so the second demo that

00:32:48,620 --> 00:32:55,070
I'm showing you today is Moses my going

00:32:52,820 --> 00:32:58,570
to be my small contribution has been a

00:32:55,070 --> 00:33:01,100
packaging this really really wild

00:32:58,570 --> 00:33:03,560
research prototype into a docker

00:33:01,100 --> 00:33:07,940
formatter so it's easy to deploy it's

00:33:03,560 --> 00:33:11,009
easy to use you have the

00:33:07,940 --> 00:33:14,789
the pipeline to train so you just take

00:33:11,009 --> 00:33:16,889
your two big text files with all Italian

00:33:14,789 --> 00:33:19,440
sentences and all Norwegian sentences

00:33:16,889 --> 00:33:20,610
for example and this sentence must not

00:33:19,440 --> 00:33:22,559
be related to each other

00:33:20,610 --> 00:33:25,320
you just take Italian Wikipedia and

00:33:22,559 --> 00:33:27,659
Norwegian Wikipedia or all the news to

00:33:25,320 --> 00:33:29,850
set up for the region for example and

00:33:27,659 --> 00:33:31,289
you stash in two separate files they

00:33:29,850 --> 00:33:33,960
don't have to be parallel they have to

00:33:31,289 --> 00:33:38,759
be monolingual then you launch the

00:33:33,960 --> 00:33:40,289
training passes a week and when the

00:33:38,759 --> 00:33:42,360
process is finished you will have a

00:33:40,289 --> 00:33:44,190
several gigabytes of files inside this

00:33:42,360 --> 00:33:46,980
train directory which is the models the

00:33:44,190 --> 00:33:49,980
phrase tables and you cannot shoot this

00:33:46,980 --> 00:33:51,480
translation server with the full with

00:33:49,980 --> 00:33:53,220
the following syntax you have to point

00:33:51,480 --> 00:33:55,919
it into the director of the model and

00:33:53,220 --> 00:33:58,139
you can now query your server with the

00:33:55,919 --> 00:33:59,369
following so you have this API which is

00:33:58,139 --> 00:34:01,499
another thing that I've built

00:33:59,369 --> 00:34:05,820
because Moses has no API server let's

00:34:01,499 --> 00:34:09,060
say so I built a flask based Python HTTP

00:34:05,820 --> 00:34:13,200
API please note that you have the verb

00:34:09,060 --> 00:34:16,619
here the query which is over in this

00:34:13,200 --> 00:34:19,859
case and the source language

00:34:16,619 --> 00:34:22,349
did I say Norwegian Swedish yes this

00:34:19,859 --> 00:34:25,230
because Moses doesn't have a Norwegian

00:34:22,349 --> 00:34:27,569
tokenizer to analyze the sentence but

00:34:25,230 --> 00:34:31,109
Swedish is really close so I used the

00:34:27,569 --> 00:34:33,810
Swedish tokenizer sorry but and the

00:34:31,109 --> 00:34:36,780
target in Italian and since this server

00:34:33,810 --> 00:34:42,839
is actually online I can also provide

00:34:36,780 --> 00:34:45,119
the working demo here it's really really

00:34:42,839 --> 00:34:48,089
really really slow because in this first

00:34:45,119 --> 00:34:50,369
version it tries to load the model in

00:34:48,089 --> 00:34:52,619
memory each time it's rather than the

00:34:50,369 --> 00:34:54,480
memory thrown away you take another

00:34:52,619 --> 00:34:57,089
sentence either loaded in memory at a

00:34:54,480 --> 00:35:00,720
time it will burn is just a show of

00:34:57,089 --> 00:35:07,079
walvis looping and these are JSON output

00:35:00,720 --> 00:35:11,400
though you have here so now I'm in order

00:35:07,079 --> 00:35:17,569
to show you how this thing work I have

00:35:11,400 --> 00:35:17,569
to go here and then

00:35:19,310 --> 00:35:28,380
select Swedish then I go here I try to

00:35:26,700 --> 00:35:31,320
disable the lookups because I'm

00:35:28,380 --> 00:35:35,730
purposely fully using a brown language I

00:35:31,320 --> 00:35:38,460
have to fix this someday so I don't get

00:35:35,730 --> 00:35:41,070
suggestions for the Swedish and then I

00:35:38,460 --> 00:35:44,600
add them Tianjin I chooses the provider

00:35:41,070 --> 00:35:47,970
Moses because I try to imitate the Moses

00:35:44,600 --> 00:35:51,930
HTTP / with the API I try to just

00:35:47,970 --> 00:35:57,300
implement it I add the whoops and rename

00:35:51,930 --> 00:36:01,560
which is mongooses okay and is the

00:35:57,300 --> 00:36:06,120
server I keep you just type a dummy you

00:36:01,560 --> 00:36:13,200
don't need it okay now we have to look

00:36:06,120 --> 00:36:17,100
for a file which I have somewhere and

00:36:13,200 --> 00:36:26,760
actually forgot where I stored it which

00:36:17,100 --> 00:36:29,370
is embarrassing and I think I what files

00:36:26,760 --> 00:36:32,460
okay I know I have a region file right

00:36:29,370 --> 00:36:36,390
now to show you but the idea is that you

00:36:32,460 --> 00:36:39,170
will end up with a very very very slow

00:36:36,390 --> 00:36:43,110
server that will actually provide

00:36:39,170 --> 00:36:45,960
translations to you not in the Google

00:36:43,110 --> 00:36:49,440
Translate speed very wise lower fashion

00:36:45,960 --> 00:36:51,990
and the translation are not even perfect

00:36:49,440 --> 00:36:54,840
but are good for a starting point to

00:36:51,990 --> 00:36:56,460
start translating your software and the

00:36:54,840 --> 00:36:59,400
advantage of these is that you don't

00:36:56,460 --> 00:37:02,130
even if you are Norwegian or Sudanese so

00:36:59,400 --> 00:37:06,360
you have a very very small presence

00:37:02,130 --> 00:37:10,530
online in terms of amount of data

00:37:06,360 --> 00:37:12,630
produced and parallel data produced with

00:37:10,530 --> 00:37:17,790
this kind of technology you can just

00:37:12,630 --> 00:37:22,490
throw it throw any enough data from

00:37:17,790 --> 00:37:26,880
books from articles from cooking recipes

00:37:22,490 --> 00:37:29,700
from subtitles of movies and that you

00:37:26,880 --> 00:37:30,520
have them a lot of if you go in Abu so

00:37:29,700 --> 00:37:32,650
the parallel

00:37:30,520 --> 00:37:35,140
corpora and you take all only the

00:37:32,650 --> 00:37:40,260
monolingual part that you are interest

00:37:35,140 --> 00:37:45,820
into then you might be able to actually

00:37:40,260 --> 00:37:48,220
build a very huge model proof is that

00:37:45,820 --> 00:37:50,380
I've launched the trade the the the

00:37:48,220 --> 00:37:52,390
model that I show you today is just a

00:37:50,380 --> 00:37:54,700
test model with few data the serious

00:37:52,390 --> 00:37:56,650
model for Italian Norwegian has been

00:37:54,700 --> 00:38:00,820
running for five days and hasn't

00:37:56,650 --> 00:38:03,730
finished training yet so I cannot really

00:38:00,820 --> 00:38:06,520
wait to you because it's just training

00:38:03,730 --> 00:38:08,380
on a 50 core machine has been running

00:38:06,520 --> 00:38:11,410
from for days and 5000 machine it's not

00:38:08,380 --> 00:38:13,720
looping it's just that it it does a

00:38:11,410 --> 00:38:17,980
really really longer refinement process

00:38:13,720 --> 00:38:21,270
so it induces this first table takes 10

00:38:17,980 --> 00:38:26,170
hours build the first model 20 minutes

00:38:21,270 --> 00:38:28,750
then starts doing some fine-tuning some

00:38:26,170 --> 00:38:33,280
parameter fine-tuning and this takes up

00:38:28,750 --> 00:38:37,150
approximately another hour or so then

00:38:33,280 --> 00:38:40,750
goes to the monolingual corpus translate

00:38:37,150 --> 00:38:43,210
it with the model it has built and the

00:38:40,750 --> 00:38:45,640
generates two parallel corpora because

00:38:43,210 --> 00:38:47,080
it goes to Norwegian applies the Italian

00:38:45,640 --> 00:38:48,730
origin model and generate a

00:38:47,080 --> 00:38:50,800
corresponding Italian translation and

00:38:48,730 --> 00:38:52,570
that's the same for the other and then

00:38:50,800 --> 00:38:55,780
now you have two very big parallel

00:38:52,570 --> 00:38:58,090
corpora that are used to augment the

00:38:55,780 --> 00:39:00,130
rough model that you started with it's

00:38:58,090 --> 00:39:02,410
like a vida be training you bootstrap

00:39:00,130 --> 00:39:05,700
with a rough model and use yourself to

00:39:02,410 --> 00:39:08,830
improve your own training so if you're

00:39:05,700 --> 00:39:11,380
supplied like me seven hundred millions

00:39:08,830 --> 00:39:13,240
of words this model takes a long time to

00:39:11,380 --> 00:39:15,610
translate seven hundred millions of

00:39:13,240 --> 00:39:18,120
stuff then you do another fine-tuning

00:39:15,610 --> 00:39:21,310
and then you do that again iterative ten

00:39:18,120 --> 00:39:24,160
tuning iterations four three iterations

00:39:21,310 --> 00:39:27,340
of back translating all your corpus from

00:39:24,160 --> 00:39:30,430
scratch takes a ridiculous amount of

00:39:27,340 --> 00:39:32,110
time so I hope to actually post some

00:39:30,430 --> 00:39:37,840
updates or how the experience went

00:39:32,110 --> 00:39:40,660
actually so in conclusion today what we

00:39:37,840 --> 00:39:43,090
add here so we have a docker packages

00:39:40,660 --> 00:39:43,850
version of the Moniz's which is ready to

00:39:43,090 --> 00:39:46,670
use to journey

00:39:43,850 --> 00:39:48,740
the training set we have an HTTP API

00:39:46,670 --> 00:39:50,630
server to query the model obtained this

00:39:48,740 --> 00:39:52,820
way and it's all available in this

00:39:50,630 --> 00:39:55,010
repository and then we have a packages

00:39:52,820 --> 00:39:57,320
version of maitika tool which is clewd

00:39:55,010 --> 00:40:01,130
also my single server and acti MQ

00:39:57,320 --> 00:40:03,110
instance virus demons to perform the

00:40:01,130 --> 00:40:05,990
analysis and the translation in the

00:40:03,110 --> 00:40:09,650
background and Apache to web app with a

00:40:05,990 --> 00:40:12,650
PHP a very Mongoose PHP Roboto

00:40:09,650 --> 00:40:14,960
rounds in docker composer but I aim to

00:40:12,650 --> 00:40:17,810
support you Benares so you can just

00:40:14,960 --> 00:40:21,410
deploy that anywhere and you can start

00:40:17,810 --> 00:40:26,440
your own very small LSP

00:40:21,410 --> 00:40:29,240
at home for your open projects kudos to

00:40:26,440 --> 00:40:31,520
the for Knights which are those were

00:40:29,240 --> 00:40:33,800
perpetrated in the cover of my

00:40:31,520 --> 00:40:35,120
presentation which is v co informant in

00:40:33,800 --> 00:40:37,100
the phrase base machine translation

00:40:35,120 --> 00:40:38,960
thomas michael off for inventing work to

00:40:37,100 --> 00:40:41,180
act which is the the mapping and

00:40:38,960 --> 00:40:42,940
language model with those vectors at

00:40:41,180 --> 00:40:45,290
ampush gate for inventing pi torch

00:40:42,940 --> 00:40:47,120
without which we will be never be able

00:40:45,290 --> 00:40:49,310
to actually train anything Niro and

00:40:47,120 --> 00:40:50,930
Michael are station for putting all

00:40:49,310 --> 00:40:53,990
together and be the author of the paper

00:40:50,930 --> 00:40:56,110
I took inspiration from thank you very

00:40:53,990 --> 00:40:56,110
much

00:41:00,130 --> 00:41:10,940
so are there any questions is the faces

00:41:07,160 --> 00:41:15,580
of the people the cover is sergeant

00:41:10,940 --> 00:41:15,580
pepper's Lonely Hearts Club Band yes

00:41:18,410 --> 00:41:21,520
[Music]

00:41:26,270 --> 00:41:29,670
[Music]

00:41:30,059 --> 00:41:40,930
yes yes yes so your colleague is asking

00:41:37,150 --> 00:41:43,809
when you translate a sentence which has

00:41:40,930 --> 00:41:47,440
some markup into another language who

00:41:43,809 --> 00:41:50,559
happened to remix the the words in a

00:41:47,440 --> 00:41:52,930
different order the tag formatting is

00:41:50,559 --> 00:41:55,540
not preserved because for example you

00:41:52,930 --> 00:41:57,910
may end up with a sentence between tags

00:41:55,540 --> 00:42:00,640
which is split into different sentences

00:41:57,910 --> 00:42:05,079
with something for in-between then you

00:42:00,640 --> 00:42:09,910
should duplicate the tags so this

00:42:05,079 --> 00:42:12,819
problem has been solved by Christian

00:42:09,910 --> 00:42:15,099
Bakr which is a Google brain scientist

00:42:12,819 --> 00:42:18,970
and working in the University of

00:42:15,099 --> 00:42:22,780
Edinburgh and he basically employed the

00:42:18,970 --> 00:42:25,960
trick so he factorized all the tagging

00:42:22,780 --> 00:42:29,319
if you have a sentence five words are

00:42:25,960 --> 00:42:32,829
tagged you put tags in each separate

00:42:29,319 --> 00:42:35,440
five words so you multiply the tags so

00:42:32,829 --> 00:42:38,079
that each single word is between is

00:42:35,440 --> 00:42:41,859
bolded independently then you translate

00:42:38,079 --> 00:42:44,230
you let those be remixed and then if you

00:42:41,859 --> 00:42:46,119
happen to have in the target language a

00:42:44,230 --> 00:42:48,970
sentence which to tax you can collapse

00:42:46,119 --> 00:42:50,079
them into words between one tag you

00:42:48,970 --> 00:42:53,020
solve them that way

00:42:50,079 --> 00:42:55,359
this technology that I have developed

00:42:53,020 --> 00:42:57,640
the doesn't feature the tracker which is

00:42:55,359 --> 00:43:01,109
a one of the first things I'm going to

00:42:57,640 --> 00:43:04,089
add actually because it's a very naive

00:43:01,109 --> 00:43:07,299
trick but it works very well actually

00:43:04,089 --> 00:43:09,700
and so I will scavenge that from the

00:43:07,299 --> 00:43:13,299
mid-cap project because the meta project

00:43:09,700 --> 00:43:16,599
this mate cat means machine translation

00:43:13,299 --> 00:43:18,040
unanswered the CAD tool and it shipped

00:43:16,599 --> 00:43:21,990
the original project when it was a

00:43:18,040 --> 00:43:21,990
research project with an array of

00:43:22,699 --> 00:43:28,640
little stuff like this who actually

00:43:25,150 --> 00:43:32,509
improve it a lot the the quality of the

00:43:28,640 --> 00:43:34,249
the engine I will try to go back in the

00:43:32,509 --> 00:43:36,619
old rapper when it was a research

00:43:34,249 --> 00:43:38,420
project and pulled out those back in now

00:43:36,619 --> 00:43:41,390
I've developed from scratch sure because

00:43:38,420 --> 00:43:59,569
it was just much easier to for the

00:43:41,390 --> 00:44:02,209
purpose of this conference okay if I saw

00:43:59,569 --> 00:44:04,640
your colleague asked you don't need the

00:44:02,209 --> 00:44:07,219
parallel corpus to Train monus's but

00:44:04,640 --> 00:44:09,259
what if you happen to use a to bowling

00:44:07,219 --> 00:44:13,039
word corpora which is in fact two

00:44:09,259 --> 00:44:18,489
parallel corpora will be easier or

00:44:13,039 --> 00:44:21,859
faster not faster by will be less noisy

00:44:18,489 --> 00:44:25,789
less noisy because you will end up with

00:44:21,859 --> 00:44:28,039
the good quality mappings which usually

00:44:25,789 --> 00:44:30,949
they end up for being very noisy when

00:44:28,039 --> 00:44:33,589
you use more lingual corpora because you

00:44:30,949 --> 00:44:36,469
end up with a lot of the false positives

00:44:33,589 --> 00:44:39,380
if you happen to use a parallel corpus

00:44:36,469 --> 00:44:42,709
those mappings end up for be high Kwaito

00:44:39,380 --> 00:44:44,839
mappings so your training is actually of

00:44:42,709 --> 00:44:47,390
a better quality you end up with a

00:44:44,839 --> 00:44:48,579
better model with less noise and so you

00:44:47,390 --> 00:44:51,650
have to spend less time doing

00:44:48,579 --> 00:44:54,349
fine-tuning a back translation and you

00:44:51,650 --> 00:44:57,049
don't have to create this really really

00:44:54,349 --> 00:45:00,259
really huge amount of data only for the

00:44:57,049 --> 00:45:02,779
sake of a Monte Carlo sampling that

00:45:00,259 --> 00:45:05,799
averages out the noise leaving the good

00:45:02,779 --> 00:45:05,799
stuff for in this place

00:45:21,210 --> 00:45:35,250
I am I think I didn't I understand the

00:45:24,960 --> 00:45:37,950
question yeah so all of these files

00:45:35,250 --> 00:45:43,670
which is the RTD officer the document

00:45:37,950 --> 00:45:46,769
the Excel PowerPoint also web pages also

00:45:43,670 --> 00:45:49,019
scans files it's true only if you put an

00:45:46,769 --> 00:45:51,510
advance at the filter which is a

00:45:49,019 --> 00:45:55,079
probability of translated and it does

00:45:51,510 --> 00:45:58,140
all OCR or PDF you can also just

00:45:55,079 --> 00:46:02,519
translate directly a TM x or x live and

00:45:58,140 --> 00:46:04,410
also desktop publishing stuff and vice

00:46:02,519 --> 00:46:09,359
localization formats like the properties

00:46:04,410 --> 00:46:11,279
and the ways the strings so all this

00:46:09,359 --> 00:46:14,940
thing whenever you put something in an

00:46:11,279 --> 00:46:17,519
ex-slave is generated behind the scenes

00:46:14,940 --> 00:46:20,460
which is the envelope container all the

00:46:17,519 --> 00:46:22,680
strings get pulled out and they are

00:46:20,460 --> 00:46:25,140
inserted in a temporary location when

00:46:22,680 --> 00:46:28,740
you translate you replace those strings

00:46:25,140 --> 00:46:31,140
and then when you do download or preview

00:46:28,740 --> 00:46:34,769
those strings get packet back into the X

00:46:31,140 --> 00:46:38,000
left the blob of the file is just a

00:46:34,769 --> 00:46:41,549
matter of replacing placeholders and

00:46:38,000 --> 00:46:45,569
packing all the Byner him and you end up

00:46:41,549 --> 00:46:51,509
with a PowerPoint or a docx file in the

00:46:45,569 --> 00:46:54,210
end at the other file format

00:46:51,509 --> 00:46:55,680
okay so okay you're the real question of

00:46:54,210 --> 00:46:57,569
your colleague was there was the process

00:46:55,680 --> 00:47:00,059
of adding another file format you

00:46:57,569 --> 00:47:02,759
basically have to implement a new class

00:47:00,059 --> 00:47:05,869
behind which is the class for that file

00:47:02,759 --> 00:47:12,960
format the idea is that the interface

00:47:05,869 --> 00:47:15,359
aspects here's a blob give me the Dex

00:47:12,960 --> 00:47:20,160
lift strings so you just have to

00:47:15,359 --> 00:47:22,710
implement that kind of process it might

00:47:20,160 --> 00:47:26,069
be really easy for example if it's you

00:47:22,710 --> 00:47:28,650
know llamo or JSON because you just have

00:47:26,069 --> 00:47:32,430
to come you can come up with a template

00:47:28,650 --> 00:47:33,780
ax left and you convert a JSON into an

00:47:32,430 --> 00:47:36,390
XML

00:47:33,780 --> 00:47:39,030
in the format it's life so you can just

00:47:36,390 --> 00:47:41,130
take the strings and you paste those in

00:47:39,030 --> 00:47:44,880
this kind of format if it's a more

00:47:41,130 --> 00:47:46,350
advanced of the format and I I'm coming

00:47:44,880 --> 00:47:49,050
up with the most difficult format out

00:47:46,350 --> 00:47:50,640
there which is the AutoCAD aldicott is a

00:47:49,050 --> 00:47:53,330
total nightmare because it's

00:47:50,640 --> 00:47:58,170
undocumented it's a heavily binary and

00:47:53,330 --> 00:48:00,810
the strings are basically shredded

00:47:58,170 --> 00:48:03,320
everywhere in the thing then you have to

00:48:00,810 --> 00:48:06,810
come up with something that is able to

00:48:03,320 --> 00:48:09,030
read the strings place a placeholder in

00:48:06,810 --> 00:48:11,730
the original file and you pack these

00:48:09,030 --> 00:48:14,490
interacts left and then you constrict

00:48:11,730 --> 00:48:16,350
the sentences out there I can show you

00:48:14,490 --> 00:48:27,650
acts live because it's not really the

00:48:16,350 --> 00:48:27,650
field once you see it here we are media

00:48:29,480 --> 00:48:39,390
so as you see here an excellent file you

00:48:34,590 --> 00:48:42,060
have issue have a very big blob which is

00:48:39,390 --> 00:48:44,190
the binary basis 64 encoded version of

00:48:42,060 --> 00:48:48,030
the file with the placeholders already

00:48:44,190 --> 00:48:50,280
put in between then you have the units

00:48:48,030 --> 00:48:53,160
which is a translation unit a sentence

00:48:50,280 --> 00:48:56,510
which has always a segment with a source

00:48:53,160 --> 00:48:58,770
a target so for example you eat two

00:48:56,510 --> 00:49:00,990
segments an application to my boy

00:48:58,770 --> 00:49:04,170
temporary success document and here you

00:49:00,990 --> 00:49:07,680
have the corresponding target this file

00:49:04,170 --> 00:49:12,090
is all from English to Japanese because

00:49:07,680 --> 00:49:14,070
of this Heather ear and then you go down

00:49:12,090 --> 00:49:16,290
and you keep having units a segment

00:49:14,070 --> 00:49:19,530
source and target unit segment source

00:49:16,290 --> 00:49:21,840
and target this is the very basic unit

00:49:19,530 --> 00:49:25,430
of work so if to come up with another

00:49:21,840 --> 00:49:28,680
way of producing annex life from a

00:49:25,430 --> 00:49:31,830
profit i-5 format you have to come up

00:49:28,680 --> 00:49:33,330
with a way to construe this XML once you

00:49:31,830 --> 00:49:36,720
have the sentences is really easy you

00:49:33,330 --> 00:49:39,900
can compose this example it's easy the

00:49:36,720 --> 00:49:42,900
very difficult thing is that starting

00:49:39,900 --> 00:49:45,960
from this stuff you just have to find a

00:49:42,900 --> 00:49:47,350
way to take out strings put a

00:49:45,960 --> 00:49:50,980
placeholder

00:49:47,350 --> 00:49:54,730
with the ID of the unit you have here

00:49:50,980 --> 00:49:58,300
and then you have to base64 encode and

00:49:54,730 --> 00:50:04,090
stash it here let me just show you a

00:49:58,300 --> 00:50:09,030
real one because since we are in I can

00:50:04,090 --> 00:50:14,850
show you this a real example of

00:50:09,030 --> 00:50:23,070
something we am here nope

00:50:14,850 --> 00:50:29,860
these are PowerPoint loading loading

00:50:23,070 --> 00:50:32,110
more loading okay so you can see the

00:50:29,860 --> 00:50:34,330
translation units with these the X left

00:50:32,110 --> 00:50:37,600
for version 1.0 the one I show you on

00:50:34,330 --> 00:50:45,000
Wikipedia was the 2.0 sorry for the

00:50:37,600 --> 00:50:48,840
mismatch sure the is my GPU is crunching

00:50:45,000 --> 00:50:48,840
how big was this file

00:50:55,030 --> 00:51:08,059
in the meantime I'm just okay set

00:51:02,990 --> 00:51:10,550
wrapped okay here you can see this is

00:51:08,059 --> 00:51:13,250
the base64 encoded the file the

00:51:10,550 --> 00:51:16,609
PowerPoint file encoded as a big 64 if I

00:51:13,250 --> 00:51:18,130
had take this one and do the code I can

00:51:16,609 --> 00:51:31,069
see that the binary

00:51:18,130 --> 00:51:34,940
pptx you can see the aether here at the

00:51:31,069 --> 00:51:37,300
wonder-percent CPU okay and here you

00:51:34,940 --> 00:51:40,099
have all the different translation units

00:51:37,300 --> 00:51:45,380
when you have the source

00:51:40,099 --> 00:51:47,210
doc you bananas and with the segment

00:51:45,380 --> 00:51:50,510
source and the target document Cuban

00:51:47,210 --> 00:51:51,680
eyes has been basically generated as a

00:51:50,510 --> 00:51:53,809
copy of this one

00:51:51,680 --> 00:51:55,910
so whenever when you translate the your

00:51:53,809 --> 00:51:58,040
tree right here you write on to the

00:51:55,910 --> 00:52:00,770
database and when you export you pack

00:51:58,040 --> 00:52:03,319
all the original strings in the place of

00:52:00,770 --> 00:52:06,020
this one you can see when the tags a

00:52:03,319 --> 00:52:11,839
year the G tags they were showing inside

00:52:06,020 --> 00:52:16,280
the ater boxer these are very nasty file

00:52:11,839 --> 00:52:21,410
format with a very heavy documented

00:52:16,280 --> 00:52:24,670
standard you just use this as a working

00:52:21,410 --> 00:52:27,410
table or as a workbench and then you

00:52:24,670 --> 00:52:30,200
when you have all the strings base is 60

00:52:27,410 --> 00:52:32,030
good day code this one subdued of the

00:52:30,200 --> 00:52:35,720
placeholder which pointed to a single

00:52:32,030 --> 00:52:39,319
unit save that the blob as a file and

00:52:35,720 --> 00:52:41,500
you have a fully translated file format

00:52:39,319 --> 00:52:45,349
output this is the most complicated

00:52:41,500 --> 00:52:48,220
process in extending this capital but it

00:52:45,349 --> 00:52:49,910
has been done for each of these formats

00:52:48,220 --> 00:52:54,609
any other questions

00:52:49,910 --> 00:52:54,609
sure okay

00:52:58,749 --> 00:53:07,299
so I chose Italian Norwegian because I

00:53:02,960 --> 00:53:10,249
knew their there were not a lot of

00:53:07,299 --> 00:53:12,559
resources some just some millions of

00:53:10,249 --> 00:53:18,460
words for example in English which is

00:53:12,559 --> 00:53:22,219
huge is 80 millions of sentences then

00:53:18,460 --> 00:53:25,819
times 10 for each word you go 810

00:53:22,219 --> 00:53:28,999
millions of sentences Italian Norwegian

00:53:25,819 --> 00:53:30,319
has 12 millions of sentences 6 millions

00:53:28,999 --> 00:53:35,180
and then because I was listening to

00:53:30,319 --> 00:53:37,009
invert actually that works it actually

00:53:35,180 --> 00:53:39,619
serves them they're always the one which

00:53:37,009 --> 00:53:41,269
doesn't work it's Sardinian I try to

00:53:39,619 --> 00:53:43,130
translate in Serbia but with ten

00:53:41,269 --> 00:53:44,509
thousand sentences it's impossible to

00:53:43,130 --> 00:53:47,180
run with a mapping with ten turns

00:53:44,509 --> 00:53:49,160
something this you need some millions of

00:53:47,180 --> 00:53:51,890
words by it's easy because you just need

00:53:49,160 --> 00:53:55,910
to crawl sardinia websites

00:53:51,890 --> 00:53:57,200
Sardinian books and stuff it's not

00:53:55,910 --> 00:53:59,509
difficult to find the monolingual

00:53:57,200 --> 00:54:01,579
corpora the real challenge is to find

00:53:59,509 --> 00:54:03,349
the parallel corpora but this technology

00:54:01,579 --> 00:54:05,989
turns the problem into scavenging

00:54:03,349 --> 00:54:07,999
modeling work apparently which is the

00:54:05,989 --> 00:54:10,009
more content you can just scroll in just

00:54:07,999 --> 00:54:11,239
one single language totally unrelated

00:54:10,009 --> 00:54:19,789
with the other language you want to

00:54:11,239 --> 00:54:27,289
Carolla it's just an easier task yeah

00:54:19,789 --> 00:54:31,729
google has it sure like google books for

00:54:27,289 --> 00:54:35,329
example they have the crawler then align

00:54:31,729 --> 00:54:37,789
it the 1 billion words invites languages

00:54:35,329 --> 00:54:42,160
with good books and ice the top source

00:54:37,789 --> 00:54:45,529
of the transition quality the when you

00:54:42,160 --> 00:54:48,410
actually are on google maps and you bark

00:54:45,529 --> 00:54:50,359
and a random address inside the google

00:54:48,410 --> 00:54:54,109
maps and understands you that's because

00:54:50,359 --> 00:54:58,069
it has a 1 trillion word the language

00:54:54,109 --> 00:55:01,099
model built it's just so much data that

00:54:58,069 --> 00:55:05,839
it oh it's always able to understand you

00:55:01,099 --> 00:55:07,610
because he's always seen something so

00:55:05,839 --> 00:55:09,580
you're on the region works the

00:55:07,610 --> 00:55:13,640
I wanted to show you is still training

00:55:09,580 --> 00:55:16,700
sorry about that but the idea is that it

00:55:13,640 --> 00:55:18,320
takes always weeks of training but in

00:55:16,700 --> 00:55:21,680
the end you will end up with a model

00:55:18,320 --> 00:55:24,290
trained on a lot of data Moses since the

00:55:21,680 --> 00:55:27,590
most is the last chain of the the ring

00:55:24,290 --> 00:55:30,530
of the chain is not the best translation

00:55:27,590 --> 00:55:32,510
software server out there B has been

00:55:30,530 --> 00:55:34,220
superseded by the narrow machine

00:55:32,510 --> 00:55:39,020
translation which is much more fluent

00:55:34,220 --> 00:55:44,150
but since Moses is a phrase based the

00:55:39,020 --> 00:55:45,760
counts are made as discrete counts so

00:55:44,150 --> 00:55:49,970
it's always able to come up with the

00:55:45,760 --> 00:55:54,140
probabilities even in absence of huge

00:55:49,970 --> 00:55:56,150
data examples it's a it's a discrete

00:55:54,140 --> 00:55:58,310
system opposed to a continues in the

00:55:56,150 --> 00:56:01,430
mathematical sense system for the neural

00:55:58,310 --> 00:56:05,060
networks generation quality is lower but

00:56:01,430 --> 00:56:06,740
it is a score of 26 in a blue scale

00:56:05,060 --> 00:56:09,590
which is the scale for translation

00:56:06,740 --> 00:56:11,840
quality Google with an arrow achieves

00:56:09,590 --> 00:56:14,360
the 40 points which is outstanding a

00:56:11,840 --> 00:56:18,440
26-hour capers translating star drafting

00:56:14,360 --> 00:56:19,430
I think our time's up thank you so much

00:56:18,440 --> 00:56:28,890
for attending here

00:56:19,430 --> 00:56:28,890

YouTube URL: https://www.youtube.com/watch?v=woPY5iepTuM


