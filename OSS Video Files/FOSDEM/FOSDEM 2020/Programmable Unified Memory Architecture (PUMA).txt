Title: Programmable Unified Memory Architecture (PUMA)
Publication date: 2020-07-16
Playlist: FOSDEM 2020
Description: 
	by Stijn Eyerman

At: FOSDEM 2020
https://video.fosdem.org/2020/AW1.121/graph_puma.webm

Large scale graph analytics is essential to analyze relationships in big data sets. Thereto, the DARPA HIVE program targets a leap in power efficient graph analytics. In response to this program, Intel proposes the Programmable Unified Memory Architecture (PUMA). Based on graph workload analysis insights, PUMA consists of many multi-threaded cores, fine-grained memory and network accesses, a globally shared address space and powerful offload engines. In this talk, we will describe the PUMA architecture, both in terms of hardware and the software ecosystem. We will provide initial simulation based performance estimations, showing that for graph analysis applications, a PUMA node will outperform a conventional compute node by one to two orders of magnitude. Additionally, PUMA will continue to scale across multiple nodes, which is a challenge in conventional multinode setups.

Room: AW1.121
Scheduled start: 2020-02-01 16:05:00
Captions: 
	00:00:04,560 --> 00:00:10,559
so let's continue I'm very happy to

00:00:08,100 --> 00:00:12,420
announce state who is working with Intel

00:00:10,559 --> 00:00:16,940
and he's talking about programmable

00:00:12,420 --> 00:00:20,970
Unified's in memory architecture Puma

00:00:16,940 --> 00:00:24,180
okay thank you I'm afraid there's going

00:00:20,970 --> 00:00:25,710
to be another Hardware talk but I'll

00:00:24,180 --> 00:00:28,020
keep it simple and hope that you

00:00:25,710 --> 00:00:30,390
appreciate the fact that the hardware is

00:00:28,020 --> 00:00:35,160
also evaluate evolving and that we're

00:00:30,390 --> 00:00:39,839
looking at more efficient processors for

00:00:35,160 --> 00:00:41,699
graph processing so as you all know

00:00:39,839 --> 00:00:43,320
graph processing is getting bigger and

00:00:41,699 --> 00:00:45,659
bigger and graphs are getting bigger and

00:00:43,320 --> 00:00:50,190
bigger and you want to process more data

00:00:45,659 --> 00:00:51,960
so we set ourselves a a so we found that

00:00:50,190 --> 00:00:54,749
that we need to increase the efficiency

00:00:51,960 --> 00:00:58,649
of graph processing versus existing

00:00:54,749 --> 00:01:02,339
architectures such as CPUs and GPUs so

00:00:58,649 --> 00:01:05,580
to that we propose the programmable

00:01:02,339 --> 00:01:10,369
unified memory architecture greevey ated

00:01:05,580 --> 00:01:13,020
as Puma and in this talk I want to

00:01:10,369 --> 00:01:14,700
explain to you why graph processing is

00:01:13,020 --> 00:01:17,159
actually challenging on the existing

00:01:14,700 --> 00:01:20,039
architectures what makes Puma fit for

00:01:17,159 --> 00:01:23,880
graph processing some high-level details

00:01:20,039 --> 00:01:25,759
about Puma and how it performs and after

00:01:23,880 --> 00:01:28,890
this presentation you'll probably get a

00:01:25,759 --> 00:01:30,539
question can we buy Puma unfortunately

00:01:28,890 --> 00:01:35,670
not yet because it's still on the

00:01:30,539 --> 00:01:37,049
development okay as you all know Intel

00:01:35,670 --> 00:01:42,149
is the market leader of high performance

00:01:37,049 --> 00:01:43,799
processors and these processors we've

00:01:42,149 --> 00:01:45,479
implemented a lot of things for regular

00:01:43,799 --> 00:01:48,509
applications that works very well such

00:01:45,479 --> 00:01:50,630
as branch prediction ranges and more in

00:01:48,509 --> 00:01:53,490
general regular applications are very

00:01:50,630 --> 00:01:55,409
predictable so we use that to predict

00:01:53,490 --> 00:01:58,829
branches ahead such that we can float

00:01:55,409 --> 00:02:00,719
instructions more in a continuous way we

00:01:58,829 --> 00:02:03,109
have caches that assumes that if you

00:02:00,719 --> 00:02:05,999
access some data you will access it

00:02:03,109 --> 00:02:08,580
again in the near future or its

00:02:05,999 --> 00:02:09,990
neighboring data we have vector

00:02:08,580 --> 00:02:12,330
operations that perform the same

00:02:09,990 --> 00:02:14,129
operation of neighboring data and all

00:02:12,330 --> 00:02:16,440
that works good for regular applications

00:02:14,129 --> 00:02:18,150
but graph applications as the previous

00:02:16,440 --> 00:02:24,629
speaker also explained are

00:02:18,150 --> 00:02:28,129
that nice for these architectures for

00:02:24,629 --> 00:02:30,599
example many of the graph applicator

00:02:28,129 --> 00:02:33,989
grab applicators have many branches that

00:02:30,599 --> 00:02:35,970
are data dependent so the actual outcome

00:02:33,989 --> 00:02:38,819
of the bench depends on the data that is

00:02:35,970 --> 00:02:39,599
in the graph and which is of course not

00:02:38,819 --> 00:02:44,000
predictable

00:02:39,599 --> 00:02:44,000
so branch predictors don't work loud

00:02:44,390 --> 00:02:49,859
data is also accessed in a scatter tray

00:02:47,129 --> 00:02:52,260
so that was also introduced by previous

00:02:49,859 --> 00:02:55,260
speakers so you access the neighbors and

00:02:52,260 --> 00:02:57,329
the neighbors are not the next notes in

00:02:55,260 --> 00:02:59,909
your list of notes they are scattered

00:02:57,329 --> 00:03:01,739
all over the place so caches don't wear

00:02:59,909 --> 00:03:03,930
out because you don't use the a

00:03:01,739 --> 00:03:06,540
neighboring data of them to reduce the

00:03:03,930 --> 00:03:09,209
same data and the same for vector

00:03:06,540 --> 00:03:13,769
operations so we have we see very low

00:03:09,209 --> 00:03:16,889
performance on regular CPUs for graph

00:03:13,769 --> 00:03:20,000
applications many people have proposed

00:03:16,889 --> 00:03:23,760
to use GPUs they are actually better and

00:03:20,000 --> 00:03:26,129
general because of higher bandwidth and

00:03:23,760 --> 00:03:29,400
more threads but they actually suffer

00:03:26,129 --> 00:03:31,919
from the same problems if branches

00:03:29,400 --> 00:03:34,500
diverge so go to another direction then

00:03:31,919 --> 00:03:37,500
you cannot the parallelism cannot be

00:03:34,500 --> 00:03:40,169
fully exploited scattered memory

00:03:37,500 --> 00:03:41,129
accesses prevents to use manual

00:03:40,169 --> 00:03:43,260
coalescing

00:03:41,129 --> 00:03:45,720
meaning that all of the official things

00:03:43,260 --> 00:03:48,419
you cannot use in GPUs too and we also

00:03:45,720 --> 00:03:50,220
saw before in previous presentations

00:03:48,419 --> 00:03:54,720
there is a problem of memory capacity

00:03:50,220 --> 00:03:57,209
and scaling out so what are potential

00:03:54,720 --> 00:03:59,099
solutions there have been proposals of

00:03:57,209 --> 00:04:02,459
growth accelerated so that specific

00:03:59,099 --> 00:04:04,049
chips made for graph applications that

00:04:02,459 --> 00:04:06,659
have a some functionality for example

00:04:04,049 --> 00:04:09,239
that implement sparse linear linear

00:04:06,659 --> 00:04:12,599
algebra algorithms or Verte eccentrics

00:04:09,239 --> 00:04:14,940
algorithms the problem there is that you

00:04:12,599 --> 00:04:17,340
fixed with that functionality so if your

00:04:14,940 --> 00:04:20,789
application your algorithm works best

00:04:17,340 --> 00:04:23,840
for within the sparse linear algebra but

00:04:20,789 --> 00:04:26,580
your accelerator is a vertex centric

00:04:23,840 --> 00:04:29,029
operation accelerator then you need to

00:04:26,580 --> 00:04:31,980
transfer your application and

00:04:29,029 --> 00:04:33,870
potentially and you also need a host

00:04:31,980 --> 00:04:36,030
so a CPU that that controls this

00:04:33,870 --> 00:04:39,480
accelerator and there is the data

00:04:36,030 --> 00:04:41,400
transfer between both another solution

00:04:39,480 --> 00:04:43,830
is to have a general instruction set

00:04:41,400 --> 00:04:45,300
processor like the normal CPUs but

00:04:43,830 --> 00:04:48,420
that's been optimized for graph

00:04:45,300 --> 00:04:49,830
allocations which is then more flexible

00:04:48,420 --> 00:04:52,140
because you have an instruction set you

00:04:49,830 --> 00:04:53,970
can implement other algorithms - it's

00:04:52,140 --> 00:04:56,610
self-contained you don't need a host /

00:04:53,970 --> 00:05:01,080
sake and that's actually what our

00:04:56,610 --> 00:05:05,340
approach was in Pune okay I've talked

00:05:01,080 --> 00:05:08,850
about the challenges that graph

00:05:05,340 --> 00:05:11,850
applications pose so how did we solve

00:05:08,850 --> 00:05:14,550
that in Puma most of the graph

00:05:11,850 --> 00:05:17,100
applications are very memory bound so

00:05:14,550 --> 00:05:19,530
most of the dam if you have a vengeful

00:05:17,100 --> 00:05:21,780
score as the uncle for example it's just

00:05:19,530 --> 00:05:24,630
stuck by the slow memory it's mating

00:05:21,780 --> 00:05:29,730
forever so you can't cannot use all of

00:05:24,630 --> 00:05:32,370
its speed so instead in Puma we have

00:05:29,730 --> 00:05:34,680
much lighter course much slower cost but

00:05:32,370 --> 00:05:36,360
we have many of them so they still wait

00:05:34,680 --> 00:05:39,950
for memory but because we have many of

00:05:36,360 --> 00:05:42,680
them the total throughput is higher I

00:05:39,950 --> 00:05:45,720
said that before caching was a problem

00:05:42,680 --> 00:05:47,640
so the problem is that if you know data

00:05:45,720 --> 00:05:50,340
you only need that one element that you

00:05:47,640 --> 00:05:51,930
notice that you load not the full cache

00:05:50,340 --> 00:05:54,710
length so in a conventional architecture

00:05:51,930 --> 00:05:57,660
you know the full cache rank from memory

00:05:54,710 --> 00:05:59,880
through the memory bus into the cache

00:05:57,660 --> 00:06:01,530
and then to the core but as you see it

00:05:59,880 --> 00:06:03,290
the blue only the blue points are used

00:06:01,530 --> 00:06:06,830
it's very inefficient use of the cache

00:06:03,290 --> 00:06:11,790
capacity and of the memory bandwidth so

00:06:06,830 --> 00:06:15,840
it means you waste a lot of things a lot

00:06:11,790 --> 00:06:17,730
of potential for Puma we have lot of

00:06:15,840 --> 00:06:20,190
course but we also optimize the memory

00:06:17,730 --> 00:06:23,070
accesses such that you can access only a

00:06:20,190 --> 00:06:26,430
single element we bypass caches because

00:06:23,070 --> 00:06:30,300
they are not efficient to get more chip

00:06:26,430 --> 00:06:34,680
area for course and other stuff and get

00:06:30,300 --> 00:06:38,220
more fluent flu flow of memory

00:06:34,680 --> 00:06:39,600
operations another thing is the size of

00:06:38,220 --> 00:06:41,640
the graph so if you have a very large

00:06:39,600 --> 00:06:43,969
graph you need to partition it and put

00:06:41,640 --> 00:06:46,159
it into multiple nodes

00:06:43,969 --> 00:06:49,309
multiple compute nodes multiple servers

00:06:46,159 --> 00:06:52,789
but then when the graph of algorithm

00:06:49,309 --> 00:06:55,339
wants to access data on another node it

00:06:52,789 --> 00:06:57,289
needs to go to the whole communication

00:06:55,339 --> 00:06:59,839
stack which takes a lot of time and

00:06:57,289 --> 00:07:02,749
unfortunately graph algorithms are not

00:06:59,839 --> 00:07:04,819
very predictable in their locality so it

00:07:02,749 --> 00:07:07,999
often happens of course that we need to

00:07:04,819 --> 00:07:11,110
go off note giving a large performance

00:07:07,999 --> 00:07:13,699
penalty so for Puma we have this

00:07:11,110 --> 00:07:15,019
hardware disability check memory so

00:07:13,699 --> 00:07:17,929
there is a shared memory across the

00:07:15,019 --> 00:07:20,209
whole system you don't need to think

00:07:17,929 --> 00:07:21,589
about communication

00:07:20,209 --> 00:07:25,759
the network is high-bandwidth

00:07:21,589 --> 00:07:28,360
low-latency just to reduce the latency

00:07:25,759 --> 00:07:33,379
or the performance impact of remote

00:07:28,360 --> 00:07:35,659
accesses another thing that we saw that

00:07:33,379 --> 00:07:37,599
is that there are very a lot of common

00:07:35,659 --> 00:07:40,449
patterns in graph applications and we

00:07:37,599 --> 00:07:43,669
designed some offload engines that

00:07:40,449 --> 00:07:46,699
efficiently execute these patterns for

00:07:43,669 --> 00:07:49,639
example Atomics are very much very much

00:07:46,699 --> 00:07:52,519
used in graph algorithms but for a cord

00:07:49,639 --> 00:07:54,139
that's a very intensive operation so you

00:07:52,519 --> 00:07:56,269
have to look data you have to load the

00:07:54,139 --> 00:07:58,849
data update it and write it back and

00:07:56,269 --> 00:08:01,849
unlock the data so the core is often

00:07:58,849 --> 00:08:03,979
stuck a lot a long time performing this

00:08:01,849 --> 00:08:07,339
Atomics so and Puma we have this offload

00:08:03,979 --> 00:08:09,679
engine that performs the Atomics the

00:08:07,339 --> 00:08:12,009
release the core from performing these

00:08:09,679 --> 00:08:14,749
economics so just the poor the core

00:08:12,009 --> 00:08:17,179
issues an atomic instruction but leaves

00:08:14,749 --> 00:08:19,489
the execution over to the offload engine

00:08:17,179 --> 00:08:21,489
and the co can continue in executing

00:08:19,489 --> 00:08:23,509
other instructions in the background or

00:08:21,489 --> 00:08:25,610
so the operation is done in the

00:08:23,509 --> 00:08:27,919
background and the offload engine looks

00:08:25,610 --> 00:08:30,709
where the data is located and performs

00:08:27,919 --> 00:08:32,509
the update locally similarly a gather

00:08:30,709 --> 00:08:36,589
operations for example if you want to

00:08:32,509 --> 00:08:38,809
gather the some carrots tricks the

00:08:36,589 --> 00:08:42,680
characteristics of the labors of a

00:08:38,809 --> 00:08:45,670
vertex the normal operation is that you

00:08:42,680 --> 00:08:48,259
load index of the worst first neighbor

00:08:45,670 --> 00:08:50,000
then you load the data of the neighbor

00:08:48,259 --> 00:08:53,449
and store it somewhere so that's a very

00:08:50,000 --> 00:08:55,189
intensive process for the course so we

00:08:53,449 --> 00:08:57,830
have this DMA

00:08:55,189 --> 00:08:59,930
gather offload engine so the core again

00:08:57,830 --> 00:09:02,060
just excuse this DMA gather instruction

00:08:59,930 --> 00:09:03,650
and then continuous executing and in the

00:09:02,060 --> 00:09:06,200
background the offload and he performs

00:09:03,650 --> 00:09:08,360
the necessary memory accesses without

00:09:06,200 --> 00:09:12,410
actually needing to move the data all

00:09:08,360 --> 00:09:14,990
the data to the core itself other

00:09:12,410 --> 00:09:19,700
operations are man copy barriers queues

00:09:14,990 --> 00:09:22,250
and and so forth so going to into a

00:09:19,700 --> 00:09:25,430
little bit more architectural detail of

00:09:22,250 --> 00:09:28,160
a single kuma core so that's a schematic

00:09:25,430 --> 00:09:30,080
overview of a single puma core it

00:09:28,160 --> 00:09:32,510
consists of multiple pipelines so each

00:09:30,080 --> 00:09:35,480
of these are pipelines each pipeline

00:09:32,510 --> 00:09:39,590
supports multiple threads why like the

00:09:35,480 --> 00:09:41,120
previous presenters says it said most of

00:09:39,590 --> 00:09:43,070
the time we are waiting for memory

00:09:41,120 --> 00:09:44,720
operations so if we are meeting for

00:09:43,070 --> 00:09:46,730
memory operations we just switch to

00:09:44,720 --> 00:09:49,250
another threat if this one is also

00:09:46,730 --> 00:09:51,410
waiting switch to yet another type and

00:09:49,250 --> 00:09:54,530
so on just to hide all of these memory

00:09:51,410 --> 00:09:56,210
latencies we have limited caches you

00:09:54,530 --> 00:09:59,870
have next option Casa data cache which

00:09:56,210 --> 00:10:05,080
are very small it's for a very local

00:09:59,870 --> 00:10:05,080
data such as the stack of your threat

00:10:05,950 --> 00:10:11,870
the instructions executed by discourse

00:10:09,950 --> 00:10:14,150
are a novel instruction set that we

00:10:11,870 --> 00:10:16,210
designed it's based on reduced

00:10:14,150 --> 00:10:20,720
instruction set so simple instructions

00:10:16,210 --> 00:10:23,270
and we added specific instructions that

00:10:20,720 --> 00:10:25,910
are you can then use by graph operations

00:10:23,270 --> 00:10:32,270
such as a single instruction indirect

00:10:25,910 --> 00:10:34,550
load because we have limited caching we

00:10:32,270 --> 00:10:36,460
don't want to go to main memory all of

00:10:34,550 --> 00:10:38,780
the time for the data that's been been

00:10:36,460 --> 00:10:42,020
consumed and produced locally so we have

00:10:38,780 --> 00:10:43,880
a manually accessible scratch pad so the

00:10:42,020 --> 00:10:46,670
programmer can decide whether to cache

00:10:43,880 --> 00:10:49,490
its data to use a scratch pad or to go

00:10:46,670 --> 00:10:51,020
to main memory it's part of the global

00:10:49,490 --> 00:10:55,190
address space so other costs can also

00:10:51,020 --> 00:10:57,440
access the scratch pad of this core I

00:10:55,190 --> 00:10:58,910
talked about offload engines so they

00:10:57,440 --> 00:11:01,760
perform these operations in the

00:10:58,910 --> 00:11:03,950
background we have a memory controller

00:11:01,760 --> 00:11:06,890
that's been optimized for these small

00:11:03,950 --> 00:11:08,420
accesses he died accesses and we have a

00:11:06,890 --> 00:11:11,900
network interface to connect to other

00:11:08,420 --> 00:11:13,790
course also of course using this aid

00:11:11,900 --> 00:11:17,380
packets because most of the data is

00:11:13,790 --> 00:11:21,770
trans is used in the small granules

00:11:17,380 --> 00:11:25,670
granularities okay the full permit

00:11:21,770 --> 00:11:28,430
system as we envision it as its

00:11:25,670 --> 00:11:30,620
hierarchically built we have multiple of

00:11:28,430 --> 00:11:32,750
discourse it format I'll multiples of

00:11:30,620 --> 00:11:37,760
these starts form a note multiples of

00:11:32,750 --> 00:11:39,860
the these notes form a system and that

00:11:37,760 --> 00:11:41,780
means that a since you have already that

00:11:39,860 --> 00:11:44,330
many threats on a chord you have that

00:11:41,780 --> 00:11:47,870
many course on a tower and so on that

00:11:44,330 --> 00:11:51,070
the full system can easily consists of

00:11:47,870 --> 00:11:54,140
millions of Hardware context for threats

00:11:51,070 --> 00:11:56,050
again the global address space and the

00:11:54,140 --> 00:11:58,580
memory is shared across the full system

00:11:56,050 --> 00:12:01,040
so you can access any data from any

00:11:58,580 --> 00:12:02,990
point in the system

00:12:01,040 --> 00:12:05,240
of course this requires a very high

00:12:02,990 --> 00:12:07,790
bandwidth low latency network so we use

00:12:05,240 --> 00:12:10,040
hyper X network topology this is the

00:12:07,790 --> 00:12:14,030
textbook rest representation of this so

00:12:10,040 --> 00:12:16,490
you have a hierarchical design where

00:12:14,030 --> 00:12:18,650
within each level our everything is

00:12:16,490 --> 00:12:20,600
fully connected and then on the next

00:12:18,650 --> 00:12:25,400
level there are connections between the

00:12:20,600 --> 00:12:27,530
levels furthermore we plan to have

00:12:25,400 --> 00:12:30,020
optical connections between the thousand

00:12:27,530 --> 00:12:33,770
three notes again to increase bandwidth

00:12:30,020 --> 00:12:36,910
and latency more interesting for you

00:12:33,770 --> 00:12:39,320
maybe it's how do we program this beast

00:12:36,910 --> 00:12:41,750
that's still work-in-progress so we're

00:12:39,320 --> 00:12:44,800
also working and at a software

00:12:41,750 --> 00:12:47,410
infrastructure for this architecture

00:12:44,800 --> 00:12:50,750
initially where we did our first

00:12:47,410 --> 00:12:54,710
experiments which is a simple SPMD based

00:12:50,750 --> 00:12:56,840
parallelism using c and we use in

00:12:54,710 --> 00:13:01,280
forensics special intrinsics for the

00:12:56,840 --> 00:13:06,290
puma specific instructions and our vm

00:13:01,280 --> 00:13:08,900
for building a compiler in the meantime

00:13:06,290 --> 00:13:10,370
there has been increasing support for

00:13:08,900 --> 00:13:14,630
c++ p threats

00:13:10,370 --> 00:13:17,240
open and p and some tasking and we're

00:13:14,630 --> 00:13:18,790
also looking at implementing common

00:13:17,240 --> 00:13:23,740
graph library Scala programming

00:13:18,790 --> 00:13:23,740
languages and a Python front-end

00:13:23,870 --> 00:13:31,230
okay now importantly does pull-up

00:13:28,610 --> 00:13:35,570
fulfill its promises namely that it's

00:13:31,230 --> 00:13:37,079
more efficient to general to do graph

00:13:35,570 --> 00:13:39,600
analysis on it

00:13:37,079 --> 00:13:42,029
of course the chip is not available yet

00:13:39,600 --> 00:13:44,630
and neither its own so we are also

00:13:42,029 --> 00:13:48,779
working at FPGA model but that's not

00:13:44,630 --> 00:13:50,850
ready yet so basically our results that

00:13:48,779 --> 00:13:54,509
I will show up based on simulation where

00:13:50,850 --> 00:13:56,130
you take a puma binary you have a

00:13:54,509 --> 00:13:57,990
functional simulator that decodes the

00:13:56,130 --> 00:14:00,660
instructions of the binary that performs

00:13:57,990 --> 00:14:03,149
the actual simulated operations and I

00:14:00,660 --> 00:14:05,610
have the timing simulation that gives us

00:14:03,149 --> 00:14:07,560
that models all of the hardware

00:14:05,610 --> 00:14:10,079
structures such as the course the memory

00:14:07,560 --> 00:14:12,810
scratchpad Network and so on and that

00:14:10,079 --> 00:14:18,690
gives us a performance number and also

00:14:12,810 --> 00:14:22,350
interesting for developers a profile of

00:14:18,690 --> 00:14:25,829
how the execution looks like where cores

00:14:22,350 --> 00:14:27,569
are idle that's the thing part or used

00:14:25,829 --> 00:14:30,449
or waiting for memory that's the

00:14:27,569 --> 00:14:33,120
light-blue part such that the developers

00:14:30,449 --> 00:14:34,920
can optimize their fight the bottlenecks

00:14:33,120 --> 00:14:37,199
and of team optimize their application

00:14:34,920 --> 00:14:40,100
we also have an analytical model which i

00:14:37,199 --> 00:14:43,380
won't go into detail because simulation

00:14:40,100 --> 00:14:45,839
is a very intensive process so we can

00:14:43,380 --> 00:14:49,170
simulate up to a few tiles but after

00:14:45,839 --> 00:14:51,269
that it becomes quickly infeasible and

00:14:49,170 --> 00:14:53,579
then therefore you use this analytical

00:14:51,269 --> 00:14:57,569
model which we then validate using

00:14:53,579 --> 00:15:01,380
simulation on a smaller course then we

00:14:57,569 --> 00:15:04,880
took a problem of kernels graph colors

00:15:01,380 --> 00:15:07,560
and applications we run them on a

00:15:04,880 --> 00:15:10,649
high-end in told Xeon server with four

00:15:07,560 --> 00:15:13,560
sockets we optimize them if needed and

00:15:10,649 --> 00:15:16,529
we also also ported and optimize these

00:15:13,560 --> 00:15:20,670
applications for Puma and the initial

00:15:16,529 --> 00:15:23,089
estimates show that this high-end

00:15:20,670 --> 00:15:26,250
zealand server with four circuits were

00:15:23,089 --> 00:15:29,449
approximately consumes the same power as

00:15:26,250 --> 00:15:33,649
a puma note well consumed in the future

00:15:29,449 --> 00:15:36,680
so comparing their performances also a

00:15:33,649 --> 00:15:39,840
energy efficiency

00:15:36,680 --> 00:15:41,970
we also did some multi-node on Zeon but

00:15:39,840 --> 00:15:45,320
that didn't work well because here's it

00:15:41,970 --> 00:15:48,450
you have this communication overhead

00:15:45,320 --> 00:15:50,400
that the results that the result was

00:15:48,450 --> 00:15:52,890
that we saw no speed-up for most

00:15:50,400 --> 00:15:55,830
applications or even slowdowns we

00:15:52,890 --> 00:16:00,150
projected to 16 to my notes and as I

00:15:55,830 --> 00:16:03,090
will show shortly this skills much

00:16:00,150 --> 00:16:04,500
better so here is an overview of some of

00:16:03,090 --> 00:16:07,590
the applications the performance results

00:16:04,500 --> 00:16:10,230
so it shows the speed up of a single

00:16:07,590 --> 00:16:13,500
Puma north versus a high incidence

00:16:10,230 --> 00:16:15,720
server and you see that there is always

00:16:13,500 --> 00:16:18,930
PETA there is a lot but there is a large

00:16:15,720 --> 00:16:21,360
range so it goes from T times to 200

00:16:18,930 --> 00:16:24,150
more than 200 times faster than Z own

00:16:21,360 --> 00:16:27,060
that's because some applications are

00:16:24,150 --> 00:16:29,340
more complete intensive did so they do

00:16:27,060 --> 00:16:31,410
more compute and of course the actors

00:16:29,340 --> 00:16:33,600
even can use all of its resources to

00:16:31,410 --> 00:16:36,000
efficiently do that on the other hand if

00:16:33,600 --> 00:16:39,360
it's purely memory bandwidth bound such

00:16:36,000 --> 00:16:43,200
as random walks then we see a very huge

00:16:39,360 --> 00:16:45,990
performance increase and the 16 order of

00:16:43,200 --> 00:16:48,830
injections also show that it scales much

00:16:45,990 --> 00:16:51,810
better than the others

00:16:48,830 --> 00:16:54,870
okay that was base get so I showed you

00:16:51,810 --> 00:16:57,930
that Puma is a programmable instruction

00:16:54,870 --> 00:17:00,180
set processor for graph applications it

00:16:57,930 --> 00:17:04,170
contains many features which I discussed

00:17:00,180 --> 00:17:05,939
and we show that it's true simulation

00:17:04,170 --> 00:17:09,120
and modeling that it's one to two orders

00:17:05,939 --> 00:17:12,120
of magnitude faster than a equal powers

00:17:09,120 --> 00:17:15,560
ian and it skills well to multi node and

00:17:12,120 --> 00:17:15,560
it's still a development of course

00:17:17,140 --> 00:17:25,479
[Applause]

00:17:32,360 --> 00:17:37,740
think this will be affordable as rock

00:17:36,330 --> 00:17:40,049
station or would you say no this is

00:17:37,740 --> 00:17:43,950
cloud installation that is somewhere in

00:17:40,049 --> 00:17:47,549
the service center so the question is

00:17:43,950 --> 00:17:50,370
whether this is a as a workstation PC or

00:17:47,549 --> 00:17:54,260
or desktop based thing or a no it would

00:17:50,370 --> 00:18:17,010
be more in a data center pick

00:17:54,260 --> 00:18:21,150
supercomputer setup so the question is

00:18:17,010 --> 00:18:25,950
if if we plan to have some lets it host

00:18:21,150 --> 00:18:27,900
notes or yeah we're still working on

00:18:25,950 --> 00:18:33,210
that so there is an there is an option

00:18:27,900 --> 00:18:35,880
to have potentially x86 nodes that do

00:18:33,210 --> 00:18:37,740
more of the other stuff that this is not

00:18:35,880 --> 00:18:40,040
efficient for but it's still under

00:18:37,740 --> 00:18:40,040
development

00:18:59,660 --> 00:19:37,890
it's not in the same it's an all

00:19:31,890 --> 00:19:40,890
connection but what's the okay so the

00:19:37,890 --> 00:19:44,550
question is is is data that's further in

00:19:40,890 --> 00:19:46,710
memory will it be slow were to exit yes

00:19:44,550 --> 00:19:50,070
of course because it's a very large

00:19:46,710 --> 00:19:51,360
system its distributed across the system

00:19:50,070 --> 00:19:54,330
will be distributed across multiple

00:19:51,360 --> 00:19:56,550
notes multiple racks so there will be a

00:19:54,330 --> 00:19:58,410
larger penalty but because of this all

00:19:56,550 --> 00:20:00,840
of this memory latency hiding techniques

00:19:58,410 --> 00:20:02,670
and these offload engines that perform

00:20:00,840 --> 00:20:05,310
it very efficiently you won't see as

00:20:02,670 --> 00:20:08,000
much as you see an conventional multi

00:20:05,310 --> 00:20:08,000
node set up

00:20:18,620 --> 00:20:25,850

YouTube URL: https://www.youtube.com/watch?v=BUYw5X5n9NY


