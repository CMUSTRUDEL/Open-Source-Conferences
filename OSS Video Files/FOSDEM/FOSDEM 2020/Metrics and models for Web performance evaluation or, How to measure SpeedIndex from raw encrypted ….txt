Title: Metrics and models for Web performance evaluation or, How to measure SpeedIndex from raw encrypted â€¦
Publication date: 2020-07-13
Playlist: FOSDEM 2020
Description: 
	Metrics and models for Web performance evaluation or, How to measure SpeedIndex from raw encrypted packets, and why it matters
by Dario Rossi

At: FOSDEM 2020
https://video.fosdem.org/2020/H.1309/webperf_qoe_research.webm

The World Wide Web is still among the most prominent Internet applications. While the Web landscape has been in perpetual movement since the very beginning,
these last few years have witnessed some noteworthy proposals such as SPDY, HTTP/2 and QUIC, which profoundly reshape the application-layer protocols family.
To measure the impact of such changes,  going beyond the classic W3C notion of page load time, a number of Web  performance metrics has been proposed (such as
SpeedIndex,  Above-The-Fold and variants).  At the same time, there is still a limited amount of understanding on how these metrics correlate with the user
perception (e.g., such as user ratings, user-perceived page load time, etc.). In this talk, we discuss the state of the art in metrics and models for Web
performance evaluation, and their correlation with user experience through several real-world studies. Additional information, software and datasets are
available at https://webqoe.telecom-paristech.fr

Room: H.1309 (Van Rijn)
Scheduled start: 2020-02-01 14:40:00
Captions: 
	00:00:05,270 --> 00:00:12,809
new ways of looking at things that we in

00:00:09,510 --> 00:00:14,520
the performance team love so daddy okay

00:00:12,809 --> 00:00:16,230
thank you thanks very much thanks for

00:00:14,520 --> 00:00:18,359
having me here it's very I'm really

00:00:16,230 --> 00:00:20,039
thrilled so what I'm gonna do is that

00:00:18,359 --> 00:00:22,140
I'm gonna bring basically two viewpoints

00:00:20,039 --> 00:00:24,689
I've been working on academia for the

00:00:22,140 --> 00:00:28,109
last 15 years or so and the last year I

00:00:24,689 --> 00:00:30,150
moved to what way so I basically have in

00:00:28,109 --> 00:00:32,040
the street view points with a university

00:00:30,150 --> 00:00:34,170
mindset and so today what they're gonna

00:00:32,040 --> 00:00:36,570
do is that I'm gonna talk about metrics

00:00:34,170 --> 00:00:38,100
and models for web and there's a little

00:00:36,570 --> 00:00:40,110
longer subtitle that I'm not going to

00:00:38,100 --> 00:00:42,270
comment here we're gonna see throughout

00:00:40,110 --> 00:00:44,430
things of all so of course this work

00:00:42,270 --> 00:00:46,490
wouldn't be not possible with number of

00:00:44,430 --> 00:00:48,390
people if they're in alphabetical order

00:00:46,490 --> 00:00:49,650
two are actually in the room one is

00:00:48,390 --> 00:00:51,030
Geelong to boot from Wikimedia

00:00:49,650 --> 00:00:54,810
Foundation the other is Flavia from

00:00:51,030 --> 00:00:57,270
Telecom Polytech so thanks with to them

00:00:54,810 --> 00:01:00,060
we also can discuss more interesting

00:00:57,270 --> 00:01:02,400
thing now so just to set about what

00:01:00,060 --> 00:01:03,990
we're focusing on I mean we're I'm not a

00:01:02,400 --> 00:01:05,549
web developer so I'm gonna have a

00:01:03,990 --> 00:01:07,500
completely different focus and right now

00:01:05,549 --> 00:01:09,659
I'm working on equipment vendors so I

00:01:07,500 --> 00:01:12,420
will have a very much lower Larry focus

00:01:09,659 --> 00:01:16,469
so no matter what you're working on if

00:01:12,420 --> 00:01:18,060
you're a browser maker if you're a CSP

00:01:16,469 --> 00:01:19,259
content service provider if you're an

00:01:18,060 --> 00:01:20,969
internet service provider or an

00:01:19,259 --> 00:01:22,499
equipment vendor what you care about is

00:01:20,969 --> 00:01:24,689
that the users are happy right

00:01:22,499 --> 00:01:26,999
so offer quality of experience is a

00:01:24,689 --> 00:01:30,240
common goal and of course if you

00:01:26,999 --> 00:01:31,619
something doesn't go bad so if something

00:01:30,240 --> 00:01:34,560
goes bad you want to be able to detect

00:01:31,619 --> 00:01:36,659
it fast if possible if you want just to

00:01:34,560 --> 00:01:39,960
be able to forecast before things go bad

00:01:36,659 --> 00:01:41,280
and if you're good at forecasting you

00:01:39,960 --> 00:01:43,170
could also try to prevent things from

00:01:41,280 --> 00:01:45,539
going bad you know that for your user

00:01:43,170 --> 00:01:46,950
not to churn so detecting quality of the

00:01:45,539 --> 00:01:49,619
gradation quality of experience the

00:01:46,950 --> 00:01:51,420
gradation is important now how do you

00:01:49,619 --> 00:01:52,920
detect quality of experience and all you

00:01:51,420 --> 00:01:55,229
define it well typically would need to

00:01:52,920 --> 00:01:56,819
have a good idea of how the user if they

00:01:55,229 --> 00:01:59,549
are happy or not and then try to

00:01:56,819 --> 00:02:01,619
correlate some of the telemetry so like

00:01:59,549 --> 00:02:02,789
for instance the boomerang is collecting

00:02:01,619 --> 00:02:05,069
a lot of telemetry and we try to

00:02:02,789 --> 00:02:06,929
correlate that with the user quality of

00:02:05,069 --> 00:02:08,369
experience now if you're taking the

00:02:06,929 --> 00:02:10,289
point of view of equipment vendor of

00:02:08,369 --> 00:02:11,670
Internet service provider well you're

00:02:10,289 --> 00:02:13,650
gonna have a little bit harder time

00:02:11,670 --> 00:02:15,120
because you're not in the browser so you

00:02:13,650 --> 00:02:17,030
don't have all the rich telemetry and

00:02:15,120 --> 00:02:18,650
encryption is really

00:02:17,030 --> 00:02:20,360
it's really gonna be painful because

00:02:18,650 --> 00:02:22,580
you're not only gonna see stream

00:02:20,360 --> 00:02:24,410
unencrypted traffic silly we want to do

00:02:22,580 --> 00:02:26,330
something because otherwise your user

00:02:24,410 --> 00:02:27,440
will turn if the user will turn the

00:02:26,330 --> 00:02:30,560
equipment vendor will not be able to

00:02:27,440 --> 00:02:34,580
sell equipment and so there's a loss of

00:02:30,560 --> 00:02:36,800
money as well so it's important to get a

00:02:34,580 --> 00:02:38,410
hand on what's the quality of experience

00:02:36,800 --> 00:02:40,400
and use the quality of experience is

00:02:38,410 --> 00:02:42,620
basically affected by a lot of things

00:02:40,400 --> 00:02:46,040
including for instance the context or

00:02:42,620 --> 00:02:48,620
where is the user work other places if

00:02:46,040 --> 00:02:51,470
it's a pessimistic guy or if it's an old

00:02:48,620 --> 00:02:54,620
lady probably not have an accessory the

00:02:51,470 --> 00:02:56,810
same perception of delay and there so of

00:02:54,620 --> 00:02:59,450
course system influence factor if you

00:02:56,810 --> 00:03:01,610
are dancing in the building ground - or

00:02:59,450 --> 00:03:03,260
- - level floor problem your signal is

00:03:01,610 --> 00:03:05,750
not very good so we have slow

00:03:03,260 --> 00:03:08,630
performance so in order to factor all

00:03:05,750 --> 00:03:09,890
those five be an engineer of course

00:03:08,630 --> 00:03:11,989
you're gonna ask the user but you're

00:03:09,890 --> 00:03:14,090
gonna try to infer these things from

00:03:11,989 --> 00:03:16,100
looking from the system perspective so

00:03:14,090 --> 00:03:17,720
system perspective starts from the lower

00:03:16,100 --> 00:03:20,150
layer the network so over there you will

00:03:17,720 --> 00:03:22,090
able to measure some quality of service

00:03:20,150 --> 00:03:24,680
indication these will in turn affect

00:03:22,090 --> 00:03:26,810
application performance application

00:03:24,680 --> 00:03:29,440
metrics application QoS metrics like the

00:03:26,810 --> 00:03:32,810
one that boomerang was reporting or

00:03:29,440 --> 00:03:35,209
other like webpagetest are reporting to

00:03:32,810 --> 00:03:36,860
you some telemetry and from that we have

00:03:35,209 --> 00:03:38,660
an influence on the way in which the

00:03:36,860 --> 00:03:42,350
user experience in the browsing behavior

00:03:38,660 --> 00:03:43,790
and so what you're going to do is that

00:03:42,350 --> 00:03:45,829
you're going to be able to measure some

00:03:43,790 --> 00:03:47,360
of these metrics like from an end-to-end

00:03:45,829 --> 00:03:48,890
view point what is the latency what is

00:03:47,360 --> 00:03:50,870
the bandwidth which is a packet loss or

00:03:48,890 --> 00:03:53,150
a point-to-point what is the Wi-Fi

00:03:50,870 --> 00:03:54,470
quality of course it doesn't make sense

00:03:53,150 --> 00:03:55,880
if you look at the true put of a single

00:03:54,470 --> 00:03:56,690
connection because you want to put them

00:03:55,880 --> 00:03:58,940
all together

00:03:56,690 --> 00:04:01,070
in order to be able to tell meaningful

00:03:58,940 --> 00:04:02,450
metric from a session viewpoint session

00:04:01,070 --> 00:04:04,670
means for instance if you're looking at

00:04:02,450 --> 00:04:06,799
a web application is going to be page

00:04:04,670 --> 00:04:09,019
real-time or speed index that we're

00:04:06,799 --> 00:04:11,450
going to see later there are also

00:04:09,019 --> 00:04:13,459
session metrics that are correlating

00:04:11,450 --> 00:04:15,079
measureable multiple sessions so for

00:04:13,459 --> 00:04:16,820
instance engagement so measuring if

00:04:15,079 --> 00:04:19,400
you're staying on a website for long

00:04:16,820 --> 00:04:21,100
means that you typically would are happy

00:04:19,400 --> 00:04:23,510
with the quality of experience a serving

00:04:21,100 --> 00:04:25,940
and of course you can go and readily ask

00:04:23,510 --> 00:04:27,590
the user how he feels about the service

00:04:25,940 --> 00:04:29,800
you are giving him so that they can ask

00:04:27,590 --> 00:04:31,659
many user all around the room

00:04:29,800 --> 00:04:32,919
you get five stars and then you do the

00:04:31,659 --> 00:04:35,289
averages you've been your penis score

00:04:32,919 --> 00:04:36,699
you can also ask different thing and of

00:04:35,289 --> 00:04:38,289
course if you know about the device type

00:04:36,699 --> 00:04:40,030
if you have a cheap phone or if you have

00:04:38,289 --> 00:04:41,319
a high-end phone maybe your expectation

00:04:40,030 --> 00:04:45,280
are different maybe the phones are also

00:04:41,319 --> 00:04:46,659
rendering differently so all of that of

00:04:45,280 --> 00:04:48,340
course is very complex so today we're

00:04:46,659 --> 00:04:49,509
going to focus on a subset of eight

00:04:48,340 --> 00:04:52,120
particular we're going to look at the

00:04:49,509 --> 00:04:54,280
web that's the web dev room so we're

00:04:52,120 --> 00:04:55,930
gonna look into performance materialized

00:04:54,280 --> 00:04:58,060
page real time speed index try to see

00:04:55,930 --> 00:05:00,629
how this correlate with the min opinion

00:04:58,060 --> 00:05:03,340
score your user other user feedback and

00:05:00,629 --> 00:05:05,259
of course we're also gonna adopt the

00:05:03,340 --> 00:05:07,479
viewpoints of the lower layer carriers

00:05:05,259 --> 00:05:10,030
where they are only been able to measure

00:05:07,479 --> 00:05:11,620
some weak signals they don't see

00:05:10,030 --> 00:05:13,900
anything about the middle layer because

00:05:11,620 --> 00:05:16,360
squeak HTTP or whatever other kind of

00:05:13,900 --> 00:05:19,090
encryption and so they either want to

00:05:16,360 --> 00:05:20,979
try to from the network us you learn

00:05:19,090 --> 00:05:23,319
something about the application QoS or

00:05:20,979 --> 00:05:26,349
make a big step and go to the quality of

00:05:23,319 --> 00:05:27,580
experience of the user so that's

00:05:26,349 --> 00:05:30,509
basically the agenda for today so

00:05:27,580 --> 00:05:33,520
they're gonna delve into four different

00:05:30,509 --> 00:05:35,830
aspects data collection so the modeling

00:05:33,520 --> 00:05:37,990
parts the metric part and then again

00:05:35,830 --> 00:05:42,759
some method that allows you to go from

00:05:37,990 --> 00:05:44,080
row to top up so we have a putt if

00:05:42,759 --> 00:05:46,479
you're from the net to you need to start

00:05:44,080 --> 00:05:49,900
with your method learning something

00:05:46,479 --> 00:05:52,419
which is metrics about the browser can

00:05:49,900 --> 00:05:53,319
easily measure you need to learn the

00:05:52,419 --> 00:05:54,969
metrics that are useful

00:05:53,319 --> 00:05:56,800
so for doing that you need to couple two

00:05:54,969 --> 00:05:58,509
things you need to couple measurement

00:05:56,800 --> 00:06:00,639
involving the user asking user whether

00:05:58,509 --> 00:06:02,919
they help you or not and building models

00:06:00,639 --> 00:06:04,930
that based on your metric are hopefully

00:06:02,919 --> 00:06:07,900
able to extract the information from

00:06:04,930 --> 00:06:09,129
automatically collected one so in the

00:06:07,900 --> 00:06:10,150
agenda today we're going to work there

00:06:09,129 --> 00:06:13,210
are these top-down so we're going to

00:06:10,150 --> 00:06:14,740
start with with the data collection so

00:06:13,210 --> 00:06:16,750
that a collection typically what you do

00:06:14,740 --> 00:06:18,840
is that you build up some a crowd

00:06:16,750 --> 00:06:22,029
sourcing campaign they have a huge cost

00:06:18,840 --> 00:06:23,680
and there are no perfect campaign in the

00:06:22,029 --> 00:06:24,879
last years we have been doing three type

00:06:23,680 --> 00:06:27,610
of different things we've been asking

00:06:24,879 --> 00:06:29,789
user what is the mini opinion score so

00:06:27,610 --> 00:06:32,590
write your experience from one to five

00:06:29,789 --> 00:06:34,150
we have also been asking user when do

00:06:32,590 --> 00:06:35,889
you think that the page was finished or

00:06:34,150 --> 00:06:38,560
what is your user perceived page load

00:06:35,889 --> 00:06:40,570
time or seeing two pages at the same

00:06:38,560 --> 00:06:42,729
time which page did you think it finish

00:06:40,570 --> 00:06:43,810
at first so to get a little bit an idea

00:06:42,729 --> 00:06:46,410
of how the user

00:06:43,810 --> 00:06:49,210
even the web and finally with Wikipedia

00:06:46,410 --> 00:06:50,950
live in collaboration with Wikipedia we

00:06:49,210 --> 00:06:53,740
started asking the user whether they are

00:06:50,950 --> 00:06:56,380
satisfied with experience they have

00:06:53,740 --> 00:06:59,919
while browsing Wikipedia so of course

00:06:56,380 --> 00:07:01,840
there's no perfect solution in the first

00:06:59,919 --> 00:07:05,470
data set we were doing lab experiments

00:07:01,840 --> 00:07:07,590
this means that we were having few panel

00:07:05,470 --> 00:07:10,540
of people that were typically volunteer

00:07:07,590 --> 00:07:12,490
close 150 250 people recruiting

00:07:10,540 --> 00:07:13,930
universities so you have very specific

00:07:12,490 --> 00:07:17,620
class of population it will definitely

00:07:13,930 --> 00:07:19,030
not fit the grandma's behavior we the

00:07:17,620 --> 00:07:20,830
good side is that we were using real

00:07:19,030 --> 00:07:22,960
servers real protocols we were able to

00:07:20,830 --> 00:07:25,660
control the conditions but the number of

00:07:22,960 --> 00:07:27,850
web pages of course is not as completely

00:07:25,660 --> 00:07:28,900
representative of the Internet so then

00:07:27,850 --> 00:07:31,060
you can do something else

00:07:28,900 --> 00:07:32,740
stepping up by moving into crowdsourcing

00:07:31,060 --> 00:07:34,240
so you have for instance Amazon medical

00:07:32,740 --> 00:07:36,520
work mechanical turk so you can leverage

00:07:34,240 --> 00:07:39,070
a large pool of people but over there

00:07:36,520 --> 00:07:40,600
you need to you cannot let them access a

00:07:39,070 --> 00:07:42,490
web server so you will typically put

00:07:40,600 --> 00:07:45,190
videos of the web page rendering process

00:07:42,490 --> 00:07:47,560
so these not really exactly like Rosie

00:07:45,190 --> 00:07:48,850
you reach a larger audience but this

00:07:47,560 --> 00:07:51,490
what you're also interested in getting

00:07:48,850 --> 00:07:53,800
paid for for the task is so you need to

00:07:51,490 --> 00:07:55,830
filter out a lot of people that are just

00:07:53,800 --> 00:07:59,020
there to make money

00:07:55,830 --> 00:07:59,979
so let's film we did with Wikipedia is

00:07:59,020 --> 00:08:02,200
very interesting because we have

00:07:59,979 --> 00:08:05,140
actually we are polling the user so

00:08:02,200 --> 00:08:08,050
there's 1 billion pages visit monthly

00:08:05,140 --> 00:08:10,240
roughly and a tiny fraction of that is

00:08:08,050 --> 00:08:11,590
going to be polled for performance

00:08:10,240 --> 00:08:13,840
metric and a tiny fraction of that is

00:08:11,590 --> 00:08:15,580
going to also be pulled for binary

00:08:13,840 --> 00:08:16,960
feedback it's slightly more than minor

00:08:15,580 --> 00:08:18,610
feedback about whether they were happy

00:08:16,960 --> 00:08:20,650
or not so it is good because you're

00:08:18,610 --> 00:08:22,690
gonna pull users that are in the real

00:08:20,650 --> 00:08:24,669
service from the service they like the

00:08:22,690 --> 00:08:26,050
service they use typically the downside

00:08:24,669 --> 00:08:28,120
is that you have a huge Atarot Jannetty

00:08:26,050 --> 00:08:31,270
remember on top of my head that we were

00:08:28,120 --> 00:08:32,950
polling on 65,000 people they had they

00:08:31,270 --> 00:08:35,110
were looking at 42,000 different

00:08:32,950 --> 00:08:40,270
wikipedia pages 3,000

00:08:35,110 --> 00:08:42,310
networks of 250 devices and 45 browsers

00:08:40,270 --> 00:08:43,930
so there's a lot of heterogeneity and so

00:08:42,310 --> 00:08:46,750
building a single model is not

00:08:43,930 --> 00:08:48,730
necessarily trivial when I'm putting the

00:08:46,750 --> 00:08:51,370
icon there is that the data set are

00:08:48,730 --> 00:08:52,870
available so if peoples are interested

00:08:51,370 --> 00:08:55,570
there are people that are doing research

00:08:52,870 --> 00:08:57,670
on that like we're saying before sharing

00:08:55,570 --> 00:09:01,570
tools is important sharing performance

00:08:57,670 --> 00:09:02,980
in evaluation is important sharing the

00:09:01,570 --> 00:09:04,540
data is even more important because it

00:09:02,980 --> 00:09:06,250
allows you to replicate and see whether

00:09:04,540 --> 00:09:09,100
the performance that are reported or -

00:09:06,250 --> 00:09:12,340
or not so now that you've got the data

00:09:09,100 --> 00:09:15,250
ok cool what we do well basically we're

00:09:12,340 --> 00:09:18,070
gonna have a way to go from the data so

00:09:15,250 --> 00:09:19,810
our Y to find some function that based

00:09:18,070 --> 00:09:23,680
on some of the things that we are able

00:09:19,810 --> 00:09:25,990
to measure like our incognita Y plug

00:09:23,680 --> 00:09:27,910
into a formula F is going to be able to

00:09:25,990 --> 00:09:31,840
tell us what is magically if you want

00:09:27,910 --> 00:09:33,490
the user performance so here by each

00:09:31,840 --> 00:09:35,410
typically people use a single scalar

00:09:33,490 --> 00:09:36,970
metric Janell the page load time the

00:09:35,410 --> 00:09:38,980
function has been predetermined by an

00:09:36,970 --> 00:09:41,440
expert and there are typically two

00:09:38,980 --> 00:09:42,850
approaches that are being used one is a

00:09:41,440 --> 00:09:46,210
queueing support disease we are using

00:09:42,850 --> 00:09:48,460
our exponential model and here with a

00:09:46,210 --> 00:09:50,200
logarithmic model which is said to the

00:09:48,460 --> 00:09:52,960
weber Fechner law which is applicable

00:09:50,200 --> 00:09:55,000
heavier model that tells that the human

00:09:52,960 --> 00:09:57,730
response for stimulus is locally Tomica

00:09:55,000 --> 00:09:59,410
related and this is for instance used by

00:09:57,730 --> 00:10:00,670
a standard so what you do we do a lot of

00:09:59,410 --> 00:10:02,080
measurement all the points here are

00:10:00,670 --> 00:10:04,060
different answer from the different user

00:10:02,080 --> 00:10:06,030
and then you do a fitting and here the

00:10:04,060 --> 00:10:09,070
fitting you we can be happy with that

00:10:06,030 --> 00:10:11,020
now there are limits because typically

00:10:09,070 --> 00:10:13,810
there are a lot of metrics although

00:10:11,020 --> 00:10:15,460
telemetry that is made from browsers and

00:10:13,810 --> 00:10:17,260
so here we are only using a single

00:10:15,460 --> 00:10:19,780
metric so you can go one step further

00:10:17,260 --> 00:10:21,730
and instead of picking a single metric

00:10:19,780 --> 00:10:23,830
that you like and a single function that

00:10:21,730 --> 00:10:25,770
you like and although the fitting seems

00:10:23,830 --> 00:10:28,480
nice you could do something which is

00:10:25,770 --> 00:10:31,060
much learning driven so basically having

00:10:28,480 --> 00:10:32,770
a factor of input features and having an

00:10:31,060 --> 00:10:34,510
automated way to select what is the

00:10:32,770 --> 00:10:36,910
optimal fitting of the function by

00:10:34,510 --> 00:10:38,440
minimizing some error so here the trick

00:10:36,910 --> 00:10:39,820
is that whenever you select a very

00:10:38,440 --> 00:10:41,680
specific machine learning algorithm

00:10:39,820 --> 00:10:43,210
you're implicitly selecting which are

00:10:41,680 --> 00:10:46,270
the type of function that you will go a

00:10:43,210 --> 00:10:47,860
little bit to learn and here you see

00:10:46,270 --> 00:10:51,010
that you have a slight gain with respect

00:10:47,860 --> 00:10:53,140
to the typical models that you have here

00:10:51,010 --> 00:10:55,090
by considering more metrics of course

00:10:53,140 --> 00:10:56,620
there are different models that are

00:10:55,090 --> 00:10:58,750
available we're not going to delve in

00:10:56,620 --> 00:10:59,950
the detail of that just to say that for

00:10:58,750 --> 00:11:01,630
me there's still some room for

00:10:59,950 --> 00:11:03,940
improvement from going to the future

00:11:01,630 --> 00:11:05,620
that we have to the user experience but

00:11:03,940 --> 00:11:08,860
still you have a good and quite high

00:11:05,620 --> 00:11:10,540
correlation so this brings us to the

00:11:08,860 --> 00:11:11,260
metric what are the metrics that we can

00:11:10,540 --> 00:11:13,840
work on so

00:11:11,260 --> 00:11:15,640
in order to be quite clear about

00:11:13,840 --> 00:11:17,830
everything I have a very small animation

00:11:15,640 --> 00:11:20,020
about how is the web page loading

00:11:17,830 --> 00:11:22,570
process after you go and click on a link

00:11:20,020 --> 00:11:24,370
so we start something that you're gonna

00:11:22,570 --> 00:11:25,660
start downloading and at some point you

00:11:24,370 --> 00:11:27,820
will have an event that is gonna be

00:11:25,660 --> 00:11:29,380
fired by the browser document object

00:11:27,820 --> 00:11:30,870
model so at this point you know the

00:11:29,380 --> 00:11:32,950
structural page and you can start

00:11:30,870 --> 00:11:34,860
putting things around so we have a

00:11:32,950 --> 00:11:38,200
visual progress of the page that

00:11:34,860 --> 00:11:40,000
increases from zero to upward then you

00:11:38,200 --> 00:11:41,860
keep it downloading more things until at

00:11:40,000 --> 00:11:44,380
some point which is called typically

00:11:41,860 --> 00:11:46,240
above default all the portion of the all

00:11:44,380 --> 00:11:48,100
the visible portion of the page has been

00:11:46,240 --> 00:11:50,770
downloaded and shown to the screen

00:11:48,100 --> 00:11:52,780
that's called the ATF and your visual

00:11:50,770 --> 00:11:54,700
progress is increasing and you can

00:11:52,780 --> 00:11:56,830
represent here your visual progress is a

00:11:54,700 --> 00:11:58,750
function X of T that is growing from 0

00:11:56,830 --> 00:12:00,340
to 1 where 1 is basically everything

00:11:58,750 --> 00:12:02,980
that needed to be rendered for the page

00:12:00,340 --> 00:12:04,990
to be visually complete is finished so X

00:12:02,980 --> 00:12:06,490
of T of course you can also do something

00:12:04,990 --> 00:12:08,440
a little bit more fancy so basically

00:12:06,490 --> 00:12:11,530
here the integral of the residual of

00:12:08,440 --> 00:12:14,380
this function is the area the gray

00:12:11,530 --> 00:12:15,670
shaded area above the core and this gray

00:12:14,380 --> 00:12:18,130
shaded there above the curve is what

00:12:15,670 --> 00:12:19,630
Google defined the speed index so we're

00:12:18,130 --> 00:12:21,490
gonna come into that in a moment and

00:12:19,630 --> 00:12:22,840
then of course I mean you can keep

00:12:21,490 --> 00:12:24,580
downloading more content that it's not

00:12:22,840 --> 00:12:26,290
necessarily available and immediately

00:12:24,580 --> 00:12:28,300
visible but it's gonna be available when

00:12:26,290 --> 00:12:29,740
you scroll and that's when all the

00:12:28,300 --> 00:12:32,410
content is loaded is typically the page

00:12:29,740 --> 00:12:34,630
load time so now we have two type of

00:12:32,410 --> 00:12:35,980
metrics so one are the time instant

00:12:34,630 --> 00:12:37,930
metrics we have for instance ten to the

00:12:35,980 --> 00:12:39,610
first byte Dom 10 to the first paint

00:12:37,930 --> 00:12:42,340
about default Pedro x is a very specific

00:12:39,610 --> 00:12:43,450
time which are important to somebody and

00:12:42,340 --> 00:12:45,790
then you have something else which is

00:12:43,450 --> 00:12:48,760
the integral form of it which is

00:12:45,790 --> 00:12:50,710
basically looking at all the air above

00:12:48,760 --> 00:12:52,480
the curve so why this thing intuitively

00:12:50,710 --> 00:12:55,090
is important imagine that you have two

00:12:52,480 --> 00:12:56,680
realization of two pages that are

00:12:55,090 --> 00:12:58,990
exactly the same page real time so they

00:12:56,680 --> 00:13:01,330
finish exactly the same time but this

00:12:58,990 --> 00:13:03,280
one shows out for the content very fast

00:13:01,330 --> 00:13:06,430
and this one shows out of the content

00:13:03,280 --> 00:13:08,140
almost much more later right so in which

00:13:06,430 --> 00:13:10,630
of the two you would be happier in this

00:13:08,140 --> 00:13:12,430
one so whenever the area above the curve

00:13:10,630 --> 00:13:15,580
is smaller then it's better and it's

00:13:12,430 --> 00:13:17,440
faster so one additional comment is that

00:13:15,580 --> 00:13:19,180
given that you are integrating something

00:13:17,440 --> 00:13:21,400
that is a dimensional I mean integrating

00:13:19,180 --> 00:13:24,190
over time also the air above the curve

00:13:21,400 --> 00:13:25,089
is a time in dimension so physically if

00:13:24,190 --> 00:13:26,860
you are engineers

00:13:25,089 --> 00:13:28,540
would think that over time is a time

00:13:26,860 --> 00:13:31,360
unit of measure and you can think it is

00:13:28,540 --> 00:13:34,449
a virtual time that is explicit ating

00:13:31,360 --> 00:13:36,430
alphas was the the rendering process now

00:13:34,449 --> 00:13:38,110
you can define a family of metric like

00:13:36,430 --> 00:13:40,209
this and depending on what you put is X

00:13:38,110 --> 00:13:42,279
of T you're gonna have the speed index

00:13:40,209 --> 00:13:44,319
if you're looking for instance at the

00:13:42,279 --> 00:13:46,660
difference in the east gram that were

00:13:44,319 --> 00:13:48,430
shown for the colors on the page you

00:13:46,660 --> 00:13:50,050
have room speed index that is measuring

00:13:48,430 --> 00:13:51,699
the areas that each of the different

00:13:50,050 --> 00:13:53,559
object that are drawn on the page are

00:13:51,699 --> 00:13:55,449
gonna put and they're going to compare

00:13:53,559 --> 00:13:58,389
with the amount of rectangular should

00:13:55,449 --> 00:14:00,639
have been drawn at the end you can look

00:13:58,389 --> 00:14:02,620
at as the same PSSI perceptual speed

00:14:00,639 --> 00:14:04,269
index using sec metric which are much

00:14:02,620 --> 00:14:07,149
more advanced so all of that is very

00:14:04,269 --> 00:14:08,649
good because is visual progress but

00:14:07,149 --> 00:14:10,029
there are downsides so for instance you

00:14:08,649 --> 00:14:11,709
can only measure them in browsers and

00:14:10,029 --> 00:14:13,209
some of them are actually processing

00:14:11,709 --> 00:14:15,339
intensive so if you need to do a seam if

00:14:13,209 --> 00:14:16,779
there's a lot of computation in to do so

00:14:15,339 --> 00:14:19,329
some years ago we were proposing to do

00:14:16,779 --> 00:14:21,699
as a proxy of this mod mansard metric

00:14:19,329 --> 00:14:23,709
they were very simple inputs like object

00:14:21,699 --> 00:14:25,360
index or bytes just looking at the bytes

00:14:23,709 --> 00:14:28,779
that are coming you would get a pretty

00:14:25,360 --> 00:14:30,490
decent idea of what is coming to your

00:14:28,779 --> 00:14:32,379
browser if it's coming fast or not we're

00:14:30,490 --> 00:14:35,379
gonna see a little bit later if it's

00:14:32,379 --> 00:14:37,660
work or not good side is that you can do

00:14:35,379 --> 00:14:39,879
it in layer frame the network is correct

00:14:37,660 --> 00:14:41,649
with speed index doesn't necessarily is

00:14:39,879 --> 00:14:44,769
good for creative experience so that's a

00:14:41,649 --> 00:14:46,839
question that you need to address and

00:14:44,769 --> 00:14:50,050
I'm not going to go into these kind of

00:14:46,839 --> 00:14:51,339
details but you can also have effect in

00:14:50,050 --> 00:14:53,230
for instance the cutoff or the integral

00:14:51,339 --> 00:14:54,459
in order to optimize some of those

00:14:53,230 --> 00:14:56,800
metrics but I'm not going to go into

00:14:54,459 --> 00:14:58,449
this little so now if you are in the

00:14:56,800 --> 00:14:59,769
browser or if you're a content service

00:14:58,449 --> 00:15:01,509
provider what you have is that you are a

00:14:59,769 --> 00:15:03,309
pretty good picture of everything that

00:15:01,509 --> 00:15:05,589
is happening you per domain the vision

00:15:03,309 --> 00:15:07,569
of all the different objects also the

00:15:05,589 --> 00:15:09,550
type if there are images or not CSS

00:15:07,569 --> 00:15:11,679
whatever and you can reconstruct this

00:15:09,550 --> 00:15:15,610
picture with quite

00:15:11,679 --> 00:15:17,769
accuracy now if you are in in the dark

00:15:15,610 --> 00:15:19,720
so if you are an HP 400 people vendor

00:15:17,769 --> 00:15:21,519
what you will see is basically a series

00:15:19,720 --> 00:15:22,689
of packets coming from different flows

00:15:21,519 --> 00:15:24,220
and the only thing that you're gonna

00:15:22,689 --> 00:15:26,170
read is that okay these are this is a

00:15:24,220 --> 00:15:28,929
packet this is a packet full packet size

00:15:26,170 --> 00:15:31,019
until and it's a smaller one so what do

00:15:28,929 --> 00:15:34,569
you make out of it in order to

00:15:31,019 --> 00:15:36,309
extrapolate from this so again I'm not

00:15:34,569 --> 00:15:37,080
going to go into a lot of details I'll

00:15:36,309 --> 00:15:39,120
let

00:15:37,080 --> 00:15:41,520
rather gonna show you why this thing

00:15:39,120 --> 00:15:42,750
could work but basically the idea is if

00:15:41,520 --> 00:15:46,950
you are familiar with machine learning

00:15:42,750 --> 00:15:48,600
you need to perform some amount some

00:15:46,950 --> 00:15:50,490
really simple amount of signal

00:15:48,600 --> 00:15:52,470
processing in order to make your input

00:15:50,490 --> 00:15:54,180
to be homogeneous we're using supervised

00:15:52,470 --> 00:15:55,740
technique so supervised technique means

00:15:54,180 --> 00:15:59,520
that we need to have exactly the same

00:15:55,740 --> 00:16:01,260
input and then different models that we

00:15:59,520 --> 00:16:02,640
are using extreme gradient boosting

00:16:01,260 --> 00:16:04,590
which is an example method based on

00:16:02,640 --> 00:16:06,060
trays or 1d convolutional neural network

00:16:04,590 --> 00:16:08,460
what we do is that we present them with

00:16:06,060 --> 00:16:11,100
lot of samples and with a look this

00:16:08,460 --> 00:16:14,130
sample and we also explain them for

00:16:11,100 --> 00:16:15,840
instance had this above default value we

00:16:14,130 --> 00:16:17,340
build another model provided the same

00:16:15,840 --> 00:16:19,650
example and providing what is the page

00:16:17,340 --> 00:16:21,030
load time or the speed index of any

00:16:19,650 --> 00:16:23,100
metric that you're interested and we

00:16:21,030 --> 00:16:25,350
provide many samples to train a model

00:16:23,100 --> 00:16:28,380
and we test it over previously unseen

00:16:25,350 --> 00:16:31,620
cases to give an intuition why this

00:16:28,380 --> 00:16:33,090
should work so here we have the web page

00:16:31,620 --> 00:16:35,220
rendering so this is basically the user

00:16:33,090 --> 00:16:36,990
here is what we see in the browser where

00:16:35,220 --> 00:16:38,520
every burst is going to be one object

00:16:36,990 --> 00:16:40,770
and we have one color per different

00:16:38,520 --> 00:16:43,800
domain actually we're presenting only

00:16:40,770 --> 00:16:45,000
the top three domains and the others we

00:16:43,800 --> 00:16:46,680
are using the same color that was the

00:16:45,000 --> 00:16:49,080
pitch would be really really to color it

00:16:46,680 --> 00:16:51,330
and here it is what you see from the

00:16:49,080 --> 00:16:52,380
network so we're gonna have one packet

00:16:51,330 --> 00:16:54,300
with a bit more we're aggregating

00:16:52,380 --> 00:16:56,400
packets in 10 milliseconds and then

00:16:54,300 --> 00:16:59,070
you're gonna see one color per IP server

00:16:56,400 --> 00:17:00,900
so when I'm starting if I click on the

00:16:59,070 --> 00:17:04,170
right place you see that okay now this

00:17:00,900 --> 00:17:06,060
is a Chinese web page so it starts late

00:17:04,170 --> 00:17:07,320
at some point you see those finger

00:17:06,060 --> 00:17:08,760
progra saying here there was a big

00:17:07,320 --> 00:17:11,130
object this big object has been a lot of

00:17:08,760 --> 00:17:12,600
multiple packets same thing here the

00:17:11,130 --> 00:17:14,699
green packets correspond to this big

00:17:12,600 --> 00:17:16,830
object and you see that these course are

00:17:14,699 --> 00:17:18,420
slightly different but you see that

00:17:16,830 --> 00:17:22,470
there is some similarity right they are

00:17:18,420 --> 00:17:24,750
not completely different and indeed if

00:17:22,470 --> 00:17:26,790
you systematically perform this

00:17:24,750 --> 00:17:28,020
experiment this was just one example to

00:17:26,790 --> 00:17:30,450
show you how these things look like in

00:17:28,020 --> 00:17:32,070
for real then you can go and make an

00:17:30,450 --> 00:17:33,810
experiment where you monitoring the

00:17:32,070 --> 00:17:35,940
network so you taking the real encrypted

00:17:33,810 --> 00:17:37,290
traffic you are monitoring the browser

00:17:35,940 --> 00:17:38,910
so at the ground truth so you have the

00:17:37,290 --> 00:17:41,610
above default whatever metric you're

00:17:38,910 --> 00:17:45,210
interested in and you can repeat this

00:17:41,610 --> 00:17:47,280
process and try to see extrapolates own

00:17:45,210 --> 00:17:49,640
accuracy number so here is the only

00:17:47,280 --> 00:17:51,330
accuracy picture that I'm gonna show

00:17:49,640 --> 00:17:52,950
this is report

00:17:51,330 --> 00:17:54,269
in the absolute error in milliseconds I

00:17:52,950 --> 00:17:56,309
could use the median and this is the

00:17:54,269 --> 00:17:58,110
25th percentile and this is the 75

00:17:56,309 --> 00:17:59,820
percentile so and this is basically in

00:17:58,110 --> 00:18:00,899
the 75 per case your error is going to

00:17:59,820 --> 00:18:02,250
be much lower than this and in the

00:18:00,899 --> 00:18:04,380
median case is going to be this one and

00:18:02,250 --> 00:18:07,320
you can see here we have two different

00:18:04,380 --> 00:18:09,600
approach one is we even without machine

00:18:07,320 --> 00:18:11,250
learning are not going to explain why

00:18:09,600 --> 00:18:13,289
the colors before in the picture had a

00:18:11,250 --> 00:18:14,580
mathematical interpretation but I didn't

00:18:13,289 --> 00:18:17,190
want to bring it up today it's not a

00:18:14,580 --> 00:18:19,049
point but with an algorithm a ston that

00:18:17,190 --> 00:18:22,019
we can have already something that it's

00:18:19,049 --> 00:18:24,720
going to learn only a single function

00:18:22,019 --> 00:18:26,100
which is the byte index and we can

00:18:24,720 --> 00:18:27,480
approximate the white index learn from

00:18:26,100 --> 00:18:29,880
the network with the approximate with

00:18:27,480 --> 00:18:32,760
the application byte index that we learn

00:18:29,880 --> 00:18:35,370
in the browser and that one has a six

00:18:32,760 --> 00:18:36,630
percent error on top of that this

00:18:35,370 --> 00:18:39,240
without machine learning is a very

00:18:36,630 --> 00:18:40,529
simple online algorithm on top of that

00:18:39,240 --> 00:18:42,450
you can add machine learning and you can

00:18:40,529 --> 00:18:44,669
compensate for for deserters so you can

00:18:42,450 --> 00:18:46,769
reach a lower error and then you can

00:18:44,669 --> 00:18:48,389
learn generalize to any metric so we're

00:18:46,769 --> 00:18:50,190
learning the below the page load time

00:18:48,389 --> 00:18:52,230
the ability index the speed index or

00:18:50,190 --> 00:18:53,250
room speed index the Dom if you if

00:18:52,230 --> 00:18:56,309
you're interested in learning the Dom

00:18:53,250 --> 00:19:00,029
with this kind of errors so we did tests

00:18:56,309 --> 00:19:01,559
with orange on number of pages that we

00:19:00,029 --> 00:19:03,630
were never seen before in number

00:19:01,559 --> 00:19:05,490
networks were not seen before and these

00:19:03,630 --> 00:19:07,440
are the accuracy estimated indeed in

00:19:05,490 --> 00:19:09,990
those setting so it's a pretty good

00:19:07,440 --> 00:19:11,190
portable and ok not to make an

00:19:09,990 --> 00:19:12,870
advertisement but given that the

00:19:11,190 --> 00:19:15,269
algorithm works we are supporting it

00:19:12,870 --> 00:19:17,340
into our work products now there was one

00:19:15,269 --> 00:19:19,500
catch they didn't talk in this work due

00:19:17,340 --> 00:19:22,710
to lack of time is that we are also able

00:19:19,500 --> 00:19:24,840
to end a multi session so if I go to

00:19:22,710 --> 00:19:28,669
back here we see that there are a lot of

00:19:24,840 --> 00:19:28,669
packets coming from a lot of different

00:19:28,909 --> 00:19:33,179
flows but you need before to be able to

00:19:31,679 --> 00:19:35,519
isolate the flows that are going to go

00:19:33,179 --> 00:19:36,990
to the same session so this is something

00:19:35,519 --> 00:19:38,309
that you need in order to be able to

00:19:36,990 --> 00:19:39,870
apply your machine learning technique

00:19:38,309 --> 00:19:42,750
and it is also something that is done

00:19:39,870 --> 00:19:46,169
but we just didn't talk in for lack of

00:19:42,750 --> 00:19:47,970
time now so basically after okay now is

00:19:46,169 --> 00:19:51,210
where we stand so where we could go to

00:19:47,970 --> 00:19:52,860
go further so I'm going to just took

00:19:51,210 --> 00:19:54,779
about three three couple of three

00:19:52,860 --> 00:19:56,760
ideas so for people that are familiar

00:19:54,779 --> 00:19:59,159
with machine learning unfortunately in

00:19:56,760 --> 00:20:01,020
the web QE domain we're still at expert

00:19:59,159 --> 00:20:02,549
driven feature engineers so basically we

00:20:01,020 --> 00:20:04,590
have somebody that is defining speed

00:20:02,549 --> 00:20:06,480
index and why should be

00:20:04,590 --> 00:20:08,669
PDX seams are very natural and very

00:20:06,480 --> 00:20:11,850
bright idea but we have no clue whether

00:20:08,669 --> 00:20:14,429
it is really a proxy for quality of

00:20:11,850 --> 00:20:15,779
experience so a better approach I'm not

00:20:14,429 --> 00:20:18,390
saying more explainable so it's less

00:20:15,779 --> 00:20:20,970
intuitive would take raw input ro

00:20:18,390 --> 00:20:23,429
sensory data from the user and try to do

00:20:20,970 --> 00:20:24,990
what to learn the features by the

00:20:23,429 --> 00:20:27,270
learning process learning process gonna

00:20:24,990 --> 00:20:29,039
in the neural network through an atom

00:20:27,270 --> 00:20:30,390
throw back propagation is going to

00:20:29,039 --> 00:20:32,809
create some features that are the most

00:20:30,390 --> 00:20:35,580
relevant in order to find and explain

00:20:32,809 --> 00:20:38,549
what the why the user voted a given

00:20:35,580 --> 00:20:40,590
score okay so that's definitely not

00:20:38,549 --> 00:20:42,480
interpretable is more versatile the

00:20:40,590 --> 00:20:45,929
downside it is requires a lot of sample

00:20:42,480 --> 00:20:47,399
okay so here what we did was taking

00:20:45,929 --> 00:20:49,620
packets and learning any of this

00:20:47,399 --> 00:20:51,870
function similarly we could use these

00:20:49,620 --> 00:20:53,850
inputs and trying to learn functions

00:20:51,870 --> 00:20:55,500
which are user happiness course getting

00:20:53,850 --> 00:20:57,990
the data is difficult because you would

00:20:55,500 --> 00:20:59,730
want to be as less intrusive as possible

00:20:57,990 --> 00:21:00,809
so if you need to put sensors like this

00:20:59,730 --> 00:21:03,210
maybe you're affecting the user

00:21:00,809 --> 00:21:05,340
experience and other things that may be

00:21:03,210 --> 00:21:07,529
okay you can leverage so I know people

00:21:05,340 --> 00:21:08,700
that are working on happiness

00:21:07,529 --> 00:21:11,340
recognition through photo recognition

00:21:08,700 --> 00:21:12,929
but over there if you're happy it may be

00:21:11,340 --> 00:21:14,340
for the content of the message that you

00:21:12,929 --> 00:21:16,710
receive or the page that you're visiting

00:21:14,340 --> 00:21:18,840
and not whether the experience you're

00:21:16,710 --> 00:21:21,450
loading that page was was happy what was

00:21:18,840 --> 00:21:24,330
good so it's quite difficult to get the

00:21:21,450 --> 00:21:25,500
sex story part working second thing is

00:21:24,330 --> 00:21:27,450
that I was speaking about single model

00:21:25,500 --> 00:21:30,570
and actually we need single models

00:21:27,450 --> 00:21:32,640
because they are easy deployment but of

00:21:30,570 --> 00:21:35,730
course wide web is really really large

00:21:32,640 --> 00:21:37,200
so for instance Wikipedia as not it's

00:21:35,730 --> 00:21:39,690
not image intensive and you will have

00:21:37,200 --> 00:21:41,580
other websites that are mostly done by

00:21:39,690 --> 00:21:44,340
images or video so all can a single

00:21:41,580 --> 00:21:46,440
model fits all so of course you increase

00:21:44,340 --> 00:21:48,929
the accuracy you should go purple mod

00:21:46,440 --> 00:21:50,399
purple page here just an example picture

00:21:48,929 --> 00:21:53,100
where you have black line is one average

00:21:50,399 --> 00:21:54,570
model and these are all the points

00:21:53,100 --> 00:21:56,070
you're getting and of course if you have

00:21:54,570 --> 00:21:58,049
many per page models they're gonna be

00:21:56,070 --> 00:21:59,730
they're gonna have a better fit now the

00:21:58,049 --> 00:22:01,440
problem is that inherently this process

00:21:59,730 --> 00:22:03,990
is not scalable so how to make it

00:22:01,440 --> 00:22:05,909
scalable well by prioritizing things

00:22:03,990 --> 00:22:08,100
that are more important for instance if

00:22:05,909 --> 00:22:10,740
you have top 100 webpage you can build

00:22:08,100 --> 00:22:12,659
reliably model for the top 100 pages

00:22:10,740 --> 00:22:15,149
that are more frequently visited by by

00:22:12,659 --> 00:22:17,420
people then you can have a second

00:22:15,149 --> 00:22:19,700
approach in which we cluster

00:22:17,420 --> 00:22:21,670
top 1 million webpages for instance here

00:22:19,700 --> 00:22:24,920
you see number of clusters out of which

00:22:21,670 --> 00:22:26,060
24 pages were extracted and inside each

00:22:24,920 --> 00:22:28,160
each of these classes there are

00:22:26,060 --> 00:22:29,600
thousands of pages this cluster have

00:22:28,160 --> 00:22:31,190
similarity in terms of the number of

00:22:29,600 --> 00:22:33,170
domains the number of objects and at the

00:22:31,190 --> 00:22:34,820
size of the page so there are higher

00:22:33,170 --> 00:22:36,860
chances that if you build models that

00:22:34,820 --> 00:22:38,480
are accurate for pages in this class

00:22:36,860 --> 00:22:40,520
then you're going to also be able to

00:22:38,480 --> 00:22:42,710
cover more accurately the top 1 million

00:22:40,520 --> 00:22:44,360
and then of course okay for the rest to

00:22:42,710 --> 00:22:46,310
top 1 billion pages you're gonna use a

00:22:44,360 --> 00:22:48,470
single average model and pray it will

00:22:46,310 --> 00:22:50,840
work but at least you're gonna already

00:22:48,470 --> 00:22:53,050
have in a better operational point in

00:22:50,840 --> 00:22:55,580
the accuracy vs. scalability trade-off

00:22:53,050 --> 00:22:57,380
then finally comment which is a

00:22:55,580 --> 00:22:59,120
community comment if you are working in

00:22:57,380 --> 00:23:01,790
this space the first thing you need is

00:22:59,120 --> 00:23:04,880
data so keep collecting and sharing data

00:23:01,790 --> 00:23:07,970
is very important so I'm very happy that

00:23:04,880 --> 00:23:10,040
finally we with working with Wikimedia

00:23:07,970 --> 00:23:12,130
we were able to release data set in a

00:23:10,040 --> 00:23:14,180
properly anonymized form that was

00:23:12,130 --> 00:23:16,220
protecting successfully the privacy of

00:23:14,180 --> 00:23:17,900
people and also letting people doing the

00:23:16,220 --> 00:23:20,870
research not to build models better than

00:23:17,900 --> 00:23:21,800
the one we built so we you need to take

00:23:20,870 --> 00:23:23,210
into account when you go to the

00:23:21,800 --> 00:23:25,190
supermarket you already find this

00:23:23,210 --> 00:23:26,690
machine right and asking you are you

00:23:25,190 --> 00:23:28,370
happy or not and you click on it and you

00:23:26,690 --> 00:23:30,590
don't think even about it when you're

00:23:28,370 --> 00:23:32,060
calling over Skype or Facebook at the

00:23:30,590 --> 00:23:35,510
end of a call there's something that is

00:23:32,060 --> 00:23:37,070
calling as can you rate your call also

00:23:35,510 --> 00:23:38,870
my phone started asking me did you find

00:23:37,070 --> 00:23:40,790
the suggestion useful so to a binary

00:23:38,870 --> 00:23:42,920
feedback from him and this is them from

00:23:40,790 --> 00:23:45,230
Wikipedia so what you would you gain for

00:23:42,920 --> 00:23:48,650
keeping this date steady data collection

00:23:45,230 --> 00:23:50,840
is two things one you have until your

00:23:48,650 --> 00:23:52,730
model will not be good you really have

00:23:50,840 --> 00:23:54,590
some information from the user so you

00:23:52,730 --> 00:23:57,050
already know if something happens that

00:23:54,590 --> 00:23:58,760
is users of your service that are

00:23:57,050 --> 00:24:00,620
telling you directly and you don't need

00:23:58,760 --> 00:24:01,910
to go over twitter and try to understand

00:24:00,620 --> 00:24:03,980
if the user are complaining about your

00:24:01,910 --> 00:24:05,360
service through other channel second

00:24:03,980 --> 00:24:08,150
this continuous stream of data it's

00:24:05,360 --> 00:24:09,620
going to also be able to make your model

00:24:08,150 --> 00:24:12,620
better or if there's anything that

00:24:09,620 --> 00:24:14,000
changes next protocol so we had the HTTP

00:24:12,620 --> 00:24:17,150
to now it's going to be HTTP three

00:24:14,000 --> 00:24:18,410
sooner sooner over HTTP over quick maybe

00:24:17,150 --> 00:24:20,660
your model need to be retrained so you

00:24:18,410 --> 00:24:22,730
will need to have this kind of data and

00:24:20,660 --> 00:24:24,230
if the producer population is large

00:24:22,730 --> 00:24:26,810
enough they're also limited on cited

00:24:24,230 --> 00:24:29,720
only it is a risk of annoying users if

00:24:26,810 --> 00:24:32,510
you are having a small

00:24:29,720 --> 00:24:34,250
if you'll ever expand small planets so

00:24:32,510 --> 00:24:37,130
without so this is basically a talk that

00:24:34,250 --> 00:24:39,770
is based on these resources here I put

00:24:37,130 --> 00:24:41,900
the different papers they put also the

00:24:39,770 --> 00:24:44,180
icons for the different data set some of

00:24:41,900 --> 00:24:46,790
the implementation that we release and

00:24:44,180 --> 00:24:49,040
everything is accessible from from here

00:24:46,790 --> 00:24:53,600
in this page there are things that are

00:24:49,040 --> 00:24:56,120
not out yet so more will come so for

00:24:53,600 --> 00:24:57,920
with all this I figure I'm done so I

00:24:56,120 --> 00:24:59,210
would like to thank you for listening so

00:24:57,920 --> 00:25:00,620
far if you have any question please go

00:24:59,210 --> 00:25:09,900
ahead

00:25:00,620 --> 00:25:09,900
[Applause]

00:25:17,360 --> 00:25:40,429
if you if you shout they can also repeat

00:25:19,460 --> 00:25:43,940
the question yeah okay so there are

00:25:40,429 --> 00:25:45,320
studies also so the question is what if

00:25:43,940 --> 00:25:47,419
unable to break the key so what you're

00:25:45,320 --> 00:25:50,529
doing things about the encrypted stuff

00:25:47,419 --> 00:25:53,179
where you can break the encryption so

00:25:50,529 --> 00:25:55,039
government guys so there are two answers

00:25:53,179 --> 00:25:56,629
so if you are having the if you are able

00:25:55,039 --> 00:25:59,090
to decrypt probably you're not

00:25:56,629 --> 00:26:01,700
interested in web performance so you're

00:25:59,090 --> 00:26:04,369
breaking this you know the to look at

00:26:01,700 --> 00:26:05,960
the different information secondly what

00:26:04,369 --> 00:26:07,399
study are usenix

00:26:05,960 --> 00:26:11,239
telling you what is the fraction of

00:26:07,399 --> 00:26:12,940
India you got proxies for instance in in

00:26:11,239 --> 00:26:14,779
some of the institutions you have

00:26:12,940 --> 00:26:17,210
approximate allocating and you're

00:26:14,779 --> 00:26:19,340
accepting the key and in a PC that is

00:26:17,210 --> 00:26:21,559
managed by your organization indeed you

00:26:19,340 --> 00:26:23,419
have proxies so from which it is not

00:26:21,559 --> 00:26:25,850
necessarily useful now in GDP are these

00:26:23,419 --> 00:26:27,710
pretty serious now so definitely if you

00:26:25,850 --> 00:26:29,359
are one way there is going to be twice

00:26:27,710 --> 00:26:32,809
as more concern as if you're a regular

00:26:29,359 --> 00:26:34,659
vendor right now in this moment and so

00:26:32,809 --> 00:26:36,889
of course I mean having that your

00:26:34,659 --> 00:26:37,879
devices are totally not interested in

00:26:36,889 --> 00:26:39,649
looking in the payload because they

00:26:37,879 --> 00:26:41,570
don't need it okay it's much more

00:26:39,649 --> 00:26:43,070
important so here basically what we are

00:26:41,570 --> 00:26:45,409
doing is that we are leveraging very

00:26:43,070 --> 00:26:46,820
weak signals that are intrinsically in

00:26:45,409 --> 00:26:49,999
the timing information that comes in

00:26:46,820 --> 00:26:53,389
packets much like I mean Debussy was

00:26:49,999 --> 00:26:55,340
saying that the music is the silence

00:26:53,389 --> 00:26:57,590
between the notes right so here in

00:26:55,340 --> 00:26:58,940
somehow we're waiting the information

00:26:57,590 --> 00:27:00,169
that we see even if you are not

00:26:58,940 --> 00:27:01,879
listening to the notes not looking at

00:27:00,169 --> 00:27:04,700
the content to try to reconstruct the

00:27:01,879 --> 00:27:07,220
signals the finger was showing today was

00:27:04,700 --> 00:27:10,779
not for the government was more for in

00:27:07,220 --> 00:27:10,779
the internet service provider and

00:27:10,899 --> 00:27:15,470
equipment vendor but if you go up to the

00:27:13,369 --> 00:27:17,659
Chrome browser for instance they still

00:27:15,470 --> 00:27:20,330
the missing link is between the layer

00:27:17,659 --> 00:27:23,600
seven the layer eight the user so how do

00:27:20,330 --> 00:27:25,759
you can you ensure that means it would

00:27:23,600 --> 00:27:28,399
be a talking animal is asian of timing

00:27:25,759 --> 00:27:29,990
AP is so you want to normalize something

00:27:28,399 --> 00:27:32,270
that is relevant for the user

00:27:29,990 --> 00:27:34,960
so and this is the part where Indy

00:27:32,270 --> 00:27:37,370
defeat from going normalizing that from

00:27:34,960 --> 00:27:39,110
level seven point of view and if he is

00:27:37,370 --> 00:27:43,059
relevant we can learn it also from layer

00:27:39,110 --> 00:27:43,059
free without breaking an encryption key

00:27:44,260 --> 00:27:56,600
know if it clarifies or yeah okay so

00:27:54,890 --> 00:27:58,370
that's a very yeah so that's a very good

00:27:56,600 --> 00:27:59,870
question so actually it's seasonality so

00:27:58,370 --> 00:28:01,940
basically things that are non session

00:27:59,870 --> 00:28:04,190
alert I'm and particular seasonality

00:28:01,940 --> 00:28:07,670
means there's periodicity is something

00:28:04,190 --> 00:28:10,610
that we look out we extensively look out

00:28:07,670 --> 00:28:12,380
in the that in datasets for instance

00:28:10,610 --> 00:28:14,450
with Wikipedia we have measurement of

00:28:12,380 --> 00:28:16,490
months worth of studies so we were

00:28:14,450 --> 00:28:18,559
expecting to find a night effect weekend

00:28:16,490 --> 00:28:19,940
with taste effect we didn't find any

00:28:18,559 --> 00:28:22,010
about the happiness of the user was

00:28:19,940 --> 00:28:25,700
amazing like stationary during the

00:28:22,010 --> 00:28:28,340
period so this is documentary in WWI /

00:28:25,700 --> 00:28:30,040
and we also extended that see there I

00:28:28,340 --> 00:28:32,630
don't know what there's no seasonality

00:28:30,040 --> 00:28:50,059
we were expecting it the data now is

00:28:32,630 --> 00:28:53,570
available so yeah in do this much okay

00:28:50,059 --> 00:28:56,540
so that's basically I went very very

00:28:53,570 --> 00:28:59,720
fast yeah I'm gonna repeat the question

00:28:56,540 --> 00:29:02,240
when I come to here so basically the

00:28:59,720 --> 00:29:04,550
question is okay what is the magic how

00:29:02,240 --> 00:29:06,410
can you learn from the the pockets and

00:29:04,550 --> 00:29:08,600
actually we're not learning directly

00:29:06,410 --> 00:29:10,910
from the pockets because every webpage

00:29:08,600 --> 00:29:12,800
is a different number of pockets and our

00:29:10,910 --> 00:29:15,290
supervised method which are regression

00:29:12,800 --> 00:29:16,700
met the need to have a fixed at input so

00:29:15,290 --> 00:29:18,050
what we're doing is basically that we

00:29:16,700 --> 00:29:21,590
are shopping the time into regular

00:29:18,050 --> 00:29:24,620
interval of times and what happens is

00:29:21,590 --> 00:29:26,390
that basically you are sampling

00:29:24,620 --> 00:29:28,670
periodically signal you're sampling it

00:29:26,390 --> 00:29:31,370
periodically this signal when you are

00:29:28,670 --> 00:29:33,230
every so often every delta T you are

00:29:31,370 --> 00:29:35,120
looking at the pocket will calm put an

00:29:33,230 --> 00:29:38,929
integral there and you basically

00:29:35,120 --> 00:29:41,090
sampling this curve here right so this

00:29:38,929 --> 00:29:43,840
is the way which we get the input which

00:29:41,090 --> 00:29:46,450
is by just cumulating over small period

00:29:43,840 --> 00:29:48,669
time time arrivals pockets belong to the

00:29:46,450 --> 00:29:50,740
same session and that words make the

00:29:48,669 --> 00:29:51,909
team so there's the basical signal

00:29:50,740 --> 00:29:54,520
processing level amount to feature

00:29:51,909 --> 00:29:56,770
engineering to normalize your input to

00:29:54,520 --> 00:30:02,230
be able to fade it to a neural network

00:29:56,770 --> 00:30:06,159
and okay four minutes left so I was too

00:30:02,230 --> 00:30:09,610
fast so you can sorry guys you have a

00:30:06,159 --> 00:30:11,710
question okay cool I can ask directly in

00:30:09,610 --> 00:30:13,240
a microphone do you have an estimate of

00:30:11,710 --> 00:30:14,650
how many data points we collected on

00:30:13,240 --> 00:30:17,200
Wikipedia and like a typical week during

00:30:14,650 --> 00:30:18,640
the study how many data points we

00:30:17,200 --> 00:30:21,010
collected in a typical week during this

00:30:18,640 --> 00:30:24,279
study on Wikipedia you ain't even a week

00:30:21,010 --> 00:30:26,200
so on now I know that basically finger

00:30:24,279 --> 00:30:33,190
change a bit so I think I have some make

00:30:26,200 --> 00:30:34,870
up slides Oh too many backup slides so

00:30:33,190 --> 00:30:38,289
this is okay about the stationarity you

00:30:34,870 --> 00:30:40,529
get your picture there which is here so

00:30:38,289 --> 00:30:42,970
I know that we were collecting the

00:30:40,529 --> 00:30:47,169
62,000 data points during the first

00:30:42,970 --> 00:30:49,840
period which was basically first test

00:30:47,169 --> 00:30:52,059
case in which we were if I remember

00:30:49,840 --> 00:30:53,919
correctly web performance timing our

00:30:52,059 --> 00:30:56,230
trigger at once every 10,000 page visit

00:30:53,919 --> 00:30:59,190
in Wikipedia and out of those we were

00:30:56,230 --> 00:31:01,419
sampling one over 1,000 at the beginning

00:30:59,190 --> 00:31:03,279
at the end of that we step up a little

00:31:01,419 --> 00:31:05,620
bit so you you step up a little bit the

00:31:03,279 --> 00:31:09,399
sampling but this is basically over this

00:31:05,620 --> 00:31:11,289
time period of time so hidden is the

00:31:09,399 --> 00:31:13,990
fact that we basically issued the sample

00:31:11,289 --> 00:31:17,830
the query to 1.4 million people and only

00:31:13,990 --> 00:31:20,679
62k replied so because people are they

00:31:17,830 --> 00:31:24,159
can willingly or not accept to click on

00:31:20,679 --> 00:31:26,230
those or not so the numbers per week I

00:31:24,159 --> 00:31:28,510
don't have them in my mind because you

00:31:26,230 --> 00:31:31,360
are mostly focusing on can we get a

00:31:28,510 --> 00:31:33,760
breakdown of all the users are happy and

00:31:31,360 --> 00:31:36,250
in this case in Wikipedia 85 percent of

00:31:33,760 --> 00:31:38,770
the users are consistently happy with no

00:31:36,250 --> 00:31:42,870
seasonality and no correlation with some

00:31:38,770 --> 00:31:42,870
events ok

00:32:01,300 --> 00:32:09,350
okay so we can go back this is through

00:32:05,930 --> 00:32:11,450
slide 1 which is so you're here you want

00:32:09,350 --> 00:32:12,800
to know if things break or not right so

00:32:11,450 --> 00:32:14,210
if you are measure and indeed from the

00:32:12,800 --> 00:32:15,650
browser it's because you are in the

00:32:14,210 --> 00:32:18,350
browser you because you're a service

00:32:15,650 --> 00:32:21,890
provider now what what we do as a

00:32:18,350 --> 00:32:24,350
business is basically selling boxes to

00:32:21,890 --> 00:32:26,600
operators in operators what they do is

00:32:24,350 --> 00:32:29,030
that they sell pipe capacity to their

00:32:26,600 --> 00:32:30,350
customer which are the user and from

00:32:29,030 --> 00:32:33,410
time to time they have problems because

00:32:30,350 --> 00:32:34,850
the service doesn't work and the people

00:32:33,410 --> 00:32:36,710
will complain to the SP but actually

00:32:34,850 --> 00:32:39,050
it's not the SP the problem may be the

00:32:36,710 --> 00:32:41,150
content service provider may be the DNS

00:32:39,050 --> 00:32:42,580
may be BGP so over there basically

00:32:41,150 --> 00:32:45,830
there's a need for troubleshooting tools

00:32:42,580 --> 00:32:49,040
in order to be able to tell oh yeah

00:32:45,830 --> 00:32:50,870
it's our problem so it's our net - blade

00:32:49,040 --> 00:32:52,880
is down so we're gonna fix it or loot

00:32:50,870 --> 00:32:55,280
guys everything that we have on our side

00:32:52,880 --> 00:32:57,500
is ok but there are lots of problems on

00:32:55,280 --> 00:32:59,930
that web sites everywhere in order to be

00:32:57,500 --> 00:33:01,520
able to say so you need to know what is

00:32:59,930 --> 00:33:02,990
the typical page load time of your user

00:33:01,520 --> 00:33:05,660
or the technique whether this is

00:33:02,990 --> 00:33:07,340
changing so this is why indeed before I

00:33:05,660 --> 00:33:10,400
was working more on the if you want

00:33:07,340 --> 00:33:12,260
layer 7 what kinds of aspects and there

00:33:10,400 --> 00:33:14,360
the question was ok we have this PD

00:33:12,260 --> 00:33:16,520
index ok we have the above default but

00:33:14,360 --> 00:33:18,380
nobody tried to compare whether this was

00:33:16,520 --> 00:33:20,600
really relevant for the user so this is

00:33:18,380 --> 00:33:23,000
where we started involving users and now

00:33:20,600 --> 00:33:25,340
this bit about ok and then I'm working

00:33:23,000 --> 00:33:27,140
for in our equipment vendors so am i

00:33:25,340 --> 00:33:28,730
able to do the same things but from a

00:33:27,140 --> 00:33:30,440
more challenging viewpoint which is

00:33:28,730 --> 00:33:34,400
starting from a completely encrypted

00:33:30,440 --> 00:33:38,360
traffic just for I mean it is research

00:33:34,400 --> 00:33:40,520
so it's fun but then given that I'm no

00:33:38,360 --> 00:33:42,140
longer in University and I'm in wall

00:33:40,520 --> 00:33:43,580
with this business model behind because

00:33:42,140 --> 00:33:45,170
basically if you are able to detect

00:33:43,580 --> 00:33:46,970
whether there is a problem then you can

00:33:45,170 --> 00:33:48,350
fix it and then it will not have user

00:33:46,970 --> 00:33:50,690
churn and so you're not losing money

00:33:48,350 --> 00:33:52,550
right so the same thing from the content

00:33:50,690 --> 00:33:56,120
provider why they optimizing because

00:33:52,550 --> 00:33:57,650
there are ads except on Wikipedia so

00:33:56,120 --> 00:33:59,600
there are in Wikipedia there's no nation

00:33:57,650 --> 00:34:01,760
but if if you are Google

00:33:59,600 --> 00:34:04,490
or being Ephraim Michaels Facebook you

00:34:01,760 --> 00:34:06,260
you're showing us and you're this is the

00:34:04,490 --> 00:34:08,419
way you get money so if your webpage is

00:34:06,260 --> 00:34:10,669
law they were studied by Google by being

00:34:08,419 --> 00:34:12,440
they were showing that for any amount of

00:34:10,669 --> 00:34:14,090
milliseconds you ads from hundreds

00:34:12,440 --> 00:34:16,340
millisecond you have a loss in the

00:34:14,090 --> 00:34:18,320
number of people that are gonna go to

00:34:16,340 --> 00:34:20,960
the server click on the ads and so you

00:34:18,320 --> 00:34:24,470
have a losses of revenue and if you

00:34:20,960 --> 00:34:27,080
multiply two percent loss by 1.2 billion

00:34:24,470 --> 00:34:30,679
people on visiting that's that's big

00:34:27,080 --> 00:34:36,679
numbers so same thing but from encrypted

00:34:30,679 --> 00:34:38,780
pipe from the network guys few points ok

00:34:36,679 --> 00:34:39,889
so thanks thanks a lot

00:34:38,780 --> 00:34:48,379
for

00:34:39,889 --> 00:34:48,379

YouTube URL: https://www.youtube.com/watch?v=U2VLAEe5hBg


