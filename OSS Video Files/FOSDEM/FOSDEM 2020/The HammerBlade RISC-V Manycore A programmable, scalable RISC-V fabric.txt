Title: The HammerBlade RISC-V Manycore A programmable, scalable RISC-V fabric
Publication date: 2020-07-10
Playlist: FOSDEM 2020
Description: 
	by Michael Taylor and Max H Ruttenberg

At: FOSDEM 2020
https://video.fosdem.org/2020/K.3.401/riscv_hammerblade.webm

HammerBlade is an open source RISC-V manycore that has been under development since 2015 and has already been silicon validated with a 511-core chip in 16nm TSMC. It features extensions to the RISC-V ISA that target GPU-competitive performance for parallel programs (i.e. GPGPU) including graphs and ML workloads. In this talk we will overview the components of the HW and software ecosystem in the latest version, and show you how to get up and running as an open source user or contributor in either SW or HW on Amazon F1 cloud FPGAs. HammerBlade is an open source RISC-V manycore that has been under development since 2015 and has already been silicon validated with a 511-core chip in 16nm TSMC. It features extensions to the RISC-V ISA that target GPU-competitive performance for parallel programs (i.e. GPGPU) including graphs and ML workloads. In this talk we will overview the components of the HW and software ecosystem in the latest version, and show you how to get up and running as an open source user or contributor in either SW or HW on Amazon F1 cloud FPGAs. We will overview the HW-architecture, the CUDA-like programming environment, the runtime software, the HW architecture, and our Amazon F1 cloud emulation and cosimulation environments, and our suite of performance analysis tools.

Room: K.3.401
Scheduled start: 2020-02-01 10:10:00
Captions: 
	00:00:04,660 --> 00:00:09,040
I'm excited to present here I'm Michael

00:00:07,570 --> 00:00:11,350
Thao and professor at University of

00:00:09,040 --> 00:00:14,049
Washington also known as u dubs

00:00:11,350 --> 00:00:15,700
sometimes and this is max Ruttenberg

00:00:14,049 --> 00:00:17,820
who's one of my PhD students we'll be

00:00:15,700 --> 00:00:20,530
presenting the second half of the talk

00:00:17,820 --> 00:00:24,580
so the talk is entitled the hammer Glade

00:00:20,530 --> 00:00:26,849
risk by Maker and it's essentially a

00:00:24,580 --> 00:00:29,019
programmable scalable respite fabric

00:00:26,849 --> 00:00:30,789
that you know in some senses is

00:00:29,019 --> 00:00:34,900
equivalent to a GPGPU but it's a

00:00:30,789 --> 00:00:38,800
completely open source so for motivation

00:00:34,900 --> 00:00:40,960
you know especially in this room you

00:00:38,800 --> 00:00:43,149
know we're experiencing this open source

00:00:40,960 --> 00:00:45,579
Renaissance right we got open source is

00:00:43,149 --> 00:00:48,370
A's open source CAD tools open source

00:00:45,579 --> 00:00:50,530
processors open source libraries in RTL

00:00:48,370 --> 00:00:51,910
and so I think you know we're going to

00:00:50,530 --> 00:00:53,680
see enormous transformation in the

00:00:51,910 --> 00:00:55,960
industry because of all this open source

00:00:53,680 --> 00:00:58,180
stuff in hardware that we're doing and

00:00:55,960 --> 00:01:00,760
then at the same time we have all of

00:00:58,180 --> 00:01:02,769
these new application domains that are

00:01:00,760 --> 00:01:05,830
enabled essentially by Moore's Law

00:01:02,769 --> 00:01:08,890
winding down right so and the key to

00:01:05,830 --> 00:01:11,979
technologies really for enabling these

00:01:08,890 --> 00:01:14,560
domains are the development of new dsls

00:01:11,979 --> 00:01:16,479
domain-specific languages that help us

00:01:14,560 --> 00:01:20,110
make the specification of pelo

00:01:16,479 --> 00:01:21,759
compilation or make the process of

00:01:20,110 --> 00:01:23,009
compiling some application and getting

00:01:21,759 --> 00:01:25,390
it to run in parallel in some way

00:01:23,009 --> 00:01:29,590
feasible and then also the availability

00:01:25,390 --> 00:01:30,670
of new parallel computers and those are

00:01:29,590 --> 00:01:32,470
the things they're going to get us the

00:01:30,670 --> 00:01:33,579
energy efficiency that we need in order

00:01:32,470 --> 00:01:35,560
to get that you know the high

00:01:33,579 --> 00:01:38,680
performance within a given thermal

00:01:35,560 --> 00:01:40,689
envelope so the hammer blade many-core

00:01:38,680 --> 00:01:43,240
is kind of seeking to be the base class

00:01:40,689 --> 00:01:44,560
of these parallel computer bricks that

00:01:43,240 --> 00:01:47,320
are needed for these new application

00:01:44,560 --> 00:01:50,070
domains so what is a hammer glade mini

00:01:47,320 --> 00:01:53,020
course so it's a highly programmable

00:01:50,070 --> 00:01:55,750
highly energy-efficient spatial fabric

00:01:53,020 --> 00:01:57,700
for sort of mixed sparse dense compute

00:01:55,750 --> 00:02:00,759
so the idea is this is trying to address

00:01:57,700 --> 00:02:03,219
not just codes that would run reasonably

00:02:00,759 --> 00:02:06,280
well in a traditional GPU but also new

00:02:03,219 --> 00:02:07,930
codes like graph codes that are much

00:02:06,280 --> 00:02:10,810
less well behaved in a much more

00:02:07,930 --> 00:02:13,120
challenging to get some parallel

00:02:10,810 --> 00:02:14,980
speed-up and at the very heart of the

00:02:13,120 --> 00:02:17,140
many cores we have a super high

00:02:14,980 --> 00:02:18,690
efficiency compute tile it's a one

00:02:17,140 --> 00:02:22,720
instruction per cycle overs five

00:02:18,690 --> 00:02:24,460
and then each tile has an instruction

00:02:22,720 --> 00:02:26,710
cache and a local data scratch pad and

00:02:24,460 --> 00:02:27,820
you can adjust the size of this but

00:02:26,710 --> 00:02:29,140
generally you want it to be small

00:02:27,820 --> 00:02:30,550
because the smaller it is the more

00:02:29,140 --> 00:02:31,690
course you can fit on the chip right so

00:02:30,550 --> 00:02:33,610
there's a trade-off there and we'll show

00:02:31,690 --> 00:02:35,820
you how the architecture actually allows

00:02:33,610 --> 00:02:38,440
you to flexibly change that trade-off

00:02:35,820 --> 00:02:40,330
each core also has an FPU and then a

00:02:38,440 --> 00:02:42,700
little router to talk to the other

00:02:40,330 --> 00:02:45,700
course and then it's very scalable you

00:02:42,700 --> 00:02:47,680
just stamp out as many course as you

00:02:45,700 --> 00:02:49,090
want if you have you know a silicon area

00:02:47,680 --> 00:02:54,330
you just stamp them out until you fill

00:02:49,090 --> 00:02:58,030
up the area so so we have very good

00:02:54,330 --> 00:03:01,630
efficiency for the mini course so this

00:02:58,030 --> 00:03:03,670
is a picture of a floorplan and tsmc 16

00:03:01,630 --> 00:03:06,070
nanometer and then this is actually a

00:03:03,670 --> 00:03:09,220
die photo from the actual chip so this

00:03:06,070 --> 00:03:11,400
was a chip we presented at the risk 5

00:03:09,220 --> 00:03:15,790
summit the fourth one I think it was in

00:03:11,400 --> 00:03:19,080
2017 and what's amazing about this core

00:03:15,790 --> 00:03:22,570
is that even with super small

00:03:19,080 --> 00:03:24,790
instruction memory and data memory the

00:03:22,570 --> 00:03:26,620
memories themselves occupy 64 percent of

00:03:24,790 --> 00:03:28,900
the tile area so it's a kind of existing

00:03:26,620 --> 00:03:30,430
proof that yeah you pick try to go in

00:03:28,900 --> 00:03:31,960
like refactor this core and try to

00:03:30,430 --> 00:03:33,640
squeeze it down a little more but the

00:03:31,960 --> 00:03:37,000
maximum improvement you could possibly

00:03:33,640 --> 00:03:40,030
get would be about 30 36 percent so it's

00:03:37,000 --> 00:03:42,970
a so small in fact that you can fit 40

00:03:40,030 --> 00:03:45,190
of these per millimeter and 16 nanometer

00:03:42,970 --> 00:03:49,990
and at 7 nanometer you be able to fit

00:03:45,190 --> 00:03:51,160
120 of these per millimeter so zooming

00:03:49,990 --> 00:03:52,570
out you know we have this array of

00:03:51,160 --> 00:03:54,850
course but we need so we need to also

00:03:52,570 --> 00:03:59,560
integrate them into a pillow memory

00:03:54,850 --> 00:04:01,960
system and so the architectural model is

00:03:59,560 --> 00:04:04,030
you have the C of cores and then at the

00:04:01,960 --> 00:04:06,220
edge you have these l2 Victim caches

00:04:04,030 --> 00:04:07,450
that are then connected to many parallel

00:04:06,220 --> 00:04:08,020
memory channels so it could be for

00:04:07,450 --> 00:04:12,430
example

00:04:08,020 --> 00:04:14,170
HBM to which you know in in GPU systems

00:04:12,430 --> 00:04:17,250
you can have 64 parallel memory channels

00:04:14,170 --> 00:04:20,230
these days and these caches are also

00:04:17,250 --> 00:04:23,860
adaptive and can essentially change

00:04:20,230 --> 00:04:25,540
their favor on-the-fly as the as you

00:04:23,860 --> 00:04:27,940
learn more about the workload in the

00:04:25,540 --> 00:04:30,760
input data set

00:04:27,940 --> 00:04:33,280
now it's pretty easy to create a see of

00:04:30,760 --> 00:04:35,380
course so the real special sauce is how

00:04:33,280 --> 00:04:37,480
you weave these things together so the

00:04:35,380 --> 00:04:39,370
interesting thing about how this is done

00:04:37,480 --> 00:04:41,920
in our system is that every single

00:04:39,370 --> 00:04:43,930
memory location in every single local

00:04:41,920 --> 00:04:45,490
course scratchpad every single memory

00:04:43,930 --> 00:04:47,140
location in the l2 is every single

00:04:45,490 --> 00:04:48,940
memory location in the drn's it's all

00:04:47,140 --> 00:04:50,320
addressable by all the course so that

00:04:48,940 --> 00:04:51,730
means they can very easily collaborate

00:04:50,320 --> 00:04:53,560
just by doing loads and store

00:04:51,730 --> 00:04:56,860
instructions which will automatically

00:04:53,560 --> 00:05:00,580
get routed over to the whichever cord is

00:04:56,860 --> 00:05:02,890
or cache it is that owns that particular

00:05:00,580 --> 00:05:05,950
location and so a core can actually

00:05:02,890 --> 00:05:07,810
issue you know many repeated loads and

00:05:05,950 --> 00:05:09,040
stores those things will go out in the

00:05:07,810 --> 00:05:11,650
network in parallel and then come back

00:05:09,040 --> 00:05:14,470
potentially out of order and the core

00:05:11,650 --> 00:05:17,020
can continue executing instructions it's

00:05:14,470 --> 00:05:20,620
only when it actually tries to use the

00:05:17,020 --> 00:05:22,030
register that was the result of the load

00:05:20,620 --> 00:05:25,690
instruction that it might actually stall

00:05:22,030 --> 00:05:29,140
waiting for the data so one of the

00:05:25,690 --> 00:05:31,960
concepts that we have in trying to map

00:05:29,140 --> 00:05:33,520
software to the many cores then we have

00:05:31,960 --> 00:05:37,000
this big array of tiles and now we want

00:05:33,520 --> 00:05:38,620
to sort of group them together for two

00:05:37,000 --> 00:05:40,150
reasons so one is we may want to execute

00:05:38,620 --> 00:05:41,980
you know many different programs at the

00:05:40,150 --> 00:05:43,900
same time on their many core so we have

00:05:41,980 --> 00:05:46,000
this concept of tile group and here we

00:05:43,900 --> 00:05:48,400
show a tile group which is a 4 by 4 tile

00:05:46,000 --> 00:05:52,660
group where I allocated you know these

00:05:48,400 --> 00:05:56,169
16 tiles on the mini core and the many

00:05:52,660 --> 00:05:58,090
core essentially can the computation

00:05:56,169 --> 00:05:59,919
that's running on the mini core inside

00:05:58,090 --> 00:06:02,380
that's how a group can share all the

00:05:59,919 --> 00:06:05,250
collective memories of those tiles so

00:06:02,380 --> 00:06:07,690
this is a way for you to manage not only

00:06:05,250 --> 00:06:09,310
how much parallelism you have by having

00:06:07,690 --> 00:06:11,590
more fewer course but also how much

00:06:09,310 --> 00:06:13,780
working memory that you have so if you

00:06:11,590 --> 00:06:15,310
have a big working set then you can use

00:06:13,780 --> 00:06:17,410
a larger group of cores in order to

00:06:15,310 --> 00:06:19,540
increase the amount of local memory that

00:06:17,410 --> 00:06:21,010
you have and of course because those

00:06:19,540 --> 00:06:23,470
cores are located near each other the

00:06:21,010 --> 00:06:25,840
latency is very low in order to access

00:06:23,470 --> 00:06:31,510
the data whereas you know if you go off

00:06:25,840 --> 00:06:33,220
chip the latency would be higher so in

00:06:31,510 --> 00:06:35,500
order to program this we have of course

00:06:33,220 --> 00:06:37,180
we have dsls which are more user facing

00:06:35,500 --> 00:06:39,100
but in terms of writing you know very

00:06:37,180 --> 00:06:39,850
high performance library code for those

00:06:39,100 --> 00:06:41,650
dsls

00:06:39,850 --> 00:06:44,500
we have something we call cooter light

00:06:41,650 --> 00:06:45,699
which is very analogous to CUDA and the

00:06:44,500 --> 00:06:47,650
idea is that you know there's a big

00:06:45,699 --> 00:06:50,650
developer base for CUDA that's familiar

00:06:47,650 --> 00:06:52,750
with us you know programming constructs

00:06:50,650 --> 00:06:54,850
and we would like to essentially take

00:06:52,750 --> 00:06:56,110
the the knowledge that people have

00:06:54,850 --> 00:06:58,630
embedded in CUDA code and then

00:06:56,110 --> 00:07:02,380
relatively easily move that knowledge

00:06:58,630 --> 00:07:05,440
over to executing on the many core and

00:07:02,380 --> 00:07:07,449
so this is an example comparison of a

00:07:05,440 --> 00:07:08,979
snippet of device that wrote on CUDA and

00:07:07,449 --> 00:07:10,660
the equivalent code included light and

00:07:08,979 --> 00:07:14,620
it's you know there's there's it's

00:07:10,660 --> 00:07:16,060
fairly one two what in terms of being

00:07:14,620 --> 00:07:18,010
able to move code over there there's

00:07:16,060 --> 00:07:19,360
certainly some differences in terms of

00:07:18,010 --> 00:07:20,919
optimization and what would be

00:07:19,360 --> 00:07:22,780
preferable in one architecture the other

00:07:20,919 --> 00:07:26,910
but in terms of getting something up and

00:07:22,780 --> 00:07:29,199
running it's fairly straightforward so

00:07:26,910 --> 00:07:31,780
of course you have the many core and

00:07:29,199 --> 00:07:34,090
these cores are highly specialized for a

00:07:31,780 --> 00:07:36,250
dense compute but we would like to put a

00:07:34,090 --> 00:07:38,500
control processor of some form so

00:07:36,250 --> 00:07:41,440
there's two flavors of this in our

00:07:38,500 --> 00:07:45,099
system so the first is we support doing

00:07:41,440 --> 00:07:46,720
PCIe attached acceleration so in this

00:07:45,099 --> 00:07:49,449
case you know we have a chip it's a mini

00:07:46,720 --> 00:07:51,070
courtship it's on a PCIe board and then

00:07:49,449 --> 00:07:54,729
we put we connect it to a Xeon server

00:07:51,070 --> 00:07:57,460
and we have this up and running on f1 so

00:07:54,729 --> 00:07:59,590
in f1 s case there's an FPGA board which

00:07:57,460 --> 00:08:02,139
is simulating our mini core and then we

00:07:59,590 --> 00:08:04,360
have our you know host code running on

00:08:02,139 --> 00:08:06,490
the Xeon so you have x86 code talking to

00:08:04,360 --> 00:08:08,199
the menu court together in the longer

00:08:06,490 --> 00:08:10,750
term our goal is to actually have a

00:08:08,199 --> 00:08:13,180
black parrot integrated on the same SOC

00:08:10,750 --> 00:08:15,400
as the many core so for example you

00:08:13,180 --> 00:08:17,349
could imagine right now we would run PI

00:08:15,400 --> 00:08:20,320
torch on the Xeon and it offloads calls

00:08:17,349 --> 00:08:23,110
to the PCIe board but eventually you

00:08:20,320 --> 00:08:25,710
know when PI torch runs on verse 5 we

00:08:23,110 --> 00:08:28,900
would be running that on a black parent

00:08:25,710 --> 00:08:30,789
so I'd like to mention that we've

00:08:28,900 --> 00:08:32,979
actually been through many iterations so

00:08:30,789 --> 00:08:35,919
we have a very agile methodology where

00:08:32,979 --> 00:08:38,080
we do tape outs we build software on the

00:08:35,919 --> 00:08:39,669
devices we get experience than we

00:08:38,080 --> 00:08:42,700
develop the next generation so we're

00:08:39,669 --> 00:08:44,740
actually entering our fifth generation

00:08:42,700 --> 00:08:48,310
an iteration of this so the fifth

00:08:44,740 --> 00:08:50,410
silicon iteration of the mini core and

00:08:48,310 --> 00:08:52,870
so we started out in 180 nanometer and

00:08:50,410 --> 00:08:55,510
then we did two chips in 16 nanometer

00:08:52,870 --> 00:08:57,250
that had a 511

00:08:55,510 --> 00:08:59,200
miss five-course so it broke the world

00:08:57,250 --> 00:09:02,020
record for risk 5 performance actually

00:08:59,200 --> 00:09:04,990
probably still holds it and also for

00:09:02,020 --> 00:09:08,380
core mark for any is a and the latest

00:09:04,990 --> 00:09:10,180
system a hammer one is where we've

00:09:08,380 --> 00:09:12,580
really been focusing on programmability

00:09:10,180 --> 00:09:16,360
improvements to grow the user base and

00:09:12,580 --> 00:09:21,000
also a floating-point as support so I'd

00:09:16,360 --> 00:09:25,350
like to mention that in addition to the

00:09:21,000 --> 00:09:28,210
the many core itself my group has

00:09:25,350 --> 00:09:30,070
website B jump at org we have a bunch of

00:09:28,210 --> 00:09:32,050
stuff that might be of interest to you

00:09:30,070 --> 00:09:33,790
if you're developing open source

00:09:32,050 --> 00:09:36,880
hardware so one of them is something we

00:09:33,790 --> 00:09:39,490
call bechamel STL dan I mentioned this

00:09:36,880 --> 00:09:41,020
in the last talk so this is a library

00:09:39,490 --> 00:09:42,790
very high quality implementations of

00:09:41,020 --> 00:09:44,470
almost every hardware primitive that you

00:09:42,790 --> 00:09:46,660
can think of you know what routers you

00:09:44,470 --> 00:09:49,210
want arbiters you want caches you want

00:09:46,660 --> 00:09:51,010
networks you want high-speed links that

00:09:49,210 --> 00:09:52,540
go off your chip all of these things you

00:09:51,010 --> 00:09:53,830
know we've developed over you know the

00:09:52,540 --> 00:09:56,560
last ten years in our group and we've

00:09:53,830 --> 00:09:58,300
really iterated on their quality and

00:09:56,560 --> 00:10:01,270
also results and it's all been taped out

00:09:58,300 --> 00:10:04,360
validated we also have open source

00:10:01,270 --> 00:10:06,610
motherboards so you can design your ASIC

00:10:04,360 --> 00:10:08,320
to a particular interface standard that

00:10:06,610 --> 00:10:10,420
we specified and then you can just take

00:10:08,320 --> 00:10:12,190
your race they can plug it into a socket

00:10:10,420 --> 00:10:13,360
and we'll just work with this board so

00:10:12,190 --> 00:10:15,550
you don't have to go and design a board

00:10:13,360 --> 00:10:19,090
in order to I use the silicon that you

00:10:15,550 --> 00:10:21,220
developed we also have open source BGA

00:10:19,090 --> 00:10:23,560
packages these are much higher kin out

00:10:21,220 --> 00:10:25,990
and you can get off the shelf so if you

00:10:23,560 --> 00:10:27,280
design your chip to this interface

00:10:25,990 --> 00:10:29,590
standard that when you specify this

00:10:27,280 --> 00:10:31,900
Asics akut standard then you can get

00:10:29,590 --> 00:10:33,880
much more higher performance bandwidth

00:10:31,900 --> 00:10:38,530
out of your piece of silicon than you

00:10:33,880 --> 00:10:40,450
would on your own so I'm going to wrap

00:10:38,530 --> 00:10:42,550
up the hardware part I wanted to also

00:10:40,450 --> 00:10:45,160
mention that you know we have developed

00:10:42,550 --> 00:10:47,490
a methodology for taking cores out and

00:10:45,160 --> 00:10:50,410
replacing them with accelerators in this

00:10:47,490 --> 00:10:52,330
fabric and in fact our collaborators at

00:10:50,410 --> 00:10:54,580
Cornell have been developing several

00:10:52,330 --> 00:10:57,700
accelerators and have you know done

00:10:54,580 --> 00:10:59,620
proof of concept already with having you

00:10:57,700 --> 00:11:02,440
know hybrid many core accelerator

00:10:59,620 --> 00:11:04,990
systems so now I'm going to turn it over

00:11:02,440 --> 00:11:08,760
to max he's going to talk about some of

00:11:04,990 --> 00:11:08,760
our hammer hammer blade software stacks

00:11:28,709 --> 00:11:34,209
thank you

00:11:30,600 --> 00:11:35,829
so yeah so I'm gonna hi I'm max I'm

00:11:34,209 --> 00:11:39,730
gonna give an overview of our software

00:11:35,829 --> 00:11:41,640
stack so one of the primary goals of

00:11:39,730 --> 00:11:44,260
hammer blade is programmability and

00:11:41,640 --> 00:11:45,930
portability of code that already exists

00:11:44,260 --> 00:11:48,610
that's written higher-level frameworks

00:11:45,930 --> 00:11:51,370
so CUDA light which is our low-level

00:11:48,610 --> 00:11:54,550
programming API is the building block

00:11:51,370 --> 00:11:56,230
for these higher-level frameworks our

00:11:54,550 --> 00:11:58,750
collaborators at Cornell are working on

00:11:56,230 --> 00:12:01,959
pi torch our back-end for pi torch for

00:11:58,750 --> 00:12:05,410
hammer Glade we at u-dub are also

00:12:01,959 --> 00:12:10,180
working on D GL which is a Python

00:12:05,410 --> 00:12:11,680
library for targeting a crap structured

00:12:10,180 --> 00:12:15,459
data but using machine learning

00:12:11,680 --> 00:12:18,040
techniques and T VM which is a machine

00:12:15,459 --> 00:12:20,279
learning ir and we're also targeting

00:12:18,040 --> 00:12:27,730
graphic which is a DSL for

00:12:20,279 --> 00:12:30,630
high-performance graph analytics so just

00:12:27,730 --> 00:12:33,640
a quick introduction to graph it its

00:12:30,630 --> 00:12:36,370
main feature is that it decouples the

00:12:33,640 --> 00:12:38,920
the semantics of the program from the

00:12:36,370 --> 00:12:42,010
optimizations that are applied this is

00:12:38,920 --> 00:12:44,589
important because with graph algorithms

00:12:42,010 --> 00:12:46,329
there's a whole the field of

00:12:44,589 --> 00:12:47,860
optimizations is pretty wide and they

00:12:46,329 --> 00:12:49,959
don't always work on every single input

00:12:47,860 --> 00:12:53,680
sometimes the optimizations that work

00:12:49,959 --> 00:12:57,130
are highly reliant on the input the two

00:12:53,680 --> 00:12:59,110
main types are edge sets vertex sets and

00:12:57,130 --> 00:13:02,079
then there's a scheduling which which

00:12:59,110 --> 00:13:05,160
you can see down here which says what

00:13:02,079 --> 00:13:05,160
optimizations to apply

00:13:07,750 --> 00:13:13,420
so we ported this to hammer blade and

00:13:11,500 --> 00:13:15,069
here's sort of an example so the same

00:13:13,420 --> 00:13:19,870
snippet of code I actually took it from

00:13:15,069 --> 00:13:24,250
BFS basically the frontier is just being

00:13:19,870 --> 00:13:27,069
updated by traversing the existing

00:13:24,250 --> 00:13:29,290
frontier applying an update function and

00:13:27,069 --> 00:13:32,319
then creating a new frontier from the

00:13:29,290 --> 00:13:34,750
notes that were updated and seeing the

00:13:32,319 --> 00:13:37,029
scheduling language that we've said that

00:13:34,750 --> 00:13:39,699
we've add that we've said that we want

00:13:37,029 --> 00:13:43,480
the dents pull direction and that we

00:13:39,699 --> 00:13:45,040
want to generate kemberly code down here

00:13:43,480 --> 00:13:47,230
you can see the host side that is

00:13:45,040 --> 00:13:52,089
generated so this runs on the x86 or

00:13:47,230 --> 00:13:54,490
black Barrett you can see that it's just

00:13:52,089 --> 00:14:01,149
the outer loop but we're offloading the

00:13:54,490 --> 00:14:04,029
work here's the generated risk 5 code

00:14:01,149 --> 00:14:06,939
it's C++ code the work is self assigned

00:14:04,029 --> 00:14:10,779
we have a local range function and then

00:14:06,939 --> 00:14:12,459
we are doing a parallel dance update and

00:14:10,779 --> 00:14:20,170
then finally when all the work is done

00:14:12,459 --> 00:14:22,420
we do a tile group sync so graphs are

00:14:20,170 --> 00:14:25,209
kind of challenging because they are

00:14:22,420 --> 00:14:27,579
typically memory intensive so taking

00:14:25,209 --> 00:14:30,069
full advantage of all your cores can be

00:14:27,579 --> 00:14:32,610
tricky you might add spending they might

00:14:30,069 --> 00:14:36,579
spend a lot of time waiting for memory

00:14:32,610 --> 00:14:38,319
so one way you might be able to fix this

00:14:36,579 --> 00:14:39,790
address this problem on hammer blade is

00:14:38,319 --> 00:14:42,250
doing something similar to what we might

00:14:39,790 --> 00:14:46,120
do on GPUs week and Tylar memory

00:14:42,250 --> 00:14:48,339
accesses so in this case where the

00:14:46,120 --> 00:14:52,600
proposed plan would be to take vertex

00:14:48,339 --> 00:14:57,519
data and block and keep an axis in a

00:14:52,600 --> 00:15:00,399
block faction yeah fashion so we would

00:14:57,519 --> 00:15:02,050
pull it in on one block for a group of

00:15:00,399 --> 00:15:05,680
tiles and they would pull it into their

00:15:02,050 --> 00:15:09,040
local memory and then they could just do

00:15:05,680 --> 00:15:11,259
sparse updates from the from the edges

00:15:09,040 --> 00:15:13,959
so they can access the edges you would

00:15:11,259 --> 00:15:16,059
keep them you would keep the edges

00:15:13,959 --> 00:15:17,709
partitioned across DRAM channels this

00:15:16,059 --> 00:15:19,920
would maximize your message transfer

00:15:17,709 --> 00:15:19,920
rate

00:15:24,420 --> 00:15:29,440
and then you could restrict your updates

00:15:27,130 --> 00:15:32,140
to a given range so this would keep your

00:15:29,440 --> 00:15:35,260
lookout this would keep you from

00:15:32,140 --> 00:15:36,640
breaking your cache and it will reduce

00:15:35,260 --> 00:15:39,340
the amount of time you spend waiting

00:15:36,640 --> 00:15:44,590
around for memory it will maximize the

00:15:39,340 --> 00:15:46,810
parallelism of your course so now I've

00:15:44,590 --> 00:15:50,920
given you a overview of mapping an

00:15:46,810 --> 00:15:52,480
application Tamarack blade I want to I'm

00:15:50,920 --> 00:15:57,910
bored of you all with how you can get

00:15:52,480 --> 00:16:01,540
involved so our main simulating

00:15:57,910 --> 00:16:05,530
environment is C C++ co-simulation

00:16:01,540 --> 00:16:08,470
environment allows you to simulate your

00:16:05,530 --> 00:16:12,040
entire stack the host software mini core

00:16:08,470 --> 00:16:16,180
software we use synopsis as our RTL

00:16:12,040 --> 00:16:18,280
simulator I should note that most of

00:16:16,180 --> 00:16:20,850
code in our the RTL on our group is

00:16:18,280 --> 00:16:23,350
there later friendly so it would be a

00:16:20,850 --> 00:16:27,760
straightforward and solid contribution

00:16:23,350 --> 00:16:30,100
to get there later working the codes all

00:16:27,760 --> 00:16:33,190
up in github you cloned this repository

00:16:30,100 --> 00:16:35,530
called Blade Runner you get the related

00:16:33,190 --> 00:16:37,120
sub projects and then there are

00:16:35,530 --> 00:16:39,910
instructions and the readme about to get

00:16:37,120 --> 00:16:41,110
up and running if you have the tools in

00:16:39,910 --> 00:16:46,210
the environment it's fairly

00:16:41,110 --> 00:16:50,200
straightforward we also support

00:16:46,210 --> 00:16:51,940
deployment on AWS you need them the bada

00:16:50,200 --> 00:16:56,140
tools you need to design links Vlado

00:16:51,940 --> 00:17:01,030
compiler but it's all part of the same

00:16:56,140 --> 00:17:03,190
project and we have instruct detailed

00:17:01,030 --> 00:17:05,140
instructions in the room yeah about how

00:17:03,190 --> 00:17:06,550
you can build the fpga image hiking

00:17:05,140 --> 00:17:08,050
building machine image and you can get

00:17:06,550 --> 00:17:10,360
up and running and machine image will

00:17:08,050 --> 00:17:12,280
have everything they'll have runtime

00:17:10,360 --> 00:17:16,890
libraries development tools for risk 5

00:17:12,280 --> 00:17:16,890
the fpga bit image all of it

00:17:18,720 --> 00:17:25,390
and lastly I'd like to suggest some

00:17:23,230 --> 00:17:27,040
directions you could take to help us

00:17:25,390 --> 00:17:31,540
contribute to the Mittman so a hammer

00:17:27,040 --> 00:17:33,730
blade software stack all of our software

00:17:31,540 --> 00:17:35,380
frameworks that we've reported our works

00:17:33,730 --> 00:17:38,410
in progress everything down to CUDA

00:17:35,380 --> 00:17:40,120
light but there's some other really

00:17:38,410 --> 00:17:42,370
interesting directions we could take

00:17:40,120 --> 00:17:46,500
that we haven't explored yet so halide

00:17:42,370 --> 00:17:49,000
is for image processing we haven't

00:17:46,500 --> 00:17:51,990
looked at that yet but we're pretty

00:17:49,000 --> 00:17:55,510
confident that would be a good fit

00:17:51,990 --> 00:17:59,380
tensorflow right now we have PI torch

00:17:55,510 --> 00:18:03,820
but tensorflow would also be excellent F

00:17:59,380 --> 00:18:07,690
of T W is a library for doing

00:18:03,820 --> 00:18:10,000
FFTs on GPUs there's also ku F of T for

00:18:07,690 --> 00:18:13,740
CUDA we would really love one for hammer

00:18:10,000 --> 00:18:15,790
blame any core as well on top of that

00:18:13,740 --> 00:18:17,290
Cornell has been working on building

00:18:15,790 --> 00:18:19,870
their own accelerators and gnash and

00:18:17,290 --> 00:18:22,030
adding them to our mess and it would be

00:18:19,870 --> 00:18:25,230
great if you could add for people to add

00:18:22,030 --> 00:18:32,080
their own accelerators and to expand our

00:18:25,230 --> 00:18:34,900
application domains so we have a full

00:18:32,080 --> 00:18:36,850
stack team and they work very very hard

00:18:34,900 --> 00:18:41,500
on this project and I'd like to thank

00:18:36,850 --> 00:18:43,660
them before including and on behalf of

00:18:41,500 --> 00:18:45,700
the hammer blade team we sleep all we

00:18:43,660 --> 00:18:47,490
really hope you contribute and thank you

00:18:45,700 --> 00:18:55,109
very much

00:18:47,490 --> 00:18:55,109
[Applause]

00:18:59,710 --> 00:19:13,990
oh so the question was how how many of

00:19:09,670 --> 00:19:15,640
these can you fit on the FPGA so so we

00:19:13,990 --> 00:19:17,410
we haven't done too much with smaller

00:19:15,640 --> 00:19:20,770
DJ's but we can fit a hundred and ninety

00:19:17,410 --> 00:19:22,270
two cores on a big FPGA so a reasonable

00:19:20,770 --> 00:19:24,540
number of cores and fit on a small at

00:19:22,270 --> 00:19:24,540

YouTube URL: https://www.youtube.com/watch?v=45qHg0AmfUQ


