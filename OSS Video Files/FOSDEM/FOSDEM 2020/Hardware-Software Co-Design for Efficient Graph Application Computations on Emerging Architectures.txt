Title: Hardware-Software Co-Design for Efficient Graph Application Computations on Emerging Architectures
Publication date: 2020-07-15
Playlist: FOSDEM 2020
Description: 
	by Margaret Martonosi and Aninda Manocha

At: FOSDEM 2020
https://video.fosdem.org/2020/AW1.121/graph_hardware_software_co_design.webm

Graph databases and applications have attracted much attention in the past few years due to the efficiency with which they can represent big data, connecting different layers of data structures and allowing analysis while preserving contextual relationships.
This has resulted in a fast-growing community that has been developing various database and algorithmic innovations in this area, many of which will be gathering together in this conference. We joined this field as computer architecture researchers and are currently building a complete hardware-software design, called DECADES, that aims to accelerate the execution of these algorithms.


From a computer architecture point of view, applications involving dense matrix operations such as neural networks have garnered much attention for their acceleration through specialized hardware such as GPUs and TPUs, while graph applications remain difficult to improve even with modern specialized accelerator designs. The reason for this is the characteristic pointer-based data structures of graph applications and the resulting irregular memory accesses performed by many of these workloads. Such irregular memory accesses result in memory latency bottlenecks that dominate the total execution time. In this talk, as part of the DECADES infrastructure, we present an elegant hardware-software codesign solution, named FAST-LLAMAs, to overcome these memory-bottlenecks, and thus, accelerate graph and sparse applications in an energy efficient way.
Graph databases and applications have attracted much attention in the past few years due to the efficiency with which they can represent big data, connecting different layers of data structures and allowing analysis while preserving contextual relationships.
This has resulted in a fast-growing community that has been developing various database and algorithmic innovations in this area, many of which will be gathering together in this conference. We joined this field as computer architecture researchers and are currently building a complete hardware-software design, called DECADES, that aims to accelerate the execution of these algorithms.


From a computer architecture point of view, applications involving dense matrix operations such as neural networks have garnered much attention for their acceleration through specialized hardware such as GPUs and TPUs, while graph applications remain difficult to improve even with modern specialized accelerator designs. The reason for this is the characteristic pointer-based data structures of graph applications and the resulting irregular memory accesses performed by many of these workloads. Such irregular memory accesses result in memory latency bottlenecks that dominate the total execution time. In this talk, as part of the DECADES infrastructure, we present an elegant hardware-software codesign solution, named FAST-LLAMAs, to overcome these memory-bottlenecks, and thus, accelerate graph and sparse applications in an energy efficient way.


We propose a 40 minute talk which includes a rigorous characterization of the problem, and an in-depth analysis of our software-hardware co-design solution, FAST LLAMAs. We will present results based on a simulated model of our system which show significant performance improvements (up to 8x), as well as energy improvements (up to 20x) on a set of fundamental graph algorithms and important real-world datasets. Our system is completely open-source, and includes a compiler and cycle-accurate simulator. Our proposed system is compatible and easily extendable to many of the open-source graph analytic and database frameworks and we are excited to engage with the open-source community of this increasingly important domain.


The work is part of a large collaboration from three academic groups: Margaret Martonosi (PI Princeton), David Wentzlaff (PI Princeton), Luca Carloni (PI Columbia) with students/researchers: Juan L. Aragón (U. of Murcia, Spain), Jonathan Balkind, Ting-Jung Chang, Fei Gao, Davide Giri, Paul J. Jackson, Aninda Manocha, Opeoluwa Matthews, Tyler Sorensen, Esin Türeci, Georgios Tziantzioulis, and Marcelo Orenes Vera. In addition to the submission author, portions of the talk may be offered by others in the collaboration.

Room: AW1.121
Scheduled start: 2020-02-01 15:40:00
Captions: 
	00:00:07,180 --> 00:00:11,570
[Applause]

00:00:08,889 --> 00:00:13,400
good afternoon everyone I'm a ninja

00:00:11,570 --> 00:00:15,410
Manoah and I'm today I'm going to be

00:00:13,400 --> 00:00:18,050
presenting on behalf of the decades team

00:00:15,410 --> 00:00:19,610
our work on hardware/software co.design

00:00:18,050 --> 00:00:23,079
for efficient graph application

00:00:19,610 --> 00:00:25,579
computations on emerging architectures

00:00:23,079 --> 00:00:28,369
I'm gonna give a brief overview of what

00:00:25,579 --> 00:00:30,140
the decades project is so decades is

00:00:28,369 --> 00:00:32,329
part of the DARPA software-defined

00:00:30,140 --> 00:00:34,760
hardware program which aims to design

00:00:32,329 --> 00:00:36,530
runtime reconfigurable hardware that can

00:00:34,760 --> 00:00:38,510
accelerate a variety of data intensive

00:00:36,530 --> 00:00:40,010
software applications in the broad

00:00:38,510 --> 00:00:42,980
domains of machine learning and graphic

00:00:40,010 --> 00:00:44,839
analytics so the decades approach is to

00:00:42,980 --> 00:00:46,899
design a heterogeneous tile based chip

00:00:44,839 --> 00:00:48,920
that is a combination of core

00:00:46,899 --> 00:00:50,780
accelerator and intelligent storage

00:00:48,920 --> 00:00:53,210
tiles as you can see on this image on

00:00:50,780 --> 00:00:55,999
the right and this is a collaborative

00:00:53,210 --> 00:00:58,519
effort between researchers at Princeton

00:00:55,999 --> 00:01:00,559
and Columbia University so all of our

00:00:58,519 --> 00:01:06,020
tools are or will be in the very near

00:01:00,559 --> 00:01:07,700
future open source at this link below so

00:01:06,020 --> 00:01:09,290
many machine learning and data science

00:01:07,700 --> 00:01:11,840
applications need to process large

00:01:09,290 --> 00:01:14,530
amounts of dense data in the for example

00:01:11,840 --> 00:01:16,580
images composed of many pixels

00:01:14,530 --> 00:01:18,620
fortunately huge strides have been made

00:01:16,580 --> 00:01:21,130
in processing this type these types of

00:01:18,620 --> 00:01:23,480
data like neural network accelerators

00:01:21,130 --> 00:01:25,970
meanwhile graphs can efficiently

00:01:23,480 --> 00:01:28,100
represent big data although their data

00:01:25,970 --> 00:01:30,310
layouts are often sparse and so they

00:01:28,100 --> 00:01:32,900
require different computing paradigms

00:01:30,310 --> 00:01:35,000
due to the ubiquity of graph databases

00:01:32,900 --> 00:01:36,500
and data structures graph applications

00:01:35,000 --> 00:01:39,050
are at the heart of many big data

00:01:36,500 --> 00:01:42,080
analytics as you all know for example

00:01:39,050 --> 00:01:43,520
recommendation systems here's an example

00:01:42,080 --> 00:01:46,490
of Twitter's use of a recommendation

00:01:43,520 --> 00:01:48,290
system so if a user goes to positives

00:01:46,490 --> 00:01:53,900
Twitter page they will be recommended

00:01:48,290 --> 00:01:57,410
other free and open source software so

00:01:53,900 --> 00:01:59,470
in order to process big data modern

00:01:57,410 --> 00:02:01,550
technology trends have employed

00:01:59,470 --> 00:02:03,470
specialized hardware which has led to

00:02:01,550 --> 00:02:06,010
accelerator oriented heterogeneity and

00:02:03,470 --> 00:02:09,019
parallelism as you can see in the graph

00:02:06,010 --> 00:02:11,360
the purple black and orange lines show

00:02:09,019 --> 00:02:14,330
the trends with these performance over

00:02:11,360 --> 00:02:16,219
time and these have significantly

00:02:14,330 --> 00:02:18,170
benefited compute bound workloads but as

00:02:16,219 --> 00:02:19,609
you can see by the Green Line there's a

00:02:18,170 --> 00:02:20,900
gap between processor and memory

00:02:19,609 --> 00:02:22,760
performance

00:02:20,900 --> 00:02:25,459
and in the context of amtel's law as

00:02:22,760 --> 00:02:27,470
computers growing faster their access

00:02:25,459 --> 00:02:30,980
the relative of memory access time is

00:02:27,470 --> 00:02:33,579
only growing slower unfortunately many

00:02:30,980 --> 00:02:36,530
graphs applications are memory bound and

00:02:33,579 --> 00:02:38,780
furthermore these graphic locations need

00:02:36,530 --> 00:02:40,549
to process data sets that are massive

00:02:38,780 --> 00:02:41,900
and continuing to grow exponentially so

00:02:40,549 --> 00:02:45,200
like the Twitter network contains

00:02:41,900 --> 00:02:47,689
millions of nodes and the ability to

00:02:45,200 --> 00:02:49,220
process these networks hasn't kept up so

00:02:47,689 --> 00:02:50,959
we need efficient graph processing

00:02:49,220 --> 00:02:55,879
techniques that can keep up with our

00:02:50,959 --> 00:02:57,920
modern data sets so in order to design

00:02:55,879 --> 00:02:59,780
efficient graph processing techniques we

00:02:57,920 --> 00:03:01,609
need to understand their bottlenecks and

00:02:59,780 --> 00:03:03,170
because many graph applications are

00:03:01,609 --> 00:03:07,159
memory bound we look at their data

00:03:03,170 --> 00:03:08,959
access patterns so as you saw in the

00:03:07,159 --> 00:03:10,639
last presentation we were introduced to

00:03:08,959 --> 00:03:12,739
the idea of a frontier so we look at

00:03:10,639 --> 00:03:14,930
many graph applications that are from

00:03:12,739 --> 00:03:16,879
iterative and frontier based and this

00:03:14,930 --> 00:03:18,739
includes the widespread breadth-first

00:03:16,879 --> 00:03:21,650
search single source shortest paths and

00:03:18,739 --> 00:03:24,560
PageRank algorithms so what does it mean

00:03:21,650 --> 00:03:27,379
to be iterative frontier based well like

00:03:24,560 --> 00:03:28,910
we saw we have a frontier of node we

00:03:27,379 --> 00:03:31,340
have multiple iterations to traverse the

00:03:28,910 --> 00:03:33,169
graph and within each iteration of the

00:03:31,340 --> 00:03:35,030
graph our iteration of the algorithm we

00:03:33,169 --> 00:03:38,239
have a frontier of nodes that contains

00:03:35,030 --> 00:03:40,310
the IDS we want to process and then we

00:03:38,239 --> 00:03:43,400
also have this flat array node Val's

00:03:40,310 --> 00:03:45,079
which stores the per node properties so

00:03:43,400 --> 00:03:46,549
depending on the objective of the

00:03:45,079 --> 00:03:48,859
algorithm we store a different type of

00:03:46,549 --> 00:03:50,479
data for each node so in breadth-first

00:03:48,859 --> 00:03:53,629
search this would be the number of hops

00:03:50,479 --> 00:03:55,699
away from our given source node and then

00:03:53,629 --> 00:03:57,530
on the right we have the kernel template

00:03:55,699 --> 00:03:59,419
for these iterative reference to your

00:03:57,530 --> 00:04:01,609
base graft applications so I'm going to

00:03:59,419 --> 00:04:02,840
walk through this template in the

00:04:01,609 --> 00:04:05,900
context of the breadth-first search

00:04:02,840 --> 00:04:08,959
algorithm so we're starting with our

00:04:05,900 --> 00:04:11,299
root node of 0 and so for every node in

00:04:08,959 --> 00:04:13,790
our frontier will you do a processing of

00:04:11,299 --> 00:04:15,709
that node and then we look at all of

00:04:13,790 --> 00:04:19,340
those note all of that nodes neighbors

00:04:15,709 --> 00:04:22,039
and so in this case we well we do an

00:04:19,340 --> 00:04:24,560
update of update neighbor function on

00:04:22,039 --> 00:04:26,150
that neighbor and the exact details of

00:04:24,560 --> 00:04:27,710
this function depend on the objective of

00:04:26,150 --> 00:04:30,020
the algorithm in the case of

00:04:27,710 --> 00:04:32,150
breadth-first search we do a load to the

00:04:30,020 --> 00:04:34,400
node Val's array in order to determine

00:04:32,150 --> 00:04:36,410
if the node has been visited and

00:04:34,400 --> 00:04:38,810
if it has not been visited then we need

00:04:36,410 --> 00:04:41,650
to store the number of hops away that

00:04:38,810 --> 00:04:44,720
note is from our given source node and

00:04:41,650 --> 00:04:46,610
so because these updates depend on the

00:04:44,720 --> 00:04:49,850
location of these neighbors these

00:04:46,610 --> 00:04:52,490
updates need an indirect memory access

00:04:49,850 --> 00:04:54,770
and this leads to as you can see in this

00:04:52,490 --> 00:04:59,210
flat array this leads to irregular

00:04:54,770 --> 00:05:01,310
accesses within this array so this is

00:04:59,210 --> 00:05:05,300
going to be the key thing to think about

00:05:01,310 --> 00:05:07,490
going forward and then looking at the

00:05:05,300 --> 00:05:09,169
notes that have not been visited we add

00:05:07,490 --> 00:05:12,110
those to the frontier for the next

00:05:09,169 --> 00:05:15,320
iteration of the algorithm and then this

00:05:12,110 --> 00:05:20,750
process continues until we regeneration

00:05:15,320 --> 00:05:22,820
where our frontier is empty so why are

00:05:20,750 --> 00:05:25,100
irregular memory accesses problematic

00:05:22,820 --> 00:05:27,710
well modern memory hierarchies are

00:05:25,100 --> 00:05:30,260
composed of multiple caches and caches

00:05:27,710 --> 00:05:32,210
are designed to store frequently

00:05:30,260 --> 00:05:34,370
accessed data that is stored in

00:05:32,210 --> 00:05:36,440
contiguous blocks so when your memory

00:05:34,370 --> 00:05:39,530
accesses are irregular caches are not

00:05:36,440 --> 00:05:42,050
amenable to these accesses you can see

00:05:39,530 --> 00:05:45,440
this is highlighted by this sample

00:05:42,050 --> 00:05:47,330
memory hierarchy below as we have to

00:05:45,440 --> 00:05:48,919
traverse the memory hierarchy and as we

00:05:47,330 --> 00:05:50,750
missed in each level of the cache we

00:05:48,919 --> 00:05:53,510
eventually go off chip to the main

00:05:50,750 --> 00:05:55,729
memory and so if you recall in the

00:05:53,510 --> 00:05:57,889
previous kernel-based in the kernel

00:05:55,729 --> 00:05:59,120
template the update neighbors function

00:05:57,889 --> 00:06:01,729
that performed the irregular memory

00:05:59,120 --> 00:06:03,979
accesses was inside a nested loop and so

00:06:01,729 --> 00:06:05,720
it occurred very frequently and so we

00:06:03,979 --> 00:06:09,229
define a regular memory accesses that

00:06:05,720 --> 00:06:12,680
occur frequently as llamas this is our

00:06:09,229 --> 00:06:16,130
acronym for them and so to quantify why

00:06:12,680 --> 00:06:17,900
llamas are problematic we look at five

00:06:16,130 --> 00:06:20,660
different graph and sparse applications

00:06:17,900 --> 00:06:23,389
and break down their run times into

00:06:20,660 --> 00:06:24,979
compute versus memory so all of the

00:06:23,389 --> 00:06:27,500
compute is highlighted by the orange

00:06:24,979 --> 00:06:30,320
bars and the memory accesses are broken

00:06:27,500 --> 00:06:32,840
into the llamas versus our non llamas in

00:06:30,320 --> 00:06:34,460
the yellow and as you can see the llamas

00:06:32,840 --> 00:06:38,960
are dominating the run time for all of

00:06:34,460 --> 00:06:41,599
these applications this graph below

00:06:38,960 --> 00:06:44,570
shows specifically the llamas last level

00:06:41,599 --> 00:06:46,039
cache miss rate so if you look at all of

00:06:44,570 --> 00:06:47,689
these five different applications you

00:06:46,039 --> 00:06:51,169
can see that the last level cache

00:06:47,689 --> 00:06:53,719
rate is 0.5 or above which means that 50

00:06:51,169 --> 00:06:55,879
more than 50% of the time these llamas

00:06:53,719 --> 00:06:59,239
are performing an expensive long lengthy

00:06:55,879 --> 00:07:00,829
memory access to main memory so because

00:06:59,239 --> 00:07:02,959
llamas have a disproportionately large

00:07:00,829 --> 00:07:04,729
impact on the performance of these graph

00:07:02,959 --> 00:07:08,629
applications our work seeks to

00:07:04,729 --> 00:07:11,659
specifically address them and thus we

00:07:08,629 --> 00:07:14,749
introduced our approach fast llamas so

00:07:11,659 --> 00:07:17,209
fast llamas is another acronym short for

00:07:14,749 --> 00:07:18,829
full stack approach and specialization

00:07:17,209 --> 00:07:22,039
techniques for hiding long latency

00:07:18,829 --> 00:07:23,869
memory accesses so this at a high level

00:07:22,039 --> 00:07:26,479
is a data supply approach that

00:07:23,869 --> 00:07:28,959
efficiently Maps graph applications on

00:07:26,479 --> 00:07:31,579
two pairs of producer and consumer cores

00:07:28,959 --> 00:07:33,919
and we have a programming model that can

00:07:31,579 --> 00:07:35,899
allow for more explicit mapping of these

00:07:33,919 --> 00:07:37,219
applications as well as specialized

00:07:35,899 --> 00:07:39,499
hardware support that can a

00:07:37,219 --> 00:07:42,369
synchronously issue irregular memory

00:07:39,499 --> 00:07:44,479
accesses and we do get results of

00:07:42,369 --> 00:07:48,289
impressive speed ups from this and I

00:07:44,479 --> 00:07:50,149
will show those at the end first I'm

00:07:48,289 --> 00:07:54,349
going to give a brief overview of the

00:07:50,149 --> 00:07:56,959
decoupling technique so decoupling is

00:07:54,349 --> 00:07:58,849
the TEC is a technique where a program

00:07:56,959 --> 00:08:01,009
is statically divided into two

00:07:58,849 --> 00:08:03,019
independent instruction streams one of

00:08:01,009 --> 00:08:05,300
these streams is mapped onto a producer

00:08:03,019 --> 00:08:08,029
core and this core is responsible for

00:08:05,300 --> 00:08:10,759
all memory accesses and the necessary

00:08:08,029 --> 00:08:12,889
address computation for these accesses

00:08:10,759 --> 00:08:14,839
and then the consumer core is

00:08:12,889 --> 00:08:17,629
responsible for all the valued

00:08:14,839 --> 00:08:20,089
computation so these cores run

00:08:17,629 --> 00:08:21,739
independently and in parallel and so

00:08:20,089 --> 00:08:24,860
this creates a form of heterogeneous

00:08:21,739 --> 00:08:26,929
parallelism it's to Ellis trait the

00:08:24,860 --> 00:08:29,389
contrast these two execution timelines

00:08:26,929 --> 00:08:31,759
below show on the Left homogeneous

00:08:29,389 --> 00:08:35,419
parallelism where the two threads the

00:08:31,759 --> 00:08:37,399
top and the bottom rows are performing

00:08:35,419 --> 00:08:39,050
the same types of computation and memory

00:08:37,399 --> 00:08:41,990
access whereas when you have

00:08:39,050 --> 00:08:43,819
heterogeneous parallelism one thread is

00:08:41,990 --> 00:08:45,889
responsible for the memory accesses this

00:08:43,819 --> 00:08:47,180
is the producer core and then the other

00:08:45,889 --> 00:08:50,569
thread is doing the cup of the

00:08:47,180 --> 00:08:52,550
computation so homogeneous parallelism

00:08:50,569 --> 00:08:54,709
is great when you have a very compute

00:08:52,550 --> 00:08:56,870
bound application whereas when you have

00:08:54,709 --> 00:09:00,060
a memory bound application heterogeneous

00:08:56,870 --> 00:09:02,580
parallelism comes into play

00:09:00,060 --> 00:09:05,100
so that main idea of decoupling is to

00:09:02,580 --> 00:09:07,890
tolerate memory latency and this is done

00:09:05,100 --> 00:09:09,660
by having the producer issue requests to

00:09:07,890 --> 00:09:12,390
the memory hierarchies and retrieving

00:09:09,660 --> 00:09:15,210
the data before the consumer needs it

00:09:12,390 --> 00:09:17,640
and so these cores utilize a specialized

00:09:15,210 --> 00:09:20,070
hardware the communication queue in

00:09:17,640 --> 00:09:22,440
order to have the producer store the

00:09:20,070 --> 00:09:27,480
data before the consumer needs to eat

00:09:22,440 --> 00:09:28,740
this data and this timeline on the

00:09:27,480 --> 00:09:31,140
bottom right that I pointed to earlier

00:09:28,740 --> 00:09:34,350
just illustrates how this can tolerate

00:09:31,140 --> 00:09:36,090
memory latency so there's a warm-up

00:09:34,350 --> 00:09:38,220
period where these two cores start

00:09:36,090 --> 00:09:40,050
running at the same time so the producer

00:09:38,220 --> 00:09:42,210
needs to gather its run ahead but it

00:09:40,050 --> 00:09:45,120
does this very quickly and once it's

00:09:42,210 --> 00:09:47,300
done this then once the producer can

00:09:45,120 --> 00:09:50,240
asynchronously issue memory accesses and

00:09:47,300 --> 00:09:53,310
the consumers then never has to stall

00:09:50,240 --> 00:09:55,080
whereas typically these long lean these

00:09:53,310 --> 00:09:56,450
memory accesses would be long latency

00:09:55,080 --> 00:10:01,440
and the consumer would have to wait

00:09:56,450 --> 00:10:03,450
waiting for this data because decoupling

00:10:01,440 --> 00:10:05,310
creates two different instruction

00:10:03,450 --> 00:10:07,560
streams that are independent but the

00:10:05,310 --> 00:10:10,230
original dependencies in the program are

00:10:07,560 --> 00:10:12,180
now changed and remapped so that

00:10:10,230 --> 00:10:15,030
dependencies are only respective to the

00:10:12,180 --> 00:10:17,190
individual slices so there might be a

00:10:15,030 --> 00:10:19,170
dependency on the producers memory

00:10:17,190 --> 00:10:21,390
access but now this could be mapped onto

00:10:19,170 --> 00:10:23,580
the consumer and so when this does

00:10:21,390 --> 00:10:26,700
happen we take advantage of this with

00:10:23,580 --> 00:10:29,670
asynchronous accesses so asynchronous

00:10:26,700 --> 00:10:32,040
accesses are memory accesses where the

00:10:29,670 --> 00:10:34,710
data is not later used by the producer

00:10:32,040 --> 00:10:36,810
so this is where the producer can hand

00:10:34,710 --> 00:10:38,850
it off to the consumer and move on to

00:10:36,810 --> 00:10:41,940
issue its other memory it's later memory

00:10:38,850 --> 00:10:43,740
accesses and so as a result it doesn't

00:10:41,940 --> 00:10:46,560
have to occupy its Hardware structures

00:10:43,740 --> 00:10:49,050
or its pipeline resources and this is

00:10:46,560 --> 00:10:50,760
illustrated on the right where we have

00:10:49,050 --> 00:10:53,880
two different execution timelines the

00:10:50,760 --> 00:10:55,890
top one shows the scenario where there

00:10:53,880 --> 00:10:58,590
are no asynchronous memory accesses so

00:10:55,890 --> 00:11:00,840
as you can see that each memory access

00:10:58,590 --> 00:11:03,000
the producer needs to issue depends on

00:11:00,840 --> 00:11:04,830
the previous one and this leads to

00:11:03,000 --> 00:11:07,830
frequent stalling both on the producer

00:11:04,830 --> 00:11:10,440
and the consumer whereas when we have

00:11:07,830 --> 00:11:12,840
asynchronous memory accesses the

00:11:10,440 --> 00:11:13,920
producer can issue a request and move on

00:11:12,840 --> 00:11:15,690
to its next one with

00:11:13,920 --> 00:11:19,940
having to wait for the previous one to

00:11:15,690 --> 00:11:22,800
finish and following sorry

00:11:19,940 --> 00:11:26,690
following this warmup period the

00:11:22,800 --> 00:11:29,190
consumer never has to stall as a result

00:11:26,690 --> 00:11:30,690
so now I'm going to talk about how fast

00:11:29,190 --> 00:11:36,420
Mama's leverages this decoupling

00:11:30,690 --> 00:11:39,570
technique to tolerate latency so to

00:11:36,420 --> 00:11:41,120
illustrate or to provide a contrast this

00:11:39,570 --> 00:11:44,310
is the original

00:11:41,120 --> 00:11:45,300
kernel for the iterative graph it are

00:11:44,310 --> 00:11:48,750
different to your based graph

00:11:45,300 --> 00:11:51,420
applications this is broken down into

00:11:48,750 --> 00:11:53,880
three high-level functions the process

00:11:51,420 --> 00:11:56,670
node which is highlighted by the orange

00:11:53,880 --> 00:11:59,339
boxes and then we have the update

00:11:56,670 --> 00:12:01,680
neighbors which is our llama this is

00:11:59,339 --> 00:12:04,260
highlighted by the red boxes and then

00:12:01,680 --> 00:12:06,779
the conditional a conditional addition

00:12:04,260 --> 00:12:09,870
of nodes to the frontier is highlighted

00:12:06,779 --> 00:12:13,589
by the blue boxes and so when we execute

00:12:09,870 --> 00:12:16,620
this template on an inorder core we can

00:12:13,589 --> 00:12:21,540
see that the llamas are dominating the

00:12:16,620 --> 00:12:23,910
runtime but fast llamas decouples this

00:12:21,540 --> 00:12:26,519
program so that the process node

00:12:23,910 --> 00:12:28,529
function it's mapped onto the producer

00:12:26,519 --> 00:12:31,170
so in this execution timeline on the

00:12:28,529 --> 00:12:33,600
lower left we have the producer is the

00:12:31,170 --> 00:12:37,110
top row and the consumer is the bottom

00:12:33,600 --> 00:12:38,850
row and then the middle row this wide

00:12:37,110 --> 00:12:40,800
row shows what's happening

00:12:38,850 --> 00:12:43,649
asynchronously in the memory hierarchy

00:12:40,800 --> 00:12:45,209
so this isn't mapped onto a core the

00:12:43,649 --> 00:12:48,390
producer and the consumer are the two

00:12:45,209 --> 00:12:50,100
cores running in parallel and so the

00:12:48,390 --> 00:12:54,180
producer does the note processing and

00:12:50,100 --> 00:12:55,980
then it can issue llamas this is shown

00:12:54,180 --> 00:12:58,230
by the small boxes with the indet

00:12:55,980 --> 00:13:01,110
written on them so it can issue a

00:12:58,230 --> 00:13:03,029
regular memory access and then continue

00:13:01,110 --> 00:13:04,980
to issue on its next one and these are

00:13:03,029 --> 00:13:07,199
not time consuming operations and then

00:13:04,980 --> 00:13:09,360
the llamas are issued or running

00:13:07,199 --> 00:13:11,880
asynchronously in the memory hierarchy

00:13:09,360 --> 00:13:14,550
and then when their data comes back the

00:13:11,880 --> 00:13:17,490
consumer can eat the data and continue

00:13:14,550 --> 00:13:19,829
on with its respective functions so

00:13:17,490 --> 00:13:21,329
there's a warm-up period again where the

00:13:19,829 --> 00:13:23,730
producer needs to gain its initial run

00:13:21,329 --> 00:13:24,690
ahead but then it could continue from

00:13:23,730 --> 00:13:26,819
there

00:13:24,690 --> 00:13:29,940
and the llamas are asynchronously issued

00:13:26,819 --> 00:13:32,069
after this warmup period and as a result

00:13:29,940 --> 00:13:34,800
the consumer is never stalling waiting

00:13:32,069 --> 00:13:40,350
for these llamas and fast llamas is able

00:13:34,800 --> 00:13:42,990
to tolerate memory latency so this is

00:13:40,350 --> 00:13:44,790
this is a relatively detailed hardware

00:13:42,990 --> 00:13:47,279
diagram I'm not going to talk about all

00:13:44,790 --> 00:13:49,230
of the individual parts of it but I'm

00:13:47,279 --> 00:13:51,870
gonna go over the main additions that

00:13:49,230 --> 00:13:56,069
fast llamas uses to support this in

00:13:51,870 --> 00:13:58,110
hardware so we have an specialised

00:13:56,069 --> 00:14:00,629
buffer called the asynchronous access

00:13:58,110 --> 00:14:03,329
buffer and so this is when this is used

00:14:00,629 --> 00:14:05,730
when the producer issues a memory access

00:14:03,329 --> 00:14:08,970
and then this asynchronous access buffer

00:14:05,730 --> 00:14:11,910
can store the addresses of the inflight

00:14:08,970 --> 00:14:13,050
memory requests and then when the data

00:14:11,910 --> 00:14:15,990
comes back from the memory hierarchy

00:14:13,050 --> 00:14:18,660
then these this data is matched with its

00:14:15,990 --> 00:14:20,310
corresponding address and then the data

00:14:18,660 --> 00:14:22,019
is passed to the communication queue

00:14:20,310 --> 00:14:25,459
between the producer and consumer cores

00:14:22,019 --> 00:14:28,110
and then the consumer can use this data

00:14:25,459 --> 00:14:31,620
so when we have an asynchronous memory

00:14:28,110 --> 00:14:33,389
access its issued by the producer sent

00:14:31,620 --> 00:14:35,279
to the memory hierarchy its address is

00:14:33,389 --> 00:14:38,399
tracked like I mentioned and then when

00:14:35,279 --> 00:14:40,949
the data comes back sometimes the data

00:14:38,399 --> 00:14:43,439
might be modified and this can be sent

00:14:40,949 --> 00:14:48,240
directly to the memory hierarchy or onto

00:14:43,439 --> 00:14:51,060
the queue the communication queue so now

00:14:48,240 --> 00:14:54,600
I'm gonna show some results of this

00:14:51,060 --> 00:14:56,610
approach we looked at five different

00:14:54,600 --> 00:14:58,949
graph in sparse applications these are

00:14:56,610 --> 00:15:00,990
the five that I mentioned before with

00:14:58,949 --> 00:15:03,300
the llama graph so we have two

00:15:00,990 --> 00:15:05,490
applications on top element-wise farce

00:15:03,300 --> 00:15:07,829
dense is a matrix multiplication between

00:15:05,490 --> 00:15:09,930
a sparse matrix and a dense matrix and

00:15:07,829 --> 00:15:11,880
then we have bipartite graph projections

00:15:09,930 --> 00:15:13,800
which are which is an algorithm that

00:15:11,880 --> 00:15:16,019
operates on a bipartite graph and it

00:15:13,800 --> 00:15:18,259
relates nodes in one graph based off of

00:15:16,019 --> 00:15:21,000
their common neighbors and the other one

00:15:18,259 --> 00:15:23,939
and then we have a vertex programmable

00:15:21,000 --> 00:15:25,980
graph processing algorithm so we use

00:15:23,939 --> 00:15:27,209
three of the most widespread algorithms

00:15:25,980 --> 00:15:30,269
breadth first search single source

00:15:27,209 --> 00:15:32,399
shortest paths and PageRank and the

00:15:30,269 --> 00:15:34,500
difference between these algorithms and

00:15:32,399 --> 00:15:36,689
the two above are that these algorithms

00:15:34,500 --> 00:15:38,760
require an explicit annotation by the

00:15:36,689 --> 00:15:41,010
programmer so our programming

00:15:38,760 --> 00:15:43,980
support sack of annotation that allows

00:15:41,010 --> 00:15:47,580
the programmer to explicitly guide

00:15:43,980 --> 00:15:49,920
mapping so it can tell the compiler that

00:15:47,580 --> 00:15:53,820
performs our decoupling to map and

00:15:49,920 --> 00:15:57,420
memory access onto the consumer and then

00:15:53,820 --> 00:15:59,100
the top two applications do not they can

00:15:57,420 --> 00:16:04,440
automatically be sliced with our

00:15:59,100 --> 00:16:06,870
compiler so going back to the decades

00:16:04,440 --> 00:16:08,970
Hardware we have the notion of quartiles

00:16:06,870 --> 00:16:12,060
and so these quartiles can be

00:16:08,970 --> 00:16:13,760
reconfigured so we can have them as two

00:16:12,060 --> 00:16:16,470
parallel quartiles that run

00:16:13,760 --> 00:16:18,630
simultaneously or we could have a fast

00:16:16,470 --> 00:16:21,720
llamas pair which is a producer quartile

00:16:18,630 --> 00:16:24,770
and a consumer quartile and so we

00:16:21,720 --> 00:16:27,600
evaluate both of these configurations

00:16:24,770 --> 00:16:29,790
when we compare these two we could see

00:16:27,600 --> 00:16:32,730
that highlighted by the blue and the

00:16:29,790 --> 00:16:36,060
yellow bars in this graph which this

00:16:32,730 --> 00:16:38,280
graph shows the geo mean of each of

00:16:36,060 --> 00:16:39,900
these five applications so we run we ran

00:16:38,280 --> 00:16:42,150
these applications on multiple different

00:16:39,900 --> 00:16:43,980
types of data sets a combination of real

00:16:42,150 --> 00:16:46,770
and synthetic networks but we're just

00:16:43,980 --> 00:16:49,080
showing their geo means here and looking

00:16:46,770 --> 00:16:50,520
at the geo means we can see that fast

00:16:49,080 --> 00:16:54,200
llamas outperform traditional

00:16:50,520 --> 00:16:56,610
parallelism by up to two categories and

00:16:54,200 --> 00:16:58,680
then because graph applications are

00:16:56,610 --> 00:17:00,090
memory bound we look at an inorder core

00:16:58,680 --> 00:17:02,280
with the perfect cache because this

00:17:00,090 --> 00:17:05,070
provides a performance idealization as

00:17:02,280 --> 00:17:07,949
if every memory access had only latency

00:17:05,070 --> 00:17:09,660
of one cycle and so we can see that

00:17:07,949 --> 00:17:13,199
looking across the board at the orange

00:17:09,660 --> 00:17:15,720
and yellow bars fast I was able to

00:17:13,199 --> 00:17:19,020
achieve up to 96.2%

00:17:15,720 --> 00:17:21,120
of perfect cache performance and then

00:17:19,020 --> 00:17:23,459
looking at fast llamas compared to our

00:17:21,120 --> 00:17:26,400
baseline performance which is that of a

00:17:23,459 --> 00:17:29,370
single inorder quartile we see up to a

00:17:26,400 --> 00:17:31,110
5.3 2x performance improvement here but

00:17:29,370 --> 00:17:33,480
when we looked at individual application

00:17:31,110 --> 00:17:39,540
input combinations we saw up to an eight

00:17:33,480 --> 00:17:42,000
point six exact speedo so this work was

00:17:39,540 --> 00:17:45,150
supported by DARPA as I ventured before

00:17:42,000 --> 00:17:46,500
and so in conclusion fast llamas is

00:17:45,150 --> 00:17:48,630
Hardware soft it's a hardware software

00:17:46,500 --> 00:17:51,559
co.design approach that tolerates

00:17:48,630 --> 00:17:53,779
latency on graph applications with

00:17:51,559 --> 00:17:56,029
it's programming model its compiler that

00:17:53,779 --> 00:17:58,730
can perform the automatic slicing into

00:17:56,029 --> 00:18:00,230
producer-consumer pairs and this the

00:17:58,730 --> 00:18:03,769
specialized hardware support for

00:18:00,230 --> 00:18:06,559
asynchronous memory accesses are team

00:18:03,769 --> 00:18:08,149
members so decades is a large effort

00:18:06,559 --> 00:18:10,730
between prison and Columbia so our key

00:18:08,149 --> 00:18:13,700
members are listed here and then you can

00:18:10,730 --> 00:18:15,830
access our applications our compiler and

00:18:13,700 --> 00:18:18,529
the simulator we use to get these

00:18:15,830 --> 00:18:22,129
performance results at these links below

00:18:18,529 --> 00:18:24,019
and this is also being implemented to be

00:18:22,129 --> 00:18:25,580
designed on our chip so the RTL is in

00:18:24,019 --> 00:18:50,330
progress but that will be available soon

00:18:25,580 --> 00:18:52,850
as well so the question was can this

00:18:50,330 --> 00:18:54,860
architecture be or mitigate latency in

00:18:52,850 --> 00:18:58,970
Deptford surging strongly connected

00:18:54,860 --> 00:19:00,769
components so this will will see the

00:18:58,970 --> 00:19:03,200
most impressive speed ups when you have

00:19:00,769 --> 00:19:04,820
these llamas these like long links

00:19:03,200 --> 00:19:08,269
memory axises dong dominating the

00:19:04,820 --> 00:19:10,100
performance this could be I guess it

00:19:08,269 --> 00:19:11,629
depends on the implementation of the

00:19:10,100 --> 00:19:13,399
algorithm but we study the most work

00:19:11,629 --> 00:19:16,279
efficient ones where the long least

00:19:13,399 --> 00:19:18,529
memory accesses are exposed so I think

00:19:16,279 --> 00:19:20,149
in depth-first search the long lean

00:19:18,529 --> 00:19:26,259
scenarios are not as much of a problem

00:19:20,149 --> 00:19:26,259
there but it could work

00:19:50,130 --> 00:19:54,740
and then my question is about how you

00:20:06,100 --> 00:20:11,880
so I think you're asking about cases

00:20:08,140 --> 00:20:11,880
where the consumer needs

00:20:31,299 --> 00:20:35,249
you you because of the memory

00:20:43,200 --> 00:20:49,050
oh so you are you asking if there's like

00:20:47,220 --> 00:20:54,000
a limited window which the decoupling

00:20:49,050 --> 00:20:56,040
can attack okay so the when we do the

00:20:54,000 --> 00:20:59,880
decoupling the compiler so the program

00:20:56,040 --> 00:21:02,870
is sliced so the compiler producer has

00:20:59,880 --> 00:21:06,930
it sees its list of a memory accesses to

00:21:02,870 --> 00:21:08,490
issue one an instance is where the

00:21:06,930 --> 00:21:10,770
programming model actually comes in so

00:21:08,490 --> 00:21:13,350
we have an annotation in our programming

00:21:10,770 --> 00:21:15,510
model that can map or tell the compiler

00:21:13,350 --> 00:21:18,300
just to put certain memory accesses on

00:21:15,510 --> 00:21:21,120
the consumer and so in that case we

00:21:18,300 --> 00:21:23,040
would leverage that annotation so that

00:21:21,120 --> 00:21:24,630
the consumer could just do these memory

00:21:23,040 --> 00:21:27,140
accesses and not have to wait for the

00:21:24,630 --> 00:21:27,140

YouTube URL: https://www.youtube.com/watch?v=5cexgG-H9Q4


