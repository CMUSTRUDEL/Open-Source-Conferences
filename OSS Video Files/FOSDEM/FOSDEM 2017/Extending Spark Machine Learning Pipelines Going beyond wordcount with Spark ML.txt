Title: Extending Spark Machine Learning Pipelines Going beyond wordcount with Spark ML
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Holden Karau

At: FOSDEM 2017

Apache Spark is one of the most popular new "big data" technologies, and nowhas a sci-kit-learn inspired pipeline API. This talk looks at how the pipelineAPI works as well as how to add your own custom algorithms to Apache Spark.

Apache Spark is one of the most popular new "big data" technologies, and nowhas a sci-kit-learn inspired pipeline API. This talk looks at how the pipelineAPI works as well as how to add your own custom algorithms to Apache Spark.The talk will be focused in Scala, but the same techniques can be used in Javaor with other JVM languages. Sadly extending the pipeline API can notcurrently be done in non-JVM languages, but the information on how to use thepipeline API will be useful to Python and R users as well.


Room: H.2213
Scheduled start: 2017-02-04 13:25:00
Captions: 
	00:00:00,000 --> 00:00:02,420
cool

00:00:05,840 --> 00:00:09,559
so thank you for staying because you're

00:00:07,970 --> 00:00:11,990
all stuck here anyways

00:00:09,559 --> 00:00:16,180
this is extending spark ml super happy

00:00:11,990 --> 00:00:16,180
new pipeline stage time yay

00:00:16,279 --> 00:00:20,570
I'm holdin my preferred pronouns are she

00:00:18,500 --> 00:00:23,270
or her it's tattooed on my wrist in case

00:00:20,570 --> 00:00:25,550
you forget don't it up and I'm a

00:00:23,270 --> 00:00:27,619
principal software engineer at IBM spark

00:00:25,550 --> 00:00:29,239
Technology Center if anyone's looking to

00:00:27,619 --> 00:00:33,470
relocate to America for some reason

00:00:29,239 --> 00:00:34,640
still come and talk to me and I've been

00:00:33,470 --> 00:00:37,039
doing this had a bunch of other

00:00:34,640 --> 00:00:39,949
companies for a while you can follow me

00:00:37,039 --> 00:00:41,800
on Twitter and it's mostly just tweets

00:00:39,949 --> 00:00:44,600
being really sad about America right now

00:00:41,800 --> 00:00:46,309
so if you're too happy following me on

00:00:44,600 --> 00:00:49,100
Twitter and I'll fix that for you and

00:00:46,309 --> 00:00:52,039
then we'll talk about software will be

00:00:49,100 --> 00:00:54,079
worthwhile we're gonna talk about what

00:00:52,039 --> 00:00:55,370
spark ml pipelines look like and then

00:00:54,079 --> 00:00:58,609
we're gonna jump into extending them

00:00:55,370 --> 00:01:00,800
really quickly and the topic said it was

00:00:58,609 --> 00:01:03,019
going beyond word count but we've got

00:01:00,800 --> 00:01:07,340
ten minutes so it's sort of beyond what

00:01:03,019 --> 00:01:09,920
counted so yeah spark ml pipelines are

00:01:07,340 --> 00:01:11,660
pretty much the spark developers looking

00:01:09,920 --> 00:01:16,010
at scikit-learn and going you shiny

00:01:11,660 --> 00:01:17,480
steel unfortunately their big data

00:01:16,010 --> 00:01:19,400
developers and so they got kind of

00:01:17,480 --> 00:01:21,590
halfway through stealing I didn't bother

00:01:19,400 --> 00:01:23,450
stealing all of the algorithms but they

00:01:21,590 --> 00:01:26,330
they stole the api's so if you're

00:01:23,450 --> 00:01:27,680
familiar with scikit-learn spark ml

00:01:26,330 --> 00:01:30,590
pipelines are gonna be really familiar

00:01:27,680 --> 00:01:32,240
to you and they're exposed in Scala and

00:01:30,590 --> 00:01:34,880
Python and you can use it to do

00:01:32,240 --> 00:01:37,190
distributed machine learning but the

00:01:34,880 --> 00:01:39,080
algorithm selection isn't like 100

00:01:37,190 --> 00:01:40,490
percent right you might have some really

00:01:39,080 --> 00:01:42,290
crazy problem that you're trying to

00:01:40,490 --> 00:01:44,360
solve and you need an algorithm that

00:01:42,290 --> 00:01:46,850
isn't already in spark and the good news

00:01:44,360 --> 00:01:49,520
is you can implement it yourself

00:01:46,850 --> 00:01:52,000
and the stages in these pipelines are

00:01:49,520 --> 00:01:54,470
either estimators or transformers

00:01:52,000 --> 00:01:55,640
transformers are the little blue boxes

00:01:54,470 --> 00:01:57,740
and those are just things which don't

00:01:55,640 --> 00:01:59,030
need to be trained on data and so

00:01:57,740 --> 00:02:00,800
they're really simple to implement we

00:01:59,030 --> 00:02:02,750
can just write in a normal spark job and

00:02:00,800 --> 00:02:04,700
add a little bit of sugar and make a

00:02:02,750 --> 00:02:06,440
transformer but if you want to build

00:02:04,700 --> 00:02:07,820
something fancy you like a machine

00:02:06,440 --> 00:02:09,920
learning algorithm or something which

00:02:07,820 --> 00:02:12,170
needs to be trained on data you have to

00:02:09,920 --> 00:02:15,340
build an estimator and we'll look at how

00:02:12,170 --> 00:02:15,340
to do that really quickly

00:02:15,910 --> 00:02:19,970
yeah these tourists

00:02:17,840 --> 00:02:23,060
whatever just cover up this part of the

00:02:19,970 --> 00:02:26,090
slides no one sees this part but we have

00:02:23,060 --> 00:02:27,980
to provide a transform schema and a copy

00:02:26,090 --> 00:02:29,870
function and these are the two sort of

00:02:27,980 --> 00:02:32,690
like non-standard spark bits that we

00:02:29,870 --> 00:02:35,750
have to add if we want to turn our spark

00:02:32,690 --> 00:02:37,280
job into an ml pipeline component and

00:02:35,750 --> 00:02:39,410
then we had can add some configuration

00:02:37,280 --> 00:02:42,019
as well and then if you add the

00:02:39,410 --> 00:02:44,420
configuration with the spark way you can

00:02:42,019 --> 00:02:46,040
do like parameter grid searches and and

00:02:44,420 --> 00:02:48,019
automatically tune your models and

00:02:46,040 --> 00:02:49,730
because I'm really bad at statistics I

00:02:48,019 --> 00:02:51,890
frequently use parameter grid search

00:02:49,730 --> 00:02:55,160
because I don't know how to pick my

00:02:51,890 --> 00:02:58,519
own regularization parameter you do that

00:02:55,160 --> 00:03:00,799
for me so here is our hard-coded word

00:02:58,519 --> 00:03:02,390
count stage as a licensed Big Data

00:03:00,799 --> 00:03:04,879
instructor I have to put word count

00:03:02,390 --> 00:03:07,819
everywhere you may wonder why it's in

00:03:04,879 --> 00:03:10,640
every single talk ever and this is why

00:03:07,819 --> 00:03:12,950
but we've got a copy function and our

00:03:10,640 --> 00:03:14,480
constructor it's not super important you

00:03:12,950 --> 00:03:16,370
can pretty much just steal this and

00:03:14,480 --> 00:03:19,790
change the name from hard-coded word

00:03:16,370 --> 00:03:22,310
count to hard-coded magic algorithm and

00:03:19,790 --> 00:03:23,989
for copy you've just called default copy

00:03:22,310 --> 00:03:27,709
unless you need to do something really

00:03:23,989 --> 00:03:30,680
weird in my sample set of two calling

00:03:27,709 --> 00:03:33,200
default copy works 100% of the time if

00:03:30,680 --> 00:03:35,840
it doesn't work for you please don't

00:03:33,200 --> 00:03:40,099
tell me I'll have to change the talk and

00:03:35,840 --> 00:03:41,870
that that would just be sad and so now

00:03:40,099 --> 00:03:45,049
this is the part which is different than

00:03:41,870 --> 00:03:46,400
our normal spark code because these

00:03:45,049 --> 00:03:48,919
machine learning pipelines can take a

00:03:46,400 --> 00:03:51,280
really long time to run and because data

00:03:48,919 --> 00:03:53,720
frames aren't compile time type checked

00:03:51,280 --> 00:03:56,959
we want to do at least some type

00:03:53,720 --> 00:03:57,980
checking before we start our like 8 hour

00:03:56,959 --> 00:04:00,139
or 24 hour

00:03:57,980 --> 00:04:02,750
machine learning job and then come back

00:04:00,139 --> 00:04:05,030
halfway through and be like oh it failed

00:04:02,750 --> 00:04:07,340
because like I got the name wrong of

00:04:05,030 --> 00:04:09,620
this one random thing and so transform

00:04:07,340 --> 00:04:11,450
schema is a thing which gives us the

00:04:09,620 --> 00:04:13,519
ability to verify that our inputs are

00:04:11,450 --> 00:04:15,620
sort of what we're expecting and tell

00:04:13,519 --> 00:04:17,479
the downstream people what our outputs

00:04:15,620 --> 00:04:19,970
are going to be like I mean if this code

00:04:17,479 --> 00:04:21,769
looks kind of weird to you it's ok it's

00:04:19,970 --> 00:04:23,330
just the data frame code and it's really

00:04:21,769 --> 00:04:25,789
simple there's a lot of introductions to

00:04:23,330 --> 00:04:28,250
it and that's where like these types

00:04:25,789 --> 00:04:30,229
come from this string type is not the

00:04:28,250 --> 00:04:31,550
Scala string type it's the spark string

00:04:30,229 --> 00:04:35,810
type which is mildly differ

00:04:31,550 --> 00:04:37,610
in ways that are exciting and not for a

00:04:35,810 --> 00:04:42,110
10-minute talk and then we can just add

00:04:37,610 --> 00:04:44,930
our result here and then we can do our

00:04:42,110 --> 00:04:46,159
work and so this is really simple right

00:04:44,930 --> 00:04:47,479
we're just counting the words that are

00:04:46,159 --> 00:04:49,879
being put in I'm not even doing the

00:04:47,479 --> 00:04:53,060
proper word count but it's really simple

00:04:49,879 --> 00:04:56,030
and you could do something actually

00:04:53,060 --> 00:04:58,699
complex I find that doing this with the

00:04:56,030 --> 00:05:00,979
UDF even though spark has a really nice

00:04:58,699 --> 00:05:04,610
way to do integrated functional

00:05:00,979 --> 00:05:06,800
programming and relational stuff it

00:05:04,610 --> 00:05:08,780
doesn't work really well with this part

00:05:06,800 --> 00:05:11,150
because generally what we want to do is

00:05:08,780 --> 00:05:12,440
we want to just add things to the data

00:05:11,150 --> 00:05:14,599
that's coming and we don't want to like

00:05:12,440 --> 00:05:16,130
get rid of anything and we don't

00:05:14,599 --> 00:05:17,960
necessarily know all of the types that

00:05:16,130 --> 00:05:20,120
are coming in so it's hard to use the

00:05:17,960 --> 00:05:23,449
full data set API so you end up having

00:05:20,120 --> 00:05:24,770
to use this kind of ugly UDF syntax but

00:05:23,449 --> 00:05:27,110
it's not the end of the world right

00:05:24,770 --> 00:05:28,400
usually this is much nicer than writing

00:05:27,110 --> 00:05:31,460
a high view D Athens

00:05:28,400 --> 00:05:34,699
I promise you've never written a high

00:05:31,460 --> 00:05:36,680
view TF you're lucky and and this is

00:05:34,699 --> 00:05:39,020
where your fun can be and if you're a

00:05:36,680 --> 00:05:40,669
researcher you can spend like months

00:05:39,020 --> 00:05:42,289
working on the code that's gonna live

00:05:40,669 --> 00:05:43,990
inside of transform if you're in

00:05:42,289 --> 00:05:46,310
industry you can spend like a day or two

00:05:43,990 --> 00:05:48,110
or if your boss is wondering what you're

00:05:46,310 --> 00:05:50,889
doing just like have them email me I'll

00:05:48,110 --> 00:05:53,440
be like no it's super important trust me

00:05:50,889 --> 00:05:55,759
and you can go and do some crazy stuff

00:05:53,440 --> 00:05:58,819
and then we need to configure our

00:05:55,759 --> 00:06:01,940
pipeline stage for pipeline stages in

00:05:58,819 --> 00:06:04,340
SPARC we use this parameter interface

00:06:01,940 --> 00:06:07,610
and this just gives spark a standard way

00:06:04,340 --> 00:06:10,430
of being able to configure and do sort

00:06:07,610 --> 00:06:11,990
of parameter good searching you probably

00:06:10,430 --> 00:06:13,819
wouldn't do a parameter grid search on

00:06:11,990 --> 00:06:15,889
your output column let's be a little

00:06:13,819 --> 00:06:18,560
weird but you might do it on your input

00:06:15,889 --> 00:06:20,180
cone and you're not limited to strings

00:06:18,560 --> 00:06:22,130
here right so these these are string

00:06:20,180 --> 00:06:25,819
parameters but these could definitely be

00:06:22,130 --> 00:06:27,229
doubles you know floats bins whatever so

00:06:25,819 --> 00:06:29,509
you could do a parameter search on your

00:06:27,229 --> 00:06:31,039
on your tuna Graham and then you provide

00:06:29,509 --> 00:06:36,259
some setters and those are for like

00:06:31,039 --> 00:06:38,960
humans use and it's really simple and

00:06:36,259 --> 00:06:41,870
yeah so we do it this way so that sparks

00:06:38,960 --> 00:06:43,699
meta algorithms can work on it if you

00:06:41,870 --> 00:06:46,310
don't know what parameters you should be

00:06:43,699 --> 00:06:48,580
adding to your machine learning

00:06:46,310 --> 00:06:50,930
you can go look at shared pram Scala

00:06:48,580 --> 00:06:52,910
inside of Sparks codebase and just like

00:06:50,930 --> 00:06:55,430
steal the ones that you want you'll have

00:06:52,910 --> 00:06:57,440
to cut and paste the code or lie about

00:06:55,430 --> 00:06:59,780
being inside of package org Apache spark

00:06:57,440 --> 00:07:01,580
because it's all private if you lie

00:06:59,780 --> 00:07:04,160
about being inside a package org Apache

00:07:01,580 --> 00:07:07,040
spark I don't know who you are it's not

00:07:04,160 --> 00:07:09,560
my fault but I do it pretty often myself

00:07:07,040 --> 00:07:11,530
and so you know no harm no foul but your

00:07:09,560 --> 00:07:14,120
cause is probably gonna break me upward

00:07:11,530 --> 00:07:14,900
what's the future use problem though and

00:07:14,120 --> 00:07:16,669
if I've learned anything about

00:07:14,900 --> 00:07:18,290
scientists it's that they don't seem to

00:07:16,669 --> 00:07:20,780
think about future you when it comes to

00:07:18,290 --> 00:07:24,380
their software no offense

00:07:20,780 --> 00:07:26,210
so that's really boring we made word

00:07:24,380 --> 00:07:28,820
count and I promised we were gonna go

00:07:26,210 --> 00:07:29,330
beyond word count um so let's go beyond

00:07:28,820 --> 00:07:31,580
word count

00:07:29,330 --> 00:07:34,460
we're gonna make an estimator this is

00:07:31,580 --> 00:07:37,220
the fancy thing no one's excited but

00:07:34,460 --> 00:07:40,910
it's okay it's fancy it's gonna like

00:07:37,220 --> 00:07:43,280
actually train on some input data we do

00:07:40,910 --> 00:07:46,040
pretty much the same thing so what our

00:07:43,280 --> 00:07:48,320
estimator does is we write a fit

00:07:46,040 --> 00:07:51,590
function we have the same parameters to

00:07:48,320 --> 00:07:53,539
configure it and then our fit function

00:07:51,590 --> 00:07:58,610
is going to return our transformer and

00:07:53,539 --> 00:08:01,100
so our it's really simple yeah I owe

00:07:58,610 --> 00:08:05,120
this says November oh yeah that actually

00:08:01,100 --> 00:08:08,900
is old but let's let's look at a really

00:08:05,120 --> 00:08:11,419
simple estimate ah you know oh this is

00:08:08,900 --> 00:08:13,789
an old version of the slides dammit okay

00:08:11,419 --> 00:08:16,310
well so I don't have the code for the

00:08:13,789 --> 00:08:20,620
estimator but it's a fit function which

00:08:16,310 --> 00:08:23,180
is just gonna return our transformer yay

00:08:20,620 --> 00:08:25,100
so pretend that there's a fit function

00:08:23,180 --> 00:08:27,200
and it's gonna construct new hard-coded

00:08:25,100 --> 00:08:29,660
word count stage or whatever it is you

00:08:27,200 --> 00:08:32,839
want and if you want to see the code for

00:08:29,660 --> 00:08:36,680
that it's totally in this really nice

00:08:32,839 --> 00:08:39,050
blog post which is a lot longer than 10

00:08:36,680 --> 00:08:42,260
minutes unfortunately but hopefully

00:08:39,050 --> 00:08:44,510
maybe I trick someone into thinking that

00:08:42,260 --> 00:08:46,370
making their own spark machine learning

00:08:44,510 --> 00:08:48,290
algorithm is a good use of their time

00:08:46,370 --> 00:08:50,990
and you'll publish it to maven central

00:08:48,290 --> 00:08:53,029
and then I can use it or tell people to

00:08:50,990 --> 00:08:56,390
use it and we can all hang out and have

00:08:53,029 --> 00:09:00,070
fun if I didn't trick anyone into doing

00:08:56,390 --> 00:09:00,070
that that's okay too

00:09:00,560 --> 00:09:05,960
there's also this github repo with a

00:09:03,020 --> 00:09:09,740
bunch of examples they're written mostly

00:09:05,960 --> 00:09:11,000
by me so they're a little yeah but if

00:09:09,740 --> 00:09:12,680
you want to go ahead and look at the

00:09:11,000 --> 00:09:15,140
ones that are inside a spark itself you

00:09:12,680 --> 00:09:19,250
can find them here and those ones are a

00:09:15,140 --> 00:09:20,570
little more I want to say professional

00:09:19,250 --> 00:09:23,450
or production-ready

00:09:20,570 --> 00:09:24,860
they're not like all great but they put

00:09:23,450 --> 00:09:27,350
a lot more thought into them and I put

00:09:24,860 --> 00:09:29,300
into mine so you can definitely check

00:09:27,350 --> 00:09:31,370
those out and they're gonna use internal

00:09:29,300 --> 00:09:33,230
api's but it's okay you can just like

00:09:31,370 --> 00:09:36,709
lie and say you're inside a package org

00:09:33,230 --> 00:09:41,690
apache spark and there's api

00:09:36,709 --> 00:09:43,490
documentation and fun stuff as an author

00:09:41,690 --> 00:09:44,839
of some books i would be remiss if i

00:09:43,490 --> 00:09:50,300
didn't try and get you to give me your

00:09:44,839 --> 00:09:52,160
money and so and these aren't all my

00:09:50,300 --> 00:09:55,459
books to be clear i don't have that much

00:09:52,160 --> 00:09:58,070
time in my life don't buy this one

00:09:55,459 --> 00:10:00,200
that's kind of out of date authors won't

00:09:58,070 --> 00:10:02,779
normally tell you that but you should

00:10:00,200 --> 00:10:05,120
buy all of these other books even the

00:10:02,779 --> 00:10:07,010
ones I didn't write but if you have an

00:10:05,120 --> 00:10:11,240
expense account does anyone have an

00:10:07,010 --> 00:10:13,640
expense account no okay well and then

00:10:11,240 --> 00:10:15,620
don't buy several copies of this but if

00:10:13,640 --> 00:10:16,820
you know someone with an expense account

00:10:15,620 --> 00:10:19,100
maybe someone that works at Bloomberg

00:10:16,820 --> 00:10:21,560
you can get them to buy several copies

00:10:19,100 --> 00:10:23,810
of high-performance spark it is the

00:10:21,560 --> 00:10:27,470
holiday gift of the season for whatever

00:10:23,810 --> 00:10:28,910
holiday is coming up next if for some

00:10:27,470 --> 00:10:31,970
reason you don't want to buy a book

00:10:28,910 --> 00:10:34,310
which doesn't exist yet that's that's

00:10:31,970 --> 00:10:36,110
okay I understand you can give me your

00:10:34,310 --> 00:10:38,120
email address at high performance bar

00:10:36,110 --> 00:10:41,870
comm and they'll spend the hell out of

00:10:38,120 --> 00:10:43,550
you as soon as the book is finished but

00:10:41,870 --> 00:10:45,320
please give me your money today rather

00:10:43,550 --> 00:10:48,750
than tomorrow because money today is

00:10:45,320 --> 00:10:56,290
coffee so thank you

00:10:48,750 --> 00:11:04,220
[Applause]

00:10:56,290 --> 00:11:05,839
okay Oh doggies name is boo she comes

00:11:04,220 --> 00:11:08,949
with me to all of my talks where I

00:11:05,839 --> 00:11:08,949

YouTube URL: https://www.youtube.com/watch?v=Z_4Jm7mVqq0


