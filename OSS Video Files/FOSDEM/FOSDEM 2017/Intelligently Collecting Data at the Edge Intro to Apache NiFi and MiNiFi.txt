Title: Intelligently Collecting Data at the Edge Intro to Apache NiFi and MiNiFi
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Andy LoPresto

At: FOSDEM 2017

Apache NiFi provided a revolutionary data flow management system with a broadrange of integrations with existing data production, consumption, and analysisecosystems, all covered with robust data delivery and provenanceinfrastructure. Now learn about the follow-on project which expands the reachof NiFi to the edge, Apache MiNiFi. MiNiFi is a lightweight application whichcan be deployed on hardware orders of magnitude smaller and less powerful thanthe existing standard data collection platforms. With both a JVM compatibleand native agent, MiNiFi allows data collection in brand new environments â€”sensors with tiny footprints, distributed systems with intermittent orrestricted bandwidth, and even disposable or ephemeral hardware. Not only canthis data be prioritized and have some initial analysis performed at the edge,it can be encrypted and secured immediately. Local governance and regulatorypolicies can be applied across geopolitical boundaries to conform with legalrequirements. And all of this configuration can be done from central command &control using an existing NiFi with the trusted and stable UI data flowmanagers already love.


Room: H.2213
Scheduled start: 2017-02-04 14:30:00
Captions: 
	00:00:11,180 --> 00:00:20,310
to introduce a putz in my fight all

00:00:18,510 --> 00:00:21,810
right thank you thank you for to FOSDEM

00:00:20,310 --> 00:00:23,490
for having me here I apologize for my

00:00:21,810 --> 00:00:26,040
appearance I'm from LA and I forgot that

00:00:23,490 --> 00:00:28,260
it rained anywhere else in the world so

00:00:26,040 --> 00:00:30,120
yeah I am Angela presto I work at Horton

00:00:28,260 --> 00:00:32,730
works on a project management committee

00:00:30,120 --> 00:00:36,000
member and a committer on Apache knife I

00:00:32,730 --> 00:00:37,469
and the sub-project Apache minify and

00:00:36,000 --> 00:00:41,399
we're gonna talk about intelligently

00:00:37,469 --> 00:00:43,019
collecting data at the edge so I don't

00:00:41,399 --> 00:00:45,719
know if you can quite read this its

00:00:43,019 --> 00:00:49,050
introduction both to the application and

00:00:45,719 --> 00:00:51,089
to myself I have been working on

00:00:49,050 --> 00:00:54,289
security topics for about 13 years

00:00:51,089 --> 00:00:58,440
I joined the knife I open-source team

00:00:54,289 --> 00:01:01,829
got here as I said from LA and that's a

00:00:58,440 --> 00:01:04,080
little bit about me so we'll talk very

00:01:01,829 --> 00:01:05,760
briefly about what dataflow is and what

00:01:04,080 --> 00:01:08,400
the challenges involved in collecting

00:01:05,760 --> 00:01:10,259
the data are I apologize this is a

00:01:08,400 --> 00:01:11,700
fairly long presentation that's been

00:01:10,259 --> 00:01:13,649
compressed down to combine multiple

00:01:11,700 --> 00:01:15,030
things I will stick around afterwards

00:01:13,649 --> 00:01:19,290
for any questions but I'm gonna go

00:01:15,030 --> 00:01:21,539
through a little bit quickly today it's

00:01:19,290 --> 00:01:23,520
very simple right data flow is getting

00:01:21,539 --> 00:01:26,549
data from one plate one place one point

00:01:23,520 --> 00:01:28,979
to another we have producers so that

00:01:26,549 --> 00:01:32,270
could be computers log files it could be

00:01:28,979 --> 00:01:34,530
devices it could be user interaction

00:01:32,270 --> 00:01:37,049
send it usually over the Internet and

00:01:34,530 --> 00:01:40,820
then somebody or something needs to make

00:01:37,049 --> 00:01:43,500
sense of that data but it's hard and

00:01:40,820 --> 00:01:45,810
it's not the first time this problem has

00:01:43,500 --> 00:01:47,969
tried to be solved and many people have

00:01:45,810 --> 00:01:50,609
tried to do it in many different ways so

00:01:47,969 --> 00:01:54,750
the classic example of 14 to 15

00:01:50,609 --> 00:01:56,700
standards why is moving data effectively

00:01:54,750 --> 00:01:59,429
hard because moving data is not that

00:01:56,700 --> 00:02:01,799
hard right you can just put a modem

00:01:59,429 --> 00:02:03,719
somewhere and you have data flow but

00:02:01,799 --> 00:02:05,369
doing it correctly doing it effectively

00:02:03,719 --> 00:02:06,960
doing it in a way that makes sense for

00:02:05,369 --> 00:02:09,840
the consumers and producers of that data

00:02:06,960 --> 00:02:11,880
is very difficult like I said we have

00:02:09,840 --> 00:02:13,890
standards we have different formats we

00:02:11,880 --> 00:02:15,060
have exactly once delivery right which

00:02:13,890 --> 00:02:17,099
if anybody has worked with data before

00:02:15,060 --> 00:02:18,090
is a really difficult problem we have

00:02:17,099 --> 00:02:19,200
the protocol

00:02:18,090 --> 00:02:22,530
the veracity and validity of the

00:02:19,200 --> 00:02:24,390
information ensuring security over

00:02:22,530 --> 00:02:25,950
comings of security which as a security

00:02:24,390 --> 00:02:28,650
person I object to this completely

00:02:25,950 --> 00:02:32,720
accurate statement compliance schemas

00:02:28,650 --> 00:02:35,700
etc etc and again exactly once delivery

00:02:32,720 --> 00:02:36,629
so we're gonna connect a to B to C and

00:02:35,700 --> 00:02:38,879
it's very simple and straightforward

00:02:36,629 --> 00:02:40,560
we're gonna read log files off of the

00:02:38,879 --> 00:02:43,260
system we're gonna ingest them into

00:02:40,560 --> 00:02:44,910
sequel we're going to collect all that

00:02:43,260 --> 00:02:46,980
data from a bunch of different instances

00:02:44,910 --> 00:02:49,379
put it into a data Lake and run all of

00:02:46,980 --> 00:02:51,450
our queries and machine learning and

00:02:49,379 --> 00:02:54,930
modeling and everything on that data big

00:02:51,450 --> 00:02:56,310
data well this is easy right you write a

00:02:54,930 --> 00:02:58,500
bash script to put onto every computer

00:02:56,310 --> 00:03:01,459
you have a little bit of Python and you

00:02:58,500 --> 00:03:04,799
can script it together and there you go

00:03:01,459 --> 00:03:06,390
let's look at a model of courier service

00:03:04,799 --> 00:03:09,090
so I guess DHL is the mid 1 over here

00:03:06,390 --> 00:03:10,349
right we've got mobile devices everybody

00:03:09,090 --> 00:03:12,209
has a scanner that they're scanning the

00:03:10,349 --> 00:03:14,310
packages doesn't deliver them there are

00:03:12,209 --> 00:03:17,129
registers in the store where we're

00:03:14,310 --> 00:03:19,680
accepting and delivery parcels

00:03:17,129 --> 00:03:21,209
we have trucks we have delivers and all

00:03:19,680 --> 00:03:22,859
this goes to a gateway server that's in

00:03:21,209 --> 00:03:24,209
each store and it goes up to a

00:03:22,859 --> 00:03:27,180
distribution center where we have a

00:03:24,209 --> 00:03:29,190
cluster and we even have a data center

00:03:27,180 --> 00:03:30,720
right professional data center and we

00:03:29,190 --> 00:03:35,880
can do all of our machine learning and

00:03:30,720 --> 00:03:38,190
Big Data there so great I could get all

00:03:35,880 --> 00:03:40,260
this data now what I'm going to do with

00:03:38,190 --> 00:03:42,299
it well I'm gonna throw it into Kafka

00:03:40,260 --> 00:03:43,980
and use that to send it to storm and

00:03:42,299 --> 00:03:46,280
spark and flink and apex and anything

00:03:43,980 --> 00:03:48,690
else that's on Apaches homepage today

00:03:46,280 --> 00:03:50,340
you you can send it to any other

00:03:48,690 --> 00:03:51,750
consumer right I mean somebody is gonna

00:03:50,340 --> 00:03:53,430
want as soon as you start collecting

00:03:51,750 --> 00:03:56,220
data somebody's gonna want it ok that's

00:03:53,430 --> 00:03:57,810
a universal truth usually they want the

00:03:56,220 --> 00:04:01,620
data you don't have yet that's even

00:03:57,810 --> 00:04:02,940
harder so now let's scale this up

00:04:01,620 --> 00:04:05,220
because we're going to be an

00:04:02,940 --> 00:04:08,459
international courrier service and we

00:04:05,220 --> 00:04:09,870
have people and computers on every

00:04:08,459 --> 00:04:11,940
different continent every different

00:04:09,870 --> 00:04:13,169
corner of the globe raise your hand if

00:04:11,940 --> 00:04:17,760
you want to maintain the Python scripts

00:04:13,169 --> 00:04:19,320
for that for the rest of your life so

00:04:17,760 --> 00:04:20,910
let's talk about Apache knife I I've

00:04:19,320 --> 00:04:24,210
definitely stated some problems let's

00:04:20,910 --> 00:04:26,130
try and solve some knife I is an open

00:04:24,210 --> 00:04:29,250
source project part of the Apache

00:04:26,130 --> 00:04:30,830
Software Foundation it provides a ton of

00:04:29,250 --> 00:04:32,120
features that

00:04:30,830 --> 00:04:34,220
or we're gonna be very important for

00:04:32,120 --> 00:04:38,479
your data flow and data collection data

00:04:34,220 --> 00:04:41,780
buffering is huge now if I comes from an

00:04:38,479 --> 00:04:44,870
area where the importance of data can't

00:04:41,780 --> 00:04:47,449
be overstated and protecting that data

00:04:44,870 --> 00:04:49,580
but also not losing that data is really

00:04:47,449 --> 00:04:51,560
really important one of the features

00:04:49,580 --> 00:04:53,569
that I probably won't get into in too

00:04:51,560 --> 00:04:56,419
much detail but backpressure allows you

00:04:53,569 --> 00:04:58,849
to provide custom configurations so that

00:04:56,419 --> 00:05:00,740
if some piece of your chain starts

00:04:58,849 --> 00:05:02,870
slowing down it can actually send

00:05:00,740 --> 00:05:04,759
information to the predecessors and

00:05:02,870 --> 00:05:07,370
inform them of that information and

00:05:04,759 --> 00:05:08,900
allow them to either prioritize the data

00:05:07,370 --> 00:05:10,009
that you need and stop sending

00:05:08,900 --> 00:05:10,430
everything that's just first-in

00:05:10,009 --> 00:05:13,250
first-out

00:05:10,430 --> 00:05:15,979
it can start sending it to a different

00:05:13,250 --> 00:05:17,780
flow and backing it up putting it to

00:05:15,979 --> 00:05:19,340
buffer all kinds of things you can do

00:05:17,780 --> 00:05:21,470
with it that's really valuable

00:05:19,340 --> 00:05:22,819
like I said prioritize queuing we have

00:05:21,470 --> 00:05:24,800
quality of service that can be

00:05:22,819 --> 00:05:26,270
customized based on the flow data

00:05:24,800 --> 00:05:28,610
provenance is something that I will get

00:05:26,270 --> 00:05:31,699
into a little bit more detail we support

00:05:28,610 --> 00:05:34,039
push and pull models and it's a visual

00:05:31,699 --> 00:05:35,780
tool it's it's not something where you

00:05:34,039 --> 00:05:37,250
have to have a masters in order to start

00:05:35,780 --> 00:05:39,889
the thing up it's something that anybody

00:05:37,250 --> 00:05:42,409
with domain knowledge can use which

00:05:39,889 --> 00:05:44,509
really makes it easy to get the right

00:05:42,409 --> 00:05:46,310
people involved right because you're not

00:05:44,509 --> 00:05:48,289
always gonna be able to say everybody

00:05:46,310 --> 00:05:50,240
who has valuable insight to this problem

00:05:48,289 --> 00:05:52,009
is able to run the terminal and do

00:05:50,240 --> 00:05:53,150
everything there right so we're trying

00:05:52,009 --> 00:05:54,380
to open it up and make sure that you get

00:05:53,150 --> 00:05:59,050
the right people involved in your

00:05:54,380 --> 00:06:00,979
process not by originated with

00:05:59,050 --> 00:06:04,699
consequent flow based programming

00:06:00,979 --> 00:06:06,620
so there is some vocabulary basically a

00:06:04,699 --> 00:06:08,900
glossary here you'll hear me talk about

00:06:06,620 --> 00:06:10,279
flow files that's basically your atomic

00:06:08,900 --> 00:06:12,139
unit of data okay that's the thing

00:06:10,279 --> 00:06:14,029
that's moving through your system flow

00:06:12,139 --> 00:06:16,580
file processor is some component some

00:06:14,029 --> 00:06:18,169
black box it doesn't matter what the

00:06:16,580 --> 00:06:20,180
internal implementation is it's

00:06:18,169 --> 00:06:24,050
something that you can connect data in

00:06:20,180 --> 00:06:25,090
to get data out of and operate obviously

00:06:24,050 --> 00:06:27,770
in order to do that you need connections

00:06:25,090 --> 00:06:30,620
flow controller is essentially a

00:06:27,770 --> 00:06:32,210
scheduler right it's the the overarching

00:06:30,620 --> 00:06:33,650
system that's running all of this and

00:06:32,210 --> 00:06:35,930
orchestrating it and then a process

00:06:33,650 --> 00:06:38,419
group is basically just a collection of

00:06:35,930 --> 00:06:41,830
these processors and connections

00:06:38,419 --> 00:06:41,830
logically grouped together

00:06:42,379 --> 00:06:47,009
so knife I use completely data agnostic

00:06:45,029 --> 00:06:49,319
we really don't care what you're using

00:06:47,009 --> 00:06:51,089
it for but it was designed understanding

00:06:49,319 --> 00:06:54,149
that users have to care about the

00:06:51,089 --> 00:06:56,369
specifics and it allows you to do a lot

00:06:54,149 --> 00:07:01,649
of transformation or manipulation of the

00:06:56,369 --> 00:07:04,619
formats and protocols within the tool so

00:07:01,649 --> 00:07:07,229
a really good example to understand a

00:07:04,619 --> 00:07:10,649
flow file is to think of it using the

00:07:07,229 --> 00:07:13,080
analogy of an HTTP data all right so

00:07:10,649 --> 00:07:14,369
this is an HTTP response you can see

00:07:13,080 --> 00:07:17,159
that there's a header and then there's

00:07:14,369 --> 00:07:20,789
some content below a flow file is very

00:07:17,159 --> 00:07:23,039
similar we have attributes which is key

00:07:20,789 --> 00:07:25,439
value mapping and then we have the

00:07:23,039 --> 00:07:26,729
binary content of the flow file and the

00:07:25,439 --> 00:07:28,769
reason that this is important to be

00:07:26,729 --> 00:07:31,139
separated logically is that the

00:07:28,769 --> 00:07:33,959
attributes are maintained in memory

00:07:31,139 --> 00:07:35,819
they're accessible all the time they're

00:07:33,959 --> 00:07:36,959
usually pretty small right they're used

00:07:35,819 --> 00:07:39,659
for routing they're used for

00:07:36,959 --> 00:07:41,969
classification tagging access control

00:07:39,659 --> 00:07:44,699
the content could be anything from a

00:07:41,969 --> 00:07:47,759
couple bites of text to ten gigs of

00:07:44,699 --> 00:07:48,899
video okay when you're routing data you

00:07:47,759 --> 00:07:50,399
really don't want to be moving all of

00:07:48,899 --> 00:07:52,289
that through the heap constantly right

00:07:50,399 --> 00:07:54,959
so what we do is we've split this into

00:07:52,289 --> 00:07:56,610
two different storage capacities one is

00:07:54,959 --> 00:07:59,039
what we call the flow flow file

00:07:56,610 --> 00:08:01,379
repository which stores all of these

00:07:59,039 --> 00:08:04,319
flow files with their attributes but it

00:08:01,379 --> 00:08:06,749
then has similar to a pointer write a

00:08:04,319 --> 00:08:08,669
reference pointer it has a claim to

00:08:06,749 --> 00:08:11,339
content that's in a content repository

00:08:08,669 --> 00:08:13,860
so what we do is everything is dealt

00:08:11,339 --> 00:08:16,559
with through streaming interfaces the

00:08:13,860 --> 00:08:18,479
data the content is read into a content

00:08:16,559 --> 00:08:19,919
repository it's referenced from there

00:08:18,479 --> 00:08:21,929
but it means that it's not always on the

00:08:19,919 --> 00:08:24,449
heap whereas the attributes are always

00:08:21,929 --> 00:08:26,519
available for operation this allows it

00:08:24,449 --> 00:08:29,369
to process a ton of data very quickly

00:08:26,519 --> 00:08:30,509
and respecting the resources that you

00:08:29,369 --> 00:08:32,550
have available I'm not gonna pretend

00:08:30,509 --> 00:08:34,079
like you don't need to have resources in

00:08:32,550 --> 00:08:35,669
order to do this if you're operating on

00:08:34,079 --> 00:08:37,919
that scale of data but it makes it

00:08:35,669 --> 00:08:38,909
easier and it's not unnecessarily

00:08:37,919 --> 00:08:42,569
wasting the resources that you have

00:08:38,909 --> 00:08:45,360
available let's talk about the user

00:08:42,569 --> 00:08:47,699
interface we want less I mean I like

00:08:45,360 --> 00:08:51,149
this but in general we want less of this

00:08:47,699 --> 00:08:52,709
and more of this okay and let's talk

00:08:51,149 --> 00:08:55,860
about the your interface I'm actually

00:08:52,709 --> 00:08:58,050
gonna go up to the board a little bit

00:08:55,860 --> 00:08:59,850
so here we have what's called the

00:08:58,050 --> 00:09:02,310
navigation palette and then the

00:08:59,850 --> 00:09:04,470
operation palette down here across the

00:09:02,310 --> 00:09:06,450
top this this header these are

00:09:04,470 --> 00:09:09,870
components which you can drag on to the

00:09:06,450 --> 00:09:11,340
flow the canvas on the far right the

00:09:09,870 --> 00:09:13,890
hamburger menu I guess we're still

00:09:11,340 --> 00:09:15,690
Conniff that allows you to do get into

00:09:13,890 --> 00:09:18,120
some of the system maintenance and

00:09:15,690 --> 00:09:20,250
operations and then you can see on the

00:09:18,120 --> 00:09:30,020
graph itself your components connections

00:09:20,250 --> 00:09:32,310
etc this is mostly to illustrate that

00:09:30,020 --> 00:09:35,280
people tend to think of data processing

00:09:32,310 --> 00:09:37,080
as happening just in some core data

00:09:35,280 --> 00:09:39,150
center somewhere right you have a team

00:09:37,080 --> 00:09:40,470
that only cares about doing something

00:09:39,150 --> 00:09:42,180
with the data they really don't care

00:09:40,470 --> 00:09:43,140
about how the data got well they care if

00:09:42,180 --> 00:09:44,370
it doesn't actually get there but they

00:09:43,140 --> 00:09:46,760
don't care about how you're collecting

00:09:44,370 --> 00:09:50,040
the data where it's coming from

00:09:46,760 --> 00:09:51,420
on the other hand you have people and

00:09:50,040 --> 00:09:53,190
resources out in the field or at the

00:09:51,420 --> 00:09:56,430
edge which are doing the data collection

00:09:53,190 --> 00:09:57,720
right and not only that there's usually

00:09:56,430 --> 00:10:00,000
some other value they're providing or

00:09:57,720 --> 00:10:01,920
all the tool wouldn't be there one of

00:10:00,000 --> 00:10:05,130
the things that knife eye allows is for

00:10:01,920 --> 00:10:07,620
bi-directional data flow what I mean by

00:10:05,130 --> 00:10:09,960
that is we're extracting or exfilling

00:10:07,620 --> 00:10:11,370
some kind of important data from the

00:10:09,960 --> 00:10:13,620
edge and bring it back to a central

00:10:11,370 --> 00:10:16,740
resource but we also need to be able to

00:10:13,620 --> 00:10:19,530
send commands communication out to the

00:10:16,740 --> 00:10:22,500
edge in order to update the flow

00:10:19,530 --> 00:10:24,510
prioritize what we're getting ask

00:10:22,500 --> 00:10:26,520
somebody to resend something right so

00:10:24,510 --> 00:10:28,380
with this bi-directional data what we

00:10:26,520 --> 00:10:31,260
call the data plane and then the command

00:10:28,380 --> 00:10:33,540
plane or control plane it allows for us

00:10:31,260 --> 00:10:36,030
to improve the data collection

00:10:33,540 --> 00:10:41,070
make sure it's robust and stable as we

00:10:36,030 --> 00:10:42,660
move on we have over 180 processors that

00:10:41,070 --> 00:10:45,930
are available as part of the default

00:10:42,660 --> 00:10:48,870
knife I installation everything from

00:10:45,930 --> 00:10:51,720
sequel and sequel like data stores to

00:10:48,870 --> 00:10:53,730
Big Data to all the different edge

00:10:51,720 --> 00:10:54,090
formats and protocols that you might

00:10:53,730 --> 00:10:59,370
encounter

00:10:54,090 --> 00:11:03,780
so listen TCP log readers just file

00:10:59,370 --> 00:11:07,130
extractors kafka database integration

00:11:03,780 --> 00:11:07,130
all kinds of stuff there

00:11:07,599 --> 00:11:12,999
so HTTP email basically if you can think

00:11:11,679 --> 00:11:14,889
of it if we haven't written the

00:11:12,999 --> 00:11:16,809
processor for it already we can probably

00:11:14,889 --> 00:11:17,889
knock one out unless it's the new

00:11:16,809 --> 00:11:18,219
version of Kafka which will take some

00:11:17,889 --> 00:11:22,119
time

00:11:18,219 --> 00:11:24,569
uh-huh we can do a ton of different

00:11:22,119 --> 00:11:27,119
operations to that data so usually

00:11:24,569 --> 00:11:29,979
manipulating data transforming data

00:11:27,119 --> 00:11:31,929
requires some custom knowledge of that

00:11:29,979 --> 00:11:33,459
format right I want to go from XML to

00:11:31,929 --> 00:11:35,259
JSON well I understand X and I'll have

00:11:33,459 --> 00:11:36,579
to understand JSON I may have to

00:11:35,259 --> 00:11:40,419
understand different schemas that are

00:11:36,579 --> 00:11:43,719
involved here you drag a box on to the

00:11:40,419 --> 00:11:46,799
canvas and you have XML to JSON like I'm

00:11:43,719 --> 00:11:49,599
it really is that easy

00:11:46,799 --> 00:11:51,369
so now let's talk about minify and and I

00:11:49,599 --> 00:11:54,539
am blazing through this so I apologize

00:11:51,369 --> 00:11:57,339
if anything's getting a little confusing

00:11:54,539 --> 00:12:00,369
minify is gonna let us get out to the

00:11:57,339 --> 00:12:03,099
very edge so everybody in this room has

00:12:00,369 --> 00:12:04,449
a computer that's capable of the

00:12:03,099 --> 00:12:07,959
calculations needed to put a man on the

00:12:04,449 --> 00:12:10,799
moon 50 years ago right that kind of

00:12:07,959 --> 00:12:13,389
data collection data processing is

00:12:10,799 --> 00:12:17,579
unprecedented and will only continue to

00:12:13,389 --> 00:12:20,379
grow right data lives in the data center

00:12:17,579 --> 00:12:22,149
where knife I has has brought it in

00:12:20,379 --> 00:12:24,729
transformed it and provided it to

00:12:22,149 --> 00:12:26,979
whatever follow-on system exists but the

00:12:24,729 --> 00:12:29,139
data doesn't start there and so the

00:12:26,979 --> 00:12:32,529
closer we can get to the creation of the

00:12:29,139 --> 00:12:34,919
data the better quality will get the

00:12:32,529 --> 00:12:37,329
better control over that data will have

00:12:34,919 --> 00:12:39,129
we will be able to prioritize it we'll

00:12:37,329 --> 00:12:42,189
be able to secure it will get

00:12:39,129 --> 00:12:44,229
granularity of provenance of history in

00:12:42,189 --> 00:12:50,919
a way that we've never been able to

00:12:44,229 --> 00:12:51,219
before why not just put knife I out

00:12:50,919 --> 00:12:56,409
there

00:12:51,219 --> 00:12:59,499
well knife is big 726 megabytes for last

00:12:56,409 --> 00:13:01,149
release okay it's not a tiny thing it is

00:12:59,499 --> 00:13:05,469
respectful of the hardware like I said

00:13:01,149 --> 00:13:07,449
you can run it on a Raspberry Pi but you

00:13:05,469 --> 00:13:09,789
would like to run it on the heaviest

00:13:07,449 --> 00:13:11,709
machine you can find right we have a

00:13:09,789 --> 00:13:17,169
friend who has a machine running right

00:13:11,709 --> 00:13:18,300
now with 768 gigabytes of RAM and it

00:13:17,169 --> 00:13:21,250
uses it

00:13:18,300 --> 00:13:23,490
but what we'd also like to do is have

00:13:21,250 --> 00:13:25,840
minify where we can put that out on to

00:13:23,490 --> 00:13:30,880
client liberate library on somebody's

00:13:25,840 --> 00:13:33,400
iOS or Android device on every IOT chip

00:13:30,880 --> 00:13:34,930
with six legs that you can stick on a

00:13:33,400 --> 00:13:41,260
wall somewhere all right

00:13:34,930 --> 00:13:47,230
connected car knife eye 726 mix minified

00:13:41,260 --> 00:13:50,470
Java is 45 mix okay minify c++ is 700

00:13:47,230 --> 00:13:57,580
kilobytes so it's a 1,000 times

00:13:50,470 --> 00:13:59,650
improvement on space requirement so one

00:13:57,580 --> 00:14:01,420
of the things that we we did all go into

00:13:59,650 --> 00:14:04,090
an example here of putting minify on a

00:14:01,420 --> 00:14:07,900
connected car this is a project we did

00:14:04,090 --> 00:14:14,140
with qualcomm it allows us to tag data

00:14:07,900 --> 00:14:17,170
immediately so this is a model of the

00:14:14,140 --> 00:14:18,640
network inside of a car and I think it's

00:14:17,170 --> 00:14:21,520
still a little new to a lot of people

00:14:18,640 --> 00:14:25,000
that your car has computers in it now

00:14:21,520 --> 00:14:29,260
and really can't run without them unless

00:14:25,000 --> 00:14:31,720
you have a 50 year old car you have the

00:14:29,260 --> 00:14:33,940
can bus which is your main network

00:14:31,720 --> 00:14:35,500
within the car there's usually Ethernet

00:14:33,940 --> 00:14:38,170
onboard there's also like an

00:14:35,500 --> 00:14:39,640
interconnect Network and then there's

00:14:38,170 --> 00:14:41,860
whatever else has yet to be invented

00:14:39,640 --> 00:14:44,140
right Tesla is certainly not stopping on

00:14:41,860 --> 00:14:47,920
their cars every car manufacturer is

00:14:44,140 --> 00:14:49,990
moving in this direction minify living

00:14:47,920 --> 00:14:53,200
in the car I mean literally on that chip

00:14:49,990 --> 00:14:54,280
in the central computer of the car can

00:14:53,200 --> 00:14:55,600
ingest

00:14:54,280 --> 00:14:57,760
all of the data that's flowing across

00:14:55,600 --> 00:15:00,640
these different networks so everything

00:14:57,760 --> 00:15:05,320
from speed data to break measurements to

00:15:00,640 --> 00:15:07,750
GPS process all of that tag it

00:15:05,320 --> 00:15:10,540
prioritize it maybe filter stuff out

00:15:07,750 --> 00:15:12,250
based on geographic location if you're

00:15:10,540 --> 00:15:13,720
operating in China you're not allowed to

00:15:12,250 --> 00:15:15,730
send any of that information to a

00:15:13,720 --> 00:15:19,120
computer that lives outside of China so

00:15:15,730 --> 00:15:21,460
you can't just say oh well Ford has a

00:15:19,120 --> 00:15:22,780
data center for the entire world we'll

00:15:21,460 --> 00:15:24,070
get all the information in we'll do some

00:15:22,780 --> 00:15:26,320
machine learning and modelling there and

00:15:24,070 --> 00:15:28,450
then we'll send out our learnings now

00:15:26,320 --> 00:15:29,950
you have to have one I think maybe even

00:15:28,450 --> 00:15:31,390
now one for Europe one for the u.s. one

00:15:29,950 --> 00:15:32,110
for China one for Antarctica whatever

00:15:31,390 --> 00:15:34,000
you want to do

00:15:32,110 --> 00:15:35,530
so men if I can start routing that

00:15:34,000 --> 00:15:37,720
information encrypting that information

00:15:35,530 --> 00:15:40,570
prioritizing or filtering it while it's

00:15:37,720 --> 00:15:43,150
still on the car taking data off of a

00:15:40,570 --> 00:15:46,120
car is very expensive if you are near a

00:15:43,150 --> 00:15:48,760
Wi-Fi hotspot the car will prioritize

00:15:46,120 --> 00:15:50,140
that and send more data over Wi-Fi but

00:15:48,760 --> 00:15:51,910
when you're driving on a highway and

00:15:50,140 --> 00:15:53,230
there's somebody around it uses an LTE

00:15:51,910 --> 00:15:54,850
modem that's in the car and that's

00:15:53,230 --> 00:15:56,440
extremely expensive so you really don't

00:15:54,850 --> 00:15:57,760
want to send a bunch of wasted data a

00:15:56,440 --> 00:15:59,590
bunch of unnecessary data or

00:15:57,760 --> 00:16:03,640
uncompressed data over that if you don't

00:15:59,590 --> 00:16:06,220
have to so here's a little diagram the

00:16:03,640 --> 00:16:09,280
map on the right and the boxes there are

00:16:06,220 --> 00:16:11,170
actually showing the ratio of the data

00:16:09,280 --> 00:16:14,950
that's getting exfilled live via LTE

00:16:11,170 --> 00:16:17,350
versus Wi-Fi on the left you see the

00:16:14,950 --> 00:16:19,290
data flow from knife I and that is

00:16:17,350 --> 00:16:21,190
ingesting data off of those networks

00:16:19,290 --> 00:16:23,590
processing it filtering it and then

00:16:21,190 --> 00:16:27,880
sending it via the radios that are in

00:16:23,590 --> 00:16:29,230
the car one of the next things that

00:16:27,880 --> 00:16:32,200
we're really focused on developing is

00:16:29,230 --> 00:16:34,180
the flow versioning right as you develop

00:16:32,200 --> 00:16:35,470
your data flow all of this we'd like to

00:16:34,180 --> 00:16:37,930
use the analogy you're not building

00:16:35,470 --> 00:16:39,580
pipes you're a farmer digging irrigation

00:16:37,930 --> 00:16:41,290
ditches right that water is always

00:16:39,580 --> 00:16:42,610
flowing that river is not stopping

00:16:41,290 --> 00:16:45,370
because oh I need to update this

00:16:42,610 --> 00:16:47,320
processor so what you can do is continue

00:16:45,370 --> 00:16:49,450
a data flow build something new

00:16:47,320 --> 00:16:50,710
alongside of it make sure that works on

00:16:49,450 --> 00:16:52,540
the same data that's coming through

00:16:50,710 --> 00:16:54,490
without stopping anything any follow-on

00:16:52,540 --> 00:16:56,020
system and then when you have a new flow

00:16:54,490 --> 00:16:58,030
that's better you kill the old flow

00:16:56,020 --> 00:17:00,280
you've never stopped moving the data

00:16:58,030 --> 00:17:01,870
from the source to the the destination

00:17:00,280 --> 00:17:05,199
but you've improved your system as

00:17:01,870 --> 00:17:06,670
you're doing that unfortunately you

00:17:05,199 --> 00:17:08,320
can't prove that and then six weeks

00:17:06,670 --> 00:17:09,550
later you find out oh I needed something

00:17:08,320 --> 00:17:11,320
from the other flow that I didn't forgot

00:17:09,550 --> 00:17:12,910
to check well how do you go back to that

00:17:11,320 --> 00:17:14,920
old one so that's where flow versioning

00:17:12,910 --> 00:17:17,530
whether it's on the six week time frame

00:17:14,920 --> 00:17:19,600
or six seconds right I'm reading from

00:17:17,530 --> 00:17:20,770
IOT sources and I need to change the

00:17:19,600 --> 00:17:23,470
priority based on the available

00:17:20,770 --> 00:17:26,290
bandwidth I might want everything if I

00:17:23,470 --> 00:17:27,910
have the bandwidth I might want only top

00:17:26,290 --> 00:17:31,060
priority data when the bandwidth starts

00:17:27,910 --> 00:17:32,920
to get spotty so I can have my might

00:17:31,060 --> 00:17:36,190
control my command control system knife

00:17:32,920 --> 00:17:38,590
I write these rules into my flow and

00:17:36,190 --> 00:17:41,230
then send them back to minify and not

00:17:38,590 --> 00:17:42,730
just one instance event if I all 6.2

00:17:41,230 --> 00:17:44,470
million instances of minify they're

00:17:42,730 --> 00:17:45,970
running on that flow and that's all

00:17:44,470 --> 00:17:49,059
going to be taken care of

00:17:45,970 --> 00:17:51,940
seemlessly think I am getting close okay

00:17:49,059 --> 00:17:53,259
there's plenty more anybody who's

00:17:51,940 --> 00:17:55,299
interested is welcome to come talk to me

00:17:53,259 --> 00:17:56,290
I'll hang out outside and thank you very

00:17:55,299 --> 00:18:03,059
much for having me

00:17:56,290 --> 00:18:11,019
[Applause]

00:18:03,059 --> 00:18:12,580
sorry yeah question is it possible to

00:18:11,019 --> 00:18:14,500
make cues that are consumer controlled

00:18:12,580 --> 00:18:18,070
in knife I I don't understand the

00:18:14,500 --> 00:18:20,409
question fully but my answer is yes yes

00:18:18,070 --> 00:18:22,480
almost definitely it depends on what

00:18:20,409 --> 00:18:35,320
specific consumer format you're talking

00:18:22,480 --> 00:18:47,620
about but yes yes knife I will yes it's

00:18:35,320 --> 00:18:49,389
the opposite of Kafka yes yes yes yeah

00:18:47,620 --> 00:18:53,139
the question was about exactly once

00:18:49,389 --> 00:18:56,169
delivery knife I uses what's called our

00:18:53,139 --> 00:18:58,919
right ahead log to track all of the data

00:18:56,169 --> 00:19:01,450
that's flowing through the system and so

00:18:58,919 --> 00:19:04,990
it will guarantee exactly what's

00:19:01,450 --> 00:19:07,990
delivery here so that that's actually

00:19:04,990 --> 00:19:09,879
this copy-on-write the data is not

00:19:07,990 --> 00:19:11,919
manipulated in place the data exists

00:19:09,879 --> 00:19:13,870
permanently as long as you have storage

00:19:11,919 --> 00:19:15,669
to hold it when it gets sent to a

00:19:13,870 --> 00:19:18,610
follow-on system it receives a

00:19:15,669 --> 00:19:20,169
confirmation it's a two-phase commit or

00:19:18,610 --> 00:19:22,840
signal that the information was received

00:19:20,169 --> 00:19:26,889
if it wasn't not if I can replay that

00:19:22,840 --> 00:19:29,159
information to the following system yes

00:19:26,889 --> 00:19:29,159
and back

00:19:35,440 --> 00:19:42,920
I'm sorry can you repeat that a little

00:19:37,460 --> 00:19:44,930
louder please is it possible to make

00:19:42,920 --> 00:19:46,700
data transformations on the fly yes

00:19:44,930 --> 00:19:48,320
that's that's the whole point of knife I

00:19:46,700 --> 00:19:51,380
I mean that one of the whole points and

00:19:48,320 --> 00:19:52,430
I find so acts like format XML to JSON

00:19:51,380 --> 00:19:55,130
yes

00:19:52,430 --> 00:19:57,340
CSV to parsing records into different

00:19:55,130 --> 00:20:06,140
atomic units yes absolutely

00:19:57,340 --> 00:20:08,000
yes the question was performance versus

00:20:06,140 --> 00:20:09,440
Kafka or other systems that's a whole

00:20:08,000 --> 00:20:10,910
nother talk I mean that we could talk

00:20:09,440 --> 00:20:12,260
about that for an hour I'll take it

00:20:10,910 --> 00:20:19,790
offline with you if you want absolutely

00:20:12,260 --> 00:20:22,760
yes system requirements for minify and

00:20:19,790 --> 00:20:28,130
embedded devices the ability to run C

00:20:22,760 --> 00:20:30,350
and 700k minified bundles it's a

00:20:28,130 --> 00:20:32,030
completely new implementation of the

00:20:30,350 --> 00:20:35,570
same system so we have it in both Java

00:20:32,030 --> 00:20:41,180
on the JVM and a C implementation bare

00:20:35,570 --> 00:20:46,270
metal so Ram I believe it's four

00:20:41,180 --> 00:20:46,270
megabytes of RAM yes

00:20:52,530 --> 00:20:56,610
the question was schema list versus

00:20:54,570 --> 00:20:58,620
schema formats yeah we have plenty of

00:20:56,610 --> 00:21:01,020
processors for Avro we can immediately

00:20:58,620 --> 00:21:03,480
there's one literally called infer Avro

00:21:01,020 --> 00:21:05,430
schema so you bring in arbitrary data

00:21:03,480 --> 00:21:07,110
and it will basically build out the

00:21:05,430 --> 00:21:08,070
schema from that that Avro and

00:21:07,110 --> 00:21:24,690
information

00:21:08,070 --> 00:21:28,490
yes sure the question was two-phase

00:21:24,690 --> 00:21:35,220
commit right ahead log cluster scaling

00:21:28,490 --> 00:21:37,800
okay yeah it scales extremely well it is

00:21:35,220 --> 00:21:40,230
built to be a clustered system so

00:21:37,800 --> 00:21:42,000
there's an entire cluster coordinator it

00:21:40,230 --> 00:21:44,310
uses embedded zookeeper if another

00:21:42,000 --> 00:21:45,540
instances and available resource

00:21:44,310 --> 00:21:46,590
management in allocation is still

00:21:45,540 --> 00:21:48,870
something that we're continuing to work

00:21:46,590 --> 00:21:51,180
on and integrating with like Muzo sir

00:21:48,870 --> 00:21:54,360
yarn or some other resources manager for

00:21:51,180 --> 00:21:55,590
that but knife I will encapsulate the

00:21:54,360 --> 00:21:56,730
resource management it has cluster

00:21:55,590 --> 00:21:58,980
coordinator heartbeat and all that kind

00:21:56,730 --> 00:22:01,200
of stuff as well two-phase commit that's

00:21:58,980 --> 00:22:03,360
for the following systems to acknowledge

00:22:01,200 --> 00:22:05,220
they've received information the right

00:22:03,360 --> 00:22:06,810
ahead log implementation is so that

00:22:05,220 --> 00:22:08,700
we're again the copy-on-write so that

00:22:06,810 --> 00:22:10,290
it's not manipulated data in place you

00:22:08,700 --> 00:22:15,950
don't lose the the record of what that

00:22:10,290 --> 00:22:15,950
data was was there another one okay

00:22:22,120 --> 00:22:30,990
[Applause]

00:22:37,550 --> 00:22:41,070
yeah sorry the question is autonomous

00:22:39,750 --> 00:22:43,680
cars is that appropriate for this

00:22:41,070 --> 00:22:47,870
I mean morally or just like technically

00:22:43,680 --> 00:22:47,870

YouTube URL: https://www.youtube.com/watch?v=OjhliRwc3ZM


