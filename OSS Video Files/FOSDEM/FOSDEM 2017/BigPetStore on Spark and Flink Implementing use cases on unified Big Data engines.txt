Title: BigPetStore on Spark and Flink Implementing use cases on unified Big Data engines
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Marton Balassi

At: FOSDEM 2017

Implementing use cases on unified data platforms. Having a unified dataprocessing engine empowers Big Data application developers as it makesconnections between seemingly unrelated use cases natural. This talk discussesthe implementation of the so-called BigPetStore project (which is a part ofApache Bigtop) in Apache Spark and Apache Flink. The aim BigPetStore is toprovide a common suite to test and benchmark Big Data installations. The talkfeatures best practices and implementation with the batch, streaming, SQL,DataFrames and machine learning APIs of Apache Spark and Apache Flink side byside. A range of use cases are outlined in both systems from data generation,through ETL, recommender systems to online prediction.

Session type Lecture

Session length 30 min + 5 min discussion

Expected prior knowledge / intended audience Basic exposure to Big DataSystems

Speaker bio MÃ¡rton Balassi is a Solution Architect at Cloudera and a PMCmember at Apache Flink. He focuses on Big Data application development,especially in the streaming space. Marton is a regular contributor to opensource and has been a speaker of a number of open source Big Data relatedconferences including Hadoop Summit and Apache Big Data and meetups recently.


Room: H.2213
Scheduled start: 2017-02-04 15:30:00
Captions: 
	00:00:04,930 --> 00:00:11,330
hello everyone I'm Martin I'm a solution

00:00:09,440 --> 00:00:13,429
architect at Cloudera which makes that I

00:00:11,330 --> 00:00:17,360
help customers with Flynn Club there is

00:00:13,429 --> 00:00:19,250
one of the spark sorry which is one of

00:00:17,360 --> 00:00:21,619
the cloud there is one of the vendors

00:00:19,250 --> 00:00:24,410
that you can get out there to support

00:00:21,619 --> 00:00:26,689
you and spark and I'm PMC member of

00:00:24,410 --> 00:00:28,160
Apache Fling so I have quite some

00:00:26,689 --> 00:00:32,449
experience with both of these systems

00:00:28,160 --> 00:00:35,420
and then I'm going to walk you through a

00:00:32,449 --> 00:00:37,940
couple of use cases in this talk mostly

00:00:35,420 --> 00:00:39,680
focusing on the API not necessarily the

00:00:37,940 --> 00:00:42,199
the performance a little bit on the

00:00:39,680 --> 00:00:44,659
architecture of course I'm ready to play

00:00:42,199 --> 00:00:46,400
questions on those as well but basically

00:00:44,659 --> 00:00:49,699
what I would like to give you as a

00:00:46,400 --> 00:00:52,400
takeaway message is these systems are

00:00:49,699 --> 00:00:56,750
evolving a lot in terms of the api's and

00:00:52,400 --> 00:00:58,519
you can go much further with such a

00:00:56,750 --> 00:01:01,010
system than you would imagine

00:00:58,519 --> 00:01:03,229
in terms of use cases so the setup that

00:01:01,010 --> 00:01:05,720
we are going to use for this is the soul

00:01:03,229 --> 00:01:08,299
is this big fat story example she's

00:01:05,720 --> 00:01:10,280
nothing really fancy we will have a

00:01:08,299 --> 00:01:12,560
couple of pet stores so the model that

00:01:10,280 --> 00:01:15,140
we will see is we have a map with a

00:01:12,560 --> 00:01:17,150
couple of stores and we have customers

00:01:15,140 --> 00:01:19,610
shopping around in these stores

00:01:17,150 --> 00:01:21,890
generating transactions and we will take

00:01:19,610 --> 00:01:25,130
it from there

00:01:21,890 --> 00:01:27,350
so we have already best word count today

00:01:25,130 --> 00:01:29,030
a couple of times I will continue with

00:01:27,350 --> 00:01:31,100
that I guess that's that's something

00:01:29,030 --> 00:01:34,820
that everyone has to do to a certain

00:01:31,100 --> 00:01:37,070
level then I will elaborate a little bit

00:01:34,820 --> 00:01:40,100
on the model and then we will see how to

00:01:37,070 --> 00:01:43,310
do a little bit of data generation with

00:01:40,100 --> 00:01:46,610
the spark and flink also how to use

00:01:43,310 --> 00:01:49,370
their sequel api's or the api's that are

00:01:46,610 --> 00:01:51,290
very close to our data frames if you are

00:01:49,370 --> 00:01:53,180
familiar with that then we will do a

00:01:51,290 --> 00:01:55,460
little bit of machine learning and then

00:01:53,180 --> 00:01:58,310
prediction in streaming already near

00:01:55,460 --> 00:02:00,409
real-time so it's going to be a little

00:01:58,310 --> 00:02:02,360
too much code because I also had to cut

00:02:00,409 --> 00:02:05,450
a little bit of the slides from the

00:02:02,360 --> 00:02:09,349
theoretical part of course so what I

00:02:05,450 --> 00:02:11,209
suggest to try to do together is I would

00:02:09,349 --> 00:02:14,060
like to tell you the story and the code

00:02:11,209 --> 00:02:16,790
is going to be there and having the

00:02:14,060 --> 00:02:18,380
story and not necessarily understanding

00:02:16,790 --> 00:02:20,090
the code a hundred percent that

00:02:18,380 --> 00:02:23,930
that's completely what we are shooting

00:02:20,090 --> 00:02:26,450
for today so infamous word count example

00:02:23,930 --> 00:02:29,360
who have already seen a word count in

00:02:26,450 --> 00:02:31,040
their life okay good enough but that

00:02:29,360 --> 00:02:33,680
that's why I put it up there because

00:02:31,040 --> 00:02:34,040
it's it's just half of the people in the

00:02:33,680 --> 00:02:36,470
room

00:02:34,040 --> 00:02:38,780
so basically word count is the hello

00:02:36,470 --> 00:02:40,940
world of big data and that's exactly

00:02:38,780 --> 00:02:43,850
what it is it gives you the first

00:02:40,940 --> 00:02:46,580
impression it has certain merits and we

00:02:43,850 --> 00:02:49,160
should give those to it basically it yes

00:02:46,580 --> 00:02:51,110
unfortunately it may be a little low if

00:02:49,160 --> 00:02:53,240
you are sitting in the background but it

00:02:51,110 --> 00:02:56,510
gives you the idea that the MapReduce

00:02:53,240 --> 00:02:58,820
paradigm does the following for you even

00:02:56,510 --> 00:03:01,490
though you will you have a huge chunk of

00:02:58,820 --> 00:03:04,010
data and even though you will never have

00:03:01,490 --> 00:03:05,840
a global view of it on any single

00:03:04,010 --> 00:03:08,960
machine because it doesn't fit into one

00:03:05,840 --> 00:03:11,960
single machine you can still produce a

00:03:08,960 --> 00:03:15,380
global global account count for a given

00:03:11,960 --> 00:03:18,080
key in it so it it has this power and

00:03:15,380 --> 00:03:19,790
word count exactly demonstrates that but

00:03:18,080 --> 00:03:22,910
it doesn't give you much more people

00:03:19,790 --> 00:03:26,030
sometimes use this also for and that's

00:03:22,910 --> 00:03:28,160
what I'm trying to get into benchmarking

00:03:26,030 --> 00:03:30,140
a big data application okay I have

00:03:28,160 --> 00:03:32,360
installed spark now let's learn a word

00:03:30,140 --> 00:03:33,010
count and see whether it's worse okay or

00:03:32,360 --> 00:03:36,020
not

00:03:33,010 --> 00:03:38,120
well it will benchmark a little bit on

00:03:36,020 --> 00:03:40,100
on the shuffling face so at least you

00:03:38,120 --> 00:03:42,320
have you have this network communication

00:03:40,100 --> 00:03:44,720
maybe the disk i/o in the beginning and

00:03:42,320 --> 00:03:47,330
in in the end but that's basically it

00:03:44,720 --> 00:03:49,310
what happens if you want to do data

00:03:47,330 --> 00:03:51,680
streaming or machine learning or graph

00:03:49,310 --> 00:03:56,150
processing is it going to be enough for

00:03:51,680 --> 00:03:59,240
you well most likely not and that's very

00:03:56,150 --> 00:04:01,940
start bashing word count of course so

00:03:59,240 --> 00:04:04,880
then the other cool stuff that basically

00:04:01,940 --> 00:04:07,400
every one who is talking today has to do

00:04:04,880 --> 00:04:09,950
is give a shout out to Big Top because

00:04:07,400 --> 00:04:10,340
Big Top is cool so I have to do that as

00:04:09,950 --> 00:04:13,760
well

00:04:10,340 --> 00:04:16,459
the this project in general is part of

00:04:13,760 --> 00:04:19,609
Apogee big top you can have a look at

00:04:16,459 --> 00:04:23,240
the code there and references to the big

00:04:19,609 --> 00:04:30,320
bad store model basically big big top

00:04:23,240 --> 00:04:32,150
not only gives you this ability to to

00:04:30,320 --> 00:04:35,330
install your big data systems

00:04:32,150 --> 00:04:37,910
with with the puppet scripts and the

00:04:35,330 --> 00:04:39,950
juju charms and all these cool stuff but

00:04:37,910 --> 00:04:42,080
also when it's already installed you

00:04:39,950 --> 00:04:43,970
might be able to benchmark it I would

00:04:42,080 --> 00:04:46,130
definitely say that this is not the most

00:04:43,970 --> 00:04:48,290
major part of Victor but it's a new

00:04:46,130 --> 00:04:51,410
initiative very contributions are very

00:04:48,290 --> 00:04:55,220
welcome well in terms of the model that

00:04:51,410 --> 00:04:57,140
we are using I already mentioned that

00:04:55,220 --> 00:04:59,210
this is this big pet store model there's

00:04:57,140 --> 00:05:01,610
a scientific paper out on and basically

00:04:59,210 --> 00:05:06,410
the idea that you need to have here is

00:05:01,610 --> 00:05:10,160
that it can generate data in a big data

00:05:06,410 --> 00:05:12,800
setting that even if it's small scare or

00:05:10,160 --> 00:05:15,470
big scale it's sort of still relevant on

00:05:12,800 --> 00:05:18,100
on testing your big data applications so

00:05:15,470 --> 00:05:21,080
for that reason it might be nice for you

00:05:18,100 --> 00:05:23,600
it's it spaced on the distributions why

00:05:21,080 --> 00:05:24,950
it might be relevant okay so the two

00:05:23,600 --> 00:05:28,670
systems that we are going to deal with

00:05:24,950 --> 00:05:32,060
today and mostly their API is spark and

00:05:28,670 --> 00:05:34,210
flink - data processing systems and I

00:05:32,060 --> 00:05:36,590
would like to just highlight one

00:05:34,210 --> 00:05:38,630
fundamental difference between the two

00:05:36,590 --> 00:05:41,260
in terms of the architecture and the

00:05:38,630 --> 00:05:44,060
other parts are just matching each other

00:05:41,260 --> 00:05:46,550
when you look at the spark runtime the

00:05:44,060 --> 00:05:49,070
basic runtime of Apache spark is the so

00:05:46,550 --> 00:05:52,010
called RDD Raziel and distributed data

00:05:49,070 --> 00:05:53,990
set api that's the batch API so it

00:05:52,010 --> 00:05:56,240
suggests that you already have your data

00:05:53,990 --> 00:06:00,110
sitting on your local file system or

00:05:56,240 --> 00:06:02,510
HDFS or hs3 wherever and it's there and

00:06:00,110 --> 00:06:05,030
it's a finite data set so that's what

00:06:02,510 --> 00:06:07,940
spark runs on natively and then they

00:06:05,030 --> 00:06:10,700
build streaming on top of it the main

00:06:07,940 --> 00:06:13,190
difference in flink is that in flink

00:06:10,700 --> 00:06:15,140
data in the data stream api the

00:06:13,190 --> 00:06:17,630
streaming one and the data set api the

00:06:15,140 --> 00:06:19,970
batch one those are on the same level so

00:06:17,630 --> 00:06:22,400
if you want to have this finite data set

00:06:19,970 --> 00:06:24,250
or if you want to have something that is

00:06:22,400 --> 00:06:27,800
near real-time maybe giving answers

00:06:24,250 --> 00:06:29,780
under a second that's that sort of both

00:06:27,800 --> 00:06:31,790
so native implementation the streaming

00:06:29,780 --> 00:06:35,830
doesn't get translated on top of a batch

00:06:31,790 --> 00:06:38,720
job and that has different implications

00:06:35,830 --> 00:06:41,090
because of dibs it's much easier in

00:06:38,720 --> 00:06:42,740
SPARC to integrate within a streaming

00:06:41,090 --> 00:06:45,190
and a batch job because a streaming job

00:06:42,740 --> 00:06:47,380
is just a sequence of batch jobs

00:06:45,190 --> 00:06:49,480
but it might be live eating in a couple

00:06:47,380 --> 00:06:53,080
of cases so it's it it includes the

00:06:49,480 --> 00:06:55,300
trade of and we can define the mapping

00:06:53,080 --> 00:06:57,820
that we are going to go through today so

00:06:55,300 --> 00:06:59,800
we have the batch API smashed up we'll

00:06:57,820 --> 00:07:02,800
have a look at the streaming ones the

00:06:59,800 --> 00:07:08,140
machine learning ones and also the the

00:07:02,800 --> 00:07:09,880
sequel slash table ones I this talk is a

00:07:08,140 --> 00:07:12,220
little too short to also include the

00:07:09,880 --> 00:07:16,690
graph ones but maybe next time for Vysya

00:07:12,220 --> 00:07:22,120
I have to do that as well so then let's

00:07:16,690 --> 00:07:25,660
let's go to the coding part run out one

00:07:22,120 --> 00:07:29,560
of the authors of set paper he also

00:07:25,660 --> 00:07:31,900
coded the whole generation process in

00:07:29,560 --> 00:07:35,620
java classes of course both of these

00:07:31,900 --> 00:07:37,360
systems do have Java and Scala api's I'm

00:07:35,620 --> 00:07:39,430
going to stick with the Scala one

00:07:37,360 --> 00:07:42,880
because it's just easier to read in a

00:07:39,430 --> 00:07:46,480
slide who considers themselves Scala

00:07:42,880 --> 00:07:51,790
developer okay a couple

00:07:46,480 --> 00:07:54,850
Java developer good enough maybe Python

00:07:51,790 --> 00:07:59,500
that also else good okay I'm safe thank

00:07:54,850 --> 00:08:01,510
you very much so just a little bit of

00:07:59,500 --> 00:08:03,850
basically we have a way to generate

00:08:01,510 --> 00:08:06,520
these classes on a single thread that

00:08:03,850 --> 00:08:08,260
row not has already provided for us but

00:08:06,520 --> 00:08:08,770
I would like to use these distributed

00:08:08,260 --> 00:08:10,600
systems

00:08:08,770 --> 00:08:12,580
I would like to generate a whole bunch

00:08:10,600 --> 00:08:15,190
of this data in a distributed fashion

00:08:12,580 --> 00:08:18,030
that's basically my point and of course

00:08:15,190 --> 00:08:21,640
it's pretty easy to to do that the Veda

00:08:18,030 --> 00:08:23,500
we are doing it here is for this data

00:08:21,640 --> 00:08:26,620
generation to work we need to generate

00:08:23,500 --> 00:08:28,750
the so-called ground truth which is our

00:08:26,620 --> 00:08:30,730
customer IDs and the name of the

00:08:28,750 --> 00:08:33,070
customers basic customer information

00:08:30,730 --> 00:08:36,160
also basic information on the stores and

00:08:33,070 --> 00:08:38,110
on our product category so in this case

00:08:36,160 --> 00:08:40,839
product categories is like a doggy leash

00:08:38,110 --> 00:08:43,930
or cat food or whatever you would buy in

00:08:40,839 --> 00:08:46,570
a pet store and then in SPARC

00:08:43,930 --> 00:08:49,390
you have basically two main ways of

00:08:46,570 --> 00:08:52,210
passing data to your distributed

00:08:49,390 --> 00:08:53,920
operators remember these will get this

00:08:52,210 --> 00:08:56,440
used in Anna clusters maybe you will

00:08:53,920 --> 00:08:58,420
have ten instances running and

00:08:56,440 --> 00:09:02,850
generating your data

00:08:58,420 --> 00:09:05,290
one is of course to paralyze some

00:09:02,850 --> 00:09:07,270
collection that you already have or read

00:09:05,290 --> 00:09:09,370
from a file so that's the standard data

00:09:07,270 --> 00:09:11,560
set that is going to flow through your

00:09:09,370 --> 00:09:14,470
pipeline but that's also an alternative

00:09:11,560 --> 00:09:16,540
if you have just a smaller data set that

00:09:14,470 --> 00:09:19,720
you would like to propagate to all of

00:09:16,540 --> 00:09:22,210
your nodes and for example the stories

00:09:19,720 --> 00:09:25,330
is going to be such a data set in our

00:09:22,210 --> 00:09:29,590
example you can use sparks broadcast

00:09:25,330 --> 00:09:32,890
variable to do such such a thing then of

00:09:29,590 --> 00:09:36,310
the spark has this so-called functional

00:09:32,890 --> 00:09:39,790
API we already seen a lot of functional

00:09:36,310 --> 00:09:42,820
api's today in the display dev room and

00:09:39,790 --> 00:09:45,550
one of the main features there is you

00:09:42,820 --> 00:09:47,320
can you can use these functions for

00:09:45,550 --> 00:09:51,670
example a map function or reduce

00:09:47,320 --> 00:09:54,340
function or a flat map function and pass

00:09:51,670 --> 00:09:56,620
your user define functional if vivid in

00:09:54,340 --> 00:09:59,620
that you see one example here I just

00:09:56,620 --> 00:10:03,370
adjusted the time when we started

00:09:59,620 --> 00:10:05,860
generating the process that's that's

00:10:03,370 --> 00:10:08,580
basically the whole structure is how you

00:10:05,860 --> 00:10:13,690
would code the data generation in SPARC

00:10:08,580 --> 00:10:16,300
like let's switch to fling I also

00:10:13,690 --> 00:10:19,120
generate the grounds response changes in

00:10:16,300 --> 00:10:21,520
SPARC we called the basis of building

00:10:19,120 --> 00:10:23,650
this jog graph SPARC context now we call

00:10:21,520 --> 00:10:26,520
it execution environment of course

00:10:23,650 --> 00:10:29,620
relying on Ronald is the same and now

00:10:26,520 --> 00:10:31,960
it's slightly different the way how we

00:10:29,620 --> 00:10:34,690
they use extra information on our

00:10:31,960 --> 00:10:36,970
functions in Flint we use the Rick's

00:10:34,690 --> 00:10:39,490
trig function interface in SPARC we use

00:10:36,970 --> 00:10:41,920
another solution for that it's slightly

00:10:39,490 --> 00:10:44,620
different how we use the broadcast

00:10:41,920 --> 00:10:47,410
variables but as you see the basics

00:10:44,620 --> 00:10:52,180
having a mapping is is on the same level

00:10:47,410 --> 00:10:53,800
it's mapped out the same and that this

00:10:52,180 --> 00:10:58,000
is just a summary of what I've already

00:10:53,800 --> 00:11:00,100
said so we have already written our JSON

00:10:58,000 --> 00:11:03,190
output file so it this would be written

00:11:00,100 --> 00:11:06,010
in Jason just the sake of having able to

00:11:03,190 --> 00:11:08,290
look at it and let's do a little bit of

00:11:06,010 --> 00:11:11,050
ETL on top of the data that we already

00:11:08,290 --> 00:11:12,100
have so of course this is usually

00:11:11,050 --> 00:11:14,829
variable

00:11:12,100 --> 00:11:18,940
with such a process because some someone

00:11:14,829 --> 00:11:22,920
has already provided for you so how can

00:11:18,940 --> 00:11:26,980
we be will let's have two goals for this

00:11:22,920 --> 00:11:29,380
first first of all we we would like to

00:11:26,980 --> 00:11:31,810
feed this data into a recommender system

00:11:29,380 --> 00:11:33,880
eventually so we will we would need a

00:11:31,810 --> 00:11:35,920
customer at product pairs where a

00:11:33,880 --> 00:11:37,600
customer has purchased the given product

00:11:35,920 --> 00:11:40,269
that but the format that we have in the

00:11:37,600 --> 00:11:43,540
JSON SS that this given customer

00:11:40,269 --> 00:11:46,560
generated a transaction that has these

00:11:43,540 --> 00:11:50,279
ten products in it so we would like to

00:11:46,560 --> 00:11:52,750
get into this puffle format instant

00:11:50,279 --> 00:11:56,139
first thing first we don't really have

00:11:52,750 --> 00:11:57,519
an ID with our products first yet we

00:11:56,139 --> 00:12:00,100
only have that for the product

00:11:57,519 --> 00:12:02,290
categories so we could select the

00:12:00,100 --> 00:12:04,449
distinct of those and zip them with the

00:12:02,290 --> 00:12:07,660
unique ID these are all spark functions

00:12:04,449 --> 00:12:13,060
already included you can access it from

00:12:07,660 --> 00:12:15,910
the standard course part and then you

00:12:13,060 --> 00:12:18,370
joined this information together with

00:12:15,910 --> 00:12:21,670
the customers who already have an ID and

00:12:18,370 --> 00:12:23,980
then you have two IDs here this is just

00:12:21,670 --> 00:12:25,750
some basic mapping and the join and a

00:12:23,980 --> 00:12:28,420
dis thing and what's really interesting

00:12:25,750 --> 00:12:31,480
these are this is the batch API if I

00:12:28,420 --> 00:12:33,910
switch to flank practically not nothing

00:12:31,480 --> 00:12:35,889
changes the only part that this is

00:12:33,910 --> 00:12:39,040
exactly the same the only part that

00:12:35,889 --> 00:12:42,189
changes here is the way spark and fling

00:12:39,040 --> 00:12:45,100
like to speak about their day join and

00:12:42,189 --> 00:12:49,689
basically the signature is of the join

00:12:45,100 --> 00:12:52,720
but basically it's the same and the way

00:12:49,689 --> 00:12:55,509
they handle keys so if you could almost

00:12:52,720 --> 00:12:57,759
recompile it with which blink instead of

00:12:55,509 --> 00:13:00,490
spark and the other way you learn that

00:12:57,759 --> 00:13:06,579
so this keys is is the notion of the

00:13:00,490 --> 00:13:10,079
pyro RDD okay some fun next step we have

00:13:06,579 --> 00:13:12,939
already heard this too today from many

00:13:10,079 --> 00:13:15,250
speakers that it's not necessarily

00:13:12,939 --> 00:13:19,029
enough to provide the Java and Scala API

00:13:15,250 --> 00:13:21,130
because other people who have very good

00:13:19,029 --> 00:13:24,010
previous systems they may or may be used

00:13:21,130 --> 00:13:25,730
to sequel or R or Python so why not

00:13:24,010 --> 00:13:29,209
provide an interface for

00:13:25,730 --> 00:13:33,620
as well and both systems give you signal

00:13:29,209 --> 00:13:36,260
interfaces of course they usually they

00:13:33,620 --> 00:13:39,350
have the support standard that's a

00:13:36,260 --> 00:13:43,130
little older so I think SPARC currently

00:13:39,350 --> 00:13:46,130
supports with spark to the sequel mm

00:13:43,130 --> 00:13:49,639
sander the flink is on the sequel 92

00:13:46,130 --> 00:13:51,860
standard currently so antsy well in both

00:13:49,639 --> 00:13:53,720
cases but they are catching up and

00:13:51,860 --> 00:13:56,380
making sure you that you can also have

00:13:53,720 --> 00:13:59,360
complex queries queries on top of these

00:13:56,380 --> 00:14:00,980
so for example here in this query I

00:13:59,360 --> 00:14:03,709
would like to have a look at the data

00:14:00,980 --> 00:14:06,440
and select the stores that have the most

00:14:03,709 --> 00:14:10,639
transactions and you can achieve that

00:14:06,440 --> 00:14:13,940
with 2 sequel statement but one of the

00:14:10,639 --> 00:14:16,370
downsides of heaviness using sequel is

00:14:13,940 --> 00:14:19,880
you sort of lose the type of information

00:14:16,370 --> 00:14:23,329
that you have in your data because when

00:14:19,880 --> 00:14:26,389
you look you just type this declarative

00:14:23,329 --> 00:14:29,389
query as a string and you lose the

00:14:26,389 --> 00:14:32,329
information that actually this ID was a

00:14:29,389 --> 00:14:34,970
string or or this was an int and this

00:14:32,329 --> 00:14:37,880
name was a string so if you also want to

00:14:34,970 --> 00:14:41,779
have that then you can use the table or

00:14:37,880 --> 00:14:44,930
or data frames api's we also heard today

00:14:41,779 --> 00:14:49,029
that it's it's very difficult to write a

00:14:44,930 --> 00:14:52,459
UDF in hive for example and there

00:14:49,029 --> 00:14:56,389
sparkin flink I think give you very nice

00:14:52,459 --> 00:14:58,339
solutions in terms of writing UDF for

00:14:56,389 --> 00:15:00,139
example I would like to register this

00:14:58,339 --> 00:15:03,560
month's function which is a scalar

00:15:00,139 --> 00:15:06,199
function so that I can use it in my

00:15:03,560 --> 00:15:08,269
sequel statement this is literally this

00:15:06,199 --> 00:15:10,880
much code you write the scalar function

00:15:08,269 --> 00:15:13,130
you call register and then here you can

00:15:10,880 --> 00:15:15,800
use it so I think it's it's super

00:15:13,130 --> 00:15:18,980
convenient it it gets propagated to your

00:15:15,800 --> 00:15:23,060
working nodes the NOI already refers to

00:15:18,980 --> 00:15:25,339
the table API soon I the sequel I showed

00:15:23,060 --> 00:15:27,800
you in spark now I'm going to show the

00:15:25,339 --> 00:15:29,630
table or data frames API in flink both

00:15:27,800 --> 00:15:32,360
have actually both we just don't have

00:15:29,630 --> 00:15:34,850
enough time to visit everything so this

00:15:32,360 --> 00:15:37,850
is very pretty close to sequel it's

00:15:34,850 --> 00:15:39,530
still Scala it feels like sequel so the

00:15:37,850 --> 00:15:42,050
statements that you write is

00:15:39,530 --> 00:15:45,050
tables of group buy and select and join

00:15:42,050 --> 00:15:47,090
and very but you are still in Scala and

00:15:45,050 --> 00:15:49,280
you still have the type information and

00:15:47,090 --> 00:15:52,160
it's and if you want to go back to

00:15:49,280 --> 00:15:55,730
standard flink or standard it's

00:15:52,160 --> 00:16:00,610
very easier and here I complete the same

00:15:55,730 --> 00:16:00,610
let's select the most important stories

00:16:01,090 --> 00:16:08,510
ok and other topic is finally we

00:16:05,990 --> 00:16:10,850
realized than this customer and and

00:16:08,510 --> 00:16:12,560
product rares we would like to feed it

00:16:10,850 --> 00:16:15,140
to a machine learning system and if you

00:16:12,560 --> 00:16:17,810
are familiar with like it learned and

00:16:15,140 --> 00:16:22,700
the API should look pretty similar for

00:16:17,810 --> 00:16:25,760
you basically they have this estimator

00:16:22,700 --> 00:16:28,640
predictor API that's that's very

00:16:25,760 --> 00:16:30,980
familiar but from scikit-learn you so

00:16:28,640 --> 00:16:33,920
you have a model you call fit on it and

00:16:30,980 --> 00:16:37,010
then you can predict with the testing

00:16:33,920 --> 00:16:39,550
data set so that's that's what we would

00:16:37,010 --> 00:16:42,110
we would be able to call here the

00:16:39,550 --> 00:16:45,110
recommender implementation that we are

00:16:42,110 --> 00:16:47,270
using here is called ALS but there are

00:16:45,110 --> 00:16:50,180
also other solutions to to solve this

00:16:47,270 --> 00:16:52,940
problem instead of predicting it right

00:16:50,180 --> 00:16:54,680
here I'm just saving the model and we

00:16:52,940 --> 00:16:57,260
will use it later in in a streaming

00:16:54,680 --> 00:16:59,150
topology so right now with this much

00:16:57,260 --> 00:17:02,000
code and this is actually how much code

00:16:59,150 --> 00:17:04,339
you have to write in in ml lib it will

00:17:02,000 --> 00:17:06,650
be very similar in string canal you can

00:17:04,339 --> 00:17:10,130
train a machine learning model and save

00:17:06,650 --> 00:17:13,699
it and then we can reuse it later in

00:17:10,130 --> 00:17:15,829
flink it would be this much code this is

00:17:13,699 --> 00:17:18,280
really how much you would have to write

00:17:15,829 --> 00:17:21,530
there okay

00:17:18,280 --> 00:17:24,380
in terms of contouring the ML API is the

00:17:21,530 --> 00:17:27,110
spark on is definitely more mature and

00:17:24,380 --> 00:17:30,170
what API level the well they are almost

00:17:27,110 --> 00:17:33,380
as close as the batch API sorry so I

00:17:30,170 --> 00:17:35,330
think in given given that the fling

00:17:33,380 --> 00:17:37,070
phone catches up with algorithms it will

00:17:35,330 --> 00:17:40,460
be really easy to to switch between

00:17:37,070 --> 00:17:42,890
those and then let's play a little bit

00:17:40,460 --> 00:17:44,600
with streaming so we have worried they

00:17:42,890 --> 00:17:47,929
have a machine learning model and the

00:17:44,600 --> 00:17:50,570
tricky part there is usually when you

00:17:47,929 --> 00:17:53,030
have a recommender system model then the

00:17:50,570 --> 00:17:56,030
problem that you are trying to solve is

00:17:53,030 --> 00:17:58,160
I have this user he have watched five

00:17:56,030 --> 00:18:00,290
videos on my website and I would like to

00:17:58,160 --> 00:18:01,970
give recommendations so for three more

00:18:00,290 --> 00:18:04,550
well if you don't give you those

00:18:01,970 --> 00:18:06,650
recommendations in approximate with 200

00:18:04,550 --> 00:18:08,750
milliseconds the user is gone so they

00:18:06,650 --> 00:18:13,130
are worthless that's why we need

00:18:08,750 --> 00:18:16,250
streaming or near real-time processing

00:18:13,130 --> 00:18:18,710
and as I mentioned in spike it's really

00:18:16,250 --> 00:18:21,410
easy to accomplish such a thing because

00:18:18,710 --> 00:18:23,780
we just load the model that we have

00:18:21,410 --> 00:18:26,510
already prepared in batch and then we

00:18:23,780 --> 00:18:29,750
will we have a query which is coming

00:18:26,510 --> 00:18:33,080
from a socket I am just typing in user

00:18:29,750 --> 00:18:35,680
IDs and then I would like to to do the

00:18:33,080 --> 00:18:38,120
prediction I go back to my batch

00:18:35,680 --> 00:18:39,980
pipeline and use the prediction there

00:18:38,120 --> 00:18:42,050
this is something that is currently not

00:18:39,980 --> 00:18:45,620
available in flink and it's super

00:18:42,050 --> 00:18:47,420
convenient to code in spike of course

00:18:45,620 --> 00:18:51,440
you have other trade-offs in inspired

00:18:47,420 --> 00:18:54,380
and just to give you a sense of how to

00:18:51,440 --> 00:18:56,090
switch to a Java API this is actually

00:18:54,380 --> 00:18:57,740
coded in Java of course you don't see

00:18:56,090 --> 00:19:01,280
much difference the only difference is

00:18:57,740 --> 00:19:03,890
the semicolon here because almost

00:19:01,280 --> 00:19:06,770
already looks like Scala but in flink I

00:19:03,890 --> 00:19:09,650
decided and the code is available on

00:19:06,770 --> 00:19:12,940
github I decided to code the whole

00:19:09,650 --> 00:19:15,950
prediction instead of relying on the

00:19:12,940 --> 00:19:17,780
implementation in flink ml I coded it in

00:19:15,950 --> 00:19:20,300
in streaming which is because it's very

00:19:17,780 --> 00:19:22,070
straightforward this just multiplying a

00:19:20,300 --> 00:19:26,900
matrix with the vector and then

00:19:22,070 --> 00:19:29,330
selecting the top K yeah in terms of

00:19:26,900 --> 00:19:31,160
differences I think this is the part

00:19:29,330 --> 00:19:33,590
where spark and flink are the most

00:19:31,160 --> 00:19:36,410
difference because of the choice of

00:19:33,590 --> 00:19:37,880
architecture fling bus definitely

00:19:36,410 --> 00:19:40,820
barrier when it comes to stateful

00:19:37,880 --> 00:19:44,960
processing and timeliness what spark is

00:19:40,820 --> 00:19:47,840
improving in that department with with

00:19:44,960 --> 00:19:51,170
the new structure streaming API so as a

00:19:47,840 --> 00:19:53,960
summary you should definitely go beyond

00:19:51,170 --> 00:19:56,060
word count with the big bad story the

00:19:53,960 --> 00:19:58,400
batch concepts of spark and slink are

00:19:56,060 --> 00:20:02,180
very close but they have the differences

00:19:58,400 --> 00:20:04,610
in streaming you have seen a lot of use

00:20:02,180 --> 00:20:06,220
cases with under five hundred five

00:20:04,610 --> 00:20:07,960
hundred lines of code and it's

00:20:06,220 --> 00:20:10,419
available on github and you are very

00:20:07,960 --> 00:20:14,409
welcome to check it out and of course an

00:20:10,419 --> 00:20:16,809
Apache pet project is always fun big

00:20:14,409 --> 00:20:19,510
thanks to the guys who have helped me to

00:20:16,809 --> 00:21:01,570
accomplish the project itself and thank

00:20:19,510 --> 00:21:05,350
you very much yes so basic and yes so

00:21:01,570 --> 00:21:07,450
the question was how does spark and

00:21:05,350 --> 00:21:12,070
fling come together in Apache beam and

00:21:07,450 --> 00:21:14,470
and what's the future of Apache being in

00:21:12,070 --> 00:21:16,480
my understanding I think Apogee beam is

00:21:14,470 --> 00:21:20,440
a very interesting project in the sense

00:21:16,480 --> 00:21:24,700
that the Google guys who published the

00:21:20,440 --> 00:21:28,809
data flow paper which was the driving

00:21:24,700 --> 00:21:30,970
factor behind beam really got the

00:21:28,809 --> 00:21:35,610
timeliness of streaming application

00:21:30,970 --> 00:21:35,610
rights so their model there is it really

00:21:37,169 --> 00:21:41,710
goes into details in how to handle

00:21:39,850 --> 00:21:44,230
timeliness and late events and and

00:21:41,710 --> 00:21:47,919
that's one of the main powers of data

00:21:44,230 --> 00:21:49,900
flow and I think that if you look at the

00:21:47,919 --> 00:21:52,720
the spark and fling Krannert

00:21:49,900 --> 00:21:55,780
implementations currently none of this

00:21:52,720 --> 00:21:57,880
the system's can support all of the

00:21:55,780 --> 00:22:02,650
features of data flow but flink is much

00:21:57,880 --> 00:22:05,950
closer currently and the basic reason

00:22:02,650 --> 00:22:08,350
why spark is currently not supporting

00:22:05,950 --> 00:22:10,809
all of these streaming features because

00:22:08,350 --> 00:22:13,000
in spark because of this architectural

00:22:10,809 --> 00:22:14,649
choice for trance was very easy to

00:22:13,000 --> 00:22:17,919
accomplish because it was done batch

00:22:14,649 --> 00:22:21,789
fortress it gives it's done already but

00:22:17,919 --> 00:22:23,559
when you have this mini-batches so then

00:22:21,789 --> 00:22:25,390
it's really you don't really have a way

00:22:23,559 --> 00:22:29,020
to communicate between two mini-batch

00:22:25,390 --> 00:22:31,510
that's that's the main issue so that was

00:22:29,020 --> 00:22:33,640
one of the driving forces by a spark is

00:22:31,510 --> 00:22:37,320
coming out with a new streaming API this

00:22:33,640 --> 00:22:40,720
structure streaming to make up for a

00:22:37,320 --> 00:22:43,600
couple of these semantic differences

00:22:40,720 --> 00:22:45,940
between the watt spark streaming and and

00:22:43,600 --> 00:22:49,960
fling core data flow so it's definitely

00:22:45,940 --> 00:23:00,490
getting there but spark has to improve

00:22:49,960 --> 00:23:03,549
in terms of semantics yes please I think

00:23:00,490 --> 00:23:05,950
natalie is is a little bit in a

00:23:03,549 --> 00:23:13,299
different domain but I would like to

00:23:05,950 --> 00:23:14,540
learn more about med live as well thank

00:23:13,299 --> 00:23:22,630
you very much

00:23:14,540 --> 00:23:22,630

YouTube URL: https://www.youtube.com/watch?v=PmFL3VBUR84


