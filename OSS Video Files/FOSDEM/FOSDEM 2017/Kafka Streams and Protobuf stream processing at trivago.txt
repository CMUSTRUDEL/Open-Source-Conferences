Title: Kafka Streams and Protobuf stream processing at trivago
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Clemens Valiente

At: FOSDEM 2017

A lot of components in the Kafka and hadoop ecosystem assume you are workingwith avro messages. In this talk I will highlight some of the advantages andchallenges of stream processing with protobuf encoded messages, and why itmight be worth using them.

A powerful feature of protobuf messages is its handling of unknown fields, andthe ability to partially parse messages with different schema versions or noschema at all. This helps in stream processing when you don't actually need toknow all message fields in order to process and forward them.


Room: H.2213
Scheduled start: 2017-02-04 16:30:00
Captions: 
	00:00:04,720 --> 00:00:09,670
so a very quick introduction into a

00:00:06,880 --> 00:00:12,190
Kafka then some concepts about Kafka

00:00:09,670 --> 00:00:15,129
streams introduction on Google protocol

00:00:12,190 --> 00:00:18,390
buffers and what kind of stream

00:00:15,129 --> 00:00:18,390
processing you can do with that

00:00:19,740 --> 00:00:24,880
right-click over your Apache Kafka is a

00:00:22,900 --> 00:00:26,890
message queue system nowadays it's

00:00:24,880 --> 00:00:30,250
evolved into a distributed streaming

00:00:26,890 --> 00:00:33,070
platform so you can put messages since I

00:00:30,250 --> 00:00:37,570
refuse their queues are called topic in

00:00:33,070 --> 00:00:39,670
the sorry yeah all right so cough kites

00:00:37,570 --> 00:00:42,699
are distributed streaming platform you

00:00:39,670 --> 00:00:44,350
have message queues several you which

00:00:42,699 --> 00:00:46,960
are called topics each topic is

00:00:44,350 --> 00:00:49,629
partitioned and replicated so it allows

00:00:46,960 --> 00:00:51,069
you to scale out by putting more servers

00:00:49,629 --> 00:00:53,920
in your cluster and putting more

00:00:51,069 --> 00:00:56,499
partitions in your topic and each topic

00:00:53,920 --> 00:00:59,100
is each partition is usually replicated

00:00:56,499 --> 00:01:01,539
so if a machine goes down you have the

00:00:59,100 --> 00:01:06,250
data also somewhere else on a different

00:01:01,539 --> 00:01:08,080
machine and one important important part

00:01:06,250 --> 00:01:09,400
especially when it comes to Kafka

00:01:08,080 --> 00:01:13,840
streams is all of the consumer group

00:01:09,400 --> 00:01:16,330
logic so you can have several consumers

00:01:13,840 --> 00:01:19,630
forming a consumer group and reading

00:01:16,330 --> 00:01:22,390
from a particular message queue and this

00:01:19,630 --> 00:01:25,270
allows you also to scale the reading

00:01:22,390 --> 00:01:28,230
process and it works also quite similar

00:01:25,270 --> 00:01:31,270
so in this case we have two different

00:01:28,230 --> 00:01:33,910
consumer groups which are doing

00:01:31,270 --> 00:01:36,870
different things each but within each

00:01:33,910 --> 00:01:39,040
group you have several threats and

00:01:36,870 --> 00:01:40,960
basically they coordinate with each

00:01:39,040 --> 00:01:43,690
other to distribute the different

00:01:40,960 --> 00:01:45,190
partitions among each other so if one of

00:01:43,690 --> 00:01:48,310
your consumers and that group goes down

00:01:45,190 --> 00:01:51,340
the remaining the take over the work

00:01:48,310 --> 00:01:53,500
that that that consumer was doing and if

00:01:51,340 --> 00:01:55,480
you add more consumers than again the

00:01:53,500 --> 00:01:57,580
consumers will coordinate with each

00:01:55,480 --> 00:02:00,420
other to distribute the workload again

00:01:57,580 --> 00:02:03,940
which allows you also to easily scale

00:02:00,420 --> 00:02:09,550
the reading for the consuming from your

00:02:03,940 --> 00:02:11,739
cough adjuster all right so what is

00:02:09,550 --> 00:02:13,690
Kafka streams so they have been a lot of

00:02:11,739 --> 00:02:16,240
stream processing frameworks out there

00:02:13,690 --> 00:02:17,470
and sometimes you might wonder why is

00:02:16,240 --> 00:02:20,950
there need for another

00:02:17,470 --> 00:02:23,140
why is Marius Kafka streams different so

00:02:20,950 --> 00:02:24,820
one very important feature is it's a

00:02:23,140 --> 00:02:27,670
really small library it's not a huge

00:02:24,820 --> 00:02:29,470
framework that like you get this Park it

00:02:27,670 --> 00:02:33,070
doesn't have any further external

00:02:29,470 --> 00:02:34,540
dependencies so then you write a stream

00:02:33,070 --> 00:02:37,150
processing up in Kafka streams you'll

00:02:34,540 --> 00:02:41,290
just put a dependency on Kafka streams

00:02:37,150 --> 00:02:42,880
in your cradle and that's it and in the

00:02:41,290 --> 00:02:45,130
end what we get is a very simple small

00:02:42,880 --> 00:02:48,340
java application usually less than 10

00:02:45,130 --> 00:02:50,440
megabytes and you can run that one

00:02:48,340 --> 00:02:54,190
everywhere so you can put it on yarn on

00:02:50,440 --> 00:02:56,080
me source docker when it is all that you

00:02:54,190 --> 00:02:59,140
can also just copy the char on a server

00:02:56,080 --> 00:03:00,610
somewhere and started there so it's

00:02:59,140 --> 00:03:03,370
pretty agnostic on how you want to

00:03:00,610 --> 00:03:05,290
deploy it by most other stream

00:03:03,370 --> 00:03:09,040
processing frameworks they really have

00:03:05,290 --> 00:03:13,840
very explicit SOI requirements on where

00:03:09,040 --> 00:03:16,630
you deploy them they the Kafka streams

00:03:13,840 --> 00:03:19,150
basically uses this Kafka consumer probe

00:03:16,630 --> 00:03:21,370
logic that I introduced for scaling and

00:03:19,150 --> 00:03:23,170
fault tolerance so other stream

00:03:21,370 --> 00:03:25,840
processing frameworks had to really

00:03:23,170 --> 00:03:29,380
invest a lot of work to get elastic

00:03:25,840 --> 00:03:31,780
scaling and all that working and Kafka

00:03:29,380 --> 00:03:36,400
streams basically got it too for free by

00:03:31,780 --> 00:03:38,110
using the consumer logic and that

00:03:36,400 --> 00:03:40,600
consumer logic with Kafka has been

00:03:38,110 --> 00:03:42,670
already tested in battle-hardened for

00:03:40,600 --> 00:03:45,930
many years by now so it's also something

00:03:42,670 --> 00:03:49,299
that by now has proven to be working and

00:03:45,930 --> 00:03:50,830
it's due to its very small footprint

00:03:49,299 --> 00:03:53,170
it's ideal for a micro service

00:03:50,830 --> 00:03:56,200
architecture so you can have your

00:03:53,170 --> 00:03:58,720
different services or run Kafka streams

00:03:56,200 --> 00:04:01,450
applications that basically pull the

00:03:58,720 --> 00:04:03,519
transformations that read from the Kafka

00:04:01,450 --> 00:04:06,370
cluster and then expose that information

00:04:03,519 --> 00:04:10,720
back as a Kafka cluster as a topic in

00:04:06,370 --> 00:04:14,080
the Kafka cluster and Kafka streams has

00:04:10,720 --> 00:04:16,780
two different API so both in Java at the

00:04:14,080 --> 00:04:19,060
moment high level DSL which is very

00:04:16,780 --> 00:04:22,750
similar what you would get in spark with

00:04:19,060 --> 00:04:25,720
those filtered so in map functions that

00:04:22,750 --> 00:04:27,820
you can use and a low level processor

00:04:25,720 --> 00:04:29,950
API which is more explicit and at the

00:04:27,820 --> 00:04:31,180
moment also a bit more powerful it's

00:04:29,950 --> 00:04:38,229
similar to

00:04:31,180 --> 00:04:40,840
you get in storm in Samsa it also has a

00:04:38,229 --> 00:04:45,520
the concept of tables and streams that

00:04:40,840 --> 00:04:47,889
are separate so streams are basically

00:04:45,520 --> 00:04:49,509
infinite event blocks or you have all

00:04:47,889 --> 00:04:51,030
these messages coming in and the

00:04:49,509 --> 00:04:54,639
assumption is that it goes on forever

00:04:51,030 --> 00:04:58,229
and tables are finite with updates

00:04:54,639 --> 00:05:01,479
deletes inserts on their primary key and

00:04:58,229 --> 00:05:04,479
both can be represented as message

00:05:01,479 --> 00:05:07,539
queues as kafka topics by just taking

00:05:04,479 --> 00:05:10,840
the change look of a table so we have

00:05:07,539 --> 00:05:13,660
that year if a table and each action

00:05:10,840 --> 00:05:18,220
that modifies its content is a change

00:05:13,660 --> 00:05:21,940
log entry in a stream and this allows

00:05:18,220 --> 00:05:24,580
you to basically have stuff for example

00:05:21,940 --> 00:05:27,940
static look-up tables that you join with

00:05:24,580 --> 00:05:29,770
incoming data streams so one classic

00:05:27,940 --> 00:05:31,180
example is that you have customer events

00:05:29,770 --> 00:05:35,460
coming in and you want to look up

00:05:31,180 --> 00:05:35,460
customer information and enrich that

00:05:37,409 --> 00:05:44,740
kafka streams of course also allows

00:05:39,880 --> 00:05:46,780
stateful processing and it stores its

00:05:44,740 --> 00:05:48,639
state locally in a rocks DP by default

00:05:46,780 --> 00:05:55,990
you can also plug in any other DP if you

00:05:48,639 --> 00:05:57,909
look like and Oh doesn't skip yeah all

00:05:55,990 --> 00:06:01,389
right so you have a usually I rocks DP

00:05:57,909 --> 00:06:03,520
as a local state and this state with

00:06:01,389 --> 00:06:06,039
rocks DP is a key value store it's again

00:06:03,520 --> 00:06:08,770
some kind of a table which as we saw

00:06:06,039 --> 00:06:11,199
before can be seen as a change look and

00:06:08,770 --> 00:06:17,409
change look it's a stream so the way

00:06:11,199 --> 00:06:19,659
this state store is is made available is

00:06:17,409 --> 00:06:21,460
that it's written back to Kafka as a

00:06:19,659 --> 00:06:23,560
topic as a change book topic for your

00:06:21,460 --> 00:06:27,250
state store and this is the way that

00:06:23,560 --> 00:06:30,190
Kafka streams handers rebalancing and

00:06:27,250 --> 00:06:31,900
for tolerance because any other instance

00:06:30,190 --> 00:06:36,210
of your application can just read the

00:06:31,900 --> 00:06:38,680
store of your with the state of your

00:06:36,210 --> 00:06:43,360
streaming application from that change

00:06:38,680 --> 00:06:45,280
look and recover it so instead of these

00:06:43,360 --> 00:06:47,290
checkpointing or lineage that we get

00:06:45,280 --> 00:06:49,960
another streaming application you really

00:06:47,290 --> 00:06:52,000
can recover the full state of your

00:06:49,960 --> 00:06:54,550
application in case it goes down or in

00:06:52,000 --> 00:06:57,610
case you want to launch additional

00:06:54,550 --> 00:07:01,840
instances of it so this allows you to

00:06:57,610 --> 00:07:04,270
really scale your streaming applications

00:07:01,840 --> 00:07:06,160
up very seamlessly you just launch

00:07:04,270 --> 00:07:07,510
another application on a different

00:07:06,160 --> 00:07:10,450
computer or different server for example

00:07:07,510 --> 00:07:12,130
it connects to the consumer group it

00:07:10,450 --> 00:07:16,650
reads from the changelog recovery

00:07:12,130 --> 00:07:18,730
ripples the local state and takes over

00:07:16,650 --> 00:07:22,770
nowadays in the new version you can

00:07:18,730 --> 00:07:24,760
actually even vary the local state so

00:07:22,770 --> 00:07:27,190
depending on your use case you don't

00:07:24,760 --> 00:07:29,140
even need to worry okay now I my

00:07:27,190 --> 00:07:31,060
streaming application rights to the data

00:07:29,140 --> 00:07:32,800
back to Kafka and then from Kafka I need

00:07:31,060 --> 00:07:34,990
to write it to us some other kind of

00:07:32,800 --> 00:07:37,930
database and from that database like my

00:07:34,990 --> 00:07:40,930
application can finally read it no this

00:07:37,930 --> 00:07:43,450
way you can actually query the state of

00:07:40,930 --> 00:07:45,400
your streaming application directly you

00:07:43,450 --> 00:07:48,580
don't need additional infrastructure in

00:07:45,400 --> 00:07:51,340
between which again allows you to build

00:07:48,580 --> 00:07:56,280
very lightweight applications that don't

00:07:51,340 --> 00:07:56,280
need a lot of additional infrastructure

00:07:59,360 --> 00:08:04,580
a lot of tools and components are on the

00:08:02,330 --> 00:08:06,590
confluent platform and cough can make

00:08:04,580 --> 00:08:10,069
the implicit assumption that you're

00:08:06,590 --> 00:08:12,229
primarily using overall so as to it's

00:08:10,069 --> 00:08:15,650
like Kafka Connect which is responsible

00:08:12,229 --> 00:08:18,909
for connecting Kafka to other systems

00:08:15,650 --> 00:08:22,430
like relational databases or a tube and

00:08:18,909 --> 00:08:24,710
they usually work with our own messages

00:08:22,430 --> 00:08:28,069
same with a schema registry which at the

00:08:24,710 --> 00:08:29,780
moment only works for avro messages so

00:08:28,069 --> 00:08:32,599
overall usually is the natural choice if

00:08:29,780 --> 00:08:35,000
you start using Kafka it doesn't require

00:08:32,599 --> 00:08:37,669
a lot of effort to implement since you

00:08:35,000 --> 00:08:43,279
already get everything it's also a great

00:08:37,669 --> 00:08:44,899
choice in solution but one downside is

00:08:43,279 --> 00:08:47,420
that you always need to have the correct

00:08:44,899 --> 00:08:50,120
schema the message was written in in

00:08:47,420 --> 00:08:53,750
order to be able to read that message

00:08:50,120 --> 00:08:55,339
reliably and if you don't have the

00:08:53,750 --> 00:08:58,459
schema available to you for some reason

00:08:55,339 --> 00:09:00,140
then all your data is just white garbage

00:08:58,459 --> 00:09:05,930
that you can't really do anything with

00:09:00,140 --> 00:09:11,420
it and this is where Google protocol

00:09:05,930 --> 00:09:13,160
buffers come in it's very similar to

00:09:11,420 --> 00:09:16,250
Avro in a lot of features it's a binary

00:09:13,160 --> 00:09:18,620
message it has a defined schema it has a

00:09:16,250 --> 00:09:22,760
support for lots of languages to read

00:09:18,620 --> 00:09:24,470
and write photo buff messages but a big

00:09:22,760 --> 00:09:26,050
difference is that you can also read

00:09:24,470 --> 00:09:28,370
these polar buff messages with a

00:09:26,050 --> 00:09:30,680
different version of the schema or in

00:09:28,370 --> 00:09:35,209
this no schema at all you can get quite

00:09:30,680 --> 00:09:37,250
some information out of it so for as an

00:09:35,209 --> 00:09:41,769
example if you're one of the tutorial

00:09:37,250 --> 00:09:44,660
protobuf messages which is a person and

00:09:41,769 --> 00:09:49,220
this person message has several fields a

00:09:44,660 --> 00:09:52,670
name and ID as required fields and an

00:09:49,220 --> 00:09:54,120
email as a string and then phone numbers

00:09:52,670 --> 00:09:56,249
as

00:09:54,120 --> 00:09:58,649
an embedded message so you can also have

00:09:56,249 --> 00:10:01,769
basically hierarchies of protobuf

00:09:58,649 --> 00:10:05,970
messages and each field basically

00:10:01,769 --> 00:10:09,180
consists of four parts in dignify if

00:10:05,970 --> 00:10:12,480
it's required optional or repeated the

00:10:09,180 --> 00:10:21,059
type like string or integer field name

00:10:12,480 --> 00:10:24,480
and then a field ID so this is one

00:10:21,059 --> 00:10:26,220
example person John Doe you can see here

00:10:24,480 --> 00:10:28,199
with full schema we get all the

00:10:26,220 --> 00:10:32,220
information we have the name the ID

00:10:28,199 --> 00:10:34,889
email and phone number and as a binary

00:10:32,220 --> 00:10:36,959
message it looks like this so you can

00:10:34,889 --> 00:10:39,990
still see all the strings in that there

00:10:36,959 --> 00:10:44,399
but everything else is just fights that

00:10:39,990 --> 00:10:46,439
you can't as a human really read but now

00:10:44,399 --> 00:10:48,629
we can also read this message back with

00:10:46,439 --> 00:10:50,220
a different schema version so let's say

00:10:48,629 --> 00:10:53,519
we have an old schema that doesn't know

00:10:50,220 --> 00:10:55,829
about the email field yet we still get

00:10:53,519 --> 00:10:58,589
all the fields that we know about the

00:10:55,829 --> 00:11:00,990
name the ID and the phone but also

00:10:58,589 --> 00:11:03,899
additionally we get this field that we

00:11:00,990 --> 00:11:06,480
don't know anything about but the email

00:11:03,899 --> 00:11:10,199
and they are we just get a field ID and

00:11:06,480 --> 00:11:12,540
not the field name but we can still even

00:11:10,199 --> 00:11:16,019
just by looking at it kind of guess that

00:11:12,540 --> 00:11:18,240
this is supposed to be an email so even

00:11:16,019 --> 00:11:20,279
though we we haven't even heard of this

00:11:18,240 --> 00:11:22,249
field before we can still get the

00:11:20,279 --> 00:11:25,620
information out of there and read it and

00:11:22,249 --> 00:11:28,350
if we try to read the message here

00:11:25,620 --> 00:11:30,149
without any schema at all this is what

00:11:28,350 --> 00:11:33,540
we're gonna get we still have all the

00:11:30,149 --> 00:11:35,339
fields in the same order we just lose

00:11:33,540 --> 00:11:39,839
the field names instead have only the

00:11:35,339 --> 00:11:41,999
IDS of the field so even with no knowing

00:11:39,839 --> 00:11:44,600
nothing at all about the message we can

00:11:41,999 --> 00:11:47,910
still recover quite a lot of information

00:11:44,600 --> 00:11:50,670
this is basically the output of proto CD

00:11:47,910 --> 00:11:52,410
code raw you can put any protobuf

00:11:50,670 --> 00:11:54,329
message in there and it will tell you

00:11:52,410 --> 00:11:56,040
what fields are in there and this is a

00:11:54,329 --> 00:11:59,300
kind of thing that would be very hard to

00:11:56,040 --> 00:11:59,300
do with overall

00:12:00,130 --> 00:12:06,140
what another very cool feature our

00:12:03,650 --> 00:12:08,350
unknown fields so these are the fields

00:12:06,140 --> 00:12:10,640
that we don't know anything about and

00:12:08,350 --> 00:12:12,110
not only can you read them as I showed

00:12:10,640 --> 00:12:16,040
you with your schema you can also pass

00:12:12,110 --> 00:12:17,930
them on so if you read a message and you

00:12:16,040 --> 00:12:20,240
write it a crate and you want from this

00:12:17,930 --> 00:12:22,610
one then all the unknown fields that you

00:12:20,240 --> 00:12:24,560
can find in your original message will

00:12:22,610 --> 00:12:26,720
be copied over to the new one which

00:12:24,560 --> 00:12:28,940
allows you to basically to say things

00:12:26,720 --> 00:12:31,700
like decorations or extracting common

00:12:28,940 --> 00:12:33,260
fields or envelopes without needing to

00:12:31,700 --> 00:12:34,850
know there's a schema information of

00:12:33,260 --> 00:12:37,340
your original message you just put that

00:12:34,850 --> 00:12:39,740
message inside another message we will

00:12:37,340 --> 00:12:42,820
take over all the fields and send it on

00:12:39,740 --> 00:12:45,620
and you would not have to worry about

00:12:42,820 --> 00:12:51,230
breaking or destroying their original

00:12:45,620 --> 00:12:54,290
content it will still be fully there so

00:12:51,230 --> 00:12:56,300
now how can you use those two components

00:12:54,290 --> 00:12:59,600
together for a stream processing

00:12:56,300 --> 00:13:02,390
architecture so generally you still have

00:12:59,600 --> 00:13:05,620
all the usual message producers like

00:13:02,390 --> 00:13:10,510
logs databases api's external sources

00:13:05,620 --> 00:13:15,230
they all write their information in

00:13:10,510 --> 00:13:17,900
protobuf to your Kafka cluster where you

00:13:15,230 --> 00:13:21,500
have streaming processors that enrich

00:13:17,900 --> 00:13:23,810
showing filter aggregate all the usual

00:13:21,500 --> 00:13:26,450
stuff with your topics and create new

00:13:23,810 --> 00:13:29,390
topics with the output and that one can

00:13:26,450 --> 00:13:32,240
go to your various consumers for example

00:13:29,390 --> 00:13:35,450
databases reports dashboards other

00:13:32,240 --> 00:13:39,530
applications machine learning or storing

00:13:35,450 --> 00:13:43,220
it in Hadoop for example and you can

00:13:39,530 --> 00:13:45,110
exchange schema versions through for

00:13:43,220 --> 00:13:47,210
example a git repository or some other

00:13:45,110 --> 00:13:52,990
central place where everyone can

00:13:47,210 --> 00:13:54,920
retrieve these definitions and the

00:13:52,990 --> 00:13:57,650
developers engineers that write the

00:13:54,920 --> 00:14:00,170
message they write the input data they

00:13:57,650 --> 00:14:01,610
provide their protobuf definitions the

00:14:00,170 --> 00:14:04,630
engineers writing the streaming

00:14:01,610 --> 00:14:07,160
processors they also provide the

00:14:04,630 --> 00:14:09,380
definitions for their output while

00:14:07,160 --> 00:14:12,800
reading the one from the engineers and

00:14:09,380 --> 00:14:14,899
the consumers they just need to read the

00:14:12,800 --> 00:14:18,769
schema definitions to be able to parse

00:14:14,899 --> 00:14:22,370
everything correctly so how would that

00:14:18,769 --> 00:14:26,769
look as a likeness in an example here we

00:14:22,370 --> 00:14:30,560
have three different systems that emit

00:14:26,769 --> 00:14:34,120
events to Kafka so for example a web

00:14:30,560 --> 00:14:37,490
service that writes block events to your

00:14:34,120 --> 00:14:39,890
first stream so log events from your

00:14:37,490 --> 00:14:42,170
website for example in there we have our

00:14:39,890 --> 00:14:45,290
time stem field a session ID and the

00:14:42,170 --> 00:14:47,329
type like okay you looked at a product

00:14:45,290 --> 00:14:49,490
or a customer put a product in their

00:14:47,329 --> 00:14:52,300
shopping cart and then we have the

00:14:49,490 --> 00:14:55,610
option of use customer ID and Product ID

00:14:52,300 --> 00:14:58,970
then from your customer database you get

00:14:55,610 --> 00:15:02,810
these customer messages which again have

00:14:58,970 --> 00:15:05,690
a customer ID name and an email and your

00:15:02,810 --> 00:15:10,130
product database provides the product

00:15:05,690 --> 00:15:15,260
messages and with a product ID a name

00:15:10,130 --> 00:15:16,640
and a price these then you have a

00:15:15,260 --> 00:15:19,040
streaming application joining these

00:15:16,640 --> 00:15:22,430
three topics and emitting a new message

00:15:19,040 --> 00:15:24,350
which is a very simple message my rich

00:15:22,430 --> 00:15:27,110
event message basically has three fields

00:15:24,350 --> 00:15:30,350
one event field one customer field and

00:15:27,110 --> 00:15:33,350
one product field so I actually don't

00:15:30,350 --> 00:15:35,120
need to read to write a lot of schema

00:15:33,350 --> 00:15:39,230
definitions because I can just import

00:15:35,120 --> 00:15:41,270
the previous ones there and the output

00:15:39,230 --> 00:15:44,600
of this deeming application is again a

00:15:41,270 --> 00:15:48,110
stream in my Kafka cluster of these rich

00:15:44,600 --> 00:15:51,410
events and then in that I can for

00:15:48,110 --> 00:15:53,450
example plug in another application that

00:15:51,410 --> 00:15:56,360
does something like session association

00:15:53,450 --> 00:16:00,620
so it takes the session ID out of there

00:15:56,360 --> 00:16:02,570
and goes by them and my message my

00:16:00,620 --> 00:16:07,730
session message is also again very

00:16:02,570 --> 00:16:12,140
simple session message just consists of

00:16:07,730 --> 00:16:14,329
repeated which event messages so makes

00:16:12,140 --> 00:16:15,800
it very easy to define these things of

00:16:14,329 --> 00:16:17,990
course at this step I can also do

00:16:15,800 --> 00:16:20,240
further occasions like include fields

00:16:17,990 --> 00:16:22,310
for counting how many different products

00:16:20,240 --> 00:16:24,560
the user has looked at or how many

00:16:22,310 --> 00:16:25,790
interactions they did with our website

00:16:24,560 --> 00:16:28,870
and so on

00:16:25,790 --> 00:16:31,370
it's basically a very simple version and

00:16:28,870 --> 00:16:33,560
that one is then something that you can

00:16:31,370 --> 00:16:35,510
for example read by machine learning

00:16:33,560 --> 00:16:38,030
algorithms for personalization or

00:16:35,510 --> 00:16:41,540
recommendation engines you can put it in

00:16:38,030 --> 00:16:44,300
reports or you can let your analysts

00:16:41,540 --> 00:16:52,580
read it and do analyst and - there are

00:16:44,300 --> 00:16:55,190
talk reports on it now going now going

00:16:52,580 --> 00:16:57,260
back to this chart so this git

00:16:55,190 --> 00:16:59,150
repository or whatever you're using I

00:16:57,260 --> 00:17:01,430
mean it's basically just like they are

00:16:59,150 --> 00:17:03,260
Oh schema registry that comes with a

00:17:01,430 --> 00:17:05,570
confluent platform of Kafka so why is

00:17:03,260 --> 00:17:09,170
this one what's the advantage of this

00:17:05,570 --> 00:17:11,690
one well it can always happen that not

00:17:09,170 --> 00:17:14,170
all the product definitions there are

00:17:11,690 --> 00:17:16,940
actually up-to-date for example the

00:17:14,170 --> 00:17:19,940
developer could have forgotten to commit

00:17:16,940 --> 00:17:22,160
their message or the host of your kid

00:17:19,940 --> 00:17:24,770
repository accidentally deleted their

00:17:22,160 --> 00:17:27,140
production database and is now trying to

00:17:24,770 --> 00:17:30,110
recover up back up so it could always

00:17:27,140 --> 00:17:32,930
happen that this one doesn't quite work

00:17:30,110 --> 00:17:36,110
and someone doesn't have all the schema

00:17:32,930 --> 00:17:39,170
versions that they need so what happens

00:17:36,110 --> 00:17:42,350
in that case let's for example assume

00:17:39,170 --> 00:17:45,140
that in your product our database

00:17:42,350 --> 00:17:47,420
someone at some new field for the color

00:17:45,140 --> 00:17:53,030
of your product and starts writing that

00:17:47,420 --> 00:17:55,010
for the Kafka product stream now your

00:17:53,030 --> 00:17:56,690
three main application is starting to or

00:17:55,010 --> 00:18:00,200
is processing these events and putting

00:17:56,690 --> 00:18:02,690
them in the rich event message and all

00:18:00,200 --> 00:18:05,150
that happens is now that the product

00:18:02,690 --> 00:18:07,370
contains an unknown field which is the

00:18:05,150 --> 00:18:11,590
color but everything else will just keep

00:18:07,370 --> 00:18:11,590
on working and

00:18:12,900 --> 00:18:17,160
and even further down it causes no

00:18:16,230 --> 00:18:19,380
trouble at all

00:18:17,160 --> 00:18:20,580
my session will still contain these

00:18:19,380 --> 00:18:23,130
future events and these future events

00:18:20,580 --> 00:18:27,750
all will contain a product with unknown

00:18:23,130 --> 00:18:29,250
fields and even at the end the

00:18:27,750 --> 00:18:34,080
applications can still read these

00:18:29,250 --> 00:18:39,240
messages and if the VI are a list or a

00:18:34,080 --> 00:18:41,550
data scientists at the end looks at the

00:18:39,240 --> 00:18:43,500
messages he can find that unknown field

00:18:41,550 --> 00:18:46,830
it can look at the value and might even

00:18:43,500 --> 00:18:49,410
be able to find a determine that it's a

00:18:46,830 --> 00:18:50,670
supposed to be color maybe he was even

00:18:49,410 --> 00:18:52,230
the one that wrote the task to the

00:18:50,670 --> 00:18:54,720
engineer in the first place to put the

00:18:52,230 --> 00:19:00,350
field in there so the whole pipeline

00:18:54,720 --> 00:19:02,880
basically still works the same way and

00:19:00,350 --> 00:19:07,680
similar if it's just a need a field like

00:19:02,880 --> 00:19:09,210
here the price for a product off there's

00:19:07,680 --> 00:19:10,980
no difference between a field that

00:19:09,210 --> 00:19:13,740
simply doesn't exist in an empty field

00:19:10,980 --> 00:19:16,530
with a null value they both are simply

00:19:13,740 --> 00:19:18,990
not included in the protobuf message so

00:19:16,530 --> 00:19:22,530
you're going to have now is product that

00:19:18,990 --> 00:19:25,350
contains a null price and since your

00:19:22,530 --> 00:19:29,120
price since the very beginning was

00:19:25,350 --> 00:19:30,840
marked as an optional hopefully the

00:19:29,120 --> 00:19:33,840
engineers writing the stream

00:19:30,840 --> 00:19:36,360
applications kind of put in logic to

00:19:33,840 --> 00:19:38,640
handle null prices already so your

00:19:36,360 --> 00:19:43,920
streaming applications will continue

00:19:38,640 --> 00:19:46,500
running uninterrupted so in general what

00:19:43,920 --> 00:19:48,570
does this allow you so means that the

00:19:46,500 --> 00:19:50,610
different teams that are responsible for

00:19:48,570 --> 00:19:52,590
data for producing the data for

00:19:50,610 --> 00:19:55,290
processing the data for consuming the

00:19:52,590 --> 00:19:57,660
data they can all move at their own

00:19:55,290 --> 00:19:59,610
speed there's no real strict alignment

00:19:57,660 --> 00:20:02,190
for releases necessary so if you have a

00:19:59,610 --> 00:20:03,810
traditional OLTP system that feeds into

00:20:02,190 --> 00:20:06,990
a data warehouse and you want to do

00:20:03,810 --> 00:20:09,000
schema changes across the system with

00:20:06,990 --> 00:20:11,820
several for example my Secret Service

00:20:09,000 --> 00:20:14,790
involved it's very very painful because

00:20:11,820 --> 00:20:18,000
you need to make sure that each of the

00:20:14,790 --> 00:20:20,250
queries takes that into account but here

00:20:18,000 --> 00:20:23,430
we can really do a handoff and data

00:20:20,250 --> 00:20:24,990
engineering the the people in omit in

00:20:23,430 --> 00:20:26,850
the middle writing the screening process

00:20:24,990 --> 00:20:29,040
so they don't need to do anything

00:20:26,850 --> 00:20:31,610
they just rewrite that read the tape

00:20:29,040 --> 00:20:36,660
later and write it to the output and

00:20:31,610 --> 00:20:39,990
everyone can basically upgrade to new to

00:20:36,660 --> 00:20:44,070
new version schema versions at their own

00:20:39,990 --> 00:20:46,730
speed and only the actual producers and

00:20:44,070 --> 00:20:49,740
consumers need to align on including new

00:20:46,730 --> 00:20:51,660
information in your messages and the

00:20:49,740 --> 00:20:57,690
pipeline in between just forwards them

00:20:51,660 --> 00:21:00,780
without needing to know about it of

00:20:57,690 --> 00:21:02,700
course not everything is great about

00:21:00,780 --> 00:21:06,650
protobuf there are also some downsides

00:21:02,700 --> 00:21:06,650
compared to other also there's no

00:21:13,470 --> 00:21:19,390
like you kinda need to hand

00:21:16,450 --> 00:21:22,030
after your scheming be very considerate

00:21:19,390 --> 00:21:25,750
about it so you cannot reuse your field

00:21:22,030 --> 00:21:28,990
IDs that's the important thing and you

00:21:25,750 --> 00:21:30,730
need to stick to them and you need to

00:21:28,990 --> 00:21:32,470
also think about what you may mark as

00:21:30,730 --> 00:21:35,080
required and optional because once

00:21:32,470 --> 00:21:37,870
something is required you basically can

00:21:35,080 --> 00:21:39,910
never change it there's also less

00:21:37,870 --> 00:21:42,670
implementations for it around Kafka and

00:21:39,910 --> 00:21:44,970
Hadoop ecosystem and less people using

00:21:42,670 --> 00:21:47,830
it so you will find out that you will

00:21:44,970 --> 00:21:49,900
often have to write a serialized sati

00:21:47,830 --> 00:21:52,450
serialize for yourself there forever or

00:21:49,900 --> 00:21:56,170
you basically find enough

00:21:52,450 --> 00:21:59,200
implementations of that a big issue is

00:21:56,170 --> 00:22:00,790
that Google kind of wants to remove

00:21:59,200 --> 00:22:04,150
these unknown fields and protobuf free

00:22:00,790 --> 00:22:09,070
so very long discussion now going on in

00:22:04,150 --> 00:22:11,230
the related issue on github and one more

00:22:09,070 --> 00:22:14,530
thing is that protobuf messages usually

00:22:11,230 --> 00:22:17,380
are one or two bytes not sure and after

00:22:14,530 --> 00:22:19,000
all messages nowadays I don't think

00:22:17,380 --> 00:22:25,180
that's a really big deal but for some

00:22:19,000 --> 00:22:27,040
people that is important factor alright

00:22:25,180 --> 00:22:29,440
thank you I hope you learned something

00:22:27,040 --> 00:22:31,660
about kafka streams and pro buff and

00:22:29,440 --> 00:22:35,460
maybe have some inspiration yourself and

00:22:31,660 --> 00:22:35,460
we have any comments or questions

00:22:37,660 --> 00:22:57,040
I mean if you have so the question was

00:22:54,790 --> 00:22:58,840
if I only have two servers in my chakra

00:22:57,040 --> 00:23:01,270
cluster and one of them goes down how

00:22:58,840 --> 00:23:03,280
can I make sure that I don't lose any

00:23:01,270 --> 00:23:05,980
data and things still keep on running

00:23:03,280 --> 00:23:08,650
but if you set your topics to all have a

00:23:05,980 --> 00:23:10,680
replication factor of two it means each

00:23:08,650 --> 00:23:13,620
of your partitions everything should be

00:23:10,680 --> 00:23:16,360
existing on both servers so in that case

00:23:13,620 --> 00:23:18,460
basically they would be identical they

00:23:16,360 --> 00:23:19,810
both would have the full data set and if

00:23:18,460 --> 00:23:22,360
one of them goes down the other one

00:23:19,810 --> 00:23:25,210
still has everything and the producers

00:23:22,360 --> 00:23:28,030
would then all go to that one running

00:23:25,210 --> 00:23:30,550
path Katla server and write everything

00:23:28,030 --> 00:23:33,550
there and the consumers would also start

00:23:30,550 --> 00:23:36,060
on your reading from that one did that

00:23:33,550 --> 00:23:36,060
answer the question

00:23:39,510 --> 00:23:48,590
yep

00:23:42,450 --> 00:23:51,360
I mean usually the producer sends out

00:23:48,590 --> 00:23:54,210
the Kafka cluster sends out a signal

00:23:51,360 --> 00:23:55,740
back to the producer that it received

00:23:54,210 --> 00:23:56,370
the message and processed it and you can

00:23:55,740 --> 00:23:58,470
also define

00:23:56,370 --> 00:24:00,950
vended us that so we can say only

00:23:58,470 --> 00:24:03,450
acknowledge a message if it has been

00:24:00,950 --> 00:24:06,600
replicated on both machines so you can

00:24:03,450 --> 00:24:08,670
make sure that nothing is going missing

00:24:06,600 --> 00:24:11,640
let's actually a talk by Queen Shakira

00:24:08,670 --> 00:24:13,500
about when messages absolutely have to

00:24:11,640 --> 00:24:15,060
be there if you want to have a very

00:24:13,500 --> 00:24:17,220
strong guarantee that your messages are

00:24:15,060 --> 00:24:23,480
there recommend looking that up it's

00:24:17,220 --> 00:24:23,480

YouTube URL: https://www.youtube.com/watch?v=e9jpFHbS4XY


