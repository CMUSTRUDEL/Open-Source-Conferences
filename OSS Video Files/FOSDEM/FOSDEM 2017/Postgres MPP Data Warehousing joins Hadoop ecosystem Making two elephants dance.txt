Title: Postgres MPP Data Warehousing joins Hadoop ecosystem Making two elephants dance
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Roman Shaposhnik

At: FOSDEM 2017

Hadoop has been touted as a replacement for data warehouses. In practiceHadoop has had success offloading ETL/ELT workloads, but still has gapsserving requirements for operational analytics.

Apache Bigtop now includes Greenplum Database in deployment of big datasolutions. Greenplum Database is, an open source massively parallel datawarehouse based on PostgreSQL, and is an excellent addition to the Hadoopecosystem.

In this session we'll cover: * Introduction to Greenplum * Bigtop Support forGreenplum * External tables in Hadoop by Greenplum * Parallel reads and writesto Hadoop by Greenplum * Running advanced analytics on structured andunstructured data in both Hadoop and Greenplum via Apache MADlib (incubating)* Geospatial and Machine Learning in Greenplum based on HDFS data * Storingdata from a data lake in Greenplum for high throughput analytical queries


Room: H.2213
Scheduled start: 2017-02-04 15:00:00
Captions: 
	00:00:00,000 --> 00:00:02,030
Oh

00:00:04,580 --> 00:00:09,800
hello everyone my name is Roman sharpish

00:00:07,400 --> 00:00:12,440
Nick and I work at pivotal I also happen

00:00:09,800 --> 00:00:14,990
to be a co organizer of this dev room

00:00:12,440 --> 00:00:17,210
which by the way was my sneaky plan to

00:00:14,990 --> 00:00:19,160
get the stock accepted because I really

00:00:17,210 --> 00:00:21,970
think that today I will be talking about

00:00:19,160 --> 00:00:24,410
something that's quite interesting

00:00:21,970 --> 00:00:26,960
before I do that let me introduce myself

00:00:24,410 --> 00:00:29,480
a little bit more my involvement with

00:00:26,960 --> 00:00:32,270
Big Data goes all the way back to the

00:00:29,480 --> 00:00:35,899
original Hadoop team at Yahoo which I

00:00:32,270 --> 00:00:38,269
joined in 2010 and since then I managed

00:00:35,899 --> 00:00:41,690
to work at Cloudera pivotal basically

00:00:38,269 --> 00:00:43,070
companies that do data do Hadoop and

00:00:41,690 --> 00:00:45,620
with Hadoop it was interesting because

00:00:43,070 --> 00:00:47,559
when it first appeared everybody felt

00:00:45,620 --> 00:00:50,179
like it would totally replace

00:00:47,559 --> 00:00:52,609
traditional data warehouse systems and

00:00:50,179 --> 00:00:53,809
for a good reason because Hadoop had a

00:00:52,609 --> 00:00:57,350
lot going for it

00:00:53,809 --> 00:01:00,289
it was open source it was scalable it

00:00:57,350 --> 00:01:02,600
was developer friendly it was basically

00:01:00,289 --> 00:01:06,200
a really good system and for some time

00:01:02,600 --> 00:01:09,380
it really seemed like it was going to

00:01:06,200 --> 00:01:11,960
happen but recently an interesting trend

00:01:09,380 --> 00:01:14,210
started to appear more and more

00:01:11,960 --> 00:01:16,640
enterprises started using Hadoop in

00:01:14,210 --> 00:01:18,890
addition to data warehouse systems not

00:01:16,640 --> 00:01:22,760
replacing them but basically augmenting

00:01:18,890 --> 00:01:25,280
them with Hadoop and at first this could

00:01:22,760 --> 00:01:27,350
look like a very strange architecture

00:01:25,280 --> 00:01:29,810
because why would you have two different

00:01:27,350 --> 00:01:32,450
systems you know kind of doing similar

00:01:29,810 --> 00:01:34,430
things why not replace it with Jessica

00:01:32,450 --> 00:01:37,160
dupe and have the promised enterprise

00:01:34,430 --> 00:01:39,230
data Lake so today we will be talking

00:01:37,160 --> 00:01:42,080
about why those architectures actually

00:01:39,230 --> 00:01:45,500
make sense and what drives those types

00:01:42,080 --> 00:01:49,850
of architectures all right all right ok

00:01:45,500 --> 00:01:51,830
awesome so yes so basically the first

00:01:49,850 --> 00:01:53,120
time I was actually starting to see this

00:01:51,830 --> 00:01:55,160
type of architectures that they were

00:01:53,120 --> 00:01:56,630
building I want like huh what what are

00:01:55,160 --> 00:01:58,370
you doing so this is actually the slide

00:01:56,630 --> 00:02:02,000
that was publicly presented so I can

00:01:58,370 --> 00:02:03,890
steal it without writing my own that was

00:02:02,000 --> 00:02:06,590
at the IBM conference and the guy was

00:02:03,890 --> 00:02:08,030
from Seagate so not even the customer I

00:02:06,590 --> 00:02:10,780
was dealing with and he was explaining

00:02:08,030 --> 00:02:12,950
how Seagate is dealing with sort of the

00:02:10,780 --> 00:02:14,630
Internet of Things because you know they

00:02:12,950 --> 00:02:15,680
basically manufacture a lot of hard

00:02:14,630 --> 00:02:17,220
drives and they want to collect

00:02:15,680 --> 00:02:19,020
information from the

00:02:17,220 --> 00:02:20,430
drives so they basically had a whole

00:02:19,020 --> 00:02:22,400
bunch of Hadoop clusters kind of an

00:02:20,430 --> 00:02:24,330
impoverished periphery you know

00:02:22,400 --> 00:02:25,500
aggregating all of the information but

00:02:24,330 --> 00:02:27,090
then all the time they actually had a

00:02:25,500 --> 00:02:29,550
traditional enterprise data warehousing

00:02:27,090 --> 00:02:31,410
right in the middle kind of actually

00:02:29,550 --> 00:02:33,090
taking care of the important data point

00:02:31,410 --> 00:02:35,250
and I was like what why not

00:02:33,090 --> 00:02:37,290
Hadoop and then we had a couple of more

00:02:35,250 --> 00:02:39,930
customers like that at P Hotel and the

00:02:37,290 --> 00:02:42,710
architecture that emerge throughout the

00:02:39,930 --> 00:02:46,440
group of customers was basically this

00:02:42,710 --> 00:02:48,180
and I would love if during the Q&A you

00:02:46,440 --> 00:02:50,070
could challenge me on this and maybe you

00:02:48,180 --> 00:02:51,209
can tell me how how wrong it is but

00:02:50,070 --> 00:02:54,450
that's actually what I'm seeing in the

00:02:51,209 --> 00:02:55,800
field so almost all of these guys who I

00:02:54,450 --> 00:02:58,110
will be talking about their building

00:02:55,800 --> 00:03:00,030
back-end architectures to essential

00:02:58,110 --> 00:03:01,830
support data driven applications and

00:03:00,030 --> 00:03:03,510
what I mean by that is that they

00:03:01,830 --> 00:03:04,770
basically have to optimize the

00:03:03,510 --> 00:03:06,780
relationship that they have with their

00:03:04,770 --> 00:03:08,400
customers and the devices or whatever it

00:03:06,780 --> 00:03:10,910
is that they're managing so it's

00:03:08,400 --> 00:03:13,500
typically like a bunch of either web or

00:03:10,910 --> 00:03:15,240
iut traffic coming into a data center

00:03:13,500 --> 00:03:17,910
and sometimes even multiple data centers

00:03:15,240 --> 00:03:19,290
and that's on the Left right and on the

00:03:17,910 --> 00:03:21,330
right you basically have a bunch of

00:03:19,290 --> 00:03:23,489
users you know using the application

00:03:21,330 --> 00:03:25,110
from mobile and desktop technologies and

00:03:23,489 --> 00:03:27,350
whatnot so the question then becomes

00:03:25,110 --> 00:03:30,510
what do you build in a data center as a

00:03:27,350 --> 00:03:32,100
complete rm2 ant architecture as opposed

00:03:30,510 --> 00:03:34,410
to just building a data science piece or

00:03:32,100 --> 00:03:36,840
you know a data management piece right

00:03:34,410 --> 00:03:38,010
and what we typically see is that and

00:03:36,840 --> 00:03:39,690
again you know some of it I'm just

00:03:38,010 --> 00:03:41,310
throwing names here you can use

00:03:39,690 --> 00:03:42,750
different projects within the Apache

00:03:41,310 --> 00:03:45,120
ecosystem but the name gives you a

00:03:42,750 --> 00:03:46,410
flavor for what typically gets used so

00:03:45,120 --> 00:03:48,630
typically like all of these events you

00:03:46,410 --> 00:03:49,860
know get represented as Kafka queues so

00:03:48,630 --> 00:03:51,330
the saying goes there if you know

00:03:49,860 --> 00:03:53,730
whatever it is on the outside of the

00:03:51,330 --> 00:03:55,470
datacenter must look like an Kafka to

00:03:53,730 --> 00:03:58,049
you you know inside of a data center and

00:03:55,470 --> 00:04:00,870
then you basically have some kind of an

00:03:58,049 --> 00:04:02,730
ETL 2.0 and what I mean by that is that

00:04:00,870 --> 00:04:04,410
it's not ETL that basically takes data

00:04:02,730 --> 00:04:06,030
from one existing place you know does

00:04:04,410 --> 00:04:07,680
something to it and put it into a

00:04:06,030 --> 00:04:09,989
different place it's actually ETL that

00:04:07,680 --> 00:04:11,850
does it on the data that's in flight so

00:04:09,989 --> 00:04:14,940
you know you basically have a bunch of

00:04:11,850 --> 00:04:16,640
ETL functions that are running on some

00:04:14,940 --> 00:04:18,600
kind of substrate so knife I the

00:04:16,640 --> 00:04:21,479
presentation right before me was a good

00:04:18,600 --> 00:04:25,500
example of something like that pivotal

00:04:21,479 --> 00:04:26,430
has the spring cloud data flow that can

00:04:25,500 --> 00:04:29,849
do some of that too

00:04:26,430 --> 00:04:30,750
but basically what gets what gets done

00:04:29,849 --> 00:04:33,360
here

00:04:30,750 --> 00:04:35,370
that you essentially split the traffic

00:04:33,360 --> 00:04:37,320
that's coming into your data center into

00:04:35,370 --> 00:04:38,880
multiple different things so first of

00:04:37,320 --> 00:04:40,980
all you basically get some of that

00:04:38,880 --> 00:04:42,330
traffic right straight into HDFS and you

00:04:40,980 --> 00:04:44,100
just dump it there and you leave it

00:04:42,330 --> 00:04:46,230
there and something will happen to it

00:04:44,100 --> 00:04:48,540
later on then some of that traffic is

00:04:46,230 --> 00:04:50,340
actually getting into the in-memory data

00:04:48,540 --> 00:04:52,020
grid and that's the thing that actually

00:04:50,340 --> 00:04:53,550
connects your application to the data

00:04:52,020 --> 00:04:56,850
that's in flight right now you know

00:04:53,550 --> 00:04:58,380
whatever this sensor is producing right

00:04:56,850 --> 00:05:00,630
now you will actually gather through

00:04:58,380 --> 00:05:02,370
that thing but then and that's actually

00:05:00,630 --> 00:05:03,950
interesting then you actually get some

00:05:02,370 --> 00:05:06,360
of the data into a more of a traditional

00:05:03,950 --> 00:05:07,890
MPP type of a solution and the good news

00:05:06,360 --> 00:05:08,760
is that you actually have an open source

00:05:07,890 --> 00:05:13,170
one and greenplum

00:05:08,760 --> 00:05:16,380
today and the split of all three here is

00:05:13,170 --> 00:05:18,870
interesting because what happens then is

00:05:16,380 --> 00:05:20,610
within all of these systems you

00:05:18,870 --> 00:05:22,350
basically have people who are typically

00:05:20,610 --> 00:05:24,030
you know either data scientist of data

00:05:22,350 --> 00:05:25,680
engineers who essentially maintain data

00:05:24,030 --> 00:05:28,800
models based on the raw data that gets

00:05:25,680 --> 00:05:31,740
into all of these systems right and you

00:05:28,800 --> 00:05:34,020
only get as much usability out of the

00:05:31,740 --> 00:05:35,370
data is the kind of data model

00:05:34,020 --> 00:05:37,080
interesting data models that you can

00:05:35,370 --> 00:05:38,220
then feed back into the application you

00:05:37,080 --> 00:05:40,200
get and these are the guys who

00:05:38,220 --> 00:05:42,240
maintaining it but even those guys

00:05:40,200 --> 00:05:45,150
basically sort of a lot of times at

00:05:42,240 --> 00:05:47,070
least in the customers that I see kind

00:05:45,150 --> 00:05:49,080
of push those data map models back into

00:05:47,070 --> 00:05:52,050
this middle layer which is the

00:05:49,080 --> 00:05:56,550
traditional MPP so the question is why

00:05:52,050 --> 00:05:58,110
and I think before we can answer that

00:05:56,550 --> 00:05:59,520
question again let me repeat the fact

00:05:58,110 --> 00:06:01,890
that what we're building here is not

00:05:59,520 --> 00:06:03,810
just a data science piece we're not just

00:06:01,890 --> 00:06:05,760
doing it for let's say you know life

00:06:03,810 --> 00:06:07,530
science is trying to analyze human DNA

00:06:05,760 --> 00:06:09,540
or something right we're building an EM

00:06:07,530 --> 00:06:12,990
to end architecture for essentially a

00:06:09,540 --> 00:06:15,600
modern way application you know the way

00:06:12,990 --> 00:06:17,670
you get it from uber or from Netflix or

00:06:15,600 --> 00:06:18,630
from Airbnb and for all of these guys

00:06:17,670 --> 00:06:20,850
you know the data that they're

00:06:18,630 --> 00:06:22,620
collecting must manifest itself in the

00:06:20,850 --> 00:06:25,590
application that's the end game for all

00:06:22,620 --> 00:06:27,990
of them right you know in uber it's

00:06:25,590 --> 00:06:30,690
basically you know tracking and sort of

00:06:27,990 --> 00:06:32,790
predicting and doing this search pricing

00:06:30,690 --> 00:06:35,370
you know Netflix must recommend the new

00:06:32,790 --> 00:06:37,200
movie for you and all of these guys

00:06:35,370 --> 00:06:39,510
again building this entrant architecture

00:06:37,200 --> 00:06:40,830
so can we just do it with Hadoop well

00:06:39,510 --> 00:06:43,290
again like I'm saying hopefully during

00:06:40,830 --> 00:06:44,190
the Q&A you will try to reconvince me

00:06:43,290 --> 00:06:47,580
that maybe we can

00:06:44,190 --> 00:06:49,380
but here's what I've seen so far in

00:06:47,580 --> 00:06:50,610
again talking talking to the customer

00:06:49,380 --> 00:06:52,470
because it's a pretty pragmatic

00:06:50,610 --> 00:06:55,320
presentation that I'm trying to do here

00:06:52,470 --> 00:06:56,820
so first answer is like yes absolutely

00:06:55,320 --> 00:06:59,610
but then you know the customer turns

00:06:56,820 --> 00:07:01,200
around and asks you but what do you mean

00:06:59,610 --> 00:07:02,880
by Hadoop what this could do these days

00:07:01,200 --> 00:07:04,830
really because if you're just talking

00:07:02,880 --> 00:07:07,560
about you know some kind of a scale out

00:07:04,830 --> 00:07:08,790
storage you know file system like HDFS

00:07:07,560 --> 00:07:10,890
yeah that's awesome you know we can

00:07:08,790 --> 00:07:12,960
totally use HDFS maybe you're also

00:07:10,890 --> 00:07:15,540
talking about scheduling frameworks

00:07:12,960 --> 00:07:17,550
something like Yaron that's fine but

00:07:15,540 --> 00:07:22,440
then there are all these other questions

00:07:17,550 --> 00:07:23,850
right a lot of the customers that are

00:07:22,440 --> 00:07:24,870
building those types of applications

00:07:23,850 --> 00:07:27,330
right now there are very traditional

00:07:24,870 --> 00:07:28,980
enterprises so they do a lot of what's

00:07:27,330 --> 00:07:31,380
known as bi you know business handle

00:07:28,980 --> 00:07:32,940
intelligence and bi is typically done

00:07:31,380 --> 00:07:34,620
through the tooling that actually

00:07:32,940 --> 00:07:37,410
expects a sequel database on the other

00:07:34,620 --> 00:07:38,880
end so the most you know traditional

00:07:37,410 --> 00:07:40,260
example of the tooling like that is a

00:07:38,880 --> 00:07:42,090
tableau so you know very

00:07:40,260 --> 00:07:44,220
well-established or the few I for doing

00:07:42,090 --> 00:07:45,990
bi and there is absolutely nothing that

00:07:44,220 --> 00:07:48,120
you can like hook up tableau right away

00:07:45,990 --> 00:07:50,000
to in Hadoop ecosystem you can kind of

00:07:48,120 --> 00:07:53,220
have to build additional bits and pieces

00:07:50,000 --> 00:07:54,810
into that that one is actually also

00:07:53,220 --> 00:07:57,030
interesting because a lot of them are

00:07:54,810 --> 00:07:59,160
telling me like this is awesome that all

00:07:57,030 --> 00:08:01,050
of the Big Data community is talking

00:07:59,160 --> 00:08:04,410
about machine learning in Scala and on

00:08:01,050 --> 00:08:07,110
SPARC and that's basically great for the

00:08:04,410 --> 00:08:09,810
five people that know how to do it and

00:08:07,110 --> 00:08:11,610
all the rest of us who are still stuck

00:08:09,810 --> 00:08:13,650
with sequel and you know stored

00:08:11,610 --> 00:08:16,110
procedures we actually want it to be

00:08:13,650 --> 00:08:17,940
given to us we want to contribute to the

00:08:16,110 --> 00:08:20,970
machine learning as much as the guys who

00:08:17,940 --> 00:08:23,070
actually do Scala and Java and whatnot

00:08:20,970 --> 00:08:25,620
so I actually tend to call it democracy

00:08:23,070 --> 00:08:27,690
democracy is bi and machine learning if

00:08:25,620 --> 00:08:31,230
you can open up it to the people who

00:08:27,690 --> 00:08:33,450
have the old-school skill sets then the

00:08:31,230 --> 00:08:35,219
the first thing that you're doing you're

00:08:33,450 --> 00:08:37,650
basically getting in getting them into

00:08:35,219 --> 00:08:39,030
the game right you cannot really put

00:08:37,650 --> 00:08:40,770
that barrier in front of them and say

00:08:39,030 --> 00:08:42,630
like you gotta learn scale you know go

00:08:40,770 --> 00:08:44,520
learn spark you know to be productive

00:08:42,630 --> 00:08:45,780
you kind of have to invite them over and

00:08:44,520 --> 00:08:47,160
you know extend the hand and say like

00:08:45,780 --> 00:08:49,380
yeah you can be productive you know and

00:08:47,160 --> 00:08:51,120
I'll show you how and maybe gradually

00:08:49,380 --> 00:08:54,240
you know you will build build out your

00:08:51,120 --> 00:08:55,740
skills have and the other two like once

00:08:54,240 --> 00:08:57,360
you do that the other to them becomes

00:08:55,740 --> 00:08:57,810
you know the question is like oh so now

00:08:57,360 --> 00:08:59,850
I

00:08:57,810 --> 00:09:01,560
not just five guys within my company who

00:08:59,850 --> 00:09:03,420
know how to do spark in Scala I have

00:09:01,560 --> 00:09:05,639
hundreds of people who actually are very

00:09:03,420 --> 00:09:07,470
comfortable doing bi but what happens if

00:09:05,639 --> 00:09:09,569
I open up that system to all of these

00:09:07,470 --> 00:09:12,839
guys to kinda like hammer on at once can

00:09:09,569 --> 00:09:13,980
it actually handle the load and the last

00:09:12,839 --> 00:09:15,149
one is again if you're building an

00:09:13,980 --> 00:09:16,649
application you kind of have to have

00:09:15,149 --> 00:09:18,870
something that you would be able to hook

00:09:16,649 --> 00:09:23,370
up in a traditional in memory scale out

00:09:18,870 --> 00:09:25,230
you know layer and a sequel database is

00:09:23,370 --> 00:09:26,939
typically a good option again you can do

00:09:25,230 --> 00:09:28,410
HBase or something like that but then

00:09:26,939 --> 00:09:30,749
you're sort of assembling bits and

00:09:28,410 --> 00:09:32,670
pieces in a very ad hoc manner instead

00:09:30,749 --> 00:09:34,620
of just having a central piece of your

00:09:32,670 --> 00:09:36,839
architecture that can do a lot of it can

00:09:34,620 --> 00:09:39,120
answer a lot of these questions as

00:09:36,839 --> 00:09:42,240
opposed to you know trying to do just

00:09:39,120 --> 00:09:45,540
one thing like many of the Hadoop

00:09:42,240 --> 00:09:49,050
ecosystem projects do so to summarize

00:09:45,540 --> 00:09:51,449
what I was trying to say I guess after I

00:09:49,050 --> 00:09:56,100
worked with who do four well I guess it

00:09:51,449 --> 00:09:58,680
would be ten years by now I can really

00:09:56,100 --> 00:10:00,689
vouch for these so coop is really great

00:09:58,680 --> 00:10:02,850
for hadouken hadouken system is really

00:10:00,689 --> 00:10:07,079
great for elastic storage capacity

00:10:02,850 --> 00:10:08,790
I mean HDFS is pretty well debugged and

00:10:07,079 --> 00:10:10,259
awesome at this point you know it has

00:10:08,790 --> 00:10:12,149
all the bells and whistles like high

00:10:10,259 --> 00:10:13,889
availability you can configure it you

00:10:12,149 --> 00:10:15,629
know to be pretty fault tolerant so if

00:10:13,889 --> 00:10:17,490
you want to just dump data you know to

00:10:15,629 --> 00:10:19,529
some place very quickly remember that

00:10:17,490 --> 00:10:20,910
you know bottom arrow on my slide you

00:10:19,529 --> 00:10:22,319
know just like the data comes in and you

00:10:20,910 --> 00:10:24,449
need to just store it for later

00:10:22,319 --> 00:10:26,459
processing HDFS is great it's great for

00:10:24,449 --> 00:10:28,050
lending data it's great for discovery is

00:10:26,459 --> 00:10:30,059
great for trying to figure out what's

00:10:28,050 --> 00:10:31,589
inside of your data and that is actually

00:10:30,059 --> 00:10:33,240
a job for a couple of people in the

00:10:31,589 --> 00:10:36,689
enterprise and here's why because all of

00:10:33,240 --> 00:10:38,910
these enterprises are really bound by

00:10:36,689 --> 00:10:42,629
regulations and just internal you know

00:10:38,910 --> 00:10:45,480
rules of what data can be exposed and

00:10:42,629 --> 00:10:48,240
how and if all you get is just raw

00:10:45,480 --> 00:10:50,490
streams of data that is coming in you

00:10:48,240 --> 00:10:52,050
typically want to reduce the number of

00:10:50,490 --> 00:10:54,269
people who can actually explore that

00:10:52,050 --> 00:10:56,069
data you want that exploration process

00:10:54,269 --> 00:10:59,040
to happen within a very limited group of

00:10:56,069 --> 00:11:01,079
people so that later on you know you can

00:10:59,040 --> 00:11:03,149
actually open it up but you can open up

00:11:01,079 --> 00:11:04,740
in a meaningful way so maybe you will be

00:11:03,149 --> 00:11:05,939
masking some of the data fields maybe

00:11:04,740 --> 00:11:07,920
you will be transforming it into

00:11:05,939 --> 00:11:10,199
something else maybe you will be putting

00:11:07,920 --> 00:11:11,570
a particular Eckles in place you know to

00:11:10,199 --> 00:11:13,730
actually make it more excessive

00:11:11,570 --> 00:11:16,040
but the initial exploration you know the

00:11:13,730 --> 00:11:17,270
fact that all of a sudden the log files

00:11:16,040 --> 00:11:18,980
that are streaming through your

00:11:17,270 --> 00:11:20,930
application contain you know personally

00:11:18,980 --> 00:11:21,910
identifiable information that cannot be

00:11:20,930 --> 00:11:24,140
overlooked

00:11:21,910 --> 00:11:25,820
and you know if you want to do

00:11:24,140 --> 00:11:27,890
sophisticated machine learning a

00:11:25,820 --> 00:11:30,260
duplicate system is great so

00:11:27,890 --> 00:11:32,390
complementary today to that traditional

00:11:30,260 --> 00:11:35,060
MPP which stands from massively parallel

00:11:32,390 --> 00:11:37,700
processing if you wondering what what

00:11:35,060 --> 00:11:39,410
MPP is it's actually really great for

00:11:37,700 --> 00:11:41,090
scheming right which is basically once

00:11:39,410 --> 00:11:42,710
you know what the data is look like you

00:11:41,090 --> 00:11:44,660
know what the data it is that you're

00:11:42,710 --> 00:11:46,430
dealing with you actually want it to be

00:11:44,660 --> 00:11:48,560
available to as many people as possible

00:11:46,430 --> 00:11:50,630
and then you create all these views and

00:11:48,560 --> 00:11:53,150
tables you know that it's very

00:11:50,630 --> 00:11:55,310
traditional bi person would expect but

00:11:53,150 --> 00:11:58,280
then you are making the data known you

00:11:55,310 --> 00:11:59,870
actually have an API between you who is

00:11:58,280 --> 00:12:01,310
maintaining the data infrastructure and

00:11:59,870 --> 00:12:03,140
the people who will be analyzing and

00:12:01,310 --> 00:12:04,820
creating the data model that is a very

00:12:03,140 --> 00:12:07,630
well understood API it's a bunch of

00:12:04,820 --> 00:12:09,830
views sometimes a bunch of tables

00:12:07,630 --> 00:12:11,150
transactionality sometimes comes into

00:12:09,830 --> 00:12:13,280
picture because it's easier to do

00:12:11,150 --> 00:12:15,980
transactions on MPP and to some

00:12:13,280 --> 00:12:19,610
customers is actually meaningful it's

00:12:15,980 --> 00:12:21,580
interesting and the last one for those

00:12:19,610 --> 00:12:25,550
of you who cannot actually see it I'll

00:12:21,580 --> 00:12:28,670
read the bottom one so that democratized

00:12:25,550 --> 00:12:30,680
machine learning you can all of a sudden

00:12:28,670 --> 00:12:32,150
use are and you can all of a sudden use

00:12:30,680 --> 00:12:35,750
your traditional sort of sequel like

00:12:32,150 --> 00:12:38,300
tools which I will cover in a bit so

00:12:35,750 --> 00:12:40,820
what do you do it will there's maybe a

00:12:38,300 --> 00:12:42,920
few choices in the open source you know

00:12:40,820 --> 00:12:44,210
the one that I will be getting you

00:12:42,920 --> 00:12:46,400
through today is called green plum

00:12:44,210 --> 00:12:47,960
database so green plum database is

00:12:46,400 --> 00:12:51,770
actually a proud member of the progress

00:12:47,960 --> 00:12:53,450
family of relational databases it's kind

00:12:51,770 --> 00:12:55,010
of like a long timeline but a couple of

00:12:53,450 --> 00:12:56,690
points I wanted to pull out you know

00:12:55,010 --> 00:12:58,460
politics actually got created a long

00:12:56,690 --> 00:13:01,880
time ago like really a long time ago in

00:12:58,460 --> 00:13:03,380
86 so green plum database got created

00:13:01,880 --> 00:13:05,960
based on progress eight you know it

00:13:03,380 --> 00:13:07,490
around 2005 interestingly enough there

00:13:05,960 --> 00:13:09,440
was exactly the year when Hadoop got

00:13:07,490 --> 00:13:13,010
created now who do got created around

00:13:09,440 --> 00:13:16,100
2005 as a sub project of match 2006 is

00:13:13,010 --> 00:13:18,110
when sort of Yahoo got really interested

00:13:16,100 --> 00:13:20,840
in it and the rest is history but the

00:13:18,110 --> 00:13:22,520
point that I wanted to make is that we

00:13:20,840 --> 00:13:24,750
open source we implement database in

00:13:22,520 --> 00:13:28,140
2015 and we

00:13:24,750 --> 00:13:29,700
even rebased it on Postgres 8.4 so it's

00:13:28,140 --> 00:13:31,200
still not possible sign and we're

00:13:29,700 --> 00:13:33,000
working on that but at least it's close

00:13:31,200 --> 00:13:34,350
enough to really sort of the kind of

00:13:33,000 --> 00:13:36,960
posters that you would expect

00:13:34,350 --> 00:13:40,680
so very quickly Green clamp database is

00:13:36,960 --> 00:13:43,320
a MPP project you know it's something

00:13:40,680 --> 00:13:44,910
that has been in development for more

00:13:43,320 --> 00:13:47,660
than ten years if you want to

00:13:44,910 --> 00:13:50,940
participate your best option is

00:13:47,660 --> 00:13:54,750
www.drinklab.org G you know all of the

00:13:50,940 --> 00:13:56,610
community aspects are on that website if

00:13:54,750 --> 00:13:59,220
you're wondering what green plum is and

00:13:56,610 --> 00:14:01,230
what it does think of it as a Charlotte

00:13:59,220 --> 00:14:03,150
Postgres so you basically have a bunch

00:14:01,230 --> 00:14:04,800
of what we call segments and these

00:14:03,150 --> 00:14:07,200
segments are essentially individual

00:14:04,800 --> 00:14:09,000
posters databases you also have a master

00:14:07,200 --> 00:14:11,220
course right here that is you know

00:14:09,000 --> 00:14:13,230
coordinating things like query planning

00:14:11,220 --> 00:14:15,450
and you know what data needs to get

00:14:13,230 --> 00:14:17,010
where but basically the way to think

00:14:15,450 --> 00:14:19,290
like the mental model is very sort of

00:14:17,010 --> 00:14:21,930
simplified mental model is it's a

00:14:19,290 --> 00:14:24,270
traditional progress database but every

00:14:21,930 --> 00:14:27,030
table that you create has a column that

00:14:24,270 --> 00:14:28,920
you need to designate and that column is

00:14:27,030 --> 00:14:32,839
used for sharding so basically whatever

00:14:28,920 --> 00:14:35,730
the hash of the value in that column

00:14:32,839 --> 00:14:37,770
depending on the value of that hash your

00:14:35,730 --> 00:14:42,150
data will end up in different segments

00:14:37,770 --> 00:14:44,610
on different hosts you still get if you

00:14:42,150 --> 00:14:46,080
as though it is a database that's just

00:14:44,610 --> 00:14:47,880
available on a single system it's a

00:14:46,080 --> 00:14:49,430
traditional process database so anything

00:14:47,880 --> 00:14:52,290
that works with partners would work with

00:14:49,430 --> 00:14:53,850
greenplum you can just connect you know

00:14:52,290 --> 00:14:55,320
your traditional PC pool command and

00:14:53,850 --> 00:14:56,790
connect to a master and then you know

00:14:55,320 --> 00:15:00,200
you're querying the database is just

00:14:56,790 --> 00:15:02,730
like you would do with pause-press

00:15:00,200 --> 00:15:04,500
it's a shared nothing architecture so

00:15:02,730 --> 00:15:08,040
basically like I said the master just

00:15:04,500 --> 00:15:10,280
you know does coordination and a lot of

00:15:08,040 --> 00:15:13,589
times we sort of do the high speed

00:15:10,280 --> 00:15:16,470
interconnects for pipeline so you know

00:15:13,589 --> 00:15:19,470
the the guys talk you know talk through

00:15:16,470 --> 00:15:21,720
this interconnect greenplum is a topic

00:15:19,470 --> 00:15:23,580
software solution so if you've seen

00:15:21,720 --> 00:15:25,440
green plum in the card where incarnation

00:15:23,580 --> 00:15:27,000
you know you might have seen some of the

00:15:25,440 --> 00:15:29,370
networking that was specifically

00:15:27,000 --> 00:15:31,560
optimized for it it helps that you don't

00:15:29,370 --> 00:15:33,990
have to do it so you can just launch

00:15:31,560 --> 00:15:35,550
green plum on Amazon or Azure and you're

00:15:33,990 --> 00:15:36,930
in your own data center that's that's

00:15:35,550 --> 00:15:38,550
just fine

00:15:36,930 --> 00:15:41,040
so a couple couple of the points that I

00:15:38,550 --> 00:15:43,770
wanted to make is that green plum

00:15:41,040 --> 00:15:46,650
actually has a concept that is now in

00:15:43,770 --> 00:15:50,160
Patras known as foreign dater rappers FD

00:15:46,650 --> 00:15:51,420
W's in green plum it's a different

00:15:50,160 --> 00:15:53,700
slightly different one because when

00:15:51,420 --> 00:15:56,580
green plum got it partners didn't

00:15:53,700 --> 00:15:58,320
actually have FGM w's but green plum can

00:15:56,580 --> 00:16:00,480
connect to a lot of data sources right

00:15:58,320 --> 00:16:02,010
so green pumpkin actually represent as

00:16:00,480 --> 00:16:05,880
an external table something that's one

00:16:02,010 --> 00:16:08,339
is three or in you know files on your

00:16:05,880 --> 00:16:10,200
file system or even in HDFS itself and

00:16:08,339 --> 00:16:11,370
that is exactly how we're connecting it

00:16:10,200 --> 00:16:14,610
to the Hadoop ecosystem so that

00:16:11,370 --> 00:16:17,310
connection to the HDFS is the Hadoop

00:16:14,610 --> 00:16:19,920
ecosystem connection interesting cool

00:16:17,310 --> 00:16:23,130
things that are sort of part of green

00:16:19,920 --> 00:16:24,750
plum Orca is the first query optimizer

00:16:23,130 --> 00:16:26,760
you know that is basically planning

00:16:24,750 --> 00:16:29,370
queries knowing that it does it for an

00:16:26,760 --> 00:16:32,790
MPP type of a system and it's optimizing

00:16:29,370 --> 00:16:34,980
the query to run for long it's actually

00:16:32,790 --> 00:16:37,529
a pretty sophisticated piece of software

00:16:34,980 --> 00:16:40,770
based on a scientific research that's

00:16:37,529 --> 00:16:42,270
been done for at least five years it's a

00:16:40,770 --> 00:16:45,360
standalone component so it can be

00:16:42,270 --> 00:16:48,150
plugged into any kind of database it's

00:16:45,360 --> 00:16:49,529
easier to plug it into well conceptually

00:16:48,150 --> 00:16:51,240
at least it's easier to plug it into

00:16:49,529 --> 00:16:52,860
Postgres you know family of databases

00:16:51,240 --> 00:16:54,900
but you can actually plug it into any

00:16:52,860 --> 00:16:58,050
database because it's implemented as a

00:16:54,900 --> 00:17:00,870
separate standalone service that does

00:16:58,050 --> 00:17:03,050
the query planning for you speaking of

00:17:00,870 --> 00:17:06,300
the democratized machine learning so

00:17:03,050 --> 00:17:08,040
greenplum actually comes with the

00:17:06,300 --> 00:17:10,260
machine learning library called Madlib

00:17:08,040 --> 00:17:11,850
and it's an interesting one because it

00:17:10,260 --> 00:17:15,420
does machine learning as though you are

00:17:11,850 --> 00:17:19,050
doing queries on tables via traditional

00:17:15,420 --> 00:17:21,120
sequel so for example if we want to do a

00:17:19,050 --> 00:17:23,370
linear regression and basically just

00:17:21,120 --> 00:17:25,949
train our model instead of writing any

00:17:23,370 --> 00:17:28,020
kind of code we would just invoke one of

00:17:25,949 --> 00:17:30,000
the pre-canned functions that madly but

00:17:28,020 --> 00:17:31,770
the library gives us now the invocation

00:17:30,000 --> 00:17:33,179
would look like a select statement you

00:17:31,770 --> 00:17:35,370
know that's the training of the model so

00:17:33,179 --> 00:17:38,100
we're basically passing tables here and

00:17:35,370 --> 00:17:40,260
what we get is we get the output that is

00:17:38,100 --> 00:17:41,760
the model that is trained and of course

00:17:40,260 --> 00:17:43,380
you know the way we would then use that

00:17:41,760 --> 00:17:45,570
model is again through the same Select

00:17:43,380 --> 00:17:47,460
you know similar select statement so all

00:17:45,570 --> 00:17:50,000
we're doing is we're essentially calling

00:17:47,460 --> 00:17:52,220
out functions that are being

00:17:50,000 --> 00:17:54,740
to us through the sequel interface by

00:17:52,220 --> 00:17:57,230
the Madlib that got installed on our

00:17:54,740 --> 00:17:58,910
cluster so again the flow between Madlib

00:17:57,230 --> 00:18:01,160
and the rest of the system is pretty

00:17:58,910 --> 00:18:03,380
simple actually it's kind of like you

00:18:01,160 --> 00:18:05,480
know the MapReduce without exposing to

00:18:03,380 --> 00:18:06,860
you the MapReduce part right so you

00:18:05,480 --> 00:18:08,420
essentially have a client which could be

00:18:06,860 --> 00:18:09,140
like I said anything you know could be

00:18:08,420 --> 00:18:10,460
Jupiter

00:18:09,140 --> 00:18:13,310
you know Apaches Apple II and if you're

00:18:10,460 --> 00:18:14,780
doing machine learning P sequel works

00:18:13,310 --> 00:18:17,120
just fine so you can run all over this

00:18:14,780 --> 00:18:19,970
examples in piece equal so the sequel

00:18:17,120 --> 00:18:22,130
client talks to the database server and

00:18:19,970 --> 00:18:24,620
then the interesting stuff begins on the

00:18:22,130 --> 00:18:26,690
master so you basically end up with a

00:18:24,620 --> 00:18:29,060
stored procedure that gets called on the

00:18:26,690 --> 00:18:30,710
master whole bunch of the same stored

00:18:29,060 --> 00:18:32,210
procedures get called on the segments

00:18:30,710 --> 00:18:34,010
you know then the aggregation happens if

00:18:32,210 --> 00:18:36,500
it needs to happen so it's kind of like

00:18:34,010 --> 00:18:41,990
fanning out and then a grinning there is

00:18:36,500 --> 00:18:43,700
a way for Madlib to also do coalescing

00:18:41,990 --> 00:18:46,130
of the data and basically around

00:18:43,700 --> 00:18:47,690
algorithm that need to converge so if

00:18:46,130 --> 00:18:49,130
the convergence needs to happen that the

00:18:47,690 --> 00:18:50,810
convergence would be happening through

00:18:49,130 --> 00:18:52,130
the master so that's the only bottleneck

00:18:50,810 --> 00:18:53,540
that you would have because you know the

00:18:52,130 --> 00:18:55,820
segment's will not be able to talk to

00:18:53,540 --> 00:18:57,320
each other but the convergence still

00:18:55,820 --> 00:19:00,710
could happen so if you're converging to

00:18:57,320 --> 00:19:03,410
a reasonably small value or small in

00:19:00,710 --> 00:19:04,670
size right in how you represent it then

00:19:03,410 --> 00:19:06,680
it works pretty well and that's how

00:19:04,670 --> 00:19:12,140
actually we do some of the graph

00:19:06,680 --> 00:19:13,280
algorithms within Madlib Madlib is

00:19:12,140 --> 00:19:13,730
interesting because it's not just

00:19:13,280 --> 00:19:15,800
greenplum

00:19:13,730 --> 00:19:17,540
we actually have people from the

00:19:15,800 --> 00:19:20,780
progress community who are using MATLAB

00:19:17,540 --> 00:19:25,370
to learn to do machine learning with in

00:19:20,780 --> 00:19:26,930
the progress installation itself so even

00:19:25,370 --> 00:19:28,190
if you're running a single node progress

00:19:26,930 --> 00:19:29,990
and you have a whole bunch of data

00:19:28,190 --> 00:19:31,970
locked into that single node progress

00:19:29,990 --> 00:19:33,740
you want to run you know simple linear

00:19:31,970 --> 00:19:35,270
regression or you want to run you know

00:19:33,740 --> 00:19:37,460
some graph algorithms that we're

00:19:35,270 --> 00:19:41,120
developing right now you can actually do

00:19:37,460 --> 00:19:43,040
it with MATLAB and just again for you to

00:19:41,120 --> 00:19:44,960
understand what's going on the pieces

00:19:43,040 --> 00:19:47,360
that you will be dealing with and

00:19:44,960 --> 00:19:49,970
basically at the very bottom you have a

00:19:47,360 --> 00:19:52,400
C API that progress exposes to anything

00:19:49,970 --> 00:19:54,020
that happens to be a function then you

00:19:52,400 --> 00:19:55,400
have a whole bunch of low-level

00:19:54,020 --> 00:19:57,170
abstractions you know that just

00:19:55,400 --> 00:19:59,000
mechanics of how Madlib itself is

00:19:57,170 --> 00:20:01,040
implemented and then a whole bunch of

00:19:59,000 --> 00:20:02,840
traditional machine learning functions

00:20:01,040 --> 00:20:03,830
that been implemented you know on top of

00:20:02,840 --> 00:20:05,330
these abstractions

00:20:03,830 --> 00:20:06,740
but essentially the idea is simple you

00:20:05,330 --> 00:20:09,980
know the poker segment gives you the

00:20:06,740 --> 00:20:11,450
data in form of tuples right you know

00:20:09,980 --> 00:20:13,160
think of it as an array of data that you

00:20:11,450 --> 00:20:14,840
can read through and then these

00:20:13,160 --> 00:20:16,910
functions basically do all of the work

00:20:14,840 --> 00:20:19,100
and if there's anything that needs to

00:20:16,910 --> 00:20:21,320
happen between different functions sort

00:20:19,100 --> 00:20:22,550
of running on different segments then

00:20:21,320 --> 00:20:25,130
that would happen through the

00:20:22,550 --> 00:20:26,840
abstractions that are provided here here

00:20:25,130 --> 00:20:28,460
you know you basically have a whole

00:20:26,840 --> 00:20:30,200
bunch of libraries that we've written

00:20:28,460 --> 00:20:33,350
for Madlib so python is one of them

00:20:30,200 --> 00:20:35,030
there's a integration between MATLAB and

00:20:33,350 --> 00:20:36,800
Python so you can actually do a lot of

00:20:35,030 --> 00:20:38,210
traditional machine learning you know

00:20:36,800 --> 00:20:40,490
talking to the back end which is a

00:20:38,210 --> 00:20:42,470
cluster another example of this layer is

00:20:40,490 --> 00:20:44,630
R so a lot of times we would actually

00:20:42,470 --> 00:20:46,700
have people who would call Madlib

00:20:44,630 --> 00:20:48,290
functions through the our interface

00:20:46,700 --> 00:20:50,480
because we have that integration

00:20:48,290 --> 00:20:52,010
provided to them where a Madlib function

00:20:50,480 --> 00:20:54,380
would actually look like a R function

00:20:52,010 --> 00:20:56,720
now of course again anything at this

00:20:54,380 --> 00:20:58,550
level you don't want a lot of data to

00:20:56,720 --> 00:21:00,530
being transferred back and forth so you

00:20:58,550 --> 00:21:03,350
basically just programming a cluster and

00:21:00,530 --> 00:21:05,180
you're telling the cluster what to do so

00:21:03,350 --> 00:21:06,440
that's Madlib it's pretty useful piece

00:21:05,180 --> 00:21:08,570
you know standalone piece it's actually

00:21:06,440 --> 00:21:09,800
an app a trapeze right now so it got

00:21:08,570 --> 00:21:12,500
transferred into the Apache Software

00:21:09,800 --> 00:21:13,910
Foundation it's an incubating project if

00:21:12,500 --> 00:21:16,220
you're interested you know we're more

00:21:13,910 --> 00:21:17,540
than welcome or you as a contributor you

00:21:16,220 --> 00:21:18,770
know there's quite a lot of interesting

00:21:17,540 --> 00:21:21,290
stuff to do

00:21:18,770 --> 00:21:22,940
greenplum itself is not an Apache

00:21:21,290 --> 00:21:27,880
project that's under the Apache License

00:21:22,940 --> 00:21:27,880
but it's a standalone project go to

00:21:31,180 --> 00:21:52,280
www.howtogetthejobideserve.com and then

00:21:50,660 --> 00:21:54,080
Debian was the first distribution that

00:21:52,280 --> 00:21:55,190
kind of like put it all together and

00:21:54,080 --> 00:21:57,050
then a whole bunch of secondary

00:21:55,190 --> 00:21:58,910
distributions you know got created based

00:21:57,050 --> 00:22:01,700
on Debian so Big Top is trying to do

00:21:58,910 --> 00:22:03,470
that with big data so you know we have

00:22:01,700 --> 00:22:05,210
Hadoop ecosystem but not just Hadoop

00:22:03,470 --> 00:22:07,280
ecosystem now we also have you know

00:22:05,210 --> 00:22:10,040
things like our green palm which I was

00:22:07,280 --> 00:22:11,840
talking about today there is big top and

00:22:10,040 --> 00:22:13,790
then a whole bunch of distributions that

00:22:11,840 --> 00:22:16,649
use big top to create products that they

00:22:13,790 --> 00:22:20,200
give to their customers

00:22:16,649 --> 00:22:22,149
so far we've laid the groundwork in the

00:22:20,200 --> 00:22:24,970
big-top community to integrate greenplum

00:22:22,149 --> 00:22:26,649
into the Hadoop ecosystem to allow you

00:22:24,970 --> 00:22:29,590
the kind of architectures I was talking

00:22:26,649 --> 00:22:31,690
about today so the basic functionality

00:22:29,590 --> 00:22:33,759
packaging deployment and docker

00:22:31,690 --> 00:22:35,889
orchestration is there so you can

00:22:33,759 --> 00:22:36,999
actually give the RPM and DB n packages

00:22:35,889 --> 00:22:38,799
for greenplum

00:22:36,999 --> 00:22:40,570
you know from Big Top community you can

00:22:38,799 --> 00:22:43,570
deploy them on your cluster using puppet

00:22:40,570 --> 00:22:45,039
code that is provided there are docker

00:22:43,570 --> 00:22:46,690
containers you know for that stuff as

00:22:45,039 --> 00:22:49,139
well there's basic integration with

00:22:46,690 --> 00:22:52,629
MATLAB so christian who is sitting here

00:22:49,139 --> 00:22:54,519
did a demo i think it lost for them

00:22:52,629 --> 00:22:56,169
maybe or some other conference where we

00:22:54,519 --> 00:22:59,049
basically demonstrated how you can use

00:22:56,169 --> 00:23:00,940
mad libs through apache Zeppelin which

00:22:59,049 --> 00:23:02,470
is a really nice tool similar to Jupiter

00:23:00,940 --> 00:23:04,929
you know from the Python community

00:23:02,470 --> 00:23:06,369
that's very well known in the Apache Big

00:23:04,929 --> 00:23:07,720
Data ecosystem you know if you're doing

00:23:06,369 --> 00:23:10,269
machine learning and you're trying to do

00:23:07,720 --> 00:23:13,149
this you know notebooks right for data

00:23:10,269 --> 00:23:14,619
scientists pretty useful tool we're

00:23:13,149 --> 00:23:16,179
actually interested in you know some of

00:23:14,619 --> 00:23:17,739
the juju deployment throw the basic

00:23:16,179 --> 00:23:18,970
rudimentary capabilities are there but

00:23:17,739 --> 00:23:21,669
if you're interested you know talk to me

00:23:18,970 --> 00:23:24,039
after this presentation and the HDFS

00:23:21,669 --> 00:23:26,109
integration is there you can get data in

00:23:24,039 --> 00:23:27,249
and out of HDFS as external tables but

00:23:26,109 --> 00:23:29,289
it's slow and we're trying to optimize

00:23:27,249 --> 00:23:32,619
it now so again talk to me after the

00:23:29,289 --> 00:23:35,879
presentation here's the stuff that were

00:23:32,619 --> 00:23:38,100
interested in doing and if you want

00:23:35,879 --> 00:23:41,320
additional items on this list

00:23:38,100 --> 00:23:43,059
be BRB be our guest I mean it's

00:23:41,320 --> 00:23:46,809
basically as much as you know you could

00:23:43,059 --> 00:23:51,309
you could possibly think of I'm

00:23:46,809 --> 00:23:55,179
particularly interested in this one and

00:23:51,309 --> 00:23:57,220
this this one so pause verse 9 is one of

00:23:55,179 --> 00:23:58,299
the big deals that we have to kind of

00:23:57,220 --> 00:23:59,679
like accomplish I don't think will

00:23:58,299 --> 00:24:02,200
accomplish it the same way we did it

00:23:59,679 --> 00:24:03,759
with progress eight which you know Heike

00:24:02,200 --> 00:24:06,159
for those of you who know him you know

00:24:03,759 --> 00:24:08,739
one of the sort of core progress guys

00:24:06,159 --> 00:24:10,389
kind of like just did this huge rebase

00:24:08,739 --> 00:24:12,519
of the entire code based on progress aid

00:24:10,389 --> 00:24:14,950
progress 9 is too big of a chance for us

00:24:12,519 --> 00:24:16,989
to bite that way so we'll probably just

00:24:14,950 --> 00:24:19,299
back port features from progress 9 you

00:24:16,989 --> 00:24:20,830
know feature at the time and that's

00:24:19,299 --> 00:24:23,379
actually where your feedback would be

00:24:20,830 --> 00:24:24,909
extremely useful because we need to know

00:24:23,379 --> 00:24:26,470
which features to prioritize you know

00:24:24,909 --> 00:24:28,239
for backporting so for example we know

00:24:26,470 --> 00:24:30,160
that binary JSON is super interesting

00:24:28,239 --> 00:24:31,330
right and the only reason we know is

00:24:30,160 --> 00:24:36,340
because you know people talked to us and

00:24:31,330 --> 00:24:38,410
told us so and this one is where I think

00:24:36,340 --> 00:24:40,000
the real interesting integration between

00:24:38,410 --> 00:24:43,570
Hadoop ecosystem and greenplum will

00:24:40,000 --> 00:24:45,430
begin so we want to make it a full sort

00:24:43,570 --> 00:24:47,200
of fledged member of the Cadoo PK system

00:24:45,430 --> 00:24:49,240
so all of the tools that exist within

00:24:47,200 --> 00:24:52,870
that ecosystem can actually benefit from

00:24:49,240 --> 00:24:54,580
green plum and vice versa so that's it

00:24:52,870 --> 00:24:56,500
that's that's all I have you know let's

00:24:54,580 --> 00:24:58,660
try to build it together if it sounds

00:24:56,500 --> 00:25:01,090
interesting at all to you and I'll leave

00:24:58,660 --> 00:25:14,920
you with this quote and open up for

00:25:01,090 --> 00:25:19,420
questions could you speak up I cannot

00:25:14,920 --> 00:25:23,500
really so that's that's what I'm saying

00:25:19,420 --> 00:25:25,120
we basically have to rely on that ETL

00:25:23,500 --> 00:25:30,970
that was in the middle so remember I was

00:25:25,120 --> 00:25:32,410
showing you the architecture we do but

00:25:30,970 --> 00:25:37,300
in a different way so the question is

00:25:32,410 --> 00:25:38,980
how do we deal with unstructured data so

00:25:37,300 --> 00:25:40,300
these are the architectures that I'm

00:25:38,980 --> 00:25:43,000
seeing so the way you deal with

00:25:40,300 --> 00:25:46,060
unstructured data is either here so your

00:25:43,000 --> 00:25:49,180
ETL basically puts some structure on

00:25:46,060 --> 00:25:51,310
that data right and that is when you

00:25:49,180 --> 00:25:53,290
know how to do it so you know that

00:25:51,310 --> 00:25:55,690
certain you know fields can be extracted

00:25:53,290 --> 00:25:57,550
in flight right away so then you extract

00:25:55,690 --> 00:25:59,260
those fields and you put it in directly

00:25:57,550 --> 00:26:02,080
into green plant database and you know

00:25:59,260 --> 00:26:03,910
in form of tables that's fine another

00:26:02,080 --> 00:26:07,090
way is you have your unstructured data

00:26:03,910 --> 00:26:08,290
at the HDFS level and these guys you

00:26:07,090 --> 00:26:11,080
know they're basically building

00:26:08,290 --> 00:26:14,260
exploratory data models right the data

00:26:11,080 --> 00:26:16,930
models that are constantly being tweaked

00:26:14,260 --> 00:26:19,120
but then they export it and make them

00:26:16,930 --> 00:26:21,010
available to a bigger audience within

00:26:19,120 --> 00:26:23,530
the enterprise by essentially again

00:26:21,010 --> 00:26:26,560
syncing up those data models as tables

00:26:23,530 --> 00:26:28,000
at the level of greenplum database so

00:26:26,560 --> 00:26:29,650
that then you can actually pick them up

00:26:28,000 --> 00:26:31,030
you know through MATLAB and you know all

00:26:29,650 --> 00:26:33,780
these other tools so that's how we deal

00:26:31,030 --> 00:26:33,780
with instruction later

00:26:35,490 --> 00:26:39,910
right and pxf you know one of the

00:26:37,750 --> 00:26:41,680
features that I was talking about

00:26:39,910 --> 00:26:43,450
integration wise is again in the

00:26:41,680 --> 00:26:45,940
direction of helping us deal with the

00:26:43,450 --> 00:26:52,690
unstructured data as quickly as possible

00:26:45,940 --> 00:26:54,880
so to speak yes right so there is a

00:26:52,690 --> 00:26:58,120
support so again I obviously couldn't go

00:26:54,880 --> 00:27:00,430
into great details about the

00:26:58,120 --> 00:27:02,050
architecture of the green plum but the

00:27:00,430 --> 00:27:05,140
question is do we support failover yes

00:27:02,050 --> 00:27:06,370
we do so you can have basically two

00:27:05,140 --> 00:27:12,160
masters you know in different

00:27:06,370 --> 00:27:14,050
configurations and yes you would

00:27:12,160 --> 00:27:17,680
basically have to note it's it's similar

00:27:14,050 --> 00:27:22,270
to how HDFS does AJ if you know that you

00:27:17,680 --> 00:27:23,470
know no it isn't right correct

00:27:22,270 --> 00:27:25,390
so again like I'm saying it's kind of

00:27:23,470 --> 00:27:27,940
like with HDFS where you have two named

00:27:25,390 --> 00:27:36,220
nodes you know that's the same approach

00:27:27,940 --> 00:27:39,490
yes so green plum is similar to situs DB

00:27:36,220 --> 00:27:43,060
so green plum and Silas take slightly

00:27:39,490 --> 00:27:46,140
different approaches to how to deal with

00:27:43,060 --> 00:27:48,760
progress and you get different sort of

00:27:46,140 --> 00:27:51,420
design constraints based on that so

00:27:48,760 --> 00:27:54,220
scientists made a decision to be

00:27:51,420 --> 00:27:56,830
essentially a plugin into a Postgres

00:27:54,220 --> 00:27:58,960
which makes it super easy to instantiate

00:27:56,830 --> 00:28:01,030
the situs cluster you basically just

00:27:58,960 --> 00:28:02,440
enable a plug-in on a bunch of progress

00:28:01,030 --> 00:28:06,310
nodes and you know you get a situs

00:28:02,440 --> 00:28:08,320
cluster but then on the flip side you're

00:28:06,310 --> 00:28:10,330
being constrained by whatever it is that

00:28:08,320 --> 00:28:12,250
is given to you by Postgres so for

00:28:10,330 --> 00:28:14,860
example green plum actually invests a

00:28:12,250 --> 00:28:17,380
lot in optimizing the inter interconnect

00:28:14,860 --> 00:28:19,480
ciders doesn't really have an ability to

00:28:17,380 --> 00:28:21,700
do those types of optimizations so it is

00:28:19,480 --> 00:28:23,710
similar but performance wise weight

00:28:21,700 --> 00:28:25,810
typically faster on this sort of the

00:28:23,710 --> 00:28:27,190
benchmarks that I've seen it doesn't

00:28:25,810 --> 00:28:30,460
mean that we're always faster because

00:28:27,190 --> 00:28:33,940
again depends on the workload but we do

00:28:30,460 --> 00:28:35,890
get more ability to optimize because we

00:28:33,940 --> 00:28:37,330
don't depend on Postgres as much as itis

00:28:35,890 --> 00:28:39,690
does so that would be the quickest way

00:28:37,330 --> 00:28:39,690
to answer

00:28:44,250 --> 00:28:53,089

YouTube URL: https://www.youtube.com/watch?v=6BFzW0RK0co


