Title: Using BigBench to compare Hive and Spark versions and features BigBench in Hive and Spark
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Nicolas Poggi and Alejandro Montero

At: FOSDEM 2017

BigBench is the brand new standard for benchmarking and testing Big Datasystems. This talk first introduces BigBench and how problems can it solve.Then, presents both Hive and Spark benchmark results with with theirrespective 1 and 2 versions under different configurations. Results arefurther classified by use cases, showing where each platform shines (ordoesn't), and why, based on performance metrics and log-file analysis.

BigBench is the brand new standard (TPCx-BB) for benchmarking and testing BigData systems. The BigBench specification describes several application usecases combining the need for SQL queries, Map/Reduce, user code (UDF), MachineLearning, and even streaming. From the available implementation, we can testthe different framework combinations such as Hadoop+Hive+Tez (with Mahout) andSpark (SparkSQL+SparkML) in their different versions and configurations,helping us to spot problems and possible optimizations of our data stacks.

This talk first introduces BigBench and how problems can it solve. Then,presents both Hive and Spark benchmark results with with their respective 1and 2 versions under different configurations including: Tez, LLAP, and fileformats. Experiments are run on Cloud and On-Prem clusters of differentnumbers of nodes and testing data scales, taking into account interactive andbatch usage. Results are further classified by use cases, showing where eachplatform shines (or doesn't), and why, based on performance metrics and log-file analysis. The talk concludes with the main findings, the scalability andlimits of each framework.


Room: H.2213
Scheduled start: 2017-02-04 13:35:00
Captions: 
	00:00:06,370 --> 00:00:11,510
all right so continuing on our spark

00:00:08,840 --> 00:00:14,299
theme next stop is Nicholas and

00:00:11,510 --> 00:00:16,250
Alexandra who actually was taking us

00:00:14,299 --> 00:00:29,750
through a cage match between spark and

00:00:16,250 --> 00:00:34,660
pipe so take it away guys yes but you

00:00:29,750 --> 00:00:34,660
also need to take for the recording

00:00:41,739 --> 00:00:47,079
ah all right first of all thank you very

00:00:45,040 --> 00:00:49,510
much for attending this line in talk I'm

00:00:47,079 --> 00:00:51,550
Alejandra Montero master student the

00:00:49,510 --> 00:00:52,960
personal supercomputing Center and in

00:00:51,550 --> 00:00:54,760
the last couple of months we've been

00:00:52,960 --> 00:00:56,320
working with big bench to compare the

00:00:54,760 --> 00:00:58,960
performance of different big data

00:00:56,320 --> 00:01:02,170
engines specifically we're focusing on -

00:00:58,960 --> 00:01:04,269
spark so big bench is an ax

00:01:02,170 --> 00:01:06,399
specification based benchmark with an

00:01:04,269 --> 00:01:09,190
open source implementation and recently

00:01:06,399 --> 00:01:11,710
it's been proposed to be the first big

00:01:09,190 --> 00:01:14,550
data benchmark on the world and that's

00:01:11,710 --> 00:01:16,679
because right now it's the only one that

00:01:14,550 --> 00:01:19,179
covers all measure big data

00:01:16,679 --> 00:01:22,270
characteristics and as you may know they

00:01:19,179 --> 00:01:24,640
are the volume variety and velocity some

00:01:22,270 --> 00:01:27,130
very quick characteristics of big bench

00:01:24,640 --> 00:01:30,640
it's an extension of TCP des that some

00:01:27,130 --> 00:01:32,200
new SQL queries and use cases as your

00:01:30,640 --> 00:01:35,170
machine learning natural language

00:01:32,200 --> 00:01:37,390
processing some others it comes from

00:01:35,170 --> 00:01:40,690
multiple implementations and genes and

00:01:37,390 --> 00:01:42,759
table formats for hive it can also

00:01:40,690 --> 00:01:44,890
execute multiple parallel streams at the

00:01:42,759 --> 00:01:46,990
same time in the same cluster and can

00:01:44,890 --> 00:01:48,820
define different scale factors we used

00:01:46,990 --> 00:01:53,320
scale factor of 100 which is

00:01:48,820 --> 00:01:57,060
approximately 100 gigs internal size so

00:01:53,320 --> 00:01:59,619
big bench what it's doing it's emulating

00:01:57,060 --> 00:02:02,740
store that it's selling items both

00:01:59,619 --> 00:02:06,219
physically and unbe a web page and for

00:02:02,740 --> 00:02:09,940
that reason it provides these these data

00:02:06,219 --> 00:02:11,380
structure first we have the structured

00:02:09,940 --> 00:02:15,040
data which is the one we're used to

00:02:11,380 --> 00:02:17,590
easily index and recoverable and we add

00:02:15,040 --> 00:02:19,420
two more which is the web block which

00:02:17,590 --> 00:02:20,799
contains a click streams of every user

00:02:19,420 --> 00:02:24,250
that it's navigating through the web

00:02:20,799 --> 00:02:26,109
page and the reviews of the users that

00:02:24,250 --> 00:02:29,350
actually have bought an item and want to

00:02:26,109 --> 00:02:31,060
make a review so for the workload itself

00:02:29,350 --> 00:02:34,390
there's three queries divided in four

00:02:31,060 --> 00:02:36,160
kind of use cases 14 pure pure coil

00:02:34,390 --> 00:02:38,799
queries which retrieve information from

00:02:36,160 --> 00:02:41,170
the structure section then we have four

00:02:38,799 --> 00:02:43,329
queries of MapReduce pre-processing of

00:02:41,170 --> 00:02:45,010
the data before selecting it seven

00:02:43,329 --> 00:02:49,359
natural language processing queries and

00:02:45,010 --> 00:02:51,750
five machine learning queries so don't

00:02:49,359 --> 00:02:54,400
be overwhelmed by this this is the

00:02:51,750 --> 00:02:55,569
server stack of the implementation we've

00:02:54,400 --> 00:02:57,430
been using

00:02:55,569 --> 00:02:59,950
very fast starting from the bottom to

00:02:57,430 --> 00:03:03,099
top all files are physically stored in

00:02:59,950 --> 00:03:05,200
the in distributed file system but as

00:03:03,099 --> 00:03:08,530
we're running queries we need to have a

00:03:05,200 --> 00:03:12,010
middleware emitter store to store the

00:03:08,530 --> 00:03:14,200
logical tables and on top of that we

00:03:12,010 --> 00:03:16,329
need that SQL engine that is receiving

00:03:14,200 --> 00:03:18,609
the the credits from a big bench

00:03:16,329 --> 00:03:20,530
it's parsing them and it's retrieving

00:03:18,609 --> 00:03:24,069
the information from the meta store once

00:03:20,530 --> 00:03:25,870
we have the location of the of the of

00:03:24,069 --> 00:03:28,299
the files we want to recover we can use

00:03:25,870 --> 00:03:30,129
one of these three engines and one of

00:03:28,299 --> 00:03:31,569
the execution engines to actually

00:03:30,129 --> 00:03:34,150
retrieve the physical information from

00:03:31,569 --> 00:03:36,879
each DFS the engines can be classic

00:03:34,150 --> 00:03:38,889
MapReduce tests which maybe you don't

00:03:36,879 --> 00:03:41,079
know about it it's a hack on top of

00:03:38,889 --> 00:03:43,120
MapReduce to clear create a director

00:03:41,079 --> 00:03:45,159
city graph to reduce latencies and

00:03:43,120 --> 00:03:47,730
improve overall performance of of

00:03:45,159 --> 00:03:51,519
mappers and reducers and spark engine

00:03:47,730 --> 00:03:53,889
and for the machine learning queries we

00:03:51,519 --> 00:03:56,530
also need a new application to perform

00:03:53,889 --> 00:03:57,760
the DES learning techniques and we can

00:03:56,530 --> 00:04:00,040
use two applications

00:03:57,760 --> 00:04:02,919
mahout which is based on MapReduce or

00:04:00,040 --> 00:04:07,930
spark Mlle a custom-built spark Emily

00:04:02,919 --> 00:04:09,579
library so of course John is the one

00:04:07,930 --> 00:04:12,220
that's managing all the containers for

00:04:09,579 --> 00:04:14,889
every single application and here

00:04:12,220 --> 00:04:18,070
so we've benchmarked all permutations of

00:04:14,889 --> 00:04:20,620
the engines you see in in this slide but

00:04:18,070 --> 00:04:23,590
we still have a few working progress

00:04:20,620 --> 00:04:25,780
things and we have some results for hive

00:04:23,590 --> 00:04:28,150
two but the they are quite odd so we're

00:04:25,780 --> 00:04:30,760
still working on that and also for spark

00:04:28,150 --> 00:04:33,039
- it was compatible with mahout but the

00:04:30,760 --> 00:04:36,220
custom library was not binary compatible

00:04:33,039 --> 00:04:37,930
with a spark - so major code refractor

00:04:36,220 --> 00:04:41,830
is needed and we hope to get results

00:04:37,930 --> 00:04:44,229
pretty soon huh twice we're using an

00:04:41,830 --> 00:04:49,750
hdinsight platform as a service cluster

00:04:44,229 --> 00:04:52,449
model dd4b39 outs featuring a hike an

00:04:49,750 --> 00:04:55,120
intelligent CPU with eight cores twenty

00:04:52,449 --> 00:04:57,759
eight gigs of ram and the HDFS is

00:04:55,120 --> 00:05:00,009
completely remote for the software stack

00:04:57,759 --> 00:05:01,330
AGI is realizing on Hortonworks that a

00:05:00,009 --> 00:05:03,849
platform 2.5

00:05:01,330 --> 00:05:06,070
we've noticed that both MapReduce and

00:05:03,849 --> 00:05:09,310
des are really well tuned so we decided

00:05:06,070 --> 00:05:10,810
not to change a bit of the configuration

00:05:09,310 --> 00:05:13,450
what we did notice though is that

00:05:10,810 --> 00:05:17,050
sparked was recently added and the

00:05:13,450 --> 00:05:19,060
computations what strange it's only

00:05:17,050 --> 00:05:21,070
using one executed per walking node and

00:05:19,060 --> 00:05:23,950
that executor has three out of eight

00:05:21,070 --> 00:05:26,620
cores available in the machine so for

00:05:23,950 --> 00:05:29,770
the results we decided to divide it in

00:05:26,620 --> 00:05:31,780
use cases starting for pure quell as

00:05:29,770 --> 00:05:34,030
expected mapreduces this lowest one on

00:05:31,780 --> 00:05:36,070
the group followed by the fastest one

00:05:34,030 --> 00:05:37,900
which is spark two which is very close

00:05:36,070 --> 00:05:41,710
to the other engine spark one and five

00:05:37,900 --> 00:05:43,540
tests we wanted to see a little bit more

00:05:41,710 --> 00:05:47,110
what was happening inside a pure core

00:05:43,540 --> 00:05:48,520
query so this is a trace of the CPU

00:05:47,110 --> 00:05:52,360
behavior of one of the queries for a

00:05:48,520 --> 00:05:55,690
twelve to be correct consist what we can

00:05:52,360 --> 00:05:57,970
see here is that test it's reaching 100%

00:05:55,690 --> 00:06:02,440
of the CPU usage which indicates its CPU

00:05:57,970 --> 00:06:04,360
bounded also in this case you don't you

00:06:02,440 --> 00:06:07,900
cannot see the numbers because of

00:06:04,360 --> 00:06:11,170
resizing reasons well it's a lot faster

00:06:07,900 --> 00:06:13,840
than both the other engines test is

00:06:11,170 --> 00:06:16,510
finishing in 100 seconds sparks is

00:06:13,840 --> 00:06:20,080
finishing in 200 seconds and spark 2 in

00:06:16,510 --> 00:06:21,580
116 moving on we see that spark one

00:06:20,080 --> 00:06:25,750
spark to you both are reaching at the

00:06:21,580 --> 00:06:29,470
top of 30 percent of CPU usage sorry you

00:06:25,750 --> 00:06:32,110
can't see the y-axis most interestingly

00:06:29,470 --> 00:06:33,700
we can see that spark a spark one has a

00:06:32,110 --> 00:06:36,640
lot of fire wait for some reason and

00:06:33,700 --> 00:06:41,110
spark do deals with that it doesn't show

00:06:36,640 --> 00:06:44,290
any more I wait and it ends lot faster

00:06:41,110 --> 00:06:46,450
in this case also it is using only 30%

00:06:44,290 --> 00:06:48,310
of the of the CPU and that may be

00:06:46,450 --> 00:06:53,320
because of the software configuration I

00:06:48,310 --> 00:06:55,210
just talked about you for very fast for

00:06:53,320 --> 00:06:57,250
the second use case custom reducers to

00:06:55,210 --> 00:06:59,500
pre-process data before selecting it we

00:06:57,250 --> 00:07:02,380
see that high test is the fastest one

00:06:59,500 --> 00:07:04,270
here followed by spark 2 and very close

00:07:02,380 --> 00:07:07,810
to spark one my previews again is the

00:07:04,270 --> 00:07:10,360
slowest one Mui GaN natural language

00:07:07,810 --> 00:07:12,960
processing here tests once again it's

00:07:10,360 --> 00:07:16,420
the winner by a longshot in these case

00:07:12,960 --> 00:07:18,130
followed by spark to spark one really

00:07:16,420 --> 00:07:20,599
close to spark 2 and my previews is

00:07:18,130 --> 00:07:22,369
really really slow

00:07:20,599 --> 00:07:24,740
and finally for the machine learning

00:07:22,369 --> 00:07:26,270
sections we can see two interesting

00:07:24,740 --> 00:07:28,669
things here first of all is that

00:07:26,270 --> 00:07:30,740
changing one execution engine for the

00:07:28,669 --> 00:07:33,649
other doesn't bring as any real

00:07:30,740 --> 00:07:35,300
difference in performance but what does

00:07:33,649 --> 00:07:37,520
givers are different in performance is

00:07:35,300 --> 00:07:38,679
changing the application that actually

00:07:37,520 --> 00:07:41,509
performs the machine learning and

00:07:38,679 --> 00:07:44,419
changing from mahout in n is any of the

00:07:41,509 --> 00:07:47,199
of the engines to spark ml Alip give us

00:07:44,419 --> 00:07:50,029
a two times improvement in performance

00:07:47,199 --> 00:07:52,729
as I said before unfortunately we were

00:07:50,029 --> 00:07:55,369
not able to test spark to with Sparky

00:07:52,729 --> 00:07:57,020
Mela leap but we're hoping to see two

00:07:55,369 --> 00:08:00,139
times improvement and well as in the

00:07:57,020 --> 00:08:02,080
other cases and finally for the

00:08:00,139 --> 00:08:04,849
aggregated results for the for use cases

00:08:02,080 --> 00:08:07,520
what we can see is that for the whole

00:08:04,849 --> 00:08:11,899
group the fastest one here is tests plus

00:08:07,520 --> 00:08:13,759
a spark Mlle second in line is a spark

00:08:11,899 --> 00:08:17,599
one with spark Emily leap followed by

00:08:13,759 --> 00:08:19,909
spark two but plasma mahout we're hoping

00:08:17,599 --> 00:08:22,639
to see spark two plus spark ml a leap to

00:08:19,909 --> 00:08:24,559
be lot faster when we have results but

00:08:22,639 --> 00:08:26,990
right now it's on the third position and

00:08:24,559 --> 00:08:31,999
mapreduces this law was one on the on

00:08:26,990 --> 00:08:33,979
the group so just to finish some

00:08:31,999 --> 00:08:38,079
conclusions we can gather from from

00:08:33,979 --> 00:08:40,310
these results fist of five plus tests is

00:08:38,079 --> 00:08:40,729
improving the SQL performance by a long

00:08:40,310 --> 00:08:44,600
shot

00:08:40,729 --> 00:08:47,149
by over MapReduce it's slightly faster

00:08:44,600 --> 00:08:51,230
than spark one but it is lightly it's

00:08:47,149 --> 00:08:53,810
lower than spark two and we have to make

00:08:51,230 --> 00:08:55,880
clear something in in that point in this

00:08:53,810 --> 00:08:57,889
point is that the implementation spark

00:08:55,880 --> 00:09:00,319
of the queries I mean the implementation

00:08:57,889 --> 00:09:02,600
spark is using it the same as they have

00:09:00,319 --> 00:09:04,880
one so they are using the same the very

00:09:02,600 --> 00:09:06,740
same SQL queries and in this

00:09:04,880 --> 00:09:10,519
implementation these words are very

00:09:06,740 --> 00:09:13,180
optimized for hive so maybe tweaking for

00:09:10,519 --> 00:09:15,800
spark may give us out different results

00:09:13,180 --> 00:09:18,050
second conclusion we got from these

00:09:15,800 --> 00:09:20,660
these studies that spark amela leap is

00:09:18,050 --> 00:09:23,360
way faster than Mahad we encourage you

00:09:20,660 --> 00:09:26,329
to use it instead of it and finally the

00:09:23,360 --> 00:09:28,939
best production company at the moment is

00:09:26,329 --> 00:09:30,850
using apache tests for the SQL sections

00:09:28,939 --> 00:09:33,030
of your queries and if you need to do a

00:09:30,850 --> 00:09:35,550
machine learning technique

00:09:33,030 --> 00:09:39,210
stick with Emily which is the fastest

00:09:35,550 --> 00:09:41,190
one so before finishing I encourage you

00:09:39,210 --> 00:09:44,430
to assist tomorrow to our presentation

00:09:41,190 --> 00:09:47,220
my colleague Nico is doing using for

00:09:44,430 --> 00:09:50,190
using a non-volatile memory to improve

00:09:47,220 --> 00:09:52,400
performance of HBase Hadoop and and all

00:09:50,190 --> 00:09:56,610
this stuff quite interesting building

00:09:52,400 --> 00:09:59,820
you at 12 o'clock so encourage you to

00:09:56,610 --> 00:10:00,330
attend and that would be all thank you

00:09:59,820 --> 00:10:14,880
very much

00:10:00,330 --> 00:10:17,900
we've had any question all right well

00:10:14,880 --> 00:10:17,900

YouTube URL: https://www.youtube.com/watch?v=F4z74sl3Qds


