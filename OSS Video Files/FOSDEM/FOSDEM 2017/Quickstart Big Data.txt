Title: Quickstart Big Data
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Olaf Flebbe

At: FOSDEM 2017

How to scale out an engineering workload in HPC with Apache Bigtop.

The Speaker will present how to setup up Big Data scenarios using the toolscontained within the Apache Bigtop project. Apache Bigtop is a framework tocompile, package, test and deploy Big Data components supporting all majorLinux distros. The speaker will demonstrate how to install and configure asingle node or deploy to a cluster using the supplied package repositories andpuppet manifests. As an example usecase he will demonstrate how to run ApacheSpark to scale out a python engineering workload from single node to a HPCenvironment.


Room: H.2213
Scheduled start: 2017-02-04 13:10:00
Captions: 
	00:00:00,290 --> 00:00:03,410
thank you

00:00:05,130 --> 00:00:11,370
for letting me into this room it's very

00:00:07,439 --> 00:00:15,960
crowded outside it's a great honor to be

00:00:11,370 --> 00:00:20,130
sent inside so I will talk about a

00:00:15,960 --> 00:00:22,350
consulting job I did first about some

00:00:20,130 --> 00:00:25,130
words about me I'm indeed my PhD and

00:00:22,350 --> 00:00:28,560
computational physics a long time ago

00:00:25,130 --> 00:00:33,150
was involved in projects like MINIX for

00:00:28,560 --> 00:00:37,100
68k the mathematics library for linux i

00:00:33,150 --> 00:00:41,160
did the high-precision mass Paul Patton

00:00:37,100 --> 00:00:43,260
reporting to the prior 5 flight years if

00:00:41,160 --> 00:00:45,870
flight simulator had some computer

00:00:43,260 --> 00:00:48,899
contributions and I'm a maintainer for

00:00:45,870 --> 00:00:53,460
the MS KT util it's accepted actually

00:00:48,899 --> 00:00:55,949
cobbles integration - right now for the

00:00:53,460 --> 00:00:59,129
moment I am the pn pn c of the apache

00:00:55,949 --> 00:01:01,770
victor project more about that later in

00:00:59,129 --> 00:01:04,220
my real life the otherworld life I'm a

00:01:01,770 --> 00:01:09,330
software he take for connected ebikes

00:01:04,220 --> 00:01:14,070
and the thing I was I'm talking about

00:01:09,330 --> 00:01:17,729
this how to solve how to attack a

00:01:14,070 --> 00:01:19,530
problem when you do some have to do some

00:01:17,729 --> 00:01:23,220
consulting jobs and with a big tailor

00:01:19,530 --> 00:01:26,130
label on it and the problem I had to

00:01:23,220 --> 00:01:31,140
solve was from cam and fro metrics you

00:01:26,130 --> 00:01:35,070
have for the the development of medical

00:01:31,140 --> 00:01:39,689
drugs so the problem at hand is you have

00:01:35,070 --> 00:01:43,619
a database a large database of molecules

00:01:39,689 --> 00:01:47,070
which can be may produced quite cheaply

00:01:43,619 --> 00:01:51,540
for instance and you have this many

00:01:47,070 --> 00:01:53,220
medical database database of things is

00:01:51,540 --> 00:01:56,810
written down for instance in the

00:01:53,220 --> 00:02:01,439
smallest notation that's a complicated

00:01:56,810 --> 00:02:03,570
notation - that's the the chemical

00:02:01,439 --> 00:02:05,490
structure of this thing here I didn't

00:02:03,570 --> 00:02:08,070
remember the name of it just I just

00:02:05,490 --> 00:02:12,329
picked randomly one order out of it and

00:02:08,070 --> 00:02:16,020
the problem is to look for substructures

00:02:12,329 --> 00:02:17,600
into these molecules which have some

00:02:16,020 --> 00:02:20,250
healthy

00:02:17,600 --> 00:02:23,630
can make you healthy or something like

00:02:20,250 --> 00:02:26,370
that so there that's not a new problem

00:02:23,630 --> 00:02:30,450
there's a commercial solution for it you

00:02:26,370 --> 00:02:32,010
had me Hugh throw enterprise database in

00:02:30,450 --> 00:02:34,440
it and you have something like a

00:02:32,010 --> 00:02:38,300
cartridge in it and you can make a

00:02:34,440 --> 00:02:44,000
sequel query and you get results the

00:02:38,300 --> 00:02:47,700
problem is that is the duration is very

00:02:44,000 --> 00:02:50,220
long for instance it needs about a day

00:02:47,700 --> 00:02:54,900
or something it's not very reliable and

00:02:50,220 --> 00:02:57,780
it's very expensive so the customer

00:02:54,900 --> 00:03:00,210
looked out for a big data approach to

00:02:57,780 --> 00:03:03,660
this problem and fortunately there is an

00:03:00,210 --> 00:03:07,050
open source project card called Rd kit

00:03:03,660 --> 00:03:09,090
it's a beautiful library for chem

00:03:07,050 --> 00:03:11,760
informatics and you can do something

00:03:09,090 --> 00:03:14,790
like that you read in a molecule from

00:03:11,760 --> 00:03:16,020
smiles and you put in this mass

00:03:14,790 --> 00:03:18,420
notification and you know if your

00:03:16,020 --> 00:03:21,030
molecule object the person object and

00:03:18,420 --> 00:03:22,890
can say it simply say it has a

00:03:21,030 --> 00:03:25,200
substructure match from the other small

00:03:22,890 --> 00:03:32,010
notation so it gives you true or false

00:03:25,200 --> 00:03:35,820
that's fine and ok so this always the

00:03:32,010 --> 00:03:37,590
same thing we have the ingredients a

00:03:35,820 --> 00:03:40,200
time-consuming job

00:03:37,590 --> 00:03:42,900
we have a large data set and make it

00:03:40,200 --> 00:03:46,680
fast so and we have an eye on the

00:03:42,900 --> 00:03:49,770
environment we wanted to use is a big

00:03:46,680 --> 00:03:55,290
data cluster or an HPC cluster we have

00:03:49,770 --> 00:03:59,160
both so what we can be doing so now

00:03:55,290 --> 00:04:00,870
about talking how not to scale out you

00:03:59,160 --> 00:04:03,120
can simply benchmark it and you see that

00:04:00,870 --> 00:04:05,220
the reading in the small notation and

00:04:03,120 --> 00:04:08,870
constructing the molecule object is the

00:04:05,220 --> 00:04:14,370
most time-consuming thing in this whole

00:04:08,870 --> 00:04:20,310
problem and ok we can simply read it in

00:04:14,370 --> 00:04:22,710
one time and dumping the civilizing the

00:04:20,310 --> 00:04:25,800
molecules is called pickling in C

00:04:22,710 --> 00:04:28,470
important to him dumpling to a to file

00:04:25,800 --> 00:04:29,620
as though you do not have to reconstruct

00:04:28,470 --> 00:04:33,100
all the molecules

00:04:29,620 --> 00:04:36,699
anytime so that's a huge gain in run

00:04:33,100 --> 00:04:40,750
time but it does not scale anything it's

00:04:36,699 --> 00:04:44,020
we have we are looking out for scale so

00:04:40,750 --> 00:04:49,449
- in order to make the program faster so

00:04:44,020 --> 00:04:53,139
more machines in it so we simply looking

00:04:49,449 --> 00:04:56,110
at the problem it's simply an HP AEP

00:04:53,139 --> 00:04:59,169
problem it's embarrassingly parallel so

00:04:56,110 --> 00:05:02,080
we can simply divide the database into

00:04:59,169 --> 00:05:06,370
small chunks and so each chunks at

00:05:02,080 --> 00:05:08,860
different machines quite easy and the

00:05:06,370 --> 00:05:11,350
big data approach is to distribute the

00:05:08,860 --> 00:05:15,760
algorithm to the machines and not the

00:05:11,350 --> 00:05:18,210
data coming to one machine so yes and

00:05:15,760 --> 00:05:21,669
the beautiful a framework for SSS Park

00:05:18,210 --> 00:05:25,240
it's the sparkle no special ingredients

00:05:21,669 --> 00:05:27,060
needed and we you see the RDD power

00:05:25,240 --> 00:05:31,060
dignity this resident artists

00:05:27,060 --> 00:05:36,479
distribution ya think it's called that

00:05:31,060 --> 00:05:39,520
is that the files are hacked into

00:05:36,479 --> 00:05:41,849
different components so we can

00:05:39,520 --> 00:05:44,470
distribute it to all the machines and

00:05:41,849 --> 00:05:47,830
SPARC is beautiful because it runs or

00:05:44,470 --> 00:05:50,590
each on HPC and picked our environments

00:05:47,830 --> 00:05:56,169
it does not need it stuff is not tied to

00:05:50,590 --> 00:05:58,979
the HDFS for instance so all I did is

00:05:56,169 --> 00:06:03,820
read the instructions at spark I never

00:05:58,979 --> 00:06:10,870
haven't used spark before and it's quite

00:06:03,820 --> 00:06:12,870
easy we read in and a text file okay in

00:06:10,870 --> 00:06:16,360
fact I'm reading in the pickle for

00:06:12,870 --> 00:06:20,260
forget about it text file I have the

00:06:16,360 --> 00:06:22,360
input and I can this is a beauty

00:06:20,260 --> 00:06:25,240
algorithm that's the algorithm here the

00:06:22,360 --> 00:06:27,039
function to the cluster and here and do

00:06:25,240 --> 00:06:30,760
some aggregation in order to gets

00:06:27,039 --> 00:06:34,300
benchmarking results that's all all I

00:06:30,760 --> 00:06:37,270
have to do as its if I remember about

00:06:34,300 --> 00:06:40,700
construction in MPI job one thing what I

00:06:37,270 --> 00:06:43,820
have to do with all the initialization

00:06:40,700 --> 00:06:45,170
and it is a fail-safe think this is

00:06:43,820 --> 00:06:47,090
inherently failsafe

00:06:45,170 --> 00:06:49,880
because spark handles if you didn't

00:06:47,090 --> 00:06:52,070
notice crying or had misbehaving or

00:06:49,880 --> 00:06:56,960
something like that and only relies on

00:06:52,070 --> 00:06:59,390
that every note can read that file so if

00:06:56,960 --> 00:07:02,000
you have if I have some kind of shared

00:06:59,390 --> 00:07:03,280
file system I can distribute it and run

00:07:02,000 --> 00:07:07,310
it in the environment

00:07:03,280 --> 00:07:10,220
so if we see mode is quite so

00:07:07,310 --> 00:07:15,380
straightforward use a cluster for a

00:07:10,220 --> 00:07:18,080
filesystem dump the air the spark jar on

00:07:15,380 --> 00:07:21,470
it distributed data in the same

00:07:18,080 --> 00:07:24,710
directory and okay it does not use the

00:07:21,470 --> 00:07:29,060
locality of data store but this is CPU

00:07:24,710 --> 00:07:34,430
bound not data bound and we can use the

00:07:29,060 --> 00:07:37,450
standalone mode of spark and okay let's

00:07:34,430 --> 00:07:43,520
compare about with a big data set up

00:07:37,450 --> 00:07:46,300
okay I have to make some advertisement

00:07:43,520 --> 00:07:51,740
for the Apache Victoire distribution and

00:07:46,300 --> 00:07:54,740
only one minute left okay oh yes

00:07:51,740 --> 00:07:56,630
okay the apache big top is a debian of

00:07:54,740 --> 00:07:59,690
potatoes and solutions is who used by

00:07:56,630 --> 00:08:03,890
google cloud era canonical and the UDP i

00:07:59,690 --> 00:08:06,710
please turn around Roman and it contains

00:08:03,890 --> 00:08:09,380
all the usual stuff and it's important

00:08:06,710 --> 00:08:11,270
to have a comparable environment we have

00:08:09,380 --> 00:08:13,970
a repositories we have provisioning the

00:08:11,270 --> 00:08:15,590
Tucker compose for testing of course the

00:08:13,970 --> 00:08:19,310
OpenStack is broken oh sorry

00:08:15,590 --> 00:08:21,140
the deployment the templates based on

00:08:19,310 --> 00:08:23,720
puppet orchestration

00:08:21,140 --> 00:08:27,020
if one needs to like to have it through

00:08:23,720 --> 00:08:29,720
the charms that's the canonical thing we

00:08:27,020 --> 00:08:31,880
have an automotive auto Moloch testing

00:08:29,720 --> 00:08:36,590
environment and we have non Intel

00:08:31,880 --> 00:08:38,990
architectures and that's a glimpse for

00:08:36,590 --> 00:08:40,790
CI composites them that's the

00:08:38,990 --> 00:08:44,740
distributions the Linux distribution

00:08:40,790 --> 00:08:49,850
center has six seven six debian fedora

00:08:44,740 --> 00:08:53,870
fedora on PowerPC and Ubuntu on arm 64

00:08:49,850 --> 00:09:00,569
and that's like something like

00:08:53,870 --> 00:09:03,209
Hadoop she love hive and something like

00:09:00,569 --> 00:09:07,860
that that's all these big data ecosystem

00:09:03,209 --> 00:09:10,949
and it's simply a simply deploying it

00:09:07,860 --> 00:09:12,779
with the puppet scripts you see HDFS put

00:09:10,949 --> 00:09:16,709
the data into it use the yarn motor

00:09:12,779 --> 00:09:19,829
spark and unpack the spark to because we

00:09:16,709 --> 00:09:24,089
do not support it until now and run okay

00:09:19,829 --> 00:09:27,569
works preview of picked up 1.2 is we

00:09:24,089 --> 00:09:29,459
will have spark too but unfortunately

00:09:27,569 --> 00:09:31,439
it's not finished until now and we need

00:09:29,459 --> 00:09:37,529
more help please join us at picked up

00:09:31,439 --> 00:09:40,019
Apache or so let me conclude the problem

00:09:37,529 --> 00:09:43,079
runs much better on the HPC environment

00:09:40,019 --> 00:09:45,810
because it's compute intensive not data

00:09:43,079 --> 00:09:48,959
pipeline that's simply the cause the Big

00:09:45,810 --> 00:09:51,180
Data environment is optimized for soup

00:09:48,959 --> 00:09:53,339
route data to put not for HPC runtimes

00:09:51,180 --> 00:09:56,279
oh and the problem

00:09:53,339 --> 00:10:00,839
it scales really well it's over for over

00:09:56,279 --> 00:10:04,139
m but unfortunately t if we have that's

00:10:00,839 --> 00:10:06,959
the number of machines we put in and we

00:10:04,139 --> 00:10:09,720
have some runtime the total runtime I'm

00:10:06,959 --> 00:10:12,660
waiting for the job that's the

00:10:09,720 --> 00:10:17,899
commercial solution because it's a fixed

00:10:12,660 --> 00:10:24,420
environment and our n is and that one is

00:10:17,899 --> 00:10:28,050
too much for the customer to pay so we

00:10:24,420 --> 00:10:32,939
will have to investing some how someone

00:10:28,050 --> 00:10:37,399
has to investigate how wealthy where we

00:10:32,939 --> 00:10:37,399
can speed up it much better thanks

00:10:41,439 --> 00:10:44,859

YouTube URL: https://www.youtube.com/watch?v=Sbe2Zi8WtfE


