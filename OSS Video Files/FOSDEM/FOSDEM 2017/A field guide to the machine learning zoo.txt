Title: A field guide to the machine learning zoo
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Theodore Vasiloudis

At: FOSDEM 2017

As machine learning (ML) finds its way into more and more areas in our life,software developers from all fields are asked to navigate an increasinglycomplex maze of tools and algorithms to extract value out of massive datasets.In this talk we'll try to help the aspiring ML developer by describing:

  * a conceptual framework that most ML algorithms fall under  * considerations about data readiness, algorithms, and software tools from an open-source perspective  * some common mistakes and misconceptions in the development and deployment of ML systems

The goal of the talk is to aid the audience to think about ML problems in anintegrated manner; facilitating the process of going from problem toprototype, making an informed choice about the algorithms and software to use,and providing examples of issues that can, and do come up in production.

The talk is designed to be informative and entertaining, with little previousknowledge required.


Room: H.2213
Scheduled start: 2017-02-04 14:00:00
Captions: 
	00:00:04,590 --> 00:00:10,920
our next speaker is Theodore who will be

00:00:08,250 --> 00:00:12,959
guiding us through all the projects and

00:00:10,920 --> 00:00:17,430
things that you might need to know about

00:00:12,959 --> 00:00:19,320
if you're doing machinery thank you so

00:00:17,430 --> 00:00:21,240
my name is Theodore I'm a researcher at

00:00:19,320 --> 00:00:23,009
the Swedish astute of computer science I

00:00:21,240 --> 00:00:25,560
work on large K machine learning systems

00:00:23,009 --> 00:00:27,660
and the goal for today's presentations

00:00:25,560 --> 00:00:29,939
is to just put out some guidelines for

00:00:27,660 --> 00:00:31,890
people looking into putting their first

00:00:29,939 --> 00:00:34,350
machine learning project into production

00:00:31,890 --> 00:00:35,790
so the idea is that the you already have

00:00:34,350 --> 00:00:37,530
a product you have some data gathered

00:00:35,790 --> 00:00:38,850
and you would like to use a machine

00:00:37,530 --> 00:00:40,920
learning in order to improve some parts

00:00:38,850 --> 00:00:42,660
of it and one thing that I should

00:00:40,920 --> 00:00:43,770
clarify from the beginning that almost

00:00:42,660 --> 00:00:45,540
all of the things that I will present

00:00:43,770 --> 00:00:47,340
here was not written by me but by other

00:00:45,540 --> 00:00:48,570
much more experienced people and what

00:00:47,340 --> 00:00:50,010
I've done here is that just I've

00:00:48,570 --> 00:00:51,210
gathered all them all together and put

00:00:50,010 --> 00:00:53,550
them in context in order to make them

00:00:51,210 --> 00:00:54,720
more useful and all the sources that I

00:00:53,550 --> 00:00:58,110
have used are of course included that

00:00:54,720 --> 00:01:00,180
they at the end of the presentation so

00:00:58,110 --> 00:01:02,690
we'll just jump straight ahead into the

00:01:00,180 --> 00:01:04,890
content and we see how we move

00:01:02,690 --> 00:01:06,509
essentially from an idea into something

00:01:04,890 --> 00:01:08,909
that we can actually put in production

00:01:06,509 --> 00:01:11,939
machine learning and what I'll try to do

00:01:08,909 --> 00:01:13,649
is present a simple framework that we

00:01:11,939 --> 00:01:16,439
can use to think about most machine

00:01:13,649 --> 00:01:17,820
learning problems and one thing that I

00:01:16,439 --> 00:01:20,219
found very useful definitely as a

00:01:17,820 --> 00:01:22,740
programmer is to create interfaces for

00:01:20,219 --> 00:01:25,590
things so what we do is we find common

00:01:22,740 --> 00:01:28,829
things that all our problems have in

00:01:25,590 --> 00:01:30,149
common and then we use this that

00:01:28,829 --> 00:01:32,939
language in order to describe what we

00:01:30,149 --> 00:01:34,259
want to do so then we can do the same

00:01:32,939 --> 00:01:35,759
actually for machine learning problems

00:01:34,259 --> 00:01:36,659
and what we can try to figure out what

00:01:35,759 --> 00:01:39,630
are the things that machine learning

00:01:36,659 --> 00:01:41,429
problems also have in common so the

00:01:39,630 --> 00:01:42,689
first thing that we have in common is

00:01:41,429 --> 00:01:44,429
definitely the reason that we're using

00:01:42,689 --> 00:01:46,499
machine learning in the first place is

00:01:44,429 --> 00:01:48,509
that we want to describe the behavior of

00:01:46,499 --> 00:01:50,219
a very complex system and because the

00:01:48,509 --> 00:01:51,630
behavior is very complicated it's not

00:01:50,219 --> 00:01:53,579
possible for us to sit down and actually

00:01:51,630 --> 00:01:56,850
write a program for it we need some kind

00:01:53,579 --> 00:01:57,929
of algorithm that will learn it and in

00:01:56,850 --> 00:01:59,850
order to do that we need a way to

00:01:57,929 --> 00:02:01,799
describe the system in a way that is

00:01:59,850 --> 00:02:03,479
concise and understandable and this is

00:02:01,799 --> 00:02:05,789
what models do for us right they allow

00:02:03,479 --> 00:02:08,310
us to reason about this complex behavior

00:02:05,789 --> 00:02:09,509
in a mathematical language and the

00:02:08,310 --> 00:02:11,970
second thing that all machine learning

00:02:09,509 --> 00:02:13,140
problems have in common is data so in

00:02:11,970 --> 00:02:14,280
machine learning almost all of the

00:02:13,140 --> 00:02:15,989
models that we're going to use are

00:02:14,280 --> 00:02:17,640
statistical models and what that means

00:02:15,989 --> 00:02:18,360
is that we make assumptions about how

00:02:17,640 --> 00:02:19,950
the world behave

00:02:18,360 --> 00:02:21,360
and then we use the data the

00:02:19,950 --> 00:02:22,680
observations that we have in order to

00:02:21,360 --> 00:02:25,470
narrow down the assumptions in order

00:02:22,680 --> 00:02:26,790
that is much more specific so we always

00:02:25,470 --> 00:02:29,250
need data in order to train our

00:02:26,790 --> 00:02:30,870
algorithms and one thing that we

00:02:29,250 --> 00:02:33,590
definitely need is to be able to tell

00:02:30,870 --> 00:02:36,480
and how well our model is able to

00:02:33,590 --> 00:02:37,890
emulate the behavior that behavior so we

00:02:36,480 --> 00:02:40,140
need to some way to estimate the quality

00:02:37,890 --> 00:02:41,610
of the model and something that can

00:02:40,140 --> 00:02:45,960
guide us when we are trying to optimize

00:02:41,610 --> 00:02:47,880
the model and so that is our objective

00:02:45,960 --> 00:02:48,990
function basically so an objective

00:02:47,880 --> 00:02:50,880
function is the measure of the quality

00:02:48,990 --> 00:02:51,990
of our model so for example if we're

00:02:50,880 --> 00:02:53,610
trying to predict the temperature

00:02:51,990 --> 00:02:55,470
tomorrow an objective function could be

00:02:53,610 --> 00:02:56,640
the squared difference of the actual

00:02:55,470 --> 00:02:57,630
temperature versus the temperature that

00:02:56,640 --> 00:03:00,570
being predicted

00:02:57,630 --> 00:03:02,790
right and next we might have some

00:03:00,570 --> 00:03:04,590
knowledge and desires about the model so

00:03:02,790 --> 00:03:06,600
for examples maybe in our data set we

00:03:04,590 --> 00:03:08,250
have 10000 features but our assumption

00:03:06,600 --> 00:03:10,230
or our desire is that only a few of them

00:03:08,250 --> 00:03:12,270
are actually relevant so we might use

00:03:10,230 --> 00:03:13,980
something like regularization or our

00:03:12,270 --> 00:03:15,120
prior knowledge about the problem in

00:03:13,980 --> 00:03:18,600
order to bring down the end the number

00:03:15,120 --> 00:03:20,160
of features that we have and if we

00:03:18,600 --> 00:03:22,230
combine all of these then we get our

00:03:20,160 --> 00:03:24,360
machine learning program so my signaling

00:03:22,230 --> 00:03:26,040
program essentially is a function that

00:03:24,360 --> 00:03:27,450
takes the model that we have the data

00:03:26,040 --> 00:03:29,730
that we have and it provides an answer

00:03:27,450 --> 00:03:31,080
and in order to optimize this function

00:03:29,730 --> 00:03:34,050
we use the objective function that we

00:03:31,080 --> 00:03:36,120
have been created and this is usually

00:03:34,050 --> 00:03:37,739
done in an iterative manner where we

00:03:36,120 --> 00:03:39,330
take the model that we have and we

00:03:37,739 --> 00:03:40,860
update it by taking a look at the data

00:03:39,330 --> 00:03:42,300
and we do this again and again until we

00:03:40,860 --> 00:03:44,790
are satisfied with the quality of our

00:03:42,300 --> 00:03:46,500
model and all of these are the basic

00:03:44,790 --> 00:03:47,850
primitives that can we can use to

00:03:46,500 --> 00:03:49,950
describe almost any machine learning

00:03:47,850 --> 00:03:52,530
problem and we can see how this works in

00:03:49,950 --> 00:03:54,480
a specific example so let's say that we

00:03:52,530 --> 00:03:56,190
are Twitter and we observe that our

00:03:54,480 --> 00:03:58,739
users are leaving the platform and never

00:03:56,190 --> 00:04:01,410
returning and after some research we

00:03:58,739 --> 00:04:03,420
decide or we discover that users users

00:04:01,410 --> 00:04:05,640
that live usually do not engage with the

00:04:03,420 --> 00:04:06,930
platform and we what we decide to do as

00:04:05,640 --> 00:04:08,340
a solution is we try to increase the

00:04:06,930 --> 00:04:10,050
engagement of the users by trying to

00:04:08,340 --> 00:04:11,610
make them to read through it more and

00:04:10,050 --> 00:04:12,870
this is actually what the engineering

00:04:11,610 --> 00:04:14,340
decision started so this is something

00:04:12,870 --> 00:04:17,549
that the management could do for example

00:04:14,340 --> 00:04:19,890
up until now so the engineering decision

00:04:17,549 --> 00:04:21,600
is that we can that we can make is we

00:04:19,890 --> 00:04:23,250
can take this interface that we created

00:04:21,600 --> 00:04:26,310
before and then just fill in the blanks

00:04:23,250 --> 00:04:28,020
in order to solve our problem so first

00:04:26,310 --> 00:04:29,580
we have the common components and we

00:04:28,020 --> 00:04:31,080
start out of course as always with the

00:04:29,580 --> 00:04:32,130
data and the data that we have is

00:04:31,080 --> 00:04:33,960
features about

00:04:32,130 --> 00:04:36,030
the user themselves like the profile of

00:04:33,960 --> 00:04:37,440
the user and we have a features about

00:04:36,030 --> 00:04:39,690
the tweet the word that contains

00:04:37,440 --> 00:04:42,750
hashtags everything and then we have the

00:04:39,690 --> 00:04:44,730
labels which is the decision or what we

00:04:42,750 --> 00:04:46,500
have observed this tweet get retweeted

00:04:44,730 --> 00:04:49,160
by a specific user that would be the Y

00:04:46,500 --> 00:04:51,990
in our case what we're trying to predict

00:04:49,160 --> 00:04:53,370
next up is our model so what we said is

00:04:51,990 --> 00:04:55,650
what we want to find tweets that are

00:04:53,370 --> 00:04:59,190
more likely to be retweeted so ideally

00:04:55,650 --> 00:05:01,170
we want to have a probability as an

00:04:59,190 --> 00:05:02,730
output from our model and logistic

00:05:01,170 --> 00:05:04,260
regression here is a very good first

00:05:02,730 --> 00:05:06,060
model to try because it's a well studied

00:05:04,260 --> 00:05:09,540
classification algorithm and it gives us

00:05:06,060 --> 00:05:11,730
probabilistic out and in this particular

00:05:09,540 --> 00:05:13,020
model the model itself the logistic

00:05:11,730 --> 00:05:14,460
regression will actually give us the

00:05:13,020 --> 00:05:17,280
objective function as well so we take

00:05:14,460 --> 00:05:18,930
that as a given and you should know we

00:05:17,280 --> 00:05:20,940
should know that the more flexible

00:05:18,930 --> 00:05:22,380
algorithms we can actually sit down and

00:05:20,940 --> 00:05:24,320
design our own objective function in

00:05:22,380 --> 00:05:26,670
order to fit our problem much better

00:05:24,320 --> 00:05:28,440
and finally we choose which algorithm

00:05:26,670 --> 00:05:30,780
we're going to use to optimize the

00:05:28,440 --> 00:05:32,310
objective function and one thing that I

00:05:30,780 --> 00:05:34,290
should note is that pretty much you're

00:05:32,310 --> 00:05:36,000
done with the algorithm design at this

00:05:34,290 --> 00:05:37,620
point but there is a bunch of other

00:05:36,000 --> 00:05:39,540
problems that might come up and come up

00:05:37,620 --> 00:05:40,980
as you go from the data all the way down

00:05:39,540 --> 00:05:43,140
to the algorithm and what they presented

00:05:40,980 --> 00:05:45,060
here is very simplified a lot can go

00:05:43,140 --> 00:05:46,890
wrong if we're not carefully and we're

00:05:45,060 --> 00:05:51,330
going to talk a lot a bit more about

00:05:46,890 --> 00:05:53,100
these problems now so data is the common

00:05:51,330 --> 00:05:54,330
thing FM underpins all machine learning

00:05:53,100 --> 00:05:57,570
problems and it's perhaps the most

00:05:54,330 --> 00:05:59,520
important part of your pipeline in data

00:05:57,570 --> 00:06:00,960
came in can come in with millions of

00:05:59,520 --> 00:06:02,580
problems we can have measurement errors

00:06:00,960 --> 00:06:05,850
we can have privacy issues we can have

00:06:02,580 --> 00:06:08,100
even spread spread seed the errors in a

00:06:05,850 --> 00:06:10,260
function and in machine learning I think

00:06:08,100 --> 00:06:12,360
most pressingly as in most principles

00:06:10,260 --> 00:06:13,980
the output that we're going to get from

00:06:12,360 --> 00:06:16,230
a la algorithm is only going to be as

00:06:13,980 --> 00:06:18,690
good as the input that we give it and

00:06:16,230 --> 00:06:21,540
what we want to do here is to get to

00:06:18,690 --> 00:06:23,040
find a way to to quality to quantify the

00:06:21,540 --> 00:06:26,280
quality of our data in a principled

00:06:23,040 --> 00:06:28,050
manner and need Lauren's here is a

00:06:26,280 --> 00:06:30,660
researcher from a sample University she

00:06:28,050 --> 00:06:33,030
works a lot with in medicine which has

00:06:30,660 --> 00:06:35,370
not only bought datasets and he recently

00:06:33,030 --> 00:06:37,520
presented the idea of data rate and

00:06:35,370 --> 00:06:40,140
readiness which I'll try to explain here

00:06:37,520 --> 00:06:41,730
so what we want to do is we want to make

00:06:40,140 --> 00:06:43,050
it easier to reason about how

00:06:41,730 --> 00:06:45,600
appropriate the data set that we

00:06:43,050 --> 00:06:47,160
currently have is for learning so what

00:06:45,600 --> 00:06:49,410
this is that we create different levels

00:06:47,160 --> 00:06:51,510
of readiness for our data set and we

00:06:49,410 --> 00:06:53,550
move from one level to the next and when

00:06:51,510 --> 00:06:56,700
we are sure that they are data fulfills

00:06:53,550 --> 00:06:57,930
certain quality criteria so we can start

00:06:56,700 --> 00:07:00,240
by having three bands for these

00:06:57,930 --> 00:07:01,950
different levels let's say C B and a and

00:07:00,240 --> 00:07:05,070
maybe we can create sub levels for each

00:07:01,950 --> 00:07:07,110
one of those so the lowest band is bad C

00:07:05,070 --> 00:07:09,300
and it's about the accessibility of our

00:07:07,110 --> 00:07:11,070
data so when starting out at this level

00:07:09,300 --> 00:07:13,800
we might actually have what we call a

00:07:11,070 --> 00:07:15,450
hearsay data data that somebody has told

00:07:13,800 --> 00:07:17,100
you exists but it does you're not

00:07:15,450 --> 00:07:18,630
actually sure if it's there so this is

00:07:17,100 --> 00:07:20,490
very commonly when starting out a new

00:07:18,630 --> 00:07:22,620
machine learning project where you might

00:07:20,490 --> 00:07:24,180
hear from other teams like yeah we

00:07:22,620 --> 00:07:25,770
should have that or we have we are

00:07:24,180 --> 00:07:27,390
logging it out or we have been logging

00:07:25,770 --> 00:07:29,040
that for many years but until you

00:07:27,390 --> 00:07:30,690
actually sit down extract the data set

00:07:29,040 --> 00:07:32,700
and look at it there's no way that you

00:07:30,690 --> 00:07:34,140
can actually be sure about this so in

00:07:32,700 --> 00:07:36,060
order to graduate your data set from

00:07:34,140 --> 00:07:38,040
this level you need to ensure first that

00:07:36,060 --> 00:07:40,010
they data exist what kind of format it

00:07:38,040 --> 00:07:42,660
comes in if there is any privacy

00:07:40,010 --> 00:07:45,090
concerns or legal concerns in using it

00:07:42,660 --> 00:07:46,710
and anything that can can make it

00:07:45,090 --> 00:07:48,540
difficult to actually obtain the data so

00:07:46,710 --> 00:07:50,850
and at the end of this level which say

00:07:48,540 --> 00:07:52,530
we can call it C 1 and we have cleared

00:07:50,850 --> 00:07:53,910
all these obstacles and the data set is

00:07:52,530 --> 00:07:57,870
actually ready to be loaded into

00:07:53,910 --> 00:07:59,310
analysis software then at level B we are

00:07:57,870 --> 00:08:02,130
concerned with the faithfulness of the

00:07:59,310 --> 00:08:04,260
data so first of all the data that we

00:08:02,130 --> 00:08:06,210
have actually record the correct thing

00:08:04,260 --> 00:08:08,040
and what is the level of error in the

00:08:06,210 --> 00:08:10,440
measurements and did any sampling occur

00:08:08,040 --> 00:08:13,110
in the in the in the data and how did we

00:08:10,440 --> 00:08:14,460
treat the missing values so all of these

00:08:13,110 --> 00:08:16,110
things are very important in order to

00:08:14,460 --> 00:08:17,910
graduate this level we need to be fully

00:08:16,110 --> 00:08:20,010
aware of the faithfulness of our of our

00:08:17,910 --> 00:08:22,080
data about the representation and the

00:08:20,010 --> 00:08:24,530
truth that is actually from that comes

00:08:22,080 --> 00:08:28,290
from what we originally wanted to record

00:08:24,530 --> 00:08:29,760
and one day which is the last level is

00:08:28,290 --> 00:08:32,340
the first level where we can actually

00:08:29,760 --> 00:08:34,229
make questions about how appropriate the

00:08:32,340 --> 00:08:36,060
data is for what we're trying to answer

00:08:34,229 --> 00:08:38,310
so here is the first time that we

00:08:36,060 --> 00:08:41,160
actually answer can we use this data set

00:08:38,310 --> 00:08:43,380
want to predict add clicks from users or

00:08:41,160 --> 00:08:45,390
can we use this data set to predict how

00:08:43,380 --> 00:08:48,030
the time to failure for a specific

00:08:45,390 --> 00:08:49,650
component and here we might actually

00:08:48,030 --> 00:08:51,960
discover that we may need additional

00:08:49,650 --> 00:08:53,700
data sets we may need human annotation

00:08:51,960 --> 00:08:55,500
we may need to iterate through the whole

00:08:53,700 --> 00:08:57,330
pipeline again so again this is an

00:08:55,500 --> 00:08:58,470
iterative process and every time that

00:08:57,330 --> 00:08:59,510
you discover that you need some more

00:08:58,470 --> 00:09:00,770
data set you need to go through

00:08:59,510 --> 00:09:02,420
the whole thing again and make sure that

00:09:00,770 --> 00:09:04,220
the data said that you have at the end

00:09:02,420 --> 00:09:08,450
is actually fulfills all the quality

00:09:04,220 --> 00:09:10,130
criteria and the idea with these levels

00:09:08,450 --> 00:09:11,990
is to provide a common language so teams

00:09:10,130 --> 00:09:13,610
can communicate their data readiness

00:09:11,990 --> 00:09:15,470
levels and we can ask ask and answer

00:09:13,610 --> 00:09:17,450
concrete questions about the state of

00:09:15,470 --> 00:09:19,580
the data and trying to skip any of these

00:09:17,450 --> 00:09:22,100
parts will almost always leads a lead to

00:09:19,580 --> 00:09:23,750
problems and that's a final warning do

00:09:22,100 --> 00:09:25,250
not underestimate the time and effort

00:09:23,750 --> 00:09:27,530
required to bring this data from the

00:09:25,250 --> 00:09:28,910
level C and B and they perhaps am a

00:09:27,530 --> 00:09:34,370
least exciting we have a branch but they

00:09:28,910 --> 00:09:36,080
are all equally important so next after

00:09:34,370 --> 00:09:37,640
you selected your objective function and

00:09:36,080 --> 00:09:39,860
ensure that your data is in a good state

00:09:37,640 --> 00:09:41,390
to learn from what comes up is the

00:09:39,860 --> 00:09:43,730
selection of your software and your

00:09:41,390 --> 00:09:45,800
algorithm so I've labeled this here as

00:09:43,730 --> 00:09:46,970
easy choices because I actually believe

00:09:45,800 --> 00:09:48,620
that the other parts of your pipeline

00:09:46,970 --> 00:09:53,000
are much more important for the success

00:09:48,620 --> 00:09:53,960
of your of your product launch so when I

00:09:53,000 --> 00:09:55,520
was first thinking about this

00:09:53,960 --> 00:09:57,290
presentation I had to image this in mind

00:09:55,520 --> 00:09:57,950
the first one is this one from

00:09:57,290 --> 00:09:59,720
scikit-learn

00:09:57,950 --> 00:10:01,190
which is kind of like it's its it to

00:09:59,720 --> 00:10:03,740
guide people in order to select a

00:10:01,190 --> 00:10:04,910
specific algorithm and you can already

00:10:03,740 --> 00:10:06,350
see that there is a multitude of

00:10:04,910 --> 00:10:08,090
algorithmic choices but there's not too

00:10:06,350 --> 00:10:10,610
many so here I call this a farm it's not

00:10:08,090 --> 00:10:11,840
enough animals here and the second one

00:10:10,610 --> 00:10:13,100
this one is from the Assam of a

00:10:11,840 --> 00:10:15,710
Institute where they try to illustrate

00:10:13,100 --> 00:10:18,290
some of the most popular and neural

00:10:15,710 --> 00:10:20,300
network architectures and my point here

00:10:18,290 --> 00:10:21,860
with these two images is that already at

00:10:20,300 --> 00:10:24,200
algorithm so it is a staggering amount

00:10:21,860 --> 00:10:26,090
of choices that you have to make and the

00:10:24,200 --> 00:10:28,250
reality is that for your first load this

00:10:26,090 --> 00:10:29,870
is mostly unnecessary for your first

00:10:28,250 --> 00:10:32,000
launch but you should be focusing on its

00:10:29,870 --> 00:10:34,610
simplicity and there are a lot of good

00:10:32,000 --> 00:10:37,910
reasons to for that and we'll see those

00:10:34,610 --> 00:10:39,440
next so if you've read a couple of

00:10:37,910 --> 00:10:41,300
things about the machine learning you

00:10:39,440 --> 00:10:43,400
probably come across this suggestion

00:10:41,300 --> 00:10:45,680
we're picking the the simplest most

00:10:43,400 --> 00:10:47,450
model possible is often motivated by

00:10:45,680 --> 00:10:50,210
theoretically by things like Occam's

00:10:47,450 --> 00:10:52,730
razor or the the census so forward

00:10:50,210 --> 00:10:54,410
fitting if you use a complex model but

00:10:52,730 --> 00:10:55,670
what I would like to point out here is

00:10:54,410 --> 00:10:57,440
that there is also very tangible

00:10:55,670 --> 00:11:02,540
engineering benefits in using a simpler

00:10:57,440 --> 00:11:04,640
model and first the initial model that

00:11:02,540 --> 00:11:06,830
you that you will deploy it's more about

00:11:04,640 --> 00:11:08,990
getting all the infrastructure right so

00:11:06,830 --> 00:11:10,370
when you deploy your model you already

00:11:08,990 --> 00:11:11,810
have to deal with serving your

00:11:10,370 --> 00:11:13,390
predictions and making sure that the

00:11:11,810 --> 00:11:15,279
data is fed correctly into the albergue

00:11:13,390 --> 00:11:16,660
the predictions our output and provided

00:11:15,279 --> 00:11:18,940
to the user so there's a lot of

00:11:16,660 --> 00:11:20,829
complexity there even before dealing

00:11:18,940 --> 00:11:22,839
with the algorithms so if you add to

00:11:20,829 --> 00:11:25,390
that algorithmic complexity and try to

00:11:22,839 --> 00:11:28,089
figure out why the algorithm replied

00:11:25,390 --> 00:11:29,589
that that it did then you you're going

00:11:28,089 --> 00:11:31,329
to have a bad time basically and

00:11:29,589 --> 00:11:33,279
everything Google article actually

00:11:31,329 --> 00:11:34,899
suggests that you aim for your first

00:11:33,279 --> 00:11:36,250
learns to be neutral which means that

00:11:34,899 --> 00:11:37,329
you just aim for the to get the thing

00:11:36,250 --> 00:11:39,279
out there make sure that it doesn't

00:11:37,329 --> 00:11:43,540
break anything and then you can focus on

00:11:39,279 --> 00:11:46,360
games later second simpler models are

00:11:43,540 --> 00:11:47,829
usually interpreted if you run a linear

00:11:46,360 --> 00:11:49,240
regression every weight that you're

00:11:47,829 --> 00:11:50,980
gonna get in your model actually means

00:11:49,240 --> 00:11:55,120
something and that becomes very useful

00:11:50,980 --> 00:11:56,649
when you try to debug the the algorithms

00:11:55,120 --> 00:11:58,510
in made and try to explain why it made

00:11:56,649 --> 00:11:59,769
these predictions all of these things

00:11:58,510 --> 00:12:02,200
are very important when you're starting

00:11:59,769 --> 00:12:03,550
out so make sure that you're using the

00:12:02,200 --> 00:12:05,260
simple model because they're actually in

00:12:03,550 --> 00:12:06,700
interpretable which is much much harder

00:12:05,260 --> 00:12:08,529
to do if you have a neural network with

00:12:06,700 --> 00:12:13,209
1 million weights that's impossible to

00:12:08,529 --> 00:12:15,310
do basically and thirdly the the use of

00:12:13,209 --> 00:12:16,600
complex models are old binaries so what

00:12:15,310 --> 00:12:18,040
do we mean here like in software

00:12:16,600 --> 00:12:20,019
engineering we use concepts like

00:12:18,040 --> 00:12:21,430
abstraction and encapsulation in order

00:12:20,019 --> 00:12:22,779
to isolate different parts of the code

00:12:21,430 --> 00:12:24,880
so they don't affect each other if we

00:12:22,779 --> 00:12:26,260
make changes right but in machine

00:12:24,880 --> 00:12:28,300
learning we actually very often mix a

00:12:26,260 --> 00:12:29,829
signal so we have features that interact

00:12:28,300 --> 00:12:32,079
with each other and this is in the

00:12:29,829 --> 00:12:34,060
nature of the algorithms itself and this

00:12:32,079 --> 00:12:35,920
leads to a principle that's called the

00:12:34,060 --> 00:12:38,079
cake principle we're changing anything

00:12:35,920 --> 00:12:39,970
changes everything and this principle

00:12:38,079 --> 00:12:41,380
applies not only to the features but

00:12:39,970 --> 00:12:43,269
also to the hyper parameters and

00:12:41,380 --> 00:12:45,010
sampling process pretty much every knob

00:12:43,269 --> 00:12:46,329
that you can tweak in your machine

00:12:45,010 --> 00:12:49,209
learning pipeline will actually affect

00:12:46,329 --> 00:12:50,890
every other thing in your pipeline so by

00:12:49,209 --> 00:12:52,449
making every single part of the pipeline

00:12:50,890 --> 00:12:54,130
as simple as possible you're making much

00:12:52,449 --> 00:12:56,949
of your life much much easier as an

00:12:54,130 --> 00:12:58,630
engineer and of course there's a bunch

00:12:56,949 --> 00:13:00,370
of other things that can affect your or

00:12:58,630 --> 00:13:01,690
your choice of algorithm here but I

00:13:00,370 --> 00:13:03,250
think for people who are starting out

00:13:01,690 --> 00:13:05,709
just going for the simplest possible is

00:13:03,250 --> 00:13:07,769
actually a very good piece of advice so

00:13:05,709 --> 00:13:10,360
with that we can move on to software and

00:13:07,769 --> 00:13:12,250
actually I believe this is an even less

00:13:10,360 --> 00:13:13,570
important the decision to make a

00:13:12,250 --> 00:13:14,980
comeback to the rest of the pipeline so

00:13:13,570 --> 00:13:16,839
you only have like a single slide on

00:13:14,980 --> 00:13:19,480
this topic and I would just like to

00:13:16,839 --> 00:13:21,010
illustrate that the machine learning

00:13:19,480 --> 00:13:23,260
software so these are at some of the

00:13:21,010 --> 00:13:24,850
most popular open-source machine

00:13:23,260 --> 00:13:26,950
learning libraries on github all of them

00:13:24,850 --> 00:13:28,630
or almost all of them have more than one

00:13:26,950 --> 00:13:30,670
and stars which means that all of them

00:13:28,630 --> 00:13:32,110
are popular in the wrong way and my

00:13:30,670 --> 00:13:34,630
original plan was to pick a few of these

00:13:32,110 --> 00:13:36,519
and they try to talk about them more and

00:13:34,630 --> 00:13:40,360
explain them but I think that it's

00:13:36,519 --> 00:13:41,889
better to to point out that by now it

00:13:40,360 --> 00:13:43,180
machine learning software has become a

00:13:41,889 --> 00:13:45,790
commodity and there is very little

00:13:43,180 --> 00:13:47,829
differentiation between the top the top

00:13:45,790 --> 00:13:50,260
choices so what I would suggest for

00:13:47,829 --> 00:13:52,800
people starting out is that pick one but

00:13:50,260 --> 00:13:54,639
that's that you are comfortable with

00:13:52,800 --> 00:13:56,589
maybe something that your team has

00:13:54,639 --> 00:13:58,120
already worked on and focus on other

00:13:56,589 --> 00:13:59,980
parts of the album that will have a much

00:13:58,120 --> 00:14:02,829
bigger role in the success of your

00:13:59,980 --> 00:14:04,720
project so with that will I'd like to

00:14:02,829 --> 00:14:07,329
move on to another more neglected part

00:14:04,720 --> 00:14:08,980
of machine learning and that is what

00:14:07,329 --> 00:14:12,430
happens when your model comes in touch

00:14:08,980 --> 00:14:13,990
with with the world and this is

00:14:12,430 --> 00:14:16,240
something that you won't find a lot of

00:14:13,990 --> 00:14:18,070
research on and everyone seems to comes

00:14:16,240 --> 00:14:19,300
up with their own solution so what I

00:14:18,070 --> 00:14:20,680
would like to do in this section is

00:14:19,300 --> 00:14:23,170
point out problems that are common when

00:14:20,680 --> 00:14:24,310
deploying a machine learning model so

00:14:23,170 --> 00:14:26,260
first I would like to note the

00:14:24,310 --> 00:14:28,060
expectation versus reality of having a

00:14:26,260 --> 00:14:29,350
machine learning system in production so

00:14:28,060 --> 00:14:31,449
in an ideal world so this is the

00:14:29,350 --> 00:14:33,730
academic setting here right so we have

00:14:31,449 --> 00:14:36,160
data sisters are clean and standardized

00:14:33,730 --> 00:14:37,959
we develop the model and then we test it

00:14:36,160 --> 00:14:40,300
on some benchmark data set and that's it

00:14:37,959 --> 00:14:41,620
done right but the problem comes when

00:14:40,300 --> 00:14:44,949
you actually deploy the machine learning

00:14:41,620 --> 00:14:46,329
a model in production it needs to

00:14:44,949 --> 00:14:47,440
interact with the real world and that

00:14:46,329 --> 00:14:50,110
means that it's probably going to end up

00:14:47,440 --> 00:14:51,399
looking more much more like this so to

00:14:50,110 --> 00:14:53,680
have a running machine learning system

00:14:51,399 --> 00:14:55,209
you will need to have a large number of

00:14:53,680 --> 00:14:57,579
components around it it's with its own

00:14:55,209 --> 00:14:59,620
complexities and in recent Google's our

00:14:57,579 --> 00:15:03,279
survey the others mentioned that in a

00:14:59,620 --> 00:15:04,839
running matured program often only 5% of

00:15:03,279 --> 00:15:07,600
the code is actually machine learning

00:15:04,839 --> 00:15:09,519
code logic and 95% of the code is all

00:15:07,600 --> 00:15:13,060
that polemic that is required in order

00:15:09,519 --> 00:15:14,440
to make this whole thing work and then

00:15:13,060 --> 00:15:16,060
what are some common pitfalls when

00:15:14,440 --> 00:15:18,760
deploying machine learning programs in a

00:15:16,060 --> 00:15:20,740
complex setting like this one so first

00:15:18,760 --> 00:15:22,870
we almost always have data dependencies

00:15:20,740 --> 00:15:24,490
and similar to this similar to how you

00:15:22,870 --> 00:15:25,990
would have code dependencies in a

00:15:24,490 --> 00:15:28,329
project but they even harder to deal

00:15:25,990 --> 00:15:30,279
with so they are to some respect

00:15:28,329 --> 00:15:32,579
unavoidable in machine learning because

00:15:30,279 --> 00:15:34,959
at any point in a machine learning

00:15:32,579 --> 00:15:36,819
program we need to have our data set and

00:15:34,959 --> 00:15:38,470
we usually pass it through a complex

00:15:36,819 --> 00:15:40,450
data processing pipeline in order to

00:15:38,470 --> 00:15:42,459
make it and prepare it to be ready to

00:15:40,450 --> 00:15:45,370
for learning and this can create a bunch

00:15:42,459 --> 00:15:47,200
of problems so a common problem is that

00:15:45,370 --> 00:15:48,250
the the source data is unstable

00:15:47,200 --> 00:15:50,260
so that means that it can change

00:15:48,250 --> 00:15:52,480
distribution or it can have even more

00:15:50,260 --> 00:15:54,100
dramatic changes an example would be if

00:15:52,480 --> 00:15:56,079
a different data team owns the data

00:15:54,100 --> 00:15:58,570
pipeline in the different team does the

00:15:56,079 --> 00:16:02,649
learning so for example let's say that

00:15:58,570 --> 00:16:04,389
the data team starts monitoring the time

00:16:02,649 --> 00:16:06,100
that the users spend on a website using

00:16:04,389 --> 00:16:07,750
seconds because they want to have it for

00:16:06,100 --> 00:16:08,949
their own purposes and then you as a

00:16:07,750 --> 00:16:10,240
machine learning team you take that

00:16:08,949 --> 00:16:12,459
feature and they use it in order to do

00:16:10,240 --> 00:16:13,810
recommendations so maybe three months

00:16:12,459 --> 00:16:15,550
later the data team decides that they

00:16:13,810 --> 00:16:16,990
want to have more accurate accuracy in

00:16:15,550 --> 00:16:18,670
their measurements and they start

00:16:16,990 --> 00:16:21,190
measuring the time using milliseconds

00:16:18,670 --> 00:16:22,779
now if the data team does not have

00:16:21,190 --> 00:16:24,639
proper infrastructure to detect or the

00:16:22,779 --> 00:16:26,230
consumers of the data set or if the

00:16:24,639 --> 00:16:28,329
machine learning team does not have the

00:16:26,230 --> 00:16:29,709
infrastructure for monitoring to to to

00:16:28,329 --> 00:16:31,329
detect this change of distribution in

00:16:29,709 --> 00:16:32,680
the data the model will actually

00:16:31,329 --> 00:16:35,949
continue working and it will start

00:16:32,680 --> 00:16:37,329
producing a bogus predictions so so

00:16:35,949 --> 00:16:39,190
users to this would be to have very

00:16:37,329 --> 00:16:41,320
strict ACS for your data you can have a

00:16:39,190 --> 00:16:42,940
good monitoring or pipeline but what I

00:16:41,320 --> 00:16:44,500
prefer actually is to here for teams to

00:16:42,940 --> 00:16:46,420
actually have full ownership from of the

00:16:44,500 --> 00:16:47,949
pipeline from from serving the cryptic

00:16:46,420 --> 00:16:51,370
predictions all the way down to creating

00:16:47,949 --> 00:16:53,589
the data set a second related problems

00:16:51,370 --> 00:16:55,120
are feedback loops so feedback loops can

00:16:53,589 --> 00:16:57,790
be direct which are easier to deal with

00:16:55,120 --> 00:16:59,170
or indirect so in a direct feedback loop

00:16:57,790 --> 00:17:01,510
the model actually affects its own

00:16:59,170 --> 00:17:04,600
training set so this for example if your

00:17:01,510 --> 00:17:06,490
model ranks items in a list they list

00:17:04,600 --> 00:17:07,959
the items that it puts in towards the

00:17:06,490 --> 00:17:09,669
top of the list are always going to get

00:17:07,959 --> 00:17:10,870
clicked more often and then the

00:17:09,669 --> 00:17:13,360
algorithm will believe that it's also

00:17:10,870 --> 00:17:15,669
more probable that will get clicked and

00:17:13,360 --> 00:17:16,750
the solution to this type of things is

00:17:15,669 --> 00:17:18,339
that you can just remove all the

00:17:16,750 --> 00:17:18,730
predictions that have been passed by an

00:17:18,339 --> 00:17:20,380
algorithm

00:17:18,730 --> 00:17:22,299
but that of course would be very bad

00:17:20,380 --> 00:17:24,280
because you would be reducing your data

00:17:22,299 --> 00:17:26,079
so it can buy quite a lot so a better

00:17:24,280 --> 00:17:28,089
idea for this is to actually include the

00:17:26,079 --> 00:17:29,770
ranking of the item that your album

00:17:28,089 --> 00:17:31,150
produces in these features so if you

00:17:29,770 --> 00:17:32,799
include that as a feature then the

00:17:31,150 --> 00:17:34,870
algorithm says to figure out the

00:17:32,799 --> 00:17:36,309
importance of the rankings that the item

00:17:34,870 --> 00:17:38,970
has and that can help you quite a lot

00:17:36,309 --> 00:17:41,260
without you having to do anything and

00:17:38,970 --> 00:17:43,720
indirect feedback loops are much harder

00:17:41,260 --> 00:17:46,179
to deal with so for example Netflix uses

00:17:43,720 --> 00:17:48,909
curve uses a learning system in order to

00:17:46,179 --> 00:17:51,340
provide different covers for each item

00:17:48,909 --> 00:17:52,630
that the user see and then it will use a

00:17:51,340 --> 00:17:54,310
learning system in order to figure out

00:17:52,630 --> 00:17:55,840
which is the best cover for each of

00:17:54,310 --> 00:17:57,400
the items and of course it has a

00:17:55,840 --> 00:18:00,190
recommendation system that recommends

00:17:57,400 --> 00:18:01,450
its item in the first place right so

00:18:00,190 --> 00:18:03,010
then you have to think about what will

00:18:01,450 --> 00:18:05,500
happen if I get a good recommendation

00:18:03,010 --> 00:18:06,760
but with a bad cover so if Netflix is

00:18:05,500 --> 00:18:08,530
not careful in the way that they

00:18:06,760 --> 00:18:10,120
implement their elgu's it's very likely

00:18:08,530 --> 00:18:11,860
that one system will actually are

00:18:10,120 --> 00:18:13,450
starting flensing the output of the

00:18:11,860 --> 00:18:15,790
other system and the data set that it's

00:18:13,450 --> 00:18:18,960
strained with and that can be a hidden

00:18:15,790 --> 00:18:21,730
feedback loop that's very hard to detect

00:18:18,960 --> 00:18:23,440
so I think I would like to conclude here

00:18:21,730 --> 00:18:26,260
more or less and the idea is how do we

00:18:23,440 --> 00:18:28,570
bring this all together so I think that

00:18:26,260 --> 00:18:30,520
the key takeaways are that you can use a

00:18:28,570 --> 00:18:31,990
common interface to define most of the

00:18:30,520 --> 00:18:34,750
machine learning problems and this is

00:18:31,990 --> 00:18:36,280
very useful when starting out that you

00:18:34,750 --> 00:18:37,960
should determine the readiness of you of

00:18:36,280 --> 00:18:39,460
your data before starting learning and

00:18:37,960 --> 00:18:41,620
make sure that you monitor the readiness

00:18:39,460 --> 00:18:43,120
of your data at all times as well and

00:18:41,620 --> 00:18:44,980
that you shouldn't spend too much time

00:18:43,120 --> 00:18:46,510
worrying at first about the selection of

00:18:44,980 --> 00:18:48,220
algorithms or the selection of software

00:18:46,510 --> 00:18:50,350
because it's not as important as it may

00:18:48,220 --> 00:18:51,730
seem and finally I think that you should

00:18:50,350 --> 00:18:53,770
worry much more about what will happen

00:18:51,730 --> 00:18:55,030
when your model comes in touch with with

00:18:53,770 --> 00:18:57,760
the world and what will happen when you

00:18:55,030 --> 00:18:59,020
put it out in the world and without I

00:18:57,760 --> 00:19:07,819
think

00:18:59,020 --> 00:19:07,819
[Applause]

00:20:16,940 --> 00:20:23,809
features all the kind of like sitting on

00:20:19,710 --> 00:20:23,809
a very sort of fragile structure because

00:20:36,950 --> 00:20:46,860
yeah I think this is yeah okay so the

00:20:43,230 --> 00:20:49,080
question is like okay so I think that

00:20:46,860 --> 00:20:50,430
the main thing is that as you move on in

00:20:49,080 --> 00:20:52,500
a fast moving environment that a new

00:20:50,430 --> 00:20:54,300
company you always aggregate more and

00:20:52,500 --> 00:20:55,650
more technical debt right and the thing

00:20:54,300 --> 00:20:57,000
is that especially machine learning as

00:20:55,650 --> 00:20:58,350
another place do you have to pay it off

00:20:57,000 --> 00:20:59,940
at some point it doesn't work like you

00:20:58,350 --> 00:21:01,860
can just no you cannot just keep

00:20:59,940 --> 00:21:03,450
including more and more fragile fragile

00:21:01,860 --> 00:21:05,010
thing because at times at some point the

00:21:03,450 --> 00:21:07,050
whole thing is going to break so it's

00:21:05,010 --> 00:21:09,450
actually a very good idea that at some

00:21:07,050 --> 00:21:11,130
point to just stop and do this thing

00:21:09,450 --> 00:21:13,680
where we call this neutral loans right

00:21:11,130 --> 00:21:15,390
so it would be much better to just say

00:21:13,680 --> 00:21:16,890
okay we're stop worrying about the model

00:21:15,390 --> 00:21:18,600
but what we're gonna do now is we're

00:21:16,890 --> 00:21:19,590
gonna be deploying new thing where the

00:21:18,600 --> 00:21:21,420
new thing is going to be a new

00:21:19,590 --> 00:21:22,560
infrastructure and what is going what

00:21:21,420 --> 00:21:24,210
that is going to do is that we're going

00:21:22,560 --> 00:21:25,980
to build if it's just an easier system

00:21:24,210 --> 00:21:27,870
to make things that everything works we

00:21:25,980 --> 00:21:29,580
might have a prediction that this I

00:21:27,870 --> 00:21:31,500
don't know the mean of everything right

00:21:29,580 --> 00:21:32,940
that's always fine okay so that's our

00:21:31,500 --> 00:21:34,440
model now it's very simple we know what

00:21:32,940 --> 00:21:36,060
will happen what about the

00:21:34,440 --> 00:21:37,980
infrastructure all over around right

00:21:36,060 --> 00:21:39,570
it's much easier to just develop one

00:21:37,980 --> 00:21:40,830
thing at a time if you try to change the

00:21:39,570 --> 00:21:42,600
infrastructure at the same time as your

00:21:40,830 --> 00:21:44,010
algorithms that's not going to work so I

00:21:42,600 --> 00:21:46,110
think it's a very good idea to just go

00:21:44,010 --> 00:21:47,130
for one sprint or something that I don't

00:21:46,110 --> 00:21:48,840
know what system you're using

00:21:47,130 --> 00:21:50,640
that's hey okay we're gonna do a newer

00:21:48,840 --> 00:21:52,830
model deployment they were going to do

00:21:50,640 --> 00:21:54,390
an infrastructure around it as easy as

00:21:52,830 --> 00:21:55,980
possible it's actually very good to

00:21:54,390 --> 00:21:57,540
actually invest that time and effort

00:21:55,980 --> 00:21:58,160
into it because it's going to pay off in

00:21:57,540 --> 00:22:01,160
the future

00:21:58,160 --> 00:22:01,160
yep

00:22:04,440 --> 00:22:08,110
yeah

00:22:05,680 --> 00:22:10,600
so that is yeah so building your own

00:22:08,110 --> 00:22:11,920
model versus using the existing model

00:22:10,600 --> 00:22:13,330
here are you talking about the

00:22:11,920 --> 00:22:15,100
theoretical model itself or are you

00:22:13,330 --> 00:22:16,780
talking about using your own

00:22:15,100 --> 00:22:18,460
implementation of logistic regression

00:22:16,780 --> 00:22:20,020
instead of this part one so you're using

00:22:18,460 --> 00:22:23,500
a you're thinking about building your

00:22:20,020 --> 00:22:25,750
own what I found yeah so what I found

00:22:23,500 --> 00:22:28,780
from speaking to people in smaller and

00:22:25,750 --> 00:22:30,670
larger companies is that the bigger

00:22:28,780 --> 00:22:32,380
companies always almost always end up

00:22:30,670 --> 00:22:34,210
using their own because there is no

00:22:32,380 --> 00:22:35,530
infrastructure out there open-source

00:22:34,210 --> 00:22:37,180
that actually works in the scale that

00:22:35,530 --> 00:22:40,780
companies that with millions of clients

00:22:37,180 --> 00:22:43,030
say I want so I've talked with retail as

00:22:40,780 --> 00:22:45,010
well and in Spotify in other places and

00:22:43,030 --> 00:22:46,930
all of them end up building their own

00:22:45,010 --> 00:22:49,150
because it's impossible to do these

00:22:46,930 --> 00:22:50,830
things and there's so many moving parts

00:22:49,150 --> 00:22:53,650
that you end up breaking at some at some

00:22:50,830 --> 00:22:55,120
point always but for the smaller

00:22:53,650 --> 00:22:57,220
companies I think that they're good to

00:22:55,120 --> 00:22:59,110
try to just start with whatever they

00:22:57,220 --> 00:23:00,340
want but they should be aware that at

00:22:59,110 --> 00:23:03,960
some point they will probably have to

00:23:00,340 --> 00:23:03,960
roll their own everything basically

00:23:10,570 --> 00:23:16,429
no no so that is okay so if if it's a

00:23:15,200 --> 00:23:19,130
good idea to use different programming

00:23:16,429 --> 00:23:21,110
languages between training and producing

00:23:19,130 --> 00:23:22,850
the model the ideal thing is that you

00:23:21,110 --> 00:23:25,309
use as much code as possible in

00:23:22,850 --> 00:23:27,740
production or for serving the model as

00:23:25,309 --> 00:23:29,120
you do for training it so changing its

00:23:27,740 --> 00:23:31,519
changing language is changing the

00:23:29,120 --> 00:23:33,559
infrastructure same thing anything it

00:23:31,519 --> 00:23:36,200
does its add it just adds a lot more

00:23:33,559 --> 00:23:37,789
complexity and so one of the the

00:23:36,200 --> 00:23:38,960
articles that I've linked at at the end

00:23:37,789 --> 00:23:41,179
which is which is originating from

00:23:38,960 --> 00:23:42,980
google makes a specific point about all

00:23:41,179 --> 00:23:45,350
of this to increase the code reuse

00:23:42,980 --> 00:23:47,659
between these two parts of the pipeline

00:23:45,350 --> 00:23:50,809
as much as possible and also to make

00:23:47,659 --> 00:23:52,940
sure that the the difference in in the

00:23:50,809 --> 00:23:55,340
in the quality of your model when you

00:23:52,940 --> 00:23:56,899
train it and when you serve it is one of

00:23:55,340 --> 00:23:59,090
the most important indicators that you

00:23:56,899 --> 00:24:01,490
have for the quality so there are a lot

00:23:59,090 --> 00:24:03,169
of points to make about that as well so

00:24:01,490 --> 00:24:05,210
they need to be very active monitoring

00:24:03,169 --> 00:24:06,440
between the error that you get in the

00:24:05,210 --> 00:24:07,760
training and the error that you get in

00:24:06,440 --> 00:24:15,769
actual production because it can

00:24:07,760 --> 00:24:18,370
indicate a lot of problems can you speak

00:24:15,769 --> 00:24:18,370
up a little bit

00:24:34,669 --> 00:24:39,960
yeah it's definitely possible okay so

00:24:37,350 --> 00:24:41,580
the question is since a lot of models

00:24:39,960 --> 00:24:44,220
for example I'm guessing you're talking

00:24:41,580 --> 00:24:46,139
about in your networks here that can

00:24:44,220 --> 00:24:49,230
take days or weeks to train so if

00:24:46,139 --> 00:24:50,809
there's if it's any way to just make

00:24:49,230 --> 00:24:53,009
this whole pipeline go faster and

00:24:50,809 --> 00:24:54,389
usually I mean this is as in any

00:24:53,009 --> 00:24:55,860
computer science problem there's two

00:24:54,389 --> 00:24:57,990
ways to do that you can either add more

00:24:55,860 --> 00:24:59,820
hardware and hope that it works or you

00:24:57,990 --> 00:25:02,580
can do sampling and in machine learning

00:24:59,820 --> 00:25:04,470
the sampling part is it's so complicated

00:25:02,580 --> 00:25:06,840
that there's a whole subfield of science

00:25:04,470 --> 00:25:08,580
around around that so if you have

00:25:06,840 --> 00:25:10,320
samples from users you want to make sure

00:25:08,580 --> 00:25:11,549
that they are stratified and you're

00:25:10,320 --> 00:25:12,629
trying to predict I don't know a class

00:25:11,549 --> 00:25:15,389
you want to make sure that they're

00:25:12,629 --> 00:25:17,370
stratified by class as well so it's very

00:25:15,389 --> 00:25:20,009
very hard to actually get a true

00:25:17,370 --> 00:25:23,129
representative sample in order in order

00:25:20,009 --> 00:25:24,840
to train but there are cases where just

00:25:23,129 --> 00:25:28,620
taking a random sample from your data if

00:25:24,840 --> 00:25:30,269
you're looking into exploratory analysis

00:25:28,620 --> 00:25:31,679
it's not so important if you're not

00:25:30,269 --> 00:25:33,629
really worried about the quality of your

00:25:31,679 --> 00:25:35,399
modeling you just want to try out stuff

00:25:33,629 --> 00:25:37,320
it's better to just sample it down your

00:25:35,399 --> 00:25:38,879
data and use it in your own in your

00:25:37,320 --> 00:25:42,330
computer instead of running it on the

00:25:38,879 --> 00:25:44,009
cluster and iterate on that it's going

00:25:42,330 --> 00:25:46,110
to be much harder to try to do that with

00:25:44,009 --> 00:25:48,330
your complete data set so it's very

00:25:46,110 --> 00:25:50,070
important to just finish more as so to

00:25:48,330 --> 00:25:51,750
speak with your exploratory analysis as

00:25:50,070 --> 00:25:52,889
soon as possible and then you can go

00:25:51,750 --> 00:25:56,660
back and iterate through the whole

00:25:52,889 --> 00:26:00,829
pipeline again thank you

00:25:56,660 --> 00:26:00,829

YouTube URL: https://www.youtube.com/watch?v=tdtpa1e5fsU


