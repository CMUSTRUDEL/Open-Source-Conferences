Title: Democratizing Deep Learning with Tensorflow on Hops Hadoop
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Jim Dowling and Gautier Berthou

At: FOSDEM 2017

According to Andrej Kaparthy, there are four main factors holding back AI:Compute, Data, Algorithms, and Infrastructure. In this talk, we will show howwe attack the Data and Infrastructure challenges for Deep Learning.Specifically, we will show how we integrated Tensorflow with the world's mostscalable and human-friendly distribution of Hadoop, Hops (www.hops.io). Hopsis a new European distribution of Hadoop with a distributed metadataarchitecture and 16X the performance of HDFS. Hops also includes a human-friendly UI, called Hopsworks, with support for the Apache Zeppelin Notebook.We will show how users can run tensorflow programs in Apache Zeppelin on hugedatasets in Hadoop. Moreover, we will show how Hopsworks makes discovering anddownloading huge datasets a piece of cake with peer-to-peer sharing ofdatasets between Hopsworks clusters. Within minutes, you can installHopsworks, discover curated important datasets and download them to train DeepNeural networks using Tensorflow. Hops is the first Hadoop distribution tosupport Tensorflow. Hops and Hopsworks are both Apache v2 licensed projectsand have been developed primarily at KTH Royal Institute of Technology andSICS Swedish ICT in Stockholm.

According to Andrej Kaparthy, there are four main factors holding back AI:Compute, Data, Algorithms, and Infrastructure. In this talk, we will show howwe attack the Data and Infrastructure challenges for Deep Learning.Specifically, we will show how we integrated Tensorflow with the world's mostscalable and human-friendly distribution of Hadoop, Hops (www.hops.io). Hopsis a new European distribution of Hadoop with a distributed metadataarchitecture and 16X the performance of HDFS. Hops also includes a human-friendly UI, called Hopsworks, with support for the Apache Zeppelin Notebook.We will show how users can run tensorflow programs in Apache Zeppelin on hugedatasets in Hops Hadoop. Moreover, we will show how Hopsworks makesdiscovering and downloading huge datasets a piece of cake with custom peer-to-peer sharing of datasets between Hopsworks clusters. A new user can, withinminutes, install Hopsworks, discover curated important datasets and downloadthem to train Deep Neural networks using Tensorflow. Hops is the first Hadoopdistribution to support Tensorflow. Hopsworks itself is a self-service UI forHops Hadoop, that is based around projects, users, and dataset concepts. Userscollaborate in projects that contain datasets. Data owners can give usersaccess to process data (but not download it, copy it outside of the project,or cross-link it with data outside the project). Hopsworks, thus, providesstronger access control guarantees than are available in Hadoop, enablingsenstive data to securely reside on shared Hadoop clusters. Since April 2016,Hopsworks has provided Hadoop/Spark/Flink/Kafka-as-a-service to researchers inSweden from the Swedish ICT SICS Data Center at www.hops.site.

Hops and Hopsworks are both Apache v2 licensed projects and have beendeveloped primarily at KTH Royal Institute of Technology and SICS Swedish ICTin Stockholm.


Room: H.2213
Scheduled start: 2017-02-04 16:00:00
Captions: 
	00:00:04,760 --> 00:00:09,780
note here and he will be talking about

00:00:07,170 --> 00:00:12,720
one of the most exciting and hot areas

00:00:09,780 --> 00:00:16,219
in big data these days which is deep

00:00:12,720 --> 00:00:16,219
learning yeah

00:00:16,580 --> 00:00:24,900
so I'm going to be a researcher at

00:00:20,869 --> 00:00:27,119
Swedish Institute of computer science I

00:00:24,900 --> 00:00:29,670
was supposed to do the presentation with

00:00:27,119 --> 00:00:35,449
Jim during that he couldn't come here

00:00:29,670 --> 00:00:39,690
today so what I would speak about is

00:00:35,449 --> 00:00:42,450
deep learning on the platform we are

00:00:39,690 --> 00:00:45,719
developing can help people to start with

00:00:42,450 --> 00:00:49,020
deep learning because the thing is that

00:00:45,719 --> 00:00:51,809
my team at the Swedish Institute of

00:00:49,020 --> 00:00:54,030
computer science and we have created a

00:00:51,809 --> 00:01:00,329
start-up and we are what we are doing is

00:00:54,030 --> 00:01:02,879
developing a new adult distribution it's

00:01:00,329 --> 00:01:05,220
the first European Arab distribution on

00:01:02,879 --> 00:01:10,470
somatic aspect of it can be very useful

00:01:05,220 --> 00:01:13,740
for starting with deep learning to start

00:01:10,470 --> 00:01:16,350
with raise a question what are the

00:01:13,740 --> 00:01:19,680
factors that all back artificial

00:01:16,350 --> 00:01:21,840
intelligence today if we look at bender

00:01:19,680 --> 00:01:26,850
we can think that it's alcohol on sex

00:01:21,840 --> 00:01:29,070
but according to andre capaci I don't

00:01:26,850 --> 00:01:34,229
exactly know you pronounce his name if

00:01:29,070 --> 00:01:37,320
you don't know him he is working at open

00:01:34,229 --> 00:01:39,659
ie he on his blog is a quite entry on

00:01:37,320 --> 00:01:43,229
blog in deep learning so this situation

00:01:39,659 --> 00:01:45,619
is taken from his blog and they say that

00:01:43,229 --> 00:01:48,420
there is four factor that all back

00:01:45,619 --> 00:01:51,180
artificial intelligence first one is a

00:01:48,420 --> 00:01:52,799
compute its a compute power we have so

00:01:51,180 --> 00:01:56,970
this is ad where this is our imagination

00:01:52,799 --> 00:02:00,600
you are our power for our GPU CPU on

00:01:56,970 --> 00:02:03,630
things like that the second one is the

00:02:00,600 --> 00:02:06,719
data is that to do deep longing you do

00:02:03,630 --> 00:02:10,799
you need big data you need big data that

00:02:06,719 --> 00:02:13,410
are clean that right now it's not so

00:02:10,799 --> 00:02:16,080
easy to find them the that are kind of

00:02:13,410 --> 00:02:17,910
all over the place on internet don't

00:02:16,080 --> 00:02:19,740
really know where to find them and you

00:02:17,910 --> 00:02:22,050
when you find some data you don't know

00:02:19,740 --> 00:02:26,550
if they are really clean something like

00:02:22,050 --> 00:02:31,680
that then the third component it is

00:02:26,550 --> 00:02:35,300
algorithm so it's what you will do if

00:02:31,680 --> 00:02:38,520
you work on if you want to develop new

00:02:35,300 --> 00:02:41,100
artificial intelligence system and the

00:02:38,520 --> 00:02:46,470
last one is infrastructure is what do

00:02:41,100 --> 00:02:49,380
you run your deep learning system on so

00:02:46,470 --> 00:02:51,200
is the operating system but also where

00:02:49,380 --> 00:02:55,500
you store your data on thing like that

00:02:51,200 --> 00:02:57,750
on what Oh platform address the point

00:02:55,500 --> 00:03:00,840
that all perform address is the data

00:02:57,750 --> 00:03:05,520
problem on the infrastructure problem so

00:03:00,840 --> 00:03:08,010
we'll start with the data problem so the

00:03:05,520 --> 00:03:10,800
data challenge how do you find good on

00:03:08,010 --> 00:03:13,470
it using data okay if you are in a big

00:03:10,800 --> 00:03:15,150
company your Spotify or you have the

00:03:13,470 --> 00:03:19,920
data that house they own you can use

00:03:15,150 --> 00:03:24,390
them but if you are a researcher and you

00:03:19,920 --> 00:03:26,430
want to experiment with data you don't

00:03:24,390 --> 00:03:29,490
necessarily have them there and you look

00:03:26,430 --> 00:03:31,380
for them and then it's right now it's

00:03:29,490 --> 00:03:34,320
kind of going to e care but going

00:03:31,380 --> 00:03:35,700
directly to the last part you have data

00:03:34,320 --> 00:03:38,160
all over the place

00:03:35,700 --> 00:03:40,380
they are all in box you have to find the

00:03:38,160 --> 00:03:43,410
box that go together you have to bring

00:03:40,380 --> 00:03:45,510
that out on by yourself and then you

00:03:43,410 --> 00:03:50,670
build it and then you find out that oh

00:03:45,510 --> 00:03:54,450
it's not what I wanted so so how can we

00:03:50,670 --> 00:03:57,090
help with that yeah and then what our

00:03:54,450 --> 00:03:59,190
problem now as I say you bring it back

00:03:57,090 --> 00:04:01,170
home how do you do that right now is

00:03:59,190 --> 00:04:02,910
that we don't see but it's not so

00:04:01,170 --> 00:04:06,209
important it's a line of code that tell

00:04:02,910 --> 00:04:09,090
you how to download some data from from

00:04:06,209 --> 00:04:11,670
s3 on most of the time you have to run

00:04:09,090 --> 00:04:14,160
it 100 time to have all the data on if

00:04:11,670 --> 00:04:17,760
one of them crash in the middle you have

00:04:14,160 --> 00:04:22,370
to do it manually to find out we are the

00:04:17,760 --> 00:04:25,070
student a PhD student downloading

00:04:22,370 --> 00:04:29,040
whether that

00:04:25,070 --> 00:04:31,950
it took him one week the first time and

00:04:29,040 --> 00:04:33,510
then we implemented the client to

00:04:31,950 --> 00:04:38,250
download the data and it went a lot

00:04:33,510 --> 00:04:39,900
faster but underneath each you have data

00:04:38,250 --> 00:04:42,090
and you want to make them public how do

00:04:39,900 --> 00:04:44,700
you do that there is no straightforward

00:04:42,090 --> 00:04:50,460
solution now you can okay you can put

00:04:44,700 --> 00:04:54,060
them on Amazon or on Google cloud but

00:04:50,460 --> 00:04:57,990
then we pay for that so we pay for each

00:04:54,060 --> 00:04:59,790
time someone download and then you have

00:04:57,990 --> 00:05:03,570
to make some webpage to publish

00:04:59,790 --> 00:05:05,550
something like that there isn't no very

00:05:03,570 --> 00:05:09,000
nice platform where in few click you can

00:05:05,550 --> 00:05:12,750
say okay now my data appealing and this

00:05:09,000 --> 00:05:17,630
is what this is one of the thing we do

00:05:12,750 --> 00:05:22,260
in no platform I wanted to make a demo

00:05:17,630 --> 00:05:25,320
but the problem is that for I can't

00:05:22,260 --> 00:05:30,120
connect my computer to the video

00:05:25,320 --> 00:05:32,460
projector on the demo so I will describe

00:05:30,120 --> 00:05:34,560
the demo on so some squid short because

00:05:32,460 --> 00:05:39,900
the demo is dois is running we have

00:05:34,560 --> 00:05:42,240
deployed to the automation in my lab and

00:05:39,900 --> 00:05:45,360
I need my computer to access to them

00:05:42,240 --> 00:05:47,460
because of firewall problem but so the

00:05:45,360 --> 00:05:52,919
idea that we are to the optimizing with

00:05:47,460 --> 00:05:55,710
a full stack with the hop stack so which

00:05:52,919 --> 00:06:01,440
contain we wished all the data in HDFS

00:05:55,710 --> 00:06:03,450
and then use Y on thing like that so

00:06:01,440 --> 00:06:06,150
it's a full atop stack and we have data

00:06:03,450 --> 00:06:10,680
in that on we want to share them and we

00:06:06,150 --> 00:06:14,190
have a nice interface to see your data

00:06:10,680 --> 00:06:15,630
in the in the HDFS I will show it a

00:06:14,190 --> 00:06:18,330
little more when I speak over the

00:06:15,630 --> 00:06:24,660
infrastructure but we have data in one

00:06:18,330 --> 00:06:27,300
of the via virtual machine and what we

00:06:24,660 --> 00:06:30,290
do is just we right-click on them on say

00:06:27,300 --> 00:06:34,530
shell is that other is a shelter that

00:06:30,290 --> 00:06:37,080
and then this data now public they have

00:06:34,530 --> 00:06:38,100
a description which is a file Winley

00:06:37,080 --> 00:06:41,100
file that is in

00:06:38,100 --> 00:06:45,300
the in the folder on that describe what

00:06:41,100 --> 00:06:47,520
are in the data and then we on the other

00:06:45,300 --> 00:06:51,000
via termination we can search for the

00:06:47,520 --> 00:06:53,070
name of this data or for something in

00:06:51,000 --> 00:06:57,050
the description we use elastic search

00:06:53,070 --> 00:07:01,140
for the research and we will find the

00:06:57,050 --> 00:07:03,600
will get the result of all the the

00:07:01,140 --> 00:07:08,760
public that I said that out there and we

00:07:03,600 --> 00:07:12,870
will say okay download it screenshot are

00:07:08,760 --> 00:07:15,300
very bad when we download it we say to

00:07:12,870 --> 00:07:18,960
which project we want to add them and

00:07:15,300 --> 00:07:21,900
then if it's HDFS we can directly dump

00:07:18,960 --> 00:07:24,060
it to HDFS so if it's stuff that are the

00:07:21,900 --> 00:07:27,660
good format for Kafka we can also put

00:07:24,060 --> 00:07:31,560
them in Kafka and once we have done that

00:07:27,660 --> 00:07:34,860
it stopped downloading and we can start

00:07:31,560 --> 00:07:37,650
proceeding on them how does it work in

00:07:34,860 --> 00:07:42,330
the bed the background is it's a

00:07:37,650 --> 00:07:45,420
peer-to-peer torrent like protocol so if

00:07:42,330 --> 00:07:47,880
a lot of people have these data set in

00:07:45,420 --> 00:07:54,000
the cluster then it will go faster to

00:07:47,880 --> 00:07:57,060
delay them and then we use the lab dot

00:07:54,000 --> 00:08:00,660
for let bat for the transport protocol

00:07:57,060 --> 00:08:03,870
the idea is that left at it is known as

00:08:00,660 --> 00:08:06,150
creative so we know that if you have

00:08:03,870 --> 00:08:08,670
that I said that our public it may not

00:08:06,150 --> 00:08:10,740
be your priority to upload them to

00:08:08,670 --> 00:08:12,570
people that want to download them your

00:08:10,740 --> 00:08:16,440
priority is to serve the people that use

00:08:12,570 --> 00:08:18,320
your cluster directly so let bat is good

00:08:16,440 --> 00:08:23,070
for that because it will use only the

00:08:18,320 --> 00:08:25,260
bandwidth that is not used by TCP and so

00:08:23,070 --> 00:08:27,450
if people are not using your cluster it

00:08:25,260 --> 00:08:29,490
will be able to upload at maximum speed

00:08:27,450 --> 00:08:31,280
but as soon as you have TCP connections

00:08:29,490 --> 00:08:35,130
that start to use your cluster it will

00:08:31,280 --> 00:08:38,610
go down on useless bond with the other

00:08:35,130 --> 00:08:42,599
interests of lab rat is that it works a

00:08:38,610 --> 00:08:46,260
lot better than the anticip you eyelet

00:08:42,599 --> 00:08:48,270
on CI bond with connection which are the

00:08:46,260 --> 00:08:50,610
connection you have when you have

00:08:48,270 --> 00:08:52,620
cluster on both sides of the Atlantic

00:08:50,610 --> 00:08:57,870
sourcing like that

00:08:52,620 --> 00:09:00,990
I had number that because the problem

00:08:57,870 --> 00:09:05,640
with TCP is that the congestion windows

00:09:00,990 --> 00:09:08,060
is patty reacted to a latency and so

00:09:05,640 --> 00:09:12,150
when the latency increases the

00:09:08,060 --> 00:09:14,700
throughput at which TCP can transmit

00:09:12,150 --> 00:09:18,360
over on the network is going down let

00:09:14,700 --> 00:09:20,580
that is is measuring the latency on

00:09:18,360 --> 00:09:24,630
these looking at the change in latency

00:09:20,580 --> 00:09:29,390
to know if it should decrease or

00:09:24,630 --> 00:09:29,390
increase the throughput at which its on

00:09:29,630 --> 00:09:35,220
so and this has also the advantage that

00:09:32,970 --> 00:09:37,770
it's under better packet loss

00:09:35,220 --> 00:09:41,100
because it will not take a packet loss

00:09:37,770 --> 00:09:43,860
as okay now I need to decrease the size

00:09:41,100 --> 00:09:45,930
of my window by half it will say okay

00:09:43,860 --> 00:09:47,730
there is one packet that was lost but

00:09:45,930 --> 00:09:49,500
globally the latency is the same so

00:09:47,730 --> 00:09:52,400
maybe it was just these packets which

00:09:49,500 --> 00:09:56,460
can happen on very lonely

00:09:52,400 --> 00:09:59,550
lastly we don't load the piece in order

00:09:56,460 --> 00:10:06,120
so you can start to process what you are

00:09:59,550 --> 00:10:09,180
downloading while it's downloading so

00:10:06,120 --> 00:10:10,890
now for the infrastructure part so if

00:10:09,180 --> 00:10:13,860
you want to develop artificial

00:10:10,890 --> 00:10:15,900
intelligence on you want to be to do the

00:10:13,860 --> 00:10:16,950
artificial entities on nose alcoholism

00:10:15,900 --> 00:10:19,110
and stuff like that you don't

00:10:16,950 --> 00:10:20,700
necessarily want to deal with all the

00:10:19,110 --> 00:10:27,720
infrastructure and all that you don't

00:10:20,700 --> 00:10:30,650
want to be this guide for that we

00:10:27,720 --> 00:10:36,380
propose to use a ton software on a Duke

00:10:30,650 --> 00:10:40,160
so white onshore flow if you were there

00:10:36,380 --> 00:10:43,310
to presentation ago it was two or three

00:10:40,160 --> 00:10:47,430
you saw that there is a lot of existing

00:10:43,310 --> 00:10:49,470
system to do deep learning which will

00:10:47,430 --> 00:10:53,240
start off low because it's the big one

00:10:49,470 --> 00:10:57,560
right now is the one everybody use it's

00:10:53,240 --> 00:10:59,670
Google behind it so I guess it's helped

00:10:57,560 --> 00:11:01,560
personally more developing the I do part

00:10:59,670 --> 00:11:04,500
so I don't know so much about it tons of

00:11:01,560 --> 00:11:09,370
flow sorry about that

00:11:04,500 --> 00:11:13,120
and then we think that a dupe is a it's

00:11:09,370 --> 00:11:17,110
good for because you want to have big

00:11:13,120 --> 00:11:19,590
data at the beginning the data you need

00:11:17,110 --> 00:11:19,590
to Train

00:11:25,600 --> 00:11:27,660
you

00:11:31,600 --> 00:11:43,269
your loop is not so good it's the reason

00:11:40,089 --> 00:11:45,009
is that the way people do it is that

00:11:43,269 --> 00:11:47,050
they have the data in I do they have

00:11:45,009 --> 00:11:49,420
tons of ruin on another machine and they

00:11:47,050 --> 00:11:50,230
take the data to weapons of fluids they

00:11:49,420 --> 00:11:53,680
went on the floor

00:11:50,230 --> 00:11:55,779
they run the algorithm and then they put

00:11:53,680 --> 00:12:03,130
back the result in art this is not

00:11:55,779 --> 00:12:05,769
efficient luckily they have been now it

00:12:03,130 --> 00:12:07,899
has been developed system so that you

00:12:05,769 --> 00:12:10,209
can run tons of flu use that as a tear

00:12:07,899 --> 00:12:12,660
in a dupe and you know platform we have

00:12:10,209 --> 00:12:16,000
integrated all of this together so that

00:12:12,660 --> 00:12:19,630
people can directly run the are terms of

00:12:16,000 --> 00:12:22,060
flow job in Japan on using data in

00:12:19,630 --> 00:12:26,440
Hadoop without having to do all this

00:12:22,060 --> 00:12:30,040
transfer it's not perfect yet there are

00:12:26,440 --> 00:12:33,880
still a lot of work to do and this will

00:12:30,040 --> 00:12:40,420
be part of a future work so I will just

00:12:33,880 --> 00:12:52,240
do a quick demo for system the

00:12:40,420 --> 00:12:59,259
resolution will be shipped so all we

00:12:52,240 --> 00:13:13,269
like so the thing is that we we run this

00:12:59,259 --> 00:13:15,639
platform so the thing is that we have

00:13:13,269 --> 00:13:20,800
the data center in luleÃ¥ in the north of

00:13:15,639 --> 00:13:25,350
Sweden the same place where I do pub the

00:13:20,800 --> 00:13:25,350
worst Facebook as a European data center

00:13:30,360 --> 00:13:42,230
I just want to zoom out Mitch okay oops

00:13:42,350 --> 00:13:45,409
[Music]

00:13:56,870 --> 00:14:19,370
yeah but I use the Swedish okay so so

00:14:16,280 --> 00:14:29,910
the idea that we have a front on for

00:14:19,370 --> 00:14:33,120
Duke system and we have a project system

00:14:29,910 --> 00:14:36,330
where everything in a dupe is a project

00:14:33,120 --> 00:14:39,030
on inside project you have users that

00:14:36,330 --> 00:14:42,900
have different role and we unsure is

00:14:39,030 --> 00:14:45,450
relation between the project so if you

00:14:42,900 --> 00:14:47,010
have data in a dupe on the part of a

00:14:45,450 --> 00:14:48,720
project you know that people that are

00:14:47,010 --> 00:14:58,160
not part of this project contacts as to

00:14:48,720 --> 00:15:01,140
it and then we can go to space nets it's

00:14:58,160 --> 00:15:05,310
so on this data center we get access to

00:15:01,140 --> 00:15:11,340
of student of the deep learning course

00:15:05,310 --> 00:15:15,800
and they did their homework and on it on

00:15:11,340 --> 00:15:25,350
one of the group did the space net

00:15:15,800 --> 00:15:27,060
challenge so it's the idea on again when

00:15:25,350 --> 00:15:35,750
they run it on the cluster they just

00:15:27,060 --> 00:15:35,750
went to Zeppelin on in the plane

00:15:36,650 --> 00:15:44,150
creating not book on the idea of space

00:15:39,090 --> 00:15:44,150
net is that it's a satellite picture of

00:15:47,420 --> 00:15:53,700
so it's not a notebook this is all terms

00:15:50,250 --> 00:15:56,570
of Records understand picture so it's

00:15:53,700 --> 00:15:59,540
some satellite picture of the

00:15:56,570 --> 00:16:02,900
we're on the goal is to recognize if

00:15:59,540 --> 00:16:04,430
there is building in the picture you can

00:16:02,900 --> 00:16:06,410
see that when there is building it's

00:16:04,430 --> 00:16:19,430
easily the part where we're building up

00:16:06,410 --> 00:16:24,220
I want to first so so you can see with

00:16:19,430 --> 00:16:27,500
this platform it's easy to download new

00:16:24,220 --> 00:16:29,920
data set on to start working on the with

00:16:27,500 --> 00:16:29,920
Zeppelin

00:16:39,730 --> 00:16:44,500
no stinking thing

00:16:46,450 --> 00:16:53,130
[Laughter]

00:16:50,040 --> 00:16:53,130
[Music]

00:16:53,330 --> 00:17:01,079
[Laughter]

00:16:58,070 --> 00:17:04,000
[Music]

00:17:01,079 --> 00:17:05,500
so future work I only wrote the future

00:17:04,000 --> 00:17:10,449
work concerning terms of flow because

00:17:05,500 --> 00:17:13,990
it's the only spot I spoke about for

00:17:10,449 --> 00:17:16,240
platform so something we are working on

00:17:13,990 --> 00:17:19,360
right now is a distributed sense of flow

00:17:16,240 --> 00:17:22,839
so this exists but this is the we don't

00:17:19,360 --> 00:17:26,020
think that the implementation is so good

00:17:22,839 --> 00:17:29,470
right now and it's not it doesn't work

00:17:26,020 --> 00:17:31,210
on yawn and as we want to run it on top

00:17:29,470 --> 00:17:33,460
of a dupe we want to run it on top of

00:17:31,210 --> 00:17:38,140
young so we have some people working on

00:17:33,460 --> 00:17:39,880
that making that you can you will be

00:17:38,140 --> 00:17:42,970
able in the future to say okay I want to

00:17:39,880 --> 00:17:45,130
cluster with these machines and I want

00:17:42,970 --> 00:17:49,230
to end distance or flow program on it

00:17:45,130 --> 00:17:49,230
and then yon should take you out of that

00:17:50,130 --> 00:17:55,960
then that means also adding some thing

00:17:53,530 --> 00:17:59,409
too young because we want to be able to

00:17:55,960 --> 00:18:02,650
and within feed even on GPUs which young

00:17:59,409 --> 00:18:07,630
doesn't underwrite now right now Yun and

00:18:02,650 --> 00:18:09,309
Leon the CPU on memory so we will we we

00:18:07,630 --> 00:18:16,480
are also working on adding this

00:18:09,309 --> 00:18:20,890
information into yarn so I was saying we

00:18:16,480 --> 00:18:24,150
have a cluster up in Lille oh it's

00:18:20,890 --> 00:18:28,510
already used so it's code that that are

00:18:24,150 --> 00:18:32,740
that is in prediction that student on

00:18:28,510 --> 00:18:39,039
research are using if you want to test

00:18:32,740 --> 00:18:44,380
your code or platform you can either go

00:18:39,039 --> 00:18:47,830
to hops dot IO and then you go you have

00:18:44,380 --> 00:18:51,150
vagrant you can deploy the full tak with

00:18:47,830 --> 00:18:51,150
a quantum start to play with it

00:18:51,510 --> 00:18:55,990
we can also give you access to cluster

00:18:54,700 --> 00:18:57,970
in Lunia

00:18:55,990 --> 00:18:59,110
but you won't have access to the GPU

00:18:57,970 --> 00:19:01,030
cards

00:18:59,110 --> 00:19:06,700
because we don't have enough so we keep

00:19:01,030 --> 00:19:09,670
them for still then I just want to thank

00:19:06,700 --> 00:19:14,410
all these people so we have a lot of

00:19:09,670 --> 00:19:16,299
enemy because we are research people so

00:19:14,410 --> 00:19:18,580
it's a lot of students that I've been

00:19:16,299 --> 00:19:21,150
working with us and that I've done the

00:19:18,580 --> 00:19:23,770
very hard work but as you see we are

00:19:21,150 --> 00:19:29,950
quite a small team working on this

00:19:23,770 --> 00:19:33,330
project and we we created a startup to

00:19:29,950 --> 00:19:36,700
develop it it's called logical clocks

00:19:33,330 --> 00:19:40,179
and then we are looking for people that

00:19:36,700 --> 00:19:43,179
want to either use on or system on give

00:19:40,179 --> 00:19:46,780
us feedback or develop it it's all open

00:19:43,179 --> 00:19:50,850
so you can go on github or ops to

00:19:46,780 --> 00:19:50,850
Taiyuan have more information about this

00:19:52,530 --> 00:20:00,240
[Applause]

00:20:08,130 --> 00:20:12,240

YouTube URL: https://www.youtube.com/watch?v=sWApuYwlyKY


