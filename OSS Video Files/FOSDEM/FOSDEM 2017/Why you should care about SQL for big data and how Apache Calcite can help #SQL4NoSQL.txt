Title: Why you should care about SQL for big data and how Apache Calcite can help #SQL4NoSQL
Publication date: 2018-03-06
Playlist: FOSDEM 2017
Description: 
	by Christian  Tzolov

At: FOSDEM 2017

When working with BigData & IoT systems we often feel the need for a CommonQuery Language. The platform specific languages are often harder to integratewith and require longer adoption time.

To fill this gap many NoSql (Not-only-Sql) vendors are building SQL layers fortheir platforms. It is worth exploring the driving forces behind this trend,how it fits in your BigData stacks and how we can adopt it in our favoritetools. However building SQL engine from scratch is a daunting job andframeworks like Apache Calcite can help you with the heavy lifting. Calciteallow you to integrate SQL parser, cost-based optimizer, and JDBC with yourbig data system.

Calcite has been used to empower many Big-Data platforms such as Hive, Spark,Drill Phoenix to name some.

I will walk you through the process of building a SQL access layer for ApacheGeode (In-Memory Data Grid). I will share my experience, pitfalls andtechnical consideration like balancing between the SQL/RDBMS semantics and thedesign choices and limitations of the data system.

Hopefully this will enable you to add SQL capabilities to your prefered NoSQLdata system.

❯ It will be interesting to see what happens if an established NoSQL databasedecides to implement a reasonably standard SQL; the only predictable outcomefor such an eventuality is plenty of argument. \- NoSQL Distilled, MartinFowler - 2012

The Relational Databases (RDBMS) are an essential component of the computingecosystem. Yet in the past decade we have witnessed a wave of alternative datamanagement technologies often branded as NoSQL and BigData - an ambiguous andlacking prescriptive definition names.

To understand the NoSQL/BigData "movement" one need to understand the forcesfueling it:

  * The rise of Internet (Web, Mobile, IoT...) leading to Data {Volume, Velocity and Variety} challenges  * Object-relational impedance mismatch  * Cloud computing - Infrastructure Automation and Elasticity  * Shift from Integration to Application databases  * Data-Value vs. Storage-Cost Economics Shift

The various approaches in addressing those challenges have led to a multitudeof over 150 commercially supported NoSQL/BigData platforms.

Such diversity means that an organization will adopt a mixture of data storagetechnologies for handling different circumstances (Polyglot Persistence).

How does an organization integrate the mix of data technologies?

To fill the gap many NoSql/BigData vendors are (or are considering) buildingSQL and SQL-based layers for their platforms.

It is worth exploring the driving forces behind this trend ...


Room: H.2213
Scheduled start: 2017-02-04 17:30:00
Captions: 
	00:00:37,150 --> 00:00:43,160
good evening everyone can you hear me

00:00:39,769 --> 00:00:45,650
it's okay can you hear me

00:00:43,160 --> 00:00:47,269
I appreciate that indeed that you stay

00:00:45,650 --> 00:00:48,680
tuned to the light presentation and

00:00:47,269 --> 00:00:50,379
higher do realize that I'm the last

00:00:48,680 --> 00:00:53,089
person standing between you and

00:00:50,379 --> 00:00:57,080
reception I guess beer so now try to

00:00:53,089 --> 00:00:58,820
keep it short I work at people time is

00:00:57,080 --> 00:01:02,119
near there there were a focus on big

00:00:58,820 --> 00:01:04,100
data distributed systems I'm privileged

00:01:02,119 --> 00:01:05,810
to spend enough time on working on open

00:01:04,100 --> 00:01:09,340
source project I am a geometer and peace

00:01:05,810 --> 00:01:13,100
PMC member for a particular project

00:01:09,340 --> 00:01:15,200
today I'm going to talk about two

00:01:13,100 --> 00:01:17,299
interesting trends one is has happened

00:01:15,200 --> 00:01:18,890
in the last decade the last 10 years

00:01:17,299 --> 00:01:21,409
this is the no Seco in big data the

00:01:18,890 --> 00:01:23,689
other one is the implementation of sicko

00:01:21,409 --> 00:01:25,910
interfaces for various notes no secure

00:01:23,689 --> 00:01:28,700
data systems which seem to be happening

00:01:25,910 --> 00:01:31,580
it's a big team in the last year in year

00:01:28,700 --> 00:01:34,930
and a half and this is a quote I found

00:01:31,580 --> 00:01:38,390
from Martin for work from 2012 where he

00:01:34,930 --> 00:01:40,040
predicts that if it's ever if ever

00:01:38,390 --> 00:01:41,480
sequel gets implemented on top of

00:01:40,040 --> 00:01:43,909
establishing no sequel this at least

00:01:41,480 --> 00:01:46,040
would generate plenty of arguments and I

00:01:43,909 --> 00:01:48,470
believe this talk is one of those going

00:01:46,040 --> 00:01:51,860
to be one of those arguments so a little

00:01:48,470 --> 00:01:56,240
bit of context the big data and no

00:01:51,860 --> 00:01:59,420
secure and non-secure big bank we've

00:01:56,240 --> 00:02:01,760
been so this is a landscape of Big Data

00:01:59,420 --> 00:02:03,500
technologies for the last year you see

00:02:01,760 --> 00:02:05,870
actually from according to some sources

00:02:03,500 --> 00:02:07,640
we have over 350 commercially supported

00:02:05,870 --> 00:02:10,540
mochiko and big data systems out there

00:02:07,640 --> 00:02:12,470
so what actually explains this explosion

00:02:10,540 --> 00:02:14,420
considering that for decades we've been

00:02:12,470 --> 00:02:17,659
happy or some people have been happy

00:02:14,420 --> 00:02:20,420
with relational databases it's difficult

00:02:17,659 --> 00:02:22,340
to judge by it by by the name of those

00:02:20,420 --> 00:02:25,549
projects no sequel and actually the

00:02:22,340 --> 00:02:27,769
strange no sequel big data they really

00:02:25,549 --> 00:02:29,510
don't write any prescriptive definitions

00:02:27,769 --> 00:02:31,849
but if you think of them more like a

00:02:29,510 --> 00:02:33,630
movement and try to understand what are

00:02:31,849 --> 00:02:35,850
the driving forces behind them

00:02:33,630 --> 00:02:38,040
I think this helps to position them and

00:02:35,850 --> 00:02:41,640
to understand why they're so vibrant and

00:02:38,040 --> 00:02:44,190
what makes them makes them think tick on

00:02:41,640 --> 00:02:45,570
the first place I think there's some

00:02:44,190 --> 00:02:47,400
concession about this one of the main

00:02:45,570 --> 00:02:48,660
driving forces behind those technologies

00:02:47,400 --> 00:02:51,060
and the boom of those technologies is

00:02:48,660 --> 00:02:53,670
the boom of Internet itself the fact

00:02:51,060 --> 00:02:55,200
that explosion of what mobile

00:02:53,670 --> 00:02:58,500
technologies Internet of Things

00:02:55,200 --> 00:03:00,000
practically any device now even smaller

00:02:58,500 --> 00:03:02,250
one is the source of information and

00:03:00,000 --> 00:03:06,650
generate information and the this in

00:03:02,250 --> 00:03:08,970
turn generates a cause three main

00:03:06,650 --> 00:03:10,770
challenges this is how to handle the

00:03:08,970 --> 00:03:13,770
volume of data the velocity of data and

00:03:10,770 --> 00:03:17,670
the variety of all these data sources so

00:03:13,770 --> 00:03:19,350
and Internet is the cause for these

00:03:17,670 --> 00:03:20,790
technologies or to fulfill the demand

00:03:19,350 --> 00:03:22,320
for those challenges the other hand it

00:03:20,790 --> 00:03:23,910
is actually the solution as well in

00:03:22,320 --> 00:03:26,580
order to handle to scale to provide

00:03:23,910 --> 00:03:28,770
technologies that can handle and address

00:03:26,580 --> 00:03:30,780
those challenges you are going to use

00:03:28,770 --> 00:03:32,910
again distributed systems and in turn

00:03:30,780 --> 00:03:35,070
using distributed system for dos for

00:03:32,910 --> 00:03:37,940
resolving those challenges means that

00:03:35,070 --> 00:03:40,230
some of the already presumed

00:03:37,940 --> 00:03:42,570
technologies and a guarantee is that

00:03:40,230 --> 00:03:45,060
relational databases provide like asset

00:03:42,570 --> 00:03:46,770
or - two-phase commit would be

00:03:45,060 --> 00:03:48,840
challenged and has to be addressed in

00:03:46,770 --> 00:03:51,390
different way it's known for example

00:03:48,840 --> 00:03:53,490
that the two-phase commit would Hank in

00:03:51,390 --> 00:03:56,510
- in certain cases in failure

00:03:53,490 --> 00:04:00,950
distributed system so all new class of

00:03:56,510 --> 00:04:04,140
approaches not for example the

00:04:00,950 --> 00:04:05,640
consistency and availability in case of

00:04:04,140 --> 00:04:09,120
partitioning of distributed system this

00:04:05,640 --> 00:04:11,010
is the cap or theorem is are meant to do

00:04:09,120 --> 00:04:13,110
and this is part of this tributed system

00:04:11,010 --> 00:04:17,100
and many of the technologies that that

00:04:13,110 --> 00:04:20,190
that implements this Paxos on the other

00:04:17,100 --> 00:04:22,890
hand and as I mentioned the two-phase

00:04:20,190 --> 00:04:25,140
commit is not really reliable approach -

00:04:22,890 --> 00:04:27,660
to ensure consistency within the

00:04:25,140 --> 00:04:29,970
distributed environment so a class of

00:04:27,660 --> 00:04:32,520
consist consensus based quorum based

00:04:29,970 --> 00:04:34,230
systems like taxes heavy merchants and

00:04:32,520 --> 00:04:35,630
many of the NAS ecosystems actually are

00:04:34,230 --> 00:04:39,780
based on those type of technologies

00:04:35,630 --> 00:04:41,640
another not that popular but I think

00:04:39,780 --> 00:04:43,140
very important driving forces the

00:04:41,640 --> 00:04:44,490
so-called

00:04:43,140 --> 00:04:46,550
relational or object-relational

00:04:44,490 --> 00:04:48,470
impedance mismatch

00:04:46,550 --> 00:04:50,210
which a relational database I'm not sure

00:04:48,470 --> 00:04:51,800
many of you maybe have done some

00:04:50,210 --> 00:04:54,470
application development they know that

00:04:51,800 --> 00:04:56,479
for some class of applications you just

00:04:54,470 --> 00:04:59,330
need to persist your application state

00:04:56,479 --> 00:05:01,880
which mark is it very often is a object

00:04:59,330 --> 00:05:03,319
based state into relational database for

00:05:01,880 --> 00:05:05,569
this you would need some sort of ORM

00:05:03,319 --> 00:05:07,580
technologies which is unnecessary for

00:05:05,569 --> 00:05:10,849
many use cases as I mentioned and this

00:05:07,580 --> 00:05:12,949
gave birth to technologies like or

00:05:10,849 --> 00:05:14,599
document based datastore this actually

00:05:12,949 --> 00:05:15,130
is a huge group of technologies out

00:05:14,599 --> 00:05:18,139
there

00:05:15,130 --> 00:05:20,389
driven by by this mismatch furthermore

00:05:18,139 --> 00:05:22,250
different type of stores and demands

00:05:20,389 --> 00:05:24,759
like graph based databases or full-text

00:05:22,250 --> 00:05:26,930
search where the traditional

00:05:24,759 --> 00:05:29,449
representation of database models in

00:05:26,930 --> 00:05:31,610
strict very often the raw base the

00:05:29,449 --> 00:05:35,000
relational model is not very very

00:05:31,610 --> 00:05:36,560
appropriate to to do it and there many

00:05:35,000 --> 00:05:39,380
other factors but I think those three

00:05:36,560 --> 00:05:41,210
kind of are powerful enough to to

00:05:39,380 --> 00:05:43,250
explain why there is such a search of

00:05:41,210 --> 00:05:45,500
technologies out there last one I want

00:05:43,250 --> 00:05:48,919
to talk is the cloud computing race and

00:05:45,500 --> 00:05:50,539
wife itself the possibilities to program

00:05:48,919 --> 00:05:51,800
the infrastructure actually talk to make

00:05:50,539 --> 00:05:54,530
this infrastructure to have

00:05:51,800 --> 00:05:57,590
infrastructure on demand is a main

00:05:54,530 --> 00:05:59,570
driver for eliminating the operational

00:05:57,590 --> 00:06:01,370
complexity and the cost and there is

00:05:59,570 --> 00:06:04,009
another side effect of this and this

00:06:01,370 --> 00:06:06,229
market textural there is this sub sub

00:06:04,009 --> 00:06:07,969
sub movement I would say which is shift

00:06:06,229 --> 00:06:09,770
from integration to application type of

00:06:07,969 --> 00:06:13,250
databases this is very popular into the

00:06:09,770 --> 00:06:15,169
micro services type of data applications

00:06:13,250 --> 00:06:16,610
so the idea is that if you have your

00:06:15,169 --> 00:06:19,520
application instead of having a single

00:06:16,610 --> 00:06:22,280
data store as the application state for

00:06:19,520 --> 00:06:24,530
your distributed application you rather

00:06:22,280 --> 00:06:27,139
have a dedicated application store for

00:06:24,530 --> 00:06:29,750
each application and have a well defined

00:06:27,139 --> 00:06:32,210
protocol application level means between

00:06:29,750 --> 00:06:36,229
this application so the database is not

00:06:32,210 --> 00:06:39,680
shared among them I think I hope those

00:06:36,229 --> 00:06:40,969
those those forces explain the reason

00:06:39,680 --> 00:06:42,949
why there is such a multitude of

00:06:40,969 --> 00:06:46,639
technologies out there and I don't want

00:06:42,949 --> 00:06:48,979
to justify this orto to dive further the

00:06:46,639 --> 00:06:52,009
point is that there are out there there

00:06:48,979 --> 00:06:54,250
lot and as I said over 450 commercially

00:06:52,009 --> 00:06:58,400
supported one and one of the interesting

00:06:54,250 --> 00:06:59,990
consequences of this is that almost any

00:06:58,400 --> 00:07:01,190
organization

00:06:59,990 --> 00:07:05,570
that would have a list here of those

00:07:01,190 --> 00:07:07,810
technologies deployed in in their data

00:07:05,570 --> 00:07:10,370
store data in the infrastructure so

00:07:07,810 --> 00:07:12,020
interesting question race how they're

00:07:10,370 --> 00:07:14,360
going to integrate those technologies

00:07:12,020 --> 00:07:16,310
this multitude of database that would

00:07:14,360 --> 00:07:18,500
usually don't have at least few data

00:07:16,310 --> 00:07:21,290
storage technologies dedicated a

00:07:18,500 --> 00:07:26,420
particularly good for particular type of

00:07:21,290 --> 00:07:29,270
use cases and it was discussed today

00:07:26,420 --> 00:07:32,270
I've observed I have observed so far too

00:07:29,270 --> 00:07:34,640
many trains that trying to provide this

00:07:32,270 --> 00:07:36,530
type of integration and by the way the

00:07:34,640 --> 00:07:39,500
integration of technologies is very big

00:07:36,530 --> 00:07:41,960
deal to do usually did standard ETL

00:07:39,500 --> 00:07:43,640
technologies and systems are trying to

00:07:41,960 --> 00:07:45,620
cope with this that's not I'm going to

00:07:43,640 --> 00:07:47,720
talk today I'm really trying to talk

00:07:45,620 --> 00:07:50,600
about how organization can provide a

00:07:47,720 --> 00:07:52,220
single holistic view over the data that

00:07:50,600 --> 00:07:54,620
is spread across different data stores

00:07:52,220 --> 00:07:56,450
which might be useful for certain use

00:07:54,620 --> 00:07:58,720
cases very often those are the

00:07:56,450 --> 00:08:01,610
analytical or maybe some data science

00:07:58,720 --> 00:08:05,690
type of use cases to train their data

00:08:01,610 --> 00:08:09,200
set so two main trends are emerging that

00:08:05,690 --> 00:08:11,480
are aiming to - in my opinion - to

00:08:09,200 --> 00:08:12,980
converging to provide more unified view

00:08:11,480 --> 00:08:16,220
on the data system and data processing

00:08:12,980 --> 00:08:18,020
system one is more functional based its

00:08:16,220 --> 00:08:19,820
unified programming model and you can

00:08:18,020 --> 00:08:21,080
today it was mentioned there was a very

00:08:19,820 --> 00:08:24,200
nice presentation comparing the

00:08:21,080 --> 00:08:25,370
interfaces of spark and fling you have

00:08:24,200 --> 00:08:27,650
noticed that they're very close and

00:08:25,370 --> 00:08:28,400
actually they're not the only two that

00:08:27,650 --> 00:08:31,190
are very similar

00:08:28,400 --> 00:08:33,380
apex Apache currents cascading Apache

00:08:31,190 --> 00:08:36,020
beam Deo actually inspired by a common

00:08:33,380 --> 00:08:40,220
one paper bought from 2010 I think is

00:08:36,020 --> 00:08:43,700
from Java Google paper as a type of API

00:08:40,220 --> 00:08:45,980
and there is a trend now at least that

00:08:43,700 --> 00:08:48,650
spark link and epics are implementing

00:08:45,980 --> 00:08:52,010
and converging at certain level under

00:08:48,650 --> 00:08:53,960
Apache beam in the project so this is

00:08:52,010 --> 00:08:56,780
and this is example snippet of how

00:08:53,960 --> 00:08:58,520
Apache beam looks as a notation the

00:08:56,780 --> 00:09:01,400
second trend and that's what I'm going

00:08:58,520 --> 00:09:02,990
to focus now today is apparently a lot

00:09:01,400 --> 00:09:05,990
of the Gnostic vendors and big data

00:09:02,990 --> 00:09:08,540
vendors are starting to implement sequel

00:09:05,990 --> 00:09:10,400
interfaces for their data stores or some

00:09:08,540 --> 00:09:12,830
sort sequel like interfaces for for

00:09:10,400 --> 00:09:14,480
diabetics data stores and

00:09:12,830 --> 00:09:17,510
some statistics from the last couple of

00:09:14,480 --> 00:09:19,730
years or Hadoop so shows that apparently

00:09:17,510 --> 00:09:22,010
majority of the tasks that are run on

00:09:19,730 --> 00:09:24,950
Hadoop nowadays are either high base or

00:09:22,010 --> 00:09:27,589
sequel some sort of seek on Hadoop type

00:09:24,950 --> 00:09:29,260
of solutions out there also for spark

00:09:27,589 --> 00:09:31,730
and there is a report from last year

00:09:29,260 --> 00:09:33,620
states that the most used production

00:09:31,730 --> 00:09:36,320
component is the spark sequel within

00:09:33,620 --> 00:09:38,990
their system so there is this and I

00:09:36,320 --> 00:09:41,120
stated Google F one paper actually has a

00:09:38,990 --> 00:09:42,950
quote there that any data system has to

00:09:41,120 --> 00:09:44,209
provide sequel interface and I found

00:09:42,950 --> 00:09:46,519
this particularly important because a

00:09:44,209 --> 00:09:48,950
lot of the Big Data technologies are now

00:09:46,519 --> 00:09:50,810
in the open space are influencing from

00:09:48,950 --> 00:09:52,700
the Google papers so there is this shift

00:09:50,810 --> 00:09:55,190
and ideas and it's interesting movement

00:09:52,700 --> 00:09:58,940
to and trend to observe apparently a lot

00:09:55,190 --> 00:10:01,040
of company I think it it's worthy to to

00:09:58,940 --> 00:10:02,660
try to reason what are the reason what

00:10:01,040 --> 00:10:05,990
what is the main driver for this

00:10:02,660 --> 00:10:07,940
movement for this converging it seems

00:10:05,990 --> 00:10:09,890
like the desire there is a lot of tools

00:10:07,940 --> 00:10:11,300
that know how toxic well out there with

00:10:09,890 --> 00:10:13,459
within the organization so it seems like

00:10:11,300 --> 00:10:15,170
Sukhoi pretty easy way to integrate with

00:10:13,459 --> 00:10:17,630
with with those existing tools of this

00:10:15,170 --> 00:10:21,079
more like legacy legacy reason secondary

00:10:17,630 --> 00:10:25,450
and I think this is a more important one

00:10:21,079 --> 00:10:28,490
is the relational model that stays under

00:10:25,450 --> 00:10:31,250
usually sequel engines and this is the

00:10:28,490 --> 00:10:33,020
the hot pits that we can talk about we

00:10:31,250 --> 00:10:34,430
should talk about today and I think this

00:10:33,020 --> 00:10:38,899
is important slide although it doesn't

00:10:34,430 --> 00:10:40,790
seems very very bright I can argue that

00:10:38,899 --> 00:10:42,740
practically any useful data system out

00:10:40,790 --> 00:10:45,500
there in one another form provides

00:10:42,740 --> 00:10:48,230
implements the set or back semantics so

00:10:45,500 --> 00:10:49,670
operators like projection filtering if

00:10:48,230 --> 00:10:52,490
the system is more advanced that some

00:10:49,670 --> 00:10:54,410
sort of join or group by would be

00:10:52,490 --> 00:10:56,209
present so in order to to provide some

00:10:54,410 --> 00:10:58,070
usefulness for the users the system have

00:10:56,209 --> 00:11:00,380
to implement explicitly or implicitly

00:10:58,070 --> 00:11:02,870
dissipate those operators so having this

00:11:00,380 --> 00:11:04,579
and acknowledging that this exists and

00:11:02,870 --> 00:11:07,570
very often in order to implement a

00:11:04,579 --> 00:11:09,649
pipeline or or query execution

00:11:07,570 --> 00:11:11,990
statement you have to chain multiple

00:11:09,649 --> 00:11:13,519
operators like this and when you start

00:11:11,990 --> 00:11:14,810
to play with this concept you realize

00:11:13,519 --> 00:11:17,540
it's practically this is the same

00:11:14,810 --> 00:11:19,940
relational algebra concept that are very

00:11:17,540 --> 00:11:21,440
common in the relational space and there

00:11:19,940 --> 00:11:23,570
is a tools that are very good in

00:11:21,440 --> 00:11:25,120
optimizing subtype of change and those

00:11:23,570 --> 00:11:26,559
are the planners and

00:11:25,120 --> 00:11:28,990
indeed the relational expression

00:11:26,559 --> 00:11:30,519
optimizers are very desirable feature

00:11:28,990 --> 00:11:31,779
for many of the technologies Big Data

00:11:30,519 --> 00:11:40,059
technologies simply they're very

00:11:31,779 --> 00:11:41,410
difficult to implement now and that's at

00:11:40,059 --> 00:11:43,990
least that word very difficult to

00:11:41,410 --> 00:11:45,699
implement until recently now there's at

00:11:43,990 --> 00:11:47,920
least couple of open-source technologies

00:11:45,699 --> 00:11:51,309
out there that provide some some some

00:11:47,920 --> 00:11:54,579
some help and are useful to to consider

00:11:51,309 --> 00:11:56,920
and try to use and leverage in order to

00:11:54,579 --> 00:11:59,019
provide this type of relational

00:11:56,920 --> 00:12:04,689
expressions relational cumulation within

00:11:59,019 --> 00:12:07,019
the existing big data systems I'm

00:12:04,689 --> 00:12:09,550
proposing here I mean this is again just

00:12:07,019 --> 00:12:11,050
simple subset of it's possible way how

00:12:09,550 --> 00:12:13,240
you can integrate how organization are

00:12:11,050 --> 00:12:14,769
I'm dealing with a lot of customers out

00:12:13,240 --> 00:12:16,930
there so I have some first-hand

00:12:14,769 --> 00:12:19,420
experience with some big players in the

00:12:16,930 --> 00:12:23,110
field and see and experience how they

00:12:19,420 --> 00:12:24,249
preserve and Pacey them the usefulness

00:12:23,110 --> 00:12:26,439
of the data how they're trying to

00:12:24,249 --> 00:12:28,839
integrate that their data and it seems

00:12:26,439 --> 00:12:31,480
like the most common approaches this one

00:12:28,839 --> 00:12:33,790
so this any standard for the writer

00:12:31,480 --> 00:12:35,649
database system approach so you pick one

00:12:33,790 --> 00:12:38,019
database that allows you to implement

00:12:35,649 --> 00:12:41,050
the connectors to external databases or

00:12:38,019 --> 00:12:42,879
the external data systems no sequel and

00:12:41,050 --> 00:12:45,189
then you can provide kind of single

00:12:42,879 --> 00:12:50,139
review on your system just via external

00:12:45,189 --> 00:12:52,480
tables which representation sorry which

00:12:50,139 --> 00:12:55,269
representation of the external logical

00:12:52,480 --> 00:12:57,429
data systems in this case I have

00:12:55,269 --> 00:12:59,559
experienced with Patrick Hope which is

00:12:57,429 --> 00:13:01,899
yet another to derive from the green

00:12:59,559 --> 00:13:04,029
plan that we discussed today again which

00:13:01,899 --> 00:13:06,399
was derived from Postgres it's MPP

00:13:04,029 --> 00:13:07,620
shared nothing solution which has very

00:13:06,399 --> 00:13:10,410
powered actually share very similar

00:13:07,620 --> 00:13:13,600
optimizer or its orc optimizer inside

00:13:10,410 --> 00:13:16,240
right what's more important provides a

00:13:13,600 --> 00:13:18,279
pxf framework this is a Java extension

00:13:16,240 --> 00:13:20,379
framework which allows you to actually

00:13:18,279 --> 00:13:23,230
implement a plug-ins for external system

00:13:20,379 --> 00:13:24,939
so this is I call it an 1/n model

00:13:23,230 --> 00:13:27,670
because the organization would use a

00:13:24,939 --> 00:13:29,470
single sequel usually post grad G DBC

00:13:27,670 --> 00:13:31,929
connector or DB seat to talk with one

00:13:29,470 --> 00:13:33,519
MPP database and where the extension

00:13:31,929 --> 00:13:36,639
mechanism would be able to see some

00:13:33,519 --> 00:13:39,110
portions of them and the Gnostic code

00:13:36,639 --> 00:13:41,779
system itself outside

00:13:39,110 --> 00:13:43,339
second approach is far more interesting

00:13:41,779 --> 00:13:46,190
in my opinion because it provide more

00:13:43,339 --> 00:13:48,320
autonomous autonomous e4 for them nós

00:13:46,190 --> 00:13:49,700
ecosystem themselves is to implement a

00:13:48,320 --> 00:13:51,740
Sequoia doctor for each system in

00:13:49,700 --> 00:13:53,660
isolation and for this purpose there is

00:13:51,740 --> 00:13:56,570
a frame or very powerful framework out

00:13:53,660 --> 00:13:58,940
there called Apache calcite and as you

00:13:56,570 --> 00:14:00,860
can see in this case each of the NOS

00:13:58,940 --> 00:14:03,200
ecosystem would actually have its own

00:14:00,860 --> 00:14:05,089
sequel representation interface its own

00:14:03,200 --> 00:14:06,230
optimization the advantage of this is

00:14:05,089 --> 00:14:09,500
that you might be able to tuned

00:14:06,230 --> 00:14:12,380
optimizers 4cq optimizers and the

00:14:09,500 --> 00:14:15,170
relational algebra expression optimizers

00:14:12,380 --> 00:14:19,459
better to the particulars Pacific's of

00:14:15,170 --> 00:14:21,470
the nas ecosystem and there is an

00:14:19,459 --> 00:14:23,209
interesting gyre ticket the Jireh issued

00:14:21,470 --> 00:14:25,339
that recently popped up which is

00:14:23,209 --> 00:14:30,350
exploring the possibilities to bridge

00:14:25,339 --> 00:14:32,000
those two approaches just one slide

00:14:30,350 --> 00:14:34,070
about the first the federated database

00:14:32,000 --> 00:14:35,810
approach how it looks like so on the

00:14:34,070 --> 00:14:37,579
sequel standpoint you're creating a

00:14:35,810 --> 00:14:39,829
table that looks like this standard type

00:14:37,579 --> 00:14:41,660
the interesting bit is that here you're

00:14:39,829 --> 00:14:43,730
providing location to your nas record

00:14:41,660 --> 00:14:45,620
data system where you want to wrap and

00:14:43,730 --> 00:14:47,269
you have to implement three classes

00:14:45,620 --> 00:14:50,260
which is the fragmented accessor and

00:14:47,269 --> 00:14:53,449
there is over the the purpose of the

00:14:50,260 --> 00:14:55,010
experimenter is that if the dating the

00:14:53,449 --> 00:14:57,199
Gnostic or data store allows you to

00:14:55,010 --> 00:14:59,120
partition the data in streams that you

00:14:57,199 --> 00:15:00,829
can process in parallel the row of the

00:14:59,120 --> 00:15:03,380
fermentor is indeed to establish this

00:15:00,829 --> 00:15:05,390
partition for each of the streams or

00:15:03,380 --> 00:15:06,949
separate streams in parallel the

00:15:05,390 --> 00:15:08,930
accessor actually breaks them into a

00:15:06,949 --> 00:15:11,209
collection of rows key value rows and

00:15:08,930 --> 00:15:12,829
for each of the this value rows there is

00:15:11,209 --> 00:15:16,250
over the last component you have to

00:15:12,829 --> 00:15:19,010
implement would convert it into a column

00:15:16,250 --> 00:15:21,380
a column list which would match this

00:15:19,010 --> 00:15:23,750
interfaces there is much more internals

00:15:21,380 --> 00:15:25,940
you can pass analytics and stuff to to

00:15:23,750 --> 00:15:28,130
configure the statistics in order to

00:15:25,940 --> 00:15:30,050
optimize the help the optimizer to

00:15:28,130 --> 00:15:32,600
adjust according this particular data

00:15:30,050 --> 00:15:35,120
store and this is very powerful approach

00:15:32,600 --> 00:15:35,930
if you for example already have Hadoop

00:15:35,120 --> 00:15:38,570
and Hokie

00:15:35,930 --> 00:15:40,459
like system in your infrastructure and

00:15:38,570 --> 00:15:42,350
you can just implement this such type of

00:15:40,459 --> 00:15:46,449
plugins and wrap and provide holistic

00:15:42,350 --> 00:15:46,449
view on your back-end system

00:15:47,500 --> 00:15:53,000
the second approach for direct one is to

00:15:51,140 --> 00:15:55,640
implement a sequel interface or

00:15:53,000 --> 00:15:58,310
you're an leverage tikka optimizer

00:15:55,640 --> 00:16:00,370
around your nasi code database and for

00:15:58,310 --> 00:16:04,010
this the Apache frame magic outside

00:16:00,370 --> 00:16:05,360
framework provides you query parser this

00:16:04,010 --> 00:16:07,580
is sequel query parts the validator

00:16:05,360 --> 00:16:10,040
optimizer I think this is the most

00:16:07,580 --> 00:16:11,750
important bit here as the bones you get

00:16:10,040 --> 00:16:13,880
the G DBC driver which you can talk with

00:16:11,750 --> 00:16:15,680
the system and one very important to

00:16:13,880 --> 00:16:17,360
design decision about the calcite is to

00:16:15,680 --> 00:16:19,580
stay out of the business of how data is

00:16:17,360 --> 00:16:22,490
stored and processed which in turn makes

00:16:19,580 --> 00:16:24,650
is very useful to implement and grab

00:16:22,490 --> 00:16:26,960
almost any existing data store out there

00:16:24,650 --> 00:16:29,120
I think this is by design and it's very

00:16:26,960 --> 00:16:31,040
powerful decision and if you take a look

00:16:29,120 --> 00:16:33,050
about the various technologies out there

00:16:31,040 --> 00:16:34,460
that use in one another way Apache

00:16:33,050 --> 00:16:38,630
calcite you would see that most of the

00:16:34,460 --> 00:16:40,430
big play already using it inside what I

00:16:38,630 --> 00:16:42,950
did I'm working on a patchy geode

00:16:40,430 --> 00:16:45,170
adapter the Apache geo TC memory data

00:16:42,950 --> 00:16:47,270
grid yet another key value store

00:16:45,170 --> 00:16:48,830
distributed hash and I'm going to use in

00:16:47,270 --> 00:16:51,260
some of the example just as a reference

00:16:48,830 --> 00:16:53,300
to illustrate how it looks like so this

00:16:51,260 --> 00:16:55,490
assuming that you decide to implement a

00:16:53,300 --> 00:16:58,640
sequel adapter using a particle size

00:16:55,490 --> 00:17:00,560
drone back-end data store no sequel data

00:16:58,640 --> 00:17:03,350
store their cup of decision you have to

00:17:00,560 --> 00:17:05,870
make and they're very important

00:17:03,350 --> 00:17:07,760
regarding the from one side to how much

00:17:05,870 --> 00:17:09,860
sequel completed you are going to expose

00:17:07,760 --> 00:17:13,040
and compliant with the sequel standard

00:17:09,860 --> 00:17:15,079
and other hand how much you're going to

00:17:13,040 --> 00:17:17,209
to leverage the power of the nice

00:17:15,079 --> 00:17:20,030
agnostic or not no sequel system you

00:17:17,209 --> 00:17:22,069
have so the first thing is you have to

00:17:20,030 --> 00:17:24,170
decide how we're going to convert your

00:17:22,069 --> 00:17:25,730
data type from the existing nas

00:17:24,170 --> 00:17:27,740
ecosystem let's say this is key value

00:17:25,730 --> 00:17:29,630
store or it could be even like some sort

00:17:27,740 --> 00:17:31,910
of graph representation into a tabular

00:17:29,630 --> 00:17:33,770
format that is expect bike outside and

00:17:31,910 --> 00:17:35,660
cow side has the standard metadata

00:17:33,770 --> 00:17:37,460
expected is the catalog schema which is

00:17:35,660 --> 00:17:39,980
collection of tables table which is

00:17:37,460 --> 00:17:42,260
collection of rows and row is just a

00:17:39,980 --> 00:17:45,380
list of columns represent to the

00:17:42,260 --> 00:17:47,270
relational data type so this is

00:17:45,380 --> 00:17:49,250
important decision because let's say

00:17:47,270 --> 00:17:51,110
that you want to express as some sort of

00:17:49,250 --> 00:17:53,510
a JSON or Java object which has

00:17:51,110 --> 00:17:55,610
hierarchy and you have to flatter in

00:17:53,510 --> 00:17:56,930
some tabular format you have to decide

00:17:55,610 --> 00:17:59,000
whether you we are going to spend

00:17:56,930 --> 00:18:00,890
computation and a lot of civilization

00:17:59,000 --> 00:18:02,900
and to achieve this or you just can't

00:18:00,890 --> 00:18:04,610
afford to implement only top-level

00:18:02,900 --> 00:18:06,680
fields or some smartness in stuff so

00:18:04,610 --> 00:18:08,570
it's up to you to decide what is the

00:18:06,680 --> 00:18:11,240
it's a trade-off so how much you're

00:18:08,570 --> 00:18:13,100
going to expose from your model as a as

00:18:11,240 --> 00:18:15,920
an opposite of the performance that

00:18:13,100 --> 00:18:17,270
you're going to gain or lose and the

00:18:15,920 --> 00:18:18,830
second more important thing that this is

00:18:17,270 --> 00:18:21,230
general principle for any data

00:18:18,830 --> 00:18:22,670
distributed data system is move the

00:18:21,230 --> 00:18:26,570
computation next to the data not other

00:18:22,670 --> 00:18:29,780
way around particular that means that if

00:18:26,570 --> 00:18:32,150
you in case of in the context of the

00:18:29,780 --> 00:18:34,730
sequel query you would like to run the

00:18:32,150 --> 00:18:37,400
executions of this query next to the

00:18:34,730 --> 00:18:38,810
node to where data is stored rather than

00:18:37,400 --> 00:18:40,490
actually moving the data to some central

00:18:38,810 --> 00:18:44,000
node where this processing is happening

00:18:40,490 --> 00:18:45,740
and then moving it back and forth in the

00:18:44,000 --> 00:18:47,930
context of Apache prowl site you have

00:18:45,740 --> 00:18:50,420
two approaches to to achieve this the

00:18:47,930 --> 00:18:52,550
first one is simple I call it simple it

00:18:50,420 --> 00:18:54,470
just allows you to to implement a very

00:18:52,550 --> 00:18:58,220
simple simple inter type of interface

00:18:54,470 --> 00:19:00,010
with ability to push down the predicates

00:18:58,220 --> 00:19:01,880
predicates

00:19:00,010 --> 00:19:04,280
operators relation imperative like

00:19:01,880 --> 00:19:06,800
filters and projections that means that

00:19:04,280 --> 00:19:09,500
if you have select some fields where

00:19:06,800 --> 00:19:11,990
from some table where something it makes

00:19:09,500 --> 00:19:13,700
any sense this fuse and there were

00:19:11,990 --> 00:19:16,010
clouds at the conditions to be pushed

00:19:13,700 --> 00:19:18,290
down to your not equal solution and you

00:19:16,010 --> 00:19:20,240
pre-filter and reprocess and return back

00:19:18,290 --> 00:19:24,280
only the amount of data that is

00:19:20,240 --> 00:19:27,890
necessary for okay for for then for the

00:19:24,280 --> 00:19:31,490
system to process I'm going to hurry up

00:19:27,890 --> 00:19:33,620
here's this is an example how it looks

00:19:31,490 --> 00:19:36,230
the simple scenario you're connecting to

00:19:33,620 --> 00:19:39,560
an orifice visa what is JDBC adopt this

00:19:36,230 --> 00:19:42,860
is the logical side JDBC protocol so

00:19:39,560 --> 00:19:44,510
when you connect to JDBC by the particle

00:19:42,860 --> 00:19:46,940
size gdb seed driver to your back-end

00:19:44,510 --> 00:19:49,100
system you have to provide a model in

00:19:46,940 --> 00:19:50,270
JSON format the only thing that model

00:19:49,100 --> 00:19:53,020
contains is your entry point

00:19:50,270 --> 00:19:57,050
implementation of the schema factory

00:19:53,020 --> 00:19:59,840
with some appearance that are relevant

00:19:57,050 --> 00:20:02,240
for your back-end systems the role of

00:19:59,840 --> 00:20:03,950
this schema is usually one liner of

00:20:02,240 --> 00:20:06,380
implementation is to create a schema

00:20:03,950 --> 00:20:08,030
based on those operators that appearance

00:20:06,380 --> 00:20:10,100
that you have provided the schema in

00:20:08,030 --> 00:20:12,230
turn depends of the mappings that you

00:20:10,100 --> 00:20:13,910
have decided to implement for your

00:20:12,230 --> 00:20:15,410
back-end system Nestico system and the

00:20:13,910 --> 00:20:18,560
relational staff would create a list of

00:20:15,410 --> 00:20:20,460
tables and the important things is that

00:20:18,560 --> 00:20:24,049
you have to implement a column types

00:20:20,460 --> 00:20:27,450
in these tables then when a query comes

00:20:24,049 --> 00:20:30,119
it would be passed to the scan parameter

00:20:27,450 --> 00:20:32,100
so that from this book order usually

00:20:30,119 --> 00:20:35,279
what means that it was going to collect

00:20:32,100 --> 00:20:38,369
try to extract this data set from your

00:20:35,279 --> 00:20:40,200
logical database and convert it in the

00:20:38,369 --> 00:20:43,190
convert method convert it into type that

00:20:40,200 --> 00:20:45,690
is compliant with the table definitions

00:20:43,190 --> 00:20:47,309
that you hear is that in this simple

00:20:45,690 --> 00:20:48,809
implementation you could get all date so

00:20:47,309 --> 00:20:50,399
there is not any moving of the

00:20:48,809 --> 00:20:54,330
computation code into the data

00:20:50,399 --> 00:20:56,369
everything goes to the central processor

00:20:54,330 --> 00:20:58,529
and get process they're led to version

00:20:56,369 --> 00:21:00,779
that you can optimize this computation

00:20:58,529 --> 00:21:02,669
it's called that instead of scannable

00:21:00,779 --> 00:21:04,139
table you can experiment filter

00:21:02,669 --> 00:21:06,960
scannable table projected future

00:21:04,139 --> 00:21:08,759
scannable table which allows you to push

00:21:06,960 --> 00:21:10,649
down the filters and projectors but

00:21:08,759 --> 00:21:12,119
that's everything that you can do as an

00:21:10,649 --> 00:21:13,649
optimization if you have a joint

00:21:12,119 --> 00:21:15,600
operators or group by operators

00:21:13,649 --> 00:21:18,570
everything would happen central place on

00:21:15,600 --> 00:21:20,909
the client side and the second approach

00:21:18,570 --> 00:21:23,580
that the calcite allow provides you with

00:21:20,909 --> 00:21:26,820
is to implement your own relational

00:21:23,580 --> 00:21:28,259
rules and in relation operators that

00:21:26,820 --> 00:21:30,749
would allow you to improvise

00:21:28,259 --> 00:21:33,269
implementation much closer to the to the

00:21:30,749 --> 00:21:36,830
native noshiko system in this case this

00:21:33,269 --> 00:21:40,470
is a apache geode and this is very fast

00:21:36,830 --> 00:21:42,090
going so how it looks like gdb see this

00:21:40,470 --> 00:21:44,460
is everything Hebrew is the standard

00:21:42,090 --> 00:21:45,720
Apache calcite framework this is the

00:21:44,460 --> 00:21:48,179
beads that you have to implement in

00:21:45,720 --> 00:21:50,009
order to to provide the adapter and this

00:21:48,179 --> 00:21:52,700
is code generated by the outside so

00:21:50,009 --> 00:21:55,110
sequel query comes it is passed to the

00:21:52,700 --> 00:21:58,200
parcel sequence converted into a

00:21:55,110 --> 00:22:00,600
relational expression and 3 it goes

00:21:58,200 --> 00:22:02,340
through the planner which performs some

00:22:00,600 --> 00:22:05,249
optimizations actually while performing

00:22:02,340 --> 00:22:07,860
these optimizations optimize tree is

00:22:05,249 --> 00:22:09,600
passed to the innumerable component

00:22:07,860 --> 00:22:12,119
which is responsible to convert this

00:22:09,600 --> 00:22:13,830
logical plan into physical plan and this

00:22:12,119 --> 00:22:15,600
is already the tricky part in this

00:22:13,830 --> 00:22:17,759
process actually if you have implemented

00:22:15,600 --> 00:22:19,200
your own rules and operators the

00:22:17,759 --> 00:22:22,009
enumerable components would consult

00:22:19,200 --> 00:22:23,879
those and would provide a wealth I word

00:22:22,009 --> 00:22:25,619
implementation actually chooses the

00:22:23,879 --> 00:22:27,570
expression tree this is concept from

00:22:25,619 --> 00:22:29,119
League for link 4j I think this is

00:22:27,570 --> 00:22:31,799
something that come from from from

00:22:29,119 --> 00:22:33,359
Microsoft and this interpreter actually

00:22:31,799 --> 00:22:34,320
generate Java code that's optimized for

00:22:33,359 --> 00:22:36,960
this implement a

00:22:34,320 --> 00:22:40,320
compiled and the JDBC query is

00:22:36,960 --> 00:22:43,200
executed so that's the whole 400

00:22:40,320 --> 00:22:45,240
complete line and I'm going to skip this

00:22:43,200 --> 00:22:47,060
those are the internals of the if you're

00:22:45,240 --> 00:22:49,590
going to implement your own rules and

00:22:47,060 --> 00:22:52,200
operators in the outside you would have

00:22:49,590 --> 00:22:53,820
to get acquainted with this and this is

00:22:52,200 --> 00:22:55,500
the generic pattern if you see any of

00:22:53,820 --> 00:22:57,510
the implementations of the calcite

00:22:55,500 --> 00:22:59,640
adapter out there more or less

00:22:57,510 --> 00:23:02,760
implemented for these these steps here

00:22:59,640 --> 00:23:04,410
and just I would finish with the example

00:23:02,760 --> 00:23:07,040
let's say that you have this relation of

00:23:04,410 --> 00:23:09,300
expressions this is the relation of

00:23:07,040 --> 00:23:12,090
operators that you would see now we have

00:23:09,300 --> 00:23:15,900
a join of two tables filter and project

00:23:12,090 --> 00:23:17,970
two of the fields so the optimizer throw

00:23:15,900 --> 00:23:19,680
would be to convert and to push some of

00:23:17,970 --> 00:23:22,320
the operators closer to the date so you

00:23:19,680 --> 00:23:24,660
see the project's so to reduce the

00:23:22,320 --> 00:23:26,550
amount of data that moves a port so this

00:23:24,660 --> 00:23:29,340
is usually the logical part of the

00:23:26,550 --> 00:23:32,750
parcel of the optimizer and this is the

00:23:29,340 --> 00:23:35,730
real example with the Apache geode

00:23:32,750 --> 00:23:37,590
adapter that I work on if you have this

00:23:35,730 --> 00:23:40,110
query this is the logical plan you see

00:23:37,590 --> 00:23:41,550
that you have no any optimization the

00:23:40,110 --> 00:23:45,060
scan on the tables

00:23:41,550 --> 00:23:48,060
then join perform then filter on the

00:23:45,060 --> 00:23:51,630
this see one of the field and projection

00:23:48,060 --> 00:23:54,360
extract two of the fields if you use the

00:23:51,630 --> 00:23:56,040
simple scannable approach which doesn't

00:23:54,360 --> 00:23:57,810
implement any rules you would have some

00:23:56,040 --> 00:24:00,150
advantage so that the planner would

00:23:57,810 --> 00:24:01,650
already reorder the sword or this this

00:24:00,150 --> 00:24:04,110
operator in a way that would mean they'd

00:24:01,650 --> 00:24:05,640
be executed much more efficiently but

00:24:04,110 --> 00:24:07,260
still most of the computation would

00:24:05,640 --> 00:24:09,900
happen on the client side where the did

00:24:07,260 --> 00:24:12,540
the visual dock the sequel part is

00:24:09,900 --> 00:24:14,250
performed if you move one step and you

00:24:12,540 --> 00:24:15,840
implement your own rules and have

00:24:14,250 --> 00:24:18,510
something that's much more tailored to

00:24:15,840 --> 00:24:22,230
your nest nautical data system in this

00:24:18,510 --> 00:24:25,230
case I have implemented geo project

00:24:22,230 --> 00:24:26,940
filter but there's also group by you see

00:24:25,230 --> 00:24:28,860
that actually it uses those operators

00:24:26,940 --> 00:24:30,780
and it leverages practically a lot of

00:24:28,860 --> 00:24:33,840
dos operators now executed on the nos

00:24:30,780 --> 00:24:35,460
ecosystem itself still you see that

00:24:33,840 --> 00:24:37,950
joint is not implemented there I'm

00:24:35,460 --> 00:24:40,470
working on this that means that now this

00:24:37,950 --> 00:24:42,870
query practice could be converted into

00:24:40,470 --> 00:24:44,610
two sub queries run on the nas ecosystem

00:24:42,870 --> 00:24:46,200
and the result would be returned and be

00:24:44,610 --> 00:24:47,580
performed on the client side but this is

00:24:46,200 --> 00:24:51,390
something that you can

00:24:47,580 --> 00:24:54,120
progress on typical gdb see how we are

00:24:51,390 --> 00:24:58,560
going to use this boom stand Java

00:24:54,120 --> 00:25:01,260
standpoint and I'm frightened over time

00:24:58,560 --> 00:25:01,480
so I'm sorry for this try to to run for

00:25:01,260 --> 00:25:07,690
it

00:25:01,480 --> 00:25:07,690

YouTube URL: https://www.youtube.com/watch?v=vb04eF9PSR0


