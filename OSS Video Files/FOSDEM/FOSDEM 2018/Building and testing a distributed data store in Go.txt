Title: Building and testing a distributed data store in Go
Publication date: 2018-02-04
Playlist: FOSDEM 2018
Description: 
	by Matt Bostock

At: FOSDEM 2018
Room: H.1308 (Rolin)
Scheduled start: 2018-02-03 15:30:00+01
Captions: 
	00:00:07,140 --> 00:00:13,510
thanks Francesca um so hi my name is

00:00:10,330 --> 00:00:15,610
Matt Bostick I am a platform engineer

00:00:13,510 --> 00:00:17,020
working for cloud fair in London I'm

00:00:15,610 --> 00:00:19,930
interested in distributed systems in

00:00:17,020 --> 00:00:21,460
performance and until September I was

00:00:19,930 --> 00:00:23,369
studying a master's in computer science

00:00:21,460 --> 00:00:26,349
at night school for the last two years

00:00:23,369 --> 00:00:27,790
which was hard but super interesting so

00:00:26,349 --> 00:00:29,169
today I want to talk to you about

00:00:27,790 --> 00:00:31,270
building and designing a distributed

00:00:29,169 --> 00:00:36,310
datastore which I did as part of my

00:00:31,270 --> 00:00:38,170
Master's final project so this won't be

00:00:36,310 --> 00:00:39,940
a talk about stuff I'm working on at

00:00:38,170 --> 00:00:42,179
work it's really a personal project I'm

00:00:39,940 --> 00:00:44,079
the the project is called timbala

00:00:42,179 --> 00:00:46,090
that's the logo that's actually the

00:00:44,079 --> 00:00:49,179
second logo because logos are important

00:00:46,090 --> 00:00:50,649
and it was original Athens TV so if you

00:00:49,179 --> 00:00:53,350
see references to Athens dB

00:00:50,649 --> 00:00:56,769
that's why and so Tim Bala is a

00:00:53,350 --> 00:00:59,410
distributed time series database or will

00:00:56,769 --> 00:01:01,960
would be if it was finished did she beat

00:00:59,410 --> 00:01:03,460
the system is a huge topic I won't be

00:01:01,960 --> 00:01:04,570
able to explain everything during this

00:01:03,460 --> 00:01:05,340
talk but hopefully you'll be able to

00:01:04,570 --> 00:01:08,950
take something away

00:01:05,340 --> 00:01:10,390
subscribe to broad torque and I'm gonna

00:01:08,950 --> 00:01:14,710
talk really about how I approach the

00:01:10,390 --> 00:01:17,050
problem so it's more about the design of

00:01:14,710 --> 00:01:19,680
building a system like this and doing

00:01:17,050 --> 00:01:23,830
that and go rather than going into like

00:01:19,680 --> 00:01:25,480
very deep technical details so just a

00:01:23,830 --> 00:01:26,080
disclaimer Tim bar is not

00:01:25,480 --> 00:01:27,520
production-ready

00:01:26,080 --> 00:01:30,910
so please please don't use it for

00:01:27,520 --> 00:01:32,410
anything you care about in production so

00:01:30,910 --> 00:01:34,210
what is distributor why am I talking

00:01:32,410 --> 00:01:36,850
about distributed systems well a

00:01:34,210 --> 00:01:38,590
distributed system is a system where you

00:01:36,850 --> 00:01:41,650
essentially have coordination between

00:01:38,590 --> 00:01:43,630
networked computers and this is how our

00:01:41,650 --> 00:01:45,580
Wikipedia defines it and I think the

00:01:43,630 --> 00:01:48,000
coordination part is is really important

00:01:45,580 --> 00:01:50,980
and it's really hard as we'll find out

00:01:48,000 --> 00:01:52,990
so why why distributed why why do we

00:01:50,980 --> 00:01:54,460
need a distributed system well first of

00:01:52,990 --> 00:01:58,420
all we want to survive the failure of

00:01:54,460 --> 00:02:00,190
individual servers so if an individual

00:01:58,420 --> 00:02:02,080
server dies we want the system to keep

00:02:00,190 --> 00:02:04,630
working and we also want to be able to

00:02:02,080 --> 00:02:07,660
add more servers to meet demand so we

00:02:04,630 --> 00:02:09,640
have this it's a distributed time-series

00:02:07,660 --> 00:02:10,989
database if we have lots of users

00:02:09,640 --> 00:02:12,849
sending metrics we want to be able to

00:02:10,989 --> 00:02:13,660
scale up the number of servers to cope

00:02:12,849 --> 00:02:15,280
with that demand

00:02:13,660 --> 00:02:16,930
so although otherwise known as

00:02:15,280 --> 00:02:19,329
horizontal scaling

00:02:16,930 --> 00:02:20,560
so there's a great list of fallacies

00:02:19,329 --> 00:02:23,349
edition' be computing that came out of

00:02:20,560 --> 00:02:25,000
sun microsystems I think it's got some

00:02:23,349 --> 00:02:26,829
great things in here like the network is

00:02:25,000 --> 00:02:28,569
reliable if you're running a distributed

00:02:26,829 --> 00:02:30,040
system that tells you that the network

00:02:28,569 --> 00:02:34,989
must be reliable for the system to work

00:02:30,040 --> 00:02:36,730
be be very distrustful of it so this

00:02:34,989 --> 00:02:40,209
kind of thing makes building distributed

00:02:36,730 --> 00:02:41,049
systems interesting and so the use case

00:02:40,209 --> 00:02:44,049
for timbala

00:02:41,049 --> 00:02:48,069
was to create a durable on long term

00:02:44,049 --> 00:02:50,260
storage system for metrics and so I've

00:02:48,069 --> 00:02:52,209
worked a lot with Prometheus and really

00:02:50,260 --> 00:02:55,120
what I wanted was a place to store my

00:02:52,209 --> 00:02:58,959
Prometheus metrics over a long period of

00:02:55,120 --> 00:03:00,849
time say five to ten years and it was

00:02:58,959 --> 00:03:03,480
important that it this system could

00:03:00,849 --> 00:03:08,319
store multidimensional metrics so

00:03:03,480 --> 00:03:11,019
metrics defined using labels or tags and

00:03:08,319 --> 00:03:12,849
it also must be also capable of storing

00:03:11,019 --> 00:03:17,230
more metrics than could be accommodated

00:03:12,849 --> 00:03:18,519
by a single commodity server so why not

00:03:17,230 --> 00:03:19,870
just use the cloud right everyone's

00:03:18,519 --> 00:03:22,599
using the cloud you know you could just

00:03:19,870 --> 00:03:23,889
use like s3 put your data in there well

00:03:22,599 --> 00:03:26,769
first of all it wouldn't make for a very

00:03:23,889 --> 00:03:29,500
good masters project so that was what

00:03:26,769 --> 00:03:31,409
the first thing but also sometimes you

00:03:29,500 --> 00:03:33,220
want to run stuff on-premise maybe for

00:03:31,409 --> 00:03:34,540
because you don't want to run your data

00:03:33,220 --> 00:03:37,659
you don't want to put your data in the

00:03:34,540 --> 00:03:40,659
cloud and you have an off data that you

00:03:37,659 --> 00:03:42,730
need a system that can handle a lot a

00:03:40,659 --> 00:03:44,290
reasonable amount of data but maybe it's

00:03:42,730 --> 00:03:47,260
not so much data that you want to run a

00:03:44,290 --> 00:03:51,190
system like say Hadoop or safe which are

00:03:47,260 --> 00:03:54,310
big complex systems and can be can be

00:03:51,190 --> 00:03:56,590
difficult to operate so the other the

00:03:54,310 --> 00:03:58,900
other use case I had in mind was I

00:03:56,590 --> 00:04:00,909
needed this system needed to be highly

00:03:58,900 --> 00:04:03,239
performance so it needs to be able to

00:04:00,909 --> 00:04:05,409
ingest a lot of metrics very quickly and

00:04:03,239 --> 00:04:07,030
I also wanted to be really easy to

00:04:05,409 --> 00:04:08,560
operate I think this is really important

00:04:07,030 --> 00:04:10,930
with distributed systems that you have

00:04:08,560 --> 00:04:14,040
to bear in mind how they're going to be

00:04:10,930 --> 00:04:16,269
operated in production so being able to

00:04:14,040 --> 00:04:17,680
see what is happening in the system at

00:04:16,269 --> 00:04:21,699
any point in time was really important

00:04:17,680 --> 00:04:22,780
so what are the requirements so well

00:04:21,699 --> 00:04:24,849
first of all the system needs to be able

00:04:22,780 --> 00:04:26,320
to shard the data so it needs to be able

00:04:24,849 --> 00:04:28,240
to store more data than can fit in a

00:04:26,320 --> 00:04:29,590
single node and by shorting I mean

00:04:28,240 --> 00:04:30,340
essentially spreading data across

00:04:29,590 --> 00:04:33,490
multiple sir

00:04:30,340 --> 00:04:35,590
by splitting it up into chunks it also

00:04:33,490 --> 00:04:38,410
the system also needed to replicate data

00:04:35,590 --> 00:04:40,810
so we wanted to make copies of data in

00:04:38,410 --> 00:04:44,169
case a single node dies we would still

00:04:40,810 --> 00:04:48,669
have a copy of the data on on one of the

00:04:44,169 --> 00:04:50,320
other servers I mentioned throughput so

00:04:48,669 --> 00:04:53,350
the system needs to be highly available

00:04:50,320 --> 00:04:55,510
for data ingestion specifically so I

00:04:53,350 --> 00:04:58,900
focused on high availability for the

00:04:55,510 --> 00:05:00,430
right path because when you're reading

00:04:58,900 --> 00:05:01,990
data when you're querying the system a

00:05:00,430 --> 00:05:04,270
human can always retire you want to

00:05:01,990 --> 00:05:06,370
avoid that if at all possible

00:05:04,270 --> 00:05:09,220
but if worst comes to worst they can do

00:05:06,370 --> 00:05:12,280
that if you're ingestion if you're your

00:05:09,220 --> 00:05:14,710
right path is not available then your

00:05:12,280 --> 00:05:16,840
your the data that you need to ingest is

00:05:14,710 --> 00:05:19,750
going to start backing up you're gonna

00:05:16,840 --> 00:05:21,400
have back pressure and then when you try

00:05:19,750 --> 00:05:23,860
to catch up you need to ingest your

00:05:21,400 --> 00:05:26,470
normal your normal traffic plus the

00:05:23,860 --> 00:05:28,150
traffic that is backlogged as well so we

00:05:26,470 --> 00:05:29,770
want to try and avoid that and really

00:05:28,150 --> 00:05:34,770
make sure that they're the right path is

00:05:29,770 --> 00:05:36,970
is as available as possible and

00:05:34,770 --> 00:05:40,060
operational simplicity so it needs to be

00:05:36,970 --> 00:05:41,770
simple to operate maintain I wanted to

00:05:40,060 --> 00:05:44,350
keep the number of configuration open

00:05:41,770 --> 00:05:47,050
options to a minimum so less less things

00:05:44,350 --> 00:05:49,840
to tune which hopefully would translate

00:05:47,050 --> 00:05:51,970
to less things to get wrong and also add

00:05:49,840 --> 00:05:55,210
good instrumentation so logging metrics

00:05:51,970 --> 00:05:58,300
and tracing so that you can see if a

00:05:55,210 --> 00:06:00,010
certain client hit a given server which

00:05:58,300 --> 00:06:01,660
other servers that requests had to

00:06:00,010 --> 00:06:05,020
traverse through to be able to serve a

00:06:01,660 --> 00:06:07,390
request and the other requirement I had

00:06:05,020 --> 00:06:09,010
was interoperability with Prometheus so

00:06:07,390 --> 00:06:11,410
I mentioned the original use case I had

00:06:09,010 --> 00:06:13,419
in mind for this was to store Prometheus

00:06:11,410 --> 00:06:15,400
data long-term so I wanted to reuse

00:06:13,419 --> 00:06:17,830
Prometheus best the best features which

00:06:15,400 --> 00:06:20,590
is the query language and its data model

00:06:17,830 --> 00:06:22,330
and also it has API is already defined

00:06:20,590 --> 00:06:24,910
so I didn't want to have to redesign

00:06:22,330 --> 00:06:27,880
those api's when when I could reuse

00:06:24,910 --> 00:06:29,860
those so that helps me to focus the

00:06:27,880 --> 00:06:31,570
project and it allowed me to focus

00:06:29,860 --> 00:06:34,000
really on the distributed part of the

00:06:31,570 --> 00:06:36,850
system which was the part that was most

00:06:34,000 --> 00:06:39,130
interesting for this project so it's

00:06:36,850 --> 00:06:40,630
easy to think about distributed systems

00:06:39,130 --> 00:06:42,159
if you have a really small amount of

00:06:40,630 --> 00:06:43,419
data because you know you could just put

00:06:42,159 --> 00:06:44,860
it on

00:06:43,419 --> 00:06:47,349
and it's not really distributed system

00:06:44,860 --> 00:06:49,449
so I think it helps to kind of at least

00:06:47,349 --> 00:06:51,460
have a target to work with in terms of

00:06:49,449 --> 00:06:53,620
numbers so I just looked at CloudFlare

00:06:51,460 --> 00:06:55,150
is open T STP insulation this is where

00:06:53,620 --> 00:06:57,520
we keep metrics for long-term storage

00:06:55,150 --> 00:07:00,180
currently and every 2017 we're storing

00:06:57,520 --> 00:07:03,430
700,000 data points per second so

00:07:00,180 --> 00:07:05,499
700,000 metrics observations ingested

00:07:03,430 --> 00:07:07,569
per second and 70 million unique time

00:07:05,499 --> 00:07:11,039
series and those are multi-dimensional

00:07:07,569 --> 00:07:14,409
metrics so 70 million uni unique metrics

00:07:11,039 --> 00:07:15,430
so this kind of gave me a goal

00:07:14,409 --> 00:07:17,620
I knew I wasn't going to achieve this

00:07:15,430 --> 00:07:18,969
for my Master's project but at least

00:07:17,620 --> 00:07:20,349
helped me to think about what the

00:07:18,969 --> 00:07:24,699
constraints of the system would be and

00:07:20,349 --> 00:07:26,889
how I need to design it so how do you

00:07:24,699 --> 00:07:28,719
build a distributed system like where do

00:07:26,889 --> 00:07:30,490
you start you have all these servers

00:07:28,719 --> 00:07:33,099
talking to each other like it seems like

00:07:30,490 --> 00:07:35,560
really complex and difficult so I had a

00:07:33,099 --> 00:07:36,729
hard time coming up with an MB MVP and I

00:07:35,560 --> 00:07:39,849
didn't have a lot of time to build this

00:07:36,729 --> 00:07:43,089
cuz I need to do it on evenings so I

00:07:39,849 --> 00:07:44,650
started thinking about ingestion versus

00:07:43,089 --> 00:07:45,909
querying so the read in the right path

00:07:44,650 --> 00:07:47,650
so it was one of the first places I

00:07:45,909 --> 00:07:49,210
started and then I thought well how can

00:07:47,650 --> 00:07:51,400
I also reduce the scope of what I need

00:07:49,210 --> 00:07:52,960
to do well we're using third-party code

00:07:51,400 --> 00:07:55,810
wherever possible so reason those

00:07:52,960 --> 00:07:58,389
Prometheus libraries was one of the most

00:07:55,810 --> 00:08:01,479
beneficial decisions in the project and

00:07:58,389 --> 00:08:03,279
so I reused the poem QL query library so

00:08:01,479 --> 00:08:06,039
I didn't need to reinvent my own query

00:08:03,279 --> 00:08:08,620
engine and I reused the IPA code as well

00:08:06,039 --> 00:08:10,389
so the so the system would be API

00:08:08,620 --> 00:08:12,039
compatible with Prometheus so any

00:08:10,389 --> 00:08:14,830
existing integrations would would work

00:08:12,039 --> 00:08:16,930
with it so I came up some milestones the

00:08:14,830 --> 00:08:18,520
first one was to just get the system

00:08:16,930 --> 00:08:19,870
working on a single node so no

00:08:18,520 --> 00:08:22,210
coordination no communication between

00:08:19,870 --> 00:08:23,919
servers but just be able to store data

00:08:22,210 --> 00:08:25,960
on a single server and then query it

00:08:23,919 --> 00:08:27,310
back out afterwards and then the next

00:08:25,960 --> 00:08:29,050
milestone was to actually get the

00:08:27,310 --> 00:08:30,479
service talking to each other start

00:08:29,050 --> 00:08:32,289
sorting the data across the nodes

00:08:30,479 --> 00:08:34,539
replicating the data so that we have

00:08:32,289 --> 00:08:36,690
enough copies to survive single node

00:08:34,539 --> 00:08:40,360
failures and then also look at

00:08:36,690 --> 00:08:44,320
rebalancing data between between servers

00:08:40,360 --> 00:08:47,140
so that we can recover from from server

00:08:44,320 --> 00:08:49,209
loss so and then there are other things

00:08:47,140 --> 00:08:51,910
I want to do if I could take this beyond

00:08:49,209 --> 00:08:53,410
a minimum viable product so one of those

00:08:51,910 --> 00:08:56,500
things was read repairs so that's the

00:08:53,410 --> 00:08:59,770
ability to as you're reading data

00:08:56,500 --> 00:09:01,510
you can see if a given server is missing

00:08:59,770 --> 00:09:02,890
some data I basically tell it hey you're

00:09:01,510 --> 00:09:04,720
missing data you should you should have

00:09:02,890 --> 00:09:07,480
a copy of this so that was one thing

00:09:04,720 --> 00:09:09,490
hinted handoff is the ability to store

00:09:07,480 --> 00:09:12,130
data on behalf of another node in the

00:09:09,490 --> 00:09:14,020
system so that if that nodes down you

00:09:12,130 --> 00:09:15,310
can basically hold on to its data until

00:09:14,020 --> 00:09:17,110
it comes back up and then send that data

00:09:15,310 --> 00:09:19,870
across to it and then the other thing I

00:09:17,110 --> 00:09:21,600
wanted to look at was an activity

00:09:19,870 --> 00:09:23,890
entropy which is a fancy way of saying

00:09:21,600 --> 00:09:27,790
having a background process that runs

00:09:23,890 --> 00:09:29,530
and tries to detect missing data in data

00:09:27,790 --> 00:09:32,050
that you might not read very frequently

00:09:29,530 --> 00:09:33,490
so with repair read repair only works

00:09:32,050 --> 00:09:35,020
for data that you're reading and with

00:09:33,490 --> 00:09:37,750
metrics you're often reading just very

00:09:35,020 --> 00:09:39,820
recent data so active and active anti

00:09:37,750 --> 00:09:42,640
entropy would allow you to repair data

00:09:39,820 --> 00:09:43,690
that is maybe further back in time but I

00:09:42,640 --> 00:09:46,180
was pretty sure I wouldn't be able to

00:09:43,690 --> 00:09:47,530
finish that for this project so I was

00:09:46,180 --> 00:09:49,150
like okay this is cool this is really

00:09:47,530 --> 00:09:51,940
exciting I'm gonna like start reading

00:09:49,150 --> 00:09:53,350
like research and papers and this was

00:09:51,940 --> 00:09:54,700
really cool like I've read all these

00:09:53,350 --> 00:09:57,070
things about Numa and writes

00:09:54,700 --> 00:09:59,500
amplification and how to work with SSDs

00:09:57,070 --> 00:09:59,800
and em up and hashing all this kind of

00:09:59,500 --> 00:10:03,190
stuff

00:09:59,800 --> 00:10:04,750
and it was super interesting but like

00:10:03,190 --> 00:10:06,070
yeah this there's so much to work with

00:10:04,750 --> 00:10:08,470
here and I needed to start small and

00:10:06,070 --> 00:10:10,930
needs to get something working so like

00:10:08,470 --> 00:10:12,160
let's ignore that for now so back to the

00:10:10,930 --> 00:10:14,020
essentials so what did I need to think

00:10:12,160 --> 00:10:15,220
about for the system to work well the

00:10:14,020 --> 00:10:17,920
service needs to be able to talk to each

00:10:15,220 --> 00:10:20,350
other need to to to coordinate Peter

00:10:17,920 --> 00:10:22,150
Bergen wrote a blog post about his

00:10:20,350 --> 00:10:24,130
system for ingesting logs called ok log

00:10:22,150 --> 00:10:26,530
and that was really influential to me in

00:10:24,130 --> 00:10:28,830
terms of framing the problem in terms of

00:10:26,530 --> 00:10:31,060
coordination between servers and then

00:10:28,830 --> 00:10:33,580
indexing how do you know where your data

00:10:31,060 --> 00:10:35,950
is stored in the system how do you store

00:10:33,580 --> 00:10:38,860
the data on disk how do you know which

00:10:35,950 --> 00:10:40,030
nodes should be in the cluster and when

00:10:38,860 --> 00:10:41,320
you know which nodes should be in the

00:10:40,030 --> 00:10:43,240
cluster how do you decide where to send

00:10:41,320 --> 00:10:45,400
the data between them and then finally

00:10:43,240 --> 00:10:48,160
how will the system fail because it will

00:10:45,400 --> 00:10:49,750
fail at some point so to try to

00:10:48,160 --> 00:10:50,920
understand the problem more let's let's

00:10:49,750 --> 00:10:52,570
consider some of the traits or

00:10:50,920 --> 00:10:55,240
assumptions we can make about time

00:10:52,570 --> 00:10:57,700
series data so the first one I made was

00:10:55,240 --> 00:10:59,980
the data once ingested will be immutable

00:10:57,700 --> 00:11:02,290
so basically their data would be append

00:10:59,980 --> 00:11:04,060
only so there would be no worry - no

00:11:02,290 --> 00:11:06,080
need to worry about updates to the data

00:11:04,060 --> 00:11:08,209
so we don't have to worry about

00:11:06,080 --> 00:11:10,610
like updating a row essentially in a

00:11:08,209 --> 00:11:12,019
relational database or the date having

00:11:10,610 --> 00:11:16,519
to manage multiple versions of the same

00:11:12,019 --> 00:11:19,130
data so that helps relax some of the

00:11:16,519 --> 00:11:20,930
requirements because it makes it we

00:11:19,130 --> 00:11:23,480
don't have to worry about managing all

00:11:20,930 --> 00:11:26,269
those versions and so the other thing

00:11:23,480 --> 00:11:28,640
about metrics is the data types can be

00:11:26,269 --> 00:11:30,410
really simple so time series can include

00:11:28,640 --> 00:11:33,800
events but in this case I was just focus

00:11:30,410 --> 00:11:36,230
on numbers and numbers can compress

00:11:33,800 --> 00:11:38,480
really well prometheus

00:11:36,230 --> 00:11:41,209
to two point two point oh and a bar F

00:11:38,480 --> 00:11:42,829
uses a variant of the gorilla

00:11:41,209 --> 00:11:44,930
compression algorithm from Facebook and

00:11:42,829 --> 00:11:47,720
this uses double Delta compression for

00:11:44,930 --> 00:11:49,370
64-bit floats so it takes the difference

00:11:47,720 --> 00:11:50,300
between the two numbers and then it

00:11:49,370 --> 00:11:52,640
takes the difference between that

00:11:50,300 --> 00:11:54,950
difference and it uses that to compress

00:11:52,640 --> 00:11:57,560
the data and if you're interested the

00:11:54,950 --> 00:11:59,260
the gorilla paper explains how that

00:11:57,560 --> 00:12:02,230
works

00:11:59,260 --> 00:12:05,269
so the other thing to bear in mind with

00:12:02,230 --> 00:12:06,560
time series data bay time series data is

00:12:05,269 --> 00:12:08,029
the tension between the rate reading the

00:12:06,560 --> 00:12:09,019
right patterns and this is really

00:12:08,029 --> 00:12:10,850
important when thinking about how to

00:12:09,019 --> 00:12:12,800
design the system so you essentially

00:12:10,850 --> 00:12:14,480
have continuous writes across the

00:12:12,800 --> 00:12:16,880
majority of your individual time series

00:12:14,480 --> 00:12:19,640
so in the case of CloudFlare for example

00:12:16,880 --> 00:12:21,680
we had 70 million unique time series we

00:12:19,640 --> 00:12:23,510
might have maybe 40 million of those

00:12:21,680 --> 00:12:26,120
time series being rich written to within

00:12:23,510 --> 00:12:28,130
the last five minutes so you have a lot

00:12:26,120 --> 00:12:30,079
of updates across a broad range of data

00:12:28,130 --> 00:12:32,300
but then when you're reading back from

00:12:30,079 --> 00:12:34,430
the system you're often reading for a

00:12:32,300 --> 00:12:36,620
given time range of data so you have

00:12:34,430 --> 00:12:38,420
this tension between the right path

00:12:36,620 --> 00:12:40,550
which is touching many different time

00:12:38,420 --> 00:12:42,589
series and the read path where you're

00:12:40,550 --> 00:12:45,649
going across time so that was something

00:12:42,589 --> 00:12:47,510
that was difficult to there's one of the

00:12:45,649 --> 00:12:50,089
most interesting properties of storing

00:12:47,510 --> 00:12:51,589
time series data and now if fabiÃ¡n from

00:12:50,089 --> 00:12:54,920
the Prometheus project goes into this in

00:12:51,589 --> 00:12:57,110
more detail in his blog post so I looked

00:12:54,920 --> 00:12:59,209
up I was like okay what exists currently

00:12:57,110 --> 00:13:01,279
I won't go into that into detail now but

00:12:59,209 --> 00:13:03,440
the main thing I drew from these

00:13:01,279 --> 00:13:05,180
existing systems was the idea of storing

00:13:03,440 --> 00:13:08,630
data in columns so column the data

00:13:05,180 --> 00:13:11,390
stores and from Amazon's dynamo paper

00:13:08,630 --> 00:13:13,399
the idea of using consistent hashing to

00:13:11,390 --> 00:13:13,970
determine where to place they turn in

00:13:13,399 --> 00:13:17,690
the system

00:13:13,970 --> 00:13:19,490
so I mentioned coordination and

00:13:17,690 --> 00:13:20,780
coordination being able to think about

00:13:19,490 --> 00:13:23,000
the system in terms of coordination was

00:13:20,780 --> 00:13:25,010
really helpful and the thing I realized

00:13:23,000 --> 00:13:27,140
was if I wanted to support a high

00:13:25,010 --> 00:13:28,610
throughput on ingestion I wanted to keep

00:13:27,140 --> 00:13:30,260
coordination to an absolute minimum

00:13:28,610 --> 00:13:34,190
because that would help reduce the

00:13:30,260 --> 00:13:36,230
complexity of the system and and all as

00:13:34,190 --> 00:13:38,150
a result make it more reliable and also

00:13:36,230 --> 00:13:40,640
avoid coordination bottlenecks because

00:13:38,150 --> 00:13:42,470
those bottlenecks could be couldn't be a

00:13:40,640 --> 00:13:47,720
bottleneck for the for the ingestion

00:13:42,470 --> 00:13:50,660
throughput so the other thing was to

00:13:47,720 --> 00:13:52,730
know which which servers were part of

00:13:50,660 --> 00:13:53,960
the cluster at any given time so I could

00:13:52,730 --> 00:13:55,550
just do this statically I could just

00:13:53,960 --> 00:13:57,530
tell each server what other services it

00:13:55,550 --> 00:13:58,910
exists but that's gonna be kind of

00:13:57,530 --> 00:14:01,760
painful especially if you had a lot of

00:13:58,910 --> 00:14:05,960
nodes and also I needed to know if a

00:14:01,760 --> 00:14:08,870
node when a node fails so being able to

00:14:05,960 --> 00:14:11,570
detect a node failure so for that is the

00:14:08,870 --> 00:14:15,560
member list library this is used by

00:14:11,570 --> 00:14:17,540
Hoshi Corp Cerf and also console it's a

00:14:15,560 --> 00:14:19,430
if the goal it's a goal I brie eases the

00:14:17,540 --> 00:14:21,170
swim gossip protocol and by gossip

00:14:19,430 --> 00:14:23,690
protocol I mean that the service talked

00:14:21,170 --> 00:14:25,190
to each other using UDP and they

00:14:23,690 --> 00:14:27,140
synchronize their state with each other

00:14:25,190 --> 00:14:30,290
using UDP and then occasionally they

00:14:27,140 --> 00:14:32,240
have a TCP mechanism for reliably

00:14:30,290 --> 00:14:33,800
synchronizing their state and they talk

00:14:32,240 --> 00:14:36,440
to each other and basically tell each

00:14:33,800 --> 00:14:38,180
other they they still exist and so even

00:14:36,440 --> 00:14:40,520
has some really nice properties one of

00:14:38,180 --> 00:14:42,830
them is that you can detect if a node is

00:14:40,520 --> 00:14:45,080
still alive even if you can't access it

00:14:42,830 --> 00:14:47,780
directly so the nodes can snitch on each

00:14:45,080 --> 00:14:50,000
other on each other and you can

00:14:47,780 --> 00:14:52,400
indirectly detect if a server is still

00:14:50,000 --> 00:14:53,840
alive so that worked really well the

00:14:52,400 --> 00:14:55,960
membership library was member list

00:14:53,840 --> 00:14:58,130
library was really easy to integrate

00:14:55,960 --> 00:15:00,740
Peter Berger and also uses this in okay

00:14:58,130 --> 00:15:02,089
log and it was a lot easier than I

00:15:00,740 --> 00:15:04,880
thought to get this up and running so

00:15:02,089 --> 00:15:06,470
that was really really good to use

00:15:04,880 --> 00:15:08,089
so indexing how do you find the data

00:15:06,470 --> 00:15:09,170
quickly like you have all these nodes in

00:15:08,089 --> 00:15:11,570
the cluster where do you know where your

00:15:09,170 --> 00:15:14,210
data is so I could use a centralized

00:15:11,570 --> 00:15:17,210
index and I've worked with the raft

00:15:14,210 --> 00:15:18,800
protocol before I anew bit about

00:15:17,210 --> 00:15:20,839
consensus and I started thinking like

00:15:18,800 --> 00:15:22,190
I'm sure I need consensus for the system

00:15:20,839 --> 00:15:24,890
and then I thought about it I thought

00:15:22,190 --> 00:15:26,600
well actually probably only need

00:15:24,890 --> 00:15:28,820
consensus if I have a centralized index

00:15:26,600 --> 00:15:29,840
and using a centralized index you have a

00:15:28,820 --> 00:15:31,970
consistent view of

00:15:29,840 --> 00:15:35,750
what should exist in the system at any

00:15:31,970 --> 00:15:37,610
one time but to do that you need to be

00:15:35,750 --> 00:15:40,190
able to coordinate between servers so

00:15:37,610 --> 00:15:41,960
that you can decide what that

00:15:40,190 --> 00:15:43,760
centralized index should be and that's

00:15:41,960 --> 00:15:46,340
likely to become a bottleneck high

00:15:43,760 --> 00:15:48,740
ingestion volumes so I wanted to try and

00:15:46,340 --> 00:15:49,820
avoid that if possible so the other

00:15:48,740 --> 00:15:52,280
thing you could do is just use a local

00:15:49,820 --> 00:15:54,650
index in have each server know about

00:15:52,280 --> 00:15:56,930
what it stores itself the big

00:15:54,650 --> 00:15:59,120
disadvantage to this is that if you lose

00:15:56,930 --> 00:16:00,950
data you don't really know exactly what

00:15:59,120 --> 00:16:02,660
you've lost because if you've lost the

00:16:00,950 --> 00:16:04,820
index along with your data then you

00:16:02,660 --> 00:16:06,200
don't know what you've lost but maybe

00:16:04,820 --> 00:16:08,500
maybe we could work with this maybe we

00:16:06,200 --> 00:16:10,910
could do something so like dynamo

00:16:08,500 --> 00:16:13,330
cassandra and react they all use this

00:16:10,910 --> 00:16:15,920
idea of constant consistent hashing to

00:16:13,330 --> 00:16:18,050
determine where data should go so I

00:16:15,920 --> 00:16:19,910
started looking at that and so I was

00:16:18,050 --> 00:16:21,830
looking at data placement and how we how

00:16:19,910 --> 00:16:24,620
we place dates are on the different

00:16:21,830 --> 00:16:27,560
servers so consistent hashing is

00:16:24,620 --> 00:16:29,780
essentially a way of placing items into

00:16:27,560 --> 00:16:31,640
buckets and hashing is just a way of

00:16:29,780 --> 00:16:33,770
using maths to put items into pockets

00:16:31,640 --> 00:16:35,690
and consistent hashing aims to keep the

00:16:33,770 --> 00:16:37,550
disruption to a minimum when the number

00:16:35,690 --> 00:16:40,550
of buckets changes and so in our system

00:16:37,550 --> 00:16:42,650
that translates to when the number of

00:16:40,550 --> 00:16:44,600
nodes in the cluster changes then we

00:16:42,650 --> 00:16:46,340
want to keep the the disruption to a

00:16:44,600 --> 00:16:48,170
minimum and we want the amount of data

00:16:46,340 --> 00:16:51,040
that is displaced when all the servers

00:16:48,170 --> 00:16:53,480
to keep to stay to a minimum as well so

00:16:51,040 --> 00:16:55,820
in this example if we have five nodes in

00:16:53,480 --> 00:16:57,590
a cluster and one node fails we should

00:16:55,820 --> 00:16:59,840
only only a fifth of the data should

00:16:57,590 --> 00:17:01,750
have to move to another node assuming

00:16:59,840 --> 00:17:04,640
that we're not able to replace that node

00:17:01,750 --> 00:17:06,500
immediately so I looked into consistent

00:17:04,640 --> 00:17:08,690
hashing algorithms there's a decision

00:17:06,500 --> 00:17:10,940
record on the github repository for this

00:17:08,690 --> 00:17:13,390
project that goes into more detail and

00:17:10,940 --> 00:17:15,560
I'll show these slides afterwards

00:17:13,390 --> 00:17:18,830
basically the first one I looked at was

00:17:15,560 --> 00:17:20,780
the Karger algorithm and then and then I

00:17:18,830 --> 00:17:23,300
kept iterating in and worked on looked

00:17:20,780 --> 00:17:25,580
at jump ash I'd encourage you to look at

00:17:23,300 --> 00:17:26,870
the of link to the papers here I won't

00:17:25,580 --> 00:17:30,080
explain them in detail right now cuz I

00:17:26,870 --> 00:17:31,820
don't have the time but I'll just show

00:17:30,080 --> 00:17:34,550
you the the jump past implementation

00:17:31,820 --> 00:17:36,320
which i think is super elegant jump ash

00:17:34,550 --> 00:17:38,530
is an improvement over the column

00:17:36,320 --> 00:17:41,300
algorithm because it uses less memory

00:17:38,530 --> 00:17:42,120
and it's a lot faster another thing I

00:17:41,300 --> 00:17:45,990
really like about

00:17:42,120 --> 00:17:48,150
is it that stood out to me was that it

00:17:45,990 --> 00:17:51,059
uses this magic constant which is a

00:17:48,150 --> 00:17:53,850
64-bit congruence or random number

00:17:51,059 --> 00:17:56,180
generator and it uses this magic

00:17:53,850 --> 00:17:58,680
constant to to make this faster so

00:17:56,180 --> 00:18:00,450
daemon risky has an implementation of

00:17:58,680 --> 00:18:04,110
this and like this is the this is the

00:18:00,450 --> 00:18:06,420
whole jump hash algorithm so the other

00:18:04,110 --> 00:18:09,660
thing I needed to figure out was well

00:18:06,420 --> 00:18:11,640
the hash function that's that I using

00:18:09,660 --> 00:18:13,890
consistent hashing needs a partition key

00:18:11,640 --> 00:18:16,410
so I need just figure out what that

00:18:13,890 --> 00:18:18,660
partition key could be and the choice of

00:18:16,410 --> 00:18:20,880
partition key could have a big impact on

00:18:18,660 --> 00:18:23,550
how many nodes you have to query when

00:18:20,880 --> 00:18:25,020
you're querying data and also how many

00:18:23,550 --> 00:18:28,770
nodes are ingesting data at any one

00:18:25,020 --> 00:18:29,940
point in time so again I went into a lot

00:18:28,770 --> 00:18:33,300
I've gone into detail on the github

00:18:29,940 --> 00:18:34,620
repository for how to choose this but

00:18:33,300 --> 00:18:37,470
what to include in this was a critical

00:18:34,620 --> 00:18:40,080
decision in the system and we need to

00:18:37,470 --> 00:18:42,530
store three copies of each shard of each

00:18:40,080 --> 00:18:44,820
copy of data so I achieved this by

00:18:42,530 --> 00:18:48,360
prepending the replica number two the

00:18:44,820 --> 00:18:50,640
partition key so then I was like okay

00:18:48,360 --> 00:18:52,490
well I know like where to put the data

00:18:50,640 --> 00:18:54,990
and how to index it but like how do i

00:18:52,490 --> 00:18:56,460
how do I store it on disk and I started

00:18:54,990 --> 00:18:58,920
looking at different kind of structures

00:18:56,460 --> 00:19:03,570
different libraries I'd like to started

00:18:58,920 --> 00:19:05,220
working with Roxie B and really I mean

00:19:03,570 --> 00:19:07,080
storage is really hard and I could have

00:19:05,220 --> 00:19:07,980
spent I mean this could have been a

00:19:07,080 --> 00:19:10,590
project in itself

00:19:07,980 --> 00:19:12,660
and luckily around the same time Fabian

00:19:10,590 --> 00:19:15,030
and the Prometheus team were working on

00:19:12,660 --> 00:19:16,800
this TST B library and I thought well

00:19:15,030 --> 00:19:18,720
I'm already integrating with heavily

00:19:16,800 --> 00:19:23,070
with Prometheus so why not just use this

00:19:18,720 --> 00:19:25,500
library so the the interfaces for this

00:19:23,070 --> 00:19:27,000
library were really clean to use and the

00:19:25,500 --> 00:19:30,110
conclusion I got from this was a good

00:19:27,000 --> 00:19:32,940
program programmers or lazy programmers

00:19:30,110 --> 00:19:34,350
constructively so because if you can

00:19:32,940 --> 00:19:37,740
reuse something then why build it

00:19:34,350 --> 00:19:40,590
yourself so I solved the on disk storage

00:19:37,740 --> 00:19:42,030
problem by using an existing library so

00:19:40,590 --> 00:19:45,210
this is the architecture that I came up

00:19:42,030 --> 00:19:47,220
with so no centralized index to keep the

00:19:45,210 --> 00:19:49,860
ingestion throughput high so the only

00:19:47,220 --> 00:19:51,960
shared state is a node metadata each

00:19:49,860 --> 00:19:54,570
node in the system has its has the same

00:19:51,960 --> 00:19:58,350
role and any

00:19:54,570 --> 00:20:02,220
no oneno can receive data and any node

00:19:58,350 --> 00:20:05,250
can be queried as well so what about

00:20:02,220 --> 00:20:06,840
testing well I found that integration

00:20:05,250 --> 00:20:08,429
tests were the most useful because I

00:20:06,840 --> 00:20:09,630
could quickly iterate on the system

00:20:08,429 --> 00:20:12,059
without having to worry about breaking

00:20:09,630 --> 00:20:13,799
my unit tests and so integration tests

00:20:12,059 --> 00:20:16,409
I had unit tests as well but integration

00:20:13,799 --> 00:20:23,250
tests really gave me the most the most

00:20:16,409 --> 00:20:26,909
value so for the unit tests one thing I

00:20:23,250 --> 00:20:28,500
looked at is how is how even is the data

00:20:26,909 --> 00:20:30,899
distribution across the nodes in the

00:20:28,500 --> 00:20:32,909
cluster so I wanted to make sure that no

00:20:30,899 --> 00:20:36,179
single node we're storing more data than

00:20:32,909 --> 00:20:37,620
the others and also are all replicas of

00:20:36,179 --> 00:20:40,679
the data stored on separate nodes so if

00:20:37,620 --> 00:20:41,879
I store three copies of the data I want

00:20:40,679 --> 00:20:43,740
to make sure they don't all end up on

00:20:41,879 --> 00:20:46,950
the same node so I wrote unit tests to

00:20:43,740 --> 00:20:50,039
do this and with little histograms and

00:20:46,950 --> 00:20:53,610
you can build histones like this using

00:20:50,039 --> 00:20:56,399
the in using the go testing library

00:20:53,610 --> 00:20:59,659
really simply just repeating a character

00:20:56,399 --> 00:21:02,820
and I used the standard deviation to

00:20:59,659 --> 00:21:05,220
measure what the what the distribution

00:21:02,820 --> 00:21:10,830
was between nodes and I used the the go

00:21:05,220 --> 00:21:12,600
sub tests to test to test this against

00:21:10,830 --> 00:21:14,070
different numbers of replicas and

00:21:12,600 --> 00:21:16,409
different numbers of nodes in the

00:21:14,070 --> 00:21:19,080
cluster and this was really helpful

00:21:16,409 --> 00:21:20,669
because when I changed when I changed

00:21:19,080 --> 00:21:22,230
the consistent hashing algorithm I could

00:21:20,669 --> 00:21:23,610
actually see the difference and see the

00:21:22,230 --> 00:21:26,580
improvement in balance when I switch to

00:21:23,610 --> 00:21:28,259
jump ash so the other thing I looked at

00:21:26,580 --> 00:21:30,509
was data displacement so if I remove a

00:21:28,259 --> 00:21:32,190
node how much data has to move and this

00:21:30,509 --> 00:21:36,029
helped me find a book about because I

00:21:32,190 --> 00:21:37,559
was I was sorting the list of nodes

00:21:36,029 --> 00:21:38,879
alphabetically cuz I figured that would

00:21:37,559 --> 00:21:41,279
make it more deterministic and

00:21:38,879 --> 00:21:42,899
determinism is a good thing but in this

00:21:41,279 --> 00:21:44,399
case it didn't actually help because it

00:21:42,899 --> 00:21:46,440
worked against the jump hash the way

00:21:44,399 --> 00:21:47,700
that jump hash works and in the in the

00:21:46,440 --> 00:21:49,259
jump hash paper it says that the main

00:21:47,700 --> 00:21:51,929
limitation is the buckets must be

00:21:49,259 --> 00:21:54,840
numbered sequentially and I was treating

00:21:51,929 --> 00:21:57,450
I was treating them as names of servers

00:21:54,840 --> 00:21:59,399
rather than just numbers of pockets so

00:21:57,450 --> 00:22:01,710
my conclusion was that each node in the

00:21:59,399 --> 00:22:05,539
cluster needed to remember which in

00:22:01,710 --> 00:22:05,539
which order it joined the cluster

00:22:06,050 --> 00:22:09,200
the other kind of sensor wrote war

00:22:07,490 --> 00:22:12,050
acceptance tests and I did these by

00:22:09,200 --> 00:22:14,510
executing the the binary itself and that

00:22:12,050 --> 00:22:15,770
allowed me to do things like test if my

00:22:14,510 --> 00:22:19,220
command line flags were still working

00:22:15,770 --> 00:22:21,110
and I also did things like test if I

00:22:19,220 --> 00:22:23,140
look for certain canary metrics to make

00:22:21,110 --> 00:22:25,730
sure I hadn't forgot to register metrics

00:22:23,140 --> 00:22:28,280
but also other things like can I query

00:22:25,730 --> 00:22:30,850
can I can I find the result of 1+1 and

00:22:28,280 --> 00:22:32,930
can I query data out of the system I

00:22:30,850 --> 00:22:35,000
mentioned the integration test is being

00:22:32,930 --> 00:22:35,960
the most effective there wasn't

00:22:35,000 --> 00:22:38,990
crossover between these in the

00:22:35,960 --> 00:22:42,140
acceptance tests but they they were by

00:22:38,990 --> 00:22:44,150
far but the the integration test focused

00:22:42,140 --> 00:22:47,420
more on the integration with other

00:22:44,150 --> 00:22:48,860
services such as Prometheus and I

00:22:47,420 --> 00:22:51,200
actually had Prometheus writing data

00:22:48,860 --> 00:22:52,700
into the system and then I was able to

00:22:51,200 --> 00:22:54,800
query back out again to make sure that

00:22:52,700 --> 00:22:57,110
that worked and I when I queried it back

00:22:54,800 --> 00:22:58,250
out I would use the official Prometheus

00:22:57,110 --> 00:22:59,750
client library so I could be sure that

00:22:58,250 --> 00:23:01,070
my system worked with both the

00:22:59,750 --> 00:23:02,690
Prometheus server and the official

00:23:01,070 --> 00:23:04,490
client libraries and I use docker

00:23:02,690 --> 00:23:06,620
compose for this for portability it was

00:23:04,490 --> 00:23:08,510
really easy to get set up and I had the

00:23:06,620 --> 00:23:11,030
docker compose integration test running

00:23:08,510 --> 00:23:13,490
in Travis CI for every every pull

00:23:11,030 --> 00:23:17,930
request and this actually worked really

00:23:13,490 --> 00:23:20,120
well so I'd highly recommend this I also

00:23:17,930 --> 00:23:22,910
set up a benchmarking framework so I

00:23:20,120 --> 00:23:24,830
could see how the system is working as

00:23:22,910 --> 00:23:28,820
it's running and not allowed me to do

00:23:24,830 --> 00:23:32,120
things like graph my throughput and also

00:23:28,820 --> 00:23:35,620
CPU usage usage and memory and again I

00:23:32,120 --> 00:23:40,340
used the benchmarking framework was was

00:23:35,620 --> 00:23:43,790
harness was in using docker compose I'll

00:23:40,340 --> 00:23:44,990
just go through these I use PDF to help

00:23:43,790 --> 00:23:46,730
determine where I could speed things up

00:23:44,990 --> 00:23:48,590
I think that's been covered and like to

00:23:46,730 --> 00:23:52,460
talk in more detail so I was just skip

00:23:48,590 --> 00:23:55,820
through that and the last thing I want

00:23:52,460 --> 00:23:57,440
to say is that I what I think a chance

00:23:55,820 --> 00:24:00,350
to do this but I really wanted to do it

00:23:57,440 --> 00:24:02,090
is in failure injection in in with

00:24:00,350 --> 00:24:04,100
docker compose you can use failure

00:24:02,090 --> 00:24:06,140
injection to test how your how you

00:24:04,100 --> 00:24:08,870
distributed system manages with failure

00:24:06,140 --> 00:24:10,660
so you can have a privileged container

00:24:08,870 --> 00:24:13,850
that can stop nodes in the cluster

00:24:10,660 --> 00:24:15,230
inject packet loss latency and that

00:24:13,850 --> 00:24:16,870
allows you to see how well the gossip

00:24:15,230 --> 00:24:18,670
protocol works

00:24:16,870 --> 00:24:21,610
and how well the system copes with

00:24:18,670 --> 00:24:23,080
failure so our conclusions I think the

00:24:21,610 --> 00:24:26,320
greatest challenge in this writing

00:24:23,080 --> 00:24:27,790
distributed systems is anticipating how

00:24:26,320 --> 00:24:29,800
that will fail and how they lose data

00:24:27,790 --> 00:24:31,060
the implementation is already hardened

00:24:29,800 --> 00:24:33,040
itself but it's even harder to figure

00:24:31,060 --> 00:24:34,300
out how they're going to fail and the

00:24:33,040 --> 00:24:35,680
other conclusion I took away is make

00:24:34,300 --> 00:24:37,540
sure you understand the trade-offs that

00:24:35,680 --> 00:24:39,220
your production systems are making

00:24:37,540 --> 00:24:42,970
because they are they are making

00:24:39,220 --> 00:24:44,710
trade-offs and finally use get used app

00:24:42,970 --> 00:24:47,470
it's awesome so thank you so much to Sam

00:24:44,710 --> 00:24:50,020
and the other contributors to DEP and if

00:24:47,470 --> 00:24:52,180
you're interested in reading more all

00:24:50,020 --> 00:24:54,790
the all the links are up here and I'll

00:24:52,180 --> 00:25:01,300
share the slides on on the FOSDEM site

00:24:54,790 --> 00:25:04,300
afterwards thank you before we do the QA

00:25:01,300 --> 00:25:05,980
before we do the QA I want those two

00:25:04,300 --> 00:25:09,270
people that are taking care of the doors

00:25:05,980 --> 00:25:13,150
to go to the doors so do not leave yet

00:25:09,270 --> 00:25:15,870
just let you go to that door and Marcelo

00:25:13,150 --> 00:25:18,700
can you go to that door Oh perfect okay

00:25:15,870 --> 00:25:20,170
because otherwise gonna be a chaos in a

00:25:18,700 --> 00:25:23,470
minute because there's a huge amount of

00:25:20,170 --> 00:25:25,660
people outside already now you're gonna

00:25:23,470 --> 00:25:29,640
start repairing and in the meanwhile we

00:25:25,660 --> 00:25:32,640
have the QA thank you any questions oh

00:25:29,640 --> 00:25:32,640
yeah

00:25:35,090 --> 00:25:39,900
so the question was what tools that I

00:25:37,620 --> 00:25:41,130
used to benchmark the system so I wrote

00:25:39,900 --> 00:25:44,400
a little tool that would generate load

00:25:41,130 --> 00:25:46,260
it generated random metrics using a seed

00:25:44,400 --> 00:25:49,200
so that they would be generated

00:25:46,260 --> 00:25:51,720
deterministically and the benchmarks run

00:25:49,200 --> 00:25:54,090
in doc campos so i'd i'd spend up three

00:25:51,720 --> 00:25:56,790
nodes of the cluster generate metrics

00:25:54,090 --> 00:26:01,850
ingest those and then i'd see how they

00:25:56,790 --> 00:26:01,850
performed in prometheus yes

00:26:15,570 --> 00:26:20,590
so I'm not sure I fully understood the

00:26:18,429 --> 00:26:22,570
question but I think it was Kaji did the

00:26:20,590 --> 00:26:29,080
system provide one interface to query

00:26:22,570 --> 00:26:30,520
all of the metrics no so this I mean

00:26:29,080 --> 00:26:32,470
this system is designed to stand alone

00:26:30,520 --> 00:26:34,270
so it's it integrates with Prometheus

00:26:32,470 --> 00:26:36,250
but you could use it without Prometheus

00:26:34,270 --> 00:26:43,179
it just it implements all the Prometheus

00:26:36,250 --> 00:26:48,840
API is essentially the majority any

00:26:43,179 --> 00:26:54,799
other questions thank you

00:26:48,840 --> 00:26:54,799

YouTube URL: https://www.youtube.com/watch?v=Su2WqMHVoAA


