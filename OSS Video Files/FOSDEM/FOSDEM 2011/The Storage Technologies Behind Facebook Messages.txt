Title: The Storage Technologies Behind Facebook Messages
Publication date: 2011-12-22
Playlist: FOSDEM 2011
Description: 
	By Nicolas Spiegelberg.

The engineering team behind Facebook Messages spent the past year building out a robust, scalable infrastructure. We spent a few weeks setting up a test framework to evaluate clusters of MySQL, Apache Cassandra, Apache HBase, and a couple of other systems. This talk looks at why we ultimately chose HBase and a number of the infrastructure decisions the team made to handle well over 100 billion messages per month.

FOSDEM (Free and Open Source Development European Meeting) is a European event centered around Free and Open Source software development. It is aimed at developers and all interested in the Free and Open Source news in the world. Its goals are to enable developers to meet and to promote the awareness and use of free and open source software. More info at http://fosdem.org
Captions: 
	00:00:04,460 --> 00:00:08,230
you

00:00:06,519 --> 00:00:10,299
so

00:00:08,230 --> 00:00:13,269
hi my name is Nicolas Pig Oberg I'm with

00:00:10,299 --> 00:00:14,559
Facebook messages and I'm going to let

00:00:13,269 --> 00:00:18,099
you for the first couple minutes before

00:00:14,559 --> 00:00:20,529
we get started here see my big

00:00:18,099 --> 00:00:23,759
beautifully bald manager talked about

00:00:20,529 --> 00:00:23,759
Facebook messages for a couple minutes

00:00:32,280 --> 00:00:35,770
over a year ago we started looking at

00:00:34,540 --> 00:00:37,719
ways to improve our Facebook message

00:00:35,770 --> 00:00:39,670
product and we realized pretty quickly

00:00:37,719 --> 00:00:41,590
that the problems that we face are

00:00:39,670 --> 00:00:45,030
actually bigger than just Facebook a lot

00:00:41,590 --> 00:00:45,030
of sharing should be simpler than it is

00:00:50,559 --> 00:00:54,160
when I want to reach out to my cousin

00:00:52,210 --> 00:00:55,420
Danny I have to use text message because

00:00:54,160 --> 00:00:57,460
you just graduated high school and

00:00:55,420 --> 00:00:59,079
that's all he really uses my grandmother

00:00:57,460 --> 00:01:00,309
she only uses email I can't send her a

00:00:59,079 --> 00:01:02,800
Facebook message and expect her to get

00:01:00,309 --> 00:01:04,570
back to I'm keeping this lookup table in

00:01:02,800 --> 00:01:06,580
my head how to reach out to each person

00:01:04,570 --> 00:01:09,430
why isn't this easier why don't all

00:01:06,580 --> 00:01:10,750
these technologies work together all you

00:01:09,430 --> 00:01:14,790
just need to send someone a message is

00:01:10,750 --> 00:01:14,790
the person and the message and that's it

00:01:20,050 --> 00:01:23,500
people should share however they want to

00:01:21,910 --> 00:01:25,450
share and if they want to connect via

00:01:23,500 --> 00:01:26,800
email they should be able to do that so

00:01:25,450 --> 00:01:28,780
we're giving every user of Facebook the

00:01:26,800 --> 00:01:32,500
option of getting at facebook.com/ email

00:01:28,780 --> 00:01:33,910
address this product is an email but it

00:01:32,500 --> 00:01:39,610
allows people who do use email to

00:01:33,910 --> 00:01:41,080
connect with the rest of us we've

00:01:39,610 --> 00:01:42,280
modeled this entire system after chat

00:01:41,080 --> 00:01:44,650
you know there's no subject lines

00:01:42,280 --> 00:01:45,880
there's no CC there's no BCC you press

00:01:44,650 --> 00:01:47,860
the Enter key it sends your message

00:01:45,880 --> 00:01:49,270
right away in real-time you want this to

00:01:47,860 --> 00:01:50,440
feel like a conversation among friends

00:01:49,270 --> 00:01:52,480
and when you come back to your computer

00:01:50,440 --> 00:01:55,020
or your phone you should pick up right

00:01:52,480 --> 00:01:55,020
where you left off

00:02:02,990 --> 00:02:06,020
it's always seemed like a problem to me

00:02:04,909 --> 00:02:07,490
that when I look at my email

00:02:06,020 --> 00:02:07,850
occasionally I see a message for my

00:02:07,490 --> 00:02:09,319
mother

00:02:07,850 --> 00:02:11,060
sandwiched between a bank statement in a

00:02:09,319 --> 00:02:11,270
bill we should be able to do better than

00:02:11,060 --> 00:02:14,900
that

00:02:11,270 --> 00:02:16,040
so we created the social in box when you

00:02:14,900 --> 00:02:17,330
log into Facebook and look at your

00:02:16,040 --> 00:02:18,980
messages all you're going to see by

00:02:17,330 --> 00:02:22,390
default are messages from your friends

00:02:18,980 --> 00:02:22,390
and their friends and that's it

00:02:31,190 --> 00:02:34,560
historically once you give out your

00:02:33,030 --> 00:02:35,850
email address or phone number it's just

00:02:34,560 --> 00:02:38,640
a matter of time before it winds up in

00:02:35,850 --> 00:02:40,920
the hands the wrong person and your only

00:02:38,640 --> 00:02:43,080
options at that point to change it or

00:02:40,920 --> 00:02:44,190
live with the compromise we believe

00:02:43,080 --> 00:02:45,780
people should have control over what

00:02:44,190 --> 00:02:47,490
gets delivered to their inbox no matter

00:02:45,780 --> 00:02:49,200
what the media and so with the new

00:02:47,490 --> 00:02:51,120
Facebook messages if you change your

00:02:49,200 --> 00:02:52,860
privacy settings such that only friends

00:02:51,120 --> 00:02:54,360
can send you messages then we will

00:02:52,860 --> 00:02:55,820
actually bounce emails that come from

00:02:54,360 --> 00:02:59,070
anyone who's not your friend

00:02:55,820 --> 00:03:01,080
by default the inbox only shows messages

00:02:59,070 --> 00:03:02,160
from your friends and their friends but

00:03:01,080 --> 00:03:04,020
if your grandmother who all needs as

00:03:02,160 --> 00:03:06,060
email sends you a message and it finds

00:03:04,020 --> 00:03:07,560
its way into the other folder you can

00:03:06,060 --> 00:03:09,210
always promote it into your inbox and

00:03:07,560 --> 00:03:11,700
from then on your conversations with her

00:03:09,210 --> 00:03:13,650
will be front and center and of course

00:03:11,700 --> 00:03:15,630
you can block any one or any email from

00:03:13,650 --> 00:03:17,130
sending you messages between this and

00:03:15,630 --> 00:03:20,570
our privacy settings it's like your own

00:03:17,130 --> 00:03:20,570
personal do not email list

00:03:23,810 --> 00:03:29,150
the most important part of messages is

00:03:25,910 --> 00:03:31,100
context you know email is organized by

00:03:29,150 --> 00:03:32,690
subject lines when we look at the

00:03:31,100 --> 00:03:34,760
subject lines for Facebook messages the

00:03:32,690 --> 00:03:36,650
top three were no subject hi

00:03:34,760 --> 00:03:42,230
and yo it's not a great way to organize

00:03:36,650 --> 00:03:45,069
conversations we said it the organized

00:03:42,230 --> 00:03:47,660
conversations around people not subject

00:03:45,069 --> 00:03:49,130
it also made sense to bring together all

00:03:47,660 --> 00:03:51,050
the different conversations you've ever

00:03:49,130 --> 00:03:53,270
had with one person into a single thread

00:03:51,050 --> 00:03:56,450
and what those happen in chat or SMS or

00:03:53,270 --> 00:03:57,680
email those all now live in one place if

00:03:56,450 --> 00:03:59,690
you want to reach out to your friend via

00:03:57,680 --> 00:04:02,239
text and they want to respond via email

00:03:59,690 --> 00:04:03,500
that's possible if you want to use chat

00:04:02,239 --> 00:04:05,390
while they're using Facebook messages

00:04:03,500 --> 00:04:07,190
that'll work too and all those different

00:04:05,390 --> 00:04:10,670
communications will all live in one

00:04:07,190 --> 00:04:12,260
conversation imagine you the entire

00:04:10,670 --> 00:04:13,910
history of your conversations with your

00:04:12,260 --> 00:04:15,920
boyfriend or your girlfriend I mean

00:04:13,910 --> 00:04:18,410
everything from hey you want to get

00:04:15,920 --> 00:04:19,430
coffee later all the way to you've got

00:04:18,410 --> 00:04:21,979
to pick up the kids to hide soccer

00:04:19,430 --> 00:04:23,030
practice my grandmother had that it was

00:04:21,979 --> 00:04:24,310
a box of letters written by my

00:04:23,030 --> 00:04:26,450
grandfather from when they were dating

00:04:24,310 --> 00:04:27,890
that kind of thing is increasingly rare

00:04:26,450 --> 00:04:29,930
and I'm left to ask you know where is my

00:04:27,890 --> 00:04:32,450
box of letters it's locked up on a phone

00:04:29,930 --> 00:04:36,040
it's locked up an email it's not in one

00:04:32,450 --> 00:04:36,040
place until now

00:04:42,840 --> 00:04:49,199
Americans we love to act sentimental

00:04:44,940 --> 00:04:53,130
don't we hi so my name is Nicolas

00:04:49,199 --> 00:04:55,860
Spiegel Berg I am an engineer a software

00:04:53,130 --> 00:04:57,510
engineer with Facebook in particular I

00:04:55,860 --> 00:05:00,300
joined Facebook a year and a half ago

00:04:57,510 --> 00:05:02,520
right when this messages product was

00:05:00,300 --> 00:05:04,979
just getting off the ground and I joined

00:05:02,520 --> 00:05:06,840
in working on the new storage layer that

00:05:04,979 --> 00:05:08,639
we're using to power this messages and

00:05:06,840 --> 00:05:12,419
the power of the volume the high volume

00:05:08,639 --> 00:05:13,650
of data that goes through it so I want

00:05:12,419 --> 00:05:15,780
to talk a little bit about the

00:05:13,650 --> 00:05:17,880
technology behind messages and in

00:05:15,780 --> 00:05:21,630
particular the open source technology

00:05:17,880 --> 00:05:23,910
about with messages we use a lot of open

00:05:21,630 --> 00:05:26,190
source solutions we make sure that we

00:05:23,910 --> 00:05:27,990
publicize those open source solutions as

00:05:26,190 --> 00:05:29,580
quickly as possible I mean for us it's

00:05:27,990 --> 00:05:31,740
really beneficial that we don't have all

00:05:29,580 --> 00:05:33,630
the secret sauce here that we're just

00:05:31,740 --> 00:05:36,660
contributing back with the community and

00:05:33,630 --> 00:05:42,630
working together to build great scalable

00:05:36,660 --> 00:05:44,070
solutions for technology so a messages

00:05:42,630 --> 00:05:47,550
product I don't know if you've seen the

00:05:44,070 --> 00:05:50,550
new one you know more product side again

00:05:47,550 --> 00:05:53,639
on more infrastructure side but the big

00:05:50,550 --> 00:05:55,440
thing is being able to have carry on a

00:05:53,639 --> 00:05:57,960
chat message with somebody else who's

00:05:55,440 --> 00:06:00,120
going over SMS and not having to really

00:05:57,960 --> 00:06:01,830
understand what the medium is that

00:06:00,120 --> 00:06:03,810
you're giving these messages by to just

00:06:01,830 --> 00:06:05,669
have it all sort of be seamless and to

00:06:03,810 --> 00:06:08,430
have it all being focused on the people

00:06:05,669 --> 00:06:10,650
right it's not the subject isn't hey yo

00:06:08,430 --> 00:06:14,039
the subject is my friend will Bailey

00:06:10,650 --> 00:06:16,919
right that's the difference so the major

00:06:14,039 --> 00:06:19,860
pieces that we had for the new Facebook

00:06:16,919 --> 00:06:23,940
messaging product was we needed a

00:06:19,860 --> 00:06:25,889
frontside a client product right we

00:06:23,940 --> 00:06:28,700
needed a male pipeline to handle the new

00:06:25,889 --> 00:06:32,310
email we already had sort of an XMPP

00:06:28,700 --> 00:06:34,530
layer with chat we needed to add spam

00:06:32,310 --> 00:06:37,889
filters to make sure that you only got

00:06:34,530 --> 00:06:40,289
emails you cared about then we need an

00:06:37,889 --> 00:06:43,250
application server to sort of move a lot

00:06:40,289 --> 00:06:45,419
of the the load that with smaller

00:06:43,250 --> 00:06:47,099
services you could just keep it on PHP

00:06:45,419 --> 00:06:49,470
we needed to move that to something like

00:06:47,099 --> 00:06:51,900
Java or C++ where we could handle the

00:06:49,470 --> 00:06:53,880
load better and then the main area that

00:06:51,900 --> 00:06:56,639
I'm working on and have expertise on is

00:06:53,880 --> 00:07:00,330
the last two areas which is the storage

00:06:56,639 --> 00:07:06,210
and migration so what sort of things do

00:07:00,330 --> 00:07:07,919
we need to store so the four main areas

00:07:06,210 --> 00:07:10,129
where we can communicate here within the

00:07:07,919 --> 00:07:13,620
Facebook messages product our

00:07:10,129 --> 00:07:15,960
traditional messages pipeline which of

00:07:13,620 --> 00:07:19,860
course already had tons of data in it

00:07:15,960 --> 00:07:22,370
our chat which we went from having sort

00:07:19,860 --> 00:07:24,810
of this temporary thing that's you know

00:07:22,370 --> 00:07:26,639
goes away every week or so to being

00:07:24,810 --> 00:07:28,979
something persistent that's interleaved

00:07:26,639 --> 00:07:30,240
with your messages that you can chat

00:07:28,979 --> 00:07:32,370
with somebody who has an email address

00:07:30,240 --> 00:07:35,189
you can chat with somebody who has an

00:07:32,370 --> 00:07:39,349
SMS all that stuff is logged and it's

00:07:35,189 --> 00:07:39,349
one nice big personal history together

00:07:39,439 --> 00:07:44,610
so what sort of volume is our

00:07:42,090 --> 00:07:47,550
infrastructure having to handle for this

00:07:44,610 --> 00:07:49,560
new messages product we haven't taken

00:07:47,550 --> 00:07:51,090
measurements recently but the most

00:07:49,560 --> 00:07:53,819
recent measurements we sat down and did

00:07:51,090 --> 00:07:56,849
was the monthly data prior to launch

00:07:53,819 --> 00:07:59,759
which is November and at the time we

00:07:56,849 --> 00:08:01,889
were doing 15 billion email messages

00:07:59,759 --> 00:08:05,810
with our legacy email product we were

00:08:01,889 --> 00:08:08,580
doing 120 billion chat messages and

00:08:05,810 --> 00:08:10,289
obviously our regular mail messages were

00:08:08,580 --> 00:08:12,000
a little bit larger there about 1k when

00:08:10,289 --> 00:08:13,949
we looked at it and it was about a

00:08:12,000 --> 00:08:15,479
hundred bytes for chat messages but

00:08:13,949 --> 00:08:17,069
still between those two that means that

00:08:15,479 --> 00:08:19,589
we're getting in every single month

00:08:17,069 --> 00:08:22,259
around 25 terabytes worth of data that

00:08:19,589 --> 00:08:25,529
we need to store the other interesting

00:08:22,259 --> 00:08:27,270
thing about this data is that you know

00:08:25,529 --> 00:08:28,439
this is constantly growing I mean the

00:08:27,270 --> 00:08:30,539
number of people that are adding

00:08:28,439 --> 00:08:33,029
messages versus the leading messages the

00:08:30,539 --> 00:08:35,760
deletions are such a smaller portion so

00:08:33,029 --> 00:08:38,010
we need to have a data store solution

00:08:35,760 --> 00:08:40,140
that can not only handle this monthly

00:08:38,010 --> 00:08:43,140
volume but can handle this monthly

00:08:40,140 --> 00:08:47,040
volume coming in for years and as the

00:08:43,140 --> 00:08:49,620
data store needs to grow so what's

00:08:47,040 --> 00:08:50,399
really interesting is for our data

00:08:49,620 --> 00:08:52,949
back-end

00:08:50,399 --> 00:08:56,490
one thing that we did not choose was my

00:08:52,949 --> 00:08:59,040
sequel we actually decided to use HBase

00:08:56,490 --> 00:09:02,459
for our small and medium data so your

00:08:59,040 --> 00:09:04,019
normal messages your chats if we're

00:09:02,459 --> 00:09:06,480
going to do like a cache of your front

00:09:04,019 --> 00:09:09,990
page a snapshot of that and also your

00:09:06,480 --> 00:09:12,060
inverted search index for attachments

00:09:09,990 --> 00:09:13,740
large messages we already had a really

00:09:12,060 --> 00:09:14,820
good existing infrastructure with

00:09:13,740 --> 00:09:16,320
haystack

00:09:14,820 --> 00:09:17,730
which again is a sort of a separate

00:09:16,320 --> 00:09:21,959
product that they're working on trying

00:09:17,730 --> 00:09:24,690
to get that more public as well but we

00:09:21,959 --> 00:09:26,760
have them handle attachments for us so

00:09:24,690 --> 00:09:28,050
we can really focus on sort of small to

00:09:26,760 --> 00:09:32,070
medium data load that's constantly

00:09:28,050 --> 00:09:33,870
growing so the basic gist of our

00:09:32,070 --> 00:09:38,130
architecture is we have this client

00:09:33,870 --> 00:09:39,720
front-end and if you're used to sort of

00:09:38,130 --> 00:09:41,279
my sequel world which you realize is

00:09:39,720 --> 00:09:45,300
when you go past a scale of a single

00:09:41,279 --> 00:09:47,310
service server you need to shard and you

00:09:45,300 --> 00:09:48,660
know in facebook terms those shards

00:09:47,310 --> 00:09:50,550
you're talking of thousands and

00:09:48,660 --> 00:09:52,649
thousands of servers that are charting

00:09:50,550 --> 00:09:55,920
for a single table that's not really

00:09:52,649 --> 00:09:58,170
acceptable we can handle smaller levels

00:09:55,920 --> 00:10:00,870
of shards but we really would like to

00:09:58,170 --> 00:10:04,170
have less granular sharding we would

00:10:00,870 --> 00:10:07,220
like to have ability to grow our cluster

00:10:04,170 --> 00:10:09,630
as we need to without needing to

00:10:07,220 --> 00:10:13,470
recharge our entire database so we have

00:10:09,630 --> 00:10:15,810
these cells for HBase for our smart

00:10:13,470 --> 00:10:18,480
scale storage and we have a user

00:10:15,810 --> 00:10:22,470
directory service which uses zookeeper

00:10:18,480 --> 00:10:24,270
to map every single Facebook ID to what

00:10:22,470 --> 00:10:27,839
sort of cluster so your traditional sort

00:10:24,270 --> 00:10:30,120
of sharding message so the client goes

00:10:27,839 --> 00:10:35,010
what's the cell for this user responds

00:10:30,120 --> 00:10:37,740
back cell one after then the client just

00:10:35,010 --> 00:10:39,600
can talk directly to the cell which has

00:10:37,740 --> 00:10:41,880
the application server which has all the

00:10:39,600 --> 00:10:44,070
storage information the application

00:10:41,880 --> 00:10:48,750
server can then contact haystack or

00:10:44,070 --> 00:10:51,660
HBase as needed and again like I said we

00:10:48,750 --> 00:10:55,290
have multiple shards of these cells but

00:10:51,660 --> 00:10:57,420
versus sort of my sequel a single cell

00:10:55,290 --> 00:11:00,329
contains a lot more data we have less

00:10:57,420 --> 00:11:00,950
shards the net and I'll talk about that

00:11:00,329 --> 00:11:03,540
more in a minute

00:11:00,950 --> 00:11:06,899
so what sort of open source solutions

00:11:03,540 --> 00:11:09,450
are we using here I mean there's a

00:11:06,899 --> 00:11:10,350
couple on here that are very popular

00:11:09,450 --> 00:11:12,690
already

00:11:10,350 --> 00:11:15,180
we have memcached right so you can throw

00:11:12,690 --> 00:11:17,040
a memcache box in front and all of a

00:11:15,180 --> 00:11:19,260
sudden a lot of your read worries go

00:11:17,040 --> 00:11:20,699
away right so you have you need to have

00:11:19,260 --> 00:11:22,440
decent read performance on the backend

00:11:20,699 --> 00:11:25,320
you need to have good right format

00:11:22,440 --> 00:11:27,030
and you really need to now worry about

00:11:25,320 --> 00:11:30,180
sort of the the maintenance headaches

00:11:27,030 --> 00:11:33,770
with having to shard with having hot

00:11:30,180 --> 00:11:36,960
regions stuff like that for zoo keeper

00:11:33,770 --> 00:11:38,520
zookeeper is used actually for our user

00:11:36,960 --> 00:11:40,830
directory service which is doing this

00:11:38,520 --> 00:11:43,020
sort of mapping it's an eventually

00:11:40,830 --> 00:11:45,480
consistent data store but it's really

00:11:43,020 --> 00:11:48,060
meant for small amounts of data it's not

00:11:45,480 --> 00:11:53,100
meant for you know multiple terabytes

00:11:48,060 --> 00:11:55,110
worth of data HBase is really of the

00:11:53,100 --> 00:11:56,730
five it's the most controversial it's

00:11:55,110 --> 00:11:58,170
the one that we had to talk about the

00:11:56,730 --> 00:12:00,750
most it's the one that we had to do sort

00:11:58,170 --> 00:12:04,740
of the most of development on to get up

00:12:00,750 --> 00:12:06,840
to handling this sort of data volume and

00:12:04,740 --> 00:12:09,300
it's basically a database storage engine

00:12:06,840 --> 00:12:14,310
for message data you know the buzzword

00:12:09,300 --> 00:12:15,960
is no sequel right but basically you

00:12:14,310 --> 00:12:18,150
know we had to evaluate a lot of

00:12:15,960 --> 00:12:19,890
different data store Suites because we

00:12:18,150 --> 00:12:22,110
realized that this sort of sharding

00:12:19,890 --> 00:12:23,760
problem with my sequel wasn't going to

00:12:22,110 --> 00:12:26,870
go away and was going to require a lot

00:12:23,760 --> 00:12:31,230
of effort if we wanted to stay on that

00:12:26,870 --> 00:12:33,570
so HBase uses even underneath it HDFS

00:12:31,230 --> 00:12:35,580
which is a distributed file system so

00:12:33,570 --> 00:12:39,480
you can basically think of say 100 nodes

00:12:35,580 --> 00:12:42,420
is a single file system server it

00:12:39,480 --> 00:12:45,240
automatically handles if a server dies

00:12:42,420 --> 00:12:47,610
it handles doing multiple replication

00:12:45,240 --> 00:12:50,340
making sure you have data locality

00:12:47,610 --> 00:12:53,130
between your application and your file

00:12:50,340 --> 00:12:56,700
system and then finally we have a dupe

00:12:53,130 --> 00:12:59,580
for a lot of MapReduce jobs we had the

00:12:56,700 --> 00:13:04,380
question a couple months ago like how

00:12:59,580 --> 00:13:06,000
much do we use MapReduce and my coworker

00:13:04,380 --> 00:13:07,440
immediately responded like this is our

00:13:06,000 --> 00:13:08,850
bread and butter like we're going to go

00:13:07,440 --> 00:13:11,250
down if we don't have something like

00:13:08,850 --> 00:13:13,550
Hadoop to be able to handle all these

00:13:11,250 --> 00:13:15,420
synchronous tasks that we need for

00:13:13,550 --> 00:13:17,880
application the application and

00:13:15,420 --> 00:13:23,940
migration doing caching doing snapshots

00:13:17,880 --> 00:13:28,070
so the other ones are fairly well known

00:13:23,940 --> 00:13:34,790
the big mystery here is HBase

00:13:28,070 --> 00:13:35,850
so what exactly is HBase HBase is a

00:13:34,790 --> 00:13:37,769
strict

00:13:35,850 --> 00:13:40,319
consistent no sequel sort of

00:13:37,769 --> 00:13:44,250
architecture where what you have is you

00:13:40,319 --> 00:13:46,139
have a keeper cluster so that's doing

00:13:44,250 --> 00:13:48,810
sort of eventual consistency but you're

00:13:46,139 --> 00:13:51,750
being able to have say five zookeeper

00:13:48,810 --> 00:13:53,850
nodes so you don't have a single point

00:13:51,750 --> 00:13:55,889
of failure there the zookeeper cluster

00:13:53,850 --> 00:13:58,440
says who's the current master the

00:13:55,889 --> 00:14:01,370
Masters job is to handle if a server die

00:13:58,440 --> 00:14:05,730
is to do recovery to do load balancing

00:14:01,370 --> 00:14:07,079
and the the basic idea is you have these

00:14:05,730 --> 00:14:10,949
regions servers so they're your

00:14:07,079 --> 00:14:14,100
individual data stores but your actual

00:14:10,949 --> 00:14:16,920
data is sharded logically versus

00:14:14,100 --> 00:14:20,490
physically so each region server has a

00:14:16,920 --> 00:14:22,649
number of regions and so say you have

00:14:20,490 --> 00:14:24,779
ten regions per region server if you

00:14:22,649 --> 00:14:26,790
have a single server die then you can

00:14:24,779 --> 00:14:29,190
distribute the load right each new

00:14:26,790 --> 00:14:32,089
server only takes one tenth of that load

00:14:29,190 --> 00:14:35,069
instead of your traditional thing where

00:14:32,089 --> 00:14:36,449
maybe you have a master slave and you're

00:14:35,069 --> 00:14:39,449
doing double reads well now all of a

00:14:36,449 --> 00:14:43,319
sudden you got a 200% increase if one of

00:14:39,449 --> 00:14:44,970
them dies the main sort of idea is to

00:14:43,319 --> 00:14:47,430
keep in mind I'll get a little more

00:14:44,970 --> 00:14:49,230
technical at the end is you have this

00:14:47,430 --> 00:14:52,199
region server that's writing new

00:14:49,230 --> 00:14:54,240
database it's data it's writing it to a

00:14:52,199 --> 00:14:56,250
log so it's all doing sequential writes

00:14:54,240 --> 00:14:59,579
on disk one of the problems that we have

00:14:56,250 --> 00:15:01,889
with my sequel is that you can get into

00:14:59,579 --> 00:15:04,290
cases where you're doing a lot of random

00:15:01,889 --> 00:15:06,630
writes because of the tree updates

00:15:04,290 --> 00:15:09,839
whereas you have this log that's all

00:15:06,630 --> 00:15:12,240
doing sequential writes on disk and then

00:15:09,839 --> 00:15:15,389
as an asynchronous task when the log

00:15:12,240 --> 00:15:19,019
gets too large it flushes that to a

00:15:15,389 --> 00:15:23,310
basically like two level B tree file

00:15:19,019 --> 00:15:25,079
called a store file so basically when

00:15:23,310 --> 00:15:30,649
you do a get you're basically doing an n

00:15:25,079 --> 00:15:33,600
way merge of multiple store files and

00:15:30,649 --> 00:15:36,180
when that end gets too large of course

00:15:33,600 --> 00:15:37,889
your get performance is becoming

00:15:36,180 --> 00:15:39,480
problematic right you're having to do a

00:15:37,889 --> 00:15:42,720
lot of disk reads that's where this

00:15:39,480 --> 00:15:45,720
compaction comes in to unify those n

00:15:42,720 --> 00:15:48,000
files into one and then the last thing

00:15:45,720 --> 00:15:50,610
is if you have like I said the region

00:15:48,000 --> 00:15:54,120
server has multiple regions

00:15:50,610 --> 00:15:56,670
if a single region gets too large on a

00:15:54,120 --> 00:15:59,089
single server it can split so it can

00:15:56,670 --> 00:16:02,250
split that into two regions which again

00:15:59,089 --> 00:16:04,560
you get sort of this scenario where

00:16:02,250 --> 00:16:06,690
maybe a region server starts off with

00:16:04,560 --> 00:16:09,630
only ten regions but it doesn't have a

00:16:06,690 --> 00:16:11,310
lot of data there for data recovery is

00:16:09,630 --> 00:16:13,290
really quick there's not a lot of stuff

00:16:11,310 --> 00:16:16,170
to replay well all of a sudden your data

00:16:13,290 --> 00:16:17,670
store is starts growing but the

00:16:16,170 --> 00:16:19,589
splitting comes in and now instead of

00:16:17,670 --> 00:16:21,990
having ten regions you have 40 regions

00:16:19,589 --> 00:16:25,230
so now you have four times the amount of

00:16:21,990 --> 00:16:28,740
data but you can distribute this load

00:16:25,230 --> 00:16:31,769
now into 40 parallel tasks right so you

00:16:28,740 --> 00:16:38,639
can have fast recovery of a single

00:16:31,769 --> 00:16:41,880
server going down so choosing HBase uh

00:16:38,639 --> 00:16:43,860
you know it's a really hot topic of what

00:16:41,880 --> 00:16:46,589
sort of no sequel data store should you

00:16:43,860 --> 00:16:49,529
use what's the trade-offs I mean I think

00:16:46,589 --> 00:16:51,839
people tend to get very zealous about

00:16:49,529 --> 00:16:55,170
this and they have their favorites and

00:16:51,839 --> 00:16:57,209
they say why everything else is bad we

00:16:55,170 --> 00:16:58,769
don't really look at it that way we were

00:16:57,209 --> 00:17:01,139
really looking for a very pragmatic

00:16:58,769 --> 00:17:03,779
solution which was very slow time to

00:17:01,139 --> 00:17:06,209
market something that's interesting is

00:17:03,779 --> 00:17:09,030
one of the big competing no sequel data

00:17:06,209 --> 00:17:10,770
stores is Cassandra which actually the

00:17:09,030 --> 00:17:14,299
three original developers of Cassandra

00:17:10,770 --> 00:17:16,740
are on the team that's using HBase and

00:17:14,299 --> 00:17:18,689
Cassandra is a great database system

00:17:16,740 --> 00:17:20,669
it's just that when we sat there and

00:17:18,689 --> 00:17:21,929
evaluated for this exact product what do

00:17:20,669 --> 00:17:24,390
we need and what gives us the lowest

00:17:21,929 --> 00:17:26,790
time to market we saw that HBase had a

00:17:24,390 --> 00:17:30,080
number of advantages so the advantages

00:17:26,790 --> 00:17:32,429
were it has a strong consistency model a

00:17:30,080 --> 00:17:35,669
lot of people think that eventual

00:17:32,429 --> 00:17:39,360
consistency is not that big a deal it's

00:17:35,669 --> 00:17:40,650
not that hard to program in good luck if

00:17:39,360 --> 00:17:42,240
you think that you might not be doing it

00:17:40,650 --> 00:17:44,820
quite the correct way it's it's really

00:17:42,240 --> 00:17:46,740
rough even with zookeeper and having say

00:17:44,820 --> 00:17:48,570
a thousand lines of code in HBase to

00:17:46,740 --> 00:17:50,190
deal with this eventual consistency we

00:17:48,570 --> 00:17:53,309
had a lot of problems we had a lot of

00:17:50,190 --> 00:17:54,450
really subtle bugs that show up the

00:17:53,309 --> 00:17:56,909
biggest problem with eventual

00:17:54,450 --> 00:17:59,490
consistency is here you do a write on X

00:17:56,909 --> 00:18:02,309
you immediately do a read you really

00:17:59,490 --> 00:18:04,070
have no guarantee that you're up-to-date

00:18:02,309 --> 00:18:05,899
on the data when you do the read

00:18:04,070 --> 00:18:07,940
right because you could you're right

00:18:05,899 --> 00:18:09,259
doesn't go to all the servers at once so

00:18:07,940 --> 00:18:13,609
you could be getting stale data

00:18:09,259 --> 00:18:15,799
immediately after you wrote to it so

00:18:13,609 --> 00:18:18,619
that was a great thing to us about HBase

00:18:15,799 --> 00:18:20,690
it also had this automatic failover like

00:18:18,619 --> 00:18:23,299
I said you have these region servers

00:18:20,690 --> 00:18:25,879
that if they die it can automatically

00:18:23,299 --> 00:18:28,729
paralyze recovery you know here you're

00:18:25,879 --> 00:18:31,039
talking trying to get it down to seconds

00:18:28,729 --> 00:18:34,549
for when a server dies and when all the

00:18:31,039 --> 00:18:37,820
users are up again and this is really

00:18:34,549 --> 00:18:39,889
great when failure is the norm and when

00:18:37,820 --> 00:18:41,690
you need like UDP class machines that

00:18:39,889 --> 00:18:44,450
need an out a lot of servicing with like

00:18:41,690 --> 00:18:49,549
my sequel you get into these issues

00:18:44,450 --> 00:18:52,369
where my sequel you do this master sort

00:18:49,549 --> 00:18:55,700
of slave thing you shard it you have say

00:18:52,369 --> 00:19:00,229
a thousand servers right so 500 masters

00:18:55,700 --> 00:19:01,789
500 slaves one server dies well you you

00:19:00,229 --> 00:19:04,159
got to get ops on it right away right

00:19:01,789 --> 00:19:06,139
because if that's say it's the master

00:19:04,159 --> 00:19:08,599
dies if the slave dies you just had data

00:19:06,139 --> 00:19:11,960
loss whereas here it's automatically

00:19:08,599 --> 00:19:14,869
redistributing the load for you we had

00:19:11,960 --> 00:19:17,029
situations where in our clusters we had

00:19:14,869 --> 00:19:19,009
10% of the machines down while we were

00:19:17,029 --> 00:19:20,840
doing on dark launch and you just didn't

00:19:19,009 --> 00:19:22,519
notice it it automatically failed over

00:19:20,840 --> 00:19:24,919
for you it automatically handled the

00:19:22,519 --> 00:19:27,139
load balancing for you and having to get

00:19:24,919 --> 00:19:29,059
those 10% of machines online was sort of

00:19:27,139 --> 00:19:30,200
an asynchronous background task instead

00:19:29,059 --> 00:19:32,299
of something that you really had to

00:19:30,200 --> 00:19:35,570
worry about right now which is a huge

00:19:32,299 --> 00:19:37,519
benefit for us so that talks about sort

00:19:35,570 --> 00:19:40,669
of the automatic failable over the

00:19:37,519 --> 00:19:42,139
multiple shards you know preventing the

00:19:40,669 --> 00:19:45,820
cascading failures that you can balance

00:19:42,139 --> 00:19:49,639
this stuff being able to do compression

00:19:45,820 --> 00:19:51,529
we do lzo compression we get like a five

00:19:49,639 --> 00:19:55,299
to one ratio on the disk which is really

00:19:51,529 --> 00:19:57,470
great uh let's see read-modify-write

00:19:55,299 --> 00:19:58,999
operations support like counter

00:19:57,470 --> 00:20:02,359
increments we have a lot of applications

00:19:58,999 --> 00:20:07,519
that overwrite data or that increment

00:20:02,359 --> 00:20:10,059
data a lot of HBase was optimized for

00:20:07,519 --> 00:20:12,529
sort of the counter stats use case and

00:20:10,059 --> 00:20:15,259
MapReduce is supported out of the box it

00:20:12,529 --> 00:20:17,510
interesting thing you know you look at

00:20:15,259 --> 00:20:20,180
the old SVN revisions

00:20:17,510 --> 00:20:23,270
a doug cutting he's a old hadoop guy you

00:20:20,180 --> 00:20:25,550
can see his him in svm blame a couple

00:20:23,270 --> 00:20:27,410
times here there's this whole ecosystem

00:20:25,550 --> 00:20:30,710
where the same people that worked on

00:20:27,410 --> 00:20:34,280
Hadoop and HDFS we're also working on

00:20:30,710 --> 00:20:35,900
HBase and we have a large data storage

00:20:34,280 --> 00:20:37,640
team so it's great that we can take

00:20:35,900 --> 00:20:39,560
these data storage guys that have

00:20:37,640 --> 00:20:41,690
experience with these subsystems we can

00:20:39,560 --> 00:20:44,510
easily bring them on and have them help

00:20:41,690 --> 00:20:45,530
out with the HBase use case at times and

00:20:44,510 --> 00:20:47,750
you're not having to necessarily

00:20:45,530 --> 00:20:52,450
separate like your data analytics team

00:20:47,750 --> 00:20:52,450
and your database team which is good

00:20:53,000 --> 00:20:57,830
HBase uses HDFS like I said it has a lot

00:20:55,760 --> 00:21:00,620
of really attractive features when we

00:20:57,830 --> 00:21:02,900
were looking at say Cassandra we were

00:21:00,620 --> 00:21:06,080
going to have to do strict consistency

00:21:02,900 --> 00:21:07,610
model whereas HDFS just had this out of

00:21:06,080 --> 00:21:10,670
the box they had checksums

00:21:07,610 --> 00:21:11,990
I know Cassandra has been working on

00:21:10,670 --> 00:21:13,460
getting some of those things in but the

00:21:11,990 --> 00:21:15,020
nice thing is is here we had these

00:21:13,460 --> 00:21:17,210
developers that had three years

00:21:15,020 --> 00:21:20,060
experience that knew you know exactly

00:21:17,210 --> 00:21:22,010
what was wrong with HDFS exactly what

00:21:20,060 --> 00:21:24,500
the little plan points were and how to

00:21:22,010 --> 00:21:26,000
administer it so we really only had to

00:21:24,500 --> 00:21:27,470
focus on the database side of things we

00:21:26,000 --> 00:21:30,260
didn't have to focus on the data loss

00:21:27,470 --> 00:21:33,110
side of things due to file system

00:21:30,260 --> 00:21:35,540
replication like I said it's battle

00:21:33,110 --> 00:21:38,300
tested as as well it's running pay to

00:21:35,540 --> 00:21:41,630
byte scale clusters a lot of our hive

00:21:38,300 --> 00:21:43,520
clusters are Hadoop clusters HDFS is

00:21:41,630 --> 00:21:51,080
sort of the core file system technology

00:21:43,520 --> 00:21:54,980
underlying all of that so that doesn't

00:21:51,080 --> 00:21:56,540
show that well so interesting thing a

00:21:54,980 --> 00:21:58,430
lot of people asked about sort of the

00:21:56,540 --> 00:22:00,290
cluster layout like I said we have

00:21:58,430 --> 00:22:02,060
multiple clusters that we're using to

00:22:00,290 --> 00:22:04,520
handle this messaging product but we

00:22:02,060 --> 00:22:06,110
basically you know when we're talking

00:22:04,520 --> 00:22:08,150
about a server cluster we're talking

00:22:06,110 --> 00:22:12,620
about a hundred servers

00:22:08,150 --> 00:22:17,360
we're sharding them so that you know you

00:22:12,620 --> 00:22:19,010
have a switch per rack right and then

00:22:17,360 --> 00:22:20,930
you have a master switch connecting them

00:22:19,010 --> 00:22:24,380
so you have 20 servers per rack so you

00:22:20,930 --> 00:22:26,270
can get a lot of you know when you're

00:22:24,380 --> 00:22:27,980
having to write 2-3 replicas one of the

00:22:26,270 --> 00:22:30,230
big problems that you get into is you

00:22:27,980 --> 00:22:31,399
have network i/o problems now right so

00:22:30,230 --> 00:22:32,989
if you can get a lot of

00:22:31,399 --> 00:22:34,849
data locality and if you can get a lot

00:22:32,989 --> 00:22:36,710
of wrack locality so maybe you're only

00:22:34,849 --> 00:22:40,639
having to send run replicas actually

00:22:36,710 --> 00:22:42,889
over to the master switch versus if

00:22:40,639 --> 00:22:44,570
you're doing sort of like a Cassandra

00:22:42,889 --> 00:22:46,429
model where you're writing all three of

00:22:44,570 --> 00:22:50,839
them simultaneously you might be

00:22:46,429 --> 00:22:52,519
flooding the master switch so we

00:22:50,839 --> 00:22:54,799
basically separate it into twenty

00:22:52,519 --> 00:22:57,169
servers per rack five racks we try to

00:22:54,799 --> 00:22:59,529
isolate all of our components so that we

00:22:57,169 --> 00:23:03,229
can really trace if a master goes down

00:22:59,529 --> 00:23:05,389
as you saw I mean there's HBase there's

00:23:03,229 --> 00:23:07,969
HDFS and there's a dupe so there's a lot

00:23:05,389 --> 00:23:10,159
of big subsystems and processes that are

00:23:07,969 --> 00:23:12,289
all interacting so being able to isolate

00:23:10,159 --> 00:23:16,700
that is really good for debugging

00:23:12,289 --> 00:23:21,649
capability for the region servers we

00:23:16,700 --> 00:23:23,869
have you know basically we try to go for

00:23:21,649 --> 00:23:25,909
more hard drives because the more high

00:23:23,869 --> 00:23:28,339
you know it's better to have you know

00:23:25,909 --> 00:23:30,259
for one terabyte hard drives than to

00:23:28,339 --> 00:23:32,719
have one 4 terabyte hard drive because

00:23:30,259 --> 00:23:34,339
you start getting bottlenecked on the

00:23:32,719 --> 00:23:36,229
disk writing there right where you can

00:23:34,339 --> 00:23:42,999
paralyze that for ways if you have for

00:23:36,229 --> 00:23:45,229
one terabyte drives so data migration

00:23:42,999 --> 00:23:48,830
you know it was a big problem like I

00:23:45,229 --> 00:23:51,710
said you're getting 25 terabytes of data

00:23:48,830 --> 00:23:54,739
a month coming in we have this existed

00:23:51,710 --> 00:23:57,039
messages product people expect their old

00:23:54,739 --> 00:23:59,479
messages to work on this new format

00:23:57,039 --> 00:24:00,710
we're dealing with huge scale this was

00:23:59,479 --> 00:24:03,139
all stored in my sequel in the

00:24:00,710 --> 00:24:12,320
background and we have to go to HBase

00:24:03,139 --> 00:24:15,489
for the win okay so one of the biggest

00:24:12,320 --> 00:24:19,879
problems is that my sequel is normalized

00:24:15,489 --> 00:24:22,369
so we have three main tables that we

00:24:19,879 --> 00:24:25,969
basically had to denormalize migrated

00:24:22,369 --> 00:24:28,879
user over and it's one insanely huge

00:24:25,969 --> 00:24:31,099
join of terabytes worth of data spread

00:24:28,879 --> 00:24:34,210
across many different machines you know

00:24:31,099 --> 00:24:34,210
slightly problematic Oh

00:24:35,520 --> 00:24:39,910
yeah but so multiple terabytes of data

00:24:38,350 --> 00:24:43,060
spread across machines failure is the

00:24:39,910 --> 00:24:44,620
norm and you get really odd problems so

00:24:43,060 --> 00:24:48,730
one thing that was kind of anecdotal

00:24:44,620 --> 00:24:50,230
that we discovered is as you saw in

00:24:48,730 --> 00:24:51,730
there you basically have an inbox for

00:24:50,230 --> 00:24:54,220
your friends and then an inbox for other

00:24:51,730 --> 00:24:56,350
people who aren't your friends uh you

00:24:54,220 --> 00:24:58,840
know I guess we have these teeny boppers

00:24:56,350 --> 00:25:00,700
in America that they don't want to be

00:24:58,840 --> 00:25:02,200
tagged in photos so what they'll do is

00:25:00,700 --> 00:25:04,810
they'll actually like destroy their

00:25:02,200 --> 00:25:06,190
account when they leave and then

00:25:04,810 --> 00:25:08,800
recreate their account when they log

00:25:06,190 --> 00:25:10,300
back on so people can't tag them but

00:25:08,800 --> 00:25:12,220
that's yeah that's great when you're

00:25:10,300 --> 00:25:13,930
having to you know move terabytes and

00:25:12,220 --> 00:25:16,030
terabytes of data over and depending on

00:25:13,930 --> 00:25:17,710
what time you ask they either have no

00:25:16,030 --> 00:25:22,030
friends and their account doesn't exist

00:25:17,710 --> 00:25:24,700
or they have 500 friends so we had a lot

00:25:22,030 --> 00:25:27,370
of interesting problems and you know a

00:25:24,700 --> 00:25:30,040
lot of times it what's the failure the

00:25:27,370 --> 00:25:33,100
reason for it in that case the failure

00:25:30,040 --> 00:25:34,660
wasn't even anything operationally

00:25:33,100 --> 00:25:36,130
though it was wrong it was the users

00:25:34,660 --> 00:25:38,670
were destroying and creating accounts

00:25:36,130 --> 00:25:43,480
and I guess we still let him do it so

00:25:38,670 --> 00:25:46,060
more power uh so basically we get a

00:25:43,480 --> 00:25:49,690
snapshot of the my sequel data and we

00:25:46,060 --> 00:25:51,220
stored into HBase we store it in sort of

00:25:49,690 --> 00:25:53,530
a flat structure that's not really

00:25:51,220 --> 00:25:55,420
organized and then we join the data with

00:25:53,530 --> 00:25:58,180
MapReduce to actually put it in the

00:25:55,420 --> 00:26:00,040
table in the organized format load it

00:25:58,180 --> 00:26:02,260
into the final cluster using bulk

00:26:00,040 --> 00:26:03,640
loaders um so something that's

00:26:02,260 --> 00:26:07,210
interesting is a lot of people will do

00:26:03,640 --> 00:26:09,420
like HBase performance stats and they'll

00:26:07,210 --> 00:26:13,360
talk about like put operational costs

00:26:09,420 --> 00:26:15,370
one of the benefits that my HBase has is

00:26:13,360 --> 00:26:17,590
if you're importing data while you're

00:26:15,370 --> 00:26:19,360
serving live cluster data right if you

00:26:17,590 --> 00:26:21,720
just do a bunch of puts well then you're

00:26:19,360 --> 00:26:24,580
worried about your synchronization locks

00:26:21,720 --> 00:26:26,650
your intermixing your live data with

00:26:24,580 --> 00:26:28,990
this sort of background data that's not

00:26:26,650 --> 00:26:31,180
as performance oriented it's great if

00:26:28,990 --> 00:26:33,790
just off in the background you can

00:26:31,180 --> 00:26:36,040
create these b-tree tables or store file

00:26:33,790 --> 00:26:38,350
tables yourself and then just shove them

00:26:36,040 --> 00:26:40,300
in after you created your entire shard

00:26:38,350 --> 00:26:41,980
of the database so that's what we do

00:26:40,300 --> 00:26:45,700
with these bulk loaders to sort of take

00:26:41,980 --> 00:26:47,770
the load off our live data as we're

00:26:45,700 --> 00:26:48,320
importing you know millions of users in

00:26:47,770 --> 00:26:55,850
at a time

00:26:48,320 --> 00:26:58,779
um see yeah let's get this I'm not at my

00:26:55,850 --> 00:27:04,669
gracing guy I just work with them on TV

00:26:58,779 --> 00:27:07,700
okay working with HBase and HDFS so our

00:27:04,669 --> 00:27:10,250
main goal one of the big problems with a

00:27:07,700 --> 00:27:14,179
lot of these no sequel databases is data

00:27:10,250 --> 00:27:16,909
loss yes a lot of algorithms sound

00:27:14,179 --> 00:27:18,799
wonderful on paper and then you look

00:27:16,909 --> 00:27:21,139
into the gritty details of what happens

00:27:18,799 --> 00:27:23,779
if server a dies followed by server be

00:27:21,139 --> 00:27:26,750
followed by server C oh you get data

00:27:23,779 --> 00:27:29,149
loss but that's not a big deal well you

00:27:26,750 --> 00:27:31,549
know with Facebook messages people tend

00:27:29,149 --> 00:27:33,980
to have this tiny little strong opinion

00:27:31,549 --> 00:27:35,210
about Facebook for whatever reason so we

00:27:33,980 --> 00:27:41,929
got to make sure we're not losing your

00:27:35,210 --> 00:27:43,490
data and protecting it so the goal of

00:27:41,929 --> 00:27:44,960
zero data loss that was one of the main

00:27:43,490 --> 00:27:47,210
things we worked on that was the one

00:27:44,960 --> 00:27:49,129
that was the main thing we were worried

00:27:47,210 --> 00:27:53,389
about was making sure that no users lost

00:27:49,129 --> 00:27:55,370
data that in the face of normal failures

00:27:53,389 --> 00:27:56,779
you could have crazy scenarios happen

00:27:55,370 --> 00:27:59,919
and we would still be able to recover

00:27:56,779 --> 00:28:02,450
for it so one of the main problems was

00:27:59,919 --> 00:28:04,850
HBase didn't have sink support so if

00:28:02,450 --> 00:28:08,570
your server died you lost say the last

00:28:04,850 --> 00:28:12,259
seconds worth of data because a HDFS the

00:28:08,570 --> 00:28:14,210
master and the slaves weren't

00:28:12,259 --> 00:28:18,590
communicating properly so we added that

00:28:14,210 --> 00:28:20,720
in we added a acid compliant writes at a

00:28:18,590 --> 00:28:23,419
row level granularity so that we could

00:28:20,720 --> 00:28:25,100
do acid sort of transactions here we did

00:28:23,419 --> 00:28:27,679
early log rolling so we found a lot of

00:28:25,100 --> 00:28:29,720
cases where we did a lot of tight

00:28:27,679 --> 00:28:32,919
integration between the file system and

00:28:29,720 --> 00:28:35,509
the database so the second that we saw

00:28:32,919 --> 00:28:38,720
everything by default is stored in three

00:28:35,509 --> 00:28:41,899
replicas right you could get into these

00:28:38,720 --> 00:28:43,370
weird scenarios where one slave died so

00:28:41,899 --> 00:28:45,529
now you have two replicas but you go on

00:28:43,370 --> 00:28:47,120
your merry little way now another slave

00:28:45,529 --> 00:28:49,159
dies you have one replicas you go on

00:28:47,120 --> 00:28:51,379
your merry little way right but you have

00:28:49,159 --> 00:28:53,149
now you have a j-bot disk with only one

00:28:51,379 --> 00:28:55,039
replicas and you could potentially have

00:28:53,149 --> 00:28:56,750
data loss so we did a lot of tight

00:28:55,039 --> 00:28:58,820
integration to make sure the second you

00:28:56,750 --> 00:29:00,889
went down to two replicas we started

00:28:58,820 --> 00:29:02,100
triggering replication we started

00:29:00,889 --> 00:29:04,110
rolling off to a new law

00:29:02,100 --> 00:29:07,320
so it we got a brand new 3-way

00:29:04,110 --> 00:29:09,990
replication scheme and then we also did

00:29:07,320 --> 00:29:12,929
a lot of stuff with the HBase master

00:29:09,990 --> 00:29:14,730
redesign one of the fundamental problems

00:29:12,929 --> 00:29:16,470
that you hear a lot of times with HBase

00:29:14,730 --> 00:29:19,590
is people will say well it's a single

00:29:16,470 --> 00:29:23,490
point of failure right which is normally

00:29:19,590 --> 00:29:25,710
a huge problem where we basically kind

00:29:23,490 --> 00:29:27,240
of moved HBase from being a single point

00:29:25,710 --> 00:29:29,669
of failure to where you can have a

00:29:27,240 --> 00:29:32,010
master you can have multiple backup

00:29:29,669 --> 00:29:34,230
masters that are willing to come in and

00:29:32,010 --> 00:29:37,140
then you really store your critical

00:29:34,230 --> 00:29:42,150
information that needs to be preserved

00:29:37,140 --> 00:29:43,830
across data server loss in zookeeper so

00:29:42,150 --> 00:29:45,750
like I said it's this eventual quorum

00:29:43,830 --> 00:29:46,890
that we needed the fast writes that was

00:29:45,750 --> 00:29:48,630
really sort of tricky because it was

00:29:46,890 --> 00:29:50,220
eventually consistent but the great

00:29:48,630 --> 00:29:53,280
thing about it is now we can have five

00:29:50,220 --> 00:29:55,230
servers so it you know you no longer

00:29:53,280 --> 00:29:56,640
have a single point of failure you have

00:29:55,230 --> 00:30:02,880
to have five things dying in a row

00:29:56,640 --> 00:30:07,760
before your hosts which is great okay um

00:30:02,880 --> 00:30:09,990
I mean a lot of it was sort of uh

00:30:07,760 --> 00:30:12,120
getting down to the nitty-gritty

00:30:09,990 --> 00:30:14,789
you know I think one of the problems

00:30:12,120 --> 00:30:17,820
that you hear a lot of times is people

00:30:14,789 --> 00:30:18,990
will do this amazing new technology and

00:30:17,820 --> 00:30:21,840
they'll want to add all these cool

00:30:18,990 --> 00:30:24,659
features I have added a new feature

00:30:21,840 --> 00:30:26,340
called - here called bloom filters which

00:30:24,659 --> 00:30:28,080
is doing probabilistic hashing

00:30:26,340 --> 00:30:30,360
algorithms and sounds great and I got to

00:30:28,080 --> 00:30:31,770
read a lot of PhD articles about it you

00:30:30,360 --> 00:30:35,400
know and then I get a 2x performance

00:30:31,770 --> 00:30:37,169
gain and then my boss comes around here

00:30:35,400 --> 00:30:39,780
and he starts looking at the

00:30:37,169 --> 00:30:42,330
decompression code and goes oh we should

00:30:39,780 --> 00:30:45,120
buffer those RPC calls 20x performance

00:30:42,330 --> 00:30:46,380
gain you know a lot of it is really sort

00:30:45,120 --> 00:30:48,860
of understanding your system

00:30:46,380 --> 00:30:51,360
understanding where the bottlenecks are

00:30:48,860 --> 00:30:52,799
getting down to the nitty-gritty making

00:30:51,360 --> 00:30:54,929
sure that you're not sitting there going

00:30:52,799 --> 00:30:57,600
well I'm not exactly sure how this works

00:30:54,929 --> 00:30:59,940
you know we can't afford to say I'm not

00:30:57,600 --> 00:31:01,620
exactly sure how this works we get a

00:30:59,940 --> 00:31:03,570
random unit test failure that happens

00:31:01,620 --> 00:31:05,250
one out of every 100 times it means we

00:31:03,570 --> 00:31:08,610
don't understand what's going on we got

00:31:05,250 --> 00:31:12,150
to go figure that out so we did a lot of

00:31:08,610 --> 00:31:14,010
work on really understanding the system

00:31:12,150 --> 00:31:15,480
really adding small availability and

00:31:14,010 --> 00:31:17,280
operational improvements to

00:31:15,480 --> 00:31:20,250
adding rolling restarts so we can

00:31:17,280 --> 00:31:22,740
upgrade versions without needing any

00:31:20,250 --> 00:31:25,590
downtime for the users being able to

00:31:22,740 --> 00:31:27,120
interrupt any long-running operations so

00:31:25,590 --> 00:31:28,800
you know a lot of times you create a

00:31:27,120 --> 00:31:29,730
server and then it just stalls for a

00:31:28,800 --> 00:31:32,190
minute because something weird is going

00:31:29,730 --> 00:31:34,020
on you're like I don't know what what it

00:31:32,190 --> 00:31:35,370
is we had to find all those little areas

00:31:34,020 --> 00:31:38,010
so that we could shut it down in a

00:31:35,370 --> 00:31:40,740
second restart it when we needed to

00:31:38,010 --> 00:31:42,540
HBase fsck to do background filesystem

00:31:40,740 --> 00:31:44,640
checking for us you know because we're

00:31:42,540 --> 00:31:46,740
not just storing your data we're storing

00:31:44,640 --> 00:31:47,970
your data we're backing it up and then

00:31:46,740 --> 00:31:49,680
we have separate utilities that are

00:31:47,970 --> 00:31:51,300
going there and scraping and making sure

00:31:49,680 --> 00:31:55,380
that both those systems are saying the

00:31:51,300 --> 00:31:57,540
correct things right like I said

00:31:55,380 --> 00:32:02,330
performance you have bloom filters

00:31:57,540 --> 00:32:05,190
column seeking a lot of this stuff was

00:32:02,330 --> 00:32:07,020
making sure that we minimized our disk

00:32:05,190 --> 00:32:09,510
seeks making sure that we parallels the

00:32:07,020 --> 00:32:11,340
eyes in the right locations really

00:32:09,510 --> 00:32:12,840
looking at our exact use case and

00:32:11,340 --> 00:32:14,760
instead of just adding a bunch of

00:32:12,840 --> 00:32:16,830
features that look cool adding features

00:32:14,760 --> 00:32:20,580
that would help us launch Facebook

00:32:16,830 --> 00:32:25,290
messages um yeah another interesting

00:32:20,580 --> 00:32:27,510
thing is so I think another important

00:32:25,290 --> 00:32:29,850
thing that we did was feature isolation

00:32:27,510 --> 00:32:32,010
I mean obviously with a bunch of new

00:32:29,850 --> 00:32:36,420
products that we had or integrating them

00:32:32,010 --> 00:32:38,360
together with HBase HDFS Hadoop we

00:32:36,420 --> 00:32:41,340
really wanted to sort of isolate

00:32:38,360 --> 00:32:43,290
problematic areas or just isolate you

00:32:41,340 --> 00:32:45,510
know if we don't know how a feature

00:32:43,290 --> 00:32:48,060
works we shouldn't just keep it unable

00:32:45,510 --> 00:32:49,680
on production we should disable it until

00:32:48,060 --> 00:32:51,300
we can learn how it works before we

00:32:49,680 --> 00:32:54,780
enable it one of the big things was

00:32:51,300 --> 00:32:56,790
doing those splits was sort of a

00:32:54,780 --> 00:32:59,700
temperamental sort of thing where you

00:32:56,790 --> 00:33:02,070
could have occasional data loss we had a

00:32:59,700 --> 00:33:03,510
lot of people working on that but we

00:33:02,070 --> 00:33:06,900
thought well that's not our focus our

00:33:03,510 --> 00:33:08,730
focus is consistent data we can just

00:33:06,900 --> 00:33:12,060
manually do the splits we can control

00:33:08,730 --> 00:33:14,250
them we can automatically shard them and

00:33:12,060 --> 00:33:16,860
that's really great for you know when

00:33:14,250 --> 00:33:18,780
you have a 100 thousand log line log

00:33:16,860 --> 00:33:21,870
file and you have to analyze your

00:33:18,780 --> 00:33:24,210
regions and then you have to analyze it

00:33:21,870 --> 00:33:25,770
over the past week well your region just

00:33:24,210 --> 00:33:26,740
got renamed from this to this to this to

00:33:25,770 --> 00:33:29,080
this

00:33:26,740 --> 00:33:30,790
and it's a nightmare to debug it's great

00:33:29,080 --> 00:33:32,770
if you can just have a consistent hash

00:33:30,790 --> 00:33:34,900
key that you can look up the region over

00:33:32,770 --> 00:33:41,590
a period of weeks and sort of do

00:33:34,900 --> 00:33:43,720
performance measuring on that so let's

00:33:41,590 --> 00:33:46,120
see other operational challenges one of

00:33:43,720 --> 00:33:48,220
the big things we try to do here is

00:33:46,120 --> 00:33:50,800
since we had such a huge amount of

00:33:48,220 --> 00:33:53,080
infrastructure we had a dark launch that

00:33:50,800 --> 00:33:57,670
was basically mirroring our existing

00:33:53,080 --> 00:34:01,240
inbox traffic so you know when we were

00:33:57,670 --> 00:34:03,610
when we were say it like 1% for release

00:34:01,240 --> 00:34:05,170
in dark launch we were at 10% so we were

00:34:03,610 --> 00:34:07,570
always making sure that we were scaling

00:34:05,170 --> 00:34:10,030
our dark launch above and beyond and

00:34:07,570 --> 00:34:13,810
trying to find failures well before the

00:34:10,030 --> 00:34:16,149
users would find them and you know the

00:34:13,810 --> 00:34:18,669
best data to work with is real data you

00:34:16,149 --> 00:34:20,710
can sit there on run benchmarks all day

00:34:18,669 --> 00:34:22,869
long but in the end if you can replicate

00:34:20,710 --> 00:34:25,540
the data if you can have it running over

00:34:22,869 --> 00:34:27,220
months at a time and make sure that you

00:34:25,540 --> 00:34:30,100
can find all the little weird bugs that

00:34:27,220 --> 00:34:33,669
happen once a month that's that's really

00:34:30,100 --> 00:34:35,230
the best way to do it let's see we did a

00:34:33,669 --> 00:34:40,840
lot of deployment monitoring sort of

00:34:35,230 --> 00:34:43,179
stuff tons of scripts dashboards one of

00:34:40,840 --> 00:34:45,310
the things is great is you go and look

00:34:43,179 --> 00:34:46,990
here on our HBase and we just have tons

00:34:45,310 --> 00:34:49,450
and tons of graphs and we sit there and

00:34:46,990 --> 00:34:51,760
analyze the graphs since we're doing

00:34:49,450 --> 00:34:53,950
manual splitting we can sit there and

00:34:51,760 --> 00:34:56,500
see how a region is behaving over months

00:34:53,950 --> 00:34:59,950
at a time and do certain performance

00:34:56,500 --> 00:35:03,310
tweaks understand how the system works

00:34:59,950 --> 00:35:08,280
as data grows and as you know certain

00:35:03,310 --> 00:35:08,280
ratios between our files change

00:35:10,070 --> 00:35:17,570
so the last area that I really want to

00:35:12,470 --> 00:35:21,710
talk about is working within the Apache

00:35:17,570 --> 00:35:25,100
community like I said my you know

00:35:21,710 --> 00:35:27,440
personal opinion is Facebook is a very

00:35:25,100 --> 00:35:29,120
open source friendly company you know I

00:35:27,440 --> 00:35:30,560
didn't have to go sign a whole bunch of

00:35:29,120 --> 00:35:32,570
waivers just to get started on this

00:35:30,560 --> 00:35:34,430
project in fact I didn't realize that

00:35:32,570 --> 00:35:35,780
companies made you sign waivers until a

00:35:34,430 --> 00:35:37,970
couple months after I was working on

00:35:35,780 --> 00:35:39,920
this and they said oh that's not us you

00:35:37,970 --> 00:35:42,080
know they and they really want you

00:35:39,920 --> 00:35:45,500
working closely with the community we

00:35:42,080 --> 00:35:47,900
started HBase wasn't like this thing

00:35:45,500 --> 00:35:49,970
that Facebook built and Facebook made

00:35:47,900 --> 00:35:52,040
popular right this is a product it's

00:35:49,970 --> 00:35:53,750
already been going for four years it's

00:35:52,040 --> 00:35:55,070
not like they had you know this sick

00:35:53,750 --> 00:35:57,020
community they already had this

00:35:55,070 --> 00:35:58,670
wonderful community of multiple

00:35:57,020 --> 00:36:00,500
companies with smart people that were

00:35:58,670 --> 00:36:01,850
working on this product and we didn't

00:36:00,500 --> 00:36:04,910
want to come and be the flash in the pan

00:36:01,850 --> 00:36:06,830
we wanted to come and work because we

00:36:04,910 --> 00:36:08,420
thought we could get our product done

00:36:06,830 --> 00:36:09,980
faster and help them get their products

00:36:08,420 --> 00:36:13,490
done faster if we work together in an

00:36:09,980 --> 00:36:17,780
open source environment we had in-house

00:36:13,490 --> 00:36:19,040
expertise and both HDFS and HBase which

00:36:17,780 --> 00:36:20,720
again like I said it need to be tightly

00:36:19,040 --> 00:36:23,660
integrated so we tried to bring in

00:36:20,720 --> 00:36:25,760
people from even outside of messages

00:36:23,660 --> 00:36:29,510
within Facebook to help work with this

00:36:25,760 --> 00:36:33,410
open source community we tried to when

00:36:29,510 --> 00:36:35,870
we're evaluating different Suites like

00:36:33,410 --> 00:36:37,550
Cassandra like hyper table we weren't

00:36:35,870 --> 00:36:39,110
just looking at what features they had

00:36:37,550 --> 00:36:40,850
as well we were looking at like what's

00:36:39,110 --> 00:36:44,210
the community want what are the

00:36:40,850 --> 00:36:46,040
community's goals is what we're doing

00:36:44,210 --> 00:36:48,410
helping the community and it's what the

00:36:46,040 --> 00:36:51,770
community doing so hopefully helping us

00:36:48,410 --> 00:36:54,170
as well and we try to increase community

00:36:51,770 --> 00:36:56,960
involvement uh you know there's active

00:36:54,170 --> 00:37:00,080
encouragement to submit dips and patches

00:36:56,960 --> 00:37:02,480
whenever possible I mean I personally

00:37:00,080 --> 00:37:04,460
try to stay within a week of the actual

00:37:02,480 --> 00:37:06,980
open source branch so I mean what you

00:37:04,460 --> 00:37:10,370
guys are seeing when you go on to HBase

00:37:06,980 --> 00:37:12,050
Apache org you know Facebook doesn't

00:37:10,370 --> 00:37:13,730
have this huge amount of secret sauce in

00:37:12,050 --> 00:37:15,500
the background you're roughly seeing

00:37:13,730 --> 00:37:18,380
exactly what Facebook's using you're

00:37:15,500 --> 00:37:19,760
seeing patches roughly as we create them

00:37:18,380 --> 00:37:22,430
we really want to work with the

00:37:19,760 --> 00:37:23,630
community we think it's in our best

00:37:22,430 --> 00:37:25,609
interest and in the

00:37:23,630 --> 00:37:30,079
these best interests if we're being open

00:37:25,609 --> 00:37:32,450
and sharing and additionally that means

00:37:30,079 --> 00:37:35,059
that like I'm a committer now with the

00:37:32,450 --> 00:37:39,049
HBase project we have another person

00:37:35,059 --> 00:37:40,670
who's in the primary committee and we're

00:37:39,049 --> 00:37:43,069
actively encouraging you know when we

00:37:40,670 --> 00:37:45,950
brought in coops tried to get them the

00:37:43,069 --> 00:37:48,559
community you know work with it try to

00:37:45,950 --> 00:37:50,450
get people outside of the project that

00:37:48,559 --> 00:37:52,729
had an interest in running HBase in

00:37:50,450 --> 00:37:54,529
their application to try to commit to

00:37:52,729 --> 00:37:56,059
the community not just have it be the

00:37:54,529 --> 00:38:00,920
little HBase group that's doing all the

00:37:56,059 --> 00:38:02,749
work and I mean there were massive

00:38:00,920 --> 00:38:03,950
feature improvements that we did with

00:38:02,749 --> 00:38:07,249
the community and we couldn't have done

00:38:03,950 --> 00:38:09,529
without it so we had this HDFS 20 up n

00:38:07,249 --> 00:38:13,309
branch so we not only had to work with

00:38:09,529 --> 00:38:14,589
multiple organizations within HBase we

00:38:13,309 --> 00:38:18,289
also had to work with multiple

00:38:14,589 --> 00:38:19,910
organizations within HDFS to try to

00:38:18,289 --> 00:38:22,789
integrate all this stuff together so

00:38:19,910 --> 00:38:24,890
that we can have zero data loss we did

00:38:22,789 --> 00:38:27,019
an HBase Master rewrite which basically

00:38:24,890 --> 00:38:28,160
took two of the people from HBase two of

00:38:27,019 --> 00:38:30,499
the people from the open source project

00:38:28,160 --> 00:38:32,089
I mean they sat there for hours and

00:38:30,499 --> 00:38:34,819
we're hashing out the design of this

00:38:32,089 --> 00:38:37,969
product and and building it

00:38:34,819 --> 00:38:40,670
collaboratively and continually

00:38:37,969 --> 00:38:42,349
interacting you know you can talk about

00:38:40,670 --> 00:38:43,640
major features but at the end of the day

00:38:42,349 --> 00:38:45,410
when you talk about working with the

00:38:43,640 --> 00:38:47,509
community it's a lot of little

00:38:45,410 --> 00:38:50,569
day-to-day stuff so an interesting

00:38:47,509 --> 00:38:53,200
problem that we had was about a week

00:38:50,569 --> 00:38:56,749
before our launch we were getting

00:38:53,200 --> 00:38:59,479
extremely large responses you know

00:38:56,749 --> 00:39:01,430
people texting up a storm and they go

00:38:59,479 --> 00:39:04,069
over two gigabytes worth of data that

00:39:01,430 --> 00:39:08,210
we're now RP seeing we had to learn how

00:39:04,069 --> 00:39:11,329
to shard that and a perfect example is

00:39:08,210 --> 00:39:13,759
we saw this and we went ah we have too

00:39:11,329 --> 00:39:16,160
large an RPC immediately what we do is

00:39:13,759 --> 00:39:18,920
we go on IRC we start chatting with the

00:39:16,160 --> 00:39:20,539
other PMC developers saying have you

00:39:18,920 --> 00:39:24,559
seen this problem before what's your

00:39:20,539 --> 00:39:27,049
thoughts uh we have a committer with

00:39:24,559 --> 00:39:28,579
StumbleUpon who goes oh yeah well that's

00:39:27,049 --> 00:39:30,859
actually a good feature to add I've been

00:39:28,579 --> 00:39:34,009
wanting to add that for a couple weeks

00:39:30,859 --> 00:39:35,509
it just wasn't it was next on my docket

00:39:34,009 --> 00:39:37,280
so you guys need it within a week right

00:39:35,509 --> 00:39:41,180
so

00:39:37,280 --> 00:39:43,730
he ends up writing the permanent feature

00:39:41,180 --> 00:39:46,460
meanwhile we have another guy within our

00:39:43,730 --> 00:39:48,350
team that's doing the sort of a

00:39:46,460 --> 00:39:52,310
lightweight feature doing performance

00:39:48,350 --> 00:39:53,780
testing on it that you know make sure

00:39:52,310 --> 00:39:55,820
that this is a theoretically sound

00:39:53,780 --> 00:39:58,580
implementation before we delve into the

00:39:55,820 --> 00:40:01,280
full thing he delves into it emails us

00:39:58,580 --> 00:40:03,200
the patch we go we do peer reviews on

00:40:01,280 --> 00:40:04,580
the patch send it back to him he goes

00:40:03,200 --> 00:40:06,740
does even more peer reviews on the patch

00:40:04,580 --> 00:40:08,330
you know that's how you get a nice

00:40:06,740 --> 00:40:10,760
stable product out fastest you have

00:40:08,330 --> 00:40:12,650
people working together which you know

00:40:10,760 --> 00:40:14,660
it wasn't just Facebook wanting this

00:40:12,650 --> 00:40:16,130
feature there were multiple companies

00:40:14,660 --> 00:40:18,440
that wanted this feature we all wanted

00:40:16,130 --> 00:40:20,840
to work together talk together we didn't

00:40:18,440 --> 00:40:21,830
want to have you know who developed this

00:40:20,840 --> 00:40:25,220
and who gets the glory

00:40:21,830 --> 00:40:29,720
we just wanted something working and

00:40:25,220 --> 00:40:32,870
yeah interesting statistic is when we

00:40:29,720 --> 00:40:34,850
joined we were at ODOT 20 the next Rev

00:40:32,870 --> 00:40:37,840
they bumped it up to was o dot 90 to

00:40:34,850 --> 00:40:42,080
reflect that they had a pen capability

00:40:37,840 --> 00:40:44,150
we had over 1,000 patches that were

00:40:42,080 --> 00:40:48,350
between these two revs so if you've

00:40:44,150 --> 00:40:55,970
tested on with HBase 20 before it's a

00:40:48,350 --> 00:40:59,440
little bit old all right yeah and that's

00:40:55,970 --> 00:40:59,440
it so you guys have any questions

00:40:59,920 --> 00:41:03,920
regarding romantic involved operations

00:41:03,380 --> 00:41:06,680
you have

00:41:03,920 --> 00:41:07,970
large set of data around of your handle

00:41:06,680 --> 00:41:11,630
that uh

00:41:07,970 --> 00:41:13,700
so the question was a lot you have a lot

00:41:11,630 --> 00:41:16,850
of volatile operations I think I was

00:41:13,700 --> 00:41:18,350
talking about the the people creating

00:41:16,850 --> 00:41:20,360
and destroying accounts and you have to

00:41:18,350 --> 00:41:23,330
move migrate you know terabytes worth of

00:41:20,360 --> 00:41:26,660
data how do you do yeah I mean carefully

00:41:23,330 --> 00:41:28,910
right so to be honest my migration was

00:41:26,660 --> 00:41:32,240
the area of this where I was the least

00:41:28,910 --> 00:41:34,310
expertise on a lot of it was doing large

00:41:32,240 --> 00:41:36,410
MapReduce scripts handling a lot of

00:41:34,310 --> 00:41:40,490
failures again taking a lot of

00:41:36,410 --> 00:41:43,610
statistical measurements you know trying

00:41:40,490 --> 00:41:45,110
to measure any little odd error case

00:41:43,610 --> 00:41:46,760
that happened that people normally take

00:41:45,110 --> 00:41:56,210
for granted and they say return false on

00:41:46,760 --> 00:42:03,230
like your network and proper way of you

00:41:56,210 --> 00:42:04,670
structure uh a large pipe yeah yeah I

00:42:03,230 --> 00:42:07,370
mean how do you structure that yeah I

00:42:04,670 --> 00:42:10,190
mean so beyond beyond that beyond like

00:42:07,370 --> 00:42:19,240
how do you programmatically do it yeah

00:42:10,190 --> 00:42:22,150
different questions ah next the wait

00:42:19,240 --> 00:42:25,359
the most reformers and I read some

00:42:22,150 --> 00:42:29,499
reporter Cassandra after all

00:42:25,359 --> 00:42:32,680
yeah so the question was what about

00:42:29,499 --> 00:42:35,589
HBase performance uh sort of what's

00:42:32,680 --> 00:42:37,509
interesting you know performance was

00:42:35,589 --> 00:42:41,140
definitely something that we looked at

00:42:37,509 --> 00:42:43,660
uh it wasn't necessarily an immediate

00:42:41,140 --> 00:42:46,779
top goal the the big goal was is the

00:42:43,660 --> 00:42:48,400
architecture sound theoretically is the

00:42:46,779 --> 00:42:50,940
development stable or the features

00:42:48,400 --> 00:42:54,249
stable do we not have data loss you know

00:42:50,940 --> 00:42:56,859
we sit here and you know in two weeks we

00:42:54,249 --> 00:42:58,989
increase the the write performance of

00:42:56,859 --> 00:43:01,779
one of our use cases by three hundred

00:42:58,989 --> 00:43:03,579
percent right so I mean why do we care

00:43:01,779 --> 00:43:06,309
exactly what the performance is if we

00:43:03,579 --> 00:43:10,979
can triple our speed what we care about

00:43:06,309 --> 00:43:10,979
is the zero data loss so

00:43:25,140 --> 00:43:33,970
yeah shows yeah so the question was

00:43:31,900 --> 00:43:34,930
basically if you guys have heard of the

00:43:33,970 --> 00:43:37,900
cap theorem

00:43:34,930 --> 00:43:41,350
that's a consistency availability and

00:43:37,900 --> 00:43:42,940
partitioning uh and talking about you

00:43:41,350 --> 00:43:46,480
know Cassandra does more of like an

00:43:42,940 --> 00:43:48,640
eventually consistent model so what

00:43:46,480 --> 00:43:50,080
you're sacrificing as a recall is sort

00:43:48,640 --> 00:43:51,970
of the partitioning where you cannot

00:43:50,080 --> 00:43:56,830
have sort of a split brain sort of

00:43:51,970 --> 00:43:59,500
scenario in here I mean that's roughly

00:43:56,830 --> 00:44:01,600
what you're what you're dealing with is

00:43:59,500 --> 00:44:03,130
a for some reason like your networks

00:44:01,600 --> 00:44:06,220
which cuts off and you have two brains

00:44:03,130 --> 00:44:08,470
you can't serve out of both clusters you

00:44:06,220 --> 00:44:09,910
have to serve out of a single one in the

00:44:08,470 --> 00:44:11,200
in reality it's not a big deal because

00:44:09,910 --> 00:44:12,940
like I said you got three replicas

00:44:11,200 --> 00:44:14,890
you're doing the replicas across Iraq

00:44:12,940 --> 00:44:18,040
we've literally you know you can have a

00:44:14,890 --> 00:44:19,780
rack die and go offline and it still

00:44:18,040 --> 00:44:23,040
works great it's still auto rebalances

00:44:19,780 --> 00:44:23,040
for you so

00:44:26,599 --> 00:44:35,250
so the other question is what about

00:44:29,869 --> 00:44:39,750
different data centers so replication is

00:44:35,250 --> 00:44:41,580
support is already working with HBase so

00:44:39,750 --> 00:44:43,260
in addition to having the three replicas

00:44:41,580 --> 00:44:44,970
you can have three replicas among

00:44:43,260 --> 00:44:49,500
different data centers right now it's

00:44:44,970 --> 00:44:55,080
doing sort of a master/slave you know it

00:44:49,500 --> 00:44:56,609
the realistic is the where we're working

00:44:55,080 --> 00:44:59,220
on that that's a high priority for us

00:44:56,609 --> 00:45:01,470
but the realistic is those three

00:44:59,220 --> 00:45:04,859
replicas are great you know you rarely

00:45:01,470 --> 00:45:07,770
get below two replicas is the reality so

00:45:04,859 --> 00:45:10,349
definitely I mean the cross regional

00:45:07,770 --> 00:45:11,640
like having say a data center here in

00:45:10,349 --> 00:45:17,780
Europe it's really for the read

00:45:11,640 --> 00:45:17,780
latencies right any other questions

00:45:18,109 --> 00:45:21,860
you're monitoring what you still your

00:45:20,910 --> 00:45:32,840
methods

00:45:21,860 --> 00:45:35,300
hdfs for what coming out right up so

00:45:32,840 --> 00:45:39,650
basically we have a internal application

00:45:35,300 --> 00:45:41,150
that's very similar to yes we have an

00:45:39,650 --> 00:45:43,760
internal application that's very similar

00:45:41,150 --> 00:45:44,960
to oh sorry repeat the question what do

00:45:43,760 --> 00:45:46,550
you do about metrics where do you store

00:45:44,960 --> 00:45:48,920
the metrics we have an application

00:45:46,550 --> 00:45:52,640
that's very similar to ganglia it's an

00:45:48,920 --> 00:45:56,240
open source project arm that we use for

00:45:52,640 --> 00:45:58,250
storing our metrics in HBase by default

00:45:56,240 --> 00:46:01,820
we'll export your metrics on a variety

00:45:58,250 --> 00:46:07,550
of of areas we chose to export it with

00:46:01,820 --> 00:46:09,020
JMX so we collect JMX data and you know

00:46:07,550 --> 00:46:13,310
show up graphs that way but it's very

00:46:09,020 --> 00:46:15,050
easy again to do on ganglia I believe

00:46:13,310 --> 00:46:30,020
ganglia is what a couple of the major

00:46:15,050 --> 00:46:33,100
companies working with HBase using the

00:46:30,020 --> 00:46:39,710
append a 9-hour D then you have no real

00:46:33,100 --> 00:46:41,690
Yahoo's from data right so the question

00:46:39,710 --> 00:46:43,790
was sort of some technical details about

00:46:41,690 --> 00:46:46,450
ganglia to be honest like I said it's

00:46:43,790 --> 00:46:49,340
something that's very similar to ganglia

00:46:46,450 --> 00:46:52,160
so I can't really it's not my area of

00:46:49,340 --> 00:46:56,870
expertise I just give them to data and

00:46:52,160 --> 00:46:59,540
they worry about it so I know that

00:46:56,870 --> 00:47:01,550
there's you could probably I mean the

00:46:59,540 --> 00:47:04,490
HBase community if you go on pound HBase

00:47:01,550 --> 00:47:06,700
on IRC they're very friendly they're

00:47:04,490 --> 00:47:09,230
very willing to answer any questions uh

00:47:06,700 --> 00:47:10,400
you know there's a lot of open source

00:47:09,230 --> 00:47:17,770
solutions and they can tell you the

00:47:10,400 --> 00:47:17,770
trade-offs for those so other questions

00:47:27,190 --> 00:47:29,250

YouTube URL: https://www.youtube.com/watch?v=cSNGGAKJqwk


