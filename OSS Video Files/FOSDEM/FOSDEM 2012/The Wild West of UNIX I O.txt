Title: The Wild West of UNIX I O
Publication date: 2013-02-05
Playlist: FOSDEM 2012
Description: 
	FOSDEM (Free and Open Source Development European Meeting) is a European event centered around Free and Open Source software development. It is aimed at developers and all interested in the Free and Open Source news in the world. Its goals are to enable developers to meet and to promote the awareness and use of free and open source software. More info at http://fosdem.org
Captions: 
	00:00:03,319 --> 00:00:07,319
hi everyone can you all hear me okay sir

00:00:06,060 --> 00:00:09,780
levels right great

00:00:07,319 --> 00:00:12,179
my name is Anil and I'm a researcher at

00:00:09,780 --> 00:00:14,009
the University of Cambridge where we do

00:00:12,179 --> 00:00:16,369
a whole bunch of fun kind of open source

00:00:14,009 --> 00:00:18,750
hacking and things like we worked in Zen

00:00:16,369 --> 00:00:20,670
when it when it was first a research

00:00:18,750 --> 00:00:22,590
project and more recently been working a

00:00:20,670 --> 00:00:25,230
lot of experimental microkernels and

00:00:22,590 --> 00:00:26,279
ways of doing data processing so the

00:00:25,230 --> 00:00:28,680
work here I'm going to talk about is

00:00:26,279 --> 00:00:30,570
with my very fun colleagues from both

00:00:28,680 --> 00:00:32,790
Nottingham and I'm from Cambridge and

00:00:30,570 --> 00:00:35,460
you can see their names are there and it

00:00:32,790 --> 00:00:38,910
it began as a bunch of experiments in

00:00:35,460 --> 00:00:41,100
building microkernels as I said and it

00:00:38,910 --> 00:00:43,829
turned into something that odd it turns

00:00:41,100 --> 00:00:46,590
out these days at IBC which was normally

00:00:43,829 --> 00:00:48,750
not a really big problem in in UNIX it

00:00:46,590 --> 00:00:50,489
was something that say a Zen geek or an

00:00:48,750 --> 00:00:52,230
elf or gate would worry about and it

00:00:50,489 --> 00:00:53,719
turns out that IPC performance that is

00:00:52,230 --> 00:00:56,129
inter-process communication between

00:00:53,719 --> 00:00:58,410
between processes is turning into a

00:00:56,129 --> 00:01:01,140
really really important factor in in in

00:00:58,410 --> 00:01:03,180
UNIX so honey just give me an idea how

00:01:01,140 --> 00:01:05,850
many people here use virtualization in

00:01:03,180 --> 00:01:07,770
some shape form three how many of you

00:01:05,850 --> 00:01:08,880
have a multi-core machine say with more

00:01:07,770 --> 00:01:10,530
than two two cores

00:01:08,880 --> 00:01:13,260
actually yeah yeah pretty much all of

00:01:10,530 --> 00:01:15,780
you right now and so there's actually

00:01:13,260 --> 00:01:19,130
these reasons why IPC is is moving

00:01:15,780 --> 00:01:21,659
beyond a niche problem it's because our

00:01:19,130 --> 00:01:25,140
privileged layers as we walk up through

00:01:21,659 --> 00:01:26,340
the software stack are increasing so

00:01:25,140 --> 00:01:27,390
whereas previously you'd be running

00:01:26,340 --> 00:01:29,369
directly in the bare metal

00:01:27,390 --> 00:01:31,590
these days it's pretty common to have

00:01:29,369 --> 00:01:33,540
some kind of a layer between you and and

00:01:31,590 --> 00:01:35,159
and the hardware and this is in the form

00:01:33,540 --> 00:01:37,500
of you know a low level hypervisor like

00:01:35,159 --> 00:01:39,720
Zen or VMware hosted hypervisors like

00:01:37,500 --> 00:01:42,259
ADM or VirtualBox which are in many ways

00:01:39,720 --> 00:01:45,420
much easier to use and even increasingly

00:01:42,259 --> 00:01:47,430
high level language runtimes also impose

00:01:45,420 --> 00:01:49,920
privileged boundaries as a result of the

00:01:47,430 --> 00:01:52,829
desire for more security at the same

00:01:49,920 --> 00:01:55,049
time this vertical increase in layers is

00:01:52,829 --> 00:01:57,600
being also being followed by by this

00:01:55,049 --> 00:01:59,939
whole trend towards Numa architectures

00:01:57,600 --> 00:02:01,890
so if if you worked and if you're of a

00:01:59,939 --> 00:02:03,450
certain age and you worked on say SGI's

00:02:01,890 --> 00:02:05,430
in the 90s

00:02:03,450 --> 00:02:07,799
you remember Numa was very relevant and

00:02:05,430 --> 00:02:09,989
and a big feature in in the big iron

00:02:07,799 --> 00:02:11,250
operating systems of that time these

00:02:09,989 --> 00:02:14,050
days we're heading in exactly the same

00:02:11,250 --> 00:02:16,240
direction so in just the last five years

00:02:14,050 --> 00:02:19,180
gone from hyper-threading is a quick

00:02:16,240 --> 00:02:21,430
hack by Intel to to do let us exploit a

00:02:19,180 --> 00:02:22,840
bit of bit of parallelism and now it's

00:02:21,430 --> 00:02:24,130
very common to have Neumann machines and

00:02:22,840 --> 00:02:26,290
multi-core machines so I'll actually

00:02:24,130 --> 00:02:28,360
talk about some of the some of the

00:02:26,290 --> 00:02:31,870
experiments we did did run this just to

00:02:28,360 --> 00:02:33,580
see how bad the impact is so just how

00:02:31,870 --> 00:02:34,600
much do we have to worry about this so I

00:02:33,580 --> 00:02:36,040
thought I'd give you a quick sneak

00:02:34,600 --> 00:02:38,230
preview some day from some of the

00:02:36,040 --> 00:02:40,420
performance results so in many cases

00:02:38,230 --> 00:02:43,510
we're observing performance improvements

00:02:40,420 --> 00:02:46,240
if you suddenly become Numa and and and

00:02:43,510 --> 00:02:48,280
locality aware or say six times an AMD

00:02:46,240 --> 00:02:50,680
and this is from going from a TCP

00:02:48,280 --> 00:02:53,020
connection to to an optimized share

00:02:50,680 --> 00:02:55,270
memory transport and similarly on Intel

00:02:53,020 --> 00:02:57,520
on the newer i7 it's it's it's often

00:02:55,270 --> 00:03:01,210
even worse so in this case going for a

00:02:57,520 --> 00:03:02,680
TCP no delay algorithm to a shared

00:03:01,210 --> 00:03:05,800
memory transport that I'll describe

00:03:02,680 --> 00:03:08,020
during my talk resulted in just hundreds

00:03:05,800 --> 00:03:10,240
of gigabits of throughput versus versus

00:03:08,020 --> 00:03:11,980
tens of gigabits now again why should

00:03:10,240 --> 00:03:14,740
you worry if you're say using the cloud

00:03:11,980 --> 00:03:16,600
then increasingly memory to memory

00:03:14,740 --> 00:03:18,610
communication is the way that you

00:03:16,600 --> 00:03:20,140
communicate with virtual machines so if

00:03:18,610 --> 00:03:22,000
you're an Amazon ec2 and you have a web

00:03:20,140 --> 00:03:23,709
server and a database server then you

00:03:22,000 --> 00:03:24,970
will of course be able to sending a lot

00:03:23,709 --> 00:03:26,709
of data between them and this will

00:03:24,970 --> 00:03:28,239
eventually take the form of IPC just

00:03:26,709 --> 00:03:30,100
memory to memory communication between

00:03:28,239 --> 00:03:32,470
between hosts and actually hitting

00:03:30,100 --> 00:03:35,040
hitting a NIC is is relatively rare and

00:03:32,470 --> 00:03:37,720
these kind of big big iron deployments

00:03:35,040 --> 00:03:39,220
so for my talk I'm just gonna break it

00:03:37,720 --> 00:03:40,870
up to three bits first of all white

00:03:39,220 --> 00:03:42,370
matters and I'm just going to show you

00:03:40,870 --> 00:03:44,470
some fun benchmarks that we've been

00:03:42,370 --> 00:03:46,090
running and then I'll talk about what

00:03:44,470 --> 00:03:47,860
some of the fixes were trying are but of

00:03:46,090 --> 00:03:50,260
course that's just one of the possible

00:03:47,860 --> 00:03:51,760
attempts and also since this is falls

00:03:50,260 --> 00:03:53,950
down the reason I'm here really is to

00:03:51,760 --> 00:03:55,180
try to rally some help for an experiment

00:03:53,950 --> 00:03:57,010
in data gathering that we're trying

00:03:55,180 --> 00:03:58,750
because of obviously we're just a very

00:03:57,010 --> 00:04:01,570
small research group so we need we need

00:03:58,750 --> 00:04:04,750
some help from you guys so let's try an

00:04:01,570 --> 00:04:06,910
experiment so we have a 48 core box we

00:04:04,750 --> 00:04:08,590
bought it for about I know 7,000 pounds

00:04:06,910 --> 00:04:10,570
or something it's got 48 cores

00:04:08,590 --> 00:04:13,660
it's a it's it's quite a lot of course

00:04:10,570 --> 00:04:15,280
is the 64 gigabytes of RAM and you know

00:04:13,660 --> 00:04:18,220
it's it's running at AMD magny-cours

00:04:15,280 --> 00:04:19,419
processor running Linux 3.1 on exit 64

00:04:18,220 --> 00:04:22,300
and an unmodified

00:04:19,419 --> 00:04:24,550
Xen also in 6 for grid mode and all of

00:04:22,300 --> 00:04:27,880
the tests I'm gonna describe are running

00:04:24,550 --> 00:04:29,560
in in in RAM so very straightforward

00:04:27,880 --> 00:04:31,450
and so what are the experiments were

00:04:29,560 --> 00:04:33,730
running so we thought well in order to

00:04:31,450 --> 00:04:35,890
figure out are all these cores really

00:04:33,730 --> 00:04:37,600
made equal what do we just run some

00:04:35,890 --> 00:04:39,550
latency tests between between all of

00:04:37,600 --> 00:04:41,650
these cores so by running these latency

00:04:39,550 --> 00:04:44,350
tests it's very straightforward you just

00:04:41,650 --> 00:04:47,050
you just run the full combination

00:04:44,350 --> 00:04:48,310
between all CPUs but only one at a time

00:04:47,050 --> 00:04:51,880
so you might measure the performance

00:04:48,310 --> 00:04:53,200
core 1 and core 6 you pin the processes

00:04:51,880 --> 00:04:54,790
so they're only running a core one in

00:04:53,200 --> 00:04:56,230
core 6 there's pretty much nothing else

00:04:54,790 --> 00:04:59,140
running in the machine everything that

00:04:56,230 --> 00:05:01,840
can be killed is being killed and we

00:04:59,140 --> 00:05:05,800
then plot the grid of combinations so we

00:05:01,840 --> 00:05:07,900
plot x and y of core 1 2 core 6 and etc

00:05:05,800 --> 00:05:10,210
etc so it's a pretty straightforward way

00:05:07,900 --> 00:05:12,520
of just exploring your your-your-your

00:05:10,210 --> 00:05:14,410
system and so in terms of what are we

00:05:12,520 --> 00:05:15,610
measuring the latency of well there's a

00:05:14,410 --> 00:05:18,730
whole bunch of mechanisms I'll describe

00:05:15,610 --> 00:05:20,380
so things like TCP UDP some shared

00:05:18,730 --> 00:05:24,310
memory transfers between them and also

00:05:20,380 --> 00:05:26,590
things like pipes so why don't we take a

00:05:24,310 --> 00:05:30,250
look at the first result so this is a

00:05:26,590 --> 00:05:33,790
described is our 48 core machine and the

00:05:30,250 --> 00:05:35,290
x-axis marks the the source core and the

00:05:33,790 --> 00:05:37,120
yd marks the target cord that you're

00:05:35,290 --> 00:05:39,550
going to and the grid each each of these

00:05:37,120 --> 00:05:40,870
little pixels represents a single test

00:05:39,550 --> 00:05:43,510
so there are a lot of tests run for this

00:05:40,870 --> 00:05:45,580
this particular result so this is gonna

00:05:43,510 --> 00:05:47,320
make sense to anyone the can you start

00:05:45,580 --> 00:05:49,060
you can sort of see patterns in this

00:05:47,320 --> 00:05:50,890
right so what are some of the patterns

00:05:49,060 --> 00:05:53,650
that we can see in this in this what

00:05:50,890 --> 00:05:55,390
does it tell us so first of all blue is

00:05:53,650 --> 00:05:58,030
the lowest latency so this is actually

00:05:55,390 --> 00:05:59,920
measuring latency on the bit for each

00:05:58,030 --> 00:06:01,720
individual message so you can see here

00:05:59,920 --> 00:06:03,670
that if you're on exactly the same core

00:06:01,720 --> 00:06:05,620
then you're actually not communicating

00:06:03,670 --> 00:06:06,850
across cores so we've actually got

00:06:05,620 --> 00:06:09,880
pretty good latency for TCP it's the

00:06:06,850 --> 00:06:11,530
lowest most efficient way to to to to

00:06:09,880 --> 00:06:13,380
talk some miss obviously to talk to

00:06:11,530 --> 00:06:16,270
yourself then what are all these other

00:06:13,380 --> 00:06:20,770
cross hatch patterns why why why are

00:06:16,270 --> 00:06:22,150
they in such a distinct order well to

00:06:20,770 --> 00:06:23,320
answer this question you need start

00:06:22,150 --> 00:06:25,210
looking at the hardware architecture

00:06:23,320 --> 00:06:27,460
that the machine is running so this

00:06:25,210 --> 00:06:30,610
seemingly flat layout of course is

00:06:27,460 --> 00:06:32,500
actually interconnected by bicycle hyper

00:06:30,610 --> 00:06:33,850
transport and it's actually a very

00:06:32,500 --> 00:06:36,340
hierarchical mechanism right it's all

00:06:33,850 --> 00:06:38,890
it's all laid out MD does not make a 48

00:06:36,340 --> 00:06:40,330
core CPU they make six core CPUs and

00:06:38,890 --> 00:06:41,680
they packed them into what's called a

00:06:40,330 --> 00:06:44,350
multiprocessor module

00:06:41,680 --> 00:06:46,509
then those six cores are packed into a

00:06:44,350 --> 00:06:49,539
socket which is actually about 12 cores

00:06:46,509 --> 00:06:51,789
in size and then do you have these inter

00:06:49,539 --> 00:06:54,039
socket transports that actually decide

00:06:51,789 --> 00:06:56,020
how to how do i delay our connections

00:06:54,039 --> 00:06:57,490
but between each of these things he

00:06:56,020 --> 00:06:59,080
there later than kind of a grid pattern

00:06:57,490 --> 00:07:00,729
so you can see that this NPM is

00:06:59,080 --> 00:07:02,740
connected directly to this one and it

00:07:00,729 --> 00:07:04,840
goes through a longer interconnect to go

00:07:02,740 --> 00:07:06,310
to two other places and this is just one

00:07:04,840 --> 00:07:08,860
machine right so this is required

00:07:06,310 --> 00:07:11,169
digging into the usual reading on intend

00:07:08,860 --> 00:07:12,610
and some of the AMD hardware manuals

00:07:11,169 --> 00:07:15,910
it's just trying to figure out exactly

00:07:12,610 --> 00:07:18,039
what it looks like under the hood so so

00:07:15,910 --> 00:07:19,990
given this this actually makes sense

00:07:18,039 --> 00:07:22,330
right so we can see the fact that when

00:07:19,990 --> 00:07:23,979
you're when you're if you're they're

00:07:22,330 --> 00:07:25,570
going to your nearest socket or if

00:07:23,979 --> 00:07:27,610
you're going to a further socket there's

00:07:25,570 --> 00:07:31,150
a very distinct gradiation between

00:07:27,610 --> 00:07:32,889
between between the various

00:07:31,150 --> 00:07:35,349
communication mechanisms so great this

00:07:32,889 --> 00:07:36,669
is totally obvious so what happens when

00:07:35,349 --> 00:07:38,530
we start adding some of these software

00:07:36,669 --> 00:07:41,590
and errors that I described so let's

00:07:38,530 --> 00:07:43,960
take the exact same test and let's

00:07:41,590 --> 00:07:47,020
virtualize it so this just means we

00:07:43,960 --> 00:07:49,449
boots then and the domain 0 which is the

00:07:47,020 --> 00:07:54,820
the controlling domain is now running

00:07:49,449 --> 00:07:56,550
48v cpus and this should be should be no

00:07:54,820 --> 00:07:58,419
different right because you're just

00:07:56,550 --> 00:08:01,979
you're not really running any further

00:07:58,419 --> 00:08:03,970
virtual machines so what happens well

00:08:01,979 --> 00:08:07,060
things do start getting a bit more

00:08:03,970 --> 00:08:09,400
random so can anyone here explain why

00:08:07,060 --> 00:08:11,080
this might happen and I'll just put both

00:08:09,400 --> 00:08:12,370
of these these graphs up here just so

00:08:11,080 --> 00:08:14,860
you can see what's going on this is the

00:08:12,370 --> 00:08:16,289
same this is just a 48v CPU Zen and this

00:08:14,860 --> 00:08:20,130
is running a native

00:08:16,289 --> 00:08:20,130
any ideas govern

00:08:25,570 --> 00:08:34,370
right right precisely so just to repeat

00:08:32,720 --> 00:08:35,930
the answer he says that did weep in the

00:08:34,370 --> 00:08:38,390
VC views and so the answer is no we've

00:08:35,930 --> 00:08:40,220
just set up a domain now the expectation

00:08:38,390 --> 00:08:42,860
is that if you have a just one domain

00:08:40,220 --> 00:08:45,560
running your your scheduler is probably

00:08:42,860 --> 00:08:47,420
not rebalancing the CPUs but what's

00:08:45,560 --> 00:08:48,500
actually happening is that even even

00:08:47,420 --> 00:08:52,010
though you just have one domain on the

00:08:48,500 --> 00:08:54,350
box Zen is in fact remapping the the

00:08:52,010 --> 00:08:56,150
CPUs dynamically so this is effectively

00:08:54,350 --> 00:08:57,380
just noise we have no idea what the

00:08:56,150 --> 00:09:00,980
underlying topology of the hardware is

00:08:57,380 --> 00:09:02,270
at this point and the absolute latency

00:09:00,980 --> 00:09:03,500
is interested you're double but it's not

00:09:02,270 --> 00:09:05,330
really absolute Lyndsey's we're worried

00:09:03,500 --> 00:09:08,690
about it is as you say just what is the

00:09:05,330 --> 00:09:10,790
effective of rescheduling on the on this

00:09:08,690 --> 00:09:13,070
domain so this is of course an advantage

00:09:10,790 --> 00:09:15,080
you want Zen to figure out and rebalance

00:09:13,070 --> 00:09:16,370
your CPUs but sometimes you don't

00:09:15,080 --> 00:09:18,560
sometimes you just want to have control

00:09:16,370 --> 00:09:20,420
over what's going on and especially when

00:09:18,560 --> 00:09:21,890
you have just a single domain it just

00:09:20,420 --> 00:09:26,600
adds another layer of scheduling and

00:09:21,890 --> 00:09:30,080
multiplexing on to the hood so what

00:09:26,600 --> 00:09:34,010
happens when we try to pin the domains

00:09:30,080 --> 00:09:36,350
so you can take a hypervisor you can run

00:09:34,010 --> 00:09:38,990
in the case of Zen you can run Excel CPU

00:09:36,350 --> 00:09:42,530
pin and so we pinned zero two zero one

00:09:38,990 --> 00:09:45,080
two one two two two and you have a PCP

00:09:42,530 --> 00:09:46,550
UID and a vcp ID so in this situation

00:09:45,080 --> 00:09:48,950
there you can see that we can actually

00:09:46,550 --> 00:09:51,680
start to see a topology underneath them

00:09:48,950 --> 00:09:53,240
but it's not the same as the one that we

00:09:51,680 --> 00:09:55,070
had last time so this is the pinned

00:09:53,240 --> 00:09:57,560
version this is the unpinned so it's

00:09:55,070 --> 00:09:59,660
clearly more predictable but if you

00:09:57,560 --> 00:10:02,360
compare it to the native version it's

00:09:59,660 --> 00:10:04,370
just a little bit different right and so

00:10:02,360 --> 00:10:07,160
again the reason for this is that it

00:10:04,370 --> 00:10:09,290
turns out that when you design an

00:10:07,160 --> 00:10:12,230
enumeration mechanism for CPU numbers is

00:10:09,290 --> 00:10:14,120
different from the native one so it's

00:10:12,230 --> 00:10:15,680
consistent there's always a model of

00:10:14,120 --> 00:10:17,000
what that is but it's just different

00:10:15,680 --> 00:10:19,460
from the Linux enumeration because of

00:10:17,000 --> 00:10:22,790
various details of a CPI and so on

00:10:19,460 --> 00:10:23,900
and so the problem the other problem

00:10:22,790 --> 00:10:25,550
with this is that it requires root

00:10:23,900 --> 00:10:28,040
access to your cloud right your cluster

00:10:25,550 --> 00:10:29,690
so the only people with access to this

00:10:28,040 --> 00:10:31,640
sort of level of the of the system

00:10:29,690 --> 00:10:33,530
administration hierarchy are people who

00:10:31,640 --> 00:10:35,420
who control the clusters if you are just

00:10:33,530 --> 00:10:36,950
running a high-performance job on Amazon

00:10:35,420 --> 00:10:38,450
you wouldn't be able to do this pinning

00:10:36,950 --> 00:10:40,280
so again the problem with these

00:10:38,450 --> 00:10:41,690
is that there's you add more social

00:10:40,280 --> 00:10:44,660
layers as well than people who can and

00:10:41,690 --> 00:10:47,390
cannot control this stuff with with

00:10:44,660 --> 00:10:49,100
other people so the problem is that we

00:10:47,390 --> 00:10:50,870
can almost predict it but you really

00:10:49,100 --> 00:10:52,700
have to run the experiment to figure out

00:10:50,870 --> 00:10:54,200
exactly what its gonna look like and of

00:10:52,700 --> 00:10:56,510
course this isn't just in one one

00:10:54,200 --> 00:10:59,030
machine right so what do we try

00:10:56,510 --> 00:11:01,460
something else what are we what are the

00:10:59,030 --> 00:11:03,380
effects of Numa so we instead of using

00:11:01,460 --> 00:11:05,420
TCP just pick a simpler transport

00:11:03,380 --> 00:11:06,920
mechanism and in this case we picked our

00:11:05,420 --> 00:11:09,020
UNIX domain sockets which should be

00:11:06,920 --> 00:11:11,300
really just memory to memory copies with

00:11:09,020 --> 00:11:13,100
very little CPU usage and I'll explain

00:11:11,300 --> 00:11:14,600
some of these differences later on now

00:11:13,100 --> 00:11:16,490
you can see here that this is less clear

00:11:14,600 --> 00:11:18,170
than TCP but there's still an

00:11:16,490 --> 00:11:20,960
interesting effect where if you're on

00:11:18,170 --> 00:11:22,670
the same local socket you can see it's

00:11:20,960 --> 00:11:24,890
much lower latency and it's generally

00:11:22,670 --> 00:11:26,810
just more latent outside of your your

00:11:24,890 --> 00:11:29,030
local socket but it's still very clear

00:11:26,810 --> 00:11:32,240
there's this pattern of staying within

00:11:29,030 --> 00:11:34,030
your socket is a good idea so what

00:11:32,240 --> 00:11:36,980
happens when we run this one under Zen

00:11:34,030 --> 00:11:38,780
well you get an entirely different graph

00:11:36,980 --> 00:11:41,450
right and it's not random there's

00:11:38,780 --> 00:11:43,550
clearly this trend and tracking this one

00:11:41,450 --> 00:11:45,100
down took a little while so who who

00:11:43,550 --> 00:11:47,210
reckons they know the answer to this one

00:11:45,100 --> 00:11:48,500
so far I'm buying you a beer cuz you

00:11:47,210 --> 00:11:50,690
guessed the depending if someone else

00:11:48,500 --> 00:11:54,790
can guess this you get the free the free

00:11:50,690 --> 00:11:54,790
beer anyone any ideas why Oh

00:11:57,590 --> 00:12:00,320
all right Richard you get the second

00:11:59,060 --> 00:12:01,280
beer although you don't card you work

00:12:00,320 --> 00:12:04,340
for Red Hat that doesn't that doesn't

00:12:01,280 --> 00:12:05,510
work so so so what's actually happening

00:12:04,340 --> 00:12:07,490
here is that whenever you allocate

00:12:05,510 --> 00:12:09,050
memory memory is also in a hierarchy

00:12:07,490 --> 00:12:11,660
it's it's attached to what's called the

00:12:09,050 --> 00:12:13,700
Numa domaine and so the neumann domain

00:12:11,660 --> 00:12:15,260
basically decides when you run malloc it

00:12:13,700 --> 00:12:17,180
decides where you allocate the memory

00:12:15,260 --> 00:12:19,010
from and usually that's based on what

00:12:17,180 --> 00:12:20,600
CPU you're associated with what's called

00:12:19,010 --> 00:12:24,350
what core are you running on so in this

00:12:20,600 --> 00:12:26,600
case we are we should be allocating it

00:12:24,350 --> 00:12:28,520
on our local socket and in fact in the

00:12:26,600 --> 00:12:30,980
case of Native it is but the problem is

00:12:28,520 --> 00:12:32,810
at Zen actually and in fact many

00:12:30,980 --> 00:12:33,920
hypervisors although one of the reasons

00:12:32,810 --> 00:12:36,230
for come here is try to figure out what

00:12:33,920 --> 00:12:37,370
other hypervisors do apart from digging

00:12:36,230 --> 00:12:39,470
through lots of lots of source code is

00:12:37,370 --> 00:12:40,580
that they don't actually expose the Numa

00:12:39,470 --> 00:12:42,860
layout to the guest

00:12:40,580 --> 00:12:45,770
so it actually just tells it to all this

00:12:42,860 --> 00:12:46,880
allocate from memory into main 0 so in

00:12:45,770 --> 00:12:49,220
fact whenever you're allocating these

00:12:46,880 --> 00:12:51,920
buffers the further we get from domain 0

00:12:49,220 --> 00:12:53,839
the the the more latent it gets

00:12:51,920 --> 00:12:55,489
so again it's kind of not obvious but

00:12:53,839 --> 00:12:57,019
it's it's it's startling to see just how

00:12:55,489 --> 00:12:58,639
much of a difference it makes all we're

00:12:57,019 --> 00:12:59,809
doing is just in our hardware just

00:12:58,639 --> 00:13:05,269
picking a different part of the topology

00:12:59,809 --> 00:13:06,739
to to work from so so we've we've done a

00:13:05,269 --> 00:13:08,419
lot of digging right and we've gone

00:13:06,739 --> 00:13:10,819
through and this this is just a small

00:13:08,419 --> 00:13:11,959
subset of the experiments and I'll show

00:13:10,819 --> 00:13:13,609
you a website where we have lots of lots

00:13:11,959 --> 00:13:15,619
more experiments and there's something

00:13:13,609 --> 00:13:17,179
just cannot explain so I will definitely

00:13:15,619 --> 00:13:19,730
buy some of the beer if you can explain

00:13:17,179 --> 00:13:22,220
and only in the case of domain sockets

00:13:19,730 --> 00:13:23,749
and not in the case of anything else if

00:13:22,220 --> 00:13:27,679
you're in the same core domain tokens

00:13:23,749 --> 00:13:30,109
are more latent then then then if you're

00:13:27,679 --> 00:13:31,970
running a native but in the case of TCP

00:13:30,109 --> 00:13:33,499
in the case of other transports this is

00:13:31,970 --> 00:13:35,959
not the case it only happens in some

00:13:33,499 --> 00:13:38,509
versions of them so some things are

00:13:35,959 --> 00:13:39,949
ongoing research well we uh we still try

00:13:38,509 --> 00:13:41,720
to dig what's going on but it's very

00:13:39,949 --> 00:13:43,369
very very clear that there's a lot of

00:13:41,720 --> 00:13:44,749
layers of software and play here right

00:13:43,369 --> 00:13:45,889
and it things are only getting worse

00:13:44,749 --> 00:13:49,249
there's just more and more stuff

00:13:45,889 --> 00:13:50,509
happening so the easiest ways all see

00:13:49,249 --> 00:13:52,669
it's trying a bit of science is trying

00:13:50,509 --> 00:13:55,040
to establish the absolute lowest bound

00:13:52,669 --> 00:13:56,899
what is the simplest possible mechanism

00:13:55,040 --> 00:13:58,819
by which we can discover a property of

00:13:56,899 --> 00:14:00,230
our computer and so this is actually

00:13:58,819 --> 00:14:02,209
something called inter-process

00:14:00,230 --> 00:14:04,999
interrupts so whenever you have these 48

00:14:02,209 --> 00:14:07,669
cores they can send an interrupt to the

00:14:04,999 --> 00:14:08,809
other core and this interrupts will wake

00:14:07,669 --> 00:14:10,579
up the other core and tell it that

00:14:08,809 --> 00:14:12,739
something is happening and it then goes

00:14:10,579 --> 00:14:13,999
off and and looks at its various IO

00:14:12,739 --> 00:14:17,230
channels to figure out what's going on

00:14:13,999 --> 00:14:17,230
so we owe

00:14:45,440 --> 00:14:50,550
so so the the hypothesis here is that

00:14:48,290 --> 00:14:52,110
the memory the traffic is going through

00:14:50,550 --> 00:14:53,580
domain 0 and it's it's been looked at

00:14:52,110 --> 00:14:55,380
twice it's not quite true because

00:14:53,580 --> 00:14:57,690
memories directly mapped in the domain

00:14:55,380 --> 00:15:00,090
so it's coming from the Numa node the

00:14:57,690 --> 00:15:01,860
Dom 0 is in but also unfortunately the

00:15:00,090 --> 00:15:04,440
memory is actually allocated for all of

00:15:01,860 --> 00:15:06,750
the socket backends from the same memory

00:15:04,440 --> 00:15:09,560
node so TCP it should be the same as

00:15:06,750 --> 00:15:11,490
UNIX domain sockets in in this case but

00:15:09,560 --> 00:15:12,660
let's talk about this afterwards because

00:15:11,490 --> 00:15:14,100
I didn't ask the audience for that

00:15:12,660 --> 00:15:15,120
because we've sped hours talking about

00:15:14,100 --> 00:15:18,060
this one right and try to track it down

00:15:15,120 --> 00:15:19,440
so but it's Jeffie there's something in

00:15:18,060 --> 00:15:20,720
there we're not quite sure what and

00:15:19,440 --> 00:15:23,400
we're working our way through it

00:15:20,720 --> 00:15:26,370
but getting back to getting back to IP

00:15:23,400 --> 00:15:27,810
eyes what we did was we modified Xen so

00:15:26,370 --> 00:15:30,300
there are no domains running at all so

00:15:27,810 --> 00:15:32,310
it's started day if the hypervisor comes

00:15:30,300 --> 00:15:34,830
up and it's just configuring its CPUs

00:15:32,310 --> 00:15:36,450
and then it starts playing a little game

00:15:34,830 --> 00:15:39,180
where it ping pongs between CPUs and

00:15:36,450 --> 00:15:41,160
this graph just shows us in nanoseconds

00:15:39,180 --> 00:15:42,510
so it is really very low level you can

00:15:41,160 --> 00:15:44,940
very clearly see the microarchitecture

00:15:42,510 --> 00:15:47,040
of the underlying domain so having this

00:15:44,940 --> 00:15:48,270
kind of self test just a few layers of

00:15:47,040 --> 00:15:50,130
your software up is actually quite

00:15:48,270 --> 00:15:52,440
useful just for getting hard data into

00:15:50,130 --> 00:15:54,330
what's going on now Citrix very kindly

00:15:52,440 --> 00:15:57,030
lent us this beast it's called Hydra

00:15:54,330 --> 00:16:00,420
monster right and Hydra monster is 160

00:15:57,030 --> 00:16:02,430
core machine Westmere exp system very

00:16:00,420 --> 00:16:04,440
expensive very nice and also completely

00:16:02,430 --> 00:16:06,360
different and so if you dig through the

00:16:04,440 --> 00:16:07,410
intel architecture manuals it turns out

00:16:06,360 --> 00:16:10,080
that the West mayor has a bi-directional

00:16:07,410 --> 00:16:11,820
ring between CPUs and this actually

00:16:10,080 --> 00:16:14,730
reflects on the fact that when you're

00:16:11,820 --> 00:16:17,610
talking to a different socket on one of

00:16:14,730 --> 00:16:18,780
these machines then the individual core

00:16:17,610 --> 00:16:20,730
you're addressing within the socket

00:16:18,780 --> 00:16:21,900
makes a big difference as to what's

00:16:20,730 --> 00:16:24,210
going on so it does matter if

00:16:21,900 --> 00:16:25,950
something's near or far although it that

00:16:24,210 --> 00:16:27,270
matters to it also matters who you talk

00:16:25,950 --> 00:16:30,300
to the other side because he's going

00:16:27,270 --> 00:16:31,860
over this ring structure so the the the

00:16:30,300 --> 00:16:33,210
point I'm trying to make is that it's

00:16:31,860 --> 00:16:34,860
just really hard to know what's going on

00:16:33,210 --> 00:16:36,600
right these are just two machines are

00:16:34,860 --> 00:16:38,570
described two bits of software and

00:16:36,600 --> 00:16:42,120
there's lots and lots and lots of layers

00:16:38,570 --> 00:16:44,190
so what on earth can we do to try to fix

00:16:42,120 --> 00:16:46,890
this problem we we know that it's

00:16:44,190 --> 00:16:47,790
probably quite quite varied we know that

00:16:46,890 --> 00:16:48,420
we're going to have more layers of

00:16:47,790 --> 00:16:50,160
virtualization

00:16:48,420 --> 00:16:51,510
if we deploy the internal things for

00:16:50,160 --> 00:16:52,980
example the inter of threads my values

00:16:51,510 --> 00:16:54,390
from the last talk we're clearly going

00:16:52,980 --> 00:16:55,770
to have a lot of a lot of variance in

00:16:54,390 --> 00:16:57,720
this kind of stuff and we'd like to

00:16:55,770 --> 00:16:58,380
maintain your gigabit and low latency

00:16:57,720 --> 00:17:01,800
perform

00:16:58,380 --> 00:17:03,630
right so obviously being researchers we

00:17:01,800 --> 00:17:04,949
thought let's have a challenge and let's

00:17:03,630 --> 00:17:05,430
just go hack up a benchmark over the

00:17:04,949 --> 00:17:07,110
weekend

00:17:05,430 --> 00:17:10,319
just a few grids how hard could it be

00:17:07,110 --> 00:17:12,990
and it turns out that yeah the Devils in

00:17:10,319 --> 00:17:14,760
the detail when you look at UNIX IO so

00:17:12,990 --> 00:17:16,980
the first part of designing one of these

00:17:14,760 --> 00:17:18,540
benchmark Suites is pretty obvious you

00:17:16,980 --> 00:17:20,850
have to take a variety of transports

00:17:18,540 --> 00:17:22,470
just different ones and hopefully some

00:17:20,850 --> 00:17:24,690
of the common ones that we use everyday

00:17:22,470 --> 00:17:26,250
in our software and some slightly less

00:17:24,690 --> 00:17:28,830
common ones that I'll talk about shortly

00:17:26,250 --> 00:17:30,660
so the four four ones are pipes domain

00:17:28,830 --> 00:17:32,520
sockets TCP and I'll talk about shared

00:17:30,660 --> 00:17:33,960
memory so what I'm gonna do now is just

00:17:32,520 --> 00:17:35,370
briefly describe each of these things

00:17:33,960 --> 00:17:37,560
and hopefully give you an idea of what

00:17:35,370 --> 00:17:39,540
the costs and the drawbacks are between

00:17:37,560 --> 00:17:41,910
each of these transfer mechanisms so

00:17:39,540 --> 00:17:43,320
firstly I familiar okay who here knows

00:17:41,910 --> 00:17:46,890
the difference between a pipe and a

00:17:43,320 --> 00:17:47,970
domain socket put your hand up right so

00:17:46,890 --> 00:17:49,740
we I think we're talking about 20

00:17:47,970 --> 00:17:51,000
percent the audience so in a in a

00:17:49,740 --> 00:17:52,920
strawpoll of the University of Cambridge

00:17:51,000 --> 00:17:54,270
computer laboratory I think maybe two

00:17:52,920 --> 00:17:55,740
people knew the difference and only one

00:17:54,270 --> 00:17:57,750
was sure and here's the guy Robert

00:17:55,740 --> 00:18:00,120
Watson who wrote the the FreeBSD code

00:17:57,750 --> 00:18:01,530
for for doing this so it it really is

00:18:00,120 --> 00:18:03,450
not obvious between all of these very

00:18:01,530 --> 00:18:04,830
very old-school politics things exactly

00:18:03,450 --> 00:18:06,150
how they work under hood and it turns

00:18:04,830 --> 00:18:07,500
out Linux and FreeBSD are actually

00:18:06,150 --> 00:18:10,110
completely different anyway just because

00:18:07,500 --> 00:18:11,700
of the the nature so the fundamental

00:18:10,110 --> 00:18:13,860
decisions that we make with transport

00:18:11,700 --> 00:18:15,450
mechanisms in UNIX errata quite

00:18:13,860 --> 00:18:16,770
straightforward we have a privilege

00:18:15,450 --> 00:18:18,570
boundary between user space in the

00:18:16,770 --> 00:18:20,970
kernel and doing a system call is

00:18:18,570 --> 00:18:22,560
normally quite expensive doing a system

00:18:20,970 --> 00:18:24,390
called when you're virtualized is really

00:18:22,560 --> 00:18:25,560
expensive unless you're in hardware

00:18:24,390 --> 00:18:27,960
virtualization where it's less expensive

00:18:25,560 --> 00:18:30,450
so it's just a bit different right but

00:18:27,960 --> 00:18:32,220
this varies quite a lot and also you're

00:18:30,450 --> 00:18:34,260
in the in a different address space when

00:18:32,220 --> 00:18:36,030
you're when you're in user space and so

00:18:34,260 --> 00:18:37,770
moving data between these two can be

00:18:36,030 --> 00:18:39,120
quite expensive but in the kernel and

00:18:37,770 --> 00:18:41,130
actually pretty cheap right you just

00:18:39,120 --> 00:18:44,310
transfer the ownership of this memory to

00:18:41,130 --> 00:18:47,640
to another process structure so with

00:18:44,310 --> 00:18:48,870
this in mind how does a pipe work so

00:18:47,640 --> 00:18:50,490
when you call the pipe system call in

00:18:48,870 --> 00:18:52,050
UNIX what's happening is that you have

00:18:50,490 --> 00:18:54,330
say two applications and they want to

00:18:52,050 --> 00:18:56,310
communicate so this application is in

00:18:54,330 --> 00:18:59,250
user space it writes something into its

00:18:56,310 --> 00:19:01,200
buffer then it calls a familiar write

00:18:59,250 --> 00:19:03,870
system call which copies it into the

00:19:01,200 --> 00:19:05,760
kernel so in the kernel this is then

00:19:03,870 --> 00:19:08,100
transferred onto the receive queue of

00:19:05,760 --> 00:19:09,300
the or the other target process so

00:19:08,100 --> 00:19:10,620
there's no copying going on here it's

00:19:09,300 --> 00:19:12,269
just a bit of housekeeping by the kernel

00:19:10,620 --> 00:19:14,549
then at this point the car

00:19:12,269 --> 00:19:16,709
you call the read system call in UNIX is

00:19:14,549 --> 00:19:18,570
normal and it's copied back into the

00:19:16,709 --> 00:19:20,219
into the buffer so it's a pretty

00:19:18,570 --> 00:19:21,779
straightforward kind of data flow here

00:19:20,219 --> 00:19:23,820
that we can understand now of course

00:19:21,779 --> 00:19:25,469
some of these transitions have variable

00:19:23,820 --> 00:19:28,409
performance so if you're going from user

00:19:25,469 --> 00:19:30,179
space to kernel anzen in pv 64-bit mode

00:19:28,409 --> 00:19:31,799
it's very very expensive because the

00:19:30,179 --> 00:19:34,169
various technical restrictions of the

00:19:31,799 --> 00:19:35,339
way you can power virtualize so we're

00:19:34,169 --> 00:19:37,469
not quite sure what the cost of these

00:19:35,339 --> 00:19:40,109
are but we know the overall flow of the

00:19:37,469 --> 00:19:41,070
system which is which is good so the

00:19:40,109 --> 00:19:42,719
difference between pops into main

00:19:41,070 --> 00:19:44,579
sockets is that they actually use the

00:19:42,719 --> 00:19:46,259
same data flow under the hood but

00:19:44,579 --> 00:19:48,239
sockets just have more semantic

00:19:46,259 --> 00:19:49,379
abstractions so with sockets they

00:19:48,239 --> 00:19:51,899
they're guaranteed to be bi-directional

00:19:49,379 --> 00:19:54,479
whereas pipes but according to POSIX

00:19:51,899 --> 00:19:55,649
anyway or single directional and the

00:19:54,479 --> 00:19:57,599
socket semantics that you're familiar

00:19:55,649 --> 00:19:59,339
with participe all exist so you have a

00:19:57,599 --> 00:20:01,139
stream mode you have a Datagram mode you

00:19:59,339 --> 00:20:03,450
have credentials for determining the

00:20:01,139 --> 00:20:05,039
next peer but in terms of just raw

00:20:03,450 --> 00:20:06,479
performance it's just a bit more CPU in

00:20:05,039 --> 00:20:07,999
a bit more memory but it shouldn't

00:20:06,479 --> 00:20:10,799
really make that much of a difference

00:20:07,999 --> 00:20:13,339
then finally TCP is just obviously the

00:20:10,799 --> 00:20:15,359
juggernaut of data transport right so

00:20:13,339 --> 00:20:17,249
you have the same kind of model as a

00:20:15,359 --> 00:20:18,179
pipe when you write into a buffer but

00:20:17,249 --> 00:20:20,339
then there's a whole packetization

00:20:18,179 --> 00:20:23,759
process so TCP has to deal with sequence

00:20:20,339 --> 00:20:25,049
numbering reassembly and and the

00:20:23,759 --> 00:20:27,599
transmission of these kind of this

00:20:25,049 --> 00:20:29,399
packetized stream to either a NIC or

00:20:27,599 --> 00:20:30,509
another host so it's just it's just more

00:20:29,399 --> 00:20:32,219
complicated and there's more

00:20:30,509 --> 00:20:34,139
housekeeping and even when you go to

00:20:32,219 --> 00:20:36,269
localhost so if you're just saying your

00:20:34,139 --> 00:20:37,739
TCP to one two seven zero zero one the

00:20:36,269 --> 00:20:39,749
kernel is still forced to do all this

00:20:37,739 --> 00:20:41,129
work because you might introduce a

00:20:39,749 --> 00:20:43,139
firewall in there it'd have to go

00:20:41,129 --> 00:20:44,789
through the IP layer it really cannot

00:20:43,139 --> 00:20:46,109
just fake out this illusion of TCP it

00:20:44,789 --> 00:20:50,369
has to actually do a lot of the work

00:20:46,109 --> 00:20:52,619
there's a very few bits in there so with

00:20:50,369 --> 00:20:53,940
so far these are all things that anyone

00:20:52,619 --> 00:20:57,690
who's programmed the network application

00:20:53,940 --> 00:20:59,309
would have used so let's go to shared

00:20:57,690 --> 00:21:01,320
memory right because ideally we would

00:20:59,309 --> 00:21:03,629
never go into the kernel and you'd all

00:21:01,320 --> 00:21:04,859
this gen user space cuz this is kind of

00:21:03,629 --> 00:21:06,389
the obviously to do right why do we need

00:21:04,859 --> 00:21:06,629
this kernel it's expensive it gets in

00:21:06,389 --> 00:21:08,459
the way

00:21:06,629 --> 00:21:10,529
I just want to transfer data from A to B

00:21:08,459 --> 00:21:11,849
I I don't need this thing so how many

00:21:10,529 --> 00:21:16,049
people here designed to shared memory

00:21:11,849 --> 00:21:18,389
transport whoa okay I need to talk all

00:21:16,049 --> 00:21:19,679
of you but before I leave right so this

00:21:18,389 --> 00:21:22,679
is the bit that took more than a weekend

00:21:19,679 --> 00:21:25,349
so first thing is that you have the

00:21:22,679 --> 00:21:26,100
shared buffer and this year buffer needs

00:21:25,349 --> 00:21:29,220
to

00:21:26,100 --> 00:21:31,470
needs to have some some mechanism of

00:21:29,220 --> 00:21:33,480
encoding all the various payloads so

00:21:31,470 --> 00:21:35,370
previously whenever you are you're

00:21:33,480 --> 00:21:37,530
writing these payloads into the kernel

00:21:35,370 --> 00:21:39,150
and out you're writing in the form of

00:21:37,530 --> 00:21:40,440
individual payloads write read and write

00:21:39,150 --> 00:21:41,910
in this world

00:21:40,440 --> 00:21:43,500
we have to have a data structure where

00:21:41,910 --> 00:21:45,990
you have a consumer pointer which says

00:21:43,500 --> 00:21:47,669
you have something to read and produce a

00:21:45,990 --> 00:21:49,289
pointer and you have to have the

00:21:47,669 --> 00:21:51,049
variable length pillows encoded you have

00:21:49,289 --> 00:21:53,429
to tell it for synchronization purposes

00:21:51,049 --> 00:21:54,960
whether or not this is a valid payload

00:21:53,429 --> 00:21:56,429
or not and then of course you have to

00:21:54,960 --> 00:21:58,320
have the list as a as a bit of a ring

00:21:56,429 --> 00:22:00,330
this is a very very common structure

00:21:58,320 --> 00:22:02,880
that normally has a subtly different

00:22:00,330 --> 00:22:04,919
layout between so for example Xen uses

00:22:02,880 --> 00:22:06,570
this to talk between VMs we use this in

00:22:04,919 --> 00:22:09,120
user space to talk between between

00:22:06,570 --> 00:22:11,820
applications but although POSIX in

00:22:09,120 --> 00:22:13,590
particular defines a mechanism to create

00:22:11,820 --> 00:22:15,270
shared memory it doesn't tell you how to

00:22:13,590 --> 00:22:17,760
communicate using shared memory there's

00:22:15,270 --> 00:22:20,789
no standard POSIX sockets version of

00:22:17,760 --> 00:22:22,860
shared memory for various reasons and so

00:22:20,789 --> 00:22:24,360
the question really becomes how do we

00:22:22,860 --> 00:22:26,130
synchronize between these two processes

00:22:24,360 --> 00:22:28,049
because whenever I write something I

00:22:26,130 --> 00:22:30,150
need to wake the other side up and that

00:22:28,049 --> 00:22:31,380
really is where a lot of the variance

00:22:30,150 --> 00:22:32,940
comes in when you're designing shared

00:22:31,380 --> 00:22:35,159
memory protocols it's not really

00:22:32,940 --> 00:22:36,960
designing this structure it's figuring

00:22:35,159 --> 00:22:41,220
how to wake the other side up using the

00:22:36,960 --> 00:22:42,450
least intrusive mechanism possible so if

00:22:41,220 --> 00:22:43,919
you work at an investment bank and

00:22:42,450 --> 00:22:45,780
you're doing high-frequency trading you

00:22:43,919 --> 00:22:46,830
only care about latency and that's all

00:22:45,780 --> 00:22:48,059
you care about right you make it you

00:22:46,830 --> 00:22:50,400
make one hundred million dollars if

00:22:48,059 --> 00:22:52,409
you're 10 nanoseconds faster whatever

00:22:50,400 --> 00:22:54,900
the number is these days what you do is

00:22:52,409 --> 00:22:56,669
that you would spin so in userspace

00:22:54,900 --> 00:22:58,650
both applications would just constantly

00:22:56,669 --> 00:23:00,360
sit in the loop testing for a flag

00:22:58,650 --> 00:23:01,860
testing a bit of memory and trying to

00:23:00,360 --> 00:23:03,059
find out whether or not they should do

00:23:01,860 --> 00:23:04,230
something with the shared buffer because

00:23:03,059 --> 00:23:05,820
the other side has written something and

00:23:04,230 --> 00:23:07,380
this is normally pretty good right

00:23:05,820 --> 00:23:09,480
because you're you're fully utilizing

00:23:07,380 --> 00:23:11,549
your system so you're 100% CPU you're

00:23:09,480 --> 00:23:14,549
spinning away and then unfortunately

00:23:11,549 --> 00:23:17,070
your IT department decides to to

00:23:14,549 --> 00:23:18,929
virtualize your careful setup and at

00:23:17,070 --> 00:23:20,460
this point you realize that you're not

00:23:18,929 --> 00:23:22,020
actually you don't actually own the CPU

00:23:20,460 --> 00:23:24,030
anymore so this is the perfect mechanism

00:23:22,020 --> 00:23:25,260
if you don't care about energy and you

00:23:24,030 --> 00:23:26,640
don't care about the fact that you're

00:23:25,260 --> 00:23:28,860
virtualized and somewhere other domain

00:23:26,640 --> 00:23:30,570
is now taking all your CPU because the

00:23:28,860 --> 00:23:33,000
credit scheduler in Zen has decided

00:23:30,570 --> 00:23:34,740
you're you're abusing the system so we

00:23:33,000 --> 00:23:36,270
can't spin right spinning is kind of

00:23:34,740 --> 00:23:38,580
what you do when you control everything

00:23:36,270 --> 00:23:39,870
and you're sure of your environment so

00:23:38,580 --> 00:23:43,050
the next mechanism

00:23:39,870 --> 00:23:44,970
is also do we trust the other side so if

00:23:43,050 --> 00:23:46,170
you go back here you notice that we're

00:23:44,970 --> 00:23:47,940
actually writing into a buffer and

00:23:46,170 --> 00:23:49,050
reading from a buffer and then because

00:23:47,940 --> 00:23:50,700
you're just grabbing a shared buffer

00:23:49,050 --> 00:23:52,890
what if this application either by

00:23:50,700 --> 00:23:54,510
mistake or maliciously decides to write

00:23:52,890 --> 00:23:55,230
something in so this one you also need

00:23:54,510 --> 00:23:57,030
to worry about that

00:23:55,230 --> 00:23:59,070
so we also need to worry about things

00:23:57,030 --> 00:24:00,000
like trust in the socket API you never

00:23:59,070 --> 00:24:01,500
trust the other side

00:24:00,000 --> 00:24:03,240
you always get a copy if you're doing

00:24:01,500 --> 00:24:04,980
zero copy you got to kind of figure out

00:24:03,240 --> 00:24:06,690
do I copy this into a private buffer or

00:24:04,980 --> 00:24:08,400
not how much does it matter if this

00:24:06,690 --> 00:24:10,620
changes while I'm executing this is a

00:24:08,400 --> 00:24:12,179
decision you make as a programmer so

00:24:10,620 --> 00:24:13,410
there's other mechanisms Linux has

00:24:12,179 --> 00:24:16,410
something called few taxes which are

00:24:13,410 --> 00:24:18,360
fast user space few mutexes and they're

00:24:16,410 --> 00:24:19,640
pretty cool what they let you do is that

00:24:18,360 --> 00:24:22,200
you can mostly stay in user space

00:24:19,640 --> 00:24:24,179
because most of the time you don't block

00:24:22,200 --> 00:24:26,160
and it's only when you need to

00:24:24,179 --> 00:24:27,780
synchronize for example signal the fact

00:24:26,160 --> 00:24:29,910
that you want to block and take a look

00:24:27,780 --> 00:24:31,770
then you drop into the kernel and then

00:24:29,910 --> 00:24:33,690
you drop back into user space so most of

00:24:31,770 --> 00:24:35,010
the time you're it see it's like you're

00:24:33,690 --> 00:24:36,570
spinning right because if I could write

00:24:35,010 --> 00:24:38,280
and there's space I'll just write I

00:24:36,570 --> 00:24:39,780
don't need to talk to anyone and only if

00:24:38,280 --> 00:24:41,429
I need to wait for something to you call

00:24:39,780 --> 00:24:43,410
those they're probably those is it

00:24:41,429 --> 00:24:44,880
they're not portable and again this is

00:24:43,410 --> 00:24:47,070
an example of where by leaving POSIX

00:24:44,880 --> 00:24:48,780
behind you get a lot of cool new

00:24:47,070 --> 00:24:51,210
interfaces but these are evolving all

00:24:48,780 --> 00:24:53,309
the time and so freebsd has a equivalent

00:24:51,210 --> 00:24:54,630
that's almost but not quite the same and

00:24:53,309 --> 00:24:55,920
there's you know interesting differences

00:24:54,630 --> 00:24:57,179
between them only interesting if you're

00:24:55,920 --> 00:24:58,770
working this stuff not interesting you

00:24:57,179 --> 00:25:01,320
if you're trying to just do your job and

00:24:58,770 --> 00:25:03,570
and design a shared memory protocol and

00:25:01,320 --> 00:25:05,340
so there's a lot of these mechanisms so

00:25:03,570 --> 00:25:07,830
some of the observations from doing this

00:25:05,340 --> 00:25:09,780
work is that the POSIX API is obviously

00:25:07,830 --> 00:25:11,190
very well understood but the internals

00:25:09,780 --> 00:25:13,170
are not necessarily so understood

00:25:11,190 --> 00:25:14,280
the performance implications and also it

00:25:13,170 --> 00:25:16,440
makes it a lot of assumptions about

00:25:14,280 --> 00:25:18,390
trust and so on and you need at least

00:25:16,440 --> 00:25:20,910
one data copy so you want to build zero

00:25:18,390 --> 00:25:22,650
zero data copying you can't do that in

00:25:20,910 --> 00:25:25,140
POSIX easily you need to drop the shared

00:25:22,650 --> 00:25:26,429
memory of some kind and also as

00:25:25,140 --> 00:25:28,200
something shared memories there is no

00:25:26,429 --> 00:25:29,640
standard for doing that so really would

00:25:28,200 --> 00:25:31,740
like to start thinking about new

00:25:29,640 --> 00:25:33,210
mechanisms for Transport to give you an

00:25:31,740 --> 00:25:34,410
idea of throughput now I'm not going to

00:25:33,210 --> 00:25:36,050
go through all these graphs right but

00:25:34,410 --> 00:25:38,130
what I have done is if you if you go to

00:25:36,050 --> 00:25:39,630
I'll show you RL later on so it's up

00:25:38,130 --> 00:25:41,370
there I missed that this actually has a

00:25:39,630 --> 00:25:42,540
number of graphs that we auto generate

00:25:41,370 --> 00:25:44,160
from a whole bunch of data sources and

00:25:42,540 --> 00:25:45,750
you can see that if you're on the same

00:25:44,160 --> 00:25:47,370
core and these are all of the different

00:25:45,750 --> 00:25:48,900
mechanisms that I talked about plus some

00:25:47,370 --> 00:25:50,400
additional ones um

00:25:48,900 --> 00:25:52,560
the takeaway from this is that

00:25:50,400 --> 00:25:53,620
performance just makes a big difference

00:25:52,560 --> 00:25:55,630
this is the

00:25:53,620 --> 00:25:57,669
Rupert right the actual performance

00:25:55,630 --> 00:25:59,799
through pert and you can see that we're

00:25:57,669 --> 00:26:02,320
still the gigabits but the white the

00:25:59,799 --> 00:26:03,880
poor white one at the bottom is TCP so

00:26:02,320 --> 00:26:05,620
this is the one that we often use to

00:26:03,880 --> 00:26:07,630
connect here my sequel or whatever and

00:26:05,620 --> 00:26:10,570
all the other ones are just variations

00:26:07,630 --> 00:26:13,179
on safe or unsafe shared memory and on

00:26:10,570 --> 00:26:14,320
corn off core and it just really is

00:26:13,179 --> 00:26:16,029
quite hard to predict what's going on

00:26:14,320 --> 00:26:18,880
right but it is clear that if you use

00:26:16,029 --> 00:26:20,049
TCP you're missing out your things are

00:26:18,880 --> 00:26:21,549
just going really really slowly for

00:26:20,049 --> 00:26:24,159
whatever reason and this could be

00:26:21,549 --> 00:26:25,390
because of the the flow control

00:26:24,159 --> 00:26:27,429
mechanism see speeding or dealing with

00:26:25,390 --> 00:26:29,529
with fast mechanisms or it could just be

00:26:27,429 --> 00:26:31,149
the fact of shared memory and all the

00:26:29,529 --> 00:26:32,649
privileged boundaries are just better so

00:26:31,149 --> 00:26:35,950
it's clear that I would rather be up

00:26:32,649 --> 00:26:37,330
there in the 25,000 in fact it's I'll

00:26:35,950 --> 00:26:39,130
show you afterwards is that on the

00:26:37,330 --> 00:26:40,630
Intel's you get up to 100 gigabits that

00:26:39,130 --> 00:26:43,990
you just cannot even hope to you with

00:26:40,630 --> 00:26:45,520
TCP so hopefully I've convinced you that

00:26:43,990 --> 00:26:47,020
it's complicated and the question is

00:26:45,520 --> 00:26:48,580
just how complicated is it right I mean

00:26:47,020 --> 00:26:50,590
I've shown you a few graphs there's some

00:26:48,580 --> 00:26:52,480
stuff it's so it's all a bit variant so

00:26:50,590 --> 00:26:54,250
when we actually built some of these

00:26:52,480 --> 00:26:55,899
benchmarks it turns out that there's a

00:26:54,250 --> 00:26:57,669
lot of decisions you make when you when

00:26:55,899 --> 00:26:58,779
you want to do i oh so you start off

00:26:57,669 --> 00:27:00,250
with some of the ones that describe like

00:26:58,779 --> 00:27:02,860
the instruction set architecture and the

00:27:00,250 --> 00:27:04,720
fact that it's it's Intel based are we

00:27:02,860 --> 00:27:06,010
virtualized which hypervisor they're all

00:27:04,720 --> 00:27:07,899
different they're all good in their own

00:27:06,010 --> 00:27:09,039
ways but it is different how do you

00:27:07,899 --> 00:27:10,870
synchronize are you running freebies

00:27:09,039 --> 00:27:13,029
here Linux and then more generally at

00:27:10,870 --> 00:27:14,409
the high level how do you want to do

00:27:13,029 --> 00:27:16,330
flow control in congestion do you care

00:27:14,409 --> 00:27:18,820
if you block for a while or not or just

00:27:16,330 --> 00:27:21,370
need constant bitrate do you trust the

00:27:18,820 --> 00:27:22,809
other side and SSL if anyone has program

00:27:21,370 --> 00:27:24,909
to open SL can be challenging to get

00:27:22,809 --> 00:27:27,429
right and there's lots of cool new

00:27:24,909 --> 00:27:28,990
mechanisms for doing a much more CPU

00:27:27,429 --> 00:27:31,659
friendly encryption like tcp crypt from

00:27:28,990 --> 00:27:34,240
from stanford and UCL and at the higher

00:27:31,659 --> 00:27:36,549
level are using java or using python or

00:27:34,240 --> 00:27:38,590
using a camel or haskell all of these

00:27:36,549 --> 00:27:40,330
bindings make a big difference and so

00:27:38,590 --> 00:27:42,250
each of these might be death by 10

00:27:40,330 --> 00:27:43,210
percent or 100 percent but when you add

00:27:42,250 --> 00:27:44,770
them all up if you don't get them all

00:27:43,210 --> 00:27:47,350
right to align with your environment

00:27:44,770 --> 00:27:49,960
you're losing out on on on gigabits of

00:27:47,350 --> 00:27:51,970
performance so we've been hacking on an

00:27:49,960 --> 00:27:53,230
open-source suite and it just a

00:27:51,970 --> 00:27:57,039
systematically measured this stuff kind

00:27:53,230 --> 00:27:59,320
of a self-test in in this world just got

00:27:57,039 --> 00:28:00,580
a PC bench it's open source it's up

00:27:59,320 --> 00:28:01,570
there and github and it's very much a

00:28:00,580 --> 00:28:03,399
work in progress right we've been

00:28:01,570 --> 00:28:06,250
hacking as furiously trying to do our

00:28:03,399 --> 00:28:07,030
kind of CSI Numa work at the same time

00:28:06,250 --> 00:28:08,380
but

00:28:07,030 --> 00:28:10,780
right now there's a whole bunch of stuff

00:28:08,380 --> 00:28:12,100
if anyone's interested that we own it

00:28:10,780 --> 00:28:14,020
imported FreeBSD for examples for

00:28:12,100 --> 00:28:15,790
Robertson is working that but also it's

00:28:14,020 --> 00:28:16,750
not just a matter of porting it it's a

00:28:15,790 --> 00:28:18,310
matter of trying to figure out how do

00:28:16,750 --> 00:28:20,620
you best use that platform that you've

00:28:18,310 --> 00:28:22,120
got so if I'm putting this FreeBSD I do

00:28:20,620 --> 00:28:23,800
want to just use TCP I want to figure

00:28:22,120 --> 00:28:25,150
out what kernel abstraction is make

00:28:23,800 --> 00:28:26,860
available

00:28:25,150 --> 00:28:28,570
what kind of few Tex mechanisms and

00:28:26,860 --> 00:28:30,340
really just try to win the game of of

00:28:28,570 --> 00:28:32,410
IPC performance as we go through there

00:28:30,340 --> 00:28:34,300
and the bigger problem is that we just

00:28:32,410 --> 00:28:36,670
don't have much hardware all right it's

00:28:34,300 --> 00:28:38,620
frustrating to see just the fact that

00:28:36,670 --> 00:28:39,760
just from looking at the data that we

00:28:38,620 --> 00:28:42,010
have we just maybe eight or nine

00:28:39,760 --> 00:28:44,290
machines we've learned so many little

00:28:42,010 --> 00:28:46,180
facts that we somehow need to encode in

00:28:44,290 --> 00:28:48,460
our operating system so you know Zen and

00:28:46,180 --> 00:28:50,050
Numa have significant effects that the

00:28:48,460 --> 00:28:51,580
architectural variance makes a huge

00:28:50,050 --> 00:28:53,260
difference as you might expect but just

00:28:51,580 --> 00:28:55,390
how big well what's the implication when

00:28:53,260 --> 00:28:56,380
you go to AMD bulldozer versus the

00:28:55,390 --> 00:28:59,080
magnet cores that were using right now

00:28:56,380 --> 00:29:00,670
hyper threads are just this terrible

00:28:59,080 --> 00:29:03,190
idea that are just stuck around and and

00:29:00,670 --> 00:29:05,170
when we have 160 core box it's a

00:29:03,190 --> 00:29:06,460
combination of an Intel Architecture

00:29:05,170 --> 00:29:08,140
combined with hyper threads and

00:29:06,460 --> 00:29:08,980
performance just goes all over the place

00:29:08,140 --> 00:29:11,260
when you're when you're dealing with

00:29:08,980 --> 00:29:13,270
with hyper threads some conditions are

00:29:11,260 --> 00:29:15,190
just particularly bad if VM supplies for

00:29:13,270 --> 00:29:16,900
example on VirtualBox is is terrible

00:29:15,190 --> 00:29:18,370
that these are all just facts that don't

00:29:16,900 --> 00:29:20,230
really fit into a framework how do you

00:29:18,370 --> 00:29:22,000
use these as a programmer so what we

00:29:20,230 --> 00:29:24,370
really want is the ability to run IPC

00:29:22,000 --> 00:29:25,510
bench we want to crowdsource the results

00:29:24,370 --> 00:29:26,230
we want we're trying to make it as

00:29:25,510 --> 00:29:28,060
simple as possible

00:29:26,230 --> 00:29:30,120
for people who run these self tests and

00:29:28,060 --> 00:29:33,070
then just give us kind of long-term data

00:29:30,120 --> 00:29:35,110
so if you are Microsoft for example then

00:29:33,070 --> 00:29:37,420
you do get this huge set of interesting

00:29:35,110 --> 00:29:39,750
performance data from from the windows

00:29:37,420 --> 00:29:42,040
crash reports and and and Google and

00:29:39,750 --> 00:29:44,290
Vowell for example galleries for games

00:29:42,040 --> 00:29:46,000
we as a community do not do this and we

00:29:44,290 --> 00:29:47,260
really do need to start because when

00:29:46,000 --> 00:29:48,430
we're doing this work and design these

00:29:47,260 --> 00:29:50,560
transports I would have loved to have

00:29:48,430 --> 00:29:52,120
gone back through time to say well 10

00:29:50,560 --> 00:29:54,100
years ago what would this have looked

00:29:52,120 --> 00:29:55,960
like and there are just obviously a lot

00:29:54,100 --> 00:29:57,610
of challenges how do we host this is an

00:29:55,960 --> 00:29:59,320
open way so right now we're experiment

00:29:57,610 --> 00:30:01,090
with github so that the idea is you can

00:29:59,320 --> 00:30:02,980
run IPC bench with the Python script and

00:30:01,090 --> 00:30:04,840
it will just issue a pull request to us

00:30:02,980 --> 00:30:06,520
but of course if we get bored and go

00:30:04,840 --> 00:30:07,750
into other research projects anyone can

00:30:06,520 --> 00:30:09,220
look at these github branches and just

00:30:07,750 --> 00:30:10,990
do their own analysis and aggregation

00:30:09,220 --> 00:30:12,250
but there are some challenges when you

00:30:10,990 --> 00:30:13,660
actually try to make this real so how do

00:30:12,250 --> 00:30:15,100
you version this stuff so if you're

00:30:13,660 --> 00:30:16,300
running a benchmark all over you know

00:30:15,100 --> 00:30:17,620
the course of 10 years or whatever we

00:30:16,300 --> 00:30:18,910
want to do how do you someone has to

00:30:17,620 --> 00:30:20,890
really keep track of this stuff and

00:30:18,910 --> 00:30:23,830
perform the analyses and so on

00:30:20,890 --> 00:30:26,170
so right now we have a few PhD students

00:30:23,830 --> 00:30:27,910
who are interested but overall I'd love

00:30:26,170 --> 00:30:29,980
to hear from anyone who can do this and

00:30:27,910 --> 00:30:31,270
also if you go to this URL you can

00:30:29,980 --> 00:30:32,410
actually just try to start doing it and

00:30:31,270 --> 00:30:33,640
just email us the results we haven't

00:30:32,410 --> 00:30:36,520
quite finished the the github

00:30:33,640 --> 00:30:38,620
integration so far so that'll be great

00:30:36,520 --> 00:30:40,059
so we're also trying something highly

00:30:38,620 --> 00:30:42,130
experimental that I'm gonna spend the

00:30:40,059 --> 00:30:42,400
last ten minutes of my talk talking

00:30:42,130 --> 00:30:45,070
about

00:30:42,400 --> 00:30:47,200
and this is super external alright so if

00:30:45,070 --> 00:30:49,929
if you need to flame me this is probably

00:30:47,200 --> 00:30:52,720
this is probably the time so I believe

00:30:49,929 --> 00:30:54,549
that UNIX abstractions for communication

00:30:52,720 --> 00:30:56,860
are just stuck in the in the dark ages

00:30:54,549 --> 00:30:58,540
and it really is time to apply a lot of

00:30:56,860 --> 00:31:00,640
the lessons we've learned from from the

00:30:58,540 --> 00:31:02,799
last kind of ten years of of especially

00:31:00,640 --> 00:31:04,660
from Network processing where we we have

00:31:02,799 --> 00:31:06,820
things like open flow and technologies

00:31:04,660 --> 00:31:08,200
to deal with flows of communication

00:31:06,820 --> 00:31:10,510
instead of individual packets or

00:31:08,200 --> 00:31:11,950
individual system calls so what we

00:31:10,510 --> 00:31:14,500
really need in UNIX is this notion of

00:31:11,950 --> 00:31:16,809
just making IO first class I need to

00:31:14,500 --> 00:31:19,390
open up a a connection to another host

00:31:16,809 --> 00:31:21,549
and that connection needs to be

00:31:19,390 --> 00:31:23,290
persistent so something is tracking the

00:31:21,549 --> 00:31:25,390
fact that I am communicating from from A

00:31:23,290 --> 00:31:26,620
to B kind of a data flow graph I don't

00:31:25,390 --> 00:31:28,000
want to have to worry about what the

00:31:26,620 --> 00:31:30,429
best transport choice for my environment

00:31:28,000 --> 00:31:34,360
is I only just declare the fact that I

00:31:30,429 --> 00:31:36,070
need a low latency low latency but I

00:31:34,360 --> 00:31:38,020
don't mind the occasional pause yeah I

00:31:36,070 --> 00:31:39,610
want to declare my constraints and have

00:31:38,020 --> 00:31:42,340
my operating system or my hypervisor

00:31:39,610 --> 00:31:44,620
just decide all so I'm going to talk

00:31:42,340 --> 00:31:45,970
about dynamic reconfiguration so when

00:31:44,620 --> 00:31:49,000
you're running these communications why

00:31:45,970 --> 00:31:50,230
would you want to reconfigure your IO so

00:31:49,000 --> 00:31:51,900
it's actually a really good really good

00:31:50,230 --> 00:31:54,490
use so you're running in the cloud

00:31:51,900 --> 00:31:56,919
you've deployed something on Amazon ec2

00:31:54,490 --> 00:31:58,270
and what's gonna happen so you're

00:31:56,919 --> 00:32:01,000
running a web server in a database and

00:31:58,270 --> 00:32:03,730
you just configure them say my sequel it

00:32:01,000 --> 00:32:06,220
might figure out it's on the same host

00:32:03,730 --> 00:32:07,870
but you tell it to use TCP to

00:32:06,220 --> 00:32:10,630
communicate because that's the easiest

00:32:07,870 --> 00:32:12,940
mechanism and then when you live migrate

00:32:10,630 --> 00:32:14,500
to another host right under the hood you

00:32:12,940 --> 00:32:16,540
this might be done automatically by

00:32:14,500 --> 00:32:18,669
Amazon it still works because there's

00:32:16,540 --> 00:32:22,030
TCP right it just goes over the same ho2

00:32:18,669 --> 00:32:24,460
over the network channel instead so what

00:32:22,030 --> 00:32:26,500
happens if you then do some super

00:32:24,460 --> 00:32:27,549
optimal shared memory transport so Zen

00:32:26,500 --> 00:32:28,870
has something called live each hand

00:32:27,549 --> 00:32:30,700
which is a memory to memory

00:32:28,870 --> 00:32:32,740
communication it's really cool

00:32:30,700 --> 00:32:34,360
written by Dell degree from the NSA and

00:32:32,740 --> 00:32:37,090
it's just super fast it's great

00:32:34,360 --> 00:32:40,149
now of course when you live migrate your

00:32:37,090 --> 00:32:41,620
database to studies because it is shared

00:32:40,149 --> 00:32:43,630
memory and the memories is disappeared

00:32:41,620 --> 00:32:45,820
and Amazon has no idea what you're doing

00:32:43,630 --> 00:32:47,769
in you in your in your and your host and

00:32:45,820 --> 00:32:49,690
your in your guest so it will just not

00:32:47,769 --> 00:32:51,340
worry about it and so what you really

00:32:49,690 --> 00:32:52,570
want is this atom where something was

00:32:51,340 --> 00:32:54,490
tracking the fact that there was data

00:32:52,570 --> 00:32:56,919
going between these it'll reconfigure it

00:32:54,490 --> 00:32:58,539
on the basis that oh we can no longer

00:32:56,919 --> 00:33:01,029
use shared memory therefore I need to

00:32:58,539 --> 00:33:02,679
switch to SSL on TCP and do we even need

00:33:01,029 --> 00:33:04,360
SSL if it's gone to a remote cloud

00:33:02,679 --> 00:33:05,710
provider you want this abstraction where

00:33:04,360 --> 00:33:07,419
you can just reconfigure stuff as an

00:33:05,710 --> 00:33:09,340
operating some service and have that

00:33:07,419 --> 00:33:10,299
just work for you and so that's where

00:33:09,340 --> 00:33:12,580
we're working on this system called

00:33:10,299 --> 00:33:14,500
fable so it's an extension to UNIX and

00:33:12,580 --> 00:33:17,169
to some extent is end as well because it

00:33:14,500 --> 00:33:18,880
it works in at many levels and all we do

00:33:17,169 --> 00:33:19,690
is we add a name demon a cunning named

00:33:18,880 --> 00:33:22,330
him into Unix

00:33:19,690 --> 00:33:25,419
that gives every process every entity a

00:33:22,330 --> 00:33:27,789
first class name so each process might

00:33:25,419 --> 00:33:30,490
be identified by hierarchy by it paid

00:33:27,789 --> 00:33:31,990
and by by where it is and you have some

00:33:30,490 --> 00:33:33,669
flow manager that can keep track of all

00:33:31,990 --> 00:33:35,740
of the aisle flows between them so this

00:33:33,669 --> 00:33:37,570
for example is just within a single kind

00:33:35,740 --> 00:33:39,070
of kernel then it might talk to

00:33:37,570 --> 00:33:40,570
different virtual machines on the host

00:33:39,070 --> 00:33:43,690
so these have each each virtual machine

00:33:40,570 --> 00:33:45,250
has its own name you might then go to a

00:33:43,690 --> 00:33:46,600
different cluster and have a lots of

00:33:45,250 --> 00:33:48,220
lots of communication flows between them

00:33:46,600 --> 00:33:49,480
but they're all systemically tracked

00:33:48,220 --> 00:33:51,039
either wide by a cluster wide name

00:33:49,480 --> 00:33:52,330
daemon which were prototyping using

00:33:51,039 --> 00:33:54,220
zookeeper which deals with fault

00:33:52,330 --> 00:33:56,200
tolerance and and failover and so on or

00:33:54,220 --> 00:33:57,820
locally just by a scheduler running in

00:33:56,200 --> 00:33:59,110
the in the kernel or by something

00:33:57,820 --> 00:34:00,370
running the hypervisor you just want a

00:33:59,110 --> 00:34:03,010
systematic he he could keep track of

00:34:00,370 --> 00:34:04,779
this stuff and then the the i/o

00:34:03,010 --> 00:34:06,760
mechanism is natively zero copy

00:34:04,779 --> 00:34:08,619
so rather than POSIX where we bacon

00:34:06,760 --> 00:34:10,300
assumptions about trust we're trying

00:34:08,619 --> 00:34:11,619
designed this low-level API the

00:34:10,300 --> 00:34:13,480
basically just says all you have to do

00:34:11,619 --> 00:34:15,220
is give me a name you have to connect to

00:34:13,480 --> 00:34:16,480
a remote host you have to have the

00:34:15,220 --> 00:34:18,010
ability to get these buffers that you

00:34:16,480 --> 00:34:19,659
can read it right into and then you can

00:34:18,010 --> 00:34:21,310
commit a release in so it really is just

00:34:19,659 --> 00:34:23,050
a very very simple low-level mechanism

00:34:21,310 --> 00:34:26,190
and all of the policy can be implemented

00:34:23,050 --> 00:34:28,270
in various bits of the kernel and so on

00:34:26,190 --> 00:34:30,490
so what does this look like in practice

00:34:28,270 --> 00:34:32,830
so if I'm starting up a fable connection

00:34:30,490 --> 00:34:34,570
then I want to establish a handle

00:34:32,830 --> 00:34:37,510
between my my virtual machines so let's

00:34:34,570 --> 00:34:39,460
say I have foo and bar and then this

00:34:37,510 --> 00:34:41,260
might actually represent a process or

00:34:39,460 --> 00:34:43,599
might represent a VM i I don't really

00:34:41,260 --> 00:34:45,909
know and then you connect in a URL type

00:34:43,599 --> 00:34:48,010
scheme so I just say open me up in xio

00:34:45,909 --> 00:34:51,070
connection to this this other side

00:34:48,010 --> 00:34:53,290
now because this is a URL scheme this

00:34:51,070 --> 00:34:55,179
could this xio represents the fact that

00:34:53,290 --> 00:34:57,370
I want to automatically figure out the

00:34:55,179 --> 00:34:59,860
transport mechanism I could also say TCP

00:34:57,370 --> 00:35:02,170
colon slash slash colon 80 to connect to

00:34:59,860 --> 00:35:03,640
an existing service this basically just

00:35:02,170 --> 00:35:05,980
decides do I want to stay within my

00:35:03,640 --> 00:35:08,140
Fable world of automatic communication

00:35:05,980 --> 00:35:09,280
or do you want to go externally and so

00:35:08,140 --> 00:35:10,960
the Fable then takes care of

00:35:09,280 --> 00:35:13,570
automatically deciding exactly what the

00:35:10,960 --> 00:35:14,650
communication mechanism is and it

00:35:13,570 --> 00:35:17,680
provides a certain amount of feature

00:35:14,650 --> 00:35:19,030
proofing for your your system so it

00:35:17,680 --> 00:35:21,220
might decide well you know these two

00:35:19,030 --> 00:35:22,810
nodes are in the same machine therefore

00:35:21,220 --> 00:35:24,580
just go to a shared memory and a few

00:35:22,810 --> 00:35:25,930
text based approach because my local

00:35:24,580 --> 00:35:29,110
benchmarks have decided that's the case

00:35:25,930 --> 00:35:32,080
or TCP or if you went to the extremely

00:35:29,110 --> 00:35:34,210
cool talk by a crystal passion or

00:35:32,080 --> 00:35:35,680
belgian friends could it just use multi

00:35:34,210 --> 00:35:37,390
pass tcp could have figure out the fact

00:35:35,680 --> 00:35:38,590
that we can actually have all these

00:35:37,390 --> 00:35:39,720
different endpoints and we really should

00:35:38,590 --> 00:35:42,070
be communicating via two routes

00:35:39,720 --> 00:35:43,540
deploying these newer technologies is

00:35:42,070 --> 00:35:44,860
very difficult right now because we

00:35:43,540 --> 00:35:47,500
bacon many assumptions about these

00:35:44,860 --> 00:35:48,700
transports into our applications so xio

00:35:47,500 --> 00:35:51,180
kind of lifts up the game a little bit

00:35:48,700 --> 00:35:53,860
and tries to tries to make that simpler

00:35:51,180 --> 00:35:55,600
so in practice you can also make things

00:35:53,860 --> 00:35:58,480
very very fast with this kind of

00:35:55,600 --> 00:35:59,920
approach because you're separating the

00:35:58,480 --> 00:36:01,510
connection establishment from your

00:35:59,920 --> 00:36:03,310
actual buffer management these

00:36:01,510 --> 00:36:05,800
individual buffers can be highly

00:36:03,310 --> 00:36:07,900
optimized for the purpose at hand so I

00:36:05,800 --> 00:36:10,480
might have I want to communicate between

00:36:07,900 --> 00:36:12,250
these two processes or VMs I might get a

00:36:10,480 --> 00:36:13,750
shared memory context that I then get a

00:36:12,250 --> 00:36:16,870
shared memory buffer that I can write

00:36:13,750 --> 00:36:19,720
directly into or I might get a TCP one

00:36:16,870 --> 00:36:21,370
where it's page aligned it's very very

00:36:19,720 --> 00:36:23,020
friendly for the type of network

00:36:21,370 --> 00:36:24,250
communication you need to do so if you

00:36:23,020 --> 00:36:26,290
try to do the high performance network

00:36:24,250 --> 00:36:28,270
communication you need to ensure that

00:36:26,290 --> 00:36:29,980
your your buffers are page aligned

00:36:28,270 --> 00:36:32,200
because you can just flip them directly

00:36:29,980 --> 00:36:33,940
to your your target NIC but also that

00:36:32,200 --> 00:36:35,650
you reserve some header space for the IP

00:36:33,940 --> 00:36:36,760
header and the TCP header table just

00:36:35,650 --> 00:36:38,380
takes care of all that it just gives you

00:36:36,760 --> 00:36:39,880
a buffer the where the pointer is in the

00:36:38,380 --> 00:36:41,200
right place and you just write your data

00:36:39,880 --> 00:36:43,390
into it and if you need to write more

00:36:41,200 --> 00:36:44,980
data you just request more buffers so

00:36:43,390 --> 00:36:46,630
it's a very very simple mechanism that

00:36:44,980 --> 00:36:49,720
hides all of the transport details from

00:36:46,630 --> 00:36:52,090
you at different times so the uses are

00:36:49,720 --> 00:36:53,410
myriad for this stuff so what started

00:36:52,090 --> 00:36:55,540
off is kind of a fun benchmarking

00:36:53,410 --> 00:36:58,210
experiment over the weekend turn into a

00:36:55,540 --> 00:36:59,950
a very long weekend but also the fact

00:36:58,210 --> 00:37:01,780
that we just found a lot of our users so

00:36:59,950 --> 00:37:04,480
some of some of the more concrete

00:37:01,780 --> 00:37:07,780
putting this for is we can upgrade the

00:37:04,480 --> 00:37:09,460
ring then and then as the hypervisor I'm

00:37:07,780 --> 00:37:11,650
most familiar with but this really does

00:37:09,460 --> 00:37:13,120
exist but with every hypervisor is they

00:37:11,650 --> 00:37:15,880
implement some kind of a user space

00:37:13,120 --> 00:37:17,650
communication mechanism and this stuff

00:37:15,880 --> 00:37:19,300
tends to be very hard to benchmark

00:37:17,650 --> 00:37:21,400
systematically so we believe that you

00:37:19,300 --> 00:37:23,170
can do automatic upgrading of there's a

00:37:21,400 --> 00:37:25,480
resent ring API and dramatically improve

00:37:23,170 --> 00:37:29,560
or rather the improve the predictability

00:37:25,480 --> 00:37:30,850
and the performance of Io so one of one

00:37:29,560 --> 00:37:31,870
of the one of the more concrete ones is

00:37:30,850 --> 00:37:34,690
that we've been working on an

00:37:31,870 --> 00:37:36,670
alternative Big Data mechanism to Hadoop

00:37:34,690 --> 00:37:40,030
so if how many people know what Hadoop

00:37:36,670 --> 00:37:41,620
is they do not produce great terrible

00:37:40,030 --> 00:37:44,230
program model because all you can do is

00:37:41,620 --> 00:37:45,790
Map Reduce CL is more fun because it's

00:37:44,230 --> 00:37:47,560
cheering powerful you can run arbitrary

00:37:45,790 --> 00:37:49,030
term powerful computation on any

00:37:47,560 --> 00:37:51,280
language it just uses distributed

00:37:49,030 --> 00:37:53,140
continuations to let you know you could

00:37:51,280 --> 00:37:55,030
write MapReduce and Haskell as just the

00:37:53,140 --> 00:37:56,350
obvious functions and CL will take care

00:37:55,030 --> 00:37:57,880
of suspending and starting up on

00:37:56,350 --> 00:37:59,650
multiple hosts with fault tolerance and

00:37:57,880 --> 00:38:02,560
so on but this is just a great example

00:37:59,650 --> 00:38:05,170
of if CL is obvious or project but if

00:38:02,560 --> 00:38:06,700
it's far far more real but both of these

00:38:05,170 --> 00:38:10,060
need the ability to communicate and send

00:38:06,700 --> 00:38:12,220
data across HDFS and whatever fast music

00:38:10,060 --> 00:38:14,200
very very fast so just improving the

00:38:12,220 --> 00:38:16,780
speed of cloud data processing so that

00:38:14,200 --> 00:38:19,360
you're not scared of virtualization as

00:38:16,780 --> 00:38:21,160
being slowing on Io is a big deal we're

00:38:19,360 --> 00:38:22,420
also building really fun microkernel

00:38:21,160 --> 00:38:24,370
systems so this one is written in a

00:38:22,420 --> 00:38:26,440
camel which I probably Richard is the

00:38:24,370 --> 00:38:28,300
only person who's heard about but it's

00:38:26,440 --> 00:38:30,400
it's a it's an effort to just start

00:38:28,300 --> 00:38:32,200
building simpler and more predictable

00:38:30,400 --> 00:38:34,450
guest virtual machines that run directly

00:38:32,200 --> 00:38:35,590
on the cloud under Xen so mirage is a

00:38:34,450 --> 00:38:37,300
self hosting system where he writes

00:38:35,590 --> 00:38:39,310
about camel code and it Capas directly

00:38:37,300 --> 00:38:40,660
into microkernel which is great until

00:38:39,310 --> 00:38:41,920
you really start missing unix because

00:38:40,660 --> 00:38:43,420
when you when you work at microkernels

00:38:41,920 --> 00:38:45,610
you don't have all of the nice features

00:38:43,420 --> 00:38:48,490
of communication selection and stuff the

00:38:45,610 --> 00:38:50,080
UNIX provides you and so the fable is

00:38:48,490 --> 00:38:52,030
actually support library to deal with

00:38:50,080 --> 00:38:53,830
communicating between micro kernels as

00:38:52,030 --> 00:38:55,120
well as as normal normal virtual

00:38:53,830 --> 00:38:56,830
machines and a lot of this research

00:38:55,120 --> 00:38:58,570
applies to many languages so if you

00:38:56,830 --> 00:39:00,730
wanna run nodejs and your javascript

00:38:58,570 --> 00:39:02,410
system is a microkernel it's actually

00:39:00,730 --> 00:39:03,880
very straightforward but this actually

00:39:02,410 --> 00:39:05,320
gives you a lot of support libraries to

00:39:03,880 --> 00:39:08,350
just make that a lot more practical in

00:39:05,320 --> 00:39:09,670
in in production use and my good friend

00:39:08,350 --> 00:39:11,830
Robert Watson my colleague from the lab

00:39:09,670 --> 00:39:14,620
is working on a new project to build a

00:39:11,830 --> 00:39:15,579
reconfigurable FPGA hardware that boots

00:39:14,620 --> 00:39:16,809
previously so this is

00:39:15,579 --> 00:39:18,729
definitely the most insane project we're

00:39:16,809 --> 00:39:20,049
working on but it can also use a bit of

00:39:18,729 --> 00:39:22,299
future proofing from this kind of i/o

00:39:20,049 --> 00:39:23,859
system so I've given you a bit of a

00:39:22,299 --> 00:39:26,170
whirlwind tour I hope I woke you up with

00:39:23,859 --> 00:39:28,359
some graphs and and I hope you're gonna

00:39:26,170 --> 00:39:29,650
punch me about my ideas about i/o but

00:39:28,359 --> 00:39:31,420
really what I'm sure what we're trying

00:39:29,650 --> 00:39:33,249
to do is just trying to understand the

00:39:31,420 --> 00:39:35,680
complexity of unix i/o and we're trying

00:39:33,249 --> 00:39:37,509
to propose one mechanism to understand

00:39:35,680 --> 00:39:40,900
it better which is to build IPC bench

00:39:37,509 --> 00:39:42,789
and i really hope that if you guys have

00:39:40,900 --> 00:39:45,369
Linux laptops you can just get clone

00:39:42,789 --> 00:39:47,589
this almost no no dependencies I think

00:39:45,369 --> 00:39:49,900
you might need letting you Numa dev but

00:39:47,589 --> 00:39:51,130
just you know stick a pull request or an

00:39:49,900 --> 00:39:52,989
issue in there and just run that and

00:39:51,130 --> 00:39:54,279
just give us results and we promise to

00:39:52,989 --> 00:39:56,529
make them as open as we possibly can

00:39:54,279 --> 00:39:58,749
within within our constraints which is

00:39:56,529 --> 00:40:00,519
both public domain right and and

00:39:58,749 --> 00:40:03,339
meanwhile we're hacking on this this

00:40:00,519 --> 00:40:06,239
fable system where we with the minimal

00:40:03,339 --> 00:40:09,430
set of intrusions to Linux and to to

00:40:06,239 --> 00:40:11,529
various of the bsts we're just trying to

00:40:09,430 --> 00:40:12,729
transparently upgrade IO so there's also

00:40:11,529 --> 00:40:14,739
an emulation layer in there that works

00:40:12,729 --> 00:40:15,999
with sockets and so on and and and a

00:40:14,739 --> 00:40:18,519
whole bunch of name daemons

00:40:15,999 --> 00:40:19,390
so between these the goal is next year's

00:40:18,519 --> 00:40:21,609
falls down we should be able to do a

00:40:19,390 --> 00:40:23,430
demo where you have 10 gigabits of

00:40:21,609 --> 00:40:25,690
throughput between your Zen system and

00:40:23,430 --> 00:40:28,029
virtualization should genuinely be an

00:40:25,690 --> 00:40:29,829
invisible perform set so right now

00:40:28,029 --> 00:40:31,749
virtualization is beautiful for CPU it

00:40:29,829 --> 00:40:32,619
just it just does not get in the way but

00:40:31,749 --> 00:40:34,180
it definitely does get in the way for

00:40:32,619 --> 00:40:35,680
i/o and these kind of systems should

00:40:34,180 --> 00:40:37,749
just fix that problem and just make it

00:40:35,680 --> 00:40:40,509
really a genuine management gain with no

00:40:37,749 --> 00:40:42,969
downsides for for the purposes of of

00:40:40,509 --> 00:40:44,170
running your high performance tasks so

00:40:42,969 --> 00:40:44,589
with that in mind are there any

00:40:44,170 --> 00:40:46,469
questions

00:40:44,589 --> 00:40:49,269
apart from I need to talk to you about

00:40:46,469 --> 00:40:53,880
domain new mer node allocation and such

00:40:49,269 --> 00:40:53,880
things hi

00:41:01,079 --> 00:41:04,079
yep

00:41:05,080 --> 00:41:13,930
yeah oh it's a third yes so the the the

00:41:11,800 --> 00:41:16,150
CL work I mentioned we actually added

00:41:13,930 --> 00:41:17,770
MPI support to CL so it's why I'm

00:41:16,150 --> 00:41:19,450
actually working with professor Dave

00:41:17,770 --> 00:41:22,450
page from NASA at the moment and he's

00:41:19,450 --> 00:41:24,370
got a moon probe called diviner and

00:41:22,450 --> 00:41:26,740
their goal is to retrace whole planets

00:41:24,370 --> 00:41:28,930
and they use MPI to build computation

00:41:26,740 --> 00:41:31,120
models of the thermodynamics of the moon

00:41:28,930 --> 00:41:32,650
and of various various planets and

00:41:31,120 --> 00:41:34,990
they're hitting this classic kind of

00:41:32,650 --> 00:41:36,010
scalability curve where you just you cut

00:41:34,990 --> 00:41:37,210
you've once you had more than ten toes

00:41:36,010 --> 00:41:39,210
nodes that's it you're out of luck so

00:41:37,210 --> 00:41:41,620
we're we're aiming to use this to

00:41:39,210 --> 00:41:43,540
automatically upgrade MPI based

00:41:41,620 --> 00:41:45,310
transports into a more data flow based

00:41:43,540 --> 00:41:47,350
model at the same time so they're the

00:41:45,310 --> 00:41:51,040
MPI community has lots of incredibly low

00:41:47,350 --> 00:41:52,630
level tweaks to use InfiniBand and

00:41:51,040 --> 00:41:53,710
transport to make this work the probe

00:41:52,630 --> 00:41:55,960
course is how do you move that out of

00:41:53,710 --> 00:41:57,340
MPI into more general frameworks like

00:41:55,960 --> 00:41:58,300
Hadoop and so on so it's actually an

00:41:57,340 --> 00:42:01,000
active area we're working on at the

00:41:58,300 --> 00:42:02,770
moment and I have an unpublished paper

00:42:01,000 --> 00:42:04,900
on this and see how I can I can happily

00:42:02,770 --> 00:42:06,970
forward you about our MPI adapter for

00:42:04,900 --> 00:42:10,440
Big Data at the same time but yeah it's

00:42:06,970 --> 00:42:15,250
definitely a much more general situation

00:42:10,440 --> 00:42:42,130
david ige

00:42:15,250 --> 00:42:43,150
I can hear you I'll repeat that's one of

00:42:42,130 --> 00:42:44,530
the reasons we don't want to build in

00:42:43,150 --> 00:42:45,040
some structure by ourselves right just

00:42:44,530 --> 00:42:49,630
too small

00:42:45,040 --> 00:42:51,250
so absolutely i'll so the to repeat the

00:42:49,630 --> 00:42:53,410
question mozilla is doing a lot of work

00:42:51,250 --> 00:42:55,840
and open data gathering and they're very

00:42:53,410 --> 00:42:57,520
kindly offered lots of help to the

00:42:55,840 --> 00:42:59,020
alphas integral actually bench with with

00:42:57,520 --> 00:43:00,940
with their techniques and of course

00:42:59,020 --> 00:43:02,550
there's a lot more things than IPC that

00:43:00,940 --> 00:43:04,480
we can measure right so you can measure

00:43:02,550 --> 00:43:06,190
imagine anything you wanted open your

00:43:04,480 --> 00:43:08,200
system you can you ideally want to build

00:43:06,190 --> 00:43:15,550
a unit test and just gather the data on

00:43:08,200 --> 00:43:16,690
an ongoing basis so I think we're

00:43:15,550 --> 00:43:18,250
running out of time so I'll happily just

00:43:16,690 --> 00:43:19,060
hang around and and and shot two people

00:43:18,250 --> 00:43:20,440
in division afterwards

00:43:19,060 --> 00:43:23,880
but thank you very much for attention

00:43:20,440 --> 00:43:23,880

YouTube URL: https://www.youtube.com/watch?v=Ss4pUbq09Lw


