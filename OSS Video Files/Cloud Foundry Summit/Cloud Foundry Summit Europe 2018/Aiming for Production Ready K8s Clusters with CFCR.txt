Title: Aiming for Production Ready K8s Clusters with CFCR
Publication date: 2018-10-10
Playlist: Cloud Foundry Summit Europe 2018
Description: 
	Aiming for Production Ready K8s Clusters with CFCR - Morena De Liddo & Bengt Gadelha Hammarlund, Pivotal

Kubernetes offers an extensive set of features that can provide for different purposes, ranging from test deployments on localhost to deployments with heavy ML computations on GPUs. It can get overwhelming to figure out the specific features, or combinations of features, which may be needed for making a secure and resilient service. CFCR is an open source wrapper around Kubernetes that provides an opinionated K8S deployment, leveraging BOSH functionalities like scaling and self-healing. It offers a subset of Kubernetes features enriched by some functionalities of BOSH, and facilitates an easy learning curve. In this talk, we intend to explore what can make a K8s cluster ‘production ready’ for different business and how CFCR helps reduce the complexity of spinning a K8s cluster and surface only the most commonly used features.

https://cfseu18.sched.com/event/GATJ/aiming-for-production-ready-k8s-clusters-with-cfcr-morena-de-liddo-bengt-gadelha-hammarlund-pivotal
Captions: 
	00:00:00,030 --> 00:00:04,770
thank you everyone for coming we're here

00:00:02,820 --> 00:00:07,350
today to discuss strategies they have

00:00:04,770 --> 00:00:10,139
CFC are uses to help you deploy

00:00:07,350 --> 00:00:13,320
production-ready kubernetes clusters and

00:00:10,139 --> 00:00:13,920
we will try to highlight and share with

00:00:13,320 --> 00:00:15,509
you

00:00:13,920 --> 00:00:17,880
learnings that you can have from our

00:00:15,509 --> 00:00:18,690
experience but first let's introduce

00:00:17,880 --> 00:00:21,720
ourselves

00:00:18,690 --> 00:00:24,240
I'm banked I'm a software engineer at

00:00:21,720 --> 00:00:27,269
the cloud of steam in pivotal at Dublin

00:00:24,240 --> 00:00:29,820
and my team is responsible for

00:00:27,269 --> 00:00:32,219
maintaining production environments like

00:00:29,820 --> 00:00:36,390
DPL tracker and other kinds of

00:00:32,219 --> 00:00:38,309
deployments including CFC our Mirena I

00:00:36,390 --> 00:00:40,260
also work in the Dublin office at

00:00:38,309 --> 00:00:42,989
pivotal and I've been on the CFC our

00:00:40,260 --> 00:00:44,820
team for more than one year so we're

00:00:42,989 --> 00:00:46,350
gonna start by trying to pinpoint what's

00:00:44,820 --> 00:00:47,969
important when you want to set up a

00:00:46,350 --> 00:00:49,950
cornetist cluster that's ready for

00:00:47,969 --> 00:00:53,010
production especially describing the

00:00:49,950 --> 00:00:54,719
challenges you can face and what

00:00:53,010 --> 00:00:56,520
benefits you can gain from taking care

00:00:54,719 --> 00:00:59,570
of these aspects we're then gonna

00:00:56,520 --> 00:01:02,190
describe how kubernetes poses some

00:00:59,570 --> 00:01:05,070
obstacle to these efforts and how CFC

00:01:02,190 --> 00:01:07,320
our uses strategies to give a smoother

00:01:05,070 --> 00:01:11,340
developer and operator experiences and

00:01:07,320 --> 00:01:14,299
finally how CFC are works with

00:01:11,340 --> 00:01:18,240
kubernetes upgrades let's get started

00:01:14,299 --> 00:01:20,070
cool so when you're planning to

00:01:18,240 --> 00:01:22,110
provision a production environment you

00:01:20,070 --> 00:01:24,000
should be able to focus on all the

00:01:22,110 --> 00:01:26,930
elements that help you run your

00:01:24,000 --> 00:01:29,759
workloads as smoothly as possible

00:01:26,930 --> 00:01:34,790
coping with real-world traffic traffic

00:01:29,759 --> 00:01:37,560
and being able to protect the user data

00:01:34,790 --> 00:01:39,689
so we came up with this four main areas

00:01:37,560 --> 00:01:41,340
of concern that you should focus on when

00:01:39,689 --> 00:01:44,220
planning a production environment they

00:01:41,340 --> 00:01:46,200
are reliability security up-to-date Ness

00:01:44,220 --> 00:01:48,390
and performance

00:01:46,200 --> 00:01:53,369
so let's scratch the surface of each one

00:01:48,390 --> 00:01:55,770
so when we talk about reliability we

00:01:53,369 --> 00:01:58,320
picture operators that don't want or

00:01:55,770 --> 00:02:00,689
don't need to be actively running the

00:01:58,320 --> 00:02:04,860
environment themselves all the time and

00:02:00,689 --> 00:02:07,140
at the same time the development team or

00:02:04,860 --> 00:02:09,450
whoever has access to deploying into

00:02:07,140 --> 00:02:11,700
production should be able to do so and

00:02:09,450 --> 00:02:13,560
in the time that makes sense for the

00:02:11,700 --> 00:02:16,800
business and not on some kind of the

00:02:13,560 --> 00:02:19,739
Windell or any kind of policy that it's

00:02:16,800 --> 00:02:22,910
not part of running the business that

00:02:19,739 --> 00:02:25,410
will give you a requirement that we

00:02:22,910 --> 00:02:29,880
environmental that requires minimal

00:02:25,410 --> 00:02:37,950
intervention to be kept running when we

00:02:29,880 --> 00:02:40,200
talk about security yeah we talk about

00:02:37,950 --> 00:02:42,989
security we're talking about things such

00:02:40,200 --> 00:02:45,450
as being up-to-date with CV fixes and

00:02:42,989 --> 00:02:47,840
bug patches we talk about having control

00:02:45,450 --> 00:02:50,540
over who accesses the environment and

00:02:47,840 --> 00:02:53,760
minimizing damage when there's a breach

00:02:50,540 --> 00:02:57,630
which means that the tools that the

00:02:53,760 --> 00:02:59,549
environment also has already has make

00:02:57,630 --> 00:03:01,200
you focus on the workload security

00:02:59,549 --> 00:03:04,140
because you already control who accesses

00:03:01,200 --> 00:03:06,150
your VMs and your containers thank you

00:03:04,140 --> 00:03:09,180
as you can probably tell there's overlap

00:03:06,150 --> 00:03:10,950
between security and up-to-date news but

00:03:09,180 --> 00:03:12,750
the latter also involves having access

00:03:10,950 --> 00:03:14,519
to new tools and features that you can

00:03:12,750 --> 00:03:16,680
leverage to improve your workload and

00:03:14,519 --> 00:03:18,390
your environment as soon as possible and

00:03:16,680 --> 00:03:20,760
it also means that you don't end up

00:03:18,390 --> 00:03:25,470
using outdated and unsupported versions

00:03:20,760 --> 00:03:27,630
of software so this means that if you

00:03:25,470 --> 00:03:29,489
have control over your upgrade process

00:03:27,630 --> 00:03:31,500
you have boil upgrades where you don't

00:03:29,489 --> 00:03:35,430
have to work to worry about things such

00:03:31,500 --> 00:03:39,150
as workload downtime or what to do when

00:03:35,430 --> 00:03:41,579
the upgrade fails finally for

00:03:39,150 --> 00:03:43,920
performance we mean that your resources

00:03:41,579 --> 00:03:45,420
are easily optimizable and that your

00:03:43,920 --> 00:03:49,709
environment can adapt to the amount of

00:03:45,420 --> 00:03:51,900
traffic it gets which gives you a build

00:03:49,709 --> 00:03:53,819
so it means that you have ability to

00:03:51,900 --> 00:03:56,609
scale up and down vertically and

00:03:53,819 --> 00:04:01,230
horizontally so you can again adapt to

00:03:56,609 --> 00:04:02,970
your needs so at this point if you tried

00:04:01,230 --> 00:04:04,290
running or if you run kubernetes you

00:04:02,970 --> 00:04:06,150
know that deploying can be the smoothest

00:04:04,290 --> 00:04:07,769
part because there are many tools such

00:04:06,150 --> 00:04:10,560
as installers that help you during that

00:04:07,769 --> 00:04:12,630
phase data operations instead are

00:04:10,560 --> 00:04:14,459
significantly more complex and we're

00:04:12,630 --> 00:04:18,680
mostly talking about the production

00:04:14,459 --> 00:04:21,209
concerns that we are going to focus on

00:04:18,680 --> 00:04:22,950
this year one of the special interest

00:04:21,209 --> 00:04:24,770
groups in the kubernetes community

00:04:22,950 --> 00:04:28,430
conducted a survey and they show

00:04:24,770 --> 00:04:30,560
18% of the users was using unsupported

00:04:28,430 --> 00:04:32,389
versions of kubernetes they were at

00:04:30,560 --> 00:04:34,130
least three minor versions behind and

00:04:32,389 --> 00:04:36,259
this is not surprising because when you

00:04:34,130 --> 00:04:38,630
want to update you need a plan to make

00:04:36,259 --> 00:04:40,849
sure that all the parts that make up

00:04:38,630 --> 00:04:43,789
kubernetes behave as they should during

00:04:40,849 --> 00:04:45,919
and after the process and possibly

00:04:43,789 --> 00:04:48,470
without disrupting workload and API

00:04:45,919 --> 00:04:49,580
uptime this is not trivial and requires

00:04:48,470 --> 00:04:53,090
a very good knowledge of the

00:04:49,580 --> 00:04:56,000
chlorinators internals yeah and another

00:04:53,090 --> 00:04:59,360
crucial aspect is what platform you're

00:04:56,000 --> 00:05:03,169
planning to run your clusters on so if

00:04:59,360 --> 00:05:07,159
you choose let's for instance GAE you

00:05:03,169 --> 00:05:09,409
have most operations automated but you

00:05:07,159 --> 00:05:11,330
might not be willing to be locked into a

00:05:09,409 --> 00:05:13,250
vendor or you might already have a

00:05:11,330 --> 00:05:17,509
contract with some other called provider

00:05:13,250 --> 00:05:19,069
or you might have a hardware on frame

00:05:17,509 --> 00:05:21,979
Hardware where you want to run your

00:05:19,069 --> 00:05:24,319
clusters on the other thing to take into

00:05:21,979 --> 00:05:26,690
account is the security model that you

00:05:24,319 --> 00:05:29,810
are planning to use kubernetes has its

00:05:26,690 --> 00:05:34,159
own security recommendations that are

00:05:29,810 --> 00:05:36,469
important to keep your closers safe so

00:05:34,159 --> 00:05:38,570
let's take a look at so this is barely

00:05:36,469 --> 00:05:40,849
scratching the surface on how complex

00:05:38,570 --> 00:05:42,979
human at ease is let's take a look at

00:05:40,849 --> 00:05:47,150
how see FCR helps you achieve all those

00:05:42,979 --> 00:05:49,070
goals yes CSTR or Cloud Foundry

00:05:47,150 --> 00:05:51,409
container runtime previously known as

00:05:49,070 --> 00:05:53,300
scoobo tries to answer these questions

00:05:51,409 --> 00:05:56,389
by the way kuba is also the name of our

00:05:53,300 --> 00:05:58,909
mascot since he is an open-source boss

00:05:56,389 --> 00:06:00,530
release for kubernetes which i'll tries

00:05:58,909 --> 00:06:02,419
to take advantage of both the

00:06:00,530 --> 00:06:03,800
flexibility of kubernetes and the

00:06:02,419 --> 00:06:06,380
experience and the opinions that the

00:06:03,800 --> 00:06:10,069
bush community built with time it's

00:06:06,380 --> 00:06:11,840
available on GCP AWS OpenStack vSphere

00:06:10,069 --> 00:06:14,030
and SUNY Nasser and it's currently used

00:06:11,840 --> 00:06:17,150
in production by three customers and

00:06:14,030 --> 00:06:19,159
we're going to describe how it helps us

00:06:17,150 --> 00:06:21,169
with production concerns and kubernetes

00:06:19,159 --> 00:06:23,539
complexities especially focusing on

00:06:21,169 --> 00:06:25,719
what's provided by default and the key

00:06:23,539 --> 00:06:29,870
takeaways that you can learn from it

00:06:25,719 --> 00:06:31,639
cool so what you get by default when you

00:06:29,870 --> 00:06:35,599
use the FCR to deploy at keep it at ease

00:06:31,639 --> 00:06:38,750
cluster you'll get by default three

00:06:35,599 --> 00:06:40,700
master nodes the master knows that

00:06:38,750 --> 00:06:42,140
knows that contains all the processes

00:06:40,700 --> 00:06:45,610
that make up the kubernetes control

00:06:42,140 --> 00:06:47,900
plane you get a co-located sed process

00:06:45,610 --> 00:06:52,120
since you have three masters you have a

00:06:47,900 --> 00:06:55,520
cluster a net city cluster with three

00:06:52,120 --> 00:06:58,010
members the Exedy is the distributed

00:06:55,520 --> 00:07:00,970
database that Karina is uses to maintain

00:06:58,010 --> 00:07:05,240
the cluster state you get by default

00:07:00,970 --> 00:07:07,160
three worker nodes the worker knows that

00:07:05,240 --> 00:07:10,190
the ones that contains the processes

00:07:07,160 --> 00:07:14,000
that manages the containers that run

00:07:10,190 --> 00:07:16,550
your workloads so you get three worker

00:07:14,000 --> 00:07:19,220
nodes all all of these spread across

00:07:16,550 --> 00:07:22,870
three different availability zones you

00:07:19,220 --> 00:07:25,880
see why is this important and assume

00:07:22,870 --> 00:07:28,100
let's start with reliability now we're

00:07:25,880 --> 00:07:31,490
going to describe how csr takes care of

00:07:28,100 --> 00:07:33,830
production concerns for production

00:07:31,490 --> 00:07:35,419
readiness you want a stable product we

00:07:33,830 --> 00:07:38,180
had pivotal use test-driven development

00:07:35,419 --> 00:07:39,919
and CFCs fully tested to make sure that

00:07:38,180 --> 00:07:42,500
every change we introduced as a break

00:07:39,919 --> 00:07:44,690
the existing setup we have unit tests

00:07:42,500 --> 00:07:46,970
integration tests and turbulence tests

00:07:44,690 --> 00:07:50,180
which introduce failure scenarios to

00:07:46,970 --> 00:07:54,530
verify what happens in those disaster

00:07:50,180 --> 00:07:56,690
cases what you want is make sure that

00:07:54,530 --> 00:07:58,400
you cover all the code yard for example

00:07:56,690 --> 00:08:00,500
if you have custom scripts for your

00:07:58,400 --> 00:08:04,820
upgrades and test all the switches and

00:08:00,500 --> 00:08:07,669
knobs your kits configuration plus we

00:08:04,820 --> 00:08:10,130
use the official representation packages

00:08:07,669 --> 00:08:12,470
we don't have a for kubernetes so we are

00:08:10,130 --> 00:08:14,090
vanilla and we run conformance tests

00:08:12,470 --> 00:08:16,280
which are parts of a certification

00:08:14,090 --> 00:08:18,560
program in the case community to make

00:08:16,280 --> 00:08:20,270
sure that our users code we run as

00:08:18,560 --> 00:08:22,850
expected based on the communicates

00:08:20,270 --> 00:08:24,500
functionality so you either want to run

00:08:22,850 --> 00:08:29,030
these tests against your environment or

00:08:24,500 --> 00:08:32,150
use a conformant installer so continuing

00:08:29,030 --> 00:08:34,669
reliability aj or high availability is

00:08:32,150 --> 00:08:37,520
really important if you want to have a

00:08:34,669 --> 00:08:40,000
reliable environment

00:08:37,520 --> 00:08:42,680
what CFC r does so help you on that is

00:08:40,000 --> 00:08:44,480
providing you three master nodes spread

00:08:42,680 --> 00:08:47,270
across different availability zones so

00:08:44,480 --> 00:08:51,050
even if one of them goes down you still

00:08:47,270 --> 00:08:52,160
have a working cluster it's important to

00:08:51,050 --> 00:08:54,589
notice that you can

00:08:52,160 --> 00:08:57,290
the LCD for located at the master so you

00:08:54,589 --> 00:09:00,290
have the fcd spread across the

00:08:57,290 --> 00:09:05,769
availability zones too it's important

00:09:00,290 --> 00:09:08,810
that the @ CD yet city uses a specific

00:09:05,769 --> 00:09:11,360
algorithm to maintain its consistency

00:09:08,810 --> 00:09:14,569
and he needs at least three nodes so you

00:09:11,360 --> 00:09:18,139
get that by the phone CFC R and less

00:09:14,569 --> 00:09:19,670
least but not less than at least you get

00:09:18,139 --> 00:09:21,920
three worker nodes spread across the

00:09:19,670 --> 00:09:28,310
availability zones too so you avoid

00:09:21,920 --> 00:09:31,279
workload downtime so apart from sending

00:09:28,310 --> 00:09:33,290
up H a components we take advantage of

00:09:31,279 --> 00:09:37,610
the auto healing capabilities that Bosch

00:09:33,290 --> 00:09:39,649
offers for VMs and monitor processes so

00:09:37,610 --> 00:09:41,949
these two aspects help reduce

00:09:39,649 --> 00:09:45,500
maintenance overhead for operators and

00:09:41,949 --> 00:09:47,660
increase sorry and relieve pressure in

00:09:45,500 --> 00:09:51,019
case of disasters for in for example

00:09:47,660 --> 00:09:53,930
infrastructure disasters finally we use

00:09:51,019 --> 00:09:55,790
BB our Bosh back embarrassed or for in

00:09:53,930 --> 00:09:59,329
the wreck acid backing up and restoring

00:09:55,790 --> 00:10:01,220
our LCD data and we use the sed CLI for

00:09:59,329 --> 00:10:02,750
managing snapshots you want to make sure

00:10:01,220 --> 00:10:05,720
you have a strategy for backing up a

00:10:02,750 --> 00:10:09,230
restoring both for when for example you

00:10:05,720 --> 00:10:12,199
have infrastructure disasters and you

00:10:09,230 --> 00:10:14,660
want to use backups or if you're running

00:10:12,199 --> 00:10:19,149
an upgrade and you want to roll back in

00:10:14,660 --> 00:10:22,430
case of issues cool going on to security

00:10:19,149 --> 00:10:25,610
as I said before Cuban Eris has its own

00:10:22,430 --> 00:10:27,380
recommendations on security one of them

00:10:25,610 --> 00:10:28,699
is that all communication between the

00:10:27,380 --> 00:10:33,110
processes that make up the cluster

00:10:28,699 --> 00:10:35,800
should be protected over TLS so you get

00:10:33,110 --> 00:10:38,870
that by default on if you use the FCR

00:10:35,800 --> 00:10:41,029
the sed cluster all the nodes you need

00:10:38,870 --> 00:10:43,360
to communicate with each other and all

00:10:41,029 --> 00:10:46,069
the communication is done over TLS and

00:10:43,360 --> 00:10:47,750
all the processes the make up the

00:10:46,069 --> 00:10:50,449
kubernetes cluster on the master and the

00:10:47,750 --> 00:10:52,519
worker most of them need to talk to the

00:10:50,449 --> 00:10:56,120
API server on the worker on the master

00:10:52,519 --> 00:10:58,459
nodes and the API server needs to talk

00:10:56,120 --> 00:11:00,050
to the cubelet which is the process on

00:10:58,459 --> 00:11:03,829
the worker nodes that manages the

00:11:00,050 --> 00:11:05,720
containers and talk to the HDD node to

00:11:03,829 --> 00:11:10,790
maintain the cluster state all

00:11:05,720 --> 00:11:14,480
this is done over TLS the dashboard is

00:11:10,790 --> 00:11:17,449
also protected so you don't run into the

00:11:14,480 --> 00:11:20,660
problem that Tesla had that they had

00:11:17,449 --> 00:11:24,110
clusters with unprotected dashboards and

00:11:20,660 --> 00:11:29,899
they had hackers that were using their

00:11:24,110 --> 00:11:32,170
resources to do crypto mining all these

00:11:29,899 --> 00:11:37,910
certificates are auto-generated and

00:11:32,170 --> 00:11:41,329
securely stored using cred hub more on

00:11:37,910 --> 00:11:43,399
security kubernetes also recommends that

00:11:41,329 --> 00:11:46,550
you use role based access control or our

00:11:43,399 --> 00:11:49,550
buck so what CSer does it binds

00:11:46,550 --> 00:11:53,779
permissions for security for specific

00:11:49,550 --> 00:11:56,060
users and service accounts so that the

00:11:53,779 --> 00:11:57,829
cluster admin has control complete

00:11:56,060 --> 00:11:59,930
control over the cluster while the

00:11:57,829 --> 00:12:01,610
kubernetes processes have only the

00:11:59,930 --> 00:12:05,750
necessary permissions they need to run

00:12:01,610 --> 00:12:08,329
and finally we use Bosh themselves to be

00:12:05,750 --> 00:12:10,220
always up to date when it comes to for

00:12:08,329 --> 00:12:12,050
example purchasing the operating system

00:12:10,220 --> 00:12:14,059
and it's really important that you test

00:12:12,050 --> 00:12:15,980
your kubernetes configuration against

00:12:14,059 --> 00:12:23,019
your operating system you're migrating

00:12:15,980 --> 00:12:23,019
to moving on to up-to-date news are

00:12:27,459 --> 00:12:32,720
should be running yes

00:12:29,689 --> 00:12:35,990
our pipeline tests upgrades between the

00:12:32,720 --> 00:12:37,790
latest released CFC our version and the

00:12:35,990 --> 00:12:40,579
latest changes in our APIs so we catch

00:12:37,790 --> 00:12:42,610
disruptive changes and we have smooth

00:12:40,579 --> 00:12:46,730
migrations so we can make guaranteed

00:12:42,610 --> 00:12:50,709
migrations between consecutive CFC our

00:12:46,730 --> 00:12:53,300
versions our upgrade tests focus on

00:12:50,709 --> 00:12:56,899
minimal workload and API downtime we

00:12:53,300 --> 00:12:58,550
have a 99% threshold and this is

00:12:56,899 --> 00:13:00,829
especially important when we bump gates

00:12:58,550 --> 00:13:03,649
because we want to catch breaking

00:13:00,829 --> 00:13:05,600
changes in the latest versions so it's

00:13:03,649 --> 00:13:07,519
really important that when you are

00:13:05,600 --> 00:13:09,439
planning to upgrade you check released

00:13:07,519 --> 00:13:12,410
notes so that things such as the

00:13:09,439 --> 00:13:14,089
vacations and changes in the default

00:13:12,410 --> 00:13:18,890
values don't come as surprises when you

00:13:14,089 --> 00:13:21,140
upgrade cool let's jump into performance

00:13:18,890 --> 00:13:22,760
so as we said before it's important to

00:13:21,140 --> 00:13:26,390
be able to scale up and down both

00:13:22,760 --> 00:13:29,810
vertically and horizontally using Bosch

00:13:26,390 --> 00:13:32,900
that's easy to do and you're just a

00:13:29,810 --> 00:13:34,940
Bosch deploy command away from that so

00:13:32,900 --> 00:13:39,830
this this is a screenshot of a scale up

00:13:34,940 --> 00:13:42,830
yamo and you could just easily as easily

00:13:39,830 --> 00:13:47,570
modify the original manifest that used

00:13:42,830 --> 00:13:49,730
to deploy the cluster to so as I said

00:13:47,570 --> 00:13:52,570
this is all a Bosch deploy away from you

00:13:49,730 --> 00:13:55,700
and in this case we're changing the

00:13:52,570 --> 00:13:57,740
number of VMs both on the masters and on

00:13:55,700 --> 00:14:00,800
the workers to five from the original

00:13:57,740 --> 00:14:03,650
three which is the result of scaling and

00:14:00,800 --> 00:14:05,660
we are changing the vm type to have more

00:14:03,650 --> 00:14:10,880
memory which is a kind of vertical

00:14:05,660 --> 00:14:12,860
scaling something else that we expose is

00:14:10,880 --> 00:14:16,250
a feature from Cuban Ares called

00:14:12,860 --> 00:14:19,490
horizontal part autoscaler you can set a

00:14:16,250 --> 00:14:21,500
threshold on CPU usage and other custom

00:14:19,490 --> 00:14:24,640
metrics to be able to automatically

00:14:21,500 --> 00:14:26,600
scale up your pots you brought replicas

00:14:24,640 --> 00:14:29,570
when the threshold is met

00:14:26,600 --> 00:14:32,150
so as for performance the really key

00:14:29,570 --> 00:14:33,800
aspect is that you are able to scale up

00:14:32,150 --> 00:14:35,750
and down because you have a repeatable

00:14:33,800 --> 00:14:37,700
and reproducible deployment process and

00:14:35,750 --> 00:14:40,190
we get this from Bosch but you want to

00:14:37,700 --> 00:14:44,540
find a strategy to have the same kind of

00:14:40,190 --> 00:14:50,630
reproducibility so let's talk about

00:14:44,540 --> 00:14:52,630
upgrading the cluster what does the a

00:14:50,630 --> 00:14:55,700
good upgrade process look like

00:14:52,630 --> 00:14:58,400
you should always upgrade a healthy

00:14:55,700 --> 00:15:02,840
cluster so you should check the cluster

00:14:58,400 --> 00:15:05,570
health first do a backup and then

00:15:02,840 --> 00:15:08,600
upgrade e at C denotes the master nodes

00:15:05,570 --> 00:15:10,490
and the worker nodes then after the

00:15:08,600 --> 00:15:12,170
upgrade it should check the cluster

00:15:10,490 --> 00:15:17,270
health again to make sure that the

00:15:12,170 --> 00:15:22,520
upgrade was actually successful so how

00:15:17,270 --> 00:15:27,320
the CFC R does the closer upgrade so we

00:15:22,520 --> 00:15:29,930
start with the master nodes so the first

00:15:27,320 --> 00:15:32,070
thing that happens is that the add city

00:15:29,930 --> 00:15:34,950
instance will leave the cluster

00:15:32,070 --> 00:15:36,630
we do that so the sed cluster is aware

00:15:34,950 --> 00:15:39,210
that that instance is not part of the

00:15:36,630 --> 00:15:46,650
cluster anymore so you can maintain the

00:15:39,210 --> 00:15:49,920
cluster consistency so now Bosch is can

00:15:46,650 --> 00:15:51,960
safely upgrade the processes that run on

00:15:49,920 --> 00:15:55,670
the master node so it will shut down the

00:15:51,960 --> 00:15:59,580
processes upgrade them and restart them

00:15:55,670 --> 00:16:02,430
the same process will of course and the

00:15:59,580 --> 00:16:03,660
and then DHCD rejoins the cluster so now

00:16:02,430 --> 00:16:07,980
you have the three nodes in the cluster

00:16:03,660 --> 00:16:12,510
and the same process will go on on all

00:16:07,980 --> 00:16:14,000
the other master nodes so now it's time

00:16:12,510 --> 00:16:17,610
for the worker nodes but they're

00:16:14,000 --> 00:16:21,240
slightly different because they're

00:16:17,610 --> 00:16:23,910
running pots yeah so pods are the

00:16:21,240 --> 00:16:25,530
minimal deployable unit that you can use

00:16:23,910 --> 00:16:29,570
to deploy something and keep a nice

00:16:25,530 --> 00:16:34,140
cluster so you need a different strategy

00:16:29,570 --> 00:16:37,470
we use a process called drain to safely

00:16:34,140 --> 00:16:39,180
upgrade the worker node so the first

00:16:37,470 --> 00:16:41,430
thing that happens is that the work the

00:16:39,180 --> 00:16:44,640
worker node that's being upgraded it's

00:16:41,430 --> 00:16:47,600
made not scheduled so no new work load

00:16:44,640 --> 00:16:53,040
will be deployable and this worker node

00:16:47,600 --> 00:16:54,780
then we will stop each each pod or

00:16:53,040 --> 00:16:56,520
workload is running on the worker and

00:16:54,780 --> 00:16:58,590
they will be rescheduled by this

00:16:56,520 --> 00:17:01,500
scheduler which is a process that run on

00:16:58,590 --> 00:17:06,089
the master node they will be rescheduled

00:17:01,500 --> 00:17:09,780
and then Bosch can safely start

00:17:06,089 --> 00:17:11,940
upgrading the worker node so we'll stop

00:17:09,780 --> 00:17:16,320
all the processes replace them by the

00:17:11,940 --> 00:17:20,160
new version and start them again the

00:17:16,320 --> 00:17:23,490
same thing oh yeah and the node will be

00:17:20,160 --> 00:17:26,570
made scheduled again so the same thing

00:17:23,490 --> 00:17:30,120
will go on on all the other worker nodes

00:17:26,570 --> 00:17:34,370
made on scheduled upgrade and then

00:17:30,120 --> 00:17:39,240
schedule again and then you have a

00:17:34,370 --> 00:17:41,460
upgraded cluster let's look at the

00:17:39,240 --> 00:17:44,100
differences between the previous cluster

00:17:41,460 --> 00:17:45,809
and the updated cluster basically they

00:17:44,100 --> 00:17:48,600
are the same they look the same

00:17:45,809 --> 00:17:52,080
they'd have a different version of Cuban

00:17:48,600 --> 00:17:55,080
Aries potentially running and one thing

00:17:52,080 --> 00:17:57,480
that you might that you want to be aware

00:17:55,080 --> 00:18:00,360
of is that the workloads are scheduled

00:17:57,480 --> 00:18:03,509
differently than the original previous

00:18:00,360 --> 00:18:05,940
version so you have to keep in mind that

00:18:03,509 --> 00:18:08,549
to avoid workload downtime you should

00:18:05,940 --> 00:18:11,610
have at least two replicas of each part

00:18:08,549 --> 00:18:15,360
that you're running on the workers so

00:18:11,610 --> 00:18:16,529
this is just the default way that CFC

00:18:15,360 --> 00:18:18,509
are deals with upgrades

00:18:16,529 --> 00:18:21,659
maybe you want a different upgrade

00:18:18,509 --> 00:18:23,700
strategy especially first for scheduling

00:18:21,659 --> 00:18:26,730
because for example you might want to

00:18:23,700 --> 00:18:29,639
have instead of just three three nodes

00:18:26,730 --> 00:18:31,799
you might want to add a node so that you

00:18:29,639 --> 00:18:35,759
don't have just the node that's been

00:18:31,799 --> 00:18:37,379
operating without without pods or maybe

00:18:35,759 --> 00:18:39,869
you want to deploy something on top of

00:18:37,379 --> 00:18:41,789
kubernetes that deals with scheduling in

00:18:39,869 --> 00:18:43,919
a better way instead of leaving the

00:18:41,789 --> 00:18:46,889
third node empty at the end of the

00:18:43,919 --> 00:18:49,950
upgrade so this is almost the end of the

00:18:46,889 --> 00:18:56,869
talk I'm just giving you a quick recap

00:18:49,950 --> 00:19:00,539
so oh sorry we said that for reliability

00:18:56,869 --> 00:19:03,419
reliability gives us an environment that

00:19:00,539 --> 00:19:05,490
needs minimal intervention for to be

00:19:03,419 --> 00:19:08,149
kept running so this is achieved by

00:19:05,490 --> 00:19:10,860
having a fully tested product product

00:19:08,149 --> 00:19:12,659
possibly conformant by having a che

00:19:10,860 --> 00:19:14,820
components possibly Auto healing

00:19:12,659 --> 00:19:19,259
components and having a backup research

00:19:14,820 --> 00:19:20,999
strategy as for security we can focus on

00:19:19,259 --> 00:19:23,820
workload security because we already

00:19:20,999 --> 00:19:25,940
have the kubernetes recommendation baked

00:19:23,820 --> 00:19:30,090
in our environment so we have

00:19:25,940 --> 00:19:32,580
communication over TLS our back and we

00:19:30,090 --> 00:19:37,740
have access to Samsung and operating

00:19:32,580 --> 00:19:39,450
system patches having control over our

00:19:37,740 --> 00:19:41,279
upgrade process means that we have

00:19:39,450 --> 00:19:43,200
boring upgrades and we do this by

00:19:41,279 --> 00:19:45,210
testing our upgrades before going to

00:19:43,200 --> 00:19:49,220
production and making sure we know

00:19:45,210 --> 00:19:53,029
what's new in the new kits version and

00:19:49,220 --> 00:19:55,830
finally we have a performant teamwork

00:19:53,029 --> 00:19:57,539
environment which is able to scale

00:19:55,830 --> 00:19:58,650
because we have a repeatable deployment

00:19:57,539 --> 00:20:00,830
process and we use

00:19:58,650 --> 00:20:05,570
such as the original portal 2's killer

00:20:00,830 --> 00:20:08,820
so these are just some references ah

00:20:05,570 --> 00:20:10,650
repos are slack in our backlog feel free

00:20:08,820 --> 00:20:12,440
to reach out and thanks for your

00:20:10,650 --> 00:20:18,880
attention thank you

00:20:12,440 --> 00:20:18,880

YouTube URL: https://www.youtube.com/watch?v=VXlCI62nH_E


